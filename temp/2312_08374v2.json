{
  "id": "http://arxiv.org/abs/2312.08374v2",
  "title": "Unsupervised Social Event Detection via Hybrid Graph Contrastive Learning and Reinforced Incremental Clustering",
  "authors": [
    "Yuanyuan Guo",
    "Zehua Zang",
    "Hang Gao",
    "Xiao Xu",
    "Rui Wang",
    "Lixiang Liu",
    "Jiangmeng Li"
  ],
  "abstract": "Detecting events from social media data streams is gradually attracting\nresearchers. The innate challenge for detecting events is to extract\ndiscriminative information from social media data thereby assigning the data\ninto different events. Due to the excessive diversity and high updating\nfrequency of social data, using supervised approaches to detect events from\nsocial messages is hardly achieved. To this end, recent works explore learning\ndiscriminative information from social messages by leveraging graph contrastive\nlearning (GCL) and embedding clustering in an unsupervised manner. However, two\nintrinsic issues exist in benchmark methods: conventional GCL can only roughly\nexplore partial attributes, thereby insufficiently learning the discriminative\ninformation of social messages; for benchmark methods, the learned embeddings\nare clustered in the latent space by taking advantage of certain specific prior\nknowledge, which conflicts with the principle of unsupervised learning\nparadigm. In this paper, we propose a novel unsupervised social media event\ndetection method via hybrid graph contrastive learning and reinforced\nincremental clustering (HCRC), which uses hybrid graph contrastive learning to\ncomprehensively learn semantic and structural discriminative information from\nsocial messages and reinforced incremental clustering to perform efficient\nclustering in a solidly unsupervised manner. We conduct comprehensive\nexperiments to evaluate HCRC on the Twitter and Maven datasets. The\nexperimental results demonstrate that our approach yields consistent\nsignificant performance boosts. In traditional incremental setting,\nsemi-supervised incremental setting and solidly unsupervised setting, the model\nperformance has achieved maximum improvements of 53%, 45%, and 37%,\nrespectively.",
  "text": "Unsupervised Social Event Detection via Hybrid Graph Contrastive Learning and\nReinforced Incremental Clustering\nYuanyuan Guoa,b, Zehua Zanga,b, Hang Gaoa,b, Xiao Xuc, Rui Wanga,b, Lixiang Liua,b, \u0000Jiangmeng Lia,b\naScience & Technology on Integrated Information System Laboratory, Institute of Software, Chinese Academy of Sciences, Beijing, China.\nbUniversity of Chinese Academy of Sciences, Beijing, China.\ncNational Defense University, Beijing, China.\nAbstract\nDetecting events from social media data streams is gradually attracting researchers. The innate challenge for detecting events is\nto extract discriminative information from social media data thereby assigning the data into different events. Due to the excessive\ndiversity and high updating frequency of social data, using supervised approaches to detect events from social messages is hardly\nachieved. To this end, recent works explore learning discriminative information from social messages by leveraging graph con-\ntrastive learning (GCL) and embedding clustering in an unsupervised manner. However, two intrinsic issues exist in benchmark\nmethods: conventional GCL can only roughly explore partial attributes, thereby insufficiently learning the discriminative informa-\ntion of social messages; for benchmark methods, the learned embeddings are clustered in the latent space by taking advantage of\ncertain specific prior knowledge, which conflicts with the principle of unsupervised learning paradigm. In this paper, we propose a\nnovel unsupervised social media event detection method via hybrid graph contrastive learning and reinforced incremental clustering\n(HCRC), which uses hybrid graph contrastive learning to comprehensively learn semantic and structural discriminative information\nfrom social messages and reinforced incremental clustering to perform efficient clustering in a solidly unsupervised manner. We\nconduct comprehensive experiments to evaluate HCRC on the Twitter and Maven datasets. The experimental results demonstrate\nthat our approach yields consistent significant performance boosts. In traditional incremental setting, semi-supervised incremental\nsetting and solidly unsupervised setting, the model performance has achieved maximum improvements of 53%, 45%, and 37%,\nrespectively.\nKeywords: Event detection, Unsupervised learning, Graph contrastive learning, Incremental clustering, Reinforcement learning\n1. Introduction\nWith the continuous development of social networking ser-\nvices, the rapidly growing users spread worldwide. According\nto statistics, there are 4.74 billion social media users around\nthe world, equating to 59.3% of the total global population [1].\nSocial media becomes a focal point for researchers to gather in-\nformation on events happening immediately [2]. A long-lasting\nchallenge of such behavior is countless routine instant messages\non social media. For instance, around 10,033 tweets are posted\nper second on average as of May 2022 [3], and the events of\nsuch messages are generally inconsistent with historical data so\nthat classifying the instant events based solely on historical data\nis inaccessible and labeling enough new events requires incon-\nceivable efforts. However, the previous works [4, 5, 6, 7, 8]\nprimarily focused on static event detection, which clearly does\nnot align with real-world application scenarios. Therefore, re-\ncent works explore capturing discriminative information from\ninstant events thereby performing the incremental clustering\nby leveraging graph neural networks (GNNs) and contrastive\nlearning in an unsupervised manner [9].\nEmail address: jiangmeng2019@iscas.ac.cn (\u0000Jiangmeng Li)\nIn the realm of unsupervised social event detection, one en-\ngages in the intricate process of unearthing clusters that embody\nreal-world events within the ever-flowing social stream (refer\nto Section 4.2 for details). State-of-the-art unsupervised social\nevent detection methods [10, 11, 9, 12] explore to jointly learn\nsemantic and structural information from the social data by\nleveraging GNNs. Specifically, the content and corresponding\nattributes, e.g., location, post time, etc., of social messages are\nmapped from the data space into the latent space by a fixed pre-\ntrained feature extractor, e.g., en core web lg [13]. Benchmark\nmethods explore the relationships between social messages and\nfurther extract discriminative information from the raw data to\nconvert it into graph-based data, i.e., nodes of the graph de-\nnote the social messages, thereby assigning the data into dif-\nferent events. The semantic and structural information can be\njointly captured by leveraging a well-designed GNN and fur-\nther contrasting the node embeddings. However, such a learn-\ning paradigm cannot sufficiently explore the semantic informa-\ntion of social messages since only partial attributes are consid-\nered by the model, and the semantic information is learned by\na fixed feature extractor. In the latent space, the learned em-\nbeddings of social messages are clustered. Yet, there exists an\nintrinsic issue with current approaches. In detail, the adopted\nclustering approach requires specific prior knowledge, e.g., the\nPreprint submitted to Knowledge-Based Systems\nDecember 18, 2023\narXiv:2312.08374v2  [cs.SI]  15 Dec 2023\n(a) Traditional\n(b) Semi-Supervised\n(c) Solid\nNMI\nrandom\nrandom\n10%\n10%\n10%\nFigure 1:\nComparison of various settings using NMI Metric. Experimental results show our method outperforms the baselines under different settings (refer to\nSection 5.2 for details).\npre-set hyperparameter k for K-Means, which conflicts with the\nprinciple of the unsupervised learning paradigm.\nTo this end, we propose HCRC, short for Hybrid graph\nContrastive learning and Reinforced incremental Clustering,\nwhich is orthogonal to existing methods in two key ingredi-\nents: 1) HCRC innovatively proposes a simple yet effective\napproach to build social message graphs comprehensively in-\ncluding the content and attributes, and the proposed hybrid\ngraph contrastive learning contains the graph-level and node-\nlevel contrasts, which jointly empowers the model to suffi-\nciently learn the semantic and structural information from the\nsocial data. The graph-level contrast builds a trainable approach\nto learning discriminative semantic information from the con-\ntent and attributes of social messages, and the node-level con-\ntrast improves the model to capture valuable structural informa-\ntion from the social message graph; 2) The proposed reinforced\nincremental clustering enables HCRC to perform efficient clus-\ntering on the instantly updated social data in a solidly unsu-\npervised manner, which is proved in Fig. 1. Concretely, the\ncontributions of this paper are four-fold:\n• We present a novel unsupervised social event detection\narchitecture, namely HCRC, and empirically demonstrate\nthe effectiveness of HCRC on various benchmarks.\n• We propose a simple yet effective approach to building so-\ncial message graphs, and the proposed hybrid graph con-\ntrastive learning boosts the model’s capacity to learn dis-\ncriminative social message embeddings.\n• Guided by deep reinforcement learning, a density-based\nspatial clustering module is proposed to perform incre-\nmental social event detection in a solidly unsupervised\nmanner.\n• Sufficient experiments further prove the interpretability\nand effectiveness of the proposed HCRC.\n2. Related Works\n2.1. Social Event Detection\nAn event is an occurrence causing a change in the vol-\nume of text data that discusses the associated topic at a spe-\ncific time [14]. Social event detection aims at clustering so-\ncial messages based on their correlations from social media\nstreams.\nSome classic works [15, 16] design different fea-\nture engineering to build social message objects. Later, more\nworks [17, 18, 19] adopt pre-trained language models to get\nbetter representations of social messages. To better model re-\nlationships between messages, KPGNN [9] first uses a hetero-\ngeneous GNN-based knowledge-preserving incremental social\nevent detection model.In order to dynamically adjust to the\nevolving data, KPGNN incorporates contrastive loss terms that\neffectively handle varying numbers of event classes [9]. QS-\nGNN [12] enhances the transfer of knowledge from known to\nunknown domains by leveraging the most valuable informa-\ntion from known samples and reliable knowledge transfer tech-\nniques. Researchers also detect events and discover event evo-\nlution in heterogeneous information graphs [20]. Due to the\never-changing nature of social media, some works focus on\ndynamic representations of heterogeneous information graphs\n[21, 22, 23].\n2.2. Graph Contrastive Learning\nA graph contrastive learning (GCL) framework usually con-\nsists of a graph views generation component to construct posi-\ntive and negative views and a contrastive objective to discrim-\ninate positive pairs from negative pairs [24]. Grace [25] gen-\nerates two graph views by corruption and learns node repre-\nsentation by maximizing the agreement of node representations\nin these two views. Further, ProGCL [26] constitutes a mea-\nsure for negatives’ hardness together with similarity to tackle\nthe problem of hard negative samples. Several works have pro-\nposed trainable augmentation strategies [27, 28] to learn a drop\nprobability distribution over nodes or edges. Differently, Sim-\nGRACE [29] proposes a Simple framework for GCL, which\ndoes not require data augmentations.\n2\n2.3. Incremental Clustering Algorithm\nAn incremental algorithm can process its input serially, i.e.,\nin the order that the input is fed to the algorithm, unlike an of-\nfline algorithm with the entire input available from the start. For\nexample, using the hash strategy and avoiding much similarity\ncalculation, Locality-Sensitive Hashing (LSH) [30] is widely\nused for data clustering and nearest neighbor search. SinglePass\nclustering [31] is a simple and efficient incremental clustering\nalgorithm. Since each data only needs to flow through the algo-\nrithm once, the efficiency is much higher than offline algorithms\nsuch as K-Means [32] or KNN [33]. There are also works that\nimprove other offline algorithms to incremental scenarios, such\nas incremental K-Means and incremental DBSCAN algorithms\n[34]. Some recent works focus on dynamically adjusting algo-\nrithms to better adapt to streaming data.\n2.4. Deep Reinforcement Learning\nDeep Reinforcement Learning (DRL) is learning an agent\nmaking sequential decisions to maximize accumulative re-\nwards.\nThere are two categories of DRL methods: value-\nbased and policy-gradient methods. The value-based methods\n[35, 36, 37, 38] are limited to the environments with discrete\naction space estimating the Q-value of the actions and choosing\nthe largest one. By comparison, the policy-gradient methods\n[39, 40, 41, 42, 43] are designed to work with environments\nthat have either discrete or continuous action spaces, and they\nuse action distributions, such as Normal distribution, to sample\nactions during the learning process.\n3. Preliminary\n3.1. Graph Convolutional Network\nGNNs [44, 45, 46, 47] have received much attention recently.\nSpecifically, graph convolutional network (GCN) [46] is widely\nused due to its excellent ability to analyze graph-structured\ndata. In detail, the architecture of GCN is defined as:\nH(l+1) = σ( ˆD\n−1/2 ˆA ˆD\n−1/2H(l)W(l)),\n(1)\nwhere H(l) is the node embedding matrix of the l-th layer for\nl ∈[1,. . . , L]. ˆA is the adjacency matrix with self-loops.\nˆD\nis the degree matrix and\nˆDii = P\nj ˆAij. W(l) is the trainable\nweight matrix of the l-th layer. σ(·) is an activation function,\ne.g., ReLU(·) = max(0, ·).\n3.2. Graph Contrastive Learning\nRecently, graph contrastive learning (GCL) has emerged as\na promising approach to learning graph representations. The\nprimary objective of GCL is to facilitate the creation of highly\neffective representations through the agglomeration of semanti-\ncally similar pairs and the divergence of dissimilar pairs. For a\ngiven graph G, two graph views, ˆG1 and ˆG2, are generated via\naugmentations T (·), which consist of node dropping, edge per-\nturbation, attribute masking, and subgraph [48]. Then, a GNN-\nbased encoder, denoted as f(·), extracts node representations\nU and V for different views. Specifically, node embeddings in\nthe two generated views are denoted as U = f (X1, A1) and\nV = f (X2, A2), where X∗and A∗are the feature matrices and\nadjacency matrices of the views. After that, a contrastive objec-\ntive is employed to contrast the embeddings of the same node\nin the two views with other node embeddings. Specifically, for\nany node k, the embedding obtained in one view uk is deemed\nas the anchor, and the embedding of it in the other view vk is\nregarded as the positive sample, whereas the remaining embed-\ndings in two views are considered negative samples. Referring\nto the loss proposed in GCA [49], we define the pairwise objec-\ntive for each positive pair (uk, vk) as\nℓ(uk, vk) = log\neθ(uk,vk)/τ\neθ(uk,vk)/τ + P\nm,k eθ(uk,vm)/τ + P\nm,k eθ(uk,um)/τ ,\n(2)\nwhere τ is a temperature parameter.\nu∗∈U and v∗∈V.\nθ(u, v) = s(g(u), g(v)), where s(·, ·) is the cosine similarity and\ng(·) is the nonlinear projection, which is a two-layer perception\nmodel. The objective to be maximized is defined as the average\nover all positive pairs given by\nJ = 1\n2N\nN\nX\nk=1\n[ℓ(uk, vk) + ℓ(vk, uk)] .\n(3)\n3.3. Reinforcement Learning\nConcepts. The state space denoted as S represents the agent’s\ncurrent situation, which is treated as the input of agents. The\naction space, denoted as A, is a set of candidate actions for the\nagent. The reward function denoted as R, trains agents with\nrespect to maximizing the cumulative reward.\nTrajectory. For reinforcement learning, at t-th step, the agent\ngets the state st from the environment and then samples an ac-\ntion from its policy at ∼π(at|st), where 0 ≤t ≤T. The executed\naction leads the environment to a new state st+1 ∼p(st|at+1), and\nthe agent gets a new reward rt = r(st, at, st+1) where p is the dy-\nnamics of the environment, and r is the reward function. The\ntrajectory τ is represented as {s0, a0, r0, ..., sT−1, aT−1, rT−1, sT}.\n4. Methodology\n4.1. Notations\nWe summarize the main notations in Table 1.\n4.2. Problem Formulation\nThe social stream is a continuous sequence of messages. Mi\ndenotes a message block containing all the messages during a\ncertain time period and Mi =\nn\nmj|1 ≤j ≤|Mi |\no\n, where |Mi| is\nthe total number of messages contained by Mi, and mj is a spe-\ncific message. As shown in Fig. 3, given a message block Mi,\na social event detection model aims to find clusters and each\ncluster denotes a real-world event containing a set of correlated\nsocial messages. Further, an incremental social event detection\nmodel detects events from continuous message blocks, which\nadds newly arrived messages to previous events or generates\nnew event clusters successively.\n3\nMessage \nBlock M�\nGraph \nConstruction\nGraph Contrastive \nLearning\nDRL-SinglePass\nClustering\nNext\n Block M�+1\nInput Data \nTwo Methods\nNode & Graph Level\nModify Threshold\nNext Stage\nFigure 2: Pipeline of the proposed HCRC.\n…\n…\n…\nHCRC\n…\n…\n…\n…\nFigure 3: The visual representation of proposed problem.\n4.3. Overview\nWhen a message block Mi is received, the pipeline illustrated\nin Fig. 2 is employed. Two different approaches are utilized\nto construct graphs, and node-level and graph-level contrastive\nlearning is conducted to obtain the hybrid social message repre-\nsentation. Then, DRL-SinglePass clustering is used to compute\nthe state based on the clusters of the block, and reinforcement\nlearning is employed to learn an appropriate threshold (refer to\nSection 4.4, 4.5 and 4.6 for details). The learned threshold is\napplied to SinglePass clustering to cluster the current message\nblock Mi and obtain predicted labels, which are compared with\nthe ground-truth labels to derive clustering results.\n4.4. Graph Construction\nAs shown in Fig.\n4, we adopt two methods to construct\ngraphs to simultaneously learn the attribute information of so-\ncial messages and the information interrelated between mes-\nsages. The social message attribute graph emphasizes the spe-\ncific characteristics and information within an individual mes-\nsage, while the social message relation graph focuses on learn-\ning the correlations between multiple messages.\n• Social Message Attribute Graph. We adopt a simple but\neffective graph structure for social messages. Referring\nto the star topology structure, we take a message as the\ncentral node, linked by its neighboring attribute nodes.\nSpecifically, we connect the component words, location,\ntopic, and other attributes to the central node. Then, we\nTable 1: Glossary of Notations.\nNotation\nDescription\nMi\nMessage block i\nmi\nA message\nX∗\nThe feature matrices\nA∗\nThe adjacency matrices\nS\nState space\nA\nAction space\nR\nReward function\nG\nSocial message attribute graph\nM\nSocial message relation graph\nˆG∗\nAugmented social message attribute graph\nˆ\nM∗\nAugmented social message relation graph\nhG\n∗\nGlobal representation of ˆG∗\nzG\n∗\nhG\n∗through projection head\nHM\n∗\nThe node representations of M\nZM\n∗\nHM\n∗through projection head\nf(·)\nGNN-encoder\ng(·)\nProjection head\nϵt\nThe current minimum neighbor distance\ncoht\nThe average cohesion distance\nsept\nThe average separation distance\nDIt\nDunn Index\nS t\nSilhouette coefficient\nVW\nt\nThe overall within-cluster variance\nV B\nt\nThe overall between-cluster variance\ncan obtain an attribute graph G containing all the informa-\ntion for each social message. Furthermore, when dealing\nwith social messages from different sources, we only need\nto connect or remove the attribute node instead of design-\ning a new feature acquisition approach.\n• Social Message Relation Graph. To begin with, we in-\ntegrate various attributes of messages, including words,\nlocation, topic, and other relevant characteristics, as\nwell as users and messages themselves, as nodes in our\nmodel. We then connect messages with their respective\nelements, forming a heterogeneous information network\ngraph. Then, we convert this graph into a homogeneous\nmessage graph M, which includes only message nodes\nand edges connecting messages that have shared features.\nThe transformation aims to prioritize learning correlations\nbetween messages in the homogeneous graph over retain-\ning diverse node types in the model. As shown in Fig. 4,\nfollowing the mapping process in KPGNN [9], we derive\nthe homogeneous message graph M containing the mes-\nsages {m1, m2, m3, m4, . . .} as nodes.\n4\nProjection \nHead \nGNN -\nEncoder\nProjection \nHead \nContrast\nGNN -\nEncoder\nGNN - \nAugmenter\nGNN - \nAugmenter\n@user1 Nov 20th, 2022 \nJunk Kook of BTS \nperforms at the opening \nceremony of the World \nCup!#Qatar 2022 \n#FIFAWorldCup\n@user2 Nov 20th, 2022 \nMorgan Freeman and \nBTS' Jung Kook \nheadline the \n#FIFAWorldCup \nWorld Cup opening \nceremony\n@user3 Nov 21th, 2022 \nSubhan'Allah! During\nthe opening ceremony of \nthe Qatar World Cup,a \nverse of the Quran was \nrecited! #Qatar 2022 \n@user3 Nov 21th, 2022 \nBTS member Jung \nkook's offical \n#FIFAWorldCup\n theme song is gaining\nexplosive popularity by \ntopping music charts\naround the world \n@user2.\nSocial Message Attribute Graphs\nGraph-level Contrastive Learning Module\n+\nGraph-level Embedding\nExponential Moving Average\npooling\npooling\n+\n+\nNode-level Embedding\nHeterogeneous Social Message Relation Graph\nHomogeneous Social Message \nRelation Graph\nuser1\nuser2\nuser3\nMorgan Freeman\nJung Kook\n#Qatar 2022 \n#FIFAWorldCup\n...\n...\n...\nHybird Embedding \nProjection \nHead \nGNN -\nencoder\nProjection \nHead\nGNN -\nencoder\nExponential Moving Average\nContrast\nNode-level Contrastive Learning Module\nGraph Augmentation\n^\n^\nFigure 4: Details of the Graph Construction and Hybrid Graph Contrastive Learning.\n4.5. Hybrid Graph Contrastive Learning\nAs shown in Fig. 4, we adopt two different modules, i.e.,\nthe graph-level and node-level contrastive learning modules to\nlearn different information.\n4.5.1. Graph Augmentation\nThe beneficial augmentation types can be dataset-specific.\nConsidering that the edge perturbation benefits social networks\n[48], we apply such an augmentation on two kinds of graphs.\nSpecifically, during the edge perturbation process, we remove\nsome edges and add more edges in the social message attribute\ngraphs and heterogeneous social message relation graph, while\navoiding the generation of new isolated points. The number of\nedges removed and added is equal to one-tenth of the total num-\nber of edges in the graph, and importantly, the removed edges\nand added edges are non-overlapping.\n4.5.2. Graph-level Contrastive Learning\nThe given social message attribute graph G undergoes graph\ndata augmentations to obtain two correlated views ˆG1, ˆG2, as\na positive pair. Following Equation 1, we use the GNN-based\nencoder [46] and a global attention pooling layer as the encoder\nto extract graph representations vectors hG\n1 , hG\n2 from augmented\ngraphs ˆG1, ˆG2. A two-layer perceptron, as the projection head,\nis applied to map hG\n1 , hG\n2 into zG\n1 , zG\n2 for the further contrast.\nDuring the training process, we consider representations of\nthe augmented social message attribute graphs, zG\n1 and zG\n2 , as\npositive pairs, while negative pairs are generated from the re-\nmaining augmented graphs in the message block. Then, we ap-\nply Equation 3 to enforce maximizing the consistency between\npositive pairs zG\n1 , zG\n2 compared with negative pairs.\n4.5.3. Node-level Contrastive Learning\nAnalogically, we use the graph augmentation to get ˆ\nM1, ˆ\nM2\nfrom the homogeneous social graph M. Then, we follow Equa-\ntion 1 to perform the GNN-based encoder, without the pooling\nlayer, to learn the node representations HM\n1 and HM\n2 , and the\nprojection head is imposed to learn the ultimate representations\nZM\n1 and ZM\n2 .\nAs shown in Fig. 4, each node in M represents a social mes-\nsage m. For any node, its embeddings generated in two views\nform the positive sample, and the other nodes in the two views\nare regarded as negative samples. Finally, the objective in Equa-\ntion 3 is maximized to train this module.\n4.5.4. Hybrid Embedding\nThe social message attribute graph pays attention to the char-\nacteristic information of a single message, while the social het-\nerogeneous graph focuses on learning the correlations between\nmessages. At the testing stage, we concat the graph embed-\nding of G and the node embedding of M to derive the hybrid\nembedding of the social message to represent social messages\ncomprehensively.\n4.6. Deep Reinforcement Learning Guided SinglePass\nIn real-world scenarios, it’s difficult to specify hyperparam-\neters such as k for K-Means in a supervised manner since the\nnumber of topics is not available. Orthogonal to the baselines\nadopting K-Means in each block for clustering, we propose an\nimproved SinglePass-based incremental clustering algorithm.\nCompared with K-Means, the algorithm can be completed in\none pass, which is more suitable for real-world scenarios. But\nSinglePass is highly sensitive to the threshold value, which\ngreatly affects the resulting clustering outcome.\nWe propose Deep Reinforcement Learning Guided Sin-\nglePass, dubbed DRL-SinglePass, to improve the SinglePass\nalgorithm in streaming data. As shown in Fig. 5, our model\ncan get performance boosts after adjusting the threshold from a\nfixed hyperparameter to a well-selected value that is continuous\nand trainable. Such a reinforcement learning approach enables\n5\nTable 2: The statistics of each message block in the Twitter dataset.\nBlocks\nM0\nM1\nM2\nM3\nM4\nM5\nM6\nM7\nM8\nM9\nM10\n#\n20,254\n8,722\n1,491\n1,835\n2,010\n1,834\n1,276\n5,278\n1,560\n1,363\n1,096\nBlocks\nM11\nM12\nM13\nM14\nM15\nM16\nM17\nM18\nM19\nM20\nM21\n#\n1,232\n3,237\n1,972\n2,956\n2,549\n910\n2,676\n1,887\n1,399\n893\n2,410\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10 11 12 13 14 15 16 17 18 19 20 21\nMessage Block Number \n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nNMI\nHCRCNMI\nHCRCunified\nFigure 5: Compare HCRCNMI and HCRCunified wth the metric of NMI. In\nHCRCunified, we set the threshold of all message blocks to 0.6. The threshold\nof HCRCNMI is selected from 0.6, 0.65, 0.7, 0.75, and 0.8 based on NMI.\nthe model to adjust the threshold adaptively instead of randomly\nselecting from the preset thresholds.\nDRL-SinglePass regards the social data as the environment,\nthe hyperparameter adjustment approach as an agent, and for-\nmally expresses the process as a Markov Decision Process\n(MDP)\n\u0010\nSclu, Aclu, Rclu\u0011\n, where Sclu is state space, Aclu is ac-\ntion space, Rclu is reward function. Specifically, at time step\nt, when receiving the new message block Mt, we define three\nelements as follows:\nState. The clustering result observed by the agent after each\nparameter adjustment episode is represented by the state, which\nis described as follows:\nsclu\nt\n= \bϵt, coht, sept, DIt, S t\n\t ,\n(4)\nwhich consists of the current minimum neighbor distance ϵt, the\naverage cohesion distance coht, the average separation distance\nsept, the Dunn Index DIt [50], and the Silhouette coefficient S t\n[51]. All of these serve to assess the quality of clustering and\ndo not depend on any prior knowledge of ground-truth labels.\nAction. We define the action aclu\nt\nat time step t as the change\nof the threshold parameter of SinglePass that should be selected\nfor the current state sclu\nt . According to practical prior knowl-\nedge, the action space is a continuous value that is restricted in\nthe range of [0.6, 0.8].\nReward. We apply the Calinski-Harabasz evaluation index\n[52] as the reward function at time step t, of which essence is\nthe ratio of the inter-cluster distance to the intra-cluster distance\nas follows:\nrclu\nt\n=\nVB\nt\n|E|t−1/\nVW\nt\nN−|E|t ,\n(5)\nwhere VW\nt\nis the overall within-cluster variance and VB\nt is the\noverall between-cluster variance. N is the total number of mes-\nsages. |E| is the number of clusters.\nOptimization. We apply the classic policy-gradient method\nPPO [42] as the updating method for the policy network.\n5. Experiment\n5.1. Experimental Settings\n5.1.1. Datasets\nTo evaluate HCRC, we conduct experiments on two large,\npublicly available social media datasets, i.e., the Twitter dataset\n[53] and the MAVEN dataset [54], following KPGNN [9].\nAfter data cleaning, Twitter contains 68,841 tweets, covering\n503 event classes and spreading over a period of four weeks.\nMAVEN is a general-domain event detection dataset, used for\ntraining and evaluating event detection systems, containing\n10,242 messages and covering 164 event types. In incremen-\ntal evaluation, we split Twitter into several message blocks by\ndate. We use the first week’s messages as an initial message\nblock M0 and the remaining messages in Twitter to form sev-\neral message blocks M1, M2, ..., and M21 by date. Table 2\nshows the statistics of each message block.\n5.1.2. Baselines\nWe compare the proposed HCRC with eleven baselines, in-\ncluding Word2vec [4], LDA [5], WMD [6], BERT [7], BiL-\nSTM [8], PP-GCN [10], EventX [11], KPGNN [9], KPGNNt\n[9], QSGNN [12] and QSGNN∗. Word2vec converts all words\nin a message to vectors, calculates their average as the repre-\nsentation of the message. LDA is a generative model that uti-\nlizes latent topics and word distributions to obtain representa-\ntions of messages. WMD measures the similarity between two\nmessages by calculating the minimum distance between word\nembeddings in one message and the word embeddings in an-\nother message. BERT utilizes large-scale unlabeled corpora for\ntraining to obtain word embeddings of the words in a message,\nand takes the average of these word embeddings as the repre-\nsentation of the message. BiLSTM learns the bidirectional de-\npendency between a word and other words, capturing the con-\ntextual information in a message to obtain the representation\nof the message. PP-GCN is a fine-grained social event detec-\ntion method based on GCN. EventX is a model that performs\n6\nTable 3: Traditional incremental evaluation NMIs. The best results are in bold, and the second-best are underlined.\nBlocks\nM1\nM2\nM3\nM4\nM5\nM6\nM7\nWord2vec\n.19±.00\n.50±.00\n.39±.00\n.34±.00\n.41±00\n.53±.00\n.25±.00\nLDA\n.11±.00\n.27±.01\n.28±.00\n.25±.00\n.26±.00\n.32±.00\n.18±.01\nWMD\n.32±.00\n.71±.00\n.67±.00\n.50±.00\n.61±.00\n.61±.00\n.46±.00\nBERT\n.36±.00\n.78±.00\n.75±.00\n.60±.00\n.72±.00\n.78±.00\n.54±.00\nBiLSTM\n.24±.00\n.50±.00\n.39±.00\n.40±.00\n.41±.00\n.50±.00\n.33±.00\nPP-GCN\n.23±.00\n.57±.02\n.55±.01\n.46±.01\n.48±.01\n.57±.01\n.37±.00\nEventX\n.36±.00\n.68±.00\n.63±.00\n.63±.00\n.59±.00\n.70±.00\n.51±.00\nKPGNNt\n.38±.01\n.78±.01\n.77±.00\n.68±.01\n.73±.01\n.81±.00\n.54±.01\nKPGNN\n.39±.00\n.79±.01\n.76±.00\n.67±.00\n.73±.01\n.82±.01\n.55±.01\nQSGNN\n.43±.01\n.81±.02\n.78±.01\n.71±.01\n.75±.00\n.83±.01\n.57±.01\nQSGNN∗\n.34±.02\n.73±.01\n.56±.01\n.58±.00\n.58±.02\n71±.01\n35±.01\nHCRCNMI\n.30±.01\n.85±.00\n.83±.00\n.71±.01\n.77±.00\n.85±.00\n.52±.00\n∆\n↓13%\n↑4%\n↑5%\n↑0%\n↑2%\n↑2%\n↓5%\nBlocks\nM8\nM9\nM10\nM11\nM12\nM13\nM14\nWord2vec\n.46±.00\n.35±.00\n.51±.00\n.37±.00\n.30±00\n.37±.00\n.36±.00\nLDA\n.37±.01\n.34±.00\n.44±.01\n.33±.01\n.22±.01\n.27±.00\n.21±.00\nWMD\n.67±.00\n.55±.00\n.61±.00\n.50±.00\n.60±.00\n.54±.00\n.66±.00\nBERT\n.79±.00\n.70±.00\n.74±.00\n.68±.00\n.59±.00\n.63±.00\n.64±.00\nBiLSTM\n.49±.00\n.43±.00\n.50±.00\n.49±.00\n.39±.00\n.46±.00\n.44±.00\nPP-GCN\n.55±.02\n.51±.02\n.55±.02\n.50±.01\n.45±.01\n.47±.01\n.44±.01\nEventX\n.71±.00\n.67±.00\n.68±.00\n.65±.00\n.61±.00\n.58±.00\n.57±.00\nKPGNNt\n.79±.01\n.74±.01\n.79±.00\n.73±.00\n.69±.01\n.68±.01\n.68±.01\nKPGNN\n.80±.00\n.74±.02\n.80±.01\n.74±.01\n.68±.01\n.69±.01\n.69±.00\nQSGNN\n.79±.01\n.77±.02\n.82±.02\n.75±.01\n.70±.00\n.68±.02\n.68±.01\nQSGNN∗\n.61±.02\n.55±.00\n.62±.01\n.57±.01\n.45±.01\n.54±.00\n.45±.01\nHCRCNMI\n.81±.00\n.79±.00\n.84±.00\n.80±.00\n.70±.01\n.79±.00\n.71±.01\n∆\n↑2%\n↑2%\n↑2%\n↑5%\n↑0%\n↑10%\n↑3%\nBlocks\nM15\nM16\nM17\nM18\nM19\nM20\nM21\nWord2vec\n.27±.00\n.49±.00\n.33±.00\n.29±.00\n.37±00\n.38±.00\n.31±.00\nLDA\n.21±.00\n.35±.01\n.19±.00\n.18±.00\n.29±.01\n.35±.00\n.19±.00\nWMD\n.51±.00\n.60±.00\n.55±.00\n.63±.00\n.54±.00\n.58±.00\n.58±.00\nBERT\n.54±.00\n.75±.00\n.63±.00\n.57±.00\n.66±.00\n.68±.00\n.59±.00\nBiLSTM\n.40±.00\n.53±.00\n.45±.00\n.44±.00\n.44±.00\n.48±.00\n.41±.00\nPP-GCN\n.39±.01\n.55±.01\n.48±.00\n.47±.01\n.51±.02\n.51±.01\n.41±.02\nEventX\n.49±.00\n.62±.00\n.58±.00\n.59±.00\n.60±.00\n.67±.00\n.53±.00\nKPGNNt\n.57±.01\n.78±.01\n.69±.01\n.68±.01\n.73±.00\n.73±.00\n.59±.01\nKPGNN\n.58±.00\n.79±.01\n.70±.01\n.68±.02\n.73±.01\n.72±.02\n.60±.00\nQSGNN\n.59±.01\n.78±.01\n.71±.01\n.70±.01\n.73±.00\n.73±.02\n.61±.01\nQSGNN∗\n.39±.01\n.55±.02\n.43±.01\n.42±.01\n.50±.00\n.52±.01\n.35±.00\nHCRCNMI\n.70±.01\n.87±.00\n.75±.00\n.63±.01\n.76±.01\n.72±.00\n.62±.00\n∆\n↑11%\n↑8%\n↑4%\n↓7%\n↑3%\n↓1%\n↑1%\nonline event detection on streaming text data. KPGNN is an\nincremental social event detection method via heterogeneous\ngraph neural network. KPGNNt removes the global-local pair\nloss term from the loss function of KPGNN and only utilizes\nthe triplet loss term. QSGNN is a social event detection method\nbased on quality-aware self-improving graph neural network.\nQSGNN∗are implemented based on the official code provided\nby QSGNN [12].\nWe further compare KPGNN, QSGNN and HCRC in dif-\nferent settings.\nSpecifically, HCRCNMI’s threshold for ev-\nery message block is determined by NMI [55]; HCRC10%’s\nthreshold, KPGNN10%’s k and QSGNN10%’s k for every mes-\nsage block are determined by using 10% of the ground-\ntruth label; HCRCrandom’s threshold, KPGNNrandom’s k and\nQSGNNrandom’s k for every message block are randomly de-\ntermined. HCRC’s threshold is learned by the proposed DRL-\nSinglePass. k and the threshold are the hyperparameters for\nK-Means in KPGNN and QSGNN and SinglePass in HCRC,\nrespectively.\n5.1.3. Implementation Details\nThe number of units in each layer of the GCN is set to 256.\nMoreover, the learning rates for graph-level and node-level con-\ntrastive learning are set to 6e-7 and 1e-5, respectively. Addi-\ntionally, we set the moving average decay for the teacher net-\nwork to 0.9. In HCRCNMI, we employ the SinglePass cluster-\ning method with varying thresholds to obtain multiple cluster-\ning results from the pre-trained message representations. The\nfinal clustering result is obtained by selecting the one with the\nhighest NMI score. For HCRC10%, we use SinglePass clus-\n7\nTable 4: Traditional incremental evaluation AMIs. The best results are marked in bold, and the second-best are underlined.\nBlocks\nM1\nM2\nM3\nM4\nM5\nM6\nM7\nWord2vec\n.08±.00\n.41±.00\n.31±.00\n.24±.00\n.33±00\n.40±.00\n.13±.00\nLDA\n.08±.00\n.20±.01\n.22±.01\n.17±.00\n.21±.00\n.20±.00\n.12±.01\nWMD\n.30±.00\n.69±.00\n.63±.00\n.45±.00\n.57±.00\n.57±.00\n.46±.00\nBERT\n.34±.00\n.76±.00\n.73±.00\n.55±.00\n.71±.00\n.74±.00\n.50±.00\nBiLSTM\n.12±.00\n.41±.00\n.31±.00\n.30±.00\n.33±.00\n.36±.00\n.20±.00\nPP-GCN\n.21±.00\n.55±.02\n.52±.01\n.42±.01\n.46±.01\n.52±.02\n.34±.00\nEventX\n.06±.00\n.29±.00\n.18±.00\n.19±.00\n.14±.00\n.27±.00\n.13±.00\nKPGNNt\n.36±.01\n.77±.01\n.75±.00\n.65±.01\n.71±.01\n.78±.00\n.50±.01\nKPGNN\n.37±.00\n.78±.01\n.74±.00\n.64±.01\n.71±.01\n.79±.01\n.51±.01\nQSGNN\n.41±.02\n.80±.01\n.76±.01\n.68±.01\n.73±.00\n.80±.01\n.54±.00\nQSGNN∗\n.32±.01\n.70±.02\n.53±.01\n.54±.00\n.55±.01\n.65±.02\n.29±.00\nHCRCNMI\n.29±.01\n.83±.00\n.81±.01\n.64±.01\n.73±.00\n.81±.00\n.44±.02\n∆\n↓12%\n↑3%\n↑5%\n↓4%\n↑0%\n↑1%\n↓10%\nBlocks\nM8\nM9\nM10\nM11\nM12\nM13\nM14\nWord2vec\n.33±.00\n.24±.00\n.39±.00\n.26±.00\n.23±00\n.23±.00\n.26±.00\nLDA\n.24±.01\n.24±.00\n.36±.01\n.25±.01\n.16±.01\n.19±.00\n.15±.00\nWMD\n.63±.00\n.46±.00\n.57±.00\n.42±.00\n.58±.00\n.50±.00\n.64±.00\nBERT\n.75±.00\n.66±.00\n.70±.00\n.65±.00\n.56±.00\n.59±.00\n.61±.00\nBiLSTM\n.35±.00\n.32±.00\n.39±.00\n.37±.00\n.32±.00\n.31±.00\n.34±.00\nPP-GCN\n.49±.02\n.46±.02\n.51±.02\n.46±.01\n.42±.01\n.43±.01\n.41±.01\nEventX\n.21±.00\n.19±.00\n.24±.00\n.24±.00\n.16±.00\n.16±.00\n.14±.00\nKPGNNt\n.75±.01\n.70±.01\n.76±.01\n.70±.00\n.66±.01\n.65±.01\n.65±.01\nKPGNN\n.76±.01\n.71±.02\n.78±.01\n.71±.01\n.66±.01\n.67±.01\n.65±.00\nQSGNN\n.75±.01\n.75±.02\n.80±.03\n.72±.01\n.68±.00\n.66±.01\n.66±.01\nQSGNN∗\n.53±.00\n.48±.01\n.56±.01\n.51±.02\n.40±.00\n.48±.02\n.40±.01\nHCRCNMI\n.75±.01\n.72±.01\n.82±.00\n.76±.00\n.62±.02\n.76±.00\n.67±.01\n∆\n↓1%\n↓3%\n↑2%\n↑4%\n↓6%\n↑9%\n↑1%\nBlocks\nM15\nM16\nM17\nM18\nM19\nM20\nM21\nWord2vec\n.15±.00\n.36±.00\n.24±.00\n.21±.00\n.28±00\n.24±.00\n.21±.00\nLDA\n.13±.00\n.27±.01\n.13±.00\n.12±.00\n.22±.01\n.23±.00\n.13±.00\nWMD\n.47±.00\n.59±.00\n.57±.00\n.60±.00\n.49±.00\n.55±.00\n.52±.00\nBERT\n.50±.00\n.72±.00\n.60±.00\n.53±.00\n.63±.00\n.62±.00\n.57±.00\nBiLSTM\n.26±.00\n.41±.00\n.35±.00\n.35±.00\n.35±.00\n.34±.00\n.31±.00\nPP-GCN\n.35±.01\n.52±.01\n.45±.00\n.45±.01\n.48±.02\n.45±.02\n.38±.02\nEventX\n.07±.00\n.19±.00\n.18±.00\n.16±.00\n.16±.00\n.18±.00\n.10±.00\nKPGNNt\n.53±.01\n.75±.01\n.67±.01\n.66±.01\n.70±.00\n.68±.00\n.57±.01\nKPGNN\n.54±.00\n.77±.01\n.68±.01\n.66±.02\n.71±.01\n.68±.02\n.57±.00\nQSGNN\n.55±.01\n.76±.02\n.69±.01\n.68±.01\n.70±.01\n.69±.02\n.58±.00\nQSGNN∗\n.33±.01\n49±.00\n.40±.00\n.36±.01\n.45±.02\n.42±.02\n.31±.01\nHCRCNMI\n.66±.01\n.86±.00\n.72±.00\n.50±.03\n.72±.01\n.61±.00\n.55±.01\n∆\n↑11%\n↑9%\n↑3%\n↓18%\n↑1%\n↓8%\n↓3%\ntering with varying thresholds on 10% labeled data to deter-\nmine the optimal threshold for the entire message block, and\nthen perform clustering once again to generate the final result.\nIn DRL-SinglePass, we set the learning range to be between\n0.6 and 0.8 based on the experimental results obtained from\nHCRCNMI, where the majority of message blocks achieved op-\ntimal results. In DRL-SinglePass, we adopt a pre-clustering\napproach wherein one-tenth of the tweets within each mes-\nsage block are initially grouped. Subsequently, we leverage\nreinforcement learning to determine an optimal threshold for\nthe entire message block, utilizing the unsupervised clustering\nevaluation results as a basis for learning. To ensure a fair com-\nparison, we randomly run the experiments ten times and report\nthe average results with standard deviations. Our implementa-\ntion is available at https://github.com/guoyy49/HCRC.\n5.1.4. Evaluation Metrics\nTo evaluate HCRC and baselines, we use normalized mutual\ninformation (NMI) [55], adjusted mutual information(AMI)\n[56] and adjusted rand index (ARI) [56] to measure the similari-\nties between the detected message clusters and the ground-truth\nclusters. NMI is one of the vital metrics for social event detec-\ntion, which measures the similarity of clustering results ranging\nfrom 0 to 1. A higher NMI value signifies a stronger align-\nment between the detected message clusters and the ground-\ntruth clusters, indicating a more successful clustering process.\nConversely, a lower NMI value suggests a greater divergence\nbetween the clustering results and the true cluster assignments,\nindicating a potential mismatch or inconsistency in the cluster-\ning outcomes [11, 10]. AMI penalizes random assignment of\ncluster labels to ensure that the quality of clustering is not over-\n8\nTable 5: Traditional incremental evaluation ARIs. The best results are marked in bold, and the second-best are underlined.\nBlocks\nM1\nM2\nM3\nM4\nM5\nM6\nM7\nWord2vec\n.01±.00\n.49±.00\n.16±.00\n.07±.00\n.17±00\n.25±.00\n.02±.00\nLDA\n.00±.00\n.08±.00\n.02±.01\n.07±.00\n.06±.00\n.07±.01\n.00±.00\nWMD\n.04±.00\n.48±.00\n.28±.00\n.11±.00\n.26±.00\n.16±.00\n.08±.00\nBERT\n.03±.00\n.64±.00\n.43±.00\n.19±.00\n.44±.00\n.44±.00\n.07±.00\nBiLSTM\n.03±.00\n.49±.00\n.17±.00\n.11±.00\n.19±.00\n.18±.00\n.12±.00\nPP-GCN\n.05±.00\n.67±.03\n.47±.01\n.24±.01\n.34±.00\n.55±.03\n.11±.02\nEventX\n.01±.00\n.45±.00\n.09±.00\n.07±.00\n.04±.00\n.14±.00\n.02±.00\nKPGNNt\n.06±.01\n.76±.01\n.60±.02\n.30±.01\n.48±.01\n.67±.05\n.11±.01\nKPGNN\n.07±.01\n.76±.02\n.58±.01\n.29±.01\n.47±.03\n.72±.03\n.12±.00\nQSGNN\n-\n-\n-\n-\n-\n-\n-\nQSGNN∗\n.13±.02\n.74±.00\n.36±.01\n.28±.01\n.32±.00\n.45±.01\n.13±.01\nHCRCNMI\n.18±.05\n.82±.00\n.70±.01\n.41±.01\n.60±.01\n.81±.01\n.19±.02\n∆\n↑5%\n↑6%\n↑10%\n↑11%\n↑12%\n↑9%\n↑6%\nBlocks\nM8\nM9\nM10\nM11\nM12\nM13\nM14\nWord2vec\n.17±.00\n.08±.00\n.23±.00\n.09±.00\n.09±00\n.06±.00\n.10±.00\nLDA\n.03±.01\n.03±.01\n.09±.02\n.03±.01\n.02±.00\n.00±.00\n.02±.00\nWMD\n.22±.00\n.12±.00\n.20±.00\n.12±.00\n.27±.00\n.13±.00\n.33±.00\nBERT\n.50±.00\n.33±.00\n.44±.00\n.27±.00\n.31±.00\n.14±.00\n.30±.00\nBiLSTM\n.17±.00\n.13±.00\n.30±.00\n.16±.00\n.14±.00\n.10±.00\n.17±.00\nPP-GCN\n.43±.04\n.31±.02\n.50±.07\n.38±.02\n.34±.03\n.19±.01\n.29±.01\nEventX\n.09±.00\n.07±.00\n.13±.00\n.16±.00\n.07±.00\n.04±.00\n.10±.00\nKPGNNt\n.59±.02\n.45±.02\n.64±.01\n.48±.01\n.50±.03\n.28±.01\n.43±.02\nKPGNN\n.60±.01\n.46±.02\n.70±.06\n.49±.03\n.48±.01\n.29±.03\n.42±.02\nQSGNN\n-\n-\n-\n-\n-\n-\n-\nQSGNN∗\n.31±.00\n.26±.02\n.40±.01\n.26±.00\n.22±.00\n.25±.01\n.22±.01\nHCRCNMI\n.58±.01\n.55±.02\n.81±.00\n.78±.00\n.44±.07\n.72±.00\n.54±.03\n∆\n↓2%\n↑9%\n↑11%\n↑29%\n↓6%\n↑43%\n↑11%\nBlocks\nM15\nM16\nM17\nM18\nM19\nM20\nM21\nWord2vec\n.03±.00\n.19±.00\n.10±.00\n.07±.00\n.14±00\n.10±.00\n.06±.00\nLDA\n.00±.00\n.11±.01\n.02±.00\n.02±.00\n.03±.00\n.02±.01\n.00±.01\nWMD\n.16±.00\n.32±.00\n.26±.00\n.35±.00\n.12±.00\n.19±.00\n.19±.00\nBERT\n.10±.00\n.41±.00\n.24±.00\n.24±.00\n.32±.00\n.33±.00\n.18±.00\nBiLSTM\n.08±.00\n.27±.00\n.22±.00\n.19±.00\n.16±.00\n.20±.00\n.16±.00\nPP-GCN\n.15±.00\n.51±.03\n.35±.03\n.39±.03\n.41±.02\n.41±.01\n.20±.03\nEventX\n.01±.00\n.08±.00\n.12±.00\n.08±.00\n.07±.00\n.11±.00\n.01±.00\nKPGNNt\n.16±.02\n.62±.03\n.41±.03\n.46±.02\n.50±.01\n.51±.01\n.01±.00\nKPGNN\n.17±.00\n.66±.05\n.43±.05\n.47±.04\n.51±.03\n.51±.04\n.20±.01\nQSGNN\n-\n-\n-\n-\n-\n-\n-\nQSGNN∗\n.13±.01\n.34±.01\n.22±.02\n.22±.00\n.28±.01\n.24±.00\n.13±.01\nHCRCNMI\n.70±.05\n.87±.00\n.70±.04\n.32±.06\n.58±.03\n.40±.00\n.36±.00\n∆\n↑53%\n↑21%\n↑27%\n↓15%\n↑7%\n↓11%\n↑16%\nestimated due to randomness. The typical range of AMI values\nis between -1 and 1, where 0 signifies similarity between the\nclustering results and true labels that is equivalent to random\nassignment. A score of 1 indicates a perfect match, meaning\nthat the clustering results are identical to the true labels. Val-\nues below 0 imply that the similarity between the clustering\nresults and true labels is worse than random assignment, pos-\nsibly suggesting the negative correlation [56]. The Rand Index\n(RI) measures the proportion of “correct decision-making” in\nclustering analysis. It compares the similarity between pairs\nof samples in their true labels and the clustering results. On\nthe other hand, ARI is a normalized version of the Rand Index\nthat ranges from -1 to 1. A higher ARI value indicates a better\nclustering effect, where 1 represents a perfect clustering result\nand 0 indicates a random distribution. Conversely, a negative\nARI value suggests that the clustering result is worse than ran-\ndom chance [56]. Note that the results of all baselines in the\noffline evaluation and traditional incremental clustering refer to\nKPGNN [9] and QSGNN[12].\n5.2. Incremental Evaluation\n5.2.1. Traditional Incremental Clustering\nSince the clustering method in KPGNN and QSGNN is K-\nMeans, the ground-truth label must be required to determine\nthe hyperparameter k. For fairness, we compare KPGNN and\nQSGNN with HCRCNMI, and such setting is called traditional\nincremental clustering.\nTable 3, 4 and 5 summarize the results. We observe that\nHCRCNMI achieves the best or second-best performance across\nmost message blocks. HCRCNMI outperforms EventX by 13%\n9\nTable 6: Semi-supervised and solid incremental evaluation NMIs. The best results are marked in bold.\nBlocks\nM1\nM2\nM3\nM4\nM5\nM6\nM7\nKPGNN10%\n.27±.01\n.68±.01\n.60±.01\n.57±.01\n.54±.02\n.70±.02\n.37±.01\nQSGNN10%\n.25±.01\n.75±.00\n.65±.01\n.59±.02\n.60±.01\n.65±.01\n.34±.02\nHCRC10%\n.24±.03\n.82±.02\n.79±.01\n.70±.01\n.76±.00\n.81±.02\n.34±.01\n∆\n↓2%\n↑7%\n↑14%\n↑11%\n↑16%\n↑11%\n↓3%\nKPGNNrandom\n.27±.01\n.71±.01\n.64±.02\n.59±.01\n.61±.03\n.71±.03\n.42±.02\nQSGNNrandom\n.31±.02\n.77±.01\n.65±.01\n.50±.00\n.60±.02\n.75±.00\n.39±.01\nHCRCrandom\n.27±.01\n.82±.00\n.79±.03\n.63±.05\n.70±.01\n.80±.00\n.46±.05\nHCRC\n.27±.00\n.83±.00\n.81±.01\n.67±.03\n.74±.01\n.83±.01\n.50±.01\n∆\n↓4%\n↑6%\n↑16%\n↑8%\n↑13%\n↑8%\n↑8%\nBlocks\nM8\nM9\nM10\nM11\nM12\nM13\nM14\nKPGNN10%\n.69±.02\n.55±.03\n.68±.03\n.61±.02\n.47±.02\n.56±.05\n.41±.03\nQSGNN10%\n.60±.03\n.52±.01\n.63±.00\n.57±.00\n.46±.02\n.56±.01\n.44±.02\nHCRC10%\n.79±.00\n.75±.02\n.74±.03\n.78±.02\n.68±.02\n.76±.04\n.65±.02\n∆\n↑10%\n↑20%\n↑6%\n↑17%\n↑21%\n↑20%\n↑21%\nKPGNNrandom\n.72±.02\n.62±.03\n.69±.02\n.64±.01\n.51±.03\n.58±.01\n.50±.04\nQSGNNrandom\n.71±.00\n.68±.02\n.65±.01\n.52±.01\n.63±.00\n.49±.01\n.50±.00\nHCRCrandom\n.75±.01\n.70±.01\n.78±.01\n.67±.01\n.65±.01\n.67±.01\n.68±.01\nHCRC\n.78±.01\n.76±.00\n.80±.01\n.70±.02\n.69±.00\n.69±.04\n.68±.01\n∆\n↑6%\n↑8%\n↑11%\n↑6%\n↑6%\n↑11%\n↑18%\nBlocks\nM15\nM16\nM17\nM18\nM19\nM20\nM21\nKPGNN10%\n.27±.04\n.71±.02\n.48±.03\n.36±.03\n.49±.02\n.53±.04\n.37±.01\nQSGNN10%\n.42±.00\n.63±.01\n.43±.00\n.51±.03\n.52±.00\n.53±.01\n.37±.01\nHCRC10%\n.59±.00\n.85±.01\n.65±.07\n.62±.01\n.75±.03\n.64±.05\n.57±.04\n∆\n↑17%\n↑14%\n↑17%\n↑11%\n↑23%\n↑11%\n↑20%\nKPGNNrandom\n.41±.03\n.67±.03\n.58±.03\n.48±.06\n.57±.02\n.63±.02\n.45±.04\nQSGNNrandom\n.51±.01\n.58±.00\n.56±.01\n.45±.01\n.58±.00\n.64±.02\n.42±.00\nHCRCrandom\n.58±.01\n.80±.02\n.67±.01\n.60±.01\n.69±.02\n.69±.01\n.55±.03\nHCRC\n.68±.02\n.86±.02\n.71±.02\n.61±.01\n.74±.01\n.69±.02\n.56±.01\n∆\n↑17%\n↑19%\n↑13%\n↑13%\n↑16%\n↑5%\n↑11%\nin NMI, 49% in AMI, and 48% in ARI, and BERT by 8% in\nNMI, 6% in AMI, and 27% in ARI on average. This is because\nEventX only considers community detection and BERT ignores\nthe structural information of social networks.\nFurthermore,\nHCRCNMI outperforms KPGNN for most message blocks, be-\ncause HCRCNMI not only learns the structural information be-\ntween messages but also effectively learns the semantic infor-\nmation of a single message, but KPGNN only establishes the\nstructural relationship between messages. And, as shown in Ta-\nble 3, 4 and 5, it is observed that HCRCNMI demonstrates im-\nprovements of 2%, 2%, and 29% over QSGNN in NMI, AMI\nand ARI. However, limited by the experimental environment,\nwe construct multiple social message relation graphs on larger\nmessage blocks, such as M1 and M7, rather than a single graph\nlike on other message blocks, leading to lower NMI, but higher\nARI. Some message blocks have most messages with similar\nattributes, resulting in the social message relation graph that is\nclose to a complete graph and causes mediocre clustering per-\nformance with HCRCNMI, such as with M18 and M20.\n5.2.2. Semi-Supervised Incremental Clustering\nIn practical scenarios, ground-truth labels are difficult to\nobtain, so the traditional incremental clustering cannot suf-\nficiently fit the real-world setting.\nTo this end, we con-\nduct the semi-supervised incremental clustering, which only\nprovides 10% available ground-truth labels.\nThe first two\nrows of Table 6, 7 and 8 show the experimental results for\nKPGNN10%, QSGNN10% and HCRC10%.\nHCRC10% outper-\nforms KPGNN10% by 16% in NMI, 13% in AMI and 18% in\nARI on average. HCRC10% outperforms QSGNN10% by 16%\nin NMI, 15% in AMI and 17% in ARI on average.\nThis\nis because there are only 10% available ground-truth labels,\nand KPGNN10% and QSGNN10% cannot get the real number\nof clusters, which causes K-Means to fail to cluster normally.\nHCRC10% can better explore the discriminative information\nfrom the social data thereby determining the appropriate thresh-\nolds within limited ground-truth labels. However, as demon-\nstrated in traditional incremental clustering shown in Section\n5.2.1, the performance of HCRC in dealing with large message\nblocks is not satisfactory. This issue is further exacerbated as\nthe amount of available label information decreases, leading to\nreduced NMI and ARI values for both M1 and M7.\n5.2.3. Solid Incremental Clustering\nTo comprehensively evaluate the performance of the pro-\nposed HCRC, we further perform solid incremental clustering\ncomparisons, which require that no label information is avail-\nable. In the last three rows of Table 6, 7 and 8, the results\nof HCRCrandom, KPGNNrandom, QSGNNrandom, and HCRC are\nprovided. The empirical results indicate that on average, HCRC\n10\nTable 7: Semi-supervised and solid incremental evaluation AMIs. The best results are marked in bold.\nBlocks\nM1\nM2\nM3\nM4\nM5\nM6\nM7\nKPGNN10%\n.27±.01\n.66±.00\n.57±.02\n.54±.01\n.53±.01\n.68±.02\n.36±.01\nQSGNN10%\n.24±.00\n.73±.00\n.64±.01\n.56±.02\n.58±.01\n.61±.00\n.31±.01\nHCRC10%\n.22±.02\n.77±.03\n.77±.02\n.64±.01\n.73±.01\n.73±.03\n.31±.02\n∆\n↓5%\n↑4%\n↑13%\n↑8%\n↑15%\n↑5%\n↓5%\nKPGNNrandom\n.24±.01\n.67±.02\n.58±.03\n.54±.03\n.57±.04\n.64±.03\n.36±.02\nQSGNNrandom\n.28±.02\n.75±.00\n.50±.01\n.57±.00\n.64±.00\n.70±.02\n.28±.01\nHCRCrandom\n.14±.01\n.77±.00\n.73±.03\n.56±.01\n.60±.02\n.73±.01\n.37±.03\nHCRC\n.19±.00\n.81±.01\n.77±.01\n.61±.02\n.67±.01\n.77±.04\n.43±.01\n∆\n↓9%\n↑6%\n↑19%\n↑4%\n↑3%\n↑7%\n↑7%\nBlocks\nM8\nM9\nM10\nM11\nM12\nM13\nM14\nKPGNN10%\n.66±.03\n.49±.02\n.65±.03\n.59±.02\n.46±.02\n.57±.02\n.40±.01\nQSGNN10%\n.54±.02\n.47±.00\n.58±.00\n.54±.01\n.44±.01\n.53±.00\n.42±.01\nHCRC10%\n.73±.02\n.71±.02\n.71±.03\n.73±.03\n.62±.01\n.74±.04\n.62±.02\n∆\n↑7%\n↑22%\n↑6%\n↑14%\n↑16%\n↑17%\n↑20%\nKPGNNrandom\n.65±.02\n.56±.03\n.60±.03\n.56±.04\n.45±.03\n.51±.02\n.44±.04\nQSGNNrandom\n.63±.00\n.55±.00\n.64±.01\n.59±.01\n.44±.02\n.58±.02\n.42±.00\nHCRCrandom\n.60±.00\n.63±.00\n.71±.03\n.54±.01\n.45±.01\n.57±.01\n.60±.03\nHCRC\n.71±.01\n.69±.00\n.75±.01\n.60±.03\n.59±.01\n.66±.04\n.61±.01\n∆\n↑6%\n↑13%\n↑11%\n↑1%\n↑14%\n↑8%\n↑17%\nBlocks\nM15\nM16\nM17\nM18\nM19\nM20\nM21\nKPGNN10%\n.28±.02\n.67±.01\n.48±.03\n.35±.03\n.46±.02\n.51±.02\n.35±.01\nQSGNN10%\n.40±.01\n.58±.01\n.42±.00\n.38±.02\n.49±.01\n.47±.01\n.33±.00\nHCRC10%\n.56±.00\n.82±.01\n.64±.07\n.50±.03\n.71±.02\n.56±.04\n.52±.04\n∆\n↑16%\n↑15%\n↑16%\n↑12%\n↑22%\n↑5%\n↑17%\nKPGNNrandom\n.34±.02\n.60±.05\n.54±.04\n.42±.05\n.51±.03\n.51±.04\n.39±.04\nQSGNNrandom\n.43±.01\n.56±.00\n.51±.00\n.39±.02\n.53±.01\n.54±.00\n.37±.01\nHCRCrandom\n.43±.01\n.75±.03\n.66±.01\n.52±.00\n.59±.01\n.58±.01\n.51±.02\nHCRC\n.61±.03\n.83±.02\n.66±.05\n.43±.06\n.70±.03\n.47±.08\n.36±.04\n∆\n↑18%\n↑23%\n↑12%\n↑10%\n↑17%\n↑4%\n↑12%\nperforms better than HCRCrandom 3% in NMI, 4% in AMI and\n7% in ARI, outperforms KPGNNrandom by 11% in NMI, 11%\nin AMI and 19% in ARI, and outperforms QSGNNrandom by\n13% in NMI, 10% in AMI and 11% in ARI. The improvements\ncan be attributed to the well-designed DRL-SinglePass, which\ndemonstrates that DRL-SinglePass cannot only break through\nthe limitation of K-Means in the incremental clustering, i.e., re-\nquiring the ground-truth label information, but also boost the\nperformance of the clustering model by leveraging the deep re-\ninforcement learning to derive appropriate thresholds. This ob-\nservation also shows that the proposed HCRC is robust against\nthe negative impact brought by the partial availability of the\nground-truth label information. On the contrary, the blocked\naccessibility of label information excessively degenerates the\nperformance of benchmark methods. To better understand the\neffectiveness of the proposed DRL-SinglePass, we summarize\nthe derived thresholds for message blocks in Table 9, and the re-\nsults support that DRL-SinglePass can indeed learn appropriate\nthresholds for HCRC.\n5.3. Extended Evaluation\nIn this subsection, we compare HCRCNMI to other baselines\nin an offline traditional setting, whereby all datasets are par-\ntitioned into training, testing, and validation sets at a ratio of\n70%, 20%, and 10%, respectively. The experimental results, as\ndemonstrated in Table 10 and 11, reveal that HCRCNMI outper-\nforms other baselines across all metrics. This is attributed to\nthe fact that baselines such as Word2vec, LDA, WMD, BERT,\nand BiLSTM disregard the latent structural information in so-\ncial networks. Furthermore, PP-GCN presumes a stationary\ngraph structure, which is inadequate in capturing dynamic so-\ncial streams [9]. EventX tends to generate more clusters, re-\ngardless of whether it captures any additional information or\nnot [9]. KPGNN prioritizes structural information over seman-\ntic information, as it solely constructs a social network among\nmessages. Although QSGNN primarily focuses on generaliz-\ning the model from known data to unknown data, it is similar to\nKPGNN in that it still places emphasis on structural informa-\ntion. Different from them, HCRCNMI leverages both semantic\nand structural information in social networks to acquire a more\nextensive understanding.\nFurther, we perform the significance test, i.e., t-test, and ob-\nserve that the P values are consistently lower than 0.05, e.g.,\n0.012 on the MAVEN dataset. This indicates that the improve-\nment achieved by HCRC is statistically significant, further re-\ninforcing the effectiveness and superiority of HCRC.\n5.4. Visualization Results\nIn this subsection, we use t-Distributed Stochastic Neigh-\nbor Embedding (T-SNE) [57] to reduce the dimensionality of\n11\nTable 8: Semi-supervised and solid incremental evaluation ARIs. The best results are marked in bold.\nBlocks\nM1\nM2\nM3\nM4\nM5\nM6\nM7\nKPGNN10%\n.12±.01\n.66±.00\n.42±.00\n.27±.02\n.35±.02\n.63±.04\n.19±.02\nQSGNN10%\n.08±.02\n.64±.02\n.45±.00\n.30±.02\n.39±.01\n.54±.01\n.15±.00\nHCRC10%\n.05±.01\n.78±.02\n.69±.09\n.34±.00\n.55±.00\n.75±.02\n.10±.04\n∆\n↓7%\n↑12%\n↑24%\n↑4%\n↑16%\n↑12%\n↓9%\nKPGNNrandom\n.02±.01\n.63±.05\n.31±.08\n.20±.06\n.30±.08\n.40±.03\n.04±.00\nQSGNNrandom\n.04±.00\n.68±.02\n.33±.01\n.32±.01\n.33±.03\n.41±.01\n.13±.00\nHCRCrandom\n.02±.00\n.78±.01\n.57±.02\n.35±.02\n.37±.02\n.74±.01\n.07±.02\nHCRC\n.01±.00\n.79±.00\n.59±.04\n.42±.05\n.47±.02\n.78±.04\n.11±.01\n∆\n↓3%\n↑11%\n↑26%\n↑10%\n↑14%\n↑37%\n↓2%\nBlocks\nM8\nM9\nM10\nM11\nM12\nM13\nM14\nKPGNN10%\n.53±.01\n.30±.01\n.57±.02\n.45±.02\n.29±.02\n.34±.04\n.29±.00\nQSGNN10%\n.34±.01\n.33±.00\n.49±.02\n.44±.00\n.26±.02\n.33±.01\n.29±.01\nHCRC10%\n.53±.05\n.53±.01\n.52±.01\n.70±.11\n.42±.04\n.67±.01\n.41±.04\n∆\n↑0%\n↑20%\n↓5%\n↑25%\n↑13%\n↑33%\n↑12%\nKPGNNrandom\n.45±.02\n.28±.04\n.27±.03\n.25±.08\n.23±.02\n.16±.09\n.17±.02\nQSGNNrandom\n.38±.01\n.32±.00\n.43±.02\n.32±.01\n.21±.03\n.49±.00\n.20±.01\nHCRCrandom\n.37±.02\n.38±.02\n.51±.07\n.22±.02\n.17±.02\n.29±.01\n.39±.05\nHCRC\n.48±.01\n.42±.01\n.64±.02\n.37±.07\n.31±.02\n.56±.06\n.41±.00\n∆\n↑3%\n↑10%\n↑21%\n↑5%\n↑8%\n↑7%\n↑21%\nBlocks\nM15\nM16\nM17\nM18\nM19\nM20\nM21\nKPGNN10%\n.11±.04\n.57±.01\n.36±.03\n.15±.04\n.21±.04\n.36±.03\n.10±.01\nQSGNN10%\n.24±.01\n.43±.01\n.29±.02\n.23±.00\n.29±.01\n.33±.01\n.18±.00\nHCRC10%\n.69±.00\n.78±.03\n.58±.13\n.26±.04\n.52±.03\n.28±.04\n.42±.03\n∆\n↑45%\n↑21%\n↑22%\n↑3%\n↑23%\n↓8%\n↑24%\nKPGNNrandom\n.06±.01\n.30±.02\n.26±.02\n.18±.03\n.27±.08\n.27±.07\n.09±.02\nQSGNNrandom\n.23±.01\n.45±.01\n.36±.00\n.24±.01\n.32±.01\n.38±.02\n.20±.01\nHCRCrandom\n.26±.01\n.65±.05\n.45±.02\n.19±.01\n.39±.01\n.22±.02\n.28±.04\nHCRC\n.59±.04\n.81±.03\n.53±.08\n.20±.08\n.50±.05\n.21±.07\n.07±.02\n∆\n↑36%\n↑36%\n↑17%\n↓4%\n↑18%\n↓17%\n↑8%\nTable 9: Thresholds learned by the proposed DRL-SinglePass.\nBlocks\nM0\nM1\nM2\nM3\nM4\nM5\nM6\nM7\nM8\nM9\nM10\nThreshold\n-\n0.68\n0.67\n0.70\n0.62\n0.73\n0.69\n0.66\n0.67\n0.68\n0.73\nBlocks\nM11\nM12\nM13\nM14\nM15\nM16\nM17\nM18\nM19\nM20\nM21\nThreshold\n0.74\n0.70\n0.61\n0.71\n0.69\n0.69\n0.68\n0.79\n0.65\n0.80\n0.72\nTable 10: Offline Evaluation Results on the Twitter dataset. The best results are in bold, and the second-best are underlined.\nMetrics\nWord2vec\nLDA\nWMD\nBERT\nBiLSTM\nPP-GCN\nEventX\nKPGNN\nQSGNN∗\nHCRCNMI\n∆\nNMI\n.44±.00\n.29±.00\n.65±.00\n.64±.00\n.63±.00\n.68±.02\n.72±.00\n.70±.01\n.69±.01\n.75±.02\n↑3%\nARI\n.02±.00\n.01±.00\n.06±.00\n.07±.00\n.17±.00\n.20±01\n.05±.00\n.22±.01\n.25±.02\n.37±.01\n↑12%\nTable 11: Offline Evaluation Results on the MAVEN dataset. The best results are in bold, and the second-best are underlined.\nMetrics\nWord2vec\nLDA\nWMD\nBERT\nBiLSTM\nPP-GCN\nEventX\nKPGNN\nQSGNN∗\nHCRCNMI\n∆\nNMI\n.42±.00\n.35±.00\n.46±.00\n.45±.00\n.44±.00\n.49±.01\n.69±.00\n.52±.01\n.55±.03\n.70±.03\n↑1%\nARI\n.02±.00\n.01±.00\n.04±.00\n.02±.00\n.02±.00\n.06±.00\n.00±.00\n.10±.00\n.09±.02\n.13±.02\n↑3%\nthe message representation in M3 and M13 to two dimensions.\nWe further present visualizations of the clustering outcomes ob-\ntained from KPGNN, QSGNN and HCRC across three distinct\nexperimental settings, aiming to provide additional evidence of\nthe superiority of HCRC. Our attention is predominantly di-\nrected towards the five most prevalent events in terms of tweet\nvolume, taking into account the long-tail challenge prevalent in\nsocial data. Tweets pertaining to the same event are represented\nusing consistent color markers. The results depicted in Fig. 6\nprovide compelling evidence that HCRC consistently outper-\nforms both KPGNN and QSGNN in terms of producing a more\ncompact clustering outcome with clearly defined boundaries.\nThis superiority of HCRC holds true across all experimental\nsettings, indicating its robustness and effectiveness across all\nexperimental settings.\nHCRC clearly achieves superior per-\nformance and demonstrates greater adaptability to incremental\nevent detection.\n5.5. Analysis of Hyperparameter\nIn this section, we analyze two crucial hyperparameters, the\nreinforcement learning feature proportion coefficient and the\nsize of embedding. To explore their sensitivity, we conduct a\ncomprehensive evaluation of the model’s performance on mes-\nsage block M3.\nThe reinforcement learning feature proportion coefficient is\nused in DRL-SinglePass to learn data features from how much\nproportion of tweets in order to obtain an appropriate thresh-\nold.\nAs illustrated in Fig.\n7 (a), the model’s performance\ndemonstrates minimal fluctuations when adjusting the propor-\ntion coefficient, signifying its insensitivity to this particular hy-\nperparameter. In light of practical considerations and the need\n12\nFigure 6: T-SNE visualization of the learned message representation on M3 and M13.\nTable 12: Ablation Study. HCRCG−CL and HCRCN−CL represent solely using graph-level contrastive learning and node-level contrastive learning module, respec-\ntively, during the model training process. The best results are marked in bold.\nBlocks\nM1\nM2\nM3\nM4\nM5\nM6\nM7\nHCRC\n.27±.00\n.83±.00\n.81±.01\n.67±.03\n74±.01\n.83±.01\n.50±.01\nHCRCrandom\n.27±.01\n.82±.00\n.79±.03\n.63±.05\n.70±.01\n.80±.00\n.46±.05\nHCRCNMI\n.30±.01\n.85±.00\n.83±.00\n.71±.01\n.77±.00\n.85±.00\n.52±.00\nHCRCG−CL\n.27±.00\n.77±.01\n.73±.01\n.65±.00\n.65±.01\n.80±.01\n.46±.01\nHCRCN−CL\n.25±.00\n.77±.01\n.76±.02\n.70±.00\n.70±.00\n.76±.02\n.48±.00\nBlocks\nM8\nM9\nM10\nM11\nM12\nM13\nM14\nHCRC\n.78±.01\n.76±.00\n.80±.01\n.70±.02\n.69±.00\n.69±.04\n.68±.01\nHCRCrandom\n.75±.01\n.70±.01\n.78±.01\n.67±.01\n.65±.01\n.67±.01\n.68±.01\nHCRCNMI\n.81±.00\n.79±.00\n.84±.00\n.80±.00\n.70±.01\n.79±.00\n.71±.01\nHCRCG−CL\n.77±.02\n.70±.01\n.78±.01\n.64±.00\n.62±.00\n.68±.03\n.65±.01\nHCRCN−CL\n.73±.01\n.74±.00\n.74±.00\n.73±.00\n.70±.01\n.74±.01\n.67±.01\nBlocks\nM15\nM16\nM17\nM18\nM19\nM20\nM21\nHCRC\n.68±.02\n.86±.02\n.71±.02\n.61±.01\n.74±.01\n.69±.02\n.56±.01\nHCRCrandom\n.58±.01\n.80±.02\n.67±.01\n.60±.01\n.69±.02\n.69±.01\n.55±.03\nHCRCNMI\n.70±.01\n.87±.00\n.75±.00\n.63±.01\n.76±.01\n.72±.00\n.62±.00\nHCRCG−CL\n.54±.01\n.82±.03\n.63±.02\n.56±.01\n.65±.00\n.71±.01\n.57±.01\nHCRCN−CL\n.67±.01\n.78±.01\n.68±.01\n.63±.00\n.71±.00\n.70±.00\n.58±.01\n(b)\nFigure 7: Analysis of Hyperparameter.\nto optimize training efficiency, we have chosen to compro-\nmise a marginal fraction of the model’s performance by setting\nthe reinforcement learning feature proportion coefficient to 0.1.\nBased on Fig. 7 (b), we observed that within the range of (64,\n128, 256, 512), the model demonstrates the best performance as\nthe embedding size increases. When the embedding size is set\nto 256, although there is an increase in computational complex-\nity, HCRC can capture the tweets’ more semantic information\nand exhibit better discriminative power. Therefore, we decide\nto set the embedding size to 256.\n5.6. Ablation Study\nIn this subsection, we conduct the ablation study on HCRC\nusing NMI, analyzing the effectiveness of its constituent com-\nponents, and the comparisons are shown in Table 12. From the\nfirst two rows of Table 12, it can be seen that in the absence of\nany available label information, DRL-SinglePass demonstrates\nits effectiveness in social event detection. From the last three\nrows of Table 12, compared with HCRCNMI, the variants elim-\ninating either graph-level contrastive learning (G-CL) or node-\nlevel contrastive learning (N-CL) generally underperform the\ncomplete model, which demonstrates the effectiveness of the\nproposed graph-level and node-level contrastive learning. From\nthe results in Table 3 and Table 12, we conclude that although\nKPGNN generally beats both the HCRC variants without G-CL\nor N-CL, the complete HCRC can outperform KPGNN. The re-\nmarkable performance boost verifies the superiority of hybrid\ngraph contrastive learning.\n13\n6. Conclusion and Future Work\nWe clarify the issues existing in benchmark methods, i.e.,\nthe adopted GCL cannot sufficiently capture the semantic in-\nformation of social messages. Current embedding clustering\napproaches exceptionally adopt the data-related information re-\nsulting in the breach of the solidly unsupervised warranty. To\nthis end, we propose HCRC to learn the comprehensive seman-\ntic and structural information from social messages by using\nhybrid graph contrastive learning, and the proposed reinforced\nincremental clustering empowers HCRC to perform solid incre-\nmental clustering. Empirically, HCRC outperforms baselines in\nvarious experimental settings.\nDue to the incremental nature of HCRC, we do not im-\npose any practical restrictions on the dataset size, such that\nthe Twitter dataset used in our experiments can be continuously\nmaintained, thereby enabling the extension of HCRC to larger\ndatasets. Due to the limitation of the available datasets, we can\nonly perform the evaluation of our approach on the adopted\ndatasets that meet the required criteria. Therefore, in future\nwork, our principal emphasis will center on exploring the ap-\nplication of HCRC to large-scale datasets.\nAcknowledgments\nThe authors would like to thank the anonymous reviewers for\ntheir valuable comments. This work is supported by the Fun-\ndamental Research Program, Grant No. JCKY2022130C020,\nand the Strategic Priority Research Program of the Chinese\nAcademy of Sciences, Grant No. XDA19020500.\nReferences\n[1] Kepios, Global social media statistics (2022).\nURL https://datareportal.com/social-media-users\n[2] A. Ritter, O. Etzioni, S. Clark, Open domain event extraction from twitter,\nin: Proceedings of the 18th ACM SIGKDD international conference on\nKnowledge discovery and data mining, 2012, pp. 1104–1112.\n[3] Y. Mohammad, How many tweets per day 2022 (2022).\nURL https://www.renolon.com/number-of-tweets-per-day/\n[4] T. Mikolov, K. Chen, G. Corrado, J. Dean, Efficient estimation of word\nrepresentations in vector space, in: Y. Bengio, Y. LeCun (Eds.), 1st In-\nternational Conference on Learning Representations, ICLR 2013, Scotts-\ndale, Arizona, USA, May 2-4, 2013, Workshop Track Proceedings, 2013.\nURL http://arxiv.org/abs/1301.3781\n[5] D. M. Blei, A. Y. Ng, M. I. Jordan, Latent dirichlet allocation, J. Mach.\nLearn. Res. 3 (2003) 993–1022.\nURL http://jmlr.org/papers/v3/blei03a.html\n[6] M. J. Kusner, Y. Sun, N. I. Kolkin, K. Q. Weinberger, From word embed-\ndings to document distances, in: F. R. Bach, D. M. Blei (Eds.), Proceed-\nings of the 32nd International Conference on Machine Learning, ICML\n2015, Lille, France, 6-11 July 2015, Vol. 37 of JMLR Workshop and Con-\nference Proceedings, JMLR.org, 2015, pp. 957–966.\nURL http://proceedings.mlr.press/v37/kusnerb15.html\n[7] J. Devlin, M. Chang, K. Lee, K. Toutanova, BERT: pre-training of deep\nbidirectional transformers for language understanding, in: J. Burstein,\nC. Doran, T. Solorio (Eds.), Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for Computational Linguis-\ntics: Human Language Technologies, NAACL-HLT 2019, Minneapolis,\nMN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), As-\nsociation for Computational Linguistics, 2019, pp. 4171–4186.\ndoi:\n10.18653/v1/n19-1423.\nURL https://doi.org/10.18653/v1/n19-1423\n[8] A. Graves, J. Schmidhuber, Framewise phoneme classification with bidi-\nrectional LSTM and other neural network architectures, Neural Networks\n18 (5-6) (2005) 602–610. doi:10.1016/j.neunet.2005.06.042.\nURL https://doi.org/10.1016/j.neunet.2005.06.042\n[9] Y. Cao, H. Peng, J. Wu, Y. Dou, J. Li, P. S. Yu, Knowledge-preserving in-\ncremental social event detection via heterogeneous gnns, in: J. Leskovec,\nM. Grobelnik, M. Najork, J. Tang, L. Zia (Eds.), WWW ’21: The\nWeb Conference 2021, Virtual Event / Ljubljana, Slovenia, April 19-23,\n2021, ACM / IW3C2, 2021, pp. 3383–3395. doi:10.1145/3442381.\n3449834.\nURL https://doi.org/10.1145/3442381.3449834\n[10] H. Peng, J. Li, Q. Gong, Y. Song, Y. Ning, K. Lai, P. S. Yu, Fine-grained\nevent categorization with heterogeneous graph convolutional networks,\nin: S. Kraus (Ed.), Proceedings of the Twenty-Eighth International Joint\nConference on Artificial Intelligence, IJCAI 2019, Macao, China, August\n10-16, 2019, ijcai.org, 2019, pp. 3238–3245. doi:10.24963/ijcai.\n2019/449.\nURL https://doi.org/10.24963/ijcai.2019/449\n[11] B. Liu, F. X. Han, D. Niu, L. Kong, K. Lai, Y. Xu, Story forest: Extract-\ning events and telling stories from breaking news, ACM Trans. Knowl.\nDiscov. Data 14 (3) (2020) 31:1–31:28. doi:10.1145/3377939.\nURL https://doi.org/10.1145/3377939\n[12] J. Ren, L. Jiang, H. Peng, Y. Cao, J. Wu, P. S. Yu, L. He, From known to\nunknown: Quality-aware self-improving graph neural network for open\nset social event detection, in: M. A. Hasan, L. Xiong (Eds.), Proceedings\nof the 31st ACM International Conference on Information & Knowledge\nManagement, Atlanta, GA, USA, October 17-21, 2022, ACM, 2022, pp.\n1696–1705. doi:10.1145/3511808.3557329.\nURL https://doi.org/10.1145/3511808.3557329\n[13] Y. Vasiliev, Natural Language Processing with Python and SpaCy: A\nPractical Introduction, No Starch Press, 2020.\n[14] W. Dou, X. Wang, W. Ribarsky, M. Zhou, Event detection in social media\ndata, in: IEEE VisWeek workshop on interactive visual text analytics-task\ndriven analytics of social media content, 2012, pp. 971–980.\n[15] H. Becker, M. Naaman, L. Gravano, Beyond trending topics: Real-world\nevent identification on twitter, in:\nL. A. Adamic, R. Baeza-Yates,\nS. Counts (Eds.), Proceedings of the Fifth International Conference on\nWeblogs and Social Media, Barcelona, Catalonia, Spain, July 17-21,\n2011, The AAAI Press, 2011.\nURL\nhttp://www.aaai.org/ocs/index.php/ICWSM/ICWSM11/\npaper/view/2745\n[16] S. Phuvipadawat, T. Murata, Breaking news detection and tracking in\ntwitter, in: Proceedings of the 2010 IEEE/WIC/ACM International Con-\nference on Web Intelligence and International Conference on Intelligent\nAgent Technology - Workshops, Toronto, Canada, August 31 - Septem-\nber 3, 2010, IEEE Computer Society, 2010, pp. 120–123. doi:10.1109/\nWI-IAT.2010.205.\nURL https://doi.org/10.1109/WI-IAT.2010.205\n[17] K. Morabia, L. B. M. Neti, A. Malapati, S. S. Samant, Sedtwik:\nSegmentation-based event detection from tweets using wikipedia, in:\nS. Kar, F. Nadeem, L. Burdick, G. Durrett, N. Han (Eds.), Proceedings\nof the 2019 Conference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Language Technologies,\nNAACL-HLT 2019, Minneapolis, MN, USA, June 3-5, 2019, Student Re-\nsearch Workshop, Association for Computational Linguistics, 2019, pp.\n77–85. doi:10.18653/v1/n19-3011.\nURL https://doi.org/10.18653/v1/n19-3011\n[18] A. Dusart, K. Pinel-Sauvagnat, G. Hubert, Tssubert: Tweet stream sum-\nmarization using BERT, CoRR abs/2106.08770 (2021). arXiv:2106.\n08770.\nURL https://arxiv.org/abs/2106.08770\n[19] K. Chakma, S. D. Swamy, A. Das, S. Debbarma, 5w1h-based semantic\nsegmentation of tweets for event detection using bert, in: International\nConference on Machine Learning, Image Processing, Network Security\nand Data Sciences, Springer, 2020, pp. 57–72.\n[20] H. Peng, J. Li, Y. Song, R. Yang, R. Ranjan, P. S. Yu, L. He, Streaming so-\ncial event detection and evolution discovery in heterogeneous information\nnetworks, ACM Trans. Knowl. Discov. Data 15 (5) (2021) 89:1–89:33.\ndoi:10.1145/3447585.\nURL https://doi.org/10.1145/3447585\n[21] X. Wang, Y. Lu, C. Shi, R. Wang, P. Cui, S. Mou, Dynamic hetero-\n14\ngeneous information network embedding with meta-path based proxim-\nity, IEEE Trans. Knowl. Data Eng. 34 (3) (2022) 1117–1132.\ndoi:\n10.1109/TKDE.2020.2993870.\nURL https://doi.org/10.1109/TKDE.2020.2993870\n[22] Y. Fang, X. Zhao, P. Huang, W. Xiao, M. de Rijke, Scalable repre-\nsentation learning for dynamic heterogeneous information networks via\nmetagraphs, ACM Trans. Inf. Syst. 40 (4) (2022) 64:1–64:27.\ndoi:\n10.1145/3485189.\nURL https://doi.org/10.1145/3485189\n[23] Z. Wen, Y. Fang, TREND: temporal event and node dynamics for graph\nrepresentation learning, in: F. Laforest, R. Troncy, E. Simperl, D. Agar-\nwal, A. Gionis, I. Herman, L. M´edini (Eds.), WWW ’22: The ACM Web\nConference 2022, Virtual Event, Lyon, France, April 25 - 29, 2022, ACM,\n2022, pp. 1159–1169. doi:10.1145/3485447.3512164.\nURL https://doi.org/10.1145/3485447.3512164\n[24] Y. Xie, Z. Xu, J. Zhang, Z. Wang, S. Ji, Self-supervised learning of graph\nneural networks: A unified review, IEEE Trans. Pattern Anal. Mach. In-\ntell. 45 (2) (2023) 2412–2429. doi:10.1109/TPAMI.2022.3170559.\nURL https://doi.org/10.1109/TPAMI.2022.3170559\n[25] Y. Zhu, Y. Xu, F. Yu, Q. Liu, S. Wu, L. Wang, Deep graph contrastive\nrepresentation learning, CoRR abs/2006.04131 (2020).\narXiv:2006.\n04131.\nURL https://arxiv.org/abs/2006.04131\n[26] J. Xia, L. Wu, G. Wang, J. Chen, S. Z. Li, Progcl: Rethinking hard nega-\ntive mining in graph contrastive learning (2021).\n[27] Y. You, T. Chen, Y. Shen, Z. Wang, Graph contrastive learning auto-\nmated, in: M. Meila, T. Zhang (Eds.), Proceedings of the 38th Interna-\ntional Conference on Machine Learning, ICML 2021, 18-24 July 2021,\nVirtual Event, Vol. 139 of Proceedings of Machine Learning Research,\nPMLR, 2021, pp. 12121–12132.\nURL http://proceedings.mlr.press/v139/you21a.html\n[28] Y. Yin, Q. Wang, S. Huang, H. Xiong, X. Zhang, Autogcl: Automated\ngraph contrastive learning via learnable view generators, in: Thirty-Sixth\nAAAI Conference on Artificial Intelligence, AAAI 2022, Thirty-Fourth\nConference on Innovative Applications of Artificial Intelligence, IAAI\n2022, The Twelveth Symposium on Educational Advances in Artificial\nIntelligence, EAAI 2022 Virtual Event, February 22 - March 1, 2022,\nAAAI Press, 2022, pp. 8892–8900.\nURL\nhttps://ojs.aaai.org/index.php/AAAI/article/view/\n20871\n[29] J. Xia, L. Wu, J. Chen, B. Hu, S. Z. Li, Simgrace: A simple framework\nfor graph contrastive learning without data augmentation, in: F. Lafor-\nest, R. Troncy, E. Simperl, D. Agarwal, A. Gionis, I. Herman, L. M´edini\n(Eds.), WWW ’22: The ACM Web Conference 2022, Virtual Event,\nLyon, France, April 25 - 29, 2022, ACM, 2022, pp. 1070–1079. doi:\n10.1145/3485447.3512156.\nURL https://doi.org/10.1145/3485447.3512156\n[30] R. Anand, U. Jeffrey David, Mining of massive datasets, Cambridge uni-\nversity press, 2011.\n[31] R. Papka, J. Allan, et al., On-line new event detection using single pass\nclustering, University of Massachusetts, Amherst 10 (290941.290954)\n(1998).\n[32] A. K. Jain, R. C. Dubes, Algorithms for clustering data, Prentice-Hall,\nInc., 1988.\n[33] E. Fix, J. L. Hodges, Discriminatory analysis. nonparametric discrimina-\ntion: Consistency properties, International Statistical Review/Revue In-\nternationale de Statistique 57 (3) (1989) 238–247.\n[34] S. Chakraborty, N. K. Nagwani, L. Dey, Performance comparison\nof incremental k-means and incremental DBSCAN algorithms, CoRR\nabs/1406.4751 (2014). arXiv:1406.4751.\nURL http://arxiv.org/abs/1406.4751\n[35] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wier-\nstra, M. A. Riedmiller, Playing atari with deep reinforcement learning,\nCoRR abs/1312.5602 (2013). arXiv:1312.5602.\nURL http://arxiv.org/abs/1312.5602\n[36] H. van Hasselt, A. Guez, D. Silver, Deep reinforcement learning with\ndouble q-learning, in:\nD. Schuurmans, M. P. Wellman (Eds.), Pro-\nceedings of the Thirtieth AAAI Conference on Artificial Intelligence,\nFebruary 12-17, 2016, Phoenix, Arizona, USA, AAAI Press, 2016, pp.\n2094–2100.\nURL\nhttp://www.aaai.org/ocs/index.php/AAAI/AAAI16/\npaper/view/12389\n[37] Z. Wang, T. Schaul, M. Hessel, H. van Hasselt, M. Lanctot, N. de Fre-\nitas, Dueling network architectures for deep reinforcement learning, in:\nM. Balcan, K. Q. Weinberger (Eds.), Proceedings of the 33nd Interna-\ntional Conference on Machine Learning, ICML 2016, New York City,\nNY, USA, June 19-24, 2016, Vol. 48 of JMLR Workshop and Conference\nProceedings, JMLR.org, 2016, pp. 1995–2003.\nURL http://proceedings.mlr.press/v48/wangf16.html\n[38] M. Hessel, J. Modayil, H. van Hasselt, T. Schaul, G. Ostrovski, W. Dab-\nney, D. Horgan, B. Piot, M. G. Azar, D. Silver, Rainbow: Combining\nimprovements in deep reinforcement learning, in: S. A. McIlraith, K. Q.\nWeinberger (Eds.), Proceedings of the Thirty-Second AAAI Conference\non Artificial Intelligence, (AAAI-18), the 30th innovative Applications\nof Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on\nEducational Advances in Artificial Intelligence (EAAI-18), New Orleans,\nLouisiana, USA, February 2-7, 2018, AAAI Press, 2018, pp. 3215–3222.\nURL\nhttps://www.aaai.org/ocs/index.php/AAAI/AAAI18/\npaper/view/17204\n[39] D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, M. A. Riedmiller,\nDeterministic policy gradient algorithms, in: Proceedings of the 31th\nInternational Conference on Machine Learning, ICML 2014, Beijing,\nChina, 21-26 June 2014, Vol. 32 of JMLR Workshop and Conference\nProceedings, JMLR.org, 2014, pp. 387–395.\nURL http://proceedings.mlr.press/v32/silver14.html\n[40] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Sil-\nver, D. Wierstra, Continuous control with deep reinforcement learning,\nin: Y. Bengio, Y. LeCun (Eds.), 4th International Conference on Learn-\ning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016,\nConference Track Proceedings, 2016.\nURL http://arxiv.org/abs/1509.02971\n[41] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. P. Lillicrap, T. Harley,\nD. Silver, K. Kavukcuoglu, Asynchronous methods for deep reinforce-\nment learning, in: M. Balcan, K. Q. Weinberger (Eds.), Proceedings of\nthe 33nd International Conference on Machine Learning, ICML 2016,\nNew York City, NY, USA, June 19-24, 2016, Vol. 48 of JMLR Workshop\nand Conference Proceedings, JMLR.org, 2016, pp. 1928–1937.\nURL http://proceedings.mlr.press/v48/mniha16.html\n[42] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, O. Klimov, Proximal\npolicy optimization algorithms, CoRR abs/1707.06347 (2017). arXiv:\n1707.06347.\nURL http://arxiv.org/abs/1707.06347\n[43] T. Haarnoja, A. Zhou, P. Abbeel, S. Levine, Soft actor-critic: Off-policy\nmaximum entropy deep reinforcement learning with a stochastic actor, in:\nJ. G. Dy, A. Krause (Eds.), Proceedings of the 35th International Confer-\nence on Machine Learning, ICML 2018, Stockholmsm¨assan, Stockholm,\nSweden, July 10-15, 2018, Vol. 80 of Proceedings of Machine Learning\nResearch, PMLR, 2018, pp. 1856–1865.\nURL http://proceedings.mlr.press/v80/haarnoja18b.html\n[44] M. Gori, G. Monfardini, F. Scarselli, A new model for learning in graph\ndomains, in: Proceedings. 2005 IEEE international joint conference on\nneural networks, Vol. 2, 2005, pp. 729–734.\n[45] P. Velickovic, G. Cucurull, A. Casanova, A. Romero, P. Li`o, Y. Bengio,\nGraph attention networks, in: 6th International Conference on Learning\nRepresentations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3,\n2018, Conference Track Proceedings, OpenReview.net, 2018.\nURL https://openreview.net/forum?id=rJXMpikCZ\n[46] T. N. Kipf, M. Welling, Semi-supervised classification with graph con-\nvolutional networks, in: 5th International Conference on Learning Rep-\nresentations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference\nTrack Proceedings, OpenReview.net, 2017.\nURL https://openreview.net/forum?id=SJU4ayYgl\n[47] K. Xu, W. Hu, J. Leskovec, S. Jegelka, How powerful are graph neural\nnetworks?, in: 7th International Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019, OpenReview.net,\n2019.\nURL https://openreview.net/forum?id=ryGs6iA5Km\n[48] Y. You, T. Chen, Y. Sui, T. Chen, Z. Wang, Y. Shen, Graph contrastive\nlearning with augmentations, in: H. Larochelle, M. Ranzato, R. Hadsell,\nM. Balcan, H. Lin (Eds.), Advances in Neural Information Processing\nSystems 33:\nAnnual Conference on Neural Information Processing\nSystems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.\n15\nURL\nhttps://proceedings.neurips.cc/paper/2020/hash/\n3fe230348e9a12c13120749e3f9fa4cd-Abstract.html\n[49] Y. Zhu, Y. Xu, F. Yu, Q. Liu, S. Wu, L. Wang, Graph contrastive learning\nwith adaptive augmentation, in: J. Leskovec, M. Grobelnik, M. Najork,\nJ. Tang, L. Zia (Eds.), WWW ’21: The Web Conference 2021, Virtual\nEvent / Ljubljana, Slovenia, April 19-23, 2021, ACM / IW3C2, 2021, pp.\n2069–2080. doi:10.1145/3442381.3449802.\nURL https://doi.org/10.1145/3442381.3449802\n[50] J. C. Dunn, Well-separated clusters and optimal fuzzy partitions, Journal\nof cybernetics 4 (1) (1974) 95–104.\n[51] P. J. Rousseeuw, Silhouettes: a graphical aid to the interpretation and\nvalidation of cluster analysis, Journal of computational and applied math-\nematics 20 (1987) 53–65.\n[52] T. Cali´nski, J. Harabasz, A dendrite method for cluster analysis, Commu-\nnications in Statistics-theory and Methods 3 (1) (1974) 1–27.\n[53] A. J. McMinn, Y. Moshfeghi, J. M. Jose, Building a large-scale cor-\npus for evaluating event detection on twitter, in: Q. He, A. Iyengar,\nW. Nejdl, J. Pei, R. Rastogi (Eds.), 22nd ACM International Conference\non Information and Knowledge Management, CIKM’13, San Francisco,\nCA, USA, October 27 - November 1, 2013, ACM, 2013, pp. 409–418.\ndoi:10.1145/2505515.2505695.\nURL https://doi.org/10.1145/2505515.2505695\n[54] X. Wang, Z. Wang, X. Han, W. Jiang, R. Han, Z. Liu, J. Li, P. Li,\nY. Lin, J. Zhou, MAVEN: A massive general domain event detection\ndataset, in: B. Webber, T. Cohn, Y. He, Y. Liu (Eds.), Proceedings of the\n2020 Conference on Empirical Methods in Natural Language Processing,\nEMNLP 2020, Online, November 16-20, 2020, Association for Compu-\ntational Linguistics, 2020, pp. 1652–1671. doi:10.18653/v1/2020.\nemnlp-main.129.\nURL https://doi.org/10.18653/v1/2020.emnlp-main.129\n[55] P. A. Est´evez, M. Tesmer, C. A. Perez, J. M. Zurada, Normalized mutual\ninformation feature selection, IEEE Trans. Neural Networks 20 (2) (2009)\n189–201. doi:10.1109/TNN.2008.2005601.\nURL https://doi.org/10.1109/TNN.2008.2005601\n[56] X. V. Nguyen, J. Epps, J. Bailey, Information theoretic measures for\nclusterings comparison: Variants, properties, normalization and correc-\ntion for chance, J. Mach. Learn. Res. 11 (2010) 2837–2854.\ndoi:\n10.5555/1756006.1953024.\nURL https://dl.acm.org/doi/10.5555/1756006.1953024\n[57] L. Van der Maaten, G. Hinton, Visualizing data using t-sne., Journal of\nmachine learning research 9 (11) (2008).\n16\n",
  "categories": [
    "cs.SI",
    "cs.AI"
  ],
  "published": "2023-12-08",
  "updated": "2023-12-15"
}