{
  "id": "http://arxiv.org/abs/2312.09983v2",
  "title": "Toward Computationally Efficient Inverse Reinforcement Learning via Reward Shaping",
  "authors": [
    "Lauren H. Cooke",
    "Harvey Klyne",
    "Edwin Zhang",
    "Cassidy Laidlaw",
    "Milind Tambe",
    "Finale Doshi-Velez"
  ],
  "abstract": "Inverse reinforcement learning (IRL) is computationally challenging, with\ncommon approaches requiring the solution of multiple reinforcement learning\n(RL) sub-problems. This work motivates the use of potential-based reward\nshaping to reduce the computational burden of each RL sub-problem. This work\nserves as a proof-of-concept and we hope will inspire future developments\ntowards computationally efficient IRL.",
  "text": "Preprint\nTOWARD\nCOMPUTATIONALLY\nEFFICIENT\nINVERSE\nREINFORCEMENT LEARNING VIA REWARD SHAPING\nLauren H. Cooke*, Harvey Klyne*, Edwin Zhang*\nHarvard University\nCassidy Laidlaw\nUniversity of California, Berkeley\nMilind Tambe, Finale Doshi-Velez\nHarvard University\nABSTRACT\nInverse reinforcement learning (IRL) is computationally challenging, with com-\nmon approaches requiring the solution of multiple reinforcement learning (RL)\nsub-problems. This work motivates the use of potential-based reward shaping to\nreduce the computational burden of each RL sub-problem. This work serves as a\nproof-of-concept and we hope will inspire future developments towards computa-\ntionally efficient IRL.\n1\nINTRODUCTION\nInverse reinforcement learning (IRL) is the task of deriving a reward function that recovers expert\nbehavior within an environment (Ng & Russell, 2000) and can be computationally expensive to\nsolve. IRL algorithms typically consist of a loop in which every step requires finding the optimal\npolicy for the current reward estimate (e.g. Abbeel & Ng (2004); Ramachandran & Amir (2007);\nWulfmeier et al. (2016)). This means that within a single IRL optimization multiple reinforcement\nlearning (RL) problems need to be solved, each of which may be challenging. One can solve RL\ntasks by planning actions sufficiently far into the future (Sutton & Barto, 2018), and the necessary\nplanning depth is a measure of the computational challenge of the problem. In the special case where\nthe RL optimization makes use of a sample-based solver, planning depth can be thought of in terms\nof sample complexity (Kakade, 2003). Previous works have attempted to reduce the overall cost of\nIRL by deliberately truncating the planning depth, accepting an approximation to the optimal policy\nat each iteration (MacGlashan & Littman, 2015; Xu et al., 2022).\nSince multiple reward functions can encode an optimal policy (Russell, 1998; Cao et al., 2021), we\nhave some choice about what reward function we optimize at each iteration. We envision using\npotential-based reward shaping (Ng et al., 1999) to reduce the computational cost of each RL sub-\nproblem without altering any of the optimal policies. This itself is too large a goal for the present\nwork, so we focus our efforts on demonstrating a proof-of-concept in a simplified setting. In par-\nticular, we examine how sample trajectories from both optimal and random policies may be used\nto select a potential function for an initial feasible reward (one which encodes the optimal policy),\nwhich we call planning-aware reward shaping. Previous work on reward shaping includes Hu et al.\n(2020); Dong et al. (2020); Cheng et al. (2021); Gupta et al. (2022); De Lellis et al. (2023). To be\nclear, our present procedure does not directly address the problem of making IRL more computa-\ntionally efficient, but we hope that the conclusions drawn may inspire future work.\n2\nPLANNING-AWARE REWARD SHAPING\nSuppose we have been given a Markov Decision Process without a reward M \\ R = (S, A, γ, P),\nand using optimal trajectories have learned a feasible reward R0 using some IRL algorithm. We also\nassume access to a set of trajectories which have selected actions uniformly at random, and we use\nthis additional exploration information to make a one-step adjustment to R0. This adjustment takes\n*Equal contribution. First author order is alphabetical.\n1\narXiv:2312.09983v2  [cs.LG]  18 Dec 2023\nPreprint\nthe form of a potential function Φ : S →R, with our final reward function estimate taking the form\nRΦ(s, a) := R0(s, a) + γEs′∼p(·|s,a)[Φ(s′)] −Φ(s).\n(1)\nThe optimal policies for R0 and RΦ are the same for any Φ, and any equivalent reward may be\nwritten in the form RΦ for some Φ (Ng et al., 1999). Crucially, the planning depths associated\nwith rewards RΦ may differ across choices of potential function Φ. We stress that the goal of\nthis work is to inspire future investigation into computationally efficient IRL, a problem we do not\nclaim to have solved here. Our goal is to choose a shaped reward RΦ which minimizes a certain\nbound on an algorithm-agnostic measure of planning depth (Laidlaw et al., 2023). This bound has\nbeen found to be strongly correlated with the sample complexities of modern deep learning RL\nprocedures — including DQN (Mnih et al., 2015) and PPO (Schulman et al., 2017) — across a\nrange of tasks. Denote by Qrand\nΦ (s, a) the Q-function associated with the uniform-at-random policy\nπrand(a | s) := 1/|A|, and write V ∗\nΦ(s) for the value function associated with the optimal policy π∗.\nRecall that the optimal policy π∗does not depend on Φ. Following the derivation in Appendix A,\nwe find that our objective is minimized (potentially non-uniquely) by\nΦ(s) =\n\b\nmax\na∈A Qrand\n0\n(s, a) + V ∗\n0 (s)\n\t\n/2.\n(2)\nBoth V ∗\n0 and Qrand\n0\nare learnable from the expert and random exploration trajectories respectively.\nHowever, assuming access to the optimal value function V ∗\n0 trivializes the forward RL challenge\n(e.g. set R(s) = V ∗\n0 (s) and act greedily over next actions). We anticipate that IRL algorithms may\nbe able to iteratively update the shaping potential Φ based on the current estimates of Qrand\n0\nand\nV ∗\n0 , which may improve the overall computational efficiency. We further remark that estimating\nQrand\n0\nis much easier than estimating V ∗\n0 , so shaping based on (2) might have better finite-sample\nperformance than potentials based on estimates of V ∗\n0 alone.\n3\nEXPERIMENTS\nFigure 1: Return (measured by R0) obtained\nby DQN. We compare learning with R0 (pur-\nple), RΦ (green), and RMaxEnt (orange), av-\neraged across 500 random DQN seeds, each\ntraining for 50K steps. Our shaped reward\nRΦ enables DQN to converge to the opti-\nmal value faster than the initial reward R0,\ndemonstrating a reduction of planning depth.\nDQN fails to optimize RMaxEnt.\nWe demonstrate that an oracle version of our proce-\ndure reduces the sample complexity for DQN (Fig-\nure 1), which we use as a proxy for planning depth.\nWe evaluate our method in a 5×5 deterministic grid-\nworld since we can easily find the optimal policy, yet\nDQN struggles and takes tens of thousands of steps\nto converge (Laidlaw et al. (2023, Tab. G.4)).\nIn these experiments, we first fix transition dynamics\nand an initial reward R0 before solving for the opti-\nmal policy π∗by value iteration. As an IRL baseline,\nwe also perform Maximum Entropy IRL (Ziebart\net al., 2008) on π∗to obtain a reward RMaxEnt. We\ncompute the optimal value function V ∗\n0 and the ran-\ndom policy Q-function Qrand\n0\nusing Monte Carlo, the\npotential function Φ using (2), and the shaped reward\nRΦ using (1). Note that R0, RΦ, and RMaxEnt all en-\ncode the same optimal policy. We compare the plan-\nning depths of these three rewards using the sample\nefficiency of DQN, finding that the shaped reward\nRΦ enables DQN to converge to the optimal solu-\ntion faster than the initial reward R0. We also find\nthat DQN fails to optimize RMaxEnt, with the policy\ngetting stuck in a local optimum state rather than reaching the goal state. Implementation details and\ncode to reproduce our experiments are included in Appendix B and the supplementary materials.\n4\nCONCLUSION\nIn this work, we motivate planning-aware reward shaping to reduce the computational complexity\nof IRL. We demonstrate that an oracle version of our procedure reduces the planning depth of an\n2\nPreprint\nRL task, as measured by the sample complexity of DQN (Section 3). Compared to existing IRL ap-\nproaches, we leverage the additional information included in random trajectories to apply automatic\nreward shaping. Our hope is that our procedure may inspire novel IRL algorithms which are more\ncomputationally efficient. While we focus on the IRL setting, adaptive shaping procedures such as\nours may also be of interest to the broader RL community.\nREFERENCES\nPieter Abbeel and Andrew Y. Ng. Apprenticeship learning via inverse reinforcement learning. In\nProceedings of the Twenty-First International Conference on Machine Learning, pp. 1–8, 2004.\nHaoyang Cao, Samuel N. Cohen, and Łukasz Szpruch. Identifiability in inverse reinforcement learn-\ning. In Proceedings of the 35th Conference on Neural Information Processing Systems, pp. 1–11,\n2021.\nChing-An Cheng, Andrey Kolobov, and Adith Swaminathan. Heuristic-guided reinforcement learn-\ning. arXiv, pp. 2106.02757, 2021.\nFrancesco De Lellis, Marco Coraggio, Giovanni Russo, Mirco Musolesi, and Mario di Bernardo.\nGuaranteeing control requirements via reward shaping in reinforcement learning.\narXiv, pp.\n2311.10026, 2023.\nYunlong Dong, Xiuchuan Tang, and Ye Yuan. Principled reward shaping for reinforcement learning\nvia Lyapunov stability theory. Neurocomputing, 393:83–90, 2020.\nAbhishek Gupta, Aldo Pacchiano, Yuexiang Zhai, Sham M. Kakade, and Sergey Levine. Unpacking\nreward shaping: Understanding the benefits of reward engineering on sample complexity. arXiv,\npp. 2210.09579, 2022.\nYujing Hu, Weixun Wang, Hangtian Jia, Yixiang Wang, Yingfeng Chen, Jianye Hao, Feng Wu, and\nChangjie Fan. Learning to Utilize Shaping Rewards: A New Approach of Reward Shaping. arXiv,\npp. 2011.02669, 2020.\nSham M. Kakade. On the Sample Complexity of Reinforcement Learning. PhD thesis, University\nCollege London, 2003.\nDiederik P Kingma and Jimmy Ba.\nAdam: A method for stochastic optimization.\narXiv, pp.\n1412.6980, 2014.\nCassidy Laidlaw, Stuart Russell, and Anca Dragan. Bridging RL Theory and Practice with the\nEffective Horizon. arXiv, pp. 2304.09853, 2023.\nJames MacGlashan and Michael L. Littman. Between imitation and intention learning. In Qiang\nYang and Michael Wooldridge (eds.), Proceedings of the Twenty-Fourth International Joint Con-\nference on Artificial Intelligence, pp. 3692–3698, 2015.\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Belle-\nmare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen,\nCharles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wier-\nstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning.\nNature, 518:529–533, 2015.\nAndrew Y. Ng and Stuart J. Russell. Algorithms for inverse reinforcement learning. In Proceedings\nof the Seventeenth International Conference on Machine Learning, pp. 663–670, 2000.\nAndrew Y. Ng, Daishi Harada, and Stuart J. Russell. Policy invariance under reward transforma-\ntions: Theory and application to reward shaping. In Proceedings of the Sixteenth International\nConference on Machine Learning, pp. 278–287, 1999.\nDeepak Ramachandran and Eyal Amir. Bayesian inverse reinforcment learning. In Proceedings of\nthe 20th International Joint Conference on Artificial Intelligence, pp. 2586–2591, 2007.\n3\nPreprint\nStuart Russell. Learning Agents for Uncertain Environments (Extended Abstract). In Proceedings\nof the Eleventh Annual Conference on Computational Learning Theory, pp. 101–103. Association\nfor Computing Machinery, 1998. ISBN 1581130570.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\noptimization algorithms. arXiv, pp. 1707.06347, 2017.\nR.S. Sutton and A.G. Barto. Reinforcement Learning, second edition: An Introduction. Adaptive\nComputation and Machine Learning series. MIT Press, 2018. ISBN 9780262352703.\nMarkus Wulfmeier, Peter Ondruska, and Ingmar Posner. Maximum entropy deep inverse reinforce-\nment learning. arXiv, pp. 1507.04888, 2016.\nYiqing Xu, Wei Gao, and David Hsu. Receding horizon inverse reinforcement learning. In Alice H.\nOh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Proceedings of the Thirty-\nSixth Conference on Neural Information Processing Systems, 2022.\nBrian D. Ziebart, Andrew Maas, J. Andrew Bagnell, and Anind K. Dey. Maximum entropy inverse\nreinforcement learning. In Anthony Cohn (ed.), Proceedings of the 23rd National Conference on\nArtificial Intelligence, pp. 1433–1438, 2008.\nAPPENDIX\nA\nREWARD SHAPING OPTIMIZATION OBJECTIVE\nLaidlaw et al. (2023) consider algorithm-agnostic proxies for the sample complexities of modern\ndeep RL approaches across measures of correlation, tightness, and accuracy. They introduce the\neffective horizon H := mink k + log|A| mk, where k is a tuning parameter in a simple Monte Carlo\nalgorithm (see Laidlaw et al. (2023, Alg. 1)) and mk is the minimum sample size this algorithm\nrequires to find an optimal policy with probability at least 1/2. It is not feasible to compute H\nin closed form, but they find that a particular bound serves as a good proxy (Laidlaw et al. (2023,\nThm. 5.4)). Considering an MDP with finite-time horizon T and γ = 1, it holds that:\nH ≤\nmin\nk=1,...,T k +\nmax\nt∈T,s∈S,a∈A log|A|\n\u0012Qk\nt (s, a)V ∗\nt (s)\n∆k\nt (s)2\n\u0013\n+ logA 6 log(2T|A|k),\nwhere\n∆k\nt (s) = max\na∈A Qk\nt (s, a) −\nmax\na′ /∈arg maxa Qk\nt (s,a) Qk\nt (s, a′),\nand Qk\nt , V ∗\nt , and ∆k\nt are defined as in Laidlaw et al. (2023).\nWe relax this bound by fixing k = 1 — which we think is reasonable considering how k is fixed to 1\nin practice (e.g. Laidlaw et al. (2023, Sec. F.1)) — but the potential function Φ we derive generalizes\nto other choices of k. We consider MDPs with T = ∞, γ < 1 and time-invariant policies, motivating\nthe following optimization for our planning-aware reward shaping:\nmax\nΦ:S→R\n\u001a\nmax\ns∈S,a∈A log|A|\n\u0012Qrand\nΦ (s, a)V ∗\nΦ(s)\n∆Φ(s)2\n\u0013\u001b\n.\n(3)\nThis is strictly increasing in the following criteria:\nℓ(Φ; R0) :=\nmax\ns∈S,a∈A\nQrand\nΦ (s, a)V ∗\nΦ(s)\n∆Φ(s)2\n.\nIn fact, for any policy π the associated Q-function and value function transform linearly under\nreward shaping (Ng et al., 1999, Cor. 2):\nQπ\nΦ(s, a) = Qπ\n0(s, a) −Φ(s);\nV π\nΦ (s) = V π\n0 (s) −Φ(s).\nTherefore ∆Φ(s) = ∆0(s) for all potentials Φ, so the objective reduces to\nℓ(Φ; R0) =\nmax\ns∈S,a∈A\n\b\nQrand\n0\n(s, a) −Φ(s)\n\t\b\nV ∗\n0 (s) −Φ(s)\n\t\n∆0(s)2\n.\nThe potential function (2) solves this quadratic for every s ∈S, and is thus a global minimizer. The\nsolution can be found by straightforwardly taking the derivative of Equation 3 and setting to 0.\n4\nPreprint\nB\nIMPLEMENTATION DETAILS FOR EXPERIMENTS\nIn Figure 1 we plot the returns achieved by each optimization procedure at each training episode by\ntaking an average across 500 random seeds, along with 95% bootstrapped confidence intervals. Each\nseed determined one individual training process, wherein we train for 500 episodes of 100 steps each,\nfor a total of 50K training steps. Returns are evaluated at the end of each episode with respect to the\ninitial reward function R0, regardless of which reward function is used during training. This ensures\nthat returns are comparable between objectives. We set a 100 timestep limit on the environment.\nHyperparameter\nValue\nDQN HP\nOptimizer\nAdam (Kingma & Ba, 2014)\nCritic architecture\nMLP\nCritic learning rate\n1e-3\nCritic hidden layers\n1\nCritic hidden dim\n24\nCritic activation function\nReLU\nMini-batch size\n1024\nNumber of gradient steps\n50K\nDiscount factor\n0.99\nTarget update rate\n1\nTarget update period\n8\nLoss Function\nHuber Bellman Error\nTable 1: Hyperparameters for the DQN algorithm used in Section 3.\n.\nTo perform DQN we use an MIT-licensed implementation (github.com/mswang12/minDQN)\nwith hyperparameters as in Table 1. Code to reproduce our experiments is included in the supple-\nmentary materials.\n5\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "stat.ML"
  ],
  "published": "2023-12-15",
  "updated": "2023-12-18"
}