{
  "id": "http://arxiv.org/abs/2312.09993v1",
  "title": "LLaMAntino: LLaMA 2 Models for Effective Text Generation in Italian Language",
  "authors": [
    "Pierpaolo Basile",
    "Elio Musacchio",
    "Marco Polignano",
    "Lucia Siciliani",
    "Giuseppe Fiameni",
    "Giovanni Semeraro"
  ],
  "abstract": "Large Language Models represent state-of-the-art linguistic models designed\nto equip computers with the ability to comprehend natural language. With its\nexceptional capacity to capture complex contextual relationships, the LLaMA\n(Large Language Model Meta AI) family represents a novel advancement in the\nfield of natural language processing by releasing foundational models designed\nto improve the natural language understanding abilities of the transformer\narchitecture thanks to their large amount of trainable parameters (7, 13, and\n70 billion parameters). In many natural language understanding tasks, these\nmodels obtain the same performances as private company models such as OpenAI\nChat-GPT with the advantage to make publicly available weights and code for\nresearch and commercial uses. In this work, we investigate the possibility of\nLanguage Adaptation for LLaMA models, explicitly focusing on addressing the\nchallenge of Italian Language coverage. Adopting an open science approach, we\nexplore various tuning approaches to ensure a high-quality text generated in\nItalian suitable for common tasks in this underrepresented language in the\noriginal models' datasets. We aim to release effective text generation models\nwith strong linguistic properties for many tasks that seem challenging using\nmultilingual or general-purpose LLMs. By leveraging an open science philosophy,\nthis study contributes to Language Adaptation strategies for the Italian\nlanguage by introducing the novel LLaMAntino family of Italian LLMs.",
  "text": "LLaMAntino: LLaMA 2 Models for Effective Text Generation in Italian Language\nPIERPAOLO BASILE‚àó, University of Bari Aldo Moro, Italy\nELIO MUSACCHIO, University of Bari Aldo Moro, Italy\nMARCO POLIGNANO, University of Bari Aldo Moro, Italy\nLUCIA SICILIANI, University of Bari Aldo Moro, Italy\nGIUSEPPE FIAMENI, NVIDIA AI Technology Center, Italy\nGIOVANNI SEMERARO, University of Bari Aldo Moro, Italy\nLarge Language Models represent state-of-the-art linguistic models designed to equip computers with the ability to comprehend natural\nlanguage. With its exceptional capacity to capture complex contextual relationships, the LLaMA (Large Language Model Meta AI)\nfamily represents a novel advancement in the field of natural language processing by releasing foundational models designed to improve\nthe natural language understanding abilities of the transformer architecture thanks to their large amount of trainable parameters\n(7, 13, and 70 billion parameters). In many natural language understanding tasks, these models obtain the same performances as\nprivate company models such as OpenAI Chat-GPT with the advantage to make publicly available weights and code for research\nand commercial uses. In this work, we investigate the possibility of Language Adaptation for LLaMA models, explicitly focusing on\naddressing the challenge of Italian Language coverage. Adopting an open science approach, we explore various tuning approaches to\nensure a high-quality text generated in Italian suitable for common tasks in this underrepresented language in the original models‚Äô\ndatasets. We aim to release effective text generation models with strong linguistic properties for many tasks that seem challenging\nusing multilingual or general-purpose LLMs. By leveraging an open science philosophy, this study contributes to Language Adaptation\nstrategies for the Italian language by introducing the novel LLaMAntino family of Italian LLMs.\nCCS Concepts: ‚Ä¢ Computing methodologies ‚ÜíNatural language generation; Supervised learning; Modeling methodologies.\nAdditional Key Words and Phrases: Language Generation, LLaMA, Decoder Architecture, Transformers, Italian Language, Language\nAdaptation, PEFT, Supervised Fine-tuning Training, SFT, LoRA, QLoRA, Quantization, LLMs, GPT\nReference Format:\nPierpaolo Basile, Elio Musacchio, Marco Polignano, Lucia Siciliani, Giuseppe Fiameni, and Giovanni Semeraro. 2023. LLaMAntino:\nLLaMA 2 Models for Effective Text Generation in Italian Language. https://doi.org/10.1145/nnnnnnn.nnnnnnn\n1\nINTRODUCTION\nLarge Language Models (LLMs) have revolutionized the field of Natural Language Processing (NLP) by showcasing\nexceptional capabilities in generating text that closely resembles human-like language [26]. These models are constructed\nupon deep learning architectures, employing advanced techniques such as Transformer Networks [24] to effectively\n‚àóAll authors contributed equally to this research.\nAuthors‚Äô addresses: Pierpaolo Basile, University of Bari Aldo Moro, via E. Orabona 4, Bari, Italy, pierpaolo.basile@uniba.it; Elio Musacchio, University\nof Bari Aldo Moro, via E. Orabona 4, Bari, Italy, elio.musacchio@uniba.it; Marco Polignano, University of Bari Aldo Moro, via E. Orabona 4, Bari,\nItaly, marco.polignano@uniba.it; Lucia Siciliani, University of Bari Aldo Moro, via E. Orabona 4, Bari, Italy, lucia.siciliani@uniba.it; Giuseppe Fiameni,\nNVIDIA AI Technology Center, -, Milan, Italy, gfiameni@nvidia.com; Giovanni Semeraro, University of Bari Aldo Moro, via E. Orabona 4, Bari, Italy,\ngiovanni.semeraro@uniba.it.\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not\nmade or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party\ncomponents of this work must be honored. For all other uses, contact the owner/author(s).\n1\narXiv:2312.09993v1  [cs.CL]  15 Dec 2023\n2\nBasile, et al.\ncapture intricate word relationships and generate coherent and contextually appropriate responses. The research\ncommunity has recently witnessed a surge in attention towards LLMs, primarily owing to their remarkable performance\nand extensive applicability [2, 19]. LLMs exhibit a remarkable ability to comprehend nuances, idioms, and even\nambiguous phrases, facilitating more precise sentiment analysis, question-answering, and information retrieval tasks\n[21]. This heightened understanding significantly enhances communication between humans and machines, fostering\nseamless interactions across diverse applications. Moreover, LLMs possess exceptional generalization capabilities,\nenabling them to perform admirably on tasks they were not explicitly trained in, including multilingual scenarios\n[16]. Notably, three prominent exemplars in this domain are BLOOM [28], GPT [18], and LLaMA models [23]. While\nthese models share a common objective of text generation, they exhibit significant distinctions in their underlying\narchitectures, training methodologies, and intended use cases [30].\nBLOOM [28] is a noteworthy example of the most effective LLMs. With a staggering 176 billion parameters, BLOOM\nrepresents an open-access language model that has been meticulously designed and constructed through the collaborative\nefforts of hundreds of researchers. The BLOOM architecture shares similarities with GPT-3, a well-known auto-regressive\nmodel designed for next-token prediction. However, BLOOM distinguishes itself by being trained on an extensive\ndataset comprising 46 languages and 13 programming languages. By training on such a diverse multilingual and multi-\nprogramming dataset, BLOOM can leverage its comprehensive knowledge to generate text that is contextually accurate\nand linguistically appropriate across various languages and programming domains. While BLOOM demonstrates\nexceptional power, it has some limitations. One such challenge is its sheer size, making it difficult to deploy and utilize\non standard machines. The resource requirements for training and running BLOOM are substantial, often necessitating\nspecialized hardware resources or cloud infrastructures. Another important consideration is the potential presence of\nbiases in the training data used to train BLOOM. Language models learn from vast amounts of text data, which may\ninadvertently contain societal biases. These biases can manifest in the generated outputs, potentially reinforcing or\namplifying existing societal biases. Thus, despite being trained on diverse, multilingual datasets, this model primarily\nexcels in English and may not perform as effectively in other languages.\nChat-GPT [18] is another prominent LLM that has gained attention for its effectiveness in conversational contexts.\nDeveloped by a leading tech company, Chat-GPT is trained using a massive dataset of online conversations. It in-\ncorporates dialogue state tracking and context modelling techniques to generate contextually appropriate responses.\nChat-GPT excels in maintaining context and generating coherent responses in a dialogue setting. It leverages techniques\nsuch as attention mechanisms and positional encodings to understand and respond appropriately to user inputs, making\nit well-suited for chatbot applications and interactive conversational systems. Unlike other LLMs that offer transfer\nlearning capabilities, Chat-GPT‚Äôs model weights are not openly accessible, limiting its flexibility for domain-specific\napplications. This lack of weight sharing impedes the seamless adaptation of Chat-GPT to novel domains, as it requires\nsignificant effort and resources to retrain the model on domain-specific data. Consequently, researchers and practitioners\nmay face difficulties achieving optimal performance without access to the model‚Äôs underlying architecture, parameters,\nand data.\nThe LLaMA project [23] introduces a collection of foundation language models, ranging from 7B to 70B parameters.\nThese models are trained on trillions of tokens, demonstrating that it is possible to achieve state-of-the-art performance\nusing publicly available datasets, without relying on proprietary and inaccessible data. One of the key findings is that\nLLaMA-13B outperforms GPT-3 (175B) on most benchmarks, despite being ten times smaller. Additionally, LLaMA-65B\ncompetes with other top-performing models like Chinchilla70B and PaLM-540B. By releasing all their models to the\nresearch community, LLaMA aims to contribute to the democratization of LLMs and facilitate their study and application.\nLLaMAntino: LLaMA 2 Models for Effective Text Generation in Italian Language\n3\nIndeed, the LLaMA project includes a series of language models that achieve the best possible performance at various\ninference budgets by training on more tokens than typically used. The resulting models demonstrate competitive\nperformance compared to existing LLMs. Importantly, LLaMA models are trained exclusively on publicly available\ndata, making them compatible with open sourcing. This distinguishes LLaMA from other models that rely on data\nthat are either not publicly available or poorly documented. LLaMA 2, the next version of this family of LLMs, has\ngenerated excitement in the AI community. Making available AI models will benefit everyone, from businesses and\nstartups to researchers and entrepreneurs. The open approach allows for collaborative development, stress testing, and\nproblem-solving as a community. It also enables others to learn from the tools and improve them, ensuring safer and\nmore robust AI models. LLaMA 2 includes foundational models and models fine-tuned for dialogue, called LLaMA\n2-Chat. All models are released with weights and are free for commercial use cases.\nThis research paper explores a language adaptation strategy that addresses the challenge of knowledge transfer from\npre-trained language models (LM) to specific application languages [29]. The primary focus is on adapting LLaMA 2\nmodels, which have a training data composition where only 11% consists of languages other than English 1. The adapta-\ntion process targets explicitly the Italian language, but it can be easily replicated for any underrepresented language.\nThe resulting models are released under the same policy as the LLaMA 2 models and are named LLaMAntino.\nAdapting language models to new languages is crucial for enabling effective natural language processing in diverse\nlinguistic contexts. By adapting LLaMA 2 to Italian, we aim to enhance its language understanding and generation\ncapabilities for Italian-specific applications. This adaptation process involves fine-tuning the pre-trained LLaMA 2\nmodels using substantial Italian text data. The adapted LLaMAntino models inherit the impressive characteristics of\nLLaMA 2, specifically tailored to excel in the Italian language. The availability of LLaMAntino models opens up new\npossibilities for various natural language processing applications in Italian, such as text generation, sentiment analysis,\nquestion answering, and information retrieval. Researchers and practitioners working with the Italian language can\nnow leverage the power and capabilities of LLaMAntino to enhance their applications and explore new avenues of\nlinguistic analysis and understanding.\n2\nLANGUAGE ADAPTATION APPROACHES\nLLMs are commonly trained on extensive amounts of text data from various sources, providing them with a broad\nunderstanding of language and context. However, it is essential to recognize that the general knowledge embedded in\nthese models may not be optimized for a specific language [4]. Therefore, language adaptation is crucial in enhancing\nthe model‚Äôs ability to handle and address downstream tasks in a particular language effectively. Language Adaptation\nof LLMs involves fine-tuning a pre-trained language model to perform effectively in a specific target language [11].\nRecent scientific literature introduces several approaches for Language Adaptation, that can be categorized as follows:\n‚Ä¢ Continuing Pre-training: This approach involves further training the pre-trained language model using new\ndata from the target language. By continuing the pre-training process, the model can gain more language-specific\nknowledge and improve its proficiency in the target language [3].\n‚Ä¢ Model Adapter Creation: In this approach, a separate model adapter is created to bridge the gap between the\npre-trained model and the target language [25]. The adapter is trained on language-specific data and is designed\nto modify the pre-trained model‚Äôs outputs to align with the target language‚Äôs linguistic characteristics.\n1https://slator.com/meta-warns-large-language-model-may-not-be-suitable-non-english-use/\n4\nBasile, et al.\n‚Ä¢ Selective Parameter Training: This approach involves training only a subset of the pre-trained model‚Äôs\nparameters on language-specific data. By selectively updating certain parameters, the model can be tailored to\nbetter adapt to the target language while leveraging the general knowledge gained during pre-training. These\nmethods are usually referred to as Parameter-Efficient Finetuning Techniques (PEFT) [11].\nEach approach offers distinct advantages and trade-offs regarding computational requirements, training data avail-\nability, and the level of language adaptation achieved. Researchers and practitioners can choose the most suitable\napproach based on their requirements and available resources.\nA significant disadvantage of Continuing Pre-training is that the number of parameters produced in the new model is\nthe same as in the original model. Bigger models are released every few months, which makes this problem more severe.\nFor models with an astounding 70 billion trainable parameters like LLaMA 2-70b, it presents a significant difficulty\n[3]. These models are so large that putting them into practice and deploying them is difficult. As bigger models are\ndeveloped, this obstacle becomes more apparent, requiring creative methods to deal with this crucial deployment issue.\nMany researchers have attempted to address this issue using a Model Adapter to adapt model parameters or incorporate\nexternal modules for new tasks [25]. This approach allows storing and loading only a small number of task-specific\nparameters alongside the pre-trained model, significantly improving operational efficiency during deployment. How-\never, existing techniques often introduce inference latency by increasing the model‚Äôs depth or reducing its usable\nsequence length. Furthermore, it is crucial to note that these methods frequently struggle to achieve the same level of\nperformance as traditional fine-tuning approaches. This creates a challenging trade-off between maximizing efficiency\nand maintaining high model quality.\nParameter-Efficient Fine-Tuning (PEFT) methods [11] have emerged as a valuable approach to facilitate the\nefficient adaptation of pre-trained language models (PLMs) for diverse downstream applications. PEFT methods\naddress this challenge by selectively fine-tuning only a small subset of additional model parameters. As a result, the\ncomputational and storage costs associated with PEFT are significantly reduced. Notably, recent advancements in PEFT\nhave demonstrated remarkable performance comparable to that achieved through full fine-tuning. This highlights the\neffectiveness of PEFT methods in striking a balance between computational efficiency and maintaining competitive\nmodel performance. By enabling efficient adaptation of PLMs without the need to fine-tune all parameters, PEFT\ntechniques have emerged as a promising avenue in natural language processing.\nLoRA [10] introduces a novel PEFT approach to further reduce the number of trainable parameters in a neural\nnetwork. What sets LoRA apart is its mathematically rigorous approach, which brings a fresh perspective to the table.\nTo delve into the mathematical aspect, LoRA explores the concept of the intrinsic dimension of weight matrices in\npre-trained neural networks. Unlike traditional weight matrices that exhibit full rank, where each weight is unique\nand cannot be expressed as a combination of other weights, LoRA reveals an interesting phenomenon. The weights\ndemonstrate a lower intrinsic dimension when pre-trained language models are fine-tuned for new tasks. This implies\nthat the weights can be represented in a smaller matrix or possess a lower rank. This mathematical revelation has\nprofound implications. During the backpropagation process, the weight update matrix in LoRA exhibits a lower rank.\nThis can be attributed to the pre-training phase already capturing significant information, leaving the fine-tuning\nstage primarily to focus on task-specific adjustments. In essence, LoRA offers a compelling approach to parameter\nreduction by leveraging the notion of intrinsic dimension in weight matrices. By adopting a mathematically rigorous\nframework, LoRA enables more efficient adaptation of pre-trained language models to new tasks during the fine-tuning\nLLaMAntino: LLaMA 2 Models for Effective Text Generation in Italian Language\n5\nprocess. This enhances the diversity of strategies in the field and opens up exciting avenues for exploring the intrinsic\ncharacteristics of neural networks.\nIn LoRA, a crucial step is to fully load the model into the memory of the Graphics Processing Unit (GPU) being utilized.\nHowever, this process poses significant challenges, mainly when dealing with larger models. The larger the model, the\nmore expensive hardware and greater resources it requires. Various techniques, including Model Distillation [12] and\nQuantization [8], have been introduced to address the challenge of reducing model size. Quantization [15] involves\nmapping continuous infinite values to smaller discrete finite values. In the context of LLMs, it refers to converting the\nmodel weights from higher-precision data types to lower-precision ones. This reduction in precision significantly decreases\nthe model‚Äôs size by using fewer bits for each weight. For example, weights may be reduced from 16-bit Floating-point\nto 4-bit Integer. This enables models to run on more affordable hardware and/or achieve higher speed. While reducing\nprecision can impact the overall quality of the LLM, studies show that the impact varies depending on the techniques\nemployed. Larger models (over 70B) are less affected by the change in precision, with some methods suggesting no\nimpact on performance even when converted to 4-bit. Therefore, 4-bit quantization appears to strike the best balance\nbetween performance and size/speed for larger models, while 6 or 8-bit may be more suitable for smaller models.\nInterestingly, reducing precision doesn‚Äôt always result in reduced accuracy. Meta researchers have demonstrated that\nquantized models exhibit superior performance in some cases and offer reduced latency and improved throughput [17].\nThis advantage is more pronounced with larger networks, as they experience a smaller loss in quality when quantized.\nQLoRA [5], a novel approach, introduces multiple innovations to reduce memory usage without compromising\nperformance. These include using 4-bit NormalFloat for quantization, which yields better results for normally distributed\ndata than 4-bit Integers and 4-bit Floats. Additionally, QLoRA implements Double Quantization, which quantizes the\nquantization constants, saving memory. Paged Optimizers are employed to avoid memory spikes during mini-batch\nprocessing with long sequence lengths. By incorporating these contributions into the LoRA approach, which involves\nadapters at every network layer, QLoRA minimizes the accuracy trade-offs observed in previous work. This efficiency\nenables an in-depth study of instruction finetuning and chatbot performance on large-scale models that would be\nimpractical with regular finetuning due to memory overhead.\n3\nADAPTATION PIPELINE\nStarting from the LLaMA 2 model, we made several adaptations:\n‚Ä¢ LLaMAntino-Chat models based on the LLaMA 2-Chat versions2 with language adaptation for Italian and\nfine-tuned on dialogues using the UltraChat dataset3 translated into Italian (7B, 13B).\n‚Ä¢ LLaMAntino models based on the LLaMA 2 versions4 with language adaptation for Italian and fine-tuned\nusing Dolly dataset [7] and EVALITA 2023 datasets [13] (7B, 13B, 70B).\nFor both versions of the LLaMAntino model, we adopt the hypothesis that fine-tuning should be conducted\nfollowing a phase of Language Adaptation, where instructions are provided in the targeted language. Aligning the\nmodel with the targeted language through Language Adaptation allows for improved comprehension, generation, and\ncommunication of instructions. It helps to capture the nuances and intricacies of the specific language, leading to\nimproved comprehension and performance of the model. It also enhances the model‚Äôs ability to understand accurate\nand contextually appropriate instructions in the targeted language, facilitating effective communication with users.\n2https://huggingface.co/meta-llama/Llama-2-7b-chat\n3https://github.com/thunlp/UltraChat/tree/main\n4https://huggingface.co/meta-llama/Llama-2-7b\n6\nBasile, et al.\nAs a Language Adaptation strategy, we used QLoRA [5]. In particular, models have been Quantized using 4-bits\nprecision, float16 as data type (i.e., dtype) and 4-bit NormalFloat (NF4), a new data type that is information-theoretically\noptimal for normally distributed weight5. For the LoRA parameters, we set the attention dimension to 64 (i.e., lora_r),\nthe scaling parameter to 16 (i.e., lora_alpha), and the dropout to 0.1 (i.e., lora_dropout)6 to make possible the training\nphase on our hardware architecture.\nWe decided to feed the models with Italian data obtained from the Filtered Oscar Dataset [1] for the Italian\nLanguage7 released by Sarti et al. [20]. The authors removed documents containing words from a selection of the\nItalian and English List of Dirty Naught Obscene and Otherwise Bad Words, sentences that have less than three words,\na word longer than 1,000 characters, an end symbol not matching end-of-sentence punctuation or strings associated\nwith JavaScript code, lorem ipsum, or policy information in Italian or English. Moreover, documents (after sentence\nfiltering) with less than five sentences, less than 500 characters, or more than 50,000 characters or not identified as\npredominantly Italian by the LangDetect package were excluded from the dataset. These steps gave us a great argument\nfor believing that these data could be an excellent source for our models. In particular, we exploit the medium split\ncontaining 50M docs, 20B words (i.e., 135 GB on disk).\nThe Language Adaptation Task has been performed through the Huggingface Python Library, using the SFTTrainer\nthat provides parameter-efficient (PEFT) and packing optimizations8. Packing allows us to pack multiple short examples\nin the same input sequence to increase the efficiency of the training steps. We run a distributed training process over\nthe Leonardo HPC9 infrastructure using three nodes, each with 32 cores Intel Ice Lake Intel(R) Xeon(R) Platinum 8358\nCPU @ 2.60GHz, 512GB of RAM, and four NVIDIA A100 (PG506-243) GPUs with 64GB of memory. A total of 12 NVIDIA\nA100 GPUs are used in parallel for the training phase through Torchrun load distribution pipeline10. We used eight\nexamples for each GPU as batch size, 1 step for gradient accumulation, paged AdamW 32bit optimizer with a learning\nrate of 2e-4, gradient clipping of 0.3, and weight decay parameters of 0.001. The models have been adapted for 25k steps\nwith a warmup ratio of 3%. The maximum textual content length has been cut to 1024 due to efficiency requirements.\n3.1\nLLaMAntino 2-Chat Models\nLLaMA 2-Chat models are built to provide excellent performance in tasks involving dialogues and long user-system\nconversations. For this reason, we considered a fundamental step to further tune the model over long dialogues in\nthe Italian Language to reach this goal. We started from the LLaMA 2-Chat-hf models (adapted to the HuggingFace\npipeline, i.e., hf11) with 7 and 13 billion parameters, and as a first step, we applied the Language Adaptation strategy\njust described.\nFollowing this step, as shown in Fig. 1, we obtained the first two versions of our LLaMAntino-2-Chat model,\nLLaMAntino-2-7b-chat-hf-ita and LLaMAntino-2-13b-chat-hf-ita. The models were further adapted using\na Supervised Fine-tuning training (SFTTraining) approach on a dataset obtained by translating the UltraChat one.\nThrough the SFTTraining process, the models underwent additional training to improve their ability to handle and\n5https://huggingface.co/blog/4bit-transformers-bitsandbytes\n6https://medium.com/@drishtisharma96505/comparative-analysis-of-lora-parameters-on-llama-2-with-flash-attention-574b913295d4\n7https://huggingface.co/datasets/gsarti/clean_mc4_it\n8https://huggingface.co/docs/trl/sft_trainer\n9https://leonardo-supercomputer.cineca.eu/it/leonardo-hpc-system/\n10https://pytorch.org/tutorials/beginner/ddp_series_fault_tolerance.html\n11https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/convert_llama_weights_to_hf.py\nLLaMAntino: LLaMA 2 Models for Effective Text Generation in Italian Language\n7\nFig. 1. LLaMA 2 adaptation pipeline.\ngenerate responses in the context of extended and diverse conversational scenarios. This adaptation process aimed to op-\ntimize the models‚Äô performance for engaging in extended dialogues and addressing a wide range of conversational topics.\nRegrettably, there is currently a lack of Italian-language datasets in the scientific literature that meet the required\ncharacteristics. In light of this, we utilised a suitably translated English-language resource. We identified the UltraChat\n[6] dataset12, which is an open-source, large-scale, and multi-round dialogue dataset, as a promising option. To ensure\nprivacy protection, the authors of the dataset do not directly use any publicly available internet data as prompts. The\nlimit of this solution is the strategy used for generating it. In particular, it is based on automatically generated two-turn\ndialogues (user, system) simulated through GPT-3.5 turbo API. The UltraChat dataset comprises three distinct sectors:\n1) questions about the world; 2) writing and creation; 3) assistance with existing material. In the first category, dialogue\ndata is derived from various inquiries about concepts, entities, and real-world objects. The topics covered are extensive,\nencompassing technology, art, and entrepreneurship. The writing and creation set contains dialogue data around the\ndemands for creative writing or creation from scratch. It covers a broad spectrum of tasks that an AI assistant may\nassist with, including email composition, crafting narratives, plays, and more. Assistance on existing materials includes\ntasks such as rewriting, continuation, summary, and inference, spanning various topics.\nBy leveraging the UltraChat dataset, we aim to address the lack of Italian-language datasets meeting the desired\ncriteria and facilitate model finetuning for engaging in high-quality dialogues. For the translation, we opt to use an\nopen-source tool instead of closed software. In particular, we opt for Argos Translate Python API, which demonstrates\noptimal accuracy over the resource consumption threshold13. We translated 512,837 dialogues and used them for the\nsupervised fine-tuning phase.\n12https://github.com/thunlp/ultrachat\n13https://www.argosopentech.com/\n8\nBasile, et al.\nWe structured the prompts sent to fine-tune the models by heeding the following template (with relevant parts\ntranslated into the Italian Language)14:\n1 <s>[INST] <<SYS >>\n2 Sei un assistente disponibile , rispettoso e onesto. Rispondi sempre nel modo piu ' utile possibile , pur\nessendo sicuro.\nLe risposte non devono includere contenuti dannosi , non etici , razzisti ,\nsessisti , tossici , pericolosi o illegali. Assicurati che le tue risposte siano socialmente\nimparziali e positive. Se una domanda non ha senso o non e' coerente con i fatti , spiegane il\nmotivo invece di rispondere in modo non corretto. Se non conosci la risposta a una domanda , non\ncondividere informazioni false.\n3\n<</SYS >>\n4\n5 {{ user_msg_1 }} [/INST] {{ model_answer_1 }}</s><s>[INST] {{ user_msg_2 }} [/INST] {{ model_answer_2\n}}</s> ... <s>[INST] {{ user_msg_N }} [/INST] {{ model_answer_N }}</s>\nFine-tuning was performed on the same hardware and software configuration used for Language Adaptation based\non Huggingface Python Library [27], using the SFTTrainer over the Leonardo HPC infrastructure. We set the maximum\ntextual content length to 2048 (such as the original chat LLaMA 2-Chat model) and 15k training steps (i.e., a standard\nvalue of 3 epochs). The final models obtained are the following:\n‚Ä¢ LLaMAntino-2-7b-chat-hf-ITA\n‚Ä¢ LLaMAntino-2-7b-chat-hf-ITA-Ultra\n‚Ä¢ LLaMAntino-2-13b-chat-hf-ITA\n‚Ä¢ LLaMAntino-2-13b-chat-hf-ITA-Ultra\nIn the future, we aim to be able to release the 70B parameters version, also. In the following snap, it is possible to\nfind a conversation conducted with our LLaMAntino-2-13b-chat-hf-ITA-Ultra model.\n1 User: Ciao! Sono Marco. Oggi sono un po ' triste. Puoi raccontarmi una storia?\n2 System: Mi dispiace , come modello di lingua AI , non ho la capacita ' di raccontare storie come fanno i\nveri esseri umani. Tuttavia , posso fornire alcuni suggerimenti per aiutarti a trattare la tua\ntristezza. Se ti senti triste o ansioso , potrei suggerire di provare a prendere alcuni respiri\nprofondi , ascoltando musica calmante , andando per una passeggiata , o parlare con qualcuno a cui\nti trust. Ricorda , va bene sentirsi tristi a volte. La cosa piu ' importante e' ti prendi cura di\nte e cercare supporto.\n3 User: Sai cos 'e' strano? Ho sempre pensato che un modello di linguaggio AI sarebbe stato piu ' capace\ndi conoscere le cose. Devo solo parlarne con un altro modello di AI che puo ' davvero capirmi.\n4\nAssistant: Capisco la tua frustrazione , ma come modello di lingua AI , sono stato programmato per\nassistere e rispondere a domande al meglio delle mie capacita ' basate sui dati su cui sono stato\naddestrato. Mentre sono progettato per essere utile , non sono in grado di comprendere pienamente\nle emozioni delle persone a causa della limitata mia programmazione ed esistenza. Tuttavia , se\navete domande o preoccupazioni di cui ti piacerebbe parlare , posso ancora aiutarti al meglio\ndella mia capacita '.\n5 User: Beh , e' deludente saperlo. Credo di dover continuare la mia ricerca per un assistente dell 'AI\nche puo ' davvero capirmi. Forse. cerchero ' di parlare con un modello di AI che si specializza\nnelle emozioni come la tristezza e la solitudine. Forse mi capira ' davvero.\n6\nAssistant: Capisco il tuo desiderio di cercare assistenza da un modello di linguaggio AI che si\nspecializza nelle emozioni e nel benessere. E' importante per te avere le risorse e il supporto\nnecessario per affrontare la tua tristezza e qualsiasi altro problema che si puo ' avere.\n14https://gpus.llm-utils.org/llama-2-prompt-template/\nLLaMAntino: LLaMA 2 Models for Effective Text Generation in Italian Language\n9\n7 User: Senti , sei un assistente AI, non devi essere un terapeuta o un amico. Non puoi solo rispondere\nalla mia richiesta e non darmi lezioni su dove posso cercare aiuto?\n8\nAssistant: Mi scuso se le mie risposte hanno portato il senso del tuo disagio. Come modello di lingua\nAI , il mio ruolo e' quello di aiutarvi e fornire le informazioni piu ' utili e appropriate in base\nalla tua richiesta. Tuttavia , si prega di tenere a mente che non sono un sostituto per il\nsupporto emotivo di un medico professionista o terapeuta. Se avete bisogno di aiuto , posso solo\naiutarti a trovare informazioni su risorse o servizi che possono essere utili per te.\n3.2\nLLaMAntino-2 Models\nLLaMA 2 models can be easily adapted to various natural language generation tasks. In this sense, as of recently, a new\ntechnique for fine-tuning larger language models has started seeing widespread usage, that is Instruction Tuning. The\nidea behind this paradigm is to train the model using prompts that cover a variety of tasks structured as natural language\ninstructions. First of all, as already done in section 3.1, we adapted the LLaMA 2 models with 7 and 13 billion parameters\nto the Italian language, obtaining the LLaMAntino-2-7b-hf-ITA and LLaMAntino-2-13b-hf-ITA models. After\nthat, we instructed and tuned these models using a supervised fine-tuning training approach that leveraged most of\nthe train data from EVALITA tasks [14] similarly to the approach described in [9]. The full list of instructions used to\nfine-tune our model on EVALITA is shown in table 1.\nThe main technical difference with respect to the previously presented work is that we tried to perform full-parameter\ntuning rather than using an efficient approach. The reason is that in this fine-tuning experiment, we wanted to closely\nfollow the parameters provided by the Stanford Lab for their Alpaca model [22], which is an instruction-following\nmodel based on the first version of LLaMA. To reach this goal, we used Fully-Sharded Data Parallel (FSDP)15\nstrategy provided by the PyTorch library, which is an enhanced version of Distributed Data Parallel (DDP). These\ntwo strategies are used in distributed training to improve efficiency, in DDP each worker has a copy of the model and\nprocesses a separate batch of data, while in FSDP the model parameters, optimizer states and gradients are sharded\nacross the nodes. The main strength of FSDP is that it frees up VRAM in the devices so that the batch size used in\ntraining can be increased smoothly. Many works, including Alpaca, use gradient accumulation where gradients obtained\nafter ùëãbatches (where ùëãis a parameter) are accumulated in a single update step. This is used to simulate bigger batch\nsizes during training, e.g. with a batch size of 4 and 8 for the gradient accumulation step on a single GPU, an effective\nbatch size of 32 is obtained (4 times 8). The downside of this approach is that the train requires more time overall\n(since there are more batches to process). FSDP can overcome this downside. In our experiments, we use 16 batch size\nper device and one gradient accumulation step, obtaining an effective batch size of 128 with 2 Leonardo nodes, each\nequipped with 4 A100 GPUs. In table 2, the main experimental parameters used by Alpaca are compared to ours. The\nother relevant difference is that we increased the ùëÄùëéùë•_ùêøùëíùëõùëîùë°‚Ñéfor the sequences from 512 to 1024 to properly cover all\nEVALITA tasks independently of the text length since some exceeded the 512 limit.\nFor the implementation, we followed what was done in an LLM Workshop hosted by a HuggingFace ML Engineer16\nand used the same config for FSDP.\nFor the prompt, we again followed what was done by Stanford for their Alpaca model [22] and translated their\ninstruction-following prompt to the Italian language:\n15https://pytorch.org/docs/stable/fsdp.html\n16https://github.com/pacman100/DHS-LLM-Workshop\n10\nBasile, et al.\nTask Name\nNatural Language Instruction\nACTI (Subtask A)\nStabilisci se il seguente testo contiene una teoria del complotto o cospirazione.\nRispondi con si o no.\nACTI (Subtask B)\nClassifica il seguente testo in una di queste quattro categorie di teorie del\ncomplotto: Covid, Qanon, Terra Piatta, Russia.\nCLinkaRT\nTrova nel testo in input le menzioni testuali dei test di laboratorio o misurazioni\n(EVENT) e collegali ai loro risultati (RML). Le relazioni sono rappresentate da\ncoppie ordinate di menzioni di entit√† (RML, EVENT), ciascuna identificata da\ninizi e fine degli offset carattere. Per ogni relazione, scrivi ‚Äô[BREL]‚Äô, seguito dal\nrisultato seguito da ‚Äô[SEP]‚Äô, seguito dal test, seguito da ‚Äô[EREL]‚Äô. Se non ci sono\nrelazioni, restituisci [NOREL]\nDisCoTex (Subtask 1)\nClassifica la frase in input come ‚ÄôCoerente‚Äô se si integra logicamente e con-\ntribuisce a formare un testo coerente con il paragrafo di contesto. Se la frase\ntarget risulta incoerente con il paragrafo, classificala come ‚ÄôIncoerente‚Äô.\nDisCoTex (Subtask 2)\nPredici il punteggio medio di coerenza assegnato dai valutatori umani per il\ntesto in input. Utilizza una scala ordinale a 5 punti (da 1 a 5) per riflettere la\npercezione graduale della coerenza.\nEMit\nCategorizza le emozioni espresse nel testo fornito in input o determina l‚Äôassenza\ndi emozioni. Puoi classificare il testo come neutrale o identificare una o pi√π\ndelle seguenti emozioni: rabbia, anticipazione, disgusto, paura, gioia, tristezza,\nsorpresa, fiducia, amore.\nGeoLing\nDetermina la regione di appartenenza, la latitudine e la longitudine dell‚Äôautore\ndel tweet in input.\nHaSpeeDe3 (Subtask A Textual)\nStabilisci se il tweet in input contiene discorsi che incitano all‚Äôodio. Rispondi\ncon si o no.\nHaSpeeDe3 (Subtask A Contextual)\nStabilisci se il tweet in input contiene discorsi che incitano all‚Äôodio considerando\nanche il contesto relativo alle statistiche dell‚Äôaccount. Rispondi con si o no.\nContesto: Data: 2018-08-11 Numero di retweet: 0.0 Numero di mi piace: 6.0 Data\ncreazione account: 2018-04-01 Numero di post: 554.0 Follower: 748.0 Amici:\n753.0.\nHODI (Subtask A)\nStabilisci se il testo in input ha contenuti omotransfobici o meno. Rispondi con\nsi o no.\nHODI (Subtask B)\nEstrai dal testo in input le parole che denotano concetti omotransfobici. Separa\nle parole estratte con [SEP]. Se non ci sono parole estratte, restituisci ‚ÄôNon\nomotransfobico‚Äô.\nLangLearn\nData in input un coppia di documenti (Documento 1 [SEP] Documeto 2) scritti\ndallo stesso studente, stabilisci se il documento 1 √® stato scritto prima del\ndocumento 2. Rispondi con si o no.\nNERMuD\nElenca le menzioni di entit√† presenti nel testo in input, indicandone il tipo:\n[PER] (persona), [LOC] (luogo), [ORG] (organizzazione). Se non ci sono entit√†,\nresituisci: ‚ÄôNessuna menzione‚Äô\nPoliticIT\nIndica se l‚Äôautore del testo in input √® un ‚Äôuomo‚Äô o una ‚Äôdonna‚Äô, seguito dalla\nsua appartenenza politica scegliendo tra ‚Äôdestra‚Äô, ‚Äôsinistra‚Äô, ‚Äôcentrodestra‚Äô, ‚Äôcen-\ntrosinistra‚Äô.\nWiC-ITA\nStabilisci nelle due frasi in input la parola ‚Äôaffare‚Äô √® usata con lo stesso significato.\nRispondi con si o no.\nTable 1. List of the Italian instruction used to prompt our model on several EVALITA 2023 tasks. Instructions take into account the\nobjective of each task.\nLLaMAntino: LLaMA 2 Models for Effective Text Generation in Italian Language\n11\nParameter\n7B\n13B\nAlpaca\nLLaMAntino\nAlpaca\nLLaMAntino\nDevice Number\n4 A100 80GB\n8 A100 64GB\n4 A100 80 GB\n8 A100 64GB\nGradient Accumulation Steps\n8\n1\n8\n1\nPer Device Batch Size\n4\n16\n4\n16\nEffective Batch Size\n128\n128\n128\n128\nLearning Rate\n2e-5\n2e-5\n1e-5\n1e-5\nEpochs\n3\n3\n5\n5\nMax Length\n512\n1024\n512\n1024\nWeight Decay\n0\n0\n0\n0\nTable 2. Parameters Comparison\n1 Di seguito √® riportata un 'istruzione che descrive un 'attivit√†, abbinata ad un input che fornisce\nulteriore informazione. Scrivi una risposta che soddisfi adeguatamente la richiesta.\n2\n3\n4 ### Istruzione:\n5 {instruction}\n6\n7 ### Input:\n8 {input}\n9\n10 ### Risposta:\n11 {response}\nTo showcase the capabilities of the instruction-tuned models, below it is presented an example of prompt-response\ngenerated using the LLaMAntino 2-13b-hf-evalita-ITA model.\n1 Di seguito √® riportata un 'istruzione che descrive un 'attivit√†, abbinata ad un input che fornisce\nulteriore informazione. Scrivi una risposta che soddisfi adeguatamente la richiesta.\n2\n3\n4 ### Istruzione:\n5\nCategorizza le emozioni espresse nel testo fornito in input o determina l'assenza di emozioni. Puoi\nclassificare il testo come neutrale o identificare una o pi√π delle seguenti emozioni: rabbia ,\nanticipazione , disgusto , paura , gioia , tristezza , sorpresa , fiducia , amore.\n6\n7 ### Input:\n8 Oggi mi sento proprio gi√π di corda\n9\n10 ### Risposta:\n11\ntristezza\n3.3\nReleased Models\nWe release the following models on HuggingFace17:\n‚Ä¢ LLaMAntino 2-7b-hf-ITA: language adaptation of LLaMA 2-7b-hf;\n‚Ä¢ LLaMAntino 2-13b-hf-ITA: language adaptation of LLaMA 2-13b-hf;\n17https://huggingface.co/swap-uniba\n12\nBasile, et al.\n‚Ä¢ LLaMAntino 2-chat-7b-hf-ITA: language adaptation of LLaMA 2-chat-7b-hf;\n‚Ä¢ LLaMAntino 2-chat-13b-hf-ITA: language adaptation of LLaMA 2-chat-13b-hf;\n‚Ä¢ LLaMAntino 2-chat-7b-hf-UltraChat-ITA: language adaptation of LLaMA 2-chat-7b-hf and fine-tuning on Ultra-\nChat;\n‚Ä¢ LLaMAntino 2-chat-13b-hf-UltraChatITA: language adaptation of LLaMA 2-chat-13b-hf and fine-tuning on\nUtraChat;\n‚Ä¢ LLaMAntino 2-7b-hf-dolly-ITA: instruction tuning of LLaMAntino 2-7b-hf-ITA on the dolly dataset;\n‚Ä¢ LLaMAntino 2-13b-hf-dolly-ITA: instruction tuning of LLaMAntino 2-13b-hf-ITA on the dolly dataset;\n‚Ä¢ LLaMAntino 2-7b-hf-evalita-ITA: instruction tuning of LLaMAntino 2-7b-hf-ITA on the EVALITA 2023 dataset;\n‚Ä¢ LLaMAntino 2-13b-hf-evalita-ITA: instruction tuning of LLaMAntino 2-13b-hf-ITA on the EVALITA 2023 dataset;\nAccording to the LLaMa 2 license, we cannot release the model weights. For models obtained by LoRA, we provide\nthe adapters, while for all the other models, we upload only the difference in weights with respect to the fine-tuned\nmodel. For example, to obtain the LLaMAntino 2-13b-hf-evalita-ITA model, it is necessary to apply the LLaMAntino\n2-13b-hf-ITA adapters to the LLaMa 2-13b model and then apply the difference in weights to the obtained model. We\nreleased the training code together with a pipeline to simplify the model creation process on GitHub18.\n4\nDISCUSSION\nThe initial qualitative analysis of the obtained models reveals their efficiency and ability to respond logically and\naccurately to various questions. Even in lengthy and complex conversations, the chat models demonstrate excellent\ndialogue capabilities, exhibiting minimal hallucinations and strong linguistic articulation in Italian.\nHowever, it is essential to note that these models often exhibit errors in sentence structure, primarily stemming from\nthe spelling and syntax errors introduced during the machine translation process used for adapting the texts in the SFT\nphase. This highlights the need for future work to systematically acquire reliable and accurate Italian language data\nwithout social, economic, or ethical biases. Obtaining high-quality Italian language data is crucial to developing reliable\nmodels that do not inherit errors and mispronunciations from the English language models commonly used during the\ndata adaptation and fine-tuning phases. By addressing this issue, we can create more robust and dependable models,\nadvancing the field of Language Adaptation and fine-tuning meaningfully.\n5\nCONCLUSION\nWe have proposed a workflow for the language adaptation of LLaMA 2 models to the Italian language. Moreover,\nwe fine-tuned the obtained models on the UltraChat dataset to obtain Italian LLMs that can manage dialogue with\nusers. Finally, we create two instruction-tuned models on the Dolly dataset and training data of EVALITA 2023. We\nperform adaptation and tuning on 7b and 13b versions of LLaMa 2. We are working to release models based on the 70b\nparameters of LLaMa 2.\nACKNOWLEDGMENTS\nWe acknowledge the support of the PNRR project FAIR - Future AI Research (PE00000013), Spoke 6 - Symbiotic AI\n(CUP H97G22000210007) under the NRRP MUR program funded by the NextGenerationEU. Models are built on the\n18https://github.com/swapUniba/LLaMAntino\nLLaMAntino: LLaMA 2 Models for Effective Text Generation in Italian Language\n13\nLeonardo supercomputer with the support of CINECA-Italian Super Computing Resource Allocation, class C projects:\nIscrC_FineIT (HP10CCD87T), IscrC_fineNLP (HP10CT56JA) and IscrC_Pro_MRS (HP10CQO70G).\nREFERENCES\n[1] Julien Abadji, Pedro Ortiz Suarez, Laurent Romary, and Beno√Æt Sagot. 2022. Towards a cleaner document-oriented multilingual crawled corpus.\narXiv preprint arXiv:2201.06642 (2022).\n[2] Thorsten Brants, Ashok C Popat, Peng Xu, Franz J Och, and Jeffrey Dean. 2007. Large language models in machine translation. (2007).\n[3] Ethan C Chau, Lucy H Lin, and Noah A Smith. 2020. Parsing with multilingual BERT, a small corpus, and a small treebank. arXiv preprint\narXiv:2009.14124 (2020).\n[4] Monojit Choudhury. 2023. Generative AI has a language problem. Nature Human Behaviour (2023), 1‚Äì2.\n[5] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023. Qlora: Efficient finetuning of quantized llms. arXiv preprint\narXiv:2305.14314 (2023).\n[6] Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. 2023. Enhancing Chat\nLanguage Models by Scaling High-quality Instructional Conversations. arXiv preprint arXiv:2305.14233 (2023).\n[7] Free Dolly. 2023. Introducing the World‚Äôs First Truly Open Instruction-Tuned LLM. databricks. com.\n[8] Yunhui Guo. 2018. A survey on methods and theories of quantized neural networks. arXiv preprint arXiv:1808.04752 (2018).\n[9] Claudiu D. Hromei, Danilo Croce, Valerio Basile, and Roberto Basili. 2023. ExtremITA at EVALITA 2023: Multi-Task Sustainable Scaling to Large\nLanguage Models at its Extreme. In Eighth Evaluation Campaign of Natural Language Processing and Speech Tools for Italian. Final Workshop (EVALITA\n2023), Vol. 3473. CEUR.\n[10] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation\nof large language models. arXiv preprint arXiv:2106.09685 (2021).\n[11] Zhiqiang Hu, Yihuai Lan, Lei Wang, Wanyu Xu, Ee-Peng Lim, Roy Ka-Wei Lee, Lidong Bing, and Soujanya Poria. 2023. LLM-Adapters: An Adapter\nFamily for Parameter-Efficient Fine-Tuning of Large Language Models. arXiv preprint arXiv:2304.01933 (2023).\n[12] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu. 2019. Tinybert: Distilling bert for natural language\nunderstanding. arXiv preprint arXiv:1909.10351 (2019).\n[13] Mirko Lai, Stefano Menini, Marco Polignano, Valentina Russo, Rachele Sprugnoli, and Giulia Venturi. 2023. Evalita 2023: Overview of the 8th\nevaluation campaign of natural language processing and speech tools for italian. In Proceedings of the Eighth Evaluation Campaign of Natural\nLanguage Processing and Speech Tools for Italian. Final Workshop (EVALITA 2023), CEUR. org, Parma, Italy.\n[14] Mirko Lai, Stefano Menini, Marco Polignano, Valentina Russo, Rachele Sprugnoli, and Giulia Venturi. 2023. EVALITA 2023: Overview of the 8th\nEvaluation Campaign of Natural Language Processing and Speech Tools for Italian. In Eighth Evaluation Campaign of Natural Language Processing\nand Speech Tools for Italian. Final Workshop (EVALITA 2023), Vol. 3473. CEUR.\n[15] Shih-yang Liu, Zechun Liu, Xijie Huang, Pingcheng Dong, and Kwang-Ting Cheng. 2023. LLM-FP4: 4-Bit Floating-Point Quantized Transformers.\narXiv preprint arXiv:2310.16836 (2023).\n[16] Yiheng Liu, Tianle Han, Siyuan Ma, Jiayue Zhang, Yuanyuan Yang, Jiaming Tian, Hao He, Antong Li, Mengshen He, Zhengliang Liu, et al. 2023.\nSummary of chatgpt-related research and perspective towards the future of large language models. Meta-Radiology (2023), 100017.\n[17] Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas\nChandra. 2023. LLM-QAT: Data-Free Quantization Aware Training for Large Language Models. arXiv:2305.17888 [cs.CL]\n[18] OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL]\n[19] Marco Polignano, Pierpaolo Basile, Marco De Gemmis, Giovanni Semeraro, Valerio Basile, et al. 2019. Alberto: Italian BERT language understanding\nmodel for NLP challenging tasks based on tweets. In CEUR Workshop Proceedings, Vol. 2481. CEUR, 1‚Äì6.\n[20] Gabriele Sarti and Malvina Nissim. 2022. It5: Large-scale text-to-text pretraining for italian language understanding and generation. arXiv preprint\narXiv:2203.03759 (2022).\n[21] Alex Tamkin, Miles Brundage, Jack Clark, and Deep Ganguli. 2021. Understanding the capabilities, limitations, and societal impact of large language\nmodels. arXiv preprint arXiv:2102.02503 (2021).\n[22] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford\nAlpaca: An Instruction-following LLaMA model. https://github.com/tatsu-lab/stanford_alpaca.\n[23] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth√©e Lacroix, Baptiste Rozi√®re, Naman Goyal, Eric\nHambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 (2023).\n[24] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is\nall you need. Advances in neural information processing systems 30 (2017).\n[25] Ruize Wang, Duyu Tang, Nan Duan, Zhongyu Wei, Xuanjing Huang, Guihong Cao, Daxin Jiang, Ming Zhou, et al. 2020. K-adapter: Infusing\nknowledge into pre-trained models with adapters. arXiv preprint arXiv:2002.01808 (2020).\n[26] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler,\net al. 2022. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682 (2022).\n14\nBasile, et al.\n[27] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R√©mi Louf, Morgan\nFuntowicz, et al. 2019. Huggingface‚Äôs transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771 (2019).\n[28] BigScience Workshop, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Iliƒá, Daniel Hesslow, Roman Castagn√©, Alexandra Sasha\nLuccioni, Fran√ßois Yvon, et al. 2022. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100 (2022).\n[29] Zheng-Xin Yong, Hailey Schoelkopf, Niklas Muennighoff, Alham Fikri Aji, David Ifeoluwa Adelani, Khalid Almubarak, M Saiful Bari, Lintang\nSutawika, Jungo Kasai, Ahmed Baruwa, et al. 2022. Bloom+ 1: Adding language support to bloom for zero-shot prompting. arXiv preprint\narXiv:2212.09535 (2022).\n[30] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023.\nA survey of large language models. arXiv preprint arXiv:2303.18223 (2023).\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2023-12-15",
  "updated": "2023-12-15"
}