{
  "id": "http://arxiv.org/abs/2312.10884v1",
  "title": "Contextual Reinforcement Learning for Offshore Wind Farm Bidding",
  "authors": [
    "David Cole",
    "Himanshu Sharma",
    "Wei Wang"
  ],
  "abstract": "We propose a framework for applying reinforcement learning to contextual\ntwo-stage stochastic optimization and apply this framework to the problem of\nenergy market bidding of an off-shore wind farm. Reinforcement learning could\npotentially be used to learn close to optimal solutions for first stage\nvariables of a two-stage stochastic program under different contexts. Under the\nproposed framework, these solutions would be learned without having to solve\nthe full two-stage stochastic program. We present initial results of training\nusing the DDPG algorithm and present intended future steps to improve\nperformance.",
  "text": "Contextual Reinforcement Learning for Offshore\nWind Farm Bidding\nDavid L. Cole\nDepartment of Chemical\nand Biological Engineering\nUniversity of Wisconsin-Madison\nMadison, WI 53706\ndlcole3@wisc.edu\nHimanshu Sharma\nElectricity Infrastructure\nand Buildings Division\nPacific Northwest National Laboratory\nRichland, WA 99354\nhimanshu.sharma@pnnl.gov\nCorresponding Author\nWei Wang\nElectricity Infrastructure\nand Buildings Division\nPacific Northwest National Laboratory\nRichland, WA 99354\nw.wang@pnnl.gov\nAbstract\nWe propose a framework for applying reinforcement learning to contextual two-\nstage stochastic optimization and apply this framework to the problem of energy\nmarket bidding of an off-shore wind farm. Reinforcement learning could potentially\nbe used to learn close to optimal solutions for first stage variables of a two-stage\nstochastic program under different contexts. Under the proposed framework, these\nsolutions would be learned without having to solve the full two-stage stochastic\nprogram. We present initial results of training using the DDPG algorithm and\npresent intended future steps to improve performance.\n1\nIntroduction\nPower systems are a major contributor to climate change because energy production often results in\nsignificant greenhouse gas emissions. To combat this challenge, many efforts are being focused on\nhow to integrate, manage, and plan renewable energy (RE) use in the power grid [1, 2, 3]. However,\nthese RE systems (such as wind or solar) can create new difficulties. The energy production of these\nsystems is often stochastic (e.g., there is uncertainty around when or how much the wind will blow).\nThese RE systems also must operate within energy markets which can vary by region and have\nstochastic prices. Regions typically have two or more markets, often including a long-term–often\nday-ahead (DA)–market where energy is bid and committed ahead of time, and a short-term, real-time\n(RT) market, where energy is bid and committed in real time. Markets can compound the challenge\nof uncertainty, as energy may need to be committed ahead of time without knowing the exact final\nproduction of the RE system. Further, power electronic devices (PELs) can greatly influence the\noperation dynamics in the grid. For example, installing battery storage with a wind farm (WF) can\ninfluence how much energy can be committed at different times. To make these RE systems more\nproductive and resilient in the grid, many algorithms, models, and tools are being designed to help\nmake decisions for RE systems in energy markets under different sources of uncertainty [4].\nTwo-stage stochastic programs (SP) are a common approach for decision making under uncertainty\nand has been used in many different studies involving RE (see for example, [5, 6, 7]). In two-stage\nPreprint. Under review.\narXiv:2312.10884v1  [eess.SY]  18 Dec 2023\nSP, there are a set of first stage (primary) decision variables which must be decided first and a set\nof second stage (recourse) decision variables that are decided after the realization of the first stage\nvariables. Two-stage SP often considers a large set of scenarios that contain realizations of the\nuncertain problem data. In this way, the first stage variables are the same across all scenarios, while\nthe second stage variables can vary between scenarios. Scalability is a key challenge of two-stage SP\nas a large number of scenarios may be necessary for finding meaningful solutions, and the problem\nsize can increase significantly with more scenarios. To combat this challenge, there are a variety of\nworks that seek to simplify or speed up solution of two-stage SP, such as scenario reduction [8, 9],\ndecomposition techniques [10, 11], or through machine learning [12, 13].\nIn this work, we propose using reinforcement learning (RL) to learn the first stage solution of a\ncontextual 2-stage SP for the day-ahead bidding strategy of an offshore wind farm (WF). Contextual\n2-stage SP differs from 2-stage SP in a sense that there is \"context\" or data that influences the optimal\nsolution but is not a decision variable in the problem (e.g., objective function coefficients). This is\nmotivated in part by the work of Nair et al. [14] who used RL to train a policy to choose first stage\nsolutions of a contextual two-stage SP. However, in their work, all first stage variables were binary\nwhile the problem we consider requires continuous first stage variables. Yilmaz and Büyüktahtakin\n[15] also used RL for 2-stage SP, but they did not incorporate context and used a different setup than\nwe present below. In addition, there are a number of works that use RL for bidding strategies of WFs\noutside of SP problems [16, 17, 18]. To our knowledge, RL has not been used for learning solutions\nof contextual 2-stage SP with continuous first stage variables and has not been applied to the offshore\nWF use case. We present herein our preliminary efforts to build an RL agent and show initial results\nthat suggest that the RL agent starts to learn better actions it should take, but more work is needed to\ndevelop a reliable RL agent.\n2\nMethods\nThe two-stage SP we consider is adapted from [19] where we consider an offshore WF with a battery\nfor storing electricity that can commit to a DA or RT market as shown in (1).\nmax\nT\nX\nt\nλDA\nt\nP DA\nt\n+\nΩ\nX\nω\npω\nT\nX\nt\n\u0000λRT\nt,ω P RT\nt,ω −λop\nt,ωP op\nt,ω −λup\nt,ωP up\nt,ω\n\u0001\n(1a)\ns.t. Gt,ω = P DA\nt\n+ P RT\nt,ω + P op\nt,ω −P up\nt,ω + P ch\nt,ω −P dis\nt,ω\n∀t ∈T , ω ∈Ω\n(1b)\n0 ≤P DA\nt\n, P RT\nt,ω , P op\nt,ω, P up\nt,ω,\nEmin ≤Et,ω ≤Emax\n∀t ∈T , ω ∈Ω\n(1c)\n0 ≤P ch\nt,ω ≤¯P ch,\n0 ≤P dis\nt,ω ≤¯P dis\n∀t ∈T , ω ∈Ω\n(1d)\nEt,ω −Eω,t−1 = ηchP ch\nt,ω −ηdisP dis\nt,ω\n∀t ∈T /{1, 2}, ω ∈Ω\n(1e)\nE2,ω −E1 = ηchP ch\nt,ω −ηdisP dis\nt,ω\n∀ω ∈Ω\n(1f)\nE|T |,ω ≥Efinal,\n∀ω ∈Ω\n(1g)\nHere, T is the set of time points in the time horizon (24 hours) and Ωis the set of scenarios. Decision\nvariables for time t and scenario ω include the power committed to the DA and RT markets (P DA\nt\nand\nP RT\nt\n), the amounts of under-produced or over-produced power (P up\nt,ω and P op\nt,ω), the power charged\nor discharged from the battery (P ch\nt,ω and P dis\nt,ω ), and the current energy level of the battery (Et,ω).\nParameters include the DA and RT market prices (λDA\nt\nand λRT\nt\n), the cost to buy power when under-\nproducing (λup\nt,ω), the cost to curtail power when overproducing (λop\nt,ω), the total energy produced by\nthe WF in each scenario (Gt,ω), maximum charge or discharge levels ( ¯P ch and ¯P dis), charge and\ndischarge efficiencies (ηch and ηdis), minimum or maximum battery energy levels (Emin and Emax),\nthe initial battery energy level (E1), and a bound on the final battery energy level (Efinal).\nThe RL framework we use is visualized in Figure 1. Under this framework, we use RL to predict the\nfirst stage variables ({P DA\nt\n}t∈T ) based on {λDA\nt\n}t∈T , E1, forecasts of wind production ({Gt}t∈T )\nand RT prices ({λRT\nt\n}t∈T ), and based on problem context (Efinal, Emax, ηch, and ηdis). We use\nan actor-critic agent trained using a DDPG algorithm [20] (we also tried the PPO [21] algorithm,\nbut initial performance was better with DDPG). Under this framework, a single step consists of the\nagent observing the environment ({λDA\nt\n, Gt, λRT\nt\n, E1, Efinal, Emax, ηch, ηdis}t∈T ) and predicting\nan action ({P DA\nt\n}t∈T ) based on that observation. The action is then passed to the environment, which\n2\nFigure 1: One step of the RL framework for learning the first stage variables. The agent observes\nthe environment, chooses an action, and a reward is computed by solving the second stage of the\ntwo-stage SP.\nreturns a reward to the agent. Building and training the agent were done using Stable-baselines3\n[22]. While we apply this framework to a specific two-stage SP, the general approach can be extended\nto other two-stage SPs.\nThe environment consisted of real price and wind production data, probability distributions relating\nto the data, and battery characteristics (context). RT and DA price data were for the northeastern\nUnited states retrieved from the New England ISO [23], and the wind production data came from\n[24] (wind and price data came from different locations because we are most interested in a proof of\nconcept for this work). Battery characteristics were randomly generated for each new episode. To\ncompute the reward, the environment solves (1) with {P DA\nt\n}t∈T fixed to be the action of the agent.\nThe optimal value is then divided by\n\u0000P\nω∈Ω\nP\nt∈T Gt,ω max\n\u0000λDA\nt\n, λRT\nt,ω\n\u0001\u0001\n/|Ω| and returned to the\nagent as the reward (this is effectively a \"normalized\" reward to account for differences between wind\nforecasts on different days). For (1), the environment computes Ωby sampling from distributions\nof noise for the RT price data and wind speed data and adding noise to the forecasts to generate\nunique scenarios (we use |Ω| = 10 and pω = 0.1, ∀ω ∈Ω). To compute the noise distributions, we\nfit an auto-regressive moving average (ARMA) model to the RT price data and the wind speed data\n(speeds are converted to power using a power curve), and the residuals (error) in the models were\nfit to a probability distribution (see [25] and [26]). For RT price data, we used p = 5 and q = 2,\nand for wind speed data, we used p = 3 and q = 0. Lastly, to simplify model training, we set the\naction space to be [0, 1]|T |. The actions are then multiplied by the forecasted wind power (forcing\n0 ≤P DA\nt\n≤Gt, ∀t ∈T ). Thus, the agent gives {P DA\nt\n}t∈T by returning a fraction of the wind\nforecast to commit to the DA market. This theoretically simplifies model training as the model cannot\nmajorly over-commit to the DA market.\n3\nResults and Discussion\nWe trained the above RL agent for 500,000 time steps and tested the resulting agent. For evaluation,\nwe disconnected the agent from the training loop, generated 2,000 new environments, passed an\nobservation of each environment to the agent, and used the agent’s predicted action to compute the\nexpected revenue of (1). This expected revenue (denoted f ∗\nRL) is the optimal solution of (1) with\n{P DA\nt\n}t∈T fixed to be the action chosen by the agent. For comparison, we also computed the actual\nsolution of (1) without {P DA\nt\n}t∈T being fixed (i.e., the optimal solution of the two-stage SP with\nthe same Ωas for computing f ∗\nRL), and we denote this as f ∗\nSP . We also created a simple benchmark\nbidding strategy where P DA\nt\n= Gt if λDA\nt\n> λRT\nt\nand P DA\nt\n= 0 if λDA\nt\n≤λRT\nt\n, ∀t ∈T and solved\n(1) with {P DA\nt\n}t∈T fixed under this strategy (denoted f ∗\nbench). Lastly, we trained an identical RL\nagent for only 10,000 time steps, used it to predict {P DA\nt\n}t∈T , and computed the expected revenue\nfrom this agent’s decisions to confirm that the original agent is learning (denoted f ∗\nRL′).\nThe results of this test are shown in Figure 2, where Figure 2a shows the distribution of solutions\nas a fraction of the corresponding f ∗\nSP , and Figure 2b shows the distributions of the agent’s actions\ncompared with the optimal solutions of the two stage problem. Figure 2a shows that the bidding\n3\nFigure 2: Results from RL agent. (a) shows the distribution of solutions as a fraction of the optimal\nsolution. (b) shows the distribution of decisions chosen by the agent after training for both 10k and\n500k steps. Decisions > 1 for Figure b are considered in the rightmost bar.\nbenchmark problem gets more solutions close (within 95%) to the optimal solution, but also has more\ndecisions that were < 85% of the optimal solution. On average, the results of the trained RL agent\nwere ∼1.5% better than the bidding benchmark strategy (or about $4,000 more per day). Figure 2b\nsuggests that the model is learning the general distribution (the distribution of actions is similar to the\noptimal values), but when those actions are taken differs between the RL agent and optimal solution.\nOverall, the above RL approach for learning the first stage solutions of a contextual two-stage SP\nis under-performing what we would hope would be achievable. There are several adjustments that\ncould be pursued in future work which could improve performance. First, we only used 10 scenarios\nfor computing the expected reward, and using more scenarios may be necessary for consistent\nperformance. This could be done with little cost to training time because, once {P DA\nt\n}t∈T are\nfixed, (1) becomes deterministic and each scenario can be solved independently (i.e., scenarios can\nbe solved in parallel). While we did some initial tuning of hyperparameters using Optuna [27] for\nthe PPO algorithm, further hyperparameter tuning could be explored, particularly for the DDPG\nalgorithm. In addition, changing the architecture of the policy network (we used 3 layers with 16\nnodes each) could yield better results as we noticed that the current implementation converges to\nnear-identical actions for many different observations. We are also interested in training for longer\nthan 500,000 time steps (going into the millions of steps) as this may be necessary to learn the entire\nenvironment. Further, the reward function may not penalize sub-optimal behavior enough, and our\ncurrent reward function could be revisited. Lastly, we could also consider the approach of [15], who\nused two RL agents: one to learn the behavior of the second stage variables and the other to learn\nthe behavior of the first stage variables. This latter approach, while requiring two agents, could be\ncomputationally more efficient because no optimization problem needs to be solved.\nWe recognize that there are both benefits and shortcomings in constructing an RL agent for two-stage\nSP. First, there is a trade-off between computational cost of training the RL agent compared with\nthe computational cost of solving the two-stage SP. The intention of using RL is to create a model\nthat 1) can be used under a variety of different contexts (which otherwise would each require solving\na separate SP), 2) is computationally efficient after training is complete, and 3) does not require\never solving the full two-stage problem. For the second point, this means that the RL agent could\nbe used after training with minimal computational cost and could therefore be used within other\nlarge-scale models (e.g., as an input to a larger power grid model or as a surrogate model). For (1),\nthe RL framework is likely much more expensive than solving the two-stage SP directly because\nthis equation is a linear program which can be efficiently solved, even with a variety of contexts.\nHowever, if the above framework is effective, it could likely be applied to more complex problems\nthat are far more difficult to solve (e.g., problems with mixed integer variables) and that have a\nhigher computational cost, making the RL route potentially more desirable. In addition, there is\nmore complexity that could be added to the second stage, such as including other PEL devices (e.g.,\ninverters or converters) which could involve binary variables. In this latter case, the RL framework\ncould be helpful since the second stage problems would be deterministic, meaning several smaller\nmixed-integer problems can be solved instead of ever solving the larger mixed integer two-stage SP.\nIn conclusion, we believe that effectively training a RL agent to learn close-to-optimal first-stage\nvariables of a contextual two-stage SP is possible. However further work needs to be done in solving\n4\nwith more scenarios, optimizing hyperparameters and structure, training for more iterations, and\nrevisiting the reward function.\nAcknowledgments and Disclosure of Funding\nThis research was supported by the Energy System Co-Design with Multiple Objectives and Power\nElectronics (E-COMP) Initiative, under the Laboratory Directed Research and Development (LDRD)\nProgram at Pacific Northwest National Laboratory (PNNL). PNNL is a multi-program national\nlaboratory operated for the U.S. Department of Energy (DOE) by Battelle Memorial Institute under\nContract No. DE-AC05-76RL01830. The authors have no competing interests to report.\nReferences\n[1] Mohlin, K., Bi, A., Brooks, S., Camuzeaux, J., & Stoek, T. (2019) Turning the corner on US power sector\nCO2 emissions–a 1990-2015 state level analysis. Environmental Research Letters, 14(8), 084049.\n[2] Osman, A. I., Chen, L., Yang, M., Msigwa, G., Fraghali, M., Fawzy, S., Rooney, D.W., & Yap, P.S.\n(2023). Cost, environmental impact, and resilience of renewable energy under a changing climate: a review.\nEnvironmental Chemistry Letters, 21(2), 741-764.\n[3] Karunathilake, H., Hewage, K., Mérida, W., & Sadiq, R. (2019) Renewable energy selection for net-zero\nenergy communities: Life cycle based decision making under uncertainty. Renewable energy, 130, 558-573.\n[4] Zakaria, A., Ismail, F.B., Lipu, M.H., & Hannan, M.A. (2020) Uncertainty models for stochastic optimization\nin renewable energy applications. Renewable Energy, 145, 1543-1571.\n[5] Couto, A., Rodrigues, L., Costa, P., Silva, J., & Estanqueiro, A. (2016) Wind power participation in electricity\nmarkets–the role of wind power forecasts. In 2016 IEEE 16th International Conference on Environment and\nElectrical Engineering (EEEIC) (pp. 1-6). IEEE.\n[6] Aghajani, A., Kazemzadeh, R., & Ebrahimi, A. (2018) Optimal energy storage sizing and offering strategy\nfor the presence of wind power plant with energy storage in the electricity market. International Transactions on\nElectrical Energy Systems, 28(11), e2621.\n[7] Atakan, S., Gangammanavar, H., & Sen, S. (2022). Towards a sustainable power grid: stochastic hierarchical\nplanning for high renewable integration. European Journal of Operational Research 302(1), 381-391.\n[8] Bertsimas, D., & Mundru, N. (2023) Optimization-based scenario reduction for data-driven two-stage\nstochastic optimization. Operations Research, 71(4), 1343-1361.\n[9] Wu, Y., Song, W., Cao, Z., & Zhang, J. (2021) Learning scenario representation for solving two-stage\nstochastic integer programs. In International Conference on Learning Representations.\n[10] Soares, J., Canizes, B., Ghazvini, M.A.F., Vale, Z., & Venayagamoorthy, G.K. (2017) Two-stage stochastic\nmodel using Benders’ decomposition for large-scale energy resource management in smart grids. IEEE\nTransactions on Industry Applications, 53(6), 5905-5914.\n[11] Torres, J.J., Li, C., Apap, R.M., & Grossmann, I.E. (2022) A review on the performance of linear and mixed\ninteger two-stage stochastic programming software. Algorithms, 15(4), 103.\n[12] Bengio, Y., Frejinger, E., Lodi, A., Patel, R., & Sankaranarayanan, S. (2020) A learning-based algorithm to\nquickly compute good primal solutions for stochastic integer programs. In Integration of Constraint Program-\nming, Artificial Intelligence, and Operations Research: 17th International Conference, CPAIOR 2020, Vienna,\nAustria, September 21–24, 2020, Proceedings 17 (pp. 99-111). Springer International Publishing.\n[13] Patel, R. M., Dumouchelle, J., Khalil, E., & Bodur, M. (2022). Neur2SP: Neural two-stage stochastic\nprogramming. 36th Conference on Neural Information Processing Systems (NeurIPS 2022), 23992-24005.\n[14] Nair, V., Dvijotham, D., Dunning, I., & Vinyals, O. (2018) Learning fast optimizers for contextual stochastic\ninteger programs. In UAI (pp. 591-600).\n[15] Yilmaz, D. & Büyüktahtakin, ˙I.E. (2023). A deep reinforcement learning framework for solving two-stage\nstochastic programs. Optimization Letters, 1-28.\n[16] Wei, X., Xiang, Y., Li, J., & Zhang, X. (2022). Self-dispatch of wind-storage integrated system: a deep\nreinforcement learning approach. IEEE Transactions on Sustainable Energy, 13(3), 1861-1864.\n5\n[17] Lehna, M., Hoppmann, B., Scholz, C., & Heinrich, R. (2022) A reinforcement learning approach for the\ncontinuous electricity market of Germany: trading from the perspective of a wind park operator. Energy and AI,\n8, 100139\n[18] Jeong, J., Kim, S.W., & Kim, H. (2023) Deep reinforcement learning based real-time renewable energy\nbidding with battery control. IEEE Transactions on Energy Markets, Policy, and Regulation.\n[19] Kordkheili, R.A., Pourakbari-Kasmaei, M., Lehtonen, M., & Pouresmaeil, E. (2020) Optimal bidding\nstrategy for offshore wind farms equipped with energy storage in the electricity markets. In IEEE PES Innovative\nSmart Grid Technologies Europe (ISGT-Europe) (pp. 859-863). IEEE.\n[20] Lillicrap, T.P., Hunt, J.J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., & Wierstra, D. (2015).\nContinuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971.\n[21] Schulman, J., Wolski, F., Dhariwal, P., Radford, A., & Klimov, O. (2017) Proximal policy optimization\nalgorithms. arXiv preprint arXiv:1707.06347.\n[22] Raffin, A., Hill, A., Gleave, A., Kanervisto, A., Ernestus, M., & Dormann, N. (2021). Stable-baselines3:\nreliable reinforcement learning implementations. The Journal of Machine Learning Research, 22(1), 12348-\n12355.\n[23] ISO New England Inc. (2023). Selectable day-ahead and preliminary real-time hourly LMPs. https:\n//www.iso-ne.com/isoexpress/web/reports/pricing/-/tree/lmp-by-node. Accessed 9-22-2023.\n[24] Grothe, O., Kächele, F., & Watermeyer, M. (2022). Analyzing Europe’s biggest offshore wind farms: a data\nset with 40 years of hourly wind speeds and electricity production. Energies, 15(5), 1700.\n[25] Gangammanavar, H. & Sen, S. (2016). Two-scale stochastic optimization for controlling distributed storage\ndevices. IEEE Transactions on Smart Grid, 9(4): 2691-2702.\n[26] Sharma, K.C., Bhakar, R., Tiwari, H.P., & Chawda, S. (2017). Scenario based uncertainty modeling\nof electricity market prices. In 2017 6th International Conference on Computer Applications in Electrical\nEngineering-Recent Advances (CERA) (pp. 164-168). IEEE.\n[27] Akiba, T., Sano, S., Yanase, T., Ohta, T., & Koyama, M. (2019). Optuna: a next-generation hyperparameter\noptimization framework. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge\nDiscovery & Data Mining (pp.2623-2631).\n6\n",
  "categories": [
    "eess.SY",
    "cs.AI",
    "cs.LG",
    "cs.SY",
    "math.OC"
  ],
  "published": "2023-12-18",
  "updated": "2023-12-18"
}