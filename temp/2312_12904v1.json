{
  "id": "http://arxiv.org/abs/2312.12904v1",
  "title": "PGN: A perturbation generation network against deep reinforcement learning",
  "authors": [
    "Xiangjuan Li",
    "Feifan Li",
    "Yang Li",
    "Quan Pan"
  ],
  "abstract": "Deep reinforcement learning has advanced greatly and applied in many areas.\nIn this paper, we explore the vulnerability of deep reinforcement learning by\nproposing a novel generative model for creating effective adversarial examples\nto attack the agent. Our proposed model can achieve both targeted attacks and\nuntargeted attacks. Considering the specificity of deep reinforcement learning,\nwe propose the action consistency ratio as a measure of stealthiness, and a new\nmeasurement index of effectiveness and stealthiness. Experiment results show\nthat our method can ensure the effectiveness and stealthiness of attack\ncompared with other algorithms. Moreover, our methods are considerably faster\nand thus can achieve rapid and efficient verification of the vulnerability of\ndeep reinforcement learning.",
  "text": "PGN: A perturbation generation network against deep reinforcement learning\nXiangjuan Li∗, Feifan Li∗†, Yang Li∗and Quan pan∗\n∗Northwestern Polytechnical University, Xi’an, China\nlixiangjuan@mail.nwpu.edu.cn, {liff, liyangnpu, quanpan}@nwpu.edu.cn\nAbstract—Deep reinforcement learning has advanced greatly\nand applied in many areas. In this paper, we explore the vul-\nnerability of deep reinforcement learning by proposing a novel\ngenerative model for creating effective adversarial examples to\nattack the agent. Our proposed model can achieve both targeted\nattacks and untargeted attacks. Considering the specificity of\ndeep reinforcement learning, we propose the action consistency\nratio as a measure of stealthiness, and a new measurement\nindex of effectiveness and stealthiness. Experiment results show\nthat our method can ensure the effectiveness and stealthiness of\nattack compared with other algorithms. Moreover, our methods\nare considerably faster and thus can achieve rapid and efficient\nverification of the vulnerability of deep reinforcement learning.\nKeywords-Deep reinforcement learning, adversarial attack,\ngenerative network\nI. INTRODUCTION\nIn recent years, Deep Reinforcement Learning (DRL)\nhas achieved excellent results in many application areas\nsuch as games[1], robot control[2], and other tasks[3–5],\nand even surpassed human levels. However, as a combi-\nnation of deep learning and reinforcement learning, DRL\ninevitably has vulnerabilities, especially in deep learning,\nwhere images face a variety of complex attack forms, such\nas the Fast Gradient Sign Method (FGSM)[6], Projected\nGradient Descent (PGD)[7], Carlini & Wagner (CW)[8] and\nFew Pixel Attack[9]. These cause the application of DRL\nis vulnerable to adversarial attacks, which can render the\nsystem ineffective or even catastrophic in critical scenarios\nsuch as autonomous driving. Thus, it’s necessary to verify\nthe vulnerability of deep reinforcement learning in the form\nof an adversarial attack in advance to prevent it from causing\nirreparable losses in the application.\nIn DRL, the agent’s observations are generally presented\nin the form of images, so image attack methods can be\nused to tamper with the agent’s observations, resulting in\nincorrect decision-making. There are many works about how\nto verify the vulnerability of the attack. Adversarial attacks\nagainst DRL were first introduced in 2015 by Huang et\nal.[10] and Kos et al.[11], who adapted FGSM from image\nclassification to the DRL. Then Lin et al.[12] and Sun et\nal.[13] focused on how to reduce the attack frequency and\nattack only at critical moments. Li et al.[14] investigated how\nto use an adversarial strategy network to adaptively select\nthe moment of attack and generate an adversarial example\nfor the attack, considering both the long-term reward and\nshort-term reward. Although the aforementioned algorithms\nreduced the number of attacks, they still used the traditional\nmethod to produce the adversarial examples, which has\nthe problems of high time complexity and low quality of\nthe generated example images to verify the vulnerability\nquickly and efficiently. In addition, [15] pointed out that\nthe effect of attacking against the observation of an agent\nusing traditional image transformation operations is much\ngreater than that of traditional attack methods such as\nCW. Therefore, it is necessary to research how to produce\nadversarial examples against DRL to verify the vulnerability\nas fast as possible and insidiously. In the meantime, there\nis still a lack of an efficient evaluation metric for attacking\nstealthiness.\nBased on the above research, in this paper, we design\na perturbation generation network (PGN) for observation-\nbased attacks against DRL. The network aims to generate\nperturbation that makes the agent choose the wrong action\nand eventually leads to the failure of the task. The trained\nnetwork can generate perturbations adaptively for the obser-\nvations of the agent at different moments. Compared with\ntraditional adversarial example generation algorithms, the\nadversarial examples generated by PGN make the agent less\ndetectable, while ensuring the effectiveness of the attack and\nthe quality of the adversarial examples, balancing the effec-\ntiveness and stealthiness of the attack. The key contributions\nof this paper are as follows:\n• To address the shortcomings of traditional attacks with\nhigh time complexity, a perturbation generation net-\nwork is designed to generate perturbations quickly and\nefficiently to perturb the observation space of an agent\nin real-time.\n• Combined with the characteristics of the attack against\nDRL, the perturbation network is constructed by intro-\nducing Q value and can achieve targeted and untargeted\nattacks.\n• The ratio of consistent actions without and with an\nattack is introduced to measure the stealthiness of the\nattack.\n• A new attack effect measurement index is proposed by\ncomprehensively consider the effectiveness and stealth-\niness of attack.\nThe rest of the paper is organized as follows. Section II\nintroduces related works. Section III describes the proposed\nmethod. Section IV presents the details of the experiment\narXiv:2312.12904v1  [cs.LG]  20 Dec 2023\nand analyse the results. Finally, Section V concludes the\npaper and draws the future work.\nII. RELATED WORKS\nA. Deep reinforcement learning\nDeep reinforcement learning enables agents to learn by\ninteracting with the environment to achieve long-term re-\nwards. The agent’s interaction with the environment can be\nrepresented as Markov Decision Process(MDP). In general,\nit can be interpreted by a tuple < S, A, P, r, γ >, where S is\nthe state space, A is the action space, P:S × A × S →[0:1]\nis the transition probability, r denotes the reward, and\nγ ∈[0, 1) is the discount factor. At each time step, the\ncurrent state of the agent can be represented as st, and then\nit will select action at to enter the next state st+1 according\nto the policy network π, where the policy π(st, at) ∈[0, 1]\nrepresents the possibility of choosing an action and can\nalso directly output the optimal action in the state. r(st, at)\nrepresents the immediate reward after executing the action\nat. The goal of the agent is to learn an optimal strategy to\nmaximize the final reward R, which can be interpreted as:\nR =\nT −1\nX\nt\nEπ(st,at)[γtr(st, at)]\n(1)\nGenerally, DRL can be categorized into value-based\nand policy gradient-based. The former primarily employs\ndeep neural networks to approximate the target action-value\nfunction, which represents the cumulative reward obtained\nby reaching a certain state or performing a certain ac-\ntion. The representative methods such as Deep Q-Network\n(DQN)[16]. The latter[17, 18], on the other hand, param-\neterizes the policy and employs deep neural networks to\napproximate it while seeking the optimal policy along the\ndirection of policy gradients.\nB. Adversarial attacks against machine learning\nAdversarial attacks in the context of deep reinforcement\nlearning draw inspiration from the concept of adversarial\nattacks against machine learning models and can be con-\nsidered a subcategory of such attacks. Adversarial attacks\nin machine learning refer to the technique of deceiving\nmachine learning systems into producing incorrect outputs or\ndecisions when exposed to adversarial examples. Adversarial\nexamples are modified versions of the original examples\nthat are artificially created by adding, deleting, or changing\nfeatures or noises. These examples can be generated through\nmethods like gradient-based methods[6, 7], optimization\nmethods[8] and generative adversarial methods [19–21].\nAccording to the target, adversarial attacks can be classi-\nfied into two types: targeted and untargeted. The former aims\nto classify an image into a designated category. The latter\naims to make the model produce incorrect decisions without\nany specific goal. Adversarial attacks can also be classified\ninto two types based on the level of information available to\nthe attacker: clear box and opaque box. In a clear box attack,\nthe attacker has complete access to the model information,\nincluding its structure and parameters. In contrast, an opaque\nbox attack limits the attacker’s access to only the model’s\ninput and output.\nCommon adversarial attack methods pose a significant\nchallenge to the robustness and reliability of machine learn-\ning by carefully crafting the small perturbations to mislead\nthe model. However, adversarial attacks toward deep rein-\nforcement learning follow a similar principle but are tailored\nto the specific setting of deep reinforcement learning agents.\nIn summary, adversarial attacks in the domain of deep\nreinforcement learning are rooted in the broader concept\nof adversarial attacks against machine learning models.\nHowever, they address the unique challenges posed by\nreinforcement learning settings and pose a crucial area of\nresearch for improving the robustness and security of deep\nreinforcement learning agents.\nC. Adversarial Attacks against DRL\nAs DRL is increasingly applied to decision-making tasks\nrequiring high security, researchers are conducting more\ninvestigations into its vulnerability using adversarial attacks.\nAdversarial attacks in DRL can be categorized into attack\nbased reward[22], attack based policies[23], attack based\nobservations[12, 14, 24], attack based environment[25], and\nattack based actions[26]. The objectives of the above attacks\nare all aimed at causing the agent to make incorrect decisions\nand resulting in the minimization of cumulative reward.\nCurrently, adversarial attacks based on observations in\nDRL mainly rely on carefully selecting the attack moment\nand generating adversarial examples. Huang et al.[10] in-\njected attacks at each time step, which reduced the perfor-\nmance of the agent, but made it easy for the attacker to\nobserve the agent’s actions going beyond acceptable limits\nand detect the attack. Later attacks focused on fewer specific\ntime steps to ensure stealthiness. In the time strategy attack\nproposed by Lin et al.[12], they calculated the difference\nbetween the action probabilities for each step and launched\nattacks when the difference exceeded a given threshold,\nwhich reduced the number of attacks to some extent. Li\net al.[14] studied how to use an adaptive adversarial policy\nnetwork to select attack moments and generate adversarial\nexamples for the attack while considering both the long-term\nreward and short-term reward of the agent. However, the\nabove methods require either calculation of attack moment\nor a large amount of pre-training and still use traditional\nmethods such as FGSM, CW, and PGD to generate ad-\nversarial examples, neglecting the time complexity required\nfor example generation. When verifying the vulnerability\nof DRL algorithms, it is essential to reduce the time and\ncomputational costs as much as possible.\nIII. METHODOLOGY\nIn this paper, we mainly research the clear box attack\nagainst well-trained DQN agents to verify the vulnerability\nof DRL. We design a generative model PGN that can add\nappropriate perturbations to the agent’s observations, result-\ning in the final reward being reduced, while also ensuring\nthe effectiveness and stealthiness of the attack. The structure\nof AutoEncoder-based PGN is in Figure 1. Given a noise z,\nPGN generates a δ and adds it to the original observation x,\nand then will be clipped to get h(x), which will be passed\nto the agent and confuses it.\nA. Adversarial Network Model\nGiven an agent model f : X →Y , the observation of the\nagent can be represented as x, and the goal of the attacker\nis to find the perturbation δ and get the adversarial example\nx + δ, which satisfies f(x) ̸= f(x + δ). The PGN can be\ndescribed as: h(x) = x + δ, and that satisfies constraint in\nDQN:\narg max\na\nQ(x, a) ̸= arg max\na\nQ(h(x), a)\n(2)\nTo ensure effectiveness, the ideal goal of PGN is to find a\nδ that also satisfies:\nh(x) = arg max\nx+δ Q(x + δ, arg min\na Q(x, a))\n(3)\nand ∥δ∥2 < ϵ for ϵ > 0. To achieve the above objective, the\nloss function of PGN can be designed as:\nL(θ) = αLx(x, h(x)) + Ly(y, y′) + βLc\n(4)\nThe former represents the Euclidean distance between the\noriginal observation and adversarial observation:\nLx(x, h(x)) =\nv\nu\nu\nt\nn\nX\ni=1\n(xi −h(x)i)2\n(5)\nwhere n represents the dimension, xi and h(x)i represent the\ncomponent in dimension i of original observation and adver-\nsarial example, by minimizing it can ensure the maximum\nsimilarity of original observation and adversarial observa-\ntion. The second is the loss of effectiveness. Inspired by\nGenerative Adversarial Networks (GANs), the effectiveness\nloss can be designed by the following two schemes:\nUntargeted attack (U-PGN): We introduce Ly1 that maxi-\nmizes the difference between the maximum Q value Q(x, at)\nunder normal observations and the Q value under adversarial\nobservation and related action at. This encourages the agent\nto select actions other than the optimal one, similar to non-\ntargeted attacks, in order to disrupt the agent’s decision-\nmaking:\nLy1 = 10 −log L(Qmax(x, at), Q(h(x), arg maxaQ(x, a)))\n(6)\nTargeted attack (T-PGN): to ensure the effectiveness of the\nattack, the target action is defined as the action correspond-\ning to the minimum Q value under the original observation.\nAs a result, the difference between the Q values of the target\naction under original and adversarial observations should be\nmaximized, thereby ensuring that the Q value of the target\naction under adversarial observation is maximized as much\nas possible, and the probability of it being selected is also\nmaximized:\nLy2 = L(Q(h(x), atargeted), R(Q(x), atargeted))\n(7)\natargeted = arg minaQ(x, a)\n(8)\nin which R(·) is the re-ranking strategy.\nAll of the two schemes are calculated by Mean Squared\nError (MSE) as follows:\nLy = 1\nN\nN\nX\ni=1\n(yi −ˆyi)2\n(9)\nwhere n represents the dimension, yi and ˆyi represents the\ncomponent in dimension i.\nAt last, we introduced the hinge loss term by comparing\nthe L2 norm to constrain the perturbation within a certain\nrange, in which the C means the threshold of perturbation:\nLc = max(∥δ∥2 −C, 0)\n(10)\nwe will penalize the PGN if the L2 norm of perturbation is\ngreater than C.\nB. Re-ranking Strategy\nIn order to achieve a strong-targeted attack, in this work\nwe introduced the re-ranking strategy inspired by ATN[27].\nEspecially, the re-ranking strategy R(·) can magnify the\nQ value Q(st, atargeted) of the target action atargeted and\nmaintain the order of other actions. In this case, the pertur-\nbation can be ensured small enough and relevant only to the\ntarget action. The re-ranking strategy can be described as:\nR(Q, atargeted) = norm\n\u0012\u001a κ · max Q\nif at = atargeted\nQ(st, at)\notherwise\n\u001b\u0013\n(11)\nGiven the Q value of the original observation, the action\ncorresponding to the minimum Q value is taken as the target\naction. Then the Q(st, atargeted) will be changed to κ times\nthe maximum Q value, and κ > 1 is an additional parameter\nspecifying how much larger the targeted Q value should be\nthan the original max Q value. norm(·) is a normalization\nfunction that rescales its input to be a valid probability\ndistribution.\nFigure 1.\nThe structure of AutoEncoder-based PGN and training process\nC. Action Consistency Ratio\nIn adversarial attacks against DRL, attackers aim to\nmanipulate the agent’s observations to make it choose the\nwrong actions, leading to the failure of the task. To ensure\nthe stealthiness of the attack, attackers generally choose to\ninject attacks in as few time steps as possible to avoid being\ndiscovered. So it is essential to maintain a high level of\nconsistency in the agent’s action selection, which is known\nas the action consistency ratio. The goal is to make the\nattack virtually undetectable to bystanders while reducing\nthe reward obtained by the agent. From a bystander’s per-\nspective, it is often difficult to determine whether an agent’s\nobservations have been tampered with, but can only judge\nwhether the agent is attacked according to its actions.\nFor instance, in the game of Pong, an agent typically\nmoves toward the ball. If an agent’s actions are always\ninconsistent with human expectations, it is highly likely that\nthe agent has been attacked. When the ball is downward, the\nagent should also downward, but after the attack the agent\nchooses upward and away from the ball. If this happens\nfrequently, then we consider the agent to be under attack.\nTherefore, we introduce the action consistency ratio ACR to\nrepresent the stealthiness of the attack. If the agent’s actions\nremain consistent with what is expected in most cases but\nthe reward is ultimately reduced, then the stealthiness of the\nattack is guaranteed. The formula is expressed as:\nACR = Nsame/Ntotal\n(12)\nThe Nsame means the number of actions that are consistent\nwith both normal and attacked observations, and the Ntotal\nmeans the total number of actions.\nIV. EXPERIMENT\nA. Experiment Settings\nThe purpose of PGN is to effectively attack a normal agent\nin a real-time scenario. To achieve this, it is assumed that\nthe attacker has access to the policy network of the agent.\nBy predicting the action distribution of the agent before\nand after adding perturbations and using the difference in\nQ value distribution as one of the loss terms, the model can\nbe further adjusted.\nIn the experiment, four DQN agents corresponding to the\nPong, MsPacman, SpaceInvaders, and Qbert games were\npre-trained using PyTorch, Tianshou1 and Gym2 libraries.\nThen PGN utilizes the interaction of pre-trained agents with\nthe environment to construct an offline dataset for training.\nThe parameters of DQN and PGN are recorded in Table I and\nTable II respectively. Generally, the corresponding reward\nachieved by the four agents is shown in the “Normal” row\nof Table III.\nWe compared the PGN with CW, FGSM, and PGD under\nthe same attack frequency which is 100%. The ϵ in FGSM\nis 0.1, and the iterations of the CW and PGD are 50. PGN is\ndesigned with two different structures: AutoEncoder-based\n(as shown in Figure 1) and Generator-based (the structure\nof PGN is similar to the generator in GANs). Thus the\ntargeted attack T-PGN can be represented as “T-PGNA” and\n“T-PGNG”, the untargeted attack U-PGN can be represented\nas “U-PGNA” and “U-PGNG”.\nTable I\nHYPER-PARAMETERS IN DQN\nparameter\nvalue\nparameter\nvalue\nlearning rate\n1e-3\nbatch size\n32\ndiscount factor γ\n0.99\ntarget network update frequency\n500\nreplay buffer\n1e5\ntraining episode\n16\nϵ-greedy start\n1\nϵ-greedy end\n0.05\nB. Results\nThe cumulative reward and ACR of different attack\nmethods in each game are shown in Table III. Each result\nis an average of over 10 episodes. In Table III, “Normal”\n1Tianshou is a PyTorch-based reinforcement learning framework de-\nsigned to provide efficient implementation and easy-to-use API.\n2Gym is a Python open-source library designed for reinforcement learn-\ning which provides standardized interfaces and environments.\nTable II\nHYPER-PARAMETERS IN PGN\nparameter\nvalue\nparameter\nvalue\nlearning rate\n1e-3\nbatch size\n128\nepoch\n40\ntraining episode\n20\nα\n1e-2\nβ\n1\nκ\n10\nC\n0.1\nFigure 2.\nTime complexity of different attack methods\nrepresents the reward without attack, “T-PGNA” and “T-\nPGNG” represents the targeted attack of PGN, while “U-\nPGNA” and “U-PGNG” represents the targeted attack of\nPGN, and the bold data is the best result.\nAnd Table IV records the Peak Signal-to-Noise Ratio\n(PSNR) of different attack methods. The PSNR is used to\nmeasure the image quality of the adversarial example in an\nimage attack. This index is based on the MSE of the image.\nThe smaller the MSE, the larger PSNR, indicating better\nimage quality. The PSNR value can be used to measure the\nimage quality of the adversarial example. Specifically, the\nhigher the PSNR value, the closer the adversarial example\nis to the original image; the lower the PSNR value, the worse\nthe image quality.\nIf the ACR\n> 50% and the percentage of reward\nreduced is less than 50%, the attack is considered too weak.\nTherefore, according to the experimental results, U-PGNA\nand U-PGNG are weak attacks. We found that only U-PGNA\nperforms the best in Pong, in the rest of the experiments\nboth U-PGNG and U-PGNA have a low attack effect, but\nthe ACR is very high, which also shows that if the ratio of\nconsistency of action with and without attack is very high\nit means the attack is almost ineffective. Thus we will not\ncompare U-PGNA and U-PGNG in the following work.\nFrom Table III we find that considering only the reward\nand PSNR, the lower the reward of the traditional attack\nmethod, the lower PSNR, i.e., the lower the image quality\nof the adversarial example. That is the attack effect at the\nexpense of PSNR. In most cases, PGD has the lowest reward\nand the lowest PSNR, while CW has the highest reward\nand the highest PSNR in most cases. FGSM is in between.\nHowever, we should realize that different from traditional\nimage classification attacks, attacks against DRL cannot be\nbased solely on the quality of the adversarial examples as\na measure of stealthiness. An attacker can easily determine\nwhether an attack has suffered directly from changes in the\nagent’s action. Therefore, the action consistency ratio ACR\nis more important than PSNR in attack agents, and it is more\nmeaningful to use it as a basis for judging stealthiness.\nComparing the ACR, we find that T-PGNA and T-PGNG\nare always higher than traditional methods, even up to\n32.59% in SpaceInvaders, nearly 20% higher than the other\nalgorithms. However, the cumulative reward is slightly lower\nthan PGD in several cases. It’s an important question of\nhow to balance the cumulative reward, ACR, and PSNR.\nThen we proposed a new index AR for measuring the\neffectiveness and stealthiness based on the ratio of reward\nvariation, ACR and PSNR. The new index AR can be\nexpressed as:\nAR = α × ∆R + β × ACR + γ × PSNR\n(13)\nα + β + γ = 1\n(14)\nwhere α, β, and γ are weighting coefficients, ∆R means\nthe ratio of reward variation, the numerator is the difference\nbetween the cumulative reward of the normal agent and the\nminimum cumulative reward of the victim, and then the\ndenominator is the difference between the cumulative reward\nof the normal agent and the victim, namely:\n∆R =\nRnormal −Rattacked\nRnormal −min (Rattacked)\n(15)\nin which Rnormal represents normal reward of agent and\nRattacked represents reward after being attacked. According\nto different rules of games, the min (Rattacked) value of\nPong is -21, and 0 in the remaining three games. The\nfirst item of AR represents effectiveness, and the last two\nrepresent the stealthiness of the attack. In calculation, the\n∆R and the ACR are in 0 ∼1, PSNR is in 0 ∼100, thus\nthe weighting coefficients α, β and γ are 0.5, 0.49 and 0.01\nrespectively.\nThen we compared AR in Table V. Results show that the\nAR of T-PGNA and T-PGNG are in 0.6 ∼0.7, while others\nare lower than 0.6 in most cases. The results demonstrated\nthat T-PGNA/G strikes a good balance between effectiveness\nand stealthiness compared to traditional attack methods.\nThen, Figure 2 shows the time complexity of CW, FGSM,\nPGD, T-PGNA, and T-PGNG. We find that T-PGN is the\nfastest, with an order of magnitude around 1e-4 (T-PGNA)\nand 1e-3 (T-PGNG), FGSM is the next, and PGD is inferior\nto it, while CW is the slowest with an order of magnitude\naround 1e-1. The high time complexity is due to the fact\nthat both PGD and CW are based on iterative optimization\nalgorithms. This means that PGN can achieve fast and effi-\ncient verification of vulnerability in DRL, greatly reducing\ntime costs.\nFigure 3.\nUnperturbed observations, perturbation, and perturbed observations of T-PGNA, T-PGNG, CW, FGSM, and PGD\nTable III\nREWARD AND ACR OF FOUR ATTACK METHODS\nMethods\nPong\nMsPacman\nSpaceInvaders\nQbert\nreward\nACR\nreward\nACR\nreward\nACR\nreward\nACR\nNormal\n21.0\n–\n2155.0\n–\n719.0\n–\n5460.0\n–\nCW\n-17.5\n0\n982.0\n0\n465.5\n10.95%\n520.0\n0.11%\nFGSM\n-21.0\n0.33%\n519.0\n0.02%\n258.5\n1.15%\n225.0\n2.10%\nPGD\n-21.0\n6.62%\n222.0\n0\n179.0\n0\n35.0\n0\nT-PGNA\n-21.0\n8.77%\n210.0\n0\n285.0\n32.59%\n125.0\n11.92%\nT-PGNG\n-20.9\n13.95%\n315.0\n3.13%\n182.5\n30.56%\n70.0\n19.80%\nU-PGNA\n-21.0\n3.88%\n2144.0\n98.57%\n499.5\n96.70%\n4390.0\n97.73%\nU-PGNG\n13.6\n34.25%\n1600.0\n96.48%\n547.0\n63.48%\n4465.0\n76.77%\nTable IV\nPSNR VALUE OF FOUR ATTACK METHODS\nMethods\nPong\nMsPacman\nSpaceInvaders\nQbert\nCW\n10.65\n11.38\n24.15\n14.94\nFGSM\n6.06\n10.94\n18.01\n13.23\nPGD\n7.16\n8.05\n9.59\n9.17\nT-PGNA\n15.86\n16.29\n21.91\n12.46\nT-PGNG\n8.01\n8.97\n13.11\n9.23\nU-PGNG\n9.16\n9.55\n11.13\n9.08\nU-PGNA\n22.40\n16.06\n24.15\n18.36\nTable V\nAR VALUE OF FOUR ATTACK METHODS\nMethods\nPong\nMsPacman\nSpaceInvaders\nQbert\nCW\n0.56\n0.39\n0.47\n0.60\nFGSM\n0.56\n0.49\n0.51\n0.62\nPGD\n0.60\n0.53\n0.47\n0.59\nT-PGNA\n0.70\n0.61\n0.69\n0.67\nT-PGNG\n0.65\n0.53\n0.65\n0.68\nConsidering the effectiveness of the attack, Figure 3\nrecords the unperturbed observations, perturbation, and per-\nturbed observations of T-PGNA, T-PGNG, CW, FGSM, and\nPGD in four games. The L2 norm of each perturbation is\nlimited to 0.1. As depicted in Figure 3, it is evident that\nthe perturbations generated by T-PGNA and CW tend to\nbe localized, particularly noticeable in SpaceInvaders and\nQbert. Conversely, the perturbations produced by FGSM and\nPGD exhibit a more global influence. Simultaneously, the\nPSNR values for T-PGNA and CW significantly surpass\nthose of FGSM and PGD. This disparity arises because\nFGSM and PGD employ gradient-based attack methods,\noptimizing perturbations by maximizing the loss function\ngradient. Consequently, these approaches affect the entire\ninput image, resulting in global perturbations.\nIn contrast, CW follows a localized search strategy,\nstriving to discover minimal perturbations within specific\nimage regions to achieve adversarial objectives. It aims to\nidentify regions with minimal perturbation that minimize the\nobjective function. Furthermore, T-PGNA reconstructs input\nnoise through encoding and subsequent decoding, ensuring\nmaximum efficiency and minimal size.\nV. CONCLUSION\nIn this paper, we explored how to verify the vulnerability\nof DRL quickly and efficiently by proposing the generative\nmodel PGN, which is based on GAN and AutoEncoder, en-\nsuring the stealthiness and effectiveness of the attack. Mean-\nwhile, considering the relationship between the attacker,\nthe agent, and the environment, we proposed the action\nconsistency ratio ACR to measure the attack’s stealthiness.\nFinally, we proposed an index AR that can combine reward,\nACR, and PSNR to measure the effectiveness and stealthi-\nness of the attack. Experimental results show that compared\nto traditional attack methods FGSM, CW, and PGD, PGN\ncan effectively reduce the agent’s final reward while ensuring\na higher ACR and the time complexity is greatly reduced.\nThe model in this paper can be used to verify the vulner-\nability of various DRL algorithms. However, in practical\napplications, algorithms with higher robustness tend to be\ndirectly selected instead of considering the vulnerability of\nthe algorithm itself. Therefore, verifying the vulnerability of\nDRL is only the first step, and how to improve its robustness\nis the key. Therefore, in the following work, we will consider\nstudying how to apply the detection method in [28] on attack\ndetection of DRL, and how to use regularization methods to\nfurther study the improvement of DRL robustness.\nACKNOWLEDGMENT\nThis research is supported by the National Natural Science\nFoundation of China (No.62103330, 62203358, 62233014),\nthe Fundamental Research Funds for the Central Universi-\nties (3102021ZDHQD09), and the Practice and Innovation\nFunds for Graduate Students of Northwestern Polytechnical\nUniversity.\nREFERENCES\n[1] G. Lample and D. S. Chaplot, “Playing FPS games\nwith deep reinforcement learning,” in 2017 Thirty-First\nAAAI Conference on Artificial Intelligence (AAAI),\nvol. 31, no. 1, 2017, pp. 600–603.\n[2] L. Tai, G. Paolo, and M. Liu, “Virtual-to-real deep\nreinforcement learning: Continuous control of mobile\nrobots for mapless navigation,” in 2017 IEEE/RSJ\nInternational Conference on Intelligent Robots and\nSystems (IROS), 2017, pp. 31–36.\n[3] S. Miao, Y. Li, and Q. Pan, “Research on industrial\ncyber range based on multi-agent cooperative optimiza-\ntion,” in Proceedings of 2021 International Conference\non Autonomous Unmanned Systems (ICAUS 2021).\nSpringer, 2022, pp. 843–851.\n[4] H. Peng, Y. Ma, S. Poria, Y. Li, and E. Cambria,\n“Phonetic-enriched text representation for chinese sen-\ntiment analysis with reinforcement learning,” Informa-\ntion Fusion, vol. 70, pp. 88–99, 2021.\n[5] Z. Ning, S. Sun, X. Wang, L. Guo, G. Wang, X. Gao,\nand R. Y. Kwok, “Intelligent resource allocation in mo-\nbile blockchain for privacy and security transactions:\na deep reinforcement learning based approach,” Sci.\nChina Inf. Sci., vol. 64, no. 6, 2021.\n[6] I.\nJ.\nGoodfellow,\nJ.\nShlens,\nand\nC.\nSzegedy,\n“Explaining and harnessing adversarial examples,”\nin 2015 3rd International Conference on Learning\nRepresentations, 2015, pp. 1–11. [Online]. Available:\nhttp://arxiv.org/abs/1412.6572\n[7] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and\nA. Vladu, “Towards deep learning models resistant to\nadversarial attacks,” in 6th International Conference on\nLearning Representations, 2018.\n[8] N. Carlini and D. Wagner, “Towards evaluating the ro-\nbustness of neural networks,” in 2017 IEEE Symposium\non Security and Privacy (SP), 2017, pp. 39–57.\n[9] Y. Li, Q. Pan, Z. Feng, and E. Cambria, “Few pixels\nattacks with generative model,” Pattern Recognition,\nvol. 144, p. 109849, 2023.\n[10] S. Huang, N. Papernot, I. Goodfellow, Y. Duan, and\nP. Abbeel, “Adversarial attacks on neural network\npolicies,” in Proceedings of the 5th International Con-\nference on Learning Representations, 2017, pp. 1–11.\n[11] J. Kos and D. Song, “Delving into adversarial attacks\non deep policies,” in 5th International Conference on\nLearning Representations, 2017, pp. 1–12.\n[12] Y. Lin, Z. Hong, Y. Liao, M. Shih, M. Liu, and M. Sun,\n“Tactics of adversarial attack on deep reinforcement\nlearning agents,” in Proceedings of the 26th Interna-\ntional Joint Conference on Artificial Intelligence, 2017,\npp. 3756–3762.\n[13] J. Sun, T. Zhang, X. Xie, L. Ma, Y. Zheng, K. Chen,\nand Y. Liu, “Stealthy and efficient adversarial attacks\nagainst deep reinforcement learning,” in The 34th AAAI\nConference on Artificial Intelligence, vol. 34, 2020, pp.\n5883–5891.\n[14] Y. Li, Q. Pan, and E. Cambria, “Deep-attack over the\ndeep reinforcement learning,” Knowl. Based Syst., vol.\n250, p. 108965, 2022.\n[15] E. Korkmaz, “Adversarial robust deep reinforcement\nlearning requires redefining robustness,” CoRR, vol.\nabs/2301.07487, 2023.\n[16] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves,\nI. Antonoglou, D. Wierstra, and M. Riedmiller, “Play-\ning atari with deep reinforcement learning,” arXiv\npreprint arXiv:1312.5602, 2013.\n[17] J. Schulman, S. Levine, P. Abbeel, P. Moritz, M. Jor-\ndan, P. Isola, B. Zoph, O. Vinyals, and J. Wu,\n“Trust region policy optimization,” arXiv preprint\narXiv:1502.05477, 2015.\n[18] T. Lillicrap, J. Hunt, A. Pritzel, N. Heess, T. Erez,\nY. Tassa, M. Vorobyov, and D. Wierstra, “Continu-\nous control with deep reinforcement learning,” arXiv\npreprint arXiv:1502.05761, 2015.\n[19] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu,\nD. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio,\n“Generative adversarial networks,” Communications of\nthe ACM, vol. 63, no. 11, pp. 139–144, 2020.\n[20] A. Garg, N. Tandon, and A. S. Varde, “I am guessing\nyou can’t recognize this: Generating adversarial im-\nages for object detection using spatial commonsense\n(student abstract),” in The 34th AAAI Conference on\nArtificial Intelligence, 2020, pp. 13 789–13 790.\n[21] N. Jiawei, L. Zhunga, P. Quan, Y. Yanbo, and L. Yang,\n“Conditional self-attention generative adversarial net-\nwork with differential evolution algorithm for imbal-\nanced data classification,” Chinese Journal of Aeronau-\ntics, vol. 36, no. 3, pp. 303–315, 2023.\n[22] X. Zhang, Y. Ma, A. Singla, and X. Zhu, “Adap-\ntive reward-poisoning attacks against reinforcement\nlearning,” in Proceedings of the 37th International\nConference on Machine Learning, ser. Proceedings\nof Machine Learning Research, vol. 119, 2020, pp.\n11 225–11 234.\n[23] V. Behzadan and W. H. Hsu, “Adversarial exploitation\nof policy imitation,” in Proceedings of the Workshop\non Artificial Intelligence Safety 2019 co-located with\nthe 28th International Joint Conference on Artificial\nIntelligence, vol. 2419, 2019.\n[24] X. Li, Y. Li, Z. Feng, Z. Wang, and Q. Pan, “ATS-\nO2A: A state-based adversarial attack strategy on deep\nreinforcement learning,” Comput. Secur., vol. 129, p.\n103259, 2023.\n[25] X. Bai, W. Niu, J. Liu, X. Gao, Y. Xiang, and J. Liu,\n“Adversarial examples construction towards white-box\nq table variation in dqn pathfinding training,” in 2018\nIEEE Third International Conference on Data Science\nin Cyberspace (DSC).\nIEEE, 2018, pp. 781–787.\n[26] X. Y. Lee, S. Ghadai, K. L. Tan, C. Hegde, and\nS. Sarkar, “Spatiotemporally constrained action space\nattacks on deep reinforcement learning agents,” in The\n34th AAAI Conference on Artificial Intelligence, 2020,\npp. 4577–4584.\n[27] S. Baluja and I. Fischer, “Learning to attack: Adver-\nsarial transformation networks,” in Proceedings of the\nAAAI Conference on Artificial Intelligence, vol. 32,\nno. 1, 2018.\n[28] B. Dong, A. S. Varde, D. Li, B. K. Samanthula, W. Sun,\nL. Zhao et al., “Cyber intrusion detection by using\ndeep neural networks with attack-sharing loss,” arXiv\npreprint arXiv:2103.09713, 2021.\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2023-12-20",
  "updated": "2023-12-20"
}