{
  "id": "http://arxiv.org/abs/2312.13754v1",
  "title": "Cross-Layer Optimization for Fault-Tolerant Deep Learning",
  "authors": [
    "Qing Zhang",
    "Cheng Liu",
    "Bo Liu",
    "Haitong Huang",
    "Ying Wang",
    "Huawei Li",
    "Xiaowei Li"
  ],
  "abstract": "Fault-tolerant deep learning accelerator is the basis for highly reliable\ndeep learning processing and critical to deploy deep learning in\nsafety-critical applications such as avionics and robotics. Since deep learning\nis known to be computing- and memory-intensive, traditional fault-tolerant\napproaches based on redundant computing will incur substantial overhead\nincluding power consumption and chip area. To this end, we propose to\ncharacterize deep learning vulnerability difference across both neurons and\nbits of each neuron, and leverage the vulnerability difference to enable\nselective protection of the deep learning processing components from the\nperspective of architecture layer and circuit layer respectively. At the same\ntime, we observe the correlation between model quantization and bit protection\noverhead of the underlying processing elements of deep learning accelerators,\nand propose to reduce the bit protection overhead by adding additional\nquantization constrain without compromising the model accuracy. Finally, we\nemploy Bayesian optimization strategy to co-optimize the correlated cross-layer\ndesign parameters at algorithm layer, architecture layer, and circuit layer to\nminimize the hardware resource consumption while fulfilling multiple user\nconstraints including reliability, accuracy, and performance of the deep\nlearning processing at the same time.",
  "text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2024\n1\nCross-Layer Optimization for Fault-Tolerant Deep\nLearning\nQing Zhang, Cheng Liu, Member, IEEE, Bo Liu, Haitong Huang, Ying Wang, Member, IEEE,\nHuawei Li, Senior Member, IEEE, Xiaowei Li, Senior Member, IEEE\nAbstract—Fault-tolerant deep learning accelerator is the basis\nfor highly reliable deep learning processing and critical to deploy\ndeep learning in safety-critical applications such as avionics and\nrobotics. Since deep learning is known to be computing- and\nmemory-intensive, traditional fault-tolerant approaches based on\nredundant computing will incur substantial overhead including\npower consumption and chip area. To this end, we propose to\ncharacterize deep learning vulnerability difference across both\nneurons and bits of each neuron, and leverage the vulnerability\ndifference to enable selective protection of the deep learning\nprocessing components from the perspective of architecture layer\nand circuit layer respectively. At the same time, we observe\nthe correlation between model quantization and bit protec-\ntion overhead of the underlying processing elements of deep\nlearning accelerators, and propose to reduce the bit protection\noverhead by adding additional quantization constrain without\ncompromising the model accuracy. Finally, we employ Bayesian\noptimization strategy to co-optimize the correlated cross-layer\ndesign parameters at algorithm layer, architecture layer, and\ncircuit layer to minimize the hardware resource consumption\nwhile fulfilling multiple user constraints including reliability,\naccuracy, and performance of the deep learning processing at\nthe same time.\nIndex Terms—Cross-layer Optimization, Fault-tolerant Deep\nLearning Accelerator, Vulnerability Factor, Hybrid Architecture,\nSelective Redundancy.\nI. INTRODUCTION\nD\nEEP learning is continuously penetrating into broader\napplication fields beyond traditional areas such as com-\nputer vision and natural language processing. More and more\napplications involve safety-critical scenarios [1], [2], [5], [8],\n[9], [22], [23], [29], such as autonomous driving, aerospace,\nand robotics.Compared with conventional deep learning ap-\nplications, safety-critical deep learning applications have ad-\nditional requirements for reliability on top of accuracy and\nspeed. Moreover, reliability has become a key indicator for\nsuch deep learning applications, directly determining whether\nthey can be accepted for use. For example, the on-board\ndeep learning module for vehicles must meet the reliability\nThis work was supported in part by the National Natural Science Foundation\nof China under Grant No. 62174162 and No. 62090024, and the Space Trusted\nComputing and Electronic Information Technology Laboratory of Beijing\nInstitute of Control Engineering (BICE) under Grant OBCandETL-2022-07.\n(Corresponding author: Cheng Liu.)\nQing Zhang, Cheng Liu, Haitong Huang, Ying Wang, Huawei Li, and\nXiaowei Li are with both State Key Lab of Processors, Institute of Computing\nTechnology, Chinese Academy of Sciences, Beijing 100190, China, and\nUniversity of Chinese Academy of Sciences, Beijing 100190, China. (e-mail:\n{zhangqing22s, liucheng}@ict.ac.cn)\nBo Liu is with Beijing Institite of Control Engineering, Beijing 100190,\nChina.\nstandards specified in ISO26262 in order to be applied to the\nvehicle system [1], [5], [8]. As the core dedicated processor\nsupporting high-energy-efficient deep learning processing [11],\n[13], the deep learning accelerator (DLA), accompanied by\nhigher transistor integration and lower threshold voltage in\nadvanced nano-level processes, will inevitably be affected\nby soft faults, leading to abnormal errors in deep learning\ninference and even causing safety accidents [3], [7].Therefore,\ndesigning fault-tolerant DLAs to effectively tolerate faults\nand achieve high-reliable inference is crucial to driving the\napplication of deep learning in safety-critical fields.\nThe processing of deep learning is a typical compute- and\nmemory-intensive task. Although traditional chip reliability\ndesign methods such as triple modular redundancy (TMR)\ncan solve the problem of accuracy loss caused by faults,but,\na large amount of redundant computation can greatly affect\nthe processing speed, chip area, and power consumption of\ndeep learning. This conflicts with the fundamental design goals\nof deep learning accelerators, such as speed and energy effi-\nciency, also limits their applications in safety-critical areas.On\nthe other hand, the outputs of deep learning processing are\ntypically discrete, and many small computational errors have\nlittle impact on the inference results of deep learning. Ad-\nditionally, deep learning processing involves many non-linear\nfunctions, which can filter out many intermediate calculation\nerrors and mitigate the impact of soft errors on inference\nresults. These features make deep learning processing more\nfault-tolerant compared to general-purpose computing. Many\nreliability design works for deep learning accelerators (DLA)\nutilize this natural fault-tolerant property of deep learning to\nreduce the fault-tolerant cost of deep learning, achieving a\njoint optimization of accuracy, reliability, and performance.\nOne major approach is to use selective redundancy pro-\ntection based on the differential sensitivity of different parts\nof deep learning models to faults, thereby reducing the cost\nof fault tolerance without compromising the reliability and\naccuracy of deep learning computations. C. Schorn et al. [26]\nand Liu et al. [3], [12] propose to divide the computation\narray of DLA into different reliability regions for processing\nneurons of different importance, so that corresponding fault-\ntolerant protection can be applied to different regions to\nreduce the cost of fault tolerance. H. R. Mahdiani et al. [31]\npropose to focus on protecting the high-order calculation part\nof multipliers to enhance the overall reliability of DLA. Some\nother works directly apply selective redundant computation\nat the model or algorithm level [6], [14], [28], [38], [39]\nto improve the fault tolerance of deep learning and reduce\narXiv:2312.13754v1  [cs.AR]  21 Dec 2023\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2024\n2\nthe cost of fault tolerance.These works indicate that we can\nexplore the inherent fault-tolerance differences in deep learn-\ning algorithms, accelerator architectures, circuits provide more\nprotection for vulnerable parts to reduce fault-tolerance costs.\nSince different abstraction levels involve different information,\nthere are significant differences in the corresponding fault-\ntolerance effects and costs. The circuit layer can accurately\nimprove the fault tolerance of overall design but lacks flex-\nibility and is difficult to directly use information from the\napplication and architecture layers, and the fault tolerance\ncost is often relatively high. The algorithm layer can make\nbetter use of the information in the model layer, customize\nfault-tolerance choices for specific models, and reduce fault-\ntolerance costs but lacks low-level details. In cases where\nthe Fault Rate is high and reliability requirements are also\nhigh, the fault-tolerance cost will increase sharply. The fault\ntolerance characteristics of the architecture layer are between\nthe circuit and algorithm layers, with only some algorithm\nand circuit layer information ,but still lacking a unified fault-\ntolerant deep learning architecture design. Previous DLA fault\ntolerance usually improved the reliability of DLA from one\nor two layers, lacking a system of cross-layer collaborative\noptimization and unable to fully utilize the advantages of\ndifferent layer fault-tolerance designs. In addition, the relation-\nships between cross-layer parameters are complex, the design\nspace is large and manual design is not only inefficient and\nerror-prone but also difficult to achieve global optimization,\nand it is challenging to quickly provide optimization solutions\nfor different application requirements.\nTo address the above issues, we propose a cross-layer\noptimization design framework for fault-tolerant DLAs. At the\nmodel level, we analyze the sensitivity of deep learning com-\nputations to soft faults from two dimensions, neuron dimension\nand bit dimension. We categorize deep learning computation\ntasks into important neuron computations and ordinary neuron\ncomputations, and further divide neuron data into important bit\nand ordinary bit data to guide fine-grained selective protection\nof fault-tolerant DLA. At the architecture level, we propose a\nheterogeneous computing framework based on recomputing to\nprocess important neuron computations and common neuron\ncomputations separately, which supports selective protection\nof neuron computations. At the circuit level, we propose a\nbit-wise protected circuit redundancy design that only protects\nthe important bit logical of the computing unit, and provides\ndifferent circuit level redundancy protection designs for two\ntypes of computing arrays based on the sensitivity analysis\nof the algorithm layer, further reducing redundancy costs. In\naddition, we propose for the first time to reduce the cost of\ncircuit-level bit-wise redundancy protection by quantization\nconstraint, which limits the direct association logic size of\nimportant bit of deep learning neurons. Finally, according to\nthe requirements of the target application, we automate the\ndesign selection of different layers and optimize the reliability,\nresource consumption, and performance of fault-tolerant DLA\ndesign.\nThe contributions of this work are summarized as follows:\n1) We analyze the sensitivity of deep learning models to\nsoft faults in two dimensions: different neurons and\nDifferent bits of neurons. Based on this analysis, we pro-\npose fault-tolerant design of DLA across circuit, archi-\ntecture, and algorithm levels, while optimizing multiple\ndesign goals including reliability, resource consumption,\nand performance.\n2) We propose for the first time the combination of a\nquantization strategy using deep learning models and\nbit redundancy design in DLA computing units. By con-\nstraining the quantization choices of the model, we can\nreduce the size of high-bit circuits that need protection\nwithout sacrificing accuracy. This leads to a lower cost\nof redundancy design at the circuit level for DLA.\n3) We have designed a fault-tolerant heterogeneous deep\nlearning accelerator architecture based on recomputa-\ntion. This architecture achieves fine-grained selective\nprotection at the neuron level by taking into account\nthe sensitivity differences of neurons to soft errors. It\nalso tolerates the distribution differences of important\nneurons, reducing fault tolerance costs while ensuring\ncomputational accuracy.\n4) The experiments demonstrate that the cross-layer opti-\nmization design method proposed in this paper exhibits\nsignificant advantages in terms of reliability, resource\nconsumption, and overall performance in DLA, com-\npared to selective fault-tolerant designs at the individual\ncircuit layer, architecture layer, and algorithm layer.\nII. RELATED WORK\nFault-tolerant design of DLA is the foundation to ensure\nthe reliability of deep learning inference. In order to improve\nthe reliability of DLA, many previous works have proposed\nvarious fault-tolerant design methods at different levels of\nabstraction such as circuit, architecture, and algorithm. We\nwill introduce the relevant research works separately.\nAt the circuit level, traditional fault-tolerant coding tech-\nniques such as Error Correction Code (ECC) can be used\nto protect on-chip caches, and redundancy can protect con-\ntrol and computational logic. In order to reduce the cost\nof direct redundancy, H.R. Mahdiani et al. [31] proposed\ngiving higher priority protection to the high-order bits of deep\nlearning computational units while ignoring the logic of low-\nbit positions to reduce the cost of redundant protection. Some\nworks [40]–[42] proposed using logic circuits such as random\ncomputing and approximate computing to replace traditional\nbinary computing logic to enhance fault tolerance or improve\ncomputational energy efficiency. Brandon Reagen et al. [10]\nproposed using Razor circuits to detect timing faults caused\nby voltage drops, and then using the inherent fault-tolerance\nproperties of deep learning models to add word and bit masks\nto repair SRAM data errors, thereby improving computational\nenergy efficiency without sacrificing accuracy. J.J. Zhang et al.\n[20] proposed adding a constant 0 bypass to the calculation\nunit of DLA. When the corresponding calculation unit has\na hard fault, it can be bypassed as a constant 0 to mitigate\nthe large numerical fluctuations and precision loss caused by\nfaults. J.A. Clemente et al. [34] proposed adding redundant\nconnections to the Hopfield neural network accelerator and\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2024\n3\nusing voting logic to implement error correction. This strat-\negy can significantly reduce fault-tolerant design overhead\ncompared to triple modular redundancy. These works mainly\nenhance fault tolerance through circuit design, but also require\nfine-tuning of the numerical bias introduced by the circuit layer\nin the algorithm or model layer.\nAt the architecture level, Liu et al. [3], [12] proposed a\nheterogeneous computing architecture to solve the problem of\narbitrary hard faults in the DLA computation array, using a dot\nproduct computation array different from the two-dimensional\npulsating array to achieve re-computation of tasks on any\ncomputing unit. C. Schorn et al. [26] proposed dividing the\nDLA’s computation array into highly reliable computation\nregions and ordinary computation regions, which are used to\nprocess fault-sensitive and fault-insensitive computing tasks,\nrespectively. However, the distribution of fault-sensitive and\ninsensitive computing tasks usually changes with the model\nor even the input, so some detailed microstructure support is\nstill lacking. Zhen Gao et al. [30] proposed adding integrated\nlearning units on top of the deep learning accelerator, which\nallows for the parallel computation of multiple lightweight\ndeep learning models to tolerate hardware faults and improve\ninference reliability. However, it depends on the redesign of the\nmodel. E. Ozen et al. [25] proposed adding parity check units\non the deep learning computation array, utilizing algorithm-\nbased fault tolerance (ABFT) technology to achieve real-time\nfault detection and correction, which is limited to low fault-\nrate scenarios due to the fault-tolerant and error-correcting\ncapabilities of ABFT technology.\nAt the algorithm level, many works enhance the fault\ntolerance of deep learning models to hardware failures by\nmining their own fault tolerance and parameter redundancy,\nwhile ensuring accuracy through changing model parameters\nor even structure, without changing the DLA hardware design\n[17], [28], [36], [37]. These works usually rely on fault-\ntolerant training to change deep learning model parameters or\narchitecture. Such model training-based fault-tolerant designs\noften depend on sample data and a large amount of retraining,\nand are sensitive to fault rates, making it difficult to fully\nconsider during the training phase. In contrast to fault-tolerant\nmethods that heavily rely on large amounts of application\ndata training, some methods improve model fault tolerance by\nusing equivalent computational methods [6], [24], introducing\nnew activation functions or numerical constraints [32], [33],\n[35], or using checksum mechanisms for error correction [24].\nThese methods generally require only a small amount of data\nfine-tuning or even no application data, achieving high pre-\ndiction accuracy at a low computational cost under low fault\nrates, which is very attractive. However, when the hardware\nfailure rate is high, reliability drops sharply, and the cost of\nfault tolerance also significantly increases. Another orthogonal\nclass of techniques to the fault tolerance methods that exploit\nthe fault tolerance of the model itself is redundancy protection.\nTo avoid the high computational cost of direct redundancy,\nmany works further analyze the differences in internal fragility\nof deep learning models [15], [16], [18], [21]–[23], [26]and\nreduce the cost of redundancy protection through differential\nprotection methods [26]–[28], [38], [39] . Since deep learning\ncomputation engines generally support layered computation\npatterns, the differences in fragility between model layers\ncan be easily applied to various DLA architectures. Corre-\nspondingly, the analysis of deep learning fragility focuses on\nthe differences between layer fragilities [15], [16], [21], [22],\nwhile selective protection focuses on layer redundancy, which\ncan be achieved through both spatial and temporal redundancy.\nTheoretically, a more granular analysis of fragility differences\n[26] would be helpful for more efficient selective protection,\nbut currently, there is still a lack of DLA architecture and\ncircuit support.\nAs can be seen from the above, most fault-tolerant tech-\nniques for deep learning focus mainly on one level of circuit,\narchitecture, or algorithm. Different techniques have signifi-\ncant advantages and disadvantages, and there are also various\nlimitations in their usage scenarios. Although there are also\nsome fault-tolerant techniques that involve the coordination\nof different levels of fault tolerance to improve inference\naccuracy or reduce fault tolerance costs.The computation unit\nbypass technology proposed by J.J. Zhang et al. [20][20] can\nbe combined with model training to enable the model to match\nspecific fault bypass settings, thereby reducing accuracy loss\ncaused by bypass. However, there is still a lack of system-\natic cross-layer optimization design methods for fault-tolerant\nDLAs oriented towards soft errors. Therefore, based on the\nidea of cross-layer chip optimization design, this paper first\nproposes sensitivity analysis from two dimensions, namely,\nneurons dimension and the different bits of nuerons dimension,\nat the algorithmic layer. Based on this, selective redundancy\nprotection technologies are explored at the architecture layer\nand circuit layer, respectively, to minimize fault-tolerant design\ncosts while ensuring reliability and performance.\nIII. CROSS-LAYER OPTIMIZATION FOR FAULT-TOLERANT\nDEEP LEARNING ACCELERATORS\nTo address the multi-objective design requirements for fault-\ntolerant DLA in various scenarios, we propose a systematic\ncross-layer optimization design framework that integrates the\nfault-tolerant advantages of circuit layer, architecture layer,\nand algorithm layer, while minimizing fault-tolerant cost under\nthe premise of meeting inference accuracy and performance.\nWe will first introduce the cross-layer optimization frame-\nwork for fault-tolerant DLA, and then expand on the fault-\ntolerant design strategies corresponding to the algorithm layer,\narchitecture layer, and circuit layer. Finally, we demonstrate\nan automated cross-layer parameter design space exploration\nmethod.\nA. Overall Architecture\nThe overall architecture for cross-layer optimization of\nfault-tolerant DLA is shown in Figure 1. First, the user\nneeds to determine the design objectives and constraints, such\nas performance, reliability, and resource costs. Reliability is\nusually measured by accuracy under fault scenarios, to some\nextent overlapping with the accuracy metric of the model.\nResource costs mainly include additional chip area introduced\nby fault-tolerant design.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2024\n4\nAfter the design goals and constraints are given, the frame-\nwork analyzes the sensitivity of different neurons to soft faults\nat the algorithm level, and uses this as a basis to divide\nthe neural computation in deep learning into important and\ncommon parts. Compared to ordinary neural computation,\nimportant neural computation is more sensitive to soft faults\nand leads to greater loss in model accuracy, therefore requiring\nstronger fault-tolerant design. On this basis, we further refine\nthe sensitivity of neurons to soft faults from the dimension\nof bit width, where high bit-width errors in neurons will\ncause larger numerical deviations and have a greater impact\non model accuracy, thus requiring relatively stronger fault-\ntolerant protection for high bit-width computation as well. In\nsummary, we divide deep learning computation into important\nand ordinary parts from two dimensions: neurons and data\nbit-widths.\nIn addition, model quantization is a crucial step in achieving\nhigh energy efficiency in deep learning processing, enabling\nthe use of lower data bit-widths for computation. To support\nfixed-point quantization, basic computation units in DLA such\nas multiply-accumulate units typically need to be truncated\naccording to the quantization. Truncation not only affects\nprecision but also directly impacts the computational logic size\nof the calculation unit output values in different bit-widths,\nas well as the selective redundancy protection cost in DLA\ncircuitry layer, which will be detailed in our redundancy design\nat the circuitry layer.\nAlthough we divide the processing of deep learning into\nimportant neuron computations and common neuron compu-\ntations at the algorithmic level, it is difficult to separate and\nperform different fault-tolerant protections on important and\ncommon neuron computations due to the typical streaming\ncomputation used in DLA architecture design and the need\nfor further partitioning (tiling) of deep learning models for\ncomputation on DLA through time division and reuse. Addi-\ntionally, there are significant differences in the distribution of\nimportant neuron computations across different deep learning\nmodels and the distribution of important neuron computations\nin different partitions of the same deep learning model, further\nincreasing the difficulty of separating important and regular\nneuron computations in DLA architecture. To address this is-\nsue, we extended the HyCA architecture [3], [12]and proposed\na flexible and configurable heterogeneous DLA architecture,\nFlexHyCA, which uses a conventional 2D computing array to\nprocess regular neuron computations and a heterogeneous dot\nproduct processing unit (DPPU) to process a small amount of\nimportant neuron computations that require higher reliability.\nBy loading real-time distribution information of important\nneurons on the 2D array and segmenting the computation of\ndifferent neurons, DPPU can flexibly choose to reuse the data\nloaded in the 2D array cache or directly read the required data\nfrom DRAM based on the proportion of important neurons,\nensuring that DPPU will not block the 2D array computation\nand eliminating the impact on DLA performance.\nIn response to the difference in fault sensitivity between\nimportant and ordinary bits of neurons in deep learning\nmodels, we designed selective bit protection circuits on the\ncomputing units of FlexHyCA. Only the important bits of\nFig. 1: Design framework for cross-layer optimization\nordinary neurons and the directly related logic circuits of\nimportant neurons’ important bits were redundantly protected\nto reduce the cost of fault-tolerant design.\nWe found that the choices of fault-tolerant design at differ-\nent levels can influence each other. For example, the proportion\nof important neurons at the algorithmic level directly deter-\nmines the size of the DPPU in the FlexHyCA architecture,\nwhich affects the hardware resource cost. Similarly, the num-\nber of important bits in neurons also corresponds to the size of\nlogic circuits that need to be protected, ultimately affecting the\ncost of bit protection circuit design. The quantization choices\nof the model also affect the truncation of the computational\nunit, thus impacting the size of the circuit that needs to be\ncovered by fault tolerance and the associated cost. The ratio\nof important neurons to ordinary neurons also affects the\nnumber of important bits in each, and these design param-\neters interact with each other, making manual optimization of\nfault-tolerant DLAs very challenging and difficult to address\nthe diverse needs of users’ design goals and constraints in\ndifferent scenarios. Therefore, we introduce a design space\nexploration mechanism based on the Bayesian algorithm, and\nuse the mutual relationships of local parameters to prune the\ndesign space, achieving rapid automated cross-layer parameter\noptimization.\nB. Algorithm-level fault-tolerant design\nWe first analyze the differences in the sensitivity to soft\nfaults between models from two dimensions: neuron compu-\ntation and nueron bit-width. The neural network model can\nbe viewed as a complex functionf(x) where x represents\ninput data. By first-order Taylor expansion of the function, as\nshown in Equation (1), the first-order error f(x + ∆x) −f(x)\ncaused by perturbation near the input data is proportional to the\nperturbation size∆x and the gradient valuef ′(x). Therefore, it\ncan be approximated that the sensitivity of neurons is related to\ntheir gradient on data input. Neurons with larger gradients may\ncause larger numerical fluctuations due to perturbations, which\nmay have a greater potential impact on the model accuracy.\nIt can be considered that the corresponding neurons are more\nsensitive to faults, and we define them as important neurons.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2024\n5\nSimilar methods for analyzing neuron importance have also\nbeen applied in [26][44].\nf(x + ∆x) −f(x) = ∆xf ′(x) + O(∆x)\n(1)\nWe identify the top S TH% neurons with the highest\ngradient values as the important neurons, and provide detailed\ngradient-based neuron importance analysis in Algorithm 1.\nAlgorithm 1: Gradient-based important neuron selec-\ntion algorithm.\nInput: Neural network model:M; Dataset: D; Total\nnumber of neurons: N; Important neuron\nproportion: S TH;\nOutput: set of important neurons: S\nGetImportantNeurons(M, D, N, S TH);\nInitialize the gradients[N] of each neuron;\nfor i ←1 to N do\nforeach input ∈D do\noutput ←inferenceModel(M, Input);\ngrad ←backwardModel(M, Output);\ngradients[i] ←gradients[i] + Abs(grad) ;\nend\nend\nSort gradients in descending order;\nSelect top N × S TH neurons based on gradients;\nPut the selected neurons in S;\nreturn S;\nDue to the greater numerical disturbance caused by high-bit\nflips than by low-bit flips, we define the high NB TH bits of\neach neuron as the important bit positions of ordinary neurons.\nImportant neurons are more affected by flips and require more\nprotected bits, so the high IB TH bits of important neurons\nare defined as the important bit positions of important neurons.\nDue to the limited data bit width of fixed-point deep learning\nmodels and the higher importance of high bits than low bits,\nwe always prioritize the protection of high bits for neurons of\nthe same type. The compromise between important neurons\nand ordinary neurons depends on the model’s precision and\nthe cost of bit protection. We choose the settings with the\nlowest protection cost while meeting the precision require-\nments. Since the number of combinations of IB TH and\nNB TH settings is not large for fixed-point models with\nlimited data bit width, we use a simple enumeration algorithm\nto determine IB TH and NB TH, where the precision is\nobtained through fault injection experiments and the cost of\nbit protection can be obtained through logic synthesis of the\nunits. The specific design and cost evaluation of bit protection\nwill be described in detail in the subsequent circuit-level fault-\ntolerant design section, and the optimization of important bit\nposition settings is shown in Algorithm 2.\nQuantization is a crucial step in deep learning model quan-\ntization, which not only affects the accuracy of the model but\nalso affects the high-level logic in DLA computing units. For\nquantized DLAs, the output data bit-width of the computing\nunit is usually larger. For example, in an 8-bit DLA, the output\ndata bit-width of the computing unit is at least 16 bits. To\nAlgorithm 2: Bit importance evaluation\nInput: Neural network model:M; Dataset: D; Set of\nImportant Neuron:N; Neuron data bit width:B;\nModel Accuracy Objective:ACC;\nOutput: The important bits of the important neurons\nare the high IB TH bits; The important bit\nin regular neurons is the high NB TH bit.\nGetBitConfig(M, D, N, IB TH, NB TH, ACC);\nfor IB TH ←1 to B do\nfor NB TH ←1 to B do\nacc ←RunModel(M, N, D, ib th, nb th);\ncost ←getCost(M, N, ib th, nb th);\nif acc > ACC and cost < opt cost then\nopt ib th, opt nb th ←ib th, nb th;\nopt cost ←cost;\nend\nend\nend\nreturn opt ib th, opt nb th;\nprevent overflow, many DLA designs choose settings larger\nthan 16 bits, even up to 32 bits. Since the calculations of\nsubsequent layers in the model are still 8 bits, the output of\nthe computing unit needs to be truncated during the calculation\nprocess according to the model’s quantization. This means that\nthe data bits outside of the truncation have little impact on\nthe model inference, and the importance of the logic circuits\ndirectly related to these bits is relatively low. Similarly, the\nimportance of the logic circuits corresponding to the ordinary\nbit positions in the output data is also relatively low. However,\nthe quantization selection of general deep learning models has\nno limitations. Different deep learning models and different\nlayers of the same model may have different quantization\nselections, which leads to different important logic circuits in\nthe computing unit. In order to ensure the worst-case scenario\nin DLA design, the weight of the important logic circuits is\nsignificant. Figure 2 shows the positions of the important bit\npositions of the accumulators and multipliers corresponding\nto different quantization selections, as well as the direct\ncalculation logic areas. Assuming an input data bit width of 8\nbits, a multiplier output of 16 bits, and an accumulator output\nof 24 bits, the top 2 bits of the output data are important bit\npositions. When the output data location intercepted by the\naccumulator is from the 2nd to the 9th bit, the top 2 bits of\nthe output data are important bit positions. The corresponding\ndirect calculation logic corresponds to the blue area in Figure\n2(a), where the important calculation logic of the multiplier\nincludes a column of 8 (1-bit sums) and a column of 7 (1-bit\nsums). Without quantization constraints, the bit positions 6 to\n15 of the multiplier output may all be important, and the circuit\narea that needs protection corresponds to the entire red dotted\nline area, which will inevitably lead to high redundancy costs.\nWe define the lowest bit of the quantized truncated data as\nQ scale. When we set Q scale=5, the quantization is limited\nso that the range of the truncated data bits is reduced to the\n5th to 24th bits. The range of the top 2 bits of the multiplier is\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2024\n6\nfrom the 11th to the 15th bit. The calculation logic area that\nneeds to be protected in the multiplier can be significantly\nreduced, as shown by the red dotted line area in Figure\n2(b). When we perform three-mode redundancy protection on\nthe important calculation logic, quantization constraints can\nsignificantly reduce the cost of redundancy protection.\n(a) Data with unquantized limits and accumulator truncation can be\nlocated at any contiguous position of 24-bit data\n(b) There are quantization constraints, and the data truncated by the\naccumulator can be located at any continuous position between the\n5th and 23rd bits\nFig. 2: The range of the important bits of the accumulators and\nmultipliers in DLA, as well as the directly associated compu-\ntational logic, vary under different quantization constraints.\nC. Architecture-level fault-tolerant design\nIn order to support selective fault-tolerant protection, the\ncore issue faced by the design of the DLA architecture is\nhow to differentiate between important neuron computations\nand normal neuron computations on top of the basic DLA\narchitecture, and provide stronger fault-tolerant protection for\na small number of important neurons. Specifically, the archi-\ntecture design must also be able to tolerate the variability in\nthe distribution of important neurons. The variability in the\ndistribution of important neurons is mainly reflected in three\naspects: (1) the distribution of important neurons in different\ndeep learning models varies; (2) the distribution of important\nneurons in different layers of the same deep learning model\nis also not the same; (3) the convolution of the same deep\nlearning model in the same layer often needs to be partitioned\nFig. 3: FlexHyCA Architecture\nand mapped to the computational array of DLA in a time-\nshared manner due to the limited resources of DLA, and the\ndistribution of important neurons on different task slices will\nstill vary.\nTo achieve this, we utilize HyCA, proposed by Liu et al.\n[3][12], for arbitrary position computing unit fault tolerance\nin DLA, as the basis infrastructure to separate the compu-\ntation of important neurons from ordinary ones. Since most\nof the neurons are classified as ordinary, we use the two-\ndimensional computation array of HyCA to process ordinary\nneuron computation, in order to better reuse data and weights.\nFor a small number of sparsely distributed important neurons,\nwe dynamically load them into the heterogeneous dot product\nprocessing unit (DPPU) for processing, fully utilizing the\ninternal parallelism of neuron computation. Since the DPPU\nof HyCA needs to reuse data in the dedicated cache of\nthe two-dimensional array, when the proportion of important\nneuron computation that needs to be protected is too high,\nthe DPPU will block the computation of the two-dimensional\narray. On the other hand, the proportion of important neuron\ncomputation varies greatly, and setting the DPPU size for\nworst-case scenarios will introduce significant area overhead.\nTo address this issue, we have added a more flexible data\nloading mechanism to the DPPU on top of HyCA. When the\nproportion of important neuron computation is too high, the\nrequired data is loaded directly from DRAM, avoiding the\nperformance degradation caused by DPPU blocking the two-\ndimensional array computation, while only adding a small\namount of I/O. When the distribution of important neurons is\nrelatively uniform, FlexHyCA is compatible with the original\nHyCA design, prioritizing the reuse of data in the two-\ndimensional array cache to avoid additional I/O. This new\nFlexHyCA architecture can better adapt to the problem of\ndifferences in the distribution of important neurons, as shown\nin Figure 3.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2024\n7\nD. Circuit-level fault-tolerant design\nTo support selective fault-tolerant protection, we propose\na configurable bit protection design for the basic multiply-\nadd units in DLA based on the differences in importance\nof individual bit positions in the neurons. We provide triple\nmodular redundancy (TMR) protection for logical calculations\non important bit positions of neurons to avoid significant\ncomputation biases caused by soft faults. Since the core of\nthe multiply-add unit is the multiplier section, we illustrate\na configurable bit-protected multiplier based on an 8-bit basic\nmultiplier as an example. As shown in Figure 4, the multiplier\noutput data width is 16 bits. When the 8-bit data truncated by\nthe accumulator corresponds to the [m:n] bits of the multiplier\noutput, the most significant bits of the multiplier correspond\nto the [m:m+s-1] bits. Therefore, we consider the computation\nlogic from column m-s+1 to column m as the protected area.\nSince the position of m varies with the choice of quantization,\nwe need to protect the entire red area indicated in Figure\n4. However, for a specific choice of quantization, we only\nuse a continuous s columns in the red dashed line area for\ncomputation. To reduce overhead, we only need to protect the\ntwo largest blue columns in the area. For different choices of\nquantization, we need to change the Mux selection to enable\nthe redundancy units to protect the corresponding columns.\nHowever, the computation tasks on the left of the red dashed\nline area are much smaller than the largest two computation\ntasks. When a simple Mux replacement strategy is used, there\nwill be significant waste of redundancy array when the left two\ncolumns are selected for computation in quantization, and the\nredundancy calculation units correspond to a large fan-out of\nsignals, introducing additional delay and area. To solve this\nproblem, we merge adjacent calculation units on the left, and\nif any column of the merged units is selected as an important\ncomputation unit, the merged units will be protected, further\nimproving the utilization of redundancy units and reducing\nthe fan-out of signals. When s=1 and Q scale=2, as shown in\nFigure 4, the maximum fan-out of the redundancy calculation\nunits for the leftmost 3 columns is reduced from 6 to 4. When\nquantization results in the most important column being in the\nleft area, redundancy calculation can also be used to protect\nmore computation columns. Although Figure 4 only shows the\neffect of bit protection for a conventional shift multiplier, the\nbit protection strategy can also be applied to other multiplier\nstructures. Figure 4(b) illustrates an example of applying the\nbit protection strategy to a Wallace tree multiplier.\nE. Cross-layer spatial exploration design\nDue to the interconnected of design parameters across\ndifferent layers, optimizing each layer separately is difficult\nto achieve the optimal design goal. Therefore, we constructed\na unified cross-layer optimization space to collaboratively op-\ntimize the design parameters of different layers. We specified\nthe accuracy requirement and performance loss of the model\nunder a designated fault rate as the user’s design constraints,\nwhile minimizing the additional chip area introduced by\nredundancy protection. The design parameters of the algorithm\nlayer mainly include the percentage of important neurons\n(a) The schematic diagram of the shift multiplier bit protection\n(b) The schematic diagram of the Wallace tree multiplier bit protec-\ntion\nFig.\n4:\nConfigurable\nbit-protecting\nmultiplier\nschematic\n(Q scale=2).\nin the model (S TH%), the selection strategy of important\nneurons (S policy), the number of important bit positions\nfor important neurons (IB TH), the number of important bit\npositions for ordinary neurons (NB TH), and the quantization\ntruncation parameter (Q scale). The design parameters of the\narchitecture layer mainly include the two-dimensional array\nsize (Array size) and dot product array size (Dot size) of\nFlexHyCA, and whether the heterogeneous array data is reused\nor not (Data reuse). The design parameters of the circuit layer\ninclude the implementation strategy of bit redundancy for\ncomputing units (PE policy), such as direct redundancy or\nconfigurable redundancy.\nThe cross-layer parameter design space for fault-tolerant\nDLA is large, and many parameters will affect different layers.\nIt is difficult to achieve complex design goals manually, and\nthe design efficiency is relatively low. To solve the problem\nof cross-layer parameter optimization, we first formalized\nthe cross-layer parameter optimization problem. As shown in\nEquation 2, all design parameters are uniformly represented\nby a vector V, and each component corresponds to the design\nparameters of different layers. We use performance, reliability,\nand accuracy as constraints, set the typical fault rate as the\nreliability and accuracy indicator for model inference, and\nminimize the chip area introduced by redundant design as the\noptimization goal.\nTo solve the problem of cross-layer parameter optimization\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2024\n8\nfor fault-tolerant DLA, we use the Bayesian optimization\nalgorithm as the basic space exploration method and imple-\nment it as shown in Algorithm 3. We also added a limit on\nthe maximum iteration number (ITER MAX STEP). Each\niteration needs to evaluate the corresponding configuration’s\nreliability, accuracy, performance, and redundant chip area. We\nuse Scale-Sim [19] to evaluate performance, Synopsys Design\nCompiler to evaluate the area of bit-protected computing\nunits. Due to the repetition of many bit-protected options\nin the design space, we pre-evaluated the cost of possible\nbit-protected designs and constructed an area cost table. In\nthe Bayesian optimization process, different configurations\nof FlexHyCA area can be quickly obtained by looking up\nthe table. We use our self-developed fault injection simu-\nlator to evaluate the model’s accuracy under different fault\nrates. To speed up the search of the design space, we also\nuse empirical parameters to prune some parameters in the\ndesign space in advance. For example, more bit protection\ncan improve the model’s fault tolerance, and under the same\nfault rate, the corresponding accuracy is higher, but it will\nalso introduce higher redundant protection costs. Essentially,\nthe bit protection parameter is monotonically increasing with\nboth accuracy and chip redundancy. Similarly, with other\nsettings unchanged, the proportion of important neurons and\nthe accuracy and redundancy costs are also monotonically\nincreasing. This empirical information can be used to quickly\nprune design choices that do not meet the constraints. For\nexample, if the bit protection settings violate the accuracy\nor reliability requirements, fewer bit protections will also\nviolate the accuracy or reliability requirements, so such design\nparameters can be skipped without evaluation.\nV = S TH, IN TH, NB TH, Q scale,\nS policy, Array size...\n(2)\narg min\nvϵV Area\ns.t\nACChigh ≥0.97ACC0\nACClow ≥0.95ACC0\nPerf ≤1.10Perf0\nBandwidth ≤1.10Bandwidth0\nIV. EXPERIMENT\nA. Experiment Setup\nThis section mainly introduces the experimental settings\nfrom the perspectives of optimization objectives and exper-\nimental constraints, dataset and model testing benchmarks,\nhardware implementation configuration, and software simu-\nlation configuration.\nUser optimization objectives and design constraints: We\nrefer to prior works such as [10], [22] and use the bit error\nrate (BER) to describe the probability of soft errors, although\nit represents to some extent the probability of single particle\nflips occurring in each storage unit containing a cache and a\nregister on-chip, it is actually an abstract fault rate for deep\nlearning applications that differs from specific target hardware.\nFor two scenarios, fault rate I (BER = 1E-4) and fault rate\nII (BER = 2E-4), we set two different reliability/accuracy\nAlgorithm 3: Gradient-based important neuron selec-\ntion algorithm.\nInput: Design Parameter Space for Cross-Layer\nDesign:V ; Design Constraints: R;\nOutput: Optimal Parameter Selection:v\nBayesDesignOpt(V, R); v ←sample(V), step ←0;\nwhile step < ITER MAX STEP do\nArea, Acc, Perf ←getDesignV al(v);\nv ←bayesOptStep(v, V, Area), step ←step + 1;\nif not meetRestriction(Acc, Perf, Bandwidth, R)\nthen\nv ←bayesOptPurning(v, V );\nend\nend\nreturn V\nconstraints. Compared to a fault-free deep learning model, the\naccuracy loss is less than 3% under fault rate I and less than\n5% under fault rate II, with a performance loss and bandwidth\nloss both less than 10%. The chip area cost is minimized under\nthe premise of meeting accuracy, performance, and bandwidth\nconstraints.\nDataset and baseline models: In our experiments, we em-\nployed ImageNet as the dataset and VGG16 and Resnet50\nas typical deep learning models for benchmark testing. Both\nmodels were quantized using 8-bit integer quantization, re-\nsulting in quantized model accuracies of 72.95% and 75.96%,\nrespectively.\nHardware experiment configuration: The 2D computing\narray in the FlexHyCA base design is fixed at 32x32, with\na weight cache of 512KB and a data cache of 256KB.\nThe basic computing unit uses a Wallace tree to implement\nmultiplication, and the accumulator data width is 24 bits. The\nparameters of the FlexHyCA 2D array are fixed and do not\nchange based on user constraints and target design, while the\ncache and computing array size of the 1D array are determined\nby the optimization tool. We modeled FlexHyCA in Verilog\nand obtained chip area under different configurations using\nSynopsys DC tools under the TSMC 65nm process.\nSoftware experiment configuration: We implemented per-\nformance simulation for FlexHyCA architecture based on\nSCALE-Sim [19] to evaluate the performance of deep learn-\ning under different hardware settings. We implemented fault\ninjection simulation in PyTorch to evaluate the accuracy loss\nof deep learning models under different fault rates. To facilitate\nthe implementation of fault injection in PyTorch, we referred\nto previous works such as [10], [22], [26], [28] and mainly\nperformed random bit flip fault injection on neurons and\nweights.\nComparison settings of fault-tolerant DLAs: To verify the\nproposed cross-layer optimized fault-tolerant design method\nfor DLAs, we compared the basic DLA design (Base), circuit-\nlevel selective triple modular redundancy design (TMR-CRT1,\nTMR-CRT2, TMR-CRT3), architecture-level selective triple\nmodular redundancy design (TMR-ARCH), algorithm-level\nselective triple modular redundancy design (TMR-ALG), and\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2024\n9\nthe cross-layer redundancy design proposed in this paper\n(TMR-CL) in terms of hardware reliability (accuracy), hard-\nware resource overhead, and performance. The circuit-level\ntriple modular redundancy design mainly protects all basic\ncomputing units of DLA, as in [31], by only protecting the\nhigh bit-width operation parts. We selected the high-1-bit triple\nmodular redundancy (TMR-CRT1), high-2-bit triple modular\nredundancy (TMR-CRT2), and high-3-bit triple modular re-\ndundancy design (TMR-CRT3). The architecture-level selec-\ntive triple modular redundancy (TMR-ARCH) mainly protects\nthe fault-sensitive layers based on the sensitivity difference\nbetween layers. Triple modular redundancy is implemented\nthrough spatial multiplexing and real-time voting selection.\nThe basic DLA is divided into three equal parts, and voting\nlogic is added. For the layers that are relatively less sensitive to\nfaults, normal calculations are performed. The algorithm-level\nselective triple modular redundancy (TMR-ARG) does not\nrequire changes to the DLA architecture and repeats the exe-\ncution of the fault-sensitive deep learning layers through time-\ndivision multiplexing. It can be easily implemented on deep\nlearning accelerators or general-purpose computing platforms\n[28]. The algorithm-level selective triple modular redundancy\nis essentially similar to the architecture-level selective triple\nmodular redundancy, but the architecture-level selective triple\nmodular redundancy can also be used to protect critical control\npaths [15]. However, since this experiment mainly focuses\non protecting data paths, the evaluation of architecture-level\nselective redundancy is not sufficient. For the cross-layer\noptimized design proposed in this paper, the design parameters\nare determined by the design space exploration process, and\nthe exploration process and optimized parameters will be\ndescribed in detail in Section 5.6.\nThe differential sensitivity of different layers in deep learn-\ning is a crucial basis for many selective redundant protection\nstrategies. Firstly, we obtained the sensitivity comparison\nof different layers in VGG16 and ResNet50 through fault\ninjection experiments. At a specified fault rate, we took the\nimprovement in accuracy of a single layer deep learning model\nunder complete protection relative to the situation where the\nentire model was unprotected as the sensitivity of that layer. To\nsimplify the analysis, we treated a block in ResNet as a whole\nfor analysis. Figure 5 shows the sensitivity data of different\nlayers in VGG16 and ResNet50 under fault rate I and fault\nrate II, indicating significant differences in sensitivity among\ndifferent layers, with the highest sensitivity layer differing\nfrom the lowest layer by more than 10 percentage points.\nBased on the sensitivity differences between model layers, we\nfurther analyzed the accuracy improvement brought by layer-\nby-layer protection according to sensitivity differences. The\naccuracy improvement curve is shown in Figure 6, from which\nwe can see that the speed of accuracy improvement is initially\nrapid and then becomes relatively slow, further demonstrating\nthe effectiveness of selective fault tolerance based on sensi-\ntivity analysis. Additionally, we can also determine the set\nof model layers that need to be protected at least based on\nuser reliability/accuracy requirements according to this figure.\nIn the experiment, both TMR-ARCH and TMR-ALG decided\nthe set of deep learning model layers or blocks that needed to\nbe protected primarily based on the aforementioned sensitivity.\n(a) VGG16\n(b) Resnet50\nFig. 5: The sensitivity of different layers/blocks of a deep\nlearning model under different fault rates.\nB. Overall Analysis of the Experiment\nIn order to optimize the objectives and constraints of the\nexperiment, we selectively applied protection at three levels:\ncircuit, architecture, and algorithm. We then compared the\nresults from three perspectives: reliability/accuracy, perfor-\nmance, and resource consumption with both the basic design\nand the cross-layer design proposed in this paper.\nReliability/Accuracy: As shown in Figure 7, we compared\nthe accuracy of typical fault-tolerant DLA designs under fault\nrates I and II in our experimental settings. Overall, different\nfault-tolerant strategies can meet the accuracy requirements\nfor use. However, designs such as TMR-ARCH, TMR-ALG,\nand TMR-CL are relatively flexible, allowing for the discovery\nof settings that precisely meet the user’s accuracy constraints.\nIn contrast, the granularity of TMR-CRT is relatively coarse,\nand the accuracy differences are significant. TMR-CRT1 was\nunable to meet the accuracy constraints under fault rate II,\nwhile TMR-CRT2 and TMR-CRT3 exceeded the user’s accu-\nracy requirements.\nPerformance: The performance data is shown in Figure 8.\nTMR-CRT does not alter the architecture of DLA or upper-\nlayer software, and does not result in any performance loss.\nTMR-CL adopts a strategy of distributing important neurons\nevenly across layers, so that the recalculations of important\nneurons are matched with the size of the DPPU, and the\ndifferences between blocks can be solved with a small amount\nof IO cost. Essentially, the high proportion of important\nneurons in one block can be shared with the low proportion of\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2024\n10\n(a) VGG16\n(b) Resnet50\nFig. 6: Changes in model accuracy due to sensitivity-based\nlayer/block protection at different fault rates.\nimportant neurons in another block, so the overall impact on\nperformance can be negligible. In contrast, at the architecture\nand algorithm levels, selective redundancy is achieved through\nspatial and temporal multiplexing, respectively, with little or\nno hardware cost, but theoretically, the sensitivity of the layers\nwhere redundancy is introduced will reduce performance to\none-third of the original. The sensitive deep learning model\nlayers have a relatively high proportion, which ultimately leads\nto an increase in execution time of nearly double, making it\nimpossible to meet design constraints.\nHardware resource overhead: We evaluated the additional\nchip area overhead introduced by typical fault-tolerant DLA\ndesigns. This paper mainly focuses on fault tolerance in the\ncomputing array, and unless otherwise specified, the chip\narea overhead only considers the computing array part. For\nease of comparison, the additional chip area overhead is\nnormalized relative to the original unprotected computing\narray area, and the experimental results are shown in Figure\n9. TMR-ALG essentially uses time-division multiplexing to\nprovide redundant protection for sensitive layers and does not\nincur additional hardware cost. TMR-ARCH mainly utilizes\nspace-division multiplexing at the architecture level to provide\nredundancy for sensitive layers, requiring a small number of\ncontrollers and selectors, resulting in a slight increase in area\ncompared to Base. Although TMR-CRT only needs to protect\nthe high-order bits, the specific location and quantization of the\nhigh-order bits are related, and the corresponding circuit logic\narea is large under the premise of unrestricted quantization,\nresulting in a high cost for circuit-level protection. TMR-\nCL takes into account the mutual influence of different layer\nparameters and significantly reduces the cost at the circuit\nlevel through quantization restrictions. On the other hand,\nby using the FlexHyCA architecture to separate and protect\na small number of important neurons, although the area\nis larger than Base and TMR-ARCH, it is still acceptable\nrelative to the benefits provided. Additionally, the TMR-\nCL is susceptible to the influence of uneven distribution of\nimportant neurons, which causes FlexHyCA to repeatedly load\nthe dependencies of data and weights for DPPU, thereby\nintroducing extra IO. Since other design schemes do not have\nextra IO growth, we will separately analyze the additional\nbandwidth consumption. The IO growth in TMR-CL mainly\ncomes from two aspects: (1) when the proportion of important\nneurons in a computing unit exceeds the computing power\nof DPPU in a block calculation, we no longer reuse the\ndata cached in the two-dimensional array, but directly load\nthe required data from DRAM to avoid blocking the two-\ndimensional computing array. (2) We need to calculate the\nposition of the two-dimensional array computing unit where\nthe important neurons are located during the compilation phase\nso that FlexHyCA can selectively recompute. Finally, for the\nVGG16 and ResNet50 models, the additional IO introduced\nby TMR-CL is 8.2% and 9.9% relative to the weight data of\nthe model itself, which will not have a significant impact on\nthe overall DLA design’s IO.\nC. Analysis of Algorithmic Layer Parameters\nIn order to better understand the relationship between the\ncore parameters of the algorithm, we analyze the relationship\nbetween the proportion of important neurons in the algorithm\nlayer and the number of important bit positions in the neurons,\nusing ResNet50 as an example. We also investigate the impact\nof model quantization constraints on model accuracy.\nTo study the relationship between the proportion of im-\nportant neurons and the number of important bit positions,\nwe set the proportion of important neurons to nine values:\nS TH=0.02, 0.05...0.3, ... , 0.35, 0.4. We also considered six\ncombinations of important bit positions for important neurons\nand regular neurons: ¡2,1¿, ¡3,1¿, ¡4,1¿, ¡3,2¿, ¡4,2¿, ¡4,3¿.\nThe experimental results, shown in Figure 10, indicate that\nthe proportion of important neurons has a significant impact\non model accuracy when there are fewer bit protections. As\nthe rate of important neurons increases, the accuracy also\nincreases. However, in the presence of faults, the accuracy\nimprovement shows a stage-like growth pattern. When the\nproportion of important neurons increases from 2% to 5%,\nthere is a noticeable improvement in accuracy. However, when\nthe proportion increases from 5% to 20%, the improvement\nis not significant. Only when the proportion increases to\n25% does the accuracy show a significant improvement. This\nsuggests that the model can achieve high accuracy with only\na small number of high bit positions and important neurons.\nHowever, further improvement in accuracy requires more bit\npositions and important neurons, which also means higher\nprotection costs. When there are more bit protections, the\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2024\n11\n(a) Accuracy of VGG16 in failure rate I\n(b) Accuracy of VGG16 in failure rate II\n(c) Accuracy of ResNet50 in failure rate I\n(d) Accuracy of ResNet50 in failure rate II\nFig. 7: Model accuracy optimized by different fault-tolerant DLA design strategies\neffect of the proportion of important neurons becomes very\nweak. Essentially, this is because after the high bit positions\nof the deep learning model are fully protected, faults in the\nlow bit parts have little impact on accuracy, whether they\nare important or regular neurons. However, the proportion of\nimportant neurons still plays a significant role in achieving\nideal accuracy. Overall, when the user’s requirements for\nreliability or accuracy are not very high, selecting a lower pro-\nportion of important neurons has a higher cost-effectiveness.\nFor high accuracy requirements, selecting more bit protections\nhas higher overall value, but we still tend to choose a lower\nproportion of important neurons.\nTo simplify the analysis of the relationship between model\nquantization constraints and model accuracy, we applied a uni-\nfied quantization constraint to the entire model. The accumu-\nlator data bit-width was set to 24 bits, and without constraints,\nthe data range was any continuous position from bit 0 to bit 23.\nAfter adding the quantization constraint, we set the lowest bit\nof the truncated data as Q scale. Figure 11 shows the effect of\nQ scale on model accuracy, where the quantization selection\nspace becomes smaller as Q scale increases, leading to a\ncorresponding decrease in model accuracy. From the figure,\nit can be observed that the model accuracy drops very little\nwhen Q scale is less than 7. As discussed in Section 2.2,\nincreasing Q scale can effectively reduce the logical circuit\nsize corresponding to important bit positions. This means that\nwithin the allowed range of accuracy, there is still a significant\namount of space to reduce the cost of selective bit protection.\nD. Analysis of Architecture layer parameters\nIn order to help understand the relationship between the\nmain design parameters at the architecture level, we fixed\nthe two-dimensional array of FlexHyCA to 32×32. Then, we\nanalyzed the changes in chip area under different DPPU sizes\nand important bit settings, normalized to the chip area of\nthe two-dimensional calculation array without protection. The\nexperimental results are shown in Figure 12. Although the\nDPPU adopts more bit protection than the two-dimensional\narray, the number of calculation units in the DPPU is usually\nmuch smaller than that in the two-dimensional array. There-\nfore, the overall proportion of chip area introduced is relatively\nlow. In contrast, the two-dimensional calculation array has\na larger number of calculation units, and the introduction\nof chip area increases significantly with the increase of bit\nprotection. Therefore, in the parameter optimization process,\nthe bit protection setting of important neurons can be set larger,\nwithout causing a significant increase in chip area. In contrast,\nthe bit protection setting of the two-dimensional calculation\narray should be kept as low as possible to control the overall\nredundant protection cost. In addition, if the DPPU is too large,\nit will inevitably introduce a larger chip area cost, but if it is\ntoo small, it will limit the proportion of important neurons.\nEssentially, the introduction of DPPU will only have good\nbenefits if the selective protection provided by DPPU can\ncover the demand for bit protection of the two-dimensional\narray and the area cost of DPPU itself.\nWe provide a direct data access path for DPPU, which\navoids the blocking problem caused by the reuse of data in the\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2024\n12\n(a) VGG16\n(b) Resnet50\nFig. 8: The execution time of the optimized models with\ndifferent fault-tolerant DLA design strategies.\nFig. 9: To compare the relative area of chips corresponding to\ndifferent fault-tolerant DLAs.\ntwo-dimensional array cache. However, this introduces addi-\ntional memory access. Meanwhile, the design of FlexHyCA\nrequires a significant amount of important neuron position\ninformation tables, which also leads to additional memory\naccess. Therefore, we further evaluated the increase in I/O\nunder different important neuron ratio settings and normalized\nit relative to the weight data. As shown in the figure, the\nadditional DRAM data access linearly increases with the\nthreshold design of important neurons. Thus, the additional\ndata access records the positions of these important neurons.\nWhen the threshold design of neurons (S TH) is 0.1, the\nadditional data loading exceeds 10%, which does not meet\n(a) Fault rate I\n(b) Fault rate II\nFig. 10: The relationship between the proportion of important\nneurons in ResNet50 and the number of important bit positions\nof neurons.\nFig. 11: The impact of the quantization constraint parameter\nQ-scale on the accuracy of the model.\nFig. 12: The impact of DPPU size and bit protection settings\non chip area\nour original design requirements. Therefore, the additional\ndata loading also imposes certain limitations on the threshold\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2024\n13\nFig. 13: The weight of important neurons has an impact on\nthe input and output of FlexHyCA.\ndesign of neurons.\nE. Analysis of Circuit-level Parameters\nTo understand the parameters of circuit-level protection\ndesign, we analyzed the impact of different bit protections\non the chip area of an 8-bit integer multiplier under uncon-\nstrained and constrained quantization, respectively. For the\nquantization constraint, we set Q scale to 4 and 7, based on\nthe previous experiments where the impact of quantization\nconstraint settings on the accuracy of ResNet and VGG\nwas minimal. Additionally, for the constrained bit protection\ndesign, we further compared the direct redundancy and the\nreconfigurable redundancy implementation, where the direct\nredundancy method applies triple modular redundancy to the\nentire important logic area, while the reconfigurable redun-\ndancy uses MUX to selectively protect only the important\nbits according to the specific quantization. The experimental\nresults are shown in Figure 14, which demonstrates that the\narea of fault-tolerant protection with constrained redundancy\nproposed in this paper is significantly reduced compared to\nthe high-bit redundancy design without constraints. Compared\nto the simplest direct redundancy implementation, the re-\ndundancy area is reduced by an average of 71.4%, mainly\nbecause only a few columns in the middle of the multiplier\nperform more calculations, resulting in higher redundancy\ncosts. When we introduce quantization constraints, a small\namount of constraint can bypass the redundancy of this part\nof the calculation, significantly reducing redundancy costs.\nCompared with direct redundancy, reconfigurable redundancy\ncan reduce the number of redundancy calculation units, further\nreducing redundancy costs. Moreover, from the figure, it can\nbe seen that the unconstrained redundancy strategy will also\nintroduce significant area overhead when the number of bit\nprotections is 1, because the logic scale of high-bit positions\nis still large under unconstrained quantization. In contrast, the\ncost of constrained redundancy becomes more stable with the\nincrease of bit protection number, which is conducive to the\nscalability of the design.\nF. Cross-Layer Design Parameter Search\nFor the cross-layer design parameter search, we mainly\nuse the Bayesian optimization algorithm to search for cross-\nlayer design parameters in the design space. We aim to\nminimize the chip area while satisfying the accuracy and\nperformance constraints. The main design parameters and\nFig. 14: The calculation array area corresponding to different\nbit protection designs.\ntheir ranges are shown in Table 1. Figure 14 shows the\ndata points obtained using Bayesian search under different\nfault rates and their corresponding Pareto curves. Overall,\nit can be seen that the chip area vary greatly for different\ndesign parameters. The parameter with the largest area cost\ncorresponds to a chip area close to three times that of the\noptimized parameter. This also indicates the necessity of cross-\nlayer parameter optimization. Meanwhile, from the figure, we\ncan see that for both fault rates, when the model accuracy\nloss is around 3% and 5%, respectively, many data points\nin cross-layer optimization correspond to smaller chip areas.\nThis is significantly advantageous compared to traditional\ntriple modular redundancy. However, as the model accuracy\nrequirement increases, the chip area cost corresponding to the\ndata points obtained from cross-layer optimization increases\nsignificantly. Many design choices correspond to chip area\ncosts that are more than twice the original chip area. This\nalso indicates that after the accuracy requirement of fault-\ntolerant DLA increases, the corresponding fault-tolerant cost\nincreases dramatically. Even if we only focus on the design\nchoices on the Pareto curve, similar situations can be observed.\nWhen the reliability/accuracy requirement is low, the relative\nchip area cost can be controlled within 5%. However, when\nthe reliability/accuracy requirement increases, the optimized\nchip area cost can reach more than 40%. Essentially, with the\nincrease of accuracy requirement, the area redundancy cost\nincreases gradually at first. This is because for the data points\nwithin this range, the high-one-bit protection is used in the\ntwo-dimensional computing array, and the threshold S TH%\nfor most important neurons is set relatively low (basically less\nthan 10%). Additionally, a configurable redundancy scheme is\nused. Under this scheme, the difference in area redundancy\ncost for protecting different bits of important neurons is not\nsignificant, and the area of DPPU is much smaller than that\nof the two-dimensional computing array. However, as the\naccuracy requirement further increases, the threshold or data\nbits that need to be protected for important neurons need to\nbe further increased. The area of DPPU becomes significant\ncompared to that of the two-dimensional computing array,\nand the required area redundancy cost increases significantly.\nTherefore, the reasonable requirements for deep learning re-\nliability and accuracy have a significant impact on the final\nfault-tolerant design choice.\nIn addition, we found that the collaborative design of quan-\ntization and circuit layers greatly reduces the fault-tolerant cost\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2024\n14\nTABLE I: Cross layer design parameter search space\nParameter\nvariables\nValue\nmeaning\nS TH\n5,10...35,40\nPercentage of\nimportant neurons\nIN TH\n2,3,4\nImportant bits of\nimportant neurons\nNB TH\n1,2,3\nimportant bits of\ncommon neurons\nQ scale\n1,2 ... 16\nTruncate constraints\nS policy\nlayers\nsharding\nSelection strategies\nfor important neurons\nDot size\n8,16, ... ,256\nPoints multiply\narray dimensions\nData Reuse\nTrue,False\nHeterogeneous array\ndata reuse\nPE policy\nDirect\nConfigurable\nCompute unit bit\nprotection policy\nTABLE II: The best Cross layer design parameter of different\nmodel\nDifferent Fault Rate\nFault I\nFault II\nS TH\n5\n5\nIN TH\n2\n3\nNB TH\n1\n1\nQ scale\n7\n8\nS policy\nUniform proportions\nUniform proportions\nDot size\n52\n52\nData Reuse\nTrue\nTrue\nPE policy\nConfigurable\nConfigurable\nof the circuit layer. In scenarios where there is no distinction\nbetween important and normal neurons, the fault-tolerant cost\nis greatly reduced, and the space for cross-layer optimization\nis significantly narrowed. In the figure, we particularly marked\nthe data points corresponding to high 1-bit protection, high 2-\nbit protection, and high 3-bit protection with squares. It can\nbe seen that they are either on the Pareto curve or very close\nto it. Especially when many design parameters that meet the\nmodel accuracy requirements between high 1-bit protection\nand high 2-bit protection are shielded by the design of high\n2-bit protection, a large blank space appears in the middle of\nthe Pareto curve.\nThe corresponding optimized parameter results according\nto different experimental design goals are shown in Table 2.\nOverall, due to the strong correlation between many design\nparameters under different faults, such as the proportion of\nimportant neurons (IN TH) and the size of the dot product ar-\nray (Dot size), the selection space for these two parameters is\nrelatively small. This leads to some related parameter selection\nbeing affected, and as previously analyzed, the collaborative\ndesign of circuit layer and quantization greatly reduces the cost\nof circuit bit protection, significantly compressing the search\nspace. On the other hand, the proportion of important neurons\nand the Dot size parameter are both discrete, which also limits\nthe search space to a certain extent. These factors make the\nresults of parameter search relatively close.\nV. CONCLUSION\nDeep learning accelerators have become one of the main-\nstream computing engines for deep learning inference, and\ntheir reliability is a key factor in ensuring high-reliability\n(a) Fault rate I\n(b) Fault rate II\nFig. 15: The data points sampled during the Bayesian opti-\nmization process.\ndeep learning. High-reliability deep learning accelerators have\nfurther increased the demand for reliability beyond traditional\nperformance, energy efficiency, and area metrics, making the\ndesign space large and optimization difficult. To address the\ndesign of high-reliability deep learning accelerators, this paper\nproposes to explore the differences in the chip’s sensitivity to\nsoft faults for deep learning processing from two dimensions:\nneuron computation and neuron bit width, and based on\nthese differences, propose corresponding selective protection\nmethods from algorithm, architecture, and circuit design. At\nthe same time, the relationship between cross-layer design\nparameters is systematically explored. Finally, with the help\nof Bayesian optimization, cross-layer parameter optimization\nis achieved, which can provide automated cross-layer fault-\ntolerant DLA design for different user requirements, mini-\nmize fault-tolerant protection costs while satisfying reliabil-\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2024\n15\nity/accuracy, performance, and other constraints. Experiments\nshow that compared with traditional fault-tolerant designs that\nfocus on a single layer, cross-layer fault-tolerant DLA better\ncombines deep learning features with architecture and circuit\ndesign, greatly reducing the fault-tolerant cost of deep learning\naccelerators.\nThis paper systematically explores selective fault-tolerant\nprotection methods for deep learning accelerators at the algo-\nrithm, architecture, and circuit levels. However, there are still\nseveral deficiencies that need to be addressed in the future in\nthe following areas:\n(1) The two-dimensional computational array in the acceler-\nator architecture proposed in this paper currently cannot distin-\nguish between regular neuron computation and important neu-\nron computation. Essentially, important neurons are processed\non both the two-dimensional array and the DPPU, which leads\nto computational waste and also limits the scale of the DPPU.\nIn the future, sparse deep learning accelerator architecture can\nbe used to remove important neuron computation from the\ntwo-dimensional computational array, further reducing fault-\ntolerant costs.\n(2) Currently, we use a static method to separate important\nneuron computation from regular neuron computation, which\nlacks consideration of input data differences. In the future,\nwe hope to further increase support for dynamic neuron\nimportance, which will also help to further reduce fault-\ntolerant costs.\n(3) There are still many other redundant protection methods\nat the algorithm level, such as fault-tolerant training, fault\ndetection and correction based on checksum mechanisms, etc.,\nespecially in the case of low fault rates, which overlap with the\nfault-tolerant design strategies proposed in this paper. These\nmethods can be incorporated into the cross-layer fault-tolerant\ndesign framework proposed in this paper, which is expected\nto obtain better fault-tolerant design solutions.\nAuthor contributions statement: Author 1 completed the\ncircuit design and conducted the experimental work, drafted\nthe manuscript. Author 2 proposed the overall design scheme\nand revised the manuscript. Author 4 designed the algorithm\nfor importance analysis and cross-layer parameter space ex-\nploration. Author 3, 5, and 6 provided guidance and feedback.\nREFERENCES\n[1] S.-H. Jeon, J.-H. Cho, Y. Jung, S. Park, and T.-M. Han, “Automotive\nhardware development according to iso 26262,” in 13th international\nconference on advanced communication technology (ICACT2011). IEEE,\n2011, pp. 588–592.\n[2] S. Grigorescu, B. Trasnea, T. Cocias, and G. Macesanu, “A survey of deep\nlearning techniques for autonomous driving,” Journal of Field Robotics,\nvol. 37, no. 3, pp. 362–386, 2020.\n[3] C. Liu, C. Chu, D. Xu, Y. Wang, Q. Wang, H. Li, X. Li, and K.-T. Cheng,\n“Hyca: A hybrid computing architecture for fault-tolerant deep learning,”\nIEEE Transactions on Computer-Aided Design of Integrated Circuits and\nSystems, vol. 41, no. 10, pp. 3400–3413, 2021.\n[4] C. Liu, Z. Gao, S. Liu, X. Ning, H. Li, and X. Li, “Special session:\nFault-tolerant deep learning: A hierarchical perspective,” in 2022 IEEE\n40th VLSI Test Symposium (VTS).\nIEEE, 2022, pp. 1–12.\n[5] M. Jenihhin, M. S. Reorda, A. Balakrishnan, and D. Alexandrescu,\n“Challenges of reliability assessment and enhancement in autonomous\nsystems,” in 2019 IEEE International Symposium on Defect and Fault\nTolerance in VLSI and Nanotechnology Systems (DFT).\nIEEE, 2019,\npp. 1–6.\n[6] X. Xue, H. Huang, C. Liu, T. Luo, L. Zhang, and Y. Wang, “Winograd\nconvolution: A perspective from fault tolerance,” in Proceedings of the\n59th ACM/IEEE Design Automation Conference, 2022, pp. 853–858.\n[7] A. Dixit and A. Wood, “The impact of new technology on soft error rates,”\nin 2011 International Reliability Physics Symposium.\nIEEE, 2011, pp.\n5B–4.\n[8] M. Rabe, S. Milz, and P. Mader, “Development methodologies for\nsafety critical machine learning applications in the automotive domain: A\nsurvey,” in Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, 2021, pp. 129–141.\n[9] S. Mittal, “A survey on modeling and improving reliability of dnn\nalgorithms and accelerators,” Journal of Systems Architecture, vol. 104,\np. 101689, 2020.\n[10] Y. Chen, T. Luo, S. Liu, S. Zhang, L. He, J. Wang, L. Li, T. Chen, Z. Xu,\nN. Sun et al., “Dadiannao: A machine-learning supercomputer,” in 2014\n47th Annual IEEE/ACM International Symposium on Microarchitecture.\nIEEE, 2014, pp. 609–622.\n[11] D. Xu, C. Chu, Q. Wang, C. Liu, Y. Wang, L. Zhang, H. Liang, and\nK.-T. Cheng, “A hybrid computing architecture for fault-tolerant deep\nlearning accelerators,” in 2020 IEEE 38th International Conference on\nComputer Design (ICCD).\nIEEE, 2020, pp. 478–485.\n[12] Y.-H. Chen, J. Emer, and V. Sze, “Eyeriss: A spatial architecture\nfor energy-efficient dataflow for convolutional neural networks,” ACM\nSIGARCH computer architecture news, vol. 44, no. 3, pp. 367–379, 2016.\n[13] D. Xu, K. Xing, C. Liu, Y. Wang, Y. Dai, L. Cheng, H. Li, and L. Zhang,\n“Resilient neural network training for accelerators with computing errors,”\nin 2019 IEEE 30th International Conference on Application-specific\nSystems, Architectures and Processors (ASAP), vol. 2160.\nIEEE, 2019,\npp. 99–102.\n[14] D. Xu, Z. Zhu, C. Liu, Y. Wang, S. Zhao, L. Zhang, H. Liang, H. Li, and\nK.-T. Cheng, “Reliability evaluation and analysis of fpga-based neural\nnetwork acceleration system,” IEEE Transactions on Very Large Scale\nIntegration (VLSI) Systems, vol. 29, no. 3, pp. 472–484, 2021.\n[15] D. Xu, Z. Zhu, C. Liu, Y. Wang, H. Li, L. Zhang, and K.-T. Cheng,\n“Persistent fault analysis of neural networks on fpga-based acceleration\nsystem,” in 2020 IEEE 31st International Conference on Application-\nspecific Systems, Architectures and Processors (ASAP).\nIEEE, 2020, pp.\n85–92.\n[16] W. Li, X. Ning, G. Ge, X. Chen, Y. Wang, and H. Yang, “Ftt-nas:\nDiscovering fault-tolerant neural architecture,” in 2020 25th Asia and\nSouth Pacific Design Automation Conference (ASP-DAC).\nIEEE, 2020,\npp. 211–216.\n[17] M. A. Hanif, R. Hafiz, and M. Shafique, “Error resilience analysis for\nsystematically employing approximate computing in convolutional neural\nnetworks,” in 2018 Design, Automation & Test in Europe Conference &\nExhibition (DATE).\nIEEE, 2018, pp. 913–916.\n[18] A. Samajdar, Y. Zhu, P. Whatmough, M. Mattina, and T. Kr-\nishna, “Scale-sim: Systolic cnn accelerator simulator,” arXiv preprint\narXiv:1811.02883, 2018.\n[19] J. J. Zhang, T. Gu, K. Basu, and S. Garg, “Analyzing and mitigating\nthe impact of permanent faults on a systolic array based neural network\naccelerator,” in 2018 IEEE 36th VLSI Test Symposium (VTS).\nIEEE,\n2018, pp. 1–6.\n[20] M.-L. Li, P. Ramachandran, S. K. Sahoo, S. V. Adve, V. S. Adve, and\nY. Zhou, “Understanding the propagation of hard errors to software and\nimplications for resilient system design,” ACM Sigplan Notices, vol. 43,\nno. 3, pp. 265–276, 2008.\n[21] B. Reagen, U. Gupta, L. Pentecost, P. Whatmough, S. K. Lee, N. Mul-\nholland, D. Brooks, and G.-Y. Wei, “Ares: A framework for quantifying\nthe resilience of deep neural networks,” in Proceedings of the 55th Annual\nDesign Automation Conference, 2018, pp. 1–6.\n[22] M. A. Neggaz, I. Alouani, P. R. Lorenzo, and S. Niar, “A reliability study\non cnns for critical embedded systems,” in 2018 IEEE 36th International\nConference on Computer Design (ICCD).\nIEEE, 2018, pp. 476–479.\n[23] K. Zhao, S. Di, S. Li, X. Liang, Y. Zhai, J. Chen, K. Ouyang, F. Cappello,\nand Z. Chen, “Ft-cnn: Algorithm-based fault tolerance for convolutional\nneural networks,” IEEE Transactions on Parallel and Distributed Systems,\nvol. 32, no. 7, pp. 1677–1689, 2020.\n[24] E. Ozen and A. Orailoglu, “Sanity-check: Boosting the reliability of\nsafety-critical deep neural network applications,” in 2019 IEEE 28th Asian\nTest Symposium (ATS).\nIEEE, 2019, pp. 7–75.\n[25] C. Schorn, A. Guntoro, and G. Ascheid, “Accurate neuron resilience\nprediction for a flexible reliability management in neural network accel-\nerators,” in 2018 Design, Automation & Test in Europe Conference &\nExhibition (DATE).\nIEEE, 2018, pp. 979–984.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2024\n16\n[26] F. Libano, B. Wilson, J. Anderson, M. J. Wirthlin, C. Cazzaniga,\nC. Frost, and P. Rech, “Selective hardening for neural networks in fpgas,”\nIEEE Transactions on Nuclear Science, vol. 66, no. 1, pp. 216–222, 2018.\n[27] D. Xu, M. He, C. Liu, Y. Wang, L. Cheng, H. Li, X. Li, and K.-T.\nCheng, “R2f: A remote retraining framework for aiot processors with\ncomputing errors,” IEEE Transactions on Very Large Scale Integration\n(VLSI) Systems, vol. 29, no. 11, pp. 1955–1966, 2021.\n[28] M. Shafique, M. Naseer, T. Theocharides, C. Kyrkou, O. Mutlu,\nL. Orosa, and J. Choi, “Robust machine learning systems: Challenges,\ncurrent trends, perspectives, and the road ahead,” IEEE Design & Test,\nvol. 37, no. 2, pp. 30–57, 2020.\n[29] Z. Gao, H. Zhang, Y. Yao, J. Xiao, S. Zeng, G. Ge, Y. Wang, A. Ullah,\nand P. Reviriego, “Soft error tolerant convolutional neural networks on\nfpgas with ensemble learning,” IEEE Transactions on Very Large Scale\nIntegration (VLSI) Systems, vol. 30, no. 3, pp. 291–302, 2022.\n[30] H. R. Mahdiani, S. M. Fakhraie, and C. Lucas, “Relaxed fault-tolerant\nhardware implementation of neural networks in the presence of multiple\ntransient errors,” IEEE transactions on neural networks and learning\nsystems, vol. 23, no. 8, pp. 1215–1228, 2012.\n[31] L.-H. Hoang, M. A. Hanif, and M. Shafique, “Ft-clipact: Resilience\nanalysis of deep neural networks and improving their fault tolerance\nusing clipped activation,” in 2020 Design, Automation & Test in Europe\nConference & Exhibition (DATE).\nIEEE, 2020, pp. 1241–1246.\n[32] J. Zhan, R. Sun, W. Jiang, Y. Jiang, X. Yin, and C. Zhuo, “Improving\nfault tolerance for reliable dnn using boundary-aware activation,” IEEE\nTransactions on Computer-Aided Design of Integrated Circuits and Sys-\ntems, vol. 41, no. 10, pp. 3414–3425, 2021.\n[33] J. A. Clemente, W. Mansour, R. Ayoubi, F. Serrano, H. Mecha, H. Ziade,\nW. El Falou, and R. Velazco, “Hardware implementation of a fault-\ntolerant hopfield neural network on fpgas,” Neurocomputing, vol. 171,\npp. 1606–1609, 2016.\n[34] Z. Chen, G. Li, and K. Pattabiraman, “A low-cost fault corrector for\ndeep neural networks through range restriction,” in 2021 51st Annual\nIEEE/IFIP International Conference on Dependable Systems and Net-\nworks (DSN).\nIEEE, 2021, pp. 1–13.\n[35] H. Wang, R. Feng, Z.-F. Han, and C.-S. Leung, “Admm-based algorithm\nfor training fault tolerant rbf networks and selecting centers,” IEEE\ntransactions on neural networks and learning systems, vol. 29, no. 8,\npp. 3870–3878, 2017.\n[36] X. He, L. Ke, W. Lu, G. Yan, and X. Zhang, “Axtrain: Hardware-oriented\nneural network training for approximate inference,” in Proceedings of the\ninternational symposium on low power electronics and design, 2018, pp.\n1–6.\n[37] A. Ruospo, G. Gavarini, I. Bragaglia, M. Traiola, A. Bosio, and\nE. Sanchez, “Selective hardening of critical neurons in deep neural net-\nworks,” in 2022 25th International Symposium on Design and Diagnostics\nof Electronic Circuits and Systems (DDECS).\nIEEE, 2022, pp. 136–141.\n[38] T. G. Bertoa, G. Gambardella, N. J. Fraser, M. Blott, and J. McAllister,\n“Fault tolerant neural network accelerators with selective tmr,” IEEE\nDesign & Test, 2022.\n[39] V. T. Lee, A. Alaghi, R. Pamula, V. S. Sathe, L. Ceze, and M. Os-\nkin, “Architecture considerations for stochastic computing accelerators,”\nIEEE Transactions on Computer-Aided Design of Integrated Circuits and\nSystems, vol. 37, no. 11, pp. 2277–2289, 2018.\n[40] A. Ardakani, A. Ardakani, and W. J. Gross, “Fault-tolerance of bina-\nrized and stochastic computing-based neural networks,” in 2021 IEEE\nWorkshop on Signal Processing Systems (SiPS).\nIEEE, 2021, pp. 52–57.\n[41] W. Qian, X. Li, M. D. Riedel, K. Bazargan, and D. J. Lilja, “An\narchitecture for fault-tolerant computation with stochastic logic,” IEEE\ntransactions on computers, vol. 60, no. 1, pp. 93–105, 2010.\n[42] P. Pandey, P. Basu, K. Chakraborty, and S. Roy, “Greentpu: Improving\ntiming error resilience of a near-threshold tensor processing unit,” in\nProceedings of the 56th Annual Design Automation Conference 2019,\n2019, pp. 1–6.\n[43] A. Mahmoud, S. K. S. Hari, C. W. Fletcher, S. V. Adve, C. Sakr,\nN. Shanbhag, P. Molchanov, M. B. Sullivan, T. Tsai, and S. W. Keckler,\n“Hardnn: Feature map vulnerability evaluation in cnns,” arXiv preprint\narXiv:2002.09786, 2020.\n",
  "categories": [
    "cs.AR",
    "cs.AI",
    "cs.LG"
  ],
  "published": "2023-12-21",
  "updated": "2023-12-21"
}