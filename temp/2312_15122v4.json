{
  "id": "http://arxiv.org/abs/2312.15122v4",
  "title": "Scaling Is All You Need: Autonomous Driving with JAX-Accelerated Reinforcement Learning",
  "authors": [
    "Moritz Harmel",
    "Anubhav Paras",
    "Andreas Pasternak",
    "Nicholas Roy",
    "Gary Linscott"
  ],
  "abstract": "Reinforcement learning has been demonstrated to outperform even the best\nhumans in complex domains like video games. However, running reinforcement\nlearning experiments on the required scale for autonomous driving is extremely\ndifficult. Building a large scale reinforcement learning system and\ndistributing it across many GPUs is challenging. Gathering experience during\ntraining on real world vehicles is prohibitive from a safety and scalability\nperspective. Therefore, an efficient and realistic driving simulator is\nrequired that uses a large amount of data from real-world driving. We bring\nthese capabilities together and conduct large-scale reinforcement learning\nexperiments for autonomous driving. We demonstrate that our policy performance\nimproves with increasing scale. Our best performing policy reduces the failure\nrate by 64% while improving the rate of driving progress by 25% compared to the\npolicies produced by state-of-the-art machine learning for autonomous driving.",
  "text": "Scaling Is All You Need: Autonomous Driving with JAX-Accelerated\nReinforcement Learning\nMoritz Harmel\nzoox.com\nAnubhav Paras\nzoox.com\nAndreas Pasternak\nzoox.com\nNicholas Roy\nzoox.com\nGary Linscott\nzoox.com\nAbstract\nReinforcement learning has been demonstrated to out-\nperform even the best humans in complex domains like\nvideo games. However, running reinforcement learning ex-\nperiments on the required scale for autonomous driving is\nextremely difficult.\nBuilding a large scale reinforcement\nlearning system and distributing it across many GPUs is\nchallenging. Gathering experience during training on real\nworld vehicles is prohibitive from a safety and scalability\nperspective.\nTherefore, an efficient and realistic driving\nsimulator is required that uses a large amount of data from\nreal-world driving. We bring these capabilities together and\nconduct large-scale reinforcement learning experiments for\nautonomous driving. We demonstrate that our policy per-\nformance improves with increasing scale. Our best per-\nforming policy reduces the failure rate by 64% while im-\nproving the rate of driving progress by 25% compared to\nthe policies produced by state-of-the-art machine learning\nfor autonomous driving.\n1. Introduction\nIn this paper, we set up a scalable reinforcement learning\nframework and combine it with an efficient simulator for\nautonomous driving based on real-world data. We then con-\nduct experiments on billions of agent steps with different\nmodel sizes in order to determine if we can overcome the\nconstrained state space of the simulator by using increas-\ningly more real-world data.\nThe main contributions of our work are:\n1. We demonstrate how to use prerecorded real-world driv-\ning data in a hardware-accelerated simulator as part of\ndistributed reinforcement learning to achieve improving\npolicy performance with increasing experiment size.\n2. We demonstrate that our largest scale experiments, us-\ning a 25M parameter model on 6000 h of human-expert\ndriving from San Francisco training on 2.5 billion agent\nsteps, reduced the failure rate compared to the current\nstate of the art [9] by 64%.\n2. Related work\nIn this paper we present a hardware-accelerated au-\ntonomous driving simulator for real-world driving scenar-\nios, which is similar to Waymax [6]. Waymax also trains\nRL agents but only reported results from 30 million agent\nsteps, approximately two orders of magnitude less than our\nexperiments.\nTraining highly performant policies in complex and con-\ntinuous real-world domains has mainly been achieved with\ndistributed and scalable reinforcement learning using actor-\ncritic methods. However, most existing results focus not\non real-world problems but on video game environments\n[1, 11].\nOf the techniques that have been used to train au-\ntonomous driving policies, the closest to our work is Imi-\ntation Is Not Enough [9] which also uses a combination of\nimitation learning (IL) and RL to train strong driving poli-\ncies on a large dataset of real-world driving scenarios. That\nwork established two fundamental driving metrics (failure\nrate and progress ratio) and provides a detailed description\nof the mining process of their evaluation dataset. The pre-\nsented policies are state of the art (SOTA) and we will com-\npare our best policy to theirs. Other work focuses on real-\nistic traffic simulation [13], but also trains policies with a\ncombined IL and RL approach. However, their datasets are\napproximately 3 orders of magnitude smaller than ours and\nfocus on highway driving, which does not pose as complex\nchallenges as the dense urban driving we focus on. Imita-\ntion learning approaches in open loop [3, 12, 14] and closed\nloop [2, 8] have also been proposed for autonomous driv-\ning. As [9] argue, IL approaches lack explicit knowledge of\nunsafe driving and can respond inappropriate in rare, long\ntail scenarios.\n3. Large scale RL for autonomous driving\nThe challenges of using large scale reinforcement learning\nfor autonomous driving are manifold. To achieve scale, the\nreal-world problem must be modeled by a simulator, which\nrequires creating realistic traffic scenes and modeling the\ninteractions between different dynamic agents. The simu-\n1\narXiv:2312.15122v4  [cs.LG]  5 Nov 2024\nlator must be sufficiently efficient to generate environment\ninteractions on the order of billions of steps in reasonable\ntime and computational cost. Finally, a distributed learning\narchitecture must be identified that can learn efficiently and\nscalably. Learners and simulation actors must be created\nacross many machines, each leveraging parallel hardware,\ni.e. GPUs.\n3.1. Scene generation and agent interactions\nThere are different ways of creating traffic scenes for sim-\nulation.\nSimulations scenarios can be generated entirely\nsynthetically, including the placement of roads, agents, and\ntraffic signals, for example as in the Carla simulator [4].\nWhile entirely synthetic simulation gives very fine grained\ncontrol over training scenario distribution, this approach\nraises the challenge of identifying what that scenario distri-\nbution should be, and how best to sample training instances\nfrom it. A different approach is to create scenes based on\nreal-world data recorded from vehicles equipped with sen-\nsors, such as cameras and lidar sensors, driving on public\nroads. From the recorded sensor data, a 3D-scene can be\ncreated that contains, for example, the observed traffic light\nstates and challenging obstacles, such as pedestrians, cy-\nclists, and cars [6]. In this work, we use scenarios that have\nbeen created from real-world driving, for the fidelity of their\nrepresentation of the real world.\n3.2. Accelerated autonomous driving simulator\nBecause our ultimate goal is to run reinforcement learning\nexperiments with billions of agent steps, the simulator must\nbe very efficient. This can be achieved by running the simu-\nlation on accelerated hardware, such as GPUs. The parallel\ncomputing power of accelerated hardware can lead to mas-\nsive speed ups compared to CPUs. However, challenges\nregarding the simulation data structures and control flow\nneed to be addressed to enable large scale parallelism. In\nparticular, all data need to be of the same size and no log-\nical branches depending on the values of the data can be\nintroduced.\n3.2.1\nPreparing data for parallel execution\nA traffic scene can be described by data that changes over\ntime (dynamic data) and data that is constant throughout\nthe scenario (static data). An example of dynamic data are\nthe agents in the scene — the number of agents can change\nduring a scenario as well as between scenarios. An example\nfor static data are the road segments, which do not change\nwithin a scenario but change across scenarios as different\nlocations require different roads. Furthermore, the number\nof time steps per scene can vary.\nData segments of different sizes are not suitable for par-\nallel execution on hardware accelerators. To overcome this\nissue, a common maximum size for each type of data is de-\nfined. All data elements in the dynamic data are then padded\nto the defined maximum size for each time step.\n3.2.2\nThe accelerated simulation utilizing JAX\nConceptually, the simulation can be divided into a phase\nof action generation and a phase of advancing the environ-\nment state by applying the selected actions to the active\nagent and updating all other agent positions based on the\nlogged trajectories.\nFrom the updated environment state\nincluding the recorded data, the required observations can\nbe retrieved and used in the next step of action generation\nfrom the learning system as described in Section 3.4. The\naction generation is well-suited for batched inference, so\nthe primary challenge in simulating on parallel hardware is\nto implement the environment update function to process\nbatched data in parallel. We used the JAX library to rewrite\nthe update function, so it can be jit-compiled and executed\non batches on the GPU. Finally, to maximize the simula-\ntion speed, we combine the batched environment call with\nthe batched model call and scan along the time axis of the\ndynamic data via the jax.lax.scan primitive. The entire sim-\nulation is then jit-compiled into a single graph and run in\nXLA.\n3.2.3\nSimulator performance benchmark\nTo demonstrate the performance of our simulator, we com-\npare the environment step time with Waymax [6], which is\nclosest to our work. Table 1 reports the step time for dif-\nferent batch sizes on Nvidia v100 GPUs, showing that our\nsimulator runs slightly faster.\n3.3. RL problem formulation\nFor our reinforcement learning approach we need to specify\nhow we retrieve the model inputs (observations) from the\nstate s and how we generate the physical actions from the\nmodel outputs. The state s is the state of the simulation\ndescribed previously, i.e., agents, roads, traffic lights etc.\nWe also need to define the rewards, so the simulated data\ncan be used to calculate the parameter update from an RL\nmethod.\nTable 1. Runtime comparison for different batch sizes (BS) be-\ntween Waymax [6] and our simulator for one controlled agent.\nStep time [ms]\nSimulator\nDevice\nBS 1\nBS 16\nWaymax\nv100\n0.75\n2.48\nOurs\nv100\n0.52\n0.82\n2\n3.3.1\nObservation space\nThe observation space retrieves a subset of the information\nof the environment state and transforms the fields of the ob-\nservation vector into an agent-centric coordinate frame.\n3.3.2\nAction space\nOur model directly controls the longitudinal acceleration as\nwell as the steering angle rate. Using these controls guaran-\ntees that the associated dynamic constraints are not violated.\nWe are using discrete actions, which we found to be more\nstable than continuous actions during reinforcement learn-\ning training.\n3.3.3\nRewards\nThe goal of the policy is to navigate safely through traf-\nfic. In particular, the agent should make progress along the\ndesired route while not colliding with other agents and ad-\nhering to basic traffic rules. In order to achieve this, we\nintroduce dense rewards as well as done signals that are as-\nsociated with sparse rewards.\nDone signals have been introduced for collisions, off-\nroute driving, as well as running red lights and stop lines\nand are associated with high negative rewards. Dense re-\nwards are introduced for the progress along the planned\nroute (positive), velocity above the speed limit, as well as\non the squared lateral and longitudinal acceleration (all neg-\native).\n3.4. Distributed learning system\nWe can now establish the reinforcement learning approach.\nOur base reinforcement learner uses actor-critic Proximal\nPolicy Optimization (PPO) [10]. However, to achieve scale,\nwe set up an asynchronous reinforcement learning system\nsimilar to Dota2 [1]. The asynchronous setting allows to\nrun the learners and actors independently avoiding any slow\ndown. This in turn causes the data to be off-policy. We ad-\ndress this challenge for actor-critic methods by using the V-\ntrace off-policy correction algorithm [5]. Furthermore, we\npre-train a policy via behavioral cloning, similar to AlphaS-\ntar [11] because a good initial policy can speed up the RL\ntraining. However, only pre-training the policy and not the\nvalue network poses challenges to the stability at the start\nof RL training. Therefore, we use the discounted return of\nthe expert trajectories as the value target. We calculate the\ndiscounted return by replaying the expert trajectories in our\nsimulator and assigning the defined RL rewards.\n4. Evaluation\nThis section describes the different metrics and the dataset\nused for policy evaluation and comparison.\n4.1. Metrics\nThe goal of the metrics are to measure the quality of the\ntrained policy. This is already a complex problem for au-\ntonomous driving as the quality of driving comprises many\ndifferent aspects. For the scope of this paper, we follow the\nwork of [9] who introduced the failure rate and progress ra-\ntio as relevant metrics for autonomous driving. The failure\nrate measures the fundamental safety of the policy. If the\nagent collides or drives off-road in the simulation the sce-\nnario is considered as failed. The progress ratio is the dis-\ntance traveled by the agent in the simulation divided by the\ndistance traveled of the vehicle in the original log. When\nthe agent travels the same distance as the vehicle in the log,\nthis metric becomes 100 %.\nIn addition to collisions and off-route failures, we also\nimplemented metrics for stop line and traffic light viola-\ntions.\nWe did not include these violations in the failure\nrate to maintain consistency with previously reported re-\nsults. However, we do report these metrics in Table 2.\n4.2. Dataset\nAs the current state-of-the-art policies [9] are evaluated on\nproprietary datasets, our goal was to achieve the fairest\ncomparison by following the same dataset mining proce-\ndure. A dataset of 10k randomly sampled 10 s segments was\ncreated using data collected from human-expert driving in\nSan Francisco. This is comparable to the ”All” evaluation\ndataset in [9].\n5. Experiments\nCombining the real-world driving simulator, the scalable re-\ninforcement learning framework and the described evalua-\ntion metrics and dataset, we conduct experiments with dif-\nferent training dataset and model sizes. For all these exper-\niments we keep the hyperparameters the same. In particu-\nlar we run our experiments for larger models across more\nGPUs to achieve the same batch size.\nWe mined three different training datasets of 600 h, 2000\nh and 6000 h from human-expert driving in San Francisco.\nWe also created three different model sizes of 0.75M, 2.5M\nand 25M parameters by increasing the attention dimensions\nof the network.\nEach model is trained first by behavior\ncloning for 20 epochs on the given dataset. The pre-trained\npolicy is then refined by reinforcement learning on 2.5B\nagent steps. We evaluate the policy during reinforcement\nlearning every 20M agent steps and after training select the\ncheckpoint with the lowest failure rate on the evaluation\ndataset.\nWe conduct experiments on all combinations of model\nsize and dataset size, with the exception of the small 600 h\ndataset in combination with the large 25M parameter model.\nFigure 1a shows that increasing the dataset size improves\n3\n0.75M\n2.5M\n25M\n3.12 %\n1.92 %\n600 h\n2.27 %\n1.19 %\n1.35 %\n2000 h\n1.38 %\n1.14 %\n0.88 %\n6000 h\n(a) Minimum failure rate\n0.75M\n2.5M\n25M\n345 h\n822 h\n600 h\n333 h\n975 h\n16118 h\n2000 h\n342 h\n805 h\n18476 h\n6000 h\n(b) GPU time\nFigure 1. Results for experiments with different model sizes (rows) and dataset sizes (columns). Colors represent the numerical results on\ncolor scales. (a) The performance of the policy improves with increasing model and dataset size. (b) The model size is the major driver of\nthe required GPU time and therefore cost of training. Dataset size has no effect on the training time, but it can affect one time costs during\ndata preprocessing which is not considered here.\nthe performance of the trained policy in terms of failure\nrate. Increasing the model size in general also improves\nthe policy performance. The 2.5M model is strictly better\nthan the 0.75M model and the best policy is trained on the\n25M model. However, we observe that increasing the model\nsize only helps when sufficient real-world driving data is\navailable. On the 2000 h dataset the 25M performs worse\nthan the 2.5M model and only on the 6000 h dataset it per-\nforms better. The largest experiment achieves a failure rate\nof 0.88 %.\nIn Table 2 we compare the policy performance of our\nlargest setting after behavioral cloning and after reinforce-\nment learning training with the current SOTA [9]. Our be-\nhavioral cloning policy performs quite poorly, achieving a\nfailure rate of 19.85 %. This is much higher than the pure\nBC failure rate of 3.64 % reported in the current SOTA [9].\nThe reinforcement learning training improves the policy\nand achieves a failure rate of 0.88 % and a progress ratio of\n120.8 %. Compared to the best policy of the current SOTA\non a similar dataset, the failure rate is reduced by 64% and\nthe progress ratio improved by 25%.\n6. Conclusions\nIn this paper we combined an efficient and realistic au-\ntonomous driving simulator with a scalable reinforcement\nlearning framework.\nThis allowed us to run large scale\nreinforcement learning experiments training on billions of\nagents steps with increasing model size on different dataset\nsizes of real-world driving.\nOur data shows that we can obtain similar scaling be-\nhavior as in other reinforcement learning settings [7] when\nusing increasingly large datasets of real-world driving. In\nparticular, we were able to obtain better policies with larger\nmodels when using sufficiently large datasets. Our best pol-\nicy reduces the failure rate compared to the current SOTA\n[9] by 64% while improving progress by 25%. These re-\nsults are very encouraging, and motivate further experi-\nments with increasing size. However, to ultimately answer\nwhether the presented approach can be scaled beyond hu-\nman performance a validation framework that can reliably\ncompare the safety of the policy to human drivers is also\nrequired.\nTable 2. Comparison of our policies with the current SOTA [9].\nBC [9]\nMGAIL [9]\nSAC [9]\nBC+SAC [9]\nOur BC\nOur BC+PPO\nFailure Rate [%] [9]\n3.64\n2.45\n5.60\n2.81\n19.85\n0.88\nProgress Ratio [%] [9]\n98.1\n96.6\n71.1\n87.6\n96.91\n120.8\nCollisions [%]\n-\n-\n-\n-\n10.32\n0.46\nOff-Route Events [%]\n-\n-\n-\n-\n10.35\n0.49\nStop Line Violations [%]\n-\n-\n-\n-\n2.47\n0.02\nTraffic Light Violations [%]\n-\n-\n-\n-\n2.08\n0.28\n4\nReferences\n[1] Christopher Berner, Greg Brockman, Brooke Chan, Vicki\nCheung, Przemyslaw Debiak, Christy Dennison, David\nFarhi, Quirin Fischer, Shariq Hashme, Chris Hesse, Rafal\nJ´ozefowicz, Scott Gray, Catherine Olsson, Jakub Pachocki,\nMichael Petrov, Henrique P. d. O. Pinto, Jonathan Raiman,\nTim Salimans, Jeremy Schlatter, Jonas Schneider, Szymon\nSidor, Ilya Sutskever, Jie Tang, Filip Wolski, and Susan\nZhang. Dota 2 with large scale deep reinforcement learning,\n2019. 1, 3\n[2] Eli Bronstein, Mark Palatucci, Dominik Notz, Brandyn\nWhite, Alex Kuefler, Yiren Lu, Supratik Paul, Payam Nikdel,\nPaul Mougin, Hongge Chen, Justin Fu, Austin Abrams, Punit\nShah, Evan Racah, Benjamin Frenkel, Shimon Whiteson,\nand Dragomir Anguelov. Hierarchical model-based imita-\ntion learning for planning in autonomous driving, 2022. 1\n[3] Eli Bronstein, Sirish Srinivasan, Supratik Paul, Aman Sinha,\nMatthew O’Kelly, Payam Nikdel, and Shimon Whiteson.\nEmbedding synthetic off-policy experience for autonomous\ndriving via zero-shot curricula, 2022. 1\n[4] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio\nLopez, and Vladlen Koltun. Carla: An open urban driving\nsimulator, 2017. 2\n[5] Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Si-\nmonyan, Volodymir Mnih, Tom Ward, Yotam Doron, Vlad\nFiroiu, Tim Harley, Iain Dunning, Shane Legg, and Koray\nKavukcuoglu. Impala: Scalable distributed deep-rl with im-\nportance weighted actor-learner architectures, 2018. 3\n[6] Cole Gulino, Justin Fu, Wenjie Luo, George Tucker, Eli\nBronstein, Yiren Lu, Jean Harb, Xinlei Pan, Yan Wang,\nXiangyu Chen, John D. Co-Reyes, Rishabh Agarwal, Re-\nbecca Roelofs, Yao Lu, Nico Montali, Paul Mougin, Zoey\nYang, Brandyn White, Aleksandra Faust, Rowan McAllister,\nDragomir Anguelov, and Benjamin Sapp. Waymax: An ac-\ncelerated, data-driven simulator for large-scale autonomous\ndriving research, 2023. 1, 2\n[7] Jacob Hilton, Jie Tang, and John Schulman. Scaling laws for\nsingle-agent reinforcement learning, 2023. 4\n[8] Maximilian Igl, Daewoo Kim, Alex Kuefler, Paul Mougin,\nPunit Shah, Kyriacos Shiarlis, Dragomir Anguelov, Mark\nPalatucci, Brandyn White, and Shimon Whiteson.\nSym-\nphony: Learning realistic and diverse agents for autonomous\ndriving simulation, 2022. 1\n[9] Yiren Lu, Justin Fu, George Tucker, Xinlei Pan, Eli Bron-\nstein, Rebecca Roelofs, Benjamin Sapp, Brandyn White,\nAleksandra Faust, Shimon Whiteson, Dragomir Anguelov,\nand Sergey Levine. Imitation is not enough: Robustifying\nimitation with reinforcement learning for challenging driv-\ning scenarios, 2023. 1, 3, 4\n[10] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Rad-\nford, and Oleg Klimov. Proximal policy optimization algo-\nrithms, 2017. 3\n[11] Oriol Vinyals, Igor Babuschkin, Wojciech M. Czarnecki,\nMicha¨el Mathieu,\nAndrew Dudzik,\nJunyoung Chung,\nDavid H. Choi, Richard Powell, Timo Ewalds, Petko\nGeorgiev, Junhyuk Oh, Dan Horgan, Manuel Kroiss, Ivo\nDanihelka, Aja Huang, Laurent Sifre, Trevor Cai, John P.\nAgapiou, Max Jaderberg, Alexander S. Vezhnevets, R´emi\nLeblond, Tobias Pohlen, Valentin Dalibard, David Budden,\nYury Sulsky, James Molloy, Tom L. Paine, Caglar Gulcehre,\nZiyu Wang, Tobias Pfaff, Yuhuai Wu, Roman Ring, Dani\nYogatama, Dario W¨unsch, Katrina McKinney, Oliver Smith,\nTom Schaul, Timothy Lillicrap, Koray Kavukcuoglu, Demis\nHassabis, Chris Apps, and David Silver. Grandmaster level\nin starcraft ii using multi-agent reinforcement learning. Na-\nture, 575(7782):350–354, 2019. 1, 3\n[12] Matt Vitelli, Yan Chang, Yawei Ye, Maciej Wołczyk, Bła˙zej\nOsi´nski, Moritz Niendorf, Hugo Grimmett, Qiangui Huang,\nAshesh Jain, and Peter Ondruska. Safetynet: Safe planning\nfor real-world self-driving vehicles using machine-learned\npolicies, 2021. 1\n[13] Chris Zhang, James Tu, Lunjun Zhang, Kelvin Wong, Simon\nSuo, and Raquel Urtasun. Learning realistic traffic agents in\nclosed-loop, 2023. 1\n[14] Zhejun Zhang, Alexander Liniger, Dengxin Dai, Fisher Yu,\nand Luc Van Gool. End-to-end urban driving by imitating a\nreinforcement learning coach, 2021. 1\n5\nScaling Is All You Need: Autonomous Driving with JAX-Accelerated\nReinforcement Learning\nSupplementary Material\n7. Roads observations\nThe implemented roads library in our simulator delivers im-\nportant information for our rewards and observations. For\nexample, it can calculate the distance to the next stop line.\nWe also use it to create observations of the route and the\nnearby road segments. This is illustrated in Figure 2.\n(a) Route border points of all drivable lanes. Only one lane is a\nvalid connection for the right turn the agent is performing. Before\nand after the turn, other lanes are also valid.\n(b) Road network points visualized by poly-lines. The color de-\npends on the properties of the annotation, for example the stop\nline being visualized in red and bike lane boundaries in green.\nFigure 2. Route and road network observations obtained from the\nroads library.\n8. Metric validation\nWe validated the collision and off-route detection by taking\n25 positive and 25 negative samples for each metric from\nthe validation dataset. These have been inspected by human\ntriagers to confirm whether the metric is correct. For the\ncollision and the off-route detection this validation found\nthat all 50 scenarios for each metric were labeled correctly\naccording to our definition. However, overall both metrics\nwere found to be conservative, leading potentially to higher\nfailure rates.\nFor the collision-free metric a bounding box approximat-\ning the vehicle is checked against the bounding box of other\nvehicles. As the bounding box includes all parts of the ve-\nhicle, for example the mirrors and sensors, checking colli-\nsions against the bounding box is conservative. Figure 3a\nillustrates this conservative check.\nIn many situations the permissible lane along the route\nis overly restrictive leading to off-route events. This oc-\ncurs particularly in junctions (Figure 3b) and lane merges or\nbranches (Figure 3c). Human expert drivers were found to\nnot exactly follow these lane geometries either. Future work\nto relax the conditions in these cases to the entire drivable\nsurface is required. On top of that, the check is also on the\nconservative side due to the increased bounding box size.\n9. Framework scalability\nWhen scaling experiments to more GPUs it is important that\nthe policy performance is not affected and that the overall\nruntime is reduced. Ideally, the reduction in runtime is in-\nversely proportional to the number of GPUs used, so the\noverall GPU time and, therefore, the cost to run the exper-\niment stays the same. Table 3 shows the results for experi-\nments running on 8, 16, and 32 machines. The overall pol-\nicy performance is very similar and the differences can be\nconsidered noise. The overall GPU time sees a marginal\nuptick. We calculated the normalized GPU time by divid-\ning the total GPU time by the total GPU time of the 8-GPU\nexperiment. As the numbers are close to 1, the framework\nscales almost perfectly.\nTable 3. Runtime and policy performance metrics for experiments\nrunning on different numbers of GPUs. The policy performance is\nvery similar, but the runtime goes down as expected.\nNumber of GPUs\n8\n16\n32\nRuntime [h]\n41.58\n21.94\n11.99\nTotal GPU time [h]\n332.6\n351.0\n383.7\nNormalized GPU time\n1\n1.05\n1.15\nFailure Rate\n1.28\n1.25\n1.45\n10. Ablation of SL pre-training\nFor this ablation we removed the SL pre-training in order to\nunderstand the effectiveness of this step. We used the large\n1\n(a) Detected collision with agent 85753 be-\ncause of the conservative bounding box check.\n(b) Overly restrictive lane permissibility in a\njunction. Off-route event detected due to in-\ncreased bounding box size.\n(c) Overly restrictive lane permissibility on a\nroad segment with branching lanes.\nFigure 3. Examples of conservative detection of collisions and off-route events.\n6000 h dataset, the 2.5M model and again trained on 2.5B\nagent steps.\nEven without the SL pre-training the agent can learn to\nmake progress and reduce the failure rate over the course\nof training as depicted in Figure 4. However, after 2.5B\nagent steps the policy without pre-training is still at about 2\n% failure rate, which the pre-trained policy reached already\nafter 0.5B steps. Also the failure rate at 2.5B agent steps is\nlower for the policy that has been pre-trained. The increased\ntraining speed is also observed for the progress ratio. The\npre-trained policy reaches values around 120 % after 0.5B\nsteps and the policy without pre-training catches up to that\nvalue at around 2.5B steps. These results overall confirm\nthe effectiveness of the pre-training step in terms of speed.\nPre-training also helps to reach better final performance in\nterms of safety.\n11. Hyperparameters\nTable 4 shows the most important hyperparameters for both\nthe behavioral cloning stage and the reinforcement learning\nstage.\nTable 4. BC and RL Hyperparameters\nBC\nRL\nBatch size\n32768\n512\nSequence length\n-\n32\nLearning rate\n2 ∗10−3\n5.6 ∗10−5\nPPO clip param\n-\n0.3\nValue loss scaling\n10−4\n10−2\nPPO entropy coefficient\n-\n3 ∗10−2\n2\n0\n0.5B\n1B\n1.5B\n2B\n2.5B\n0.01\n2\n3\n4\n5\n6\n789\n0.1\n2\n3\n4\n5\n6\n789\n1\n2.5M, 6000 h\n2.5M, 6000 h, No BC\nAgent Steps\nMean Failure Rate\n(a) Failure Rate\n0\n0.5B\n1B\n1.5B\n2B\n2.5B\n0.9\n0.95\n1\n1.05\n1.1\n1.15\n1.2\n1.25\n2.5M, 6000 h\n2.5M, 6000 h, No BC\nAgent Steps\nMean Relative Progress\n(b) Progress Ratio\nFigure 4. Training curves for experiments with and without the SL pre-training on the 2.5M parameter model and the 6000 h dataset.\n3\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.RO"
  ],
  "published": "2023-12-23",
  "updated": "2024-11-05"
}