{
  "id": "http://arxiv.org/abs/2312.15713v1",
  "title": "PersianLLaMA: Towards Building First Persian Large Language Model",
  "authors": [
    "Mohammad Amin Abbasi",
    "Arash Ghafouri",
    "Mahdi Firouzmandi",
    "Hassan Naderi",
    "Behrouz Minaei Bidgoli"
  ],
  "abstract": "Despite the widespread use of the Persian language by millions globally,\nlimited efforts have been made in natural language processing for this\nlanguage. The use of large language models as effective tools in various\nnatural language processing tasks typically requires extensive textual data and\nrobust hardware resources. Consequently, the scarcity of Persian textual data\nand the unavailability of powerful hardware resources have hindered the\ndevelopment of large language models for Persian. This paper introduces the\nfirst large Persian language model, named PersianLLaMA, trained on a collection\nof Persian texts and datasets. This foundational model comes in two versions,\nwith 7 and 13 billion parameters, trained on formal and colloquial Persian\ntexts using two different approaches. PersianLLaMA has been evaluated for\nnatural language generation tasks based on the latest evaluation methods,\nnamely using larger language models, and for natural language understanding\ntasks based on automated machine metrics. The results indicate that\nPersianLLaMA significantly outperforms its competitors in both understanding\nand generating Persian text. PersianLLaMA marks an important step in the\ndevelopment of Persian natural language processing and can be a valuable\nresource for the Persian-speaking community. This large language model can be\nused for various natural language processing tasks, especially text generation\nlike chatbots, question-answering, machine translation, and text summarization",
  "text": " \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nPersianLLaMA: Towards Building First Persian Large Language Model \nMohammad Amin Abbasi1, Arash Ghafouri2∗, Mahdi Firouzmandi2, Hassan Naderi1 and \nBehrouz Minaei Bidgoli1 \n1 Department of Computer Engineering Iran University of Science and Technology, Tehran, Iran. \n 2 Department of Artificial Intelligence and Cognitive Science, Imam Hossein Comprehensive University, \nTehran, Iran. \n \n* Corresponding author. E-mail: krghafouri@ihu.ac.ir; \nContributing authors E-mail: m_abbasi1378@comp.iust.ac.ir; firouzmandi@ihu.ac.ir; Naderi@iust.ac.ir; \nB_minaei@iust.ac.ir. \n \nABSTARCT \nDespite the widespread use of the Persian language by millions globally, limited efforts have been made in natural language processing \nfor this language. The use of large language models as effective tools in various natural language processing tasks typically requires \nextensive textual data and robust hardware resources. Consequently, the scarcity of Persian textual data and the unavailability of powerful \nhardware resources have hindered the development of large language models for Persian. This paper introduces the first large Persian \nlanguage model, named PersianLLaMA, trained on a collection of Persian texts and datasets. This foundational model comes in two \nversions, with 7 and 13 billion parameters, trained on formal and colloquial Persian texts using two different approaches. PersianLLaMA \nhas been evaluated for natural language generation tasks based on the latest evaluation methods, namely using larger language models, \nand for natural language understanding tasks based on automated machine metrics. The results indicate that PersianLLaMA significantly \noutperforms its competitors in both understanding and generating Persian text. PersianLLaMA marks an important step in the development \nof Persian natural language processing and can be a valuable resource for the Persian-speaking community. This large language model \ncan be used for various natural language processing tasks, especially text generation like chatbots, question-answering, machine \ntranslation, and text summarization.  \nKeywords: Large language models, Persian language, Natural language generation, Natural language understanding. \n1 INTRODUCTION \nIn recent decades, significant advances in artificial intelligence have led to the emergence of large language models as \npivotal and transformative tools in the field of artificial intelligence and natural language processing. These AI models, \n2 \nutilizing deep neural networks and learning from extensive datasets, are capable of understanding and generating texts with \nstructures similar to human language. \nLanguage modeling is a primary approach for advancing machine language intelligence. Generally, the aim of language \nmodeling is to model the probability of word sequences to predict the likelihood of future words. Research in language \nmodeling has gained extensive attention in recent years, leading to the emergence of pre-trained language models. Initially, \nthe ELMo(Peters et al., 2018) model was introduced to understand content in word embedding, which involved pre-training \na bi-directional LSTM (biLSTM)(Staudemeyer & Morris, 2019) network and then fine-tuning the biLSTM network based \non sub-tasks. Also, the BERT(Devlin et al., 2018) model, utilizing the structure of Transformers and the Encoder part of \nthe Transformer model, was introduced. Pre-trained on large unlabelled text datasets, this model was ready for use in sub-\ntasks, significantly enhancing the performance of natural language understanding tasks. These models often require the \npre-trained language model to be finely tuned for compatibility with sub-tasks. \nSubsequently, researchers discovered that increasing the number of parameters in pre-trained language models generally \nleads to better results in downstream tasks. Some studies in this field have explored the performance of increasingly large \npre-trained language models, such as the GPT-3(Floridi & Chiriatti, 2020) model with 175 billion parameters and the \nPaLM(Chowdhery et al., 2023) model with 540 billion parameters. Thus, the research community coined the term \"large \nlanguage models\" for these types of models. A notable practical application of large language models is the ChatGPT(Ray, \n2023) model, which uses the GPT series models for conversation, demonstrating remarkable ability in interactions with \nhumans. \nLLaMA(Touvron, Lavril, et al., 2023; Touvron, Martin, et al., 2023) is a large language model developed by Facebook. It \nis an artificial neural network that has been trained on a vast dataset of text and code. LLaMA comprises a collection of \nopen-source language models and is available in two versions. The first and second versions have parameters ranging from \n7 to 65 billion and from 7 to 70 billion, respectively, and are competitive with state-of-the-art language models. Notably, \nLLaMA-13B, a version of LLaMA with 13 billion parameters, performs better in a multitude of tasks compared to GPT-\n3, while being approximately ten times smaller in size. Unlike most large language models, LLaMA has been trained on \nspecific and open-source datasets and has achieved state-of-the-art performance without relying on proprietary data. \nAdditionally, fine-tuning LLaMA on different data sets leads to very good results. \nHowever, large language models also face challenges and limitations; due to their numerous parameters, these models \nrequire significant computational resources for training and usage. Additionally, concerns about their capability to generate \nincorrect content or false news exist. \nDespite the remarkable capabilities of these large language models, they have been predominantly developed in English \nand limiting their applicability to other languages. This language bias has created a significant gap in the development and \nutilization of language models for non-English languages, including Persian. The Persian language, spoken by millions \nworldwide, lacks comprehensive language models that can effectively understand and generate Persian text. \nRecognizing this gap, our research aims to develop PersianLLaMA, the first large language model dedicated to the Persian \nlanguage. The development of this model is not only a technical challenge but also a step towards linguistic inclusivity in \nthe field of AI and natural language processing. By focusing on the Persian language, PersianLLaMA seeks to provide a \nrobust tool for understanding and generating Persian text, which can be utilized in various applications, from chatbots to \ntext summarization. \n3 \n2 RELATED WORKS  \nGiven the absence of a large language model in Persian, this section aims to examine the closest works related to our \nresearch. These include large Asian language models based on the LLaMA framework and both single-language and \nmultilingual pre-trained models that support Persian. \n2.1 LLaMA-Based Models \nSeveral studies have recently been conducted on developing large language models for Asian languages. For instance, \nTAMIL-LLaMA(Balachandran, 2023), an Asian language model that leverages LLaMA and incorporates 16,000 Tamil \ntokens, has shown significant improvements. This model employs the LoRA(Hu et al., 2021) technique for efficient \ntraining on Tamil datasets, including translated Alpaca datasets and a custom subset of the OpenOrca(Mukherjee et al., \n2023) dataset for fine-tuning. Performance evaluations demonstrate notable enhancements in Tamil text generation, with \nthe 13B version outperforming OpenAI's GPT-3.5-turbo in Tamil language tasks. \nSeaLLMs(Nguyen et al., 2023) is a suite of language models dedicated to Southeast Asian (SEA) languages, built on the \nLLaMA-2 model with extended vocabularies and specific configurations to cater to the cultural nuances of SEA languages. \nThese models excel in various linguistic tasks and comply with local cultural standards and legal considerations. They \nperform better than models like ChatGPT-3.5 in non-Latin SEA languages, including Thai, Khmer, Lao, and Burmese. \nAdditionally, SeaLLMs have significantly advanced AI tools with cultural awareness. \nTAIWAN-LLM(Lin & Chen, 2023) is a large language model for Traditional Chinese, particularly used in Taiwan. \nAvailable in 7 and 13 billion parameter versions, it is developed using a comprehensive pre-training dataset and fine-tuning \ndatasets that capture the linguistic and cultural essence of Taiwan. TAIWAN-LLM outperforms many models in \nunderstanding and generating Traditional Chinese text. \n2.2 Pre-Trained Persian Language Models \nPersian language models play a crucial role in advancing research related to Persian language processing. These models, \npre-trained on extensive Persian text datasets, are adept at accurately comprehending and interpreting Persian language \nstructures and meanings.  \nAriaBERT(Ghafouri et al., 2023) is recognized as the leading language model in understanding the Persian language. \nUtilizing the RoBERTa(Liu et al., 2019) architecture and the Byte-Pair Encoding(Sennrich et al., 2015) tokenizer, \nAriaBERT has been trained with over 32 gigabytes of various Persian textual data. This includes a mix of colloquial and \nformal texts comprising tweets, news, poetry, medical texts, encyclopedia articles, user comments on websites, and other \ntext types. \nParsBERT is a monolingual model based on Google's BERT architecture, trained on a large corpus of Persian texts \ncovering diverse topics with more than 3.9 million documents and 16 gigabytes of data. It outperforms multilingual models \nlike Multilingual BERT (M-BERT) in various tasks such as sentiment analysis, text classification, named entity \nrecognition, and named entity disambiguation in Persian texts. \nSINA-BERT(Taghizadeh et al., 2021) Introduced as a BERT-based language model, is published for high-quality Persian \nlanguage coverage in the medical field. It utilizes an extensive collection of medical content, including both formal and \ninformal texts. The primary use of this model includes medical question classification, sentiment analysis in medicine, and \nmedical question retrieval. \nThe mT5(Xue et al., 2020) model, known for its text-to-text transformation, is another prominent NLP model developed \nby Google. Based on the Transformer architecture, it frames all NLP tasks as converting input text to output text. This \n4 \nversatile adaptability is evident during fine-tuning, where the model aligns with specific tasks using labeled data. The \nmultilingual version, MT5, trained on a vast corpus of web-crawled text in 101 languages including Persian, exemplifies \nthis adaptability. \nThe mGPT(Shliazhko et al., 2022) models, similar to GPT2(Radford et al., 2019), come in 1.3 and 13 billion parameter \nversions and are trained on 60 languages. Their training dataset includes mc4(Raffel et al., 2020) and Wikipedia, totaling \napproximately 600 gigabytes. mGPT, built on the decoder component of Transformer architecture, has been evaluated in \nvarious scenarios, including language modeling, downstream task evaluation, and knowledge-based searches, showing \nremarkable performance in Persian among other supported languages. Table 1 shows a comprehensive overview of recent \nlanguage models and Figure 1 shows a timeline of existing large language models in recent years. \nTable 1:   Comprehensive overview of recent language models \nModel \nRelease \nTime \nParameter \nSize \nBase \nModel \nTrain Data Size \nHardware \nType \nGemini(Anil et al., 2023) \nDec-2023 \n176B \n- \n6.3T tokens \n- \nCausal decoder \nSeaLLMs \nDec-2023 \n13B \nLLaMA2 \n- \n- \nCausal decoder \nTAMIL-LLaMA \nNov-2023 \n13B \nLLaMA2 \n- \n- \nCausal decoder \nTAIWAN-LLM \nNov-2023 \n13B \nLLaMA2 \n35B tokens \n- \nCausal decoder \nAriaBERT \nNov -2023 \n355M \nRoBERTa 102M documents \n4 A100 40G \nMasked LM \nFalcon-40B(Penedo et al., 2023) \nJul-2023 \n40B \n- \n1T tokens \n384* A100 40GB Causal decoder \nLLaMA 2 \nJul-2023 \n70B \n- \n2T tokens \n2000*80G A100 Causal decoder \nVicuna(Zheng et al., 2023) \nJun-2023 \n13B \nLLaMA \n125K instructions \n- \nauto-regressive \nlanguage \nmodel \nPaLM2 \nMay-2023 \n16B \n- \n100B tokens \n- \nCausal decoder \nAlpaca \nMay-2023 \n13B \nLLaMA \n52K instructions \n- \n- \nWizardLM(Xu et al., 2023) \nApr-2023 \n7B \nLLaMA \n250k instructions \n- \n- \nKoala \nApr-2023 \n13B \nLLaMA \n472 instructions \n8* A100 \n- \numT5(Chung et al., 2023) \nApr-2023 \n13B \nT5 \n29T characters \n- \nEncoder-\ndecoder \nGPT-4(Koubaa, 2023) \nMar-2023 \n1.8T \n- \n- \n- \nCausal decoder \nLLaMA \nFeb-2023 \n65B \n- \n1.4T tokens \n2048*A100 80G Causal decoder \nmGPT \nApr-2022 \n13B \nGPT-3 \n440B tokens \n256*Nvidia V100 Causal decoder \nPaLM \nApr-2022 \n540B \n- \n780B tokens \n6144 TPU v4 \nCausal decoder \nSINA-BERT \nApr-2021 \n110M \nBERT \n2.8M documents \n- \nMasked LM \nmT5 \nOct-2020 \n13B \nT5 \n1T tokens \n- \nEncoder-\ndecoder \nParsBERT \nMay-2020 \n110M \nBERT \n3.9M documents \n- \nMasked LM \nGPT-3 \nMay-2020 \n175B \n- \n300B tokens \n- \nCausal decoder \n5 \n \nFigure 1: A timeline of existing large language models in recent years. The timeline was established mainly according to the release date \n(e.g., the submission date to arXiv) of the technical paper for a model \n3 PROPOSED METHOD \nIn this research, two distinct approach for developing a large Persian language model are introduced and compared. The \nfirst approach involves training a LLaMA 2 model from scratch using a Persian text dataset exceeding 90 gigabytes. A \nPersian tokenizer was trained using this dataset. Due to hardware resource limitations, a 7 billion parameter LLaMA 2 \nmodel was utilized, and the entire neural network was trained from scratch. The second approach involves training a \nLLaMA model as an adapter on a pre-trained English LLaMA 2 model. Initially, a Persian tokenizer was trained on Persian \nWikipedia texts, then combined with the tokenizer of the English LLaMA 2 model. Subsequently, an adapter neural \nnetwork was placed on the English LLaMA 2 model, and its weights were trained. The 13 billion parameter LLaMA 2 \nmodel was used in this approach, as it requires fewer hardware resources, only necessitating training of the adapter part \nwhile keeping the core neural network unchanged. Details of both approachs are thoroughly explained in the subsequent \nsections. \n3.1 LLaMA-Based Models \nTo train PersianLLaMA, we selected two datasets comprising various and diverse texts to ensure comprehensive training \nacross different topics. The training dataset includes: \n• \nOSCAR Dataset (Open Super-large Crawled Aggregated Resource)(Suárez et al., 2020): A rich source of \nmultilingual texts collected from the web, widely used in natural language processing and machine learning \nresearch. It includes website texts in multiple languages, offering a broad variety of topics and linguistic \nstructures. The Persian section contains 23 million texts (93 GB, 9 billion tokens). \n6 \n• \nPersian Wikipedia: A vast and diverse source of textual information, Persian Wikipedia serves as a valuable \nresource due to its extensive coverage of topics across various fields. It provides a rich and comprehensive \nlinguistic collection for training our model. The dataset allows our model to develop an extensive vocabulary \nand a precise understanding of Persian language structures and semantics. Utilizing Persian Wikipedia as a \nfoundational dataset ensures that the model is well-prepared to generate coherent and relevant Persian text, \ncovering a wide range of domains. This dataset includes 2.4 million texts (1.3 GB, 184 million tokens). \n3.2 Preprocessing \nIn developing the PersianLLaMA text generation model, effective preprocessing of training data is crucial to ensure the \nquality and coherence of the generated text. This section explains the preprocessing steps undertaken to prepare the training \ndataset for PersianLLaMA, utilizing a combination of techniques and libraries suited for Persian text. \nThe process begins with text normalization using the Hazm library, a powerful tool for processing Persian texts. This step \ninvolves correcting common issues related to spacing and normalizing characters in Persian texts. Subsequently, HTML \ntags present in texts are removed. Additional tasks include managing Unicode, normalizing spaces, and removing links, \nemails, and phone numbers. Persian texts may contain various punctuation marks that can impact analysis and underlying \ntasks. To address this, superfluous punctuation marks are removed from the texts. Additionally, mentions starting with '@' \nin tweets are deleted. Finally, multiple consecutive spaces are condensed into a single space. In summary, text \npreprocessing meticulously cleans and normalizes the training dataset, ensuring its suitability for training the Persian text \ngeneration model. \n3.3 Tokenizer \nIn the development of natural language processing models for Persian, training an appropriate tokenizer is a fundamental \nand vital aspect. The tokenizer divides text into smaller units (tokens) to make it understandable and usable by neural \nmodels. PersianLLaMA models use the SentencePiece tokenizer, an unsupervised text tokenizer used for neural network-\nbased text generation systems. SentencePiece(Kudo & Richardson, 2018) supports word separation using BPE (Byte Pair \nEncoding)(Wang et al., 2020) method. BPE is a popular text encoding and segmentation method used to reduce vocabulary \nsize and create subword representations. Its implementation in Persian ensures that unknown words during training are not \nregistered as unfamiliar tokens and can be effectively segmented. This is beneficial for the Persian language with its \ncomplex words and diverse subword compositions. \nTwo distinct approaches were employed in developing the PersianLLaMA tokenizers, each serving a unique model \narchitecture and objective. The details of these tokenizer implementations are discussed below: \n3.3.1Monolingual Tokenizer \nFor the first model, a Persian-specific tokenizer was trained for a learning model developed from scratch in Persian. This \ntokenizer, focusing exclusively on Persian, covers 50,000 tokens and does not recognize or process English or other \nlanguages, aligning with the monolingual nature of the model. \n3.3.2Multilingual Tokenizer With Expanding Vocabulary \nThe second model, built using LoRA architecture and based on an English foundational model, uses a broader tokenizer \ncombining English and Persian vocabularies. Initially, a Persian tokenizer with 32,000 tokens was trained. Then, using the \nEnglish tokenizer of LLaMA 2, a combined tokenizer with a total of 64,000 tokens was created, allowing the model to \n7 \nunderstand and generate both Persian and English. This tokenizer is suitable for bilingual applications like translation or \ncombined content generation, given the complexities of bilingual contexts. \nFigure 2 show the training pipeline of PersianLLaMA model with two approachs. \n \n \nFigure 2: The training pipeline of PersianLLaMA model with two approachs. Our training flow can be separated into two approachs, \nnamely model training model from scratch on Persian data from OSCAR dataset and training with LoRA on Persian Wikipedia data. In \nthe first approach, we give LLaMA 23 million Persian texts to train the model. In the second approach, we train the LLaMA model with \nLoRA on all Persian Wikipedia articles, which contain 2.4 million documents \n3.4 Model Implementation \nThe PersianLLaMA models are trained on the Causal Language Modeling task to predict and generate the next word. \nTraining from scratch utilizes DeepSpeed (Rasley et al., 2020) and TencentPretrain(Zhao et al., 2022), two advanced \nframeworks for optimizing deep learning training. TencentPretrain offers optimizations for memory usage, mixed-\nprecision training, and distributed training, enhancing efficiency and scalability. For the first version of PersianLLaMA, \nwith 7 billion parameters, a cosine scheduler with a decay of 0.1 and a learning rate of 4e-3 was used. This model was \ntrained on two A100 GPUs with 80 GB VRAM over 12 days on Wikipedia and Oscar datasets. \nThe conventional training pattern of large language models (LLMs) with full parameters is costly. Low-Rank Adaptation \n(LoRA) is a parameter-efficient method that retains the weights of pre-trained models while introducing trainable rank-\ndecomposition matrices. LoRA freezes the weights of the pre-trained model and injects trainable low-rank matrices into \neach layer. This approach significantly reduces the trainable parameters, working with just 533 million parameters, making \nthe training of LLMs feasible with much less computational resources. The second version, with 13 billion parameters, \nuses LoRA with attention modules and MLP layers applied to the adapter. This Persian LLaMA model was trained with \nthe original English LLaMA weights, using FP16, and supports a maximum text length of 2048, currently the longest \namong Persian language models. This model was trained on an A100 GPU with 80 GB VRAM over 70 hours on Wikipedia \n8 \ndata. Table 2 and 3 details the training specifications in two approchs, and Figure 3 illustrates the training process based \non the LoRA method. \nTable 2:   Approach-1 training configurations \nLearning rate \nMax length \nscheduler \nDecay \nDropout \nparams \nTrainable params Torch dtype \n0.004 \n768 \ncosine scheduler \n0.1 \n0.1 \n7B \n7B (100%) \nFloat32 \n \n \nTable 3:   Approach-2 training configurations \nLearning rate \nMax length \nLoRA rank  \nLoRA alpha \nLoRA weights \nparams \nTrainable params Torch dtype \n0.0002 \n2048 \n8 \n32 \nQKVO, MLP \n13B \n533M (4.10%) \nFloat16 \n \n \n \nFigure 3: Train the second PersianLLaMA model on wikipedia using the LoRA \n \nFigure 4 shows the training loss during the training of the PersianLLaMA-Zero and PersianLLaMA-LoRA models. As \nevident, training with LoRA leads to a lower error rate in a much shorter time during training. \n9 \n \nFigure 4: The training loss during the training of the PersianLLaMA-Zero and PersianLLaMA-LoRA models. \n4 EVALUATION \nFor the evaluation of models, it is necessary to fine-tune them on datasets related to natural language generation and \nunderstanding, and compare their results with other Persian language generation models. To evaluate the results in natural \nlanguage understanding, it's crucial to define evaluation criteria for the tasks being compared: \n• \nSentiment Analysis and Categorization: F1-score is used to assess and compare model outputs in sentiment \nanalysis and topic categorization tasks. \n• \nQuestion-Answering: Model efficiency is evaluated by checking if the response to each question is the actual \nanswer. Models sometimes include additional explanations in their text. After review, accuracy is used as the \nmetric for comparison. \n• \nText Summarization: While ROUGE and similar metrics like BLEU and METEOR provide quantitative \nmeasures, they often fall short in depicting the true nature of a summary and correlate less with human ratings. \nGiven the advancements in LLMs in producing fluent and coherent summaries, traditional metrics like ROUGE \nmight inadvertently undermine these models' performance, especially when summaries are expressed differently \nbut still accurately encapsulate the main information. ROUGE relies on the exact presence of words in both \npredicted and reference texts and fails in semantic interpretation. To address this, inspired by the \nBertScore(Zhang et al., 2019) paper, ParsBert model's text embeddings are used to compare semantic similarities \nbetween the model-generated text and the reference text. \n \nHowever, evaluating and comparing results in datasets related to natural language generation poses certain challenges. In \nthe past, the best method of evaluation was based on human assessment, but human evaluation comes with challenges that \nmight make it difficult to operationalize. These challenges include high financial costs for hiring human evaluators, time \nconsumed for conducting evaluations, and differences of opinion among different individuals. Additionally, human \nevaluation might produce inaccurate results due to the inability to evaluate on a large scale and maintain consistency in \noperations. Inspired by the paper Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca(Anil et al., 2023), \nChatGPT was used for evaluating the models. ChatGPT, as an intelligent language model capable of understanding and \ngenerating text, has emerged as an effective tool in the evaluation process of language models. ChatGPT can automatically \nevaluate generated texts and provide a credible score for the quality of text production. Using ChatGPT significantly \n10 \nreduces time and costs, and can fairly score texts generated by different language models. Similar to the approach in the \ncited paper, we also used prompt shown in Table 4 for evaluating the generated texts. \nTable 4:  Used prompt for evaluating the generated texts with ChatGPT 3.5 \nThe followings are two ChatGPT-like systems’ outputs. Please rate an overall score on a ten-point scale for each and give \nexplanations to justify your scores. \nPrompt: \n{prompt-input} \nsystem1: \n{system1-output} \nsystem2: \n{system2-output} \nsystem3: \n{system3-output} \nsystem4: \n{system4-output} \n4.1 Fine-Tuning \nFor fine-tuning the PersianLlama-Zero model, or the model trained from scratch, the TencentPretrain framework has been \nused to train the model on selected datasets. For the fine-tuning of PersianLlama-Lora, after pre-training the model on \nLoRA with Persian Wikipedia data, the LoRA neural networks were combined with the original model to form a unified \nllama model. In fine-tuning this model, the weights of PersianLlama were kept fixed, and the LoRA technique was used \nfor fine-tuning. For fine-tuning PersianLlama models on various datasets, mixed precision training (FP16) was utilized to \nexpedite the training process. In the following sections, we introduce different datasets used to fine-tune PersianLLaMA \nand evaluate its performance in natural language understanding and generation. \n4.1.1Natural Language Generation Datasets \nIn this part, datasets used for fine-tuning in the field of text generation are introduced. \n• \nMeDiaQA(Suri et al., 2021): This dataset includes 8,903 questions and answers from medical dialogues, based \non physician-patient interactions on Persian medical websites. The dataset contains responses from 150 experts. \n• \nPerCQA(Jamali et al., 2021): The first Persian dataset for Community Question Answering, it contains questions \nand answers gathered from the most famous Persian forum. It includes 989 questions and 21,915 tagged answers. \nQuestions are categorized as valid or invalid, with invalid ones including advertisements, surveys, news, and \ncollaboration announcements. Questions with less than 3 or more than 300 answers are also marked invalid. \nAnswers are tagged as Bad, Potential, or Good. Bad answers include advertisements, non-Persian responses, \ngreetings, empathy, thanks, stickers, and Persian typed in English characters. Potential answers refer to other \nsources, and Good answers are partial or complete and relevant to the question. For fine-tuning PersianLLaMA, \n10,468 question-answer pairs were extracted, including valid questions and appropriate answers. \n• \nAlpaca: This dataset is command-response style, created by OpenAI's Davinci-003 engine. The original dataset \nis in English, and its Persian-translated version is used for fine-tuning language models. \n• \nOASST1 (OpenAssistant Conversations Dataset)(Köpf et al., 2023): Consists of 161,443 messages in 35 \ndifferent languages, styled as human assistant conversations. This dataset includes various conversation trees. \nEach conversation tree starts with an initial message as the root node, which can have several child messages as \nresponses, and these child messages can have multiple responses. All messages have a role attribute, indicating \n11 \nwhether the speaker is an assistant or a \"commander\". The translated version of this dataset was used for fine-\ntuning the model. \n \nFigure 5 shows a representation of a machine learning workflow for fine-tuning a PersianLLaMA. The process starts \nwith various datasets: MeDiAaA (a dataset on medical dialogues with 8,903 medical questions and answers), PerCQA \n(a Persian community question answering dataset with 989 questions and 21,915 annotated answers), the Alpaca \nDataset from Stanford (structured for instruction-response format), and OASSTI (OpenAssistant Conversations \nDataset, a human-generated and annotated dataset for assistant-style conversation corpus). \n \nFigure 5: Fine-tune the second PersianLLaMA model using the LoRA \n4.1.2Natural Language Understanding Datasets \nIn this part, datasets used for fine-tuning in the field of NLU are introduced. \n• \nArmanEmo(Mirzaee et al., 2022): A Persian dataset for emotion detection in text. It includes a collection of \nPersian sentences and texts, each labeled with an emotion such as sadness, anger, fear, surprise, disgust, and \nothers. The dataset comprises 6,125 records for training and 1,125 for testing. It's used for evaluating models' \nsentiment analysis capabilities in Persian. \n12 \n• \nPersian News1: This dataset is a structured summarization dataset for the Persian language, consisting of 93,207 \nrecords. Each line of the dataset includes a title, main text, summary, and category. It's used for topic \nclassification and summarization. \n• \nSyntran-fa2: A Persian question-answering dataset offering a coherent and complete response for each question. \nIt contains about 50,000 question-answer records. The purpose of evaluating models on this dataset is to compare \nthe general knowledge learned by the models. \n \nTo compare the PersianLLaMA model, other natural language generation models capable of producing Persian language \nwere fine-tuned on the mentioned datasets. We selected 4 models: mGPT, mT5-XL, GPT2-Persian3, and Parsgpt4.  No \narticle has been officially published for the mGPT, mT5, GPT2-Persian, and parsgpt, and due to the lack of Persian text \ngeneration models, these models were used. The details of these models are described in Table 5. \nTable 5:   Details of the models used for evaluation and comparison \nModel \nInfrastructures \nMain Application \nBase Model \nmT5 \nEncoder + Decoder \nNLG +NLU \nT5 \nmGPT \nDecoder \nNLG \nGPT-2 \nGPT2-Persian \nDecoder \nNLG \nGPT-2 \nparsgpt \nDecoder \nNLG \nGPT-2 \nTo achieve the most accurate and best results, configurations suggested by the creators of the mentioned models were \nutilized. Table 6 provides the configurations applied for fine-tuning each model. \nTable 6:   Configurations applied for fine-tuning each model. \nModel \nMax length \nepochs \nBatch size \nLr \nWarmup steps \nPersianLLaMA-Lora \n2048 \n1 \n2 \n1e-3 \n100 \nPersianLLaMA-Zero \n768 \n1 \n2 \n1e-3 \n100 \nmT5 \n256 \n3 \n8 \n5e-5 \n500 \nmGPT \n512 \n3 \n8 \n1e-5 \n- \nGPT2-Persian \n256 \n3 \n8 \n5e-4 \n100 \nparsgpt \n512 \n3 \n8 \n1e-3 \n- \n4.2 Results \nIn the Appendix section, examples and analyses of samples from each model are provided. Table 7 presents the evaluation \nresults of the outputs of each model, scored by ChatGPT 3.5, and the averages in the field of text generation, depending \non the dataset used for fine-tuning. \nTable 7:   The results of evaluating the outputs of each model based on different datasets in the field of NLG. \n \nAlpaca \nOASST1 \nPerCQA \nMeDiaQA \nPersianLLaMA-Lora \n7.3 \n8.1 \n6.8 \n6.8 \nPersianLLaMA-Zero \n4.3 \n4.2 \n3.8 \n3.8 \n \n1 https://huggingface.co/datasets/pn_summary \n2 https://huggingface.co/datasets/SLPL/syntran-fa \n3 https://huggingface.co/bolbolzaban/gpt2-persian \n4 https://huggingface.co/HooshvareLab/gpt2-fa \n13 \n \nAlpaca \nOASST1 \nPerCQA \nMeDiaQA \nGPT2-Persian \n3.9 \n2.3 \n3.0 \n3.0 \nmGPT \n5.1 \n4.0 \n3.4 \n3.4 \nparsgpt \n1.4 \n1.1 \n0.2 \n0.2 \nmT5 \n2.5 \n3.6 \n0.9 \n0.9 \nThe experiments conducted on various models in the field of natural language generation indicate a definite superiority of \nPersianLLaMA-LoRA in all tests. PersianLLaMA-LoRA's outputs demonstrate its good understanding of the input text, \nability to maintain context, and produce logical, acceptable responses. PersianLLaMA-Zero generates relevant, though \nsometimes brief, responses. It seems to comprehend the context better than some models but lacks in attention to details. \nGPT2Persian generally shows poor performance in understanding and text generation. MGPT displays a good grasp of the \ncontext and provides relatively good responses, although sometimes it can be repetitive or overly extensive. Parsgpt's \nperformance varies depending on the datasets; it generates relevant answers for some questions and irrelevant outputs for \nothers. mT5 often produces overly brief, irrelevant, and inappropriate responses lacking depth and logic, being suitable \nonly in the context of summarization. Overall, the results demonstrate that, in understanding and responding to input text, \nPersian LLaMA-LoRA performs significantly better than the other models. \nTable 8 shows the evaluation results of each model based on the type of natural language understanding tasks. \nTable 8:   The results of evaluating the outputs of each model based on different datasets in the field of NLU. \n \nArmanEmo \nPersianQA \nPersian News Classification \nPersian News Summary \nPersianLLaMA-Lora \n65 \n8.26 \n82.38 \n83.47 \nPersianLLaMA-Zero \n42.13 \n2.82 \n14.21 \n49.65 \nGPT2-Persian \n8 \n0 \n11.23 \n41.41 \nmGPT \n45.6 \n5.47 \n19.76 \n81.54 \nparsgpt \n59.92 \n2.91 \n38.46 \n60.35 \nmT5 \n06.85 \n4.58 \n12.74 \n64.02 \n \nAs the results indicate, PersianLLaMA-Lora performs well in natural language understanding tasks, making it a viable \noption for Persian language understanding and generation tasks, yielding good results. It's important to note that some \nlabels in datasets may be ambiguous. For example, in the ArmanEmo dataset, the text \"ز مه اب\nی تس ی مآ تملاسم\nیز\" (peaceful \ncoexistence) is labeled as having undefined emotions, but PersianLLaMA interprets the emotion of this text as happiness, \nwhich seems more accurate than the dataset's labeling. \n \n \n5 LIMITATIONS \nThe PersianLLaMA large language model represents the start of an ongoing journey in improving understanding and \ngeneration of the Persian language. In this section, we discuss the current limitations not only as challenges but also as \nopportunities for research and development. Our aim is to ensure that PersianLLaMA is a significant step towards the \ngrowth and evolution of researchers in the field of Persian language processing. \n• \nTraining Data Scope Limitation: Given that our training dataset lacks diversity, our models' knowledge is limited. \nFuture works aim to use a more diverse training dataset to achieve a more comprehensive model knowledge. \n14 \n• \nLack of Ethical Safeguards: During this research, no measures were taken to cleanse or prevent the generation \nof unethical texts. This means that the models might unintentionally produce biased, toxic, offensive, or harmful \ncontent and fail to filter such content. \n• \nVariability in Logical and Numerical Processing: The models may not perform well in logical reasoning and \nmathematical calculations, a challenge common to all large language models. They might handle some scenarios \nwell but struggle in others due to current training methodologies. This presents an important area for improving \nnumerical problem-solving capabilities in models. \n• \nAbsence of a Specific Persian Evaluation Dataset: The lack of a specialized dataset for evaluating Persian text \ngeneration significantly limits our ability to precisely evaluate and compare the performance of models in Persian \nlanguage tasks. This absence hinders comprehensive comparative analysis with other models in the same \nlanguage. \n• \nHardware Limitations: Our training was limited due to restricted access to computational resources, specifically \ntwo A100 GPUs with 80 GB capacity for a short duration. This not only reduced the depth of our training process \nbut also limited the scope of experiments and extensive improvements of the models. \n \n6 CONCLUSION \nThis study marks a significant milestone in Persian natural language processing by introducing PersianLLaMA, the first \nlarge-scale language model for Persian. PersianLLaMA demonstrates remarkable proficiency in understanding and \ngenerating Persian text, outperforming existing models. Its development opens new avenues for advanced NLP \napplications tailored to the Persian language, from enhanced prompt-base tasks to sophisticated text analysis tools. This \nachievement not only showcases the potential of language-specific models but also highlights the importance of linguistic \ndiversity in AI research. Despite the achievements reported in this research, PersianLLaMa faces challenges such as limited \nknowledge base and hardware limitations, underscoring the need for continuous development. These limitations provide \ndirections for future improvements. Looking forward, the success of PersianLLaMA paves the way for future innovations \nin language technology, especially for underrepresented languages, encouraging similar endeavors worldwide. \n7 FUTURE WORK \nIn future research, the team plans to expand the model's capabilities by training it on a larger and more diverse dataset. \nThis approach aims to enhance the model's understanding and generation of Persian text, making it more versatile and \neffective across a wider range of applications. By incorporating a broader variety of text sources, the model can achieve a \nmore comprehensive understanding and generating of the Persian language, covering both formal and colloquial styles in \ngreater depth. Additionally, the team intends to explore the development of models with an increased number of \nparameters. This advancement is expected to significantly improve the model's performance by enabling it to capture more \ncomplex language patterns and nuances. The enhanced model will be subjected to rigorous human evaluation to compare \nits performance against current models. This step is crucial for assessing the model's practical effectiveness in real-world \napplications and for guiding further refinements. The aim is to develop a state-of-the-art Persian language model that sets \na new benchmark in natural language processing for Persian. \n \n15 \nDeclarations \nEthical Approval \n\"Not Applicable\" \nAvailability of supporting data \nThe datasets generated and analysed during the current study are not publicly available because they constitute an excerpt \nof research in progress, but are available from the corresponding author upon reasonable request. \nCompeting interests \nThe authors have no relevant financial or non-financial interests to disclose. The authors have no conflicts of interest to \ndeclare that are relevant to the content of this article. All authors certify that they have no affiliations with or involvement \nin any organization or entity with any financial interest or non-financial interest in the subject or materials discussed in this \nmanuscript. The authors have no financial or proprietary interests in any material discussed in this article. \nFunding \nThe authors did not receive support from any organization for the submitted work. No funding was received to assist with \nthe preparation of this manuscript. No funding was received for conducting this study. No funds, grants, or other support \nwere received. \nAuthors' contributions \nAll authors contributed to the study's conception and design. Material preparation, data collection, and analysis were \nperformed by Abbasi, Ghafouri and Firouzmandi. The first draft of the manuscript was written by Abbasi and Ghafouri, \nand all authors commented on previous versions of the manuscript. All authors read and approved the final manuscript. \nAcknowledgments \nThe authors wish to express their sincere gratitude to Vira Intelligent Data Mining Company for generously providing \naccess to their GPU resources. This support was instrumental in facilitating the computational aspects of our research. \nTheir contribution not only enhanced the efficiency of our model training processes but also enabled us to push the \nboundaries of our study, leading to more robust and comprehensive outcomes. We acknowledge and appreciate their \nvaluable contribution to the advancement of our research. \n \nREFERENCES \n[1] \nAnil, G. T. G. R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., Millican, K., Silver, D., Petrov, S., \nJohnson, M., Antonoglou, I., Schrittwieser, J., Glaese, A., Chen, J., Pitler, E., Lillicrap, T., . . . Vinyals, O. (2023). Gemini: A Family of Highly Capable \nMultimodal Models.  \n[2] \nBalachandran, A. (2023). Tamil-Llama: A New Tamil Language Model Based on Llama 2. arXiv preprint arXiv:2311.05845.  \n[3] \nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., & Gehrmann, S. (2023). Palm: \nScaling language modeling with pathways. Journal of Machine Learning Research, 24(240), 1-113.  \n[4] \nChung, H. W., Constant, N., Garcia, X., Roberts, A., Tay, Y., Narang, S., & Firat, O. (2023). Unimax: Fairer and more effective language sampling for \nlarge-scale multilingual pretraining. arXiv preprint arXiv:2304.09151.  \n16 \n[5] \nDevlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv \npreprint arXiv:1810.04805.  \n[6] \nFloridi, L., & Chiriatti, M. (2020). GPT-3: Its nature, scope, limits, and consequences. Minds and Machines, 30, 681-694.  \n[7] \nGhafouri, A., Abbasi, M. A., & Naderi, H. (2023). AriaBERT: A Pre-trained Persian BERT Model for Natural Language Understanding.  \n[8] \nHu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., & Chen, W. (2021). Lora: Low-rank adaptation of large language models. \narXiv preprint arXiv:2106.09685.  \n[9] \nJamali, N., Yaghoobzadeh, Y., & Faili, H. (2021). Percqa: Persian community question answering dataset. arXiv preprint arXiv:2112.13238.  \n[10] Köpf, A., Kilcher, Y., von Rütte, D., Anagnostidis, S., Tam, Z.-R., Stevens, K., Barhoum, A., Duc, N. M., Stanley, O., & Nagyfi, R. (2023). \nOpenAssistant Conversations--Democratizing Large Language Model Alignment. arXiv preprint arXiv:2304.07327.  \n[11] Koubaa, A. (2023). GPT-4 vs. GPT-3.5: A concise showdown.  \n[12] Kudo, T., & Richardson, J. (2018). Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. \narXiv preprint arXiv:1808.06226.  \n[13] Lin, Y.-T., & Chen, Y.-N. (2023). Taiwan LLM: Bridging the Linguistic Divide with a Culturally Aligned Language Model. arXiv preprint \narXiv:2311.17487.  \n[14] Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., & Stoyanov, V. (2019). Roberta: A robustly optimized \nbert pretraining approach. arXiv preprint arXiv:1907.11692.  \n[15] Mirzaee, H., Peymanfard, J., Moshtaghin, H. H., & Zeinali, H. (2022). ArmanEmo: A Persian Dataset for Text-based Emotion Detection. arXiv preprint \narXiv:2207.11808.  \n[16] Mukherjee, S., Mitra, A., Jawahar, G., Agarwal, S., Palangi, H., & Awadallah, A. (2023). Orca: Progressive learning from complex explanation traces \nof gpt-4. arXiv preprint arXiv:2306.02707.  \n[17] Nguyen, X.-P., Zhang, W., Li, X., Aljunied, M., Tan, Q., Cheng, L., Chen, G., Deng, Y., Yang, S., & Liu, C. (2023). SeaLLMs--Large Language \nModels for Southeast Asia. arXiv preprint arXiv:2312.00738.  \n[18] Penedo, G., Malartic, Q., Hesslow, D., Cojocaru, R., Cappelli, A., Alobeidli, H., Pannier, B., Almazrouei, E., & Launay, J. (2023). The RefinedWeb \ndataset for Falcon LLM: outperforming curated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116.  \n[19] Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., & Zettlemoyer, L. (2018, June). Deep Contextualized Word Representations. \nIn M. Walker, H. Ji, & A. Stent, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: \nHuman Language Technologies, Volume 1 (Long Papers) New Orleans, Louisiana. \n[20] Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019). Language models are unsupervised multitask learners. OpenAI blog, 1(8), \n9.  \n[21] Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., & Liu, P. J. (2020). Exploring the limits of transfer learning \nwith a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1), 5485-5551.  \n[22] Rasley, J., Rajbhandari, S., Ruwase, O., & He, Y. (2020). Deepspeed: System optimizations enable training deep learning models with over 100 billion \nparameters. Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining,  \n[23] Ray, P. P. (2023). ChatGPT: A comprehensive review on background, applications, key challenges, bias, ethics, limitations and future scope. Internet \nof Things and Cyber-Physical Systems.  \n[24] Sennrich, R., Haddow, B., & Birch, A. (2015). Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909.  \n[25] Shliazhko, O., Fenogenova, A., Tikhonova, M., Mikhailov, V., Kozlova, A., & Shavrina, T. (2022). mgpt: Few-shot learners go multilingual. arXiv \npreprint arXiv:2204.07580.  \n[26] Staudemeyer, R. C., & Morris, E. R. (2019). Understanding LSTM--a tutorial into long short-term memory recurrent neural networks. arXiv preprint \narXiv:1909.09586.  \n[27] Suárez, P. J. O., Romary, L., & Sagot, B. (2020). A monolingual approach to contextualized word embeddings for mid-resource languages. arXiv \npreprint arXiv:2006.06202.  \n[28] Suri, H., Zhang, Q., Huo, W., Liu, Y., & Guan, C. (2021). MeDiaQA: A Question Answering Dataset on Medical Dialogues. arXiv preprint \narXiv:2108.08074.  \n[29] Taghizadeh, N., Doostmohammadi, E., Seifossadat, E., Rabiee, H. R., & Tahaei, M. S. (2021). SINA-BERT: a pre-trained language model for analysis \nof medical texts in Persian. arXiv preprint arXiv:2104.07613.  \n17 \n[30] Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., & Azhar, F. (2023). Llama: Open \nand efficient foundation language models. arXiv preprint arXiv:2302.13971.  \n[31] Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., & Bhosale, S. (2023). Llama 2: Open \nfoundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288.  \n[32] Wang, C., Cho, K., & Gu, J. (2020). Neural machine translation with byte-level subwords. Proceedings of the AAAI conference on artificial \nintelligence,  \n[33] Xu, C., Sun, Q., Zheng, K., Geng, X., Zhao, P., Feng, J., Tao, C., & Jiang, D. (2023). Wizardlm: Empowering large language models to follow complex \ninstructions. arXiv preprint arXiv:2304.12244.  \n[34] Xue, L., Constant, N., Roberts, A., Kale, M., Al-Rfou, R., Siddhant, A., Barua, A., & Raffel, C. (2020). mT5: A massively multilingual pre-trained \ntext-to-text transformer. arXiv preprint arXiv:2010.11934.  \n[35] Zhang, T., Kishore, V., Wu, F., Weinberger, K. Q., & Artzi, Y. (2019). Bertscore: Evaluating text generation with bert. arXiv preprint \narXiv:1904.09675.  \n[36] Zhao, Z., Li, Y., Hou, C., Zhao, J., Tian, R., Liu, W., Chen, Y., Sun, N., Liu, H., & Mao, W. (2022). TencentPretrain: A Scalable and Flexible Toolkit \nfor Pre-training Models of Different Modalities. arXiv preprint arXiv:2212.06385.  \n[37] Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., & Xing, E. (2023). Judging LLM-as-a-judge with MT-\nBench and Chatbot Arena. arXiv preprint arXiv:2306.05685.  \n \nA  APPENDICES \nA.1 Analysis of Outputs from Persian Models \nIn this section, we present a comparative analysis of responses generated by several Persian language models. Each model \nwas calibrated on a distinct Persian dataset, catering to various domains and styles. Our goal is to evaluate and compare \nthe practical performance of these models in creating relevant and linguistically precise responses. For each dataset, we \nprovided the same input to all models to ensure consistency in comparison. The responses of each model were then \ncompiled side by side to enable direct comparison of the quality of output, relevance, coherence, and linguistic accuracy. \nThis analysis aids in highlighting the strengths and weaknesses of each model in addressing the subtle differences of the \nPersian language. In the tables below, an example of the input and output of the models from the test datasets is presented. \nTable 9:   An example of an input and the models' outputs from the MeDiaQA test dataset is presented. \nInput مزم رق ل خادزا م بل نمیهش هاگو\nی اه هن اد\nی \nسفیی ورد م مب ل\nهنز ی م تسه شز وس اب شمه و\nی لد منود ب متسا وخ\nشل ی چی  ت\nچوی\nکی ابرا\n؟درکد  \nMy lips are red from the inside and sometimes there are white spots on my lips and it is burning. I \nwanted to know what is the reason and what can be done? \nActual Output ميراذگب صيخشت مينوتب ات هشب هتفرگ نوتلكشم زا قيقد لاح حرش و هشب ماجنا ىكينيلك هنياعم دياب ًامتح. \n \nA clinical examination must be done and a detailed history of your problem must be taken so that we \ncan make a diagnosis. \nPersianLLaMA-LoRA هش یر \nالد هب تسا نکمم ش\nلی کی و تنوفع\nیری سو ای رتکاب تنوفعییی ا \nبنآ ناهد رد ود ره هک ،دشا\nداج ی ا اه یم ارب .دننک\nی خشت\nیص\nی قدق، رتهب\nنی هن ی زگ نیا گب سامت صصختم کشزپ اب هک تسا\nدیری ای یک اعمیهن هد ماجنا ناهد\nدی زجت ات\nیه \nولحت \nیل \nردنهد ماجنا ا. \n \nIts root may be due to a viral infection or a bacterial infection, both of which develop in their mouth. \nFor an accurate diagnosis, the best option is to contact a specialist doctor or do an oral examination so \nthat they can perform an analysis. \nPersianLLaMA-Zero وش هجلاعم تسوپ صصختم طسوت تسا رتهب ،هناد زورب لامتحا هب هجوت ابید امزآ ندوب رظن تحت تهج ویش \nادوش ماجن. \n \nDue to the possibility of acne, it is better to be treated by a dermatologist and to be tested for \nmonitoring. \n18 \nGPT2-Persian \nنم نم نم  \nI I I \nmGPT مارگ تسود\nی نزا ی اعم هبیهن چراق تنوفع تسا نکمم .تسا\nی \nبنک هعجارم تسوپ صصختم هب .دشاید\nوراد نامرد .\nیی نزی  هب .دوش ماجن\nب قلح شوگ صصختم\nینی تروص رد .دبنک هعجارمی \nکن دوبهب ه\nتفا ی \nبلخاد صصختم ه\nی نک هعجارم. \n \nDear friend, you need to be examined. It may be a fungal infection. See a dermatologist. Drug \ntreatment should also be done. Refer to an otolaryngologist. If there is no improvement, see an \ninternist. \nParsgpt \nدمارگ تسو\nی ضوت هب هجوت ابیتاح \nخلسانت لاخبت هب التبا لامتحا دو\nی \nمم حرطیدوش\nامن هعجارم وم و تسوپ صصختم هب .\nد یی.  \nDear friend, according to your explanation, the possibility of contracting genital herpes is raised. See \na skin and hair specialist \nmT5 اعم تسا مزالیهن وشید \nYou need to be examined \n \nThe response from PersianLLaMA-LoRA is detailed and relevant, suggesting a potential cause and appropriate action. \nPersianLLaMA-Zero's and mGPT's responses are also practical, though less specific. GPT2-Persian's output is irrelevant, \nand Parsgpt's response is inaccurate for the described symptoms. mT5's response, while safe, lacks helpful details. \nTable 10:   An example of an input and the models' outputs from the Alpaca test dataset is presented. \nInput هد حرش ار درز گنرید \nYou need to be examined \nActual Output \nزرکی د شروخ رون اب لومعم روط هب هک تسا شوج و بنج رپ و نشور گنری\nد، ی داش ا .تسا طبترم امرگ و\nین یک \nرلصا گن\nی ط رد\nیف \nمئری\nب هک تسا\nنی جنران و زبس\nی رون رظن زا .دراد رارق گنر هخرچ رد\nی، ب جوم لوط اب رون گنر\nین \n570\n \nتا 590\n ماگنه .تسا رتمونان\nی  رد درز ه\nبطیتع هد ید یم شو،د م ،اه لگ رد بلغا\nهو ی اه گرب و اه\nی یز یی اپ م گنر .دراد دوجو\nی اس زا دناوت\nیه ی اه رقت و گنر مک\nاب ی \nپلتسا\nی  گنر \nی اه قی مع نغ و رت\nی الط دننام رت\nیی ای \nخی لدر غتمیر نچمه .دشاب\nنی هجت رد ًالومعم\nتاز ی ی نم یا م هدافتسا سابل و\nی اج ،دوش\nیی د هک\nید زیاد\nم هتسجرب ار نآ\nی دنک. \n \nYellow is a bright and vibrant color that is typically associated with sunshine, happiness and warmth. It \nis a primary color in the visible spectrum that lies between green and orange on the color wheel. Optically, \nthe color of light with a wavelength between 570 and 590 nm. When yellow is seen in nature, it is often \nin flowers, fruits, and autumn leaves. The color can range from pale and almost pastel shades to deeper \nand richer colors such as golden or mustard. It is also commonly used in safety equipment and clothing, \nwhere it is highly visible \nPersianLLaMA-LoRA \nزلصا گنر در\nی \nابآ لباقم رد الومعم هک تس\nی ب رد و دراد رارق\nنی فیط اهداب زا رون\nی عی ام م هداد ناشن درز دناب ات\nی داش اب بلغا .دوش\nی،  ام\nژرنا و\nی \nملک روط هب .تسا طبتر\nی، این یک \nرم بوسحم شوج و بنج رپ و داش گن \nی ارب هک دوش\nی ی را ی سب \nاما دامن اه گنهرف ز\nید،  و ها\nق ترد\nتسا.صلاخ دننام و تدش هب هتسب \nی، حم و رون\nیط \nام ،نآ فارط\nی اه گنر زا دناوت\nی \nگ رت مر\nیا \nسفلتخم عاونا .دشاب رت د ر\nی اس زا\nیه اهی\nم هک دراد دوجو درز\nی ون طسوت ناوت\nی\nناگدنس بش ،گنریه اه زاگ ،زبسرس نمچ گنر هب\nی \nآ باتف\nای اه باقع\nی نازوس. \nYellow is the main color that is usually opposite to blue and is shown in the spectrum of light from liquid \nwinds to the yellow band. It is often associated with happiness, warmth and energy. In general, it is \nconsidered a happy and vibrant color that symbolizes hope, prosperity and strength for many cultures. \nDepending on the intensity and purity, the light and the surrounding environment, it can be of warmer or \ncooler colors. There are many different shades of yellow that can be painted by writers, similar to the \ncolor of green grass, sun rays or burning eagles. \nPersianLLaMA-Zero شروخ اب درزید \nمم طبتریدوش انشور دامن ناونع و\nیی م رارق هدافتسا دروم امرگ وییگرد. \nYellow is associated with the sun and is used as a symbol of brightness and warmth. \nGPT2-Persian \nم تکرح هب عورش هحفص طسو زا الومعم اهگنر نوچ ؟ ارچ بوخی\nکدنن \nthen why? Because the colors usually start moving from the middle of the screen \nmGPT گنر درز\nی \nگشمارآ و مر\nا .تسا هارمه شمارآ و شمارآ اب بلغا هک تسا شخب\nین \nربط اب بلغا گنی\nع،ت  بلغا و تسا هارمه نوکس و شمار\nی ارب داج یا \nحتحار و شمارآ س\nی م هدافتسایدوش\nگنر ناونع هب بلغا درز .\nی \nآماربمارآ و شخ\nارب شخب\nی داج یا مارآ سح\nی نی ب رد  هدافتسا دارف\n19 \nمیدوش ا و\nبلغ ی ارب داج یا کی طی حم م هدافتسا مارآ و مارآیدوش \nکم هیدناوت \nبلک روط هب .دنک کمک زکرمت و شمارآ هی، گنر درز\nی \nآشمار\nزی گنا\nومارآ \nم هک تسا روآیدناوت ا ار شمارآ سح\nداج ی ا هب و دنک\nداج ی کی طی حم ی مارآ مه هب .دنک کمک شمارآ و\nنی لی لد سب هک تسا\nی را ی  ز\nرب نآ زا دارفا\nیا ایداج یک \nمحیط \nرم هدافتسا شمارآ و تحایدننک. گنر درز ،هصالخ روط هب یک \nرمارآ و مرگ گن\nریگ م هک تسایناوت  ار \nب ناونع ه\nیک \nرمارآ گنبمارآ ،شخب شخ\nیا \nآمار\nتفرگ رظن رد شخب. \n \nYellow is a warm and relaxing color that is often associated with calmness and relaxation. This color is \noften associated with nature, peace and tranquility and is often used to create a sense of calm and comfort. \nYellow is often used as a calming and soothing color to create a sense of calm among people, and is often \nused to create a calm and peaceful environment that can aid in relaxation and concentration. In general, \nyellow is a soothing and calming color that can create a sense of calm and help create a calm and peaceful \nenvironment. That is why many people use it to create a comfortable and relaxing environment. In short, \nyellow is a warm and soothing color that can be thought of as a soothing, calming, or calming color \nParsgpt ز تسا زبس گنر درز\nار ی \nنناش\nشروخ رون هدنهد\nید \nوی بآ  \nان زبس گنر بلغا و تس\nزی مانیهد م\nیدوش\nنی ا . \nرسب گن\nرا ی ز ،تسا هدنزرس\nار ی \nنناش\n هدن\nژرنا\nی، ی داش تحار و\nی الم بلغا درز گنر ،زور لوط رد .تسا\nمیت،ر \nآمارتمارآ و ر\nز ،تسا رت\nار ی نهذ هافر رب\nی، \nعفطا\nی فطاع و\nی \nتری ثأ می\nگدراذ \nYellow is green because it represents sunlight and blue, and is often called the color green. This color is \nvery lively because it represents energy, happiness and comfort. During the day, yellow is often softer, \ncalmer, and more peaceful, as it affects mental, emotional, and emotional well-being. \nmT5 \nزتسا درز گنر گنر در \nYellow is the color yellow \n \nPersianLLaMA-LoRA's response is creatively engaging but overly complex. PersianLLaMA-Zero's answer is succinct and \nsomewhat relevant. GPT2-Persian is off-topic again. mGPT provides an overly detailed and somewhat repetitive response, \nwhile Parsgpt's answer is confusing and incorrect. mT5's response is technically accurate but uninformative. \nTable 11:   An example of an input and the models' outputs from the OASST1 test dataset is presented. \nInput هچ GPU ق نازرا\nتم ی یا ارب ار\nی \nآ شزومیک صوت نابز لدم\nیه می \nکنید \nWhat cheap GPU would you recommend for training a language model? \nActual Output نیا گتسب تدش هب\nی رامعم و هزادنا هب\nی م امش رگا ؟دنتسه شزومآ هب دنم هقالع امش لدم زا عون هچ !شزومآ لابند هب امش لدم\nی هاوخید\nب هب بقع هیک م راک هجدوب زا لدم\nی هد عالطا نم هب ،دننکید رتهب نم و\nنی ارب ار دوخ شالت\nی پیاد فارگ تراک ندرکیک ی ارب  لدم شزوم\nی اه یری گدا ی نی شام \nب سپس و ،ار دوخ لوپ ا\nکی صوت هارمه هزادنا و لدم\nهی یم منک. \nIt highly depends on the size and architecture of the model you are looking for! What type of model are \nyou interested in training? If you want to work back to a model of the budget, let me know and I will do \nmy best to find a graphics card to train machine learning models with your money, and then recommend \na model and size. \nPersianLLaMA-LoRA ن ساسا رب\nی\nاهزا\nی صوت ،همانرب و امشیه یم منک Nvidia GTX 1050 ای AMD Radeon RX 570 گب رظن رد ار\nنی ا . دیری اه تراک\nی \nGPU \nبلک روط ه\nی ق نازرا\nتم ی \nهعقاو نامز رد اهنآ درکلمع دروم رد و دنتس\nی ای ی زاب ای ی ارب یای گدری کی قحت نابز لدمیتاق ی بوخ  دو\nد لاس زا .درا2021\n \n، اب \nوت ا ، رازاب زرا خرن هب هج\nنی تم یق مک تسا نکمم اه\nی \nمدنشاب توافت. \nBased on your needs and application, I recommend considering the Nvidia GTX 1050 or the AMD \nRadeon RX 570. These GPU cards are generally inexpensive and there is good research on their \nperformance in real-time or gaming for learning a language model. From 2021, depending on the market \nexchange rate, these prices may be slightly different. \nPersianLLaMA-Zero ی ارب \nآم شزو\nیک \nمم لدیناوت \nبهب تروص ه\nهن ی اه شور زا هدش\nی دی دج ا هک درک هدافتسا\nنی م ثعابیدوش رتمک نامز رد دناوتب لدمی\nآبب شزوم\nدن ی. \nTo train a model, new methods can be used in an optimized way, which makes the model able to be \ntrained in less time. \nGPT2-Persian \nتم یق آییدیا وبراک\nی ارب بسانم\nی ا هنابزود نابزیی نار \nوجراخ \nی  \nThe price of IAD Karboi is suitable for bilingual Iranian and foreign languages \n20 \nmGPT \nGPUی اه یکی فارگ \nانازر\nتم یق ک اب و\nتیفی ارب الاب\nی \nآم شزو\nیک \nملد\nی اه \nزخرب .دنتسه ناب\nی زگ زا\nینی اه ه لماش بوبحم NVIDIA \nGeForce RTX\n۳۰۹۰\n، \nNVIDIA Ge Force RTX \n۴۰۹۰\n و NVIDIA Geforce RTX \n۴۰۸۰\n ا .تسا\nنی GPUاراد اه\nی  هظفا\nو  هظفاح\nزیی دا م و دنتسهیدنناوت ی ارب فی اظو لوت دننام فلتخمید واصت\nیر، شی ار یو یودیو، \nتدی لو غ و نتم\nهر ی ا رب هوالع .دنوش هدافتسا\nین،  اهن\nی اراد کی ربراک طبار\nی ارب ناسآ و هداس\nی ارب ار اهنآ هک دنتسه هدافتسا\nی م بسانم نابز ناصصختم و نازومآ شنادیدنک\nا رب هوالع .\nنی  اه\nی اراد کی فارگ طبار\nیکی انشآ و هداس\nیی \nب هب ار اهنآ هک دنتسه نابز ا\nکی هن ی زگ ارب بوبحم\nی دبت نازومآ شناد\nلی میدنک\nنی ا . GPU ی اراد\nکی فارگ طبار\nی هاگآ و هداس\nی \nب ناونع هب ار اهنآ هک تسا نابز ایک هن ی زگ ارب بسانم\nی دبت نازومآ شناد\nیل می\nکدن. \nGraphics Processing Units (GPUs) are cheap and high quality for training language models. Some \npopular options include NVIDIA GeForce RTX 3090, NVIDIA GeForce RTX 4090, and NVIDIA \nGeforce RTX 4080. These GPUs have a lot of RAM and memory and can be used for various tasks like \nimage generation, video editing, text generation, etc. In addition, they have a simple and easy-to-use \ninterface that makes them suitable for students and language professionals. In addition, they have a simple \ngraphical interface and familiarity with the language, which makes them a popular choice for students. \nThese GPUs have a simple graphical interface and language awareness, which makes them a suitable \noption for students. \nParsgpt \nGPU ارب بسانم\nی نیا راک NVIDIA یم\nدشاب. \nThe suitable GPU for this work is NVIDIA. \nmT5 ق نازرا\nتم ی GPU \nاق نازر\nتم ی GPU ق نازرا\nتم ی GPU ق نازرا\nیمت GPU\n \nInexpensive CPU Inexpensive CPU Inexpensive CPU Inexpensive GPU \n \nPersianLLaMA-LoRA gives a practical and relevant recommendation. PersianLLaMA-Zero's response is too general. \nGPT2-Persian is off-topic, mGPT's suggestions are high-end and not budget-friendly, Parsgpt's response is too vague, and \nmT5's answer is repetitive and irrelevant. \nTable 12:   An example of an input and the models' outputs from the PerCQA test dataset is presented. \nInput ه هدروخامرس هزور دنچ.. هشهامود و لاسود مچب\nی م شهب ورادیمد هد ی اف باوخ هک نالا هرادنید من راگنایهنوت ا لثم هشکب سفن\nهکن ی هن یس  ش\nچ  .تسه تنوفع رپ\nی ؟؟نوترظنب مدب شهب  \nMy child is 2 years and 2 months old. He has had a cold for a few days. I give him medicine, but it doesn't \nhelp. Now that he is sleeping, he can't breathe, as if his chest is full of infection. What do you think I \nshould give him??  \nActual Output اه هچب\nی مه ستفه ود نم\nنی ی روط \nشلک مدرب رتکد راب هس ند\nی ی تنآ یبوکیت اف ندروخ\nهد ی درب بشما تشادن\nمی ساسح تفگ رتکدی\nته\nآ منم هچب.دز لوپم\nی\nهشلاسک م عمج شولگ وت طلخ دقنایهش می\nداوخ \nخباوخن هفرس زا هبش ود. هشب هفیهد  \nMy children have been like this for two weeks. I took them to the doctor three times. Taking antibiotics \ndidn't help. Tonight we went to the doctor, who said it was an allergy and gave him an injection. My \nchild is one year old, so much phlegm collects in his throat that he wants to suffocate. He hasn't slept for \ntwo nights because of his cough. \nPersianLLaMA-LoRA بر \nاس هئارا زا لبق تسا مزال ،هدش هئارا تاعالطا سا\nکی زرا عماج خساپ\nیایب سفنت مئالع .دوش\nی، ر تنوفع و ناطرس\nهی  هب رجنم تسا نک\nیبی کرت \nام هک لماوع ز\nی ثأت باوخ رب دناوت\nری راوشد دننام ،د راذگب\nی \nتس درد ،سفن\nهن ی \nوتحاران \nی \nبی ند.م امش رگا \nی هاوخید صوت\nیه \nهنام یا  \nرطم\nح ی نکد، گب سامت کشزپ اب تسا رتهب\nدیری ای ی ارب کی سررب\nی کشزپ\nی نک هعجارمید عضو ات\nتی رام یب زرا ار\nی با ی دی نک و کی نامرد همانربی\nا بسانمیداج \nکن.د \nBased on the information provided, it is necessary to evaluate before providing a comprehensive answer. \nRespiratory symptoms, cancer, and lung infection may lead to a combination of factors that can affect \nsleep, such as difficulty breathing, chest pain, and physical discomfort. If you want to make our \nrecommendations, it is better to contact your doctor or go for a medical examination to evaluate the \npatient's condition and create an appropriate treatment plan. \nPersianLLaMA-Zero مز ی زع هی وگب رتکد هب مه راب \nMy dear, tell the doctor once more \n21 \nGPT2-Persian \nب هچ\nرام یی \nwhat disease \nmGPT مه منم رتخدی\nنهی روج.لو .مداد شهب وراد منم \nی \nخم مراد منم .دوب بویمروخ.ی لو  م ومراد منم .هبوخ بخیمروخ م ویمروخ ویل  هب .هبو\nم هراد منم .هدب شهب مرظنیمروخ میمدروخ\nی لو . \nخو.هک هب \nMy daughter is like that. I gave him medicine. But it was good. I am eating too. But that's fine. I eat and \neat, but it's good. I think give it to him. I am eating. I was eating. But that's good. \nParsgpt ب وت تقو ره\nی\nناتسرام \nبماتسا شه\nی\nنفون اب هدب\nدی هشب بوخ تعاس دنچ ات  \nWhenever you give her acetaminophen in the hospital, she should get better in a few hours \nmT5 می\nشه \ncan \n \nPersianLLaMA-LoRA's response is comprehensive and cautious, suggesting a medical evaluation. PersianLLaMA-Zero's \nadvice is simple yet appropriate. GPT2-Persian's output is non-informative. mGPT provides a personal anecdote without \nclear relevance. Parsgpt's advice is specific but lacks context. mT5's response is too vague to be helpful. \nA.2 Analysis of PersianLLaMA Outputs Fine-Tuned on the Alpaca Dataset \nPrompt-based models, particularly since the emergence of ChatGPT, have gained significant importance in the field of \nnatural language processing. The concept of prompt-based modeling revolves around the idea of providing a model with a \n'prompt' – a piece of text that guides or stimulates the model to generate a specific type of response or perform a specific \ntask. In the second section of the appendix, we focus on the PersianLLaMA-LoRA model, fine-tuned on the Alpaca dataset, \nwhich includes prompts and outputs. Here, we display a series of inputs and corresponding outputs generated by our Persian \nmodel. The selected inputs and outputs highlight the model's skill in understanding and responding in Persian, \ndemonstrating its ability to manage complex linguistic structures and specific cultural references. Table 13 presents \nexamples of the performance of the PersianLLaMA-LoRA model in prompt-based tasks. \nTable 13:   Examples of the performance of the PersianLLaMA-LoRA model in prompt-based tasks. \nInput ا تاساسحا\nین \nملحت ار نت\nلی اش :نکید دی گب \nالو هدش رتهب داصتق\nی گدنز نم\nی ی بوخ \nنمراد \n \nGenerated output \nاح هدش هداد نتم تاساس\"م رظن هب\nی \nرس دوبهب داصتقا هک د\nهتفا ی \nاس نم اما ت\nکی گدنز\nی \nعی لا مرادن\" ی کرتبی داش زا\nی اضران وییت \nت .\nداش ساسحا\nی لد هب\nلی لاح رد تسا داصتقا دوبهبی \nکماان سح ه\nیدی لد هب\nلی صخش هبرجت مدع\nی \nشی دا \nا.تس \nInput کی روگلا\nمت ی ارب هداس\nی ل دمآراک ندرک بترمی\nسیت حص دادعا زا\nحی ترت هب\nبی دوعص\nی حارط\nی ی نکدآرف .\nدن ی ضوت ار ماگ هب ماگ\nحی دی هد  \nیپچی گد ی ی نامز زجت ار دوخ لح هار\nیه \nولحت \nلی نکیقم . دیسا یری ذپ گب رظن رد ار\nدیری رانس دروم رد ویی اهو ا هوقلاب\nی نک ثحب\nدی  نآ رد ه\nگلا\nمت ی رو \nمهب درکلمع تسا نکم\nهن ی \nد دشاب هتشا\nای اه شلاچ اب\nیی \nرو.دوش ورب \n \nGenerated output ی ا رد\nاجن کی ارب هداس شور\nی \nمل دمآراک ندرک بتر\nتس ی حص دادعا\nحی ترت هب\nبی شی ازفا \nوجو :دراد د1\n. کی هی ارآ ی لاخ \nبا هزادنا n+1\n (n  ل\nتس یل دورو\nا ) ییداج نکی\nد، یی اج \nک هn ل رد رصانع دادعت\nتس ی دورو\nی \nاس .ت2رط زا رارکت .\nقی ل رد رصانع مامت\nتس ی دورو\nی. \n3 رد رصنع رگا \nمتی عقو \ni-1\n عقوم رد رصنع زا رتمک\nتی i وعت ،دشاب\nضی گ تروص\nدر ی. 4اپ ات رارکت .\nنا ی پ .هقلحییچدیگ ی نامز انی روگلا\nمت ی \nO ((n\n  .تسا\nیپچی گد ی اضف\nیی ن نآ\nیز \nO(nز تسا )\nار ی هی ارآ ی ارب هر ی خذ اه تلاح رد رصانع ندرک\nی م هدافتسا الاب\nی \nشو .د\nیکی ازم زا\nیای نیا ی ور\nدرک نیا\nن هک تسا\nی زا ی ضف هب\nای \nای فاض ارآ دننام\nهی ی اه جراخ\nی ی ارب هر ی خذ ی زاس ای راکتسد\nی ن رصانع\nتس ی.نچمه \nین \nققم لبا\nسا ی ی دنب ی ارب لیست\nی اه \nبز گر\nای تس یل ی اه ارآ هک روطنامه ،تسا کچوک\nیه \nها  .دنتسه رتگرزب تباث هزادنا اب\nیک \nچلامتحا شلا\nی \nوج رگا دراد دوn \nکم زا رتn \n+ 1\n ز ،دشاب\nار ی هی ارآ \nهی م ا ناوت\nدن عقوم هب\nتی ی اه ریز \nاض.دنوش هفا \nInput انعم\nی گدنز\nی \nر رظنم زا ا\nکی ضوت لتاق\nحی ی هدد \n \n22 \nGenerated output د زا\nی\nهاگد کی انعم ،لتاق\nی \nزگدن\nی ی م ار \nت فده اب ناو\nیا \nهفد\nیاه رعت دوخ\nفی ا .درک\nین \nف ناونع هب هک دنک ساسحا تسا نکمم دریک  لت\nم لمع\nی \nکظو ساسحا تسا نکمم وا .دهد ماجنا ار تلادع و دنک کاپ ار ناهج ات دن\nهف ی دهت ار هعماج هک ار رورش دارفا هک دنک\nدی می  دن\nا .دنک تازاجم\nنی انعم هب راک\nی ی عقاو م هملک\nی نعم و فده سح دناوت\nی ایداج ارب نآ رد بلغا هک دنک\nی سبیی را نچمه .تسا راوشد مدرم زا\nین\nارب تسا نکمم\nی \nقا لت\nکی اضر عون\nتی نورد\nی ز ،دنک مهارف\nار ی م ساسحا بلغا اهنآ\nی \nکظو هک دنن\nهف ی قالخا\nی اش و\nی\nهتس یا  هتفرگ هدهع رب ه\nان.دنا هداد ماجنا ار د \nInput ی ارب مرسمه کی ونب هناقشاع همان\nیس \n \nGenerated output م هاگن ار وت هک نامز ره\nی ز ساسحا ،منک\nیی اب ی یم \nکن ناونع هب هک م\nکی  و دنامب نم شوغآ رد ات دوب هدش هداتسرف تشهب زا لماک دوجو\nز نامشچ اب وت .دنک رپ ار مقشع\nاب ی \nو  هب ار مبلق ،مرگ دنخبل\nکی ب جوم\nی نا ی اپ دبت تاساسحا\nلی می ی نک من ار نم هک\nی دان مناوتیهد مر ی گب\nتو ارم \nزا \nلمسج ظاح\nی، \nعفطا\nی ونعم و\nی \nتتی وق می \nکین دوجو وت هک متسه رازگساپس نم و\nی \nرارب ا\nی ا هدرک مهارف نم\nی گدنز نآ اب هکی\nیم ا زا .منک\nهکن ی \nتشخب و\nی \nاگدنز ز\nی \nاتسه م\nی، اهزور .مرازگساپس\nی رپس وت اب ار\nی ش تاظحل و هدرکینیر \nوسب راد هدنخ \nی را ی  کارتشا هب \nا هتشاذگینی ا . م تسه نم اب وت هک روطنامه .دش دهاوخن فقوتم زگره ام قشعی، مهیهش مع قشع اب .نآ زا دعب و دبا ات ،دوب مهاوخ وت اب\nیق  \nچ ره زا\nیزی \nکه.ما هتشاد قلعت نآ هب ًالبق  \nInput \nل افط\nیک \nد هرابرد ناتسایک \nدوگب کچوک رتخ \n \nGenerated output \nدکی ر \nز رد ،زارد و رود نامیک اتسور\nی م رد هک کچوک\nنا ی اه هپت\nی اه نمچ و دنلب\nی \nسب ،تشاد رارق ز\nکی ل مان هب ناوج رتخد\nیلی \nزگدنی\nمی \nکبط رطاخ هب وا .در\nتع ی مه و دوب هدش هتخانش شا هناتسود و داش\nهش ی ور رب دنخبل اب\nی م شتروص\nی ا زا لبق ،حبص زور ره .دمآ\nهکن ی قبیه\nراتسو\nیی اه بیراد ل ،دنوش\nیلی ب باوخ زایراد یم ح هب ات دش\nی\nتاناو م نمچ هب ار دنفسوگ وا .دنک کمک هعرزم\nی ذغت ار دنفسوگ ،دربیه یم  د\nم هناخ رد ار اه غرم و\nی ارب ،نآ زا سپ .دروآ\nی روآ عمج\nی اهراک ماجنا و اه غرم مخت\nی د فلتخم\nیگ،ر م هعرزم فارطا هب وا\nی \nرل .تف\nیلی  تس\nب تشاد\nنور ی ز ،دشاب\nار ی ح رد .دوب شفارطا شوج و بنج رپ ناهج قشاع وای\nاط \nمهاگ وا ،هعرز\nی م تاقوا\nی د تسناوتید اهادص و\nی  ار ناگدن\nا غاب رد هک گرزب طولب ناتخرد تشپ رد .دونشبیهداتس م وا ،دندوب\nی ادص تسناوت\nی \nپر .دونشب ار ناگدن \nInput \nدبط نابز شزادرپ هرابر\nیعی ضوت\nیح \nبهد \n \nGenerated output \nپبط نابز شزادرییع (\nNLP\n) کی عونصم شوه زا هزوح\nی اه لدم تخاس اب هک تسا\nی پماک\nی رتو ی ی ارب سفت ،کرد\nری لوت و\nدی  راک ناسنا ناب\nیم ا .دنک\nین \nش لما\nیایری گد ی شامن، یری گدا ی معیق بط نابز شزادرپ وییع \nاس .تNLP\n کی مزیهن ای وپ اه لاس رد و تسا\nی ری خا یپشتفر\nیی اه و هب ،تسا هتشاد\nی\nازفا اب هژ\nشی روگلا هعسوت و هداد عبانم\nمت ی ی اه یپ\nهتفرش \nتبصع هکبش دننام ر\nی\nاهدربراک .\nی \nNLP\n تسد لماشی\nانار\nزاجمی، \nتشام همجر\nینی، زجت ،راتفگ شزادرپیه \nولحت \nلی شام همجرت ،تاساسحاینی، \nچنتم تاعالطا جارختسا و اه تاب ت\nی \nاا .تس\nنی روانف\nی\nا ناکمایداج اه همانرب\nی دربراک\nی م مهارف ار\nی \nکن م هک د\nی ن و دننک لماعت اه ناسنا اب رثوم روط هب دنناوت\nی\nاهزا\nی  هار و ،دننک هدروآرب ار اه\nاه لح\nی ی را ی سب پ تالکشم زا\nیچهد ی \nرم هئارا ا\nی \nد.ده \nInput بوخ زا\nی اف و اه\nهد ی ی اه وگب جاودزا \n \nGenerated output یکی ازم زا\nیای \nای لص ا جاودزا\nین \nاس اب دارفا هک ت\nرگ ی دک ی یم \nتو دننا\nکی کی رش گدنز\nی پیاد \nکامح ،کمک اهنآ هب هک دنن\nتی م هئارا قشع و\nی\nا رب هوالع .دهد\nین، سبیی را هار جاودزا هک دندقتعم اه جوز زا\nی ی لاع ی ارب \nنش هب دوخ دهعت و دهعت ،قشع نداد نا\nرگ ی دک ی  ساسحا بلغا و تس\nای نمتتشپ ،\nی ناب ی اضر و\nتی بیی رتش نچمه جاودزا .دنراد\nنی یم ازم دناوت\nیای \nمی لا لام دننامیتا \nکو هب ،دشاب هتشاد رتم\nهژ ی اراد جوز رگای\nا رب هوالع .دنشاب نادنزرف\nین، اه جوز\nیی لام رظن زا هک\nی ن دنتسه هباشم\nزی یم مصت دنناوت\nیمی تا \nرنماان و رقف لامتحا هک دننک ذاختا ا\nی داصتقای\nار م شهاک\nی ا رب هوالع .دنهد\nین، \nام جاودز\nی فطاع تابث هب رجنم دناوتی، نالوط رمع لوط ،رتهب ناور تمالس\nی اضر رتالاب حطس و رت\nیت  \nی دنز \nشاهن رد .دو\nیت، \nدا نتش\nکی ا و رسمه\nداج ی یک \nخم هداونا\nی ا هنازاورپدنلب فادها اب ار درف دناوتی من لابند ًالبق هک\nی  دشکب شلاچ هب ،\nوصخش دشر \nی \nرا.دنک مهارف  \n \n \nThis section discusses the evaluation of PersianLLaMA, based on its responses to various inputs. Key observations include: \n• \nContextual Understanding: The model demonstrates a good grasp of context in its responses, accurately identifying and \nresponding to the emotional tone of inputs, showcasing an understanding of subtle human emotions. \n• \nCreative and Narrative Skills: In creative tasks like writing love letters or storytelling, the model displays strong abilities in \ngenerating coherent and engaging narratives, using descriptive language to exhibit its creative writing capabilities. \n23 \n• \nAdaptability to Diverse Perspectives: The model's responses to requests requiring perspective-taking, like explaining life from \na murderer's viewpoint, show its capability to adapt to diverse and complex viewpoints, important for creating empathy and \nrich content. \n• \nGeneral Technical Knowledge: The model can provide general, informative explanations covering the basics, as seen in \nresponses to technical queries, like those about natural language processing. \n \nOverall, PersianLLaMA demonstrates proficiency in a range of tasks from creative writing to basic technical explanations, with a strong \ncontextual understanding. However, it shows potential areas for further development in handling highly technical or specialized topics, as \nwell as sensitive ethical issues. Its adaptability and creative capabilities are highlighted as major strengths, making it a versatile tool for a \nwide range of applications in Persian language processing. \n",
  "categories": [
    "cs.CL",
    "cs.AI"
  ],
  "published": "2023-12-25",
  "updated": "2023-12-25"
}