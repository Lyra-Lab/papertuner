{
  "id": "http://arxiv.org/abs/2312.16074v2",
  "title": "Unsupervised Learning of Phylogenetic Trees via Split-Weight Embedding",
  "authors": [
    "Yibo Kong",
    "George P. Tiley",
    "Claudia Solis-Lemus"
  ],
  "abstract": "Unsupervised learning has become a staple in classical machine learning,\nsuccessfully identifying clustering patterns in data across a broad range of\ndomain applications. Surprisingly, despite its accuracy and elegant simplicity,\nunsupervised learning has not been sufficiently exploited in the realm of\nphylogenetic tree inference. The main reason for the delay in adoption of\nunsupervised learning in phylogenetics is the lack of a meaningful, yet simple,\nway of embedding phylogenetic trees into a vector space. Here, we propose the\nsimple yet powerful split-weight embedding which allows us to fit standard\nclustering algorithms to the space of phylogenetic trees. We show that our\nsplit-weight embedded clustering is able to recover meaningful evolutionary\nrelationships in simulated and real (Adansonia baobabs) data.",
  "text": "Unsupervised Learning of Phylogenetic Trees via Split-Weight Embedding\nYibo Kong\nGeorge P. Tiley\nClaudia Sol´ıs-Lemus1\nUniversity of Wisconsin-Madison\nKew Royal Botanic Gardens\nUniversity of Wisconsin-Madison\nAbstract\nMotivation: Unsupervised learning has become a staple in classical machine learning, successfully\nidentifying clustering patterns in data across a broad range of domain applications.\nSurprisingly,\ndespite its accuracy and elegant simplicity, unsupervised learning has not been sufficiently exploited\nin the realm of phylogenetic tree inference.\nThe main reason for application gap is the lack of a\nmeaningful, yet simple, way of embedding phylogenetic trees into a vector space. Indeed, much work\nhas been done in the embedding of genomic sequences, but embedding of graphical structures as\nphylogenetic trees for clustering tasks remains understudied.\nResults: Here, we propose the simple yet powerful split-weight embedding which embeds phylogenetic\ntrees into a Euclidean space allowing us to fit standard clustering algorithms to samples of phylogenetic\ntrees. Via extensive simulations and applications to real (Adansonia baobabs) data, we show that our\nsplit-weight embedded clustering is able to recover meaningful evolutionary relationships.\nAvailability and Implementation: We created the first open-source software for unsupervised learning\nof phylogenetic trees in the new Julia package PhyloClustering.jl available on GitHub https://\ngithub.com/solislemuslab/PhyloClustering.jl.\nContact: solislemus@wisc.edu.\n1\nIntroduction\nThe Tree of Life is a massive graphical structure which represents the evolutionary process from single cell organisms\ninto the immense biodiversity of living species in present time. Estimating the Tree of Life would represent one of\nthe greatest accomplishments in evolutionary biology and systematics, and it would allow scientists to understand\nthe development and evolution of important biological traits in nature, in particular, those related to resilience to\nextinction when exposed to environmental threats such as climate change. Therefore, the development of statistical\nand machine-learning theory to reconstruct the Tree of Life, especially those scalable to big data, are paramount in\nevolutionary biology, systematics, and conservation efforts.\nThe graphical structure that represents the evolutionary process of a group of organisms is called a phylogenetic tree.\nA phylogenetic tree is a binary tree whose internal nodes represent ancestral species that over time differentiate into\ntwo separate species giving rise to its two children nodes (see Figure 1 left). The evolutionary process is then depicted\nby this bifurcating tree from the root (the origin of life) to the external nodes of the tree (also called leaves) which\nrepresent the living organisms today. Mathematically, a rooted phylogenetic tree T on taxon set X is a connected\ndirected acyclic graph with vertices V = {r} ∪VL ∪VT , edges E and a bijective leaf-labeling function f : VL →X\nsuch that the root r has indegree 0 and outdegree 2; any leaf v ∈VL has indegree 1 and outdegree 0, and any internal\nnode v ∈VT has indegree 1 and outdegree 2. An unrooted tree results from the removal of the root node r and the\nmerging of the two edges leading to the outgroup (taxon 4 in Figure 1 left). Traditionally, phylogenetic trees are drawn\nwithout nodes (Figure 1 center) given that only the bifurcating pattern is necessary to understand the evolutionary\nprocess. The specific bifurcating pattern (without edge weights) is called the tree topology. Edges in the tree have\nweight we ∈(0, ∞) that can represent different units, evolutionary time or expected substitutions per site being the\nmost common. Trees with edge weights are called metric trees.\nOne of the main challenges when inferring the evolutionary history of species (denoted the species trees) is the fact that\ndifferent genes in the data can have different evolutionary histories due to biological processes such as introgression,\nhybridization or horizontal gene transfer [38, 13, 31]. An example is depicted in Figure 1 (right) which has one gene\nflow event drawn as a green arrow. This gene flow event represents the biological scenario in which some genes in\ntaxon 2 get transferred from the lineage of taxon 3, and thus, when reconstructing the evolutionary history of this\ngroup of four taxa, some genes will depict the phylogenetic tree that clusters taxa 1 and 2 in a clade (Figure 1 left)\nand some genes will depict the phylogenetic tree that clusters taxa 2 and 3 in a clade (Figure 1 center). Evolutionary\nbiologists are interested in inferring the correct evolutionary history of the taxon group (species tree) accounting for\nthe different evolutionary histories of individual genes.\n1Corresponding author: solislemus@wisc.edu\n1\narXiv:2312.16074v2  [q-bio.PE]  3 May 2024\nIn the absence of gene tree estimation error, the identification of the main trees that represent the evolution of certain\ngene groups would be trivial.\nHowever, estimation error combined with unaccounted biological processes such as\nincomplete lineage sorting or gene duplications complicate the identification of the main gene evolutionary histories\nin the data. Thus, novel and scalable tools to accurately classify gene trees that share common evolutionary patterns\nare needed.\nFigure 1: Left: Phylogenetic tree in 4 taxa. Internal (gray) nodes rep-\nresent speciation events in which an ancestral species differentiates into\ntwo.\nExternal (blue) nodes, also called leaves, represent living species\n(here denoted 1,2,3,4). Edge weights (in gray) also called branch lengths\ncan represent evolutionary time or expected substitutions per site. Cen-\nter:\nDifferent phylogenetic tree on the same 4 taxa in which taxon 2\nis grouped with taxon 3 rather than with taxon 1. Nodes are no longer\ndrawn as is the most common representation of phylogenetic trees. Right:\nPhylogenetic tree with gene flow event depicted as a green arrow. This\nbiological scenario represents the possibility that some genes have the\nevolutionary history of the phylogenetic tree on the left (with clade\n(1, 2)) and some genes, the evolutionary history of the center tree (with\nclade (2, 3)).\nSurprisingly, unsupervised learning methods have not fully been\nexplored in phylogenetics. The only implementations of unsuper-\nvised learning in phylogenetics aim to cluster DNA sequences [24,\n29] or species [8, 30], not trees.\nThe main challenge to imple-\nment unsupervised learning algorithms for phylogenetic trees is\nthe discreteness of tree space. While there exists a proper distance\nfunction (Robinson-Foulds distance [34]) in the space of phyloge-\nnetic trees that can be used in clustering algorithms [39], the new\ntrend in machine learning is to embed data points into an appro-\npriately selected vector space [41, 40, 15] to preserve meaningful\nsimilarity among vectors. Traditionally, there are multiple natural\nembeddings designed for the space of phylogenetic trees. For ex-\nample, the phylogenetic tree space introduced by Billera, Holmes,\nand Vogtmann (BHV space) [2] is a continuous space for rooted\nn-taxon phylogenetic trees in which each tree is represented as a\nvector of edge weights in an orthant defined by its topology (bi-\nfurcating pattern). While the space is equipped in the geodesic\ndistance, it is not a Euclidean space which can complicate the algorithms for the embedding and computation of\ndistances. As a result, standard clustering algorithms, which rely on Euclidean geometry for efficiently measuring\ndistances and defining cluster centroids, may not directly apply or need suboptimal adaptations to work within this\ncomplex, non-Euclidean framework. Other embeddings such as tropical geometric space [26, 21] or the hyperbolic\nembedding [23, 16] are mathematically sophisticated, yet unnecessarily complicated for standard clustering tasks.\nHere, we define a simpler, yet equally powerful, embedding which we denote split-weight which relies on the edge\nweights of taxon splits to embed an unrooted phylogenetic tree into a fully Euclidean vector space, and we implement\nthree standard unsupervised algorithms (K-means [22], Gaussian mixture model [33], and hierarchical [28]) on the\nembedding space. By testing the embedding on simulated and real data, we prove that it preserves phylogenetic tree\nsimilarity and allows us to cluster samples of gene trees into biologically meaningful groups. Furthermore, we present\nthe first computational tool for the unsupervised learning of phylogenetic trees in the novel open-source Julia package\nPhyloClustering.jl available on GitHub https://github.com/solislemuslab/PhyloClustering.jl which is well-\ndocumented and user friendly for maximum outreach in the evolutionary biology.\n2\nMethods\n2.1\nSplit-Weight Embedding of Phylogenetic Trees\nindex\nbipartition\ny(T)i\n0\n1 | 2 3 4\n2.06\n1\n2 | 1 3 4\n2.06\n2\n3 | 1 2 4\n2.46\n3\n4 | 1 2 3\n6.04\n4\n1 2 | 3 4\n0.39\n5\n1 3 | 2 4\n0\n6\n1 4 | 2 3\n0\nTable 1:\nAll possible bipartitions for 4 taxa\n(B{1,2,3,4}) with the numerical entry (third\ncolumn) in the split-weight vector embedding\nfor the unrooted version of the phylogenetic tree\nT in Figure 1 (left).\nWe define important notation in this section.\nBipartitions. A bipartition (or split) of the whole set of taxa (X) into two groups\nX1 and X2 is represented as X1|X2 such that X1 ∩X2 = ∅and X1 ∪X2 = X. Let\nn = |X| denote the total number of taxa in the data. The number of bipartitions\nis given by 2n−1 −1. For example, for the case of n = 4 taxa (Figure 1), there are\n7 splits (Table 1).\nSplit-Edge Equivalence. Let BX be the set of all bipartitions for taxon set X\n(with cardinality 2n−1 −1 as mentioned). Let T be an unrooted phylogenetic tree\non taxon set X. For every edge e ∈E in T, there exists a bipartition be ∈BX such\nthat the removal of e from the tree T results in the two subsets of taxa defined by be.\nFor example, the edge with weight 0.39 in Figure 1 (left) is an internal edge which,\nif removed from the tree, would split the taxa into two groups X1 = {1, 2} and\nX2 = {3, 4}, and thus, this edge is uniquely mapped to the bipartition be = 12|34.\nThus, there is a mapping function fb : E →BX such that fb(e) is the bipartition\n2\ndefined by e. We note that not every bipartition is defined by an edge in T. For example, the split 13|24 is not defined\nby any edge in the tree in Figure 1 (left), and thus, the range of the mapping function fb(E) ⊂BX.\nSplit Representation for Trees. Let T be an unrooted n-taxon phylogenetic tree with taxon set X. For simplicity,\ninstead of fb(E), we denote by BT the bipartitions defined by T as the splits {fb(e)} defined for every edge e ∈E in T.\nFor example, the unrooted version of the phylogenetic tree in Figure 1 (left) has five edges that define the bipartitions:\nBT = {1|234, 2|134, 12|34, 3|124, 4|123} in post order traversal. Note that for the unrooted version of this tree, the two\nchildren edges of the root are merged into one edge with weight 6.04 = 1.79 + 4.25. In addition, we define a mapping\nfunction fw : BT →(0, ∞) that assigns a numerical value to each bipartition which corresponds to the edge weight of\nthe uniquely mapped edge in the tree. For example, fw(12|34) = 0.39.\nSplit-Weight Embedding.\nLet T be an n-taxon unrooted phylogenetic tree on taxon set X.\nLet BT be the\nbipartitions defined by T, and let BX be the set of all bipartitions of X. We define the split-weight embedding of T\nas a (2n−1 −1)–dimensional numerical vector y(T) ∈[0, ∞)2n−1−1 such that the ith element of y(T) corresponds to\nthe ith bipartition (bi) in BX with entry value given by:\ny(T)i =\n\u001a\nfw(fb(e))\nif f −1\nb\n(bi) = e ∈E\n0\notherwise\n,\nThat is, if there is an edge e ∈E in T such that fb(e) = bi, then the ith entry of y(T) is given by the corresponding\nedge weight fw(fb(e)). For example, for 4 taxa, there are 7 bipartitions; five of them correspond to edges in the tree in\nFigure 1 left (Table 1) and for those, their value in the split-weight embedding vector correspond to the edge weights\nin T. In contrast, the two bipartitions that do not belong to BT (13|24 and 14|23) have a value in the split-weight\nembedding vector of 0, as in [20]. Thus, the split-weight embedding for the unrooted version of the phylogenetic tree\nT in Figure 1 (left) is the numerical vector y(T) = (2.06, 2.06, 2.46, 6.04, 0.39, 0, 0).\n2.2\nClustering Algorithms\nFor a sample of N n-taxon unrooted phylogenetic trees {T1, ..., TN}, we first embed them into the split-weight numerical\nvectors: {y(T1), ..., y(TN)}. Note that all vectors have the same dimension as all the trees have the same taxon set\n(X).\nThe input matrix (M ∈[0, ∞)(2n−1−1)×N) for unsupervised learning algorithms becomes the concatenated\nembedded vectors. Then, we implement three different types of unsupervised learning methods which are modified to\nuse the input matrix and directly output the predicted labels for each tree in the sample. Figure 2 shows a graphical\nrepresentation of our unsupervised learning strategy.\n...\nTrees\nembedding\nclustering\nCluster 1\nCluster 2\nT1\n...\nTn\na\ni\nb\nm\ne+f\nn\n...\n...\n...\nc\n0\n0\nk+l\na\nb\nc\nd\ne\nf\nh\ni\nk\nl\nm\nn\nT1\nTn\nFigure 2:\nGraphical representation of our unsupervised learning algorithm for phylogenetic trees.\nIn this case, we are embedding 4-taxon trees into a Euclidean space\nusing the split-weight embedding to fit unsupervised learning models. Each point in the scatter plot is a tree. The dimension reduction strategy we use in this case for\nvisualization is principal component analysis (PCA).\nK-means. The K-means algorithm [22] is a partitioning method that divides data into K distinct, non-overlapping\nclusters based on their characteristics, with each cluster represented by the mean (centroid) of its members. It starts\nby selecting K initial centroids and iterates through two main steps: assigning data points to the nearest centroids,\nand then updating the position of the centroids to the mean of their assigned points. The algorithm minimizes the\n3\nsquared Euclidean distances within each cluster and ends when the positions of the centroids no longer move. Instead\nof traditional K-means, we use Yinyang K-means [9] which has less runtime and memory usage on large datasets.\nTraditional K-means calculates the distance from all data points to centroids for each iteration. Yinyang K-means\nuses triangular inequalities to construct and maintain upper and lower bounds on the distances of data points from\nthe centroids, with global and local filtering to minimize unneeded calculations.\nThe parameters of this algorithm include the number of clusters (K), the initialization method, and the random seed\nfor initialization. To select the initial centroids efficiently, we use the K-means++ [1] as default initializing algorithm.\nK-means++ initializes centroids by first randomly selecting one data point as a centroid, and then iteratively choosing\nsubsequent centroids from the remaining points with probabilities proportional to the square of their distances from\nthe nearest existing centroid. The resulting centroids are typically spread out across the entire dataset.\nWe also employ the repeating strategy [12] for large datasets to mimic the behavior of a user who runs the algorithm\nmultiple times and selects the most reasonable data in real-world situations. Meanwhile, this strategy avoids the\ninstability of standard K-means clustering algorithm due to the initial centroid selection. This instability can easily\nresult into convergence to a local minimum, even with the help of a better initial point selection algorithm like K-\nmeans++. Therefore, we use a simple strategy of repeating the K-means clustering at least 10 times and retaining\nthe result with the highest accuracy when calculating the accuracy for each large group in our simulations.\nGaussian mixture model (GMM). The GMM algorithm [5] is a model-based probabilistic method that assumes\ndata points are generated from a mixture of several Gaussian distributions with unknown parameters. It uses the\nExpectation Maximization (EM) algorithm to update the parameters iteratively in order to optimize the log-likelihood\nof the data until convergence. The choice of covariance type in the GMM shapes how clusters are formed by allowing\nfor different levels of feature interaction. This choice impacts the model’s accuracy and its ability to generalize to new\ndata. It is similar yet more flexible than K-means by permitting mixed membership of data points among clusters.\nThe parameters of this model include the number of clusters, the initialization method, and the covariance type. We\nuse K-means as the method for selecting initial centrals and choose diagonal covariance as covariance type. We also\nemploy repeating K-means [12] to find the best starting centers.\nHierarchical clustering. The algorithm [28] is a hierarchical method that creates a clustering tree called a den-\ndrogram. Each leaf on the tree is a data point, and branches represent the joining of clusters. We can ‘cut’ the\ntree at different heights to get a different number of clusters. This method does not require the number of clusters\nto be specified in advance and can be either agglomerative (bottom-up) or divisive (top-down). The parameters of\nthis model are the linkage method, and the number of clusters. The linkage method in hierarchical clustering defines\nthe metric used to compute the distance between observed datasets, which directly affects the shape and size of the\nclusters. As default, we use Ward’s method [42] as linkage method for hierarchical clustering.\n2.3\nSimulation Study\nWe describe specific simulation scenarios in more details in the following subsections, but as an overall description:\nwe perform simulation tests of K-means, GMM, and hierarchical clustering with K = 2 for samples of 2N trees, each\ntree with n taxa where we vary N and n as described below. For a given combination of n and N, we select two\nspecies trees to serve as the theoretical centroids of each cluster. We generate 100 datasets to account for performance\nvariability. Tree variability within a cluster is simulated with the PhyloCoalSimulations Julia package [11] which\ntakes a species tree as input and simulates gene trees under the coalescent model [32] which generates tree variability\ndue to random sorting of lineages within populations. In this sense, we expect simulated trees to cluster around the\nchosen species tree. The level of expected variability in the sample of gene trees is governed by the edge weights in\nthe species tree (longer branches resulting in less tree variability) [7]. All trees are embedded into the split-weight\nspace, and the vectors are normalized (i.e., set the mean to 0 and the standard deviation to 1) prior to clustering.\nVisualization of clusters is performed via PCA. To quantify the performance of unsupervised learning models, we use\nthe Hungarian algorithm [27] to find the grouping method with the largest overlap between predicted labels and true\nlabels. In this grouping method, the fact that the predicted label is different from the true label means that the tree\nis not classified into the correct group. Therefore, the accuracy is defined as 2N−t\n2N\nwhere t is the number of trees with\ndifferent predicted and true labels. Higher accuracy means that unsupervised learning models are able to capture\nsimulated trees originating from the same species tree more successfully.\nClustering of trees with different topologies. For n = 4 taxa, there are 15 possible bifurcating patterns (tree\ntopologies). For each of the 15 4-taxon species trees, we simulated samples of N = 50, 100, 500, 1000, 5000 trees with\nPhyloCoalSimulations [11] for two choices of edge weights in the species trees: 1) all edge weights set to 1.0 coalescent\n4\nunit, or 2) each edge weight randomly selected from an uniform distribution (0.5, 2) coalescent units. The rationale\nis that 1.0 coalescent unit generates medium levels of gene tree discordance [7], and thus, we expect the clustering\nalgorithms to perform accurately, whereas shorter branches (e.g. 0.5 coalescent units) produce more tree heterogeneity\nfurther complicating clustering.\nFor the case of more than 4 taxa, we cannot list all of the tree topologies (10,395 total unrooted trees for 8 taxa and\n213,458,046,676,875 total unrooted trees for 16 taxa), so we randomly generate 15 8-taxon trees and 15 16-taxon trees\nusing the simulating algorithm in the R package SiPhyNetwork [17] under a birth-death model. We focus on the case\nof edge weights randomly chosen uniformly in the interval (0.5, 2).\nClustering of trees in NNI-neighborhoods. The Nearest Neighbor Interchange (NNI) move is a type of phylo-\ngenetic tree arrangement that selects an internal branch of a given tree and then swaps adjacent subtrees across that\nbranch. It generates alternative tree topologies that are “nearest neighbours” to the original tree, differing only in the\nlocal arrangement of the subtrees connected by the chosen branch. For a given tree, we can define a NNI-neighborhood\nas all the trees that are one NNI move away from the selected tree (or any number of NNI moves away). In this section,\nwe test whether the clustering algorithms are accurate enough to distinguish trees within the same NNI-neighborhood.\nUsing the simulating algorithm in the R package SiPhyNetwork [17] under a birth-death model, we randomly generate\none 8-taxon species tree and one 16-taxon species tree, and then we perform 1, 2, 3, 4 and 5 NNI moves on each tree\nto produce 10 new trees (5 with 8 taxa and 5 with 16 taxa). The NNI function for phylogenetic trees is implemented\nin PhyloNetworks [36]. The edge weights are randomly selected from an uniform distribution (0.5, 2) coalescent units.\nFor each of the species trees, we simulate samples of N = 50, 100, 500, 1000, 5000 trees with PhyloCoalSimulations\n[11].\nClustering of trees with the same topology, but different edge weights. To test the performance of the\nclustering algorithms to classify trees that have the same topologies, but different edge weights, we simulate trees\nunder the same species tree topology with six different sets of edge weights: 1) all edge weights equal to 1 (denoted\norg in the figures); 2) all edge weights lengths equal to 1.5 (denoted inc in the figures); 3) randomly selected edge\nweights uniformly in (1, 2) (denoted inc r in the figures); 4) all branch lengths equal to 0.5 (denoted dec in the figures);\n5) randomly selected edge weights uniformly in (0, 1) (denoted dec r in the figures), and 6) randomly selected edge\nweights uniformly in (0, 2) (denoted all r in the figures). We only focus on the 4-taxon tree topologies for these tests.\nFor each of the species trees, we simulated samples of N = 50, 100, 500, 1000, 5000 trees with PhyloCoalSimulations\n[11].\nClustering performance when clusters of trees are imbalanced. We also test the clustering algorithms on\nimbalanced datasets with species trees with n = 4, and n = 8 taxa. For a given taxon size (n), we randomly generate\ntwo species tree, and for each species tree, we simulate N trees with PhyloCoalSimulations [11]. In one scenario,\none of the clusters will have N = 100 trees, and the other will have N = 1000 trees (denoted 2N = 1100 in the\nResults). In the other scenario, one of the clusters will have N = 1000 trees, and the other will have N = 6000 trees\n(denoted 2N = 6000 in the Results). Since our naive definition of accuracy cannot work on imbalanced data, we\nemploy the Hubert-Arabie adjusted Rand index (ARI)[37] to quantify the performance of clustering algorithms. The\nARI is a refined version of the Rand index [14] that adjusts for the chance grouping of elements, thus providing a\nmore accurate measure of the similarity between two clusterings by considering both the agreement and disagreement\nbetween clusters. An ARI of 1 indicates that the clustering results are perfectly identical, while an ARI of -1 signifies\nthat the clustering results are entirely dissimilar. A score close to 0 suggests that the clustering outcomes are largely\nrandom.\nComparison of clustering from split-weight embeddings to Robinson-Foulds tree distances. As mentioned\nin the Introduction, clustering could be performed using pairwise tree distances from a inherent distance function on\ntrees. The most widely used tree distance is Robinson-Foulds distance [34]. We compare the performance (in terms of\naccuracy, time and memory) of the clustering task from split-weight embeddings vs using Robinson-Foulds distance on\nthe samples of trees directly. We choose trees with n = 4, 8 taxa, and clusters of size N = 1000. For this comparison,\nwe focus on hierarchical clustering.\n2.4\nReticulate evolution in baobabs\nIn-frame codon alignments of baobab target-enrichment data [19] are used to estimate gene trees under maximum\nlikelihood (ML) [10] with IQTREE v.2.1.3 and default settings [25]. The ML analyses treats alignments as nucleotide\ndata and the best model is determined by ModelFinderPlus [18], which uses the Bayesian Information Criterion for\nmodel selection. The data then consist of 372 estimated trees in 8 species of Adansonia: A. digitata (continental\n5\nFigure 3: Left: Heatmaps of classificacion accuracy on the 15 trees with 4 taxa. Each panel corresponds to the classification accuracy of pairwise comparisons for two clusters\noriginated on two trees (row and column) for one clustering method (columns: K-means, GMM and Hierarchical) and one sample size (rows: N = 50 and N = 5000). Within\neach panel, trees are sorted based on their unrooted topology (first 5 rows correspond to split 12|34; next 5 to the split 13|24, and last 5 to the split 14|23). Off-diagonal\nblocks (trees with different unrooted representation) can be accurately separated as different clusters by GMM with large sample size (N = 5000), and by K-means for any\nsample size. Right: Heatmaps on the number of algorithms that reach 80% classification accuracy (0, 1, 2, or 3 algorithms) by edge weight strategy (columns: edge weights\nequal to 1 or edge weights randomly chosen in (0.5, 2)) and sample size (rows: N = 50, 5000).\nAfrica), A. gregorii (Australia), A. grandidieri (Madagascar), A. suarezensis (Madagascar), A. madagascariensis\n(Madagascar), A. perrieri (Madagascar), A. za (Madagascar), and A. rubrostipa (Madagascar). The data also contain\nan outgroup Scleronema micranthaum, so in total, there are 9 taxa. We remove 4 trees that only had 8 taxa, and 5\noutlier trees with pathologically long edges so that the final dataset contain 363 trees which we embed in the split-\nweight space. We standarize the resulting matrix as in the simulation study, and cluster the vectors using the K-means\nalgorithm (K = 2). After clustering, we use Densitree [3] to identify the consensus tree of each cluster via the root\ncanal method.\n3\nResults\n3.1\nSimulation Study\nFigure 3 (left) shows a heatmap of the prediction accuracy of the different clustering algorithms to assess the perfor-\nmance of clustering of trees with different topologies for n = 4 taxa. Each cell in the heatmap represents a comparison\nbetween the row and column tree (only rows labeled, but the order of the columns is the same). Trees are ordered\ndepending of their unrooted topology. For example, the first 5 rows (and columns) correspond to the 5 different rooted\nversions of the unrooted tree corresponding to the split 12|34; the next 5 rows (and columns) to the split 13|24, and the\nlast 5 rows (and columns) to the split 14|23. The darker the color, the more accurate the classification of the two trees.\nWe can see a diagonal block pattern in the heatmaps which illustrates the difficulty of separating two clusters defined\nby two rooted trees with the same unrooted representation. The heatmaps are arranged by clustering algorithm (three\ncolumns: K-means, GMM and hierarchical) and number of trees in the sample (N = 50 and N = 5000). Similar plot\nfor N = 100, 500, 1000 can be found in the Appendix (Figure 5). We can see that the accuracy of K-means is robust\nto sample size, while the accuracy of GMM is higher for larger number of trees (N = 5000). Hierarchical clustering\nshows the worst performance of the three methods.\nFigure 3 (right) shows a different type of heatmap in which we summarize the performance of all three methods. Each\ncell in the heatmap can have four values: 0 if none of the three methods have a classification accuracy above 80%; 1 if\none of the three methods has a classification accuracy above 80%; 2 if two of the three methods have a classification\naccuracy above 80%, and 3 if all three methods have a classification accuracy above 80%. The two columns correspond\nto the strategy to set edge weights (all edge weights equal to 1.0 in the left column and edge weights randomly selected\nin (0.5, 2) in the right column). Unlike standard phylogenetic methods that tend to perform better when edges are 1.0\ncoalescent unit long, the clustering methods here tested perform better with variable edge weights. Furthermore, we\ncan see that with larger sample sizes (N = 5000), most methods are able to distinguish samples originated from trees\nthat do not have the same unrooted topology (off-diagonal blocks).\n6\nFigures 6 and 7 in the Appendix show the classification of all methods to cluster samples of trees for n = 8 taxa\nand n = 16 taxa respectively. The results for NNI-neighborhoods (Figures 8 and 9), same topology (Figures 10, 11\nand 12) and imbalanced number of trees per cluster (Table 3) are also in the Appendix. In all cases, the split-weight\nembedding is able to accurately cluster samples of trees.\nTable 2 shows the results of the comparison with the Robinson-Foulds hierarchical clustering.\nWhile accuracy is\ncomparable, the split-weight embedding requires less time and less memory.\nn\nRunning time (seconds)\nMemory cost (GiB)\nAccuracy\nSplit-weight\n4\n1.906\n0.850\n0.942\nRobinson-Foulds\n4\n11.193\n5.295\n0.927\nSplit-weight\n8\n5.929\n4.572\n0.996\nRobinson-Foulds\n8\n37.146\n25.557\n0.999\nTable 2: Running time in seconds, memory cost in gibibytes (GiB), and accuracy for hierarchical clustering under two types of distances: distance from the split-weight\nembeddings and Robinson-Foulds distance on trees directly.\n3.2\nReticulate evolution in baobabs\nAfter clustering of the sample of gene trees, the two resulting clusters are not balanced: 341 trees in cluster 1 and 22\ntrees in cluster 2 (Figure 13 in the Appendix). This is expected given the evolutionary history of the baobabs group\n(Figure 4 left). The original publication [19] identified one reticulation event (blue arrow) representing 11.8% gene\nflow. This means that we expect 11.8% of the genes to follow the blue arrow back to the root in their evolutionary\nhistory, and thus, most of the genes (88.2%) will have a tree that puts clade ((A.mad, A.ped), A.zaa) sister to (A.gra,\nA.sua) (Figure 4 center; clade highlighted in blue) whereas a few genes (11.8%) will place ((A.mad, A.ped), A.zaa)\nsister to A.rub (Figure 4 right; clade highlighted in green).\nThe accurate identification of the consensus trees for each cluster, in spite of estimation error and other biological\nprocesses in addition to reticulation, is an important result for phylogenetic inference. Estimating a phylogenetic\nnetwork with 9 taxa (tree plus gene flow event in Figure 4 left) could take up to two days of compute time depending\non the method used [35]. Clustering the input trees, building the consensus trees and reconstructing the network\nfrom the consensus trees cannot take more than a few minutes. Our work shows the promise of machine learning\n(unsupervised learning specifically) to aid in the estimation of phylogenetic trees and networks in a scalable manner.\nFigure 4: Clustering via split-weight embedding recovers the correct evolutionary relationship in baobabs. Estimated gene trees from the baobabs group cluster around two\nconsensus trees (341/363 genes around the center tree and 22/363 genes around the right tree) that represent the two correct possibilities created by the gene flow event\nin the phylogenetic network for the group (left) as estimated by the original study [19]. The placement of the (A.mad, A.per, A.zaa) clade changes from being sister to clade\n(A.gra, A.sua) in the center tree (clade highlighted in blue) to be sister of taxon A.rub in the right tree (clade highlighted in green).\n4\nDiscussion\nHere, we apply the first (to our knowledge) implementation of unsupervised learning in the space of phylogenetic trees\nvia the novel and powerful split-weight embedding. The split-weight embedding is related to BHV space embedding\n[2] in that it uses a vector of edge weights to represent a tree. The main difference is that the split-weight embedding\nis more sparse by the addition of zeroes for splits not present in the tree. This sparsity allows embedding vectors to\nbe in the same Euclidean space (which is not true for BHV space), but it will impose computational challenges for\nlarge trees (more below). Tropical geometric space [26, 21] or the hyperbolic embedding [23, 16] embed nodes in the\n7\ntrees as vectors that preserve phylogenetic similarity (such as pairwise distances among the leaves). While these spaces\ncan enhance phylogenetic hierarchical structure better than Euclidean spaces, there are no simple implementations of\nembedding and distance algorithms for use among the evolutionary biology community, and we show here that the much\nsimpler split-weight embedding preserves enough phylogenetic signal to provide biologically meaningful clustering.\nVia extensive simulations, we show that the split-weight embedding is able to capture meaningful evolutionary rela-\ntionships while keeping the simplicity of a standard Euclidean space. Our implementation is able to cluster trees with\ndifferent topologies, and even trees with the same topology, but different edge weights. As usual in machine learning\napplications, the larger the sample size (number of trees), the more accurately the different clusters were recovered.\nOn average, K-means was the desired choice of algorithm as it showed robust performance across sample size and\nnumber of taxa, yet for large trees (8 or 16 taxa), hierarchical clustering outperforms K-means in terms of running\nefficiency and accuracy. For the case of 4 taxa, GMM outperforms K-means when the sample size increased.\nThe bottleneck of our implementation is the curse of dimensionality. In its current version, we do not perform dimension\nreduction except for visualization purposes. The dimension of the split-weight embedded vector is given by the number\nof bipartitions which is 2n−1 −1 for n taxa. The exponentially increasing nature of vectors may limit the application\nof split-weight embedding and incur significant computational cost. In addition, the embedded vector is highly sparse.\nFor a tree of n taxa, there are 2n −3 edges, and thus, only 2n −3 entries of the embedded vector will be different than\nzero. So, for example, for n = 8 taxa, the embedded vector is 127-dimensional with 13 non-zero entries; for n = 16\ntaxa, the embedded vector is 32767-dimensional with 29 non-zero entries. The field of phylogenetic trees deals with\ndatasets of hundreds or thousands of taxa consistently. Future work will involve the study of dimension reduction\ntechniques, as well as compression, such as autoencoder models in order to improve the scalability and stability of our\nalgorithms.\nFinally, here we utilize Densitree [3] to obtain a representation of the consensus tree per cluster. We can, however,\nexplore similar ideas to those in BHV space regarding the computation Fr´echet sample means and Fr´echet sample\nvariances [4] which could set the foundation of classical statistical theory on the split-weight embedding space, for\nexample, selective inference for the estimated clusters [6].\n5\nData and code availability\nThe baobabs dataset was made publicly available by the original publication [19] and can be accessed through the\nDryad Digital Repository http://doi.org/10.5061/dryad.mf1pp3r. The split-weight embedding and unsupervised\nlearning algorithms are implemented in the open source publicly available Julia package available in the GitHub\nrepository https://github.com/solislemuslab/PhyloClustering.jl. All the simulation and real data scripts for\nour work are available in the public GitHub repository https://github.com/YiboK/PhyloClustering-scripts.\n6\nAcknowledgements\nThis work was supported by the National Science Foundation [DEB-2144367 to CSL]. The authors thank Marianne\nBj¨orner and Reed Nelson for help with setting up tests, and Zhaoxing Wu for providing testing data.\nReferences\n[1]\nDavid Arthur and Sergei Vassilvitskii. “K-means++: The advantages of careful seeding”. In: Proceedings of the\nEighteenth Annual ACM-SIAM Symposium on Discrete Algorithms. SODA ’07. New Orleans, Louisiana: Society\nfor Industrial and Applied Mathematics, 2007, pp. 1027–1035. isbn: 9780898716245.\n[2]\nLouis J Billera, Susan P Holmes, and Karen Vogtmann. “Geometry of the space of phylogenetic trees”. In:\nAdvances in Applied Mathematics 27.4 (2001), pp. 733–767.\n[3]\nRemco R. Bouckaert. “DensiTree: Making sense of sets of phylogenetic trees”. In: Bioinformatics 26.10 (Mar.\n2010), pp. 1372–1373. issn: 1367-4803.\n[4]\nDaniel G Brown and Megan Owen. “Mean and variance of phylogenetic trees”. In: Systematic biology 69.1 (2020),\npp. 139–154.\n8\n[5]\nLars Buitinck et al. “API design for machine learning software: experiences from the scikit-learn project”. In:\narXiv 1309.0238 (2013).\n[6]\nYiqun T. Chen and Daniela M. Witten. “Selective inference for k-means clustering”. In: Journal of Machine\nLearning Research 24.152 (2023), pp. 1–41. url: http://jmlr.org/papers/v24/22-0371.html.\n[7]\nJames H Degnan and Noah A Rosenberg. “Gene tree discordance, phylogenetic inference and the multispecies\ncoalescent”. In: Trends in ecology & evolution 24.6 (2009), pp. 332–340.\n[8]\nShahan Derkarabetian et al. “A demonstration of unsupervised machine learning in species delimitation”. In:\nMolecular phylogenetics and evolution 139 (2019), p. 106562.\n[9]\nYufei Ding et al. “Yinyang K-Means: A drop-in replacement of the classic K-Means with consistent speedup”.\nIn: Proceedings of the 32nd International Conference on Machine Learning, ICML 2015. Lille, France, July 2015,\npp. 579–587.\n[10]\nJoseph Felsenstein. “Evolutionary trees from DNA sequences: a maximum likelihood approach”. In: Journal of\nmolecular evolution 17 (1981), pp. 368–376.\n[11]\nJohn Fogg, Elizabeth S Allman, and C´ecile An´e. “PhyloCoalSimulations: A simulator for network multispecies\ncoalescent models, including a new extension for the inheritance of gene flow”. In: Systematic Biology (2023),\nsyad030.\n[12]\nPasi Fr¨anti and Sami Sieranoja. “How much can k-means be improved by using better initialization and repeats?”\nIn: Pattern Recognition 93 (2019), pp. 95–112. issn: 0031-3203.\n[13]\nMark S. Hibbins and Matthew W. Hahn. “The effects of introgression across thousands of quantitative traits\nrevealed by gene expression in wild tomatoes”. In: PLOS Genetics 17.11 (Nov. 2021), pp. 1–20. doi: 10.1371/\njournal.pgen.1009892. url: https://doi.org/10.1371/journal.pgen.1009892.\n[14]\nLawrence Hubert and Phipps Arabie. “Comparing partitions”. In: Journal of Classification 2.1 (Dec. 1985),\npp. 193–218.\n[15]\nYanrong Ji et al. “DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for\nDNA-language in genome”. In: Bioinformatics 37.15 (2021), pp. 2112–2120.\n[16]\nYueyu Jiang, Puoya Tabaghi, and Siavash Mirarab. “Learning hyperbolic embedding for phylogenetic tree place-\nment and updates”. In: Biology 11.9 (2022), p. 1256.\n[17]\nJoshua A. Justison, Claudia Sol´ıs-Lemus, and Tracy A. Heath. “SiphyNetwork: An R package for simulating\nphylogenetic networks”. In: Methods in Ecology and Evolution 14.7 (May 2023), pp. 1687–1698.\n[18]\nS Kalyaanamoorthy et al. “Fast model selection for accurate phylogenetic estimates., 2017, 14”. In: DOI:\nhttps://doi. org/10.1038/nmeth 4285 (), pp. 587–589.\n[19]\nNisa Karimi et al. “Reticulate Evolution Helps Explain Apparent Homoplasy in Floral Biology and Pollination in\nBaobabs (Adansonia; Bombacoideae; Malvaceae)”. In: Systematic Biology 69.3 (Nov. 2019), pp. 462–478. issn:\n1063-5157.\n[20]\nMary K. Kuhner and Joseph Felsenstein. “A simulation comparison of phylogeny algorithms under equal and\nunequal evolutionary rates”. In: Molecular Biology and Evolution 11.3 (1994), pp. 459–68.\n[21]\nBo Lin, Anthea Monod, and Ruriko Yoshida. “Tropical Geometric Variation of Tree Shapes”. In: Discrete &\nComputational Geometry 68.3 (2022), pp. 817–849.\n[22]\nJames MacQueen. “Some methods for classification and analysis of multivariate observations”. In: Proceedings\nof the fifth Berkeley symposium on mathematical statistics and probability. Vol. 1. Oakland, CA, USA, 1967,\npp. 281–297.\n[23]\nHirotaka Matsumoto, Takahiro Mimori, and Tsukasa Fukunaga. “Novel metric for hyperbolic phylogenetic tree\nembeddings”. In: Biology Methods and Protocols 6.1 (2021), bpab006.\n[24]\nPablo Mill´an Arias et al. “DeLUCS: Deep learning for unsupervised clustering of DNA sequences”. In: Plos one\n17.1 (2022), e0261531.\n[25]\nBui Quang Minh et al. “IQ-TREE 2: new models and efficient methods for phylogenetic inference in the genomic\nera”. In: Molecular biology and evolution 37.5 (2020), pp. 1530–1534.\n[26]\nAnthea Monod et al. “Tropical geometry of phylogenetic tree space: a statistical perspective”. In: arXiv preprint\narXiv:1805.12400 (2018).\n[27]\nJames Munkres. “Algorithms for the assignment and transportation problems”. In: Journal of the Society for\nIndustrial and Applied Mathematics 5.1 (1957), pp. 32–38. issn: 03684245. (Visited on 11/16/2023).\n9\n[28]\nFionn Murtagh and Pedro Contreras. “Algorithms for hierarchical clustering: an overview”. In: WIREs Data\nMining and Knowledge Discovery 2.1 (2012), pp. 86–97.\n[29]\nSamuel Ozminkowski et al. “BioKlustering: a web app for semi-supervised learning of maximally imbalanced\ngenomic data”. In: (Sept. 2022). arXiv: 2209.11730 [q-bio.GN].\n[30]\nR Alexander Pyron. “Unsupervised machine learning for species delimitation, integrative taxonomy, and biodi-\nversity conservation”. In: Molecular Phylogenetics and Evolution 189 (2023), p. 107939.\n[31]\nFernando Racimo et al. “Evidence for archaic adaptive introgression in humans.” eng. In: Nat Rev Genet 16.6\n(June 2015), pp. 359–371. issn: 1471-0064 (Electronic); 1471-0056 (Print); 1471-0056 (Linking). doi: 10.1038/\nnrg3936.\n[32]\nBruce Rannala and Ziheng Yang. “Bayes estimation of species divergence times and ancestral population sizes\nusing DNA sequences from multiple loci”. In: Genetics 164.4 (2003), pp. 1645–1656.\n[33]\nDouglas A Reynolds et al. “Gaussian mixture models.” In: Encyclopedia of biometrics 741.659-663 (2009).\n[34]\nDavid F Robinson and Leslie R Foulds. “Comparison of phylogenetic trees”. In: Mathematical biosciences 53.1-2\n(1981), pp. 131–147.\n[35]\nClaudia Sol´ıs-Lemus and C´ecile An´e. “Inferring Phylogenetic Networks with Maximum Pseudolikelihood under\nIncomplete Lineage Sorting”. en. In: PLoS Genet. 12.3 (Mar. 2016), e1005896.\n[36]\nClaudia Sol´ıs-Lemus, Paul Bastide, and C´ecile An´e. “PhyloNetworks: A package for phylogenetic networks”. In:\nMolecular Biology and Evolution 34.12 (Sept. 2017), pp. 3292–3298. issn: 0737-4038.\n[37]\nDouglas L. Steinley. “Properties of the Hubert-Arabie adjusted Rand index.” In: Psychological methods 9 3\n(2004), pp. 386–96.\n[38]\nAnton Suvorov et al. “Widespread introgression across a phylogeny of 155 Drosophila genomes”. In: Current\nBiology 32.1 (2022), 111–123.e5. issn: 0960-9822. doi: https://doi.org/10.1016/j.cub.2021.10.052. url:\nhttps://www.sciencedirect.com/science/article/pii/S0960982221014962.\n[39]\nNadia Tahiri, Bernard Fichet, and Vladimir Makarenkov. “Building alternative consensus trees and supertrees\nusing k-means and Robinson and Foulds distance”. In: Bioinformatics 38.13 (2022), pp. 3367–3376.\n[40]\nIan Tenney, Dipanjan Das, and Ellie Pavlick. “BERT rediscovers the classical NLP pipeline”. In: arXiv preprint\narXiv:1905.05950 (2019).\n[41]\nAshish Vaswani et al. “Attention is all you need”. In: Advances in neural information processing systems 30\n(2017).\n[42]\nJoe H. Ward. “Hierarchical grouping to optimize an objective function”. In: Journal of the American Statistical\nAssociation 58.301 (1963), pp. 236–244.\n10\nA\nSupplementary Figures\nA.1\nClustering of trees with different topologies\nFigure 5: Left: Heatmaps of classificacion accuracy on the 15 trees with 4 taxa.\nEach panel corresponds to the classification accuracy of pairwise comparisons for two\nclusters originated on two trees (row and column) for one clustering method (columns: K-means, GMM and Hierarchical) and one sample size (rows: N = 100, 500, 1000).\nWithin each panel, trees are sorted based on their unrooted topology (first 5 rows correspond to split 12|34; next 5 to the split 13|24, and last 5 to the split 14|23). Right:\nHeatmaps on the number of algorithms that reach 80% classification accuracy (0, 1, 2, or 3 algorithms) on edge weight strategy (columns: edge weights equal to 1 or edge\nweights randomly chosen in (0.5, 2)) and sample size (rows: N = 100, 500, 1000).\nFigure 6:\nHeatmaps of classificacion accuracy on the 15 randomly generated trees with 8 taxa.\nEach panel corresponds to the classification accuracy of pairwise\ncomparisons for two clusters originated on two trees (row and column) for one clustering method (rows:\nK-means and Hierarchical) and one sample size (columns:\nN = 50, 100, 500, 1000, 5000).\n11\nFigure 7:\nHeatmaps of classificacion accuracy on the 15 randomly generated trees with 16 taxa.\nEach panel corresponds to the classification accuracy of pairwise\ncomparisons for two clusters originated on two trees (row and column) for one clustering method (rows:\nK-means and Hierarchical) and one sample size (columns:\nN = 50, 100, 500, 1000, 5000).\nA.2\nClustering of trees in NNI-neighborhoods\nFigure 8 shows the classification accuracy of trees with 8 and 16 taxa, and their corresponding neighbor trees obtained\nby performing 1, 2, 3, 4 or 5 NNI moves on the original tree. Despite the similarity of the trees under comparison, the\nmethods are able to classify quite accurately clusters of trees originated from two similar trees. This implies that the\nsplit-weight embedding is able to preserve the necessary signal to classify phylogenetic trees even for closely related\nclusters. Figure 9 shows the same figure for N = 100, 500, 1000 trees.\nFigure 8: Heatmaps of classification accuracy on one randomly generated tree with 8 taxa (first two columns) or 16 taxa (second two columns) with 5 other trees obtained\nby 1, 2, 3, 4 or 5 NNI moves on the original tree. Note ”1” in the figure indicates 100% accuracy. Rows correspond to sample size (N = 50 trees on top, and N = 5000 trees\non bottom). Classificacion accuracy is high for both methods even for very similar trees (1 NNI move away, for example) which illustrates the preservation of phylogenetic\nsignal by the split-weight embedding.\n12\nFigure 9: Heatmaps of classification accuracy on one randomly generated tree with 8 taxa (first two columns) or 16 taxa (second two columns) with 5 other trees obtained\nby 1, 2, 3, 4 or 5 NNI moves on the original tree.\nRows correspond to sample size (N = 100, 500, 1000).\nClassificacion accuracy is high for both methods even for very\nsimilar trees (1 NNI move away, for example).\nA.3\nClustering of trees with the same topology, but different edge weights\nWhile we showed that GMM can accurately identify tree clusters defined by different topologies (Figure 3), it appears\nthat this algorithm does not have enough sensitivity to identify clusters originated from the same tree topology (Figure\n10). K-means, on the other hand, is able to identify such clusters as long as the edge weights are sufficiently different\n(dec vs inc, for example). Figures 11 and 12 for other tree topologies with similar conclusion.\n13\nFigure 10: Heatmaps of classification accuracy on 4-taxon tree (((4, 3), 2), 1) by the two methods (columns: K-means and GMM) and sample size (rows: N = 50, N = 5000)\nby different set of edge weights. Clusters defined by the same tree topology can be accurately identified by K-means if edge weights are sufficiently different (inc vs dec, for\nexample), but not by GMM.\nFigure 11:\nHeatmaps of K-means classification accuracy on three 4-taxon trees (columns) by sample size (rows:\nN = 50, 100, 500, 1000, 5000) by different set of edge\nweights. Clusters defined by the same tree topology can be accurately identified if edge weights are sufficiently different (inc vs dec, for example).\n14\nFigure 12: Heatmaps of GMM classification accuracy on three 4-taxon trees (columns) by sample size (rows: N = 50, 100, 500, 1000, 5000) by different set of edge weights.\nClusters defined by the same tree topology cannot be accurately identified compared to using K-means (Figure 11).\n15\nA.4\nClustering of trees with imbalanced number of member\nTable 3 shows the results. We can observe that when n = 4, there algorithms have lower ARI since the two groups in\ntest are closter with each other so they are harder for the algorithms to distinguish. On the other hand, when n = 8,\nthe two groups are more seperated so they are easier to cluster.\nARI for n = 4\n2N\nK-means\nGMM\nhierarchical\n1100\n0.326\n0.570\n0.376\n6000\n0.406\n0.672\n0.399\nARI for n = 8\n2N\nK-means\nGMM\nhierarchical\n1100\n0.967\n0\n0.908\n6000\n0.952\n0\n0.921\nTable 3: Left: The ARIs of clustering algorithms with n = 4. Right: The ARIs of clustering algorithms with n = 8. GMM does not work when n ≥8, so it has ARI = 0 in\nboth cases.\nA.5\nBaobabs data\nFigure 13: PCA plot for the clustering of baobabs phylogenetic trees (center).\nThe two clusters (blue and green) are highly imbalanced (341/363 vs 22/363), but these\npartition agrees with the evolutionary history of the group (Figure 4 in the main text) which involves one gene flow event for 11.2% of the genes [19]. Left and right trees\nrepresent the consensus tree created by Densitree [3] for each cluster.\n16\n",
  "categories": [
    "q-bio.PE",
    "stat.ML"
  ],
  "published": "2023-12-26",
  "updated": "2024-05-03"
}