{
  "id": "http://arxiv.org/abs/2312.16730v1",
  "title": "Foundations of Reinforcement Learning and Interactive Decision Making",
  "authors": [
    "Dylan J. Foster",
    "Alexander Rakhlin"
  ],
  "abstract": "These lecture notes give a statistical perspective on the foundations of\nreinforcement learning and interactive decision making. We present a unifying\nframework for addressing the exploration-exploitation dilemma using frequentist\nand Bayesian approaches, with connections and parallels between supervised\nlearning/estimation and decision making as an overarching theme. Special\nattention is paid to function approximation and flexible model classes such as\nneural networks. Topics covered include multi-armed and contextual bandits,\nstructured bandits, and reinforcement learning with high-dimensional feedback.",
  "text": "Foundations of Reinforcement Learning and Interactive Decision Making\nDylan J. Foster and Alexander Rakhlin\nLast Updated: December 2023\nThese lecture notes are based on a course taught at MIT in Fall 2022 and Fall 2023. This\nis a live draft, and all parts will be updated regularly. Please send us an email if you find a\nmistake, typo, or missing reference.\nContents\n1\nIntroduction\n4\n1.1\nDecision Making\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\n1.2\nA Spectrum of Decision Making Problems . . . . . . . . . . . . . . . . . . .\n4\n1.3\nMinimax Perspective . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n1.4\nStatistical Learning: Brief Refresher\n. . . . . . . . . . . . . . . . . . . . . .\n7\n1.5\nRefresher: Random Variables and Averages . . . . . . . . . . . . . . . . . .\n10\n1.6\nOnline Learning and Prediction . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n1.6.1\nConnection to Statistical Learning . . . . . . . . . . . . . . . . . . .\n14\n1.6.2\nThe Exponential Weights Algorithm . . . . . . . . . . . . . . . . . .\n15\n1.7\nExercises\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\n2\nMulti-Armed Bandits\n21\n2.1\nThe Need for Exploration . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23\n2.2\nThe ε-Greedy Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\n2.3\nThe Upper Confidence Bound (UCB) Algorithm\n. . . . . . . . . . . . . . .\n27\n2.4\nBayesian Bandits and the Posterior Sampling Algorithm⋆\n. . . . . . . . . .\n30\n2.5\nAdversarial Bandits and the Exp3 Algorithm⋆. . . . . . . . . . . . . . . . .\n34\n2.6\nDeferred Proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n35\n2.7\nExercises\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n37\n3\nContextual Bandits\n38\n3.1\nOptimism: Generic Template . . . . . . . . . . . . . . . . . . . . . . . . . .\n40\n3.2\nOptimism for Linear Models: The LinUCB Algorithm . . . . . . . . . . . .\n43\n3.3\nMoving Beyond Linear Classes: Challenges\n. . . . . . . . . . . . . . . . . .\n45\n3.4\nThe ε-Greedy Algorithm for Contextual Bandits\n. . . . . . . . . . . . . . .\n46\n3.5\nInverse Gap Weighting: An Optimal Algorithm for General Model Classes .\n49\n3.5.1\nExtending to Offline Regression . . . . . . . . . . . . . . . . . . . . .\n52\n3.6\nExercises\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n53\n1\narXiv:2312.16730v1  [cs.LG]  27 Dec 2023\n4\nStructured Bandits\n55\n4.1\nBuilding Intuition: Optimism for Structured Bandits . . . . . . . . . . . . .\n58\n4.1.1\nUCB for Structured Bandits . . . . . . . . . . . . . . . . . . . . . . .\n58\n4.1.2\nThe Eluder Dimension . . . . . . . . . . . . . . . . . . . . . . . . . .\n59\n4.1.3\nSuboptimality of Optimism . . . . . . . . . . . . . . . . . . . . . . .\n62\n4.2\nThe Decision-Estimation Coefficient\n. . . . . . . . . . . . . . . . . . . . . .\n64\n4.3\nDecision-Estimation Coefficient: Examples . . . . . . . . . . . . . . . . . . .\n69\n4.3.1\nCheating Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n69\n4.3.2\nLinear Bandits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n70\n4.3.3\nNonparametric Bandits\n. . . . . . . . . . . . . . . . . . . . . . . . .\n73\n4.3.4\nFurther Examples\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n74\n4.4\nRelationship to Optimism and Posterior Sampling\n. . . . . . . . . . . . . .\n75\n4.4.1\nConnection to Optimism . . . . . . . . . . . . . . . . . . . . . . . . .\n75\n4.4.2\nConnection to Posterior Sampling\n. . . . . . . . . . . . . . . . . . .\n77\n4.5\nIncorporating Contexts⋆. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n78\n4.6\nAdditional Properties of the Decision-Estimation Coefficient⋆. . . . . . . .\n79\n4.7\nExercises\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n80\n5\nReinforcement Learning: Basics\n80\n5.1\nFinite-Horizon Episodic MDP Formulation . . . . . . . . . . . . . . . . . . .\n81\n5.2\nPlanning via Dynamic Programming . . . . . . . . . . . . . . . . . . . . . .\n82\n5.3\nFailure of Uniform Exploration . . . . . . . . . . . . . . . . . . . . . . . . .\n84\n5.4\nAnalysis Tools\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n85\n5.5\nOptimism . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n87\n5.6\nThe UCB-VI Algorithm for Tabular MDPs . . . . . . . . . . . . . . . . . . .\n88\n5.6.1\nAnalysis for a Single Episode . . . . . . . . . . . . . . . . . . . . . .\n90\n5.6.2\nRegret Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n92\n6\nGeneral Decision Making\n93\n6.1\nSetting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n93\n6.2\nRefresher: Information-Theoretic Divergences . . . . . . . . . . . . . . . . .\n95\n6.3\nThe Decision-Estimation Coefficient for General Decision Making . . . . . .\n97\n6.3.1\nBasic Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n98\n6.4\nE2D Algorithm for General Decision Making . . . . . . . . . . . . . . . . . .\n100\n6.4.1\nOnline Estimation with Hellinger Distance . . . . . . . . . . . . . . .\n102\n6.5\nDecision-Estimation Coefficient: Lower Bound on Regret . . . . . . . . . . .\n103\n6.5.1\nThe Constrained Decision-Estimation Coefficient . . . . . . . . . . .\n104\n6.5.2\nLower Bound . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n105\n6.5.3\nProof of Proposition 28\n. . . . . . . . . . . . . . . . . . . . . . . . .\n106\n6.5.4\nExamples for the Lower Bound . . . . . . . . . . . . . . . . . . . . .\n110\n6.6\nDecision-Estimation Coefficient and E2D: Application to Tabular RL . . . .\n112\n6.6.1\nProof of Proposition 31\n. . . . . . . . . . . . . . . . . . . . . . . . .\n115\n6.7\nTighter Regret Bounds for the Decision-Estimation Coefficient\n. . . . . . .\n118\n6.7.1\nGuarantees Based on Decision Space Complexity . . . . . . . . . . .\n118\n6.7.2\nGeneral Divergences and Randomized Estimators . . . . . . . . . . .\n119\n6.7.3\nOptimistic Estimation . . . . . . . . . . . . . . . . . . . . . . . . . .\n121\n6.8\nDecision-Estimation Coefficient: Structural Properties⋆. . . . . . . . . . . .\n124\n6.9\nDeferred Proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n125\n2\n6.10 Exercises\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n127\n7\nReinforcement Learning: Function Approximation and Large State Spaces129\n7.1\nIs Realizability Sufficient? . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n129\n7.2\nLinear Function Approximation . . . . . . . . . . . . . . . . . . . . . . . . .\n131\n7.2.1\nThe LSVI-UCB Algorithm . . . . . . . . . . . . . . . . . . . . . . . .\n132\n7.2.2\nProof of Proposition 46\n. . . . . . . . . . . . . . . . . . . . . . . . .\n134\n7.3\nBellman Rank . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n137\n7.3.1\nThe BiLinUCB Algorithm\n. . . . . . . . . . . . . . . . . . . . . . . .\n139\n7.3.2\nProof of Proposition 47\n. . . . . . . . . . . . . . . . . . . . . . . . .\n141\n7.3.3\nBellman Rank: Examples . . . . . . . . . . . . . . . . . . . . . . . .\n145\n7.3.4\nGeneralizations of Bellman Rank . . . . . . . . . . . . . . . . . . . .\n147\n7.3.5\nDecision-Estimation Coefficient for Bellman Rank\n. . . . . . . . . .\n148\nA Technical Tools\n156\nA.1 Probabilistic Inequalities . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n156\nA.1.1\nTail Bounds with Stopping Times\n. . . . . . . . . . . . . . . . . . .\n156\nA.1.2\nTail Bounds for Martingales . . . . . . . . . . . . . . . . . . . . . . .\n156\nA.2 Information Theory\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n158\nA.2.1\nProperties of Hellinger Distance . . . . . . . . . . . . . . . . . . . . .\n158\nA.2.2\nChange-of-Measure Inequalities . . . . . . . . . . . . . . . . . . . . .\n158\nA.3 Minimax Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n159\n3\n1. INTRODUCTION\n1.1 Decision Making\nThis is a course about learning to make decisions in an interactive, data-driven fashion.\nWhen we say interactive decision making, we are thinking of problems such as:\n• Medical treatment: based on a patient’s medical history and vital signs, we need to\ndecide what treatment will lead to the most positive outcome.\n• Controlling a robot: based on sensor signals, we need to decide what signals to send\nto a robot’s actuators in order to navigate to a goal.\nFor both problems, we (the learner/agent) are interacting with an unknown environment.\nIn the robotics example, we do not necessarily a-priori know how the signals we send to\nour robot’s actuators change its configuration, or what the landscape it’s trying to navigate\nlooks like. However, because we are able to actively control the agent, we can learn to\nmodel the environment on the fly as we make decisions and collect data, which will reduce\nuncertainty and allow us to make better decisions in the future. The crux of the interactive\ndecision making problem is to make decisions in a way that balances (i) exploring the\nenvironment to reduce our uncertainty and (ii) maximizing our overall performance (e.g.,\nreaching a goal state as fast as possible).\nFigure 1 depicts an idealized interactive decision making setting, which we will return\nto throughout this course. Here, at each round t, the agent (doctor) observes the medical\nhistory and vital signs of a patient, summarized in a context xt, makes a treatment decision\nπt, and then observes the outcomes of the treatment in the form of a reward rt, and an\nauxiliary observation ot about, say, illness progression. With time, we hope that the doctor\nwill learn a good mapping xt 7→πt from contexts to decisions. How can we develop an\nautomated system that can achieve this goal?\nIt is tempting to cast the problem of finding a good mapping xt 7→πt as a supervised\nlearning problem.\nAfter all, modern deep neural networks are able to achieve excellent\nperformance on many tasks, such as image classification and recognition, and it is not\nout of the question that there exists a good neural network for the medical example as\nwell. The question is: how do we find it? In supervised learning, finding a good predictor\noften amounts to fitting an appropriate model—such as a neural network—to the data. In\nthe above example, however, the available data may be limited to what treatments have\nbeen assigned to patients, potentially missing better options. It is the process of active\ndata collection with a controlled amount of exploration that we would like to study in this\ncourse.\nThe decision making framework in Figure 1 generalizes many interactive decision mak-\ning problems the reader might already be familiar with, including multi-armed bandits,\ncontextual bandits, and reinforcement learning. We will cover the foundations of algorithm\ndesign and analysis for all of these settings from a unified perspective, with an emphasis on\nsample efficiency (i.e., how to learn a good decision making policy using as few rounds of\ninteraction as possible).\n1.2 A Spectrum of Decision Making Problems\nTo design algorithms for general interactive decision making problems such as Figure 1,\nthere are many complementary challenges we must overcome. These challenges correspond\n4\ncontext\ndecision\nreward\nobservation\nxt\n⇡t\nrt\not\nFigure 1: A general decision making problem.\ninteractivity\nstructure in\ncontexts\nstructure in\ndecisions\nfunction approximation\nonline \nlearning\nstatistical \nlearning\nmultiarmed \nbandits\ncontextual \nbandits\nreinforcement\nlearning\ntabular \nRL\ndata\ncomplex\nobservations\nadversarial\nnature\nstructured\nbandits\nFigure 2: Landscape of decision making problems.\nto different assumptions we can place on the underlying environment and decision making\nprotocol, and give rise to what we describe as a spectrum of decision making problems,\nwhich is illustrated in Figure 2. There are three core challenges we will focus on throughout\nthe course, which are given by the axes of Figure 2.\n• Interactivity. Does the learning agent observe data passively, or do the decisions\nthey make actively influence what data we collect? In the setting of Figure 1, the\ndoctor observes the effects of the prescribed treatments, but not the counterfactuals\n(the effects of the treatments not given). Hence, doctor’s decisions influence the data\nthey can collect, which in turn may significantly alter the ability to estimate the effects\nof different treatments. On the other hand, in classical machine learning, a dataset is\ntypically given to the learner upfront, with no control over how it is collected.\n• Function approximation and generalization. In supervised statistical learning\nand estimation, one typically employs function approximation (e.g., models such as\nneural networks, kernels, or forests) to generalize across the space of covariates. For\ndecision making, we can employ function approximation in a similar fashion, either\nto generalize across a space of contexts, or to generalize across the space of decisions.\nIn the setting of Figure 1, the context xt summarizing the medical history and vital\n5\nsigns might be a highly structured object. Likewise, the treatment πt might be a high-\ndimensional vector with interacting components, or a complex multi-stage treatment\nstrategy.\n• Data. Is the data (e.g., rewards or observations) observed by our learning algorithm\nproduced by a fixed data-generating process, or does it evolve arbitrarily, and even\nadversarially in response to our actions? If there is fixed data-generating process,\ndo we wish to directly model it, or should we instead aim to be agnostic? Do we\nobserve only the labels of images, as in supervised learning, or a full trajectory of\nstates/actions/rewards for a policy employed by the robot?\nAs shown in Figure 2, many basic decision making and learning frameworks (contextual\nbandits, structured bandits, statistical learning, online learning) can be thought of as ideal-\nized problems that each capture one or more of the possible challenges, while richer settings\nsuch as reinforcement learning encompass all of them.\nFigure 2 can be viewed as a roadmap for the course. We start with a brief introduction\nto Statistical Learning (Section 1.4) and Online Learning (Section 1.6); the concepts and\nresults stated here will serve as a backbone for the rest of the course. We will then study, in\norder, the problems of Multi-Armed Bandits (Section 2), Contextual Bandits (Section 3),\nStructured Bandits (Section 4), Tabular Reinforcement Learning (Section 5), General Deci-\nsion Making (Section 6), and Reinforcement Learning with General Function Approximation\n(Section 7). Each of these topics will add a layer of complexity, and our aim is to develop a\nunified approach to all the aforementioned problems, both in terms of statistical complexity\n(the number of interactions required to achieve the goal), and in terms of algorithm design.\n1.3 Minimax Perspective\nFor much of the course, we take a minimax point of view. Abstractly, let M be a set of possi-\nble models (or, choices for the environment) that can be encountered by the learner/decision\nmaker. The set M can be thought of as representing the prior knowledge of the learner\nabout the underlying environment. Let Alg denote a learning algorithm, and PerfT (Alg, M)\nbe some notion of performance of algorithm Alg on model M ∈M after T rounds of inter-\naction (or—in passive learning—after observing T datapoints). We would like to develop\nalgorithms that perform well, no matter what the model M ∈M is, in the sense that Alg\napproximately solves the minimax problem\nmin\nAlg\nmax\nM∈M PerfT (Alg, M).\n(1.1)\nUnderstanding the statistical complexity (or, difficulty) of a given problem amounts to\nestablishing matching (or nearly matching) upper bounds ϕT (M) and lower bounds ϕT (M)\non the minimax value in (1.1). While developing such upper and lower bounds for specific\nmodel classes M of interest might be a simple task, the grand aim of this course is to\ndevelop a more fundamental, unified understanding of what makes any model class M easy\nverus hard, and to give sharp results for all (or nearly all) M.\nOn the algorithmic side, we would like to better understand the scope of optimal algo-\nrithms that solve (1.1). While the minimax problem is itself an optimization problem, the\nspace of all algorithms is typically prohibitively large. One of the key insights to be lever-\naged in this course is that for general decision making problems, we can restrict ourselves\nto algorithms that interleave a type of supervised learning called online estimation (this\n6\nwill be described in Sections 1.4 and 1.6), with a principled choice of exploration strategy\nthat balances greedily maximizing performance (exploitation) with information acquisition\n(exploration). As we show, such algorithms achieve or nearly achieve optimality in (1.1) for\na surprisingly wide range of decision making problems.\n1.4 Statistical Learning: Brief Refresher\nWe begin with a short refresher on the statistical learning problem. Statistical learning is a\npurely passive problem in which the learner does not directly interact with the environment,\nbut it captures the challenge of generalization and function approximation in the context\nof Figure 2.\nIn the statistical learning problem, we receive examples (x1, y1), . . . , (xT, yT) ∈X × Y,\ni.i.d.\nfrom a (unknown) distribution M⋆.\nHere xt ∈X are features (sometimes called\ncontexts or covariates), and X is the feature space. yt ∈Y are called outcomes, and Y is the\noutcome space. Given (x1, y1), . . . , (xT, yT), the goal is to produce a model (or, estimator)\nbf : X →Y′ that will do a good job predicting outcomes from features for future examples\n(x, y) drawn from M⋆.1\nTo measure prediction performance, we take as given a loss function ℓ: Y′ × Y →R.\nStandard examples include:\n• Regression, where common losses include the square loss ℓ(a, b) = (a −b)2 when\nY = Y′ = R.\n• Classification, where Y = Y′ = {0, 1} and we consider the indicator (or 0-1) loss\nℓ(a, b) = I {a ̸= b}.\n• Conditional density estimation with the logarithmic loss (log loss). Here Y′ = ∆(Y),\nthe set of distributions on Y, and for p ∈Y′,\nℓlog(p, y) = −log p(y).\n(1.2)\nFor a function f : X →Y′, we measure the prediction performance via the population\n(or, “test”) loss:\nL(f) := E(x,y)∼M⋆[ℓ(f(x), y)].\n(1.3)\nLetting HT := {(xt, yt)}T\nt=1 denote the dataset, a (deterministic) algorithm is a map\nthat takes the dataset as input and returns a function/predictor:\nbf(·; HT) : X →Y′.\n(1.4)\nThe goal in designing algorithms is to ensure that E\n\u0002\nL( bf)\n\u0003\nis minimized, where E[·] denotes\nexpectation with respect to the draw of the dataset HT. Without any assumptions, it is not\npossible to learn a good predictor unless the number of examples T scales with |X| (this is\nsometimes called the no-free-lunch theorem). The basic idea behind statistical learning is\nto work with a restricted class of functions\nF ⊆{f : X →Y}\n1Note that we allow the outcome space Y to be different from the prediction space Y′.\n7\nin order to facilitate generalization. The class F can be thought of as (implicitly) encoding\nprior knowledge about the structure of the data. For example, in computer vision, if the\nfeatures xt correspond to images and the outcomes yt are labels (e.g., “cat” or “dog”), one\nmight expect that choosing F to be a class of convolutional neural networks will work well,\nsince this encodes spatial structure.\nRemark 1 (Conditional density estimation): For the problem of conditional den-\nsity estimation, we shall overload the notation and interchangeably write f(x) and\nf(·|x) for the conditional distribution. In this setting, the learner is required to com-\npute a distribution for each x rather than form a point estimate (see Figure 3). For an\noutcome y, the loss is the negative log of the conditional density for the outcome.\nx\nA\nAG23icdZXJbtswEIaZtHFTd0vaYy9CjQA9BIaVZrFvgZMuAYLsW2EbAUXRMmFqAU1EQSdemt7bN6oL9G3KUnJcUyxgjwmRt9PDmeGkhNR\nEvNW6+/c/KPHC7Uni0/rz56/ePlqafn1RwmDOFzFNKQXTkwxpQE+JwTvFVxD0HYovnfGOfH75DbOYhMEZTyM8KEXkCFBkAvXcXq91G\ng1W+qyqgO7HDRAeR1dLy/86bshSnwcERhHPfsVsQHGWScIrzej+JcQTRGHq4J4YB9HE8yFSkubUiPK41DJn4BdxS3oeKDPpxnPqOIH3I\nR7H+TDpNz3oJH7YHGQmihOMAFQsNE2rx0JLbtlzCMOI0FQOIGBGxWmgEGURcJKc+s0wR6ozLYzAaEXSb1+sr1s7h/uGJtfvx097B3tne4c\nFpve/ioci/Ema7kI0/M4yDPGOek2etpr2xKlK6KY29kVuz+D7xRvwrpjS8uRe0JapMx8yL+dN7ekuChcln2QvM0io/nb2tT17GPmV5IUx\nBtKlCb6HOxtFyMp2jLwmsBVa2E2j4AS708CLPJZWx7tUFGvCkTcOnKIwLzDPnpWDJixg+rtvjbamJ6zKCxmrpCWs12x1hOuvCrLU1/H\nJE+GRXYjPy1oijhEUPyiBaIVCQX4BoW+DwM369+oaXr2IOtzfMsLZeHMGnau0Y7MpAYrn4FlYisaKl0GMmQw8Crzl4D76lO1/AHR8AU\nt6yTpiLV6WpbAJD+NPmqGpiVWZNUNbeEI+stWGFaQ8Yd50aIyoOz3+2YVJpR7SqdEUuTcLpUa1qoqLbNMWkBxUv3vu2/pavDi7WmvZmc/\n14vbHdLb8Ai+AteAfeAxtsgW3wBRyBc4ABj/Bb3BXG9S+137UfhXo/FypeQNmrtrdP7wfXyk=</latexit>y\nf(y|x)\nFigure 3: Conditional density estimation.\nEmpirical risk minimization and excess risk.\nThe most basic and well-studied algo-\nrithmic principle for statistical learning is Empirical Risk Minimization (ERM). Define the\nempirical loss for the dataset HT as\nbL(f) = 1\nT\nT\nX\ni=1\nℓ(f(xi), yi).\n(1.5)\nThen, the empirical risk minimizer with respect to the class F is given by\nbf ∈arg min\nf∈F\nbL(f).\n(1.6)\nTo measure the performance of ERM and other algorithms that attempt to learn with\nF, we consider excess loss (or, regret)\nE(f) = L(f) −min\nf′∈F L(f′).\n(1.7)\nIntuitively, the quantity minf′∈F L(f′) in (1.7) captures the best prediction performance any\nfunction in F can achieve, even with knowledge of the true distribution. If an algorithm bf\nhas low excess risk, this means that we are predicting future outcomes nearly as well as any\nalgorithm based on samples can hope to perform. ERM and other algorithms can ensure\nthat E( bf) is small in expectation or with high probability over draw of the dataset HT.\n8\nConnection to estimation.\nAn appealing feature of the formulation in (1.7) is that it\ndoes not presuppose any relationship between the class F and the data distribution; in\nother words, it is agnostic. However, if F does happen to be good at modeling the data\ndistribution, the excess loss has an additional interpretation based on estimation.\nDefinition 1: For prediction with square loss, we say that the problem is well-specified\n(or, realizable) if the regression function f⋆(a) := E[y|x = a] is in F.\nThe regression function f⋆can also be seen as a minimizer of L(f) over measurable functions\nf, for the same reason that Ez(z −b)2 is minimized at b = E[z].\nLemma 1: For the square loss, if the problem is well-specified, then for all f : X →Y,\nE(f) = Ex\n\u0002\n(f(x) −f⋆(x))2\u0003\n(1.8)\nProof of Lemma 1. Adding and subtracting f⋆in the first term of (1.7), we have\nE(f(x) −y)2 −E(f⋆(x) −y)2 = E(f(x) −f⋆(x))2 + 2 E[(f⋆(x) −y)(f(x) −f⋆(x))].\nInspecting (1.8), we see that any f achieving low excess loss necessarily estimates the true\nregression function f⋆; hence, the goals of prediction and estimation coincide.\nGuarantees for ERM.\nWe give bounds on the excess loss of ERM for perhaps the\nsimplest special case, in which F is finite.\nProposition 1: For any finite class F, empirical risk minimization satisfies\nE\n\u0002\nE( bf)\n\u0003\n≲comp(F, T),\n(1.9)\nwhere\n1. For any bounded loss (including classification), comp(F, T) =\nq\nlog |F|\nT\n.\n2. For square loss regression, if the problem is well-specified, comp(F, T) = log |F|\nT\n.\nIn addition, there exists a (different) algorithm that achieves comp(F, T) = log |F|\nT\nfor\nboth square loss regression and conditional density estimation, even when the problem\nis not well-specified.\nHenceforth, we shall use the symbol ≲to indicate an inequality that holds up to constants,\nor other problem parameters deemed less important for the present discussion.\nAs an\nexample, the range of losses for the first part is hidden in this notation, and we only focus\non the dependence of the right-hand side on F and T.\n9\nThe rate comp(F, T) =\nq\nlog |F|\nT\nabove is sometimes referred to as a slow rate, and is\noptimal for generic losses. The rate comp(F, T) = log |F|\nT\nis referred to as a fast rate, and\ntakes advantage of additional structure (curvature, or strong convexity) of the square loss.\nCritically, both bounds scale only with the cardinality of F, and do not depend on the size\nof the feature space X, which could be infinite. This reflects the fact that working with\na restricted function class is allowing us to generalize across the feature space X. In this\ncontext the cardinality log|F| should be thought of a notion of capacity, or expressiveness\nfor F. Intuitively, choosing a larger, more expressive class will require a larger amount of\ndata, but will make the excess loss bound in (1.7) more meaningful, since the benchmark\nwill be stronger.\nRemark 2 (From finite to infinite classes): Throughout these lecture notes, we\nrestrict our attention to finite classes whenever possible in order to simplify presentation.\nIf one wishes to move beyond finite classes, a well-developed literature within statistical\nlearning provides various notions of complexity for F that lead to bounds on comp(F, T)\nfor ERM and other algorithms. These include the Vapnik-Chervonenkis (VC) dimension\nfor classification, Rademacher complexity, and covering numbers. Standard references\ninclude Bousquet et al. [20], Boucheron et al. [19], Anthony and Bartlett [10], Shalev-\nShwartz and Ben-David [77].\n1.5 Refresher: Random Variables and Averages\nTo prove Proposition 1 and similar generalization bounds, the main tools we will use are\nconcentration inequalities (or, tail bounds) for random variables.\nDefinition 2: A random variable Z is sub-Gaussian with variance factor (or variance\nproxy) σ2 if\n∀η ∈R,\nE eη(Z−E[Z]) ≤eσ2η2/2.\nNote that if Z ∼N(0, σ2) is Gaussian with variance σ2, then it is sub-Gaussian with vari-\nance proxy σ2. In this sense, sub-Gaussian random variables generalize the tail behavior of\nGaussians. A standard application of Chernoff method yields the following result.\nLemma 2: If Z1, . . . , ZT are i.i.d. sub-Gaussian random variables with variance proxy\nσ2, then\nP\n \n1\nT\nT\nX\ni=1\nZi −E[Z] ≥u\n!\n≤exp\n\u001a\n−Tu2\n2σ2\n\u001b\n(1.10)\nApplying this result with Z and −Z and taking a union bound yields the following two-sided\nguarantee:\nP\n \f\f\f\f\f\n1\nT\nT\nX\ni=1\nZi −E[Z]\n\f\f\f\f\f ≥u\n!\n≤2 exp\n\u001a\n−Tu2\n2σ2\n\u001b\n.\n(1.11)\n10\nSetting the right-hand side of (1.11) to δ and solving for u, we find that for any δ ∈(0, 1),\nwith probability at least 1 −δ,\n\f\f\f\f\f\n1\nT\nT\nX\ni=1\nZi −E[Z]\n\f\f\f\f\f ≤\nr\n2σ2 log(2/δ)\nT\n.\n(1.12)\nRemark 3 (Union bound): The factor 2 under the logarithm in (1.12) is the result\nof applying union bound to (1.10). Throughout the course, we will frequently apply the\nunion bound to multiple—say N—high probability events involving sub-Gaussian ran-\ndom variables. In this case, the union bound will result in terms of the form log(N/δ).\nThe mild logarithmic dependence is due to the sub-Gaussian tail behavior of the aver-\nages.\nThe following result shows that any bounded random variable is sub-Gaussian.\nLemma 3 (Hoeffding’s inequality): Any random variable Z taking values in [a, b]\nis sub-Gaussian with variance proxy (b −a)2/4, i.e.\n∀η ∈R,\nln E exp{−η(Z −E[Z])} ≤η2(b −a)2\n8\n.\n(1.13)\nAs a consequence, for i.i.d. random variables Z1, . . . , ZT taking values in [a, b] almost\nsurely, with probability at least 1 −δ,\n1\nT\nT\nX\ni=1\nZi −E[Z] ≤(b −a)\nr\nlog(1/δ)\n2T\n(1.14)\nIn particular, in the setting of Section 1.4,\nUsing Hoeffding’s inequality, we can prove now prove Part 1 (the slow rate) from Propo-\nsition 1.\nLemma 4 (Proposition 1, Part 1): Let F = {f : X →Y} be finite, and assume\nℓ◦f ∈[0, 1] almost surely. Then with probability at least 1 −δ, ERM satisfies\nL( bf) −min\nf∈F L(f) ≤2\nr\nlog(2|F|/δ)\n2T\n.\nProof of Lemma 4. For any f ∈F, we can write\nL( bf) −L(f) =\nh\nL( bf) −bL( bf)\ni\n+\nh\nbL( bf) −bL(f)\ni\n+\nh\nbL(f) −L(f)\ni\n.\nObserve that for all f : X →Y, we have\n\f\f\fL(f) −bL(f)\n\f\f\f =\n\f\f\f\f\fE ℓ(f(X), Y ) −1\nT\nT\nX\ni=1\nℓ(f(Xi), Yi)\n\f\f\f\f\f .\n11\nBy union bound and Lemma 3, with probability at least 1 −|F|δ,\n∀f ∈F,\n\f\f\f\f\fE ℓ(f(X), Y ) −1\nT\nT\nX\ni=1\nℓ(f(Xi), Yi)\n\f\f\f\f\f ≤\nr\nlog(2/δ)\n2T\n(1.15)\nTo deduce the in-expectation bound of Proposition 1 from the high-probability tail bound\nof Lemma 4, a standard technique of “integrating out the tail” is employed. More precisely,\nfor a nonnegative random variable U, it holds that E[U] ≤τ +\nR ∞\nτ\nP (U ≥z) dz for all τ > 0;\nchoosing τ ∝T −1/2 concludes the proof.\nTo prove the Part 2 (the fast rate) from Proposition 1, we need a more refined con-\ncentration inequality (Bernstein’s inequality), which gives tighter guarantees for random\nvariables with small variance.\nLemma 5 (Bernstein’s inequality): Let Z1, . . . , ZT , Z be i.i.d. with variance V(Zi) =\nσ2, and range |Z −E Z| ≤B almost surely. Then with probability at least 1 −δ,\n1\nT\nT\nX\ni=1\nZi −E Z ≤σ\nr\n2 log(1/δ)\nT\n+ B log(1/δ)\n3T\n.\n(1.16)\nThe proof for Part 2 is given as an exercise in Section 1.7. We refer the reader to Ap-\npendix A.1 for further background on tail bounds.\n1.6 Online Learning and Prediction\nWe now move on to the problem of online learning, or sequential prediction. The online\nlearning problem generalizes statistical learning on two fronts:\n• Rather than receiving a batch dataset of T examples all at once, we receive the\nexamples (xt, yt) one by one, and must predict yt from xt only using the examples we\nhave already observed.\n• Instead of assuming that examples are drawn from a fixed distribution, we allow\nexamples to be generated in an arbitrary, potentially adversarial fashion.\nOnline Learning Protocol\nfor t = 1, . . . , T do\nCompute predictor bf t : X →Y\nObserve (xt, yt) ∈X × Y\nIn more detail, at each timestep t, given the examples\nHt−1 = {(x1, y1), . . . , (xt−1, yt−1)}\n(1.17)\nobserved so far, the algorithm produces a predictor\nbf t = bf t(· | Ht−1),\n12\nwhich aims to predict the outcome yt from the features xt.\nThe algorithm’s goal is to\nminimize the cumulative loss over T rounds, given by\nT\nX\nt=1\nℓ( bf t(xt), yt)\nfor a known loss function ℓ: Y′ × Y →R; the cumulative loss can be thought of as a sum\nof “out-of-sample” prediction errors. Since we will not be placing assumptions on the data-\ngenerating process, it is not possible to make meaningful statements about the cumulative\nloss itself. However, we can aim to ensure that this cumulative loss is not much worse than\nthe best empirical explanation of the data by functions in a given class F. That is, we\nmeasure the algorithm’s performance via regret to F:\nReg =\nT\nX\nt=1\nℓ( bf t(xt), yt) −min\nf∈F\nT\nX\nt=1\nℓ(f(xt), yt).\n(1.18)\nOur aim is to design prediction algorithms that keep regret small for any sequence of\ndata. As in statistical learning, the class F should be thought of as capturing our prior\nknowledge about the problem, and might be a linear model or neural network. At first\nglance, keeping the regret small for arbitrary sequences might seem like an impossible task,\nas it stands in stark contrast with statistical learning, where data is generated i.i.d. from\na fixed distribution. Nonetheless, we will that algorithms with guarantees similar to those\nfor statistical learning are available.\nLet us remark that it is often useful to apply online learning methods in settings where\ndata is not fully adversarial, but evolves according to processes too difficult to directly\nmodel. For example, in the chapters that follow, we will apply online methods as a sub-\nroutine with more sophisticated algorithms for decision making. Here, the choice of past\ndecisions, while in our purview, does not look like i.i.d. or simple time-series data.\nRemark 4 (Proper learning, improper learning, and randomization): The\nonline learning protocol does not require that bf t lies in F ( bf t ∈F). A method that\nchooses functions from F will be called proper, and the one that selects predictors\noutside of F will be called improper. It will also be useful to allow for randomized\npredictions of the form\nbf t ∼qt(·|Ht−1),\nwhere qt is a distribution on functions, typically on elements of F. For randomized\npredictions, we slightly abuse notation and write regret as\nReg =\nT\nX\ni=1\nE bft∼qt\n\u0002\nℓ( bf t(xt), yt)\n\u0003\n−min\nf∈F\nT\nX\ni=1\nℓ(f(xt), yt).\n(1.19)\nThe algorithms we introduce in the sequel below ensure small regret even if data are ad-\nversarially and adaptively chosen. More precisely, for deterministic algorithms, (xt, yt)\nmay be chosen based on bf t and all the past data, while for randomized algorithms,\nNature can only base this choice on qt.\n13\nIn the context of Figure 2, online learning generalizes statistical learning by considering\narbitrary sequences of data, but still allows for general-purpose function approximation and\ngeneralization via the class F. While the setting involves making predictions in an online\nfashion, we do not think of this as an interactive decision making problem, because the\npredictions made by the learning agent do not directly influence what data the agent gets\nto observe.\n1.6.1\nConnection to Statistical Learning\nOnline learning can be thought of as a generalization of statistical learning, and in fact,\nalgorithms for online learning immediately yield algorithms for statistical learning via a\ntechnique called online-to-batch conversion. This result, which is formalized by the following\nproposition, rests on two observations: the cumulative loss of the algorithm looks like a\nsum of out-of-sample errors, and the minimum empirical fit to realized data (over F) is,\non average, a harder (that is, smaller) benchmark than the minimum expected loss in\nF.\nProposition 2: Suppose the examples (x1, y1), . . . , (xT, yT) are drawn i.i.d. from a dis-\ntribution M⋆, and suppose the loss function a 7→ℓ(a, b) is convex in the first argument\nfor all b. Then for any online learning algorithm, if we define\nbf(x) = 1\nT\nT\nX\nt=1\nbf t(x),\nwe have\nE\n\u0002\nE( bf)\n\u0003\n≤1\nT · E[Reg].\nProof of Proposition 2. Let (x, y) ∼M⋆be a fresh sample which is independent of the\nhistory HT. First, by Jensen’s inequality,\nE\nh\nL( bf)\ni\n= E\n\"\nE(x,y) ℓ\n \n1\nT\nT\nX\nt=1\nbf t(x), y\n!#\n≤E\n\"\n1\nT\nT\nX\nt=1\nE(x,y) ℓ\n\u0010\nbf t(x), y\n\u0011#\n(1.20)\nwhich is equal to\nE\n\"\n1\nT\nT\nX\nt=1\nE(xt,yt) ℓ\n\u0010\nbf t(xt), yt\u0011#\n(1.21)\nsince bf t is a function of Ht−1 and (x, y) and (xt, yt) are i.i.d. Second,\nmin\nf∈F L(f) = min\nf∈F E\n\"\n1\nT\nT\nX\nt=1\nℓ(f(xt), yt)\n#\n≥E\n\"\nmin\nf∈F\n1\nT\nT\nX\nt=1\nℓ(f(xt), yt)\n#\n(1.22)\nIn light of Proposition 2, one can interpret regret as generalizing the notion of excess risk\nfrom i.i.d. data to arbitrary sequences.\n14\nSimilar to Lemma 1 in the setting of statistical learning, the regret for online learning\nhas an additional interpretation in terms of estimation if the outcomes for the problem are\nwell-specified.\nLemma 6: Suppose that the features x1, . . . , xT are generated in an arbitrary fashion,\nbut that for all t, the variable yt is random with mean given by a fixed function f⋆∈F:\nE[yt | xt = x] = f⋆(x).\nThen for the problem of prediction with square loss,\nE[Reg] ≥E\n\" T\nX\nt=1\n( bf t(xt) −f⋆(xt))2\n#\n.\nNotably, this result holds even if the features x1, . . . , xT are generated adversarially, with no\nprior knowledge of the sequence. This is a significant departure from classical estimation\nresults in statistics, where estimation of an unknown function is typically done over a fixed,\nknown sequence (“design”) x1, . . . , xT, or with respect to an i.i.d. dataset.\n1.6.2\nThe Exponential Weights Algorithm\nThe main online learning algorithm is the Exponential Weights algorithm, which is appli-\ncable to finite classes F. At each time t, the algorithm computes a distribution qt ∈∆(F)\nvia\nqt(f) ∝exp\n(\n−η\nt−1\nX\ni=1\nℓ(f(xi), yi)\n)\n,\n(1.23)\nwhere η > 0 is a learning rate. Based on qt, the algorithm forms the prediction bf t. We give\ntwo variants of the method here.\nExponential Weights (averaged)\nfor t = 1, . . . , T do\nCompute qt in (1.23).\nLet bf t = Ef∼qt[f].\nObserve (xt, yt), incur ℓ( bf t(xt), yt).\nExponential Weights (randomized)\nfor t = 1, . . . , T do\nCompute qt in (1.23).\nSample bf t ∼qt.\nObserve (xt, yt), incur ℓ( bf t(xt), yt).\nThe only difference between these variants lies in whether we compute the prediction bf t\nfrom qt via\nbf t = Ef∼qt[f],\nor\nbf t ∼qt.\n(1.24)\nThe latter can be applied to any bounded loss functions, while the former leads to faster\nrates for specific losses such as the square loss and log loss, but is only applicable when Y′ is\nconvex. Note that the averaged version is inherently improper, while the second is proper,\nyet randomized. From the point of view of regret, the key difference between these two\nversions is the placement of “Ef∼qt”: For the averaged version it is inside the loss function,\nand for the randomized version it is outside (see (1.19)). The averaged version can therefore\ntake advantage of the structure of the loss function, such as strong convexity, leading to\n15\nfaster rates. The following result shows that Exponential Weights leads to regret bounds\nfor online learning, with rates that parallel those in Proposition 1.\nProposition 3: For any finite class F, the Exponential Weights algorithm (with ap-\npropriate choice of η) satisfies\n1\nT Reg ≲comp(F, T)\n(1.25)\nfor any sequence, where:\n1. For arbitrary bounded losses (including classification), comp(F, T) =\nq\nlog |F|\nT\n.\nThis is achieved by the randomized variant.\n2. For regression with the square loss and conditional density estimation with the\nlog loss, comp(F, T) = log |F|\nT\n. This is achieved by the averaged variant.\nWe now turn to the proof of Proposition 3. Since we are not placing any assumptions\non the data generating process, we cannot hope to control the algorithm’s loss at any\nparticular time t, but only cumulatively. It is then natural to employ amortized analysis\nwith a potential function.\nIn more detail, the proof of Proposition 3 relies on several steps, common to standard\nanalyses of online learning: (i) define a potential function, (ii) relate the increase in potential\nat each time step, to the loss of the algorithm, (iii) relate cumulative loss of any expert\nf ∈F to the final potential. For the Exponential Weights Algorithm, the proof relies on\nthe following potential for time t, parameterized by η > 0:\nΦt\nη = −log\nX\nf∈F\nexp\n(\n−η\nt\nX\ni=1\nℓ(f(xi), yi)\n)\n.\n(1.26)\nThe choice of this potential is rather opaque, and a full explanation of its origin is beyond\nthe scope of the course, but we mention in passing that there are principled ways of coming\nup with potentials in general online learning problems.\nProof of Proposition 3. We first prove the second statement, focusing on conditional density\nwith the logarithmic loss; for the square loss, see Remark 6 below.\nProof for Part 2: Log loss.\nRecall that for each x, f(x) is a distribution over Y, and\nℓlog(f(x), y) = −log f(y|x) where we abuse the notation and write f(x) and f(·|x) inter-\nchangeably. With η = 1, the averaged variant of exponential weights satisfies\nbf t(yt|xt) =\nX\nf∈F\nqt(f)f(yt|xt) =\nX\nf∈F\nf(yt|xt)\nexp\nn\n−Pt−1\ni=1 ℓlog(f(xi), yi)\no\nP\nf∈F exp\nn\n−Pt−1\ni=1 ℓlog(f(xi), yi)\no,\n(1.27)\nand thus\nℓlog( bf(xt), yt) = −log bf t(yt|xt) = Φt\n1 −Φt−1\n1\n.\n(1.28)\n16\nHence, by telescoping\nT\nX\nt=1\nℓlog( bf(xt), yt) = ΦT\n1 −Φ0\n1.\nFinally, observe that Φ0\n1 = −log |F| and, since −log is monotonically decreasing, we have\nΦT\n1 ≤−log exp\n(\n−\nT\nX\ni=1\nℓlog(f⋆(xi), yi)\n)\n=\nT\nX\ni=1\nℓlog(f⋆(xi), yi),\n(1.29)\nfor any f⋆∈F. This establishes the result for conditional density estimation with the log\nloss. As already discussed, the above proof follows the strategy: the loss on each round\nrelated to change in potential (1.28), and the cumulative loss of any expert is related to the\nfinal potential (1.29). We now aim to replicate these steps for arbitrary bounded losses.\nProof for Part 1: Generic loss. To prove this result, we build on the log loss result above.\nFirst, observe that without loss of generality, we may assume that ℓ◦f ∈[0, 1] for all f ∈F\nand (x, y), as we can always re-scale the problem. The randomized variant of exponential\nweights (1.24) satisfies\nE bft∼qt[ℓ( bf t(xt), yt)] =\nX\nf∈F\nℓ(f(xt), yt)\nexp\nn\n−η Pt−1\ni=1 ℓ(f(xi), yi)\no\nP\nf∈F exp\nn\n−η Pt−1\ni=1 ℓ(f(xi), yi)\no.\n(1.30)\nHoeffding’s inequality (1.13) implies that\nη E bft∼qt[ℓ( bf t(xt), yt)] ≤−log\nX\nf∈F\nexp{−ηℓ(f(xt), yt)} exp\nn\n−η Pt−1\ni=1 ℓ(f(xi), yi)\no\nP\nf∈F exp\nn\n−η Pt−1\ni=1 ℓ(f(xi), yi)\no\n+ η2\n8 .\n(1.31)\nNote that the right-hand side of this inequality is simply\nΦt\nη −Φt−1\nη\n+ η2\n8 ,\nestablishing the analogue of (1.28). Summing over t, this gives\nη\nT\nX\nt=1\nE bft∼qt[ℓ( bf t(xt), yt)] ≤ΦT\nη −Φ0\nη + Tη2\n8 .\n(1.32)\nAs in the first part, for any f⋆∈F, we can upper bound\nΦT\nη ≤η\nT\nX\nt=1\nℓ(f⋆(xt), yt),\nwhile Φ0\nη = −log |F|. Hence, we have that for any f⋆∈F,\nT\nX\nt=1\nE bft∼qt[ℓ( bf t(xt), yt)] −ℓ(f⋆(xt), yt) ≤Tη\n8 + log|F|\nη\n.\n17\nWith η =\nq\n8 log |F|\nT\n, we conclude that\nT\nX\nt=1\nE bft∼qt[ℓ( bf t(xt), yt)] −ℓ(f⋆(xt), yt) ≤\nr\nT log |F|\n2\n.\n(1.33)\nObserve that Hoeffding’s inequality was all that was needed for Lemma 4. Curiously\nenough, it was also the only nontrivial step in the proof of Proposition 3.\nIn fact, the\nconnection between probabilistic inequalities and online learning regret inequalities (that\nhold for arbitrary sequences) runs much deeper.\nRemark 5 (Beyond finite classes): As in statistical learning, there are (sequential)\ncomplexity measures for F that can be used to generalize the regret bounds in Propo-\nsition 3 to infinite classes. In general, the optimal regret for a class F will reflect the\nstatistical capacity of the class [69].\nRemark 6 (Mixable losses): We did not provide a proof of Proposition 3 for square\nloss. It is tempting to reduce square loss regression to density estimation by taking the\nconditional density to be a Gaussian distribution. Indeed, the log loss of a distribution\nwith density proportional to exp{−( bf t(xt)−yt)2} is, up to constants, the desired square\nloss. However, the mixture in (1.27) does not immediately lead to a prediction strategy\nfor the square loss, as the expectation appears in the wrong location. This issue is fixed\nby a notion known as mixability.\nWe say that a loss ℓis mixable with parameter η if there exists a constant c > 0\nsuch that the following holds: for any x and a distribution q ∈∆(F), there exists a\nprediction bf(x) ∈Y′ such that for all y ∈Y,\nℓ( bf(x), y) ≤−c\nη log\n\nX\nf∈F\nq(f) exp{−ηℓ(f(x), y)}\n\n.\n(1.34)\nIf loss is mixable, then given the exponential weights distribution qt, the best prediction\nbyt = bf t(xt) can be written (by bringing the right-hand side of (1.34) to the left side) as\nan optimization problem\narg min\nbyt∈Y′\nmax\nyt∈Y\n\nℓ(byt, yt) + c\nη log\n\nX\nf∈F\nqt(f) exp{−ηℓ(f(xt), yt)}\n\n\n\n\n(1.35)\nwhich is equivalent to\narg min\nbyt∈Y′\nmax\nyt∈Y\n\nℓ(byt, yt) + c\nη log\n\nX\nf∈F\nexp{−η\nt\nX\ni=1\nℓ(f(xi), yi)}\n\n\n\n\n(1.36)\n18\nonce we remove the normalization factor. With this choice, mixability allows one to\nreplicate the proof of Proposition 3 for the logarithmic loss, with the only difference\nbeing that (1.27) (after applying −log to both sides) becomes an inequality. It can be\nverified that square loss is mixable with parameter η = 2 and c = 1 when Y = Y′ = [0, 1],\nleading to the desired fast rate for square loss in Proposition 3. The idea of translating\nthe English statement “there exists a strategy such that for any outcome...” into a\nmin-max inequality will come up again in the course.\nRemark 7 (Online linear optimization): For the slow rate in Proposition 3, the\nnature of the loss and the dependence on the function f is immaterial for the proof. The\nguarantee can be stated in a more abstract form that depends only on the vector of losses\nfor functions in F as follows. Let |F| = N. For timestep t, define ℓt\nf = ℓ(f(xt), yt) and\nℓt = (ℓt\nf1, . . . , ℓt\nfN ) ∈RN for F = {f1, . . . , fN}. For a randomized strategy qt ∈∆([N]),\nexpected loss of the learner can be written as\nE bft∼qt[ℓ( bf t(xt), yt)] = ⟨qt, ℓt⟩,\nand the expected regret can be written as\nReg =\nT\nX\nt=1\n⟨qt, ℓt⟩−\nmin\nj∈{1,...,N}\nT\nX\nt=1\n⟨ej, ℓt⟩\n(1.37)\nwhere ej ∈RN is the standard basis vector with 1 in jth position. In its most general\nform, the exponential weights algorithm gives bounds on the regret in (1.37) for any\nsequence of vectors ℓ1, . . . , ℓT, and the update takes the form\nqt(k) ∝exp\n(\n−η\nt−1\nX\ni=1\nℓt(k)\n)\n.\nThis formulation can be viewed as a special case of a problem known as online linear\noptimization, and the exponential weights method can be viewed as an instance of an\nalgorithm known as mirror descent.\n1.7 Exercises\nExercise 1 (Proposition 1, Part 2.): Consider the setting of Proposition 1, where (x1, y1), . . . , (xT, yT)\nare i.i.d., F = {f : X →[0, 1]} is finite, the true regression function satisfies f ⋆∈F, and\nYi ∈[0, 1] almost surely. Prove that empirical risk minimizer bf with respect to square loss\nsatisfies the following bound on excess risk. With probability at least 1 −δ,\nE( bf) ≲log(|F|/δ)\nT\n.\n(1.38)\nFollow these steps:\n19\n1. For a fixed function f ∈F, consider the random variable\nZi(f) = (f(x\ni) −y\ni)2 −(f ⋆(x\ni) −y\ni)2\nfor i = 1, . . . , T. Show that\nE[Zi(f)] = E(f(x\ni) −f ⋆(x\ni))2 = E(f).\n2. Show that for any fixed f ∈F, the variance V(Zi(f)) is bounded as\nV(Zi(f)) ≤4 E(f(x\ni) −f ⋆(x\ni))2.\n3. Apply Bernstein’s inequality (Lemma 5) to show that with for any f ∈F, with probability\nat least 1 −δ,\nE(f) ≤2(bL(f) −bL(f ⋆)) + C log(1/δ)\nT\n,\n(1.39)\nfor an absolute constant C, where bL(f) = 1\nT\nPT\nt=1(f(xt) −yt)2.\n4. Extend this probabilistic inequality to simultaneously hold for all f ∈F by taking the\nunion bound over f ∈F. Conclude as a consequence that the bound holds for bf, the empirical\nminimizer, implying (1.38).\nExercise 2 (ERM in Online Learning): Consider the problem of Online Supervised Learn-\ning with indicator loss ℓ(f(x), y) = I {f(x) ̸= y}, Y = Y′ = {0, 1}, and a finite class F.\n1. Exhibit a class F for which ERM cannot ensure sublinear growth of regret for all sequences,\ni.e. there exists a sequence (x1, y1), . . . , (xT, yT) such that\nT\nX\nt=1\nℓ( bf\nt(x\nt), y\nt) −min\nf∈F\nT\nX\nt=1\nℓ(f(x\nt), y\nt) = Ω(T),\nwhere bf t is the empirical minimizer for the indicator loss on (x1, y1), . . . , (xt−1, yt−1). Note:\nThe construction must have |F| ≤C, where C is an absolute constant that does not depend\non T.\n2. Show that if data are i.i.d., then in expectation over the data, ERM attains a sublinear\nbound O(\np\nT log|F|) on regret for any finite class F.\nExercise 3 (Low Noise): 1. For a nonnegative random variable X, prove that for any η ≥0,\nln E exp{−η(X −E[X])} ≤η2\n2 E[X2].\n(1.40)\nHint: use the fact that ln x ≤x −1 and exp(−x) ≤1 −x + x2/2 for x ≥0.\n2. Consider the setting of Proposition 3, Part 1 (Generic Loss). Prove that the randomized\nvariant of the Exponential Weights Algorithm satisfies, for any f ⋆∈F,\nT\nX\nt=1\nE b\nf t∼qt[ℓ( bf\nt(x\nt), y\nt)] −ℓ(f ⋆(x\nt), y\nt) ≤η\n2\nT\nX\nt=1\nE b\nf t∼qt[ℓ( bf\nt(x\nt), y\nt)2] + log|F|\nη\n.\n(1.41)\n20\nfor any sequence of data and nonnegative losses. Hint: replace Hoeffding’s Lemma by (1.40).\n3. Suppose ℓ(f(x), y) ∈[0, 1] for all x ∈X, y ∈Y, and f ∈F.\nSuppose that there is a\n“perfect expert f ⋆∈F such that ℓ(f ⋆(xt), yt) = 0 for all t ∈[T]. Conclude that the above\nalgorithm, with an appropriate choice of η, enjoys a bound of O(log|F|) on the cumulative loss\nof the algorithm (equivalently, the fast rate log|F|\nT\nfor the average regret). This setting is called\n“zero-noise.”\n4. Consider the binary classification problem with indicator loss, and suppose F contains a\nperfect expert, as above. The Halving Algorithm maintains a version space F t = {f ∈F :\nf(xs) = ys, s < t} and, given xt, follows the majority vote of remaining experts in F t. Show\nthat this algorithm incurs cumulative loss at most O(log|F|). Hence, the Exponential Weights\nAlgorithm can be viewed as an extension of the Halving algorithm to settings where the optimal\nloss is non-zero.\n2. MULTI-ARMED BANDITS\nThis chapter introduces the multi-armed bandit problem, which is the simplest interactive\ndecision making framework we will consider in this course.\nMulti-Armed Bandit Protocol\nfor t = 1, . . . , T do\nSelect decision πt ∈Π := {1, . . . , A}.\nObserve reward rt ∈R.\nThe protocol (see above) proceeds in T rounds. At each round t ∈[T], the learning agent\nselects a discrete decision2 πt ∈Π = {1, . . . , A} using the data\nHt−1 = {(π1, r1), . . . , (πt−1, rt−1)}\ncollected so far; we refer to Π as the decision space or action space, with A ∈N denoting\nthe size of the space. We allow the learner to randomize the decision at step t according\nto a distribution pt = pt(· | Ht−1), sampling πt ∼pt. Based on the decision πt, the learner\nreceives a reward rt, and their goal is to maximize the cumulative reward across all T\nrounds. As an example, one might consider an application in which the learner is a doctor\n(or personalized medical assistant) who aims to select a treatment (the decision) in order\nto make a patient feel better (maximize reward); see Figure 4.\nThe multi-armed bandit problem can be studied in a stochastic framework, in which re-\nwards are generated from a fixed (conditional) distribution, or an non-stochastic/adversarial\nframework in the vein of online learning (Section 1.6). We will focus on the stochastic frame-\nwork, and make the following assumption.\nAssumption 1 (Stochastic Rewards): Rewards are generated independently via\nrt ∼M⋆(· | πt),\n(2.1)\nwhere M⋆(· | ·) is the underlying model (conditional distribution).\n2In the literature on bandits, decisions are often referred to as actions. We will use these terms inter-\nchangeably throughout this section.\n21\nWe define\nf⋆(π) := E [r | π]\n(2.2)\nas the mean reward function under r ∼M⋆(· | π). We measure the learner’s performance\nvia regret to the action π⋆:= arg maxπ∈Π f⋆(π) with highest reward:\nReg :=\nT\nX\nt=1\nf⋆(π⋆) −\nT\nX\nt=1\nEπt∼pt\n\u0002\nf⋆(πt)\n\u0003\n.\n(2.3)\nRegret is a natural notion of performance for the multi-armed bandit problem because\nit is cumulative: it measures not just how well the learner can identify an action with\ngood reward, but how well it can maximize reward as it goes. This notion is well-suited\nto settings like the personalized medicine example in Figure 4, where regret captures the\noverall quality of treatments, not just the quality of the final treatment. As in the online\nlearning framework, we would like to develop algorithms that enjoy sublinear regret, i.e.\nE[Reg]\nT\n→0\nas\nT →∞.\nThe most important feature of the multi-armed bandit problem, and what makes the\nproblem fundamentally interactive, is that the learner only receives a reward signal for the\nsingle decision πt ∈Π they select at each round. That is, the observed reward rt gives a\nnoisy estimate for f⋆(πt), but reveals no information about the rewards for other decisions\nπ ̸= πt. For example in Figure 4, if the doctor prescribes a particular treatment to the\ncontext\ndecision\nreward\nobservation\nxt\n⇡t\nrt\not\nFigure 4: An illustration of the multi-armed bandit problem. A doctor (the learner) aims\nto select a treatment (the decision) to improve a patient’s vital signs (the reward).\npatient, they can observe whether the patient responds favorably, but they do not directly\nobserve whether other possible treatments might have led to an even better outcome. This\nissue is often referred to as partial feedback or bandit feedback. Partial feedback introduces\nan element of active data collection, as it means that the information contained in the\ndataset Ht depends on the decisions made by the learner, which we will see necessitates\nexploring different actions. This should be contrasted with statistical learning (where the\ndataset is generated independently from the learner) and online learning (where losses may\nbe chosen by nature in response to the learner’s behavior, but where the outcome yt— and\nhence the full loss function ℓ(·, yt)—is always revealed).\n22\nIn the context of Figure 2, the multi-armed bandit problem constitutes our first step\nalong the “interactivity” axis, but does not incorporate any structure in the decision space\n(and does not involve features/contexts/covariates). In particular, information about one\naction does not reveal information about any other actions, so there is no hope of using\nfunction approximation to generalize across actions.3 As a result, the algorithms we will\ncover in this section will have regret that scales with Ω(|Π|) = Ω(A). This shortcoming is\naddressed by the structured bandit framework we will introduce in Section 4, which allows\nfor the use of function approximation to model structure in the decision space.4\nRemark 8 (Other notions of regret): It is also reasonable to consider empirical\nregret, defined as\nmax\nπ∈Π\nT\nX\nt=1\nrt(π) −\nT\nX\nt=1\nrt(πt),\n(2.4)\nwhere, for π ̸= πt, rt(π) denotes the counterfactual reward the learner would have\nreceived if they had played π at round t. Using Hoeffding’s inequality, one can show\nthat this is equivalent to the definition in (2.3) up to O(\n√\nT) factors.\n2.1 The Need for Exploration\nIn statistical learning, we saw that the empirical risk minimization algorithm, which greedily\nchooses the function that best fits the data, leads to interesting bounds on excess risk. For\nmulti-armed bandits, since we assume the data generating process is stochastic, a natural\nfirst attempt at designing an algorithm is to apply the greedy principle here in the same\nfashion. Concretely, at time t, we can compute an empirical estimate for the reward function\nf⋆via\nbf t(π) =\n1\nnt(π)\nX\ns<t\nrsI {πs = π} ,\n(2.5)\nwhere nt(π) is the number of times π has been selected up to time t.5 Then, we can choose\nthe greedy action\nbπt = arg max\nπ∈Π\nbf t(π).\nUnfortunately, due to the interactive nature of the bandit problem, this strategy can fail,\nleading to linear regret (Reg = Ω(T)). Consider the following problem with Π = {1, 2}\n(A = 2).\n• Decision 1 has reward 1\n2 almost surely.\n• Decision 2 has reward Ber(3/4).\n3Another way to say this is that we take F = RA, so that f ⋆∈F.\n4Throughout the lecture notes, we will exclusively use the term “multi-armed bandit” to refer to bandit\nproblems with finite action spaces, and use the term “structured bandit” for problems with large action\nspaces.\n5If nt(π) = 0, we will set bf t(π) = 0.\n23\nSuppose we initialize by playing each decision a single time to ensure that nt(π) > 0, then\nfollow the greedy strategy. One can see that with probability 1/4, the greedy algorithm will\nget stuck on action 1, leading to regret Ω(T).\nThe issue in this example is that the greedy algorithm immediately gives up on the\noptimal action and never revisits it.\nTo address this, we will consider algorithms that\ndeliberately explore less visited actions to ensure that their estimated rewards are not\nmisleading.\n2.2 The ε-Greedy Algorithm\nThe greedy algorithm for bandits can fail because it can insufficiently explore good decisions\nthat initially seem bad, leading it to get stuck playing suboptimal decisions. In light of this\nfailure, a reasonable solution is to manually force the algorithm to explore, so as to ensure\nthat this situation never occurs. This leads us to what is known as the ε-Greedy algorithm\n(e.g., Sutton and Barto [81], Auer et al. [13]).\nLet ε ∈[0, 1] be the exploration parameter. At each time t ∈[T], the ε-Greedy algorithm\ncomputes the estimated reward function bf t as in (2.5). With probability 1−ε, the algorithm\nchooses the greedy decision\nbπt = arg max\nπ\nbf t(π),\n(2.6)\nand with probability ε it samples a uniform random action πt ∼unif({1, . . . , A}). As the\nname suggests, ε-Greedy usually plays the greedy action (exploiting what it has already\nlearned), but the uniform sampling ensures that the algorithm will also explore unseen\nactions. We can think of the parameter ε as modulating the tradeoff between exploiting\nand exploring.\nProposition 4: Assume that f⋆(π) ∈[0, 1] and rt is 1-sub-Gaussian. Then for any T,\nby choosing ε appropriately, the ε-Greedy algorithm ensures that with probability at\nleast 1 −δ,\nE[Reg] ≲A1/3T 2/3 · log1/3(AT/δ).\nThis regret bound has E[Reg]\nT\n→0 with T →∞as desired, though we will see in the sequel\nthat more sophisticated strategies can attain improved regret bounds that scale with\n√\nAT.6\nProof of Proposition 4. Recall that bπt := arg maxπ bf t(π) denotes the greedy action at round\nt, and that pt denotes the distribution over πt. We can decompose the regret into two terms,\nrepresenting the contribution from choosing the greedy action and the contribution from\n6Note that\n√\nAT ≤A1/3T 2/3 whenever A ≤T, and when A ≥T both guarantees are vacuous.\n24\nexploring uniformly:\nReg =\nT\nX\nt=1\nEπt∼pt[f⋆(π⋆) −f⋆(πt)]\n= (1 −ε)\nT\nX\nt=1\nf⋆(π⋆) −f⋆(bπt) + ε\nT\nX\nt=1\nEπt∼unif([A])[f⋆(π⋆) −f⋆(bπt)]\n≤\nT\nX\nt=1\nf⋆(π⋆) −f⋆(bπt) + εT.\nIn the last inequality, we have simply written off the contribution from exploring uniformly\nby using that f⋆(π) ∈[0, 1]. It remains to bound the regret we incur from playing the\ngreedy action. Here, we bound the per-step regret in terms of estimation error using a\nsimilar decomposition to Lemma 4 (note that we are now working with rewards rather than\nlosses):\nf⋆(π⋆) −f⋆(bπt) = [f⋆(π⋆) −bf t(π⋆)] + [ bf t(π⋆) −bf t(bπt)]\n|\n{z\n}\n≤0\n+[ bf t(bπt) −f⋆(bπt)]\n(2.7)\n≤2\nmax\nπ∈{π⋆,bπt} |f⋆(π) −bf t(π)| ≤2 max\nπ\n|f⋆(π) −bf t(π)|.\n(2.8)\nNote that this regret decomposition can also be applied to the pure greedy algorithm, which\nwe have already shown can fail. The reason why ε-Greedy succeeds, which we use in the\nargument that follows, is that because we explore, the “effective” number of times that each\narm will be pulled prior to round t is of the order εt/A, which will ensure that the sample\nmean converges to f⋆. In particular, we will show that the event\nEt =\n(\nmax\nπ\n|f⋆(π) −bf t(π)| ≲\nr\nA log(AT/δ)\nεt\n)\n(2.9)\noccurs for all t with probability at least 1 −δ.\nTo prove that (2.9) holds, we first use Hoeffding’s inequality for adaptive stopping times\n(Lemma 33), which gives that for any fixed π, with probability at least 1 −δ over the draw\nof rewards,\n|f⋆(π) −bf t(π)| ≤\ns\n2 log(2T/δ)\nnt(π)\n.\n(2.10)\nFrom here, taking a union bound over all t ∈[T] and π ∈Π ensures that\n|f⋆(π) −bf t(π)| ≤\ns\n2 log(2AT 2/δ)\nnt(π)\n(2.11)\nfor all π and t simultaneously. It remains to show that the number of pulls nt(π) is suffi-\nciently large.\nLet et ∈{0, 1} be a random variable whose value indicates whether the algorithm\nexplored uniformly at step t, and let mt(π) = |{i < t : πi = π, ei = 1}|, which has\nnt(π) ≥mt(π). Let Zt = I {πt = π, et = 1}. Observe that we can write\nmt(π) =\nX\ni<t\nZi.\n25\nIn addition, Zt ∼Ber(ε/A), so we have E[mt(π)] = ε(t−1)/A. Using Bernstein’s inequality\n(Lemma 5) with Z1, . . . , Zt−1, we have that for any fixed π and all u > 0, with probability\nat least 1 −2e−u,\n\f\f\f\fmt(π) −ε(t −1)\nA\n\f\f\f\f ≤\np\n2V[Z](t −1)u + u\n3 ≤\nr\n2ε(t −1)u\nA\n+ u\n3 ≤ε(t −1)\n2A\n+ 4u\n3 ,\nwhere we have used that V[Z] = ε/A · (1 −ε/A) ≤ε/A, and then applied the arithmetic\nmean-geometric mean (AM-GM) inequality, which states that √xy ≤x\n2 + y\n2 for x, y ≥0.\nRearranging, this gives\nmt(π) ≥ε(t −1)\n2A\n−4u\n3 .\n(2.12)\nSetting u = log(2AT/δ) and taking a union bound, we are guaranteed that with probability\nat least 1 −δ, for all π ∈Π and t ∈[T]\nmt(π) ≥ε(t −1)\n2A\n−4 log(2AT/δ)\n3\n.\n(2.13)\nAs long as εt ≳A log(AT/δ) (we can write off the rounds where this does not hold), this\nyields\nnt(π) ≥mt(π) ≳εt\nA.\nTaking a union bound and combining with (2.11), this implies that with probability at least\n1 −δ, for all t,\nmax\nπ\n|f⋆(π) −bf t(π)| ≲\nr\nA log(AT/δ)\nεt\n.\nwhich leads to the overall regret bound\nReg ≤\nT\nX\nt=1\nmax\nπ\n|f⋆(π) −bf t(π)| + εT ≲\nT\nX\nt=1\nr\nA log(AT/δ)\nεt\n+ εT\n≤\nr\nAT log(AT/δ)\nε\n+ εT.\n(2.14)\nTo balance the terms on the right-hand side, we set\nε ∝\n\u0012A log(AT/δ)\nT\n\u00131/3\n,\nwhich gives the final result.\nThis proof shows that the ε-Greedy strategy allows the learner to acquire information\nuniformly for all actions, but we pay for this in terms of regret (specifically, through the\nεT factor in the final regret bound (2.14)). This issue here is that the ε-Greedy strategy\ncontinually explores all actions, even though we might expect to rule out actions with very\nlow reward after a relatively small amount of exploration. To address this shortcoming, we\nwill consider more adaptive strategies.\n26\nRemark 9 (Explore-then-commit): A relative of ε-Greedy is the explore-then-\ncommit (ETC) algorithm (e.g., Robbins [73], Langford and Zhang [57]), which uni-\nformly explores actions for the first N rounds, then estimates rewards based on the\ndata collected and commits to the greedy action for the remaining T −N rounds.\nThis strategy can be shown to attain Reg ≲A1/3T 2/3 for an appropriate choice of N,\nmatching ε-Greedy.\n2.3 The Upper Confidence Bound (UCB) Algorithm\nThe next algorithm we will study for bandits is the Upper Confidence Bound (UCB) algo-\nrithm [56, 6, 13]. The UCB algorithm attains a regret bound of the order eO(\n√\nAT), which\nimproves upon the regret bound for ε-Greedy, and is optimal (in a worst-case sense) up\nto logarithmic factors. In addition to optimality, the algorithm offers several secondary\nbenefits, including adaptivity to favorable structure in the underlying reward function.\nThe UCB algorithm is based on the notion of optimism in the face of uncertainty,\nwhich is a general principle we will revisit throughout this text in increasingly rich settings.\nThe idea behind the principle is that at each time t, we should adopt the most optimistic\nperspective of the world possible given the data collected so far, and then choose the decision\nπt based on this perspective.\nTo apply the idea of optimism to the multi-armed bandit problem, suppose that for each\nstep t, we can construct “confidence intervals”\nf t, ¯f t : Π →R,\n(2.15)\nwith the following property: with probability at least 1 −δ,\n∀t ∈[T], π ∈Π,\nf∗(π) ∈[f t(π), ¯f t(π)].\n(2.16)\nWe refer to f t as a lower confidence bound and ¯f t as a upper confidence bound, since we are\n⇧\nˆf t\nf ⇤\n⇡t\n⇡⇤\nf t\n¯f t\nFigure 5: Illustration of the UCB algorithm. Selecting the action πt optimistically ensures\nthat the suboptimality never greater exceeds the confidence width.\nguaranteed that with high probability, they lower (resp. upper) bound f⋆. Given confidence\nintervals, the UCB algorithm simply chooses πt as the “optimistic” action that maximizes\nthe upper confidence bound:\nπt = arg max\nπ∈Π\n¯f t(π).\nThe following lemma shows that the instantaneous regret for this strategy is bounded by\nthe width of the confidence interval; see Figure 5 for an illustration.\n27\nLemma 7: Fix t, and suppose that f⋆(π) ∈[f t(π), ¯f t(π)] for all π. Then the optimistic\naction\nπt = arg max\nπ∈Π\n¯f t(π)\nhas\nf⋆(π⋆) −f⋆(πt) ≤¯f t(πt) −f⋆(πt) ≤¯f t(πt) −f t(πt).\n(2.17)\nProof of Lemma 7. The result follows immediate from the observation that for any t ∈[T]\nand any π⋆∈Π, we have\nf⋆(π⋆) ≤¯f t(π⋆) ≤¯f t(πt)\nand\n−f⋆(πt) ≤−f t(πt).\nLemma 7 implies that as long as we can build confidence intervals for which the width\n¯f t(πt) −f t(πt) shrinks, the regret for the UCB strategy will be small. To construct such\nintervals, here we appeal to Hoeffding’s inequality for adaptive stopping times (Lemma 33).7\nAs long as rt ∈[0, 1], a union bound gives that with probability at least 1−δ, for all t ∈[T]\nand π ∈Π,\n| bf t(π) −f⋆(π)| ≤\ns\n2 log(2T 2A/δ)\nnt(π)\n,\n(2.18)\nwhere we recall that bf t is the sample mean and nt(π) := P\ni<t I {πi = π}. This suggests\nthat by choosing\n¯f t(π) = bf t(π) +\ns\n2 log(2T 2A/δ)\nnt(π)\n,\nand\nf t(π) = bf t(π) −\ns\n2 log(2T 2A/δ)\nnt(π)\n,\n(2.19)\nwe obtain a valid confidence interval. With this choice—along with Lemma 7—we are in a\nfavorable position, because for a given round t, one of two things must happen:\n• The optimistic action has high reward, so the instantaneous regret is small.\n• The instantaneous regret is large, which by Lemma 7 implies that confidence width\nis large as well (and nt(πt) is small). This can only happen a small number of times,\nsince nt(πt) will increase as a result, causing the width to shrink.\nUsing this idea, we can prove the following regret bound.\nProposition 5: Using the confidence bounds in (2.19), the UCB algorithm ensures\nthat with probability at least 1 −δ,\nReg ≲\np\nAT log(AT/δ).\n7While asymptotic confidence intervals in classical statistics arise from limit theorems, we are interested\nin valid non-asymptotic intervals, and thus appeal to concentration inequalities.\n28\nThis result is optimal up to the log(AT) factor, which can be removed by using the same\nalgorithm with a slightly more sophisticated confidence interval construction [11]. Note\nthat compared to the statistical learning and online learning setting, where we were able to\nattain regret bounds that scaled logarithmically with the size of the benchmark class, here\nthe optimal regret scales linearly with |Π| = A. This is the price we pay for partial/bandit\nfeedback, and reflects that fact that we must explore all actions to learn.\nProof of Proposition 5. Let us condition on the event in (2.18). Whenever this occurs, we\nhave that f⋆(π) ∈\n\u0002\nf t(π), ¯f t(π)\n\u0003\nfor all t ∈[T] and π ∈Π, so the confidence intervals are\nvalid. As a result, Lemma 7 bounds regret in terms of the confidence width:\nT\nX\nt=1\nf⋆(π⋆) −f⋆(πt) ≤\nT\nX\nt=1\n¯f t(πt) −f t(πt) =\nT\nX\nt=1\n2\ns\n2 log(2T 2A/δ)\nnt(πt)\n∧1;\n(2.20)\nhere, the “∧1” term appears because we can write off the regret for early rounds where\nnt(πt) = 0 as 1.\nTo bound the right-hand side, we use a potential argument. The basic idea is that at\nevery round, nt(π) must increase for some action π, and since there are only A actions, this\nmeans that 1/\np\nnt(πt) can only be large for a small number of rounds. This can be thought\nof as a quantitative instance of the pigeonhole principle.\nLemma 8 (Confidence width potential lemma): We have\nT\nX\nt=1\n1\np\nnt(πt)\n∧1 ≲\n√\nAT.\nProof of Lemma 8. We begin by writing.\nT\nX\nt=1\n1\np\nnt(πt)\n∧1 =\nX\nπ\nT\nX\nt=1\nI {πt = π}\np\nnt(π)\n∧1 =\nX\nπ\nnT +1(π)\nX\nt=1\n1\n√t −1 ∧1.\n(2.21)\nFor any n ∈N, we have Pn\nt=1\n1\n√t−1 ∧1 ≤1 + 2√n, which allows us to bound by\nA + 2\nX\nπ\np\nnT(π).\nThe factor of A above is a lower-order term (recall that we have A ≤\n√\nAT whenever A ≤T,\nand if A > T the regret bound we are proving is vacuous). To bound the second term, using\nJensen’s inequality, we have\nX\nπ\np\nnT(π) ≤A\nsX\nπ\nnT(π)\nA\n= A\np\nT/A =\n√\nAT.\nThe main regret bound now follows from Lemma 8 and (2.20).\n29\nTo summarize, the key steps in the proof of Proposition 5 were to:\n1. Use the optimistic property and validity of the confidence bounds to bound regret by\nthe sum of confidence widths.\n2. Use a potential argument to show that the sum of confidence widths is small.\nWe will revisit and generalize both ideas in subsequent chapters for more sophisticated\nsettings, including contextual bandits, structured bandits, and reinforcement learning.\nRemark 10 (Instance-dependent regret for UCB): The eO(\n√\nAT) regret bound\nattained by UCB holds uniformly for all models, and is (nearly) minimax-optimal, in\nthe sense that for any algorithm, there exists a model M⋆for which the regret must\nscale as Ω(\n√\nAT). Minimax optimality is a useful notion of performance, but may be\noverly pessimistic. As an alternative, it is possible to show that the UCB attains what\nis known as an instance-dependent regret bound, which adapts to the underlying reward\nfunction, and can be smaller for “nice” problem instances.\nLet ∆(π) := f⋆(π⋆) −f⋆(π) be the suboptimality gap for decision π. Then, when\nf⋆(π) ∈[0, 1], UCB can be shown to achieve\nReg ≲\nX\nπ:∆(π)>0\nlog(AT/δ)\n∆(π)\n.\nIf we keep the underlying model fixed and take T →∞, this regret bound scales only\nlogarithmically in T, which improves upon the\n√\nT-scaling of the minimax regret bound.\n2.4 Bayesian Bandits and the Posterior Sampling Algorithm⋆\nUp to this point, we have been designing and analyzing algorithms from a frequentist view-\npoint, in which we aim to minimize regret for a worst-case choice of the underlying model\nM⋆. An alterative is to adopt a Bayesian viewpoint, and assume that the underlying model\nis drawn from a known prior µ ∈∆(M).8 In this case, rather than worst-case performance,\nwe will be concerned with average regret under the prior, defined via\nRegBayes(µ) := EM⋆∼µ EM⋆[Reg],\nwhere EM⋆[·] denotes the algorithm’s expected regret when M⋆is the underlying reward\ndistribution.\nWorking in the Bayesian setting opens up additional avenues for designing algorithms,\nbecause we can take advantage of our knowledge of the prior to compute quantities of interest\nthat are not available in the frequentist setting, such as posterior distribution over π⋆after\nobserving the dataset Ht−1.\nThe most basic and well-known strategy here is posterior\nsampling (also known as Thompson sampling or probability matching) [82, 75].\n8It is important that µ is known, otherwise this is no different from the frequentist setting.\n30\nPosterior Sampling\nfor t = 1, . . . , T do\nSet pt(π) = P(π⋆= π | Ht−1), where Ht−1 = (π1, r1), . . . , (πt−1, rt−1).\nSample πt ∼pt and observe rt.\nThe basic idea is as follows. At each time t, we can use our knowledge of the prior to\ncompute the distribution P(π⋆= · | Ht−1), which represents the posterior distribution over\nπ⋆given all of the data we have collected from rounds 1, . . . , t −1. The posterior sampling\nalgorithm simply samples the learner’s action πt from this distribution, thereby “matching”\nthe posterior distribution of π⋆.\nProposition 6: For any prior µ, the posterior sampling algorithm ensures that\nRegBayes(µ) ≤\np\nAT log(A).\n(2.22)\nIn what follows, we prove a simplified version of Proposition 6; the full proof is given in\nSection 2.6.\nProof of Proposition 6 (simplified version). We will make the following simplified assump-\ntions:\n• We restrict to reward distributions where M⋆(· | π) = N(f⋆(π), 1). That is, f⋆is the\nonly part of the reward distribution that is unknown.\n• f⋆belongs to a known class F, and rather than proving the regret bound in Proposi-\ntion 6, we will prove a bound of the form\nRegBayes(µ) ≲\np\nAT log|F|,\nwhich replaces the log A factor in the proposition with log|F|.\nSince the mean reward function f⋆is the only part of the reward distribution M⋆that is\nunknown, we can simplify by considering an equivalent formulation where the prior has the\nform µ ∈∆(F). That is, we have a prior over f⋆rather than M⋆.\nBefore proceeding, let us introduce some notation. The process through which we sample\nf⋆∼µ and the run the bandit algorithm induces a joint law over (f⋆, HT), which we call\nP. Throughout the proof, we use E[·] to denote the expectation under this law. We also\ndefine Et[·] = E[· | Ht] and Pt[·] = P[· | Ht].\nWe begin by using the law of total expectation to express the expected regret as\nRegBayes(µ) = E\n\" T\nX\nt=1\nEt−1[f⋆(πf⋆) −f⋆(πt)]\n#\n.\nAbove, we have written π⋆= πf⋆to make explicit the fact that this is a random variable\nwhose value is a function of f⋆.\nWe first simplify the expected regret for each step t. Let µt(f) := P(f⋆= f | Ht−1) be the\nposterior distribution at timestep t. The learner’s decision πt is conditionally independent\nof f⋆given Ht−1, so we can write\nEt−1[f⋆(πf⋆) −f⋆(πt)] = Ef⋆∼µt,πt∼pt[f⋆(πf⋆) −f⋆(πt)].\n31\nIf we define ¯f t(π) = Ef⋆∼µt[f⋆(π)] as the expected reward function under the posterior, we\ncan further write this as\nEf⋆∼µt,πt∼pt\n\u0002\nf⋆(πf⋆) −¯f t(πt)\n\u0003\n.\nBy the design of the posterior sampling algorithm, πt ∼pt is identical in distribution to πf⋆\nunder f⋆∼µt, so this is equal to\nEf⋆∼µt\n\u0002\nf⋆(πf⋆) −¯f t(πf⋆)\n\u0003\n.\nThis quantity captures—on average—how far a given realization of f⋆deviates from the\nposterior mean ¯f t, for a specific decision πf⋆which is coupled to f⋆. The expression above\nmight appear to be unrelated to the learner’s decision distribution, but the next lemma\nshows that it is possible to relate this quantity back to the learner’s decision distribution\nusing a notion of information gain (or, estimation error).\nLemma 9 (Decoupling): For any function ¯f : Π →R, it holds that\nEf⋆∼µt\n\u0002\nf⋆(πf⋆) −¯f(πf⋆)\n\u0003\n≤\nq\nA · Ef⋆∼µt Eπt∼pt\n\u0002\n(f⋆(πt) −¯f(πt))2\u0003\n.\n(2.23)\nProof of Lemma 9. We will show a more general result. Namely, for any ν ∈∆(F) and\n¯f : Π →R, if we define p(π) = Pf∼ν(πf = π), then\nEf∼ν\n\u0002\nf(πf) −¯f(πf)\n\u0003\n≤\nq\nA · Ef∼ν Eπ∼p\n\u0002\n(f(π) −¯f(π))2\u0003\n.\n(2.24)\nThis can be thought of as a “decoupling” lemma.\nOn the left-hand side, the random\nvariables f and πf are coupled, but on the right-hand side, π is drawn from the marginal\ndistribution over πf, independent of the draw of f itself.\nTo prove the result, we use Cauchy-Schwarz as follows:\nEf∼ν\n\u0002\nf(πf) −¯f(πf)\n\u0003\n= Ef∼ν\n\"\np1/2(πf)\np1/2(πf)\n\u0000f(πf) −¯f(πf)\n\u0001\n#\n≤\n\u0012\nEf∼ν\n\u0014\n1\np(πf)\n\u0015\u00131/2\n·\n\u0010\nEf∼ν\nh\np(πf)\n\u0000f(πf) −¯f(πf)\n\u00012i\u00111/2\n.\nFor the first term, we have\nEf∼ν\n\u0014\n1\np(πf)\n\u0015\n=\nX\nf\nν(f)\np(πf) =\nX\nπ\nX\nf:πf=π\nν(f)\np(π) =\nX\nπ\np(π)\np(π) = A.\nFor the second term, we have\nEf∼ν\nh\np(πf)\n\u0000f(πf) −¯f(πf)\n\u00012i\n≤Ef∼ν\n\"X\nπ\np(π)\n\u0000f(π) −¯f(π)\n\u00012\n#\n= Ef∼ν Eπ∼p\n\u0002\n(f(π) −¯f(π))2\u0003\n.\nPutting these bounds together yields (2.24).\n32\nUsing Lemma 9, we have that\nE[Reg] ≤E\n\" T\nX\nt=1\nq\nA · Ef⋆∼µt Eπt∼pt\n\u0002\n(f⋆(πt) −¯f t(πt))2\u0003\n#\n≤\nv\nu\nu\ntAT · E\n\" T\nX\nt=1\nEf⋆∼µt Eπt∼pt\n\u0002\n(f⋆(πt) −¯f t(πt))2\u0003\n#\n.\nTo finish up we will show that PT\nt=1 Ef⋆∼µt Eπt∼pt\n\u0002\n(f⋆(πt) −¯f t(πt))2\u0003\n≤log|F|. To do this,\nwe need some additional information-theoretic tools.\n• For a random variable X with distribution P, Ent(X) ≡Ent(P) := P\nx p(x) log(1/p(x)).\n• For random variables X and Y , Ent(X | Y = y) := Ent(PX|Y =y) and Ent(X | Y ) :=\nEy∼pY [Ent(X | Y = y)].\n• For distributions P and Q, DKL(P ∥Q) = P\nx p(x) log(p(x)/q(x)).\nTo keep notation as clear as possible going forward, let us use boldface script (πt, π⋆, f ⋆,\nHt) to refer to the abstract random variables under consideration, and use non-boldface\nscript (πt, π⋆, f⋆, Ht) to refer to their realizations. Our aim will be to use the conditional\nentropy Ent(f ⋆| Ht) as a potential function, and show that for each t,\n1\n2 E\n\u0002\nEf⋆∼µt Eπt∼pt\n\u0002\n(f⋆(πt) −¯f t(πt))2\u0003\u0003\n= Ent(f ⋆| Ht−1) −Ent(f ⋆| Ht).\n(2.25)\nFrom here the result will follow, because\n1\n2 E\n\" T\nX\nt=1\nEf⋆∼µt Eπt∼pt\n\u0002\n(f⋆(πt) −¯f t(πt))2\u0003\n#\n=\nT\nX\nt=1\nEnt(f ⋆| Ht−1) −Ent(f ⋆| Ht)\n= Ent(f ⋆| H0) −Ent(f ⋆| HT)\n≤Ent(f ⋆| H0)\n≤log|F|,\nwhere the last inequality follows because the entropy of a random variable X over a set X\nis always bounded by log|X|.\nWe proceed to prove (2.25). To begin, we use Lemma 39, which implies that\n1\n2(f⋆(πt) −¯f t(πt))2 ≤DKL\n\u0000Prt|f⋆,πt,Ht−1 ∥Prt|πt,Ht−1\n\u0001\n.\nand\n1\n2 Ef⋆∼µt Eπt∼pt\n\u0002\n(f⋆(πt) −¯f t(πt))2\u0003\n= Ef⋆∼µt Eπt∼pt\n\u0002\nDKL\n\u0000Prt|f⋆,πt,Ht−1 ∥Prt|πt,Ht−1\n\u0001\u0003\nSince KL divergence satisfies Ex∼PX\n\u0002\nDKL\n\u0000PY |X=x ∥PY\n\u0001\u0003\n= Ey∼PY\n\u0002\nDKL\n\u0000PX|Y =y ∥PX\n\u0001\u0003\n, this\nis equal to\nEt−1\n\u0002\nDKL\n\u0000Pf ⋆|πt,rt,Ht−1 ∥Pf ⋆|Ht−1\n\u0001\u0003\n= Et−1\n\u0002\nDKL\n\u0000Pf ⋆|Ht ∥Pf ⋆|Ht−1\n\u0001\u0003\n.\n(2.26)\n33\nTaking the expectation over Ht−1, we can write this as\nE\n\u0002\nEt−1\n\u0002\nDKL\n\u0000Pf ⋆|Ht ∥Pf ⋆|Ht−1\n\u0001\u0003\u0003\n= EHt−1 EHt|Ht−1\n\u0002\nDKL\n\u0000Pf ⋆|Ht ∥Pf ⋆|Ht−1\n\u0001\u0003\n.\nA simple exercise shows that for random variables X, Y, Z,\nE(x,y)∼PX,Y\n\u0002\nDKL\n\u0000PZ|X=x,Y =y ∥PZ|X=x\n\u0001\u0003\n= Ent(Z | X) −Ent(Z | X, Y ).\nApplying this result above (and using that Ht−1 ⊂Ht) gives\nEHt−1 EHt|Ht−1\n\u0002\nDKL\n\u0000Pf ⋆|Ht ∥Pf ⋆|Ht−1\n\u0001\u0003\n= Ent(f ⋆| Ht−1) −Ent(f ⋆| Ht)\nas desired.\nThe analysis above critically makes use of the fact that we are concerned with Bayesian\nregret, and have access to the true prior. One might hope that by choosing a sufficiently\nuninformative prior, this approach might continue to work in the frequentist setting. In\nfact, this indeed the case for bandits, though a different analysis is required [7, 8]. However,\none can show (Sections 4 and 6) that the Bayesian analysis we have given here extends to\nsignificantly richer decision making settings, while the frequentist counterpart is limited to\nsimple variants of the multi-armed bandit.\nRemark 11 (Equivalence of min-max frequentist regret and max-min Bayesian\nregret): Using the minimax theorem, it is possible to show that under appropriate\ntechnical conditions\nmin\nAlg max\nM⋆EM⋆,Alg[Reg] =\nmax\nµ∈∆(M) min\nAlg EM⋆∼µ EM⋆,Alg[Reg].\nThat is, if we take the worst-case value of the Bayesian regret over all possible choices\nof prior, this coincides with the minimax value of the frequentist regret.\n2.5 Adversarial Bandits and the Exp3 Algorithm⋆\nWe conclude this section with a brief introduction to the multi-armed bandit problem with\nnon-stochastic/adversarial rewards, which dispenses with Assumption 1. In the context of\nFigure 2, the non-stochastic nature of rewards adds a new “adversarial data” dimension to\nthe problem. As one might expect, the solution we will present for non-stochastic bandits\nwill leverage the the online learning tools introduced in Section 1.6.\nTo simplify the presentation, suppose that the collection of rewards\n{rt(π) ∈[0, 1] : π ∈[A], t ∈[T]}\nfor each action and time step is arbitrary and fixed ahead of the interaction by an oblivious\nadversary. Since we do not posit a stochastic model for rewards, we define regret as in (2.4).\nThe algorithm we present will build upon the exponential weights algorithm studied in\nthe context of online supervised learning in Section 1.6. To make the connection as clear\nas possible, we make a temporary switch from rewards to losses, mapping rt to 1 −rt, a\ntransformation that does not change the problem itself.\n34\nRecall that pt denotes the randomization distribution for the learner at round t. As\ndiscussed in Remark 7, we can write expected regret as\nReg =\nT\nX\nt=1\n⟨pt, ℓt⟩−min\nπ∈[A]\nT\nX\nt=1\n⟨eπ, ℓt⟩\n(2.27)\nwhere ℓt ∈[0, 1]A is the vector of losses for each of the actions at time t.\nSince only the loss (equivalently, reward) of the chosen action πt ∼pt is observed, we\ncannot directly appeal to the exponential weights algorithm, which requires knowledge of\nthe full vector ℓt. To address this, we build an unbiased estimate of the vector ℓt from\na single real-valued observation ℓt(πt). At first, this might appear impossible, but it is\nstraightforward to show that\neℓ\nt(π) = ℓt(π)\npt(π) × I {πt = π}\n(2.28)\nis an unbiased estimate for all π ∈[A], or in vector notation\nEπt∼pt\n\u0002eℓ\nt\u0003\n= ℓt.\n(2.29)\nIf we apply the exponential weights algorithm with the loss vectors eℓ\nt, it can be shown to\nattain regret\nE[Reg] = E\n\" T\nX\nt=1\n⟨pt, ℓt⟩\n#\n−min\nπ\nT\nX\nt=1\n⟨eπ, ℓt⟩\n(2.30)\n= E\n\" T\nX\nt=1\nD\npt,eℓ\ntE#\n−min\nπ E\n\" T\nX\nt=1\nD\neπ,eℓ\ntE#\n≲\np\nAT log A.\n(2.31)\nThis algorithm is known as Exp3 (“Exponential Weights for Exploration and Exploitation”).\nA full proof of this result is left as an exercise in Section 2.7.\n2.6 Deferred Proofs\nProof of Proposition 6 (full version) . Let Et[·] = E[· | Ht] and Pt[·] = P[· | Ht]. We begin\nby using the law of total expectation to express the expected regret as\nRegBayes(µ) = E\n\" T\nX\nt=1\nEt−1[f⋆(π⋆) −f⋆(πt)]\n#\n.\nHere and throughout the proof, E[·] will denote the joint expectation over both M⋆∼µ and\nover the sequence HT = (π1, r1), . . . , (πT, rT) that the algorithm generates by interacting\nwith M⋆.\nWe first simplify the (conditional) expected regret for each step t. Let ¯f t(π) := Et−1[f⋆(π)]\ndenote the posterior mean reward function at time t, which should be thought of as\nthe expected value of f⋆given everything we have learned so far.\nNext, let ¯f t\nπ′(π) =\nEt−1[f⋆(π) | π⋆= π′], which is the expected reward given everything we have learned so far,\nassuming that π⋆= π′. We proceed to write the expression\nEt−1[f⋆(π⋆) −f⋆(πt)]\n35\nin terms of these quantities. For the learner’s reward, we observe that f⋆is conditionally\nindependent of πt given Ht−1, we have\nEt−1[f⋆(πt)] = Eπ∼pt[ ¯f t(π)].\nFor the reward of the optimal action, we begin by writing\nEt−1[f⋆(π⋆)] =\nX\nπ∈Π\nPt−1(π⋆= π) Et−1[f⋆(π) | π⋆= π]\n=\nX\nπ∈Π\nPt−1(π⋆= π) ¯f t\nπ(π)\n= Eπ∼pt\n\u0002 ¯f t\nπ(π)\n\u0003\n,\nwhere we have used that pt was chosen to match the posterior distribution over π⋆. This\nestablishes that\nEt−1[f⋆(π⋆) −f⋆(πt)] = Eπ∼pt\n\u0002 ¯f t\nπ(π) −¯f t(π)\n\u0003\n.\nWe now make use of the following decoupling-type inequality, which follows from (2.24):\nEπ∼pt\n\u0002 ¯f t\nπ(π) −¯f t(π)\n\u0003\n≤\nq\nA · Eπ,π⋆∼pt\n\u0002\n( ¯f t\nπ⋆(π) −¯f t(π))2\u0003\n.\n(2.32)\nTo keep notation as clear as possible going forward, let us use boldface script (πt, π⋆, f ⋆,\nHt) to refer to the abstract random variables under consideration, and use non-boldface\nscript (πt, π⋆, f⋆, Ht) to refer to their realizations. As in the simplified proof, we will\nshow that the right-hand side in (2.32) is related to a notion of information gain (that is,\ninformation about π⋆acquired at step t). Using Pinsker’s inequality, we have\nEπt,π⋆∼pt\n\u0002\n( ¯f t\nπ⋆(πt) −¯f t(πt))2\u0003\n≤Et−1\n\u0002\nDKL\n\u0000Prt|π⋆,πt,Ht−1 ∥Prt|πt,Ht−1\n\u0001\u0003\n.\nSince KL divergence satisfies EX\n\u0002\nDKL\n\u0000PY |X ∥PY\n\u0001\u0003\n= EY\n\u0002\nDKL\n\u0000PX|Y ∥PX\n\u0001\u0003\n, this is equal to\nEt−1\n\u0002\nDKL\n\u0000Pπ⋆|πt,rt,Ht−1 ∥Pπ⋆|Ht−1\n\u0001\u0003\n= Et−1\n\u0002\nDKL\n\u0000Pπ⋆|Ht ∥Pπ⋆|Ht−1\n\u0001\u0003\n.\n(2.33)\nThis is quantifying how much information about π⋆we gain by playing πt and observing rt\nat step t, relative to what we knew at step t −1. Applying (2.32) and (2.33), we have\nT\nX\nt=1\nEπ∼pt\n\u0002 ¯f t\nπ(π) −¯f t(π)\n\u0003\n≤\nT\nX\nt=1\nq\nA · Eπ,π⋆∼pt\n\u0002\n( ¯f t\nπ⋆(π) −¯f t(π))2\u0003\n≤\nT\nX\nt=1\nq\nA · Et−1\n\u0002\nDKL\n\u0000Pπ⋆|Ht ∥Pπ⋆|Ht−1\n\u0001\u0003\n≤\nv\nu\nu\ntAT ·\nT\nX\nt=1\nE\n\u0002\nDKL\n\u0000Pπ⋆|Ht ∥Pπ⋆|Ht−1\n\u0001\u0003\n.\nWe can write\nE\n\u0002\nDKL\n\u0000Pπ⋆|Ht ∥Pπ⋆|Ht−1\n\u0001\u0003\n= Ent(π⋆| Ht−1) −Ent(π⋆| Ht),\nso telescoping gives\nT\nX\nt=1\nE\n\u0002\nDKL\n\u0000Pπ⋆|Ht ∥Pπ⋆|Ht−1\n\u0001\u0003\n= Ent(π⋆| H0) −Ent(π⋆| HT) ≤log(A).\n36\n2.7 Exercises\nExercise 4 (Adversarial Bandits): In this exercise, we will prove a regret bound for adver-\nsarial bandits (Section 2.5), where the sequence of rewards (losses) is non-stochastic. To make\na direct connection to the Exponential Weights Algorithm, we switch from rewards to losses,\nmapping rt to 1 −rt, a transformation that does not change the problem itself. To simplify\nthe presentation, suppose that a collection of losses\n{ℓ\nt(π) ∈[0, 1] : π ∈[A], t ∈[T]}\nfor each action π and time step t is arbitrary and chosen before round t = 1; this is referred to\nas an oblivious adversary. We denote by ℓ\nt = (ℓ\nt(1), . . . , ℓ\nt(A)) the vector of losses at time t.\nThe protocol for the problem of adversarial multi-armed bandits (with losses) is as follows:\nMulti-Armed Bandit Protocol\nfor t = 1, . . . , T do\nSelect decision πt ∈Π := {1, . . . , A} by sampling πt ∼pt\nObserve loss ℓ\nt(πt)\nLet pt be the randomization distribution of the decision-maker on round t. Expected regret\ncan be written as\nE[Reg] = E\n\" T\nX\nt=1\n\np\nt, ℓ\nt\u000b\n#\n−min\nπ∈[A]\nT\nX\nt=1\n\neπ, ℓ\nt\u000b\n.\n(2.34)\nSince only the loss of the chosen action πt ∼pt is observed, we cannot directly appeal to the\nExponential Weights Algorithm. The solution is to build an unbiased estimate of the vector ℓ\nt\nfrom the single real-valued observation ℓ\nt(πt).\n1. Prove that the vector eℓ\nt(· | πt) defined by\neℓ\nt(π | π\nt) = ℓ\nt(π)\npt(π) × I {π\nt = π}\n(2.35)\nis an unbiased estimate for ℓ\nt(π) for all π ∈[A]. In vector notation, this means\nEπt∼pt[eℓ\nt(· | π\nt)] = ℓ\nt.\nConclude that\nE[Reg] = E\n\" T\nX\nt=1\nEπt∼pt\np\nt,eℓ\nt\u000b\n#\n−min\nπ∈[A] E\n\" T\nX\nt=1\nEπt∼pt\neπ,eℓ\nt\u000b\n#\n(2.36)\nAbove, we use the shorthand eℓ\nt = eℓ(· | πt).\n2. Show that given π′,\nEπ∼pt\nh\neℓ\nt(π | π′)2i\n= ℓ\nt(π′)2\npt(π′) ,\nso that\nEπt∼pt Eπ∼pt\nh\neℓ\nt(π | π\nt)2i\n≤A.\n(2.37)\n3. Define\np\nt(π) ∝exp\n(\n−η\nt−1\nX\ns=1\n\neπ,eℓ\ns(· | π\ns)\n\u000b\n)\n,\n37\nwhich corresponds to the exponential weights algorithm on the estimated losses eℓ\ns. Apply\n(1.41) to the estimated losses to show that for any π ∈[A],\nE\n\" T\nX\nt=1\nEπt∼pt\np\nt,eℓ\nt\u000b\n#\n−E\n\" T\nX\nt=1\nEπt∼pt\neπ,eℓ\nt\u000b\n#\n≲\np\nAT log A\nHence, the price of bandit feedback in the adversarial model, as compared to full-information\nonline learning, is only\n√\nA.\n3. CONTEXTUAL BANDITS\nIn the last section, we studied the multi-armed bandit problem, which arguably the simplest\nframework for interactive decision making. This simplicity comes at a cost: few real-world\nproblems can be modeled as a multi-armed bandit problem directly. For example, for the\nproblem of selecting medical treatments, the multi-armed bandit formulation presupposes\nthat one treatment rule (action/decision) is good for all patients, which is clearly unreason-\nable. To address this, we augment the problem formulation by allowing the decision-maker\nto select the action πt after observing a context xt; this is called the contextual bandit prob-\nlem. The context xt, which may also be thought of as a feature vector or collection of\ncontext\ndecision\nreward\nobservation\nxt\n⇡t\nrt\not\nFigure 6: An illustration of the contextual multi-armed bandit problem. A doctor (the\nlearner) aims to select a treatment based on the context (medical history, symptoms).\ncovariates (e.g., a patient’s medical history, or the profile of a user arriving at a website),\ncan be used by the learner to better maximize rewards by tailoring decisions to the specific\npatient or user under consideration.\nContextual Bandit Protocol\nfor t = 1, . . . , T do\nObserve context xt ∈X.\nSelect decision πt ∈Π = {1, . . . , A}.\nObserve reward rt ∈R.\nAs with multi-armed bandits, contextual bandits can be studied in a stochastic frame-\nwork or in an adversarial framework. In this course, we will allow the contexts x1, . . . , xT\nto be generated in an arbitrary, potentially adversarially fashion, but assume that rewards\nare generated from a fixed conditional distribution.\n38\nAssumption 2 (Stochastic Rewards): Rewards are generated independently via\nrt ∼M⋆(· | xt, πt),\n(3.1)\nwhere M⋆(· | ·, ·) is the underlying model (or conditional distribution).\nThis generalizes the stochastic multi-armed bandit framework in Section 2. We define\nf⋆(x, π) := E [r | x, π]\n(3.2)\nas the mean reward function under r ∼M⋆(· | x, π), and define π⋆(x) := arg maxπ∈Π f⋆(x, π)\nas the optimal policy, which maps each context x to the optimal action for the context. We\nmeasure performance via regret relative to π⋆:\nReg :=\nT\nX\nt=1\nf⋆(xt, π⋆(xt)) −\nT\nX\nt=1\nEπt∼pt[f⋆(xt, πt)],\n(3.3)\nwhere pt ∈∆(Π) is the learner’s action distribution at step t (conditioned on the Ht−1\nand xt). This provides a (potentially) much stronger notion of performance than what we\nconsidered for the multi-armed bandit: Rather than competing with the reward of the single\nbest action, we are competing with the reward of the best sequence of decisions tailored to\nthe context sequence we observe.\nRemark 12 (Contextual bandits versus reinforcement learning): To readers\nalready familiar with reinforcement learning, the contextual bandit setting may appear\nquite similar at first glance, with the term “context” replacing “state”. The key dif-\nference is that in reinforcement learning, we aim to control the evolution of x1, . . . , xT\n(which is why they are referred to as state), whereas in contextual bandits, we take\nthe sequence as a given, and only aim to maximize our rewards conditioned on the\nsequence.\nFunction approximation and desiderata.\nIf X, the set of possible contexts, is finite,\none might imagine running a separate MAB algorithm for each context. In this case, the\nregret bound would scale with |X|,9 an undesirable property which reflects the fact that\nthis approach does not allow for generalization across contexts. Instead, we would like to\nshare information between different contexts. After all, a doctor prescribing treatments\nmight never observe exactly the same medical history and symptoms twice, but they might\nsee similar patients or recognize underlying patterns. In the spirit of statistical learning\n(Section 1) this means assuming access to a class F that can model the mean reward\nfunction, and aiming for regret bounds that scale with log|F| (reflecting the statistical\ncapacity of F), with no dependence on the cardinality of X. To facilitate this, we will\nassume a well-specified/realizable model.\n9One can show that running an independent instance of UCB for each context leads to regret\neO(\np\nAT · |X|); see Exercise 5.\n39\nAssumption 3: The decision-maker has access to a class F ⊂{f : X × Π →R} such\nthat f⋆∈F.\nUsing the class F, we would like to develop algorithms that can model the underlying reward\nfunction for better decision making performance. With this goal in mind, it is reasonable to\ntry leveraging the algorithms and respective guarantees we have already seen for statistical\nand online supervised learning. At this point, however, the decision-making problem—with\nits exploration-exploitation dilemma—appears to be quite distinct from these supervised\nlearning frameworks. Indeed, naively applying supervised learning methods, which do not\naccount for the interactive nature of the problem, can lead to failure, as we saw with the\ngreedy algorithm in Section 2.1.\nIn spite of these apparent difficulties, in the next few\nlectures, we will show that it is possible to leverage supervised learning methods to develop\nprovable decision making methods, thereby bridging the two methodologies.\n3.1 Optimism: Generic Template\nWhat algorithmic principles should we employ to solve the contextual bandit problem?\nOne approach is to adapt solutions from the multi-armed bandit setting. There, we saw\nthat the principle of optimism (in particular, the UCB algorithm) led to (nearly) optimal\nrates for bandits, so a natural question is whether optimism can be adapted to give optimal\nguarantees in the presence of contexts. The answer to this last question is: it depends. We\nwill first describe some positive results under assumptions on F, then provide a negative\nexample, and finally turn to an entirely different algorithmic principle.\nOptimism via confidence sets.\nLet us describe a general approach (or, template) for\napplying the principle of optimism to contextual bandits [26, 1, 74, 37]. Suppose that at\neach time, we have a way to construct a confidence set\nF t ⊆F\nbased on the data observed so far, with the important property that f⋆∈F t. Given such\na confidence set we can define upper and lower confidence functions f t, ¯f t : X × Π →R via\nf t(x, π) = min\nf∈Ft f(x, π),\n¯f t(x, π) = max\nf∈Ft f(x, π).\n(3.4)\nThese functions generalize the upper and lower confidence bounds we constructed in Sec-\ntion 2. Since f⋆∈F t, they have the property that\nf t(x, π) ≤f⋆(x, π) ≤¯f t(x, π)\n(3.5)\nfor all x ∈X, π ∈Π. As such, if we consider a contextual analogue of the UCB algorithm,\ngiven by\nπt = arg max\nπ∈Π\n¯f t(xt, π),\n(3.6)\nthen as in Lemma 7, the optimistic action satisfies\nf⋆(xt, π⋆) −f⋆(xt, πt) ≤¯f t(xt, πt) −f t(xt, πt).\n40\nThat is, the suboptimality is bounded by the width of the confidence interval at (xt, πt),\nand the total regret is bounded as\nReg ≤\nT\nX\nt=1\n¯f t(xt, πt) −f t(xt, πt).\n(3.7)\nTo make this approach concrete and derive sublinear bounds on the regret, we need a way to\nconstruct the confidence set F t, ideally so that the width in (3.7) shrinks as fast as possible.\nConstructing confidence sets with least squares.\nWe construct confidence sets by\nappealing to a supervised learning method, empirical risk minimization with the square loss\n(or, least squares). Assume that f(x, a) ∈[0, 1] for all f ∈F, and that rt ∈[0, 1] almost\nsurely. Let\nbf t = arg min\nf∈F\nt−1\nX\ni=1\n(f(xi, πi) −ri)2\n(3.8)\nbe the empirical risk minimizer at round t, and with β := 8 log(|F|/δ) define F 1 = F and\nF t =\n(\nf ∈F :\nt−1\nX\ni=1\n(f(xi, πi) −ri)2 ≤\nt−1\nX\ni=1\n( bf t(xi, πi) −ri)2 + β\n)\n(3.9)\nfor t > 1. That is, our confidence set F t is the collection of all functions that have empirical\nsquared error close to that of bf t. The idea behind this construction is to set β “just large\nenough”, to ensure that we do not accidentally exclude f⋆, with the precise value for β\ninformed by the concentration inequalities we explored in Section 1. The only catch here is\nthat we need to use variants of these inequalities that handle dependent data, since xt and\nπt are not i.i.d. in (3.8). The following result shows that F t is indeed valid and, moreover,\nthat all functions f ∈F t have low estimation error on the history.\nLemma 10: Let π1, . . . , πT be chosen by an arbitrary (and possibly randomized)\ndecision-making algorithm. With probability at least 1 −δ, f⋆∈F t for all t ∈[T].\nMoreover, with probability at least 1 −δ, for all τ ≤T, all f ∈F τ satisfy\nτ−1\nX\nt=1\nEπt∼pt\n\u0002\n(f(xt, πt) −f⋆(xt, πt))2\u0003\n≤4β,\n(3.10)\nwhere β = 8 log(|F|/δ).\nLemma 10 is valid for any algorithm, but it is particularly useful for UCB as it establishes\nthe validity of the confidence bounds as per (3.5); however, it is not yet enough to show\nthat the algorithm attains low regret. Indeed, to bound the regret, we need to control the\nconfidence widths in (3.7), but there is a mismatch: for step τ, the regret bound in (3.7)\nconsiders the width at (xτ, πτ), but (3.10) only ensures closeness of functions in F τ under\n(x1, π1), . . . , (xτ−1, πτ−1). We will show in the sequel that for linear models, it is possible to\ncontrol this mismatch, but that this is not possible in general.\n41\nProof of Lemma 10. For f ∈F, define\nU t(f) = (f(xt, πt) −rt)2 −(f⋆(xt, πt) −rt)2.\n(3.11)\nIt is straightforward to check that10\nEt−1 U t(f) = Et−1(f(xt, πt) −f⋆(xt, πt))2,\n(3.12)\nwhere Et−1[·] := E[· | Ht−1, xt]. Then Zt(f) = Et−1 U t(f) −U t(f) is a martingale difference\nsequence and Pτ\nt=1 Zt(f) is a martingale. Since increments Zt(f) are bounded as |Zt(f)| ≤1\n(this holds whenever f ∈[0, 1], rt ∈[0, 1]), according to Lemma 35 with η =\n1\n8, with\nprobability at least 1 −δ, for all τ ≤T,\nτ\nX\nt=1\nZt(f) ≤1\n8\nτ\nX\nt=1\nEt−1\n\u0002\nZt(f)2\u0003\n+ 8 log(δ−1).\n(3.13)\nTo control the right-hand side, we again use that f, rt ∈[0, 1] to bound\nEt−1\n\u0002\nZt(f)2\u0003\n≤Et−1\nh\u0000(f(xt, πt) −rt)2 −(f⋆(xt, πt) −rt)2\u00012i\n(3.14)\n≤4 Et−1\n\u0002\n(f(xt, πt) −f⋆(xt, πt))2\u0003\n= 4 Et−1 U t(f)\n(3.15)\nThen, after rearranging, (3.13) becomes\n1\n2\nτ\nX\nt=1\nEt−1 U t(f) ≤\nτ\nX\nt=1\nU t(f) + 8 log(δ−1).\n(3.16)\nSince the left-hand side is nonnegative, we conclude that with probability at least 1 −δ,\nτ\nX\nt=1\n(f⋆(xt, πt) −rt)2 ≤\nτ\nX\nt=1\n(f(xt, πt) −rt)2 + 8 log(δ−1).\n(3.17)\nTaking a union bound over f ∈F, gives that with probability at least 1 −δ,\n∀f ∈F, ∀τ ∈[T],\nτ\nX\nt=1\n(f⋆(xt, πt) −rt)2 ≤\nτ\nX\nt=1\n(f(xt, πt) −rt)2 + 8 log(|F|/δ),\n(3.18)\nand in particular\n∀τ ∈[T + 1],\nτ−1\nX\nt=1\n(f⋆(xt, πt) −rt)2 ≤\nτ−1\nX\nt=1\n( bf τ(xt, πt) −rt)2 + 8 log(|F|/δ);\n(3.19)\nthat is, we have f⋆∈F τ for all τ ∈{1, . . . , T + 1}, proving the first claim. For the second\npart of the claim, observe that any f ∈F τ must satisfy\nτ−1\nX\nt=1\nU t(f) ≤β\nsince the empirical risk of f⋆is never better than the empirical risk of the minimizer bf t.\nThus from (3.16), with probability at least 1 −δ, for all τ ≤T,\nτ−1\nX\nt=1\nEt−1 U t(f) ≤2β + 16 log(δ−1).\n(3.20)\nThe second claim follows by taking union bound over f ∈F τ ⊆F, and by (3.12).\n10We leave Et−1 on the right-hand side to include the case of randomized decisions πt ∼pt.\n42\n3.2 Optimism for Linear Models: The LinUCB Algorithm\nWe now instantiate the general template for optimistic algorithms developed in the previous\nsection for the special case where F is a class of linear functions.\nLinear models.\nWe fix a feature map ϕ : X × Π →Bd\n2(1), where Bd\n2(1) is the unit-norm\nEuclidean ball in Rd. The feature map is assumed to be known to the learning agent. For\nexample, in the case of medical treatments, ϕ transforms the medical history and symptoms\nx for the patient, along with a possible treatment π, to a representation ϕ(x, π) ∈Bd\n2(1).\nWe take F to be the set of linear functions given by\nF = {(x, π) 7→⟨θ, ϕ(x, π)⟩| θ ∈Θ},\n(3.21)\nwhere Θ ⊆Bd\n2(1) is the parameter set. As before, we assume f⋆∈F; we let θ∗denote\nthe corresponding parameter vector, so that f⋆(x, π) = ⟨θ⋆, ϕ(x, π)⟩. With some abuse of\nnotation, we associate the set of parameters Θ to the corresponding functions in F.\nTo apply the technical results in the previous section, we assume for simplicity that\n|Θ| = |F| is finite. To extend our results to potentially non-finite sets, one can work with\nan ε-discretization, or ε-net, which is of size at most O(ε−d) using standard arguments.\nTaking ε ∼1/T ensures only a constant loss in cumulative regret relative to the continuous\nset of parameters, while log |F| ≲d log T.\nThe LinUCB algorithm.\nThe following figure displays an algorithm we refer to as\nLinUCB [12, 26, 1], which adapts the generic template for optimistic algorithms to the case\nwhere F is linear in the sense of (3.21).\nLinUCB\nInput: β > 0\nfor t = 1, . . . , T do\nCompute the least squares solution bθt (over θ ∈Θ) given by\nbθt = arg min\nθ∈Θ\nX\ni<t\n(⟨θ, ϕ(xi, πi)⟩−ri)2.\nDefine\neΣt =\nt−1\nX\ni=1\nϕ(xi, πi)ϕ(xi, πi) T + I.\nGiven xt, select action\nπt ∈arg max\nπ∈Π\nmax\nθ:∥θ−bθt∥\n2\neΣt≤16β+4\n⟨θ, ϕ(xt, π)⟩.\nObserve reward rt\nThe following result shows that LinUCB enjoys a regret bound that scales with the\ncomplexity log|F| of the model class and the feature dimension d.\n43\nProposition 7: Let Θ ⊆Bd\n2(1) and fix ϕ : X × Π →Bd\n2(1). For a finite set F of linear\nfunctions (3.21), taking β = 8 log(|F|/δ), LinUCB satisfies, with probability at least\n1 −δ,\nReg ≲\np\nβdT log(1 + T/d) ≲\np\ndT log(|F|/δ) log(1 + T/d)\nfor any sequence of contexts x1, . . . , xT. More generally, for infinite F, we may take\nβ = O(d log(T))a and\nReg ≲d\n√\nT log(T).\naThis follows from a simple covering number argument.\nNotably, this regret bound has no explicit dependence on the context space size |X|. Inter-\nestingly, the bound is also independent of the number of actions |Π|, which is replaced by\nthe dimension d; this reflects that the linear structure of F allows the learner to generalize\nnot just across contexts, but across decisions. We will expand upon the idea of generalizing\nacross actions in Section 4.\nProof of Proposition 7. The confidence set (3.9) in the generic optimistic algorithm tem-\nplate is\nF t =\n(\nθ ∈Θ :\nt−1\nX\ni=1\n(⟨θ, ϕ(xi, πi)⟩−ri)2 ≤\nt−1\nX\ni=1\n(⟨bθt, ϕ(xi, πi)⟩−ri)2 + β\n)\n,\n(3.22)\nwhere bθt is the least squares solution computed in LinUCB. According to Lemma 10, with\nprobability at least 1 −δ, for all t ∈[T], all θ ∈F t satisfy\nt−1\nX\ni=1\n(⟨θ −θ∗, ϕ(xi, πi)⟩)2 ≤4β,\n(3.23)\nwhich means that F t is a subset of11\nΘ′ =\nn\nθ ∈Θ : ∥θ −θ∗∥2\nΣt ≤4β\no\n,\nwhere\nΣt =\nt−1\nX\ni=1\nϕ(xi, πi)ϕ(xi, πi) T.\n(3.24)\nSince bθt ∈F t, we have that for any θ ∈Θ′, by triangle inequality,\n\r\rθ −bθt\r\r2\nΣt ≤16β.\nFurthermore, since bθt ∈Θ ⊆Bd\n2(1),\n\r\rθ −bθt\r\r\n2 ≤2. Combining the two constraints into one,\nwe find that Θ′ is a subset of\nΘ′′ =\nn\nθ ∈Rd :\n\r\rθ −bθt\r\r2\neΣt ≤16β + 4\no\n,\nwhere\neΣt =\nt−1\nX\ni=1\nϕ(xi, πi)ϕ(xi, πi) T + I.\n(3.25)\nThe definition of ¯f t in (3.4) and the inclusion Θ′ ⊆Θ′′ implies that\n¯f t(x, π) ≤\nmax\nθ:∥θ−bθt∥eΣt≤√16β+4\n⟨θ, ϕ(x, π)⟩=\n\nbθt, ϕ(x, π)\n\u000b\n+\np\n16β + 4 ∥ϕ(x, π)∥(eΣt)−1 ,\n(3.26)\n11For a PSD matrix Σ ⪰0, we define ∥x∥Σ =\np\n⟨x, Σx⟩.\n44\nand similarly f t(x, π) ≥\n\nbθt, ϕ(x, π)\n\u000b\n−√16β + 4∥ϕ(x, π)∥(eΣt)−1. We conclude that regret\nof the UCB algorithm, in view of Lemma 7, is\nReg ≤2\np\n16β + 4\nT\nX\nt=1\n∥ϕ(xt, πt)∥(eΣt)−1 ≲\nv\nu\nu\ntβT\nT\nX\nt=1\n∥ϕ(xt, πt)∥2\n(eΣt)−1.\n(3.27)\nThe above upper bound has the same flavor as the one in Lemma 8: as we obtain more and\nmore information in some direction v, the matrix eΣt has a larger and larger component in\nthat direction, and for that direction v, the term ∥v∥2\n(eΣt)−1 becomes smaller and smaller.\nTo conclude, we apply a potential argument, Lemma 11 below, to bound\nT\nX\nt=1\n∥ϕ(xt, πt)∥2\n(eΣt)−1 ≲d log(1 + T/d).\nThe following result is referred to as the elliptic potential lemma, and it can be thought\nof as a generalization of Lemma 8.\nLemma 11 (Elliptic potential lemma): Let a1, . . . , aT ∈Rd satisfy ∥at∥≤1 for all\nt ∈[T], and let Vt = I + P\ns≤t asa T\ns . Then\nT\nX\nt=1\n∥at∥2\nV −1\nt−1 ≤2d log(1 + T/d).\n(3.28)\nProof Lemma 11 (sketch). First, the determinant of Vt evolves as\ndet(Vt) = det(Vt−1)\n\u0010\n1 + ∥at∥2\nV −1\nt−1\n\u0011\n.\nSecond, using the identity u ∧1 ≤2 ln(1 + u) for u ≥0, the left-hand side of (3.28) is at\nmost 2 PT\nt=1 log\n\u0010\n1 + ∥at∥2\nV −1\nt−1\n\u0011\n. The proof concludes by upper bounding the determinant\nof Vn via the AM-GM inequality. We leave the details as an exercise; see also Lattimore\nand Szepesv´ari [60].\n3.3 Moving Beyond Linear Classes: Challenges\nWe now present an example of a class F for which optimistic methods necessarily incur\nregret that scales linearly with either the cardinality of F or with cardinality of X, meaning\nthat we do not achieve the desired log|F| scaling of regret that one might expect in (offline\nor online) supervised learning.\nExample 3.1 (Failure of optimism for contextual bandits [37]). Let A = 2, and let N ∈N\nbe given. Let πg and πb be two actions available in each context, so that Π = {πg, πb}. Let\nX = {x1, . . . , xN} be a set of distinct contexts, and define a class F = {f⋆, f1, . . . , fN} of\ncardinality N + 1 as follows. Fix 0 < ε < 1. Let f⋆(x, πg) = 1 −ε and f⋆(x, πb) = 0 for any\nx ∈X. For each i ∈[N], fi(xj, πg) = 1 −ε and fi(xj, πb) = 0 for j ̸= i, while fi(xi, πg) = 0\nand fi(xi, πb) = 1.\n45\nNow, consider a (well-specified) problem instances in which rewards are deterministic\nand given by\nrt = f⋆(xt, πt),\nwhich we note is a constant function with respect to the context. Since f⋆is the true model,\nπg is always the best action, bringing a reward of 1 −ε per round. Any time πb is chosen,\nthe decision-maker incurs instantaneous regret 1 −ε. We will now argue that if we apply\nthe generic optimistic algorithm from Section 3.1, it will choose πb every time a new context\nis encountered, leading to Ω(N) regret.\nLet St be the set of distinct contexts encountered before round t. Clearly, the exact\nminimizers of empirical square loss (see (3.8)) are f⋆, and all fi where i is such that xi /∈St.\nHence, for any choice of β ≥0, the confidence set in (3.9) contains all fi for which xi /∈St.\nThis implies that for each t ∈[T] where xt = xi /∈St, action πb has a higher upper confidence\nbound than πg, since\n¯f t(xt, πb) = fi(xi, πb) = 1 > ¯f t(xt, πg) = f⋆(xt, πg) = 1 −ε.\nHence, the cumulative regret grows by 1−ε every time a new context is presented, and thus\nscales as Ω(N(1−ε)) if the contexts are presented in order. That is, since N = |X| = |F|−1,\nthe confidence-based algorithm fails to achieve logarithmic dependence on F (note that we\nmay take ε = 1/2 for concreteness).\nLet us remark that this failure continues even if contexts are stochastic. If the contexts\nare chosen via the uniform distribution on X, then for T ≥N, at least a constant proportion\nof the domain will be presented, which still leads to a lower bound of\nE[Reg] = Ω(N) = Ω(min{|F|, |X|}).\n◁\nWhat is behind the failure of optimism in this example? The structure of F forces\noptimistic methods to over-explore, as the algorithm puts too much hope into trying the\narm πb for each new context. As a result, the confidence widths in (3.7) do not shrink\nquickly enough.\nBelow, we will see that there are alternative methods which do enjoy\nlogarithmic dependence on the size of F, with the best of these methods achieving regret\nO(\np\nAT log|F|).\nWe mention in passing that even though optimism does not succeed in general, it is\nuseful to understand in what cases it works. We saw that the structure of linear classes\nin Rd only allowed for d “different” directions, while in the example above, the optimistic\nalgorithm gets tricked by each new context, and is not able to shrink the confidence band\nquickly enough over the domain. In a few lectures (Section 4), we will introduce the eluder\ndimension, a structural property of the class F which is sufficient for optimistic methods to\nexperience low regret, generalizing the positive result for the linear setting.\n3.4 The ε-Greedy Algorithm for Contextual Bandits\nGiven that the principle of optimism only leads to low regret for classes F with special\nstructure, we are left wondering whether there are more general algorithmic principles for\ndecision making that can succeed for any class F. In this section and the following one,\nwe will present two such principles.\nBoth approaches will still make use of supervised\nlearning with the class F, but will build upon online supervised learning as opposed to\noffline/statistical learning. To make the use of supervised learning as modular as possible,\nwe will abstract this away using the notion of an online regression oracle [36].\n46\nDefinition 3 (Online Regression Oracle): At each time t ∈[T], an online regression\noracle returns, given\n(x1, π1, r1), . . . , (xt−1, πt−1, rt−1)\nwith E[ri|xi, πi] = f⋆(xi, πi) and πi ∼pi, a function bf t : X × Π →R such that\nT\nX\nt=1\nEπt∼pt( bf t(xt, πt) −f⋆(xt, πt))2 ≤EstSq(F, T, δ)\nwith probability at least 1 −δ.\nFor the results that follow, pi = pi(·|xi, Hi−1) will\nrepresent the randomization distribution of a decision-maker.\nFor example, for finite classes, the (averaged) exponential weights method introduced in\nSection 1.6 is an online regression oracle with EstSq(F, T, δ) = log(|F|/δ). More generally,\nin view of Lemma 6, any online learning algorithm that attains low square loss regret for\nthe problem of predicting of rt based on (xt, πt) leads to a valid online regression oracle.\nNote that we make use of online learning oracles for the results that follow because\nwe aim to derive regret bounds that hold for arbitrary, potentially adversarial sequences\nx1, . . . , xT. If we instead assume that contexts are i.i.d., it is reasonable to make use of\nalgorithms for offline estimation, or statistical learning with F. See Section 3.5.1 for further\ndiscussion.\nThe first general-purpose contextual bandit algorithm we will study, illustrated below,\nis a contextual counterpart to the ε-Greedy method introduced in Section 2.\nε-Greedy for Contextual Bandits\nInput: ε ∈(0, 1).\nfor t = 1, . . . , T do\nObtain bf t from online regression oracle for (x1, π1, r1), . . . , (xt−1, πt−1, rt−1).\nObserve xt.\nWith prob. ε, select πt ∼unif([A]), and with prob. 1 −ε, choose the greedy\naction\nbπt = arg max\nπ∈[A]\nbf t(xt, π).\nObserve reward rt.\nAt each step t, the algorithm uses an online regression oracle to compute a reward estimator\nbf t(x, a) based on the data Ht−1 collected so far. Given this estimator, the algorithm uses the\nsame sampling strategy as in the non-contextual case: with probability 1−ε, the algorithm\nchooses the greedy decision\nbπt = arg max\nπ\nbf t(xt, π),\n(3.29)\nand with probability ε it samples a uniform random action πt ∼unif({1, . . . , A}). The\nfollowing theorem shows that whenever the online estimation oracle has low estimation\nerror EstSq(F, T, δ), this method achieves low regret.\n47\nProposition 8: Assume f⋆∈F and f⋆(x, a) ∈[0, 1]. Suppose the decision-maker\nhas access to an online regression oracle (Definition 3) with a guarantee EstSq(F, T, δ).\nThen by choosing ε appropriately, the ε-Greedy algorithm ensures that with probability\nat least 1 −δ,\nReg ≲A1/3T 2/3 · EstSq(F, T, δ)1/3\nfor any sequence x1, . . . , xT. As a special case, when F is finite, if we use the (averaged)\nexponential weights algorithm as an online regression oracle, the ε-Greedy algorithm\nhas\nReg ≲A1/3T 2/3 · log1/3(|F|/δ).\nNotably, this result scales with log|F| for any finite class, analogous to regret bounds for\noffline/online supervised learning. The T 2/3-dependence in the regret bound is suboptimal\n(as seen for the special case of non-contextual bandits), which we will address using more\ndeliberate exploration methods in the sequel.\nProof of Proposition 8. Recall that pt denotes the randomization strategy on round t, com-\nputed after observing xt. Following the same steps as the proof of Proposition 4, we can\nbound regret by\nReg =\nT\nX\nt=1\nEπt∼pt[f⋆(xt, π⋆(xt)) −f⋆(xt, πt)] ≤\nT\nX\nt=1\nf⋆(xt, π⋆(xt)) −f⋆(xt, bπt) + εT,\nwhere the εT term represents the bias incurred by exploring uniformly.\nFix t and abbreviate π⋆(xt) = π⋆. We have\nf⋆(xt, π⋆) −f⋆(xt, bπt)\n= [f⋆(xt, π⋆) −bf t(xt, π⋆)] + [ bf t(xt, π⋆) −bf t(xt, bπt)] + [ bf t(xt, bπt) −f⋆(xt, bπt)]\n≤\nX\nπ∈{bπt,π⋆}\n|f⋆(xt, π) −bf t(xt, π)|\n=\nX\nπ∈{bπt,π⋆}\n1\np\npt(π)\np\npt(π)|f⋆(xt, π) −bf t(xt, π)|.\nBy the Cauchy-Schwarz inequality, the last expression is at most\n\n\n\nX\nπ∈{bπt,π⋆}\n1\npt(π)\n\n\n\n1/2\n\n\nX\nπ∈{bπt,π⋆}\npt(π)\n\u0010\nf⋆(xt, π) −bf t(xt, π)\n\u00112\n\n\n\n1/2\n(3.30)\n≤\nr\n2A\nε\n\u001a\nEπt∼pt\n\u0010\nf⋆(xt, πt) −bf t(xt, πt)\n\u00112\u001b1/2\n.\n(3.31)\n48\nSumming across t, this gives\nT\nX\nt=1\nf⋆(xt, π⋆(xt)) −f⋆(xt, bπt) ≤\nr\n2A\nε\nT\nX\nt=1\n\u001a\nEπt∼pt\n\u0010\nf⋆(xt, πt) −bf t(xt, πt)\n\u00112\u001b1/2\n(3.32)\n≤\nr\n2AT\nε\n( T\nX\nt=1\nEπt∼pt\n\u0010\nf⋆(xt, πt) −bf t(xt, πt)\n\u00112\n)1/2\n. (3.33)\nNow observe that the online regression oracle guarantees that with probability 1 −δ,\nT\nX\nt=1\nEπt∼pt\n\u0010\nf⋆(xt, πt) −bf t(xt, πt)\n\u00112\n≤EstSq(F, T, δ).\nWhenever this occurs, we have\nReg ≲\nr\nATEstSq(F, T, δ)\nε\n+ εT.\nChoosing ε to balance the two terms leads to the claimed result.\n3.5 Inverse Gap Weighting: An Optimal Algorithm for General Model Classes\nTo conclude this section, we present a general, oracle-based algorithm for contextual bandits\nwhich achieves\nReg ≲\np\nAT log|F|\nfor any finite class F. As with ε-Greedy, this approach has no dependence on the cardinality\n|X| of the context space, reflecting the ability to generalize across contexts. The dependence\non T improves upon ε-Greedy, and is optimal.\nTo motivate the approach, recall that conceptually, the key step of the proof of Propo-\nsition 8 involved relating the instantaneous regret\nEπt∼pt[f⋆(xt, π⋆(xt)) −f⋆(xt, πt)]\n(3.34)\nof the decision maker at time t to the instantaneous estimation error\nEπt∼pt\nh\u0000f⋆(xt, πt) −bf t(xt, πt)\n\u00012i\n(3.35)\nbetween bf t and f⋆under the randomization distribution pt.\nThe ε-Greedy exploration\ndistribution gives a way to relate these quantities, but the algorithm’s regret is suboptimal\nbecause the randomization distribution puts mass at least ε/A on every action, even those\nthat are clearly suboptimal and should be discarded. One can ask whether there exists a\nbetter randomization strategy that still admits an upper bound on (3.34) in terms of (3.35).\nProposition 9 below establishes exactly that. At first glance, this distribution might appear\nto be somewhat arbitrary or “magical”, but we will show in subsequent chapters that it\narises as a special case of more general—and in some sense, universal—principle for designing\ndecision making algorithms, which extends well beyond contextual bandits.\n49\nDefinition 4 (Inverse Gap Weighting [2, 36]): Given a vector bf = ( bf(1), . . . , bf(A)) ∈\nRA, the Inverse Gap Weighting distribution p = IGWγ( bf(1), . . . , bf(A)) with parameter\nγ ≥0 is defined as\np(π) =\n1\nλ + 2γ( bf(bπ) −bf(π))\n,\n(3.36)\nwhere bπ = arg maxπ bf(π) is the greedy action, and where λ ∈[1, A] is chosen such that\nP\nπ p(π) = 1.\nAbove, the normalizing constant λ ∈[1, A] is always guaranteed to exist, because we have\n1\nλ ≤P\nπ p(π) ≤A\nλ , and because λ 7→P\nπ p(π) is continuous over [1, A].\nLet us give some intuition behind the distribution in (3.36).\nWe can interpret the\nparameter γ as trading off exploration and exploitation. Indeed, γ →0 gives a uniform\ndistribution, while γ →∞amplifies the gap between the greedy action bπ and any action\nwith bf(π) < bf(bπ), resulting in a distribution supported only on actions that achieve the\nlargest estimated value bf(bπ).\nThe following fundamental technical result shows that playing the Inverse Gap Weight-\ning distribution always suffices to link the instantaneous regret in (3.34) in to the instanta-\nneous estimation error in (3.35).\nProposition 9: Consider a finite decision space Π = {1, . . . , A}. For any vector bf ∈RA\nand γ > 0, define p = IGWγ( bf(1), . . . , bf(A)). This strategy guarantees that for all\nf⋆∈RA,\nEπ∼p[f⋆(π⋆) −f⋆(π)] ≤A\nγ + γ · Eπ∼p\n\u0002\n( bf(π) −f⋆(π))2\u0003\n.\n(3.37)\nProof of Proposition 9. We break the “regret” term on the left-hand side of (3.37) into three\nterms:\nEπ∼p\n\u0002\nf⋆(π⋆) −f⋆(π)\n\u0003\n= Eπ∼p\n\u0002 bf(bπ) −bf(π)\n\u0003\n|\n{z\n}\n(I) exploration bias\n+ Eπ∼p\n\u0002 bf(π) −f⋆(π)\n\u0003\n|\n{z\n}\n(II) est. error on policy\n+ f⋆(π⋆) −bf(bπ)\n|\n{z\n}\n(III) est. error at opt\n.\nThe first term asks “how much would we lose by exploring, if bf were the true reward\nfunction?”, and is equal to\nX\nπ\nbf(bπ) −bf(π)\nλ + 2γ\n\u0000 bf(bπ) −bf(π)\n\u0001 ≤A −1\n2γ\n,\nwhile the second term is at most\nq\nEπ∼p\n\u0002\n( bf(π) −f⋆(π))2\u0003\n≤1\n2γ + γ\n2 Eπ∼p( bf(π) −f⋆(π))2.\nThe third term can be further written as\nf⋆(π⋆) −bf(π⋆) −( bf(bπ) −bf(π⋆)) ≤γ\n2p(π⋆)(f⋆(π⋆) −bf(π⋆))2 +\n1\n2γp(π⋆) −( bf(bπ) −bf(π⋆))\n≤γ\n2 Eπ∼p(f⋆(π) −bf(π))2 +\n\u0014\n1\n2γp(π⋆) −( bf(bπ) −bf(π⋆))\n\u0015\n.\n50\nThe term in brackets above is equal to\nλ + 2γ( bf(bπ) −bf(π⋆))\n2γ\n−( bf(bπ) −bf(π⋆)) = λ\n2γ ≤A\n2γ .\nThe simple result we just proved is remarkable. The special IGW strategy guarantees a\nrelation between regret and estimation error for any estimator bf and any f⋆, irrespective of\nthe problem structure or the class F. Proposition 9 will be at the core of the development for\nthe rest of the course, and will be greatly generalized to general decision making problems\nand reinforcement learning.\nBelow, we present a contextual bandit algorithm called SquareCB [36] which makes use\nof the Inverse Gap Weighting distribution.\nSquareCB\nInput: Exploration parameter γ > 0.\nfor t = 1, . . . , T do\nObtain bf t from online regression oracle with (x1, π1, r1), . . . , (xt−1, πt−1, rt−1).\nObserve xt.\nCompute pt = IGWγ\n\u0010\nbf t(xt, 1), . . . , bf t(xt, A)\n\u0011\n.\nSelect action πt ∼pt.\nObserve reward rt.\nAt each step t, the algorithm uses an online regression oracle to compute a reward estimator\nbf t(x, a) based on the data Ht−1 collected so far. Given this estimator, the algorithm uses\nInverse Gap Weighting to compute pt = IGWγ( bf t(xt, ·)) as an exploratory distribution, then\nsamples πt ∼pt.\nThe following result, which is a near-immediate consequence of Proposition 9, gives a\nregret bound for this algorithm.\nProposition 10: Given a class F with f⋆∈F, assume the decision-maker has access\nto an online regression oracle (Definition 3) with estimation error EstSq(F, T, δ). Then\nSquareCB with γ =\np\nTA/EstSq(F, T, δ) attains a regret bound of\nReg ≲\nq\nATEstSq(F, T, δ)\nwith probability at least 1−δ for any sequence x1, . . . , xT. As a special case, when F is\nfinite, the averaged exponential weights algorithm achieves EstSq(F, T, δ) ≲log(|F|/δ),\nleading to\nReg ≲\np\nAT log(|F|/δ).\nProof of Proposition 10. We begin with regret, then add and subtract the squared estima-\ntion error as follows:\nReg =\nT\nX\nt=1\nEπt∼pt[f⋆(xt, π⋆) −f⋆(xt, πt)]\n=\nT\nX\nt=1\nEπt∼pt\nh\nf⋆(xt, π⋆) −f⋆(xt, πt) −γ · (f⋆(xt, πt) −bf t(xt, πt))2i\n+ γ · EstSq(F, T, δ).\n51\nBy appealing to Proposition 9 with bf(xt, ·) and f⋆(xt, ·), for each step t, we have\nEπt∼pt\nh\nf⋆(xt, π⋆) −f⋆(xt, πt) −γ · (f⋆(xt, πt) −bf t(xt, πt))2i\n≤A\nγ ,\nand thus\nReg ≤TA\nγ\n+ γ · EstSq(F, T, δ).\nChoosing γ to balance these terms yields the result.\nIf the online regression oracle is minimax optimal (that is, EstSq(F, T, δ) is the “best\npossible” for F) then SquareCB is also minimax optimal for F. Thus, IGW not only provides\na connection between online supervised learning and decision making, but it does so in an\noptimal fashion. Establishing minimax optimality is beyond the scope of this course: it\nrequires understanding of minimax optimality of online regression with arbitrary F, as well\nas lower bound on regret of contextual bandits with arbitrary sequences of contexts. We\nrefer to Foster and Rakhlin [36] for details.\n3.5.1\nExtending to Offline Regression\nWhen x1, . . . , xT are i.i.d., it is natural to ask whether an online regression method that\nworks for arbitrary sequences is necessary, or whether one can work with a weaker oracle\ntuned to i.i.d.\ndata.\nFor SquareCB, it turns out that any oracle for offline regression\n(defined below) is sufficient.\nDefinition 5 (Offline Regression Oracle): Given\n(x1, π1, r1), . . . , (xt−1, πt−1, rt−1)\nwhere x1, . . . , xt−1 are i.i.d., πi ∼p(xi) for fixed p : X →∆(Π) and E[ri|xi, πi] =\nf⋆(xi, πi), an offline regression oracle returns a function bf : X × Π →R such that\nEx,π∼p(x)( bf(x, π) −f⋆(x, π))2 ≤t−1Estoff\nSq(F, t, δ)\nwith probability at least 1 −δ.\nNote that the normalization t−1 above is introduced to keep the scaling consistent with our\nconventions for offline estimation.\nBelow, we state a variant of SquareCB which is adapted to offline oracles [79]. Compared\nto the SquareCB for online oracles, the main change is that we update the estimation\noracle and exploratory distribution on an epoched schedule as opposed to updating at every\nround. In addition, the parameter γ for the Inverse Gap Weighting distribution changes as\na function of the epoch.\nSquareCB with offline oracles\nInput: Exploration parameters γ1, γ2, . . . and epoch sizes τ1, τ2, . . .\nfor m = 1, 2, . . . do\nObtain bf m from offline regression oracle with\n(xτm−2+1, πτm−2+1, rτm−2+1), . . . , (xτm−1, πτm−1, rτm−1).\n52\nfor t = τm−1 + 1, . . . , τm do\nObserve xt.\nCompute pt = IGWγm\n\u0010\nbf m(xt, 1), . . . , bf m(xt, A)\n\u0011\n.\nSelect action πt ∼pt.\nObserve reward rt.\nWhile this algorithm is quite intuitive, proving a regret bound for it is quite non-trivial—\nmuch more so than the online oracle variant. They key challenge is that, while the con-\ntexts x1, . . . , xT are i.i.d., the decisions π1, . . . , πT evolve in a time-dependent fashion, which\nmakes it unclear to invoke the guarantee in Definition 5. Nonetheless, the following remark-\nable result shows that this algorithm attains a regret bound similar to that of Proposition\n10.\nProposition 11 (Simchi-Levi and Xu [79]): Let τm = 2m and γm =\nq\nAT/Estoff\nSq(F, τm−1, δ)\nfor m = 1, 2, . . .. Then with probability at least 1−δ, regret of SquareCB with an offline\noracle is at most\nReg ≲\n⌈log T⌉\nX\nm=1\nq\nA · τm · Estoff\nSq(F, τm, δ/m2).\nUnder mild assumptions, above bound scales as\nReg ≲\nq\nA · T · Estoff\nSq(F, τm, δ/ log T).\nFor a finite class F, we recall from Section 1 that empirical risk with the square loss (least\nsquares) achieves Estoff\nSq(F, T, δ) ≲log(|F|/δ), which gives\nReg ≲\np\nAT log(|F|/δ).\n3.6 Exercises\nExercise 5 (Unstructured Contextual Bandits): Consider a contextual bandit problem\nwith a finite set X of possible contexts, and a finite set of actions A. Show that running UCB\nindependently for each context yields a regret bound of the order eO(\np\n|X||A|T) in expectation,\nignoring logarithmic factors. In the setting where F = X × A →[0, 1] is unstructured, and\nconsists of all possible functions, this is essentially optimal.\nExercise 6 (ε-Greedy with Offline Oracles): In Proposition 8, we analyzed the ε-Greedy\ncontextual bandit algorithm assuming access to an online regression oracle. Because we appeal\nto online learning, this algorithm was able to handle adversarial contexts x1, . . . , xT. In the\npresent problem, we will modify the ε-greedy algorithm and proof to show that if contexts are\nstochastic (that is xt ∼D ∀t, where D is a fixed distribution), ε-greedy works even if we use\nan offline oracle (Definition 5).\nWe consider the following variant of ε-greedy.\nThe algorithm proceeds in epochs m =\n0, 1, . . . of doubling size\n{2}, {3, 4}, {5 . . . 8}, . . . , {2m + 1, 2m+1}\n|\n{z\n}\nepoch m\n, . . . , {T/2 + 1, T};\n53\nwe assume without loss of generality that T is a power of 2, and that an arbitrary decision is\nmade on round t = 1. At the end of each epoch m −1, the offline oracle is invoked with the\ndata from the epoch, producing an estimated model bf m. This model is used for the greedy\nstep in the next epoch m. In other words, for any round t ∈[2m + 1, 2m+1] of epoch m, the\nalgorithm observes xt ∼D, chooses an action πt ∼unif[A] with probability ε and chooses the\ngreedy action\nπt = arg max\nπ∈[A]\nbf\nm(x\nt, π)\nwith probability 1 −ε. Subsequently, the reward rt is observed.\n1. Prove that for any T ∈N and δ > 0, by setting ε appropriately, this method ensures that\nwith probability at least 1 −δ,\nReg ≲A1/3T 1/3\n\n\nlog2 T\nX\nm=1\n2m/2Estoff\nSq(F, 2m−1, δ/m2)1/2\n\n\n2/3\n2. Recall that for a finite class, ERM achieves Estoff\nSq(F, T, δ) ≲log(|F|/δ). Show that with\nthis choice, the above upper bound matches that in Proposition 8, up to logarithmic in T\nfactors.\nExercise 7 (Model Misspecification in Contextual Bandits): In Proposition 10, we\nshowed that for contextual bandits with a general class F, SquareCB attains regret\nReg ≲\nq\nAT · EstSq(F, T, δ).\n(3.38)\nTo do so, we assumed that f ⋆∈F, where f ⋆(x, a) := Er∼M ⋆(·|x,a)[r]; that is, we have a well-\nspecified model. In practice, it may be unreasonable to assume that we have f ⋆∈F. Instead,\na weaker assumption is that there exists some function ¯f ∈F such that\nmax\nx∈X,a∈A| ¯f(x, a) −f ⋆(x, a)| ≤ε\nfor some ε > 0; that is, the model is ε-misspecified. In this problem, we will generalize the\nregret bound for SquareCB to handle misspecification. Recall that in the lecture notes, we\nassumed (Definition 3) that the regression oracle satisfies\nT\nX\nt=1\nEπt∼pt\u0002\n( bf\nt(x\nt, π\nt) −f ⋆(x\nt, π\nt))2\u0003\n≤EstSq(F, T, δ).\nIn the misspecified setting, this is too much to ask for. Instead, we will assume that the oracle\nsatisfies the following guarantee for every sequence:\nT\nX\nt=1\n( bf\nt(x\nt, π\nt) −r\nt)2 −min\nf∈F\nT\nX\nt=1\n(f(x\nt, π\nt) −r\nt)2 ≤RegSq(F, T).\nWhenever f ⋆∈F, we have EstSq(F, T, δ) ≲RegSq(F, T) + log(1/δ) with probability at least\n1 −δ. However, it is possible to keep RegSq(F, T) small even when f ⋆/∈F. For example, the\naveraged exponential weights algorithm satisfies this guarantee with RegSq(F, T) ≲log|F|,\nregardless of whether f ⋆∈F.\n54\nWe will show that for every δ > 0, with an appropriate choice of γ, SquareCB (that is, the\nalgorithm that chooses pt = IGWγ( bf t(xt, ·))) ensures that with probability at least 1 −δ,\nReg ≲\nq\nAT · (RegSq(F, T) + log(1/δ)) + ε · A1/2T.\nAssume that all functions in F and rewards take values in [0, 1].\n1. Show that for any sequence of estimators bf 1, . . . , bf t, by choosing pt = IGWγ( bf t(xt, ·)), we\nhave that\nReg =\nT\nX\nt=1\nEπt∼pt[f ⋆(x\nt, π⋆(x\nt)) −f ⋆(x\nt, π\nt)] ≲AT\nγ +γ\nT\nX\nt=1\nEπt∼pt\nh\n( bf\nt(x\nt, π\nt) −¯f(x\nt, π\nt))2i\n+εT.\nIf we had f ⋆= ¯f, this would follow from Proposition 9, but the difference is that in general\n( ¯f ̸= f ⋆), the expression above measures estimation error with respect to the best-in-class\nmodel ¯f rather than the true model f ⋆(at the cost of an extra εT factor).\n2. Show that the following inequality holds for every sequence\nT\nX\nt=1\n( bf\nt(x\nt, π\nt) −¯f(x\nt, π\nt))2 ≤RegSq(F, T) + 2\nT\nX\nt=1\n(r\nt −¯f(x\nt, π\nt))( bf\nt(x\nt, π\nt) −¯f(x\nt, π\nt)).\n3. Using Freedman’s inequality (Lemma 35), show that with probability at least 1 −δ,\nT\nX\nt=1\nEπt∼pt\nh\n( bf\nt(x\nt, π\nt) −¯f(x\nt, π\nt))2i\n≤2\nT\nX\nt=1\n( bf\nt(x\nt, π\nt) −¯f(x\nt, π\nt))2 + O(log(1/δ)).\n4. Using Freedman’s inequality once more, show that with probability at least 1 −δ,\n2\nT\nX\nt=1\n(r\nt−¯f(x\nt, π\nt))( bf\nt(x\nt, π\nt)−¯f(x\nt, π\nt)) ≤1\n4\nT\nX\nt=1\nEπt∼pt\nh\n( bf\nt(x\nt, π\nt) −¯f(x\nt, π\nt))2i\n+O(ε2T+log(1/δ)).\nConclude that with probability at least 1 −δ,\nT\nX\nt=1\nEπt∼pt\nh\n( bf\nt(x\nt, π\nt) −¯f(x\nt, π\nt))2i\n≲RegSq(F, T) + ε2T + log(1/δ).\n5. Combining the previous results, show that for any δ > 0, by choosing γ > 0 appropriately,\nwe have that with probability at least 1 −δ,\nReg ≲\nq\nAT · (RegSq(F, T) + log(1/δ)) + ε · A1/2T.\n4. STRUCTURED BANDITS\nUp to this point, we have focused our attention on bandit problems (with or without\ncontexts) in which the decision space Π is a small, finite set.\nThis section introduces\nthe structured bandit problem, which generalizes the basic (non-contextual) multi-armed\nbandit problem by allowing for large, potentially infinite or continuous decision spaces.\nThe protocol for the setting is as follows.\n55\nStructured Bandit Protocol\nfor t = 1, . . . , T do\nSelect decision πt ∈Π.\n// Π is large and potentially continuous.\nObserve reward rt ∈R.\nThis protocol is exactly the same as for multi-armed bandits (Section 2), except that we\nhave removed the restriction that Π = {1, . . . , A}, and now allow it to be arbitrary. This\nadded generality is natural in many applications:\n• In medicine, the treatment may be a continuous variable, such as a dosage.\nThe\ntreatment could even by a high-dimensional vector (such as dosages for many different\nmedications). See Figure 7.\n• In pricing applications, a seller might aim to select a continuous price or vector or\nprices in order to maximize their returns.\n• In routing applications, the decision space may be finite, but combinatorially large.\nFor example, the decision might be a path or flow in a graph.\nBoth contextual bandits and structured bandits generalize the basic multi-armed bandit\nproblem, by incorporating function approximation and generalization, but in different ways:\n• The contextual bandit formulation in Section 3 assumes structure in the context space.\nThe aim here was to generalize across contexts, but we restricted the decision space\nto be finite (unstructured).\n• In structured bandits, we will focus our attention on the case of no contexts, but will\nassume the decision space is structured, and aim to generalize across decisions.\nClearly, both ideas above can be combined, and we will touch on this in Section 4.5.\ncontext\ndecision\nreward\nobservation\nxt\n⇡t\nrt\not\nFigure 7: An illustration of the structured bandit problem.\nA doctor aims to select a\ncontinuous, high-dimensional treatment.\nAssumptions and regret.\nTo build intuition as to what it means to generalize across\ndecisions, and to give a sense for what sort of guarantees we might hope to prove, let us\nfirst give the formal setup for the structured bandit problem. As in preceding sections, we\nwill assume that rewards are stochastic, and generated from a fixed model.\n56\nAssumption 4 (Stochastic Rewards): Rewards are generated independently via\nrt ∼M⋆(· | πt),\n(4.1)\nwhere M⋆(· | ·) is the underlying model.\nWe define\nf⋆(π) := E [r | π]\n(4.2)\nas the mean reward function under r ∼M⋆(· | π), and measure regret via\nReg :=\nT\nX\nt=1\nf⋆(π⋆) −\nT\nX\nt=1\nEπt∼pt[f⋆(πt)].\n(4.3)\nHere, π⋆:= arg maxπ∈Π f⋆(π) as usual. We will define the history as Ht = (π1, r1), . . . , (rt, πt).\nFunction approximation.\nA first attempt to tackle the structured bandit problem might\nbe to apply algorithms for the multi-armed bandit setting, such as UCB. This would give\nregret eO(\np\n|Π|T), which could be vacuous if Π is large relative to T. However, with no\nfurther assumptions on the underlying reward function f⋆, this is unavoidable. To allow for\nbetter regret, we will make assumptions on the structure of f⋆that will allow us to share\ninformation across decisions, and to generalize to decisions that we may not have played.\nThis is well-suited for the applications described above, where Π is a continuous set (e.g.,\nΠ ⊆Rd), but we expect f⋆to be continuous, or perhaps even linear with respect some\nwell-designed set of features. To make this idea precise, we follow the same approach as in\nstatistical learning and contextual bandits, and assume access to a well-specified function\nclass F that aims to capture our prior knowledge about f⋆.\nAssumption 5: The decision-maker has access to a class F ⊂{f : Π →R} such that\nf⋆∈F.\nGiven such a class, a reasonable goal—particularly in light of the development in Section 1\nand Section 3—would be to achieve guarantees that scale with the complexity of supervised\nlearning or estimation with F, e.g. log|F| for finite classes; this is what we were able to\nachieve for contextual bandits, after all. Unfortunately, this is too good to be true, as the\nfollowing example shows.\nExample 4.1 (Necessity of structural assumptions). Let Π = [A], and let F = {fi}i∈[A],\nwhere\nfi(π) := 1\n2 + 1\n2I {π = i} .\nIt is clear that one needs Reg ≳A for this setting, yet log|F| = log(A), so a regret bound\nof the form Reg ≲\np\nT log|F| is not possible if A is large relative to T.\n◁\nWhat this example highlights is that generalizing across decisions is fundamentally dif-\nferent (and, in some sense, more challenging) than generalizing across contexts. In light\n57\nof this, we will aim for guarantees that scale with log|F|, but additionally scale with an\nappropriate notion of complexity of exploration for the decision space Π. Such a notion of\ncomplexity should reflect how much information is shared across decisions, which depends\non the interplay between Π and F.\n4.1 Building Intuition: Optimism for Structured Bandits\nOur goal is to obtain regret bounds for structured bandits that reflect the intrinsic difficulty\nof exploring the decision space Π, which should reflect the structure of the function class F\nunder consideration. To build intuition as to what such guarantees will look like, and how\nthey can be obtained, we first investigate the behavior of the optimism principle and the\nUCB algorithm when applied to structured bandits. We will see that:\n1. UCB attains guarantees that scale with log|F|, and additionally scale with a notion\nof complexity called the eluder dimension, which is small for simple problems such as\nbandits with linear rewards.\n2. In general, UCB is not optimal, and can have regret that is exponentially large com-\npared to the optimal rate.\n4.1.1\nUCB for Structured Bandits\nWe can adapt the UCB algorithm from multi-armed bandits to structured bandit by ap-\npealing to least squares and confidence sets, similar to the approach we took for contextual\nbandits [74]. Assume F = {f : Π →[0, 1]} and rt ∈[0, 1] almost surely. Let\nbf t = arg min\nf∈F\nt−1\nX\ni=1\n(f(πi) −ri)2\n(4.4)\nbe the empirical minimizer on round t, and with β := 8 log(|F|/δ), define confidence sets\nF 1 = F and\nF t =\n(\nf ∈F :\nt−1\nX\ni=1\n(f(πi) −ri)2 ≤\nt−1\nX\ni=1\n( bf t(πi) −ri)2 + β\n)\n.\n(4.5)\nDefining ¯f t(π) := maxf∈Ft f(π) as the upper confidence bound, the generalized UCB algo-\nrithm is given by\nπt = arg max\nπ∈Π\n¯f t(π).\n(4.6)\nWhen does the confidence width shrink?\nUsing Proposition 7, one can see the gen-\neralized UCB algorithm ensures that f⋆∈F t for all t with high probability. Whenever this\nhappens, regret is bounded by the upper confidence width:\nReg ≤\nT\nX\nt=1\n¯f t(πt) −f⋆(πt).\n(4.7)\nThis bound holds for all structured bandit problems, with no assumption on the structure\nof Π and F. Hence, to derive a regret bound, the only question we need to answer is when\nwill the confidence widths shrink?\n58\nFor the unstructured multi-armed bandit, we need to shrink the width for every arm\nseparately, and the best bound on (4.7) we can hope for is O(\np\n|Π|T). One might hope\nthat if Π and F have nice structure, we can do better. In fact, we have already seen one\nsuch case: For linear models, where\nF =\nn\nπ 7→⟨θ, ϕ(π)⟩| θ ∈Θ ⊂Bd\n2(1)\no\n,\n(4.8)\nProposition 7 shows that we can bound (4.7) by\np\ndT log|F|. Here, the number of decisions\n|Π| is replaced by the dimension d, which reflects the fact that there are only d truly unique\ndirections to explore before we can start extrapolating to new actions.\nIs there a more\ngeneral version of this phenomenon when we move beyond linear models?\n4.1.2\nThe Eluder Dimension\nThe eluder dimension [74] is a complexity measure that aims to capture the extent to which\nthe function class F facilitates extrapolation (i.e., generalization to unseen decisions), and\ngives a generic way of bounding the confidence width in (4.7). It is defined for a class F as\nfollows.\nDefinition 6 (Eluder Dimension): Let F ⊂(Π →R) and f⋆: Π →R be given, and\ndefine Edimf⋆(F, ε) as the length of the longest sequence of decisions π1, . . . , πd ∈Π\nsuch that for all t ∈[d], there exists f t ∈F such that\n|f t(πt) −f⋆(πt)| > ε,\nand\nX\ni<t\n(f t(πi) −f⋆(πi))2 ≤ε2.\n(4.9)\nThe eluder dimension is defined as Edimf⋆(F, ε) = supε′≥ε Edimf⋆(F, ε′) ∨1. We ab-\nbreviate Edim(F, ε) = maxf⋆∈F Edimf⋆(F, ε).\nThe intuition behind the eluder dimension is simple: It asks, for a worst-case sequence of\ndecisions, how many times we can be “surprised” by a new decision πt if we can estimate\nthe underlying model f⋆well on all of the preceding points.\nIn particular, if we form\nconfidence sets as in (4.5) with β = ε2, then the number of times the upper confidence\nwidth in (4.7) can be larger than ε is at most Edimf⋆(F, ε). We consider the definition\nEdimf⋆(F, ε) = supε′≥ε Edimf⋆(F, ε′) ∨1 instead of directly working with Edimf⋆(F, ε) to\nensure monotonicity with respect to ε, which will be useful in the proofs that follow.\nThe following result gives a regret bound for UCB for generic structured bandit prob-\nlems. The regret bound has no dependence on the size of the decision space, and scales\nonly with Edim(F, ε) and log|F|.\nProposition 12: For a finite set of functions F ⊂(Π →[0, 1]), using β = 8 log(|F|/δ),\nthe generalized UCB algorithm guarantees that with probability at least 1 −δ,\nReg ≲min\nε>0\nnp\nEdim(F, ε) · T log(|F|/δ) + εT\no\n≲\nq\nEdim(F, T −1/2) · T log(|F|/δ).\n(4.10)\n59\nFor the case of linear models in (4.8), it is possible to use the elliptic potential lemma\n(Lemma 11) to show that\nEdim(F, ε) ≲d log(ε−1).\nFor finite classes, this gives Reg ≲\np\ndT log(|F|/δ) log(T), which recovers the guarantee in\nProposition 7. Another well-known example is that of generalized linear models. Here, we\nfix link function σ : [−1, +1] →R and define\nF =\nn\nπ 7→σ\n\u0000⟨θ, ϕ(π)⟩\n\u0001\n| θ ∈Θ ⊂Bd\n2(1)\no\n.\nThis is a more flexible model than linear bandits. A well-known special case is the logistic\nbandit problem, where σ(z) = 1/(1 + e−z). One can show [74] that for any choice of σ, if\nthere exist µ, L > 0 such that µ < σ′(z) < L for all z ∈[−1, +1], then\nEdim(F, ε) ≲L2\nµ2 · d log(ε−1).\n(4.11)\nThis leads to a regret bound that scales with L\nµ\np\ndT log|F|, generalizing the regret bound\nfor linear bandits.\nIn general, the eluder dimension can be quite large. Consider the generalized linear\nmodel setup above with σ(z) = +relu(z) or σ(z) = −relu(z) (either choice of sign works),\nwhere relu(z) := max{z, 0} is the ReLU function; this can be interpreted as a neural network\nwith a single neuron. Here, we can have σ′(z) = 0, so (4.11) does not apply, and it turns\nout [61] that\nEdim(F, ε) ≳ed\n(4.12)\nfor constant ε. That is, even for a single ReLU neuron, the eluder dimension is already\nexponential, which is a bit disappointing. Fortunately, we will show in the sequel that the\neluder dimension can be overly pessimistic, and it is possible to do better, but this will\nrequire changing the algorithm.\nProof of Proposition 12. Define\nF t =\n(\nf ∈F |\nX\ni<t\n(f(πi) −f⋆(πi))2 ≤4β\n)\n.\nBy Lemma 10, we have that with probability at least 1 −δ, for all t:\n1. f⋆∈F t.\n2. F t ⊆F t.\nLet us condition on this event. As in Lemma 7 , since f⋆∈F t, we can upper bound\nReg ≤\nT\nX\nt=1\n¯f t(πt) −f⋆(πt).\nNow, define\nwt(π) = sup\nf∈Ft\n[f(π) −f⋆(π)],\n60\nwhich is a useful upper bound on the upper confidence width at time t. Since F t ⊆F t, we\nhave\nReg ≤\nT\nX\nt=1\nwt(πt).\nWe now appeal to the following technical lemma concerning the eluder dimension.\nLemma 12 (Russo and Van Roy [74], Lemma 3): Fix a function class F, function\nf⋆∈F, and parameter β > 0. For any sequence π1, . . . , πT, if we define\nwt(π) = sup\nf∈F\n(\nf(π) −f⋆(π) :\nX\ni<t\n(f(πi) −f⋆(πi))2 ≤β\n)\n,\nthen for all α > 0,\nT\nX\nt=1\nI {wt(πt) > α} ≤\n\u0012 β\nα2 + 1\n\u0013\n· Edimf⋆(F, α).\nNote that for the special case where β = α2, the bound in Lemma 12 immediately\nfollows from the definition of the eluder dimension. The point of this lemma is to show that\na similar bound holds for all scales α simultaneously, but with a pre-factor\nβ\nα2 that grows\nlarge when α2 ≪β.\nTo apply this result, fix ε > 0, and bound\nT\nX\nt=1\nwt(πt) ≤\nT\nX\nt=1\nwt(πt)I {wt(πt) > ε} + εT.\n(4.13)\nLet us order the indices {1, . . . , T} as {i1, . . . , iT }, so that wi1(πi1) ≥wi2(πi2) ≥. . . ≥\nwiτ (πiτ ). Consider any index τ for which wiτ (πiτ ) > ε. For any α > ε, if we have wiτ (πiτ ) >\nα, then Lemma 12 (since α ≤1 ≤β) implies that\nτ ≤\nT\nX\nt=1\nI {wt(πt) > α} ≤\n\u00124β\nα2 + 1\n\u0013\nEdimf⋆(F, α) ≤5β\nα2 Edimf⋆(F, α).\n(4.14)\nSince we have restricted to α ≥ε and α 7→Edimf⋆(F, α) is decreasing, rearranging yields\nwiτ (πiτ ) ≤\nr\n5βEdim(F, ε)\nτ\n.\nWith this, we can bound the main term in (4.13) by\nT\nX\nt=1\nwt(πt)I {wt(πt) > ε} ≲\nT\nX\nt=1\nr\nβEdim(F, ε)\nt\n≲\np\nβEdim(F, ε)T.\nCombining this with (4.13) gives Reg ≲\np\nβEdim(F, ε)T + εT. Since ε > 0 was arbitrary,\nwe are free to minimize over it.\n61\nProof of Lemma 12. Let us adopt the shorthand d = Edimf⋆(F, α).\nWe begin with a\ndefinition.\nWe say π is α-independent of π1, . . . , πt if there exists f ∈F such that\n|f(π) −f⋆(π)| > α and Pt\ni=1(f(πi) −f⋆(πi))2 ≤α2. We say π is α-dependent on π1, . . . , πt\nif for all f ∈F with Pt\ni=1(f(πi) −f⋆(πi))2 ≤α2, |f(π) −f⋆(π)| ≤α.\nWe first claim that for any t, if wt(πt) > α, then πt is α-dependent on at most β/α2\ndisjoint subsequences of π1, . . . , πt−1. Indeed, let f be such that |f(πt) −f⋆(πt)| > α. If πt\nis α-dependent on a particular subsequence πi1, . . . , πik but wt(πt) > α, we must have\nk\nX\nj=1\n(f(πij) −f⋆(πij))2 ≥α2.\nIf there are M such disjoint sequences, we have\nMα2 ≤\nX\ni<t\n(f(πi) −f⋆(πi))2 ≤β,\nso M ≤\nβ\nα2 .\nNext, we claim that for τ and any sequence (π1, . . . , πτ), there is some j such that πj\nis α-dependent on at least ⌊τ/d⌋disjoint subsequences of π1, . . . , πj−1. Let N = ⌊τ/d⌋,\nand let B1, . . . , BN be subsequences of π1, . . . , πτ. We initialize with Bi = (πi). If πN+1 is\nα-dependent on Bi = (πi) for all 1 ≤i ≤N we are done. Otherwise, choose i such that\nπN+1 is α-independent of Bi, and add it to Bi. Repeat this process until we reach j such\nthat either πj is α-dependent on all Bi or j = τ. In the first case we are done, while in\nthe second case, we have PN\ni=1|Bi| ≥τ ≥dN. Moreover, |Bi| ≤d, since each πj ∈Bi\nis α-independent of its prefix (this follows from the definition of eluder dimension). We\nconclude that |Bi| = d for all i, so in this case πτ is α-dependent on all Bi.\nFinally, let (πt1, . . . , πtτ ) be the subsequence π1, . . . , πT consisting of all elements for\nwhich wii(πti) > α. Each element of the sequence is dependent on at most β/α2 disjoint\nsubsequences of (πt1, . . . , πtτ ), and by the argument above, one element is dependent on at\nleast ⌊τ/d⌋disjoint subsequences, so we must have ⌊τ/d⌋≤β/α2, and which implies that\nτ ≤(β/α2 + 1)d.\n4.1.3\nSuboptimality of Optimism\nThe following example shows a function class F for which the regret experienced by UCB\nis exponentially large compared to the regret obtained by a simple alternative algorithm.\nThis shows that while the algorithm is useful for some special cases, it does not provide a\ngeneral principle that attains optimal regret for any structured bandit problem.\nExample 4.2 (Cheating Code [9, 50]). Let A ∈N be a power of 2 and consider the following\nfunction class F.\n• The decision space is Π = [A] ∪C, where C = {c1, . . . , clog2(A)} is a set of “cheating”\nactions.\n• For all actions π ∈[A], f(π) ∈[0, 1] for all f ∈F, but we otherwise make no\nassumption on the reward.\n• For each f ∈F, rewards for actions in C take the following form. Let πf ∈[A] denote\nthe action in [A] with highest reward. Let b(f) = (b1(f), . . . , blog2(A)(f)) ∈{0, 1}log2(A)\n62\nbe a binary encoding for the index of πf ∈[A] (e.g., if πf = 1, b(f) = (0, 0, . . . , 0), if\nπf = 2, b(f) = (0, 0, . . . , 0, 1), and so on). For each action ci ∈C, we set\nf(ci) = −bi(f).\nThe idea here is that if we ignore the actions C, this looks like a standard multi-armed\nbandit problem, and the optimal regret is Θ(\n√\nAT). However, we can use the actions in C\nto “cheat” and get an exponential improvement in sample complexity. The argument is as\nfollows.\nSuppose for simplicity that rewards are Gaussian with r ∼N(f⋆(π), 1) under π. For\neach cheating action ci ∈C, since f⋆(ci) = −bi(f⋆) ∈{0, −1}, we can determine whether the\nvalue is bi(f⋆) = 0 or bi(f⋆) = 1 with high probability using eO(1) action pulls. If we do this\nfor each ci ∈C, which will incur eO(log(A)) regret (there are log(A) such actions and each one\nleads to constant regret), we can infer the binary encoding b(f⋆) = b1(f⋆), . . . , blog2(A)(f⋆)\nfor the optimal action πf⋆with high probability. At this point, we can simply stop exploring,\nand commit to playing πf⋆for the remaining rounds, which will incur no more regret. If\none is careful with the details, this gives that with probability at least 1 −δ,\nReg ≲log2(A/δ).\nIn other words, by exploiting the cheating actions, our regret has gone from linear to\nlogarithmic in A (we have also improved the dependence on T, which is a secondary bonus).\nNow, let us consider the behavior of the generalized UCB algorithm. Unfortunately,\nsince all actions ci ∈C have f(ci) ≤0 for all f ∈F, we have ¯f t(ci) ≤0. As a result, the\ngeneralized UCB algorithm will only ever pull actions in [A], ignoring the cheating actions\nand effectively turning this into a vanilla multi-armed bandit problem, which means that\nReg ≳\n√\nAT.\n◁\nThis example shows that UCB can behave suboptimally in the presence of decisions that\nreveal useful information but do not necessarily lead to high reward. Since the “cheating”\nactions are guaranteed to have low reward, UCB avoids them even though they are very\ninformative. We conclude that:\n1. Obtaining optimal sample complexity for structured bandits requires algorithms that\nmore deliberately balance the tradeoff between optimizing reward and acquiring in-\nformation.\n2. In general, the optimal strategy for picking decisions can be very different depending\non the choice of the class F. This contrasts the contextual bandit setting, where we\nsaw that the Inverse Gap Weighting algorithm attained optimal sample complexity for\nany choice of class F, and all that needed to change was how to perform estimation.\nRemark 13 (Suboptimality of posterior sampling): Recall the Bayesian bandit\nsetting in Section 2.4, where we showed that the posterior sampling algorithm attains\nregret eO(\n√\nAT) when Π = {1, . . . , A}. Posterior sampling is a general-purpose algo-\nrithm, and can be applied to directly to arbitrary structured bandit problems (as long\nas a prior is available). However, similar to UCB, the cheating code construction in\n63\nExample 4.2 implies that posterior sampling is not optimal in general. Indeed, poste-\nrior sampling will never select the cheating arms in C, as these have sub-optimal reward\nfor all models in F. As a result, the Bayesian regret of the algorithm will scale with\nReg ≳\n√\nAT for a worst-case prior.\n4.2 The Decision-Estimation Coefficient\nThe discussion in the prequel highlights two challenges in designing algorithms and under-\nstanding sample complexity for structured bandits: 1) the optimal regret (in a sense, the\ncomplexity of exploration) can depend on the class F in a subtle, sometimes surprising\nfashion, and 2) the algorithms required to achieve optimal regret can heavily depend on the\nchoice of F. In light of these challenges, it is natural to ask whether it is possible to have\nany sort of unified understanding of the optimal regret. We will now show that the answer\nis yes, and this will be achieved by a single, general-purpose principle for algorithm design.\nThe algorithm we will present in this section reduces the problem of decision making to\nthat of supervised online learning/estimation, in a similar fashion to the SquareCB method\nfor contextual bandits in Section 3. To apply this method, we require the following oracle\nfor supervised estimation.\nDefinition 7 (Online Regression Oracle): At each time t ∈[T], an online regression\noracle returns, given\n(π1, r1), . . . , (πt−1, rt−1)\nwith E[ri|πi] = f⋆(πi) and πi ∼pi, a function bf t : Π →R such that\nT\nX\nt=1\nEπt∼pt( bf t(πt) −f⋆(πt))2 ≤EstSq(F, T, δ)\nwith probability at least 1 −δ. Here, pi(·|Hi−1) is the randomization distribution for\nthe decision-maker.\nRecall, following the discussion in Section 3, that the averaged exponential weights algorithm\nachieves is an online regression oracle with EstSq(F, t, δ) ≲log(|F|/δ).\nThe following algorithm, which we call Estimation-to-Decisions or E2D [40, 43], is a\ngeneral-purpose meta-algorithm for structured bandits.\nEstimation-to-Decisions (E2D) for Structured Bandits\nInput: Exploration parameter γ > 0.\nfor t = 1, . . . , T do\nObtain bf t from online regression oracle with (π1, r1), . . . , (πt−1, rt−1).\nCompute\npt = arg min\np∈∆(Π)\nmax\nf∈F Eπ∼p\n\u0014\nf(πf) −f(π) −γ · (f(π) −bf t(π))2\n\u0015\n.\nSelect action πt ∼pt.\n64\nAt each timestep t, the algorithm calls invokes an online regression oracle to obtain an esti-\nmator bf t using the data Ht−1 = (π1, r1, . . . , πt−1, rt−1) observed so far. The algorithm then\nfinds a distribution pt by solving a min-max optimization problem involving the estimator\nbf t and the class F, then samples the decision πt from this distribution.\nThe minimax problem in E2D is derived from a complexity measure (or, structural\nparameter) for F called the Decision-Estimation Coefficient [40, 43], whose value is given\nby\ndecγ(F, bf) =\nmin\np∈∆(Π) max\nf∈F Eπ∼p\n\u0014\nf(πf) −f(π)\n|\n{z\n}\nregret of decision\n−γ ·(f(π) −bf(π))2\n|\n{z\n}\ninformation gain for obs.\n\u0015\n.\n(4.15)\nThe Decision-Estimation Coefficient can be thought of as the value of a game in which\nthe learner (represented by the min player) aims to find a distribution over decisions such\nthat for a worst-case problem instance (represented by the max player), the regret of their\ndecision is controlled by a notion of information gain (or, estimation error) relative to a\nreference model bf. Conceptually, bf should be thought of as a guess for the true model,\nand the learner (the min player) aims to—in the face of an unknown environment (the max\nplayer)—optimally balance the regret of their decision with the amount information they\nacquire. With enough information, the learner can confirm or rule out their guess bf, and\nscale parameter γ controls how much regret they are willing to incur to do this. In general,\nthe larger the value of decγ(F, bf), the more difficult it is to explore.\nTo state a regret bound for E2D, we define\ndecγ(F) =\nsup\nbf∈co(F)\ndecγ(F, bf).\n(4.16)\nHere, co(F) denotes the set of all convex combinations of elements in F. The reason we\nconsider the set co(F) is that in general, online estimation algorithms such as exponential\nweights will produce improper predictions with bf ∈co(F). In fact, it turns out (see Propo-\nsition 24) that even if we allow bf to be unconstrained above, the maximizer always lies in\nco(F) without loss of generality.\nThe main result for this section shows that the regret for E2D is controlled by the value\nof the DEC and the estimation error EstSq(F, T, δ) for the online regression oracle.\nProposition 13 (Foster et al. [40]): The E2D algorithm with exploration parameter\nγ > 0 guarantees that with probability at least 1 −δ,\nReg ≤decγ(F) · T + γ · EstSq(F, T, δ).\n(4.17)\nWe can optimize over the parameter γ in the result above, which yields\nReg ≤inf\nγ>0\n\u001a\ndecγ(F) · T + γ · EstSq(F, T, δ)\n\u001b\n≤2 · inf\nγ>0 max\n\u001a\ndecγ(F) · T, γ · EstSq(F, T, δ)\n\u001b\n.\nFor finite classes, we can use the exponential weights method to obtain EstSq(F, T, δ) ≲\nlog(|F|/δ), and this bound specializes to\nReg ≲inf\nγ>0 max\n\u001a\ndecγ(F) · T, γ · log(|F|/δ)\n\u001b\n.\n(4.18)\n65\nAs desired, this gives a bound on regret that scales only with:\n1. the complexity log|F| for estimation.\n2. the complexity of exploration in the decision space, which is captured by decγ(F).\nBefore interpreting the result further, we give the proof, which is a nearly immediate con-\nsequence of the definition of the DEC, and bears strong similarity to the proof of the regret\nbound for SquareCB (Proposition 10), minus contexts.\nProof of Proposition 13. We write\nReg =\nT\nX\nt=1\nEπt∼pt[f⋆(π⋆) −f⋆(πt)]\n=\nT\nX\nt=1\nEπt∼pt[f⋆(π⋆) −f⋆(πt)] −γ · Eπt∼pt\nh\n(f⋆(πt) −bf t(πt))2i\n+ γ · EstSq(F, T, δ).\nFor each t, since f⋆∈F, we have\nEπt∼pt[f⋆(π⋆) −f⋆(πt)] −γ · Eπt∼pt\nh\n(f⋆(πt) −bf t(πt))2i\n≤sup\nf∈F\nn\nEπt∼pt[f(πf) −f(πt)] −γ · Eπt∼pt\nh\n(f(πt) −bf t(πt))2io\n=\ninf\np∈∆(Π) sup\nf∈F\nEπ∼p\nh\nf(πf) −f(π) −γ · (f(πt) −bf t(πt))2i\n= decγ(F, bf t),\n(4.19)\nwhere the first equality above uses that pt is chosen as the minimizer for decγ(F, bf t). Sum-\nming across rounds, we conclude that\nReg ≤sup\nbf\ndecγ(F, bf) · T + γ · EstSq(F, T, δ).\nWhen designing algorithms for structured bandits, a common challenge is that the\nconnection between decision making (where the learner’s decisions influence what feedback\nis collected) and estimation (where data is collected passively) may not seem apparent a-\npriori. The power of the Decision-Estimation Coefficient is that it—by definition—provides\na bridge, which the proof of Proposition 13 highlights. One can select decisions by building\nan estimate for the model using all of the observations collected so far, then sampling\nfrom the distribution p that solves (4.15) with the estimated reward function bf plugged\nin. Boundedness of the DEC implies that at every round, any learner using this strategy\neither enjoys small regret or acquires information, with their total regret controlled by the\ncumulative online estimation error.\n66\nExample: Multi-Armed Bandit.\nOf course, the perspective above is only useful if the\nDEC is indeed bounded, which itself is not immediately apparent. In Section 6, we will show\nthat boundedness of the DEC is not just sufficient, but in fact necessary for low regret in\na fairly strong quantitative sense. For now, we will build intuition about the DEC through\nexamples. We begin with the multi-armed bandit, where Π = [A] and F = RA. Our first\nresult shows that decγ(F) ≤A\nγ , and that this is achieved with the Inverse Gap Weighting\nmethod introduced in Section 3.\nProposition 14 (IGW minimizes the DEC): For the Multi-Armed Bandit setting,\nwhere Π = [A] and F = RA, the Inverse Gap Weighting distribution p = IGW4γ( bf) in\n(3.36) is the exact minimizer for decγ(F, bf), and certifies that decγ(F, bf) = A−1\n4γ .\nBy rewriting Proposition 9, it is straightforward to deduce that the DEC is bounded by\nA\nγ , but Proposition 14 shows that IGW is actually the best possible distribution for this\nminimax problem. In this sense, the SquareCB algorithm can be seen as a (contextual)\nspecial case of the Estimation-to-Decisions principle. Note that to attain the exact optimal\nvalue (instead of a bound that is optimal up to constants), we use IGW4γ as opposed IGWγ\nas in Proposition 9; the reason why this choice for γ is optimal is related to the fact that\nthe inequality xy ≤x2 + 1\n4y2 is tight in general.\nProof of Proposition 14. We rewrite the minimax problem as\nmin\np∈∆([A]) max\nf∈RA Eπ∼p\nh\nf(πf) −f(π) −γ(f(π) −bf(π))2i\n=\nmin\np∈∆([A]) max\nf∈RA max\nπ⋆∈[A] Eπ∼p\nh\nf(π⋆) −f(π) −γ(f(π) −bf(π))2i\n=\nmin\np∈∆([A]) max\nπ⋆∈[A] max\nf∈RA Eπ∼p\nh\nf(π⋆) −f(π) −γ(f(π) −bf(π))2i\n.\nFor any fixed p and π⋆, first-order conditions for optimality imply that the choice for f that\nmaximizes this expression is\nf(π) = bf(π) −1\n2γ +\n1\n2γp(π⋆)I {π = π⋆} .\nThis choice gives\nEπ∼p[f(π⋆) −f(π)] = Eπ∼p\nh\nbf(π⋆) −bf(π)\ni\n+ 1 −p(π⋆)\n2γp(π⋆)\nand\nγ Eπ∼p\nh\n(f(π) −bf(π))2i\n= 1 −p(π⋆)\n4γ\n+ (1 −p(π⋆))2\n4γp(π⋆)\n=\n1\n4γp(π⋆) −1\n4γ .\nPlugging in and simplifying, we compute that the original minimax game is equivalent to\nmin\np∈∆([A]) max\nπ⋆∈[A]\n\u001a\nEπ∼p\nh\nbf(π⋆) −bf(π)\ni\n+\n1\n4γp(π⋆)\n\u001b\n−1\n4γ .\n(4.20)\nFinishing the proof: Ad-hoc approach. Observe that for any p ∈∆(Π), we have\nmax\nπ⋆∈[A]\n\u001a\nEπ∼p\nh\nbf(π⋆) −bf(π)\ni\n+\n1\n4γp(π⋆)\n\u001b\n≥Eπ⋆∼p\n\u0014\nEπ∼p\nh\nbf(π⋆) −bf(π)\ni\n+\n1\n4γp(π⋆)\n\u0015\n= A\n4γ ,\n67\nso no p can attain value better than\nA\n4γ . If we can show that IGW achieves this value, we\nare done.\nObserve that by setting p = IGW4γ( bf), we have that for all π⋆,\nEπ∼p\nh\nbf(π⋆) −bf(π)\ni\n+\n1\n4γp(π⋆) = Eπ∼p\nh\nbf(π⋆) −bf(π)\ni\n+ λ\n4γ + bf(bπ) −bf(π⋆)\n(4.21)\n= Eπ∼p\nh\nbf(bπ) −bf(π)\ni\n+ λ\n4γ .\nNote that the value on the right-hand side is independent of π⋆. That is, the inverse gap\nweighting distribution is an equalizing strategy. This means that for this choice of p, we\nhave\nmax\nπ⋆∈[A]\n\u001a\nEπ∼p\nh\nbf(π⋆) −bf(π)\ni\n+\n1\n4γp(π⋆)\n\u001b\n= min\nπ⋆∈[A]\n\u001a\nEπ∼p\nh\nbf(π⋆) −bf(π)\ni\n+\n1\n4γp(π⋆)\n\u001b\n= Eπ⋆∼p\n\u001a\nEπ∼p\nh\nbf(π⋆) −bf(π)\ni\n+\n1\n4γp(π⋆)\n\u001b\n= A\n4γ .\nHence, p = IGW4γ( bf) achieves the optimal value.\nFinishing the proof: Principled approach. We begin by relaxing to p ∈RA\n+. Define\ngπ⋆(p) = bf(π⋆) +\n1\n4γp(π⋆).\nLet ν ∈R be a Lagrange multiplier and p ∈RA\n+, and consider the Lagrangian\nL(p, ν) = gπ⋆(p) −\nX\nπ\np(π) bf(π) + ν\n X\nπ\np(π) −1\n!\n.\nBy the KKT conditions, if we wish to show that p ∈∆(Π) is optimal for the objective in\n(4.20), it suffices to find ν such that12\n0 ∈∂pL(p, ν),\nwhere ∂p denotes the subgradient with respect to p.\nRecall that for a convex function\nh(x) = maxy g(x, y), we have ∂xh(x) = co(\n\b\n∇g(x, y) | g(x, y) = maxy′ g(x, y′)\n\t\n).\nAs a\nresult,\n∂pL(p, ν) = ν1 −bf + co({∇pgπ⋆(p) | gπ⋆(p) = max\nπ′ gπ′(p)}).\nNow, let p = IGW4γ( bf). We will argue that 0 ∈∂pL(p, ν) for an appropriate choice of ν. By\n(4.21), we know that gπ(p) = gπ′(p) for all π, π′ (p is equalizing), so the expression above\nsimplifies to\n∂pL(p, ν) = ν1 −bf + co({∇pgπ⋆(p)}π⋆∈Π).\n(4.22)\nNoting that ∇pgπ⋆(p) = −\n1\n4γp2(π⋆)eπ⋆, we compute\nδ :=\nX\nπ\np(π)gπ(p) =\n\u001a\n−\n1\n4γp(π)\n\u001b\nπ∈Π\n=\n\u001a\n−λ\n4γ −bf(bπ) + bf(π)\n\u001b\nπ∈Π\n,\nwhich has δ ∈co({∇pgπ⋆(p)}π⋆∈Π). By choosing ν = λ\n4γ + bf(bπ), we have\nν1 −bf + δ = 0,\nso (4.22) is satisfied.\n12If p ∈∆(Π), the KKT condition that\nd\ndν L(p, ν) = 0 is already satisfied.\n68\n4.3 Decision-Estimation Coefficient: Examples\nWe now show how to bound the Decision-Estimation Coefficient for a number of examples\nbeyond finite-armed bandits—some familiar and others new—and show how this leads to\nbounds on regret via E2D.\nApproximately solving the DEC.\nBefore proceeding, let us mention that to apply\nE2D, it is not necessary to exactly solve the minimax problem (4.15). Instead, let us say\nthat a distribution p = p( bf, γ) certifies an upper bound on the DEC if, given bf and γ > 0,\nit ensures that\nsup\nf∈F\nEπ∼p\nh\nf(πf) −f(π) −γ · (f(π) −bf(π))2i\n≤decγ(F, bf)\nfor some known upper bound decγ(F, bf) ≥decγ(F, bf). In this case, letting decγ(F) :=\nsup bf decγ(F, bf), it is simple to see that if we use this distribution pt = p( bf t, γ) within E2D,\nwe have\nReg ≤decγ(F) · T + γ · EstSq(F, T, δ).\n4.3.1\nCheating Code\nFor a first example, we show that the DEC leads to regret bounds that scale with log(A)\nfor the cheating code example in Example 4.2; that is, unlike UCB and posterior sampling,\nthe DEC correctly adapts to the structured of this problem.\nProposition 15 (DEC for Cheating Code): Consider the cheating code in Exam-\nple 4.2. For this class F, we have\ndecγ(F) ≲log2(A)\nγ\n.\nNote that while the strategy p in Proposition 15 certifies a bound on the DEC, it is not\nnecessarily the exact minimizer, and hence the distributions p1, . . . , pT played by E2D may\nbe different. Nonetheless, since the regret of E2D is bounded by the DEC, this result (via\nProposition 13) implies that its regret is bounded by Reg ≲\np\nlog2(A)T log|F|. Using a\nslightly more refined version of the E2D algorithm [43], one can improve this to match the\nlog(T) regret bound given in Example 4.2.\nProof of Proposition 15. To simplify exposition, we present a bound on decγ(F, bf) for this\nexample only for bf ∈F, not for bf ∈co(F). A similar approach (albeit with a slightly\ndifferent choice for p) leads to the same bound on decγ(F). Let bf ∈F and γ > 0 be given,\nand define\np = (1 −ε)π bf + ε · unif(C).\nWe will show that if we choose ε = 2log2(A)\nγ\n, this strategy certifies that\ndecγ(F, bf) ≲log2(A)\nγ\n.\n69\nLet f ∈F be fixed, and consider the value\nEπ∼p\nh\nf(πf) −f(π) −γ · (f(π) −bf(π))2i\n.\nWe consider two cases. First the first, if πf = π bf, then we can upper bound\nEπ∼p\nh\nf(πf) −f(π) −γ · (f(π) −bf(π))2i\n≤Eπ∼p[f(πf) −f(π)] = Eπ∼p\nh\nf(π bf) −f(π)\ni\n≤2ε,\nsince f ∈[−1, 1].\nFor the second case, suppose that πf ̸= π bf. We begin by bounding\nEπ∼p\nh\nf(πf) −f(π) −γ · (f(π) −bf(π))2i\n≤2 −γ · Eπ∼p\nh\n(f(π) −bf(π))2i\n,\nusing that f ∈[−1, 1]. To proceed, we want to argue that the negative offset term above\nis sufficiently large; informally, this means that we are exploring “enough”. Observe that\nsince πf ̸= π bf, if we let b1, . . . , blog2(A) and b′\n1, . . . , b′\nlog2(A) denote the binary representations\nfor πf and π bf, there exists i such that bi ̸= b′\ni. As a result, we have\nEπ∼p\nh\n(f(π) −bf(π))2i\n≥\nε\nlog2(A)(f(ci) −bf(ci))2 =\nε\nlog2(A)(bi −b′\ni)2 =\nε\nlog2(A).\nWe conclude that in the second case,\nEπ∼p\nh\nf(πf) −f(π) −γ · (f(π) −bf(π))2i\n≤2 −γ\nε\nlog2(A).\nPutting the cases together, we have\nEπ∼p\nh\nf(πf) −f(π) −γ · (f(π) −bf(π))2i\n≤max\n\u001a\n2ε, 2 −γ\nε\nlog2(A)\n\u001b\n.\nTo balance these terms, we set\nε = 2log2(A)\nγ\n,\nwhich leads to the result.\n4.3.2\nLinear Bandits\nWe next consider the problem of linear bandits linear bandit [2, 13, 28, 26, 1], which is\na special case of the linear contextual bandit problem we saw in Section 3. We let Π be\narbitrary, and define F = {π 7→⟨θ, ϕ(π)⟩| θ ∈Θ}, where Θ ⊆Bd\n2(1) is a parameter set and\nϕ : Π →Bd\n2(1) is a fixed feature map that is known to the learner.\nTo prove bounds on the DEC for this setting, we make use of a primitive from convex\nanalysis and experimental design known as the G-optimal design.\nProposition 16 (G-optimal design [52]): For any compact set Z ⊆Rd with\ndim span(Z) = d, there exists a distribution p ∈∆(Z), called the G-optimal design,\nwhich has\nsup\nz∈Z\n\nΣ−1\np z, z\n\u000b\n≤d,\n(4.23)\n70\nwhere Σp := Ez∼p\n\u0002\nzz⊤\u0003\n.\nThe G-optimal design ensures coverage in every direction of the decision space, generalizing\nthe notion of uniform exploration for finite action spaces. In this sense, it can be thought\nof as a “universal” exploratory distribution for linearly structured action spaces. Special\ncases include:\n• When Z = ∆([A]), we can take p = unif(e1, . . . , eA) as an optimal design\n• When Z = Bd\n2(1), we can again take p = unif(e1, . . . , eA) as an optimal design.\n• For any positive definite matrix A ≻0, the set Z =\n\b\nz ∈Rd | ⟨Az, z⟩≤1\n\t\nis an\nellipsoid. Letting λ1, . . . , λd and v1, . . . , vd denote the eigenvalues and eigenvectors for\nA, respectively, the distribution p = unif(λ−1/2\n1\nv1, . . . , λ−1/2\nd\nvd) is an optimal design.\nTo see how the G-optimal design can be used for exploration, consider the following\ngeneralization of the ε-greedy algorithm.\n• Let q ∈∆(Π) be the G-optimal design for the set {ϕ(π)}π∈Π.\n• At each step t, obtain bf t from a supervised estimation oracle. Play bπt = π bft with\nprobability 1 −ε, and sample πt ∼q otherwise.\nIt is straightforward to show that this strategy gives Reg ≲d1/3T 2/3 log|F| for linear\nbandits. The basic idea is to replace (3.30) in the proof of Proposition 8 with the optimal\ndesign property (4.23), using that the reward functions under consideration are linear. The\nintuition is that even though we are no longer guaranteed to explore every single action\nwith some minimum probability, by exploring with the optimal design, we ensure that some\nfraction of the data we collect covers every possible direction in action space to the greatest\nextent possible.\nThe following result shows that by combining optimal design inverse gap weighting, we\ncan obtain a d/γ bound on the DEC, which leads to an improved\n√\ndT regret bound.\nProposition 17 (DEC for Linear Bandits): Consider the linear bandit setting. Let\na linear function bf and γ > 0 be given, consider the following distribution p:\n• Define ϕ(π) = ϕ(π)/\nq\n1 + γ\nd\n\u0000 bf(π bf) −bf(π)\n\u0001\n, where π bf = arg maxπ∈Π bf(π).\n• Let ¯q ∈∆(Π) be the G-optimal design for the set {ϕ(π)}π∈Π, and define q =\n1\n2 ¯q + 1\n2Iπ b\nf .\n• For each π ∈Π, set\np(π) =\nq(π)\nλ + γ\nd( bf(π bf) −bf(π))\n,\nwhere λ ∈[1/2, 1] is chosen such that P\nπ p(π) = 1.a\nThis strategy certifies that\ndecγ(F) ≲d\nγ .\n71\naThe normalizing constant λ always exists because we have\n1\n2λ ≤P\nπ p(π) ≤1\nλ.\nOne can show that decγ(F) ≳d\nγ for this setting as well, so this is the best bound we can\nhope for. Combining this result with Proposition 13 and using the averaged exponential\nweights algorithm for estimation as in (4.18) gives Reg ≲\np\ndT log(|F|/δ).\nProof of Proposition 17. Fix f ∈F. Let us abbreviate η = γ\nd. As in Proposition 9, we\nbreak regret into three terms:\nEπ∼p\n\u0002\nf(πf) −f(π)\n\u0003\n= Eπ∼p\n\u0002 bf(π bf) −bf(π)\n\u0003\n|\n{z\n}\n(I) exploration bias\n+ Eπ∼p\n\u0002 bf(π) −f(π)\n\u0003\n|\n{z\n}\n(II) est error on policy\n+ f(πf) −bf(π bf)\n|\n{z\n}\n(III) est error at opt\n.\nThe first term captures the loss in exploration that we would incur if bf we true the reward\nfunction, and is equal to:\nX\nπ\nq(π)( bf(π bf) −bf(π))\nλ + η\n\u0010\nbf(π bf) −bf(π)\n\u0011 ≤\nX\nπ\nq(π)\nη\n≤1\nη,\nand the second term, as before, is at most\nq\nEπ∼p\n\u0002\n( bf(π) −f(π))2\u0003\n≤1\n2γ + γ\n2 Eπ∼p( bf(π) −f(π))2.\nThe third term can be written as\n(III) = f(πf) −bf(πf) −( bf(π bf) −bf(πf)) =\n\nθ −bθ, ϕ(πf)\n\u000b\n−( bf(π bf) −bf(πf)),\nwhere θ, bθ ∈Θ are parameters such that f(π) = ⟨θ, ϕ(π)⟩and bf(π) = ⟨bθ, ϕ(π)⟩. Defining\nΣp = Eπ∼p\n\u0002\nϕ(π)ϕ(π)⊤\u0003\n, we can bound\n\nθ −bθ, ϕ(πf)\n\u000b\n=\n\nΣ1/2\np\n(θ −bθ), Σ−1/2\np\nϕ(πf)\n\u000b\n≤∥Σ1/2\np\n(θ −bθ)∥2∥Σ−1/2\np\nϕ(πf)∥2\n≤γ\n2∥Σ1/2\np\n(θ −bθ)∥2\n2 + 1\n2γ ∥Σ−1/2\np\nϕ(πf)∥2\n2.\nNote that ∥Σ1/2\np\n(θ −bθ)∥2\n2 = Eπ∼p[( bf(π) −f(π))2] and ∥Σ−1/2\np\nϕ(πf)∥2\n2 = ⟨ϕ(πf), Σ−1\np ϕ(πf)⟩,\nso we have\n(III) ≤γ\n2 Eπ∼p[( bf(π) −f(π))2] + 1\n2γ ⟨ϕ(πf), Σ−1\np ϕ(πf)⟩−( bf(π bf) −bf(πf))\n|\n{z\n}\n(IV)\n.\nTo proceed, observe that\nΣp ⪰1\n2\nX\nπ\n¯q(π)\nλ + η( bf(π bf) −bf(π))\nϕ(π)ϕ(π)⊤\n⪰1\n2\nX\nπ\n¯q(π)\n1 + η( bf(π bf) −bf(π))\nϕ(π)ϕ(π)⊤⪰1\n2\nX\nπ\n¯q(π)ϕ(π)ϕ(π)⊤=: 1\n2Σ¯q\n72\nThis means that we can bound\n⟨ϕ(πf), Σ−1\np ϕ(πf)⟩≤2⟨ϕ(πf), Σ−1\n¯q ϕ(πf)⟩\n= 2(1 + η( bf(π bf) −bf(πf))⟨ϕ(πf), Σ−1\n¯q ϕ(πf)⟩\n≤2d(1 + η( bf(π bf) −bf(πf)),\nwhere the last line uses that ¯q is the G-optimal design for {ϕ(π)}π∈Π. We conclude that\n(IV) ≤2d\n2γ + 2dη\n2γ ( bf(π bf) −bf(πf)) −( bf(π bf) −bf(πf)) ≤d\nγ .\nRemark 14: In fact, it can be shown [39] that when Θ = Rd, the exact minimizer of\nthe DEC for linear bandits is given by\np = arg max\np∈∆(Π)\n\u001a\nEπ∼p\n\u0002 bf(π)\n\u0003\n+ 1\n4γ log det(Eπ∼p[ϕ(π)ϕ(π)⊤])\n\u001b\n.\n4.3.3\nNonparametric Bandits\nFor all of the examples so far, we have shown that\ndecγ(F) ≲eff-dim(F, Π)\nγ\n,\nwhere eff-dim(F, Π) is some quantity that (informally) reflects the amount of exploration\nrequired for the class F under consideration (A for bandits, log2(A) for the cheating code,\nand d for linear bandits). In general though, the Decision-Estimation Coefficient does not\nalways shrink at a γ−1 rate, and can have slower decay for problems where the optimal\nrate is worse than\n√\nT. We now consider such a setting: a standard nonparametric bandit\nproblem called Lipschitz bandits in metric spaces [14, 54].\nWe take Π to be a metric space equipped with metric ρ, and define\nF = {f : Π →[0, 1] | f is 1-Lipschitz w.r.t ρ}.\nWe give a bound on the Decision-Estimation Coefficient which depends on the covering\nnumber for the space Π (with respect to the metric ρ). Let us say that Π′ ⊆Π is an ε-cover\nwith respect to ρ if\n∀π ∈Π\n∃π′ ∈Π′\ns.t.\nρ(π, π′) ≤ε,\nand let Nρ(Π, ε) denote the size of the smallest such cover.\nProposition 18 (DEC for Lipschitz Bandits): Consider the Lipschitz bandit set-\nting, and suppose that there exists d > 0 such that Nρ(Π, ε) ≤ε−d for all ε > 0. Let\nbf : Π →[0, 1] and γ ≥1 be given and consider the following distribution:\n1. Let Π′ ⊆Π witness the covering number Nρ(Π, ε) for a parameter ε > 0.\n73\n2. Let p be the result of applying the inverse gap weighting strategy in (3.36) to bf,\nrestricted to the (finite) decision space Π′.\nBy setting ε ∝γ−\n1\nd+1 , this strategy certifies that\ndecγ(F, bf) ≲γ−\n1\nd+1 .\nIgnoring dependence on EstSq(F, T, δ), this result leads to regret bounds that scale as T\nd+1\nd+2\n(after tuning γ in Proposition 13), which cannot be improved.\nProof of Proposition 18. Let f ∈F be fixed. Let Π′ be the ε-cover for Π. Since f is 1-\nLipschitz, for all π ∈Π there exists a corresponding covering element ι(π) ∈Π′ such that\nρ(π, ι(π)) ≤ε, and consequently for any distribution p,\nEπ∼p[f(πf) −f(π)] ≤Eπ∼p[f(ι(πf)) −f(π)] + |f(πf) −f(ι(πf))|\n≤Eπ∼p[f(ι(πf)) −f(π)] + ρ(πf, ι(πf))\n≤Eπ∼p[f(ι(πf)) −f(π)] + ε.\nAt this point, since ι(πf) ∈Π′, Proposition 9 ensures that if we choose p using inverse gap\nweighting over Π′, we have\nEπ∼p[f(ι(πf)) −f(π)] ≤|Π′|\nγ\n+ γ · Eπ∼p\nh\n(f(π) −bf(π))2i\n.\nFrom our assumption on the growth of Nρ(Π, ε), |Π′| ≤ε−d, so the value is at most\nε + ε−d\nγ .\nWe choose ε ∝γ−\n1\nd+1 to balance the terms, leading to the result.\n4.3.4\nFurther Examples\nWe state the following additional upper bounds on the DEC without proof; details can be\nfound in [40].\nExample 4.3 (Decision-Estimation Coefficient subsumes Eluder Dimension). Consider any\nclass F with values in [0, 1]. For all γ ≥e, we have\ndecγ(F) ≲inf\nε>0\n\u001a\nε + Edim(F −F, ε) log2(γ)\nγ\n\u001b\n+ γ−1.\n(4.24)\n◁\nAs a special case, this example implies that E2D enjoys a regret bound for generalized\nlinear bandits similar to that of UCB.\nExample 4.4 (Bandits with Concave Rewards). The concave (or convex, if one considers\nlosses rather than rewards) bandit problem [53, 35, 4, 21, 23, 58] is a generalization of the\nlinear bandit. We take Π ⊆Bd\n2(1) and define\nF = {f : Π →[0, 1] | f is concave and 1-Lipschitz w.r.t ℓ2}.\n74\nFor this setting, whenever F ⊆(Π →[0, 1]), results of Lattimore [58] imply that\ndecγ(F) ≲d4\nγ · polylog(d, γ)\n(4.25)\nfor all γ > 0.\n◁\nFor the function class\nF =\nn\nf(π) = −relu(⟨ϕ(π), θ⟩) | θ ∈Θ ⊂Bd\n2(1)\no\n,\n(4.25) leads to a\np\npoly(d)T regret bound for E2D. This highlights a case where the Eluder\ndimension is overly pessimistic, since we saw that it grows exponentially for this class.\n4.4 Relationship to Optimism and Posterior Sampling\nWe close this section by highlighting some connections between the Decision-Estimation\nCoefficient and E2D and other techniques we have covered so far: Optimism (UCB) and\nPosterior Sampling. Additional connections to optimism can be found in Section 6.7.3.\n4.4.1\nConnection to Optimism\nThe E2D meta-algorithm and the Decision-Estimation Coefficient can be combined with the\nidea of confidence sets that we used in the UCB algorithm. Consider the following variant\nof E2D.\nEstimation-to-Decisions (E2D) with Confidence Sets\nInput: Exploration parameter γ > 0, confidence radius β > 0.\nfor t = 1, . . . , T do\nObtain bf t from online regression oracle with (π1, r1), . . . , (πt−1, rt−1).\nSet\nF t =\n(\nf ∈F |\nX\ni<t\nEπi∼pi\nh\n( bf i(πi) −f⋆(πi))2i\n≤β\n)\n.\nCompute\npt = arg min\np∈∆(Π)\nmax\nf∈Ft Eπ∼p\n\u0014\nf(πf) −f(π) −γ · (f(π) −bf t(π))2\n\u0015\n.\nSelect action πt ∼pt.\nThis strategy is the same as the basic E2D algorithm, except that at each step, we compute\na confidence set F t and modify the minimax problem so that the max player is restricted to\nchoose f ∈F t.13 With this change, the distribution pt can be interpreted as the minimizer\nfor decγ(F t, bf t).\nTo analyze this algorithm, we show that as long as f⋆∈F t for all t, the same per-step\nanalysis as in Proposition 13 goes through, with F replaced by F t. This allows us to prove\nthe following result.\n13Note that compared to the confidence sets used in UCB, a slight difference is that we compute F t using\nthe estimates bf 1, . . . , bf T produced by the online regression oracle (this is sometimes referred to as “online-\nto-confidence set conversion”) as opposed to using ERM; this difference is unimportant, and the later would\nwork as well.\n75\nProposition 19: For any δ ∈(0, 1) and γ > 0, if we set β = EstSq(F, T, δ), then E2D\nwith confidence sets ensures that with probability at least 1 −δ,\nReg ≤\nT\nX\nt=1\ndecγ(F t) + γ · EstSq(F, T, δ).\n(4.26)\nThis bound is never worse than the one in Proposition 13, but it can be smaller if the\nconfidence sets F 1, . . . , F T shrink quickly. For a proof, see Exercise 9.\nRemark 15: In fact, the regret bound in (4.26) can be shown to hold for any sequence\nof confidence sets F 1, . . . , F T, as long as f⋆∈F t\n∀t with probability at least 1 −\nδ; the specific construction we use within the E2D variant above is chosen only for\nconcreteness.\nRelation to confidence width and UCB.\nIt turns out that the usual UCB algo-\nrithm, which selects πt = arg maxπ∈Π ¯f t(π) for ¯f t(π) = maxf∈Ft f t(π), certifies a bound\non decγ(F t) which is never worse than usual confidence width we use in the UCB analy-\nsis.\nProposition 20: The UCB strategy πt = arg maxπ∈Π ¯f t(π) certifies that\ndec0(F t) ≤¯f t(πt) −f(πt).\n(4.27)\nProof of Proposition 20. By choosing πt = arg maxπ∈Π ¯f t(π), we have that for any bf,\ndec0(F t, bf) =\ninf\np∈∆(Π) sup\nf∈Ft Eπ∼p\nh\nmax\nπ⋆f(π⋆) −f(π)\ni\n≤sup\nf∈Ft\nh\nmax\nπ⋆f(π⋆) −f(πt)\ni\n≤sup\nf∈Ft\nh\nmax\nπ⋆\n¯f t(π⋆) −f(πt)\ni\n= sup\nf∈Ft\n\u0002 ¯f t(πt) −f(πt)\n\u0003\n= ¯f t(πt) −f t(πt).\nAs we saw in the analysis of UCB for multi-armed bandits with Π = {1, . . . , A} (Section 2.3),\nthe confidence width in (4.27) might be large for a given round t, but by the pigeonhole\nargument (Lemma 8), when we sum over all rounds we have\nT\nX\nt=1\ndec0(F t) ≤\nT\nX\nt=1\n¯f t(πt) −f t(πt) ≤eO(\n√\nAT).\nHence, even though UCB is not the optimal strategy to minimize the DEC, it can still lead\nto upper bounds on regret when the confidence width shrinks sufficiently quickly. Of course,\nas examples like the cheating code show, we should not expect this to happen in general.\n76\nInterestingly, the bound on the DEC in Proposition 20 holds for γ = 0, which only leads\nto meaningful bounds on regret because F 1, . . . , F T are shrinking. Indeed, Proposition 14\nshows that with F = RA, we have\ndecγ(F) ≳A\nγ ,\nso the unrestricted class F has decγ(F) →∞as γ →0. By allowing for γ > 0, we can\nprove the following slightly stronger result, which replaces f t by bf t.\nProposition 21: For any γ > 0, the UCB strategy πt = arg maxπ∈Π ¯f t(π) certifies\nthat\ndecγ(F t, bf t) ≤¯f t(πt) −bf t(πt) + 1\n4γ .\nProof of Proposition 21. This is a slight generalization of the proof of Proposition 20. By\nchoosing πt = arg maxπ∈Π ¯f t(π), we have\ndecγ(F, bf t) =\nmin\np∈∆(Π) max\nf∈Ft Eπ∼p\n\u0014\nmax\nπ⋆f(π⋆) −f(π) −γ ·( bf t(π) −f(π))2\n\u0015\n≤max\nf∈Ft\n\u0014\nmax\nπ⋆f(π⋆) −f(πt) −γ ·( bf t(πt) −f(πt))2\n\u0015\n≤max\nf∈Ft\n\u0014\n¯f t(πt) −f(πt) −γ ·( bf t(πt) −f(πt))2\n\u0015\n= max\nf∈Ft\n\u0014\nbf t(πt) −f(πt) −γ ·( bf t(πt) −f(πt))2\n\u0015\n|\n{z\n}\n≤1\n4γ\n+ ¯f t(πt) −bf t(πt).\n4.4.2\nConnection to Posterior Sampling\nThe Decision-Estimation Coefficient (4.15) is a min-max optimization problem, which we\nhave mentioned can be interpreted as a game in which the learner (the “min” player) aims\nto find a decision distribution p that optimally trades off regret and information acquisition\nin the face of an adversary (the “max” player) that selects a worst-case model in M. We\ncan define a natural dual (or, max-min) analogue of the DEC via\ndecγ(F, bf) =\nsup\nµ∈∆(F)\ninf\np∈∆(Π) Ef∼µ Eπ∼p\nh\nf(πf) −f(π) −γ · (f(π) −bf(π))2i\n.\n(4.28)\nThe dual Decision-Estimation Coefficient has the following Bayesian interpretation. The\nadversary selects a prior distribution µ over models in M, and the learner (with knowledge\nof the prior) finds a decision distribution p that balances the average tradeoff between regret\nand information acquisition when the underlying model is drawn from µ.\nUsing the minimax theorem (Lemma 41), one can show that the Decision-Estimation\nCoefficient and its Bayesian counterpart coincide.\n77\nProposition 22 (Equivalence of primal and dual DEC): Under mild regularity\nconditions, we have\ndecγ(F, bf) = decγ(F, bf).\n(4.29)\nThus, any bound on the dual DEC immediately yields a bound on the primal DEC. This\nperspective is useful because it allows us to bring existing tools for Bayesian bandits and\nreinforcement learning to bear on the primal Decision-Estimation Coefficient. As an ex-\nample, we can adapt the posterior sampling/probability matching strategy introduced in\nSection 2. When applied to the Bayesian DEC—this approach selects p to be the action\ndistribution induced by sampling f ∼µ and selecting πf. Using Lemma 9, one can show\nthat this strategy certifies that\ndecγ(F) ≲|Π|\nγ\nfor the multi-armed bandit. In fact, existing analysis techniques for the Bayesian setting\ncan be viewed as implicitly providing bounds on the dual Decision-Estimation Coefficient\n[75, 22, 21, 76, 59, 58]. Notably, the dual DEC is always bounded by a Bayesian complexity\nmeasure known as the information ratio, which is used throughout the literature on Bayesian\nbandits and reinforcement learning [40].\nBeyond the primal and dual Decision-Estimation Coefficient, there are deeper connec-\ntions between the DEC and Bayesian algorithms, including a Bayesian counterpart to the\nE2D algorithm itself [40].\n4.5 Incorporating Contexts⋆\nThe Decision-Estimation Coefficient and E2D algorithm trivially extend to handle contextual\nstructured bandits. This approach generalizes the SquareCB method introduced in Section 3\nfrom finite action spaces to general action spaces. Consider the following protocol.\nContextual Structured Bandit Protocol\nfor t = 1, . . . , T do\nObserve context xt ∈X.\nSelect decision πt ∈Π.\n// Π is large and potentially continuous.\nObserve reward rt ∈R.\nThis is the same as the contextual bandit protocol in Section 3, except that we allow Π to\nbe large and potentially continuous. As in that section, we allow the contexts x1, . . . , xT to\nbe generated in an arbitrary, potentially adversarial fashion, but assume that\nrt ∼M⋆(· | xt, πt),\nand define f⋆(x, π) = Er∼M⋆(·|x,π). We assume access to a function class F such that f⋆∈F,\nand assume access to an estimation oracle for F that ensures that with probability at least\n1 −δ,\nT\nX\nt=1\nEπt∼pt( bf t(xt, πt) −f⋆(xt, πt))2 ≤EstSq(F, T, δ).\nFor f ∈F, we define πf(x) = arg maxπ∈Π f(x, π).\nTo extend the E2D algorithm to this setting, at each time t we solve the minimax\nproblem corresponding to the DEC, but condition on the context xt.\n78\nEstimation-to-Decisions (E2D) for Contextual Structured Bandits\nInput: Exploration parameter γ > 0.\nfor t = 1, . . . , T do\nObserve xt ∈X.\nObtain bf t from online regression oracle with (x1, π1, r1), . . . , (xt−1, πt−1, rt−1).\nCompute\npt = arg min\np∈∆(Π)\nmax\nf∈F Eπ∼p\n\u0014\nf(xt, πf(xt)) −f(xt, π) −γ · (f(xt, π) −bf t(xt, π))2\n\u0015\n.\nSelect action πt ∼pt.\nFor x ∈X, define\nF(x, ·) = {f(x, ·) | f ∈F}\nas the projection of F onto x ∈X. The following result shows that whenever the DEC is\nbounded conditionally—that is, whenever it is bounded for F(x, ·) for all x—this strategy\nhas low regret.\nProposition 23: The E2D algorithm with exploration parameter γ > 0 guarantees\nthat\nReg ≤sup\nx∈X\ndecγ(F(x, ·)) · T + γ · EstSq(F, T, δ),\n(4.30)\nWe omit the proof of this result, which is nearly identical to that of Proposition 13. The\nbasic idea is that for each round, once we condition on the context xt, the DEC allows us\nto link regret to estimation error in the same fashion non-contextual setting.\nWe showed in Proposition 14 that the IGW distribution exactly solves the DEC minimax\nproblem when F = RA. Hence, the SquareCB algorithm in Section 3 is precisely the special\ncase of Contextual E2D in which F = RA.\nGoing beyond the finite-action setting, it is simplest to interpret Proposition 23 when\nF(x, ·) has the same structure for all contexts.\nOne example is contextual bandits with\nlinearly structured action spaces. Here, we take\nF = {f(x, a) = ⟨ϕ(x, a), g(x)⟩| g ∈G},\nwhere ϕ(x, a) ∈Rd is a fixed feature map and G ⊂(X →Bd\n2(1)) is an arbitrary func-\ntion class. This setting generalizes the linear contextual bandit problem from Section 3,\nwhich corresponds to the case where G is a set of constant functions.\nWe can apply\nProposition 17 to conclude that supx∈X decγ(F(x, ·)) ≲\nd\nγ , so that Proposition 23 gives\nReg ≲\np\ndT · EstSq(F, T, δ).\n4.6 Additional Properties of the Decision-Estimation Coefficient⋆\nThe following proposition indicates that the value of the Decision-Estimation Coefficient\ndecγ(F, bf) cannot be increased by taking references models bf outside the convex hull of\nF:\n79\nProposition 24: For any γ > 0,\nsup\nbf:Π→R\ndecγ(F, bf) =\nsup\nbf∈co(F)\ndecγ(F, bf).\n4.7 Exercises\nExercise 8 (Posterior Sampling for Multi-Armed Bandits): Prove that for the standard\nmulti-armed bandit,\ndecγ(F) ≲|Π|\nγ ,\nby using the Posterior Sampling strategy (select p to be the action distribution induced by\nsampling f ∼µ and selecting πf), and applying the decoupling lemma (Lemma 9). Recall that\nhere, decγ(F) is the “maxmin” version of the DEC (4.28).\nExercise 9: Prove Proposition 19.\nExercise 10: In this exercise, we will prove Proposition 24 as follows. First, show that the\nleft-hand side is an upper bound on the right-hand side. For the other direction:\n1. Prove that\ninf\nb\nf∈co(F)\nEf∼µ Eπ∼p(f(π) −bf(π))2 ≤\ninf\nb\nf:Π→R\nEf∼µ Eπ∼p(f(π) −bf(π))2.\n(4.31)\n2. Use the Minimax Theorem (Lemma 41 in Appendix A.3) to conclude Proposition 24.\n5. REINFORCEMENT LEARNING: BASICS\nWe now introduce the framework of reinforcement learning, which encompasses a rich set\nof dynamic, stateful decision making problems.\nConsider the task of repeated medical\ntreatment assignment, depicted in Figure 4. To make the setting more realistic, it is natural\nto allow the decision-maker to apply multi-stage strategies rather simple one-shot decisions\nsuch as “prescribe a painkiller.” In principle, in the language of structured bandits, nothing\nis preventing us from having each decision πt be a complex multi-stage treatment strategy\nthat, at each stage, acts on the patient’s dynamic state, which evolves as a function of\nthe treatments at previous stages.\nAs an example, intermediate actions of the type “if\npatient’s blood pressure is above X then do Y” can form a decision tree that defines the\ncomplex strategy πt. Methods from the previous lectures provide guarantees for such a\nsetting, as long as we have a succinct model of expected rewards. What sets RL apart from\nstructured bandits is the additional information about the intermediate state transitions\nand intermediate rewards. This information facilitates credit assignment, the mechanism for\nrecognizing which of the actions led to the overall (composite) decision to be good or bad.\nThis extra information can reduce what would otherwise be exponential sample complexity\nin terms of the number of stages, states, and actions in multi-stage decision making.\n80\nThis section is structured as follows. We first present the formal reinforcement learning\nframework and present basic principles including Bellman optimality and dynamic pro-\ngramming, which facilitate efficiently computing optimal decisions when the environment\nis known.\nWe then consider the case in which the environment is unknown, and give\nalgorithms for perhaps the simplest reinforcement learning setting, tabular reinforcement\nlearning, where the state and action spaces are finite. Algorithms for more complex rein-\nforcement learning settings are given in Section 5.\n5.1 Finite-Horizon Episodic MDP Formulation\nWe consider an episodic finite-horizon reinforcement learning framework. With H denoting\nthe horizon, a Markov Decision Process (MDP) M takes the form\nM =\n\b\nS, A, {P M\nh }H\nh=1, {RM\nh }H\nh=1, d1\n\t\n,\nwhere S is the state space, A is the action space,\nP M\nh : S × A →∆(S)\nis the probability transition kernel at step h,\nRM\nh : S × A →∆(R)\nis the reward distribution, and d1 ∈∆(S) is the initial state distribution. We allow the\nreward distribution and transition kernel to vary across MDPs, but assume for simplicity\nthat the initial state distribution is fixed and known.\nFor a fixed MDP M, an episode proceeds under the following protocol. At the beginning\nof the episode, the learner selects a randomized, non-stationary policy\nπ = (π1, . . . , πH),\nwhere πh : S →∆(A); we let Πrns for “randomized, non-stationary” denote the set of\nall such policies. The episode then evolves through the following process, beginning from\ns1 ∼d1. For h = 1, . . . , H:\n• ah ∼πh(sh).\n• rh ∼RM\nh (sh, ah) and sh+1 ∼P M\nh (sh, ah).\nFor notational convenience, we take sH+1 to be a deterministic terminal state. The Markov\nproperty refers to the fact that under this evolution,\nPM(sh+1 = s′ | sh, ah) = PM(sh+1 = s′ | sh, ah, sh−1, ah−1, . . . , s1, a1).\nThe value for a policy π under M is given by\nf M(π) := EM,π\n\" H\nX\nh=1\nrh\n#\n,\n(5.1)\nwhere EM,π[·] denotes expectation under the process above. We define an optimal policy\nfor model M as\nπM ∈arg max\nπ∈Πrns\nf M(π).\n(5.2)\n81\nValue functions.\nMaximization in (5.2) is a daunting task, since each policy π is a\ncomplex multi-stage object. It is useful to define intermediate “reward-to-go” functions to\nstart breaking this complex task into smaller sub-tasks. Specifically, for a given model M\nand policy π, we define the state-action value function and state value function via\nQ\nM,π\nh\n(s, a) = EM,π\n\" H\nX\nh′=h\nrh′ | sh = s, ah = a\n#\n,\nand\nV\nM,π\nh\n(s) = EM,π\n\" H\nX\nh′=h\nrh′ | sh = s\n#\n.\nHence, the definition in (5.1) reads\nf M(π) = Es∼d1,a∼π1(s)\n\u0002\nQ\nM,π\n1\n(s, a)\n\u0003\n= Es∼d1\n\u0002\nV\nM,π\n1\n(s)\n\u0003\nOnline RL.\nFor reinforcement learning, our main focus will be on what is called the\nonline reinforcement learning problem, in which we interact with an unknown MDP M⋆for\nT episodes. For each episode t = 1, . . . , T, the learner selects a policy πt ∈Πrns. The policy\nis executed in the MDP M⋆, and the learner observes the resulting trajectory\nτ t = (st\n1, at\n1, rt\n1), . . . , (st\nH, at\nH, rt\nH).\nThe goal is to minimize the total regret\nT\nX\nt=1\nEπt∼pt\n\u0002\nf M⋆(πM⋆) −f M⋆(πt)\n\u0003\n(5.3)\nagainst the optimal policy πM⋆for M⋆.\nThe online RL framework is a strict generalization of (structured) bandits and contextual\nbandits (with i.i.d. contexts). Indeed, if S = {s0} and H = 1, each episode amounts to\nchoosing an action at ∈A and observing a reward rt with mean f M(at), which is precisely\na bandit problem. On the other hand, taking S = X and H = 1 puts us in the setting of\ncontextual bandits, with d1 being the distribution of contexts. In both cases, the notion of\nregret (5.3) coincides with the notion of regret in the respective setting.\nWe mention in passing that many alternative formulations for Markov decision processes\nand for the reinforcement learning problem appear throughout the literature. For example,\nMDPs can be studied with infinite horizon (with or without discounting), and an alternative\nto minimizing regret is to consider PAC-RL which aims to minimize the sub-optimality of\na final output policy produced after exploring for T rounds.\n5.2 Planning via Dynamic Programming\nIn some reinforcement learning problems, it is natural to assume that the true MDP M⋆is\nknown. This may be the case with games, such as chess or backgammon, where transition\nprobabilities are postulated by the game itself. In other settings, such as robotics or medical\ntreatment, the agent interacts with an unknown M⋆and needs to learn at least some aspects\nof this environment. The online reinforcement learning problem described above falls in the\nlatter category. Before attacking the learning problem, we need to understand the structure\nof solutions to (5.2) in the case where M⋆is known to the decision-maker. In this section,\nwe show that the problem of maximizing f M(π) over π ∈Π in a known model M (known\nas planning) can be solved efficiently via the principle of dynamic programming. Dynamic\n82\nprogramming can be viewed as solving the problem of credit assignment by breaking down\na complex multi-stage decision (policy) into a sequence of small decisions.\nWe start by observing that the optimal policy πM in (5.2) may not be uniquely defined.\nFor instance, if d1 assigns zero probability to some state s1, the behavior of πM on this state\nis immaterial. In what follows, we introduce a fundamental result, Proposition 25, which\nguarantees existence of an optimal policy πM = (πM,1, . . . , πM,H) that maximizes V\nM,π\n1\n(s)\nover π ∈Πrns for all states s ∈S simultaneously (rather than just on average, as in (5.2)).\nThe fact that such a policy exists may seem magical at first, but it is rather straightforward.\nIndeed, if πM,h(s) is defined for all s ∈S and h = 2, . . . , H, then defining the optimal πM,1(s)\nat any s is a matter of greedily choosing an action that maximizes the sum of the expected\nimmediate reward and the remaining expected reward under the optimal policy. Indeed,\nthis observation is Bellman’s principle of optimality, stated more generally as follows [17]:\nTo state the result formally, we introduce the optimal value functions as\nQ\nM,⋆\nh\n(s, a) = max\nπ∈Πrns EM,π\n\" H\nX\nh′=h\nrh′ | sh = s, ah = a\n#\nand\nV\nM,⋆\nh\n(s) = max\na\nQ\nM,⋆\nh\n(s, a) (5.4)\nfor all s ∈S, a ∈A, and h ∈[H]; we adopt the convention that V\nM,⋆\nH+1(s) = Q\nM,⋆\nH+1(s, a) = 0.\nSince these optimal values are separate maximizations for each s, a, h, it is reasonable to ask\nwhether there exists a single policy that maximizes all these value functions simultaneously.\nIndeed, the following lemma shows that there exists πM such that for all s, a, h,\nQ\nM,⋆\nh\n(s, a) = Q\nM,πM\nh\n(s, a),\nand\nV\nM,⋆\nh\n(s) = V\nM,πM\nh\n(s).\n(5.5)\nProposition 25 (Bellman Optimality): The optimal value function (5.4) for MDP\nM can be computed via V\nM,πM\nH+1 (s) := 0, and for each s ∈S,\nV\nM,πM\nh\n(s) = max\na∈A EM\u0002\nrh + V\nM,πM\nh+1\n(sh+1) | sh = s, ah = a\n\u0003\n.\n(5.6)\nThe optimal policy is given by\nπM,h(s) ∈arg max\na∈A\nEM\u0002\nrh + V\nM,πM\nh+1\n(sh+1) | sh = s, ah = a\n\u0003\n.\n(5.7)\nEquivalently, for all s ∈S, a ∈A,\nQ\nM,πM\nh\n(s, a) = EM\n\u0014\nrh + max\na′∈A Q\nM,πM\nh+1 (sh+1, a′) | sh = s, ah = a\n\u0015\n,\n(5.8)\n83\nand an the optimal policy is given by\nπM,h(s) ∈arg max\na∈A\nQ\nM,πM\nh\n(s, a).\n(5.9)\nThe update in (5.8) is referred to as value iteration (VI). It is useful to introduce a more suc-\ncinct notation for this update. For an MDP M, define the Bellman Operators T M\n1 , . . . , T M\nH\nvia\n[T M\nh Q](s, a) = Esh+1∼P M\nh (s,a),rh∼RM\nh (s,a)\n\u0014\nrh(s, a) + max\na′∈A Q(sh+1, a′)\n\u0015\n(5.10)\nfor any Q : S ×A →R. Going forward, we will write the expectation above more succinctly\nas\n[T M\nh Q](s, a) = EM\n\u0014\nrh(sh, ah) + max\na′∈A Q(sh+1, a′) | sh = s, ah = a\n\u0015\n(5.11)\nIn the language of Bellman operators, (5.8) can be written as\nQ\nM,πM\nh\n= T M\nh Q\nM,πM\nh+1 .\n(5.12)\n5.3 Failure of Uniform Exploration\nThe task of planning using dynamic programming—which requires knowledge of the MDP—\nis fairly straightforward, at least if we disregard the computational concerns. In this course,\nhowever, we are interested in the problem of learning to make decisions in the face of an\nunknown environment. Minimizing regret in an unknown MDP requires exploration. As\nthe next example shows, exploration in MDPs is a more delicate issue than in bandits.\nRecall that ε-Greedy, a simple algorithm, is a reasonable solution for bandits and con-\ntextual bandits, albeit with a suboptimal rate (T 2/3 as opposed to\n√\nT). The next (classical)\nexample, a so-called “combination lock,” shows that such a strategy can be disastrous in\nreinforcement learning, as it leads to exponential (in the horizon H) regret.\nConsider the MDP depicted in Figure 8, with H + 2 states, and two actions ag and\nab, and a starting state 1. The “good” action ag deterministically leads to the next state\nin the chain, while the “bad” action deterministically leads to a terminal state. The only\nplace where a non-zero reward can be received is the last state H, if the good action is\nchosen. The starting state is 1, and so the only way to receive non-zero reward is to select\nag for all the H time steps within the episode. Since the length of the episode is also H,\nselecting actions uniformly brings no information about the optimal sequence of actions,\nunless by chance all of the actions sampled happen to be good; the probability that this\noccurs is exponentially small in H. This means that T needs to be at least O(2H) to achieve\nnontrivial regret, and highlights the need for more strategic exploration.\n…\n1\n2\nag\nag\nag\nab\nab\nab\nH\nH + 1\nag\nr = 1\nr = 0\nr = 0\nr = 0\nFigure 8: Combination Lock MDP.\n84\nGiven the failure of ε-Greedy for this example, one can ask whether other algorithmic\nprinciples also fail.\nAs we will show now, the principle of optimism succeeds, and an\nanalogue of the UCB method yields a regret bound that is polynomial in the parameters\n|S|, |A|, and H. Before diving into the details, we present a collection of standard tools for\nanalysis in MDPs, which will find use throughout the remainder of the lecture notes.\n5.4 Analysis Tools\nOne of the most basic tools employed in the analysis of reinforcement learning algorithms is\nthe performance difference lemma, which expresses the difference in values for two policies\nin terms of differences in single-step decisions made by the two policies. The simple proof,\nstated below, proceeds by successively changing one policy into another and keep track of\nthe ensuing differences in expected rewards. One may also interpret this lemma as a version\nof the credit assignment mechanism.\nHenceforth, we adopt the following simplified notation. When a policy π is applied to\nthe random variable sh, we drop the subscript h and write π(sh) instead of πh(sh), whenever\nthis does not cause confusion.\nLemma 13 (Performance Difference Lemma): For any s ∈S, and π, π′ ∈Πrns,\nV\nM,π′\n1\n(s) −V\nM,π\n1\n(s) =\nH\nX\nh=1\nEM,πh\nQ\nM,π′\nh\n(sh, π′(sh)) −Q\nM,π′\nh\n(sh, ah) | s1 = s\ni\n(5.13)\nProof of Lemma 13. Fix a pair of policies π, π′ and define\nπh = (π1, . . . , πh−1, π′\nh, . . . , π′\nH),\nwith π1 = π′ and πH = π. By telescoping, we can write\nV\nM,π′\n1\n(s) −V\nM,π\n1\n(s) =\nH\nX\nh=1\nV\nM,πh\n1\n(s) −V\nM,πh+1\n1\n(s).\n(5.14)\nObserve that for each h, we have\nV\nM,πh\n1\n(s) −V\nM,πh+1\n1\n(s) = EM,πh\n\" H\nX\nt=1\nrt | s1 = s\n#\n−EM,πh+1\n\" H\nX\nt=1\nrt | s1 = s\n#\n.\n(5.15)\nHere, one process evolves according to (M, πh) and the one evolves according to (M, πh+1).\nThe processes only differ in the action taken once the state sh is reached. In the former,\nthe action π′(sh) is taken, whereas in the latter it is π(sh). Hence, (5.15) is equal to\nEsh∼(M,π) EM,πh\nQ\nM,π′\nh\n(sh, π′(sh)) −Q\nM,π′\nh\n(sh, π(sh)) | s1 = s\ni\n(5.16)\nwhich can be written as\nE(sh,ah)∼(M,π) EM,πh\nQ\nM,π′\nh\n(sh, π′(sh)) −Q\nM,π′\nh\n(sh, ah) | s1 = s\ni\n.\n(5.17)\n85\nIn contrast to the performance difference lemma, which relates the values of two policies\nunder the same MDP, the next result relates the performance of the same policy under two\ndifferent MDPs. Specifically, the difference in initial value for two MDPs is decomposed\ninto a sum of errors between layer-wise value functions.\nLemma 14 (Bellman residual decomposition): For any pair of MDPs M =\n(P M, RM) and c\nM = (P\nc\nM, R\nc\nM), for any s ∈S, and policies π ∈ΠRNS,\nV\nM,π\n1\n(s) −V\nc\nM,π(s) =\nH\nX\nh=1\nE\nc\nM,π\u0002\nQ\nM,π\nh\n(sh, ah) −rh −V\nM,π\nh+1 (sh+1) | s1 = s\n\u0003\n(5.18)\nHence, for M, c\nM with the same initial state distribution,\nf M(π) −f\nc\nM(π) =\nH\nX\nh=1\nE\nc\nM,π\u0002\nQ\nM,π\nh\n(sh, ah) −rh −V\nM,π\nh+1 (sh+1)\n\u0003\n.\n(5.19)\nIn addition, for any MDP M and function Q = (Q1, . . . , QH, QH+1) with QH+1 ≡0,\nletting πQ,h(s) = arg maxa∈A Qh(s, a), we have\nmax\na∈A Q1(s, a) −V\nM,πQ\n1\n(s) =\nH\nX\nh=1\nEM,πQ\u0002\nQh(sh, ah) −[T M\nh Qh+1](sh, ah) | s1 = s\n\u0003\n. (5.20)\nand, hence,\nEs1∼d1\n\u0002\nmax\na∈A Q1(s1, a)\n\u0003\n−f M(πQ) =\nH\nX\nh=1\nEM,πQ\u0002\nQh(sh, ah) −[T M\nh Qh+1](sh, ah)\n\u0003\n. (5.21)\nNote that for the second part of Lemma 14 Q = (Q1, . . . , QH) can be any sequence of\nfunctions, and need not be a value function corresponding to a particular policy or MDP.\nIt is worth noting that Q gives rise to the greedy policy πQ, which, in turn, gives rise to\nQM,πQ (the value of πQ in model M), but it may well be the case that QM,πQ ̸= Q.\nProof of Lemma 14. We will prove (5.19), and omit the proof for (5.18), which is similar\nbut more verbose. We have\nH\nX\nh=1\nE\nc\nM,π\u0002\nQ\nM,π\nh\n(sh, ah) −rh −V\nM,π\nh+1 (sh+1)\n\u0003\n=\nH\nX\nh=1\nE\nc\nM,π\u0002\nQ\nM,π\nh\n(sh, ah) −V\nM,π\nh+1 (sh+1)\n\u0003\n−E\nc\nM,π\n\" H\nX\nh=1\nrh\n#\n=\nH\nX\nh=1\nE\nc\nM,π\u0002\nQ\nM,π\nh\n(sh, ah) −V\nM,π\nh+1 (sh+1)\n\u0003\n−f\nc\nM(π).\n86\nOn the other hand, since V\nM,π\nh\n(s) = Ea∼πh(s)[Q\nM,π\nh\n(s, a)], a telescoping argument yields\nH\nX\nh=1\nE\nc\nM,π\u0002\nQ\nM,π\nh\n(sh, ah) −V\nM,π\nh+1 (sh+1)\n\u0003\n=\nH\nX\nh=1\nE\nc\nM,π\u0002\nV\nM,π\nh\n(sh) −V\nM,π\nh+1 (sh+1)\n\u0003\n= E\nc\nM,π\u0002\nV\nM,π\n1\n(s1)\n\u0003\n−E\nc\nM,π\u0002\nV\nM,π\nH+1(sH+1)\n\u0003\n= f M(π),\nwhere we have used that V\nM,π\nH+1 = 0, and that both MDPs have the same initial state\ndistribution. We prove (5.21) (omitting the proof of (5.20)) using a similar argument. We\nhave\nH\nX\nh=1\nEM,πQ\u0002\nQh(sh, ah) −rh −max\na∈A Qh+1(sh+1, a)\n\u0003\n=\nH\nX\nh=1\nEM,πQ\u0002\nQh(sh, ah) −max\na∈A Qh+1(sh+1, a)\n\u0003\n−EM,πQ\n\" H\nX\nh=1\nrh\n#\n=\nH\nX\nh=1\nEM,πQ\u0002\nQh(sh, ah) −max\na∈A Qh+1(sh+1, a)\n\u0003\n−f M(πQ).\nSince ah+1 = πQ,h(sh+1) = arg maxa∈A Qh+1(sh+1, a), we have\nEM,πQ\u0002\nQh(sh, ah) −max\na∈A Qh+1(sh+1, a)\n\u0003\n= EM,πQ\u0002\nQh(sh, ah) −Qh+1(sh+1, ah+1)\n\u0003\n,\nand the result follows by telescoping.\nAnother similar analysis tool for MDPs, the simulation lemma, is deferred to Section 6\n(Lemma 23). This result can be proven as a consequence of Lemma 14.\n5.5 Optimism\nTo develop algorithms for regret minimization in unknown MDPs, we turn to the principle\nof optimism, which we have seen is successful in tackling multi-armed bandits and linear\nbandits (in small dimension). Recall that for bandits, Lemma 7 gave a way to decompose\nthe regret of optimistic algorithms into width of confidence intervals. What is the analogue\nof Lemma 7 for MDPs? Thinking of optimistic estimates at the level of expected rewards for\npolicies π is unwieldy, and we need to dig into the structure of these multi-stage decisions. In\nparticular, the approach we employ is to construct a sequence of optimistic value functions\nQ1, . . . , QH which are guaranteed to over-estimate the optimal value function QM,⋆. For\nmulti-armed bandits, implementing optimism amounted to adding “bonuses,” constructed\nfrom past data, to estimates for the reward function. We will construct optimistic value\nfunctions in a similar fashion.\nBefore giving the construction, we introduce a technical\nlemma, which quantifies the error in using such optimistic estimates in terms of Bellman\nresiduals; Bellman residuals measure self-consistency of the optimistic estimates under the\napplication of the Bellman operator.\n87\nLemma 15 (Error decomposition for optimistic policies): Let {Q1, . . . , QH} be\na sequence of functions Qh : S × A →R with the property that for all (s, a),\nQ\nM,⋆\nh\n(s, a) ≤Qh(s, a)\n(5.22)\nand set QH+1 ≡0. Let bπ = (bπ1, . . . , bπH) be such that bπh(s) = arg maxa Qh(s, a). Then\nfor all s ∈S,\nV\nM,⋆\n1\n(s) −V\nM,bπ\n1\n(s) ≤\nH\nX\nh=1\nEM,bπ\u0002\n(Qh −T M\nh Qh+1)(sh, bπ(sh)) | s1 = s\n\u0003\n.\n(5.23)\nLemma 15 tells us that closeness of Qh to the Bellman backup T M\nh Qh+1 implies closeness\nof bπ to πM in terms of the value. As a sanity check, if Qh = Q\nM,⋆\nh\n, the right-hand side\nof (5.23) is zero, since Q\nM,⋆\nh\n= T M\nh Q\nM,⋆\nh+1. Crucially, errors do not accumulate too fast as a\nfunction of the horizon. This fact should not be taken for granted: in general, if Q is not\noptimistic, it could have been the case that small changes in Qh exponentially degrade the\nquality of the policy bπ.\nAnother important aspect of the decomposition (5.23) is the on-policy nature of the\nterms in the sum. Observe that the law of sh for each of the terms is given by executing\nbπ in model M. The distribution of sh is often referred to as the roll-in distribution; when\nthis distribution is induced by the policy executed by the algorithm, we may have a better\ncontrol of the error than in the off-policy case when the roll-in distribution is given by πM\nor another unknown policy.\nProof of Lemma 15. Let V h(s) := maxa∈A Qh(s, a). Just as in the proof of Lemma 7, the\nassumption that Qh is “optimistic” implies that\nQ\nM,⋆\nh\n(sh, πM(sh)) ≤Qh(sh, πM(sh)) ≤Qh(sh, bπ(sh))\nand, hence, V\nM,⋆\n1\n(s) ≤V 1(s). Then, (5.20) applied to Q = Q and πQ = bπ states that\nV 1(s) −V\nM,bπ\n1\n(s) =\nH\nX\nh=1\nEM,bπ\u0002\nQh(sh, ah) −\n\u0002\nT M\nh Qh+1\n\u0003\n(sh, ah) | s1 = s\n\u0003\n.\n(5.24)\nRemark 16: In fact, the proof of Lemma 15 only uses that the initial value Q1 is opti-\nmistic. However, to construct a value function with this property, the algorithms we con-\nsider will proceed by backwards induction, producing optimistic estimates Q1, . . . , QH\nin the process.\n5.6 The UCB-VI Algorithm for Tabular MDPs\nWe now instantiate the principle of optimism to give regret bounds for online reinforcement\nlearning in tabular MDPs. Tabular RL may be thought of as an analogue of finite-armed\n88\nbandits: we assume no structure across states and actions, but require that the state and\naction spaces are small. The regret bounds we present will depend polynomially on S = |S|\nand A = |A|, as well as the horizon H.\nPreliminaries.\nFor simplicity, we assume that the reward function is known to the\nlearner, so that only the transition probabilities are unknown. This does not change the\ndifficulty of the problem in a meaningful way, but allows us to keep notation light.\nAssumption 6: Rewards are deterministic, bounded, and known to the learner: RM\nh (s, a) =\nδrh(s,a) for known rh : S × A →[0, 1], for all M. In addition, assume for simplicity that\nV\nM,⋆\n1\n(s) ∈[0, 1] for any s ∈S.\nDefine, with a slight abuse of notation,\nnt\nh(s, a) =\nt−1\nX\ni=1\nI\n\b\n(si\nh, ai\nh) = (s, a)\n\t\n,\nand\nnt\nh(s, a, s′) =\nt−1\nX\ni=1\nI\n\b\n(si\nh, ai\nh, si\nh+1) = (s, a, s′)\n\t\n,\nas the empirical state-action and state-action-next state frequencies. We can estimate the\ntransition probabilities via\nbP t\nh(s′ | s, a) = nt\nh(s, a, s′)\nnt\nh(s, a) .\n(5.25)\nThe UCB-VI algorithm.\nThe following algorithm, UCB-VI (“Upper Confidence Bound\nValue Iteration”) [16], combines the notion of optimism with dynamic programming.\nUCB-VI\nfor t = 1, . . . , T do\nLet V t\nH+1 ≡1.\nfor h = H, . . . , 1 do\nUpdate nt\nh(s, a), nt\nh(s, a, s′), and bt\nh,δ(s, a), for all (s, a) ∈S × A.\n// bt\nh,δ(s, a) is a bonus computed in (5.27).\nCompute:\nQt\nh(s, a) =\nn\nrh(s, a) + Es′∼bP t\nh(·|s,a) V t\nh+1(s′) + bt\nh,δ(s, a)\no\n∧1.\n(5.26)\nSet V t\nh(s) = maxa∈A Qt\nh(s, a) and bπt\nh(s) = arg maxa∈A Qt\nh(s, a).\nCollect trajectory (st\n1, at\n1, rt\n1), . . . , (st\nH, at\nH, rt\nH) according to bπt.\nThe UCB-VI algorithm will be analyzed using Lemma 15. In constructing functions Qh, we\nwill need to satisfy two goals: (1) ensure that with high probability (5.22) is satisfied, i.e.\nQhs are optimistic; and (2) that Qhs are “self-consistent,” in the sense that the Bellman\nresiduals in (5.23) are small. The second requirement already suggests that we should define\nQh approximately as a Bellman backup T M\nh Qh+1, going backwards for h = H + 1, . . . , 1\nas in dynamic programming, while ensuring the first requirement.\nIn addition to these\nconsiderations, we will have to use a surrogate for the Bellman operator T M\nh , since the model\n89\nM is not known. This is achieved by estimating M using empirical transition frequencies.\nPutting these ideas together gives the update in (5.26). We apply the principle of value\niteration, except that\n1. For each episode t, we augment the rewards rh(s, a) with a “bonus” bt\nh,δ(s, a) designed\nto ensure optimism.\n2. The Bellman operator is approximated using the estimated transition probabilities in\n(5.25).\nThe bonus functions play precisely the same role as the width of the confidence interval in\n(2.19): these bonuses ensure that (5.22) holds with high probability, as we will show below\nin Lemma 16.\nThe following theorem shows that with an appropriate choice of bonus, this algorithm\nachieves a polynomial regret bound.\nTheorem 1: For any δ > 0, UCB-VI with\nbt\nh,δ(s, a) = 2\ns\nlog(2SAHT/δ)\nnt\nh(s, a)\n(5.27)\nguarantees that with probability at least 1 −δ,\nReg ≲HS\n√\nAT ·\np\nlog(SAHT/δ)\nWe mention that a slight variation on Lemma 18 below (using the Freedman inequal-\nity instead of the Azuma-Hoeffding inequality) yields an improved rate of O(H\n√\nSAT +\npoly(H, S, A) log T), and the optimal rate can be shown to be Θ(\n√\nHSAT); this is achieved\nthrough a more careful choice for the bonus bt\nh,δ and a more refined analysis. We remark\nthat care should be taken in comparing results in the literature, as scaling conventions for\nthe individual and cumulative rewards (as in Assumption 6) can vary.\n5.6.1\nAnalysis for a Single Episode\nOur aim is to bound the regret\nReg =\nT\nX\nt=1\nf M(πM⋆) −f M(πt)\nfor UCB-VI. To do so, we first prove several helper lemmas concerning the performance\nwithin each episode t. In what follows, we fix t and drop the superscript t.\nGiven the estimated transitions\n\b bPh(· | s, a)\n\t\nh,s,a, define the estimated MDP c\nM =\nn\nS, A, { bPh}H\nh=1, {RM\nh }H\nh=1, d1\no\n. The associated Bellman operator is\nT\nc\nM\nh Q(s, a) = rh(s, a) + Es′∼bPh(·|s,a) max\na\nQ(s′, a)\n(5.28)\nfor Q : S × A →R. Consider the sequence of functions Qh : S × A →[0, 1], V h : S →[0, 1],\nfor h = 1, . . . , H + 1, with QH+1 ≡0 and\nQh(s, a) =\nn\n[T\nc\nM\nh Qh+1](s, a) + bh,δ(s, a)\no\n∧1,\nand\nV h(s) = max\na\nQh(s, a)\n(5.29)\n90\nfor bonus functions bh,δ : S × A →R to be chosen later. Henceforth, we follow the usual\nnotation that for functions f, g over the same domain, f ≤g indicates pointwise inequality\nover the domain.\nThe first lemma we present shows that as long as the bonuses bh,δ are large enough to\nbound the error between the estimated transition probabilities and true transition proba-\nbilities, the functions Q1, . . . , QH constructed above will be optimistic.\nLemma 16: Suppose we have estimates\n\b bPh(· | s, a)\n\t\nh,s,a and a function bh,δ : S×A →\nR with the property that for all s ∈§, a ∈A,\n\f\f\f\f\f\nX\ns′\nbPh(s′ | s, a)V\nM,⋆\nh\n(s′) −\nX\ns′\nP M\nh (s′ | s, a)V\nM,⋆\nh\n(s′)\n\f\f\f\f\f ≤bh,δ(s, a).\n(5.30)\nThen for all h ∈[H], we have\nQh ≥Q\nM,⋆\nh\n,\nand\nV h ≥V\nM,⋆\nh\n(5.31)\nfor Qh, V h defined in (5.29).\nProof of Lemma 16. The proof proceeds by backward induction on the statement\nV h ≥V\nM,⋆\nh\nwith h = H + 1 down to h = 1. We start with the base case h = H + 1, which is trivial\nbecause V H+1 = V\nM,⋆\nH+1 ≡0. Now, assume V h+1 ≥V\nM,⋆\nh+1, and let us prove the induction\nstep. Fix (s, a) ∈S × A. If Qh(s, a) = 1, then, trivially, Qh(s, a) ≥Q\nM,⋆\nh\n(s, a). Otherwise,\nQh(s, a) = T\nc\nM\nh Qh+1(s, a) + bh,δ(s, a), and thus\nQh(s, a) −Q\nM,⋆\nh\n(s, a) = bh,δ(s, a) + Es′∼bPh(·|s,a) V h+1(s′) −Es′∼P M\nh (·|s,a) V\nM,⋆\nh+1(s′)\n≥bh,δ(s, a) + Es′∼bPh(·|s,a) V\nM,⋆\nh+1(s′) −Es′∼P M\nh (·|s,a) V\nM,⋆\nh+1(s′) ≥0.\nThis, in turn, implies that V h(s) = maxa Qh(s, a) ≥maxa Q\nM,⋆\nh\n(s, a) = V\nM,⋆\nh\n(s), concluding\nthe induction step.\nWe now analyze the effect of using an estimated model c\nM for the Bellman operator\nrather than the true unknown T M\nh .\nLemma 17: Suppose we have estimates\n\b bPh(· | s, a)\n\t\nh,s,a and b′\nh,δ(s, a) : S × A →R\nwith the property that\nmax\nV ∈{0,1}S\n\f\f\f\f\f\nX\ns′\nbPh(s′ | s, a)V (s′) −\nX\ns′\nP M\nh (s′ | s, a)V (s′)\n\f\f\f\f\f ≤b′\nh,δ(s, a)\n(5.32)\nThen the Bellman residual satisfies\nQh −T M\nh Qh+1 ≤(bh,δ + b′\nh,δ) ∧1.\n(5.33)\n91\nfor Qh, V h defined in (5.29).\nProof of Lemma 17. That Qh −T M\nh Qh+1 ≤1 is immediate.\nTo prove the main result,\nobserve that\nQh −T M\nh Qh+1 =\nn\nT\nc\nM\nh Qh+1 + bh,δ\no\n∧1 −T M\nh Qh+1 ≤(T\nc\nM\nh −T M\nh )Qh+1 + bh,δ\n(5.34)\nFor any Q ∈S × A →[0, 1],\n(T\nc\nM\nh −T M\nh )Q(s, a) = Es′∼bPh(·|s,a) max\na\nQ(s′, a) −Es′∼P M\nh (·|s,a) max\na\nQ(s′, a)\n(5.35)\n≤\nmax\nV ∈[0,1]S|Es′∼bPh(·|s,a) V (s′) −Es′∼P M\nh (·|s,a) V (s′)|.\n(5.36)\nSince the maximum is achieved at a vertex of [0, 1]S, the statement follows.\n5.6.2\nRegret Analysis\nWe now bring back the time index t and show that the estimated transition probabilities\nin UCB-VI satisfy conditions of Lemma 16 and Lemma 17, ensuring that the functions\nQt\n1, . . . , Qt\nH are optimistic.\nLemma 18: Let\n\b bP t\nh\n\t\nh∈[H],t∈[T] be defined as in (5.25). Then with probability at least\n1 −δ, the functions\nbt\nh,δ(s, a) = 2\ns\nlog(2SAHT/δ)\nnt\nh(s, a)\n,\nand\nb\n′t\nh,δ(s, a) = 8\ns\nS log(2SAHT/δ)\nnt\nh(s, a)\nsatisfy the assumptions of Lemma 16 and Lemma 17, respectively, for all s ∈S, a ∈A,\nh ∈[H], and t ∈[T] simultaneously.\nProof of Lemma 18. We leave the proof as an exercise.\nProof of Theorem 1. Putting everything together, we can now prove Theorem 1. Under\nthe event in Lemma 18, the functions Qt\n1, . . . , Qt\nH are optimistic, which means that the\nconditions of Lemma 15 hold, and the instantaneous regret on round t (conditionally on\ns1 ∼d1) is at most\nH\nX\nh=1\nEM,bπt\u0002\n(Qt\nh −T M\nh Qt\nh+1)(st\nh, bπt\nh(st\nh)) | s1 = s\n\u0003\n≤\nH\nX\nh=1\nEM,bπt\u0002\n(bh,δ(st\nh, bπt\nh(st\nh)) + b′\nh,δ(st\nh, bπt\nh(st\nh))) ∧1\n\u0003\n,\nwhere the second inequality invokes Lemma 17. Summing over t = 1, . . . , T, and applying\nthe Azuma-Hoeffding inequality, we have that with probability at least 1 −δ, the regret of\nUCB-VI is bounded by\nT\nX\nt=1\nH\nX\nh=1\nEM,bπt\u0002\n(bh,δ(st\nh, bπt\nh(st\nh)) + b′\nh,δ(st\nh, bπt\nh(st\nh))) ∧1\n\u0003\n≲\nT\nX\nt=1\nH\nX\nh=1\n(bh,δ(st\nh, bπt\nh(st\nh)) + b′\nh,δ(st\nh, bπt\nh(st\nh))) ∧1 +\np\nHT log(1/δ).\n92\nUsing the bonus definition in (5.27), the bonus term above is bounded by\nT\nX\nt=1\nH\nX\nh=1\ns\nS log(2SAHT/δ)\nnt\nh(st\nh, bπt\nh(st\nh))\n∧1 ≤\np\nS log(2SAHT/δ)\nT\nX\nt=1\nH\nX\nh=1\n1\np\nnt\nh(st\nh, bπt\nh(st\nh)) ∧1 (5.37)\nThe double summation can be handled in the same fashion as Lemma 8:\nT\nX\nt=1\nH\nX\nh=1\n1\np\nnt\nh(st\nh, bπt\nh(st\nh)) ∧1 =\nH\nX\nh=1\nX\n(s,a)\nT\nX\nt=1\nI {(st\nh, bπt\nh(st\nh)) = (s, a)}\np\nnt\nh(s, a)\n∧1\n≲\nH\nX\nh=1\nX\n(s,a)\nq\nnT\nh(s, a) ≤H\n√\nSAT.\n6. GENERAL DECISION MAKING\nSo far, we have covered three general frameworks for interaction decision making: The\ncontextual bandit problem, the structured bandit problem, and the episodic reinforcement\nlearning problem; all of these frameworks generalize the classical multi-armed bandit prob-\nlem in different directions. In the context of structured bandits, we introduced a complexity\nmeasure called the Decision-Estimation Coefficient (DEC), which gave a generic approach\nto algorithm design, and allowed us to reduce the problem of interactive decision making\nto that of supervised online estimation.\nIn this section, we will build on this develop-\nment on two fronts: First, we will introduce a unified framework for decision making,\nwhich subsumes all of the frameworks we have covered so far. Then, we will show that\ni) the Decision-Estimation Coefficient and its associated meta-algorithm (E2D) extend to\nthe general decision making framework, and ii) boundedness of the DEC is not just suf-\nficient, but actually necessary for low regret, and thus constitutes a fundamental limit.\nAs an application of the general tools we introduce, we will show how to use the (general-\nized) Decision-Estimation Coefficient to solve the problem of tabular reinforcement learning\n(Section 6.6), offering an alternative to the UCB-VI method we introduced in Section 5.\n6.1 Setting\nFor the remainder of the course, we will focus on a framework called Decision Making with\nStructured Observations (DMSO), which subsumes all of the decision making frameworks\nwe have encountered so far.\nThe protocol proceeds in T rounds, where for each round\nt = 1, . . . , T:\n1. The learner selects a decision πt ∈Π, where Π is the decision space.\n2. Nature selects a reward rt ∈R and observation ot ∈O based on the decision, where\nR ⊆R is the reward space and O is the observation space. The reward and observation\nare then observed by the learner.\nWe focus on a stochastic variant of the DMSO framework.\n93\nAssumption 7 (Stochastic Rewards and Observations): Rewards and observa-\ntions are generated independently via\n(rt, ot) ∼M⋆(· | πt),\n(6.1)\nwhere M⋆: Π →∆(R × O) is the underlying model.\nTo facilitate the use of learning and function approximation, we assume the learner has\naccess to a model class M that contains the model M⋆. Depending on the problem do-\nmain, M might consist of linear models, neural networks, random forests, or other complex\nfunction approximators; this generalizes the role of the reward function class F used in\ncontextual/structured bandits. We make the following standard realizability assumption,\nwhich asserts that M is flexible enough to express the true model.\nAssumption 8 (Realizability): The model class M contains the true model M⋆.\nFor a model M ∈M, let EM,π[·] denote the expectation under (r, o) ∼M(π). Further,\nfollowing the notation in Section 5, let\nf M(π) := EM,π[r]\ndenote the mean reward function, and let\nπM := arg max\nπ∈Π\nf M(π)\ndenote the optimal decision with maximal expected reward. Finally, define\nFM := {f M | M ∈M}\n(6.2)\nas the induced class of mean reward functions. We evaluate the learner’s performance in\nterms of regret to the optimal decision for M⋆:\nReg :=\nT\nX\nt=1\nEπt∼pt\n\u0002\nf M⋆(πM⋆) −f M⋆(πt)\n\u0003\n,\n(6.3)\nwhere pt ∈∆(Π) is the learner’s distribution over decisions at round t. Going forward, we\nabbreviate f⋆= f M⋆and π⋆= πM⋆,.\nThe DMSO framework is general enough to capture most online decision making prob-\nlems. Let us first see how it subsumes the structured bandit and contextual bandit problems.\nExample 6.1 (Structured bandits). When there are no observations (i.e., O = {∅}), the\nDMSO framework is equivalent to structured bandits studied earlier in Section 4. Therein,\nwe defined a structured bandit instance by specifying a class F of mean reward functions\nand a general class of reward distributions, such as sub-Gaussian or bounded. In the DMSO\nframework, we may equivalently start with a set of models M and let FM be the induced\nclass (6.2).\nBy changing the class F, this encompasses all of the concrete examples of\nstructured bandit problems we studied in Section 4, including linear bandits, nonparametric\nbandits, and concave/convex bandits.\n◁\n94\nExample 6.2 (Contextual bandits). The DMSO framework readily captures contextual\nbandits (Section 3) with stochastic contexts (see Assumption 2). To make this precise,\nwe will slightly abuse the notation and think of πt as functions mapping the context xt\nto an action in Π = [A]. To this end, on round t, the decision-maker selects a mapping\nπt : X →[A] from contexts to actions, and the context ot = xt is observed at the end of the\nround. This is equivalent to first observing xt and selecting πt(xt) ∈[A].\nFormally, let O = X be the space of contexts, Π = [A] be the set of actions, and\nΠ : X →[A] be the space of decisions.\nThe distribution (r, x) ∼M(π) then has the\nfollowing structure: x ∼DM and r ∼RM(·|x, π(x)) for some context distribution DM and\nreward distribution RM. In other words, the distribution DM for the context x (treated as\nan observation) is part of the model M.\nWe mention in passing that the DMSO framework also naturally extends to the case\nwhen contexts are adversarial rather than i.i.d., as in Section 4.5; see Foster et al. [40].\n◁\nExample 6.3 (Online reinforcement learning). The online reinforcement learning frame-\nwork we introduced in Section 5 immediately falls into the DMSO framework by taking\nΠ = Πrns, rt = PH\nh=1 rt\nh, and ot = τ t. While we have only covered tabular reinforcement\nlearning so far, the literature on online reinforcement learning contains algorithms and\nsample complexity bounds for a rich and extensive collection of different MDP structures\n(e.g., Dean et al. [30], Yang and Wang [87], Jin et al. [48], Modi et al. [65], Ayoub et al.\n[15], Krishnamurthy et al. [55], Du et al. [33], Li [62], Dong et al. [31]). All of these settings\ncorrespond to specific choices for the model class M in the DMSO framework, and we will\ncover this topic in detail in Section 7.\n◁\nWe adopt the DMSO framework because it gives simple, yet unified approach to describ-\ning and understanding what is—at first glance—a very general and seemingly complicated\nproblem. Other examples that are covered by the DMSO framework include:\n• Partially Observed Markov Decision Processes (POMDPs)\n• Bandits with graph-structured feedback\n• Partial monitoring⋆\n6.2 Refresher: Information-Theoretic Divergences\nTo develop algorithms and complexity measures for general decision making, we need a\nway to measure the distance between distributions over abstract observations (this was\nnot a concern for the structured and contextual bandit settings, where we only needed to\nconsider the mean reward function). To do this, we will introduce the notion of the Csiszar\nf-divergence, which generalizes a number of familiar divergences including the Kullback-\nLeibler (KL) divergence, total variation distance, and Hellinger distance.\nLet P and Q be probability distributions over a measurable space (Ω, F). We say that P\nis absolutely continuous with respect to Q if for all events A ∈F, Q(A) = 0 =⇒P(A) = 0;\nwe denote this by P ≪Q.\nFor a convex function f : (0, ∞) →R with f(1) = 0, the\nassociated f-divergence for P and Q is given by\nDf(P ∥Q) := EQ\n\u0014\nf\n\u0012 dP\ndQ\n\u0013\u0015\n(6.4)\n95\nwhenever P ≪Q. More generally, defining p = dP\ndν and q = dQ\ndν for a common dominating\nmeasure ν, we have\nDf(P ∥Q) :=\nZ\nq>0\nqf\n\u0012p\nq\n\u0013\ndν + P(q = 0) · f′(∞),\n(6.5)\nwhere f′(∞) := limx→0+ xf(1/x).\nWe will make use of the following f-divergences, all of which have unique properties\nthat make them useful in different contexts.\n• Choosing f(t) = 1\n2|t −1| gives the total variation (TV) distance\nDTV(P, Q) = 1\n2\nZ \f\f\f\f\ndP\ndν −dQ\ndν\n\f\f\f\fdν,\nwhich can also be written as\nDTV(P, Q) = sup\nA∈F\n|P(A) −Q(A)|.\n• Choosing f(t) = (1 −\n√\nt)2 gives squared Hellinger distance\nD2\nH(P, Q) =\nZ  r\ndP\ndν −\nr\ndQ\ndν\n!2\ndν.\n• Choosing f(t) = t log(t) gives the Kullback-Leibler divergence:\nDKL(P ∥Q) =\n\u001a R\nlog\n\u0000 dP\ndQ\n\u0001\ndP,\nP ≪Q,\n+∞,\notherwise.\nNote that for TV distance and Hellinger distance, we use the notation D(·, ·) rather than\nD(· ∥·) to emphasize that the divergence is symmetric. Other standard examples include\nthe Neyman-Pearson χ2-divergence.\nLemma 19: For all distributions P and Q,\nD2\nTV(P, Q) ≤D2\nH(P, Q) ≤DKL(P ∥Q).\n(6.6)\nIt is known that DTV(P, Q) = 1 if and only if D2\nH(P, Q) = 2, and DTV(P, Q) = 0 if and\nonly if D2\nH(P, Q) = 0 (more generally, D2\nH(P, Q) ≤2DTV(P, Q)). Moreover, they induce\nsame topology, i.e. a sequence converges in one distance if and only if it converges the\nother. KL divergence cannot be bounded by TV distance or Hellinger distance in general,\nbut the following lemma shows that it is possible to relate these quantities if the density\nratios under consideration are bounded.\nLemma 20: Let P and Q be probability distributions over a measurable space (Ω, F).\nIf supF∈F\nP(F)\nQ(F) ≤V , then\nDKL(P ∥Q) ≤(2 + log(V ))D2\nH(P, Q).\n(6.7)\n96\nOther properties we will use include:\n• Boundedness of TV (by 1) and Hellinger (by 2).\n• Triangle inequality for TV and Hellinger distance.\n• The data-processing inequality, which is satisfied by all f-divergences.\n• Chain rule and subadditivity properties for KL and Hellinger divergence (see Lemma\n22).\n• A variational representation for TV distance:\nDTV(P, Q) =\nsup\ng:Ω→[0,1]\n|EP[g] −EQ[g]|\n(6.8)\nSee Polyanskiy [68] for further background.\n6.3 The Decision-Estimation Coefficient for General Decision Making\nDeveloping algorithms for the general decision making framework poses a number of ad-\nditional challenges compared to the basic bandit frameworks we have studied so far. The\nproblem of understanding how to optimally explore and make decisions for a given model\nclass M is deeply connected to the problem of understanding the optimal statistical com-\nplexity (i.e., minimax regret) for M. Any notion of problem complexity needs to capture\nboth i) simple problems like the multi-armed bandit, where the mean rewards serve as\na sufficient statistic, and ii) problems with rich, structured feedback (e.g., reinforcement\nlearning), where observations, or even structure in the noise itself, can provide non-trivial\ninformation about the underlying problem instance. In spite of these apparent difficulties,\nwe will show that by incorporating an appropriate information-theoretic divergence, we can\nuse the Decision-Estimation Coefficient to address these challenges, in a similar fashion to\nSection 4.\nFor a model class M, reference model c\nM ∈M, and scale parameter γ > 0, the Decision-\nEstimation Coefficient for general decision making [40, 43] is defined via\ndecγ(M, c\nM) =\ninf\np∈∆(Π) sup\nM∈M\nEπ∼p\n\u0014\nf M(πM) −f M(π)\n|\n{z\n}\nregret of decision\n−γ · D2\nH\n\u0000M(π), c\nM(π)\n\u0001\n|\n{z\n}\ninformation gain for obs.\n\u0015\n.\n(6.9)\nWe further define\ndecγ(M) =\nsup\nc\nM∈co(M)\ndecγ(M, c\nM).\n(6.10)\nThe DEC in (6.9) should look familiar to the definition we used for structured bandits in\nSection 4 (Eq. (4.15)). The main difference is that instead of being defined over a class F\nof reward functions, the general DEC is defined over the class of models M, and the notion\nof estimation error/information gain has changed to account for this. In particular, rather\n97\nthan measuring information gain via the distance between mean reward functions, we now\nconsider the information gain\nEπ∼p\nh\nD2\nH\n\u0010\nM(π), c\nM(π)\n\u0011i\n,\nwhich measures the distance between the distributions over rewards and observations under\nthe models M and c\nM (for the learner’s decision π). This is a stronger notion of distance since\ni) it incorporates observations (e.g., trajectories for reinforcement learning), and ii) even for\nbandit problems, we consider distance between distributions as opposed to distance between\nmeans; the latter feature means that this notion of information gain can capture fine-grained\nproperties of the models under consideration, such as noise in the reward distribution.\n6.3.1\nBasic Examples\nTo build intuition as to how the general Decision-Estimation Coefficient adapts to the\nstructure of the model class M, let us review a few examples—some familiar, and some\nnew.\nExample 6.4 (Multi-armed bandit with Gaussian rewards). Let Π = [A], R = R, O = {∅}.\nWe define\nMMAB-G = {M : M(π) = N(f(π), 1), f : Π →[0, 1]}.\nWe claim that\ndecγ(MMAB-G) ∝A\nγ .\n(6.11)\nTo prove this, consider the case where c\nM ∈M for simplicity. Recall that we have previ-\nously shown that this behavior holds for the squared error version of the DEC defined in\n(4.15). Thus, it is sufficient to argue that the squared Hellinger divergence for Gaussian\ndistributions reduces to square difference between the means:\nD2\nH\n\u0010\nM(π), c\nM(π)\n\u0011\n∝(f M(π) −f\nc\nM(π))2.\nThe claim will then follow from Proposition 14. To prove this, first note that\nD2\nH\n\u0010\nM(π), c\nM(π)\n\u0011\n≤DKL\n\u0010\nM(π) ∥c\nM(π)\n\u0011\n= 1\n2(f M(π) −f\nc\nM(π))2.\n(6.12)\nIn the other direction, one can directly compute\nD2\nH\n\u0010\nM(π), c\nM(π)\n\u0011\n= 1 −exp\n\u001a\n−1\n8(f M(π) −f\nc\nM(π))2\n\u001b\nand using that 1 −exp{−x} ≥(1 −e−1)x for x ∈[0, 1], we establish\nD2\nH\n\u0010\nM(π), c\nM(π)\n\u0011\n≥c · (f M(π) −f\nc\nM(π))2\nfor c = 1−1/e\n8\n.\n◁\nIn fact, one can show that the general DEC in (6.9) coincides with the basic squared\nerror version from Section 4 for general structured bandit problems, not just multi-armed\nbandits; see Proposition 41.\nLet us next consider a twist on the bandit problem that is more information-theoretic in\nnature, and highlights the need to work with information-theoretic divergences if we want\nto handle general decision making problems.\n98\nExample 6.5 (Bandits with structured noise). Let Π = [A], R = R, O = {∅}. We define\nMMAB-SN = {M1, . . . , MA} ∪\nn\nc\nM\no\nwhere Mi(π) := N(1/2, 1) for π ̸= i and Mi(π) := Ber(3/4) for π = i; we further define\nc\nM(π) := N(1/2, 1) for all π ∈Π. Before proceeding with the calculations, observe that we\ncan solve the general decision making problem when the underlying model is M⋆∈M with\na simple algorithm. It is sufficient to select every action in [A] only once: all suboptimal\nactions have Bernoulli rewards and give r ∈{0, 1} almost surely, while the optimal action\nhas Gaussian rewards, and gives r /∈{0, 1} almost surely. Thus, if we select an action and\nobserve a reward r /∈{0, 1}, we know that we have identified the optimal action.\nThe valuable information contained in the reward distribution is reflected in the Hellinger\ndivergence, which attains its maximum value when comparing a continuous distribution to\na discrete one:\nD2\nH\n\u0010\nMi(π), c\nM(π)\n\u0011\n= 2I {π = i} .\nTo use this property to derive the upper bound on decγ(MMAB-SN, c\nM), first note that the\nmaximum over M in the definition of decγ(MMAB-SN, c\nM) is not attained at M = c\nM, since\nin that case both the divergence and regret terms are zero, irrespective of p. Now, take\np = unif[A]. Then for any M ∈{M1, . . . , MA},\nEπ∼p[f M(πM) −f M(π)] = (1 −1/A)(3/4 −1/2),\nand\ndecγ(M, c\nM) ≲(1 −1/A)(3/4 −1/2) −γ 2\nA ≲I {γ ≤A/4} .\nThis leads to an upper bound\ndecγ(MMAB-SN, c\nM) ≲I {γ ≤A/4}\n(6.13)\nwhich can also be shown to be tight.\n◁\nExample 6.6 (Bandits with Full Information). Consider a “full-information” learning set-\nting. We have Π = [A] and R = [0, 1], and for a given decision π we observe a reward\nr as in the standard multi-armed bandit, but also receive an observation o = (r(π′))π′∈[A]\nconsisting of (counterfactual) rewards for every action.\nFor a given model M, let MR(π) denote the distribution over the reward r for π, and let\nMO(π) denote the distribution of o. Then for any decision π, since all rewards are observed,\nthe data processing inequality implies that for all M, c\nM ∈M and π′ ∈Π,\nD2\nH\n\u0010\nM(π), c\nM(π)\n\u0011\n≥D2\nH\n\u0010\nMO(π), c\nMO(π)\n\u0011\n(6.14)\n= D2\nH\n\u0010\nMO(π′), c\nMO(π′)\n\u0011\n≥D2\nH\n\u0010\nMR(π′), c\nMR(π′)\n\u0011\n.\n(6.15)\nUsing this property, we will show that for any c\nM ∈M,\ndecγ(M, c\nM) ≤1\nγ .\n(6.16)\n99\nComparing to the finite-armed bandit, we see that the DEC for this example is independent\nof A, which reflects the extra information contained in the observation o.\nTo prove (6.16), for a given model c\nM ∈M we choose p = Iπ c\nM (i.e. the decision maker\nselects π c\nM deterministically), and bound Eπ∼p[f M(πM) −f M(π)] by\nf M(πM) −f M(π c\nM) ≤f M(πM) −f M(π c\nM) + f\nc\nM(π c\nM) −f\nc\nM(πM)\n≤2 ·\nmax\nπ∈{πM,π c\nM}|f M(π) −f\nc\nM(π)|\n≤2 ·\nmax\nπ∈{πM,π c\nM} DH\n\u0010\nMR(π), c\nMR(π)\n\u0011\n.\nWe then use the AM-GM inequality, which implies that for any γ > 0,\nmax\nπ∈{πM,π c\nM} D2\nH\n\u0010\nMR(π), c\nMR(π)\n\u0011\n≲γ ·\nmax\nπ∈{πM,π c\nM} D2\nH\n\u0010\nMR(π), c\nMR(π)\n\u0011\n+ 1\nγ\n≤γ · D2\nH\n\u0010\nM(π c\nM), c\nM(π c\nM)\n\u0011\n+ 1\nγ ,\nwhere the final inequality uses (6.14). This certifies that for all M ∈M, the choice for p\nabove satisfies\nEπ∼p\nh\nf M(πM) −f M(π) −γ · D2\nH\n\u0010\nM(π), c\nM(π)\n\u0011i\n≲1\nγ ,\nso we have decγ(M, c\nM) ≲1\nγ .\n◁\nIn what follows, we will show that the different behavior for the DEC for these examples\nreflects the fact that the optimal regret is fundamentally different.\n6.4 E2D Algorithm for General Decision Making\nEstimation to Decision-Making (E2D) for General Decision Making\nparameters: Exploration parameter γ > 0.\nfor t = 1, 2, · · · , T do\nObtain c\nM t from online estimation oracle with (π1, r1, o1), . . . , (πt−1, rt−1, ot−1).\nCompute\n// Minimizer for decγ(M, c\nM t).\npt = arg min\np∈∆(Π)\nsup\nM∈M\nEπ∼p\n\u0014\nf M(πM) −f M(π) −γ · D2\nH\n\u0000M(π), c\nM t(π)\n\u0001\u0015\n.\nSample decision πt ∼pt and update estimation algorithm with (πt, rt, ot).\nEstimation-to-Decisions (E2D), the meta-algorithm based on the DEC that we gave for\nstructured bandits in Section 4, readily extends to general decision making [40, 43]. The\ngeneral version of the meta-algorithm is given above.\nCompared to structured bandits,\nthe main difference is that rather than trying to estimate the reward function f⋆, we now\nestimate the underlying model M⋆. To do so, we appeal once again to the notion of an\nonline estimation oracle, but this time for model estimation.\nAt each timestep t, the algorithm calls invokes an online estimation oracle to obtain an\nestimate c\nM t for M⋆using the data Ht−1 = (π1, r1, o1), . . . , (πt−1, rt−1, ot−1) observed so far.\n100\nUsing this estimate, E2D proceeds by computing the distribution pt that achieves the value\ndecγ(M, c\nM t) for the Decision-Estimation Coefficient. That is, we set\npt = arg min\np∈∆(Π)\nsup\nM∈M\nEπ∼p\n\u0014\nf M(πM) −f M(π) −γ · D2\nH\n\u0000M(π), c\nM t(π)\n\u0001\u0015\n.\n(6.17)\nE2D then samples the decision πt from this distribution and moves on to the next round.\nLike structured bandits, one can show that by running Estimation-to-Decisions in the\ngeneral decision making setting, the regret for decision making is bounded in terms of the\nDEC and a notion of estimation error for the estimation oracle. The main difference is that\nfor general decision making, the notion of estimation error we need to control is the sum of\nHellinger distances between the estimates from the supervised estimation oracle M⋆, which\nwe define via\nEstH :=\nT\nX\nt=1\nEπt∼pt\nh\nD2\nH\n\u0010\nM⋆(πt), c\nM t(πt)\n\u0011i\n.\n(6.18)\nWith this definition, we can show that E2D enjoys the following bound on regret, analogous\nto Proposition 13.\nProposition 26 (Foster et al. [40]): E2D with exploration parameter γ > 0 guar-\nantees that\nReg ≤\nsup\nc\nM∈c\nM\ndecγ(M, c\nM) · T + γ · EstH,\n(6.19)\nalmost surely, where c\nM is any set such that c\nM t ∈c\nM for all t ∈[T].\nNote that we can optimize over the parameter γ in the result above, which yields\nReg ≤inf\nγ>0\n\u001a\nsup\nc\nM∈c\nM\ndecγ(M, c\nM)·T +γ·EstH\n\u001b\n≤2· inf\nγ>0 max\n\u001a\nsup\nc\nM∈c\nM\ndecγ(M, c\nM)·T, γ·EstH\n\u001b\n.\nWe will show in the sequel that for any finite class M, the averaged exponential weights\nalgorithm with the logarithmic loss achieves EstH ≲log(|M|/δ) with probability at least\n1 −δ. For this algorithm, and most others we will consider, one can take c\nM = co(M). In\nfact, one can show (via an analogue of Proposition 24) that for any c\nM, even if c\nM /∈co(M),\nwe have decγ(M, c\nM) ≤supc\nM∈co(M) deccγ(M, c\nM) ≤deccγ(M) for any absolute constant\nc > 0. This means we can restrict our attention to the convex hull without loss of generality.\nPutting these facts together, we see that for any finite class, it is possible to achieve\nReg ≤decγ(M) · T + γ · log(|M|/δ)\n(6.20)\nwith probability at least 1 −δ.\nProof of Proposition 26. We write\nReg =\nT\nX\nt=1\nEπt∼pt\n\u0002\nf M⋆(πM⋆) −f M⋆(πt)\n\u0003\n=\nT\nX\nt=1\nEπt∼pt\n\u0002\nf M⋆(πM⋆) −f M⋆(πt)\n\u0003\n−γ · Eπt∼pt\nh\nD2\nH\n\u0000M⋆(πt), c\nM t(πt)\n\u0001i\n+ γ · EstH.\n101\nFor each t, since M⋆∈M, we have\nEπt∼pt\n\u0002\nf M⋆(πM⋆) −f M⋆(πt)\n\u0003\n−γ · Eπt∼pt\nh\nD2\nH\n\u0000M⋆(πt), c\nM t(πt)\n\u0001i\n≤sup\nM∈M\nEπt∼pt[f M(πM) −f M(πt)] −γ · Eπt∼pt\nh\nD2\nH\n\u0000M(πt), c\nM t(πt)\n\u0001i\n=\ninf\np∈∆(Π) sup\nM∈M\nEπ∼p\nh\nf M(πM) −f M(π) −γ · D2\nH\n\u0000M(π), c\nM t(π)\n\u0001i\n= decγ(M, c\nM t).\n(6.21)\nSumming over all rounds t, we conclude that\nReg ≤\nsup\nc\nM∈c\nM\ndecγ(M, c\nM) · T + γ · EstH.\nExamples for the upper bound.\nWe now revisit the examples from Section 6.3 and\nuse E2D and Proposition 26 to derive regret bounds for them.\nExample 6.4 (cont’d). For the Gaussian bandit problem from Example 6.4, plugging the\nbound decγ(MMAB-G) ≲A/γ into Proposition 26 yields\nReg ≲AT\nγ\n+ γ · EstH,\nChoosing γ =\np\nAT/EstH balances the terms above and gives\nReg ≲\np\nAT · EstH.\n◁\nExample 6.5 (cont’d). For the bandit-type problem with structured noise from Exam-\nple 6.5, the bound decγ(MMAB-SN) ≲I {γ ≤A/4} yields\nReg ≲I {γ ≤A/4} · T + γ · EstH.\nWe can choose γ = A, which gives\nReg ≲A · EstH.\n◁\n6.4.1\nOnline Estimation with Hellinger Distance\nLet us now give some more detail as to how to perform the online model estimation required\nby Proposition 26. Model estimation is a more challenging problem than regression, since we\nare estimating the underlying conditional distribution rather than just the conditional mean.\nIn spite of this difficulty, estimating the model M⋆with respect to Hellinger distance is a\nclassical problem that we can solve using the online learning tools introduced in Section 1.6;\nin particular, online conditional density estimation with the log loss. This generalizes the\nmethod of online regression employed in Sections 3 and 4.\n102\nInstead of directly performing estimation with respect to Hellinger distance, the simplest\nway to develop conditional density estimation algorithms is to work with the logarithmic\nloss. Given a tuple (πt, rt, ot), define the logarithmic loss for a model M as\nℓt\nlog(M) = log\n\u0012\n1\nmM(rt, ot | πt)\n\u0013\n,\n(6.22)\nwhere we define mM(·, · | π) as the conditional density for (r, o) under M. We define regret\nunder the logarithmic loss as:\nRegKL =\nT\nX\nt=1\nℓt\nlog(c\nM t) −inf\nM∈M\nT\nX\nt=1\nℓt\nlog(M).\n(6.23)\nThe following result shows that a bound on the log-loss regret immediately yields a bound\non the Hellinger estimation error.\nLemma 21: For any online estimation algorithm, whenever Assumption 8 holds, we\nhave\nE[RegKL] ≥E\n\" T\nX\nt=1\nDKL\n\u0010\nM⋆(πt) ∥c\nM t(πt)\n\u0011#\n,\n(6.24)\nso that\nE[EstH] ≤E[RegKL].\n(6.25)\nFurthermore, for any δ ∈(0, 1), with probability at least 1 −δ,\nEstH ≤RegKL + 2 log(δ−1).\n(6.26)\nThis result is desirable because regret minimization with the logarithmic loss is a well-\nstudied problem in online learning. Efficient algorithms are known for model classes of\ninterest [27, 84, 51, 44, 67, 70, 38, 63], and this is complemented by theory which provides\nminimax rates for generic model classes [78, 66, 24, 18]. One example we have already seen\n(Section 1) is the averaged exponential weights method, which guarantees\nRegKL ≤log|M|\nfor finite classes M. Another example is that for linear models, where (i.e., mM(r, o | π) =\n⟨ϕ(r, o, π), θ⟩for a fixed feature map in ϕ ∈Rd), algorithms with RegKL = O(d log(T)) are\nknown [72, 78]. All of these algorithms satisfy c\nM = co(M). We refer the reader to Chapter\n9 of [25] for further examples and discussion.\nWhile (6.25) is straightforward, (6.26) is rather remarkable, as the remainder term does\nnot scale with T. Indeed, a naive attempt at applying concentration inequalities to control\nthe deviations of the random quantities EstH and RegKL would require boundedness of the\nloss function, which is problematic because the logarithmic loss can be unbounded. The\nproof exploits unique properties of the moment generating function for the log loss.\n6.5 Decision-Estimation Coefficient: Lower Bound on Regret\nUp to this point, we have been focused on developing algorithms that lead to upper bounds\non regret for specific model classes.\nWe now turn our focus to lower bounds, and the\n103\nquestion of optimality: That is, for a given class of models M, what is the best regret that\ncan be achieved by any algorithm? We will show that in addition to upper bounds, the\nDecision-Estimation Coefficient actually leads to lower bounds on the optimal regret.\nBackground: Minimax regret.\nWhat does it mean to say that an algorithm is optimal\nfor a model class M? There are many notions of optimality, but in this course we will focus\non minimax optimality, which is one of the most basic and well-studied notions.\nFor a model class M, we define the minimax regret via14\nM(M, T) =\ninf\np1,...,pT\nsup\nM⋆∈M\nEM⋆,p[Reg(T)],\n(6.27)\nwhere pt = pt(· | Ht−1) is the algorithm’s strategy for step t (a function of the history Ht−1),\nand where we write regret as Reg(T) to make the dependence on T explicit. Intuitively,\nminimax regret asks what is the best any algorithm can perform on a worst-case model\n(in M) possibly chosen with the algorithm in mind. Another way to say this is: For any\nalgorithm, there exists a model in M for which E[Reg(T)] ≥M(M, T). We will say that\nan algorithm is minimax optimal if it achieves (6.27) up to absolute constants that do not\ndepend on M or T.\n6.5.1\nThe Constrained Decision-Estimation Coefficient\nWe now show how to lower bound the minimax regret for any model class M in terms of\nthe DEC for M. Instead of working with the quantity decγ(M) appearing in Proposition\n26 directly, it will be more convenient to work with a related quantity called the constrained\nDecision-Estimation Coefficient, which we define for a parameter ε > 0 as15\ndecc\nε(M, c\nM) =\ninf\np∈∆(Π) sup\nM∈M\nn\nEπ∼p[f M(πM) −f M(π)] | Eπ∼p\nh\nD2\nH\n\u0010\nM(π), c\nM(π)\n\u0011i\n≤ε2o\n,\nwith\ndecc\nε(M) :=\nsup\nc\nM∈co(M)\ndecc\nε(M ∪{c\nM}, c\nM).\nThis is similar to the definition for the DEC we have been working with so far— which\nwe will call the offset DEC going forward—-except that it places a hard constraint on the\ninformation gain as opposed to subtracting the information gain.\nBoth quantities have\na similar interpretation, since subtracting the information gain implicitly biases the max\nplayer towards model where the gain is small. Indeed, the offset DEC can be thought of as\n14Here, for any algorithm p = p1, . . . , pT , EM⋆,p denotes the expectation with respect to the observation\nprocess (rt, ot) ∼M ⋆(πt) and any randomization used by the algorithm, when M ⋆is the true model.\n15We adopt the convention that the value of decc\nε(M, c\nM) is zero is there exists p such that the set of\nM ∈M with Eπ∼p\nh\nD2\nH\n\u0010\nM(π), c\nM(π)\n\u0011i\n≤ε2 is empty.\n104\na Lagrangian relaxation of the constrained DEC, and always upper bounds it via\ndecc\nε(M, c\nM) =\ninf\np∈∆(Π) sup\nM∈M\nn\nEπ∼p[f M(πM) −f M(π)] | Eπ∼p\nh\nD2\nH\n\u0010\nM(π), c\nM(π)\n\u0011i\n≤ε2o\n=\ninf\np∈∆(Π) sup\nM∈M\ninf\nγ≥0\nn\nEπ∼p[f M(πM) −f M(π)] −γ\n\u0010\nEπ∼p\nh\nD2\nH\n\u0010\nM(π), c\nM(π)\n\u0011i\n−ε2\u0011o\n∨0\n≤inf\nγ≥0\ninf\np∈∆(Π) sup\nM∈M\nn\nEπ∼p[f M(πM) −f M(π)] −γ\n\u0010\nEπ∼p\nh\nD2\nH\n\u0010\nM(π), c\nM(π)\n\u0011i\n−ε2\u0011o\n∨0\n= inf\nγ≥0\nn\ndecγ(M, c\nM) + γε2o\n∨0.\nFor the opposite direction, it is straightforward to show that\ndecγ(M) ≲decc\nγ−1/2(M).\nThis inequality is lossy, but cannot be improved in general. That is, there some classes\nfor which the constrained DEC is meaningfully smaller than the offset DEC. However, it is\npossible to relate the two quantities if we restrict to a “localized” sub-class of models that\nare not “too far” from the reference model c\nM.\nProposition 27: Given a model c\nM and parameter α, define the localized subclass\naround c\nM via\nMα(c\nM) =\nn\nM ∈M : f\nc\nM(π c\nM) ≥f M(πM) −α\no\n.\n(6.28)\nFor all ε > 0 and γ ≥c1 · ε−1, we have\ndecc\nε(M) ≤c3 ·\nsup\nγ≥c1ε−1\nsup\nc\nM∈co(M)\ndecγ(Mα(ε,γ)(c\nM), c\nM),\n(6.29)\nwhere α(ε, γ) := c2 · γε2, and c1, c2, c3 > 0 are absolute constants.\nFor many “well-behaved” classes one can consider (e.g., multi-armed bandits and linear\nbandits), one has decγ(Mα(ε,γ)(c\nM), c\nM) ≈decγ(M, c\nM) whenever decγ(M, c\nM) ≈γε2 (that\nis, localization does not change the complexity), so that lower bounds in terms of the\nconstrained DEC immediately imply lower bounds in terms of the offset DEC. In general,\nthis is not the case, and it turns out that it is possible to obtain tighter upper bounds that\ndepend on the constrained DEC by using a refined version of the E2D algorithm. We refer\nto Foster et al. [43] for details and further background on the constrained DEC.\n6.5.2\nLower Bound\nThe main lower bound based on the constrained DEC is as follows.\nProposition 28 (DEC Lower Bound [43]): Let εT := c ·\n1\n√\nT , where c > 0 is a\nsufficiently small numerical constant. For all T such that the conditiona\ndecc\nεT (M) ≥10εT\n(6.30)\n105\nis satisfied, it holds that for any algorithm, there exists a model M ∈M for which\nE[Reg(T)] ≳decc\nεT (M) · T.\n(6.31)\naThe numerical constant here is not important.\nProposition 28 shows that for any algorithm and model class M, the optimal regret\nmust scale with the constrained DEC in the worst-case. As a concrete example, we will\nshow in the sequel that for the multi-armed bandit with A actions, decc\nε(M) ∝ε\n√\nA, which\nleads to\nE[Reg] ≳\n√\nAT.\nWe mention in passing that by combining Proposition 28 with Proposition 27, we obtain\nthe following lower bound based on the (localized) offset DEC.\nCorollary 1: Fix T ∈N. Then for any algorithm, there exists a model M ∈M for\nwhich\nE[Reg(T)] ≳\nsup\nγ≳\n√\nT\nsup\nc\nM∈co(M)\ndecγ(Mα(T,γ)(c\nM), c\nM),\n(6.32)\nwhere α(T, γ) := c · γ/T for an absolute constant c > 0\nThe DEC is necessary and sufficient.\nTo understand the significance of Proposition\n28 more broadly, we state but do not prove the following upper bound on regret based on\nthe constrained DEC, which is based on a refined variant of E2D.\nProposition 29 (Upper bound for constrained DEC [43]): Let M be a finite\nclass, and set εT := c·\nq\nlog(|M|/δ)\nT\n, where c > 0 is a sufficiently large numerical constant.\nUnder appropriate technical conditions, there exists an algorithm that achieves\nE[Reg(T)] ≲decc\nεT (M) · T\n(6.33)\nwith probability at least 1 −δ.\nThis matches the lower boud in Proposition 28 upper to a difference in the radius: we have\nεT ∝\nq\n1\nT for the lower bound, and εT ∝\nq\nlog(|M|/δ)\nT\nfor the upper bound. This implies\nthat for any class where log|M| < ∞, the constrained DEC is necessary and sufficient for\nlow regret. By the discussion in the prequel, a similar conclusion holds for the offset DEC\n(albeit, with a polynomial loss in rate).\nThe interpretation of the log|M| gap between\nthe upper and lower bounds is that the DEC is capturing the complexity of exploring the\ndecision space, but the statistical capacity required to estimate the underlying model is a\nseparate issue which is not captured.\n6.5.3\nProof of Proposition 28\nBefore proving Proposition 28, let us give some background on a typical approach to proving\nlower bounds on the minimax regret for a decision making problem.\n106\nAnatomy of a lower bound.\nHow should one go about proving a lower bound on the\nminimax regret in (6.27)? We will follow a general recipe which can be found throughout\nstatistics, information theory, and decision making [32, 89, 83]. The approach will be to\nfind a pair of models M and c\nM that satisfy the following properties:\n1. Any algorithm with regret much smaller than the DEC must query substantially\ndifferent decisions in Π depending on whether the underlying model is M or c\nM.\nIntuitively, this means that any algorithm that achieves low regret must be able to\ndistinguish between the two models.\n2. M and c\nM are “close” in a statistical sense (typically via total variation distance or\nanother f-divergence), which implies via change-of-measure arguments that the deci-\nsions played by any algorithm which interacts with the models only via observations\n(in our case, (πt, rt, ot)) will be similar for both models. In other words, the models\nare difficult to distinguish.\nOne then concludes that the algorithm must have large regret on either M or c\nM.\nTo make this approach concrete, classical results in statistical estimation and supervised\nlearning choose the models M and c\nM in a way that is oblivious to the algorithm under\nconsideration [32, 89, 83]. However, due to the interactive nature of the decision making\nproblem, the lower bound proof we present now will choose the models in an adaptive\nfashion.\nSimplifications.\nRather than proving the full result in Proposition 28, we will make the\nfollowing simplifying assumptions:\n• There exists a constant C such that\nDKL\n\u0000M(π) ∥M′(π)\n\u0001\n≤C · D2\nH\n\u0000M(π), M′(π)\n\u0001\n(6.34)\nfor all M, M′ ∈M and π ∈Π.\n• Rather than proving a lower bound that scales with decc\nε(M) = supc\nM∈co(M) decc\nε(M∪\n{c\nM}, c\nM), we will prove a weaker lower bound that scales with supc\nM∈M decc\nε(M, c\nM).\nWe refer to Foster et al. [43] for a full proof that removes these restrictions.\nPreliminaries.\nWe use the following technical lemma for the proof of Proposition 28.\nLemma 22 (Chain Rule for KL Divergence): Let (X1, F1), . . . , (Xn, Fn) be a\nsequence of measurable spaces, and let X i = Qi\ni=t Xt and F i = Ni\nt=1 Ft. For each\ni, let Pi(· | ·) and Qi(· | ·) be probability kernels from (X i−1, F i−1) to (Xi, Fi). Let P\nand Q be the laws of X1, . . . , Xn under Xi ∼Pi(· | X1:i−1) and Xi ∼Qi(· | X1:i−1)\nrespectively. Then it holds that\nDKL(P ∥Q) = EP\n\" n\nX\ni=1\nDKL(Pi(· | X1:i−1) ∥Qi(· | X1:i−1))\n#\n.\n(6.35)\n107\n⇧\npc\nM\npM\n⇧\nf c\nM\nf M\n⇡M\n⇡c\nM\nGM\nFigure 9: Models M and c\nM with corresponding mean rewards and average action distri-\nbutions. The overlap between the action distributions is at least 0.9, while near-optimal\nchoices for one model incur large regret for the other.\nProof of Proposition 28. Fix T ∈N and consider any fixed algorithm, which we recall is\ndefined by a sequence of mappings p1, . . . , pT, where pt = pt(· | Ht−1). Let PM denote the\ndistribution over HT for this algorithm when M is the true model, and let EM denote the\ncorresponding expectation.\nViewed as a function of the history Ht−1, each pt is a random variable, and we can\nconsider its expected value under the model M. To this end, for any model M ∈M, let\npM := EM\n\"\n1\nT\nT\nX\nt=1\npt\n#\n∈∆(Π)\nbe the algorithm’s average action distribution when M is the true model. Our aim is to\nshow that we can find a model in M for which the algorithm’s regret is at least as large as\nthe lower bound in (6.32).\nLet T ∈N, and fix a value ε > 0 to be chosen momentarily. Fix an arbitrary model\nc\nM ∈M and set\nM = arg max\nM∈M\nn\nEπ∼p c\nM [f M(πM) −f M(π)] | Eπ∼p c\nM\nh\nD2\nH\n\u0010\nM(π), c\nM(π)\n\u0011i\n≤ε2o\n,\n(6.36)\nThe model M should be thought of as a “worst-case alternative” g to c\nM, but only for the\nspecific algorithm under consideration. We will show that the algorithm needs to have large\nregret on either M or c\nM. To this end, we establish some basic properties; let us abbreviate\ngM(π) = f M(πM) −f M(π) going forward:\n• For all models M, we have\n1\nT EM[Reg(T)] = Eπ∼pM [gM(π)].\n(6.37)\nSo, to prove the desired lower bound, we need to show that either Eπ∼pM [gM(π)] or\nEπ∼p c\nM\n\u0002\ng\nc\nM(π)\n\u0003\nis large.\n• By the definition of the constrained DEC, we have\nEπ∼p c\nM [gM(π)] ≥decc\nε(M, c\nM) =: ∆,\n(6.38)\n108\nsince by (6.36), the model M is the best response to a potentially suboptimal choice\np c\nM.\nThis is almost what we want, but there is a mismatch in models, since gM\nconsiders the model M while p c\nM considers the model c\nM.\n• Using the chain rule for KL divergence, we have\nDKL\n\u0010\nP\nc\nM ∥PM\u0011\n= E\nc\nM\n\" T\nX\nt=1\nEπt∼pt DKL\n\u0010\nc\nM(πt) ∥M(πt)\n\u0011#\n≤C · E\nc\nM\n\" T\nX\nt=1\nEπt∼pt D2\nH\n\u0010\nc\nM(πt), M(πt)\n\u0011#\n= CT · Eπ∼p c\nM\nh\nD2\nH\n\u0010\nc\nM(π), M(π)\n\u0011i\n.\nTo see why the first equality holds, we apply the chain rule to the sequence π1, z1, . . . , πT, zT\nwith zt = (rt, ot). Let us use the bold notation zt to refer to a random variable under\nconsideration, and let zt refer to its realization. Then we have\nDKL\n\u0010\nP\nc\nM ∥PM\u0011\n= E\nc\nM\n\" T\nX\nt=1\nDKL\n\u0010\nP\nc\nM(zt|Ht−1, πt) ∥PM(zt|Ht−1, πt)\n\u0011\n+ DKL\n\u0010\nP\nc\nM(πt|Ht−1 ∥PM(πt|Ht−1)\n\u0011#\n= E\nc\nM\n\" T\nX\nt=1\nDKL\n\u0010\nc\nM(πt) ∥M(πt)\n\u0011#\nsince conditionally on Ht−1, the law of πt does not depend on the model.\nWe can now choose ε = c1 ·\n1\n√\nCT , where c1 > 0 is a sufficiently small numerical\nconstant, to ensure that\nD2\nTV\n\u0010\nP\nc\nM, PM\u0011\n≤DKL\n\u0010\nP\nc\nM ∥PM\u0011\n≤1/100.\n(6.39)\nIn other words, with constant probability, the algorithm can fail to distinguish M and\nc\nM.\nFinally, we will make use of the fact that since rewards are in [0, 1], we have\nEπ∼p c\nM\nh\nf M(π) −f\nc\nM(π)\ni\n≤Eπ∼p c\nM\nh\nDTV\n\u0010\nM(π), c\nM(π)\n\u0011i\n≤\nr\nEπ∼p c\nM\nh\nD2\nH\n\u0010\nM(π), c\nM(π)\n\u0011i\n≤ε.\n(6.40)\nStep 1.\nDefine GM = {π ∈Π | gM(π) ≤∆/10}. Observe that\nEπ∼pM [gM(π)] ≥∆\n10 · pM(π /∈GM) ≥∆\n10 · (p c\nM(π /∈GM) −DTV(pM, p c\nM))\n(6.41)\n≥∆\n10 · (p c\nM(π /∈GM) −1/10),\n(6.42)\nsince DTV(pM, p c\nM) ≤DTV\n\u0000PM, P\nc\nM\u0001\n≤1/10 by the data-processing inequality and (6.39).\nGoing forward, let us assume that\nEπ∼p c\nM\n\u0002\ng\nc\nM(π)\n\u0003\n≤∆/10,\n(6.43)\nor else we are done, by (6.37). Our aim is to show that under this assumption, p c\nM(π /∈\nGM) ≥1/2, which will imply that Eπ∼pM [gM(π)] ≳∆via (6.42).\n109\nStep 2.\nBy adding the inequalities (6.43) and (6.38), we have that\nf M(πM) −f\nc\nM(π c\nM) ≥Eπ∼p c\nM\nh\ngM(π) −g\nc\nM(π)\ni\n−Eπ∼p c\nM\nh\n|f M(π) −f\nc\nM(π)|\ni\n≥9\n10∆−Eπ∼p c\nM\nh\n|f M(π) −f\nc\nM(π)|\ni\n.\nIn addition, by (6.40), we have Eπ∼p c\nM\n\u0002\n|f M(π) −f\nc\nM(π)|\n\u0003\n≤ε, so that\nf M(πM) −f\nc\nM(π c\nM) ≥9\n10∆−ε.\n(6.44)\nHence, as long as ε ≤1\n10∆, which is implied by (6.30), we have\nf M(πM) −f\nc\nM(π c\nM) ≥4\n5∆.\n(6.45)\nStep 3.\nObserve that if π ∈GM, then\n|f M(π) −f\nc\nM(π)|+ ≥|f M(πM) −f\nc\nM(π) −∆/10|+ ≥|f M(πM) −f\nc\nM(π c\nM) −∆/10|+ ≥7\n10∆,\nwhere we have used (6.45). As a result, using (6.40),\nε ≥Eπ∼p c\nM\nh\n|f M(π) −f\nc\nM(π)|+\ni\n≥7\n10∆· p c\nM(π ∈GM).\nHence, since ε ≤∆/10 by (6.30), we have\n∆\n10 ≥7\n10∆· p c\nM(π ∈GM),\nor p c\nM(π ∈GM) ≤1/7. Combining this with (6.42) gives\n1\nT EM[Reg(T)] = Eπ∼pM [gM(π)] ≥∆\n10 · (1 −1/7 −1/10) ≥∆\n20.\nFinishing up.\nNote that since the choice of c\nM ∈M for this lower bound was arbitrary,\nwe are free to choose c\nM to maximize decc\nε(M, c\nM).\n6.5.4\nExamples for the Lower Bound\nWe now instantiate the lower bound in Proposition 28 for concrete model classes of interest.\nWe begin by revisiting the examples at the beginning of the section.\nExample 6.4 (cont’d). Let us lower bound the constrained DEC for the Gaussian ban-\ndit problem from Example 6.4. Set c\nM(π) = N(1/2, 1), and let {M1, . . . , MA} ⊆M be\na sub-family of models with Mi(π) = N(f Mi(π), 1), where f Mi(π) :=\n1\n2 + ∆I {π = i}\nfor a parameter ∆whose value will be chosen in a moment.\nObserve that for all i,\nEπ∼p\nh\nD2\nH\n\u0010\nMi(π), c\nM(π)\n\u0011i\n≤\n1\n2∆2p(i) by (6.12), and Eπ∼p\n\u0002\nf Mi(πMi) −f Mi(π)\n\u0003\n= (1 −\np(i))∆, so we can lower bound\ndecc\nε(M, c\nM) =\ninf\np∈∆(Π) sup\nM∈M\nn\nEπ∼p[f M(πM) −f M(π)] | Eπ∼p\nh\nD2\nH\n\u0010\nM(π), c\nM(π)\n\u0011i\n≤ε2o\n≥\ninf\np∈∆(Π) max\ni\n\u001a\n(1 −p(i))∆| p(i)∆2\n2 ≤ε2\n\u001b\n110\nFor any p, there exists i such that p(i) ≤1/A. If we choose ∆= ε ·\n√\n2A, this choice for i\nwill satisfy the constraint p(i)∆2\n2 ≤ε2, and we will be left with\ndecc\nε(M, c\nM) ≥(1 −p(i))∆≥ε\np\nA/2,\nsince 1 −p(i) ≥1/2.\nPlugging this lower bound on the constrained Decision-Estimation Coefficient into Propo-\nsition 28 yields\nE[Reg] ≥eΩ(\n√\nAT).\n◁\nGeneralizing the argument above, we can prove a lower bound on the Decision-Estimation\nCoefficient for any model class M that “embeds” the multi-armed bandit problem in a cer-\ntain sense.\nProposition 30: Let a reference model c\nM be given, and suppose that a class M con-\ntains a sub-class {M1, . . . , MN} and collection of decisions π1, . . . , πN with the property\nthat for all i:\n1. D2\nH\n\u0010\nMi(π), c\nM(π)\n\u0011\n≤β2 · I {π = πi}.\n2. f Mi(πMi) −f Mi(π) ≥α · I {π ̸= πi}.\nThen\ndecc\nε(M, c\nM) ≳α · I\nn\nε ≥β/\n√\nN\no\n.\nThe examples that follow can be obtained by applying this result with an appropriate\nsub-family.\nExample 6.5 (cont’d). Recall the bandit-type problem with structured noise from Exam-\nple 6.5, where we have M = {M1, . . . , MA}, with Mi(π) = N(1/2, 1)I {π ̸= i}+Ber(3/4)I {π = i}.\nIf we set c\nM(π) = N(1/2, 1), then this family satisfies the conditions of Proposition 30 with\nα = 1/4 and β2 = 2. As a result, we have decc\nε(MMAB-SN) ≳I\nn\nε ≥\np\n2/A\no\n, which yields\nE[Reg] ≳eO(A)\nif we apply Proposition 28.\n◁\nExample 6.6 (cont’d). Consider the full-information variant of the bandit setting in\nExample 6.6. By adapting the argument in Example 6.4, one can show that\ndecc\nε(M) ≳ε,\nwhich leads to a lower bound of the form\nE[Reg] ≳\n√\nT.\n◁\n111\nNext, we revisit some of the structured bandit classes considered in Section 4.\nExample 6.7. Consider the linear bandit setting in Section 4.3.2, with F = {π 7→⟨θ, ϕ(π)⟩| θ ∈Θ},\nwhere Θ ⊆Bd\n2(1) is a parameter set and ϕ : Π →Rd is a fixed feature map that is known to\nthe learner. Let M be the set of all reward distributions with f M ∈F and 1-sub-Gaussian\nnoise. Then\ndecc\nε(M) ≳ε\n√\nd,\nwhich gives\nE[Reg] ≳\n√\ndT.\n◁\nExample 6.8. Consider the Lipschitz bandit setting in Section 4.3.3, where Π is a metric\nspace with metric ρ, and\nF = {f : Π →[0, 1] | f is 1-Lipschitz w.r.t ρ}.\nLet M be the set of all reward distributions with f M ∈F and 1-sub-Gaussian noise. Let\nd > 0 be such that the covering number for Π satisfies\nNρ(Π, ε) ≥ε−d.\nThen\ndecc\nε(M) ≳ε\n2\nd+2 ,\nwhich leads to E[Reg] ≳T\nd+1\nd+2 .\n◁\nSee Foster et al. [40, 43] for further details.\n6.6 Decision-Estimation Coefficient and E2D: Application to Tabular RL\nIn this section, we use the Decision-Estimation Coefficient and E2D meta-algorithm to\nprovide regret bounds for the tabular reinforcement learning. This will be the most complex\nexample we consider in this section, and showcases the full power of DEC for general\ndecision making. In particular, the example will show how the DEC can take advantage\nof the observations ot, in the form of trajectories. This will provide an alternative to the\noptimistic algorithm (UCB-VI) we introduced in Section 5, and we will build on this approach\nto give guarantees for reinforcement learning with function approximation in Section 7.\nTabular reinforcement learning.\nWhen we view tabular reinforcement learning as a\nspecial case of the general decision making framework, M is the collection of all non-\nstationary MDPs M =\n\b\nS, A, {P M\nh }H\nh=1, {RM\nh }H\nh=1, d1\n\t\n(cf.\nSection 5), with state space\nS = [S], action space A = [A], and horizon H. The decision space Π = Πrns is the collection\nof all randomized, non-stationary Markov policies (cf.\nExample 6.3).\nWe assume that\nrewards are normalized such that PH\nh=1 rh ∈[0, 1] almost surely (so that R = [0, 1]). Recall\nthat for each M ∈M, {P M\nh }H\nh=1 and {RM\nh }H\nh=1 denote the associated transition kernels and\nreward distributions, and d1 is the initial state distribution.\n112\nOccupancy measures.\nThe results we present make use of the notion of occupancy\nmeasures for an MDP M. Let PM,π(·) denote the law of a trajectory evolving under MDP\nM and policy π. We define state occupancy measures via\nd\nM,π\nh\n(s) = PM,π(sh = s)\nand state-action occupancy measures via\nd\nM,π\nh\n(s, a) = PM,π(sh = s, ah = a).\nNote that we have d\nM,π\n1\n(s) = d1(s) for all M and π.\nBounding the DEC for tabular RL.\nRecall, that to certify a bound on the DEC, we\nneed to—given any parameter γ > 0 and estimator c\nM, exhibit a distribution (or, “strategy”)\np such that\nsup\nM∈M\nEπ∼p\nh\nf M(πM) −f M(π) −γ · D2\nH\n\u0010\nM(π), c\nM(π)\n\u0011i\n≤decγ(M, c\nM)\nfor some upper bound decγ(M, c\nM). For tabular RL, we will choose p using an algorithm\ncalled Policy Cover Inverse Gap Weighting. As the name suggests, the approach combines\nthe inverse gap weighting technique introduced in the multi-armed bandit setting with the\nnotion of a policy cover—that is, a collection of policies that ensures good coverage on every\nstate [33, 64, 47].\nPolicy Cover Inverse Gap Weighting (PC-IGW)\nparameters: Estimated model c\nM, Exploration parameter η > 0.\nDefine inverse gap weighted policy cover Ψ = {πh,s,a}h∈[H],s∈[S],a∈[A] via\nπh,s,a = arg max\nπ∈Πrns\nd\nc\nM,π\nh\n(s, a)\n2HSA + η(f c\nM(π c\nM) −f c\nM(π)).\n(6.46)\nFor each policy π ∈Ψ ∪{π c\nM}, define\np(π) =\n1\nλ + η(f c\nM(π c\nM) −f c\nM(π)),\n(6.47)\nwhere λ ∈[1, 2HSA] is chosen such that P\nπ p(π) = 1.\nreturn p.\nThe algorithm consists of two steps. First, in (6.46), we compute the collection of policies\nΨ = {πh,s,a}h∈[H],s∈[S],a∈[A] that constitutes the policy cover. The basic idea here is that\neach policies in the policy cover should balance (i) regret and (ii) coverage—that is—ensure\nthat all the states are sufficiently reached, which means we are exploring. We accomplish\nthis by using policies of the form\nπh,s,a := arg max\nπ∈Πrns\nd\nc\nM,π\nh\n(s, a)\n2HSA + η(f c\nM(π c\nM) −f c\nM(π))\n113\nwhich—for each (s, a, h) tuple—maximize the ratio of the occupancy measure for (s, a)\nat layer h to the regret gap under c\nM. This inverse gap weighted policy cover balances\nexploration and exploration by trading off coverage with suboptimality. With the policy\ncover in hand, the second step of the algorithm computes the exploratory distribution p by\nsimply applying inverse gap weighting to the elements of the cover and the greedy policy\nπ c\nM.\nThe bound on the Decision-Estimation Coefficient for the PC-IGW algorithm is as fol-\nlows.\nProposition 31: Consider the tabular reinforcement learning setting with PH\nh=1 rh ∈\nR := [0, 1]. For any γ > 0 and c\nM ∈M, the PC-IGW strategy with η =\nγ\n21H2 , ensures\nthat\nsup\nM∈M\nEπ∼p\nh\nf M(πM) −f M(π) −γ · D2\nH\n\u0000M(π), c\nM(π)\n\u0001i\n≲H3SA\nγ\n,\nand consequently certifies that decγ(M, c\nM) ≲H3SA\nγ\n.\nWe remark that it is also possible to prove this bound non-constructively, by moving to\nthe Bayesian DEC and adapting the posterior sampling approach described in Section 4.4.2.\nRemark 17 (Computational efficiency): The PC-IGW strategy can be implemented\nin a computationally efficient fashion. Briefly, the idea is to solve (6.46) by taking a\ndual approach and optimizing over occupancy measures rather than policies.\nWith\nthis parameterization, (6.46) becomes a linear-fractional program, which can then be\ntransformed into a standard linear program using classical techniques.\nHow to estimate the model.\nThe bound on the DEC we proved using the PC-IGW\nalgorithm assumes that c\nM ∈M, but in general, estimators from online learning algorithm\nsuch as exponential weights will produce c\nM t ∈co(M). While it is possible to show that\nthe same bound on the DEC holds for c\nM ∈co(M), a slightly more complex version of the\nalgorithm is required to certify such a bound. To run the PC-IGW algorithm as-is, we can\nuse a simple approach to obtain a proper estimator c\nM ∈M.\nAssume for simplicity that rewards are known, i.e. RM\nh (s, a) = Rh(s, a) for all M ∈M.\nInstead of directly working with an estimator for the entire model M, we work with layer-\nwise estimators AlgEst;1, . . . , AlgEst;H. At each round t, given the history {(πi, ri, oi)}t−1\ni=1,\nthe layer-h estimator AlgEst;h produces an estimate bP t\nh for the true transition kernel P M⋆\nh\n.\nWe measure performance of the estimator via layer-wise Hellinger error:\nEstH;h :=\nT\nX\nt=1\nEπt∼pt EM⋆,πth\nD2\nH\n\u0010\nP M⋆\nh\n(sh, ah), bP t\nh(sh, ah)\n\u0011i\n.\n(6.48)\nWe obtain an estimation algorithm for the full model M⋆by taking c\nM t as the MDP that\nhas bP t\nh as the transition kernel for each layer h. This algorithm has the following guaran-\ntee.\n114\nProposition 32: The estimator described above has\nEstH ≤O(log(H)) ·\nH\nX\nh=1\nEstH;h.\nIn addition, c\nM t ∈M.\nFor each layer, we can obtain EstH;h ≤eO(S2A) using the averaged exponential weights\nalgorithm, by applying the approach described in Section 6.4.1 to each layer. That is, for\neach layer, we obtain bP t\nh by running averaged exponential weights with the loss ℓt\nlog(Ph) =\n−log(Ph(sh+1 | sh, ah)). We obtain EstH;h ≤eO(S2A) with this approach because there are\nS2A parameters for the transition distribution at each layer.\nA lower bound on the DEC.\nWe state, but do not prove a complementary lower bound\non the DEC for tabular RL.\nProposition 33: Let M be the class of tabular MDPs with S ≥2 states, A ≥2\nactions, and PH\nh=1 rh ∈R := [0, 1]. If H ≥2 log2(S/2), then\ndecc\nε(M) ≳ε\n√\nHSA.\nUsing Proposition 28, this gives E[Reg] ≳\n√\nHSAT.\n6.6.1\nProof of Proposition 31\nToward proving Proposition 31, we provide some general-purpose technical lemmas which\nwill find further use in Section 7. First, we provide a simulation lemma, which allow us to\ndecompose the difference in value functions for two MDPs into errors between their per-layer\nreward functions and transition probabilities.\nLemma 23 (Simulation lemma): For any pair of MDPs M = (P M, RM) and c\nM =\n(P\nc\nM, R\nc\nM) with the same initial state distribution and PH\nh=1 rh ∈[0, 1], we have\n\f\f\ff M(π) −f\nc\nM(π)\n\f\f\f ≤DTV\n\u0010\nM(π), c\nM(π)\n\u0011\n(6.49)\n≤DH\n\u0010\nM(π), c\nM(π)\n\u0011\n≤1\n2η + η\n2D2\nH\n\u0010\nM(π), c\nM(π)\n\u0011\n∀η > 0,\n(6.50)\n115\nand\nf M(π) −f\nc\nM(π)\n=\nH\nX\nh=1\nE\nc\nM,πhh\n(P M\nh −P\nc\nM\nh )V\nM,π\nh+1\ni\n(sh, ah)\ni\n+\nH\nX\nh=1\nE\nc\nM,πh\nErh∼RM\nh (sh,ah)[rh] −Erh∼R c\nM\nh (sh,ah)[rh]\ni\n(6.51)\n≤\nH\nX\nh=1\nE\nc\nM,πh\nDTV\n\u0010\nP M\nh (sh, ah), P\nc\nM\nh (sh, ah)\n\u0011\n+ DTV\n\u0010\nRM\nh (sh, ah), R\nc\nM\nh (sh, ah)\n\u0011i\n. (6.52)\nNext, we provide a “change-of-measure” lemma, which allows one to move from between\nquantities involving an estimator c\nM and those involving another model M.\nLemma 24 (Change of measure for RL): Consider any MDP M and reference\nMDP c\nM which satisfy PH\nh=1 rh ∈[0, 1]. For all p ∈∆(Π) and η > 0 we have\nEπ∼p[f M(πM) −f M(π)]\n≤Eπ∼p\nh\nf M(πM) −f\nc\nM(π)\ni\n+ η Eπ∼p\nh\nD2\nH\n\u0010\nM(π), c\nM(π)\n\u0011i\n+ 1\n4η.\n(6.53)\nand\nEπ∼p E\nc\nM,π\n\" H\nX\nh=1\nD2\nTV\n\u0010\nP M(sh, ah), P\nc\nM(sh, ah)\n\u0011\n+ D2\nTV\n\u0010\nRM(sh, ah), R\nc\nM(sh, ah)\n\u0011#\n≤8H Eπ∼p\nh\nD2\nH\n\u0010\nM(π), c\nM(π)\n\u0011i\n.\n(6.54)\nProof of Proposition 31. Let M ∈M be fixed. The main effort in the proof will be to\nbound the quantity\nEπ∼p\nh\nf M(πM) −f\nc\nM(π)\ni\nin terms of the quantity on the right-hand side of (6.54), then apply change of measure\n(Lemma 24). We begin with the decomposition\nEπ∼p\nh\nf M(πM) −f\nc\nM(π)\ni\n= Eπ∼p\nh\nf\nc\nM(π c\nM) −f\nc\nM(π)\ni\n|\n{z\n}\n(I)\n+ f M(πM) −f\nc\nM(π c\nM)\n|\n{z\n}\n(II)\n.\n(6.55)\nFor the first term (I), which may be thought of as exploration bias, we have\nEπ∼p\nh\nf\nc\nM(π c\nM) −f\nc\nM(π)\ni\n=\nX\nπ∈Ψ∪{π c\nM}\nf\nc\nM(π c\nM) −f\nc\nM(π)\nλ + η(f c\nM(π c\nM) −f c\nM(π)) ≤2HSA\nη\n,\n(6.56)\nwhere we have used that λ ≥0. We next bound the second term (II), which entails showing\nthat the PC-IGW distribution explores enough. We have\nf M(πM) −f\nc\nM(π c\nM) = f M(πM) −f\nc\nM(πM) −(f\nc\nM(π c\nM) −f\nc\nM(πM)).\n(6.57)\n116\nWe use the simulation lemma to bound\nf M(πM) −f\nc\nM(πM) ≤\nH\nX\nh=1\nE\nc\nM,πM\nh\nDTV\n\u0010\nP M\nh (sh, ah), P\nc\nM\nh (sh, ah)\n\u0011\n+ DTV\n\u0010\nRM\nh (sh, ah), R\nc\nM\nh (sh, ah)\n\u0011i\n=\nH\nX\nh=1\nX\ns,a\nd\nc\nM,πM\nh\n(s, a)errM\nh (s, a),\nwhere errM\nh (s, a) := DTV\n\u0000P M(s, a), P\nc\nM(s, a)\n\u0001\n+ DTV\n\u0000RM(s, a), R\nc\nM(s, a)\n\u0001\n. Define ¯dh(s, a) =\nEπ∼p\n\u0002\nd\nc\nM,π\nh\n(s, a)\n\u0003\n. Then, using the AM-GM inequality, we have that for any η′ > 0,\nH\nX\nh=1\nX\ns,a\nd\nc\nM,πM\nh\n(s, a)[errM\nh (s, a)] =\nH\nX\nh=1\nX\ns,a\nd\nc\nM,πM\nh\n(s, a)\n\u0012 ¯dh(s, a)\n¯dh(s, a)\n\u00131/2\n(errM\nh (s, a))2\n≤1\n2η′\nH\nX\nh=1\nX\ns,a\n(d\nc\nM,πM\nh\n(s, a))2\n¯dh(s, a)\n+ η′\n2\nH\nX\nh=1\nX\ns,a\n¯dh(s, a)(errM\nh (s, a))2\n= 1\n2η′\nH\nX\nh=1\nX\ns,a\n(d\nc\nM,πM\nh\n(s, a))2\n¯dh(s, a)\n+ η′\n2\nH\nX\nh=1\nEπ∼p E\nc\nM,π\u0002\n(errM\nh (sh, ah))2\u0003\n.\nThe second term is exactly the upper bound we want, so it remains to bound the ratio of\noccupancy measures in the first term. Observe that for each (h, s, a), we have\nd\nc\nM,πM\nh\n(s, a)\n¯dh(s, a)\n≤d\nc\nM,πM\nh\n(s, a)\nd\nc\nM,πh,s,a\nh\n(s, a)\n·\n1\np(πh,s,a) ≤d\nc\nM,πM\nh\n(s, a)\nd\nc\nM,πh,s,a\nh\n(s, a)\n\u0010\n2HSA + η(f\nc\nM(π c\nM) −f\nc\nM(πh,s,a)\n\u0011\n,\nwhere the second inequality follows from the definition of p and the fact that λ ≤2HSA.\nFurthermore, since\nπh,s,a = arg max\nπ∈Πrns\nd\nc\nM,π\nh\n(s, a)\n2HSA + η(f c\nM(π c\nM) −f c\nM(π)),\nand since πM ∈Πrns, we can upper bound by\nd\nc\nM,πM\nh\n(s, a)\nd\nc\nM,πM\nh\n(s, a)\n\u0010\n2HSA + η(f\nc\nM(π c\nM) −f\nc\nM(πM)\n\u0011\n= 2HSA + η(f\nc\nM(π c\nM) −f\nc\nM(πM).\n(6.58)\nAs a result, we have\nH\nX\nh=1\nX\ns,a\n(d\nc\nM,πM\nh\n(s, a))2\n¯dh(s, a)\n≤\nH\nX\nh=1\nX\ns,a\nd\nc\nM,πM\nh\n(s, a)(2HSA + η(f\nc\nM(π c\nM) −f\nc\nM(πM))\n= 2H2SA + ηH(f\nc\nM(π c\nM) −f\nc\nM(πM)).\nPutting everything together and returning to (6.57), this establishes that\nf M(πM) −f\nc\nM(π c\nM)\n≤H2SA\nη′\n+ η′\n2\nH\nX\nh=1\nEπ∼p E\nc\nM,π\u0002\n(errM\nh (sh, ah))2\u0003\n+ ηH\n2η′ (f\nc\nM(π c\nM) −f\nc\nM(πM)) −(f\nc\nM(π c\nM) −f\nc\nM(πM)).\n117\nWe set η′ = ηH\n2\nso that the latter terms cancel and we are left with\nf M(πM) −f\nc\nM(π c\nM) ≤2HSA\nη\n+ ηH\n4\nH\nX\nh=1\nEπ∼p E\nc\nM,π\u0002\n(errM\nh (sh, ah))2\u0003\n.\nCombining this with (6.55) and (6.56) gives\nEπ∼p\nh\nf M(πM) −f\nc\nM(π)\ni\n≤4HSA\nη\n+ ηH\n4\nH\nX\nh=1\nEπ∼p E\nc\nM,π\u0002\n(errM\nh (sh, ah))2\u0003\n≤4HSA\nη\n+ ηH\n2\nH\nX\nh=1\nEπ∼p E\nc\nM,πh\nD2\nTV\n\u0010\nP M(sh, ah), P\nc\nM(sh, ah)\n\u0011\n+ D2\nTV\n\u0010\nRM(sh, ah), R\nc\nM(sh, ah)\n\u0011i\n.\nWe conclude by applying the change-of-measure lemma (Lemma 24), which implies that for\nany η′ > 0,\nEπ∼p[f M(πM) −f M(π)] ≤4HSA\nη\n+ (4η′)−1 + (4H2η + η′) · Eπ∼p\nh\nD2\nH\n\u0010\nM(π), c\nM(π)\n\u0011i\n.\nThe result follows by choosing η = η′ =\nγ\n21H2 (we have made no effort to optimize the\nconstants here).\n6.7 Tighter Regret Bounds for the Decision-Estimation Coefficient\nTo close this section, we provide a number of refined regret bounds based on the Decision-\nEstimation Coefficient, which improve upon Proposition 26 in various situations.\n6.7.1\nGuarantees Based on Decision Space Complexity\nIn general, low estimation complexity (i.e., a small bound on EstH or log|M|) is not required\nto achieve low regret for decision making. This is because our end goal is to make good\ndecisions, so we can give up on accurately estimating the model in regions of the decision\nspace that do not help to distinguish the relative quality of decisions. The following result\nprovides a tighter bound that scales only with log|Π|, at the cost of depending on the DEC\nfor a larger model class: co(M) rather than M.\nProposition 34: There exists an algorithm that for any δ > 0, ensures that with\nprobability at least 1 −δ,\nReg ≲inf\nγ>0{decγ(co(M)) · T + γ · log(|Π|/δ)}.\n(6.59)\nCompared to (6.20), this replaces the estimation term log|M| with the smaller quantity\nlog|Π|, replaces decγ(M) with the potentially larger quantity decγ(co(M)). Whether or\nnot this leads to an improvement depends on the class M. For multi-armed bandits, linear\nbandits, and convex bandits, M is already convex, so this offers strict improvement. For\nMDPs though, M is not convex: Even for the simple tabular MDP setting where |S| = S\n118\nand |A| = A, grows exponentially decγ(co(M)) in either H or S, whereas decγ(M) is\npolynomial in all parameters.\nWe mention in passing that this result is proven using a different algorithm from E2D;\nsee Foster et al. [40, 42] for more background.\n6.7.2\nGeneral Divergences and Randomized Estimators\nE2D for General Divergences and Randomized Estimators\nparameters: Exploration parameter γ > 0, divergence D(· ∥·).\nfor t = 1, 2, · · · , T do\nObtain randomized estimate νt ∈∆(M) from estimation oracle with {(πi, ri, oi)}i<t.\nCompute\n// Eq. (6.61).\npt = arg min\np∈∆(Π)\nsup\nM∈M\nEπ∼p\n\u0014\nf M(πM) −f M(π) −γ · Ec\nM∼νt\nh\nDπ\u0010\nc\nM ∥M\n\u0011i\u0015\n.\nSample decision πt ∼pt and update estimation algorithm with (πt, rt, ot).\nIn this section we give a generalization of the E2D algorithm that incorporates two extra\nfeatures: general divergences and randomized estimators.\nGeneral divergences.\nThe Decision-Estimation Coefficient measures estimation error\nvia the Hellinger distance D2\nH\n\u0000M(π), c\nM(π)\n\u0001\n, which is fundamental in the sense that it\nleads to lower bounds on the optimal regret (Proposition 28).\nNonetheless, for specific\napplications and model classes, it can be useful to work with alternative distance measures\nand divergences. For a non-negative function (“divergence”) Dπ(· ∥·), we define\ndecD\nγ (M, c\nM) =\ninf\np∈∆(Π) sup\nM∈M\nEπ∼p\n\u0014\nf M(πM) −f M(π) −γ · Dπ\u0010\nc\nM ∥M\n\u0011\u0015\n.\n(6.60)\nThis variant of the DEC naturally leads to regret bounds in terms of estimation error under\nDπ(· ∥·). Note that we use notation Dπ\u0010\nc\nM ∥M\n\u0011\ninstead of say, D\n\u0000c\nM(π), M(π)\n\u0001\n, to reflect\nthat fact that the divergence may depend on M (resp. c\nM) and π through properties other\nthan M(π) (resp. c\nM(π)).\nRandomized estimators.\nThe basic version of E2D assumes that at each round, the\nonline estimation oracle provides a point estimate c\nM t. In some settings, it useful to consider\nrandomized estimators that, at each round, produce a distribution νt ∈∆(M) over models.\nFor this setting, we further generalize the DEC by defining\ndecD\nγ (M, ν) =\ninf\np∈∆(Π) sup\nM∈M\nEπ∼p\n\u0014\nf M(πM) −f M(π) −γ · Ec\nM∼ν\nh\nDπ\u0010\nc\nM ∥M\n\u0011i\u0015\n(6.61)\nfor distributions ν ∈∆(M). We additionally define decD\nγ (M) = supν∈∆(M) decD\nγ (M, ν).\n119\nAlgorithm.\nA generalization of E2D that incorporates general divergences and random-\nized estimators is given above on page 119. The algorithm is identical to E2D with Option\nI, with the only differences being that i) we play the distribution that solves the minimax\nproblem (6.61) with the user-specified divergence Dπ(· ∥·) rather than squared Hellinger\ndistance, and ii) we use the randomized estimate νt rather than a point estimate. Our per-\nformance guarantee for this algorithm depends on the estimation performance of the oracle’s\nrandomized estimates ν1, . . . , νT ∈∆(M) with respect to the given divergence Dπ(· ∥·),\nwhich we define as\nEstD :=\nT\nX\nt=1\nEπt∼pt Ec\nMt∼νt\nh\nDπt\u0010\nc\nM t ∥M⋆\u0011i\n.\n(6.62)\nWe have the following guarantee.\nProposition 35: The algorithm E2D for General Divergences and Randomized Esti-\nmators with exploration parameter γ > 0 guarantees that\nReg ≤decD\nγ (M) · T + γ · EstD\n(6.63)\nalmost surely.\nSufficient statistics and benefits of general divergences.\nMany divergences of in-\nterest have the useful property that they depend on the estimated model c\nM only through\na “sufficient statistic” for the model class under consideration. Formally, there exists a\nsufficient statistic space Ψ and sufficient statistic ψ : M →Ψ with the property that we\ncan write (overloading notation)\nDπ\u0000M ∥M′\u0001\n= Dπ\u0000ψ(M) ∥M′\u0001\n,\nf M(π) = fψ(M)(π),\nand\nπM = πψ(M)\nfor all models M, M′. In this case, it suffices for the online estimation oracle to directly\nestimate the sufficient statistic by producing a randomized estimator νt ∈∆(Ψ), and we\ncan write the estimation error as\nEstD :=\nT\nX\nt=1\nEπt∼pt E bψt∼νt\nh\nDπt\u0010\nbψt ∥M⋆\u0011i\n.\n(6.64)\nThe benefit of this perspective is that for many examples of interest, since the divergence\ndepends on the estimate only through ψ, we can derive bounds on Est that scale with\nlog|Ψ| instead of log|M|.\nFor example, in structured bandit problems, one can work with the divergence\nDSq\n\u0010\nc\nM(π), M(π)\n\u0011\n:= (f M(π) −f\nc\nM(π))2\nwhich uses the mean reward function as a sufficient statistic, i.e. ψ(M) = f M. Here, it is\nclear that one can achieve EstD ≲log|F|, which improves upon the rate EstH ≲log|M| for\nHellinger distance, and recovers the specialized version of the E2D algorithm we considered\nin Section 4. Analogously, for reinforcement learning, one can consider value functions as a\nsufficient statistic, and use an appropriate divergence based on Bellman residuals to derive\nestimation guarantees that scale with the complexity log|Q| of a given value function class\nQ; see Section 7 for details.\n120\nDoes randomized estimation help?\nNote that whenever D is convex in the first argu-\nment, we have decD\nγ (M) ≤supc\nM∈co(M) decD\nγ (M, c\nM) = decD\nγ (M) (that is, the randomized\nDEC is never larger than the vanilla DEC), but it is not immediately apparent whether the\nopposite direction of this inequality holds, and one might hope that working with the ran-\ndomized DEC in (6.61) would lead to improvements over the non-randomized counterpart.\nThe next result shows that this is not the case: Under mild assumptions on the divergence\nD, randomization offers no improvement.\nProposition 36: Let D be any bounded divergence with the property that for all\nmodels M, M′, c\nM and π ∈Π,\nDπ\u0000M ∥M′\u0001\n≤C\n\u0010\nDπ\u0010\nc\nM ∥M\n\u0011\n+ Dπ\u0010\nc\nM ∥M′\u0011\u0011\n.\n(6.65)\nThen for all γ > 0,\nsup\nc\nM\ndecD\nγ (M, c\nM) ≤dec\nD\nγ/(2C)(M).\n(6.66)\nSquared Hellinger distance is symmetric and satisfies Condition (6.65) with C = 2. Hence,\nwriting decH\nγ (M) as shorthand for decD\nγ (M) with D = D2\nH(·, ·), we obtain the following\ncorollary.\nProposition 37: Suppose that R ⊆[0, 1]. Then for all γ > 0,\ndecH\nγ (M) ≤\nsup\nc\nM∈co(M)\ndecH\nγ (M, c\nM) ≤sup\nc\nM\ndecH\nγ (M, c\nM) ≤decH\nγ/4(M).\nThis shows that for Hellinger distance—at least from a statistical perspective—there is no\nbenefit to using the randomized DEC compared to the original version. In some cases,\nhowever, strategies p that minimize decH\nγ (M, ν) can be simpler to compute than strategies\nthat minimize decH\nγ (M, c\nM) for c\nM ∈co(M).\n6.7.3\nOptimistic Estimation\nTo derive stronger regret bounds that allow for estimation with general divergences, we\ncan combine Estimation-to-Decisions with a specialized estimation approach introduced by\nZhang [90] (see also Dann et al. [29], Agarwal and Zhang [3], Zhong et al. [91]), which we\nrefer to as optimistic estimation. The results we present here are based on Foster et al. [41].\nLet a divergence Dπ(· ∥·) be fixed. An optimistic estimation oracle AlgEst is an al-\ngorithm which, at each step t, given Ht−1 = (π1, r1, o1), . . . , (πt−1, rt−1, ot−1), produces a\nrandomized estimator νt ∈∆(M). Compared to the previous section, the only change is\nthat for a parameter γ > 0, we will measure the performance of the oracle via optimistic\nestimation error, defined as\nOptEstD\nγ :=\nT\nX\nt=1\nEπt∼pt Ec\nMt∼νt\nh\nDπ\u0010\nc\nM t ∥M⋆\u0011\n+ γ−1(f M⋆(πM⋆) −f\nc\nMt(π c\nMt)\ni\n.\n(6.67)\n121\nThis quantity is similar to (6.62), but incorporates a bonus term\nγ−1(f M⋆(πM⋆) −f\nc\nMt(π c\nMt)),\nwhich encourages the estimation algorithm to over-estimate the optimal value f M⋆(πM⋆)\nfor the underlying model, leading to a form of optimism.\nExample 6.9 (Structured bandits). Consider any structured bandit problem with decision\nspace Π, function class F ⊆(Π →[0, 1]), and O = {∅}. Let MF be the class\nMF = {M | f M ∈F, M(π) is 1-sub-Gaussian ∀π}.\nTo derive bounds on the optimistic estimation error, we can appeal to an augmented version\nof the (randomized) exponential weights algorithm which, for a learning rate parameter\nη > 0, sets\nνt(f M) ∝exp\n \n−η\n X\ni<t\n(f M(πi) −ri)2 −γ−1f M(πM)\n!!\n.\nFor an appropriate choice of η, this method achieves E\n\u0002\nOptEstD\nγ\n\u0003\n≲log|F|+\np\nT log|F|/γ\nfor D = DSq(·, ·) [90].\n◁\nOptimistic E2D.\nOptimistic E2D (E2D.Opt)\nparameters: Exploration parameter γ > 0, divergence D(· ∥·).\nfor t = 1, 2, · · · , T do\nObtain randomized estimate νt ∈∆(M) from optimistic estimation oracle with\n{(πi, ri, oi)}i<t.\nCompute\n// Eq. (6.68).\npt = arg min\np∈∆(Π)\nsup\nM∈M\nEπ∼p Ec\nM∼νt\n\u0014\nf\nc\nM(π c\nM) −f M(π) −γ · Dπ\u0010\nc\nM ∥M\n\u0011\u0015\n.\nSample decision πt ∼pt and update estimation algorithm with (πt, rt, ot).\nE2D.Opt is an optimistic variant of E2D, which we refer to as E2D.Opt. At each timestep\nt, the algorithm calls the estimation oracle to obtain a randomized estimator νt using the\ndata (π1, r1, o1), . . . , (πt−1, rt−1, ot−1) collected so far. The algorithm then uses the estimator\nto compute a distribution pt ∈∆(Π) and samples πt from this distribution. The main\nchange relative to the version of E2D on page 119 is that the minimax problem in E2D.Opt\nis derived from an “optimistic” variant of the DEC tailored to the optimistic estimation\nerror in (6.67). This quantity, which we refer to as the Optimistic Decision-Estimation\nCoefficient, is defined for ν ∈∆(M) as\no-decD\nγ (M, ν) =\ninf\np∈∆(Π) sup\nM∈M\nEπ∼p Ec\nM∼ν\nh\nf M(πM) −f M(π) −γ · Dπ\u0010\nc\nM ∥M\n\u0011i\n.\n(6.68)\nand\no-decD\nγ (M) =\nsup\nν∈∆(M)\no-decD\nγ (M, ν).\n(6.69)\n122\nThe Optimistic DEC the same as the generalized DEC in (6.61), except that the optimal\nvalue f M(πM) in (6.61) is replaced by the optimal value f\nc\nM(π c\nM) for the (randomized)\nreference model c\nM ∼ν. This seemingly small change is the main advantage of incorporating\noptimistic estimation, and makes it possible to bound the Optimistic DEC for certain\ndivergences D for which the value of the generalized DEC in (6.61) would otherwise be\nunbounded.\nRemark 18: When the divergence D admits a sufficient statistic ψ : M →Ψ, for any\ndistribution ν ∈∆(M), if we define ν ∈∆(Ψ) via ν(ψ) = ν({M ∈M : ψ(M) = ψ}),\nwe have\no-decD\nγ (M, ν) =\ninf\np∈∆(Π) sup\nM∈M\nEπ∼p Eψ∼ν\nh\nfψ(πψ) −f M(π) −γ · Dπ(ψ ∥M)\ni\n.\nIn this case, by overloading notation slightly, we may simplify the definition in (6.69)\nto\no-decD\nγ (M) =\nsup\nν∈∆(Ψ)\no-decD\nγ (M, ν).\nRegret bound for optimistic E2D.\nThe following result shows that the regret of Op-\ntimistic Estimation-to-Decisions is controlled by the Optimistic DEC and the optimistic\nestimation error for the oracle.\nProposition 38: E2D.Opt ensures that\nReg ≤o-decD\nγ (M) · T + γ · OptEstD\nγ\n(6.70)\nalmost surely.\nThis regret bound has the same structure as that of Proposition 35, but the DEC and\nestimation error are replaced by their optimistic counterparts.\nWhen does optimistic estimation help?.\nWhen does the regret bound in Proposition\n38 improve upon its non-optimistic counterpart in Proposition 35? It turns out that for\nasymmetric divergences such as those found in the context of reinforcement learning, the\nregret bound in (6.70) can be much smaller than the corresponding bound in (6.63); see\nSection 7.3.5 for an example. However, for symmetric divergences such as Hellinger distance,\nwe will show now that the result never improves upon Proposition 35.\nGiven a divergence D, we define the flipped divergence, which swaps the first and second\narguments, by\nqDπ\u0010\nc\nM ∥M\n\u0011\n:= Dπ\u0010\nM ∥c\nM\n\u0011\n.\nProposition 39 (Equivalence of optimistic DEC and randomized DEC): As-\nsume that For all pairs of models M, c\nM ∈co(M), we have (f\nc\nM(π) −f M(π))2 ≤\n123\nL2\nlip · Dπ\u0010\nc\nM ∥M\n\u0011\nfor a constant Llip > 0. Then for all γ > 0,\ndec\nqD\n3γ/2(M) −\nL2\nlip\n2γ ≤o-decD\nγ (M) ≤dec\nqD\nγ/2(M) +\nL2\nlip\n2γ .\n(6.71)\nThis result shows that the optimistic DEC with divergence D is equivalent to the generalized\nDEC in (6.61), but with the arguments to the divergence flipped. Thus, for symmetric\ndivergences, the quantities are equivalent. In particular, we can combine Proposition 39\nwith Proposition 36 to derive the following corollary for Hellinger distance.\nProposition 40: Suppose that rewards are bounded in [0, 1]. Then for all γ > 0,\no-decH\n2γ(M) −1\nγ ≤sup\nM\ndecH\nγ (M, M) ≤o-decH\nγ/6(M) + 3\nγ .\nFor asymmetric divergences, in settings where there exists an estimation oracle for which\nthe flipped estimation error\nEst\nqD =\nT\nX\nt=1\nEπ∼pt Ec\nMt∼νt\nh\nDπt\u0010\nM⋆∥c\nM t\u0011i\nis controlled, Proposition 39 shows that to match the guarantee in Proposition 38, optimism\nis not required, and it suffices to run the non-optimistic algorithm on page 119. However,\nwe show in Section 7.3.5 that for certain divergences found in the context of reinforcement\nlearning, estimation with respect to the flipped divergence is not feasible, yet working with\nthe optimistic DEC E2D.Opt leads to meaningful guarantees.\n[Note: This subsection will be expanded in the next version.]\n6.8 Decision-Estimation Coefficient: Structural Properties⋆\nIn what follows, we state some structural properties of the Decision-Estimation Coefficient,\nwhich are useful for calculating the value for specific model classes of interest.\nProposition 41 (Square loss is sufficient for structured bandit problems):\nConsider any structured bandit problem with decision space Π, function class F ⊆\n(Π →[0, 1]), and O = {∅}. Let MF be the class\nMF = {M | f M ∈F, M(π) is 1-sub-Gaussian ∀π}.\nThen, letting\ndecSq\nγ (F, bf) =\ninf\np∈∆(Π) sup\nf∈F\nEπ∼p\nh\nf(πf) −f(π) −γ(f(π) −bf(π))2i\n,\nwe have\ndecSq\nc1γ(F) ≤decγ(MF) ≤decSq\nc2γ(F),\n124\nwhere c1, c2 ≥0 are numerical constants.\nProposition 42 (Filtering irrelevant information): Adding observations that are\nunrelated to the model under consideration never changes the value of the Decision-\nEstimation Coefficient. In more detail, consider a model class M with observation space\nO1, and consider a class of conditional distributions D over a secondary observation\nspace O2, where each D ∈D has the form D(π) ∈∆(O2). For M ∈M and D ∈D, let\n(M ⊗D)(π) be the model that, given π ∈Π, samples (r, o1) ∼M(π) and o2 ∼D(π),\nthen emits (r, (o1, o2)). Set\nM ⊗D = {M ⊗D | M ∈M, D ∈D}.\nThen for all c\nM ∈M and bD ∈D,\ndecγ(M ⊗D, c\nM ⊗bD) = decγ(M, c\nM).\nThis can be seen to hold by restricting the supremum in (6.9) to range over models of\nthe form M ⊗bD.\nProposition 43 (Data processing): Passing observations through a channel never\ndecreases the Decision-Estimation Coefficient.\nConsider a class of models M with\nobservation space O. Let ρ : O →O′ be given, and define ρ ◦M to be the model\nthat, given decision π, samples (r, o) ∼M(π), then emits (r, ρ(o)).\nLet ρ ◦M :=\n{ρ ◦M | M ∈M}. Then for all c\nM ∈M, we have\ndecγ(M, c\nM) ≤decγ(ρ ◦M, ρ ◦c\nM).\nThis is an immediate consequence of the data processing inequality for Hellinger dis-\ntance, which implies that D2\nH\n\u0010\u0000ρ ◦M\n\u0001\n(π),\n\u0000ρ ◦c\nM\n\u0001\n(π)\n\u0011\n≤D2\nH\n\u0010\nM(π), c\nM(π)\n\u0011\n.\n6.9 Deferred Proofs\nProof of Lemma 21. We first prove the in-expectation bound. By assumption, we have that\nT\nX\nt=1\nℓt\nlog(c\nM t) −\nT\nX\nt=1\nℓt\nlog(M⋆) ≤RegKL.\nTaking expectations, Assumption 8 implies that\nT\nX\nt=1\nE\nh\nDKL\n\u0010\nM⋆(πt) ∥c\nM t(πt)\n\u0011i\n≤E[RegKL].\nThe bound now follows from Lemma 19.\n125\nWe now prove the high-probability bound using Lemma 34. Define Zt = 1\n2(ℓt\nlog(c\nM t) −\nℓt\nlog(M⋆)). Applying Lemma 34 with the sequence (−Zt)t≤T , we are guaranteed that with\nprobability at least 1 −δ,\nT\nX\nt=1\n−log\n\u0000Et−1\n\u0002\ne−Zt\u0003\u0001\n≤\nT\nX\nt=1\nZt + log(δ−1) = 1\n2\nT\nX\nt=1\n\u0010\nℓt\nlog(c\nM t) −ℓt\nlog(M⋆)\n\u0011\n+ log(δ−1).\nLet t be fixed, and define abbreviate zt = (rt, ot). Let ν(· | π) be any (conditional) domi-\nnating measure for m\nc\nMt and mM⋆, and observe that\nEt−1\n\u0002\ne−Zt | πt\u0003\n= Et−1\n\n\ns\nm c\nMt(zt | πt)\nmM⋆(zt | πt) | πt\n\n\n=\nZ\nmM⋆(z | πt)\ns\nm c\nMt(z | πt)\nmM⋆(z | πt)ν(dz | πt)\n=\nZ q\nmM⋆(z | πt)m c\nMt(z | πt)ν(dz | πt) = 1 −1\n2D2\nH\n\u0010\nM⋆(πt), c\nM t(πt)\n\u0011\n.\nHence,\nEt−1\n\u0002\ne−Zt\u0003\n= 1 −1\n2 Et−1\nh\nD2\nH\n\u0010\nM⋆(πt), c\nM t(πt)\n\u0011i\nand, since −log(1 −x) ≥x for x ∈[0, 1], we conclude that\n1\n2\nT\nX\nt=1\nEt−1\nh\nD2\nH\n\u0010\nM⋆(πt), c\nM t(πt)\n\u0011i\n≤1\n2\nT\nX\nt=1\n\u0010\nℓt\nlog(c\nM t) −ℓt\nlog(M⋆)\n\u0011\n+ log(δ−1).\nProof of Lemma 23. We first prove (6.49).\nLet X = PH\nh=1 rh.\nSince X ∈[0, 1] almost\nsurely, we have\n\f\f\ff M(π) −f\nc\nM(π)\n\f\f\f =\n\f\f\fEM,π[X] −E\nc\nM,π[X]\n\f\f\f ≤DTV\n\u0010\nM(π), c\nM(π)\n\u0011\n≤DH\n\u0010\nM(π), c\nM(π)\n\u0011\n.\nThe final result now follows from the AM-GM inequality.\nWe now prove (6.51). From Lemma 14, we have\nf M(π) −f\nc\nM(π) =\nH\nX\nh=1\nE\nc\nM,π\u0002\nQ\nM,π\nh\n(sh, ah) −rh −V\nM,π\nh+1 (sh+1)\n\u0003\n=\nH\nX\nh=1\nE\nc\nM,πh\u0002\nP M\nh V\nM,π\nh+1\n\u0003\n(sh, ah) −V\nM,π\nh+1 (sh+1) + Erh∼RM\nh (sh,ah)[rh] −Erh∼R c\nM\nh (sh,ah)[rh]\ni\n=\nH\nX\nh=1\nE\nc\nM,πhh\n(P M\nh −P\nc\nM\nh )V\nM,π\nh+1\ni\n(sh, ah)\ni\n+\nH\nX\nh=1\nE\nc\nM,πh\nErh∼RM\nh (sh,ah)[rh] −Erh∼R c\nM\nh (sh,ah)[rh]\ni\n≤\nH\nX\nh=1\nE\nc\nM,πh\nDTV\n\u0010\nP M\nh (sh, ah), P\nc\nM\nh (sh, ah)\n\u0011\n+ DTV\n\u0010\nRM\nh (sh, ah), R\nc\nM\nh (sh, ah)\n\u0011i\n,\nwhere we have used that V\nM,π\nh+1 (s) ∈[0, 1].\n126\nProof of Lemma 24. We first prove (6.53). For all η > 0, we have\nEM∼µ Eπ∼p[f M(πM) −f M(π)]\n≤EM∼µ Eπ∼p\nh\nf M(πM) −f\nc\nM(π)\ni\n+ η EM∼µ Eπ∼p\nh\nD2\nH\n\u0010\nM(π), c\nM(π)\n\u0011i\n+ 1\n4η.\nWe now prove (6.54). Using Lemma 37, we have that for all h,\nE\nc\nM,πh\nD2\nH\n\u0010\nP M(sh, ah), P\nc\nM(sh, ah)\n\u0011i\n+E\nc\nM,πh\nD2\nH\n\u0010\nRM(sh, ah), R\nc\nM(sh, ah)\n\u0011i\n≤8D2\nH\n\u0010\nM(π), c\nM(π)\n\u0011\n.\nAs a result,\nE\nc\nM,π\n\" H\nX\nh=1\nD2\nH\n\u0010\nP M(sh, ah), P\nc\nM(sh, ah)\n\u0011\n+ D2\nH\n\u0010\nRM(sh, ah), R\nc\nM(sh, ah)\n\u0011#\n≤8HD2\nH\n\u0010\nM(π), c\nM(π)\n\u0011\n.\nSince this holds uniformly for all π, we conclude that\nEπ∼p E\nc\nM,π\n\" H\nX\nh=1\nD2\nTV\n\u0010\nP M(sh, ah), P\nc\nM(sh, ah)\n\u0011\n+ D2\nTV\n\u0010\nRM(sh, ah), R\nc\nM(sh, ah)\n\u0011#\n≤8H Eπ∼p\nh\nD2\nH\n\u0010\nM(π), c\nM(π)\n\u0011i\n.\n6.10 Exercises\nExercise 11: Prove Lemma 19.\nExercise 12: In this exercise, we will prove Proposition 37 as follows:\n1. Prove the first two inequalities.\n2. Use properties of the Hellinger distance to show that for any π ∈Π, µ ∈∆(M), and c\nM,\nEM∼µ D2\nH\n\u0010\nM(π), c\nM(π)\n\u0011\n≥1\n4 EM,M ′∼µ D2\nH(M(π), M ′(π)).\nHint: start with the right-hand side and use symmetry and triangle inequality for Hellinger\ndistance.\n3. With the help of Part 2, show that for any c\nM,\ndecγ(M, c\nM) ≤\nsup\nµ∈∆(M)\ninf\np∈∆(Π) Eπ∼p,M∼µ\n\u0014\nf\nM(πM) −f\nM(π) −γ\n4EM ′∼µ D2\nH(M(π), M ′(π))\n\u0015\n.\n4. Argue that\ndecγ(M, c\nM) ≤\nsup\nν∈∆(M)\nsup\nµ∈∆(M)\ninf\np∈∆(Π) Eπ∼p,M∼µ\n\u0014\nf\nM(πM)−f\nM(π)−γ\n4EM ′∼ν D2\nH(M(π), M ′(π))\n\u0015\n.\nand conclude the third inequality in Proposition 37.\n127\n5. Show that\nsup\nc\nM\ndecγ(M, c\nM) ≤\nsup\nc\nM∈co(M)\ndecγ/4(M, c\nM).\n(6.72)\nIn other words, the estimation oracle cannot significantly increase the value of the DEC by\nselecting models outside co(M).\nExercise 13 (Lower Bound on DEC for Tabular RL): We showed that for Gaussian\nbandits,\ndecc\nε(M, c\nM) ≥ε\np\nA/2,\nfor all ε ≲1/\n√\nA by considering a small sub-family models and explicitly computing the DEC\nfor this sub-family. Show that if M is the set of all tabular MDPs with |S| = S, |A| = A, and\nPH\nh=1 rh ∈[0, 1],\ndecc\nε(M, c\nM) ≳ε\n√\nSA\nfor all ε ≲1/\n√\nSA, as long as H ≳logA(S).\nExercise 14 (Structured Bandits with ReLU Rewards): We will show that structured\nbandits with ReLU rewards suffer from the curse of dimensionality. Let relu(x) = max{x, 0}\nand take Π = Bd\n2(1) =\n\b\nπ ∈Rd | ∥π∥2 ≤1\n\t\n. Consider the class of value functions of the form\nfθ(π) = relu(⟨θ, π⟩−b),\n(6.73)\nwhere θ ∈Θ = Sd−1, is an unknown parameter vector and b ∈[0, 1] is a known bias parameter.\nHere Sd−1 :=\n\b\nv ∈Rd | ∥v∥= 1\n\t\ndenotes the unit sphere. Let M = {Mθ}θ∈Θ, where for all π,\nMθ(π) := N(fθ(π), 1).\nWe will prove that for all d ≥16, there exists M ∈M such that for all γ > 0,\ndecγ(M, M) ≳ed/8\nγ\n∧1,\n(6.74)\nfor an appropriate choice of bias b.\nBy slightly strengthening this result and appealing to\n(6.32), it is possible to show that any algorithm must have E[Reg] ≳ed/8.\nTo prove (6.74), we will use the fact that for large d, a random vector v chosen uniformly\nfrom the unit sphere is nearly orthogonal to any direction π. This fact is quantified as follows\n(see Ball ’97):\nPv∼unif(Sd−1)(⟨π, v⟩> α) ≤exp\n\u0012\n−α2\n2 d\n\u0013\n.\n(6.75)\nfor any π with ∥π∥= 1.\n1. Prove that for all π ∈Π, v ∈Θ, and any choice of b,\nmax\nπ′∈Π fv(π′) −fv(π) ≥(1 −b)I {⟨v, π⟩≤b}\nIn other words, instantaneous regret is at least (1 −b) whenever the decision π does not align\nwell with v.\n128\n2. Let M(π) = N(0, 1). Show that for all π ∈Π, v ∈Θ, and for any choice of b,\nD2\nH\n\u0000Mv(π), M(π)\n\u0001\n≤1\n2f 2\nv (π) ≤(1 −b)2\n2\nI {⟨v, π⟩> b} ,\ni.e. information is obtained by the decision-maker only if the decision π aligns well with v in\nthe model Mv.\n3. Show that\ndecγ(M, M) ≥\ninf\np∈∆(Π) Ev∼unif(Sd−1) Eπ∼p\n\u0014\n(1 −b) −(1 −b)I {⟨v, π⟩> b} −γ (1 −b)2\n2\nI {⟨v, π⟩> b}\n\u0015\n.\n4. Set ε := 1 −b. Use (6.75) and Part 3 above to argue that\ndecγ(M′, M) ≥ε −ε exp(−d/8) −γ ε2\n2 exp(−d/8).\nConclude that for d ≥8,\ndecγ(M′, M) ≥ε\n2 −γ ε2\n2 exp(−d/8)\n5. Show that by choosing ε = ed/8\n6γ ∧1\n2 and recalling that b = 1 −ε, we get (6.74).\n7. REINFORCEMENT LEARNING: FUNCTION APPROXIMATION AND\nLARGE STATE SPACES\nIn this section, we consider the problem of online reinforcement learning with function ap-\nproximation. The framework is the same as that of Section 5 but, in developing algorithms,\nwe no longer assume that the state and action spaces are finite/tabular, and in particular\nwe will aim for regret bounds that are independent of the number of states. To do this,\nwe will make use of function approximation—either directly modeling the transition prob-\nabilities for the underlying MDP, or modeling quantities such as value functions—and our\ngoal will be to design algorithms that are capable of generalizing across the state space as\nthey explore. This will pose challenges similar to that of the structured and contextual\nbandit settings, but we now face the additional challenge of credit assignment. Note that\nthe online reinforcement learning framework is a special case of the general decision making\nsetting in Section 6, but the algorithms we develop in this section will be tailored to the\nMDP structure.\nRecall (Section 5) that for reinforcement learning, each MDP M takes the form\nM =\n\b\nS, A, {P M\nh }H\nh=1, {RM\nh }H\nh=1, d1\n\t\n,\nwhere S is the state space, A is the action space, P M\nh : S × A →∆(S) is the probability\ntransition kernel at step h, RM\nh : S × A →∆(R) is the reward distribution, and d1 ∈∆(S1)\nis the initial state distribution. All of the results in this section will take Π = ΠRNS, and\nwe will assume that PH\nh=1 rh ∈[0, 1] unless otherwise specified.\n7.1 Is Realizability Sufficient?\nFor the frameworks we have considered so far (contextual and structured bandits, general\ndecision making), all of the algorithms we analyzed leveraged the assumption of realizability,\n129\nwhich asserts that we have a function class that is capable of modeling the underlying\nenvironment well. For reinforcement learning, there are various realizability assumptions\none can consider:\n• Model realizability: We have a model class M of MDPs that contains the true MDP\nM⋆.\n• Value function realizability: We have a class Q of state-action value functions (Q-\nfunctions) that contains the optimal function QM⋆,⋆for the underlying MDP.\n• Policy realizability: We have a class Π of policies that contains the optimal policy\nπM⋆.\nNote that model realizability implies value function realizability, which in turn implies policy\nrealizability. Ideally, we would like to be able to say that whenever one of these assumptions\nholds, we can obtain regret bounds that scale with the complexity of the function class (e.g.,\nlog|M| for model realizability, or log|Q| for value function realizability), but do not depend\non the number of states |S| or other properties of the underlying MDP, analogous to the\nsituation for statistical learning. Unfortunately, the following result shows that this is too\nmuch to ask for.\nProposition 44 (e.g., Krishnamurthy et al. [55]): For any S ∈N and H ∈N,\nthere exists a class of horizon-H MDPs M with |S| = S, |A| = 2, and log|M| = log(S),\nyet any algorithm must have\nE[Reg] ≳\nq\nmin{S, 2H} · T.\nThe interpretation of this result is that even if model realizability holds, any algorithm needs\nregret that scales with min{|S|, |M|, 2H}. This means additional structural assumptions on\nthe underlying MDP M⋆—beyond realizability—are required if we want to obtain sample-\nefficient learning guarantees. Note that since this construction satisfies model realizability,\nthe strongest form of realizability, it also rules out sample-efficient results for value function\nand policy realizability.\nIn what follows, we will explore different structural assumptions that facilitate low regret\nfor reinforcement learning with function approximation. Briefly, the idea will be to make\nassumptions that either i) allow for extrapolation across the state space, or ii) control the\nnumber of “effective” state distributions the algorithm can encounter. We will begin by\ninvestigating reinforcement learning with linear models, then explore a general structural\nproperty known as Bellman rank.\nRemark 19 (Comparison to structured bandits): Proposition 44 is analogous\nto the impossibility result we proved for structured bandits (Example 4.1), which is\nsubsumed by the RL framework. That result required a large number of actions, while\nProposition 44 holds even when |A| = 2.\n130\nRemark 20 (Further notions of realizability): There are many notions of realiz-\nability beyond those we consider above. For example, for value function approximation,\none can assume that QM⋆,π ∈Q for all π, or assume that the class Q obeys certain\nnotions of consistency with respect to the Bellman operator for M⋆.\n7.2 Linear Function Approximation\nToward understanding the complexity of RL with function approximation, let us consider\nperhaps the simplest possible modeling approach: Linear function approximation. A natural\nidea here is to assume linearity of the underlying Q-function corresponding to the true model\nM, generalizing the linear bandit setting in Section 4:\nQ\nM,⋆\nh\n(s, a) = ⟨ϕ(s, a), θM\nh ⟩,\n∀h ∈[H]\n(7.1)\nwhere ϕ(s, a) ∈Bd\n2(1) is a feature map that is known to the learner and θM\nh ∈Bd\n2(1) is an\nunknown parameter vector. Equivalently, we can define\nQ =\nn\nQh(s, a) = ⟨ϕ(s, a), θh⟩| θh ∈Bd\n2(1) ∀h\no\n,\n(7.2)\nand assume that QM,⋆∈Q. This is called the Linear-Q⋆model.\nLinearity is a strong assumption, and it is reasonable to imagine that this would be\nsufficient for low regret. Indeed, one might hope that using linearity, we can extrapolate\nthe value of QM,⋆once we estimate it for a small number of states. Unfortunately, even for\nthis very simple class of functions, it turns out that realizability is still insufficient.\nProposition 45 (Weisz et al. [86], Wang et al. [85]): For any d ∈N and H ∈N\nsufficiently large, any algorithm for the Linear-Q⋆model must have\nE[Reg] ≳min\nn\n2Ω(d), 2Ω(H)o\n.\nThis contrasts the situation for contextual bandits and linear bandits, where linear rewards\nwere sufficient for low regret. The intuition is that, even though QM,⋆is linear, it might\ntake a very long time to estimate the value for even a small number of states. That is,\nlinearity of the optimal value function is not a useful assumption unless there is some kind\nof additional structure that can guide us toward the optimal value function to being with.\nWe mention in passing that Proposition 45 can be proven by lower bounding the\nDecision-Estimation Coefficient [40].\nThe Low-Rank MDP model.\nProposition 45 implies that linearity of the optimal Q-\nfunction alone is not sufficient for sample-efficient RL. To proceed, we will make a stronger\nassumption, which asserts that the transition probabilities themselves have linear structure:\nFor all s ∈S, a ∈A, and h ∈[H], we have\nP M\nh (s′ | s, a) =\n\nϕ(s, a), µM\nh (s′)\n\u000b\n,\nand\nE[rh|s, a] = ⟨ϕ(s, a), wM\nh ⟩.\n(7.3)\nHere, ϕ(s, a) ∈Bd\n2(1) is a feature map that is known to the learner, µM\nh (s′) ∈Rd is another\nfeature map which is unknown to the learner, and wM\nh ∈Bd\n2(\n√\nd) is an unknown parameter\n131\nvector.\nAdditionally, for simplicity, we assume that\n\r\rP\ns′∈S |µM\nh (s′)|\n\r\r ≤\n√\nd, which in\nparticular holds in the tabular example below. As before, assume that both cumulative and\nindividual-step rewards are in [0, 1]. For the remainder of the subsection, we let M denote\nthe set of MDPs with these properties.\nThe linear structure in (7.3) implies that the transition matrix has rank at most d, thus\nfacilitating (as we shall see shortly) information sharing and generalization across states,\neven when the cardinality of S and A is large or infinite. For this reason, we refer to MDPs\nwith this structure as low-rank MDPs [71, 88, 48, 5].\nJust as linear bandits generalize unstructured multi-armed bandits, the low-rank MDP\nmodel (7.3) generalizes tabular RL, which corresponds to the special case in which d =\n|S| · |A|, ϕ(s, a) = es,a, and (µh(s′))s,a = P M\nh (s′ | s, a).\nProperties of low-rank MDPs.\nThe linear structure of the transition probabilities and\nmean rewards is a significantly more stringent assumption than linearity of Q\nM,⋆\nh\n(s, a) in\n(7.1). Notably, it implies that Bellman backups of arbitrary functions are linear.\nLemma 25: For any low-rank MDP M ∈M and any Q : S ×A →R and any h ∈[H],\nthe Bellman operator is linear in ϕ:\n[T M\nh Q](s, a) =\n\nϕ(s, a), θM\nQ\n\u000b\nfor some θM\nQ ∈Rd. In particular, this implies that for any policy π = (π1, . . . , πH),\nfunctions Q\nM,π\nh\nare linear in ϕ for every h. Finally, for Q : S × A →[0, 1], it holds that\n\r\rθM\nQ\n\r\r ≤2\n√\nd.\nAs a special case, this lemma implies that for low-rank MDPs, Q\nM,π\nh\nis linear for all π.\nProof of Lemma 25. We have\n[T M\nh Q](s, a) = ⟨ϕ(s, a), wM\nh ⟩+\nX\ns′\nP M\nh (s′ | s, a) max\na′\nQ(s′, a′)\n(7.4)\n= ⟨ϕ(s, a), wM\nh ⟩+\nX\ns′\n\nϕ(s, a), µM\nh (s′)\n\u000b\nmax\na′\nQ(s′, a′)\n(7.5)\n=\n*\nϕ(s, a), wM\nh +\nX\ns′\nµM\nh (s′) max\na′\nQ(s′, a′)\n+\n.\n(7.6)\nThe second statement follows since Q\nM,π\nh\n=\n\u0002\nT M\nh Q\nM,π\nh+1\n\u0003\n. For the last statement,\n\r\rθM\nQ\n\r\r ≤∥wM\nh ∥+\n\r\r\r\r\r\nX\ns′\nµM\nh (s′)Q(s′)\n\r\r\r\r\r ≤2\n√\nd,\n(7.7)\nsince µM\nh is a vector of distributions on S.\n7.2.1\nThe LSVI-UCB Algorithm\nTo provide regret bounds for the low-rank MDP model, we analyze an algorithm called\nLSVI-UCB (“Least Squares Value Iteration UCB”), which was introduced and analyzed in\n132\nthe influential paper of Jin et al. [48]. Similar to the UCB-VI algorithm we analyzed for\ntabular RL, the main idea behind the algorithm is to compute a state-action value Qt with\nthe optimistic property that\nQt\nh(s, a) ≥Q\nM,⋆\nh\n(s, a)\nfor all s, a, h. This is achieved by combining the principle of dynamic programming with\nan appropriate choice of bonus to ensure optimism.\nHowever, unlike UCB-VI, the algo-\nrithm does not directly estimate transition probabilities (which is not feasible when µM is\nunknown), and instead implements approximate value iteration by solving a certain least\nsquares objective.\nLSVI-UCB\nInput: R, ρ > 0\nfor t = 1, . . . , T do\nLet Qt\nH+1 ≡0.\nfor h = H, . . . , 1 do\nCompute least-squares estimator\nbθt\nh = arg min\nθ∈Bd\n2(ρ)\nX\ni<t\n\u0010\n⟨ϕ(si\nh, ai\nh), θ⟩−ri\nh −max\na\nQt\nh+1(si\nh+1, a)\n\u00112\n,\nand let bQt\nh(s, a) :=\n\nϕ(s, a), bθt\nh\n\u000b\n.\nDefine\nΣt\nh =\nX\ni<t\nϕ(si\nh, ai\nh)ϕ(si\nh, ai\nh)⊤+ I.\nCompute bonus:\nbt\nh,δ(s, a) =\n√\nR∥ϕ(s, a)∥(Σt\nh)−1.\nCompute optimistic value function:\nQt\nh(s, a) =\nn\nbQt\nh(s, a) + bt\nh,δ(s, a)\no\n∧1.\nSet V t\nh(s) = maxa∈A Qt\nh(s, a) and bπt\nh(s) = arg maxa∈A Qt\nh(s, a).\nCollect trajectory (st\n1, at\n1, rt\n1), . . . , (st\nH, at\nH, rt\nH) according to bπt.\nIn more detail, for each episode t, the algorithm computes Qt\n1, . . . , Qt\nH through approximate\ndynamic programming. At layer h, given Qt\nh+1, the algorithm computes a linear Q-function\nbQt\nh(s, a) :=\n\nϕ(s, a), bθt\nh\n\u000b\n, by solving a least squares problem in which X = ϕ(sh, ah) is the\nfeature vector and Y = rh + maxa Qt\nh+1(sh+1, a) is the target/outcome. This is motivated\nby Lemma 25, which asserts that the Bellman backup\n\u0002\nT M\nh Qt\nh+1\n\u0003\n(s, a) is linear. Given bQt\nh,\nthe algorithm forms the optimistic estimate Qt\nh via\nQt\nh(s, a) =\nn\nbQt\nh(s, a) + bt\nh,δ(s, a)\no\n∧1,\nwhere\nbt\nh,δ(s, a) =\n√\nR∥ϕ(s, a)∥(Σt\nh)−1,\nwith\nΣt\nh =\nX\ni<t\nϕ(si\nh, ai\nh)ϕ(si\nh, ai\nh)⊤+ I,\n133\nis an elliptic bonus analogous to the bonus used within LinUCB. With this, the algorithm\nproceeds to the next layer h−1. Once Qt is computed for every layer, the algorithm executes\nthe optimistic policy bπt given by bπt\nh(s) = arg maxa∈A Qt\nh(s, a).\nThe LSVI-UCB algorithm enjoys the following regret bound.\nProposition 46: If any δ > 0, if we set R = c · d2 log(HT/δ) for a sufficiently large\nnumerical constant c and ρ = 2\n√\nd, LSVI-UCB has that with probability at least 1 −δ,\nReg ≲H\np\nd3 · T log(HT/δ).\n(7.8)\n7.2.2\nProof of Proposition 46\nThe starting point of our analysis for UCB-VI was Lemma 15, which states that it is sufficient\nto construct optimistic estimates {Q1, . . . , QH} (i.e. Q\nM,⋆\nh\n≤Qh) such that the Bellman\nresiduals EM,bπ\u0002\n(Qh −T M\nh Qh+1)(sh, ah)\n\u0003\nare small under the greedy (with respect to Q’s)\npolicy bπ. In order to control these residuals, we constructed an estimated model c\nM and\ndefined empirical Bellman operators T\nc\nM\nh\nin terms of estimated transition kernels. We then\nset Qh to be the empirical Bellman backup T\nc\nM\nh Qh+1, plus an optimistic bonus term. In\ncontrast, LSVI-UCB does not directly estimate the model. Instead, it performs regression\nwith a target that is an empirical Bellman backup. As we shall see shortly, subtleties arise\nin the analysis of this regression step due to lack of independence.\nTechnical lemmas for regression.\nRecall from Lemma 25 that for any fixed Q : S×A →\nR,\nEMh\nri\nh + max\na\nQ(si\nh+1, a) | si\nh, ai\nh\ni\n= [T M\nh Q](si\nh, ai\nh).\n(7.9)\nHowever, for layer h, the regression problem within LSVI-UCB concerns a data-dependent\nfunction Q = Qt\nh+1 (with i < t), which is chosen as a function of all the trajectories\nτ 1, . . . , τ t−1. This dependence implies that the regression problem solved by LSVI-UCB is\nnot of the type studied, say, in Proposition 1. Instead, in the language of Section 1.4, the\nmean of the outcome variable is itself a function that depends on all the data. The saving\ngrace here is that this dependence does not result in arbitrarily complex functions, which\nwill allow us to appeal to uniform convergence arguments. In particular, for every h and t,\nQt\nh belongs to the class\nQ :=\nn\n(s, a) 7→\nn\n⟨θ, ϕ(s, a)⟩+\n√\nR∥ϕ(s, a)∥(Σ)−1\no\n∧1 : ∥θ∥≤2\n√\nd, σmin(Σ) ≥1\no\n.\n(7.10)\nTo make use of this fact, we first state an abstract result concerning regression with depen-\ndent outcomes.\nLemma 26: Let G be an abstract set with |G| < ∞. Let x1, . . . , xT ∈X be fixed, and\nfor each g ∈G, let y1(g), . . . , yT (g) ∈R be 1-subGaussian outcomes satisfying\nE[yi(g) | xi] = fg(xi)\n134\nfor fg ∈F ⊆{f : X →R}.a In addition, assume that y1(g), . . . , yT (g) are conditionally\nindependent given x1, . . . , xT . For any latent g ∈G, define the least-squares solution\nbfg ∈arg min\nf∈F\nT\nX\ni=1\n(yi(g) −f(xi))2.\nWith probability at least 1 −δ, simultaneously for all g ∈G,\nT\nX\ni=1\n( bfg(xi) −fg(xi))2 ≲log(|F||G|/δ).\naThe random variables {yi(g)}g∈G may be correlated.\nProof of Lemma 26. Fix g ∈G. To shorten the notation, it is useful to introduce empirical\nnorms ∥f∥2\nT = 1\nT\nPT\ni=1 f(xi)2 and empirical inner product ⟨f, f′⟩T = PT\ni=1 f(xi)f′(xi) for\nf, f′ ∈F. Optimality of bfg implies that\nT\nX\ni=1\n(yi(g) −bfg(xi))2 ≤\nT\nX\ni=1\n(yi(g) −fg(xi))2\nwhich can be written succinctly (with a slight abuse of notation) as\n\r\rYg−bfg\n\r\r2\nT ≤∥Yg −fg∥2\nT\nfor Yg = (y1(g), . . . , yT (g)). This implies\n\r\r bfg −fg\n\r\r2\nT ≤2\n\nYg −fg, bfg −fg\n\u000b\nT.\nDividing both sides by\n\r\r bfg −fg\n\r\r\nT and taking supremum over bfg ∈F leads to\n\r\r bfg −fg\n\r\r\nT ≤2 max\nf∈F\n\nYg −fg,\nf −fg\n\r\rf −fg\n\r\r\nT\n\u000b\nT.\n(7.11)\nThe random vector Yg−fg has independent zero-mean 1-subGaussian entries by assumption,\nwhile the multiplier\nf−fg\n\r\rf−fg\n\r\r\nT\nis simply a T-dimensional vector of Euclidean length\n√\nT, for\neach f ∈F. Hence, each inner product in (7.11) is a sub-Gaussian vector with variance\nproxy\n1\nT (see Definition 2). Thus, with probability at least 1 −δ, the maximum on the\nright-hand side does not exceed C\np\nlog(|F|/δ)/T for an appropriate constant C. Taking\nthe union bound over g and squaring both sides of (7.11) yields the desired bound.\nWe may now apply Lemma 26 to analyze the regression step of LSVI-UCB.\nLemma 27: With probability at least 1 −δ, we have that for all t and h,\nX\ni<t\n\u0010\nbQt\nh(si\nh, ai\nh) −\n\u0002\nT M\nh Qt\nh+1\n\u0003\n(si\nh, ai\nh)\n\u00112\n≲d2 log(HT/δ).\n(7.12)\nProof sketch for Lemma 27. Let t ∈[T] and h ∈[H] be fixed. To make the correspondence\nwith Lemma 26 explicit, for the data (si\nh, ai\nh, si\nh+1, ri\nh), we define xi = ϕ(si\nh, ai\nh) and yi(Q) =\nri\nh + maxa Q(si\nh+1, a), with Q ∈Q playing the role of the index g ∈G. With this, we have\nE[yi(Q) | xi] = EMh\nri\nh + max\na\nQ(si\nh+1, a) | si\nh, ai\nh\ni\n= [T M\nh Q](si\nh, ai\nh) =\n\nϕ(si\nh, ai\nh), θM\nQ\n\u000b\n135\nwhich is linear in xi = ϕ(si\nh, ai\nh), with the vector of coefficients θM\nQ depending on Q. The\nregression problem is well-specified as long as we choose\nF =\nn\nϕ(s, a) 7→⟨ϕ(s, a), θ⟩: ∥θ∥≤2\n√\nd\no\nand Q as in (7.10). While both of these sets are infinite, we can to a standard covering\nnumber argument for an appropriate scale ε. The cardinalities of ε-discretized classes can\nbe shown to be of size eO(d) and eO(d2), respectively, up to factors logarithmic in 1/ε and\nd.\nThe statement follows after checking that discretization incurs a small price due to\nLipschitzness with respect to parameters. Finally, we union bound over t and h.\nEstablishing optimism.\nThe next lemma shows that closeness of the regression estimate\nto the Bellman backup on the data {(si\nh, ai\nh)}i<t translates into closeness at an arbitrary\n(s, a) pair as long as ϕ(s, a) is sufficiently covered by the data collected so far. This, in turn,\nimplies that Qt\n1, . . . , Qt\nH are optimistic.\nLemma 28: Whenever the event in Lemma 27 occurs, we have that for all (s, a, h)\nand t ∈[T],\n\f\f\f bQt\nh(s, a) −\n\u0002\nT M\nh Qt\nh+1\n\u0003\n(s, a)\n\f\f\f ≲\np\nd2 log(HT/δ) · ∥ϕ(s, a)∥(Σt\nh)−1 =: bt\nh,δ(s, a).\n(7.13)\nand\nQt\nh(s, a) ≥Q\nM,⋆\nh\n(s, a).\n(7.14)\nProof of Lemma 28. Writing the Bellman backup, via Lemma 25, as\n\u0002\nT M\nh Qt\nh+1\n\u0003\n(s, a) = ⟨ϕ(s, a), θt\nh⟩\nfor some θt\nh ∈Rd with ∥θt\nh∥2 ≤2\n√\nd, we have that\n\f\f\f bQt\nh(s, a) −\n\u0002\nT M\nh Qt\nh\n\u0003\n(s, a)\n\f\f\f =\n\f\f\f\nD\nϕ(s, a), bθt\nh −θt\nh\nE\f\f\f\n=\n\f\f\f\nD\n(Σt\nh)−1/2ϕ(s, a), (Σt\nh)1/2(bθt\nh −θt\nh)\nE\f\f\f\n≤\n\r\rϕ(s, a)\n\r\r\n(Σt\nh)−1 ·\n\r\rbθt\nh −θt\nh\n\r\r\nΣt\nh.\nLemma 27 then implies (7.13), since\n\r\rbθt\nh −θt\nh\n\r\r2\nΣt\nh = (bθt\nh −θt\nh) T\n X\ni<t\nϕ(si\nh, ai\nh)ϕ(si\nh, ai\nh)⊤+ I\n!\n(bθt\nh −θt\nh)\n(7.15)\n=\nX\ni<t\n\u0010\nbQt\nh(si\nh, ai\nh) −\n\u0002\nT M\nh Qt\nh+1\n\u0003\n(si\nh, ai\nh)\n\u00112\n+\n\r\rbθt\nh −θt\nh\n\r\r2\n(7.16)\nand\n\r\rbθt\nh −θt\nh\n\r\r2 ≲d by (7.7).\nTo show (7.14), we proceed by induction on V t\nh ≥V\nM,⋆\nh\n, as in the proof of Lemma\n16. We start with the base case h = H + 1, which has V t\nH+1 = V\nM,⋆\nH+1 ≡0. Assuming\n136\nV t\nh+1 ≥V\nM,⋆\nh+1, we first observe that T M\nh\nis monotone and T M\nh V t\nh+1 ≥T M\nh V\nM,⋆\nh+1 = Q\nM,⋆\nh\n.\nHence,\nbQt\nh = bQt\nh −T M\nh V h+1 + T M\nh V h+1\n(7.17)\n≥bQt\nh −T M\nh V h+1 + Q\nM,⋆\nh\n(7.18)\n≥−bt\nh,δ + Q\nM,⋆\nh\n(7.19)\nand thus bQt\nh +bt\nh,δ ≥Q\nM,⋆\nh\n. Since Q\nM,⋆\nh\n≤1, the clipped version Qt\nh also satisfies Qt\nh ≥Q\nM,⋆\nh\n.\nThis, in turn, implies V t\nh ≥V\nM,⋆\nh\n.\nFinishing the proof.\nWith the technical results above established, the proof of Propo-\nsition 46 follows fairly quickly.\nProof of Proposition 46. Let M be the true model. Condition on the event in Lemma 27.\nThen, since Q is optimistic by Lemma 28, we have that for each timestep t,\nf M(πM) −f M(bπt) ≤Es1∼d1\n\u0002\nV t\n1(s1)\n\u0003\n−f M(bπt)\n=\nH\nX\nh=1\nEM,bπt\u0002\nQt\nh(sh, ah) −\n\u0002\nT M\nh Qt\nh+1\n\u0003\n(sh, ah)\n\u0003\nby Lemma 14. Using the definition of bt\nh,δ and Lemma 28, we have\nH\nX\nh=1\nEM,bπt\u0002\nQt\nh(sh, ah) −\n\u0002\nT M\nh Qt\nh+1\n\u0003\n(sh, ah)\n\u0003\n≲\n√\nR\nH\nX\nh=1\nEM,bπth\n∥ϕ(sh, ah)∥(Σt\nh)−1\ni\n.\nSumming over all timesteps t gives\nReg ≤\n√\nR\nT\nX\nt=1\nH\nX\nh=1\nEM,bπth\n∥ϕ(sh, ah)∥(Σt\nh)−1\ni\n.\nBy Hoeffding’s inequality, we have that with probability at least 1 −δ, this is at most\n√\nR\nT\nX\nt=1\nH\nX\nh=1\n∥ϕ(st\nh, at\nh)∥(Σt\nh)−1 +\np\nRHT log(1/δ).\nThe elliptic potential lemma (Lemma 11) now allows us to bound\nT\nX\nt=1\n∥ϕ(st\nh, at\nh)∥(Σt\nh)−1 ≲\np\ndT log(T/d)\nfor each h, which gives the result.\n7.3 Bellman Rank\nIn this section, we continue our study of value-based methods, which assume access to a\nclass Q of state-action value functions such that QM⋆,⋆∈Q. In the prequel, we saw that\nthe Low-Rank MDP assumption facilitates sample-efficient reinforcement learning when Q\nis a class of linear functions, but what if we want to learn with nonlinear functions such as\nneural networks? To this end, we will introduce a new structural property, Bellman rank\n[46, 34, 49], which allows for sample-efficient learning with general classes Q, and subsumes\na number of well-studied MDP families, including:\n137\n• Low-Rank MDPs [87, 48, 5]\n• Block MDPs and reactive POMDPs\n[55, 33].\n• MDPs with Linear Q⋆and V ⋆[34].\n• MDPs with low occupancy complexity\n[34].\n• Linear mixture MDPs [65, 15].\n• Linear dynamical systems (LQR) [30].\nWe will learn about these examples in Section 7.3.3.\nBuilding intuition.\nBellman rank is a property of the underlying MDP M⋆which gives\na way of controlling distribution shift—that is, how many times a deliberate algorithm can\nbe surprised by a substantially new state distribution dM,π when it updates its policy. To\nmotivate the property, let us revisit the low-rank MDP model. Let M be a low-rank MDP\nwith feature map ϕ(s, a) ∈Rd, and let Qh(s, a) =\n\nϕ(s, a), θ\nQ\nh\n\u000b\nbe an arbitrary linear value\nfunction. Observe that since M is a Low-Rank MDP, we have [T M\nh Q](s, a) =\nD\nϕ(x, a), ˜θ\nM,Q\nh\nE\n,\nwhere ˜θ\nM,Q\nh\n:= wM\nh +\nR\nµM\nh (s′) maxa′ Qh+1(s′, a′)ds′. As a result, for any policy π, we can\nwrite the Bellman residual for Q as\nEM,πh\nQh(sh, ah) −rh −max\na\nQh+1(sh+1, a)\ni\n=\nD\nEM,π[ϕ(sh, ah)], θ\nQ\nh −wM\nh −˜θ\nM,Q\nh\nE\n(7.20)\n= ⟨XM\nh (π), W M\nh (Q)⟩,\n(7.21)\nwhere XM\nh (π) := EM,π[ϕ(sh, ah)] ∈Rd is an “embedding” that depends on π but not Q,\nand W M\nh (Q) := θ\nQ\nh −wM\nh −˜θ\nM,Q\nh\n∈Rd is an embedding that depends on Q but not π (both\nembeddings depend on M). Notably, if we view the Bellman residual as a huge Π × Q\nmatrix Eh(·, ·) ∈RΠ×Q with\nEh(π, Q) := EM,πh\nQh(sh, ah) −\n\u0010\nrh + max\na\nQh+1(sh+1, a)\n\u0011i\n,\n(7.22)\nthen the property (7.21) implies that rank(Eh(·, ·)) ≤d. Bellman rank is an abstraction of\nthis property.16\nDefinition 8 (Bellman rank): For an MDP M with value function class Q and policy\nclass Π, the Bellman rank is defined as\ndB(M) = max\nh∈[H] rank({Eh(π, Q)}π∈Π,Q∈Q).\n(7.23)\nEquivalently, Bellman rank is the smallest dimension d such that for all h, there exist\nembeddings XM\nh (π), W M\nh (Q) ∈Rd such that\nEh(π, Q) = ⟨XM\nh (π), W M\nh (Q)⟩\n(7.24)\nfor all π ∈Π and Q ∈Q.\n16Bellman rank was originally introduced in the pioneering work of Jiang et al. [46]. The definition of\nBellman rank we present, which is slightly different from the original definition, is taken from the later work\nof Du et al. [34], Jin et al. [49], and is often referred to as Q-type Bellman rank.\n138\nThe utility of Bellman rank is that the factorization in (7.24) gives a way of controlling\ndistribution shift in the MDP M, which facilitates the application of standard generaliza-\ntion guarantees for supervised learning/estimation. Informally, there are only d effective\ndirections in which we can be “surprised” by the state distribution induced by a policy π,\nto the extent that this matters for the class Q under consideration; this property was used\nimplicitly in the proof of the regret bound for LSVI-UCB. As we will see, low Bellman rank\nis satisfied in many settings that go beyond the Low-Rank MDP model.\n7.3.1\nThe BiLinUCB Algorithm\nWe now present an algorithm, BiLinUCB [34], which attains low regret for MDPs with low\nBellman rank under the realizability assumption that\nQM⋆,⋆∈Q.\nLike many of the algorithms we have covered, BiLinUCB is based on confidence sets and\noptimism, though the way we will construct the confidence sets and implement optimism is\nnew.\nPAC versus regret.\nFor technical reasons, we will not directly give a regret bound for\nBiLinUCB. Instead, we will prove a PAC (“probably approximately correct”) guarantee. For\nPAC, the algorithm plays for T episodes, then outputs a final policy bπ, and its performance\nis measured via\nf M⋆(πM⋆) −f M⋆(bπ).\n(7.25)\nThat is, instead of considering cumulative performance as with regret, we are only concerned\nwith final performance. For PAC, we want to ensure that f M⋆(πM⋆) −f M⋆(bπ) ≤ε for some\nε ≪1 using a number of episodes that is polynomial in ε−1 and other problem parameters.\nThis is an easier task than achieving low regret: If we have an algorithm that ensures that\nE[Reg] ≲\n√\nCT for some problem-dependent constant C, we can turn this into an algorithm\nthat achieves PAC error ε using O\n\u0000 C\nε2\n\u0001\nepisodes via online-to-batch conversion. In the other\ndirection, if we have an algorithm that achieves PAC error ε using O\n\u0000 C\nε2\n\u0001\nepisodes, one can\nuse this to achieve E[Reg] ≲C1/3T 2/3 using a simple explore-then-commit approach; this\nis lossy, but is the best one can hope for in general.\nAlgorithm overview.\nBiLinUCB proceeds in K iterations, each of which consists of n\nepisodes. The algorithm maintains a confidence set Qk ⊆Q of value functions (generalizing\nthe confidence sets we constructed for structured bandits in Section 4), with the property\nthat QM⋆,⋆∈Q with high probability. Each iteration k consists of two parts:\n• Given the current confidence set Qk, the algorithm computes a value function Qk and\ncorresponding policy πk := πQk that is optimistic on average\nQk = arg max\nQ∈Qk\nEs1∼d1[Q1(s1, πQ(s1))].\nThe main novelty here is that we are only aiming for optimism with respect to the\ninitial state distribution.\n139\n• Using the new policy πk, the algorithm gathers n episodes and uses these to compute\nestimators {bE k\nh(Q)}h∈[H] which approximate the Bellman residual Eh(πk, Q) for all\nQ ∈Q. Then, in (7.26), the algorithm computes the new confidence set Qk+1 by\nrestricting to value functions for which the estimated Bellman residual is small for\nπ1, . . . , πk. Eliminating value functions with large Bellman residual is a natural idea,\nbecause we know from the Bellman equation that QM⋆,⋆has zero Bellman residual.\nBiLinUCB\nInput: β > 0, iteration count K ∈N, batch size n ∈N.\nQ1 ←Q.\nfor iteration k = 1, . . . , K do\nCompute optimistic value function:\nQk = arg max\nQ∈Qk\nEs1∼d1[Q1(s1, πQ(s1))].\nand let πk := πQk.\nfor l = 1, . . . , n do\nExecute πk for an episode and observe trajectory (s\nk,l\n1 , a\nk,l\n1 , r\nk,l\n1 ), . . . , (s\nk,l\nH , a\nk,l\nH , r\nk,l\nH ).\nCompute confidence set\nQk+1 =\n\n\nQ ∈Q |\nX\ni≤k\n(bE i\nh(Q))2 ≤β\n∀h ∈[H]\n\n\n,\n(7.26)\nwhere\nbE i\nh(Q) := 1\nn\nn\nX\nl=1\n\u0012\nQh(s\ni,l\nh , a\ni,l\nh ) −r\ni,l\nh −max\na∈A Qh+1(s\ni,l\nh+1, a)\n\u0013\n.\nLet bk = arg maxk∈[K] bV k, where bV k := 1\nn\nPn\nl=1\nPH\nh=1 r\nk,l\nh .\nReturn bπ = π\nbk.\nMain guarantee.\nThe main result for this section is the following PAC guarantee for\nBiLinUCB.\nProposition 47: Suppose that M⋆has Bellman rank d and QM⋆,⋆∈Q. For any ε > 0\nand δ > 0, if we set n ≳H3d log(|Q|/δ)\nε2\n, K ≳Hd log(1+n/d), and β ∝c·K log|Q|+log(HK/δ)\nn\n,\nthen BiLinUCB learns a policy bπ such\nf M⋆(πM⋆) −f M⋆(bπ) ≤ε\nwith probability at least 1 −δ, and does so using\neO\n\u0012H4d2 log(|Q|/δ)\nε2\n\u0013\nepisodes.\n140\nThis result shows that low Bellman rank suffices to learn a near-optimal policy, with sample\ncomplexity that only depends on the rank d, the horizon H, and the capacity log|Q| for the\nvalue function class; this reflects that the algorithm is able to generalize across the state\nspace, with d and log|Q| controlling the degree of generalization. The basic principles at\nplay are:\n• By choosing Qk optimistically, we ensure that the suboptimality of the algorithm is\ncontrolled by the Bellman residual for Qk, on-policy, similar to what we saw for UCB-\nVI and LSVI-UCB. An important difference compared to the LSVI-UCB algorithm we\ncovered in the previous section is that BiLinUCB is only optimistic “on average” with\nrespect to the initial state distribution, i.e.,\nEs1∼d1\n\u0002\nQk\n1(s1, πQk(s1))\n\u0003\n≥f M⋆(πM⋆),\nwhile LSVI-UCB aims to find a value function that is uniformly optimistic for all states\nand actions.\n• The confidence set construction (7.26) explicitly removes value functions that have\nlarge Bellman residual on the policies encountered so far. The key role of the Bellman\nrank property is to ensure that there are only eO(d) “effective” state distributions\nthat lead to substantially different values for the Bellman residual, which means that\neventually, only value functions with low residual will remain.\nInterestingly, the Bellman rank property is only used for analysis, and the algorithm does\nnot explicitly compute or estimate the factorization.\nRegret bounds.\nThe BiLinUCB algorithm can be lifted to provide a regret guarantee via\na explore-then-commit strategy: Run the algorithm for T0 episodes to learn a ε-optimal\npolicy, then commit to this policy for the remaining rounds. It is a simple exercise to show\nthat by choosing T0 appropriately, this approach gives\nReg ≤eO\n\u0010\n(H4d2 log(|Q|/δ))1/3 · T 2/3\u0011\n.\nUnder an additional assumption known as Bellman completeness, it is possible to attain\n√\nT with a variant of this algorithm that uses a slightly different confidence set construction\n[49].\n7.3.2\nProof of Proposition 47\nRecall from the definition of Bellman rank that there exist embeddings XM⋆\nh (π), W M⋆\nh\n(Q) ∈\nRd such that for all π ∈Π and Q ∈Q,\nEh(π, Q) =\n\nXM⋆\nh (π), W M⋆\nh\n(Q)\n\u000b\n.\nWe assume throughout this proof that\n\r\rXM⋆\nh (π)\n\r\r,\n\r\rW M⋆\nh\n(Q)\n\r\r\n2 ≤1 for simplicity.\nTechnical lemmas.\nBefore proceeding, we state two technical lemmas. The first lemma\nestablishes validity for the confidence set Qk constructed by BiLinUCB.\n141\nLemma 29: For any δ > 0, if we set β = c·K log|Q|+log(HK/δ)\nn\n, where c > 0 is sufficiently\nlarge absolute constant, then with probability at least 1 −δ, for all k ∈[K]:\n1. All Q ∈Qk have\nX\ni<k\n(Eh(πi, Q))2 ≲β\n∀h ∈[H].\n(7.27)\n2. QM⋆,⋆∈Qk.\nProof of Lemma 29. Using Hoeffding’s inequality and a union bound (Lemma 3), we have\nthat with probability at least 1 −δ, for all k ∈[K], h ∈[H], and Q ∈Q,\n\f\f\fbE k\nh(Q) −Eh(πk, Q)\n\f\f\f ≤C ·\nr\nlog(|Q|HK/δ)\nn\n,\n(7.28)\nwhere c is an absolute constant.\nTo prove Part 1, we observe that for all k, using the AM-GM inequality, we have that\nfor all Q ∈Q,\nX\ni<k\n(Eh(πi, Q))2 ≤2\nX\ni<k\n(bE i\nh(Q))2 + 2\nX\ni<k\n(Eh(πi, Q) −bE i\nh(Q))2.\nFor Q ∈Qk, the definition of Qk implies that P\ni<k(bE i\nh(Q))2 ≤β, while (7.28) implies that\nP\ni<k(Eh(πi, Q) −bE i\nh(Q))2 ≲β, which gives the result.\nFor Part 2, we similarly observe that for all k, h and Q ∈Q,\nX\ni<k\n(bE i\nh(Q))2 ≤2\nX\ni<k\n(Eh(πi, Q))2 + 2\nX\ni<k\n(Eh(πi, Q) −bE i\nh(Q))2.\nSince QM⋆,⋆has Eh(π, QM⋆,⋆) = 0 ∀π by Bellman optimality, we have\nX\ni<k\n(bE i\nh(QM⋆,⋆))2 ≤2\nX\ni<k\n(Eh(πi, QM⋆,⋆) −bE i\nh(QM⋆,⋆))2 ≤2C2 log(|Q|HK/δ)\nn\n,\nwhere the last inequality uses (7.28). It follows that as long as β ≥2C2 log(|Q|HK/δ)\nn\n, we will\nhave QM⋆,⋆∈Qk for all k.\nThe next result shows that whenever the event in the previous lemma holds, the value\nfunctions constructed by BiLinUCB are optimistic.\nLemma 30: Whenever the event in Lemma 29 occurs, the following properties hold:\n1. Define\nΣk\nh =\nX\ni<k\nXM⋆\nh (πi)XM⋆\nh (πi)⊤.\n(7.29)\nFor all k ∈[K], all Q ∈Qk satisfy\n\r\rW M⋆\nh\n(Q)\n\r\r2\nΣk\nh ≲β.\n(7.30)\n142\n2. For all k, Qk is optimistic in the sense that\nEs1∼d1[Qk\n1(s1, πQ(s1))] ≥Es1∼d1\n\u0002\nQ\nM⋆,⋆\n1\n(s1, πM⋆(s1))\n\u0003\n= f M⋆(πM⋆).\n(7.31)\nProof of Lemma 30. For Part 1, recall that by the Bilinear class property, we can write\nEh(πk, Q) =\n\nXM⋆(πk), W M⋆(Q)\n\u000b\n, so that (7.27) implies that\n\r\rW M⋆(Q)\n\r\r\nΣk\nh =\nX\ni<k\n\nXM⋆(πi), W M⋆(Q)\n\u000b2 =\nX\ni<k\n(Eh(πi, Q))2 ≲β.\nFor Part 2, we observe that for all k, since QM⋆,⋆∈Qk, we have\nEs1∼d1[Qk\n1(s1, πQ(s1))] = sup\nQ∈Q\nEs1∼d1\n\u0002\nQ1(s1, πQ(s1))\n\u0003\n≥Es1∼d1\n\u0002\nQ\nM⋆,⋆\n1\n(s1, πM⋆(s1))\n\u0003\n= f M⋆(πM⋆).\nProving the main result.\nEquipped with the lemmas above, we prove Proposition 47.\nProof of Proposition 47. We first prove a generic bound on the suboptimality of each policy\nπk for k ∈[K]. Let us condition on the event in Lemma 29, which occurs with probability\nat least 1 −δ. Whenever this event occurs, Lemma 30 implies that Qk is optimistic, so we\nhave can bound\nf M⋆(πM⋆) −f M⋆(πk) ≤Es1∼d1\n\u0002\nQk\n1(s1, πQk(s1)\n\u0003\n−f M⋆(πk)\n(7.32)\n=\nH\nX\nh=1\nEM⋆,πk\u0014\nQk\nh(sh, ah) −rh −max\na∈A Qk\nh+1(sh+1, a)\n\u0015\n(7.33)\n=\nH\nX\nh=1\n\nXM⋆\nh (πk), W M⋆\nh\n(Qk)\n\u000b\n,\n(7.34)\nwhere the first equality uses the Bellman residual decomposition (Lemma 14), and the\nsecond inequality uses the Bellman rank assumption. For any λ ≥0, using Cauchy-Schwarz,\nwe can bound\nH\nX\nh=1\n\nXM⋆\nh (πk), W M⋆\nh\n(Qk)\n\u000b\n≤\nH\nX\nh=1\n\r\rXM⋆\nh (πk)\n\r\r\n(λI+Σk\nh)−1\n\r\rW M⋆\nh\n(Qk)\n\r\r\nλI+Σk\nh\nFor each h ∈[H], applying the bound in (7.30) gives\n\r\rW M⋆\nh\n(Qk)\n\r\r\nλI+Σk\nh ≤\nq\nλ\n\r\rW M⋆\nh\n(Qk)\n\r\r2\n2 + β ≤λ1/2 + β1/2,\nwhere we have used that\n\r\rW M⋆\nh\n(Qk)\n\r\r\n2 ≤1 by assumption. This allows us to bound\nH\nX\nh=1\n\nXM⋆\nh (πk), W M⋆\nh\n(Qk)\n\u000b\n≲(λ1/2 + β1/2) ·\nH\nX\nh=1\n\r\rXM⋆\nh (πk)\n\r\r\n(λI+Σk\nh)−1.\n(7.35)\nIf we can find a policy πk for which the right-hand side of (7.35) is small, this policy will be\nguaranteed to have low regret. The following lemma shows that such a policy is guaranteed\nto exist.\n143\nLemma 31: For any λ > 0, as long as K ≥Hd log\n\u00001 + λ−1K/d\n\u0001\n, there exists k ∈[K]\nsuch that\n\r\rXM⋆\nh (πk)\n\r\r2\n(λI+Σk\nh)−1 ≲Hd log\n\u00001 + λ−1K/d\n\u0001\nK\n∀h ∈[H].\n(7.36)\nWe choose λ = β, which implies that it suffices to take K ≳Hd log(1 + n/d) to satisfy\nthe condition in Lemma 31. By choosing k to satisfy (7.36) and plugging this bound into\n(7.35), we conclude that the policy πk has\nf M⋆(πM⋆) −f M⋆(πk) ≲H\nr\nβ · Hd log(1 + β−1K/d)\nK\n≲eO\n \nH3/2\nr\nd log(|Q|/δ)\nn\n!\n≲ε\n(7.37)\nas desired.\nFinally, we need to argue that the policy bπ returned by the algorithm is at least as good\nas πk. This is straightforward and we only sketch the argument: By Hoeffding’s inequality\nand a union bound, we have that with probability at least 1 −δ, for all k,\n\f\f\ff M⋆(πk) −bV k\f\f\f ≲\nr\nlog(K/δ)\nn\n,\nwhich implies that f M⋆(bπ) ≳f M⋆(πk) −\nq\nlog(K/δ)\nn\n. The error term here is of lower order\nthan (7.37).\nDeferred proofs.\nTo finish up, we prove Lemma 31.\nProof of Lemma 31. To prove the result, we need a variant of the elliptic potential lemma\n(Lemma 11).\nLemma 32 (e.g. Lemma 19.4 in Lattimore and Szepesv´ari [60]): Let a1, . . . , aT ∈\nRd satisfy ∥at∥2 ≤1 for all t ∈[T]. Fix λ > 0, and let Vt = λI + P\ns<t asa T\ns . Then\nT\nX\nt=1\nlog(1 + ∥at∥2\nV −1\nt\n) ≤d log\n\u00001 + λ−1T/d\n\u0001\n.\n(7.38)\nFor any λ > 0, applying this result for each h ∈[H] and summing gives\nK\nX\nk=1\nH\nX\nh=1\nlog\n\u00001 +\n\r\rXM⋆\nh (πk)\n\r\r2\n(λI+Σk\nh)−1\n\u0001\n≤Hd log\n\u00001 + λ−1K/d\n\u0001\n.\nThis implies that there exists k such that\nH\nX\nh=1\nlog\n\u00001 +\n\r\rXM⋆\nh (πk)\n\r\r2\n(λI+Σk\nh)−1\n\u0001\n≤Hd log\n\u00001 + λ−1K/d\n\u0001\nK\n,\n144\nwhich means that for all h ∈[H], log\n\u00001 +\n\r\rXM⋆\nh (πk)\n\r\r2\n(λI+Σk\nh)−1\n\u0001\n≤\nHd log(1+λ−1K/d)\nK\n, or\nequivalently:\n\r\rXM⋆\nh (πk)\n\r\r2\n(λI+Σk\nh)−1 ≤exp\n \nHd log\n\u00001 + λ−1K/d\n\u0001\nK\n!\n−1.\nAs long as K ≥Hd log\n\u00001 + λ−1K/d\n\u0001\n, using that ex ≤1 + 2x for 0 ≤x ≤1, we have\n\r\rXM⋆\nh (πk)\n\r\r2\n(λI+Σk\nh)−1 ≤2Hd log\n\u00001 + λ−1K/d\n\u0001\nK\n.\n7.3.3\nBellman Rank: Examples\nWe now highlight concrete examples of models with low Bellman rank. We start with famil-\niar examples, the introduce new models that allow for nonlinear function approximation.\nExample 7.1 (Tabular MDPs). If M is a tabular MDP with |S| ≤S and |A| ≤A, we can\nwrite the Bellman residual for any function Q and policy π as\nEh(π, Q) = EM,πh\nQh(sh, ah) −\n\u0010\nrh + max\na\nQh+1(sh+1, a)\n\u0011i\n=\nX\ns,a\nd\nM,π\nh\n(s, a) EM\n\u0014\nQh(s, a) −\n\u0012\nrh + max\na′\nQh+1(sh+1, a′)\n\u0013\n| sh = s, ah = a\n\u0015\n.\nIt follows that if we define\nXM\nh (π) =\n\b\nd\nM,π\nh\n(s, a)\n\t\ns∈S,a∈A ∈RSA\nand\nW M\nh (Q) =\n\u001a\nEM\n\u0014\nQh(s, a) −\n\u0012\nrh + max\na′\nQh+1(sh+1, a′)\n\u0013\n| sh = s, ah = a\n\u0015\u001b\ns∈S,a∈A\n∈RSA,\nwe have\nEh(π, Q) = ⟨XM\nh (π), W M\nh (Q)⟩.\nThis shows that dB(M) ≤SA.\n◁\nExample 7.2 (Low-Rank MDPs). The calculation in (7.21) shows that by choosing XM\nh (π) :=\nEM,π[ϕ(sh, ah)] ∈Rd and W M\nh (Q) := θ\nQ\nh −wM\nh −˜θ\nM,Q\nh\n∈Rd, any Low-Rank MDP M has\ndB(M) ≤d. When specialized to this setting, the regret of BiLinUCB is worse than that\nof LSVI-UCB (though still polynomial in all of the problem parameters). This is because\nBiLinUCB is a more general algorithm, and does not take advantage of an additional feature\nof the Low-Rank MDP model known as Bellman completeness: If M is a Low-Rank MDP,\nthen for all Q ∈Q, we have T M\nh Qh+1 ∈Qh+1. By using a more specialized relative of\nBiLinUCB that incorporates a modified confidence set construction to exploit completeness,\nit is possible to match and actually improve upon the regret of LSVI-UCB [49].\n◁\nWe now explore Bellman rank for some MDP families that have not already been covered.\n145\nExample 7.3 (Low Occupancy Complexity). An MDP M is said to have low occupancy\ncomplexity if there exists a feature map ϕM(s, a) ∈Rd such that for all π, there exists\nθ\nM,π\nh\n∈Rd such that\nd\nM,π\nh\n(s, a) =\n\nϕM(s, a), θ\nM,π\nh\n\u000b\n.\n(7.39)\nNote that neither ϕM nor θM,π is assumed to be known to the learner.\nIf M has low\noccupancy complexity, then for any value function Q and policy π, we have\nEh(π, Q) = EM,πh\nQh(sh, ah) −\n\u0010\nrh + max\na\nQh+1(sh+1, a)\n\u0011i\n=\nX\ns,a\nd\nM,π\nh\n(s, a) EM\n\u0014\nQh(s, a) −\n\u0012\nrh + max\na′\nQh+1(sh+1, a′)\n\u0013\n| sh = s, ah = a\n\u0015\n=\nX\ns,a\n\nϕM(s, a), θ\nM,π\nh\n\u000b\nEM\n\u0014\nQh(s, a) −\n\u0012\nrh + max\na′\nQh+1(sh+1, a′)\n\u0013\n| sh = s, ah = a\n\u0015\n=\n*\nθ\nM,π\nh\n,\nX\ns,a\nϕM(s, a) EM\n\u0014\nQh(s, a) −\n\u0012\nrh + max\na′\nQh+1(sh+1, a′)\n\u0013\n| sh = s, ah = a\n\u0015+\n.\nIt follows that if we define\nXM\nh (π) = θ\nM,π\nh\nand\nW M\nh (Q) =\nX\ns,a\nϕM(s, a) EM\n\u0014\nQh(s, a) −\n\u0012\nrh + max\na′\nQh+1(sh+1, a′)\n\u0013\n| sh = s, ah = a\n\u0015\n,\nthen Eh(π, Q) = ⟨XM\nh (π), W M\nh (Q)⟩, which shows that dB(M) ≤d.\nThis setting subsumes tabular MDPs and low-rank MDPs, but is substantially more\ngeneral. Notably, low occupancy complexity allows for nonlinear function approximation:\nAs long as the occupancies satisfy (7.39), the Bellman rank is at most d for any class Q,\nwhich might consist of neural networks or other nonlinear models.\n◁\nWe close with two more new examples.\nExample 7.4 (LQR). A classical problem in continuous control is the Linear Quadratic\nRegulator, or LQR. Here, we have S = A = Rd, and states evolve via\nsh+1 = AMsh + BMah + ζh,\nwhere ζh ∼N(0, I), and s1 ∼N(0, I). We assume that rewards have the form17\nrh = −s⊤\nh QMsh −a⊤\nh RMah\nfor matrices QM, RM ⪰0. A classical result, dating back to Kalman, is that the optimal\ncontroller for this system is a linear mapping of the form\nπM,h(s) = KM\nh s,\nand that the value function QM,⋆(s, a) = (s, a)⊤P M\nh (s, a) is quadratic. Hence, it suffices to\ntake Q to be the set of all quadratic functions in (s, a). With this choice, it can be shown\nthat dB(M) ≤d2 + 1. The basic idea is to choose XM\nh (π) = (vec(EM,π\u0002\nshs⊤\nh\n\u0003\n), 1) and use\nthe quadratic structure of the value functions.\n◁\n17LQR is typically stated in terms of losses; we negate because we consider rewards.\n146\nExample 7.5 (Linear Q⋆/V ⋆). In Proposition 45, we showed that for RL with linear\nfunction approximation, assuming only that QM⋆,⋆is linear is not enough to achieve low\nregret. It turns out that if we also assume in addition that V M⋆,⋆is linear, the situation\nimproves.\nConsider an MDP M. Assume that there known feature maps ϕ(s, a) ∈Rd and ψ(s′) ∈\nRd such that\nQ\nM,⋆\nh\n(s, a) = ⟨ϕ(s, a), θM\nh ⟩,\nand\nV\nM,⋆\nh\n(s) = ⟨ψ(s), wM\nh ⟩.\nLet\nQ =\n\u001a\nQ | Qh(s, a) = ⟨ϕ(s, a), θh⟩: θh ∈Rd, ∃w max\na∈A ⟨ϕ(s, a), θh⟩= ⟨ψ(s), w⟩∀s\n\u001b\n.\nThen dB(M) ≤2d. We will not prove this result, but the basic idea is to choose XM\nh (π) =\nEM,π[(ϕ(sh, ah), ψ(sh+1))].\n◁\nSee Jiang et al. [46], Du et al. [34] for further examples.\n7.3.4\nGeneralizations of Bellman Rank\nWhile we gave (7.24) as the definition for Bellman rank, there are many variations on the\nassumption that also lead to low regret. One well-known variant is V-type Bellman rank\n[46], which asserts that for all π ∈Π and Q ∈Q,\nEM,π EM\nsh+1|sh,ah∼πQ(sh)\n\u0014\nQh(sh, ah) −rh −max\na∈A Qh+1(sh+1, a)\n\u0015\n= ⟨XM\nh (π), W M\nh (Q)⟩. (7.40)\nThis is the same as the definition (7.24) (which is typically referred to as Q-type Bellman\nrank), except that we take ah = πQ(sh) instead of ah = π(sh).18\nWith an appropriate\nmodification, BiLinUCB can be shown to give sample complexity guarantees that scale with\nV-type Bellman rank instead of Q-type.\nThis definition captures meaningful classes of\ntractable RL models that are not captured by the Q-type definition (7.24), with a canonical\nexample being Block MDPs.\nExample 7.6 (Block MDP). The Block MDP [46, 33, 64] is a model in which the (“ob-\nserved”) state space S is large/high-dimensional, but the dynamics are governed by a (small)\nlatent state space Z. Formally, a Block MDP M = (S, A, P, R, H, d1) is defined based on\nan (unobserved) latent state space Z, with zh denoting the latent state at layer h. We first\ndescribe the dynamics for the latent space. Given initial latent state z1, the latent states\nevolve via\nzh+1 ∼P\nM,latent\nh\n(zh, ah).\nThe latent state zh is not observed. Instead, we observe\nsh ∼qM\nh (zh),\nwhere qM\nh : Z →∆(S) is an emission distribution with the property that supp(qh(z)) ∩\nsupp(qh(z′)) = ∅if z ̸= z′. This property (decodability) ensures that there exists a unique\n18The name “V-type” refers to the fact that (7.40) only depends on Q through the induced V -function\ns 7→Qh(s, πQ(s)), while (7.24) depends on the full Q-function, hence “Q-type”.\n147\nmapping ϕM\nh : S →Z that maps the observed state sh to the corresponding latent state zh.\nWe assume that RM\nh (s, a) = RM\nh (ϕM\nh (s), a), which implies that optimal policy πM depends\nonly on the endogenous latent state, i.e. πM,h(s) = πM,h(ϕM\nh (s)).\nThe main challenge of learning in Block MDPs is that the decoder ϕM is not known\nto the learner in advance.\nIndeed, given access to the decoder, one can obtain regret\npoly(H, |Z|, |A|) ·\n√\nT by applying tabular reinforcement learning algorithms to the latent\nstate space. In light of this, the aim of the Block MDP setting is to obtain sample com-\nplexity guarantees that are independent of the size of the observed state space |S|, and\nscale as poly(|Z|, |A|, H, log|F|), where F is an appropriate class of function approximators\n(typically either a value function class Q containing QM,⋆or a class of decoders Φ that\nattempts to model ϕM directly).\nWe now show that the Block MDP setting admits low V-type Bellman rank. Observe\nthat we can write\nEM,π EM\nsh+1|ah,sh∼πQ(sh)\n\u0014\nQh(sh, πQ(sh)) −rh −max\na∈A Qh+1(sh+1, πQ(sh+1))\n\u0015\n=\nX\nz∈Z\nd\nM,π\nh\n(z) Es∼qM\nh (z) EM\nsh+1|sh,ah∼πQ(sh)\n\u0014\nQh(sh, πQ(sh)) −rh −max\na∈A Qh+1(sh+1, πQ(sh+1))\n\u0015\n.\nThis implies that we can take\nXM\nh (π) =\n\b\nd\nM,π\nh\n(z)\n\t\nz∈Z\nand\nW M\nh (Q) =\n\u001a\nEs∼qM\nh (z) EM\nsh+1|sh,ah∼πQ(sh)\n\u0014\nQh(sh, πQ(sh)) −rh −max\na∈A Qh+1(sh+1, πQ(sh+1))\n\u0015\u001b\nz∈Z\nso that the V-type Bellman rank is at most |Z|. This means that as long as Q contains\nQM,⋆, we can obtain sample complexity guarantees that scale with |Z| rather than |S|, as\ndesired.\n◁\nThere are a number of variants of Bellman rank, including Bilinear rank [34] and\nBellman-Eluder dimension [49], which subsume and slightly generalize both Bellman rank\ndefinitions.\n7.3.5\nDecision-Estimation Coefficient for Bellman Rank\nAn alternative to the BiLinUCB method is to appeal to the E2D meta-algorithm and\nthe Decision-Estimation Coefficient.\nThe following result [40] shows that the Decision-\nEstimation Coefficient is always bounded for classes with low Bellman rank.\nProposition 48: For any class of MDPs M for which all M ∈M have Bellman rank\nat most d, we have\ndecγ(M) ≲H2d\nγ\n.\n(7.41)\n148\nThis implies that that E2D meta-algorithm has E[Reg] ≲H√dT · EstH whenever we have\naccess to a realizable model class with low Bellman rank. As a special case, for any finite\nclass M, using averaged exponential weights as an estimation oracle gives\nE[Reg] ≲H\np\ndT log|M|.\n(7.42)\nWe will not prove Proposition 48, but interested readers can refer to Foster et al. [40].\nThe result can be proven using two approaches, both of which build on the techniques we\nhave already covered. The first approach is to apply a more general version of the PC-IGW\nalgorithm from Section 6.6, which incorporates optimal design in the space of policies. The\nsecond approach is to move to the Bayesian DEC and appeal to posterior sampling, as in\nSection 4.4.2.\nValue-based guarantees via optimistic estimation.\nIn general, the model estimation\ncomplexity log|M| in (7.42) can be arbitrarily large compared to the complexity log|Q| for\na realizable value function class (consider the low-rank MDP—since µ is unknown, it is\nnot possible to construct a small model class M). To derive value-based guarantees along\nthe lines of what BiLinUCB achieves in Proposition 47, a natural approach is to replace\nthe Hellinger distance appearing in the DEC with a divergence tailored to value function\niteration, following the development in Sections 6.7.2 and 6.7.3. Once such choice is the\ndivergence\nDπ\nsbr(Q ∥M) =\nH\nX\nh=1\n\u0010\nEM,πh\nQh(sh, ah) −\n\u0010\nrh + max\na\nQh+1(sh+1, a)\n\u0011i\u00112\n,\nwhich measures the squared bellman residual for an estimated value function under M.\nWith this choice, we appeal to the optimistic E2D algorithm (E2D.Opt) from Section 6.7.3.\nOne can show that the optimistic DEC for this divergence is bounded as\no-decDsbr\nγ\n(M) ≲H · d\nγ\n.\nThis implies that E2D.Opt, with an appropriate choice of estimation algorithm tailored to\nDπ\nsbr(· ∥·), achieves\nE[Reg] ≲(H2d log|Q|)1/2T 3/4.\nNote that due to the asymmetric nature of Dπ\nsbr(· ∥·), it is critical to appeal to optimistic\nestimation to derive this result. Indeed, the non-optimistic generalized DEC decDsbr\nγ\ndoes\nnot enjoy a polynomial bound. See Foster et al. [41] for details.\n[Note: This subsection will be expanded in the next version.]\n149\nReferences\n[1] Y. Abbasi-Yadkori, D. P´al, and C. Szepesv´ari. Improved algorithms for linear stochastic\nbandits. In Advances in Neural Information Processing Systems, 2011.\n[2] N. Abe and P. M. Long. Associative reinforcement learning using linear probabilis-\ntic concepts.\nIn Proceedings of the Sixteenth International Conference on Machine\nLearning, pages 3–11. Morgan Kaufmann Publishers Inc., 1999.\n[3] A. Agarwal and T. Zhang. Model-based RL with optimistic posterior sampling: Struc-\ntural conditions and sample complexity. arXiv preprint arXiv:2206.07659, 2022.\n[4] A. Agarwal, D. P. Foster, D. Hsu, S. M. Kakade, and A. Rakhlin. Stochastic convex\noptimization with bandit feedback. SIAM Journal on Optimization, 23(1):213–240,\n2013.\n[5] A. Agarwal, S. Kakade, A. Krishnamurthy, and W. Sun. FLAMBE: Structural com-\nplexity and representation learning of low rank MDPs. Neural Information Processing\nSystems (NeurIPS), 2020.\n[6] R. Agrawal. Sample mean based index policies by o (log n) regret for the multi-armed\nbandit problem. Advances in applied probability, 27(4):1054–1078, 1995.\n[7] S. Agrawal and N. Goyal. Analysis of thompson sampling for the multi-armed bandit\nproblem. In Conference on learning theory, pages 39–1. JMLR Workshop and Confer-\nence Proceedings, 2012.\n[8] S. Agrawal and N. Goyal.\nThompson sampling for contextual bandits with linear\npayoffs. In International conference on machine learning, pages 127–135. PMLR, 2013.\n[9] K. Amin, M. Kearns, and U. Syed. Bandits, query learning, and the haystack dimen-\nsion. In Proceedings of the 24th Annual Conference on Learning Theory, pages 87–106.\nJMLR Workshop and Conference Proceedings, 2011.\n[10] M. Anthony and P. L. Bartlett.\nNeural network learning: Theoretical foundations.\nCambridge University Press, 2009.\n[11] J.-Y. Audibert and S. Bubeck. Minimax policies for adversarial and stochastic bandits.\nIn COLT, volume 7, pages 1–122, 2009.\n[12] P. Auer. Using confidence bounds for exploitation-exploration trade-offs. Journal of\nMachine Learning Research, 3(Nov):397–422, 2002.\n[13] P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analysis of the multiarmed bandit\nproblem. Machine learning, 47(2-3):235–256, 2002.\n[14] P. Auer, R. Ortner, and C. Szepesv´ari. Improved rates for the stochastic continuum-\narmed bandit problem. In International Conference on Computational Learning The-\nory, pages 454–468. Springer, 2007.\n[15] A. Ayoub, Z. Jia, C. Szepesvari, M. Wang, and L. Yang.\nModel-based reinforce-\nment learning with value-targeted regression. In International Conference on Machine\nLearning, pages 463–474. PMLR, 2020.\n150\n[16] M. G. Azar, I. Osband, and R. Munos.\nMinimax regret bounds for reinforcement\nlearning. In International Conference on Machine Learning, pages 263–272, 2017.\n[17] R. Bellman. The theory of dynamic programming. Bulletin of the American Mathe-\nmatical Society, 60(6):503–515, 1954.\n[18] B. Bilodeau, D. J. Foster, and D. Roy. Tight bounds on minimax regret under loga-\nrithmic loss via self-concordance. In International Conference on Machine Learning,\n2020.\n[19] S. Boucheron, O. Bousquet, and G. Lugosi. Theory of classification: A survey of some\nrecent advances. ESAIM: probability and statistics, 9:323–375, 2005.\n[20] O. Bousquet, S. Boucheron, and G. Lugosi. Introduction to statistical learning theory.\nIn Advanced lectures on machine learning, pages 169–207. Springer, 2004.\n[21] S. Bubeck and R. Eldan. Multi-scale exploration of convex functions and bandit convex\noptimization. In Conference on Learning Theory, pages 583–589, 2016.\n[22] S. Bubeck, O. Dekel, T. Koren, and Y. Peres. Bandit convex optimization:\n√\nT regret\nin one dimension. In Conference on Learning Theory, pages 266–278, 2015.\n[23] S. Bubeck, Y. T. Lee, and R. Eldan. Kernel-based methods for bandit convex opti-\nmization. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of\nComputing, pages 72–85, 2017.\n[24] N. Cesa-Bianchi and G. Lugosi. Minimax regret under log loss for general classes of\nexperts. In Conference on Computational Learning Theory, 1999.\n[25] N. Cesa-Bianchi and G. Lugosi. Prediction, Learning, and Games. Cambridge Univer-\nsity Press, New York, NY, USA, 2006. ISBN 0521841089.\n[26] W. Chu, L. Li, L. Reyzin, and R. E. Schapire. Contextual bandits with linear payoff\nfunctions. In International Conference on Artificial Intelligence and Statistics, 2011.\n[27] T. M. Cover. Universal portfolios. Mathematical Finance, 1991.\n[28] V. Dani, T. P. Hayes, and S. M. Kakade. Stochastic linear optimization under bandit\nfeedback. In Conference on Learning Theory (COLT), 2008.\n[29] C. Dann, M. Mohri, T. Zhang, and J. Zimmert. A provably efficient model-free posterior\nsampling method for episodic reinforcement learning. Advances in Neural Information\nProcessing Systems, 34:12040–12051, 2021.\n[30] S. Dean, H. Mania, N. Matni, B. Recht, and S. Tu. On the sample complexity of the\nlinear quadratic regulator. Foundations of Computational Mathematics, 20(4):633–679,\n2020.\n[31] S. Dong, B. Van Roy, and Z. Zhou. Provably efficient reinforcement learning with\naggregated states. arXiv preprint arXiv:1912.06366, 2019.\n[32] D. L. Donoho and R. C. Liu. Geometrizing rates of convergence. Annals of Statistics,\n1987.\n151\n[33] S. Du, A. Krishnamurthy, N. Jiang, A. Agarwal, M. Dudik, and J. Langford. Prov-\nably efficient RL with rich observations via latent state decoding. In International\nConference on Machine Learning, pages 1665–1674. PMLR, 2019.\n[34] S. S. Du, S. M. Kakade, J. D. Lee, S. Lovett, G. Mahajan, W. Sun, and R. Wang. Bi-\nlinear classes: A structural framework for provable generalization in RL. International\nConference on Machine Learning, 2021.\n[35] A. D. Flaxman, A. T. Kalai, and H. B. McMahan. Online convex optimization in the\nbandit setting: gradient descent without a gradient. In Proceedings of the sixteenth\nannual ACM-SIAM symposium on Discrete algorithms, pages 385–394, 2005.\n[36] D. J. Foster and A. Rakhlin. Beyond UCB: Optimal and efficient contextual bandits\nwith regression oracles. International Conference on Machine Learning (ICML), 2020.\n[37] D. J. Foster, A. Agarwal, M. Dud´ık, H. Luo, and R. E. Schapire. Practical contextual\nbandits with regression oracles. International Conference on Machine Learning, 2018.\n[38] D. J. Foster, S. Kale, H. Luo, M. Mohri, and K. Sridharan. Logistic regression: The\nimportance of being improper. Conference on Learning Theory, 2018.\n[39] D. J. Foster, C. Gentile, M. Mohri, and J. Zimmert. Adapting to misspecification in\ncontextual bandits. Advances in Neural Information Processing Systems, 33, 2020.\n[40] D. J. Foster, S. M. Kakade, J. Qian, and A. Rakhlin. The statistical complexity of\ninteractive decision making. arXiv preprint arXiv:2112.13487, 2021.\n[41] D. J. Foster, N. Golowich, J. Qian, A. Rakhlin, and A. Sekhari. A note on model-\nfree reinforcement learning with the decision-estimation coefficient.\narXiv preprint\narXiv:2211.14250, 2022.\n[42] D. J. Foster, A. Rakhlin, A. Sekhari, and K. Sridharan. On the complexity of adver-\nsarial decision making. arXiv preprint arXiv:2206.13063, 2022.\n[43] D. J. Foster, N. Golowich, and Y. Han. Tight guarantees for interactive decision making\nwith the decision-estimation coefficient. arXiv preprint arXiv:2301.08215, 2023.\n[44] E. Hazan and S. Kale. An online portfolio selection algorithm with regret logarithmic\nin price variation. Mathematical Finance, 2015.\n[45] S. R. Howard, A. Ramdas, J. McAuliffe, and J. Sekhon. Time-uniform chernoff bounds\nvia nonnegative supermartingales. Probability Surveys, 17:257–317, 2020.\n[46] N. Jiang, A. Krishnamurthy, A. Agarwal, J. Langford, and R. E. Schapire. Contex-\ntual decision processes with low Bellman rank are PAC-learnable. In International\nConference on Machine Learning, pages 1704–1713, 2017.\n[47] C. Jin, A. Krishnamurthy, M. Simchowitz, and T. Yu. Reward-free exploration for\nreinforcement learning. In International Conference on Machine Learning, pages 4870–\n4879. PMLR, 2020.\n[48] C. Jin, Z. Yang, Z. Wang, and M. I. Jordan. Provably efficient reinforcement learning\nwith linear function approximation. In Conference on Learning Theory, pages 2137–\n2143, 2020.\n152\n[49] C. Jin, Q. Liu, and S. Miryoosefi. Bellman eluder dimension: New rich classes of RL\nproblems, and sample-efficient algorithms.\nNeural Information Processing Systems,\n2021.\n[50] K.-S. Jun and C. Zhang. Crush optimism with pessimism: Structured bandits beyond\nasymptotic optimality. Advances in Neural Information Processing Systems, 33, 2020.\n[51] A. Kalai and S. Vempala.\nEfficient algorithms for universal portfolios.\nJournal of\nMachine Learning Research, 2002.\n[52] J. Kiefer and J. Wolfowitz. The equivalence of two extremum problems. Canadian\nJournal of Mathematics, 12:363–366, 1960.\n[53] R. Kleinberg. Nearly tight bounds for the continuum-armed bandit problem. Advances\nin Neural Information Processing Systems, 17:697–704, 2004.\n[54] R. Kleinberg, A. Slivkins, and E. Upfal. Bandits and experts in metric spaces. Journal\nof the ACM (JACM), 66(4):1–77, 2019.\n[55] A. Krishnamurthy, A. Agarwal, and J. Langford. PAC reinforcement learning with rich\nobservations. In Advances in Neural Information Processing Systems, pages 1840–1848,\n2016.\n[56] T. L. Lai and H. Robbins. Asymptotically efficient adaptive allocation rules. Advances\nin Applied Mathematics, 6(1):4–22, 1985.\n[57] J. Langford and T. Zhang. The epoch-greedy algorithm for multi-armed bandits with\nside information. In Advances in neural information processing systems, pages 817–824,\n2008.\n[58] T. Lattimore. Improved regret for zeroth-order adversarial bandit convex optimisation.\nMathematical Statistics and Learning, 2(3):311–334, 2020.\n[59] T. Lattimore and C. Szepesv´ari. An information-theoretic approach to minimax regret\nin partial monitoring. In Conference on Learning Theory, pages 2111–2139. PMLR,\n2019.\n[60] T. Lattimore and C. Szepesv´ari. Bandit algorithms. Cambridge University Press, 2020.\n[61] G. Li, P. Kamath, D. J. Foster, and N. Srebro. Eluder dimension and generalized rank.\narXiv preprint arXiv:2104.06970, 2021.\n[62] L. Li. A unifying framework for computational reinforcement learning theory. Rutgers,\nThe State University of New Jersey—New Brunswick, 2009.\n[63] H. Luo, C.-Y. Wei, and K. Zheng. Efficient online portfolio with logarithmic regret. In\nAdvances in Neural Information Processing Systems, 2018.\n[64] D. Misra, M. Henaff, A. Krishnamurthy, and J. Langford. Kinematic state abstrac-\ntion and provably efficient rich-observation reinforcement learning. In International\nconference on machine learning, pages 6961–6971. PMLR, 2020.\n153\n[65] A. Modi, N. Jiang, A. Tewari, and S. Singh.\nSample complexity of reinforcement\nlearning using linearly combined model ensembles.\nIn International Conference on\nArtificial Intelligence and Statistics, pages 2010–2020. PMLR, 2020.\n[66] M. Opper and D. Haussler. Worst case prediction over sequences under log loss. In\nThe Mathematics of Information Coding, Extraction and Distribution, 1999.\n[67] L. Orseau, T. Lattimore, and S. Legg. Soft-bayes: Prod for mixtures of experts with\nlog-loss. In International Conference on Algorithmic Learning Theory, 2017.\n[68] Y. Polyanskiy. Information theoretic methods in statistics and computer science. 2020.\nURL https://people.lids.mit.edu/yp/homepage/sdpi course.html.\n[69] A. Rakhlin and K. Sridharan.\nStatistical learning and sequential prediction, 2012.\nAvailable at http://www.mit.edu/∼rakhlin/courses/stat928/stat928 notes.pdf.\n[70] A. Rakhlin, K. Sridharan, and A. Tewari. Sequential complexities and uniform martin-\ngale laws of large numbers. Probability Theory and Related Fields, 161(1-2):111–153,\n2015.\n[71] S. Rendle, C. Freudenthaler, and L. Schmidt-Thieme. Factorizing personalized markov\nchains for next-basket recommendation. In Proceedings of the 19th International Con-\nference on World Wide Web, WWW 2010, Raleigh, North Carolina, USA, April 26-30,\n2010, pages 811–820. ACM, 2010.\n[72] J. Rissanen. Complexity of strings in the class of markov sources. IEEE Transactions\non Information Theory, 32(4):526–532, 1986.\n[73] H. Robbins. Some aspects of the sequential design of experiments. 1952.\n[74] D. Russo and B. Van Roy. Eluder dimension and the sample complexity of optimistic\nexploration. In Advances in Neural Information Processing Systems, pages 2256–2264,\n2013.\n[75] D. Russo and B. Van Roy. Learning to optimize via posterior sampling. Mathematics\nof Operations Research, 39(4):1221–1243, 2014.\n[76] D. Russo and B. Van Roy. Learning to optimize via information-directed sampling.\nOperations Research, 66(1):230–252, 2018.\n[77] S. Shalev-Shwartz and S. Ben-David. Understanding machine learning: From theory\nto algorithms. Cambridge university press, 2014.\n[78] Y. M. Shtar’kov. Universal sequential coding of single messages. Problemy Peredachi\nInformatsii, 1987.\n[79] D. Simchi-Levi and Y. Xu. Bypassing the monster: A faster and simpler optimal algo-\nrithm for contextual bandits under realizability. Mathematics of Operations Research,\n2021.\n[80] M. Sion. On general minimax theorems. Pacific J. Math., 8:171–176, 1958.\n[81] R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction. MIT press,\n2018.\n154\n[82] W. R. Thompson. On the likelihood that one unknown probability exceeds another in\nview of the evidence of two samples. Biometrika, 25(3/4):285–294, 1933.\n[83] A. B. Tsybakov. Introduction to Nonparametric Estimation. Springer Publishing Com-\npany, Incorporated, 2008.\n[84] V. Vovk. A game of prediction with expert advice. In Proceedings of the eighth annual\nconference on computational learning theory, pages 51–60. ACM, 1995.\n[85] Y. Wang, R. Wang, and S. M. Kakade.\nAn exponential lower bound for linearly-\nrealizable MDPs with constant suboptimality gap. Neural Information Processing Sys-\ntems (NeurIPS), 2021.\n[86] G. Weisz, P. Amortila, and C. Szepesv´ari. Exponential lower bounds for planning in\nMDPs with linearly-realizable optimal action-value functions. In Algorithmic Learning\nTheory, pages 1237–1264. PMLR, 2021.\n[87] L. Yang and M. Wang. Sample-optimal parametric Q-learning using linearly additive\nfeatures. In International Conference on Machine Learning, pages 6995–7004. PMLR,\n2019.\n[88] H. Yao, C. Szepesv´ari, B. ´A. Pires, and X. Zhang. Pseudo-mdps and factored linear\naction models. In 2014 IEEE Symposium on Adaptive Dynamic Programming and Re-\ninforcement Learning, ADPRL 2014, Orlando, FL, USA, December 9-12, 2014, pages\n1–9. IEEE, 2014.\n[89] B. Yu. Assouad, Fano, and Le Cam. In Festschrift for Lucien Le Cam, pages 423–435.\nSpringer, 1997.\n[90] T. Zhang.\nFeel-good thompson sampling for contextual bandits and reinforcement\nlearning. SIAM Journal on Mathematics of Data Science, 4(2):834–857, 2022.\n[91] H. Zhong, W. Xiong, S. Zheng, L. Wang, Z. Wang, Z. Yang, and T. Zhang. A posterior\nsampling framework for interactive decision making. arXiv preprint arXiv:2211.01962,\n2022.\n155\nA. TECHNICAL TOOLS\nA.1 Probabilistic Inequalities\nA.1.1\nTail Bounds with Stopping Times\nLemma 33 (Hoeffding’s inequality with adaptive stopping time): For i.i.d.\nrandom variables Z1, . . . , ZT taking values in [a, b] almost surely, with probability at\nleast 1 −δ,\n1\nT ′\nT ′\nX\ni=1\nZi −E[Z] ≤(b −a)\nr\nlog(T/δ)\n2T ′\n∀1 ≤T ′ ≤T.\n(A.1)\nAs a consequence, for any random variable τ ∈[T] with the property that for all t ∈[T],\nI {τ ≤t} is a measurable function of Z1, . . . , Zt−1 (τ is called a stopping time), we have\nthat with probability at least 1 −δ,\n1\nτ\nτ\nX\ni=1\nZi −E[Z] ≤(b −a)\nr\nlog(T/δ)\n2τ\n.\n(A.2)\nProof of Lemma 33. Lemma 3 states that for any fixed T ′ ∈[T], with probability at least\n1 −δ,\n1\nT ′\nT ′\nX\ni=1\nZi −E[Z] ≤(b −a)\nr\nlog(T/δ)\n2T ′\n.\n(A.1) follows by applying this result with δ′ = δ/T and taking a union bound over all T\nchoices for T ′ ∈[T]. For (A.2), we observe that\n1\nτ\nτ\nX\ni=1\n(Zi −E[Z]) −(b −a)\nr\nlog(T/δ)\n2τ\n≤max\nT ′∈[T]\n(\n1\nT ′\nT ′\nX\ni=1\n(Zi −E[Z]) −(b −a)\nr\nlog(T/δ)\n2T ′\n)\n.\nThe result now follows from (A.1).\nA.1.2\nTail Bounds for Martingales\nLemma 34: For any sequence of real-valued random variables (Xt)t≤T adapted to a\nfiltration (Ft)t≤T , it holds that with probability at least 1 −δ, for all T ′ ≤T,\nT ′\nX\nt=1\nXt ≤\nT ′\nX\nt=1\nlog\n\u0000Et−1\n\u0002\neXt\u0003\u0001\n+ log(δ−1).\n(A.3)\nProof of Lemma 34. We claim that the sequence\nZτ := exp\n τ\nX\nt=1\nXt −log\n\u0000Et−1\n\u0002\neXt\u0003\u0001\n!\n156\nis a nonnegative supermartingale with respect to the filtration (Fτ)τ≤T . Indeed, for any\nchoice of τ, we have\nEτ−1[Zτ] = Eτ−1\n\"\nexp\n τ\nX\nt=1\nXt −log\n\u0000Et−1\n\u0002\neXt\u0003\u0001\n!#\n= exp\n τ−1\nX\nt=1\nXt −log\n\u0000Et−1\n\u0002\neXt\u0003\u0001\n!\n· Eτ−1\n\u0002\nexp\n\u0000Xτ −log\n\u0000Eτ−1\n\u0002\neXτ \u0003\u0001\u0001\u0003\n= exp\n τ−1\nX\nt=1\nXt −log\n\u0000Et−1\n\u0002\neXt\u0003\u0001\n!\n= Zτ.\nSince Z0 = 1, Ville’s inequality (e.g., Howard et al. [45]) implies that for all λ > 0,\nP0(∃τ : Zτ > λ) ≤1\nλ.\nThe result now follows by the Chernoff method.\nThe next result is a martingale counterpart to Bernstein’s inequality (Lemma 5).\nLemma 35 (Freedman’s inequality (Bernstein for martingales)): Let (Xt)t≤T\nbe a real-valued martingale difference sequence adapted to a filtration (Ft)t≤T .\nIf\n|Xt| ≤R almost surely, then for any η ∈(0, 1/R), with probability at least 1 −δ, for\nall T ′ ≤T,\nT ′\nX\nt=1\nXt ≤η\nT ′\nX\nt=1\nEt−1\n\u0002\nX2\nt\n\u0003\n+ log(δ−1)\nη\n.\nProof of Lemma 35. Without loss of generality, let R = 1, and fix η ∈(0, 1). The result\nfollows by invoking Lemma 34 with ηXt in place of Xt, and by the facts that ea ≤1 + a +\n(e −2)a2 for a ≤1 and 1 + b ≤eb for all b ∈R.\nThe following result is an immediate consequence of Lemma 35.\nLemma 36: Let (Xt)t≤T be a sequence of random variables adapted to a filtration\n(Ft)t≤T . If 0 ≤Xt ≤R almost surely, then with probability at least 1 −δ,\nT\nX\nt=1\nXt ≤3\n2\nT\nX\nt=1\nEt−1[Xt] + 4R log(2δ−1),\nand\nT\nX\nt=1\nEt−1[Xt] ≤2\nT\nX\nt=1\nXt + 8R log(2δ−1).\n157\nA.2 Information Theory\nA.2.1\nProperties of Hellinger Distance\nLemma 37: For any distributions P and Q over a pair of random variables (X, Y ),\nEX∼PX\n\u0002\nD2\nH\n\u0000PY |X, QY |X\n\u0001\u0003\n≤4D2\nH(PX,Y , QX,Y ).\nProof of Lemma 37. Since squared Hellinger distance is an f-divergence, we have\nEX∼PX\n\u0002\nD2\nH\n\u0000PY |X, QY |X\n\u0001\u0003\n= D2\nH\n\u0000PY |X ⊗PX, QY |X ⊗PX\n\u0001\n.\nNext, using that Hellinger distance satisfies the triangle inequality, along with the elemen-\ntary inequality (a + b)2 ≤2(a2 + b2), we have,\nEX∼PX\n\u0002\nD2\nH\n\u0000PY |X, QY |X\n\u0001\u0003\n≤2D2\nH\n\u0000PY |X ⊗PX, QY |X ⊗QX\n\u0001\n+ 2D2\nH\n\u0000QY |X ⊗PX, QY |X ⊗QX\n\u0001\n= 2D2\nH(PX,Y , QX,Y ) + 2D2\nH(PX, QX)\n≤4D2\nH(PX,Y , QX,Y ),\nwhere the final line follows from the data processing inequality.\nLemma 38 (Subadditivity for squared Hellinger distance): Let (X1, F1), . . . , (Xn, Fn)\nbe a sequence of measurable spaces, and let X i = Qi\ni=t Xt and F i = Ni\nt=1 Ft. For\neach i, let Pi(· | ·) and Qi(· | ·) be probability kernels from (X i−1, F i−1) to (Xi, Fi). Let\nP and Q be the laws of X1, . . . , Xn under Xi ∼Pi(· | X1:i−1) and Xi ∼Qi(· | X1:i−1)\nrespectively. Then it holds that\nD2\nH(P, Q) ≤102 log(n) · EP\n\" n\nX\ni=1\nD2\nH(Pi(· | X1:i−1), Qi(· | X1:i−1))\n#\n.\n(A.4)\nA.2.2\nChange-of-Measure Inequalities\nLemma 39 (Pinsker for sub-Gaussian random variables): Suppose that X ∼P\nand Y ∼Q are both σ2-sub-Gaussian. Then\n|EP[X] −EQ[Y ]| ≤\np\n2σ2 · DKL(P ∥Q).\nLemma 40 (Multiplicative Pinsker-type inequality for Hellinger distance):\nlemmampmin Let P and Q be probability measures on (X, F). For all h : X →R with\n0 ≤h(X) ≤R almost surely under P and Q, we have\n|EP[h(X)] −EQ[h(X)]| ≤\nq\n2R(EP[h(X)] + EQ[h(X)]) · D2\nH(P, Q).\n(A.5)\n158\nIn particular,\nEP[h(X)] ≤3 EQ[h(X)] + 4R · D2\nH(P, Q).\n(A.6)\nProof of Lemma 40. Let a measurable event A be fixed. Let p = P(A) and q = Q(A). Then\nwe have\n(p −q)2\n2(p + q) ≤(√p −√q)2 ≤D2\nH((p, 1 −p), (q, 1 −q)) ≤D2\nH(P, Q),\nwhere the third inequality is the data-processing inequality. It follows that\n|p −q| ≤\nq\n2(p + q)D2\nH(P, Q),\nTo deduce the final result for R = 1, we observe that EP[h(X)] =\nR 1\n0 P(h(X) > t)dt and\nlikewise for EQ[h(X)], then apply Jensen’s inequality. The result for general R follows by\nrescaling.\nThe inequality in (A.6) follows by applying the AM-GM inequality to (A.5) and rear-\nranging.\nA.3 Minimax Theorem\nLemma 41 (Sion’s Minimax Theorem [80]): Let X and Y be convex sets in linear\ntopological spaces, and assume X is compact. Let f : X ×Y →R be such that (i) f(x, ·)\nis concave and upper semicontinuous over Y for all x ∈X and (ii) f(·, y) is convex and\nlower semicontinuous over X for all y ∈Y. Then\ninf\nx∈X sup\ny∈Y\nf(x, y) = sup\ny∈Y\ninf\nx∈X f(x, y).\n(A.7)\n159\n",
  "categories": [
    "cs.LG",
    "math.OC",
    "math.ST",
    "stat.ML",
    "stat.TH"
  ],
  "published": "2023-12-27",
  "updated": "2023-12-27"
}