{
  "id": "http://arxiv.org/abs/2401.00330v3",
  "title": "Two-Step Offline Preference-Based Reinforcement Learning with Constrained Actions",
  "authors": [
    "Yinglun Xu",
    "Tarun Suresh",
    "Rohan Gumaste",
    "David Zhu",
    "Ruirui Li",
    "Zhengyang Wang",
    "Haoming Jiang",
    "Xianfeng Tang",
    "Qingyu Yin",
    "Monica Xiao Cheng",
    "Qi Zeng",
    "Chao Zhang",
    "Gagandeep Singh"
  ],
  "abstract": "Preference-based reinforcement learning (PBRL) in the offline setting has\nsucceeded greatly in industrial applications such as chatbots. A two-step\nlearning framework where one applies a reinforcement learning step after a\nreward modeling step has been widely adopted for the problem. However, such a\nmethod faces challenges from the risk of reward hacking and the complexity of\nreinforcement learning. To overcome the challenge, our insight is that both\nchallenges come from the state-actions not supported in the dataset. Such\nstate-actions are unreliable and increase the complexity of the reinforcement\nlearning problem at the second step. Based on the insight, we develop a novel\ntwo-step learning method called PRC: preference-based reinforcement learning\nwith constrained actions. The high-level idea is to limit the reinforcement\nlearning agent to optimize over a constrained action space that excludes the\nout-of-distribution state-actions. We empirically verify that our method has\nhigh learning efficiency on various datasets in robotic control environments.",
  "text": "TWO-STEP OFFLINE PREFERENCE-BASED\nREINFORCEMENT\nLEARNING\nWITH\nCONSTRAINED\nACTIONS\nYinglun Xu1∗, Tarun Suresh1, Rohan Gumaste1, David Zhu1, Ruirui Li2,\nZhengyang Wang2, Haoming Jiang2, Xianfeng Tang2, Qingyu Yin2,\nMonica Xiao Cheng2, Qi Zeng1, Chao Zhang3, Gagandeep Singh1\n1University of Illinois Urbana-Champaign, 2Amazon, 3Georgia institute of technology\nABSTRACT\nPreference-based reinforcement learning (PBRL) in the offline setting has suc-\nceeded greatly in industrial applications such as chatbots. A two-step learning\nframework where one applies a reinforcement learning step after a reward model-\ning step has been widely adopted for the problem. However, such a method faces\nchallenges from the risk of reward hacking and the complexity of reinforcement\nlearning. To overcome the challenge, our insight is that both challenges come from\nthe state-actions not supported in the dataset. Such state-actions are unreliable and\nincrease the complexity of the reinforcement learning problem at the second step.\nBased on the insight, we develop a novel two-step learning method called PRC:\npreference-based reinforcement learning with constrained actions. The high-level\nidea is to limit the reinforcement learning agent to optimize over a constrained\naction space that excludes the out-of-distribution state-actions. We empirically\nverify that our method has high learning efficiency on various datasets in robotic\ncontrol environments.\n1\nINTRODUCTION\nDeep Reinforcement Learning (DRL) is a learning paradigm for solving sequential decision-making\nproblems and has rich real-world applications (Sutton & Barto, 2018; Luong et al., 2019; Haydari\n& Yılmaz, 2020). However, traditional DRL approaches require numerical reward feedback during\nlearning, which in practice can be hard to design or obtain. In contrast, non-numerical preference\nfeedback is more accessible in most cases (Wirth & F¨urnkranz, 2013). As a result, preference-based\nreinforcement learning (PBRL) that only requires preference feedback has become a more realistic\nlearning paradigm (Christiano et al., 2017). Recently, as a special instance of PBRL, reinforcement\nlearning from human feedback (RLHF) that utilizes human preference has drawn much attention\nand achieved great success in many NLP tasks (Ouyang et al., 2022). Although a majority of current\nPBRL works focus on the online learning setting (Ibarz et al., 2018; Lee et al., 2021), the PBRL\nlearning in the offline setting is more realistic Rafailov et al. (2023). In the online learning setting,\nthe learning agent continuously interacts with the learning environment to collect new preference\ndata throughout the training process; In the offline learning setting, the agent receives a batch of\npreference data before training starts. Compared to the online learning setting, the offline learning\nsetting is more accessible as it does not require customized feedback throughout the training process.\nIt is critical to investigate how to make PBRL learning efficient and practical in the offline setting.\nParticularly, we are interested in the most popular learning framework that is widely applied in\nvarious PBRL literature and industrial applications, which we call ‘two-step learning.’\nTwo-step learning framework: In a typical two-step learning framework (Ibarz et al., 2018; Chris-\ntiano et al., 2017), the first learning step is reward modeling, where the learning agent approximates\nthe reward model of the environment that can best explain the observations from the training dataset.\nThe second learning step is a reinforcement learning phase where the learning agent learns a policy\nbased on the reward model acquired in the previous step. There are multiple reasons behind the vast\n∗Work done during an internship at Amazon. Corresponding author: yinglun6@illinois.edu\n1\narXiv:2401.00330v3  [cs.LG]  25 Oct 2024\npopularity of the two-step learning paradigm: 1. similar to the motivation of inverse reinforcement\nlearning (Arora & Doshi, 2021), the learned environment model provides a succinct and transferable\ndefinition of the task; 2. the method is modular in that each phase of learning can be implemented\nby existing well-developed methods. Despite its popularity, this method faces two main challenges.\n• Reward over-optimization/Pessimistic learning: Pessimistic learning has always been\nthe key challenge in offline learning settings. Due to the distribution mismatch between the\ntrajectories induced by the policy and that of the dataset, the agent cannot properly evaluate\nall possible possible policies.\n• Reinforcement learning complexity: Implementing Reinforcement learning efficiently is\nknown to be challenging Dulac-Arnold et al. (2021). In addition, for preference feedback,\nthe reward model is learned from indirect preference signals rather than actual true rewards.\nThe learned reward model may look very different from the actual reward, making the\ntraining step more unstable.\nThe challenges are empirically verified in Section 5. To solve the first challenge, the most popular\napproach is to apply a penalty on the evaluated performance of a policy during the reinforcement\nlearning step. The penalty is the KL divergence between a policy and the behavior policy that can\nbest describe the training data distribution. Under the penalty, the agent is encouraged to stay close\nto the dataset distribution while trying to find a policy that optimizes the learned reward model.\nHowever, the reinforcement learning problem in this problem may not be easy to solve due to the\nsecond challenge, which is empirically verified in Section 5.\nThe approaches to solving the second challenge mainly focus on the bandit setting for NLP applica-\ntions. The bandit setting is a special case of RL that has no state transition. Current methods include\ndirect preference alignment Rafailov et al. (2023) and rejection sampling Liu et al. (2023), both of\nwhich cannot be directly applied to the general RL setting. To the best of our knowledge, no existing\ntwo-phase learning approach considers tackling both challenges simultaneously.\nOur contributions. In this work, we propose a novel two-step learning PBRL algorithm PRC that\ntackles the above two main challenges simultaneously. Our key insight is that both challenges are\ninduced by the same element: the state actions that are out of the dataset distribution. For pessimistic\nlearning, the agent must avoid the policies that are more likely to visit such state-actions. These poli-\ncies are not well covered by the dataset, and the agent cannot evaluate their performance accurately.\nMeanwhile, these out-of-distribution state-actions contribute to the complexity of the learning prob-\nlem. Therefore, our insight is to constrain the action space to exclude all such out-of-distribution\nstate-actions. Based on the insight, the key step in our approach is to constrain the action space to\ninclude only the actions with a high probability of being sampled by a behavior clone policy that\nrepresents the dataset action distribution. In other words, our method only focuses on the policies\nthat are in the neighborhood of the behavior clone policy. As a result, our method not only inherits\nthe pessimistic idea of behavior regularization but also reduces the complexity of the corresponding\nreinforcement learning problem. In the experiments, we construct offline PBRL datasets based on\nstandard offline RL benchmark D4RL Fu et al. (2020). The details of dataset construction can be\nfound in Section 5. We empirically verify that on the offline datasets we construct, our approach can\nalways find policies of higher performance than the behavior policies of the dataset, suggesting that\nthe improvement in our PBRL training is reliable.\n2\nRELATED WORK\nOffline Reinforcement learning: There have been many studies on the standard offline reinforce-\nment learning setting with reward feedback (Levine et al., 2020; Cheng et al., 2022; Yu et al., 2021),\nin contrast to preference feedback considered in our setting. The key idea behind offline reinforce-\nment learning is pessimism which encourages the learning agent to focus on the policies that are\nsupported by the dataset (Cheng et al., 2022). However, the gap between learning from reward\nfeedback and preference feedback is not clear at present.\nOnline PBRL: Preference-based reinforcement learning is popular in the online learning scenario.\nVarious types of preference models and preference labels have been considered in the tabular case\n(F¨urnkranz et al., 2012; Wirth et al., 2017). Similar to the works that consider the general case, in our\n2\nwork we consider the preference models to be Bradley-Terry models and output binary preference\nlabels (Ibarz et al., 2018; Lee et al., 2021). The two-phase learning approach mentioned in Section\n1 is prevalent in the online PBRL setting. Note that directly applying online methods to the offline\nscenario is also likely to be inefficient even when it is possible (Van Hasselt et al., 2018), as the\nonline learning approaches do not need to be pessimistic.\nOffline PBRL: Offline preference-based reinforcement learning is a relatively new topic. Zhan et al.\n(2023) propose an optimization problem following the two-phase learning framework and theoret-\nically prove that the solution to the problem is a near-optimal policy with respect to the training\ndataset. However, how to solve the optimization problem in practice remains unknown. Concur-\nrently, Zhu et al. (2023) formulates a similar optimization problem under the linear assumption on\nthe environment and proposes a way to solve the problem, but the results cannot be extended to the\ngeneral case without the linear assumption. Kim et al. (2023) study methods for learning the utility\nmodels based on preference provided by humans. They propose to use a transformer-based archi-\ntecture to predict the utility function behind the preference model. Rafailov et al. (2023) propose a\nmethod that directly learns a policy without learning a utility model explicitly. Unlike the setting\nconsidered in our work, they focus on the NLP task and require trajectories in each pair to have the\nsame prompt (initial state). Hejna & Sadigh (2023) propose a method that learns a policy and the Q\nfunction of the environment instead of the utility function. They also require access to an additional\ntrajectory dataset that only contains trajectories to make their algorithm efficient. Our work focuses\non developing a practical and efficient two-phase learning approach for the offline PBRL setting that\nonly has access to a preference dataset with no requirement on the trajectories in the dataset.\nRLHF: Reinforcement learning from human feedback is a special instance of PBRL that its prefer-\nence labels are provided by humans. It has been a popular topic recently. Success in various robotic\ncontrol and NLP taasks have been achieved by fine-tuning pre-trained models through reinforcement\nlearning based on preference feedback from human (Ouyang et al., 2022; Bai et al., 2022). These\nstudies focus on solving specific tasks instead of the general problem considered in our work.\n3\nPRELIMINARY\n3.1\nOFFLINE PREFERENCE REINFORCEMENT LEARNING PROBLEM\nWe consider an offline preference-based reinforcement learning setting (Zhan et al., 2023). An\nenvironment is characterized by an incomplete Markov Decision Process (MDP) without a reward\nfunction M = (S, A, P) where S is the state space, A is the action space, and P is the state\ntransition function. A policy π : S →∆(A) is a mapping from a state to a probability distribution\nover the action space, representing a way to behave in the environment by taking the action at a\nstate. A deterministic policy is a special kind of policy that maps a state to a single action. We\nuse ΠS,A to denote the class of all policies, and ΠD\nS,A to denote the class of deterministic policies.\nA trajectory of length t is a sequence of states and actions (s1, a1, . . . , st, at, st+1) where ai ∈\nA, si+1 ∼P(·|si, ai)∀i ∈[t]. A preference model F takes a pair of trajectories τ1, τ2 as input, and\noutputs a preference σ ∈{≻, ≺}, indicating its preference over the two trajectories, i.e. τ1 ≻τ2\nor τ1 ≺τ2. In this work, following the conventions, we consider the preference models that are\nBradley-Terry (BT) models (Bradley & Terry, 1952). Specifically, there exists an utility function\nu : S × A →R, such that the probability distribution of the output preference of F given τ1, τ2\nsatisfies:\nPr{τ1 ≻τ2} =\nexp(P\n(s,a)∈τ1 u(s, a))\nexp(P\n(s,a)∈τ1 u(s, a)) + exp(P\n(s,a)∈τ2 u(s, a))\n(1)\nWe define the performance of a policy π on a utility function u as the expected cumulative\nutility of a trajectory generated by the policy: Eτ∼(π,P)[P\ns,a∈τ u(s, a)].\nAn offline dataset\nD = {(τ 1\n1 , τ 1\n2 , σ1), . . . , (τ N\n1 , τ N\n2 , σN)} consists of multiple preference data over different trajec-\ntory pairs. We use the term ‘behavior policy’ to represent how the trajectories are generated in the\ndataset. A learning agent is given access to the incomplete MDP M = (S, A, P) of the environment\nand a dataset D whose preferences are generated by a preference model F. The learning goal is to\n3\nlearn a policy that has high performance on the utility function u of the preference model F, and we\nsay a learning algorithm is efficient if it can learn such a high-performing policy.\n3.2\nTRADITIONAL PPO LEARNING WITH KL-REGULARIZATION\nHere, we introduce the prevalent two-step learning framework that has been widely applied for\nsolving PBRL problems Christiano et al. (2017) and is closely related to our algorithm in this work.\nIn general, the algorithm aims at solving the optimization problem below:\narg max\nπ∈ΠS,A\nEτ∼(π,P)\nX\n(s,a)∈τ\n[ˆu(s, a)] −α · KL(π, πb)\ns.t.ˆu = arg min\nu Lu(u, D)\nπb = arg min\nπ∈Πb Lπ(π, D)\n(2)\nHere, α is a parameter to control the degree of pessimism. ˆu is a learned reward model and πb is a\nbehavior imitation policy. Lu(u, D) is a loss function to evaluate the quality of the utility model on\ninterpreting the preference labels in the dataset. Lπ(u, D) is a loss function to evaluate how well the\nbehavior imitation policy reproduces the behavior demonstrated by the dataset.\nTo solve the optimization problem, the first step is to find the optimal reward model and behavior\nimitation policy that minimize their corresponding losses. The second step is to use a reinforce-\nment learning algorithm such as PPO to solve for the policy that maximizes the optimization goal.\nPessimism is achieved through the KL regularization in the optimization goal, which encourages\nthe policy not to be very different from the behavior imitation policy. Formally, Algorithm 1 below\nrepresents a typical way to solve the optimization problem in Eq 2.\nAlgorithm 1: Typical two-step training with KL-regularization\nInputs\n: Environment (S, A, P), Dataset D\n1. Train a utility function ˆu on D that minimize Lu(·, D) to approximate the preference model\nin D.\n2. Train a policy πb from a class of policy Πb that minimizes Lπ(π, D) to approximate the\nbehavior policy behind D.\n3. Find the policy π∗that optimizes\nπ∗= arg maxπ∈ΠS,A Eτ∼(π,P )[P\n(s,a)∈τ ˆu(s, a)] −α · KL(π, πb)\n4. Output π∗\n4\nPREFERENCE BASED REINFORCEMENT LEARNING ON A CONSTRAINED\nACTION SPACE (PRC)\n4.1\nGENERAL PRC ALGORITHM\nFirst, we introduce our main algorithm PRC. In Alg 2, we formally show the general form of PRC.\nHere, we abuse the notation of π(a|s). If the action space is discrete, then π(a|s) represents the\nprobability of the policy to generate the response a at the state s. If the action space is continuous,\nthe π(a|s) becomes the probability density. In the first step, the learning agent learns a behavior\npolicy that can best imitate the behavior demonstrated in the dataset and then learns a reward model\nthat best interprets the dataset’s preference label. This is the same as the first step in the typical two-\nstep learning framework Alg 1. In the second learning step, the agent learns the policy supported on\na clipped action space that maximizes the cumulative return on the learned reward model. At this\nstep, the learning agent only considers the high probability actions according to the behavior policy.\nFormally, let the incomplete MDP of the environment be (S, A, P). Let the behavior policy and\nreward model learned in the first step be π0 and R respectively. We define a constrained action\nspace A′ based A and π0. Let A′|s be the set of actions A′ at a state s. The constrained action\n4\nAlgorithm 2: Preference Based Reinforcement Learning on a Constrained Action Space\nInputs\n: Environment (S, A, P), Dataset D\n1. Train a utility function ˆu on D that minimize Lu(·, D) to approximate the preference model\nin D.\n2. Train a policy πb from a class of policy Πb that minimizes Lπ(π, D) to approximate the\nbehavior policy behind D.\n3. Construct a clipped action space A′ such that at a state s, the clipped action space is\nA′|s = {a : π0(a|s) ≥p}.\n4. Find the policy π∗supported on the clipped action space that optimizes\nπ∗= arg maxπ∈ΠS,A′ Eτ∼(π,P )[P\n(s,a)∈τ ˆu(s, a)]\n4. Output π∗\nspace A′ consists of all actions that have a probability higher than a threshold to be sampled from\nthe behavior policy at a state s: A′|s = {a : π0(a|s) ≥p}. Then, at the second learning step, the\nagent solves an RL problem where the MDP is (S, A′, P, R).\n4.2\nANALYSIS\nHere, we show that the reinforcement learning step in PRC is essentially optimizing the reward\nfunction under a special behavior regularization constraint. Recap the prevalent PBRL framework in\nAlg 1. The pessimism in the algorithm is achieved by using KL regularization, which encourages the\npolicy to be not very different from the behavior policy. Here, we consider a different regularization\nas below.\nCp(π, πb) =\n\u001a−∞\nif ∃(s, a) ∈(S, A), π(a|s) > 0, πb(a|s) < p\n0,\notherwise\nThe\noptimization\nproblem\nunder\nsuch\na\nregularization\nis\narg maxπ∈ΠS,A Eτ∼(π,P)\nP\n(s,a)∈τ[ˆu(s, a)] −α · Cp(π, πb).\nUnder such a regularization,\nthe optimal policy can only be supported in a constrained action space where the probability density\nof the behavior policy is greater than the threshold p, as otherwise, it will suffer from an infinite\npenalty from the regularization.\nNext, we discuss why choosing such a regularization is reasonable. First, unlike the soft constraint,\nsuch as KL regularization, our regularization is a hard constraint that forces the policy to stay close to\nthe behavior policy. It is conceptually more conservative in that it explicitly decreases the possibility\nof reward hacking by outputting some policy that is not close to the behavior policy. Second, it\nreduces the complexity of finding the optimal policy under the regularization. This can make the\nreinforcement learning step more accessible and reduce the optimization loss at this step.\n4.3\nPRACTICAL IMPLEMENTATION\nHere, we introduce a practical implementation of our PRC algorithm, which is later used in our ex-\nperiments. First, we can train a deterministic policy π0 that has a high probability of reproducing the\nbehavior demonstrated by the dataset. The action given by the deterministic policy can be thought\nof as the center of the distribution of the actual underlying behavior policy of the dataset. Since we\nhave no prior knowledge about the shape of the distribution of the behavior policy, we may only in-\nfer that an action is more likely to be sampled by the behavior policy if it is closer to the center of its\ndistribution. Then, we can approximate the space of A′ as a box whose center is at the deterministic\npolicy. The radius of the action space is a hyper-parameter related to the degree of pessimism.\nFormally, Alg 3 is a practical implementation for PRC algorithm. Lines 1-2 learn a utility model\nand a behavior clone policy, which a standard supervised learning approach can achieve. The loss\nfunctions are set in Eq 3 below. In section 5, we empirically verify that training on a constrained\naction space is an efficient pessimistic learning approach, and the reinforcement learning is also\nmuch easier on a constrained action space.\n5\nLu(u, D) = −\nX\n(τ1,τ2,σ)∈D\nlog\nexp(P\n(s,a)∈τ ∗u(s, a))\nexp(P\n(s,a)∈τ1 u(s, a)) + exp(P\n(s,a)∈τ2 u(s, a))\nLπ(π, D) =\nX\n(s,a)∼D\n∥π(s) −a∥2\n(3)\nLines 3-5 implement an RL environment with a constrained action space. For a policy πclip supported\non S, A′, if it outputs an action a′ ∈A′, then its corresponding actual policy outputs action f(s, a′),\nand the state transition follows P(s, f(s, a′)) = P′(s, a′), where P′(s, a′) is the state transition\nin Line 5. Therefore, we can search for the optimal clipped policy π∗from ΠS,A′ on the MDP\nwith state space S, action space A′, state transition P′, and output the corresponding actual policy\nof π∗. In practice, we can solve this by applying the SAC or PPO algorithm on the MDP M′ =\n(S, A′, P′, ˆu).\nAlgorithm 3: PRC practical implementation\nInputs\n: Environment (S, A, P), Dataset D\nParameters: Positive real number r\n1. Train a utility function ˆu that minimizes Lu(·, D) to approximate the preference model in D.\n2. Train a deterministic behavior clone policy π0\nb that minimizes Lπ(·, D).\n3. Construct a constrained action space A′ = RN where N is the dimensions of A. Each\ndimension is constrained on [−r, r].\n4. Construct a mapping f : S × A′ →A as f(s, a′) = ProjA(π0\nb(s) + a′)\n5. Construct a state transition P′ : S × A′ →∆(S) as P′(s, a′) = P(s, f(s, a′))\n6. Find the optimal policy π∗that optimizes arg maxπ∈ΠS,A′ Eτ∼(π,P′)[P\n(s,a)∈τ ˆu(s, a)]\n7. Output π : π(f(s, a)|s) = π∗(a|s)\n5\nEXPERIMENTS\n5.1\nSETUP\nDataset: Following previous studies Hejna & Sadigh (2023), we construct our offline preference\ndataset from D4RL benchmark Fu et al. (2020), and generate synthetic preference following the\nstandard techniques in previous PBRL studies (Kim et al., 2023; Christiano et al., 2017). Based\non the definition of a standard offline preference-based reinforcement learning setting in Section\n3, given a reward-based dataset from D4RL, we construct a preference-based dataset through the\nfollowing process:\n1. Randomly sample pairs of trajectory clips from the D4RL dataset. Following previous studies\n(Christiano et al., 2017), the length of the clip is set to be 20 steps.\n2. For each pair of trajectory clips, compute the probability of a trajectory to be preferred based\non the reward signals. To ensure consistency between different datasets, the reward signals are\nregularized to be bound in [−1, 1].\n3. For each pair of trajectory clips, randomly generate a preference label through a Bernoulli trial\nwith the probability computed above.\n4. Return the preference dataset consisting of the trajectory clip pairs and the corresponding prefer-\nence labels.\nWe consider various datasets from D4RL to represent the general learning scenarios. The robot con-\ntrol environments we choose include ‘Hopper’, ‘HalfCheetah’, and ‘Walker’. The types of trajecto-\nries we choose include ‘Medium’, ‘Medium-Replay’, and ‘Medium-Expert’. To make the number\nof state-action in the dataset aligned with that of the D4RL benchmark, the total number of trajectory\npairs with a preference label is 1 × 106.\nBaseline Algorithms: Here, we consider multiple two-step learning algorithms as baselines and a\nstate-of-the-art attack in the related offline PBRL setting, which are listed below:\n6\n1. Two-step learning with KL-regularization: We adopt the traditional two-step learning baseline\nwith KL-regularization as introduced in Section 3. To make it a stronger baseline and the com-\nparison fairer, we allow the algorithm to initialize from the behavior clone policy.\n2. Naive two-step learning: To show a naive baseline where pessimistic learning is entirely absent,\nwe adopt a naive two-step learning baseline, which is the same as the traditional two-step learning\nabove except that there is no regularization at all during the reinforcement learning step. Note\nthat we also allow the algorithm to initialize from the behavior clone policy in this baseline.\n3. Reward modeling followed by standard offline RL algorithms: Another simple yet efficient two-\nstep learning approach for offline PBRL problems is combining reward modeling with a standard\nreward-based offline RL algorithm. First, a reward model is learned from the preference dataset.\nThen, the reward model is used to provide scalar reward labels to the state-action in the dataset.\nFinally, we apply a standard offline RL algorithm IQL Kostrikov et al. (2021) training on the\ndataset with scalar reward labels.\n4. Oracle: The oracle is trained with true rewards instead of preference labels. Here, we apply\nIQL training on the base D4RL dataset with true reward signals. Note that the information from\nthe reward signals is strictly more than that from the preference signals in this case. The oracle\nshould be considered as an upper bound on the performance of an offline PBRL algorithm.\n5. Inverse preference learning (IPL) Hejna & Sadigh (2023): IPL is the state-of-the-art learning\nalgorithm for a related offline PBRL setting that requires a preference dataset and a behavior\ndataset. To avoid underestimating the performance of IPL, we apply this algorithm to our PBRL\nsetting and allow it to check the behaviors in the full D4RL dataset.\nTo ensure a fair comparison, all the methods that require reward modeling share the same learned\nreward model during training.\nTraining setup: To learn a reward model, we follow a standard supervised learning framework.\nSpecifically, we use a multilayer perceptron (MLP) structure for the neural network to approximate\nthe utility model. The neural network consists of 3 hidden layers, and each has 64 neurons. We use\na tanh function as the output activation so that the output is bound between [−1, 1].\nTo train a deterministic behavior clone policy, the neural network we use to represent the clone policy\nhas the same structure as that for the utility model. To train a stochastic behavior clone policy, the\nnetwork outputs the mean and standard deviation of a Gaussian distribution separately. The network\nis an MLP with 3 hidden layers each with 64 neurons. The last layer is split into two for the two\noutputs. For the mean output, we use a linear function for the last layer. For the standard deviation,\nwe use a linear function and an exp activation for the last layer.\nIn the reinforcement learning step, we use either the SAC algorithm or the PPO algorithm, depending\non the dataset. The neural network for the actor is the same as the network for training a stochastic\nbehavior clone policy. The neural network for the critic is the same as the network for training the\nreward model.\n5.2\nLEARNING EFFICIENCY EVALUATION\nHere, we compare the efficiency of PRC against other baseline algorithms on different datasets. To\nstraightforwardly compare the performance of different learning algorithms, we show the perfor-\nmance of the best policy learned by a method during training.\nThe scores in Table 1 are the standard D4RL score of the learned policies. The results show that\nthe PRC algorithm has high learning efficiency. It is generally more efficient than other baselines\nand sometimes even competes with the oracle. Our algorithm performs better than other two-step\nlearning baselines initialized from the behavior clone policy. This observation indicates that starting\nfrom the behavior clone policy or its neighborhood is not enough to achieve high learning efficiency,\nand training on the constrained action space is the key to high learning efficiency for PRC.\nNext, we empirically analyze why PRC should be an efficient learning algorithm from the aspect of\npessimistic learning and reduced reinforcement learning complexity.\n7\nDataset\nOracle\nPRC\nBehavior Clone\nNaive two-step\nKL two-step\nIPL\nRM\nHalfCheetah-Medium\n47.3 ± 0.2\n47 ± 0.5\n41.7 ± 1.0\n40.1 ± 0.5\n41.9 ± 0.1\n42.7 ± 0.1\n43.2 ± 0.1\nHalfCheetah-Medium-Replay\n46.1 ± 0.1\n43.3 ± 0.2\n32.9 ± 9.2\n31.9 ± 0.8\n32.9 ± 0.9\n34.9 ± 3.1\n40.1 ± 0.7\nHalfCheetah-Medium-Expert\n92.1 ± 0.7\n78.4 ±2.9\n44.5 ± 3.6\n40.9 ± 0.2\n40.4 ± 0.5\n41.7 ± 1.0\n48.2 ± 0.8\nHopper-Medium\n76.1 ± 1.2\n71 ± 7.5\n51.4 ± 3.9\n67.5 ± 2.4\n73.5 ± 4.0\n72.4 ± 7.4\n67.2 ± 0.3\nHopper-Medium-Replay\n76.7 ± 5.3\n47 ± 19.5\n40.8 ± 8.5\n49.4 ± 7.4\n53.1 ± 2.2\n39.5 ± 17.5\n32.2 ± 0.4\nHopper-Medium-Expert\n113.1 ± 0.4\n100.2 ± 9.5\n46.7 ± 10.9\n67.3 ± 7.1\n71.4 ± 10.9\n76.9 ± 8.1\n97.1 ± 5.2\nWalker2d-Medium\n84.5 ± 0.3\n84.4 ± 0.8\n73.5 ± 1.7\n81 ± 0.8\n79.5 ± 2.1\n80.8 ± 1.6\n81.9 ± 2.5\nWalker2d-Medium-Replay\n83.1 ± 2.3\n87.9 ± 6.1\n11.6 ± 0.6\n49.8 ± 7.3\n45.4 ± 0.1\n49.3 ± 3.5\n71.8 ± 6.4\nWalker2d-Medium-Expert\n111.5 ± 0.4\n110.4 ± 0.6\n95.9 ± 3.1\n93.4 ± 4.9\n92.1 ± 2.3\n107.5 ± 3.0\n105.7 ± 6.9\nSum Totals\n730.5\n669.6\n439\n521.3\n530.2\n545.7\n587.4\nTable 1: Comparison between the performance of different learning methods.\n5.3\nPESSIMISM EFFECTIVENESS\nHere, we empirically verify that by training on a constrained action space, the PRC algorithm\nachieves effective pessimistic learning.\nIn Figure 1, we show some representative examples. For the PRC algorithm, during training, the\nperformance trend of the learned policies measured on the learned reward model is aligned with\nthat measured on the true reward. This suggests that the pessimistic learning in the PRC algorithm\nis effective: it mainly considers the policies that are supported by the dataset so that the agent can\nevaluate their relative performance accurately. In contrast, for naive two-step learning and KL-\nregularized two-step learning, we find multiple cases where the trends can even be opposite when\nevaluated on the reward model and on the true reward. These results suggest pessimistic learning is\nnot efficient enough in the baseline methods compared to PRC.\nFigure 1: Comparison between the trend of the performance of the learned policies on the learned\n(simulated performance) and true reward models (true performance) during training. An algorithm\nis not pessimistic enough if the two trends are not aligned.\n5.4\nREINFORCEMENT LEARNING EFFICIENCY ON CONSTRAINED ACTION SPACE\nHere, we empirically verify that reinforcement learning is much easier on the constrained action\nspace. In this case, we focus on the performance of the learned policies evaluated on the learned\nreward model.\nWe observe that in most cases, the performance of the learned policies in the PRC method is much\nhigher than in the baseline two-step learning methods. In Figure 2, we show some representative\nexamples. It is clear that while the baseline method struggles with low-performing policies, the PRC\nmethod learns policies of much higher performance.\n8\nFigure 2: Comparison between the performance of the learned policies on the learned reward models\nduring training. The reinforcement learning complexity is less in a setting if the simulated perfor-\nmance is high.\n5.5\nREINFORCEMENT LEARNING COMPLEXITY IN PBRL\nHere, we empirically show that training on a reward model learned from preference signals is harder\nthan that from true reward signals. Given the same D4RL dataset, we train two reward models based\non the true reward signals and synthetic preference signals. Then, we train an efficient RL algorithm\non the two rewards and compare the learning efficiency in the two cases.\nThe results in Figure 3 show that when learning on the reward model trained from true rewards,\nthe RL agent can quickly learn some policies that have high performance on the reward model. In\ncomparison, it can take much longer for the agent to find a good policy on the reward model from\npreference signals, and sometimes, the agent may not be able to find a decent policy after a lot\nof training epochs. This indicates that it is harder to learn from a reward model that is trained on\npreference signals instead of true reward signals.\nFigure 3: For each dataset, a pair of reward models are trained on the true rewards and preference\nsignals. The same RL algorithm is applied to learn on both reward models. The learning difficulty\non a reward model is less if the PPO algorithm can learn better policies according to the reward\nmodel.\n6\nCONCLUSION AND LIMITATION\nIn this work, we propose a novel two-step learning algorithm PRC for the offline PBRL setting.\nWe empirically show that the PRC has high learning efficiency and provide evidence for why it is\na more efficient two-step learning algorithm than others. Our framework is limited to the typical\noffline learning setting and does not answer the question of which trajectories are more worthy of\nreceiving preference labels. Our experimental evaluation is limited to continuous control problems,\nthe standard benchmark in RL studies.\n9\n7\nREPRODUCIBILITY\nIn the main paper, we explain the setting of the problem we study. The codes we use for the experi-\nments can be found in the supplementary materials.\nREFERENCES\nSaurabh Arora and Prashant Doshi. A survey of inverse reinforcement learning: Challenges, meth-\nods and progress. Artificial Intelligence, 297:103500, 2021.\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn\nDrain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al.\nTraining a helpful and harmless\nassistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862,\n2022.\nRalph Allan Bradley and Milton E Terry. Rank analysis of incomplete block designs: I. the method\nof paired comparisons. Biometrika, 39(3/4):324–345, 1952.\nChing-An Cheng, Tengyang Xie, Nan Jiang, and Alekh Agarwal. Adversarially trained actor critic\nfor offline reinforcement learning. In International Conference on Machine Learning, pp. 3852–\n3878. PMLR, 2022.\nPaul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep\nreinforcement learning from human preferences. Advances in neural information processing sys-\ntems, 30, 2017.\nGabriel Dulac-Arnold, Nir Levine, Daniel J Mankowitz, Jerry Li, Cosmin Paduraru, Sven Gowal,\nand Todd Hester. Challenges of real-world reinforcement learning: definitions, benchmarks and\nanalysis. Machine Learning, 110(9):2419–2468, 2021.\nJustin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep\ndata-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020.\nJohannes F¨urnkranz, Eyke H¨ullermeier, Weiwei Cheng, and Sang-Hyeun Park. Preference-based\nreinforcement learning: a formal framework and a policy iteration algorithm. Machine learning,\n89:123–156, 2012.\nAmmar Haydari and Yasin Yılmaz. Deep reinforcement learning for intelligent transportation sys-\ntems: A survey. IEEE Transactions on Intelligent Transportation Systems, 23(1):11–32, 2020.\nJoey Hejna and Dorsa Sadigh. Inverse preference learning: Preference-based rl without a reward\nfunction. arXiv preprint arXiv:2305.15363, 2023.\nBorja Ibarz, Jan Leike, Tobias Pohlen, Geoffrey Irving, Shane Legg, and Dario Amodei. Reward\nlearning from human preferences and demonstrations in atari. Advances in neural information\nprocessing systems, 31, 2018.\nChangyeon Kim, Jongjin Park, Jinwoo Shin, Honglak Lee, Pieter Abbeel, and Kimin Lee. Pref-\nerence transformer: Modeling human preferences using transformers for rl.\narXiv preprint\narXiv:2303.00957, 2023.\nIlya Kostrikov, Ashvin Nair, and Sergey Levine. Offline reinforcement learning with implicit q-\nlearning. arXiv preprint arXiv:2110.06169, 2021.\nKimin Lee, Laura Smith, and Pieter Abbeel.\nPebble:\nFeedback-efficient interactive rein-\nforcement learning via relabeling experience and unsupervised pre-training.\narXiv preprint\narXiv:2106.05091, 2021.\nSergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tuto-\nrial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.\nTianqi Liu, Yao Zhao, Rishabh Joshi, Misha Khalman, Mohammad Saleh, Peter J Liu, and\nJialu Liu.\nStatistical rejection sampling improves preference optimization.\narXiv preprint\narXiv:2309.06657, 2023.\n10\nNguyen Cong Luong, Dinh Thai Hoang, Shimin Gong, Dusit Niyato, Ping Wang, Ying-Chang\nLiang, and Dong In Kim. Applications of deep reinforcement learning in communications and\nnetworking: A survey. IEEE Communications Surveys & Tutorials, 21(4):3133–3174, 2019.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow\ninstructions with human feedback.\nAdvances in Neural Information Processing Systems, 35:\n27730–27744, 2022.\nRafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and Chelsea\nFinn. Direct preference optimization: Your language model is secretly a reward model. arXiv\npreprint arXiv:2305.18290, 2023.\nRichard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.\nHado Van Hasselt, Yotam Doron, Florian Strub, Matteo Hessel, Nicolas Sonnerat, and Joseph Mo-\ndayil. Deep reinforcement learning and the deadly triad. arXiv preprint arXiv:1812.02648, 2018.\nChristian Wirth and Johannes F¨urnkranz. Preference-based reinforcement learning: A preliminary\nsurvey. In Proceedings of the ECML/PKDD-13 Workshop on Reinforcement Learning from Gen-\neralized Feedback: Beyond Numeric Rewards. Citeseer, 2013.\nChristian Wirth, Riad Akrour, Gerhard Neumann, Johannes F¨urnkranz, et al. A survey of preference-\nbased reinforcement learning methods. Journal of Machine Learning Research, 18(136):1–46,\n2017.\nTianhe Yu, Aviral Kumar, Rafael Rafailov, Aravind Rajeswaran, Sergey Levine, and Chelsea Finn.\nCombo: Conservative offline model-based policy optimization. Advances in neural information\nprocessing systems, 34:28954–28967, 2021.\nWenhao Zhan, Masatoshi Uehara, Nathan Kallus, Jason D Lee, and Wen Sun. Provable offline\nreinforcement learning with human feedback. arXiv preprint arXiv:2305.14816, 2023.\nBanghua Zhu, Jiantao Jiao, and Michael I Jordan. Principled reinforcement learning with human\nfeedback from pairwise or k-wise comparisons. arXiv preprint arXiv:2301.11270, 2023.\n11\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2023-12-30",
  "updated": "2024-10-25"
}