{
  "id": "http://arxiv.org/abs/2401.01053v3",
  "title": "Cheetah: Natural Language Generation for 517 African Languages",
  "authors": [
    "Ife Adebara",
    "AbdelRahim Elmadany",
    "Muhammad Abdul-Mageed"
  ],
  "abstract": "Low-resource African languages pose unique challenges for natural language\nprocessing (NLP) tasks, including natural language generation (NLG). In this\npaper, we develop Cheetah, a massively multilingual NLG language model for\nAfrican languages. Cheetah supports 517 African languages and language\nvarieties, allowing us to address the scarcity of NLG resources and provide a\nsolution to foster linguistic diversity. We demonstrate the effectiveness of\nCheetah through comprehensive evaluations across six generation downstream\ntasks. In five of the six tasks, Cheetah significantly outperforms other\nmodels, showcasing its remarkable performance for generating coherent and\ncontextually appropriate text in a wide range of African languages. We\nadditionally conduct a detailed human evaluation to delve deeper into the\nlinguistic capabilities of Cheetah. The introduction of Cheetah has\nfar-reaching benefits for linguistic diversity. By leveraging pretrained models\nand adapting them to specific languages, our approach facilitates the\ndevelopment of practical NLG applications for African communities. The findings\nof this study contribute to advancing NLP research in low-resource settings,\nenabling greater accessibility and inclusion for African languages in a rapidly\nexpanding digital landscape. We publicly release our models for research.",
  "text": "Cheetah\n: Natural Language Generation for 517 African Languages\nIfe Adebara1,⋆AbdelRahim Elmadany1,⋆Muhammad Abdul-Mageed1,2\n1Deep Learning & Natural Language Processing Group, The University of British Columbia\n2Department of Natural Language Processing & Department of Machine Learning, MBZUAI\n{ife.adebara@,a.elmadany@,muhammad.mageed@}ubc.ca\nAbstract\nLow-resource African languages pose unique\nchallenges for natural language processing\n(NLP) tasks,\nincluding natural language\ngeneration (NLG). In this paper, we develop\nCheetah,\na massively multilingual NLG\nlanguage\nmodel\nfor\nAfrican\nlanguages.\nCheetah supports 517 African languages and\nlanguage varieties, allowing us to address\nthe scarcity of NLG resources and provide\na solution to foster linguistic diversity. We\ndemonstrate\nthe\neffectiveness\nof\nChee-\ntah through comprehensive evaluations across\nsix generation downstream tasks. In five of the\nsix tasks, Cheetah significantly outperforms\nother models,\nshowcasing its remarkable\nperformance for generating coherent and\ncontextually appropriate text in a wide range\nof African languages. We additionally conduct\na detailed human evaluation to delve deeper\ninto the linguistic capabilities of Cheetah.\nThe introduction of Cheetah has far-reaching\nbenefits for linguistic diversity. By leveraging\npretrained models and adapting them to\nspecific languages, our approach facilitates the\ndevelopment of practical NLG applications\nfor African communities.\nThe findings of\nthis study contribute to advancing NLP\nresearch in low-resource settings, enabling\ngreater accessibility and inclusion for African\nlanguages in a rapidly expanding digital\nlandscape. We will publicly release our models\nfor research. 1\n1\nIntroduction\nThe linguistic diversity present in African lan-\nguages poses unique challenges for NLG sys-\ntems. With over 2, 000 languages spoken across\nthe African continent (Eberhard et al., 2021), the\nneed for effective NLG solutions that can accom-\nmodate this rich linguistic ecosystem cannot be\n1https://github.com/UBC-NLP/Cheetah\n⋆Authors contributed equally.\nFigure 1: Cheetah is trained on 517 African languages\nand language varieties across 14 language families. The\nlanguages are domiciled in 50 of 54 African countries\nand are written in six different scripts.\nover-emphasized. This is especially important be-\ncause traditional NLG approaches have primarily\nfocused on high-resource languages, such as Eng-\nlish and French due to the availability of large-scale\ndatasets and resources. Consequently, low-resource\nlanguages, including numerous African languages,\nhave been marginalized in NLG research and de-\nvelopment. Developing robust NLG systems for\nthe diverse needs of African communities is chal-\nlenging due to the scarcity of extensive language\ndatasets, limited linguistic research, and variations\nacross these languages.\nTo address these chal-\nlenges, recent advancements in language modeling\nand transfer learning techniques have shown prom-\nise in supporting NLG in low-resource languages.\nPretrained language models, such as GPT-3 (Rad-\nford et al., 2018, 2019; Brown et al., 2020), mT5\n(Xue et al., 2021), and mT0 (Muennighoff et al.,\n2022), have demonstrated remarkable capabilities\nin understanding and generating human-like text.\nThese models capture the statistical regularities\narXiv:2401.01053v3  [cs.CL]  10 Jan 2024\nand syntactic structures of the languages they are\ntrained on, making them valuable starting points\nfor supporting NLG in low-resource settings.\nIn this paper, we present a pioneering work on\nNLG in African languages by introducing Chee-\ntah: a novel language model (LM) specifically de-\nsigned to support 517 African languages and lan-\nguage varieties. To the best of our knowledge,\nCheetah supports the largest number of African\nlanguages and language varieties. Leveraging a\nvast corpus of text data collected from diverse\nsources, Cheetah learns some intricate linguistic in-\nformation that characterize each African language.\nThe contributions of this research are three fold.\nFirst, we address the scarcity of NLG resources\nfor African languages by providing a comprehens-\nive language model that covers a wide range of\nlanguages spoken on the continent. Second, we\ndemonstrate the efficacy of our approach through\nextensive evaluations across six downstream task\nclusters. Each cluster includes multiple languages,\nshowcasing the model’s ability to generate coher-\nent and contextually appropriate text in different\nAfrican languages. Third, we perform fine grained\nhuman analysis of Cheetah using a controlled ma-\nchine translation (MT) test set.\nThis uncovers\nmodel behaviour that is not visible with automatic\nmetrics. By supporting NLG in African languages,\nwe foster linguistic diversity, empower African\ncommunities to express themselves in their nat-\nive languages, and bridge the digital divide. This\npaper serves as a foundational step towards promot-\ning Afrocentric NLP (Adebara and Abdul-Mageed,\n2022) that prioritizes inclusivity and cultural preser-\nvation in language technology, emphasizing the im-\nportance of catering to the unique linguistic needs\nof diverse populations.\nThe rest of the paper is organized as follows: In\nSection 2, we discuss related work. In Section, 4\nwe describe AfroNLG, the benchmark we create\nfor evaluation. We provide details of Cheetah in\nSection 3. We present performance of Cheetah in\nSection 5 and compare it to other multilingual mod-\nels. We present controlled test sets in Section 5.1.\nWe conclude in Section 6, and outline a number of\nlimitations and use cases for our work in Section 7\nand Section 8.\n2\nLiterature Review\nOne of the challenges in NLG is to generate coher-\nent and semantically meaningful text. Various ap-\nproaches have been proposed, including template-\nbased (Becker, 2002; Van Deemter et al., 2005),\nrule-based (Dušek and Jurˇcíˇcek, 2015; van Milten-\nburg et al., 2020), and statistical approaches (Li\net al., 2016). More recently, deep learning ap-\nproaches (Sutskever et al., 2014) including the\ntransformer model (Vaswani et al., 2017) have\nachieved SoTA results in various NLG tasks such\nas text summarization (Shi et al., 2021) and ma-\nchine translation (Vaswani et al., 2017).\nWhile these models have shown impressive res-\nults, they often require a large amount of training\ndata and computing resources. However, only a few\nAfrican languages have benefited from these ad-\nvancements due to inadequate data. To address this\nissue, researchers have proposed transfer learning-\nbased approaches, where a pretrained model is\nfinetuned for a specific NLG task. Transfer learn-\ning (Raffel et al., 2020; He et al., 2022; Ruder\net al., 2019) has enabled the use of low-resource\nlanguages on various NLP tasks. Due to lack of ad-\nequate (or good quality) pretraining data (Kreutzer\net al., 2021), transfer learning is often the most\naccessible method for only a few low-resource lan-\nguages leaving behind a vast majority of extremely\nlow-resource languages. This is because about 90%\nof the world’s languages is claimed to be either left-\nbehinds, in that it is probably impossible to build\nNLP resources for them, or scraping-bys with no\nlabelled datasets (Joshi et al., 2020). For the left-\nbehinds, labelled and unlabelled data are unavail-\nable and even transfer learning approaches are bey-\nond reach while the scraping-by languages have\nno labelled data with which to evaluate model per-\nformance.\n2.1\nLanguage Models\nOnly a few African languages have benefited from\nthe recent advancement of language models (LM)\ndue to inadequate data sizes. We now describe\nencoder-decoder LMs that support NLP tasks in\nAfrican languages. We describe these under two\nbroad headings: massively multilingual models\nand African models. We summarize the models\nand African languages they cover in Table 1.\nMultilingual Models: The massively multilin-\ngual models such as mBART (Liu et al., 2020),\nMT0 (Muennighoff et al., 2022), and mT5 (Xue\net al., 2021) are trained on several languages. How-\never, in most cases, only a few African languages\nare represented. Among the mentioned models,\nmT0 is pretrained on the highest number of African\nlanguages (n=13).\nAfrican Models. Adelani et al. (2022) use pre-\ntrained T5, mT5, and mBART models and develop\nAfriByT5, AfriMT5, AfriMBART respectively to\ninvestigate machine translation in zero-shot and\nout-of-domain settings. The authors experiment\non 17 African languages and demonstrate that fur-\nther pretraining is effective for adding new lan-\nguages to pretrained models. Jude Ogundepo et al.\n(2022) train AfriTeVa, an encoder-decoder lan-\nguage model from scratch on 10 African languages\nand English using similar training objectives like\nT5 model.\nAfrican Natural Language Understanding. Sev-\neral works attempt to improve the perform-\nance on African NLU tasks by proposing mul-\ntilingual and African-dedicated models such as\nmBERT (Devlin et al., 2019), XLM-R (Conneau\net al., 2020), AfriBERTa (Ogueji et al., 2021),\nAfroLM (Dossou et al., 2022), Afro-XLM-R (Alabi\net al., 2022), KINYaBERT (Nzeyimana and Niy-\nongabo Rubungo, 2022), and SERENGETI (Ade-\nbara et al., 2023).\n2.2\nBenchmarks\nMultiple benchmarks have been developed for\nNLG. However, only a few of Africa’s 2, 000 lan-\nguages have been supported to date. In most cases,\nthe benchmarks support only the machine transla-\ntion task. We provide a brief overview under two\nheadings: African and multilingual. We summarize\nkey information about each benchmark in Table 2.\nAfrican Benchmarks. AfroMT (Reid et al., 2021a)\nis a multilingual machine translation benchmark.\nIt consists of translation tasks between English\nand eight African languages — Afrikaans, Xhosa,\nZulu, Rundi, Sesotho, Swahili, Bemba, and Lin-\ngala. Menyo-20k (Adelani et al., 2021) is an MT\nevaluation benchmark for English-Yorùbá.\nMultilingual\nwith\nAfrican\nLanguages.\nFLoRES-200 (Costa-jussà et al., 2022; Guzmán\net al., 2019) is an evaluation benchmark that\nprovides MT evaluation support in 200 languages\nincluding 52 African languages.\nGEM (Gehr-\nmann et al., 2021, 2022) referenced as “living\"\nbenchmark, comprises of 40 tasks and supports\n52 languages including 10 African languages.\nNLLB\nSeed\nData (Costa-jussà et al., 2022)\nis a set of professionally-translated sentences\nsampled from Wikipedia. It consists of around\nsix thousand sentences in 39 languages which\ninclude 8 African language.\nSimilarly, NLLB\nMulti\nDomain (Costa-jussà et al., 2022) is an\nMT evaluation benchmark made from a set of\nprofessionally-translated sentences in the news\nand health domains. It consists of approximately\n3, 000 sentences in each domain and supports\n8 languages including 2 African languages.\nToxicity-200 (Costa-jussà et al., 2022) is an\nevaluation benchmark to evaluate the presence of\ntoxic items in the MT text. It provides support for\n50 African languages. XGLUE (Liang et al., 2020) is\na cross-lingual, multi-task benchmark created with\nmultilingual and bilingual corpora. It supports 19\nlanguages and one African language, i.e., Swahili.\n3\nCheetah\n3.1\nPretraining Data\nWe are guided by three main principles in devel-\noping this data: quality, linguistic diversity, and\ncoverage.\nQuality. Developing NLP technologies for low re-\nsource languages poses a significant challenge due\nto the limited availability of high-quality training\ndata. To address this issue, we undertook the task\nof manually curating a diverse corpus spanning\nmultiple domains, including news articles, health\ndocuments, religious texts, legal documents, and\nsocial media feeds. This manual curation approach\nwas necessary because there were no existing data-\nsets available for the majority of the languages we\naimed to support, and we wanted to ensure the\nutilization of reliable and high-quality data.\nCoverage. In all, we train Cheetah using a 42G\nmulti-domain corpus across 517 African languages\nand language varieties. The languages are spoken\nin 50 of 54 African countries and they are written\nwith five scripts. This provides support to at least\n500M Africans.\nLinguistic Diversity. The inclusion of languages\nfrom various domains, geographical regions, and\nlinguistic typologies, along with the utilization of\nreliable data sources, contributes to enhancing the\nrobustness and quality of Cheetah. Our data con-\nsists of languages from 14 language families in\nAfrica written in five different orthographies. Fur-\nthermore, our data spans languages with a vast\narray of exotic linguistic features including tone,\nvowel and consonant harmony, reduplication, word\norders, and word classes.\nWe provide further details on the data used for\nCategory\nLM\nLang/Total\nAfrican Languages\nFamilies\nMultilingual\nMBART\n3/50\nafr, swh, yor.\n2\nMT0\n14/101\nafr, amh, hau, ibo, lin, mlg, nyj, orm, sot,\n4\nsna, som, swh, xho, yor, and zul\nMT5\n12/101\nafr, amh, nya, hau, ibo, mlg, sna, som, swh, xho, yor, and zul\n3\nAfrican\nAfriVeTa\n10/10\ngaz, amh, Gahuza, hau, ibo, pcm, som, swa, tir, and yor.\n3\nAfriMT5\n17/17\nbam, bbj, ewe, fon, hau, ibo, lug, luo, pcm, mos, swa, tsn, twi, wol, yor, zul.\n3\nAfriByT5\n17/17\nbam, bbj, ewe, fon, hau, ibo, lug, luo, pcm, mos, swa, tsn, twi, wol, yor, zul.\n3\nAfriMBART\n17/17\nafr, amh, nya, hau, orm, som, swh, xho.\n3\nCheetah\n517/517\nIncludes 517 African languages.\n14\nTable 1: Comparing with available encoder-decoder models with African languages represented. Lang/Total.\ndescribe the number of African languages comparing with the covered languages in the pretrained language models.\nFamilies. describes the number of covered language families.\nCategory\nBenchmark\nReference\nTask\nLang/Total\nDatasets\nTasks\nMultilingual\nFLoRES200\n(Costa-jussà et al., 2022)\n52/200\nMT\nWiki\n1\nGEMv1\n(Gehrmann et al., 2021)\nDRG, DT, RES, TS, SMP\n10/52\n18\n13\nGEMv2\n(Gehrmann et al., 2021)\nDRG, DT, PPH, QA,\nRES, TS, SLG, SMP, TS\n10/52\n50\n9\nIndicNLG\n(Kumar et al., 2022)\nBG, HG, SUM, PARA, QA\n0/11\n5\n5\nIndoNLG\n(Cahyawijaya et al., 2021)\nSUM, QA, Chit-Chat\n0/3\n5\n3\nNLLB M.D.\n(Costa-jussà et al., 2022)\nMT\n2/8\nWiki\n1\nNLLB S.D.\n(Costa-jussà et al., 2022)\nMT\n2/8\nWiki\n1\nToxicity200\n(Costa-jussà et al., 2022)\nMT\n50/200\nWiki\n1\nXGLUE\n(Liang et al., 2020)\nNER, POS, MLQA, PAWS-X,\nXLNI, NC, QADSM, WPR,\nQAM, QG, NTG\n1/19\n19\n11\nAfrican\nAfroMT\n(Reid et al., 2021a)\nMT\n8/8\n5\n1\nMenyo-20k\n(Adelani et al., 2021)\nMT\n1/2\n6\n1\nAfroNLG\nOur Work\nCloze, CS, MT, QA, TG, SUM, PARA\n517/527\n67\n7\nTable 2: A Comparison of AfroNLG with other multilingual Benchmarks. MT: Machine translation, QA: Ques-\ntion Answering, CS: Code-Switching, TG: Title Generation, SUM: Summarization, PARA: Paraphrase, NER:\nNamed Entity Recognition, POS: Part-Of-Speech Tagging, MLQA: Multilingual Question Answering, PAWS-X:\nParallel Aggregated Word Scrambling for Cross-Lingual Understanding, XNLI: Cross-Lingual Natural Language\nInterference, NC: News Classification, QADSM: Query-AD Matching, WPR: Web Page Ranking, QAM: QA\nMatching, NTG: News Title Generation, BG: WikiBio Biography Generation, and HG: Headline Generation. SD:\nSeed Data, MD: Multi Domain. DRG: Dialogue Response Generator, DT: Data-to-Text, RES: Reasoning, TS:\nText Summarization, SMP: Text Simplification, PPH: Paraphrase, SLG: Slide Generation\npretraining in Section A in the Appendix.\n3.2\nImplementation Details\nVocabulary. We use SentencePiece (Kudo and\nRichardson, 2018) to encode text as WordPiece\ntokens (Sennrich et al., 2016) with 250K Word-\nPieces.\nWe also include data covering the ten\ntop spoken languages globally: Arabic, English,\nFrench, German, Greek, Italian, Portuguese, Rus-\nsian, Spanish, and Turkish. We use Wikipedia\ndumps for these ten languages. We use 1M sen-\ntences for each language. However, we only in-\nclude it in the vocabulary.\nModels Architecture. We pretrain Cheetah using\nthe encoder-decoder architecture (Xue et al., 2021).\nEach of the encoder and decoder components is\nsimilar in size and configuration to T5, with 12\nlayers each with 12 attention heads, and 768 hidden\nunits for the base model. In total, this results in a\nmodel with ∼580 million parameters. We provide\nfurther details in Table 3.\nObjective. We use an unsupervised (denoising)\nobjective. The main idea is to feed the model with\nmasked (corrupted) versions of the original sen-\ntence, and train it to reconstruct the original se-\nquence. The denoising objective (Xue et al., 2021)\nworks by randomly sampling and dropping out 15%\nof tokens in the input sequence. All consecutive\nspans of dropped-out tokens are then replaced by a\nsingle sentinel token.\nPretraining Procedure For pretraining Cheetah,\nwe use a learning rate of 0.01, a batch size of 1, 024\nsequences, and a maximum sequence length of\n1, 024. We pretrain each model for 1M steps. We\nModel\nSize\nParams\nNo._heads\nNo._layers\nD_model\nVocab\nS._Len\nB. Size\n#Train_Steps\n#Langs\n#A.Langs\nmT0\nbase\n580M\n12\n12\n768\n∼250k\n1024\n1024\nUNK\n101\n13\nmT5\nbase\n580M\n12\n12\n768\n250K\n1024\n1024\n1M\n101\n13\nAfriMT5\nbase\n580M\nUNK\nUNK\nUNK\nUNK\nUNK\n2048\nUNK\n17\n17\nAfriTeVa\nbase\n229M\n12\n12\n768\n40K\n512\n256\n500K\n10\n10\nCheetah\nbase\n580M\n12\n12\n768\n250K\n1024\n1024\n1M\n527\n517\nTable 3: Parameters of Cheetah compared with other models.\ntrain our models on Google Cloud TPU with 128\ncores (v3 −128) from TensorFlow Research Cloud\n(TFRC).2\n4\nAfroNLG Benchmark\nWe create AfroNLG, a multi-lingual, multi-task\nbenchmark comprising 67 test sets across six task\nclusters. Specifically, AfroNLG includes the fol-\nlowing: cloze tasks, machine translation, para-\nphrase, question answering, summarization, and\ntitle generation. AfroNLG supports 527 languages,\nincluding 517 African languages and language vari-\neties and the top 10 world languages. To the best\nof our knowledge, this is the most extensive bench-\nmark till date for African languages. Table 2 shows,\nat a glance, how our benchmark compares to others\nbenchmark. We provide the details of each task\ncluster and datasets in what follows. For detailed\nstatistics about the task clusters, we refer to Ap-\npendix B.\nCloze Test. In order to comprehensively evaluate\nCheetah across all the languages it was pretrained\non, we employ cloze-tasks as our evaluation ap-\nproach and perform two cloze tasks experiments.\nThese tasks assess the model’s ability to fill in miss-\ning information. In the first cloze task, which we\nhenceforth call mask-one, we randomly mask only\none token in each sentence. In the second cloze-\ntask, which we call mask-at-least-one, we ran-\ndomly mask at least one token and not more than\n10% of the tokens in each sentence. For each of the\n517 languages, we construct a cloze-task dataset\ncomprising 200 data points for each language in\nthe Train set, 100 examples for each language in\nthe Test set, and 50 data points for each language\nin the Dev set. We ensure that there is no overlap\nbetween the data used for the cloze tasks and the\npretraining data. We show an example of our cloze\ntask in Figure 2.\nMachine Translation. We include only datasets\npertaining African languages in our benchmark. In\nselecting the languages for our MT benchmark, we\n2https://sites.research.google/trc/about/\nFigure 2: Examples from the mask-one and mask-at-\nleast-one cloze task data.\nstrive to keep datasets that have been used in any\npublished machine translation task. This allows us\nto cover a diverse set of languages and compare our\nmodels to existing SoTA across a large number of\nlanguage pairs. Our benchmark thus contains data\nfrom Afro-MT3 (Reid et al., 2021b), Lafand-MT4\n(Adelani et al., 2022), PidginUNMT5 (Ogueji and\nAhia, 2019), and SALT6 (Akera et al., 2022). The\ndatasets we consider make up 35 language pairs.\nParaphrase. A paraphrase task aims to create se-\nmantically similar and fluent paraphrases given an\ninput text (Chen et al., 2023; Palivela, 2021). We\nuse the TaPaCo dataset (Scherrer, 2020) for our\nparaphrase generation benchmark. TaPaCo is a\nfreely available paraphrase corpus for 73 languages\nextracted from the Tatoeba database. The dataset\nhas four African languages: Afrikaans, Berber (a\nmacro-language), Amazigh, and Kirundi.\nQuestion Answering.\nThe QA task aims to\nprovide answers to questions based on a knowledge\nbase also referred to as contexts. We use TYDIA7\nQA dataset (Clark et al., 2020). The dataset has a\nprimary task and a gold passage task. In our bench-\nmark, we only include the gold passage task, where\na correct answer is predicted from a passage con-\ntaining one answer, similar to the existing reading\ncomprehension task.\nSummarization. Summarization is the task of\ngenerating an abridged version of a text, while\n3https://github.com/machelreid/afromt\n4https://github.com/masakhane-io/lafand-mt\n5https://github.com/keleog/PidginUNMT\n6https://github.com/SunbirdAI/salt\n7https://github.com/google-research-datasets/tydiqa\ncapturing the salient ideas and the intended in-\nformation from the original text (Nallapati et al.,\n2016; King et al., 2022). We use the subset of XL-\nSum (Hasan et al., 2021), an abstractive summar-\nization dataset, that consists of African languages\nincluding Amharic, Hausa, Igbo, Kirundi, Oromo,\nPidgin, Somali, Swahili, Tigrinya, and Yorùbá. We\nalso develop new test sets using data we crawled\nfrom the web, which are non-overlapping with XL-\nSum. Specifically, we crawl data from BBC and\nVoice of Africa (webpages) for Hausa, Ndebele,\nand Swahili.\nTitle Generation. The title generation task returns\na single sentence title for a given article. Similar to\nthe summarization task, we use XL-SUM to create\na news title generation dataset. We also collect a\nnew test set for title generation across 15 languages.\nThe dataset comprises ∼6, 000 BBC and Voice of\nAfrica articles, non-overlapping with XL-Sum, and\nis particularly useful for zero-shot title generation.\n5\nEvaluation and Results\nWe evaluate Cheetah on six task clusters of\nAfroNLG benchmark and compare to performance\non mT0, mT5, Afri-MT5, and AfriTeVa. We report\nresults in Table 4. For all models, we finetune on\nthe training data split (Train) for 20 epochs with an\nearly stopping of 5 epochs, learning-rate of 5e −5,\nbatch size of 16, and sequence length of 512. All\nexperiments were performed on 4 GPUs (Nvidia\nV100). We report the results of each experiment\nas an average of three runs, each with a different\nseed.8 For multilingual datasets in each task cluster,\nwe show evaluation results per language. Chee-\ntah outperforms other models on many languages\nacross the six task clusters. We provide detailed\ninformation of model performance next.\nCloze Test. Cheetah outperforms all other models\non both cloze tasks as in Table 4. We show the\nresults for each language that is supported by the\nmodels compared in Table D.1 and Table D.2. The\nperformance of all models on mask-one is better\nthan the performance on mask-at-least-one, reflect-\ning how increasing the number of masked tokens\nmakes the task more challenging. It is also import-\nant to mention that since evaluation is based on\nBLEU it does not reflect correct synonyms that\neach model may have generated to replace the\nmasked tokens.\nMachine Translation. Cheetah sets a new SOTA\n8Specifically, we use seed values 41, 1512, and 20235.\non 23 tasks surpassing previous models. The mT0\nand AfriTEVA models also demonstrate strong per-\nformance on six languages. Notably, pairs with\nFrench as the source language tend to yield the\nlowest BLEU scores, indicating relatively lower\ntranslation quality. On the other hand, the language\npair involving English to Nigerian Pidgin, specific-\nally on LafandMT and PidginUNMT, showcases\nthe highest BLEU scores. We assume that the simil-\narity between the Nigerian Pidgin and English con-\ntributes favourably to translation quality in these\nscenarios. We also report CHRF and CHRF++ res-\nults in Table B.3 and Table B.4 in the Appendix.\nParaphrase. In the three paraphrase tasks, Chee-\ntah demonstrates remarkable superiority over all\nother models. Specifically, we achieve an impress-\nive ROUGE score of 46.0 on the Berber paraphrase\ntask, surpassing the second-best model by a margin\nof approximately two points.\nQuestion Answering. In the task of question an-\nswering, mT0 exhibits superior performance com-\npared to both Cheetah and other models. While\nmT5 achieves the second-highest performance,\nCheetah attains the third-highest performance in\nthis task.\nSummarization. Cheetah sets a new SOTA on\n11 languages, outperforming other models by an\naverage margin of at least three points. Detailed\nresults can be found in Table 4.\nTitle Generation. On the Title generation task,\nCheetah sets a new SOTA on 11 languages. We\nreport results in Table 4.\n5.1\nInvestigating linguistic capabilities\nIn order to further test the utility of our models,\nwe use grammar templates to construct test data\nin English. We use nine linguistic rules and 19\nlexical items to generate 152 sentences. Next, we\nuse our model to translate from source to target and\nmanually evaluate the quality of the generated data.\nWe design new evaluation metrics, faithfulness and\nfluency, for the manual evaluation. A detailed de-\nscription follows.\nGrammar templates. We use grammar templates\n(McCoy et al., 2019) developed with context-free\ngrammars (CFG) on the source side to construct\ncontrolled test sets in English. We use CFG on\nthe source side alone because constituents and con-\nstituent order differs across languages. We adopt\nthis method for two reasons. First, utilizing gram-\nmar templates provides a standardized framework\nCluster\nTask\nMetric\nmT0\nmT5\nAfri-MT5\nAfriTeVa\nCheetah\nMachine Translation (MT)\nEnglish →Afrikaans\nBleu\n20.38±0.3\n12.35±1.1\n7.12±2.67\n7.75±1.67\n19.72±0.75\nEnglish →Bemba\nBleu\n19.19±0.3\n12.28±0.48\n11.73±12.3\n20.5±0.87\n18.9±1.22\nEnglish →Lingala\nBleu\n15.98±1.16\n14.12±0.56\n14.32±12.74\n13.88±1.04\n9.64±1.11\nEnglish →Rundi\nBleu\n12.26±0.47\n8.82±0.43\n9.57±0.42\n7.83±1.04\n10.54±0.54\nEnglish →Sesotho\nBleu\n11.04±1.2\n12.74±0.75\n10.0±1.79\n10.76±1.4\n13.3±1.38\nEnglish →Swahili\nBleu\n10.59±1.84\n9.33±0.58\n3.08±0.57\n7.24±0.46\n11.08±0.61\nEnglish →Xhosa\nBleu\n10.04±0.98\n8.25±0.7\n3.86±1.35\n7.5±0.32\n12.34±0.51\nEnglish →Zulu\nBleu\n17.65±1.86\n17.97±1.69\n1.9±1.11\n13.45±1.81\n19.49±1.16\nEnglish →Hausa\nBleu\n5.06±0.21\n4.96±0.16\n0.85±0.04\n7.32±0.00\n9.22±0.08\nEnglish →Igbo\nBleu\n13.05±0.17\n11.57±0.23\n1.12±0.09\n12.34±0.23\n16.75±0.26\nEnglish →Luganda\nBleu\n2.17±2.77\n3.33±0.35\n0.09±0.01\n4.21±0.77\n9.75±0.01\nEnglish →N. Pidgin\nBleu\n33.17±0.28\n32.65±0.19\n2.39±0.23\n9.39±0.18\n32.64±0.14\nEnglish →Swahili\nBleu\n22.04±2.89\n23.2±0.23\n2.79±0.08\n22.39±0.28\n28.11±0.14\nEnglish →Zulu\nBleu\n6.83±0.29\n0.58±1.37\n0.4±0.03\n4.45±0.37\n11.75±0.38\nEnglish →Twi\nBleu\n3.4±0.12\n1.23±0.03\n0.03±0.0\n1.68±0.94\n4.64±0.13\nEnglish →Yoruba\nBleu\n5.42±0.85\n2.58±3.1\n0.04±0.0\n3.63±4.01\n7.83±0.14\nEnglish →Zulu\nBleu\n10.28±0.49\n1.31±2.26\n0.14±0.03\n3.8±4.2\n12.13±0.1\nFrench →Bambara\nBleu\n2.0±2.6\n0.37±0.19\n0.15±0.01\n3.18±0.18\n3.06±0.27\nFrench →Ghomálá’\nBleu\n0.4±0.09\n0.33±0.01\n0.07±0.0\n0.96±0.01\n0.28±0.25\nFrench →Ewe\nBleu\n0.7±0.35\n0.31±0.36\n0.09±0.07\n0.84±0.16\n3.47±0.03\nFrench →Fon\nBleu\n0.69±0.31\n0.8±0.13\n1.52±0.06\n1.73±0.53\n1.29±0.16\nFrench →Moore\nBleu\n0.27±0.06\n0.12±0.05\n0.19±0.02\n0.47±0.04\n1.66±0.86\nFrench →Wolof\nBleu\n4.02±0.12\n0.3±0.05\n0.11±0.01\n3.08±0.25\n3.01±0.07\nEnglish →N. Pidgin (UNMT)\nBleu\n27.44±0.26\n23.42±1.61\n7.05±1.37\n22.54±0.84\n26.56±0.04\nAcholi →English\nBleu\n16.41±0.08\n11.16±4.77\n4.9±0.11\n8.37±8.12\n19.33±0.1\nAcholi →Lugbara\nBleu\n2.57±0.21\n1.48±1.31\n2.44±0.37\n8.29±0.14\n7.21±0.69\nAcholi →Luganda\nBleu\n3.64±0.07\n1.74±0.12\n0.92±0.01\n5.53±0.34\n8.03±0.38\nAcholi →Nyankore\nBleu\n2.17±0.14\n0.79±0.51\n0.46±0.03\n4.26±0.54\n5.1±0.14\nAcholi →Ateso\nBleu\n1.64±2.34\n1.94±0.25\n4.9±0.11\n7.74±0.33\n6.33±0.6\nEnglish →Lugbara\nBleu\n6.19±6.33\n8.38±0.49\n5.93±0.22\n10.95±0.32\n11.61±0.28\nEnglish →Luganda\nBleu\n12.08±0.03\n10.58±0.25\n2.59±0.73\n12.41±0.35\n17.12±0.16\nEnglish →Nyankore\nBleu\n6.46±0.08\n5.69±0.02\n1.4±0.39\n7.88±0.18\n9.04±0.24\nEnglish →Ateso (salt)\nBleu\n10.24±0.06\n8.28±0.19\n4.91±0.59\n11.64±0.49\n11.12±0.38\nLugbara →Ateso\nBleu\n2.21±0.35\n1.5±0.2\n2.22±0.15\n6.67±0.32\n3.68±0.31\nLuganda →Lugbara\nBleu\n3.96±0.57\n2.61±0.12\n3.44±0.32\n8.05±0.23\n7.99±0.47\nLuganda →Ateso\nBleu\n4.47±0.08\n3.01±0.16\n2.5±0.22\n8.17±0.18\n8.13±0.33\nNyankore →Lugbara\nBleu\n3.45±0.29\n2.1±0.32\n2.6±0.29\n7.5±0.09\n7.29±0.09\nNyankore →Luganda\nBleu\n8.54±0.17\n6.91±0.23\n2.01±0.25\n6.77±6.73\n6.25±10.26\nNyankore →Ateso\nBleu\n3.33±0.11\n2.25±0.23\n2.12±0.4\n6.27±0.12\n6.36±0.4\nParaphrase\nMultilingual\nBleu\n41.79±0.28\n41.75±0.21\n34.72±0.51\n43.02±1.25\n43.23±0.09\nBerber\nBleu\n44.84±0.31\n44.03±0.24\n36.08±0.83\n**46.41±0.71\n46.0±0.27\nKabyle\nBleu\n25.91±0.13\n25.32±0.46\n11.56±0.73\n16.06±14.79\n26.27±0.56\nQuestion Answering\nQA Swahili\nF1\n79.84±0.19\n72.04±0.54\n0\n62.64±0.78\n71.98±1.18\nSummarization\nMultilingual\nRougeL\n22.31±0.12\n22.23±0.04\n5.34±0.48\n18.97±0.06\n24.86±0.02\nAmharic\nRougeL\n13.81±0.04\n13.09±0.03\n4.4±1.07\n8.29±0.51\n15.09±0.1\nIgbo\nRougeL\n18.9±0.73\n13.22±0.46\n14.24±0.39\n16.05±0.49\n17.36±0.43\nOromo\nRougeL\n11.28±0.03\n10.51±0.07\n3.52±0.49\n7±1.73\n14.53±0.1\nRundi\nRougeL\n19.63±0.01\n18.02±0.13\n11.82±0.39\n16.13±0.03\n22.57±0.04\nSwahili\nRougeL\n26.38±0.02\n24.81±0.11\n15.07±0.17\n21.59±0.13\n29.05±0.13\nYoruba\nRougeL\n21.57±0.05\n20.06±0.12\n13.52±0.18\n17.3±0.11\n22.49±0.0\nHausa\nRougeL\n26.46±0.06\n25.76±0.02\n19.96±0.26\n25.19±0.11\n30.07±0.31\nNigerian Pidgin\nRougeL\n26.54±0.05\n25.79±0.1\n14.28±1.23\n20.29±0.12\n27.08±0.02\nSomali\nRougeL\n20.69±0.08\n19.21±0.06\n13.62±0.81\n19.27±0.18\n23.92±0.04\nTigrinya\nRougeL\n15.84±0.13\n13.93±0.11\n6.53±0.42\n10.07±0.09\n16.88±0.12\nTitle Generation\nMultilingual\nBleu\n6.53±0.02\n6.65±0.08\n0.1±0.02\n5.2±0.02\n7.52±0.07\nAmharic\nBleu\n3.13±0.23\n2.65±0.68\n0.34±0.14\n2.31±0.14\n4.34±0.34\nIgbo\nBleu\n6.95±0.13\n6.9±0.22\n0.77±0.12\n4.61±0.14\n8.47±0.07\nOromo\nBleu\n1.1±1.84\n2.66±0.19\n0.21±0.06\n1.54±0.17\n3.26±0.21\nRundi\nBleu\n4.4±0.28\n4.13±0.22\n0.84±0.07\n3.33±0.23\n6.05±0.5\nSwahili\nBleu\n9.1±0.23\n9.31±0.11\n1.22±0.09\n7.01±0.09\n10.59±0.6\nYoruba\nBleu\n6.8±0.16\n7.23±0.59\n0.34±0.05\n5.04±2.0\n7.97±0.32\nHausa\nBleu\n8.11±0.24\n7.3±0.34\n2.59±0.01\n6.69±0.18\n8.48±0.23\nNigerian Pidgin\nBleu\n6.75±0.6\n3.96±4.3\n0.89±0.02\n4.72±0.84\n6.22±0.28\nSomali\nBleu\n3.37±0.21\n3.31±0.16\n0.38±0.11\n2.82±0.47\n5.25±0.14\nTigrinya\nBleu\n2.99±0.1\n2.94±1.09\n0.7±0.18\n1.92±0.26\n5.1±0.05\nCloze-task\nMask-one\nBleu\n13.61±0.91\n8.18±3.94\n0.00±0.00\n8.36±3.42\n13.98±0.32\nMask-at-least-one\nBleu\n2.36±0.11\n2.66±0.09\n0.93±0.12\n0.68±0.09\n7.07±0.09\nAfroNLG Score\n12.56\n11.05\n5.15\n10.84\n14.25\nTable 4: Average performance of finetuned African and multilingual models across three runs on AfroLNG\nbenchmark test sets.\nCategory\nExample\nIntransitive\nHe left\nIntransitive + Negation\nWe did not leave\nTransitive\nYou left Lagos\nTransitive + Negation\nShe did not leave them\nTable 5: Some examples of sentences generated with\nthe templates\nthat ensures that the same grammatical phenomena\nare tested consistently. By employing a uniform\napproach, we can effectively isolate and evaluate\nspecific linguistic features, facilitating a more rigor-\nous and meaningful comparison of language model\nperformance. Second, grammar templates exhibit\na high degree of flexibility, allowing for easy modi-\nfication and extension to encompass a wide range\nof linguistic phenomena. This adaptability not only\nfacilitates the incorporation of new linguistic fea-\ntures but also enables the evolution of our test sets\nto match the dynamic landscape of natural language\nprocessing research.\nOther alternatives to templates include using\nparsed corpora (Bender et al., 2011) or naturally\noccurring sentences. For the languages we explore,\nthere are no good quality parsers, making automatic\nparsing inaccessible for this analysis. Furthermore,\nwhen a corpus is parsed automatically, the likeli-\nhood of encountering parsing errors escalates with\nthe intricacy of the sentence structure (Bender et al.,\n2011; Marvin and Linzen, 2018). Conversely, if\nthe test set exclusively comprises sentences with\naccurate gold parses, sourcing an ample quant-\nity of instances showcasing syntactic complexit-\nies becomes an arduous task (Marvin and Linzen,\n2018). Furthermore, the utilization of naturally\noccurring sentences introduces potential complic-\nations that might confound the interpretation of\nexperiments (Ettinger et al., 2018).\nThe templates include transitive and intransit-\nive structures, negative and affirmative structures,\nand structures with gender and number. Table 5\nprovides examples of generated sentences using\nthe templates9.\nInference.\nWe test three of our finetuned ma-\nchine translation models with the generated data-\nset.\nThis allows us to evaluate how much lin-\nguistic information the models have acquired dur-\ning pretraining and finetuning. Specifically, we\nuse the English→Hausa, English→Swahili, and\nEnglish→Yorùbá based on MT0, MT5, AfriTEVA,\nand Cheetah models that were finetuned on the La-\n9The entire generated grammar is available at our GitHub:\nanonymous link.\nLang.\nFamily\n# Tone\nGender\nMorphology\nHausa\nAfro-Asiatic\nTwo\nTwo\nIsolating\nSwahili\nN.C. Bantu\nNone\nFive\nAgglutinative\nYourba\nN.C. Non-Bantu\nThree\nNone\nIsolating\nTable 6: Some linguistic differences between Hausa,\nSwahili, and Yoruba. N.C. refers to Niger-Congo\nfandMT dataset. We do not include Afri-MT5 in\nthis analysis because it has very low scores across\nseveral tasks as shown in Table 4. Notably, Hausa,\nSwahili, and Yorùbá have distinct typologies and\nthe performance of each model on each language\ngives further insight of performance across vary-\ning typological features (See Section C for details).\nTable 6 shows some linguistic differences between\nthe three languages. This method can be general-\nized to any African language.\n5.2\nHuman evaluation\nTo evaluate the effectiveness of each model across\ndifferent languages, we assess the generated out-\nput’s faithfulness and fluency using a five-point\nLikert scale. Faithfulness measures how accurately\na model’s output corresponds to the input sentence,\nwhile fluency assesses the grammatical coherence\nand plausibility of the generated output. We use\nboth metrics because a model can produce coherent\noutput that may not be faithful to the input sentence.\nThis way, if faithfulness penalizes a model for out-\nputs that are not true to the input or that include\nadditional unnecessary information, fluency com-\nplements our evaluation of the quality of the same\nmodel if the output is fluent. For each grammar cat-\negory, we return the average Likert point for each\nlanguage and across the different models model.\n5.3\nAnnotation\nWe annotated each model’s output for faithfulness\nand fluency. For Hausa and Yorùbá, two expert\nannotators evaluated the model’s output for faith-\nfulness and fluency. We ensured that each annotator\nhas native speaker competency in reading and writ-\ning (while some had a linguistic background). We\ngave specific annotation instructions (See Section\nE in the Appendix) to ensure the values are not\nassigned arbitrarily. We also ensured that the an-\nnotators do not know who created which models to\nprevent any biases. We report the Kappa scores for\ninter-annotator agreement in Table 7. For Swahili,\nonly one annotator made it to the final annotation\ntask since we could not acquire high quality annota-\ntions from other annotators. The Swahili annotator\nFigure 3: Faithfulness and fluency for Hausa, Swahili, and Yorùbá\nhau\nyor\nModel\nFaith.\nFlu.\nFaith.\nFlu.\nmT0\n90.54\n97.62\n96.57\n93.92\nmT5\n93.51\n96.48\n82.23\n81.10\nAfriTeVa\n87.27\n96.94\n88.56\n84.73\nCheetah\n96.61\n97.26\n87.11\n92.64\nTable 7: Kappa scores for Faithfulness (i.e., Faith.) and\nFluency (i.e., Flu.) across the four models and three\nlanguages we evaluate.\nwho did the final annotation is a university lecturer\nwho teaches Swahili and has a Ph.D. in linguistics.\n5.4\nFluency and Faithfulness Performance\nWe report the distribution of faithfulness and flu-\nency scores across all models and languages in\nFigure 3. Overall, Cheetah produces more faith-\nful and more fluent outputs than other models on\nall languages. We now go on to provide detailed\nanalysis of model performance.\nIntransitives In the case of Hausa examples, all\nthree models manage to produce intransitive ex-\namples. However, Cheetah consistently appends\nobjects to these intransitive examples. This inclina-\ntion to add objects might stem from biases within\nthe data used for pretraining or finetuning Cheetah.\nNevertheless, it is worth noting that Cheetah outper-\nforms other models by generating more fluent and\nmore faithful Hausa outputs. In the Swahili context,\nall models successfully generate intransitive trans-\nlations, with model errors primarily related to tense.\nThis performance discrepancy in Swahili can be\nattributed to its agglutinative structure, with models\npotentially lacking exposure to a comprehensive\nrange of grammatical features during pretraining\nor finetuning. In the context of Yorùbá, all mod-\nels consistently incorporate at least one object in\neach intransitive case. Notably, mT0 generates an\noutput without an object approximately 5.88% of\nthe time. This may be because intransitive sen-\ntences inherently lack a clear direct object, making\nit more challenging for machine translation models\nto grasp context and select the accurate transla-\ntion. In certain instances, some intransitive phrases\ncan be polysemous, further complicating the trans-\nlation process. Intransitive English verbs do not\nalways retain their intransitive nature in Yorùbá.\nFurthermore, transitives with optional/truncated ob-\njects tend to have a compulsory object in Yorùbá.\nThis phenomenon potentially contributes to the\nmodels’ tendency to append objects to intransitive\nYorùbá phrases. For instance, whereas the intrans-\nitive \"slept\" in \"John slept\" maps to the intrans-\nitive form \"John sùn\" in Yorùbá, the intransitive\nverb \"prayed\", in \"John prayed\" becomes \"John\ngbàdúrà\", a transitive verb in Yorùbá. On the other\nhand, the transitive verb \"ate\" in \"John ate\", has an\noptional/truncated object in English but becomes\n\"John je\n˙\nun\", a transitive with an obligatory object.\nIn Yorùbá, both \"ate\" and \"prayed\" are transitive\nverbs that require an object. They are derived from\n\"je\n˙\n\" (eat) and \"oúnje\n˙\n\" (food), which give rise to\n\"je\n˙\nun\" and \"gbà\" (collect) and \"àdúrà\" (prayer),\nresulting in \"gbàdúrà\" respectively. We report the\ndistribution of scores in Figure F.1.\nTransitives In the context of transitives, Chee-\ntah stands out as the top-performing model across\nall three languages, as illustrated in Figure 3. Chee-\ntah demonstrates the capability to provide three\ndistinct semantic senses for the polysemous transit-\nive verb treated whereas the other models typically\nproduce only a single semantic interpretation. In\nSwahili examples, certain instances exhibit the de-\nletion or simplification of object markers in an un-\ngrammatical manner. For a visual representation of\nthe annotation of intransitive sentences in Yorùbá,\nplease refer to Figure F.3. Figure F.2 shows the\ndistribution of model performance on transitives.\nNegative In the context of Yorùbá, all models are\nable to produce the correct negation marker includ-\ning the correct tone marks. The tone patterns on\nnegation markers may vary based on the context of\nwords before and after the negation marker and it\nwas interesting to see these variations in the models\noutputs. Despite this, mT0, MT5, and AfriTeVa\nhave a tendency to output the negation of the ant-\nonym of the verb in each sentence rather than the\nnegation of the verb. Cheetah also makes similar\nmistakes about 5% of the time.\nAffirmative The models generally perform better\nin the context of the affirmative examples than on\nthe negated examples. However, in the context\nof Hausa, mT5, mT0, and AfriTeVa consistently\noutput the antonym of the verb to be negated. For\ninstance, the models return “Sara left\" rather than\n“Sara did not leave\". In the Swahili examples, we\nalso find cases of double negation (which is not\ngrammatically correct in Swahili). We show the\ndistribution of results in Figure F.5 and Figure F.4.\nGender/Agreement We find interesting cases\nof gender in the model’s output. For example,\nwhereas Yorùbá grammar does not distinguish\ngender, Cheetah uses Arábìrin (female) before\nevery occurrence of the name “Sara\" to indicate\nthat the it has a high probability of being femin-\nine (see Figure F.3). It is important to mention\nthat “Fred\" is not annotated this way. For Hausa,\nwhich requires agreement between the gender of\nthe noun and the verb, we find Cheetah outper-\nforming both mt0 and mt5 significantly. AfriTeVa,\nhowever, has very low accuracy in the context of\ngender. Furthermore, mt0, mt5, and Cheetah return\nconnotations for love and relationships for each ex-\namples where a male and female pronoun co-occur\ncross-lingually.\nNumber Cheetah significantly outperforms all\nthree models in accurately assigning appropriate\nnumber markers. We also find that when translating\nthe word \"you\" into Hausa, Swahili, or Yorùbá, all\nfour models use either singular or plural forms. We\nassume that this is due to the fact that the second\nperson in English (i.e., “you\") can be either singu-\nlar or plural while each of these languages have a\ndifferent word for the singular and plural forms.\n6\nConclusion\nIn this work, we introduced Cheetah, a massively\nmultilingual language model designed for African\nnatural language generation.\nWe also propose\na new African language generation benchmark,\ndubbed AfroNLG. Our evaluation benchmark is\nboth sizeable and diverse. We evaluate Cheetah on\nAfroNLG comparing it to three other models, two\nmultilingual and one dedicated to African lan-\nguages. The performance of Cheetah surpasses\nthat of all other models we evaluate. This is demon-\nstrated by its superior AfroNLG score, which is\napproximately three times better than the combined\nperformance of other models. Furthermore, Chee-\ntah outperforms all other models across 48 out of\n65 test sets spanning six task clusters. We further\nanalyze our model’s robustness to lexical complex-\nity and carry out human evaluation to inspect the\nmodel’s perform on a controlled test set. Again,\nour results underscore superiority of our model.\n7\nLimitations\nWe identify the following limitations for our work:\n1. The limitations of our language model include\nthe limited scope of our evaluation. Future\nwork should focus on increasing the subset\nof languages evaluated manually in order to\nensure quality. We believe automatic analyses\nare not sufficient for development of models\nthat get deployed in particular applications.\n2. Another limitation is related to our inability\nto perform extensive analysis of biases and\nhateful speech present in our pretraining data.\nAgain, this is due to relatively restricted ac-\ncess to native speakers (and even automated\ntools) to perform this analysis. As a result, we\ncannot fully ensure that our models are free\nfrom biases and socially undesirable effects.\nTherefore, it is important that these models be\nused with care and caution, and be analyzed\nfor biases and socially undesirable effects be-\nfore use.\n3. Additionally, due to unavailability of suffi-\ncient computing resources, we were unable\nto evaluate larger multilingual language mod-\nels.\n8\nEthics Statement and Wider Impacts\nCheetah aligns with Afrocentric NLP where the\nneeds of African people is put into consideration\nwhen developing technology. We believe Chee-\ntah will not only be useful to speakers of the lan-\nguages supported, but also researchers of African\nlanguages such as anthropologists and linguists.\nWe discuss below some use cases for Cheetah and\noffer a number of broad impacts.\n1. Cheetah aims to address the lack of access\nto technology in about 90% of the world’s\nlanguages, which automatically discriminates\nagainst native speakers of those languages.\nMore precisely, it does so by focusing on\nAfrica. To the best of our knowledge, Chee-\ntah is the first massively multilingual PLM de-\nveloped for African languages and language\nvarieties. A model with knowledge of 517\nAfrican languages, is by far the largest to date\nfor African NLP.\n2. Cheetah enables improved access of import-\nant information to the African community in\nIndigenous African languages. This is espe-\ncially beneficial for people who may not be\nfluent in other languages. This will potentially\nconnect more people globally.\n3. Cheetah affords opportunities for language\npreservation for many African languages. To\nthe best of our knowledge, Cheetah consists\nof languages that have not been used for any\nNLP task until now. We believe that it can\nhelp encourage continued use of these lan-\nguages in several domains, as well as trigger\nfuture development of language technologies\nfor many of these languages.\n4. Although LMs are useful for a wide range of\napplications, they can also be misused. Chee-\ntah is developed using publicly available data-\nsets that may carry biases. Although we strive\nto perform analyses and diagnostic case stud-\nies to probe performance of our models, our\ninvestigations are by no means comprehens-\nive nor guarantee absence of bias in the data.\nIn particular, we do not have access to native\nspeakers of most of the languages covered.\nThis hinders our ability to investigate samples\nfrom each (or at least the majority) of the lan-\nguages.\nReferences\nJulien Abadji, Pedro Javier Ortiz Suárez, Laurent Ro-\nmary, and Benoît Sagot. 2021. Ungoliant: An optim-\nized pipeline for the generation of a very large-scale\nmultilingual web corpus. Proceedings of the Work-\nshop on Challenges in the Management of Large\nCorpora (CMLC-9) 2021. Limerick, 12 July 2021\n(Online-Event), pages 1 – 9, Mannheim. Leibniz-\nInstitut für Deutsche Sprache.\nIfe Adebara and Muhammad Abdul-Mageed. 2022. To-\nwards afrocentric NLP for African languages: Where\nwe are and where we can go. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n3814–3841, Dublin, Ireland. Association for Compu-\ntational Linguistics.\nIfe Adebara, AbdelRahim Elmadany, Muhammad\nAbdul-Mageed, and Alcides Alcoba Inciarte. 2023.\nSerengeti: Massively multilingual language models\nfor africa.\nDavid Adelani, Jesujoba Alabi, Angela Fan, Julia\nKreutzer, Xiaoyu Shen, Machel Reid, Dana Ruiter,\nDietrich Klakow, Peter Nabende, Ernie Chang, Tajud-\ndeen Gwadabe, Freshia Sackey, Bonaventure F. P.\nDossou, Chris Emezue, Colin Leong, Michael Beuk-\nman, Shamsuddeen Muhammad, Guyo Jarso, Oreen\nYousuf, Andre Niyongabo Rubungo, Gilles Hacheme,\nEric Peter Wairagala, Muhammad Umair Nasir,\nBenjamin Ajibade, Tunde Ajayi, Yvonne Gitau,\nJade Abbott, Mohamed Ahmed, Millicent Ochieng,\nAnuoluwapo Aremu, Perez Ogayo, Jonathan Mukiibi,\nFatoumata Ouoba Kabore, Godson Kalipe, Derguene\nMbaye, Allahsera Auguste Tapo, Victoire Memd-\njokam Koagne, Edwin Munkoh-Buabeng, Valen-\ncia Wagner, Idris Abdulmumin, Ayodele Awokoya,\nHappy Buzaaba, Blessing Sibanda, Andiswa Bukula,\nand Sam Manthalu. 2022. A few thousand transla-\ntions go a long way! leveraging pre-trained models\nfor African news translation. In Proceedings of the\n2022 Conference of the North American Chapter\nof the Association for Computational Linguistics:\nHuman Language Technologies, pages 3053–3070,\nSeattle, United States. Association for Computational\nLinguistics.\nDavid Adelani, Dana Ruiter, Jesujoba Alabi, Damilola\nAdebonojo, Adesina Ayeni, Mofe Adeyemi, Ayo-\ndele Esther Awokoya, and Cristina España-Bonet.\n2021. The effect of domain and diacritics in Yoruba–\nEnglish neural machine translation.\nIn Proceed-\nings of Machine Translation Summit XVIII: Research\nTrack, pages 61–75, Virtual. Association for Machine\nTranslation in the Americas.\nBenjamin Akera, Jonathan Mukiibi, Lydia Sanyu Nag-\ngayi, Claire Babirye, Isaac Owomugisha, Solomon\nNsumba, Joyce Nakatumba-Nabende, Engineer Bain-\nomugisha, Ernest Mwebaze, and John Quinn. 2022.\nMachine translation for african languages: Com-\nmunity creation of datasets and models in uganda.\nJesujoba O. Alabi, David Ifeoluwa Adelani, Marius\nMosbach, and Dietrich Klakow. 2022. Adapting pre-\ntrained language models to African languages via\nmultilingual adaptive fine-tuning. In Proceedings of\nthe 29th International Conference on Computational\nLinguistics, pages 4336–4349, Gyeongju, Republic\nof Korea. International Committee on Computational\nLinguistics.\nTilman Becker. 2002. Practical, template–based natural\nlanguage generation with TAG. In Proceedings of\nthe Sixth International Workshop on Tree Adjoining\nGrammar and Related Frameworks (TAG+6), pages\n80–83, Universitá di Venezia. Association for Com-\nputational Linguistics.\nEmily M. Bender, Dan Flickinger, Stephan Oepen, and\nYi Zhang. 2011. Parser evaluation over local and\nnon-local deep dependencies in a large corpus. In\nProceedings of the 2011 Conference on Empirical\nMethods in Natural Language Processing, pages 397–\n408, Edinburgh, Scotland, UK. Association for Com-\nputational Linguistics.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clem-\nens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In\nProceedings of the 34th International Conference on\nNeural Information Processing Systems, NIPS’20,\nRed Hook, NY, USA. Curran Associates Inc.\nSamuel Cahyawijaya, Genta Indra Winata, Bryan\nWilie, Karissa Vincentio, Xiaohong Li, Adhiguna\nKuncoro, Sebastian Ruder, Zhi Yuan Lim, Syafri Ba-\nhar, Masayu Khodra, Ayu Purwarianti, and Pascale\nFung. 2021. IndoNLG: Benchmark and resources for\nevaluating Indonesian natural language generation.\nIn Proceedings of the 2021 Conference on Empir-\nical Methods in Natural Language Processing, pages\n8875–8898, Online and Punta Cana, Dominican Re-\npublic. Association for Computational Linguistics.\nJiaao Chen, Derek Tam, Colin Raffel, Mohit Bansal,\nand Diyi Yang. 2023. An Empirical Survey of Data\nAugmentation for Limited Data Learning in NLP.\nTransactions of the Association for Computational\nLinguistics, 11:191–211.\nJonathan H. Clark, Eunsol Choi, Michael Collins, Dan\nGarrette, Tom Kwiatkowski, Vitaly Nikolaev, and\nJennimaria Palomaki. 2020. TyDi QA: A benchmark\nfor information-seeking question answering in typo-\nlogically diverse languages. Transactions of the As-\nsociation for Computational Linguistics, 8:454–470.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary,\nGuillaume Wenzek,\nFran-\ncisco Guzmán, Edouard Grave, Myle Ott, Luke\nZettlemoyer, and Veselin Stoyanov. 2020. Unsuper-\nvised cross-lingual representation learning at scale.\nIn Proceedings of the 58th Annual Meeting of the As-\nsociation for Computational Linguistics, pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nMarta R Costa-jussà, James Cross, Onur Çelebi, Maha\nElbayad, Kenneth Heafield, Kevin Heffernan, Elahe\nKalbassi, Janice Lam, Daniel Licht, Jean Maillard,\net al. 2022.\nNo language left behind: Scaling\nhuman-centered machine translation. arXiv preprint\narXiv:2207.04672.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nBonaventure F. P. Dossou, Atnafu Lambebo Tonja,\nOreen Yousuf, Salomey Osei, Abigail Oppong, Iy-\nanuoluwa Shode, Oluwabusayo Olufunke Awoyomi,\nand Chris Chinenye Emezue. 2022.\nAfrolm: A\nself-active learning-based multilingual pretrained lan-\nguage model for 23 african languages.\nMatthew S. Dryer and Martin Haspelmath, editors. 2013.\nWALS Online. Max Planck Institute for Evolutionary\nAnthropology, Leipzig.\nOndˇrej Dušek and Filip Jurˇcíˇcek. 2015.\nTraining a\nnatural language generator from unaligned data. In\nProceedings of the 53rd Annual Meeting of the As-\nsociation for Computational Linguistics and the 7th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 451–461,\nBeijing, China. Association for Computational Lin-\nguistics.\nDavid M Eberhard, F Simons Gary, and Charles D Fen-\nnig (eds). 2021. Ethnologue: Languages of the world.\nTwenty-fourth edition, Dallas, Texas: SIL Interna-\ntional.\nAllyson Ettinger, Ahmed Elgohary, Colin Phillips, and\nPhilip Resnik. 2018. Assessing composition in sen-\ntence vector representations. In Proceedings of the\n27th International Conference on Computational Lin-\nguistics, pages 1790–1801, Santa Fe, New Mexico,\nUSA. Association for Computational Linguistics.\nSebastian Gehrmann, Tosin Adewumi, Karmanya\nAggarwal,\nPawan\nSasanka\nAmmanaman-\nchi,\nAnuoluwapo\nAremu,\nAntoine\nBosselut,\nKhyathi Raghavi Chandu, Miruna-Adriana Clinciu,\nDipanjan Das, Kaustubh Dhole, Wanyu Du, Esin Dur-\nmus, Ondˇrej Dušek, Chris Chinenye Emezue, Varun\nGangal, Cristina Garbacea, Tatsunori Hashimoto,\nYufang Hou,\nYacine Jernite,\nHarsh Jhamtani,\nYangfeng Ji, Shailza Jolly, Mihir Kale, Dhruv\nKumar, Faisal Ladhak, Aman Madaan, Mounica\nMaddela, Khyati Mahajan, Saad Mahamood, Bod-\nhisattwa Prasad Majumder, Pedro Henrique Martins,\nAngelina McMillan-Major, Simon Mille, Emiel van\nMiltenburg, Moin Nadeem, Shashi Narayan, Vitaly\nNikolaev, Andre Niyongabo Rubungo, Salomey\nOsei,\nAnkur\nParikh,\nLaura\nPerez-Beltrachini,\nNiranjan Ramesh Rao, Vikas Raunak, Juan Diego\nRodriguez,\nSashank\nSanthanam,\nJoão\nSedoc,\nThibault Sellam, Samira Shaikh, Anastasia Shimor-\nina, Marco Antonio Sobrevilla Cabezudo, Hendrik\nStrobelt, Nishant Subramani, Wei Xu, Diyi Yang,\nAkhila Yerukola, and Jiawei Zhou. 2021.\nThe\nGEM benchmark:\nNatural language generation,\nits evaluation and metrics. In Proceedings of the\n1st Workshop on Natural Language Generation,\nEvaluation, and Metrics (GEM 2021), pages 96–120,\nOnline. Association for Computational Linguistics.\nSebastian Gehrmann, Abhik Bhattacharjee, Abinaya\nMahendiran, Alex Wang, Alexandros Papangelis,\nAman Madaan, Angelina Mcmillan-major, Anna\nShvets, Ashish Upadhyay, Bernd Bohnet, Bingsheng\nYao, Bryan Wilie, Chandra Bhagavatula, Chaobin\nYou, Craig Thomson, Cristina Garbacea, Dakuo\nWang, Daniel Deutsch, Deyi Xiong, Di Jin, Dimitra\nGkatzia, Dragomir Radev, Elizabeth Clark, Esin\nDurmus, Faisal Ladhak, Filip Ginter, Genta Indra\nWinata, Hendrik Strobelt, Hiroaki Hayashi, Jekater-\nina Novikova, Jenna Kanerva, Jenny Chim, Jiawei\nZhou, Jordan Clive, Joshua Maynez, João Sedoc,\nJuraj Juraska, Kaustubh Dhole, Khyathi Raghavi\nChandu, Laura Perez Beltrachini, Leonardo F . R.\nRibeiro, Lewis Tunstall, Li Zhang, Mahim Pushkarna,\nMathias Creutz, Michael White, Mihir Sanjay Kale,\nMoussa Kamal Eddine, Nico Daheim, Nishant Sub-\nramani, Ondrej Dusek, Paul Pu Liang, Pawan Sas-\nanka Ammanamanchi, Qi Zhu, Ratish Puduppully,\nReno Kriz, Rifat Shahriyar, Ronald Cardenas, Saad\nMahamood, Salomey Osei, Samuel Cahyawijaya,\nSanja Štajner, Sebastien Montella, Shailza Jolly, Si-\nmon Mille, Tahmid Hasan, Tianhao Shen, Tosin Ad-\newumi, Vikas Raunak, Vipul Raheja, Vitaly Nikolaev,\nVivian Tsai, Yacine Jernite, Ying Xu, Yisi Sang,\nYixin Liu, and Yufang Hou. 2022. GEMv2: Mul-\ntilingual NLG benchmarking in a single line of code.\nIn Proceedings of the 2022 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 266–281, Abu Dhabi, UAE.\nAssociation for Computational Linguistics.\nFrancisco Guzmán, Peng-Jen Chen, Myle Ott, Juan\nPino, Guillaume Lample, Philipp Koehn, Vishrav\nChaudhary, and Marc’Aurelio Ranzato. 2019. The\nFLORES evaluation datasets for low-resource ma-\nchine translation:\nNepali–English and Sinhala–\nEnglish. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Nat-\nural Language Processing (EMNLP-IJCNLP), pages\n6098–6111, Hong Kong, China. Association for Com-\nputational Linguistics.\nTahmid Hasan, Abhik Bhattacharjee, Md. Saiful Is-\nlam, Kazi Mubasshir, Yuan-Fang Li, Yong-Bin Kang,\nM. Sohel Rahman, and Rifat Shahriyar. 2021. XL-\nsum: Large-scale multilingual abstractive summariza-\ntion for 44 languages. In Findings of the Association\nfor Computational Linguistics: ACL-IJCNLP 2021,\npages 4693–4703, Online. Association for Computa-\ntional Linguistics.\nJunxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-\nKirkpatrick, and Graham Neubig. 2022. Towards a\nunified view of parameter-efficient transfer learning.\nIn International Conference on Learning Representa-\ntions.\nPhilip J. Jaggar. 2017. The Hausa “Grade 5” verb:\nMorphosyntactic preliminaries, 1 edition, pages 18–\n27. Harrassowitz Verlag.\nPratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika\nBali, and Monojit Choudhury. 2020. The state and\nfate of linguistic diversity and inclusion in the NLP\nworld. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n6282–6293, Online. Association for Computational\nLinguistics.\nOdunayo\nJude\nOgundepo,\nAkintunde\nOladipo,\nMofetoluwa Adeyemi, Kelechi Ogueji, and Jimmy\nLin. 2022.\nAfriTeVA: Extending ?small data?\npretraining approaches to sequence-to-sequence\nmodels. In Proceedings of the Third Workshop on\nDeep Learning for Low-Resource Natural Language\nProcessing, pages 126–135, Hybrid. Association for\nComputational Linguistics.\nDaniel King,\nZejiang Shen,\nNishant Subramani,\nDaniel S. Weld, Iz Beltagy, and Doug Downey. 2022.\nDon’t say what you don’t know: Improving the con-\nsistency of abstractive summarization by constraining\nbeam search. In Proceedings of the 2nd Workshop on\nNatural Language Generation, Evaluation, and Met-\nrics (GEM), pages 555–571, Abu Dhabi, United Arab\nEmirates (Hybrid). Association for Computational\nLinguistics.\nJulia Kreutzer, Isaac Caswell, Lisa Wang, Ahsan Wa-\nhab, Daan van Esch, Nasanbayar Ulzii-Orshikh, Al-\nlahsera Tapo, Nishant Subramani, Artem Sokolov,\nClaytone Sikasote, Monang Setyawan, Supheakmun-\ngkol Sarin, Sokhar Samb, Benoît Sagot, Clara Rivera,\nAnnette Rios, Isabel Papadimitriou, Salomey Osei,\nPedro Ortiz Suárez, Iroro Orife, Kelechi Ogueji, An-\ndre Niyongabo Rubungo, Toan Q. Nguyen, Math-\nias Müller, André Müller, Shamsuddeen Hassan\nMuhammad, Nanda Muhammad, Ayanda Mnyakeni,\nJamshidbek Mirzakhalov, Tapiwanashe Matangira,\nColin Leong, Nze Lawson, Sneha Kudugunta, Yacine\nJernite, Mathias Jenny, Orhan Firat, Bonaventure\nF. P. Dossou, Sakhile Dlamini, Nisansa de Silva, Sak-\nine Çabuk Ballı, Stella Biderman, Alessia Battisti,\nAhmed Baruwa, Ankur Bapna, Pallavi Baljekar, Is-\nrael Abebe Azime, Ayodele Awokoya, Duygu Ata-\nman, Orevaoghene Ahia, Oghenefego Ahia, Sweta\nAgrawal, and Mofetoluwa Adeyemi. 2021. Quality\nat a glance: An audit of web-crawled multilingual\ndatasets. arXiv preprint arXiv:2103.12028.\nTaku Kudo and John Richardson. 2018. SentencePiece:\nA simple and language independent subword token-\nizer and detokenizer for neural text processing. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 66–71, Brussels, Belgium.\nAssociation for Computational Linguistics.\nAman Kumar, Himani Shrotriya, Prachi Sahu, Amogh\nMishra, Raj Dabre, Ratish Puduppully, Anoop Kun-\nchukuttan, Mitesh M. Khapra, and Pratyush Kumar.\n2022. IndicNLG benchmark: Multilingual datasets\nfor diverse NLG tasks in Indic languages. In Proceed-\nings of the 2022 Conference on Empirical Methods\nin Natural Language Processing, pages 5363–5394,\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\nXiao Li, Kees van Deemter, and Chenghua Lin. 2016.\nStatistics-based lexical choice for NLG from quant-\nitative information. In Proceedings of the 9th In-\nternational Natural Language Generation confer-\nence, pages 104–108, Edinburgh, UK. Association\nfor Computational Linguistics.\nYaobo Liang, Nan Duan, Yeyun Gong, Ning Wu, Fenfei\nGuo, Weizhen Qi, Ming Gong, Linjun Shou, Daxin\nJiang, Guihong Cao, Xiaodong Fan, Ruofei Zhang,\nRahul Agrawal, Edward Cui, Sining Wei, Taroon\nBharti, Ying Qiao, Jiun-Hung Chen, Winnie Wu,\nShuguang Liu, Fan Yang, Daniel Campos, Rangan\nMajumder, and Ming Zhou. 2020. XGLUE: A new\nbenchmark dataset for cross-lingual pre-training, un-\nderstanding and generation. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 6008–6018,\nOnline. Association for Computational Linguistics.\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey\nEdunov, Marjan Ghazvininejad, Mike Lewis, and\nLuke Zettlemoyer. 2020. Multilingual denoising pre-\ntraining for neural machine translation.\nTransac-\ntions of the Association for Computational Linguist-\nics, 8:726–742.\nRebecca Marvin and Tal Linzen. 2018. Targeted syn-\ntactic evaluation of language models. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 1192–1202,\nBrussels, Belgium. Association for Computational\nLinguistics.\nTom McCoy, Ellie Pavlick, and Tal Linzen. 2019. Right\nfor the wrong reasons: Diagnosing syntactic heurist-\nics in natural language inference. In Proceedings of\nthe 57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 3428–3448, Florence,\nItaly. Association for Computational Linguistics.\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika,\nAdam Roberts, Stella Biderman, Teven Le Scao,\nM Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey\nSchoelkopf, Xiangru Tang, Dragomir Radev, Al-\nham Fikri Aji, Khalid Almubarak, Samuel Albanie,\nZaid Alyafeai, Albert Webson, Edward Raff, and\nColin Raffel. 2022.\nCrosslingual generalization\nthrough multitask finetuning.\nRamesh Nallapati, Bowen Zhou, Cicero dos Santos,\nÇa˘glar Gulçehre, and Bing Xiang. 2016. Abstract-\nive text summarization using sequence-to-sequence\nRNNs and beyond.\nIn Proceedings of the 20th\nSIGNLL Conference on Computational Natural Lan-\nguage Learning, pages 280–290, Berlin, Germany.\nAssociation for Computational Linguistics.\nAntoine Nzeyimana and Andre Niyongabo Rubungo.\n2022.\nKinyaBERT: a morphology-aware Kinyar-\nwanda language model. In Proceedings of the 60th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 5347–\n5363, Dublin, Ireland. Association for Computational\nLinguistics.\nKelechi Ogueji and Orevaoghene Ahia. 2019. Pidgin-\nunmt: Unsupervised neural machine translation from\nwest african pidgin to english.\narXiv preprint\narXiv:1912.03444.\nKelechi Ogueji, Yuxin Zhu, and Jimmy Lin. 2021.\nSmall data? no problem! exploring the viability\nof pretrained multilingual language models for low-\nresourced languages. In Proceedings of the 1st Work-\nshop on Multilingual Representation Learning, pages\n116–126, Punta Cana, Dominican Republic. Associ-\nation for Computational Linguistics.\nHemant Palivela. 2021. Optimization of paraphrase\ngeneration and identification using language mod-\nels in natural language processing.\nInternational\nJournal of Information Management Data Insights,\n1(2):100025.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nMachel Reid, Junjie Hu, Graham Neubig, and Yutaka\nMatsuo. 2021a. AfroMT: Pretraining strategies and\nreproducible benchmarks for translation of 8 african\nlanguages. In Conference on Empirical Methods in\nNatural Language Processing (EMNLP), Punta Cana,\nDominican Republic.\nMachel Reid, Junjie Hu, Graham Neubig, and Yutaka\nMatsuo. 2021b. AfroMT: Pretraining strategies and\nreproducible benchmarks for translation of 8 African\nlanguages. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 1306–1320, Online and Punta Cana, Domin-\nican Republic. Association for Computational Lin-\nguistics.\nSebastian Ruder, Matthew E. Peters, Swabha Swayam-\ndipta, and Thomas Wolf. 2019. Transfer learning in\nnatural language processing. In Proceedings of the\n2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Tu-\ntorials, pages 15–18, Minneapolis, Minnesota. Asso-\nciation for Computational Linguistics.\nYves Scherrer. 2020. TaPaCo: A corpus of sentential\nparaphrases for 73 languages. In Proceedings of the\nTwelfth Language Resources and Evaluation Confer-\nence, pages 6868–6873, Marseille, France. European\nLanguage Resources Association.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 1715–1725,\nBerlin, Germany. Association for Computational Lin-\nguistics.\nTian Shi, Yaser Keneshloo, Naren Ramakrishnan, and\nChandan K. Reddy. 2021. Neural abstractive text\nsummarization with sequence-to-sequence models.\nACM/IMS Trans. Data Sci., 2(1).\nIlya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.\nSequence to sequence learning with neural networks.\nIn Proceedings of the 27th International Conference\non Neural Information Processing Systems - Volume\n2, NIPS’14, page 3104–3112. MIT Press.\nKees Van Deemter, Emiel Krahmer, and Mariët Theune.\n2005. Real versus template-based natural language\ngeneration: A false opposition? Comput. Linguist.,\n31(1):15–24.\nEmiel van Miltenburg, Chris van der Lee, Thiago Castro-\nFerreira, and Emiel Krahmer. 2020. Evaluation rules!\non the use of grammars and rule-based systems for\nNLG evaluation. In Proceedings of the 1st Workshop\non Evaluating NLG Evaluation, pages 17–27, On-\nline (Dublin, Ireland). Association for Computational\nLinguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Proceedings of the 31st International\nConference on Neural Information Processing Sys-\ntems, NIPS’17, page 6000–6010, Red Hook, NY,\nUSA. Curran Associates Inc.\nLinting Xue, Noah Constant, Adam Roberts, Mihir\nKale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua,\nand Colin Raffel. 2021. mT5: A massively multi-\nlingual pre-trained text-to-text transformer. In Pro-\nceedings of the 2021 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages\n483–498, Online. Association for Computational Lin-\nguistics.\nAppendices\nA\nPretraining Data\nWe provide details of our pretraining data below:\nReligious Domain. Our religious data is taken\nfrom online Bibles, Qurans, and data crawled from\nthe Jehovah’s witness website. We also include\nreligious texts from the book of Mormon.\nNews Domain. We collect data from online news-\npapers (Adebara and Abdul-Mageed, 2022) and\nnews sites such as Voice of America, Voice of Ni-\ngeria, BBC, Global voices, and DW news sites. We\ncollect local newspapers from 27 languages from\nacross Africa.\nGovernment Documents. We collect government\ndocuments South African Centre for Digital Lan-\nguage Resources (SADiLaR), and the Universal\nDeclaration of human rights (UDHR) in multiple\nlanguages.\nHealth Documents. We collect multiple health\ndocuments from the Department of Health, State\nGovernment of Victoria, Australia. We collect doc-\numents in Amharic, Dinka, Harari, Oromo, Somali,\nSwahili, and Tigrinya.\nExisting Corpora. We collect corpora available\non the web for different African languages, includ-\ning from Project Gutenberg for Afrikaans, South\nAfrican News data.\nfor Sepedi and Setswana,\nOSCAR (Abadji et al., 2021) for Afrikaans, Am-\nharic, Somali, Swahili, Oromo, Malagasy, and Yor-\nuba. We also used Tatoeba for Afrikaans, Amharic,\nBemba, Igbo, Kanuri, Kongo, Luganda, Malagasy,\nSepedi, Ndebele, Kinyarwanda, Somali, Swahili,\nTsonga, Xhosa, Yoruba, and Zulu; Swahili Lan-\nguage Modelling Data for Swahili; Ijdutse cor-\npus for Hausa; Data4Good corpora for Luganda,\nCC-100 for Amharic, Fulah, Igbo, Yoruba, Hausa,\nTswana, Lingala, Luganada, Afrikaans, Somali,\nSwahili, Swati, North Sotho, Oromo, Wolof,\nXhosa, and Zulu; Afriberta-Corpus for Afaan /\nOromo, Amharic, Gahuza, Hausa, Igbo, Pidgin,\nSomali, Swahili, Tigrinya and Yoruba; mC4 for\nAfrikaans, Amharic, Hausa, Igbo, Malagasy, Chi-\nchewa, Shona, Somali, Sepedi, Swahili, Xhosa,\nYoruba and Zulu.\nB\nAfroNLG Benchmark\nWe report statistics of AfroNLG benchmark in\nTable B.1 and 2 respectively.\nDataset\nPairs\nTrain\nDev\nTest\nLafand\neng-hau\n5,866\n1,301\n1,501\neng-ibo\n6,945\n1,457\n1,412\neng-lug\n4,076\n1,501\n1,501\neng-pcm\n4,791\n1,485\n1,565\neng-swa\n30,783\n1,792\n1,836\neng-tsn\n2,101\n1,343\n1,501\neng-twi\n3,338\n1,285\n1,501\neng-yor\n6,645\n1,545\n1,559\neng-zul\n3,540\n1,462\n1,001\nfra-bam\n3,014\n1,501\n1,501\nfra-bbj\n2,233\n1,134\n1,431\nfra-ewe\n2,027\n1,415\n1,564\nfra-fon\n2,638\n1,228\n1,580\nfra-mos\n2,494\n1,493\n1,575\nfra-wol\n3,361\n1,507\n1,501\nAfroMT\neng-afr\n25,799\n3,226\n3,226\neng-bem\n12,043\n1,506\n1,506\neng-lin\n17,679\n2,211\n2,210\neng-run\n12,475\n1,560\n1,560\neng-sot\n28,844\n3,607\n3,606\neng-swa\n28,084\n3,511\n3,512\neng-xho\n26,091\n3,263\n3,262\neng-zul\n29,127\n3,641\n3,642\nPidginUNMT\neng-pcm\n1,682\n211\n211\nSALT\nAll-pairs\n20,006\n2,501\n2,502\nTable B.1: Statistics of the MT data in our benchmark.\nAll-pairs each have the same size of data. They include\nach-eng, ach-lgg, ach-lug, ach-nyn, ach-teo, ach-teo,\neng-lgg, eng-lug, eng-nyn, eng-teo, lgg-teo, lug-lgg,\nlug-teo, nyn-lgg, nyn-lug, and nyn-teo\nB.1\nCHRF and CHRF++ Results\nC\nLinguistic Details\nMorphology Morphologically, both Hausa and\nSwahili are classified as agglutinative languages\n(Jaggar, 2017; Dryer and Haspelmath, 2013), char-\nacterized by the systematic addition of prefixes,\nsuffixes, and affixes to root words or stems. This\nprocess imparts precise grammatical meanings, en-\ncompassing tense, case, mood, person, number,\nand more. Conversely, Yorùbá exhibits an analytic\nstructure, relying on word order and discrete func-\ntion words to denote grammatical relationships,\nwith minimal use of inflections or affixes. The fol-\nlowing are examples from the generated (1) Hausa,\n(2) Swahili, and (3) Yorùbá, respectively.\n(1)\na.\nBai\nneg.masculine\nbarshi\nleave\nba\nat-all\n‘he did not leave him’\nb.\nBata\nNeg.feminine\nbarshi\nleave\nba\nat-all\n‘she did not leave him’\nTask Cluster\nTest Set\nSource\nTrain\nDev\nTest\nCloze test\n517 languages\nOurs\n103,400\n25,850\n51,700\nParaphrase\nMultilingual††\n(Scherrer, 2020)\n22,390\n2,797\n2,794\nBerber\n17,607\n2,200\n2,200\nKabyle\n4,441\n555\n555\nQuestion Answering\nSwahili\n(Clark et al., 2020)\n49,881\n499\nn/a\nSummarization\nMultilingual†\n(Hasan et al., 2021)\n63,040\n7,875\n7875\nAmharic\n5,761\n719\n719\nIgbo\n4,183\n522\n522\nOromo\n6,063\n757\n757\nRundi\n5,746\n718\n718\nSwahili\n7,898\n987\n987\nYorùbá\n6,350\n793\n793\nHausa\n6,418\n802\n802\nNigerian Pidgin\n9,208\n1,151\n1,151\nSomali\n5,962\n745\n745\nTigrinya\n5,451\n681\n681\nMultilingual⋆†\nOurs\n428\nTitle Generation\nMultilingual†\n(Hasan et al., 2021)\n63,040\n7,875\n7875\nAmharic\n5,761\n719\n719\nIgbo\n4,183\n522\n522\nOromo\n6,063\n757\n757\nRundi\n5,746\n718\n718\nSwahili\n7,898\n987\n987\nYorùbá\n6,350\n793\n793\nHausa\n6,418\n802\n802\nNigerian Pidgin\n9,208\n1,151\n1,151\nSomali\n5,962\n745\n745\nTigrinya\n5,451\n681\n681\nMultilingual⋆\nOurs\n5899\nTable B.2: Statistics of the data in our benchmark. †† includes amh, ber, kab, run. † has amh, ibo, orm, run, swa, yor,\nhau, pcm, som, and tir. ⋆† is a newly created summarization test set including ‘hau’, ‘nde’ (zero-shot), and ‘swa’. ⋆\nis a newly created test set across 15 languages: ‘amh’, ‘gag’ (zero-shot), ‘hau’, ‘ibo’, ‘pcm’, ‘som’, ‘swa’, ‘tir’,\n‘yor’, ‘kin’ (zero-shot), ‘afr’, ‘mlg’ (zero-shot), ‘orm’, ‘nde’ (zero-shot), ‘sna’(zero-shot)\nTask\nMetric\nmT0\nmT5\nafri-mt5\nAfriTeVa\nCheetah\nTranslate English to Afrikaans\nChrf\n26.97±4.75\n26.11±4.12\n14.66±8.79\n20.75±4.02\n39.88±0.81\nTranslate English to Bemba\nChrf\n10.27±0.89\n6.39±1.96\n20.23±13.97\n9.94±10.05\n15.76±0.19\nTranslate English to Rundi\nChrf\n21.51±1.39\n17.56±3.13\n24.91±3.59\n31.58±2.33\n28.65±3.55\nTranslate English to Sesotho\nChrf\n21.08±3.54\n12.08±10.91\n23.75±4.77\n29.57±1.61\n29.05±2.41\nTranslate English to Swahili\nChrf\n23.26±0.16\n20.35±4.87\n24.60±0.2\n20.5±4.88\n37.24±0.04\nTranslate English to Xhosa\nChrf\n27.44±3.1\n25.88±4.94\n34.97±2.49\n20.25±15.35\n33.45±0.21\nTranslate English to Zulu\nChrf\n27.12±3.49\n21.54±2.16\n37.8±1.41\n25.39±16.55\n43.75±0.11\nTranslate English to Hausa\nChrf\n28.53±0.26\n27.65±0.53\n19.99±0.42\n31.68±0.29\n34.9±0.32\nTranslate English to Igbo\nChrf\n40.31±0.17\n37.18±0.34\n22.01±0.7\n33.24±0.23\n44.37±0.31\nTranslate English to Luganda\nChrf\n25.94±2.41\n23.33±0.31\n15.57±1.45\n24.16±2.55\n36.22±0.09\nTranslate English to N. Pidgin\nChrf\n63.49±0.05\n63.9±0.1\n24.79±0.68\n53.76±0.01\n62.95±0.17\nTranslate English to Swahili\nChrf\n50.52±3.33\n51.76±0.12\n21.00±0.7\n44.84±0.33\n56.36±0.15\nTranslate English to Setswana\nChrf\n30.89±0.36\n16.62±0.22\n13.17±1.73\n23.75±0.45\n35.87±0.64\nTranslate English to Twi\nChrf\n23.56±0.24\n15.8±1.29\n12.74±1.33\n17.47±3.26\n25.89±0.2\nTranslate English to Yoruba\nChrf\n19.41±1.97\n16.51±0.38\n11.49±0.29\n20.62±0.36\n25.09±0.07\nTranslate English to Zulu\nChrf\n35.4±1.27\n16.13±7.84\n15.04±1.1\n12.75±0.56\n38.81±0.21\nTranslate French to Bambara\nChrf\n16.49±0.39\n7.44±1.12\n10.16±1.58\n19.41±0.53\n19.91±0.05\nTranslate French to Ghomálá’\nChrf\n8.3±0.76\n6.53±0.57\n6.72±3.75\n13.16±0.4\n8.57±3.15\nTranslate French to Ewe\nChrf\n10.19±2.32\n5.46±3.02\n6.96±3.02\n13.44±1.64\n21.6±0.22\nTranslate French to Fon\nChrf\n5.67±2.65\n6.09±0.72\n5.82±1.58\n11.88±1.83\n12.71±0.41\nTranslate French to Moore\nChrf\n7.86±1.43\n5.16±2.20\n7.79±0.97\n11.42±0.7\n12.34±0.56\nTranslate French to Wolof\nChrf\n17.55±0.2\n3.15±0.12\n11.26±1.91\n17.58±0.44\n16.67±0.21\nTranslate English to N. Pidgin (pidginUNMT)\nChrf\n41.83±0.17\n37.12±0.77\n21.65±1.33\n39.04±0.50\n40.2±0.17\nTranslate Acholi to English\nChrf\n39.12±0.1\n33.07±5.49\n21.65±1.33\n34.19±0.06\n42.17±0.05\nTranslate Acholi to Lugbara\nChrf\n25.05±0.85\n20.61±5.92\n28.71±0.34\n34.01±0.29\n32.31±1.11\nTranslate Acholi to Luganda\nChrf\n22.13±0.63\n25.75±0.02\n24.31±0.1\n32.77±0.68\n37.34±0.47\nTranslate Acholi to Nyankore\nChrf\n27.52±0.45\n20.03±3.88\n24.50±0.02\n32.39±0.92\n35.0±0.33\nTranslate Acholi to Ateso\nChrf\n26.0±1.99\n22.16±1.63\n28.33±0.01\n35.37±0.61\n34.62±1.05\nTranslate English to Lugbara\nChrf\n38.84±0.01\n37.12±0.77\n39.11±0.01\n38.94±0.3\n40.2±0.17\nTranslate English to Luganda\nChrf\n43.71±0.08\n41.05±0.19\n35.34±1.11\n43.14±0.22\n49.38±0.02\nTranslate English to Nyankore\nChrf\n40.43±0.21\n38.38±0.13\n36.8±0.07\n40.36±0.17\n43.67±0.32\nTranslate English to Ateso (salt)\nChrf\n41.98±0.13\n38.91±0.05\n39.76±1.35\n42.1±0.42\n42.96±0.48\nTranslate Lugbara to Ateso\nChrf\n22.67±1.51\n20.47±0.7\n28.13±0.58\n34.3±0.64\n29.04±0.3\nTranslate Luganda to Lugbara\nChrf\n28.65±1.5\n25.74±0.5\n30.87±0.12\n34.26±0.24\n34.94±0.6\nTranslate Luganda to Ateso\nChrf\n31.74±0.22\n27.66±0.64\n34.04±0.01\n37.19±0.07\n39.05±0.49\nTranslate Nyankore to Lugbara\nChrf\n27.47±0.45\n24.63±0.76\n15.01±0.01\n33.17±0.21\n33.2±0.19\nTranslate Nyankore to Luganda\nChrf\n39.34±0.14\n37.34±0.16\n35.26±0.13\n40.48±0.63\n45.29±0.01\nTranslate Nyankore to Ateso\nChrf\n28.6±0.11\n24.64±1.05\n30.69±0.16\n34.37±0.14\n35.52±0.64\nAverage\n28.07\n23.88\n22.62\n28.77\n34.08\nTable B.3: Performance of various models on MT data using CHRF\nTask\nMetric\nmT0\nmT5\nafri-mt5\nAfriTeVa\nCheetah\nTranslate English to Afrikaans\nChrf++\n22.86±3.74\n22.32±2.80\n11.62±6.72\n17.27±2.91\n34.02±0.7\nTranslate English to Bemba\nChrf++\n9.04±0.79\n5.46±1.78\n23.65±1.87\n7.85±7.45\n13.9±0.13\nTranslate English to Rundi\nChrf++\n18.06±1.16\n14.41±2.53\n20.36±2.88\n25.39±1.57\n23.94±3.03\nTranslate English to Sesotho\nChrf++\n17.34±3.09\n10.2±8.75\n19.31±3.94\n23.85±1.43\n23.9±2.03\nTranslate English to Swahili\nChrf++\n18.5±0.31\n16.28±4.48\n19.42±2.2\n16.16±3.93\n30.6±0.11\nTranslate English to Xhosa\nChrf++\n21.34±2.66\n19.96±4.05\n26.94±1.92\n15.76±11.49\n27.0±1.01\nTranslate English to Zulu\nChrf++\n21.14±2.6\n17.32±3.17\n28.97±1.14\n19.29±12.69\n40.97±1.10\nTranslate English to Hausa\nChrf++\n25.98±0.27\n25.22±0.5\n18.28±0.41\n28.56±0.22\n32.23±0.29\nTranslate English to Igbo\nChrf++\n37.82±0.15\n34.8±0.32\n20.25±0.68\n29.89±0.22\n41.87±0.31\nTranslate English to Luganda\nChrf++\n23.15±2.19\n20.74±0.36\n13.43±1.28\n20.27±2.21\n33.12±0.08\nTranslate English to N. Pidgin\nChrf++\n60.57±0.15\n60.12±0.07\n23.85±0.64\n49.72±0.36\n59.74±0.18\nTranslate English to Swahili\nChrf++\n47.67±3.33\n48.95±0.13\n19.01±1.69\n40.84±0.31\n53.67±0.15\nTranslate English to Setswana\nChrf++\n29.02±0.35\n14.87±0.16\n11.77±1.61\n21.25±0.36\n34.05±0.64\nTranslate English to Twi\nChrf++\n21.25±0.22\n13.63±1.18\n11.7±1.13\n15.39±3.02\n23.96±0.2\nTranslate English to Yoruba\nChrf++\n18.41±1.89\n15.47±0.4\n10.19±0.25\n18.99±0.27\n24.1±0.06\nTranslate English to Zulu\nChrf++\n30.99±1.13\n13.86±6.85\n11.34±2.1\n10.58±0.77\n34.31±0.2\nTranslate French to Bambara\nChrf++\n15.75±0.36\n6.8±0.97\n10.2±1.41\n18.28±0.49\n19.65±0.14\nTranslate French to Ghomálá’\nChrf++\n7.0±0.77\n5.64±0.44\n5.84±3.04\n11.13±0.34\n7.28±2.83\nTranslate French to Ewe\nChrf++\n9.09±2.21\n4.75±2.76\n6.56±3.19\n11.72±1.4\n20.53±0.23\nTranslate French to Fon\nChrf++\n5.24±2.33\n5.57±0.63\n5.28±1.38\n10.94±1.93\n11.76±0.45\nTranslate French to Moore\nChrf++\n7.08±1.33\n4.63±2.02\n7.18±0.79\n10.31±0.64\n11.2±0.54\nTranslate French to Wolof\nChrf++\n16.27±0.24\n2.65±0.11\n10.23±1.73\n15.73±0.33\n15.58±0.19\nTranslate English to N. Pidgin (pidginUNMT)\nChrf++\n42.12±0.18\n37.67±1.64\n22.53±1.31\n28.38±0.98\n39.58±0.49\nTranslate Acholi to English\nChrf++\n37.96±0.1\n27.18±0.36\n28.24±0.38\n31.83±0.07\n41.06±0.06\nTranslate Acholi to Lugbara\nChrf++\n23.41±0.84\n19.57±5.04\n27.18±0.36\n31.45±0.29\n30.68±1.02\nTranslate Acholi to Luganda\nChrf++\n25.67±0.34\n19.59±0.56\n21.52±0.02\n28.52±0.63\n33.93±0.48\nTranslate Acholi to Nyankore\nChrf++\n24.02±0.41\n17.35±3.35\n21.38±0.23\n27.73±0.84\n31.04±0.29\nTranslate Acholi to Ateso\nChrf++\n23.65±1.87\n20.07±1.53\n25.81±0.04\n31.56±0.57\n31.83±0.99\nTranslate English to Lugbara\nChrf++\n36.83±0.03\n38.3±0.13\n37.29±0.12\n34.3±0.77\n35.85±0.01\nTranslate English to Luganda\nChrf++\n40.1±0.06\n37.56±0.19\n32.18±1.05\n38.28±0.2\n45.82±0.04\nTranslate English to Nyankore\nChrf++\n35.93±0.18\n34.07±0.12\n32.59±0.05\n34.88±0.15\n39.17±0.33\nTranslate English to Ateso (salt)\nChrf++\n37.98±0.11\n38.93±0.01\n36.83±1.23\n37.85±0.4\n39.87±0.47\nTranslate Lugbara to Ateso\nChrf++\n20.55±1.38\n18.54±0.65\n25.6±0.64\n30.48±0.59\n26.43±0.32\nTranslate Luganda to Lugbara\nChrf++\n26.79±1.49\n23.94±0.48\n29.13±0.11\n31.56±0.24\n33.04±0.58\nTranslate Luganda to Ateso\nChrf++\n28.94±0.22\n25.11±0.59\n31.26±0.01\n33.18±0.05\n35.99±0.45\nTranslate Nyankore to Lugbara\nChrf++\n22.89±0.73\n25.75±0.44\n12.07±0.11\n30.54±0.2\n31.35±0.2\nTranslate Nyankore to Luganda\nChrf++\n35.7±0.12\n33.73±0.15\n31.99±0.07\n35.74±0.54\n41.63±0.0\nTranslate Nyankore to Ateso\nChrf++\n26.03±0.08\n22.35±0.98\n28.05±0.09\n30.53±0.13\n32.65±0.62\nAverage\n25.58\n21.67\n20.50\n25.16\n31.24\nTable B.4: Performance of various models on MT data using CHRF++\n(2)\na.\nHa-ku-mu-a-cha\n3pl.sg.sub-neg-3pl.sg.obj-leave\n‘He did not leave him’\nb.\nHa-ku-mu-a-cha\n3pl.sg.sub-neg-3pl.sg.obj-leave\n‘She did not leave him’\n(3)\na.\nÒhun\n3pl.sg.sub\nò\nneg\nkúrò\nleave\nlÓ\n˙\ndÒ\n˙\nfrom\nè\n˙3pl.sg.obj\n‘He did not leave him’\nb.\nÒhun\n3pl.sg.sub\nò\nneg\nkúrò\nleave\nlÓ\n˙\ndÒ\n˙\nfrom\nè\n˙3pl.sg.obj\n‘She did not leave him’\nPhonology In terms of phonology, Yorùbá and\nHausa are tonal languages, where pitch distinctions\ncontribute to word differentiation. However, Hausa\nfeatures a relatively simpler tone system compared\nto Yorùbá and in most cases tone is not marked\nin Hausa orthography. Only dictionaries and ped-\nagogical materials indicate tone in text. Yorùbá\non the other hand has three tones and indicating\ntones in orthography significantly reduces ambigu-\nity (Adebara and Abdul-Mageed, 2022). Swahili,\nin contrast, is devoid of tones altogether.\nD\nCloze Task Results\nWe provide results on the performance of each\nmodel on individual languages. We use a dash ’-’\nto indicate that a specific model does not support a\nlanguage.\nE\nAnnotation\nWe gave the following annotation rules to our an-\nnotators: Faithfulness refers to how close to the\nEnglish sentence the model output is. It should\nbe annotated with values between 1 and 5. Faith-\nfulness should be evaluated independently of the\nfluency of the model output. Below are some de-\ntailed explanations for the scale for faithfulness:\n• Give a value 1 if model output is not related\nto the source sentence.\n• Give a value 2 if the model output is the op-\nposite of the source sentence.\nISO\nMT0\nMT5\nAfriMT5\nAfriTeVa\nCheetah\nafr\n0\n0\n-\n-\n20.45\namh\n0\n0\n-\n0\n0\nbam\n-\n-\n0\n-\n0\nbbj\n-\n5.21\n0\n-\n8.45\newe\n-\n-\n0\n-\n0\nfon\n-\n-\n0\n-\n0\nhau\n0\n0\n0\n0\n13.41\nibo\n0\n0\n0\n0\n0\nlin\n0\n-\n-\n-\n25.35\nlug\n-\n-\n0\n-\n0\nluo\n-\n-\n0\n-\n9.35\nmos\n-\n-\n0\n-\n14.53\nmlg\n0\n0\n-\n-\n15.65\nnya\n-\n-\n-\n-\n7.64\nnyj\n-\n-\n-\norm\n0\n-\n-\n-\n0\npcm\n-\n-\n0\n0\n10.10\nsna\n0\n0\n-\n-\n0\nsom\n0\n0\n-\n0\n10.39\nsot\n4.69\n-\n-\n-\n15.23\nswa\n-\n-\n0\n0\n7.02\nswh\n-\n-\ntir\n-\n-\n-\n-\n6.33\ntsn\n-\n-\n0\n-\n0\ntwi\n-\n-\n0\n-\n0\nwol\n-\n-\n0\n-\n0\nxho\n0\n0\n-\n-\n6.92\nyor\n0\n3.61\n0\n0\n6.42\nzul\n0\n0\n0\n-\n8.05\nTable D.1: Bleu scores for mask-one cloze task on the\nunion of languages represented in the four models we\ncompare Cheetah with. Red describes zero-shot per-\nformance greater than 0.\nISO\nMT0\nMT5\nAfriMT5\nAfriTeVa\nCheetah\nafr\n0\n0\n-\n-\n0\namh\n0\n0\n-\n0\n0\nbam\n-\n-\n0\n-\n0\nbbj\n-\n-\n0\n-\n0\newe\n-\n-\n0\n-\n0\nfon\n-\n-\n0\n-\n0\nhau\n0\n-\n0\n0\n6\nibo\n0\n-\n0\n0\n8\nlin\n0\n-\n-\n-\n0\nlug\n-\n-\n0\n-\n0\nluo\n-\n-\n0\n-\n0\nmos\n-\n-\n0\n-\n0\nmlg\n0\n-\n-\n-\n0\nnya\n0\n0\n-\n-\n12\nnyj\n-\n-\n-\norm\n0\n-\n0\n-\n0\npcm\n-\n-\n0\n0\n0\nsna\n0\n0\n-\n-\n0\nsom\n0\n0\n-\n0\n4\nsot\n-\n-\n-\n-\n10\nswa\n-\n-\n0\n0\n12\nswh\n-\n-\ntir\n-\n-\n0\n0\n0\ntsn\n-\n-\n0\n-\n0\ntwi\n-\n-\n0\n-\n0\nwol\n-\n-\n0\n-\n0\nxho\n0\n0\n0\n-\n6\nyor\n0\n0\n0\n0\n0\nzul\n0\n0\n0\n-\n0\nTable D.2: Bleu scores for mask-at-least-one cloze task\non the union of languages represented in the four models\nwe compare Cheetah with.\n• Give a value 3 if the model output is somewhat\nrelated to the source sentence. It should have\nsome words or phrases that make it related to\nthe source.\n• Give a value 4 if the model output is closely\nrelated but changes the meaning slightly (e.g\ndifference in gender, number etc)\n• Give a value 5 if the model output is an exact\ntranslation\nFluency is how grammatically correct the model\nis. Faithfulness and fluency should be judged inde-\npendently. That is, even if the output is not faithful,\ndon’t use it to determine the fluency score and vice\nversa. Here are some detailed explanations on how\nto assign the values:\n• Give a value 1 if model output is completely\nungrammatical and nonsensical.\n• Give a value 2 if the model output is reas-\nonable but includes some foreign words or\ngibberish.\n• Give a value 3 if the model output contains\nsome grammatical phrases but also contains\nsome ungrammatical phrases.\n• Give a value 4 if the model output is almost\ngrammatical (but may have a few errors like\nspelling mistakes)\n• Give a value 5 if the model output is very\nfluent and sounds looks like what a native\nspeaker will say.\nF\nResults on Quality Evaluation\nFigure F.1: Faithfulness and fluency for Intransitives in Hausa, Swahili, and Yorùbá\nFigure F.2: Faithfulness and fluency for Transitives in Hausa, Swahili, and Yorùbá\nFigure F.3: Performance on some intransitive examples in the Yorùbá test set. The correct words have no highlights,\nplausible words or phrases are highlighted with yellow ink while wrong words and phrases are highlighted with\ngrey highlights. We use plausible to refer to words or phrases that can be used in place of the gold or which give\nadditional information.\nFigure F.4: Faithfulness and fluency for Intransitives + Negation in Hausa, Swahili, and Yorùbá\nFigure F.5: Faithfulness and fluency for Transitives + Negation in Hausa, Swahili, and Yorùbá\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2024-01-02",
  "updated": "2024-01-10"
}