{
  "id": "http://arxiv.org/abs/2401.03069v4",
  "title": "Towards Enhancing the Reproducibility of Deep Learning Bugs: An Empirical Study",
  "authors": [
    "Mehil B. Shah",
    "Mohammad Masudur Rahman",
    "Foutse Khomh"
  ],
  "abstract": "Context: Deep learning has achieved remarkable progress in various domains.\nHowever, like any software system, deep learning systems contain bugs, some of\nwhich can have severe impacts, as evidenced by crashes involving autonomous\nvehicles. Despite substantial advancements in deep learning techniques, little\nresearch has focused on reproducing deep learning bugs, which is an essential\nstep for their resolution. Existing literature suggests that only 3% of deep\nlearning bugs are reproducible, underscoring the need for further research.\n  Objective: This paper examines the reproducibility of deep learning bugs. We\nidentify edit actions and useful information that could improve the\nreproducibility of deep learning bugs.\n  Method: First, we construct a dataset of 668 deep-learning bugs from Stack\nOverflow and GitHub across three frameworks and 22 architectures. Second, out\nof the 668 bugs, we select 165 bugs using stratified sampling and attempt to\ndetermine their reproducibility. While reproducing these bugs, we identify edit\nactions and useful information for their reproduction. Third, we used the\nApriori algorithm to identify useful information and edit actions required to\nreproduce specific types of bugs. Finally, we conducted a user study involving\n22 developers to assess the effectiveness of our findings in real-life\nsettings.\n  Results: We successfully reproduced 148 out of 165 bugs attempted. We\nidentified ten edit actions and five useful types of component information that\ncan help us reproduce the deep learning bugs. With the help of our findings,\nthe developers were able to reproduce 22.92% more bugs and reduce their\nreproduction time by 24.35%.\n  Conclusions: Our research addresses the critical issue of deep learning bug\nreproducibility. Practitioners and researchers can leverage our findings to\nimprove deep learning bug reproducibility.",
  "text": "Noname manuscript No.\n(will be inserted by the editor)\nTowards Enhancing the Reproducibility of Deep\nLearning Bugs: An Empirical Study\nMehil B. Shah · Mohammad Masudur\nRahman · Foutse Khomh\nReceived: date / Accepted: date\nAbstract\nContext: Deep learning has achieved remarkable progress in various do-\nmains. However, like any software system, deep learning systems contain bugs,\nsome of which can have severe impacts, as evidenced by crashes involving au-\ntonomous vehicles. Despite substantial advancements in deep learning tech-\nniques, little research has focused on reproducing deep learning bugs, which\nis an essential step for their resolution. Existing literature suggests that only\n3% of deep learning bugs are reproducible, underscoring the need for further\nresearch.\nObjective: This paper examines the reproducibility of deep learning bugs.\nWe identify edit actions and useful information that could improve the repro-\nducibility of deep learning bugs.\nMethod: First, we construct a dataset of 668 deep learning bugs from\nStack Overflow and GitHub across three frameworks and 22 architectures.\nSecond, out of the 668 bugs, we select 165 bugs using stratified sampling and\nattempt to determine their reproducibility. While reproducing these bugs, we\nidentify edit actions and useful information for their reproduction. Third, we\nused the Apriori algorithm to identify useful information and edit actions\nrequired to reproduce specific types of bugs. Finally, we conduct a user study\ninvolving 22 developers to assess the effectiveness of our findings in real-life\nsettings.\nMehil Shah\nDalhousie University, Canada\nE-mail: shahmehil@dal.ca\nMohammad Masudur Rahman\nDalhousie University, Canada\nE-mail: masud.rahman@dal.ca\nFoutse Khomh\nPolytechnique Montreal, Canada\nE-mail: foutse.khomh@polymtl.ca\narXiv:2401.03069v4  [cs.SE]  22 Oct 2024\n2\nMehil B. Shah et al.\nResults: We successfully reproduced 148 out of 165 bugs attempted. We\nidentified ten edit actions and five useful types of component information that\ncan help us reproduce the deep learning bugs. With the help of our findings,\nthe developers were able to reproduce 22.92% more bugs and reduce their\nreproduction time by 24.35%.\nConclusions: Our research addresses the critical issue of deep learning\nbug reproducibility. Practitioners and researchers can leverage our findings to\nimprove deep learning bug reproducibility.\n1 Introduction\nDeep learning (hereby DL) has been widely used in many application do-\nmains, including natural language processing, finance, cybersecurity [1, 2, 3],\nautonomous vehicles [4, 5], and healthcare systems [6].\nLike any software system, deep learning systems are prone to bugs. Bugs\nin deep learning systems could arise from various sources, including errors in\nthe dataset, incorrect hyperparameters, and incorrect structure of the deep\nlearning model [7]. These bugs could lead to program failures, poor perfor-\nmance, or incorrect functionality, as reported by existing literature [8]. They\ncan also lead to serious consequences, as shown by the fatal crash involv-\ning Uber’s self-driving car [9]. Thus, we must fix the bugs before deploying\na deep learning model in production. However, one must reproduce the bugs\nbefore fixing them to verify their presence/absence in the system. Unfortu-\nnately, reproducing DL bugs is challenging due to deep learning systems’ mul-\ntifaceted nature and dependencies, which encompass data, hardware, libraries,\nframeworks, and client programs. Furthermore, deep learning systems are in-\nherently non-deterministic (i.e., random weight initialization), which leads to\ndifferent outcomes across multiple runs and thus makes the reproduction of\ndeep learning bugs challenging [10]. Moreover, they also suffer from a lack\nof interpretability [11], which makes the reproduction of deep learning bugs\nchallenging in comparison to traditional software bugs. According to existing\ninvestigations [12], only 3% of their analyzed deep learning bugs were repro-\nducible, further demonstrating the challenges in reproducing DL bugs.\nExisting literature investigates the challenges of reproducing programming\nerrors or bugs from various sources. Mondal et al. [13] investigate the chal-\nlenges in reproducing programming issues reported on Stack Overflow and\nsuggest several edit actions to help reproduce them. Rahman et al. [14] con-\nduct a multi-modal study to understand the factors behind the non-reprodu-\ncibility of software bugs. They identify 11 significant factors behind bug non-\nreproducibility, including missing information, bug duplication, false positive\nbugs, and intermittency. Overall, these studies highlight the challenges in re-\nproducing traditional software bugs but do not deal with any deep learning\nbugs, which warrants further investigation.\nRecently, a few studies have attempted to tackle the challenges of deep\nlearning bugs. Liang et al. [15] provide a dataset of 64 deep learning bugs col-\nTitle Suppressed Due to Excessive Length\n3\nlected from GitHub issues. They classify these bugs into six categories accord-\ning to the taxonomy of Humbatova et al. [16]. Moravati et al. [12] constructed\na benchmark dataset containing 100 deep learning bugs collected from Stack-\nOverflow and GitHub; they reproduced each of them. Although these studies\noffer benchmark datasets containing reproducible bugs, their primary focus is\ndataset construction. They do not report any detailed instructions (e.g., se-\nquence of actions) essential to the reproduction of the deep learning bugs. Our\nwork attempts to fill this important gap in the literature.\nIn this paper, we conduct an empirical study to better understand the\nchallenges in reproducing deep learning bugs. First, we collect a total of 568 DL\nbugs from Stack Overflow posts. Then, we extend them with 100 DL bugs from\nthe benchmark dataset of Moravati et al. [12], which makes up our final dataset\nof 668 DL bugs. Using the taxonomy of Humbatova et al. [12], we divide these\nbugs into five categories: 167 model, 213 tensor, 145 training, 113 GPU, and 30\nAPI bugs. Second, by using stratified sampling, we select 165 of these bugs and\ndetermine their reproducibility status by attempting to reproduce them using\nthe code snippet and complementary information from Stack Overflow and the\nbenchmark dataset. Third, the first author manually analyzed the produced\nartefacts to determine the categories of information that are useful for deep\nlearning bug reproduction. Fourth, we use the information gathered during\nthe bug reproduction and create a dataset of transactions that link the type of\nbug with edit actions and component information. Then, our study establishes\nconnections among the component information, edit actions, and the type of\nbugs by employing the Apriori algorithm [17]. Finally, to further validate our\nfindings, we conducted a developer study with 22 participants from industry\nand academia. Half of the participants were asked to reproduce bugs using our\nidentified information, while the other half were asked to reproduce the same\nbugs without access to that information. Results from the user study show\nthat our recommended edit actions and component information can reduce\nthe time to reproduce the bug by 24.35%. Thus, we answer three important\nresearch questions as follows:\n– RQ1: Which edit actions are useful for reproducing deep learning\nbugs?\nDetermining the key edit actions that are crucial for reproducing deep\nlearning bugs is important since almost none of the bugs can be repro-\nduced using the verbatim code snippet. By manually reproducing 148 deep\nlearning bugs, we identify ten key edit actions that could be useful to re-\nproduce deep learning bugs (e.g., input data generation, neural network\nconstruction, hyperparameter initialization).\n– RQ2: What types of component information and edit actions are\nuseful for reproducing specific types of deep learning bugs?\nDifferent types of DL bugs need different types of information or edit ac-\ntions, which warrants further investigation. Using the Apriori algorithm,\nwe have determined the top 3 pieces of information and the top 5 edit\nactions that can help one reproduce each type of bug. These insights can\n4\nMehil B. Shah et al.\nbe used not only to detect the missing information in a submitted bug\nreport but also to formulate the follow-up questions soliciting the missing\ninformation.\n– RQ3: How do the suggested edit actions and information affect\nthe reproducibility of deep learning bugs?\nTo assess the effectiveness of our suggested edit actions and component\ninformation in improving bug reproducibility, we conducted a user study\ninvolving 22 professional developers. In our developer study, the partici-\npants assigned to the experimental group used our suggested edit actions\nand component information to reproduce deep learning bugs. In contrast,\nthe participants in the control group reproduced the bugs without access\nto the suggested edit actions and component information. We found that\nour recommended edit actions and component information (a) helped the\ndevelopers reproduce 22.92% more bugs and (b) decreased their time to\nreproduce the deep learning bugs by 24.35%.\nThe remainder of the paper is organized as follows. Section 2 presents a\nmotivating example that highlights the challenges in reproducing deep learn-\ning bugs. Section 3 introduces the necessary background information for deep\nlearning bugs and their bug reports. Section 4 then describes our study method-\nology in detail, including data collection, dataset construction, environment\nsetup, qualitative analysis, and user study design. Next, Section 5 presents\nthe findings by answering our three research questions. It summarizes the key\nedit actions, component information, and their relationships with bug types\nthat enhance deep learning bug reproducibility. Section 6 discusses our results\nand shows how our findings can be used to improve LLMs. Section 7 discusses\nthe threats to the validity of this study. Finally, Section 8 reviews related\nliterature, and Section 9 concludes the paper.\n2 Motivating Example\nBug reports or programming Q&A posts might not always provide sufficient\ninformation to reproduce a deep learning bug. Let us consider the example\nquestion shown in Fig. 1(a) [18]. Here, the user attempts to train a neural\nnetwork using Keras. The input data type is a Sequence object, which is used\nas the base object for fitting a sequence of data, such as a dataset [19] The user\naims to pass the training dataset as a Sequence object to the fit generator()\nmethod. However, s/he discovers that their Sequence object is not recognized\nby the Keras library. The user also provides a code snippet to aid the repro-\nducibility of the bug. Unfortunately, the issue cannot be reproduced since the\nprovided information does not contain the required dependencies and imports.\nIn Stack Overflow, this question has failed to receive a precise response. Even\nthough the above code (Fig. 1(a)) could be made compilable or executable\nusing edit actions, the bug cannot be reproduced due to its complex nature.\nSince the earlier study [13] does not deal with any deep learning bugs, their\nsuggested edits might also not be effective.\nTitle Suppressed Due to Excessive Length\n5\n(a) Bug reported by the Stack Overflow User\n(b) Answers highlighting the non-reproducibility of the bug\nFig. 1: An Irreproducible Bug from Stack Overflow\nLet us consider another example question shown in Fig. 2(a) [20]. Here,\nthe user wants to obtain the confusion matrix from a multi-class classifica-\ntion model. Unfortunately, s/he runs into a runtime error, as the confusion\nmatrix() method does not support multiple output labels. To reproduce this\nbug, we first generate a synthetic multi-class dataset since the training data\nis missing from the question. We also add the required import statement for\nthe ‘confusion matrix’ function. Then, we initialize the model hyperparame-\nters based on the code snippet. When we run the code on this synthetic data,\nit triggers the same runtime error due to the wrong data shape being passed\nto ‘confusion matrix()’. Thus, by extracting the key information and apply-\ning the necessary edit actions, we were able to reproduce the bug. Similarly,\nthis issue was reproduced by other users of Stack Overflow, and the question\nreceived a correct solution within two hours of its submission, as shown in\nFig. 2(b). To summarize, by generating a synthetic dataset, adding the nec-\nessary import statement, and initializing the model hyperparameters, we can\nreproduce the example deep learning bug. Our study in this article proposes\nthe methodology to extract the information and determine the edit actions\nnecessary for systematically reproducing deep learning bugs.\n6\nMehil B. Shah et al.\n(a) Bug reported by the Stack Overflow User\n(b) Accepted Answer\nFig. 2: A Reproducible Bug from Stack Overflow\n3 Background\n3.1 Deep Learning Bugs\nDevelopers often encounter different types of bugs when they write their deep-\nlearning software. According to existing literature [16, 21], these bugs can be\nclassified into five categories: Training, Model, Tensor and Input, API, and\nGPU. Table. 1 shows the prevalence of these bugs in deep learning systems.\nWe discuss these bugs in detail below.\n– Training Bugs: Training bugs encompass a wide range of issues that im-\npact various aspects of the training process, including data preprocessing,\nhyperparameter tuning, loss function selection, and model optimization.\nData preprocessing steps, such as normalization and input scaling, are fre-\nquently overlooked or improperly executed, leading to subpar results. Qual-\nTitle Suppressed Due to Excessive Length\n7\nTable 1: Prevalence of Different Types of Deep Learning Bugs\nType of Bug\nTraining\nModel\nTensor and Input\nAPI\nGPU\nPrevalence\n52.5%\n19.7%\n19.5%\n5.3%\n2.9%\nity of training data is another critical factor that can affect of the training\nprocess [22]. Insufficient, imbalanced, or incorrectly labelled training data\ncan significantly impact the model’s ability to generalize and produce accu-\nrate predictions. Hyperparameter problems are quite common, with issues\nsuch as incorrect learning rates, inappropriate batch sizes, and inadequate\nselection of the number of epochs [16]. Similarly, the selection and im-\nplementation of loss functions could be suboptimal or flawed. The choice\nand tuning of optimizers can also pose challenges, as incorrect selections\nor settings can hinder the model performance. Additional problems that\nmay arise during training include memory mismanagement and the ab-\nsence of data augmentation techniques. Overall, the multifaceted nature of\nneural network training, with its numerous hyperparameters and intricate\narchitectures, often introduces bugs into the training process.\n– Model Bugs: Model bugs stem from the neural network architectures and\ntheir components. Common faults include wrong model type for a task, in-\ncorrect network depth with too few or too many layers, and issues with\nspecific layers. For instance, choosing a multilayer perceptron over a con-\nvolutional neural network for an image classification task leads to poor per-\nformance, as the model type does not fit the problem. Similarly, networks\nwith inadequate or excessive depth may fail to capture patterns or may\noverfit, respectively. Faulty layer design, such as the omission of data ma-\nnipulation layers, could lead to flaws in feature extraction, whereas redun-\ndant layers increase the complexity of the model. Other frequent sources\nof bugs are suboptimal layer parameters like neuron counts, kernel dimen-\nsions, and stride lengths. Finally, inaccurate activation function selection,\nsuch as incorrectly using sigmoid instead of ReLU in the hidden layers, can\ndegrade the model performance. Overall, model bugs stem from improper\nnetwork design and configuration, leading to a fundamentally unsuitable\nmodel structure.\n– Tensor & Input Bugs: Tensor bugs stem from incorrect tensor shapes\nprovided in the deep learning models, i.e. mismatches between the expected\ntensor dimensions and the actual shape of the input tensor. Potential causes\ninclude errors in padding, indexing, or tensor transformations like trans-\nposing before model ingestion. Input bugs occur when the input data does\nnot match the specifications the model requires. Examples include incor-\nrect data types (e.g. strings instead of numeric), inconsistent data shapes\n(e.g. wrong matrix dimensions), and improper data formats (e.g. channels\nreversed in images). While tensor bugs arise from incorrect shaping of the\ninput data, input bugs stem from invalid or incompatible characteristics of\n8\nMehil B. Shah et al.\nthe raw input itself. Both categories can seriously hurt the model perfor-\nmance by propagating flawed representations, even if no crashes occur.\n– API Bugs: API bugs primarily occur due to an incorrect usage of the API.\nSeveral factors can lead to incorrect API usage. First, API definitions often\nchange across framework versions, causing developers to use outdated or\nincompatible syntax. Second, there can be API incompatibility within the\nsame framework. For example, in TensorFlow 1.x, the developers could use\nthe Estimator API with model fn() to build models. But, in TensorFlow\n2.0, Estimators were removed, so code written for v1 would cause compile-\ntime errors when the framework was updated. Finally, unclear or confusing\ndocumentation can result in developers misunderstanding the proper use of\nan API. This can lead to multiple issues, such as using an API incorrectly\nin a manner that deviates from the framework’s intended use. Furthermore,\ndevelopers may encounter challenges when the documentation lacks clear\nspecifications of crucial API calls for a workflow or misplaces API call\nexplanations within the code.\n– GPU Bugs: GPU bugs are the most difficult bugs to localize and repro-\nduce, as observed by Jahan et al. [23]. GPU bugs can occur due to various\nfactors such as incorrect GPU device referencing, failed parallelism, im-\nproper state sharing between subprocesses, and faulty data transfers to\nthe GPU. Incorrect device referencing stems from developers misunder-\nstanding the mapping between CPU and GPU environments [24]. Failed\nparallelism and improper state sharing often result from subtle data races\nor synchronisation errors, respectively [25, 26]. Faulty data transfers arise\nfrom mismatches between CPU and GPU data formats [27]. Overall, the\ndomain-specific nature of GPU programming, the rapid evolution of GPU\narchitectures, and the lack of sufficient validation could lead to difficult-\nto-diagnose GPU bugs.\n3.2 Deep Learning Bug Reports\nTo date, only limited investigation has been conducted to understand the\npatterns of deep learning (DL) bug reports. Long et al. [28] performed the\nfirst exploratory study to analyze the bug reporting trends and patterns for\ndeep learning frameworks. Their work revealed several key themes that provide\ninsight into the nature and lifecycle of these bug reports:\nRoot Causes and Prevalence: Their study has found that low training\nspeed is the most common symptom for submitting performance-related bug\nreports, ranging from 27% to 67% across the different DL frameworks. How-\never, no consistent pattern was observed for the root causes of accuracy-related\nreports. This suggests that performance issues, especially those manifesting as\nslow execution speed, are a major pain point encountered by the users of DL\nframeworks.\nAffected Stages of DL Pipeline: Across the frameworks studied, the\ntraining stage was found to be the most prevalent in performance and accu-\nTitle Suppressed Due to Excessive Length\n9\nracy bug reports, ranging from 38% to 77%. This finding is not surprising, as\ntraining is typically the most computationally intensive and time-consuming\nstage of the DL pipeline [29]. Performance bottlenecks or accuracy issues en-\ncountered during training can significantly impact the overall usability and\nefficiency of the framework.\nReport Quality and Resolution: Their study found that a majority\nof the closed reports (69% to 100%) were either not classified, or their titles,\nlabels, and content did not match the actual bugs reported. Furthermore,\naround 50% of the reports that did reveal bugs were not resolved by direct\ncode patches. These findings highlight inefficiencies in the bug reporting and\nresolution process.\nIn summary, the paper classifies performance and accuracy bug reports\nbased on symptoms, pipeline stages affected, and GitHub resolution states to\nbetter understand the bugs from deep learning frameworks.\n4 Study Methodology\nFig. 3 shows the schematic diagram of our empirical study. We discuss different\nsteps of our study as follows.\n4.1 Selection of Data Sources\nWe select Stack Overflow as a primary data source for our study. It is the\nlargest programming Q&A site for programming topics containing over 24\nmillion questions and 35 million answers [30]. Thus, Stack Overflow could be\na potential source for deep learning (DL) bugs. Developers often submit their\nencountered problems on Stack Overflow, when building their deep learning\napplications. According to a recent work [31], Stack Overflow contains at least\n30 topics and 80K posts related to deep learning issues, which makes it a\npotential source for our data. We also select the benchmark dataset of Moravati\net al. [12] (Defects4ML in Fig. 3) containing 100 DL bugs as our second source\nof data. Specifically, we use 63 GitHub issues and 37 Stack Overflow posts\nfrom the Defects4ML dataset in our study.\nTo gather relevant posts from Stack Overflow, we use the Stack Exchange\nData Explorer platform1. We employ multiple filters to capture relevant posts\ndiscussing deep learning bugs. First, we select the recent posts (i.e., submit-\nted between May 2020 and May 2023) with the following tags: ‘tensorflow’,\n‘keras’, and ‘pytorch’. These tags were selected since they represent the most\nfrequently used frameworks for deep learning [32]. This filtration resulted in\n14,065 posts for PyTorch, 14,971 posts for Tensorflow, and 3,152 posts for\nKeras. We then applied the following four filtration criteria to remove concep-\ntual questions from our collection:\n1 https://data.stackexchange.com/stackoverflow/query/new\n10\nMehil B. Shah et al.\nFig. 3: Schematic diagram of our empirical study\n– Keyword Filtering: This filtration removes the how-to questions and the\nquestions requesting installation instructions. To filter these questions, we\nuse appropriate keywords (‘how’, ‘install’, and ‘build’), as recommended\nby Humbatova et al. [16].\n– Code Snippet: Given our focus on the reproducibility of reported issues,\nthe questions must include code segments. Therefore, we consider such\nquestions that contain at least one line of code, as suggested by Mondal et\nal. [13].\n– Accepted Answer: We only select the questions with accepted answers,\nensuring that each reported issue (e.g., DL bug) was reproduced and fixed,\nas advised by Humbatova et al. [16].\n– Negative Question Score: This filtration helps us discard questions that\nthe community has found to be of low quality, as proposed by Ponzanelli\net al. [33].\nWe combine all four filtration criteria above, construct a SQL query, and then\nexecute the query against the Stack Exchange Data Explorer [34]. The SQL\nquery is available in our replication package [35]. After this filtration step,\nwe obtained a total of 279 posts for Keras, 1,433 posts for Tensorflow, and\n1,700 posts for PyTorch. To ensure the validity of our chosen filtration criteria\nand confirm that the posts are related to DL bugs, we conducted a follow-up\nmanual analysis on a representative sample of the filtered posts. We determined\nthe appropriate sample size using Cochran’s sample size formula [36]:\nn = Z2 · p · (1 −p)\ne2\nWhere:\n– n = sample size\n– Z = Z-value for the desired confidence level (1.645 for 90% confidence)\n– p = population proportion (assumed to be 0.5 for maximum variability)\n– e = margin of error (0.1 or 10%)\nAfter plugging in the values:\nTitle Suppressed Due to Excessive Length\n11\nn = 1.6452 · 0.5 · (1 −0.5)\n0.12\n= 67.65\nTherefore, a representative sample of 67 posts was needed to achieve 90%\nconfidence with a 10% margin of error. We chose these parameters to balance\nthe precision of the estimates with the manual effort required for the analy-\nsis. Moreover, higher confidence levels or lower margins of error would have\nnecessitated significantly larger sample sizes and analysis time as follows:\n– 95% confidence, 5% margin of error: n = 346 posts (86.5 hours)\n– 99% confidence, 1% margin of error: n = 2, 832 posts (708 hours)\nFor each of the 67 sampled posts, we analyzed the entire Q&A discussion\nthread from Stack Overflow and the corresponding code changes in GitHub.\nThis manual analysis took approximately 15 minutes per post, resulting in a\ntotal of around 17 hours of effort. We found that ≈96% of the sampled posts\ndiscussed deep learning bugs, confirming the validity of our filtration criteria\n(Steps 1-2, Fig. 3).\n4.2 Dataset Construction\nThe above filtration step resulted in 3,412 Stack Overflow posts. To construct\nour final dataset, we apply another filter to these posts where we leverage the\ntags from Humbatova et al.[16]. We aimed to select representative samples\nfrom different types of deep learning bugs. To reduce the risk of miscategoriz-\ning bug types, we selected tags from Table 1 that exactly matched the leaves\nof the bug taxonomy proposed by Humbatova et al. [12]. We identified several\nStack Overflow tags that partially matched the leaves of the taxonomy. How-\never, to eliminate confusion about bug types, we only used tags that had an\nexact match with the taxonomy. Through this manual analysis of posts, bugs\nand tags, we selected the tags that best represented each bug taxonomy/sub-\ntaxonomy from Humbatova et al. [16]. Table. 2 shows our selected 3 tags for\nmodel bugs, 1 for tensor bugs, 11 for training bugs, 3 for GPU bugs, and 7 for\nAPI bugs. We look for these tags in the above Stack Overflow posts and collect\n113 Model bugs, 193 Tensor Bugs, 95 Training Bugs, 101 GPU Bugs, and 24\nAPI Bugs. We also discover 42 bugs belonging to multiple categories (e.g., 5\nModel & Tensor Bugs, 5 Tensor & GPU Bugs, 3 Training & API Bugs). Thus,\nthe final dataset contains a total of 668 bugs (568 from our dataset + 100 from\nDefects4ML [12]) and captures a balanced representation from different types\nof DL bugs, as shown in Table 3 (Step 3, Fig. 3).\n4.3 Environment Setup\nFor our experiments, we use the following environment setup:\n12\nMehil B. Shah et al.\nTable 2: Tags used for filtering different types of bugs\nType of Bug\nTags\nModel\nlayer, model, activation-function\nTensor\ntensor\nTraining\nloss-function, training-data, optimization, loss,\ndata-augmentation, performance, learning-rate,\nhyperparameters, initialization, imbalanced-data, nan\nGPU\ngpu, nvidia, cuda\nAPI\ntypeerror, valueerror, attributeerror, importerror,\ncompilererrors, syntaxerror, modulenotfounderror\nTable 3: Summary of the constructed dataset\nType of Bugs\nTotal Number of Bugs\nModel\n113\nTraining\n95\nGPU\n101\nAPI\n24\nTensor and Input\n193\nMixed\n42\nTotal\n568\n– Code Editors: We use Visual Studio Code v1.79.0 2 and PyCharm 2023.1.1 3\nto execute code snippets and reproduce bugs from our dataset. Visual Stu-\ndio Code and PyCharm are popular code editors for building DL-based\napplications [37].\n– Dependencies: To detect the API libraries adopted by the code snippets,\nwe use the pipreqs package4. We also install the dependencies for each bug\ninto a separate virtual environment using the venv 5 module.\n– Frameworks: Since our dataset contains bugs from Tensorflow, Keras,\nand PyTorch, we used all three frameworks in our experiments.\n– Libraries: For generating the random inputs and visualizing the training\nmetrics, we leveraged several scientific computing libraries such as numpy,\npandas, and matplotlib.\n– Python Version: During our experimentation, we used Python v3.10 to\nreproduce the deep learning bugs, the latest stable version. If the Stack\nOverflow issue reported a Python version other than v3.10, we reproduce\nthe bug using the mentioned version instead.\n– Hardware Config: Our experiments were run on a desktop computer\nhaving a 64-bit Windows 11 Operating System with 16GB primary memory\n(i.e., RAM) and 8GB GPU Memory (Intel(R) Iris XE Graphics).\n2 https://code.visualstudio.com/\n3 https://www.jetbrains.com/pycharm/\n4 https://pypi.org/project/pipreqs/\n5 https://docs.python.org/3/library/venv.html\nTitle Suppressed Due to Excessive Length\n13\n4.4 Qualitative Analysis\nBefore conducting any qualitative analysis, we first determine if each post\ntargets a deep learning bug. If a post is not related to a deep learning bug,\nwe exclude it from further analysis. We also excluded the posts not reporting\nthe evaluation metrics of the buggy model from our study. Without these\nmetrics, it was impossible to determine if a bug was successfully reproduced\nor not. This two-step filtering process helped our analysis focus on relevant\nand measurable deep-learning bug reports.\nOnce we confirm that the post is relevant to a DL bug, we check if each\nselected post accurately represents the type of bug it is assigned to. To validate\nthe bug categorization, we manually analyze each selected Stack Overflow post\nand review the issue description, stack trace, observed behaviour, expected\nbehaviour, and accepted answer. This ensures that the tag-based selection of\nposts from Stack Overflow does not affect our results’ validity. We perform this\nmanual verification since the Stack Overflow posts might lack an independently\nverified category label. In contrast, the GitHub issues in the benchmark dataset\nby Morovati et al. [12] already contain a validated category label.\n4.4.1 Reproducing Deep Learning Bugs from Stack Overflow\nOnce we confirm the category of each post or issue, we follow a two-step ap-\nproach to reproduce the deep-learning bugs from Stack Overflow posts. First,\nwe gather complementary information about each bug, such as the dataset,\ncode snippet, library versions, and the framework used. Second, we attempt to\nreproduce the bug using the code snippet and supporting data (e.g., dataset\ninformation, environment configurations, hyperparameters, and training logs).\n4.4.2 Reproducing Bugs from Github Issues\nTo reproduce the bugs from GitHub issues, we employed a systematic ap-\nproach. First, we located the bug-inducing commit of a bug using the commit\nID provided by the benchmark. Next, we cloned the corresponding repository\nand checked out the buggy version of the code using the commit ID. We then\nset up the development environment according to the reported bug, which\nmay include installing the required version of the programming language and\nnecessary dependencies or libraries, configuring environment variables and set-\nting up any necessary databases, services, or external dependencies the code\nrequires. If applicable, we applied necessary edit actions to the code, such as\nmodifying specific lines or adding/removing code snippets, to trigger the buggy\nbehaviour. We then ran the updated code snippet or the specific part of the\ncodebase where the bug was expected to occur. To verify the presence of a bug,\nwe compared the observed behaviour with the reported buggy behaviour and\nran the test cases from the existing benchmark (Defects4ML). Finally, if the\nbug was reproduced, we recorded the edit actions used and critical information\nnecessary for the bug reproduction.\n14\nMehil B. Shah et al.\n4.4.3 Agreement Analysis\nThe first author and one independent collaborator conducted the bug repro-\nduction process. We first reproduced 10 bugs from our dataset and achieved\na Cohen Kappa of 54.5%. Then, we had two meetings to identify the main\nreasons for our disagreements and resolved them. In the next round, we repro-\nduced 10 more bugs and achieved a Cohen Kappa of 89.1%, which is considered\nan almost perfect agreement [38]. After achieving an almost perfect agreement,\nthe first author reproduced the remaining bugs (i.e., 128) while the indepen-\ndent collaborator checked the reproduction, achieving an average Cohen Kappa\nof 85.4%. If the first author fails to reproduce the bug within 60 minutes, the\nindependent collaborator also attempts to reproduce the bug. If both of them\nfailed to reproduce the bug, the bug was marked as irreproducible. We spent\n≈280 person-hours on the manual bug reproduction process.\n4.5 Verification of Bug Reproduction\nTo verify the successful reproduction of bugs, we employed different strategies\ndepending on the nature of the bugs.\n4.5.1 Explicit Bugs\nAn explicit bug results in an error message or exception. To verify the repro-\nduction of this bug, we adopted a straightforward approach. We extracted the\nerror message from the bug report and attempted to reproduce the bug under\nthe reported conditions. We considered the bug reproduction successful when\nthe observed error message matched the error message in the bug report.\n4.5.2 Silent Bugs\nSilent bugs are also referred to as functional or numerical errors. They do not\nresult in system crashes or hangs and do not display error messages, but they\nlead to incorrect behaviour [39]. We adopted a comprehensive approach to\nverify the reproduction of silent bugs, as they often manifest subtly through\nsilent issues like slow training or low accuracy.\nWe reproduced the silent bugs from Stack Overflow posts and GitHub\nissues. Subsequently, we used the evaluation metrics reported in the original\nposts or issues as the ground truth to verify the buggy behaviour. In cases\nwhere the code snippet was incomplete, we applied our edit actions to make\nit compilable, executable, and runnable.\nTo determine if a bug was successfully reproduced, we followed these steps:\n– We executed the modified code snippet five times, each time with a different\nrandom seed, and calculated the average evaluation metric across these\nruns.\nTitle Suppressed Due to Excessive Length\n15\n– We compared the average evaluation metric to the reported evaluation\nmetric of the buggy model.\n– If the average evaluation metric was within a 5% error margin of the re-\nported metric, we considered the bug to be reproduced.\nWe selected the 5% threshold for error margin based on the existing litera-\nture [40, 41]. Pham et al.[40] found that implementation-level non-determinism\ncould account for ≈3% variance in the training and evaluation metrics of DL\nmodels, often caused by factors such as parallel processing issues, automatic\nselection of primitive operations, task scheduling, and floating-point precision\ndifferences. Furthermore, Alahmari et al.[41] demonstrated that the variance\nin evaluation metrics could vary from 3% to 7% for models trained using the\nsame dataset and code. Thus, our 5% threshold accounts for the inherent\nvariability in deep-learning models while still maintaining a reasonable stan-\ndard for bug reproduction. This systematic approach allows us to verify the\nsuccessful reproduction of silent bugs, ensuring the reliability of our findings.\n4.5.3 Information Collection During Verification\nFollowing the successful reproduction of any bug above, we capture various\ninformation related to each bug, such as the deep learning architecture in-\nvolved, edit actions used to reproduce the bug, time taken to reproduce the\nbug, type of bug reproduced, and the type of information present in the bug\nreport. All these data gathered during bug reproduction helped us identify key\nedit actions and information components to reproduce specific types of deep\nlearning bugs (Step 4, Fig. 3).\n4.6 Identifying Type Specific Information and Edit Actions\n4.6.1 Algorithm Selection\nTo establish a relationship among the bug types, component information, and\nthe key editing actions necessary for bug reproduction, we use the Apriori [17]\nalgorithm. Apriori is a well-known algorithm for mining frequent itemsets from\na list of transactions. It helps one identify common patterns and associations\nbetween different elements.\nThe Apriori algorithm exploits the principle that if an itemset is frequent\nacross the transactions, then all of its subsets must also be frequent. It starts\nby identifying frequent individual items in the dataset and extends them to\nlarger and larger itemsets as long as they meet certain constraints (e.g., sup-\nport threshold). The algorithm terminates when no further successful exten-\nsions are found. More specifically, the Apriori algorithm consists of two main\nsteps: the join step and the prune step. In the join step, the algorithm generates\nnew candidate itemsets by joining the frequent itemsets found in the previous\niteration. In the prune step, the algorithm checks the support count of each\ncandidate itemset and discards the itemsets that do not meet the minimum\n16\nMehil B. Shah et al.\nsupport threshold. This process is repeated until no more frequent itemsets\nare generated. In our context, we apply the Apriori algorithm to analyze the\ninformation gathered during bug reproduction. Our goal was to determine the\nfrequent combinations of bug types and component information, as well as edit\nactions during bug reproduction. We leveraged the Apriori algorithm’s system-\natic pattern-mining capabilities to establish these relationships. By identifying\nthe frequent itemsets, we can gain insights into the common patterns and as-\nsociations among bug types, component information, and edit actions, which\ncan help us understand and improve the bug reproduction process.\n4.6.2 Generating the Transactions\nTo create our datasets for the Apriori algorithm, we employed a character\nencoding, where we encoded all labels into a unique character (e.g., ‘Training\nBug’ was encoded as ‘T’, ‘Model Bug’ was encoded as ‘M’, ‘Obsolete Parameter\nRemoval’ was encoded as ‘O’) and converted the data into transactions using\nthe following format.\nBug Type →Information Category\nExample: T →DH\nDescription: The transaction above indicates that the reproduced bug is a\ntraining bug (T). The corresponding bug report contains useful information\nabout the bug, such as the dataset used for training (D) and hyperparameters\nused by the model (H).\nBug Type →Edit Action\nExample: M →OLN\nDescription: The transaction above indicates that the reproduced bug is a\nmodel bug (M). To reproduce the bug, we performed three edit actions, as\ndescribed below:\n– Obsolete Parameter Removal (O): We removed some of the parameters\nthat were absent in the recent library and framework version to ensure\nthat the code compiles.\n– Logging (L): We logged various intermediate program states to verify the\nbug’s presence.\n– Neural Network Definition (N): We reconstructed the neural network based\non the information provided in the bug report.\nFollowing the specified format, we created two datasets of transactions that\nassociate bug types with crucial information and edit actions, respectively.\nAfter creating the transactions, we use the Apriori algorithm to compute the\nsupport and confidence for our generated itemsets and association rules. We\ntalk about these metrics in detail below.\nTitle Suppressed Due to Excessive Length\n17\n4.6.3 Metrics for Apriori Algorithm\nSupport is the proportion of transactions in the dataset that contain a particu-\nlar itemset. Mathematically, the support of an itemset X is defined as the ratio\nof the number of transactions containing X to the total number of transactions.\nIt is expressed as:\nSupport(X) =\nTransactions containing X\nTotal number of transactions\nFor a rule X ⇒Y , where X and Y are two itemsets, the support is calcu-\nlated for the combined itemset X ∪Y .\nConfidence measures the likelihood that a rule X ⇒Y holds. It is defined\nas the ratio of the support of the combined itemset X ∪Y to the support of\nthe antecedent itemset X. Mathematically, confidence is expressed as:\nConfidence(X ⇒Y ) = Support(X ∪Y )\nSupport(X)\nConfidence values range from 0 to 1. A high confidence value indicates a\nstrong association between antecedents and consequent itemsets.\n4.6.4 Definitions for Apriori Algorithm\n– Itemset: An itemset is a set of one or more items. In our context, an item\ncan be a bug type, an information category, or an edit action. For example,\n{T, D, H} is an itemset containing three items: bug type T (training bug),\ninformation category D (dataset), and information category H (hyperpa-\nrameters).\n– Transaction: A transaction is a record that contains one or more items.\nIn our study, we have two types of transactions as follows:\n– Bug Type →Information Category: These transactions associate a bug\ntype with the useful component information for reproducing that bug.\n– Bug Type →Edit Action: These transactions associate a bug type with\nthe edit actions performed to reproduce that bug.\n– Rule: A rule is an implication of the form X ⇒Y , where X and Y are\nitemsets. It suggests that if itemset X is present in a transaction, then\nitemset Y is likely to be present as well. In our study, rules are gener-\nated from the transactions to establish associations between bug types and\ncomponent information or edit actions.\n4.6.5 Association Rule Generation\nWe conducted two separate association rule mining operations in our study -\none focused on component information while the other focused on edit actions\nused to reproduce deep learning bugs. The Apriori algorithm generates rules\nby first identifying frequent itemsets and then creating rules from them. The\nsteps below explain the process of rule generation with an example.\n18\nMehil B. Shah et al.\n– Identify frequent itemsets: The algorithm scans the transactions to\nfind itemsets that occur frequently while satisfying the minimum support\nthreshold. For example, if the item T, D appears in 20% of the transactions\nand the minimum support threshold is 10%, it is considered a frequent item.\n– Generate rules: Once frequent itemsets are identified, the algorithm gen-\nerates rules from them. For each frequent itemset, the algorithm creates\nrules by splitting the itemset into antecedent (left-hand side) and conse-\nquent (right-hand side). For example, from the item T, D, H, the following\nrules can be generated:\n– T ⇒D, H\n– D ⇒T, H\n– H ⇒T, D\n– T, D ⇒H\n– T, H ⇒D\n– D, H ⇒T\n– Calculate confidence: For each generated rule, the algorithm calculates\nthe confidence value. Confidence measures the likelihood that the conse-\nquent itemset appears in a transaction given that the antecedent itemset\nis present. Rules with confidence values above a minimum threshold are\nconsidered strong associations.\nIn particular, we extracted 27 itemsets and 34 rules for component informa-\ntion, highlighting the useful information for reproducing deep learning bugs.\nSimilarly, we extracted 126 itemsets and 284 rules for edit actions, captur-\ning the edit actions needed to reproduce bugs. We generated association rules\nbased on the entries in our dataset and did not filter or remove any rules before\ndetermining confidence values. We then calculate the confidence values for all\ngenerated rules to identify the most influential ones for connecting bug types\nwith edit actions and useful information.\n4.6.6 Computation of Confidence Values for the Generated Association Rules\nAs discussed earlier, support indicates how frequently a rule occurs, while\nconfidence indicates the generality of the rule. To compute the confidence\nvalues for each association rule, we performed the following steps:\n– Calculate Support for Antecedent (X): We calculate the support for\nthe antecedent, which is the bug type in our case (e.g., ‘T’ for Training Bug\nor ‘M’ for Model Bug). Support for X is the proportion of all transactions\nthat contain the specific bug type.\n– Calculate Support for Combined Itemset (X ∪Y ): We then calculate\nthe support for the combined itemset, X ∪Y , which includes both the bug\ntype and the information category or edit action (e.g., ‘T ∪H’ for Training\nBug associated with Hyperparameter Information, or ‘M ∪O’ for Model\nBug associated with Obsolete Parameter Removal).\n– Compute Confidence for the Rule (X ⇒Y ): The confidence of the\nrule X ⇒Y is computed by dividing the support of the combined itemset\nTitle Suppressed Due to Excessive Length\n19\nX∪Y by the support of the antecedent X. This step gives us the confidence\nvalue, which indicates how often the information category or edit action Y\nis associated with the bug type X in our transactions.\nFor example, consider the rule T ⇒D. This rule indicates that if the type of\nbug is a ‘Training Bug’ (T), it can be reproduced by the edit action ‘Input\nData Generation’ (D). To calculate the confidence of this rule, we count the\nnumber of transactions in which the training bug is reproduced by using the\nedit action ‘Input Data Generation’ and divide this count by the total number\nof transactions involving training bugs in the dataset.\n4.6.7 Identification of High Confidence Associations\nWe use high confidence and support values to detect the rules that reliably\ncapture the core factors necessary to reproduce specific types of deep learn-\ning bugs. Based on these high-confidence rules, we identify the top 3 pieces\nof useful information and the top 5 edit actions used to reproduce each bug\ntype. The decision to select three useful pieces of information and five edit\nactions was influenced by two key factors. First, we adhere to the Parsimony\nPrinciple [42], which suggests that selecting the simplest set of rules is prefer-\nable when multiple rules can predict or describe the same phenomenon. We\nthus concentrate on the most significant factors by selecting the top 3 and top\n5 rules for edit actions and useful information, respectively. Second, we filter\nthe rules based on minimum confidence values of 30%, as suggested by Liu\net al. [43]. With our limited dataset, a 30% confidence threshold can indicate\na substantial pattern since finding associations in 30% of cases points to a\nmeaningful correlation given the data size. Furthermore, the 30% minimum\nconfidence helps filter out spurious correlations with small datasets that can\noccur by chance. Therefore, for our dataset, the selected threshold strikes an\neffective balance - it is high enough to identify meaningful associations in the\ndata while eliminating noise from false correlations. Overall, this filtration left\nus with 23 rules for edit actions and 20 for useful information. Focusing on\nthese high-confidence, high-support rules can reveal the patterns that repro-\nduce deep learning bugs (Step 5a, 5b, Fig. 3).\n4.7 User Study\nTo assess the benefits and implications of our findings in a real-life setting, we\nconduct a user study involving 22 developers (10 from academia + 12 from\nindustry) (Step 6, Fig. 3). We discuss our study setup, including instrument\ndesign and participation selection, as follows:\nInstrument Design: We used Opinio, an online survey tool recommended\nby our institution, to construct and distribute our questionnaire. Opinio en-\nabled us to track the time spent by the participants on each individual ques-\ntion, which proved to be useful for our further analysis. The use of Opinio\nalso did not require any additional effort from the participants, which made it\n20\nMehil B. Shah et al.\na suitable choice for our user study. We divided our questionnaire into three\nsections. We discuss them in detail below:\n– Introduction: We first summarize our findings on the reproducibility of\ndeep learning bugs to provide the participants with the necessary context\nand background information. For the survey itself, we do not give the\nrespondents a fixed time to complete it, but we specify that it should take\n≈60 minutes on average; this number was derived from our pilot study. This\nwas done to ensure that the respondents do not work under time pressure.\nSince the participants have different levels of experience, allocating a fixed\ntime for bug reproduction might affect our results.\n– Demographic Information: After providing the contextual information\nabout our study, we collect demographic information from developers (e.g.,\nexperience bug fixing in deep learning frameworks). We then ask the de-\nvelopers to elaborate on the challenges that they face when reproducing\ndeep learning bugs in their daily lives.\n– Questionnaire Preparation: First, we select eight bugs (2 Tensor, 2\nAPI, 2 Model, and 2 Training Bugs) from our dataset constructed during\nmanual bug reproduction and dataset creation. During this process, we\ncategorized the bugs by difficulty level and type based on the number of\nedit actions and critical information required to reproduce them. We use\nstratified random sampling to pick 2 bugs from each type; one of the bug\ntypes is relatively easy to reproduce with only 1 edit action, and the other\ntype is relatively difficult to reproduce warranting multiple edit actions.\nWe pick 8 bugs following this approach and, then randomly assign them\nto four sets, each containing 1 easy and 1 difficult bug. Second, we provide\nthe users with the issue description and the code snippet from the original\nStack Overflow post. We also provide a Google Colaboratory notebook\ncontaining the code for sample edit operations to aid the bug reproduction\nprocess. Finally, we include a free-text box in the form to allow users to\nshare any additional information about edit actions not covered in our\nstudy.\nStudy Session: During our study session, each participant completes the\nfollowing five tasks. First, the participant provides their demographic informa-\ntion. Second, the participant explains their daily challenges when reproducing\ndeep learning bugs. Third, each participant reproduces two deep learning bugs\nand self-reports the edit actions and information they used to reproduce the\nbugs. Fourth, the participant also provides the rationale behind their self-\nreported edit actions and component information used to reproduce the bugs.\nFinally, the participant provides information about any other edit actions that\nthey might have used to reproduce the bugs but are not covered in our study.\nParticipant Selection: We first conducted a pilot study with two re-\nsearchers and two developers. Based on their feedback, we rephrased ambigu-\nous questions and added sample code to aid the manual bug reproduction by\nthe users. Incorporating this constructive feedback enabled us to refine and\nimprove the quality of our final questionnaire. Then, we invite professional de-\nTitle Suppressed Due to Excessive Length\n21\nvelopers and researchers with relevant deep learning experience to our study.\nWe send our invitations to the potential participants using direct correspon-\ndences, organization mailing lists (e.g., Mozilla Firefox), and public forums\n(e.g., LinkedIn and Twitter). A total of 22 participants responded to our invi-\ntations. Out of them, 10 (45%) came from academia, and 12 (55%) came from\nindustry. In terms of bug-fixing experience with deep learning bugs, 14 par-\nticipants (63.63%) had 1-5 years of experience, 4 (18.18%) had 5-10 years of\nexperience, and the remaining 4 (18.18%) had less than one year of experience.\nIn terms of deep learning frameworks, 19 participants (86.34%) reported hav-\ning working experience with Tensorflow, 21 (95.45%) reported experience with\nPyTorch, and 20 (90.91%) reported experience with Keras. All these statistics\nindicate a high level of cross-framework expertise within our participants.\nDefining Control and Experimental Groups: We carefully divided the\nstudy participants into control and experiment groups, as shown in Table. 4.\nUsing these two groups, we wanted to assess the benefit of our recommended\ninformation in the context of bug reproduction. The control group receives\nno hints about how to reproduce a bug. In contrast, the experimental group\nreceives hints (e.g., useful information, edit actions) that could help them\nreproduce a bug.\nEnsuring Similar Experience Levels: Our guiding principle was to\nensure that both groups had a similar distribution in their relevant experience.\nSpecifically, we surveyed all the developers about their experience and used a\nstratified random sampling approach to assign them to the two groups. This\nrandomization allowed us to minimize potential bias and confounding factors\nacross the groups.\nTable 4: Distribution of participants in the control and experimental groups\nDL Experience\nDevelopers in Control Group\nDevelopers in Experimental Group\n<1 Year\n3 (27.27%)\n3 (27.27%)\n1-5 Years\n6 (54.54%)\n5 (45.45%)\n5-10 Years\n2 (18.18%)\n3 (27.27%)\nLeveraging Associations to Produce Hints for User Study: We use\nhigh-confidence associations from our RQ2 (Step 3.6.6) to develop hints for our\nuser study. These associations revealed the edit actions and critical information\nneeded to reproduce specific types of bugs. We use the steps below to construct\nthe hints in a systematic way for our user study.\n1. Bug Categorization:\n(a) Analyze the information found in a bug report to identify the specific\ntype or category of the bug.\n(b) Use the identified bug category to retrieve relevant component infor-\nmation and edit actions.\n2. Retrieve component information and edit actions:\n22\nMehil B. Shah et al.\n(a) Based on the identified bug category, retrieve the top 3 components\nand top 5 edit actions from the findings of RQ2.\n(b) The component information includes the most important aspects re-\nquired to reproduce or understand a bug (e.g., shape of input data,\ntraining code, error messages).\n(c) The edit actions refer to the most frequently associated actions to a\nspecific bug category.\n3. Determining the Most Relevant Statement: We also collect the most\nrelevant statement from the bug description as follows.\n(a) Split the bug report’s text into a list of sentences using ’.’ as the delim-\niter.\n(b) Collect a list of key phrases for the components retrieved in Step 2(a).\nThese key phrases are derived from our qualitative analysis of the bug\nreports and can be found in the replication package [35].\n(c) Generate the Sentence-BERT embeddings for every statement in the\nbug report and the keywords above using the ’sentence-transformers-\n/all-MiniLM-L6-v2’ pre-trained model.\n(d) For each statement in the bug report, calculate the cosine similarity\nbetween the statement embedding and the embeddings of the keywords.\n(e) Identify the relevant statement for the most prevalent component (i.e.,\ntop component from Step 2(a)) based on their cosine similarity score.\n(f) Prepare a context-specific hint that guides the user to a particular state-\nment using the following template: ‘Focus on the statement: <Statement\nextracted in Step 3(e)>’\n4. Hint Formulation:\n(a) Combine the retrieved component information and edit actions from\nthe research findings with the context-specific hint to formulate the\ncomplete hint for the user study.\n(b) Use the template below to formulate the hint for each bug.\nTemplate for Hint Generation\nHints\n1. <CI1>, <CI2>, <CI3> can be useful information for reproducing\nthe bug.\n2. <EA1>, <EA2>, <EA3>, <EA4>, <EA5> can be useful edit actions\nfor reproducing the bug.\n3. Focus on the Statement: “<Most Relevant Statement from the\nSO Post>”\nWe ensure a systematic formulation of hints for our study by following the steps\nabove. This approach incorporates high-confidence associations from Step 3.6.6\nand the bug description for individual bug. With this intervention, we plan\nto measure if and how our recommended information help participants repro-\nduce the bugs more accurately or quickly. The code and results for the hint\nformulation are available in our replication package [35].\nTitle Suppressed Due to Excessive Length\n23\nTable 5: Summary of the Reproduced Bugs\nType of Bug\nBugs Reproduced\nModel Architectures Covered\nTraining (T)\n50\nCNN, LSTM, AutoEncoder, MLP, RCNN,\nResNet\nModel (M)\n42\nBERT, CNN, GMM, LSTM, MLP\nVGG16, Transformers\nAPI (A)\n20\nCNN, GAN, MLP, Transformers, VGG19,\nVariational RNN\nGPU (G)\n3\n-\nTensor and Input (I)\n29\nCNN, GAN, Logistic Regression, MLP, ResNet\nMixed (X)\n4\nCNN, BERT, MLP\n4.8 User Study Results Analysis\nTo assess the effectiveness of our edit actions, we analyze the participants’\nresponses and qualitative feedback. Specifically, we analyze the edit actions\nused by the participants and compare them with those used in our manual\nbug reproduction process. If similar edit actions were used, it would indicate\nthat our recommended edit actions and component information were effective\nin the reproduction of the bugs.\nIn our user study, the participants first reproduce their assigned bugs and\nreport their used information or actions to reproduce their bugs. To analyze\nthe effectiveness of our recommended information, we compare the control and\nexperimental groups in terms of their success rates in bug reproduction and the\ntime taken to reproduce the bugs. We select bug reproducibility rate and time\ntaken for bug reproduction as our key metrics to evaluate the effectiveness\nof our findings. These quantitative metrics directly measure how successful\nand efficient our recommended information is in assisting the developers to\nreproduce deep learning bugs. Higher reproducibility rates and reduced repro-\nduction times in the experimental group compared to the control group would\nindicate that our findings are effective for improving deep learning bug repro-\nducibility. We also analyze the qualitative feedback from the participants to\ndetermine if our provided hints were useful or not. We also analyze them to\nuncover new insights into deep learning bug and their reproduction.\nBy analyzing the user study results and the developers’ qualitative feed-\nback, we gain new and valuable insights into the reproducibility of deep learn-\ning bugs. The user study results and feedback also provide information about\nthe effectiveness of our recommended actions for bug reproduction. This en-\nables us to deliver novel and actionable insights regarding the current state of\ndeep learning bug reproducibility based on empirical evidence (Step 7, Fig. 3).\n5 Study Findings\nIn this section, we present the findings of our study by answering three research\nquestions as follows.\n24\nMehil B. Shah et al.\n5.1 RQ1: Which edit actions are crucial for reproducing deep learning bugs?\nTo answer the first research question, we worked with 165 bugs, and repro-\nduced 148 bugs of different types and architectures. Table. 5 briefly summa-\nrizes our reproduced bugs. Through our comprehensive reproduction process,\nwe identify ten edit actions that are crucial for reproducing deep learning bugs.\nTable. 6 shows the identified actions from our qualitative analysis. We explain\nthese actions as follows.\nInput Data Generation (A1) is one of the key edit actions for repro-\nducing deep learning bugs. This action involves programmatically generating\nsynthetic input data that closely matches the characteristics of the original\ndata used for training the model. The key objective of input data generation\nis to simulate representative data that can trigger or reproduce the erroneous\nmodel behavior described in the bug report. This allows for the reproduction\nof issues that manifest only in the presence of specific data properties or dis-\ntributions. To perform input data generation, we leverage any details about\nthe data that are provided in the bug report, such as data types, value ranges,\nshapes, distributions, preprocessing steps, etc. For example, for image data, the\nreport may specify that inputs are RGB images of size 224x224x3 with pixel\nvalues normalized to [0,1]. Similarly, for text data, the description may indi-\ncate sequences of 512 tokens processed using a particular tokenizer. Using this\ninformation, we can systematically generate synthetic data matching the prop-\nerties through appropriate library functions. For images, we can use libraries\nlike OpenCV [44] or PIL [45] to construct random images of the required size\nand channels. For text, we can sample token sequences from a standard corpus\nor use specialized generative models like GPT-2 [46]. Our manual bug repro-\nduction shows that ≈73% of our collected posts from Stack Overflow have the\nrelevant data characteristics. Let us consider the issue reported in the Stack\nOverflow post (Issue #61781193). In this post, the reporter suspects that the\nmodel is not learning - as evidenced by the constant training loss across the\nepochs. The following text from the post shows how the reporter might submit\nthe input data distribution.\nR: My training data has input as a sequence of 80 numbers in which each\nrepresent a word and target value is just a number between 1 and 3.\nUsing this information, we generated the random input data as follows and\nwere able to reproduce the corresponding bug.\ntrain_data = torch.utils.data. TensorDataset (torch.randint (0, 200, (1000 ,\n80)), torch.randint (1, 3, (1000 ,)))\nNeural Network Construction (A2) was one of the most used edit ac-\ntions during our bug reproduction. In this edit action, we construct a neural\nnetwork based on the architecture provided by the reporter. Similar to the\ndata characteristics, the information about the neural network is present in\n≈65% of the reproducible issue reports. Using the neural network description\nfrom the issue reports, we were able to construct the models. Let us consider\nthe issue reported in the Stack Overflow post (Issue #63204176). In this post,\nTitle Suppressed Due to Excessive Length\n25\nTable 6: Edit Actions for Reproducing Deep Learning Bugs\nEdit Action\nOverview\nA1: Input Data Generation (D)\nGenerating input data that simulates\nthe data used for training the model.\nA2: Neural Network Construction (N)\nReconstructing or modifying the neural\nnetwork based on the information pro-\nvided.\nA3: Hyperparameter Initialization (H)\nInitializing\nthe\nhyperparameters\nfor\ntraining, such as batch size and num-\nber of epochs.\nA4: Import Addition and Dependency Resolu-\ntion (R)\nDetermining the dependencies in the\ncode snippet and adding the missing\nimport statements.\nA5: Logging (L)\nAdding appropriate logging statements\nto capture relevant information during\nreproduction.\nA6: Obsolete Parameter Removal (O)\nRemoving\noutdated\nparameters\nor\nfunctions to match the parameters of\nthe latest library versions.\nA7: Compiler Error Resolution (C)\nDebugging and resolving compiler er-\nrors that arise due to syntactic errors\nin the provided code snippet.\nA8: Dataset Procurement (P)\nAcquiring the necessary datasets and\nusing them to train the model.\nA9: Downloading Models & Tokenizers (M)\nFetching pre-trained models and tok-\nenizers from external sources.\nA10: Version Migration (V)\nUpdating code to adapt to changes in-\ntroduced in newer versions of libraries\nand frameworks.\nthe reporter submits an issue where a CrossEntropy loss function within a\nloop is overwritten with a Tensor, causing a TypeError in later iterations.\nWhen we analyze the post, we find that the reporter mentions that they have\nused a logistic regression model (1-layer neural network with a sigmoid acti-\nvation function [47]). However, the reporter does not provide the code snippet\nnecessary for reproducing the bug.\nR: I am trying to write a simple multinomial logistic regression using mnist\ndata.\nTo reproduce the bug, we constructed multiple logistic regression mod-\nels for the MNIST dataset, as highlighted in the post. We added the import\nstatements, wrote the code to load the MNIST dataset, and completed the\ncode. Using the training loop’s code snippet from the original code and our\nedit actions, we could complete the code snippet and reproduce the bug suc-\ncessfully. Using this partial information, we constructed multiple multinomial\nlogistic regression models, and despite the lack of relevant code examples, we\nsuccessfully reproduced the corresponding bug.\nHyperparameter Initialization (A3) is one of the core edit actions for\nreproducing deep learning bugs. As a part of this edit action, we initialize\nvarious hyperparameters (e.g., number of epochs, batch size, optimizer) for\n26\nMehil B. Shah et al.\ntraining the neural networks. Sometimes, the reporter did not provide these\nconfigurations, and we had to initialize them using default parameters to re-\nproduce the bug. This is further evidenced by the fact that only ≈53% of\nour collected posts from Stack Overflow have information about the hyper-\nparameters. Let us consider the issue reported in the Stack Overflow post\n(Issue #31880720), where the author gets a poor test accuracy of 13.9% when\ntraining a neural network on a synthetic binary classification dataset. After\nmanual analysis of the post and the code snippet, we observe that some of the\nhyperparameters in the code snippet have not been initialized.\nmodel.fit(X_train , Y_train , batch_size=batch_size , nb_epoch=nb_epoch ,\nshow_accuracy =True , verbose =2,\nvalidation_data =( X_test , Y_test))\nTo reproduce the bug, we initialized the batch size with commonly-used\nvalues {32, 64, 128} and the number of epochs ranging from 1 to 10. Since we\ninitialized these hyperparameters, we ran the edited code snippet five times\nto confirm the effectiveness of our edit operation. We were able to reproduce\nthe bug in all five iterations. We performed the steps mentioned above for all\n56 bugs involving hyperparameter initialization.\nImport Addition and Dependency Resolution (A4) are one of the\nmost common edit actions for all types of bugs. In this edit action, we analyze\nthe code manually and determine the dependencies required for the code snip-\npet to run. Then, we install the dependencies and manually import them to\ncomplete the code snippet. This edit action was also used to reproduce tradi-\ntional software bugs, as reported by Mondal et al. [13]. In the Stack Overflow\nposts we collected, ≈47% of the issue reports lacked the required dependency\ninformation. This significant absence of the dependency information further\nhighlights the need for our suggested edit action. Let us consider the issue re-\nported in the Stack Overflow post (Issue #50306988). In this post, the neural\nnetwork model with softmax activation struggles to fit a simple 2-feature clas-\nsification dataset, converging extremely slowly compared to logistic regression,\nwhich achieves 100% accuracy. However, the code snippet reported in the post\nwas incomplete, and the dependency details were missing. Hence, we resolved\nthe dependencies (keras, numpy and random) and added the required import\nstatements to the code snippet. With the resolved dependencies, we were able\nto reproduce the bug successfully.\nLogging (A5) plays a crucial role in the reproduction of different types of\nbugs. This action involves adding log statements to the provided code snippet\nto verify the reproduction of a bug. Let us consider the issue reported in the\nStack Overflow question (Issue #70546468). The reporter provides a numpy\narray with shape (1, 3). The reporter then vertically concatenates (stack depth-\nwise) multiple copies of this array to create tensor groups with shapes (2, 3)\nand (1, 3). After this vertical concatenation process, the reporter expects the\ntensor’s shape to be (2, None, 3). Unfortunately, the tensor’s shape was (2,\nNone, None), according to the reporter. To verify the claim made by the re-\nporter, we introduce a log statement within the code snippet. When we run\nTitle Suppressed Due to Excessive Length\n27\nthe modified code snippet, we observe the shape of the stacked tensor to be\n(2, None, None), thereby confirming the reproduction and validity of the bug.\nObsolete Parameter Removal (A6) is a crucial edit action that en-\nhances bug reproducibility when differences in library and framework versions\ncause compatibility issues. It involves removing obsolete parameters no longer\nsupported by newer versions. Frequent updates to deep learning frameworks\nand APIs can cause breaking changes, i.e., the code written for older versions\nof the framework is incompatible with newer versions. Developers reproduc-\ning these bugs often face challenges when the bug report’s environment is\nsignificantly older than their current working environment, and downgrading\nto match those outdated versions is not always feasible due to various con-\nstraints. In some cases, the bug report uses significantly outdated versions\nthat are no longer supported or maintained by the framework developers. Ad-\nditionally, the developer’s current project may have dependencies that require\nnewer versions of Python or the frameworks, making it impractical to down-\ngrade. Furthermore, organizations or development teams may have policies in\nplace that mandate using the latest stable versions for security, performance,\nand maintainability reasons, preventing the use of older versions.\nIn such cases, developers must port the bug-reproducing code to their\ncurrent environment, where Obsolete Parameter Removal edit action proves\nhighly beneficial. It makes the code compatible with newer framework versions\nwhile preserving the bug-reproducing behaviour. Consider the issue reported\nin the Stack Overflow post (Issue #65992364). In this post, the reporter at-\ntempts to optimize an object detection model using ‘pytorch-mobile’, but the\ncode snippet fails to optimize the model file size. The code snippet in the issue\nalso contains the following line of code.\nscript_model_vulkan = optimize_for_mobile (script_model , backend=‘‘Vulkan\")\nHowever, according to the API documentation of PyTorch 1.6.0 [48], the\nbackend parameter was no longer supported. We removed the obsolete pa-\nrameter and thus were able to reproduce the bug.\nCompiler Error Resolution (A7) is one of the extensively employed edit\nactions in reproducing bugs. In this edit action, we resolve compiler errors to\nget the code running and reproduce the bug. While downgrading the compiler\nor library versions can sometimes resolve errors or enable the bug reproduction\nwith deprecated functionality, it may not always be feasible. For example,\nmajor framework releases often remove support for older versions, making\ndowngrading impossible. Additionally, Python version mismatches between\nthe original buggy code and the current development environment can prevent\nsuccessful downgrading. In cases where downgrading is infeasible due to such\nconstraints, correction of compiler errors enables reproducing deep learning\nbugs despite incompatible environments.\nFor example, in the Stack Overflow post (Issue #71514447), the reporter\nstates that the training loss is significantly increasing after every epoch. They\nsuspect that the bug might be due to the incorrect computation of loss values.\nFurthermore, to help the developers reproduce the bug, they provide a detailed\n28\nMehil B. Shah et al.\ndescription of the bug and a complementary code snippet. However, the code\nsnippet provided by the user does not compile, as the criterion function has\nnot been defined. Hence, we replaced the function parameter with the default\nvalue of cross-entropy loss, as shown below.\ndef\ntrain_model (model , optimizer , train_loader ,\nnum_epochs , criterion = nn\n. CrossEntropyLoss ()):\nUsing the edit action above, we resolved the compiler errors and thus were\nable to reproduce the corresponding bug.\nDataset Procurement (A8) is a critical edit action to reproduce bugs\nthat require specific datasets. In this edit action, we analyze the issue report\nand attempt to procure the dataset mentioned in the report. For instance, in\nthe Stack Overflow post (Issue #73966797), the reporter mentioned that they\nused the CIFAR-10, a well-known dataset for object detection. To reproduce\nthe bug, we downloaded the dataset from its original source6, and using the\ndataset and the training code, we could reproduce the bug.\nDownloading Models & Tokenizers (A9) is one of the edit actions\nthat was frequently used to reproduce bugs in Large Language Models and\nTransformer-based architectures. In this edit action, we download the pre-\ntrained models and tokenizers for reproducing a bug. Let us consider the\nissue reported in the Stack Overflow post (Issue #69660201). In this post,\nthe reporter faces a ValueError when fitting a text classification model with\na BERT tokenizer, due to a mismatch between the model’s expected input\nand the tf.data.Dataset created from the text corpus and labels. For example,\nin the Stack Overflow post (Issue #69660201), the reporter has provided the\nfollowing information.\nR: In the preprocessing layer, I’m using a BERT preprocessor from TF-\nHub.\nBased on the above information, we added the relevant URLs and config-\nured the code to download the preprocessor and encoder. After downloading\nthe preprocessor and encoder, we successfully reproduced the corresponding\nbug.\ntfhub_handle_preprocess = ‘‘https :// tfhub.dev/tensorflow/\nbert_en_uncased_preprocess /3\"\ntfhub_handle_encoder = ‘‘https :// tfhub.dev/tensorflow / small_bert /\nbert_en_uncased_L -4_H -512_A -8/1\"\nVersion Migration (A10) is a vital edit action for reproducing bugs in\ndeep learning frameworks and libraries. It involves adapting code written for\nolder versions to the latest version, confirming the same bug’s presence in the\ncurrent environment. Mondal et al. [13] introduced a similar concept called\n‘Code Migration’. Similar to Obsolete Parameter Removal, Version Migration\naims to address compatibility issues caused by the updates in deep learning\nframeworks and APIs. While Obsolete Parameter Removal focuses on remov-\ning deprecated parameters, Version Migration contains additional modifica-\ntions required to make the code follow the newer version’s syntax, APIs, and\n6 https://www.cs.toronto.edu/ kriz/cifar.html\nTitle Suppressed Due to Excessive Length\n29\nfunctionalities. This may involve updating function calls, class constructors,\nimport statements, and other code elements affected by the version changes.\nThe necessity for Version Migration often arises when the versions used\nin the bug report are significantly outdated compared to the developer’s cur-\nrent working environment. Bug reports may contain code snippets written for\nolder framework versions that are no longer actively maintained or supported\nby the developers. In such cases, downgrading to the exact reported version\nis not feasible, as it would require reverting to unsupported and potentially\ninsecure versions. Additionally, the developer’s current project may have de-\npendencies that require newer versions of Python or deep learning frameworks,\nmaking it impractical to downgrade. Furthermore, organizations or develop-\nment teams may have policies in place that mandate using the latest stable\nversions for security, performance, and maintainability reasons, preventing the\nuse of older versions. To demonstrate the utility of the Version Migration, let us\nconsider the issue reported in the post on Stack Overflow (Issue #45711636).\nThe post describes an issue with the CNN architecture constructed by the\nuser. When the user passes the input through the CNN, they encounter a Val-\nueError, which highlights a problem with the negative dimension value of the\ninput. However, the code snippet provided in the issue uses Tensorflow 1.3.0.\nTo reproduce this issue in TensorFlow 2.14.0, we carefully migrated the model\nbuilt in TensorFlow 1.3.0 to the syntax of Tensorflow version 2.14.0. Specifi-\ncally, we updated the Sequential, Conv2D and MaxPooling2D constructors to\nmatch the TensorFlow 2.14.0 API syntax. We also changed padding and pool\nsize parameter schemes and removed the obsolete input shapes. Finally, we\nupgraded the model compilation and imported the required Tensorflow 2.14.0\nmodules. After modifying the code snippet, we successfully reproduced the\nbug in our runtime environment. We verified the reproduction by comparing\nthe error message from our updated code snippet with the one reported in the\nStack Overflow post.\nSummary of RQ1: By manually reproducing 148 deep learning bugs, we\nidentify ten key edit actions that could be useful to reproduce deep learning\nbugs (e.g., input data generation, neural network construction, hyperparam-\neter initialization). These edit actions can help developers complete the code\nsnippets and thus reproduce their deep learning bugs.\n5.2 RQ2: What component information and edit actions are useful for\nreproducing specific types of deep learning bugs?\n5.2.1 Identifying useful Information in Bug Reports\nWhile reproducing deep learning bugs, we kept track of information from bug\nreports that helped us reproduce them. After reproducing 148 bugs success-\n30\nMehil B. Shah et al.\nfully, we have identified five useful pieces of information that can improve the\nchance of reproducing a bug. We discuss these factors in detail below.\nData (F1): Data is one of the essential factors in ensuring the repro-\nducibility of deep learning bugs. Deep learning systems heavily rely on the\ndata [49], and reproducing deep learning bugs becomes easier with access to\nthe original data. Data helps us reproduce the deep learning bugs by providing\nthe exact sample inputs that trigger the erroneous behaviour. By understand-\ning the training data distributions and ranges, we can reconstruct the original\ntraining environment, which is crucial for reproducibility. However, the issue\nreports often lack direct information about the data. To address this problem,\nwe collect information about the data by extracting the shape of the data,\ndata distribution, type of variables and their corresponding ranges from the\nissue description. Leveraging this information and our proposed edit actions,\nnamely Input Data Generation (A1) and Dataset Procurement (A8), we can\ngenerate or obtain the necessary data to reproduce deep learning bugs. Accord-\ning to our investigation, ≈77% of our reproduced bugs contained information\nabout the data (see Table. 7) in their issue reports. For example, in the Stack\nOverflow post (Issue #43464835), the reporter has provided the dimensions\nand sample row of the training dataset, as shown below.\nR: I have a train dataset of the following shape: (300, 5, 720)\nSample Input: [[[ 6. 11. 389. ..., 0. 0. 0.]]]]\nUsing this information and our proposed edit action Input Data Generation\n(A1), we first generated a data frame of size (300, 5, 720) that contains values\nin the range from 0 to 400. Then, by leveraging the generated data and other\nuseful information, we successfully reproduced the corresponding bug.\nModel (F2): The model architecture describes the components of a deep\nlearning model and how they transform inputs into outputs. Understanding the\nmodel architecture is crucial for reproducing deep learning bugs, as it provides\ninsights into the model’s components, connectivity, and the required neural\nnetwork architecture. According to our investigation, ≈58% of our reproduced\nbugs contained information about the model architecture (see Table. 7) in\ntheir issue reports. However, the complete source code implementing the model\narchitecture might not always be available in the issue reports. To overcome\nthis limitation, we carefully gather information about a model’s architecture,\ni.e., the number of layers, layer properties and activation function from the\nissue description. Leveraging this information and our proposed edit action\nNeural Network Construction (A2), we reconstruct the model and reproduce\nthe deep learning bugs. For example, let us consider the following text from a\nStack Overflow post (Issue #63204176) that mentions the use of multinomial\nlogistic regression on the MNIST dataset, as shown below. Unfortunately, the\nreporter fails to provide the code snippet for the model.\nR: I am trying to write a simple multinomial logistic regression using mnist\ndata.\nDespite the absence of the code snippet, the issue description provides\nuseful hints about the model architecture and dataset. Using them and our\nTitle Suppressed Due to Excessive Length\n31\nedit action – Neural Network Construction (A2), we successfully reproduced\nthe bug.\nHyperparameters (F3): Hyperparameters play a crucial role in control-\nling the learning process and model behaviour during training. They encom-\npass parameters such as learning rate, batch size, number of epochs, optimizer,\nregularization techniques, and loss functions. The specific values chosen for\nthese hyperparameters significantly impact a model’s performance, training\nprocess, and bug manifestation. Thus, reporting the complete set of hyper-\nparameters is essential to help reproduce deep learning bugs. According to\nour investigation, 48% of our reproduced bugs contain information about the\nhyperparameters used in their issue reports. For the 52% of bug reports that\ndid not include hyperparameter information, we used the Hyperparameter\nInitialization edit action (A2) to reproduce the bugs by initializing the hy-\nperparameters with default values. In the example Stack Overflow post (Issue\n#65993928), we can see how hyperparameters can play a crucial role in repro-\nducing the bug.\nloss2 = (2 * ( log_sigma_infer\n- log_sigma_prior )).exp () \\ +(( mu_infer\n-\nmu_prior)/ log_sigma_prior .exp()) ** 2 \\ - 2 * ( log_sigma_infer\n-\nlog_sigma_prior ) - 1\nloss2 = 0.5 * loss2.sum(dim = 1).mean ()\nSince the reporter was using a custom loss function, it was vital for them\nto share the value of constants and the formula used to calculate the loss. The\nreporter’s custom loss function, with constants 2 and 0.5 and its formula, was\ncrucial for reproducing a bug in model training due to incorrect loss calcula-\ntion, emphasizing the need to provide a complete set of hyperparameters.\nCode Snippet (F4): Code snippets are critical for reproducing bugs, as\nhighlighted by the fact that 98% of professional developers consider them an\nessential component of bug reproducibility [50]. They include the data pre-\nprocessing, data splitting technique, code used for training the model, and\nimplementation of the evaluation metrics. However, from our manual bug re-\nproduction, we observe that even though code snippets are present in ≈82% of\nthe bug reports, only 9.41% of them can be used verbatim for bug reproduc-\ntion. To address this limitation, we use several of our proposed edit actions,\nsuch as Import Addition and Dependency Resolution (A4), Logging (A5), Ob-\nsolete Parameter Removal (A6), Compiler Error Resolution (A7) and Version\nMigration (A10). These edit actions help us fix the errors in the code and make\nthe code compileable and runnable.\nTo demonstrate the importance of submitting a complete code snippet,\nlet us consider the Stack Overflow post with Issue #76186890. In this issue,\na high-quality code snippet helps us reproduce the bug related to a compli-\ncated architecture (T5) without significant changes. The code snippet uses the\nT5 model from HuggingFace Transformers for text-to-text translation. It pre-\nprocesses the input text data, configures the target IDs to predict a specific\n32\nMehil B. Shah et al.\nTable 7: Prevalence of Useful Information in Reproducible Issue Reports\nFactor\nData\nModel\nHyperparameters\nCode Snippet\nLogs\nPrevalence\n77.4%\n58.1%\n47.9%\n82.1%\n87.6%\nTable 8: Top 3 Useful Component Information for Reproducing Specific Types\nof Deep Learning Bugs\nTraining\nModel\nCode Snippet (0.86)\nLogs (0.7857)\nData (0.82)\nCode Snippet (0.7143)\nLogs (0.76)\nModel (0.6429)\nTensor\nAPI\nData (0.9655)\nLogs (0.85)\nLogs (0.9310)\nCode Snippet (0.75)\nCode Snippet (0.7241)\nModel (0.70)\nanswer, calculates loss and perplexity metrics, and trains the model. Having\na complete and runnable snippet was helpful in reproducing the bug. It pro-\nvides sufficient details like data preparation, loss calculation, model training,\nand evaluation to improve the ease of reproducing deep learning bugs, which\nwere essential for bug reproduction.\nLogs (F5): Logs provide a real-time record of the model’s behaviour during\ntraining and inference. Traditional software logs consist of information such as\nevent logs and stack traces, whereas deep learning systems consist of compiler\nerror logs, training error logs, and evaluation logs [51]. Sharing these logs is\ncrucial for reproducing deep learning bugs as they allow us to verify if we can\nreproduce the same erroneous behaviour reported in the original issue. From\nour manual bug reproduction, we discover that ≈88% of our reproduced bugs\ncontain the necessary logs in their issue reports. Such a high presence of logs\nin our dataset of reproduced bugs highlights the importance of logging in deep\nlearning bug reproduction. By matching the logs from the original issue and\nthe logs from reproduction on our local machines, we were able to confirm the\npresence of several bugs in the deep learning systems.\nThe Stack Overflow post (Issue #34311586) shows how logs can be used\nto confirm the presence of deep learning bugs. In this particular issue, the\nreporter shared the training logs and the code snippet. We modified the code\nsnippet with our proposed edit actions to make it compilable and runnable.\nWhen executed, we observed the same anomalous behaviour in the training\nlogs as described by the original reporter. This demonstrates the importance\nof sharing training and evaluation logs in the issue report. We also found them\nto be one of the most useful pieces of information to reproduce deep learning\nbugs.\nTitle Suppressed Due to Excessive Length\n33\n5.2.2 Relationship between Useful Information and Type of Bugs\nDuring our manual analysis (Section 5.2.1), we identify useful information for\nreproducing deep learning bugs. We used the Apriori algorithm to determine\nthe relationship between the type of bug and the information required to re-\nproduce the bug. The insights from this analysis could serve two purposes\n– detecting missing information in submitted bug reports and formulating\nfollow-up questions to obtain any missing information needed for reproducibil-\nity. Table. 8 summarizes our findings, and we discuss them in detail below.\nData: The accurate reproduction of bugs in deep learning systems heavily\nrelies on the presence of data and its characteristics. They play a crucial role\nin reproducing two key categories of bugs - training bugs and tensor bugs.\nFor training bugs, the data has a confidence value of 82.00% according to\nour generated rules (check Table. 8). This high confidence highlights the signif-\nicance of data in reproducing the numerous training issues that can manifest\nduring model development. The details about the data, such as the number\nof samples, class distribution, feature distributions, data splitting ratios, and\npreprocessing steps, are instrumental in reproducing training bugs. For ex-\nample, training a classification model on a highly skewed dataset is prone to\noverfitting. With the knowledge of the data distribution, we can recreate a rep-\nresentative dataset, which can help us reproduce the bugs, even if the actual\ndataset is not available.\nFor tensor bugs, data characteristics have an even higher confidence of\n96.55%, according to our generated rules (Table. 8). This suggests that tensor\nattributes like shape, data type, sparsity, value ranges and origin are pivotal\nfor reliably reproducing many bugs stemming from invalid dimensions or pre-\ncision issues. For instance, bugs arising from tensor shape mismatches can\nemerge if the shape of the input data does not match the expected input\nshape. Furthermore, real-world data is often more vulnerable to human error\nand biases compared to synthetic data [52]. Therefore, comprehensive docu-\nmentation and availability of these salient data characteristics are imperative\nfor the reliable reproduction of the numerous training and tensor bugs in\ndeep learning systems.\nModel: A neural network model’s architecture and implementation details\nare crucial to reproducing model and API bugs. The model architecture has\n78.57% and 70.00% confidence values for model and API bugs, respectively,\naccording to our generated rules. These high values signify that access to model\ndetails is vital for reliably reproducing issues stemming from model capacity,\nconnectivity, and API usage.\nAn access to the model architecture (e.g., layer types, layer connectivity,\nweight initialization schemes, etc.) can help us systematically reproduce model\nbugs. Insufficient learning capacity in specific layers and improper weight ini-\ntialization might lead to exploding/vanishing gradients [53], whereas incorrect\nlayer connectivity might lead to representational bottlenecks [54]. These issues\nusually manifest as model bugs during training and inference. Thus, informa-\n34\nMehil B. Shah et al.\ntion about the model architecture can help us localize and reproduce such\nbugs.\nFor reproducing the API bugs, model architecture can provide fundamen-\ntal context. Details like layer dimensions, bottlenecks, parallelization needs,\nand memory requirements show how the model interacts with the API [55].\nBottlenecks within a model, areas where data processing slows down, and\nthe need for parallel processing to handle large-scale computations shape how\nAPIs are utilized. Additionally, the varying memory requirements of different\narchitectures impact how the model leverages the system’s resources via the\nAPI. This includes memory allocation for storing weights and activations and\nmanaging the flow of data through the network during operations like forward\nand backward propagation. Understanding the model architecture is vital for\nreproducing bugs, as it indicates whether issues stem from the model’s design\nor from its interaction with the API. For example, a bug might arise due to the\nmodel’s inability to handle certain operations efficiently, or it could be a re-\nsult of the API not properly supporting specific architectural features. Hence,\nmodel architecture helps us reproduce the bugs triggered by incorrect usage\nof API. Therefore, the presence of model architecture allows the reproduction\nof both model bugs and API bugs in deep learning systems.\nCode Snippet: Code snippets are invaluable for reproducing deep learn-\ning bugs since they isolate and encapsulate the core logic that triggers them.\nDevelopers can demonstrate and share the essence of buggy behaviour by\ncreating a minimal reproducible example. Code snippets have very high con-\nfidence values of 86.0% for training bugs, 72.41% for tensor bugs, 71.43% for\nmodel bugs, and 75.0% for API bugs, according to our generated rules. These\nhigh values across all bug types signify that code snippets are critical for reli-\nably reproducing deep learning bugs.\nEach code snippet may contain the relevant model, data processing, train-\ning and evaluation scripts. Developers can use such a snippet to recreate the\nbug-inducing steps. For example, a snippet may compactly capture just a\nfew lines, mishandling tensor shapes or performing incorrect gradient calcula-\ntions, eliminating any confounding factors and enabling developers to repro-\nduce these bugs. Developers can systematically execute, analyze, and debug\nthe code to reproduce various bugs, including training, tensor, model, and API\nbugs.\nLogs: Logs play a fundamental role in deep learning by facilitating the re-\nproduction and resolution of bugs. They comprehensively record every detail\nduring model training, capturing information about various conditions, con-\nfigurations, and events before the failures. Logs have high confidence values\nof 76.00% for training bugs, 93.10% for tensor bugs, 78.57% for model bugs,\nand 85.00% for API bugs according to our generated rules. Such high confi-\ndence levels across all bug types highlight that comprehensive log recording is\nimportant for the reliable reproduction of deep-learning bugs.\nThese logs include essential components such as hyperparameters, dataset\ncharacteristics, hardware specifications, framework versions, and random num-\nber generator seed values. The true power of logs lies in their ability to recreate\nTitle Suppressed Due to Excessive Length\n35\nTable 9: Top 5 Edit Actions for Reproducing Specific Types of Deep\nLearning Bugs\nTraining\nModel\nInput Data Generation (0.5625)\nHyperparameter Initialization (0.5142)\nImport Addition (0.4583)\nDataset Procurement (0.4390)\nCompiler Error Resolution (0.3750)\nCompiler Error Resolution (0.4146)\nDataset Procurement (0.3542)\nImport Addition (0.4146)\nHyperparameter Initialization (0.3333)\nNeural Network Construction (0.3659)\nTensor\nAPI\nHyperparameter Initialization (0.5517)\nInput Data Generation (0.40)\nInput Data Generation (0.5172)\nHyperparameter Initialization (0.40)\nImport Addition (0.5172)\nImport Addition (0.35)\nDataset Procurement (0.4138)\nLogging (0.25)\nObsolete Parameter Removal (0.3448)\nObsolete Parameter Removal (0.25)\npast training runs precisely. Developers can isolate and reproduce the environ-\nment that led to the original bugs using the logged hyperparameters, such as\nbatch size, learning rate schedules, and gradient clipping thresholds. Logs can\nalso help developers reproduce specific deep-learning bugs. For example, logged\nrandom seeds can help the developers recreate a specific weight initialization\nor data batching order, which can then be used to reproduce training bugs.\nSimilarly, logs can be used to reproduce Model, Tensor and API Bugs. Thus,\nlogs are invaluable for accurately recreating conditions that result in errors and\nenabling the reproduction of deep learning bugs.\n5.2.3 Relationship between Edit Actions and Type of Bugs\nAfter determining association between the useful information and the type of\nbug, we derived the relationship between the edit actions and the type of bug.\nWe use the Apriori algorithm to determine the relationship, as done earlier.\nTable. 9 summarizes our findings, and we discuss them in detail below.\nTraining Bug: Input data generation (56.25% confidence) is the most fre-\nquently used edit action for reproducing training bugs. However, our analysis\n(Table. 7) shows that ≈77% of the bug reports/SO posts contain informa-\ntion about the data. Leveraging this information and our proposed edit action\n– Input Data Generation (A1), we can reproduce the training bugs in deep\nlearning systems. Furthermore, import addition (45.8% confidence) and com-\npiler error resolution (37.5% confidence) are other edit actions which are often\nused to reproduce training bugs. Although 79% of bug reports contain code\nsnippets, they are often incomplete. To reproduce the bug effectively, we thus\nneed to complete these snippets by adding the necessary import statements\nand migrating them to the latest versions of libraries and frameworks. Finally,\ndataset procurement (35.42% confidence) and hyperparameter initialization\n(33.33% confidence) may also help reproduce training bugs by procuring the\ndataset required and initializing the missing hyperparameters.\n36\nMehil B. Shah et al.\nSince the training bugs have specific sub-faults, we manually analyse the\nStack Overflow posts and Github issue reports for different sub-types and\ndiscuss the edit actions, which can be used to reproduce the specific sub-faults\nbelow.\n– Optimisation: Optimization bugs can be reproduced by employing a com-\nbination of edit actions, with a primary focus on three key areas: Hyperpa-\nrameter Initialization (60.00% confidence), Input Data Generation (60.00%\nconfidence), and Neural Network Construction (40.00% confidence). These\nactions involve configuring the optimizer and learning rate, creating repre-\nsentative training data, and building the model architecture, respectively.\nAdditionally, logging (40.00% confidence) can help monitor the training\nprocess, while Import Addition (20.00% confidence), Compiler Error Res-\nolution (20.00% confidence), and Version Migration (20.00% confidence)\nensure compatibility within the environment. By systematically testing\ndifferent optimizer configurations, generating appropriate input data, and\nconstructing the relevant model architecture, developers can effectively re-\nproduce optimization issues and gain insight into the underlying causes.\n– Loss Function: Loss function bugs, which can arise from incorrect loss\ncalculations or suboptimal loss function choices, can be reproduced through\na combination of edit actions. Hyperparameter Initialization (66.67% con-\nfidence) plays a crucial role in configuring the loss function while Logging\n(58.33% confidence) captures the output of intermediate loss values dur-\ning training. Since the design of the output layer in the neural network\ndirectly influences the calculation of loss, Neural Network Construction\n(41.67% confidence) may be necessary to reproduce the bugs related to the\nloss function. Furthermore, it is essential to generate appropriate train-\ning data through Input Data Generation (33.33% confidence) to effectively\nreproduce bugs related to the loss function.\n– Hyperparameters: Hyperparameter sub-faults, such as suboptimal batch\nsize, suboptimal number of epochs, and suboptimal learning rate, can\nbe reproduced by utilizing the Hyperparameter Initialization edit action\n(64.29% confidence). This involves initializing various hyperparameters,\nsuch as batch size, epochs, and learning rate, that may be missing or\nsuboptimal in the provided code. By experimenting with different com-\nmonly used values for these hyperparameters, we were able to reproduce\nthem. In addition to that, Input Data Generation (50.00% confidence)\nproves to be valuable for reproducing hyperparameter bugs. Generating\nrepresentative input data is crucial for triggering hyperparameters-related\nissues during the training process. Furthermore, actions like Import Ad-\ndition (42.86% confidence), Logging (35.71% confidence), and Compiler\nError Resolution (35.71% confidence) are helpful in ensuring that the code\nruns smoothly and allows for testing different hyperparameter configura-\ntions. These actions contribute to creating a compatible environment to\naddress hyperparameter-related concerns effectively.\nTitle Suppressed Due to Excessive Length\n37\nModel Bug: Model bugs are often reproduced by using the edit action –\nhyperparameter initialization (51.4% confidence). From our manual analysis,\nwe observed that the hyperparameters were not often reported for model bugs.\nHence, hyperparameter initialization was often used to reproduce the model\nbugs. Additionally, dataset procurement (43.90% confidence) and compiler er-\nror resolution (41.46% confidence) are typical edit actions to reproduce the\nmodel bugs. Moreover, the code snippets for deep learning bugs are often in-\ncomplete, as observed in our manual analysis; hence, import addition (41.46%\nconfidence) is a critical edit action for completing the code snippet and repro-\nducing model bugs. Finally, model bugs might be reproduced by modifying\na neural network’s architecture, as shown by the moderate confidence value\n(36.6%) for the edit action – neural network construction. Since model bugs\nare primarily caused by errors in the neural network architecture [16], recon-\nstructing the neural network might be the first step to reproduce them. Thus,\nmodel bugs can be reliably reproduced through several edits that initialize\nhyperparameters, resolve compiler errors, procure datasets, add imports, and\nconstruct neural networks.\nSimilar to training bugs, model bugs also have specific sub-faults. Hence,\nwe report the edit actions for reproducing specific sub-faults below.\n– Layer Type & Properties: To reproduce specific model bugs related to\nindividual layers, such as incorrect layer types, suboptimal filter sizes, or\ninappropriate activation functions, it is necessary to carefully define the\nlayers as part of the Neural Network Construction (78.57% confidence)\naction. By accurately specifying the layers in the model architecture, de-\nvelopers can trigger the desired layer-specific bugs for further analysis and\nresolution. It is also crucial to initialize the appropriate hyperparameters\nthrough the Hyperparameter Initialization (57.14% confidence) action. Ad-\nditionally, obtaining the required input data shapes via Input Data Gen-\neration (42.86% confidence) is essential for triggering layer-specific issues\nduring the training process. To ensure compatibility within the project en-\nvironment, actions such as Compiler Error Resolution (42.86% confidence)\nand Version Migration (42.86% confidence) are beneficial for updating the\nlayer definitions and reproducing any compatibility issues that may arise.\n– Model Type & Properties: Bugs associated with suboptimal model\narchitecture, incorrect network structure, or missing layers can often be\nreproduced using the Neural Network Construction edit action (71.43%\nconfidence). By carefully constructing the neural network based on the ar-\nchitecture details provided in a bug report, developers can reproduce sub-\nfaults from the model bugs category. The interaction between the hyperpa-\nrameters and the model architecture could be important for model bugs.\nTherefore, the Hyperparameter Initialization action (42.86% confidence)\nalso plays a significant role in effectively reproducing model-related issues.\nTo ensure compatibility with the latest library and framework version,\nactions such as Import Addition (35.71% confidence), Obsolete Parame-\nter Removal (28.57% confidence), and Compiler Error Resolution (28.57%\n38\nMehil B. Shah et al.\nconfidence) are essential. These actions help update the codebase and en-\nsure that the necessary dependencies are met to define the desired model\narchitecture accurately.\nTensor Bug: Data is the most important information for reproducing ten-\nsor bugs, as shown by the strong correlation between Tensor Bug & Data (see\nTable. 8). This phenomenon can be attributed to the fact that tensor bugs\nprimarily relate to the data [16]. Since tensor bugs are so data-dependent,\ndataset procurement emerges as an important edit action to reproduce them.\nBug reports reference the exact dataset used (if well-known) or describe the\ndata type, shape and distribution. With this information, input data genera-\ntion and dataset procurement can generate or procure the data required for\nthe reproduction of the tensor bugs. Like other deep learning bugs, import\naddition (51.72% confidence) and hyperparameter initialization (55.17% con-\nfidence) also play a key role in completing the code snippet and reproducing\nthe tensor bugs. Thus, tensor bugs can be reliably reproduced through several\nedits that procure the datasets, generate input data, add logging, import the\nrequired dependencies, and initialize the hyperparameters.\nAPI Bug: Since API Bugs stem from an improper usage of application\nprogramming interfaces (APIs), they do not focus as heavily on the model\ntraining process. As a result, bug reports might lack detailed information about\nthe input data or hyperparameters used. This encourages the use of common\nedit actions such as input data generation (40.00% confidence) and hyperpa-\nrameter initialization (40.00% confidence) when reproducing API bugs. Addi-\ntionally, code snippets in bug reports are often incomplete, necessitating edit\nactions like import addition (35.00% confidence) and logging (25.00% confi-\ndence) to trace the intermediate states and outputs of the API. Thus, API\nbugs can be reliably reproduced through several edit actions that generate\ninput data, import the required dependencies, add logging, initialize hyperpa-\nrameters and remove obsolete parameters.\nSummary of RQ2: By applying the Apriori algorithm on the data produced\nfrom our reproduction of 148 deep learning bugs, we identified the top 3\nmost important pieces of information and the top 5 edit actions needed to\nreproduce each category of deep learning bug. This provides insights into the\nmissing information that should be solicited in bug reports as well as the edit\nactions required to reproduce specific bug types.\n5.3 RQ3: How do the suggested edit actions and component information\naffect the reproducibility of deep learning bugs?\nWe conduct a developer study to determine if our suggested edit actions and\ninformation help one reproduce deep learning bugs. We prepared four sets of\nbugs, each consisting of two different types of bugs (see Section 4.6). We ran-\ndomly assigned one of the four bug sets to each participant and instructed\nTitle Suppressed Due to Excessive Length\n39\nTable 10: Percentage of bugs successfully reproduced by control and experi-\nmental group across different sets.\n% of bugs successfully re-\nproduced by Control group\n% of bugs successfully re-\nproduced by Experimental\ngroup\n% Increase\nSet 1\n75.00\n100.00\n25.00\nSet 2\n66.66\n83.33\n16.66\nSet 3\n83.33\n100.00\n16.66\nSet 4\n66.66\n100.00\n33.33\nAverage\n72.91\n95.83\n22.92\nthem to reproduce the bugs in their assigned set. We divided the participants\ninto the control and experimental groups using stratified random sampling\n(see Section 4.6). The developers in both the control and experimental groups\nreproduced the same bugs from our prepared set of bugs. The only difference\nwas that the experimental group received hints based on our findings to help\nreproduce the bugs, while the control group did not receive these hints. Since\nboth groups received randomly selected sets from the same pool of four identi-\ncal bug sets, each bug was reproduced independently by at least five developers\nacross both control and experimental groups.\nTable. 10 and Fig. 4 shows the bug reproducibility rate of the control and\nexperimental groups across different sets of bugs. We found that developers\nin the control group could reproduce 72.91% bugs without hints. In contrast,\nthe developers in the experimental group could reproduce 95.83% of bugs\nwith our hints - a 22.92% increase. This significant increase in reproducibility\nrate demonstrates that our identified edit actions and information improved\ndevelopers’ ability to reproduce deep learning bugs.\nWe also collect the qualitative feedback from the participants to better\nunderstand their pain points in bug reproduction. Their feedback provided\nnew insights not covered in our original study. Notably, 31.78% of developers\nhighlighted the lack of standardized debugging tools as the primary challenge in\nbug reproduction. Furthermore, the developers mentioned missing information\n(data, logs, hyperparameters, and dependencies), and lack of unit testing or\nversion control in DL systems as the most challenging aspects of reproducing\ndeep learning bugs, as shown below.\nQuestion: What are the challenges of bug reproduction in day-to-day activi-\nties?\nR1: lack of debugging support and missing information\nR2: data quality issues and lack of standardized debugging procedures and\ntools.\nR3: no standardized debugging practices, and lack of clarity on the information\nneeded, also lot of dependencies (libraries, framework, data, infra and so on)\nR4: the flaky nature of deep learning models, and the unclear expectations of\nhow model is supposed to behave.\n40\nMehil B. Shah et al.\nFig. 4: Bug Reproduction Success Rates by Control and Experimental Groups\nR5: memory issues, documentation issues (missing information in issues), weak\ndebugging support\nR6: version control of deep learning is tricky, because of multiple snapshots\nof models, model management and reproducibility is tricky. also, the lack of\nstandardized debugging practices makes it more tricky.\nR7: distributed computing makes it difficult to find and reproduce the bugs.\ntest coverage is also a problem as we cannot find the bugs properly because of\nlack of coverage and that is a problem with the reproduction of bugs.\nWe also analyze how our suggested actions and information help the partic-\nipants in bug reproduction. According to the qualitative responses, 40.91% of\ndevelopers found our suggested edit actions to be helpful for reproducing their\nassigned bugs. 54.55% of developers report that our suggested hints about\nthe useful information helped them narrow down where to look in the code.\nThus, the qualitative feedback below highlights the benefits of our findings in\nreal-life settings.\nR1: I followed the guidance in the survey and used the hints to generate the\nappropriate training dataset, which then allowed me to reliably reproduce the\nproblematic behavior during model training.\nR2: had to add manual imports for torch and nn, and fixed the compiler errors\nrelated to imports, as highlighted by the hint provided\nTitle Suppressed Due to Excessive Length\n41\nTable 11: Time taken for bug reproduction by control and experimental groups\nAverage time taken by Con-\ntrol Group (seconds)\nAverage time taken by Ex-\nperimental Group (seconds)\n% Decrease\nSet 1\n1437\n1088\n24.28\nSet 2\n1803\n1563\n13.31\nSet 3\n2548\n1792\n29.67\nSet 4\n2165\n1512\n30.16\nR3: Following the hint, I systematically generated all of the necessary input\ndata that would be required in order to reliably reproduce the software bug\nduring testing.\nR4: The hint mentions the Iris dataset, so I used the edit action called “Dataset\nProcurement” and got the dataset, downloaded it and edited the code snippet\nto reproduce the bug.\nR5: with the given hint, I generated the input data required for the bug\nreproduction.\nR6: The imports were missing and as per the hint, data frame was generated\nfor specific columns, which helped the resolution of the bug.\nR7: As hinted in the survey, explicitly specifying the columns when construct-\ning the data frame helped in the bug reproduction\nWe also calculate the time the control and experimental groups took to\nreproduce their assigned bugs. This helps us assess our findings’ benefits in\nreducing the time required for deep learning bug reproduction. Table. 11 and\nFig. 5 show the time taken to reproduce bugs by the control and experimental\ngroups. The experimental group outpaced the control group in bug reproduc-\ntion across all sets, with the most notable difference in Set 4, where the experi-\nmental group was 30.16% faster. On average, the control group lagged behind\nthe experimental group by 24.35% across all sets. These results demonstrate\nthat our recommended edit actions and component information enabled the\nexperimental group to reproduce deep learning bugs much faster than the\ncontrol group that did not receive this information.\n5.3.1 Impact of Hints on Bug Reproducibility\nTo determine the impact of our hints on bug reproducibility, we constructed\na generalized linear model (GLM) using the Binomial family with a logit link\nfunction [56]. This allowed us to test the statistical significance of multiple\nindependent factors on bug reproducibility (i.e, our dependent variable). The\nfactors were experience with deep learning bug fixing, experience with deep\nlearning, profession, and the presence/absence of hints, with ‘Hints’ being our\nmain factor of interest. The DLBugFixExp * factors represent the participants’\nexperience in fixing deep learning bugs, with levels 0, 1, and 2 correspond-\ning to experience of 0-4, 5-9, and 10+ years of experience, respectively. We\nchose Odds Ratio as our effect size metric for two main reasons. First, it has\n42\nMehil B. Shah et al.\nFig. 5: Time Taken for Bug Reproduction Across Control and Experimental\nGroups\nbeen commonly used in similar studies within software engineering research,\nas demonstrated by Ceccato et al. [57]. Second, the Odds Ratio is appropriate\nfor a logistic regression-based model with a binary target variable, which we\nused in our study.\nTable 12: GLM Model for Assessing the Impact of Various Factors on the\nReproducibility of Deep Learning Bugs\nVariable\nEstimate\nStd. Error\nz value\nPr > |z|\nEffect Size (OR)\nIntercept\n-3.4229\n1.898\n-1.803\n0.071\n-\nDLBugFixExp 0\n0.2251\n0.856\n0.263\n0.793\n1.252493\nDLBugFixExp 1\n-0.2449\n0.564\n-0.435\n0.664\n0.782786\nDLBugFixExp 2\n-3.4032\n2.283\n-1.491\n0.136\n0.033268\nHints\n3.0899\n0.991\n3.118\n0.002\n21.974308\nDLExp\n2.5448\n1.725\n1.475\n0.140\n12.740844\nField\n0.1915\n1.145\n0.167\n0.867\n1.211112\nBased on the regression results in Table 12, we can see that the presence\nof hints had a statistically significant positive effect on the reproducibility of\ndeep learning bugs (p = 0.002 < .05). The effect size, as measured by the odds\nratio, indicates that the presence of hints increases the odds of reproducing\na deep learning bug by a factor of 21.97 compared to the absence of hints.\nThe intercept term, with an estimate of -3.4229 (p = 0.071), represents the\nbaseline probability of reproducing a deep learning bug when no hints are\nTitle Suppressed Due to Excessive Length\n43\nprovided, the participant has no experience with deep learning bug fixing,\ndeep learning in general, and their profession is not considered. Moreover,\nas shown in Table. 12, factors like experience with deep learning bug fixing,\ngeneral deep learning experience, and profession (academia vs industry) did\nnot significantly influence reproducibility.\nOverall, the above results suggest that the presence of targeted hints pos-\nitively impacts the reproducibility of deep learning bugs with a statistically\nsignificant margin and a large effect size. Other factors, such as experience and\nprofession, do not play a significant role in bug reproducibility.\nSummary of RQ3: In the user study, developers were assigned to either\nthe experimental or control group where the experimental group received our\nsuggested edit actions and component information. The experimental group\nreproduced 22.92% more deep learning bugs and decreased repro-\nduction time by 24.35% compared to the control group. This demonstrates\nthat the identified edit actions and component information substantially im-\nprove the reproducibility rate and reduce the time needed to reproduce deep\nlearning bugs.\n6 Discussions\nIn this section, we first provide actionable insights about the reproducibility\nof deep learning bugs and recommend potential directions for further research\n(see Section 6.1). We then demonstrate how our findings can improve the\nreproducibility of the DL bugs with the use of large language models (e.g.,\nLlama 3) (see Section 6.2).\n6.1 Reproducibility of Deep Learning Bugs\nFrom our manual reproduction and user study, we observe that API, Model,\nand Tensor bugs are relatively more straightforward to reproduce. This be-\nhaviour can be explained by the fact that these bugs are more specific to\nthe location in the code where they originate. For example, faulty input data\nusually triggers tensor bugs, whereas incorrect usage of a framework’s API\ncauses API bugs. In contrast, training bugs cover multiple issues related to\ndeep learning model training, and GPU bugs relate to GPU devices for deep\nlearning and manifest across GPU interactions.\nThis behaviour is further supported by the reproducibility rates and ef-\nforts involved in reproducing different types of bugs. The reproducibility rates\nof Training bugs and GPU bugs were 89.65% and 42.85%, respectively. On\nthe other hand, the reproducibility rates for API, Model and Tensor bugs\nwere 81.81%, 88.46%, and 80.75%, respectively. Even though the training bugs\ncould be reproduced reliably, the efforts involved in reproducing the training\n44\nMehil B. Shah et al.\nbugs were significantly more than those of other bugs. The average time to\nreproduce the API, Model and Tensor bugs was 45.5, 43.9 and 43.8 minutes,\nrespectively. On the other hand, the average time to reproduce the Training\nbugs was 52.5 and 57.33 minutes, respectively. These statistics highlight that\nwe need to put more effort into reproducing the Training and GPU bugs, which\ncalls for further research into the specific nature of Training and GPU bugs.\nTo assist future research in the reproducibility of deep learning bugs, we\nprovide directions for future research below:\nUnderstanding Training Bug Reproducibility: Training bugs are the most\ncommon type of bug in deep learning systems, accounting for 52.5% of all\nbugs [16]. Training bugs have high reproducibility rates but also require signifi-\ncant effort to reproduce. This behaviour presents an opportunity to understand\nbetter what factors make training bugs more reproducible or harder to repro-\nduce. By analyzing the training procedures, model architectures, optimizers,\nhyperparameters, and other elements that either aid or hinder reproducibility,\nwe can uncover insights to guide the diagnosis and repair of training bugs.\nThere is also room to develop improved tools and methodologies explicitly\nfocused on efficiently reproducing the nuanced nature of training bugs.\nAnalyzing the GPU Bug Reproducibility Gap: The lower reproducibility\nrate for GPU bugs highlights a gap in understanding the interactions between\ndeep learning code and the underlying GPU hardware or drivers. By further\nstudying irreproducible GPU bugs and quantifying the aspects that impede\nreproducibility, such as hardware differences, software dependencies, and en-\nvironmental factors, we can work towards solutions to increase reproducibil-\nity. Opportunities exist to build infrastructure, leverage containerization or\nvirtualization, and create testing tools to better control for sources of non-\ndeterminism that impact GPU bug reproduction.\nReproducible Testbeds for Deep Learning: Ultimately, the variability in re-\nproducing different categories of deep learning bugs motivates the need for\nreproducible testbeds. Shared sets of reproducible deep learning bugs with\nassociated test cases, model architectures, training configurations, dependen-\ncies, and environmental contexts will accelerate future research. Constructing\nsuch testbeds requires systematic characterization of how these factors influ-\nence reproducibility. Reproducible testbeds also support developing specialized\ntechniques for efficiently reproducing and debugging deep learning bugs.\n6.2 Challenges in Reproducing Deep Learning Bugs: A Comparison between\nStack Overflow and GitHub\nReproducing deep learning bugs from Stack Overflow posts and GitHub repos-\nitories presents different challenges and requires varying levels of effort. The\nmain differences lie in the availability of code and data, environmental setup,\ncontext, completeness, and reproducibility expectations. We explain them briefly\nas follows.\nTitle Suppressed Due to Excessive Length\n45\nFirst, GitHub repositories often provide the complete codebase and some-\ntimes the accompanying datasets, whereas Stack Overflow posts typically in-\nclude small code snippets without the broader context and data. Hence, we\nneed more effort in reconstructing the code and generating synthetic data\nwhen reproducing bugs from Stack Overflow. Second, GitHub repositories\ncommonly include setup instructions, dependency files, and development envi-\nronment configurations, making it easier to recreate the original environment\naccurately. On the contrary, Stack Overflow posts rarely provide such details,\nrequiring guesswork about software versions, dependencies, and environment\nsettings. Finally, GitHub issues tend to have more contextual information,\nsuch as detailed problem descriptions, steps to reproduce a bug, and error\nlogs supporting root cause analysis of the issue. Stack Overflow posts may\nlack this level of detail, making it harder to comprehend and reproduce a bug\naccurately.\nDespite the additional resources available in GitHub repositories, repro-\nducing bugs from them still presents several challenges:\n– Incomplete or outdated repositories: Critical files, dependencies, or code\nchanges may be missing or outdated, making it difficult to reproduce the\nexact environment and conditions under which a bug occurred.\n– Large and complex codebases: Deep learning projects often have extensive\nand intricate codebases, requiring significant time and expertise to set up\nand navigate them.\n– Proprietary or sensitive data: Many projects may involve proprietary data\nthat cannot be shared publicly, making it challenging to reproduce data-\nrelated bugs without access to the original data.\n– Insufficient documentation: Many GitHub repositories fail to provide clear\ndocumentation on project architecture, installation steps, and expected be-\nhaviours. The lack of clear documentation makes it difficult for developers\nto reproduce bugs, as they face challenges in understanding the codebase\nand configuring the project locally.\n– Dependency management: Resolving dependency conflicts or managing\ncompatibility issues across different project versions can be a significant\nchallenge when reproducing bugs.\nTo address these challenges, we recommend several changes or adaptations\nto the status quo as follows. First, Stack Overflow should leverage its user\nbase and search functionality for discussing common deep-learning bugs and\nquick solutions, while GitHub should be the primary platform for in-depth\nbug reproduction and resolution. Second, Stack Overflow should introduce a\nspecialized format for deep learning bug reports, expanding on its traditional\nminimum working example approach to include dataset details, framework ver-\nsions, hardware specifications, and reproducible code snippets. GitHub repos-\nitories should be kept up-to-date, well-documented, and complete. Finally,\nboth platforms should incentivize active participation in bug reporting and\nresolution through specialized badges, reputation points, or recognition. Stack\nOverflow could introduce “Deep Learning Debug Rooms” for real-time, collab-\n46\nMehil B. Shah et al.\norative problem-solving, and GitHub could highlight top contributors in bug\nresolution. By combining Stack Overflow’s community-driven approach with\nGitHub’s comprehensive project management features, we can develop a more\nefficient ecosystem for deep learning bug resolution, leading to more robust\ndeep learning systems.\n7 Threats to Validity\nThreats to external validity relate to the generalizability of our findings. We\nhave reproduced multiple bugs from five different types to mitigate this threat.\nMoreover, we have reproduced bugs from 14 different architectures, ranging\nfrom Logistic Regression, CNN, Transformers and BERT-based architectures.\nFinally, we have also reproduced the bugs from the past six years (2017-2023),\nand our findings align with and extend the previous findings of software bug\nreproducibility [13], possibly indicating the generalizability of our study.\nThreats to internal validity pertain to experimental errors and confounding\nvariables during the reproduction of bugs. The possibility of introducing new\nbugs into the code exists during bug reproduction. We have implemented sev-\neral strategies and precautions to mitigate these threats in our experimental\ndesign. For instance, bug reproduction attempts are carried out in controlled\nand isolated Python virtual environments. We refrain from using hardcoded\nor fixed values in our code snippets to prevent potential errors during bug\nreproduction. For instance, during the bug reproduction, we utilize random\nhyperparameter initialization, construct multiple neural networks, and gen-\nerate randomized input data that aligns with the original distribution. This\nmethodology mitigates the influence of unforeseen variables on the outcomes\nof our experiments. To further mitigate this threat, we ran the updated code\nsnippets five times for all the edit actions. Following bug reproduction, we\nundertake rigorous manual analysis to identify any discrepancies or anomalies\nin the outcomes.\nA potential threat to the validity of our findings arises from the inclusion\nof context-specific, natural language hints from bug reports. This could po-\ntentially introduce bias when assessing the experimental group’s performance.\nHowever, our analysis of qualitative feedback from participants indicates that\nparticipants primarily benefited from the hints derived from our Apriori al-\ngorithm (RQ2), with minimal mention of context-specific hints from the bug\nreport. In particular, the participants reported using the guidance to generate\nappropriate datasets, address import issues, create necessary input data, and\napply specific edit actions, which are closely tied to our algorithm’s outputs\nrather than the natural language hints. Thus, the threat posed by the inclusion\nof natural language hints might be negligible.\nAnother threat to validity is the incomplete or incorrect information in\nthe bug report, which could impact the applicability of certain edit actions\nsuggested by our technique. In particular, edit actions that require additional\ncontext from the report, such as those involving input data generation, neural\nTitle Suppressed Due to Excessive Length\n47\nnetwork construction, dataset procurement, and downloading models/tokeniz-\ners, may be limited by missing details in the reports. However, the majority\nof our suggested edit actions focus on code modifications that can be derived\nfrom the provided code snippets without relying heavily on external context.\nTherefore, while missing information may constrain our capability for data and\nmodel-driven actions, our core technique and findings remain widely applicable\nfor many code-focused edit recommendations. To mitigate this threat further,\nwe apply extensive filtration based on the criteria suggested by Moravati et\nal. [12] and Humbatova et al. [16], which leads to a clean and generalizable\ndataset.\nFinally, the way in which the contextual information is presented to the\ndevelopers could potentially threaten the validity of the control and experiment\ngroups in the user study. To mitigate this threat, we ensure that the contextual\ninformation is presented in an identical format to the control and experiment\ngroups.\nThreats to construct validity relate to the use of appropriate metrics for\nevaluating the results of a user study. In the user study, we used the repro-\nducibility rate and time to reproduce the deep learning bugs as the metrics\nto evaluate the benefits of our findings. To measure these accurately, we had\nparticipants justify why and how they used the edit actions or found certain in-\nformation important. We verified these justifications against our ground truth\nto determine if the bug was successfully reproduced. After checking the user\njustifications, we calculated the reproducibility rate and spent time. By includ-\ning user justifications and verifying them, we ensured that the metrics directly\nmeasured how easily bugs could be reproduced. We also had clear criteria for\ndetermining reproduction success and used Opinio for precise timing data [58],\nmitigating threats related to measurement errors or subjective assessments.\nAnother threat to the validity might be the accuracy of tags used to cat-\negorize the bugs. The incorrect categorization of bugs might lead to incorrect\nconclusions for different bugs. To mitigate this threat, we only select the tags\nthat are present in the taxonomy and the sub-taxonomies defined by Humbat-\nova et al. [16]. For example, we specifically chose the “loss-function” as one of\nthe tags to distinguish the training bugs since the Loss Function falls within\nthe sub-taxonomy of Training bugs. Thus, the threats to construct validity\nwere effectively mitigated.\n8 Related Work\nThere have been several studies that focus on the aspects of reproducibility of\nsoftware bugs [13, 59, 60], or focus on why some bugs cannot be reproduced [14,\n61]. Many studies attempt to learn the nature of deep learning bugs [62, 8, 16,\n63], and how they can be localized automatically [64, 65, 66]. A few studies\nalso attempt to learn about the state of reproducibility of deep learning in\nsoftware engineering [67, 68], and reproduce deep learning bugs as a part of\nbenchmark dataset creation [12]. Unfortunately, only a little research has been\n48\nMehil B. Shah et al.\ndone to understand the challenges in reproducing deep learning bugs and how\nwe can improve their reproducibility.\nMondal et al. [13] extensively investigated the reproducibility of program-\nming issues reported on Stack Overflow. They identified several key edit ac-\ntions required to reproduce programming errors and traditional software bugs.\nWhile their work is a source of inspiration, it does not provide the edit ac-\ntions for reproducing deep learning bugs. Since their dataset is constructed in\nJava programming language with no questions related to deep learning, their\nfindings might not apply to deep learning bugs.\nChen et al. [67] proposed a unified framework to train reproducible deep\nlearning models. They also provide reproducibility guidelines and mitigation\nstrategies for conducting a reproducible training process for DL models. How-\never, the primary focus of their study was on the reproducibility of deep learn-\ning models, not deep learning bugs. Furthermore, the guidelines for deep learn-\ning models might not always extend to deep learning bugs.\nMoravati et al. [12] constructed the faultload benchmark containing deep\nlearning bugs, defects4ML. As a part of their benchmark dataset, they re-\nproduced 100 bugs as a part of the reproducibility criterion for benchmark\ndatasets. While their study was the first to reproduce deep learning bugs ac-\ntively, they did not report the techniques and actions used to reproduce bugs.\nFurthermore, they did not report the useful information from the issue reports,\nwhich helped them reproduce the bugs.\nUnlike many earlier studies above, we conduct an extensive empirical study\nto understand the challenges of deep learning bug reproduction and how we\ncan improve the reproducibility of deep learning bugs. We construct a dataset\nof 668 bugs and manually reproduce 148 bugs, spanning three frameworks, five\ntypes of bugs, and 14 architectures. We not only (1) define ten edit actions\nwhich can be used to reproduce deep learning bugs but also (2) explore the\nassociations among the type of bugs, the information required to reproduce\nthem, and the specific edit actions that can be used to reproduce them, which\nmakes our work novel. Our findings are also generalizable to Tensorflow and\nPyTorch bugs due to the diversity of our dataset. To ensure transparency and\nreproducibility, we have made our dataset and replication package publicly\navailable7.\n9 Conclusion & Future Work\nThe reproducibility of deep learning bugs is a significant challenge for soft-\nware practitioners, given that these bugs can lead to severe consequences. In\nthis paper, we conduct an empirical study by reproducing 148 deep learning\nbugs and identifying ten key edit actions that could reproduce these bugs.\nFurthermore, we investigate the relationships among bug types, component\ninformation, and edit actions. To validate our findings, we conducted a user\n7 https://github.com/mehilshah/Bug Reproducibility DL Bugs\nTitle Suppressed Due to Excessive Length\n49\nstudy with 22 participants. The developers equipped with our recommended\nedit actions and information could reproduce 22.92% more bugs using 24.35%\nless time compared to the developers without such hints. This demonstrates\nthe real-world value of our findings in improving the reproducibility of deep\nlearning bugs. Finally, we provide actionable insights about the state of deep\nlearning bugs that could improve the reproducibility of deep learning bugs.\nTo demonstrate the practical use of our findings, we show how they aid large\nlanguage models in improving the reproducibility of deep-learning bugs. For\nfuture work, we plan to expand our study to additional deep-learning frame-\nworks and model types. Moreover, we plan to conduct a large-scale study to\ndetermine the sufficiency of information in deep learning bug reports, similar\nto the study conducted by Mondal et al. [69]. Using these insights, we will build\na tool that automatically completes the reports for better bug reproducibility.\nFurthermore, our findings open up the ability to create automated systems\nfor reproducing deep learning bugs. By training machine learning models on\nour dataset of reproduction actions matched to bug reports, we can build AI\nagents that take bug reports as input and automatically generate reproducibil-\nity scripts, similar to the work by White et al. [70]. This would significantly\nreduce the manual effort required to reproduce bugs and enable large-scale\ndebugging and testing of deep learning systems.\nData Availability Statement (DAS)\nAll the data generated or analyzed during this study are available in the\nGitHub repository to help reproduce our results [35].\nConflict of Interest\nThe authors declare that they have no conflict of interest.\nAcknowledgements\nWe would like to thank Usmi Mukherjee for her invaluable contributions during\nthe manual bug reproduction process as an independent collaborator.\nReferences\n1. D. Shen, G. Wu, and H.-I. Suk, “Deep learning in medical image analysis,”\nAnnual review of biomedical engineering, vol. 19, pp. 221–248, 2017.\n2. P. M. Addo, D. Guegan, and B. Hassani, “Credit risk analysis using ma-\nchine and deep learning models,” Risks, vol. 6, no. 2, p. 38, 2018.\n50\nMehil B. Shah et al.\n3. D. S. Berman, A. L. Buczak, J. S. Chavis, and C. L. Corbett, “A survey\nof deep learning methods for cyber security,” Information, vol. 10, no. 4,\np. 122, 2019.\n4. K. Pei, Y. Cao, J. Yang, and S. Jana, “Deepxplore: Automated whitebox\ntesting of deep learning systems,” Commun. ACM, vol. 62, no. 11, p.\n137–145, oct 2019. [Online]. Available: https://doi.org/10.1145/3361566\n5. L. Ma, F. Juefei-Xu, F. Zhang, J. Sun, M. Xue, B. Li, C. Chen, T. Su, L. Li,\nY. Liu, J. Zhao, and Y. Wang, “Deepgauge: Multi-granularity testing\ncriteria for deep learning systems,” in Proceedings of the 33rd ACM/IEEE\nInternational Conference on Automated Software Engineering, ser. ASE\n’18.\nNew York, NY, USA: Association for Computing Machinery, 2018,\np. 120–131. [Online]. Available: https://doi.org/10.1145/3238147.3238202\n6. A. Esteva, A. Robicquet, B. Ramsundar, V. Kuleshov, M. DePristo,\nK. Chou, C. Cui, G. Corrado, S. Thrun, and J. Dean, “A guide to deep\nlearning in healthcare,” Nature medicine, vol. 25, no. 1, pp. 24–29, 2019.\n7. H. B. Braiek and F. Khomh, “On testing machine learning pro-\ngrams,” Journal of Systems and Software, vol. 164, p. 110542, 2020.\n[Online]. Available: https://www.sciencedirect.com/science/article/pii/\nS0164121220300248\n8. M. J. Islam, G. Nguyen, R. Pan, and H. Rajan, “A comprehensive\nstudy on deep learning bug characteristics,” ser. ESEC/FSE 2019.\nNew\nYork, NY, USA: Association for Computing Machinery, 2019, p. 510–520.\n[Online]. Available: https://doi.org/10.1145/3338906.3338955\n9. D. Wakabayashi, “Self-driving uber car kills pedestrian in arizona,\nwhere robots roam,” Mar 2018, accessed on December 17, 2023.\n[Online]. Available: https://www.nytimes.com/2018/03/19/technology/\nuber-driverless-fatality.html\n10. P. Nagarajan, G. Warnell, and P. Stone, “The impact of nondeterminism\non reproducibility in deep reinforcement learning,” 2018.\n11. M. Krishnan, “Against interpretability: a critical examination of the in-\nterpretability problem in machine learning,” Philosophy & Technology,\nvol. 33, no. 3, pp. 487–502, 2020.\n12. M. M. Morovati, A. Nikanjam, F. Khomh, and Z. M. J. Jiang,\n“Bugs in machine learning-based systems: A faultload benchmark,”\nEmpirical Softw. Engg., vol. 28, no. 3, apr 2023. [Online]. Available:\nhttps://doi.org/10.1007/s10664-023-10291-1\n13. S. Mondal, M. M. Rahman, and C. K. Roy, “Can issues reported at\nstack overflow questions be reproduced? an exploratory study,” in 2019\nIEEE/ACM 16th International Conference on Mining Software Reposito-\nries (MSR), 2019, pp. 479–489.\n14. M. M. Rahman, F. Khomh, and M. Castelluccio, “Why are some bugs non-\nreproducible? : –an empirical investigation using data fusion–,” in 2020\nIEEE International Conference on Software Maintenance and Evolution\n(ICSME), 2020, pp. 605–616.\n15. Y. Liang, Y. Lin, X. Song, J. Sun, Z. Feng, and J. S. Dong, “gdefects4dl:\na dataset of general real-world deep learning program defects,” in Pro-\nTitle Suppressed Due to Excessive Length\n51\nceedings of the ACM/IEEE 44th International Conference on Software\nEngineering: Companion Proceedings, 2022, pp. 90–94.\n16. N. Humbatova, G. Jahangirova, G. Bavota, V. Riccio, A. Stocco, and\nP. Tonella, “Taxonomy of real faults in deep learning systems,” in Pro-\nceedings of the ACM/IEEE 42nd International Conference on Software\nEngineering, ser. ICSE ’20.\n17. R. Agrawal and R. Srikant, “Fast algorithms for mining association rules\nin large databases,” in Proceedings of the 20th International Conference\non Very Large Data Bases, ser. VLDB ’94.\nSan Francisco, CA, USA:\nMorgan Kaufmann Publishers Inc., 1994, p. 487–499.\n18. 2019,\naccessed\non\nJanuary\n3,\n2024.\n[Online].\nAvailable:\nhttps:\n//stackoverflow.com/q/58190114\n19. K. Team, “Keras documentation: Python & numpy utilities.” [Online].\nAvailable: https://keras.io/2.16/api/utils/python utils/#sequence-class\n20. 2018,\naccessed\non\nDecember\n28,\n2023.\n[Online].\nAvailable:\nhttps:\n//stackoverflow.com/q/50920908\n21. M. J. Islam, G. Nguyen, R. Pan, and H. Rajan, “A comprehensive study\non deep learning bug characteristics,” in Proceedings of the 2019 27th\nACM Joint Meeting on European Software Engineering Conference and\nSymposium on the Foundations of Software Engineering, ser. ESEC/FSE\n2019.\nNew York, NY, USA: Association for Computing Machinery, 2019,\np. 510–520. [Online]. Available: https://doi.org/10.1145/3338906.3338955\n22. R. Croft, M. A. Babar, and M. M. Kholoosi, “Data quality for software\nvulnerability datasets,” in 2023 IEEE/ACM 45th International Confer-\nence on Software Engineering (ICSE).\nIEEE, 2023, pp. 121–133.\n23. S. Jahan, M. B. Shah, and M. M. Rahman, “Towards understanding the\nchallenges of bug localization in deep learning systems,” arXiv preprint\narXiv:2402.01021, 2024.\n24. J. Sanders and E. Kandrot, CUDA by example: an introduction to general-\npurpose GPU programming.\nAddison-Wesley Professional, 2010.\n25. A. Aviram, S.-C. Weng, S. Hu, and B. Ford, “Efficient system-enforced\ndeterministic parallelism,” Communications of the ACM, vol. 55, no. 5,\npp. 111–119, 2012.\n26. K. Zhang, B. He, J. Hu, Z. Wang, B. Hua, J. Meng, and L. Yang, “{G-\nNET}: Effective {GPU} sharing in {NFV} systems,” in 15th USENIX\nSymposium on Networked Systems Design and Implementation (NSDI 18),\n2018, pp. 187–200.\n27. D. Tiwari, S. Gupta, J. Rogers, D. Maxwell, P. Rech, S. Vazhkudai,\nD. Oliveira, D. Londo, N. DeBardeleben, P. Navaux et al., “Understanding\ngpu errors on large-scale hpc systems and the implications for system de-\nsign and operation,” in 2015 IEEE 21st International Symposium on High\nPerformance Computer Architecture (HPCA).\nIEEE, 2015, pp. 331–342.\n28. G. Long and T. Chen, “On reporting performance and accuracy bugs for\ndeep learning frameworks: An exploratory study from github,” in Proceed-\nings of the 26th International Conference on Evaluation and Assessment\nin Software Engineering, 2022, pp. 90–99.\n52\nMehil B. Shah et al.\n29. S. Shi, Q. Wang, P. Xu, and X. Chu, “Benchmarking state-of-the-art deep\nlearning software tools,” in 2016 7th International Conference on Cloud\nComputing and Big Data (CCBD).\nIEEE, 2016, pp. 99–104.\n30. S. Exchange, “All sites - stack exchange,” accessed on December 12,\n2023. [Online]. Available: https://stackexchange.com/sites?view=list\n31. H. Zhao, Y. Li, F. Liu, X. Xie, and L. Chen, “State and tendency: an em-\npirical study of deep learning question&answer topics on stack overflow,”\nScience China Information Sciences, vol. 64, pp. 1–23, 2021.\n32. T. Zhang, C. Gao, L. Ma, M. Lyu, and M. Kim, “An empirical study\nof common challenges in developing deep learning applications,” in 2019\nIEEE 30th International Symposium on Software Reliability Engineering\n(ISSRE).\nIEEE, 2019, pp. 104–115.\n33. L. Ponzanelli, A. Mocci, A. Bacchelli, and M. Lanza, “Understanding and\nclassifying the quality of technical forum questions,” in 2014 14th Inter-\nnational Conference on Quality Software, 2014, pp. 343–352.\n34. [Online]. Available: https://data.stackexchange.com/\n35. M. Shah, “mehilshah/bug reproducibility dl bugs,” accessed on Jan-\nuary\n3,\n2024.\n[Online].\nAvailable:\nhttps://github.com/mehilshah/\nBug Reproducibility DL Bugs\n36. W. G. Cochran, Sampling techniques.\njohn wiley & sons, 1977.\n37. Jan\n2023,\naccessed\non\nDecember\n25,\n2023.\n[Online].\nAvailable:\nhttps://www.geeksforgeeks.org/best-ides-for-machine-learning/\n38. M. L. McHugh, “Interrater reliability: the kappa statistic,” Biochemia\nmedica, vol. 22, no. 3, pp. 276–282, 2012.\n39. F. Tambon, A. Nikanjam, L. An, F. Khomh, and G. Antoniol, “Silent bugs\nin deep learning frameworks: an empirical study of keras and tensorflow,”\nEmpirical Software Engineering, vol. 29, no. 1, p. 10, 2024.\n40. H. V. Pham, S. Qian, J. Wang, T. Lutellier, J. Rosenthal, L. Tan, Y. Yu,\nand N. Nagappan, “Problems and opportunities in training deep learn-\ning software systems: An analysis of variance,” in Proceedings of the 35th\nIEEE/ACM international conference on automated software engineering,\n2020, pp. 771–783.\n41. S. S. Alahmari, D. B. Goldgof, P. R. Mouton, and L. O. Hall, “Challenges\nfor the repeatability of deep learning models,” IEEE Access, vol. 8, pp.\n211 860–211 868, 2020.\n42. M. Gori, A. Betti, and S. Melacci, Machine Learning: A constraint-based\napproach.\nElsevier, 2023.\n43. B. Liu, W. Hsu, and Y. Ma, “Mining association rules with multiple min-\nimum supports,” in Proceedings of the fifth ACM SIGKDD international\nconference on Knowledge discovery and data mining, 1999, pp. 337–341.\n44. Jun 2024. [Online]. Available: https://opencv.org/\n45. Jun 2024. [Online]. Available: https://python-pillow.org/\n46. A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever et al., “Lan-\nguage models are unsupervised multitask learners,” OpenAI blog, vol. 1,\nno. 8, p. 9, 2019.\nTitle Suppressed Due to Excessive Length\n53\n47. [Online].\nAvailable:\nhttps://machinelearningmastery.com/\nbuilding-a-logistic-regression-classifier-in-pytorch/\n48. PyTorch. [Online]. Available: https://pytorch.org/docs/1.6.0/\n49. E. Breck, N. Polyzotis, S. Roy, S. Whang, and M. Zinkevich, “Data vali-\ndation for machine learning.” in MLSys, 2019.\n50. M. Soltani, F. Hermans, and T. B¨ack, “The significance of bug report\nelements,” Empirical Software Engineering, vol. 25, pp. 5255–5294, 2020.\n51. B. Chen and Z. M. J. Jiang, “A survey of software log instrumentation,”\nACM Computing Surveys, vol. 54, no. 4, p. 1–34, May 2022. [Online].\nAvailable: https://dl.acm.org/doi/10.1145/3448976\n52. D. Talwar, S. Guruswamy, N. Ravipati, and M. Eirinaki, “Evaluating\nvalidity of synthetic data in perception tasks for autonomous vehicles,”\nin 2020 IEEE International Conference On Artificial Intelligence Testing\n(AITest).\nIEEE, 2020, pp. 73–80.\n53. R. Grosse, “Lecture 15: Exploding and vanishing gradients,” University of\nToronto Computer Science, 2017.\n54. N. Tishby and N. Zaslavsky, “Deep learning and the information\nbottleneck principle,” in 2015 IEEE Information Theory Workshop\n(ITW), Apr. 2015, p. 1–5. [Online]. Available: https://ieeexplore.ieee.\norg/abstract/document/7133169\n55. Y. Yang, T. He, Z. Xia, and Y. Feng, “A comprehensive empirical study on\nbug characteristics of deep learning frameworks,” Information and Soft-\nware Technology, vol. 151, p. 107004, 2022.\n56. J. A. Nelder and R. W. Wedderburn, “Generalized linear models,” Journal\nof the Royal Statistical Society Series A: Statistics in Society, vol. 135,\nno. 3, pp. 370–384, 1972.\n57. M. Ceccato, M. Di Penta, P. Falcarin, F. Ricca, M. Torchiano, and\nP. Tonella, “A family of experiments to assess the effectiveness and ef-\nficiency of source code obfuscation techniques,” Empirical Software Engi-\nneering, vol. 19, pp. 1040–1074, 2014.\n58. [Online]. Available: https://surveys.dal.ca/opinio/admin/folder.do\n59. S. Mondal, M. M. Rahman, C. K. Roy, and K. Schneider, “The repro-\nducibility of programming-related issues in stack overflow questions,” Em-\npirical Software Engineering, vol. 27, no. 3, p. 62, 2022.\n60. S. Mondal and B. Roy, “Reproducibility of issues reported in stack overflow\nquestions: Challenges, impact & estimation,” Impact & Estimation.\n61. M. M. Rahman, F. Khomh, and M. Castelluccio, “Works for me! cannot\nreproduce–a large scale empirical study of non-reproducible bugs,” Em-\npirical Software Engineering, vol. 27, no. 5, p. 111, 2022.\n62. Y. Zhang, Y. Chen, S.-C. Cheung, Y. Xiong, and L. Zhang, “An\nempirical study on tensorflow program bugs,” in Proceedings of the\n27th ACM SIGSOFT International Symposium on Software Testing\nand Analysis, ser. ISSTA 2018.\nNew York, NY, USA: Association\nfor\nComputing\nMachinery,\n2018,\np.\n129–140.\n[Online].\nAvailable:\nhttps://doi.org/10.1145/3213846.3213866\n54\nMehil B. Shah et al.\n63. T. Makkouk, D. J. Kim, and T.-H. P. Chen, “An empirical study on per-\nformance bugs in deep learning frameworks,” in 2022 IEEE International\nConference on Software Maintenance and Evolution (ICSME), 2022, pp.\n35–46.\n64. F. Jafarinejad, K. Narasimhan, and M. Mezini, “Nerdbug: Automated\nbug detection in neural networks,” in Proceedings of the 1st ACM\nInternational Workshop on AI and Software Testing/Analysis, ser. AISTA\n2021.\nNew York, NY, USA: Association for Computing Machinery, 2021,\np. 13–16. [Online]. Available: https://doi.org/10.1145/3464968.3468409\n65. M. Yan, J. Chen, X. Zhang, L. Tan, G. Wang, and Z. Wang, “Exposing nu-\nmerical bugs in deep learning via gradient back-propagation,” in Proceed-\nings of the 29th ACM Joint Meeting on European Software Engineering\nConference and Symposium on the Foundations of Software Engineering,\n2021, pp. 627–638.\n66. Y. Zhang, L. Ren, L. Chen, Y. Xiong, S.-C. Cheung, and T. Xie, “Detect-\ning numerical bugs in neural network architectures,” in Proceedings of the\n28th ACM Joint Meeting on European Software Engineering Conference\nand Symposium on the Foundations of Software Engineering, 2020, pp.\n826–837.\n67. B. Chen, M. Wen, Y. Shi, D. Lin, G. K. Rajbahadur, and Z. M. J. Jiang,\n“Towards training reproducible deep learning models,” in Proceedings of\nthe 44th International Conference on Software Engineering, ser. ICSE ’22.\nNew York, NY, USA: Association for Computing Machinery, 2022, p.\n2202–2214. [Online]. Available: https://doi.org/10.1145/3510003.3510163\n68. C. Liu, C. Gao, X. Xia, D. Lo, J. Grundy, and X. Yang, “On the\nreproducibility and replicability of deep learning in software engineering,”\nACM Trans. Softw. Eng. Methodol., vol. 31, no. 1, oct 2021. [Online].\nAvailable: https://doi.org/10.1145/3477535\n69. S. Mondal, M. M. Rahman, and C. K. Roy, “Can we identify stack over-\nflow questions requiring code snippets? investigating the cause & effect of\nmissing code snippets,” arXiv preprint arXiv:2402.04575, 2024.\n70. M. White, M. Linares-V´asquez, P. Johnson, C. Bernal-C´ardenas, and\nD. Poshyvanyk, “Generating reproducible and replayable bug reports from\nandroid application crashes,” in 2015 IEEE 23rd International Conference\non Program Comprehension, 2015, pp. 48–59.\n71. A. Rutherford, ANOVA and ANCOVA: a GLM approach.\nJohn Wiley &\nSons, 2011.\nTitle Suppressed Due to Excessive Length\n55\nAppendix A: Flowchart for Manual Analysis\nThe flowchart in Fig. 6 illustrates a systematic process for analyzing Stack Overflow posts\nrelated to deep learning bugs. It begins by determining if the post discusses a deep-learning\nbug. If it does, the bug category is identified based on the symptoms, potential root causes,\npost tags and explanation by the user. The flowchart then guides the user through a series of\nchecks to assess the reproducibility of the bug, considering factors such as the completeness\nof the code snippet, availability of necessary data, presence of relevant logs or error mes-\nsages, specification of hyperparameters, and clarity of the model structure. We then try to\nreproduce the bug. If the reproduction is successful with extra steps, the steps, information,\nand assumptions needed are documented, or else the bug is marked as non-reproducible.\nFig. 6: Manual Analysis of Stack Overflow Posts\n56\nMehil B. Shah et al.\nAppendix B: Improving the Reproducibility of Deep Learning Bugs\nusing LLMs\nIn this section, we further aim to verify the practical utility of our findings in bug repro-\nduction. To achieve this, we leverage the capabilities of a large language model (LLM) and\noffer more actionable recommendations to developers.\nWe conduct experiments using the LLaMA3, a 70B parameter LLM, to evaluate its\neffectiveness in assisting developers with the reproduction of deep learning bugs. To conduct\nthese experiments, we construct a dataset of 40 bugs. We used stratified random sampling\nand collect 40 of our manually reproduced bugs, with the following distribution: 10 Training,\n10 API, 10 Tensor and 10 Model Bugs. For each of these four bug categories, we ensure that\nthe dataset contains an equal number of easy and difficult bugs, determined by the number\nof edit actions required to reproduce the bug. Specifically, within each bug category, there\nare 5 bugs that require a relatively small number of edit actions (classified as easy), and\n5 bugs that require a larger number of edit actions (classified as difficult). The prompt\nfor the LLaMA3 model contained four key inputs: the bug report describing the issue,\nrelevant code snippets providing context, the instruction to generate a code snippet that\ncould reproduce the reported bug, and, crucially, the edit actions and component information\nderived from our findings. Fig. 7 demonstrates an example prompt for our experiments. Once\nthe model generates the code snippets, we check their ability to reproduce the bugs through\nmanual execution and comparison with the erroneous behaviour outlined in the bug reports.\nFurthermore, we compare the generated code with the ground truth code snippets curated\nduring our manual bug reproduction process.\nFig. 7: Prompt for LLaMA3\nThe results of our experiments are as follows:\n– Performance without Augmentation: When the LLM was provided with only the\nbug report and code snippet, its suggestions were less useful. Its generated code snippets\nlack correctness, despite being compilable and executable. The absence of guidance from\nour findings diminishes the LLM’s performance, with the model generating reproducible\ncode snippets for only 25 out of the 40 bugs (62.5%).\n– Performance with Augmentation using Hints: When guided by our edit actions\nand component information, the LLM generates reproducible code snippets for 33 out\nof the 40 bugs (82.5%). Analysis of the generated code snippets reveals that the model\neffectively understands the bug, uses appropriate edit actions, and generates complete,\ncompilable, and executable code snippets. Additionally, in some scenarios, the model\nprovides a textual description of the circumstances under which the bug might be re-\nproducible, offering valuable extra information to developers. Code snippets generated\nby Llama3, which demonstrate the model’s improved understanding after augmentation\nusing hints, are also available in the replication package [35].\nTitle Suppressed Due to Excessive Length\n57\nManual Analysis of Generated Code Snippets: We perform manual analysis of\nthe generated code snippets to understand the behaviour of LLMs. From the manual anal-\nysis of the generated code snippets, we observed that the LLM performs relatively well in\ngenerating code snippets for Training and Model bugs, even without augmentation. This\nbehaviour could be explained by the high prevalence of these bugs in real-life scenarios [16].\nOn the other hand, the LLM faces more challenges in generating the code snippets for re-\nproducing the Tensor and API bugs without guidance. Tensor bugs usually involve complex\nmanipulations of tensor shapes and dimensions, which might not be fully understood by\nLLMs. Similarly, API bugs are sensitive to constant updates to the API documentation.\nThe frequent changes to the API documentation make it harder for LLMs to stay up-to-\ndate and generate accurate code snippets to reproduce these bugs. However, when provided\nwith our guidance, the reproducibility of API bugs goes up from 56% to 76%, and Tensor\nbugs reproducibility goes up from 58% to 75%. This demonstrates the utility of our findings\nin improving the ability of language models to generate code snippets for bugs that are\nrelatively difficult to reproduce.\nOur experiments demonstrate that augmenting the LLM with our suggested edit actions\nand component information enhances its effectiveness in reproducing deep learning bugs by\n20%. This improvement underscores the potential of integrating our findings with LLMs to\nprovide developers with more actionable guidance for reproducing deep learning bugs.\nStatistical Significance Tests for our Experiments\nSince the recommendations of LLMs are stochastic by design, different runs with the same\nprompt can produce different reproducible snippets. This introduces an additional source\nof variability which may invalidate our previous observations. To address this issue, we\nrepeat the code generation task for each bug five times and record their rates of success\nin bug reproduction. To assess the statistical significance of the LLM results, we employ\nthe RM-ANOVA test [71]. RM-ANOVA is suitable for this scenario because we have a\nwithin-subjects design with one independent variable (augmentation with hints or not) and\none dependent variable (bug reproduction rate) measured repeatedly across trials. The test\nyields a significant p-value of 5.8 × 10−5, demonstrating the positive impact of hints on bug\nreproduction abilities of large language models. Additionally, we have used the partial-eta\nsquared (η2\np) metric for the effect size, as it is a common measure in ANOVA representing\nthe proportion of total variance attributable to a factor adjusted for other factors in the\nmodel. The η2\np for the Guidance factor (hint augmentation) is 0.188324, which is considered\na large effect size. This substantial effect size, coupled with the significant p-value, provides\nstrong evidence for both the statistical and practical significance of using hints to improve\nLLM performance in bug reproduction tasks.\n",
  "categories": [
    "cs.SE",
    "cs.LG"
  ],
  "published": "2024-01-05",
  "updated": "2024-10-22"
}