{
  "id": "http://arxiv.org/abs/2401.03175v1",
  "title": "Part-of-Speech Tagger for Bodo Language using Deep Learning approach",
  "authors": [
    "Dhrubajyoti Pathak",
    "Sanjib Narzary",
    "Sukumar Nandi",
    "Bidisha Som"
  ],
  "abstract": "Language Processing systems such as Part-of-speech tagging, Named entity\nrecognition, Machine translation, Speech recognition, and Language modeling\n(LM) are well-studied in high-resource languages. Nevertheless, research on\nthese systems for several low-resource languages, including Bodo, Mizo,\nNagamese, and others, is either yet to commence or is in its nascent stages.\nLanguage model plays a vital role in the downstream tasks of modern NLP.\nExtensive studies are carried out on LMs for high-resource languages.\nNevertheless, languages such as Bodo, Rabha, and Mising continue to lack\ncoverage. In this study, we first present BodoBERT, a language model for the\nBodo language. To the best of our knowledge, this work is the first such effort\nto develop a language model for Bodo. Secondly, we present an ensemble DL-based\nPOS tagging model for Bodo. The POS tagging model is based on combinations of\nBiLSTM with CRF and stacked embedding of BodoBERT with BytePairEmbeddings. We\ncover several language models in the experiment to see how well they work in\nPOS tagging tasks. The best-performing model achieves an F1 score of 0.8041. A\ncomparative experiment was also conducted on Assamese POS taggers, considering\nthat the language is spoken in the same region as Bodo.",
  "text": "Accepted to Natural Language Engineering\nARTICLE\nPart-of-Speech Tagger for Bodo Language using Deep\nLearning approach\nDhrubajyoti Pathak, Sanjib Narzary, Sukumar Nandi, and Bidisha Som\nCentre for Linguistic Science and Technology,\nIIT Guwahati\ndrbj153@iitg.ac.in\nCentre for Linguistic Science and Technology,\nIIT Guwahati\nsanjib_narzary@iitg.ac.in\nCentre for Linguistic Science and Technology,\nIIT Guwahati\nsukumar@iitg.ac.in\nCentre for Linguistic Science and Technology,\nIIT Guwahati\nbidisha@iitg.ac.in\nAbstract\nLanguage Processing systems such as Part-of-speech tagging, Named entity recognition,\nMachine translation, Speech recognition, and Language modeling (LM) are well-studied in\nhigh-resource languages. Nevertheless, research on these systems for several low-resource lan-\nguages, including Bodo, Mizo, Nagamese, and others, is either yet to commence or is in its\nnascent stages. Language model plays a vital role in the downstream tasks of modern NLP.\nExtensive studies are carried out on LMs for high-resource languages. Nevertheless, languages\nsuch as Bodo, Rabha, and Mising continue to lack coverage. In this study, we first present\nBodoBERT, a language model for the Bodo language. To the best of our knowledge, this work\nis the first such effort to develop a language model for Bodo. Secondly, we present an ensemble\nDL-based POS tagging model for Bodo. The POS tagging model is based on combinations\nof BiLSTM with CRF and stacked embedding of BodoBERT with BytePairEmbeddings. We\ncover several language models in the experiment to see how well they work in POS tagging\ntasks. The best-performing model achieves an F1 score of 0.8041. A comparative experiment\nwas also conducted on Assamese POS taggers, considering that the language is spoken in the\nsame region as Bodo.\n1. Introduction\nPart-of-speech (POS) tagging is one of the building blocks of modern Natural Language\nProcessing (NLP). POS tagging automatically assigns grammatical class types to words in\na sentence, such as Noun, Pronoun, Verb, Adjective, Adverb, Conjunction, Punctuation,\netc. Various approaches are there for automatic POS tagging; however, the Deep neural\nnetwork approach has achieved state-of-the-art (SOTA) accuracy for resource-rich lan-\nguages. POS tagging plays a vital role in various text processing tasks, such as Named\nentity recognition (NER) (Aguilar et al. 2019), Machine translation (MT) (Niehues and\nCho 2017), Information extraction (IE) (Bhutani et al. 2019), Question Answering (QA)\n(Le-Hong and Bui 2018) and Constituency parsing (Shen et al. 2018).\nTherefore, a\n2\nwell-performing POS tagger is inevitable in developing a successful NLP system for a\nlanguage.\nBodo (Boro) is a Tibeto-Burmana morphologically rich language, mainly spoken in\nAssam, a state in northeastern India. The Bodoland Territorial Region (BTR), an inde-\npendent entity in Assam, uses it as its oﬀicial language. The Devanagari script is used\nto write Bodo.\nAs per the 2011 Indian census, Bodo has about 1.5 million speakers,\nmaking it the 20th most spoken language in India among the 22 scheduled languages.\nEven though most Bodo speakers are ethnic Bodos, Assamese, Rabha, Koch Rajbongshi,\nSanthali, Garo, and the Muslim community in the Bodoland Territorial Region also speak\nthe language.\nHowever, the state of the NLP systems for Bodo is in a very nascent stage.\nThe\nstudy of fundamental NLP tasks in Bodo language, such as Lemmatization, Dependency\nparser, Language Identification, Language modeling, POS tagging, and NER has yet\nto start.\nWhile Word Embeddings or Language models play a crucial role in a Deep\nLearning approach, we observe that no existing pre-trained Language Models cover the\nBodo language. As a result, we could not find any downstream NLP tools developed using\na deep learning method.\nWith 1.5 million Bodo speakers, the need for developing NLP resources for the Bodo\nlanguage is highly demanding. Motivated by this gap in research of the NLP areas of the\nBodo language, we present the first language model for Bodo language- BodoBERT based\non BERT architecture (Devlin et al. 2018). We prepare a monolingual training corpus\nto train the LM by collecting and acquiring corpus from various sources. After that, we\ndevelop a POS tagger for Bodo language by employing the BodoBERT.\nWe explore different sequence labeling architectures to get the best POS tagging model.\nWe conduct three experiments to train the POS tagging models- a) Fine-tuning based,\nb) Conditional Random Field (CRF) based, and c) Long-short term memory (BiLSTM)-\nCRF based. Among the three, the BiLSTM-CRF-based POS tagging model performs\nbetter than the others.\nBodo language uses the Devanagari script, which the Hindi language also uses.\nTherefore, we conducted the POS tagging experiment with LMs that were trained in\nHindi to compare the performance with the BodoBERT. We cover LMs such as Fasttext,\nByte Pair Embeddings (BPE), Contextualise Character Embedding (FlairEmbedding),\nMuRIL, XLM-R, and IndicBERT. We employ two different methods to embed the words\nin the sentence to train POS tagging models using BiLSTM-CRF architecture- Individual\nand Stacked methods. In the Individual method, the model trained using BodoBERT\nachieves the highest F1 score of 0.7949.\nAfter that, we experiment with the Stacked\nmethod to combine the performance of the BodoBERT with other LMs. The highest F1\nscore in the stacked method reached 0.8041. We believe this is not only the first Neural\nNetwork-based POS tagging model for the language but also the first POS tagger in Bodo.\nOur contributions can be summarized as follows:\n• Proposed a Language model for Bodo language based on the BERT framework. To\nthe best of our knowledge, this is the first Language model for Bodo.\n• Presented comparison of different POS tagging models for Bodo by employing state-\nof-the-art sequence tagger frameworks such as CRF, Fine-tuned LMs, and BiLSTM-\nCRF.\naThe Sino-Tibetan language family includes the Tibeto-Burman languages. Tibeto-Burman language does\nnot include Chinese languages.\n3\n• Comparison of POS tagging performance of different LMs in POS tagging mod-\nels such as Fasttext, BPE, XLM-R, FlairEmbedding, IndicBERT, and MuRIL\nembedding using Individual and Stacked embedding methods.\n• The top-performing Bodo POS tagging model and BodoBERT are made publicly\navailable b.\nThis paper is organized as follows- Section 2 describes related works on POS tagging\nfor similar language. The details about the pre-training of BodoBERT and Bodo corpus\nused in training are presented in Section 3. Section 4 presents the experiments carried\nout to develop the Neural Network POS tagger. The Section also includes the description\nof the annotated dataset and POS tagset in the experiment. In Section 5, we present all\nthe experiment results of all three models using different sequence tagging architectures.\nFinally, we conclude our paper in Section 6.\n2. Related Work\nThis section presents related works about POS tagging in different languages. In our\nliterature study, we do not find any prior work on Language Modeling for Bodo Langauge.\nFurthermore, to the best of our knowledge, there is no previous work on POS tagging for\nBodo Language. Therefore, we discuss recent research works reported on neural network-\nbased POS tagging in various low-resource languages belonging to the northeastern part\nof India.\nIn paper (Pathak et al. 2022), an Assamese POS tagger based on Deep Learning (DL)\napproach is proposed. Various DL architectures are used to develop the POS model, which\nincludes CRF, Bidirectional Long Short-term Memory (BiLSTM) with CRF, and Gated\nRecurrent Unit (GRU) with CRF. The BiLSTM-CRF model achieved the highest tagging\nF1 score of 86.52. Pathak et al. (2023) presented an ensemble POS tagger for Assamese,\nwhere two DL-based taggers and a rule-based tagger are combined. The ensemble model\nachieved an F1 score of 0.925 in POS tagging. Warjri et al. (2021) presented a POS-\ntagged corpus for Khasi language. The paper also presented a DL-based POS tagger for\nthe language using different architectures, including BiLSTM, CRF, and Character-based\nembedding with BiLSTM. The top performing tagger achieved an accuracy of 96.98%\nusing BiLSTM with CRF technique. Alam et al. (2016) proposed a neural network-based\nPOS tagging system for Bangla using BiLSTM and CRF architecture. They also used a\npre-trained word embedding model during training. The model achieved an accuracy of\n86.0%. In paper (Pandey et al. 2022), reported work on POS tagging of the Mizo lan-\nguage. They employed classical LSTM and Quantum-enhanced Long short-term memory\n(QLSTM) in training and reported a tagging accuracy of 81.86% using LSTM network.\nKabir et al. (2016) presented their research on POS tagger for Bengali language using\na DL-based approach.\nThe tagger achieves an accuracy of 93.33% using Deep Belief\nNetwork (DBN) architecture. There are other works that have been reported over the\nyears on POS tagging for a variety of languages, including Assamese, Mizo, Manipuri,\nKhasi, and Bengali. However, they are based on traditional methods such as Rule-based\nand HMM-based models.\nbhttps://anonymous.4open.science/status/BodoPoS-4C10\n4\n3. BodoBERT\nThe Transformer-based language model BERT achieves state-of-the-art performance on\nmany NLP tasks. However, the success of BERT models on downstream NLP tasks is\nmostly limited to high-resource languages such as English, Chinese, Spanish, Arabic, etc.\nThe BERT model is trained in English (Devlin et al. 2018). After that, the multilingual\npre-trained models were released for 104 languages. However, Bodo language is not cov-\nered. We could not find any pre-trained language models that cover the Bodo language.\nSo, this motivates us to develop a language model for Bodo using the vanilla BERT (Devlin\net al. 2018) architecture.\n3.1 Bodo raw corpus\nA large monolingual corpus is required to train a well-performed language model like\nBERT. Moreover, training BERT is a computationally intensive task, and it requires\nsubstantial hardware resources. On the other hand, getting decent monolingual raw corpus\nfor Bodo has been an enduring problem for NLP research community. Although Bodo has\na rich oral literary tradition, however, there was no standard writing script for writing\nuntil the year 2003. After a long history of the Bodo script movement, the Bodo language\nis recognized as a scheduled language of India by the Government of India, and the\nDevanagari script for writing is oﬀicially adopted.\nWe have curated the corpus after acquiring it from the Linguistic Data Consortium for\nIndian Languages (LDC-IL) (Ramamoorthy et al. 2019; Choudhary 2021). The text of\nthe raw corpus is from different domains such as Aesthetics (Culture, Cinema, Literature,\nBiographies, and Folklore), Commerce, Mass media (Classified, Discussion, Editorial,\nSports, General news, Health, Weather, and Social), Science and Technology (Agriculture,\nEnvironmental Science, Textbook, Astrology, Mechanical Engineering, and Environmental\nScience) and Social Sciences (Economics, Education, Political Science, Linguistics, Health\nand Family Welfare, History, Text Book, Law, etc). We also acquired another corpus from\nthe work (Narzary et al. 2022). The final consolidated corpus has 1.6 million tokens and\n191k sentences.\n3.2 BodoBERT model training\nThe architecture of the BodoBERT model is based on a multi-layer bidirectional\nTransformer framework (Vaswani et al. 2017).\nWe use the BERT framework (Devlin\net al. 2018) to train the LM using the masked language model and next-sentence pre-\ndiction tasks. WordPiece tokenizer (Wu et al. 2016) is used for embeddings with 50,000\nvocabularies. The BERT model architecture is described in the guide “The Annotated\nTransformer”c and the implementation details are provided in “Tensorflow code and BERT\nmodel”d.\nThe model is trained with six layers of transformers block with a hidden layer size of 768\nand the number of self-attention heads as 6. There are approximately 103M parameters.\nThe model was trained on Nvidia Tesla P100 GPU (3584 Cuda Cores) for 300K steps with\na maximum sequence length of 128 and batch size of 64. We used the Adam optimizer\n(Kingma and Ba 2014) with a warm-up of the first 3000 steps to a peak learning rate of\n2e-5. The pre-training took approximately seven days to complete.\nchttp://nlp.seas.harvard.edu/2018/04/03/attention.html\ndhttps://github.com/google-research/bert\n5\n4. Bodo POS Tagger\nPart-of-speech tagging belongs to the sequence labeling tasks. There are different stages\nin developing a DL-based POS tagger. This section presents various stages of development\nof the Bodo POS tagger.\n4.1 Annotated Dataset\nA DL-based method requires a large size of properly annotated corpus to train a POS\ntagging model. The performance of a well-performed tagging model depends upon the\nquality of the annotated dataset. On the other hand, the availability of annotated corpus\nin the public domain is very rare.\nMoreover, it is also a tedious and time-consuming\ntask to annotate a corpus manually. Therefore, it is a challenging task to build a deep\nlearning model for a low-resource language. In our literature study, we could find only\none Bodo annotated corpus - Bodo Monolingual Text Corpus (ILCI-II 2020b). The cor-\npus is tagged by language experts manually as part of the project Indian Languages\nCorpora Initiative Phase–II (ILCI Phase- II), initiated by the Ministry of Electronics and\nInformation Technology (MeitY), Government of India, and Jawaharlal Nehru University,\nNew Delhi. The corpus contains approximately 30k sentences and 240k tokens comprised\nof different domains. The statistics of the annotated dataset are reported in Table 1. The\ncorpus is tagged according to the Bureau of Indian Standards (BIS) tagset. We use this\ndataset for all our experiments to train neural-based taggers for Bodo.\nTable 1.\nStatistics of Bodo POS annotated dataset\nFile Name\nSentence Count\nToken Count\nTraining set\n24003\n192k\nDev set\n2325\n23k\nTest set\n3161\n23k\nWe prepare the tagged dataset in CoNLL-2003 (Sang and De Meulder 2003) column\nformat in which each line contains one word, and the corresponding POS tag is separated\nby tab space. An empty line represents the sentence boundary in the dataset. In Table\n2, the sample column format is shown.\nFor training, we first randomized the dataset; after that, the dataset was divided into\n80:10:10 train, development, and test sets, respectively.\n4.2 POS taggset\nThe TDIL dataset is tagged according to the Bureau of Indian Standards (BIS) tagset,\nwhich is considered the national standard for annotating Indian languages. The dataset\ncontains eleven (11) top-level categories that include 34 tags. The complete tagset used\nin our experiment is reported in Table 3.\n4.3 Language Model\nLanguage models are a crucial component in a deep learning-based model. These models\nare typically trained on large unlabeled text corpus to capture both semantic and syntactic\nmeanings. The size of the training corpora (Bojanowski et al. 2017) impacts the quality\nof the language models. In our literature survey, we could not find any other LMs that\n6\nTable 2.\nDataset format\nWord\nTag\nिबयो\nPR_PRP\n88\nQT_QTC\nसानावनो\nN_NST\nसानखौ\nN_NN\nखेबसे\nQT_QTC\nिगिंद͆खनो\nV_VM\n(\nRD_PUNC\nRevolution\nRD_RDF\n)\nRD_PUNC\n।\nRD_PUNC\ncovered the Bodo language. The lack of availability of text corpus is one of the main factors\nbehind this. Consequently, there exist no other LMs that are trained on Bodo except\nour newly trained BodoBERT. Therefore, to accomplish our comparative experiment, we\nconsider pre-trained language models for Hindi as it shares the same written script with\nBodo-Devanagari. Table 4 provides the details about the LMs that are used for training\nthe Bodo POS tagging model. A brief description of these models is described below.\nFastText embedding (Bojanowski et al. 2017) uses sub-word embedding technique\nand skip-gram method. It is trained on character n-grams of words to get the internal\nstructure of a word.\nTherefore, it has the ability to get the word vectors for out-of-\nvocabulary (OOV) words by using the sub-word information from the previously trained\nmodel.\nByte-Pair embedding (Heinzerling and Strube 2018) are pre-computed on sub-word\nlevel. They can embed by splitting words into subwords or character sequences, looking\nup the pre-computed subword embeddings. It has the capability to deal with unknown\nwords and has the ability to infer meaning from unknown words.\nFlair Embedding (Akbik et al. 2018) is a type of character level embedding. It is a\ncontextualized word embedding that captures word semantics in context, meaning that a\nword has different vector representations under different contexts. The embedding method\nbased on recent advances in neural language modeling (Sutskever et al. 2014; Karpathy\net al. 2015) that provides sentences to be modeled as distributions over sequences of\ncharacters instead of words (Sutskever et al. 2011; Kim et al. 2016).\nMultilingual Representations for Indian Languages (MuRIL) (Khanuja et al.\n2021) is a multilingual language model based on BERT architecture. It is pre-trained in\n17 Indian languages.\nehttps://fasttext.cc/docs/en/pretrained-vectors.html\nfhttps://github.com/bheinzerling/bpemb\nghttps://anonymous.4open.science/status/BodoPoS-4C10\nhhttps://github.com/flairNLP/flair/blob/master/resources/docs/embeddings/FLAIR_EMBEDDINGS.md\nihttps://huggingface.co/google/muril-base-cased\njhttps://tinyurl.com/XLM-R-Embed\nkhttps://indicnlp.ai4bharat.org/indic-bert/\n7\nTable 3. Tagset used in the dataset\nS.No\nCategory\nType\nTag\n1\nNoun\nProper Noun\nN_NNP\nNoun (Location)\nN_NST\nNoun (unclassified)\nN_NN\n2\nPronoun\nPersonal\nPR_PRP\nReflexive\nPR_PRF\nReciprocal\nPR_PRC\nRelative\nPR_PRL\nWh-words\nPR_PRQ\nIndefinite\nPR_PRI\n3\nDemonstrative\nDeictic\nDM_DMD\nRelative\nDM_DMR\nWh-words\nDM_DMQ\nIndefinite\nDM_DMI\n4\nVerb\nAuxiliary Verb\nV_VAUX\nMain Verb\nV_VM\nFinite\nV_VM_VF\nNon-Finite\nV_VAUX_VNF\n5\nAdjective\nAdjective\nJJ\n6\nAdverb\nRB\n7\nPost Position\nPSP\n8\nConjunction\nConjunction\nCC_CCD\nCo-ordinator\nCC_CCS\n9\nParticles\nClassifier\nRP_RPD\nInterjection\nRP_INJ\nNegation\nRP_NEG\nIntensifier\nRP_INTF\n10\nQuantifiers\nGeneral\nQT_QTF\nCardinals\nQT_QTC\nOrdinals\nQT_QTO\n11\nResiduals\nForeign word\nRD_RDF\nSymbol\nRD_SYM\nPunctuation\nRD_PUNC\nEchowords\nRD_ECH\nUnknown\nRD_UNK\nXLM-R (Conneau et al. 2020) uses self-supervised training techniques in cross-lingual\nunderstanding, a task in which a model is trained in one language and then used with\nother languages without additional training data.\nIndicBERT (Kakwani et al. 2020) based on Fasttext-based word embedding and\nALBERT-based language models for 11 languages trained on the IndicCorp dataset.\n8\nTable 4. Training datasize of different language models\nLanguage model\nTrained\nCorpus\nTrained\nDatasize\nFastTextEmbeddings (Bojanowski et al. 2017)e\nWiki\n< 29M\nByte Pair (Heinzerling and Strube 2018)f\nWiki\n29M\nBodoBERT (Bodo) g\nBodo corpus (Narzary et al. 2022)\n1.6M\nFlair Embeddings (Akbik et al. 2018) h\nWiki+OPUS\n≈29M\nMuRIL (Khanuja et al. 2021) i\nCommonCrawl + Wiki\n788M\nXLM-R Embedding (Conneau et al. 2020) j\nCC-100 corpus\n1.7B\nIndicBERT (Kakwani et al. 2020) k\nScraping\n1.84B\n4.4 Experiment on POS models\nIn this section, we describe the experiments conducted to develop POS tagging models\nusing different LMs, including BodoBERT. The experiment can be divided into three\nphases. In the first phase, we employ three different sequence labeling architectures that\nare shown to be a well performer in other languages, namely - Fine-tuning BodoBERT\nmodel for POS tagging, CRF (Lafferty et al. 2001) and the third one with BiLSTM-\nCRF (Hochreiter and Schmidhuber 1997; Rei et al. 2016). The pre-trained BodoBERT\nis used to get the embedding vector of the words present in the sentences during training\nwith CRF and BilSTM-CRF (Huang et al. 2015) architecture. It is observed that the\nBiLSTM-CRF-based model outperforms the other two models.\nTable 5. Performance of POS tagging model in different methods\nEmbeddings\nTagging model\nF1-score(Micro)\nF1 score (Weighted)\nBodoBERT\nCRF\n0.7583\n0.7454\nFine-tuned BERT\n0.7754\n0.7775\nBiLSTM + CRF\n0.7949\n0.7898\nTherefore, the BiLSTM-CRF architecture is used in the second phase to develop POS\ntagging models employing different language models, including BodoBERT.\nTable 6. The F1 score of different Language models in POS tagging task on Bodo and Assamese language\nEmbeddings\nBodo\nAssamese\nFastTextEmbeddings\n0.7686\n0.6981\nBytePairEmbeddings\n0.7669\n0.7099\nBodoBERT\n0.7949\n0.7033\nFlairEmbeddings (Multi)\n0.7885\n0.7076\nMuRIL\n0.7708\n0.7286\nXLM-R\n0.7638\n0.7001\nIndicBert\n0.7235\n0.7293\nApart from Bodo, we also conducted the same experiment on Assamese, another low-\nresource language. Existing Assamese pre-trained LMs are used for the experiment. To\nconduct the training for Assamese POS model, we acquired dataset (ILCI-II 2020a) from\nTDIL, which was tagged by language experts manually. There are 35k sentences from\n9\ndifferent domains and 404k words in the POS datasets. The Assamese annotated dataset\nalso follows the BIS tagset and contains 41 tags with 11 top-level categories.\nIn the third phase, we further experiment with the Stacked embedding method to\ndevelop the Bodo POS tagging mode. The Stacked method allows us to learn how well\none embedding performs when combined with others during the training process. The\ntop-performing LM (BodoBERT) in the second phase is selected for further training. In\nthe Stacked method, each one of the LMs is combined with BodoBERT to get the final\nword embedding vector.\nCompared to the best Individual method, the Stacked embedding approach using\nBiLSTM-CRF improves the performance score for POS tagging by around 2%-7%. The\nmodel with BodoBERT+BytePairEmbedding attains the highest F1 score of 0.8041.\nThe results of the experiment are listed in Table 7. The POS model architecture is illus-\ntrated in Figure 1. In all experiments, the Flair frameworkl is used to train the sequence\ntagging model.\nFigure 1. Block diagram of POS tagging model\nTable 7. POS tagging performance in the stacked method using BiLSTM-CRF architecture\nStacked Embeddings\nF1 score\nBodoBERT + FastTextEmbeddings\n0.7928\nBodoBERT + BytePairEmbeddings\n0.8041\nBodoBERT + mBERT\n0.799\nBodoBERT + FlairEmbeddings\n0.801\nBodoBERT + MuRIL\n0.785\nBodoBERT + XLM-R\n0.8003\nBodoBERT + IndicBert\n0.793\nWe explored different hyperparameters to optimize the configurations concerning the\nhardware constraint. After that, we use the same set of hyperparameters in all three\nexperiments. We use a fixed mini-batch size of 32 to account for memory constraints.\nThe early stopping technique is used if there is no improvement in the validation data\naccuracy. We use the Learning Rate Annealing factor for early stopping.\nlhttps://github.com/flairNLP/flair\n10\n5.\nResult and Analysis\nIn this section, we present an analysis of our experiment and the performance of taggers.\nWe evaluated the performance of the three sequence tagging methods: CRF, Fine-tuning\nof BodoBERT, and BiLSTM-CRF by measuring the micro F1 score. The weighted average\nof the tagging performance is also reported.\nTable 5 illustrates an overview of the tagging performance in F1 score of the three\nmodels. We observe that the BiLSTM-CRF based model with BodoBERT performs the\nbest with an F1 score of 0.7949 and Weighed the average F1 score as 0.7898. In contrast,\nthe Fine-tune-based and CRF-based tagging model achieves an F1 score of 0.7754 and\n0.7583, respectively. The performance comparison result of different LMs is reported in\nTable 6. We observe BodoBERT outperforms all the other pre-trained models. The Flair\nEmbedding model achieves almost similar performance in tagging with an F1 score of\n0.7885.\nWe also cover a similar low-resource language, Assamese spoken in the same region.\nThe same set of experimental setups is used for the experiment. It provides us with an\noverview of how the models work on similar languages with almost the same size as the\nannotated dataset. It has been observed that the highest F1 score achieved in the case of\nAssamese is 0.7293 using IndicBERT. This is almost ≈7 less than the highest of the Bodo\nPOS tagging model. It could be because of the difference in the number of tagsets used\nin Assamese (41 tags) versus Bodo (34 tags).\nData\nAugmentation\nexperiment:\nThe\noverall\nbest-performing\nmodel\n(BodoBERT+BytePairEmbedding) is employed to annotate a new set of sentences\nfrom another corpus. The corpus is taken from the Bodo corpus created by (Narzary\net al. 2022).\nThe annotated dataset is further manually corrected.\nThe new dataset\nthat comprises 10k is added to the existing training dataset. In order to evaluate the\nperformance of the model when increasing the training dataset, the same architectures\n(BodoBERT+BytePairEmbedding in BiLSTM-CRF) are employed in further training\nthe POS model with the same set of parameters. The test and dev are kept the same. It\nis observed that the model performance is enhanced by 1≈2%. The model achieves an\nF1 score of 0.8494.\nThe tag-wise performance score of precision, recall, and F1 score, along with support\nconsolidated micro, macro, and weighted score, are reported in Table 8. The learning\ncurve of training and validation for the best-performing model is shown in Figure 2.\nThe learning curve implies that the dataset needs improvement as the training dataset\ndoes not provide suﬀicient information to learn the sequence tagging problem relative to\nthe validation dataset used to evaluate it. Whatsoever, we get the first POS tagging model\nfor the Bodo language. Eventually, it becomes the de facto baseline for the Bodo tagging.\nThe reference sentences and their corresponding tagged results from the proposed tagger\nare given below.\n• Reference sentences\nSentence 1: ितकेन< N_NNP > बर'आ< N_NNP > सासे< QT_QTC > मोजां< JJ >\nफोराेंिगिर< N_ANN > ।< RD_PUNC >\nIPA:\ntikEn\nboroA\nsase\nmojaN\nforoNgiri\nEnglish:\nTiken Bodo is a good teacher\nSentence 2: बडलेन्ड< N_NNP > मुलुगसोलाेंसािलआ< N_NNP > कक्राझारआव< N_NNP >\nदं<V_VAUX > ।< RD_PUNC >\nIPA:\nboroland\nmulugOsoloNsAliyA\nkokrAjhArAO\n11\nFigure 2. Learning curve of BodoBERT + BytePairEmbeddings based POS model\ndON\nEnglish:\nBodoland University is situtated in Kokrajhar\n• Tagged by the proposed DL-based tagger\nSentence 1: ितकेन< N_NNP > बर'आ< N_NN > सासे< QT_QTC > मोजां< JJ >\nफोराेंिगिर< N_NN > ।< RD_PUNC >\nIPA:\ntikEn\nboroA\nsase\nmojaN\nforoNgiri\nEnglish:\nTiken Bodo is a good teacher\nSentence 2: बडलेन्ड< N_NNP > मुलुगसोलाेंसािलआ< N_NN > कक्राझारआव< N_NN >\nदं<V_VAUX > ।< RD_PUNC >\nIPA:\nboroland\nmulugOsoloNsAliyA\nkokrAjhArAO\ndON\nEnglish:\nBodoland University is situtated in Kokrajhar\nIn the above example, the word ‘बर'आ’ /boroA/ (‘Bodo’) in sentence 1, and मुलुगसो-\nलाेंसािलआ/mulugOsoloNsAliA/ (University + nominative marker ‘-A’ ) and कक्राझारआव\n/kokrAjhArAO/ (Kokrajhar + locative marker ‘-AO’) in sentence 2 are proper nouns\n(N_NNP). But the tagger considers them as NOUN (generalized form) and tags\nthem accordingly as N_NN. Likewise, फोराेंिगिर/foroNgiri/ (Teacher) is an abstract\nnoun (N_ANN). However, it is tagged as N_NN by the tagger. In other cases, the\nPOS tagged the words correctly. If we consider only the top-level tag, the tagger\nperformance increases.\n12\nTable 8. Tag-wise performance of best performing Bodo POS tagging model\nTags\nPrecision\nRecall\nF1 score\nSupport\nN_NN\n0.7439\n0.7826\n0.7628\n11560\nV_VM\n0.8945\n0.9366\n0.9150\n9005\nRD_PUNC\n0.9927\n0.9960\n0.9944\n6815\nN_NNP\n0.7180\n0.6727\n0.6946\n5264\nJJ\n0.6665\n0.5554\n0.6059\n1975\nN_NST\n0.5703\n0.5194\n0.5436\n1421\nCC_CCD\n0.9059\n0.9634\n0.9338\n1039\nDM_DMD\n0.9699\n0.9593\n0.9646\n908\nPSP\n0.6492\n0.7624\n0.7012\n665\nQT_QTC\n0.7319\n0.8743\n0.7968\n565\nRB\n0.7295\n0.4958\n0.5904\n593\nPR_PRP\n0.8584\n0.8491\n0.8537\n464\nRD_UNK\n0.6613\n0.0861\n0.1524\n476\nRD_ECH\n0.2155\n0.4266\n0.2864\n143\nQT_QTF\n0.3673\n0.1949\n0.2547\n277\nPR_PRI\n0.6552\n0.6683\n0.6617\n199\nCC_CCS\n0.4789\n0.5574\n0.5152\n122\nRP_INTF\n0.3699\n0.4154\n0.3913\n65\nV_VAUX\n0.4493\n0.5082\n0.4769\n61\nPR_PRF\n0.1205\n0.2222\n0.1562\n45\nRD_SYM\n0.9600\n0.6486\n0.7742\n74\nQT_QTO\n0.5758\n0.7308\n0.6441\n52\nDM_DMI\n0.2292\n0.1864\n0.2056\n59\nRD_RDF\n1.0000\n0.9455\n0.9720\n55\nPR_PRC\n0.1379\n0.2963\n0.1882\n27\nPR_PRL\n0.5000\n0.1346\n0.2121\n52\nPR_PRQ\n0.3704\n0.3571\n0.3636\n28\nRP_RPD\n0.0400\n0.0526\n0.0455\n19\nDM_DMQ\n0.4615\n0.8000\n0.5854\n15\nRP_NEG\n0.0000\n0.0000\n0.0000\n5\nRP_INJ\n0.0000\n0.0000\n0.0000\n4\nDM_DMR\n0.0000\n0.0000\n0.0000\n4\nmicro avg\n0.8041\n0.8041\n0.8041\n42056\nmacro avg\n0.5320\n0.5187\n0.5076\n42056\nweighted avg\n0.8021\n0.8041\n0.7990\n42056\nThe reported highest score is arguably lower than the state-of-the-art score on resource-\nrich languages. This could be due to a variety of factors.\n(1) The size of the annotated corpus may not be adequate.\n13\n(2) The training data size of BodoBERT may not be suﬀicient enough to capture the\nlinguistic features of the language.\n(3) The language model BodoBERT may need more improvement in capturing the\nlinguistic characteristics of the Bodo language.\n(4) The assignment of the tags to the words in the dataset may need some correction.\nObservation from confusion matrix: The confusion matrix of the top-performing\nBodo POS tagging model is reported in Figure 3. The confusion matrix covers the top\ntwelve POS tags in terms of frequency counts in the test set. The diagonal entries of\nthe matrix represent the correctly predicted tags, and the off-diagonal entries represent\nincorrectly classified tags.\nFigure 3. Confusion matrix of BodoBERT + BytePairEmbeddings POS model\nIt is observed that the common error occurs in Noun (N_NN), Proper Noun (N_NNP),\nLocative Noun (N_NST), VERB (V_VM), Adjective (JJ), and Adverb (RB) in the\ntaggers. We can draw the following observation from the confusion matrix.\n• The most common error in the dataset is intra-class confusion, i.e., confusing Noun\n(N_NN) with Proper (N_NNP) and Noun (N_NN) with Locative noun (N_NST).\nThis might have occurred due to the similarity in the attached features of nouns,\npronouns, and locative nouns.\n• In some instances, the tagger incorrectly predicts a noun when its class type changes\nto an adjective in a sentence. It happens when it describes another noun, e.g., ‘Bodo\nlanguage’; in this case, although Bodo is a proper noun, it is being used to describe\nthe noun- language. Therefore it becomes an adjective.\n• Sometimes, it becomes diﬀicult to figure out the correct tag- JJ or N_NN, V_VM\nor N_NN, and RB or N_NN. So, many times the tagger incorrectly tags as N_NN\nfor JJ, V_VM, and RB.\n• Furthermore, Bodo has no similar orthographic conventions to differentiate the\nproper nouns as done using capitalization in English. Therefore, it is diﬀicult for a\nmachine to differentiate a proper noun from other nouns.\n14\n6. Conclusion\nIn this work, we presented a language model, BodoBERT, for the Bodo language based on\nBERT architecture. We develop a POS tagging model employing three distinct sequence\ntagging architectures using the BodoBERT. Upon applying the BodoBERT language\nmodel in POS tagging, we obtained two outcomes: first, an evaluation of the pre-trained\nBodoBERT’s performance on a downstream task in different architectures; Second, we\nobtained a model for POS tagging in the Bodo language. We also compared the perfor-\nmance of BodoBERT to that of other LMs using two methods: Individual and Stacked.\nThe Stacked method improves the performance of the POS tagging model. In our exper-\niment, the model that uses BodoBERT and BytePairEmbeddings together in a stacked\nmethod does better.\nDespite the fact that the Bodo POS tagger is unable to attain state-of-the-art accuracy\nin comparison to resource-rich languages, we feel that our POS model can serve as a\nbaseline for future studies. Our contributions may be useful to the research community\nin terms of using the language model BodoBERT, and the POS tagging model for various\ndownstream tasks.\nReferences\nAguilar, G., Maharjan, S., López-Monroy, A. P., and Solorio, T. 2019. A multi-task approach for\nnamed entity recognition in social media data. arXiv preprint arXiv:1906.04135.\nAkbik, A., Blythe, D., and Vollgraf, R. 2018. Contextual string embeddings for sequence labeling. In\nProceedings of the 27th international conference on computational linguistics, pp. 1638–1649.\nAlam, F., Chowdhury, S. A., and Noori, S. R. H. 2016. Bidirectional lstms—crfs networks for bangla\npos tagging. In 2016 19th International Conference on Computer and Information Technology (ICCIT),\npp. 377–382. IEEE.\nBhutani, N., Suhara, Y., Tan, W.-C., Halevy, A., and Jagadish, H. V. 2019. Open information\nextraction from question-answer pairs. arXiv preprint arXiv:1903.00172.\nBojanowski, P., Grave, E., Joulin, A., and Mikolov, T. 2017. Enriching word vectors with subword\ninformation. Transactions of the Association for Computational Linguistics, 5:135–146.\nChoudhary, N. 2021. Ldc-il:\nThe indian repository of resources for language technology. Language\nResources and Evaluation, 55(3):855–867.\nConneau, A., Khandelwal, K., Goyal, N., Chaudhary, V., Wenzek, G., Guzmán, F., Grave, É.,\nOtt, M., Zettlemoyer, L., and Stoyanov, V. 2020. Unsupervised cross-lingual representation learning\nat scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp.\n8440–8451.\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. 2018. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint arXiv:1810.04805.\nHeinzerling, B. and Strube, M. 2018. BPEmb: Tokenization-free Pre-trained Subword Embeddings in\n275 Languages. In chair), N. C. C., Choukri, K., Cieri, C., Declerck, T., Goggi, S., Hasida, K.,\nIsahara, H., Maegaard, B., Mariani, J., Mazo, H., Moreno, A., Odijk, J., Piperidis, S., and\nTokunaga, T., editors, Proceedings of the Eleventh International Conference on Language Resources\nand Evaluation (LREC 2018), Miyazaki, Japan. European Language Resources Association (ELRA).\nHochreiter, S. and Schmidhuber, J. 1997. Long short-term memory. Neural computation, 9(8):1735–\n1780.\nHuang, Z., Xu, W., and Yu, K. 2015. Bidirectional lstm-crf models for sequence tagging. arXiv preprint\narXiv:1508.01991.\nILCI-II, J. 2020a. Assamese monolingual text corpus ilci-ii.\nILCI-II, J. 2020b. Bodo monolingual text corpus ilci-ii.\nKabir, M. F., Abdullah-Al-Mamun, K., and Huda, M. N. 2016. Deep learning based parts of speech\ntagger for bengali. In 2016 5th International Conference on Informatics, Electronics and Vision (ICIEV),\npp. 26–29. IEEE.\nKakwani, D., Kunchukuttan, A., Golla, S., Gokul, N., Bhattacharyya, A., Khapra, M. M., and\nKumar, P. 2020. inlpsuite: Monolingual corpora, evaluation benchmarks and pre-trained multilingual\nlanguage models for indian languages. In Proceedings of the 2020 Conference on Empirical Methods in\nNatural Language Processing: Findings, pp. 4948–4961.\nKarpathy, A., Johnson, J., and Fei-Fei, L. 2015. Visualizing and understanding recurrent networks.\narXiv preprint arXiv:1506.02078.\nKhanuja, S., Bansal, D., Mehtani, S., Khosla, S., Dey, A., Gopalan, B., Margam, D. K.,\nAggarwal, P., Nagipogu, R. T., Dave, S., Gupta, S., Gali, S. C. B., Subramanian, V., and\nTalukdar, P. 2021. Muril: Multilingual representations for indian languages.\nKim, Y., Jernite, Y., Sontag, D., and Rush, A. 2016. Character-aware neural language models. In\nProceedings of the AAAI conference on artificial intelligence, volume 30.\nKingma,\nD. P. and Ba,\nJ. 2014. Adam:\nA method for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nLafferty, J., McCallum, A., and Pereira, F. C. 2001. Conditional random fields: Probabilistic models\nfor segmenting and labeling sequence data.\nLe-Hong, P. and Bui, D.-T. 2018. A factoid question answering system for vietnamese. In Companion\nProceedings of the The Web Conference 2018, pp. 1049–1055.\nNarzary, S., Brahma, M., Narzary, M., Muchahary, G., Singh, P. K., Senapati, A., Nandi,\nS., and Som, B. 2022. Generating monolingual dataset for low resource language bodo from old books\nusing google keep. In Proceedings of the Language Resources and Evaluation Conference, pp. 6563–6570,\nMarseille, France. European Language Resources Association.\nNiehues, J. and Cho, E. 2017. Exploiting linguistic resources for neural machine translation using multi-\ntask learning. arXiv preprint arXiv:1708.00993.\nPandey, S., Dadure, P., Nunsanga, M. V., and Pakray, P. 2022. Parts of speech tagging towards\nclassical to quantum computing. In 2022 IEEE Silchar Subsection Conference (SILCON), pp. 1–6. IEEE.\nPathak, D., Nandi, S., and Sarmah, P. 2022. Aspos: Assamese part of speech tagger using deep learning\napproach. In 2022 IEEE/ACS 19th International Conference on Computer Systems and Applications\n(AICCSA), pp. 1–8. IEEE.\nPathak, D., Nandi, S., and Sarmah, P. 2023. Part-of-speech tagger for assamese using ensembling\napproach. ACM Trans. Asian Low-Resour. Lang. Inf. Process., 22(10).\nRamamoorthy, L., Choudhary, N., Basumatary, B., and Daimary, F. 2019. Gold standard bodo\nraw text corpus.\nRei, M., Crichton, G. K., and Pyysalo, S. 2016. Attending to characters in neural sequence labeling\nmodels. arXiv preprint arXiv:1611.04361.\nSang, E. F. and De Meulder, F. 2003. Introduction to the conll-2003 shared task: Language-independent\nnamed entity recognition. arXiv preprint cs/0306050.\nShen, Y., Lin, Z., Jacob, A. P., Sordoni, A., Courville, A., and Bengio, Y. 2018. Straight to the\ntree: Constituency parsing with neural syntactic distance. arXiv preprint arXiv:1806.04168.\nSutskever, I., Martens, J., and Hinton, G. E. 2011. Generating text with recurrent neural networks.\nIn ICML.\nSutskever, I., Vinyals, O., and Le, Q. V. 2014. Sequence to sequence learning with neural networks. In\nAdvances in neural information processing systems, pp. 3104–3112.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and\nPolosukhin, I. 2017. Attention is all you need. Advances in neural information processing systems, 30.\nWarjri, S., Pakray, P., Lyngdoh, S. A., and Maji, A. K. 2021. Part-of-speech (pos) tagging using deep\nlearning-based approaches on the designed khasi pos corpus. Transactions on Asian and Low-Resource\nLanguage Information Processing, 21(3):1–24.\nWu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., Macherey, W., Krikun, M., Cao, Y.,\nGao, Q., Macherey, K., and others 2016. Google’s neural machine translation system: Bridging the\ngap between human and machine translation. arXiv preprint arXiv:1609.08144.\n",
  "categories": [
    "cs.CL",
    "cs.AI",
    "cs.LG",
    "I.2.7"
  ],
  "published": "2024-01-06",
  "updated": "2024-01-06"
}