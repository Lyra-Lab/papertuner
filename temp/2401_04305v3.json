{
  "id": "http://arxiv.org/abs/2401.04305v3",
  "title": "Advancing Deep Active Learning & Data Subset Selection: Unifying Principles with Information-Theory Intuitions",
  "authors": [
    "Andreas Kirsch"
  ],
  "abstract": "At its core, this thesis aims to enhance the practicality of deep learning by\nimproving the label and training efficiency of deep learning models. To this\nend, we investigate data subset selection techniques, specifically active\nlearning and active sampling, grounded in information-theoretic principles.\nActive learning improves label efficiency, while active sampling enhances\ntraining efficiency. Supervised deep learning models often require extensive\ntraining with labeled data. Label acquisition can be expensive and\ntime-consuming, and training large models is resource-intensive, hindering the\nadoption outside academic research and \"big tech.\" Existing methods for data\nsubset selection in deep learning often rely on heuristics or lack a principled\ninformation-theoretic foundation. In contrast, this thesis examines several\nobjectives for data subset selection and their applications within deep\nlearning, striving for a more principled approach inspired by information\ntheory. We begin by disentangling epistemic and aleatoric uncertainty in single\nforward-pass deep neural networks, which provides helpful intuitions and\ninsights into different forms of uncertainty and their relevance for data\nsubset selection. We then propose and investigate various approaches for active\nlearning and data subset selection in (Bayesian) deep learning. Finally, we\nrelate various existing and proposed approaches to approximations of\ninformation quantities in weight or prediction space. Underpinning this work is\na principled and practical notation for information-theoretic quantities that\nincludes both random variables and observed outcomes. This thesis demonstrates\nthe benefits of working from a unified perspective and highlights the potential\nimpact of our contributions to the practical application of deep learning.",
  "text": "Advancing Deep Active Learning & Data\nSubset Selection: Unifying Principles with\nInformation-Theory Intuitions\nAndreas Kirsch\nExeter College\nUniversity of Oxford\nA thesis submitted for the degree of\nDoctor of Philosophy\nTrinity 2023\narXiv:2401.04305v3  [cs.LG]  8 Mar 2024\nTo my family and friends.\nIn particular:\nmy parents, for their support;\nArmin, for his advice; and\nAlex, in his memory.\nAbstract\nAt its core, this thesis aims to enhance the practicality of deep learning by improving the\nlabel and training efficiency of deep learning models. To this end, we investigate data\nsubset selection techniques, specifically active learning and active sampling, grounded\nin information-theoretic principles. Active learning improves label efficiency, while\nactive sampling enhances training efficiency.\nSupervised deep learning models often require extensive training with labeled data.\nLabel acquisition can be expensive and time-consuming, and training large models is\nresource-intensive, hindering the adoption outside academic research and “big tech.”\nExisting methods for data subset selection in deep learning often rely on heuristics\nor lack a principled information-theoretic foundation. In contrast, this thesis examines\nseveral objectives for data subset selection and their applications within deep learning,\nstriving for a more principled approach inspired by information theory.\nWe begin by disentangling epistemic and aleatoric uncertainty in single forward-\npass deep neural networks, which provides helpful intuitions and insights into different\nforms of uncertainty and their relevance for data subset selection. We then propose\nand investigate various approaches for active learning and data subset selection in\n(Bayesian) deep learning. Finally, we relate various existing and proposed approaches\nto approximations of information quantities in weight or prediction space.\nUnderpinning this work is a principled and practical notation for information-\ntheoretic quantities that includes both random variables and observed outcomes. This\nthesis demonstrates the benefits of working from a unified perspective and highlights\nthe potential impact of our contributions to the practical application of deep learning.\nAcknowledgements\nTo my family and friends: thank you for your unwavering support throughout my\nDPhil journey. In particular, I am grateful to my parents, who have supported me in\nso many ways over the years; to my cousin, who shared with me the ups and downs of\npursuing a doctorate at the same time in Romania; and my old friends in my hometown\nin Germany: Michi, Daniel, and Marina. I hope to see you all soon.\nSpecial thanks go to my old friend, Armin Krupp, who directed me towards the\nexciting world of machine learning at the University of Oxford and the blog of an\nincoming associate professor, Yarin Gal. Your positivity and support have been a\ndriving force in my academic and professional journey. And just as much am I grateful\nto Daniel and Martin for sticking with me since our time together in Munich.\nI also owe much to the many friends who have made my time in Oxford worthwhile:\nespecially, Alessandro, Kevin, Alessio, Maja, Jonas, Charlotte, Lorika, Bastiaan,\nCharles, Prateek, Luisa, Jan, Clemens, Fabian, Ana, Rob, Brenda, Neil, Theo, Kayla,\nAnna, Aleksandra, Fiona, Serban, Stefan, Patricia, Olmo, Carli, Simone, Mar, John,\nEd, Josh, Shad, Sam & Tim; your friendship has been invaluable, whether on (coffee)\nwalks, at shared dinners and co-working sessions, in Italy, singing karaoke, surviving\nthe pandemic, hiking up the tallest mountains or in the Cotswold, visiting me, advising\nme, partying or going to formals together. The same goes for the Society Cafe regulars\n& baristas over the last five years: Nadia, Dom, Laura, Em, Amina, Will, Johnny,\nMarie, Kat, Eleanor, Ricardo, Shae, Ruth, Lauren & Noor; thank you for being so\nmuch fun every day; and to my DnD group: Lasya, Cyril, Mihir, Nico, Ned & Luca;\nmy experience would not have been the same without you and Gogol.\nI am very grateful to the AIMS CDT program and its incredible administrator,\nWendy Poole, for providing funding and support throughout my studies and generally\nbeing a force for good. I am also very lucky to be a student of Exeter College which I\nthank from my heart for their support and nurturing environment—an environment\nwhich starkly contrasts with my earlier experiences at Kellogg College.\nI am also grateful to my collaborators, especially my joint co-authors, for their\nsupport and feedback: Tom Rainforth for his mentorship and invaluable advice on\nresearch and writing—meeting in University Parks during the pandemic with a small\nwhiteboard to discuss information theory and transductive acquisition functions, which\ndirectly led to the current §7 [Kirsch et al., 2021b; Bickford-Smith et al., 2023], is\nsomething I will always remember; my external collaborators, including Dustin Tran,\nJasper Snoek, Frédéric Branchaud-Charron, Parmida Atighehchian, and Uri Shalit; and\nmy OATML collaborators, Joost van Amersfoort, Jishnu Mukhoti, Sebastian Farquhar,\nFreddie Bickford Smith, Muhammed Razzak, Andrew Jesson, Jannik Kossen, Clare Lyle,\nJan Brauner, Sören Mindermann, Panagiotis Tigas, and Aidan Gomez—your expertise,\nencouragement, and patience have been invaluable to me and this thesis impossible\nwithout you. My thanks go to the other members of OATML, too, for their support\nand feedback: especially Lisa Schut, Tim Rudner, Milad Alizadeh, and Lewis Smith.\nv\nWhile I have strived for research integrity as a guiding principle, it has tested my\nresilience and highlighted the delicate balance between being critical and patient in\nscholarly conversations as you can imagine. Being referred to as an ‘unhinged reviewer\n2’ in the summer of 2022 by an NYU professor (and former PhD colleague of my\nsupervisor) for engaging in scholarly discourse and highlighting inconsistencies, while\ndifficult, taught me about assertiveness and perseverance. These experiences have\nenriched my understanding of the academic landscape, both on how to face criticism\nand in making thoughtful contributions1. The encouragement and kindness of the\nbroader academic community despite this have played a pivotal role in my growth as a\nscholar, and I’m grateful for the opportunity to learn this during my DPhil.\nOf course, I also want to thank my supervisor, Yarin Gal, for supporting my DPhil\njourney, helping me refine my time- and project-management skills, and providing\nfeedback on most of my projects. He set up and guided my starter project, BatchBALD\n(§4), initiated many fruitful collaborations, and his input significantly contributed\nto the development of Mukhoti et al. [2023] (§3) during the pandemic in early 2021.\nFinally, he encouraged me to start drafting this thesis in the summer of 2022 when\nhe felt I was ready. As my DPhil and our collaboration evolved, I learned to work\nindependently, finally submitting my last paper (§10) as a sole author and taking full\nownership of my work. This final year was not without challenges, though. When faced\nwith artificial time constraints2, I completed the remaining 80% of this thesis within\nan accelerated three-week time frame. During my burn-out afterwards, I discovered\nthat my supervisor had contributed as a co-author to a paper investigating a similar\ntransductive information-theoretic quantity3, as a research project I worked on with\nhim for a year4 without letting me know—yet again—and then also found unattributed\ncode of mine5 and a potentially crucial bug6 in the paper. I raised these issues with\nYarin, and due to my conflicted judgement, also separately with the conference chairs,\nwho determined it was concurrent work and promised they would reach out to the\nauthors for my code to be acknowledged. Yarin was understandably not happy with the\nsituation, which had caused the co-authors in the lab, him, and me anxiety and stress,\nand I left the lab before my thesis defence when he asked me to—two months before\nthat paper’s camera-ready deadline—after I declared I would write a reproducibility\nreport if the issues I had raised were not fixed to my satisfaction before the conference.\nI absolutely believe Yarin’s assurance that the above was unintentional: he thought\nabout the paper’s theory from a different perspective, not making the connection\nbetween the two research projects. In hindsight, the connection seems manifest7.\n1§B.1 [Kirsch, 2023b] is evidence of this I hope.\n2I was away on an internship for three months at end of 2022 after completing 20% of my thesis\ndraft, and while my supervisor agreed to suspensions for such time spent away for others before and\nafter, he refused in my case to the surprise of the department—but see the above.\n3It is based on a similar overarching goal and also leads to a similar approximation, but in a totally\ndifferent setting and with a novel subsampling strategy.\n4Starting with an initial pitch in the summer of 2020, an early report and workshop paper in 2021,\nit eventually led to a conference paper in 2023, see §7.\n5It took some pushing to get my code properly attributed: #1, #2, #3.\n6The code accidentally used the harmonic mean instead of the arithmetic mean in the entropy\ncomputation—in the end, six months later, the corrected code slightly improved the results, or at\nleast wasn’t performing worse.\n7The connection became more evident once I read the paper and earlier work, and the\nimplementation of our earlier method as an ablation ends up a one-line addition.\nvi\nDespite earlier assurances, it required some progressively less subtle nudging to have\na corrected paper uploaded and the fixed code published, some time after the actual\nconference8. Supervision experiences are highly individual, and Yarin has been very\nsupportive of his students’ well-being throughout the years and tried to work things\nout. He suggested a “two-year cool-off period [of] not reading and commenting on lab\npapers for a while to avoid me doing this on a ‘low’ day which leads to triggering me\nand acting irrationally” and has kindly agreed to write reference letters that “only\ncomment on my analytical skills and mention that I had some personal issues that\nhave led to high anxiety affecting my performance.” I believe in many parts thanks\nto him, I have become an independent and self-aware researcher who cares about\nacademic integrity and conduct—and, indeed, while it might not have ended as I had\nimagined or would have liked, Yarin was the most instrumental factor in my finishing\nthis DPhil in 2023 and passing my defence overall.\nFinally, I want to thank all those who have provided feedback on my thesis, especially\nmy assessors, Kevin Murphy and Mike Osborne, and the many anonymous conference\nreviewers who have helped shape my work.\nWhile I have failed in my research at times, this thesis stands as a testament to\nthe collective efforts, support, and encouragement of the remarkable individuals who\nhave been part of this adventure. My deepest and sincerest thanks to you all.\nAndreas Kirsch, Oxford, 2023 9\n8Academic commitments and personal circumstances of one of the three first-authors have likely\nplayed a part in some of the delay.\n9I also want to thank Yarin for iterating on the acknowledgements with me in early 2024.\nContents\n1\nPreliminaries\n1\n1.1\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1\n1.2\nBackground & Literature Review\n. . . . . . . . . . . . . . . . . . . . .\n2\n1.3\nThesis Outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\n1.4\nContributions to Joint Work . . . . . . . . . . . . . . . . . . . . . . . .\n23\n2\nA Practical & Unified Notation for Information Quantities\n27\n2.1\nExample Application: Stirling’s Approximation for Binomial Coefficients 32\n3\nSingle Forward-Pass Aleatoric and Epistemic Uncertainty\n35\n3.1\nEntropy ̸= Epistemic Uncertainty . . . . . . . . . . . . . . . . . . . . .\n38\n3.2\nAleatoric & Epistemic Uncertainty\n. . . . . . . . . . . . . . . . . . . .\n52\n3.3\nObjective Mismatch . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n56\n3.4\nDeep Deterministic Uncertainty . . . . . . . . . . . . . . . . . . . . . .\n60\n3.5\nEmpirical Validation . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n63\n3.6\nComparison to Prior Work . . . . . . . . . . . . . . . . . . . . . . . . .\n74\n3.7\nDiscussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n75\n4\nDiverse Batch Acquisition for Bayesian Active Learning\n79\n4.1\nBatchBALD . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n80\n4.2\nEmpirical Validation . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n84\n4.3\nDiscussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n88\n5\nStochastic Batch Acquisition for Deep Active Learning\n90\n5.1\nProblem Setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n91\n5.2\nMethod\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n92\n5.3\nRelated Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n95\n5.4\nEmpirical Validation . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n96\n5.5\nFurther Investigations\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n99\n5.6\nDiscussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102\n6\nMarginal and Joint Cross-Entropies & Predictives\n103\n6.1\nMarginal and Joint Cross-Entropy . . . . . . . . . . . . . . . . . . . . . 105\n6.2\nOnline Bayesian Inference\n. . . . . . . . . . . . . . . . . . . . . . . . . 106\n6.3\nNew Evaluations & Applications . . . . . . . . . . . . . . . . . . . . . . 107\n6.4\nEmpirical Validation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109\n6.5\nRelated Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111\n6.6\nDiscussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111\nContents\nviii\n7\nPrediction- & Distribution-Aware Bayesian Active Learning\n113\n7.1\nShortfalls of BALD . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115\n7.2\nDistribution-Aware Acquisition Functions . . . . . . . . . . . . . . . . . 119\n7.3\nExpected Predictive Information Gain\n. . . . . . . . . . . . . . . . . . 120\n7.4\nJoint Expected Predictive Information Gain\n. . . . . . . . . . . . . . . 128\n7.5\nRelated Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136\n7.6\nDiscussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137\n8\nPrioritized Data Selection during Training\n139\n8.1\nActive Sampling: Online Batch Selection . . . . . . . . . . . . . . . . . 140\n8.2\n(Joint) Predictive Information Gain & Reducible Holdout Loss Selection 141\n8.3\nEmpirical Validation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145\n8.4\nRelated Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151\n8.5\nDiscussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152\n9\nUnifying Approaches in Active Learning and Active Sampling\n153\n9.1\nSetting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154\n9.2\nSecond-Order Posterior Approximation . . . . . . . . . . . . . . . . . . 156\n9.3\nFisher Information\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 158\n9.4\nApproximating Information Quantities\n. . . . . . . . . . . . . . . . . . 161\n9.5\nSimilarity Matrices and One-Sample Approximations . . . . . . . . . . 167\n9.6\nInformation Quantities in Prior Literature . . . . . . . . . . . . . . . . 168\n9.7\nDiscussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172\n10 Black-Box Batch Active Learning for Regression\n174\n10.1 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 175\n10.2 Methodology\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 176\n10.3 Empirical Validation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183\n10.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 186\n11 Conclusion\n187\nAppendices\nA Causal-BALD: Deep Bayesian Active Learning of Outcomes to Infer\nTreatment-Effects from Observational Data\n190\nA.1 Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192\nA.1.1\nEstimation of Personalized Treatment-Effects\n. . . . . . . . . . 192\nA.1.2\nActive Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . 193\nA.2 Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 193\nA.2.1\nNaive Acquisition Functions, Training Data Bias, and the Effect\non the Estimated CATE Function.\n. . . . . . . . . . . . . . . . 194\nContents\nix\nA.2.2\nCausal–BALD . . . . . . . . . . . . . . . . . . . . . . . . . . . . 195\nA.3 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 196\nA.4 Empirical Validation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 197\nA.4.1\nResults . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 198\nA.5 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 198\nA.6 Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 199\nA.6.1\nTheoretical Results . . . . . . . . . . . . . . . . . . . . . . . . . 199\nA.6.2\nBaselines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 204\nA.6.3\nDatasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205\nA.6.4\nActive Learning Setup Details . . . . . . . . . . . . . . . . . . . 206\nA.6.5\nMore Results\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . 206\nA.6.6\nCompute . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 208\nA.6.7\nNeural Network Architecture . . . . . . . . . . . . . . . . . . . . 208\nB Reproducibility Analysis\n213\nB.1 Deep Learning on a Data Diet . . . . . . . . . . . . . . . . . . . . . . . 213\nB.1.1\nInvestigation\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . 214\nB.1.2\nDiscussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216\nB.1.3\nDetails . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 217\nB.2 A Note on “Assessing Generalization of SGD via Disagreement” . . . . 217\nB.2.1\nBackground & Setting\n. . . . . . . . . . . . . . . . . . . . . . . 220\nB.2.2\nRephrasing Jiang et al. [2022] in a Probabilistic Context\n. . . . 221\nB.2.3\nGDE is Class-Aggregated Calibration in Expectation . . . . . . 223\nB.2.4\nDeterioration of Calibration under Increasing Disagreement . . . 227\nB.2.5\nRelated Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . 229\nB.2.6\nDiscussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 230\nB.2.7\nDetails . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 230\nB.3 Dirichlet Model of a Deep Ensemble’s Softmax Predictions . . . . . . . 243\nB.3.1\nMethodology\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . 243\nB.3.2\nQualitative Empirical Validation\n. . . . . . . . . . . . . . . . . 245\nB.3.3\nDiscussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 246\nB.3.4\nDetails . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 246\nC Single Forward-Pass Aleatoric and Epistemic Uncertainty\n252\nC.1 Experimental Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . 252\nC.1.1\nDirty-MNIST . . . . . . . . . . . . . . . . . . . . . . . . . . . . 252\nC.1.2\nOoD Detection Training Setup . . . . . . . . . . . . . . . . . . . 252\nC.1.3\nSemantic Segmentation Training Setup . . . . . . . . . . . . . . 252\nC.1.4\nCompute Resources . . . . . . . . . . . . . . . . . . . . . . . . . 253\nC.2 Additional Results\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 253\nC.3 Additional Ablations & Toy Experiments . . . . . . . . . . . . . . . . . 262\nContents\nx\nC.3.1\nQUBIQ Challenge\n. . . . . . . . . . . . . . . . . . . . . . . . . 262\nC.3.2\nAdditional Calibration Metrics . . . . . . . . . . . . . . . . . . . 263\nC.4 Big Figure 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 263\nD Diverse Batch Acquisition for Bayesian Active Learning\n265\nD.1 Proof of Submodularity . . . . . . . . . . . . . . . . . . . . . . . . . . . 265\nD.2 BALD as an Upper-Bound of BatchBALD . . . . . . . . . . . . . . . . 266\nD.3 Sampling of Configurations . . . . . . . . . . . . . . . . . . . . . . . . . 266\nD.4 Ablation Study on Repeated-MNIST . . . . . . . . . . . . . . . . . . . 267\nD.5 Additional Results for Repeated-MNIST\n. . . . . . . . . . . . . . . . . 267\nD.6 Example Visualisation of EMNIST\n. . . . . . . . . . . . . . . . . . . . 268\nD.7 Entropy and Per-Class Acquisitions (including Random Acquisition) . . 268\nE Stochastic Batch Acquisition for Deep Active Learning\n269\nE.1\nProof of Proposition 5.1\n. . . . . . . . . . . . . . . . . . . . . . . . . . 269\nE.2\nEmpirical Validation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 270\nE.2.1\nExperimental Setup & Compute . . . . . . . . . . . . . . . . . . 270\nE.2.2\nRepeated-MNIST . . . . . . . . . . . . . . . . . . . . . . . . . . 273\nE.2.3\nMIO-TCD . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 274\nE.2.4\nEMNIST . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 274\nE.2.5\nEdge Cases in Synbols . . . . . . . . . . . . . . . . . . . . . . . 275\nE.2.6\nCLINC-150\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 277\nE.3\nComparing Power, Softmax and Soft-Rank . . . . . . . . . . . . . . . . 278\nE.3.1\nEmpirical Evidence . . . . . . . . . . . . . . . . . . . . . . . . . 278\nE.3.2\nInvestigation\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . 280\nE.4\nEffect of Changing β . . . . . . . . . . . . . . . . . . . . . . . . . . . . 281\nE.4.1\nRepeated-MNIST . . . . . . . . . . . . . . . . . . . . . . . . . . 281\nE.4.2\nCausalBALD: Synthetic Dataset . . . . . . . . . . . . . . . . . . 283\nE.4.3\nCLINC-150\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 283\nF Prediction- & Distribution-Aware Bayesian Active Learning\n284\nF.1\nBALD Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 284\nF.1.1\nCategorical Predictive Distribution . . . . . . . . . . . . . . . . 284\nF.1.2\nGaussian Predictive Distribution\n. . . . . . . . . . . . . . . . . 284\nF.2\nEPIG Derivation\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 285\nF.3\nEPIG Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 285\nF.3.1\nCategorical Predictive Distribution . . . . . . . . . . . . . . . . 285\nF.3.2\nGaussian Predictive Distribution\n. . . . . . . . . . . . . . . . . 286\nF.3.3\nConnection to Foster et al. [2019] . . . . . . . . . . . . . . . . . 286\nF.4\nDataset Construction . . . . . . . . . . . . . . . . . . . . . . . . . . . . 287\nF.4.1\nUCI Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 287\nF.4.2\nMNIST Data\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . 287\nF.5\nEPIG & JEPIG . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 287\nF.6\nA Practical Approximation of JEPIG . . . . . . . . . . . . . . . . . . . 288\nContents\nxi\nG Prioritized Data Selection during Training\n291\nG.1 Steps Required for a Given Test Accuracy\n. . . . . . . . . . . . . . . . 291\nG.2 Experiment Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 291\nG.3 Robustness to Noise\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . 293\nG.4 Irreducible Holdout Loss Approximation . . . . . . . . . . . . . . . . . 294\nG.5 Experimental Details for Assessing Impact of Approximations\n. . . . . 296\nG.6 Ablation of Percentage Selected . . . . . . . . . . . . . . . . . . . . . . 297\nG.7 Active Learning Baselines\n. . . . . . . . . . . . . . . . . . . . . . . . . 297\nH Unifying Approaches in Active Learning and Active Sampling\n299\nH.1 Fisher Information: Additional Derivations & Proofs\n. . . . . . . . . . 299\nH.1.1\nSpecial Case: Exponential Family . . . . . . . . . . . . . . . . . 300\nH.1.2\nSpecial Case: Generalized Linear Models . . . . . . . . . . . . . 301\nH.2 Approximating Information Quantities\n. . . . . . . . . . . . . . . . . . 302\nH.2.1\nApproximate Expected Information Gain . . . . . . . . . . . . . 302\nH.2.2\nApproximate Expected Predicted Information Gain . . . . . . . 303\nH.2.3\nApproximate Predictive Information Gain\n. . . . . . . . . . . . 304\nH.2.4\nApproximate Joint (Expected) Predictive Information Gain . . . 304\nH.3 Similarity Matrices and One-Sample Approximations . . . . . . . . . . 306\nH.4 Connection to Other Acquisition Functions in the Literature . . . . . . 307\nH.4.1\nSIMILAR [Kothawade et al., 2021] and PRISM [Kothawade et al.,\n2022] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 307\nH.4.2\nExpected Gradient Length . . . . . . . . . . . . . . . . . . . . . 308\nH.4.3\nDeep Learning on a Data Diet . . . . . . . . . . . . . . . . . . . 309\nH.5 Preliminary Empirical Comparison of Information Quantity Approxima-\ntions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 309\nI\nBlack-Box Batch Active Learning for Regression\n312\nBibliography\n318\nWhen you can do nothing, what can you do?\n1\nPreliminaries\n1.1\nIntroduction\nOver a decade ago, the deep learning revolution began, making significant strides in\nvarious research fields, including areas once considered exclusive domains of human\ningenuity and creativity. Despite its success, deep learning still faces challenges that\nhinder its deployment in more practical, everyday settings. This thesis aims to address\nsome of these challenges and make deep learning more accessible by reducing the costs of\ngathering and labeling data, speeding up training, and doing so in a principled fashion.\nDeep learning models have yet to gain a strong foothold outside big tech, maybe\ndue to issues such as lack of interpretability, robustness, and generalization guarantees.\nHowever, history tells us that such concerns are unlikely to deter the use of a new\ntechnology.\nSo, what does?\nMore practical concerns, such as the time-consuming and expensive processes of\ngathering and labeling high-quality data and training models, may also limit the\ndeployment of deep learning.\nResearch often overlooks the data pipeline, which\nfrequently is the bottleneck that constrains model performance.\nThe primary objective of this thesis is to address these practical challenges and\nmake deep learning more accessible by exploring how to reduce the cost of gathering\nand labeling data and how to speed up training. At the same time while maintaining a\nprincipled approach allows for a better understanding of trade-offs and connections\nbetween different methods.\nIn particular, this thesis focuses on data subset selection in the broader sense of\nactive learning and active sampling for deep learning models, especially through the\nprincipled application of information theory. Active learning, like semi-supervised\nand unsupervised approaches, increases label efficiency. It does so by selecting the\nmost informative samples to label from a pool set of unlabeled data, potentially\nsignificantly reducing the number of required labels for good model performance.\nSimilarly, active sampling improves training efficiency by filtering the available training\ndata to focus on the most informative samples for the model during training. The\nmain research questions in this thesis are:\n1. How can uncertainty quantification, specifically aleatoric and epistemic uncer-\ntainty, be better understood and applied in the context of active learning and\nactive sampling?\n2. How can a deeper understanding of the theoretical foundations of active learning\nand active sampling contribute to the progress of the field and improve the\npractical application of these techniques?\n1. Preliminaries\n2\n3. How can the cost of gathering and labeling data be reduced, and how can training\nbe sped up in a principled fashion?\n4. What are the connections between different active learning and active sampling\napproaches, and how can information theory be used to unify these approaches?\nTo answer these questions, we rely on information theory which helps quantify the\ninformation content of random variables and events, offers valuable insights and\nintuitions, and provides a principled framework for reasoning about uncertainty.\nInformation diagrams (I-diagrams, see e.g. Figure 1.2) are particularly useful for\nbuilding intuition, as they resemble Venn diagrams and stem from the realization that\ninformation-theoretic operations behave similarly to set operations [Yeung, 1991; Lang\net al., 2022]. In a Bayesian setting, where model parameters are treated as random\nvariables with associated uncertainty, information theory can be used to express and\nexamine different concepts of ’informativeness’ that are useful for data subset selection.\nThis thesis examines objectives and extensions that follow from these intuitions and\ndemonstrates that these objectives can be successfully applied in data subset selection.\nIn the next two sections of this chapter, we lay out the background and the\nstructure of the thesis: §1.2 introduces necessary background material and points\ntowards relevant literature for the thesis overall; and §1.3 provides an overview of\nthe thesis and its structure.\n1.2\nBackground & Literature Review\nIn this section, we introduce concepts that are relevant for the whole thesis. We revisit\ngeneral background in information theory, Bayesian neural networks, active learning,\nand active sampling (in this order), and highlight important literature. Related work\nand background that are only relevant for their respective chapters are kept there:\nwe hope that this aids the reader by keeping relevant information close to where it is\nneeded. The next chapters generally follow the consistent notation introduced below,\nand we point out when we deviate from that in the specific chapters.\n1.2.1\nInformation Theory\nInformation theory has provided insights for deep learning: information bottlenecks\nexplain objectives both for supervised and unsupervised learning of high-dimensional\ndata [Shwartz-Ziv and Tishby, 2017; Kirsch et al., 2020; Jónsson et al., 2020]; similarly,\ninformation theory has inspired Bayesian experiment design, Bayesian optimization,\nand active learning as well as motivated research into submodularity in general [Lindley,\n1956; Foster et al., 2019].\nA practical notation conveys valuable intuitions and concisely expresses new ideas.\nThe currently employed notation in information theory, however, can be ambiguous\nfor more complex expressions found in applied settings and often deviates between\npublished works because researchers are from different backgrounds such as statistics,\ncomputer science, information engineering, which all use information theory. For\nexample, H(X, Y ) is sometimes used to denote the cross-entropy between X and Y ,\nwhich conflicts with common notation of the joint entropy H(X, Y ) for X and Y , or it\nis not clarified that H[X | Y ] as conditional entropy of X given Y is an expectation\nover Y . Here, we present a disambiguated and consistent notation while striving to\nstay close to known notation when possible.\n1. Preliminaries\n3\nFor a general introduction to information theory, we refer to Cover and Thomas\n[2005] and Yeung [2008]. In the following section, we introduce our practical and\nunified notation. In §2, we extend this notation to also include observed outcomes and\npoint-wise mutual information, information gain and information-theoretic surprise.\nWe start with notation that is explicit about the underlying probability distribution\np( · ). Note that we do not require q to be normalized at this stage as it allows\nfor greater notational flexibility.\nDefinition 1.1. Let Shannon’s information content H( · ), cross-entropy H( · ∥· ),\nentropy H( · ), and KL divergence DKL( · ∥· ) (Kullback-Leibler divergence) be defined\nfor a probability distribution p and non-negative function q for a random variable X\nand non-negative real number ρ as:\nH(ρ) ≜−ln ρ\n(1.1)\nH(p(X) ∥q(X)) ≜Ep(x) H(q(x))\n(1.2)\nH(p(X)) ≜H(p(X) ∥p(X))\n(1.3)\nDKL(p(X) ∥q(X)) ≜H(p(X) ∥q(X)) −H(p(X)),\n(1.4)\nwhere we fix H(0) ≜−∞with 0 · H(0) = 0 as usual.\nShannon [1948] introduced the information content as negative logarithm due to its\nadditivity for independent messages: H(p(x, y)) = H(p(x)) + H(p(y)) for independent\nrandom variables X and Y .\nProposition 1.1. For a random variable X with probability distributions p, p1 and\np2, and non-negative functions q, q1 and q2 and α ∈[0, 1]:\nH(p ∥α q) = H(p ∥q) + H(α),\n(1.5)\nH(p ∥qα) = α H(p ∥q)\n(1.6)\nH(p ∥q1 q2) = H(p ∥q1) + H(p ∥q2),\n(1.7)\nH(α p1 +(1 −α) p2 ∥q) =\n(1.8)\n= α H(p1 ∥q) + (1 −α) H(p2 ∥q)\n(1.9)\n= H(p1 ∥qα) + H(p2 ∥q1−α),\n(1.10)\nwhere we have left out “(X)” everywhere for brevity.\nProof. The statements follow from the linearity of the expectation and the additivity\nof the logarithm for products.\nThis can be extended to show that cross-entropies are linear in their left-hand\nargument and log-linear in their right-hand argument.\nWhen we want to emphasize that we approximate the true distribution p using a\ndifferent distribution q and the true probability distribution p is understood, we use\nthe notation Hq[ · ] for H(p( · ) ∥q( · )) following notation in Kirsch et al. [2020]\nand Xu et al. [2020]:\nDefinition 1.2. When the true probability distribution p is understood from context,\nwe will use the following shorthand notation:\nH[X] ≜H(p(X))\n(1.11)\n1. Preliminaries\n4\nHq[X] ≜H(p(X) ∥q(X)).\n(1.12)\nWhen we have a parameterized distribution qθ with parameters θ, we will write Hθ[ · ]\ninstead of Hqθ[ · ] when the context is clear.\nApproximating a possibly intractable distribution with a parameterized one is\ncommon when performing variational inference. The main motivation for this notation\nis that when q is a density,\nR q(x) dx = 1, we have Hq[ · ] ≥H[ · ] with equality when\nq = p. Concretely, we have the following useful identities:\nProposition 1.2. We have the following lower-bounds for the cross-entropy and KL,\nwith Zq ≜\nR q(x) dx:\nH(p(X) ∥q(X)) ≥H(p(X)) + H(Zq),\n(1.13)\nDKL(p(X) ∥q(X)) ≥H(Zq),\n(1.14)\nwith equality exactly when q /Zq = p for Zq ≜\nR q(x) dx.\nProof. The statements follow from Jensen’s inequality and the convexity of H( · ).\nThis also implies the non-negativity of the KL for densities when we substitute\nZq = 1 in above statements. We repeat the result as it is often used:\nCorollary 1.3. When q is a probability distribution, we have:\nH(p(X) ∥q(X)) ≥H(p(X)),\n(1.15)\nDKL(p(X) ∥q(X)) ≥0,\n(1.16)\nwith equality exactly when q = p.\nNote that for continuous distributions, above equality p = q only has to hold\nalmost everywhere.\nAbove definitions are trivially extended to joints of random variables by substituting\nthe random variable of the product space. Similarly, the conditional entropy is defined\nby taking the expectation over both X and Y . For example:\nProposition 1.4. Given random variables X and Y , we have:\nH[X, Y ] = Ep(x,y) H(p(x, y));\n(1.17)\nH[X | Y ] = Ep(x,y) H(p(x | y)).\n(1.18)\nIn particular, note that H[X | Y ] is an expectation over X and Y .\nFor cross-entropies and KL divergences, we expand the definitions similarly. In par-\nticular, we have the following equality for cross-entropies, which follows from these defini-\ntions:\nH(p(X | Y ) ∥q(X | Y )) = Ep(x,y) H(q(x | y))\n= H(p(X, Y ) ∥q(X | Y )).\n(1.19)\nThe last idiosyncrasy only applies to cross-entropies. For KL divergences we have:\nDKL(p(X | Y ) ∥q(X | Y )) = H(p(X, Y ) ∥q(X | Y )) −H(p(X | Y ))\n(1.20)\n1. Preliminaries\n5\nH[X, Y ]\nH[X]\nH[Y ]\nI[X; Y ]\nH[X | Y ]\nH[Y | X]\nFigure 1.1:\nReproduction of Figure 8.1 from MacKay [2003] using the new suggested\nnotation: The relationship between joint entropy, marginal entropy, conditional entropy and\nmutual information.\nDKL(p(X, Y ) ∥q(X | Y )) = H(p(X, Y ) ∥q(X | Y )) −H(p(X, Y )).\n(1.21)\nThe second terms on each right-hand side are usually not equal H(p(X | Y )) ̸=\nH(p(X, Y )), and the two expressions are thus not equivalent. The reader might wonder\nwhen we are interested in DKL(p(X, Y ) ∥q(X | Y )). It can arise when performing\nsymbolic manipulations, so we mention it explicitly here.\nThe mutual information and point-wise mutual information [Fano, 1962; Church\nand Hanks, 1989] are defined as:\nDefinition 1.3. For random variables X and Y and outcomes x and y respectively,\nthe point-wise mutual information I[x; y] and the mutual information I[X; Y ] are:\nI[x; y] ≜H[x] −H[x | y] = H(p(x) p(y)\np(x, y) )\n(1.22)\nI[X; Y ] ≜H[X] −H[X | Y ] = Ep(x,y) I[x; y].\n(1.23)\nThis is similarly extended to I[X; Y | Z] = H[X | Z] −H[X | Y, Z] or I[X1, X2; Y ] =\nH[X1, X2] −H[X1, X2 | Y ] and so on.\nMacKay [2003] has an elegant visualization for information quantities, which we\nreproduce in Figure 1.1.\n1.2.1.1\nInformation Diagrams (I-Diagrams)\nSimilarly, Yeung [1991] introduces information diagrams (I-diagrams) which provide\nanother useful intuitive approach: they show that the intuitions that map information\nquantities to set expressions can be made principled using a specially defined signed\ninformation measure. Note that interaction information [McGill, 1954] follows as\ncanonical generalization of the mutual information to multiple variables from that\nwork. Lang et al. [2022] further generalize these results.\nInformation diagrams, like the one depicted in Figure 1.2, visualize the relationship\nbetween information quantities: Yeung [1991] shows that we can define a signed\nmeasure µ* such that these well-known quantities map to abstract sets and are\nconsistent with set operations.\nµ*[A] = H[A]\nµ*[∪iAi] = H[A1, . . . , An]\nµ*[∪iAi −∪iBi] = H[A1, . . . , An | B1, . . . , Bn]\n1. Preliminaries\n6\nH[X | Y, Z]\nH[X]\nH[Y | X, Z]\nH[Y ]\nH[Z | X, Y ]\nH[Z]\nI[X; Y ; Z]\nI[X; Z | Y ]\nI[X; Y | Z]\nI[Y ; Z | Y ]\nFigure 1.2: Example of an I-Diagram for three random variables X, Y, Z. All seven atomic\nquantities are depicted as well as the overall entropies H[X], H[Y ], H[Z].\nµ*[∩iAi] = I[A1; . . . ; An]\nµ*[∩iAi −∪iBi] = I[A1; . . . ; An | B1, . . . , Bn]\nIn other words, equalities can be read off directly from I-diagrams: an information\nquantity is the sum of its parts in the corresponding I-diagram. This is similar to Venn\ndiagrams. The sets used in I-diagrams are just abstract symbolic objects, however.\nAn important distinction between I-diagrams and Venn diagrams is that while we\ncan always read off inequalities in Venn diagrams, this is not true for I-diagrams in\ngeneral because mutual information terms in more than two variables can be negative.\nIn Venn diagrams, a set is always larger or equal any subset.\nHowever, if we show that all information quantities are non-negative, we can\ntreat an I-diagram like a Venn diagram and read off both equalities and inequalities\nfrom it. Nevertheless, caution is warranted sometimes. As the signed measure can\nbe negative, µ*[X ∩Y ] = 0 does not imply X ∩Y = ∅: deducing that a mutual\ninformation term is 0 does not imply that one can simply remove the corresponding\narea in the I-diagram.\nThere could be Z with µ*[(X ∩Y ) ∩Z] < 0, such that\nµ*[X ∩Y ] = µ*[X ∩Y ∩Z] + µ*[X ∩Y −Z] = 0 but X ∩Y ̸= ∅. This also means that\nwe cannot drop the term from expressions when performing symbolic manipulation,\nand we cannot remove the area from an I-diagram without loss of generality:\nWhile a mutual information term of two random variables equalling zero implies\nthose two random variables are independent, the mutual information of those two\nrandom variables with a third might not be zero, and thus does not allow us to draw\nthem as disjoint areas.\n1. Preliminaries\n7\nThe only time when one can safely remove an area from the diagram is for atomic\nquantities, which are quantities which reference all the available random variables\n[Yeung, 1991]. For example, when we only have three variables X, Y, Z, I[X; Y ; Z]\nand I[X; Y | Z] are atomic quantities, like in Figure 1.2.\nWe can safely remove\natomic quantities from I-diagrams when they are 0 because there are no other random\nvariables left that could lead to the issues described above.\nContinuing the example, for I[X; Y ] = µ*[X ∩Y ] = 0, having 0 = I[X; Y ; Z] =\nµ*[X ∩Y ∩Z] would imply X ∩Y ∩Z = ∅, and we could remove that area from\nthe diagram without loss of generality. Moreover, the atomic quantity I[X; Y | Z] =\nµ*[X ∩Y −Z] = 0 and could be removed from the diagram as well in that case.\nWe only use I-diagrams for the three variable case, but they supply us with tools\nto easily come up with equalities and inequalities for information quantities. In the\ngeneral case with multiple variables, they can be difficult to draw, but for Markov\nchains they can be of great use [Yeung, 1991].\n1.2.2\nBayesian Neural Networks (BNNs)\nIn this thesis, we focus on Bayesian neural networks (BNNs) [Neal, 1995; MacKay,\n1992c]. Unlike regular neural networks, BNNs maintain a distribution over their weights\ninstead of point estimates. This allows for better uncertainty quantification and for\ndisentangling different types of uncertainties [Kendall and Gal, 2017]. Compared to\nother Bayesian approaches, BNNs scale well to high-dimensional inputs, such as images\nwhile remaining close to neural networks conceptually, allowing to apply advances in\ndeep learning in Bayesian settings [Sharma et al., 2022; Gal et al., 2017]. Hence, BNNs\nhave become a powerful alternative to traditional neural networks.\nChallenges for BNNs. However, performing exact inference in BNNs is intractable\nfor any reasonably sized model, so we resort to using a variational approximation. The\nintractability of exact Bayesian inference in deep learning has led to the development\nof approximate inference methods [Hinton and van Camp, 1993; Hernández-Lobato\nand Adams, 2015; Blundell et al., 2015; Gal and Ghahramani, 2016a]. Improvements in\napproximate inference [Blundell et al., 2015; Gal and Ghahramani, 2016a] have enabled\ntheir usage for high dimensional data such as image for Bayesian active learning of\nimages [Gal et al., 2017]. Similar to Gal et al. [2017], we will mainly use MC dropout\n[Gal and Ghahramani, 2016a], which is easy to implement, scales well to large models\nand datasets, and is straightforward to optimize. Deep Ensembles [Lakshminarayanan\net al., 2017] can also be considered as an alternative to BNNs.\nProbabilistic Model. We consider supervised learning of a probabilistic predictive\nmodel, p(y,ωωω | x), where X is an input, Y is a label, and ΩΩΩare the model parameters\nof our model M with prior distribution p(ωωω):\np(y,ωωω | x) = p(y | x,ωωω) p(ωωω).\n(1.24)\nImportantly, we assume that the model’s predictions are independent given ΩΩΩ, such\nthat we can write:\np(y1, . . . , yn | x1, . . . , xn,ωωω) =\nn\nY\ni=1\np(yi | xi,ωωω).\n(1.25)\nWe assume classification tasks, and thus Y ∈[C] ≜{1, . . . , C}.\nThe exceptions\nare §10 and Appendix A.\n1. Preliminaries\n8\nDatasets. Further, we assume we have datasets Dtrain and Dtest, with Dtest ∼\nˆptrue(y, x), the ‘true’ underlying data distribution. We use D to represent additional\ndata that we might condition on. We will define additional datasets in the respec-\ntive chapters and as we go along (for example, the unlabeled pool set in active\nlearning as Dpool).\nBayesian Model Averaging and Bayesian Inference. We are interested in the\nBMA (Bayesian model averaging) prediction given the Bayesian posterior p(ωωω | Dtrain).\nBayesian model averaging (BMA) is performed by marginalizing over ΩΩΩto obtain\nthe predictive distribution p(y | x, Dtrain):\np(y | x, Dtrain) = Ep(ωωω|Dtrain)[p(y | x,ωωω)],\n(1.26)\nwhere we use Bayesian inference to obtain a posterior p(ωωω | Dtrain) for data Dtrain by:\np(ωωω | Dtrain) ∝p(Dtrain | ωωω) p(ωωω).\n(1.27)\np(Dtrain | ωωω) is the likelihood of the data given the parameters ΩΩΩ, and p(ωωω | Dtrain)\nis the new posterior distribution over ΩΩΩ.\nVariational Inference. As mentioned, Bayesian inference is often intractable, and\ninstead, we use variational inference methods to approximate the posterior p(ωωω | Dtrain)\nvia a simpler distribution q(ωωω) ≈p(ωωω|Dtrain). Usually, this is phrased as an optimization\nproblem where we try to find the distribution q from a variational family of potential\ndistributions that minimizes a given divergence measure between q(ωωω) and p(ωωω | Dtrain):\nq(ωωω) = arg min\nq(ωωω)\nD(q(ωωω) || p(ωωω | Dtrain)).\n(1.28)\nWe will use the KL divergence DKL(q(ωωω) ∥p(ωωω | Dtrain)) as divergence measure in this\nthesis, but other divergence measures have been proposed and are used in practice, as\nwell. Instead of above KL divergence which is usually intractable, the ELBO (Evidence\nLower Bound) is commonly used to optimize the variational approximation instead.\nBy expanding above KL divergence, we obtain:\nDKL(q(ωωω) ∥p(ωωω | Dtrain))\n(1.29)\n= Eq(ωωω)[−log p(ytrain\n1..N | xtrain\n1..N ,ωωω)\n|\n{z\n}\nlog likelihood\n] + DKL(q(ωωω) ∥p(ωωω))\n|\n{z\n}\nprior regularization\n+ log p(Dtrain)\n|\n{z\n}\nmodel evidence\n≥0,\n(1.30)\nThe model evidence is independent of q(ωωω). It can be ignored during optimization.\nRearranging, leaves us with the ELBO, which we then maximize:\nEq(ωωω)[log p(ytrain\n1..N | xtrain\n1..N ,ωωω)] −DKL(q(ωωω) ∥p(ωωω)) ≤log p(Dtrain).\n(1.31)\nWe can use the local reparameterization trick and Monte-Carlo (MC) dropout to\nobtain an implicit q(ωωω) distribution we can draw samples from in a deep learning\ncontext [Gal and Ghahramani, 2016a]. The posterior BMA using likelihood p(Dtrain |\nωωω) and prior p(ωωω) is then approximated by:\np(y | x, Dtrain) ≈Eq(ωωω|Dtrain)[p(y | x,ωωω)].\n(1.32)\nWe often omit making this last step explicit in our deductions and use p(ωωω | Dtrain)\nwhen, in practice, we would substitute q(ωωω).\n1. Preliminaries\n9\nStochastic Parameters and Model Choices. Importantly, we often use of Bayesian\nmodels in the narrow sense that we only require some stochastic parameters ΩΩΩwith\na distribution p(ωωω). This choice of p(ωωω) covers (deep) ensembles [Dietterich, 2000;\nLakshminarayanan et al., 2017], neural networks with stochasticity in a subset of\nparameters [Sharma et al., 2022], as well as models with additional stochastic inputs\n[Osband et al., 2021a] or randomized training data through subsampling of the training\nset, e.g., bagging [Breiman, 1996]. Similar views have been put forward by He et al.\n[2020]; Wilson and Izmailov [2020] for deep ensembles, and for random forests [Shaker\nand Hüllermeier, 2020].\n1.2.3\nUncertainty Quantification\nUncertainty quantification is a broad field, but two types of uncertainty are often of\ninterest in machine learning: epistemic uncertainty, which is inherent to the model,\ncaused by a lack of training data, and hence reducible with more data, and aleatoric\nuncertainty, caused by inherent noise or ambiguity in data, and hence irreducible\nwith more data [Der Kiureghian and Ditlevsen, 2009]. Disentangling these two is\ncritical for tasks such as:\n• Active Learning. In active learning, we want to avoid inputs with high aleatoric\nand low epistemic uncertainty, as these will not be informative for the model [Gal\net al., 2017].\n• Out-of-Distribution Detection. In OoD detection [Hendrycks and Gimpel,\n2017], we want to avoid flagging ambiguous in-distribution (iD) examples as OoD.\n• Deferral of Uncertain Predictions. To defer predictions [Filos et al., 2019],\nwe seek inputs with either high epistemic uncertainty or high aleatoric uncertainty,\nfor different purposes: in the former case, we want to defer to an expert for a\ndecision, while in the latter case, we want to defer to the data source for a better\nmeasurement.\nThese tasks and distinctions matter in particular for noisy and ambiguous datasets\nfound in safety-critical applications like autonomous driving [Huang and Chen, 2020]\nand medical diagnosis [Esteva et al., 2017; Filos et al., 2019].\nWhile there are many ways to approximate these uncertainties using metrics,\nwe will focus on three in the next chapters:\nmutual information, entropy, and\nfeature-space density.\nIn this subsection, we will define epistemic and aleatoric uncertainty in more detail\nand give a brief overview of these three metrics and relevant concepts:\nEpistemic Uncertainty. For point x, epistemic uncertainty is a quantity which is\nhigh for a previously unseen x, and decreases when x is added to the training set\nand the model is updated [Kendall and Gal, 2017]. This conforms with using mutual\ninformation in Bayesian models and deep ensembles [Smith and Gal, 2018] and\nfeature-space density in deterministic models as surrogates for epistemic uncertainty\n[Postels et al., 2020] as we will examine in chapter §3.\nAleatoric Uncertainty. For a point x, aleatoric uncertainty is a quantity which\nis high for ambiguous or noisy samples [Kendall and Gal, 2017]. For example, in\nclassification, aleatoric uncertainty will be high when multiple labels were to be\nobserved at x. Crucially, aleatoric uncertainty does not decrease with more data\nbecause it is inherent to the data. As we gather more data, it can actually increase as\n1. Preliminaries\n10\nwe might have undersampled the data distribution. Note that aleatoric uncertainty\nis only meaningful in-distribution, as, by definition, it quantifies the level of noise or\nambiguity which might be observed. More practically speaking, if the probability of\nobserving x under the data generating distribution is zero, the conditional probability\np(y|x) = p(x,y)\np(x) is undefined, and hence, neither is the respective entropy as a measure\nof aleatoric uncertainty.\nBayesian Models. To measure uncertainty, principled approaches exist for Bayesian\nmodels [Gal, 2016]. Given a Bayesian model p(y,ωωω | x), its predictive entropy H[Y |\nx, Dtrain] upper-bounds the epistemic uncertainty, where epistemic uncertainty is\nquantified as the mutual information I[Y ;ΩΩΩ| x, Dtrain] between parameters ΩΩΩand\noutput Y for a given input x [Gal, 2016; Smith and Gal, 2018]:\nH[Y | x, Dtrain]\n|\n{z\n}\npredictive\n= I[Y ;ΩΩΩ| x, Dtrain]\n|\n{z\n}\nepistemic\n+ H[Y | x,ΩΩΩ, Dtrain]\n|\n{z\n}\naleatoric (for iD x)\n(1.33)\n⇔I[Y ;ΩΩΩ| x, Dtrain] = H[Y | x, Dtrain] −H[Y | x,ΩΩΩ, Dtrain].\n(1.34)\nPredictive entropy will be high for both iD ambiguous samples (high aleatoric\nuncertainty) and for OoD samples (high epistemic uncertainty). Hence, predictive\nentropy is a good measure for informativeness for active learning or as a metric for\nOoD detection only when used with curated datasets that do not contain ambiguous\nsamples. Note that aleatoric uncertainty is only meaningful in-distribution because\nit quantifies the level of noise or ambiguity which might be observed for input x.\nLooking at the two terms in equation (1.34), for the mutual information to be high,\nthe left term has to be high and the right term low. The left term is the entropy of\nthe model prediction, which is high when the model’s prediction is uncertain. The\nright term is an expectation of the entropy of the model prediction over the posterior\nof the model parameters and is low when the model is overall certain for each draw\nof model parameters from the posterior. Both can only happen when the model\nhas many possible ways to explain the data, which means that the different models\ninduced by different parameter samples are disagreeing among themselves.\nThis intuitively satisfies the definition of epistemic uncertainty above, as adding a\npoint to the training set ought to decrease the epistemic uncertainty as there will be\nless disagreement among the models induced by different parameter samples after\ntraining with the point.\nDeep Ensembles. The predictions of a deep ensemble [Lakshminarayanan et al.,\n2017] are the average of the outputs of an ensemble of neural networks. The total\nuncertainty of the prediction is then estimated as the entropy of the averaged softmax\noutputs. This can be viewed as approximating the BMA over the distribution of\nall possibly trained models [Wilson and Izmailov, 2020], as each ensemble member,\nproducing a softmax output p(y | x,ωωω), can be considered to be drawn from some\ndistribution p(ωωω) of the possibly trained model parameters ΩΩΩ, which is induced by\nthe push forward of the weight initialization under stochastic optimization. As a\nresult, Equation 1.34 can also be applied to Deep Ensembles to disentangle aleatoric\nand epistemic uncertainty from predictive uncertainty.\n1. Preliminaries\n11\nDespite the high computational overhead at training and test time, Deep Ensembles\nalong with recent extensions [Smith and Gal, 2018; Wen et al., 2020; Dusenberry\net al., 2020] form the state-of-the-art in uncertainty quantification in deep learning.\nIn practice, both mutual information I[Y ;ΩΩΩ| x, Dtrain] and predictive entropy H[Y |\nx, Dtrain] are used in the literature for active learning and to detect OoD data, but\npredictive entropy will be high whenever either epistemic uncertainty is high, or when\naleatoric uncertainty is high: it upper-bounds the mutual information. This can help\nseparate iD and OoD data better for curated iD datasets, offering an explanation for\nprevious empirical findings of predictive entropy outperforming mutual information\n[Malinin and Gales, 2018]. With ambiguous iD samples, it can lead to confounding,\nhowever, which we analyze in chapter §3.\nDeterministic Models. A single deterministic model, in the sense that we use a\nsingle parameter point estimate, produces a softmax distribution p(y | x,ωωω), and\ncommonly either the softmax confidence maxc p(y = c | x,ωωω) or the softmax entropy\nH[Y | x,ωωω] are used as a measure of uncertainty [Hendrycks and Gimpel, 2017]. In\nactive learning, the softmax entropy and confidence are often used as a baseline\nmeasure of informativeness. Note that the confidence is known as variation ratios\nin active learning Freeman [1965]. In both settings, they do not perform as well as\ndeep ensembles [Beluch et al., 2018].\nPopular approaches to improve these metrics include pre-processing of inputs and\npost-hoc calibration methods [Liang et al., 2018; Guo et al., 2017], alternative\nobjective functions [Lee et al., 2018a; DeVries and Taylor, 2018], and exposure to\noutliers [Hendrycks et al., 2019]. However, these methods are known to suffer from\nshortcomings: they can fail under distribution shift [Ovadia et al., 2019], require\nsignificant changes to the training setup, or assume the availability of OoD samples\nduring training.\nFeature-Space Distances & Feature-Space Density. Feature-space distances\n[Lee et al., 2018b; van Amersfoort et al., 2020; Liu et al., 2020a] and feature-space\ndensity [Postels et al., 2020; Settles, 2010] based on the training set offer a different\napproach for estimating uncertainty in deterministic models: satisfying the definition\nof epistemic uncertainty above, they decrease when previously unseen samples are\nadded to the training set. This is the case for feature-space distance and density\nmethods because they estimate distance or density, respectively, of the training data\nin feature space. A previously unseen point with high distance (low density), once\nadded to the training data, will have low distance (high density). Hence, they can\nbe used as a proxy for epistemic uncertainty, under important assumptions about\nthe feature space as detailed below. We will examine this in more detail in §3. None\nof these methods, however, is competitive with Deep Ensembles, in uncertainty\nquantification, potentially for the reasons discussed next.\nFeature Collapse. van Amersfoort et al. [2020] argue that feature collapse is why\ndistance and density estimation in the feature space may fail to capture epistemic\nuncertainty: feature extractors might map the features of OoD inputs into iD regions\nin the feature space [van Amersfoort et al., 2021], making it difficult for the later\nstages to distinguish between iD and OoD samples.\n1. Preliminaries\n12\nSmoothness & Sensitivity. To prevent feature collapse, smoothness and sensitivity\ncan be encouraged by subjecting the feature extractor fθ, with parameters θ to a\nbi-Lipschitz constraint:\nKL dI(x1, x2) ≤dF(fθ(x1), fθ(x2)) ≤KU dI(x1, x2),\nfor all inputs, x1 and x2, where dI and dF denote metrics for the input and feature\nspace respectively, and KL and KU the lower and upper Lipschitz constants [Liu et al.,\n2020a]. The lower bound ensures sensitivity to distances in the input space, and the\nupper bound ensures smoothness in the features, preventing them from becoming\ntoo sensitive to input variations, which, otherwise, can lead to poor generalization\nand loss of robustness [van Amersfoort et al., 2020]. Methods of encouraging bi-\nLipschitzness include: i) gradient penalty, by applying a two-sided penalty to the\nL2 norm of the Jacobian [Gulrajani et al., 2017], and ii) spectral normalization\n[Miyato et al., 2018] in models with residual connections, like ResNets [He et al.,\n2016]. [Smith et al., 2021] provides an in-depth analysis which supports that spectral\nnormalization leads to bi-Lipschitzness. Compared to the Jacobian gradient penalty\n[van Amersfoort et al., 2020], spectral normalization is significantly faster and has\nmore stable training dynamics.\n1.2.4\nActive Learning\nOne key problem in deep learning is data efficiency. While excellent performance\ncan be obtained with modern approaches, these are often data-hungry, rendering the\ndeployment of deep learning in the real-world challenging for many tasks. Active\nlearning [Atlas et al., 1989; Cohn et al., 1996] is a powerful technique for improving\nlabelling efficiency; it has a rich history in the machine learning community, with its\norigins dating back to seminal works such as Atlas et al. [1989]; Lindley [1956]; Fedorov\n[1972]; MacKay [1992b]. A comprehensive survey of early active learning methods can\nbe found in Settles [2010], while more recent surveys of contemporary deep learning\nmethods can be found in Ren et al. [2022] and Zhan et al. [2022a].\nInstead of a priori collecting and labelling a large dataset, which often comes at\na significant expense, active learning provides a mechanism for effective training of\nmachine learning models in settings where unlabeled data is plentiful, but labelling\nis expensive by carefully selecting which data points to acquire labels for, using\ninformation from previously acquired data to establish the points whose labels will be\nmost informative for training. After each acquisition step, the newly labeled points\nare added to the training set, and the model is retrained. This process is repeated\nuntil a suitable level of accuracy is achieved. The goal of active learning is to minimize\nthe amount of data that needs to be labeled. Active learning has made real-world\nimpact in manufacturing [Tong, 2001], robotics [Calinon et al., 2007], recommender\nsystems [Adomavicius and Tuzhilin, 2005], medical imaging [Hoi et al., 2006], and NLP\n[Siddhant and Lipton, 2018], motivating further exploration of this fascinating topic.\nOrigins. The conceptual origins of active learning can be traced back to Bayesian-\noptimal experiment design [Chaloner and Verdinelli, 1995a; Lindley, 1956; Rainforth\net al., 2023]. In machine learning, the non-Bayesian approaches started in the sequential\n(stream-based) setting as selective sampling [Atlas et al., 1989] before being referred\nto more generally as active learning [Atlas et al., 1989]. Bayesian methods and active\n1. Preliminaries\n13\nlearning were connected early on by e.g. MacKay [1992b] with objectives that are\nstill highly relevant today—as we will see, these objectives are especially relevant for\nthis thesis. An excellent literature review of the original non-Bayesian active learning\nparadigms can be found in Settles [2010].\nBayesian Optimization. Active learning is also closely related to Bayesian Optimiza-\ntion (BO), which is a well-established methodology for global optimization of black-box\nfunctions [Mockus, 1974; Jones et al., 1998], especially when the function evaluations\nare expensive. Typical applications of BO span from machine learning and statistics to\nengineering and experimental design [Snoek et al., 2012; Shahriari et al., 2016; Saleh\net al., 2022] under various constraints [Nguyen et al., 2017; Siivola et al., 2021].\nBO is based on building a probabilistic surrogate model of the black-box function,\noften a Gaussian process [Williams and Rasmussen, 2006], and using an acquisition func-\ntion to drive the search towards the regions of the input space that are likely to improve\nupon the current best solution [Shahriari et al., 2016]. The queries are sequentially\nselected and have to balance exploration and exploitation [Srinivas et al., 2010].\nMany BO methods utilize the expected improvement heuristic [Hernández-Lobato\net al., 2014, 2015, 2016] or rely on Thompson sampling (TS) [Thompson, 1933;\nRusso and Roy, 2013].\nBatch Bayesian Optimization. The classical BO setting is sequential, where the\nnext query directly depends on the outcomes of the previous evaluations. However,\nthis does not take into account that often experiments can be conducted in parallel,\nsuch as in large-scale computing environments or in laboratory experiments where\nmeasurements may come from different sources and may introduce significant waiting\ntimes otherwise [Folch et al., 2023].\nTo address this, batch Bayesian optimization (BBO) methods have been developed,\nwhere multiple decisions are made simultaneously and evaluated in parallel, providing\na significant acceleration over the sequential setting [Groves and Pyzer-Knapp, 2018;\nChowdhury and Gopalan, 2019; Oh et al., 2021]. Many BBO methods have been\nproposed in the literature, including methods based on the expected improvement\nheuristic [Shah and Ghahramani, 2015; González et al., 2016; Alvi et al., 2019],\nThompson sampling [Kandasamy et al., 2018], and upper confidence bound [Contal\net al., 2013; Daxberger and Low, 2017].\nSemi-Supervised Learning. A related approach to active learning is semi-supervised\nlearning (also sometimes referred to as weakly-supervised), in which the labeled data\nis commonly assumed to be fixed, and the unlabeled data is used for unsupervised\nlearning [Kingma et al., 2014; Rasmus et al., 2015]. Wang et al. [2017]; Sener and\nSavarese [2018]; Sinha et al. [2019] explore combining it with active learning.\nPool-Based Active Learning. Active learning is most often done in a pool-based\nsetting [Lewis and Gale, 1994], wherein we start with a large reservoir of unlabeled\ndata points, known as the pool set Dpool, from which we sequentially choose points to\nlabel, after which they are removed from Dpool and added to the training dataset Dtrain\ntogether with their acquired label. In this thesis, we focus on such settings. The steps of\nan active learning loop in a pool-based setting are depicted in Figure 1.3(a). The main\nchallenge in pool-based batch active learning is the choice of the acquisition function.\n1. Preliminaries\n14\nAcquisition Functions. The mechanism by which we choose points to label is known\nas an acquisition strategy and most commonly corresponds to choosing the data\npoint which maximizes a prespecified acquisition function that reflects the utility of\nacquiring a label for that point. (Similarly, as we will see, when we perform batch\nacquisition, we will want to find the batch of points that maximize a joint utility\nscore.) We will define acquisition functions as functions that return a score for an\nindividual sample a (x, M) given the current model M. We then acquire labels for\nthe sample that maximizes the score:\nxacq,∗≜arg max\nxacq∈Dpool a (xacq, M).\n(1.35)\nThere are several simple acquisition functions which are often used as baselines\n[Gal et al., 2017]:\nEntropy. The predictive entropy is used as acquisition score. It is defined as:\naEntropy(x; M) ≜H[Y | x].\n(1.36)\nIt is non-negative and measures the total uncertainty that model assigns to an input\nx.\nVariation Ratio. Also known as “least confidence”, the variation-ratio is the com-\nplement of the most-confident class prediction and thus selects samples with the\nlowest confidence for acquisition:\naVariation−Ratios(x; M) ≜1 −max\ny\np(y | x).\n(1.37)\nThis scoring function is non-negative and a score of 0 means that the sample is\nuninformative: a score of 0 means that the respective prediction is one-hot, and then\nthat the expected information gain is also 0, as can be easily verified.\nStandard Deviation. The standard deviation score function measures the sum\nof the class probability deviations and is closely related to the BALD scores\n(Proposition B.9, Smith and Gal [2018]):\naStd−Dev(x; M) ≜\nX\ny\nq\nVarp(ωωω)[p(y | x,ωωω)].\n(1.38)\nThis scoring function is also non-negative, and zero variance for the predictions\nimplies a zero expected information gain and thus an uninformative sample.\nInformation-Theoretic Acquisition Functions. Strategies for constructing such\nan acquisition function can be based on principled information-theoretic considerations\nthat allow formalizing the notion of the information that will be gained for labelling\nany given point. These approaches usually require a probabilistic model p(y | x,ωωω) for\nlabel y given input x, where ωωω represents a realization of stochastic model parameters\nΩΩΩ; a particularly common choice of model is a Bayesian neural network, wherein\nΩΩΩrepresents the weights and biases.\n1. Preliminaries\n15\n(Re-)train model on Dtrain\nlabeled training set\nSelect unlabeled pool points\nxacq\n1..K\n∈\nDpool\nwith highest acquisition score\nfor acquisition function a(xacq\n1..K)\nAcquire labels for\nacquisition batch: xacq\n1..K\nby querying the label oracle\n(e.g. experts)\nAdd the newly labeled samples\nDacq to Dtrain and remove\nthem from Dpool\n(a) Active Learning Loop\nEvaluate the model on\n(a subsample of)\nthe training set Dtrain\nSelect a minibatch\n(xi, yi)n\ni=1\n∈Dtrain\nwith highest acquisition\nscore a((xi, yi)n\ni=1)\nTrain model on the\nselected minibatch (xi, yi)n\ni=1\n(b) Active Sampling Loop\nFigure 1.3: (Batch) Active Learning and Active Sampling Loops. Both active learning and\nactive sampling loops share the same basic structure but differ in the way they are used to\ntrain the model. Active learning is used to train a model on a small dataset, while active\nsampling is used to train a model on a larger dataset. Active learning acquires labels using\nan expert and adds them to the dataset, while active sampling has access to the labels. Both\nuse an acquisition function to score to select the most informative samples individually or in\nbatches, which is more common in deep learning applications.\n1.2.4.1\nBayesian Active Learning\nThe Bayesian active learning setup consists of an unlabeled dataset Dpool, the current\ntraining set Dtrain, a Bayesian model M with model parameters ωωω ∼p(ωωω | Dtrain),\nand output predictions p(y | x,ωωω, Dtrain) for data point x and prediction y ∈[C] in\nthe classification case. The conditioning of ωωω on Dtrain expresses that the model has\nbeen trained with Dtrain. Furthermore, an oracle can provide us with the correct\nlabel ˜y for a data point in the unlabeled pool x ∈Dpool. The goal is to obtain a\ncertain level of prediction accuracy with the least amount of oracle queries. At each\nacquisition step, a batch of data points xacq,∗\n1..K\n= {xacq,∗\n1\n, . . . , xacq,∗\nK\n} is selected using\nan acquisition function a which scores a candidate batch of unlabeled data points\nxacq\n1..K = {x1, . . . , xK} ⊆Dpool using the current model parameters p(ωωω | Dtrain):\nxacq,∗\n1..K\n= arg max\nxacq\n1..K⊆Dpool a\n\u0010\nxacq\n1..K, p(ωωω | Dtrain)\n\u0011\n.\n(1.39)\n1. Preliminaries\n16\nThere are a number of intuitive choices for the acquisition function. We focus on the\nmodel uncertainty, which is also known as the expected information gain or BALD\n[Houlsby et al., 2011], and has proven itself in the context of deep learning [Gal et al.,\n2017; Shen et al., 2018; Janz et al., 2017]. BALD scores points based on how well their\nlabel would inform us about the true model parameter distribution.\n1.2.4.2\nBALD & Expected Information Gain\nBALD (Bayesian Active Learning by Disagreement) [Houlsby et al., 2011] uses an\nacquisition function that estimates the mutual information between the model pre-\ndictions and the model parameters. It is also referred to as the Expected Information\nGain [Lindley, 1956] or the Total Information Gain [MacKay, 1992c].\nThe Expected Information Gain (EIG) for ΩΩΩunder p(y | x,ωωω) was originally\nintroduced in Bayesian-optimal experiment design (BOED) [Lindley, 1956; Chaloner\nand Verdinelli, 1995b; Rainforth et al., 2023] to quantify the utility of data and has a\nlong history [Fedorov, 1972]. The framework of Bayesian experimental design has many\napplications outside active learning, and in these applications the model parameters are\ncommonly the quantity of interest—Bayesian optimization [Hennig and Schuler, 2012;\nHernández-Lobato et al., 2014; Villemonteix et al., 2009] being a notable exception.\nThe EIG in the parameters is thus often a natural acquisition function in BOED:\nEIG(x) ≜Ep(y|x,Dtrain)[H(p(ΩΩΩ| Dtrain)) −H(p(ΩΩΩ| y, x, Dtrain))],\n(1.40)\nwhich is equal to the mutual information I[ΩΩΩ; Y | x] between the parameters and the\nlabel given the data point x [Cavagnaro et al., 2010]. The first term in the expectation\nis the entropy of the prior distribution over ΩΩΩand the second term is the entropy\nof the posterior distribution over ΩΩΩgiven the label of x. Intuitively, it measures\nthe expected reduction in uncertainty about ΩΩΩafter observing the label of x, where\np(y | x, Dtrain) = Ep(y|x,ωωω)[ωωω | Dtrain] is the marginal predictive distribution of the model.\nComputing the EIG using above expansion can be difficult as we need to compute\nthe conditional entropy H[ΩΩΩ| Y, x]—yet this is what many recent approaches effectively\nattempt to do via the Fisher information. (We do not need to compute the entropy\nof the parameters H[ΩΩΩ], as it does not depend on the data x and can be ignored.)\nWe will examine this further in §9.\nWhile the EIG focuses on the formulation of the mutual information term as\na reduction in model posterior uncertainty given a potential sample, the EIG is\nalso equal to the conditional mutual information between the parameters and the\nlabel, I[Y ;ΩΩΩ| x, Dtrain]. This equivalent formulation, which focuses on the predictive\ndisagreement, was popularized as BALD in deep active learning by [Gal et al., 2017] as\nBALD (Bayesian Active Learning by Disagreement) [Houlsby et al., 2011] when used\nwith Bayesian neural networks [Neal, 1995]. BALD can be much easier to evaluate\nby sampling from ΩΩΩwithout the need for additional Bayesian inference. Originally\nintroduced outside the context of deep learning, the only requirement on the model\nis that it is Bayesian, but notably BALD is often used even when inference is not\nprecisely Bayesian, for example when using Monte Carlo dropout in a neural network\n[Gal and Ghahramani, 2016a]. Concretely, BALD is defined following (1.34) as\nBALD(x) ≜I[Y ;ΩΩΩ| x, Dtrain] = H[Y | x, Dtrain] −H[Y | x,ΩΩΩ, Dtrain],\n(1.41)\nwhich we already introduced as a means of measuring epistemic uncertainty.\n1. Preliminaries\n17\nOverall, both formulations express how strongly the model predictions for a given\ndata point and the model parameters are tied, implying that finding out about the true\nlabel of data points with high mutual information will also inform us about the true\nmodel parameters. Another way to see how BALD captures predictive disagreement\nis to see that above definition is equivalent to the expected KL divergence between\nthe marginal distribution and the individual distribution, which measures the average\ndisagreement of the posterior predictions:\nI[Y ;ΩΩΩ| x, Dtrain] = DKL(p(Y | x,ΩΩΩ) ∥p(Y | x))\n(1.42)\n= Ep(ωωω|Dtrain)[DKL(p(Y | x,ωωω) ∥p(Y | x))].\n(1.43)\n1.2.5\nBatch Active Learning\nAs two chapters (§4 and §5) of the thesis are concerned with batch active learning,\nwe examine some relevant work here.\nResearchers in active learning [Atlas et al., 1989; Settles, 2010] have identified\nthe importance of batch acquisition as well as the failures of top-K acquisition using\nstraightforward extensions of single-sample methods in a range of settings including\nsupport vector machines [Campbell et al., 2000; Schohn and Cohn, 2000; Brinker, 2003;\nGuo and Schuurmans, 2007], GMMs [Azimi et al., 2012], and neural networks [Sener\nand Savarese, 2018; Ash et al., 2020; Baykal et al., 2021]. Many of these methods aim\nto introduce structured diversity to batch acquisition that accounts for the interaction\nof the points acquired in the learning process.\nMaintaining diversity when acquiring a batch of data has been attempted using\nconstrained optimization [Guo and Schuurmans, 2007] and in Gaussian Mixture Models\n[Azimi et al., 2012]. In active learning of molecular data, the lack of diversity in\nbatches of data points acquired using the BALD objective has been noted by Janz\net al. [2017], who propose to resolve it by limiting the number of MC dropout samples\nand relying on noisy estimates.\nIn deep learning, another practical aspect that favors batch acquisition over individ-\nual point acquisition is that retraining a model can take a substantial amount of time.\nOne way to extend the formalism of acquisition functions to the batch acquisition\ncase is by scoring batches of samples instead of individual samples: a (x1..n). In batch\nactive learning, we then want to maximize over possible subsets (of size K):\nxacq,∗\n1..K\n= arg max\nxacq\n1..K⊆Dpool a (xacq\n1..K, M).\n(1.44)\nTop-K BALD. BALD was originally intended for acquiring individual data points\nand immediately retraining the model. Applications of BALD [Gal and Ghahramani,\n2016a; Janz et al., 2017] usually acquire the top K samples. This can be expressed\nas summing over individual scores:\naBALD\n\u0010\nxacq\n1..K, p(ωωω | Dtrain)\n\u0011\n=\nK\nX\ni=1\nBALD(xacq\ni\n)\n(1.45)\n=\nK\nX\ni=1\nI[ΩΩΩ; Y acq\ni\n| xacq\ni\n, Dtrain],\n(1.46)\nand finding the optimal batch for this acquisition function using a greedy algorithm,\nwhich reduces to picking the top-K highest-scoring data points.\n1. Preliminaries\n18\n1.2.6\nActive Sampling\nAnother key problem in deep learning is training efficiency. While large models can\nexhibit great performance, training them can be time-consuming and expensive. Active\nsampling is concerned with improving model performance as quickly as possible by\nselecting the samples to train on from within a larger training set of labeled samples.\nUnlike in active learning, labels are available, and the goal of active sampling is to\nselect the most informative samples at each step to train on.\nActive sampling selects labeled samples to train on during training. Like in active\nlearning, candidate batches of labeled samples can be scored via an acquisition function.\nThis results in an active sampling loop which is similar to the active learning loop.\nHowever, whereas in active learning the model is usually reset and retrained between\niterations [Ash and Adams, 2020], the model weights are, of course, not reset after\neach iteration. The active sampling loop is depicted in Figure 1.3(b).\nGiven the conceptual similarities between active learning and active sampling,\nwe will consider how to unify objectives in §9. Recent work has shown that active\nlearning methods can even outperform many active sampling methods without label\ninformation in certain circumstances [Park et al., 2022].\nOrigins. It is not clear where the name active sampling originates from. The author is\naware of it being used in meetings with collaborators from Google. A possible reference\nis Abernethy et al. [2022], which selects samples to train on based on fairness metrics.\nData Subset Selection Methods. Active sampling also encompasses data pruning\n[Paul et al., 2021; Siddiqui et al., 2023] and core-set selection methods [Campbell\nand Broderick, 2019, 2018; Mirzasoleiman et al., 2020; Borsos et al., 2020, 2021;\nZhou et al., 2023] as well as other data subset selection approaches [Aljundi et al.,\n2019; Killamsetty et al., 2020, 2021a; Kaushal et al., 2021]: those could be seen as\n‘single-step’ active sampling methods. Guo et al. [2022] contain a good overview of\ncurrent data subset selection methods.\nInformation Gain. A natural information-theoretic choice is the information gain\nI[ΩΩΩ; y | x] [Sun et al., 2022], which unlike the expected information gain, takes into\naccount the available label. We discuss this notation in §2 in more detail and examine\nthe information gain in §9.\n1.3\nThesis Outline\nIn this thesis, we examine objectives and extensions that follow from information-\ntheoretic intuitions. Similar objectives can be adapted with success for both active\nlearning and active sampling. The thesis is structured as follows:\n§2 A Practical & Unified Notation for Information Quantities. We define\ninformation quantities using a notation that allows us to take into account outcomes\nas well.\nUsually, information quantities are only explicitly considered between\nunobserved random variables.\nThe exception to that is the point-wise mutual\ninformation, which is well-known in NLP. Our notation unifies the mutual information\nand the point-wise mutual information (and the information gain and information\nsurprise). This chapter is entirely based on Kirsch and Gal [2021]:\n1. Preliminaries\n19\nAndreas Kirsch and Yarin Gal.\nA Practical & Unified Notation for\nInformation-Theoretic Quantities in ML. arXiv preprint, 2021.\n§3 Single Forward-Pass Aleatoric and Epistemic Uncertainty. Epistemic\nuncertainty is important as an informativeness score in active learning. We examine\naleatoric and epistemic uncertainty in more detail, and based on several simple\nbut crucial high-level observations, we propose a new baseline for uncertainty\nquantification using single forward-pass deep neural networks, e.g. deterministic\nneural networks instead of deep ensembles or similar. We draw a connection between\nfeature-space density and informativeness (epistemic uncertainty) and show that\nwith proper inductive biases, our simple approach can quantify epistemic uncertainty\nwell with competitive results in active learning (and out-of-distribution detection)\nwithout having to be Bayesian. This chapter redrafts Mukhoti et al. [2023]:\nJishnu Mukhoti*, Andreas Kirsch*, Joost van Amersfoort, Philip H. S.\nTorr, and Yarin Gal. Deterministic Neural Networks with Appropriate\nInductive Biases Capture Epistemic and Aleatoric Uncertainty. IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR), 2023,\nwith additional figures, explanations, and results (§3 and §3.1.3, Figure 3.1, and Propo-\nsition 3.2).\n§4 Diverse Batch Acquisition for Bayesian Active Learning. Active learning\nis often extended to batch active learning by greedily selecting the top-K samples\nthat individually have the highest informativeness score. We note that this is not a\nprincipled approach and can lead to worse active learning performance as it does\nnot take dependencies and redundancies between the samples into account. We\nderive BatchBALD, a principled extension of the BALD acquisition score for batch\nacquisition, which avoids this issue. The employed techniques and insights are not\nlimited to BALD and are used throughout the thesis. This chapter extends Kirsch\net al. [2019]:\nAndreas Kirsch*, Joost van Amersfoort*, and Yarin Gal. BatchBALD:\nEfficient and Diverse Batch Acquisition for Deep Bayesian Active Learning.\nIn Advances in Neural Information Processing Systems, 2019,\nwith additional figures (Figures 4.3 and 4.5).\n§5 Stochastic Batch Acquisition for Deep Active Learning. BatchBALD has\ndifficulties in scaling to higher batch acquisition sizes, both due to the computational\ncost and the scores becoming too uniform at higher batch acquisition sizes. Instead,\nwe take a different approach, and we examine how progressive acquisitions in active\nlearning change the EIG scores and their effect on top-K batch acquisitions. Based\non this, we examine a simple stochastic baseline that avoids the pathologies of\ntop-K acquisition by adding Gumbel noise to the individual scores. This very simple\nbaseline is surprisingly strong and also much cheaper to compute than the principled\nextension of the EIG to batch acquisition in §4. This chapter is entirely based on\nKirsch et al. [2023]:\n1. Preliminaries\n20\nAndreas Kirsch*, Sebastian Farquhar*, Parmida Atighehchian, Andrew\nJesson, Frederic Branchaud-Charron, and Yarin Gal. Stochastic Batch\nAcquisition for Deep Active Learning. Transactions on Machine Learning\nResearch, 2023.\n§6 Marginal and Joint Cross-Entropies & Predictives. Focusing on joint\npredictives rather than marginal predictives can highlight the potential of Bayesian\ndeep learning in real-world applications. In this chapter, we discuss online Bayesian\ninference, which allows making predictions while taking into account additional data\nwithout retraining, and propose new challenging evaluation settings using active\nlearning and active sampling. By examining marginal and joint predictives, their\nrespective cross-entropies, and their role in offline and online learning, we highlight\npreviously unidentified gaps in current research and emphasize the need for better\napproximate joint predictives. This chapter builds on insights from Wen et al.\n[2021] and Osband et al. [2022b], and we suggest further experiments to explore the\nfeasibility of current BDL inference techniques in high-dimensional parameter spaces.\nThis chapter is entirely based on Kirsch et al. [2022]:\nAndreas Kirsch, Jannik Kossen, and Yarin Gal. Marginal and Joint Cross-\nEntropies & Predictives for Online Bayesian Inference, Active Learning,\nand Active Sampling. arXiv preprint, 2022.\n§7 Prediction- & Distribution-Aware Bayesian Active Learning. The EIG\ndoes not take the target distribution of inputs into account at all. In other words\n(and if the target distribution matches the pool set distribution), it does not make use\nof the unlabeled data to guide the sample selection. To account for this, we derive\nthe expected predictive information gain (EPIG), an information quantity, which\ntakes the data distribution into account. We show that it equivalently measures the\nexpected reduction in generalization loss on the target distribution. We also examine\nand compare to the joint expected predictive information gain (JEPIG), a related\nquantity, which we connect to Bayesian model selection. This chapter extends Kirsch\net al. [2021a] and Bickford-Smith et al. [2023]:\nAndreas Kirsch, Tom Rainforth, and Yarin Gal. Active Learning under\nPool Set Distribution Shift and Noisy Data. arXiv preprint, 2021a,\nFreddie Bickford-Smith*, Andreas Kirsch*, Sebastian Farquhar, Yarin Gal,\nAdam Foster, and Tom Rainforth. Prediction-Oriented Bayesian Active\nLearning. International Conference on Artificial Intelligence and Statistics,\n2023,\nwith additional and reworked sections, examples, and figures (Example 7.1, §7.2.2,\n§7.4.1 and §7.4.2, Figure 7.3, and Proposition 7.2).\n§8 Prioritized Data Selection during Training. For active sampling (label-aware\nactive learning), we examine an adaptation of the previous two acquisition functions\nthat take the labels into account: the (joint) predictive information gain—(J)PIG.\nTraining on web-scale data can take months. But most computation and time is\nwasted on redundant and noisy points that are already learned or not learnable.\n1. Preliminaries\n21\nTo accelerate training, based on JPIG, we perform a series of approximations and\nintroduce the Reducible Holdout Loss Selection (RHO-LOSS) in a non-Bayesian\nsetting using two non-Bayesian (deterministic) models for active sampling. RHO-\nLOSS is a simple but principled technique which selects approximately those points\nfor training that most reduce the model’s generalization loss and trains in far fewer\nsteps than prior art, improves accuracy, and speeds up training on a wide range of\ndatasets, hyperparameters, and architectures (MLPs, CNNs, and BERT). On a large\nweb-scraped image dataset (Clothing-1M), RHO-LOSS trains in 18x fewer steps and\nreaches 2% higher final accuracy than uniform data shuffling. This chapter is based\non Mindermann et al. [2022]:\nSören Mindermann*, Jan Markus Brauner*, Muhammed Razzak*, Mrinank\nSharma*, Andreas Kirsch, Winnie Xu, Benedikt Höltgen, Aidan N. Gomez,\nAdrien Morisot, Sebastian Farquhar, and Yarin Gal. Prioritized Training\non Points that are Learnable, Worth Learning, and not yet Learnt. In\nProceedings of the International Conference on Machine Learning (ICML),\n2022,\nwith a reworked theory section (§8.2).\n§9 Unifying Approaches in Active Learning and Active Sampling. Many\napproaches in data subset selection use Fisher information, Hessians, similarity\nmatrices based on gradients, or the gradient length to estimate how informative\ndata is for a model’s training. Are these different approaches connected, and if so,\nhow? We revisit the fundamentals of Bayesian optimal experiment design and show\nthat these recently proposed methods can be understood as approximations to the\ninformation-theoretic quantities examined in this thesis: EIG/BALD and (J)EPIG\nfor active learning, and the information gain (IG) and (J)PIG for active sampling.\nWe develop a comprehensive set of approximations using Fisher information and the\nobserved information and derive a unified framework that connects above seemingly\ndisparate literature. Although Bayesian methods are often seen as separate from\nnon-Bayesian ones, the sometimes fuzzy notion of “informativeness” expressed in\nvarious non-Bayesian objectives leads to the same couple of information quantities,\nwhich were, in principle, already known by Lindley [1956] and MacKay [1992b]. This\nchapter is entirely based on Kirsch and Gal [2022b]:\nAndreas Kirsch and Yarin Gal. Unifying Approaches in Active Learning\nand Active Sampling via Fisher Information and Information-Theoretic\nQuantities. Transactions on Machine Learning Research, 2022b.\n§10 Black-Box Batch Active Learning for Regression. §9 show that different\nactive learning methods approximate the same information-theoretic quantities using\ntwo different perspectives:\n• Bayesian methods (including deep ensembles) often use (sampled) predictions\nover the parameter distribution to approximate information quantities, while\n• non-Bayesian methods often use the weight space (using the score, i.e. log loss\ngradients) to approximate the same information quantities.\nThe difference between these two perspectives can be viewed as being between\nwhite-box approaches, which are limited to differentiable parametric models (weight\n1. Preliminaries\n22\nspace) and black-box approaches, which only use model predictions (prediction space).\nWhite-box methods score unlabeled points using acquisition functions based on\nmodel embeddings or first- and second-order derivatives. We utilize recent kernel-\nbased approaches and turn a wide range of existing state-of-the-art white-box batch\nactive learning methods (BADGE, BAIT, LCMD) into black-box approaches. We\ndemonstrate the effectiveness of our approach through extensive experimental evalu-\nations on regression datasets, achieving surprisingly strong performance compared\nto white-box approaches for deep learning models. This chapter is entirely based on\nKirsch [2023a]:\nAndreas Kirsch. Black-Box Batch Active Learning for Regression. Trans-\nactions on Machine Learning Research, 2023a.\nAppendices In the appendix, we provide additional details on the information-\ntheoretic quantities and the approximations used in this thesis. We also include\nadditional works that are related to the topics in this thesis and reproducibility\nanalyses of several existing works.\nAppendix A Causal-BALD: Deep Bayesian Active Learning of Outcomes\nto Infer Treatment-Effects from Observational Data. As a different\napplication of combining information-theoretic intuitions and active learning, we\ndevelop acquisition functions for estimating the conditional average treatment\neffects from observational data for causal active learning. Unlike the previous\napplications, it does not apply to classification tasks but regression tasks. This\nchapter is entirely based on Jesson et al. [2021]:\nAndrew Jesson*, Panagiotis Tigas*, Joost van Amersfoort, Andreas\nKirsch, Uri Shalit, and Yarin Gal. Causal-BALD: Deep Bayesian Active\nLearning of Outcomes to Infer Treatment-Effects from Observational\nData. In Advances in Neural Information Processing Systems, 2021.\nAppendix B Reproducibility Analysis. We also include reproducibility analyses\nof several papers that provide additional relevant insights (which are connected\nto themes in this thesis):\nAppendix B.1 Deep Learning on a Data Diet. We reproduce parts of\nPaul et al. [2021]. This chapter is entirely based on Kirsch [2023b]:\nAndreas Kirsch. Does “Deep Learning on a Data Diet” reproduce?\nOverall yes, but GraNd at Initialization does not. Transactions on\nMachine Learning Research, 2023b.\nAppendix B.2 A Note on “Assessing Generalization of SGD via\nDisagreement” We examine details of Jiang et al. [2022]. This chapter is\nentirely based on Kirsch and Gal [2022a]:\nAndreas Kirsch and Yarin Gal. A Note on “Assessing Generalization\nof SGD via Disagreement”. Transactions on Machine Learning\nResearch, 2022a.\nAppendix B.3 Dirichlet Model of a Deep Ensemble’s Softmax Predic-\ntions We examine how well Dirichlet distributions can model the predictions\nof deep ensembles and their members to approximate information quantities\n1. Preliminaries\n23\nwe care about and to sample from the posterior predictive distribution.\nThis chapter is entirely based on Mukhoti et al. [2023]:\nJishnu Mukhoti*, Andreas Kirsch*, Joost van Amersfoort, Philip\nH. S. Torr, and Yarin Gal. Deterministic Neural Networks with\nAppropriate Inductive Biases Capture Epistemic and Aleatoric\nUncertainty.\nIEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), 2023,\nand extends it with additional results.\n1.4\nContributions to Joint Work\nThe following declaration of contributions summarizes my contributions to the papers\nthat are used as a basis for my thesis. Several of the papers are joint-authored with\nme as joint first-author. Prof. Yarin Gal supervised all my research projects except\nfor Kirsch [2023b] and Kirsch [2023a].\nA Practical & Unified Notation for Information Quantities with Observed\nOutcomes [Kirsch and Gal, 2021]\nAndreas Kirsch and Yarin Gal.\nA Practical & Unified Notation for\nInformation-Theoretic Quantities in ML. arXiv preprint, 2021\nContributions. I developed the paper and idea.\nDeep Deterministic Uncertainty: A Simple Baseline [Mukhoti et al., 2023]\nJishnu Mukhoti*, Andreas Kirsch*, Joost van Amersfoort, Philip H. S.\nTorr, and Yarin Gal. Deterministic Neural Networks with Appropriate\nInductive Biases Capture Epistemic and Aleatoric Uncertainty. IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR), 2023\nContributions. Jishnu and I co-authored the paper. Jishnu and I wrote the\npaper together. I contributed significantly to the theory, in particular the observations,\nand suggested using density scores for OoD detection (using GDA) instead of scores\nbased on the predictive distribution (entropy). Yarin and I developed the idea of the\nDirty-MNIST dataset. Jishnu implemented and ran all experiments and trained all\nthe models while Joost and I provided code reviews and feedback.\nBatchBALD: Efficient and Diverse Batch Acquisition for Deep Bayesian\nActive Learning [Kirsch et al., 2019]\nAndreas Kirsch*, Joost van Amersfoort*, and Yarin Gal. BatchBALD:\nEfficient and Diverse Batch Acquisition for Deep Bayesian Active Learning.\nIn Advances in Neural Information Processing Systems, 2019\nContributions. Joost and I are joint first-authors. I developed the idea and\nalgorithm and implemented and ran the experiments. Joost developed the Repeated-\nMNIST experimental setting and came up with the acquisition size-time plot. Joost\nand I co-wrote the paper. I wrote the proofs and created the plots.\n1. Preliminaries\n24\nStochastic Batch Acquisition: A Simple Baseline for Deep Active Learning\n[Kirsch et al., 2023]\nAndreas Kirsch*, Sebastian Farquhar*, Parmida Atighehchian, Andrew\nJesson, Frederic Branchaud-Charron, and Yarin Gal. Stochastic Batch\nAcquisition for Deep Active Learning. Transactions on Machine Learning\nResearch, 2023\nContributions. Sebastian and I are joint first-authors. I implemented and ran\nthe initial experiments and wrote the workshop submission while Sebastian was on\nan internship. The initial idea of stochastic acquisition was independently and then\njointly developed. Sebastian designed the rank correlation experiment, and we jointly\nrewrote the paper with him leading and editing the paper. I created the plots and\nran experiments. Frederic, Parmida, and Andrew ran experiments.\nMarginal and Joint Cross-Entropies & Predictives for Online Bayesian\nInference, Active Learning, and Active Sampling [Kirsch et al., 2022]\nAndreas Kirsch, Jannik Kossen, and Yarin Gal. Marginal and Joint Cross-\nEntropies & Predictives for Online Bayesian Inference, Active Learning,\nand Active Sampling. arXiv preprint, 2022\nContributions.\nI was the lead author on the workshop submission and the\npreprint. Jannik supported the project’s experiment design and provided feedback\nand helped edit the paper.\nTest Distribution–Aware Active Learning: A Principled Approach Against\nDistribution Shift and Outliers [Kirsch et al., 2021a]\nAndreas Kirsch, Tom Rainforth, and Yarin Gal. Active Learning under\nPool Set Distribution Shift and Noisy Data. arXiv preprint, 2021a\nContributions. I was the lead author on the workshop submission, which focused\non JEPIG. Tom Rainforth helped redraft the paper for the second arxiv submission\nwhich focused on JEPIG, and we jointly developed the EPIG term.\nPrediction-Oriented Bayesian Active Learning [Bickford-Smith et al., 2023]\nFreddie Bickford-Smith*, Andreas Kirsch*, Sebastian Farquhar, Yarin Gal,\nAdam Foster, and Tom Rainforth. Prediction-Oriented Bayesian Active\nLearning. International Conference on Artificial Intelligence and Statistics,\n2023\nContributions. Freddie and I are joint first-authors. I helped Freddie with the\nexperiment design and provided feedback in discussion and encouragement throughout\nthe project. The paper was significantly and majorly redrafted by Freddie, Tom, and\nAdam based on the earlier arXiv preprint.\n1. Preliminaries\n25\nPrioritized Training on Points that are Learnable, Worth Learning, and\nNot Yet Learnt [Mindermann et al., 2022]\nSören Mindermann*, Jan Markus Brauner*, Muhammed Razzak*, Mrinank\nSharma*, Andreas Kirsch, Winnie Xu, Benedikt Höltgen, Aidan N. Gomez,\nAdrien Morisot, Sebastian Farquhar, and Yarin Gal. Prioritized Training\non Points that are Learnable, Worth Learning, and not yet Learnt. In\nProceedings of the International Conference on Machine Learning (ICML),\n2022\nContributions. While not being a joint first author, I contributed to the theory\nand helped draft the information theory subsection of the workshop submission. I wrote\nthe comparison to EPIG in the appendix of the workshop submission. I independently\ndeveloped the idea of a label-aware version of JEPIG before we realized that it was\nequivalent to what is now the unapproximated RHO-LOSS and joined the project.\nUnifying Approaches in Active Learning and Active Sampling via Fisher\nInformation and Information-Theoretic Quantities [Kirsch and Gal, 2022b]\nAndreas Kirsch and Yarin Gal. Unifying Approaches in Active Learning\nand Active Sampling via Fisher Information and Information-Theoretic\nQuantities. Transactions on Machine Learning Research, 2022b\nContributions. I developed the paper and idea.\nBlack-Box Batch Active Learning for Regression [Kirsch, 2023a]\nAndreas Kirsch. Black-Box Batch Active Learning for Regression. Transac-\ntions on Machine Learning Research, 2023a\nContributions. I am the single author of the paper.\nDoes “Deep Learning on a Data Diet” reproduce? Overall yes, but GraNd\nat Initialization does not [Kirsch, 2023b]\nAndreas Kirsch. Does “Deep Learning on a Data Diet” reproduce? Overall\nyes, but GraNd at Initialization does not. Transactions on Machine Learning\nResearch, 2023b\nContributions. I am the single author of the paper and received helpful feedback\nfrom the first author and senior author of the original paper, Paul et al. [2021].\nA Note on “Assessing Generalization of SGD via Disagreement” [Kirsch\nand Gal, 2022a]\nAndreas Kirsch and Yarin Gal. A Note on “Assessing Generalization of SGD\nvia Disagreement”. Transactions on Machine Learning Research, 2022a\nContributions. I developed the paper and idea.\n1. Preliminaries\n26\nCausal-BALD: Deep Bayesian Active Learning of Outcomes to Infer Treatment-\nEffects from Observational Data [Jesson et al., 2021]\nAndrew Jesson*, Panagiotis Tigas*, Joost van Amersfoort, Andreas Kirsch,\nUri Shalit, and Yarin Gal. Causal-BALD: Deep Bayesian Active Learning of\nOutcomes to Infer Treatment-Effects from Observational Data. In Advances\nin Neural Information Processing Systems, 2021\nContributions. While not being a joint first author, I contributed to the theory\nand proofs of ρ-BALD in particular.\nThe limits of my language mean the limits of my\nworld.\nLudwig Wittgenstein\n2\nA Practical & Unified Notation for\nInformation Quantities\nIn §1.2.1, we have introduced well-known information-theoretic quantities using a\nmore consistent notation.\nNow, we further canonically extend the definitions to\ntie random variables to specific observed outcomes, e.g. X = x. We will use this\nextensively in the following chapters. We refer to X when we have X = x in an\nexpression as tied random variable as it is tied to an outcome. If we mix (untied)\nrandom variables and tied random variables, we define H[ · ] as an operator which takes\nan expectation of Shannon’s information content for the given expression over the\n(untied) random variables conditioned on the tied outcomes. For example, H[X, Y = y |\nZ, W = w] = Ep(X,Z|y,w) H(p(x, y | z, w)) following this notation. We generally shorten\nY = y to y when the connection is clear from context—except for the datasets\nDpool, Dtrain, etc., which are sets of outcomes (either only containing inputs x when\nunlabeled or containing both inputs x and targets y). Similarly, we have H(p(X |\ny) ∥q(X | y)) = Ep(x|y) H(q(x | y)).\nAs a memory hook for the reader, lower-case letters are always used for tied random\nvariables and upper-case letters for (untied) random variables over which we take\nan expectation. This makes it easy to differentiate between the two cases and write\ndown the actual expressions.\nImportantly, the definitions above maintain the identities H[X, Y ] = Ep(x) H[x, Y ] =\nEp(y) H[X, y], which is the motivation behind these extensions. Figure 2.1 provides\nan overview over the quantities for two random variables X and Y when Y = y is\nobserved. We define everything in detail below and provide intuitions.\nDefinition 2.1. Given random variables X and Y and outcome y, we define:\nH[y] ≜H(p(y))\n(2.1)\nH[X, y] ≜Ep(x|y) H[x, y] = Ep(x|y) H(p(x, y))\n(2.2)\nH[X | y] ≜Ep(x|y) H[x | y] = Ep(x|y) H(p(x | y))\n(2.3)\nH[y | X] ≜Ep(x|y) H[y | x] = Ep(x|y) H(p(y | x)),\n(2.4)\nwhere we have shortened Y = y to y.\nAgain, H[y], H[X, y] are shorthands for H(p(y)), H(p(X, y)), and so on.\nThe intuition from information theory behind these definitions is that, e.g., H[X, y]\nmeasures the average length of transmitting X and Y together when Y = y unbeknownst\n2. A Practical & Unified Notation for Information Quantities\n28\nH[X, y]\nH[y]\nH[X]\nI[X; y]\nI[y; X]\nH[X | y]\nH[y | X]\nEp(x|y) H[x]\nFigure 2.1: The relationship between joint entropy H[X, y], entropies H[X], H[y], conditional\nentropies H[X | y], H[y | X], information gain I[X; y] and surprise I[y; X] when Y = y is\nobserved. We include Ep(x|y) H[x] to visualize Proposition 2.1. The figure follows Figure 8.1\nin MacKay [2003].\nto the sender and receiver, and H[y | X] measures how much additional information\nneeds to be transferred on average for the receiver to learn y when it already knows X |y.\nFrom above definition, we also have H[x, y] = H(p(x, y)) and H[x | y] = H(p(x |\ny)). Beware, however, that while we have H[X | y] = H[X, y] −H[y], for H[y | X],\nthere is no such equality for H[y | X]:\nProposition 2.1. Given random variables X and Y and outcome y, we generally\nhave:\nH[X | y] = H[X, y] −H[y]\n(2.5)\nH[y | X] = H[X, y] −Ep(x|y) H[x]\n̸= H[X, y] −H[X],\n(2.6)\nProof. H[X | y] = H[X, y] −H[y] follows immediately from the definitions.\nH[y |\nX] ̸= H[X, y]−H[X] follows because, generally, Ep(x|y) H[x] ̸= H[X] when p(x|y) ̸= p(x).\nE.g., for X and Y only taking binary values, 0 or 1, let1 p(x, y) = 1\n31{x=0=y}, then\nEp(x|y) H[x] = log\n\u0010\n3\n2\n\u0011\n̸= log\n\u0010\n3 3√\n2\n2\n\u0011\n= H[X].\nThere are two common, sensible quantities we can define when we want to consider\nthe information overlap between a random variable and an outcome: the information\ngain, also known as specific information and the surprise [DeWeese and Meister, 1999;\nButts, 2003]. These two quantities are usually defined separately in the cognitive\nsciences and neuroscience [Williams, 2011]; however, we can unify them after relaxing\nthe symmetry of the mutual information as done above:\nDefinition 2.2. Given random variables X and Y and outcome y for Y , we define the\ninformation gain I[X; y] and the surprise I[y; X] as:\nI[X; y] ≜H[X] −H[X | y]\n(2.7)\nI[y; X] ≜H[y] −H[y | X].\n(2.8)\n1See\nalso\nhttps://colab.research.google.com/drive/1HvLXUMQYcxMGZ4S_\na00xddGmfz0IHaR3.\n2. A Practical & Unified Notation for Information Quantities\n29\nThis unifying definition is novel to the best of our knowledge. It works by breaking\nthe symmetry that otherwise exists for the regular and point-wise mutual information.\nNote that the surprise can also be expressed as I[y; X] = DKL(p(X | y) ∥p(X)). For\nexample, this is done in Bellemare et al. [2016]—even though the paper mistakenly\ncalls this surprise an information gain when it is not (in our sense).\nWe enumerate a few equivalent ways of writing the mutual information and surprise—\nthe information gain has no such equivalences. This can be helpful to spot these\nquantities in the wild.\nProposition 2.2. We have\nI[X; Y ] = DKL(p(X, Y ) ∥p(X) p(Y ))\n(2.9)\nI[y; X] = Ep(x|y) I[y; x]\n(2.10)\n= Ep(x|y)[H[x]] −H[X | y]\n(2.11)\n= DKL(p(X | y) ∥p(X)).\n(2.12)\nThe information gain I[X; y] for X given y measures the reduction in uncertainty\nabout H[X] when we observe y. H[X] is the uncertainty about the true X that we want\nto learn: the entropy quantifies the amount of additional information that we need to\ntransmit to fix X, and similarly H[X | y] quantities the additional information we need\nto transmit to fix X once y is known both to the sender and the receiver [Lindley, 1956].\nOn the other hand, the surprise I[y; X] of y for X measures how much the posterior X |\ny lies in areas where p(x) was small before observing y [DeWeese and Meister, 1999].\nAn important difference between the two is that the information gain can be\nchained while the surprise cannot:\nProposition 2.3. Given random variables X, Y1, and Y2 and outcomes y1 and y2 for\nY1 and Y2, respectively, we have:\nI[X; y1, y2] = I[X; y1] + I[X; y2 | y1]\n(2.13)\nI[y1, y2; X] ̸= I[y1; X] + I[y2; X | y1].\n(2.14)\nProof. We have\nI[X; y1, y2] = H[X] −H[X | y1, y2]\n= H[X] −H[X | y1] + H[X | y1] −H[X | y1, y2]\n= I[X; y1] + I[X; y2 | y1],\nwhile\nI[y1, y2; X] = Ep(x|y1,y2) I[y1, y2; x]\n=\nEp(x|y1,y2) I[y1; x]\n|\n{z\n}\n̸= Ep(x|y1) I[y1; x] = I[y1; X]\n+ Ep(x|y1,y2) I[y2; x | y1]\n|\n{z\n}\n= I[y2; X | y1]\n.\nThat is, generally, Ep(x|y1,y2) I[y1; x] ̸= I[y1; X]. To conclude the proof, we instantiate p(x|\ny1, y2) ̸= p(x|y1): for X, Y1, and Y2 taking binary values 0, 1 only, let p(y1) = 1\n2, p(x, y2|\n2. A Practical & Unified Notation for Information Quantities\n30\ny1 = 0) =\n1\n4, p(x | y2 = 0, y1 = 1) =\n1\n2, p(x = 0 | y2 = 1, y1 = 1) = 1.\nThen\nEp(x|y1,y2) I[y1; x] = log\n\u0010\n2\n√\n3 4√\n5\n5\n\u0011\n̸= log\n\u0010\n6\n5\n\u0011\n= I[y1; X] for y1 = 1, y2 = 1 as the reader\ncan easily verify2.\nHowever, both quantities do chain in their (untied) random variables:\nProposition 2.4. Given random variables X1, X2, Y , and outcome y for Y :\nI[X1, X2; y] = I[X1; y] + I[X2; y | X1]\n(2.15)\nI[y; X1, X2] = I[y; X1] + I[y; X2 | X1].\n(2.16)\nProof. We have\nI[X1; y] + I[X2; y | X1] =\n= H[X1] −H[X1 | y] + H[X2 | X1] + H[X2 | X1, y]\n= H[X1] + H[X2 | X1]\n|\n{z\n}\n=H[X1,X2]\n−(H[X1 | y] + H[X2 | X1, y]\n|\n{z\n}\n=H[X1,X2|y]\n)\n= I[X1, X2; y].\nSimilarly, we have\nI[y; X1] + I[y; X2 | X1] =\n= H[y] −H[y | X1] + H[y | X1] −H[y | X1, X2]\n= H[y] −H[y | X1, X2]\n= I[y; X1, X2].\nThese extensions of the mutual information are canonical as they permute with\ntaking expectations over tied variables to obtain the regular (untied) quantities:\nProposition 2.5. For random variables X and Y :\nI[X; Y ] = Ep(y) I[X; y] = Ep(y) I[y; X] = Ep(x,y) I[x, y].\n(2.17)\nProof. Follows immediately from substituting the definitions.\nLikewise, when all random variables are tied to a specific outcome, the quantities\nbehave as expected:\nProposition 2.6. For random variables X, Y , Y1 and Y2:\nI[X; Y ] = I[Y ; X], and\n(2.18)\nI[x; y] = I[y; x];\n(2.19)\nI[X; Y1, Y2] = I[X; Y1] + I[X; Y1 | Y2], and\n(2.20)\nI[x; y1, y2] = I[x; y1] + I[x; y2 | y1].\n(2.21)\n2See also https://colab.research.google.com/drive/1gn6oQohRMqXKEhyCogiVDcx1VZFkShaQ.\n2. A Practical & Unified Notation for Information Quantities\n31\nProof. The only interesting equality is I[x; y1, y2] = I[x; y1] + I[x; y2 | y1]:\nI[x; y1] + I[x; y2 | y1] = H(p(x) p(y1)\np(x, y1)\np(x, y1) p(y1, y2) p(y1)\np(y1) p(y1) p(x, y1, y2) )\n= H(p(x) p(y1, y2)\np(x, y1, y2) )\n= I[x; y1, y2].\nWe can extend this to triple mutual information terms by adopting the extension\nI[X; Y ; Z] = I[X; Y ] −I[X; Y | Z] [Yeung, 2008] for outcomes as well: I[X; Y ; z] =\nI[X; Y ] −I[X; Y | z], which also works for higher-order terms.\nOverall, for the reader, there will be little surprise when working with the fully point-\nwise information-theoretic quantities, that is, when all random variables are observed.\nBut the mixed ones require more care. We refer the reader back to Figure 2.1 to recall\nthe relationships which also provide intuitions for the inequalities we will examine next.\nInequalities. We review some well-known inequalities first:\nProposition 2.7. For random variables X and Y , we have:\nI[X; Y ] ≥0\n(2.22)\nH[X] ≥H[X | Y ],\n(2.23)\nand if X is a discrete random variable, we also have:\nH[X] ≥0\n(2.24)\nI[X; Y ] ≤H[X].\n(2.25)\nProof. The first two statements follow from:\nH[X] −H[X | Y ] = I[X; Y ]\n= DKL(p(X, Y ) ∥p(X) p(Y ))\n≥0.\n(2.26)\nThe third statement follows from the monotony of the expectation and p(x) ≤1 for all\nx.\nFor mixed outcomes we find similar inequalities:\nProposition 2.8. For random variables X and Y with outcome y, we have:\nI[y; X] ≥0\n(2.27)\nH[y] ≥H[y | X]\n(2.28)\nEp(x|y)H[x] ≥H[X | y],\n(2.29)\nand if Y is a discrete random variable, we also have:\nH[y | X], H[y] ≥0\n(2.30)\nI[y; X] ≤H[y],\n(2.31)\nand if X is also a discrete random variable, we gain:\nI[y; X] ≤Ep(x|y) H[x].\n(2.32)\n2. A Practical & Unified Notation for Information Quantities\n32\nProof. Again, the first two statements follow from:\nH[y] −H[y | X] = I[y; X]\n= Ep(x|y) I[y; x]\n= Ep(x|y)[H[x] −H[x | y]]\n(2.33)\n= DKL(p(X | y) ∥p(X))\n≥0.\n(2.34)\nThe third statement follows from Equation 2.33 above as 0 ≤Ep(x|y)[H[x] −H[x |\ny]] = Ep(x|y) H[x] −H[X | y]. The fourth statement follows from p(y|x) ≤1 when Y is a\ndiscrete random variable, and thus H[y |X] ≥0 due to the monotony of the expectation.\nThe fifth statement follows from the fourth statement and I[y; X] = H[y] −H[y |\nX] ≤H[y]. Finally, if X is a discrete random variable as well, we also have H[X |\ny] ≥0, and thus\nI[y; X] = Ep(x|y)[H[x] −H[X | y]] ≤Ep(x|y) H[x].\nNote that there are no such bounds for I[X; y], H[X | y] and H[y | X].\nCorollary 2.9. We have I[y; X] = 0 exactly when p(x | y) = p(x) for all x for given y.\nProof. This follows from 0 = I[y; X] = DKL(p(X | y) ∥p(X)) exactly when p(x |\ny) = p(x).\nIn particular, there is a misleading intuition that the information gain I[X; y] =\nH[X]−H[X |y] ought to be non-negative for any y. This is not true. This intuition may\nexist because in many cases when we look at posterior distributions, we only model\nthe mean and assume a fixed variance. The uncertainty around the mean does indeed\nreduce with additional observations; however, the uncertainty around the variance\nmight not. The reader is invited to experiment with a normal distribution with known\nmean and compute the information gain on the variance depending on new observations.\nIn a sense, the information-theoretic surprise is much better behaved than the infor-\nmation gain because we can bound it in various ways, which does not seem possible for\nthe information gain. The (expected) information gain is a more useful quantity though\nfor active sampling, active learning and Bayesian optimal experimental design. Thus,\nunified notation that includes and differentiates between both quantities is beneficial.\n2.1\nExample Application: Stirling’s Approximation\nfor Binomial Coefficients\nIn MacKay [2003] on page 2, the following simple approximation for a binomial\ncoefficient is introduced:\nlog\n N\nr\n!\n≃(N −r) log\nN\nN −r + r log N\nr .\n(2.35)\nWe will derive this result using the proposed extension to observed outcomes as it\nallows for an intuitive deduction. Moreover, we will see that this allows us to use other\ntools from probability theory to estimate the approximation error.\n2. A Practical & Unified Notation for Information Quantities\n33\nH[B, r]\nH[r]\nH[B | r]\n-\nEp(b|r) H[b]\nFigure 2.2: The relationship between the information quantities used in §2.1. B is the joint\nof the binomial random variables, R is the number of successes in B with observed outcome r.\nThe arrow below H[r] symbolizes that we minimize H[r] by optimizing the success probability\nρ to close the gap between Ep(b|r) H[b] and H[B | r].\nSetup. Let B1, . . . , BN be N Bernoulli random variables with success probability\np, and let B be the joint of these random variables.\nFurther, let R be the random variable that counts the number of successes in B. R\nfollows a Binomial distribution with success probability ρ and N trials.\nMain Idea. For a given outcome r of R, we have:\nH[B, r] = H[B | r] + H[r] ≥H[B | r],\n(2.36)\nas H[·] is non-negative for discrete random variables. We will examine this inequality\nto obtain the approximation in Equation 2.35.\nNote that H[B | r] is the additional number of bits needed to encode B when the\nnumber of successes is already known. Similarly, H[B, r] is the number of bits needed\nto encode both B and R under the circumstance that R = r.\nDetermining H[B, r]. R is fully determined by B, and thus we have H[B, R] =\nH[B] and hence3:\nH[B, r] = Ep(b|r) H[b].\n(2.37)\nEp(b|r) H[b] is the expected number of bits needed to transmit the outcome b of B when\nr is given. When we encode B, we do not know r upfront, so we need to transmit N\nBernoulli outcomes. Hence, we need to transmit r successes and N −r failures. Given\nthe success probability ρ, the optimal message length for this is:\nEp(b|r) H[b] = r H(ρ) + (N −r) H(1 −ρ)\n(2.38)\n= −r log ρ −(N −r) log(1 −ρ).\n(2.39)\nAll this is visualized in Figure 2.2.\nAlternative Argument. We can also look at the terms H[B | r] + H[r] sep-\narately.\nWe have\nH[r] = −log p(r) = −log\n  N\nr\n!\nρr (1 −ρ)N−r\n!\n,\n(2.40)\nand\nH[B | r] = −Ep(b|r) log p(b | r) = log\n N\nr\n!\n.\n(2.41)\n3This also follows immediately from H[R | B] = 0 =⇒∀r : H[r | B] = 0.\n2. A Practical & Unified Notation for Information Quantities\n34\nThe former follows from R being binomial distributed. For the latter, we observe\nthat we need to encode B while knowing r already. Given r, p(b | r) = const for\nall valid b. There are\n\u0010N\nr\n\u0011\npossible b for fixed r. Hence, we can simply create a\ntable with all possible configurations with r successes. There are\n\u0010N\nr\n\u0011\nmany. We\nthen encode the index into this table.\nEach configuration with r successes has an equal probability of happening, so we\nhave a uniform discrete distribution with entropy log\n\u0010N\nr\n\u0011\nand obtain the same result.\nDetermining ρ.\nWe already have\nH[B | r] + H[r] = −r log ρ −(N −r) log(1 −ρ)\n≥log\n N\nr\n!\n= H[B | r].\n(2.42)\nHow do we make this inequality as tight as possible?\nWe need to minimize the gap H[r] which creates the inequality in the first place,\nand H[r] = −log p(r) is minimized exactly when p(r) becomes maximal.\nHence, we choose the success probability ρ to do so: the maximum likelihood\nsolution arg maxp p(r | ρ) is ρ =\nr\nN . The Binomial distribution of R then has its\nmode, mean, and median at r.\nAltogether, after substituting ρ =\nr\nN and rearranging, we see that the wanted\napproximation is actually an inequality:\nlog\n N\nr\n!\n≤−r log ρ −(N −r) log(1 −ρ)\n(2.43)\n= r log N\nr + (N −r) log\nN\nN −r.\n(2.44)\nApproximation Error H[r]. The approximation error is just H[r] as we can\nread off from Equation 2.42. We can easily upper-bound it with H[r] ≤log N: First,\nH[R] ≤log N as the uniform distribution with entropy log N is the maximum entropy\ndistribution in this case (discrete random variable with finite support).\nSecond,\nH[R] is the expectation over different H[R = r′]. We have chosen ρ =\nr\nN such that\nr is the mean of binomial distribution and has maximal probability mass.\nThis\nmeans it has minimal information content. Hence, H[r] ≤log N by contraposition\nas otherwise log N < H[r] ≤H[R].\nSimplicity is the ultimate sophistication.\nLeonardo da Vinci\n3\nSingle Forward-Pass Aleatoric and\nEpistemic Uncertainty\nIn this chapter, we delve deeper into aleatoric and epistemic uncertainty and apply the\ninsights gained to single forward-pass neural networks to disentangle these uncertainties.\nUncertainty quantification has garnered interest for such approaches because most\nwell-known methods of uncertainty quantification in deep learning [Blundell et al.,\n2015; Gal and Ghahramani, 2016a; Lakshminarayanan et al., 2017; Wen et al., 2020;\nDusenberry et al., 2020] require multiple forward passes at test time. Among these\nmethods, deep ensembles have generally exhibited superior performance in uncertainty\nprediction [Ovadia et al., 2019]. However, their substantial memory and computational\nrequirements during training and test time impede their adoption in real-life (e.g.\nmobile applications). As a result, there has been a growing interest in uncertainty\nquantification using deterministic single forward-pass neural networks, which offer a\nsmaller footprint and reduced latency. We empirically validate our findings using active\nlearning and out-of-distribution (OoD) detection on computer vision datasets.\nTo clarify, since OoD detection is not a well-defined term, we will investigate OoD\ndetection for ‘related distributions’, such as CIFAR-10 versus SVHN or CIFAR-100,\nfollowing the definition in Farquhar and Gal [2022]. Another term for this is ‘near OoD’\n[Winkens et al., 2020]. Curiously, active learning and OoD detection are rarely evaluated\ntogether; thus, we will explore some of their nuances (recall §1.2.3) and contrast them.\nActive Learning ̸= OoD Detection\nWe can conceptually differentiate active learning and OoD detection as follows:\n• Active learning seeks to identify the most informative samples for labeling, while\n• OoD detection aims to recognize samples that are not in-distribution, regardless\nof their informativeness.\nThis subtle yet crucial distinction implies that the informativeness signal in active\nlearning will generally be useful for OoD detection (assuming the training data defines\nthe in-distribution), but not the other way around: a sample that is ‘obviously’ OoD will\nnot be informative for active learning. See Figure 3.1 for a visualization of this concept.\nIn OoD literature, the terms ‘near OoD’ and ‘far OoD’ [Winkens et al., 2020] are often\nused to differentiate between points that are close to the in-distribution, leading to\nconflicting predictions, and those that are far away, making their detection unanimous.\nNear-OoD points will likely be highly informative (high epistemic uncertainty) as\nthe model might disagree about their interpretation, while far-OoD points will have\nlow informativeness (low epistemic uncertainty) as the model will confidently detect\n3. Single Forward-Pass Aleatoric and Epistemic Uncertainty\n36\niD Signal p(x)\nAleatoric\nUncertainty\nEpistemic\nUncertainty\nIn-Distribution\nNot Noisy\nIn-Distribution\nNoisy\n‘Of Course’\nOut-of-Distribution\n(‘Far OoD’)\n‘Don’t Know’\nOut-of-Distribution\n(‘Near OoD’)\nFigure 3.1: The four quadrants of the OoD detection landscape. Active learning focuses\non the bottom-right quadrant, as data points in this area will be informative for the model.\nOoD detection is concerned with the lower half, as it aims to identify out-of-distribution data\npoints (the training set is considered in-distribution). The ‘Don’t Know’ quadrant is referred\nto as near OoD, and the ‘Of Course’ quadrant as far OoD. Consequently, feature-space density\nprovides a suitable signal for active learning when the available pool data does not contain\nactual OoD data or outliers, as these will be confounded with informative in-distribution\npoints. Equivalently, epistemic uncertainty as an active learning signal will only provide a\nreliable OoD signal for near OoD but not for far OoD.\nthem as OoD. This becomes an issue when using only epistemic uncertainty for OoD\ndetection1, as reported by Xia and Bouganis [2022]. Methods that employ outlier\nexposure are particularly prone to this problem, as epistemic uncertainty will also\nbe low for outliers used during training. However, this issue can arise in general as\nthe available training data increases: as the model parameters concentrate, epistemic\nuncertainty will decrease, and the model will become more confident in its predictions,\ncausing more OoD data to become ‘far OoD.’\nAs evident from this discussion, intriguing conceptual questions remain, yet active\nlearning and OoD detection are rarely examined together. This chapter serves as an ex-\nception.\nRelevant Literature\nThere are several single forward-pass uncertainty approaches in the literature that of\nparticular relevance for this chapter. We will focus on methods that use feature-space\ndistances and density [Settles, 2010; Lee et al., 2018b; van Amersfoort et al., 2020;\nLiu et al., 2020a; Postels et al., 2020].\nMahalanobis Distance. Among these approaches, Lee et al. [2018b] uses Mahalanobis\ndistances to quantify uncertainty by fitting a class-wise Gaussian distribution (with\nshared covariance matrices) on the feature space of a pre-trained ResNet encoder. They\ndo not consider the structure of the underlying feature-space however, which might\n1It is unclear whether combining epistemic uncertainty with other signals has been explored in\ndepth.\n3. Single Forward-Pass Aleatoric and Epistemic Uncertainty\n37\nexplain why their competitive results require input perturbations, the ensembling of\nOoD metrics over multiple layers, and fine-tuning on OoD hold-out data.\nDUQ & SNGP. Two recent works in single forward-pass uncertainty, DUQ [van\nAmersfoort et al., 2020] and SNGP [Liu et al., 2020a], propose distance-aware output\nlayers, in the form of RBFs (radial basis functions) or GPs (Gaussian processes), and\nintroduce additional inductive biases in the feature extractor using a Jacobian penalty\n[Gulrajani et al., 2017] or spectral normalization [Miyato et al., 2018], respectively,\nwhich encourage smoothness and sensitivity in the latent space.\nThese methods\nperform well and are almost competitive with deep ensembles on OoD benchmarks.\nHowever, they require training to be changed substantially, and introduce additional\nhyperparameters due to the specialized output layers used at training. Furthermore,\nDUQ and SNGP cannot disentangle aleatoric and epistemic uncertainty. Particularly,\nin DUQ, the feature representation of an ambiguous data point, high on aleatoric\nuncertainty, will be in between two centroids, but due to the exponential decay of the\nRBF it will seem far from both and thus have uncertainty similar to epistemically\nuncertain data points that are far from all centroids. In SNGP, the predictive variance\nis computed using a mean-field approximation of the softmax likelihood, which cannot\nbe disentangled. The variance can also be computed using MC samples of the softmax\nlikelihood which, in theory, can allow disentangling uncertainties (see Equation 1.34),\nbut requires modelling the covariance between the classes, which is not the case in\nSNGP. We provide a more extensive review of related work in §3.6.\nOutline\nConcretely, we will focus on the following research questions:\n1. Are complex methods to estimate uncertainty, like in DUQ and SNGP, necessary\nbeyond feature-space regularization that encourages bi-Lipschitzness?\n2. What are the conceptual challenges of different uncertainty metrics, and what\ninstructive insights can be learned from these?\n3. How can we disentangle aleatoric and epistemic uncertainty with single forward-\npass neural networks, as DUQ and SNGP do not address this directly?\nWe make some simple but crucial observations that help answer our research ques-\ntions in this chapter:\n1. Entropy is arguably the wrong proxy for epistemic uncertainty, despite its frequent\nuse for active learning and OoD detection. Specifically, we find that:\n(a) The predictive entropy as a metric confounds aleatoric and epistemic\nuncertainty (Figure 3.2(b)). This can be an issue in active learning in\nparticular.\nYet, this issue is often not visible for standard benchmark\ndatasets without aleatoric noise. To examine this failure in more detail, we\nintroduce a new dataset, Dirty-MNIST, which showcases the issue more\nclearly than artificially curated datasets like MNIST or CIFAR-10. Dirty-\nMNIST is an expanded version of MNIST [LeCun et al., 1998] with additional\nambiguous digits (Ambiguous-MNIST) having multiple plausible labels and\nthus higher aleatoric uncertainty (Figure 3.2(a)).\n(b) The softmax entropy of a deterministic model trained with maximum\nlikelihood, while being high for ambiguous points (i.e., with high aleatoric un-\ncertainty), might not be consistent for points with high epistemic uncertainty,\n3. Single Forward-Pass Aleatoric and Epistemic Uncertainty\n38\ni.e., the softmax entropy for an OoD sample might be low, high, or anything\nin between for different models trained on the same data (Figure 3.2(b)).\n2. To disentangle aleatoric and epistemic uncertainty, feature-space density can be\nused to estimate epistemic uncertainty, and entropy for aleatoric uncertainty.\nHowever, feature-space regularization [Liu et al., 2020a] is crucial2. Without such\nregularization, feature-space density alone might not separate iD from OoD data,\npossibly explaining the limited empirical success of previous approaches which\nattempt to use feature-space density [Postels et al., 2020]. This can be seen in\nFigure 3.2(c) where the feature-space density of a VGG-16 or LeNet model is\nnot able to differentiate iD Dirty-MNIST from OoD FashionMNIST, while a\nResNet-18 with spectral normalization can do so better.\n3. Objectives for density estimation and classification might have different optima\n(except on unambiguous, well-separable datasets), and using a single mixture\nmodel (e.g., a GMM) leads to suboptimal performance due to this objective\nmismatch [Murphy, 2012]. Hence, one should separately estimate the feature-\nspace density for epistemic uncertainty and predictive entropy for aleatoric\nuncertainty.\nBased on these observations, we examine an approach we call ‘Deep Deterministic\nUncertainty (DDU)’, which uses Gaussian Discriminant Analysis (GDA) for feature-\nspace density on a trained model & the original softmax layer for estimating aleatoric\nuncertainty and making classification predictions. Using DDU, we empirically investi-\ngate whether complex methods to estimate uncertainty, like in DUQ and SNGP, are\nnecessary beyond feature-space regularization that encourages bi-Lipschitzness. When\nwe use spectral normalization like SNGP does, the short answer is an empirical no.\nAs we only perform GDA after training, the original softmax layer is trained\nusing cross-entropy as a proper scoring rule [Gneiting and Raftery, 2007] and can be\ntemperature-scaled to provide good in-distribution calibration and aleatoric uncertainty.\nDDU outperforms regular softmax neural networks, as illustrated in Figure 3.2.\nFurthermore, DDU is competitive with deep ensembles [Lakshminarayanan et al.,\n2017] and outperforms SNGP and DUQ [van Amersfoort et al., 2020; Liu et al., 2020a],\nwith no changes to the model architecture beyond spectral normalization, in several\nOoD benchmarks and active learning settings. Using DeepLab-v3+ [Chen et al., 2017]\non Pascal VOC 2012 [Everingham et al., 2010], we also show that DDU improves\nupon two classic uncertainty methods—MC Dropout [Gal and Ghahramani, 2016a]\nand deep ensembles—on the task of semantic segmentation, while being significantly\nfaster to compute.\n3.1\nEntropy ̸= Epistemic Uncertainty\nIn this section, we observe that:\n• using entropy for OoD detection is inherently problematic as it cannot distin-\nguish between aleatoric uncertainty of ambiguous iD samples and the epistemic\nuncertainty of near OoD samples; and\n• the softmax entropy of a single model is even more problematic as it is unreliable\nspecifically for samples with high epistemic uncertainty, i.e., near OoD samples.\n2[Pearce et al., 2021] argue for softmax confidence and entropy in their paper, yet feature-space\ndensity performs better in their experiments, too.\n3. Single Forward-Pass Aleatoric and Epistemic Uncertainty\n39\nMNIST\nAmbiguous-MNIST\nFashion-MNIST (OoD)\nDirty-MNIST (iD)\n(a) Dirty-MNIST (iD, = MNIST + Ambiguous-MNIST) and FashionMNIST (OoD)\n0.30\n0.35\n0\n1\n2\nEntropy LeNet\n0.0\n0.1\n0.2\nFraction\n0.30\n0.35\n0\n1\n2\nEntropy VGG-16\n0.0\n0.1\n0.2\nFraction\n0.30\n0.35\n0\n1\n2\nEntropy ResNet-18+SN\n0.0\n0.1\n0.2\nFraction\n(b) Softmax entropy\n200\n100\n0\n100\nLog Density LeNet\n0.0\n0.1\n0.2\nFraction\n1000\n1500\n2000\n2500\n3000\nLog Density VGG-16\n0.00\n0.05\n0.10\nFraction\n1000\n0\n1000\n2000\nLog Density ResNet-18+SN\n0.0\n0.1\n0.2\n0.3\nFraction\n(c) Feature-space density\nFigure 3.2:\nDisentangling aleatoric and epistemic uncertainty on Dirty-MNIST (iD) and\nFashionMNIST (OoD) (a) requires using softmax entropy (b) and feature-space density\n(GMM) (c) with a well-regularized feature space (ResNet-18+SN vs LeNet & VGG-16 without\nsmoothness & sensitivity). (b): Softmax entropy captures aleatoric uncertainty for iD data\n(Dirty-MNIST), thereby separating unambiguous MNIST samples and Ambiguous-MNIST\nsamples (stacked histogram). However, iD and OoD are confounded: softmax entropy has\narbitrary values for OoD, indistinguishable from iD. (c): With a well-regularized feature\nspace (DDU with ResNet-18+SN), iD and OoD densities do not overlap, capturing epistemic\nuncertainty.\nHowever, without such feature space (LeNet & VGG-16), feature density\nsuffers from feature collapse: iD and OoD densities overlap. Generally, feature-space density\nconfounds unambiguous and ambiguous iD samples as their densities overlap.\nBoth observations are tied to the very reason why a deep ensembles’ mutual information\ncaptures epistemic uncertainty well and can be used to detect adversarial examples and\nnear-OoD data, too [Smith and Gal, 2018]. To exemplify the issues, we will introduce\nDirty-MNIST as a dataset with a long tail of ambiguous samples. We will conclude\nwith an empirical analysis of the relationship between the softmax entropy and the\npredictive entropy (of a respective deep ensembles).\n3.1.0.1\n5-Ensemble Visualization\nWe start with a visualization of a 5-ensemble (with five deterministic softmax networks)\nto see how softmax entropy fails to capture epistemic uncertainty precisely because the\nmutual information (MI) of an ensemble does. This is illustrated in Figure 3.3 and\nprovides intuition for the second point that softmax entropy is unreliable for samples\n3. Single Forward-Pass Aleatoric and Epistemic Uncertainty\n40\n6\n4\n2\n0\n2\n4\n6\nx\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nLabel/Softmax output\n(a) Softmax Output\n6\n4\n2\n0\n2\n4\n6\nx\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nSoftmax entropy\n(b) Softmax Entropy\n6\n4\n2\n0\n2\n4\n6\nx\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nEnsemble PE/MI\nPE\nMI\n(c) 5-Ensemble\nFigure 3.3: Softmax outputs & entropies for 5 softmax models along with the predictive\nentropy (PE) and mutual information (MI) for the resulting 5-Ensemble. (a) and (b) show\nthat the softmax entropy is only reliably high for ambiguous iD points (±3.5–4.5), whereas\nit can be low or high for OoD points (-2–2). The different colors are the different ensemble\ncomponents. Similarly, (c) shows that the MI of the ensemble is only high for OoD, whereas\nthe PE is high for both OoD and for regions of ambiguity. See §3.1.0.1.\nwith high epistemic uncertainty. We train the networks on 1-dimensional data with\nbinary labels 0 and 1. The data is shown in Figure 3.3(a). From Figure 3.3(a) and\nFigure 3.3(b), we find that the softmax entropy is high in regions of ambiguity where\nthe label can be both 0 and 1 (i.e. x between -4.5 and -3.5, and between 3.5 and 4.5).\nThis indicates that softmax entropy can capture aleatoric uncertainty. Furthermore,\nin the x interval (−2, 2), we find that the deterministic softmax networks disagree in\ntheir predictions (see Figure 3.3(a)) and have softmax entropies which can be high,\nlow or anywhere in between (see Figure 3.3(b)) following our claim in §3.2. In fact,\nthis disagreement is the very reason why the MI of the ensemble is high in the interval\n(−2, 2), thereby reliably capturing epistemic uncertainty. Finally, the predictive entropy\n(PE) of the ensemble is high both in the OoD interval (−2, 2) as well as at points of\nambiguity (i.e. at -4 and 4). This indicates that the PE of a deep ensemble captures\nboth epistemic and aleatoric uncertainty well. From these visualizations, we draw the\nconclusion that the softmax entropy of a deterministic softmax model cannot capture\nepistemic uncertainty precisely because the MI of a deep ensemble can.\n3.1.1\nDirty-MNIST\nTo show that entropy is inappropriate for OoD detection, we train a LeNet [LeCun\net al., 1998], a VGG-16 [Simonyan and Zisserman, 2015] and a ResNet-18 with spectral\nnormalization, ResNet+SN3[He et al., 2016; Miyato et al., 2018] on Dirty-MNIST,\na modified version of MNIST [LeCun et al., 1998] with additional ambiguous digits\n(Ambiguous-MNIST), depicted in Figure 3.2(a), which we introduce below.\nDirty-MNIST poses a challenge for using entropy for OoD detection as it confounds\naleatoric and epistemic uncertainty: Figure 3.2(b) shows that the softmax entropy\nof a deterministic model is unable to distinguish between iD (Dirty-MNIST) and\nFashionMNIST samples [Xiao et al., 2017] as near OoD: the entropy for the latter\nheavily overlaps with the entropy for Ambiguous-MNIST samples. With the ambiguous\ndata having various levels of aleatoric uncertainty, Dirty-MNIST is more representative\n3Liu et al. [2020a] show that spectral normalization regularizes the latent space in a way that is\nbeneficial for OoD detection, so we also include a model trained on this recent approach.\n3. Single Forward-Pass Aleatoric and Epistemic Uncertainty\n41\nFigure 3.4: Samples from Ambiguous-MNIST.\nof real-world datasets compared to well-cleaned curated datasets, like MNIST and\nCIFAR-10, commonly used for benchmarking [Krizhevsky, 2009].\n3.1.1.1\nAmbiguous-MNIST\nEach sample in Ambiguous-MNIST is constructed by decoding a linear combination of\nlatent representations of 2 different MNIST digits from a pre-trained VAE [Kingma\nand Welling, 2014]. Every decoded image is assigned several labels sampled from\nthe softmax probabilities of an off-the-shelf MNIST neural network ensemble, with\npoints filtered based on an ensemble’s MI to remove “junk” images and then stratified\nclass-wise based on their softmax entropy. All off-the-shelf MNIST neural networks\nwere then discarded, and new models were trained to generate Figure 3.2—and as can\nbe seen, the ambiguous points we generate indeed have high entropy regardless of the\nmodel architecture used. We create 60K such training and 10K test images to construct\nAmbiguous-MNIST. Finally, the Dirty-MNIST dataset in this experiment contains\nMNIST and Ambiguous-MNIST samples in a 1:1 ratio (thus, in total 120K training and\n20K test samples). In Figure 3.4, we provide some samples from Ambiguous-MNIST.\nThis provides intuition for the first point that entropy for OoD detection is inherently\nproblematic as it cannot distinguish between aleatoric uncertainty of ambiguous iD\nsamples and the epistemic uncertainty of near OoD samples.\n3.1.2\nPotential Pitfalls of Predictive and Softmax Entropy\nNow let us discuss potential pitfalls of predictive entropy in general and softmax\nentropy of deterministic models in particular in more detail.\n3.1.2.1\nPotential Pitfalls of Predictive Entropy\nConceptually, predictive entropy confounds epistemic and aleatoric uncertainty. Since\nensembling can also be interpreted as Bayesian Model Averaging [He et al., 2020;\nWilson and Izmailov, 2020], with each ensemble member approximating a sample\nfrom a posterior, eq. (1.34) can be applied to ensembles to disentangle epistemic and\naleatoric uncertainty. Both mutual information I[Y ; ω | x, Dtrain] and predictive entropy\nH[Y | x, Dtrain] could be used to detect OoD samples. However, previous empirical\nfindings show predictive entropy outperforming mutual information [Malinin and Gales,\n3. Single Forward-Pass Aleatoric and Epistemic Uncertainty\n42\n2018]. Indeed, much of the recent literature only focuses on predictive entropy for\nOoD detection.\nTable 3.1 shows a selection of recently published papers which use\nentropy or confidence as OoD score. Only two papers examine using mutual information\nwith deep ensembles as OoD score at all. None of the papers examines the possible\nconfounding of aleatoric and epistemic uncertainty when using predictive entropy or\nconfidence, or the consistency issues of softmax entropy (and softmax confidence),\ndetailed in §3.2. This list is not exhaustive, of course. We explain these findings\nusing the following (obvious) observation:\nObservation 3.1. When we already know that either aleatoric or epistemic\nuncertainty is low for an iD sample, predictive entropy is an appropriate measure of\nthe other uncertainty type.\nThus, predictive entropy, as an upper-bound of mutual information, can separate\niD and OoD data better when datasets are curated and have low aleatoric uncertainty.\nHowever, as seen in eq. (1.34), predictive entropy can be high for both iD ambiguous\nsamples (high aleatoric) as well as near OoD samples (high epistemic) (see Figure 3.3)\nand might not be an effective measure for OoD detection when used with datasets\nthat are not curated with ambiguous samples, like Dirty-MNIST, as seen in our\nactive learning results.\n3.1.2.2\nPotential Pitfalls of Softmax Entropy\nThe softmax entropy for deterministic models trained with maximum likelihood can be\ninconsistent. As we have noted in §1.2.3, Equation 1.34 can be used with deep ensembles,\nas each ensemble member can be considered a sample from some distribution p(ω |\nDtrain) over model parameters ω ⊂Ω(e.g. a uniform distribution over K trained\nensemble members ω1, ..., ωK):\nH[Y | x, Dtrain]\n|\n{z\n}\npredictive\n= I[Y ;ΩΩΩ| x, Dtrain]\n|\n{z\n}\nepistemic\n+ H[Y | x,ΩΩΩ, Dtrain]\n|\n{z\n}\naleatoric (for iD x)\n.\n(1.34)\nNote that the mutual information I[Y ; ω | x, Dtrain] isolates epistemic from aleatoric\nuncertainty for deep ensembles as well, whereas the predictive entropy H[Y | x, Dtrain]\n(often used with deep ensembles) measures predictive uncertainty, which will be high\nwhenever either epistemic or aleatoric uncertainties are high.\nRank Inconsistency. Crucially, the mechanism underlying deep ensemble uncertainty\nthat can push epistemic uncertainty to be high on OoD data is the function disagree-\nment between different ensemble members, i.e., arbitrary and disagreeing predictive\nextrapolations of the softmax models composing the ensemble due to a lack of relevant\ntraining data4: Deep ensembles demonstrating high epistemic uncertainty (mutual\ninformation) on OoD data entails that at least two ensemble members must extrapolate\ndifferently (‘arbitrary extrapolations’) on that data, since the predictive and aleatoric\nterms in Equation 1.34 must cancel out (leading the “aleatoric” term in eq. (1.34) to\nvanish [Smith and Gal, 2018]): This is because for OoD data points Equation 1.34\nguarantees that whenever the epistemic uncertainty is high, the predictive entropy\nmust be high as well. The following simple qualitative result captures this intuition:\n4Something similar has also recently been reported in the context of ensemble calibration [Jordan,\n2023].\n3. Single Forward-Pass Aleatoric and Epistemic Uncertainty\n43\nTable 3.1:\nA sample of recently published papers and OoD metrics.\nMany recently\npublished papers only use Predictive Entropy or Predictive Confidence (for deep ensembles)\nor Softmax Confidence (for deterministic models) as OoD scores without addressing the\npossible confounding of aleatoric and epistemic uncertainty, that is ambiguous iD samples\nwith OoD samples. Only two papers examine using mutual information with deep ensembles\nas OoD score at all.\nTitle [Citation]\nSoftmax\nConfidence\nPredictive\nConfidence\nPredictive\nEntropy\nMutual\nInformation\nA Baseline for Detecting Misclassified and Out-of-Distribution Exam-\nples in Neural Networks\n[Hendrycks and Gimpel, 2017]\n✓\n✗\n✗\n✗\nDeep Anomaly Detection with Outlier Exposure\n[Hendrycks et al., 2019]\n✓\n✗\n✗\n✗\nEnhancing The Reliability of Out-of-distribution Image Detection in\nNeural Networks\n[Liang et al., 2018]\n✓\n✗\n✗\n✗\nTraining Confidence-calibrated Classifiers for Detecting Out-of-\nDistribution Samples\n[Lee et al., 2018a]\n✓\n✗\n✗\n✗\nLearning Confidence for Out-of-Distribution Detection in Neural\nNetworks\n[DeVries and Taylor, 2018]\n✓\n✗\n✗\n✗\nSimple and Scalable Predictive Uncertainty Estimation using Deep\nEnsembles\n[Lakshminarayanan et al., 2017]\n✗\n✓\n✓\n✗\nPredictive Uncertainty Estimation via Prior Networks\n[Malinin and Gales, 2018]\n✗\n✓\n✓\n✓\nEnsemble Distribution Distillation\n[Malinin et al., 2019]\n✗\n✗\n✓\n✓\nGeneralized ODIN: Detecting Out-of-Distribution Image Without\nLearning From Out-of-Distribution Data\n[Hsu et al., 2020]\n✓\n✗\n✗\n✗\nBeing Bayesian, Even Just a Bit, Fixes Overconfidence in ReLU\nNetworks\n[Kristiadi et al., 2020]\n✗\n✓\n✗\n✗\nProposition 3.1. Let x1 and x2 be points such that x1 has higher epistemic uncer-\ntainty than x2 under the ensemble:\nI[Y1; ω | x1, Dtrain] > I[Y2; ω | x2, Dtrain] + δ,\n(3.1)\nδ ≥0. Further, assume both have similar predictive entropy\n| H[Y1 | x1, Dtrain] −H[Y2 | x2, Dtrain]| ≤ϵ,\n(3.2)\nϵ ≥0. Then, there exist sets of ensemble members Ω′ (assuming a countable ensemble),\nwith p(Ω′ | Dtrain) > 0, such that for all softmax models ω′ ∈Ω′ the softmax entropy of\nx1 is lower than the softmax entropy of x2:\nH[Y1 | x1, ω′] < H[Y2 | x2, ω′] −(δ −ϵ).\n(3.3)\nProof. From Equation 1.34, we obtain:\n| H[Y1 | x1, Dtrain] −H[Y2 | x2, Dtrain]| ≤ϵ\n(3.4)\n⇔| I[Y1; ω | x1, Dtrain] + Ep(ω|Dtrain)[H[Y1 | x1, ω]]\n3. Single Forward-Pass Aleatoric and Epistemic Uncertainty\n44\n−I[Y2; ω | x2, Dtrain] −Ep(ω|Dtrain)[H[Y2 | x2, ω]]| ≤ϵ,\n(3.5)\nand hence we have:\nEp(ω|Dtrain)[H[Y1 | x1, ω]] −Ep(ω|Dtrain)[H[Y2 | x2, ω]]\n+ (I[Y1; ω | x1, Dtrain] −I[Y2; ω | x2, Dtrain])\n|\n{z\n}\n>δ\n≤ϵ.\n(3.6)\nWe can rearrange the terms:\nEp(ω|Dtrain)[H[Y1 | x1, ω]] < Ep(ω|Dtrain)[H[Y2 | x2, ω]] −(δ −ϵ).\n(3.7)\nNow, the statement follows by contraposition: if H[Y1 | x1, ω] ≥H[Y2 | x2, ω] −(δ −ϵ)\nfor all ω, the monotonicity of the expectation would yield Ep(ω|Dtrain)[H[Y1 | x1, ω]] ≥\nEp(ω|Dtrain)[H[Y2 | x2, ω]] −(δ −ϵ). Thus, there is a set Ω′ with p(Ω′ | Dtrain) > 0, such\nthat\nH[Y1 | x1, ω] < H[Y2 | x2, ω] −(δ −ϵ),\n(3.8)\nfor all ω ∈Ω′.\nIf a sample is assigned higher epistemic uncertainty (in the form of mutual\ninformation) by a deep ensemble than another sample, it will necessarily be assigned\nlower softmax entropy by at least one of the ensemble’s members. As a result, a\npriori, we cannot know whether a softmax model preserves the order or not, and\nthe empirical observation that the mutual information of an ensemble can quantify\nepistemic uncertainty well implies that the softmax entropy of a deterministic model\nmight not. We see this in Figure 3.2(b), 3.3 and in §3.1.3 where softmax entropy\nfor OoD samples can be high, low or anywhere in between. While not all model\narchitectures might behave like this, when the mutual information of a deep ensemble\nworks well empirically, Proposition 3.1 holds.\nThis directly impacts the quality of the OoD detection, as we will verify in §3.1.3\nand §3.5. For OoD detection, the changes in the score ranks can induce additional false\npositive or false negative—the AUROC, for example, directly measures the probability\nthat an iD point has higher score than an OoD point. For active learning, the changes\nin the order can lead to less uninformative samples being selected—on the other hand,\nthe additional noise this introduces can also be beneficial as we investigate in §5.\nBias-Variance Trade-Off. We can take a different perspective and view the softmax\nentropy of a single model as a (biased) estimator of the predictive entropy of the\nensemble. What is the root mean squared error of this estimator?\nProposition 3.2. The root mean squared error of the softmax entropy of a single\nmodel as an estimator of the predictive entropy of the ensemble:\nRMSEωωω(H[Y | x,ΩΩΩ, Dtrain], H[Y | x, Dtrain]) = Ep(ωωω|Dtrain)[\n\u0000H[Y | x,ωωω] −H[Y | x, Dtrain]\n\u00012]1/2\n(3.9)\ndecomposes into a bias-variance trade-off with bias I[Y ;ωωω | x, Dtrain] and variance\nVarp(ωωω|Dtrain)[H[Y | x,ωωω]].\nProof. We will drop conditioning on Dtrain for this proof.\n1. We use that I[Y ;ωωω | x, Dtrain] ≜H[Y | x] −H[Y | x,ωωω, Dtrain] (from §2),\n3. Single Forward-Pass Aleatoric and Epistemic Uncertainty\n45\n2. As H[Y | x] is independent of ΩΩΩ, we have:\nVarωωω[H[Y | x,ωωω]] = Varωωω[I[Y ;ωωω | x]].\n(3.10)\n3. Expanding the variance, we obtain:\nVarωωω[I[Y ;ωωω | x]] = Eωωω[I[Y ;ωωω | x]2] −Eωωω[I[Y ;ωωω | x]]2\n(3.11)\n= Eωωω[I[Y ;ωωω | x]2] −I[Y ;ΩΩΩ| x]2.\n(3.12)\n4. We substitute the definition of I[Y ;ωωω | x] in the first term and use the equality of\nthe variances above. Rearranging:\nEωωω[(H[Y | x] −H[Y | x,ωωω, Dtrain])2] = Varωωω[H[Y | x,ωωω]] + I[Y ;ΩΩΩ| x]2.\n(3.13)\n5. Taking the square root yields the result:\nRMSEωωω(H[Y | x,ΩΩΩ], H[Y | x])\n(3.14)\n= Eωωω[(H[Y | x] −H[Y | x,ωωω, Dtrain])2]1/2\n(3.15)\n=\nq\nVarωωω[H[Y | x,ωωω]]\n|\n{z\n}\nVariance\n+I[Y ;ΩΩΩ| x]\n|\n{z\n}\nBias\n2.\n(3.16)\nHence, the expected deviation from the predictive entropy becomes the largest\nfor high mutual information/epistemic uncertainty. This is in line with the empirical\nobservations that the softmax entropy of a deterministic model might not be a good\nestimator of the predictive entropy of the ensemble from §3.1.3 below.\nGiven that we can view an ensemble member as a single deterministic model and\nvice versa, these two propositions provide an intuitive explanation for why single\ndeterministic models can report inconsistent and widely varying predictive entropies\nand confidence scores for OoD samples for which a deep ensemble would report high\nepistemic uncertainty (expected information gain) and high predictive entropy.\n3.1.3\nQualitative Empirical Validation\nWhile the 5-ensemble visualization and qualitative and quantitative statements provide\nus with an intuition for why ensemble members and thus deterministic models cannot\nprovide epistemic uncertainty reliably through their softmax entropies, to gain further\ninsights, we empirically analyze the relationship between softmax entropies and\npredictive entropies more precisely next.\nTo do so we train deep ensembles of 25 members on CIFAR-10 using different\nmodel architectures (VGG-16, Wide-ResNet-28-10/+SN) and visualize the relationship\nbetween the softmax entropies of the ensemble members, and the mutual information\n(epistemic uncertainty/EIG/BALD) and the predictive entropies of the overall ensemble\non two OoD datasets (CIFAR-100 and SVHN). Details for the experiment setup\ncan be found in §3.5.\n3. Single Forward-Pass Aleatoric and Epistemic Uncertainty\n46\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nNats\n0\n5\n10\n15\n20\n25\n30\n35\n40\nPercentage\nSoftmax Entropy\nPredictive Entropy\n(a) VGG16: CIFAR-100 (OoD)\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nNats\n0\n5\n10\n15\n20\n25\n30\n35\n40\nPercentage\nSoftmax Entropy\nPredictive Entropy\nSVHN (OoD)\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nNats\n0\n5\n10\n15\n20\n25\n30\n35\n40\nPercentage\nSoftmax Entropy\nPredictive Entropy\n(b) WideResNet-28-10: CIFAR-100 (OoD)\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nNats\n0\n5\n10\n15\n20\n25\n30\n35\n40\nPercentage\nSoftmax Entropy\nPredictive Entropy\nSVHN (OoD)\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nNats\n0\n5\n10\n15\n20\n25\n30\n35\n40\nPercentage\nSoftmax Entropy\nPredictive Entropy\n(c) WideResNet-28-10+SN: CIFAR-100 (OoD)\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nNats\n0\n5\n10\n15\n20\n25\n30\n35\n40\nPercentage\nSoftmax Entropy\nPredictive Entropy\nSVHN (OoD)\nFigure 3.5: Softmax entropy vs. predictive entropy trained on CIFAR-10 (iD) using different\nmodel architectures (25 models each).\nWe see that predictive and softmax entropy are\ndistributed very differently. (+SN refers to models trained with spectral norm and small\nmodifications to the architecture described in §3.4.1.)\nSoftmax Entropy vs Predictive Entropy. Figure 3.5 shows that the softmax\nentropy of the ensemble members is not a good estimator of the predictive entropy of\nthe ensemble. Not even the histograms look similar. Figures 3.6 and 3.7 show histograms\nof the different ensemble members. We see that on SVHN there is a lot more variation\nin the softmax entropy distribution of the ensemble members than on CIFAR-100. This\nis also reflected in the predictive entropy of the ensemble, which is often higher on\nSVHN than on CIFAR-100. Figure 3.8, which also includes the mutual information\n3. Single Forward-Pass Aleatoric and Epistemic Uncertainty\n47\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\nEntropy\n0\n10\n20\n30\n40\nPrecentage\n(a) SVHN: Overlaid Histograms\n0\n10\n20\n30\n40\n50\n0\n10\n20\n30\n40\n50\n0\n10\n20\n30\n40\n50\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEntropy\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFraction\nIndividual Histograms\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\nEntropy\n0\n10\n20\n30\n40\nPrecentage\n(b) CIFAR-100: Overlaid Histograms\n0\n10\n20\n30\n40\n50\n0\n10\n20\n30\n40\n50\n0\n10\n20\n30\n40\n50\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEntropy\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFraction\nIndividual Histograms\nFigure 3.6: VGG-16: Empirical distribution of softmax entropies of the ensemble members (25 models, 24 are shown). We see that the empirical\ndistribution of softmax entropies can vary a lot between different models of the same architecture. (a) the empirical distribution of softmax\nentropies for SVHN as OoD displays a lot of variance; while (b) the empirical distribution of softmax entropies for CIFAR-100 as OoD display\nvery little variation. +SN refers to models trained with spectral norm and small modifications to the architecture described in §3.4.1.\n3. Single Forward-Pass Aleatoric and Epistemic Uncertainty\n48\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\nEntropy\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\n20.0\nPrecentage\n(a) SVHN: Overlaid Histograms\n0\n5\n10\n15\n20\n25\n0\n5\n10\n15\n20\n25\n0\n5\n10\n15\n20\n25\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEntropy\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFraction\nIndividual Histograms\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\nEntropy\n0\n5\n10\n15\n20\n25\n30\nPrecentage\n(b) CIFAR-100: Overlaid Histograms\n0\n5\n10\n15\n20\n25\n30\n0\n5\n10\n15\n20\n25\n30\n0\n5\n10\n15\n20\n25\n30\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEntropy\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFraction\nIndividual Histograms\nFigure 3.7: Wide-ResNet-28-10+SN: Empirical distribution of softmax entropies of the ensemble members (25 models, 24 are shown). We\nsee that the empirical distribution of softmax entropies can vary a lot between different models of the same architecture. (a) the empirical\ndistribution of softmax entropies for SVHN as OoD displays a lot of variation between models; while (b) the empirical distribution of softmax\nentropies for CIFAR-100 as OoD display very little variation. (+SN refers to models trained with spectral norm and small modifications to the\narchitecture described in §3.4.1.)\n3. Single Forward-Pass Aleatoric and Epistemic Uncertainty\n49\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nNats\n0\n5\n10\n15\n20\n25\n30\n35\n40\nPercentage\nSoftmax Entropy\nPredictive Entropy\nMutual Information\n(a) VGG16: CIFAR-100 (OoD)\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nNats\n0\n5\n10\n15\n20\n25\n30\n35\n40\nPercentage\nSoftmax Entropy\nPredictive Entropy\nMutual Information\nSVHN (OoD)\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nNats\n0\n5\n10\n15\n20\n25\n30\n35\n40\nPercentage\nSoftmax Entropy\nPredictive Entropy\nMutual Information\n(b) WideResNet-28-10+SN: CIFAR-100 (OoD)\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nNats\n0\n5\n10\n15\n20\n25\n30\n35\n40\nPercentage\nSoftmax Entropy\nPredictive Entropy\nMutual Information\nSVHN (OoD)\nFigure 3.8: Mutual Information (Epistemic Uncertainty). Trained on CIFAR-10 (iD) using\ndifferent model architectures (25 models each). The mutual information is better behaved\nthan the softmax entropies, but less broad than the predictive entropy. (+SN refers to models\ntrained with spectral norm and small modifications to the architecture described in §3.4.1.)\n(EIG) of the ensemble, validates this: the mutual information near 0 is much lower on\nCIFAR-100 than on SVHN. The model expresses less disagreement on CIFAR-100.\nFigure 3.9 provides a more complete picture of all three information quantities we\ncare about (mutual information, predictive entropy, and expected softmax entropy).\nThe model architectures perform very differently on the two datasets: the VGG-16\nmodel has smaller predictive entropies overall which pushes the expected softmax\nentropies below the mutual information for many OoD samples.\nFigure 3.10 shows density plots for predictive entropy versus softmax entropy (and\nmutual information versus softmax entropy) across all ensemble members. We see\nthat most of the softmax entropies are very low even when the same sample has very\nhigh predictive entropy or mutual information. This implies that even small aleatoric\nuncertainty in iD samples will lead the model to confound them with OoD samples.\nSoftmax Entropy Variance. Figure 3.11 show density plots for the mutual informa-\ntion vs the variance of the softmax entropy of the ensemble. As the mutual information\nincreases, the variance of the softmax entropy first increases and then decreases slightly.\nOverall, it is mostly quite low compared to the mutual information. Figure 3.12\nvalidates this. It shows a histogram of the RMSE following Proposition 3.2 vs the\nmutual information. Indeed, the error is dominated by its bias, the mutual information.\n3. Single Forward-Pass Aleatoric and Epistemic Uncertainty\n50\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\nMutual Information\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\nExpected Softmax Entropy\n0.6\n1.2\n1.7\n2.3\n(a) VGG16: CIFAR-100 (OoD)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\nMutual Information\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\nExpected Softmax Entropy\n0.6\n1.2\n1.7\n2.3\nSVHN (OoD)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\nMutual Information\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\nExpected Softmax Entropy\n0.6\n1.2\n1.7\n2.3\n(b) WideResNet-28-10+SN: CIFAR-100 (OoD)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\nMutual Information\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\nExpected Softmax Entropy\n0.6\n1.2\n1.7\n2.3\nSVHN (OoD)\nFigure 3.9: Mutual Information (Epistemic Uncertainty) vs Expected Softmax Entropy\n(Aleatoric Uncertainty) vs Predictive Entropy (Total Uncertainty). Trained on CIFAR-10 (iD)\nusing different model architectures (25 models each). The predictive entropy is shown via its\niso-lines (anti-diagonals). Darker is denser.(+SN refers to models trained with spectral norm\nand small modifications to the architecture described in §3.4.1.)\n3.1.4\nDiscussion\nBoth through quantitative and qualitative statements as well as through empirical\nvalidation, we see that neither the predictive entropy of deep ensembles nor the softmax\nentropy of deterministic models is appropriate for measuring epistemic uncertainty and\nOoD detection tasks. This holds in particularly for real world datasets that contain\nmore ambiguous data than the curated datasets that are employed for benchmarking.\nProposition 3.1 shows that the softmax entropy does not provide a stable ranking of\npoints with high epistemic uncertainty, and that indeed, the predictive entropy of deep\nensembles captures epistemic uncertainty through the variance of the softmax entropies\nof deterministic models. Proposition 3.2 shows that when we view softmax entropies\nas estimators of the respective predictive entropies of an ensemble and decompose the\nerror using a bias-variance trade-off, the mutual information (epistemic uncertainty) is\nthe bias of this estimator, and the variance of the softmax entropy is the variance of\nthe estimator. Additionally, we have empirically analyzed several model architectures\nand found that the softmax entropy varies considerably across the ensemble members\nand is neither a good measure of epistemic uncertainty (with the mutual information\nas proxy) nor of predictive entropy, which could be surprising given that we usually\ndo not differentiate between softmax and predictive entropy.\n3. Single Forward-Pass Aleatoric and Epistemic Uncertainty\n51\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nMutual Information\n0.0\n0.5\n1.0\n1.5\n2.0\nSoftmax Entropy\nVGG16: CIFAR-100\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nMutual Information\n0.0\n0.5\n1.0\n1.5\n2.0\nSoftmax Entropy\nSVHN\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMutual Information\n0.0\n0.5\n1.0\n1.5\n2.0\nSoftmax Entropy\nWRN+SN: CIFAR-100\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMutual Information\n0.0\n0.5\n1.0\n1.5\n2.0\nSoftmax Entropy\nSVHN\n(a) Mutual Information vs Softmax Entropy\n0.0\n0.5\n1.0\n1.5\n2.0\nPredictive Entropy\n0.0\n0.5\n1.0\n1.5\n2.0\nSoftmax Entropy\nVGG16: CIFAR-100 (OoD)\n0.0\n0.5\n1.0\n1.5\n2.0\nPredictive Entropy\n0.0\n0.5\n1.0\n1.5\n2.0\nSoftmax Entropy\nSVHN (OoD)\n0.0\n0.5\n1.0\n1.5\n2.0\nPredictive Entropy\n0.0\n0.5\n1.0\n1.5\n2.0\nSoftmax Entropy\nWideResNet-28-10+SN: CIFAR-100 (OoD)\n0.0\n0.5\n1.0\n1.5\n2.0\nPredictive Entropy\n0.0\n0.5\n1.0\n1.5\n2.0\nSoftmax Entropy\nSVHN (OoD)\n(b) Predictive Entropy vs Softmax Entropy\nFigure 3.10: Predictive Entropy (Total Uncertainty) & Mutual Information (Epistemic\nUncertainty) vs Expected Softmax Entropy (Aleatoric Uncertainty) Trained on CIFAR-10 (iD)\nusing different model architectures (25 models each). Darker is denser.(+SN refers to models\ntrained with spectral norm and small modifications to the architecture described in §3.4.1.)\nWhile our criticism of softmax entropy seems generally valid, mutual information\n(expected information gain/epistemic uncertainty) is not necessarily a good measure\nfor far OoD detection as we have argued in §3. Predictive entropy can be high for\nboth far OoD points and near OoD points, which it can confound with ambiguous\niD points, however.\nIs there a way to ensure that the model will have high uncertainty for OoD points\nin general (and thus minimize the amount of possible far OoD points)? Yes, feature-\nspace regularization, whether implicit and explicit, can do just that: in §1.2.3, we\nintroduced bi-Lipschitzness as a concept that can encourage OoD points to be separated\nfrom iD points, thus allowing different ensemble members to potentially (hopefully)\nexpress higher disagreement on these points.\n3. Single Forward-Pass Aleatoric and Epistemic Uncertainty\n52\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMutual Information\n0.0\n0.1\n0.2\n0.3\n0.4\nSoftmax Entropy Variance\nEmpirical\n(a) SVHN (OoD): VGG-16\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMutual Information\n0.0\n0.1\n0.2\n0.3\n0.4\nSoftmax Entropy Variance\nEmpirical\nWide-ResNet-28-10+SN\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMutual Information\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nSoftmax Entropy Variance\nEmpirical\n(b) CIFAR-100 (OoD): VGG-16\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMutual Information\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nSoftmax Entropy Variance\nEmpirical\nWide-ResNet-28-10+SN\nFigure 3.11: Mutual Information (Epistemic Uncertainty) vs Softmax Entropy Variance\n(ESV). Trained on CIFAR-10 (iD) using different model architectures (25 models each). For\nboth SVHN and CIFAR-100 as OoD dataset, we see that the softmax variance first increases\nas the mutual information increases and then decreases. Overall, compared to the mutual\ninformation, it is quite small. (+SN refers to models trained with spectral norm and small\nmodifications to the architecture described in §3.4.1).\nImportantly, note that outlier exposure, a popular method to improve OoD detection\nperformance, trains the model on held-out “OoD” data. This breaks the equivalence\n“OoD data ⇐⇒high epistemic uncertainty” that underlies using epistemic uncertainty\nfor OoD detection: training using outlier exposure transforms epistemic uncertainty into\naleatoric uncertainty, which can be captured by the softmax entropy of deterministic\nmodels as well as the predictive entropy of a deep ensemble—even though it confounds\nambiguous iD samples with OoD samples. However, is such data still truly OoD when\nwe start training or fine-tuning on it, or are we simply moving the goal posts?\n3.2\nAleatoric & Epistemic Uncertainty\nThe failure of softmax entropy to capture epistemic uncertainty motivates us to\nstudy feature-space density as an alternative for single-forward pass approaches.\nIndeed, feature-space density is a well-known acquisition function in active learn-\ning [Settles, 2010].\nEpistemic Uncertainty via Feature-Space Density. In §1.2.3, we noted that\nfeature-space density conceptually fulfills the requirement of epistemic uncertainty as\nbeing a reducible uncertainty. Here, we will investigate this further—or rather, to\n3. Single Forward-Pass Aleatoric and Epistemic Uncertainty\n53\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\nMutual Information\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\nRMSE(SE, PE)\n(a) SVHN (OoD): VGG-16\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMutual Information\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRMSE(SE, PE)\nWide-ResNet-28-10+SN\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\nMutual Information\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\nRMSE(SE, PE)\n(b) CIFAR-100 (OoD): VGG-16\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMutual Information\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nRMSE(SE, PE)\nWide-ResNet-28-10+SN\nFigure 3.12: Mutual Information (Epistemic Uncertainty) vs (Entropy) Root Mean Squared\nError (RMSE, Proposition 3.2). Trained on CIFAR-10 (iD) using different model architectures\n(25 models each). For both SVHN and CIFAR-100 as OoD dataset, we see that the RMSE\nincreases as the mutual information (the bias) increases. The bias seems to be dominating\nthe error compared to the softmax variance. (+SN refers to models trained with spectral norm\nand small modifications to the architecture described in §3.4.1).\nbe more precise—we will investigate the negative feature-space density as a proxy\nfor epistemic uncertainty.\nWith a well-regularized feature space using spectral normalization, we find that\nsimply performing GDA (Gaussian Discriminant Analysis) after training as feature-\nspace density estimator can reliably capture epistemic uncertainty. However, unlike\nLee et al. [2018b], which does not place any constraints on the feature space, training\non “OoD” hold-out data, feature ensembling, and input pre-processing are not needed\nto obtain good performance (see Table 3.5). This results in a conceptually simpler\nmethod. Moreover, we find that using a separate covariance matrix for each class\nimproves OoD detection performance as compared to a shared covariance matrix.\nCrucially, feature-space density cannot express aleatoric uncertainty: an iD sample\nought to have a high density regardless of whether it is ambiguous (with high aleatoric\nuncertainty) or not, as is the case with Dirty-MNIST in Figure 3.2(c). However,\nsoftmax entropy is suitable for that.\nHence, we can use the softmax entropy to\npredict the aleatoric uncertainty on iD samples with low epistemic uncertainty, for\nwhich is it meaningfully defined:\n3. Single Forward-Pass Aleatoric and Epistemic Uncertainty\n54\nObservation 3.2. The softmax entropy of a deterministic model together with its\nfeature-space density can disentangle epistemic and aleatoric uncertainty while either\nalone cannot.\nSensitivity & Smoothness. As discussed in §1.2.3, feature extractors that do not\nfulfill a sensitivity constraint can suffer from feature collapse: they might map OoD\nsamples to iD regions of the feature space. Thus, we encourage these properties using\nresidual connections and spectral normalization. The effects of these properties on\nfeature collapse are visible in Figure 3.2. In the case of feature collapse, we must\nhave some OoD inputs for which the features are mapped on top of the features of iD\ninputs. The distances of these OoD features to each class centroid must be equal to\nthe distances of the corresponding iD inputs to class centroids, and hence the density\nfor these OoD inputs must be equal to the density of the iD inputs. If the density\nhistograms for given iD and OoD samples do not overlap, no feature collapse can be\npresent for those samples. We see no overlapping densities in Figure 3.2(c)(right most),\ntherefore we indeed have no feature collapse between Dirty-MNIST and FashionMNIST.\nSimilarly, smoothness constraints are necessary to encourage generalization when using\nfeature-space density as a proxy for epistemic uncertainty [van Amersfoort et al., 2020].\nGaussian & Linear Discriminant Analysis. Given feature vector z and class label\ny, we model the class-conditional probabilities q(z | y) of feature vectors given class\nlabels using Gaussian Discriminant Analysis (GDA), which is a generative classifier\nq(z, y) based on a Gaussian mixture model (GMM) q(z | y) q(y), where q(y) is the\nclass prior and q(z | y) is a multivariate Gaussian density. GDA is closely related\nto Linear Discriminant Analysis (LDA) [Murphy, 2012]. To predict the class label\nq(y | z) of a new sample z, we use Bayes’ theorem:\nq(y | z) ∝q(z | y) q(y)\n(3.17)\nIn GDA, each class is modeled by a Gaussian mixture component with its own covari-\nance matrix. The class-conditional probabilities are given by q(z|y = c) = N(z; µc, Σc),\nwhere µc and Σc are the mean and covariance matrix for class c, respectively.\nOn the other hand, LDA models the class-conditional probabilities using a single\nmultivariate Gaussian distribution per class, with a shared covariance matrix Σ\namong all classes.\nThe class-conditional probabilities in LDA are given by q(z |\ny = c) ∼N(z; µc, Σ).\nThus, the difference between GDA and LDA lies in their modeling of the covariance\nmatrix. While GDA allows for a separate covariance matrix for each class, LDA assumes\na shared covariance matrix among all classes. This distinction influences the flexibility\nand complexity of the models: GDA is more flexible but potentially more prone to\noverfitting, while LDA is more constrained but may be more robust in some scenarios.\nScope. We only use GDA for estimating the feature-space density as it is straight-\nforward to implement and does not require performing expectation maximization or\nvariational inference like other density estimators. Normalizing flows [Dinh et al., 2015]\nor other more complex density estimators might provide even better density estimates,\nof course. Yet despite its simplicity, GDA is already sufficient to outperform other\nmore complex approaches and obtain good results as we report in §3.5.\nOur focus is on obtaining a well-regularized feature space using spectral normaliza-\ntion in model architectures with residual connections, following Liu et al. [2020a]. Note\n3. Single Forward-Pass Aleatoric and Epistemic Uncertainty\n55\n0.0\n0.5\n1.0\n1.5\n2.0\nSoftmax Entropy\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\nFraction\n1% of D-MNIST\n2% of D-MNIST\n10% of D-MNIST\n8\n6\n4\n2\n0\nGMM Log Density\n1e14\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nFraction\n1% of D-MNIST\n2% of D-MNIST\n10% of D-MNIST\n(a) Dirty-MNIST\n0.0\n0.1\n0.2\n0.3\n0.4\nMI\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nFraction\n10% of D-MNIST\n2% of D-MNIST\n1% of D-MNIST\n0.0\n0.5\n1.0\n1.5\n2.0\nSoftmax Entropy\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nFraction\n10% of CIFAR-10\n20% of CIFAR-10\n100% of CIFAR-10\n6000\n5000\n4000\n3000\n2000\n1000\n0\nGMM Log Density\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nFraction\n10% of CIFAR-10\n20% of CIFAR-10\n100% of CIFAR-10\n(b) CIFAR-10\n0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4\nMI\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nFraction\n10% of CIFAR-10\n20% of CIFAR-10\n100% of CIFAR-10\nFigure 3.13: Comparison of epistemic and aleatoric uncertainty captured by ResNet-18+SN\non increasingly large subsets of Dirty-MNIST and CIFAR-10.\nFeature density captures\nepistemic uncertainty which reduces when the model is trained on increasingly large subsets\nof training data, whereas softmax entropy (SE) does not. For comparison, we also plot a\ndeep-ensemble’s epistemic uncertainty, through mutual information (MI) for the same settings.\nFor more details, see Table 3.2.\nthat unsupervised methods using contrastive learning [Winkens et al., 2020] might also\nobtain such a feature space by training on very large datasets, but training on them can\nbe very expensive [Sun et al., 2017]. Generally, as the amount of training data available\ngrows and feature extractors improve, the quality of feature representations might\nimprove as well. The underlying motivation of this chapter is that simple approaches\nwill remain more applicable than more complex ones as our empirical results suggest.\n3.2.1\nReducible Feature-Space Density & Irreducible Entropy\nTo empirically verify the connection between feature-space density and epistemic\nuncertainty on the one hand and the connection between softmax entropy and aleatoric\nuncertainty on the other hand, we train ResNet-18+SN models on increasingly large sub-\nsets of Dirty-MNIST and CIFAR-10 and evaluate the epistemic and aleatoric uncertainty\non the corresponding test sets using the feature-space density and softmax entropy,\nrespectively. Moreover, we also train a 5-ensemble on the same subsets of data and use\nthe ensemble’s mutual information as a baseline measure of epistemic uncertainty.\nIn Figure 3.13 and Table 3.2, we see that with larger training sets, the average\nfeature-space density increases which is consistent with the epistemic uncertainty\ndecreasing as more data is available as reducible uncertainty. This is also evident from\nthe consistent strong positive correlation between the negative log density and mutual\ninformation of the ensemble. On the other hand, the softmax entropy stays roughly the\nsame which is consistent with aleatoric uncertainty as irreducible uncertainty, which\n3. Single Forward-Pass Aleatoric and Epistemic Uncertainty\n56\nTable 3.2:\nAverage softmax entropy (SE) and feature-space density of the test set for\nmodels trained on different amounts of the training set (Dirty-MNIST and CIFAR-10) behave\nconsistently with aleatoric and epistemic uncertainty.\nAleatoric uncertainty for individual\nsamples does not change much as more data is added to the training set while epistemic\nuncertainty decreases as more data is added. This is also consistent with Table 3 in Kendall\nand Gal [2017]. Finally, we observe a consistent strong positive correlation between the\nnegative log feature space density and the mutual information (MI) of a deep ensemble trained\non the same subsets of data for both Dirty-MNIST and CIFAR-10. However, the correlation\nbetween softmax entropy and MI is not consistent.\nTraining Set\nAvg Test SE (≈)\nAvg Test Log GMM Density (↑)\nAvg Test MI\nCorrelation(SE || MI)\nCorrelation(-Log GMM Density || MI)\n1% of D-MNIST\n0.7407\n−2.7268e + 14\n0.0476\n−0.79897\n0.8132\n2% of D-MNIST\n0.6580\n−7.8633e + 13\n0.0447\n10% of D-MNIST\n0.8295\n−1279.1753\n0.0286\n10% of CIFAR-10\n0.3189\n−1715.3516\n0.4573\n0.5663\n0.9556\n20% of CIFAR-10\n0.2305\n−1290.1726\n0.2247\n100% of CIFAR-10\n0.2747\n−324.8040\n0.0479\nis independent of the training data. Importantly, all of this is also consistent with\nthe experiments comparing epistemic and aleatoric uncertainty on increasing training\nset sizes in Table 3 of [Kendall and Gal, 2017].\n3.3\nObjective Mismatch\nSo far, we have seen that the feature-space density of a model can be used as a proxy\nto quantify the epistemic uncertainty of the model, and that the softmax entropy of\nthe model can be used as a proxy to quantify the aleatoric uncertainty of the model.\nWhy did we use softmax entropy to estimate aleatoric uncertainty? Why not use\nthe predictive probability q(y | z) of the GDA model that we use to estimate the\nfeature-space density p(z)? It is not a matter of convenience but rather a matter\nof potentially conflicting objectives:\nIn this section, we show that the feature-space density and softmax entropy are\noptimal for the respective uncertainty quantification tasks, and that this is because of\nan objective mismatch between the two tasks. The predictive probability induced by a\nfeature-density estimator will generally not be well-calibrated as there is an objective\nmismatch. This was overlooked in previous research on uncertainty quantification\nfor deterministic models: Lee et al. [2018b]; Liu et al. [2020a]; van Amersfoort et al.\n[2020]; He et al. [2016]; Postels et al. [2020]. Specifically, a mixture model q(y, z) =\nP\ny q(z | y) q(y), using one component per class, cannot be optimal for both feature-\nspace density and predictive distribution estimation as there is an objective mismatch\n[Murphy, 2012, Ex.\n4.20, p.\n145]:\nProposition 3.3. For an input x, let z = fθ(x) denote its feature representation in a\nfeature extractor fθ with parameters θ. Then the following hold:\n1. a discriminative classifier qθ(y | z), e.g. a softmax layer, is well-calibrated in its\npredictions when it maximizes the conditional log-likelihood qθ(y | z);\n2. a feature-space density estimator qθ(z) is optimal when it maximizes the marginalized\nlog-likelihood log q(z);\n3. a mixture model qθ(y, z) =\nP\ny qθ(z | y) q(y) might not be able to maximize both\nobjectives, conditional log-likelihood and marginalized log-likelihood, at the same time.\n3. Single Forward-Pass Aleatoric and Epistemic Uncertainty\n57\nIn the specific instance that a GMM with one component per class does maximize\nboth, the resulting model must be a GDA (but the opposite does not hold).\nFollowing the notation form §1.2.1, qθ(·) denotes a probability distribution pa-\nrameterized by θ. As cross-entropies upper-bound the respective entropies, we have:\nHθ[Y, Z] ≥H[Y, Z], Hθ[Z] ≥H[Z], and Hθ[Y | Z] ≥H[Y | Z].\nProof. We prove the statements in order. The first two are trivial.\n1. The conditional log-likelihood is a strictly proper scoring rule [Gneiting and Raftery,\n2007]. The optimization objective can be rewritten as\nmax\nθ\nE[log pθ(y | z)] = −min\nθ\nHθ[Y | Z] ≤−H[Y | Z].\n(3.18)\nAn optimal discriminative classifier qθ(y | z) with equality above would thus capture\nthe true (empirical) distribution everywhere: qθ(y | z) = ˆptrue(y | z).\n2. For density estimation qθ(z), we maximize the log-likelihood E[log qθ(z)] using the\nempirical data distribution. We can rewrite this as\nmax\nθ\nEˆptrue(y,z) log pθ(z) = −min\nθ\nHθ[Z] ≤−H[Z].\n(3.19)\nWhen we have equality, we have pθ(z) = ˆptrue(z).\n3. Using Hθ[Y, Z] = Hθ[Y | Z] + Hθ[Z], we can relate the objectives from Equation 3.18\nand (3.19) to each other. First, we characterize a shared optimum, and then we show\nthat both objectives are generally not minimized at the same time. For both objectives\nto be minimized, we have ∇Hθ[Y | Z] = 0 and ∇Hθ[Z] = 0, and we obtain\n∇Hθ[Y, Z] = ∇Hθ[Y | Z] + ∇Hθ[Z] = 0.\n(3.20)\nFrom this we conclude that minimizing both objectives also minimizes Hθ[Y, Z], and\nthat generally the objectives trade-off with each other at stationary points θ of Hθ[Y, Z]:\n∇Hθ[Y | Z] = −∇Hθ[Z]\nwhen ∇Hθ[Y, Z] = 0.\n(3.21)\nThis tells us that to construct a case where the optima do not coincide, discriminative\nclassification needs to be opposed better density estimation.\nExistence. Let us also show that the cases described above can occur. Specifically,\nwhen we have a GMM with one component per class, minimizing Hθ[Y, Z] on an\nempirical data distribution is equivalent to Gaussian Discriminant Analysis, as is easy\nto check, and minimizing Hθ[Z] is equivalent to fitting a density estimator, following\nEquation 3.19. The difference is that using a GMM as a density estimator does\nnot constrain the component assignment for a sample, unlike in GDA. Overall, we\nsee that both objectives can be minimized at the same time exactly when the feature\nrepresentations of different classes are perfectly separated, such that a GMM fit as density\nestimator would assign each class’s feature representations to a single component.\nBy the above, we can construct simple examples for both cases: if the samples of\ndifferent classes are not separated in feature-space, optima for the objectives will not\ncoincide. For example, if samples were drawn from the same Gaussian and labeled\n3. Single Forward-Pass Aleatoric and Epistemic Uncertainty\n58\n10\n5\n0\n5\n10\nx\n10\n5\n0\n5\n10\ny\nmin H [Y|Z]\n10\n5\n0\n5\n10\nx\n10\n5\n0\n5\n10\ny\nmin H [Y, Z]\n10\n5\n0\n5\n10\nx\n10\n5\n0\n5\n10\ny\nmin H [Z]\n(a) Density. Contours at 68.26%, 95.44%, and 99.7%.\n10\n5\n0\n5\n10\nx\n10\n5\n0\n5\n10\ny\nmin H [Y|Z]\n10\n5\n0\n5\n10\nx\n10\n5\n0\n5\n10\ny\nmin H [Y, Z]\n0.00\n0.15\n0.30\n0.45\n0.60\n0.75\n0.90\n1.05\n1.20\n(b) Entropy. Darker is lower.\nFigure 3.14:\n3-component GMM fitted to a synthetic dataset with 3 different classes\n(differently colored) with 4% label noise using different objectives. (a): The optima for\nconditional log-likelihood Hθ[Y | Z], joint log-likelihood Hθ[Y, Z], and marginalized log-\nlikelihood Hθ[Z] all differ. Hence, the best calibrated model (Hθ[Y | Z]) will not provide the\nbest density estimate (Hθ[Z]), and vice-versa. (b): A mixture model that optimizes Hθ[Y, Z]\n(GDA) does not have calibrated decision boundaries for aleatoric uncertainty: the ambiguous\nsample (due to label noise) marked by the yellow star has no aleatoric uncertainty under the\nGDA model. See §3.3.1 for details.\nrandomly. On the other hand, when the features of different classes all lie in well-\nseparated clusters, GDA can minimize all objectives at the same time.\nGiven that perfect separation is impossible with ambiguous data for a GMM, a\nshared optimum will be rare with noisy real-world data, but only then would GDA\nbe optimal. In all other cases, GDA does not optimize both objectives, and neither\ncan any other GMM with one component per class.\nEquation 3.21 tells us that a GMM fit using Expectation Maximization is likely\na better density estimator than GDA, and a softmax layer is a better classifier, as\noptimizing the density objective H[Z] or softmax objective Hθ[Y | Z] using gradient\ndescent will move away from the optimum of the GDA objective\n3.3.1\nToy Example\nTo explain Proposition 3.3 in an intuitive way, we focus on a simple synthetic 2D dataset\nwith three classes and 4% label noise and fit GMMs using the different objectives.\nTo construct, the dataset we sample “latents” z from three different Gaussians (each\nrepresenting a different class y) with 4% label noise. Following the construction in the\nproof, this will lead the objectives to have different optima. Note that label noise and\n3. Single Forward-Pass Aleatoric and Epistemic Uncertainty\n59\nTable 3.3: Realized objective scores (columns) for different optimization objectives (rows)\nfor the synthetic 2D toy example depicted in Figure 3.14. Smaller is better. We see that\neach objective minimizes its own score while being suboptimal in regard to the other two\nobjectives (when it is possible to compute the scores). This empirically further validates\nProposition 3.3.\nLoss →\nHθ[Y | Z] (↓)\nHθ[Y, Z] (↓)\nHθ[Z] (↓)\nObjective ↓\nmin Hθ[Y | Z]\n0.1794\n5.4924\n5.2995\nmin Hθ[Y, Z]\n0.2165\n4.9744\n4.7580\nmin Hθ[Z]\nn/a\nn/a\n4.7073\nnon-separability of features are common issues in real-world datasets.\nIn Table 3.3 and Figure 3.14(a), we see that each solution minimizes its own\nobjective best. The regular GMM (which optimizes the density) provides the best\ndensity model (best fit according to the entropy), while the LDA (like a softmax linear\nlayer) provides the best NLL for the labels. The GDA provides a density model that\nis almost as good as the GMM. Let us discuss the different objectives in Figure 3.14\nand the resulting scores in more detail:\nmin Hθ[Y | Z]. A softmax linear layer is equivalent to an LDA (Linear Discriminant\nAnalysis) with conditional likelihood as detailed in Murphy [2012]. We optimize\nan LDA with the usual objective \"min −1/N P log q(y | z)\", i.e. the cross-entropy\nof q(y | z) or (average) negative log-likelihood (NLL). Because we optimize only\nq(y | z), q(z) does not affect the objective and is thus not optimized. Indeed, the\ncomponents do not actually cover the latents well, as can be seen in the first density\nplot of Figure 3.14(a). However, it does provide the lowest NLL.\nmin Hθ[Y, Z]. We optimize a GDA for the combined objective \"min −1/N P log q(y, z)\",\ni.e. the cross-entropy of q(y, z). We use the shorthand \"min Hθ[Y | Z]\" for this.\nmin Hθ[Z]. We optimize a GMM for the objective \"min −1/N\nP log q(z)\", i.e. the\ncross-entropy of q(z). We use the shorthand \"min Hθ[Z]\" for this. Scores for Hθ[Y |\nZ] and Hθ[Y, Z] for the third objective min Hθ[Z] are not provided in Table 3.3 as it\ndoes not depend on Y , and hence the different components do not actually model\nthe different classes. Hence, we also use a single color to visualize the components\nfor this objective in Figure 3.14(a).\nEntropy Plot. Looking at the entropy plots in Figure 3.14(b), we first notice that\nthe LDA solution optimized for min Hθ[Y | Z] has a wide decision boundary. This\nis due to the overlap of the Gaussian components, which is necessary to provide\nthe right aleatoric uncertainty.\nOptimizing the negative log-likelihood −log p(y | z) is a proper scoring rule, and\nhence is optimized for calibrated predictions.\nCompared to this, the GDA solution (optimized for min Hθ[Y, Z]) has a much\nnarrower decision boundary and cannot capture aleatoric uncertainty as well. This\nis reflected in the higher NLL. Moreover, unlike for LDA, GDA decision boundaries\nbehave differently than one would naively expect due to the untied covariance matrices.\nThey can be curved, and the decisions change far away from the data [Murphy, 2012].\n3. Single Forward-Pass Aleatoric and Epistemic Uncertainty\n60\nTo show the difference between the two objectives we have marked an ambiguous\npoint near (0, −5) with a yellow star. Under the first objective min Hθ[Y, Z], the point\nhas high aleatoric uncertainty (high entropy), as seen in the left entropy plot while\nunder the second objective (min Hθ[Y, Z]) the point is only assigned very low entropy.\nThe GDA optimized for the second objective thus is overconfident.\nAs explained above, we do not show an entropy plot of Y | Z for the third\nobjective min Hθ[Z] in Figure 3.14(b) because the objective does not depend on Y ,\nand there are thus no class predictions.\nIntuitively, for aleatoric uncertainty, the Gaussian components need to overlap\nto express high aleatoric uncertainty (uncertain labelling). At the same time, this\nnecessarily provides looser density estimates. On the other hand, the GDA density is\nmuch tighter, but this comes at the cost of NLL for classification because it cannot\nexpress aleatoric uncertainty that well. Figure 3.14 visualizes how the objectives\ntrade-off between each other, and why we use the softmax layer trained for p(y | z) for\nclassification and aleatoric uncertainty, and GDA as density model for q(z).\n3.3.2\nDiscussion\nWe have shown that the objectives for a GMM with one component per class are\nobviously not equivalent, and that the optima of these objectives do not need to\ncoincide. Hence, importantly, the above statement tells us that we can expect better\nperformance by using both a discriminative classifier (e.g., softmax layer) to capture\naleatoric uncertainty for iD samples and a separate feature-density estimator to capture\nepistemic uncertainty even on a model trained using conditional log-likelihood, i.e. the\nusual cross-entropy objective. As noted in the previous section §3.2, we focus on the\nGDA objective instead of the GMM objective as it is easier to compute, even though it\nalso suffers from an objective mismatch. However, both in our toy example (Table 3.3),\nwhere the difference between the GDA objective to the softmax objective is larger than\nthe difference to the GMM objective for the relevant metrics, and in our experiments\n(Table 3.9), we found that the GDA objective is sufficient for good performance.\n3.4\nDeep Deterministic Uncertainty\nBased on the previous sections, we propose the following method:\nDeep Deterministic Uncertainty (DDU) is a simple baseline method for uncertainty\nquantification for deterministic neural networks. It uses a deterministic neural\nnetwork with an appropriately regularized feature-space, using spectral normalization\n[Liu et al., 2020a], which can disentangle aleatoric and epistemic uncertainty. It\nestimates:\n1. aleatoric uncertainty using softmax entropy, and\n2. epistemic uncertainty by fitting a GDA after training.\nThere is no need for any additional precessing steps: no hold-out “OoD” data, feature\nensembling, or input pre-processing, unlike in Lee et al. [2018b].\nEnsuring Sensitivity & Smoothness. We ensure sensitivity and smoothness using\nspectral normalization in models with residual connections. In addition, we make\nminor changes to the standard residual block to further encourage sensitivity without\nsacrificing accuracy (see details in §3.4.1).\n3. Single Forward-Pass Aleatoric and Epistemic Uncertainty\n61\nAlgorithm 1 Deep Deterministic Uncertainty\n1: Definitions:\n- Regularized feature extractor fθ : x →Rd\n- Softmax output predictions: p(y | x)\n- GMM density: q(z) = P\ny q(z | y = c) q(y = c)\n- Dataset (X, Y )\n2: procedure train\n3:\ntrain regularized NN p(y | fθ(x)) with (X, Y )\n4:\nfor each class c with samples xc ⊂X do\n5:\nµc ←\n1\n|xc|\nP\nxc fθ(xc)\n6:\nΣc ←\n1\n|xc|−1(fθ(xc) −µc)(fθ(xc) −µc)T\n7:\nπc ←\nP\nxc 1\n|X|\n8:\nend for\n9: end procedure\n10: function disentangle_uncertainty(sample x)\n11:\ncompute feature representation z = fθ(x)\n12:\ncompute density under GMM: q(z) = P\ny q(z | y) q(y) with q(z | y) ∼\nN(µy; σy), q(y) = πy\n13:\ncompute softmax entropy: Hp[Y |x]\n14:\nif low density q(z) then\n15:\nreturn (−q(z), ∅)\n16:\nelse if high density q(z) then\n17:\nreturn (−q(z), Hp[Y |x])\n18:\nend if\n19: end function\nDisentangling Epistemic & Aleatoric Uncertainty. To quantify epistemic uncer-\ntainty, we fit a feature-space density estimator after training. We use GDA, a GMM\nq(y, z) with a single Gaussian component per class, and fit each class component\nby computing the empirical mean and covariance, per class, of the feature vectors\nz = fθ(x), which are the outputs of the last convolutional layer of the model computed\non the training samples x. Note that we do not require OoD data to fit these and\nunlike Lee et al. [2018b] we use a separate covariance matrix for each class. Fitting\na GDA on the feature space, thus requires no further training and only requires a\nsingle forward-pass through the training set.\nEvaluation. At test time, we estimate the epistemic uncertainty by evaluating the\nmarginal likelihood of the feature representation under our density q(z) = P\ny q(z |\ny) q(y). To quantify aleatoric uncertainty for in-distribution samples, we use the entropy\nH[Y |x, θ] of the softmax distribution p(y|x, θ). Note that the softmax distribution thus\nobtained can be further calibrated using temperature scaling [Guo et al., 2017]. Thus, for\na given input, a high feature-space density indicates low epistemic uncertainty (iD), at\nwhich point, we can trust the aleatoric estimate from the softmax entropy. The sample\ncan then be either unambiguous (low softmax entropy) or ambiguous (high softmax\nentropy). Conversely, a low feature density indicates high epistemic uncertainty (OoD),\nand we cannot trust softmax predictions. The algorithm is depicted in Algorithm 1.\n3. Single Forward-Pass Aleatoric and Epistemic Uncertainty\n62\n# instantiate\nmodels\nmodel = create_sensitive_smooth_model ()\ngda = create_gda ()\n# train\ntraining_samples , training_labels = load_training_set ()\nmodel.fit(training_samples , training_labels )\ntraining_features = model.features( training_samples )\ngda.fit(training_features , training_labels )\n# test\ntest_features = model.features( test_sample )\nepistemic_uncertainty = -gda. log_density ( test_features )\nis_ood = epistemic_uncertainty\n<= ood_threshold\nif not\nis_ood:\npredictions = model. softmax_layer ( test_features )\naleatoric_uncertainty = entropy( predictions )\nreturn\nepistemic_uncertainty , aleatoric_uncertainty\n# aleatoric\nuncertainty\nis only\nvalid\nfor iD\nreturn\nepistemic_uncertainty , None\nListing 3.1: Deep Deterministic Uncertainty Pseudo-Code\n3.4.0.1\nComputational Complexity\nLet N be the number of samples; D, the feature space dimensionality; and C, the\nnumber of classes; with ≈N/C samples per class (balanced). For fitting the GMM via\nGDA: computing the covariance matrix per class requires O(C(N/C)D2) = O(ND2)\ncomplexity. Computing the inverse and determinant of the covariance matrices via the\nCholesky decomposition requires O(D3) per class. Thus, the total computational cost\nfor GDA is O(ND2 + CD3). Evaluating density of a single point: distance from class\nmeans requires O(CD), and matrix vector multiplications requires O(CD2). Hence,\nthe total cost for evaluating density on a single point is O(CD2).\n3.4.1\nImplementation\nHere, we describe our reference implementation in more detail. A simple Python\npseudocode using a ‘scikit-learn’-like API [Buitinck et al., 2013] is shown in Listing 3.1.\nNote that in order to compute thresholds for low and high density or entropy, we\nsimply use the training set containing iD data. We consider all points having density\nlower than the 99% quantile as OoD.\nIncreasing sensitivity. Using residual connections to enforce sensitivity works well in\npractice when the layer is defined as x′ = x+f(x). However, there are several places in\nthe network where additional spatial downsampling is done in f(·) (through a strided\nconvolution), and in order to compute the residual operation x needs to be downsampled\nas well. These downsampling operations are crucial for managing memory consumption\nand generalization. The way this is traditionally done in ResNets is by introducing\nan additional function g(·) on the residual branch (obtaining x′ = g(x) + f(x)) which\nis a strided 1x1 convolution. In practice, the stride is set to 2 pixels, which leads to\nthe output of g(·) only being dependent on the top-left pixel of each 2x2 patch, which\nreduces sensitivity. We overcome this issue by making an architectural change that\nimproves uncertainty quality without sacrificing accuracy. We use a strided average\n3. Single Forward-Pass Aleatoric and Epistemic Uncertainty\n63\npooling operation instead of a 1x1 convolution in g(·). This makes the output of g(·)\ndependent on all input pixels. Additionally, we use leaky ReLU activation functions,\nwhich are equivalent to ReLU activations when the input is larger than 0, but below\n0 they compute p ∗x with p = 0.01 in practice. These further improve sensitivity\nas all negative activations still propagate in the network.\n3.5\nEmpirical Validation\nWe evaluate DDU on active learning and OoD detection tasks:\nVisualizations. We detail toy experiments on how DDU can disentangle epistemic\nand aleatoric uncertainty and the effect of feature-space regularization in, depicted\nin Figure 3.2, and on the well-known Two Moons toy dataset in §3.5.1.1.\nActive Learning. For active learning [Cohn et al., 1996], we evaluate DDU using\nMNIST, CIFAR-10 and an ambiguous version of MNIST (Dirty-MNIST).\nOoD Detection. Understanding the caveats detailed in §3, we can use OoD detection\nto evaluate using DDU for estimating epistemic uncertainty and aleatoric uncertainty\nsuch that it will be meaningful for active learning as well: we will focus on ‘near\nOoD’ datasets such that good OoD detection performance is a good proxy for good\nepistemic uncertainty performance, and we do not use ‘far OoD’ datasets as they\nwould not be informative for active learning performance.\nThus, we evaluate DDU’s quality of epistemic uncertainty estimation on several OoD\ndetection settings for:\n• image classification on CIFAR-10 vs SVHN/CIFAR-100/Tiny-ImageNet/CIFAR-\n10-C, CIFAR-100 vs SVHN/Tiny-ImageNet and ImageNet vs ImageNet-O\ndataset pairings, where we outperform other deterministic single-forward-pass\nmethods and perform on par with deep ensembles;\n• semantic segmentation on Pascal VOC, comparing with a deterministic model,\nMC Dropout (MCDO) Gal and Ghahramani [2016a] and deep ensembles;\n• on the real-world QUBIQ challenge in §C.3.1\nAblations. We ablate feature space density on different model architectures, compare\nGDA and LDA, examine the effect of the objective mismatch on CIFAR-10, and\nprovide results on additional baselines, with additional ablations in Appendix C.2.\nWhile the focus of this thesis is on data subset selection, the bulk of the experiments\nfor DDU is on OoD detection as these experiments are easier to run and ablate\nthan active learning experiments.\n3.5.1\nVisualizations\nTwo toy experiments illustrate the effect of feature-space regularization on the quality\nof epistemic and aleatoric uncertainty estimation. The first experiment is on a simple\n2D toy dataset, and the second experiment is on the MNIST dataset.\n3. Single Forward-Pass Aleatoric and Epistemic Uncertainty\n64\n2\n1\n0\n1\n2\n3\n3\n2\n1\n0\n1\n2\n3\n(a) Softmax Entropy\n2\n1\n0\n1\n2\n3\n3\n2\n1\n0\n1\n2\n3\n(b) Ensemble Predictive Entropy\n2\n1\n0\n1\n2\n3\n3\n2\n1\n0\n1\n2\n3\n(c) DDU Feature-space density\n2\n1\n0\n1\n2\n3\n3\n2\n1\n0\n1\n2\n3\n(d) FC-Net Feature-space density\nFigure 3.15: Uncertainty on Two Moons dataset. Blue indicates high uncertainty and yellow\nindicates low uncertainty. Both the softmax entropy of a single model and the predictive\nentropy of a deep ensemble are uncertain only along the decision boundary whereas the\nfeature-space density of DDU is uncertain everywhere except on the data distribution (the\nideal behavior). However, the feature density of a normal fully connected network (FC-Net)\nwithout any inductive biases can’t capture uncertainty properly.\n3.5.1.1\nTwo Moons\nIn this section, we evaluate DDU’s performance on a well-known toy setup: the Two\nMoons dataset. We use scikit-learn’s datasets package to generate 2000 samples with a\nnoise rate of 0.1. We use a 4-layer fully connected architecture, ResFFN-4-128 with\n128 neurons in each layer and a residual connection, following [Liu et al., 2020a]. As\nan ablation, we also train using a 4-layer fully connected architecture with 128 neurons\nin each layer, but without the residual connection. We name this architecture FC-Net.\nThe input is 2-dimensional and is projected into the 128 dimensional space using a\nfully connected layer. Using the ResFFN-4-128 architecture we train 3 baselines:\nSoftmax. We train a single softmax model and use the softmax entropy as the\nuncertainty metric.\n3-Ensemble. We train an ensemble of 3 softmax models and use the predictive\nentropy of the ensemble as the measure of uncertainty.\nDDU. We train a single softmax model applying spectral normalization on the fully\nconnected layers and using the feature density as the measure of model confidence.\n3. Single Forward-Pass Aleatoric and Epistemic Uncertainty\n65\nTable 3.4: ECE for Dirty-MNIST test set and AUROC for Dirty-MNIST vs FashionMNIST\nas proxies for aleatoric and epistemic uncertainty quality respectively.\nModel\nECE (↓)\nAUROC\nSoftmax Entropy (↑)\nFeature Density (↑)\nLeNet\n2.22\n84.23\n71.41\nVGG-16\n2.11\n84.04\n89.01\nResNet-18+SN (DDU)\n2.34\n83.01\n99.91\nEach model is trained using the Adam optimizer for 150 epochs. In Figure 3.15,\nwe show the uncertainty results for all the above 3 baselines. It is clear that both\nthe softmax entropy and the predictive entropy of the ensemble is uncertain only\nalong the decision boundary between the two classes whereas DDU is confident only\non the data distribution and is not confident anywhere else. It is worth mentioning\nthat even DUQ and SNGP perform well in this setup and deep ensembles have been\nknown to underperform in the Two-Moons setup primarily due to the simplicity of the\ndataset causing all the ensemble components to generalize in the same way. Finally,\nalso note that the feature space density of FC-Net without residual connections is\nnot able to capture uncertainty well (Figure 3.15(d)), thereby reaffirming our claim\nthat proper inductive biases are indeed a necessary component to ensure that feature\nspace density captures uncertainty reliably.\n3.5.1.2\nMotivational Example in Figure 3.2\nAs mentioned, in Figure 3.2 we train a LeNet [LeCun et al., 1998], a VGG-16 [Simonyan\nand Zisserman, 2015] and a ResNet-18 with spectral normalization [He et al., 2016;\nMiyato et al., 2018] (ResNet-18+SN) on Dirty-MNIST. Figure 3.2(b) shows that the\nsoftmax entropy of a deterministic model is unable to distinguish between iD (Dirty-\nMNIST) and OoD (FashionMNIST [Xiao et al., 2017]) samples as the entropy for the\nlatter heavily overlaps with the entropy for Ambiguous-MNIST samples. However, the\nfeature-space density of the model with our inductive biases in Figure 3.2(c) captures\nepistemic uncertainty reliably and is able to distinguish iD from OoD samples. The\nsame cannot be said for LeNet or VGG in Figure 3.2(c), whose densities are unable to\nseparate OoD from iD samples. This demonstrates the importance of the inductive\nbias to ensure the sensitivity and smoothness of the feature space as we further argue\nbelow. Finally, Figure 3.2(b) and Figure 3.2(c) demonstrate that our method is able to\nseparate aleatoric from epistemic uncertainty: samples with low feature density have\nhigh epistemic uncertainty, whereas those with both high feature density and high\nsoftmax entropy have high aleatoric uncertainty—note the high softmax entropy for\nthe most ambiguous Ambiguous-MNIST samples in Figure 3.2(b).\nDisentangling Epistemic and Aleatoric Uncertainty Table 3.4 gives a quan-\ntitative evaluation of the qualitative results in §3. The AUROC metric reflects the\nquality of the epistemic uncertainty as it measures the probability that iD and OoD\nsamples can be distinguished, and OoD samples are never seen during training while\niD samples are semantically similar to training samples. The ECE metric measures the\nquality of the aleatoric uncertainty. The softmax outputs capture aleatoric uncertainty\nwell, as expected, and all 3 models obtain similar ECE scores on the Dirty-MNIST\ntest set. However, with an AUROC of around 84% for all the 3 models, on Dirty-\n3. Single Forward-Pass Aleatoric and Epistemic Uncertainty\n66\n50\n100\n150\n200\n250\n300\nAcquired dataset size\n30\n40\n50\n60\n70\n80\n90\nMNIST Test Accuracy\nSoftmax\nDDU\nEnsemble PE\nEnsemble MI\n(a) MNIST\n2500\n5000\n7500\n10000\n12500\n15000\n17500\n20000\nAcquired dataset size\n55\n60\n65\n70\n75\n80\n85\n90\n95\nCIFAR-10 Test Accuracy\nSoftmax\nDDU\nEnsemble PE\nEnsemble MI\n(b) CIFAR-10\n50\n100\n150\n200\n250\n300\nAcquired dataset size\n0\n10\n20\n30\n40\n50\n60\n70\n80\nMNIST Test Accuracy\nSoftmax\nDDU\nEnsemble PE\nEnsemble MI\nVGG GMM\nSNGP\nDUQ\n(c) Dirty-MNIST\nFigure 3.16: Active Learning experiments. Acquired training set size vs test accuracy. DDU\nperforms on par with deep ensembles.\nMNIST vs FashionMNIST, we conclude that softmax entropy is unable to capture\nepistemic uncertainty well. This is reinforced in Figure 3.2(b), which shows a strong\noverlap between the softmax entropy of OoD and ambiguous iD samples. At the\nsame time, the feature-space densities of LeNet and VGG-16, with AUROC scores\naround 71% and 89% respectively, are unable to distinguish OoD from iD samples,\nindicating that simply using feature-space density without appropriate inductive biases\n(as seen in [Lee et al., 2018b]) is not sufficient.\nOnly by fitting a GMM on top of a feature extractor with appropriate inductive\nbiases (DDU) and using its feature density are we able to obtain performance far better\n(with AUROC of 99.9%) than the alternatives in the ablation study (see Table 3.4, but\nthis is also noticeable in Figure 3.2(c)). The entropy of a softmax model can capture\naleatoric uncertainty, even without additional inductive biases, but it cannot be used to\nestimate epistemic uncertainty (see §3.2). On the other hand, feature-space density can\nonly be used to estimate epistemic uncertainty when the feature extractor is sensitive\nand smooth, as achieved by using a ResNet and spectral normalization in DDU.\n3.5.2\nActive Learning\nWe evaluate DDU on three different active learning setups:\n1. with clean MNIST samples in the pool set,\n2. with clean CIFAR-10 samples in the pool set, and\n3. with Dirty-MNIST, having a 1:60 ratio of MNIST to Ambiguous-MNIST samples,\nin the pool set.\nIn the first two setups, we compare three baselines as for two moons:\n• a ResNet-18 with softmax entropy as the acquisition function,\n3. Single Forward-Pass Aleatoric and Epistemic Uncertainty\n67\n• DDU trained using a ResNet-18 with feature density as acquisition function, and\n• a deep ensemble of 3 ResNet-18s with the predictive entropy (PE) and mutual\ninformation (MI) of the ensemble as the acquisition functions.\nFor Dirty-MNIST, in addition to the above 3 approaches, we also compare to:\n• feature density of a VGG-16 instead of ResNet-18+SN as an ablation to see if\nfeature density of a model without inductive biases performs well, as well as\n• SNGP [Lee et al., 2020] and DUQ [van Amersfoort et al., 2020] as additional\nbaselines.\nFor MNIST and Dirty-MNIST, we start with an initial training-set size of 20 randomly\nchosen MNIST points, and in each iteration, acquire the 5 samples with the highest\nreported epistemic uncertainty. We re-train the models after each batch acquisition\nstep using Adam [Kingma and Ba, 2015] for 100 epochs and choose the model with the\nbest validation set accuracy. We stop the process when the training set size reaches\n300. For CIFAR-10, we start with 1000 samples and go up to 20000 samples with\nan acquisition size of 500 samples in each step.\nMNIST & CIFAR-10. In Figure 3.16(a) and Figure 3.16(b), for regular curated\nMNIST and CIFAR-10 in the pool set, DDU clearly outperforms the deterministic\nsoftmax baseline and is competitive with deep ensembles. For MNIST, the softmax\nbaseline reaches 90% test-set accuracy at a training-set size of 245. DDU reaches 90%\naccuracy at a training-set size of 160, whereas deep ensemble reaches the same at 185\nand 155 training samples with PE and MI as the acquisition functions respectively.\nNote that DDU is three times faster than a deep ensemble, which needs to train three\nmodels independently after every acquisition.\nDirty-MNIST. Real-life datasets often contain observation noise and ambiguous\nsamples. What happens when the pool set contains a lot of such noisy samples having\nhigh aleatoric uncertainty? In such cases, it becomes important for models to identify\nunseen and informative samples with high epistemic uncertainty and not with high\naleatoric uncertainty. To study this, we construct a pool set with samples from Dirty-\nMNIST (see §3.1.1.1). We significantly increase the proportion of ambiguous samples\nby using a 1:60 split of MNIST to Ambiguous-MNIST (a total of 1K MNIST and 60K\nAmbiguous-MNIST samples). In Figure 3.16(c), for Dirty-MNIST in the pool set, the\ndifference in the performance of DDU and the deterministic softmax model is stark.\nWhile DDU achieves a test set accuracy of 70% at a training set size of 240 samples,\nthe accuracy of the softmax baseline peaks at a mere 50%. In addition, all baselines,\nincluding SNGP, DUQ and the feature density of a VGG-16, which fail to solely capture\nepistemic uncertainty, are significantly outperformed by DDU and the MI baseline of the\ndeep ensemble. However, note that DDU also performs better than deep ensembles with\nthe PE acquisition function. The difference gets larger as the training set size grows:\nDDU’s feature density and deep ensemble’s MI solely capture epistemic uncertainty and\nhence, do not get confounded by iD ambiguous samples with high aleatoric uncertainty.\n3.5.3\nOoD Detection\nNear-OoD detection is an application of epistemic uncertainty quantification: if we\ndo not train on OoD data, we expect near OoD data points to have higher epistemic\nuncertainty than iD data.\n3. Single Forward-Pass Aleatoric and Epistemic Uncertainty\n68\nTable 3.5: OoD detection performance of different baselines using a Wide-ResNet-28-10 architecture with the CIFAR-10 vs SVHN/CIFAR-\n100/Tiny-ImageNet and CIFAR-100 vs SVHN/Tiny-ImageNet dataset pairs averaged over 25 runs. SN: Spectral Normalization, JP: Jacobian\nPenalty. The best deterministic single-forward pass method and the best method overall are in bold for each metric.\nTrain Dataset\nMethod\nPenalty\nAleatoric Uncertainty\nEpistemic Uncertainty\nAccuracy (↑)\nECE (↓)\nAUROC\nSVHN (↑)\nCIFAR-100 (↑)\nTiny-ImageNet (↑)\nCIFAR-10\nSoftmax\n-\nSoftmax Entropy\nSoftmax Entropy\n95.98 ± 0.02\n0.85 ± 0.02\n94.44 ± 0.43\n89.39 ± 0.06\n88.42 ± 0.05\nEnergy-based [Liu et al., 2020b]\n-\nSoftmax Density\n94.56 ± 0.51\n88.89 ± 0.07\n88.11 ± 0.06\nDUQ [van Amersfoort et al., 2020]\nJP\nKernel Distance\nKernel Distance\n94.6 ± 0.16\n1.55 ± 0.08\n93.71 ± 0.61\n85.92 ± 0.35\n86.83 ± 0.12\nSNGP [Liu et al., 2020a]\nSN\nPredictive Entropy\nPredictive Entropy\n96.04 ± 0.09\n1.8 ± 0.1\n94.0 ± 1.3\n91.13 ± 0.15\n89.97 ± 0.19\nDDU (ours)\nSN\nSoftmax Entropy\nGDA Density\n95.97 ± 0.03\n0.85 ± 0.04\n97.86 ± 0.19\n91.34 ± 0.04\n91.07 ± 0.05\n5-Ensemble\n-\nPredictive Entropy\nPredictive Entropy\n96.59 ± 0.02\n0.76 ± 0.03\n97.73 ± 0.31\n92.13 ± 0.02\n90.06 ± 0.03\n[Lakshminarayanan et al., 2017]\nMutual Information\n97.18 ± 0.19\n91.33 ± 0.03\n90.90 ± 0.03\nAccuracy (↑)\nECE (↓)\nSVHN (↑)\nTiny-ImageNet (↑)\nCIFAR-100\nSoftmax\n-\nSoftmax Entropy\nSoftmax Entropy\n80.26 ± 0.06\n4.62 ± 0.06\n77.42 ± 0.57\n81.53 ± 0.05\nEnergy-based [Liu et al., 2020b]\n-\nSoftmax Density\n78 ± 0.63\n81.33 ± 0.06\nSNGP [Liu et al., 2020a]\nSN\nPredictive Entropy\nPredictive Entropy\n80.00 ± 0.11\n4.33 ± 0.01\n85.71 ± 0.81\n78.85 ± 0.43\nDDU (ours)\nSN\nSoftmax Entropy\nGMM Density\n80.98 ± 0.06\n4.10 ± 0.08\n87.53 ± 0.62\n83.13 ± 0.06\n5-Ensemble\n-\nPredictive Entropy\nPredictive Entropy\n82.79 ± 0.10\n3.32 ± 0.09\n79.54 ± 0.91\n82.95 ± 0.09\n[Lakshminarayanan et al., 2017]\nMutual Information\n77.00 ± 1.54\n82.82 ± 0.04\n3. Single Forward-Pass Aleatoric and Epistemic Uncertainty\n69\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nCorruption Intensity\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\nAverage AUROC\nDDU\nEnsemble\nSNGP\nSoftmax\n(a) WRN-28-10\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nCorruption Intensity\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\nAverage AUROC\nDDU\nEnsemble\nSNGP\nSoftmax\n(b) RN-50\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nCorruption Intensity\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\nAverage AUROC\nDDU\nEnsemble\nSNGP\nSoftmax\n(c) RN-110\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nCorruption Intensity\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\nAverage AUROC\nDDU\nEnsemble\nSNGP\nSoftmax\n(d) DN-121\nFigure 3.17: AUROC vs corruption intensity averaged over all corruption types in CIFAR-\n10-C for 4 architectures. More details in §3.5.3 and more ablations in §C.1 in the appendix.\n3.5.3.1\nImage Classification\nWe evaluate CIFAR-10 vs SVHN/CIFAR-100/Tiny-ImageNet/CIFAR-10-C, CIFAR-\n100 vs SVHN/Tiny-ImageNet and ImageNet vs ImageNet-O as iD vs OoD dataset\npairs for this experiment [Krizhevsky, 2009; Netzer et al., 2011; Deng et al., 2009;\nHendrycks and Dietterich, 2019]. We also evaluate DDU on different architectures:\nWide-ResNet-28-10, Wide-ResNet-50-2, ResNet-50, ResNet-110 and DenseNet-121\n[Zagoruyko and Komodakis, 2016; He et al., 2016; Huang et al., 2017]. The training\nsetup is described in §C.1.2. In addition to using softmax entropy of a deterministic\nmodel (Softmax) for both aleatoric and epistemic uncertainty, we also compare with\nthe following baselines that do not require training or fine-tuning on OoD data:\nEnergy-based model [Liu et al., 2020b]: We use the softmax entropy of a deter-\nministic model as aleatoric uncertainty and the unnormalized softmax density (the\nlogsumexp of the logits) as epistemic uncertainty without regularization to avoid\nfeature collapse. We only compare with the version that does not train on OoD data.\nDUQ [van Amersfoort et al., 2020] & SNGP [Liu et al., 2020a]: We compare\nwith the state-of-the-art deterministic methods for uncertainty quantification includ-\ning DUQ and SNGP. For SNGP, we use the exact predictive covariance computation.\n3. Single Forward-Pass Aleatoric and Epistemic Uncertainty\n70\nTable 3.6: Pascal VOC validation set mIoU and runtime in milliseconds averaged over 10\nforward passes. For MC Dropout, we perform 5 stochastic forward passes.\nBaseline\nSoftmax\nMC Dropout\nDeep Ensemble\nDDU\nmIoU\n78.53\n78.61\n78.47\n78.53\nRuntime (ms)\n275.48 ± 1.91\n1576.75 ± 1.56\n875.87 ± 0.79\n263.83 ± 2.79\nWe measure uncertainty via the entropy of the average of the MC softmax samples.\nFor DUQ, we use the closest kernel distance. Note that for CIFAR-100, DUQ’s\none-vs-all objective did not converge during training and hence, we do not include\nthe DUQ baseline for CIFAR-100.\n5-Ensemble: We use an ensemble of 5 networks with the same architecture and\ncompute the predictive entropy of the ensemble as both epistemic and aleatoric\nuncertainty and mutual information as epistemic uncertainty.\nResults. Table 3.5 presents the AUROC for Wide-ResNet-28-10 models on CIFAR-\n10 vs SVHN/CIFAR-100/Tiny-ImageNet and CIFAR-100 vs SVHN/Tiny-ImageNet\nalong with their respective test set accuracy and ECE post temp-scaling (additional\ncalibration scores in §C.3.2 and comparison with more baselines in §3.5.4).\nThe\nequivalent results for other architectures: ResNet-50/110 and DenseNet-121 can be\nfound in Table C.1, Table C.2 and Table C.3 in the appendix. Note that for DDU,\npost-hoc calibration with temperature scaling [Guo et al., 2017], is simple as it does\nnot affect the GMM density. We also plot the AUROC averaged over corruption types\nvs corruption intensity for CIFAR-10 vs CIFAR-10-C in Figure 3.17, with AUROC\nplots per corruption type in Figure C.2, Figure C.3, Figure C.4 and Figure C.5 of the\nappendix. Finally, in Table 3.7, we present AUROC for models trained on ImageNet.\nFor OoD detection, DDU outperforms all other deterministic single-forward-pass\nmethods, DUQ, SNGP and the energy-based model approach from [Liu et al., 2020b],\non CIFAR-10 vs SVHN/CIFAR-100/Tiny-ImageNet, CIFAR-10 vs CIFAR-10-C and\nCIFAR-100 vs SVHN/Tiny-ImageNet, often performs on par with state-of-the-art\ndeep ensembles—and even performing better in a few cases.\nThis holds true for\nall the architectures we experimented on.\nSimilar observations can be made on\nImageNet vs ImageNet-O as well. Importantly, the great performance in OoD detection\ncomes without compromising on the single-model test set accuracy in comparison\nto other deterministic methods.\n3.5.3.2\nSemantic Segmentation\nIn this section, we apply DDU to the task of semantic segmentation on Pascal VOC\n2012 Everingham et al. [2010], comparing with a vanilla softmax model, MC Dropout\nand deep ensembles. Semantic segmentation [Long et al., 2015], classifies every pixel of\na given to one of a fixed set of classes. Since different classes can have different levels\nof representation in a segmentation dataset, it forms a classic example of a problem\nwith class imbalance, thereby requiring reliable estimates of epistemic uncertainty.\nFurthermore, due to the computationally heavy nature of semantic segmentation,\nclassic uncertainty quantification approaches like MC Dropout and deep ensembles\nare often impractical in real-world applications.\n3. Single Forward-Pass Aleatoric and Epistemic Uncertainty\n71\n(a) Accuracy\n(b) MCD PE\n(c) MCD MI\n(d) En PE\n(e) En MI\n(f) Entropy\n(g) Density\nFigure 3.18: Visualization of uncertainty baselines on four PASCAL VOC validation samples (rows). Columns: (a) shows pixel-wise accuracy;\n(b), (c) predictive entropy (PE) and mutual information (MI) obtained for MC Dropout (MCD); (d), (e) for deep ensembles; (f) per-pixel\nsoftmax entropy, the aleatoric estimate of DDU; and (g) feature density, the epistemic component of DDU. For all but (g): the brighter, the more\nuncertain, whereas DDU’s density (g) captures certainty: hence, the brighter, the more certain.\n3. Single Forward-Pass Aleatoric and Epistemic Uncertainty\n72\n0\n20\n40\n60\n80\n100\nUncertainty Percentiles\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\np(accurate|certain)\nSoftmax\nEnsemble PE\nEnsemble MI\nMC Dropout PE\nMC Dropout MI\nDDU\n0\n20\n40\n60\n80\n100\nUncertainty Percentiles\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\np(uncertain|inaccurate)\nSoftmax\nEnsemble PE\nEnsemble MI\nMC Dropout PE\nMC Dropout MI\nDDU\n0\n20\n40\n60\n80\n100\nUncertainty Percentiles\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nPAVPU\nSoftmax\nEnsemble PE\nEnsemble MI\nMC Dropout PE\nMC Dropout MI\nDDU\nFigure 3.19:\np(accurate | certain), p(uncertain | inaccurate) and PAvPU evaluated on\nPASCAL VOC validation set. DDU outperforms all other baselines.\nPixel-Independent Class-Wise Means and Covariances. As each pixel has a\ncorresponding prediction in semantic segmentation, it is natural to ask if the Gaussian\nmeans and covariance matrices need to be computed per pixel. To examine this, in\nFigure C.1 of §C.1.3, we plot the L2 distances between feature space means of all pairs\nof classes obtained from a DeepLab-v3+ Chen et al. [2017] model with a ResNet-101\nbackbone for two “distant” pixels. We observe that pixels of the same class are much\ncloser in the feature space than pixels of different classes, irrespective of their location\nin the image. In spirit of a new simple baseline, we thus compute the Gaussian means\nand covariances per class, taking each pixel as a separate data point.\nArchitecture, Training and Evaluation Metrics. As mentioned above, we evaluate\nDDU on Pascal VOC 2012 and compare to a vanilla softmax model, MC Dropout\nwith 5 forward passes at test time, and a deep ensembles with 3 members. We use\nDeepLab-v3+ with a ResNet-101 backbone as the model architecture. Additional\ntraining details are in §C.1.3.\nFinally, to evaluate the uncertainty estimates, we\nuse patch-based metrics proposed in Mukhoti and Gal [2018]: p(accurate | certain),\np(uncertainty | inaccurate) and the Patch Accuracy vs Patch Uncertainty PAvPU.\np(accurate | certain) computes the probability of the model being accurate given that\nit is confident. Similarly, p(uncertainty | inaccurate) measures probability of the model\nbeing uncertain given that it is inaccurate, and PAvPU computes the probability\nof the model being confident on accurate predictions and uncertain on inaccurate\nones, so the accuracy depending on the uncertainty threshold similar to a rejection\nplot. Ideally, high values for these metrics indicate better uncertainty estimates in\nsegmentation. Furthermore, note that these metrics can be computed at different\nthresholds of uncertainty (defining if a model is certain or not).\nResults and Discussion. In Figure 3.19, we present the above 3 metrics for all\nsegmentation baselines evaluated on the Pascal VOC validation set. We also report the\nval set accuracy and runtime of a single forward pass in Table 3.6. Finally, we visualize\nuncertainty estimates from each baseline in Figure 3.18. Firstly, from Table 3.6, it\nis clear that DDU has the runtime of a deterministic model which is significantly\nfaster than both MC Dropout and deep ensembles. Also note that DDU’s mIoU is\nthe same as that of the vanilla softmax model. Secondly, from Figure 3.19, we see\nthat DDU consistently performs better on all 3 evaluation metrics compared to the\nother baselines. Finally, Figure 3.18 qualitatively validates that DDU’s feature-space\ndensity captures epistemic uncertainty while the softmax entropy captures aleatoric\n3. Single Forward-Pass Aleatoric and Epistemic Uncertainty\n73\nuncertainty. For DDU, for the first two samples (first two rows, Figure 3.18(g)), the\nepistemic uncertainty is not high and only aleatoric uncertainty is captured along\nedges of objects. However, for the last sample (4th row, Figure 3.18(g)), the epistemic\nuncertainty is high for a relatively large patch on the image which is inaccurately\npredicted by the model as well. Note that only DDU’s feature density is significantly\nlower for that entire region, whereas softmax entropy does not capture high uncertainty\nthere and is only high along the edges. These observations are in line with [Kendall\nand Gal, 2017]: aleatoric uncertainty is high on edges of objects as they correspond\nto regions of high ambiguity and noise; on the other hand, epistemic uncertainty is\nhigh for regions of the image which are previously unseen.\n3.5.4\nAblations\nAdditional ablations for the CIFAR-10/100 experiments are detailed in §C.2, Table C.4\nand C.5.\nWe highlight a few results here:\nFeature Density. These tables along with observations in Table 3.7, show that the\nfeature density of a VGG-16 (i.e. without residual connections and spectral normal-\nization) is unable to beat a VGG-16 ensemble, whereas a Wide-ResNet-28-10 with\nspectral normalization outperforms its corresponding ensemble in almost all the cases.\nThis result further validates the importance of having a regularized feature space\non the model to obtain smoothness and sensitivity. Also note that, even without\nspectral normalization, a Wide-ResNet-28 has residual connections built into its model\narchitecture, which can be a contributing factor towards good performance as residual\nconnections make the model sensitive to changes in the input space.\nGDA vs. LDA. We also provide an ablation using LDA [Lee et al., 2018b], which uses\na shared covariance matrix over all classes, instead of GDA with covariance matrices\nper class. The resulting AUROC for Wide-ResNet-28-10 trained on CIFAR-10/100 and\nfor Wide-ResNet-50-2 and ResNet-50 trained on ImageNet in Table 3.8 in §C.2. LDA\nonly outperforms GDA when using SVHN as an OoD dataset. In all other cases, GDA\nobtains significantly higher AUROC, thereby indicating the advantage of modeling\ndensity using individual covariance matrices per class.\nObjective Mismatch with Wide-ResNet-28-10 on CIFAR-10. We further\nvalidate Proposition 3.3 by running an ablation on Wide-ResNet-28-10 on CIFAR-10.\nTable 3.9 shows that the feature-space density estimator indeed performs worse than\nthe softmax layer for aleatoric uncertainty (accuracy and ECE).\nAdditional Baselines. We provide an ablation with additional baselines on OoD\ndetection for comparison with DDU. In particular, we provide comparisons with\nFeature Space Singularity (FSSD) [Huang et al., 2021], Batch Ensemble (BE) [Wen\net al., 2020] and SWAG [Maddox et al., 2019] using Wide-ResNet-28-10 as additional\nrecent baselines. Huang et al. [2021] computes the distance to the centroid of noise\nsamples in feature-space together with input perturbations, like in [Lee et al., 2018b].\nNoise samples count as ‘far OoD’. [Wen et al., 2020] and [Maddox et al., 2019] are\ncomputationally cheaper ensembling methods.\nWe also use the Wide-ResNet-28-10 feature extractor trained using SNGP loss and\nfit DDU (i.e., GDA) on its feature space. Since SNGP also uses a sensitive smooth\nfeature space with residual connections and spectral normalization, its feature space\nmakes for a good candidate to apply DDU. In Table 3.10, we provide the AUROC\n3. Single Forward-Pass Aleatoric and Epistemic Uncertainty\n74\nscores for models trained on CIFAR-10 and CIFAR-100. Broadly, DDU outperforms\nall competitive baselines. Additionally, we observe a broad improvement in AUROC\nwhen DDU is applied on the SNGP feature extractor as compared to vanilla SNGP.\nHowever, DDU on a feature extractor trained using softmax loss is still superior to\nDDU on the SNGP feature extractor.\n3.6\nComparison to Prior Work\nSeveral existing approaches model uncertainty using feature-space density but require\nfine-tuning on OoD data. This chapter has identified feature collapse and objective\nmismatch as possible reasons for this.\nAmong these approaches, we have already discussed Mahalanobis distances, DUQ,\nand SNGP [Lee et al., 2018b; van Amersfoort et al., 2020; Lee et al., 2020] and the\nimportant findings they provide. In this section, we contrast them to the approach\npresented in this chapter. For one, the best results of Lee et al. [2018b] require input\nperturbations, ensembling GMM densities from multiple layers, and fine-tuning on\nOoD hold-out data. Lee et al. [2018b] do not discuss any constraints which the ResNet\nfeature encoder should satisfy, and therefore, are vulnerable to feature collapse—we\nrecall that in Figure 3.2(c), for example, the feature density of a LeNet and a VGG\nare unable to distinguish OoD from iD samples. Our method also improves upon van\nAmersfoort et al. [2020] and Liu et al. [2020a] by alleviating the need for additional\nhyperparameters: DDU only needs minimal changes from the standard softmax setup\nto outperform DUQ and SNGP on uncertainty benchmarks, and our GMM parameters\nare optimized for the already trained model using the training set. The insights in\n§3.2 might also explain why Liu et al. [2020a] found that an ablation that uses the\nsoftmax entropy instead of the feature-space density of a deterministic network with\nbi-Lipschitz constraints underperforms.\nAmong other related works—there are many, and we can only highlight very few\nhere—Postels et al. [2020], Liu et al. [2020b], and [Winkens et al., 2020] are the most\nrelevant: [Postels et al., 2020] propose a density-based estimation of aleatoric and\nepistemic uncertainty. Similar to [Lee et al., 2018b], they do not constrain their pre-\ntrained ResNet encoder. They do discuss feature collapse though, noting that they do\nnot address this problem. They also do not consider the objective mismatch that arises\n(see Proposition 3.3 below) and use a single estimator for both epistemic and aleatoric\nuncertainty. Consequently, they report worse epistemic uncertainty: 74% AUROC on\nCIFAR-10 vs SVHN, which we show to considerably fall behind modern approaches for\nuncertainty estimation in deep learning in §3.5. Indeed, Postels et al. [2020] report that\nin their case deeper layers provide better aleatoric uncertainty while shallower layers\nprovide better epistemic uncertainty. This might be a result of the objective mismatch\nand not regularizing the feature space using appropriate inductive biases Likewise, Liu\net al. [2020b] compute an unnormalized density based on the softmax logits without\ntaking into account the need for inductive biases to ensure smoothness and sensitivity of\nthe feature space. Finally, Winkens et al. [2020] use contrastive training on the feature\nextractor before estimating the feature-space density. Our method is orthogonal to\nthis work as we restrict ourselves to the supervised setting and show that the inductive\nbiases that encourage bi-Lipschitzness [van Amersfoort et al., 2020; Liu et al., 2020a] are\nsufficient for the feature-space density to more reliably capture epistemic uncertainty.\n3. Single Forward-Pass Aleatoric and Epistemic Uncertainty\n75\nOverall, compared to other methods that require held-out “OoD data” for outlier\nexposure, DDU does not require training or fine-tuning with OoD data in any form.\n3.7\nDiscussion\nWe began this chapter by looking at uncertainty quantification from a conceptual\nview, comparing how active learning and OoD detection use uncertainty quantification.\nHaving identified potential pitfalls when using the predictive entropy of deep ensembles\nor the softmax entropy of deterministic models as proxy for epistemic uncertainty—\nthese potential pitfalls likely apply in general when using the predictive distribution to\nmeasure epistemic uncertainty—we investigated using feature-space density as a proxy\nfor epistemic uncertainty and the entropy of the predictive distribution for aleatoric\nuncertainty, and looked at a possible objective mismatch in detail.\nBased on these insights and detailed experiments, a simple method, DDU, was\nproposed that can obtain reliable epistemic and aleatoric uncertainty estimates for\nsingle-pass, deterministic models. By fitting a GDA to estimate feature-space density\nafter training an off-the-shelf neural network with appropriate inductive biases: residual\nconnections and spectral normalization [Lee et al., 2018b; Liu et al., 2020a], our method\nwas able to outperform state-of-the-art deterministic single-pass uncertainty methods\nin active learning and OoD detection, while performing as well as deep ensembles\nin several settings. Hence, DDU provides a very simple method that presents an\nalternative to deep ensembles without requiring the complexities or computational cost\nof deep ensembles while still providing reliable uncertainty quantification.\nThis is crucial in applications like active learning, which require a reliable estimate\nof epistemic uncertainty, but reliable uncertainty quantification is also an important\nrequirement to make deep neural nets safe for deployment. Thus, we hope the insights\nfrom this chapter can help increase safety, reliability and trust in AI in the future.\n3. Single Forward-Pass Aleatoric and Epistemic Uncertainty\n76\nTable 3.7: OoD detection performance of different baselines using ResNet-50, Wide-ResNet-50-2 and VGG-16 architectures on ImageNet vs\nImageNet-O [Hendrycks et al., 2021]. Best AUROC scores are marked in bold.\nModel\nAccuracy (↑)\nECE (↓)\nAUROC (↑)\nDeterministic\n3-Ensemble\nDeterministic\n3-Ensemble\nSoftmax Entropy\nEnergy-based Model\nDDU\n3-Ensemble PE\n3-Ensemble MI\nResNet-50\n74.8 ± 0.05\n76.01\n2.08 ± 0.11\n2.07\n51.42 ± 0.61\n55.76 ± 0.81\n71.29 ± 0.08\n60.3\n62.43\nWide-ResNet-50-2\n76.75 ± 0.11\n77.58\n1.18 ± 0.07\n1.22\n52.71 ± 0.23\n57.13 ± 0.4\n73.12 ± 0.19\n60.45\n64.81\nVGG-16\n72.48 ± 0.02\n73.54\n2.62 ± 0.11\n2.59\n50.67 ± 0.22\n52.04 ± 0.23\n54.32 ± 0.14\n58.74\n60.56\n3. Single Forward-Pass Aleatoric and Epistemic Uncertainty\n77\nTable 3.8: LDA vs GDA ablation for OoD detection performance using Wide-ResNet-50-2, ResNet-50, Wide-ResNet-50-2 architectures (depending\non dataset) on CIFAR-10 vs SVHN/CIFAR-100/Tiny-ImageNet, CIFAR-100 vs SVHN/Tiny-ImageNet, and ImageNet vs ImageNet-O [Hendrycks\net al., 2021]. Best AUROC (↑) scores are marked in bold. GDA performs better, except with SVHN as OoD dataset.\nModel\nWRN-28-10\nWRN-28-10\nWRN-50-2\nRN-50\niD\nCIFAR-10\nCIFAR-100\nImageNet\nOoD\nSVHN\nCIFAR-100\nTiny-ImageNet\nSVHN\nTiny-ImageNet\nImageNet-O\nLDA (Maha Lee et al. [2018b])\n98.41 ± 0.09\n82.90 ± 0.23\n82.48 ± 0.25\n92.53 ± 0.62\n68.86 ± 0.13\n64.19 ± 0.23\n61.68 ± 0.14\nGDA (DDU, ours)\n97.86 ± 0.19\n91.34 ± 0.04\n91.07 ± 0.05\n87.53 ± 0.62\n83.13 ± 0.06\n73.12 ± 0.19\n71.29 ± 0.08\n3. Single Forward-Pass Aleatoric and Epistemic Uncertainty\n78\nTable 3.9: Objective Mismatch Ablation with WideResNet-28-10 models with and without\nspectral normalization on CIFAR-10.\nWhile the GDA objective performs much better\nthan cross-entropy objective for feature-space density/epistemic uncertainty estimation, it\nunderperforms for aleatoric uncertainty estimation: both accuracy and in particular ECE are\nmuch worse than a regular softmax layer. Averaged over 25 runs.\nModel\nPrediction Source\nAccuracy in % (↑)\nECE (↓)\nWideResNet-28-10\nSoftmax\n95.98 ± 0.02\n2.29 ± 0.02\nGMM\n95.86 ± 0.02\n4.13 ± 0.02\nWideResNet-28-10+SN\nSoftmax\n95.97 ± 0.03\n2.23 ± 0.03\nGMM\n95.88 ± 0.02\n4.12 ± 0.02\nTable 3.10: OoD detection ablation with WRN-28-10 model with additional baselines, FSSD\n[Huang et al., 2021], Batch Ensemble (BE) [Wen et al., 2020] and SWAG [Maddox et al.,\n2019] as well as using DDU with a feature extractor trained on SNGP. For comparison, we\nalso provide performance for vanilla SNGP, deep ensemble and DDU.\nTrain Dataset\nMethod\nAUROC (↑)\nSVHN\nCIFAR-100\nTiny-ImageNet\nCIFAR-10\nFSSD [Huang et al., 2021]\n97.24\n89.88\n90.23\nBE [Wen et al., 2020]\n95.36\n87.63\n88.14\nSWAG [Maddox et al., 2019]\n96.37\n90.33\n90.24\nSNGP [Liu et al., 2020a]\n94.0 ± 1.3\n91.13 ± 0.15\n89.97 ± 0.19\n5-Ensemble [Lakshminarayanan et al., 2017]\n97.73 ± 0.31\n92.13 ± 0.02\n90.06 ± 0.03\nSNGP + DDU\n96.47 ± 0.7\n89.97 ± 0.13\n90.3 ± 0.12\nDDU (Ours)\n97.86 ± 0.19\n91.34 ± 0.04\n91.07 ± 0.05\nCIFAR-100\nSVHN\nTiny-ImageNet\nFSSD [Huang et al., 2021]\n87.64\n82.2\nBE Wen et al. [2020]\n86.44\n78.33\nSWAG Maddox et al. [2019]\n81.41\n81.67\nSNGP Liu et al. [2020a]\n85.71 ± 0.81\n78.85 ± 0.43\n5-Ensemble Lakshminarayanan et al. [2017]\n79.54 ± 0.91\n82.95 ± 0.09\nSNGP + DDU\n87.34 ± 0.76\n79.62 ± 0.36\nDDU (Ours)\n87.53 ± 0.62\n83.13 ± 0.06\nBatch learning refined,\nWith correlations in mind,\nYields diverse knowledge\n4\nDiverse Batch Acquisition for Bayesian\nActive Learning\nIn practical active learning applications, instead of single data points, batches of data\npoints are acquired during each acquisition step to reduce the number of times the\nmodel is retrained and expert-time is requested. Reasons for this are that model\nretraining becomes a computational bottleneck for larger models and expert time is\nexpensive. Consider, for example, the effort that goes into commissioning a medical\nspecialist to label a single MRI scan, then waiting until the model is retrained, and\nthen commissioning a new medical specialist to label the next MRI scan, and the\nextra amount of time this takes.\nIn Gal et al. [2017], batch acquisition, i.e. the acquisition of multiple points, takes\nthe top K points with the highest BALD acquisition score. This naive approach leads\nto acquiring points that are individually very informative, but not necessarily so jointly.\nSee Figure 4.1 for such a batch acquisition of BALD in which it performs poorly\nwhereas scoring points jointly (‘BatchBALD’) can find batches of informative data\npoints. Similarly, Figure 4.3 provides additional anecdotal evidence that the naive\napproach to batch acquisition can lead to poor acquisitions. Figure 4.2 shows how a\ndataset consisting of repeated MNIST digits (with added Gaussian noise) leads BALD to\nperform worse than random acquisition while BatchBALD sustains good performance.\nNaively finding the best batch to acquire requires enumerating all possible subsets\nwithin the available data, which is intractable as the number of potential subsets grows\nexponentially with the acquisition size K and the size of available points to choose from.\nInstead, we develop a greedy algorithm that selects a batch in linear time, and show that\nit is at worst a 1 −1/e approximation to the optimal choice for our acquisition function.\nThe main contributions of this chapter are:\n1. BatchBALD, a data-efficient active learning method that acquires sets of high-\ndimensional image data, leading to improved data efficiency and reduced total\nrun time, section 4.1;\n2. a greedy algorithm to select a batch of points efficiently, section 4.1.1; and\n3. an estimator for the acquisition function that scales to larger acquisition sizes\nand to datasets with many classes, section 4.1.2.\nWe provide two open-source implementations of BatchBALD in https://github.com/\nBlackHC/BatchBALD and https://github.com/BlackHC/BatchBALD_redux.\n4. Diverse Batch Acquisition for Bayesian Active Learning\n80\nBatchBALD\nBALD\nFigure 4.1: Idealized acquisitions of BALD\nand BatchBALD. If a dataset were to contain\nmany (near) replicas for each data point,\nthen BALD would select all replicas of a\nsingle informative data point at the expense\nof other informative data points, wasting data\nefficiency.\nFigure 4.2:\nPerformance on Repeated\nMNIST with acquisition size 10.\nSee sec-\ntion 4.2.1 for further details.\nBatchBALD\noutperforms BALD while BALD performs\nworse than random acquisition due to the\nreplications in the dataset.\nFigure 4.3:\nBALD scores for 1000 randomly-chosen points from the MNIST dataset\n(handwritten digits). The points are color-coded by digit label and sorted by score. The\nmodel used for scoring has been trained to 90% accuracy first. If we were to pick the top\nscoring points (e.g. scores above 0.6), most of them would be 8s, even though we can assume\nthat after acquiring the first couple of them our model would consider them less informative\nthan other available data. Points are slightly jittered on the x-axis by digit label to avoid\noverlaps.\n4.1\nBatchBALD\nWe propose BatchBALD as an extension of BALD whereby we jointly score points\nby estimating the mutual information between a joint of multiple data points and\nthe model parameters:1\naBatchBALD\n\u0010\nxacq\n1..K, p(ωωω | Dtrain)\n\u0011\n= I[Y acq\n1..K ;ΩΩΩ| xacq\n1..K, Dtrain].\n(4.1)\n1We use the notation I[x, y; z | c] to denote the mutual information between the joint of the random\nvariables x, y and the random variable z conditioned on c.\n4. Diverse Batch Acquisition for Bayesian Active Learning\n81\nX\ni\nI[Yi;ΩΩΩ| xi, Dtrain] =\nX\ni\nµ*[Yi ∩ΩΩΩ]\n(a) BALD\nI[Y1..n;ΩΩΩ| x1..n, Dtrain] = µ*\n\"[\ni\nYi ∩ΩΩΩ\n#\n(b) BatchBALD\nFigure 4.4: Intuition behind BALD and BatchBALD using I-diagrams [Yeung, 1991]. BALD\noverestimates the joint mutual information. BatchBALD, however, takes the overlap between\nvariables into account and will strive to acquire a better cover of ΩΩΩ. Areas contributing to\nthe respective score are shown in gray, and areas that are double-counted in dark gray.\nThis builds on the insight that independent selection of a batch of data points leads\nto data inefficiency as correlations between data points in an acquisition batch are\nnot taken into account.\nTo understand how to compute the mutual information between a set of points\nand the model parameters, we express Xacq\n1..K, and Y acq\n1..K through joint random variables\nin a product probability space and use the definition of the mutual information\nfor two random variables:\nI[Y acq\n1..K ;ΩΩΩ| xacq\n1..K, Dtrain] = H[Y acq\n1..K | xacq\n1..K, Dtrain] −H[Y acq\n1..K | xacq\n1..K,ΩΩΩ, Dtrain].\n(4.2)\nIntuitively, the mutual information between two random variables can be seen as\nthe intersection of their information content. In fact, Yeung [1991] shows that a\nsigned measure µ∗can be defined for discrete random variables X, Y , such that\nI[X; Y ] = µ*[X ∩Y ], H[X, Y ] = µ*[X ∪Y ], H[X | Y ] = µ*[X \\ Y ], and so on, where\nwe identify random variables with their counterparts in information space.\nUsing this perspective, BALD can be viewed as the sum of individual intersections\nP\ni µ*[Yi ∩ΩΩΩ], which double counts overlaps between the Yi. Naively extending BALD\nto the mutual information between Y acq\n1..K |xacq\n1..K and ΩΩΩ, which is equivalent to µ*[T\ni Yi ∩ΩΩΩ],\ncan lead to selecting similar data points instead of diverse ones under maximization.\nBatchBALD, on the other hand, takes overlaps into account by computing µ*[S\ni Yi ∩ΩΩΩ]\nand is more likely to acquire a more diverse cover under maximization:\nI[Y acq\n1..K ;ΩΩΩ| xacq\n1..K, Dtrain] = H[Y acq\n1..K | xacq\n1..K, Dtrain] −Ep(ωωω|Dtrain) H[Y acq\n1..K | xacq\n1..K,ωωω, Dtrain]\n(4.3)\n= µ*\n\"[\ni\nyi\n#\n−µ*\n\"[\ni\nYi \\ ΩΩΩ\n#\n= µ*\n\"[\ni\nYi ∩ΩΩΩ\n#\n(4.4)\nThis is depicted in Figure 4.4 and also motivates that aBatchBALD ≤aBALD, which we prove\nin Appendix D.2. For acquisition size 1, BatchBALD and BALD are equivalent.\n4. Diverse Batch Acquisition for Bayesian Active Learning\n82\nAlgorithm 2 Greedy BatchBALD 1 −1/e-approximate algorithm\nInput: acquisition size b, unlabeled dataset Dpool, model parameters\np(ωωω | Dtrain)\n1 A0 ←∅\n2 for n ←1 to b do\n3\nforeach x ∈Dpool \\ An−1 do sx ←aBatchBALD\n\u0000An−1 ∪{x} , p(ωωω | Dtrain)\u0001\n4\nxn ←arg max\nx∈Dpool\\An−1\nsx\n5\nAn ←An−1 ∪{xn}\n6 end\nOutput: acquisition batch An = {x1, ..., xb}\nFigure 4.5: Why consistent MC dropout is necessary: BatchBALD scores for different\nsets of 100 sampled model parameters. This shows the BatchBALD scores for 1000 randomly\nselected points from the pool set while selecting the 10th point in a batch for an MNIST\nmodel that has already reached 90% accuracy. The scores for a single set of 100 model\nparameters (randomly chosen) are shown in blue. The BatchBALD estimates show strong\nbanding with the score differences between different sets of sampled parameters being larger\nthan the differences between different data points within a specific set of model parameters.\nWithout consistent sampling, the arg max would essentially be randomly sampled and not be\ninformative.\n4.1.1\nGreedy Approximation Algorithm for BatchBALD\nTo avoid the combinatorial explosion that arises from jointly scoring subsets of\npoints, we introduce a greedy approximation for computing BatchBALD, depicted\nin Algorithm 2. In Appendix D.1, we prove that aBatchBALD is submodular, which\nmeans the greedy algorithm is 1 −1/e-approximate [Nguyen et al., 2013; Krause et al.,\n2008; Nemhauser et al., 1978].\n4. Diverse Batch Acquisition for Bayesian Active Learning\n83\n4.1.2\nApproximating aBatchBALD via Consistent Monte-Carlo\nSampling\nFor brevity, we leave out conditioning on x1..n, and Dtrain, and p(ωωω) denotes p(ωωω |\nDtrain) in this section. aBatchBALD is then written as:\naBatchBALD ({x1..n} , p(ωωω)) = H[Y1..n] −H[Y1..n | ΩΩΩ]\n(4.5)\n= H[Y1..n] −Ep(ωωω)[H[Y1..n | ωωω]].\n(4.6)\nBecause the Yi are independent when conditioned on ωωω, computing the right term of\nequation (4.5) is simplified as the conditional joint entropy decomposes into a sum.\nWe can approximate the expectation using a Monte-Carlo estimator with k samples\nfrom our model parameter distribution ˆωωωj ∼p(ωωω):\nEp(ωωω)[H[Y1..n | ωωω]] =\nn\nX\ni=1\nEp(ωωω)[H[Yi | ωωω]] ≈1\nk\nn\nX\ni=1\nk\nX\nj=1\nH[Yi | ˆωωωj].\n(4.7)\nCrucially, the samples have to stay fixed across different pool samples. This is not\nhow Monte-Carlo dropout [Gal and Ghahramani, 2016a] is usually implemented. We\ncall this consistent Monte-Carlo dropout. See Figure 4.5 for why this is necessary\n(based on real data).\nComputing the left term of equation (4.5) is difficult because the unconditioned\njoint probability does not factorize. Applying the equality p(y) = Ep(ωωω)[p(y | ωωω)], and,\nusing sampled ˆωωωj, we compute the entropy by summing over all possible configu-\nrations ˆy1:n of y1:n:\nH[Y1..n] = Ep(y1..n)[−log p(y1..n)]\n(4.8)\n= Ep(ωωω) Ep(y1..n|ωωω)[−log Ep(ωωω)[p(y1..n | ωωω)]]\n(4.9)\n≈−\nX\nˆy1:n\n\n1\nk\nk\nX\nj=1\np(ˆy1:n | ˆωωωj)\n\nlog\n\n1\nk\nk\nX\nj=1\np(ˆy1:n | ˆωωωj)\n\n.\n(4.10)\n4.1.3\nEfficient Estimation\nIn each iteration of the algorithm, x1, . . . , xn−1 stay fixed while xn varies over Dpool \\\nAn−1. We can reduce the required computations by factorizing p(y1:n |ωωω) into p(y1:n−1 |\nωωω) p(yn | ωωω). We store p(ˆy1:n−1 | ˆωωωj) in a matrix ˆP1:n−1 of shape Cn−1 × k and p(yn |\nˆωωωj) in a matrix ˆPn of shape C × k. The sum Pk\nj=1 p(ˆy1:n | ˆωωωj) in (4.10) can be then\nbe turned into a matrix product:\n1\nk\nk\nX\nj=1\np(ˆy1:n | ˆωωωj) = 1\nk\nk\nX\nj=1\np(ˆy1:n−1 | ˆωωωj) p(ˆyn | ˆωωωj) =\n\u00121\nk\nˆP1:n−1 ˆP T\nn\n\u0013\nˆy1:n−1,ˆyn\n.\n(4.11)\nThis can be further sped up by using batch matrix multiplication to compute the\njoint entropy for different xn.\nˆP1:n−1 only has to be computed once, and we can\nrecursively compute ˆP1:n using ˆP1:n−1 and ˆPn, which allows us to sample p(y | ˆωωωj) for\neach x ∈Dpool only once at the beginning of the algorithm.\nFor larger acquisition sizes, we use m MC samples of y1:n−1 as enumerating all\npossible configurations becomes infeasible. See Appendix D.3 for details.\n4. Diverse Batch Acquisition for Bayesian Active Learning\n84\n(a) BALD\n(b) BatchBALD\nFigure 4.6: Performance on MNIST for increasing acquisition sizes. BALD’s performance\ndrops drastically as the acquisition size increases. BatchBALD maintains strong performance\neven with increasing acquisition size.\nMonte-Carlo sampling bounds the time complexity of the full BatchBALD algorithm\nto O(KC · min{CK, m} · |Dpool| · k) compared to O(CK · |Dpool|\nK · k) for naively finding\nthe exact optimal batch and O((K + k) · |Dpool|) for BALD2.\n4.2\nEmpirical Validation\nIn our experiments, we start by showing how a naive application of the BALD algorithm\nto an image dataset can lead to poor results in a dataset with many (near) duplicate\ndata points and show that BatchBALD solves this problem in a grounded way while\nobtaining favorable results (Figure 4.2). We then illustrate BatchBALD’s effectiveness\non standard active learning datasets: MNIST and EMNIST. EMNIST [Cohen et al.,\n2017] is an extension of MNIST that also includes letters, for a total of 47 classes\nand has twice as large a training set. See Appendix D.6 for examples of the dataset.\nWe show that BatchBALD provides a substantial performance improvement in these\nscenarios, too, and has more diverse acquisitions. Finally, we look at BatchBALD\nin the setting of transfer learning, where we fine-tune a large pretrained model on a\nmore difficult dataset called CINIC-10 [Darlow et al., 2018], which is a combination\nof CIFAR-10 and down-scaled ImageNet.\nIn our experiments, we repeatedly go through active learning loops. One active\nlearning loop consists of training the model on the available labeled data and subse-\nquently acquiring new data points using a chosen acquisition function. As the labeled\ndataset is small in the beginning, it is important to avoid overfitting. We do this by\nusing early stopping after 3 epochs of declining accuracy on the validation set. We\npick the model with the highest validation accuracy. Throughout our experiments, we\nuse the Adam [Kingma and Ba, 2015] optimizer with learning rate 0.001 and betas\n0.9/0.999. All our results report the median of 6 trials, with lower and upper quartiles.\nWe use these quartiles to draw the filled error bars on our figures.\nWe reinitialize the model after each acquisition, similar to Gal et al. [2017]:\nempirically, we found this helps the model improve even when very small batches\n2K is the acquisition size, C is the number of classes, k is the number of MC dropout samples, and\nm is the number of sampled configurations of y1:n−1.\n4. Diverse Batch Acquisition for Bayesian Active Learning\n85\nFigure 4.7: Performance on MNIST. Batch-\nBALD outperforms BALD with acquisition\nsize 10 and performs close to the optimum of\nacquisition size 1.\nFigure 4.8: Relative total time on MNIST.\nNormalized to training BatchBALD with ac-\nquisition size 10 to 95% accuracy. The stars\nmark when 95% accuracy is reached for each\nmethod.\nare acquired. It also decorrelates subsequent acquisitions as final model performance\nis dependent on a particular initialization [Frankle and Carbin, 2019].\nWhen computing p(y | x,ωωω, Dtrain), it is important to keep the dropout masks\nin MC dropout consistent while sampling from the model.\nThis is necessary to\ncapture dependencies between the inputs for BatchBALD, and it makes the scores\nfor different points more comparable by removing this source of noise. We do not\nkeep the masks fixed when computing BALD scores because its performance usually\nbenefits from the added noise. We also do not need to keep these masks fixed for\ntraining and evaluating the model.\nIn all our experiments, we either compute joint entropies exactly by enumerating\nall configurations, or we estimate them using 10,000 MC samples, picking whichever\nmethod is faster. In practice, we compute joint entropies exactly for roughly the first 4\ndata points in an acquisition batch and use MC sampling thereafter.\n4.2.1\nRepeated-MNIST\nAs demonstrated in the introduction, naively applying BALD to a dataset that contains\nmany (near) replicated data points leads to poor performance. We show how this\nmanifests in practice by taking the MNIST dataset and replicating each data point\nin the training set twice (obtaining a training set that is three times larger than\nthe original). After normalizing the dataset, we add isotropic Gaussian noise with a\nstandard deviation of 0.1 to simulate slight differences between the duplicated data\npoints in the training set. All results are obtained using an acquisition size of 10 and\n10 MC dropout samples. The initial dataset was constructed by taking a balanced\nset of 20 data points3, two of each class, similar to [Gal et al., 2017].\nOur model consists of two blocks of [convolution, dropout, max-pooling, relu], with\n32 and 64 5x5 convolution filters. These blocks are followed by a two-layer MLP that\nincludes dropout between the layers and has 128 and 10 hidden units. The dropout\nprobability is 0.5 in all three locations. This architecture achieves 99% accuracy with\n3These initial data points were chosen by running BALD 6 times with the initial dataset picked\nrandomly and choosing the set of the median model. They were subsequently held fixed.\n4. Diverse Batch Acquisition for Bayesian Active Learning\n86\nTable 4.1: Number of required data points on MNIST until 90% and 95% accuracy are\nreached. 25%-, 50%- and 75%-quartiles for the number of required data points when available.\nAccuracy\n90%\n95%\nBatchBALD\n70 / 90 / 110\n190 / 200 / 230\nBALD 4\n120 / 120 / 170\n250 / 250 / >300\nBALD [Gal et al., 2017]\n145\n335\n10 MC dropout samples during test time on the full MNIST dataset.\nThe results can be seen in Figure 4.2. In this illustrative scenario, BALD performs\npoorly, and even randomly acquiring points performs better. However, BatchBALD\nis able to cope with the replication perfectly. In Appendix D.4, we look at varying\nthe repetition number and show that as we increase the number of repetitions BALD\ngradually performs worse. In Appendix D.5, we also compare with Variation Ratios\n[Freeman, 1965], and Mean STD [Kendall et al., 2017] which perform on par with\nrandom acquisition.\n4.2.2\nMNIST\nFor the second experiment, we follow the setup of Gal et al. [2017] and perform active\nlearning on the MNIST dataset using 100 MC dropout samples. We use the same model\narchitecture and initial dataset as described in section 4.2.1. Due to differences in\nmodel architecture, hyperparameters and model retraining, we significantly outperform\nthe original results in Gal et al. [2017] as shown in table 4.1.\nWe first look at BALD for increasing acquisition size in Figure 4.6(a). As we increase\nthe acquisition size from the ideal of acquiring points individually and fully retraining\nafter each point (acquisition size 1) to 40, there is a substantial performance drop.\nBatchBALD, in Figure 4.6(b), is able to maintain performance when doubling\nthe acquisition size from 5 to 10. Performance drops only slightly at 40, possibly\ndue to estimator noise.\nThe results for acquisition size 10 for both BALD and BatchBALD are compared in\nFigure 4.7. BatchBALD outperforms BALD. Indeed, BatchBALD with acquisition size\n10 performs close to the ideal with acquisition size 1. The total run time of training\nthese three models until 95% accuracy is visualized in Figure 4.8, where we see that\nBatchBALD with acquisition size 10 is much faster than BALD with acquisition size\n1, and only marginally slower than BALD with acquisition size 10.\n4.2.3\nEMNIST\nIn this experiment, we show that BatchBALD also provides a significant improvement\nwhen we consider the more difficult EMNIST dataset [Cohen et al., 2017] in the\nBalanced setup, which consists of 47 classes, comprising letters and digits.\nThe\ntraining set consists of 112,800 28x28 images balanced by class, of which the last\n18,800 images constitute the validation set. We do not use an initial dataset and\n4reimplementation using reported experimental setup\n4. Diverse Batch Acquisition for Bayesian Active Learning\n87\nFigure 4.9:\nPerformance on EMNIST.\nBatchBALD consistently outperforms both\nrandom acquisition and BALD while BALD\nis unable to beat random acquisition.\nFigure 4.10: Entropy of acquired class labels\nover acquisition steps on EMNIST. Batch-\nBALD steadily acquires a more diverse set\nof data points.\nFigure 4.11:\nHistogram of acquired class labels on EMNIST. BatchBALD left and\nBALD right. Classes are sorted by number of acquisitions. Several EMNIST classes are\nunderrepresented in BALD and random acquisition while BatchBALD acquires classes more\nuniformly. The histograms were created from all acquired points at the end of an active\nlearning loop\ninstead perform the initial acquisition step with the randomly initialized model and\nuse 10 MC dropout samples.\nWe use a similar model architecture as before, but with added capacity. Three\nblocks of [convolution, dropout, max-pooling, relu], with 32, 64 and 128 3x3 convolution\nfilters, and 2x2 max pooling. These blocks are followed by a two-layer MLP with\n512 and 47 hidden units, with again a dropout layer in between. We use dropout\nprobability 0.5 throughout the model.\nThe results for acquisition size 5 can be seen in Figure 4.9. BatchBALD outperforms\nboth random acquisition and BALD while BALD is unable to beat random acquisition.\nFigure 4.10 gives some insight into why BatchBALD performs better than BALD. The\nentropy of the categorical distribution of acquired class labels is consistently higher,\nmeaning that BatchBALD acquires a more diverse set of data points. In Figure 4.11,\nthe classes on the x-axis are sorted by number of data points that were acquired of that\nclass. We see that BALD undersamples classes while BatchBALD is more consistent.\n4.2.4\nCINIC-10\nCINIC-10 is an interesting dataset because it is large (270k data points) and its data\ncomes from two different sources: CIFAR-10 and ImageNet. To get strong performance\n4. Diverse Batch Acquisition for Bayesian Active Learning\n88\nFigure 4.12: Performance on CINIC-10. BatchBALD outperforms BALD from 500 acquired\nsamples onwards.\non the test set it is important to obtain data from both sets. Instead of training\na very deep model from scratch on a small dataset, we opt to run this experiment\nin a transfer learning setting, where we use a pretrained model and acquire data\nonly to fine-tune the original model. This is common practice and suitable in cases\nwhere there is plenty of data available in an auxiliary domain, but it is expensive\nto label data for the domain of interest.\nFor the CINIC-10 experiment, we use 160k training samples for the unlabeled\npool, 20k validation samples, and the other 90k as test samples. We use an ImageNet\npretrained VGG-16, provided by PyTorch [Paszke et al., 2017], with a dropout layer\nbefore a 512 hidden unit (instead of 4096) fully connected layer. We use 50 MC\ndropout samples, acquisition size 10 and repeat the experiment for 6 trials. The\nresults are in Figure 4.12, with the 59% mark reached at 1170 for BatchBALD\nand 1330 for BALD (median).\n4.3\nDiscussion\nWe have introduced a new batch acquisition function, BatchBALD, for Deep Bayesian\nActive Learning, and a greedy algorithm that selects good candidate batches compared\nto the intractable optimal solution. Acquisitions show increased diversity of data points\nand improved performance over BALD and other methods.\nWhile our method comes with additional computational cost during acquisition,\nBatchBALD is able to significantly reduce the number of data points that need to be\nlabeled and the number of times the model has to be retrained, potentially saving con-\nsiderable costs and filling an important gap in practical Deep Bayesian Active Learning.\nThis proposed method of batch acquisition has some weaknesses and limitations,\nwhich we discuss here.\n• Unbalanced Datasets. BALD and BatchBALD do not work well when the test\nset is unbalanced as they aim to learn well about all classes and do not follow\nthe density of the dataset. However, if the test set is balanced, but the training\nset is not, we expect BatchBALD to perform well.\n• Unlabeled Data. BatchBALD does not take into account any information\nfrom the unlabeled dataset. However, BatchBALD uses the underlying Bayesian\nmodel for estimating uncertainty for unlabeled data points, and semi-supervised\nlearning could improve these estimates by providing more information about the\n4. Diverse Batch Acquisition for Bayesian Active Learning\n89\nunderlying structure of the feature space. We leave a semi-supervised extension\nof BatchBALD to future work.\n• Noisy Estimator. A significant amount of noise is introduced by MC-dropout’s\nvariational approximation to training BNNs. Sampling of the joint entropies\nintroduces additional noise. The quality of larger acquisition batches would be\nimproved by reducing this noise. We examine this further in §5 and 6.\nI have no mind of my own or mouth to speak, yet I\nreply when you call. In valleys and halls, my voice\nmay be found, but only as an answer without its\nown sound. What am I?\n5\nStochastic Batch Acquisition for Deep\nActive Learning\nWhile many acquisition schemes are designed to acquire labels one at a time [Houlsby\net al., 2011; Gal et al., 2017], we have already highlighted the importance of batch\nacquisition in §1.2.4 and §4. Unfortunately, existing batch acquisition schemes are\ncomputationally expensive (Table 5.1). Intuitively, this is because batch acquisition\nschemes face combinatorial complexity when accounting for the interactions between\npossible acquisition points. Recent works [Ash et al., 2020, 2021] trade off a principled\nmotivation with various approximations to remain tractable. A commonly used, though\nextreme, heuristic is to take the top-K highest scoring points from an acquisition\nscheme designed to select a single point.\nThis chapter introduces a simple baseline for batch active learning that can be\ncompetitive with methods that cost orders of magnitude more across a wide range of\nexperimental contexts. The presented method is motivated by noticing that single-\nacquisition score methods such as BALD [Houlsby et al., 2011] act as a noisy proxy for\nfuture acquisition scores (Figure 5.1). This observation leads us to stochastically acquire\npoints following a distribution determined by the single-acquisition scores. Importantly,\nsuch a simple approach can match the prior state of the art for batch acquisition despite\nbeing very simple. Moreover, this acquisition scheme has a time complexity of only\nO(M log K) in the pool size M and acquisition size K, just like top-K acquisition.\nWe show empirically that the presented stochastic strategy performs as well as\nor better than top-K acquisition with almost identical computational cost on several\ncommonly used acquisition scores, making it a strictly-better batch strategy. Strikingly,\nthe empirical comparisons between this stochastic strategy and the evaluated more\ncomplex methods cast doubt on whether they function as well as claimed. Concretely,\nin this chapter, we:\n• examine a family of three computationally cheap stochastic batch acquisition\nstrategies;\n• demonstrate that these strategies are preferable to the commonly used top-K\nacquisition heuristic; and\n• identify the failure of existing SotA batch acquisition strategies to outperform\nthis vastly cheaper and more heuristic strategy.\nIn §5.1, we present active learning notation and commonly used acquisition functions.\nWe propose stochastic extensions in §5.2, relate them to previous work in §5.3, and\nvalidate them empirically in §5.4 on various datasets, showing that these extensions are\ncompetitive with some much more complex active learning approaches despite being\n5. Stochastic Batch Acquisition for Deep Active Learning\n91\nTable 5.1: Acquisition runtime (in seconds, 5 trials, ± s.d.). The examined stochastic\nacquisition methods are as fast as top-K, and orders of magnitude faster than BADGE or\nBatchBALD. Synthetic pool set with M = 10, 000 pool points with 10 classes. BatchBALD\nand BALD with 20 parameter samples.\nK\nTop-K\nStochastic\nBADGE\nBatchBALD\n10\n0.2 ± 0.0\n0.2 ± 0.0\n9.2 ± 0.3\n566.0 ± 17.4\n100\n0.2 ± 0.0\n0.2 ± 0.0\n82.1 ± 2.5\n5, 363.6 ± 95.4\n500\n0.2 ± 0.0\n0.2 ± 0.0\n409.3 ± 3.7\n29, 984.1 ± 598.7\norders of magnitude computationally cheaper. Finally, we validate the underlying\ntheoretical motivation in §5.5 and discuss limitations in §5.6.\n5.1\nProblem Setting\nThe stochastic approach we examine applies to batch acquisition for active learning in\na pool-based setting [Settles, 2010] where we have access to a large unlabeled pool set,\nbut we can only label a small subset of the points. The challenge of active learning is\nto use what we already know to pick which points to label in the most efficient way,\nand generally, we want to avoid labelling points similar to those already labeled.\nNotation. Following Farquhar et al. [2021], and unlike in the rest of the thesis, we\nformulate active learning over indices instead over data points. This simplifies the\nnotation. The large, initially fully unlabeled, pool set containing M input points is\nDpool = {xi}i∈Ipool,\n(5.1)\nwhere Ipool = {1, . . . , M} is the initial full index set. We initialize a training dataset\nwith N0 randomly selected points from Dpool by acquiring their labels, yi,\nDtrain = {(xi, yi)}i∈Itrain,\n(5.2)\nwhere Itrain is the index set of Dtrain, initially containing N0 indices between 1 and M.\nA model of the predictive distribution, p(y | x), can then be trained on Dtrain.\nActive Learning. At each acquisition step, we select additional points for which to\nacquire labels. Although many methods acquire one point at a time [Houlsby et al.,\n2011; Gal et al., 2017], one can alternatively acquire a whole batch of K examples.\nAn acquisition function a takes Itrain and Ipool and returns K indices from Ipool to\nbe added to Itrain. We then label those K data points and add them to Itrain while\nmaking them unavailable from the pool set. That is,\nItrain ←Itrain ∪a(Itrain, Ipool),\n(5.3)\nIpool ←Ipool \\ Itrain.\n(5.4)\nA common way to construct the acquisition function is to define some scoring function,\ns, and then select the point(s) that score the highest.\nWe consider the following scoring functions:\n5. Stochastic Batch Acquisition for Deep Active Learning\n92\nBALD. For each candidate pool index, i, the BALD score is\nsBALD(i; Itrain) ≜I[Y ; Ω| X = xi, Dtrain]\n= H[Y | X = xi, Dtrain] −Ep(ω|Dtrain)[H[Y | X = xi, ω, Dtrain]]. (5.5)\nEntropy. Entropy does not require Bayesian models, unlike BALD, and performs\nworse for data with high observation noise as we have noted in §3. It is identical to\nthe first term of the BALD score\nsentropy(i; Itrain) ≜H[Y | X = xi, Dtrain].\n(5.6)\nSee §1.2.4 for other acquisition functions.\nAcquisition Functions. These scoring functions were introduced for single-point ac-\nquisition:\nas(Itrain) ≜arg max\ni∈Ipool s(i; Itrain).\n(5.7)\nFor deep learning in particular, single-point acquisition is computationally expensive due\nto retraining the model for every acquired sample. Moreover, it also means that labelling\ncan only happen sequentially instead of in bulk. Thus, single-point acquisition functions\nwere expanded to multi-point acquisition via acquisition batches in batch active learning.\nThe most naive batch acquisition function selects the highest K scoring points\nabatch\ns\n(Itrain; K) ≜arg max\nI⊆Ipool,|I|=K\nX\ni∈I\ns(i; Itrain).\n(5.8)\nMaximizing this sum is equivalent to taking the top-K scoring points, which cannot\naccount for the interactions between points in an acquisition batch because individual\npoints are scored independently. Some acquisition functions are explicitly designed for\nbatch acquisition, e.g. BatchBALD from §4 or BADGE from Ash et al. [2020]. They\ntry to account for the interaction between points, which can improve performance\nrelative to simply selecting the top-K scoring points. However, existing methods are\ncomputationally expensive. For example, BatchBALD rarely scales to acquisition sizes\nof more than 5–10 points as noted in §4; see Table 5.1.\nBADGE. Ash et al. [2020] propose Batch Active learning by Diverse Gradient\nEmbeddings: it motivates its batch selection approach using a k-Determinantal Point\nProcess [Kulesza and Taskar, 2011] based on the (inner product) similarity matrix of\nthe scores (gradients of the log loss) using hard pseudo-labels (the highest probability\nclass according to the model’s prediction) for each pool sample. In §9 we provide a\nmore detailed analysis. In practice, they use the initialization step of k-MEANS++\nwith Euclidian distances between the scores to select an acquisition batch. BADGE\nis also computationally expensive.\n5.2\nMethod\nWe observe that selecting the top-K points at acquisition step t amounts to the\nassumption that the informativeness of these points is independent of each other.\nImagine adding the top-K points at a given acquisition step t to the training set one\n5. Stochastic Batch Acquisition for Deep Active Learning\n93\nTable 5.2: Summary of stochastic acquisition variants. Perturbing the scores si themselves\nwith ϵi ∼Gumbel(0; β−1) i.i.d. yields a softmax distribution. Log-scores result in a power\ndistribution, with assumptions that are reasonable for active learning. Using the score-ranking,\nri finally is a robustifying assumption. β is included for completeness; we use β ≜1 in our\nexperiments—except for the ablation in §5.5.1.\nPerturbation\nDistribution\nProbability mass\nsi + ϵi\nSoftmax\n∝exp βsi\nlog si + ϵi\nPower\n∝sβ\ni\n−log ri + ϵi\nSoft-rank\n∝r−β\ni\n0\n50\n100\n150\n200\nAcquisition Step n\n0.5\n1.0\nRank Correlation s0:sn\nFigure 5.1: Early acquisition scores are only\na loose proxy for later scores. Specifically, the\nSpearman rank-correlation between acquisi-\ntion scores on the first and n’th time-step falls\nwith n. While top-K acquisition incorrectly im-\nplicitly assumes the rank-correlation remains\n1, stochastic acquisitions do not. BNN trained\non MNIST at initial 20 points and 73% initial\naccuracy, score ranks over test set.\n0.6\n0.7\n0.8\n0.9\nAccuracy\n50\n100\n150\n200\n250\n300\nMin Training Set Size\nPowerBALD\nSoftmaxBALD\nSoftrankBALD\nBatchBALD 5\nBADGE 20\nUniform\nBALD\nFigure\n5.2:\nPerformance on Repeated-\nMNIST with 4 repetitions (5 trials). Up and\nto the right is better (↗). PowerBALD\noutperforms (top-K) BALD and BADGE and\nis on par with BatchBALD. This is despite\nbeing orders of magnitude faster. Acquisition\nsizes: BatchBALD–5, BADGE–20, others–10.\nSee Figure E.2 in the appendix for an ablation\nstudy of BADGE’s acquisition size.\nat a time. Each time, you retrain the model. Of course, the acquisition scores for the\nmodels trained with these additional points will be different from the first set of scores.\nAfter all, the purpose of active learning is to add the most informative points: those\nthat will update the model the most. Yet selecting a top-K batch in one step implicitly\nassumes that the score ranking will not change due to these points. This is clearly\nwrong. We provide empirical confirmation that, in fact, the ranking of acquisition\nscores at step t and t+K is decreasingly correlated as K grows; see Figure 5.1. Moreover,\nthis effect is the strongest for the most informative points; see §5.5 for more details.\nInstead, this chapter uses stochastic sampling to acknowledge the uncertainty\nwithin the batch acquisition step using a simple noise process model governing how\nscores change. We examine three simple stochastic extensions of single-sample scoring\nfunctions s(i; Itrain) that make slightly different assumptions. These methods are\ncompatible with conventional active learning frameworks that typically take the top-K\nhighest scoring samples. For example, it is straightforward to adapt entropy, BALD,\nand other scoring functions for use with these extensions.\n5. Stochastic Batch Acquisition for Deep Active Learning\n94\nThese stochastic acquisition distributions assume that future scores differ from the\ncurrent score by a perturbation. We model the noise distribution of this perturbation\nas the addition of Gumbel-distributed noise ϵi ∼Gumbel(0; 1), which is used frequently\nfor modelling extrema. The choice of a Gumbel distribution for the noise is one of\nmathematical convenience, in the spirit of providing a simple baseline. For example,\nthe maximum of sets of many other standard distributions, such as the Gaussian\ndistribution, is not analytically tractable.\nTaking the highest-scoring points from this perturbed distribution is equivalent to\nsampling from a softmax distribution1 without replacement with a ‘coldness’ parameter\nβ ≥0, which represents the expected rate at which the scores change as more data is\nacquired. This follows from the Gumbel-Max trick [Gumbel, 1954; Maddison et al.,\n2014] and, more specifically, the Gumbel-Top-K trick [Kool et al., 2019]. We provide\na short proof in appendix E.1. Expanding on Maddison et al. [2014]:\nProposition 5.1. For scores si, i ∈{1, . . . , n}, and k ≤n and β > 0, if we draw\nϵi ∼Gumbel(0; β−1) independently, then arg topk{si + ϵi}i is an (ordered) sample\nwithout replacement from the categorical distribution Categorical(exp(β si)/P\nj exp(β sj), i ∈\n{1, . . . , n}).\nIn the spirit of providing a simple and surprisingly effective baseline without\nhyperparameters, we fix β ≜1. For β →∞, this distribution will converge towards\ntop-K acquisition. Whereas for β →0, it will converge towards uniform acquisition.\nWe examine ablations of β in §5.5.1.\nWe apply the perturbation to three quantities in the three sampling schemes: the\nscores themselves, the log scores, and the rank of the scores. Perturbing the log scores\nassumes that scores are non-negative and uninformative points should be avoided.\nPerturbing the ranks can be seen as a robustifying assumption that requires the relative\nscores to be reliable but allows the absolute scores to be unreliable. We summarize the\nthree versions with their associated sampling distributions are in Table 5.2.\nSoft-Rank Acquisition. This first variant only relies on the rank order of the scores\nand makes no assumptions on whether the acquisition scores are meaningful beyond\nthat. It thus uses the least amount of information from the acquisition scores. It only\nrequires the relative score order to be useful and ignores the absolute score values.\nIf the absolute scores provide useful information, we would expect this method to\nperform worse than the variants below, which make use of the score values. As we\nwill see, this is indeed sometimes the case.\nRanking the scores s(i; Itrain) with descending ranks {ri}i∈Ipool such that s(ri; Itrain) ≥\ns(rj; Itrain) for ri ≤rj and smallest rank being 1, we sample index i with probability\npsoftrank(i) ∝r−β\ni\nwith coldness β. This is invariant to the actual scores. We can draw\nϵi ∼Gumbel(0; β−1) and create a perturbed ‘rank’\nssoftrank(i; Itrain) := −log ri + ϵi.\n(5.9)\nTaking the top-K samples is now equivalent to sampling without replacement from\nthe rank distribution psoftrank(i).\n1Also known as Boltzmann/Gibbs distribution.\n5. Stochastic Batch Acquisition for Deep Active Learning\n95\nSoftmax Acquisition. The next simplest variant uses the actual scores instead of\nthe ranks. Again, it perturbs the scores by a Gumbel-distributed random variable\nϵi ∼Gumbel(0; β−1)\nssoftmax(i; Itrain) := s(i; Itrain) + ϵi.\n(5.10)\nHowever, this makes no assumptions about the semantics of the absolute values of\nthe scores: the softmax function is invariant to constants shifts. Hence, the sampling\ndistribution will only depend on the relative scores and not their absolute value.\nPower Acquisition. For many scoring functions, the scores are non-negative, and a\nscore close to zero means that the sample is not informative in the sense that we do\nnot expect it will improve the model—we do not want to sample it. This is the case\nwith commonly used score functions such as BALD and entropy. BALD measures the\nexpected information gain. When it is zero for a sample, we do not expect anything to\nbe gained from acquiring a label for that sample. Similarly, entropy is upper-bounding\nBALD, and the same consideration applies. This assumption also holds ideally for\nother scoring functions that are easily transformed to be non-negative (see §1.2.4). To\ntake this into account, the last variant models the future log scores as perturbations\nof the current log score with Gumbel-distributed noise\nspower(i; Itrain) := log s(i; Itrain) + ϵi.\n(5.11)\nBy Proposition 5.1, this is equivalent to sampling from a power distribution\nppower(i) ∝\n \n1\ns(i; Itrain)\n!−β\n.\n(5.12)\nThis may be seen by noting that exp(β log s(i; Itrain)) = s(i; Itrain)β. Importantly,\nas scores →0, the (perturbed) log scores →−∞and will have probability mass\n→0 assigned. This variant takes the absolute scores into account and avoids data\npoints with score 0.\nSummary. Given the above considerations, when using BALD, entropy, and other\nappropriate scoring functions, power acquisition is the most sensible. Thus, we expect\nit to work best. Indeed, we find this to be the case in the toy experiment on Repeated-\nMNIST (see §4) depicted in Figure 5.2. However, even soft-rank acquisition works\nwell in practice, suggesting that the choice of score perturbation is not critical for its\neffectiveness; see also appendix §E.3 for a more in-depth comparison. In the rest of\nthis chapter, we focus on power acquisition—we include results for all methods in §E.2.\n5.3\nRelated Work\nIn most cases, the computational complexity scales poorly with the acquisition size\n(K) or pool size (M), for example because of the estimation of joint mutual information\n(§4), the O(KM) complexity of using a k-means++ initialization scheme [Ash et al.,\n2020], which approximates k-DPP-based batch active learning [Bıyık et al., 2019], or\nthe O(M2 log M) complexity of methods based on K-center coresets [Sener and Savarese,\n2018] (although heuristics and continuous relaxations can improve this somewhat). In\ncontrast, we examine simple and efficient stochastic strategies for adapting well-known\n5. Stochastic Batch Acquisition for Deep Active Learning\n96\nsingle-sample acquisition functions to the batch setting. The proposed stochastic\nstrategies are based on observing that acquisition scores would change as new points\nare added to the acquisition batch and modelling this difference for additional batch\nsamples in the most naive way, using Gumbel noise. The presented stochastic extensions\nhave the same complexity O(M log K) as naive top-K batch acquisition, yet outperform\nit, and they can perform on par with above more complex methods.\nFor multi-armed bandits, it has been shown that Thompson sampling from the poste-\nrior is effective for choosing informative batches [Kalkanli and Özgür, 2021]. Compared\nto using the Bayesian model average of the posterior, this can be seen as noising the\nBMA acquisition scores. Similarly, in reinforcement learning, stochastic prioritization\nhas been employed as prioritized replay [Schaul et al., 2016] which may be effective for\nreasons analogous to those motivating the approach examined in this chapter.\nWhile stochastic sampling has not been extensively explored for acquisition in deep\nactive learning, most recently it has been used as an auxiliary step in diversity-based\nactive learning methods that rely on clustering as main mechanism [Ash et al., 2020;\nCitovsky et al., 2021]. In §4 we noted that additional noise in the acquisition scores\nseems to benefit top-K batch acquisition in our experiments but did not investigate\nfurther. Fredlund et al. [2010] suggest modeling single-point acquisition as sampling\nfrom a “query density” modulated by the (unknown) sample density p(x) and analyze a\nbinary classification toy problem. They model the query density using a parameterized\nmodel. Farquhar et al. [2021] propose stochastic acquisition as part of debiasing\nactively learned estimators.\nMost relevant to this chapter, and building on Fredlund et al. [2010]; Farquhar et al.\n[2021], Zhan et al. [2022b] propose a stochastic acquisition scheme that is asymptotically\noptimal. They normalize the acquisition scores via the softmax function to obtain a\nquery density function for unlabeled samples and draw an acquisition batch from it,\nsimilar to SoftmaxEntropy. Their method aims to achieve asymptotic optimality for\nactive learning processes by mitigating the impact of bias. However, in this chapter, we\npropose multiple stochastic acquisition strategies based on score-based or rank-based\ndistributions and apply these strategies to several single-sample acquisition functions,\nsuch as BALD and entropy (and standard deviation, variation ratios, see Figure E.3).\nWe focus on active learning in a (Bayesian) deep learning setting and not in a classical\nmachine learning setting. As such the empirical results and other proposed strategies\ncan be seen as complementary to their work.\nThus, while stochastic sampling is generally well-known within acquisition functions,\nentirely simple stochastic sampling have not been investigated as alternatives to naive\ntop-K acquisition in (Bayesian) deep active learning and compared to more complex\napproaches in various settings.\n5.4\nEmpirical Validation\nIn this section, we empirically verify that the presented stochastic acquisition methods\n(a) outperform top-K acquisition and (b) are competitive with specially designed batch\nacquisition schemes like BADGE [Ash et al., 2020] and BatchBALD (§4); and are\nvastly cheaper than these more complicated methods.\nTo demonstrate the seriousness of the possible weakness of recent batch acquisition\nmethods, we use a range of datasets. These experiments show that the performance of\nthe stochastic extensions is not dependent on the specific characteristics of any particular\n5. Stochastic Batch Acquisition for Deep Active Learning\n97\ndataset. Our experiments include computer vision, natural language processing (NLP),\nand causal inference (in §5.5.1). We show that stochastic acquisition helps avoid\nselecting redundant samples on Repeated-MNIST (§4), examine performance in active\nlearning for computer vision on EMNIST [Cohen et al., 2017], MIO-TCD [Luo et al.,\n2018], Synbols [Lacoste et al., 2020], and CLINC-150 [Larson et al., 2019] for intent\nclassification in NLP. MIO-TCD is especially close to real-world datasets in size and\nquality. In appendix E.2.5, we further investigate edges cases using the Synbols dataset\nunder different types of biases and aleatoric uncertainty.\nHere, we consider both BALD and predictive entropy as scoring functions. We\nexamine other scoring functions on Repeated-MNIST in appendix E.2.2.1 and observe\nsimilar results. For the sake of legible figures, we focus on power acquisition in this\nsection, as it fits BALD and entropy best: the scores are non-negative, and zero\nscores imply uninformative samples. We show that all three methods (power, softmax,\nsoftrank) perform similarly in appendix E.3.\nWe are not always able to compare to BADGE and BatchBALD because of compu-\ntational limitations of those methods. BatchBALD is computationally infeasible for\nlarge acquisition sizes (> 10) because of time constraints, cf. Table 5.1. When possible,\nwe use BatchBALD with acquisition size 5 as baseline. Similarly, BADGE runs out of\nmemory for large dataset sizes, such as EMNIST ‘ByMerge’ with 814,255 examples.\nFigures interpolate linearly between available points, and we show 95% confidence in-\ntervals.\nExperimental Setup & Compute. We document the experimental setup and\nmodel architectures in detail in appendix E.2.1. Our experiments used about 25,000\ncompute hours on Titan RTX GPUs.\nRuntime Measurements. We emphasize that the stochastic acquisition strategies\nare much more computationally efficient compared to specialized batch-acquisition\napproaches like BADGE and BatchBALD. Runtimes, shown in Table 5.1, are essentially\nidentical for top-K and the stochastic versions. Both are orders of magnitude faster than\nBADGE and BatchBALD even for small batches. Unlike those methods, stochastic\nacquisition scales linearly in pool size and logarithmically in acquisition size. Runtime\nnumbers do not include the cost of retraining models (identical in each case). The\nruntimes for top-K and stochastic acquisition appear constant over K because the\nexecution time is dominated by fixed-cost memory operations. The synthetic dataset\nused for benchmarking has 4,096 features, 10 classes, and 10,000 pool points.\nRepeated-MNIST. Repeated-MNIST (§4) duplicates MNIST a specified number\nof times and adds Gaussian noise to prevent perfect duplicates. Redundant data are\nincredibly common in industrial applications but are usually removed from standard\nbenchmark datasets. The controlled redundancies in the dataset allow us to showcase\npathologies in batch acquisition methods. We use an acquisition size of 10 and 4\ndataset repetitions.\nFigure 5.2 shows that PowerBALD outperforms top-K BALD. While much cheaper\ncomputationally, cf.\nTable 5.1, PowerBALD also outperforms BADGE and even\nperforms on par with BatchBALD. For BatchBALD, we use an acquisition size of 5,\nand for BADGE of 20. Note that BatchBALD performs better for smaller acquisition\nsizes while BADGE (counterintuitively) performs better for larger ones; see Figure E.2\nin the appendix for an ablation. BatchBALD, BALD, and the stochastic variants all\n5. Stochastic Batch Acquisition for Deep Active Learning\n98\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nAccuracy\n0\n50\n100\n150\n200\n250\n300\nMin Training Set Size\nPowerBALD\nBatchBALD 5\nBADGE\nBALD\n(a) EMNIST (Balanced) (5 trials)\n0.0\n0.2\n0.4\n0.6\nAccuracy\n0\n100\n200\n300\nMin Training Set Size\nPowerBALD\nBatchBALD 5\nBALD\n(b) EMNIST (ByMerge) (5 trials)\n0.96\n0.97\n0.98\n0.99\nAccuracy\n0\n5000\n10000\n15000\nMin Training Set Size\nPowerBALD\nBADGE\nBALD\nUniform\n(c) MIO-TCD (3 trials)\n0.80\n0.85\n0.90\n0.95\nAccuracy\n0\n5000\n10000\n15000\nMin Training Set Size\nPowerBALD\nBADGE\nBALD\nUniform\n(d) Synbols with minority groups (10 trials)\nFigure 5.3: Performance on various datasets. BatchBALD took infeasibly long on these\ndatasets & acquisition sizes. ((a)) EMNIST ‘Balanced’: On 132k samples, PowerBALD (acq.\nsize 10) outperforms BatchBALD (acq. size 5) and BADGE (acq. size 40). ((b)) EMNIST\n‘ByMerge’: On 814k samples, PowerBALD (acq. size 10) outperforms BatchBALD (acq. size\n5). BADGE (not shown) OOM’ed, and BatchBALD took > 12 days for 115 acquisitions.\n((c)) MIO-TCD: PowerBALD performs better than BALD and on par with BADGE (all acq.\nsize 100). ((d)) Synbols with minority groups: PowerBALD performs on par with BADGE\n(all acq. size 100).\nbecome equivalent for acquisition size 1 when individual points are sampled, which\nperforms best (§4).\nComputer Vision: EMNIST. EMNIST [Cohen et al., 2017] contains handwritten\ndigits and letters and comes with several splits: we examine the ‘Balanced‘ split with\n131,600 samples in Figure 5.3(a)2 and the ‘ByMerge‘ split with 814,255 samples in\nFigure 5.3(b). Both have 47 classes. We use an acquisition size of 5 for BatchBALD,\nof 40 for BADGE, and of 10 otherwise.\nWe see that the stochastic methods outperform BatchBALD on it and both BADGE\nand BatchBALD on ‘Balanced’ (Figure 5.3(a)). They do not have any issues with the\nhuge pool set in ‘ByMerge‘ (Figure 5.3(b)). In the appendix, Figures E.16 and E.17\nshow results for all three stochastic extensions, and Figure E.8 shows an ablation\nof different acquisition batch sizes for BADGE. For ‘ByMerge’, BADGE ran out of\nmemory on our machines, and BatchBALD took more than 12 days for 115 acquisitions\nwhen we halted execution.\n2This result exactly reproduces BatchBALD’s trajectory in Figure 7 from §4.\n5. Stochastic Batch Acquisition for Deep Active Learning\n99\nComputer Vision:\nMIO-TCD. The Miovision Traffic Camera Dataset (MIO-\nTCD) [Luo et al., 2018] is a vehicle classification and localization dataset with 648,959\nimages designed to exhibit realistic data characteristics like class imbalance, duplicate\ndata, compression artifacts, varying resolution (between 100 and 2,000 pixels), and\nuninformative examples; see Figure E.1 in the appendix. As depicted in Figure 5.3(c),\nPowerBALD performs better than BALD and essentially matches BADGE despite\nbeing much cheaper to compute. We use an acquisition size of 100 for all methods.\nComputer Vision: Synbols. Synbols [Lacoste et al., 2020] is a character dataset\ngenerator which can demonstrate the behavior of batch active learning under various\nedge cases [Lacoste et al., 2020; Branchaud-Charron et al., 2021]. In Figure 5.3(d),\nwe evaluate PowerBALD on a dataset with minority character types and colors.\nPowerBALD outperforms BALD and matches BADGE. Further details as well as\nan examination of the ‘spurious correlation’ and ‘missing symbols’ edge cases [Lacoste\net al., 2020; Branchaud-Charron et al., 2021] can be found in appendix E.2.5.\nNatural Language Processing: CLINC-150. We perform intent classification\non CLINC-150 [Larson et al., 2019], which contains 150 intent classes plus an out-\nof-scope class.\nThis setting captures data seen in production for chatbots.\nWe\nfine-tune a pretrained DistilBERT model from HuggingFace [Dosovitskiy et al., 2020]\non CLINC-150 for 5 epochs with Adam as optimizer. In appendix E.2.6, we see that\nPowerEntropy shows strong performance. This demonstrates that our technique is\ndomain independent and can be easily reused for other tasks.\nSummary. We have verified that stochastic acquisition functions outperform top-K\nbatch acquisition in several settings and perform on par with more complex methods\nsuch as BADGE or BatchBALD. Moreover, we refer the reader to Jesson et al. [2021],\nMurray et al. [2021], Tigas et al. [2022], Holmes et al. [2022] for additional works\nthat use the proposed stochastic acquisition functions from this chapter and provide\nfurther empirical validation.\n5.5\nFurther Investigations\nIn this section, we validate our assumptions about the underlying score dynamics by\nexamining the score rank correlations across acquisitions. We further hypothesize about\nwhen top-K acquisition is the most detrimental to active learning.\nRank Correlations Across Acquisitions. Our method is based on assuming: (1) the\nacquisition scores st at step t are a proxy for scores st′ at step t′ > t; (2) the larger t′−t is,\nthe worse a proxy st is for s′\nt; (3) this effect is the largest for the most informative points.\nWe demonstrate these empirically by examining the Spearman rank correlation\nbetween scores during acquisition. Specifically, we train a model for n steps using\nBALD as single-point acquisition function. We compare the rank order at each step\nto the starting rank order at step t.\nFigure 5.1 shows that acquisition scores become less correlated as more points are\nacquired. Figure 5.4(a) shows this in more detail for the top and bottom 1%, 10% or\n100% of scorers of the test set across acquisitions starting at step t = 0 for a model\ninitialized with 20 points. The top-10% scoring points (solid green) quickly become\nuncorrelated across acquisitions and even become anti-correlated. In contrast, the\npoints overall (solid blue) correlate well over time (although they have a much weaker\n5. Stochastic Batch Acquisition for Deep Active Learning\n100\n0\n25\n50\n75\n100\nAcquisition Delta Step n\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nRank Correlation st:st + n\nTop\nBottom\n \n1%\n10%\n100%\n(a) t = 0\n0\n25\n50\n75\n100\nAcquisition Delta Step n\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nRank Correlation st:st + n\n(b) t = 100\nFigure 5.4: Rank correlations for BALD scores on MNIST\nbetween the initial scores and future scores of the top- or\nbottom-scoring 1%, 10% and 100% of test points (smoothed\nwith a size-10 Parzen window). The rank order decorrelates\nfaster for the most informative samples and in the early stages\nof training. The top-1% scorers’ ranks anti-correlate after\nroughly 40 (100) acquisitions unlike the bottom-1%. Later in\ntraining, the acquisition scores stay more strongly correlated.\nThis suggests the acquisition size could be increased later in\ntraining.\n0\n50\n100\n150\n200\nTraining Set Size\n0.75\n0.80\n0.85\n0.90\n0.95\nAccuracy\nFigure 5.5:\nTop-K acquisi-\ntion hurts less later in train-\ning (BALD on MNIST). At\nt ∈{20, 100} (blue), we keep\nacquiring samples using the\nBALD scores from those two\nsteps. At t = 20 (orange), the\nmodel performs well for ≈20\nacquisitions; at t = 120 (green),\nfor ≈50; see §5.5.\ntraining signal on average). This result supports all three of our hypotheses.\nAt the same time, we see that as training progresses, and we converge towards the\nbest model, the order of scores becomes more stable across acquisitions. In Figure 5.4(b)\nthe model begins with 120 points (t = 100), rather than 20 (t = 0). Here, the most\ninformative points are less likely to change their rank—even the top-1% ranks do\nnot become anti-correlated, only uncorrelated. Thus, we hypothesize that further in\ntraining, we might be able to choose larger K.\nIncreasing Top-K Analysis. Another way to investigate the effect of top-K selection\nis to freeze the acquisition scores during training and then continue single-point ‘active\nlearning’ as if those were the correct scores. Comparing this to the performance of\nregular active learning with updated single-point scores allows us to examine how well\nearlier scores perform as proxies for later scores. We perform this toy experiment\non MNIST, showing that freezing scores early on greatly harms performance while\ndoing it later has only a small effect (Figure 5.5). For frozen scores at a training set\nsize of 20 (73% accuracy, t = 0), the accuracy matches single-acquisition BALD up\nto a training set size of roughly 40 (dashed orange lines) before diverging to a lower\nlevel. But when freezing the scores of a more accurate model, at a training set size of\n120 labels (93% accuracy, t = 100), selecting the next fifty points according to those\nfrozen scores performs indistinguishably from step-by-step acquisition (dashed green\nlines). This result shows that top-K acquisition hurts less later in training but can\nnegatively affect performance at the beginning of training.\nThese observations lead us to ask whether we could dynamically change the\nacquisition size: with smaller acquisition batches at the beginning and larger ones\ntowards the end of active learning. We leave the exploration of this for future work.\n5.5.1\nAblation:\nChanging β\nSo far, we have set β = 1 in the spirit of providing a simple baseline without additional\nhyperparameters. The results above show that this already works well and matches\n5. Stochastic Batch Acquisition for Deep Active Learning\n101\n0.80\n0.85\n0.90\n0.95\nAccuracy\n100\n200\n300\nMin Training Set Size\nPowerBALD \n= 8\nBatchBALD 5\nPowerBALD\n(a) Repeated-MNISTx4\n100\n200\n300\n400\nTraining Set Size\n1.0\n1.5\n2.0\n2.5\nPEHE\n= 4\nTop-K\n= 1\nUniform\n(b) IHDP\nFigure 5.6:\nEffect of changing β.\n((a)) Repeated-MNISTx4 (5 trials): PowerBALD\noutperforms BatchBALD for β = 8. ((b)) IHDP (400 trials): At high temperature (β = 0.1),\nCausalBALD with power acquisition is like random acquisition. As the temperature decreases,\nthe performance improves (lower √ϵPEHE), surpassing top-K acquisition.\nthe performance of much more expensive methods, raising questions about their value.\nIn addition, however, tuning β may be able to further improve performance. Next, we\nshow that other values of β can yield even higher performance on Repeated-MNIST and\nwhen estimating causal treatment effects; we provide additional results in appendix E.4.\nRepeated-MNIST. In Figure 5.6(a), we see that for PowerBALD the best-performing\nvalue, β = 8, outperforms BatchBALD.\nCausal Treatment Effects: Infant Health Development Programme. Active\nlearning for Conditional Average Treatment Effect (CATE) estimation Heckman et al.\n[1997, 1998]; Hahn [1998]; Abrevaya et al. [2015] on data from the Infant Health and\nDevelopment Program (IHDP) estimates the causal effect of treatments on an infant’s\nhealth from observational data. Statistical estimands of the CATE are obtainable from\nobservational data under certain assumptions. Jesson et al. [2021] show how to use\nactive learning to acquire data for label-efficient estimation. Among other subtleties,\nthis prioritizes the data for which matched treated/untreated pairs are available.\nWe follow the experiments of Jesson et al. [2021] on both synthetic data and the\nsemisynthetic IHDP dataset [Hill, 2011], a commonly used benchmark for causal effects\nestimation. In Figure 5.6(b) we show that power acquisition performs significantly\nbetter than both top-K and uniform acquisition, using an acquisition size of 10 in all\ncases with further. We provide additional results on semisynthetic data in appendix\nE.4.2. Note that methods such as BADGE and BatchBALD are not well-defined for\ncausal-effect estimation, while our approach remains applicable and is effective when\nfine-tuning β: BatchBALD and BADGE are specifically designed for active learning\ngiven (classification) predictions, which is not the same as estimating causal effects.\nPerformance on these tasks is measured using the expected Precision in Estimation\nof Heterogeneous Effect (PEHE) [Hill, 2011] such that √ϵPEHE =\nq\nE[(eτ(X) −τ(X))2]\n[Shalit et al., 2017] where eτ is the estimated CATE and τ is CATE (i.e. a form of RMSE).\nLimitations. Although we highlight the possibility for future work to adapt β to\nspecific datasets or score functions, our aim is not to offer a practical recipe for this\nto practitioners. Our focus is on showing how even the simplest form of stochastic\nacquisition already raises questions for some recent more complex methods.\n5. Stochastic Batch Acquisition for Deep Active Learning\n102\n5.6\nDiscussion\nWe have demonstrated a surprisingly effective and efficient baseline for batch acquisition\nin active learning. The presented stochastic extensions are orders of magnitude faster\nthan sophisticated batch-acquisition strategies like BADGE and BatchBALD while\nretaining comparable performance in many settings. Compared to the flawed top-K\nbatch acquisition heuristic, it is never worse: we see no reason to continue using\ntop-K acquisition.\nImportantly, this chapter raises serious questions about these current methods.\nIf they fail to outperform such a simple baseline in a wide range of settings, do\nthey model the interaction between points sufficiently well? If so, are the scores\nthemselves unreliable?\nAt the same time, this presented framework opens doors for improved methods.\nAlthough our stochastic model is put forward for its computational and mathematical\nsimplicity, future work could explore more sophisticated modelling of the predicted\nscore changes that take the current model and dataset into account. In its simplest\nform, this might mean adapting the temperature of the acquisition distribution to the\ndataset or estimating it online. Our experiments also highlight that the acquisition\nsize could be dynamic, with larger batch sizes acceptable later in training.\nThe whole universe is contained in a single flower.\nToshiro Kawase\n6\nMarginal and Joint Cross-Entropies &\nPredictives\nBeyond deep ensembles [Lakshminarayanan et al., 2017], more principled methods of\ndeep learning that attempt to be (approximately) Bayesian, commonly referred to as\n(approximate) Bayesian Neural Networks (BNN), have arguably not lived up to their\nfull potential [Ovadia et al., 2019; Beluch et al., 2018]. This might be because the\nfocus in their evaluation has been on marginal predictions q(y | x), where they can\nonly provide marginal:D improvements over unprincipled regular NNs.\nYet the strength of a Bayesian approach for deep learning might not solely lie in\nmarginal predictions but in joint predictions and in allowing for online learning via\nonline Bayesian inference. Online Bayesian inference (OBI) refers to incorporating\nadditional data into the posterior predictive without explicitly retraining in the common\nsense, i.e. by computing gradients and optimizing the model parameters further1. This\ncould offer important performance benefits for applications that would otherwise\nrequire repeated retraining like active learning and could have important implications\nfor how we could use large supervised models in production: currently, they are seen\nas strictly static; however, online Bayesian inference would allow them to dynamically\nadapt to new data on the fly.\nGenerally, the difference between an approximate BNN and a regular NN is that\nthe former assumes a distribution q(ω) over the model parameters ω, where q(ω)\napproximates the Bayesian posterior p(ω | Dtrain), which is the optimal distribution\ngiven prior information p(ω) and training data Dtrain: q(ω) ≈p(ω | Dtrain).\nOnline Bayesian Inference. To incorporate new data {yi, xi} ∼ˆptrue(y, x)n, via\nonline Bayesian inference, we simply apply Bayes’ theorem: for a test point x, the\npredictive q(y | x, yn, xn, . . . , y1, x1) is proportional to its joint predictive. We obtain:\nq(y | x, yn, xn, . . . , y1, x1) = q(y, yn, . . . , y1 | x, xn, . . . , x1)\nq(yn . . . , y1 | xn, . . . , x1)\n(6.1)\n∝q(y, yn, . . . , y1 | x, xn, . . . , x1).\n(6.2)\nWe can thus use a joint predictive q(y, y1, . . . , yn |x, x1, . . . , xn) to incorporate fixed{yi,\nxi}n and make predictions for x without explicit retraining.\n1Maddox et al. [2021] have referred to this approach as ‘Online Variational Conditioning’ in the\ncontext of Gaussian processes. However, we would argue that in the context of this chapter OBI is the\nbetter term because there is nothing variational about it. Indeed, OBI applies Bayesian inference to\nan ‘imperfect’ variational intermediate posterior instead of performing Bayesian inference end-to-end\nfrom the original prior.\n6. Marginal and Joint Cross-Entropies & Predictives\n104\nHence, for online Bayesian inference, we require joint predictives, which only\nBayesian methods can give us:2 through BNNs in the parametric case or through\nGaussian processes in the non-parameteric case, for example. This strongly contrasts\nwith marginal predictives which can also be modelled by regular NNs.\nCan we perform online Bayesian inference well for high-dimensional inputs and\nparameters using current approximate BNNs? The quality of the resulting predictions\ncrucially depends on the joint predictives. However, computing joint predictives can be\nchallenging. For example, in Bayesian literature, the joint predictive of all samples in\nthe training set marginalized over the prior distribution is just the well-known marginal\nlikelihood, which can be used for model selection [MacKay, 2003; Lyle et al., 2020;\nLlorente et al., 2023], and is known to be difficult to estimate in high-dimensional\nparameter spaces [Lotfi et al., 2022]. Similar challenges can be expected for performing\nonline Bayesian inference using approximate posterior distributions.\nBased on recent works by [Wen et al., 2021; Osband et al., 2022b], we examine the\njoint predictives for online Bayesian inference using naive prior sampling. However, we\nfind negative results when using MC dropout [Gal and Ghahramani, 2016b].\nRelevant Literature. Osband et al. [2022b]; Wen et al. [2021]; Osband et al. [2021b,a]\nmention the importance of joint predictives in the context of combinatorial decision\nproblems, sequential predictions and multi-armed bandits in low dimensions. Compared\nto these previous works, we explore important connections to supervised learning, e.g. in\nactive learning [Atlas et al., 1989; Settles, 2010] and Bayesian optimal experimental\ndesign [Lindley, 1956; Foster, 2022], and focus on online Bayesian inference in high\ndimensions using deep neural networks.\nThis is also an important difference to\nMaddox et al. [2021] which examines online Bayesian inference in the context of\nGaussian processes.\nIn addition to these works, we clarify that both marginal and joint cross-entropies\nhave their use, and it is not the case that one is always preferable over the other.\nSimply put, we argue that they capture different quantities that are separately useful\nin offline and online learning, and we combine them to evaluate the performance of\nonline Bayesian inference using approximate BNNs. We provide more details in §6.5.\nMarginal Cross-Entropy. As will become evident, the marginal cross-entropy for\na fixed predictive model captures the expected performance (under log loss) when\nthe model does not adapt to data at test time. For supervised tasks, the marginal\ncross-entropy is what is commonly referred to as cross-entropy loss and represents\ncommon practice: we obtain a fixed set of parameters by training the model on a\ntraining set, and we re-use these parameters at test time without any further updates.\nThe performance of the model does not change as it observes more test data, and\nthere is no feedback loop of any sort. In this offline learning setting, the marginal\ncross-entropy is the right choice to estimate performance.\nJoint Cross-Entropy. On the other hand, the joint cross-entropy for a predictive\nmodel captures the performance in a sequential learning setting, where sequential\nmodel updates take place. Here, the parameter distribution q(ω) serves as a prior\nfor online Bayesian inference.\nThis fits the context used in Wen et al. [2021] in\nwhich the model makes a prediction for the next step, observes the outcome, and\n2For consistent joint predictives, adhering to the chain rule of probability, a model needs to adhere\nto the “Bayesian update rule”, i.e. Bayes’ theorem, and thus is Bayesian. See also Equation 6.7 below.\n6. Marginal and Joint Cross-Entropies & Predictives\n105\nthen updates the model (agent).\nApplications & Experiments. We will also see that the joint predictive is important\nfor data selection in active learning and active sampling. This chapter also connects\nseveral recent works [Wen et al., 2021; Osband et al., 2022b] with active learning\nand active sampling and present new realistic and challenging experimental settings.\nMost importantly, we examine online Bayesian inference within these contexts as it\nallows us to avoid retraining across acquisitions.\nNotation. This chapter mentions many cross-entropies. We will use a more concise\nnotation to save space:\nHp ∥q[·] ≜H(p( · ) ∥q( · )) = Ep(·)[−log q(·)].\n(6.3)\nFor example: Hp ∥q[Y | x] = Ep(y|x)[−log q(y | x)].\nBackground. The setting in this chapter deviates slightly from the one introduced in\n§1.2.2: The important difference in this chapter to §1.2.2 is that we investigate how\nq compares to p and do not ignore the difference. That is, we assume an underlying\nparametric predictive model p(y | x, ω) for input samples x with targets or labels\ny with a prior parameter distribution p(ω) over ω, i.e. our model is Bayesian. As\nnoted previously, capturing the true posterior distribution p(ω | Dtrain) is infeasible,\nand we assume we have an approximate distribution q(ω) ≈p(ω | Dtrain) that we use\ninstead of the true posterior. For example, q(ω) could be based on a deep ensemble\n[Lakshminarayanan et al., 2017] as a mixture of Dirac delta distributions positioned at\nthe parameters of individually trained ensemble members, or it could be an MC dropout\nmodel that is trained using variational inference [Gal and Ghahramani, 2016a]. We use\nq(y | x) to denote the predictions after marginalizing over q(ω): q(y | x) = Eq(ω)[p(y |\nx, ω)]. Note that the underlying discriminative model (or the likelihood function) p(y |\nx, ω) remains the same—we only exchange the distribution over its parameters ω.\n6.1\nMarginal and Joint Cross-Entropy\nWe begin by contrasting marginal and joint predictive cross-entropies and revisiting\nhow they are useful for offline and online learning separately.\nMarginal Cross-Entropy. Given an underlying, possibly empirical, data distribution\nˆptrain(x, y), the marginal cross-entropy is:\nHˆptrain∥q[Y | X] = Eˆptrain(x,y)[−log Eq(ω)[p(y | x, ω)]]\n= Eˆptrain(x,y)[−log q(y | x)],\n(6.4)\nwhere we use Hˆptrain∥q[Y | X] to denote the cross-entropy. This cross-entropy is the\npopulation loss when q(ω) is not updated after seeing new samples. Each sample x, y\nis treated independently. Hence, the marginal cross-entropy captures the expected\nperformance in an offline learning setting.\nJoint Cross-Entropy. On the other hand, given an initial parameter distribution\nq(ω) above, the joint cross-entropy measures how well the parameter distribution\ncan adapt to new data D.\nTo show how this connects to joint cross-entropies, we can look at the joint\ncross-entropy of specific samples D = {yi, xi}n\ni=1 (without taking an expectation).\n6. Marginal and Joint Cross-Entropies & Predictives\n106\nThe joint cross-entropy for these specific samples is just the sum of (negative) log\nmarginal likelihoods using the chain rule, where each yi is conditioned on all ‘pre-\nvious’ observations D<i:\nHˆptrain∥q[y1, . . . , yn | x1, . . . , xn] = −log q(y1, . . . , yn | x1, . . . , xn)\n(6.5)\n= −log\nY\ni\nq(yi | xi, yi−1, xi−1, . . . , y1, x1)\n(6.6)\n= −\nX\ni\nlog q(yi | xi, D<i)\n(6.7)\n=\nX\ni\nHˆptrain∥q[yi | xi, D<i],\n(6.8)\nwhere D<i denotes “yi−1, xi−1, . . . , y1, x1”, and the marginal predictive is:\nq(yi |\nxi, D<i) = Eq(ω|D<i)[p(yi | xi, ω)]. Semantically, we compute the following in each\niteration of the sum: we update the parameter posterior q(ω | D<i), compute the losses\nfor our predictions at outcomes yi for xi, and then include yi, xi in our observed data.\nWe will denote this as the online learning setting.\nWhen we are interested in the expected loss given arbitrary data, we can compute\nthe following joint cross-entropy:\nOLL(n) ≜E(xi,yi)n\ni=1∼ˆptrue(xi,yi)n[−log q(y1, . . . , yn | x1, . . . xn)]\n(6.9)\n= E(xi,yi)n\ni=1∼ˆptrue(xi,yi)n[Hˆptrain∥q[y1, . . . , yn | x1, . . . xn]]\n(6.10)\n= Hˆptrain∥q[Y1, . . . , Yn | X1, . . . Xn],\n(6.11)\nwhere OLL stands for “online learning loss”.\nConnection to the Conditional Cross-Entropy Rate. As an aside, if we let\nn →∞, we also have OLL(n) →∞. This is not helpful, so instead we can look at the\naverage:\n1\nnOLL(n). In the limit, this average is just the cross-entropy rate:\nHˆptrain∥q[Y | X] ≜lim\nn→∞\n1\nn Hˆptrain∥q[Y1, . . . , Yn | X1, . . . Xn],\n(6.12)\nwhich we define analogously to the entropy rate in Cover and Thomas [2005].3\nSummary. The marginal cross-entropy is useful for offline learning as it predicts the\nperformance of a fixed model on the data distribution. The joint cross-entropy is useful\nfor online learning as it predicts the performance of a model as it adapts to additional\ndata. Hence, it is important for sequential decision-making.\n6.2\nOnline Bayesian Inference\nTo see what we mean by incorporating new data, assume we have sampled n additional\npoints xi, yi ∼ˆptrue(xi, yi). Traditionally, we would now update the posterior approxi-\nmation q(w) to take this new data into account for our predictions at future test points.\nHowever, this can be prohibitively expensive—especially in applications that require\nfrequent retraining. Instead, online Bayesian inference allows Bayesian models to adapt\ntheir predictions without explicitly updating the posterior approximation.\n3Cf. the entropy rate, which is: H[X] = limn→∞1\nn H[X1, . . . Xn].\n6. Marginal and Joint Cross-Entropies & Predictives\n107\nFollowing Equation 6.1, for a test point x, the predictive q(y | x, yn, xn, . . . , y1, x1)\nis proportional to the joint predictive:\nq(y | x, yn, xn, . . . , y1, x1) = q(y, yn, . . . , y1 | x, xn, . . . , x1)\nq(yn . . . , y1 | xn, . . . , x1)\n(6.13)\n∝q(y, yn, . . . , y1 | x, xn, . . . , x1),\n(6.14)\nsince the normalization constant q(yn . . . , y1 | xn, . . . , x1) is independent of y and x.\nHence, this allows us to make predictions that take into account new data without\nexplicit retraining by simply computing the joint predictive of the test point and\nnewly observed data.\nWe refer to this as online Bayesian inference (OBI). While this inference is precisely\nBayesian, q(ω) is commonly only an approximate posterior, and thus the quality of\nthis inference depends on the properties of the approximation and how we estimate\nthe joint predictive.\nThe simplest approach to estimate the joint predictive is via sampling, which\napplies to e.g. Monte-Carlo dropout, deep ensembles, and deep ensembles with prior\nfunctions [Gal and Ghahramani, 2016a; Lakshminarayanan et al., 2017; Osband et al.,\n2018], by factorizing the joint:\nq(y, yn, . . . , y1 | x, xn, . . . , x1) = Eq(ω)[p(y, yn, . . . , y1 | x, xn, . . . , x1, ω)]\n(6.15)\n= Eq(ω)\n\"\np(y | x, ω)\nn\nY\ni=1\np(yi | xi, ω)\n#\n.\n(6.16)\nThus, if we draw fixed parameter samples ωj ∼q(ω), we can pre-compute Qn\ni=1 p(yi |\nxi, ωj) for each j and estimate the joint predictive.\nFinally, we can view Qn\ni=1 p(yi | xi, ω) as unnormalized importance weights:\nq(ω)\nn\nY\ni=1\np(yi | xi, ω) ∝q(ω | yn, xn, . . . , y1, x1),\n(6.17)\nand hence, overall:\nEq(ω)\n\"\np(y | x, ω)\nn\nY\ni=1\np(yi | xi, ω)\n#\n∝Eq(ω|yn,xn,...,y1,x1)[p(y | x, ω)].\n(6.18)\nTo evaluate the performance, we can use these predictions to compute the following\nmarginal cross-entropy which incorporates the additional samples using OBI:\nHˆptrain∥q[Y | X, yn, xn, . . . , y1, x1],\n(6.19)\nwhere X, Y are sampled from the data distribution. Comparing this entropy with\nthe performance of a fully retrained model allows us to obtain a practical estimate\nfor the quality of the approximate posterior q(ω).\n6.3\nNew Evaluations & Applications\nWe suggest new experimental settings that allow us to evaluate the quality of the joint\npredictive and compare the settings to ones suggested in prior work.\n6. Marginal and Joint Cross-Entropies & Predictives\n108\n6.3.1\nPerformance in Active Learning and Active Sampling\nMethods\nA conceptually simple set of downstream tasks is to evaluate the performance of the\njoint predictives in different active learning or active sampling settings using different\napproximate model architectures (e.g. based on Epistemic Neural Networks Osband\net al. [2021a] as abstraction). Wang et al. [2021] show that performance for transductive\nactive learning is correlated to the quality of the joint predictive. We recall that we\nnoticed the same in §4 for batch active learning with non-transductive acquisition\nfunctions, i.e. BatchBALD’s performance heavily on the number of MC dropout\nsamples which informs the quality of the joint predictive.\nWe suggest, similar to Repeated-MNIST (§4), to duplicate the underlying pool\nsets or to simply allow the same sample to be selected multiple times. Importantly,\napproximate BNNs that do not provide good joint predictives will greedily select the\nsame sample over and over again or degrade to uninformed data acquisitions. This avoids\nan issue pointed out by Wang et al. [2021] and Osband et al. [2022b] as we explain next.\nConnection to Total Correlation. Wang et al. [2021] argue that the joint cross-\nentropy is dominated by the sum of the individual marginal cross-entropy scores. This\nis equivalent to saying that the total correlation between samples is negligible since the\ndifference between the joint cross-entropy and its individual marginal cross-entropies\nfor specific yi, xi is just the total correlation:\nTCq[y1, . . . , yn | x1, . . . , xn] ≜\nX\ni\nHq[yi | xi] −Hq[y1, . . . , yn | x1, . . . , xn].\n(6.20)\nThe total correlation measures the amount of information shared between the samples.\nFor random batches, the total correlation is, indeed, likely going to be negligible\nbecause most random batches are not very informative overall, and importantly, on\ncurated datasets, they are most likely uncorrelated as it is unlikely that observing\nyi, xi in the batch informs prediction for yj, xj for most j ̸= i as curated datasets\nare usually as diverse as possible.\nOnly with increasing redundancy in the dataset, e.g. by duplicating samples like\nin Repeated-MNIST (§4), random batches will become more correlated on average\nand the total correlation larger.\nThis setup is similar to the dyadic sampling proposed by Osband et al. [2022b]\nwhich repeatedly samples yj\ni for xi, with i ∈{1, 2} and evaluates the joint predictive.\nHowever, this setting in essence only measures the ability of the approximate model to\nperform Bayesian updates on two fixed training samples at a time. Hence, we suggest\nthat a better adaption to evaluate joint predictives using active learning is to duplicate\nthe dataset or to simply allow the same sample to be selected multiple times.\n6.3.2\nPerformance of Online Bayesian Inference\nAs a practical “ground truth”, we can compare the performance of retrained models\nafter acquiring additional samples with the performance of OBI as explained in §6.2.\nA particular challenging scenario for OBI is to use acquisition sequences (xi, yi)T\ni=1\nthat were collected using active learning or active sampling on the dataset. We evaluate\nOBI on models trained at different Dtrain\nt\n= {yi, xi}t\ni=1 with increasing subsets of\nonline data {yi, xi}T\ni=t+1 from these acquisition sequences. This scenario is particularly\n6. Marginal and Joint Cross-Entropies & Predictives\n109\nchallenging for OBI because the sequence of acquisition is selected to result in large\nchanges in the predictives and posterior distributions.\nThe average performance difference between OBI and fully retrained models across\ndifferent training acquisition sequences will tell us how good a given joint predictive is\nfor “meaningful” online learning. The expectation is that for most approximate BNNs,\nOBI will quickly suffer from degraded performance compared to the retrained models.\nThat is, we compare the performance of OBI as we acquire new samples xi, yi to\nan approximate BNN retrained with the same additional data for increasing n:\nHˆptrain∥q[Y | X, yn, xn, . . . , y1, x1] −Hˆptrain∥q′[Y | X],\n(6.21)\nwhere q′(ω) ≈p(ω | yn, xn, . . . , y1, x1, Dtrain) is the parameter distribution of an\n(approximate) BNN after retraining with the additional yn, xn, . . . , y1, x1.\nIdeally, we would compare to the predictions from the correct updated posterior\ndistribution; however, this is infeasible in most practical scenarios. Instead, when we\nuse an approximate BNN q′(ω) that is similar to the one used for q(ω), we can measure\nthe practical degradation between OBI and retraining. Here, the ideal would be for\nOBI to behave exactly like a fully retrained model—even if the latter does not match\nexact Bayesian inference—as such an approach would be self-consistent. Note that with\nexact Bayesian inference, we would have q(ω) = q′(ω), and the above would be zero.\nComparison to Dyadic Sampling. Unlike Osband et al. [2022b] which focuses\non selecting labels for dyadic samples repeatedly, this experiment setting is both\nmore practical and more insightful: active learning picks samples that are the most\ninformative and are supposed to update the posterior the most. This is because,\nfor informative samples, we would expect the changes in model predictions to be\nthe largest. Hence, one could expect that these samples pose the most significant\nchallenge to approximate BNNs and their joint predictives. Ideally, we would hope that\nOBI would keep up with retrained model, but this might prove to be challenging\nin high-dimensional scenarios.\nSample Selection Bias. The suggested evaluation is orthogonal to any sample\nselection bias that is added through the data acquisition process itself as we use\nthe same training data at each step for both OBI and retraining in Equation 6.21.\nSpecifically, Farquhar et al. [2021] observed that active learning introduces a bias by\nsampling from the data distribution using an acquisition function and not uniformly.\n6.3.3\nApplication:\nActive Learning with Online Bayesian\nInference\nIn many settings, retraining models for when only few new samples are added is\nprohibitively expensive.\nThis motivates batch active learning, where batches are\nacquired instead of individual samples. Expanding on this, when equipped with models\nthat perform well under OBI, one could avoid retraining models when acquiring new\ndata by using OBI. Only when OBI degrades, fully retraining will become necessary.\nWe can evaluate this both for individual acquisition and for batch acquisition.\n6.4\nEmpirical Validation\nFollowing the newly suggested experimental settings, we want to run comparisons using\ndifferent kinds of approximate BNNs in active learning and active sampling settings.\n6. Marginal and Joint Cross-Entropies & Predictives\n110\n20\n30\n40\n50\n60\n70\n80\n90\nTraining Set Size\n0.6\n0.7\n0.8\n0.9\nAccuracy\nRandom Acquisition Sequence\n20\n30\n40\n50\n60\n70\n80\n90\nTraining Set Size\nActive Acquisition Sequence\nBaseline\nOBI: +5 samples\nRetraining: +5 samples\n20\n30\n40\n50\n60\n70\n80\n90\nTraining Set Size\n0.5\n1.0\n1.5\nCross-Entropy\nRandom Acquisition Sequence\n20\n30\n40\n50\n60\n70\n80\n90\nTraining Set Size\nActive Acquisition Sequence\nBaseline\nOBI: +5 samples\nRetraining: +5 samples\nFigure 6.1: Comparison between online Bayesian inference and retraining for 5 additional\nsamples on MNIST. We compare the dynamics between using a random acquisition sequence\nand using an active acquisition sequence, drawn using active sampling. While OBI does not\nappear to be significantly worse or better on the random acquisition sequence, it markedly\ndeteriorates when using the active acquisition sequence. OBI struggles with the informative\nsamples that change the posterior a lot: the active acquisition sequence reaches 90% accuracy\nwith just 70 samples, unlike the random one.\nTable 6.1: Comparison between online Bayesian inference and retraining for 5 additional\nsamples on MNIST. OBI performs worse than the baseline (i.e. not taking into account\nnew samples at all) when using an active acquisition sequence. On the random acquisition\nsequence, it only performs just as well as not updating at all. (Mean for 5 model trials and 5\nOBI trials for each.)\nBaseline vs\nOBI: +5 samples\nRetraining: +5 samples\nAvg\n∆Cross-Entropy (↓)\n∆Accuracy (↑)\n∆Cross-Entropy (↓)\n∆Accuracy (↑)\nActive Acquisition Sequence\n0.16\n-5.7%\n-0.062\n2.0%\nRandom Acquisition Sequence\n0.00\n0.0%\n-0.075\n1.8%\nMoreover, given fixed acquisition sequences, determined using active learning and active\nsampling, we want to evaluate the difference between OBI and fully retrained models.\nAn initial experiment shows that OBI in high dimensions might not be feasible\nusing simple Monte Carlo approximations of the expectations in parameter space.\nThis is likely because of the much higher dimensionality of the problems we consider—\nespecially in comparison to Osband et al. [2021b].\n6. Marginal and Joint Cross-Entropies & Predictives\n111\nSpecifically, we use an acquisition sequence created using active sampling which\nachieves\n90% accuracy on MNIST with just 70 samples and use the same model\nand training setup as Kirsch and Gal [2021]4. After every new data point (starting\nfrom 20), we evaluate a retrained model using OBI with 5 additional data points\nand compare it to a fully retrained model with the same 5 additional data points\nas well as to a model that is not retrained at all. We train the models (5 trials) for\neach training set size and (bootstrap) sample 10000 MC dropout samples for OBI 5\ntimes out of 20000 MC dropout samples using consistent MC dropout (§4) for each\nmodel (5 sub-trials) to reduce the variance.\nIdeally, when using OBI, we should recover the same performance as if we fully\nretrained the models using the additional data. However, as is visible in Figure 6.1, this\nis not the case when using the challenging acquisition sequence from active sampling.\nTable 6.1 shows the average performance difference when using OBI with additional\nsamples from the acquisition sequence and when fully retraining. In all cases, OBI\nperforms worse than retraining. On the random acquisition sequence, it performs\nonly as well as not updating at all, while on the active acquisition sequence, it\nalways performs worse.\n6.5\nRelated Work\nThe most relevant works and indeed one of the inspirations for this work are Wen\net al. [2021] and Osband et al. [2022b], which are recommended reading. We see\nthis chapter as a contribution that provides a different and differentiated position\non the benefits of marginal versus joint predictives and respective cross-entropies as\nperformance metrics and that puts greater focus on OBI.\nMoreover, our suggested experimental settings expand on these prior works and draw\nattention to active learning and active sampling as more realistic use-cases. Measuring\nthe error between OBI and retrained models expands on the experiments in Wen et al.\n[2021] while evaluating performance in active learning and active sampling on highly\nredundant datasets (allowing to re-select previously selected points) expands on the\nidea of dyadic sampling from Osband et al. [2022b].\nLastly, Wen et al. [2021] focus on the KL divergence between the exact Bayesian\njoint predictive and the joint predictive of an approximate Bayesian model for different\nnumbers of samples in the joint.\nOur suggested experimental settings focus on\nevaluating downstream tasks.\nWang et al. [2021] also examine the quality of joint predictives on low-dimensional\ndatasets using a more synthetic evaluation method, the cross-normalized log-likelihood.\nThey also evaluate the quality of joint predictives in regression settings using active\nlearning experiments. Our evaluation settings from §6.3 extend these.\n6.6\nDiscussion\nIn this short chapter, we have revisited the difference between marginal and joint\ncross-entropies and predictives, clarifying in which contexts either is appropriate: for\noffline learning, the marginal cross-entropy is the right choice to evaluate performance\nwhile for online learning, it is the joint cross-entropy. We have shown how the joint\n4The preprint version contains information gain experiments.\n6. Marginal and Joint Cross-Entropies & Predictives\n112\npredictive plays an important role in information-theoretic acquisition functions in\nactive learning and active sampling.\nImportantly, we argue that online Bayesian inference could provide many benefits\nand have proposed new more practical and challenging experimental settings which\nexpand on prior art by using active learning and active sampling.\nGiven the results of the presented experiment, it is an open question how much\nbetter other sampling-based approaches can be when using high-dimensional parameter\nspaces. Especially deep ensembles which usually provide a much smaller “sample count”\n(i.e. number of ensemble members) might not perform well under online Bayesian\ninference because the hypothesis space will be exhausted faster—even when the ensemble\nmembers are diverse. Future research will hopefully offer further experimental evaluation\nfollowing §6.3, e.g. improving the quality of online Bayesian inference by studying higher\nquality posterior distributions such as those from HMC or efficient low-dimensional\nposterior approximations that might make parameter-space integrals tractable. Prior\nresearch into failures of Bayesian model averaging under model misspecification might\nprovide further insights and paths to improvements [Minka, 2002].\nUsing the suggested experimental settings from this chapter (or rather the original\npreprint), Herde et al. [2022] suggest using last-layer Laplace. Similarly, Osband et al.\n[2022a] explore active learning for evaluating their proposed EpiNets with great results.\nStrategy without tactics is the slowest route to\nvictory.\nTactics without strategy is the noise\nbefore defeat.\nSun Tzu, The Art of War\n7\nPrediction- & Distribution-Aware Bayesian\nActive Learning\nHistorically the literature in active learning and Bayesian experimental design has\nfocused on trying to maximize the expected information gain (EIG) in the model\nparameters. This yields the acquisition function typically known as BALD, having been\npopularized by a method called Bayesian active learning by disagreement [Houlsby\net al., 2011], which has been successfully applied in settings including computer\nvision [Gal et al., 2017] and natural-language processing [Shen et al., 2018] as we\nhave seen already in this thesis.\nIn this chapter, we highlight that BALD can be misaligned with our typical\noverarching goal of making accurate predictions on unseen inputs. In machine learning,\nwe often care about predictions more than the parameter distribution—the parameters\nΩΩΩare not actually what we care about: they are merely a stepping stone to the\npredictions we will later make, and these future predictions are the ultimate quantity of\ninterest. Why can BALD be misaligned with this, and why is this distinction important?\nUnfortunately, BALD neglects a crucial fact: not all information about the model\nparameters is equally useful when it comes to making predictions. This distinction can\nbe surprisingly important: accurate predictions may not correspond to the most precise\nknowledge of the model parameters themselves. With a non-parametric model, for\ninstance, we can gain an infinite amount of information about the model parameters\nwithout any of it being relevant to prediction on inputs of interest. More generally,\ndifferent information about the model parameters may not be equal in regard to how\nit enables effective prediction for a particular target input distribution over possible\ninputs x. The models we deploy are often of limited capacity or misspecified, and there\nare trade-offs in the performance of the model depending on what we value [Cobb\net al., 2018]. In short, BALD lacks a notion of how the model will be used and so fails\nto ensure that the data acquired is relevant to our particular predictive task. As a\nresult, BALD is liable to acquire labels that are informative with respect to the model\nparameters but not with respect to the predictions of interest.\nRelevance. This has considerable practical implications. Real-world datasets are\noften messy, with inputs that vary widely in their relevance to a given task. Large\npools of audio, images and text commonly fit this description [Ardila et al., 2020;\nGemmeke et al., 2017; Mahajan et al., 2018; Radford et al., 2021; Raffel et al., 2020;\nSun et al., 2017]. We may have data from different sources of varying fidelity and\nrelevance to our task. Indeed, if we have a large unlabeled dataset generated by\nscraping the internet, for example, we might have a significant variance in how closely\n7. Prediction- & Distribution-Aware Bayesian Active Learning\n114\nEPIG\nBALD\nTraining Data\nTest Distribution\nPrediction\n95% confidence interval\n(a) 1D Toy Example\np1(x)\np2(x)\ny = 0\n↑\n↓\ny = 1\nData distributions\nTraining data and predictions\nBALD\nEPIG with peval =p1\nEPIG with peval =p2\n(b) 2D Toy Example\nFigure 7.1: The expected predictive information gain (EPIG) can differ dramatically from\nthe expected information gain in the model parameters (BALD). (a): We consider a simple\n1D regression task with a Gaussian process model. Larger values of the acquisition function\nindicate a preference for those points. While BALD is predominantly concerned with labeling\ninputs far away from the data it has already seen, EPIG takes into account the target input\ndistribution (the test distribution). (b): BALD increases (darker shading) as we move away\nfrom the existing data, yielding a distant acquisition (star) when maximized. It seeks a global\nreduction in parameter uncertainty, regardless of any input distribution. In contrast, EPIG\nis maximized only in regions of relatively high density under the target input distribution,\npeval(xeval). It seeks a reduction in parameter uncertainty only insofar as it reduces predictive\nuncertainty on samples from peval(xeval). See §7.3.4 for details.\nrelated individual data points are to the task we actually care about—if the task is\nto detect hate speech in text, social-media posts are much more useful than articles\nabout set theory. Likewise, consider the outputs of large-scale experiments: if we\nwant to predict the behavior of plasma in a new fusion-reactor configuration, results\nfrom similar configurations will likely be more pertinent than those from completely\ndifferent ones. In an extreme case, the task distribution might consist of samples that\ncannot even be directly labeled. This might apply in protein folding, where complex\nhuman proteins might make up the task distribution of interest, which would be too\ncostly to crystallize, while the pool set might contain simpler proteins which could\nbe more easily crystallized and their three-dimensional structure learned. Here, we\nwill show that BALD can be actively counterproductive in cases like these, picking\nout the most obscure and least relevant inputs, potentially.\nEPIG. To address BALD’s shortcomings, we propose the expected predictive informa-\ntion gain (EPIG), an alternative acquisition function. We derive EPIG by returning\nto the foundational framework of Bayesian experimental design [Lindley, 1956], from\nwhich BALD itself is derived. Whereas BALD is the EIG in the model parameters,\nEPIG is the EIG in the model’s predictions: it measures how much information the\nlabel of a candidate input is expected to provide about the label of a random target\n7. Prediction- & Distribution-Aware Bayesian Active Learning\n115\ninput. While BALD favors global reductions in parameter uncertainty, EPIG favors\nonly information that reduces downstream predictive uncertainty (Figure 7.1). Thus,\nEPIG allows us to directly seek improvements in predictive performance.\nThe randomness of the target input in EPIG is critical.\nWe do not aim for\npredictive information gain on a particular input or set of inputs. Instead, the gain\nis in expectation with respect to a target input distribution. This can be chosen to\nbe the same distribution that the pool of unlabeled inputs is drawn from, or it can\nbe a distinct distribution that reflects a downstream task of interest.\nWe find that EPIG often produces notable gains in final predictive performance\nover BALD across a range of datasets and models. EPIG’s gains are largest when the\npool of unlabeled inputs contains a high proportion of irrelevant inputs with respect to\nthe target input distribution. But its advantage still holds when the pool is directly\ndrawn from this distribution. As such, it can provide a simple and effective drop-in\nreplacement for BALD in many settings.\nJEPIG. We also provide a preliminary examination of an alternative target input\ndistribution acquisition function, which we name the joint expected predictive informa-\ntion gain (JEPIG). Like the EIG, JEPIG is still based around targeting information\ngain in the model parameters, but it discounts information that is not relevant to\nmodel’s prediction for the target input distribution. Specifically, it discards from\nthe EIG any information that will remain unknown after we would have acquired\nthe labels for the target input distribution, on the basis that this information would\nnot be helpful for prediction.\n7.1\nShortfalls of BALD\nIn this section, we highlight that BALD can be poorly suited to the prediction-\noriented settings that constitute much of machine learning. We explain that this\nstems from the mismatch that can exist between parameter uncertainty and predictive\nuncertainty. Importantly, we find that BALD does not take the input distribution\ninto account at all—it, thus, cannot target the predictive uncertainty of the inputs\nwe actually want to make predictions on.\n7.1.1\nNo Focus on Predictions\nIn statistics, it is common for the model parameters to be valued in their own right\n[Beck and Arnold, 1977; Blei et al., 2001; Fisher, 1925]. But in many machine-learning\ncontexts, particularly the supervised settings where BALD is typically applied, the\nparameters are only valued insofar as they serve a prediction-oriented goal. We often,\nfor example, seek the parameters that maximize the model’s predictive performance\non a test data distribution [Hastie et al., 2001]. This frequentist notion of success\noften remains our motivation even if we use a Bayesian approach to data acquisition\nand/or learning [Komaki, 1996; Snelson and Ghahramani, 2005].\n7.1.2\nNo Notion of an Input Distribution\nIn order to reason about what information is relevant to prediction, we need some\nnotion of the inputs on which we want to make predictions. Without this we have no\nmechanism to ensure the model we learn is well-suited to the task we care about. Our\n7. Prediction- & Distribution-Aware Bayesian Active Learning\n116\n10\n20\n30\n40\n50\nNumber of labels\n80\n85\n90\n95\nTest accuracy (%)\nPool size\n102\n103\n104\n105\n(a) Pool Size Ablation\n10\n20\n30\n40\n50\nNumber of labels\n80\n85\n90\n95\nTest accuracy (%)\nAcquisition function\nrandom\nBALD\nEPIG\n(b) EPIG vs BALD vs Random\nFigure 7.2: Shortfalls of BALD, and how we fix them. See Figure 7.1 for intuition and\n§7.3.4 for details. (a) BALD can fail catastrophically on big pools. A bigger pool typically\ncontains more inputs with low density under the data-generating distribution. Often these\ninputs are of low relevance if the aim is to maximize expected predictive performance. BALD\ncan nevertheless favor these inputs. (b) In contrast with BALD, EPIG deals effectively with\na big pool (105 unlabeled inputs). BALD is overwhelming counterproductive even relative to\nrandom acquisition.\nmodel could be highly effective on inputs from one region of input space but useless\nfor typical samples from an input distribution of interest.\nAppreciating the need to account for which inputs might arise at test time, it\nbecomes clear why BALD can be problematic. BALD focuses on the model parameters\nin isolation, with no explicit connection to prediction. As such, it does not account\nfor the distribution over inputs.\n7.1.2.1\nFailures under Real-World Data\nBALD can be particularly problematic in the very settings that often motivate active\nlearning: those where we have access to a large pool of unlabeled inputs whose\nrelevance to some task of interest varies widely. In contrast with the carefully curated\ndatasets often used in basic research, real-world data is often drawn from many\nsources of varying fidelity and relation to the task.\nPools of web-scraped audio,\nimages and text are canonical examples of this. Active learning ought to help deal\nwith the mess by identifying only the most useful inputs to label. But BALD can\nin fact be worse than random acquisition in these settings, targeting obscure data\nthat is not helpful for prediction.\nThe experiment presented in Figure 7.2(a) highlights this flaw. As we increase the\nsize of the pool that BALD is maximized over, inputs of greater obscurity become\nmore likely to be included in the pool, and BALD produces worse and worse predictive\naccuracy. This result is corroborated by the work of Karamcheti et al. [2021]. Focusing\non visual-question-answering tasks, they found that BALD failed to outperform random\nacquisition when using uncurated pools, and that a substantial amount of curation\nwas required before this shortfall could be overturned.\n7.1.2.2\nFailures without Distribution Shift\nIt might be tempting to just think of this problem with BALD as being analogous to\nthe issues caused by train-test input-distribution shifts elsewhere in machine learning.\nBut the problem is more deep-rooted than this: BALD has no notion of any input\ndistribution in the first place. This is why increasing the size of the pool can induce\n7. Prediction- & Distribution-Aware Bayesian Active Learning\n117\nfailures as in Figure 7.2(a), without any distribution shift or changes to the distribution\nthat the pool inputs are drawn from. Distribution shift can cause additional problems\nfor BALD, as the results in §7.3.3 show. But it is by no means a necessary condition\nfor failure to occur.\n7.1.3\nNot All Information is Equal\nIn some models, such as linear models, parameters and predictions are tightly coupled,\nbut more generally the coupling can be loose.\nThis means that a reduction in\nparameter uncertainty typically might not yield a wholesale reduction in predictive\nuncertainty [Chaloner and Verdinelli, 1995a]. Deep neural networks, for instance, can\nhave substantial redundancy in their parameters [Belkin et al., 2019], while Bayesian\nnon-parametric models can be thought of as having an infinite number of parameters\n[Hjort et al., 2010]. When the coupling is loose, parameter uncertainty can be reduced\nwithout a corresponding reduction in predictive uncertainty on inputs of interest. In\nfact, it is possible to gain an infinite amount of parameter information while seeing\nan arbitrarily small reduction in predictive uncertainty.\nExample 7.1 (Infinite Information Gain). Consider the supervised-learning problem\ndepicted in Figure 7.1(a), where x ∈R is an input, y ∈R is a label, and we use\na model consisting of a Gaussian likelihood function, p(y|x, f) = N(f(x), 1), and a\nzero-mean Gaussian-process prior, f ∼GP(0, k), with covariance function k(x, x′) =\nexp(−(x−x′)2/2l2), where l is the length scale of the model [Williams and Rasmussen,\n2006], e.g. l = 2. Suppose we are interested in making predictions Y eval | Xeval in the\nregion, e.g. xeval ∼N(2, (1/4)2), and consider we are observing only odd integers x:\nx1, x2, . . . , xM for some M ∈N+ with predictions Y1, Y2, . . . , YM. We only allow odd\nintegers to avoid direct overlap with the region of interest as we decrease as the length\nscale (to make this example easier to reason about).\nWe will vary the length scale and the number of points M to show that BALD can\nbecome arbitrarily large while the predictive uncertainty of interest, H[Y eval | Xeval, . . .],\nwill change arbitrarily little.\nBALD is the EIG between the predictions and GP functions f ∼GP(0, k) (as a\nnon-parametric model):\nI[F; Y1, Y2, . . . , YM; | x1, x2, . . . , xM] = . . .\n(7.1)\n= H[Y1, Y2, . . . , YM | x1, x2, . . . , xM] −H[Y1, Y2, . . . , YM | x1, x2, . . . , xM, F].\nThe first term is the (joint) entropy of the predictions (incl. the Gaussian likelihood),\nand the second term is the (joint) entropy of the Gaussian likelihood itself. The entropy\nof the multivariate Gaussian distribution with covariance Σ ∈RM×M is [Cover and\nThomas, 2005]:\nH[N(µ, Σ)] = 1\n2 log\n\u0010\n(2π)M det Σ\n\u0011\n.\n(7.2)\nFor the two terms, we have:\nH[Y1, Y2, . . . , YM | x1, x2, . . . , xM] = 1\n2 log\n\u0010\n(2π)M det ((k(xi, xj))ij + IdM)\n\u0011\n,\n(7.3)\nH[Y1, Y2, . . . , YM | x1, x2, . . . , xM, F] = 1\n2 log\n\u0010\n(2π)M det (IdM)\n\u0011\n.\n(7.4)\n7. Prediction- & Distribution-Aware Bayesian Active Learning\n118\nWe see that the log (2π)M constants cancel out in both terms, and we end up with:\nI[Y1, Y2, . . . , YM; F | x1, x2, . . . , xM]\n(7.5)\n= 1\n2 log det ((k(xi, xj))ij + IdM) −1\n2 log det IdM\n|\n{z\n}\n=log 1=0\n= 1\n2 log det ((k(xi, xj))ij + IdM) .\n(7.6)\nNow we can use the length scale: for l →0, we have k(xi, xj) →1{i = j}—the\nkernel matrix becomes an identity matrix—and thus, det ((k(xi, xj))ij + IdM) →\ndet (2IdM) →2M. Similarly, we are interested in the reduction in the predictive\nuncertainty, H[Y eval | Xeval, . . .]:\nH[Y eval | Xeval] −H[Y eval | Xeval, Y1, x1, Y2, x2, . . . , YM, xM].\n(7.7)\nWith our eyes now well-trained for information quantities, we can see that this just\nthe EIG of the predictive Y eval | Xeval with respect to the samples Y1, Y2, . . . , YM |\nx1, x2, . . . , xM:\nI[Y eval; Y1, Y2, . . . , YM | Xeval, x1, x2, . . . , xM].\n(7.8)\nThis is the expected predictive information gain, which we will introduce in §7.3. We\ncan compute this quantity via H[A|B] = H[A, B]−H[B]. Let us fix xeval. The constant\nlog terms cancel out again, and we have:\nI[Y eval; Y1, Y2, . . . , YM | xeval, x1, x2, . . . , xM]\n= H[Y eval | xeval] + H[Y1, Y2, . . . , YM | x1, x2, . . . , xM]\n(7.9)\n−H[Y eval, Y1, Y2, . . . , YM | xeval, x1, x2, . . . , xM]\n= 1\n2 log det\n\u0010\n(k(xeval, xeval)) + IdM\n\u0011\n+ 1\n2 log det ((k(xi, xj))ij + IdM)\n(7.10)\n−1\n2 log det\n\u0010\n(k(xi, xj))i,j={∗,1..M} + IdM\n\u0011\nl→0\n→1\n2 log 2 + 1\n2M log 2 −1\n2(M + 1) log 2\n(7.11)\n= 0.\n(7.12)\nThen, taking the expectation over xeval, we have:\nI[Y eval; Y1, Y2, . . . , YM | Xeval, x1, x2, . . . , xM]\n(7.13)\n= Ep(xeval)[I[Y eval; Y1, Y2, . . . , YM | xeval, x1, x2, . . . , xM]]\nl→0\n→Ep(xeval)[0] = 0.\n(7.14)\nAltogether, as l →0 and for M →∞, BALD diverges to infinity while the EIG in the\nprediction of interest converges to zero:\nI[F; Y1, Y2, . . . , YM | x1, x2, . . . , xM]\nl→0\n→M\n2 log 2\nM→∞\n→\n∞,\n(7.15)\nI[Y eval; Y1, Y2, . . . , YM | Xeval, x1, x2, . . . , xM]\nl→0\n→0 =⇒H[Y eval | Xeval, . . .] ≈const.\n(7.16)\nThis example is a concrete demonstration that a high BALD score need not coincide\nwith any reduction in the predictive uncertainty of interest. If the aim is to predict,\nthen maximizing BALD is not guaranteed to help to any extent whatsoever.\n7. Prediction- & Distribution-Aware Bayesian Active Learning\n119\n7.2\nDistribution-Aware Acquisition Functions\nMotivated by BALD’s weakness in prediction-oriented settings and the need for\nprincipled approaches, we return to the framework of Bayesian experimental design\nthat underlies BALD, and derive two acquisition functions that we call the expected\npredictive information gain (EPIG) and the joint expected predictive information gain\n(JEPIG). Whereas BALD targets a reduction in parameter uncertainty, EPIG and\nJEPIG directly target a reduction in predictive uncertainty on inputs of interest.\n7.2.1\nWhy not use Filtering Heuristics?\nWe might suppose we could just discard irrelevant data before deploying BALD. But\nthis filtering process would require us to be able to determine each input’s relevance at\nthe outset of training, which is impractical in many cases. Even if we have access to\na target input distribution, this on its own can be insufficient for judging relevance\nto a task of interest. A candidate input could have relatively low density under the\ntarget distribution but nevertheless share high-level features with a target input, such\nthat the two inputs’ labels are highly mutually informative. With high-dimensional\ninputs, it can also be surprisingly difficult to identify unrepresentative inputs purely\nthrough their density [Nalisnick et al., 2019]. Rather than trying to design an auxiliary\nprocess to mitigate BALD’s problematic behavior, we seek an acquisition function\nthat can automatically determine what is relevant.\n7.2.2\nTwo Alternative Expected Information Gains\nTo be precise, when we talk about EIG and BALD, these are the parameter EIG, which\ndoes not focus on predictions and is not distribution-aware. But we can also look at\ndifferent EIGs which focus on the predictions: target EIG or predictive EIG1.\nEvaluation Distribution. To reason about the predictions we are interested in,\nwe need an explicit notion of the predictions we want to make with our model.\nWe therefore introduce a random target input, xeval ∼peval(xeval), for unlabeled\nevaluation samples from the target input distribution and define our goal to be the\nconfident prediction2 of yeval | xeval.\nSimilar to other unlabeled samples in active learning, these samples could be either\nprovided in a stream-based setting, where we can repeatedly draw new unlabeled\nsamples i.i.d., or in a pool-based setting, where we only have a fixed reservoir of\nevaluation samples, which we call the evaluation set Deval. In the pool-based setting,\nwe denote the empirical distribution of the evaluation set by peval(xeval), whereas in a\nstream-based setting, we could set peval(xeval) ≜ptest(xeval). This allows us to abstract\naway the exact setting in the following exposition.\nPredictive EIG. Let us fix a single target input xeval ∼peval(xeval) with prediction\nY eval | xeval, and consider the expected information gain given a single pool sample\nxacq ∈Dpool with prediction Y acq |xacq. The corresponding predictive EIG is, of course:\nI[Y eval; Y acq | xeval, xacq].\n(7.17)\n1Any naming scheme is going to be confusing and ambiguous given different prior art, so we might\nsadly add to this confusion.\n2We still have to hope that by virtue of providing correct labels for training, confident predictions\nwill also be likely correct predictions.\n7. Prediction- & Distribution-Aware Bayesian Active Learning\n120\nI[Y eval; Y acq | Xeval, xacq] / I[Y eval\n1..E ; Y acq | xeval\n1..E, xacq]\nI[ΩΩΩ; Y acq | xacq]\nH[ΩΩΩ]\nH[Y acq | xacq]\nH[Y eval | Xeval]/H[Y eval\n1..E | xeval\n1..E]\nFigure 7.3:\nVisualizing both EPIG I[Y eval\n1..E ; Y acq | xeval\n1..E , xacq] & JEPIG I[Y eval; Y acq |\nXeval, xacq] vs. EIG I[ΩΩΩ; Y acq | xacq] in the same I-Diagram. MacKay’s ‘total information\ngain’ for the EIG is a fitting term because we can immediately read off that it upper-bounds\nboth EPIG and JEPIG.\nThe case when we want to take the whole distribution peval(xeval) over xeval into\naccount is slightly more complicated. There are two alternatives that we can examine:\n• the (mean) marginal predictive EIG,\nI[Y eval; Y acq | Xeval, xacq],\n(7.18)\nwhere we take an expectation over Xeval, hence ‘mean’; and\n• the joint predictive EIG,\nI[Y eval\n1..E ; Y acq | xeval\n1..E, xacq],\n(7.19)\nfor an empirical evaluation set Deval = xeval\n1..E. (We will look at the general case\nbelow.)\nWe will see that the marginal predictive EIG is equivalent to the expected reduction\nin generalization loss if we were to acquire xacq, while the joint predictive EIG is\nequivalent to performing Bayesian model selection on the pool set.\nIn the next sections, we will explore both using slightly different names, the joint\npredictive EIG as joint expected predictive information gain, and the (marginal)\npredictive EIG as expected predictive information gain. Sadly, there are several naming\nschemes. EPIG and JEPIG are mentioned in already published papers that spun out\nof this thesis, and while above names seem more fitting in retrospect, they also do\nnot exactly match the ones from MacKay [1992b], who referred to them as ‘mean\nmarginal information gain’ and ‘joint information gain’, respectively, and left out\nthe rather important term ‘predictive’. The parameter EIG was referred to as ‘total\ninformation gain’ in the same paper, which also makes sense: Figure 7.3 shows that\nthe (parameter) EIG upper-bounds both EPIG and JEPIG. Hence, it is the total\ninformation gain, while EPIG and JEPIG only focus on the part of the information\ngain that is relevant for the predictions.\n7.3\nExpected Predictive Information Gain\nTo derive EPIG, we first consider the information gain (IG) in yeval that results from\nconditioning on new data, (x, y) and fix xeval ∼p(xeval):\nI[Y eval; y | xeval, x, Dtrain] = H[Y eval | xeval, Dtrain] −H[Y eval | xeval, y, x, Dtrain] (7.24)\n7. Prediction- & Distribution-Aware Bayesian Active Learning\n121\n□7.1 – Expected Reduction in Generalization Loss\nThe same samples maximize EPIG and minimize the expected generalization\nloss [Roy and McCallum, 2001]. EPIG measures the expected reduction in the\nuncertainty in a model’s predictions on target points xeval ∼peval(xeval):\nI[Y eval; Y acq\n1..K | Xeval, xacq\n1..K, Dtrain] =\nH[Y eval | Xeval, Dtrain] −H[Y eval | Xeval, Y acq\n1..K , xacq\n1..K, Dtrain].\n(7.20)\nImportantly, this objective is equivalent to minimizing the expected generalization\nloss under the model’s predictions:\nH[Y eval | Xeval, Y acq\n1..K , xacq\n1..K, Dtrain] =\n= Epeval(xeval) Ep(yacq\n1..K|xacq\n1..K,Dtrain) Ep(yeval|xeval,yacq\n1..K,xacq\n1..K,Dtrain) L(xeval, yeval),\n(7.21)\nL(xeval, yeval) ≜−log p(yeval | xeval, yacq\n1..K, xacq\n1..K, Dtrain),\n(7.22)\nwhere we have marginalized over the model parameters\nωωω ∼p(ωωω | yacq\n1..K, xacq\n1..K, Dtrain).\n(7.23)\n= H(p(yeval | xeval, Dtrain)) −H(p(yeval | xeval, y, x, Dtrain))\n(7.25)\nwhere p(yeval | xeval, y, x, Dtrain) = Ep(ωωω|y,x,Dtrain)[p(yeval | xeval, y, x,ωωω)]. Note that this\nis a function of xeval as well as x and y. Next we take an expectation over both the\nrandom target input, xeval, and the unknown label, y:\nEPIG(x) = Epeval(xeval) p(y|x,Dtrain)[I[Y eval; y | xeval, x, Dtrain]]\n(7.26)\n= I[Y eval; Y | Xeval, x, Dtrain],\n(7.27)\nwhere we were able to use our concise & unified notation for great benefit. Thus, we\nsee that EPIG is the expected reduction in predictive uncertainty at a randomly\nsampled target input, xeval.\nWe can also take a frequentist perspective and relate it to expected reduction in\nprediction uncertainty, which is equivalent to the expected reduction in generalization\nloss (over peval(xeval)) as we detail in □7.1 Expected Reduction in Generalization Loss .\nThere are other interpretations too. For example, we could write EPIG as an\nexpected KL divergence. More interesting is that we can write EPIG as an expected\ninformation gain between (Xeval, Y eval) and Y | x:\nI[(Xeval, Y eval); Y | x, Dtrain] = I[Xeval; Y | x, Dtrain]\n|\n{z\n}\n=0\n+ I[Y eval; Y | Xeval, x, Dtrain].\n(7.28)\nThe first term on the right-hand side is 0 because Xeval and Y | x are independent.\nWe can also write EPIG as difference between two EIGs:\nI[Y eval; Y | Xeval, x, Dtrain]\n(7.29)\n7. Prediction- & Distribution-Aware Bayesian Active Learning\n122\n= I[Y eval; Y | Xeval, x, Dtrain] + I[Y eval; Y | Xeval, x,ΩΩΩ, Dtrain]\n|\n{z\n}\n=0\n(7.30)\n= I[Y eval; Y ;ΩΩΩ| Xeval, x, Dtrain]\n(7.31)\n= I[Y eval;ΩΩΩ| Xeval, Dtrain] −I[Y eval;ΩΩΩ| Xeval, Y, x, Dtrain].\n(7.32)\nI[Y eval; Y | Xeval, x,ΩΩΩ, Dtrain] is zero because Y eval and Y are independent given ΩΩΩ.\nThus, we can also view EPIG as the expected reduction in epistemic uncertainty for\nevaluation samples, Y eval, given the model parameters, ΩΩΩ.\n7.3.1\nSampling Target Inputs\nEPIG involves an expectation with respect to a target input distribution of evaluation\nsamples, peval(xeval). In practice, we estimate this expectation by Monte Carlo and\nso require a sampling mechanism.\nIn many active-learning settings an input distribution is implied by the existence of\na pool of unlabeled inputs. There are cases where we know (or are happy to assume)\nthe pool has been sampled from peval(xeval). Alternatively we might be forced to\nassume this is the case: perhaps we know the pool is not sampled from peval(xeval)\nbut lack access to anything better. In these cases we can simply subsample from\nthe pool to obtain samples of xeval. Empirically we find that this can work well\nrelative to acquisition with BALD (§7.3.3).\nAnother important case is where we have access to samples from peval(xeval), but\nwe cannot label them. Limits on the ability to acquire labels might arise due to\nprivacy-related and other ethical concerns, geographical restrictions, the complexity\nof the labelling process for some inputs, or the presence of commercially sensitive\ninformation in some inputs. At the same time there might be a pool of inputs for\nwhich we have no labelling restrictions. In a case like this we can estimate EPIG\nusing samples from peval(xeval) while using only the pool as a source of candidates\nfor labelling. Thus, we can target information gain in predictions on samples from\npeval(xeval) without labelling those samples themselves.\nA further scenario that we might encounter is a classification problem where the\npool is representative of the target class-conditional input distribution but not the\ntarget marginal class distribution: that is, ppool(xeval | yeval) = peval(xeval | yeval) but\nppool(yeval) ̸= peval(yeval). The pool might, for example, consist of uncurated web-\nscraped inputs from many more classes than those we care about. In this scenario it\ncan often be the case that we know or can reasonably approximate the distribution over\nclasses that we are targeting, peval(yeval). With this we can approximately sample from\npeval(xeval) using a combination of our model, p(y | x), and the pool. We first note that\nppool(xeval | yeval) =\nppool(xeval) ppool(yeval | xeval)\nR ppool(x) ppool(Y = yeval | x)dx.\n(7.33)\nThen, using the fact that ppool(xeval | yeval) = peval(xeval | yeval), we get\npeval(xeval) =\nX\nyeval\npeval(yeval) peval(xeval | yeval)\n(7.34)\n= ppool(xeval)\nX\nyeval\npeval(yeval) ppool(yeval | xeval)\nR ppool(x)ppool(y = yeval|x)dx\n(7.35)\n7. Prediction- & Distribution-Aware Bayesian Active Learning\n123\n≈ppool(xeval)\nX\nyeval\npeval(yeval) p(yeval | xeval)\n1\nN\nP\nx∈Dpool p(y = yeval | x)\n(7.36)\n= ppool(xeval) w(xeval),\n(7.37)\nwhere we have approximated ppool(yeval | xeval) with our model. Now we can approxi-\nmately sample from peval(xeval) by subsampling inputs from the pool using a categorical\ndistribution with probabilities w(xeval)/N.\n7.3.2\nEstimation\nThe best way to estimate EPIG depends on the task and model of interest. In the\nempirical evaluations in this chapter we focus on classification problems and use\nmodels whose marginal and joint predictive distributions are not known in closed\nform.\nThis leads us to use EPIG(x) ≈\n1\nM\nM\nX\nj=1\nDKL(ˆp(Y, Y eval | x, xeval\nj\n) ∥ˆp(Y | x) ˆp(Y eval | xeval\nj\n)),\n(7.38)\nwhere xeval\nj\n∼peval(xeval), and ˆp denotes Monte Carlo approximations of the respective\npredictive distributions (see also Equations 1.24 and 1.25). Classification is an instance\nof where the required expectation over y and yeval can be computed analytically, such\nthat our only required estimation is from marginalizations over Θ.\nIf we cannot integrate over y and yeval analytically, we can revert to nested Monte\nCarlo estimation [Rainforth et al., 2018]. For this we first note that, using Equation 1.25,\nwe can sample y, yeval ∼p(y, yeval | x, xeval) exactly by drawing a θ and then a y and\nyeval conditioned on this θ. By also drawing samples for θ, we can then construct\nthe estimator EPIG(x) ≈\n1\nM\nM\nX\nj=1\nlog\nK PK\ni=1 p(yj | x, θi) p(yeval\nj\n| xeval\nj\n, θi)\nPK\nk=1 p(yj | x, θk)\nPK\nk=1 p(yeval\nj\n| xeval\nj\n, θk),\n(7.39)\nwhere yj, yeval\nj\n∼p(y, yeval | x, xeval\nj\n), θi ∼p(θ), and xeval\nj\n∼peval(xeval). Subject to some\nweak assumptions on p, this estimator converges as K, M →∞[Rainforth et al., 2018].\nThe EPIG estimators in Equations 7.38 and 7.39 each have a total computational\ncost of O(MK). This is comparable to BALD estimation for regression problems. But it\ncan be more expensive than BALD estimation for classification problems: BALD can be\ncollapsed to a non-nested Monte Carlo estimation for an O(K) cost, but EPIG cannot.\nOther possible estimation schemes include a variational approach inspired by Foster\net al. [2019]. This is too expensive to be practically applicable in the settings we\nconsider but could be useful elsewhere. See Appendix F.3 for details.\n7.3.3\nEmpirical Validation\nFor consistency with existing work on active learning for prediction, our empirical\nevaluation of EPIG focuses on classification problems. Code for reproducing our results\nis available at github.com/fbickfordsmith/epig.\n7.3.4\nSynthetic Data (Figures 7.1, 7.2(a) and 7.2(b))\nFirst we demonstrate the difference between BALD and EPIG in a setting that is easy\nto visualize and understand: binary classification with two-dimensional inputs.\n7. Prediction- & Distribution-Aware Bayesian Active Learning\n124\nData. The first input distribution of interest, denoted p1(x) in Figure 7.1, is a bivariate\nStudent’s t distribution with ν = 5 degrees of freedom, location µ = [0, 0] and scale\nmatrix Σ = 0.8I. The second distribution, denoted p2(x) in Figure 7.1, is a scaled\nand shifted version of the first, with parameters ν = 5, µ ≈[0.8, 0.9] and Σ = 0.4I.\nThis serves to illustrate in Figure 7.1 how EPIG’s value changes with the target\ninput distribution; it is not used elsewhere. The conditional label distribution is\ndefined as p(y = 1|x) = Φ(20(tanh(2x[1]) −x[2])), where x[i] denotes the component of\ninput x in dimension i, and Φ is the cumulative distribution function of the standard\nnormal distribution. For the training data in Figure 7.1, we sample ten input-label\npairs, Dtrain = {(xi, yi)}10\ni=1, where xi, yi ∼p(y|x)p1(x). Likewise, we sample Dtest =\n{(xi, yi)}10,000\ni=1\nfor evaluating the model’s performance in active learning.\nModel and Training. We use a model with a probit likelihood function, p(y = 1 |\nx, θ) = Φ(θ(x)), where Φ is defined as above, and a Gaussian-process prior, θ ∼GP(0, k),\nwhere k(x, x′) = 10 · exp (−∥x −x′∥2/2). The posterior over latent-function values cannot\nbe computed exactly, so we optimize an approximation to it using variational inference\n[Hensman et al., 2015]. To do this we run 10,000 steps of full-batch gradient descent\nusing a learning rate of 0.005 and a momentum factor of 0.95.\nActive Learning. We initialize the training dataset, Dtrain, with two randomly\nsampled inputs from each class. Thereafter, we run the active-learning loop described\nin §1.2.4 until a budget of 50 labels is used up. We acquire data using three acquisition\nfunctions: random, BALD and EPIG. Random acquisition involves sampling uniformly\nfrom the pool without replacement. We estimate BALD using Equation F.6 and\nEPIG using Equation 7.38, in both cases drawing 5,000 samples from the model’s\napproximate posterior. For EPIG we sample xeval ∼p1(xeval). After each time the\nmodel is trained, we evaluate its predictive accuracy on Dtest as defined above. Using\na different random-number-generator seed each time, we run active learning with\neach acquisition function 100 times. We report the test accuracy (mean ± standard\nerror) as a function of the size of Dtrain.\nDiscussion Figure 7.2(b) shows a striking gap between BALD and EPIG in active\nlearning. Figures 7.1 and 7.2(a) provide some intuition about the underlying cause\nof this disparity: BALD has a tendency to acquire labels at the extrema of the input\nspace, regardless of their relevance to the predictive task of interest.\n7.3.5\nUCI Data (Figure 7.4)\nNext we compare BALD and EPIG in a broader range of settings. We use problems\ndrawn from a repository maintained at UC Irvine (UCI; Dua and Graff, 2017), which\nhas been widely used as a data source in past work on Bayesian methods [Gal and\nGhahramani, 2016a; Lakshminarayanan et al., 2017; Sun et al., 2018; Zhang et al., 2018].\nThe problems we use vary in terms of the number of classes, the input dimension and\nany divergence between the pool and target data distributions. We assume knowledge of\npeval(xeval) when estimating EPIG but note that this assumption has little significance\nif ppool(x) and peval(xeval) match, which is true for two out of the three problems.\nData. We use three classification datasets from the UCI repository, each with a\ndifferent number of classes, C, and input dimension, D: Magic (C = 2, D = 11),\nSatellite (C = 6, D = 36) and Vowels (C = 11, D = 10). The inputs are telescope\nreadings in Magic, satellite images in Satellite and speech recordings in Vowels. Magic\n7. Prediction- & Distribution-Aware Bayesian Active Learning\n125\n0\n100\n200\n300\nNumber of labels\n50\n60\n70\n80\nTest accuracy (%)\nRandom forest on Magic\n0\n100\n200\n300\nNumber of labels\n70\n80\nRandom forest on Satellite\n100\n200\n300\nNumber of labels\n30\n40\n50\nRandom forest on Vowels\n0\n100\n200\n300\nNumber of labels\n60\n70\nTest accuracy (%)\nNeural network on Magic\n0\n100\n200\n300\nNumber of labels\n70\n75\n80\n85\nNeural network on Satellite\n100\n200\n300\nNumber of labels\n30\n40\n50\nNeural network on Vowels\nAcquisition function\nrandom\nBALD\nEPIG\nFigure 7.4: EPIG outperforms or matches BALD across three standard classification\nproblems from the UCI machine-learning repository (Magic, Satellite and Vowels) and two\nmodels (random forest and neural network). See §7.3.5 for details.\nis interesting because it serves as a natural instance of a mismatch between pool\nand target distributions (see Appendix F.4.1).\nModels and Training. We use two different models. The first is a random forest\n[Breiman, 2001]. To emphasize that EPIG works with an off-the-shelf setup, we use the\nScikit-learn [Pedregosa et al., 2011b] implementation with its default parameters. The\nsecond model is a dropout-enabled fully connected neural network with three hidden\nlayers and a softmax output layer. A dropout rate of 0.1 is used during both training\nand testing. Training the neural network consists of running up to 50,000 steps of\nfull-batch gradient descent using a learning rate of 0.1. We use a loss function consisting\nof the negative log likelihood (NLL) of the training data combined with an l2 regularizer\n(with coefficient 0.0001) on the model parameters. To mitigate overfitting we use early\nstopping: we track the model’s NLL on a small validation set (approximately 20% of\nthe size of the training-label budget) and stop training if this NLL does not decrease\nfor more than 10,000 consecutive steps. We then restore the model parameters to the\nconfiguration that achieved the lowest validation-set NLL.\nActive Learning. We use largely the same setup as described in §7.3.4. Here the\nlabel budget is 300, and we run active learning 20 times with different seeds. We use\nthe same BALD and EPIG estimators as before, treating each tree in the random forest\nas a different θ value, and treating each stochastic forward pass through the neural\nnetwork (we compute 100 of them) as corresponding to a different θ value. To estimate\nEPIG we sample xeval from a set of inputs designed to be representative of peval(xeval).\nDiscussion. Figure 7.4 shows EPIG performing convincingly better than BALD in\nsome cases while matching it in others. These results provide broader validation of\nEPIG, complementing the results in Figure 7.2(b).\n7. Prediction- & Distribution-Aware Bayesian Active Learning\n126\n50\n100\n150\n200\n250\n300\nNumber of labels\n70\n80\n90\n100\nTest accuracy (%)\nCurated MNIST\n50\n100\n150\n200\n250\n300\nNumber of labels\nUnbalanced MNIST\n0\n50\n100\n150\n200\n250\n300\nNumber of labels\nRedundant MNIST\nAcquisition function\nrandom\nBALD\nEPIG\nFigure 7.5: EPIG outperforms BALD across three image-classification settings. Curated\nMNIST reflects the data often used in academic research.\nThe pool and target input\ndistributions, ppool(x) and peval(xeval) match; the marginal class distributions, ppool(y) and\npeval(yeval), are uniform. Unbalanced MNIST is a step closer to real-world data. While\npeval(yeval) remains uniform, ppool(y) is non-uniform: the pool contains more inputs from\nsome classes than others. Redundant MNIST simulates a separate practical problem. Whereas\npeval(yeval) only has nonzero mass on two classes of interest, ppool(y) has substantial mass\nacross all classes. See §7.3.6 for details.\n50\n100\n150\n200\n250\n300\nNumber of labels\n70\n80\n90\n100\nTest accuracy (%)\nCurated MNIST\n50\n100\n150\n200\n250\n300\nNumber of labels\nUnbalanced MNIST\n0\n50\n100\n150\n200\n250\n300\nNumber of labels\nRedundant MNIST\nAcquisition function\nEPIG\nentropy\nBADGE\nFigure 7.6: EPIG outperforms two acquisition functions popularly used as baselines in\nthe active-learning literature. The first is the model’s predictive entropy, H(p(y | y)) [Settles\nand Craven, 2008]. The second is BADGE [Ash et al., 2020]. Calculating BADGE involves\ncomputing a gradient-based embedding for each candidate input in the pool and then applying\nk-means++ initialization [Arthur and Vassilvitskii, 2007] in embedding space to select a\ndiverse batch of inputs for labelling. We acquire 10 labels at a time with BADGE.\n7.3.6\nMNIST Data (Figures 7.5 to 7.7)\nFinally, we evaluate BALD and EPIG in settings intended to capture challenges that\noccur when applying deep neural networks to high-dimensional inputs. Our starting\npoint is the MNIST dataset [LeCun et al., 1998], in which each input is an image of a\nhandwritten number between 0 and 9. This dataset has been widely used in related\nwork on Bayesian active learning with deep neural networks [Beluch et al., 2018; Gal\net al., 2017; Jeon, 2020; Lee and Kim, 2019; Tran et al., 2019]. We construct three\nsettings based on this dataset, each corresponding to a different practical scenario:\nCurated MNIST, Unbalanced MNIST and Redundant MNIST.\nAs well as investigating how BALD and EPIG perform across these settings, we\nseek to understand the effect on EPIG of varying the amount of knowledge we have of\nthe target data distribution, peval(xeval). To this end we assume we know this for one\nset of runs (Figure 7.5) and then relax this assumption for another set (Figure 7.7).\n7. Prediction- & Distribution-Aware Bayesian Active Learning\n127\n50\n100\n150\n200\n250\n300\nNumber of labels\n70\n80\n90\n100\nTest accuracy (%)\nCurated MNIST\n50\n100\n150\n200\n250\n300\nNumber of labels\nUnbalanced MNIST\n0\n50\n100\n150\n200\n250\n300\nNumber of labels\nRedundant MNIST\nAcquisition function\nEPIG with peval (xeval)\nEPIG with peval (yeval)\nEPIG with ppool(xeval)\nFigure 7.7: Even without knowledge of the target input distribution, peval(xeval), EPIG\nretains its strong performance on Curated MNIST, Unbalanced MNIST and Redundant\nMNIST. “EPIG with peval(xeval)” assumes exact samples from peval(xeval), as in Figure 7.5.\n“EPIG with peval(yeval)” corresponds to using the approximate-sampling scheme outlined in\n§7.3.1, using knowledge of peval(yeval). “EPIG with ppool(xeval)” corresponds to using samples\nfrom the pool as a proxy for peval(xeval). See §7.3.6 for details.\nData. Curated MNIST is intended to reflect the data often used in academic machine-\nlearning research. The pool and target class distributions, ppool(y) and peval(yeval),\nare both uniform over all 10 classes. In terms of class distributions, this effectively\nrepresents a worst-case scenario for active learning relative to random acquisition. Given\nmatching class-conditional input distributions, namely ppool(xeval | yeval) = peval(xeval |\nyeval), uniformly sampling from the pool input distribution, ppool(x), is equivalent\nto uniformly sampling from the target input distribution, peval(xeval). Thus, random\nacquisition is a strong baseline in this setting.\nUnbalanced MNIST is a step closer to real-world data. We might expect peval(yeval)\nto be uniform—that is, the task of interest might involve classifying examples in\nequal proportion from each class—but it could be difficult to curate a pool that is\nsimilarly uniform in its class distribution. To reflect this we consider a case with\na non-uniform ppool(()y): classes 0 to 4 each have probability 1/55 and classes 5\nto 9 each have probability 10/55.\nRedundant MNIST captures a separate practical problem that occurs, for instance,\nwhen using web-scraped data. The pool might contain inputs from many more classes\nthan we want to focus on in the predictive task of interest. To simulate this we suppose\nthat the task involves classifying just images of 1s and 7s, occurring in equal proportion—\nthat is, peval(yeval) places probability mass of 1/2 on class 1, 1/2 on class 7, and 0 on all\nother classes—while ppool(y) is uniform over all 10 classes. If the acquisition function\nselects an input from a class other than 1 and 7, the labelling function produces a\n“neither” label. Thus, we have three-way classification during training: 1 vs 7 vs neither.\nModel and Training. For both runs we use the same dropout-enabled convolutional\nneural network we already used in §4. The dropout rate here is 0.5. Training is similar\nto as described in §7.3.5, except that the learning rate is 0.01 and early stopping\ntriggers after 5,000 consecutive steps of non-decreasing validation-set NLL.\nActive Learning. Initially we retain the setup described in §7.3.5, with peval(xeval)\nknown (Figure 7.5). Then we investigate the sensitivity of EPIG to removing full access\nto peval(xeval), focusing on two different settings (Figure 7.7). In one setting, we assume\nknowledge of peval(yeval) and use the resampling technique discussed in §7.3.1. In the\nother, we simply sample target inputs from the pool: xeval ∼ppool(()xeval).\n7. Prediction- & Distribution-Aware Bayesian Active Learning\n128\n□7.2 – Expected Reduction of Epistemic Uncertainty\nBy rewriting the joint expected predictive information gain (JEPIG) as a triple\nmutual information which takes into account the model parameters we can rephrase\nit as a difference of two (parameter) EIG terms (dropping Dtrain for clarity):\nI[Y eval\n1..E ; Y acq\n1..K | xacq\n1..K, xeval\n1..E]\n= I[Y eval\n1..E ; Y acq\n1..K | xacq\n1..K, xeval\n1..E] −I[Y eval\n1..E ; Y acq\n1..K | xacq\n1..K, xeval\n1..E,ΩΩΩ]\n|\n{z\n}\n=0\n(7.40)\n= I[Y eval\n1..E ; Y acq\n1..K ;ΩΩΩ| xacq\n1..K, xeval\n1..E]\n(7.41)\n= I[Y acq\n1..K ;ΩΩΩ| xacq\n1..K]\n|\n{z\n}\n5\n−I[Y acq\n1..K ;ΩΩΩ| xacq\n1..K, Y eval\n1..E , xeval\n1..E]\n|\n{z\n}\n6\n.\n(7.42)\nSince BALD is known to measure epistemic uncertainty [Smith and Gal, 2018],\nwe can take another look at these two EIGs: Intuitively, the first EIG 5 is large\nwhen the model has high epistemic uncertainty about its prediction at xacq\n1..K, and\nlearning the true label would thus be informative for the model parameters, while\nthe second EIG 6 captures the epistemic uncertainty about the model’s prediction\nat xacq\n1..K assuming we had obtained labels for evaluation samples. This second term\nis small when x is similar to drawn evaluation samples and the model can explain\nit well given the pseudo-labels. Thus, JEPIG is large when the first term is large\nand the second term is small, so learning xacq\n1..K is informative for the model, and\nxacq\n1..K is similar to evaluation samples. In reverse, JEPIG is small, when knowing\nabout evaluation samples makes no difference for the epistemic uncertainty of the\nacquisition candidates, which means that they are unrelated.\nDiscussion. Figure 7.5 shows EPIG again outperforming BALD and random across\nall three dataset variants when given access to peval(xeval). (EPIG additionally beats\npredictive entropy [Settles and Craven, 2008] and BADGE [Ash et al., 2020], acquisition\nfunctions commonly studied in the active-learning literature, as shown in ??.) EPIG’s\nadvantage over BALD is appreciable on Curated MNIST and Unbalanced MNIST.\nBut it is emphatic on Redundant MNIST. This suggests EPIG is particularly useful\nwhen working with highly diverse pools.\nFigure 7.7 shows the even more impressive result that EPIG retains its strong\nperformance even when no access to peval(xeval) is assumed. We thus see that EPIG\nprovides a good degree of robustness in its performance to the level of knowledge\nabout the target data distribution.\n7.4\nJoint Expected Predictive Information Gain\nIn this section, we examine joint expected predictive information gain (JEPIG),\nI[Y eval\n1..E ; Y acq | xeval\n1..E, xacq, Dtrain],\n(7.43)\nin more detail. As explained in □7.2 Expected Reduction of Epistemic Uncertainty ,\nJEPIG also has an intuitive explanation as expected reduction in epistemic uncertainty\n7. Prediction- & Distribution-Aware Bayesian Active Learning\n129\nwhen taking into account evaluation samples.\nFirst, we will show that we can take the limit of the number of evaluation set samples\nto model peval(xeval). Then, we connect the criterion to Bayesian model selection and the\nmarginal log likelihood. Finally, we contrast JEPIG with BALD and EPIG and present\na practical reason why JEPIG might be easier to compute in the batch acquisition\nsetting. We report some early results on the performance of JEPIG in §7.4.4.\n7.4.1\nJEPIG for E →∞\nIf we do not have a finite number of evaluation samples but a distribution, we might\nwant to take the limit to take into account the full distribution. For countable many\nxeval, we can easily show that the limit exist.\nProposition 7.1. The following limit exists for countable peval(xeval):\nlim\nE→∞I[Y eval\n1..E ; Y acq | xeval\n1..E, xacq].\n(7.44)\nProof. Fix E > 1, and fix the order of xeval\n1..E. JEPIG for fixed E is upper-bounded by\nthe predictive entropy of the acquisition batch:\nI[Y eval\n1..E ; Y acq | xeval\n1..E, xacq] ≤H[Y acq | xacq],\n(7.45)\nand it is non-decreasing:\nI[Y eval\n1..E ; Y acq | xeval\n1..E, xacq]\n= I[Y eval\n1..E−1; Y acq | xeval\n1..E−1, xacq] + I[Y eval\nE\n; Y acq | xeval\nE\n, Y eval\n1..E−1, xeval\n1..E−1, xacq]\n|\n{z\n}\n≥0\n.\n(7.46)\nHence, the limit exists. However, it is not clear if the order of the evaluation samples\nmatters.\nTo show that the limits are equal for arbitrary orderings of the evaluation, let’s first\nfix a natural ordering (1, 2, . . .) and define:\nI(n) ≜I[Y eval\n1..n ; Y acq | xeval\n1..n, xacq].\n(7.47)\nLet (a(1), a(2), . . .) denote another arbitrary ordering, and let Ia(n) denote the respec-\ntive truncated JEPIG term:\nIa(n) ≜I[Y eval\na(1)..a(n); Y acq | xeval\na(1)..a(n), xacq].\n(7.48)\nFinally, let’s denote the limits for n →∞as I and Ia, respectively, which exist\naccording to the argument above. We can now show that I = Ia for all orderings a.\nFix ϵ ≥0. Then, ∃n ∀k ≥n : |Ia(k) −Ia(k)| ≤ϵ because of the limit. For any\nn, we choose an M such that {1, . . . , M} ⊇{a(1), . . . , a(n)}—we can simply take\nM ≜max(a(1), . . . , a(n)). Then, following the same argument as in Equation 7.46:\nI(K) ≥Ia(n)\n∀K ≥M.\n(7.49)\nFurther, ∀K ≥M, we can choose an N(K), such that {a(1), . . . , a(N)} ⊇{1, . . . , K}—\nas a is an ordering, it is bijective, and we can set N(K) ≜max(a−1(1), . . . , a−1(K))—\nand we obtain:\nIa ≥Ia(N(K)) ≥I(K).\n(7.50)\n7. Prediction- & Distribution-Aware Bayesian Active Learning\n130\nHence, we can sandwich I(K) between Ia(n) and Ia:\nIa ≥I(K) ≥Ia(n).\n(7.51)\nThis concludes the proof because for all K ≥M:\n|Ia −I(K)| ≤|Ia −Ia(n)| ≤ϵ,\n(7.52)\nand we obtain the limit I(K) →Ia. As already I(K) →I, we have Ia = I.\nWe conjecture that the limit also exists for continuous peval(xeval).\n7.4.2\nJEPIG as Bayesian Model Selection\nJEPIG can also be viewed as a reduction in expected marginal log likelihood for\nthe evaluation set:\nhow much does acquiring a label for an acquisition sample\nimprove the model’s ability to adapt to the evaluation set? We can view this as\nBayesian model selection:\nIn Bayesian model selection, for a set of models Mi and a random variable M\nthat represents the chosen model, we are interested in inferring p(M | Dtrain). This\nquantifies the model that is best for our purposes.\nConventionally, the marginal\nlikelihood p(Dtrain | M) is maximized, which assumes a uniform prior distribution over\nM [MacKay, 2003]. The marginal likelihood does not actually measure the performance\nof the fully trained model but its generalization ‘speed’ [Lyle et al., 2020]. To measure\nthe performance of a model, we can use cross-validation, which can be cast into a\nBayesian setting [Fong and Holmes, 2020]. This was also explored in [Lotfi et al., 2022]\n(but see also Kirsch [2022] for a critical review). We explore aspects of this further in §6.\nWhen Deval ⊆Dpool, maximizing JEPIG performs Bayesian model selection towards\nselecting the best acquisition batch in expectation: we precisely perform Bayesian\nmodel selection in the above sense by trying to find the acquisition batch that will help\nthe model best adapt to future data points (assuming we aim to acquire all pool points\neventually.) Obviously, this is not true when the evaluation set is disjoint from the pool.\n7.4.3\nJEPIG & EPIG vs. BALD\nFrom Figure 7.3, we already know that BALD upper-bounds both JEPIG and EPIG.\nLet us examine this in more detail. From Equation 7.42 from the intuition box for\nJEPIG, we also see that we have\nI[Y eval\n1..E ; Y acq\n1..K ;ΩΩΩ| xacq\n1..K, xeval\n1..E, Dtrain]\n≤I[Y acq\n1..K ;ΩΩΩ| xacq\n1..K, Dtrain],\n(7.53)\nthat is, JEPIG is upper-bounded by BALD—a two-term mutual information is\nalways non-negative—and JEPIG is equivalent to BALD exactly when I[Y acq\n1..K ;ΩΩΩ|\nxacq\n1..K, Y eval\n1..E , xeval\n1..E, Dtrain] is zero. This is the case when either there is no uncertainty\nabout the parameters left, or when the distribution of Y acq\n1..K | xacq\n1..K is fully determined\nby conditioning the posterior on yeval\n1..E | xeval\n1..E ∼p(yeval\n1..E | xeval\n1..E, Dtrain). When MacKay\n[1992b] analyzed JEPIG, they stated that it was equivalent to BALD because in\nthe specific context of that work, using Bayesian linear regression with simple ho-\nmoscedastic noise, it was.\n7. Prediction- & Distribution-Aware Bayesian Active Learning\n131\nBALD and JEPIG will trivially be equal when xacq\n1..K ⊆xeval\n1..E. In reverse, for non-\nparametric models, this will trivially not be the case when xacq\n1..K is not ‘near’ the\nevaluation set. Intuitively, JEPIG is different from BALD exactly when BALD fails:\nfor outlier pool samples which are not similar to the test–time distribution, in which\ncase JEPIG will tend towards 0 as the two terms cancel out.\nFinally, a qualitative result:\nProposition 7.2. EPIG lower-bounds ‘averaged’ JEPIG:\nI[Y eval; Y acq | Xeval, xacq, Dtrain] ≤1/E I[Y eval\n1..E ; Y acq | xeval\n1..E, xacq] + ceval,\n(7.54)\nup to an additive constant (ceval) that only depends on the evaluation samples and is\nindependent of xacq. The inequality gap is the total correlation:\n1/E TC[Y eval\n1\n; . . . ; Y eval\nE\n| xeval\n1..E, Y acq, xacq]\n(7.55)\nWe have equality when it is zero, that is when the predictions on the evaluation set are\nindependent (given the acquisition samples).\nSee Appendix F.5 for the formal proof. This result might seem unintuitive given that\nwe have the opposite for BatchBALD in §4. However, it is important to notice that the\nredundancy that JEPIG removes is in the evaluation set. In the case of BatchBALD, the\nredundancy is in the acquisition set. Hence, the different direction of the inequality sign.\nBatch Acquisition using EPIG. A practical reason for examining JEPIG is\nbatch active learning with EPIG. While the expectation in EPIG can be evaluated for\nindividual acquisition, the batch setting is computationally more complex. Following\n§4, for batch acquisition, we need to maximize I[Y eval; Y acq\n1..K | Xeval, xacq\n1..K, Dtrain]. Unlike\nthe expected information gain, this term is not submodular. However, as the global\nsubset problem is not feasible, we will examine the case of using greedy selection\nnonetheless. Computing I[Y eval; Y acq\n1..K | Xeval, xacq\n1..K, Dtrain] requires estimating a joint\ndensity in i+1 many variables for each xeval sample for the i-th element in the acquisition\nbatch: p(yeval, yacq\n1..K | xeval, xacq\n1..K). Overall, for an acquisition batch of size B, this requires\nO(|Deval| |Dpool|B) many joint densities with 2 to B + 1 many variables compared to\nO(|Dpool|B) for BatchBALD with 1 to B many variables. Unfortunately, BatchBALD\nhas already been found to be computationally intractable for larger acquisition batch\nsizes in practice as noted in §4 and 5, and here we consider an additional variable by\ndefault, while we also have to evaluate this term for many xeval. This does not spark joy.\nEvaluation of JEPIG. We could maximize I[Y eval\n1..E ; Y acq | xeval\n1..E, xacq, Dtrain] via\nI[Y eval\n1..E ; Y acq | xeval\n1..E, xacq, Dtrain] =\n= H[Y acq | xacq, Dtrain]\n−H[Y acq | xacq, Y eval\n1..E , xeval\n1..E, Dtrain].\n(7.56)\nNote that the second term is expensive to evaluate:\nH[Y acq\n1..K | xacq\n1..K, Y eval\n1..E , xeval\n1..E, Dtrain] =\n= Ep(yeval\n1..E |xeval\n1..E ,Dtrain) H[Y acq\n1..K | xacq\n1..K, yeval\n1..E , xeval\n1..E, Dtrain].\n(7.57)\nThat is, we need to train a model on Dtrain then sample joint label predictions yeval\n1..E\nfor the evaluation set xeval\n1..E using the model, and then, for each such sampled joint\n7. Prediction- & Distribution-Aware Bayesian Active Learning\n132\nprediction, we evaluate the conditional entropy H[Y acq\n1..K | xacq\n1..K,ΩΩΩ′] using new ωωω′ ∼p(ωωω |\nyeval\n1..E , xeval\n1..E, Dtrain), which requires additional training using the labeled training set\naugmented with the sampled labels Dtrain ∪{yeval\ni\n, xeval\ni\n}i.\nIn appendix §F.6, we present a computationally tractable approximation for this\nterm using ‘self-distillation‘, where we train a new model on predictions for evaluation\nsamples from the current model trained on Dtrain. On a high-level, this yields a model\nthat fulfills the intuitions presented in □7.2 Expected Reduction of Epistemic Uncertainty .\nWe refer to the appendix for more details.\n7.4.4\nEmpirical Validation\nWe evaluate the performance of JEPIG using a form of self-distillation described in\nappendix §F.6 in regular active learning and under distribution shift. Moreover, we\nprovide an ablation with different evaluation set sizes. We use approximate BNNs\nbased on MC dropout, see §1.2.2.\nSetup. We compare EPIG and JEPIG with different acquisition sizes to BALD,\neither using the top-k of individual scores [Gal and Ghahramani, 2016a], the batch\nversion BatchBALD (§4)—which is equivalent to the previous for individual acquisition—\nor SoftmaxBALD (§5) for larger acquisition batch sizes.\nSoftmaxBALD samples\nwithout replacement from the pool set using the Softmax of the acquisition scores\nwith temperature 8.\nOn MNIST and MNISTx2 (Repeated-MNIST), we use a LeNet-5 model [LeCun\net al., 1998], which we train as described in §4.\nFor CIFAR-10 [Krizhevsky, 2009], we use a ResNet18 model [He et al., 2016] which\nwas modified as described in §4 to add MC dropout to the classifier head and also\nfollows the described training regime. We train with an acquisition batch size of 250\nand an initial training set size of 1000. We use MC dropout models with 100 dropout\nsamples when computing the acquisition scores.\nPerformance on Regular Active Learning. We evaluate whether ignoring\nthe test–time input distribution has a detrimental effect on BALD even when the\npool set distribution matches the test distribution.\nFor this, we compare BALD and JEPIG in Figure 7.10 on MNISTx2 and EPIG\nand JEPIG in Figure 7.8 on MNIST. In the regular active learning setup, there is no\ndistribution shift between the pool set and test set, so we use the whole unlabeled\npool set as evaluation set.\nBoth in the top-k and the batch variant, EPIG and JEPIG outperform BALD on\nMNISTx2 (and also MNIST, not shown). On CIFAR-10, JEPIG also outperforms\nSoftmaxBALD, as depicted in Figure 7.9.\nHowever, why does JEPIG outperform EPIG? We hypothesize that this is because\nEPIG takes an expectation over the evaluation set using individual points which by\nitself might be myopic. It might work well for simple models, but taking the whole\nevaluation set into account and retraining the model might allow for learning better\nabstraction in deep models, which might not be the case for EPIG.\nPerformance on Active Learning under Distribution Shift with MNIST\nand FashionMNIST. We want to evaluate how BALD and JEPIG behave under\ndistribution shift, that is when pool set and test distribution do not match. For this,\nwe add junk out-of-distribution data to the pool set. In this experiment, the pool set\ncontains MNIST and FashionMNIST [Xiao et al., 2017] while the test set contains\n7. Prediction- & Distribution-Aware Bayesian Active Learning\n133\n20\n40\n60\n80\n100\n120\nTraining Set Size (Individual Acquisition)\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\nAccuracy\nAcquisition Function\nEPIG (ours)\nJEPIG (ours)\nBALD\nFigure 7.8:\nEPIG vs JEPIG vs BALD\nwith Bayesian Neural Networks on MNIST.\nJEPIG performs better under than MC\nDropout than EPIG.\n2500\n5000\n7500\n10000\n12500\n15000\n17500\n20000\nTraining Set Size\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\nAccuracy\nAcquisition Function\nSoftmaxJEPIG (ours)\nSoftmaxBALD\nUniform\nFigure 7.9: BALD vs JEPIG on CIFAR-\n10. JEPIG outperforms BALD. 5 trials each.\nWith batch acquisition size 250, and initial\ntraining size 1000. Median accuracy after\nsmoothing with a Parzen window filter over\n30 acquisition steps to denoise.\n50\n100\n150\n200\n250\n300\nTraining Set Size\n0.5\n0.6\n0.7\n0.8\n0.9\nAccuracy\nAcquisition Size = 5\n50\n100\n150\n200\n250\n300\nTraining Set Size\nAcquisition Size = 10\nAcquisition Function\nBatchJEPIG (ours)\nBatchBALD\nJEPIG (ours)\nUniform\nBALD\nFigure 7.10: (Batch)BALD vs (Batch)JEPIG on Repeated-MNIST (MNISTx2). JEPIG\noutperforms BALD.\none or the other. We deal with an acquisition function attempting to acquire OoD\ndata in two different modes: OoD rejection rejects OoD data from the batch and\ndoes not acquire it; while OoD exposure acquires OoD data with uniform targets,\nsimilar to outlier exposure methods in OoD detection [Hendrycks et al., 2019]. We\nuse an evaluation set with 2000 unlabeled samples.\nJEPIG outperforms BALD on in all combinations, see Figure 7.11(a). In all cases\nbut one, JEPIG acquires fewer junk/OoD samples, see Figure 7.11(b). The ablation in\nFigure 7.13 shows that larger evaluation sets are beneficial. Note that the evaluation\nset is unlabeled and thus does not count towards sample acquisitions.\nFor CIFAR-10 and SVHN [Netzer et al., 2011], JEPIG outperforms BALD under\ndistribution shift in all but one combination and selects fewer OoD samples, see\nFigure 7.12. We use an evaluation set with 1000 unlabeled samples.\n7. Prediction- & Distribution-Aware Bayesian Active Learning\n134\n50\n100\n150\n200\n0.7\n0.8\n0.9\nAccuracy\nOoD Exposure: MNIST (iD) + FashionMNIST (OoD)\n50\n100\n150\n200\n0.7\n0.8\n0.9\nOoD Rejection: MNIST (iD) + FashionMNIST (OoD)\n50\n100\n150\n200\nTotal Acquisitions\n0.60\n0.65\n0.70\n0.75\nAccuracy\nOoD Exposure: FashionMNIST (iD) + MNIST (OoD)\n50\n100\n150\n200\nTotal Acquisitions\n0.600\n0.625\n0.650\n0.675\n0.700\nOoD Rejection: FashionMNIST (iD) + MNIST (OoD)\nAcquisition Function\nBatchJEPIG\nBatchBALD\n(a) Accuracy\n50\n100\n150\n200\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAcquired OoD Fraction\nOoD Exposure: MNIST (iD) + FashionMNIST (OoD)\n50\n100\n150\n200\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nOoD Rejection: MNIST (iD) + FashionMNIST (OoD)\n50\n100\n150\n200\nTotal Acquisitions\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAcquired OoD Fraction\nOoD Exposure: FashionMNIST (iD) + MNIST (OoD)\n50\n100\n150\n200\nTotal Acquisitions\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nOoD Rejection: FashionMNIST (iD) + MNIST (OoD)\nAcquisition Function\nBatchJEPIG\nBatchBALD\n(b) Acquired OoD Ratio\nFigure 7.11: MNIST and FashionMNIST pairings with OoD rejection or exposure. JEPIG\nperforms better than BALD. 5 trials.\n50\n100\n150\n200\n0.10\n0.15\n0.20\n0.25\n0.30\nAccuracy\nOoD Rejection: SVHN (iD) vs CIFAR-10 (ooD)\n50\n100\n150\n200\n0.10\n0.15\n0.20\n0.25\n0.30\nOoD Exposure: SVHN (iD) vs CIFAR-10 (ooD)\n50\n100\n150\n200\nTotal Acquisitions\n0.20\n0.25\n0.30\nAccuracy\nOoD Rejection: CIFAR-10 (iD) vs SVHN (ooD)\n50\n100\n150\n200\nTotal Acquisitions\n0.20\n0.25\n0.30\nOoD Exposure: CIFAR-10 (iD) vs SVHN (ooD)\nAcquisition Function\nBatchJEPIG\nBatchBALD\n(a) Accuracy\n50\n100\n150\n200\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nAcquired OoD Fraction\nOoD Rejection: SVHN (iD) vs CIFAR-10 (ooD)\n50\n100\n150\n200\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nOoD Exposure: SVHN (iD) vs CIFAR-10 (ooD)\n50\n100\n150\n200\nTotal Acquisitions\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nAcquired OoD Fraction\nOoD Rejection: CIFAR-10 (iD) vs SVHN (ooD)\n50\n100\n150\n200\nTotal Acquisitions\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nOoD Exposure: CIFAR-10 (iD) vs SVHN (ooD)\nAcquisition Function\nBatchJEPIG\nBatchBALD\n(b) Acquired OoD Ratio\nFigure 7.12: CIFAR-10 and SVHN pairings with OoD rejection or exposure.\nJEPIG\nperforms better than BALD. 5 trials. Acquisition size 5. Initial training size 5.\n7. Prediction- & Distribution-Aware Bayesian Active Learning\n135\n50\n100\n150\n200\n0.60\n0.65\n0.70\n0.75\nAccuracy\nOoD Exposure: FashionMNIST (iD) + MNIST (OoD)\n50\n100\n150\n200\n0.575\n0.600\n0.625\n0.650\n0.675\n0.700\nOoD Rejection: FashionMNIST (iD) + MNIST (OoD)\n50\n100\n150\n200\nTotal Acquisitions\n0.7\n0.8\n0.9\nAccuracy\nOoD Exposure: MNIST (iD) + FashionMNIST (OoD)\n50\n100\n150\n200\nTotal Acquisitions\n0.7\n0.8\n0.9\nOoD Rejection: MNIST (iD) + FashionMNIST (OoD)\nEvaluation Set Size\n10\n250\n2000\nFigure 7.13: Evaluation Set Size Ablation. MNIST and FashionMNIST pairings with OoD\nrejection or exposure. A larger evaluation set performs better. 5 trials.\n7. Prediction- & Distribution-Aware Bayesian Active Learning\n136\n7.5\nRelated Work\nThe idea of using the EIG to quantify the utility of data was introduced by Lindley\n[1956] and has a long history of use in experimental design [Chaloner and Verdinelli,\n1995a; Rainforth et al., 2023]. The framework of Bayesian experimental design has many\napplications outside active learning, and in these applications the model parameters are\ncommonly the quantity of interest—Bayesian optimization [Hennig and Schuler, 2012;\nHernández-Lobato et al., 2014; Villemonteix et al., 2009] being a notable exception.\nThe EIG in the parameters is thus often a natural acquisition function.\nThe EIG in the parameters was originally suggested as an acquisition function for\nactive learning by MacKay [1992a,b], who called it the total information gain. Despite\nits shortfalls, some of which were also briefly discussed by Freund et al. [1997] and\nMacKay [1992b] for special cases such as Bayesian linear regression, it has been widely\nused in cases where the model’s predictions, not the model parameters, are the ultimate\nobjects of interest [Atighehchian et al., 2020; Beluch et al., 2018; Gal et al., 2017;\nHoulsby et al., 2011; Jeon, 2020; Lee and Kim, 2019; Munjal et al., 2022; Pinsler et al.,\n2019; Shen et al., 2018; Siddhant and Lipton, 2018; Tran et al., 2019].\nMaximizing the information gathered about a quantity other than the model\nparameters has been proposed a number of times as an approach to active learning.\nThe predictive information as mutual information between the past and future was\nintroduced by Bialek and Tishby [1999] and has been used to increase sample efficiency\nin reinforcement learning [Lee et al., 2020]. Perhaps most relevant to this chapter,\nMacKay [1992a,b] also discussed two other acquisition functions, namely the mean\nmarginal information gain and the joint information gain, which correspond to EPIG\nand JEPIG, respectively.\nThe mean marginal information gain measures the average information gain in the\npredictions made on a fixed set of inputs based on a Gaussian approximation of the\nposterior over the model parameters. Though the mean marginal information gain has\nsince received surprisingly little attention in the literature, it was discussed by Huszár\n[2013] and later used by Wang et al. [2021] to evaluate the quality of predictive-posterior\ncorrelations. Wang et al. [2021] extended the mean marginal information gain to the\nbatch setting following insights from BatchBALD for regression tasks and evaluated\nit in a transductive active learning setting. Wang et al. [2021] only examine the case\nwhere pool and target input distribution are identical. The transductive approach to\nactive learning [Vapnik, 2006; Yu et al., 2006] seeks to maximize the performance on a\nfixed set of inputs—in contrast with the input distribution considered by EPIG.\nThe joint information gain has received even less attention than the mean marginal\ninformation gain in the literature. This might be because MacKay showed that it\nis equivalent to BALD when assuming constant aleatoric noise (with a sufficiently\nlarge number of evaluation samples). Constant aleatoric noise is also a common and\nconvenient choice for Gaussian Processes, which could explain why it was not considered\nby works in Bayesian optimization either. However, for deep neural networks, constant\naleatoric noise is not a common assumption, and indeed the main benefit of using\nBALD over entropy is that it performs well when aleatoric uncertainty varies across\nsamples because it estimates epistemic uncertainty and not aleatoric uncertainty (see\nalso §3). Moreover, our detailed re-examination of the relationship between BALD and\nJEPIG reaches a more varied conclusion than MacKay [1992b] by specifically taking\ninto account the support of pool and evaluation samples.\n7. Prediction- & Distribution-Aware Bayesian Active Learning\n137\nAside from the work of MacKay [1992a,b], there are numerous prediction-oriented\nmethods [Afrabandpey et al., 2019; Chapelle, 2005; Cohn, 1993; Cohn et al., 1996;\nDaee et al., 2017; Donmez and Carbonell, 2008; Evans et al., 2015; Filstroff et al.,\n2021; Krause et al., 2008; Neiswanger et al., 2021; Seo et al., 2000; Sundin et al.,\n2018, 2019; Tan et al., 2021; Yu et al., 2006; Zhao et al., 2021a,b,c; Zhu et al., 2003].\nMany of these, with notable examples including the work of Cohn et al. [1996], Krause\net al. [2008] and Neiswanger et al. [2021], are tied to a particular model class or\napproximation scheme and so lack EPIG’s generality.\nThere is an additional limitation associated with techniques based on the idea,\ndue to Roy and McCallum [2001], of measuring the expected loss reduction that\nwould result from updating the model on a given input-label pair. These techniques\noften require updating the model within the computation of the acquisition function,\nwhich can be extremely expensive. Despite a strong conceptual connection to the\nacquisition function proposed by Roy and McCallum [2001], EPIG allows a significantly\nlower computational cost: its information-theoretic formulation allows us to derive\nan estimator that does not require nested model updating.\nConcurrently to Kirsch [2021] (which this chapter derives from), Neiswanger et al.\n[2021] introduce Bayesian Algorithm Execution (BAX), which goes beyond traditional\nBayesian optimization by steering the acquisition for estimating computable properties\nof black-box functions. This is achieved without the need to modify the underlying\nalgorithm. Instead, BAX uses an information-theoretic approach to guide the selection\nof queries, based on the mutual information between the potential queries and the\ncomputable properties. This is approximated with samples from execution paths of\nthe algorithm computing the property of interest. BAX expands the applicability\nof Bayesian experimental design principles to a wider array of practical problems.\nWithin this context, EPIG and JEPIG can be seen as a special case of BAX, where\nthe computable property of interest is the model’s predictive performance on a set or\ndistribution of inputs. However, the methods in this chapter specifically make us of\nthe Bayesian model’s predictive joint distribution to reduce the computational cost.\nUnlike much active learning literature whose experiment setting implicitly assumes\nthat that pool and test samples are drawn from the same distribution, EPIG and\nJEPIG support the setting in which pool and test distribution do not match. This\nis not the case for other diversity-based active learning methods such as BADGE\n[Ash et al., 2020], for example, which implicitly use the empirical pool set distribution\nto select diverse samples via clustering.\n7.6\nDiscussion\nWe have demonstrated that BALD, a widely used acquisition function for Bayesian\nactive learning, can be suboptimal.\nWhile much of machine learning focuses on\nprediction, BALD targets information gain in a model’s parameters in isolation and\nso can seek labels that have limited relevance to the predictions of interest.\nMotivated by this, we have proposed EPIG, an acquisition function that targets\ninformation gain in terms of predictions. Our results show EPIG outperforming BALD\nacross a number of data settings (low- and high-dimensional inputs, varying degrees\nof divergence between the pool and target data distributions, and varying degrees\nof knowledge of the target distribution) and across multiple different models. This\n7. Prediction- & Distribution-Aware Bayesian Active Learning\n138\nsuggests EPIG can serve as a compelling drop-in replacement for BALD, with particular\nscope for performance gains when using large, diverse pools of unlabeled data.\nWhile EPIG can be evaluated efficiently in the individual acquisition case, it becomes\nvery costly for batch acquisition (when using the same estimators for classification). Our\ninitial investigations of JEPIG with self-distillation shows it to be an approximation\nthat is also well motivated and potentially faster to evaluate in the batch setting.\nHowever, it requires training an additional model. While this could be sped up by\nusing warm-starting [Ash and Adams, 2020], it is still a significant cost. At the same\ntime, two (Batch)BALD terms need to be computed for JEPIG which can also be slow\nand which does not scale well beyond small acquisition batch sizes. Thus, EPIG and\nJEPIG represent a trade-off between the time it takes to compute the expectation over\nevaluation samples and training another model. In the case of individual acquisition,\nEPIG is strongly favored. In §5, we discuss a simple stochastic extension to avoid\ncomputing BatchBALD terms. We do not examine this approach in detail for the\nacquisition functions presented in this chapter.\nImportantly, unlike BALD, EPIG is not submodular, and thus greedy acquisition is\nnot guaranteed to obtain 1 −1\ne optimality for batch acquisition, even though we have\nnot experienced any degradation in comparison with BALD/BatchBALD empirically\nin our initial experiments. Indeed, JEPIG performs on par or better than BALD in\nthe regular active learning case without distribution shift. At the same time, while\nneither BALD nor JEPIG are adaptive submodular, and no statements about global\noptimality have been presented so far [Golovin and Krause, 2011; Foster, 2022], this\nhas not been an issue in practice. Ash et al. [2021] recently introduced a new forward-\nbackward strategy for a weight-space version of JEPIG. We leave an evaluation using\nthe estimators we have used to future work.\nFinally, we present two scenarios where EPIG might not perform better than\nBALD: Firstly, if a task’s performance is dominated by a more general “sub-task”,\nthere might be many samples highly informative for this general sub-task. For image\ndata, this could be the case when feature learning is of particular importance, and\nconvolution kernels can be learned from image data, no matter the label or actual task.\nIn this regime, both BALD and EPIG will perform similarly, yet will likely outperform\nrandom acquisition. Secondly, if the model’s architecture and its inductive biases have\nspecifically been evolved for a task (and dataset), there will be a high overlap between\nthe model parameters and the task’s target input distribution as model architectures\nwhich converge faster are preferred as research outputs. BALD and EPIG might\nperform similarly in this case, too. That is, prediction-oriented acquisition functions\nmight show their strength when the task performance is highly specific and dependent\non specific samples in the pool set, and the model’s architecture is over-parameterized\nand not yet adapted to the task. We leave investigation of this for the future.\nI am always in front of you but can never be seen.\nWhat am I?\n8\nPrioritized Data Selection during Training\nNow, let us take a look at active sampling. We will apply information-theoretic ideas\nto it. Active sampling is important because state-of-the-art models such as GPT-3\n[Brown et al., 2020], CLIP [Radford et al., 2021], and ViT [Dosovitskiy et al., 2020]\nachieve remarkable results by training on vast amounts of web-scraped data, yet despite\nintense parallelization, training such a model takes weeks or months [Radford et al.,\n2021; Chowdhery et al., 2022]. Even practitioners who work with smaller models\nface slow development cycles, due to numerous iterations of algorithm design and\nhyperparameter selection. As a result, the total time required for training is a core\nconstraint in the development of such deep learning models.\nIf it further sped up training, practitioners with sufficient resources would use much\nlarger batches and distribute them across many more machines [Anil et al., 2018].\nHowever, this has rapidly diminishing returns [LeCun et al., 2012], to a point where\nadding machines does not reduce training time [McCandlish et al., 2018; Anil et al.,\n2018]—see e.g. GPT-3 and PaLM [Chowdhery et al., 2022].\nAdditional machines can, however, still speed up training by filtering out less\nuseful samples [Alain et al., 2015]. Many web-scraped samples are noisy, i.e. their\nlabel is incorrect or inherently ambiguous. For example, the text associated with a\nweb-scraped image is rarely an accurate description of the image. Other samples are\nlearned quickly and are then redundant. Redundant samples are commonly part of\nobject classes that are over-represented in web-scraped data [Tian et al., 2021], and\nthey can often be left out without losing performance. Given that web-scraped data is\nplentiful—often enough to finish training in a single epoch [Komatsuzaki, 2019; Brown\net al., 2020]—one can afford to skip less useful points.\nHowever, there is no consensus on which data points are the most useful. Some\nworks, including curriculum learning, suggest prioritizing easy points with low label\nnoise before training on all points equally [Bengio et al., 2009]. While this approach\nmay improve convergence and generalization, it lacks a mechanism to skip points\nthat are already learned (redundant). Other works instead suggest training on points\nthat are hard for the model, thereby avoiding redundant points, whose loss cannot\nbe further reduced. Online batch selection methods [Loshchilov and Hutter, 2015;\nKatharopoulos and Fleuret, 2018; Jiang et al., 2019] do so by selecting points with\nhigh loss or high gradient norm.\nWe show two failure modes of prioritizing hard examples. Firstly, in real-world noisy\ndatasets, high loss examples may be mislabeled or ambiguous. Indeed, in controlled\nexperiments, points selected by high loss or gradient norm are overwhelmingly those with\nnoise-corrupted labels. Our results show that this failure mode degrades performance\n8. Prioritized Data Selection during Training\n140\n0\n20000\n40000\n60000\n80000\nTraining steps\n60\n62\n64\n66\n68\n70\n72\nTest accuracy (%)\n18x speedup\nRHO-LOSS\nselection (ours)\nUniform random\ndata selection\nMaximum accuracy\nreached by uniform\nFigure 8.1: Speedup on large-scale classification of web-scraped data (Clothing-\n1M). RHO-LOSS trains all architectures with fewer gradient steps than standard uniform\ndata selection (i.e. shuffling), helping reduce training time. Thin lines: ResNet-50, MobileNet\nv2, DenseNet121, Inception v3, GoogleNet, mean across seeds. Bold lines: mean across all\narchitectures.\nseverely. More subtly, we show that some samples are hard because they are outliers—\npoints with unusual features that are less likely to appear at test time. For the aim\nof reducing test loss, such points are less worth learning.\nTo overcome these limitations, we introduce the reducible holdout loss selection\n(RHO-LOSS) in this chapter. We propose a selection function grounded in probabilistic\nand information-theoretic modelling that quantifies by how much each point would\nreduce the loss on unseen data if we were to train on it, without actually training\non it. We show that optimal points for reducing holdout loss are non-noisy, non-\nredundant, and task-relevant. To approximate optimal selection, we derive an efficient\nand easy-to-implement selection function: the reducible holdout loss.\nWe explore RHO-LOSS in extensive experiments on 7 datasets. We evaluate the\nreduction in required training steps compared to uniform sampling and state-of-the-\nart batch selection methods. Our evaluation includes Clothing-1M, the main large\nbenchmark with noisy, web-scraped labels, matching our main application. RHO-LOSS\nreaches target accuracy in 18x fewer steps than uniform selection and achieves 2% higher\nfinal accuracy (Figure 8.1). Further, RHO-LOSS consistently outperforms prior art\nand speeds up training across datasets, modalities, architectures, and hyperparameter\nchoices. Explaining this, we show that methods selecting “hard” points prioritize noisy\nand less relevant examples. In contrast, RHO-LOSS chooses low-noise, task-relevant,\nnon-redundant points—points that are learnable, worth learning, and not yet learned.\n8.1\nActive Sampling: Online Batch Selection\nUnlike the setting we use in most of the other chapters, which we introduced in\n§1.2.2, active sampling follows a different paradigm by virtue of being an online\n8. Prioritized Data Selection during Training\n141\nalgorithm and using labeled data. We will try to use most of the notation from\n§1.2.2 and §1.2.4; however, note that the labels are available in the pool set and\nevaluation set in this chapter.\nWe consider a model p(y | x, θ) with parameters θ that we want to train using\nstochastic gradient descent (SGD) on a subset of samples from the available labeled\ndata Dpool = {(xi, yi)}M\ni=1 using the cross-entropy loss. At each training step t, we\nload a batch bt of size K from Dpool. In online batch selection [Loshchilov and Hutter,\n2015], we uniformly pre-sample a larger batch Bt of size K′ ≫K. Then, we construct a\nsmaller batch bt that consists of the top-ranking K points in Bt ranked by a label-aware\nselection function S(xi, yi). We perform a gradient step to minimize a mini-batch\nloss H[yi | xi, θ] summed over i ∈bt. The next large batch Bt+1 is then pre-sampled\nfrom Dpool without replacement of previously sampled points (points are only replaced\nat the start of the next epoch).\n8.2\n(Joint) Predictive Information Gain & Reducible\nHoldout Loss Selection\nPrevious online batch selection methods, such as loss or gradient norm selection, aim\nto select points that, if we were to train on them, would minimize the training set\nloss. [Loshchilov and Hutter, 2015; Katharopoulos and Fleuret, 2018; Kawaguchi and\nLu, 2020; Alain et al., 2015]. Instead, we aim to select points that minimize the loss\non a holdout set. In spirit of this thesis, we will refer to this holdout set as a labeled\nevaluation set1 Deval. It would be too expensive to naively train on every candidate\npoint and evaluate the holdout loss each time.\nIn this section, we show how to (approximately) find the points that would most\nreduce the holdout loss if we were to train the current model on them (without actually\ntraining on them). For simplicity, we first assume only one point (x, y) ∈Bt is selected\nfor training at each time step t (we discuss selection of multiple points below). p(y′ |\nx′; Dtrain) is the predictive distribution of the current model, where Dtrain is the sequence\nof data the model was trained on before training step t. Deval = {(xeval\ni\n, yeval\ni\n)}E\ni=1,\nwritten as xeval\n1..E and yeval\n1..E , respectively, for brevity, is a labeled evaluation set drawn\nfrom the same data-generating distribution ptrue(x′, y′) as is the set of available training\ndata Dpool—this is the holdout set.\nReduction in Holdout Loss. We aim to acquire the point (x, y) ∈Bt that, if\nwe were to train on it, would minimize the cross-entropy loss on the holdout set,\nour labeled evaluation set:\narg min\n(x,y)∈Bt\nH(peval(xeval, yeval) ∥p(yeval | xeval; (y, x), Dtrain)).\n(8.1)\nThinking back to §7, we see that in the active learning case, we were looking for a\nreduction in expected loss, while here we are looking for a reduction in the holdout loss.\n(Joint) Predictive Information Gain. Using the notation from §2, the cross-\nentropy loss over the empirical Deval is just:\nH(peval(xeval, yeval) ∥p(yeval | xeval, (y, x), Dtrain))\n(8.2)\n1Supposedly, we should then also rename the loss to REV-LOSS in this thesis, but let us not.\n8. Prioritized Data Selection during Training\n142\n= Epeval(xeval,yeval)[H[yeval\n1..E | xeval\n1..E, (y, x), Dtrain]].\n(8.3)\nThus, we can write the minimization above as a maximization of a (point-wise) mutual\ninformation analogously to derivations in §7:\narg min Epeval(xeval,yeval)[H[yeval | xeval, (y, x), Dtrain]]\n(8.4)\n= arg max Epeval(xeval,yeval)[I[yeval; y | xeval, x, Dtrain]].\n(8.5)\nThis is the predictive information gain:\nDefinition 8.1. The predictive information gain (PIG) of a point (x, y) is defined as:\nPIG(x, y) = Epeval(xeval,yeval)[I[yeval | xeval, (y, x), Dtrain]].\n(8.6)\nThe motivation for it is the same as for EPIG in §7.3 with the crucial different\nlabel information is available here, making batch evaluation easier.\nSimilarly, we can define a joint predictive information gain.\nDefinition 8.2. The joint predictive information gain (JPIG) of a point (x, y) is\ndefined as:\nJPIG(x, y) = I[yeval\n1..E | xeval\n1..E, (y, x), Dtrain]\n(8.7)\n= H[yeval\n1..E | xeval\n1..EDtrain] −H[yeval\n1..E | xeval\n1..E, (y, x), Dtrain]\n(8.8)\n= −log p(yeval\n1..E | xeval\n1..E, Dtrain) −(−log p(yeval\n1..E | xeval\n1..E, (y, x), Dtrain)).\n(8.9)\nThe motivation for JPIG is also the same as for JEPIG in §7.4.2, again with\nthe crucial difference that label information is available here. JPIG is even closer to\nthe minimization of the log marginal likelihood, which is usually the proxy goal\nof Bayesian model selection.\nIn §6, we will take a different perspective on these quantities and examine them as\nmarginal and joint cross-entropies (or marginal and joint cross-mutual information),\nwhich is more sensible, perhaps. However, we will still use the notation of PIG and\nJPIG for the rest of this chapter and this thesis otherwise, as it ties the different\ninformation quantities together more nicely2.\nDeriving a Tractable Selection Function. We now derive a tractable expression\nfor the term in Eq. (8.1) that does not require us to train on each candidate point\n(x, y) ∈Bt and then evaluate the loss on Deval. To make our claims precise and our\nassumptions transparent, we use the language of Bayesian probability theory. We treat\nmodel parameters as a random variable with prior p(θ) and infer a posterior p(θ |Dtrain)\nusing the already-seen training data Dtrain. The model has a predictive distribution\np(y | x, Dtrain) =\nR\nθ p(y | x, θ) p(θ | Dtrain)dθ. When using a point estimate of θ, the\npredictive distribution can be written as an integral with respect to a Dirac delta.\nWe have already motivated that Equation 8.1 is equivalent to maximizing the PIG\nobjective. As a first step, we will switch to using the JPIG objective (Approximation\n0), which is more closely related to the log marginal likelihood and model selection.\n2Although we could also follow a different naming scheme and call them T(E)IG, J(E)IG, and\nM(E)IG, which would be more similar to MacKay [1992b]. Naming things is not easy.\n8. Prioritized Data Selection during Training\n143\nWe will show that it leads to a tractable selection function: We simply rewrite JPIG\nusing the symmetry of the point-wise mutual information as:\nI[yeval\n1..E | xeval\n1..E, (y, x), Dtrain] = H[y | x, Dtrain] −H[y | x, yeval\n1..E , xeval\n1..E, Dtrain].\n(8.10)\nAs exact Bayesian inference (conditioning on Dtrain or Deval) is intractable in neural\nnetworks [Blundell et al., 2015], we fit the models with SGD instead (Approximation\n1). We study the impact of this approximation in §8.3.1. The first term, H[y | x, Dtrain],\nis then the training loss on the point (x, y) using the current model trained on Dtrain.\nThe second term, H[y |x, Deval, Dtrain], is the loss of a model trained on Dtrain and Deval.\nAlthough the selection function in Eq. (8.10) is tractable, it is still somewhat\nexpensive to compute, as both terms must be updated after each acquisition of a new\npoint. However, we can approximate the second term with a model trained only on\nthe holdout dataset, H[y | x, Deval, Dtrain] ≈H[y | x, Deval] (Approximation 2). This\napproximation saves a lot of compute: it is now sufficient to compute the term once\nbefore the first epoch of training. Later on, we show that this approximation empirically\ndoes not hurt performance on any tested dataset and even has additional benefits\n(§8.3.1 and Appendix G.4). We term H[y | x, Deval] the irreducible holdout loss (IL)\nsince it is the remaining loss on point (x, y) ∈Dpool after training on the holdout set\n(labeled evaluation set) Deval; in the limit of Deval being large, it would be the lowest\nloss that the model can achieve without training on (x, y). Accordingly, we name\nour approximation of Eq. (8.10) the reducible holdout loss—the difference between\nthe training loss and the irreducible holdout loss (IL).\nOur method still requires us to train a model on a holdout set (labeled evaluation\nset), but a final approximation greatly reduces that cost. We can efficiently compute\nthe IL with an “irreducible loss model” (IL model) that is smaller than the target\nmodel and has low accuracy (Approximation 3). We show this and explain it in\nSections 8.3.1, 8.3.2, and 8.3.3. Counterintuitively, the reducible holdout loss can\ntherefore be negative. Additionally, one IL model can be reused for many target model\nruns, amortizing its cost (§8.3.2). For example, we trained all 40 seeds of 5 target\narchitectures in Figure 8.1 using a single ResNet18 IL model. Further, this model\ntrained for 37x fewer steps than each target model (reaching only 62% accuracy). §8.4\ndetails further possible efficiency improvements.\nIn summary, selecting a point that minimizes the holdout loss in Eq. (8.1), for a\nmodel trained on Dtrain, can be approximated with the following easy-to-compute objec-\ntive:\nReducible holdout loss selection (RHO-LOSS)\narg max\n(x,y)∈Bt\nreducible holdout loss\nz\n}|\n{\nH[y | x, Dtrain]\n|\n{z\n}\ntraining loss\n−\nH[y | x, Deval]\n|\n{z\n}\nirreducible holdout loss (IL)\n(8.11)\nAlthough we required additional data Deval, this is not essential for large (§8.3.0)\nnor small (§8.3.2) datasets.\nUnderstanding the Reducible Loss. We now provide intuition on why reducible\nholdout loss selection (RHO-LOSS) avoids redundant, noisy, and less relevant points.\ni) Redundant points. We call a training point redundant when the model has already\n8. Prioritized Data Selection during Training\n144\nAlgorithm 3 Reducible holdout loss selection (RHO-LOSS)\n1: Input: Small model p(y | x; Deval) trained on a labeled evaluation set Deval\n(or holdout set), batch size K, large batch size K′ > K, learning rate η.\n2: for (xi, yi) in training set do\n3:\nIrreducibleLoss[i] ←H[yi | xi, Deval]\n4: Initialize parameters θ0 and set t = 0\n5: for t = 0, 1, . . . do\n6:\nRandomly select a large batch Bt of size K′.\n7:\n∀i ∈Bt, compute Loss[i], the train loss of point i given parameters θt\n8:\n∀i ∈Bt, compute RHOLOSS[i] ←Loss[i]−IrreducibleLoss[i]\n9:\nbt ←top-K samples in Bt in terms of RHOLOSS.\n10:\ngt ←mini-batch gradient on bt using parameters θt\n11:\nθt+1 ←θt −ηgt\nlearned it, i.e. its training loss cannot be further reduced. Since redundant points\nhave low training loss, and the reducible loss is always less than the training loss\n(Eq. (8.11)), such points have low reducible loss and are not selected. And if the\nmodel forgets them, they are revisited in the next epoch. ii) Noisy points. While\nprior methods select based on high training loss (or gradient norm), not all points\nwith high loss are informative—some may have an ambiguous or incorrect (i.e. noisy)\nlabel. The labels of such points cannot be predicted using the holdout set (labeled\nevaluation set) [Chen et al., 2019]. Such points have high IL and, consequently, low\nreducible loss. These noisy points are less likely to be selected compared to equivalent\npoints with less noise.\niii) Less relevant points.\nLoss-based selection has an\nadditional pitfall. The training loss is likely higher for outliers in input space—values\nof x far from most of the training data, in regions with low input density under\nptrue(x). Points with low ptrue(x) should not be prioritized, all else equal. Consider an\n‘outlier’ (x, y) and a non-outlier (x′, y′), with ptrue(x) < ptrue(x′) but equal training\nloss H[y|x, Dtrain] = H[y′|x′, Dtrain]. As the holdout set (labeled evaluation set) Deval is\nalso drawn from ptrue, Deval will contain fewer points from the region around x in input\nspace compared to the region around x′. Thus, training on (x, y) is likely to reduce\nthe holdout loss (Eq. (8.1)) less, and so we prefer to train on the non-outlier (x′, y′).\nIn the specific sense described, (x, y) is thus less relevant to the holdout set (labeled\nevaluation set). As desired, RHO-LOSS deprioritizes (x, y): since Deval contains few\npoints from the region around x, the IL of (x, y) will be large.\nIn short, RHO-LOSS deprioritizes points that are redundant (low training loss),\nnoisy (high IL), or less relevant to the holdout set (high IL). That is, RHO-LOSS\nprioritizes points that are not yet learned, learnable, and worth learning. We provide\nempirical evidence for these claims in §8.3.3. See Algorithm 3 for the implemen-\ntation of RHO-LOSS.\nBatch Selection. We showed which point is optimal when selecting a single point\n(x, y). When selecting an entire batch bt, we select the points with the top-K scores\nfrom the randomly pre-sampled set Bt. This is nearly optimal when assuming that\neach point has little effect on the score of other points, which is often used as a\nsimplifying assumption in active learning. This assumption is much more reasonable\n8. Prioritized Data Selection during Training\n145\nin our case than in active learning because model predictions are not changed much\nby a single gradient step on one mini-batch.\nSimple Batch Selection. For large-scale neural network training, practitioners with\nsufficient resources would use many more machines if it further sped up training [Anil\net al., 2018]. However, as more workers are added in synchronous or asynchronous\ngradient descent, the returns diminish to a point where adding more workers does not\nfurther improve wall clock time [Anil et al., 2018; McCandlish et al., 2018]. For example,\nthere are rapidly diminishing returns for using larger batch sizes or distributing a\ngiven batch across more workers, for multiple reasons [McCandlish et al., 2018; Keskar\net al., 2017]. The same holds for distributing the model across more workers along its\nwidth or depth dimension [Rasley et al., 2020; Shoeybi et al., 2019; Huang et al., 2019].\nHowever, we can circumvent these diminishing returns by adding a new dimension\nof parallelization, namely, for data selection.\nSince parallel forward passes do not suffer from such diminishing returns, one\ncan use extra workers to evaluate training losses in parallel [Alain et al., 2015]. The\ntheoretical runtime speedup can be understood as follows. The cost per training\nstep of computing the selection function on Bt is K′\n3K times as much as the cost of the\nforward-backward pass needed to train on bt since a forward pass requires at least\n3x less computation than a forward-backward pass [Jouppi et al., 2017]. One can\nreduce the time for the selection phase almost arbitrarily by adding more workers\nthat compute training losses using a copy of the model being trained. The limit is\nreached when the time for selection is dominated by the communication of parameter\nupdates to workers. More sophisticated parallelization strategies allow reducing the\ntime overhead even further (§8.4). To avoid assumptions about the particular strategy\nused, we report experiment results in terms of the required number of training epochs.\n8.3\nEmpirical Validation\nWe evaluate our selection method on several datasets (both in controlled environments\nand real-world conditions) and show significant speedups compared to prior art, in the\nprocess shedding light on the properties of different selection functions.\nRecall that our setting assumes training time is a bottleneck, but data is abundant—\nmore than we can train on (see Bottou and LeCun [2003]). This is common e.g. for\nweb-scraped data where state-of-the-art performance is often reached in less than half\nof one epoch [Komatsuzaki, 2019; Brown et al., 2020]. As data is abundant, we can\nset aside a holdout set (labeled evaluation set) for training the IL model with little\nto no downside. For the large Clothing-1M dataset, we implement RHO-LOSS by\ntraining the IL model on 10% of the training data, while all baselines are trained on\nthe full 100% of the training data. For the smaller datasets, we simulate abundance\nof data by reserving a holdout set and training all methods only on the remaining\ndata. However, RHO-LOSS also works on small datasets without additional data\nby double-using the training set (§8.3.2).\nDatasets. We evaluate on 7 datasets: 1) QMNIST [Yadav and Bottou, 2019] extends\nMNIST [LeCun et al., 1998] with 50k extra images which we use as the holdout set\n(labeled evaluation set). 2) On CIFAR-10 [Krizhevsky, 2009] we train on half of the\ntraining set and use the other half as a holdout to train the irreducible loss (IL) model.\n3) CIFAR-100: same as CIFAR-10. 4) CINIC-10 [Darlow et al., 2018] has 4.5x more\n8. Prioritized Data Selection during Training\n146\n0\nNo speedup\n3x\n6x\nRHO-LOSS speedup over uniform selection\nDefault\nSmall irreducible loss model\nNo holdout set\nArchitecture transfer\nHyperparameter transfer\nDataset\nCIFAR10\nCIFAR100\nCINIC10\nFigure 8.2: The irreducible loss model can be small, trained with no holdout\ndata, and reused across target architectures and hyperparameters. Here, we use\nclean datasets, where speedups are smallest. The x-axis shows speedup, i.e. after how many\nfewer epochs RHO-LOSS exceeds the highest accuracy uniform selection achieves within\n100 epochs. Row 1 uses a ResNet18 as irreducible loss model. All other rows instead use a\nsmall, cheap CNN. Each dot shows an experiment with a given combination of irreducible\nloss model and target model (mean across 2-3 seeds for all but the last row).\nimages than CIFAR-100 and includes a validation set (which we use as holdout set)\nand a test set with 90k images each. 5) Clothing-1M [Xiao et al., 2015], which contains\nover 1 million 256x256-resolution clothing images from 14 classes. The dataset is fully\nweb-scraped—a key application area of this chapter—and is the most widely accepted\nbenchmark for image recognition with noisy labels [Algan and Ulusoy, 2021]. We use\nthe whole training set for training and reuse 10% of it to train the IL model. We further\nevaluate on two NLP datasets from GLUE [Wang et al., 2019]: 6) CoLA (grammatical\nacceptability) and 7) SST-2 (sentiment). We split their training sets as for CIFAR.\nBaselines. Aside from uniform sampling (without replacement, i.e. random shuffling),\nwe also compare to selection functions that have achieved competitive performance\nin online batch selection recently: the (training) loss, as implemented by Kawaguchi\nand Lu [2020], gradient norm, and gradient norm with importance sampling (called\ngradient norm IS in our figures), as implemented by Katharopoulos and Fleuret [2018].\nWe also compare to the core-set method Selection-via-Proxy (SVP) that selects data\noffline before training [Coleman et al., 2020]. We report results using maximum entropy\nSVP and select with the best-performing model, ResNet18. We further compare to\nfour baselines from active learning, shown in Appendix G.7 as they assume labels are\nunobserved. Finally, we include selection using the negative IL (see Eq. 8.11) to test if\nit is sufficient to only skip noisy and less relevant but not redundant points.\nModels and Hyperparameters. To show our method needs no tuning, we use\nthe PyTorch default hyperparameters (with the AdamW optimizer [Loshchilov and\nHutter, 2019]) and\nK\nK′ = 0.1. We test many additional hyperparameter settings in\nFigs. 8.2 (row 5) and G.5. We test various architectures in Figs. 8.1 and 8.2 (row 4).\nIn all other figures, we use a 3 layer MLP for experiments on QMNIST, a ResNet-18\nadapted for small images for CIFAR-10/CIFAR-100/CINIC-10, and a ResNet-50 for\nClothing-1M. All models for Clothing-1M are pre-trained on ImageNet (standard for\nthis dataset Algan and Ulusoy [2021]) and the IL model is always a ResNet-18. For\nthe NLP datasets, we use a pre-trained ALBERT v2 [Lan et al., 2020]. We always use\n8. Prioritized Data Selection during Training\n147\nTable 8.1: Spearman’s rank correlation of rankings of data points by selection functions\nthat are increasingly less faithful approximations of Eq. (8.10), compared to the most faithful\napproximation. Approximations added from left to right. Mean across 3 seeds.\nNon-\nNot\nNot updating\nSmall\nBayesian\nconverged\nIL model\nIL model\nRank correlation\n0.75\n0.76\n0.63\n0.51\nthe IL model checkpoint with the lowest validation loss (not the highest accuracy);\nthis performs best. Details in Appendix G.2.\nEvaluation. We measure speedup in terms of the number of epochs needed to reach a\ngiven test accuracy. We measure epochs needed, rather than wall clock time, as our\nfocus is on evaluating a new selection function, not an entire training pipeline. Wall\nclock time depends primarily on the hardware used and implementation details that\nare beyond our scope. Most importantly, data selection is amenable to parallelization\nbeyond standard data parallelism as discussed in §8.2.\n8.3.1\nImpact of Approximations\nIn §8.2, we introduced a function for selecting exactly the points that most reduce\nthe model’s loss on a holdout set (labeled evaluation set). To make this selection\nfunction efficient for deep neural networks, we made several approximations. Here, we\nstudy how these approximations affect the points selected, by successively introducing\none approximation after the other.\nBecause the exact selection function (Eq. (8.10)) is intractable, we start with a\nclose (and expensive) approximation as the gold standard (Approximation 0). To\nmake Approximation 0 feasible, the experiments are conducted on an easy dataset—\nQMNIST (with 10% uniform label noise and data duplication to mimic the properties\nof web-scraped data). We then successively introduce the Approximations 1, 2, and 3\ndescribed in §8.2. To assess the impact of each approximation, we train a model without\nand with the approximations, and then compute the rank correlation (Spearman’s\ncorrelation coefficient) of the selection function evaluated on each batch Bt. Across the\nfirst epoch, we present the mean of the rank correlations. Since each approximation\nselects different data, the corresponding models become more different over time; this\ndivergence likely causes some of the observed differences in the points they select.\nSee Appendix G.5 for details.\nApproximation 0. To get as close as possible to the Bayesian inference/conditioning\nused in Eq. (8.10), we use a deep ensemble of 5 neural networks and train them\nto convergence after every time step t on the acquired dataset bt ∪Dtrain Wilson\nand Izmailov [2020].\nApproximation 1: SGD instead of Bayesian inference/conditioning. Approximation\n0 is a close approximation of Eq. (8.10), but training an ensemble to convergence at\nevery step t is far too expensive in practice. Starting from this gold-standard, we\nintroduce two stronger approximations (1a and 1b) to move to standard neural network\nfitting with AdamW. 1a) First, we replace the ensemble with a single model, while\nstill training to convergence at each time step. The Spearman’s coefficient between\n8. Prioritized Data Selection during Training\n148\nCIFAR10 CIFAR100 CINIC10\n0\n20\n40\nProportion of selected points\nalready classified correctly (%)\nRedundant Points\nCIFAR10 CIFAR100 CINIC10\n0\n10\n20\n30\nProportion of selected points\nwith corrupted labels (%)\nNoisy Points\nCIFAR100 Relevance\n0\n20\n40\nProportion of selected\n points less relevant (%)\nLess Relevant Points\nSelection Method\nReducible Loss (Ours)\nReducible Loss (Ours)\nSmall IL Model\nUniform Sampling\nGradient Norm\nLoss\nFigure 8.3: Properties of RHO-LOSS and other methods. RHO-LOSS prioritizes points\nthat are non-noisy, task-relevant, and non-redundant—even when the irreducible loss (IL)\nmodel is a small CNN. In contrast, loss and gradient norm prioritize noisy and less relevant\npoints (while also avoiding redundant points). Left. Proportion of selected points with\ncorrupted labels. We added 10% uniform label noise, i.e., we randomly switched each point’s\nlabel with 10% probability. Middle. Proportion of selected points from low relevance classes\non CIFAR100 Relevance dataset. Right. Proportion of selected points that are already\nclassified correctly, which is a proxy for redundancy. Mean over 150 epochs of training and\n2-3 seeds.\nthis approximation and Approximation 0 is 0.75, suggesting similar points are selected\n(“Non-Bayesian” in Table 8.1). 1b) Next, we only take one gradient step on each\nnew batch bt. The Spearman’s coefficient, when comparing this to Approximation\n0, is 0.76 (“Not Converged” in Table 8.1).\nApproximation 2. Not updating the IL model on the acquired data Dtrain. Second,\nwe save compute by approximating H[y | x, Dtrain, Deval] with H[y | x, Deval].\nThe\npoints selected are still similar to Approximation 0 (Spearman’s coefficient 0.63, “Not\nupdating IL model” in Table 8.1). This approximation also performs well on other\ndatasets (Appendix G.4).\nApproximation 3: Small IL model. Lastly, we use a model with 256 hidden units\ninstead of 512 (4x fewer parameters) as the IL model and see again that similar\npoints are selected (Spearman’s coefficient 0.51). We study cheaper IL models in\nother forms and datasets in the next section.\n8.3.2\nCheap Irreducible Loss Models & Robustness\nRHO-LOSS requires training an IL model on a holdout set (labeled evaluation set),\nwhich poses additional costs. Here, we show how to minimize these costs and amortize\nthem across many training runs of target models. The same experiments also show\nthe robustness of RHO-LOSS across architectures and hyperparameter settings. To\nfit our computational budget, we perform these experiments on moderate-sized clean\nbenchmark datasets although RHO-LOSS speeds up training more on noisy or redundant\nweb-scraped data (see §8.3.4).\nIrreducible Loss Models: Small & Cheap. In our default setting (Figure 8.2, row\n1), both the target model and IL model have the same architecture (ResNet-18). In\nrows 2 and below, we instead used a small CNN similar to LeNet as the IL model\n8. Prioritized Data Selection during Training\n149\n[LeCun et al., 1989]. It has 21x fewer parameters and requires 29x fewer FLOP per\nforward pass than the ResNet-18. The smaller IL model accelerates training as\nmuch or more than the larger model, even though its final accuracy is far lower\nthan the target ResNet-18 (11.5% lower on CIFAR-10, 7% on CIFAR-100, and 8.1%\non CINIC-10). We examine in §8.3.3 why this useful result holds.\nIrreducible Loss Models: No Holdout Data. Web-scraped datasets are often so\nlarge that even a small fraction of the overall data can be sufficient to train the IL\nmodel. E.g., in our experiments on Clothing-1M (Figure 8.1), the holdout set (labeled\nevaluation set) is only 10% as large as the main train set. Additionally, we can train\nthe IL model without any holdout data (Figure 8.2, row 3). We split the training\nset Dpool into two halves and train an IL model on each half (still using small IL\nmodels). Each model computes the IL for the half of Dpool that it was not trained\non. Training two IL models costs no additional compute since each model is trained\non half as much data compared to the default settings.\nIrreducible Loss Models: Reuse for Different Target Architectures. We find\nthat a single small CNN IL model accelerates the training of 7 target architectures\n(Figure 8.2, row 4): VGG11 (with batch norm), GoogleNet, Resnet34, Resnet50,\nDensenet121, MobileNet-v2, Inception-v3. RHO-LOSS does not accelerate training\non CIFAR-10 for VGG11, which is also the architecture on which uniform training\nperforms the worst; i.e. RHO-LOSS empirically does not “miss” a good architecture.\nNot only is RHO-LOSS robust to architectures choice, a single IL model can also be\nreused by many practitioners who use different architectures (as we did in Figure 8.1).\nIrreducible Loss Models: Reuse for Hyperparameter Sweeps. We find that\na single small CNN accelerates the training of ResNet-18 target models across a\nhyperparameter grid search (Figure 8.2, last row). We vary the batch size (160, 320, 960),\nlearning rate (0.0001, 0.001, 0.01), and weight decay coefficient (0.001, 0.01, 0.1). RHO-\nLOSS speeds up training compared to uniform on nearly all target hyperparameters.\nThe few settings in which it doesn’t speed up training are also settings in which uniform\ntraining performs very poorly (< 30% accuracy on CIFAR-100, < 80% on CIFAR-10).\n8.3.3\nProperties of RHO-LOSS & Other Selection Functions\nWe established that RHO-LOSS can accelerate the training of various target architec-\ntures with a single IL model, even if the IL model is smaller and has considerably lower\naccuracy than the target models (§8.3.2).\nThis suggests robustness to target-IL\narchitecture mismatches.\nTo understand this robustness, we investigate the properties of points selected\nby RHO-LOSS, when the target and IL model architectures are identical, and when\nthey differ. In both cases, we find that RHO-LOSS prioritizes points that are non-\nnoisy, task-relevant, and not redundant. We also investigate the properties of points\nselected by prior art.\nNoisy Points. We investigate how often different methods select noisy points by\nuniformly corrupting the labels for 10% of points and tracking what proportion of\nselected points are corrupted. RHO-LOSS deprioritizes noisy points for both IL models\n(Figure 8.3). We observe a failure mode of the widely-used loss and gradient norm\nselection functions: they select far more noisy points than uniform. These methods also\n8. Prioritized Data Selection during Training\n150\nTable 8.2:\nEpochs required to reach a given target test accuracy (final accuracy in\nparentheses).\nFigs. G.1 and G.2 (Appendix) show all training curves.\nSome datasets\nhave 10% uniform label noise added. Results averaged across 2-4 seeds. Best performance in\nbold. RHO-LOSS performs best in both epochs required and final accuracy. NR indicates\nthat the target accuracy was not reached. ∗On CIFAR10/100, CoLA, and SST-2, only half of\nthe data is used for training (§8.3.0).\nDataset\nTarget Acc\nNumber of epochs method needs to reach target accuracy ↓(Final accuracy in parentheses)\nTrain Loss\nGrad Norm\nGrad Norm IS\nSVP\nIrred Loss\nUniform\nRHO-LOSS\nClothing-1M\n60.0%\n8\n13\n2\nNR\nNR\n2\n1\n69.0%\nNR (65%)\nNR (64%)\n9 (70%)\nNR (55%)\nNR (48%)\n30 (70%)\n2 (72%)\nCIFAR10∗\n80.0%\n81\nNR\n57\nNR\nNR\n79\n39\n87.5%\n129 (90%)\nNR (61%)\n139 (89%)\nNR (55%)\nNR (60%)\nNR (87%)\n65 (91%)\nCIFAR10∗\n75.0%\nNR\nNR\n57\nNR\nNR\n62\n27\n(Label Noise)\n85.0%\nNR (28%)\nNR (23%)\nNR (84%)\nNR (48%)\nNR (62%)\nNR (85%)\n49 (91%)\nCIFAR100∗\n40.0%\n138\n139\n71\nNR\n93\n65\n48\n52.5%\nNR (42%)\nNR (42%)\n132 (55%)\nNR (18%)\nNR (43%)\n133 (54%)\n77 (61%)\nCIFAR100∗\n40.0%\nNR\nNR\n94\nNR\n89\n79\n49\n(Label Noise)\n47.5%\nNR (4%)\nNR (4%)\n142 (48%)\nNR (14%)\nNR (43%)\n116 (50%)\n65 (60%)\nCINIC10\n70.0%\nNR\nNR\n34\nNR\nNR\n38\n27\n77.5%\nNR (36%)\nNR (50%)\n64 (82%)\nNR (39%)\nNR (60%)\n97 (80%)\n38 (83%)\nCINIC10\n60.0%\nNR\nNR\n22\nNR\n30\n24\n13\n(Label Noise)\n67.5%\nNR (16%)\nNR (16%)\n35 (79%)\nNR (39%)\nNR (64%)\n38 (78%)\n17 (82%)\nSST2∗\n82.5%\n8\n2\n3\nNR\n7\n1\n1\n90.0%\nNR (87%)\n4 (91%)\nNR (89.7%)\nNR (66%)\nNR (83%)\n6 (90%)\n3 (92%)\nCoLA∗\n75.0%\n8\n6\n16\nNR\nNR\n34\n3\n80.0%\nNR (78%)\nNR (79%)\nNR (78%)\nNR (62%)\nNR (69%)\nNR (76%)\n39 (80%)\nseverely drop in accuracy when the noise follows the class confusion matrix [Rolnick\net al., 2017] and when we add ambiguous images [Mukhoti et al., 2023] (Appendix G.3).\nTogether, this suggests that noisy points have high loss (and gradient norm), but also\nhigh IL and thus low reducible loss. Their IL is high even when the IL model is small\nas noisy labels cannot be predicted well using the holdout set (labeled evaluation set).\nRelevant Points. We study how often less relevant points are selected by creating\nthe CIFAR100 Relevance dataset, in which 80% of the data comes from 20% of the\nclasses. This mimics natural distributions of NLP and vision data where most data\ncomes from few object classes, topics, or words [Baayen and Lieber, 1996; Tian et al.,\n2021]. Concretely, we retain all examples from 20 randomly chosen “high relevance”\nclasses but only 6% of the examples from other, “low relevance” classes. Intuitively,\nsince the high relevance classes have higher ptrue(x) and are 17x more likely to appear\nat test time, improving their accuracy improves the test accuracy much more than\nimproving the accuracy of less relevant classes.\nThe loss and gradient norm methods select more points than uniform selection\nfrom the low relevance classes (Figure 8.3). In contrast, RHO-LOSS selects somewhat\nfewer low relevance points, suggesting these classes have high IL. Since the less relevant\nclasses are less abundant in the holdout set (labeled evaluation set), both the small\n8. Prioritized Data Selection during Training\n151\nand large IL models have higher loss on them.\nRedundant Points. To investigate whether methods select redundant points, we\ntrack the percentage of selected points that are already classified correctly. This is only\na proxy for redundancy; points that are classified correctly but with low confidence are\nnot fully redundant, since their loss can be further reduced. We control for the different\naccuracy reached by each method by averaging only over epochs in which test accuracy\nis lower than the final accuracy reached by the weakest performing method. Figure 8.3\nshows that all methods select fewer redundant points than uniform sampling.\n8.3.4\nSpeedup\nFinally, we evaluate how much different selection methods speed up training. Recall\nthat the main application area for this chapter is large web-scraped datasets, known\nfor high levels of noise and redundancy. Clothing-1M is such a dataset (§8.3.0). We\nalso include smaller, clean benchmarks from vision (CIFAR-10, CIFAR-100, CINIC-\n10) and NLP (CoLA, SST-2). Finally, we study if selection functions are robust to\nthe controlled addition of label noise.\nSpeedup on Clean Data. RHO-LOSS reaches target accuracies in fewer epochs\nthan uniform selection on all datasets (Table 8.2). It also outperforms state-of-the-art\nmethods by a clear margin in terms of speed and final accuracy. On the challenging\nCoLA language understanding dataset, the speedup over uniform selection exceeds 10x.\nIn Table G.1 (Appendix G.1), we find similar speedups when using no holdout data.\nSpeedup on Noisy Data. When adding 10% label noise, batch selection with RHO-\nLOSS achieves greater speedups while, as hypothesized, prior art degrades (Table 8.2).\nNotably, on noisier data, the speedup over uniform selection grows.\nSpeedup on Large Web-Scraped Data. On Clothing-1M, loss-based and gradient\nnorm-based selection fail to match uniform selection, suggesting they are not robust\nto noise. In contrast, RHO-LOSS reaches the highest accuracy that uniform selection\nachieves during 50 epochs in just 2 epochs and improves final accuracy (72% vs 70%).\nNotably, this was possible even though the IL model we used has low accuracy (62.2%)\nand was trained on ca. 10x fewer data. RHO-LOSS also used 2.7x fewer FLOPs to\nreach the peak accuracy of uniform selection, including the cost of training the IL\nmodel (which could be amortized) and despite our implementation being designed to\nsave time, not compute. While Table 8.2 shows results for a Resnet-50, Figure 8.1\nincludes several additional architectures, with an average speedup of 18x.\n8.4\nRelated Work\nTime-Efficient Data Selection. Forward passes for selection can be accelerated\nusing low-precision hardware or parallelization. While backward passes typically require\nhigh precision, forward passes can tolerate lower precision [Jouppi et al., 2017; Jiang\net al., 2019], especially as we only need the loss (not the activations which would be\nneeded for backpropagation). A forward pass by default requires roughly 3x less time\nthan a forward-backward pass, but this speedup can be increased to a factor around\n10x when using the low-precision cores available in modern GPUs and TPUs [Jouppi\net al., 2017; Jiang et al., 2019]. Further, prior work uses a set of workers that perform\n8. Prioritized Data Selection during Training\n152\nforward passes on Bt or on the entire dataset asynchronously while the master process\ntrains on recently selected data [Alain et al., 2015].\nCompute-Efficient Data Selection. While we limit our scope to comparing selection\nfunctions, and we compute them naively, this choice is inefficient in practice. Selection\ncan be made cheaper by reusing losses computed in previous epochs [Loshchilov and\nHutter, 2015; Jiang et al., 2019] or training a small model to predict them [Katharopou-\nlos and Fleuret, 2017; Zhang et al., 2019a; Coleman et al., 2020]. Alternatively, core\nset methods perform selection once before training [Mirzasoleiman et al., 2020; Borsos\net al., 2020], although typically with more expensive selection functions.\nData Selection Functions. RHO-LOSS is best understood as an alternative to\nexisting selection functions, which can be categorized by the properties of points they\nselect and whether they use information about labels. “Hard” points are selected\nboth by high loss [Loshchilov and Hutter, 2015; Kawaguchi and Lu, 2020; Jiang et al.,\n2019] and high prediction uncertainty [Settles, 2010; Li and Sethi, 2006; Coleman\net al., 2020]. However, prediction uncertainty does not require labels and can thus\nbe used for active learning. Despite this, they both suffer from the same problem:\nhigh loss and high uncertainty can be caused by noisy (in particular, ambiguous)\nlabels. This also applies to selection of points whose labels are easily forgotten during\ntraining [Toneva et al., 2019]. Noisy points are avoided by our negative IL baseline and\nsimilar methods [Pleiss et al., 2020; Chen et al., 2019]. Points that reduce (expected)\nholdout loss are also selected for other applications [Killamsetty et al., 2021b; Ren\net al., 2018], although using much more computation.\nVariance Reduction Methods. Online batch selection is also used to reduce the\nvariance of the gradient estimator computed by SGD [Katharopoulos and Fleuret,\n2018, 2017; Johnson and Guestrin, 2018; Alain et al., 2015]. Such methods typically\nuse importance sampling—points with high (approximate) gradient norm are sampled\nwith high probability but then down-weighted in the gradient calculation to de-bias\nthe gradient estimate.\nWithout de-biasing, methods like RHO-LOSS also create\nselection bias. However, bias can improve test performance, both in theory and practice\n[Farquhar et al., 2021; Kawaguchi and Lu, 2020].\n8.5\nDiscussion\nTo reduce excessive training times, we introduce a theoretically grounded selection\nfunction that enables substantial speedups on clean data and even larger speedups\non noisy and web-scraped data. By illuminating three properties of optimal selection,\nwe hope to motivate new directions in batch selection.\nHowever, our selection\nfunction should be combined with methods in §8.4 for cheap and fast selection\nwith maximal speedups.\nIf you understand, things are just as they are. If\nyou do not understand, things are just as they are.\n9\nUnifying Approaches in Active Learning and\nActive Sampling\nThe topic of this chapter is the explicit connection between Bayesian active learning\nand active sampling, and the connection between these and other recent approaches\nin data subset selection: amongst them, BADGE [Ash et al., 2020], BAIT [Ash et al.,\n2021], PRISM1[Kothawade et al., 2022], SIMILAR1 [Kothawade et al., 2021], and\nGraNd [Paul et al., 2021]. Specifically, we connect the acquisition functions used\nto select informative samples in these approaches to information-theoretic quantities\n(short: information quantities) that are known from Bayesian optimal experiment\ndesign [Lindley, 1956; MacKay, 1992b].\nBy examining how Fisher information and second-order posterior approximations\n(Gaussian approximations) can be used for estimating information quantities, we develop\na unifying perspective and relate these recent methods to information quantities used in\nBayesian active learning (and introduced in previous chapters): for active learning, the\nexpected information gain/(Batch)BALD, which we examined in §1.2.4.2 and §4, the\n(joint) expected predictive information gain from §7; and, for active sampling, the infor-\nmation gain, mentioned in §1.2.6, and the (joint) predictive information gain from §8.\nThese connections point us towards possible failure modes of above methods and\npotential extensions in principled ways. Reciprocaly, they also point towards new\nextensions of what we have introduced in the previous chapters—very exciting!\nWe examine well-known approximations that lead to last-layer approaches, find a\npotential estimation bias when using similarity matrices (kernels) in active learning,\ncompare trace and log determinant approximations in regard to batch acquisition\npathologies, and trade off weight- and prediction-space methods in principle.\nLimitations\nIt is important to note the limitations of this chapter. We string together different\nresearch areas and disparate literature, while focusing on providing an information-\ntheoretic perspective. For a perspective that is focused on kernel methods and Gaussian\nProcess approximations of neural networks, Holzmüller et al. [2022] extensively covers\nsomve of the mentioned works in active learning above in great detail, which provides\na useful addition to this chapter.\nHierarchy of Approximations. Although our results employ a hierarchy of ap-\nproximations, we do not examine the error terms in detail. This is in line with how\nthese approximations are used in deep learning, where the approximations often only\nprovide motivation for useful mechanisms. However, we try to identify where these\n9. Unifying Approaches in Active Learning and Active Sampling\n154\napproximations might break, enumerate their limitations, and raise several (empirical)\nresearch questions for the future.\nLog Loss. While many active learning and active sampling methods are motivated\nindependently of the underlying loss, we will remain focused on log losses, such as the\ncommon cross-entropy loss or squared error loss (Gaussian error), as these log losses\ncan be viewed through an information-theoretic and probabilistic lens.\nChapter Structure\nIn §9.2, we look at second-order posterior approximations (Gaussian approximations),\nwhich we use to revisit Fisher information, its properties, special cases, and ap-\nproximations in §9.3. Our contribution here is to summarize results and provide a\nconsistent notation that simplifies reasoning about information quantities, observed\ninformation, and Fisher information.\nIn §9.4, we approximate the information quantities mentioned above using observed\ninformation and Fisher information. We provide a comprehensive overview to un-\nderstand the differences and similarities and make it easier to spot applications of\nthese approximations in the literature. We pay special attention to the limitations:\nfor example, we will see that some approximations that use the trace of the Fisher\ninformation do not take redundancies between samples into account. They exhibit the\nsame pathologies as other methods that, in essence, score points individually (§5). In\n§9.5, we expand our approach to approximations that use similarity matrices of log-loss\ngradients. Our contributions are a comprehensive overview of the approximations\nand the connection to similarity matrices.\nIn §9.6, we show that (Batch-)BALD and EPIG on the one hand; and BADGE,\nBAIT, PRISM1, and SIMILAR1 on the other hand can be seen as optimizing the same\nobjectives. The difference is that (Batch-)BALD ( §4; Houlsby et al. [2011]) and EPIG\n(§7) operate in prediction space, while Fisher information-based methods operate in\nweight space: we show that an approximation of EPIG, a transductive active learning\nobjective, using Fisher information, matches the BAIT objective [Ash et al., 2021].\nSimilarly, we show how BADGE [Ash et al., 2020] approximates the EIG, using the\nconnection to similarity matrices. Finally, we find that submodularity-based approaches\n[Iyer et al., 2021] such as SIMILAR [Kothawade et al., 2021] and PRISM [Kothawade\net al., 2022], which report their best results using the log determinant of similarity\nmatrices, approximate information quantities when they perform best. We also show\nthat gradient-length-based methods like EGL [Settles et al., 2007] and GraNd [Paul\net al., 2021] can be connected to information quantities.\n9.1\nSetting\nThis section some additional notation, concepts, and probabilistic model that we\nuse in this chapter.\nTransductive Acquisition Functions. When an acquisition function in data subset\nselection uses (additional) data Deval, unlabeled or labeled, to guide acquisitions, we\nrefer to the objective as a transductive objective [Yu et al., 2006; Wang et al., 2021].\n1using log determinant objectives\n9. Unifying Approaches in Active Learning and Active Sampling\n155\nActive Learning. To increase label efficiency, instead of labeling data indiscriminately,\nactive learning iteratively selects and acquires labels for the most informative unlabeled\ndata from a pool set Dpool according to some acquisition function. An acquisition\nfunction scores the informativeness of an unlabeled candidate sample xacq, and the\nsample that maximizes this score is selected for labeling.\nAfter each acquisition\nstep, the model is retrained to take the newly labeled data into account. Labels can\nbe acquired individually or in batches (batch acquisition, see below). The expected\ninformation gain (EIG)\nI[Ω; Y acq | xacq]\n(EIG/BALD)\nand (joint) expected predictive information gain (JEPIG and EPIG, respectively)\nI[{Y eval\ni\n}; Y acq | {xeval\ni\n}, xacq],\n(JEPIG)\nI[Y eval; Y acq | Xeval, xacq]\n(EPIG)\nare examples of such acquisition functions.\nEPIG and JEPIG are transductive\nacquisition functions as they depend on Xeval, {xeval\ni\n}, respectively.\nActive Sampling. To increase training efficiency, instead of training with all samples,\nactive sampling (sometimes also called data pruning) [Paul et al., 2021] selects the\nmost informative sample (xacq, yacq) from the training set to train on next. This can\nbe done statically before training the model, in which case this is also referred to as\ncore-set selection, or dynamically, in which case it is also referred to as curriculum\nlearning.\nThe information gain (IG)\nI[Ω; yacq | xacq],\n(IG)\nand (joint) predictive information gain (JPIG or PIG, respectively)\nI[{yeval\ni\n}; yacq | {xeval\ni\n}, xacq],\n(JPIG)\nI[Y eval; yacq | Xeval, xacq]\n(= Eˆptrue(xeval,yeval) I[yeval; yacq | xeval, xacq])\n(PIG)\nare examples of such acquisition functions.\nPIG and JPIG are transductive ac-\nquisition functions.\nSubmodular Acquisition Functions. Choosing the subset {xacq\ni\n} naively is in-\ntractable due to the exponential number of possible acquisition batches. Instead\nof maximizing the acquisition function on all possible batches, we can often use\nsubmodularity [Nemhauser et al., 1978]. A set function f is submodular when:\nf(A ∪B) ≤f(A) + f(B) −f(A ∩B).\n(submodular)\nAn acquisition batch {xacq\ni\n} can be constructed greedily by selecting the samples\nthat increase the acquisition function the most one-by-one. This greedy algorithm\nis guaranteed to find a 1 −1\ne-optimal acquisition batch for monotone submodular\nacquisition functions.\nAlthough the EIG is (monotone) submodular, leading to efficient batch acquisition\n(§4), the other information quantities (IG, EPIG, JEPIG, PIG) are usually not sub-\nmodular. We examine the details of this and compare to the relevant literature in §9.6.\n9. Unifying Approaches in Active Learning and Active Sampling\n156\nTable 9.1: Taxonomy of Information Quantities for Data Subset Selection. In general,\ninformation quantities can be split into ones for active sampling or active learning, into non-\ntransductive and transductive ones, and in the transductive case, into taking an expectation\nor the joint over (additional) evaluation samples. Here, we show the information quantities\nfor individual acquisition. For batch acquisition, {Y acq\ni\n}, {yacq\ni\n}, {xacq\ni\n} can be substituted.\nActive Learning\nActive Sampling\nNon-Transductive\nEIG/BALD\nI[Ω; Y acq | xacq]\nIG\nI[Ω; yacq | xacq]\nTransductive\n(using Deval)\nExpectation\nEPIG\nEˆptrue(xeval) I[Y eval; Y acq | xeval, xacq]\nPIG\nEˆptrue(yeval,xeval) I[yeval; yacq | xeval, xacq]\nJoint\nJEPIG\nI[{Y eval\ni\n}; Y acq | {xeval\ni\n}, xacq]\nJPIG\nI[{yeval\ni\n}; yacq | {xeval\ni\n}, xacq]\nTaxonomy of Information Quantities. Table 9.1 shows the information quantities\nalong three dimensions:\nactive learning vs active sampling, non-transductive vs\ntransductive, and taking the expectation vs the joint over evaluation samples for\ntransductive information quantities.\n9.2\nSecond-Order Posterior Approximation\nLaplace approximations are a standard tool in Bayesian statistics and machine learning\n[Daxberger et al., 2021; Immer et al., 2021]. In this section, we review the Laplace\napproximation and introduce it as a special case of a more flexible second-order\nposterior approximation, a Gaussian approximation. It is central to approximating\ninformation quantities using observed information, defined in this section, and Fisher\ninformation, defined in §9.3.\nOur goal is to approximate the posterior p(ω | D, Dtrain) using a (multivariate)\nGaussian distribution, where D = {(xi, yi)}N\ni=1 are additional (new) samples, and\nwe start with p(ω | Dtrain) as the “prior” distribution—we will drop Dtrain and use\np(ω) when possible, to shorten the notation.\nTo begin, we complete the square of a second-order Taylor approximation around\nthe log-parameter likelihood for a fixed ω∗:\nlog p(ω)\n≈log p(ω∗) + ∇ω[log p(ω∗)](ω −ω∗) + 1\n2(ω −ω∗)T∇2\nω[log p(ω∗)](ω −ω∗)\n(9.1)\n= 1\n2(ω −(ω∗−∇2\nω[log p(ω∗)]−1∇ω[log p(ω∗)])T\n∇2\nω[log p(ω∗)]\n(ω −(ω∗−∇2\nω[log p(ω∗)]−1∇ω[log p(ω∗)]))\n+ . . . .\n(9.2)\nImportantly, we can express this more concisely by extending the notation of H[·]\nto its derivatives:\nNotation 9.1. We write H′[·] for the Jacobian and H′′[·] for the Hessian of H[·]:\nH′[·] ≜−∇ω log p(·),\n(9.3)\nH′′[·] ≜−∇2\nω log p(·).\n(9.4)\n9. Unifying Approaches in Active Learning and Active Sampling\n157\nThis notation will be helpful throughout this chapter, as both observed information\nand Fisher information can be expressed in terms of the Hessian of the negative\nlog-parameter likelihood. The Jacobian of the entropy is also known as score function.\nThen, we can write:\nH[ω] ≈H[ω∗] + H′[ω∗](ω −ω∗) + 1\n2(ω −ω∗)T H′′[ω∗] (ω −ω∗)\n(9.5)\n= 1\n2(ω −(ω∗−H′′[ω∗]−1 H′[ω∗])T H′′[ω∗] (ω −(ω∗−H′′[ω∗]−1 H′[ω∗]))\n+ . . . .\n(9.6)\nComparing this to the information content of a multivariate Gaussian distribution:\nH[N(w; µ, Σ)] = 1\n2(ω −µ)T Σ−1 (ω −µ) + . . . ,\n(9.7)\nwe obtain the Gaussian approximation, which we will apply throughout this chapter:\nProposition 9.2. The Gaussian approximation of the distribution p(ω) of Ωaround\nsome ω∗is given by:\nΩ\n≈∼N(ω∗−H′′[ω∗]−1 H′[ω∗], H′′[ω∗]−1),\n(9.8)\nwhere H′′[ω∗] must be positive-definite. If ω∗is also a (global) minimizer of H[ω] (that\nis, H′[ω∗] = 0), we obtain the Laplace approximation:\nΩ\n≈∼N(ω∗, H′′[ω∗]−1).\n(9.9)\nApproximation Quality. However, this approximation can be arbitrarily bad de-\npending on p(ω) and ω∗.\nGiven enough data, it is often argued that p(ω) will\nconcentrate around the maximum a posteriori (MAP) estimate, giving rise to the\nLaplace approximation. In statistics, the Bernstein-von Mises theorem is often used\nto motivate this, but insufficient data to reach concentration of parameters and\nmultimodality in over-parameterized models [Long, 2022] can be an issue for deep\nactive learning and active sampling.\nFlat Minimum Intuition. A positive definite Hessian implies that the information\ncontent (point-wise entropy) is convex around ω∗and, equivalently, that the (log)\nposterior is concave around ω∗. The latter provides an intuition for the Gaussian\napproximation: the Hessian measures curvature, and the “flatter” the Hessian, e.g.,\nthe smaller the largest eigenvalue or the smaller the determinant, the less the loss\nchanges when ω∗is perturbed. This leads to the search for flat minima as a way to\nimprove generalization [Hinton and van Camp, 1993; Hochreiter and Schmidhuber,\n1994; Smith and Le, 2018].\nNotation 9.3. To further shorten the notation, we write H′′[D |ω∗] instead of H′′[{yi}|\n{xi}, ω∗].\nPosterior Approximation of Ω| D. While the Laplace approximation is centered\non a (global) minimizer, the Gaussian approximation can be used for a (potentially\n9. Unifying Approaches in Active Learning and Active Sampling\n158\nlow-quality) posterior approximation in general. We can expand H[ω∗| D] using Bayes’\ntheorem and the additivity of the logarithm. That is, we have:\nH[ω∗| D] = H[D | ω∗] + H[ω∗] −H[D],\n(9.10)\nand then, as H[D] is independent of ω:\nH′[ω∗| D] = H′[D | ω∗] + H′[ω∗] + 0 = H′[D | ω∗] + H′[ω∗],\n(9.11)\nH′′[ω∗| D] = H′′[D | ω∗] + H′′[ω∗].\n(9.12)\nProposition 9.4. The observed information H′′[{yi} | {xi}, ω∗] is additive:\nH′′[{yi} | {xi}, ω∗] =\nX\ni\nH′′[yi | xi, ω∗] =\nX\ni\n−∇2\nω log p(yi | xi, ω∗).\n(9.13)\nNote that the observed information has the opposite sign compared to other works\nbecause it simplifies the exposition.\nUninformative Prior. For a Gaussian prior p(ω) ∼N(µ, Σ), we have H′′[ω∗] = Σ−1\nand H′′[ω∗| D] = H′′[D | ω∗] + Σ−1. For an uninformative prior with “infinite prior\nvariance” Σ−1 →0, we have H′′[ω∗] = 0, and H′′[ω∗| D] = H′′[D | ω∗].\nProposition 9.5. The entropy of the second-order approximation of p(ω) around\nω∗is\nH[Ω] ≈−1\n2 log det H′′[ω∗] + Ck,\n(9.14)\nwhere Ck = k\n2 log 2πe is a constant (independent of D and ω∗) and k is the number\nof dimensions of ω.\nWhile Proposition 9.5 is straightforward, it is the main result for this section as it\nwill allow us to approximate all the mentioned information quantities in §9.4 and §9.5.\n9.3\nFisher Information\nFisher information plays a central role in the approximations of information quantities\nbecause, unlike the observed information, it is always positive semi-definite. We use\nFisher information to unify various acquisition functions in §9.6. The following section\nrevisits Fisher information, its properties, special cases, and common approximations.\nAll proofs are given in §H.1.\nIn particular, we look at two special cases with more favorable properties: following\nKunstner et al. [2019], when we can write our model as p(y | ˆz = ˆf(x; ω)), where\nˆf(x; ω) are the logits, and p(y | ˆz) is a distribution from the exponential family,\nFisher information is independent of y, which has useful consequences as we shall\nsee; and following Chaudhuri et al. [2015], when we have a Generalized Linear Model\n(GLM), observed information also is independent of y. The results for the GLM are\noften applied as an approximation known as Generalized Gauss-Newton approximation\n(GGN). Together with numerical approximations, such as a diagonal approximation\nor low-rank factorizations, observed information and Fisher information can then be\nefficiently approximated for large deep neural networks [Daxberger et al., 2021].\n9. Unifying Approaches in Active Learning and Active Sampling\n159\nDefinition 9.1. The Fisher information H′′[Y | x, ω∗] is the expectation over observed\ninformation using the model’s own predictions p(y | x, ω∗) for a given x at ω∗:\nH′′[Y | x, ω∗] = Ep(y|x,ω∗)[H′′[y | x, ω∗]].\n(9.15)\nThis notation of the Fisher information is consistent with the notation for infor-\nmation quantities we have used far (§1.2.1 and §2), but extended to the observed\ninformation: the Fisher is but an expectation over the observed information, and the\nobserved information is the Hessian of the negative log-likelihood.\nProposition 9.6. Like observed information, Fisher information is additive:\nH′′[{Yi} | {xi}, ω∗] =\nX\ni\nH′′[{Yi} | xi, ω∗].\n(9.16)\nThere are two other equivalent definitions of Fisher information:\nProposition 9.7. Fisher information is equivalent to:\nH′′[Y | x, ω∗] = Ep(y|x,ω∗)[H′[y | x, ω∗]T H′[y | x, ω∗]] = Cov[H′[Y | x, ω∗]].\n(9.17)\nSpecial Case: Exponential Family. Kunstner et al. [2019] show in their appendix\nthat if we split a discriminative model into prelogits ˆf(x; ω) and a predictor p(y | ˆz =\nˆf(x; ω)), Fisher information does not depend on y when p(y|ˆz) is a distribution from an\nexponential family (independent of ω). This covers a normal distribution for regression\nparameterized by mean and variance predictions or a categorical distribution via the\nsoftmax function. The following statements and proofs follow Kunstner et al. [2019]:\nProposition 9.8. The Fisher information H′′[Y | x, ω∗] for a model p(y | ˆz = ˆf(x; ω∗))\nis equivalent to:\nH′′[Y | x, ω∗] = ∇ω ˆf(x; ω∗)\nT Ep(y|x,ω∗)[∇2\nˆz H[y | ˆz = ˆf(x; ω∗)]]∇ω ˆf(x; ω∗),\n(9.18)\nwhere ∇2\nˆz H[y | ˆz = ˆf(x; ω∗)] is short for ∇2\nˆz H[y | ˆz]|ˆz= ˆf(x;ω∗).\nProposition 9.9. The Fisher information H′′[Y | x, ω∗] of a model of the form p(y |\nˆz = ˆf(x; ω∗)) is independent of y, where p(y | ˆz) is a distribution from an exponential\nfamily, i.e., log p(y | ˆz) = ˆzTT(y) −A(ˆz) + log h(y):\nH′′[Y | x, ω∗] = ∇ω ˆf(x; ω∗)\nT ∇2\nˆzA(ˆz = ˆf(x; ω∗)) ∇ω ˆf(x; ω∗).\n(9.19)\nIt is crucial that the exponential distribution not depend on ω. This simplifies\ncomputing Fisher information: no expectation over ys is needed anymore. The full\nouter product may not be needed explicitly either.\nAs examples, we will consider two common parameteric distributions from the\nexponential family:\nGaussian Distribution. When p(y | ˆz) = N(y; ˆz, 1), we have H′′[y | ˆz] = 1 for\nall y, ˆz, and thus\nH′′[Y | x, ω∗] = ∇ω ˆf(x; ω∗)\nT ∇ω ˆf(x; ω∗).\n(9.20)\n9. Unifying Approaches in Active Learning and Active Sampling\n160\nCategorical Distribution. When p(y | ˆz) = softmax(ˆz)y, we have H′′[y | ˆz] =\ndiag(π) −π πT, with πy = p(y | ˆz), and thus:\nH′′[Y | x, ω∗] = ∇ω ˆf(x; ω∗)\nT (diag(π) −π πT) ∇ω ˆf(x; ω∗).\n(9.21)\nSpecial Case: Generalized Linear Models. Chaudhuri et al. [2015] require that\nobserved information is independent of y, which we will also use later. This holds\nfor Generalized Linear Models:\nDefinition 9.2. A generalized linear model (GLM) is a model p(y | ˆz = ˆf(x; ω))\nsuch that log p(y | ˆz) = ˆzTT(y) −A(ˆz) + log h(y) is a distribution of the exponential\nfamily, independent of ω, and ˆf(x; ω) = ωT x is linear in the parameters ω.\nProposition 9.10. The observed information H′′[y | x, ω∗] of a GLM is independent\nof y.\nH′′[y | x, ω∗] = ∇ω ˆf(x; ω∗)\nT ∇2\nˆz H[y | ˆz = ˆf(x; ω∗)] ∇ω ˆf(x; ω∗)\n(9.22)\n= ∇ω ˆf(x; ω∗)\nT ∇2\nˆzA(wTx) ∇ω ˆf(x; ω∗).\n(9.23)\nProposition 9.11. For a model such that the observed information H′′[y | x, ω∗] is\nindependent of y, we have:\nH′′[Y | x, ω∗] = H′′[y∗| x, ω∗]\n(9.24)\nfor any y∗, and also trivially:\nH′′[Y | x, ω∗] = Ep(y|x)[H′′[y | x, ω∗]].\n(9.25)\nNote that the expectation is over p(y|x) and not p(y|x, ω∗), and Ep({yi}|{xi})[H′′[{yi}|\n{xi}, ω∗]] = H′′[{Yi} | {xi}, ω∗] is additive then.\nProposition 9.12. For a GLM, when ˆf(x; ω) : RD →RC, where C is the number of\nclasses (outputs), D is the number of input dimensions, ω ∈RD×C, and assuming the\nparameters are flattened into a single vector for the Jacobian, we have ∇ω ˆf(x; ω) =\nIdC ⊗xT ∈RC×(C·D), where ⊗denotes the Kronecker product, and:\n∇ω ˆf(x; ω∗)\nT ∇2\nˆzA(ωTx) ∇ω ˆf(x; ω∗) = ∇2\nˆzA(wTx) ⊗x xT.\n(9.26)\nThis property is useful for computing the Fisher information of a GLM in practice\n[Ash et al., 2021].\np(y | x, ω∗) vs p(y | x). Having a GLM solves an important issue we will encounter\nin §9.4: approximating the EIG requires taking an expectation over p(y | x) and not\np(y | x, ω∗). One can approximate p(y | x) ≈p(y | x, ω∗), which can be justified in\nthe limit, but this is probably not a good approximation in the cases interesting for\nactive learning and active sampling. With a GLM, this is not a problem.\n9. Unifying Approaches in Active Learning and Active Sampling\n161\nGeneralized Gauss-Newton Approximation. In the case of an exponential fam-\nily but not a GLM, the equality in Proposition 9.10 is often used as an approximation\nfor the observed information—we simply use the respective Fisher information as\nan approximation of observed information (via Proposition 9.9):\nH′′[y | x, ω∗] ≈H′′[Y | x, ω∗] = ∇ω ˆf(x; ω∗)\nT ∇2\nˆzA(wTx) ∇ω ˆf(x; ω∗).\n(9.27)\nThis is known as Generalized Gauss-Newton (GGN) approximation [Kunstner et al.,\n2019; Immer et al., 2021]. This approximation has the advantage that it is always\npositive semi-definite unlike the true Hessian.\nLast-Layer Approaches. GLMs are often used in deep active learning [Ash et al.,\n2020, 2021; Kothawade et al., 2022, 2021]. If we split the model into p(y | x, ω) =\np(y | z = ωT f(x)), where z = f(x) are the embeddings and treat the encoder\nf(x) as fixed, we obtain a GLM based on the weights of the last layer, which\nuses the embeddings as input.\nArmed with this knowledge, we can now derive approximations for the information\nquantities of interest using observed information and Fisher information and consider\ntheir properties. The GGN approximation and last-layer approaches feature heavily in\nthe literature to make computing these approximations more tractable as they reduce\ncomputational requirements and memory usage.\n9.4\nApproximating Information Quantities\nWe now derive approximations and proxy objectives for information quantities. We\nbase them on observed information and Fisher information introduced in the previous\nsections. These approximations help us connect the information quantities to existing\nthe literature in non-Bayesian data subset selection in §9.6.\nIn particular, we derive approximations for EIG and EPIG as they show the\nqualitative differences between non-transductive and transductive objectives, and\ncompare the approximations of the IG and EIG: importantly, there is no difference\nbetween the latter when we use a GLM or the GGN approximation. This covers two\nof the three dimensions in Table 9.1. We examine JEPIG and the other quantities in\nthe appendix in §H.2. We find that the trace approximations of the EPIG and JEPIG\nobjective matches, suggesting that using the trace approximations might be too loose\nan approximation to capture important qualities of EPIG (§7). Additional derivations\nand details can also be found in §H.2. All this leads to Figure 9.1, which relates the\ndifferent approximations to each other and shows that they follow simple patterns.\n9.4.1\nApproximate Expected Information Gain\nThe expected information gain is a popular acquisition function in Bayesian optimal\nexperimental design [Lindley, 1956] and in active learning, where it is also known\nas BALD [Houlsby et al., 2011; Gal et al., 2017].\nWe can approximate the EIG I[Ω; {Y acq\ni\n} | {xacq\ni\n}] of acquisition candidates {xacq\ni\n}\nusing Gaussian approximations:\nI[Ω; {Y acq\ni\n} | {xacq\ni\n}]\n9. Unifying Approaches in Active Learning and Active Sampling\n162\n= H[Ω] −H[Ω| {Y acq\ni\n}, {xacq\ni\n}]\n(9.28)\n= H[Ω] −Ep({yacq\ni\n}|{xacq\ni\n})[H[Ω| {yacq\ni\n}, {xacq\ni\n}]]\n(9.29)\n≈−1\n2 log det H′′[ω∗] −Ep({yacq\ni\n}|{xacq\ni\n})[−1\n2 log det H′′[ω | {yacq\ni\n}, {xacq\ni\n}]]\n(9.30)\n= 1\n2 Ep({yacq\ni\n}|{xacq\ni\n})[log det\n\u0010\n(H′′[{yacq\ni\n} | {xacq\ni\n}, ω∗] + H′′[ω∗]) H′′[ω∗]−1\u0011\n]\n(9.31)\n= 1\n2 Ep({yacq\ni\n}|{xacq\ni\n})[log det\n\u0010\nH′′[{yacq\ni\n} | {xacq\ni\n}, ω∗] H′′[ω∗]−1 + Id\n\u0011\n].\n(9.32)\nusing Proposition 9.5 twice, where the constant Ck cancels out in Equation 9.30 as\nwe subtract two entropy terms.\nGeneralized Linear Model. When we have a GLM, we can use Proposition 9.11 to ob-\ntain:\nI[Ω; {Y acq\ni\n} | {xacq\ni\n}]\n(9.33)\n≈. . . = 1\n2 Ep({yacq\ni\n}|{xacq\ni\n})[log det\n\u0010\nH′′[{yacq\ni\n} | {xacq\ni\n}, ω∗] H′′[ω∗]−1 + Id\n\u0011\n]\n(9.34)\n= 1\n2log det\n\u0010\nH′′[{Y acq\ni\n} | {xacq\ni\n}, ω∗] H′′[ω∗]−1 + Id\n\u0011\n.\n(9.35)\nWe can upper-bound the log determinant and obtain:\n≤1\n2 tr\n\u0010\nH′′[{Y acq\ni\n} | {xacq\ni\n}, ω∗] H′′[ω∗]−1\u0011\n(9.36)\n= 1\n2\nX\ni\ntr\n\u0010\nH′′[{Y acq\ni\n} | xacq\ni, ω∗] H′′[ω∗]−1\u0011\n.\n(9.37)\nwhere we have used the following inequality (proof in §H.2.1):\nLemma 9.13. For symmetric, positive semi-definite matrices A, we have (with equality\niff A = 0):\nlog det(A + Id) ≤tr(A).\n(9.38)\nGeneral Case & Exponential Family. For the general case, we need to make\na strong approximation:\np({yacq\ni\n} | {xacq\ni\n}) ≈p({yacq\ni\n} | {xacq\ni\n}, ω∗),\n(9.39)\nwhich might hold for a mostly converged posterior but probably not in cases with little\ndata. This turns the approximation into an upper bound. Alternatively, we could\nuse the GGN approximation when we have an exponential family for the same result\n(but not an upper bound). See §H.2.1 for the derivation.\n9. Unifying Approaches in Active Learning and Active Sampling\n163\nProposition 9.14 (EIG). The expected information gain can be approximately upper\nbounded via:\nI[Ω; {Y acq\ni\n} | {xacq\ni }, Dtrain]\n≈\n≤1\n2log det\n X\ni\nH′′[Y acq\ni | xacq\ni, ω∗] H′′[ω∗| Dtrain]−1 + Id\n!\n(9.40)\n≤1\n2\nX\ni\ntr\n\u0010\nH′′[Y acq\ni | xacq\ni, ω∗] H′′[ω∗| Dtrain]−1\u0011\n.\n(9.41)\nFurthermore, we have the following proxy objective:\narg max\n{xacq\ni\n}\nI[Ω; {Y acq\ni\n} | {xacq\ni }, Dtrain] = arg max\n{xacq\ni\n}\n−H[Ω| {Y acq\ni\n}, {xacq\ni }, Dtrain], (9.42)\nwith\n−H[Ω| {Y acq\ni\n}, {xacq\ni }, Dtrain]\n≈\n≤1\n2 log det\n X\ni\nH′′[Y acq\ni | xacq\ni, ω∗] + H′′[ω∗| Dtrain]\n!\n−Ck.\n(9.43)\nThe second statement follows from Equation 9.28, since H[Ω| Dtrain] is constant\nand provides a proxy objective when we are only interested in optimizing the EIG. In\n§9.6, we connect it to the expected gradient length approach in active learning and\nshow that an ablation in Ash et al. [2021] examines the wrong objective.\nBatch Acquisition Pathologies. Importantly, this approximation of the EIG using\nthe trace is additive, whereas the one using the log determinant is not. This means\nthat the trace approximation ignores the dependencies between the samples and\ncan only lead to naive top-k batch acquisition; see ?? and §5 for details of the\npathologies of top-k batch acquisition.\n9.4.2\nApproximate Information Gain\nFollowing the same steps, we can also approximate the information gain, which is\nuseful for active sampling:\n9. Unifying Approaches in Active Learning and Active Sampling\n164\nProposition 9.15 (IG). The information gain I[Ω; {yacq\ni\n} | {xacq\ni }, Dtrain] = H[Ω|\nDtrain] −H[Ω| {yacq\ni\n}, {xacq\ni }, Dtrain] can be approximately upper bounded via:\nI[Ω; {yacq\ni\n} | {xacq\ni }, Dtrain]\n≈1\n2log det\n\u0010\nH′′[{yacq\ni\n} | {xacq\ni }, ω∗] H′′[ω∗| Dtrain]−1 + Id\n\u0011\n(9.44)\n≤1\n2\nX\ni\ntr\n\u0010\nH′′[yacq\ni | xacq\ni, ω∗] H′′[ω∗| Dtrain]−1\u0011\n.\n(9.45)\nFurthermore, we have the following proxy objective:\narg max\n{xacq\ni\n}\nI[Ω; {yacq\ni\n} | {xacq\ni }, Dtrain] = arg max\n{xacq\ni\n}\n−H[Ω| {yacq\ni\n}, {xacq\ni }, Dtrain]\n(9.46)\nwith\n−H[Ω| {yacq\ni\n}, {xacq\ni\n}, Dtrain] ≈1\n2 log det\n\u0000H′′[{yacq\ni\n} | {xacq\ni\n}, ω∗] + H′′[ω∗| Dtrain]\n\u0001\n−Ck. (9.47)\nComparison to EIG. Importantly, when we have a GLM or use the GGN approxi-\nmation, this approximation of the IG is equal to the one of the EIG. This tells us that\nactive learning on a GLM with the EIG approximation will work as well as if we had\naccess to the labels. Equivalently, active sampling via IG with the GGN approximation\nwill not work better than the respective active learning approach.\n9.4.3\nApproximate (Joint) Expected Predictive Information\nGain\nIn transductive active learning, we have access to an (empirical) distribution ˆptrue(xeval),\ne.g., the pool set, and want to find the {xacq\ni\n} that maximize the expected predictive\ninformation gain from §7). The approximations here will help us connect BAIT [Ash\net al., 2021] to EPIG. For simplicity, we consider the non-batch case here. The batch\ncase can be handled analogously. The EPIG objective is defined as:\narg max\nxacq\nI[Y eval; Y acq | Xeval, xacq] = arg max\nxacq\nEˆptrue(xeval) I[Y eval; Y acq | xeval, xacq],\n(9.48)\nWe expand the objective as follows:\nI[Y eval; Y acq | Xeval, xacq] = I[Ω; Y eval | Xeval] −I[Ω; Y eval | Xeval, Y acq, xacq],\n(9.49)\nwhere I[Ω; Y eval | Xeval] can be removed from the objective because it is independent of\nxacq. Thus, optimizing EPIG is equivalent to minimizing I[Ω; Y eval | Xeval, Y acq, xacq]:\narg max\nxacq\nI[Y eval; Y acq | Xeval, xacq] = arg min\nxacq\nI[Ω; Y eval | Xeval, Y acq, xacq].\n(9.50)\nFollowing Proposition 9.14, this can be approximated by:\nI[Ω; Y eval | Xeval, Y acq, xacq]\n≈1\n2 Ep(yeval,yacq|xeval,xacq) ˆptrue(xeval)\n(9.51)\nh\nlog det\n\u0010\nH′′[yeval | xeval, ω∗] (H′′[yacq | xacq, ω∗] + H′′[ω∗])−1 + Id\n\u0011i\n.\n(9.52)\n9. Unifying Approaches in Active Learning and Active Sampling\n165\nGeneralized Linear Model. For a generalized linear model, we can drop the\nexpectation and obtain:\nI[Ω; Y eval | Xeval, Y acq, xacq]\n≈1\n2 Eˆptrue(xeval)[log det\n\u0000H′′[Y eval | xeval, ω∗] (H′′[Y acq | xacq, ω∗] + H′′[ω∗])−1 + Id\n\u0001\n]\n(9.53)\n≤1\n2log det\n\u0000Eˆptrue(xeval)[H′′[Y eval | xeval, ω∗]] (H′′[Y eval | xacq, ω∗] + H′′[ω∗])−1 + Id\n\u0001\n(9.54)\n≤1\n2tr\n\u0010\nEˆptrue(xeval)\nh\nH′′[Y eval | xeval, ω∗]\ni\n(H′′[Y acq | xacq, ω∗] + H′′[ω∗])−1\u0011\n,\n(9.55)\nwhere we have used the concavity of the log determinant and Lemma 9.13.\nGeneral Case & Exponential Family. To our knowledge, there is no rigorous way\nto obtain a similar result in the general case as the Fisher information for an acquisition\ncandidate now lies within an inverted term. Of course, the GGN approximation can\nbe applied when we have an exponential family, which leads to the GLM result above\nas an approximation. See §H.2.2 for more details.\nProposition 9.16 (EPIG). For a generalized linear model (or with the GGN\napproximation), we have:\narg max\n{xacq\ni\n}\nI[Y eval; {Y acq\ni\n} | Xeval, {xacq\ni\n}, Dtrain] = arg min\n{xacq\ni\n}\nI[Ω; Y eval | Xeval, {Y acq\ni\n}, {xacq\ni\n}, Dtrain],\n(9.56)\nwith\nI[Ω; Y eval | Xeval, {Y acq\ni\n}, {xacq\ni }, Dtrain]\n≈Eˆptrue(xeval)[log det\n\u0010\nH′′[Y eval | xeval, ω∗] (H′′[{Y acq\ni\n} | {xacq\ni }, ω∗] + H′′[ω∗])−1 + Id\n\u0011\n]\n(9.57)\n≤1\n2log det\n\u0000Eˆptrue(xeval)[H′′[Y eval | xeval, ω∗]] (H′′[{Y acq\ni\n} | {xacq\ni\n}, ω∗] + H′′[ω∗| Dtrain])−1 + Id\n\u0001\n(9.58)\n≤1\n2tr\n\u0010\nEˆptrue(xeval)\nh\nH′′[Y eval | xeval, ω∗]\ni\n(H′′[{Y acq\ni\n} | {xacq\ni }, ω∗] + H′′[ω∗| Dtrain])−1\u0011\n.\n(9.59)\nBatch Acquisition Pathologies. Unlike for the EIG, the trace approximation of\nEPIG is not additive in {xacq\ni\n}, and we cannot conclude that it suffers from batch\nacquisition pathologies like the trace approximation of the EIG.\nApproximations for JEPIG, PIG and JPIG. We can follow the same derivation for\nJEPIG:\nI[Ω; {Y eval\ni\n} | {xeval\ni\n}, Y acq, xacq]\n(9.60)\n≈1\n2log det\n\u0010\nEp({yeval\ni\n},yacq|{xeval\ni\n},xacq)[H′′[{yeval\ni\n} | {xeval\ni\n}, ω∗]] (H′′[yacq | xacq, ω∗] + H′′[ω∗])−1 + Id\n\u0011\n.\nThen, applying the steps after Equation 9.54, we can devise similar approximations.\nPIG and JPIG follow the same pattern. Details can be found in §H.2.3 and §H.2.4.\nBut how do all these approximations relate to each other?\n9. Unifying Approaches in Active Learning and Active Sampling\n166\nEIG\ntransductive?\nEPIG\nw/ labels\nPIG\nIG\nJEPIG\nJPIG\nw/ labels\nw/ labels\njoint?\nFisher Information\nObserved Information\n(a) log det Proxy Objectives\nEIG\ntransductive?\nIG\nw/ labels\nFisher Information\nObserved Information\n(J)EPIG\n(J)PIG\nw/ labels\n(b) Matrix Trace Proxy Objectives\nFigure 9.1: Comparison of the Approximations/Proxy Objectives. The difference between\nactive learning and active sampling objectives is in using the Fisher information, which is\nlabel independent, or the observed information, which uses label information. JEPIG and\nEPIG have equivalent proxy objectives when using the matrix trace; see §H.2.4. The notation\nmakes it obvious that in the GLM case, or when the GGN approximation is used, active\nlearning and active sampling approximations match as H′′[y | x, ω∗] = (resp. ≈) H′′[Y | x, ω∗].\n9.4.4\nComparison of the Different Information Quantity Ap-\nproximations\nFigure 9.1 compares the different information quantity approximations for both the log-\ndeterminant and trace approximations. We empirically compare the approximations\nwith prediction-space methods in §H.5. Importantly, the trace approximations of\n(E)PIG match those of J(E)PIG up to a constant factor (unlike the log-determinant\napproximations); see §H.2.4 for details.\nIn §9, we argued that JEPIG converges to BALD in the data limit of the evaluation\nset—when there are no outliers in the pool set— while EPIG does not. The trace\napproximation is too strong to preserve this difference. Does this difference matter\nin practice? We leave this for future work.\nCrucially, for GLMs or when using the GGN approximation, the respective active\nlearning and active sampling objectives (EIG and IG, etc.) are equivalent as Fisher\n9. Unifying Approaches in Active Learning and Active Sampling\n167\ninformation and observed information are the same. In contrast, in the general case,\nthe approximations for EPIG and JEPIG do not have a principled derivation.\n9.5\nSimilarity Matrices and One-Sample Approx-\nimations\nMany data subset selection methods [Iyer et al., 2021; Kothawade et al., 2022, 2021;\nAsh et al., 2020] use similarity matrices of the loss Jacobians H′[ˆy |x] (gradient kernels),\nwhere ˆy is usually a hypothesized pseudo-label: often the arg max prediction of the\nmodel for x. Here, we connect such similarity matrices to the Fisher information\nand the approximations of information quantities from §9.4. The proofs are given in\n§H.3. Together with §9.4, this section provides a unified framework for understanding\nthe approximations of information quantities using Fisher information and lays the\nfoundation for the next section, which will connect the cited works in §9 to the\napproximations of information quantities.\nConnection to Fisher Information. Crucially, given D = {(yi, xi)i}, if we let\nˆH′[D | ω∗] ≜\n\n\n\n\n\n\n\n...\nH′[yi | xi, ω∗]\n...\n\n\n\n\n\n\n\n(9.61)\nbe a “data matrix” of the Jacobians, then ˆH′[D | ω∗]ˆH′[D | ω∗]\nT gives the similarity\nmatrix S[D | ω∗] using the Euclidean inner product:\nS[D | ω]ij ≜⟨H′[yi | xi, ω∗], H′[yj | xj, ω∗]⟩= ˆH′[D | ω∗]ˆH′[D | ω∗]\nT.\n(9.62)\nSampling {yi} ∼p({yi} | {xi}, ω∗), the “flipped” product ˆH′[D | ω∗]\nT ˆH′[D | ω∗] yields\na one-sample estimate of the Fisher information H′′[{Yi} | {xi}, ω∗]:\nH′′[{Yi} | {xi}, ω∗] =\nX\ni\nH′′[Yi | xi, ω∗] = Ep({yi}|{xi},ω∗)\nX\ni\nH′[yi | xi, ω∗]T H′[yi | xi, ω∗]\n(9.63)\n= Ep({yi}|{xi},ω∗) ˆH′[D | ω∗]\nT ˆH′[D | ω∗].\n(9.64)\nHard Pseudo-Labels. Importantly, using the arg max class for yi, we only obtain\na biased estimate [Kunstner et al., 2019, §B].\nConnection to the Expected Information Gain. When we define an inner product\n⟨·, ·⟩H′′[ω∗|Dtrain] using the Hessian, we can connect the similarity matrix, which uses\nthis inner product:\nSH′′[ω∗|Dtrain][D | ω∗] ≜ˆH′[D | ω∗] H′′[ω∗| Dtrain]−1 ˆH′[D | ω∗]\nT,\n(9.65)\nto our information gain approximations.\nSpecifically, we apply the matrix-determinant lemma det(AB+M) = det M det(Id+\nBM −1A) to obtain:\n9. Unifying Approaches in Active Learning and Active Sampling\n168\nProposition 9.17. Given Dtrain, {xacq\ni } and (sampled) {yacq\ni\n}, we have for the EIG:\nI[Ω; {Y acq\ni\n} | {xacq\ni }, Dtrain]\n≈\n≤1\n2log det\n\u0010\nSH′′[ω∗|Dtrain][Dacq | ω∗] + Id\n\u0011\n(9.66)\n≤1\n2tr SH′′[ω∗|Dtrain][Dacq | ω∗]\n(9.67)\nProposition 9.18. Assuming an uninformative posterior H′′[ω∗| Dtrain] = λId for\nλ →0, and given Dtrain, {xacq\ni }, and (sampled) {yacq\ni\n}, we have for the EIG (before\ntaking λ →0):\nI[Ω; {Y acq\ni\n} | {xacq\ni }, Dtrain]\n≈\n≤1\n2 log det (S[Dacq | ω∗] + λId) −|Dacq|\n2\nlog λ.\n(9.68)\nAs the second term is independent of {xacq\ni }, we can use the following proxy objective\nin the limit:\n1\n2 log det (S[Dacq | ω∗]) .\n(9.69)\nConnection to Other Approximate Information Quantities. Interestingly, we\ncan use the above to obtain approximations of the predictive information gains (EPIG\nand JEPIG) because the terms that would tend towards −∞cancel out; see §H.3\nfor details.\nFor EPIG, we have:\nProposition 9.19. Given Deval, Dtrain, {xacq\ni } and (sampled) {yacq\ni\n}, we have for the\nEPIG:\nI[Y eval; {Y acq\ni\n} | Xeval, {xacq\ni }, Dtrain]\n≈1\n2 log det\n\u0000SH′′[ω∗|Dtrain][Deval | ω∗] + Id\n\u0001\n−1\n2log det\n\u0000SH′′[ω∗|Dtrain][Dacq, Deval | ω∗] + Id\n\u0001\n+ 1\n2log det\n\u0000SH′′[ω∗|Dtrain][Dacq | ω∗] + Id\n\u0001.\n(9.70)\nFor an uninformative prior, we have:\n≈1\n2log det\n\u0000S[Deval | ω∗]\n\u0001\n−1\n2 log det\n\u0000S[Dacq, Deval | ω∗]\n\u0001\n+ 1\n2 log det (S[Dacq | ω∗]).\n(9.71)\nWe can drop the terms that only depend on Deval when we are interested in proxy\nobjectives for optimization.\nThese results help connect the objectives of PRISM and SIMILAR to the EIG\nand EPIG in the next section.\n9.6\nInformation Quantities in Prior Literature\nNow, we can connect approaches in non-Bayesian literature to information quantities.\nAdditional proofs are given in §H.4.\n9.6.1\nBAIT, ActiveSetSelect, and (J)EPIG\nBAIT in “Gone Fishing” [Ash et al., 2021], ActiveSetSelect in “Convergence Rates\nof Active Learning for Maximum Likelihood Estimation” [Chaudhuri et al., 2015],\nand (J)EPIG (§7) approximate the same objective.\n9. Unifying Approaches in Active Learning and Active Sampling\n169\nAsh et al. [2021] introduce the BAIT objective for deep active learning:\narg min\n{xacq\ni\n}\ntr\n\u0010\u0000H′′[{Y acq\ni\n} | {xacq\ni\n}, ω∗] + H′′[Y train | xtrain, ω∗] + λI\n\u0001−1 H′′[{Y eval\ni\n} | {xeval\ni\n}, ω∗]\n\u0011\n,\n(BAIT)\nwhere λ is a hyperparameter2.\nBAIT is based on a similar objective for GLMs from Chaudhuri et al. [2015].\nWhile Ash et al. [2021] apply this objective to DNNs, they only use the last layer\nto approximate the Fisher information. The last layer, with appropriate activation\nfunctions and losses, constitutes a GLM as seen in §9.3.\nFollowing Proposition 9.16, we immediately see that Ash et al. [2021] perform\ntransductive active learning (using the pool set as an evaluation set) and approximate\na proxy objective for (J)EPIG:\nProposition 9.20. Both Ash et al. [2021] and Chaudhuri et al. [2015] perform\ntransductive active learning, approximating (J)EPIG (§7) using a last-layer approach\n(or GLM):\narg max\nxacq\nI[Y eval; {Y acq\ni\n} | Xeval, {xacq\ni }]\n(9.72)\n≈arg min\nxacq\ntr(H′′[{Y eval\ni\n} | {xeval\ni\n}, ω∗] (H′′[{Y acq\ni\n} | {xacq\ni }, ω∗] + H′′[ω∗| Dtrain])−1),\nwith H′′[ω∗| Dtrain] = H′′[Dtrain | ω∗] + H′′[ω∗] and H′′[ω∗] = λ Id.\nProof. This follows immediately for GLM (last-layer approaches) when we expand\nH′′[ω∗| Dtrain]. Chaudhuri et al. [2015] in particular uses an uninformative prior, that\nis λ = 0. Comparing the resulting objectives yields the statement.\nThus, Ash et al. [2021] and we in §7 employ the same underlying acquisition\nfunction, albeit using very different approaches: Ash et al. [2021] use a last-layer\nFisher information matrix, whereas we use approximate BNNs and sample joint\npredictions in §7.\nResearch Questions. EPIG is not submodular, and the greedy selection of an\nacquisition batch does not come with any optimality guarantees. While we sidesteps this\nin §7 by focusing on individual acquisitions mainly, Ash et al. [2021] propose a heuristic\nthat empirically performs better: They greedily select additional acquisition candidates\nin forward pass (twice the intended batch acquisition size) and then greedily remove\nthe least informative samples from the batch in a backward pass. Would this heuristic\nalso prove beneficial for all the other information quantities that are not submodular?\nWhile Ash et al. [2021] state that they only use the last-layer approach for\nperformance reasons, following §9.3, it does not seem that this approach translates\nbeyond a last-layer approach for DNNs in a principled fashion (see §H.2.2). Is there\na principled approach for the general case that goes beyond last-layer active learning\nwhen using Fisher information without the GGN approximation?\n2This is the BAIT objective as computed in Algorithm 1 in Ash et al. [2021] and in the pub-\nlished implementation https://github.com/JordanAsh/badge/blob/master/query_strategies/\nbait_sampling.py.\n9. Unifying Approaches in Active Learning and Active Sampling\n170\nAsh et al. [2021] ablate trace and determinantal approaches, similar to comparing\nEquation 9.59 and Equation 9.58, yet they do not include +Id in the log determinant\nexpression, which leads them to examine the EIG in their ablation3:\narg min\nxacq\nlog det(H′′[{Y eval\ni\n} | {xeval\ni\n}, ω∗] (\nX\ni\nH′′[Y acq\ni | xacq\ni, ω∗] + H′′[ω∗| Dtrain])−1)\n= arg min\nxacq\nlog det H′′[{Y eval\ni\n} | {xeval\ni\n}, ω∗] −log det(\nX\ni\nH′′[Y acq\ni | xacq\ni, ω∗] + H′′[ω∗| Dtrain])\n(9.73)\n= arg max\nxacq\nlog det(\nX\ni\nH′′[Y acq\ni | xacq\ni, ω∗] + H′′[ω∗| Dtrain]) + C,\n(9.74)\nand the last term matches the EIG in Equation 9.58 up constant terms independent of\nxacq. Thus, the ablations in Ash et al. [2021] only compares EIG and EPIG. Could\ncomparing Equation 9.59 and Equation 9.58 provide more insightful results about the\ntrade-offs between trace and determinant approximations?\n9.6.2\nBADGE and BatchBALD\nBADGE [Ash et al., 2020] performs batch acquisition using a similarity matrix: Using\nthe concepts of §9.5, BADGE uses hard pseudo-labels together with last-layer gradient\nembeddings for the similarity matrix S[Dacq | ω∗]. The authors sample from a k-DPP\n[Kulesza and Taskar, 2011] based on this similarity matrix to select a diverse batch\nof samples for acquisition. However, to further speed up acquisitions, BADGE uses\nk-MEANS++ [Arthur and Vassilvitskii, 2007; Ostrovsky et al., 2012] instead of a\nk-DPP: it uses the Jacobians H′[y | x, ω∗] of the data matrix directly and samples a\ndiverse batch based on the Euclidean distance between these Jacobians. However,\nsampling from k-DPPs does not pick the most informative batch overall, and the\nablations in Ash et al. [2020] show that k-MEANS++ outperforms k-DPP. Finally,\nthe paper only motivates using gradient embeddings with hard pseudo-labels through\nintuitions: the gradient length captures information about the model’s uncertainty,\nand diverse update directions capture information about the model’s diversity [Ash\net al., 2020]. The paper makes no explicit connection to information theory.\nFollowing Proposition 9.17, since BADGE can be seen as using a last-layer approach\nfor the similarity matrix S[Dacq | ω∗] with hard pseudo-labels, BADGE approximates\nI[Ω; {Y acq\ni\n} | {xacq\ni\n}, Dtrain] with an uninformative posterior distribution:\nProposition 9.21. BADGE maximizes an approximation of the EIG with an\nuninformative prior.\nComparison to BatchBALD. Similarly, BatchBALD (§4) approximates the EIG\nin the batch acquisition case but by using prediction-space samples.\nMoreover,\nBatchBALD uses a greedy approach to select batch candidates instead of sampling\nvia a k-DPP or k-MEANS++.\nAs the EIG is submodular, determining the acquisition batch is a submodular\noptimization problem and, therefore, can be solved by greedy selection with 1 −1\ne\noptimality [Nemhauser et al., 1978].\n3Ash et al. [2021] accidentally writes arg max instead of arg min in §5.1 in their paper, but c.f.\nAlgorithm 1 with the trace objective. Algorithm 2 & 3 in §B in the appendix use the correct final\nobjective.\n9. Unifying Approaches in Active Learning and Active Sampling\n171\nResearch Questions. Hard pseudo-labels lead to biased estimates.\nWould one-\nsample estimates perform better? And could greedy batch selection work better than\nsampling via a k-DPP? This would be closer to the batch acquisition strategy fol-\nlowed by BatchBALD.\n9.6.3\nSIMILAR and PRISM\nBased on Iyer et al. [2021], Kothawade et al. [2021] and Kothawade et al. [2022]\ninvestigate submodular active learning for DNNs: they take an information function\nf, which is a non-negative, montone/non-decreasing, submodular function (and then\nis also subadditive as a consequence):\nf(A) ≥0,\n(non-negative)\nf(A) ≤f(B) for A ⊆B,\n(monotone)\nf(A ∪B) ≤f(A) + f(B) −f(A ∩B),\n(submodular)\nf(A ∪B) ≤f(A) + f(B)\n(subadditive)\nfor all A, B ⊆Dpool and define a “submodular conditional gain“ and an ”submodular\n(conditional) mutual information” as\nHf(A | B) ≜f(A ∪B) −f(B)\n(9.75)\nIf(A; B) ≜f(A ∪B) −f(A) −f(B).\n(9.76)\nFor f({xi}) = H[{Yi} | {xi}], this simply yields the regular information quantities.\nHence, Kothawade et al. [2021] and Kothawade et al. [2022] examine other information\nfunctions and submodular quantities in the context of active learning: amongst them set\ncovers, graph cuts, facility location, and log determinants (LogDet) of similarity matrices.\nLike BADGE [Ash et al., 2020], the similarity matrix uses hard pseudo-labels. Like\nBatchBALD (§4), they use a greedy approach for acquisition [Nemhauser et al., 1978].\nUsing our results, we immediately see that the LogDet objective, which we can write\nas log det S[Dacq | ω∗], exactly matches the EIG approximation in §9.18; furthermore,\nin §H.4.1, we show that the LogDetMI objective matches an approximation of JEPIG\n(and similarly, derive the LogDetCMI objective as well):\nProposition 9.22. The LogDet objective log det S[Dacq | ω∗] is an approximation of\nthe EIG and the LogDetMI objective\nlog det S[Dacq | ω∗] −log det(S[Dacq | ω∗] −S[Dacq; Deval | ω∗]S[Deval | ω∗]\n−1S[Deval; Dacq | ω∗])\n(9.77)\nis an approximation of a proxy objective for EPIG, where we use S[D1; D2 | ω∗] to\ndenote the (non-symmetric) similarity matrix between D1 and D2.\nNotably, the experimental results for the LogDet-based quantities are reported as\namong the best in Kothawade et al. [2021] and Kothawade et al. [2022]. As such, since\nthe LogDet quantities approximate Shannon’s information quantities (which are not\nexplicitly examined in those works), the promising experimental results compared to\nother submodular information functions support the hypothesis that approximating\nShannon’s information quantities works well in active learning and active sampling.\n9. Unifying Approaches in Active Learning and Active Sampling\n172\nResearch Questions. Similar to BADGE, the scores are biased by using hard pseudo-\nlabels. Could one-sample estimates perform better? Furthermore, as LogDetMI is not\nsubmodular, could the approach from BAIT of expanding and shrinking the acquisition\nbatch in a forward and backward pass improve performance here as well?\n9.6.4\nExpected Gradient Length\nThe Expected Gradient Length (EGL) [Settles et al., 2007; Settles, 2010] is an acquisition\nfunction in active learning and is usually defined for non-Bayesian models. Originally,\nit was an expectation over the gradient norm. In more recent literature [Huang et al.,\n2016], it is introduced using the squared gradient norm:\nEp(yacq|xacq,ω∗)\n\r\r\rH′[yacq | xacq, ω∗]\n\r\r\r\n2.\n(EGL)\nUsing a diagonal approximation of Fisher information, we show in §H.4.2:\nProposition 9.23. The EIG for a candidate sample xacq approximately lower-bounds\nthe EGL:\n2 I[Ω; Y acq | xacq]\n≈\n≤Ep(yacq|xacq,ω∗)\n\r\r\rH′[yacq | xacq, ω∗]\n\r\r\r\n2 + const.\n(9.78)\n9.6.5\nDeep Learning on a Data Diet\nIn active sampling, Paul et al. [2021] use the gradient length of given labeled samples\nx, y (averaged over multiple training runs) as an acquisition function to select the most\ninformative samples from the training set to speed up training:\nEq(ω)\n\r\r\rH′[yacq | xacq, ω]\n\r\r\r\n2,\n(GraNd)\nwhich they call the gradient norm score (GraNd). The expectation is taken over the\nmodel parameters at initialization or after training for a few epochs—as this is not\neasily expressed using a posterior distribution, we use q(w) to denote the distribution.\nProposition 9.24. The IG for a candidate sample xacq approximately lower-bounds\nthe gradient norm score (GraNd) at ω∗up to a second-order term:\n2 I[Ω; yacq | xacq]\n≈\n≤Eq(ω)[\n\r\r\rH′[yacq | xacq, ω]\n\r\r\r\n2] −Eq(ω)[tr\n ∇2\nω p(y | x, ω)\np(y | x, ω)\n!\n] + const.\n(9.79)\nThe second term might not be negligible. Hence, GraNd (the first term on the left)\nmight deviate from the information gain. How does the information gain compares\nto GraNd in practice?\nWe provide a reproducibility analysis in Appendix B.1, which helped discover a\nnow fixed bug in Paul et al. [2021].\n9.7\nDiscussion\nWe have examined Fisher information and Gaussian approximations and have derived\nweight-space approximations for various information quantities. This has allowed us\nto connect these information quantities to objectives already used in the literature.\nMoreover, we can make the following concluding points:\n9. Unifying Approaches in Active Learning and Active Sampling\n173\nLast-Layer Approaches. Methods that only use last-layer Fisher information or\nsimilar perform data subset selection on embeddings only, despite feature learning\nbeing arguably the most important strength of deep neural networks. However, these\napproaches can find great use with large pre-trained models, which are only fine-tuned\non new data domains [Tran et al., 2022].\nBias in Hard Pseudo-Labels. Several methods (BADGE, PRISM, SIMILAR) use\nhard pseudo-labels for computing the similarity matrices, leading to biased estimates.\nCan one-sample estimates perform better? Can the equivalent approximations that\ndo not make use of similarity matrices perform better?\nTrace vs log det Approximations. We have presented a hierarchy of approximations\nand bounds. Ablating these approximations along multiple dimensions, including\nwhether to use the trace or log det approximation and whether to use a GLM, the\nGGN approximation or the full Fisher information, could provide interesting insights\ninto what is attainable.\nBatch Acquisition Pathologies. Approaches that use the matrix trace instead of the\nlog determinant can end up being additive for batch candidates {xacq\ni\n} and, therefore,\nby definition, cannot take redundancies between batch candidates into account, leading\nto failures detailed in §4 and §5. This is an issue for trace approximations of the\n(E)IG. Does the trace approximation of (JE)PIG handle batch acquisition pathologies\nbetter? Similarly, most information quantities are not submodular, yet we use a\ngreedy algorithm to select the acquisition batches. Is the heuristic proposed by Ash\net al. [2021] generally beneficial?\nWeight vs. Prediction Space. BADGE, BAIT, and the LogDet objectives of PRISM\nand SIMILAR [Ash et al., 2020, 2021; Kothawade et al., 2021, 2022] approximate\ninformation quantities in weight space, while (Batch)BALD, (J)EPIG, and (J)PIG\n(§4, §7 and §8) approximate the information quantities in prediction space. Both\napproaches have their limitations:\nWeight-space approaches can suffer from the Gaussian approximation being of low\nquality: the Laplace approximation only captures the posterior distribution well once\nit concentrates sufficiently. However, this is unlikely to happen in a low-data regime.\nPrediction-space approaches can suffer from a combinatorial explosion as the batch\nacquisition size increases because prediction configurations have to be enumerated\nor sampled to approximate the information quantities. In addition, many parameter\nsamples might be needed to obtain low-variance estimates.\nImportantly, prediction space approaches also require drawing samples from the\nposterior distribution but do not estimate the information quantities using the posterior\ndistribution, unlike weight space approaches.\nInformativeness Scores. Taking a step back, we have seen that a Bayesian perspec-\ntive using information quantities connects seemingly disparate literature. Although\nBayesian methods are often seen as separate from (non-Bayesian) active learning and\nactive sampling, the sometimes fuzzy notion of “informativeness” expressed through\nvarious different objectives in non-Bayesian settings collapses to the same couple of\ninformation quantities, which were, in principle, already known by Lindley [1956]\nand MacKay [1992b].\nOld paths tread anew,\nIn light of insights accrued,\nPublications grew.\n10\nBlack-Box Batch Active Learning for\nRegression\nBy selectively acquiring labels for a subset of available unlabeled data, active learning\n[Atlas et al., 1989] is suited for situations where the acquisition of labels is costly or\ntime-consuming, such as in medical imaging or natural language processing. However,\nin deep learning, many recent batch active learning methods have focused on white-box\napproaches that rely on the model being parametric and differentiable and which use\nfirst or second-order derivatives (e.g. model embeddings)1.\nThis can present a limitation in real-world scenarios where model internals or\ngradients might not be accessible—or might be expensive to access. This is particularly\ntrue in the case of ‘foundation models‘ [Bommasani et al., 2021] and large language\nmodels such as GPT-3 [Brown et al., 2020], for example, when accessed via a third\nparty. More generally, a lack of differentiability might hinder application of white-box\nbatch active learning approaches to non-differentiable models.\nTo address these limitations, we examine black-box batch active learning (B3AL) for\nregression which is compatible with a wider range of machine learning models. By black-\nbox, we mean that our approach only relies on model predictions and does not require\naccess to model internals or gradients. Our approach is rooted in Bayesian principles\nand only requires model predictions from a small (bootstrapped) ensemble. Specifically,\nwe utilize an (empirical) predictive covariance kernel based on sampled predictions.\nWe show that the well-known gradient kernel [Kothawade et al., 2021, 2022; Ash et al.,\n2020, 2021] can be seen as an approximation of this predictive covariance kernel.\nThe proposed approach extends to non-differentiable models through a Bayesian\nview on the hypothesis space formulation of active learning, based on the ideas behind\nquery-by-committee [Seung et al., 1992]. This enables us to use batch active learning\nmethods, such as BAIT [Ash et al., 2021] and BADGE [Ash et al., 2020] in a black-box\nsetting with non-differentiable models, such as random forests or gradient-boosted trees.\nWe evaluate black-box batch active learning on a diverse set of regression datasets.\nUnlike the above white-box parametric active learning methods which scale in the\nnumber of (last-layer) model parameters or the embedding size, our method scales in\nthe number of drawn predictions, and we show that we can already obtain excellent\nresults with a small ensemble. Our results demonstrate the label efficiency of B3AL\nfor various machine learning models. For deep learning models, B3AL even performs\nbetter than the corresponding state-of-the-art white-box methods in many cases.\n1Model embeddings can also be seen as first-order derivatives of the model score under regression\nin regard to the last layer.\n10. Black-Box Batch Active Learning for Regression\n175\nWe focus on regression as classification using the same approach would require\nLaplace approximations, Monte Carlo sampling, or expectation propagation [Williams\nand Rasmussen, 2006; Hernández-Lobato et al., 2011]. This complicates a fair compari-\nson as translating existing classification methods in active learning is not straightforward.\nFor the same reasons, we also do not consider proxy-based active learning methods\n[Coleman et al., 2020], which constitutes an orthogonal direction to our investigation.\nIn summary, by leveraging the strengths of kernel-based methods and Bayesian\nprinciples, our approach improves the labeling efficiency of a range of differentiable\nand non-differentiable machine-learning models with surprisingly strong performance.\nThe rest of the chapter is organized as follows: in §10.1, we discuss related work in\nactive learning and kernel-based methods. In §10.2, we describe the relevant background\nand provide a detailed description of B3AL. In §10.3, we detail the experimental setup\nand provide the results of our experimental evaluation. Finally, §10.4 concludes with\na discussion and directions for future research.\n10.1\nRelated Work\nDifferentiable Models . Many acquisition functions approximate well-known information-\ntheoretic quantities [MacKay, 1992b], often by approximating Fisher information\nimplicitly or explicitly. This can be computationally expensive, particularly in deep\nlearning where the number of model parameters can be large—even when using last-\nlayer approximations or assuming a generalized linear model (§9). BADGE [Ash et al.,\n2020] and BAIT [Ash et al., 2021] approximate the Fisher information using last-layer\nloss gradients or the Hessian, respectively, but still have a computational cost scaling\nwith the number of last layer weights. This also applies to methods using similarity\nmatrices (kernels) based on loss gradients of last-layer weights such as SIMILAR\n[Kothawade et al., 2022] and PRISM [Kothawade et al., 2022], to only name a few.\nImportantly, all of these approaches require differentiable models.\nNon-Differentiable Models. Query-by-committee (QbC, Seung et al. [1992]) mea-\nsures the disagreement between different model instances to identify informative samples\nand has been applied to regression [Krogh and Vedelsby, 1994; Burbidge et al., 2007].\nKee et al. [2018] extend QbC to the batch setting with a diversity term based on\nthe distance of data points in input space. Nguyen et al. [2012] show batch active\nlearning for random forests. They train an ensemble of random forests and evaluate\nthe joint entropy of the predictions of the ensemble for batch active learning, which\ncan be seen as a special case of BatchBALD in regression.\nBALD. This chapter most closely aligns with the BALD-family of Bayesian active\nlearning acquisition functions [Houlsby et al., 2011], which focus on classification\ntasks, however. The crucial insight of BALD is applying the symmetry of mutual\ninformation to compute the expected information gain in prediction space instead of\nin parameter space. As a result, BALD is a black-box technique that only leverages\nmodel predictions. The estimators utilized by Gal et al. [2017] and also the ones\nfor BatchBALD from §4 enumerate over all classes, leading to a trade-off between\ncombinatorial explosion and Monte-Carlo sampling, which can result in degraded\nquality estimates as acquisition batch sizes increase. Houlsby et al. [2011], Gal et al.\n[2017] have not applied BALD to regression tasks.\n10. Black-Box Batch Active Learning for Regression\n176\nKernel-Based Methods. Holzmüller et al. [2022] examine the previously mentioned\nmethods and unify them using gradient-based kernels. Specifically, they express BALD\n[Houlsby et al., 2011], BatchBALD (§4), BAIT [Ash et al., 2021], BADGE [Ash et al.,\n2020], ACS-FW [Pinsler et al., 2019], and Core-Set [Sener and Savarese, 2018]/FF-\nActive [Geifman and El-Yaniv, 2017] using kernel-based methods for regression tasks.\nThey also propose a new method, LCMD (largest cluster maximum distance).\nComparison to This Chapter. This chapter extends the work of Houlsby et al.\n[2011] and Holzmüller et al. [2022] by combining the prediction-based approach with a\nkernel-based formulation. This trivially enables batch active learning on regression tasks\nusing black-box predictions for a wide range of existing batch active learning methods.\n10.2\nMethodology\nIn this chapter, we focus on regression, which is a common task in machine learning.\nWe assume that the target y is real-valued (∈R) with homoscedastic Gaussian noise:\nY | x,ωωω ∼N(µ(x;ωωω), σ2\nN).\n(10.1)\nEquivalently, Y | x,ωωω ∼µ(x;ωωω) + ε with ε ∼N(0, σ2\nN). As usual, we assume that the\nnoise is independent for different inputs x and parameters ωωω. Homoscedastic noise\nis a special case of the general heteroscedastic setting: the noise variance is simply a\nconstant. Our approach could easily be extended to heteroscedastic noise by substituting\na function σN(x;ωωω) for σN, but for this work we limit ourselves to the simplest case.\n10.2.1\nKernel-based Methods & Information Theory\nWe build on Holzmüller et al. [2022] which expresses contemporary batch active learning\nmethods using kernel methods. Specifically, active learning is performed using kernel-\nbased surrogates for deep neural network models. In this paper, we focus on surrogates\nusing empirical covariance kernels instead of gradient kernels. While a full treatment\nof Holzmüller et al. [2022] is beyond the scope of this chapter, we briefly review some\nkey ideas here. We refer the reader to the extensive paper for more details.\nGaussian Processes. Gaussian Processes are one way to introduce kernel-based\nmethods. A simple way to think about Gaussian Processes [Williams and Rasmussen,\n2006; Lázaro-Gredilla and Figueiras-Vidal, 2010; Rudner et al., 2022] is as a Bayesian\nlinear regression model with an implicit, potentially infinite-dimensional feature space\n(depending on the covariance kernel) that uses the kernel trick to abstract away the\nfeature map which maps the input space to the feature space.\nMultivariate Gaussian Distribution. The distinctive property of a Gaussian\nProcess is that all predictions are jointly Gaussian distributed. We can then write\nthe joint distribution for a univariate regression model as:\nY1, . . . , Yn | x1, . . . , xn ∼N(0, Cov[µ(x1), . . . , µ(xn)] + σ2\nNI),\n(10.2)\nwhere µ(x) are the observation-noise free predictions as random variables, and Cov[µ(x1),\n. . . , µ(xn)] is the covariance matrix of the predictions. The covariance matrix is defined\nvia the kernel function k(x, x′):\nCov[µ(x1), . . . , µ(xn)] =\n\u0014\nk(xi, xj)\n\u0015n,n\ni,j=1 .\n(10.3)\n10. Black-Box Batch Active Learning for Regression\n177\nThe kernel function k(x, x′) can be chosen almost arbitrarily as long as it is positive\nsemi-definite, e.g. see Williams and Rasmussen [2006, Ch.\n4].\nThe linear kernel\nk(x, x′) = x · x′ and the radial basis function kernel k(x, x′) = exp(−1\n2|x −x′|2) are\ncommon examples, as is the gradient kernel, which we examine next.\nFisher Information & Linearization. When using neural networks for regression,\nthe gradient kernel\nkgrad(x, x′ | ωωω∗) ≜∇ωωωµ(x;ωωω∗)∇2\nωωω[−log p(ωωω∗)]−1∇ωωωµ(x′;ωωω∗)⊤\n(10.4)\n= ⟨∇ωωωµ(x;ωωω∗), ∇ωωωµ(x′;ωωω∗)⟩∇2ωωω[−log p(ωωω∗)]−1\n(10.5)\nis the canonical choice, where ωωω∗is a maximum likelihood or maximum a posteriori\nestimate (MLE, MAP) and ∇2\nωωω[−log p(ωωω∗)] is the Hessian of the prior at ωωω∗. Note\nthat ∇ωωωµ(x;ωωω∗) is a row vector. Commonly, the prior is a Gaussian distribution with\nan identity covariance matrix, and thus ∇2\nωωω[−log p(ωωω∗)] = I.\nThe significance of this kernel lies in its relationship with the Fisher informa-\ntion matrix at ωωω∗[Immer, 2020; Immer et al., 2021], as we have seen in §9, or\nequivalently, with the linearization of the loss function around ωωω∗[Holzmüller et al.,\n2022]. This leads to a Gaussian approximation, which results in a Gaussian predictive\nposterior distribution when combined with a Gaussian likelihood. The use of the\nfinite-dimensional gradient kernel thus results in an implicit Bayesian linear regression\nin the context of regression models.\nPosterior Gradient Kernel. We can use the well-known properties of multivariate\nnormal distributions to marginalize or condition the joint distribution in (10.2).\nFollowing Holzmüller et al. [2022], this allows us to explicitly obtain the posterior\ngradient kernel given additional x1, . . . , xn as:\nkgrad→post(x1,...,xn)(x, x′ | ωωω∗)\n(10.6)\n≜∇ωωωµ(x;ωωω∗)\n\n\n\n\n\n\nσ−2\nN\n\n\n\n\n\n∇ωωωµ(x1;ωωω∗)\n...\n∇ωωωµ(xn;ωωω∗)\n\n\n\n\n\n\n\n\n\n\n∇ωωωµ(x1;ωωω∗)\n...\n∇ωωωµ(xn;ωωω∗)\n\n\n\n\n\n⊤\n+ ∇2\nωωω[−log p(ωωω∗)]\n\n\n\n\n\n\n−1\n∇ωωωµ(x′;ωωω∗)⊤.\nThe factor σ−2\nN\noriginates from implicitly conditioning on Yi | xi, which include\nobservation noise.\nImportantly for active learning, the multivariate normal distribution is the maximum\nentropy distribution for a given covariance matrix, and is thus an upper-bound for the\nentropy of any distribution with the same covariance matrix. The entropy is given\nby the log-determinant of the covariance matrix:\nH[Y1, . . . , Yn | x1, . . . , xn] = 1\n2 log det(Cov[µ(x1), . . . , µ(xn)] + σ2I) + Cn,\n(10.7)\nwhere Cn ≜n\n2 log(2 π e) is a constant that only depends on the number of samples n.\nConnecting kernel-based methods to information-theoretic quantities like the expected\ninformation gain, the respective acquisition scores are upper-bounds on the expected\ninformation gain, as we have examined in the previous chapter.\n10. Black-Box Batch Active Learning for Regression\n178\nExpected Information Gain for Regression. In §4, for acquisition of samples\nxacq\n1..K for classification tasks, the EIG was defined via BatchBALD as:\nI[Y acq\n1..K ;ΩΩΩ| xacq\n1..K, Dtrain] = H[Y acq\n1..K | xacq\n1..K, Dtrain] −H[Y acq\n1..K | xacq\n1..K,ΩΩΩ, Dtrain].\n(4.2)\nusing the mutual information between the parameters ωωω and the predictions {Yi} on\nan acquisition candidate batch {xi}. The conditional entropy term is precisely the\nentropy of a normal distribution given our model assumptions for the observation noise\nand can be computed exactly. In particular, for our regression task, as we assume fixed\naleatoric uncertainty, the conditional entropy is constant in x and can even be dropped\nfrom the objective. BALD and the EIG for models with fixed aleatoric are equivalent\nto entropy maximization, which is a common objective in BOED [Hernández-Lobato\net al., 2016]. To estimate BALD, we thus only need to compute the joint entropy,\nwhich can be upper-bounded by the corresponding entropy of the multivariate normal\ndistribution with the same covariance matrix, yielding an upper-bound overall.\n10.2.2\nBlack-Box Batch Active Learning\nWe more formally introduce the empirical covariance kernel and compare it to the\ngradient kernel commonly used for active learning with deep learning models in\nparameter-space. For non-differentiable models, we show how it can also be derived\nusing a Bayesian model perspective on the hypothesis space.\nIn addition to being illustrative, this section allows us to connect prediction-based\nkernels to the kernels used by Holzmüller et al. [2022], which in turns connects them\nto various SotA active learning methods.\n10.2.2.1\nPredictive Covariance Kernel\nTo perform black-box batch active learning, we directly use the predictive covariance\nof Yi|xi and Yj|xj:\nCovΩΩΩ[Yi; Yj | xi, xj] = CovΩΩΩ[µωωω\nxi; µωωω\nxj] + σ2\nN1{i = j},\n(10.8)\nwhere we have abbreviated µ(x;ωωω) with µωωω\nx, and used the law of total covariance and\nthe fact that the noise is uncorrelated between samples.\nWe define the predictive covariance kernel kpred(xi, xj) as the covariance of the\npredicted means:\nkpred(xi; xj) ≜CovΩΩΩ[µωωω\nxi; µωωω\nxj].\n(10.9)\nCompared to §10.2.1, we do not define the covariance via the kernel but the kernel\nvia the covariance.\nThis is also simply known as the covariance kernel in the literature [Shawe-Taylor\net al., 2004]. We use the prefix predictive to make clear that we look at the covariance\nof the predictions. The resulting Gram matrix is equal the covariance matrix of the\npredictions and positive definite (for positive σN and otherwise positive semi-definite),\nand thus the kernel is a valid kernel.\n10. Black-Box Batch Active Learning for Regression\n179\n10.2.2.2\nEmpirical Predictive Covariance Kernel\nFor K sampled model parameters ωωω1, . . . ,ωωωK ∼p(ωωω)—for example, the members\nof a deep ensemble—the empirical predictive covariance kernel kd\npred(xi; xj) is the\nempirical estimate:\nkd\npred(xi; xj) ≜d\nCovΩΩΩ[µωωω\nxi; µωωω\nxj] = 1\nK\nK\nX\nk=1\n \nµωωωk\nxi −1\nK\nK\nX\nl=1\nµωωωl\nxi\n!⊤ \nµωωωk\nxj −1\nK\nK\nX\nl=1\nµωωωl\nxj\n!\n(10.10)\n=\n* 1\n√\nK (¯µωωω1\nxi , . . . , ¯µωωωK\nxi ),\n1\n√\nK (¯µωωω1\nxj , . . . , ¯µωωωK\nxj )\n+\n, (10.11)\nwith centered predictions ¯µωωωk\nx ≜µωωωk\nx −1\nK\nPK\nl=1 µωωωl\nx . As we can write this kernel as an\ninner product, it also immediately follows that the empirical predictive covariance\nkernel is a valid kernel and positive semi-definite.\n10.2.2.3\nDifferentiable Models\nSimilar to Holzmüller et al. [2022, §C.1], we show that the posterior gradient kernel is\na first-order approximation of the (predictive) covariance kernel. This section explicitly\nconditions on Dtrain. The result is simple but instructive:\nProposition 10.1. The posterior gradient kernel kgrad→post(Dtrain)(xi; xj | ωωω∗) is an\napproximation of the predictive covariance kernel kpred(xi; xj).\nProof. We use a first-order Taylor expansion of the mean function µ(x;ωωω) around ωωω∗:\nµ(x;ωωω) ≈µ(x;ωωω∗) + ∇ωωωµ(x;ωωω∗) (ωωω −ωωω∗)\n|\n{z\n}\n≜∆ωωω\n.\n(10.12)\nChoose ωωω∗= Eωωω∼p(ωωω|Dtrain)[ωωω] (BMA). Then we have Ep(w|Dtrain)[µ(x;ωωω)] = µ(x;ωωω∗). We\nthen have:\nkpred(xi; xj) = Covωωω∼p(ωωω|Dtrain)[µ(xi;ωωω); µ(xj;ωωω)]\n(10.13)\n≈Eωωω∗+∆ωωω∼p(w|Dtrain)[⟨∇ωωωµωωω∗\nxi ∆ωωω, ∇ωωωµωωω∗\nxj ∆ωωω⟩]\n(10.14)\n= ∇ωωωµωωω∗\nxi Eωωω∗+∆ωωω∼p(w|Dtrain)[∆ωωω∆ωωω⊤] ∇ωωωµωωω∗\nxj\n⊤\n(10.15)\n= ∇ωωωµ(xi;ωωω∗) Cov[ΩΩΩ| Dtrain] ∇ωωωµ(xj;ωωω∗)⊤\n(10.16)\n≈kgrad→post(Dtrain)(xi; xj | ωωω∗).\n(10.17)\nThe intermediate expectation is the model covariance Cov[ΩΩΩ| Dtrain] as ωωω∗is the BMA.\nFor the last step, we use the Gauss-Newton approximation again [Immer et al., 2021]\nand approximate the inverse of the covariance using the Hessian of the negative log\nlikelihood at ωωω∗:\nCov[ΩΩΩ| Dtrain]−1 ≈∇2\nωωω[−log p(ωωω∗| Dtrain)]\n(10.18)\n= ∇2\nωωω[−log p(Dtrain | ωωω∗) −log p(ωωω∗)]\n(10.19)\n= σ−2 P\ni ∇ωωωµ(xtrain\ni\n;ωωω∗)⊤∇ωωωµ(xtrain\ni\n;ωωω∗)\n(10.20)\n−∇2\nωωω log p(ωωω∗),\n(10.21)\n10. Black-Box Batch Active Learning for Regression\n180\nwhere we have first used Bayes’ theorem and that p(Dtrain) vanishes under differentiation—\nit is constant in ωωω. Secondly, the Hessian of the negative log likelihood is just the outer\nproduct of the gradients divided by the noise variance in the homoscedastic regression\ncase. ∇2\nωωω[−log p(ωωω∗)] is the prior term. This matches (10.6).\n10.2.2.4\nNon-Differentiable Models\nHow can we apply the above result to non-differentiable models? In the following, we\nuse a Bayesian view on the hypothesis space to show that we can connect the empirical\npredictive covariance kernel to a gradient kernel here, too. With ˆΩΩΩ≜(ωωω1, . . . ,ωωωK)\nfixed—e.g. these could be the different parameters of the members of a deep ensemble—\nwe introduce a latent Ψ to represent the ‘true’ hypothesis ωωωψ ∈ˆΩΩΩfrom this empirical\nhypothesis space ˆΩΩΩ, which we want to identify. This is similar to QbC [Seung et al.,\n1992]. In essence, the latent Ψ takes on the role of ΩΩΩfrom the previous section, and\nwe are interested in learning the ‘true’ Ψ from additional data. We, thus, examine\nthe kernels for Ψ, as opposed to ΩΩΩ.\nSpecifically, we model Ψ using a one-hot categorical distribution, that is a multi-\nnomial distribution from which we draw one sample: Ψ ∼Multinomial(qqq, 1), with\nqqq ∈SK−1 parameterizing the distribution, where SK−1 denotes the K −1 simplex in\nRK. Then, qqqk = p(Ψ = ek), where ek denotes the k-th unit vector; and\nPK\nk=1 qqqk = 1.\nFor the predictive ˜µ(x; Ψ), we have:\n˜µ(x; Ψ) ≜µ(x;ωωωΨ) = ⟨µ(x; ·), Ψ⟩,\n(10.22)\nwhere we use ωωωψ to denote the ωωωk when we have ψ = ek in slight abuse of notation,\nand µ(x; ·) ∈RK is a column vector of the predictions µ(x;ωωωk) for x for all ωωωk. This\nfollows from ψ being a one-hot vector.\nWe now examine this model and its kernels. The BMA of ˜µ(x; Ψ) matches the\nprevious empirical mean, and, if we choose qqq to have an uninformative2 uniform\ndistribution over the hypotheses (qqqk ≜\n1\nK), we obtain:\n˜µ(x;qqq) ≜Ep(ψ)[µ(x;ωωωψ)] = ⟨µ(x; ·),qqq⟩=\nK\nX\nψ=1\nqqqψµ(x;ωωωψ) =\nK\nX\nψ=1\n1\nK µ(x;ωωωψ).\n(10.23)\nWhat is the predictive covariance kernel of this model? And what is the posterior\ngradient kernel for qqq?\nProposition 10.2.\n1. The predictive covariance kernel kpred,ψ(xi, xj) for ˆΩΩΩusing uniform qqq is equal to\nthe empirical predictive covariance kernel kd\npred(xi; xj).\n2. The ‘posterior’ gradient kernel kgrad,ψ→post(Dtrain)(xi; xj) for ˆΩΩΩin respect to Ψ\nusing uniform qqq is equal to the empirical predictive covariance kernel kd\npred(xi; xj).\nProof. Like for the previous differentiable model, the BMA of the model parameters Ψ\nis just qqq: E[Ψ] = qqq. The first statement immediately follows:\nkpred,ψ(xi, xj) = Covψ[˜µ(xi; ψ); ˜µ(xj; ψ)] = Ep(ψ)[¯µωωωψ\nxi ¯µωωωψ\nxj ]\n(10.24)\n2If we had additional information about the ˆΩΩΩ—for example, if we had validation losses—we could\nuse that to inform qqq.\n10. Black-Box Batch Active Learning for Regression\n181\n= 1\nK\nX\nψ\n¯µωωωψ\nxi ¯µωωωψ\nxj = kd\npred(xi; xj).\n(10.25)\nFor the second statement, we will show that we can express the predictive covariance\nkernel as a linearization around Ψ. We can read off a linearization for ∇ψ˜µ(xi; ψ) from\nthe inner product in Equation 10.23:\n∇ψ˜µ(xi; ψ) = µ(x; ·)⊤,\n(10.26)\nThis allows us to write the predictive covariance kernel as a linearization around qqq:\nkpred,ψ(xi, xj) = Covψ∼p(ψ)[˜µ(xi; ψ); ˜µ(xj; ψ)]\n(10.27)\n= Eqqq+∆ψ∼p(ψ)[∇ψ˜µ(xi; ψ)∆ψ, ∇ψ˜µ(xj; ψ)∆ψ]\n(10.28)\n= ∇ψ˜µ(xi;qqq) Cov[Ψ] ∇ψ˜µ(xi;qqq)⊤\n(10.29)\n= kgrad,ψ→post(Dtrain)(xi; xj).\n(10.30)\nThe above gradient kernel is only the posterior gradient kernel in the sense that\nwe have sampled ωωωψ from the non-differentiable model after inference on training\ndata. The samples themselves are drawn uniformly.\nThe covariance of the multinomial Ψ is: Cov[Ψ] = diag(qqq)−qqqqqq⊤. Thus, substituting,\nwe can verify that the posterior gradient kernel is indeed equal to the predictive\ncovariance kernel:\nkgrad,ψ→post(Dtrain)(xi; xj) = ∇ψ˜µ(xi;qqq) (diag(qqq) −qqqqqq⊤) ∇ψ˜µ(xi;qqq)⊤\n(10.31)\n= µ(xi; ·)⊤diag(qqq) µ(xj; ·) −(µ(xi; ·)⊤qqq) (qqq⊤µ(xj; ·))\n(10.32)\n= 1\nK\nX\nψ\nµ(xi;ωωωψ) µ(xj;ωωωψ)⊤\n(10.33)\n−\n\n1\nK\nX\nψ\nµ(xi;ωωωψ)\n\n\n\n1\nK\nX\nψ\nµ(xj;ωωωψ)\n\n\n= kd\npred(xi; xj).\n(10.34)\nThis demonstrates that a straightforward Bayesian model can be constructed on top of\na non-differentiable ensemble model. Bayesian inference in this context aims to identify\nthe most suitable member of the ensemble. Given the limited number of samples and\nlikelihood of model misspecification, it is likely that none of the members accurately\nrepresents the true model. However, for active learning purposes, the main focus is\nsolely on quantifying the degree of disagreement among the ensemble members.\nA similar Bayesian model using Bayesian Model Combination (BMC) could be set up\nwhich allows for arbitrary convex mixtures of the ensemble members. This would entail\nusing a Dirichlet distribution Ψ ∼Dirichlet(ααα) instead of the multinomial distribution.\nAssuming an uninformative prior (αααk ≜1/K), this leads to the same results up to a\nconstant factor of 1+P\nk αααk = 2. This is pleasing because it does not matter whether we\nuse a multinomial or Dirichlet distribution, that is: whether we take a hypothesis space\nview with a ‘true’ hypothesis or accept that our model is likely misspecified, and we\nare dealing with a mixture of models, the results are the same up to a constant factor.\n10. Black-Box Batch Active Learning for Regression\n182\n256\n512\n1024\n2048\n4096\nTraining set size Ntrain\n−1.8\n−1.6\n−1.4\n−1.2\n−1.0\n−0.8\nmean log RMSE\nAcq. Function\nUniform\nBALD\nACS-FW\nBatchBALD\nBAIT\nCore-Set\nBADGE\nLCMD\nMethod\n■\n□\n64\n128\n256\n512\n1024\n2048\n4096\nAcquisition batch size Nbatch\n−1.8\n−1.7\n−1.6\n−1.5\n−1.4\n−1.3\n−1.2\nmean log RMSE\nAcq. Function\nUniform\nBALD\nACS-FW\nBatchBALD\nBAIT\nCore-Set\nBADGE\nLCMD\nMethod\n■\n□\n(a) Deep Neural Networks.\n256\n512\n1024\n2048\n4096\nTraining set size Ntrain\n−1.4\n−1.2\n−1.0\n−0.8\n−0.6\nmean log RMSE\nAcq. Function\nUniform\n■BALD\n■BatchBALD\n■BAIT\n■ACS-FW\n■Core-Set\n■BADGE\n■LCMD\n64\n128\n256\n512\n1024\n2048\n4096\nAcquisition batch size Nbatch\n−1.3\n−1.2\n−1.1\n−1.0\n−0.9\n−0.8\n−0.7\nmean log RMSE\nAcq. Function\nUniform\n■BALD\n■BatchBALD\n■BAIT\n■ACS-FW\n■Core-Set\n■BADGE\n■LCMD\n(b) Random Forests.\nFigure 10.1: Mean logarithmic RMSE over 15 regression datasets. ((a)) For DNNs, we see\nthat black-box ■methods work as well as white-box □methods, and in most cases better,\nexcept for ACS-FW and BAIT. ((b)) For random forests (100 estimators) with the default\nhyperparameters from scikit-learn [Pedregosa et al., 2011a], we see that black-box methods\nperform better than the uniform baseline, except for BALD, which uses top-K acquisition. In\nthe appendix, see Table I.1 for average performance metrics and Figure I.2 and I.3 for plots\nwith additional error metrics. Averaged over 20 trials.\nApplication to DNNs, BNNs, and Other Models. The proposed approach has\nrelevance due to its versatility, as it can be applied to a wide range of models that can be\nconsistently queried for prediction, including deep ensembles [Lakshminarayanan et al.,\n2017], Bayesian neural networks (BNNs) [Blundell et al., 2015; Gal and Ghahramani,\n2016a], and non-differentiable models. The kernel used in this approach is simple to\nimplement and scales in the number of empirical predictions per sample, rather than\nin the parameter space, as seen in other methods such as Ash et al. [2021].\n10. Black-Box Batch Active Learning for Regression\n183\n256\n512\n1024\n2048\n4096\nTraining set size Ntrain\n−1.3\n−1.2\n−1.1\n−1.0\n−0.9\n−0.8\n−0.7\n−0.6\n−0.5\nmean log RMSE\nAcq. Function\n■BALD\n■BatchBALD\n■BAIT\n■ACS-FW\n■Core-Set\n■BADGE\n■LCMD\n64\n128\n256\n512\n1024\n2048\n4096\nAcquisition batch size Nbatch\n−1.3\n−1.2\n−1.1\n−1.0\n−0.9\n−0.8\n−0.7\nmean log RMSE\nAcq. Function\n■BALD\n■BatchBALD\n■BAIT\n■ACS-FW\n■Core-Set\n■BADGE\n■LCMD\n(a) Random Forests (Bagging).\n256\n512\n1024\n2048\n4096\nTraining set size Ntrain\n−1.4\n−1.2\n−1.0\n−0.8\n−0.6\nmean log RMSE\nAcq. Function\nUniform\n■BALD\n■BatchBALD\n■BAIT\n■ACS-FW\n■Core-Set\n■BADGE\n■LCMD\n64\n128\n256\n512\n1024\n2048\n4096\nAcquisition batch size Nbatch\n−1.45\n−1.40\n−1.35\n−1.30\n−1.25\n−1.20\n−1.15\n−1.10\n−1.05\nmean log RMSE\nAcq. Function\nUniform\n■BALD\n■BatchBALD\n■BAIT\n■ACS-FW\n■Core-Set\n■BADGE\n■LCMD\n(b) Gradient-Boosted Trees.\nFigure 10.2: Mean logarithmic RMSE over 15 regression datasets (cont’d). For random\nforests using bagging ((a)) with 10 bootstrapped training sets, and for gradient-boosted\ntrees [Dorogush et al., 2018] ((b)) with a virtual ensemble of 20 members, we see that only a\nfew of the black-box methods perform better than the uniform baseline: LCMD, BADGE\nand CoreSet. We hypothesize that the virtual ensembles and a bagged ensemble of random\nforests do not express as much predictive disagreement which leads to worse performance for\nactive learning. In the appendix, see Table I.1 for average performance metrics and Figure I.4\nand I.5 for plots with additional error metrics. Averaged over 20 trials.\n10.3\nEmpirical Validation\nWe follow the evaluation from Holzmüller et al. [2022] and use their framework\nto ease comparison. This allows us to directly compare to several SotA methods\nin a regression setting, respectively their kernel-based analogues. Specifically, we\ncompare to the following popular deep active learning methods: BALD [Houlsby et al.,\n2011], BatchBALD (§4), BAIT [Ash et al., 2021], BADGE [Ash et al., 2020], ACS-\nFW [Pinsler et al., 2019], Core-Set [Sener and Savarese, 2018]/FF-Active [Geifman\n10. Black-Box Batch Active Learning for Regression\n184\n−0.5 0.0\n0.5\nBALD\n−0.5\n0.0\n0.5 ■\n□\n−0.5 0.0\n0.5\nACS-FW\n■\n□\n−0.5 0.0\n0.5\nBatchBALD\n■\n□\n−0.5 0.0\n0.5\nBAIT\n■\n□\n−0.5 0.0\n0.5\nCore-Set\n■\n□\n−0.5 0.0\n0.5\nBADGE\n■\n□\n−0.5 0.0\n0.5\nLCMD\n■\n□\n□vs Uniform (↑)\n■vs Uniform (↑)\nDatasets\nct slices\ndiamonds\nfried\nkegg undir\nmethane\nmlr knn rng\nonline video\npoker\nprotein\nquery\nroad\nsarcos\nsgemm\nstock\nwec sydney\nFigure 10.3: Average Logarithmic RMSE by regression datasets for DNNs: ■vs □(vs\nUniform). Across acquisition functions, the performance of black-box methods is highly\ncorrelated with the performance of white-box methods, even though black-box methods make\nfewer assumptions about the model. We plot the improvement of the white-box □method\nover the uniform baseline on the x-axis (so for datasets with markers right of the dashed\nvertical lines, the white-box method performs better than uniform) and the improvement of\nthe black-box ■method over the uniform baseline on the y-axis (so for datasets with markers\nabove the dashed horizontal lines, the black-box method performs better than uniform). For\ndatasets with markers in the ■diagonal half, the black-box method performs better than the\nwhite-box method. The average over all datasets is marked with a star ⋆. Surprisingly, on\naverage over all acquisition rounds, the black-box methods perform slightly better than the\nwhite-box methods for all but ACS-FW and BAIT. In the appendix, see Figure I.1 for the\nfinal acquisition round and Table I.3 for details on the datasets. Averaged over 20 trials.\nand El-Yaniv, 2017], and LCMD [Holzmüller et al., 2022].\nWe also compare to\nthe random selection baseline (‘Uniform’). We use 15 large tabular datasets from\nthe UCI Machine Learning Repository [Dua and Graff, 2017] and the OpenML\nbenchmark suite [Vanschoren et al., 2013] for our experiments: sgemm (SGEMM\nGPU kernel performance); wec_sydney (Wave Energy Converters); ct_slices (Relative\nlocation of CT slices on axial axis); kegg_undir (KEGG Metabolic Reaction Network\n- Undirected); online_video (Online Video Characteristics and Transcoding Time);\nquery (Query Analytics Workloads); poker (Poker Hand); road (3D Road Network -\nNorth Jutland, Denmark); mlr_knn_rng; fried; diamonds; methane; stock (BNG\nstock); protein (physicochemical-protein); sarcos (SARCOS data). See Table I.3 in\nthe appendix for more details.\nExperimental Setup. We use the same experimental setup and hyperparameters\nas Holzmüller et al. [2022]. We report the logarithmic RMSE averaged over 20 trials\nfor each dataset and method. For ensembles, we compute the performance for each\nensemble member separately, enabling a fair comparison to the non-ensemble methods.\nPerformance differences can thus be attributed to the acquisition function, rather than\nthe ensemble. We used A100 GPUs with 40 GB of GPU memory.\nEnsemble Size. For deep learning, we use a small ensemble of 10 models, which is\nsufficient to achieve good performance. This ensemble size can be considered ‘small’\nin the regression setting [Lázaro-Gredilla and Figueiras-Vidal, 2010; Zhang and Zhou,\n2013], whereas in Bayesian Optimal Experiment Design much higher sample counts are\nregularly used [Foster et al., 2021]. In many cases, training an ensemble of regression\n10. Black-Box Batch Active Learning for Regression\n185\nmodels is still fast and could be considered cheap compared to the cost of acquiring\nadditional labels. For non-differentiable models, we experiment with random forests\n[Breiman, 2001], using the different trees as ensemble members, and a virtual ensemble\nof gradient-boosted decision trees [Prokhorenkova et al., 2017]. For random forests, we\nuse the implementation provided in scikit-learn [Pedregosa et al., 2011a] with default\nhyperparameters, that is using 100 trees per forest. We use the predictions from each\ntree as a virtual ensemble member. We do not perform any hyperparameter tuning. We\nalso report results for random forests with bagging, where we train a real ensemble of\n10 random forests. For gradient-boosted decision trees, we use a virtual ensemble of up\nto 20 members with early stopping using a validation set3. We use the implementation\nin CatBoost [Dorogush et al., 2018]. We do not perform any hyperparameter tuning.\nBlack-Box vs White-Box Deep Active Learning. In Figure 10.1(a) and 10.3,\nwe see that B3AL is competitive with white-box active learning, when using BALD,\nBatchBALD, BAIT, BADGE, and Core-Set. On average, B3AL outperforms the\nwhite-box methods on the 15 datasets we analyzed (excluding ACS-FW and BAIT).\nWe hypothesize that this is due to the implicit Fisher information approximation in\nthe white-box methods we have examined (§9), which is not as accurate in the low\ndata regime as the more explicit approximation in B3AL via ensembling. On the\nother hand, it seems that the black-box methods suffer from the low feature space\ndimensionality, as they are much closer to BALD.\nWhy can Black-Box Methods Outperform White-Box Methods? Following\n§10.2, white-box and black-box methods are based on kernels, which can be seen as\ndifferent approximations of the predictive covariance kernel.\nWhite-box methods\nimplicitly assume that the predictive covariance kernel is well approximated by\nthe Fisher information kernel and the gradient kernel (§9). However, Long [2022]\ndemonstrated that this assumption does not always hold, particularly in low data\nregimes, where a Gaussian might not approximate the parameter distribution well.\nInstead, Long [2022] suggests using a multimodal distribution. In these situations,\nmethods that employ ensembling, such as B3AL, to approximate the predictive\ncovariance kernel are more robust. The different ensemble members can reside in\ndifferent modes of the parameter distribution, allowing black-box methods to outperform\ntheir white-box counterparts.\nNon-Differentiable Models. In Figure 10.1(b), 10.2(a), and 10.2(b), we observe\nthat B3AL is effective for non-differentiable models, including random forests and\ngradient-boosted decision trees. BALD for non-differentiable models can be considered\nequivalent to QbC [Seung et al., 1992], while BatchBALD for non-differentiable models\ncan be viewed as equivalent to QbC with batch acquisition [Nguyen et al., 2012]. For\nrandom forests, all methods except BALD (using top-k selection) outperform uniform\nacquisition. However, for random forests with bagging and gradient-boosted decision\ntrees, B3AL surpasses random acquisition only when employing LMCD and BADGE.\nThis may be attributed to the reduced disagreement within a virtual ensemble for\ngradient-boosted decision trees and between distinct random forests. In particular,\nrandom forests with bagging appear to support this explanation, as a single random\nforest seems to exhibit more disagreement among its individual trees than an ensemble\n3If the virtual ensemble creation fails because there are no sufficiently many trees due to early\nstopping, we halve the ensemble size and retry. This was only needed for the poker dataset.\n10. Black-Box Batch Active Learning for Regression\n186\nof random forests with bagging does between different forests. This is evident in the\nsuperior overall active learning performance of the single random forest compared to\nthe ensemble of random forests with bagging, c.f. Figure 10.1(b) and 10.2(a).\n10.4\nDiscussion\nIn this chapter, we have demonstrated the effectiveness of a simple extension to\nkernel-based methods that utilizes empirical predictions rather than gradient kernels.\nThis modification enables black-box batch active learning with good performance.\nImportantly, B3AL also generalizes to non-differentiable models, an area that has\nreceived limited attention as of late.\nThis result also partially answer one of the research questions from the previous\nchapter (§9): how do prediction-based methods compare to parameter-based ones?\nWe find that for regression the prediction-based methods are competitive with the\nparameter-based methods in batch active learning.\nThe main limitation of our proposed approach lies in the acquisition of a sufficient\namount of empirical predictions. This could be a challenge, particularly when using deep\nensembles with larger models or non-differentiable models that cannot be parallelized\nefficiently. Our experiments using virtual ensembles indicate that the diversity of the\nensemble members plays a crucial role in determining the performance.\nLikewise, the main limitation of this chapter and the empirical comparisons is\nthat we only consider regression tasks. Extending the results to classification is an\nimportant direction for future work.\nWhat gets sharper the more you use it?\n11\nConclusion\nIn this thesis, we set out to address several research questions focused on active learning\nand data subset selection in deep learning, with an emphasis on information-theory intu-\nitions:\n1. How can uncertainty quantification, specifically aleatoric and epistemic uncer-\ntainty, be better understood and applied in the context of active learning and\nactive sampling?\n2. How can a deeper understanding of the theoretical foundations of active learning\nand active sampling contribute to the progress of the field and improve the\npractical application of these techniques?\n3. How can the cost of gathering and labeling data be reduced, and how can training\nbe sped up in a principled fashion?\n4. What are the connections between different active learning and active sampling\napproaches, and how can information theory be used to unify these approaches?\nOur main findings and contributions include the unifying perspective provided by\ninformation-theory intuitions, the discovery that many existing methods can be\nexplained using the same framework, and the effectiveness of simple methods in\nvarious applications.\nRegarding the first research question, we have demonstrated that by examining\naleatoric and epistemic uncertainty in more detail, we can propose new baselines for\nuncertainty quantification using single forward-pass deep neural networks (§3). This\napproach allows us to quantify epistemic uncertainty well and achieve competitive\nresults in active learning without having to be Bayesian.\nIn addressing the second research question, this thesis contributes to a deeper\nunderstanding of the theoretical foundations of active learning and active sampling\nby providing a unified framework based on information theory (§2 and §9). This\nframework helps researchers better understand the trade-offs and connections between\ndifferent approaches, ultimately improving the practical application of these techniques.\nTo answer the third research question, we have explored various methods for\nreducing the cost of gathering and labeling data (§3 to 5, §7 and §10) and speeding\nup training in a principled fashion (§8 and Appendix B.1). While we have provided\na principled approach for batch acquisition in §4, it is too slow at larger acquisition\nbatch sizes and can suffer from issues related to the estimation of joint predictives\nfor larger batch acquisition sizes. On the other hand, our conceptually simpler and\neffective methods, such as stochastic batch acquisition (§5) and single forward-pass\ndeterministic methods (§3), have shown promising results and have the potential to\nbenefit practitioners in a wide range of applications.\n11. Conclusion\n188\nFinally, in addressing the fourth research question, we have identified connections\nbetween different active learning and active sampling approaches by using informa-\ntion theory to unify these methods (§9 and §10). This unifying perspective allows\nus to better understand the relationships between various techniques and access\nthe principled framework for reasoning about uncertainty and informativeness that\ninformation theory provides.\nNonetheless, there are some obvious limitations to our research. One of the main\nchallenges is improving joint predictions for multiple samples (§6). Additionally, a\nmore satisfying approach for classification tasks and beyond is needed when using\nkernel-based methods. Finally, there are various related areas such as active testing,\nactive inference, and exploration in reinforcement learning that are yet to be connected\nin more detail to active learning and active sampling.\nFuture research directions could include developing more effective methods for joint\npredictions and batch selection, exploring other alternative approaches for classification,\nand investigating the quality of the chosen approximations. We detail several other\nspecific research questions in various chapters: in particular in §6 and §9.\nIn conclusion, this thesis has advanced our understanding of active learning and\ndata subset selection in deep learning by providing a unifying perspective based\non information-theory intuitions. We hope this thesis has also contributed to the\nreader’s understanding and that our contributions will inspire further research and\nhelp practitioners adopt effective techniques in their work.\nAppendices\nCan you imagine what I would do if I could do all\nI can?\nSun Tzu, The Art of War\nA\nCausal-BALD: Deep Bayesian Active\nLearning of Outcomes to Infer\nTreatment-Effects from Observational Data\nHow will a patient’s health be affected by taking a medication [Perez, 2019]? How will\na user’s question be answered by a search recommendation [Noble, 2018]? Insight into\nthese questions can be gained by learning about personalized treatment effects. Estimat-\ning personalized treatment effects from observational data is essential in situations where\nexperimental designs are infeasible, unethical or expensive. Observational data represent\na population of individuals described by a set of pre-treatment covariates (age, blood\npressure, socioeconomic status), an assigned treatment (medication, no medication),\nand a post-treatment outcome (severity of migraines). An ideal personalized treatment\neffect is the difference between the post-treatment outcome had the individual been\ntreated, and the outcome had they not been treated. But, it is impossible to observe\nboth outcomes for an individual, so the difference must instead be computed between\npopulations. Therefore, in the common setting of binary treatments, data is partitioned\ninto the treatment group (individuals that received the treatment) and the control\ngroup (individuals who did not). The personalized treatment effect is then given\nby the expected difference in outcomes between treated and controlled individuals\nwho share the same (or similar) measured covariates (difference between solid lines\nin the middle pane of Figure A.1).\nIncreasingly, pre-treatment covariates are being assembled from high-dimensional,\nheterogeneous measurements such as medical images and electronic health records\n[Sudlow et al., 2015]. Deep learning methods have been shown capable of learning\npersonalized treatment effects from such data [Shalit et al., 2017; Shi et al., 2019;\nJesson et al., 2021]. However, a key problem in deep learning is data efficiency. While\nmodern methods are capable of impressive performance, they need a significant amount\nof labeled data. Acquiring labeled data can be expensive, often requiring specialist\nknowledge or an invasive procedure to determine the outcome. Therefore, it is desirable\nto minimize the amount of labeled data needed to obtain a well-performing model.\nActive learning provides a principled framework to address this concern [Cohn et al.,\n1996]. In active learning for treatment effects [Deng et al., 2011; Sundin et al., 2019], a\nmodel is trained on available labeled data consisting of covariates, assigned treatments,\nand acquired outcomes.\nThe model predictions are then used to select the most\ninformative examples from a set of data consisting of only covariates and treatment\nindicators. Outcomes are then acquired, e.g. by performing a biopsy, for the selected\nA. Causal-BALD: Deep Bayesian Active Learning of Outcomes to Infer\nTreatment-Effects from Observational Data\n191\npatients and the model is retrained and evaluated. This process is repeated until either\na satisfactory performance level is achieved, or the labeling budget is exhausted.\nFigure A.1: Observational data. Top: data density of treatment (right) and control (left)\ngroups. Middle: observed outcome response for treatment (circles) and control (x’s) groups.\nBottom: data density for active learned training set after a number of acquisition steps.\nAt first sight this might seem simple; however, active learning induces bias resulting\nin divergence between the distribution of the acquired training data and the distribution\nof the pool set data [Farquhar et al., 2021]. In the context of learning causal effects, such\nbias has important positive and negative consequences. For example, while random\nacquisition active learning results in an unbiased sample of the training data, it can\nlead to over-allocation of resources to the mode of the data at the expense of learning\nabout underrepresented data. Conversely, while biasing acquisitions toward lower\ndensity regions of the pool data can be desirable, it can also lead to acquisitions for\nwhich we cannot know the treatment effect, which could in turn lead to uninformed,\npotentially harmful, personalized decisions.\nTo gain insight into how biasing the acquisition of training data can be beneficial for\nlearning treatment effects, consider a key difference between experimental and observa-\ntional data: the treatment assignment mechanism is not available for observational data.\nThis means that there may be unobserved variables that affect treatment assignment\n(an untestable condition), but also that the relative proportion of individuals treated to\nthose controlled can vary across different subpopulations of the data. The later point\nis illustrated in Figure A.1, where there are relatively equal proportions of treated and\ncontrolled examples for data in region 3, but the proportions become less balanced as\nwe move to either the left or the right. In extreme cases, say if a group described by\nsome covariate values were systematically excluded from treatment, the treatment effect\nfor that group cannot be known [Petersen et al., 2012]. This is illustrated in Figure A.1\nby region 1, where there are only controlled examples, and by region 5, where there are\nonly treated examples. In the language of causal inference, the necessity of seeing both\ntreated and untreated examples for each subpopulation corresponds to satisfaction of\nthe overlap (or positivity) assumption (see A.3). Regions 2 and 4 of Figure A.1 are\ninteresting as while either the treated or control group are underrepresented, there\nmay still be sufficient coverage to learn treatment effects.\nA. Causal-BALD: Deep Bayesian Active Learning of Outcomes to Infer\nTreatment-Effects from Observational Data\n192\nWe propose that the acquisition of unlabeled data should focus on exploring all\nregions with sufficient overlap, but not areas with no overlap. The bottom pane of\nFigure A.1 imagines what a resulting training set distribution could look like at an\nintermediate active learning step. It is not trivial to design such acquisition functions:\nnaively applying active learning acquisition functions results in suboptimal and sample\ninefficient acquisitions of training examples, as we show below. To this end, we develop\nan epistemic uncertainty aware method for active learning of personalized treatment\neffects from high dimensional observational data. We demonstrate the performance of\nthe proposed acquisition strategies on a synthetic and semisynthetic datasets.\nA.1\nBackground\nA.1.1\nEstimation of Personalized Treatment-Effects\nPersonalized treatment-effect estimation seeks to know the effect of a treatment T ∈T\non the outcome Y ∈Y for individuals described by covariates X ∈X. In this chapter,\nwe consider the random variable (r.v.) T to be binary (T = {0, 1}), the r.v. Y\nto be part of a bounded set Y, and X to be a multi-variate r.v. of dimension d\n(X = Rd). Under the Neyman-Rubin causal model [Neyman, 1923; Rubin, 1974], the\nindividual treatment effect (ITE) for a person u is defined as the difference in potential\noutcomes Y 1(u) −Y 0(u), where the r.v. Y 1 represents the potential outcome were\nthey treated, and the r.v. Y 0 represents the potential outcome were they controlled\n(not treated). Realizations of the random variables X, T, Y , Y 0, and Y 1 are denoted\nby x, t, y, y0, and y1, respectively.\nThe ITE is a fundamentally unidentifiable quantity, so instead we look at the\nexpected difference in potential outcomes for individuals described by X, or the\nConditional Average Treatment Effect (CATE): τ(x) ≡E[Y 1 −Y 0 | X = x] Abrevaya\net al. [2015]. The CATE is identifiable from an observational dataset D = {(xi, ti, yi)}n\ni=1\nof samples (xi, ti, yi) from the joint empirical distribution PD(X, T, Y 0, Y 1), under\nthe following three assumptions:\nAssumption A.1. (Consistency) y = tyt + (1 −t)y1−t, i.e. an individual’s observed\noutcome y given assigned treatment t is identical to their potential outcome yt.\nAssumption A.2. (Unconfoundedness) (Y 0, Y 1) ⊥⊥T | X.\nAssumption A.3. (Overlap) 0 < πt(x) < 1 : ∀t ∈T ,\nwhere πt(x) ≡P(T = t | X = x) is the propensity for treatment for individuals\ndescribed by covariates X = x Rubin [1974]. When these assumptions are satisfied,\nbτ(x) ≡E[Y | T = 1, X = x] −E[Y | T = 0, X = x] is an identifiable, unbiased\nestimator of τ(x).\nA variety of parametric [Robins et al., 2000; Tian et al., 2014; Shalit et al., 2017]\nand non-parametric estimators [Hill, 2011; Xie et al., 2012; ala, 2017; Gao and Han,\n2020] have been proposed for CATE. Here, we focus on parametric estimators for\ncompactness. Parametric CATE estimators assume that outcomes y are generated\naccording to a likelihood pω(y | x, t), given measured covariates x, observed treatment\nt, and model parameters ω. For continuous outcomes, a Gaussian likelihood can be\nused: N(y | bµω(x, t), bσω(x, t)). For discrete outcomes, a Bernoulli likelihood can be\nA. Causal-BALD: Deep Bayesian Active Learning of Outcomes to Infer\nTreatment-Effects from Observational Data\n193\nused: Bern(y | bµω(x, t)). In both cases, bµω(x, t) is a parametric estimator of E[Y | T =\nt, X = x], which leads to: bτω(x) ≡bµω(x, 1) −bµω(x, 0), a parametric CATE estimator.\nBayesian inference over the model parameters ω treated as stochastic instances of\nthe random variable Ω∈W has been shown by Jesson et al. [2020] to yield models\ncapable of quantifying when assumption A.3 (overlap) does not hold, or when there is\ninsufficient knowledge about the treatment effect τ(x) because the observed value x\nlies far from the support of PD(X, T, Y 0, Y 1). Such methods seek to enable sampling\nfrom the posterior distribution of the model parameters given the data, p(Ω| D). Each\nsample, ω ∼p(Ω| D) induces a unique CATE function bτω(x). Epistemic uncertainty\nis a measure of “disagreement” between the functions at a given value x [Kendall\nand Gal, 2017]. Jesson et al. [2020] propose Varω∼p(Ω|D)(bµω(x, 1) −bµω(x, 0)) as a\nmeasure of epistemic uncertainty in the CATE.\nA.1.2\nActive Learning\nWe use a slightly different setup here than in the other chapters. Formally, an active\nlearning setup consists of an unlabeled dataset Dpool = {xi}\nnpool\ni=1 , a labeled training set\nDtrain = {xi, yi}ntrain\ni=1 , and a predictive model with likelihood pω(y | x) parameterized by\nω ∼p(Ω| Dtrain). It is further assumed that an oracle exists to provide outcomes y for\nany data point in Dpool. After model training, a batch of data {x∗\ni }b\ni=1 is selected from\nDpool using an acquisition function a according to the informativeness of the batch.\nWe depart from the standard active learning setting and include the treatment:\nfor active learning of treatment effects, we define Dpool = {xi, ti}\nnpool\ni=1 , a labeled\ntraining set Dtrain = {xi, ti, yi}ntrain\ni=1 , and a predictive model with likelihood pω(y | x, t)\nparameterized by ω ∼p(Ω| Dtrain). The acquisition function takes as input Dpool\nand returns a batch of data {xi, ti}b\ni=1 which are labeled using an oracle and added to\nDtrain. We focus on examining when there is only access to the observed treatments\n{ti}\nnpool\ni=1 : scenarios where treatment assignment is not possible.\nAn intuitive way to define informativeness is using the estimated uncertainty of\nour model. In general, we can distinguish two sources of uncertainty: epistemic and\naleatoric uncertainty [Der Kiureghian and Ditlevsen, 2009; Kendall and Gal, 2017].\nEpistemic (or model) uncertainty, arises from uncertainty in the model parameters.\nThis is for example caused by the model not having seen similar data points before,\nand therefore is unclear what the correct label would be. As before, we focus on using\nepistemic uncertainty to identify informative points to acquire the label for.\nA.2\nMethods\nIn this section, we introduce several acquisition functions, analyze how they bias\nthe acquisition of training data, and show the resulting CATE functions learned\nfrom such training data.\nWe are interested in acquisition functions conditioned\non realizations of both x and t:\na(Dpool, p(Ω| Dtrain)) =\narg max\n{xi,ti}b\ni=1⊆Dpool I(• | {xi, ti}, Dtrain),\nwhere I(• | x, t, Dtrain) is a measure of disagreement between parametric function\npredictions given x and t over samples ω ∼p(Ω| D). We make assumptions A.1\nand A.2 (consistency, and unconfoundedness). We relax assumption A.3 (overlap) by\nA. Causal-BALD: Deep Bayesian Active Learning of Outcomes to Infer\nTreatment-Effects from Observational Data\n194\n(a) Random\n(b) Propensity\n(c) τBALD\nFigure A.2: Naive acquisition functions: How the training set is biased and how this effects\nthe CATE function with a fixed budget of 300 acquired points.\nallowing for its violation over subsets of the support of Dpool. We present all theorems,\nproofs, and detailed assumptions in Appendix A.6.1.\nA.2.1\nNaive Acquisition Functions, Training Data Bias, and\nthe Effect on the Estimated CATE Function.\nTo motivate Causal–BALD, we first look at a set of naive acquisition functions. The\nsimplest function selects data points uniformly at random from Dpool and adds them to\nDtrain. In Figure A.2(a) we have acquired 300 such examples from a synthetic dataset\nand trained a deep-kernel Gaussian process [van Amersfoort et al., 2021] on those\nlabeled examples. Comparing the top two panes, we see that Dtrain (middle) contains\nan unbiased sample of the data in Dpool (top). However, in the bottom pane we see\nthat while the CATE estimator is accurate (and certain) near the modes of Dpool,\nit becomes less accurate as we move to lower density regions. In this way random\nacquisition reflects the biases inherent in Dpool and over-allocates resources to the\nmodes of the distribution. If the mode were to coincide with a region of non-overlap,\nthe function would most frequently acquire uninformative examples.\nNext, we look at using propensity scores to bias acquisition toward regions where\nthe overlap assumption is satisfied.\nDefinition A.1. Propensity based acquisition\nI(bπt | x, t, Dtrain) ≡1 −bπt(x)\n(A.1)\nIntuitively, this function prefers points where the propensity for observing the\ncounterfactual is high. We are considering the setup where Dpool contains observations\nof both X and T, so it is straightforward to train an estimator for the propensity,\nbπt(x). Figure A.2(b) shows that while propensity score acquisition matches the treated\nand control densities in Dtrain, it still biases acquisition towards the modes of Dpool.\nBALD aims to acquire data (x, t) that maximally reduce uncertainty in the model\nparameters Ωused to predict the treatment effect. The most direct way to apply\nBALD is to use our uncertainty over the predicted treatment effect, expressed using\nthe following information theoretic quantity:\nDefinition A.2. τBALD\nI(Y 1 −Y 0; Ω| x, t, Dtrain) ≈Var\nω (bµω(x, 1) −bµω(x, 0))\nA. Causal-BALD: Deep Bayesian Active Learning of Outcomes to Infer\nTreatment-Effects from Observational Data\n195\n(a) µBALD\n(b) ρBALD\n(c) µρBALD\nFigure A.3: Causal–BALD acquisition functions: How the training set is biased and how\nthis effects the CATE function with a fixed budget of 300 acquired points.\nBuilding off the result in [Jesson et al., 2020], we show how the LHS measure about\nthe unobservable potential outcomes can be estimated by the variance over Ωof the\nidentifiable difference in expected outcomes in Theorem A.1 of the appendix. A similar\nresult has been proposed for non-parametric models Alaa and van der Schaar [2018].\nIntuitively, this measure represents the information gain for Ωif we could observe the\ndifference in potential outcomes Y 1 −Y 0 for a given measurement x and Dtrain.\nHowever, labels for the random variable Y 1 −Y 0 are never observed so τBALD\nrepresents an irreducible measure of uncertainty. That is, τBALD will be high if\nit is uncertain about the label given the unobserved treatment t′, regardless of its\ncertainty about the label given the observed treatment t, which makes τBALD highest\nfor low-density regions and regions with no overlap. Figure A.2(c) illustrates these\nconsequences. We see the acquisition biases the training data away from the modes of\nthe Dpool, where we cannot know the treatment effect (no overlap). In datasets where\nwe do not have full overlap, it leads to uninformative acquisitions.\nA.2.2\nCausal–BALD\nIn the previous section we looked at naive methods that either considered overlap,\nor considered information gain. In this section we develop a measure that take into\naccount both factors when choosing a new training data point.\nFirst, we focus only on reducible uncertainty:\nDefinition A.3. µBALD\nI(Y t; Ω| x, t, Dtrain) ≈\nVar\nω∼p(Ω|Dtrain) (bµω(x, t)) .\n(A.2)\nThis measure represents the information gain for the model parameters Ωif we\nobtain a label for the observed potential outcome Y t given a data point (x, t) and\nDtrain. Proof is given in Theorem A.4 of the appendix.\nµBALD only contains observable quantities; however, it does not take into account\nour belief about the counterfactual outcome. As illustrated in Figure A.3(a), this\napproach can prefer acquiring (x, t) when we are also very uncertain about (x, t′), even\nif (x, t′) is not in Dpool. Since we can neither reduce uncertainty over such (x, t′) nor\nknow the treatment effect, acquisition would not be data efficient.\nA. Causal-BALD: Deep Bayesian Active Learning of Outcomes to Infer\nTreatment-Effects from Observational Data\n196\nNext, we can take an information theoretic approach to combining knowledge\nabout a data point’s information gain and overlap. Let bµω(x, t) be an instance of\nthe random variable bµt\nΩ∈R corresponding to the expected outcome conditioned\non t.\nFurther, let bτω(x) be an instance of the random variable bτΩ= bµ1\nΩ−bµ0\nΩ\ncorresponding to the CATE. Then,\nDefinition A.4. ρBALD\nI(Y t; bτΩ| x, t, Dtrain) ⪆1\n2 log\n Varω(bτω(x))\nVarω(bµω(x, t′))\n!\n(A.3)\nThis measure represents the information gain for the CATE τΩif we observe the\noutcome Y for a data point (x, t) and the data we have trained on Dtrain. Proof\nfor this result is given in Theorem A.5.\nIn contrast to µ-BALD, this measure accounts for overlap in two ways. First,\nρ–BALD will be scaled by the inverse of the variance of the expected counterfactual\noutcome bµω(x, t′). This will bias acquisition towards examples for which we are certain\nabout counterfactual outcome, and so we can assume that overlap is satisfied for\nobserved (x, t). Second, ρ–BALD is discounted by Covω(bµω(x, t), bµω(x, t′)). This is\nan interesting concept that we will leave for future discussion.\nIn Figure A.3(b) we see that ρ–BALD has matched the distributions of the treated\nand control groups in a similar manner to propensity acquisition in Figure A.2(b).\nFurther, we see that the CATE estimator is more accurate over the support of the\ndata. However, there is a shortcoming of ρ–BALD that results in it under exploring\nlow density regions of Dpool, which we comment on in §A.6.1.3.\nTo combine the positive attributes of µ–BALD and ρ–BALD, while mitigating\ntheir shortcomings, we introduce µρBALD.\nDefinition A.5. µρBALD\nI(µρ | x, t, Dtrain) ≡Var\nω (bµω(x, t))\nVarω(bτω(x))\nVarω(bµω(x, t′)).\nHere, we scale Equation A.3, which has equivalent expression\nVarω(bτω(x))\nVarω(bµω(x,t′)) by our\nmeasure for µBALD such that in the cases where the ratio may be equal, there is a\npreference for data points the current model is more uncertain about. We can see\nin Figure A.3(c) that the acquisition of training data examples is more uniformly\ndistributed over the support of the pool data where overlap is satisfied. Furthermore,\nthe accuracy of the CATE estimator is the highest over that region.\nA.3\nRelated Work\nDeng et al. [2011] propose the use of Active Learning for recruiting patients to assign\ntreatments that will reduce the uncertainty of an Individual Treatment Effect model.\nHowever, their setting is different from ours—we assume that suggesting treatments\nare too risky or even potentially lethal—and instead we acquire patients for the\npurpose of revealing their outcome (e.g. by having a biopsy). Additionally, although\ntheir method uses the predictive uncertainty to identify which patients to recruit,\nit does not disentangle the sources of uncertainty and as such treatments with high\nA. Causal-BALD: Deep Bayesian Active Learning of Outcomes to Infer\nTreatment-Effects from Observational Data\n197\noutcome variance will be recruited as well. Closer to our proposal is the work from\nSundin et al. [2019], where the authors propose the use of a Gaussian process (GP)\nto model the individual treatment effect and use the expected information gain over\nthe S-type error rate, defined as the error in predicting the sign of the CATE, as\ntheir acquisition function. We compare to this in our experiment by limiting the\naccess to counterfactual observations (γ baseline) and adapting it to Deep Ensembles\n[Lakshminarayanan et al., 2017] and DUE [van Amersfoort et al., 2021] (more details\nabout the adaptation is provided in Appendix A.6.2.1).\nA.4\nEmpirical Validation\nIn this section we evaluate our acquisition objectives on synthetic and semi-synthetic\ndatasets.\nModels Our objectives rely on methods that are capable of modeling the uncertainty\nand handling high-dimensional data modalities. DUE [van Amersfoort et al., 2021]\nis an instance of Deep Kernel Learning [Wilson et al., 2016], where a deep feature\nextractor is used to transform the inputs over which a Gaussian process’ (GP) kernel is\ndefined. In particular, DUE uses a variational inducing point approximation [Hensman\net al., 2015] and a constrained feature extractor which contains residual connections\nand spectral normalisation to enable reliable uncertainty. It was previously shown to\nobtain SotA results on IHDP [van Amersfoort et al., 2021]. In DUE, we distinguish\nbetween the model parameters θ and the variational parameters ω, and we are Bayesian\nonly over the ω parameters. Since DUE is a GP, we obtain a full Gaussian posterior\nover our outputs from which we can use the mean and covariance directly. When\nnecessary, sampling is very efficient and only requires a single forward pass in the deep\nmodel. We describe all hyper parameters in Appendix A.6.7.\nBaselines We compare against the following baselines:\nRandom. This acquisition function selects points uniformly at random.\nPropensity. An acquisition function based on the propensity score (Eq. A.1). We\ntrain a propensity model on the combination of the train and pool dataset which we\nthen use acquire points based on their propensity score. Please note that this is a\nvalid assumption as training a propensity model does not require outcomes.\nγ (S-type error rate) [Sundin et al., 2019]. This acquisition function is the S-\ntype error rate based method proposed by Sundin et al. [2019]. We have adapted the\nacquisition function to use with Bayesian Deep Neural Networks. The objective is\ndefined as I(γ; Ω| x, Dtrain), where γ(x) = probit−1(−\n|Ep(τ|x,Dtrain)[τ]|\n√\nVar(τ|x,Dtrain)) and probit−1(·)\nis the cumulative distribution function of normal distribution. In contrast to the\noriginal formulation, we do not assume access to counterfactual observations at\ntraining time.\nDatasets Starting from the hypothesis that different objectives can target different\ntypes of imbalances and overlap ratios we construct a synthetic dataset [Kallus et al.,\n2019] demonstrating the different biases. Additionally, we study the performance of our\nacquisition functions on the IHDP dataset [Hill, 2011; Shalit et al., 2017], a standard\nbenchmark in causal treatment effect literature, and finally we demonstrate that our\nA. Causal-BALD: Deep Bayesian Active Learning of Outcomes to Infer\nTreatment-Effects from Observational Data\n198\nmethod is suitable for high dimensional datasets on CMNIST [Jesson et al., 2021],\nan MNIST [LeCun, 1998] based dataset adapted for causal treatment effect studies.\nDetailed descriptions of each dataset are given in Appendix A.6.3.\nFigure A.4: √ϵPEHE performance (shaded standard error) for DUE models. (left to right)\nsynthetic (40 seeds), and IHDP (200 seeds). We observe that BALD objectives outperform\nthe random, γ and propensity acquisition functions significantly, suggesting that epistemic\nuncertainty aware methods that target reducible uncertainty can be more sample efficient.\nA.4.1\nResults\nFor each of the acquisition objectives, dataset, and model we present the mean and\nstandard error of empirical square root of precision in estimation of heterogenous effect\n(PEHE) 1. We summarize each active learning setup in Table A.1.\nIn Figure A.4, we see that epistemic uncertainty aware µρBALD outperforms\nthe baselines, random, propensity and S-Type error rate (γ). As we analysed in\nsection A.2, this is expected as our acquisition objectives target the type of uncertainty\nthat can be reduced – that is the epistemic uncertainty for which we have overlap\nbetween treatment and control. Additionally, µρBALD shows superior performance\nover the other objectives in the high dimensional dataset CMNIST verifying our\nqualitative analysis in Figure A.3(c).\nA.5\nDiscussion\nWe have introduced a new acquisition function for active learning of individual-level\ncausal-treatment effects from high dimensional observational data, based on Bayesian\nActive Learning by Disagreement Houlsby et al. [2011]. We derive our proposed method\nfrom an information theoretic perspective and compared with various acquisition\n1√ϵP EHE =\nq\n1\nN\nP\nx (ˆτ(x) −τ(x))2\nA. Causal-BALD: Deep Bayesian Active Learning of Outcomes to Infer\nTreatment-Effects from Observational Data\n199\nfunctions that do not take into consideration epistemic uncertainty (like random\nor propensity based) or they target uncertainties that cannot be reduced in the\nobservational setting (i.e. when we do not have access to counterfactual observations).\nWe show that our methods significantly outperform the baselines while also studying\nthe various properties of each of our proposed objectives in both a quantitative and a\nqualitative analysis, potentially impacting areas like healthcare where sample efficiency\nin acquisition of new examples imply improved safety and reductions in costs.\nA.6\nDetails\nA.6.1\nTheoretical Results\nA.6.1.1\nτ-BALD\nTheorem A.1. Under the following assumptions:\n1. Unconfoundedness (Y 0, Y 1) ⊥⊥T | X;\n2. Consistency Y | T = Y t;\n3. Y 1 and Y 0, when conditioned on realizations x of the r.v. X and t of the r.v. T,\nare independent-normally distributed or joint-normally distributed r.v.s.\n4. bµω(x, t) is a consistent estimator of E[Y | T = t, X = x]\nthe information gain for Ωif we could observe a label for the difference in potential\noutcomes Y 1−Y 0 given measured covariates x, treatment t and a dataset of observations\nDtrain = {xi, ti, yi}n\ni=1 is approximated as\nI(Y 1 −Y 0; Ω| x, t, Dtrain) ≈\nVar\nω∼p(Ω|Dtrain) (bµω(x, 1) −bµω(x, 0))\n(A.4)\nProof.\nI(Y 1 −Y 0; Ω| x, Dtrain) = H(Y 1 −Y 0 | x, Dtrain) −\nE\np(Ω|Dtrain)\nh\nH(Y 1 −Y 0 | x, ω)\ni\n(A.5a)\n≈Var(Y 1 −Y 0 | x, Dtrain) −\nE\np(Ω|Dtrain)\nh\nVar(Y 1 −Y 0 | x, ω)\ni\n(A.5b)\n=\nVar\np(Ω|Dtrain)\n\u0010\nE[Y 1 −Y 0 | x, ω]\n\u0011\n(A.5c)\n=\nVar\np(Ω|Dtrain) (bµω(x, 1) −bµω(x, 0))\n(A.5d)\nIn (A.5a) we adapt the result of Houlsby et al. [2011] and express the information gain\nas the mutual information between the observable difference in potential outcomes\nY 1 −Y 0 and the parameters Ω; given observed covariates x, treatment t, and training\ndata Dtrain = {(xi, ti, yi)}ntrain\ni=1 . In (A.5b) we apply lemma A.2 to the r.h.s terms\nof (A.5a). We then use the result in Jesson et al. [2020] and move from (A.5b) to\n(A.5c) by application of the law of total variance. Finally, under the consistency\nand unconfoundedness assumptions we express the information gain in terms of the\nidentifiable difference in expected outcomes bµω(x, 1) −bµω(x, 0).\nLemma A.2. Under the following assumptions:\n1. Y 1, Y 0 are independent-normally distributed or joint-normally distributed r.v.s;\nA. Causal-BALD: Deep Bayesian Active Learning of Outcomes to Infer\nTreatment-Effects from Observational Data\n200\n2. With A = Var(Y 1 −Y 0): let |A −1| ≤1 and A ̸= 0. That is to say, the predictive\nvariance must be greater than 0 and less than or equal to 2;\nH(Y 1 −Y 0) ≈Var(Y 1 −Y 0)\n(A.6)\nProof. By assumption 1, Y 1 −Y 0 is also a normally distributed random variable. By\ncorollary A.3,\nH(Y 1 −Y 0) = 1\n2 + 1\n2 log(2π Var(Y 1 −Y 0))\n(A.7)\nSo given assumption 2, the first order Taylor polynomial of H(Y 1 −Y 0) is\n1\n2 + 1\n2 log(2π Var(Y 1 −Y 0)) ≈1\n2 + 1\n2(2π Var(Y 1 −Y 0) −1)\n= 1\n2 + π Var(Y 1 −Y 0) −1\n2\n= π Var(Y 1 −Y 0)\n∝Var(Y 1 −Y 0)\n(A.8)\nCorollary A.3. The entropy of a normally distributed random variable with variance\nσ2 is 1\n2 + 1\n2 log(2πσ2)\nA.6.1.2\nµ-BALD\nTheorem A.4. Under the following assumptions:\n1. Unconfoundedness (Y 0, Y 1) ⊥⊥T | X,\n2. Consistency Y | T = Y t,\n3. Y conditioned on x and t is a normally distributed random variable,\n4. bµω(x, t) is a consistent estimator of E[Y | T = t, X = x],\nthe information gain for Ωwhen we observe a label for the potential outcome Y t given\nmeasured covariates x, treatment t and a dataset of observations Dtrain = {xi, ti, yi}n\ni=1\ncan be approximated as is\nI(Y t; Ω| x, t, Dtrain) ≈1\n2 log\n Var(Y | x, t, Dtrain)\nEω[Var(Y | x, t, ω)]\n!\n,\n(A.9)\nor\nI(Y t; Ω| x, t, Dtrain) ≈\nVar\nω∼p(Ω|Dtrain) (bµω(x, t)) .\n(A.10)\nEquation (A.9) expresses the information gain as the logarithm of a ratio between\npredictive and aleatoric uncertainty in the outcome. Whereas, equation (A.9) expresses\nthe information gain as a direct estimate of the epistemic uncertainty.\nProof.\nI(Y t; Ω| x, t, Dtrain) = H(Y | x, t, Dtrain) −\nE\np(Ω|Dtrain) [H(Y | x, t, ω)]\n(A.11a)\n= 1\n2 log\n\u0010\n2π Var(Y | x, t, Dtrain)\n\u0011\n−\nE\np(Ω|Dtrain)\n1\n2 log (2π Var(Y | ω, x, t))\n(A.11b)\nA. Causal-BALD: Deep Bayesian Active Learning of Outcomes to Infer\nTreatment-Effects from Observational Data\n201\n≥1\n2 log\n\u0010\n2π Var(Y | x, t, Dtrain)\n\u0011\n−1\n2 log\n \n2π\nE\np(Ω|Dtrain) Var(Y | ω, x, t)\n!\n(A.11c)\n= 1\n2 log\n Var(Y | x, t, Dtrain)\nEω[Var(Y | x, t, ω)]\n!\n(A.11d)\nI(Y t; Ω| x, t, Dtrain) = H(Y | x, t, Dtrain) −\nE\np(Ω|Dtrain) [H(Y | x, t, ω)]\n(A.12a)\n≈Var[Y | x, t, Dtrain] −\nE\np(Ω|Dtrain) [Var[Y | x, t, ω]]\n(A.12b)\n=\nVar\nω∼p(Ω|Dtrain) (bµω(x, t))\n(A.12c)\nIn (A.12a) we express the information gain as the mutual information between the\nobserved potential outcome Y t and the parameters Ω; given observed covariates x,\ntreatment t, and training data Dtrain. By consistency, we can drop the superscript\non the potential outcome. In (A.12b) we approximate the r.h.s terms of (A.12a) by\napplication of Lemma A.2. Finally, we can move from (A.12b) to (A.12c) by application\nof the law of total variance.\nNote that for discrete or categorical Y , it is straightforward to evaluate Equation\n(A.12a) directly.\nA.6.1.3\nρ-BALD\nTheorem A.5. Under the following assumptions\n1. {bµω(x, t) : t ∈{0, 1}} are instances of the independent-normally distributed or\njoint-normally distributed random variables {bµt\nΩ= E[Y | Ω, T = t, x] : t ∈\n{0, 1}},\n2. Varω∼p(Ω|Dtrain)(bµω(x, t′)) > 0 .\nLet bτω(x) be a realization of the random variable bτΩ= bµ1\nΩ−bµ0\nΩ. The information gain\nfor bτΩif we observe the label for the potential outcome Y t given measured covariates x,\ntreatment t and a dataset of observations Dtrain = {xi, ti, yi}n\ni=1 is approximately\nI(Y t; bτΩ| x, t, Dtrain) ≈\nVarω(bτω(x))\nVarω(bµω(x, t′)),\n= Varω (bµω(x, t)) −2Covω(bµω(x, t), bµω(x, t′))\nVarω (bµω(x, t′))\n+ 1,\n(A.13)\nwhere for binary T = t, t′ = (1 −t).\nProof.\nI(Y t; bτΩ| x, t, D) = H(bτΩ| x, t, D) −H(bτΩ| Y t, x, t, D)\n(A.14a)\n= H(bτΩ| x, t, D) −\nE\nyt∼p(Y t|x,t,D) H(bτΩ| yt, x, t)\n(A.14b)\n= 1\n2 log(2π Var(bτΩ)) −\nE\nyt∼p(Y t|x,t,D)\n\u00141\n2 log(2π Var(bτΩ| yt))\n\u0015\n(A.14c)\n≥1\n2 log(2π Var(bτΩ)) −1\n2 log(2π E\nh\nVar(bτΩ| yt)\ni\n)\n(A.14d)\nA. Causal-BALD: Deep Bayesian Active Learning of Outcomes to Infer\nTreatment-Effects from Observational Data\n202\n= 1\n2 log\n \nVar(bτΩ)\nE [Var(bτΩ| yt)]\n!\n,\n(A.14e)\nand we can further expand the fraction to\nVar(bτΩ| x, t, D)\nE [Var(bτΩ| yt)] =\n(A.14f)\n=\nVar(bτΩ| x, t, D)\nVarω∼p(Ω|D)(bµω(x, t′))\n(A.14g)\n= Varω∼p(Ω|D)(bτω(x) | t)\nVarω(bµω(x, t′))\n(A.14h)\n= Varω(bµω(x, 1) −bµω(x, 0) | t)\nVarω(bµω(x, t′))\n(A.14i)\n= Varω(bµω(x, t) −bµω(x, t′))\nVarω(bµω(x, t′))\n(A.14j)\n= Varω(bµω(x, t)) + Varω(bµω(x, t′)) −2 Covω(bµω(x, t), bµω(x, t′))\nVarω(bµω(x, t′))\n(A.14k)\n= Varω(bµω(x, t)) −2 Covω(bµω(x, t), bµω(x, t′))\nVarω(bµω(x, t′))\n+ 1,\n(A.14l)\nwhere (A.14a) by definition of mutual information; (A.14a)-(A.14b) from the result of\nHoulsby et al. [2011]; (A.14b)-(A.14c) by Assumption 1. and Corollary A.3; (A.14c)-\n(A.14d) by Jensen’s inequality; (A.14d)-(A.14e) by the logarithmic quotient identity;\n(A.14f)-(A.14g) by Lemma A.6; (A.14g)-(A.14h) by definition of the variance. (A.14h)-\n(A.14i) by definition of bτω; (A.14i)-(A.14j) by symmetry of the variance of the difference\nof two random variables; (A.14j)-(A.14k) by the definition of the variance of the\ndifference of two random variables; and (A.14k)-(A.14l) by cancelling terms.\nA. Causal-BALD: Deep Bayesian Active Learning of Outcomes to Infer\nTreatment-Effects from Observational Data\n203\nLemma A.6. Under the following assumptions\n1. Consistency Y | T = Y t;\n2. Unconfoundedness (Y 0, Y 1) ⊥⊥T | X;\nE\nyt∼p(Y t|x,t,D)\nh\nVar(bτΩ| yt)\ni\n≈\nE\nyt∼p(Y t|x,t,D)\n\"\nVar\nω∼p(Ω|Dtrain)(bµω(x, t′))\n#\n,\n(A.15)\nwhere for binary T = t, t′ = (1 −t).\nProof.\nEyt∼p(Y t|x,t,D)\nh\nVar(bτΩ| yt)\ni\n= E\np(yt)\n\nE\np(ω)\n\n\n \nbτω −E\np(ω)\nh\nbτω | yti!2\n| yt\n\n\n\n,\n(A.16)\n= E\np(yt)\n\nE\np(ω)\n\n\n \nE[Y 1 −Y 0 | x, ω] −E\np(ω)\nh\nE[Y 1 −Y 0 | x, ω] | yti!2\n| yt\n\n\n\n,\n(A.17)\n=\nE\np(yt)\n\"\nE\np(ω)\n\"\u0012\nE[Y 1 | x, ω] −E[Y 0 | x, ω] −E\np(ω)\n\u0002\nE[Y 1 | x, ω] | yt\u0003\n+ E\np(ω)\n\u0002\nE[Y 0 | x, ω] | yt\u0003\u00132\n| yt\n##\n,\n(A.18)\n=\nE\np(yt)\n\"\nE\np(ω)\n\"\u0012\u0012\nE[Y 1 | x, ω] −E\np(ω)\n\u0002\nE[Y 1 | x, ω] | yt\u0003\u0013\n−\n\u0012\nE[Y 0 | x, ω] −E\np(ω)\n\u0002\nE[Y 0 | x, ω] | yt\u0003\u0013\u00132\n| yt\n##\n,\n(A.19)\n=\nE\np(yt)\n\"\nE\np(ω)\n\"\u0012\u0012\nE[Y t | x, ω] −E\np(ω)\n\u0002\nE[Y t | x, ω] | yt\u0003\u0013\n−\n\u0012\nE[Y t′ | x, ω] −E\np(ω)\nh\nE[Y t′ | x, ω] | yti\u0013\u00132\n| yt\n##\n,\n(A.20)\n=\nE\np(yt)\n\n\nE\np(ω|yt)\n\n\n \u0012\nE\np(yt|x,ω)[yt] −\nE\np(ω|yt)\n\u0014\nE\np(yt|x,ω)[yt]\n\u0015\u0013\n−\n \nE\np(yt′|x,ω)\n[yt′] −\nE\np(ω|yt)\n\"\nE\np(yt′|x,ω)\n[yt′]\n#!!2\n\n\n,\n(A.21)\n=\nE\np(yt)\n\n\nE\np(ω|yt)\n\n\n\n\n\n\n\n\u0012\nE\np(yt|x,ω)[yt] −\nE\np(ω|yt)\n\u0014\nE\np(yt|x,ω)[yt]\n\u0015\u0013\n|\n{z\n}\n≈0\n−\n \nE\np(yt′|x,ω)\n[yt′] −E\np(ω)\n\"\nE\np(yt′|x,ω)\n[yt′]\n#!\n\n\n\n\n\n2\n\n\n,\n(A.22)\n≈E\np(yt)\n\n\nE\np(ω|yt)\n\n\n \nE\np(yt′|x,ω)\n[yt′] −E\np(ω)\n\"\nE\np(yt′|x,ω)\n[yt′]\n#!2\n\n\n,\n(A.23)\n= E\np(yt)\n\n\nE\np(ω|yt)\n\n\n \nbµω(x, t′) −E\np(ω) [bµω(x, t′)]\n!2\n\n\n,\n(A.24)\n=\nE\nyt∼p(Y t|x,t,D)\n\"\nVar\nω∼p(Ω|Dtrain)(bµω(x, t′))\n#\n,\n(A.25)\nwhere (A.16) by definition of variance; (A.16)-(A.17) by definition of bτω; (A.17)-\n(A.18) by linearity of expectations; (A.18)-(A.19) by grouping terms; (A.19)-(A.20) by\nsymmetry of the square; (A.20)-(A.21) by rewriting expectations in terms of densities;\n(A.21)-(A.22) the observed potential outcome does not have an effect on the expectation\nof the model for the counterfactual outcome; (A.22)-(A.23) we drop the term as an\napproximation as we cannot estimate here how much the expected outcome is going\nA. Causal-BALD: Deep Bayesian Active Learning of Outcomes to Infer\nTreatment-Effects from Observational Data\n204\nto change—the conservative assumption is that will not change; (A.23)-(A.24) by\ndefinition of bµω; (A.24)-(A.25) by definition of variance;\nρ-BALD Failure Consider two examples in Dpool, (x1, t1) and (x2, t2) where Varω\n(bµω(x1, t1)) = Varω(bµω(x1, t′\n1)) and Varω(bµω(x2, t2)) = Varω(bµω(x2, t′\n2)).\nThat is,\nfor each point we are as uncertain about the conditional expectation given the\nobserved treatment as we would be given the counterfactual treatment.\nFurther,\nlet Covω(bµω(x1, t1), bµω(x1, t′\n1)) = Covω(bµω(x2, t2), bµω(x2, t′\n2)) and Varω(bµω(x1, t1)) >\nVarω(bµω(x2, t2)). In this scenario ρ–BALD would rank these two points equally, but\nin practice it may be preferable to choose (x1, t1) over (x2, t2) as it would more likely\nbe a point as yet unseen by the model. When naively acquiring multiple points per\nacquisition step, this method biases training data to the modes of Dpool.\nA.6.1.4\nµπBALD\nFigure A.5: µπBALD\nThe most straightforward way to combine knowledge about a data point’s in-\nformation gain and overlap is to simply multiply µBALD(A.2) by the propensity\nacquisition term (A.1):\nDefinition A.6. µπBALD\nI(µπ | x, t, Dtrain) ≡(1 −bπt(x))\nVar\nω∼p(Ω|Dtrain) (bµω(x, t)) .\nWe can see in Figure A.5 that the acquisition of training data results in matched\nsampling as we saw for propensity acquisition in Figure A.2(b), but that the tails\nof the overlapping distributions extend further into the low density regions of the\npool set support where overlap is satisfied.\nA.6.2\nBaselines\nA.6.2.1\nS-type Error Information Gain\nIn their work, Sundin et al. [2019] assume that the underlying model is a Gaussian\nProcess (GP) and also that they have access to the counterfactual outcome. Although\nGPs are suitable for uncertainty estimation, they do not scale up to high dimensional\ndatasets (e.g. images). We propose to use Deep Ensembles and DUE for alleviating the\ncapabilities issues and we modified the objective to be more suitable for our architecture.\nA. Causal-BALD: Deep Bayesian Active Learning of Outcomes to Infer\nTreatment-Effects from Observational Data\n205\nFollowing the formulation from Houlsby et al. [2011], the acquisition strategy be-\ncomes arg maxx H[γ|x, D] −EH[p(θ|D)[γ|x, θ]], where γ(x) = probit−1(−\n|Ep(τ|x,Dtrain)[τ]|\n√\nVar(τ|x,Dtrain)),\nprobit−1(·) is the cumulative distribution function of normal distribution and p(γ|x, D) =\nBernoulli(γ). With DUE (Deep Kernel Learning method) Deep Ensembles (samples\nfrom p(θ|D) we can compute those terms similarly to how we implemented our\nBALD objectives.\nBelow is an example of how this was implemented in PyTorch:\ntau_mu = mu1s - mu0s\ntau_var = var1s + var0s + 1e-07\ngammas = torch.distributions.normal.Normal(0, 1).cdf(\n-tau_mu.abs() / tau_var.sqrt()\n)\ngamma = gammas.mean(-1)\npredictive_entropy = dist.Bernoulli(gamma).entropy()\nconditional_entropy = dist.Bernoulli(gammas).entropy().mean(-1)\n# it can get negative very small number\n# because of numerical instabilities\nscores = (predictive_entropy - conditional_entropy).clamp_min(1e-07)\nA.6.3\nDatasets\nA.6.3.1\nSynthetic Data\nWe modify the synthetic dataset presented by Kallus et al. [2019]. Our dataset is\ndescribed by the following structural causal model (SCM):\nx ≜Nx,\n(A.26a)\nt ≜Nt,\n(A.26b)\ny ≜(2t −1)x + (2t −1) −2 sin(2(2t −1)x) + 2(1 + 0.5x) + Ny,\n(A.26c)\nwhere Nx ∼N(0, 1), Nt ∼Bern(sigmoid(2x + 0.5)), and Ny ∼N(0, 1).\nEach random realization of the simulated dataset generates 10000 pool set examples,\n1000 validation examples, and 1000 test examples. In the experiments we report results\nover 20 random realizations. The seeds for the random number generators are i, i + 1,\nand i + 2; {i ∈[0, 1, . . . , 19]}, for the training, validation, and test sets, respectively.\nA.6.3.2\nIHDP Data.\nInfant Health and Development Program (IHDP) is a semisynthetic dataset [Hill, 2011;\nShalit et al., 2017] commonly used in literature to study the performance of causal effect\nestimation methods. The dataset consists of 747 cases, out of which 139 are assigned\nin treatment group and 608 in control. Each unit is represented by 25 covariates\ndescribing different aspects of the infants and their mothers.\nA.6.3.3\nCMNIST Data.\nFollowing the setup from Jesson et al. [2021], we use a simulated dataset based on\nMNIST [LeCun, 1998]. CMNIST is described by the following SCM:\nx ≜Nx,\n(A.27a)\nA. Causal-BALD: Deep Bayesian Active Learning of Outcomes to Infer\nTreatment-Effects from Observational Data\n206\nϕ ≜\n\u0012\nclip\n\u0012µNx −µc\nσc\n; −1.4, 1.4\n\u0013\n−Minc\n\u0013 Maxc −Minc\n1.4 −-1.4\n(A.27b)\nt ≜Nt,\n(A.27c)\ny ≜(2t −1)ϕ + (2t −1) −2 sin(2(2t −1)ϕ) + 2(1 + 0.5ϕ) + Ny,\n(A.27d)\nwhere Nt (swapping x for ϕ), and Ny are as described in Appendix A.6.3.1. Nx is a\nsample of an MNIST image. The sampled image has a corresponding label c ∈[0, . . . , 9].\nµNx is the average intensity of the sampled image. µc and σc are the mean and standard\ndeviation of the average image intensities over all images with label c in the MNIST\ntraining set. In other words, µc = E[µNx | c] and σ2\nc = Var[µNx | c]. To map the high\ndimensional images x onto a one-dimensional manifold ϕ with domain [−3, 3] above, we\nfirst clip the standardized average image intensity on the range (−1.4, 1.4). Each digit\nclass has its own domain in ϕ, so there is a linear transformation of the clipped value\nonto the range [Minc, Maxc]. Finally, Minc = −2 + 4\n10c, and Maxc = −2 + 4\n10(c + 1).\nFor each random realization of the dataset, the MNIST training set is split into\ntraining (n = 35000) and validation (n = 15000) subsets using the scikit-learn function\ntrain_test_split(). The test set is generated using the MNIST test set (n = 10000).\nThe random seeds are {i ∈[0, 1, . . . , 19]} for the 20 random realizations generated.\nA.6.4\nActive Learning Setup Details\nTable A.1: Summary of active learning setup per dataset.\nDataset\nWarm up size\nAcq. size\n# of Acq.\nSynthetic\n0\n10\n31\nIHDP\n100\n10\n38 (max)\nCMNIST\n250\n50\n20\nA.6.5\nMore Results\nA. Causal-BALD: Deep Bayesian Active Learning of Outcomes to Infer\nTreatment-Effects from Observational Data\n207\nFigure A.6: √ϵPEHE performance (shaded standard error) for Deep Ensembles based\nmodels. (left to right) synthetic (20 seeds), IHDP (50 seeds) and CMNIST\n(5 seeds)\ndataset results, (top to bottom) comparison with baselines, comparison between BALD\nobjectives. We observe that BALD objectives outperform the random, γ and propensity\nacquisition functions significantly, suggesting that epistemic uncertainty aware methods that\ntarget reducible uncertainty can be more sample efficient.\nFigure A.7: √ϵPEHE performance (shaded standard error) for DUE models. (left to right)\nsynthetic (40 seeds), and IHDP (200 seeds). We observe that BALD objectives outperform\nthe random, γ and propensity acquisition functions significantly, suggesting that epistemic\nuncertainty aware methods that target reducible uncertainty can be more sample efficient.\nA. Causal-BALD: Deep Bayesian Active Learning of Outcomes to Infer\nTreatment-Effects from Observational Data\n208\nA.6.6\nCompute\nWe used a cluster of 8 nodes with 4 GPUs each (16 RTX 2080 and 16 Titan RTX).\nThe total GPU hours is estimated to be:\n8 baselines x (.5 + 1 + 1) days per dataset x (5 ensemble components * 0.25 GPU\nusage + 1 DUE * 0.3 GPU usage) x 24 hours = 744 GPU hours\nA.6.7\nNeural Network Architecture\nSynthetic Architecture\n===========================================================\nLayer (type:depth-idx)\nOutput Shape\n===========================================================\nSequential\n--\nNeuralNetwork: 1-1\n[64, 100]\nSequential: 2-1\n[64, 100]\nLinear: 3-1\n[64, 100]\nResidualDense: 3-2\n[64, 100]\nPreactivationDense: 4-1\n[64, 100]\nSequential: 5-1\n[64, 100]\nActivation: 6-1\n[64, 100]\nLinear: 6-2\n[64, 100]\nIdentity: 4-2\n[64, 100]\nResidualDense: 3-3\n[64, 100]\nPreactivationDense: 4-3\n[64, 100]\nSequential: 5-2\n[64, 100]\nActivation: 6-3\n[64, 100]\nLinear: 6-4\n[64, 100]\nIdentity: 4-4\n[64, 100]\nResidualDense: 3-4\n[64, 100]\nPreactivationDense: 4-5\n[64, 100]\nSequential: 5-3\n[64, 100]\nActivation: 6-5\n[64, 100]\nLinear: 6-6\n[64, 100]\nIdentity: 4-6\n[64, 100]\nActivation: 3-5\n[64, 100]\nSequential: 4-7\n[64, 100]\nIdentity: 5-4\n[64, 100]\nLeakyReLU: 5-5\n[64, 100]\nDropout: 5-6\n[64, 100]\nGMM: 1-2\n[64, 5]\nLinear: 2-2\n[64, 5]\nLinear: 2-3\n[64, 5]\nSequential: 2-4\n[64, 5]\nLinear: 3-6\n[64, 5]\nSoftplus: 3-7\n[64, 5]\n===========================================================\nTotal params: 32,115\nIHDP Architecture\nA. Causal-BALD: Deep Bayesian Active Learning of Outcomes to Infer\nTreatment-Effects from Observational Data\n209\n==============================================================\nLayer (type:depth-idx)\nOutput Shape\n==============================================================\nSequential\n--\nTARNet: 1-1\n[64, 400]\nNeuralNetwork: 2-1\n[64, 400]\nSequential: 3-1\n[64, 400]\nLinear: 4-1\n[64, 400]\nResidualDense: 4-2\n[64, 400]\nPreactivationDense: 5-1\n[64, 400]\nSequential: 6-1\n[64, 400]\nIdentity: 5-2\n[64, 400]\nResidualDense: 4-3\n[64, 400]\nPreactivationDense: 5-3\n[64, 400]\nSequential: 6-2\n[64, 400]\nIdentity: 5-4\n[64, 400]\nSequential: 2-2\n[64, 400]\nResidualDense: 3-2\n[64, 400]\nPreactivationDense: 4-4\n[64, 400]\nSequential: 5-5\n[64, 400]\nActivation: 6-3\n[64, 401]\nLinear: 6-4\n[64, 400]\nSequential: 4-5\n[64, 400]\nDropout: 5-6\n[64, 401]\nLinear: 5-7\n[64, 400]\nResidualDense: 3-3\n[64, 400]\nPreactivationDense: 4-6\n[64, 400]\nSequential: 5-8\n[64, 400]\nActivation: 6-5\n[64, 400]\nLinear: 6-6\n[64, 400]\nIdentity: 4-7\n[64, 400]\nActivation: 3-4\n[64, 400]\nSequential: 4-8\n[64, 400]\nIdentity: 5-9\n[64, 400]\nELU: 5-10\n[64, 400]\nDropout: 5-11\n[64, 400]\nGMM: 1-2\n[64, 5]\nLinear: 2-3\n[64, 5]\nLinear: 2-4\n[64, 5]\nSequential: 2-5\n[64, 5]\nLinear: 3-5\n[64, 5]\nSoftplus: 3-6\n[64, 5]\n==============================================================\nCMNIST Architecture\n=============================================================================\nLayer (type:depth-idx)\nOutput Shape\n=============================================================================\nSequential\n--\nTARNet: 1-1\n[200, 100]\nA. Causal-BALD: Deep Bayesian Active Learning of Outcomes to Infer\nTreatment-Effects from Observational Data\n210\nResNet: 2-1\n[200, 48]\nSequential: 3-1\n[200, 48, 1, 1]\nConv2d: 4-1\n[200, 12, 28, 28]\nIdentity: 4-2\n[200, 12, 28, 28]\nResidualConv: 4-3\n[200, 12, 28, 28]\nSequential: 5-1\n[200, 12, 28, 28]\nPreactivationConv: 6-1\n[200, 12, 28, 28]\nPreactivationConv: 6-2\n[200, 12, 28, 28]\nSequential: 5-2\n[200, 12, 28, 28]\nDropout2d: 6-3\n[200, 12, 28, 28]\nConv2d: 6-4\n[200, 12, 28, 28]\nResidualConv: 4-4\n[200, 24, 14, 14]\nSequential: 5-3\n[200, 24, 14, 14]\nPreactivationConv: 6-5\n[200, 12, 28, 28]\nPreactivationConv: 6-6\n[200, 24, 14, 14]\nSequential: 5-4\n[200, 24, 14, 14]\nDropout2d: 6-7\n[200, 12, 28, 28]\nConv2d: 6-8\n[200, 24, 14, 14]\nResidualConv: 4-5\n[200, 24, 14, 14]\nSequential: 5-5\n[200, 24, 14, 14]\nPreactivationConv: 6-9\n[200, 24, 14, 14]\nPreactivationConv: 6-10\n[200, 24, 14, 14]\nSequential: 5-6\n[200, 24, 14, 14]\nDropout2d: 6-11\n[200, 24, 14, 14]\nConv2d: 6-12\n[200, 24, 14, 14]\nResidualConv: 4-6\n[200, 48, 7, 7]\nSequential: 5-7\n[200, 48, 7, 7]\nPreactivationConv: 6-13\n[200, 24, 14, 14]\nPreactivationConv: 6-14\n[200, 48, 7, 7]\nSequential: 5-8\n[200, 48, 7, 7]\nDropout2d: 6-15\n[200, 24, 14, 14]\nConv2d: 6-16\n[200, 48, 7, 7]\nResidualConv: 4-7\n[200, 48, 7, 7]\nSequential: 5-9\n[200, 48, 7, 7]\nPreactivationConv: 6-17\n[200, 48, 7, 7]\nPreactivationConv: 6-18\n[200, 48, 7, 7]\nSequential: 5-10\n[200, 48, 7, 7]\nDropout2d: 6-19\n[200, 48, 7, 7]\nConv2d: 6-20\n[200, 48, 7, 7]\nResidualConv: 4-8\n[200, 48, 7, 7]\nSequential: 5-11\n[200, 48, 7, 7]\nPreactivationConv: 6-21\n[200, 48, 7, 7]\nPreactivationConv: 6-22\n[200, 48, 7, 7]\nSequential: 5-12\n[200, 48, 7, 7]\nDropout2d: 6-23\n[200, 48, 7, 7]\nConv2d: 6-24\n[200, 48, 7, 7]\nAdaptiveAvgPool2d: 4-9\n[200, 48, 1, 1]\nSequential: 2-2\n[200, 100]\nResidualDense: 3-2\n[200, 100]\nPreactivationDense: 4-10\n[200, 100]\nA. Causal-BALD: Deep Bayesian Active Learning of Outcomes to Infer\nTreatment-Effects from Observational Data\n211\nSequential: 5-13\n[200, 100]\nActivation: 6-25\n[200, 49]\nLinear: 6-26\n[200, 100]\nSequential: 4-11\n[200, 100]\nDropout: 5-14\n[200, 49]\nLinear: 5-15\n[200, 100]\nResidualDense: 3-3\n[200, 100]\nPreactivationDense: 4-12\n[200, 100]\nSequential: 5-16\n[200, 100]\nActivation: 6-27\n[200, 100]\nLinear: 6-28\n[200, 100]\nIdentity: 4-13\n[200, 100]\nActivation: 3-4\n[200, 100]\nSequential: 4-14\n[200, 100]\nIdentity: 5-17\n[200, 100]\nLeakyReLU: 5-18\n[200, 100]\nDropout: 5-19\n[200, 100]\nGMM: 1-2\n[200, 5]\nLinear: 2-3\n[200, 5]\nLinear: 2-4\n[200, 5]\nSequential: 2-5\n[200, 5]\nLinear: 3-5\n[200, 5]\nSoftplus: 3-6\n[200, 5]\n=============================================================================\nLeakyReLU or \nELU\nConv2D with \nSpectral Nrom\nDropout\nPreactivationConv\nPreactivationConv\nPreactivationConv\n+\nLeakyReLU or \nELU\nLinear with \nSpectral Norm\nDropout\nPreactivationDense\nPreactivationDense\n+\nResidualConv\nResidualDense\nFigure A.8: PreactivationConv is a convolution layer with LeakyReLU (or ELU when slope\nis negative) activation, dropout and spectral norm applied [Gouk et al., 2021; Miyato et al.,\n2018]. Similarly, PreactivationDense is a dense layer with BatchNorm [Ioffe and Szegedy,\n2015], LeakyReLU (or ELU when slope is negative) activation and spectral norm applied [Gouk\net al., 2021; Miyato et al., 2018].\nResidualConv is the residual convolution layer, de-\nfined as PreactivationConv(PreactivationConv(x)) + SpectralNorm(1x1Conv(x)) and\nResidualDense are residual dense layers, defined as PreactivationDense(x)+x).\nAll experiments were trained using Adam optimizer.\nA.6.7.1\nHyper-Parameters\nA. Causal-BALD: Deep Bayesian Active Learning of Outcomes to Infer\nTreatment-Effects from Observational Data\n212\nTable A.2: Training hyper parameters for Deep Ensemble experiments\nParameter\nSynthetic\nIHDP\nCMNIST\ndim hidden\n100\n400\n100\ndropout\n0.0\n0.15\n0.1\ndepth\n4\n3\n3\nspectral norm\n12\n0.95\n24\nlearning rate\n0.001\n0.001\n0.001\nnegative slope\n0.0\n−1.0\n0.0\nTable A.3: Training hyper parameters for DUE experiments\nParameter\nSynthetic\nIHDP\nCMNIST\ninducing points\n100\n100\n100\ndim hidden\n100\n200\n200\ndropout\n0.2\n0.1\n0.05\ndepth\n3\n3\n2\nbatch size\n200\n100\n64\nspectral norm\n0.95\n0.95\n3.0\nlearning rate\n0.001\n0.001\n0.001\nnegative slope\n0.0\n−1.0\n−1.0\nThe first principle is that you must not fool\nyourself—and you are the easiest person to fool.\nRichard Feynman\nB\nReproducibility Analysis\nB.1\nDeep Learning on a Data Diet\nThe senior author of ‘Deep Learning on a Data Diet’ [Paul et al., 2021] recently gave\na talk at our lab that explored this issue, presenting their novel metrics for pruning\ndatasets. During the talk, the author of this current work suggested a correlation\nbetween the proposed GraNd score at initialization and input norms, sparking further\nresearch into the effectiveness of these new pruning techniques. In this chapter, we\ndelve deeper into this intriguing question, exploring the practicality and efficacy of\nthese metrics for data pruning.\n‘Deep Learning on a Data Diet’.\nPaul et al. [2021] introduce two novel\nmetrics: Error L2 Norm (EL2N) and Gradient Norm at Initialization (GraNd). These\nmetrics aim to provide a more effective means of dataset pruning. It is important\nto emphasize that the GraNd score at initialization is calculated before any training\nhas taken place, averaging over several randomly initialized models. This fact has\nbeen met with skepticism by reviewers1, but Paul et al. [2021] specifically remark\non GraNd at initialization:\nPruning at initialization. In all settings, GraNd scores can be used\nto select a training subset at initialization that achieves test accuracy\nsignificantly better than random, and in some cases, competitive with\ntraining on all the data. This is remarkable because GraNd only contains\ninformation about the gradient norm at initialization. This suggests that\nthe geometry of the training distribution induced by a random network\ncontains a surprising amount of information about the structure of the\nclassification problem.\nGraNd. The GraNd score measures the magnitude of the gradient vector for a\nspecific input sample in the context of neural network training over different parameter\ndraws. The formula for calculating the (expected) gradient norm is:\nGraNd(x) = Eθt[∥∇θtL(f(x; θt), y)∥2]\n(B.1)\nwhere ∇θtL(f(x; θt), y) is the gradient of the loss function L with respect to the\nmodel’s parameters θt at epoch t, f(x; θ) is the model’s prediction for input x, and y\nis the true label for the input. We take an expectation over several training runs. The\ngradient norm provides information about the model’s sensitivity to a particular input\nand helps in identifying data points that have a strong influence on the learning process.\n1See also https://openreview.net/forum?id=Uj7pF-D-YvT&noteId=qwy3HouKSX.\nB. Reproducibility Analysis\n214\nEL2N. The EL2N score measures the squared difference between the predicted\nand (one-hot) true labels for a specific input sample. The formula for calculating\nthe EL2N score is:\nEL2N(x) = Eθt[∥f(x; θt) −y∥2\n2]\n(B.2)\nwhere f(x; θ) is the model’s prediction for input x, y is the (one-hot) true label\nfor the input, and ∥·∥2 denotes the Euclidean (L2) norm. The EL2N score provides\ninsight into the model’s performance on individual data points, allowing for a more\ntargeted analysis of errors and potential improvements.\nThe GraNd and EL2N scores are proposed in the context of dataset pruning, where\nthe goal is to remove less informative samples from the training data. Thus, one can\ncreate a smaller, more efficient dataset that maintains the model’s overall performance\nwhile reducing training time and computational resources.\nWhile GraNd at initialization does not require model training, it requires a model\nand is not cheap to compute. In contrast, the input norm of training samples is\nincredibly cheap to compute and would thus provide an exciting new baseline to\nuse for data pruning experiments. We investigate this correlation in this chapter\nand find positive evidence for it. However, we also find that the GraNd score at\ninitialization does not outperform random pruning, unlike the respective results of\nPaul et al. [2021] for GraNd at initialization.\nOutline. In §B.1.1.1, we begin by discussing the correlation between input norm\nand gradient norm at initialization. We empirically find strong correlation between\nGraNd scores at initialization and input norms as we average over models. In §B.1.1.2,\nwe explore the implication of this insight for dataset pruning and find that both GraNd\nat initialization and input norm scores do not outperform random pruning, but GraNd\nscores after a few epochs perform similar to EL2N scores at these later epochs.\nIn summary, this reproduction contributes a new insight on the relationship between\ninput norm and gradient norm at initialization and finds a failure to reproduce one\nof the six contributions of Paul et al. [2021].\nB.1.1\nInvestigation\nWe investigate the correlation between input norm and GraNd at initialization and\nthe other scores on CIFAR-10 [Krizhevsky, 2009] in three different ways: First, we\nupdate the original paper repository2 (https://github.com/mansheej/data_diet),\nwhich uses JAX [Bradbury et al., 2018], rerun the experiments for Figure 1 (second\nrow) in Paul et al. [2021] for CIFAR-10, which trains for 200 epochs, using GraNd at\ninitialization, GraNd at epoch 20, E2LN at epoch 20, Forget Score at epoch 200, and\ninput norm. Second, we reproduce the same experiments using ‘hlb’ [Balsam, 2023],\nwhich is a strongly modified version of ResNet-18 that allows to train to high accuracy\nin 12 epochs taking about 30 seconds total on an Nvidia RTX 4090 in PyTorch [Paszke\net al., 2019] For the latter, we compare GraNd at initialization, GraNd at epoch 1\n(≈20/200 · 12 epochs), EL2N at epoch 1, and input norm3. Third, we compare the\nrank correlations between the different scores for those two repositories and also use\nanother ‘minimal’ CIFAR-10 implementation [van Amersfoort, 2021] with a standard\nResNet18 architecture for CIFAR-10 to compare the rank correlations.\n2https://github.com/blackhc/data_diet\n3https://github.com/blackhc/pytorch_datadiet\nB. Reproducibility Analysis\n215\n(a) Original Repo (10 trials)\n(b) Hlb (10 trials)\n(c) Minimal (120 trials)\n(d) Minimal (120 trials)\nFigure B.1: Correlation between GraNd at Initialization and Input Norm for CIFAR-\n10’s training set. (a), (b), (c): We sort the samples by their average normalized score\n(i.e., the score minus its minimum divided by its range), plot the scores and compute\nSpearman’s rank correlation on CIFAR-10’s training data. The original repository and\nthe ‘minimal’ implementation have very high rank correlation—‘hlb’ has a lower but still\nstrong rank correlation. (d): Ratio between input norm and gradient norm. In the ‘minimal’\nimplementation, the ratio between input norm and gradient norm is roughly log-normal\ndistributed.\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nPruned Fraction\n0.91\n0.92\n0.93\n0.94\n0.95\nTest Accuracy\nMetric\nEL2N at Epoch 20\nGraNd at Epoch 20\nRandom\nInput Norm\nGraNd at Init\n(a) Original Repo (2 trials each)\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nPruned Fraction\n0.88\n0.89\n0.90\n0.91\n0.92\n0.93\n0.94\nTest Accuracy\nMetric\nEL2N at Epoch 1\nGraNd at Epoch 1\nRandom\nInput Norm\nGraNd at Init\n(b) Hlb (10 trials)\nFigure B.2: Reproduction of Figure 1 (second row) from Paul et al. [2021].\nIn both\nreproductions, GraNd at initialization performs as well as the input norm. However, it does\nnot perform better than random pruning. Importantly, it also fails to reproduce the results\nfrom Paul et al. [2021]. However, GraNd at epoch 20 (respectively at epoch 1 for ‘hlb’)\nperforms similar to EL2N and like GraNd at initialization in Paul et al. [2021].\nB. Reproducibility Analysis\n216\nB.1.1.1\nCorrelation between GraNd at Initialization and Input Norm\nTo better understand the relationship between the input norm and the gradient norm at\ninitialization, let us consider a toy example first and then appeal to empirical evidence\nas is common in deep learning research: let’s examine linear softmax classification\nwith C classes (without a bias term). The model takes the form:\nf(x) = softmax(Wx),\n(B.3)\ntogether with the cross-entropy loss function:\nL = −log f(x)y.\n(B.4)\nThe gradient of the loss function with respect to the rows wj of the weight matrix W is:\n∇wjL = (f(x)j −1{j = y})x\n(B.5)\nwhere 1{j = y} is the indicator function that is 1 if j = y and 0 otherwise. The\nsquared norm of the gradient is:\n∥∇wL∥2\n2 =\nC\nX\nj=1\n(f(x)j −1{j = y})2∥x∥2\n2.\n(B.6)\nIn expectation over W (different initializations), the norm of the gradient is:\nEW [∥∇wL∥2] = EW\n\n\n\n\nC\nX\nj=1\n(f(x)j −1{j = y})2\n\n\n1/2\n∥x∥2.\n(B.7)\nThus, we see that the gradient norm is a multiple of the input norm. The factor\ndepends on f(x)j, which we could typically expect to be 1/C.\nEmpirical Evidence. In Figure B.1, we see that on CIFAR-10’s training set,\nGraNd at initialization and the input norm are highly correlated.\nThis is true\nfor the original repository, the ‘hlb’ and the ‘minimal’ implementation. The ‘hlb’\nimplementation has a lower but still strong correlation.\nB.1.1.2\nReproducing Figure 1 of Paul et al. [2021] on CIFAR-10\nIn Figure B.2, we see that GraNd at initialization performs about as well as using\nthe input norm. However, it does not reproduce the results from Paul et al. [2021].\nIt performs worse than random pruning (for ‘hlb’). However, GraNd at epoch 20\n(respectively at epoch 1 for ‘hlb’) performs like GraNd at initialization in Paul et al.\n[2021]. Similarly, in Figure B.3, we see that GraNd at initialization and the input\nnorm are strongly correlated as are GraNd at later epochs, EL2N and the Forget\nScore, with little correlation between these two groups.\nB.1.2\nDiscussion\nIf GraNd at initialization performed as well as claimed in Paul et al. [2021], using\nthe input norm would provide a new exciting baseline for data pruning because it is\nmodel independent and cheaper to compute than GraNd or other scores. However,\nsince only GraNd at later epochs seems to perform as expected, we cannot recommend\nusing input norm or GraNd at initialization for data pruning.\nB. Reproducibility Analysis\n217\nInput Norm\nGraNd at Init\nGraNd (Epoch 20) EL2N (Epoch 20)\nForget Score\nInput Norm\nGraNd at Init\nGraNd (Epoch 20)\nEL2N (Epoch 20)\nForget Score\n1\n0.96\n-0.013\n-0.015\n-0.02\n1\n-0.024\n-0.022\n-0.023\n1\n0.99\n0.87\n1\n0.89\n1\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(a) Original Repo (2 trials each)\nInput Norm\nGraNd at Init\nGraNd at Epoch 1 EL2N at Epoch 1\nInput Norm\nGraNd at Init\nGraNd at Epoch 1\nEL2N at Epoch 1\n1\n0.39\n-0.039\n-0.067\n1\n-0.068\n-0.1\n1\n0.94\n1\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(b) Hlb (10 trials)\nFigure B.3: Rank Correlations of the Scores. Cf. Figure 12 in the appendix of Paul\net al. [2021]. In both reproductions, GraNd at initialization and input norm are positively\ncorrelated, while GraNd and EL2N at later epochs are strongly correlated with each other\nand the Forget Score (at epoch 200).\nAs to the failure to reproduce the results of Paul et al. [2021], we could not rerun\nthe code using the original JAX version because it is too old for our GPU. The\nauthors of Paul et al. [2021] were, however, able to set up a Google Cloud VM with\nan old image that was able to reproduce the original results using the original JAX\nversion. On further investigation, the author of this reproduction found a bug in\nflax.training.restore_checkpoint that was fixed in April 20214: passing a 0 step\n(i.e. initialization) would trigger loading the latest checkpoint instead of the zero-\nth checkpoint because the internal implementation was checking if step: instead\nof if step is not None: when deciding whether to fall back to loading the latest\ncheckpoint. This bug was fixed in April 2021, but the authors of Paul et al. [2021] were\nnot aware of this bug and did not rerun their experiments with newer JAX/FLAX\nversions. We have accordingly informed the authors of Paul et al. [2021].\nB.1.3\nDetails\nB.2\nA Note on “Assessing Generalization of SGD\nvia Disagreement”\nMachine learning models can cause harm when their predictions become unreliable, yet\nwe trust them blindly. This is not fiction but has happened in real-world applications\nof machine learning [Schneider, 2021]. Thus, there has been significant research interest\nin model robustness, uncertainty quantification, and bias mitigation. In particular,\nfinding ways to bound the test error of a trained deep neural network without access to\nthe labels would be of great importance: one could estimate the performance of models\nin the wild where unlabeled data is ubiquitous, labeling is expensive, and the data\noften does not match the training distribution. Crucially, it would provide a signal on\nwhen to trust the output of a model and when to defer to human experts instead.\nSeveral recent works [Chen et al., 2021; Granese et al., 2021; Garg et al., 2022;\nJiang et al., 2022] look at the question of how model predictions in a non-Bayesian\nsetting can be used to estimate model accuracy. In this chapter, we focus on one\n4See https://github.com/google/flax/commit/28fbd95500f4bf2f9924d2560062fa50e919b1a5.\nB. Reproducibility Analysis\n218\n(a) Minimal (1000 trials)\n(b) Minimal (1000 trials)\nFigure B.4: Correlation between GraNd at Initialization and Input Norm on the Test\nSet. ((a)): We sort the samples by their average normalized score (i.e., the score minus its\nminimum divided by its range), plot the scores and compute Spearman’s rank correlation\non CIFAR-10’s test data. The original repository and the ‘minimal’ implementation have\nvery high rank correlation—‘hlb’ has a lower but still strong rank correlation. ((b)): Ratio\nbetween input norm and gradient norm. In the ‘minimal’ implementation, the ratio between\ninput norm and gradient norm is roughly log-normal distributed\nwork5 [Jiang et al., 2022] specifically, and examine the theoretical and empirical\nresults from a Bayesian perspective.\nAs we have already examined in §1.2.3 and §3, epistemic uncertainty [Der Kiureghian\nand Ditlevsen, 2009] captures the uncertainty of a model about the reliability of its\npredictions, that is epistemic uncertainty quantifies the uncertainty of a model about\nits predictive distribution, while aleatoric uncertainty quantifies the ambiguity within\nthe predictive distribution and the label noise, c.f. Kendall and Gal [2017]. Epistemic\nuncertainty thus tells us whether we can trust a model’s predictions or not. Assuming\na well-specified and well-calibrated Bayesian model, when its predictive distribution\nhas low epistemic uncertainty for an input, it can be trusted. But likewise, a Bayesian\nmodel’s calibration ought to deteriorate as epistemic uncertainty increases for a sample:\nin that case, the predictions become less reliable and so does the model’s calibration.\nIn this context, calibration is an aleatoric metric for a model’s reliability [Gopal,\n2021]. Calibration captures how well a model’s confidence for a given prediction matches\nthe actual frequency of that prediction in the limit of observations in distribution:\nwhen a model is 70% confident about assigning label A, does A indeed occur with\n70% probability in these instances?\nJiang et al. [2022], while not Bayesian, make the very interesting empirical and\ntheoretical discovery that deep ensembles satisfy a ‘Generalization Disagreement\nEquality’ when they are well-calibrated according to a proposed ‘class-aggregated\ncalibration’ (or a ‘class-wise calibration’) and empirically find that the respective\ncalibration error generally bounds the absolute difference between the test error and\n‘disagreement rate.’ Jiang et al. [2022]’s theory builds upon Nakkiran and Bansal\n[2020]’s ‘Agreement Property’ and provides backing for an empirical connection between\nthe test error and disagreement rate of two separately trained networks on the same\ntraining data. Yet, while Nakkiran and Bansal [2020] limit the applicability of their\n5An ICLR 2022 Spotlight, which has spawned additional follow-up works, e.g. Baek et al. [2022].\nB. Reproducibility Analysis\n219\nAgreement Property to in-distribution data, Jiang et al. [2022] carefully extend it:\n‘our theory is general and makes no restrictions on the hypothesis class, the algorithm,\nthe source of stochasticity, or the test distributions (which may be different from the\ntraining distribution)’ with qualified evidence: ‘we present preliminary observations\nshowing that GDE is approximately satisfied even for certain distribution shifts within\nthe PACS [Li et al., 2017] dataset.’\nIn this chapter, we present a new perspective on the theoretical results using a\nstandard probabilistic approach for discriminative (Bayesian) models, whereas Jiang\net al. [2022] use a hypothesis space of models that output one-hot predictions. Indeed,\ntheir theory does not require one-hot predictions, separately trained models (deep\nensembles) or Bayesian models. Moreover, as remarked by the authors, their theoretical\nresults also apply to a single model that outputs softmax probabilities. We will see\nthat our perspective greatly simplifies the results and proofs.\nThis also means that the employed notion of disagreement rate does not capture\nepistemic uncertainty but overall uncertainty, similar to the predictive entropy, which\nis a major difference to Bayesian approaches which can evaluate epistemic uncertainty\nseparately [Smith and Gal, 2018]. Overall uncertainty is the sum of aleatoric and\nepistemic uncertainty. For in-distribution data, epistemic uncertainty will generally\nbe low: overall uncertainty will mainly capture aleatoric uncertainty and align well\nwith (aleatoric) calibration measures. However, under distribution shift, epistemic\nuncertainty can be a confounding factor.\nImportantly, we find that the connection between the proposed calibration met-\nrics and the gap between test error and disagreement rate exists because the in-\ntroduced notion of class-aggregated calibration is so strong that this connection\nfollows almost at once.\nMoreover, the suggested approach is circular6: calibration must be measured\non the data distribution we want to evaluate.\nOtherwise, we cannot bound the\ndifference between the test error and the disagreement rate and obtain a signal on\nhow trustworthy our model is. This reintroduces the need for labels on the unlabeled\ndataset, limiting practicality. Alternatively, one would have to assume that these\ncalibration metrics do not change for different datasets or under distribution shifts,\nwhich we show not to hold: deep ensembles are less calibrated the more the ensemble\nmembers disagree (even on in-distribution data).\nLastly, we draw connections and show that the ‘class-aggregated calibration error’\nand the ‘class-wise calibration error’7 are equivalent to the ‘adaptive calibration error’\nand ‘static calibration error’ introduced in Nixon et al. [2019] and its implementation.\nOutline.\nWe introduce the necessary background and notation in §B.2.1. In §B.2.2 we rephrase\nthe theoretical statements from Jiang et al. [2022] using a parameter distribution\n(instead of a version space) and auxiliary random variables. This allows us to simplify\nthe theoretical statements and proofs greatly in §B.2.3 and to examine the connection\nto Nixon et al. [2019]. Finally, in §B.2.4, we provide empirical evidence that deep\nensembles are less calibrated exactly when their ensemble members disagree.\n6This was added as a caveat to the camera-ready version of Jiang et al. [2022] after reviewing a\npreprint of the preprint of the paper this chapter is based on.\n7Which is not explicitly introduced in Jiang et al. [2022] but can be analogously constructed.\nB. Reproducibility Analysis\n220\nB.2.1\nBackground & Setting\nIn this section, we introduce additional notation, the initial Bayesian formalism, the\nconnection to deep ensembles, and the probabilistic model. We restate the statements\nfrom Jiang et al. [2022] using this formalism in §B.2.2.\nNotation. We use an implicit notation for expectations E[f(X)] when possible.\nFor additional clarity, we also use EX[f(X)] and Ep(x) f(x), which fix the random\nvariables and distribution, respectively, when needed.\nWe will use nested probabilistic expressions of the form E[p(ˆY = Y | X)]. Prima\nfacie, this seems unambiguous, but is p(ˆY = Y | X) a transformed random variable of\nonly X or also of Y (and ˆY ): what are we taking the expectation over? This is not\nalways unambiguous, so we disambiguate between the probability for an event defined\nby an expression P[. . .] = E[1{. . .}], where 1{. . .} is the indicator function8, and a\nprobability given specific outcomes for various random variables p(ˆy | x), c.f.:\nP[ ˆY = Y | X] = E ˆY,Y [1{ˆY = Y } | X] = Ep(ˆy,y|X) 1{ˆy = y},\n(B.8)\nwhich is a transformed random variable of X, while p(ˆY = Y | X) is simply a\n(transformed) random variable, applying the probability density on the random variables\nY and X. Put differently, Y is bound within the former but not the latter: P[. . . |\nX] is a transformed random variable of X, and any random variable that appears\nwithin the . . . is bound within that expression.\nProbabilistic Model. We assume classification with C classes. For inputs X\nwith ground-truth labels Y , we have a Bayesian model with parameters Ωthat\nmakes predictions ˆY :\np(y, ˆy, ω | x) = p(y | x) p(ˆy | x, ω) p(ω).\n(B.9)\nWe focus on model evaluation. (Input) samples x can come either from ‘in-distribution\ndata’ which follows the training set or from samples under covariate shift (distribu-\ntion shift).\nThe expected prediction over the model parameters is the marginal\npredictive distribution:\np(ˆy | x) = EΩ[p(ˆy | x, Ω)].\n(B.10)\nOn p(ω). The main emphasis in Bayesian modelling can be Bayesian inference\nor Bayesian model averaging [Wilson and Izmailov, 2020]. Here we concentrate on\nthe model averaging perspective, and for simplicity take the model averaging to\nbe with respect to some distribution p(ω). Hence, we will use p(ω) as the push-\nforward of models initialized with different initial seeds through SGD to minimize\nthe negative log likelihood with weight decay and a specific learning rate schedule\n(MLE or MAP)—the same we also did in §3:\nAssumption B.1. We assume that p(ω) is a distribution of possible models we obtain\nby training with a specific training regime on the training data with different seeds. A\nsingle ω identifies a single trained model.\nWe cast deep ensembles [Hansen and Salamon, 1990; Lakshminarayanan et al.,\n2017], which refer to training multiple models and averaging predictions, into the\n8The indicator function is 1 when the predicate ‘. . .’ is true and 0 otherwise.\nB. Reproducibility Analysis\n221\nintroduced Bayesian perspective above by viewing them as an empirical finite sample\nestimate of the parameter distribution p(ω). Then, ω1, . . . , ωN ∼p(ω) drawn i.i.d.\nare the ensemble members.\nAgain, the implicit model parameter distribution p(w) is given by the models\nthat are obtained through training. Hence, we can view the predictions of a deep\nensemble or the ensemble’s prediction disagreement for specific x (or over the data)\nas empirical estimates of the predictions or the model disagreement using the implicit\nmodel distribution, respectively.\nCalibration. A model’s calibration for a given x measures how well the model’s\ntop-1 (argmax) confidence\nConfTop1 ≜p(ˆY = arg max\nk\np(ˆY = k | X) | X)\n(B.11)\nmatches its top-1 accuracy\nAccTop1 ≜p(Y = arg max\nk\np(ˆY = k | X) | X),\n(B.12)\nwhere we define both as transformed random variables of X. The calibration error\nis usually defined as the absolute difference between the two:\nCE ≜|AccTop1 −ConfTop1|.\n(B.13)\nIn general, we are interested in the expected calibration error (ECE) over the data\ndistribution [Guo et al., 2017] where we bin samples by their top-1 confidence. Intuitively,\nthe ECE will be low when we can trust the model’s top-1 confidence on the given\ndata distribution.\nWe usually use top-1 predictions in machine learning. However, if we were to draw\nˆY according to p(ˆy | x) instead, the (expected) accuracy would be:\nAcc ≜P[Y = ˆY | X]\n(B.14)\n=\nX\nk\np(Y = k | X) p(ˆY = k | X)\n(B.15)\n= EY [p(ˆY = Y | X) | X],\n(B.16)\nas a random variable of X. Usually we are interested in the accuracy over the whole\ndataset:\nP[ ˆY = Y ] = E[Acc] = EX[P[ ˆY = Y | X]] = EX,Y [p(ˆY = Y | X)].\n(B.17)\nFor example, for binary classification with two classes A and B, if class A appears\nwith probability 0.7 and a model predicts class A with probability 0.2 (and thus\nclass B appears with probability 0.3, which a model predicts as 0.8), its accuracy is\n0.7 × 0.2 + 0.3 × 0.8 = 0.38, while the top-1 accuracy is 0.3. Likewise, the predicted\naccuracy is 0.22 + 0.82 = 0.68 while the top-1 predicted accuracy is 0.8.\nB.2.2\nRephrasing Jiang et al. [2022] in a Probabilistic Context\nWe present the same theoretical results as Jiang et al. [2022] but use a Bayesian formu-\nlation instead of a hypothesis space and define the relevant quantities as (transformed)\nrandom variables. As such, our definitions and theorems are equivalent and follow the\nB. Reproducibility Analysis\n222\npaper but look different. We show these equivalences in §B.2.7.1 in the appendix and\nprove the theorems and statements themselves in the next section.\nFirst, however, we note a distinctive property of Jiang et al. [2022]. It is assumed\nthat each p(ˆy | x, ω) is always one-hot for any ω. In practice, this could be achieved\nby turning a neural network’s softmax probabilities into a one-hot prediction for the\narg max class. We call this the Top1-Output-Property (TOP).\nAssumption B.2. The Bayesian model p(ˆy, ω | x) satisfies TOP: p(ˆy | x, ω) is one-hot\nfor all x and ω.\nDefinition B.1. The test error and disagreement rate, as transformed random variables\nof Ω(and Ω′), are:\nTestError ≜P[ ˆY ̸= Y | Ω]\n(B.18)\n\u0010\n= 1 −P[ ˆY = Y | Ω]\n(B.19)\n= 1 −EX,Y [p(ˆY = Y | X, Ω)],\n(B.20)\n= 1 −Ep(x,y) p(ˆY = y | x, Ω))\n\u0011\n,\n(B.21)\nDis ≜P[ ˆY ̸= ˆY ′ | Ω, Ω′]\n(B.22)\n\u0010\n= 1 −P[ ˆY = ˆY ′ | Ω, Ω′]\n(B.23)\n= 1 −EX, ˆY [p(ˆY ′ = ˆY | X, Ω′) | Ω, Ω′]\n(B.24)\n= 1 −Ep(x,ˆy|Ω) p(ˆY ′ = ˆy | x, Ω′)\n\u0011\n,\n(B.25)\nwhere for the disagreement rate, we expand our probabilistic model to take a second\nmodel Ω′ with prediction ˆY ′ into account (and which uses the same parameter\ndistribution), so:\np(y, ˆy, ω, ˆy′, ω′ | x) ≜p(y | x) p(ˆy | x, ω) p(ω) p(ˆY = ˆy′ | x, Ω= ω′) p(Ω= ω′).\nJiang et al. [2022] then introduce the property of interest:\nDefinition B.2. A Bayesian model p(ˆy, ω | x) fulfills the Generalization Disagreement\nEquality (GDE) when:\nEΩ[TestError(Ω)] = EΩ,Ω′[Dis(Ω, Ω′)]\n(⇔E[TestError] = E[Dis]).\n(B.26)\nWhen this property holds, we seemingly do not require knowledge of the labels to esti-\nmate the (expected) test error: computing the (expected) disagreement rate is sufficient.\nTwo different types of calibration are then introduced, class-wise and class-aggregated\ncalibration, and it is shown that they imply the GDE:\nDefinition B.3. The Bayesian model p(ˆy, ω | x) satisfies class-wise calibration when\nfor any q ∈[0, 1] and any class k ∈[C]:\np(Y = k | p(ˆY = k | X) = q) = q.\n(B.27)\nSimilarly, the Bayesian model p(ˆy, ω | x) satisfies class-aggregated calibration when for\nany q ∈[0, 1]:\nX\nk\np(Y = k, p(ˆY = k | X) = q) = q\nX\nk\np(p(ˆY = k | X) = q).\n(B.28)\nB. Reproducibility Analysis\n223\nTheorem B.1. When the Bayesian model p(ˆy, ω | x) satisfies class-wise or class-\naggregated calibration, it also satisfies GDE.\nFinally, Jiang et al. [2022] introduce the class-aggregated calibration error similar\nto the ECE and then use it to bound the magnitude of any GDE gap:\nDefinition B.4. The class-aggregated calibration error (CACE) is the integral of the\nabsolute difference of the two sides in Equation B.28 over possible q ∈[0, 1]:\nCACE ≜\nZ\nq∈[0,1]\n\f\f\f\nX\nk\np(Y = k, p(ˆY = k | X) = q) −q\nX\nk\np(p(ˆY = k | X) = q)\n\f\f\fdq.\n(B.29)\nTheorem B.2. For any Bayesian model p(ˆy, ω | x), we have:\n|E[TestError] −E[Dis]| ≤CACE.\nIn the following section, we simplify the definitions and prove the statements\nusing elementary probability theory, showing that notational complexity is the main\nsource of complexity.\nB.2.3\nGDE is Class-Aggregated Calibration in Expectation\nWe show that proof for Theorem B.2 is trivial if we use different but equivalent\ndefinitions of the class-wise and class-aggregate calibration. First though, we estab-\nlish a better understanding for these definitions by examining the GDE property\nE[TestError] = E[Dis]. For this, we expand the definitions of E[TestError] and E[Dis],\nand use random variables to our advantage.\nWe define a quantity which will be of intuitive use later on: the predicted accuracy\nPredAcc ≜E ˆY [p(ˆY | X) | X] =\nX\nk\np(ˆY = k | X) p(ˆY = k | X),\n(B.30)\nas a random variable of X. It measures the expected accuracy assuming the model’s\npredictions are correct, that is the true labels follow p(ˆy | x). This also assumes that\nwe draw ˆY accordingly and do not always use the top-1 prediction.\nRevisiting GDE. On the one hand, we have:\nE[TestError] = EΩ[P[ ˆY ̸= Y | Ω]]\n(B.31)\n= 1 −P[ ˆY = Y ]\n(B.32)\n= 1 −EX, ˆY [p(Y = ˆY | X)]\n(B.33)\n= 1 −E[Acc]\n(B.34)\nand on the other hand, we have:\nE[Dis] = EΩ,Ω′[P[ ˆY ̸= ˆY ′ | Ω, Ω′]]\n(B.35)\n= 1 −EΩ,Ω′[P[ ˆY = ˆY ′ | Ω, Ω′]]\n(B.36)\n= 1 −P[ ˆY = ˆY ′]\n(B.37)\n= 1 −EX, ˆY [p(ˆY ′ = ˆY | X)]\n(B.38)\nB. Reproducibility Analysis\n224\n= 1 −EX, ˆY [p(ˆY | X)]\n(B.39)\n= 1 −E[PredAcc].\n(B.40)\nThe step from (B.37) to (B.38) is valid because ˆY ⊥⊥ˆY ′ | X, and the step from (B.38)\nto (B.39) is valid because p(ˆy′ | x) = p(ˆy | x). Thus, we can rewrite Theorem B.1 as:\nLemma B.3. The model p(ˆy | x) satisfies GDE, when\nE[Acc] = E[p(Y = ˆY | X)] = E[p(ˆY | X)] = E[PredAcc],\n(B.41)\ni.e. in expectation, the accuracy of the model equals the predicted accuracy of the\nmodel, or equivalently, the error of the model equals its predicted error.\nCrucially, while Jiang et al. [2022] calls 1−EX, ˆY [p(ˆY |X)] the (expected) disagreement\nrate E[Dis], it actually is just the predicted error of the (Bayesian) model as a whole.\nEqually important, all dependencies on Ωhave vanished. Indeed, we will not\nuse Ωanymore for the remainder of this section. This reproduces the corresponding\nremark from Jiang et al. [2022]9:\nInsight B.1. The theoretical statements in Jiang et al. [2022] can be made about any\ndiscriminative model with predictions p(y | x).\nWhen is EX, ˆY [p(Y = ˆY | X)] = EX, ˆY [p(ˆY | X)]? Or in other words: when does\np(Y = ˆy | x) equal p(ˆY = ˆy | x) in expectation over p(x, y, ˆy)?\nAs a trivial sufficient condition, when the predictive distribution matches our\ndata distribution—i.e. when the model p(ˆy | x) is perfectly calibrated on average for\nall classes—and not only for the top-1 predicted class. ECE = 0 is not sufficient\nbecause the standard calibration error only ensures that the data distribution and\npredictive distribution match for the top-1 predicted class [Nixon et al., 2019]. But\nclass-wise calibration entails this equality.\nClass-Wise and Class-Aggregated Calibration. To see this, we rewrite class-\nwise and class-aggregated calibration slightly by employing the following tautology:\np(ˆY = k | p(ˆY = k | X) = q) = q,\n(B.42)\nwhich is obviously true due its self-referential nature. We provide a formal proof in\n§B.2.7.4 in the appendix. Then we have the following equivalent definition:\nLemma B.4. The model p(ˆy | x) satisfies class-wise calibration when for any q ∈[0, 1]\nand any class k ∈[C]:\np(Y = k, p(ˆY = k | X) = q) = p(ˆY = k, p(ˆY = k | X) = q).\n(B.43)\nSimilarly, the model p(ˆy|x) satisfies class-aggregated calibration when for any q ∈[0, 1]:\np(p(ˆY = Y | X) = q) = p(p(ˆY | X) = q),\n(B.44)\nand class-wise calibration implies class-aggregate calibration.\n9The remark did not exist in the first preprint version.\nB. Reproducibility Analysis\n225\nThe straightforward proof is found in §B.2.7.4 in the appendix.\nJiang et al. [2022] mention ‘level sets’ as intuition in their proof sketch. Here, we\nhave been able to make this even clearer: class-aggregated calibration means that level-\nsets for accuracy p(ˆY = Y |X) and predicted accuracy p(ˆY |X)—as random variables of\nY and X, and ˆY and X, respectively—have equal measure, that is probability, for all q.\nGDE. Now, class-aggregated calibration immediately and trivially implies GDE.\nTo see this, we use the following property of expectations:\nLemma B.5. For a random variable X, a function t(x), and the random variable\nT = t(X), it holds that\nET[T] = E[T] = EX[t(X)].\n(B.45)\nThis basic property states that we can either compute an expectation over T\nby integrating over p(T = t) or by integrating t(x) over p(X = x). This is just\na change of variable (push-forward of a measure).\nWe can use this property together with the class-aggregated calibration to see:\nE[Acc]\n=\nEX,Y [p(ˆY = Y | X)]\n=\nE[p(ˆY = Y | X)]\n=\nEq∼p( ˆY =Y |X)[q]\n=\nE[PredAcc]\n=\nEX, ˆY [p(ˆY | X)]\n=\nE[p(ˆY | X)]\n=\nEq∼p( ˆY |X)[q] ,\n(B.46)\nwhich is exactly Lemma B.3, where we start with the equality following from class-\naggregated calibration and then apply Lemma B.5 along each side. Thus, GDE is\nbut an expectation over class-aggregated calibration; we have:\nTheorem B.6. When a model p(ˆy | x) satisfies class-wise or class-aggregated\ncalibration, it satisfies GDE.\nProof. We can formalize the proof to be even more explicit and introduce two auxiliary\nrandom variables:\nS ≜p(ˆY = Y | X),\n(B.47)\nas a transformed random variable of Y and X, and\nT ≜p(ˆY | X),\n(B.48)\nas a transformed random variable of ˆY and X. Class-wise calibration implies class-\naggregated calibration. Class-aggregated calibration then is p(S = q) = p(T = q) (*).\nWriting out Equation B.46, we have\nE[p(ˆY = Y | X)] = EX,Y [S] = E[S] = ES[S]\n(B.49)\n=\nZ\np(S = q) q dq\n(B.50)\nB. Reproducibility Analysis\n226\n(∗)\n=\nZ\np(T = q) q dq\n(B.51)\n= ET[T] = E[T] = EX, ˆY [T] = E[p(ˆY | X)],\n(B.52)\nwhich concludes the proof.\nThe reader is invited to compare this derivation to the corresponding longer proof\nin the appendix of Jiang et al. [2022]. The fully probabilistic perspective greatly\nsimplifies the results, and the proofs are straightforward.\nCACE. Showing that CACE bounds the gap between test error and disagreement\nis also straightforward:\nTheorem B.7. For any model p(ˆy | x), we have:\n|E[TestError] −E[Dis]| ≤CACE.\nProof. First, we note that\nCACE =\nZ\nq∈[0,1]\n\f\f\f p(p(ˆY = Y | X) = q) −p(p(ˆY | X) = q)\n\f\f\fdq,\n(B.53)\nfollowing the equivalences in the proof of Lemma B.4. Then using the triangle inequality\nfor integrals and 0 ≤q ≤1, we obtain:\nCACE\n(B.54)\n=\nZ\nq∈[0,1]\n\f\f\f p(p(ˆY = Y | X) = q) −p(p(ˆY = ˆY | X) = q)\n\f\f\f dq\n(B.55)\n≥\nZ\nq∈[0,1] q\n\f\f\f p(p(ˆY = Y | X) = q) −p(p(ˆY = ˆY | X) = q)\n\f\f\f dq\n(B.56)\n≥\n\f\f\f\nZ\nq∈[0,1] q p(p(ˆY = Y | X) = q) dq −\nZ\nq∈[0,1] q p(p(ˆY | X) = q) dq\n\f\f\f.\n(B.57)\n=\n\f\f\fE[S] −E[T]\n\f\f\f\n(B.58)\n=\n\f\f\fE[TestError] −E[Dis]\n\f\f\f,\n(B.59)\nwhere we have used the monotonicity of integration in (B.56) and the triangle inequality\nin (B.57).\nThe bound also serves as another—even simpler—proof for Theorem B.6:\nInsight B.2. When the Bayesian model satisfies class-wise or class-aggregated\ncalibration, we have CACE = 0 and thus E[TestError] = E[Dis], i.e. the model\nsatisfies GDE.\nFurthermore, note again that a Bayesian model was not necessary for the last\ntwo theorems. The model parameters Ωwere not mentioned—except for the specific\ndefinitions of TestError and Dis which depend on Ωfollowing Jiang et al. [2022] but\nwhich we only use in expectation.\nMoreover, we see that we can easily upper-bound CACE using the triangle in-\nequality by 2, narrowing the statement in Jiang et al. [2022] that CACE can lie\nanywhere in [0, C]:\nB. Reproducibility Analysis\n227\nInsight B.3. CACE ≤2.\nAdditionally, for completeness, we can also define the class-wise calibration error\nformally and show that it is bounded by CACE using the triangle inequality:\nDefinition B.5. The class-wise calibration error (CWCE) is defined as:\nCWCE ≜\nX\nk\nZ\nq∈[0,1]\n\f\f\f p(Y = k, p(ˆY = k | X) = q) −p(ˆY = k, p(ˆY = k | X) = q)\n\f\f\f.\n(B.60)\nLemma B.8. CWCE ≥CACE ≥| E[Acc] −E[PredAcc]|.\nNote that when we compute CACE empirically, we divide the dataset into several\nbins for different intervals of p(ˆY = k | X). Jiang et al. [2022] use 15 bins. If we were\nto use a single bin, we would compute | E[Acc] −E[PredAcc]| directly.\nIn §B.2.7.2 we show that CWCE has previously been introduced as ‘adaptive\ncalibration error’ in Nixon et al. [2019] and CACE as ‘static calibration error’\n(with noteworthy differences between Nixon et al. [2019] and its implementation).\nB.2.4\nDeterioration of Calibration under Increasing Disagree-\nment\nGenerally, we can only hope to trust model calibration for in-distribution data,\nwhile under distribution shift, the calibration ought to deteriorate. In our empirical\nfalsification using models trained on CIFAR-10 and evaluated on the test sets of\nCIFAR-10 and CINIC-10, as a dataset with a distribution shift, we find in both\ncases that calibration deteriorates under increasing disagreement. We further examine\nImageNET and PACS in §B.2.7.5. Most importantly though, calibration markedly\nworsens under distribution shift.\nSpecifically, we examine an ensemble of 25 WideResNet models [Zagoruyko and\nKomodakis, 2016] trained on CIFAR-10 [Krizhevsky, 2009] and evaluated on CIFAR-10\nand CINIC-10 test data. CINIC-10 [Darlow et al., 2018] consists of CIFAR-10 and down-\nscaled ImageNet samples for the same classes, and thus includes a distribution shift. The\ntraining setup follows the one described in Mukhoti et al. [2023], see appendix §B.2.7.5.\nFigure B.5 shows rejection plots under increasing disagreement for in-distribution\ndata (CIFAR-10) and under distribution shift (CINIC-10). The rejection plots threshold\nthe dataset on increasing levels of the predicted error (disagreement rate)—which is a\nmeasure of epistemic uncertainty when there is no expected aleatoric uncertainty in\nthe dataset. We examine ECE, class-aggregated calibration error (CACE), class-wise\ncalibration error (CWCE), error E[TestError], and ‘GDE gap’, | E[Acc] −E[PredAcc]|,\nas the predicted error (disagreement rate), E[Dis] = 1 −E[PredAcc], increases. We\nobserve that all calibration metrics, ECE, CACE and CWCE, deteriorate under\nincreasing disagreement, both in distribution and under distribution shift, and also\nworsen under distribution shift overall.\nWe also observe the same for ImageNet [Deng et al., 2009] and PACS [Li et al.,\n2017], which we show in appendix §B.2.7.5.\nB. Reproducibility Analysis\n228\nMeasure\nCWCE\nCACE\nTest Error\nECE\n| Test Error - Predicted Error |\nCIFAR-10 (In-Distribution)\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nPredicted Error (Disagreement Rate)\n0.01\n0.02\n0.03\n0.04\nValue\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nPredicted Error (Disagreement Rate)\nCINIC-10 (Distribution Shift)\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nPredicted Error (Disagreement Rate)\n0.05\n0.10\n0.15\n0.20\n0.25\nValue\n(a) Ensemble with TOP\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nPredicted Error (Disagreement Rate)\n(b) Ensemble without TOP (same underlying\nmodels)\nFigure B.5:\nRejection Plot of Calibration Metrics for Increasing Disagreement In-\nDistribution (CIFAR-10) and Under Distribution Shift (CINIC-10). Different calibration\nmetrics (ECE, CWCE, CACE) vary across CIFAR-10 and CINIC-10 on an ensemble of 25\nWide-ResNet-28-10 model trained on CIFAR-10, depending on the rejection threshold of\nthe predicted error (disagreement rate). Thus, calibration cannot be assumed constant for\nin-distribution data or under distribution shift. The test error increases almost linearly with\nthe predicted error (disagreement rate), leading to ‘GDE gap’ |Test Error −Predicted Error|\nbecoming almost flat, providing evidence for the empirical observations in Nakkiran and\nBansal [2020]; Jiang et al. [2022]. The mean predicted error (disagreement rate) is shown on\nthe x-axis. (a) shows results for an ensemble using TOP (following Jiang et al. [2022]), and\n(b) for a regular deep ensemble without TOP. The regular deep ensemble is better calibrated\nbut has higher test error overall and lower test error for samples with small predicted error.\nB. Reproducibility Analysis\n229\nThis is consistent with the experimental results of Ovadia et al. [2019] which\nexamines dataset shifts. However, given that the calibration metrics change with\nthe quantity of interest, we conclude that:\nInsight B.4. The bound from Theorem B.2 might not have as much expressive power\nas hoped since the calibration metrics themselves deteriorate as the model becomes\nmore ‘uncertain’ about the data.\nAt the same time, the ‘GDE gap’, which is the actual gap between test error and\npredicted error, flattens, and the test error develops an almost linear relationship with\nthe predicted error (up to a bias). This shows that there seem to be intriguing empirical\nproperties of deep ensemble as observed previously [Nakkiran and Bansal, 2020; Jiang\net al., 2022]. However, they are not explained by the proposed calibration metrics10.\nAs described previously, the results are not limited to Bayesian or version-space\nmodels but also apply to any model p(ˆy | x), including regular deep ensembles without\nTOP. In our experiment, we find that a regular deep ensemble is better calibrated\nthan the same ensemble made to satisfy TOP. We hypothesize that each ensemble\nmember’s own predictive distribution is better calibrated than its one-hot outputs,\nyielding a better calibrated ensemble overall.\nGiven that all these calibration metrics require access to the labels, and we cannot\nassume the model to be calibrated under distribution shift, we might just as well\nuse the labels directly to assess the test error.\nB.2.5\nRelated Work\nHere, we discuss connections to Bayesian model disagreement and epistemic uncertainty,\nas well as connections to information theory. We expand on these points in much\ngreater detail in appendix §B.2.7.3.\nBayesian Model Disagreement. From a Bayesian perspective, as the epistemic\nuncertainty increases, we expect the model to become less reliable in its predictions.\nThe predicted error of the model is a measure of the model’s overall uncertainty, which\nis the total of aleatoric and epistemic uncertainty and thus correlated with epistemic\nuncertainty. Thus, we can hypothesize that as the predicted error increases, the model\nshould become less reliable, which will be reflected in increasing calibration metrics.\nThis is exactly what we have empirically validated in the previous section.\nConnection to Information Theory. At first sight, Jiang et al. [2022] seems\ndisconnected from information theory. However, we can draw a connection by using\nˆH(p) ≜1−p as a linear approximation for Shannon’s information content H(p) = −log p.\nSemantically, both this approximation and Shannon’s information content quantify\nsurprise (i.e., prediction error). Both are 0 for certain events. For unlikely events,\nthe former tends to 1 while the latter tends to +∞.\nThis leads to common-sense definitions and statements from an information-theoretic\npoint of view. We can even formulate parallel statements using information theory and\nsee that the statements relate to total uncertainty and not epistemic uncertainty\nin a Bayesian sense.\n10The simplest explanation is that very few samples have high predicted error and thus the rejection\nplots flatten. This is not true. For CINIC-10, the first bucket contains 50k samples, and each latter\nbuckets adds additional ∼10k samples.\nB. Reproducibility Analysis\n230\nOther Related Literature.\nBeyond Jiang et al. [2022], this note offers a\nperspective on Granese et al. [2021] and Garg et al. [2022], which are proposing\nrelated approaches.\nGranese et al. [2021] use the predicted error (disagreement rate), referred to as\nDα, and the predicted top-1 error, Dβ, to estimate when the model will be wrong. As\nnoted in §B.2.7.3, the predicted error can be seen as an approximation of Shannon’s\nentropy. Thus, Dα is effectively using an approximation of the prediction entropy\nfor OoD detection and rejection classification. Similarly, Dβ is the maximum class\nconfidence. Both are well-known baselines for OoD detection [Hendrycks and Gimpel,\n2017]. There is no ablation to see how Dα and Dβ differ from these baselines. We leave\nthis to future work. The paper frames the question of whether a model’s predictions\nwill be correct as binary classification problem on top of the underlying model’s output\nprobabilities and investigate this from a theoretical point of view. They also examine\nusing input perturbations similar to Liang et al. [2018] and Lee et al. [2018b].\nGarg et al. [2022] threshold the predictive entropy or maximum class confidence\nto estimate the test error under distribution shift.\nThey estimate the threshold\nby calibrating it on in-distribution labeled data: the threshold is chosen such that\nthe percentage of rejected in-distribution validation data approximately equals the\ntest error on this in-distribution data. They call this approach Average Thresholded\nConfidence (ATC). They find that ATC using entropy performs better than ATC\nusing the maximum confidence and other approaches, including GDE. Their results\nshow that ATC also degrades under increasing distribution shifts similar to what\nwe have seen for GDE in §B.2.4 as the choice of threshold is explicitly tied to the\nin-distribution11. Garg et al. [2022] explicitly examine the theoretical limits when\nno further assumptions are made.\nB.2.6\nDiscussion\nWe have found that the theoretical statements in Jiang et al. [2022] can be expressed\nand proven more concisely when using probabilistic notation for (Bayesian) models\nthat output softmax probabilities.\nMoreover, we empirically found the proposed calibration metrics to deteriorate\nunder increasing disagreement for in-distribution data, and as expected, we have found\nthe same behavior under distribution shifts.\nWhile Jiang et al. [2022] are careful to qualify their results for distribution shifts,\nabove results should give us pause: strong assumptions are still needed to conjecture\nabout model generalization, and we need to beware of circular arguments.\nB.2.7\nDetails\nB.2.7.1\nEquivalent Definitions\nJiang et al. [2022] defines a hypothesis space H.\nIn the literature, this is also\nsometimes called a version space.\nThe hypothesis space induced by a stochastic\ntraining algorithm A is named HA.\nWe can identify each hypothesis h : X →[C] with itself as parameter ωh = h and\ndefine p(ωh) as a uniform distribution over all parameters/hypotheses in HA. This\nhas the advantage of formalizing the distribution from which hypothesis are drawn\n11See also Figures 7, 8, and 9 in the appendix of Garg et al. [2022]\nB. Reproducibility Analysis\n231\n(h ∼HA), which is not made explicit in Jiang et al. [2022]. h(x) = k then becomes\narg maxˆy p(ˆy | x, ωh) = k. Moreover, as p(ˆy, ω | x) satisfies TOP, we have12\n“1{h(x) = k}′′ = p(ˆY = k | x, ωh).\n(B.61)\nThus, “TestErrD(h) ≜ED[1[h(X) ̸= Y ]]” is equivalent to:\n“ TestErrD(h) = ED[1[h(X) ̸= Y ]]′′\n(B.62)\n= EX,Y [p(ˆY ̸= Y | X, ωh)]\n(B.63)\n= EX[P[ ˆY ̸= Y | X, ωh]]\n(B.64)\n= P[ ˆY ̸= Y | ωh]\n(B.65)\n= TestError(ωh).\n(B.66)\nSimilarly, “DisD (h, h′) ≜ED [1 [h(X) ̸= h′(X)]]” is equivalent to:\n“ DisD (h, h′) ≜ED [1 [h(X) ̸= h′(X)]]′′\n(B.67)\n= E ˆY, ˆY ′[P[ ˆY ̸= ˆY ′ | ωh, ωh′]]\n(B.68)\n= Dis(ωh, ωh′).\n(B.69)\nFurther, “˜hk(x) ≜EHA[1[h(x) = k]]” is equivalent to:\n“˜hk(x) ≜EHA[1[h(x) = k]]′′\n(B.70)\n= EΩ[p(ˆY = k | x, Ω)]\n(B.71)\n= p(ˆY = k | x).\n(B.72)\nFor the GDE, “Eh,h′∼HA [DisD (h, h′)] = Eh∼HA [TestErr(h)]” is equivalent to:\n“Eh,h′∼HA [DisD (h, h′)] = Eh∼HA [TestErr(h)]′′\n⇔EΩ,Ω′[Dis(Ω, Ω′)] = EΩ[TestError(Ω)].\n(B.73)\nFor the class-wise calibration, “p(Y = k | ˜hk(X) = q) = q” is equivalent to:\n“p(Y = k | ˜hk(X) = q) = q′′\n(B.74)\n⇔p(Y = k | p(ˆY = k | X) = q) = q.\n(B.75)\nFor the class-aggregated calibration, “\nPK−1\nk=0 p(Y =k,˜hk(X)=q)\nPK−1\nk=0 p(˜hk(X)=q)\n= q” (and note in Jiang\net al. [2022], class indices run from 0..K −1) is equivalent to:\n“\nPK−1\nk=0 p(Y = k, ˜hk(X) = q)\nPK−1\nk=0 p(˜hk(X) = q)\n= q′′\n(B.76)\n⇔\nPC\nk=1 p(Y = k, p(ˆY = k | X) = q)\nPC\nk=1 p(p(ˆY = k | X) = q)\n= q\n(B.77)\n12We put definitions and expressions written using the notation and variables from Jiang et al.\n[2022] inside quotation marks “” to avoid ambiguities.\nB. Reproducibility Analysis\n232\n⇔\nC\nX\nk=1\np(Y = k, p(ˆY = k | X) = q)\n= q\nC\nX\nk=1\np(p(ˆY = k | X) = q).\n(B.78)\nFinally, for the class-aggregated calibration error, the definition is equivalent to:\n“ CACED(˜h)\n≜\nZ\nq∈[0,1]\n\f\f\f\f\f\f\nP\nk p\n\u0010\nY = k, ˜hk(X) = q\n\u0011\nP\nk p\n\u0010˜hk(X) = q\n\u0011\n−q\n\f\f\f\f\f\f ·\nX\nk\np\n\u0010˜hk(X) = q\n\u0011\ndq\n(B.79)\n=\nZ\nq∈[0,1]\n\f\f\f\nX\nk\np\n\u0010\nY = k, ˜hk(X) = q\n\u0011\n−q\nX\nk\np\n\u0010˜hk(X) = q\n\u0011 \f\f\fdq′′\n(B.80)\n=\nZ\nq∈[0,1]\n\f\f\f\nX\nk\np(Y = k, p(ˆY = k | X) = q) −q\nX\nk\np(p(ˆY = k | X) = q)\n\f\f\fdq (B.81)\nB.2.7.2\nComparison of CACE and CWCE with calibration metrics with\n‘adaptive calibration error’ and ‘static calibration error’\nNixon et al. [2019] examine shortcomings of the ECE metric and identify a lack of\nclass conditionality, adaptivity and the focus on the maximum probability (argmax\nclass) as issues. They suggest an adaptive calibration error which uses adaptive binning\nand averages of the calibration error separately for each class, thus equivalent to\nthe class-wise calibration error and class-wise calibration (up to adaptive vs. even\nbinning). In the paper, the static calibration error is defined as ACE with even instead\nof adaptive binning. However, in the widely used implementation13, SCE is defined\nas equivalent to the class-aggregated calibration error.\nB.2.7.3\nExpanded Discussion\nHere, we discuss connections to Bayesian model disagreement and epistemic un-\ncertainty, as well as connections to information theory, the bias-variance trade-off,\nand prior literature.\nBayesian Model Disagreement From a Bayesian perspective, as the epistemic\nuncertainty increases, we expect the model to become less reliable in its predictions. The\npredicted error of the model is a measure of the overall uncertainty of the model which\nis the total of aleatoric and epistemic uncertainty, and thus correlated with epistemic\nuncertainty. Thus, we can hypothesize that as the predicted error increases, the model\nshould become less reliable, which will be reflected in increasing calibration metrics.\nThis is what we have empirically validated in the previous section. We can expand on\nthe connection to the Bayesian perspective. In particular, we can connect the statements\nof Jiang et al. [2022] to a well-known Bayesian measure of model disagreement.\nIn §B.2.7.5, we also report empirical results for rejection plots based on Bayesian\nmodel disagreement instead of predicted error.\n13https://github.com/google-research/robustness_metrics/blob/\nbaa47fbe38f80913590545fe7c199898f9aff349/robustness_metrics/metrics/uncertainty.\npy#L1585, added in April 2021\nB. Reproducibility Analysis\n233\nConnection to Information Theory At first sight, Jiang et al. [2022] seems\ndisconnected from information theory. However, we can recover statements by using\nˆH(p) ≜1 −p as a linear approximation for Shannon’s information content H(p):\nˆH(p) = 1 −p ≤−log p = H(p).\n(B.82)\nˆH(p) is just the first-order Taylor expansion of H(p) = −log p around 1. Semantically,\nboth Shannon’s information content and this approximation quantify surprise. Both\nare 0 for certain events.\nFor unlikely events, the former tends to +∞while the\nlatter tends to 1.\nWe can define an approximate entropy ˆH[X] using H′:\nˆH[X] ≜E[ˆH(p(X))] = 1 −E[p(x)] = 1 −\nX\nx\np(x)2,\n(B.83)\nand an approximate mutual information ˆI[X; Y ]:\nˆI[X; Y ] ≜ˆH[X] −ˆH[X | Y ] = ˆH[X] −Ep(y) ˆH[X | y],\n(B.84)\nfollowing the semantic notion of mutual information as expected information gain in\n§B.2.1.\nˆI[ ˆY ; Ω| x] as Covariance Trace. This approximate mutual information has a\nsurprisingly nice property, which was detailed in Smith and Gal [2018] originally:\nProposition B.9. The approximate mutual information ˆI[ ˆY ; Ω| x] is equal the sum of\nthe variances of ˆy | x, Ωover all ˆy:\nˆI[ ˆY ; Ω| x] =\nK\nX\nˆy=1\nVarΩ[p(ˆy | x, Ω)] ≥0.\n(B.85)\nWe present a proof in §B.2.7.4. The sum of variances of the predictive probabilities\n(or trace of the respective covariance matrix) is a common proxy for epistemic\nuncertainty [Gal et al., 2017], and here the mutual information ˆI[ ˆY ; Ω| x] using ˆH is\njust that. This gives evidence that these definitions are sensible and connects them to\nother prior Bayesian literature. Importantly, this also shows that ˆH[ ˆY | x] ≥ˆH[ ˆY | x, Ω].\nConnection to Jiang et al. [2022]. As random variable of X and Y , ˆH[ ˆY = Y |\nX] is the test error:\nˆH[ ˆY = Y | X] = 1 −p(ˆY = Y | X) = TestError.\n(B.86)\nThus, the approximate cross-entropy\nˆH(p(Y | X) ∥p(ˆY = Y | X)) = Ep(Y |X)[ˆH(p(ˆY = Y | X))]\n(B.87)\nis the expected test error E[TestError].\nSimilarly, when TOP is fulfilled, the mutual information ˆI[ ˆY ; Ω| X] is the expected\ndisagreement rate E[Dis]. That is, when ˆY | X, Ωis one-hot, we have:\nˆH[ ˆY | X, Ω] = 1 −EX E ˆY [p(ˆY | X, Ω) | X]\n|\n{z\n}\n=1\n= 0,\n(B.88)\nB. Reproducibility Analysis\n234\nand thus:\nˆI[ ˆY ; Ω| X] = ˆH[ ˆY | X] −ˆH[ ˆY | X, Ω]\n(B.89)\n= ˆH[ ˆY | X]\n(B.90)\n= 1 −EX, ˆY [p(ˆY | X)]\n(B.91)\n= E[Dis].\n(B.92)\nLemma B.10. When the model p(ˆy | x, ω) satisfies TOP, the GDE is equivalent to:\nˆH(p(Y | X) ∥p(ˆY = Y | X)) = ˆI[ ˆY ; Ω| X].\n(B.93)\nThis relates the approximate cross-entropy loss (test error) to the approximate\nBayesian model disagreement.\nWithout TOP. If TOP does not hold, the actual expected disagreement ˆI[ ˆY ; Ω| x]\nlower-bounds the “expected disagreement rate” E[Dis], which then equals the expected\npredicted error 1 −EX, ˆY [p(ˆY | X)] when we have GDE. We have the following general\nequivalence to GDE:\nLemma B.11. For a model p(ˆy | x), the GDE is equivalent to:\nˆH(p(Y | X) ∥p(ˆY = Y | X)) = ˆH[ ˆY | X] ≥ˆI[ ˆY ; Ω| X].\n(B.94)\nThe other statements and proofs translate likewise, and intuitively seem sensible\nfrom an information-theoretic perspective. We can go further and directly establish\nanalogous properties using information theory in the next subsection.\nInformation-Theoretic Version Here, we derive an information-theoretic version of\nthe GDE both under the assumption of TOP and without. Importantly, we will not\nrequire a Bayesian model for any of the main statements as they hold for any model p(ˆy|\nx). We show that we can artificially introduce a connection to disagreement using TOP.\nInformation-Theoretic GDE. We have already introduced the BALD equation\n??, which connects expected disagreement and predictive uncertainty:\nI[ ˆY ; Ω| x] = H[ ˆY | x] −H[ ˆY | x, Ω]\nThe expected disagreement is measured by the mutual information I[ ˆY ; Ω| X], and\nthe prediction error is measured by the cross-entropy of the predictive distribution\nunder the true data generating distribution H(p(Y | X) ∥p(ˆY = Y | X)). Indeed, the\ntest error is bounded by it [Kirsch et al., 2020]:\np(Y ̸= ˆY ) ≤1 −e−H(p(Y |X)∥p( ˆY =Y |X)).\n(B.95)\nWhen our model fulfills TOP, we have H[ ˆY | X, Ω] = 0, and thus I[ ˆY ; Ω| X] = H[ ˆY |\nX]. The expected disagreement then equals the predicted label uncertainty H[ ˆY |\nX]. Generally, we can define an ‘entropic GDE’:\nDefinition B.6. A model p(ˆy | x) satisfies entropic GDE, when:\nH(p(Y | X) ∥p(ˆY = Y | X)) = H[ ˆY | X].\n(B.96)\nB. Reproducibility Analysis\n235\nLemma B.12. When a Bayesian model p(ˆy, ω | x) satisfies TOP, entropic GDE is\nequivalent to\nH(p(Y | X) ∥p(ˆY = Y | X)) = I[ ˆY ; Ω| X].\n(B.97)\nThe latter is close to GDE, especially when comparing to the previous section.\nWe can formulate an entropic class-aggregated calibration by connecting H[ˆy|x] with\nH[y | x]. That is, instead of using probabilities, we use Shannon’s information-content:\nDefinition B.7. The model p(ˆy | x) satisfies entropic class-aggregated calibration when\nfor any q ≥0:\np(H[ ˆY = Y | X] = q) = p(H[ ˆY = ˆY | X] = q).\n(B.98)\nSimilarly, we can define the entropic class-aggregated calibration error (ECACE):\nECACE ≜\nZ\nq∈[0,∞)\n\f\f\f p(H[ ˆY = Y | X] = q)\n−p(H[ ˆY = ˆY | X] = q)\n\f\f\fdq.\n(B.99)\nAs −log p is strictly monotonic and thus invertible for non-negative p, entropic\nclass-aggregated calibration and class-aggregated calibration are equivalent. ECACE\nand CACE are not, though.\nThe expectation of the transformed random variable H[ ˆY = Y | X] (in Y and\nX) is just the cross-entropy:\nEX,Y H[ ˆY = Y | X] = Ep(x,y) H[ ˆY = y | X] = H(p(Y | X) ∥p(ˆY = Y | X)).\n(B.100)\nUsing this notation, and analogous to Theorem B.2, we can show:\nTheorem B.13. When H[ˆy | x] = −log p(ˆy | x) is upper-bounded by L for all ˆy and x,\nwe have:\nECACE ≥1\nL\n\f\f\fH(p(Y | X) ∥p(ˆY = Y | X)) −H[ ˆY | X]\n\f\f\f,\n(B.101)\nand when the model satisfies TOP, equivalently:\n= 1\nL\n\f\f\fH(p(Y | X) ∥p(ˆY = Y | X)) −I[ ˆY ; Ω| X]\n\f\f\f.\n(B.102)\nThere might be better conditions than the upper-bound above, but this bound\nis in the spirit of Jiang et al. [2022]. Indeed, the proof of Theorem B.2 is the same,\nexcept that we use q ≤L instead of q ≤1. Finally, when the model satisfies entropic\nclass-aggregated calibration, ECACE = 0, cross-entropy (or negative expected log\nlikelihood) equals disagreement (respectively, predicted label uncertainty when TOP\ndoes not hold).\nThus, we have:\nTheorem B.14. When the model p(ˆy|x) satisfies entropic class-aggregated calibration,\nit trivially also satisfies entropic GDE:\nH(p(Y | X) ∥p(ˆY = Y | X)) = H[ ˆY | X] ≥I[ ˆY ; Ω| X],\n(B.103)\nand when TOP holds:\nH(p(Y | X) ∥p(ˆY = Y | X)) = I[ ˆY ; Ω| X].\n(B.104)\nB. Reproducibility Analysis\n236\nWithout TOP. Again, if we do not expect one-hot predictions for our ensemble\nmembers, the analogy put forward in Jiang et al. [2022] breaks down because the\nBayesian disagreement I[ ˆY ; Ω| X] only lower bounds the predicted label uncertainty\nH[ ˆY | X] and can not be connected to ECACE the same way. But this also breaks\ndown in the regular version in Jiang et al. [2022].\nB.2.7.4\nAdditional Proofs\nLemma B.15. For a model p(ˆy | x), we have for all k ∈[K] and q ∈[0, 1]:\np(ˆY = k | p(ˆY = k | X) = q) = q,\n(B.105)\nwhen the left-hand side is well-defined.\nProof. This is equivalent to\np(ˆY = k, p(ˆY = k | X) = q) = q p(p(ˆY = k | X) = q),\n(B.106)\nas the conditional probability is either defined or p(p(ˆY = k | X) = q) = 0. Assume\nthe former. Let p(p(ˆY = k | X) = q) > 0. Introducing the auxiliary random variable\nTk ≜p(ˆY = k | X) as a transformed random variable of X, we have\np(ˆY = k, Tk = q) = q p(Tk = q).\n(B.107)\nWe can write the probability as an expectation over an indicator function\np(ˆY = k, Tk = q)\n(B.108)\n= EX, ˆY [1{ˆY = k, Tk(X) = q}]\n(B.109)\n= EX, ˆY [1{ˆY = k} 1{Tk(X) = q}]\n(B.110)\n= EX[1{Tk(X) = q} E ˆY [1{ˆY = k} | X]]\n(B.111)\n= EX[1{Tk(X) = q} p(ˆY = k | X)].\n(B.112)\nImportantly, if 1{Tk(x) = q} = 1 for an x, we have Tk(x) = p(ˆY = k | x) = q, and\notherwise, we multiply with 0. Thus, this is equivalent to\n= EX[1{Tk(X) = q} q]\n(B.113)\n= q EX[1{Tk(X) = q}]\n(B.114)\n= q p(Tk(X) = q).\n(B.115)\nLemma B.4. The model p(ˆy | x) satisfies class-wise calibration when for any q ∈[0, 1]\nand any class k ∈[C]:\np(Y = k, p(ˆY = k | X) = q) = p(ˆY = k, p(ˆY = k | X) = q).\n(B.43)\nSimilarly, the model p(ˆy|x) satisfies class-aggregated calibration when for any q ∈[0, 1]:\np(p(ˆY = Y | X) = q) = p(p(ˆY | X) = q),\n(B.44)\nand class-wise calibration implies class-aggregate calibration.\nB. Reproducibility Analysis\n237\nProof. Beginning from\np(Y = k | p(ˆY = k | X) = q) = q,\n(B.116)\nwe expand the conditional probability to\n⇔p(Y = k, p(ˆY = k | X) = q) = q p(p(ˆY = k | X) = q),\n(B.117)\nand substitute Equation B.42 into the outer q, obtaining the first equivalence\n⇔p(Y = k, p(ˆY = k | X) = q) = p(ˆY = k, p(ˆY = k | X) = q).\n(B.118)\nFor the second equivalence, we follow the same approach. Beginning from\nX\nk\np(Y = k, p(ˆY = k | X) = q) = q\nX\nk\np(p(ˆY = k | X) = q),\n(B.119)\nwe pull the outer q into the sum and expand using (B.42)\n⇔\nX\nk\np(Y = k, p(ˆY = k | X) = q) =\nX\nk\nq p(p(ˆY = k | X) = q) =\nX\nk\np(ˆY = k, p(ˆY = k | X) = q).\n(B.120)\nIn the inner expression, k is tied to Y on the left-hand side and ˆY on the right-hand\nside, so we have\n⇔\nX\nk\np(Y = k, p(ˆY = Y | X) = q) =\nX\nk\np(ˆY = k, p(ˆY | X) = q).\n(B.121)\nSumming over k, marginalizes out Y = k and ˆY = k respectively, yielding the second\nequivalence\n⇔p(p(ˆY = Y | X) = q) = p(p(ˆY | X) = q).\n(B.122)\nFinally, class-wise calibration implies class-aggregated calibration as summing over\ndifferent k in (B.118), which is equivalent to class-wise calibration, yields (B.120),\nwhich is equivalent to class-aggregated calibration.\nProposition B.9. The approximate mutual information ˆI[ ˆY ; Ω| x] is equal the sum of\nthe variances of ˆy | x, Ωover all ˆy:\nˆI[ ˆY ; Ω| x] =\nK\nX\nˆy=1\nVarΩ[p(ˆy | x, Ω)] ≥0.\n(B.85)\nProof. We show that both sides are equal:\nˆI[ ˆY ; Ω| x] = ˆH[ ˆY | x] −ˆH[ ˆY | x, Ω]\n(B.123)\n= E ˆY [1 −p(ˆY | x)] −E ˆY,Ω[1 −p(ˆY | x, Ω)]\n(B.124)\n= E ˆY,Ω[p(ˆY | x, Ω)] −E ˆY [p(ˆY | x)]\n(B.125)\n= EΩEp(ˆy,x,Ω)[p(ˆy | x, Ω)] −Ep(ˆy|x) p(ˆy | x)\n(B.126)\nB. Reproducibility Analysis\n238\n= EΩ\n\n\nK\nX\nˆy=1\np(ˆy | x, Ω)2\n\n−\nK\nX\nˆy=1\nEΩ[p(ˆy | x, Ω)]2\n(B.127)\n=\nK\nX\nˆy=1\nEΩ\nh\np(ˆy | x, Ω)2i\n−EΩ[p(ˆy | x, Ω)]2\n(B.128)\n=\nK\nX\nˆy=1\nVarΩ[p(ˆy | x, Ω)]\n(B.129)\n≥0,\n(B.130)\nwhere we have used that Ep(ˆy|x) p(ˆy | x) = PK\nˆy=1 p(ˆy | x)2.\nB.2.7.5\nEmpirical Validation of Calibration Deterioration under Increasing\nDisagreement\nHere, we discuss additional details to allow for reproduction and present results on\nadditional datasets. In addition to the experiments on CIFAR-10 [Krizhevsky, 2009]\nand CINIC-10 [Darlow et al., 2018], we report results for ImageNet [Deng et al., 2009]\n(in-distribution) using an ensemble of pretrained models and PACS [Li et al., 2017]\n(distribution shift) where we fine-tune ImageNet models on PACS’ ‘photo’ domain,\nwhich is close to ImageNet as source domain, and evaluate it on PACS’ ‘art painting’,\n‘sketch’, and ‘cartoon’ domains. We use all three domains together for distribution\nshift evaluation to have more samples for the rejection plots.\nExperiment Setup We use PyTorch [Paszke et al., 2019] for all experiments.\nCIFAR-10 and CINIC-10. We follow the training setup from Mukhoti et al.\n[2023]: we train 25 WideResNet-28-10 models [Zagoruyko and Komodakis, 2016] for\n350 epochs on CIFAR-10. We use SGD with a learning rate of 0.1 and momentum of\n0.9. We use a learning rate schedule with a decay of 10 at 150 and 250 epochs.\nImageNet and PACS. We use pretrained models with various architectures\n(specifically: ResNet-152-D [He et al., 2018], BEiT-L/16 [Bao et al., 2021], ConvNext-L\n[Liu et al., 2022], DeiT3-L/16 [Touvron et al., 2020], and ViT-B/16 [Dosovitskiy et al.,\n2020]) from the timm package [Wightman, 2019] as base models. We freeze all weights\nexcept for the final linear layer, which we fine-tune on PACS’ ‘photo‘ domain using\nAdam [Kingma and Ba, 2015] with learning rate 5 × 10−3 and batch size 128 for 1000\nsteps. We then build an ensemble using these different models.\nAdditional Results In Figure B.6, we see that for ImageNet and PACS, the calibration\nmetrics behave like for CIFAR-10 and CINIC-10, matching the described behavior\nin the main text. We use 5 models from each of the enumerated architectures to\nbuild an ensemble of 25 models. Individual architectures also behave as expected\nas we ablate in Figure B.9.\nAdditionally, in Figure B.7 and Figure B.8, we also show rejection plots using the\nExpected Information Gain/BALD for thresholding. We observe similar trajectories.\nComparing these results with Figure B.5 and Figure B.6, we see that both the predicted\nerror and the Bayesian metric behave similarly. We hypothesize that this could be\nbecause the datasets only contain few samples with high aleatoric uncertainty (e.g.\nnoise), which would otherwise act as confounder [Mukhoti et al., 2023]. See also\nthe discussion in §B.2.5.\nB. Reproducibility Analysis\n239\nMeasure\nCWCE\nCACE\nTest Error\nECE\n| Test Error - Predicted Error |\nImageNet (In-Distribution)\n0.2\n0.4\n0.6\n0.8\n1.0\nPredicted Error (Disagreement Rate)\n0.00\n0.05\n0.10\n0.15\n0.20\nValue\n0.2\n0.4\n0.6\n0.8\n1.0\nPredicted Error (Disagreement Rate)\nPACS (Distribution Shift)\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nPredicted Error (Disagreement Rate)\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\nValue\n(a) Ensemble with TOP\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nPredicted Error (Disagreement Rate)\n(b) Ensemble without TOP (same underlying\nmodels)\nFigure B.6:\nRejection Plot of Calibration Metrics for Increasing Disagreement In-\nDistribution (ImageNet) and Under Distribution Shift (PACS ‘photo‘ domain →other\ndomains). Different calibration metrics (ECE, CWCE, CACE) vary across ImageNet and\nPACS’ ‘art painting’, ‘cartoon’, and ‘sketch’ domains across an ensemble of 5 models trained\non ImageNet and 25 models fine-tuned on PACS’ ‘photo’ domain, depending on the rejection\nthreshold of the predicted error (disagreement rate). Again, calibration cannot be assumed\nconstant for in-distribution data or under distribution shift. The mean predicted error\n(disagreement rate) is shown on the x-axis. (a) shows results for an ensemble using TOP\n(following Jiang et al. [2022]), and (b) for a regular deep ensemble without TOP. Details in\n§B.2.7.5.\nB. Reproducibility Analysis\n240\nMeasure\nCWCE\nCACE\nTest Error\nECE\n| Test Error - Predicted Error |\nCIFAR-10 (In-distribution)\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nExpected Information Gain/BALD (Bayesian Disagreement)\n0.00\n0.01\n0.02\n0.03\n0.04\nValue\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nExpected Information Gain/BALD (Bayesian Disagreement)\nCINIC-10 (Distribution Shift)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nExpected Information Gain/BALD (Bayesian Disagreement)\n0.05\n0.10\n0.15\n0.20\n0.25\nValue\n(a) Ensemble with TOP\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nExpected Information Gain/BALD (Bayesian Disagreement)\n(b) Ensemble without TOP (same underlying\nmodels)\nFigure B.7: Rejection Plot of Calibration Metrics for Increasing Bayesian Disagreement\nIn-Distribution (CIFAR-10) and Under Distribution Shift (CINIC-10). Different calibration\nmetrics (ECE, CWCE, CACE) vary across CIFAR-10 and CINIC-10, depending on the\nrejection threshold of Bayesian disagreement (Expected Information Gain/BALD). The\ntrajectory matches the one for prediction disagreement. We hypothesize this is because\nthere are few noisy samples in the dataset which would act as a confounder for prediction\ndisagreement otherwise. Details in §B.2.7.5.\nB. Reproducibility Analysis\n241\nMeasure\nCWCE\nCACE\nTest Error\nECE\n| Test Error - Predicted Error |\nImageNet (In-distribution)\n0.2\n0.4\n0.6\n0.8\n1.0\nExpected Information Gain/BALD (Bayesian Disagreement)\n0.050\n0.075\n0.100\n0.125\n0.150\n0.175\n0.200\n0.225\nValue\n0.2\n0.4\n0.6\n0.8\n1.0\nExpected Information Gain/BALD (Bayesian Disagreement)\nPACS (Distribution Shift)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\nExpected Information Gain/BALD (Bayesian Disagreement)\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\nValue\n(a) Ensemble with TOP\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\nExpected Information Gain/BALD (Bayesian Disagreement)\n(b) Ensemble without TOP (same underlying\nmodels)\nFigure B.8: Rejection Plot of Calibration Metrics for Increasing Bayesian Disagreement\nIn-Distribution (CIFAR-10) and Under Distribution Shift (CINIC-10). Different calibration\nmetrics (ECE, CWCE, CACE) vary across CIFAR-10 and CINIC-10, depending on the\nrejection threshold of Bayesian disagreement (Expected Information Gain/BALD). The\ntrajectory matches the one for prediction disagreement. We hypothesize this is because\nthere are few noisy samples in the dataset which would act as a confounder for prediction\ndisagreement otherwise. Details in §B.2.7.5.\nB. Reproducibility Analysis\n242\nMeasure\nCWCE\nCACE\nTest Error\nECE\n| Test Error - Predicted Error |\nPACS, BeiT-L/16\n0.0\n0.2\n0.4\n0.6\n0.8\nPredicted Error (Disagreement Rate)\n0.0\n0.1\n0.2\n0.3\n0.4\nValue\n0.0\n0.2\n0.4\n0.6\n0.8\nPredicted Error (Disagreement Rate)\nPACS, ResNet-152-D\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nPredicted Error (Disagreement Rate)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nValue\n(a) Ensemble with TOP\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nPredicted Error (Disagreement Rate)\n(b) Ensemble without TOP (same underlying\nmodels)\nFigure B.9: Rejection Plot of Calibration Metrics for Increasing Disagreement Under\nDistribution Shift (PACS ‘photo‘ domain →other domains) for Specific Model Architectures.\nWe use the same encoder weights and evaluate on an ensemble of 5 models, which were\nlast-layer fine-tuned on PACS. We show ResNet-152-D and BeiT-L/16. Details in §B.2.7.5.\nB. Reproducibility Analysis\n243\nB.3\nDirichlet Model of a Deep Ensemble’s Softmax\nPredictions\nHow well can these distributions capture the outputs of an ensemble? Several works\nhave examined distilling the uncertainty of deep ensembles into a single model [Malinin\net al., 2019; Fathullah et al., 2021; Ryabinin et al., 2021] or using Dirichlet distributions\nto model epistemic uncertainty [Malinin and Gales, 2018, 2019; Hammam et al., 2022].\nBut In this chapter, we qualitatively examine how well Dirichlet distributions can\ncapture predictions of deep ensembles in computer vision.\nConcretely, we examine how well they can approximate the variance of softmax\nentropies when matching the predictions and epistemic uncertainty of input samples\non OoD data (which is arguably more difficult than when trying to capture uncertainty\nof iD which will have low epistemic uncertainty).\nThis is easier to visualize for\ndatasets than individual predictions.\nWe qualitatively evaluate deep ensembles across different model architectures and\nqualitatively find that the modelled Dirichlet distributions provide more concentrated\npredictions. Hence, samples from them are also unlikely to model the actual individual\npredictions of a deep ensemble well. This does not invalidate any of the results in the\nprevious chapters, but it does suggest that the Dirichlet distribution is not a good\napproximation of the posterior predictive distribution of the ensemble members.\nB.3.1\nMethodology\nThere are two interpretations of the ensemble parameter distribution p(ω | Dtrain):\n1. we can view it as an empirical distribution given a specific ensemble with members\nωi∈{1,...,K}, or\n2. we can view it as a distribution over all possible trained models which depends\non random weight initializations, the dataset, stochasticity in the minibatches,\nand the optimization process.\nIn the latter case, any deep ensemble with K members can be seen as finite Monte-\nCarlo sample of this posterior distribution. The predictions of an ensemble then are an\nunbiased estimate of the predictive distribution Ep(ω|Dtrain)[p(y | x, ω)], and similarly\nthe expected information gain computed using the members of the deep ensemble\nis just a (biased) estimator of I[Y ; Ω| x, Dtrain].\nInverse Problem. Based on this interpretation of deep ensembles as a distribution\nover model parameters, we can look at the following inverse problem: given some value\nfor the predictive distribution and epistemic uncertainty of a deep ensemble, estimate\nwhat the softmax entropies from each ensemble component must have been. That is\nif we observe deep ensembles to have high epistemic uncertainty on (near) OoD data,\nwe can deduce from that what the distribution of softmax entropy of deterministic\nneural nets (the ensemble members) ought to look like.\nThat is, given a predictive distribution p(y | x, Dtrain) and epistemic uncertainty\nI[Y ; Ω| x, Dtrain] (expected information gain) of the deep ensemble, we can observe\nthe softmax entropy H[Y | x, ω] as a random variable of a single deterministic model,\nω ∼p(ω | Dtrain), and estimate its variance Varω|Dtrain[H[Y | x, ω]].\nEmpirically, we find the real variance to be higher by a large amount for OoD\nsamples, showing that softmax entropies do not capture epistemic uncertainty well\nfor samples with high epistemic uncertainty.\nB. Reproducibility Analysis\n244\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nNats\n0\n5\n10\n15\n20\n25\n30\n35\n40\nPercentage\nEmpirical Softmax Entropy\nSimulated Softmax Entropy\n(a) SVHN (OoD): VGG16\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nNats\n0\n5\n10\n15\n20\n25\n30\n35\n40\nPercentage\nEmpirical Softmax Entropy\nSimulated Softmax Entropy\nWideResNet-28-10+SN\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nNats\n0\n5\n10\n15\n20\n25\n30\n35\n40\nPercentage\nEmpirical Softmax Entropy\nSimulated Softmax Entropy\n(b) CIFAR-100 (OoD): VGG16\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nNats\n0\n5\n10\n15\n20\n25\n30\n35\n40\nPercentage\nEmpirical Softmax Entropy\nSimulated Softmax Entropy\nWideResNet-28-10+SN\nFigure B.10: Simulated vs Empirical Softmax Entropy and Predictive Entropy. WideResNet-\n28-10+SN and VGG16 trained on CIFAR-10 (25 models). Although we use a simple Dirichlet\nmodel, sampling from the fitted Dirichlet distributions does approximate the empirical entropy\ndistribution of softmax entropies well.\nWe will need to make several strong assumptions that limit the generality of our\nestimation, but we can show that our analysis models the resulting softmax entropy\ndistributions appropriately. This will show that deterministic softmax models can\nhave widely different entropies and confidence values.\nApproximate Model. For a fixed x, we approximate the distribution over softmax\nprobability vectors p(y | x, ω) for different ω using a Dirichlet distribution p ∼Dir(α)\nwith non-negative concentration parameters α = (α1, . . . , αK) and α0 := P αi. Note\nthat we only use the Dirichlet distribution as an analysis tool.\nConcretely, for a distribution over models p(ω | Dtrain), and a sample x, we obtain\np(y|x, Dtrain), and I[Y ; Ω|x, Dtrain]. We use moment matching with these two quantities\nto fit a Dirichlet distribution p ∼Dir(α) on p(y | x, ω) over Ω, which satisfies:\np(y | x, Dtrain) = αi\nα0\n(B.131)\nH[Y | x, Dtrain] −I[Y ; Ω| x, Dtrain] = ψ(α0 + 1) −\nK\nX\ny=1\np(y | x)ψ(α0 p(y | x) + 1).\n(B.132)\nThen, we can model the softmax distribution as given in eq. (B.20). The details\nand proofs can be found below in §B.3.4.\nB. Reproducibility Analysis\n245\n0.010\n0.005\n0.000\nNats\n0\n1000\n2000\n3000\n4000\nDensity\n0.2\n0.0\n0.2\nNats\n0\n5\n10\n15\nDensity\n0.000 0.005 0.010 0.015 0.020\nNats\n0\n50\n100\n150\n200\nDensity\nSVHN: VGG16\n0.008\n0.006\n0.004\n0.002 0.000\nNats\n0\n2000\n4000\n6000\n8000\n10000\nDensity\n0.2\n0.0\n0.2\nNats\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\nDensity\n0.000\n0.005\n0.010\n0.015\nNats\n0\n50\n100\n150\n200\n250\nDensity\nWideResNet-28-10+SN\n0.006\n0.004\n0.002 0.000\nNats\n0\n500\n1000\n1500\n2000\n2500\nDensity\n0.2\n0.0\n0.2\nNats\n0\n5\n10\n15\n20\nDensity\n0.000\n0.005\n0.010\n0.015\n0.020\nNats\n0\n50\n100\n150\nDensity\nCIFAR-100: VGG16\n0.010\n0.005\n0.000\nNats\n0\n500\n1000\n1500\n2000\nDensity\n0.2\n0.0\n0.2\nNats\n0\n5\n10\n15\n20\n25\nDensity\n0.000\n0.005\n0.010\nNats\n0\n50\n100\n150\n200\n250\nDensity\nError\nMutual Information\nExpected Softmax Entropy\nPredictive Entropy\nJenson-Shannon Divergence\n(Predictions)\nWideResNet-28-10+SN\nFigure B.11: Simulated Quantities (via Dirichlet Distributions) vs Empirical Quantities.\nWideResNet-28-10+SN and VGG16 trained on CIFAR-10 (25 models). Although we use a\nsimple Dirichlet model, sampling from the fitted Dirichlet distributions does approximate the\nempirical entropy distribution of softmax entropies well.\nB.3.2\nQualitative Empirical Validation\nWe train two deep ensembles of VGG and WideResNet-28-10+SN models (25 members\neach) on CIFAR-10 and compute the predictive entropy, mutual information, and\nsoftmax entropies for each sample in SVHN and CIFAR-100, which we use as OoD\ndistribution. Then we fit a Dirichlet distribution on the softmax entropies of the\nensemble members and use the fitted distribution to simulate the softmax entropies\nfor the OoD samples. We fit the Dirichlet distribution on the BMA prediction of the\nensemble and the corresponding mutual information (epistemic uncertainty).\nWe have already empirically verified that softmax entropies vary considerably in\nB. Reproducibility Analysis\n246\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nInformation Gain\n0.0\n0.1\n0.2\n0.3\n0.4\nSoftmax Entropy Variance\nEmpirical\nPredicted\nVGG16\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nInformation Gain\n0.0\n0.1\n0.2\n0.3\n0.4\nSoftmax Entropy Variance\nEmpirical\nPredicted\n(a) SVHN: WideResNet-28-10+SN\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nInformation Gain\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nSoftmax Entropy Variance\nEmpirical\nPredicted\nVGG16\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nInformation Gain\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nSoftmax Entropy Variance\nEmpirical\nPredicted\n(b) CIFAR-100: WideResNet-28-10+SN\nFigure B.12: Simulated & empirical softmax entropy vs mutual information (EIG) on\nWideResNet-28-10+SN and VGG16. Although we use a simple Dirichlet model, samples from\nthe fitted Dirichlet distributions approximate the three major information quantities and the\nBMA of the ensemble predictions very well.\n§3.1.3. The distribution of actual softmax entropies and predicted softmax entropies\nseems to be close in the histograms of Figure B.10, and the overall error terms for the\nthree information quantities as well as the Jensen-Shannon Divergence between the sim-\nulated BMA predictions and real ones are small Figure B.11. Yet, Figures B.12 and B.13\nshow that the distributions are quite different in terms of softmax entropy variance and\nRMSE (for the softmax entropies as estimates of the respective predictive entropy).\nB.3.3\nDiscussion\nHence, we can conclude without evaluating the simulated predictions themselves in\nmore detail that the Dirichlet model is not a very good fit for the empirical distribution\nof softmax entropies from individual members of a deep ensemble. This does not\ninvalidate that these approximations have great and useful value on downstream tasks,\nbut it raises the question for future work to find distributions that can better capture the\nempirical distribution of softmax entropies from individual members of a deep ensemble.\nB.3.4\nDetails\nBased on the interpretation of deep ensembles as a distribution over model parameters,\nwe can walk backwards and, given some value for the predictive distribution and\nepistemic uncertainty of a deep ensemble, estimate what the softmax entropies from\neach ensemble component must have been. I.e. if we observe deep ensembles to have\nB. Reproducibility Analysis\n247\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nMutual Information\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nSoftmax Entropy Variance\nEmpirical\nPredicted\n(a) SVHN: VGG16\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMutual Information\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSoftmax Entropy Variance\nEmpirical\nPredicted\nWideResNet-28-10+SN\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\nMutual Information\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\nSoftmax Entropy Variance\nEmpirical\nPredicted\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMutual Information\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nSoftmax Entropy Variance\nEmpirical\nPredicted\nFigure B.13: RMSE for simulated & empirical softmax entropy vs mutual information\n(EIG) on WideResNet-28-10+SN and VGG16.\nhigh epistemic uncertainty on OoD data, we can deduce from that what the softmax\nentropy of deterministic neural nets (the ensemble components) must look like. More\nspecifically, given a predictive distribution p(y | x, Dtrain) and epistemic uncertainty,\nthat is expected information gain I[Y ; Ω| x, Dtrain], of the infinite deep ensemble, we\nestimate the expected softmax entropy from a single deterministic model, considered as\na sample ω ∼p(ω|Dtrain) and model the variance. Empirically, we find the real variance\nto be higher by a large amount for OoD samples, showing that softmax entropies do not\ncapture epistemic uncertainty well for samples with high epistemic uncertainty. We will\nneed to make several strong assumptions that limit the generality of our estimation.\nGiven the predictive distribution p(y | x, Dtrain) and epistemic uncertainty I[Y ; Ω|\nx, Dtrain], we can approximate the distribution over softmax probability vectors p(y |\nx, ω) for different ω using its maximum-entropy estimate: a Dirichlet distribution\n(Y1, . . . , YK) ∼Dir(α) with non-negative concentration parameters α = (α1, . . . , αK)\nand α0 :=\nP αi. Note that the Dirichlet distribution is used only as an analysis tool,\nand at no point do we need to actually fit Dirichlet distributions to our data.\nPreliminaries. Before we can establish our main result, we need to look more\nclosely at Dirichlet-Multinomial distributions. Given a Dirichlet distribution Dir(α)\nand a random variable p ∼Dir(α), we want to quantify the expected entropy\nEp∼Dir(α) HY ∼Cat(p)[Y ] and its variance Varp∼Dir(α)[HY ∼Cat(p)[Y ]]. For this, we need\nto develop more theory. In the following, Γ denotes the Gamma function, ψ denotes\nthe Digamma function, ψ′ denotes the Trigamma function.\nLemma B.16. Given a Dirichlet distribution and random variable p ∼Dir(α), the\nfollowing hold:\nB. Reproducibility Analysis\n248\n1. The expectation E[log pi] is given by:\nE[log pi] = ψ(αi) −ψ(α0).\n(B.133)\n2. The covariance Cov[log pi, log pj] is given by\nCov[log pi, log pj] = ψ′(αi) δij −ψ′(α0).\n(B.134)\n3. The expectation E[pn\ni pm\nj log pi] is given by:\nE[pn\ni pm\nj log pi]\n= αn\ni αm\nj\nαn+m\n0\n(ψ(αi + n) −ψ(α0 + n + m)) ,\n(B.135)\nwhere i ̸= j, and nk = n (n + 1) . . . (n + k −1) denotes the rising factorial.\nProof. 1. The Dirichlet distribution is members of the exponential family. Therefore,\nthe moments of the sufficient statistics are given by the derivatives of the partition\nfunction with respect to the natural parameters. The natural parameters of the Dirichlet\ndistribution are just its concentration parameters αi. The partition function is\nA(α) =\nk\nX\ni=1\nlog Γ (αi) −log Γ (α0) ,\n(B.136)\nthe sufficient statistics is T(x) = log x, and the expectation E[T] is given by\nE[Ti] = ∂A(α)\n∂αi\n(B.137)\nas the Dirichlet distribution is a member of the exponential family. Substituting the\ndefinitions and evaluating the partial derivative yields\nE[log pi] =\n∂\n∂αi\n\" k\nX\ni=1\nlog Γ (αi) −log Γ\n k\nX\ni=1\nαi\n!#\n(B.138)\n= ψ (αi) −ψ (α0) ∂\n∂αi\nα0,\n(B.139)\nwhere we have used that the Digamma function ψ is the log derivative of the Gamma\nfunction ψ(x) =\nd\ndx ln Γ(x). This proves (B.133) as\n∂\n∂αiα0 = 1.\n2. Similarly, the covariance is obtained using a second-order partial derivative:\nCov[Ti, Tj] = ∂2A(α)\n∂αi ∂αi\n.\n(B.140)\nAgain, substituting yields\nCov[log pi, log pj] =\n∂\n∂αj\n[ψ (αi) −ψ (α0)]\n(B.141)\n= ψ′ (αi) δij −ψ′ (α0) .\n(B.142)\nB. Reproducibility Analysis\n249\n3. We will make use of a simple reparameterization to prove the statement using\nEquation B.133. Expanding the expectation and substituting the density Dir(p; α), we\nobtain\nE[pn\ni pm\nj log pi] =\nZ\nDir(p; α) pn\ni pm\nj log pi dp\n(B.143)\n=\nZ\nΓ (α0)\nQK\ni=1 Γ (αi)\nK\nY\nk=1\npαk−1\nk\npn\ni pm\nj log pi dp\n(B.144)\n= Γ(αi + n)Γ(αj + m)Γ(α0 + n + m)\nΓ(αi)Γ(αj)Γ(α0)\nZ\nDir(ˆp; ˆα) ˆpn\ni ˆpm\nj log ˆpi dˆp\n(B.145)\n= αn\ni αm\nj\nαn+m\n0\nE[log ˆpi],\n(B.146)\nwhere ˆp ∼Dir(ˆα) with ˆα = (α0, . . . , αi + n, . . . , αj + m, . . . , αK), and we made use of\nthe fact that Γ(z+n)\nΓ(z)\n= zn. Finally, we can apply Equation B.133 on ˆp ∼Dir(ˆα) to show\n= αn\ni αm\nj\nαn+m\n0\n(ψ(αi + n) −ψ(α0 + n + m)) .\n(B.147)\nWith this, we can already quantify the expected entropy Ep∼Dir(α) HY ∼Cat(p)[Y ]:\nLemma B.17. Given a Dirichlet distribution and a random variable p ∼Dir(α), the\nexpected entropy Ep∼Dir(α) HY ∼Cat(p)[Y ] of the categorical distribution Y ∼Cat(p) is\ngiven by\nEp(p|α) H[Y | p] = ψ(α0 + 1) −\nK\nX\ny=1\nαi\nα0\nψ(αi + 1).\n(B.148)\nProof. Applying the sum rule of expectations and Equation B.135 from Lemma B.16,\nwe can write\nE H[Y | p] = E[−\nK\nX\ni=1\npi log pi] = −\nX\ni\nE[pi log pi]\n(B.149)\n= −\nX\ni\nαi\nα0\n(ψ(αi + 1) −ψ(α0 + 1)) .\n(B.150)\nThe result follows after rearranging and making use of P\ni\nαi\nα0 = 1.\nWith these statements, we can answer a slightly more complex problem:\nLemma B.18. Given a Dirichlet distribution and a random variable p ∼Dir(α), the\ncovariance Cov[pn\ni log pi, pm\nj log pj] is given by\nCov[pn\ni log pi, pm\nj log pj]\n(B.151)\nB. Reproducibility Analysis\n250\n= αn\ni αm\nj\nαn+m\n0\n((ψ(αi + n) −ψ(α0 + n + m))\n(ψ(αj + m) −ψ(α0 + n + m))\n−ψ′(α0 + n + m))\n+ αn\ni αm\nj\nαn\n0 αm\n0\n(ψ(αi + n) −ψ(α0 + n))\n(ψ(αj + m) −ψ(α0 + n)),\n(B.152)\nfor i ̸= j, where ψ is the Digamma function and ψ′ is the Trigamma function. Similarly,\nthe covariance Cov[pn\ni log pi, pm\ni log pi] is given by\nCov[pn\ni log pi, pm\ni log pi]\n(B.153)\n= αn+m\ni\nαn+m\n0\n\u0010\n(ψ(αi + n + m) −ψ(α0 + n + m))2\n+ ψ′(αi + n + m) −ψ′(α0 + n + m))\n+ αn\ni αm\ni\nαn\n0 αm\n0\n(ψ(αi + n) −ψ(α0 + n))\n(ψ(αi + m) −ψ(α0 + n)).\n(B.154)\nRegrettably, the equations are getting large. By abuse of notation, we introduce\na convenient shorthand before proving the lemma.\nDefinition B.8. We will denote by\nE[log ˆpn,m\ni\n] = ψ(αi + n) −ψ(α0 + n + m),\n(B.155)\nand use E[log ˆpn\ni ] for E[log ˆpn,0\ni ]. Likewise,\nCov[log ˆpn,m\ni\n, log ˆpn,m\nj\n] = ψ′(αi + n)δij −ψ′(α0 + n + m).\n(B.156)\nThis notation agrees with the proof of Equation B.133 and (B.134) in Lemma B.16.\nWith this, we can significantly simplify the previous statements:\nCorollary B.19. Given a Dirichlet distribution and random variable p ∼Dir(α),\nE[pn\ni pm\nj log pi] = αn\ni αm\nj\nαn+m\n0\nE[log ˆpn,m\ni\n],\n(B.157)\nCov[pn\ni log pi, pm\nj log pj]\n(B.158)\n= αn\ni αm\nj\nαn+m\n0\n\u0010\nE[log ˆpn,m\ni\n]E[log ˆpm,n\nj\n]\nCov[log ˆpn,m\ni\n, log ˆpn,m\nj\n]\n\u0011\n+ αn\ni αm\nj\nαn\n0 αm\n0\nE[log ˆpn\ni ]E[log ˆpm\nj ]\nfor i ̸= j, and\n(B.159)\nCov[pn\ni log pi, pm\ni log pi]\n(B.160)\nB. Reproducibility Analysis\n251\n= αn+m\ni\nαn+m\n0\n\u0012\nE[log ˆpn+m\ni\n]\n2\n+Cov[log ˆpn+m\ni\n, log ˆpn+m\ni\n]\n\u0011\n+ αn\ni αm\ni\nαn\n0 αm\n0\nE[log ˆpn\ni ]E[log ˆpm\nj ].\n(B.161)\nProof of Lemma B.18. This proof applies the well-know formula (cov) Cov[X, Y ] =\nE[X Y ] −E[X] E[Y ] once forward and once backward (rcov) E[X Y ] = Cov[X, Y ] +\nE[X] E[Y ] while applying Equation B.135 several times:\nCov[pn\ni log pi, pm\nj log pj]\n(B.162)\ncov\n= E[pn\ni log(pi) pm\nj log(pj)]\n−E[pn\ni log pi] E[pm\nj log pj]\n(B.163)\n= αn\ni αm\nj\nαn+m\n0\nE[log(ˆpi,j\ni ) log(ˆpi,j\nj )]\n−E[log ˆpi\ni] E[log pj\nj]\n(B.164)\n(rcov)\n=\nαn\ni αm\nj\nαn+m\n0\n\u0010\nCov[log ˆpi,j\ni , log ˆpi,j\nj ]\n+ E[log ˆpi,j\ni ] E[log ˆpi,j\nj ]\n\u0011\n−αn\ni αm\nj\nαn\n0 αm\n0\nE[log ˆpi\ni] E[log pj\nj],\n(B.165)\nwhere pi,j ∼Dir(αi,j) with αi,j = (. . . , αi +n, . . . , αj +m, . . .). pi/j and αi/j are defined\nanalogously. Applying Equation B.134 and Equation B.133 from Lemma B.16 yields\nthe statement. For i = j, the proof follows the same pattern.\nVariance of Softmax Entropy Now, we can prove the theorem that quantifies\nthe variance of the entropy of Y :\nTheorem B.20. Given a Dirichlet distribution and a random variable p ∼Dir(α),\nthe variance of the entropy Varp∼Dir(α)[HY ∼Cat(p)[Y ]] of the categorical distribution\nY ∼Cat(p) is given by\nVar[H[Y | p]]\n(B.166)\n=\nX\ni\nα2\ni\nα2\n0\n\u0012\nCov[log ˆp2\ni , log ˆp2\ni ] + E[log ˆp2\ni ]\n2\u0013\n+\nX\ni̸=j\nαi αj\nα2\n0\n\u0010\nCov[log ˆp1\ni , log ˆp1\nj]\n+E[log ˆp1,1\ni ] E[log ˆp1,1\nj ]\n\u0011\n−\nX\ni,j\nαi αj\nα2\n0\nE[log ˆp1\ni ]E[log ˆp1\nj].\n(B.167)\nProof. We start by applying the well-known formula Var[P\ni Xi] = P\ni,j Cov[Xi, Xj]\nand then apply Lemma B.18 repeatedly.\nC\nSingle Forward-Pass Aleatoric and\nEpistemic Uncertainty\nC.1\nExperimental Details\nC.1.1\nDirty-MNIST\nWe train for 50 epochs using SGD with a momentum of 0.9 and an initial learning rate\nof 0.1. The learning rate drops by a factor of 10 at training epochs 25 and 40. Following\nSNGP [Liu et al., 2020a], we apply online spectral normalization with one step of a\npower iteration on the convolutional weights. For 1x1 convolutions, we use the exact\nalgorithm, and for 3x3 convolutions, the approximate algorithm from Gouk et al. [2021].\nThe coefficient for SN is a hyperparameter which we set to 3 using cross-validation.\nC.1.2\nOoD Detection Training Setup\nWe train the softmax baselines on CIFAR-10/100 for 350 epochs using SGD as the\noptimizer with a momentum of 0.9, and an initial learning rate of 0.1. The learning\nrate drops by a factor of 10 at epochs 150 and 250. We train the 5-Ensemble baseline\nusing this same training setup. The SNGP and DUQ models were trained using the\nsetup of SNGP and hyperparameters mentioned in their respective papers [Liu et al.,\n2020a; van Amersfoort et al., 2020]. For models trained on ImageNet, we train for\n90 epochs with SGD optimizer, an initial learning rate of 0.1 and a weight decay\nof 1e-4. We use a learning rate warm-up decay of 0.01 along with a step scheduler\nwith step size of 30 and a step factor of 0.1.\nC.1.3\nSemantic Segmentation Training Setup\nIn Figure C.1, we plot the L2 distance between feature space means of different classes\nfor a pair of randomly chosen distant pixels on the Pascal VOC 2012 val set. We\nobserve that feature space means between pairs of different classes are more distant\ncompared to the same class irrespective of the location of the pixel for the class. This\nleads us to construct a Gaussian mean and covariance matrix per class as opposed to\none mean and one covariance matrix per class per pixel, thereby greatly reducing the\ncomputational load of fitting a GMM in semantic segmentation. Similar to classification,\nwe treat each pixel in the training set as a separate sample and fit a single Gaussian\nmean and covariance matrix per class.\nFor the semantic segmentation experiment, we use a DeepLab-v3+ [Chen et al.,\n2017] model with a ResNet-101 backbone as the architecture of choice. We train\neach of the models on Pascal VOC for 50 epochs using SGD as the optimizer, with\nC. Single Forward-Pass Aleatoric and Epistemic Uncertainty\n253\nFigure C.1: L2 distances between the feature space means of different classes for a pair of\ndistant pixels on the Pascal VOC 2012 val set: (left) Pixels (10, 255) and (500, 255), (middle)\nPixels (234, 349) and (36, 22) and (right) Pixels (300, 500) and (400, 255).\na momentum of 0.9 and a weight decay of 5e −4. We set the initial learning rate\nto 0.007 with a polynomial decay during the course of training. Finally, we trained\nwith a batch size of 32 parallelized over 4 GPUs.\nC.1.4\nCompute Resources\nEach model (ResNet-18, Wide-ResNet-28-10, ResNet-50, ResNet-110, DenseNet-121\nor VGG-16) used for the large scale active learning, CIFAR-10 vs SVHN/CIFAR-\n100/Tiny-ImageNet/CIFAR-10-C and CIFAR-100 vs SVHN/Tiny-ImageNet tasks was\ntrained on a single Nvidia Quadro RTX 6000 GPU. Each model (LeNet, VGG-16 and\nResNet-18) used to get the results in Figure 3.2 and Table 3.4 was trained on a single\nNvidia GeForce RTX 2060 GPU. Each model (ResNet-50, Wide-ResNet-50-2, VGG-16)\ntrained on ImageNet was trained using 8 Nvidia Quadro RTX 6000 GPUs.\nC.2\nAdditional Results\nIn this section, we provide details of additional results on the OoD detection task\nusing CIFAR-10 vs SVHN/CIFAR-100/Tiny-ImageNet/CIFAR-10-C and CIFAR-100\nvs SVHN/Tiny-ImageNet for ResNet-50, ResNet-110 and DenseNet-121 architectures.\nWe present results on ResNet-50, ResNet-110 and DenseNet-121 for CIFAR-10 vs\nSVHN/CIFAR-100/Tiny-ImageNet and CIFAR-100 vs SVHN/Tiny-ImageNet in Ta-\nble C.1, Table C.2 and Table C.3 respectively. We also present results on individual\ncorruption types for CIFAR-10-C for Wide-ResNet-28-10, ResNet-50, ResNet-110 and\nDenseNet-121 in Figure C.2, Figure C.3, Figure C.4 and Figure C.5 respectively.\nFinally, we provide results for various ablations on DDU. As mentioned in §3.4,\nDDU consists of a deterministic softmax model trained with appropriate inductive\nbiases. It uses softmax entropy to quantify aleatoric uncertainty and feature-space\ndensity to quantify epistemic uncertainty. In the ablation, we try to experimentally\nevaluate the following scenarios:\n1. Effect of inductive biases (sensitivity + smoothness): We want to see the\neffect of removing the proposed inductive biases (i.e. no sensitivity and smoothness\nconstraints) on the OoD detection performance of a model. To do this, we train\na VGG-16 with and without spectral normalization. Note that VGG-16 does not\nhave residual connections and hence, a VGG-16 does not follow the sensitivity and\nsmoothness (bi-Lipschitz) constraints.\nC. Single Forward-Pass Aleatoric and Epistemic Uncertainty\n254\nTable C.1: OoD detection performance of different baselines using a ResNet-50 architecture with the CIFAR-10 vs SVHN/CIFAR-100/Tiny-\nImageNet and CIFAR-100 vs SVHN/Tiny-ImageNet dataset pairs averaged over 25 runs. Note: SN stands for Spectral Normalization, JP stands\nfor Jacobian Penalty. We highlight the best deterministic and best method overall in bold for each metric.\nTrain Dataset\nMethod\nPenalty\nAleatoric Uncertainty\nEpistemic Uncertainty\nAccuracy (↑)\nECE (↓)\nAUROC\nSVHN (↑)\nCIFAR-100 (↑)\nTiny-ImageNet (↑)\nCIFAR-10\nSoftmax\n-\nSoftmax Entropy\nSoftmax Entropy\n95.04 ± 0.05\n0.97 ± 0.04\n93.80 ± 0.41\n88.91 ± 0.07\n88.32 ± 0.07\nEnergy-based [Liu et al., 2020b]\n-\nSoftmax Density\n94.48 ± 0.44\n88.84 ± 0.08\n88.45 ± 0.08\nDUQ [van Amersfoort et al., 2020]\nJP\nKernel Distance\nKernel Distance\n94.05 ± 0.11\n1.71 ± 0.07\n93.14 ± 0.43\n83.87 ± 0.27\n84.28 ± 0.26\nSNGP [Liu et al., 2020a]\nSN\nPredictive Entropy\nPredictive Entropy\n94.90 ± 0.11\n1.01 ± 0.03\n93.15 ± 0.85\n89.32 ± 0.10\n88.96 ± 0.13\nDDU (ours)\nSN\nSoftmax Entropy\nGMM Density\n94.92 ± 0.06\n1 ± 0.04\n94.77 ± 0.35\n89.98 ± 0.17\n89.12 ± 0.13\n5-Ensemble\n-\nPredictive Entropy\nPredictive Entropy\n96.06 ± 0.04\n1.65 ± 0.07\n94.75 ± 0.39\n89.87 ± 0.06\n88.69 ± 0.05\n[Lakshminarayanan et al., 2017]\nMutual Information\n94.09 ± 0.20\n89.76 ± 0.06\n89.04 ± 0.03\nTest Accuracy (↑)\nTest ECE (↓)\nSVHN (↑)\nTiny-ImageNet (↑)\nCIFAR-100\nSoftmax\n-\nSoftmax Entropy\nSoftmax Entropy\n77.91 ± 0.09\n4.32 ± 0.10\n81.32 ± 0.65\n79.83 ± 0.07\nEnergy-based [Liu et al., 2020b]\n-\nSoftmax Density\n82.05 ± 0.69\n79.61 ± 0.08\nSNGP [Liu et al., 2020a]\nSN\nPredictive Entropy\nPredictive Entropy\n74.73 ± 0.22\n7.68 ± 0.13\n82.50 ± 2.09\n77.05 ± 0.16\nDDU (ours)\nSN\nSoftmax Entropy\nGMM Density\n79.26 ± 0.16\n4.07 ± 0.06\n87.34 ± 0.64\n82.11 ± 0.20\n5-Ensemble\n-\nPredictive Entropy\nPredictive Entropy\n81.06 ± 0.07\n3.54 ± 0.12\n83.42 ± 0.89\n77.69 ± 0.12\n[Lakshminarayanan et al., 2017]\nMutual Information\n84.24 ± 0.90\n81.59 ± 0.05\nC. Single Forward-Pass Aleatoric and Epistemic Uncertainty\n255\nTable C.2: OoD detection performance of different baselines using a ResNet-110 architecture with the CIFAR-10 vs SVHN/CIFAR-100/Tiny-\nImageNet and CIFAR-100 vs SVHN/Tiny-ImageNet dataset pairs averaged over 25 runs. Note: SN stands for Spectral Normalization, JP stands\nfor Jacobian Penalty. We highlight the best deterministic and best method overall in bold for each metric.\nTrain Dataset\nMethod\nPenalty\nAleatoric Uncertainty\nEpistemic Uncertainty\nAccuracy (↑)\nECE (↓)\nAUROC\nSVHN (↑)\nCIFAR-100 (↑)\nTiny-ImageNet (↑)\nCIFAR-10\nSoftmax\n-\nSoftmax Entropy\nSoftmax Entropy\n95.08 ± 0.04\n1.02 ± 0.04\n93.12 ± 0.44\n88.7 ± 0.1\n88.07 ± 0.11\nEnergy-based [Liu et al., 2020b]\n-\nSoftmax Density\n93.67 ± 0.47\n88.60 ± 0.11\n88.13 ± 0.11\nDUQ [van Amersfoort et al., 2020]\nJP\nKernel Distance\nKernel Distance\n94.32 ± 0.17\n1.21 ± 0.07\n94.02 ± 0.45\n86.17 ± 0.35\n85.24 ± 0.21\nSNGP [Liu et al., 2020a]\nSN\nPredictive Entropy\nPredictive Entropy\n94.85 ± 0.09\n1.04 ± 0.02\n93.17 ± 0.53\n89.23 ± 0.10\n88.80 ± 0.12\nDDU (ours)\nSN\nSoftmax Entropy\nGMM Density\n94.82 ± 0.06\n1.01 ± 0.04\n95.48 ± 0.30\n90.08 ± 0.13\n89.18 ± 0.15\n5-Ensemble\n-\nPredictive Entropy\nPredictive Entropy\n96.18 ± 0.05\n1.57 ± 0.05\n95.07 ± 0.45\n90.23 ± 0.04\n89 ± 0.03\n[Lakshminarayanan et al., 2017]\nMutual Information\n94.72 ± 0.34\n89.69 ± 0.05\n88.35 ± 0.05\nTest Accuracy (↑)\nTest ECE (↓)\nAUROC SVHN (↑)\nAUROC Tiny-ImageNet (↑)\nCIFAR-100\nSoftmax\n-\nSoftmax Entropy\nSoftmax Entropy\n78.65 ± 0.10\n3.93 ± 0.13\n82.04 ± 0.57\n80.13 ± 0.07\nEnergy-based [Liu et al., 2020b]\n-\nSoftmax Density\n82.78 ± 0.60\n80.01 ± 0.09\nSNGP [Liu et al., 2020a]\nSN\nPredictive Entropy\nPredictive Entropy\n76.16 ± 0.27\n6.43 ± 0.75\n83.94 ± 0.10\n78.54 ± 0.28\nDDU (ours)\nSN\nSoftmax Entropy\nGMM Density\n78.89 ± 0.17\n3.79 ± 0.07\n88.66 ± 0.56\n82.58 ± 0.24\n5-Ensemble\n-\nPredictive Entropy\nPredictive Entropy\n81.80 ± 0.10\n3.67 ± 0.11\n83.68 ± 0.33\n81.12 ± 0.13\n[Lakshminarayanan et al., 2017]\nMutual Information\n85.11 ± 0.57\n81.94 ± 0.06\nC. Single Forward-Pass Aleatoric and Epistemic Uncertainty\n256\nTable C.3: OoD detection performance of different baselines using a DenseNet-121 architecture with the CIFAR-10 vs SVHN/CIFAR-100/Tiny-\nImageNet and CIFAR-100 vs SVHN/Tiny-ImageNet dataset pairs averaged over 25 runs. Note: SN stands for Spectral Normalization, JP stands\nfor Jacobian Penalty. We highlight the best deterministic and best method overall in bold for each metric.\nTrain Dataset\nMethod\nPenalty\nAleatoric Uncertainty\nEpistemic Uncertainty\nAccuracy (↑)\nECE (↓)\nAUROC\nSVHN (↑)\nCIFAR-100 (↑)\nTiny-ImageNet (↑)\nCIFAR-10\nSoftmax\n-\nSoftmax Entropy\nSoftmax Entropy\n95.16 ± 0.03\n1.10 ± 0.04\n94 ± 0.44\n87.55 ± 0.11\n86.99 ± 0.12\nEnergy-based [Liu et al., 2020b]\n-\nSoftmax Density\n94.07 ± 0.54\n86.73 ± 0.15\n86.43 ± 0.16\nDUQ [van Amersfoort et al., 2020]\nJP\nKernel Distance\nKernel Distance\n95.02 ± 0.14\n1.08 ± 0.08\n94.67 ± 0.41\n87.38 ± 0.21\n86.72 ± 0.14\nSNGP [Liu et al., 2020a]\nSN\nPredictive Entropy\nPredictive Entropy\n94.31 ± 0.21\n1.08 ± 0.10\n94.48 ± 0.34\n88.86 ± 0.46\n88.40 ± 0.48\nDDU (ours)\nSN\nSoftmax Entropy\nGMM Density\n95.21 ± 0.03\n1.05 ± 0.03\n96.21 ± 0.31\n90.84 ± 0.06\n89.70 ± 0.06\n5-Ensemble\n-\nPredictive Entropy\nPredictive Entropy\n96.18 ± 0.05\n1.07 ± 0.07\n95.78 ± 0.11\n90.65 ± 0.03\n89.62 ± 0.06\n[Lakshminarayanan et al., 2017]\nMutual Information\n95.75 ± 0.10\n90.71 ± 0.04\n89.34 ± 0.06\nTest Accuracy (↑)\nTest ECE (↓)\nSVHN (↑)\nTiny-ImageNet (↑)\nCIFAR-100\nSoftmax\n-\nSoftmax Entropy\nSoftmax Entropy\n79.02 ± 0.08\n4.11 ± 0.08\n85.86 ± 0.42\n81.10 ± 0.07\nEnergy-based [Liu et al., 2020b]\n-\nSoftmax Density\n87.09 ± 0.49\n80.84 ± 0.08\nSNGP [Liu et al., 2020a]\nSN\nPredictive Entropy\nPredictive Entropy\n79.15 ± 0.15\n6.73 ± 0.10\n85.00 ± 0.12\n79.76 ± 0.15\nDDU (ours)\nSN\nSoftmax Entropy\nGMM Density\n79.15 ± 0.07\n4.11 ± 0.06\n88.44 ± 0.55\n81.85 ± 0.11\n5-Ensemble\n-\nPredictive Entropy\nPredictive Entropy\n81.01 ± 0.13\n4.81 ± 0.05\n88.32 ± 0.61\n81.45 ± 0.12\n[Lakshminarayanan et al., 2017]\nMutual Information\n88.36 ± 0.17\n81.73 ± 0.06\nC. Single Forward-Pass Aleatoric and Epistemic Uncertainty\n257\nTable C.4: OoD detection performance of different ablations trained on CIFAR-10 using Wide-ResNet-28-10 and VGG-16 architectures with\nSVHN, CIFAR-100 and Tiny-ImageNet as OoD datasets averaged over 25 runs. Note: SN stands for Spectral Normalization. We highlight the\nbest deterministic and best method overall in bold for each metric.\nAblations\nAleatoric Uncertainty\nEpistemic Uncertainty\nTest Accuracy (↑)\nTest ECE (↓)\nAUROC\nArchitecture\nEnsemble\nResidual\nConnections\nSN\nGMM\nSVHN (↑)\nCIFAR-100 (↑)\nTiny-ImageNet (↑)\nAblations\nAleatoric Uncertainty\nEpistemic Uncertainty\nTest Accuracy (↑)\nTest ECE (↓)\nAUROC\nArchitecture\nEnsemble\nResidual\nConnections\nSN\nGMM\nSVHN (↑)\nCIFAR-100 (↑)\nTiny-ImageNet (↑)\nWide-ResNet-28-10\n✗\n✓\n✗\n✗\nSoftmax Entropy\nSoftmax Entropy\n95.98 ± 0.02\n0.85 ± 0.02\n94.44 ± 0.43\n89.39 ± 0.06\n88.42 ± 0.05\nSoftmax Density\n94.56 ± 0.51\n88.89 ± 0.07\n88.11 ± 0.06\n✓\nSoftmax Entropy\nGMM Density\n95.98 ± 0.02\n0.85 ± 0.02\n96.08 ± 0.25\n90.94 ± 0.03\n90.62 ± 0.05\n✓\n✗\nSoftmax Entropy\nSoftmax Entropy\n95.97 ± 0.03\n0.85 ± 0.04\n94.05 ± 0.26\n90.02 ± 0.07\n89.07 ± 0.06\nSoftmax Density\n94.31 ± 0.33\n89.78 ± 0.08\n88.96 ± 0.07\n✓\nSoftmax Entropy\nGMM Density\n95.97 ± 0.03\n0.85 ± 0.04\n97.86 ± 0.19\n91.34 ± 0.04\n91.07 ± 0.05\n✓\n✓\n✗\n✗\nPredictive Entropy\nPredictive Entropy\n96.59 ± 0.02\n0.76 ± 0.03\n97.73 ± 0.31\n92.13 ± 0.02\n90.06 ± 0.03\nMutual Information\n97.18 ± 0.19\n91.33 ± 0.03\n90.90 ± 0.03\nVGG-16\n✗\n✓\n✗\n✗\nSoftmax Entropy\nSoftmax Entropy\n93.63 ± 0.04\n1.64 ± 0.03\n85.76 ± 0.84\n82.48 ± 0.14\n83.07 ± 0.12\nSoftmax Density\n84.24 ± 1.04\n81.91 ± 0.17\n82.82 ± 0.14\n✓\nSoftmax Entropy\nGMM Density\n93.63 ± 0.04\n1.64 ± 0.03\n89.25 ± 0.36\n86.55 ± 0.10\n86.78 ± 0.09\n✓\n✗\nSoftmax Entropy\nSoftmax Entropy\n93.62 ± 0.04\n1.78 ± 0.04\n87.54 ± 0.41\n82.71 ± 0.09\n83.33 ± 0.08\nSoftmax Density\n86.28 ± 0.51\n82.15 ± 0.11\n83.07 ± 0.10\n✓\nSoftmax Entropy\nGMM Density\n93.62 ± 0.04\n1.78 ± 0.04\n89.62 ± 0.37\n86.37 ± 0.14\n86.63 ± 0.11\n✓\n✓\n✗\n✗\nPredictive Entropy\nPredictive Entropy\n94.9 ± 0.05\n2.03 ± 0.03\n92.80 ± 0.18\n89.01 ± 0.08\n87.66 ± 0.08\nMutual Information\n91 ± 0.22\n88.43 ± 0.08\n88.74 ± 0.05\nC. Single Forward-Pass Aleatoric and Epistemic Uncertainty\n258\nTable C.5: OoD detection performance of different ablations trained on CIFAR-100 using Wide-ResNet-28-10 and VGG-16 architectures with\nSVHN and Tiny-ImageNet as the OoD dataset averaged over 25 runs. Note: SN stands for Spectral Normalization. We highlight the best\ndeterministic and best method overall in bold for each metric.\nAblations\nAleatoric Uncertainty\nEpistemic Uncertainty\nTest Accuracy (↑)\nTest ECE (↓)\nAUROC\nArchitecture\nEnsemble\nResidual\nConnections\nSN\nGMM\nSVHN (↑)\nTiny-ImageNet (↑)\nWide-ResNet-28-10\n✗\n✓\n✗\n✗\nSoftmax Entropy\nSoftmax Entropy\n80.26 ± 0.06\n4.62 ± 0.06\n77.42 ± 0.57\n81.53 ± 0.05\nSoftmax Density\n78.00 ± 0.63\n81.33 ± 0.06\n✓\nSoftmax Entropy\nGMM Density\n80.26 ± 0.06\n4.62 ± 0.06\n87.54 ± 0.61\n78.13 ± 0.08\n✓\n✗\nSoftmax Entropy\nSoftmax Entropy\n80.98 ± 0.06\n4.10 ± 0.08\n85.37 ± 0.36\n82.57 ± 0.03\nSoftmax Density\n86.41 ± 0.38\n82.49 ± 0.04\n✓\nSoftmax Entropy\nGMM Density\n80.98 ± 0.06\n4.10 ± 0.08\n87.53 ± 0.62\n83.13 ± 0.06\n✓\n✓\n✗\n✗\nPredictive Entropy\nPredictive Entropy\n82.79 ± 0.10\n3.32 ± 0.09\n79.54 ± 0.91\n82.95 ± 0.09\nMutual Information\n77.00 ± 1.54\n82.82 ± 0.04\nVGG-16\n✗\n✓\n✗\n✗\nSoftmax Entropy\nSoftmax Entropy\n73.48 ± 0.05\n4.46 ± 0.05\n76.73 ± 0.72\n76.43 ± 0.05\nSoftmax Density\n77.70 ± 0.86\n74.68 ± 0.07\n✓\nSoftmax Entropy\nGMM Density\n73.48 ± 0.05\n4.46 ± 0.05\n75.65 ± 0.95\n74.32 ± 1.73\n✓\n✗\nSoftmax Entropy\nSoftmax Entropy\n73.58 ± 0.06\n4.32 ± 0.06\n77.21 ± 0.77\n76.59 ± 0.06\nSoftmax Density\n77.76 ± 0.90\n74.86 ± 0.08\n✓\nSoftmax Entropy\nGMM Density\n73.58 ± 0.06\n4.32 ± 0.06\n75.99 ± 1.23\n74.06 ± 1.67\n✓\n✓\n✗\n✗\nPredictive Entropy\nPredictive Entropy\n77.84 ± 0.11\n5.32 ± 0.10\n79.62 ± 0.73\n78.66 ± 06\nMutual Information\n72.07 ± 0.48\n76.27 ± 0.05\nC. Single Forward-Pass Aleatoric and Epistemic Uncertainty\n259\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nBrightness Intensity\n0.50\n0.52\n0.54\n0.56\n0.58\n0.60\n0.62\n0.64\n0.66\nAUROC\nDDU\nEnsemble\nSNGP\nSoftmax\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nContrast Intensity\n0.5\n0.6\n0.7\n0.8\n0.9\nAUROC\nDDU\nEnsemble\nSNGP\nSoftmax\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nDefocus Blur Intensity\n0.5\n0.6\n0.7\n0.8\n0.9\nAUROC\nDDU\nEnsemble\nSNGP\nSoftmax\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nElastic Transform Intensity\n0.600\n0.625\n0.650\n0.675\n0.700\n0.725\n0.750\n0.775\n0.800\nAUROC\nDDU\nEnsemble\nSNGP\nSoftmax\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nFog Intensity\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\nAUROC\nDDU\nEnsemble\nSNGP\nSoftmax\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nFrost Intensity\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\nAUROC\nDDU\nEnsemble\nSNGP\nSoftmax\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nGaussian Blur Intensity\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAUROC\nDDU\nEnsemble\nSNGP\nSoftmax\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nGaussian Noise Intensity\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nAUROC\nDDU\nEnsemble\nSNGP\nSoftmax\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nGlass Blur Intensity\n0.82\n0.84\n0.86\n0.88\n0.90\n0.92\n0.94\n0.96\n0.98\nAUROC\nDDU\nEnsemble\nSNGP\nSoftmax\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nImpulse Noise Intensity\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nAUROC\nDDU\nEnsemble\nSNGP\nSoftmax\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nJpeg Compression Intensity\n0.625\n0.650\n0.675\n0.700\n0.725\n0.750\n0.775\n0.800\n0.825\nAUROC\nDDU\nEnsemble\nSNGP\nSoftmax\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nMotion Blur Intensity\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\nAUROC\nDDU\nEnsemble\nSNGP\nSoftmax\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nPixelate Intensity\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\nAUROC\nDDU\nEnsemble\nSNGP\nSoftmax\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nSaturate Intensity\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\nAUROC\nDDU\nEnsemble\nSNGP\nSoftmax\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nShot Noise Intensity\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nAUROC\nDDU\nEnsemble\nSNGP\nSoftmax\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nSnow Intensity\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\nAUROC\nDDU\nEnsemble\nSNGP\nSoftmax\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nSpatter Intensity\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\nAUROC\nDDU\nEnsemble\nSNGP\nSoftmax\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nSpeckle Noise Intensity\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nAUROC\nDDU\nEnsemble\nSNGP\nSoftmax\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nZoom Blur Intensity\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\nAUROC\nDDU\nEnsemble\nSNGP\nSoftmax\nFigure C.2: AUROC vs corruption intensity for all corruption types in CIFAR-10-C with\nWide-ResNet-28-10 as the architecture and baselines: Softmax Entropy, Ensemble (using\nPredictive Entropy as uncertainty), SNGP and DDU feature density.\n2. Effect of sensitivity alone: Since residual connections make a model sensitive to\nchanges in the input space by lower bounding its Lipschitz constant, we also want\nto see how a network performs with just the sensitivity constraint alone. To observe\nthis, we train a Wide-ResNet-28-10 without spectral normalization (i.e. no explicit\nupper bound on the Lipschitz constant of the model).\n3. Metrics for aleatoric and epistemic uncertainty: With the above combinations,\nwe try to observe how different metrics for aleatoric and epistemic uncertainty\nperform. To quantify aleatoric uncertainty, we use the softmax entropy of the model.\nOn the other hand, to quantify the epistemic uncertainty, we use i) the softmax\nentropy, ii) the softmax density [Liu et al., 2020b] or iii) the GMM feature density\n(as described in §3.4).\nFor the purposes of comparison, we also present scores obtained by a 5-Ensemble\nof the respective architectures (i.e. Wide-ResNet-28-10 and VGG-16) in Table C.4 for\nC. Single Forward-Pass Aleatoric and Epistemic Uncertainty\n260\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nBrightness Intensity\n0.50\n0.52\n0.54\n0.56\n0.58\n0.60\n0.62\nAUROC\nDDU\nEnsemble\nSNGP\nSoftmax\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nContrast Intensity\n0.5\n0.6\n0.7\n0.8\n0.9\nAUROC\nDDU\nEnsemble\nSNGP\nSoftmax\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nDefocus Blur Intensity\n0.5\n0.6\n0.7\n0.8\n0.9\nAUROC\nDDU\nEnsemble\nSNGP\nSoftmax\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nElastic Transform Intensity\n0.60\n0.65\n0.70\n0.75\n0.80\nAUROC\nDDU\nEnsemble\nSNGP\nSoftmax\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nFog Intensity\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\nAUROC\nDDU\nEnsemble\nSNGP\nSoftmax\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nFrost Intensity\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\nAUROC\nDDU\nEnsemble\nSNGP\nSoftmax\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nGaussian Blur Intensity\n0.5\n0.6\n0.7\n0.8\n0.9\nAUROC\nDDU\nEnsemble\nSNGP\nSoftmax\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nGaussian Noise Intensity\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\nAUROC\nDDU\nEnsemble\nSNGP\nSoftmax\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nGlass Blur Intensity\n0.84\n0.86\n0.88\n0.90\n0.92\n0.94\n0.96\nAUROC\nDDU\nEnsemble\nSNGP\nSoftmax\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nImpulse Noise Intensity\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nAUROC\nDDU\nEnsemble\nSNGP\nSoftmax\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nJpeg Compression Intensity\n0.600\n0.625\n0.650\n0.675\n0.700\n0.725\n0.750\n0.775\n0.800\nAUROC\nDDU\nEnsemble\nSNGP\nSoftmax\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nMotion Blur Intensity\n0.60\n0.65\n0.70\n0.75\n0.80\nAUROC\nDDU\nEnsemble\nSNGP\nSoftmax\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nPixelate Intensity\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\nAUROC\nDDU\nEnsemble\nSNGP\nSoftmax\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nSaturate Intensity\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\nAUROC\nDDU\nEnsemble\nSNGP\nSoftmax\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nShot Noise Intensity\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\nAUROC\nDDU\nEnsemble\nSNGP\nSoftmax\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nSnow Intensity\n0.60\n0.65\n0.70\n0.75\n0.80\nAUROC\nDDU\nEnsemble\nSNGP\nSoftmax\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nSpatter Intensity\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\nAUROC\nDDU\nEnsemble\nSNGP\nSoftmax\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nSpeckle Noise Intensity\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\nAUROC\nDDU\nEnsemble\nSNGP\nSoftmax\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nZoom Blur Intensity\n0.65\n0.70\n0.75\n0.80\n0.85\nAUROC\nDDU\nEnsemble\nSNGP\nSoftmax\nFigure C.3: AUROC vs corruption intensity for all corruption types in CIFAR-10-C with\nResNet-50 as the architecture and baselines: Softmax Entropy, Ensemble (using Predictive\nEntropy as uncertainty), SNGP and DDU feature density.\nCIFAR-10 vs SVHN/CIFAR-100 and in Table C.5 for CIFAR-100 vs SVHN. Based\non these results, we can make the following observations (in addition to the ones\nwe make in §3.5.3):\nInductive biases are important for feature density. From the AUROC scores\nin Table C.4, we can see that using the feature density of a GMM in VGG-16 without\nthe proposed inductive biases yields significantly lower AUROC scores as compared to\nWide-ResNet-28-10 with inductive biases. In fact, in none of the datasets is the feature\ndensity of a VGG able to outperform its corresponding ensemble. This provides yet\nmore evidence (in addition to Figure 3.2) to show that the GMM feature density alone\ncannot estimate epistemic uncertainty in a model that suffers from feature collapse.\nWe need sensitivity and smoothness conditions (see §3.2) on the feature space of the\nmodel to obtain feature densities that capture epistemic uncertainty.\nSensitivity creates a bigger difference than smoothness. We note that the\nC. Single Forward-Pass Aleatoric and Epistemic Uncertainty\n261\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nBrightness Intensity\n0.50\n0.52\n0.54\n0.56\n0.58\n0.60\n0.62\nAUROC\nDDU\nEnsemble\nSNGP\nSoftmax\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nContrast Intensity\n0.5\n0.6\n0.7\n0.8\n0.9\nAUROC\nDDU\nEnsemble\nSNGP\nSoftmax\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nDefocus Blur Intensity\n0.5\n0.6\n0.7\n0.8\n0.9\nAUROC\nDDU\nEnsemble\nSNGP\nSoftmax\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nElastic Transform Intensity\n0.600\n0.625\n0.650\n0.675\n0.700\n0.725\n0.750\n0.775\n0.800\nAUROC\nDDU\nEnsemble\nSNGP\nSoftmax\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nFog Intensity\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\nAUROC\nDDU\nEnsemble\nSNGP\nSoftmax\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nFrost Intensity\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\nAUROC\nDDU\nEnsemble\nSNGP\nSoftmax\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nGaussian Blur Intensity\n0.5\n0.6\n0.7\n0.8\n0.9\nAUROC\nDDU\nEnsemble\nSNGP\nSoftmax\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nGaussian Noise Intensity\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\nAUROC\nDDU\nEnsemble\nSNGP\nSoftmax\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nGlass Blur Intensity\n0.84\n0.86\n0.88\n0.90\n0.92\n0.94\n0.96\nAUROC\nDDU\nEnsemble\nSNGP\nSoftmax\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nImpulse Noise Intensity\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nAUROC\nDDU\nEnsemble\nSNGP\nSoftmax\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nJpeg Compression Intensity\n0.600\n0.625\n0.650\n0.675\n0.700\n0.725\n0.750\n0.775\n0.800\nAUROC\nDDU\nEnsemble\nSNGP\nSoftmax\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nMotion Blur Intensity\n0.60\n0.65\n0.70\n0.75\n0.80\nAUROC\nDDU\nEnsemble\nSNGP\nSoftmax\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nPixelate Intensity\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\nAUROC\nDDU\nEnsemble\nSNGP\nSoftmax\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nSaturate Intensity\n0.55\n0.60\n0.65\n0.70\nAUROC\nDDU\nEnsemble\nSNGP\nSoftmax\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nShot Noise Intensity\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\nAUROC\nDDU\nEnsemble\nSNGP\nSoftmax\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nSnow Intensity\n0.60\n0.65\n0.70\n0.75\n0.80\nAUROC\nDDU\nEnsemble\nSNGP\nSoftmax\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nSpatter Intensity\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\nAUROC\nDDU\nEnsemble\nSNGP\nSoftmax\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nSpeckle Noise Intensity\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\nAUROC\nDDU\nEnsemble\nSNGP\nSoftmax\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nZoom Blur Intensity\n0.65\n0.70\n0.75\n0.80\n0.85\nAUROC\nDDU\nEnsemble\nSNGP\nSoftmax\nFigure C.4: AUROC vs corruption intensity for all corruption types in CIFAR-10-C with\nResNet-110 as the architecture and baselines: Softmax Entropy, Ensemble (using Predictive\nEntropy as uncertainty), SNGP and DDU feature density.\ndifference between AUROC obtained from feature density between Wide-ResNet-28-10\nmodels with and without spectral normalization is minimal. Although Wide-ResNet-\n28-10 with spectral normalization (i.e. smoothness constraints) still outperforms its\ncounterpart without spectral normalization, the small difference between the AUROC\nscores indicates that it might be the residual connections (i.e. sensitivity constraints)\nthat make the model detect OoD samples better. This observation is also intuitive as\na sensitive feature extractor should map OoD samples farther from iD ones.\nDDU as a simple baseline. In DDU, we use the softmax output of a model to\nget aleatoric uncertainty. We use the GMM’s feature-density to estimate the epistemic\nuncertainty. Hence, DDU does not suffer from miscalibration as the softmax outputs\ncan be calibrated using post-hoc methods like temperature scaling. At the same\ntime, the feature-densities of the model are not affected by temperature scaling and\ncapture epistemic uncertainty well.\nC. Single Forward-Pass Aleatoric and Epistemic Uncertainty\n262\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nBrightness Intensity\n0.50\n0.52\n0.54\n0.56\n0.58\n0.60\nAUROC\nDDU\nEnsemble\nSNGP\nSoftmax\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nContrast Intensity\n0.5\n0.6\n0.7\n0.8\n0.9\nAUROC\nDDU\nEnsemble\nSNGP\nSoftmax\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nDefocus Blur Intensity\n0.5\n0.6\n0.7\n0.8\n0.9\nAUROC\nDDU\nEnsemble\nSNGP\nSoftmax\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nElastic Transform Intensity\n0.625\n0.650\n0.675\n0.700\n0.725\n0.750\n0.775\n0.800\nAUROC\nDDU\nEnsemble\nSNGP\nSoftmax\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nFog Intensity\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\nAUROC\nDDU\nEnsemble\nSNGP\nSoftmax\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nFrost Intensity\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\nAUROC\nDDU\nEnsemble\nSNGP\nSoftmax\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nGaussian Blur Intensity\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAUROC\nDDU\nEnsemble\nSNGP\nSoftmax\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nGaussian Noise Intensity\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nAUROC\nDDU\nEnsemble\nSNGP\nSoftmax\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nGlass Blur Intensity\n0.82\n0.84\n0.86\n0.88\n0.90\n0.92\n0.94\nAUROC\nDDU\nEnsemble\nSNGP\nSoftmax\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nImpulse Noise Intensity\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nAUROC\nDDU\nEnsemble\nSNGP\nSoftmax\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nJpeg Compression Intensity\n0.65\n0.70\n0.75\n0.80\nAUROC\nDDU\nEnsemble\nSNGP\nSoftmax\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nMotion Blur Intensity\n0.65\n0.70\n0.75\n0.80\n0.85\nAUROC\nDDU\nEnsemble\nSNGP\nSoftmax\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nPixelate Intensity\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\nAUROC\nDDU\nEnsemble\nSNGP\nSoftmax\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nSaturate Intensity\n0.525\n0.550\n0.575\n0.600\n0.625\n0.650\n0.675\n0.700\nAUROC\nDDU\nEnsemble\nSNGP\nSoftmax\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nShot Noise Intensity\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nAUROC\nDDU\nEnsemble\nSNGP\nSoftmax\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nSnow Intensity\n0.60\n0.65\n0.70\n0.75\n0.80\nAUROC\nDDU\nEnsemble\nSNGP\nSoftmax\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nSpatter Intensity\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\nAUROC\nDDU\nEnsemble\nSNGP\nSoftmax\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nSpeckle Noise Intensity\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nAUROC\nDDU\nEnsemble\nSNGP\nSoftmax\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nZoom Blur Intensity\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\nAUROC\nDDU\nEnsemble\nSNGP\nSoftmax\nFigure C.5: AUROC vs corruption intensity for all corruption types in CIFAR-10-C with\nDenseNet-121 as the architecture and baselines: Softmax Entropy, Ensemble (using Predictive\nEntropy as uncertainty), SNGP and DDU feature density.\nC.3\nAdditional Ablations & Toy Experiments\nHere, we provide details for the toy experiments mentioned in the main part of the\nthesis which are visualized in Figure 3.2, Figure 3.3 and Figure 3.13.\nC.3.1\nQUBIQ Challenge\nIn this section, we evaluate DDU’s performance on the real-world QUBIQ challenge\nrelated to biomedical imaging. QUBIQ has a total of 7 binary segmentation tasks in\n4 biomedical imaging datasets with multiple annotations per image. The task is to\npredict the distribution of source labels with a mask of values between 0 and 1. For\nevaluation, the annotations are averaged to provide a continuous ground-truth. The\nprediction mask and continuous ground-truth are binarized by thresholding between\n[0, 1] and a Dice score is computed between the resulting binary masks. The average\nC. Single Forward-Pass Aleatoric and Epistemic Uncertainty\n263\nTable C.6: Dice scores for the QUBIQ 2021 challenge.\nMethod\nSoftmax\nEnergy\n3-Ensemble PE\nDDU\nDice Score (↑)\n78.4 ± 1.31\n77.31 ± 1.5\n82.25 ± 0.83\n82.63 ± 1.08\nDataset\nMetric\nSoftmax & Energy\nDUQ\nSNGP\nDDU\n5-Ensemble\nCIFAR-10\nECE\n0.85 ± 0.02\n1.55 ± 0.08\n1.8 ± 0.1\n0.85 ± 0.04\n0.76 ± 0.03\nTACE\n0.63 ± 0.01\n0.84 ± 0.03\n0.9 ± 0.04\n0.61 ± 0.01\n0.48 ± 0.01\nNLL\n0.18 ± 0.06\n0.23 ± 0.07\n0.27 ± 0.08\n0.16 ± 0.06\n0.11 ± 0.02\nCIFAR-100\nECE\n4.62 ± 0.06\n-\n4.33 ± 0.01\n4.1 ± 0.08\n3.32 ± 0.09\nTACE\n1.31 ± 0.02\n-\n1.23 ± 0.04\n1.06 ± 0.03\n0.58 ± 0.03\nNLL\n1.17 ± 0.13\n-\n0.92 ± 0.16\n0.86 ± 0.14\n0.73 ± 0.09\nTable C.7: Calibration error scores ECE%, TACE% and NLL for WRN-28-10.\ndice score across thresholds, images and tasks is reported. Note that in the continuous\nground-truth, 0.5 indicates maximum uncertainty and values above or below indicate\nlower uncertainty. Thus, for our comparison, we scale all uncertainty values to the\nrange u ∈[0, 0.5] and use p + u if p = 0 and p −u if p = 1, where p is the binary\nprediction. We use a UNet model with a ResNet encoder and report the Dice scores\naveraged over 5 runs in Table C.6. Even on this real-world dataset, DDU performs\nas well as ensembles and outperforms Softmax and Energy baselines.\nC.3.2\nAdditional Calibration Metrics\nIn Table C.7, in addition to ECE%, we provide additional calibration error scores:\ntemperature scaled Thresholded Adaptive Calibration Error (TACE) % and Negative\nLog Likelihood (NLL) for Wide-ResNet-28-10 trained on CIFAR-10/100—the main\nresults for this can be found in Table 3.5. Here, we see that the results for TACE and\nNLL are consistent with what we see for ECE. Ensembles produce the most calibrated\nmodels and among deterministic baselines, DDU is the best calibrated.\nC.4\nBig Figure 1\nC. Single Forward-Pass Aleatoric and Epistemic Uncertainty\n264\nMNIST\nAmbiguous-MNIST\nFashion-MNIST (OoD)\nDirty-MNIST (iD)\n(a) Dirty-MNIST (iD) and Fashion-MNIST (OoD)\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nEntropy LeNet\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\n0.16\nFraction\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nEntropy VGG-16\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150\n0.175\n0.200\nFraction\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nEntropy ResNet-18+SN\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\n0.16\nFraction\n(b) Softmax entropy\n250\n200\n150\n100\n50\n0\n50\n100\n150\nLog Density LeNet\n0.00\n0.02\n0.04\n0.06\n0.08\nFraction\n1000\n1250\n1500\n1750\n2000\n2250\n2500\n2750\n3000\nLog Density VGG-16\n0.00\n0.01\n0.02\n0.03\n0.04\nFraction\n1000\n500\n0\n500\n1000\n1500\n2000\n2500\nLog Density ResNet-18+SN\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\nFraction\n(c) Feature-space density\nFigure C.6:\nDisentangling aleatoric and epistemic uncertainty on Dirty-MNIST (iD) and Fashion-MNIST (OoD) (a) requires using softmax\nentropy (b) and feature-space density (GMM) (c) with appropriate inductive biases (ResNet-18+SN vs LeNet & VGG-16 without them). Enlarged\nversion.\n(b): Softmax entropy captures aleatoric uncertainty for iD data (Dirty-MNIST), thereby separating unambiguous MNIST samples and\nAmbiguous-MNIST samples. However, iD and OoD are confounded: softmax entropy has arbitrary values for OoD, indistinguishable from iD. (c):\nWith appropriate inductive biases (DDU with ResNet-18+SN), iD and OoD densities do not overlap, capturing epistemic uncertainty. However,\nwithout appropriate inductive biases (LeNet & VGG-16), feature density suffers from feature collapse: iD and OoD densities overlap. Generally,\nfeature-space density confounds unambiguous and ambiguous iD samples as their densities overlap. Note: Unambiguous MNIST samples and\nAmbiguous-MNIST samples are shown as stacked histograms with the total fractions adding up to 1 for Dirty-MNIST.\nD\nDiverse Batch Acquisition for Bayesian\nActive Learning\nD.1\nProof of Submodularity\nNemhauser et al. [1978] show that if a function is submodular, then a greedy algorithm\nlike algorithm 2 is 1 −1/e-approximate. Here, we show that aBatchBALD is submodular.\nWe will show that aBatchBALD satisfies the following equivalent definition of submodu-\nlarity:\nDefinition D.1. A function f defined on subsets of Ωis called submodular if for every\nset A ⊂Ωand two non-identical points X, Y ∈Ω\\ A:\nf(A ∪{X}) + f(A ∪{Y }) ≥f(A ∪{X, Y }) + f(A)\n(D.1)\nSubmodularity expresses that there are \"diminishing returns\" for adding addi-\ntional points to f.\nLemma D.1. aBatchBALD(A, p(ωωω)) := I[A;ΩΩΩ] is submodular for A ⊂Dpool.\nProof. Let X, Y ∈Dpool, X ̸= Y . We start by substituting the definition of aBatchBALD\ninto (D.1) and subtracting I[A;ΩΩΩ] twice on both sides, using that I[A∪B;ΩΩΩ]−I[B;ΩΩΩ] =\nI[A;ΩΩΩ| B]:\nI[A ∪{Y };ΩΩΩ] + I[A ∪{X};ΩΩΩ] ≥I[A ∪{X, Y };ΩΩΩ] + I[A;ΩΩΩ]\n(D.2)\n⇔I[Y ;ΩΩΩ| A] + I[X;ΩΩΩ| A] ≥I[X, y;ΩΩΩ| A].\n(D.3)\nWe rewrite the left-hand side using the definition of the mutual information I[A; B] =\nH[A] −H[A | B] and reorder:\nI[y;ΩΩΩ| A] + I[x;ΩΩΩ| A]\n(D.4)\n= H[X | A] + H[X | A]\n|\n{z\n}\n≥H[X,Y |A]\n−(H[X | A,ΩΩΩ] + H[Y | A,ΩΩΩ])\n|\n{z\n}\n=H[X,Y |A,ΩΩΩ]\n(D.5)\n≥H[X, Y | A] −H[X, Y | A,ΩΩΩ]\n(D.6)\n= I[X, Y ;ΩΩΩ| A],\n(D.7)\nwhere we have used that entropies are subadditive in general and additive given\nX ⊥⊥Y | ΩΩΩ.\nFollowing Nemhauser et al. [1978], we can conclude that algorithm 2 is 1 −1/e-\napproximate.\nD. Diverse Batch Acquisition for Bayesian Active Learning\n266\nD.2\nBALD as an Upper-Bound of BatchBALD\nIn the following section, we show that BALD approximates BatchBALD. The BALD\nscore is an upper bound of the BatchBALD score for any candidate batch.\nUsing the subadditivity of information entropy and the independence of the yi\ngiven ωωω, we show that BALD is an approximation of BatchBALD and is always an\nupper bound on the respective BatchBALD score:\naBatchBALD\n\u0010\n{x1, . . . , xb} , p(ωωω | Dtrain)\n\u0011\n(D.8)\n= I[Y acq\n1..K ;ΩΩΩ| xacq\n1..K, Dtrain]\n(D.9)\n= H[Y acq\n1..K | xacq\n1..K, Dtrain] −H[Y acq\n1..K | xacq\n1..K,ΩΩΩ, Dtrain]\n(D.10)\n≤\nK\nX\ni=1\nH[Yi | xi, Dtrain] −\nK\nX\ni=1\nH[Yi | xi,ΩΩΩ, Dtrain]\n(D.11)\n=\nK\nX\ni=1\nI[Yi;ΩΩΩ| xi, Dtrain] = aBALD\n\u0010\n{xacq\n1..K} , p(ωωω | Dtrain)\n\u0011\n(D.12)\nRelevance for the active training loop. We see that the active training loop\nas a whole is computing a greedy 1 −1/e-approximation of the mutual information of\nall acquired data points over all acquisitions with the model parameters.\nD.3\nSampling of Configurations\nWe are using the same notation as in section 4.1.2. We factor p(y1..n | ωωω) to avoid\nrecomputations and rewrite H[Y1..n] as:\nH[Y1..n] = Ep(ωωω) Ep(y1..n|ωωω)[−log p(y1..n)]\n(D.13)\n= Ep(ωωω) Ep(y1:n−1|ωωω) p(yn|ωωω)[−log p(y1..n)]\n(D.14)\n= Ep(ωωω) Ep(y1:n−1|ωωω) Ep(yn|ωωω)[−log p(y1..n)]\n(D.15)\nTo be flexible in the way we sample y1:n−1, we perform importance sampling of\np(y1:n−1 | ωωω) using p(y1:n−1), and, assuming we also have m samples ˆy1:n−1 from\np(y1:n−1), we can approximate:\nH[Y1..n] = Ep(ωωω) Ep(y1:n−1)[p(y1:n−1 | ωωω)\np(y1:n−1)\nEp(yn|ωωω)[−log p(y1:n)]]\n(D.16)\n= Ep(y1:n−1) Ep(ωωω) Ep(yn|ωωω)[−p(y1:n−1 | ωωω)\np(y1:n−1)\nlog Ep(ωωω)[p(y1:n−1 | ωωω) p(y1:n | ωωω)]]\n(D.17)\n≈−1\nm\nm\nX\nˆy1:n−1\nX\nˆyn\n1\nk\nP\nˆωωωj p(ˆy1:n−1 | ˆωωωj) p(ˆyn | ˆωωωj)\np(ˆy1:n−1)\nlog\n\n1\nk\nX\nˆωωωj\np(ˆy1:n−1 | ˆωωωj) p(ˆyn | ˆωωωj)\n\n\n(D.18)\n= −1\nm\nm\nX\nˆy1:n−1\nX\nˆyn\n\u0010 ˆP1:n−1 ˆP T\nn\n\u0011\nˆy1:n−1,ˆyn\n\u0010 ˆP1:n−11k,1\n\u0011\nˆy1:n−1\nlog\n\u00121\nk\n\u0010 ˆP1:n−1 ˆP T\nn\n\u0011\nˆy1:n−1,ˆyn\n\u0013\n,\n(D.19)\nwhere we store p(ˆy1:n−1 | ˆωωωj) in a matrix ˆP1:n−1 of shape m × k and p(ˆyn | ˆωωωj) in a\nmatrix ˆPn of shape C × k and 1k,1 is a k × 1 matrix of 1s. Equation (D.19) allows us to\ncache ˆP1:n−1 inside the inner loop of algorithm 2 and use batch matrix multiplication\nfor efficient computation.\nD. Diverse Batch Acquisition for Bayesian Active Learning\n267\nD.4\nAblation Study on Repeated-MNIST\nTo better understand the effect of redundant data points on BALD and BatchBALD,\nwe run the RMNIST experiment with an increasing number of repetitions. The results\ncan be seen in figure D.1. We use the same setup as in section 4.2.1. BatchBALD\nperforms the same on all repetition numbers (100 data points till 90%). BALD achieves\n90% accuracy at 120 data points (0 repetitions), 160 data points (1 repetition), 280\ndata points (2 repetitions), 300 data points (4 repetitions). This shows that BALD\nand BatchBALD behave as expected.\nFigure D.1:\nPerformance of BALD on Repeated MNIST for increasing amount of\nrepetitions. We see that BALD performs worse as the number of repetitions is increased,\nwhile BatchBALD outperforms BALD with zero repetitions.\nD.5\nAdditional Results for Repeated-MNIST\nWe show that BatchBALD also outperforms Var Ratios [Freeman, 1965] and Mean\nSTD [Kendall et al., 2017].\nFigure D.2: Performance on Repeated MNIST. BALD, BatchBALD, Var Ratios, Mean\nSTD and random acquisition with acquisition size 10 and 10 MC dropout samples.\nD. Diverse Batch Acquisition for Bayesian Active Learning\n268\nD.6\nExample Visualisation of EMNIST\nFigure D.3: Examples of all 47 classes of EMNIST\nD.7\nEntropy and Per-Class Acquisitions (including\nRandom Acquisition)\nFigure D.4:\nPerformance on EMNIST.\nBatchBALD consistently outperforms both\nrandom acquisition and BALD while BALD\nis unable to beat random acquisition.\nFigure D.5: Entropy of acquired class labels\nover acquisition steps on EMNIST. Batch-\nBALD steadily acquires a more diverse set\nof data points than BALD.\nE\nStochastic Batch Acquisition for Deep\nActive Learning\nE.1\nProof of Proposition 5.1\nFirst, we remind the reader that a random variable G is Gumble distributed G ∼\nGumbel(µ; β) when its cumulative distribution function follows p(G ≤g) = exp(−exp(−g−µ\nβ )).\nFurthermore, the Gumbel distribution is closed under translation and positive scal-\ning:\nLemma E.1. Let G ∼Gumbel(µ; β) be a Gumbel distributed random variable, then:\nαG + d ∼Gumbel(d + αµ; αβ).\n(E.1)\nProof. We have p(αG + d ≤x) = p(G ≤x−d\nα ). Thus, we have:\np(αG + d ≤x) = exp(−exp(−\nx−d\nα −µ\nβ\n))\n(E.2)\n= exp(−exp(−x −(d + αµ)\nαβ\n))\n(E.3)\n⇔αG + d ∼Gumbel(d + αµ; αβ).\n(E.4)\nWe can then easily prove Proposition 5.1 using Theorem 1 from Kool et al. [2019],\nwhich we present it here slightly reformulated to fit our notation:\nLemma E.2. For k ≤n, let I∗\n1, . . . , I∗\nk = arg topk{si + ϵi}i with ϵi ∼Gumbel(0; 1),\ni.i.d.. Then I∗\n1, . . . , I∗\nk is an (ordered) sample without replacement from the categorical\ndistribution\nCategorical\n \nexp si\nP\nj∈n exp sj\n, i ∈{1, . . . , n}\n!\n,\n(E.5)\ne.g. for a realization i∗\n1, . . . , i∗\nk it holds that\nP (I∗\n1 = i∗\n1, . . . , I∗\nk = i∗\nk) =\nk\nY\nj=1\nexp si∗\nj\nP\nℓ∈N∗\nj exp sℓ\n(E.6)\nwhere N ∗\nj = N\\\nn\ni∗\n1, . . . , i∗\nj−1\no\nis the domain (without replacement) for the j-th sampled\nelement.\nE. Stochastic Batch Acquisition for Deep Active Learning\n270\n(a) A good example in MIOTCD dataset.\n(b) An example of duplicated\nsamples in the dataset.\n(c)\nAn\nexample\nof class confusion\nbetween motorcycle\nand bicycle.\n(d) An example of heavy\ncompression artifact.\n(e) An example of low\nresolution samples.\nFigure E.1: MIO-TCD Dataset is designed to include common artifacts from production\ndata. The size and quality of the images vary greatly between crops; from high-quality\ncameras on sunny days to low-quality cameras at night. (a) shows an example of clean\nsamples that can be clearly assigned to a class. (b)(c)(d) and (e) show the different categories\nof noise. (b) shows an example of many near-duplicates that exist in the dataset. (c) is a\ngood example where the assigned class is subject to interpretation (d) is a sample with heavy\ncompression artifacts and (e) is an example of samples with low resolution which again is\nconsidered a hard example to learn for the model.\nNow, it is easy to prove the proposition:\nProposition 5.1. For scores si, i ∈{1, . . . , n}, and k ≤n and β > 0, if we draw\nϵi ∼Gumbel(0; β−1) independently, then arg topk{si + ϵi}i is an (ordered) sample\nwithout replacement from the categorical distribution Categorical(exp(β si)/P\nj exp(β sj), i ∈\n{1, . . . , n}).\nProof. As ϵi ∼Gumbel(0; β−1), define ϵ′\ni ≜βϵi ∼Gumbel(0; 1). Further, let s′\ni ≜βsi.\nApplying Lemma E.2 on s′\ni and ϵ′\ni, arg topk{s′\ni + ϵ′\ni}i yields (ordered) samples without\nreplacement from the categorical distribution Categorical(\nexp(β si)\nP\nj exp(β sj), i ∈{1, . . . , n}).\nHowever, multiplication by β does not change the resulting indices of arg topk:\narg topk{s′\ni + ϵ′\ni}i = arg topk{si + ϵi}i,\n(E.7)\nconcluding the proof.\nE.2\nEmpirical Validation\nE.2.1\nExperimental Setup & Compute\nFull code for all experiments will be available at anonymized_github_repo.\nE. Stochastic Batch Acquisition for Deep Active Learning\n271\nFrameworks. We use PyTorch. Repeated-MNIST and EMNIST experiments use\nPyTorch Ignite. Synbols and MIO-TCD experiments use the BaaL library https://\ngithub.com/baal-org/baal [Atighehchian et al., 2020]. Predictive parity is calculated\nusing FairLearn [Bird et al., 2020]. The CausalBALD experiments use https://github.\ncom/anndvision/causal-bald [Jesson et al., 2021].\nCompute. Results shown in Table 5.1 were run inside Docker containers with 8\nCPUs (2.2Ghz) and 32 Gb of RAM. Other experiments were run on similar machines\nwith Titan RTX GPUs. The Repeated-MNIST and EMNIST experiments take about\n5000 GPU hours. The MIO, Synbols and CLINC-150 experiments take about 19000\nGPU hours. The CausalBALD experiments take about 1000 GPU hours.\nDataset Licenses. Repeated-MNIST is based on MNIST which is made available\nunder the terms of the Creative Commons Attribution-Share Alike 3.0 license. The\nEMNIST dataset is made available as CC0 1.0 Universal Public Domain Dedication.\nSynbols is a dataset generator. MIO-TCD is made available under the terms of the\nCreative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\nCLINC-150 is made available under the terms of Creative Commons Attribution\n3.0 Unported License.\nE.2.1.1\nRuntime Measurements\nThe synthetic dataset used for benchmarking has 4,096 features, 10 classes, and 10,000\npool points. VGG-16 models [Simonyan and Zisserman, 2015] were used to sample\npredictions and latent embeddings.\nE.2.1.2\nRepeated-MNIST\nThe Repeated-MNIST dataset (introduced in §4) with duplicated examples from MNIST\nwith isotropic Gaussian noise added to the input images (standard deviation 0.1).\nWe use the same setup as in §4: a LeNet-5-like architecture with ReLU activations\ninstead of tanh and added dropout. The model obtains 99% test accuracy when trained\non the full MNIST dataset. Specifically, the model is made up of two blocks of a\nconvolution, dropout, max-pooling, ReLU with 32 and 64 channels and 5x5 kernel size,\nrespectively. As classifier head, a two-layer MLP with 128 hidden units (and 10 output\nunits) is used that includes dropout between the layers. We use a dropout probability\nof 0.5 everywhere. The model is trained with early stopping using the Adam optimizer\nand a learning rate of 0.001. We sample predictions using 100 MC-Dropout samples\nfor BALD. Weights are reinitialized after each acquisition step.\nE.2.1.3\nEMNIST\nWe follow the setup from §4 with 20 MC dropout samples. We use a similar model\nas for Repeated-MNIST but with three blocks instead of two. Specifically, we use 32,\n64, and 128 channels and 3x3 kernel size. This is followed by a 2x2 max pooling layer\nbefore the classifier head. The classifier head is a two-layer MLP but with 512 hidden\nunits instead of 128. Again, we use dropout probability 0.5 everywhere.\nE.2.1.4\nSynbols & MIO-TCD\nThe full list of hyperparameters for the Synbols and MIO-TCD experiments is presented\nin Table E.1. Our experiments are built using the BaaL library [Atighehchian et al.,\n2020]. We compute the predictive parity using FairLearn [Bird et al., 2020]. We use\nE. Stochastic Batch Acquisition for Deep Active Learning\n272\nTable E.1: Hyperparameters used in Section 5.4 and E.2.5\nHyperparameter\nValue\nLearning Rate\n0.001\nOptimizer\nSGD\nWeight Decay\n0\nMomentum\n0.9\nLoss Function\nCross-Entropy\nTraining Duration\n10\nBatch Size\n32\nDropout p\n0.5\nMC Iterations\n20\nQuery Size\n100\nInitial Set\n500\nVGG-16 model [Simonyan and Zisserman, 2015] trained for 10 epochs using Monte\nCarlo dropout for acquisition [Gal et al., 2017] with 20 dropout samples.\nIn Figure E.1, we show a set of images with common problems that can be\nfound in MIO-TCD.\nE.2.1.5\nCLINC-150\nWe fine-tune a pretrained DistilBERT model from HuggingFace [Dosovitskiy et al.,\n2020] on CLINC-150 for 5 epochs with Adam as optimizer. Estimating epistemic\nuncertainty in transformer models is an open research question, and hence, we do not\nreport results using BALD and focus on entropy instead.\nE.2.1.6\nCausalBALD\nUsing the Neyman-Rubin framework [Neyman, 1923; Rubin, 1974; Sekhon, 2008], the\nCATE is formulated in terms of the potential outcomes, Yt, of treatment levels t ∈{0, 1}.\nGiven observable covariates, X, the CATE is defined as the expected difference between\nthe potential outcomes at the measured value X = x: τ(x) = E[Y1 −Y0 | X = x].\nThis causal quantity is fundamentally unidentifiable from observational data without\nfurther assumptions because it is not possible to observe both Y1 and Y0 for a given\nunit. However, under the assumptions of consistency, non-interference, ignoreability,\nand positivity, the CATE is identifiable as the statistical quantity eτ(x) = E[Y | T =\n1, X = x] −E[Y | T = 0, X = x] [Rubin, 1980].\nJesson et al. [2021] define BALD acquisition functions for active learning CATE\nfunctions from observational data when the cost of acquiring an outcome, y, for a given\ncovariate and treatment pair, (x, t), is high. Because we do not have labels for Y1 and\nY0 for each (x, t) pair in the dataset, their acquisition function focuses on acquiring\ndata points (x, t) for which it is likely that a matched pair (x, 1 −t) exists in the pool\ndata or has already been acquired at a previous step. We follow their experiments on\ntheir synthetic dataset with limited positivity and the semisynthetic IHDP dataset\n[Hill, 2011]. Details of the experimental setup are given in [Jesson et al., 2021], we\nuse their provided code, and implement the power acquisition function.\nThe settings for causal inference experiments are identical to those used in Jesson\net al. [2021], using the IHDP dataset [Hill, 2011]. Like them, we use a Deterministic\nE. Stochastic Batch Acquisition for Deep Active Learning\n273\nUncertainty Estimation Model [van Amersfoort et al., 2021], which is initialized with\n100 data points and acquire 10 data points per acquisition batch for 38 steps. The\ndataset has 471 pool points and a 201 point validation set.\nE.2.2\nRepeated-MNIST\n0.6\n0.7\n0.8\n0.9\nAccuracy\n0\n50\n100\n150\n200\n250\n300\nMin Training Set Size\nRepetition Factor = 1\n0.6\n0.7\n0.8\n0.9\nAccuracy\n0\n50\n100\n150\n200\n250\n300\nRepetition Factor = 2\n0.6\n0.7\n0.8\n0.9\nAccuracy\n0\n50\n100\n150\n200\n250\n300\nRepetition Factor = 4\nAcquisition Function\nBADGE 10\nBADGE 20\nBADGE 40\nFigure E.2: Repeated-MNIST x4 (5 trials): acquisition size ablation for BADGE. Acquisition\nsize 20 performs best out of {10, 20, 40}. Hence, we use that for Figure 5.2.\nBADGE Ablation. In Figure E.2, we see that BADGE performs best with acquisition\nsize 20 on Repeated-MNISTx4 overall. BADGE 40 and BADGE 20 have the highest\nfinal accuracy, cf. BADGE 10 while BADGE 20 performs better than BADGE 40\nfor small training set sizes.\nE.2.2.1\nOther Scoring Functions\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\nAccuracy\n50\n100\n150\n200\n250\n300\nMin Training Set Size\nPowerBALD\nPowerStdDev\nPowerVariationRatios\nPowerEntropy\nUniform\nBALD\nStdDev\nVariationRatios\nEntropy\nFigure E.3:\nRepeated-MNIST x4 (5 trials): Performance for other scoring functions.\nEntropy, std dev, variation ratios behave like BALD when applying our stochastic sampling\nscheme.\nIn Figure E.3 shows the performance of other scoring functions than BALD on\nRepeated-MNIST x4.\nE. Stochastic Batch Acquisition for Deep Active Learning\n274\nE.2.2.2\nRedundancy Ablation\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\nAccuracy\n50\n100\n150\n200\n250\n300\nMin Training Set Size\nRepetition Factor = 1\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\nAccuracy\n50\n100\n150\n200\n250\n300\nRepetition Factor = 2\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\nAccuracy\n50\n100\n150\n200\n250\n300\nRepetition Factor = 4\nPowerBALD\nBatchBALD 5\nBADGE 20\nUniform\nBALD\nFigure E.4: Repeated-MNIST (5 trials): Performance ablation for different repetition counts.\nIn Figure E.4, we see the same behavior in an ablation for different repetition sizes\nof Repeated-MNIST.\nE.2.3\nMIO-TCD\n0.95\n0.96\n0.97\n0.98\n0.99\nAccuracy\n0\n2000\n4000\n6000\n8000\n10000\n12000\n14000\nMin Training Set Size\nPowerBALD\nBADGE\nBALD\nUniform\n(a) BALD\n0.95\n0.96\n0.97\n0.98\n0.99\nAccuracy\n0\n2000\n4000\n6000\n8000\n10000\n12000\n14000\nMin Training Set Size\nPowerEntropy\nEntropy\nBADGE\nUniform\n(b) Entropy\nFigure E.5: MIO-TCD (5 trials).\nIn Figure E.5, we see that power acquisition performs on par with BADGE with both\nBALD and entropy as underlying score functions.\nE.2.4\nEMNIST\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nAccuracy\n0\n50\n100\n150\n200\n250\n300\nMin Training Set Size\nPowerBALD\nBatchBALD 5\nBADGE\nBALD\nFigure E.6:\nEMNIST (Balanced) (5\ntrials): Performance with BALD.\n0.0\n0.2\n0.4\n0.6\nAccuracy\n0\n100\n200\n300\nMin Training Set Size\nPowerBALD\nBatchBALD 5\nBALD\nFigure E.7:\nEMNIST (ByMerge) (5\ntrials): Performance with BALD.\nE. Stochastic Batch Acquisition for Deep Active Learning\n275\nIn Figure E.6 and E.7, we see that PowerBALD outperforms BALD, BatchBALD, and\nBADGE.\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nAccuracy\n0\n50\n100\n150\n200\n250\n300\nMin Training Set Size\nAcquisition Function\nBADGE 10\nBADGE 20\nBADGE 40\nFigure E.8: EMNIST (Balanced) (5 trials): acquisition size ablation\nfor BADGE.\nBADGE Ablation. In Figure E.8, we see that BADGE performs similarly with\nall three acquisition sizes. Acquisition size 10 is the smoothest.\nE.2.5\nEdge Cases in Synbols\nWe use Synbols [Lacoste et al., 2020] to demonstrate the behavior of batch active\nlearning in artificially constructed edge cases. Synbols is a character dataset generator\nfor classification where a user can specify the type and proportion of bias and insert\nartifacts, backgrounds, masking shapes, and so on. We selected three datasets with\nstrong biases supplied by Lacoste et al. [2020]; Branchaud-Charron et al. [2021] to\nevaluate our method. The experimental settings are described in appendix E.2.1.\nFor these tasks, performance evaluation includes ‘predictive parity’, also known as\n‘accuracy difference’, which is the maximum difference in accuracy between subgroups—\nwhich are, in this case, different colored characters. This measure is used most widely\nin domain adaptation and ethics [Verma and Rubin, 2018]. We want to maximize\nthe accuracy while minimizing the predictive parity.\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nAccuracy\n0\n2000\n4000\n6000\n8000\n10000\n12000\n14000\nMin Training Set Size\nPowerBALD\nBADGE\nBALD\nUniform\n(a) Accuracy\n0\n2000\n4000\n6000\n8000 10000 12000 14000\nTraining Set Size\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\nPredictive Parity\nPowerBALD\nBADGE\nBALD\nUniform\n(b) Predictive parity (Down and left is better.)\nFigure E.9: Performance on Synbols Spurious Correlations (3 trials) with BALD. Stochastic\nacquisition matches BADGE and BALD’s predictive parity and performance, which is\nreassuring as stochastic acquisition functions might be affected by spurious correlations.\nE. Stochastic Batch Acquisition for Deep Active Learning\n276\nSpurious Correlations. This dataset includes spurious correlations between\ncharacter color and class.\nAs shown in Branchaud-Charron et al. [2021], active\nlearning is especially strong here as characters that do not follow the correlation\nwill be informative and thus selected.\nWe compare the predictive parity between methods in Fig. E.9(b). We do not see any\nsignificant difference between our method and BADGE or BALD. This is encouraging,\nas stochastic approaches might select more examples following the spurious correlation\nand thus have higher predictive parity, but this is not the case.\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nAccuracy\n0\n2000\n4000\n6000\n8000\n10000\n12000\n14000\nMin Training Set Size\nPowerBALD\nBADGE\nBALD\nUniform\n(a) Accuracy\n0\n2000\n4000\n6000\n8000 10000 12000 14000\nTraining Set Size\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\nPredictive Parity\nPowerBALD\nBADGE\nBALD\nUniform\n(b) Predictive parity\nFigure E.10: Synbols Minority Groups (3 trials): Performance on BALD. PowerBALD\noutperforms BALD and matches BADGE for both accuracy and predictive parity.\nMinority Groups. This dataset includes a subgroup of the data that is under-\nrepresented; specifically, most characters are red while few are blue. As Branchaud-\nCharron et al. [2021] shows, active learning can improve the accuracy for these groups.\nOur stochastic approach lets batch acquisition better capture under-represented\nsubgroups. In Figure E.10(a), PowerBALD has an accuracy almost identical to that\nof BADGE, despite being much cheaper, and outperforms BALD. At the same time,\nwe see in Figure E.10(b) that PowerBALD has a lower predictive parity than BALD,\ndemonstrating a fairer predictive distribution given the unbalanced dataset.\nE. Stochastic Batch Acquisition for Deep Active Learning\n277\n0.955 0.960 0.965 0.970 0.975 0.980 0.985\nAccuracy\n0\n2000\n4000\n6000\n8000\n10000\n12000\n14000\nMin Training Set Size\nPowerBALD\nBADGE\nBALD\nUniform\nFigure E.11: BALD\n0.955 0.960 0.965 0.970 0.975 0.980 0.985\nAccuracy\n0\n2000\n4000\n6000\n8000\n10000\n12000\n14000\nMin Training Set Size\nPowerEntropy\nEntropy\nBADGE\nUniform\nFigure E.12: Entropy\nFigure E.13:\nPerformance on Synbols Missing Characters (3 trials).\nIn this dataset\nwith high aleatoric uncertainty, PowerBALD matches BADGE and BALD performance.\nPowerEntropy significantly outperforms Entropy which confounds aleatoric and epistemic\nuncertainty.\nMissing Synbols. This dataset has high aleatoric uncertainty. Some images\nare missing information required to make high-probability predictions—these images\nhave shapes randomly occluding the character—so even a perfect model would remain\nuncertain. Lacoste et al. [2020] demonstrated that entropy is ineffective on this data\nas it cannot distinguish between aleatoric and epistemic uncertainty, while BALD can\ndo so. As a consequence, entropy will unfortunately prefer samples with occluded\ncharacters, resulting in degraded active learning performance. For predictive entropy,\nstochastic acquisition largely corrects the failure of entropy acquisition to account for\nmissing data (Figure E.13) although PowerEntropy still underperforms BADGE here.\nFor BALD, we show in Figure E.11 in the appendix that, as before, our stochastic\nmethod performs on par with BADGE and marginally better than BALD.\nE.2.6\nCLINC-150\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\nAccuracy\n1000\n2000\n3000\n4000\n5000\n6000\n7000\n8000\nMin Training Set Size\nPowerEntropy\nBADGE\nEntropy\nUniform\nFigure E.14: Performance on CLINC-150 (10 trials). PowerEntropy performs much better\nthan entropy, which only performs marginally better than uniform, and almost on par with\nBADGE.\nIn Figure E.14, we see that PowerEntropy performs much better than entropy which\nonly performs marginally better than the uniform baseline. PowerEntropy also performs\nbetter than BADGE at low training set sizes, but BADGE performs better in the second\nE. Stochastic Batch Acquisition for Deep Active Learning\n278\nhalf. Between ≈2300 and 4000 samples, BADGE and PowerEntropy perform the same.\nE.3\nComparing Power, Softmax and Soft-Rank\nE.3.1\nEmpirical Evidence\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\nAccuracy\n50\n100\n150\n200\n250\n300\nMin Training Set Size\nRepetition Factor = 1\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\nAccuracy\n50\n100\n150\n200\n250\n300\nRepetition Factor = 2\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\nAccuracy\n50\n100\n150\n200\n250\n300\nRepetition Factor = 4\nPowerBALD\nSoftmaxBALD\nSoftrankBALD\n(a) BALD\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\nAccuracy\n50\n100\n150\n200\n250\n300\nMin Training Set Size\nRepetition Factor = 1\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\nAccuracy\n50\n100\n150\n200\n250\n300\nRepetition Factor = 2\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\nAccuracy\n50\n100\n150\n200\n250\n300\nRepetition Factor = 4\nPowerEntropy\nSoftmaxEntropy\nSoftrankEntropy\n(b) Entropy\nFigure E.15: Repeated-MNIST (5 trials): Performance with all three stochastic strategies.\nRepeated-MNIST. In Figure E.15, power acquisition performs the best overall,\nfollowed by soft-rank and then softmax.\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nAccuracy\n0\n50\n100\n150\n200\n250\n300\nMin Training Set Size\nPowerBALD\nSoftmaxBALD\nSoftrankBALD\nBatchBALD 5\nBADGE\nBALD\nFigure E.16:\nEMNIST (Balanced) (5\ntrials): Performance with all three stochas-\ntic strategies with BALD. PowerBALD\nperforms best.\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nAccuracy\n0\n50\n100\n150\n200\n250\n300\nMin Training Set Size\nPowerBALD\nSoftmaxBALD\nSoftrankBALD\nBatchBALD 5\nBALD\nFigure E.17:\nEMNIST (ByMerge) (5\ntrials): Performance with all three stochas-\ntic strategies with BALD. PowerBALD\nperforms best.\nEMNIST. In Figure E.16 and E.17, we see that PowerBALD performs best, but\nSoftmax- and SoftrankBALD also outperform other methods. BADGE did not run\non EMNIST (ByMerge) due to out-of-memory issues and BatchBALD took very long\nas EMNIST (ByMerge) has more than 800,000 samples.\nE. Stochastic Batch Acquisition for Deep Active Learning\n279\n0.95\n0.96\n0.97\n0.98\n0.99\nAccuracy\n0\n2000\n4000\n6000\n8000\n10000\n12000\n14000\nMin Training Set Size\nPowerBALD\nSoftmaxBALD\nSoftrankBALD\nBALD\n(a) BALD\n0.95\n0.96\n0.97\n0.98\n0.99\nAccuracy\n0\n2000\n4000\n6000\n8000\n10000\n12000\n14000\nMin Training Set Size\nPowerEntropy\nSoftmaxEntropy\nSoftrankEntropy\nEntropy\n(b) Entropy\nFigure E.18: MIO-TCD (3 trials): Performance with all three stochastic strategies.\nMIO-TCD. In Figure E.18, we see that all three stochastic acquisition methods\nperform about equally well.\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nAccuracy\n0\n2500\n5000\n7500\n10000\n12500\n15000\nMin Training Set Size\nDataset = Under-Represented Groups\n0.955\n0.960\n0.965\n0.970\n0.975\n0.980\n0.985\nAccuracy\n0\n2500\n5000\n7500\n10000\n12500\n15000\nDataset = Missing Synbols\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nAccuracy\n0\n2500\n5000\n7500\n10000\n12500\n15000\nDataset = Spurious Correlations\nPowerBALD\nSoftmaxBALD\nSoftrankBALD\n(a) BALD\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nAccuracy\n0\n2500\n5000\n7500\n10000\n12500\n15000\nMin Training Set Size\nDataset = Under-Represented Groups\n0.955\n0.960\n0.965\n0.970\n0.975\n0.980\n0.985\nAccuracy\n0\n2500\n5000\n7500\n10000\n12500\n15000\nDataset = Missing Synbols\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nAccuracy\n0\n2500\n5000\n7500\n10000\n12500\n15000\nDataset = Spurious Correlations\nPowerEntropy\nSoftmaxEntropy\nSoftrankEntropy\n(b) Entropy\nFigure E.19: Synbols edge cases (3 trials): Performance with all three stochastic strategies.\nSynbols. In Figure E.19, power acquisition seems to perform better overall—\nmainly due to the performance in Synbols Missing Characters.\nE. Stochastic Batch Acquisition for Deep Active Learning\n280\n0.0000\n0.0002\n0.0004\n0.0006\n0.0008\n0.0010\nProbability\n10\n35\n10\n29\n10\n23\n10\n17\n10\n11\n10\n5\n0.25\n1.0\n2.0\n4.0\n8.0\nDistribution Type\nPower\nSoftmax\nSoftrank\nFigure E.21: Score distribution for power and softmax acquisition of BALD scores on\nMNIST for varying Coldness β at t = 0. Linear and log plot over samples sorted by their\nBALD score. At β = 8 both softmax and power acquisition have essentially the same\ndistribution for high scoring points (closely followed by the power distribution for β = 4).\nThis might explain why the coldness ablation shows that these β to have very similar AL\ntrajectories on MNIST. Yet, while softmax and power acquisition seem transfer to RMNIST,\nthis is not the case for softrank which is much more sensitive to β. At the same time, power\nacquisition avoids low-scoring points more than softmax acquisition.\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\nAccuracy\n1000\n2000\n3000\n4000\n5000\n6000\n7000\n8000\nMin Training Set Size\nPowerEntropy\nSoftmaxEntropy\nSoftrankEntropy\nFigure E.20: CLINC-150 (10 trials): Perfor-\nmance with all three stochastic strategies.\nCLINC-150. In Figure E.20, all three stochastic methods perform similarly.\nE.3.2\nInvestigation\nTo further examine the three stochastic acquisition variants, we plot their score\ndistributions, extracted from the same MNIST toy example, in Figure E.21. Power\nand softmax acquisition distributions are similar for β = 8 (power, softmax) and\nβ = 4 (softmax). This might explain why active learning with these β shows similar\naccuracy trajectories.\nWe find that power and softmax acquisition are quite insensitive to β and thus\nselecting β = 1 might generally work quite well.\nE. Stochastic Batch Acquisition for Deep Active Learning\n281\nE.4\nEffect of Changing β\nE.4.1\nRepeated-MNIST\n0\n50\n100\n150\n200\n250\n300\nMin Training Set Size\nAcquisition Function = PowerBALD 10 | Repetition Factor = 1\n0\n50\n100\n150\n200\n250\n300\nAcquisition Function = PowerBALD 10 | Repetition Factor = 2\n0\n50\n100\n150\n200\n250\n300\nAcquisition Function = PowerBALD 10 | Repetition Factor = 4\n0\n50\n100\n150\n200\n250\n300\nMin Training Set Size\nAcquisition Function = SoftmaxBALD 10 | Repetition Factor = 1\n0\n50\n100\n150\n200\n250\n300\nAcquisition Function = SoftmaxBALD 10 | Repetition Factor = 2\n0\n50\n100\n150\n200\n250\n300\nAcquisition Function = SoftmaxBALD 10 | Repetition Factor = 4\n0\n50\n100\n150\n200\n250\n300\nMin Training Set Size\nAcquisition Function = SoftrankBALD 10 | Repetition Factor = 1\n0\n50\n100\n150\n200\n250\n300\nAcquisition Function = SoftrankBALD 10 | Repetition Factor = 2\n0\n50\n100\n150\n200\n250\n300\nAcquisition Function = SoftrankBALD 10 | Repetition Factor = 4\n0\n50\n100\n150\n200\n250\n300\nMin Training Set Size\nAcquisition Function = PowerBALD 20 | Repetition Factor = 1\n0\n50\n100\n150\n200\n250\n300\nAcquisition Function = PowerBALD 20 | Repetition Factor = 2\n0\n50\n100\n150\n200\n250\n300\nAcquisition Function = PowerBALD 20 | Repetition Factor = 4\n0\n50\n100\n150\n200\n250\n300\nMin Training Set Size\nAcquisition Function = SoftmaxBALD 20 | Repetition Factor = 1\n0\n50\n100\n150\n200\n250\n300\nAcquisition Function = SoftmaxBALD 20 | Repetition Factor = 2\n0\n50\n100\n150\n200\n250\n300\nAcquisition Function = SoftmaxBALD 20 | Repetition Factor = 4\n0\n50\n100\n150\n200\n250\n300\nMin Training Set Size\nAcquisition Function = SoftrankBALD 20 | Repetition Factor = 1\n0\n50\n100\n150\n200\n250\n300\nAcquisition Function = SoftrankBALD 20 | Repetition Factor = 2\n0\n50\n100\n150\n200\n250\n300\nAcquisition Function = SoftrankBALD 20 | Repetition Factor = 4\n0\n50\n100\n150\n200\n250\n300\nMin Training Set Size\nAcquisition Function = PowerBALD 40 | Repetition Factor = 1\n0\n50\n100\n150\n200\n250\n300\nAcquisition Function = PowerBALD 40 | Repetition Factor = 2\n0\n50\n100\n150\n200\n250\n300\nAcquisition Function = PowerBALD 40 | Repetition Factor = 4\n0\n50\n100\n150\n200\n250\n300\nMin Training Set Size\nAcquisition Function = SoftmaxBALD 40 | Repetition Factor = 1\n0\n50\n100\n150\n200\n250\n300\nAcquisition Function = SoftmaxBALD 40 | Repetition Factor = 2\n0\n50\n100\n150\n200\n250\n300\nAcquisition Function = SoftmaxBALD 40 | Repetition Factor = 4\n0.6\n0.7\n0.8\n0.9\nAccuracy\n0\n50\n100\n150\n200\n250\n300\nMin Training Set Size\nAcquisition Function = SoftrankBALD 40 | Repetition Factor = 1\n0.6\n0.7\n0.8\n0.9\nAccuracy\n0\n50\n100\n150\n200\n250\n300\nAcquisition Function = SoftrankBALD 40 | Repetition Factor = 2\n0.6\n0.7\n0.8\n0.9\nAccuracy\n0\n50\n100\n150\n200\n250\n300\nAcquisition Function = SoftrankBALD 40 | Repetition Factor = 4\n0.25\n1.0\n4.0\n8.0\n16.0\nFigure E.22: Repeated-MNIST: β ablation for *BALD.\nE. Stochastic Batch Acquisition for Deep Active Learning\n282\nE.4.1.1\nMIO-TCD and Synbols\n0.95\n0.96\n0.97\n0.98\n0.99\n0\n2500\n5000\n7500\n10000\n12500\n15000\nMin Training Set Size\nPowerBALD on MIO-TCD\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\n0\n2500\n5000\n7500\n10000\n12500\n15000\nPowerBALD on Spurious Correlations\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\n0\n2500\n5000\n7500\n10000\n12500\n15000\nPowerBALD on Under-Represented Groups\n0.955 0.960 0.965 0.970 0.975 0.980 0.985\n0\n2500\n5000\n7500\n10000\n12500\n15000\nPowerBALD on Missing Synbols\n0.95\n0.96\n0.97\n0.98\n0.99\n0\n2500\n5000\n7500\n10000\n12500\n15000\nMin Training Set Size\nSoftmaxBALD on MIO-TCD\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\n0\n2500\n5000\n7500\n10000\n12500\n15000\nSoftmaxBALD on Spurious Correlations\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\n0\n2500\n5000\n7500\n10000\n12500\n15000\nSoftmaxBALD on Under-Represented Groups\n0.955 0.960 0.965 0.970 0.975 0.980 0.985\n0\n2500\n5000\n7500\n10000\n12500\n15000\nSoftmaxBALD on Missing Synbols\n0.95\n0.96\n0.97\n0.98\n0.99\nAccuracy\n0\n2500\n5000\n7500\n10000\n12500\n15000\nMin Training Set Size\nSoftrankBALD on MIO-TCD\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nAccuracy\n0\n2500\n5000\n7500\n10000\n12500\n15000\nSoftrankBALD on Spurious Correlations\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nAccuracy\n0\n2500\n5000\n7500\n10000\n12500\n15000\nSoftrankBALD on Under-Represented Groups\n0.955 0.960 0.965 0.970 0.975 0.980 0.985\nAccuracy\n0\n2500\n5000\n7500\n10000\n12500\n15000\nSoftrankBALD on Missing Synbols\n0.125\n1.0\n8.0\nFigure E.23: MIO-TCD and Synbols: β ablation for *BALD.\n0.95\n0.96\n0.97\n0.98\n0.99\n0\n2500\n5000\n7500\n10000\n12500\n15000\nMin Training Set Size\nPowerEntropy on MIO-TCD\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\n0\n2500\n5000\n7500\n10000\n12500\n15000\nPowerEntropy on Spurious Correlations\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\n0\n2500\n5000\n7500\n10000\n12500\n15000\nPowerEntropy on Under-Represented Groups\n0.955 0.960 0.965 0.970 0.975 0.980 0.985\n0\n2500\n5000\n7500\n10000\n12500\n15000\nPowerEntropy on Missing Synbols\n0.95\n0.96\n0.97\n0.98\n0.99\n0\n2500\n5000\n7500\n10000\n12500\n15000\nMin Training Set Size\nSoftmaxEntropy on MIO-TCD\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\n0\n2500\n5000\n7500\n10000\n12500\n15000\nSoftmaxEntropy on Spurious Correlations\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\n0\n2500\n5000\n7500\n10000\n12500\n15000\nSoftmaxEntropy on Under-Represented Groups\n0.955 0.960 0.965 0.970 0.975 0.980 0.985\n0\n2500\n5000\n7500\n10000\n12500\n15000\nSoftmaxEntropy on Missing Synbols\n0.95\n0.96\n0.97\n0.98\n0.99\nAccuracy\n0\n2500\n5000\n7500\n10000\n12500\n15000\nMin Training Set Size\nSoftrankEntropy on MIO-TCD\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nAccuracy\n0\n2500\n5000\n7500\n10000\n12500\n15000\nSoftrankEntropy on Spurious Correlations\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nAccuracy\n0\n2500\n5000\n7500\n10000\n12500\n15000\nSoftrankEntropy on Under-Represented Groups\n0.955 0.960 0.965 0.970 0.975 0.980 0.985\nAccuracy\n0\n2500\n5000\n7500\n10000\n12500\n15000\nSoftrankEntropy on Missing Synbols\n0.125\n1.0\n8.0\nFigure E.24: MIO-TCD and Synbols: β ablation for *Entropy.\nE. Stochastic Batch Acquisition for Deep Active Learning\n283\nE.4.2\nCausalBALD: Synthetic Dataset\n0\n50\n100\n150\n200\n250\n300\nTraining Set Size\n0.5\n1.0\n1.5\n2.0\n2.5\nPEHE\nUniform\n= 0.01\n= 1\n= 4\n= 100\nTop-K\n(a) Overall Ablation (Subset)\n0\n50\n100\n150\n200\n250\n300\nTraining Set Size\n0.5\n1.0\n1.5\n2.0\n2.5\nPEHE\n= 4\n= 10\n= 100\nTop-K\n(b) Low Temperature Only\n0\n50\n100\n150\n200\n250\n300\nTraining Set Size\n0.5\n1.0\n1.5\n2.0\n2.5\nPEHE\nUniform\n= 0.01\n= 0.2\n= 1\n= 2\n= 4\n(c) High Temperature Only\nFigure E.25: CausalBALD: Synthetic Dataset. (a) At a very high temperature (β = 0.1),\nPowerBALD behaves very much like random acquisition, and as the temperature decreases\nthe performance of the acquisition function improves (lower √ϵPEHE). (b) Eventually, the\nperformance reaches an inflection point (β = 4.0) and any further decrease in temperature\nresults in the acquisition strategy performing more like top-K. We see that under the optimal\ntemperature, power acquisition significantly outperforms both random acquisition and top-K\nover a wide range of temperature settings.\nWe provide further β ablations for CausalBALD on the entirely synthetic dataset which\nis used by Jesson et al. [2021]. This demonstrates the ways in which β interpolates\nbetween uniform and top-K acquisition.\nE.4.3\nCLINC-150\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\nAccuracy\n2000\n4000\n6000\n8000\nMin Training Set Size\nPowerEntropy\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\nAccuracy\n2000\n4000\n6000\n8000\nSoftmaxEntropy\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\nAccuracy\n2000\n4000\n6000\n8000\nSoftrankEntropy\n0.125\n1.0\n8.0\nFigure E.26: Performance CLINC-150: β ablation for *Entropy.\nF\nPrediction- & Distribution-Aware Bayesian\nActive Learning\nF.1\nBALD Estimation\nIn general, we can estimate BALD using nested Monte Carlo [Rainforth et al., 2018]:\nBALD(x) = Ep(θ)[−Ep(y|x)[log p(y | x)] + Ep(y|x,θ)[log p(y | x, θ)]]\n(F.1)\n≈1\nM\nM\nX\nj=1\n−log\n 1\nK\nK\nX\ni=1\np(yj | x, θi)\n!\n+ log p(yj | x, θj),\n(F.2)\nwhere θi ∼p(θ), (θj, yj) ∼p(θ) p(y |x, θ). Special cases allow us to use computationally\ncheaper estimators.\nF.1.1\nCategorical Predictive Distribution\nWhen y and yeval are discrete, we can write\nBALD(x) = Ep(θ)[−Ep(y|x)[log p(y | x)] + Ep(y|x,θ)[log p(y | x, θ)]]\n(F.3)\n= −Ep(y|x)[log p(y | x)] + Ep(θ) p(y|x,θ)[log p(y | x, θ)]\n(F.4)\n= −\nX\ny∈Y\np(y | x) log p(y | x) + Ep(θ)[\nX\ny∈Y\np(y | x, θ) log p(y | x, θ)].\n(F.5)\nThis can be estimated using samples, θi ∼p(θ) [Houlsby, 2014]:\nBALD(x) ≈−\nX\ny∈Y\nˆp(y | x) log ˆp(y | x) + 1\nK\nK\nX\ni=1\nX\ny∈Y\np(y | x, θi) log p(y | x, θi),\n(F.6)\nwhere\nˆp(y | x) = 1\nK\nK\nX\ni=1\np(y | x, θi).\n(F.7)\nF.1.2\nGaussian Predictive Distribution\nSuppose we have a model whose likelihood function, p(y | x, θ), and predictive\ndistribution, p(y|x), are Gaussian. Then, using the symmetry of the mutual information\nalong with knowledge of the entropy of a Gaussian [Cover and Thomas, 2005], we have\nBALD(x) = 1\n2 log 2πe Var[Y | x] −1\n2 Ep(θ)[log 2πe Var[Y | x, θ]]\n(F.8)\nF. Prediction- & Distribution-Aware Bayesian Active Learning\n285\n= 1\n2\n\u0010\nlog Var[Y | x] −Ep(θ)[log Var[Y | x, θ]]\n\u0011\n.\n(F.9)\nRelatedly, Houlsby et al. [2011] identified a closed-form approximation of BALD for\nthe particular case of using a probit likelihood function, a Gaussian-process prior and\na Gaussian approximation to the predictive distribution.\nF.2\nEPIG Derivation\nComputing an expectation over both y and xeval gives the expected predictive informa-\ntion gain:\nEPIG(x) = Epeval(xeval) p(y|x)[H(p(yeval | xeval)) −H(p(yeval | y, x, xeval))]\n(F.10)\n= Epeval(xeval) p(y|x)[−Ep(yeval|xeval)[log p(yeval | xeval)]]\n+ Epeval(xeval) p(y|x)[Ep(yeval|y,x,xeval)[log p(yeval | y, x, xeval)]]\n(F.11)\n= Epeval(xeval) p(y,yeval|x,xeval)[log p(yeval | y, x, xeval)\np(yeval | xeval)\n]\n(F.12)\n= Epeval(xeval) p(y,yeval|x,xeval)[log p(y | x) p(yeval | y, x, xeval)\np(y | x) p(yeval | xeval)\n]\n(F.13)\n= Epeval(xeval) p(y,yeval|x,xeval)[log\np(y, yeval | x, xeval)\np(y | x) p(yeval | xeval)]\n(F.14)\n= Epeval(xeval)[I[y; yeval | x, xeval]]\n(F.15)\n= Epeval(xeval)[DKL(p(y, yeval | x, xeval) ∥p(y | x) p(yeval | xeval))].\n(F.16)\nF.3\nEPIG Estimation\nWhile in general we can use Equation 7.39 to estimate EPIG, special cases allow\ncomputationally cheaper estimators.\nF.3.1\nCategorical Predictive Distribution\nWhen y and yeval are discrete, we can write\nEPIG(x) = Epeval(xeval)[DKL(p(y, yeval | x, xeval) ∥p(y | x) p(yeval | xeval))]\n(F.17)\n= Epeval(xeval)[\nX\ny∈Y\nX\nyeval∈Y\np(y, yeval | x, xeval) log\np(y, yeval | x, xeval)\np(y | x) p(yeval | xeval)]. (F.18)\nThis can be estimated using samples, θi ∼p(θ) and xeval\nj\n∼peval(xeval):\nEPIG(x) ≈1\nM\nM\nX\nj=1\nX\ny∈Y\nX\nyeval∈Y\nˆp(y, yeval | x, xeval\nj\n) log\nˆp(y, yeval | x, xeval\nj\n)\nˆp(y | x)ˆp(yeval | xeval\nj\n),\n(F.19)\nwhere\nˆp(y, yeval | x, xeval\nj\n) = 1\nK\nK\nX\ni=1\np(y | x, θi) p(yeval | xeval\nj\n, θi)\n(F.20)\nˆp(y | x) = 1\nK\nK\nX\ni=1\np(y | x, θi)\n(F.21)\nˆp(yeval | xeval\nj\n) = 1\nK\nK\nX\ni=1\np(yeval | xeval\nj\n, θi).\n(F.22)\nF. Prediction- & Distribution-Aware Bayesian Active Learning\n286\nF.3.2\nGaussian Predictive Distribution\nConsider a joint predictive distribution that is multivariate Gaussian with mean\nvector µ and covariance matrix Σ:\np(y, yeval | x, xeval) = N(µ, Σ) = N\n\n\nµ,\n\n\nCov[x; x]\nCov[x; xeval]\nCov[x; xeval]\nCov[xeval; xeval]\n\n\n\n\n.\n(F.23)\nIn this setting the mutual information between y and yeval given x and xeval is a\nclosed-form function of Σ:\nI[Y ; Y eval | x, Xeval] = H(p(y | x)) + H(p(yeval | xeval)) −H(p(y, yeval | x, xeval)) (F.24)\n= 1\n2 log 2πe Var[p(y | x)] + 1\n2 log 2πe Var[p(yeval | xeval)]\n= −1\n2 log det 2πeΣ\n(F.25)\n= 1\n2 log Var[p(y | x)] Var[p(yeval | xeval)]\ndet Σ\n(F.26)\n= 1\n2 log Cov[x; x] Cov[xeval; xeval]\ndet Σ\n(F.27)\n= 1\n2 log\nCov[x; x] Cov[xeval; xeval]\nCov[x; x] Cov[xeval; xeval] −Cov[x; xeval]2.\n(F.28)\nWe can estimate EPIG using samples, xeval\nj\n∼peval(xeval):\nEPIG(x) = Epeval(xeval)[I[y; yeval | x, xeval]] ≈1\nM\nM\nX\nj=1\nI[y; yeval | x, xeval\nj\n]\n(F.29)\n=\n1\n2M\nM\nX\nj=1\nlog\nCov[x; x] Cov[xeval\nj\n; xeval\nj\n]\nCov[x; x] Cov[xeval\nj\n; xeval\nj\n] −Cov[x; xeval\nj\n]2.\n(F.30)\nF.3.3\nConnection to Foster et al. [2019]\nFoster et al. [2019] primarily considered variational estimation of the expected informa-\ntion gain. Since the joint density, p(y, yeval | x, xeval), that appears in EPIG is often not\nknown in closed form, EPIG estimation broadly falls under the “implicit likelihood”\ncategory of methods considered in that paper. Here, we focus on showing how the\n“posterior” or Barber-Agakov bound [Barber and Agakov, 2003] from this earlier work\napplies to EPIG estimation. We first recall Equation 7.21,\nEPIG(x) = Epeval(xeval) p(y,yeval|x,xeval)[log p(yeval | xeval, y, x)] + H(p(yeval | xeval)),\n(F.31)\nand the observation that c = H(p(yeval | xeval)) does not depend upon x and hence can\nbe neglected when choosing between designs. By Gibbs’s inequality, we must have\nEPIG(x) ≥Epeval(xeval) p(y,yeval|x,xeval)[log q(yeval|xeval, y, x)] + H(p(yeval | xeval)) (F.32)\nfor any distribution q. We can now consider a variational family, qψ(yeval|xeval, y, x),\nand a maximization over the variational parameter, ψ:\nEPIG(x) ≥sup\nψ Epeval(xeval) p(y,yeval|x,xeval)[log qψ(yeval|xeval, y, x)] + H(p(yeval | xeval)).\n(F.33)\nF. Prediction- & Distribution-Aware Bayesian Active Learning\n287\nA practical implication of this bound is that we could estimate EPIG by learning\nan auxiliary network, qψ(yeval|xeval, y, x), using data simulated from the model to\nmake one-step-ahead predictions. That is, qψ is trained to make predictions at xeval,\nincorporating the knowledge of the hypothetical acquisition (x, y). For our purposes,\ntraining such an auxiliary network at each acquisition is prohibitively expensive. But\nthis approach might be valuable in other applications of EPIG.\nF.4\nDataset Construction\nF.4.1\nUCI Data\nFor each dataset we start by taking the base dataset, Dbase, from the UCI repository.\nSatellite and Vowels have predefined test datasets, Dtest. In contrast, Magic does not\nhave a predefined train-test split. It is stated in Magic’s documentation that one of\nthe classes is underrepresented in the dataset relative to real-world data (Magic is a\nsimulated dataset). Whereas classes 0 and 1 respectively constitute 65% and 35% of the\ndataset, it is stated that class 1 constitutes the majority of cases in reality (the exact\nsplit is not stated; we assume 75% for class 1). We therefore uniformly sample 30% of\nDbase to form a test base dataset, D′\nbase; then we set Dbase ←Dbase\\D′\nbase; then we make\nDtest by removing input-label pairs from D′\nbase such that class 1 constitutes 75% of the\nsubset. With the test set defined, we proceed to sample two disjoint subsets of Dbase\nsuch that their class proportions match those of Dbase: a pool set, Dpool, whose size\nvaries between datasets, and a validation set, Dval, of 60 input-label pairs. Regardless\nof the class proportions of Dbase, we always use an initial training dataset, Dinit, of 2\ninput-label pairs per class, sampled from Dbase. Finally, we sample a representative\nset of inputs, D∗, whose class proportions match those of Dtest.\nF.4.2\nMNIST Data\nImplementing each setting starts by using the standard MNIST training data (60,000\ninput-label pairs) as the base dataset, Dbase, and the standard MNIST testing data\n(10,000 input-label pairs) as the test base dataset, D′\nbase. For Redundant MNIST we\nmake Dtest by removing input-label pairs from D′\nbase such that only classes 1 and 7\nremain. Otherwise, we set Dtest = D′\nbase. Next we construct the pool set, Dpool. For\nCurated MNIST and Redundant MNIST we sample 4,000 inputs per class from D′\nbase.\nFor Unbalanced MNIST we sample 400 inputs per class for classes 0-4 and 4,000 inputs\nper class for classes 5-9. After this we make the initial training dataset, Dinit. For\nCurated MNIST and Unbalanced MNIST we sample 2 input-label pairs per class from\nDbase. For Redundant MNIST we sample 2 input-label pairs from class 1, 2 input-label\npairs from class 7 and 1 input-label pair per class from 2 randomly selected classes other\nthan 1 and 7. Next, the validation set, Dval. For all settings this comprises 60 input-\nlabel pairs such that the class proportions match those used to form Dpool. Finally, we\nsample a representative set of inputs, D∗, whose class proportions match those of Dtest.\nF.5\nEPIG & JEPIG\nProposition 7.2. EPIG lower-bounds ‘averaged’ JEPIG:\nI[Y eval; Y acq | Xeval, xacq, Dtrain] ≤1/E I[Y eval\n1..E ; Y acq | xeval\n1..E, xacq] + ceval,\n(7.54)\nF. Prediction- & Distribution-Aware Bayesian Active Learning\n288\nup to an additive constant (ceval) that only depends on the evaluation samples and is\nindependent of xacq. The inequality gap is the total correlation:\n1/E TC[Y eval\n1\n; . . . ; Y eval\nE\n| xeval\n1..E, Y acq, xacq]\n(7.55)\nWe have equality when it is zero, that is when the predictions on the evaluation set are\nindependent (given the acquisition samples).\nProof. We drop conditioning on Dtrain in this proof. First, we remind ourselves of the\ntotal correlation:\nTC[A1; . . . ; An] =\nn\nX\ni=1\nH[Ai] −H[A1,...,n] ≥0.\n(F.34)\nWe expand EPIG to the right:\nI[Y eval; Y acq | Xeval, xacq] = H[Y eval | Xeval] −H[Y eval | Xeval, Y acq, xacq].\n(F.35)\nThe first term on the right-hand side is constant given a fixed evaluation set. We can\nexpress both terms in terms of the total correlation:\nE H[Y eval | Xeval]\n(F.36)\n= TC[Y eval\n1\n; . . . ; Y eval\nE\n| xeval\n1..E] + H[Y eval\n1..E | xeval\n1..E, Y acq, xacq],\n(F.37)\nand\nE H[Y eval | Xeval, Y acq, xacq]\n(F.38)\n= TC[Y eval\n1\n; . . . ; Y eval\nE\n| xeval\n1..E, Y acq, xacq] + H[Y eval\n1..E | xeval\n1..E, Y acq, xacq].\n(F.39)\nceval ≜1/E TC[Y eval\n1\n; . . . ; Y eval\nE\n| Xeval] is independent of the acquisition set. Hence, we\ncan conclude:\nI[Y eval; Y acq | Xeval, xacq]\n(F.40)\n= 1/E(H[Y eval\n1..E | xeval\n1..E, Y acq, xacq] −H[Y eval\n1..E | xeval\n1..E, Y acq, xacq]\n(F.41)\n−TC[Y eval\n1\n; . . . ; Y eval\nE\n| Xeval, Y acq, xacq]) + ceval\n(F.42)\n= 1/E I[Y eval\n1..E ; Y acq | xeval\n1..E, xacq] + ceval\n(F.43)\n−1/E TC[Y eval\n1\n; . . . ; Y eval\nE\n| xeval\n1..E, Y acq, xacq]\n|\n{z\n}\n≥0\n(F.44)\n≤1/E I[Y eval\n1..E ; Y acq | xeval\n1..E, xacq] + ceval.\n(F.45)\nThus, we see that the inequality gap is 1/E TC[Y eval\n1\n; . . . ; Y eval\nE\n| xeval\n1..E, Y acq, xacq], which\nis zero, if and only if the random variables Y eval\ni\n| xeval\ni\nare independent given the\nacquisition samples.\nF.6\nA Practical Approximation of JEPIG\nWe want to find an approximation ˆΩΩΩwith distribution q(ˆωωω), such that for all possible\nacquisition sets, we have:\nI[Y acq\n1..K ;ΩΩΩ| xacq\n1..K, Y eval\n1..E , xeval\n1..E, Dtrain] ≈I[Y acq\n1..K ; ˆΩΩΩ| xacq\n1..K].\n(F.46)\nWe note two properties of this conditional mutual information and the underlying\nmodels p(ΩΩΩ| yeval\n1..E , xeval\n1..E, Dtrain) for different yeval\n1..E ∼p(yeval\n1..E | Dtrain):\nF. Prediction- & Distribution-Aware Bayesian Active Learning\n289\n1. marginalizing p(ΩΩΩ| yeval\n1..E , xeval\n1..E, Dtrain) over all possible yeval\n1..E yields the predictions of\nthe original posterior p(ΩΩΩ| Dtrain), so we would like to have\nEq(ˆωωω) p(y | x, ˆωωω) = p(y | x, Dtrain);\n2. I[Y ;ΩΩΩ| x, Y eval\n1..E , xeval\n1..E, Dtrain] ≤I[Y ;ΩΩΩ| x, Dtrain], and when x ∈{xeval\ni\n}i, we expect\nI[Y ;ΩΩΩ| x, Y eval\n1..E , xeval\n1..E, Dtrain] ≪I[Y ;ΩΩΩ| x, Dtrain]. In other words, the epistemic\nuncertainty of evaluation samples xeval ought to decrease when we also train on the\nevaluation set (using pseudo-labels yeval\n1..E ), and we would like the same for ˆΩΩΩ:\nI[Y ; ˆΩΩΩ| x] ≤I[Y ;ΩΩΩ| x, Dtrain].\nNote, that the second property follows from EPIG being non-negative as mutual\ninformation in two terms and thus so is JEPIG, which means the difference between\nthe two BALD terms is non-negative and the second property is obtained from that.\nHence, as a tractable approximation ˆΩΩΩ, we choose to use a form of self-distillation,\nwhere we train a model with Dtrain and the predictions of the original model p(ωωω |\nDtrain) on xeval\n1..E using a KL-divergence loss, inspired by Hinton et al. [2015] and Zhang\net al. [2019b]1.\nThe loss function this is:\nL(xtrain\n1..N , p(ωωω), q( ˆw | Dtrain)) =\n=\n1\n|Dtrain|\nX\ni\nDKL(p(Y | xtrain\ni\n, Dtrain) ∥q(Y | xtrain\ni\n))\n+ DKL(q(ωωω) ∥p(ωωω)),\n(F.47)\nwith p(Y | xtrain\ni\n) = Ep(ωωω|Dtrain) p(Y | xtrain\ni\n,ωωω) and q(Y | xtrain\ni\n) = Eq(ˆωωω) p(Y | xtrain\ni\n, ˆωωω).\nThe resulting model posterior ˆΩΩΩfulfills both properties described above. This is similar\nto self-distillation in that we train a new model on the predictions of the original\nmodel. However, self-distillation does not use predictions on otherwise unlabeled data.\nIt is also similar to semi-supervised learning [Lee et al., 2013; Yarowsky, 1995] in\nthat we use the predictions of the model on unlabeled data to train a new model.\nHowever, semi-supervised learning only uses the samples for which the model is most\nconfident via confidence thresholding and either temperature-scales them for training\n(soft pseudo-labels) or takes the argmax (hard pseudo-labels) whereas we use the\npredictions without change for all evaluation samples.\nAdvantages of JEPIG. Compared to JEPIG, when evaluating\nI[Y eval\n1..E ; Y acq\n1..K | xacq\n1..K, xeval\n1..E, Dtrain]\n(F.48)\n= H[Y acq\n1..K | xacq\n1..K, Dtrain]\n(F.49)\n−H[Y acq\n1..K | xacq\n1..K, Y eval\n1..E , xeval\n1..E, Dtrain]\n≈Hq∅[Y acq\n1..K | xacq\n1..K] −1\nM\nX\ni\nHqi[Y acq\n1..K | xacq\n1..K]\n(F.50)\nwith separate approximate Bayesian models q∅and qi where q∅(ωωω) ≈p(ωωω | Dtrain)\nand qi(ωωω) ≈p(ωωω | y1..ni, xeval\n1..E, Dtrain) for y1..ni ∼p(yeval\n1..E | xeval\n1..E, Dtrain), i ∈1..M for M\ndraws of pseudo-labels for yeval\n1..E , we found that:\nH(q∅(Y | x,ΩΩΩ))̸= 1\nM\nX\ni\nH(qi(Y | x,ΩΩΩ)),\n1Essentially using α = 1, λ = 0\nF. Prediction- & Distribution-Aware Bayesian Active Learning\n290\nwhich violates the modelling assumption\nH[Y | x,ΩΩΩ, Dtrain] = H[Y | x,ΩΩΩ, Y eval\n1..E , xeval\n1..E, Dtrain]\nas Y ⊥⊥Y eval\n1..E |x, xeval\n1..E,ΩΩΩ. This is even more of an issue when using a single approximate\nmodel with the self-distillation described in the previous section because the two\nproperties we wish for will force H[Y | x, ˆΩΩΩ] ̸= H[Y | x, Dtrain]. This follows immediately\nfrom the expansion of the mutual information: I[Y ;ΩΩΩ|x, Dtrain] = H[Y |x, Dtrain]−H[Y |\nx,ΩΩΩ, Dtrain] as the first property will fix H[Y | x, Dtrain] and the second will force the\nH[Y | x,ΩΩΩ, Dtrain] terms apart to achieve the inequality. However, JEPIG does not\nneed this assumption as it explicitly estimates the epistemic uncertainty, and thus\nperforms better when using our approximation with self-distillation. Moreover, it\nhas its own strong intuitive motivation.\nG\nPrioritized Data Selection during Training\nG.1\nSteps Required for a Given Test Accuracy\nFigs. G.1 (vision) and G.2 (NLP) show the number of steps required to reach a given\ntest accuracy across several datasets for different selection methods. Interestingly, on\nCoLA (unbalanced and noisy), the uniform sampling baseline shows high variance\nacross seeds, while RHO-LOSS works robustly across seeds.\nTable G.1 shows results for RHO-LOSS training without holdout data. Results\nare similar to Table 8.2. Here, we train the IL model without any holdout data. We\nsplit the training set Dpool into two halves and train an IL model on each half. Each\nmodel computes the IL for the half of Dpool that it was not trained on. (This is as in\nFigure 8.2, row 3, except that previously we only used half of Dpool and further split\nit into halves of the half.) Training two IL models costs no additional compute since\neach model is trained on half as much data compared to the default settings.\nG.2\nExperiment Details\nArchitectures. We experiment with various architectures in Figs. 8.1 and 8.2 (row\n4). In all other figures and tables, we use the following architectures: For experiments\non QMNIST, we use a multi-layer perceptron with 2 hidden layers and 512 units in\neach hidden layer. For experiments on CIFAR-10, CIFAR-100 and CINIC-10, we use a\nvariant of ResNet-18 [He et al., 2016]. We adapted the ResNet18 to 32x32 images by\nmodifying the architecture to remove the downsampling effect. We replaced the spatial\ndownsampling of a strided convolution and max pooling in the original ResNet18,\nwith a convolutional layer with 64 filters and a kernel size of 3x3. We also removed\nthe average pooling at the end of the ResNet18. This ResNet18 variant is similar\nto Resnet20, just with more filters. For experiments on Clothing-1M, following the\nexperimental set-up of Yi and Wu [2019], the target model is a ResNet-50 pre-trained\non ImageNet. The irreducible loss model is a ResNet-18 with random initialization.\nThe multiple target architectures in Fig 8.2 were adapted from ?. For NLP datasets,\nwe use a pre-trained ALBERT v2 [Lan et al., 2020].\nHyperparameters. Vision: All models are trained using the AdamW optimizer with\ndefault PyTorch hyperparameters (β1=0.9, β2=0.999, and weight decay of 0.01, learning\nrate 0.001), a K = 32 (64 for CINIC-10) K′ = 320 (640 for CINIC-10), meaning we select\nK\nK′ = 10% of points. NLP: ALBERT v2 was trained using the AdamW optimizer with a\nlearning rate as indicated in the original paper (2 · 10−5) and weight decay of 0.02. We\nfine-tuned all weights, not just the final layer. The batch size K was 32, K′ = 320, meaning\nwe select\nK\nK′ = 10% of points. We use between 2 and 10 seeds for each experiment.\nG. Prioritized Data Selection during Training\n292\n0\n50\n100\n0\n104\nSteps required\nHalf of CIFAR10\n0\n50\n0\n104\nSteps required\nHalf of CIFAR100\n0\n50\nTarget Accuracy (%)\n0\n104\nSteps required\nCINIC10\n0\n50\n100\n0\n104\nHalf of CIFAR10\n(Label Noise)\n0\n50\n0\n104\nHalf of CIFAR100\n(Label Noise)\n0\n50\nTarget Accuracy (%)\n0\n104\nCINIC10 (Label Noise)\n0\n20\n40\n60\n80\nTarget Accuracy (%)\n0\n75⋅103\n15⋅104\nSteps required to reach target accuracy\nClothing-1M\nSelection Method\nRHO-LOSS (Ours)\nUniform Sampling\nIrreducible Loss\nGradient Norm\nLoss\nSVP\nGradient Norm IS\nFigure G.1: Vision datasets—gradient steps required to achieve a given test accuracy\n(lower is better). Left column: The speedup of RHO-LOSS over uniform sampling is the\ngreatest on a large-scale web-scraped dataset with noisy labels. Middle column: Speedups\nare still substantial on clean datasets and RHO-LOSS still achieves higher final accuracy than\nall prior art. Right column: Applying 10% uniform label noise to training data degrades\nother methods but increases the speedup of our method. A step corresponds to lines 5 −10\nin Algorithm 1. Lines correspond to means and shaded areas to minima and maxima across\n3 random seeds. On CIFAR10/100, only half of the data is used for training (see text).\nData Augmentation. On CIFAR-10, CIFAR-100, and CINIC-10, we train using\ndata augmentation (random crop and horizontal flip), both for training the IL model,\nand in the main training runs. Remember that we only compute the irreducible losses\nonce at the start of training, to save compute (Algorithm 3). We use the unaugmented\nimages for this as we found that using augmented images makes little difference to\nperformance but costs more compute.\nIrreducible Loss Model Training. The irreducible loss models are trained on\nholdout sets, i.e. labeled evaluation sets (not test sets, see dataset description in main\ntext). For each dataset, we select the irreducible loss model checkpoint from the\nepoch with the lowest holdout loss on Dpool (as opposed to the highest accuracy); we\nfind that this improves performance while also saving compute as the holdout loss\ntypically reaches its minimum early in training.\nBatchNorm. Like many deep-learning methods, RHO-LOSS interacts with Batch-\nNorm Ioffe and Szegedy [2015] since the loss of a given point is affected by other points\nin the same batch. Important: We compute the BatchNorm statistics for selection\nand model update separately. For selection (line 5-8 in Algorithm 3), the statistics are\nG. Prioritized Data Selection during Training\n293\n40\n50\n60\n70\n80\nTarget accuracy (%)\n0\n300\nSteps required to reach target accuracy\ncola\nRHO-LOSS (Ours)\nUniform Sampling\nIrreducible Loss\nGradient Norm\nLoss\nSVP\nGradient Norm IS\n60\n80\n100\nTarget accuracy (%)\n0\n1000\nsst2\nFigure G.2: NLP datasets—gradient steps required to achieve a given test accuracy (lower\nis better). Left: CoLA grammatical acceptability classification. Right: SST2 sentiment\nclassification. A step corresponds to lines 5 −10 in Algorithm 1. Lines correspond to means\nand shaded areas to standard deviations across 4 or more random seeds. Only half of the\ndata is used for training (see text).\nTable G.1: Epochs required to reach a given target test accuracy when using no holdout\ndata (lower is better). Final accuracy in parentheses. Results averaged across 2-3 seeds. Best\nperformance in bold. RHO-LOSS performs best in both epochs required and final accuracy.\nDataset\nTarget Accuracy\nUniform Sampling\nRHO-LOSS\nCIFAR10\n80%\n39\n17\n90%\n177 (90.8%)\n47 (92.2%)\nCIFAR100\n50%\n47\n22\n65%\n142 (67.8%)\n87 (68.1%)\nCINIC10\n70%\n37\n26\n80%\n146 (80.1%)\n70 (82.1%)\ncomputed across the large batch Bt. For training (line 9-10), the statistics are computed\nacross the small batch bt. These choices can affect performance a lot. For new datasets,\nwe recommend varying how the batch-norm statistics are computed during selection\n(trying both train mode and evaluation mode) and choose the option that works best.\nG.3\nRobustness to Noise\nIn this set of experiments, we evaluate the performance of different selection methods\nunder a variety of noise patterns on QMNIST (MNIST with extra holdout data) and\nvariations thereof. We use this dataset because it has little label noise in its original\nform, allowing us to test the effect of adding noise. Firstly, we add uniform label noise\nto 10% of training points. Secondly, we add structured label noise that affects easily\nconfused classes. We follow Rolnick et al. [2017] and flip the labels of the four most\nfrequently confused classes (in the confusion matrix of a trained model) with 50%\nG. Prioritized Data Selection during Training\n294\n500\n1000 1500\nSteps\n60\n70\n80\n90\n100\nTest Accuracy (%)\nMNIST\n500\n1000 1500\nSteps\n60\n70\n80\n90\n100\nTest Accuracy (%)\nMNIST with 10% Label Noise\n500\n1000 1500\nSteps\n60\n70\n80\n90\n100\nTest Accuracy (%)\nMNIST with Structured Noise\n500\n1000 1500\nSteps\n60\n70\n80\n90\n100\nTest Accuracy (%)\nAmbiguous MNIST\nReducible Loss (Ours)\nUniform Sampling\nIrreducible Loss\nGradient Norm\nLoss\nFigure G.3: RHO-LOSS is robust to a variety of label noise patterns, while other selection\nmethods degrade. A step corresponds to lines 6 −11 in Algorithm 3. Lines correspond to\nmeans and shaded areas to minima and maxima across 3 random seeds.\nprobability. For example, a 2 is often confused with a 5; thus we change the label of\nall 2s to 5s with 50% probability. Thirdly, we leverage the natural noise distribution\nof MNIST by using Ambiguous-MNIST [Mukhoti et al., 2023] as the training set.\nAmbiguous-MNIST contains a training set with 60k generated ambiguous digits that\nhave more than one plausible label. While selecting with loss and gradient norm trains\naccelerates training on the MNIST training set, their performance degrades on all\nthree types of noise distributions (Figure G.3).\nG.4\nIrreducible Holdout Loss Approximation\nIn this appendix section, we examine one of the key approximations made in the theory\nsection. To arrive at Eq. (8.11), we used the approximation H[y | x, Deval] ≈H[y |\nx, Deval, Dtrain]. In words, we approximated the cross-entropy loss of a model trained\non the data points acquired so far Dtrain and the holdout dataset Deval, with the cross-\nentropy loss of a model trained only on the holdout set (i.e. the labeled evaluation set).\nThis approximation saves a lot of compute: rather than having to recompute the term\nwith every change of Dt, it is now sufficient to compute it once at the start of training.\nWe have already highlighted the impact of the approximation on points selected when\ntraining on QMNIST in §8.3.1. In our main experiment setting—using neural networks\ntrained with gradient descent—we empirically find that the approximation does not\nreduce speed of target model training or final target model accuracy (Table G.2). This\nfinding holds across a range of datasets (CIFAR-10, CIFAR-100, CINIC-10). Updating\nthe irreducible loss model on Dtrain seems empirically not necessary.\nIndeed, the approximation actually has two desirable properties when used for\nneural networks trained with gradient descent. We will first describe why we expect\nthese desirable properties, and then show that they indeed appear.\nFirst, let us\nrestate both selection functions:\n• JPIG:\narg max\n(x,y)∈Bt\nH[y | x, Dtrain] −H[y | x, Deval, Dtrain]; and\n(G.1)\n• approximated JPIG:\narg max\n(x,y)∈Bt\nH[y | x, Dtrain] −H[y | x, Deval].\n(G.2)\nG. Prioritized Data Selection during Training\n295\nTable G.2: Number of epochs required to reach a given target test accuracy across several\ndatasets. Results averaged across 2-3 random seeds. NR indicates that the target accuracy\nwas not reached.\nDataset\nTarget accuracy\nRHO-LOSS: Approximated JPIG\nJPIG\nH[y | x, Dtrain] −H[y | x, Deval]\nH[y | x, Dtrain] −H[y | x, Deval, Dtrain]\n60%\n18\n13\nCIFAR10\n75%\n30\n24\n90%\n102\nNR, but reaches 88% in 157 epochs\n30%\n35\n21\nCIFAR100\n45%\n58\nNR, but reaches 43% in 61 epochs\n60%\n123\nNR\n55%\n12\n12\nCINIC10\n65%\n19\n21\n75%\n32\nNR, but reaches 74% in 68 epochs\nDesirable property 1. The approximation prevents repeated selection of undesirable\npoints. When using SGD instead of Bayesian updating, the original selection function\ncan acquire undesired points repeatedly. Let’s say that we acquire, for whatever\nreason, a noisy, redundant, or irrelevant point. We only take one gradient step each\ntime we acquire a (batch of) point(s), meaning the training loss (first term in the\nselection function) will on each only decrease somewhat. In the original selection\nfunction, the second term will also decrease somewhat, meaning that the difference\nbetween the first and second term may remain large. In the approximated selection\nfunction, the second term is constant, the difference between first and second term\nwill thus likely decrease more than under the original selection function. Under the\napproximated selection function, we are thus less likely to acquire undesired points\nagain, if we have acquired them in earlier epochs.\nDesirable property 2. The approximation prevents deterioration of the irreducible\nloss model over time. With both selection functions, we compute the second term of\nthe selection function with an \"irreducible loss model\", which we train on a holdout set\n(i.e. a labeled evaluation set) before we start target model training. In the target model\ntraining, we (greedily) acquire the points that most improve the loss of the target\nmodel (on the holdout set, the labeled evaluation set). We thus deliberately introduce\nbias into the data selection. However, this bias is tailored to the target model and\nmay not be suitable for the irreducible loss model. As a simplifying example, consider\na target model early in training, which has not yet learned a certain class, and an\nirreducible loss model, which has learned that class. Data points in that class will\nhave high training loss, low irreducible loss, and will be acquired often. This, however,\nis not useful for the irreducible loss model, and might lead to decreased accuracy\non data points from other classes. With the approximation, this can’t happen. The\ndescribed failure mode could likely also be alleviated by more sophisticated training\nschemes for the irreducible loss model, such as periodically mixing in data points\nfrom the holdout set (i.e. the labeled evaluation set). However, such training schemes\nwould require even more compute and/or overhead.\nG. Prioritized Data Selection during Training\n296\n0\n20\n40\n60\n80\n100\n120\n140\n160\nepoch\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\nPercentage of acquired points that are corrupted\napproximated objective\noriginal objective\n0\n20\n40\n60\n80\n100\n120\n140\n160\nepoch\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nTest set accuracy of the irreducible loss model\napproximated objective\noriginal objective\nFigure G.4: Desired properties of the irreducible loss model approximation. Left. The\napproximated selection function selects fewer corrupted points later on in training. Right.\nThe test set accuracy of the irreducible loss model deteriorates over time if it is updated on\nDt. With the approximation, the irreducible loss is not updated during target model training.\nResults on CIFAR-10 with 20% of data points corrupted with uniform label noise. Shaded\nareas represent standard deviation across three different random seeds.\nWe find empirically that both desired properties of the approximation indeed\nmanifest themselves. In Figure G.4, we train a target model (Resnet-18) on CIFAR-10,\nwith 20% of the data points corrupted by uniform label noise. The approximated\nselection function leads to faster target model training (the approximated selection\nfunction needs 80 epochs to reach the same target model accuracy that the original\nselection function reaches in 100 epochs) and higher final accuracy than the original\nselection function (88.6% vs 86.1%). Indeed, the original selection function leads to\nacquiring more corrupted points, especially later in training (Figure G.4, left), and\nthe accuracy of the irreducible loss model deteriorates over time (Figure G.4, right).\nWe tuned the learning rate of the irreducible loss model to 0.01 times that of the\ntarget model.\nWithout this adjustment, the results look similar but the original\nselection function performs worse.\nG.5\nExperimental Details for Assessing Impact of\nApproximations\nDataset. QMNIST, with uniform label noise applied to 10% of the dataset. Batch\nsize of 1000 is used.\nModels. Deep Ensemble contains 5 3-layer MLP’s with 512 hidden units. The weaker\nirreducible loss model is an MLP with 256 hidden units.\nTraining. For Approximation 0, we use a deep ensemble for both models.\nThe\nirreducible loss model is trained to convergence on Deval. Then the target model and\nthe irreducible model are used to acquire 10% of points each batch using the selection\nfunction. They are then trained to convergence on each batch of points acquired. The\nirreducible loss model is trained on Deval ∪Dtrain, while the target model is only trained\non Dtrain. We train for a maximum of 5 epochs, which often is to convergence, to\nG. Prioritized Data Selection during Training\n297\n0.0\n2.5\n5.0\n7.5\n10.0\nSpeedup\nCIFAR100\nCIFAR10\nCINIC10\nPercentage Selected\n5%\n10%\n15%\n20%\nFigure G.5: Varying the percent of data points selected in each training batch. Average\nover 3 random seeds.\nenable a fair comparison to further approximations. For Approximation 1a, the deep\nensembles are replaced with single MLPs. The training regime remains the same. We\ncompare the approximations over the first epoch. To compare Approximation 1b to\n0, and for all further approximations, we increase the size of the dataset five-fold, by\nduplicating samples in QMNIST. This means for approximation 1b, we have 5x the\ndata that we have for Approximation 1a, but with increased redundancy. We train the\nmodel in Approximation 1b by taking a single gradient step per data point, with the\nlarger dataset. On the other hand, we train the model for Approximation 0 (still to\nconvergence or 5 epochs) on the standard dataset size. By doing this, Approximation 0\nand 1b have taken the equivalent number of gradient steps, at the time-steps where we\nare tracking the reducible loss of points selected, enabling a fair comparison between\nthe approximations. The irreducible loss models are trained on Deval ∪Dtrain in their\nrespective set-ups. To compare Approximation 2 to Approximation 0, we compare\nupdating the irreducible loss model with a single gradient on each set of acquired points,\nto not updating the irreducible loss model on Dtrain at all. To isolate effect of not\nupdating, we utilize the same initial irreducible loss model. To compare Approximation\n3, we simply train a small irreducible model (one with 256 hidden units) and follow\nthe same training regime as Approximation 2.\nG.6\nAblation of Percentage Selected\nOur method has a hyperparameter, the percentage\nK\nK′ of evaluated points which are\nselected for training. In the experiments above, this parameter was set to 0.1. We have\nnot tuned this parameter, as we aim to analyze how well our method works “out of\nthe box”. In fact, on 2/3 datasets, performance further improves with other values of\nthis parameter. Adjusting this percentage should allow practitioners to specify their\npreferred trade-off between training time and computation, where a low percentage\ntypically corresponds to a lower training time and greater compute cost. For these\nexperiments, we kept K = 32 and adapt K′ accordingly. The percentage\nK\nK′ of data\npoints selected per batch has different effects across datasets as shown in Figure G.5.\nG.7\nActive Learning Baselines\nWe compare our method to typical methods used in the Active Learning (AL) literature.\nNote that our method is label-aware, while active learning acquires data points without\nusing labeled information. We consider the following baselines, which select the top-k\nG. Prioritized Data Selection during Training\n298\n0\n500\n1000\n1500\n2000\nSteps\n75\n80\n85\n90\n95\nTest Accuracy (%)\nMNIST Active Learning\n2000\n4000\nSteps\n20\n40\n60\nTest Accuracy (%)\nCIFAR Active Learning\nRHO-LOSS (Ours)\nUniform Sampling\nBALD\nEntropy\nConditional Entropy\nLoss Minus Conditional Entropy\nFigure G.6: Training curves for several active learning baselines on the MNIST and CIFAR10\ndatasets.\npoints using an acquisition function, α(x):\n• Bayesian Active Learning by Disagreement [Houlsby et al., 2011] with α(x) = H[y|\nx, Dt] −Ep(θ|Dt)[H[y | x, θ]].\n• (Average) conditional entropy, α(x) = Ep(θ|Dt)[H[y | x, θ]], where the average is\ntaken over the model parameter posterior.\n• (Average predictive) entropy, α(x) = H[y | x, Dt].\n• Loss minus conditional entropy α(x) = H[y | x, θ] −Ep(θ|Dt)[H[y | x, θ]]. This\nuses the (average) conditional entropy as an estimate of how noisy data point x\nis—points with high noise are deprioritized. Compared to RHO-LOSS, it replaces\nthe IL with the conditional entropy. This acquisition function uses the label and\ntherefore cannot be used for active learning.\nWe additionally compare our method to uniform sampling. We run all baselines on\nMNIST and CIFAR10. Note that several of these active learning baselines consider\nepistemic uncertainty; that is, uncertainty in predictions driven by uncertainty in\nthe model parameters. This mandates performing (approximate) Bayesian inference.\nWe use Monte-Carlo Dropout[Gal and Ghahramani, 2016a] to perform approximate\ninference. For MNIST, we use an 2 hidden layer MLP with 512 hidden units per\nhidden layer, and a dropout probability of a 0.5.\nFor experiments on CIFAR10,\nwe use a small-scale CNN with dropout probability 0.05 (the dropout probability\nfollows [Osawa et al., 2019]).\nFigure G.6 shows training curves for our method, uniform sampling, and the\nactive learning baselines. Our method accelerates training across both datasets. The\nactive learning methods accelerate training for MNIST but not for CIFAR10. This\nhighlights that active learning methods, if naively applied to online batch selection,\nmay not accelerate model training.\nH\nUnifying Approaches in Active Learning and\nActive Sampling\nH.1\nFisher Information: Additional Derivations &\nProofs\nProposition 9.6. Like observed information, Fisher information is additive:\nH′′[{Yi} | {xi}, ω∗] =\nX\ni\nH′′[{Yi} | xi, ω∗].\n(9.16)\nProof. This follows immediately from Yi ⊥⊥Yj | xi, xj, ω∗for i ̸= j and the additivity\nof the observed information:\nH′′[{Yi} | {xi}, ω∗] = Ep({yi}|{xi},ω∗)[H′′[{yi} | {xi}, ω∗]] = Ep({yi}|{xi},ω∗)[\nX\ni\nH′′[yi | xi, ω∗]]\n(H.1)\n=\nX\ni\nEp(yi|xi,ω∗)[H′′[yi | xi, ω∗]] =\nX\ni\nH′′[yi | xi, ω∗].\n(H.2)\nProposition 9.7. Fisher information is equivalent to:\nH′′[Y | x, ω∗] = Ep(y|x,ω∗)[H′[y | x, ω∗]T H′[y | x, ω∗]] = Cov[H′[Y | x, ω∗]].\n(9.17)\nTo prove Proposition 9.7, we use the two generally useful lemmas below:\nLemma H.1. For the Jacobian H′[y | x, ω∗], we have:\nH′[y | x, ω∗] = ∇ω[−log p(y | x, ω∗)] = −∇ω p(y | x, ω∗)\np(y | x, ω∗)\n,\n(H.3)\nand for the Hessian H′′[y | x, ω∗], we have:\nH′′[y | x, ω∗] = H′[y | x, ω∗]T H′[y | x, ω∗] −∇2\nω p(y | x, ω∗)\np(y | x, ω∗)\n.\n(H.4)\nProof. The result follows immediately from the application of the rules of multivariate\ncalculus.\nH. Unifying Approaches in Active Learning and Active Sampling\n300\nLemma H.2. The following expectations over the model’s own predictions vanish:\nEp(y|x,ω∗)[H′[y | x, ω∗]] = 0,\n(H.5)\nEp(y|x,ω∗)\n\"∇2\nω p(y | x, ω∗)\np(y | x, ω∗)\n#\n= 0.\n(H.6)\nProof. We use the previous equivalences and rewrite the expectations as integral; the\nresults follows:\nEp(y|x,ω∗)[H′[y | x, ω∗]] = Ep(y|x,ω∗)[−∇ω log p(y | x, ω∗)]\n(H.7)\n= −Ep(y|x,ω∗)\n\"∇ω p(y | x, ω∗)\np(y | x, ω∗)\n#\n= −\nZ\n∇ω p(y | x, ω∗) dy = −∇ω\nZ\np(y | x, ω∗) dy = −∇ω1 = 0,\n(H.8)\nEp(y|x,ω∗)\n\"∇2\nω p(y | x, ω∗)\np(y | x, ω∗)\n#\n=\nZ\n∇2\nω p(y | x, ω∗) dy\n(H.9)\n= ∇2\nω\nZ\np(y | x, ω∗) dy\n(H.10)\n= ∇2\nω1 = 0.\n(H.11)\nProof of Proposition 9.7. With the previous lemma, we have the following.\nCov[H′[Y | x, ω∗]] = E[H′[Y | x, ω∗]T H′[Y | x, ω∗]]x\n(H.12)\n−E[H′[Y | x, ω∗]T]\n|\n{z\n}\n=0\nE[H′[Y | x, ω∗]]\n|\n{z\n}\n=0\n= E[H′[Y | x, ω∗]T H′[Y | x, ω∗]].\n(H.13)\nFor the expectation over the Hessian, we plug Lemma H.1 into Lemma H.2 and obtain:\nH′′[Y | x, ω∗] = Ep(y|x,ω∗)[H′′[y | x, ω∗]]\n(H.14)\n= Ep(y|x,ω∗)\n\"\nH′[y | x, ω∗]T H′[y | x, ω∗] −∇2\nω p(y | x, ω∗)\np(y | x, ω∗)\n#\n(H.15)\n= Ep(y|x,ω∗)[H′[y | x, ω∗]T H′[y | x, ω∗]] −0\n(H.16)\n= Cov[H′[Y | x, ω∗]].\n(H.17)\nH.1.1\nSpecial Case: Exponential Family\nProposition 9.8. The Fisher information H′′[Y | x, ω∗] for a model p(y | ˆz = ˆf(x; ω∗))\nis equivalent to:\nH′′[Y | x, ω∗] = ∇ω ˆf(x; ω∗)\nT Ep(y|x,ω∗)[∇2\nˆz H[y | ˆz = ˆf(x; ω∗)]]∇ω ˆf(x; ω∗),\n(9.18)\nwhere ∇2\nˆz H[y | ˆz = ˆf(x; ω∗)] is short for ∇2\nˆz H[y | ˆz]|ˆz= ˆf(x;ω∗).\nH. Unifying Approaches in Active Learning and Active Sampling\n301\nProof. We apply the second equivalence in Proposition 9.7 twice:\nH′′[Y | x, ω∗] = Cov[H′[Y | x, ω∗]]\n(H.18)\n= Cov[∇ω ˆf(x; ω∗)\nT ∇ˆz H[y | ˆz = ˆf(x; ω∗)] ∇ω ˆf(x; ω∗)]\n(H.19)\n= ∇ω ˆf(x; ω∗)\nT Cov[∇ˆz H[y | ˆz = ˆf(x; ω∗)]] ∇ω ˆf(x; ω∗)\n(H.20)\n= ∇ω ˆf(x; ω∗)\nT Ep(y|x,ω∗)[∇2\nˆz H[y | ˆz = ˆf(x; ω∗)]] ∇ω ˆf(x; ω∗)\n(H.21)\nH.1.2\nSpecial Case: Generalized Linear Models\nProposition 9.10. The observed information H′′[y | x, ω∗] of a GLM is independent\nof y.\nH′′[y | x, ω∗] = ∇ω ˆf(x; ω∗)\nT ∇2\nˆz H[y | ˆz = ˆf(x; ω∗)] ∇ω ˆf(x; ω∗)\n(9.22)\n= ∇ω ˆf(x; ω∗)\nT ∇2\nˆzA(wTx) ∇ω ˆf(x; ω∗).\n(9.23)\nProof.\nH′′[y | x, ω∗]\n(H.22)\n= ∇ω[H′[y | x, ω∗]]\n(H.23)\n= ∇ω[∇ˆz H[y | ˆz = ˆf(x; ω∗)] ∇ω ˆf(x; ω∗)]\n(H.24)\n= ∇ˆz H[y | ˆz = ˆf(x; ω∗)] ∇2\nω ˆf(x; ω∗)\n|\n{z\n}\n=∇2ω[wT x]=0\n(H.25)\n+ ∇ω ˆf(x; ω∗)\nT ∇2\nˆz H[y | ˆz = ˆf(x; ω∗)] ∇ω ˆf(x; ω∗)\n= ∇ω ˆf(x; ω∗)\nT ∇2\nˆzA(wTx) ∇ω ˆf(x; ω∗).\n(H.26)\nProposition 9.12. For a GLM, when ˆf(x; ω) : RD →RC, where C is the number of\nclasses (outputs), D is the number of input dimensions, ω ∈RD×C, and assuming the\nparameters are flattened into a single vector for the Jacobian, we have ∇ω ˆf(x; ω) =\nIdC ⊗xT ∈RC×(C·D), where ⊗denotes the Kronecker product, and:\n∇ω ˆf(x; ω∗)\nT ∇2\nˆzA(ωTx) ∇ω ˆf(x; ω∗) = ∇2\nˆzA(wTx) ⊗x xT.\n(9.26)\nProof. We begin with a few statements that lead to the conclusion step by step, where\nx ∈RD, A ∈RC×C, G ∈RC×(C·D):\n(x xT)ij = xi xj\n(H.27)\n(IdC ⊗xT)c,d D+i = xi · 1{c = d}\n(H.28)\n(GT A G)ij =\nX\nk,l\nGki Akl Glj\n(H.29)\n(A ⊗x xT)c D+i,d D+j = Acd xi xj,\n(H.30)\n((IdC ⊗xT)T A (IdC ⊗xT))c D+i,d D+j =\nX\nk,l\n(IdC ⊗xT)k,c D+i Akl (IdC ⊗xT)l,d D+j\n(H.31)\nH. Unifying Approaches in Active Learning and Active Sampling\n302\n=\nX\nk,l\nxi · 1{k = c} Akl xj · 1{l = d} (H.32)\n= xi Acd xj\n(H.33)\n= (A ⊗x xT)c D+i,d D+j.\n(H.34)\n=⇒∇ω ˆf(x; ω∗)\nT ∇2\nˆzA(ωTx) ∇ω ˆf(x; ω∗) = ∇2\nˆzA(wTx) ⊗x xT.\n(H.35)\nProposition 9.11. For a model such that the observed information H′′[y | x, ω∗] is\nindependent of y, we have:\nH′′[Y | x, ω∗] = H′′[y∗| x, ω∗]\n(9.24)\nfor any y∗, and also trivially:\nH′′[Y | x, ω∗] = Ep(y|x)[H′′[y | x, ω∗]].\n(9.25)\nProof. This follows directly from Proposition 9.9. In particular, we have:\nH′′[Y | x, ω∗] = Ep(y|x,ω∗)[H′′[y | x, ω∗]] = H′′[y∗| x, ω∗],\n(H.36)\nwhere we have fixed y∗to an arbitrary value.\nH.2\nApproximating Information Quantities\nH.2.1\nApproximate Expected Information Gain\nLemma 9.13. For symmetric, positive semi-definite matrices A, we have (with equality\niff A = 0):\nlog det(A + Id) ≤tr(A).\n(9.38)\nProof. When A is positive semi-definite and symmetric, its eigenvalues (λi)i are real\nand non-negative. Moreover, A+Id has eigenvalues (λi +1)i; det(A+Id) = Q\ni(λi +1);\nand tr A = P\ni λi.\nThese properties easily follow from the respective eigenvalue\ndecomposition. Thus, we have:\nlog det(A + Id) ≤log\nY\ni\n(λi + 1) =\nX\ni\nlog(λi + 1) ≤\nX\ni\nλi = tr(A),\n(H.37)\nwhere we have used log(x + 1) ≤x iff equality for x = 0.\nGeneral Case. In the main text, we only skimmed the general case and mentioned\nthe main assumption. Here, we look at the general case in detail.\nFor the general case, we need to make strong approximations to be able to pursue\na similar derivation. First, we cannot drop the expectation; instead, we note that the\nlog determinant is a concave function on the positive semi-definite symmetric cone\n[Cover and Thomas, 1988], and we can use Jensen’s inequality on the log determinant\nterm from Equation 9.32 as follows:\nEp({yacq\ni\n}|{xacq\ni\n})[log det\n\u0010\nH′′[{yacq\ni\n} | {xacq\ni\n}, ω∗] H′′[ω∗]−1 + Id\n\u0011\n]\n(H.38)\nH. Unifying Approaches in Active Learning and Active Sampling\n303\n≤log det\n\u0010\nEp({yacq\ni\n}|{xacq\ni\n})[H′′[{yacq\ni\n} | {xacq\ni\n}, ω∗]] H′′[ω∗]−1 + Id\n\u0011\n.\n(H.39)\nSecond, we need to use the following approximation:\np({yacq\ni\n} | {xacq\ni\n}) ≈p({yacq\ni\n} | {xacq\ni\n}, ω∗)\n(H.40)\nto obtain a Fisher information and use its additivity. That is, we obtain:\nEp({yacq\ni\n}|{xacq\ni\n},ω∗)[H′′[{yacq\ni\n} | {xacq\ni\n}, ω∗]] = H′′[{Y acq\ni\n} | {xacq\ni\n}, ω∗]\n(H.41)\n=\nX\ni\nH′′[yacq\ni | xacq\ni, ω∗].\n(H.42)\nPlugging all of this together and applying Lemma 9.13, we obtain the same\nfinal approximation:\nI[Ω; {Y acq\ni\n} | {xacq\ni\n}] = . . .\n(H.43)\n≈1\n2 Ep({yacq\ni\n}|{xacq\ni\n})[log det\n\u0010\nH′′[{yacq\ni\n} | {xacq\ni\n}, ω∗] H′′[ω∗]−1 + Id\n\u0011\n]\n≤1\n2 log det\n\u0010\nEp({yacq\ni\n}|{xacq\ni\n})[H′′[{yacq\ni\n} | {xacq\ni\n}, ω∗]] H′′[ω∗]−1 + Id\n\u0011\n(H.44)\n≈1\n2 log det\n\u0010\nEp({yacq\ni\n}|{xacq\ni\n},ω∗)[H′′[{yacq\ni\n} | {xacq\ni\n}, ω∗]] H′′[ω∗]−1 + Id\n\u0011\n(H.45)\n= 1\n2 log det\n X\ni\nH′′[Y acq\ni | xacq\ni, ω∗] H′′[ω∗]−1 + Id\n!\n(H.46)\n≤1\n2\nX\ni\ntr\n\u0010\nH′′[Y acq\ni | xacq\ni, ω∗] H′′[ω∗]−1\u0011\n.\n(H.47)\nUnlike in the case of generalized linear models, a stronger assumption was necessary\nto reach the same result. Alternatively, we could use the GGN approximation, which\nleads to the same result.\nH.2.2\nApproximate Expected Predicted Information Gain\nIn the main text, we only briefly referred to not knowing a principled way to arrive\nat the same result of Proposition 9.16 for the general case. This is because unlike the\nexpected information gain, the Fisher information for an acquisition candidate now\nlies within a matrix inversion. Even if we used the fact that log det(Id + X Y −1) is\nconcave in X and convex in Y , we would end up with:\n· · ·\n≈1\n2 Ep(yeval,yacq|xeval,xacq) p(xeval) [log det( H′′[yeval | xeval, ω∗] (H′′[yacq | xacq, ω∗]\n+ H′′[ω∗])−1 + Id)]\n(H.48)\n≤1\n2 Ep(yacq|xacq)[log det(Ep(yeval,xeval)[H′′[yeval | xeval, ω∗]] ( H′′[yacq | xacq, ω∗] + H′′[ω∗])−1\n+ Id)]\n(H.49)\n≥1\n2 log det(Ep(yeval,xeval)[H′′[yeval | xeval, ω∗]] ( Ep(yacq|xacq)[H′′[yacq | xacq, ω∗]] + H′′[ω∗])−1\n+ Id)\n(H.50)\n= 1\n2log det(Ep(xeval)[H′′[Y eval | xeval, ω∗]] (H′′[Y acq | xacq, ω∗] + H′′[ω∗])−1 + Id)\n(H.51)\nH. Unifying Approaches in Active Learning and Active Sampling\n304\n≤1\n2tr(Ep(xeval)[H′′[Y eval | xeval, ω∗]] (H′′[Y acq | xacq, ω∗] + H′′[ω∗])−1).\n(H.52)\nNote the ≤. . . ≥, which invalidates the chain. The errors could cancel out, but a\nprincipled statement hardly seems possible using this deduction.\nH.2.3\nApproximate Predictive Information Gain\nSimilarly to Proposition 9.15, we can approximate the predictive information gain. We\nassume that we have access to an (empirical) distribution ˆptrue(xeval, yeval):\nProposition H.3. For a generalized linear model (or with the GGN approximation),\nwhen we take the expectation over ˆptrue(xeval, yeval), we have:\narg max\nxacq\nI[Y eval; yacq | Xeval, xacq, Dtrain] = arg min\nxacq\nI[Ω; Y eval | Xeval, yacq, xacq, Dtrain]\n(H.53)\nwith\nI[Ω; Y eval | Xeval, yacq, xacq, Dtrain]\n(H.54)\n= Eˆptrue(xeval,yeval) I[Ω; yeval | xeval, yacq, xacq, Dtrain]\n≈Eˆptrue(xeval,yeval)[ 1\n2 log det(\nH′′[yeval | xeval, ω∗] (H′′[yacq | xacq, ω∗] + H′′[ω∗| Dtrain])−1 + Id)]\n(H.55)\n≤1\n2 log det( Eˆptrue(xeval,yeval)[H′′[yeval | xeval, ω∗]]\n(H′′[yacq | xacq, ω∗] + H′′[ω∗| Dtrain])−1 + Id)\n(H.56)\n≤1\n2tr\n\u0010\nEˆptrue(xeval,yeval)[H′′[yeval | xeval, ω∗]] (H′′[yacq | xacq, ω∗] + H′′[ω∗| Dtrain])−1\u0011\n.\n(H.57)\nAll of this follows immediately. Only for the second inequality, we need to use\nJensen’s inequality and that the log determinant is on the positive semi-definite\nsymmetric cone [Cover and Thomas, 1988].\nLike for the information gain, there\nis no difference between having access to labels or not when we have a GLM or\nuse the GGN approximation.\nH.2.4\nApproximate Joint (Expected) Predictive Information\nGain\nA comparison of EPIG and JEPIG shows that JEPIG does not require an expectation\nover ˆptrue(xeval) but uses a set of evaluation samples {xeval\ni\n}. As such, we can easily\nadapt Proposition 9.16 to JEPIG and obtain:\nH. Unifying Approaches in Active Learning and Active Sampling\n305\nProposition H.4 (JEPIG). For a generalized linear model (or with the GGN\napproximation), we have:\narg max\n{xacq\ni\n}\nI[{Y eval\ni\n}; {Y acq\ni\n} | {xeval\ni\n}, {xacq\ni }, Dtrain]\n(H.58)\n= arg min\n{xacq\ni\n}\nI[Ω; {Y eval\ni\n} | {xeval\ni\n}, {Y acq\ni\n}, {xacq\ni }, Dtrain]\n(H.59)\nwith\nI[Ω; {Y eval\ni\n} | {xeval\ni\n}, {Y acq\ni\n}, {xacq\ni }, Dtrain]\n≈1\n2log det\n\u0000H′′[{Y eval\ni\n} | {xeval\ni\n}, ω∗] (H′′[{Y acq\ni\n} | {xacq\ni\n}, ω∗] + H′′[ω∗| Dtrain])−1 + Id\n\u0001\n(H.60)\n≤1\n2tr\n\u0010\nH′′[{Y eval\ni\n} | {xeval\ni\n}, ω∗] (H′′[{Y acq\ni\n} | {xacq\ni }, ω∗] + H′′[ω∗| Dtrain])−1\u0011\n.\n(H.61)\nSimilarly, for JPIG, we obtain without relying on the GGN approximation or GLMs:\nProposition H.5 (JPIG). We have:\narg max\n{xacq\ni\n}\nI[{yeval\ni\n}; {yacq\ni\n} | {xeval\ni\n}, {xacq\ni }, Dtrain]\n(H.62)\n= arg min\n{xacq\ni\n}\nI[Ω; {yeval\ni\n} | {xeval\ni\n}, {yacq\ni\n}, {xacq\ni }, Dtrain]\n(H.63)\nwith\nI[Ω; {yeval\ni\n} | {xeval\ni\n}, {Y acq\ni\n}, {xacq\ni }, Dtrain]\n≈1\n2log det\n\u0000H′′[{yeval\ni\n} | {xeval\ni\n}, ω∗] (H′′[{yacq\ni\n} | {xacq\ni\n}, ω∗] + H′′[ω∗| Dtrain])−1 + Id\n\u0001\n(H.64)\n≤1\n2tr\n\u0010\nH′′[{yeval\ni\n} | {xeval\ni\n}, ω∗] (H′′[{yacq\ni\n} | {xacq\ni }, ω∗] + H′′[ω∗| Dtrain])−1\u0011\n.\n(H.65)\nComparison between (E)PIG and J(E)PIG approximations. As observed\ninformation and Fisher information are additive, the difference between the approxi-\nmations when we have an empirical, that is finite, evaluation distribution ˆptrue(xeval)\nwith M samples is a factor of E inside the log determinant or trace:\nEˆptrue(xeval,yeval)[H′′[yeval | xeval, ω∗]] = 1\nE\nX\ni\nH′′[yeval\ni | xeval\ni, ω∗]\n(H.66)\n= 1\nE H′′[{yeval\ni\n} | {xeval\ni\n}, ω∗].\n(H.67)\nFor the log determinant, 1\nE log det(A+Id) ̸= log det(1\nEA+Id), but for the trace approxi-\nmation, we see that both approximations are equal up to a constant factor. For example:\n1\n2tr(Eˆptrue(xeval)\nh\nH′′[Y eval | xeval, ω∗]\ni\n(H′′[{Y acq\ni\n} | {xacq\ni\n}, ω∗] + H′′[ω∗| Dtrain])−1)\n(H.68)\nH. Unifying Approaches in Active Learning and Active Sampling\n306\n= 1\n2tr(1\nEH′′[{Y eval\ni\n} | {xeval\ni\n}, ω∗] (H′′[{Y acq\ni\n} | {xacq\ni\n}, ω∗] + H′′[ω∗| Dtrain])−1)\n(H.69)\n=\n1\n2 Etr(H′′[{Y eval\ni\n} | {xeval\ni\n}, ω∗] (H′′[{Y acq\ni\n} | {xacq\ni\n}, ω∗] + H′′[ω∗| Dtrain])−1).\n(H.70)\nH.3\nSimilarity Matrices and One-Sample Approx-\nimations\nProposition 9.17. Given Dtrain, {xacq\ni } and (sampled) {yacq\ni\n}, we have for the EIG:\nI[Ω; {Y acq\ni\n} | {xacq\ni }, Dtrain]\n≈\n≤1\n2log det\n\u0010\nSH′′[ω∗|Dtrain][Dacq | ω∗] + Id\n\u0011\n(9.66)\n≤1\n2tr SH′′[ω∗|Dtrain][Dacq | ω∗]\n(9.67)\nProof.\nI[Ω; {Y acq\ni\n} | {xacq\ni\n}, Dtrain]\n≈\n≤1\n2log det\n\u0010\nH′′[{Y acq\ni\n} | {xacq\ni\n}, ω∗] H′′[ω∗| Dtrain]−1 + Id\n\u0011\n(H.71)\n= 1\n2log det\n\u0010\n(H′′[{Y acq\ni\n} | {xacq\ni\n}, ω∗] + H′′[ω∗| Dtrain]) H′′[ω∗| Dtrain]−1\u0011\n(H.72)\n≈1\n2log det\n\u0012\n(ˆH′[Dacq | ω∗]\nT ˆH′[Dacq | ω∗] + H′′[ω∗| Dtrain]) H′′[ω∗| Dtrain]−1\n\u0013\n(H.73)\n= 1\n2log det\n\u0012\nˆH′[Dacq | ω∗] H′′[ω∗| Dtrain]−1 ˆH′[Dacq | ω∗]\nT + Id\n\u0013\n(H.74)\n= 1\n2log det\n\u0010\nSH′′[ω∗|Dtrain][Dacq | ω∗] + Id\n\u0011\n,\n(H.75)\nwhere we have used the matrix determinant lemma:\ndet(AB + M) = det(BM −1A + Id) det M.\n(H.76)\nConnection to the Joint (Expected) Predictive Information Gain. Follow-\ning Equation 9.49, JEPIG can be decomposed as the difference between two EIG terms,\nwhich we can further divide into three terms that are only conditioned on Dtrain:\nI[{Y eval\ni\n}; Y acq | {xeval\ni\n}, xacq, Dtrain]\n(H.77)\n= I[Ω; {Y eval\ni\n} | {xeval\ni\n}, Dtrain] −I[Ω; {Y eval\ni\n} | {xeval\ni\n}, Y acq, xacq, Dtrain]\n= I[Ω; {Y eval\ni\n} | {xeval\ni\n}, Dtrain] −I[Ω; {Y eval\ni\n}, Y acq | {xeval\ni\n}, xacq, Dtrain]\n+ I[Ω; Y acq | xacq, Dtrain]\n(H.78)\nUsing Proposition 9.17, we can approximate this as:\nI[{Y eval\ni\n}; Y acq | {xeval\ni\n}, xacq, Dtrain]\n(H.79)\n≈1\n2log det\n\u0010\nSH′′[ω∗|Dtrain][Deval | ω∗] + Id\n\u0011\n−1\n2log det\n\u0010\nSH′′[ω∗|Dtrain][Dacq, Deval | ω∗] + Id\n\u0011\n(H.80)\nH. Unifying Approaches in Active Learning and Active Sampling\n307\n+ 1\n2log det\n\u0010\nSH′′[ω∗|Dtrain][Dacq | ω∗] + Id\n\u0011\n(H.81)\nFurthermore, we can apply the approximation in Proposition 9.18 and find that\nthe log λ terms cancel because |Dacq| + |Dtrain| = |Dacq ∪Dtrain|. Taking the limit\nλ →0, we obtain:\nI[{Y eval\ni\n}; Y acq | {xeval\ni\n}, xacq, Dtrain]\n(H.82)\n≈1\n2 log det\n\u0010\nS[Deval | ω∗] + λId\n\u0011\n−1\n2 log det\n\u0010\nS[Dacq, Deval | ω∗] + λId\n\u0011\n+ 1\n2 log det (S[Dacq | ω∗] + λId)\n(H.83)\n→1\n2 log det\n\u0010\nS[Deval | ω∗]\n\u0011\n−1\n2 log det\n\u0010\nS[Dacq, Deval | ω∗]\n\u0011\n+ 1\n2 log det (S[Dacq | ω∗]) .\n(H.84)\nFinally, the first term is independent of Dacq, and if we are interested in approximately\nmaximizing JEPIG, we can maximize as proxy objective:\nlog det\n\u0010\nSH′′[ω∗|Dtrain][Dacq | ω∗] + Id\n\u0011\n−log det\n\u0010\nSH′′[ω∗|Dtrain][Dacq, Deval | ω∗] + Id\n\u0011\n,\n(H.85)\nor\nlog det (S[Dacq | ω∗]) −log det\n\u0010\nS[Dacq, Deval | ω∗]\n\u0011\n.\n(H.86)\nH.4\nConnection to Other Acquisition Functions in\nthe Literature\nH.4.1\nSIMILAR [Kothawade et al., 2021] and PRISM [Kothawade\net al., 2022]\nConnection to LogDetMI. If we apply the Schur decomposition to log det S[Dacq, Deval | ω∗]\nfrom Equation H.86, we obtain the following:\nlog det S[Dacq | ω∗] −log det S[Dacq, Deval | ω∗]\n(H.87)\n= log det S[Dacq | ω∗] −log det S[Deval | ω∗]\n(H.88)\n−log det(S[Dacq | ω∗] −S[Dacq; Deval | ω∗]S[Deval | ω∗]\n−1S[Deval; Dacq | ω∗]),\nwhere S[Dacq; Deval | ω∗] is the non-symmetric similarity matrix between Dacq and Deval\netc.\nDropping log det S[Deval | ω∗] which is independent of Dacq, we can instead maximize:\nlog det S[Dacq | ω∗] −log det(S[Dacq | ω∗]\n(H.89)\n−S[Dacq; Deval | ω∗]S[Deval | ω∗]\n−1S[Deval; Dacq | ω∗], )\nwhich is exactly the LogDetMI objective of SIMILAR [Kothawade et al., 2021] and\nPRISM [Kothawade et al., 2022].\nWe can further rewrite this objective by extracting S[Dacq | ω∗] from the sec-\nond term, obtaining:\nlog det S[Dacq | ω∗] −log det(S[Dacq | ω∗]\n−S[Dacq; Deval | ω∗]S[Deval | ω∗]\n−1S[Deval; Dacq | ω∗])\n(H.90)\nH. Unifying Approaches in Active Learning and Active Sampling\n308\n= −log det(Id −S[Dacq | ω∗]−1S[Dacq; Deval | ω∗]S[Deval | ω∗]\n−1S[Deval; Dacq | ω∗]).\n(H.91)\nConnection to LogDetCMI. Using information-theoretic decompositions, it\nis easy to show that:\nI[{Y eval\ni\n}; Y acq | {xeval\ni\n}, xacq, {Yi}, {xi}, Dtrain]\n(H.92)\n= I[{Y eval\ni\n}; Y acq, {Yi} | {xeval\ni\n}, xacq, {xi}, Dtrain] −I[{Y eval\ni\n}; {Yi} | {xi}, Dtrain].\n(H.93)\nThese are two JEPIG terms, and using above approximations, including (H.91), leads\nto the LogDetCMI objective:\nlog\ndet(Id −S[Dacq | ω∗]−1S[Dacq; Deval | ω∗]S[Deval | ω∗]\n−1S[Deval; Dacq | ω∗])\ndet(Id −S[Dacq, D | ω∗]−1S[Dacq, D; Deval | ω∗]S[Deval | ω∗]−1S[Deval; Dacq, D | ω∗]).\n(H.94)\nH.4.2\nExpected Gradient Length\nProposition 9.23. The EIG for a candidate sample xacq approximately lower-bounds\nthe EGL:\n2 I[Ω; Y acq | xacq]\n≈\n≤Ep(yacq|xacq,ω∗)\n\r\r\rH′[yacq | xacq, ω∗]\n\r\r\r\n2 + const.\n(9.78)\nProof. The EIG is equal to the conditional entropy up to a constant term, via\nEquation 9.43 in Proposition 9.14:\nI[Ω; Y acq | xacq]\n≈\n≤1\n2 log det\n\u0010\nH′′[Y acq | xacq, ω∗] + H′′[ω∗| Dtrain]\n\u0011\n+ const.\n(H.95)\nWe apply a diagonal approximation for the Fisher information and Hessian, noting\nthat the determinant of the diagonal matrix upper-bounds the determinant of the full\nmatrix:\n≤1\n2 log det\n\u0010\nH′′\ndiag[Y acq | xacq, ω∗] + H′′\ndiag[ω∗| Dtrain]\n\u0011\n+ const. (H.96)\n= 1\n2\nX\nk\nlog\n\u0010\nH′′\ndiag,kk[Y acq | xacq, ω∗] + H′′\ndiag,kk[ω∗| Dtrain]\n\u0011\n+ const.\n(H.97)\nWe use log x ≤x −1 and that H′′[ω∗| Dtrain] is constant:\n≤1\n2\nX\nk\n\u0010\nH′′\ndiag,kk[Y acq | xacq, ω∗] + H′′\ndiag,kk[ω∗| Dtrain]\n\u0011\n+ const. (H.98)\n≤1\n2\nX\nk\nH′′\ndiag,kk[Y acq | xacq, ω∗] + const.\n(H.99)\nFrom Proposition 9.7, we know that the Fisher information is equivalent to the outer\nproduct of the Jacobians: H′′[Y acq | xacq, ω∗] = Ep(yacq|xacq,ω∗)[H′[yacq | xacq, ω∗] H′[yacq |\nxacq, ω∗]T], and we finally obtain for the diagonal elements:\n= 1\n2\nX\nk\nEp(yacq|xacq,ω∗)\nh\nH′\nk[yacq | xacq, ω∗]2i\n+ const.\n(H.100)\n= 1\n2 Ep(yacq|xacq,ω∗)\n\u0014\r\r\rH′[yacq | xacq, ω∗]\n\r\r\r\n2\u0015\n+ const.\n(H.101)\nH. Unifying Approaches in Active Learning and Active Sampling\n309\nH.4.3\nDeep Learning on a Data Diet\nProposition 9.24. The IG for a candidate sample xacq approximately lower-bounds\nthe gradient norm score (GraNd) at ω∗up to a second-order term:\n2 I[Ω; yacq | xacq]\n≈\n≤Eq(ω)[\n\r\r\rH′[yacq | xacq, ω]\n\r\r\r\n2] −Eq(ω)[tr\n ∇2\nω p(y | x, ω)\np(y | x, ω)\n!\n] + const.\n(9.79)\nProof. For any fixed ω∗, the IG is equal to the conditional entropy up to a constant\nterm, via Proposition 9.15:\nI[Ω; Y acq | xacq]\n≈\n≤1\n2 log det\n\u0010\nH′′[yacq | xacq, ω∗] + H′′[ω∗| Dtrain]\n\u0011\n+ const.\n(H.102)\nAs in the previous proof, we apply a diagonal approximation for the Hessian, noting\nthat the determinant of the diagonal matrix upper-bounds the determinant of the full\nmatrix:\n≤1\n2 log det\n\u0010\nH′′\ndiag[yacq | xacq, ω∗] + H′′\ndiag[ω∗| Dtrain]\n\u0011\n+ const. (H.103)\n= 1\n2\nX\nk\nlog\n\u0010\nH′′\ndiag,kk[yacq | xacq, ω∗] + H′′\ndiag,kk[ω∗| Dtrain]\n\u0011\n+ const.\n(H.104)\nAgain, we use log x ≤x −1 and that H′′[ω∗| Dtrain] is constant:\n≤1\n2\nX\nk\n\u0010\nH′′\ndiag,kk[yacq | xacq, ω∗] + H′′\ndiag,kk[ω∗| Dtrain]\n\u0011\n+ const.\n(H.105)\n≤1\n2\nX\nk\nH′′\ndiag,kk[yacq | xacq, ω∗] + const.\n(H.106)\nFrom Lemma H.1, we know that the Hessian is equivalent to the outer product of\nthe Jacobians plus a second-order term: H′′[yacq | xacq, ω∗] = H′[yacq | xacq, ω∗] H′[yacq |\nxacq, ω∗]T −∇2\nω p(yacq|xacq,ω∗)\np(yacq|xacq,ω∗) , and we finally obtain for the diagonal elements:\n= 1\n2\nX\nk\nH′\nk[yacq | xacq, ω∗]2 −1\n2 tr\n ∇2\nω p(yacq | xacq, ω∗)\np(yacq | xacq, ω∗)\n!\n+ const.\n(H.107)\n= 1\n2\n\r\r\rH′[yacq | xacq, ω∗]\n\r\r\r\n2 −1\n2 tr\n ∇2\nω p(yacq | xacq, ω∗)\np(yacq | xacq, ω∗)\n!\n+ const.\n(H.108)\nTaking an expectation over ω∗∼q(ω) yields the statement.\nH.5\nPreliminary Empirical Comparison of Informa-\ntion Quantity Approximations\nThe following section describes an initial empirical evaluation1 of the bounds from §9.4\non MNIST. We train a model on a subset of MNIST and compare the approximations\n1Code at: https://github.com/BlackHC/2208.00549\nH. Unifying Approaches in Active Learning and Active Sampling\n310\n0\n200\n400\n600\n800\n1000\nPool samples (descending by avg rank)\n10\n8\n10\n6\n10\n4\n10\n2\n100\n102\n104\nScore\n(E)IG (Trace)\n(E)IG (LogDet)\nBALD (Prediction Space)\nFigure H.1:\nEIG Approximations.\nTrace and\nlog det approximations match for small scores (be-\ncause of Lemma 9.13).\nThey diverge for large\nscores.\nQualitatively, the order matches the\nprediction-space approximation using BALD with\nMC dropout.\n0\n200\n400\n600\n800\n1000\nPool samples (descending by avg rank)\nScore\nEPIG (Prediction Space)\n(E)PIG (LogDet)\nJ(E)PIG (LogDet)\n(JE)PIG (Trace)\nFigure H.2:\n(J)EPIG Approxi-\nmations (Normalized). The scores\nmatch qualitatively. Note we have\nreversed the ordering for the proxy\nobjectives for JEPIG and EPIG as\nthey are minimized while EPIG is\nmaximized.\n0.000\n0.005\n0.010\n0.015\n0.020\n0.025\nScore\nAcquisition Function = EPIG (Prediction Space)\n400\n420\n440\n460\n480\n500\n520\n540\nAcquisition Function = (JE)PIG (Trace)\n0\n200\n400\n600\n800\n1000\nPool samples (descending by avg rank)\n116\n118\n120\n122\nScore\nAcquisition Function = (E)PIG (LogDet)\n0\n200\n400\n600\n800\n1000\nPool samples (descending by avg rank)\n5550\n5555\n5560\n5565\n5570\n5575\n5580\n5585\nAcquisition Function = J(E)PIG (LogDet)\nFigure H.3: (J)EPIG Approximations. The scores match quantitatively. Note the proxy\nobjectives for JEPIG and EPIG are minimized while EPIG is maximized. The value ranges\nare off by a lot: the true EPIG score is upper bounded by log C ≈2.3 nats.\nH. Unifying Approaches in Active Learning and Active Sampling\n311\nTable H.1: Spearman Rank Correlation of Prediction-Space and Weight-Space Estimates.\nBALD and EPIG are both strongly positively rank-correlated with each other. The weight-\nspace approximations are strongly rank-correlated with the prediction-space approximations,\nbut the weight-space approximations are less accurate than the prediction-space approxima-\ntions. Note that we have reversed the ordering for the proxy objectives for JEPIG and EPIG\nas they are minimized while EPIG is maximized.\nBALD\nEIG\nEIG\nEPIG\nEPIG\nJEPIG\n(J)EPIG\n(Prediction)\n(LogDet)\n(Trace)\n(Prediction)\n(LogDet)\n(LogDet)\n(Trace)\nBALD (Prediction)\n1.000\n0.955\n0.940\n0.984\n0.948\n0.955\n0.927\nEPIG (Prediction)\n0.984\n0.918\n0.897\n1.000\n0.918\n0.918\n0.903\nof EIG and EPIG in weight space to BALD and EPIG computed in prediction space.\nWe use a last-layer approach (GLM) which means that active sampling and active\nlearning approximations are equivalent.\nWe do not attempt to estimate JEPIG\nin prediction space.\nSetup. We train a BNN using MC dropout on 80 randomly selected training\nsamples from MNIST, achieving 83% accuracy. The model architecture follows the\none described in §4. We use 100 Monte-Carlo dropout samples [Gal and Ghahramani,\n2016a] to compute the prediction-space estimates. For EPIG, we sample the evaluation\nset from the remaining training set (20000 samples). We randomly select 1000 samples\nfrom the training set as pool set. We compute BALD and EPIG in prediction space\nas described in Gal et al. [2017] and §7. For the weight-space approximations, we use\na last-layer approximation—we thus have a GLM. For the implementation, we use\nPyTorch [Paszke et al., 2019] and the laplace-torch library [Daxberger et al., 2021].\nWe chose 80 samples and 83% accuracy as the accuracy trajectory of BALD and\nEPIG is steep at this point, see e.g. §4, and thus we expect a wider range of scores.\nResults. In Figure H.1, we see a comparison of BALD with the approximations in\nEquation 9.40 and Equation 9.41. Not shown is Equation 9.43, which performs like\nEquation 9.40 (up to a constant). In Figure H.2 and Figure H.3, we show a comparison\nof EPIG with the approximations in Equation 9.58, Equation 9.59, and Equation H.60.\nFigure H.2 shows normalized scores individually as the score ranges are very different.\nImportantly, while the prediction-space scores (BALD and EPIG) have valid scores\nas the EIG/BALD and EPIG scores of a sample are bounded by the log C, the\nweight-space scores are not valid. As such, they only provide rough estimates of\nthe information quantities.\nHowever, as we see in Table H.1, the Spearman rank correlation coefficients between\nthe weight-space and prediction-space scores are very high. Thus, while the scores\nthemselves are not good estimates, their order seems informative, and this is what\nmatters for selecting acquisition samples in data subset selection.\nFuture Work. The quality of these approximations needs further verification using\nmore complex models and datasets. Comparisons in active learning and active sampling\nexperiments are also necessary to validate the usefulness of these approximations.\nHowever, given the connections shown in §9.6, we expect that the approximations\nwill be useful in these settings as well.\nI\nBlack-Box Batch Active Learning for\nRegression\n−0.5 0.0\n0.5\nBALD\n−0.5\n0.0\n0.5\n■\n□\n−0.5 0.0\n0.5\nACS-FW\n■\n□\n−0.5 0.0\n0.5\nBatchBALD\n■\n□\n−0.5 0.0\n0.5\nBAIT\n■\n□\n−0.5 0.0\n0.5\nCore-Set\n■\n□\n−0.5 0.0\n0.5\nBADGE\n■\n□\n−0.5 0.0\n0.5\nLCMD\n■\n□\n□vs Uniform (↑)\n■vs Uniform (↑)\nDatasets\nct slices\ndiamonds\nfried\nkegg undir\nmethane\nmlr knn rng\nonline video\npoker\nprotein\nquery\nroad\nsarcos\nsgemm\nstock\nwec sydney\nFigure I.1: Final Logarithmic RMSE by regression datasets for DNNs: ■vs □(vs Uniform).\nAcross acquisition functions, the performance of black-box methods is highly correlated\nwith the performance of white-box methods, even though black-box methods make fewer\nassumptions about the model. We plot the improvement of the white-box method (□) over\nthe uniform baseline on the x-axis, so for datasets with markers left of the dashed vertical lines,\nthe white-box method performs better than uniform, and the improvement of the black-box\nmethod (■) over the uniform baseline on the y-axis, so for datasets with markers above\nthe dashed horizontal lines, the black-box method performs better than uniform. Similarly,\nfor datasets with markers in the ■region, the black-box method performs better than the\nwhite-box method. The average over all datasets is marked with a star ⋆.\nI. Black-Box Batch Active Learning for Regression\n313\n256\n512\n1024\n2048\n4096\nTraining set size Ntrain\n−2.2\n−2.0\n−1.8\n−1.6\n−1.4\nmean log MAE\n256\n512\n1024\n2048\n4096\nTraining set size Ntrain\n−1.8\n−1.6\n−1.4\n−1.2\n−1.0\nmean log RMSE\n256\n512\n1024\n2048\n4096\nTraining set size Ntrain\n−1.2\n−1.0\n−0.8\n−0.6\n−0.4\n−0.2\nmean log 95% quantile\n256\n512\n1024\n2048\n4096\nTraining set size Ntrain\n−0.6\n−0.4\n−0.2\n0.0\n0.2\n0.4\nmean log 99% quantile\n256\n512\n1024\n2048\n4096\nTraining set size Ntrain\n0.7\n0.8\n0.9\n1.0\n1.1\n1.2\n1.3\n1.4\nmean log MAXE\nAcq. Function\nUniform\nBALD\nACS-FW\nBatchBALD\nBAIT\nCore-Set\nBADGE\nLCMD\nMethod\n■\n□\n(a) Learning Curves\n64\n128\n256\n512\n1024\n2048\n4096\nAcquisition batch size Nbatch\n−2.3\n−2.2\n−2.1\n−2.0\n−1.9\n−1.8\nmean log MAE\n64\n128\n256\n512\n1024\n2048\n4096\nAcquisition batch size Nbatch\n−1.8\n−1.7\n−1.6\n−1.5\n−1.4\n−1.3\n−1.2\nmean log RMSE\n64\n128\n256\n512\n1024\n2048\n4096\nAcquisition batch size Nbatch\n−1.2\n−1.1\n−1.0\n−0.9\n−0.8\n−0.7\n−0.6\nmean log 95% quantile\n64\n128\n256\n512\n1024\n2048\n4096\nAcquisition batch size Nbatch\n−0.6\n−0.4\n−0.2\n0.0\nmean log 99% quantile\n64\n128\n256\n512\n1024\n2048\n4096\nAcquisition batch size Nbatch\n0.7\n0.8\n0.9\n1.0\n1.1\n1.2\nmean log MAXE\nAcq. Function\nUniform\nBALD\nACS-FW\nBatchBALD\nBAIT\nCore-Set\nBADGE\nLCMD\nMethod\n■\n□\n(b) Acquisition Batch Sizes\nFigure I.2: DNNs: Error Metrics over 15 regression datasets. We report mean absolute error (MAE), root mean squared error (RMSE), 95%\nand 99% quantiles, and the maximum error (MAXE). Averaged over 20 trials.\nI. Black-Box Batch Active Learning for Regression\n314\n256\n512\n1024\n2048\n4096\nTraining set size Ntrain\n−1.8\n−1.6\n−1.4\n−1.2\n−1.0\nmean log MAE\n256\n512\n1024\n2048\n4096\nTraining set size Ntrain\n−1.2\n−1.0\n−0.8\n−0.6\nmean log RMSE\n256\n512\n1024\n2048\n4096\nTraining set size Ntrain\n−0.6\n−0.4\n−0.2\n0.0\nmean log 95% quantile\n256\n512\n1024\n2048\n4096\nTraining set size Ntrain\n−0.2\n0.0\n0.2\n0.4\n0.6\nmean log 99% quantile\n256\n512\n1024\n2048\n4096\nTraining set size Ntrain\n1.2\n1.3\n1.4\n1.5\nmean log MAXE\nAcq. Function\nUniform\n■BALD\n■BatchBALD\n■BAIT\n■ACS-FW\n■Core-Set\n■BADGE\n■LCMD\n(a) Learning Curves\n64\n128\n256\n512\n1024\n2048\n4096\nAcquisition batch size Nbatch\n−1.8\n−1.7\n−1.6\n−1.5\n−1.4\n−1.3\n−1.2\nmean log MAE\n64\n128\n256\n512\n1024\n2048\n4096\nAcquisition batch size Nbatch\n−1.3\n−1.2\n−1.1\n−1.0\n−0.9\n−0.8\n−0.7\nmean log RMSE\n64\n128\n256\n512\n1024\n2048\n4096\nAcquisition batch size Nbatch\n−0.7\n−0.6\n−0.5\n−0.4\n−0.3\n−0.2\n−0.1\n0.0\nmean log 95% quantile\n64\n128\n256\n512\n1024\n2048\n4096\nAcquisition batch size Nbatch\n−0.2\n−0.1\n0.0\n0.1\n0.2\n0.3\n0.4\nmean log 99% quantile\n64\n128\n256\n512\n1024\n2048\n4096\nAcquisition batch size Nbatch\n1.10\n1.15\n1.20\n1.25\n1.30\n1.35\n1.40\n1.45\nmean log MAXE\nAcq. Function\nUniform\n■BALD\n■BatchBALD\n■BAIT\n■ACS-FW\n■Core-Set\n■BADGE\n■LCMD\n(b) Acquisition Batch Size\nFigure I.3: Random Forests: Error Metrics over 15 regression datasets (cont’d). We report mean absolute error (MAE), root mean squared\nerror (RMSE), 95% and 99% quantiles, and the maximum error (MAXE). Averaged over 20 trials.\nI. Black-Box Batch Active Learning for Regression\n315\n256\n512\n1024\n2048\n4096\nTraining set size Ntrain\n−1.6\n−1.4\n−1.2\n−1.0\nmean log MAE\n256\n512\n1024\n2048\n4096\nTraining set size Ntrain\n−1.2\n−1.0\n−0.8\n−0.6\nmean log RMSE\n256\n512\n1024\n2048\n4096\nTraining set size Ntrain\n−0.6\n−0.4\n−0.2\n0.0\n0.2\nmean log 95% quantile\n256\n512\n1024\n2048\n4096\nTraining set size Ntrain\n0.0\n0.2\n0.4\n0.6\nmean log 99% quantile\n256\n512\n1024\n2048\n4096\nTraining set size Ntrain\n1.2\n1.3\n1.4\n1.5\n1.6\nmean log MAXE\nAcq. Function\n■BALD\n■BatchBALD\n■BAIT\n■ACS-FW\n■Core-Set\n■BADGE\n■LCMD\n(a) Learning Curves\n64\n128\n256\n512\n1024\n2048\n4096\nAcquisition batch size Nbatch\n−1.7\n−1.6\n−1.5\n−1.4\n−1.3\n−1.2\n−1.1\nmean log MAE\n64\n128\n256\n512\n1024\n2048\n4096\nAcquisition batch size Nbatch\n−1.3\n−1.2\n−1.1\n−1.0\n−0.9\n−0.8\n−0.7\nmean log RMSE\n64\n128\n256\n512\n1024\n2048\n4096\nAcquisition batch size Nbatch\n−0.6\n−0.5\n−0.4\n−0.3\n−0.2\n−0.1\n0.0\nmean log 95% quantile\n64\n128\n256\n512\n1024\n2048\n4096\nAcquisition batch size Nbatch\n−0.1\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nmean log 99% quantile\n64\n128\n256\n512\n1024\n2048\n4096\nAcquisition batch size Nbatch\n1.1\n1.2\n1.3\n1.4\n1.5\nmean log MAXE\nAcq. Function\n■BALD\n■BatchBALD\n■BAIT\n■ACS-FW\n■Core-Set\n■BADGE\n■LCMD\n(b) Acquisition Batch Size\nFigure I.4: Random Forests (Bagging): Error Metrics over 15 regression datasets (cont’d). We report mean absolute error (MAE), root mean\nsquared error (RMSE), 95% and 99% quantiles, and the maximum error (MAXE). Averaged over 20 trials.\nI. Black-Box Batch Active Learning for Regression\n316\n256\n512\n1024\n2048\n4096\nTraining set size Ntrain\n−1.8\n−1.6\n−1.4\n−1.2\nmean log MAE\n256\n512\n1024\n2048\n4096\nTraining set size Ntrain\n−1.4\n−1.2\n−1.0\n−0.8\nmean log RMSE\n256\n512\n1024\n2048\n4096\nTraining set size Ntrain\n−0.8\n−0.6\n−0.4\n−0.2\n0.0\nmean log 95% quantile\n256\n512\n1024\n2048\n4096\nTraining set size Ntrain\n−0.2\n0.0\n0.2\n0.4\nmean log 99% quantile\n256\n512\n1024\n2048\n4096\nTraining set size Ntrain\n1.0\n1.1\n1.2\n1.3\n1.4\n1.5\nmean log MAXE\nAcq. Function\nUniform\n■BALD\n■BatchBALD\n■BAIT\n■ACS-FW\n■Core-Set\n■BADGE\n■LCMD\n(a) Learning Curves\n64\n128\n256\n512\n1024\n2048\n4096\nAcquisition batch size Nbatch\n−1.9\n−1.8\n−1.7\n−1.6\n−1.5\nmean log MAE\n64\n128\n256\n512\n1024\n2048\n4096\nAcquisition batch size Nbatch\n−1.4\n−1.3\n−1.2\n−1.1\nmean log RMSE\n64\n128\n256\n512\n1024\n2048\n4096\nAcquisition batch size Nbatch\n−0.8\n−0.7\n−0.6\n−0.5\n−0.4\nmean log 95% quantile\n64\n128\n256\n512\n1024\n2048\n4096\nAcquisition batch size Nbatch\n−0.2\n−0.1\n0.0\n0.1\n0.2\nmean log 99% quantile\n64\n128\n256\n512\n1024\n2048\n4096\nAcquisition batch size Nbatch\n1.00\n1.05\n1.10\n1.15\n1.20\n1.25\n1.30\nmean log MAXE\nAcq. Function\nUniform\n■BALD\n■BatchBALD\n■BAIT\n■ACS-FW\n■Core-Set\n■BADGE\n■LCMD\n(b) Acquisition Batch Size\nFigure I.5: Gradient-Boosted Trees with Virtual Ensemble: Error Metrics over 15 regression datasets (cont’d). We report mean absolute error\n(MAE), root mean squared error (RMSE), 95% and 99% quantiles, and the maximum error (MAXE). Averaged over 20 trials.\nI. Black-Box Batch Active Learning for Regression\n317\nTable I.1: Average performance of black-box ■and\nwhite-box □batch active learning acquisition func-\ntions using DNNs. On average, for five acquisition\nmethods, the black-box method performs better\nthan the white-box method. Cf. Figure 10.3, which\nanalyzes the final epoch.\nAcquisition function\nMAE\nRMSE\n95%\n99%\nMAXE\nUniform\n-1.934\n-1.401\n-0.766\n-0.163\n1.107\n■BALD\n-1.794\n-1.389\n-0.713\n-0.221\n0.946\n□BALD\n-1.722\n-1.285\n-0.614\n-0.077\n1.080\n■BatchBALD\n-1.865\n-1.465\n-0.792\n-0.303\n0.892\n□BatchBALD\n-1.895\n-1.463\n-0.808\n-0.288\n0.916\n□BAIT\n-1.998\n-1.541\n-0.895\n-0.357\n0.888\n■BAIT\n-1.892\n-1.489\n-0.817\n-0.328\n0.881\n□ACS-FW\n-1.937\n-1.439\n-0.793\n-0.225\n1.016\n■ACS-FW\n-1.678\n-1.168\n-0.509\n0.085\n1.278\n■Core-Set\n-1.988\n-1.585\n-0.926\n-0.435\n0.831\n□Core-Set\n-1.923\n-1.490\n-0.831\n-0.307\n0.929\n■BADGE\n-2.042\n-1.579\n-0.931\n-0.383\n0.948\n□BADGE\n-2.007\n-1.530\n-0.895\n-0.329\n1.008\n■LCMD\n-2.048\n-1.609\n-0.965\n-0.437\n0.874\n□LCMD\n-2.033\n-1.589\n-0.940\n-0.402\n0.914\nTable I.2:\nAverage performance of\nblack-box ■batch active learning acqui-\nsition functions on non-differentiable\nmodels.\n(a) Random Forests\nAcquisition function\nMAE\nRMSE\n95%\n99%\nMAXE\nUniform\n-1.516\n-0.975\n-0.293\n0.290\n1.376\n■BALD\n-1.222\n-0.815\n-0.106\n0.365\n1.348\n■BatchBALD\n-1.449\n-1.070\n-0.401\n0.064\n1.195\n■BAIT\n-1.582\n-1.158\n-0.485\n0.021\n1.198\n■ACS-FW\n-1.502\n-0.989\n-0.310\n0.266\n1.379\n■Core-Set\n-1.449\n-1.071\n-0.403\n0.059\n1.196\n■BADGE\n-1.618\n-1.150\n-0.480\n0.070\n1.273\n■LCMD\n-1.564\n-1.116\n-0.450\n0.097\n1.270\n(b) Gradient-Boosted Decision Trees\nAcquisition function\nMAE\nRMSE\n95%\n99%\nMAXE\nUniform\n-1.675\n-1.172\n-0.533\n0.068\n1.232\n■BALD\n-1.353\n-0.979\n-0.308\n0.182\n1.225\n■BatchBALD\n-1.474\n-1.103\n-0.448\n0.052\n1.134\n■BAIT\n-1.497\n-1.122\n-0.467\n0.034\n1.137\n■ACS-FW\n-1.501\n-1.000\n-0.354\n0.241\n1.346\n■Core-Set\n-1.573\n-1.193\n-0.552\n-0.036\n1.100\n■BADGE\n-1.709\n-1.259\n-0.621\n-0.050\n1.156\n■LCMD\n-1.688\n-1.256\n-0.616\n-0.061\n1.132\nTable I.3: Overview over the used datasets. See Ballester-Ripoll et al. [2019]; Neshat et al.\n[2018]; Graf et al. [2011]; Shannon et al. [2003]; Deneke et al. [2014]; Anagnostopoulos et al.\n[2018]; Savva et al. [2018]; Friedman [1991]; Ślęzak et al. [2018]. The second column entries\nare hyperlinks to the respective web pages. Taken from Holzmüller et al. [2022].\nShort name\nInitial pool set size\nTest set size\nNumber of features\nSource\nOpenML ID\nFull name\nsgemm\n192000\n48320\n14\nUCI\nSGEMM GPU kernel performance\nwec_sydney\n56320\n14400\n48\nUCI\nWave Energy Converters\nct_slices\n41520\n10700\n379\nUCI\nRelative location of CT slices on axial axis\nkegg_undir\n50407\n12921\n27\nUCI\nKEGG Metabolic Reaction Network (Undirected)\nonline_video\n53748\n13756\n26\nUCI\nOnline Video Characteristics and Transcoding Time\nquery\n158720\n40000\n4\nUCI\nQuery Analytics Workloads\npoker\n198720\n300000\n95\nUCI\nPoker Hand\nroad\n198720\n234874\n2\nUCI\n3D Road Network (North Jutland, Denmark)\nmlr_knn_rng\n88123\n22350\n132\nOpenML\n42454\nmlr_knn_rng\nfried\n31335\n8153\n10\nOpenML\n564\nfried\ndiamonds\n41872\n10788\n29\nOpenML\n42225\ndiamonds\nmethane\n198720\n300000\n33\nOpenML\n42701\nMethane\nstock\n45960\n11809\n9\nOpenML\n1200\nBNG(stock)\nprotein\n35304\n9146\n9\nOpenML\n42903\nphysicochemical-protein\nsarcos\n34308\n8896\n21\nGPML\nSARCOS data\nBibliography\nBayesian Inference of Individualized Treatment Effects using Multi-Task Gaussian Processes.\nIn Advances in Neural Information Processing Systems, 2017.\nJacob D. Abernethy, Pranjal Awasthi, Matthäus Kleindessner, Jamie Morgenstern, Chris\nRussell, and Jie Zhang. Active Sampling for Min-Max Fairness. In Proceedings of the\nInternational Conference on Machine Learning (ICML), 2022.\nJason Abrevaya, Yu-Chin Hsu, and Robert P Lieli. Estimating conditional average treatment\neffects. Journal of Business & Economic Statistics, 2015.\nGediminas Adomavicius and Alexander Tuzhilin.\nToward the Next Generation of Rec-\nommender Systems: A Survey of the State-of-the-Art and Possible Extensions. IEEE\nTransactions on Knowledge and Data Engineering, 2005.\nHomayun Afrabandpey, Tomi Peltola, and Samuel Kaski.\nHuman-in-the-loop Active\nCovariance Learning for Improving Prediction in Small Data Sets. In Proceedings of\nthe International Joint Conference on Artificial Intelligence (IJCAI), 2019.\nAhmed M. Alaa and Mihaela van der Schaar. Bayesian Nonparametric Causal Inference:\nInformation Rates and Learning Algorithms. IEEE Journal of Selected Topics in Signal\nProcessing, 2018.\nGuillaume Alain, Alex Lamb, Chinnadhurai Sankar, Aaron C. Courville, and Yoshua Bengio.\nVariance Reduction in SGD by Distributed Importance Sampling. arXiv preprint, 2015.\nGörkem Algan and Ilkay Ulusoy. Image classification with deep learning in the presence of\nnoisy labels: A survey. Knowledge-Based Systems, 2021.\nRahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Bengio. Gradient Based Sample\nSelection for Online Continual Learning. arXiv preprint, 2019.\nAhsan S. Alvi, Bin Xin Ru, Jan-Peter Calliess, Stephen J. Roberts, and Michael A.\nOsborne. Asynchronous Batch Bayesian Optimisation with Improved Local Penalisation.\nIn Proceedings of the International Conference on Machine Learning (ICML), 2019.\nChristos Anagnostopoulos, Fotis Savva, and Peter Triantafillou.\nScalable Aggregation\nPredictive Analytics - A Query-Driven Machine Learning Approach. Applied Intelligence,\n2018.\nRohan Anil, Gabriel Pereyra, Alexandre Passos, Róbert Ormándi, George E. Dahl, and\nGeoffrey E. Hinton.\nLarge scale distributed neural network training through online\ndistillation. In International Conference on Learning Representations (ICLR), 2018.\nRosana Ardila, Megan Branson, Kelly Davis, Michael Kohler, Josh Meyer, Michael Henretty,\nReuben Morais, Lindsay Saunders, Francis M. Tyers, and Gregor Weber. Common Voice:\nA Massively-Multilingual Speech Corpus. In Proceedings of The Language Resources and\nEvaluation Conference (LREC), 2020.\nDavid Arthur and Sergei Vassilvitskii. k-means++: the advantages of careful seeding. In\nProceedings of the Annual ACM-SIAM Symposium on Discrete Algorithms, 2007.\nJordan T. Ash and Ryan P. Adams. On Warm-Starting Neural Network Training. In Advances\nin Neural Information Processing Systems, 2020.\nJordan T. Ash, Chicheng Zhang, Akshay Krishnamurthy, John Langford, and Alekh Agarwal.\nDeep Batch Active Learning by Diverse, Uncertain Gradient Lower Bounds. In International\nConference on Learning Representations (ICLR), 2020.\nJordan T. Ash, Surbhi Goel, Akshay Krishnamurthy, and Sham M. Kakade. Gone Fishing:\nNeural Active Learning with Fisher Embeddings. In Advances in Neural Information\nProcessing Systems, 2021.\nParmida Atighehchian, Frédéric Branchaud-Charron, and Alexandre Lacoste. Bayesian active\nlearning for production, a systematic study and a reusable library. arXiv preprint, 2020.\nBibliography\n319\nLes E. Atlas, David A. Cohn, and Richard E. Ladner. Training Connectionist Networks with\nQueries and Selective Sampling. In Advances in Neural Information Processing Systems,\n1989.\nJavad Azimi, Alan Fern, Xiaoli Zhang Fern, Glencora Borradaile, and Brent Heeringa. Batch\nActive Learning via Coordinated Matching. In Proceedings of the International Conference\non Machine Learning (ICML), 2012.\nR. Harald Baayen and Rochelle Lieber. Word frequency distributions and lexical semantics.\nComputational Humanities, 1996.\nChristina Baek, Yiding Jiang, Aditi Raghunathan, and Zico Kolter. Agreement-on-the-Line:\nPredicting the Performance of Neural Networks under Distribution Shift, 2022.\nRafael Ballester-Ripoll, Enrique G. Paredes, and Renato Pajarola. Sobol Tensor Trains for\nGlobal Sensitivity Analysis. Reliability Engineering and System Safety, 2019.\nTysam Balsam. hlb-CIFAR10, 2023. URL https://github.com/tysam-code/hlb-CIFAR10.\nHangbo Bao, Li Dong, Songhao Piao, and Furu Wei. BEiT: BERT Pre-Training of Image\nTransformers, 2021.\nBarber and Agakov. The IM algorithm: a variational approach to information maximization.\nNeurIPS, 2003.\nCenk Baykal, Lucas Liebenwein, Dan Feldman, and Daniela Rus. Low-Regret Active learning.\narXiv preprint, 2021.\nBeck and Arnold. Parameter Estimation in Engineering and Science. Wiley, 1977.\nBelkin, Hsu, Ma, and Mandal. Reconciling modern machine-learning practice and the classical\nbias-variance trade-off. Proceedings of the National Academy of Sciences, 2019.\nMarc G. Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and\nRémi Munos. Unifying Count-Based Exploration and Intrinsic Motivation. In Advances in\nNeural Information Processing Systems, 2016.\nWilliam H. Beluch, Tim Genewein, Andreas Nürnberger, and Jan M. Köhler. The Power of\nEnsembles for Active Learning in Image Classification. In IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), 2018.\nYoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston. Curriculum learning.\nIn Proceedings of the International Conference on Machine Learning (ICML), 2009.\nWilliam Bialek and Naftali Tishby. Predictive information. arXiv preprint, 1999.\nFreddie Bickford-Smith, Andreas Kirsch, Sebastian Farquhar, Yarin Gal, Adam Foster, and\nTom Rainforth. Prediction-Oriented Bayesian Active Learning. International Conference\non Artificial Intelligence and Statistics, 2023.\nSarah Bird, Miro Dudík, Richard Edgar, Brandon Horn, Roman Lutz, Vanessa Milan,\nMehrnoosh Sameki, Hanna Wallach, and Kathleen Walker.\nFairlearn: A toolkit for\nassessing and improving fairness in AI. Technical Report MSR-TR-2020-32, Microsoft,\n2020.\nErdem Bıyık, Kenneth Wang, Nima Anari, and Dorsa Sadigh. Batch active learning using\ndeterminantal point processes. arXiv preprint, 2019.\nDavid M. Blei, Andrew Y. Ng, and Michael I. Jordan. Latent Dirichlet Allocation. In\nAdvances in Neural Information Processing Systems, 2001.\nCharles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra.\nWeight\nUncertainty in Neural Network. In Proceedings of the International Conference on Machine\nLearning (ICML), 2015.\nRishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ B. Altman, Simran Arora, Sydney\nvon Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik\nBrynjolfsson, Shyamal Buch, Dallas Card, Rodrigo Castellon, Niladri S. Chatterji, Annie S.\nChen, Kathleen Creel, Jared Quincy Davis, Dorottya Demszky, Chris Donahue, Moussa\nBibliography\n320\nDoumbouya, Esin Durmus, Stefano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-\nFei, Chelsea Finn, Trevor Gale, Lauren Gillespie, Karan Goel, Noah D. Goodman, Shelby\nGrossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho,\nJenny Hong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil Jain, Dan Jurafsky, Pratyusha\nKalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte Khani, Omar Khattab, Pang Wei\nKoh, Mark S. Krass, Ranjay Krishna, Rohith Kuditipudi, and et al. On the Opportunities\nand Risks of Foundation Models. arXiv preprint, 2021.\nZalán Borsos, Mojmir Mutny, and Andreas Krause. Coresets via Bilevel Optimization for\nContinual Learning and Streaming. In Advances in Neural Information Processing Systems,\n2020.\nZalán Borsos, Mojmír Mutný, Marco Tagliasacchi, and Andreas Krause. Data Summarization\nvia Bilevel Optimization. arXiv preprint, 2021.\nLéon Bottou and Yann LeCun.\nLarge Scale Online Learning.\nIn Advances in Neural\nInformation Processing Systems, 2003.\nJames Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal\nMaclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and\nQiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL\nhttp://github.com/google/jax.\nFrédéric Branchaud-Charron, Parmida Atighehchian, Pau Rodríguez, Grace Abuhamad, and\nAlexandre Lacoste. Can Active Learning Preemptively Mitigate Fairness Issues? ICLR\nWorkshop on Responsible AI, 2021.\nLeo Breiman. Bagging Predictors. Machine Learning, 1996.\nLeo Breiman. Random Forests. Machine Learning, 2001.\nKlaus Brinker. Incorporating Diversity in Active Learning with Support Vector Machines. In\nProceedings of the International Conference on Machine Learning (ICML), 2003.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini\nAgarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya\nRamesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner,\nSam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language Models are\nFew-Shot Learners. In Advances in Neural Information Processing Systems, 2020.\nLars Buitinck, Gilles Louppe, Mathieu Blondel, Fabian Pedregosa, Andreas Mueller, Olivier\nGrisel, Vlad Niculae, Peter Prettenhofer, Alexandre Gramfort, Jaques Grobler, Robert\nLayton, Jake VanderPlas, Arnaud Joly, Brian Holt, and Gaël Varoquaux. API design for\nmachine learning software: experiences from the scikit-learn project. In ECML PKDD\nWorkshop: Languages for Data Mining and Machine Learning, 2013.\nRobert Burbidge, Jem J. Rowland, and Ross D. King. Active Learning for Regression Based\non Query by Committee. In Intelligent Data Engineering and Automated Learning, 2007.\nDaniel A Butts. How much information is associated with a particular stimulus? Network:\nComputation in Neural Systems, 2003.\nSylvain Calinon, Florent Guenter, and Aude Billard.\nOn Learning, Representing, and\nGeneralizing a Task in a Humanoid Robot. IEEE Transactions on Systems, Man, and\nCybernetics, Part B, 2007.\nColin Campbell, Nello Cristianini, and Alexander J. Smola. Query Learning with Large\nMargin Classifiers. In Proceedings of the International Conference on Machine Learning\n(ICML), 2000.\nTrevor Campbell and Tamara Broderick. Bayesian Coreset Construction via Greedy Iterative\nGeodesic Ascent. In Proceedings of the International Conference on Machine Learning\nBibliography\n321\n(ICML), 2018.\nTrevor Campbell and Tamara Broderick. Automated Scalable Bayesian Inference via Hilbert\nCoresets. Journal of Machine Learning Research (JMLR), 2019.\nDaniel R. Cavagnaro, Jay I. Myung, Mark A. Pitt, and Janne V. Kujala. Adaptive Design\nOptimization: A Mutual Information-Based Approach to Model Discrimination in Cognitive\nScience. Neural Computation, 2010.\nChaloner and Verdinelli. Bayesian experimental design: a review. Statistical Science, 1995a.\nKathryn Chaloner and Isabella Verdinelli. Bayesian experimental design: A review. Statistical\nScience, 1995b.\nOlivier Chapelle. Active Learning for Parzen Window Classifier. In Proceedings of the\nInternational Workshop on Artificial Intelligence and Statistics (AISTATS), 2005.\nKamalika Chaudhuri, Sham M. Kakade, Praneeth Netrapalli, and Sujay Sanghavi. Con-\nvergence Rates of Active Learning for Maximum Likelihood Estimation. In Advances in\nNeural Information Processing Systems, 2015.\nJiefeng Chen, Frederick Liu, Besim Avci, Xi Wu, Yingyu Liang, and Somesh Jha. Detecting\nErrors and Estimating Accuracy on Unlabeled Data with Self-training Ensembles, 2021.\nLiang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking\nAtrous Convolution for Semantic Image Segmentation. arXiv preprint, 2017.\nPengfei Chen, Benben Liao, Guangyong Chen, and Shengyu Zhang. Understanding and\nUtilizing Deep Neural Networks Trained with Noisy Labels.\nIn Proceedings of the\nInternational Conference on Machine Learning (ICML), 2019.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker\nSchuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes,\nYi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson,\nReiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier\nGarcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David\nLuan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan,\nShivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai,\nMarie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine\nLee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele\nCatasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and\nNoah Fiedel. PaLM: Scaling Language Modeling with Pathways. arXiv preprint, 2022.\nSayak Ray Chowdhury and Aditya Gopalan. On Batch Bayesian Optimization. arXiv preprint,\n2019.\nKenneth Ward Church and Patrick Hanks. Word Association Norms, Mutual Information\nand Lexicography. In Annual Meeting of the Association for Computational Linguistics,\n1989.\nGui Citovsky, Giulia DeSalvo, Claudio Gentile, Lazaros Karydas, Anand Rajagopalan, Afshin\nRostamizadeh, and Sanjiv Kumar. Batch Active Learning at Scale. In Advances in Neural\nInformation Processing Systems, 2021.\nAdam D. Cobb, Stephen J. Roberts, and Yarin Gal. Loss-Calibrated Approximate Inference\nin Bayesian Neural Networks. arXiv preprint, 2018.\nGregory Cohen, Saeed Afshar, Jonathan Tapson, and André van Schaik. EMNIST: Extending\nMNIST to handwritten letters. In International Joint Conference on Neural Networks\n(IJCNN), 2017.\nDavid A. Cohn. Neural Network Exploration Using Optimal Experiment Design. In Advances\nin Neural Information Processing Systems, 1993.\nBibliography\n322\nDavid A. Cohn, Zoubin Ghahramani, and Michael I. Jordan. Active Learning with Statistical\nModels. Journal of Artificial Intelligence Research, 1996.\nCody Coleman, Christopher Yeh, Stephen Mussmann, Baharan Mirzasoleiman, Peter Bailis,\nPercy Liang, Jure Leskovec, and Matei Zaharia. Selection via Proxy: Efficient Data\nSelection for Deep Learning. In International Conference on Learning Representations\n(ICLR), 2020.\nEmile Contal, David Buffoni, Alexandre Robicquet, and Nicolas Vayatis. Parallel Gaussian\nProcess Optimization with Upper Confidence Bound and Pure Exploration. In Machine\nLearning and Knowledge Discovery in Databases - European Conference, 2013.\nCover and Thomas. Elements of Information Theory. John Wiley & Sons, 2005.\nThomas M Cover and A Thomas. Determinant inequalities via information theory. SIAM\nJournal on Matrix Analysis and Applications, 1988.\nPedram Daee, Tomi Peltola, Marta Soare, and Samuel Kaski. Knowledge elicitation via\nsequential probabilistic inference for high-dimensional prediction. Machine Learning, 2017.\nLuke Nicholas Darlow, Elliot J. Crowley, Antreas Antoniou, and Amos J. Storkey. CINIC-10\nis not ImageNet or CIFAR-10. arXiv preprint, 2018.\nErik Daxberger, Agustinus Kristiadi, Alexander Immer, Runa Eschenhagen, Matthias Bauer,\nand Philipp Hennig. Laplace Redux - Effortless Bayesian Deep Learning. In Advances in\nNeural Information Processing Systems, 2021.\nErik A. Daxberger and Bryan Kian Hsiang Low.\nDistributed Batch Gaussian Process\nOptimization. In Proceedings of the International Conference on Machine Learning (ICML),\n2017.\nTewodors Deneke, Habtegebreil Haile, Sébastien Lafond, and Johan Lilius. Video Transcoding\nTime Prediction for Proactive Load Balancing. In International Conference on Multimedia\nand Expo, 2014.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Fei-Fei Li. ImageNet: A large-scale\nhierarchical image database. In IEEE Computer Society Conference on Computer Vision\nand Pattern Recognition (CVPR), 2009.\nKun Deng, Joelle Pineau, and Susan Murphy. Active Learning for Personalizing Treatment.\nIn IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning,\n2011.\nArmen Der Kiureghian and Ove Ditlevsen. Aleatory or epistemic? Does it matter? Structural\nSafety, 2009.\nTerrance DeVries and Graham W. Taylor. Learning Confidence for Out-of-Distribution\nDetection in Neural Networks. arXiv preprint, 2018.\nMichael R DeWeese and Markus Meister. How to measure the information gained from one\nsymbol. Network: Computation in Neural Systems, 1999.\nThomas G. Dietterich. Ensemble Methods in Machine Learning. In Workshop of Multiple\nClassifier Systems at IEEE/CVF Proceedings, 2000.\nLaurent Dinh, David Krueger, and Yoshua Bengio. NICE: Non-linear Independent Com-\nponents Estimation. In Workshop Track Proceedings of the International Conference on\nLearning Representations (ICLR), 2015.\nPinar Donmez and Jaime G. Carbonell. Optimizing estimated loss reduction for active\nsampling in rank learning. In Proceedings of the International Conference on Machine\nLearning (ICML), 2008.\nAnna Veronika Dorogush, Vasily Ershov, and Andrey Gulin. CatBoost: Gradient Boosting\nwith Categorical Features Support. arXiv preprint, 2018.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,\nBibliography\n323\nJakob Uszkoreit, and Neil Houlsby. An Image is Worth 16x16 Words: Transformers for\nImage Recognition at Scale, 2020.\nDheeru Dua and Casey Graff. UCI Machine Learning Repository, 2017. URL http://\narchive.ics.uci.edu/ml.\nMichael Dusenberry, Ghassen Jerfel, Yeming Wen, Yi-An Ma, Jasper Snoek, Katherine A.\nHeller, Balaji Lakshminarayanan, and Dustin Tran. Efficient and Scalable Bayesian Neural\nNets with Rank-1 Factors. In Proceedings of the International Conference on Machine\nLearning (ICML), 2020.\nAndre Esteva, Brett Kuprel, Roberto A. Novoa, Justin Ko, Susan M. Swetter, Helen M. Blau,\nand Sebastian Thrun. Dermatologist-level classification of skin cancer with deep neural\nnetworks. Nature, 2017.\nLewis P. G. Evans, Niall M. Adams, and Christoforos Anagnostopoulos. Estimating Optimal\nActive Learning via Model Retraining Improvement. arXiv preprint, 2015.\nMark Everingham, Luc Van Gool, Christopher K. I. Williams, John M. Winn, and Andrew\nZisserman. The Pascal Visual Object Classes (VOC) Challenge. International Journal of\nComputer Vision, 2010.\nRobert M. Fano. Transmission of Information. Wiley, 1962.\nSebastian Farquhar and Yarin Gal. What ‘Out-of-distribution’ Is and Is Not. \"ML Safety\nWorkshop\" Workshopat at NeurIPS, December 2022.\nSebastian Farquhar, Yarin Gal, and Tom Rainforth. On Statistical Bias In Active Learning:\nHow and When to Fix It. In International Conference on Learning Representations (ICLR),\n2021.\nYassir Fathullah, Mark JF Gales, and Andrey Malinin. Ensemble distillation approaches for\ngrammatical error correction. In IEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP), 2021.\nValerii Vadimovich Fedorov. Theory of Optimal Experiments. Elsevier, 1972.\nAngelos Filos, Sebastian Farquhar, Aidan N. Gomez, Tim G. J. Rudner, Zachary Kenton,\nLewis Smith, Milad Alizadeh, Arnoud de Kroon, and Yarin Gal. A Systematic Comparison\nof Bayesian Deep Learning Robustness in Diabetic Retinopathy Tasks. arXiv preprint,\n2019.\nLouis Filstroff, Iiris Sundin, Petrus Mikkola, Aleksei Tiulpin, Juuso Kylmäoja, and Samuel\nKaski. Targeted Active Learning for Bayesian Decision-Making. arXiv preprint, 2021.\nFisher. Statistical Methods for Research Workers. Oliver and Boyd, 1925.\nJose Pablo Folch, Robert M. Lee, Behrang Shafei, David Walz, Calvin Tsay, Mark van der\nWilk, and Ruth Misener. Combining multi-fidelity modelling and asynchronous batch\nBayesian Optimization. Journal of Computers and Chemical Engineering, 2023.\nEdwin Fong and Chris C Holmes. On the marginal likelihood and cross-validation. Biometrika,\n2020.\nAdam Foster, Martin Jankowiak, Eli Bingham, Paul Horsfall, Yee Whye Teh, Tom Rainforth,\nand Noah D. Goodman. Variational Bayesian Optimal Experimental Design. In Advances\nin Neural Information Processing Systems, 2019.\nAdam Foster, Desi R. Ivanova, Ilyas Malik, and Tom Rainforth. Deep Adaptive Design:\nAmortizing Sequential Bayesian Experimental Design. In Proceedings of the International\nConference on Machine Learning (ICML), 2021.\nAE Foster. Variational, Monte Carlo and policy-based approaches to Bayesian experimental\ndesign. PhD thesis, University of Oxford, 2022.\nJonathan Frankle and Michael Carbin. The Lottery Ticket Hypothesis: Finding Sparse,\nTrainable Neural Networks. In International Conference on Learning Representations\n(ICLR), 2019.\nBibliography\n324\nRichard Fredlund, Richard M Everson, and Jonathan E Fieldsend. A Bayesian framework\nfor active learning. In International Joint Conference on Neural Networks (IJCNN), 2010.\nLinton C Freeman. Elementary applied statistics: for students in behavioral science. John\nWiley & Sons, 1965.\nYoav Freund, H. Sebastian Seung, Eli Shamir, and Naftali Tishby. Selective Sampling Using\nthe Query by Committee Algorithm. Machine Learning, 1997.\nJerome H Friedman. Multivariate Adaptive Regression Splines. The Annals of Statistics,\n1991.\nYarin Gal. Uncertainty in Deep Learning. PhD thesis, University of Cambridge, 2016.\nYarin Gal and Zoubin Ghahramani. Dropout as a Bayesian Approximation: Representing\nModel Uncertainty in Deep Learning. In Proceedings of the International Conference on\nMachine Learning (ICML), 2016a.\nYarin Gal and Zoubin Ghahramani. Dropout as a Bayesian Approximation: Representing\nModel Uncertainty in Deep Learning. In Proceedings of the International Conference on\nMachine Learning (ICML), 2016b.\nYarin Gal, Riashat Islam, and Zoubin Ghahramani. Deep Bayesian Active Learning with\nImage Data. In Proceedings of the International Conference on Machine Learning (ICML),\n2017.\nZijun Gao and Yanjun Han. Minimax Optimal Nonparametric Estimation of Heterogeneous\nTreatment Effects. In Advances in Neural Information Processing Systems, 2020.\nSaurabh Garg, Sivaraman Balakrishnan, Zachary C. Lipton, Behnam Neyshabur, and Hanie\nSedghi. Leveraging Unlabeled Data to Predict Out-of-Distribution Performance, 2022.\nYonatan Geifman and Ran El-Yaniv. Deep Active Learning over the Long Tail. arXiv preprint,\n2017.\nJort F. Gemmeke, Daniel P. W. Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence,\nR. Channing Moore, Manoj Plakal, and Marvin Ritter. Audio Set: An ontology and\nhuman-labeled dataset for audio events. In IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP), 2017.\nTilmann Gneiting and Adrian E Raftery. Strictly proper scoring rules, prediction, and\nestimation. Journal of the American Statistical Association, 2007.\nDaniel Golovin and Andreas Krause. Adaptive Submodularity: Theory and Applications in\nActive Learning and Stochastic Optimization. Journal of Artificial Intelligence Research,\n2011.\nJavier González, Zhenwen Dai, Philipp Hennig, and Neil D. Lawrence. Batch Bayesian\nOptimization via Local Penalization. International Conference on Artificial Intelligence\nand Statistics (AISTATS), 2016.\nAchintya Gopal. Why Calibration Error is Wrong Given Model Uncertainty: Using Posterior\nPredictive Checks with Deep Learning, 2021.\nHenry Gouk, Eibe Frank, Bernhard Pfahringer, and Michael J. Cree. Regularisation of Neural\nNetworks by Enforcing Lipschitz Continuity. Machine Learning, 2021.\nFranz Graf, Hans-Peter Kriegel, Matthias Schubert, Sebastian Pölsterl, and Alexander\nCavallaro. 2D Image Registration in CT Images Using Radial Image Descriptors. In\nMedical Image Computing and Computer-Assisted Intervention, 2011.\nFederica Granese, Marco Romanelli, Daniele Gorla, Catuscia Palamidessi, and Pablo\nPiantanida. DOCTOR: A Simple Method for Detecting Misclassification Errors, 2021.\nMatthew J. Groves and Edward O. Pyzer-Knapp. Efficient and Scalable Batch Bayesian\nOptimization Using K-Means. arXiv preprint, 2018.\nIshaan Gulrajani, Faruk Ahmed, Martín Arjovsky, Vincent Dumoulin, and Aaron C. Courville.\nImproved Training of Wasserstein GANs. In Advances in Neural Information Processing\nBibliography\n325\nSystems, 2017.\nEmil Julius Gumbel. Statistical theory of extreme values and some practical applications: a\nseries of lectures, volume 33. US Government Printing Office, 1954.\nChengcheng Guo, B. Zhao, and Yanbing Bai. DeepCore: A Comprehensive Library for\nCoreset Selection in Deep Learning. International Conference on Database and Expert\nSystems Applications, 2022.\nChuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On Calibration of Modern\nNeural Networks. In Proceedings of the International Conference on Machine Learning\n(ICML), 2017.\nYuhong Guo and Dale Schuurmans. Discriminative Batch Mode Active Learning. In Advances\nin Neural Information Processing Systems, 2007.\nJinyong Hahn. On the role of the propensity score in efficient semiparametric estimation of\naverage treatment effects. Econometrica, 1998.\nAhmed Hammam, Frank Bonarens, Seyed Eghbal Ghobadi, and Christoph Stiller. Predictive\nUncertainty Quantification of Deep Neural Networks using Dirichlet Distributions. In\nProceedings of the ACM Computer Science in Cars Symposium, 2022.\nLars Kai Hansen and Peter Salamon. Neural network ensembles. IEEE Transactions on\nPattern Analysis and Machine Intelligence, 1990.\nTrevor Hastie, Jerome H. Friedman, and Robert Tibshirani. The Elements of Statistical\nLearning: Data Mining, Inference, and Prediction. Springer Series in Statistics. Springer,\n2001. ISBN 978-1-4899-0519-2.\nBobby He, Balaji Lakshminarayanan, and Yee Whye Teh. Bayesian Deep Ensembles via the\nNeural Tangent Kernel. In Advances in Neural Information Processing Systems, 2020.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for\nImage Recognition. In IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), 2016.\nTong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Junyuan Xie, and Mu Li. Bag of Tricks\nfor Image Classification with Convolutional Neural Networks, 2018.\nJames J Heckman, Hidehiko Ichimura, and Petra E Todd. Matching as an econometric\nevaluation estimator: Evidence from evaluating a job training programme. The Review of\nEconomic Studies, 1997.\nJames J Heckman, Hidehiko Ichimura, and Petra Todd. Matching as an econometric evaluation\nestimator. The Review of Economic Studies, 1998.\nDan Hendrycks and Thomas G. Dietterich. Benchmarking Neural Network Robustness\nto Common Corruptions and Perturbations. In International Conference on Learning\nRepresentations (ICLR), 2019.\nDan Hendrycks and Kevin Gimpel.\nA Baseline for Detecting Misclassified and Out-of-\nDistribution Examples in Neural Networks. In International Conference on Learning\nRepresentations (ICLR), 2017.\nDan Hendrycks, Mantas Mazeika, and Thomas G. Dietterich. Deep Anomaly Detection with\nOutlier Exposure. In International Conference on Learning Representations (ICLR), 2019.\nDan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural\nAdversarial Examples. In IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), 2021.\nPhilipp Hennig and Christian J. Schuler. Entropy Search for Information-Efficient Global\nOptimization. Journal of Machine Learning Research (JMLR), 2012.\nJames Hensman, Alexander G. de G. Matthews, and Zoubin Ghahramani. Scalable Variational\nGaussian Process Classification. In Proceedings of the International Conference on Artificial\nIntelligence and Statistics (AISTATS), 2015.\nBibliography\n326\nMarek Herde, Zhixin Huang, Denis Huseljic, Daniel Kottke, Stephan Vogt, and Bernhard\nSick. Fast Bayesian Updates for Deep Learning with a Use Case in Active Learning. arXiv\npreprint, 2022.\nDaniel Hernández-Lobato, Jose Hernández-Lobato, and Pierre Dupont. Robust Multi-Class\nGaussian Process Classification. Advances in Neural Information Processing Systems, 2011.\nDaniel Hernández-Lobato, José Miguel Hernández-Lobato, Amar Shah, and Ryan P. Adams.\nPredictive Entropy Search for Multi-objective Bayesian Optimization. In Proceedings of\nthe International Conference on Machine Learning (ICML), 2016.\nJosé Miguel Hernández-Lobato and Ryan P. Adams. Probabilistic Backpropagation for\nScalable Learning of Bayesian Neural Networks.\nIn Proceedings of the International\nConference on Machine Learning (ICML), 2015.\nJosé Miguel Hernández-Lobato, Matthew W. Hoffman, and Zoubin Ghahramani. Predictive\nEntropy Search for Efficient Global Optimization of Black-box Functions. In Advances in\nNeural Information Processing Systems, 2014.\nJosé Miguel Hernández-Lobato, Michael A. Gelbart, Matthew W. Hoffman, Ryan P. Adams,\nand Zoubin Ghahramani. Predictive Entropy Search for Bayesian Optimization with\nUnknown Constraints. In Proceedings of the International Conference on Machine Learning\n(ICML), 2015.\nJennifer L Hill.\nBayesian nonparametric modeling for causal inference.\nJournal of\nComputational and Graphical Statistics, 2011.\nGeoffrey E. Hinton and Drew van Camp. Keeping the Neural Networks Simple by Minimizing\nthe Description Length of the Weights. In Proceedings of the Annual ACM Conference on\nComputational Learning Theory (COLT), 1993.\nGeoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the Knowledge in a Neural\nNetwork. arXiv preprint, 2015.\nHjort, Holmes, Müller, and Walker. Bayesian Nonparametrics. Cambridge University Press,\n2010.\nSepp Hochreiter and Jürgen Schmidhuber. Simplifying Neural Nets by Discovering Flat\nMinima. In Advances in Neural Information Processing Systems, 1994.\nSteven C. H. Hoi, Rong Jin, Jianke Zhu, and Michael R. Lyu. Batch mode active learning\nand its application to medical image classification. In Proceedings of the International\nConference on Machine Learning (ICML), 2006.\nGeoff Holmes, Eibe Frank, Dale Fletcher, and Corey Sterling. Efficiently correcting machine\nlearning: considering the role of example ordering in human-in-the-loop training of image\nclassification models. In International Conference on Intelligent User Interfaces, 2022.\nDavid Holzmüller, Viktor Zaverkin, Johannes Kästner, and Ingo Steinwart. A Framework\nand Benchmark for Deep Batch Active Learning for Regression. arXiv preprint, 2022.\nHoulsby. Efficient Bayesian active learning and matrix modelling. PhD thesis, University of\nCambridge, 2014.\nNeil Houlsby, Ferenc Huszar, Zoubin Ghahramani, and Máté Lengyel. Bayesian Active\nLearning for Classification and Preference Learning. arXiv preprint, 2011.\nYen-Chang Hsu, Yilin Shen, Hongxia Jin, and Zsolt Kira. Generalized ODIN: Detecting Out-\nof-Distribution Image Without Learning From Out-of-Distribution Data. In IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR), 2020.\nGao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger. Densely\nConnected Convolutional Networks. In IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2017.\nHaiwen Huang, Zhihan Li, Lulu Wang, Sishuo Chen, Xinyu Zhou, and Bin Dong. Feature\nSpace Singularity for Out-of-Distribution Detection. In Workshop on Artificial Intelligence\nBibliography\n327\nSafety at AAAI Conference on Artificial Intelligence, 2021.\nJiaji Huang, Rewon Child, Vinay Rao, Hairong Liu, Sanjeev Satheesh, and Adam Coates.\nActive Learning for Speech Recognition: the Power of Gradients. arXiv preprint, 2016.\nYanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Xu Chen,\nHyoukJoong Lee, Jiquan Ngiam, Quoc V. Le, Yonghui Wu, and Zhifeng Chen. GPipe:\nEfficient Training of Giant Neural Networks using Pipeline Parallelism. In Advances in\nNeural Information Processing Systems, 2019.\nYu Huang and Yue Chen. Autonomous Driving with Deep Learning: A Survey of State-of-Art\nTechnologies. arXiv preprint, 2020.\nHuszár. Scoring rules, divergences and information in Bayesian machine learning. PhD\nthesis, University of Cambridge, 2013.\nAlexander Immer. Disentangling the Gauss-Newton Method and Approximate Inference for\nNeural Networks. arXiv preprint, 2020.\nAlexander Immer, Maciej Korzepa, and Matthias Bauer. Improving Predictions of Bayesian\nNeural Nets via Local Linearization.\nIn The International Conference on Artificial\nIntelligence and Statistics, 2021.\nSergey Ioffe and Christian Szegedy. Batch Normalization: Accelerating Deep Network Training\nby Reducing Internal Covariate Shift. In Proceedings of the International Conference on\nMachine Learning (ICML), 2015.\nRishabh K. Iyer, Ninad Khargoankar, Jeff A. Bilmes, and Himanshu Asanani. Submodular\ncombinatorial information measures with applications in machine learning. In Algorithmic\nLearning Theory at Virtual Conference, 2021.\nDavid Janz, Jos van der Westhuizen, and José Miguel Hernández-Lobato. Actively Learning\nwhat makes a Discrete Sequence Valid. arXiv preprint, 2017.\nJeon. ThompsonBALD: a new approach to Bayesian batch active learning for deep learning\nvia Thompson sampling. Master’s thesis, University College London, 2020.\nAndrew Jesson, Sören Mindermann, Uri Shalit, and Yarin Gal. Identifying Causal-Effect\nInference Failure with Uncertainty-Aware Models. In Advances in Neural Information\nProcessing Systems, 2020.\nAndrew Jesson, Sören Mindermann, Yarin Gal, and Uri Shalit. Quantifying Ignorance in\nIndividual-Level Causal-Effect Estimates under Hidden Confounding. In Proceedings of\nthe International Conference on Machine Learning (ICML), 2021.\nAndrew Jesson, Panagiotis Tigas, Joost van Amersfoort, Andreas Kirsch, Uri Shalit, and\nYarin Gal. Causal-BALD: Deep Bayesian Active Learning of Outcomes to Infer Treatment-\nEffects from Observational Data. In Advances in Neural Information Processing Systems,\n2021.\nAngela H. Jiang, Daniel L.-K. Wong, Giulio Zhou, David G. Andersen, Jeffrey Dean,\nGregory R. Ganger, Gauri Joshi, Michael Kaminsky, Michael Kozuch, Zachary C. Lipton,\nand Padmanabhan Pillai. Accelerating Deep Learning by Focusing on the Biggest Losers.\narXiv preprint, 2019.\nYiding Jiang, Vaishnavh Nagarajan, Christina Baek, and J. Zico Kolter.\nAssessing\nGeneralization of SGD via Disagreement. In The Tenth International Conference on\nLearning Representations (ICLR), 2022.\nTyler B. Johnson and Carlos Guestrin.\nTraining Deep Models Faster with Robust,\nApproximate Importance Sampling. In Advances in Neural Information Processing Systems,\n2018.\nDonald R. Jones, Matthias Schonlau, and William J. Welch. Efficient Global Optimization of\nExpensive Black-Box Functions. Journal of Global Optimization, 1998.\nHlynur Jónsson, Giovanni Cherubini, and Evangelos Eleftheriou. Convergence Behavior of\nBibliography\n328\nDNNs with Mutual-Information-Based Regularization. Entropy, 2020.\nKeller Jordan. Calibrated Chaos: Variance Between Runs of Neural Network Training is\nHarmless and Inevitable. arXiv preprint, 2023.\nNorman P. Jouppi, Cliff Young, Nishant Patil, David A. Patterson, Gaurav Agrawal, Raminder\nBajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, Rick Boyle, Pierre-luc Cantin,\nClifford Chao, Chris Clark, Jeremy Coriell, Mike Daley, Matt Dau, Jeffrey Dean, Ben\nGelb, Tara Vazir Ghaemmaghami, Rajendra Gottipati, William Gulland, Robert Hagmann,\nC. Richard Ho, Doug Hogberg, John Hu, Robert Hundt, Dan Hurt, Julian Ibarz, Aaron\nJaffey, Alek Jaworski, Alexander Kaplan, Harshit Khaitan, Daniel Killebrew, Andy Koch,\nNaveen Kumar, Steve Lacy, James Laudon, James Law, Diemthu Le, Chris Leary, Zhuyuan\nLiu, Kyle Lucke, Alan Lundin, Gordon MacKean, Adriana Maggiore, Maire Mahony,\nKieran Miller, Rahul Nagarajan, Ravi Narayanaswami, Ray Ni, Kathy Nix, Thomas Norrie,\nMark Omernick, Narayana Penukonda, Andy Phelps, Jonathan Ross, Matt Ross, Amir\nSalek, Emad Samadiani, Chris Severn, Gregory Sizikov, Matthew Snelham, Jed Souter,\nDan Steinberg, Andy Swing, Mercedes Tan, Gregory Thorson, Bo Tian, Horia Toma, Erick\nTuttle, Vijay Vasudevan, Richard Walter, Walter Wang, Eric Wilcox, and Doe Hyun Yoon.\nIn-Datacenter Performance Analysis of a Tensor Processing Unit. In Proceedings of the\nAnnual International Symposium on Computer Architecture (ISCA), 2017.\nCem Kalkanli and Ayfer Özgür. Batched Thompson Sampling. In Advances in Neural\nInformation Processing Systems, 2021.\nNathan Kallus, Xiaojie Mao, and Angela Zhou. Interval Estimation of Individual-Level\nCausal Effects Under Unobserved Confounding. In International Conference on Artificial\nIntelligence and Statistics (AISTATS), 2019.\nKirthevasan Kandasamy, Akshay Krishnamurthy, Jeff Schneider, and Barnabás Póczos.\nParallelised Bayesian Optimisation via Thompson Sampling. In International Conference\non Artificial Intelligence and Statistics (AISTATS), 2018.\nSiddharth Karamcheti, Ranjay Krishna, Li Fei-Fei, and Christopher D. Manning. Mind\nYour Outliers! Investigating the Negative Impact of Outliers on Active Learning for\nVisual Question Answering. In Proceedings of the Annual Meeting of the Association for\nComputational Linguistics and the International Joint Conference on Natural Language\nProcessing (ACL/IJCNLP), 2021.\nAngelos Katharopoulos and François Fleuret. Biased Importance Sampling for Deep Neural\nNetwork Training. arXiv preprint, 2017.\nAngelos Katharopoulos and François Fleuret. Not All Samples Are Created Equal: Deep\nLearning with Importance Sampling. In Proceedings of the International Conference on\nMachine Learning (ICML), 2018.\nVishal Kaushal, Suraj Kothawade, Ganesh Ramakrishnan, Jeff A. Bilmes, and Rishabh Iyer.\nPRISM: A Unified Framework of Parameterized Submodular Information Measures for\nTargeted Data Subset Selection and Summarization. arXiv preprint, 2021.\nKenji Kawaguchi and Haihao Lu. Ordered SGD: A New Stochastic Optimization Framework\nfor Empirical Risk Minimization.\nIn Proceedings of the International Conference on\nArtificial Intelligence and Statistics (AISTATS), 2020.\nSeho Kee, Enrique del Castillo, and George Runger. Query-by-committee Improvement with\nDiversity and Density in Batch Active Learning. Information Sciences, 2018.\nAlex Kendall and Yarin Gal. What Uncertainties Do We Need in Bayesian Deep Learning\nfor Computer Vision? In Advances in Neural Information Processing Systems, 2017.\nAlex Kendall, Vijay Badrinarayanan, and Roberto Cipolla.\nBayesian SegNet:\nModel\nUncertainty in Deep Convolutional Encoder-Decoder Architectures for Scene Understanding.\nIn British Machine Vision Conference, 2017.\nBibliography\n329\nNitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping\nTak Peter Tang. On Large-Batch Training for Deep Learning: Generalization Gap and\nSharp Minima. In International Conference on Learning Representations (ICLR), 2017.\nKrishnateja Killamsetty, D. Sivasubramanian, Ganesh Ramakrishnan, and Rishabh Iyer.\nGLISTER: Generalization based Data Subset Selection for Efficient and Robust Learning.\nProceedings of the AAAI Conference on Artificial Intelligence (AAAI), 2020.\nKrishnaTeja Killamsetty, Durga Sivasubramanian, Ganesh Ramakrishnan, Abir De, and\nRishabh Iyer. GRAD-MATCH: Gradient Matching based Data Subset Selection for Efficient\nDeep Model Training. arXiv preprint, 2021a.\nKrishnaTeja Killamsetty, Durga Sivasubramanian, Ganesh Ramakrishnan, and Rishabh K.\nIyer. GLISTER: Generalization based Data Subset Selection for Efficient and Robust\nLearning. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), 2021b.\nDiederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. In\nInternational Conference on Learning Representations (ICLR), 2015.\nDiederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes. In International\nConference on Learning Representations (ICLR), 2014.\nDiederik P. Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling. Semi-\nsupervised Learning with Deep Generative Models. In Advances in Neural Information\nProcessing Systems, 2014.\nAndreas Kirsch. PowerEvaluationBALD: Efficient Evaluation-Oriented Deep (Bayesian)\nActive Learning with Stochastic Acquisition Functions. arXiv preprint, 2021.\nAndreas Kirsch.\nPaper Review:\nBayesian Model Selection,\nthe Marginal Like-\nlihood,\nand\nGeneralization,\n2022.\nURL\nhttps://blog.blackhc.net/2022/06/\nbayesian-model-selection-marginal-likehood-generalization/.\nAndreas Kirsch. Black-Box Batch Active Learning for Regression. Transactions on Machine\nLearning Research, 2023a.\nAndreas Kirsch. Does “Deep Learning on a Data Diet” reproduce? Overall yes, but GraNd\nat Initialization does not. Transactions on Machine Learning Research, 2023b.\nAndreas Kirsch and Yarin Gal. A Practical & Unified Notation for Information-Theoretic\nQuantities in ML. arXiv preprint, 2021.\nAndreas Kirsch and Yarin Gal. A Note on “Assessing Generalization of SGD via Disagreement”.\nTransactions on Machine Learning Research, 2022a.\nAndreas Kirsch and Yarin Gal. Unifying Approaches in Active Learning and Active Sampling\nvia Fisher Information and Information-Theoretic Quantities. Transactions on Machine\nLearning Research, 2022b.\nAndreas Kirsch, Clare Lyle, and Yarin Gal. Unpacking Information Bottlenecks: Unifying\nInformation-Theoretic Objectives in Deep Learning. arXiv preprint, 2020.\nAndreas Kirsch, Tom Rainforth, and Yarin Gal. Active Learning under Pool Set Distribution\nShift and Noisy Data. arXiv preprint, 2021a.\nAndreas Kirsch, Tom Rainforth, and Yarin Gal. Test Distribution-Aware Active Learning: A\nPrincipled Approach Against Distribution Shift and Outliers, 2021b.\nAndreas Kirsch, Jannik Kossen, and Yarin Gal. Marginal and Joint Cross-Entropies &\nPredictives for Online Bayesian Inference, Active Learning, and Active Sampling. arXiv\npreprint, 2022.\nAndreas Kirsch, Joost van Amersfoort, and Yarin Gal. BatchBALD: Efficient and Diverse\nBatch Acquisition for Deep Bayesian Active Learning. In Advances in Neural Information\nProcessing Systems, 2019.\nAndreas Kirsch, Sebastian Farquhar, Parmida Atighehchian, Andrew Jesson, Frederic\nBranchaud-Charron, and Yarin Gal. Stochastic Batch Acquisition for Deep Active Learning.\nBibliography\n330\nTransactions on Machine Learning Research, 2023.\nKomaki. On asymptotic properties of predictive distributions. Biometrika, 1996.\nAran Komatsuzaki. One Epoch Is All You Need. arXiv preprint, 2019.\nWouter Kool, Herke van Hoof, and Max Welling. Stochastic Beams and Where To Find Them:\nThe Gumbel-Top-k Trick for Sampling Sequences Without Replacement. In Proceedings of\nthe International Conference on Machine Learning (ICML), 2019.\nSuraj Kothawade, Nathan Beck, KrishnaTeja Killamsetty, and Rishabh K. Iyer. SIMILAR:\nSubmodular Information Measures Based Active Learning In Realistic Scenarios.\nIn\nAdvances in Neural Information Processing Systems, 2021.\nSuraj Kothawade, Vishal Kaushal, Ganesh Ramakrishnan, Jeff A. Bilmes, and Rishabh K.\nIyer. PRISM: A Rich Class of Parameterized Submodular Information Measures for Guided\nData Subset Selection. In Proceedings of the AAAI Conference on Artificial Intelligence\n(AAAI), 2022.\nAndreas Krause, Ajit Singh, and Carlos Guestrin. Near-optimal sensor placements in Gaussian\nprocesses: Theory, efficient algorithms and empirical studies. Journal of Machine Learning\nResearch (JMLR), 2008.\nAgustinus Kristiadi, Matthias Hein, and Philipp Hennig. Being Bayesian, Even Just a Bit,\nFixes Overconfidence in ReLU Networks. In Proceedings of the International Conference\non Machine Learning (ICML), 2020.\nAlex Krizhevsky. Learning multiple layers of features from tiny images, 2009.\nAnders Krogh and Jesper Vedelsby. Neural Network Ensembles, Cross Validation, and Active\nLearning. In Advances in Neural Information Processing Systems, 1994.\nAlex Kulesza and Ben Taskar. k-DPPs: Fixed-Size Determinantal Point Processes. In\nProceedings of the International Conference on Machine Learning (ICML), 2011.\nFrederik Kunstner, Philipp Hennig, and Lukas Balles. Limitations of the empirical Fisher\napproximation for natural gradient descent. In Advances in Neural Information Processing\nSystems, 2019.\nAlexandre Lacoste, Pau Rodríguez López, Frederic Branchaud-Charron, Parmida Atighe-\nhchian, Massimo Caccia, Issam Hadj Laradji, Alexandre Drouin, Matt Craddock, Laurent\nCharlin, and David Vázquez. Synbols: Probing Learning Algorithms with Synthetic\nDatasets. In Advances in Neural Information Processing Systems, 2020.\nBalaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and Scalable\nPredictive Uncertainty Estimation Using Deep Ensembles.\nIn Advances in Neural\nInformation Processing Systems, 2017.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu\nSoricut. ALBERT: A Lite BERT for Self-supervised Learning of Language Representations.\nIn International Conference on Learning Representations (ICLR), 2020.\nLeon Lang, Pierre Baudot, Rick Quax, and Patrick Forré.\nInformation Decomposition\nDiagrams Applied beyond Shannon Entropy: A Generalization of Hu’s Theorem. arXiv\npreprint, 2022.\nStefan Larson, Anish Mahendran, Joseph J. Peper, Christopher Clarke, Andrew Lee, Parker\nHill, Jonathan K. Kummerfeld, Kevin Leach, Michael A. Laurenzano, Lingjia Tang, and\nJason Mars. An Evaluation Dataset for Intent Classification and Out-of-Scope Prediction.\nIn Proceedings of the Conference on Empirical Methods in Natural Language Processing and\nthe International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),\n2019.\nMiguel Lázaro-Gredilla and Aníbal R Figueiras-Vidal. Marginalized Neural Network Mixtures\nfor Large-Scale Regression. IEEE Transactions on Neural Networks, 2010.\nYann\nLeCun.\nThe\nMNIST\nDatabase\nof\nHandwritten\nDigits.\nBibliography\n331\nhttp://yann.lecun.com/exdb/mnist/, 1998.\nYann LeCun, Bernhard E. Boser, John S. Denker, Donnie Henderson, Richard E. Howard,\nWayne E. Hubbard, and Lawrence D. Jackel. Backpropagation Applied to Handwritten\nZip Code Recognition. Neural Computation, 1989.\nYann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning\napplied to document recognition. Proceedings of IEEE, 1998.\nYann LeCun, Léon Bottou, Genevieve B. Orr, and Klaus-Robert Müller. Efficient BackProp.\nIn Neural Networks: Tricks of the Trade - Second Edition, 2012.\nDong-Hyun Lee et al. Pseudo-label: The simple and efficient semi-supervised learning method\nfor deep neural networks. In \"Challenges in Representation Learning\" Workshop at ICML,\n2013.\nKimin Lee, Honglak Lee, Kibok Lee, and Jinwoo Shin. Training Confidence-calibrated\nClassifiers for Detecting Out-of-Distribution Samples. In International Conference on\nLearning Representations (ICLR), 2018a.\nKimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A Simple Unified Framework for\nDetecting Out-of-Distribution Samples and Adversarial Attacks. In Advances in Neural\nInformation Processing Systems, 2018b.\nKuang-Huei Lee, Ian Fischer, Anthony Liu, Yijie Guo, Honglak Lee, John Canny, and Sergio\nGuadarrama. Predictive Information Accelerates Learning in RL. In Advances in Neural\nInformation Processing Systems, 2020.\nSun-Kyung Lee and Jong-Hwan Kim. BALD-VAE: Generative Active Learning based on the\nUncertainties of Both Labeled and Unlabeled Data. In International Conference on Robot\nIntelligence Technology and Applications (RiTA), 2019.\nDavid D. Lewis and William A. Gale. A Sequential Algorithm for Training Text Classifiers.\nIn Proceedings of the Annual International ACM-SIGIR Conference on Research and\nDevelopment in Information Retrieval, 1994.\nDa Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Deeper, broader and artier\ndomain generalization. In IEEE International Conference on Computer Vision (ICCV),\n2017.\nMingkun Li and Ishwar K. Sethi. Confidence-Based Active Learning. IEEE Transactions on\nPattern Analysis and Machine Intelligence, 2006.\nShiyu Liang, Yixuan Li, and R. Srikant. Enhancing The Reliability of Out-of-distribution Im-\nage Detection in Neural Networks. In International Conference on Learning Representations\n(ICLR), 2018.\nDennis V Lindley. On a measure of the information provided by an experiment. The Annals\nof Mathematical Statistics, 1956.\nJeremiah Z. Liu, Zi Lin, Shreyas Padhy, Dustin Tran, Tania Bedrax-Weiss, and Balaji\nLakshminarayanan. Simple and Principled Uncertainty Estimation with Deterministic\nDeep Learning via Distance Awareness. In Advances in Neural Information Processing\nSystems, 2020a.\nWeitang Liu, Xiaoyun Wang, John D. Owens, and Yixuan Li.\nEnergy-based Out-of-\ndistribution Detection. In Advances in Neural Information Processing Systems, 2020b.\nZhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining\nXie. A ConvNet for the 2020s. IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), Jun 2022.\nFernando Llorente, Luca Martino, David Delgado, and Javier López-Santiago. Marginal\nLikelihood Computation for Model Selection and Hypothesis Testing: An Extensive Review.\nSIAM Review, 2023.\nJonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for\nBibliography\n332\nsemantic segmentation. In IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), 2015.\nQuan Long. Multimodal information gain in Bayesian design of experiments. Computational\nStatistics, 2022.\nIlya Loshchilov and Frank Hutter. Online Batch Selection for Faster Training of Neural\nNetworks. arXiv preprint, 2015.\nIlya Loshchilov and Frank Hutter. Decoupled Weight Decay Regularization. In International\nConference on Learning Representations (ICLR), 2019.\nSanae Lotfi, Pavel Izmailov, Gregory W. Benton, Micah Goldblum, and Andrew Gordon\nWilson.\nBayesian Model Selection, the Marginal Likelihood, and Generalization.\nIn\nProceedings of the International Conference on Machine Learning (ICML), 2022.\nZhiming Luo, Frederic Branchaud-Charron, Carl Lemaire, Janusz Konrad, Shaozi Li, Akshaya\nMishra, Andrew Achkar, Justin A. Eichel, and Pierre-Marc Jodoin. MIO-TCD: A New\nBenchmark Dataset for Vehicle Classification and Localization. IEEE Transactions on\nImage Processing, 2018.\nClare Lyle, Lisa Schut, Robin Ru, Yarin Gal, and Mark van der Wilk. A Bayesian Perspective\non Training Speed and Model Selection. In Advances in Neural Information Processing\nSystems, 2020.\nDavid J. C. MacKay. The Evidence Framework Applied to Classification Networks. Neural\nComputation, 1992a.\nDavid J. C. MacKay. Information-Based Objective Functions for Active Data Selection.\nNeural Computation, 1992b.\nDavid JC MacKay. Bayesian methods for adaptive models. PhD thesis, California Institute\nof Technology, 1992c.\nDavid JC MacKay. Information theory, inference and learning algorithms.\nCambridge\nUniversity Press, 2003.\nChris J. Maddison, Daniel Tarlow, and Tom Minka. A* Sampling. In Advances in Neural\nInformation Processing Systems, 2014.\nWesley J. Maddox, Pavel Izmailov, Timur Garipov, Dmitry P. Vetrov, and Andrew Gordon\nWilson. A Simple Baseline for Bayesian Uncertainty in Deep Learning. In Advances in\nNeural Information Processing Systems, 2019.\nWesley J Maddox, Samuel Stanton, and Andrew G Wilson. Conditioning sparse variational\ngaussian processes for online decision-making. Advances in Neural Information Processing\nSystems, 2021.\nDhruv Mahajan, Ross B. Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri,\nYixuan Li, Ashwin Bharambe, and Laurens van der Maaten. Exploring the Limits of\nWeakly Supervised Pretraining. In Computer Vision - ECCV - European Conference,\nProceedings, Part II, 2018.\nAndrey Malinin and Mark Gales. Reverse kl-divergence training of prior networks: Improved\nuncertainty and adversarial robustness. Advances in Neural Information Processing Systems,\n2019.\nAndrey Malinin and Mark J. F. Gales. Predictive Uncertainty Estimation via Prior Networks.\nIn Advances in Neural Information Processing Systems, 2018.\nAndrey Malinin, Bruno Mlodozeniec, and Mark Gales. Ensemble distribution distillation.\narXiv preprint, 2019.\nSam McCandlish, Jared Kaplan, Dario Amodei, and OpenAI Dota Team. An Empirical\nModel of Large-Batch Training. arXiv preprint, 2018.\nWilliam J. McGill.\nMultivariate information transmission.\nTransactions of the IRE\nProfessional Group on Information Theory, 1954.\nBibliography\n333\nSören Mindermann, Jan Markus Brauner, Muhammed Razzak, Mrinank Sharma, Andreas\nKirsch, Winnie Xu, Benedikt Höltgen, Aidan N. Gomez, Adrien Morisot, Sebastian\nFarquhar, and Yarin Gal.\nPrioritized Training on Points that are Learnable, Worth\nLearning, and not yet Learnt. In Proceedings of the International Conference on Machine\nLearning (ICML), 2022.\nThomas P. Minka. Bayesian model averaging is not model combination. 2002.\nBaharan Mirzasoleiman, Jeff A. Bilmes, and Jure Leskovec. Coresets for Data-efficient\nTraining of Machine Learning Models. In Proceedings of the International Conference on\nMachine Learning (ICML), 2020.\nTakeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida.\nSpectral\nNormalization for Generative Adversarial Networks.\nIn International Conference on\nLearning Representations (ICLR), 2018.\nJonas Mockus. On Bayesian Methods for Seeking the Extremum. 1974.\nJishnu Mukhoti and Yarin Gal. Evaluating Bayesian Deep Learning Methods for Semantic\nSegmentation. arXiv preprint, 2018.\nJishnu Mukhoti, Andreas Kirsch, Joost van Amersfoort, Philip H. S. Torr, and Yarin Gal.\nDeterministic Neural Networks with Appropriate Inductive Biases Capture Epistemic\nand Aleatoric Uncertainty. IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), 2023.\nPrateek Munjal, Nasir Hayat, Munawar Hayat, Jamshid Sourati, and Shadab Khan. Towards\nRobust and Reproducible Active Learning using Neural Networks.\nIn IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR), 2022.\nKevin P. Murphy. Machine learning - a probabilistic perspective. Adaptive computation and\nmachine learning series. MIT Press, 2012. ISBN 0262018020.\nChelsea Murray, James Urquhart Allingham, Javier Antorán, and José Miguel Hernández-\nLobato. Depth Uncertainty Networks for Active Learning. arXiv preprint, 2021.\nPreetum Nakkiran and Yamini Bansal.\nDistributional Generalization: A New Kind of\nGeneralization. arXiv preprint, 2020.\nEric T. Nalisnick, Akihiro Matsukawa, Yee Whye Teh, Dilan Görür, and Balaji Lakshmi-\nnarayanan. Do Deep Generative Models Know What They Don’t Know? In International\nConference on Learning Representations (ICLR), 2019.\nRadford M. Neal. Bayesian learning for neural networks. PhD thesis, University of Toronto,\nCanada, 1995.\nWillie Neiswanger, Ke Alexander Wang, and Stefano Ermon. Bayesian algorithm execution:\nEstimating computable properties of black-box functions using mutual information. In\nInternational Conference on Machine Learning, 2021.\nGeorge L. Nemhauser, Laurence A. Wolsey, and Marshall L. Fisher.\nAn analysis of\napproximations for maximizing submodular set functions - I. Mathematical Programming,\n1978.\nMehdi Neshat, Bradley Alexander, Markus Wagner, and Yuanzhong Xia.\nA Detailed\nComparison of Meta-Heuristic Methods for Optimising Wave Energy Converter Placements.\nIn Proceedings of the Genetic and Evolutionary Computation Conference, 2018.\nYuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y. Ng.\nReading Digits in Natural Images with Unsupervised Feature Learning. In \"Deep Learning\nand Unsupervised Feature Learning\" Workshop at NIPS, 2011.\nJersey Neyman. Sur les Applications de la Théorie des Probabilités aux Experiences Agricoles:\nEssai des Principes. Roczniki Nauk Rolniczych, 1923.\nHieu T. Nguyen, Joseph Yadegar, Bailey Kong, and Hai Wei. Efficient Batch-Mode Active\nLearning of Random Forest. In IEEE Statistical Signal Processing Workshop, 2012.\nBibliography\n334\nViet Cuong Nguyen, Wee Sun Lee, Nan Ye, Kian Ming Adam Chai, and Hai Leong Chieu.\nActive Learning for Probabilistic Hypotheses Using the Maximum Gibbs Error Criterion.\nIn Advances in Neural Information Processing Systems, 2013.\nVu Nguyen, Santu Rana, Sunil Gupta, Cheng Li, and Svetha Venkatesh. Budgeted Batch\nBayesian Optimization With Unknown Batch Sizes. arXiv preprint, 2017.\nJeremy Nixon, Michael W. Dusenberry, Linchuan Zhang, Ghassen Jerfel, and Dustin Tran.\nMeasuring Calibration in Deep Learning. In IEEE Conference on Computer Vision and\nPattern Recognition (CVPR) Workshops, 2019.\nSafiya Umoja Noble. Algorithms of Oppression: How Search Engines Reinforce Racism. New\nYork University Press, 2018.\nChangYong Oh, Roberto Bondesan, Efstratios Gavves, and Max Welling. Batch Bayesian\nOptimization on Permutations using the Acquisition Weighted Kernel. NeurIPS, 2021.\nKazuki Osawa, Siddharth Swaroop, Mohammad Emtiyaz Khan, Anirudh Jain, Runa\nEschenhagen, Richard E. Turner, and Rio Yokota. Practical Deep Learning with Bayesian\nPrinciples. In Advances in Neural Information Processing Systems, 2019.\nIan Osband, John Aslanides, and Albin Cassirer. Randomized Prior Functions for Deep\nReinforcement Learning. In Advances in Neural Information Processing Systems, 2018.\nIan Osband, Zheng Wen, Mohammad Asghari, Morteza Ibrahimi, Xiyuan Lu, and Ben-\njamin Van Roy. Epistemic Neural Networks, 2021a.\nIan Osband, Zheng Wen, Seyed Mohammad Asghari, Vikranth Dwaracherla, Botao Hao,\nMorteza Ibrahimi, Dieterich Lawson, Xiuyuan Lu, Brendan O’Donoghue, and Benjamin Van\nRoy. The Neural Testbed: Evaluating Predictive Distributions, 2021b.\nIan Osband, Seyed Mohammad Asghari, Benjamin Van Roy, Nat McAleese, John Aslanides,\nand Geoffrey Irving. Fine-Tuning Language Models via Epistemic Neural Networks. arXiv\npreprint, 2022a.\nIan Osband, Zheng Wen, Seyed Mohammad Asghari, Vikranth Dwaracherla, Xiuyuan Lu,\nand Benjamin Van Roy. Evaluating high-order predictive distributions in deep learning. In\nProceedings of the Conference on Uncertainty in Artificial Intelligence, 2022b.\nRafail Ostrovsky, Yuval Rabani, Leonard J. Schulman, and Chaitanya Swamy.\nThe\neffectiveness of lloyd-type methods for the k-means problem. Journal of ACM, 2012.\nYaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, D. Sculley, Sebastian Nowozin,\nJoshua Dillon, Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model's\nuncertainty? Evaluating predictive uncertainty under dataset shift. In Advances in Neural\nInformation Processing Systems, 2019.\nDongmin Park, Dimitris Papailiopoulos, and Kangwook Lee. Active Learning is a Strong\nBaseline for Data Subset Selection. In \"Has it Trained Yet?\" at NeurIPS Workshop, 2022.\nAdam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,\nZeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic Differentiation\nin PyTorch. In \"Autodiff\" Workshop at NIPS, 2017.\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,\nTrevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas\nKöpf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,\nBenoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An Imperative Style,\nHigh-Performance Deep Learning Library. In Advances in Neural Information Processing\nSystems, 2019.\nMansheej Paul, Surya Ganguli, and Gintare Karolina Dziugaite. Deep Learning on a Data\nDiet: Finding Important Examples Early in Training. In Advances in Neural Information\nProcessing Systems, 2021.\nTim Pearce, Alexandra Brintrup, and Jun Zhu. Understanding Softmax Confidence and\nBibliography\n335\nUncertainty. arXiv preprint, 2021.\nF. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel,\nP. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau,\nM. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine Learning in Python.\nJournal of Machine Learning Research (JMLR), 2011a.\nFabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion,\nOlivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake\nVanderPlas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot,\nand Edouard Duchesnay. Scikit-learn: Machine Learning in Python. Journal of Machine\nLearning Research (JMLR), 2011b.\nCaroline Criado Perez. Invisible Women: Exposing Data Bias in a World Designed for Men.\nRandom House, 2019.\nMaya L Petersen, Kristin E Porter, Susan Gruber, Yue Wang, and Mark J Van Der Laan.\nDiagnosing and Responding to Violations in the Positivity Assumption. Statistical Methods\nin Medical Research, 2012.\nRobert Pinsler, Jonathan Gordon, Eric T. Nalisnick, and José Miguel Hernández-Lobato.\nBayesian Batch Active Learning as Sparse Subset Approximation. In Advances in Neural\nInformation Processing Systems, 2019.\nGeoff Pleiss, Tianyi Zhang, Ethan R. Elenberg, and Kilian Q. Weinberger. Identifying\nMislabeled Data using the Area Under the Margin Ranking. In Advances in Neural\nInformation Processing Systems, 2020.\nJanis Postels, Hermann Blum, Cesar Cadena, Roland Siegwart, Luc Van Gool, and Federico\nTombari. Quantifying Aleatoric and Epistemic Uncertainty Using Density Estimation in\nLatent Space. arXiv preprint, 2020.\nL Prokhorenkova, G Gusev, A Vorobev, AV Dorogush, and A Gulin. CatBoost: Unbiased\nBoosting with Categorical Features. arXiv preprint, 2017.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya\nSutskever. Learning Transferable Visual Models From Natural Language Supervision. In\nProceedings of the International Conference on Machine Learning (ICML), 2021.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J. Liu. Exploring the Limits of Transfer Learning with a\nUnified Text-to-Text Transformer. Journal of Machine Learning Research (JMLR), 2020.\nTom Rainforth, Robert Cornish, Hongseok Yang, and Andrew Warrington. On Nesting Monte\nCarlo Estimators. In Proceedings of the International Conference on Machine Learning\n(ICML), 2018.\nTom Rainforth, Adam Foster, Desi R. Ivanova, and Freddie Bickford Smith. Modern Bayesian\nExperimental Design. arXiv preprint, 2023.\nJeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. DeepSpeed: System\nOptimizations Enable Training Deep Learning Models with Over 100 Billion Parameters.\nIn Proceedings of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining\nat KDD, 2020.\nAntti Rasmus, Mathias Berglund, Mikko Honkala, Harri Valpola, and Tapani Raiko. Semi-\nsupervised Learning with Ladder Networks. In Advances in Neural Information Processing\nSystems, 2015.\nMengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urtasun. Learning to Reweight Examples\nfor Robust Deep Learning. In Proceedings of the International Conference on Machine\nLearning (ICML), 2018.\nPengzhen Ren, Yun Xiao, Xiaojun Chang, Po-Yao Huang, Zhihui Li, Brij B. Gupta, Xiaojiang\nBibliography\n336\nChen, and Xin Wang. A Survey of Deep Active Learning. ACM Computing Surveys, 2022.\nJames M. Robins, Miguel Angel Hernán, and Babette Brumback. Marginal Structural Models\nand Causal Inference in Epidemiology. Epidemiology, 2000.\nDavid Rolnick, Andreas Veit, Serge J. Belongie, and Nir Shavit. Deep Learning is Robust to\nMassive Label Noise. arXiv preprint, 2017.\nNicholas Roy and Andrew McCallum. Toward Optimal Active Learning through Sampling\nEstimation of Error Reduction. In Proceedings of the International Conference on Machine\nLearning (ICML), 2001.\nDonald B Rubin. Estimating causal effects of treatments in randomized and nonrandomized\nstudies. Journal of Educational Psychology, 1974.\nDonald B Rubin. Randomization analysis of experimental data: The Fisher randomization\ntest comment. Journal of the American Statistical Association, 1980.\nTim G. J. Rudner, Zonghao Chen, Yee Whye Teh, and Yarin Gal. Tractabe Function-Space\nVariational Inference in Bayesian Neural Networks. 2022.\nDaniel Russo and Benjamin Van Roy.\nLearning to Optimize via Posterior Sampling.\nMathematics of Operations Research, 2013.\nMax Ryabinin, Andrey Malinin, and Mark Gales. Scaling ensemble distribution distillation\nto many classes with proxy targets. Advances in Neural Information Processing Systems,\n2021.\nEman Saleh, Ahmad Tarawneh, M.Z. Naser, M. Abedi, and Ghassan Almasabha. You\nonly design once (YODO): Gaussian Process-Batch Bayesian optimization framework for\nmixture design of ultra high performance concrete. Construction and Building Materials,\n2022.\nFotis Savva, Christos Anagnostopoulos, and Peter Triantafillou. Explaining Aggregates for\nExploratory Analytics. In IEEE International Conference on Big Data, 2018.\nTom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized Experience\nReplay. In International Conference on Learning Representations (ICLR), 2016.\nShira Schneider. Algorithmic Bias: A New Age of Racism. PhD thesis, 2021.\nGreg Schohn and David Cohn. Less is More: Active Learning with Support Vector Machines.\nIn Proceedings of the International Conference on Machine Learning (ICML), 2000.\nJasjeet S Sekhon. The Neyman-Rubin model of causal inference and estimation via matching\nmethods. The Oxford Handbook of Political Methodology, 2008.\nOzan Sener and Silvio Savarese. Active Learning for Convolutional Neural Networks: A\nCore-Set Approach. In International Conference on Learning Representations (ICLR),\n2018.\nSambu Seo, Marko Wallat, Thore Graepel, and Klaus Obermayer.\nGaussian Process\nRegression: Active Data Selection and Test Point Rejection. In Proceedings of the IEEE-\nINNS-ENNS International Joint Conference on Neural Networks (IJCNN), 2000.\nBurr Settles. Active Learning Literature Survey. Machine Learning, 2010.\nBurr Settles and Mark Craven. An Analysis of Active Learning Strategies for Sequence\nLabeling Tasks. In Conference on Empirical Methods in Natural Language Processing,\n2008.\nBurr Settles, Mark Craven, and Soumya Ray. Multiple-Instance Active Learning. In Advances\nin Neural Information Processing Systems, 2007.\nH. Sebastian Seung, Manfred Opper, and Haim Sompolinsky. Query by Committee. In\nProceedings of the Annual Conference on Computational Learning Theory (COLT), 1992.\nAmar Shah and Zoubin Ghahramani. Parallel Predictive Entropy Search for Batch Global\nOptimization of Expensive Objective Functions.\nIn Advances in Neural Information\nProcessing Systems, 2015.\nBibliography\n337\nBobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P. Adams, and Nando de Freitas. Taking\nthe Human Out of the Loop: A Review of Bayesian Optimization. Proceedings of IEEE,\n2016.\nMohammad Hossein Shaker and Eyke Hüllermeier. Aleatoric and Epistemic Uncertainty with\nRandom Forests. In Advances in Intelligent Data Analysis - Proceedings of the International\nSymposium on Intelligent Data Analysis, 2020.\nUri Shalit, Fredrik D. Johansson, and David A. Sontag. Estimating individual treatment effect:\ngeneralization bounds and algorithms. In Proceedings of the International Conference on\nMachine Learning (ICML), 2017.\nClaude E. Shannon. A mathematical theory of communication. Bell System Technical Journal,\n1948.\nPaul Shannon, Andrew Markiel, Owen Ozier, Nitin S Baliga, Jonathan T Wang, Daniel\nRamage, Nada Amin, Benno Schwikowski, and Trey Ideker.\nCytoscape: a software\nenvironment for integrated models of biomolecular interaction networks. Genome Research,\n2003.\nMrinank Sharma, Sebastian Farquhar, Eric Nalisnick, and Tom Rainforth. Do Bayesian\nNeural Networks Need To Be Fully Stochastic? arXiv preprint, 2022.\nJohn Shawe-Taylor, Nello Cristianini, et al. Kernel Methods for Pattern Analysis. Cambridge\nUniversity Press, 2004.\nYanyao Shen, Hyokun Yun, Zachary C. Lipton, Yakov Kronrod, and Animashree Anandkumar.\nDeep Active Learning for Named Entity Recognition. In International Conference on\nLearning Representations (ICLR), 2018.\nClaudia Shi, David M. Blei, and Victor Veitch. Adapting Neural Networks for the Estimation\nof Treatment Effects. In Advances in Neural Information Processing Systems, 2019.\nMohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and\nBryan Catanzaro. Megatron-LM: Training Multi-Billion Parameter Language Models Using\nModel Parallelism. arXiv preprint, 2019.\nRavid Shwartz-Ziv and Naftali Tishby. Opening the Black Box of Deep Neural Networks via\nInformation. arXiv preprint, 2017.\nAditya Siddhant and Zachary C. Lipton. Deep Bayesian Active Learning for Natural Language\nProcessing: Results of a Large-Scale Empirical Study. In Conference on Empirical Methods\nin Natural Language Processing, 2018.\nShoaib Ahmed Siddiqui, Nitarshan Rajkumar, Tegan Maharaj, David Krueger, and Sara\nHooker. Metadata Archaeology: Unearthing Data Subsets by Leveraging Training Dynamics.\nIn International Conference on Learning Representations (ICLR), 2023.\nEero Siivola, Akash Kumar Dhaka, Michael Riis Andersen, Javier González, Pablo Garcia\nMoreno, and Aki Vehtari. Preferential Batch Bayesian Optimization. In IEEE International\nWorkshop on Machine Learning for Signal Processing (MLSP), 2021.\nKaren Simonyan and Andrew Zisserman. Very Deep Convolutional Networks for Large-Scale\nImage Recognition. In International Conference on Learning Representations (ICLR),\n2015.\nSamarth Sinha, Sayna Ebrahimi, and Trevor Darrell. Variational Adversarial Active Learning.\nIn IEEE/CVF International Conference on Computer Vision (ICCV), 2019.\nDominik Ślęzak, Marek Grzegorowski, Andrzej Janusz, Michał Kozielski, Sinh Hoa Nguyen,\nMarek Sikora, Sebastian Stawicki, and Łukasz Wróbel. A Framework for Learning and\nEmbedding Multi-Sensor Forecasting Models into a Decision Support System: A Case\nStudy of Methane Concentration in Coal Mines. Information Sciences, 2018.\nLewis Smith and Yarin Gal. Understanding Measures of Uncertainty for Adversarial Example\nDetection. In Proceedings of the Conference on Uncertainty in Artificial Intelligence, 2018.\nBibliography\n338\nLewis Smith, Joost van Amersfoort, Haiwen Huang, Stephen J. Roberts, and Yarin Gal.\nCan convolutional ResNets approximately preserve input distances? A frequency analysis\nperspective. arXiv preprint, 2021.\nSamuel L. Smith and Quoc V. Le. A Bayesian Perspective on Generalization and Stochastic\nGradient Descent. In International Conference on Learning Representations (ICLR), 2018.\nEdward Snelson and Zoubin Ghahramani. Sparse Gaussian Processes using Pseudo-inputs.\nIn Advances in Neural Information Processing Systems, 2005.\nJasper Snoek, Hugo Larochelle, and Ryan P. Adams. Practical Bayesian Optimization of\nMachine Learning Algorithms. In Advances in Neural Information Processing Systems,\n2012.\nNiranjan Srinivas, Andreas Krause, Sham M. Kakade, and Matthias W. Seeger. Gaussian\nProcess Optimization in the Bandit Setting: No Regret and Experimental Design. 2010.\nCathie Sudlow, John Gallacher, Naomi Allen, Valerie Beral, Paul Burton, John Danesh, Paul\nDowney, Paul Elliott, Jane Green, Martin Landray, et al. UK Biobank: An Open Access\nResource for Identifying the Causes of a Wide Range of Complex Diseases of Middle and\nOld Age. Plos Medicine, 2015.\nChen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting Unreasonable\nEffectiveness of Data in Deep Learning Era. In IEEE International Conference on Computer\nVision (ICCV), 2017.\nShengyang Sun, Guodong Zhang, Chaoqi Wang, Wenyuan Zeng, Jiaman Li, and Roger B.\nGrosse. Differentiable Compositional Kernel Learning for Gaussian Processes. In Proceedings\nof the International Conference on Machine Learning (ICML), 2018.\nShengyang Sun, Daniele Calandriello, Huiyi Hu, Ang Li, and Michalis K. Titsias. Information-\ntheoretic Online Memory Selection for Continual Learning. In The Tenth International\nConference on Learning Representations (ICLR), 2022.\nIiris Sundin, Tomi Peltola, Luana Micallef, Homayun Afrabandpey, Marta Soare, Muntasir Ma-\nmun Majumder, Pedram Daee, Chen He, Baris Serim, Aki S. Havulinna, Caroline Heckman,\nGiulio Jacucci, Pekka Marttinen, and Samuel Kaski. Improving genomics-based predictions\nfor precision medicine through active elicitation of expert knowledge. Bioinformatics, 2018.\nIiris Sundin, Peter Schulam, Eero Siivola, Aki Vehtari, Suchi Saria, and Samuel Kaski. Active\nLearning for Decision-Making from Imbalanced Observational Data. In Proceedings of the\nInternational Conference on Machine Learning (ICML), 2019.\nWei Tan, Lan Du, and Wray L. Buntine. Diversity Enhanced Active Learning with Strictly\nProper Scoring Rules. In Advances in Neural Information Processing Systems, 2021.\nWilliam R Thompson. On the likelihood that one unknown probability exceeds another in\nview of the evidence of two samples. Biometrika, 1933.\nLu Tian, Ash A. Alizadeh, Andrew J. Gentles, and Robert Tibshirani. A Simple Method for\nEstimating Interactions between a Treatment and a Large Number of Covariates. Journal\nof the American Statistical Association, 2014.\nYonglong Tian, Olivier J. Hénaff, and Aäron van den Oord. Divide and Contrast: Self-\nsupervised Learning from Uncurated Data. In IEEE/CVF International Conference on\nComputer Vision (ICCV), 2021.\nPanagiotis Tigas, Yashas Annadani, Andrew Jesson, Bernhard Schölkopf, Yarin Gal, and\nStefan Bauer. Interventions, Where and How? Experimental Design for Causal Models at\nScale. arXiv preprint, 2022.\nMariya Toneva, Alessandro Sordoni, Remi Tachet des Combes, Adam Trischler, Yoshua\nBengio, and Geoffrey J. Gordon. An Empirical Study of Example Forgetting during\nDeep Neural Network Learning. In International Conference on Learning Representations\n(ICLR), 2019.\nBibliography\n339\nSimon Tong. Active learning: theory and applications. PhD thesis, Stanford University, 2001.\nHugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and\nHervé Jégou. Training data-efficient image transformers & distillation through attention,\n2020.\nDustin Tran, Jeremiah Z. Liu, Michael W. Dusenberry, Du Phan, Mark Collier, Jie Ren,\nKehang Han, Zi Wang, Zelda Mariet, Huiyi Hu, Neil Band, Tim G. J. Rudner, Karan\nSinghal, Zachary Nado, Joost van Amersfoort, Andreas Kirsch, Rodolphe Jenatton, Nithum\nThain, Honglin Yuan, Kelly Buchanan, Kevin Murphy, D. Sculley, Yarin Gal, Zoubin\nGhahramani, Jasper Snoek, and Balaji Lakshminarayanan. Plex: Towards Reliability using\nPretrained Large Model Extensions. arXiv preprint, 2022.\nToan Tran, Thanh-Toan Do, Ian D. Reid, and Gustavo Carneiro. Bayesian Generative Active\nDeep Learning. In Proceedings of the International Conference on Machine Learning\n(ICML), 2019.\nJoost van Amersfoort.\nMinimal CIFAR-10, 2021.\nURL https://github.com/y0ast/\npytorch-snippets/tree/main/minimal_cifar.\nJoost van Amersfoort, Lewis Smith, Yee Whye Teh, and Yarin Gal. Uncertainty Estimation\nUsing a Single Deep Deterministic Neural Network. In Proceedings of the International\nConference on Machine Learning (ICML), 2020.\nJoost van Amersfoort, Lewis Smith, Andrew Jesson, Oscar Key, and Yarin Gal. Improving\nDeterministic Uncertainty Estimation in Deep Learning for Classification and Regression.\narXiv preprint, 2021.\nJoaquin Vanschoren, Jan N. van Rijn, Bernd Bischl, and Luís Torgo. OpenML: Networked\nScience in Machine Learning. SIGKDD Explorations, 2013.\nVladimir Vapnik. Estimation of Dependences Based on Empirical Data, Second Editiontion.\nSpringer, 2006. ISBN 978-0-387-30865-4.\nSahil Verma and Julia Rubin. Fairness definitions explained. In Proceedings of the International\nWorkshop on Software Fairness, FairWare@ICSE, 2018.\nJulien Villemonteix, Emmanuel Vázquez, and Eric Walter. An informational approach to the\nglobal optimization of expensive-to-evaluate functions. Journal of Global Optimization,\n2009.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R.\nBowman. GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language\nUnderstanding. In International Conference on Learning Representations (ICLR), 2019.\nChaoqi Wang, Shengyang Sun, and Roger B. Grosse. Beyond Marginal Uncertainty: How\nAccurately can Bayesian Regression Models Estimate Posterior Predictive Correlations?\nIn International Conference on Artificial Intelligence and Statistics (AISTATS), 2021.\nKeze Wang, Dongyu Zhang, Ya Li, Ruimao Zhang, and Liang Lin. Cost-Effective Active\nLearning for Deep Image Classification. IEEE Transactions on Circuits and Systems for\nVideo Technology, 2017.\nYeming Wen, Dustin Tran, and Jimmy Ba.\nBatchEnsemble: an Alternative Approach\nto Efficient Ensemble and Lifelong Learning. In International Conference on Learning\nRepresentations (ICLR), 2020.\nZheng Wen, Ian Osband, Chao Qin, Xiuyuan Lu, Morteza Ibrahimi, Vikranth Dwaracherla,\nMohammad Asghari, and Benjamin Van Roy.\nFrom Predictions to Decisions: The\nImportance of Joint Predictive Distributions, 2021.\nRoss\nWightman.\nPyTorch\nImage\nModels.\nhttps://github.com/rwightman/\npytorch-image-models, 2019.\nChristopher KI Williams and Carl Edward Rasmussen. Gaussian Processes for Machine\nLearning. MIT Press, 2006.\nBibliography\n340\nPaul L Williams. Information dynamics: Its theory and application to embodied cognitive\nsystems. PhD thesis, PhD thesis, Indiana University, 2011.\nAndrew Gordon Wilson and Pavel Izmailov. Bayesian Deep Learning and a Probabilistic\nPerspective of Generalization. In Advances in Neural Information Processing Systems,\n2020.\nAndrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, and Eric P. Xing. Deep Kernel\nLearning. In International Conference on Artificial Intelligence and Statistics (AISTATS),\n2016.\nJim Winkens, Rudy Bunel, Abhijit Guha Roy, Robert Stanforth, Vivek Natarajan, Joseph R.\nLedsam, Patricia MacWilliams, Pushmeet Kohli, Alan Karthikesalingam, Simon Kohl,\nA. Taylan Cemgil, S. M. Ali Eslami, and Olaf Ronneberger. Contrastive Training for\nImproved Out-of-Distribution Detection. arXiv preprint, 2020.\nGuoxuan Xia and Christos-Savvas Bouganis. On the Usefulness of Deep Ensemble Diversity\nfor Out-of-Distribution Detection. arXiv preprint, 2022.\nHan Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-MNIST: a Novel Image Dataset for\nBenchmarking Machine Learning Algorithms. arXiv preprint, 2017.\nTong Xiao, Tian Xia, Yi Yang, Chang Huang, and Xiaogang Wang. Learning from massive\nnoisy labeled data for image classification. In IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), 2015.\nYu Xie, Jennie E. Brand, and Ben Jann. Estimating Heterogeneous Treatment Effects with\nObservational Data. Sociological Methodology, 2012.\nYilun Xu, Shengjia Zhao, Jiaming Song, Russell Stewart, and Stefano Ermon. A Theory\nof Usable Information under Computational Constraints. In International Conference on\nLearning Representations (ICLR), 2020.\nChhavi Yadav and Léon Bottou. Cold Case: The Lost MNIST Digits. In Advances in Neural\nInformation Processing Systems, 2019.\nDavid Yarowsky. Unsupervised Word Sense Disambiguation Rivaling Supervised Methods.\nIn Annual Meeting of the Association for Computational Linguistics, 1995.\nRaymond W Yeung. A new outlook on Shannon’s information measures. IEEE Transactions\non Information Theory, 1991.\nRaymond W Yeung. Information Theory and Network Coding. Information Technology:\nTransmission, Processing and Storage. Springer US, 2008. ISBN 9780387792347.\nKun Yi and Jianxin Wu. Probabilistic End-To-End Noise Correction for Learning With Noisy\nLabels. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\nKai Yu, Jinbo Bi, and Volker Tresp. Active learning via transductive experimental design. In\nProceedings of the International Conference on Machine Learning (ICML), 2006.\nSergey Zagoruyko and Nikos Komodakis. Wide Residual Networks. In Proceedings of the\nBritish Machine Vision Conference, 2016.\nXueying Zhan, Qingzhong Wang, Kuan-Hao Huang, Haoyi Xiong, Dejing Dou, and Antoni B.\nChan. A Comparative Survey of Deep Active Learning. arXiv preprint, 2022a.\nXueying Zhan, Yaowei Wang, and Antoni B Chan. Asymptotic optimality for active learning\nprocesses. In Uncertainty in Artificial Intelligence, 2022b.\nGuodong Zhang, Shengyang Sun, David Duvenaud, and Roger B. Grosse. Noisy Natural\nGradient as Variational Inference. In Proceedings of the International Conference on\nMachine Learning (ICML), 2018.\nJiong Zhang, Hsiang-Fu Yu, and Inderjit S. Dhillon. AutoAssist: A Framework to Accelerate\nTraining of Deep Neural Networks. In Advances in Neural Information Processing Systems,\n2019a.\nLinfeng Zhang, Jiebo Song, Anni Gao, Jingwei Chen, Chenglong Bao, and Kaisheng Ma. Be\nBibliography\n341\nYour Own Teacher: Improve the Performance of Convolutional Neural Networks via Self\nDistillation. In IEEE/CVF International Conference on Computer Vision (ICCV), 2019b.\nMin-Ling Zhang and Zhi-Hua Zhou.\nExploiting Unlabeled Data to Enhance Ensemble\nDiversity. Data Mining and Knowledge Discovery, 2013.\nGuang Zhao, Edward R. Dougherty, Byung-Jun Yoon, Francis J. Alexander, and Xiaoning\nQian.\nBayesian Active Learning by Soft Mean Objective Cost of Uncertainty.\nIn\nInternational Conference on Artificial Intelligence and Statistics (AISTATS), 2021a.\nGuang Zhao, Edward R. Dougherty, Byung-Jun Yoon, Francis J. Alexander, and Xiaoning\nQian. Efficient Active Learning for Gaussian Process Classification by Error Reduction. In\nAdvances in Neural Information Processing Systems, 2021b.\nGuang Zhao, Edward R. Dougherty, Byung-Jun Yoon, Francis J. Alexander, and Xiaoning\nQian. Uncertainty-aware Active Learning for Optimal Bayesian Classifier. In International\nConference on Learning Representations (ICLR), 2021c.\nXiao Zhou, Renjie Pi, Weizhong Zhang, Yong Lin, Zonghao Chen, and T. Zhang. Probabilistic\nBilevel Coreset Selection. Proceedings of the International Conference on Machine Learning\n(ICML), 2023.\nZhu, Lafferty, and Ghahramani. Combining active learning and semi-supervised learning\nusing Gaussian fields and harmonic functions. Proceedings of the International Conference\non Machine Learning (ICML), 2003.\n",
  "categories": [
    "cs.LG",
    "cs.IT",
    "math.IT"
  ],
  "published": "2024-01-09",
  "updated": "2024-03-08"
}