{
  "id": "http://arxiv.org/abs/2401.06885v1",
  "title": "Accelerating Neural Networks for Large Language Models and Graph Processing with Silicon Photonics",
  "authors": [
    "Salma Afifi",
    "Febin Sunny",
    "Mahdi Nikdast",
    "Sudeep Pasricha"
  ],
  "abstract": "In the rapidly evolving landscape of artificial intelligence, large language\nmodels (LLMs) and graph processing have emerged as transformative technologies\nfor natural language processing (NLP), computer vision, and graph-structured\ndata applications. However, the complex structures of these models pose\nchallenges for acceleration on conventional electronic platforms. In this\npaper, we describe novel hardware accelerators based on silicon photonics to\naccelerate transformer neural networks that are used in LLMs and graph neural\nnetworks for graph data processing. Our analysis demonstrates that both\nhardware accelerators achieve at least 10.2x throughput improvement and 3.8x\nbetter energy efficiency over multiple state-of-the-art electronic hardware\naccelerators designed for LLMs and graph processing.",
  "text": " \nAccelerating Neural Networks for Large Language \nModels and Graph Processing with Silicon Photonics \nSalma Afifi, Febin Sunny, Mahdi Nikdast, Sudeep Pasricha \nDepartment of Electrical and Computer Engineering \nColorado State University \nFort Collins, Colorado, USA \n{salma.afifi, febin.sunny, mahdi.nikdast, sudeep}@colostate.edu \nAbstract—In the rapidly evolving landscape of artificial \nintelligence, large language models (LLMs) and graph \nprocessing have emerged as transformative technologies for \nnatural language processing (NLP), computer vision, and \ngraph-structured data applications. However, the complex \nstructures of these models pose challenges for acceleration on \nconventional electronic platforms. In this paper, we describe \nnovel hardware accelerators based on silicon photonics to \naccelerate transformer neural networks that are used in LLMs \nand graph neural networks for graph data processing. Our \nanalysis demonstrates that both hardware accelerators achieve \nat least 10.2× throughput improvement and 3.8× better energy \nefficiency over multiple state-of-the-art electronic hardware \naccelerators designed for LLMs and graph processing.  \nKeywords— artificial intelligence, silicon photonics, hardware \naccelerators, large language models, graph neural networks \nI. INTRODUCTION \nLarge language models (LLMs) have garnered \nsignificant attention in recent years because of their \nexceptional performance in natural language processing \n(NLP) tasks. LLMs are based on the Transformer neural \nnetwork architecture which attributes its capabilities to the \nself-attention mechanism [1]. Transformer networks exhibit \nversatile capabilities in excelling at and addressing a variety \nof tasks, such as transduction and computer vision, leading to \na growing adoption across various fields. This is unlike \nearlier models restricted to specific task domains such as \nrecurrent neural networks (RNNs) and convolution neural \nnetworks (CNNs). Therefore, LLMs are widely expected to \nlead us closer to the artificial general intelligence paradigm. \nIn addition, the success of deep learning techniques has \nled to applying neural network architectures to graph \nprocessing. However, conventional deep neural networks \n(DNNs) are designed to operate on regular grid-like patterns \nwithin Euclidean space, complicating their deployment with \nnon-Euclidean data domains such as graphs. Thus, graph \nneural networks (GNNs) have emerged as a revolutionary \napproach for graph processing, attaining remarkable \nperformance in many tasks such as social network \nrelationship analysis and knowledge graphing [2]. Despite \ntheir complex structure, GNNs are crucial to enable the \ndevelopment of cutting-edge AI applications and systems.  \nThe development of hardware platforms capable of \nproviding sufficient support to execute transformers in LLMs \nand GNNs for graph processing with high performance, while \nabiding by strict power constraints, is thus of paramount \nimportance. Although hardware acceleration for neural \nnetworks such as CNNs and RNNs has been extensively \nstudied, the processing of transformer and GNN models \npresents unique challenges due to their greater complexity. \nMoreover, \nconventional \nelectronic \naccelerators \nare \nincreasingly susceptible to the limits of the post Moore’s law \nera, where diminishing performance gains are evident as \ntechnology scales [3]. These limitations pose significant \nperformance and energy efficiency obstacles when executing \nLLMs and GNNs on electronic hardware platforms. \nSilicon photonics has demonstrated its proficiency not \nonly in high-throughput communication within the telecom \nand datacom sectors but also as a viable solution for chip-\nscale communication. The incorporation of energy-efficient \noptical modulators enables photonic interconnects to achieve \nelevated modulation speeds (>50 GHz) and maintain low \nenergy consumption (<70 fJ/bit) [4]. Moreover, CMOS-\ncompatible silicon photonic components can be used for \ncomputations, such as matrix-vector multiplications and logic \ngate implementations [5]. Optical computations show the \npotential to achieve ܱ(ܰ) energy scaling for ܱ(ܰଶ) fixed-\npoint operations while energy tends to increase with ܱ(ܰଶ)  \nfor digital electronic systems [6]. \nIn this paper, based on our recent works [2], [7], we \npresent the first hardware accelerators for neural networks \nused in LLMs and graph processing that leverage silicon \nphotonics. Unlike other accelerators, our architectures are \ncarefully tailored for a wide range of building block \ncomponents leveraged in LLMs and graph processing. By \nintegrating the high-speed, low-energy optical operations and \nharnessing the inherent parallelism of silicon photonics, our \nproposed accelerators demonstrate significant enhancements \nin both throughput and energy efficiency when compared to \nthe state-of-the-art LLM and GNN hardware accelerators.  \nII. LARGE LANGUAGE MODELS \nLLMs make use of Transformer neural network models \ncharacterized by massive parameter sizes and outstanding \nlearning capabilities. The original transformer model [8] \nforms the architecture backbone for many state-of-the-art \nLLMs. Fig. 1 shows the transformer model composed of two \nmain blocks: an encoder and a decoder. The encoder \ntransforms the input sequence into a continuous abstract \nrepresentation. Subsequently, the decoder processes this \nrepresentation incrementally, generating a singular output \nwhile incorporating prior outputs.  \n \n \nFig. 1: Transformer neural network architecture overview. \nThis research was supported by the National Science Foundation (NSF) \nunder grants CNS-2046226 and CCF-1813370 \nAs shown in Fig. 1, the encoder and decoder blocks are \nusually composed of ܰ stacked layers and their main sub-\nblocks are the multi-head attention (MHA) and feed forward \n(FF) layers, along with residual connections for each and \nfollowed by layer normalization. The computation of the \nmost intricate operation within the transformer, namely the \nself-attention mechanism, is carried out within the MHA \nblock. Each MHA has ܪ self-attention heads, and each \nattention head generates the query (ܳ), key (ܭ), and value (ܸ) \nvectors to compute a scaled dot-product attention as follows: \nܪ݁ܽ݀(ܺ) =  ܽݐݐ݁݊ݐ݅݋݊(ܳ, ܭ, ܸ) =  ݏ݋݂ݐ݉ܽݔቆܳܭܶ\nඥ݀ܭ\n൘\nቇܸ,\n(1) \nwhere ܺ is the input matrix and ݀௄ is the dimension of ܳ and \nܭ. The output of the MHA is the concatenation of the self-\nattention heads’ outputs, followed by a linear layer. The FF \nnetwork is composed of two dense layers with a RELU \nactivation in between. Pre-trained language models based on \nthe transformer architecture typically either include just the \nencoder block, such as in BERT [9], or just the decoder block, \nsuch as in GPT [1]. Additionally, the vision transformer \n(ViT) models used in computer vision tasks have ܰ encoder \nlayers followed by a multi-layer perceptron [2]. Such \nvariations in Transformers across NLP and vision tasks along \nwith the complex operations and structure of Transformers \nimpose serious challenges for their acceleration.  \nTo mitigate memory-wall bottlenecks in Transformers, \nspecifically those used in LLMs, prior works have explored \nprocessing-in-memory (PIM). An in-memory computing-\nbased transformer accelerator called TransPIM was presented \nin [10] along with a novel dataflow for optimized data \nmovements to high bandwidth memory. The work in [11] \nproposed a ReRAM-based PIM accelerator for transformers. \nHowever, PIM introduces a distinct set of challenges. \nSpecifically, ReRAM cells encounter various reliability \nissues related to endurance and retention [12]. Other works \npresented FPGA-based solutions as in [13] where a hardware \naccelerator was designed to enhance the performance of \nMHA and FF layers. Their strategy involves partitioning the \nweight matrices utilized in both MHA and FF layers to enable \nresource sharing between the layers. An FPGA-based \nacceleration framework was also presented in [14]. \nIII. GRAPH PROCESSING \nBy incorporating deep learning into the domain of graph \nprocessing, GNNs have substantially transformed numerous \ntasks and applications relying on graph structures, such as \nnode classification, link prediction, and graph classification. \nGNNs leverage the connections inherent in a graph to \ncomprehend and represent the relationships among vertices. \nThey employ an iterative methodology based on the graph’s \nstructure and incorporate edges, vertices, and graph feature \nvectors, representing the known attributes of these elements. \nFig. 2 illustrates the three main stages involved in a GNN \nprocessing. First, a graph is input to the GNN that is usually \npreprocessed offline for purposes such as sampling the graph. \nSecond, the aggregation stage iteratively collects the \nneighbors of each vertex and subsequently reduces all the \ngathered data into a singular vector. The data can be reduced \nby using various arithmetic functions such as summation, \nmean, or maximum.  Lastly, the resultant feature vectors are \npassed through the combination phase which is usually \ncomposed of a neural network.  \n \nFig. 2: An overview of GNN inference: 1) Input graph showing initial feature \nvectors; 2) Aggregation phase, where each node’s neighbors are reduced to \none feature vector; 3) Combine and Update phases, where each node is \nlinearly transformed and updated using a non-linear activation function. \nAccelerating GNNs involves overcoming several \nformidable challenges. While most GNNs follow the same \nstages outlined earlier, various new and more complex GNN \nalgorithms and models have recently been introduced. For \nexample, graph convolution networks (GCNs) [15] extend \nconvolution to the graph space by having it defined on \nirregular graph structures. Various models based on GCNs \nhave also emerged such as GraphSAGE and graph \nisomorphism network (GIN). Graph attention networks \n(GATs) represent another category of GNNs where the \nmodels integrate an attention mechanism and enhance node \nfeatures by employing a pairwise function between nodes, \nintegrating learnable weights [2]. A system accelerating \nGNNs needs to have the capability to accommodate such \ndiversity found in GNN models. Furthermore, GNNs present \na set of both dense and very sparse computations and real-\nworld graphs can be extremely large and highly irregular. \nAccordingly, GNNs frequently necessitate extensive memory \nbandwidth and multiple irregular memory accesses. \nSeveral hardware accelerators for GNNs have been \ndeveloped in recent years. An electronic accelerator was \nproposed in [16] where the core unit of their architecture \nconsists of an aggregator module, a DNN accelerator, a DNN \nqueue, and a graph processing element. EnGN [17] utilizes \nclustered processing elements to handle GNNs in a single \ndataflow as concatenated matrix multiplication of feature \nvectors, adjacency matrices, and weights. Other accelerators \nsuch as HyGCN [18] and GRIP [19] are composed of separate \nengines for aggregation and combination. Multiple ReRAM \nand PIM accelerators have also been presented such as \nReGNN [20] and ReGraphX [21]. \nIV. SILICON PHOTONICS FOR NEURAL NETWORK \nACCELERATION  \nNeural network accelerators built with silicon photonics \nhave emerged as a powerful alternative to electronic-based \naccelerators due to their significant energy and performance \nimprovements [3]. Accordingly, the utilization of silicon \nphotonics for enhancing the acceleration of neural networks \nhas garnered considerable attention [22]-[27].  \nCrossLight [28] is a CNN optical accelerator that \nbenefits from cross-layer device-level, circuit-level, and \narchitecture-level optimization. The CrossLight architecture \nis composed of dedicated vector-dot-product units for \nconvolution and fully connected layers. In [29], an optical \naccelerator for sparse neural networks was introduced with a \nmodular and a vector granularity-aware structure that enables \nsignificant \nthroughput \nand \nenergy \nimprovements. \nAdditionally, an optical hardware accelerator for RNNs was \nproposed in [30]. Several other architectures have focused on \noptical-domain acceleration for CNNs, RNNs, and MLPs \n[26]. To the best of our knowledge, the accelerators we \nproposed in [2] and [7] are the first silicon photonics-based \naccelerators for GNNs and transformers, respectively.  \nNeural network acceleration using silicon photonics \nrequires imprinting the network’s parameters onto optical \nsignals which enables performing MAC operations. This can \nbe done using coherent or non-coherent implementations. \nCoherent architectures utilize a single wavelength where the \nparameters are imprinted onto the optical signal’s phase. On \nthe other hand, multiple wavelengths are leveraged in non-\ncoherent architectures and the parameters are imprinted onto \nthe optical signal’s amplitude. Our optical accelerators’ core \noperations are performed using opto-electronic microring \nresonator (MR) devices. Each MR can be designed and tuned \nto work at a specific wavelength, called MR resonant \nwavelength (ߣெோ), defined as:  \nߣெோ= 2ߨܴ\n݉݊௘௙௙,\n(2) \nwhere ܴ is the MR radius, ݉ is the order of the resonance, \nand ݊௘௙௙ is the effective index of the device.  \n \n \n \nFig. 3: (a) MR input and through ports’ wavelengths after imprinting a \nparameter onto the signal; (b) two MR devices used to perform optical \ncoherent summation to add values a1, a2, and a3; (c) MR bank arrays used to \nperform multiplication by imprinting input vector (a1-a3), followed by weight \nvector (w1-w3); (d) MR bank response and heterodyne crosstalk shown in \nblack, where CS is channel spacing and FSR is free spectral range. From [2]. \nA tuning circuit can be used to carefully alter ݊௘௙௙ which \nmodulates electronic data onto the optical signals. The tuning \ncircuit should result in a change in ݊௘௙௙, and thus a resonant \nshift (߂ߣெோ). Fig. 3(a) illustrates the transmission plots for \nthe input and the through ports’ responses after a parameter \nis imprinted onto the input signal. As a result of tuning an \nMR’s ߣெோ, a predictable change in the optical signal’s \namplitude can be observed. Our accelerators expand upon \nthis to implement their two main operations: summation and \nmultiplication, as discussed below.  \nFig. 3(b) shows an example of performing the addition \nof ܽ1, ܽ2, and ܽ3 using coherent summation where a single \nwavelength ߣெோ is used. All MR devices are configured to \noperate on the same resonant wavelength. VCSEL units are \nlaser sources that can be configured to generate an optical \nsignal with a certain wavelength and an amplitude specified \nby an input analog signal. When the three optical signals \nmeet, they undergo interference, resulting in a summation \noperation and the final output ܽ1 + ܽ2 + ܽ3 is computed. \nAn example of performing multiplication is shown in \nFig. 3(c). To enhance throughput and emulate neurons, non-\ncoherent silicon photonics is used where multiple optical \nsignals with different wavelengths are multiplexed into the \nsame waveguide using wavelength division multiplexing \n(WDM). This waveguide then passes through two banks of \nMR devices where each MR in each bank is configured to \noperate on a specific wavelength (outlined by the different \ncolors). In the example shown, three different wavelengths \nare needed to multiply the input activation vector [ܽ1, ܽ2,ܽ3] \nby the weight vector [ݓ1, ݓ2, ݓ3]. The first set of the MR \ndevices imprint the input activation values onto the three \ndifferent wavelengths. When the second set of the MR \ndevices imprint the weight values onto the same optical \nsignals, a multiplication operation occurs, and the output \noptical \nsignals \nrepresent \nthe \noutput \nvector \n[ݓ1ܽ1, ݓ2ܽ2,ݓ3ܽ3]. \nNon-coherent \nsilicon \nphotonics \nincurs \nvarious \nchallenges such as high energy overheads and heterodyne (or \nincoherent) crosstalk. The latter is shown as the shaded \nregions in black in Fig. 3(d). The crosstalk arises when a \nsegment of an optical signal from adjacent wavelengths \ninterferes with another wavelength. We thoroughly discuss \nour efforts to overcome such challenges in Section V.B. \nV. TRANSFORMER AND GNN HARDWARE ACCELERATORS \nAccelerating Transformers and GNNs requires having an \nefficient \nunderlying \nhardware \nthat \nis \ncapable \nof \naccommodating and tailoring its execution to each class of \nthe neural networks’ unique requirements. In this section, we \npresent \nour \nsilicon-photonic-based \naccelerators \nfor \nTransformers and GNNs. Section V.A discusses the tuning \ncircuit design used in our accelerators, followed by Section \nV.B which introduces the various MR device optimizations \nperformed. Sections V.C and V.D then present our \nTransformer and GNN optical hardware architecture designs. \nA. Tuning Ciruit Design \nAs discussed in section IV, MR devices require a tuning \ncircuit that can be based on the electro-optic (EO) or thermos-\noptic (TO) effect. EO tuning operates at a faster rate and \nconsumes less power, but it cannot be used for large tuning \nranges. On the other hand, TO tuning accommodates larger \ntunability range but has the drawback of higher latency and \npower consumption [30]. We have employed a hybrid tuning \napproach for our hardware accelerators, merging the benefits \nof EO and TO while mitigating their respective drawbacks. \nIn our approach, EO tuning is leveraged for fast induction of \nsmall ߂ߣெோ, whereas slower TO tuning is only enabled \ninfrequently when there is a need for larger ߂ߣெோ. \nAdditionally, our designs integrate the thermal eigenmode \ndecomposition method (TED) as outlined in [29], to \neffectively decrease the power consumption associated with \nTO tuning and mitigate thermal crosstalk.  \nB. MR Device Optimization \nOperating in the analog photonic domain presents a set of \nnoise sources that need to be addressed to ensure correct \nexecution of Transformers and GNNs. Such noise sources \ninclude thermal crosstalk, heterodyne (or incoherent) \ncrosstalk, and homodyne (or coherent) crosstalk. Our TED-\nbased method (Section V.A) mitigates the thermal crosstalk \nbetween TO tuning circuits. Heterodyne or inter-channel \ncrosstalk emerges when multiple wavelengths are used in the \nsame waveguide employing non-coherent silicon photonics. \nThis occurs when a portion of an optical signal from a \nneighboring wavelength undesirably leaks into the MR \nspectrum of another wavelength, causing inaccuracies. To \nefficiently alleviate heterodyne crosstalk, spectral overlap \nneeds to be minimized. To achieve this, various factors \nshould be considered and optimized. Specifically, key \nelements such as well-designed channel spacing, Q-factor \ntuning, ensuring a signal-to-noise ratio (SNR) in the output \nthat surpasses photodetector sensitivity, and optimizing the \ntunable range of the designed MRs must be addressed.  \nOn the other hand, homodyne crosstalk occurs due to the \npresence of undesired coupling between signals with the \nsame wavelength. As will be discussed later, some of our \naccelerators’ computation circuits are based on coherent \nsilicon photonics. A portion of a signal might end up leaking \ninto an opto-electric device and experiences a change in its \nphase. These leaked signals then interfere with the output \nsignals causing inaccuracies and impacting the SNR. One \nway to mitigate homodyne noise is to increase the crossover \ncoupling by increasing the gap between the MR input \nwaveguide and MR ring waveguide. This reduces the amount \nof crosstalk signal being coupled over from the MR to the \nmain \nwaveguide. \nFor \nachieving \nthis, \nseveral \nMR \nconfiguration parameters need to be carefully tuned. \nAs thoroughly explained in [2] and [7], we accurately \nmodeled heterodyne crosstalk, homodyne crosstalk, and the \nMR’s device configuration. Utilizing these models and the \nsimulation tool suite from Ansys Lumerical [31], we can \nidentify the design space for our MRs and the MR banks they \nconstitute. Accordingly, for our accelerators, we have \ndetermined the optimal MR design and configurations that \nwould result in negligible crosstalk noise. \nC. Transformer Hardware Accelerator Design \nOur Transformer hardware accelerator is a photonic \naccelerator capable of accelerating the inference of a broad \nfamily of neural networks used in LLMs. Fig. 4 presents an \noverview of the photonic accelerator’s architecture, with \nMHA and FF units being the core components.  \n \n \nFig. 4: Transformer neural network architecture overview. From [7]. \nThe MHA unit enables performing optically the time-\nconsuming \nmatrix \nmultiplications \n(MatMuls) \nwhich \nconstitutes the major challenge with most LLMs’ inference. \nTo minimize intermediate storage and opto-electronic \nconversions, our architecture decomposes the main MatMul \nin MHA (see (1) in Section II) as follows: \nܳ. ܭ்= ܳ. (ܺ. ܹ௄)்= (ܳ. ܹ௄\n்). ்ܺ.\n(3) \nSuch decomposition mitigates the need to convert the optical \nsignals (matrix ܭ) to the digital domain to perform its \ntranspose operation before the multiplication with matrix ܳ. \nConversely as outlined in Fig. 5(a), matrices ܺ, ܹொ, ܹ௄\n்݀௄\n⁄\n, \nand ்ܺ are computed and stored offline, which allows us to \nperform the MatMul completely in the optical domain. After \ncomputing ܳ. ܭ் with the upper MR bank arrays illustrated \nin Fig. 5(a), the accumulated partial sums are processed using \nbalanced photodetectors (BPDs). BPDs facilitate the \nhandling of both positive and negative parameter values by \nincorporating distinct positive and negative arms within the \nsame waveguide. The sum obtained from the negative arm is \nsubtracted from the sum originating from the positive arm. \nSubsequently, the results are converted to the digital domain \nto undergo softmax computation using lookup tables (LUTs) \nand simple digital circuits. The linear layer is implemented \noptically using two MR bank arrays, while the residual \nconnections are executed by adding the MHA input to its \ncurrent output using coherent photonic summation. Lastly, \nlayer normalization (LN) is implemented optically using a \nsingle MR, tuned by the LN parameter. The entire MHA \narchitecture is shown in Fig. 5(b).  \n \n \n(a) \n \n      (b) \nFig. 5: (a) Attention head unit comprised of seven MR bank arrays for \nMatMul operations, each with dimension K×N; (b) MHA unit composed of \nH attention heads, buffer and concatenate block, linear layer, and an add and \nnormalize block. From [7]. \nD. GNN Hardware Accelerator Design \nThe architecture of our GNN accelerator is depicted in \nFig. 6. Its primary components (aggregate, combine, and \nupdate) are partitioned into ܸ execution lanes. During the \ninference phase, each lane is tasked with processing one \noutput vertex concurrently with all other lanes. The aggregate \nblock \ncollects \nall \nneighboring \nvertices \nand \ntheir \ncorresponding edge data and executes a reduce function for \neach assigned output vertex. The combine block then applies \na linear transformation to each aggregated vertex feature \nvector. Lastly, the update block employs a non-linear \nactivation function to derive the updated vertex feature \nvectors ℎ௩′.  \n \n \n \nFig. 6: Overview of optical GNN accelerator architecture. From [2]. \nOne key performance challenge when accelerating \nGNNs is handling the highly sparse and irregular memory \naccesses. By employing a “buffer and partition” optimization \nas described in [2], we can alleviate this bottleneck. This \ntechnique dictates splitting the input graph into blocks of ܰ \nand ܸ where the aggregate block then is composed of ܰ edge \ncontrol units, ܸ gather units, and ܸ reduce units. Each \nexecution lane is assigned one output node per cycle while ܰ \ninput nodes are fetched by the edge control units and \nforwarded to the gather units as needed. Gather units then \nconvert this data (output node and neighboring input nodes) \nto analog signals that are used to tune the MRs. \nTo accommodate a wide range of reduction functions \n(summation, mean, and maximum), the reduce unit is \nconfigured as an optical coherent summation block (see Fig. \n7(a)). Each row in the reduce unit corresponds to a feature \nfrom the vertex feature vectors while each column is \nassociated with a neighbor input vertex. The signals produced \nby the top VCSELs traverse the MR bank to imprint the \nfeature values of neighboring nodes onto the signals. As \ndiscussed in Section IV, when the different waveguides \ncarrying signals with the same wavelength meet, they \nundergo interference. Accordingly, the result of each row is \nthe summation of all the neighbor node values imprinted onto \neach optical signal. The combine block gathers outputs from \nthe aggregate block and executes a linear transformation \nusing learned weight parameters. This linear transformation \nwithin the transform unit occurs in the optical domain using \nMR bank arrays, as depicted in Fig. 7(b). \n \n \nFig. 7: (a) Reduce unit showing the needed changes in each feature lane to \nsupport the max aggregation operation using an optical comparator; (b) \ndetailed view of transform unit. From [2]. \nGiven that linear transformation operations in GNNs are \npredominantly matrix-vector multiplications, they can be \naccomplished in the optical domain using MR bank arrays. \nThis involves passing the weight values to the transform unit \nas analog signals to fine-tune each MR while the feature \nvector values are imprinted onto the optical signals in the \nwaveguide from the reduce units. Unlike the MR arrays \nutilized in reduce units for summation operations (see Fig. \n7(a)), multiplications are performed non-coherently.  \nLastly, the update block comprises ܸ update units, each \ntasked with applying a non-linear activation function to the \noutput originating from the transform units. Non-linear \nactivation functions such as RELU, sigmoid, and tanh are \nimplemented \noptically \nusing \nsemiconductor-optical-\namplifiers (SOAs). On the other hand, non-linear activation \nfunctions that pose difficulties to be performed optically (e.g., \nsoftmax), are implemented using LUTs and simple digital \ncircuits. Additionally, the hardware accelerator supports \nvarious orchestration and scheduling optimization techniques \nincluding graph buffering and partitioning, execution \npipelining and scheduling, weight DAC sharing, and \nworkload balancing [2]. Such techniques enable the hardware \nto efficiently accommodate a broad family of GNN models \nand regulate the memory accesses during execution. \nVI. EXPERIMENTAL RESULTS \nFor evaluating our proposed hardware accelerators, we \ndeveloped comprehensive simulators in Python to estimate \nthe power and latency costs. The simulators efficiently \nperform software mapping for the various Transformer and \nGNN models used in our experiments, as well as hardware \nmapping where all the optoelectronic and electronic devices \nand circuits are modeled. For all the memories and buffers \nemployed in our accelerators, CACTI [32] was used to obtain \ntheir \nperformance \nand \nenergy \nestimates. \nMultiple \nTransformer and GNN models and datasets were used in our \nexperiments. Based on our analysis conducted for each model \nand dataset, we concluded that employing 8-bit model \nquantization yields algorithmic accuracy comparable to \nmodels utilizing full (32-bit) precision. Consequently, we \nfocused on the acceleration of Transformer and GNN models \nwith 8-bit precision. The specific architectural details of each \nhardware accelerator such as the numbers of the \ncomputational blocks, were determined through detailed \ndesign-space analysis as explained in [2] and [7].  \nOur proposed accelerators are compared against multiple \ncomputing \nplatforms \nand \nstate-of-the-art \nhardware \naccelerators. Our Transformer accelerator (referred to as \nTRON) is compared against Tesla V100-SXM2 GPU, TPU \nv2, Intel Xeon CPU, TransPIM [10], FPGA transformer \naccelerator in [13] (FPGA_Acc1), VAQF [33], and FPGA \ntransformer accelerator in [14] (FPGA_Acc2). For our GNN \naccelerator (referred to as GHOST), the platforms chosen for \ncomparison were: GRIP [19], HyGCN [18], EnGN [17], \nHW_ACC [16], ReGNN [20], ReGraphx[21], TPU v4, Intel \nXeon CPU, and NVIDIA A100 GPU. We utilized reported \npower, latency, and energy values for the chosen accelerators, \nand directly acquired outcomes from model executions on the \nGPU, CPU, and TPU platforms to calculate the Energy Per \nBit (EPB) and Giga Operations Per Second (GOPS) for each \nmodel and dataset.  \n \n \nFig. 8: EPB comparison across LLM accelerators [7]. \n \nFig. 9: Throughput comparison across LLM accelerators [7]. \nFigs. 8 and 9 show the EPB and GOPS comparisons \nbetween TRON and the other computing platforms and \nTransformer accelerators considered. On average, our \narchitecture achieves at least 14× better throughput and 8× \nbetter energy efficiency. The increased throughput and EPB \nenhancements can be ascribed to TRON's fast execution in the \noptical domain and minimal conversions to the electronic \ndomain.  \n \n \nFig. 10: EPB comparison across GNN accelerators [2]. \n \nFig. 11: Throughput comparison across GNN accelerators [2]. \nThe EPB and GOPS comparisons for GHOST are shown \nin Figs. 10 and 11. Our simulation experiments reveal that \nGHOST outperforms existing hardware accelerators for \nGNNs, exhibiting a minimum of 10.2× improvement in \nthroughput and 3.8× greater energy efficiency compared to \nGPU, TPU, CPU, and the various state-of-the-art GNN \nhardware accelerators considered. These findings highlight \nGHOST's proficiency in handling diverse graph processing \ntasks and its ability adapt its execution and memory accesses \nscheduling based on the specific GNN and dataset employed.  \nOverall, the improvements from our hardware accelerators \ncan be attributed to the considerably low latency compute \noperations and data transfers in the optical domain, relatively \nlow power consumption, and the various cross-layer \noptimization techniques employed. \nVII. CONCLUSION \nIn this paper, we presented two groundbreaking silicon \nphotonic hardware accelerators for LLMs and graph \nprocessing. The design of these accelerators incorporates \nvarious device-level, circuit-level, and architecture-level \noptimizations. Our photonic hardware LLM accelerator \nexhibited at least 14× better throughput and 8× better energy \nefficiency compared to previously proposed Transformer \naccelerators. Our photonic graph processing accelerator \nshowed a minimum of 10.2× throughput improvement and \n3.8× better energy efficiency against state-of-the-art GNN \naccelerators. The achieved results underscore the use of \nsilicon photonics for energy-efficient and high-throughput \ninference acceleration in LLMs and GNNs. Moving forward, \nboth architectures pave the way for future research and \nadvances in silicon photonic accelerators, with considerations \nfor addressing open challenges such as alternative non-\nvolatile optical memory cells, fabrication-process variations, \nand further optimizations at the device and circuit levels. \nREFERENCES \n[1]  \nY. Chang, at al., “A survey on evaluation of large language models,” \narXiv, 2023. \n[2] \nS. Afifi, F. Sunny, A. Shafiee, M. Nikdast, S. Pasricha, \"GHOST: A \nGraph Neural Network Accelerator using Silicon Photonics.\" ACM \nTransactions on Embedded Computing Systems, 2023. \n[3] \nF. Sunny, M. Nikdast, S. Pasricha, “Cross-layer design for ai \nacceleration with non-coherent optical computing,” ACM GLSVLSI, \n2023. \n[4] \nK. Shiflett, et al., \"Albireo: Energy-efficient acceleration of \nconvolutional neural networks via silicon photonics.\" ISCA, 2021. \n[5] \nF. Sunny, M. Nikdast and S. Pasricha, “A Silicon Photonic \nAccelerator for Convolutional Neural Networks with Heterogeneous \nQuantization,” ACM GLSVLSI, 2022. \n[6] \nM. A. Nahmias, et al., “Photonic multiply-accumulate operations for \nneural networks,” IEEE Journal of Selected Topics in Quantum \nElectronics, 2020. \n[7] \nS. Afifi, F. Sunny, M. Nikdast, S. Pasricha, “TRON: Transformer \nNeural Network Acceleration with Non-Coherent Silicon Photonics,” \nACM GLSVLSI, 2023. \n[8] \nA. Vaswani, et al., “Attention is All you Need,” NIPS, 2017. \n[9]  \nJ. Devlin, et al., “BERT: pre-training of deep bidirectional \ntransformers for language understanding,” CoRR, 2018. \n[10] \nM. Zhou, et al., “TransPIM: A Memory-based Acceleration via \nSoftware-Hardware Co-Design for Transformer,” IEEE HPCA, 2022. \n[11] \nX. Yang, et al., “ReTransformer: ReRAM-based processing-in-\nmemory architecture for transformer acceleration,” ICCAD, 2020. \n[12] \nY. Chen, \"ReRAM: History, Status, and Future,\" IEEE Transactions \non Electron Devices, vol. 67, no. 4, pp. 1420-1433, 2020. \n[13] \nL. Siyuan, et al., “Hardware accelerator for multi-head attention and \nposition-wise feed-forward in the transformer,” IEEE SOCC, 2020. \n[14] \nQ. Panjie, et al., “Accelerating framework of transformer by hardware \ndesign and model compression co-optimization,” ICCAD, 2021. \n[15] \nK. Thomas, and M. Welling, “Semi-supervised classification with \ngraph convolutional networks,” arXiv, 2016. \n[16] \nA. Auten, M. Tomei, and R. Kumar, “Hardware acceleration of graph \nneural networks,” ACM/IEEE DAC, 2019. \n[17] \nS. Liang, et al., “EnGN: A high-throughput and energy-efficient \naccelerator for large graph neural networks,” arXiv, 2019. \n[18] \nM. Yan, et al., “Hygcn: A gcn accelerator with hybrid architecture,” \nIEEE HPCA, pp 15-29, 2020. \n[19] \nK. Kiningham, P. Levis, C. Ré, “GRIP: A graph neural network \naccelerator architecture,” IEEE Transactions on Computers, 2022. \n[20] \nC. Liu, et al., “ReGNN: a ReRAM-based heterogeneous architecture \nfor general graph neural networks,” ACM/IEEE DAC, 2022. \n[21] \nA. Arka et al., “ReGraphX: NoC-enabled 3D heterogeneous ReRAM \narchitecture for training graph neural networks,” DATE, 2021. \n[22] \nF. Sunny, M. Nikdast and S. Pasricha, “A Silicon Photonic \nAccelerator for Convolutional Neural Networks with Heterogeneous \nQuantization,” ACM GLSVLSI, 2022. \n[23] \nA. Shafiee, S. Banerjee, K. Chakrabarty, S. Pasricha, M. Nikdast, \n“LoCI: An Analysis of the Impact of Optical Loss and Crosstalk \nNoise in Integrated Silicon-Photonic Neural Networks”, ACM \nGLSVLSI, 2022. \n[24] \nF. Sunny, E. Taheri, M. Nikdast, S. Pasricha, “Machine Learning \nAccelerators in 2.5D Chiplet Platforms with Silicon Photonics”, \nIEEE/ACM DATE, 2023. \n[25] \nF. Sunny, A. Mirza, M. Nikdast, S. Pasricha, “ROBIN: A Robust \nOptical Binary Neural Network Accelerator”, ACM TECS, 2021. \n[26] \nF. Sunny, E. Taheri, M. Nikdast, S. Pasricha, “A Survey on Silicon \nPhotonics for Deep Learning,” ACM JETC. 17:4, 2021. \n[27] \nS. Banerjee, M. Nikdast, S. Pasricha, K. Chakrabarty, “CHAMP: \nCoherent Hardware-Aware Magnitude Pruning of Integrated \nPhotonic Neural Networks,” IEEE OFC, 2022. \n[28] \nF. Sunny, A. Mirza, M. Nikdast, S. Pasricha, “CrossLight: A Cross-\nLayer Optimized Silicon Photonic Neural Network Accelerator,” \nIEEE/ACM DAC, 2021. \n[29] \nF. Sunny, M. Nikdast, and S. Pasricha, “SONIC: A Sparse Neural \nNetwork Inference Accelerator with Silicon Photonics for Energy-\nEfficient Deep Learning,” IEEE/ACM ASPDAC, 2022. \n[30] \nF. Sunny, M. Nikdast, S. Pasricha, “RecLight: A recurrent neural \nnetwork accelerator with integrated silicon photonics,” ISVLSI, 2022. \n[31] \nAnsys Optics, Online: https://www.ansys.com/products/photonics. \nLast accessed: 12/30/2023. \n[32] \nHP Labs: CACTI. [Online]: https://www.hpl.hp.com/research/cacti/. \n[33] \nS. Mengshu, et al., “VAQF: Fully automatic software-hardware co-\ndesign framework for low-bit vision transformer,” arXiv, 2022. \n \n",
  "categories": [
    "cs.AR",
    "cs.LG"
  ],
  "published": "2024-01-12",
  "updated": "2024-01-12"
}