{
  "id": "http://arxiv.org/abs/2401.09561v1",
  "title": "Sharing Knowledge in Multi-Task Deep Reinforcement Learning",
  "authors": [
    "Carlo D'Eramo",
    "Davide Tateo",
    "Andrea Bonarini",
    "Marcello Restelli",
    "Jan Peters"
  ],
  "abstract": "We study the benefit of sharing representations among tasks to enable the\neffective use of deep neural networks in Multi-Task Reinforcement Learning. We\nleverage the assumption that learning from different tasks, sharing common\nproperties, is helpful to generalize the knowledge of them resulting in a more\neffective feature extraction compared to learning a single task. Intuitively,\nthe resulting set of features offers performance benefits when used by\nReinforcement Learning algorithms. We prove this by providing theoretical\nguarantees that highlight the conditions for which is convenient to share\nrepresentations among tasks, extending the well-known finite-time bounds of\nApproximate Value-Iteration to the multi-task setting. In addition, we\ncomplement our analysis by proposing multi-task extensions of three\nReinforcement Learning algorithms that we empirically evaluate on widely used\nReinforcement Learning benchmarks showing significant improvements over the\nsingle-task counterparts in terms of sample efficiency and performance.",
  "text": "Published as a conference paper at ICLR 2020\nSHARING KNOWLEDGE IN MULTI-TASK\nDEEP REINFORCEMENT LEARNING\nCarlo D’Eramo & Davide Tateo\nDepartment of Computer Science\nTU Darmstadt, IAS\nHochschulstraße 10, 64289, Darmstadt, Germany\n{carlo.deramo,davide.tateo}@tu-darmstadt.de\nAndrea Bonarini & Marcello Restelli\nPolitecnico di Milano, DEIB\nPiazza Leonardo da Vinci 32, 20133, Milano\n{andrea.bonarini,marcello.restelli}@polimi.it\nJan Peters\nTU Darmstadt, IAS\nHochschulstraße 10, 64289, Darmstadt, Germany\nMax Planck Institute for Intelligent Systems\nMax-Planck-Ring 4, 72076, Tübingen, Germany\njan.peters@tu-darmstadt.de\nABSTRACT\nWe study the benefit of sharing representations among tasks to enable the effective\nuse of deep neural networks in Multi-Task Reinforcement Learning. We leverage\nthe assumption that learning from different tasks, sharing common properties, is\nhelpful to generalize the knowledge of them resulting in a more effective feature ex-\ntraction compared to learning a single task. Intuitively, the resulting set of features\noffers performance benefits when used by Reinforcement Learning algorithms.\nWe prove this by providing theoretical guarantees that highlight the conditions\nfor which is convenient to share representations among tasks, extending the well-\nknown finite-time bounds of Approximate Value-Iteration to the multi-task setting.\nIn addition, we complement our analysis by proposing multi-task extensions of\nthree Reinforcement Learning algorithms that we empirically evaluate on widely\nused Reinforcement Learning benchmarks showing significant improvements over\nthe single-task counterparts in terms of sample efficiency and performance.\n1\nINTRODUCTION\nMulti-Task Learning (MTL) ambitiously aims to learn multiple tasks jointly instead of learning them\nseparately, leveraging the assumption that the considered tasks have common properties which can be\nexploited by Machine Learning (ML) models to generalize the learning of each of them. For instance,\nthe features extracted in the hidden layers of a neural network trained on multiple tasks have the\nadvantage of being a general representation of structures common to each other. This translates into\nan effective way of learning multiple tasks at the same time, but it can also improve the learning\nof each individual task compared to learning them separately (Caruana, 1997). Furthermore, the\nlearned representation can be used to perform Transfer Learning (TL), i.e. using it as a preliminary\nknowledge to learn a new similar task resulting in a more effective and faster learning than learning\nthe new task from scratch (Baxter, 2000; Thrun & Pratt, 2012).\nThe same benefits of extraction and exploitation of common features among the tasks achieved\nin MTL, can be obtained in Multi-Task Reinforcement Learning (MTRL) when training a single\nagent on multiple Reinforcement Learning (RL) problems with common structures (Taylor & Stone,\n2009; Lazaric, 2012). In particular, in MTRL an agent can be trained on multiple tasks in the same\n1\narXiv:2401.09561v1  [cs.LG]  17 Jan 2024\nPublished as a conference paper at ICLR 2020\ndomain, e.g. riding a bicycle or cycling while going towards a goal, or on different but similar\ndomains, e.g. balancing a pendulum or balancing a double pendulum1. Considering recent advances\nin Deep Reinforcement Learning (DRL) and the resulting increase in the complexity of experimental\nbenchmarks, the use of Deep Learning (DL) models, e.g. deep neural networks, has become a popular\nand effective way to extract common features among tasks in MTRL algorithms (Rusu et al., 2015;\nLiu et al., 2016; Higgins et al., 2017). However, despite the high representational capacity of DL\nmodels, the extraction of good features remains challenging. For instance, the performance of the\nlearning process can degrade when unrelated tasks are used together (Caruana, 1997; Baxter, 2000);\nanother detrimental issue may occur when the training of a single model is not balanced properly\namong multiple tasks (Hessel et al., 2018).\nRecent developments in MTRL achieve significant results in feature extraction by means of algorithms\nspecifically developed to address these issues. While some of these works rely on a single deep\nneural network to model the multi-task agent (Liu et al., 2016; Yang et al., 2017; Hessel et al., 2018;\nWulfmeier et al., 2019), others use multiple deep neural networks, e.g. one for each task and another\nfor the multi-task agent (Rusu et al., 2015; Parisotto et al., 2015; Higgins et al., 2017; Teh et al., 2017).\nIntuitively, achieving good results in MTRL with a single deep neural network is more desirable\nthan using many of them, since the training time is likely much less and the whole architecture is\neasier to implement. In this paper we study the benefits of shared representations among tasks. We\ntheoretically motivate the intuitive effectiveness of our method, deriving theoretical guarantees that\nexploit the theoretical framework provided by Maurer et al. (2016), in which the authors present\nupper bounds on the quality of learning in MTL when extracting features for multiple tasks in a\nsingle shared representation. The significancy of this result is that the cost of learning the shared\nrepresentation decreases with a factor O(1/\n√\nT), where T is the number of tasks for many function\napproximator hypothesis classes. The main contribution of this work is twofold.\n1. We derive upper confidence bounds for Approximate Value-Iteration (AVI) and Approximate\nPolicy-Iteration (API)2 (Farahmand, 2011) in the MTRL setting, and we extend the approx-\nimation error bounds in Maurer et al. (2016) to the case of multiple tasks with different\ndimensionalities. Then, we show how to combine these results resulting in, to the best\nof our knowledge, the first proposed extension of the finite-time bounds of AVI/API to\nMTRL. Despite being an extension of previous works, we derive these results to justify\nour approach showing how the error propagation in AVI/API can theoretically benefit from\nlearning multiple tasks jointly.\n2. We leverage these results proposing a neural network architecture, for which these bounds\nhold with minor assumptions, that allow us to learn multiple tasks with a single regressor\nextracting a common representation. We show an empirical evidence of the consequence of\nour bounds by means of a variant of Fitted Q-Iteration (FQI) (Ernst et al., 2005), based on our\nshared network and for which our bounds apply, that we call Multi Fitted Q-Iteration (MFQI).\nThen, we perform an empirical evaluation in challenging RL problems proposing multi-\ntask variants of the Deep Q-Network (DQN) (Mnih et al., 2015) and Deep Deterministic\nPolicy Gradient (DDPG) (Lillicrap et al., 2015) algorithms. These algorithms are practical\nimplementations of the more general AVI/API framework, designed to solve complex\nproblems. In this case, the bounds apply to these algorithms only with some assumptions,\ne.g. stationary sampling distribution. The outcome of the empirical analysis joins the\ntheoretical results, showing significant performance improvements compared to the single-\ntask version of the algorithms in various RL problems, including several MuJoCo (Todorov\net al., 2012) domains.\n2\nPRELIMINARIES\nLet B(X) be the space of bounded measurable functions w.r.t. the σ-algebra σX , and similarly\nB(X, L) be the same bounded by L < ∞.\nA Markov Decision Process (MDP) is defined as a 5-tuple M =< S, A, P, R, γ >, where S is the\nstate space, A is the action space, P : S × A →S is the transition distribution where P(s′|s, a)\n1For simplicity, in this paper we refer to the concepts of task and domain interchangeably.\n2All proofs and the theorem for API are in Appendix A.2.\n2\nPublished as a conference paper at ICLR 2020\nis the probability of reaching state s′ when performing action a in state s, R : S × A × S →\nR is the reward function, and γ ∈(0, 1] is the discount factor. A deterministic policy π maps,\nfor each state, the action to perform: π : S →A. Given a policy π, the value of an action\na in a state s represents the expected discounted cumulative reward obtained by performing a\nin s and following π thereafter: Qπ(s, a) ≜E[P∞\nk=0 γkri+k+1|si = s, ai = a, π], where ri+1\nis the reward obtained after the i-th transition. The expected discounted cumulative reward is\nmaximized by following the optimal policy π∗which is the one that determines the optimal action\nvalues, i.e., the ones that satisfy the Bellman optimality equation (Bellman, 1954): Q∗(s, a) ≜\nR\nS P(s′|s, a) [R(s, a, s′) + γ maxa′ Q∗(s′, a′)] ds′. The solution of the Bellman optimality equation\nis the fixed point of the optimal Bellman operator T ∗: B(S × A) →B(S × A) defined as\n(T ∗Q)(s, a) ≜\nR\nS P(s′|s, a)[R(s, a, s′) + γ maxa′ Q(s′, a′)]ds′. In the MTRL setting, there are\nmultiple MDPs M(t) =< S(t), A(t), P(t), R(t), γ(t) > where t ∈{1, . . . , T} and T is the number\nof MDPs. For each MDP M(t), a deterministic policy πt : S(t) →A(t) induces an action-value\nfunction Qπt\nt (s(t), a(t)) = E[P∞\nk=0 γkr(t)\ni+k+1|si = s(t), ai = a(t), πt]. In this setting, the goal is to\nmaximize the sum of the expected cumulative discounted reward of each task.\nIn our theoretical analysis of the MTRL problem, the complexity of representation plays a central role.\nAs done in Maurer et al. (2016), we consider the Gaussian complexity, a variant of the well-known\nRademacher complexity, to measure the complexity of the representation. Given a set ¯X ∈X T n of n\ninput samples for each task t ∈{1, . . . , T}, and a class H composed of k ∈{1, . . . , K} functions,\nthe Gaussian complexity of a random set H(¯X) = {(hk(Xti)) : h ∈H} ⊆RKT n is defined as\nfollows:\nG(H(¯X)) = E\n\"\nsup\nh∈H\nX\ntki\nγtkihk(Xti)\n\f\f\f\f\fXti\n#\n,\n(1)\nwhere γtki are independent standard normal variables. We also need to define the following quantity,\ntaken from Maurer (2016): let γ be a vector of m random standard normal variables, and f ∈F :\nY →Rm, with Y ⊆Rn, we define\nO(F) =\nsup\ny,y′∈Y,y̸=y′ E\n\"\nsup\nf∈F\n⟨γ, f(y) −f(y′)⟩\n∥y −y′∥\n#\n.\n(2)\nEquation 2 can be viewed as a Gaussian average of Lipschitz quotients, and appears in the bounds\nprovided in this work. Finally, we define L(F) as the upper bound of the Lipschitz constant of all the\nfunctions f in the function class F.\n3\nTHEORETICAL ANALYSIS\nThe following theoretical study starts from the derivation of theoretical guarantees for MTRL in the\nAVI framework, extending the results of Farahmand (2011) in the MTRL scenario. Then, to bound\nthe approximation error term in the AVI bound, we extend the result described in Maurer (2006)\nto MTRL. As we discuss, the resulting bounds described in this section clearly show the benefit of\nsharing representation in MTRL. To the best of our knowledge, this is the first general result for\nMTRL; previous works have focused on finite MDPs (Brunskill & Li, 2013) or linear models (Lazaric\n& Restelli, 2011).\n3.1\nMULTI-TASK REPRESENTATION LEARNING\nThe multi-task representation learning problem consists in learning simultaneously a set of T tasks\nµt, modeled as probability measures over the space of the possible input-output pairs (x, y), with\nx ∈X and y ∈R, being X the input space. Let w ∈W : X →RJ, h ∈H : RJ →RK and\nf ∈F : RK →R be functions chosen from their respective hypothesis classes. The functions\nin the hypothesis classes must be Lipschitz continuous functions. Let ¯Z = (Z1, . . . , ZT ) be the\nmulti-sample over the set of tasks µ = (µ1, . . . , µT ), where Zt = (Zt1, . . . , Ztn) ∼µn\nt and\nZti = (Xti, Yti) ∼µt. We can formalize our regression problem as the following minimization\n3\nPublished as a conference paper at ICLR 2020\nproblem:\nmin\n(\n1\nnT\nT\nX\nt=1\nN\nX\ni=1\nℓ(ft(h(wt(Xti))), Yti) : f ∈FT , h ∈H, w ∈WT\n)\n,\n(3)\nwhere we use f = (f1, . . . , fT ), w = (w1, . . . , wT ), and define the minimizers of Equation (3) as ˆw,\nˆh, and ˆf. We assume that the loss function ℓ: R × R →[0, 1] is 1-Lipschitz in the first argument for\nevery value of the second argument. While this assumption may seem restrictive, the result obtained\ncan be easily scaled to the general case. To use the principal result of this section, for a generic loss\nfunction ℓ′, it is possible to use ℓ(·) = ℓ′(·)/ϵmax, where ϵmax is the maximum value of ℓ′. The expected\nloss over the tasks, given w, h and f is the task-averaged risk:\nεavg(w, h, f) = 1\nT\nT\nX\nt=1\nE [ℓ(ft(h(wt(X))), Y )]\n(4)\nThe minimum task-averaged risk, given the set of tasks µ and the hypothesis classes W, H and F is\nε∗\navg, and the corresponding minimizers are w∗, h∗and f ∗.\n3.2\nMULTI-TASK APPROXIMATE VALUE ITERATION BOUND\nWe start by considering the bound for the AVI framework which applies for the single-task scenario.\nTheorem 1. (Theorem 3.4 of Farahmand (2011)) Let K be a positive integer, and Qmax ≤Rmax\n1−γ . Then\nfor any sequence (Qk)K\nk=0 ⊂B(S × A, Qmax) and the corresponding sequence (εk)K−1\nk=0 , where\nεk = ∥Qk+1 −T ∗Qk∥2\nν, we have:\n∥Q∗−QπK∥1,ρ ≤\n2γ\n(1 −γ)2\n\u0014\ninf\nr∈[0,1] C\n1\n2\nVI,ρ,ν(K; r)E\n1\n2 (ε0, . . . , εK−1; r) +\n2\n1 −γ γKRmax\n\u0015\n,\n(5)\nwhere\nCVI,ρ,ν(K; r) =\n\u00121 −γ\n2\n\u00132\nsup\nπ′\n1,...,π′\nK\nK−1\nX\nk=0\na2(1−r)\nk\n\nX\nm≥0\nγm\u0010\ncVI1,ρ,ν(m, K −k; π′\nK)\n+cVI2,ρ,ν(m + 1; π′\nk+1, . . . , π′\nK)\n\u0011\n\n\n2\n,\n(6)\nwith E(ε0, . . . , εK−1; r) = PK−1\nk=0 α2r\nk εk, the two coefficients cVI1,ρ,ν, cVI2,ρ,ν, the distributions ρ\nand ν, and the series αk are defined as in Farahmand (2011).\nIn the multi-task scenario, let the average approximation error across tasks be:\nεavg,k(ˆwk, ˆhk,ˆfk) = 1\nT\nT\nX\nt=1\n∥Qt,k+1 −T ∗\nt Qt,k∥2\nν,\n(7)\nwhere Qt,k+1 = ˆft,k ◦ˆhk ◦ˆwt,k, and T ∗\nt is the optimal Bellman operator of task t.\nIn the following, we extend the AVI bound of Theorem 1 to the multi-task scenario, by computing\nthe average loss across tasks and pushing inside the average using Jensen’s inequality.\nTheorem 2. Let K be a positive integer, and Qmax ≤Rmax\n1−γ . Then for any sequence (Qk)K\nk=0 ⊂B(S ×\nA, Qmax) and the corresponding sequence (εavg,k)K−1\nk=0 , where εavg,k = 1\nT\nPT\nt=1∥Qt,k+1−T ∗\nt Qt,k∥2\nν,\nwe have:\n1\nT\nT\nX\nt=1\n∥Q∗\nt −QπK\nt\n∥1,ρ ≤\n2γ\n(1 −γ)2\n\u0014\ninf\nr∈[0,1] C\n1\n2\nVI(K; r)E\n1\n2avg(εavg,0, . . . , εavg,K−1; r) + 2γKRmax,avg\n1 −γ\n\u0015\n(8)\nwith Eavg = PK−1\nk=0 α2r\nk εavg,k, γ =\nmax\nt∈{1,...,T }γt, C\n1\n2\nVI(K; r) =\nmax\nt∈{1,...,T }C\n1\n2\nVI,ρ,ν(K; t, r), Rmax,avg =\n1\nT\nPT\nt=1 Rmax,t and αk =\n( (1−γ)γK−k−1\n1−γK+1\n0 ≤k < K,\n(1−γ)γK\n1−γK+1\nk = K\n.\n4\nPublished as a conference paper at ICLR 2020\nRemarks\nTheorem 2 retains most of the properties of Theorem 3.4 of Farahmand (2011), except\nthat the regression error in the bound is now task-averaged. Interestingly, the second term of the\nsum in Equation (8) depends on the average maximum reward for each task. In order to obtain this\nresult we use an overly pessimistic bound on γ and the concentrability coefficients, however this\napproximation is not too loose if the MDPs are sufficiently similar.\n3.3\nMULTI-TASK APPROXIMATION ERROR BOUND\nWe bound the task-averaged approximation error εavg at each AVI iteration k involved in (8) following\na derivation similar to the one proposed by Maurer et al. (2016), obtaining:\nTheorem 3. Let µ, W, H and F be defined as above and assume 0 ∈H and f(0) = 0, ∀f ∈F.\nThen for δ > 0 with probability at least 1 −δ in the draw of ¯Z ∼QT\nt=1 µn\nt we have that\nεavg(ˆw, ˆh,ˆf) ≤L(F)\n\u0012\nc1\nL(H) supl∈{1,...,T } G(W(Xl))\nn\n+ c2\nsupw∥w( ¯X)∥O(H)\nnT\n+c3\nminp∈P G(H(p))\nnT\n\u0013\n+ c4\nsuph,w∥h(w( ¯X))∥O(F)\nn\n√\nT\n+\ns\n8 ln( 3\nδ )\nnT\n+ ε∗\navg. (9)\nRemarks\nThe assumptions 0 ∈H and f(0) = 0 for all f ∈F are not essential for the proof and\nare only needed to simplify the result. For reasonable function classes, the Gaussian complexity\nG(W(Xl)) is O(√n). If supw∥w( ¯X)∥and suph,w∥h(w( ¯X))∥can be uniformly bounded, then\nthey are O(\n√\nnT). For some function classes, the Gaussian average of Lipschitz quotients O(·) can\nbe bounded independently from the number of samples. Given these assumptions, the first and the\nfourth term of the right hand side of Equation (9), which represent respectively the cost of learning the\nmeta-state space w and the task-specific f mappings, are both O(1/√n). The second term represents\nthe cost of learning the multi-task representation h and is O(1/\n√\nnT), thus vanishing in the multi-task\nlimit T →∞. The third term can be removed if ∀h ∈H, ∃p0 ∈P : h(p) = 0; even when this\nassumption does not hold, this term can be ignored for many classes of interest, e.g. neural networks,\nas it can be arbitrarily small.\nThe last term to be bounded in (9) is the minimum average approximation error ε∗\navg at each AVI\niteration k. Recalling that the task-averaged approximation error is defined as in (7), applying\nTheorem 5.3 by Farahmand (2011) we obtain:\nLemma 4. Let Q∗\nt,k, ∀t ∈{1, . . . , T} be the minimizers of ε∗\navg,k, ˇtk = arg maxt∈{1,...,T }∥Q∗\nt,k+1 −\nT ∗\nt Qt,k∥2\nν, and bk,i = ∥Qˇtk,i+1 −T ∗\nˇt Qˇtk,i∥ν, then:\nε∗\navg,k ≤\n \n∥Q∗\nˇtk,k+1 −(T ∗\nˇt )k+1Qˇtk,0∥ν +\nk−1\nX\ni=0\n(γˇtkCAE(ν; ˇtk, P))i+1bk,k−1−i\n!2\n,\n(10)\nwith CAE defined as in Farahmand (2011).\nFinal remarks\nThe bound for MTRL is derived by composing the results in Theorems 2 and 3, and\nLemma 4. The results above highlight the advantage of learning a shared representation. The bound\nin Theorem 2 shows that a small approximation error is critical to improve the convergence towards\nthe optimal action-value function, and the bound in Theorem 3 shows that the cost of learning the\nshared representation at each AVI iteration is mitigated by using multiple tasks. This is particularly\nbeneficial when the feature representation is complex, e.g. deep neural networks.\n3.4\nDISCUSSION\nAs stated in the remarks of Equation (9), the benefit of MTRL is evinced by the second component\nof the bound, i.e. the cost of learning h, which vanishes with the increase of the number of tasks.\nObviously, adding more tasks require the shared representation to be large enough to include all\nof them, undesirably causing the term suph,w∥h(w(¯X))∥in the fourth component of the bound to\nincrease. This introduces a tradeoff between the number of features and number of tasks; however, for\n5\nPublished as a conference paper at ICLR 2020\nhh\nw1\nw1\nw2\nw2\nwT\nwT\nf1\nf1\nf2\nf2\nfT\nfT\nInput\nOutput\nx1\nx2\nxT\ny1\ny2\nyT\n.\n.\n.\n.\n.\n.\n.\n.\n(a) Shared network\n0\n25\n50\n# Iterations\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50\nQ *\nQ\nK\nFQI\nMULTI\n0\n25\n50\n# Iterations\n0.05\n0.00\n0.05\n0.10\n0.15\nPerformance\n(b) FQI vs MFQI\n0\n25\n50\n# Iterations\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50\nQ *\nQ\nK\n1\n2\n4\n8\n(c) #Task analysis\nFigure 1: (a) The architecture of the neural network we propose to learn T tasks simultaneously.\nThe wt block maps each input xt from task µt to a shared set of layers h which extracts a common\nrepresentation of the tasks. Eventually, the shared representation is specialized in block ft and the\noutput yt of the network is computed. Note that each block can be composed of arbitrarily many\nlayers. (b) Results of FQI and MFQI averaged over 4 tasks in Car-On-Hill, showing ∥Q∗−QπK∥on\nthe left, and the discounted cumulative reward on the right. (c) Results of MFQI showing ∥Q∗−QπK∥\nfor increasing number of tasks. Both results in (b) and (c) are averaged over 100 experiments, and\nshow the 95% confidence intervals.\na reasonable number of tasks the number of features used in the single-task case is enough to handle\nthem, as we show in some experiments in Section 5. Notably, since the AVI/API framework provided\nby Farahmand (2011) provides an easy way to include the approximation error of a generic function\napproximator, it is easy to show the benefit in MTRL of the bound in Equation (9). Despite being just\nmulti-task extensions of previous works, our results are the first one to theoretically show the benefit\nof sharing representation in MTRL. Moreover, they serve as a significant theoretical motivation,\nbesides to the intuitive ones, of the practical algorithms that we describe in the following sections.\n4\nSHARING REPRESENTATIONS\nWe want to empirically evaluate the benefit of our theoretical study in the problem of jointly learning\nT different tasks µt, introducing a neural network architecture for which our bounds hold. Following\nour theoretical framework, the network we propose extracts representations wt from inputs xt for each\ntask µt, mapping them to common features in a set of shared layers h, specializing the learning of\neach task in respective separated layers ft, and finally computing the output yt = (ft ◦h ◦wt)(xt) =\nft(h(wt(xt))) (Figure 1(a)). The idea behind this architecture is not new in the literature. For\ninstance, similar ideas have already been used in DQN variants to improve exploration on the same\ntask via bootstrapping (Osband et al., 2016) and to perform MTRL (Liu et al., 2016).\nThe intuitive and desirable property of this architecture is the exploitation of the regularization effect\nintroduced by the shared representation of the jointly learned tasks. Indeed, unlike learning a single\ntask that may end up in overfitting, forcing the model to compute a shared representation of the tasks\nhelps the regression process to extract more general features, with a consequent reduction in the\nvariance of the learned function. This intuitive justification for our approach, joins the theoretical\nbenefit proven in Section 3. Note that our architecture can be used in any MTRL problem involving a\nregression process; indeed, it can be easily used in value-based methods as a Q-function regressor,\nor in policy search as a policy regressor. In both cases, the targets are learned for each task µt\nin its respective output block ft. Remarkably, as we show in the experimental Section 5, it is\nstraightforward to extend RL algorithms to their multi-task variants only through the use of the\nproposed network architecture, without major changes to the algorithms themselves.\n5\nEXPERIMENTAL RESULTS\nTo empirically evince the effect described by our bounds, we propose an extension of FQI (Ernst\net al., 2005; Riedmiller, 2005), that we call MFQI, for which our AVI bounds apply. Then, to\nempirically evaluate our approach in challenging RL problems, we introduce multi-task variants\nof two well-known DRL algorithms: DQN (Mnih et al., 2015) and DDPG (Lillicrap et al., 2015),\nwhich we call Multi Deep Q-Network (MDQN) and Multi Deep Deterministic Policy Gradient\n(MDDPG) respectively. Note that for these methodologies, our AVI and API bounds hold only with\n6\nPublished as a conference paper at ICLR 2020\n0\n25\n50\n#Epochs\n20\n40\n60\n80\nPerformance\nCart-Pole\n0\n25\n50\n#Epochs\n100\n90\n80\n70\n60\nAcrobot\n0\n25\n50\n#Epochs\n100\n95\n90\n85\n80\n75\n70\n65\nMountain-Car\n0\n25\n50\n#Epochs\n0.0\n0.1\n0.2\n0.3\n0.4\nCar-On-Hill\n0\n25\n50\n#Epochs\n0.6\n0.4\n0.2\n0.0\nInverted-Pendulum\nDQN\nMULTI\n(a) Multi-task\n0\n25\n50\n#Epochs\n100\n90\n80\n70\n60\nPerformance\nAcrobot\nNo initialization\nUnfreeze-0\nUnfreeze-10\nNo unfreeze\n(b) Transfer\nFigure 2: Discounted cumulative reward averaged over 100 experiments of DQN and MDQN for\neach task and for transfer learning in the Acrobot problem. An epoch consists of 1, 000 steps, after\nwhich the greedy policy is evaluated for 2, 000 steps. The 95% confidence intervals are shown.\nthe simplifying assumption that the samples are i.i.d.; nevertheless they are useful to show the benefit\nof our method also in complex scenarios, e.g. MuJoCo (Todorov et al., 2012). We remark that in\nthese experiments we are only interested in showing the benefit of learning multiple tasks with a\nshared representation w.r.t. learning a single task; therefore, we only compare our methods with\nthe single task counterparts, ignoring other works on MTRL in literature. Experiments have been\ndeveloped using the MushroomRL library (D’Eramo et al., 2020), and run on an NVIDIA® DGX\nStation™and Intel® AI DevCloud. Refer to Appendix B for all the details and our motivations about\nthe experimental settings.\n5.1\nMULTI FITTED Q-ITERATION\nAs a first empirical evaluation, we consider FQI, as an example of an AVI algorithm, to show the\neffect described by our theoretical AVI bounds in experiments. We consider the Car-On-Hill problem\nas described in Ernst et al. (2005), and select four different tasks from it changing the mass of the\ncar and the value of the actions (details in Appendix B). Then, we run separate instances of FQI\nwith a single task network for each task respectively, and one of MFQI considering all the tasks\nsimultaneously. Figure 1(b) shows the L1-norm of the difference between Q∗and QπK averaged\nover all the tasks. It is clear how MFQI is able to get much closer to the optimal Q-function, thus\ngiving an empirical evidence of the AVI bounds in Theorem 2. For completeness, we also show the\nadvantage of MFQI w.r.t. FQI in performance. Then, in Figure 1(c) we provide an empirical evidence\nof the benefit of increasing the number of tasks in MFQI in terms of both quality and stability.\n5.2\nMULTI DEEP Q-NETWORK\nAs in Liu et al. (2016), our MDQN uses separate replay memories for each task and the batch\nused in each training step is built picking the same number of samples from each replay memory.\nFurthermore, a step of the algorithm consists of exactly one step in each task. These are the only\nminor changes to the vanilla DQN algorithm we introduce, while all other aspects, such as the use of\nthe target network, are not modified. Thus, the time complexity of MDQN is considerably lower than\nvanilla DQN thanks to the learning of T tasks with a single model, but at the cost of a higher memory\ncomplexity for the collection of samples for each task. We consider five problems with similar\nstate spaces, sparse rewards and discrete actions: Cart-Pole, Acrobot, Mountain-Car, Car-On-Hill,\nand Inverted-Pendulum. The implementation of the first three problems is the one provided by the\nOpenAI Gym library Brockman et al. (2016), while Car-On-Hill is described in Ernst et al. (2005)\nand Inverted-Pendulum in Lagoudakis & Parr (2003).\nFigure 2(a) shows the performance of MDQN w.r.t. to vanilla DQN that uses a single-task network\nstructured as the multi-task one in the case with T = 1. The first three plots from the left show good\nperformance of MDQN, which is both higher and more stable than DQN. In Car-On-Hill, MDQN is\nslightly slower than DQN to reach the best performance, but eventually manages to be more stable.\nFinally, the Inverted-Pendulum experiment is clearly too easy to solve for both approaches, but it is\nstill useful for the shared feature extraction in MDQN. The described results provide important hints\nabout the better quality of the features extracted by MDQN w.r.t. DQN. To further demonstrate this,\nwe evaluate the performance of DQN on Acrobot, arguably the hardest of the five problems, using\na single-task network with the shared parameters in h initialized with the weights of a multi-task\n7\nPublished as a conference paper at ICLR 2020\n0\n50\n100\n#Epochs\n20\n30\n40\n50\n60\n70\n80\n90\n100\nPerformance\nInverted-Pendulum\nDDPG\nMULTI\n0\n50\n100\n#Epochs\n100\n200\n300\n400\n500\n600\n700\n800\nInverted-Double-Pendulum\n0\n50\n100\n#Epochs\n100\n80\n60\n40\n20\n0\nInverted-Pendulum-Swingup\n(a) Multi-task for pendulums\n0\n50\n100\n#Epochs\n200\n400\n600\n800\nPerformance\nInverted-Double-Pendulum\nNo initialization\nUnfreeze-0\nNo unfreeze\n(b) Transfer for pendulums\n0\n50\n100\n#Epochs\n0\n5\n10\n15\n20\n25\n30\n35\nPerformance\nHopper\n0\n50\n100\n#Epochs\n0\n10\n20\n30\n40\n50\n60\n70\nWalker\n0\n50\n100\n#Epochs\n0\n5\n10\n15\n20\n25\n30\n35\n40\nHalf-Cheetah\nDDPG\nMULTI\n(c) Multi-task for walkers\n0\n50\n100\n#Epochs\n0\n10\n20\n30\n40\nPerformance\nHopper\nNo initialization\nUnfreeze-0\nNo unfreeze\n(d) Transfer for walkers\nFigure 3: Discounted cumulative reward averaged over 40 experiments of DDPG and MDDPG for\neach task and for transfer learning in the Inverted-Double-Pendulum and Hopper problems. An\nepoch consists of 10, 000 steps, after which the greedy policy is evaluated for 5, 000 steps. The 95%\nconfidence intervals are shown.\nnetwork trained with MDQN on the other four problems. Arbitrarily, the pre-trained weights can be\nadjusted during the learning of the new task or can be kept fixed and only the remaining randomly\ninitialized parameters in w and f are trained. From Figure 2(b), the advantages of initializing the\nweights are clear. In particular, we compare the performance of DQN without initialization w.r.t.\nDQN with initialization in three settings: in Unfreeze-0 the initialized weights are adjusted, in No-\nUnfreeze they are kept fixed, and in Unfreeze-10 they are kept fixed until epoch 10 after which they\nstart to be optimized. Interestingly, keeping the shared weights fixed shows a significant performance\nimprovement in the earliest epochs, but ceases to improve soon. On the other hand, the adjustment of\nweights from the earliest epochs shows improvements only compared to the uninitialized network\nin the intermediate stages of learning. The best results are achieved by starting to adjust the shared\nweights after epoch 10, which is approximately the point at which the improvement given by the\nfixed initialization starts to lessen.\n5.3\nMULTI DEEP DETERMINISTIC POLICY GRADIENT\nIn order to show how the flexibility of our approach easily allows to perform MTRL in policy search\nalgorithms, we propose MDDPG as a multi-task variant of DDPG. As an actor-critic method, DDPG\nrequires an actor network and a critic network. Intuitively, to obtain MDDPG both the actor and critic\nnetworks should be built following our proposed structure. We perform separate experiments on two\nsets of MuJoCo Todorov et al. (2012) problems with similar continuous state and action spaces: the\nfirst set includes Inverted-Pendulum, Inverted-Double-Pendulum, and Inverted-Pendulum-Swingup as\nimplemented in the pybullet library, whereas the second set includes Hopper-Stand, Walker-Walk,\nand Half-Cheetah-Run as implemented in the DeepMind Control SuiteTassa et al. (2018). Figure 3(a)\nshows a relevant improvement of MDDPG w.r.t. DDPG in the pendulum tasks. Indeed, while in\nInverted-Pendulum, which is the easiest problem among the three, the performance of MDDPG is\nonly slightly better than DDPG, the difference in the other two problems is significant. The advantage\nof MDDPG is confirmed in Figure 3(c) where it performs better than DDPG in Hopper and equally\ngood in the other two tasks. Again, we perform a TL evaluation of DDPG in the problems where\nit suffers the most, by initializing the shared weights of a single-task network with the ones of a\nmulti-task network trained with MDDPG on the other problems. Figures 3(b) and 3(d) show evident\nadvantages of pre-training the shared weights and a significant difference between keeping them fixed\nor not.\n8\nPublished as a conference paper at ICLR 2020\n6\nRELATED WORKS\nOur work is inspired from both theoretical and empirical studies in MTL and MTRL literature. In\nparticular, the theoretical analysis we provide follows previous results about the theoretical properties\nof multi-task algorithms. For instance, Cavallanti et al. (2010) and Maurer (2006) prove the theoretical\nadvantages of MTL based on linear approximation. More in detail, Maurer (2006) derives bounds on\nMTL when a linear approximator is used to extract a shared representation among tasks. Then, Maurer\net al. (2016), which we considered in this work, describes similar results that extend to the use of\nnon-linear approximators. Similar studies have been conducted in the context of MTRL. Among the\nothers, Lazaric & Restelli (2011) and Brunskill & Li (2013) give theoretical proofs of the advantage\nof learning from multiple MDPs and introduces new algorithms to empirically support their claims,\nas done in this work.\nGenerally, contributions in MTRL assume that properties of different tasks, e.g. dynamics and reward\nfunction, are generated from a common generative model. About this, interesting analyses consider\nBayesian approaches; for instance Wilson et al. (2007) assumes that the tasks are generated from a\nhierarchical Bayesian model, and likewise Lazaric & Ghavamzadeh (2010) considers the case when\nthe value functions are generated from a common prior distribution. Similar considerations, which\nhowever does not use a Bayesian approach, are implicitly made in Taylor et al. (2007), Lazaric et al.\n(2008), and also in this work.\nIn recent years, the advantages of MTRL have been empirically evinced also in DRL, especially\nexploiting the powerful representational capacity of deep neural networks. For instance, Parisotto\net al. (2015) and Rusu et al. (2015) propose to derive a multi-task policy from the policies learned by\nDQN experts trained separately on different tasks. Rusu et al. (2015) compares to a therein introduced\nvariant of DQN, which is very similar to our MDQN and the one in Liu et al. (2016), showing how\ntheir method overcomes it in the Atari benchmark Bellemare et al. (2013). Further developments,\nextend the analysis to policy search (Yang et al., 2017; Teh et al., 2017), and to multi-goal RL (Schaul\net al., 2015; Andrychowicz et al., 2017). Finally, Hessel et al. (2018) addresses the problem of\nbalancing the learning of multiple tasks with a single deep neural network proposing a method that\nuniformly adapts the impact of each task on the training updates of the agent.\n7\nCONCLUSION\nWe have theoretically proved the advantage in RL of using a shared representation to learn multiple\ntasks w.r.t. learning a single task. We have derived our results extending the AVI/API bounds (Farah-\nmand, 2011) to MTRL, leveraging the upper bounds on the approximation error in MTL provided\nin Maurer et al. (2016). The results of this analysis show that the error propagation during the\nAVI/API iterations is reduced according to the number of tasks. Then, we proposed a practical way of\nexploiting this theoretical benefit which consists in an effective way of extracting shared representa-\ntions of multiple tasks by means of deep neural networks. To empirically show the advantages of our\nmethod, we carried out experiments on challenging RL problems with the introduction of multi-task\nextensions of FQI, DQN, and DDPG based on the neural network structure we proposed. As desired,\nthe favorable empirical results confirm the theoretical benefit we described.\n9\nPublished as a conference paper at ICLR 2020\nACKNOWLEDGMENTS\nThis project has received funding from the European Union’s Horizon 2020 research and innovation\nprogramme under grant agreement No. #640554 (SKILLS4ROBOTS) and No. #713010 (GOAL-\nRobots). This project has also been supported by grants from NVIDIA, the NVIDIA DGX Station,\nand the Intel® AI DevCloud. The authors thank Alberto Maria Metelli, Andrea Tirinzoni and Matteo\nPapini for their helpful insights during the development of the project.\nREFERENCES\nMarcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob\nMcGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay.\nIn Advances in Neural Information Processing Systems, pp. 5048–5058, 2017.\nJonathan Baxter. A model of inductive bias learning. Journal of Artificial Intelligence Research, 12:\n149–198, 2000.\nMarc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environ-\nment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:\n253–279, 2013.\nRichard Bellman. The theory of dynamic programming. Technical report, RAND Corp Santa Monica\nCA, 1954.\nGreg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and\nWojciech Zaremba. Openai gym, 2016.\nEmma Brunskill and Lihong Li. Sample complexity of multi-task reinforcement learning. In\nProceedings of the Twenty-Ninth Conference on Uncertainty in Artificial Intelligence, 2013.\nRich Caruana. Multitask learning. Machine learning, 28(1):41–75, 1997.\nGiovanni Cavallanti, Nicolo Cesa-Bianchi, and Claudio Gentile. Linear algorithms for online\nmultitask classification. Journal of Machine Learning Research, 11(Oct):2901–2934, 2010.\nCarlo D’Eramo, Davide Tateo, Andrea Bonarini, Marcello Restelli, and Jan Peters. Mushroomrl:\nSimplifying reinforcement learning research. arXiv:2001.01102, 2020.\nDamien Ernst, Pierre Geurts, and Louis Wehenkel. Tree-based batch mode reinforcement learning.\nJournal of Machine Learning Research, 6(Apr):503–556, 2005.\nAmir-massoud Farahmand. Regularization in reinforcement learning. 2011.\nMatteo Hessel, Hubert Soyer, Lasse Espeholt, Wojciech Czarnecki, Simon Schmitt, and Hado van\nHasselt. Multi-task deep reinforcement learning with popart. arXiv:1809.04474, 2018.\nIrina Higgins, Arka Pal, Andrei Rusu, Loic Matthey, Christopher Burgess, Alexander Pritzel, Matthew\nBotvinick, Charles Blundell, and Alexander Lerchner. Darla: Improving zero-shot transfer in\nreinforcement learning. In International Conference on Machine Learning, pp. 1480–1490, 2017.\nMichail G Lagoudakis and Ronald Parr. Least-squares policy iteration. Journal of machine learning\nresearch, 4(Dec):1107–1149, 2003.\nAlessandro Lazaric. Transfer in reinforcement learning: a framework and a survey. In Reinforcement\nLearning, pp. 143–173. Springer, 2012.\nAlessandro Lazaric and Mohammad Ghavamzadeh. Bayesian multi-task reinforcement learning. In\nICML-27th International Conference on Machine Learning, pp. 599–606. Omnipress, 2010.\nAlessandro Lazaric and Marcello Restelli. Transfer from multiple mdps. In Advances in Neural\nInformation Processing Systems, pp. 1746–1754, 2011.\nAlessandro Lazaric, Marcello Restelli, and Andrea Bonarini. Transfer of samples in batch rein-\nforcement learning. In Proceedings of the 25th international conference on Machine learning, pp.\n544–551. ACM, 2008.\n10\nPublished as a conference paper at ICLR 2020\nTimothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,\nDavid Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv\npreprint arXiv:1509.02971, 2015.\nLydia Liu, Urun Dogan, and Katja Hofmann. Decoding multitask dqn in the world of minecraft. In\nEuropean Workshop on Reinforcement Learning, 2016.\nAndreas Maurer. Bounds for linear multi-task learning. Journal of Machine Learning Research, 7\n(Jan):117–139, 2006.\nAndreas Maurer. A chain rule for the expected suprema of gaussian processes. Theoretical Computer\nScience, 650:109–122, 2016.\nAndreas Maurer, Massimiliano Pontil, and Bernardino Romera-Paredes. The benefit of multitask\nrepresentation learning. The Journal of Machine Learning Research, 17(1):2853–2884, 2016.\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare,\nAlex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control\nthrough deep reinforcement learning. Nature, 518(7540):529, 2015.\nIan Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via\nbootstrapped dqn. In Advances in neural information processing systems, pp. 4026–4034, 2016.\nEmilio Parisotto, Jimmy Lei Ba, and Ruslan Salakhutdinov. Actor-mimic: Deep multitask and\ntransfer reinforcement learning. arXiv preprint arXiv:1511.06342, 2015.\nMartin Riedmiller. Neural fitted q iteration–first experiences with a data efficient neural reinforcement\nlearning method. In European Conference on Machine Learning, pp. 317–328. Springer, 2005.\nAndrei A Rusu, Sergio Gomez Colmenarejo, Caglar Gulcehre, Guillaume Desjardins, James Kirk-\npatrick, Razvan Pascanu, Volodymyr Mnih, Koray Kavukcuoglu, and Raia Hadsell.\nPolicy\ndistillation. arXiv preprint arXiv:1511.06295, 2015.\nTom Schaul, Daniel Horgan, Karol Gregor, and David Silver. Universal value function approximators.\nIn International Conference on Machine Learning, pp. 1312–1320, 2015.\nYuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden,\nAbbas Abdolmaleki, Josh Merel, Andrew Lefrancq, Timothy P. Lillicrap, and Martin A. Riedmiller.\nDeepmind control suite. CoRR, abs/1801.00690, 2018.\nMatthew E Taylor and Peter Stone. Transfer learning for reinforcement learning domains: A survey.\nJournal of Machine Learning Research, 10(Jul):1633–1685, 2009.\nMatthew E Taylor, Peter Stone, and Yaxin Liu. Transfer learning via inter-task mappings for temporal\ndifference learning. Journal of Machine Learning Research, 8(Sep):2125–2167, 2007.\nYee Teh, Victor Bapst, Wojciech M Czarnecki, John Quan, James Kirkpatrick, Raia Hadsell, Nicolas\nHeess, and Razvan Pascanu. Distral: Robust multitask reinforcement learning. In Advances in\nNeural Information Processing Systems, pp. 4496–4506, 2017.\nSebastian Thrun and Lorien Pratt. Learning to learn. Springer Science & Business Media, 2012.\nEmanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.\nIn 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEE, 2012.\nAaron Wilson, Alan Fern, Soumya Ray, and Prasad Tadepalli. Multi-task reinforcement learning: a\nhierarchical bayesian approach. In Proceedings of the 24th international conference on Machine\nlearning, pp. 1015–1022. ACM, 2007.\nMarkus Wulfmeier, Abbas Abdolmaleki, Roland Hafner, Jost Tobias Springenberg, Michael Neunert,\nTim Hertweck, Thomas Lampe, Noah Siegel, Nicolas Heess, and Martin Riedmiller. Regularized\nhierarchical policies for compositional transfer in robotics. arXiv:1906.11228, 2019.\nZhaoyang Yang, Kathryn E Merrick, Hussein A Abbass, and Lianwen Jin. Multi-task deep reinforce-\nment learning for continuous action control. In IJCAI, pp. 3301–3307, 2017.\n11\nPublished as a conference paper at ICLR 2020\nA\nPROOFS\nA.1\nAPPROXIMATED VALUE-ITERATION BOUNDS\nProof of Theorem 2. We compute the average expected loss across tasks:\n1\nT\nT\nX\nt=1\n∥Q∗\nt −QπK\nt\n∥1,ρ\n≤1\nT\nT\nX\nt=1\n2γt\n(1 −γt)2\n\u0014\ninf\nr∈[0,1] C\n1\n2\nVI,ρ,ν(K; t, r)E\n1\n2 (εt,0, . . . , εt,K−1; t, r) +\n2\n1 −γt\nγK\nt Rmax,t\n\u0015\n≤\n2γ\n(1 −γ)2\n1\nT\nT\nX\nt=1\n\u0014\ninf\nr∈[0,1] C\n1\n2\nVI,ρ,ν(K; t, r)E\n1\n2 (εt,0, . . . , εt,K−1; t, r) +\n2\n1 −γt\nγK\nt Rmax,t\n\u0015\n≤\n2γ\n(1 −γ)2\n\"\n1\nT\nT\nX\nt=1\n\u0012\ninf\nr∈[0,1] C\n1\n2\nVI,ρ,ν(K; t, r)E\n1\n2 (εt,0, . . . , εt,K−1; t, r)\n\u0013\n+\n2\n1 −γ γKRmax,avg\n#\n≤\n2γ\n(1 −γ)2\n\"\ninf\nr∈[0,1]\n1\nT\nT\nX\nt=1\n\u0010\nC\n1\n2\nVI,ρ,ν(K; t, r)E\n1\n2 (εt,0, . . . , εt,K−1; t, r)\n\u0011\n+\n2\n1 −γ γKRmax,avg\n#\n≤\n2γ\n(1 −γ)2\n\"\ninf\nr∈[0,1] C\n1\n2\nVI(K; r) 1\nT\nT\nX\nt=1\n\u0010\nE\n1\n2 (εt,0, . . . , εt,K−1; t, r)\n\u0011\n+\n2\n1 −γ γKRmax,avg\n#\n(11)\nwith γ =\nmax\nt∈{1,...,T }γt, C\n1\n2\nVI(K; r) =\nmax\nt∈{1,...,T }C\n1\n2\nVI,ρ,ν(K; t, r), and Rmax,avg = 1/T PT\nt=1 Rmax,t.\nConsidering the term 1/T PT\nt=1\nh\nE\n1\n2 (εt,0, . . . , εt,K−1; t, r)\ni\n= 1/T PT\nt=1\n\u0010PK−1\nk=0 α2r\nt,kεt,k\n\u0011 1\n2 let\nαk =\n( (1−γ)γK−k−1\n1−γK+1\n0 ≤k < K,\n(1−γ)γK\n1−γK+1\nk = K\n,\nthen we bound\n1\nT\nT\nX\nt=1\n K−1\nX\nk=0\nα2r\nt,kεt,k\n! 1\n2\n≤1\nT\nT\nX\nt=1\n K−1\nX\nk=0\nα2r\nk εt,k\n! 1\n2\n.\nUsing Jensen’s inequality:\n1\nT\nT\nX\nt=1\n K−1\nX\nk=0\nα2r\nk εt,k\n! 1\n2\n≤\n K−1\nX\nk=0\nα2r\nk\n1\nT\nT\nX\nt=1\nεt,k\n! 1\n2\n.\nSo, now we can write (11) as\n1\nT\nT\nX\nt=1\n∥Q∗\nt −QπK\nt\n∥1,ρ ≤\n2γ\n(1 −γ)2\n\u0014\ninf\nr∈[0,1] C\n1\n2\nVI(K; r)E\n1\n2avg(εavg,0, . . . , εavg,K−1; r)\n+\n2\n1 −γ γKRmax,avg\n\u0015\n,\nwith εavg,k = 1/T PT\nt=1 εt,k and Eavg(εavg,0, . . . , εavg,K−1; r) = PK−1\nk=0 α2r\nk εavg,k.\nProof of Lemma 4. Let us start from the definition of optimal task-averaged risk:\nε∗\navg,k = 1\nT\nT\nX\nt=1\n∥Q∗\nt,k+1 −T ∗\nt Qt,k∥2\nν,\n12\nPublished as a conference paper at ICLR 2020\nwhere Q∗\nt,k, with t ∈[1, T], are the minimizers of εavg,k.\nConsider the task ˇt such that\nˇtk = arg\nmax\nt∈{1,...,T }∥Q∗\nt,k+1 −T ∗\nt Qt,k∥2\nν,\nwe can write the following inequality:\nq\nε∗\navg,k ≤∥Q∗\nˇtk,k+1 −T ∗\nˇt Qˇtk,k∥ν.\nBy the application of Theorem 5.3 by Farahmand (2011) to the right hand side, and defining\nbk,i = ∥Qˇtk,i+1 −T ∗\nˇt Qˇtk,i∥ν, we obtain:\nq\nε∗\navg,k ≤∥Q∗\nˇtk,k+1 −(T ∗\nˇt )k+1Qˇtk,0∥ν +\nk−1\nX\ni=0\n(γˇtkCAE(ν; ˇtk, P))i+1bk,k−1−i.\nSquaring both sides yields the result:\nε∗\navg,k ≤\n \n∥Q∗\nˇtk,k+1 −(T ∗\nˇt )k+1Qˇtk,0∥ν +\nk−1\nX\ni=0\n(γˇtkCAE(ν; ˇtk, P))i+1bk,k−1−i\n!2\n.\nA.2\nAPPROXIMATED POLICY-ITERATION BOUNDS\nWe start by considering the bound for the API framework:\nTheorem 5. (Theorem 3.2 of Farahmand (2011)) Let K be a positive integer, and Qmax ≤Rmax\n1−γ . Then\nfor any sequence (Qk)K−1\nk=0 ⊂B(S × A, Qmax) and the corresponding sequence (εk)K−1\nk=0 , where\nεk = ∥Qk −Qπk∥2\nν, we have:\n∥Q∗−QπK∥1,ρ ≤\n2γ\n(1 −γ)2\n\u0014\ninf\nr∈[0,1] C\n1\n2\nPI,ρ,ν(K; r)E\n1\n2 (ε0, . . . , εK−1; r) + γK−1Rmax\n\u0015\n,\n(12)\nwhere\nCPI,ρ,ν(K; r) =\n\u00121 −γ\n2\n\u00132\nsup\nπ′\n0,...,π′\nK\nK−1\nX\nk=0\na2(1−r)\nk\n\nX\nm≥0\nγmcPI1,ρ,ν(K −k −1, m + 1; π′\nk+1)+\nX\nm≥1\nγmcPI2,ρ,ν(K −k −1, m; π′\nk+1, π′\nk) + cPI3,ρ,ν\n\n\n2\n;\n(13)\nwith E(ε0, . . . , εK−1; r) = PK−1\nk=0 α2r\nk εk, the three coefficients cPI1,ρ,ν, cPI2,ρ,ν, cPI3,ρ,ν, the distri-\nbutions ρ and ν, and the series αk are defined as in Farahmand (2011).\nFrom Theorem 5, by computing the average loss across tasks and pushing inside the average using\nJensen’s inequality, we derive the API bounds averaged on multiple tasks.\nTheorem 6. Let K be a positive integer, and Qmax ≤Rmax\n1−γ . Then for any sequence (Qk)K−1\nk=0 ⊂B(S×\nA, Qmax) and the corresponding sequence (εavg,k)K−1\nk=0 , where εavg,k = 1\nT\nPT\nt=1∥Qt,k −Qπk\nt ∥2\nν, we\nhave:\n1\nT\nT\nX\nt=1\n∥Q∗\nt −QπK\nt\n∥1,ρ ≤\n2γ\n(1 −γ)2\n\u0014\ninf\nr∈[0,1] C\n1\n2\nPI(K; r)E\n1\n2avg(εavg,0, . . . , εavg,K−1; r)\n+γK−1Rmax,avg\n\u0003\n,\n(14)\n13\nPublished as a conference paper at ICLR 2020\nwith Eavg = PK−1\nk=0 α2r\nk εavg,k, γ =\nmax\nt∈{1,...,T }γt, C\n1\n2\nPI(K; r) =\nmax\nt∈{1,...,T }C\n1\n2\nPI,ρ,ν(K; t, r), Rmax,avg =\n1\nT\nPT\nt=1 Rmax,t and αk =\n( (1−γ)γK−k−1\n1−γK+1\n0 ≤k < K,\n(1−γ)γK\n1−γK+1\nk = K\n.\nProof of Theorem 6. The proof is very similar to the one for AVI. We compute the average expected\nloss across tasks:\n1\nT\nT\nX\nt=1\n∥Q∗\nt −QπK\nt\n∥1,ρ\n≤1\nT\nT\nX\nt=1\n2γt\n(1 −γt)2\n\u0014\ninf\nr∈[0,1] C\n1\n2\nPI,ρ,ν(K; t, r)E\n1\n2 (εt,0, . . . , εt,K−1; t, r) + γK−1\nt\nRmax,t\n\u0015\n≤\n2γ\n(1 −γ)2\n1\nT\nT\nX\nt=1\n\u0014\ninf\nr∈[0,1] C\n1\n2\nPI,ρ,ν(K; t, r)E\n1\n2 (εt,0, . . . , εt,K−1; t, r) + γK−1\nt\nRmax,t\n\u0015\n≤\n2γ\n(1 −γ)2\n\"\n1\nT\nT\nX\nt=1\n\u0012\ninf\nr∈[0,1] C\n1\n2\nPI,ρ,ν(K; t, r)E\n1\n2 (εt,0, . . . , εt,K−1; t, r)\n\u0013\n+ γK−1Rmax,avg\n#\n≤\n2γ\n(1 −γ)2\n\"\ninf\nr∈[0,1]\n1\nT\nT\nX\nt=1\n\u0010\nC\n1\n2\nPI,ρ,ν(K; t, r)E\n1\n2 (εt,0, . . . , εt,K−1; t, r)\n\u0011\n+ γK−1Rmax,avg\n#\n≤\n2γ\n(1 −γ)2\n\"\ninf\nr∈[0,1] C\n1\n2\nPI(K; r) 1\nT\nT\nX\nt=1\n\u0010\nE\n1\n2 (εt,0, . . . , εt,K−1; t, r)\n\u0011\n+ γK−1Rmax,avg\n#\n.\n(15)\nUsing Jensen’s inequality as in the AVI scenario, we can write (15) as:\n1\nT\nT\nX\nt=1\n∥Q∗\nt −QπK\nt\n∥1,ρ ≤\n2γ\n(1 −γ)2\n\u0014\ninf\nr∈[0,1] C\n1\n2\nPI(K; r)E\n1\n2avg(εavg,0, . . . , εavg,K−1; r)\n+γK−1Rmax,avg\n\u0003\n,\n(16)\nwith εavg,k = 1/T PT\nt=1 εt,k and Eavg(εavg,0, . . . , εavg,K−1; r) = PK−1\nk=0 α2r\nk εavg,k.\nA.3\nAPPROXIMATION BOUNDS\nProof of Theorem 3. Let w∗\n1, . . . , w∗\nT , h∗and f ∗\n1 , . . . , f ∗\nT be the minimizers of ε∗\navg, then:\nεavg(ˆw, ˆh,ˆf) −ε∗\navg =\n \nεavg(ˆw, ˆh,ˆf) −1\nnT\nX\nti\nℓ( ˆft(ˆh( ˆwt(Xti))), Yti)\n!\n|\n{z\n}\nA\n+\n \n1\nnT\nX\nti\nℓ( ˆft(ˆh( ˆwt(Xti))), Yti) −1\nnT\nX\nti\nℓ(f ∗\nt (h∗(w∗\nt (Xti))), Yti)\n!\n|\n{z\n}\nB\n+\n \n1\nnT\nX\nti\nℓ(f ∗\nt (h∗(w∗\nt (Xti))), Yti) −ε∗\navg\n!\n|\n{z\n}\nC\n.\n(17)\nWe proceed to bound the three components individually:\n• C can be bounded using Hoeffding’s inequality, with probability 1 −δ/2 by\np\nln(2/δ)/(2nT ),\nas it contains only nT random variables bounded in the interval [0, 1];\n14\nPublished as a conference paper at ICLR 2020\n• B can be bounded by 0, by definition of ˆw, ˆh and ˆf, as they are the minimizers of Equa-\ntion (3);\n• the bounding of A is less straightforward and is described in the following.\nWe define the following auxiliary function spaces:\n• W′ = {x ∈X →(wt(xti)) : (w1, . . . , wT ) ∈WT },\n• F′ =\n\b\ny ∈RKT n →(ft(yti)) : (f1, . . . , fT ) ∈FT \t\n,\nand the following auxiliary sets:\n• S =\n\b\n(ℓ(ft(h(wt(Xti))), Yti)) : f ∈FT , h ∈H, w ∈WT \t\n⊆RT n,\n• S′ = F′(H(W′( ¯X))) =\n\b\n(ft(h(wt(Xti)))) : f ∈FT , h ∈H, w ∈WT \t\n⊆RT n,\n• S′′ = H(W′( ¯X)) =\n\b\n(h(wt(Xti))) : h ∈H, w ∈WT \t\n⊆RKT n,\nwhich will be useful in our proof.\nUsing Theorem 9 by Maurer et al. (2016), we can write:\nεavg(ˆw, ˆh,ˆf) −1\nnT\nX\nti\nℓ( ˆft(ˆh( ˆwt(Xti))), Yti)\n≤\nsup\nw∈WT ,h∈H,f∈FT\n \nεavg(w, h, f) −1\nnT\nX\nti\nℓ(ft(h(wt(Xti))), Yti)\n!\n≤\n√\n2πG(S)\nnT\n+\ns\n9 ln( 2\nδ )\n2nT\n,\n(18)\nthen by Lipschitz property of the loss function ℓand the contraction lemma Corollary 11 Maurer et al.\n(2016): G(S) ≤G(S′). By Theorem 12 by Maurer et al. (2016), for universal constants c′\n1 and c′\n2:\nG(S′) ≤c′\n1L(F′)G(S′′) + c′\n2D(S′′)O(F′) + min\ny∈Y G(F(y)),\n(19)\nwhere L(F′) is the largest value for the Lipschitz constants in the function space F′, and D(S′′) is\nthe Euclidean diameter of the set S′′.\nUsing Theorem 12 by Maurer et al. (2016) again, for universal constants c′′\n1 and c′′\n2:\nG(S′′) ≤c′′\n1L(H)G(W′( ¯X)) + c′′\n2D(W′( ¯X))O(H) + min\np∈P G(H(p)).\n(20)\nPutting (19) and (20) together:\nG(S′) ≤c′\n1L(F′)\n\u0012\nc′′\n1L(H)G(W′( ¯X)) + c′′\n2D(W′( ¯X))O(H) + min\np∈P G(H(p))\n\u0013\n+ c′\n2D(S′′)O(F′) + min\ny∈Y G(F(y))\n= c′\n1c′′\n1L(F′)L(H)G(W′(¯X)) + c′\n1c′′\n2L(F′)D(W′(¯X))O(H) + c′\n1L(F′) min\np∈P G(H(p))\n+ c′\n2D(S′′)O(F′) + min\ny∈Y G(F(y)).\n(21)\nAt this point, we have to bound the individual terms in the right hand side of (21), following the same\nprocedure proposed by Maurer et al. (2016).\n15\nPublished as a conference paper at ICLR 2020\nFirstly, to bound L(F′), let y, y′ ∈RKT n, where y = (yti) with yti ∈RK and y′ = (y′\nti) with\ny′\nti ∈RK. We can write the following:\n∥f(y) −f(y′)∥2 =\nX\nti\n(ft(yti) −ft(y′\nti))2\n≤L(F)2 X\nti\n∥yti −y′\nti∥2\n= L(F)2∥y −y′∥2,\n(22)\nwhence L(F′) ≤L(F).\nThen, we bound:\nG(W′( ¯X)) = E\n\"\nsup\nw∈WT\nX\nkti\nγktiwtk(Xti)\n\f\f\f\f\fXti\n#\n≤\nX\nt\nsup\nl∈{1,...,T }\nE\n\"\nsup\nw∈W\nX\nki\nγkliwk(Xli)\n\f\f\f\f\fXli\n#\n= T\nsup\nl∈{1,...,T }\nG(W(Xl)).\n(23)\nThen, since it is possible to bound the Euclidean diameter using the norm of the supremum value in\nthe set, we bound D(S′′) ≤2 suph,w∥h(w( ¯X))∥and D(W′( ¯X)) ≤2 supw∈WT ∥w( ¯X)∥.\nAlso, we bound O(F′):\nE\n\u0014\nsup\ng∈F′⟨γ, g(y) −g(y′)⟩\n\u0015\n= E\n\"\nsup\nf∈FT\nX\nti\nγti (ft(yti) −ft(y′\nti))\n#\n=\nX\nt\nE\n\"\nsup\nf∈F\nX\ni\nγi (f(yti) −f(y′\nti))\n#\n≤\n√\nT\n\nX\nt\nE\n\"\nsup\nf∈F\nX\ni\nγi (f(yti) −f(y′\nti))\n#2\n\n1\n2\n≤\n√\nT\n X\nt\nO(F)2 X\ni\n∥yti −y′\nti∥2\n! 1\n2\n=\n√\nTO(F)∥y −y′∥,\n(24)\nwhence O(F′) ≤\n√\nTO(F).\nTo minimize the last term, it is possible to choose y0 = 0, as f(0) = 0, ∀f ∈F, resulting in\nminy∈Y G(F(y)) = G(F(0)) = 0.\nThen, substituting in (21), and recalling that G(S) ≤G(S′):\nG(S) ≤c′\n1c′′\n1L(F)L(H)T\nsup\nl∈{1,...,T }\nG(W(Xl)) + 2c′\n1c′′\n2L(F) sup\nw∈WT∥w( ¯X)∥O(H)\n+ c′\n1L(F) min\np∈P G(H(p)) + 2c′\n2 sup\nh,w\n∥h(w( ¯X))∥\n√\nTO(F).\n(25)\n16\nPublished as a conference paper at ICLR 2020\nNow, the first term A of (17) can be bounded substituting (25) in (18):\nεavg(ˆw, ˆh,ˆf) −1\nnT\nX\nti\nℓ( ˆft(ˆh( ˆwt(Xti))), Yti)\n≤\n√\n2π\nnT\n\u0010\nc′\n1c′′\n1L(F)L(H)T\nsup\nl∈{1,...,T }\nG(W(Xl)) + 2c′\n1c′′\n2L(F) sup\nw∈WT∥w( ¯X)∥O(H)\n+ c′\n1L(F) min\np∈P G(H(p)) + 2c′\n2 sup\nh,w\n∥h(w( ¯X))∥\n√\nTO(F)\n\u0011\n+\ns\n9 ln( 2\nδ )\n2nT\n= c1\nL(F)L(H) supl∈{1,...,T } G(W(Xl))\nn\n+ c2\nsupw∥w( ¯X)∥L(F)O(H)\nnT\n+ c3\nL(F) minp∈P G(H(p))\nnT\n+ c4\nsuph,w∥h(w( ¯X))∥O(F)\nn\n√\nT\n+\ns\n9 ln( 2\nδ )\n2nT\n.\nA union bound between A, B and C of (17) completes the proof:\nεavg(ˆw, ˆh,ˆf) −ε∗\navg ≤c1\nL(F)L(H) supl∈{1,...,T } G(W(Xl))\nn\n+ c2\nsupw∥w( ¯X)∥L(F)O(H)\nnT\n+ c3\nL(F) minp∈P G(H(p))\nnT\n+ c4\nsuph,w∥h(w( ¯X))∥O(F)\nn\n√\nT\n+\ns\n8 ln( 3\nδ )\nnT\n.\nB\nADDITIONAL DETAILS OF EMPIRICAL EVALUATION\nB.1\nMULTI FITTED Q-ITERATION\nWe consider Car-On-Hill problem with discount factor 0.95 and horizon 100. Running Adam\noptimizer with learning rate 0.001 and using a mean squared loss, we train a neural network composed\nof 2 shared layers of 30 neurons each, with sigmoidal activation function, as described in Riedmiller\n(2005). We select 8 tasks for the problem changing the mass of the car m and the value of the\ndiscrete actions a (Table 1). Figure 1(b) is computed considering the first four tasks, while Figure 1(c)\nconsiders task 1 in the result with 1 task, tasks 1 and 2 for the result with 2 tasks, tasks 1, 2, 3, and 4\nfor the result with 4 tasks, and all the tasks for the result with 8 tasks. To run FQI and MFQI, for each\ntask we collect transitions running an extra-tree trained following the procedure and setting in Ernst\net al. (2005), using an ϵ-greedy policy with ϵ = 0.1, to obtain a small, but representative dataset. The\noptimal Q-function for each task is computed by tree-search3 for 100 states uniformly picked from\nthe state space, and the 2 discrete actions, for a total of 200 state-action tuples.\nB.2\nMULTI DEEP Q-NETWORK\nThe five problems we consider for this experiment are: Cart-Pole, Acrobot, Mountain-Car, Car-On-\nHill, and Inverted-Pendulum4. The discount factors are respectively 0.99, 0.99, 0.99, 0.95, and 0.95.\nThe horizons are respectively 500, 1, 000, 1, 000, 100, and 3, 000. The network we use consists of 80\nReLu units for each wt, t ∈{1, . . . , T} block, with T = 5. Then, the shared block h consists of one\n3We follow the method described in Ernst et al. (2005).\n4The IDs of the problems in the OpenAI Gym library are: CartPole-v0, Acrobot-v1, and MountainCar-v0.\n17\nPublished as a conference paper at ICLR 2020\nTask\nMass\nAction set\n1\n1.0\n{−4.0; 4.0}\n2\n0.8\n{−4.0; 4.0}\n3\n1.0\n{−4.5; 4.5}\n4\n1.2\n{−4.5; 4.5}\n5\n1.0\n{−4.125; 4.125}\n6\n1.0\n{−4.25; 4.25}\n7\n0.8\n{−4.375; 4.375}\n8\n0.85\n{−4.0; 4.0}\nTable 1: Different values of the mass of the car and available actions chosen for the Car-On-Hill tasks\nin the MFQI empirical evaluation.\nlayer with 80 ReLu units and another one with 80 sigmoid units. Eventually, each ft has a number of\nlinear units equal to the number of discrete actions a(t)\ni , i ∈{1, . . . , #A(t)} of task µt which outputs\nthe action-value Qt(s, a(t)\ni ) = yt(s, a(t)\ni ) = ft(h(wt(s)), a(t)\ni ), ∀s ∈S(t). The use of sigmoid units\nin the second layer of h is due to our choice to extract meaningful shared features bounded between 0\nand 1 to be used as input of the last linear layer, as in most RL approaches. In practice, we have also\nfound that sigmoid units help to reduce task interference in multi-task networks, where instead the\nlinear response of ReLu units cause a problematic increase in the feature values. Furthermore, the use\nof a bounded feature space reduces the suph,w∥h(w( ¯X))∥term in the upper bound of Theorem 3,\ncorresponding to the upper bound of the diameter of the feature space, as shown in Appendix A.\nThe initial replay memory size for each task is 100 and the maximum size is 5, 000. We use Huber\nloss with Adam optimizer using learning rate 10−3 and batch size of 100 samples for each task. The\ntarget network is updated every 100 steps. The exploration is ε-greedy with ε linearly decaying from\n1 to 0.01 in the first 5, 000 steps.\nB.3\nMULTI DEEP DETERMINISTIC POLICY GRADIENT\nThe two set of problems we consider for this experiment are: one including Inverted-Pendulum,\nInverted-Double-Pendulum, and Inverted-Pendulum-Swingup, and another one including Hopper-\nStand, Walker-Walk, and Half-Cheetah-Run5. The discount factors are 0.99 and the horizons are\n1, 000 for all problems. The actor network is composed of 600 ReLu units for each wt, t ∈{1, . . . , T}\nblock, with T = 3. The shared block h has 500 units with ReLu activation function as for MDQN.\nFinally, each ft has a number of tanh units equal to the number of dimensions of the continuous\nactions a(t) ∈A(t) of task µt which outputs the policy πt(s) = yt(s) = ft(h(wt(s))), ∀s ∈S(t).\nOn the other hand, the critic network consists of the same wt units of the actor, except for the use of\nsigmoidal units in the h layer, as in MDQN. In addition to this, the actions a(t) are given as input to\nh. Finally, each ft has a single linear unit Qt(s, a(t)) = yt(s, a(t)) = ft(h(wt(s), a(t))), ∀s ∈S(t).\nThe initial replay memory size for each task is 64 and the maximum size is 50, 000. We use Huber\nloss to update the critic network and the policy gradient to update the actor network. In both cases\nthe optimization is performed with Adam optimizer and batch size of 64 samples for each task. The\nlearning rate of the actor is 10−4 and the learning rate of the critic is 10−3. Moreover, we apply\nℓ2-penalization to the critic network using a regularization coefficient of 0.01. The target networks are\nupdated with soft-updates using τ = 10−3. The exploration is performed using the action computed\nby the actor network adding a noise generated with an Ornstein-Uhlenbeck process with θ = 0.15\nand σ = 0.2. Note that most of these values are taken from the original DDPG paper Lillicrap et al.\n(2015), which optimizes them for the single-task scenario.\n5The\nIDs\nof\nthe\nproblems\nin\nthe\npybullet\nlibrary\nare:\nInvertedPendulumBulletEnv-v0,\nInvertedDoublePendulumBulletEnv-v0, and InvertedPendulumSwingupBulletEnv-v0.\nThe names of the\ndomain and the task of the problems in the DeepMind Control Suite are: hopper-stand, walker-walk, and\ncheetah-run.\n18\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2024-01-17",
  "updated": "2024-01-17"
}