{
  "id": "http://arxiv.org/abs/2401.10857v1",
  "title": "Motion Consistency Loss for Monocular Visual Odometry with Attention-Based Deep Learning",
  "authors": [
    "Andr√© O. Fran√ßani",
    "Marcos R. O. A. Maximo"
  ],
  "abstract": "Deep learning algorithms have driven expressive progress in many complex\ntasks. The loss function is a core component of deep learning techniques,\nguiding the learning process of neural networks. This paper contributes by\nintroducing a consistency loss for visual odometry with deep learning-based\napproaches. The motion consistency loss explores repeated motions that appear\nin consecutive overlapped video clips. Experimental results show that our\napproach increased the performance of a model on the KITTI odometry benchmark.",
  "text": "Copyright ¬© 2023 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including\nreprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any\ncopyrighted component of this work in other works. Published in: Proceedings of the 2023 Latin American Robotics Symposium (LARS), 2023 Brazilian Symposium on Robotics\n(SBR), and 2023 Workshop on Robotics in Education (WRE).\nMotion Consistency Loss for Monocular Visual\nOdometry with Attention-Based Deep Learning\n1st Andr¬¥e O. Franc¬∏ani\nAutonomous Computational Systems Lab (LAB-SCA)\nComputer Science Division\nAeronautics Institute of Technology\nSÀúao Jos¬¥e dos Campos, SP, Brazil\nandre.francani@ga.ita.br\n2nd Marcos R. O. A. Maximo\nAutonomous Computational Systems Lab (LAB-SCA)\nComputer Science Division\nAeronautics Institute of Technology\nSÀúao Jos¬¥e dos Campos, SP, Brazil\nmmaximo@ita.br\nAbstract‚ÄîDeep learning algorithms have driven expressive\nprogress in many complex tasks. The loss function is a core\ncomponent of deep learning techniques, guiding the learning pro-\ncess of neural networks. This paper contributes by introducing a\nconsistency loss for visual odometry with deep learning-based ap-\nproaches. The motion consistency loss explores repeated motions\nthat appear in consecutive overlapped video clips. Experimental\nresults show that our approach increased the performance of a\nmodel on the KITTI odometry benchmark.\nIndex\nTerms‚Äîdeep\nlearning,\nloss\nfunction,\ntransformer,\nmonocular visual odometry\nI. INTRODUCTION\nDeep learning (DL) techniques have shown to be state-\nof-the-art in diverse complex applications, such as computer\nvision (CV) and natural language processing (NLP). As ev-\nidence, DL models can be employed in image classification\ntasks [1], video surveillance [2], text translation [3], and others\n[4].\nA fundamental component of deep learning techniques is\nthe loss function, being a core element in the optimization\nalgorithm. The loss function is minimized during the training\nstep and the model‚Äôs parameters are adjusted according to\nthe error between the model‚Äôs prediction and the expected\nground truth. Different loss functions guide the training to\na specific task, tuning the model‚Äôs parameters to a good\nperformance in regression or classification tasks. This means\nthat the choice of the loss function impacts what the model is\nin fact learning. The focal loss [5] is a clear example where\nthe loss function played an important role in object detection\napplications, enabling inferences as fast as one-stage detectors\nwhile reaching accuracy competitive to two-stage detectors.\nA consistency loss may introduce a gain of information\ncapable of making the model more robust and more reliable,\nincreasing the performance. Thus, a consistency loss may\nAndr¬¥e Franc¬∏ani was supported by CAPES ‚Äì Coordination of Improvement\nof Higher Education Personnel under grant 88887.687888/2022-00. Marcos\nMaximo is partially funded by CNPq ‚Äî National Research Council of Brazil\nthrough grant 307525/2022-8.\n979-8-3503-1538-7/23/$31.00¬©2023 IEEE\nguide the learning process to be more accurate in its predic-\ntions regarding a particular task, reducing the error between the\npredicted output and the expected ground truth. Zhu et al. [6]\nachieved outstanding qualitative results in generative models\nby introducing a cycle consistency loss in the image-to-image\ntranslation task.\nIn this paper, our contribution is a novel consistency loss\nfunction that deals with repeated motion in overlapped clips in\nthe context of monocular visual odometry with deep learning.\nWe evaluate our method on the KITTI odometry benchmark\n[7], showing that our approach increases the performance of\nthe model.\nThe remaining of this paper is organized as follows. Sec-\ntion II provides theoretical background about visual odometry\nand attention mechanisms. Section III introduces the proposed\nmotion consistency loss. Section IV describes the experimental\nsetups. Section V shows the experimental results on the KITTI\nodometry benchmark. Finally, Section VI concludes and shares\nour ideas for future work.\nII. BACKGROUND\nA. Monocular visual odometry\nVisual odometry (VO) is widely applied in robotics and\nautonomous vehicles to estimate the camera‚Äôs pose given a\nsequence of image frames [8]. The motion between consec-\nutive time steps k ‚àí1 and k comprises a rotation matrix\nRk ‚ààSO(3) and a translation vector tk ‚ààR3√ó1. Note that Rk\nand tk, respectively, depict the rotation and translation from\ntime step k ‚àí1 to k. The complete motion can be written as\na transformation Tk ‚ààR4√ó4, defined as\nTk =\n\u0014Rk\ntk\n0\n1\n\u0015\n.\nFurthermore, the poses in VO problems are typically de-\nscribed through 6 degrees of freedom (6-DoF): three rota-\ntional, and three translational. To address the visual odometry\nproblem of estimating the 6-DoF, one can rely on traditional\ngeometry-based approaches [9], [10], deep learning-based\napproaches through end-to-end architectures [11], [12], and\nhybrid methods that mix deep learning-based and geometry-\nbased algorithms depending on the component of the visual\narXiv:2401.10857v1  [cs.CV]  19 Jan 2024\nodometry [13], [14]. Fig. 1 illustrates the camera‚Äôs motion\nbetween consecutive time steps. In Fig. 1, the blue dots are\nkeypoints in the scenario, which are used as a reference to\nestimate the motion via a geometry-based method.\n‡Øû\nùëò‚àí1\nùëò\nFig. 1. Camera‚Äôs motion between consecutive time steps.\nIn the monocular visual odometry case, where the image\nframes are captured from a single camera, the motion can\nbe obtained up to a scale, resulting in scale ambiguity. This\nis caused by the loss of depth information when projecting\nthree-dimensional (3D) objects onto the two-dimensional (2D)\nimage space. The accumulation of scale errors over time is\ncalled scale drift, being a crucial factor in decreasing the\naccuracy in monocular visual odometry systems [8].\nB. Time-space attention in monocular visual odometry\nDeep learning-based methods are able to estimate the 6-\nDoF camera‚Äôs pose directly from RGB images. In an end-\nto-end manner, the models estimate the pose given only the\nsequence of frames as input. DeepVO [11] is an example of a\ndeep learning-based method that uses a convolutional neural\nnetwork (CNN) to extract features from consecutive image\nframes, as well as long short-term memory cells (LSTM) to\ndeal with the temporal information. Furthermore, unsuper-\nvised learning techniques can also be employed, as in the\nUnDeepVO [12] that uses depth estimation to recover the\nscale.\nFor many years, LSTM cells have dominated tasks involving\ntemporal information. However, architectures based on atten-\ntion mechanisms, such as the Transformer architecture [3],\nhave outperformed the accuracy of LSTM models, and have\nbecome state-of-the-art in NLP applications.\nThe input of the attention block is composed of the query,\nkey, and value vectors, all with dimension d. Then, those\nvectors from different inputs are packed together into the\nmatrices Q, K, and V, which are the inputs of the attention\nlayer. The processing steps can be defined as the Attention\nfunction [3]:\nAttention(Q, K, V) = Softmax\n\u0012QKT\n‚àö\nd\n\u0013\nV,\n(1)\nwhere Softmax is the well-known Softmax function.\nAnother important structure is the multi-head self-attention\nlayer (MHSA), which has Nh concatenated attention layers\nrunning in parallel [3]. Since then, the deep learning com-\nmunity has started building models based on the decoder and\nencoder blocks of the Transformer, and new state-of-the-art\nnetworks have begun to emerge, such as BERT [15] and\nGPT [16]. The Transformer architecture also achieved state-\nof-the-art performance in image recognition benchmarks [1]\nand video understanding tasks [17], [18]. For vision tasks, the\nimages are decomposed into N non-overlapping patches of\nsize P √óP. Each patch is flattened and embedded into tokens\nthat are sent as input to the network [1].\nVideo understanding tasks require space and time informa-\ntion to recognize actions or events in videos. To address this\nproblem with the attention mechanism, G. Bertasius et al. [18]\nproposed mainly two different self-attention architectures to\nextract spatio-temporal features. One of the proposed archi-\ntectures is the ‚Äújoint space-time‚Äù self-attention, which relates\nall tokens in space and time together. The other one, more\nefficient computationally in terms of processing complexity,\nis the ‚Äúdivided space-time‚Äù self-attention. It applies first the\nattention over tokens with the same spatial index (temporal\naxis) followed by the attention over the tokens from the same\nframe (spatial axis).\nIn the context of visual odometry, our previous work [19]\nuses a Transformer-based network with the ‚Äúdivided space-\ntime‚Äù to estimate the 6-DoF camera‚Äôs pose for the monocular\ncase. The TSformer-VO is based on the TimeSformer archi-\ntecture with ‚Äúdivided space-time‚Äù self-attention in its encoding\nblocks. The model receives clips with Nf frames of size\nC √ó H √ó W as input, where C, H, and W are the number\nof channels, the height, and the width of the image frames,\nrespectively. Each clip is divided into N = HW/P 2 patches,\nand each patch is embedded into tokens through a linear\nmap. Finally, the outputs of the embedding layer are the input\ntokens denoted as zl\n(s,t) ‚ààREd, where Ed is the embedding\ndimension of the flattened patch at spatial location s, time\nindex t, and encoding block l. Notice that the model has Lx\nencoder blocks stacked, indicating the depth of the network in\nterms of encoding layers.\nThe encoder blocks follow the traditional layers present in\nthe Transformers encoders, namely the layer normalization\n(LN), multi-head self-attention (MHSA), residual connections,\nfully-connected layer (FC), and multilayer perceptron (MLP).\nFor the specific ‚Äúdivided space-time‚Äù self-attention, those\nlayers are related as follows:\nal\nt = MHSA\n\u0010\nLN\n\u0010\nzl‚àí1\n(s,t)\n\u0011\u0011\n+ zl‚àí1\n(s,t),\nal\ntF C = FC\n\u0000al\nt\n\u0001\n,\nal\ns = MHSA\n\u0000LN\n\u0000al\ntF C\n\u0001\u0001\n+ al\ntF C,\nzl\n(s,t) = MLP\n\u0000LN\n\u0000al\ns\n\u0001\u0001\n+ al\ns.\n(2)\nFurther details about the exact computation of the query, key,\nand value can be found in [3], [18]. Fig. 2 contains the block\ndiagram of the encoder block with ‚Äúdivided space-time‚Äù self-\nattention, summarizing (2).\nIII. PROPOSED METHOD\nThe proposed method relies on providing extra information\nduring the training step to increase overall performance. The\nMLP\nTime\nself-attention\nFC\nSpace\nself-attention\nLN\n+\nLN\nMHSA\nq\nk\nv\n+\nSelf-attention\nTransformer encoder block\nFig. 2. Encoder block with ‚Äúdivided space-time‚Äù self-attention.\ninformation gain comes from the fact that we are sampling\nconsecutive clips with Nf ‚àí1 overlapped frames. This means\nthat the same motion is estimated from different input clips,\nas shown in Fig. 3. With this in mind, we can conduct the\nlearning process of the network by introducing a consistency\nloss. The idea behind our approach is that consecutive clips\nshould estimate the same motion where they overlap, that is,\nconsecutive clips contain common motions due to the overlap-\nping introduced by sampling the clips. Without the consistency\nloss, it is not directly clear to the network that consecutive\nclips contain similar information, once our approach uses the\ninformation of the entire clip to infer all the poses in a pair\nof consecutive frames.\nThe main part of the total loss function is the mean squared\nerror (MSE) between all predicted motions and their ground\ntruths, given by:\nLMSE =\n1\nNf ‚àí1\nNf ‚àí1\nX\nw=1\n‚à•yk\nw ‚àíÀÜyk\nw‚à•2\n2,\n(3)\nwhere ‚à•¬∑ ‚à•2\n2 is the squared L2 norm, yk\nw is the flattened 6-\nDoF relative pose at position w of the clip at instant k, and\nÀÜyk\nw is its estimate by the network. When dealing with batch\nprocessing, the loss is reduced by the mean over all Bs batch\nelements.\nThe second part of the total loss function is our motion\nconsistency loss LMC. It is the sum of the squared errors of\nall predicted motions that appear in more than one input clip,\nthat is, the same motion but from different clips. Therefore,\nthe LMC is defined as:\nLMC =\n2Nf ‚àí5\nX\nj=1\nŒ≥\nX\nn=m+1\nŒª\nX\nm=¬µ\n‚à•ÀÜyk‚àím\nk‚àíj ‚àíÀÜyk‚àín\nk‚àíj ‚à•2\n2,\n(4)\nwhere ¬µ = max (j ‚àíNf + 2, 0), Œª = min (Nf ‚àí3, j ‚àí1),\nand Œ≥ = min (Nf ‚àí2, j).\nThe final loss is the sum of the MSE of all estimations\nand the motion consistency loss, this one weighted by a\nhyperparameter Œ± to measure and balance the influence of the\nconsistency loss in the training process. Therefore, the final\nloss is defined as follows:\nL = LMSE + Œ±LMC.\n(5)\nIV. EXPERIMENTAL SETUP\nThis section contains further information on the model\nconfigurations we tested in our experiments, the training setup,\nthe dataset we employed, as well as the evaluation metrics we\nchose to compare the performance of the models.\nA. Model setup\nThis work aims to study the influence of the motion con-\nsistency loss on the model‚Äôs performance. Therefore, different\narchitectures were not evaluated, and the model hyperparame-\nters were selected based on our previous work [19]. All three\nmodels in Table I have Lx = 12 encoders, each one with\nNh = 6 heads in the MHSA layer. The patch size is P = 16,\nand the embedding dimension is set to Ed = 384. Moreover,\nwe tested our approach for clips with Nf = 3 frames, meaning\nthat consecutive clips have two overlapped frames, i.e. one\nmotion in common.\nWe tested our approach considering different values for\nŒ± to weight the motion consistency loss. Table I shows the\ndefinition of the models according to their loss functions.\nB. Training setup\nWe used a computer with a Intel i9-7900X CPU 3.3GHz\nCPU and a GeForce GTX 1080 Ti GPU with 11GB VRAM\nfor computations, and all the deep learning procedures were\nimplemented with PyTorch 1.10. The models were trained\nfrom scratch using random initialization for 95 epochs with\nAdam optimization. The learning rate was set to 1√ó10‚àí5, and\nall the other hyperparameters of the Adam algorithm were the\ndefault values.\nAiming to recycle the code from our previous work [19],\nwe sampled the clips with a sliding window of size 2Nf\nand stride 1. After that, we shuffled the sampled clips and\ncreated batches of size Bs = 2. This way, clips in the\nsame batch are not consecutive to each other. However, our\napproach requires consecutive clips only for the training step\nto calculate the motion consistency loss. To solve this issue,\nwe split the batches in half, each one containing clips with\nNf frames. The main advantage of this sampling process\nis that we ensure that the clips in the same batch are still\nshuffled while having their consecutive clips in a second batch.\nTABLE I\nDEFINITION OF THE MODELS WITH THEIR CORRESPONDING LOSS\nFUNCTION\nModel\nLoss function\nA\nLMSE\nB\nLMSE + LMC\nC\nLMSE + 10LMC\nMotion\nestimator\nClip 1\nùëá‡¨µ\nùëá‡¨∂\nClip 2\nùëá‡¨∂\nùëá‡¨∑\nùëá‡∑†‡¨µ   ùëá‡∑†‡¨∂\nùëá‡∑†‡¨∂   ùëá‡∑†‡¨∑\n‚Ñí‡ØÜ‡Øå‡Ææ+ ùõº ‚Ñí‡ØÜ‡Æº\nMotion consistency\n‚Ñí‡ØÜ‡Æº(ùëá‡∑†‡¨∂, ùëá‡∑†‡¨∂)\nMSE\n‚Ñí‡ØÜ‡Øå‡Ææ(ùëá‡∑†‡¨µ, ùëá‡∑†‡¨∂, ùëá‡∑†‡¨∂, ùëá‡∑†‡¨∑)\nLoss function\ntraining\nFig. 3.\nSchematic representation of the proposed method. Input clips with overlapped frames go to a model that estimates the camera‚Äôs motion between\nconsecutive time steps. During the training step, the motion consistency loss is calculated using the estimated motions that appear in all input clips ( ÀÜT2 in this\nexample). The motion consistency loss is weighted by a hyperparameter Œ±, and this result is added to the MSE of all estimations. The final loss propagates\nback to the model during the training step.\nAfter that, we concatenated both small batches into only one\nto take advantage of the batch processing ability of deep\nlearning frameworks. Furthermore, this helped us to fit our\nGPU memory capacity.\nC. Dataset\nWe developed our algorithms considering the KITTI dataset\n[7], which is a benchmark for evaluating visual odometry\nalgorithms. The imagery is recorded from a stereo camera on a\nmoving vehicle. The scenario varies from roads and streets on\na normal day in the city, and the car‚Äôs speed varies from 0 to\n90 km/h, challenging visual odometry algorithms at high-speed\nscenarios. To make the system monocular for our study case,\nwe selected only the RGB images recorded by the left camera.\nIn the KITTI dataset, there are 22 different sequences with\ndifferent lengths corresponding to different rides. However, the\ncamera‚Äôs position and orientation over time are only available\nfor 11 of them.\nAll image frames were resized to 192 √ó 640, keeping\nthe aspect ratio of the dataset while making the dimensions\nmultiple of the patch size (P = 16) to divide the frames\ninto patches. Furthermore, sequences 00, 02, 08, and 09 are\nused as training data since they are the four largest recordings.\nConsequently, the test data is composed of sequences 01, 03,\n04, 05, 06, 07, and 10, following the choice in [11].\nD. Evaluation metrics\nWe use the Python KITTI evaluation toolbox to compare\nthe performance of the models. The metrics are defined as\nfollows:\n‚Ä¢ terr: average translational error, given in percentage (%);\n‚Ä¢ rerr: average rotational error, given in degrees per 100\nmeters (¬∫/100 m);\n‚Ä¢ ATE: absolute trajectory error, measured in meters;\n‚Ä¢ RPE: relative pose error for rotation and translation,\nmeasured frame-to-frame in degrees (¬∫) and in meters,\nrespectively for the rotation and the translation.\nhttps://github.com/Huangying-Zhan/kitti-odom-eval\nAccording\nto\n[7],\nthe\naverage\nrotational\nand\ntransla-\ntional errors are computed for all subsequences of length\n(100, 200, . . . , 800) meters.\nPrior works evaluate monocular methods with an optimiza-\ntion transformation to align the predictions with the ground\ntruths [10], [13]. Thus, our results are under the 7-DoF\nalignment due to the scale ambiguity in monocular systems.\nV. EXPERIMENTAL RESULTS\nWe evaluated the performance of the models in Table I using\nthe KITTI dataset. After the training step, we obtained the\nquantitative results present in Table II. For each sequence, the\nbest values of each evaluation metric are highlighted in bold,\nand the second-best values are underlined.\nNote that the results of sequences 00, 02, 08, and 09 should\nnot be considered in the analysis once they are used for\ntraining the models. However, they are shown in Table II for\na complete report, as well as to confirm that the models are\nnot completely overfitted to the training data.\nBy ignoring those sequences, it can be seen that the presence\nof the motion consistency loss in models B and C increased the\noverall performance if compared to model A (only the MSE\nloss). Considering the terr, model B outperforms model A in\nall sequences but in sequences 01 and 03. For the ATE, model\nB showed worse performance than model A only in sequence\n03. However, this metric reveals that the consistency loss led to\na significant improvement, especially in sequences 01, 05, and\n06. Similar conclusions can be made for the rotational metric.\nFor example, the rerr shows that model B outperforms model\nA in all sequences but in sequences 03 and 07. However,\nthe rotational metrics were not significantly affected by the\nconsistency loss, once the metric values are close to each\nother for all the models. The same happens for the relative\npose errors. Furthermore, similar conclusions can be drawn\nfor model C since it is also frequently present in the top 1\nand top 2 best values.\nTherefore, the consistency loss improved the overall per-\nformance when comparing models B and C with model A,\nespecially regarding the translational aspect.\nTABLE II\nQUANTITATIVE RESULTS FOR THE 11 KITTI SEQUENCES WITH GROUND TRUTH.\nSequence\nModel\n00\n01\n02\n03\n04\n05\n06\n07\n08\n09\n10\nA\n9.053\n31.116\n3.105\n9.867\n8.029\n12.800\n27.212\n23.475\n5.498\n5.063\n18.544\nB\n3.609\n41.534\n4.084\n10.869\n6.218\n12.263\n25.554\n25.224\n3.283\n4.099\n12.188\nterr\n(%)\nC\n4.942\n28.324\n5.062\n15.651\n4.316\n13.198\n22.334\n18.300\n5.028\n5.785\n12.602\nA\n2.018\n6.885\n1.063\n4.295\n3.630\n4.472\n9.264\n9.085\n2.264\n1.715\n6.289\nB\n1.433\n6.137\n1.528\n4.397\n3.412\n4.137\n7.766\n9.190\n1.366\n1.351\n5.496\nrerr\n(¬∫/100m)\nC\n2.076\n6.231\n1.660\n7.540\n1.733\n4.157\n6.147\n6.299\n1.977\n1.889\n4.595\nA\n62.718\n218.055\n37.431\n8.696\n3.998\n60.149\n84.704\n35.929\n47.987\n27.426\n26.291\nB\n19.424\n152.154\n80.454\n11.412\n3.971\n51.414\n69.889\n36.430\n14.708\n15.553\n23.749\nATE\n(m)\nC\n48.945\n133.490\n48.249\n15.583\n2.469\n68.946\n73.715\n30.908\n37.251\n27.547\n21.791\nA\n0.107\n0.601\n0.026\n0.112\n0.104\n0.141\n0.366\n0.164\n0.030\n0.037\n0.146\nB\n0.037\n1.060\n0.024\n0.138\n0.113\n0.141\n0.290\n0.178\n0.031\n0.033\n0.150\nRPE\n(m)\nC\n0.038\n0.646\n0.040\n0.130\n0.066\n0.140\n0.307\n0.157\n0.041\n0.031\n0.166\nA\n0.361\n0.320\n0.283\n0.288\n0.170\n0.263\n0.252\n0.289\n0.281\n0.229\n0.324\nB\n0.356\n0.299\n0.277\n0.285\n0.165\n0.262\n0.241\n0.290\n0.274\n0.221\n0.327\nRPE\n(¬∫)\nC\n0.339\n0.296\n0.267\n0.288\n0.148\n0.263\n0.237\n0.284\n0.258\n0.219\n0.321\nFollowing the analysis of the impact of the consistency\nloss in visual odometry, we displayed in Fig. 4 the predicted\ntrajectories of models A, B, and C for qualitative analysis.\nFig. 4 reveals that model B is predominantly closer to the\nground truth than the other models. Moreover, it also reveals\nthat the scale drift problem still remains pronounced, typical\nof monocular systems.\nVI. CONCLUSION\nWe proposed a consistency loss for visual odometry systems\nwith deep learning. Experimental results have demonstrated\nthat the addition of our motion consistency loss increased\nthe overall performance of the model considering the KITTI\ndataset. Our results also showed that the consistency loss was\nmost prominent for the translational metrics, not being highly\neffective in the rotational aspect. Despite an improvement\nin model performance for visual odometry, the scale drift\nproblem still remains significant for monocular systems, which\nis expected since the model is not designed to infer depth\nfeatures to estimate the scale.\nFor future research, we expect to minimize the scale drift\nproblem by addressing a depth estimator together [14] with our\napproach. Furthermore, we are looking for new deep learning-\nbased architectures to estimate the camera‚Äôs pose in an end-\nto-end manner.\nREFERENCES\n[1] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al.,\n‚ÄúAn image is worth 16x16 words: Transformers for image recognition\nat scale,‚Äù arXiv preprint arXiv:2010.11929, 2020.\n[2] D. F. Dos Santos, A. O. Franc¬∏ani, M. R. O. A. Maximo, and A. S. C.\nFerreira, ‚ÄúPerformance comparison of convolutional neural network\nmodels for object detection in tethered balloon imagery,‚Äù in 2021 Latin\nAmerican Robotics Symposium (LARS), 2021 Brazilian Symposium on\nRobotics (SBR), and 2021 Workshop on Robotics in Education (WRE).\nIEEE, 2021, pp. 246‚Äì251.\n[3] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\n≈Å. Kaiser, and I. Polosukhin, ‚ÄúAttention is all you need,‚Äù Advances in\nNeural Information Processing Systems, vol. 30, 2017.\n[4] T. Prangemeier, C. Wildner, A. O. Franc¬∏ani, C. Reich, and H. Koeppl,\n‚ÄúYeast cell segmentation in microstructured environments with deep\nlearning,‚Äù Biosystems, vol. 211, p. 104557, 2022.\n[5] T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Doll¬¥ar, ‚ÄúFocal loss for\ndense object detection,‚Äù in Proc. of the IEEE Int. Conf. on Computer\nVision, 2017, pp. 2980‚Äì2988.\n[6] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, ‚ÄúUnpaired image-to-image\ntranslation using cycle-consistent adversarial networks,‚Äù in Proc. of the\nIEEE Int. Conf. on Computer Vision, 2017, pp. 2223‚Äì2232.\n[7] A. Geiger, P. Lenz, and R. Urtasun, ‚ÄúAre we ready for Autonomous\nDriving? The KITTI Vision Benchmark Suite,‚Äù in Conf. on Computer\nVision and Pattern Recognition (CVPR), 2012.\n[8] D. Scaramuzza and F. Fraundorfer, ‚ÄúVisual odometry [tutorial],‚Äù IEEE\nRobotics & Automation Magazine, vol. 18, no. 4, pp. 80‚Äì92, 2011.\n[9] A. Geiger, J. Ziegler, and C. Stiller, ‚ÄúStereoScan: Dense 3D reconstruc-\ntion in real-time,‚Äù in Intelligent Vehicles Symposium (IV), 2011.\n[10] R. Mur-Artal and J. D. Tard¬¥os, ‚ÄúORB-SLAM2: An open-source SLAM\nsystem for monocular, stereo, and RGB-D cameras,‚Äù IEEE Transactions\non Robotics, vol. 33, no. 5, pp. 1255‚Äì1262, 2017.\n[11] S. Wang, R. Clark, H. Wen, and N. Trigoni, ‚ÄúDeepVO: Towards end-to-\nend visual odometry with deep recurrent convolutional neural networks,‚Äù\nin 2017 IEEE Int. Conf. on Robotics and Automation (ICRA).\nIEEE,\n2017, pp. 2043‚Äì2050.\n[12] R. Li, S. Wang, Z. Long, and D. Gu, ‚ÄúUnDeepVO: Monocular visual\nodometry through unsupervised deep learning,‚Äù in 2018 IEEE Int. Conf.\non Robotics and Automation (ICRA).\nIEEE, 2018, pp. 7286‚Äì7291.\n[13] H. Zhan, C. S. Weerasekera, J.-W. Bian, and I. Reid, ‚ÄúVisual odometry\nrevisited: What should be learnt?‚Äù in 2020 IEEE Int. Conf. on Robotics\nand Automation (ICRA).\nIEEE, 2020, pp. 4203‚Äì4210.\n[14] A. O. Franc¬∏ani and M. R. O. A. Maximo, ‚ÄúDense prediction transformer\nfor scale estimation in monocular visual odometry,‚Äù in 2022 Latin\nAmerican Robotics Symposium (LARS), 2022 Brazilian Symposium on\nRobotics (SBR), and 2022 Workshop on Robotics in Education (WRE),\n2022, pp. 1‚Äì6.\n[15] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‚ÄúBERT: Pre-training\nof deep bidirectional transformers for language understanding,‚Äù arXiv\npreprint arXiv:1810.04805, 2018.\n[16] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever et al., ‚ÄúImproving\nlanguage understanding by generative pre-training,‚Äù 2018.\n[17] A. Arnab, M. Dehghani, G. Heigold, C. Sun, M. LuÀáci¬¥c, and C. Schmid,\n‚ÄúViViT: A video vision transformer,‚Äù in Proc. of the IEEE/CVF Int.\nConf. on Computer Vision (ICCV), October 2021, pp. 6836‚Äì6846.\n[18] G. Bertasius, H. Wang, and L. Torresani, ‚ÄúIs space-time attention all you\nneed for video understanding?‚Äù in Proc. of the Int. Conf. on Machine\nLearning (ICML), July 2021.\n[19] A. O. Franc¬∏ani and M. R. O. A. Maximo, ‚ÄúTransformer-based model\nfor monocular visual odometry: a video understanding approach,‚Äù arXiv\npreprint arXiv:2305.06121, 2023.\n‚àí250\n0\n250\n500\n750\n1000\n1250\n1500\n1750\nx (m)\n‚àí1400\n‚àí1200\n‚àí1000\n‚àí800\n‚àí600\n‚àí400\n‚àí200\n0\n200\nz (m)\nSeq. 01\nGround truth\nModel A\nModel B\nModel C\n0\n100\n200\n300\n400\nx (m)\n0\n50\n100\n150\n200\nz (m)\nSeq. 03\nGround truth\nModel A\nModel B\nModel C\n‚àí40\n‚àí30\n‚àí20\n‚àí10\n0\n10\n20\n30\n40\nx (m)\n0\n50\n100\n150\n200\n250\n300\n350\n400\nz (m)\nSeq. 04\nGround truth\nModel A\nModel B\nModel C\n‚àí200\n‚àí100\n0\n100\n200\nx (m)\n‚àí100\n0\n100\n200\n300\n400\nz (m)\nSeq. 05\nGround truth\nModel A\nModel B\nModel C\n‚àí100\n‚àí50\n0\n50\n100\nx (m)\n‚àí100\n0\n100\n200\n300\nz (m)\nSeq. 06\nGround truth\nModel A\nModel B\nModel C\n‚àí150\n‚àí100\n‚àí50\n0\nx (m)\n‚àí100\n‚àí50\n0\n50\n100\nz (m)\nSeq. 07\nGround truth\nModel A\nModel B\nModel C\n‚àí100\n0\n100\n200\n300\nx (m)\n0\n100\n200\n300\n400\n500\nz (m)\nSeq. 09\nGround truth\nModel A\nModel B\nModel C\n0\n100\n200\n300\n400\n500\n600\n700\nx (m)\n‚àí50\n0\n50\n100\n150\nz (m)\nSeq. 10\nGround truth\nModel A\nModel B\nModel C\nFig. 4. Trajectories obtained by model A(‚ñ†‚ñ†), model B (‚ñ†‚ñ†), and model C (‚ñ†‚ñ†), compared with the ground truth (‚ñ†‚ñ†). All depicted sequences belong to the\ntest set, but sequence 09. Trajectories are obtained under the 7-DoF alignment.\n",
  "categories": [
    "cs.CV",
    "cs.RO",
    "68T45 68T07"
  ],
  "published": "2024-01-19",
  "updated": "2024-01-19"
}