{
  "id": "http://arxiv.org/abs/2401.11255v1",
  "title": "Visualization Generation with Large Language Models: An Evaluation",
  "authors": [
    "Guozheng Li",
    "Xinyu Wang",
    "Gerile Aodeng",
    "Shunyuan Zheng",
    "Yu Zhang",
    "Chuangxin Ou",
    "Song Wang",
    "Chi Harold Liu"
  ],
  "abstract": "Analysts frequently need to create visualizations in the data analysis\nprocess to obtain and communicate insights. To reduce the burden of creating\nvisualizations, previous research has developed various approaches for analysts\nto create visualizations from natural language queries. Recent studies have\ndemonstrated the capabilities of large language models in natural language\nunderstanding and code generation tasks. The capabilities imply the potential\nof using large language models to generate visualization specifications from\nnatural language queries. In this paper, we evaluate the capability of a large\nlanguage model to generate visualization specifications on the task of natural\nlanguage to visualization (NL2VIS). More specifically, we have opted for\nGPT-3.5 and Vega-Lite to represent large language models and visualization\nspecifications, respectively. The evaluation is conducted on the nvBench\ndataset. In the evaluation, we utilize both zero-shot and few-shot prompt\nstrategies. The results demonstrate that GPT-3.5 surpasses previous NL2VIS\napproaches. Additionally, the performance of few-shot prompts is higher than\nthat of zero-shot prompts. We discuss the limitations of GPT-3.5 on NL2VIS,\nsuch as misunderstanding the data attributes and grammar errors in generated\nspecifications. We also summarized several directions, such as correcting the\nground truth and reducing the ambiguities in natural language queries, to\nimprove the NL2VIS benchmark.",
  "text": "1\nVisualization Generation with Large Language\nModels: An Evaluation\nGuozheng Li, Xinyu Wang, Gerile Aodeng, Shunyuan Zheng, Yu Zhang, Chuangxin Ou,\nSong Wang, and Chi Harold Liu\nAbstract—Analysts frequently need to create visualizations in the data analysis process to obtain and communicate insights. To\nreduce the burden of creating visualizations, previous research has developed various approaches for analysts to create visualizations\nfrom natural language queries. Recent studies have demonstrated the capabilities of large language models in natural language\nunderstanding and code generation tasks. The abilities imply the potential of using large language models to generate visualization\nspecifications from natural language queries. In this paper, we evaluate the capability of a large language model to generate\nvisualization specifications on the task of natural language to visualization (NL2VIS). More specifically, we have opted for GPT-3.5 and\nVega-Lite to represent large language models and visualization specifications respectively. The evaluation is conducted on the\nnvBench dataset. In the evaluation, we utilize both zero-shot and few-shot prompt strategies. The results demonstrate that GPT-3.5\nsurpasses previous NL2VIS approaches. Additionally, the performance of few-shot prompts is higher than that of zero-shot prompts.\nWe discuss the limitations of GPT-3.5 on NL2VIS, such as misunderstanding the data attributes and grammar errors in generated\nspecifications. We also summarized several directions, such as correcting the ground truth and reducing the ambiguities in natural\nlanguage queries, to improve the NL2VIS benchmark.\nIndex Terms—Visualization generation, large language model, evaluation, declarative grammar\n✦\n1\nINTRODUCTION\nData visualization is vital in data analysis to help find\npatterns and communicate insights [1]. However, creating\nvisualizations requires knowledge of visualization design\nprinciples and skills of visualization authoring tools [2],\n[3]. One significant direction to make analysts concentrate\non the data analysis itself is automating the data visual-\nization creation based on users’ natural language queries.\nTherefore, a surge of visualization-oriented natural lan-\nguage interfaces emerged, allowing data analysts to create\nvisualizations simply by merely using natural language and\npromoting data analysis efficiency..\nThe development of the natural language processing\n(NLP) technique continuously improves the applicability of\nthe natural language to visualization (NL2VIS) approach.\nMany studies about NL2VIS are based on the libraries of\nnatural language processing, such as Articulate [4] using\nthe Stanford Parser [5], DeepEye [6] using OpenNLP [7],\nand NL4DV [8] using CoreNLP [9]. They either have con-\nstraints on user inputs or cannot understand complex nat-\nural language queries [10]. Researchers further train neural\nnetworks using deep-learning-based approaches [11], [12] to\nprocess complex natural languages. However, a single deep-\nlearning-based approach cannot perform well on various\ntasks.\n•\nGuozheng Li, Xinyu Wang, Gerile Aodeng, Shunyuan Zheng, and Chi\nHarold Liu are with Beijing Institute of Technology. E-mail: {guozheng.li,\nwang.xinyu, gerile, zsysoft, chiliu}@bit.edu.cn.\n•\nYu Zhang (corresponding author) is with the University of Oxford. Email:\nyuzhang94@outlook.com.\n•\nChuangxin Ou and Song Wang are with PICC Information Technology\nCompany Limited.\nRecently, an increasing number of large language models\n(LLMs, e.g., GPT-3.5 [13]) have been developed [14]. LLMs\npossess a remarkable capability to comprehend natural lan-\nguages and generate high-quality responses in user-defined\nformats or programming codes. They have demonstrated\noutstanding proficiency in various generative tasks, encom-\npassing code generation [15], reasoning [16], and mathemat-\nics [17]. Moreover, many research studies have evaluated\nthe capabilities of LLMs from different aspects with various\nprompt strategies like Chain of Thoughts [18], Program\nof Thoughts [19], and Least to Most [20]. Therefore, it is\nalso feasible to leverage LLMs to realize NL2VIS tasks, and\nseveral LLM-based systems have been developed, such as\nChat2VIS [21] and LIDA [22], which generate Python code\nto construct data visualizations. Maddigan et al. [23] con-\nduct an evaluation to verify the capability of the Chat2VIS\nsystem on the NL2VIS task.\nAlong with Python, domain-specific languages in JSON\nformats are now widely used for specifying visualizations\nin various applications [24]. Furthermore, the existing stud-\nies [23] have not evaluated the impact of different prompt\nconstruction strategies on the NL2VIS task. Despite the\nsubstantial progress made in LLMs and LLM-based NL2VIS\napproaches, assessing the capacity of LLMs using different\nprompt strategies in visualization generation is a critical\nrequirement. More specifically, evaluating LLMs for NL2VIS\ntasks is a fundamental step that informs their practical\nutility, drives improvements, and ensures responsible and\ninformed deployment in various applications.\nIn this paper, we conduct an evaluation to validate the\ncapability of LLMs for the NL2VIS task. We select the pop-\nular Vega-Lite visualization grammar [25] as the generative\ntarget. We first summarize existing prompt strategies and\narXiv:2401.11255v1  [cs.HC]  20 Jan 2024\n2\nselect zero-shot and few-shot strategies, which are suitable\nfor the NL2VIS task to design prompts. The prompts consist\nof three main components: the role definitions instructing\nLLMs, the sampled data table demonstrating the schema,\nand the query describing users’ tasks, including expected\nchart types and data attributes to be visualized. For the zero-\nshot prompt, we design several static rules to instruct LLMs\nto revise errors we find in the preliminary evaluation and\ngenerate correct Vega-Lite specifications. For the few-shot\nprompt, we add several examples to demonstrate the high-\nquality Vega-Lite specifications for different chart types.\nEach example consists of a query and its corresponding\nspecification. We select GPT-3.5 as the representative of\nLLMs to conduct the evaluation. In the evaluation, we use\nthe nvBench dataset [26] as the benchmark and examine\nthe performances of zero-shot and few-shot prompt strate-\ngies, respectively. For the evaluation results, we compute\nthe matching accuracy of visualization results between the\nground truth in nvBench and the outputs of GPT-3.5. We\ntake the matching accuracy as the metric and verify the\npotential of LLMs on the NL2VIS task.\nThe evaluation results demonstrate that the performance\nof the GPT-3.5 model surpasses existing studies in the Vega-\nLite generation task with few-shot prompts. In addition,\nthe performance of few-shot prompts is significantly higher\nthan that of zero-shot prompts. We also categorize and\ndiscuss the limitations of GPT-3.5 and the questions in the\nexisting benchmark itself as our findings. First, GPT-3.5 still\ncannot fully master the Vega-Lite grammar, and some out-\nputs violate the grammar and make mistakes. In addition,\nGPT-3.5 generates incorrect results compared with ground\ntruths because it cannot fully understand the meaning of\nattributes in the data table. Second, several cases need to\nbe correct in the existing benchmark. More specifically, the\nvisualization results of some cases do not conform to the\ntask description, and the textual queries of some cases have\nambiguities. The limitations in both aspects can negatively\nimpact the performance of LLMs in the NL2VIS task, which\nwe attempt to promote in future work.\nIn summary, this paper has the following contributions:\n• We evaluate the capability of LLMs for the NL2VIS task\nbased on vega-lite specifications using various prompt\nstrategies. The evaluation results on LLMs based on\nNVBench reveal that GPT-3.5 has a powerful capability\nfor generating Vega-Lite specifications.\n• Our evaluation results on LLMs demonstrate the insights\nand directions for improving the NL2VIS benchmark.\n2\nRELATED WORK\nWe review the literature on natural language to visualization\nand large language model capability evaluation to motivate\nour research.\n2.1\nNatural Language to Visualization\nThis section reviews the existing studies about NL2VIS,\nclassifying them into rule-based and neural network-based\ncategories. Since this work is to evaluate the LLM capability\nfor the NL2VIS task, we differentiate the LLM-based algo-\nrithms as the third category.\nRule-based methods. The NL2VIS studies in the early\nstage always adopt rule-based algorithms to realize natu-\nral language comprehension and users’ intent extraction.\nCox et al. [27] propose a prototypical framework to use a\nnatural language interface to generate data visualizations\nautomatically. However, the prototype system they devel-\noped exerts limitations on the queries and can only generate\ndata tables and bar charts. Articulate [4] and DataTone [28]\nuse the Stanford Parser library [5] to analyze the natural\nlanguage query and convert it to appropriate visualizations.\nIn particular, DataTone adds a customized widget to tackle\nthe ambiguity that exists in the natural language query\ncompared with Articulate. Eviza [29] adopts an ANTLR\nparser [30], strengthens the expressiveness of queries and\ncan support users in interacting with data visualizations\nthrough natural languages. DeepEye [6] uses OpenNLP [7]\nto parse an underspecified query consisting of keywords\nand rank multiple candidate visualizations to be selected.\nFlowSense [31] utilizes SEMPRE [32] and CoreNLP [9]\nto realize semantic parsing and allows users to leverage\nthe advantages of dataflow visualization systems for data\nanalysis by natural language interactions. NL4DV [8] also\nadopts CoreNLP [9] to extract data attributes from the input\nquery and decide the predefined tasks according to the\nquery, and is interface-agnostic compared with previous\napproaches. Although rule-based algorithms are relatively\neasy to apply, they have constraints on the natural language\ninputs or cannot comprehend complex queries. Thus, their\nperformances are surpassed by subsequent neural network-\nbased techniques.\nNeural network-based methods. As deep learning tech-\nniques develop and are widely applied in NLP, the vi-\nsualization community starts to explore neural network-\nbased approaches. Liu et al. propose ADVISor [11], a deep-\nlearning-based approach combined with pre-defined rules\nto generate visualizations for tabular data. After inputting\na data table plus a natural language query, users can\nobtain a visualization chart with annotations highlighting\nwhat users care about. Luo et al. [26] propose a synthe-\nsizer to utilize large-scale NL2SQL benchmarks available\nto synthesize new NL2VIS benchmarks, named nvBench,\nto promote the advancement of this field. This benchmark\ncontains about 25,000 (NL, VIS) pairs and covers over a\nhundred domains, and its overall high quality is verified\nthrough expert and crowdsourcing evaluation. They adopt\nthe nvBench benchmark to train a new seq2seq model\nncNet [12], which inputs natural language queries and\ndatasets and outputs a Vega-Lite specification. The ncNet\ncan also accept an optional chart template input to allow\nusers to state the expected chart type explicitly. Song et\nal. [33] are inspired by the code development process and\ndialogue systems and propose RGVisNet, which is a fusion\nof retrieval-based and generation-based methodologies. The\nexperiments show that the performance of RGVisNet sur-\npasses previous NL2VIS approaches. Chen et al. [34] com-\nbine the program synthesis and the BERT-based [35] NLP\ntechnique to generate visualizations from natural language\nqueries and make a corresponding system Graphy. Their\nevaluation using the NLVCorpus [36] dataset shows that\nthe performance of their approach is better than previous\nrule-based and transformer-based approaches.\n3\nLLM-based methods. With the recent emergence of\nLLMs, some studies have attempted to utilize LLMs for\nNL2VIS. Maddigan et al. [21] leverage LLMs to generate\nPython code to visualize data. They develop the Chat2VIS\nsystem to enable users to input a dataset and their analysis\nintentions through natural language, then the system con-\nverts queries to an appropriate prompt containing data table\ndescriptions and queries in natural language and finally\ngives a desired chart rendered by generated Python code.\nFurthermore, they improve their Chat2VIS system with\nadvanced features [23]. Then, the Chat2VIS system can take\nmultilingual natural language as input and allows users\nto iteratively refine their visualizations based on LLM’s\nability to understand long conversations. LIDA [22] defines\nautomated visualization generation as a four-stage task. It\ncombines LLMs and image generation models to resolve\ndatasets and analysis goals and can generate both charts\nand infographics. The researcher introduces two metrics to\nevaluate the generated visualization: visualization error rate\nand self-evaluated visualization quality. Wang et al. [37] pro-\npose the Data Formulator, a system allowing users to input\nnatural languages or instances to implement complex data\ntransformation with the help of LLMs. With this paradigm,\nusers can be freed from complex and time-consuming data\nprocessing. Wang et al. [38] leverage ChatGPT for the visu-\nalization recommendation to reduce the burden of collecting\nlarge training data. Their approach can additionally gener-\nate human-level explanations for the recommended visual-\nizations, and experiments show the performance is better\nthan or at least similar to previous approaches like Decision\nTree. Ko et al. [39] collect a real-world Vega-Lite corpus\nand propose an LLM-based framework to automatically\ngenerate natural language datasets that have rich syntactic\ndiversity and strong semantic relevance with original charts,\ntherefore contributing to the advancement of NL4VIS.\nThe NL2VIS task based on LLMs has recently attracted\nattention. Current research predominantly centers on LLM-\nbased methods for Python code generation, leaving a void\nin exploring visualization generation techniques grounded\nin declarative grammar. Moreover, there needs to be more\ninvestigation into the efficiency of various prompt strategies\nfor visualization generation. Within the context of LLMs,\nthere is a conspicuous absence of a performance baseline.\nWe aim to address these gaps and enhance the comprehen-\nsion of these aspects.\n2.2\nLarge Language Model Capability Evaluation\nLLMs have recently showcased remarkable capabilities\nacross a spectrum of natural language processing tasks.\nHowever, the quantitative performance of LLMs in different\nspecific tasks is unclear, which may be unbeneficial for users\nto take advantage of LLMs. Assessing these models’ actual\nquality, capabilities, and limitations is necessary to assist\nusers in utilizing LLMs more effectively. Previous studies\nhave systematically evaluated the capabilities of LLMs from\ndifferent perspectives, such as code generation, reasoning,\nand mathematics, but there are still gaps to be filled in the\nvisualization task.\nCode generation. Converting natural language to code\nusually includes automatic code generation, translation,\ncode completion. The generated code needs to meet mul-\ntiple standards, such as functional requirements, grammat-\nical correctness, and coding style, making the evaluation\na complex task. Hendrycks et al. [15] introduce APPS, a\nnew code generation benchmark to systematically evaluate\nthe capability of LLMs to generate Python code directly\nfrom natural language specifications. Moreover, Cassano et\nal. [40] build the first multilingual code generation bench-\nmark, MultiPL-E, providing significant experience for un-\nderstanding and improving multilingual language model\nmodels in code generation tasks. Furthermore, Liu et al. [41]\npropose a framework to expand the evaluation dataset. The\nframework uses a generator to obtain a considerable test\nset to cover different code paths and edge cases, thereby\nrigorously evaluating the accuracy of LLM-generated code.\nBased on the benchmark, Ding et al. [42] present a static\ncode-completion evaluation framework. It analyzes the er-\nrors made by pre-trained LLMs on a real-world Python\nevaluation set and identifies common static errors and the\ntrends of their occurrence frequency.\nReasoning. The reasoning task is an essential challenge\nfor LLMs. This type of task requires the model to not only\nunderstand the meaning of the question but also infer logi-\ncally and causally to generate responses with a logical struc-\nture and correct answers. Liu et al. [16] evaluate the logical\nreasoning capabilities of two language models developed by\nOpenAI, ChatGPT and GPT-4, finding that GPT-4 performs\nbetter than ChatGPT on most benchmarks, but its perfor-\nmance drops on logical reasoning natural language infer-\nence tasks. Fu et al. [43] introduce an open-source evaluation\nfor measuring LLMs’ inference performance. The current\nresults indicate that the model size and inference capability\nare generally related, showing an approximately log-linear\ntrend. Xu et al. [44] comprehensively evaluate the logical\nreasoning capability of LLMs by selecting datasets with\ndifferent reasoning forms, multiple representative LLMs,\ndifferent sample settings, refined evaluation indicators, and\ndifferent error types, finally showing a refined overall eval-\nuation framework.\nMathematics. Evaluating the mathematical capabilities\nof LLMs is an important task that requires specific datasets\nof mathematical problems, including algebra, geometry,\nprobability, and statistics. The mathematical reasoning capa-\nbility usually requires models to have a deep understanding\nof mathematical concepts rather than just pattern matching.\nTherefore, the evaluation of mathematical reasoning capa-\nbility is comprehensive and challenging. On the one hand,\nmany studies have built various datasets for evaluating the\ncapabilities of LLMs. Wei et al. [45] build a new Chinese\nmathematics textual dataset CMATH, for evaluating the\narithmetic and reasoning capabilities of LLMs. Yuan et\nal. [17] propose an arithmetic dataset called MATH 401,\nwhich focuses on evaluating the arithmetic capability of\nLLMs. Frieder et al. [46] construct a new natural language\nmathematics dataset to test different dimensions of LLMs\nin mathematical understanding based on a series of fine-\ngrained performance metrics. On the other hand, some\nstudies develop frameworks for evaluation. Wu et al. [47]\npropose a dialogue framework for chat-based LLMs, which\ncan easily integrate different prompt strategies and tools,\nallowing the model to solve mathematical problems through\n4\ndialogue with user agents. Collins et al. [48] have designed\na light-weight interactive evaluation platform to evaluate\nthe capability of LLMs in assisting mathematical theorem\nproving. Dao et al. [49] examine the overall accuracy of\nChatGPT in Vietnamese college entrance examination math-\nematics questions and specifically analyze the performance\non different difficulty levels and question types, clearly\nrevealing the strengths and weaknesses of ChatGPT.\nVisualization. Evaluating the capabilities of LLMs on\nthe visualization generation task is crucial because the eval-\nuation results assist users in understanding LLMs’ perfor-\nmance and guide further improvements and optimizations.\nMaddigan et al. [23] quantitatively evaluate the perfor-\nmance of the proposed Chat2VIS system [21] using the\nnvBench [26] and NLVCorpus [36] datasets, and the results\nshow their system has a comparable performance against\nprevious NL2VIS approaches. However, Chat2VIS outputs\nPython codes, which is not the mainstream approach for\nvisualization creation. Moreover, they have not fully opti-\nmized their prompts with various existing strategies and\nthe performance cannot present the true potential of LLMs\nin the NL2VIS task. In this paper, we investigate existing\nprompt strategies and leverage the strategies suitable for our\nNL2VIS task to construct different high-quality prompts.\nWe select Vega-Lite [25], which is a popular approach for\nvisualization creation in the visualization community, and\nevaluate the performances of LLMs using few-shot and\nzero-shot prompts to explore the potential of LLMs in the\nNL2VIS task fully.\nIn summary, while extensive studies have been carried\nout on evaluating LLMs, a notable gap still exists in the\ncomprehensive examination of the visualization construc-\ntion task. This work aims to evaluate the performance of\nthe NL2VIS task from the declarative grammar aspect and\nestablish a foundation for enhancing the efficiency of LLM-\nbased visualization generation.\n3\nPROMPTS DESIGN\nIn this section, we first summarize the prompt strategies and\nanalyze whether a strategy is suitable for the NL2VIS task.\nThen, we introduce the evaluated prompts and the iterative\nprocess of these prompts.\n3.1\nPrompt Strategies\nA prompt strategy involves crafting effective prompts or in-\nstructions to interact with LLMs, aiming to elicit the desired\nresponse or information from the model. Reasonable design\nand selection of prompts can enable task customization,\ncontrol model output, prevent misunderstandings, and save\nresources.\nThis section divides the commonly used prompt tech-\nnologies that promote LLMs reasoning for specific tasks or\nsettings into four categories (see Fig. 1) and then analyzes\nwhether each category is suitable for visualization gener-\nation tasks. It is worth emphasizing that different prompt\nstrategies can be combined, and multi-stage prompting pro-\ncesses can be integrated, but does not always need to be.\nFig. 1: A summary of prompt strategies: Each column\ncorresponds to a paper that utilizes or evaluates prompt\nstrategies. Each row corresponds to a prompt strategy. The\nstrategies can be grouped into four categories: chain-of-\nthought and its variants (yellow), rationale engineering\n(blue), problem decomposition (green), and in-context learn-\ning (gray).\n3.1.1\nChain-of-Thought and Its Variant\nIn standard prompting, LLMs are always asked to solve\nintricate multi-step problems in a singular step, while the\nhuman cognitive methodology involves tackling complex\nreasoning problems incrementally. Therefore, Wei et al. [18]\nintroduce the Chain-of-Thought (CoT) methodology, which\nintegrates multiple reasoning path steps before generating\nthe ultimate answer. In this way, LLMs can also show\nthe reasoning process when answering, leading to more\naccurate and explainable results.\nUsing a few-shot approach, CoT requires a specific man-\nual cost to design prompts. Kojima et al. [50] demonstrate\nthat LLMs can serve as zero-shot reasoners by simply ap-\npending the prompt “Let’s think step by step” before each\nresponse. We also explore some step-by-step prompting\nstrategies in the preliminary study and find that they have\nno significant improvements in performance. Therefore, we\nadd several rules about the Vega-Lite grammar to construct\nzero-shot prompts instead.\n3.1.2\nRationale Engineering\nThe original CoT prompt relies on artificially crafted ex-\namples of intermediate reasoning steps and uses greedy\ndecoding to generate a coherent reasoning process. On\nthe contrary, rationale engineering aims to utilize LLM’s\nreasoning capabilities more effectively in the following three\nways.\nRationale refinement refers to providing models with\nmore examples of intermediate steps or optimizing the\nstructure and expression of these examples. Fu et al. [51]\npropose\ncomplexity-based\nconsistency,\nsuggesting\nthat\nprompts with higher reasoning complexity have higher\nperformance on multi-step reasoning tasks. Lu et al. [52]\npropose PROMPTPG, employing policy gradient to strate-\ngically select in-context examples from a subset of training\n5\ndatasets and then construct corresponding prompts for the\nselected ones. We specifically select diverse examples in the\nfew-shot prompt, covering all categories of charts in various\ndifficulty levels, to improve the performance of LLMs.\nRationale exploration aims to select the best reasoning\nchain by discovering more potential reasoning paths. Wang\net al. [53] propose self-consistency decoding strategy to\nperform majority voting among multiple inference paths to\nobtain the final answer. However, the intermediate results\nof the NL2VIS task in this work are Vega-Lite specifications,\nwith a single visualization image potentially corresponding\nto multiple Vega-Lite specifications. Measuring the simi-\nlarity between Vega-Lite codes is challenging, making the\nrationale exploration approach not applicable in our work.\nRationale verification can be introduced to verify the\nvalidity and rationality of the reasoning steps after gener-\nating the reasoning chain. Weng et al. [54] propose a self-\nverification to enable LLMs with the capability of verifying\nanswers. A similar prompt ensembling strategy includes\nDIVERSE [55], a reasoning step validator to filter out in-\ncorrect answers and independently verify each inference\nstep. We do not adopt the rationale verification strategy\nbecause LLMs can only output results in textual form when\nevaluations and the validity and correctness of a Vega-Lite\nspecification cannot be verified without the corresponding\nvisualization image.\n3.1.3\nProblem Decomposition\nComplex tasks often require systematic analysis and solu-\ntions. Decomposing a complex problem into smaller, more\nsolvable sub-problems is a common strategy that helps\nprevent LLMs from being confused by the sheer size of the\nproblem. Problem decomposition strategies can be divided\ninto two types.\nDecomposition in the processing order generally in-\nvolves two steps: breaking the problem into sub-problems\nand solving each sub-problem in a specific sequence. Ac-\ncording to the defined sequence, the answers to previously\nsolved sub-problems can facilitate the resolution of the next\none. Based on this idea, Zhou et al. [20] present Least-to-\nMost prompting, which deconstructs a complex problem\ninto a sequence of multiple sub-problems, and solutions to\nprevious sub-problems are used as prompts to address the\nsubsequent ones. Creswell et al. [56] introduce a Selection-\nInference framework, which splits each reasoning step into\nselection and inference, thereby alternating between them\nto generate a series of explainable and arbitrary reasoning\nsteps, ultimately leading to the final answer. Wang et al. [57]\npropose Plan-and-Solve prompting, wherein an initial plan\nis formulated to systematically break down the overarching\ntask into more manageable sub-tasks, subsequently exe-\ncuting them by the devised plan. However, it is essential\nto note that the visualization specification generation task\noften cannot be decomposed into sequential steps as the\nqueries are typically executed in a single step. Consequently,\nthe sequential decomposition approaches are not suitable\nfor our work.\nDecomposition by task type refers to decoupling the\nentire problem into independent sub-problems, each corre-\nsponding to a specific task, the outcomes of which are then\naggregated to form the final result. Chen et al. [19] introduce\nthe Program of Thoughts Prompting (PoT), utilizing LLMs\npre-trained with codes to script programs, thus decoupling\ncomputations from reasoning tasks. The Vega-Lite gener-\nated in our evaluation implements the data transformation\nand visual mappings in a single specification. Therefore,\ndecomposing by the task type is not applicable to our work.\n3.1.4\nIn-Context Learning\nIn-context Learning (ICL) can improve the predictive per-\nformance of LLMs by enhancing contextual information\nthrough task-relevant examples or instructions. For the ICL\nprompting strategies, the LLM receives examples described\nin natural language templates and its demonstration con-\ntexts as prefixes, followed by a query as input, thus generat-\ning outputs that imitate the provided examples. ICL can be\ncombined with other prompting techniques like Chain-of-\nThought, with the demonstration context encompassing the\nprocess of reasoning, not just the answers. This approach\ncan effectively stimulate the reasoning ability of LLMs. Yin\net al. [58] employ a prompt including notebook contexts\nand the current intent into NL2Code tasks, making LLMs\ndirectly comprehend the task based on provided instances.\nFor complex use cases of NL2VIS tasks, the ambiguous\nintention of input queries is challenging to understand\ncorrectly without additional specifications. Inspired by the\nICL prompting strategies, our prompt involves a sample of\noriginal data tables in the context to help LLMs compre-\nhend the query, significantly improving LLM’s grounded\nunderstanding of structured knowledge and increasing the\ngeneration accuracy.\n3.2\nEvaluated Prompts\nThe LLMs can take text as input and generate codes. There-\nfore, it is a promising approach to finish the NL2VIS tasks\nbased on LLMs. Because existing NL2VIS techniques are\nend-to-end, that is, they accept natural language queries and\noutput visualization specifications, we solely consider the\none-round interaction condition in this work to simulate the\nend-to-end paradigm and simplify the evaluation process.\nTherefore, we define the process of LLM-based NL2VIS as\ntwo steps. First, users input a prompt containing task de-\nscriptions and related data tables. Second, the LLMs return\nVega-Lite specifications as response and we further obtain\nthe corresponding visualization results.\nTo make LLMs (in this paper, the GPT-3.5) generate\ncorrect and effective visualizations, we attempt to optimize\nour prompts and compare the respective performances on\nthe nvBench dataset. Our prompt’s basic version is simple:\nit merely tells the LLM what roles it plays in the system\nmessage, the task query, and the data table it needs in the\nuser message, as presented in the following.\n{\n\"role\": \"system\",\n\"content\": \"You are a data analysis\nassistant that uses Vega-Lite to create\ndata visualizations, and you should only\noutput the json format specification of\nVega-Lite.\"\n}\n6\n{\n\"role\": \"user\",\n\"content\": Create the optimal\nvisualization for the {table_name} data\ntable using Vega-Lite to complete this\ntask:\n‘‘‘{nl_query}‘‘‘\nThe {table_name} data table is as follows:\n‘‘‘{sampled_data}‘‘‘\nThe \"data\" attribute of the Vega-Lite\noutput must be: {\"url\": \"data.csv\"}\nJust output the json format, with no more\nother words.\n}\nThe variable in the curly brackets table name, nl query,\nand sampled data are the name of the data table, the task\ndescription, and the needed data table extracted from the\nnvBench dataset, respectively. Because several data tables\nin the nvBench are extremely large, we choose to transfer\na sampled data table to GPT-3.5 to decrease the number of\ntokens.\nWe find that the output given by GPT-3.5 is not a\npure Vega-Lite specification and always contains redundant\nexplanatory text. To make the output only have Vega-Lite\nspecification to facilitate following automatic processes, we\nadd an instruction at the end of the prompt to make GPT-3.5\nonly output the Vega-Lite specification.\nThe generated specifications always use inline data\nsources in specification outputs, which might cause errors\nbecause the prompt only consists of a partial data table\nsampled from the original data. Therefore, we add another\ninstruction to control the format of the data property in the\nspecification to use the URL “data.csv” as the data source.\nWe utilize this version of prompt as the basis to construct\nthe following zero-shot and few-shot prompts.\n3.2.1\nZero-shot prompts\nTo evaluate the effectiveness of the initial prompt, we first\nconduct a preliminary study on a hundred instances in the\nnvBench dataset and find that the generated specifications\nhave several types of mistakes. To promote the performance\nof LLMs, we complement some rules to make the LLMs\navoid these types of mistakes. Therefore, the zero-shot\nprompt consists of three parts: task descriptions, a sampled\ndata table, and several rules for instructing the generation\nof Vega-Lite specifications. The following discusses the mis-\ntakes and the corresponding rules in the prompt:\n(1) Formalize the version of Vega-Lite. The visualization\nspecifications based on Vega-Lite stipulate its version in the\n$schema property. The results of the preliminary study\nshow that the versions of generated Vega-Lite by GPT-3.5\nare not consistent, including both version 4 and version 5.\nIn addition, the syntax of Vega-Lite has some variances be-\ntween different versions. To make the version of generated\nVega-Lite specifications consistent, we add the following\nrule (Rule 1) in the prompt to guide the output to use\nVega-Lite in version 5, which is utilized by the visualization\ncommunity popularly.\nRule 1: The \"$schema\" property should be: \"\nhttps://vega.github.io/schema/vega-lite/v5.\njson\".\n(2) Formalize the position of the transform attribute.\nThe available variables, by default, only include the at-\ntributes in the original data table. Therefore, the generated\nspecifications always need to define a variable for the trans-\nformation operations listed in the transform property.\nHowever, over a quarter of generated specifications in our\npreliminary evaluation results put the transform prop-\nerty behind the encoding property. These instances do\nnot use the name defined in transform for the field\nproperty of axes in the encoding , leading to incorrect\nresults. Therefore, we complement the following rule (Rule\n2) about putting the transform property ahead of the\nencoding property, to prevent generated specifications\nfrom not using the variable names that have been defined.\nThe results demonstrate that when the transform prop-\nerty is ahead of the encoding property, the generated\nspecifications can reference the previously defined variable\ncorrectly. This example implies that the order of specified\nattributes in generated Vega-Lite specifications can influence\nthe final results.\nRule 2: The \"transform\" property should be put\nahead of the \"encoding\" property.\n(3) Remind the use of filter transformations in\nthe aggregate property. None of the specifications in the\npreliminary study can filter the input data according to the\nquery description successfully. We add the following rule\n(Rule 3) to remind the model to use the filter operation\nto implement data selection. The evaluation results demon-\nstrate that this instruction indeed reduces the number of\ninstances that forget to implement required data filtering.\nRule 3: Pay attention to the query description\nto determine whether you should use \"filter\"\ntransformation in the \"transform\" property.\n(4) Remind the use of groupby transformations in\nthe aggregate property. Furthermore, we also find that\nnearly half of the preliminary results forget to specify the\ngroupby property when using aggregate operation in\nthe transform property. The groupby property is neces-\nsary to clarify the data fields to be grouped by when doing\nspecific transformations. For instance, when you show the\naverage height of students for different majors or grades, the\ngroupby property should be the major or grade attribute,\nrespectively. Therefore, missing of the groupby property\nwill make Vega-Lite use a default value for it and can lead\nto wrong results. To fix this mistake, we add a correspond-\ning rule (Rule 4) to remind the GPT-3.5 model to specify\nthe groupby property in the specification. The evaluation\nresults demonstrate that with this rule the GPT-3.5 never\nforgets to specify the groupby property of aggregate\nanymore.\nRule 4: If you use \"aggregate\" operation in\nthe \"transform\" property, the \"groupby\"\nproperty of \"aggregate\" should be correctly\nspecified.\n7\n(5) Correct the use of the sort property. More than\nninety percent of generated visualization specifications in-\nvolving the sort operation are erroneous because the results\noutput by GPT-3.5 always defines the sort property in the\ntransform object, which violates the Vega-Lite grammar.\nTherefore, we complement the following rule (Rule 5) to\ncorrect its usage of the sort operation. Though this rule\ndecreases the number of erroneous instances involving the\nsort operation, many instances of this type are still erro-\nneous no matter how we revise the statement of this rule.\nWe also discuss this question in Sec. 5.1. It reveals that a\nsingle rule cannot always guide the generated specification\nresults and make them correct.\nRule 5: Make sure no \"sort\" operations exist\nin the \"transform\" property, you should define\nthe order of axes only in the \"encoding\"\nproperty.\nFurthermore, after adding this rule, we detect halluci-\nnations in the LLM’s results. More specifically, when using\nsort transformation, the generated specifications change its\npattern of mapping attributes to axes. The previous spec-\nification generally maps nominal attributes to the x-axis\nand quantitative attributes to the y-axis. With the new rule,\nthe specification maps nominal attributes to the y-axis and\nquantitative attributes to the x-axis and results in flipped\naxes. Though the flipped visualization result differs from\nground truths in appearance, the data they present are the\nsame and, therefore, do not negatively affect the correctness.\n3.2.2\nFew-shot prompts\nBesides the zero-shot prompt, we also adopt the few-shot\nprompt strategy. In the zero-shot prompt, we do not provide\nexamples of the Vega-Lite specifications to the GPT-3.5 but\njust use several rules based on the analysis of its mistakes\nand errors. In contrast, the few-shot prompt strategy does\nnot add extra rules about the Vega-Lite specification but lists\nsome examples consisting of task queries and corresponding\ncorrect Vega-Lite specifications as the training data and la-\nbel, respectively. The queries are selected from the nvBench\ndataset and we adopt the correct specifications generated\nby the GPT-3.5 using zero-shot prompts as the source of\ntraining labels.\nWe add one example for each chart type contained in\nthe Vega-Lite to avoid that the omitting of a specific chart\ntype, which may cause the performance degradation on that\ntype. Furthermore, the example for each chart type we select\nincludes at least one particular task type, such as sorting,\nfiltering, and aggregation, and we anticipate the GPT-3.5 to\nextract rules about corresponding tasks from each chart type\nexample and generalize them to other chart types. Here we\npresent the beginning and the first example of our few-shot\nprompt:\nHere are some examples that show the high-\nquality.\nVega-Lite specifications for different queries\nCase 1 is a scatter plot:\nThe query is:\n‘‘‘Show all majors and corresponding number of\nstudents by a scatter plot.‘‘‘\nThe Vega-Lite specification is:\n{\n\"$schema\": \"https://vega.github.io/schema/\nvega-lite/v5.json\",\n\"data\": {\"url\": \"data.csv\"},\n\"transform\": [\n{\"aggregate\": [{\"op\": \"count\", \"field\n\": \"Major\", \"as\": \"Number of Students\"}],\n\"groupby\": [\"Major\"]}\n],\n\"mark\": \"point\",\n\"encoding\": {\n\"x\": {\"field\": \"Major\", \"type\": \"\nnominal\"},\n\"y\": {\"field\": \"Number of Students\", \"\ntype\": \"quantitative\"}\n}\n}\nIn conclusion, our few-shot prompt consists of (query,\nVega-Lite) pair examples for each chart type in the nvBench,\ntask descriptions, query descriptions, a sampled data table,\nand two-generation rules specifying the output format.\n4\nEVALUATION\nIn this section, we first introduce the dataset and LLM we\nadopt in the evaluation, and then we present and analyze\nthe evaluation results and the comparison with previous\nNL2VIS approaches.\n4.1\nDataset and Model\nWe utilize the nvBench [26] dataset as the benchmark in our\nevaluation. The nvBench is a large-scale NL2VIS dataset,\ncovering over 100 application scenarios like sports and med-\nical, and containing 7,247 visualization instances and more\nthan 25,000 (NL, VIS) pairs. The dataset covers seven chart\ntypes: bar, pie, line, scatter, stacked bar, grouping line, and\ngrouping scatter charts. The dataset categorizes the NL2VIS\ntasks into four levels of difficulties: easy, medium, hard, and\nextra hard, based on the complexity of the data transfor-\nmations involved. Each instance in the nvBench contains a\ntask described in a natural language query proposed by Luo\net al. [26] and can be seen as a combination of chart type\nstatement and a SQL query, a visualization as the ground\ntruth, and several natural language queries.\nWe find that partial instances in the nvBench require join-\ning at least two data tables to fulfill the corresponding task.\nHowever, Vega-Lite cannot support multiple data sources\nin a single visualization specification. Therefore, we need\nto merge multiple data tables, filter the required attributes\nusing SQL queries, and take the data processing result as\nthe input of Vega-Lite specification. Because this study only\nevaluates the capability of a large language model to gen-\nerate visualization specifications, we filter these instances\nrelated to multiple data tables and use all the remaining\ninstances to implement our evaluation.\nAs for the LLM, we select the GPT-3.5 as the representa-\ntive of LLMs because it is commonly used and has a lower\n8\ncost than the GPT-4. To prevent the prompts containing few-\nshot examples and sampled data tables from exceeding the\ntoken limit, we adopt the GPT-3.5-turbo-16k to implement\nthe evaluation. We set the temperature parameter of the\nGPT-3.5 to 0 to avoid randomness.\n4.2\nMetrics\nIn the evaluation, we leverage the match accuracy between\nthe ground truth and the results generated by the GPT-\n3.5. Specifically, we convert the generated Vega-Lite into\nvisualization results and then compare the results with\nthe ground truth visualizations to check whether they are\nequivalent.\nTo measure the equivalence between two charts that are\npresented as images, we first use a naive metric, pixel-\nbased match accuracy. The pixel-based match means that\nthe generated result is correct only if each pixel of its image\nis identical to the corresponding pixel of the ground truth\nimage. However, this naive metric is too strict because only\na little difference between the two images, for instance, the\ntitle of the axes, can lead to a pixel-based mismatch. It is\nreasonable that two visualizations are equivalent if the chart\ntype and the data presented are identical. Therefore, we\nadopt a svg-json-based matching strategy, which leverages\nboth the SVG image and the Vega-Lite specification in a\nJSON format, to check the equivalence of two visualizations.\nFirst, we extract the chart type of the visualization from\nthe corresponding Vega-Lite specification in the JSON for-\nmat. If the chart type of the generated result is different from\nthat of the ground truth, we consider the generated result\nas wrong. Then, we extract the underlying values from the\ngraphical elements in the visualization shown by an SVG\nand compare these two value sets. If the two value sets are\nequal and the chart types are also the same, we consider the\ngenerated result as right.\n4.3\nEvaluation Results\nThe accuracy of the pixel-based matching strategy is 3.02%\nfor the zero-shot strategy and 5.63% for the few-shot strat-\negy. Both of these matching results are low because even\nthough the contents of two visualization charts are equiv-\nalent, their pixels are possibly not the same entirely and\ndifferent in some nuances, such as the title of axes or the\nunit value of axes.\nTable 1 presents the evaluation result based on the svg-\njson-based matching strategy. The overall match accuracies\nof zero-shot and few-shot prompt strategies on the nvBench\nare 43.23% and 50.12%, respectively and the performance\nof few-shot prompt is significantly higher than that of zero-\nshot prompt. For the performance on different chart types,\nthe zero-shot prompt generates the highest accuracy on scat-\nter plots and the second highest accuracy on pie charts, and\nthe following ranking is grouping scatter plots, bar charts\nand stacked bar charts. As for the few shot prompt, the\nperformance ranking from high to low is grouping scatter\nplots, pie charts, scatter plots, bar charts, and stacked bar\ncharts, and the accuracy of grouping scatter plots is even\ngreater than 90%.\nBoth prompt strategies perform relatively well on scatter\nplots, grouping scatter plots, and pie charts and have a\nrelatively low accuracy on bar charts and stacked bar charts.\nEspecially for stacked bar charts, the generation accuracy\ncan be slightly more than 20%. It is worth noting that though\nthe performance of the few-shot prompt is higher than that\nof the zero-shot prompt on the overall accuracy and the\naccuracies of the other four chart types, its performance\non the scatter plots is lower than the zero-shot prompt.\nWe analyze the generated Vega-Lite specifications of scatter\nplots output by two prompts, respectively, and find that the\nspecifications of the few-shot prompt have more errors in\ndata transformation compared with the zero-shot prompt.\nSpecifically, partial specifications of the few-shot prompt fail\nto implement data filtering as the task query requires, while\ncorresponding specifications given by the zero-shot prompt\ncorrectly complete the task. For instance, a query in the\nnvBench asks to make a scatter plot to show the age and\nweight of abandoned dogs whose abandoned yn attribute\nof the table is equal to 1. However, the result of the few-\nshot prompt uses all the items in the table, while the zero-\nshot result correctly selects the items whose abandoned yn\nattribute is equal to 1.\nWe further analyze why the specifications of the few-\nshot prompt have more errors in data transformation com-\npared with the zero-shot prompt for scatter plots. As ex-\nplained above, we add rules to realize the filtering operation\nin the specification based on the zero-shot prompt strategy.\nFor the few-shot prompt strategy, we list some examples in\nthe prompt. We find that the scatter plot example we use\nin the few-shot prompt does not contain a filtering opera-\ntion. However, the examples of other chart types used in\nthe few-shot prompt indeed contain the filtering operation.\nTherefore, the abnormality in the performance that the zero-\nshot prompt performs better than the few-shot prompt is\npossibly because the GPT-3.5 fails to generalize the filtering\noperation to other chart types, especially the scatter plot in\na proportion of generated specifications.\nThe task difficulty, referred to as “hardness” defined\nin the nvBench, also influences the evaluation results sig-\nnificantly. As shown in Table 1, the performance of both\nprompts decreases with increasing difficulties. The perfor-\nmance of the few-shot prompt on hard tasks is abnormally\nlower than the zero-shot prompt, which is similar to scatter\nplots. After analysis, we find that the reason is also identical.\nMore specifically, the hard tasks consist of many data trans-\nformations like data filtering. However, the results given by\nthe few-shot prompt forget to do required transformations\nor make mistakes in transformation operations, making the\nperformance of the few-shot prompt relatively lower.\nWe classify the reasons resulting in wrong generation\nresults into four categories: invalid JSON, invalid Vega-Lite,\nchart type mismatch, and chart content mismatch. Table 2\nshows the distribution of these four errors. The invalid JSON\nerror means the generated specification is not a valid JSON\nfile, and the invalid Vega-Lite error means the specification\nis a valid JSON file but cannot conform to the syntax of\nVega-Lite. The error about chart type mismatch implies that\nthe chart type of generated results is different from the\nground truth. The chart content mismatch means that the\nchart types between the ground truth and generated results\nare identical, but the underlying data contents are different.\nTable 2 presents the distributions among four different\n9\nTABLE 1: Evaluation result: The performance of GPT-3.5-turbo-16k with zero-shot and few-shot prompts on different chart\ntypes and task difficulties.\nAccuracy\nOverall\nChart type\nTask difficulty\nBar\nPie\nScatter\nStacked bar\nGrouping scatter\nEasy\nMedium\nHard\nExtra hard\nZero-shot\n43.23%\n46.00%\n57.37%\n69.68%\n21.47%\n51.05%\n48.28%\n47.15%\n37.40%\n10.93%\nFew-shot\n50.12%\n53.32%\n68.80%\n62.67%\n21.79%\n93.19%\n60.72%\n54.28%\n30.39%\n12.92%\nerrors of zero-shot and few-short prompts. We can learn that\nthe major errors in the results of both prompts are invalid\nVega-Lite and chart content mismatch. The proportion of\ninvalid Vega-Lite of the zero-shot prompt is significantly\nhigher than the few-shot prompt, while the proportion of\nthe other three error types of both prompts has a slight\ndifference. Although our zero-shot prompt adds rules to\ninstruct the GPT-3.5 to avoid syntax errors and fulfill tasks\nmore correctly, the zero-shot strategy still makes more errors\nof Vega-Lite grammar than the few-shot strategy. The reason\nmay be that the added rules interfere mutually and thus do\nnot work as expected.\n4.4\nComparison with Previous Approaches\nThe performance comparison among previous approaches\nand our zero-shot and few-shot prompt strategies using the\nGPT-3.5 is shown in Table 3. The performance of previous\napproaches is evaluated by Song et al. [33] and Maddigan\net al. [23].\nWe can learn the performance of the zero-shot prompt\nis slightly lower than the state-of-the-art performance\nof previous approaches and is approximately equal to\nChat2VIS [21], which is also an LLM-based approach using\na simple prompt that is not explicitly optimized. The perfor-\nmance of the few-shot prompt strategy surpasses the state-\nof-the-art performance of previous approaches by greater\nthan 50%, demonstrating the powerful potential of GPT-3.5\non the NL2VIS task.\n5\nFINDINGS\nIn this section, we first categorize the reasons that restrict the\nperformance of GPT-3.5 on the NL2VIS task, then introduce\nthe limitations we find in the existing NL2VIS benchmark.\n5.1\nLimitations of LLMs in NL2VIS\nThe previous section shows the performance of GPT-3.5 on\nthe nvBench dataset is mediocre. We analyze errors and\ndivide them into the following three categories to find the\nlimitations of GPT-3.5 on Vega-Lite generation to facilitate\nfurther optimization.\nError Type 1: Invalidity of Vega-Lite specification. This\nis the most common error in our evaluation results, meaning\nthat the generated Vega-Lite specifications are invalid and\nhave grammar errors, leading to the failure in transform-\ning the Vega-Lite specifications to the visualization results.\nThe reasons leading to invalid Vega-Lite specification are\ndivided into three types.\n(1) JSON format error. This type of error misuses the\nproperties of Vega-Lite specification and results in an invalid\nJSON object. Fig. 2(a) shows an example generated by GPT-\n3.5, which violates the format rules of valid JSON objects.\nMore specifically, GPT-3.5 puts the mark and encoding\nproperties into the transform property and regards the\nencoding property as the ending of the specification. Such\nmisusage always leads to a missing square bracket at the\nend of the transform and causes a JSON format violation.\n(2) Using non-existent properties in the Vega-Lite specifica-\ntion. This type of error adds non-existent properties within\nthe generated specification. Fig. 2(b) shows an example\nthat conforms to the JSON format requirements but uses a\nnon-existent sort in the transform property. According\nto the task description, the chart should sort the Director\nattribute in ascending order, which can be realized by\ndefining the sort in the x property as highlighted using\nbold text in Fig. 2(b). Although GPT-3.5 can understand\nthe task query to define the sorting operation correctly, the\nmisusage of the sort in the transform property leads\nto an incorrect specification. Moreover, the error about the\nsort property occurs frequently in tasks requiring sorting\none attribute, leading to a relatively low performance of\nGPT-3.5 on this type of task.\n(3) Misusing the Vega-Lite transformation operation. The\ntype of error misuses the transformation operations in the\nVega-Lite specification. For example, Fig. 2(c) has a correct\nJSON format and does not use non-existent operations,\nbut the usage of the bin in the transform property is\nincorrect, and the output specification still violates the Vega-\nLite grammar. The task query is to bin date address to by\nweekday, and the correct specification can achieve this goal\nby defining timeUnit in the transform property. The\noutput given by GPT-3.5 confuses the function of bin and\ntimeUnit and leads to an invalid specification.\nThe incorrectness of Vega-Lite specifications reflects that\nLLMs have non-negligible hallucination [13] problems on\nthe Vega-Lite output, such as specifying data transforma-\ntions not contained in queries or making grammar errors.\nThe above examples also inspire us that the Vega-Lite vi-\nsualization specifications consist of some grammar errors,\nproviding an effective linting strategy on the visualization\nspecifications will improve the performance. The linting\ntechniques for the output of LLMs should focus on the\ntransformations of the Vega-Lite specification.\nError Type 2: Incomprehension of Vega-Lite specifi-\ncations. The second category indicates that the generated\nVega-Lite specifications do not have grammar errors and can\nbe transformed into visualizations successfully. However,\nthe generated visualizations are incorrect compared with the\nground truth because GPT-3.5 cannot fully understand the\nmeaning of parameters in Vega-Lite.\nIncomprehension of Vega-Lite operation parameters. This\n10\nFig. 2: Errors of LLMs: This figure shows five instances in the nvBench covering six errors made by LLMs in the Vega-\nLite generation task: (a) JSON format error. (b) Using non-existent properties. (c) Misusing the Vega-Lite transformation\noperation (on the left) and incomprehension of Vega-Lite operation parameters (on the right). (d) Incomprehension of the\ninput data table. (e) Incomprehension of task queries. The task query and tabular data are the task and needed data of the\ninstance respectively. The Vega-Lite specification output is given by GPT-3.5. The visualization ground truth is the correct\nvisualization result given by the nvBench. The purple dashed box indicates the incorrect location, and text in the purple\nsolid box is the explanation about the error. The Vega-Lite codes under the strikethrough are incorrect and should be\ndeleted. The Vega-Lite in bold-typed text are correct codes which should be added to the original Vega-Lite specification.\n11\nTABLE 2: Errors distribution: The proportion of the four types of errors in the Vega-Lite specifications output by LLMs.\nInvalid JSON\nInvalid Vega-Lite\nChart Type Mismatch\nChart Content Mismatch\nZero-shot\n0.11%\n18.71%\n1.38%\n29.79%\nFew-shot\n0.22%\n14.37%\n0.75%\n28.34%\nTABLE 3: NL2VIS method accuracies: The accuracies of\nNL2VIS with GPT-3.5 using our zero-shot and few-shot\nprompts and previous methods.\nApproaches\nAccuracy\nSeq2Vis [26]\n2%\nTransformer [59]\n3%\nncNet [12]\n26%\nChat2VIS [21]\n43%\nRGVisNet [33]\n45%\nOurs (Zero-shot)\n43%\nOurs (Few-shot)\n50%\ntype of error means LLMs cannot fully understand the\noperation parameters of Vega-Lite. The query in this ex-\nample is similar to the third type of Error 1. However,\ndifferent from the specification of Error 1, this specification\nhas no grammar error, but the final generated chart is still\nincorrect compared with the ground truth. The reason is the\nincomprehension of parameters of timeUnit used in the\ntransform property. Although the query indeed asks to\nbin date address to by weekday, GPT-3.5 achieves this goal\nby defining the value of timeUnit as weekday instead of\nthe correct parameter day. This example reveals that GPT-\n3.5 needs to have comprehensive knowledge of the Vega-\nLite grammar, and can be easily affected by the query to use\nincorrect parameter assignments.\nThis type of error implies that one aspect of improving\nthe performance of LLMs on the NL4VIS task is to develop\nmore strategies to facilitate LLMs to understand and cor-\nrectly use the parameters of Vega-Lite specifications. For\nexample, the documentation of Vega-Lite [60] should be one\nimportant resource for realizing this target.\nError Type 3: Incomprehension of task descriptions and\ndata. For this type of error, the generated Vega-Lite specifica-\ntions are correct from the grammar aspect, and GPT-3.5 can\nalso correctly use the parameters in the output specification.\nHowever, GPT-3.5 may misunderstand the data tables or\ntask queries, leading to an incorrect visualization result.\n(1) Incomprehension of the input data table. The type of\nerror indicates that GPT-3.5 cannot fully understand the\nattributes of the input data and misuses the attributes in the\nspecification. As shown in Fig. 2(d), the query is to show\nthe apartment number and the number of rooms for each\napartment using a bar chart, and the needed attributes in\nthe data table are apt number and room count. This task can\nbe easily completed by presenting the apt number attribute\non the x-axis and the room count attribute on the y-axis.\nHowever, as the Vega-Lite output in Fig. 2(d), GPT-3.5 does\nnot understand the meaning of the room count attribute\nand considers that each row in the data table describes the\ninformation of a single room in an apartment. Therefore, it\nchooses to count the number of rows for each apartment\nand generates an incorrect result. The correct specification is\nshown using italic, bold text in Fig. 2(d).\n(2) Incomprehension of task queries. The second type of\nerror in this category is misunderstanding task queries.\nFig. 2(e) illustrates an example of task query incompre-\nhension. The query asks to present the rank and market\nvalues of the companies in the banking industry, and GPT-\n3.5 should first select the rows whose the Main Industry at-\ntribute value equals to Banking. However, the output shown\nin Fig. 2(e) does not use the filtering operation and generates\nan incorrect result, demonstrating that GPT-3.5 does not\nfully capture the requirements of the task description.\nThe above error implies that it is essential to understand\nthe input data table and task descriptions comprehensively\nto generate the Vega-Lite specifications correctly. This find-\ning inspires us to separate the NL4VIS task based on LLMs\ninto different steps and check the correctness of the results\nfor each step. Therefore, we can bring user interactions\ninto the visualization generation process and utilize more\nprompt strategies, such as Chain-of-Thought, to improve\nLLMs’ performance.\n5.2\nReexamining NL2VIS Benchmark\nWe analyze the mismatched results in the output of GPT-\n3.5 and further find that partial instances in the nvBench are\nnot correct, and corresponding failures of GPT-3.5 should be\nattributed to the nvBench itself. We categorize the failures\ninto five categories and discuss them in detail to inspire the\nimprovement of benchmark design for the NL2VIS task.\nIncorrect query statements. Figure 3(a) shows an exam-\nple of a question in a queries that is incorrect and con-\nfusing combined with the ground truth chart. The queries\nin Fig. 3(a) ask the number of faculty members for each\nrank, and the answer is 27 for professors, 8 for associate\nprofessors, 15 for assistant professors, and 8 for instructors.\nThe ground truth chart shows an entirely different answer,\n1:2:2:2, and GPT-3.5 successfully gives out the correct an-\nswer of 27:8:15:8.\nA more reasonable explanation is that the ground truth\nchart reflects the real intention of this instance, and the\nqueries are incorrect. The only statistic satisfying the ratio\n1:2:2:2 is the number of genders for each rank; that is, pro-\nfessors’ gender attribute has only one sort, while the other\nthree ranks have two genders. Thus, the correct queries\nshould ask the proportion of the number of genders for each\nrank.\nThe incorrect query that mismatches the content of visu-\nalizations is a severe problem in an NL2VIS benchmark. It\nis difficult for human-designed and automatically generated\nqueries to avoid this problem entirely, and thus, careful ex-\naminations are indispensable. Considering some foundation\nmodels have the multi-modal ability that makes foundation\n12\nFig. 3: Examples of potential mistakes in the nvBench dataset: (a) incorrect queries (b) unstated chart type (c) unstated\ntime unit definition (d) incorrect labels (e) dropping the decimal part during calculation. The purple dashed box indicates\nthe location that may exist problems, and text in the purple solid box is the explanation about the problem.\nmodels understand the natural language and images simul-\ntaneously, we can use them to examine whether the query\nmatches the visualization content effectively.\nInappropriate data mapping. The data mapping in-\ndicates encoding data attributes into appropriate visual\nchannels. Principles of visualization design have been well\nstudied and summarized in the existing research commu-\nnity [1], [61], [62]. Effective visualization results require\nappropriate data mapping strategies. However, we find\npart of the Vega-Lite specifications given by the nvBench\nassign inappropriate types to the data. Though these inap-\npropriate data mappings have no negative impacts within\nour evaluation process, this problem may affect evaluation\napproaches based on the contents of visualization images.\nFor example, a scatter plot of ground truths presents the\nnumber of different majors on the x-axis and regards the\nmajor number as quantitative type. However, the number\nof different majors is usually discrete integer numbers but\nnot continuous real numbers. Therefore, the major number\nof this example should be nominal type.\nAnother ground truth misuses temporal data as the nom-\ninal type, which occurs in almost every instance involving\ntemporal data. This situation conveys unreasonable data\ninsights from the view of visualization design principles\nbecause it may lead to an incorrect time sequence as the\nnominal type data is arranged alphabetically. Therefore, the\ntemporal data should be classified as a temporal or ordinal\ntype.\nTo correct inappropriate data mappings in the visualiza-\ntion, we can adopt linting techniques, which are similar to\nsolving the invalidity of Vega-Lite specifications in 5.1. we\ncan only add rules about data mappings of visualization\nand use the linting technique to check specifications in the\nbenchmark and correct inappropriate data mappings.\nIncorrect data. Many instances in the nvBench involving\nnumerical calculations such as summation and average di-\nrectly drop the decimal part of data in original tables before\ncalculation, leading to entirely different computation results\nbetween the ground truths and output of GPT-3.5.\nAs shown in Fig. 3(e), the monthly rental attribute in the\noriginal data table is the mixed decimal. However, in the\nVega-Lite specification given by the nvBench shown in the\nrightmost, the final summation results are the integer num-\nber. After examination, we find the nvBench drops the dec-\nimal part of the monthly rental attribute before summation,\nresulting in an absolute error greater than one compared\nwith correct answers.\nAnother problem subtype that existed in nvBench\nground truths is incorrect labels. Fig. 3(d) shows an example.\nThis instance asks for the proportion of products for each\nproduct name, but the ground truth chart obviously loses\nthe Sony label according to the original table. In contrast,\nthe output of GPT-3.5 is correct actually. Because the Vega-\nLite specification ground truth adopts the inline data source,\n13\nthe incorrect label problem may be caused by carelessness\nwhen generating this instance.\nTo make the data shown in the visualization consistent\nwith the original table, we should ensure the data prop-\nerty’s content is equal to that of the original table or just use\nthe inline data source in Vega-Lite specifications.\nUnstated time unit definition. Some instances in the\nnvBench requiring the presentation of temporal data in\ncharts have queries that do not explicitly state the time\nunit’s definition. Therefore, the results generated by GPT-\n3.5 define different time units compared with ground truths,\nleading to low accuracy in these instances.\nFigure 3(c) shows an example of the unstated time unit\ndefinition. The ground truth chart bins the date of notes\nattribute, which is measured in seconds, to the time unit\nof one week. However, this requirement is not explicitly nor\nimplicitly stated in the query statement, thus making GPT-\n3.5 generate an entirely different chart compared with the\nground truth.\nThis problem is similar to the incorrect query problem.\nThus, we can also use the multi-modal foundation model\nto examine the generated visualization results. If the foun-\ndation models find that the visualization defines the time\nunit not stated in corresponding queries, we can revise the\nvisualization specification and correct this problem.\nUnstated chart type. Most queries in the nvBench de-\nmand a specific chart type, but some do not state a chart\ntype explicitly, which negatively impact the judgement of\nGPT-3.5.\nThe instance shown in Fig. 3(b) has five queries, and\nonly the first and third queries require a pie chart explicitly.\nGPT-3.5 generates pie charts for these two queries (first\nand third), and gives out bar charts for the other three\nqueries. For the three queries that do not require pie charts\nexplicitly, they emphasize the proportion and quantity in the\nstatement simultaneously. Thus, bar charts are also appro-\npriate. We define the mismatch between generated results\nand ground truths as incorrect results. Thus, the problem\nof unstated chart types also decreases the performance of\nGPT-3.5 on the nvBench.\nThe queries in the nvBench are generated automatically\nby an NL2SQL-to-NL2VIS synthesizer [26], and we suppose\nthe generation procedure for natural language queries can\nbe further optimized to eliminate the ambiguity of chart\ntype statements existing in a part of queries. For instance,\nwe can use LLMs to examine queries and add the most\nappropriate chart type, which is determined by the LLM\ndepending on the rules of chart type selection we input in\nthe prompt, to the problematic query.\n6\nCONCLUSION\nIn this paper, we select GPT-3.5 and the nvBench dataset\nto evaluate the performance of LLMs on the NL2VIS task.\nSpecifically, we make GPT-3.5 generate Vega-Lite specifica-\ntions given a data table and a task query as the input and\ncompare the generated visualizations with ground truths\nusing the matching accuracy based on the chart type and\ndata contents in the visualization as the metric. We design\na zero-shot prompt and a few-shot prompt for the Vega-Lite\ngeneration and evaluate their performances respectively.\nWe also summarize and categorize several drawbacks of\nGPT-3.5 on the NL2VIS task and the nvBench itself, which\ncan facilitate the advancement of NL2VIS. We attempt to\npromote the performance of the NL2VIS task by optimizing\nprompts and introducing conversational interaction with\nLLMs in future work. The key findings of this paper are\nas follows:\n• The evaluation results demonstrate that the performance\nof the few-shot prompt is significantly higher than the\nzero-shot prompt and surpasses the SOTA performance\nof previous NL2VIS techniques.\n• The LLM has drawbacks in the Vega-Lite grammar and\nunderstanding of task descriptions and data. The linting\ntechniques and interaction between users and LLMs can\nbe leveraged to correct the mistakes or improve the\nquality of LLM’s output. The documentation of Vega-\nLite and specification examples can be used to finetune\nthe LLM or be added to prompts to promote the LLM on\nNL2VIS tasks.\n• Existing NL2VIS benchmarks also have several draw-\nbacks, which may affect the evaluation and promotion of\nthe performance of LLMs on NL2VIS tasks. Using LLMs\nto improve the quality of queries in the benchmark can\nbe considered. We can add several rules about the query,\nsuch as explicit chart-type statements and clear time unit\ndefinitions for temporal data, and make LLMs examine\nand correct benchmark queries.\nREFERENCES\n[1]\nT. Munzner, Visualization analysis and design.\nCRC press, 2014.\n[2]\nJ. D. Mackinlay, “Automating the design of graphical presenta-\ntions of relational information,” ACM Transactions on Graphics,\nvol. 5, no. 2, pp. 110–141, 1986.\n[3]\nJ. Mackinlay, P. Hanrahan, and C. Stolte, “Show me: Automatic\npresentation for visual analysis,” IEEE Transactions on Visualization\nand Computer Graphics, vol. 13, no. 6, pp. 1137–1144, 2007.\n[4]\nY. Sun, J. Leigh, A. E. Johnson, and S. Lee, “Articulate: A semi-\nautomated model for translating natural language queries into\nmeaningful visualizations,” in Proc. Int. Symp. Smart Graphics (SG),\nser. Lecture Notes in Computer Science, vol. 6133, 2010, pp. 184–\n195.\n[5]\n“The\nstanford\nparser,”\nhttp://nlp.stanford.edu/software/\nlex-parser.shtml.\n[6]\nY. Luo, X. Qin, N. Tang, G. Li, and X. Wang, “DeepEye: Creating\ngood data visualizations by keyword search,” in Proc. Int. ACM\nConf. Management of Data (SIGMOD), 2018, pp. 1733–1736.\n[7]\n“Apache opennlp,” 2017, http://opennlp.apache.org.\n[8]\nA. Narechania, A. Srinivasan, and J. T. Stasko, “NL4DV: A toolkit\nfor generating analytic specifications for data visualization from\nnatural language queries,” IEEE Transactions on Visualization and\nComputer Graphics, vol. 27, no. 2, pp. 369–379, 2021.\n[9]\nC. D. Manning, M. Surdeanu, J. Bauer, J. R. Finkel, S. Bethard, and\nD. McClosky, “The Stanford CoreNLP natural language process-\ning toolkit,” in Proc. Con. Association for Computational Linguistics\n(ACL), 2014, pp. 55–60.\n[10] L. Shen, E. Shen, Y. Luo, X. Yang, X. Hu, X. Zhang, Z. Tai, and\nJ. Wang, “Towards natural language interfaces for data visualiza-\ntion: A survey,” IEEE Transactions on Visualization and Computer\nGraphics, vol. 29, no. 6, pp. 3121–3144, 2023.\n[11] C. Liu, Y. Han, R. Jiang, and X. Yuan, “ADVISor: Automatic visu-\nalization answer for natural-language question on tabular data,”\nin Proc. IEEE Pacific Visualization Symposium (PacificVis), 2021, pp.\n11–20.\n[12] Y. Luo, N. Tang, G. Li, J. Tang, C. Chai, and X. Qin, “Natural\nlanguage to visualization by neural machine translation,” IEEE\nTransactions on Visualization and Computer Graphics, vol. 28, no. 1,\npp. 217–226, 2022.\n14\n[13] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L.\nAleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat et al.,\n“GPT-4 technical report,” preprint arXiv:2303.08774, 2023.\n[14] W. Yang, M. Liu, Z. Wang, and S. Liu, “Foundation models\nmeet visualizations: Challenges and opportunities,” Computational\nVisual Media, 2024, preprint arXiv:2310.05771.\n[15] D. Hendrycks, S. Basart, S. Kadavath, M. Mazeika, A. Arora,\nE. Guo, C. Burns, S. Puranik, H. He, D. Song, and J. Steinhardt,\n“Measuring coding challenge competence with APPS,” preprint\narXiv:2105.09938, 2021.\n[16] H. Liu, R. Ning, Z. Teng, J. Liu, Q. Zhou, and Y. Zhang, “Evalu-\nating the logical reasoning ability of chatgpt and GPT-4,” preprint\narXiv:2304.03439, 2023.\n[17] Z. Yuan, H. Yuan, C. Tan, W. Wang, and S. Huang, “How well\ndo large language models perform in arithmetic tasks?” preprint\narXiv:2304.02015, 2023.\n[18] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le,\nD. Zhou et al., “Chain-of-thought prompting elicits reasoning in\nlarge language models,” Advances in Neural Information Processing\nSystems, vol. 35, pp. 24 824–24 837, 2022.\n[19] W. Chen, X. Ma, X. Wang, and W. W. Cohen, “Program of thoughts\nprompting: Disentangling computation from reasoning for numer-\nical reasoning tasks,” preprint arXiv:2211.12588, 2022.\n[20] D. Zhou, N. Sch¨arli, L. Hou, J. Wei, N. Scales, X. Wang, D. Schuur-\nmans, C. Cui, O. Bousquet, Q. V. Le, and E. H. Chi, “Least-to-most\nprompting enables complex reasoning in large language models,”\nin Proc. Int. Con. Learning Representations (ICLR), 2023.\n[21] P. Maddigan and T. Susnjak, “Chat2VIS: Generating data visu-\nalizations via natural language using chatgpt, codex and GPT-3\nlarge language models,” IEEE Access, vol. 11, pp. 45 181–45 193,\n2023.\n[22] V. Dibia, “LIDA: A tool for automatic generation of grammar-\nagnostic visualizations and infographics using large language\nmodels,” in Proc. Con. Association for Computational Linguistics\n(ALC), 2023, pp. 113–126.\n[23] P. Maddigan and T. Susnjak, “Chat2VIS: Fine-tuning data visual-\nisations using multilingual natural language text and pre-trained\nlarge language models,” preprint arXiv:2303.14292, 2023.\n[24] X. Pu, M. Kay, S. M. Drucker, J. Heer, D. Moritz, and A. Satya-\nnarayan, “Special interest group on visualization grammars,” in\nExtended Abstacts of ACM Conf. Human Factors in Computing Sys-\ntems, 2021.\n[25] A. Satyanarayan, D. Moritz, K. Wongsuphasawat, and J. Heer,\n“Vega-lite: A grammar of interactive graphics,” IEEE Transactions\non Visualization and Computer Graphics, vol. 23, no. 1, pp. 341–350,\n2017.\n[26] Y. Luo, N. Tang, G. Li, C. Chai, W. Li, and X. Qin, “Synthesiz-\ning natural language to visualization (NL2VIS) benchmarks from\nNL2SQL benchmarks,” in Proc. Int. ACM Conf. Management of Data\n(SIGMOD), 2021, pp. 1235–1247.\n[27] K. C. Cox, R. E. Grinter, S. Hibino, L. J. Jagadeesan, and D. Man-\ntilla, “A multi-modal natural language interface to an information\nvisualization environment,” International Journal of Speech Technol-\nogy, vol. 4, no. 3, pp. 297–314, 2001.\n[28] T. Gao, M. Dontcheva, E. Adar, Z. Liu, and K. G. Karahalios,\n“DataTone: Managing ambiguity in natural language interfaces for\ndata visualization,” in Proc. ACM Conf. Symposium on User Interface\nSoftware and Technology (UIST), 2015, pp. 489–500.\n[29] V. Setlur, S. E. Battersby, M. Tory, R. Gossweiler, and A. X. Chang,\n“Eviza: A natural language interface for visual analysis,” in Proc.\nACM Conf. Symposium on User Interface Software and Technology\n(UIST), 2016, pp. 365–377.\n[30] T. J. Parr and R. W. Quong, “ANTLR: A predicated-LL(k) parser\ngenerator,” Software: Practice and Experience, vol. 25, no. 7, pp. 789–\n810, 1995.\n[31] B. Yu and C. T. Silva, “FlowSense: A natural language interface for\nvisual data exploration within a dataflow system,” IEEE Transac-\ntions on Visualization and Computer Graphics, vol. 26, no. 1, pp. 1–11,\n2020.\n[32] Y. Zhang, P. Pasupat, and P. Liang, “Macro grammars and holistic\ntriggering for efficient semantic parsing,” in Proc. Conf. on Em-\npirical Methods in Natural Language Processing (EMNLP), 2017, pp.\n1214–1223.\n[33] Y. Song, X. Zhao, R. C. Wong, and D. Jiang, “Rgvisnet: A hybrid\nretrieval-generation neural framework towards automatic data\nvisualization generation,” in Proc. Int. ACM Conf. Management of\nData (SIGMOD), 2022, pp. 1646–1655.\n[34] Q. Chen, S. Pailoor, C. Barnaby, A. Criswell, C. Wang, G. Dur-\nrett, and I. Dillig, “Type-directed synthesis of visualizations from\nnatural language queries,” Proc. ACM Program. Lang., vol. 6, no.\nOOPSLA2, pp. 532–559, 2022.\n[35] J. Devlin, M. Chang, K. Lee, and K. Toutanova, “BERT: pre-training\nof deep bidirectional transformers for language understanding,”\nin Proc. Conf. North American Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies(NAACL-HLT), 2019,\npp. 4171–4186.\n[36] A. Srinivasan, N. Nyapathy, B. Lee, S. M. Drucker, and J. T. Stasko,\n“Collecting and characterizing natural language utterances for\nspecifying data visualizations,” in Proc. ACM Conf. Human Factors\nin Computing Systems (CHI), 2021, pp. 464:1–464:10.\n[37] C.\nWang,\nJ.\nThompson,\nand\nB.\nLee,\n“Data\nFormulator:\nAi-powered concept-driven visualization authoring,” preprint\narXiv:2309.10094, 2023.\n[38] L. Wang, S. Zhang, Y. Wang, E. Lim, and Y. Wang, “LLM4Vis: Ex-\nplainable visualization recommendation using chatgpt,” preprint\narXiv:2310.07652, 2023.\n[39] H. Ko, H. Jeon, G. Park, D. H. Kim, N. W. Kim, J. Kim,\nand J. Seo, “Natural language dataset generation framework\nfor visualizations powered by large language models,” preprint\narXiv:2309.10245, 2023.\n[40] F. Cassano, J. Gouwar, D. Nguyen, S. Nguyen, L. Phipps-Costin,\nD. Pinckney, M.-H. Yee, Y. Zi, C. J. Anderson, M. Q. Feldman et al.,\n“MultiPL-E: A scalable and extensible approach to benchmarking\nneural code generation,” preprint arXiv:2208.08227, 2022.\n[41] J. Liu, C. S. Xia, Y. Wang, and L. Zhang, “Is your code generated\nby chatgpt really correct? rigorous evaluation of large language\nmodels for code generation,” preprint arXiv:2305.01210, 2023.\n[42] H. Ding, V. Kumar, Y. Tian, Z. Wang, R. Kwiatkowski, X. Li,\nM. K. Ramanathan, B. Ray, P. Bhatia, and S. Sengupta, “A static\nevaluation of code completion by large language models,” in Proc.\nCon. Association for Computational Linguistics (ACL), 2023, pp. 347–\n360.\n[43] Y. Fu, L. Ou, M. Chen, Y. Wan, H. Peng, and T. Khot, “Chain-of-\nthought hub: A continuous effort to measure large language mod-\nels’ reasoning performance,” preprint arXiv:abs/2305.17306, 2023.\n[44] F. Xu, Q. Lin, J. Han, T. Zhao, J. Liu, and E. Cambria, “Are large\nlanguage models really good logical reasoners? a comprehen-\nsive evaluation from deductive, inductive and abductive views,”\npreprint arXiv:2306.09841, 2023.\n[45] T. Wei, J. Luan, W. Liu, S. Dong, and B. Wang, “CMATH: can your\nlanguage model pass chinese elementary school math test?” 2023.\n[46] S. Frieder, L. Pinchetti, A. Chevalier, R.-R. Griffiths, T. Salvatori,\nT. Lukasiewicz, P. C. Petersen, and J. Berner, “Mathematical Capa-\nbilities of ChatGPT,” preprint arXiv:2301.13867, 2023.\n[47] Y. Wu, F. Jia, S. Zhang, H. Li, E. Zhu, Y. Wang, Y. T. Lee, R. Peng,\nQ. Wu, and C. Wang, “An empirical study on challenging math\nproblem solving with GPT-4,” preprint arXiv:2306.01337, 2023.\n[48] K. M. Collins, A. Q. Jiang, S. Frieder, L. Wong, M. Zilka, U. Bhatt,\nT. Lukasiewicz, Y. Wu, J. B. Tenenbaum, W. Hart, T. Gowers,\nW. Li, A. Weller, and M. Jamnik, “Evaluating language models\nfor mathematics through interactions,” preprint arXiv:2306.01694,\n2023.\n[49] X. Dao and N. Le, “Investigating the effectiveness of chatgpt in\nmathematical reasoning and problem solving: Evidence from the\nvietnamese national high school graduation examination,” preprint\narXiv:2306.06331, 2023.\n[50] T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa, “Large\nlanguage models are zero-shot reasoners,” Advances in neural\ninformation processing systems, vol. 35, pp. 22 199–22 213, 2022.\n[51] Y. Fu, H. Peng, A. Sabharwal, P. Clark, and T. Khot, “Complexity-\nbased prompting for multi-step reasoning,” in Proc. Int. Con.\nLearning Representations (ICLR), 2023.\n[52] P. Lu, L. Qiu, K. Chang, Y. N. Wu, S. Zhu, T. Rajpurohit, P. Clark,\nand A. Kalyan, “Dynamic prompt learning via policy gradient\nfor semi-structured mathematical reasoning,” in Proc. Int. Con.\nLearning Representations (ICLR), 2023.\n[53] X. Wang, J. Wei, D. Schuurmans, Q. V. Le, E. H. Chi, S. Narang,\nA. Chowdhery, and D. Zhou, “Self-consistency improves chain of\nthought reasoning in language models,” in Proc. Int. Con. Learning\nRepresentations (ICLR), 2023.\n[54] Y. Weng, M. Zhu, F. Xia, B. Li, S. He, K. Liu, and J. Zhao, “Large\nLanguage Models are Better Reasoners with Self-Verification,”\npreprint arXiv:2212.09561, 2023.\n15\n[55] Y. Li, Z. Lin, S. Zhang, Q. Fu, B. Chen, J.-G. Lou, and W. Chen,\n“Making language models better reasoners with step-aware veri-\nfier,” in Proc. Con. Association for Computational Linguistics (ACL),\n2023, pp. 5315–5333.\n[56] A. Creswell, M. Shanahan, and I. Higgins, “Selection-Inference:\nExploiting Large Language Models for Interpretable Logical Rea-\nsoning,” arXiv:2205.09712, 2022.\n[57] L. Wang, W. Xu, Y. Lan, Z. Hu, Y. Lan, R. K. Lee, and E. Lim, “Plan-\nand-solve prompting: Improving zero-shot chain-of-thought rea-\nsoning by large language models,” in Proc. Con. Association for\nComputational Linguistics (ACL), 2023, pp. 2609–2634.\n[58] P. Yin, W. Li, K. Xiao, A. Rao, Y. Wen, K. Shi, J. Howland,\nP. Bailey, M. Catasta, H. Michalewski, O. Polozov, and C. Sutton,\n“Natural language to code generation in interactive data science\nnotebooks,” in Proc. Con. Association for Computational Linguistics\n(ACL), 2023, pp. 126–173.\n[59] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, L. Kaiser, and I. Polosukhin, “Attention is all you need,”\nin Proc. Conf. Neural Information Processing Systems (NeurIPS), 2017,\npp. 5998–6008.\n[60] “Vega-lite document,” https://vega.github.io/vega-lite/docs/.\n[61] J. Mackinlay, “Automating the design of graphical presentations\nof relational information,” ACM Transactions on Graphics, vol. 5,\nno. 2, p. 110–141, 1986.\n[62] Y. Kim and J. Heer, “Assessing effects of task and data distribu-\ntion on the effectiveness of visual encodings,” Computer Graphics\nForum, vol. 37, no. 3, pp. 157–167, 2018.\n",
  "categories": [
    "cs.HC"
  ],
  "published": "2024-01-20",
  "updated": "2024-01-20"
}