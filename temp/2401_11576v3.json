{
  "id": "http://arxiv.org/abs/2401.11576v3",
  "title": "Quantum Architecture Search with Unsupervised Representation Learning",
  "authors": [
    "Yize Sun",
    "Zixin Wu",
    "Yunpu Ma",
    "Volker Tresp"
  ],
  "abstract": "Unsupervised representation learning presents new opportunities for advancing\nQuantum Architecture Search (QAS) on Noisy Intermediate-Scale Quantum (NISQ)\ndevices. QAS is designed to optimize quantum circuits for Variational Quantum\nAlgorithms (VQAs). Most QAS algorithms tightly couple the search space and\nsearch algorithm, typically requiring the evaluation of numerous quantum\ncircuits, resulting in high computational costs and limiting scalability to\nlarger quantum circuits. Predictor-based QAS algorithms mitigate this issue by\nestimating circuit performance based on structure or embedding. However, these\nmethods often demand time-intensive labeling to optimize gate parameters across\nmany circuits, which is crucial for training accurate predictors. Inspired by\nthe classical neural architecture search algorithm Arch2vec, we investigate the\npotential of unsupervised representation learning for QAS without relying on\npredictors. Our framework decouples unsupervised architecture representation\nlearning from the search process, enabling the learned representations to be\napplied across various downstream tasks. Additionally, it integrates an\nimproved quantum circuit graph encoding scheme, addressing the limitations of\nexisting representations and enhancing search efficiency. This predictor-free\napproach removes the need for large labeled datasets. During the search, we\nemploy REINFORCE and Bayesian Optimization to explore the latent representation\nspace and compare their performance against baseline methods. Our results\ndemonstrate that the framework efficiently identifies high-performing quantum\ncircuits with fewer search iterations.",
  "text": "QUANTUM ARCHITECTURE SEARCH WITH\nUNSUPERVISED REPRESENTATION LEARNING\nYize Sun1,2,3,∗\nZixin Wu1,∗\nYunpu Ma1,3,†\nVolker Tresp1,2,3\n∗Equal contribution\n† Corresponding author\n1 Ludwig-Maximilians-University Munich\n2 MCML\n3Siemens AG\nMunich, Germany\nyize.sun@campus.lmu.de,\nwzx19990414@gmail.com,\ncognitive.yunpu@gmail.com,\nvolker.tresp@lmu.de\nABSTRACT\nUnsupervised representation learning presents new opportunities for advanc-\ning Quantum Architecture Search (QAS) on Noisy Intermediate-Scale Quantum\n(NISQ) devices. QAS is designed to optimize quantum circuits for Variational\nQuantum Algorithms (VQAs). Most QAS algorithms tightly couple the search\nspace and search algorithm, typically requiring the evaluation of numerous quan-\ntum circuits, resulting in high computational costs and limiting scalability to larger\nquantum circuits. Predictor-based QAS algorithms mitigate this issue by estimat-\ning circuit performance based on structure or embedding. However, these meth-\nods often demand time-intensive labeling to optimize gate parameters across many\ncircuits, which is crucial for training accurate predictors. Inspired by the classi-\ncal neural architecture search algorithm Arch2vec, we investigate the potential of\nunsupervised representation learning for QAS without relying on predictors. Our\nframework decouples unsupervised architecture representation learning from the\nsearch process, enabling the learned representations to be applied across various\ndownstream tasks. Additionally, it integrates an improved quantum circuit graph\nencoding scheme, addressing the limitations of existing representations and en-\nhancing search efficiency. This predictor-free approach removes the need for large\nlabeled datasets. During the search, we employ REINFORCE and Bayesian Opti-\nmization to explore the latent representation space and compare their performance\nagainst baseline methods. Our results demonstrate that the framework efficiently\nidentifies high-performing quantum circuits with fewer search iterations.\n1\nINTRODUCTION\nQuantum Computing (QC) has made significant progress over the past decades. Advances in quan-\ntum hardware and new quantum algorithms have demonstrated potential advantages (Stein et al.,\n2023) over classical computers in various tasks, such as image processing (Wang et al., 2022), rein-\nforcement learning (Skolik et al., 2022), knowledge graph embedding (Ma et al., 2019), and network\narchitecture search (Zhang et al., 2022; Giovagnoli et al., 2023; Du et al., 2022). However, the scale\nof quantum computers is still limited by environmental noise, which leads to unstable performance.\nThese noisy intermediate-scale quantum (NISQ) devices lack fault tolerance, which is not expected\nto be achieved in the near future (Preskill, 2018). The variational quantum algorithm (VQA), a hy-\nbrid quantum algorithm that utilizes quantum operations with adjustable parameters, is considered\na leading strategy in the NISQ era (Cerezo et al., 2021). In VQA, the parameterized quantum cir-\ncuit (PQC) with trainable parameters is viewed as a general paradigm of quantum neural networks\nand has achieved notable success in quantum machine learning. These parameters control quantum\ncircuit operations, adjusting the distribution of circuit output states, and are updated by a classical\noptimizer based on a task-specific objective function. Although VQA faces challenges such as Bar-\nren Plateaus (BP) and scalability issues, it has demonstrated the potential to improve performance\nacross various domains, including image processing, combinatorial optimization, chemistry, and\nphysics (Pramanik et al., 2022; Amaro et al., 2022; Tilly et al., 2022). One example of a VQA is the\nvariational quantum eigensolver (VQE) (Peruzzo et al., 2014; Tilly et al., 2022), which approximates\n1\narXiv:2401.11576v3  [quant-ph]  23 Oct 2024\nthe ground state and offers flexibility for quantum machine learning. We are considering using VQE\nto evaluate the performance of certain quantum circuits.\nUnsupervised representation learning seeks to discover hidden patterns or structures within unla-\nbeled data, a well-studied problem in computer vision research (Radford et al., 2015). One common\napproach is the autoencoder, which is effective for feature representation. It consists of an encoder\nand decoder, which first maps images into a compact feature space and then decodes them to recon-\nstruct similar images. Beyond images, autoencoders can also learn useful features from graphs, such\nas encoding and reconstructing directed acyclic graphs (DAGs) or neural network architectures (Yan\net al., 2020; Zhang et al., 2019; Pan et al., 2018; Wang et al., 2016). In most research, architecture\nsearch and representation learning are coupled, which results in inefficient searches heavily depen-\ndent on labeled architectures that require numerous evaluations. The Arch2vec framework aims to\ndecouple representation learning from architecture search, allowing downstream search algorithms\nto operate independently (Yan et al., 2020). This decoupling leads to a smooth latent space that\nbenefits various search algorithms without requiring extensive labeling.\nQuantum architecture search (QAS) or quantum circuit architecture search is a framework for de-\nsigning quantum circuits efficiently and automatically, aiming to optimize circuit performance (Du\net al., 2022). Various algorithms have been proposed for QAS (Zhang et al., 2022; Du et al., 2022;\nZhang et al., 2021; He et al., 2023a; Giovagnoli et al., 2023). However, most algorithms combine\nthe search space and search algorithm, leading to inefficiency and high evaluation costs. The effec-\ntiveness of the search algorithm often depends on how well the search space is defined, embedded,\nand learned. Finding a suitable circuit typically requires evaluating different architectures many\ntimes. Although predictor-based QAS He et al. (2023a) can separate representation learning from\nthe search algorithm, it often relies on labeling different architectures via evaluation, and the training\nperformance depends heavily on the quantity and quality of evaluations and the embedding. In this\nwork, we are inspired by the idea of decoupling, and we aim to conduct QAS without labeling. We\nseek to explore whether decoupling can embed quantum circuit architectures into a smooth latent\nspace, benefiting predictor-free QAS algorithms.We summarise our contributions as follows:\n• We have successfully incorporated decoupling into unsupervised architecture representa-\ntion learning within QAS, significantly improving search efficiency and scalability. By\napplying REINFORCE and Bayesian optimization directly to the latent representation, we\neliminate the need for a predictor trained on large labeled datasets, thereby reducing pre-\ndiction uncertainty.\n• Our proposed quantum circuit encoding scheme overcomes limitations in existing repre-\nsentations, enhancing search performance by providing more accurate and effective em-\nbeddings.\n• Extensive experiments on quantum machine learning tasks, including quantum state prepa-\nration, max-cut, and quantum chemistry (Liang et al., 2019; Poljak & Rendl, 1995; Tilly\net al., 2022), confirm the effectiveness of our framework. The pre-trained quantum archi-\ntecture embeddings significantly enhance QAS across these applications.\n2\nRELATED WORK\nUnsupervised Graph Representation Learning.\nGraph data is becoming a crucial tool for un-\nderstanding complex interactions between real-world entities, such as biochemical molecules (Jiang\net al., 2021), social networks (Shen et al., 2023), purchase networks from e-commerce platforms\n(Li et al., 2021), and academic collaboration networks (Newman, 2001). Graphs are typically rep-\nresented as discrete data structures, making it challenging to solve downstream tasks due to large\nsearch spaces. Our work focuses on unsupervised graph representation learning, which seeks to\nembed graphs into low-dimensional, compact, and continuous representations without supervision\nwhile preserving the topological structure and node attributes. In this domain, approaches such\nas those proposed by Perozzi et al. (2014); Wang et al. (2016); Grover & Leskovec (2016); Tang\net al. (2015) use local random walk statistics or matrix factorization-based objectives to learn graph\nrepresentations. Alternatively, methods like Kipf & Welling (2016); Hamilton et al. (2017) recon-\nstruct the graph’s adjacency matrix by predicting edge existence, while others, such as Veliˇckovi´c\net al. (2018); Sun et al. (2019); Peng et al. (2020), maximize the mutual information between local\nnode representations and pooled graph representations. Additionally, Xu et al. (2019) investigate\n2\nthe expressiveness of Graph Neural Networks (GNNs) in distinguishing between different graphs\nand introduce Graph Isomorphism Networks (GINs), which are shown to be as powerful as the\nWeisfeiler-Lehman test (Leman & Weisfeiler, 1968) for graph isomorphism. Inspired by the suc-\ncess of Arch2vec (Yan et al., 2020), which employs unsupervised graph representation learning for\nclassical neural architecture search (NAS), we adopt GINs to injectively encode quantum architec-\nture structures, as quantum circuit architectures can also be represented as DAGs.\nQuantum Architecture Search (QAS).\nAs discussed in the previous section, PQCs are essential\nas ansatz for various VQAs (Benedetti et al., 2019). The expressive power and entangling capacity\nof PQCs play a crucial role in their optimization performance (Sim et al., 2019). Poorly designed\nansatz can suffer from limited expressive power or entangling capacity, making it difficult to reach\nthe global minimum for an optimization problem. Moreover, such ansatz may be more prone to noise\n(Stilck Franc¸a & Garcia-Patron, 2021), inefficiently utilize quantum resources, or lead to barren\nplateaus that hinder the optimization process (McClean et al., 2018; Wang et al., 2021). To address\nthese challenges, QAS has been proposed as a systematic approach to identify optimal PQCs. The\ngoal of QAS is to efficiently and effectively search for high-performance quantum circuits tailored\nto specific problems, minimizing the loss functions while adhering to constraints such as hardware\nqubit connections, native quantum gate sets, quantum noise models, training loss landscapes, and\nother practical considerations. Quantum architectures share many properties with neural network\narchitectures, such as hierarchical, directed, and acyclic structures. As a result, QAS methods have\nbeen heavily inspired by techniques from NAS. Specifically, approaches such as greedy algorithms\n(Mitarai et al., 2018; Tang et al., 2021), evolutionary or genetic methods (Zhang & Zhao, 2022;\nDing & Spector, 2022), RL-based engines (Kuo et al., 2021; Ostaszewski et al., 2021), Bayesian\noptimization (Duong et al., 2022), and gradient-based methods (Zhang et al., 2022) have all been\nemployed to discover improved PQCs for VQAs. However, these methods require the evaluation\nof numerous quantum circuits, which is both time-consuming and computationally expensive. To\nmitigate this issue, predictor-based approaches (Zhang et al., 2021; He et al., 2023b) have been\nintroduced, but they also face limitations. These approaches rely on large sets of labeled circuits\nto train predictors with generalized capabilities and introduce additional uncertainty into the search\nprocess, necessitating the reevaluation of candidate circuits. In this work, we propose a framework\naimed at further addressing these challenges.\n3\nQAS WITH UNSUPERVISED REPRESENTATION LEARNING\nIn this work, we present our method, as illustrated in Figure 1, which consists of two indepen-\ndent learning components: an autoencoder for circuit architecture representation learning, and a\nsearch process that includes both search and evaluation strategies. The search space is defined\nby the number of gates in a circuit and an operation pool comprising general gate types such as\nX, Y, Z, H, Rx, Ry, Rz, U3, CNOT, CY, CZ. A random generator creates a set of\ncircuit architectures based on predefined parameters, including the number of qubits, the number\nof gates, and the maximum circuit depth. These architectures are then encoded into two matrices\nand input into the autoencoder. The autoencoder independently learns a latent distribution from\nthe search space and produces pre-trained architecture embeddings for the search algorithms. The\nevaluation strategy takes the circuit architectures generated by the search algorithm and returns a\nperformance assessment. For evaluating circuit architectures, we use the ground state of a Hamilto-\nnian for max-cut and quantum chemistry problems, and fidelity for quantum state preparation tasks.\n3.1\nCIRCUIT ENCODING SCHEME\nWe represent quantum circuits as DAGs using the circuit encoding scheme EGSQAS, as described in\nHe et al. (2023b;a). Each circuit is transformed into a DAG by mapping the gates on each qubit to a\nsequence of nodes, with two additional nodes added to indicate the input and output of circuits. The\nresulting DAG is described by an adjacency matrix, as shown in Figure 1a. The set of nodes is further\ncharacterized by a gate matrix, which shows the node features including position information.\nHowever, the encoding scheme EGSQAS represents all occupied qubits as 1 without distinguishing\nbetween the control and target positions of two-qubit gates, which limits the effectiveness of cir-\ncuit representation learning and leads to confusion during circuit reconstruction. Additionally, the\n3\n(a) Architecture encoding scheme\n(b) Representation learning and search process\nFigure 1: Illustration of our algorithm. In Figure 1a, each circuit’s architecture is first transformed\ninto a DAG and represented by two matrices. Each row of the gate matrix corresponds to a node in\nthe graph, with one-hot encoding used to indicate the node type, and additional columns encoding\nposition information, such as the qubits the gate acts on. For two-qubit gates, −1 and 1 represent\nthe control and target qubits, respectively. The weights in the adjacency matrix reflect the number of\nqubits involved in each interaction. In Figure 1b, the left side depicts the process of representation\nlearning, where Z represents the latent space of circuit architectures. In the middle, the encoder\nis shown as the mechanism used to learn this latent space. On the right, Bayesian optimization\n(BO) and reinforcement learning (RL) are employed to explore the latent space for various quantum\nmachine learning tasks. The algorithm ultimately outputs a set of candidate circuits.\nadjacency matrix weights do not accurately reflect the original gate connections. To address these\nlimitations, we propose a new encoding scheme. In our method, we explicitly encode positional\ninformation for two-qubit gates, such as CNOT and CZ, by assigning −1 to the control qubit and\n1 to the target qubit. Furthermore, we represent the number of qubits involved in an edge as the\nconnection weights in the adjacency matrix, as shown in Figure 1a. These modifications enhance\ncircuit representation learning and improve the overall effectiveness of the search.\n3.2\nVARIATIONAL GRAPH ISOMORPHISM AUTOENCODER\n3.2.1\nPRELIMINARIES\nThe most common graph autoencoders (GAEs) consist of an encoder and a decoder, where the en-\ncoder maps a graph into a feature space, and the decoder reconstructs the graph from those features.\nOne prominent example is the variational graph autoencoder (VGAE), a promising framework for\nunsupervised graph representation learning that utilizes a graph convolutional network as its encoder\nand a simple inner product as its decoder (Kipf & Welling, 2016). In this work, however, we do not\nemploy the common VGAE as a framework for learning latent representations. Instead, we utilize a\nmore powerful encoder GIN (Xu et al., 2019).\nDefinition 1. We are given a circuit created by m gate types, h gates and g qubits. Then, the circuit\ncan be described by a DAG G = {V, E} with n = h + 2 = |V | gate nodes including START and\nEND. The adjacency matrix of graph G is summarized in n × n matrix A and its gate matrix X is\nin size of n × (m + 2 + g). We further introduce d-dimensional latent variables zi composing latent\nmatrix Z = {z1, .., zK}T .\n3.2.2\nENCODER\nThe encoder GIN maps the structure and node features to latent representations Z. An approxima-\ntion of the posterior distribution q(Z|X, A) is:\nq(Z|X, A) =\nK\nY\ni=1\nq(zi|X, A),\n(1)\n4\nwhere q(zi|X, A) = N(zi|µi, diag(σ2\ni )). The L-layer GIN generates the embedding matrix M (s)\nfor s-layer by:\nM (s) = MLP (s)((1 + ϵ(s)) · M (s−1) + ˆAM (s−1)), s = 1, 2, ..., L,\n(2)\nWhere M (0) = X, and ϵ(s) is a bias with a standard normal distribution for each layer. The MLP\nis a multi-layer perceptron consisting of Linear-BatchNorm-LeakyReLU layers, and ˆA = A + AT\ntransforms a directed graph into an undirected one to capture bi-directional information. In this\nwork, we introduce a new fusion layer, a fully connected layer that aggregates feature information\nfrom all GIN layers, rather than just the last one. The mean µ = GINµ(X, ˆA) = FC1(M (L)) is\ncomputed using the fully connected layer FC1, and similarly, the standard deviation σ is computed\nvia FC2. We can then sample the latent matrix Z ∼q(Z|X, A) by zi = µi + σi · ϵi. For all\nexperiments, we use L = 5 GIN layers, a 16-dimensional latent vector zi, and a GIN encoder with\nhidden sizes of 128. More details on the hyperparameters can be found in Appendix A.3.\n3.2.3\nDECODER\nThe decoder takes the sampled latent variables Z as input to reconstruct both the adjacency matrix\nA and the gate matrix X = [Xt, Xq], where Xt encodes the gate types and Xq encodes the qubits\non which the gates act. The generative process is summarized as follows:\np(A|Z) =\nK\nY\ni=1\nK\nY\nj=1\np(Aij|zi, zj), with p(Aij|zi, zj) = ReLUj(F1(zT\ni zj)),\n(3)\np(X|Z) =\nK\nY\ni=1\np(xi|zi), with p(xt\ni|zi) = softmax(F2(zi)), p(xq\ni |zi) = tanh(F2(zi)),\n(4)\nwhere both F1 and F2 are trainable linear functions.\n3.2.4\nOBJECTIVE FUNCTION\nThe weights in the encoder and decoder are optimized by maximizing the evidence lower bound\n(ELBO) L, which is defined as:\nL = Eq(Z|X,A)[log p(Xtype, Xqubit, A|Z)] −KL[(q(Z|X, A))||p(Z)],\n(5)\nwhere KL[q(·)||p(·)] represents the Kullback-Leibler (KL) divergence between q(·) and p(·). We\nfurther adopt a Gaussian prior p(Z) = Q\ni N(zi|0, I). The weights are optimized using minibatch\ngradient descent, with a batch size of 32.\n3.3\nARCHITECTURE SEARCH STRATEGIES\n3.3.1\nREINFORCEMENT LEARNING (RL)\nAfter conducting initial trials with PPO (Schulman et al., 2017) and A2C (Huang et al., 2022), we\nadopt REINFORCE (Williams, 1992) as a more effective reinforcement learning algorithm for archi-\ntecture search. In this approach, the environment’s state space consists of pre-trained embeddings,\nand the agent uses a one-cell LSTM as its policy network. The agent selects an action, corresponding\nto a sampled latent vector based on the distribution of the current state, and transitions to the next\nstate based on the chosen action. The reward for max-cut and quantum chemistry tasks is defined\nas the ratio of energy to ground energy, with values outside the range [0, 1] clipped to 0 or 1. For\nthe state preparation task, circuit fidelity is used as the reward. We employ an adaptive batch size,\nwith the number of steps per training epoch determined by the average reward of the previous epoch.\nAdditionally, we use a linear adaptive baseline, defined by the formula B = α · B + (1 −α) · Ravg,\nwhere B denotes the baseline, α is a predefined value in the range [0,1], and Ravg is the average\nreward. Each run in this work involves 1000 searches.\n3.3.2\nBAYESIAN OPTIMIZATION (BO)\nAs another search strategy used in this work without labeling, we employ Deep Networks for Global\nOptimization (DNGO)(Snoek et al., 2015) in the context of BO. We adopt a one-layer adaptive BO\n5\nregression model with a basis function extracted from a feed-forward neural network, consisting\nof 128 units in the hidden layer, to model distributions over functions. Expected Improvement\n(EI)(Mockus, 1977) is selected as the acquisition function. EI identifies the top-k embeddings for\neach training epoch, with a default objective value of 0.9. The training begins with an initial set of\n16 samples, and in each subsequent epoch, the top-k architectures proposed by EI are added to the\nbatch. The network is retrained for 100 epochs using the architectures from the updated batch. This\nprocess is iterated until the predefined number of search iterations is reached.\n4\nEXPERIMENTAL RESULTS\nTo demonstrate the effectiveness and generalization capability of our approach, we conduct exper-\niments on three well-known quantum computing applications: quantum state preparation, max-cut,\nand quantum chemistry. For each application, we start with a simple example involving 4 qubits and\nthen progress to a more complex example with 8 qubits. We utilize a random generator to create\n100,000 circuits as the search space, and all experiments are performed on a noise-free simulator\nduring the search process. Detailed settings are provided in Appendix A.2. We begin by evaluating\nthe model’s pre-training performance for unsupervised representation learning (§4.1), followed by\nan assessment of QAS performance based on the pre-trained latent representations (§4.2).\n4.1\nPRE-TRAINING PERFORMANCE\nObservation (1): GAE and VGAE (Kipf & Welling, 2016) are two popular baselines for NAS. In\nan attempt to find models capable of capturing superior latent representations of quantum circuit\narchitectures, we initially applied these two well-known models. However, due to the increased\ncomplexity of quantum circuit architectures compared to neural network architectures, these models\nfailed to deliver the expected results. In contrast, models based on GINs (Xu et al., 2019) success-\nfully obtained valid latent representations, attributed to their more effective neighbor aggregation\nscheme. Table 1 presents a performance comparison between the original model using the EGSQAS\nencoding and the improved model with our enhanced encoding for 4, 8, and 12 qubit circuits, eval-\nuated across five metrics: Accuracyops, which measures the reconstruction accuracy of gate types\nin the gate matrix for the held-out test set; Accuracyqubit, which reflects the reconstruction accu-\nracy of qubits that the gates act on; Accuracyadj, which measures the reconstruction accuracy of\nthe adjacency matrix; Falposmean, which represents the mean false positives in the reconstructed\nadjacency matrix; and KLD (KL divergence), which indicates the continuity and smoothness of the\nlatent representation. The results in the table indicate that the improved model with our enhanced\nencoding achieves comparable or better than the original. This improvement can be attributed to\ntwo factors: first, the new encoding better captures the specific characteristics of the circuits, and\nsecond, the fusion of outputs from multiple layers of GIN helps retain shallow information, resulting\nin more stable training.\nQubit\nModel\nMetric\nAccuracyops\nAccuracyqubit\nAccuracyadj\nFalposmean\nKLD\n4\nGSQAS\n99.99\n99.99\n99.91\n100.00\n0.061\n4\nOurs\n100\n99.97\n98.89\n23.41\n0.045\n8\nGSQAS\n86.69\n99.98\n99.82\n100.00\n0.038\n8\nOurs\n100\n98.65\n97.34\n7.35\n0.029\n12\nGSQAS\n86.69\n99.94\n99.70\n100.00\n0.028\n12\nOurs\n98.67\n99.14\n97.79\n4.75\n0.022\nTable 1: Pretraining model performance of 4-, 8-, and 12-qubit circuits across the four metrics.\nObservation (2): In Figure 2, we employ two popular techniques, PCA (Shlens, 2014) and t-SNE\n(Van der Maaten & Hinton, 2008), to visualize the high-dimensional latent representations of 4-\nand 12-qubit quantum machine learning (QML) applications based on our pre-trained models. The\nresults highlight the effectiveness of our new encoding approach for unsupervised clustering and\nhigh-dimensional data visualization. The figures show that the latent representation space of quan-\ntum circuits is smooth and compact, with architectures of similar performance clustering together\nwhen the search space is limited to 4 qubits. Notably, high-performance quantum circuit architec-\n6\ntures are concentrated on the right side of the visualizations. In particular, PCA yields exceptionally\nsmooth and compact representations with strong clustering effects, making it easier and more effi-\ncient to conduct QAS within such a structured latent space. This provides a robust foundation for\nour QAS algorithms.\nFor the 12-qubit latent space, high-performance circuits (shown in red) are less prominent, likely\ndue to the fact that the 100,000 circuit structures represent only a finite subset of the possibilities\nfor 12-qubit circuit. As a result, the number of circuits that can be learned is limited. Most high-\nperformance circuits are distributed along the left edge of the latent space, with a color gradient\ntransitioning from dark to light as one moves from right to left.\nCompared with subfigures 2i, 2j, 2k, 2l, 2m, and 2k, which utilize the encoding scheme EGSQAS\nand show more loosely distributed red points, our new encoding results in a more concentrated and\nsmoother latent representation, as demonstrated in subfigures 2a, 2b and 2c.\n(a) PCA4 QCH2\n(b) PCA4 Max-cut\n(c) PCA4 Fidelity\n(d) PCA12 QCLiH\n(e) t-SNE4 QCH2\n(f) t-SNE4 Max-cut\n(g) t-SNE4 Fidelity\n(h) t-SNE12 QCLiH\n(i) PCA4 Q\n(j) PCA4 M\n(k) PCA4 F\n(l) t-SNE4 Q\n(m) t-SNE4 M\n(n) t-SNE4 F\nFigure 2: The 2D smooth visualizations of the latent representations for the 4- and 12-qubit cases,\nusing PCA and t-SNE. The color encoding reflects the achieved energy of 100,000 randomly gener-\nated circuits. These latent representations are introduced for three QML tasks: Quantum Chemistry,\nMax-cut, and fidelity. The graphs illustrate the energy or fidelity distribution of the circuits, where\nred denotes circuits with an energy lower than −0.80/ −0.90/ −7.01, Ha or a fidelity higher than\n0.5. The subfigures in the first two rows display the results of our model with KL divergence, while\nthe subfigures at the bottom visualize the 4-qubit latent space using the existing encoding scheme\nEGSQAS.\n4.2\nQUANTUM ARCHITECTURE SEARCH (QAS) PERFORMANCE\nObservation (1): In Figure 3, we present the average reward per 100 searches for each experiment.\nThe results show that both the REINFORCE and BO methods effectively learn to navigate the latent\nrepresentation, leading to noticeable improvements in average reward during the early stages. In\ncontrast, Random Search fails to achieve similar improvements. Furthermore, although the plots\nindicate slightly higher variance in the average reward for the REINFORCE and BO methods com-\npared to Random Search, their overall average reward is significantly higher than that of Random\nSearch.\nObservation (2): In Figure 4, we illustrate the number of candidate circuits found to achieve a preset\nthreshold after performing 1000 searches using the three search methods. The results show that the\n7\n(a) State preparation\n(b) Max-cut\n(c) Quantum chemistry\nFigure 3: Average rewards from the six sets of experiments. In subfigures (a), (b), and (c), the\nleft panels show results from the 4-qubit experiments, while the right panels show results from the\n8-qubit experiments. Each plot presents the average reward across 50 independent runs (each with\ndifferent random seeds) given 1000 search queries. The shaded areas in the plots represent the\nstandard deviation of the average rewards.\n8-qubit experiments are more complex, resulting in fewer circuits meeting the requirements within\nthe search space. Additionally, within a limited number of search iterations, both the REINFORCE\nand BO methods are able to discover a greater number of candidate circuits that meet the threshold,\neven in the worst case, i.e., when comparing the minimal number of candidates. Notably, their\nperformance significantly surpasses that of the Random Search method, especially REINFORCE,\ndespite the fact that the difference between the minimal and maximal number of candidates indicates\nthat REINFORCE is more sensitive to the initial conditions compared to the other two approaches.\nThese findings highlight the clear improvements and advantages introduced by QAS based on the\nlatent representation, which enables the efficient discovery of numerous high-performance candidate\ncircuits while reducing the number of searches required.\n(a) 4-qubit experiments\n(b) 8-qubit experiments\nFigure 4: The candidate quantities for the 4-qubit and 8-qubit applications. RS, RL, and BO refer\nto Random Search, REINFORCE, and Bayesian Optimization, respectively. The reward threshold\nfor all 4-qubit experiments is 0.95, while for the more complex 8-qubit experiments, the thresholds\nare softer: 0.75 for state preparation, 0.925 for max-cut, and 0.95 for quantum chemistry. Each\nexperiment is performed with 1000 queries, meaning only 1000 samples are drawn from a search\nspace of 100,000 circuits. Additionally, the left-hand side of subfigures (a) and (b) shows the average\nresults over 50 runs (with different random seeds), while the right-hand side shows the maximum\nand minimum candidate quantities across the 50 runs.\nObservation (3): In Table 2, we compare various QAS methods with our approach on the 4-qubit\nstate preparation task, using a circuit space of 100,000 circuits and limiting the search to 1000\nqueries. GNNURL and GSQASURL represent predictor-based methods from He et al. (2023b) and\nHe et al. (2023a), respectively, both employing our pre-trained model. QASURL\nRL(BO) denotes the\nQAS approach with REINFORCE (BO) used in this work. The average results over 50 runs indi-\ncate that both the predictor-based methods and our approach are capable of identifying a significant\nnumber of high-performance circuits with fewer samples. However, predictor-based methods rely\non labeled circuits to train predictors, introducing uncertainty as they may inadvertently filter out\nwell-performing architectures along with poor ones. While a higher Fthr value filters out more low-\nperformance circuits, increasing the proportion of good architectures in the filtered space, it also\nsacrifices many well-performing circuits, which can lead to improved Random Search performance\n8\nMethod\nT ask\nFthr\nNlbl\nNrest\nN>0.95\nNeval\nNQAS\nNQAS/Neval\nGNNURL\nFidelity\n0.5\n1000\n21683\n780\n2000\n36\n0.0180\nMax-Cut\n0.9\n1000\n45960\n35967\n2000\n783\n0.3915\nQC-4H2\n0.8\n1000\n65598\n18476\n2000\n278\n0.1390\nGSQASURL\nFidelity\n0.5\n1000\n21014\n768\n2000\n37\n0.0185\nMax-Cut\n0.9\n1000\n43027\n33686\n2000\n785\n0.3925\nQC-4H2\n0.8\n1000\n30269\n19889\n2000\n658\n0.3290\nRandom Search\nFidelity\n-\n0\n100000\n1606\n1000\n15\n0.0150\nMax-Cut\n-\n0\n100000\n57116\n1000\n568\n0.5680\nQC-4H2\n-\n0\n100000\n37799\n1000\n371\n0.3710\nQASURL\nRL(BO)\nFidelity\n-\n0\n100000\n1606\n1000\n69(63)\n0.0690(0.0630)\nMax-Cut\n-\n0\n100000\n57116\n1000\n898(820)\n0.8980(0.8200)\nQC-4H2\n-\n0\n100000\n37799\n1000\n817(739)\n0.8170(0.7390)\nTable 2: Compare the QAS performance of different QAS methods for the 4-qubit tasks. URL\ndenotes unsupervised representation learning, Fthr is the threshold to filter poor-performance archi-\ntectures, Nlbl, Nrest and N>0.95 refer to the number of required labeled circuits, rest circuits after\nfiltering and the circuits that achieve the performance higher than 0.95 in the rest circuits respec-\ntively. Neval represents the number of evaluated circuits, i.e. the sum of the number of labeled and\nsampled circuits, NQAS is the number of searched candidates in average of 50 runs.\nMethod\nEncoding E\nNrest\nNeval\nNQAS\nNQAS/Neval\nGSQAS4\nGSQAS\n25996\n2000\n625\n0.3125\nOurs\n30269\n2000\n658\n0.3290\nGSQAS12\nGSQAS\n60088\n2000\n283\n0.1415\nOurs\n60565\n2000\n276\n0.1380\nQASRL−4\nGSQAS\n100000\n1000\n760\n0.7600\nOurs\n100000\n1000\n817\n0.8170\nQASRL−8\nGSQAS\n100000\n1000\n160\n0.1600\nOurs\n100000\n1000\n167\n0.1670\nQASRL−12\nGSQAS\n100000\n1000\n422\n0.4220\nOurs\n100000\n1000\n392\n0.3920\nTable 3: We compare the QAS performance of different encodings using various search methods.\nFor the 4- and 12-qubit quantum chemistry tasks, we select H2 and LiH, respectively, while for the\n8-qubit task, we use the TFIM. The results represent the average of 50 runs.\nbut at the cost of excluding some optimal circuits. Despite these trade-offs, our method achieves\ncomparable performance to predictor-based methods, demonstrating higher efficiency in terms of\nNQAS/Neval while requiring fewer circuit evaluations. In Appendix A.4, we present the best can-\ndidate circuits acquired by each of the three methods for every experiment.\nObservation (4): In Table 3, we present the search performance across different frameworks and\nencoding methods, focusing on 4-, 8-, and 12-qubit quantum chemistry tasks for comparison. In\nmost cases, our encoding method achieves the highest search efficiency, although the performance\nfor the 12-qubit task is slightly lower than with another encoding method. Combined with the\nrepresentation learning results in Figure 2, we observe that the search is significantly more efficient\nwhen the learned circuit representation is smooth and concentrated. For the 12-qubit experiments,\nthe circuits used for representation learning may be insufficient to fully capture the search space,\nleading to representation learning failures, as shown in Figure 2d, and resulting in a decline in\nsearch efficiency.\n5\nCONCLUSION\nInspired by the Arch2vec method (Yan et al., 2020), we focus on exploring whether unsupervised\narchitecture representation learning can enhance QAS. By decoupling unsupervised architecture rep-\nresentation learning from the QAS process, we successfully eliminate the need for a large number\nof labeled circuits. Additionally, our proposed quantum circuit encoding scheme addresses limita-\ntions in existing representations, improving search performance through more accurate and effective\n9\nembeddings. Furthermore, our framework conducts QAS without relying on a predictor by directly\napplying search algorithms, such as REINFORCE and Bayesian Optimization (BO), to the latent\nrepresentations. We have demonstrated the effectiveness of this approach through various experi-\nments. In our framework, the success of QAS depends on the quality of unsupervised architecture\nrepresentation learning and the selection of search algorithms. Thus, we recommend further in-\nvestigation into architecture representation learning for QAS, as well as the development of more\nefficient search strategies within the latent representation space.\nACKNOWLEDGMENTS\nThe project of this workshop paper is supported with funds from the German Federal Ministry of\nEducation and Research in the funding program Quantum Reinforcement Learning for industrial\nApplications (QLindA) - under project number 13N15644 and the Federal Ministry for Economic\nAffairs and Climate Action in the funding program Quantum-Classical Hybrid Optimization Al-\ngorithms for Logistics and Production Line Management (QCHALLenge) - under project number\n01MQ22008B. The sole responsibility for the paper’s contents lies with the authors.\nREFERENCES\nDavid Amaro, Matthias Rosenkranz, Nathan Fitzpatrick, Koji Hirano, and Mattia Fiorentini. A\ncase study of variational quantum algorithms for a job shop scheduling problem. EPJ Quantum\nTechnology, 9(1), feb 2022. doi: 10.1140/epjqt/s40507-022-00123-4.\nMarcello Benedetti, Erika Lloyd, Stefan Sack, and Mattia Fiorentini. Parameterized quantum cir-\ncuits as machine learning models. Quantum Science and Technology, 4(4):043001, 2019.\nMarco Cerezo, Andrew Arrasmith, Ryan Babbush, Simon C Benjamin, Suguru Endo, Keisuke Fu-\njii, Jarrod R McClean, Kosuke Mitarai, Xiao Yuan, Lukasz Cincio, et al. Variational quantum\nalgorithms. Nature Reviews Physics, 3(9):625–644, 2021.\nLi Ding and Lee Spector.\nEvolutionary quantum architecture search for parametrized quantum\ncircuits. In Proceedings of the Genetic and Evolutionary Computation Conference Companion,\npp. 2190–2195, 2022.\nYuxuan Du, Tao Huang, Shan You, Min-Hsiu Hsieh, and Dacheng Tao. Quantum circuit architecture\nsearch for variational quantum algorithms. npj Quantum Information, 8(1):62, 2022.\nTrong Duong, Sang T Truong, Minh Tam, Bao Bach, Ju-Young Ryu, and June-Koo Kevin Rhee.\nQuantum neural architecture search with quantum circuits metric and bayesian optimization.\narXiv preprint arXiv:2206.14115, 2022.\nAlessandro Giovagnoli, Volker Tresp, Yunpu Ma, and Matthias Schubert. Qneat: Natural evolution\nof variational quantum circuit architecture. In Proceedings of the Companion Conference on\nGenetic and Evolutionary Computation, pp. 647–650, 2023.\nAditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In Proceedings\nof the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining,\npp. 855–864, 2016.\nWill Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs.\nAdvances in neural information processing systems, 30, 2017.\nZhimin He, Maijie Deng, Shenggen Zheng, Lvzhou Li, and Haozhen Situ. Gsqas: Graph self-\nsupervised quantum architecture search. arXiv preprint arXiv:2303.12381, 2023a.\nZhimin He, Xuefen Zhang, Chuangtao Chen, Zhiming Huang, Yan Zhou, and Haozhen Situ. A\ngnn-based predictor for quantum architecture search. Quantum Information Processing, 22(2):\n128, 2023b.\nShengyi Huang, Anssi Kanervisto, Antonin Raffin, Weixun Wang, Santiago Onta˜n´on, and Rousslan\nFernand Julien Dossa. A2c is a special case of ppo, 2022.\n10\nJulie Jiang, Li-Ping Liu, and Soha Hassoun. Learning graph representations of biochemical networks\nand its application to enzymatic link prediction. Bioinformatics, 37(6):793–799, 2021.\nThomas N Kipf and Max Welling.\nVariational graph auto-encoders.\narXiv preprint\narXiv:1611.07308, 2016.\nEn-Jui Kuo, Yao-Lung L Fang, and Samuel Yen-Chi Chen. Quantum architecture search via deep\nreinforcement learning. arXiv preprint arXiv:2104.07715, 2021.\nAA Leman and Boris Weisfeiler. A reduction of a graph to a canonical form and an algebra arising\nduring this reduction. Nauchno-Technicheskaya Informatsiya, 2(9):12–16, 1968.\nZongxi Li, Haoran Xie, Guandong Xu, Qing Li, Mingming Leng, and Chi Zhou. Towards purchase\nprediction: A transaction-based setting and a graph-based method leveraging price information.\nPattern Recognition, 113:107824, 2021.\nYeong-Cherng Liang, Yu-Hao Yeh, Paulo EMF Mendonc¸a, Run Yan Teh, Margaret D Reid, and\nPeter D Drummond. Quantum fidelity measures for mixed states. Reports on Progress in Physics,\n82(7):076001, 2019.\nYunpu Ma, Volker Tresp, Liming Zhao, and Yuyi Wang. Variational quantum circuit model for\nknowledge graph embedding. Advanced Quantum Technologies, 2(7-8):1800078, 2019.\nJarrod R McClean, Sergio Boixo, Vadim N Smelyanskiy, Ryan Babbush, and Hartmut Neven. Bar-\nren plateaus in quantum neural network training landscapes. Nature communications, 9(1):4812,\n2018.\nKosuke Mitarai, Makoto Negoro, Masahiro Kitagawa, and Keisuke Fujii. Quantum circuit learning.\nPhysical Review A, 98(3):032309, 2018.\nJonas Mockus. On bayesian methods for seeking the extremum and their application. In IFIP\nCongress, pp. 195–200, 1977.\nMark EJ Newman. The structure of scientific collaboration networks. Proceedings of the national\nacademy of sciences, 98(2):404–409, 2001.\nMateusz Ostaszewski, Lea M Trenkwalder, Wojciech Masarczyk, Eleanor Scerri, and Vedran Dun-\njko. Reinforcement learning for optimization of variational quantum circuit architectures. Ad-\nvances in Neural Information Processing Systems, 34:18182–18194, 2021.\nShirui Pan, Ruiqi Hu, Guodong Long, Jing Jiang, Lina Yao, and Chengqi Zhang. Adversarially\nregularized graph autoencoder for graph embedding. arXiv preprint arXiv:1802.04407, 2018.\nZhen Peng, Wenbing Huang, Minnan Luo, Qinghua Zheng, Yu Rong, Tingyang Xu, and Junzhou\nHuang. Graph representation learning via graphical mutual information maximization. In Pro-\nceedings of The Web Conference 2020, pp. 259–270, 2020.\nBryan Perozzi, Rami Al-Rfou, and Steven Skiena.\nDeepwalk: Online learning of social repre-\nsentations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge\ndiscovery and data mining, pp. 701–710, 2014.\nAlberto Peruzzo, Jarrod McClean, Peter Shadbolt, Man-Hong Yung, Xiao-Qi Zhou, Peter J Love,\nAl´an Aspuru-Guzik, and Jeremy L O’brien. A variational eigenvalue solver on a photonic quan-\ntum processor. Nature communications, 5(1):4213, 2014.\nSvatopluk Poljak and Franz Rendl. Solving the max-cut problem using eigenvalues. Discrete Ap-\nplied Mathematics, 62(1-3):249–278, 1995.\nSayantan Pramanik, M Girish Chandra, C V Sridhar, Aniket Kulkarni, Prabin Sahoo, Chethan\nD V Vishwa, Hrishikesh Sharma, Vidyut Navelkar, Sudhakara Poojary, Pranav Shah, and\nManoj Nambiar.\nA quantum-classical hybrid method for image classification and segmenta-\ntion. In 2022 IEEE/ACM 7th Symposium on Edge Computing (SEC), pp. 450–455, 2022. doi:\n10.1109/SEC54971.2022.00068.\n11\nJohn Preskill. Quantum computing in the nisq era and beyond. Quantum, 2:79, 2018.\nAlec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep\nconvolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\noptimization algorithms, 2017.\nYinghan Shen, Xuhui Jiang, Zijian Li, Yuanzhuo Wang, Chengjin Xu, Huawei Shen, and Xueqi\nCheng. Uniskgrep: A unified representation learning framework of social network and knowledge\ngraph. Neural Networks, 158:142–153, 2023.\nJonathon Shlens. A tutorial on principal component analysis. arXiv preprint arXiv:1404.1100, 2014.\nSukin Sim, Peter D Johnson, and Al´an Aspuru-Guzik. Expressibility and entangling capability of\nparameterized quantum circuits for hybrid quantum-classical algorithms. Advanced Quantum\nTechnologies, 2(12):1900070, 2019.\nAndrea Skolik, Sofiene Jerbi, and Vedran Dunjko. Quantum agents in the gym: a variational quan-\ntum algorithm for deep q-learning. Quantum, 6:720, 2022.\nJasper Snoek, Oren Rippel, Kevin Swersky, Ryan Kiros, Nadathur Satish, Narayanan Sundaram,\nMostofa Patwary, Mr Prabhat, and Ryan Adams.\nScalable bayesian optimization using deep\nneural networks. In International conference on machine learning, pp. 2171–2180. PMLR, 2015.\nJonas Stein, Michael Poppel, Philip Adamczyk, Ramona Fabry, Zixin Wu, Michael K¨olle, Jonas\nN¨ußlein, Dani¨elle Schuman, Philipp Altmann, Thomas Ehmer, et al. Benchmarking quantum\nsurrogate models on scarce and noisy data. arXiv preprint arXiv:2306.05042, 2023.\nDaniel Stilck Franc¸a and Raul Garcia-Patron. Limitations of optimization algorithms on noisy quan-\ntum devices. Nature Physics, 17(11):1221–1227, 2021.\nFan-Yun Sun, Jordan Hoffmann, Vikas Verma, and Jian Tang.\nInfograph: Unsupervised and\nsemi-supervised graph-level representation learning via mutual information maximization. arXiv\npreprint arXiv:1908.01000, 2019.\nHo Lun Tang, VO Shkolnikov, George S Barron, Harper R Grimsley, Nicholas J Mayhall, Ed-\nwin Barnes, and Sophia E Economou. qubit-adapt-vqe: An adaptive algorithm for constructing\nhardware-efficient ans¨atze on a quantum processor. PRX Quantum, 2(2):020310, 2021.\nJian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. Line: Large-scale\ninformation network embedding. In Proceedings of the 24th international conference on world\nwide web, pp. 1067–1077, 2015.\nJules Tilly, Hongxiang Chen, Shuxiang Cao, Dario Picozzi, Kanav Setia, Ying Li, Edward Grant,\nLeonard Wossnig, Ivan Rungger, George H Booth, et al. The variational quantum eigensolver: a\nreview of methods and best practices. Physics Reports, 986:1–128, 2022.\nLaurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine\nlearning research, 9(11), 2008.\nPetar Veliˇckovi´c, William Fedus, William L Hamilton, Pietro Li`o, Yoshua Bengio, and R Devon\nHjelm. Deep graph infomax. arXiv preprint arXiv:1809.10341, 2018.\nJavier Villalba-Diez, Ana Gonz´alez-Marcos, and Joaqu´ın B Ordieres-Mer´e. Improvement of quan-\ntum approximate optimization algorithm for max–cut problems. Sensors, 22(1):244, 2021.\nDaixin Wang, Peng Cui, and Wenwu Zhu. Structural deep network embedding. In Proceedings of\nthe 22nd ACM SIGKDD international conference on Knowledge discovery and data mining, pp.\n1225–1234, 2016.\nSamson Wang, Enrico Fontana, Marco Cerezo, Kunal Sharma, Akira Sone, Lukasz Cincio, and\nPatrick J Coles. Noise-induced barren plateaus in variational quantum algorithms. Nature com-\nmunications, 12(1):6961, 2021.\n12\nZhaobin Wang, Minzhe Xu, and Yaonan Zhang. Review of quantum image processing. Archives of\nComputational Methods in Engineering, 29(2):737–761, 2022.\nR. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement\nlearning. Machine Learning Journal, 8:229–256, 1992.\nKeyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka.\nHow powerful are graph neural\nnetworks?, 2019.\nShen Yan, Yu Zheng, Wei Ao, Xiao Zeng, and Mi Zhang. Does unsupervised architecture repre-\nsentation learning help neural architecture search? Advances in neural information processing\nsystems, 33:12486–12498, 2020.\nAnqi Zhang and Shengmei Zhao. Evolutionary-based quantum architecture search. arXiv preprint\narXiv:2212.00421, 2022.\nMuhan Zhang, Shali Jiang, Zhicheng Cui, Roman Garnett, and Yixin Chen. D-vae: A variational\nautoencoder for directed acyclic graphs. Advances in Neural Information Processing Systems, 32,\n2019.\nShi-Xin Zhang, Chang-Yu Hsieh, Shengyu Zhang, and Hong Yao. Neural predictor based quantum\narchitecture search. Machine Learning: Science and Technology, 2(4):045027, 2021.\nShi-Xin Zhang, Chang-Yu Hsieh, Shengyu Zhang, and Hong Yao. Differentiable quantum architec-\nture search. Quantum Science and Technology, 7(4):045023, 2022.\n13\nA\nAPPENDIX\nA.1\nCIRCUIT GENERATOR SETTINGS\nThe predefined operation pool which defines allowed gates in circuits is important for QAS as well,\nbecause a terrible operation pool such as one with no rotation gates or no control gates cannot\ngenerate numerous quantum circuits with excellent expressibility and entanglement capability. This\nmakes the initial quantum search space poor, so it will influence our further pre-training and QAS\nprocess. Therefore, we choose some generally used quantum gates in PQCs as our operation pool\n{X, Y, Z, H, Rx, Ry, Rz, U3, CNOT, CZ, CY} for the circuit generator to guarantee\nthe generality of our quantum circuit space. Other settings of the circuit generator are summarized\nbelow:\nTable 4: Description of settings predefined for the circuit generator.\nHyperparameter\nHyperparameter explanation\nValue\nfor\n4/8/12-\nqubit experiments\nnum-qubits\nthe number of qubits\n4/8/12\nnum-gates\nthe number of gates in a circuit\n10/20/30\nmax-depth\nthe maximal depth in a circuit\n5\nnum-circuits\nrequired the number of circuits\n105\nA.2\nAPPLICATION SETTINGS\n(a) The target circuit of the 4-qubit state preparation\n(b) The target circuit of the 8-qubit state preparation\nFigure 5: The circuits used to generate the target states.\nQuantum State Preparation.\nIn quantum information theory, fidelity (Liang et al., 2019) is an\nimportant metric to measure the similarity of two quantum states. By introducing fidelity as the\nperformance index, we aim to maximize the similarity of the final state density operator with a\ncertain desired target state. We first obtain the target state by randomly generating a corresponding\ncircuit, and then with a limited number of sample circuits, we use the search methods to search\ncandidate circuits that can achieve a fidelity higher than a certain threshold. During the search\nprocess, the fidelity can be directly used as a normalized reward function since its range is [0, 1].\nFigure 5 shows the circuits used to generate the corresponding target states.\nMax-cut Problems.\nThe max-cut problem (Poljak & Rendl, 1995) consists of finding a decompo-\nsition of a weighted undirected graph into two parts (not necessarily equal size) such that the sum of\nthe weights on the edges between the parts is maximum. Over these years, the max-cut problem can\nbe efficiently solved with quantum algorithms such as QAOA (Villalba-Diez et al., 2021) and VQE\n(using eigenvalues). In our work, we address the problem by deriving the Hamiltonian of the graph\nand using VQE to solve it. We use a simple graph with the ground state energy −10 Ha for the\n4-qubit experiment and a relatively complex graph with the ground state energy −52 Ha in the case\n14\nof the 8-qubit experiment. Furthermore, we convert the energy into a normalized reward function\nintegral to the search process. The visual representations of these graphs are presented below:\n(a) The 4-qubit max-cut graph\n(b) The 8-qubit max-cut graph\nFigure 6: The graphs of the experiments on max-cut problems.\nQuantum Chemistry.\nIn the field of QC, VQE (Peruzzo et al., 2014; Tilly et al., 2022) is a hybrid\nquantum algorithm for quantum chemistry, quantum simulations, and optimization problems. It is\nused to compute the ground state energy of a Hamiltonian based on the variational principle. For\nthe 4- and 12-qubit quantum chemistry experiment, we use the Hamiltonian of the molecule H2 and\nLiH and its common approximate ground state energy −1.136 Ha and −7.88 Ha as the optimal\nenergy. As for the 8-qubit experiment, we consider n = 8 transverse field Ising model (TFIM) with\nthe Hamiltonian as follows:\nH =\n7\nX\ni=0\nσi\nzσ(i+1) mod 6\nz\n+ σi\nx.\n(6)\nWe design some circuits to evaluate the ground state energy of the above Hamiltonian and get an\napproximate value −10 Ha as the optimal energy. According to the approximate ground state\nenergy, we can use our methods to search candidate circuits that can achieve the energy reaching a\nspecific threshold. In the process of searching for candidates, the energy is normalized as a reward\nfunction with the range [0, 1] to guarantee search stability.\nA.3\nHYPERPARAMETERS OF PRE-TRAINING\nTable 5 shows the hyperparameter settings of the pre-training model for 4-qubit and 8-qubit experi-\nments.\nTable 5: Description of hyperparameters adopted for pre-training.\nHyperparameter\nHyperparameter explanation\nValue\nfor\n4/8/12-\nqubit experiments\nbs\nbatch size\n32\nepochs\ntraning epochs\n16\ndropout\ndecoder implicit regularization\n0.1\nnormalize\ninput normalization\nTrue\ninput-dim\ninput dimension\n2+#gates+#qubits\nhidden-dim\ndimension of hidden layer\n128\ndim\ndimension of latent space\n16\nhops\nthe number of GIN layers (L in eq.2)\n5\nmlps\nthe number of MLP layers\n2\n15\nA.4\nBEST CANDIDATE CIRCUITS\nObservation (5): In Appendix A.4, we present the best candidate circuits acquired by each of the\nthree methods for every experiment. These circuits exhibit a higher likelihood of being discovered\nby REINFORCE and BO in contrast to Random Search. This observation underscores the supe-\nrior search capabilities of REINFORCE and BO in navigating the large and diverse search space\ngenerated by our approach, which is based on a random generator derived from a fixed operation\npool. Unlike conventional approaches that adhere to layer-wise circuit design baselines, our method\nexcels in discovering circuits with fewer trainable parameters. This characteristic is of paramount\nimportance when addressing real-world optimization challenges in QAS. In conclusion, our ap-\nproach not only enhances the efficiency of candidate circuit discovery but also accommodates the\ndistinct characteristics of various problem domains through a large and diverse search space.\n(a) 4-qubit state preparation\n(b) 4-qubit max-cut\n(c) 4-qubit quantum chemistry\n(d) 8-qubit state preparation\n(e) 8-qubit max-cut\n(f) 8-qubit quantum chemistry\nFigure 7: Best candidates of the six experiments in 50 runs.\n16\n",
  "categories": [
    "quant-ph",
    "cs.LG"
  ],
  "published": "2024-01-21",
  "updated": "2024-10-23"
}