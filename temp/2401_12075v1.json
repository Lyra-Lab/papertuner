{
  "id": "http://arxiv.org/abs/2401.12075v1",
  "title": "NLP-based Relation Extraction Methods in Requirements Engineering",
  "authors": [
    "Quim Motger",
    "Xavier Franch"
  ],
  "abstract": "In the context of requirements engineering, relation extraction is the task\nof documenting the traceability between requirements artefacts. When dealing\nwith textual requirements (i.e., requirements expressed using natural\nlanguage), relation extraction becomes a cognitively challenging task,\nespecially in terms of ambiguity and required effort from domain-experts.\nHence, in highly-adaptive, large-scale environments, effective and efficient\nautomated relation extraction using natural language processing techniques\nbecomes essential. In this chapter, we present a comprehensive overview of\nnatural language-based relation extraction from text-based requirements. We\ninitially describe the fundamentals of requirements relations based on the most\nrelevant literature in the field, including the most common requirements\nrelations types. The core of the chapter is composed by two main sections: (i)\nnatural language techniques for the identification and categorization of\nrequirements relations (i.e., syntactic vs. semantic techniques), and (ii)\ninformation extraction methods for the task of relation extraction (i.e.,\nretrieval-based vs. machine learning-based methods). We complement this\nanalysis with the state-of-the-art challenges and the envisioned future\nresearch directions. Overall, this chapter aims at providing a clear\nperspective on the theoretical and practical fundamentals in the field of\nnatural language-based relation extraction.",
  "text": "NLP-based Relation Extraction Methods in RE\nQuim Motger and Xavier Franch\nDepartment of Service and Information System Engineering,\nUniversitat Polit`ecnica de Catalunya\n{joaquim.motger,xavier.franch}@upc.edu\nAbstract\nIn the context of requirements engineering, relation extraction is the task of doc-\numenting the traceability between requirements artefacts. When dealing with\ntextual requirements (i.e., requirements expressed using natural language), re-\nlation extraction becomes a cognitively challenging task, especially in terms of\nambiguity and required effort from domain-experts. Hence, in highly-adaptive,\nlarge-scale environments, effective and efficient automated relation extraction\nusing natural language processing techniques becomes essential. In this chapter,\nwe present a comprehensive overview of natural language-based relation extrac-\ntion from text-based requirements. We initially describe the fundamentals of\nrequirements relations based on the most relevant literature in the field, includ-\ning the most common requirements relations types. The core of the chapter is\ncomposed by two main sections: (i) natural language techniques for the identifi-\ncation and categorization of requirements relations (i.e., syntactic vs. semantic\ntechniques), and (ii) information extraction methods for the task of relation ex-\ntraction (i.e., retrieval-based vs. machine learning-based methods). We comple-\nment this analysis with the state-of-the-art challenges and the envisioned future\nresearch directions. Overall, this chapter aims at providing a clear perspective on\nthe theoretical and practical fundamentals in the field of natural language-based\nrelation extraction.\n1\nIntroduction\nIn the context of software development, requirements do not exist in isolation,\nbut often affect or are affected by the lifecycle of other requirements from the\nsame component, system, product or project [9]. This relational environment\nunderscores the need for effective extraction – crucial for informed decisions,\nprioritization, and strategic planning [3,49]. For instance, a requirement might\nrequire the completion of another requirement before its fulfilment, which makes\nit a key factor in determining project scope and scheduling. In contrast, a re-\nquirement might refine or elaborate further details on another, enabling a deeper\nunderstanding of its development. Moreover, multiple requirements can contain\nsimilar or even duplicate content, highlighting redundant specifications which\ncan lead to ambiguity, inefficiency and potential conflicts in development and\narXiv:2401.12075v1  [cs.SE]  22 Jan 2024\n2\nQuim Motger and Xavier Franch\ntesting phases. Ignoring these connections can harm project development trajec-\ntory, affecting design decisions and project milestones, a concern voiced by both\npractitioners and researchers [16].\nUncovering relations between requirements primarily expressed in natural\nlanguage presents hurdles due to the intricacies of human language, requiring\nmanual effort to decipher connections. This task is time-consuming and error-\nprone, especially considering the cognitive load it poses in domain-specific, spe-\ncialized environments [51,24,33,17,40]. Furthermore, relations come with distinct\nattributes like the semantics of the relation, their directionality and the degree of\ninfluence or strength, which might be difficult to infer from textual requirements.\nThe role of Natural Language Processing (NLP) in this context is pivotal. By\nleveraging syntactic and semantic knowledge, NLP techniques hold the potential\nto decipher requirement relations embedded within natural language. Linguistic-\nbased knowledge further empowers information extraction methods by enabling\nthe automatic identification of these relations from vast amounts of textual data.\nSuch techniques, ranging from rule-based approaches to advanced machine learn-\ning, deep learning models and, ultimately, Large Language Models (LLM), offer\na spectrum of strategies to capture these relations effectively. Consequently, this\nchapter aims to shed light on these techniques, equipping researchers and prac-\ntitioners in Requirements Engineering (RE) with valuable insights into NLP’s\ncapability to unravel the intricate web of requirement relations and, in doing so,\nenhance the quality of software development processes.\nThe rest of this chapter is structured as follows. Section 2 delves into the fun-\ndamentals of requirements relations. Section 3 depicts the role of NLP knowledge\ngeneration techniques to support relation extraction. Section 4 elaborates on the\ndifferent NLP-based information extraction methods covered by state-of-the-art\nliterature in the field. Section 6 enumerates the most relevant challenges from\nthe state-of-the-art and future research directions. Finally, Section 7 highlights\nthe main conclusions from this chapter.\n2\nFundamentals\n2.1\nRequirements relations\nTypically, the knowledge and formalization of relations between requirements is\nmainly covered in detail as a specific type of traceability between requirements.\nSpecifically, the Handbook Requirements Management from the International\nRequirements Engineering Board (IREB) [7] refers to three types of requirement\ntraces: (i) back to its origins, (ii) forward to its implementation in design, code\nand associated tests, and (iii) to requirements it depends on [26]. This traceabil-\nity to other requirements can refer to either the same or a different level of detail.\nSimilarly, Pohl uses the term relationship to refer to the interrelation among tex-\ntual requirements, model-based requirements, and the interplay between textual\nand model-based requirements [37], making explicit the multiple dimensions of\nrequirement relations at different levels of detail and specification [8].\nNLP-based Relation Extraction Methods in RE\n3\nWhile relation is the preferred term for referencing semantic-agnostic associ-\nations between requirements (i.e., with independence of the meaning of such re-\nlation), the term dependency is broadly used in literature at different levels of de-\ntail. Mainly, dependency is used to refer to a specific relation type expressing that\na requirement should be fulfilled first because another requirement depends on\nit [27], or more generally, to a specific traceability link type [32,37,8]. Moreover,\nrelations between requirements extending the semantics of this prioritization-\nbased perspective of the term dependency are broadly covered, including but\nnot restricted to conflicting [37,42,18,8] or duplicated [18,8] requirements (we\ndiscuss relation types in more detail in Section 2.2), among others. Actually,\nresearchers in the RE field generally use relation [2,23,44,29,13,31,43,46,24,5,4]\nand dependency [40,35,51,22,17,6,39,34,16,50,48] as indistinctive terms to refer\nto semantic associations between requirements, with independence of the cov-\nered relation types. Alternative terms like entailment [21] or link [33], while also\nused, are less frequent. In this chapter, we will use the term relation to refer\nto all types of textual requirements associations. Moreover, we will focus exclu-\nsively on relations between textual requirements and NLP-based strategies for\ntheir management.\nA relation between a pair of requirements ri\nd−→rj is defined as a directed\ntyped link from ri to rj, where d is the syntactic form of a specific type of\nsemantic association between ri and rj.\nWhile requirement relations are defined as directed links, some specific re-\nlation types d are defined as bidirectional, reciprocal relations, meaning that\nri\nd−→rj implies that rj\nd−→ri for that same relation type d.\n2.2\nRelation types\nBuilding upon the foundations laid by the RE community, and using the defini-\ntion in Section 2.1, we convey into the enumeration1 of the following requirement\nrelation types d, including whether they are unidirectional (U) or bidirectional\n(B) relations:\n– Requires (U): the fulfilment of ri requires the fulfilment of rj. This relation\nindicates that rj must be successfully addressed before ri can be considered\ncomplete. This is the most commonly addressed relation type [37,32,27], also\nreferred to simply as depends [8,26].\n– Conflicts (U): the fulfilment of ri restricts - without excluding - the fulfil-\nment of rj\n[37,42,18,8]. This relation indicates the existence of limitations\nthat need to be refined for resolving the conflicts between both requirements.\n1 This enumeration of requirement relations is not intended to be exhaustive, but rep-\nresentative of the most common relation types subject to be automatically detected\nby NLP-based techniques according to the scientific literature in the field.\n4\nQuim Motger and Xavier Franch\n– Contradicts (B): ri and rj are mutually exclusive [8] (i.e., ri conflicts with\nrj to the limit of exclusion). This relation indicates that satisfying one re-\nquirement would lead to the violation of the other.\n– Is a variant (B): ri serves as an alternative to rj, providing a variant form\nto fulfil the same underlying need or purpose [8]. This relation suggests that\nboth requirements can be considered as an alternate option or choice in place\nof each other.\n– Is similar (B): ri replicates - partially or totally - the content described by\nrj [18,37,8]. This relation implies that ri conveys information or describes\na desired behaviour also present in rj, resulting in redundancy or content\nrepetition.\n– Details (U): ri extends or refines the information in rj [8]. This relation\nimplies that ri adds more specific or detailed aspects to rj, contributing to\na deeper understanding of its behaviour, constraints or implementation.\nTable 1 provides some illustrative examples for each requirement relation\ntype d listed above.\nTable 1. Requirement relation examples for each relation type d. These examples\nhave been specifically generated for academic and illustrative purposes, using generic,\ncontext-agnostic relation types.\nType d\nRequirement ri\nRequirement rj\nRequires\nWhen the “Submit” button is\nclicked, the form data should be\nsent to the server for processing.\nThe form user interface must in-\nclude a “Submit” button.\nConflicts\nThe software shall have a max. re-\nsponse time of 2 seconds for user\nrequests.\nThe\nsoftware\nshall\nbe\ncapa-\nble of processing extremely large\ndatasets efficiently.\nContradicts\nThe software shall provide fault\ntolerance through real-time, syn-\nchronous data replication.\nThe software shall provide fault\ntolerance through offline periodic\nbackups.\nIs a variant\nThe software can use PostgreSQL\nas its database system.\nThe software can use MySQL as\nits database system.\nIs similar\nThe software shall log all user in-\nteractions for auditing.\nThe software shall record user ac-\ntivities for analysis.\nDetails\nThe project shall follow ISO 31000\nguidelines for risk management.\nThe project shall incorporate risk\nmanagement practices.\n2.3\nNLP-based relation extraction\nIn the context of NLP for RE [55], and based on the previous formalization,\nNLP-based relation extraction refers to the application of computational linguis-\ntic techniques and information extraction methods to automatically identify\nNLP-based Relation Extraction Methods in RE\n5\nand classify specific ri\nd−→rj instances between pairs of textual require-\nments ri, rj ∈R. This set R = {r1, r2, ..., rn} is the corpus of documents (i.e.,\ntextual requirements) of size n, and each document ri ∈R is an individual\ndocument from the corpus. These techniques and methods aim to automati-\ncally identify requirements relations ri\nd−→rj by leveraging NLP algorithms and\nmethodologies, including syntactic parsing, semantic analysis, conceptual mod-\nelling and machine- and deep-learning models.\nIn the rest of this chapter, we will discuss the most prominent strategies used\nfor NLP-based relation extraction, which we structure into two sequential stages.\n1. NLP Knowledge Representation. From the computational linguistics\npoint of view, these techniques relate to the processing of textual documents\nto build a structured knowledge representation from a corpus R, based on\nthe syntactic features and semantic knowledge from these textual represen-\ntations. We categorize NLP-based techniques (Section 3) into syntactic (Sec-\ntion 3.2) and semantic (Section 3.3) techniques, both of them acting after a\npre-processing step (Section 3.1).\n2. Information Extraction. These methods illustrate the practical applica-\ntion of information extraction methods using the structured syntactic- and\nsemantic-based representation from textual requirements to identify and\nclassify requirement relations. We categorize relation extraction methods\n(Section 4) into retrieval-based (Section 4.1) and machine-learning-based\n(Section 4.2) methods.\nFigure 1 illustrates a summary of the NLP techniques and relation extraction\nmethods defined in this chapter.\nFig. 1. Summary of NLP techniques and relation extraction methods.\n2.4\nSample data set: PURE\nFor illustrative purposes, we will use a sample selection of annotated requirement\npairs from the PURE (PUblic REquirements) data set [20], manually annotated\n6\nQuim Motger and Xavier Franch\nby Deshpande et al. [16]. This sample contains a sub set of requirements from\nthe functional requirements specification document (SRS) of the European Rail\nTraffic Management System (ERTMS/ETCS) [41]. The data set is composed of\ntwo sub sets documenting 190 unique requirements: one for binary classification\n(i.e., non-related vs. related requirements) and one for multiclass classification\n(i.e., multiple requirement relation types). The binary data set contains 10,859\nrequirement relations (9,606 non-related, 1,253 related), while the multiclass\ndata set contains 4,432 requirement relations (3,720 non-related, 378 require\nrelations, 334 similar relations). Table 2 contains some examples illustrating the\ndifferent annotated requirement relation types in the data set. Complementary,\nwe provide access to the data set using a Jupyter Notebook2. This notebook\nand data set will be used for exemplifying some of the techniques and methods\ndepicted in this chapter for relation extraction.\nTable 2. Requirement relation examples from the PURE annotated data set [16].\nType d\nRequirement ri\nRequirement rj\nnone\nThe driver shall be able to select\ntrain data entry on the DMI.\nOn\nlines\nfitted\nwith\nRBC\nthe\nETCS trainborne equipment shall\nbe able to transmit the location of\nthe entire train to the RBC.\nrequires\nThe ETCS on-board shall be capa-\nble of receiving information about\npantograph and power supply from\nthe trackside.\nThe information regarding lower-\ning and raising of the pantograph\nand opening/closing of the circuit\nbreaker shall be provided sepa-\nrately and in combinations.\nis similar The\ncurrent\noperational\nstatus\nshall be indicated to the driver on\nthe DMI.\nA special indication shall be shown\non the DMI.\nRead and execute Section 2. Dataset from the Jupyter Notebook to load\nand explore the PURE binary and multiclass datasets.\n3\nNLP Knowledge Representation Techniques\n3.1\nText Pre-processing Techniques\nInitial steps in most NLP tasks aim to prepare the text for further analy-\nsis and extraction of meaningful information. These include, but are not re-\nstricted to, the following tasks: tokenization, stop-word removal, case folding,\nnoisy text cleaning, standardization, sentence splitting, stemming and lemmati-\nzation. In this stage, each requirement document from a given corpus r ∈R\n2 Available at: https://github.com/quim-motger/NLP4RE_RelationExtraction\nNLP-based Relation Extraction Methods in RE\n7\nis processed through a NLP pipeline to generate a tokenized representation\nT(r) = [t1, t2, ..., tm], where each ti represents a token at the position i in the\noriginal r. Selection, combination and order of the aforementioned techniques is\ndiversely represented in practical applications of relation extraction strategies.\nSolutions range from simple tokenization, either with traditional approaches like\nwhitespace, punctuation-based tokenizers [33,48] or LLM-based tokenizers [22],\nto a full combination of these techniques [2,17]. For effective relation extraction,\nit is crucial to strike a balance between preserving data integrity and removing\nnoise, in order to maximize precision without sacrificing valuable information.\nA conservative approach might risk diminishing domain-specific nuances and\nhuman-input particularities that contribute to understanding context and iden-\ntifying patterns, while a permissive strategy could overcomplicate the task by\nprocessing excessive noise and non-relevant text input. This delicate balance\nunderscores the importance of employing pre-processing techniques tailored to\nthe data’s nature, text quality, and specific requirements, aiming to optimize\nknowledge extraction accuracy while minimizing information loss or distortion.\nFor more details on text pre-processing techniques, we refer to Chapter 1 of\nthis book.\n3.2\nSyntactic-Based NLP Knowledge Representation Techniques\nSyntactic-based techniques refer to the processing and utilization of syntac-\ntic and linguistic information to infer relations between requirements based\non the structural aspects and rules governing the arrangement of words and\nphrases in a sentence or text. These techniques leverage syntactic patterns,\nrules and structures to serve as a foundation for understanding the structural\nand grammatical aspects of requirement relations.\nSyntactic techniques focus on the extension of a tokenized representation of\na requirement r, T(r) = [t1, t2, ..., tm], with syntactic feature annotations for\neach individual token ti and its syntactic association with other tokens tj, i ̸=\nj. One of the most common syntactic analysis practices in relation extraction\nis the combination of Part-of-Speech (PoS) tagging and dependency parsing.\nPoS tagging assigns a grammatical label gi to each ti, indicating the syntactic\ncategory of the given token. Using this extended annotation, dependency parsing\nis formalized through a dependency tree DT(r), which is defined as a rooted\ndirected tree composed of token dependencies DP(ti, tj, dep), where dep is the\ntype of dependency from ti to tj. Again, we refer to Chapter 1 for more details.\nGrammatical associations at sentence level using PoS tags are the most com-\nmon syntactic-based features used in requirements relations [35,22,44,29,13,43,46,6,39,34,16,5,48,33].\nWhile syntactic annotations are mostly used to generate structured knowledge\nused by syntactic-based relation extraction methods (see Section 4.1), some\ntechniques can also be used in isolation as relation extraction methods. These\nsyntactic-based techniques include but are not limited to the following:\n– N-gram generation: generating sequences of m contiguous tokens ti→i+n−1\nto capture contextual information and encapsulate more general ideas be-\n8\nQuim Motger and Xavier Franch\nyond individual tokens [34]. N-grams are typically generated based on pre-\ndefined syntactic rules, whether explicitly set by generic and/or domain-\nspecific PoS patterns (e.g., NOUN-compound-NOUN, NOUN-amod-ADJ) or\nimplicitly inferred by term-based matching using pattern matching templates\nor ontologies (see Section 4.1). A special case of syntactic-based n-gram gen-\neration is noun phrase chunking, grouping tokens to form meaningful,\ncompound noun phrases [48,33], encompassing pronouns (PRON), proper\nnouns (PROPN), or nouns (NOUN), which can be found alongside other\ntokens serving as modifiers, such as adjectives (ADJ) or additional nouns.\nFigure 2 illustrates a couple of examples using n-gram generation, reflecting\non the extraction of complex, compound entities and its hierarchy, which\ncan later be used to refine the matching accuracy of related requirements\nbeyond single-term, context-agnostic matches.\nFig. 2. Examples of the generation of n-grams. On the left, a parent node (systems)\nis the root of two nested n-grams composed by subsequent (control and train) and\nparallel (national) direct term children. On the right, a non-relevant child (for) from\nthe root term (location) associates the second n-gram (current and location) with the\nfirst one (national and values).\n– Coreference resolution: resolving references through pronouns or noun\nphrases to the same entity in a given requirement or between multiple re-\nquirements [44,43,46,25]. While coreference resolution is a traditional NLP\ntask for disambiguation, in the context of relation extraction it supports po-\ntential identification of concurrent references to the same entities between\ndifferent requirement documents, known as cross-document coreference. The\nmost popular strategy is the use of pre-trained neural models [11] for generic,\nsyntactic-based coreferences [46], extended with domain-specific rule-based\ntechniques like context-based synonymy rules using mapping tables [1] or\nlocation-based references using order and hierarchy between requirement\ndocuments [35]. In this case, syntactic coreferences do not allow for semantic\nanalysis for inference of relation types. Figure 3 illustrates a cross-document\nreference example through noun phrase disambiguation.\nGenerally, these techniques lay the groundwork for pattern-matching based\ntechniques in which grammatical structures infer specific templates to express\na potential relation (see Section 4.1). Nevertheless, they can also be used to\nNLP-based Relation Extraction Methods in RE\n9\nFig. 3. Example of cross-document coreference. In the second requirement, the function\nrefers to the core functionality transfer to shunting mentioned in the first requirement,\nwhich is the immediate document predecessor.\ngenerate enriched, relation specific word embeddings for either unsupervised [13]\nor supervised [48] machine- and deep-learning approaches (see Section 4.2). In\ncomparison with traditional purely lexical embeddings like TF-IDF (see Section\n4), syntactic embeddings delve deeper into the relation between words based on\nthe nuances of the language syntax.\nRead and execute Section 3. Syntactic NLP Techniques from the Jupyter\nNotebook to analyse the pre-processing and syntactic analysis of a sub set\nof requirement pairs from the PURE annotated data set using the English\nRoBERTa-based transformer pipeline from Spacy [47].\n3.3\nSemantic-Based NLP Knowledge Representation Techniques\nSemantic-based\ntechniques\ninvolve\nthe\napplication\nof\nlinguistic\nand\nknowledge-based approaches to infer requirement relations based on the mean-\ning and interpretation of words, phrases, and sentences in a language. These\ntechniques aim to capture the conceptual relationships, associations, and nu-\nances of language, enhancing the comprehension and analysis of requirement\nrelations by leveraging their inherent semantic knowledge.\nSemantic techniques focus on the extension of T(r) = [t1, t2, ..., tm] with\nsemantic feature annotations for each individual token ti, as well as subsets\nof multiple tokens (i.e., n-gram combinations). Most common semantic-based\ntechniques include the following:\n– Named Entity Recognition (NER). At token level, a common example\nof semantic feature annotations are NER tags, which allow the identifica-\ntion of a set of tokens TNER(r) ∈T(r) as named entities. These named\nentities include stakeholders, services, products, systems and software com-\nponents, among others [48]. They provide semantic knowledge about the\nsubject and/or object of a relation, but not about the semantics of the re-\nlation type. NER can be performed using pre-trained models with generic\ntags (e.g., organization, product, quantities, cardinals) or fine-tuned mod-\nels for domain-specific terms and/or labels. For example, in the context of\nERTMS/ETCS [41], references to systems or components like LZB (Lin-\nienzugbeeinflussung), RBC (Radio Block Center) or DMI (Driver Machine\nInterface) might not be detected without an extension of pre-trained NER\n10\nQuim Motger and Xavier Franch\nmodels. Figure 4 illustrates potential related requirements through common\nNER assigned tags.\nFig. 4. Example of named-entity recognition (NER). All reported requirements refer\nto the display of information in the Driver Machine Interface (DMI), an interface\ncomponent between the driver and the ERTMS/ETCS system.\n– Syntactic and semantic word embeddings. Embedding generation from\npre-trained models like Word2Vec [51,46,4] or GloVe [31] map words with\nsimilar meanings or contexts to similar vector representations, enabling the\ncapture of semantic relationships. These vector-based representations can\nlater be used for multiple syntactic and semantic knowledge inference tasks,\nincluding semantic similarity [43,39,34,5] using cosine, Euclidean or Manhat-\ntan similarity algorithms [5]. Semantic similarity might not only be used to\nidentify potential duplicate relations ri\nsimilar\n−−−−−→rj at requirement level, but\nalso to automatically identify individual tokens ti or n-grams ti→i+n−1 from\nri that are semantically similar to another subset of tokens from another\nrequirement rj, meaning that these probably refer to highly-related or even\nequivalent entities. More recent solutions based on the use of LLM use the\nembedding generation suited for the selected LLM (see Section 4.2).\nRead and execute Section 4. Semantic NLP Techniques from the Jupyter\nNotebook to explore semantic analysis (i.e., NER, semantic similarity) of a sub\nset of requirement pairs from the PURE annotated data set using Word2Vec.\nIn conclusion, syntactic techniques analyse the structural aspects of language,\nunveiling relationships embedded in grammatical patterns. Conversely, semantic\ntechniques delve deeper into word meanings, capturing the underlying nuances\nof language in domain-specific scenarios. Both syntactic and semantic techniques\ncan be used either in isolation or as hybrid approaches, combining the acquired\nknowledge from both perspectives. Deciding on the use of these techniques will\nbe conditioned by the relation extraction methods applied in each context (see\nSection 4).\nNLP-based Relation Extraction Methods in RE\n11\n4\nInformation Extraction Methods\nFrom an information extraction standpoint, relation extraction methods can\nbe majorly classified into two core categories: retrieval-based (Section 4.1) and\nmachine-learning-based (Section 4.2) methods.\n4.1\nRetrieval-Based Information Extraction Methods\nRetrieval-based methods involve the application of predefined rules, patterns,\nontologies or linguistic structures to analyse and extract potential relations\nfrom a corpus R.\nLinguistic and Text-based Methods. These deterministic approaches lever-\nage syntactic and semantic extended features from each requirement r, such as\nthe tokenization T(r), the dependency tree DT(r), and the set of named entities\nTNER(r). The objective is to identify coreferent or semantically related state-\nments that imply a relation between requirement pairs ri, rj ∈R. As illustrated\nin Figure 1, common methods in this category include:\n– Token-based cross-reference detection. This formalization is based on\nthe detection of tokens indicating a crossed reference between ri and rj. The\nmost common example of cross-reference implies that a sub set of tokens\nt ⊆T(ri) refers to the same sub set of tokens t ⊆T(rj). This illustrates cross-\ndocument references to agents or components affecting or affected by related\nrequirements (e.g., DMI in Figure 4). Additionally, a crossed reference might\nalso imply that a sub set of tokens t ⊆T(ri) refers to a specific metadata\nfield (e.g., the ID) from rj. This refers to implicit relations involving explicit\nreferences to metadata textual properties, including a requirement identifier,\ncategory, product or system to which the requirement belongs to.\n– Pattern matching. This method involves utilizing multiple text-based\nNLP-based properties, including the resulted tokenization T(ri) or depen-\ndency tree DT(ri), to apply automatic matching with a set of pre-defined,\ncontext-specific rules compliant with the formalization of a relation within\na given domain. The most common pattern matching technique is keyword\nmatching, where a set of domain-specific pre-defined words or tokens are\nused as a dictionary of potential relevant keywords for relation formaliza-\ntion [35,29,39,48]. These words can either relate to co-location of entities\nand requirement artefacts with respect to the original requirement (e.g., “be-\nlow”, “the following”, “in the document above”), or with respect to another\ndocument (e.g., “appendix”, “attached file”) [35]. Complementarily, infor-\nmation in DT(ri) can be used to extend keyword matching by filtering out\nnon-relevant appearances of specific keywords (e.g., by excluding matches\nfor which the keyword is not the ROOT element of the sentence in order to\nexclude non-principal clauses).\n12\nQuim Motger and Xavier Franch\nFig. 5. Dependency tree and named entity recognition results for a pair of annotated\nrequirements ri\nrequires\n−−−−−→rj from the PURE data set.\nFigure 5 illustrates a syntactic (dependency tree) and semantic (NER) hybrid\ncombination of pattern-matching knowledge inference for relation detection of a\nrequires relation between a pair of requirements ri, rj ∈R from the annotated\nPURE data set. NER task allows for the detection of relevant context-dependent\nnamed entities TNER(ri) ⊆T(ri) and TNER(rj) ⊆T(rj) (i.e., the Driver Ma-\nchine Interface or DMI) without extended pre-training or fine-tuning. A pair\nof requirements for which TNER(ri) ∩TNER(rj) ̸= Ø is reporting two require-\nment documents for which an explicit reference to the same component is being\nreported. For a more accurate keyword detection, a more fine-grained catego-\nrization of entity types, or for a more specialized domain, supervised training on\na NER pipeline could potentially improve component identification.\nIntersection on named entities TNER(ri)∩TNER(rj) can be used as a form of\nin-text explicit cross-reference. Nevertheless, syntactic knowledge can either be\nused as a complement or a stand-alone approach to identify linguistic patterns in\nwhich a relation to another requirement part or document might be suggested.\nGiven a named entity tner ⊆T(ri), for all DP(tj, tk, dep) where j ̸= k and\ntj ∈tner or tk ∈tner, a sub-set of pre-defined dependency patterns can be used\nto determine occurrences of tner syntactically relevant for the materialization of\na relation. While the effectiveness of distinct patterns is typically context depen-\ndent (e.g., authorship of requirements, formalization style, extension, typology\nof documentation), general rules include lexical intersection on the root element\nand/or direct syntactic dependencies, and overlapping of common, shared syn-\ntactic patterns between requirements. In Figure 5, a specific named entity (i.e.,\nthe DMI) acts as nominal subject (nsubj) of the ROOT verb element as dis-\nplayed in DT(ri) (i.e., “display”), while that same named entity, by transitivity,\nacts as the object of the ROOT verb element in DT(rj) (i.e., “indicated”).\nNLP-based Relation Extraction Methods in RE\n13\nRead and execute Section 5.1. Linguistic and Text-based Methods from the\nJupyter Notebook for practical application of a NER-based cross-reference de-\ntection using pattern matching to identify related requirements.\nVectorization Methods. These methods involve creating vector-based repre-\nsentations of a requirement corpus R using the tokenized, pre-processed represen-\ntation of all ri ∈R as input. These representations are then used to build lexical-\nand/or semantic-based vector space models of the requirement documents.\n– Lexical-based detection. This method involves applying statistical tech-\nniques to identify relations between requirements. One of the most common\napproaches is the TF-IDF (Term Frequency-Inverse Document Frequency)\nvector space model. The TF-IDF score for a term tj ∈T(ri) is calculated as\nfollows:\nTF-IDF(tj, ri) = TF(tj, ri) · IDF(tj)\n(1)\nwhere TF(tj, ri) is the term frequency of term tj (i.e., the number of occur-\nrences of term tj in requirement ri), and IDF(tj) is the inverse document\nfrequency of term tj across the set of requirements, defined as:\nIDF tj = log2( n\nnj\n)\n(2)\nThe TF-IDF score can then be used to identify relationships between re-\nquirements by looking for words that have high TF-IDF scores (i.e., passing\na context-dependent threshold score thr) in both requirements.\n– Semantic-based detection. This method involves using semantic-based\ntechniques to generate requirements embeddings. This can be done through\ngeneral purpose, unsupervised approaches like Latent Semantic Analysis\n(LSA) and Universal Sentence Encoder (USE), or by employing pre-trained\nmodels such as Word2Vec, GloVe or FastText. Numerical semantic vector\nrepresentations can either be used to identify relations between requirements\nby applying statistical measures (e.g., using similarity measures) or as input\nto machine-and deep-learning based methods (see Section 4.2).\nRead and execute Section 5.2. Vectorization Methods from the Jupyter\nNotebook to analyse a Word2Vec based illustration of related requirements in\na vector-space representation.\nGraph-based Methods. These methods leverage the structural relationships\nand dependencies within the requirement corpus R to identify and extract mean-\ningful connections between requirements. Two main approaches are considered:\n– Syntactic graphs. This method involves creating a weighted, directed\ngraph model using the entire requirement corpus R and the dependency\n14\nQuim Motger and Xavier Franch\ntree for each requirement DT(ri) for ri ∈R. In syntactic or PoS-based\ngraphs, nodes are tokens t and edges are the syntactic dependencies be-\ntween each pair of tokens DP(ti, tj, dep), which might be weighted using\nfrequency-related criteria (e.g. logarithmic occurrence frequency of each edge\nin DT(ri) [13]). This method can either be used in isolation as an approach to\nidentify dependencies using pattern-matching techniques [6], or as an input\nto generate relation-specific, syntax-enhanced embeddings of requirements\nfor machine- and deep-learning based strategies [13] (see Section 4.2). The\ncombination of graph-based requirements modelling with pattern matching\ntechniques (e.g., keyword matching, cross-reference detection) allows for ef-\nficient updates and automated detection of potential dependencies based on\nthe insertion of a new edge whose target node is a domain-specific relevant\nword.\n– Semantic graphs. This method involves identifying and extracting rela-\ntionships between entities or concepts in text using semantic role labelling\n(SRL) and semantic relation graphs. SRL identifies semantically relevant\npredicates and associates their arguments with specific roles within a sen-\ntence. These roles provide information about the nature of the relationship\nbetween the predicate and its arguments. In the context of relation extrac-\ntion, SRL helps in identifying the key entities involved in a relationship and\ntheir respective roles. The semantic relation graph represents the extracted\nrelationships in the form of a directed graph, where predicates (verbs) are\nrepresented as nodes and their associated arguments (e.g., noun and person\nphrases) as additional, connected nodes. The graph also captures coreference\nrelations and deduplicates arguments.\nFigure 6 illustrates a reduced graph-based instance. This structure enables\nthe representation and analysis of complex relationships and supports graph-\nbased algorithms for further processing focused on semantic search, like spread-\ning activation [44,43]. By initiating activation at specific nodes in the graph, such\nas predicates or key entities, the activation spreads along the edges, influencing\nand activating neighbouring nodes. This activation can be used to measure the\nstrength or relevance of connections between entities, identify related concepts,\nor retrieve additional information from the graph.\nRead and execute Section 5.3. Graph-based Methods from the Jupyter Note-\nbook for practical application of the construction of a SRL graph to identify\nrelated requirements.\nOntology-based Methods. These methods use pre-defined ontologies as struc-\ntured representations of domain-specific knowledge used as gold-standards for re-\nlation extraction. These techniques leverage both semantic relationships and hi-\nerarchical structure defined in ontologies to infer and categorize relations among\nrequirements concepts. Generic lexical databases like WordNet [38] can be used\nto infer linguistic relations such as synonymy (e.g., “similar”), hypernymy (e.g.,\n“details”) or hyponymy (e.g., “variant”, “conflicts”) [45]. On the other hand,\nNLP-based Relation Extraction Methods in RE\n15\nFig. 6. Example of a relational graph instance for a ri\nrequires\n−−−−−→rj relation. The ex-\nample illustrates the node density caused by shared entities among requirements (i.e.,\nsubject ETCS on-board, verb receive, object trackside).\ndomain-specific ontologies overcome the limitations of generic knowledge bases\nby modelling semantically-oriented relations between domain-specific terms, in-\ncluding agents, products, systems or components, among others [17,34,48]. De-\nsign of domain-specific ontologies is typically done using the Web Ontology\nLanguage (OWL) [53], a semantic web language specifically designed for rep-\nresenting and encoding knowledge in a machine-readable format. OWL is based\non a formal logical foundation of expressive features, including various classes,\nproperties, and relationships, and is built upon the Resource Description Frame-\nwork (RDF) [54]. Ontology approaches use conceptual clustering and similarity\nanalysis to match each pre-processed requirement ri ∈R with the concepts of\nthe ontology, including multiple n-gram combinations [34]. Figure 7 illustrates\na relation detection using an ontology example for the PURE dataset domain.\nIn comparison with syntactic and semantic graph approaches, ontologies allow\nfor effective domain-specific modelling to provide structured knowledge on spe-\ncific industrial and scientific environments. Nevertheless, designing, generating\nand maintaining a domain-specific ontology is time-consuming and requires the\ninvolvement of domain experts.\nFig. 7. Example of a ri\nrefines\n−−−−−→rj relation detected using an ontology for the\nETCS/ERTMS domain. The ETCSLevel2 reference from ri refines implementation\ndetails from the information received referenced by rj.\n16\nQuim Motger and Xavier Franch\nRead and execute Section 5.4. Ontology-based Methods from the Jupyter\nNotebook for practical application of an ontology-based semantic matching to\nidentify related requirements.\n4.2\nMachine Learning-Based Information Extraction Methods\nMachine- and deep-learning-based methods encompass approaches that\nutilize machine- and deep-learning models to automatically learn and predict\npotential requirements relations. These methods leverage supervised, unsuper-\nvised, or weakly supervised learning approaches to detect and classify require-\nment relations within a given corpus.\nUnsupervised Methods. Unsupervised methods for relation extraction aim\nto discover patterns and relationships within the data without the use of labelled\npairs of related requirements. These methods often involve clustering, topic mod-\nelling, or graph-based algorithms to identify and group similar instances based on\ntheir inherent similarities. As shown in Figure 1, unsupervised methods include:\n– Topic modelling. Enriched syntax-enhanced and relation-specific embed-\ndings from R can be used as input for topic modelling algorithms [13].\nThese embeddings can be generated using enriched graph structures (see\nSection 3.3) as input to deep learning models like Graph Convolutional Net-\nworks (GCN) using as training objective the target token for a given to-\nken t ∈T(ri). These embeddings can later be used as input for modified\nmulti-head self-attention layers to generate representations of requirement\nsentences and relations, employing block division based on entity pairs and\nincorporating self-attention mechanisms, max pooling, and linear transfor-\nmations to encode and interact between sentences and relations for infer-\nence. Notice that this technique is agnostic to the semantics of the relation\nd, meaning that reported relations are not categorized into a particular type.\n– Clustering algorithms. Vectorization techniques can also serve as a contri-\nbution for enriched clustering algorithms. For instance, LSA enables cluster-\ning requirements into groups of similar requirements based on their semantic\nconcepts [33]. Complementarily, a clustering algorithm like k-means can be\napplied using the LSA-generated vectors as input. The algorithm identifies\nthe centroids of the data points and builds clusters around them [33,40].\nThese clusters and centroids can serve as input for human assessment on\nidentifying and/or confirming relations between requirements grouped into\nthe same cluster (e.g., suggesting requirement relations between cluster cen-\ntroids and closest data points to each cluster). Figure 8 illustrates the result\nof running a k-means clustering algorithm on a sub set of requirements from\nthe PURE data set. While “similar” relations can be computationally as-\nsessed using vector-based similarity metrics, relations with a different seman-\ntic (e.g., “contradiction”) are challenging from a cognitive point of view for\nautomated analysis. Therefore, for multiclass relation extraction, clustering\napproaches are limited to the intervention of human assessment.\nNLP-based Relation Extraction Methods in RE\n17\nFig. 8. Output of k-means clustering (k=10) for a reduced sample set of 117 require-\nments from the PURE data set. Some examples for which an annotated relation exists\nhave been highlighted.\nRead and execute Section 6.1. Supervised Methods from the Jupyter Note-\nbook for practical application of an LSA-based clustering method to identify\nrelated requirements.\nSemi-supervised Methods. Combination of labelled and unlabelled data to\nimprove relation extraction. These methods leverage a small amount of labelled\nrelations for training, while using a larger pool of unlabelled data to learn pat-\nterns and generalize relationships. Semi-supervised methods reported in Figure\n1 include:\n– Weakly supervised learning (WSL) Combination of supervised and un-\nsupervised learning to overcome the challenges of limited labelled require-\nment related pairs. In this context, multiple machine learning models like\nNaive Bayes (NB), Random Forest (RF) or Support Vector Machines (SVM)\ncan be trained on a small dataset of requirement pairs. These models are then\nutilized to classify unlabelled data [15] and used to identify instances where\nall classifiers agree on the labelling, considering them as consistent results.\nThis strategy is commonly known as ensemble machine learning (EML). The\nagreed labels are assigned as pseudo labels to the corresponding data points.\nBy iteratively refining the model using these pseudo labels, the approach\nenables the extraction of relations from unlabelled data. This WSL-based\nmethod showcases how weak supervision can be employed to tackle the chal-\nlenge of limited labelled data in relation extraction tasks. Figure 9 illustrates\nthe instantiation of a potential configuration for WSL using EML.\n– Active learning (AL). AL approaches design learning algorithms which\nrequire from interactive queries to an oracle [17], whether it is a human\n18\nQuim Motger and Xavier Franch\ndomain expert or a third software component with domain expert knowledge\n(e.g., an ontology-based dependency prediction tool [34]). In this context, an\nEML learner can be used to classify unlabelled data, but predictions are\nevaluated based on the prediction confidence level. Uncertain predictions\n(i.e., below a specific confidence threshold) are sent to an oracle to be labelled\naccordingly. On the other hand, most confident predictions (i.e., above a\nspecific confidence threshold) are incorporated to the training set. Values\nfor uncertainty thresholds respond to a context-dependent trade-off between\naccuracy and efficiency of the method. Figure 9 illustrates the instantiation\nof a potential configuration for AL using EML.\nFig. 9. Configuration example of a WSL relation extraction set up using EML (left)\nand an AL relation extraction set up based on EML (right).\nRead and execute Section 6.2. Semi-supervised Methods from the Jupyter\nNotebook for practical application of an ensemble machine learning method\nusing NB, RF and SVM to identify related requirements.\nSupervised Methods. Based on the evolution and fine-tuning of machine -\nand deep-learning models using labelled requirement pairs to accurately identify\nand extract dependencies between requirements. These methods rely on anno-\ntated data to learn patterns and linguistic cues associated with different types\nof requirement relationships, enabling the model to predict and generalize de-\npendencies in new, unseen pairs of requirements. As a generic classification, in\nFigure 1 we differentiate between the following:\n– ML classifiers. As introduced in the description of EML techniques, ML\nmethods employ traditional statistical ML classifiers like Naive Bayes, Sup-\nport Vector Machine, K-nearest neighbours [15,2]. Whether in isolation or\nthrough their combined use (EML), ML classifiers are used as a binary (type-\nagnostic) or multi-class (type-dependent) classification algorithm, where mod-\nels are pre-trained using a data set of labelled requirement pairs ri\nd−→rj and\nNLP-based Relation Extraction Methods in RE\n19\nlater used to predict new, unforeseen requirement relations. Input for these\nclassifiers can either be pre-processed natural language representations for\neach ri ∈R [16], or vectorization representations using pre-trained models\n(e.g., Word2Vec, FastText, GloVe) [2].\n– DL classifiers. Classification of requirement relations using requirements\nembeddings as input for deep learning classifiers such as Convolutional Neu-\nral Networks (CNN), Long Short-Term Memory networks (LSTM), Bidirec-\ntional LSTM (BiLSTM) and Recurrent Neural Networks (RNNs) for relation\nclassification [2,21,23], including the use of encoder-based transformer mod-\nels like BERT [22,31] for both the generation of requirements embeddings\nand for fine-tuning LLMs for classification tasks.\nRead and execute Section 6.3. Supervised Methods from the Jupyter Note-\nbook for practical application of a fine-tuning process of an encoder-only model\nlike BERT with a top layer for document classification to identify related re-\nquirements.\n5\nComparative analysis\n5.1\nEmpirical analysis\nTo illustrate the main advantages and drawbacks of retrieval-based and ma-\nchine learning-based methods respectively, we briefly report on the design prin-\nciples, effectiveness and performance efficiency of two representative methods.\nFor retrieval-based, we use an ontology-based method using an RDF ontology\nin OWL format. For machine learning-based, we use a supervised method using\na deep learning, Transformer-based classifier.\nOntology-based: OpenReq ERTMS/ETCS ontology. We use the OpenReq-\nDD tool [34] as an ontology-based representative, which includes the distribution\nand evaluation of an ontology in RDF/OWL format within the ERTMS/ETCS\ndomain [17]. A snapshot of this ontology is presented in Figure 7 (full version\nis referenced on the Notebook). The complete ontology contains 28 ontology\nclasses, including 25 entity nodes, 3 types of dependencies and 25 dependency\ninstances (7 requires, 17 refines, 1 conflicts).\nMachine learning-based: fine-tuning BERT with PURE annotated\ndataset. We use a version of BERT (BERT-base-uncased) and we extend it\nwith a fine-tuning process for the sequence classification tasks. In this context,\na pair of requirements is used as the full sequence. The fine-tuning process in-\nvolves the entire PURE annotated dataset, with a 10-fold cross-validation ap-\nproach (90% for training, 10% for testing) to aggregate results and predicted\nrequirement relations.\n20\nQuim Motger and Xavier Franch\nEmpirical analysis. Table 3 reports a summarized overview of the empirical\nanalysis we conducted to compare both strategies, including design specifica-\ntions, technologies used, retrieved dependencies and execution time. We do not\nfocus on qualitative measures for functional appropriateness (i.e., accuracy, pre-\ncision, recall, F-measure), as baseline artefacts for ground-truth analysis (i.e.,\nETC/ERTMS ontology, PURE annotated dataset) are limited and restricted\nto external constraints (e.g., the ontology does not model similar dependencies,\nwhile there are no refines annotated instances). Furthermore, the purpose of this\nanalysis is to provide a comprehensive overview of the strengths and limitations\nof each approach, rather than comparing their reliability in a purely academic,\nillustrative context.\nTable 3. Comparative analysis summary of an ontology-based approach vs. a machine\nlearning-based approach. Evaluation is conducted using the PURE annotated dataset\nfor multiclas classification, consisting on 190 unique requirements (see Section 2.4)\nType\nOntology-based\nMachine learning-based\nMethod\nOpenReq-DD\nwith\ndomain-\nspecific RDF/OWL ontology\nFine-tuned\nBERT\ninstance\nwith annotated PURE dataset\nDesign\nOntology generation (5 hours)\n–\nTechnologies\nOWL/RDF\n(ontology\ndesign)\nWordNet (similarity analysis)\nBERT (seq. classification)\nNLP Prep.\nTransformer-based tokenizer, Attribute ruler, Lemmatizer, Part-\nof-Speech tagger, N-gram generation\n#non-relevant\n16,969 pairs\n16,558 pairs\n#refines\n307 pairs\n–\n#requires\n679 pairs\n721 pairs\n#similar\n–\n676 pairs\nEx. Time\n27 seconds\n9,224 seconds (model training)\n381 seconds (inference)\nIn terms of NLP-based knowledge representation, both approaches utilize\nthe same subset of syntactic techniques (tokenization, lemmatization, PoS tag-\nging, n-gram generation). Notably, the ontology-based method requires ontology\ngeneration, which in this context took approximately 5 hours [17], highlighting\na dependence on expert knowledge and conceptual modeling, a characteristic\nnot shared by the machine learning-based method. In terms of requirement de-\npendencies, the ontology-based approach identifies 307 refines and 679 requires\ninstances, while the machine learning-based approach detects 721 requires pairs\nand 676 similar instances. This discrepancy underscores the nuanced nature of\nrelations captured by each method, limited by either the ontology knowledge or\nthe annotated dataset. Regarding execution time, the ontology-based approach\nexhibits efficiency, completing the extraction process in 27 seconds, while the\nNLP-based Relation Extraction Methods in RE\n21\nmachine learning-based method involves a training duration of 9,224 seconds\n(˜2.5 hours) and an inference time of 381 seconds (˜6 minutes) for all potentially\ndependent requirement pairs.\n5.2\nDiscussion\nThe selection of an appropriate relation extraction method is influenced by\nmultiple contextual factors (domain, data, expert knowledge, computational re-\nsources, skills...). To guide this selection process, we use the empirical analysis\nin Section 5.1 as a trigger for the elicitation of the strengths and limitations of\nretrieval-based and machine learning-based strategies.\nRetrieval-based relation extraction methods present the following strengths\nwith respect to machine learning methods:\n– Interpretability. Retrieval-based methods are relatively simple to imple-\nment and understand, with respect to the theoretical foundations of syn-\ntactic and semantic relations between requirements. On the other hand, in-\nterpretability of deep learning models can be limited, making it difficult to\nunderstand how specific relations are extracted.\n– Representativeness. Retrieval-based systems can be used to extract a\nwide, customized range of relations, while machine learning approaches may\nsuffer from limited representativeness if the data is biased or incomplete.\n– Data availability. They can be used without annotated requirement re-\nlation instances, whereas machine learning methods require substantial la-\nbelling efforts to create training datasets.\n– Computational resources. Retrieval-based approaches are typically low\nconsuming in terms of computational resources, while some deep learning\nmodels may be computationally expensive and require powerful hardware\nfor training and inference.\nOn the other hand, machine learning methods present the following strengths:\n– Expressive power. Machine learning methods can handle complex patterns\nand relationships in requirements text, whereas retrieval-based methods can\nbe brittle, meaning that they may not be able to handle unexpected or\nunusual text, including format and content.\n– Adaptability. Machine learning methods can automatically learn from data\nand adapt to new patterns without the need for explicit human-defined rules,\nmaking them more flexible. On the other hand, retrieval-based approaches\ncan be difficult to maintain, as they require the rules to be updated as the\nrequirements and relations change.\n– Scalability. Machine learning methods can typically handle a large amount\nof data efficiently, enabling scalability to big datasets, whereas retrieval-\nbased approaches might not always adapt to large-scale contexts.\n– Required knowledge. Deep learning models used in machine learning\napproaches can capture intricate semantic relationships, whereas retrieval-\nbased methods typically require human expert knowledge for conceptual\nmodelling of relations (e.g., ontologies, patterns, rules, dictionaries).\n22\nQuim Motger and Xavier Franch\n6\nChallenges and Future Directions\nAmong the most significant limitations of NLP-based relation extraction meth-\nods, and triggered by the surveyed literature in this chapter, we highlight the\nfollowing:\n– Lack of annotated data. One of the major challenges in applying NLP\ntechniques for relation extraction in RE is the scarcity of labelled data.\nSupervised machine learning methods heavily rely on annotated requirement\npairs for training, and obtaining a sufficient amount of accurately labelled\ndata is often a time-consuming and costly process.\n– Not uniform categorization of relations. Defining a standardized and\nuniform categorization of requirement relations remains a significant chal-\nlenge. In real-world scenarios, different projects and domains might require\ndistinct relation types, making it difficult to generalize extraction methods\nacross diverse RE contexts.\n– Modelling human knowledge (semantics of relation types). Cap-\nturing and incorporating domain-specific human knowledge and semantics\nrelated to dependency types into automated extraction techniques is com-\nplex. Retrieval-based and machine learning-based methods might struggle to\nfully comprehend and accurately model the intricacies of such knowledge.\nTo address the limitations and challenges mentioned above, state-of-the-art\nsolutions in the NLP4RE field focus on the following directions:\n– Extend contributions using encoder-based LLMs. Recent advance-\nments in encoder-based large language models, such as BERT, have shown\npromising results in various NLP4RE tasks, including requirements extrac-\ntion [14], classification [30] and disambiguation [19], among others. Extend-\ning the application of these models to relation extraction in RE can leverage\ntheir contextual understanding and semantic representation capabilities to\nimprove accuracy and adaptability, even in data-scarce scenarios.\n– Explore generative LLMs for zero-shot or few-shot relation ex-\ntraction. Generative language models such as GPT-4 [36], LLaMA [52] and\nPaLM [10] have demonstrated their ability to produce coherent and contex-\ntually relevant text. In the field of NLP4RE, these models are being used\nfor requirements generation and augmentation [28] and user analysis [12],\namong others. Investigating their potential for generating relation instances\nbetween requirements can assist in building larger labelled datasets and ad-\ndress the issue of data scarcity in supervised learning.\n7\nConclusions\nThe fusion of NLP and RE has unveiled innovative pathways for the auto-\nmated extraction of requirement relations. From purely retrieval-based to ma-\nchine learning-based approaches, each with distinct advantages, these methods\nNLP-based Relation Extraction Methods in RE\n23\nlay the groundwork to support the extraction and categorization of dependencies\nin specialized, domain-specific domains. Retrieval-based methods offer simplicity\nthrough linguistic patterns and predefined ontologies, while machine learning-\nbased approaches harness language models for increased adaptability and effi-\nciency in large scale domains.\nHowever, several challenges persist, such as the scarcity of annotated data\nand the absence of standardized relation categorization. Encouragingly, encoder-\nbased large language models like BERT hold promise for enhancing extraction ac-\ncuracy, even with limited data (i.e., few-shot learning). Additionally, generative\nmodels offer great potential in generating relation instances to enrich datasets\nor even assisting in the relation extraction process itself.\nThese methodologies not only deepen our understanding of the landscape of\nNLP-based relation extraction, but also inspire future innovations that bridge\nhuman expertise with automated analysis. The collaboration between linguistic\ninsights and computational advancements paves the way for improved decision-\nmaking, efficient requirement management, and enhanced system design. All in\nall, this chapter aims at encouraging researchers and practitioners to further\ncontribute to the progression of automated relation extraction in the domain of\nNLP4RE.\nAcknowledgements\nWith the support from the Secretariat for Universities and Research of the Min-\nistry of Business and Knowledge of the Government of Catalonia and the Euro-\npean Social Fund.\nReferences\n1. Abbas, M., Ferrari, A., Shatnawi, A., Enoiu, E., Saadatmand, M., Sundmark, D.:\nOn the relationship between similar requirements and similar software: A case\nstudy in the railway domain. Requirements Engineering 28(1), 23–47 (mar 2023).\nhttps://doi.org/10.1007/s00766-021-00370-4\n2. Abeba, G., Alemneh, E.: Identification of Nonfunctional Requirement Conflicts:\nMachine Learning Approach. In: Lecture Notes of the Institute for Computer Sci-\nences, Social-Informatics and Telecommunications Engineering, LNICST. vol. 411\nLNICST, pp. 435–445. Springer Science and Business Media Deutschland GmbH\n(2022). https://doi.org/10.1007/978-3-030-93709-6 29\n3. Achimugu,\nP.,\nSelamat,\nA.,\nIbrahim,\nR.,\nMahrin,\nM.N.:\nA\nsys-\ntematic\nliterature\nreview\nof\nsoftware\nrequirements\nprioritization\nre-\nsearch.\nInformation\nand\nSoftware\nTechnology\n56(6),\n568–585\n(2014).\nhttps://doi.org/https://doi.org/10.1016/j.infsof.2014.02.001\n4. Alhoshan,\nW.,\nZhao,\nL.,\nBatista-Navarro,\nR.:\nUsing\nsemantic\nframes\nto\nidentify related textual requirements: An initial validation. In: International\nSymposium\non\nEmpirical\nSoftware\nEngineering\nand\nMeasurement\n(2018).\nhttps://doi.org/10.1145/3239235.3267441\n24\nQuim Motger and Xavier Franch\n5. Alhoshan, W., Batista-Navarro, R., Zhao, L.: Semantic frame embeddings for de-\ntecting relations between software requirements. In: IWCS 2019 - Proceedings of\nthe 13th International Conference on Computational Semantics - Student Papers.\npp. 44–51 (2019). https://doi.org/10.18653/v1/w19-0606\n6. Asyrofi, R., Siahaan, D., Priyadi, Y.: Extraction Dependency Based on Evolution-\nary Requirement Using Natural Language Processing. In: 2020 3rd International\nSeminar on Research of Information Technology and Intelligent Systems, ISRITI\n2020. pp. 332–337 (2020). https://doi.org/10.1109/ISRITI51436.2020.9315489\n7. Board, I.R.E.: IREB and the CPR, https://www.ireb.org/en/about/basics/\n8. B¨uhne, S., Herrmann, A.: Handbook Requirements Management - Education and\nTraining for IREB Certified Professional for Requirements Engineering. IREB -\nInternational Requirements Engineering Board (2022)\n9. Carlshamre, P., Sandahl, K., Lindvall, M., Regnell, B., Natt och Dag, J.: An indus-\ntrial survey of requirements interdependencies in software product release planning.\nIn: Proceedings Fifth IEEE International Symposium on Requirements Engineer-\ning. pp. 84–91 (2001). https://doi.org/10.1109/ISRE.2001.948547\n10. Chowdhery, A., Narang, S., Devlin, J., et al.: Palm: Scaling language modeling\nwith pathways (2022)\n11. Clark, K., Manning, C.D.: Deep reinforcement learning for mention-ranking coref-\nerence models. In: Proceedings of the 2016 Conference on Empirical Methods\nin Natural Language Processing. pp. 2256–2262. Association for Computational\nLinguistics, Austin, Texas (Nov 2016). https://doi.org/10.18653/v1/D16-1245,\nhttps://aclanthology.org/D16-1245\n12. Clements, D., Giannis, E., Crowe, F., Balapitiya, M., Marshall, J., Papadopou-\nlos, P., Kanij, T.: An innovative approach to develop persona from application\nreviews. In: International Conference on Evaluation of Novel Approaches to Soft-\nware Engineering, ENASE - Proceedings. vol. 2023-April, p. 701 – 708 (2023).\nhttps://doi.org/10.5220/0011996000003464\n13. Cui, L., Yang, D., Cheng, J., Xiao, Y.: Incorporating Syntactic Information into\nRelation Representations for Enhanced Relation Extraction. In: Lecture Notes in\nComputer Science (including subseries Lecture Notes in Artificial Intelligence and\nLecture Notes in Bioinformatics). vol. 12714 LNAI, pp. 416–428. Springer Science\nand Business Media Deutschland GmbH (2021). https://doi.org/10.1007/978-3-\n030-75768-7 33\n14. De Ara´ujo, A.F., Marcacini, R.M.: Re-bert: Automatic extraction of soft-\nware requirements from app reviews using bert language model. In: Proceed-\nings of the ACM Symposium on Applied Computing. p. 1321 – 1327 (2021).\nhttps://doi.org/10.1145/3412841.3442006\n15. Deshpande, G.: SReYantra: Automated software requirement inter-dependencies\nelicitation, analysis and learning. In: Proceedings - 2019 IEEE/ACM 41st Interna-\ntional Conference on Software Engineering: Companion, ICSE-Companion 2019.\npp. 186–187 (2019). https://doi.org/10.1109/ICSE-Companion.2019.00076\n16. Deshpande,\nG.,\nArora,\nC.,\nRuhe,\nG.:\nData-driven\nelicitation\nand\nopti-\nmization\nof\ndependencies\nbetween\nrequirements.\nIn:\n2019\nIEEE\n27th\nIn-\nternational Requirements Engineering Conference (RE). pp. 416–421 (2019).\nhttps://doi.org/10.1109/RE.2019.00055\n17. Deshpande, G., Motger, Q., Palomares, C., Kamra, I., Biesialska, K., Franch,\nX.,\nRuhe,\nG.,\nHo,\nJ.:\nRequirements\ndependency\nextraction\nby\nintegrat-\ning active learning with ontology-based retrieval. In: 2020 IEEE 28th In-\nternational\nRequirements\nEngineering\nConference\n(RE).\npp.\n78–89\n(2020).\nhttps://doi.org/10.1109/RE48521.2020.00020\nNLP-based Relation Extraction Methods in RE\n25\n18. Dick, J., Hull, E., Jackson, K.: Requirements Engineering. Springer (2017)\n19. Ezzini, S., Abualhaija, S., Arora, C., Sabetzadeh, M.: Automated handling of\nanaphoric ambiguity in requirements: A multi-solution study. In: Proceedings -\nInternational Conference on Software Engineering. vol. 2022-May, p. 187 – 199\n(2022)\n20. Ferrari, A., Spagnolo, G.O., Gnesi, S.: Pure: A dataset of public requirements doc-\numents. In: 2017 IEEE 25th International Requirements Engineering Conference\n(RE). pp. 502–505 (2017). https://doi.org/10.1109/RE.2017.29\n21. Firmawan, D., Siahaan, D.: Bidirectional Long Short-Term Memory for En-\ntailment Identification in Requirement Specifications Using Information from\nUse\nCase\nDiagrams.\nIn:\n2021\nInternational\nSeminar\non\nMachine\nLearn-\ning, Optimization, and Data Science, ISMODE 2021. pp. 331–336 (2022).\nhttps://doi.org/10.1109/ISMODE53584.2022.9743037\n22. Fischbach, J., Frattini, J., Vogelsang, A.: CiRA: A Tool for the Automatic De-\ntection of Causal Relationships in Requirements Artifacts. In: CEUR Workshop\nProceedings. vol. 2857 (2021)\n23. Fischbach, J., Springer, T., Frattini, J., Femmer, H., Vogelsang, A., Mendez,\nD.: Fine-Grained Causality Extraction from Natural Language Requirements Us-\ning Recursive Neural Tensor Networks. In: Proceedings of the IEEE Interna-\ntional Conference on Requirements Engineering. vol. 2021-Septe, pp. 60–69 (2021).\nhttps://doi.org/10.1109/REW53955.2021.00016\n24. Frattini, J., Junker, M., Unterkalmsteiner, M., Mendez, D.: Automatic Extraction\nof Cause-Effect-Relations from Requirements Artifacts. In: Proceedings - 2020 35th\nIEEE/ACM International Conference on Automated Software Engineering, ASE\n2020. pp. 561–572 (2020). https://doi.org/10.1145/3324884.3416549\n25. Fritz,\nS.,\nJaenicke,\nM.,\nOvtcharova,\nJ.,\nWicaksono,\nH.:\nContext-\nsensitive\nAssistance\nin\nRequirements-based\nKnowledge\nManagement.\nIn:\nACM\nInternational\nConference\nProceeding\nSeries.\npp.\n47–54\n(2020).\nhttps://doi.org/10.1145/3443279.3443306\n26. Glinz, M.: A Glossary of Requirements Engineering Terminology. IREB - Interna-\ntional Requirements Engineering Board (2022)\n27. Glinz, M., van Loenhoud, H., Staal, S., B¨uhne, S.: Handbook for the CPRE Founda-\ntion Level according to the IREB Standard -Education and Training for Certified\nProfessional for Requirements Engineering (CPRE) - Foundation Level. IREB -\nInternational Requirements Engineering Board (2022)\n28. Grasler, I., Preus, D., Brandt, L., Mohr, M.: Efficient extraction of technical\nrequirements applying data augmentation. In: ISSE 2022 - 2022 8th IEEE In-\nternational Symposium on Systems Engineering, Conference Proceedings (2022).\nhttps://doi.org/10.1109/ISSE54508.2022.10005452\n29. Guo, W., Zhang, L., Lian, X.: Putting software requirements under the micro-\nscope: Automated extraction of their semantic elements. In: Proceedings of the\nIEEE International Conference on Requirements Engineering. pp. 416–417 (2021).\nhttps://doi.org/10.1109/RE51729.2021.00048\n30. Hey, T., Keim, J., Koziolek, A., Tichy, W.F.: Norbert: Transfer learning for\nrequirements classification. In: Proceedings of the IEEE International Confer-\nence on Requirements Engineering. vol. 2020-August, p. 169 – 179 (2020).\nhttps://doi.org/10.1109/RE48521.2020.00028\n31. Jadallah, N., Fischbach, J., Frattini, J., Vogelsang, A.: CATE: CAusality Tree Ex-\ntractor from Natural Language Requirements. Proceedings of the IEEE Interna-\ntional Conference on Requirements Engineering 2021-September, 77–79 (2021).\nhttps://doi.org/10.1109/REW53955.2021.00018\n26\nQuim Motger and Xavier Franch\n32. van Lamsweerde, A.: Requirements Engineering: From System Goals to UML Mod-\nels to Software Specifications. Wiley Publishing, 1st edn. (2009)\n33. Mokammel, F., Coatan´ea, E., Coatan´ea, J., Nenchev, V., Blanco, E., Pietola, M.:\nAutomatic requirements extraction, analysis, and graph representation using an\napproach derived from computational linguistics. Systems Engineering 21(6), 555–\n575 (2018). https://doi.org/10.1002/sys.21461\n34. Motger, Q., Borrull, R., Palomares, C., Marco, J.: OpenReq-DD: A requirements\ndependency detection tool. In: CEUR Workshop Proceedings. vol. 2376 (2019)\n35. Ogawa,\nT.,\nOhnishi,\nA.,\nShimakawa,\nH.:\nA\nRetrieval\nMethod\nof\nSoft-\nware\nRequirements\nfrom\nJapanese\nRequirements\nDocument\nwith\nDepen-\ndency Analysis and Keywords. In: Proceedings of IEEE Asia-Pacific Con-\nference\non\nComputer\nScience\nand\nData\nEngineering,\nCSDE\n2022\n(2022).\nhttps://doi.org/10.1109/CSDE56538.2022.10089332\n36. OpenAI: GPT-4 Technical Report (2023)\n37. Pohl, K.: Requirements Engineering: Fundamentals, Principles, and Techniques.\nSpringer Publishing Company, Incorporated, 1st edn. (2010)\n38. of Princeton University, T.: WordNet - A Lexical Database for English , https:\n//wordnet.princeton.edu/\n39. Priyadi, Y., Djunaidy, A., Siahaan, D.: Requirements Dependency Graph Mod-\neling on Software Requirements Specification Using Text Analysis. In: 2019 1st\nInternational Conference on Cybernetics and Intelligent System, ICORIS 2019.\npp. 221–226 (2019). https://doi.org/10.1109/ICORIS.2019.8874920\n40. Raatikainen, M., Motger, Q., Luders, C.M., Franch, X., Myllyaho, L., Ket-\ntunen, E., Marco, J., Tiihonen, J., Halonen, M., Mannisto, T.: Improved\nManagement of Issue Dependencies in Issue Trackers of Large Collaborative\nProjects. IEEE Transactions on Software Engineering 49(4), 2128–2148 (2022).\nhttps://doi.org/10.1109/TSE.2022.3212166\n41. for Railways, E.U.A.: European rail traffic management system, https://en.\nwikipedia.org/wiki/European_Rail_Traffic_Management_System\n42. Robertson, S., Robertson, J.: Mastering the Requirements Process: Getting Re-\nquirements Right. Addison-Wesley Professional, 3rd edn. (2012)\n43. Schlutter, A., Vogelsang, A.: Knowledge Extraction from Natural Language Re-\nquirements into a Semantic Relation Graph. In: Proceedings - 2020 IEEE/ACM\n42nd International Conference on Software Engineering Workshops, ICSEW 2020.\npp. 373–379 (2020). https://doi.org/10.1145/3387940.3392162\n44. Schlutter, A., Vogelsang, A.: Improving Trace Link Recovery Using Semantic Re-\nlation Graphs and Spreading Activation. In: Lecture Notes in Computer Science\n(including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in\nBioinformatics). vol. 12685 LNCS, pp. 37–53. Springer Science and Business Media\nDeutschland GmbH (2021). https://doi.org/10.1007/978-3-030-73128-1 3\n45. Shah, U.S., Patel, S.J., Jinwala, D.C.: Constructing a knowledge-based quality\nattributes relationship matrix to identify conflicts in non-functional requirements.\nJournal of Computational and Theoretical Nanoscience 17(1), 122–129 (2020).\nhttps://doi.org/10.1166/jctn.2020.8638\n46. Sonbol, R., Rebdawi, G., Ghneim, N.: Towards a Semantic Representation for\nFunctional Software Requirements. In: Proceedings - 7th International Work-\nshop on Artificial Intelligence and Requirements Engineering, AIRE 2020. pp. 1–8\n(2020). https://doi.org/10.1109/AIRE51212.2020.00007\n47. spaCy: English roberta-based transformer pipeline, https://spacy.io/models/\nen#en_core_web_trf\nNLP-based Relation Extraction Methods in RE\n27\n48. Sree-Kumar,\nA.,\nPlanas,\nE.,\nClaris´o,\nR.:\nExtracting\nsoftware\nproduct\nline\nfeature\nmodels\nfrom\nnatural\nlanguage\nspecifications.\nIn:\nACM\nIn-\nternational\nConference\nProceeding\nSeries.\nvol.\n1,\npp.\n43–53\n(2018).\nhttps://doi.org/10.1145/3233027.3233029\n49. Svahnberg,\nM.,\nGorschek,\nT.,\nFeldt,\nR.,\nTorkar,\nR.,\nSaleem,\nS.B.,\nShafique,\nM.U.:\nA\nsystematic\nreview\non\nstrategic\nrelease\nplanning\nmodels.\nInformation\nand\nSoftware\nTechnology\n52(3),\n237–248\n(2010).\nhttps://doi.org/https://doi.org/10.1016/j.infsof.2009.11.006\n50. Tahvili, S., Ahlberg, M., Fornander, E., Afzal, W., Saadatmand, M., Bohlin,\nM., Sarabi, M.: Functional Dependency Detection for Integration Test Cases.\nIn: Proceedings - 2018 IEEE 18th International Conference on Software Qual-\nity, Reliability, and Security Companion, QRS-C 2018. pp. 207–214 (2018).\nhttps://doi.org/10.1109/QRS-C.2018.00047\n51. Tanjong, E., Carver, D.: Improving Impact and Dependency Analysis through\nSoftware Categorization Methods. Proceedings - 2021 9th International Conference\nin Software Engineering Research and Innovation, CONISOFT 2021 pp. 142–151\n(2021). https://doi.org/10.1109/CONISOFT52520.2021.00029\n52. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix, T.,\nRozi`ere, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave,\nE., Lample, G.: Llama: Open and efficient foundation language models (2023)\n53. W3C: OWL - Semantic Web Standards, https://www.w3.org/OWL/\n54. W3C: RDF - Resource Description Framework, https://www.w3.org/RDF/\n55. Zhao, L., Alhoshan, W., Ferrari, A., Letsholo, K.J., Ajagbe, M.A., Chioasca, E.V.,\nBatista-Navarro, R.T.: Natural Language Processing for Requirements Engineering\n(jun 2021). https://doi.org/10.1145/3444689\n",
  "categories": [
    "cs.SE"
  ],
  "published": "2024-01-22",
  "updated": "2024-01-22"
}