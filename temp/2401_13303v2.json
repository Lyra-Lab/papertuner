{
  "id": "http://arxiv.org/abs/2401.13303v2",
  "title": "MaLA-500: Massive Language Adaptation of Large Language Models",
  "authors": [
    "Peiqin Lin",
    "Shaoxiong Ji",
    "Jörg Tiedemann",
    "André F. T. Martins",
    "Hinrich Schütze"
  ],
  "abstract": "Large language models (LLMs) have advanced the state of the art in natural\nlanguage processing. However, their predominant design for English or a limited\nset of languages creates a substantial gap in their effectiveness for\nlow-resource languages. To bridge this gap, we introduce MaLA-500, a novel\nlarge language model designed to cover an extensive range of 534 languages. To\ntrain MaLA-500, we employ vocabulary extension and continued pretraining on\nLLaMA 2 with Glot500-c. Our intrinsic evaluation demonstrates that MaLA-500 is\nbetter at predicting the given texts of low-resource languages than existing\nmultilingual LLMs. Moreover, the extrinsic evaluation of in-context learning\nshows that MaLA-500 outperforms previous LLMs on SIB200 and Taxi1500 by a\nsignificant margin, i.e., 11.68% and 4.82% marco-average accuracy across\nlanguages. We release MaLA-500 at https://huggingface.co/MaLA-LM",
  "text": "Preprint. Under review.\nMaLA-500: Massive Language Adaptation of Large Language\nModels\nPeiqin Lin∗1,2, Shaoxiong Ji∗3, J¨org Tiedemann3, Andr´e F. T. Martins4,5,6, Hinrich Sch¨utze1,2\n1Center for Information and Language Processing, LMU Munich\n2Munich Center for Machine Learning\n3University of Helsinki\n4Instituto Superior T´ecnico (Lisbon ELLIS Unit)\n5Instituto de Telecomunicac¸ ˜oes\n6Unbabel\nlinpq@cis.lmu.de, shaoxiong.ji@helsinki.fi\nAbstract\nLarge language models (LLMs) have advanced the state of the art in natural\nlanguage processing. However, their predominant design for English or\na limited set of languages creates a substantial gap in their effectiveness\nfor low-resource languages. To bridge this gap, we introduce MaLA-500,\na novel large language model designed to cover an extensive range of\n534 languages. To train MaLA-500, we employ vocabulary extension and\ncontinued pretraining on LLaMA 2 with Glot500-c. Our intrinsic evalua-\ntion demonstrates that MaLA-500 is better at predicting the given texts of\nlow-resource languages than existing multilingual LLMs. Moreover, the\nextrinsic evaluation of in-context learning shows that MaLA-500 outper-\nforms previous LLMs on SIB200 and Taxi1500 by a significant margin, i.e.,\n11.68% and 4.82% marco-average accuracy across languages. We release\nMaLA-500 at https://huggingface.co/MaLA-LM.\n1\nIntroduction\nLarge Language Models (LLMs), e.g., LLaMA (Touvron et al., 2023a;b), Mistral (Jiang\net al., 2023; 2024), and ChatGPT,1 have shown remarkable performance in natural language\nunderstanding and generation. Follow-up studies (Bang et al., 2023; Lai et al., 2023; Ahuja\net al., 2023a;b) observe that these English-centric LLMs, such as LLaMA with mainly English\nas the training data, are capable of handling some high-resource non-English languages,\nbenefiting from the inclusion of non-English language data during pretraining. However,\ntheir applicability to low-resource languages is still limited due to data scarcity.\nPrevious studies have released pretrained multilingual models with mostly encoder-only\ntransformer architectures, e.g., multilingual BERT (Devlin et al., 2019) and XLM-R (Conneau\net al., 2020), for around 100 languages. The paradigm shift from encoder-only to decoder-\nonly achieves scalability for large language models with billions of model parameters,\nleading to the development of open multilingual models. Recently, several generative\nmultilingual LLMs, such as XGLM (Lin et al., 2021), mGPT (Shliazhko et al., 2022), and\nBLOOM (Scao et al., 2022), have emerged. Notably, the current language coverage for these\ngenerative LLMs is limited to up to 60 languages, highlighting the remaining need for\nfurther work on massively multilingual LLMs for many natural languages.\nImaniGooghari et al. (2023) have achieved a significant milestone in the realm of massive\nlanguage adaptation by extending the language coverage of a small-scale multilingual\nlanguage model, XLM-R (Conneau et al., 2020) - an auto-encoding model with 278M param-\neters, from 100 languages to an impressive number of 534 languages, and introducing an\nextended model, Glot500-m with 395M parameters. ImaniGooghari et al. (2023) introduce\nthe Glot500-c corpora spanning 534 languages from 47 language families, and then apply\n*Equal contribution.\n1https://openai.com/blog/chatgpt\n1\narXiv:2401.13303v2  [cs.CL]  3 Apr 2024\nPreprint. Under review.\nvocabulary extension and continued pretraining to create Glot500-m. The introduction of\nGlot500-c mitigates the challenge of data scarcity for low-resource languages. Moreover,\nthe adaptation method is more favorable than training from scratch, as it requires fewer\ncomputational resources and emits a smaller carbon footprint. This success serves as a\nstrong motivation for our exploration into the massive language adaptation of LLMs.\nThis work aims to extend the capabilities of LLMs to encompass a wider range of languages.\nExisting works like ImaniGooghari et al. (2023) on language adaptation of pretrained models\nprovide extended coverage across a wide linguistic spectrum but are limited to relatively\nsmall model sizes - mostly at the hundred million scales, while other works like Yong et al.\n(2022) extended generative LLMs but are limited to a small number of languages. Our\nstudy pushes the boundaries by exploring language adaptation techniques for LLMs with\nmodel parameters scaling up to 10 billion for 534 languages. Our investigation delves\ninto generative LLMs with a substantial increase in model parameters and their in-context\nlearning capabilities in diverse languages, especially low-resource languages. This augmen-\ntation enables us to enhance contextual and linguistic relevance across a diverse range of\nlanguages.\nWe address the challenges of adapting LLMs to low-resource languages, such as data sparsity,\ndomain-specific vocabulary, and linguistic diversity. Specifically, we study continued\npretraining of open LLM, i.e., LLaMA 2 (Touvron et al., 2023b), vocabulary extension, and\nadaptation techniques, i.e., LoRA low-rank reparameterization (Hu et al., 2022). We deploy\ndistributed training and release MaLA-500 that covers more than 500 languages in various\ndomains. We evaluate MaLA-500 using intrinsic measures on held-out Glot500-c test set\nand parallel data and extrinsic metrics on downstream benchmarks: SIB200 and Taxi1500.\nThe results show that MaLA-500 outperforms existing open LLMs of close or slightly larger\nmodel size. This work broadens the accessibility of LLMs, making them valuable for a more\ndiverse set of language-specific use cases, especially for low-resource ones, and addressing\nthe equality issue by removing language barriers for speakers of many languages, especially\nthose underrepresented languages covered by existing LLMs.\n2\nMassive Language Adaptation\nThe principle of massive language adaptation of large language models accommodates the\nutilization of a massively multilingual corpus (Section 2.1), the strong base LLM (Section 2.2),\nand the technique for effective language adaptation: vocabulary extension (Section 2.3) and\ncontinued pretraining (Section 2.4).\n2.1\nData\nWe use Glot500-c (ImaniGooghari et al., 2023) covering 534 languages2 as the training data of\nMaLA-500. See §A for the list of languages with their data amounts. The original number of\nsentences ranges from 10 thousand to 63 million. Note that Glot500-c does not put full effort\ninto collecting data for high-resource languages but focuses on low-resource languages. We\nsample languages from the imbalanced dataset according to a multinomial distribution,\nwith α = 0.3 for vocabulary extension and continued pretraining. We use different scales for\nsampling data to be used in model training and vocabulary construction. After sampling,\nthe number of sentences for training ranges from 600 thousand to 8 million per language,\nleading to 1 billion sentences in total. The number of sentences for vocabulary construction\nranges from 30 thousand to 400 thousand, making a total of 50 million sentences.\n2.2\nModel\nWe choose LLaMA 2 (Touvron et al., 2023b) to start continual training. LLaMA series\nmodels (Touvron et al., 2023a), with model weights released publicly, have gained popularity\nin the research community. Despite being English-centric compared to their multilingual\n2We define languages using the ISO 639-3 code combined with the corresponding written script.\nFor example, “eng Latn” represents English written in the Latin script.\n2\nPreprint. Under review.\ncounterparts, they have shown remarkable capacity for multiple languages (Ahuja et al.,\n2023b). We choose the latest LLaMA 2, trained on 2 trillion tokens, as our base model to\nbenefit from its outstanding language capacity. Our study chooses the 7B model with 32\ntransformer layers, and leaves the extension of LLMs with larger sizes as a future work.\n2.3\nVocabulary Extension\nThe original LLaMA 2’s 32,000 tokenizer covers English and a small fraction of other Eu-\nropean languages using Latin or Cyrillic scripts. To enhance its capability and encoding\nefficiency for a broader range of languages, we extend the vocabulary with Glot500-c. Specif-\nically, we initially train a multilingual tokenizer with SentencePiece (Kudo & Richardson,\n2018) on the sampled Glot500-c with a vocabulary of 250,000. Subsequently, we merge\nthe trained tokenizer with the original LLaMA 2 tokenizer by taking the union of their\nvocabularies. As a result, we obtain the MaLA-500’s tokenizer with a vocabulary size of\n260,164. After vocabulary extension and resizing the embedding layer, the model size\nbecomes 8.6B.\nWe measure the impact of vocabulary extension on the development set of Glot500-c by\nanalyzing the reduction in segmentation length for each language. The results indicate that\nthe effect of vocabulary extension varies, ranging from 8% (English, eng Latn) to 88% (Oriya,\nori Orya). Unsurprisingly, vocabulary extension has a larger effect on languages written\nin non-Latin scripts than on those in the Latin script. However, for some low-resource\nlanguages written in the Latin script, e.g., Kabiy`e (kbp Latn) and Vietnamese (vie Latn),\nthe segmentation length is shortened by around 50%.\n2.4\nContinued Pretraining\nWe employ continued pretraining for language adaptation with low-rank adaptation (LoRA,\nHu et al., 2022) to enable parameter-efficient training, given the limitation of our computing\nresources. LoRA injects trainable rank decomposition matrices, which approximate the\nlarge weight matrices with a lower rank, to the pretrained model weights. It reduces the\ncomputational complexity and thus saves the training cost while retaining high model\nquality (Hu et al., 2022). We continually train the casual language model to update the rank-\ndecomposition matrices, embedding layer, and language modeling head while freezing the\ntransformer weights of pretrained models, allowing the continually trained language model\nto learn from new data in new languages without completely losing its previous language\ncapacity. Continual training of large language models requires substantial computational\nresources. We adopt efficient distributed training setups on supercomputers to make the\ntraining process feasible.\n2.5\nTraining\nHardware and Software\nWe train our model on the computing cluster with the theoretical\npeak performance of 2 petaflops on GPU nodes. We deploy distributed training on 24 Nvidia\nAmpere A100 GPUs. As for software, we utilize the Huggingface Transformers (Wolf et al.,\n2020), PEFT (Parameter-Efficient Fine-Tuning),3 and DeepSpeed (Rasley et al., 2020). We\nuse the ZeRO redundancy optimizer (Rajbhandari et al., 2020) and maximize the batch size\nthat fits the memory of each GPU. We employ mixed-precision training using the bfloat16\nformat.\nHyperparameters\nThe learning rate is set at 3e-4. A weight decay of 0.01 is applied to\npenalize large weights and mitigate overfitting. The trainable LoRA module targets the\nquery and value matrices. The language model head is not decomposed by a LoRA module\nbut is trained in a full-parameter manner. In our setting, the final model has 10B parameters\nin total, in which 2B parameters are trainable. The LoRA module is incorporated with a rank\nof 8, an alpha value of 32, and a dropout rate of 0.05, contributing to the model’s adaptability\nand regularization during training. The context window is 4k. We maximize the batch size\n3https://huggingface.co/docs/peft/index\n3\nPreprint. Under review.\nto fit the memory, making a global batch size of 384. The model undergoes three training\nepochs. Checkpoints are saved every 500 steps, and we employ early stopping to select the\ncheckpoint that exhibits the most favorable average performance on downstream tasks.\nEnvironmental Impacts\nWe train our model on a carbon-neutral data center, with all\nelectricity generated with renewable hydropower, and the waste heat is utilized in district\nheating to further reduce CO2 footprint.4\n3\nEvaluation\n3.1\nBenchmarks and Setup\nWe consider both intrinsic and extrinsic measures for evaluation. Evaluation dataset statistics\nare shown in Table 1.\nDatasets\nMetric\n∥Data∥\n∥Lang∥\nDomain\nIntrinsic\nGlot500-c test (ImaniGooghari et al., 2023)\nNLL\n1000\n534\nMisc\nPBC (Mayer & Cysouw, 2014)\nNLL\n500\n370\nBible\nExtrinsic\nSIB200 (Adelani et al., 2023)\nACC\n204\n177\nMisc\nTaxi1500 (Ma et al., 2023)\nACC\n111\n351\nBible\nTable 1: Evaluation dataset statistics. ∥Data∥: test set size per language. ∥Lang∥: number of\nevaluated languages. NLL: negative log-likelihood. ACC: Accuracy.\nFor intrinsic evaluation, perplexity is not comparable across models and languages due\nto different text segmentations. Inspired by Xue et al. (2022); Yu et al. (2023), we instead\nmeasure the negative log-likelihood (NLL) of the text using the given LLMs.\nWe concatenate the dataset as the input text and adopt the sliding-window strategy.5\nThe evaluation of different LLMs uses the same data with the concatenation of sentences\nper language, thus making NLL model-comparable. In addition, we consider language-\ncomparable NLL by measuring NLL on parallel data, in which every sample in different\nlanguages contains the same semantic information. We report the model-comparable NLL\nof Glot500-c test set covering all 534 considered languages (§3.2), and language-comparable\nNLL on Parallel Bible Corpus (PBC, Mayer & Cysouw, 2014), covering 370 languages (§3.3).\nFor extrinsic evaluation, we evaluate the few-shot learning capability of MaLA-500 and\ncompare it with other LLMs on SIB200 (Adelani et al., 2023) and Taxi1500 (Ma et al., 2023).\nSIB200 is a topic classification dataset. The classification task involves seven classes, namely\nscience/technology, travel, politics, sports, health, entertainment, and geography. Our\nevaluation spans a diverse set of 177 languages, obtained by intersecting the language sets\nof SIB200 and Glot500-c. Note that the flores200-based SIB200 evaluation set is included\nin the training data since Glot500-c includes flores200, but the classification labels are not\nprovided.\nTaxi1500 is another text classification dataset spanning 351 languages. It involves six classes,\nnamely, Recommendation, Faith, Description, Sin, Grace, and Violence. Our evaluation\nefforts aim to cover as many languages as possible. However, the evaluation of massively\nmultilingual language models is a challenging task. Due to the lack of real-world multilin-\ngual evaluation benchmarks, we use this benchmark that contains religious content.\nFor in-context learning evaluation, the evaluated LLM receives a structured prompt, which\nis the concatenation of few-shot examples and the sample intended for prediction. The\nformat for both a few-shot example and the sample to predict is defined as follows:\nTemplate for SIB200:\n4https://www.csc.fi/sustainable-development\n5https://huggingface.co/docs/transformers/en/perplexity\n4\nPreprint. Under review.\nThe topic of the news [sent] is [label]\nTemplate for Taxi1500:\nThe topic of the verse [sent] is [label]\nwhere [sent] is the sentence for classification, and [label] is the ground truth. [label] is\nincluded when the sample serves as a few-shot example but is omitted when predicting\nthe sample. The constructed prompt is then used as input to the LLM. Subsequently, the\nevaluated LLM is prompted to estimate the probability of the label over the label set based\non the provided prompt.\nFor SIB200, few-shots examples are randomly sampled from the in-language training sets.\nSince randomly selecting few-shot examples for in-context learning yields random results\nfor both MaLA-500 and previous LLMs on Taxi1500, we consider the retriever-based in-\ncontext learning (Liu et al., 2022). Specifically, we use average word embeddings in layer\n8 of the Glot500 (ImaniGooghari et al., 2023) for retrieving semantic-similar samples as\nsuggested in previous work (Sabet et al., 2020) for all the compared models. The evaluation\nprocess is implemented using the lm-evaluation-harness,6 and we use accuracy (ACC) to\nmeasure the performance of classification.\n3.2\nComparison across LLMs\nWe compare MaLA-500 with LLaMA 2-7B, mGPT-13B, BLOOM-7B1, and XGLM-7.5B on\nGlot500-c test set, SIB200, Taxi1500 by computing the averaged performance across lan-\nguages, and the result are given in Table 2. Among the evaluated LLMs, LLaMA 2-7B\nperforms second-best, indicating that LLaMA 2-7B has a strong multilingual capacity and\nthat it is reasonable to select it as the base model. MaLA-500 outperforms all compared\nLLMs with a close or slightly larger model size across all the evaluated tasks. Notably,\ncompared to LLaMA 2-7B, MaLA-500 gains a lower NLL on the Glot500-c test set by 39.33,\nand has 14.94% and 4.82% improvements on SIB200 and Taxi1500, respectively. It highlights\nMaLA-500’s substantial contribution to enhancing the multilingual capacity of LLMs.\nModel\nGlot500-c test (NLL ↓)\nSIB200 (ACC ↑)\nTaxi1500 (ACC ↑)\nLLaMA 2-7B\n190.58\n42.08\n44.07\nmGPT-13B\n282.46\n45.34\n40.98\nBLOOM-7B1\n202.95\n44.63\n43.98\nXGLM-7.5B\n205.07\n34.36\n43.24\nMaLA-500\n151.25\n57.02\n48.89\nTable 2: Averaged results across languages on Glot500-c test (measured by NLL), SIB200,\nand Taxi1500 (measured by accuracy (%)) of different LLMs. mGPT has no model with\naround 7B parameters, so we choose a larger one with 13B parameters. ↓indicates the lower,\nthe better. ↑indicates the higher, the better. The best results are bold.\nFigures 1 to 3 provide detailed performance analysis across languages on Glot500-c test,\nSIB200, and Taxi1500. In those figures, we group scores into different performance bins and\ndisplay them in different colors. For Glot500-c test, MaLA-500 has more languages achieving\nbetter NLL, i.e., 61 languages with NLL less than 100 and 171 languages with NLL between\n100 and 150. Besides, MaLA-500 has 54 (10%) languages achieving NLL larger than 200,\nwhich may indicate the languages are not well covered by the measured LLM. Nevertheless,\nthe number is much less than other LLMs. For example, the second-best LLM, LLaMA 2-7B,\nhas 231 (43%) languages achieving NLL larger than 200. For both SIB200 and Taxi1500,\nMaLA-500 surpasses previous LLMs in the sense that it obtains random results in fewer\nlanguages and achieves impressive performance in more languages than its counterparts.\n6https://github.com/EleutherAI/lm-evaluation-harness\n5\nPreprint. Under review.\nLLaMA 2-7B\nmGPT-13B\nBLOOM-7B1\nXGLM-7.5B\nMaLA-500\n0\n200\n400\n600\n200-\n150-200\n100-150\n0-100\nFigure 1: NLL (lower is better) on Glot500-c test with the scores grouped into four bins\ndisplayed in different colors. X-axis: the number of languages in performance ranges.\nLLaMA 2-7B\nmGPT-13B\nBLOOM-7B1\nXGLM-7.5B\nMaLA-500\n0\n50\n100\n150\n200\n0-20\n20-40\n40-60\n60-80\nFigure 2: Accuracy (higher is better) on SIB200 with the scores grouped into four bins\ndisplayed in different colors. X-axis: the number of languages in performance ranges (%).\n3.3\nComparison across Languages\nTo check in detail how MaLA-500 performs across languages, we check the performance\nacross language families7 shown in Table 3. We observe that more high-resource language\nfamilies, e.g., Indo-European (indo1319) and Dravidian (drav1251), achieve slightly better\nperformance than low-resource language families, e.g., Sino-Tibetan (sino1245).\nIn Table 4, we present a comprehensive analysis of the top 5 performance improvements\nand declines across languages on SIB200 from MaLA-500 compared to LLaMA 2-7B. We\nobserve that MaLA-500 has substantial improvements on low-resource scripts, e.g., Kan-\nnada (kan Knda), while has worse performance on high-resource languages, e.g., Swedish\n(swe Latn), which have been well covered by LLaMA 2-7B.\nIn our comprehensive analysis of contributing factors on SIB200, we note that the corpus size\nof a language exhibits a weak correlation of 0.13 with its performance gain. In contrast, the\ncorpus size of the language family to which a language belongs demonstrates a moderate\ncorrelation of 0.40. A moderately high Pearson correlation of 0.53 is observed between the\neffect of vocabulary extension, i.e., the reduction in segmentation length, and the perfor-\nmance gain. This observation holds true for languages with both non-Latin scripts, such\nas Kannada (kan Knda), Malayalam (mal Mlym), and Tigrinya (tir Ethi), as well as Latin\nscripts, such as Igbo (ibo Latn) and Yoruba (yor Latn). It demonstrates the effectiveness of\nvocabulary extension.\n7We assign languages to families based on Glottolog: https://glottolog.org/glottolog/\nfamily.\n6\nPreprint. Under review.\nLLaMA 2-7B\nmGPT-13B\nBLOOM-7B1\nXGLM-7.5B\nMaLA-500\n0\n100\n200\n300\n400\n0-40\n40-55\n50-80\nFigure 3: Accuracy (higher is better) on Taxi1500 with the scores grouped into four bins\ndisplayed in different colors. X-axis: the number of languages in performance ranges (%).\nfamily\n∥Sent∥\nPBC (NLL ↓)\nSIB200 (ACC ↑)\nTaxi1500 (ACC ↑)\nindo1319\n988M\n145.35\n63.53\n53.03\ndrav1251\n135M\n131.29\n56.25\n54.65\naust1307\n113M\n147.37\n62.83\n49.69\nturk1311\n109M\n161.71\n57.08\n52.55\nafro1255\n100M\n165.46\n52.00\n43.74\natla1278\n57M\n141.92\n42.90\n45.52\nural1272\n50M\n137.52\n66.67\n48.58\nsino1245\n29M\n155.64\n49.30\n49.31\nother\n60M\n167.69\n55.74\n46.67\nTable 3: Performance comparison across language families on PBC, SIB200, and Taxi1500.\n∥Sent∥: sentence number used for continued pretraining in total. ↓indicates the lower, the\nbetter. ↑indicates the higher, the better.\n3.4\nEffect of Number of Shots\nFigure 4 illustrates the relationship between accuracy and the number of in-context examples\n(i.e., shots) on SIB200. As the number of in-context shots increases, there is a corresponding\nrise in accuracy. Notably, with just 1-shot, accuracy exhibits randomness at 30.88%, indi-\ncating 1-shot provides limited information for task learning. The transition from 1 shot to\n2 shots/3 shots results in a notable improvement, with performances boosted by 19.83%\nand 26.14%, respectively. This highlights the effectiveness of increasing the number of\nshots. MaLA-500 achieves its peak performance at approximately 65% accuracy with 6-10\nin-context shots. This may be attributed to the multi-class nature of the SIB200 dataset,\nnecessitating more shots for learning intricate input-label mappings.\nIn Figure 5, a more nuanced portrayal of results aligns with the observations made in\nFigure 4. In the realm of 1-shot in-context learning, approximately 50 languages exhibit\nerratic results. As the number of shots increases, there is a reduction in the number of\nlanguages achieving low accuracy (25-50%), coupled with a growing cohort achieving high\naccuracy (75-100%).\nFurther examination into individual language trends reveals that some low-resource lan-\nguages require more shots to achieve better performance (e.g., pes Arab for Persian) or\neven exhibit poor performance with 10 shots (e.g., dzo Tibt for Dzongkha and ayr Latn\nfor Central Aymara). In contrast, high-resource languages, such as fra Latn for French,\ndemonstrate impressive performance even with fewer shots, and increasing the number of\nshots results in only marginal improvement.\n7\nPreprint. Under review.\nhigh end\nlow end\nLanguage\nLLaMA 2-7B\nMaLA-500\n∆\nLanguage\nLLaMA 2-7B\nMaLA-500\n∆\nkan Knda\n17.16\n57.35\n40.19\nswe Latn\n71.08\n60.29\n-10.79\nckb Arab\n19.61\n60.29\n40.68\nrus Cyrl\n71.57\n65.20\n-06.37\nasm Beng\n17.16\n58.82\n41.66\ndan Latn\n69.12\n63.24\n-05.88\npan Guru\n14.22\n58.82\n44.60\npol Latn\n74.51\n68.63\n-05.88\nsin Sinh\n15.20\n60.29\n45.09\nukr Cyrl\n71.57\n65.69\n-05.88\nTable 4: Results for five languages each with the largest (high end) and smallest (low end)\ngains from MaLA-500 vs. LLaMA 2-7B for SIB200. ∆indicates the difference between the\nscores of MaLA-500 and LLaMA 2-7B. See §B for detailed results for each task.\nNumber of shots\nAccuracy\n00.00\n20.00\n40.00\n60.00\n80.00\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nFigure 4: In-context learning macro-average accuracy (%) on SIB200 with different number\nof shots using MaLA-500.\n4\nRelated Work\n4.1\nMultilingual Language Models\nLanguage model development has endeavored to broaden the scope of pretraining lan-\nguages to address multilingual scenarios. Pretrained multilingual models have been able\nto accommodate up to a hundred or more languages. Noteworthy examples include\nmBERT Devlin et al. (2019), which supports 104 languages, XLM-R (Conneau et al., 2020)\ncovering 100 languages, mBART (Liu et al., 2020) designed for 25 languages, mT5 (Xue et al.,\n2021) spanning 101 languages, XGLM (Lin et al., 2021) across 30 languages, GPT-3 covering\n118 languages (93% English), mGPT (Shliazhko et al., 2022) accommodating 60 languages,\nand BLOOM (Scao et al., 2022) supporting 46 languages and 13 programming languages.\nSurprisingly, two recent multilingual language models have surpassed the conventional\nlimit by supporting more than 400 languages. Glot500-m (ImaniGooghari et al., 2023) spans\n511 languages through vocabulary extension and continued training based on XLM-R.\nSERENGETI (Adebara et al., 2022) goes even further by supporting 517 African languages\nand language varieties, written in five different scripts, employing models inspired by both\nELECTRA (Clark et al., 2020) and XLM-R. MADLAD (Kudugunta et al., 2023) covers 419\nlanguages and trains an 8B language model from scratch with an adapted UL2 objective (Tay\net al., 2022). Our work is concurrent with the MADLAD-400 language model. We distin-\nguish it by: 1) language coverage. Our work covered more than 500 languages, a number\ncomparable to that of encoder-only models and surpassing MADLAD-400 by an additional\n100 languages. 2) training methods. We consider continual training to benefit from the\nlearned knowledge of the original models. 3) model architecture. We adopt an open model\narchitecture, i.e., LLaMA, while MADLAD uses decoder-only T5 architecture, which has\nnot been supported by the HuggingFace ecosystem at the time of writing, thus leading to\nadditional difficulty in usage.\n8\nPreprint. Under review.\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n0\n50\n100\n150\n200\n0-25\n25-50\n50-75\n75-100\nFigure 5: Detailed results of in-context learning on SIB200 using MaLA-500. X-axis: the\nnumber of languages in different accuracy ranges (%). Y-axis: number of shots.\n4.2\nLanguage Adaptation\nBefore the advent of LLMs, diverse approaches are employed to adapt small-scale multilin-\ngual language models to new languages. These methods include using adapters (Pfeiffer\net al., 2020; ¨Ust¨un et al., 2020; Pfeiffer et al., 2020; Nguyen et al., 2021; Faisal & Anastasopou-\nlos, 2022; Yong et al., 2022), vocabulary extension and substitution (Chau et al., 2020; Wang\net al., 2020; M¨uller et al., 2020; 2021; Pfeiffer et al., 2021; Chen et al., 2023; Downey et al.,\n2023), leveraging monolingual corpora (Ebrahimi & Kann, 2021; Alabi et al., 2022), and\nutilizing bilingual lexicons (Wang et al., 2022).\nWhile language models have been scaled up notably, their coverage is limited to a specific set\nof languages. To address this constraint, various methods have been proposed to expand the\napplicability of these large language models across a broader range of languages, catering\nto both general-purpose tasks and specific applications like machine translation. These\nmethods also involve vocabulary extension (Cui et al., 2023), continued pretraining and\ninstruction-tuning (Yong et al., 2022; Cui et al., 2023; Chen et al., 2024; Zhao et al., 2024), and\nparallel corpora exploitation (Cahyawijaya et al., 2023; Yang et al., 2023; Zhu et al., 2023; Xu\net al., 2023). Despite these efforts, massive language adaptation of LLMs for general-purpose\ntasks across diverse languages, e.g., covering many languages families and more than one\nhundred languages, remains an area yet to be thoroughly explored.\n5\nConclusion and Future Work\nWe present a pioneering effort in massive language adaptation on LLMs, focusing on\nextending LLaMA 7B to our model, MaLA-500. This adaptation involves vocabulary\nextension and continued pretraining with LoRA. Our approach leads to MaLA-500 achieving\nstate-of-the-art in-context learning capabilities, as demonstrated on the benchmarks of\nSIB200 and Taxi1500. We release the training scripts and model weights publicly to facilitate\nfuture research. This work marks a substantial advancement in applying LLMs to a diverse\nrange of languages.\nOur future work will focus on further improving the model capacity, for example, on\nmachine translation across many language pairs. Alves et al. (2023) showed that LLMs\n(LLaMA-7B and LLaMA-13B) exhibited poor performance even on English-centric high-\nresource language pairs in some cases. Translation with LLMs on low-resource languages is\nmore challenging. The LLaMA-7B model performed poorly in our preliminary experiments.\nBesides, our pretraining corpus does not intentionally include bilingual texts, and our\nMaLA-500 model is not instruction-tuned with translation data. We leave the inclusion of\nbilingual text during continual pretraining, instruction fine-tuning with translation data,\nand the evaluation on machine translation as future works.\n9\nPreprint. Under review.\nEthical Statement\nLLMs have been known to exhibit biases present in their training data. When extending\nLLMs to low-resource languages, there is a risk of propagating biases from high-resource\nlanguages to underrepresented ones. Careful attention must be paid to mitigate bias and\nensure fairness in data collection and model training. The paper aims to make LLMs more\naccessible for underrepresented languages. Still, there is a risk of creating a digital language\ndivide if certain communities are left out due to limited technological access. Future work\nwould address biases by conducting bias audits on the training data, debiasing the models\nduring generation, and continuously monitoring model outputs.\nReproducibility Statement\nWe make the following efforts to ensure reproducible research. We release the model\nweights (https://huggingface.co/MaLA-LM) and codes for training and evaluation (https:\n//github.com/MaLA-LM/mala-500).\nWe use publicly available evaluation benchmarks\nwhich can be obtained freely or by request. The results are reproducible with our released\nmodel weights and evaluation scripts,\nAcknowledgements\nWe thank Jos´e Pombal for constructive suggestions on training. This work is funded by The\nEuropean Research Council (grants #740516, #771113 and #758969), EU’s Horizon Europe\nResearch and Innovation Actions (UTTER, contract 101070631), and the European Union’s\nHorizon Europe research and innovation programme under grant agreement No 101070350\nand from UK Research and Innovation (UKRI) under the UK government’s Horizon Europe\nfunding guarantee [grant #10052546]. The authors wish to acknowledge CSC – IT Center for\nScience, Finland, for generous computational resources on the Mahti supercomputer and\nLUMI supercomputer through the LUMI extreme scale access (MOOMIN and LumiNMT).\nShaoxiong Ji and Peiqin Lin acknowledge travel support from ELISE (GA no 951847).\nReferences\nIfe Adebara, AbdelRahim Elmadany, Muhammad Abdul-Mageed, and Alcides Alcoba\nInciarte. SERENGETI: Massively multilingual language models for africa. arXiv preprint\narXiv:2212.10785, 2022.\nDavid Ifeoluwa Adelani, Hannah Liu, Xiaoyu Shen, Nikita Vassilyev, Jesujoba O. Alabi,\nYanke Mao, Haonan Gao, and En-Shiun Annie Lee. SIB-200: A simple, inclusive, and\nbig evaluation dataset for topic classification in 200+ languages and dialects. CoRR,\nabs/2309.07445, 2023. doi: 10.48550/arXiv.2309.07445. URL https://doi.org/10.48550/\narXiv.2309.07445.\nKabir Ahuja, Rishav Hada, Millicent Ochieng, Prachi Jain, Harshita Diddee, Samuel Maina,\nTanuja Ganu, Sameer Segal, Maxamed Axmed, Kalika Bali, and Sunayana Sitaram. MEGA:\nmultilingual evaluation of generative AI. CoRR, abs/2303.12528, 2023a. doi: 10.48550/\narXiv.2303.12528. URL https://doi.org/10.48550/arXiv.2303.12528.\nSanchit Ahuja, Divyanshu Aggarwal, Varun Gumma, Ishaan Watts, Ashutosh Sathe, Milli-\ncent Ochieng, Rishav Hada, Prachi Jain, Maxamed Axmed, Kalika Bali, and Sunayana\nSitaram. MEGAVERSE: benchmarking large language models across languages, modali-\nties, models and tasks. CoRR, abs/2311.07463, 2023b. doi: 10.48550/ARXIV.2311.07463.\nURL https://doi.org/10.48550/arXiv.2311.07463.\nJesujoba O. Alabi, David Ifeoluwa Adelani, Marius Mosbach, and Dietrich Klakow. Adapt-\ning pre-trained language models to african languages via multilingual adaptive fine-\ntuning. In Nicoletta Calzolari, Chu-Ren Huang, Hansaem Kim, James Pustejovsky, Leo\nWanner, Key-Sun Choi, Pum-Mo Ryu, Hsin-Hsi Chen, Lucia Donatelli, Heng Ji, Sadao\n10\nPreprint. Under review.\nKurohashi, Patrizia Paggio, Nianwen Xue, Seokhwan Kim, Younggyun Hahm, Zhong He,\nTony Kyungil Lee, Enrico Santus, Francis Bond, and Seung-Hoon Na (eds.), Proceedings\nof the 29th International Conference on Computational Linguistics, COLING 2022, Gyeongju,\nRepublic of Korea, October 12-17, 2022, pp. 4336–4349. International Committee on Compu-\ntational Linguistics, 2022. URL https://aclanthology.org/2022.coling-1.382.\nDuarte Alves, Nuno Guerreiro, Jo˜ao Alves, Jos´e Pombal, Ricardo Rei, Jos´e de Souza, Pierre\nColombo, and Andr´e FT Martins. Steering large language models for machine translation\nwith finetuning and in-context learning. In Findings of the Association for Computational\nLinguistics: EMNLP 2023, pp. 11127–11148, 2023.\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy\nLovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V. Do, Yan Xu, and Pascale Fung. A\nmultitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination,\nand interactivity. CoRR, abs/2302.04023, 2023. doi: 10.48550/arXiv.2302.04023. URL\nhttps://doi.org/10.48550/arXiv.2302.04023.\nSamuel Cahyawijaya, Holy Lovenia, Tiezheng Yu, Willy Chung, and Pascale Fung. Instruct-\nalign: Teaching novel languages with to LLMs through alignment-based cross-lingual\ninstruction. CoRR, abs/2305.13627, 2023. doi: 10.48550/arXiv.2305.13627. URL https:\n//doi.org/10.48550/arXiv.2305.13627.\nEthan C. Chau, Lucy H. Lin, and Noah A. Smith. Parsing with multilingual bert, a small\ntreebank, and a small corpus. In Trevor Cohn, Yulan He, and Yang Liu (eds.), Findings of the\nAssociation for Computational Linguistics: EMNLP 2020, Online Event, 16-20 November 2020,\nvolume EMNLP 2020 of Findings of ACL, pp. 1324–1334. Association for Computational\nLinguistics, 2020. doi: 10.18653/v1/2020.findings-emnlp.118. URL https://doi.org/\n10.18653/v1/2020.findings-emnlp.118.\nPinzhen Chen, Shaoxiong Ji, Nikolay Bogoychev, Barry Haddow, and Kenneth Heafield.\nMonolingual or multilingual instruction tuning: Which makes a better Alpaca. In Findings\nof the Association for Computational Linguistics: EACL, 2024. URL https://doi.org/10.\n48550/arXiv.2309.08958.\nYihong Chen, Kelly Marchisio, Roberta Raileanu, David Ifeoluwa Adelani, Pontus Stenetorp,\nSebastian Riedel, and Mikel Artetxe. Improving language plasticity via pretraining with\nactive forgetting. CoRR, abs/2307.01163, 2023. doi: 10.48550/arXiv.2307.01163. URL\nhttps://doi.org/10.48550/arXiv.2307.01163.\nKevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. ELECTRA: Pre-\ntraining text encoders as discriminators rather than generators. In International Conference\non Learning Representations, 2020. URL https://openreview.net/forum?id=r1xMH1BtvB.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume\nWenzek, Francisco Guzm´an, ´Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin\nStoyanov. Unsupervised cross-lingual representation learning at scale. In Proceedings of\nthe 58th Annual Meeting of the Association for Computational Linguistics, pp. 8440–8451, 2020.\nYiming Cui, Ziqing Yang, and Xin Yao. Efficient and effective text encoding for Chinese\nLLaMA and Alpaca. CoRR, abs/2304.08177, 2023. doi: 10.48550/ARXIV.2304.08177. URL\nhttps://doi.org/10.48550/arXiv.2304.08177.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of\ndeep bidirectional transformers for language understanding. In Proceedings of the 2019\nConference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, 2019.\nC. M. Downey, Terra Blevins, Nora Goldfine, and Shane Steinert-Threlkeld.\nEmbed-\nding structure matters: Comparing methods to adapt multilingual vocabularies to\nnew languages. CoRR, abs/2309.04679, 2023. doi: 10.48550/arXiv.2309.04679. URL\nhttps://doi.org/10.48550/arXiv.2309.04679.\n11\nPreprint. Under review.\nAbteen Ebrahimi and Katharina Kann. How to adapt your pretrained multilingual model\nto 1600 languages. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.),\nProceedings of the 59th Annual Meeting of the Association for Computational Linguistics and\nthe 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021,\n(Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pp. 4555–4567. Association for\nComputational Linguistics, 2021. doi: 10.18653/v1/2021.acl-long.351. URL https://doi.\norg/10.18653/v1/2021.acl-long.351.\nFahim Faisal and Antonios Anastasopoulos. Phylogeny-inspired adaptation of multilingual\nmodels to new languages. CoRR, abs/2205.09634, 2022. doi: 10.48550/arXiv.2205.09634.\nURL https://doi.org/10.48550/arXiv.2205.09634.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,\nLu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In\nInternational Conference on Learning Representations, 2022. URL https://openreview.net/\nforum?id=nZeVKeeFYf9.\nAyyoob ImaniGooghari, Peiqin Lin, Amir Hossein Kargaran, Silvia Severini, Masoud\nJalili Sabet, Nora Kassner, Chunlan Ma, Helmut Schmid, Andr´e Martins, Franc¸ois\nYvon, and Hinrich Sch¨utze. Glot500: Scaling multilingual corpora and language mod-\nels to 500 languages.\nIn Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki\n(eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pp. 1082–1117, Toronto, Canada, July 2023. Associa-\ntion for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.61. URL https:\n//aclanthology.org/2023.acl-long.61.\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh\nChaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample,\nLucile Saulnier, L´elio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao,\nThibaut Lavril, Thomas Wang, Timoth´ee Lacroix, and William El Sayed. Mistral 7b.\nCoRR, abs/2310.06825, 2023. doi: 10.48550/ARXIV.2310.06825. URL https://doi.org/\n10.48550/arXiv.2310.06825.\nAlbert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary,\nChris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian\nBressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024.\nTaku Kudo and John Richardson. SentencePiece: A simple and language independent\nsubword tokenizer and detokenizer for neural text processing. In Eduardo Blanco and\nWei Lu (eds.), Proceedings of the 2018 Conference on Empirical Methods in Natural Language\nProcessing, EMNLP 2018: System Demonstrations, Brussels, Belgium, October 31 - November\n4, 2018, pp. 66–71. Association for Computational Linguistics, 2018. doi: 10.18653/v1/\nd18-2012. URL https://doi.org/10.18653/v1/d18-2012.\nSneha Kudugunta, Isaac Caswell, Biao Zhang, Xavier Garcia, Christopher A Choquette-\nChoo, Katherine Lee, Derrick Xin, Aditya Kusupati, Romi Stella, Ankur Bapna, et al.\nMADLAD-400: A multilingual and document-level large audited dataset. arXiv preprint\narXiv:2309.04662, 2023.\nViet Dac Lai, Nghia Trung Ngo, Amir Pouran Ben Veyseh, Hieu Man, Franck Dernoncourt,\nTrung Bui, and Thien Huu Nguyen. ChatGPT beyond English: Towards a comprehensive\nevaluation of large language models in multilingual learning. CoRR, abs/2304.05613, 2023.\ndoi: 10.48550/arXiv.2304.05613. URL https://doi.org/10.48550/arXiv.2304.05613.\nXi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig,\nMyle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer,\nPunit Singh Koura, Vishrav Chaudhary, Brian O’Horo, Jeff Wang, Luke Zettlemoyer,\nZornitsa Kozareva, Mona T. Diab, Veselin Stoyanov, and Xian Li. Few-shot learning with\nmultilingual language models. CoRR, abs/2112.10668, 2021. URL https://arxiv.org/\nabs/2112.10668.\n12\nPreprint. Under review.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu\nChen.\nWhat makes good in-context examples for gpt-3?\nIn Eneko Agirre, Mari-\nanna Apidianaki, and Ivan Vulic (eds.), Proceedings of Deep Learning Inside Out: The\n3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures,\nDeeLIO@ACL 2022, Dublin, Ireland and Online, May 27, 2022, pp. 100–114. Associa-\ntion for Computational Linguistics, 2022. doi: 10.18653/V1/2022.DEELIO-1.10. URL\nhttps://doi.org/10.18653/v1/2022.deelio-1.10.\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike\nLewis, and Luke Zettlemoyer. Multilingual denoising pre-training for neural machine\ntranslation. Transactions of the Association for Computational Linguistics, 8:726–742, 2020.\nChunlan Ma, Ayyoob ImaniGooghari, Haotian Ye, Ehsaneddin Asgari, and Hinrich Sch¨utze.\nTaxi1500: A multilingual dataset for text classification in 1500 languages, 2023.\nThomas Mayer and Michael Cysouw. Creating a massively parallel bible corpus. In Nicoletta\nCalzolari, Khalid Choukri, Thierry Declerck, Hrafn Loftsson, Bente Maegaard, Joseph\nMariani, Asunci´on Moreno, Jan Odijk, and Stelios Piperidis (eds.), Proceedings of the Ninth\nInternational Conference on Language Resources and Evaluation, LREC 2014, Reykjavik, Iceland,\nMay 26-31, 2014, pp. 3158–3163. European Language Resources Association (ELRA), 2014.\nURL http://www.lrec-conf.org/proceedings/lrec2014/summaries/220.html.\nBenjamin M¨uller, Benoˆıt Sagot, and Djam´e Seddah. Can multilingual language models\ntransfer to an unseen dialect? A case study on north african arabizi. CoRR, abs/2005.00318,\n2020. URL https://arxiv.org/abs/2005.00318.\nBenjamin M¨uller, Antonios Anastasopoulos, Benoˆıt Sagot, and Djam´e Seddah. When being\nunseen from mbert is just the beginning: Handling new languages with multilingual\nlanguage models. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek\nHakkani-T¨ur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao\nZhou (eds.), Proceedings of the 2021 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June\n6-11, 2021, pp. 448–462. Association for Computational Linguistics, 2021. doi: 10.18653/\nv1/2021.naacl-main.38. URL https://doi.org/10.18653/v1/2021.naacl-main.38.\nMinh Van Nguyen, Viet Dac Lai, Amir Pouran Ben Veyseh, and Thien Huu Nguyen. Trankit:\nA light-weight transformer-based toolkit for multilingual natural language processing.\nIn Dimitra Gkatzia and Djam´e Seddah (eds.), Proceedings of the 16th Conference of the\nEuropean Chapter of the Association for Computational Linguistics: System Demonstrations,\nEACL 2021, Online, April 19-23, 2021, pp. 80–90. Association for Computational Linguistics,\n2021. doi: 10.18653/v1/2021.eacl-demos.10. URL https://doi.org/10.18653/v1/2021.\neacl-demos.10.\nJonas Pfeiffer, Ivan Vulic, Iryna Gurevych, and Sebastian Ruder. MAD-X: an adapter-\nbased framework for multi-task cross-lingual transfer. In Bonnie Webber, Trevor Cohn,\nYulan He, and Yang Liu (eds.), Proceedings of the 2020 Conference on Empirical Methods in\nNatural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pp. 7654–7673.\nAssociation for Computational Linguistics, 2020. doi: 10.18653/v1/2020.emnlp-main.617.\nURL https://doi.org/10.18653/v1/2020.emnlp-main.617.\nJonas Pfeiffer, Ivan Vulic, Iryna Gurevych, and Sebastian Ruder. Unks everywhere: Adapting\nmultilingual language models to new scripts. In Marie-Francine Moens, Xuanjing Huang,\nLucia Specia, and Scott Wen-tau Yih (eds.), Proceedings of the 2021 Conference on Empirical\nMethods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican\nRepublic, 7-11 November, 2021, pp. 10186–10203. Association for Computational Linguistics,\n2021. doi: 10.18653/v1/2021.emnlp-main.800. URL https://doi.org/10.18653/v1/\n2021.emnlp-main.800.\nSamyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. ZeRO: Memory\noptimizations toward training trillion parameter models. In SC20: International Conference\nfor High Performance Computing, Networking, Storage and Analysis, pp. 1–16. IEEE, 2020.\n13\nPreprint. Under review.\nJeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. DeepSpeed: System\noptimizations enable training deep learning models with over 100 billion parameters. In\nProceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &\nData Mining, pp. 3505–3506, 2020.\nMasoud Jalili Sabet, Philipp Dufter, Franc¸ois Yvon, and Hinrich Sch¨utze. Simalign: High\nquality word alignments without parallel training data using static and contextualized\nembeddings. In Trevor Cohn, Yulan He, and Yang Liu (eds.), Proceedings of the 2020\nConference on Empirical Methods in Natural Language Processing: Findings, EMNLP 2020,\nOnline Event, 16-20 November 2020, volume EMNLP 2020 of Findings of ACL, pp. 1627–1643.\nAssociation for Computational Linguistics, 2020. doi: 10.18653/v1/2020.findings-emnlp.\n147. URL https://doi.org/10.18653/v1/2020.findings-emnlp.147.\nTeven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili´c, Daniel Hess-\nlow, Roman Castagn´e, Alexandra Sasha Luccioni, Franc¸ois Yvon, Matthias Gall´e, et al.\nBLOOM: A 176b-parameter open-access multilingual language model. arXiv preprint\narXiv:2211.05100, 2022.\nOleh Shliazhko, Alena Fenogenova, Maria Tikhonova, Vladislav Mikhailov, Anastasia\nKozlova, and Tatiana Shavrina.\nmGPT: Few-shot learners go multilingual.\nCoRR,\nabs/2204.07580, 2022. doi: 10.48550/arXiv.2204.07580. URL https://doi.org/10.48550/\narXiv.2204.07580.\nYi Tay, Mostafa Dehghani, Vinh Q Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won\nChung, Dara Bahri, Tal Schuster, Steven Zheng, et al. UL2: Unifying language learning\nparadigms. In The Eleventh International Conference on Learning Representations, 2022.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux,\nTimoth´ee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aur´elien\nRodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. LLaMA: Open and\nefficient foundation language models. CoRR, abs/2302.13971, 2023a. doi: 10.48550/arXiv.\n2302.13971. URL https://doi.org/10.48550/arXiv.2302.13971.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\nNikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas\nBlecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude\nFernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman\nGoyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas,\nViktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura,\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning\nMao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew\nPoulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva,\nEric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor,\nAdina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang,\nAngela Fan, Melanie Kambadur, Sharan Narang, Aur´elien Rodriguez, Robert Stojnic,\nSergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat\nmodels. CoRR, abs/2307.09288, 2023b. doi: 10.48550/arXiv.2307.09288. URL https:\n//doi.org/10.48550/arXiv.2307.09288.\nAhmet ¨Ust¨un, Arianna Bisazza, Gosse Bouma, and Gertjan van Noord. Udapter: Language\nadaptation for truly universal dependency parsing. In Bonnie Webber, Trevor Cohn,\nYulan He, and Yang Liu (eds.), Proceedings of the 2020 Conference on Empirical Methods in\nNatural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pp. 2302–2315.\nAssociation for Computational Linguistics, 2020. doi: 10.18653/v1/2020.emnlp-main.180.\nURL https://doi.org/10.18653/v1/2020.emnlp-main.180.\nXinyi Wang, Sebastian Ruder, and Graham Neubig. Expanding pretrained models to thou-\nsands more languages via lexicon-based adaptation. In Smaranda Muresan, Preslav\nNakov, and Aline Villavicencio (eds.), Proceedings of the 60th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland,\nMay 22-27, 2022, pp. 863–877. Association for Computational Linguistics, 2022. URL\nhttps://aclanthology.org/2022.acl-long.61.\n14\nPreprint. Under review.\nZihan Wang, Karthikeyan K, Stephen Mayhew, and Dan Roth. Extending multilingual BERT\nto low-resource languages. In Trevor Cohn, Yulan He, and Yang Liu (eds.), Findings of the\nAssociation for Computational Linguistics: EMNLP 2020, Online Event, 16-20 November 2020,\nvolume EMNLP 2020 of Findings of ACL, pp. 2649–2656. Association for Computational\nLinguistics, 2020. doi: 10.18653/v1/2020.findings-emnlp.240. URL https://doi.org/\n10.18653/v1/2020.findings-emnlp.240.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony\nMoi, Pierric Cistac, Tim Rault, R´emi Louf, Morgan Funtowicz, et al. Transformers: State-\nof-the-art natural language processing. In Proceedings of the 2020 conference on empirical\nmethods in natural language processing: system demonstrations, pp. 38–45, 2020.\nHaoran Xu, Young Jin Kim, Amr Sharaf, and Hany Hassan Awadalla. A paradigm shift in\nmachine translation: Boosting translation performance of large language models. CoRR,\nabs/2309.11674, 2023. doi: 10.48550/ARXIV.2309.11674. URL https://doi.org/10.\n48550/arXiv.2309.11674.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant,\nAditya Barua, and Colin Raffel. mT5: A massively multilingual pre-trained text-to-text\ntransformer. In Proceedings of the 2021 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, pp. 483–498, 2021.\nLinting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale,\nAdam Roberts, and Colin Raffel. Byt5: Towards a token-free future with pre-trained\nbyte-to-byte models. Trans. Assoc. Comput. Linguistics, 10:291–306, 2022. doi: 10.1162/\ntacl\\ a\\ 00461. URL https://doi.org/10.1162/tacl_a_00461.\nWen Yang, Chong Li, Jiajun Zhang, and Chengqing Zong. Bigtrans: Augmenting large\nlanguage models with multilingual translation capability over 100 languages. CoRR,\nabs/2305.18098, 2023. doi: 10.48550/arXiv.2305.18098. URL https://doi.org/10.48550/\narXiv.2305.18098.\nZheng Xin Yong, Hailey Schoelkopf, Niklas Muennighoff, Alham Fikri Aji, David Ife-\noluwa Adelani, Khalid Almubarak, M. Saiful Bari, Lintang Sutawika, Jungo Kasai,\nAhmed Baruwa, Genta Indra Winata, Stella Biderman, Dragomir Radev, and Vas-\nsilina Nikoulina.\nBLOOM+1: adding language support to BLOOM for zero-shot\nprompting. CoRR, abs/2212.09535, 2022. doi: 10.48550/arXiv.2212.09535. URL https:\n//doi.org/10.48550/arXiv.2212.09535.\nLili Yu, Daniel Simig, Colin Flaherty, Armen Aghajanyan, Luke Zettlemoyer, and Mike\nLewis. MEGABYTE: predicting million-byte sequences with multiscale transformers.\nCoRR, abs/2305.07185, 2023. doi: 10.48550/arXiv.2305.07185. URL https://doi.org/10.\n48550/arXiv.2305.07185.\nJun Zhao, Zhihao Zhang, Qi Zhang, Tao Gui, and Xuanjing Huang. LLaMA beyond English:\nAn empirical study on language capability transfer. arXiv preprint arXiv:2401.01055, 2024.\nWenhao Zhu, Yunzhe Lv, Qingxiu Dong, Fei Yuan, Jingjing Xu, Shujian Huang, Lingpeng\nKong, Jiajun Chen, and Lei Li. Extrapolating large language models to non-english by\naligning languages. CoRR, abs/2308.04948, 2023. doi: 10.48550/arXiv.2308.04948. URL\nhttps://doi.org/10.48550/arXiv.2308.04948.\nA\nLanguages\nThe list of languages of Glot500-c used to train MaLA-500 with the number of available\nsentences and language family information for each language is available in Tables 5, 6 and\n7.\n15\nPreprint. Under review.\nLang\n∥Sent∥\nFamily\nLang\n∥Sent∥\nFamily\nLang\n∥Sent∥\nFamily\nhbs Latn\n63411156\nindo1319\nhin Deva\n7046700\nindo1319\nton Latn\n1216118\naust1307\nmal Mlym\n48098273\ndrav1251\nkor Hang\n6468444\nkore1284\ntah Latn\n1190747\naust1307\naze Latn\n46300705\nory Orya\n6266475\nindo1319\nlat Latn\n1179913\nindo1319\nguj Gujr\n45738685\nindo1319\nurd Arab\n6009594\nindo1319\nsrn Latn\n1172349\nindo1319\nben Beng\n43514870\nindo1319\nswa Latn\n5989369\newe Latn\n1161605\natla1278\nkan Knda\n41836495\ndrav1251\nsqi Latn\n5526836\nindo1319\nbem Latn\n1111969\natla1278\ntel Telu\n41580525\ndrav1251\nbel Cyrl\n5319675\nindo1319\nefi Latn\n1082621\natla1278\nmlt Latn\n40654838\nafro1255\nafr Latn\n5157787\nindo1319\nbis Latn\n1070170\nindo1319\nfra Latn\n39197581\nindo1319\nnno Latn\n4899103\nindo1319\norm Latn\n1067699\nspa Latn\n37286756\nindo1319\ntat Cyrl\n4708088\nturk1311\nhaw Latn\n1062491\naust1307\neng Latn\n36122761\nindo1319\nast Latn\n4683554\nindo1319\nhmo Latn\n1033636\npidg1258\nfil Latn\n33493255\naust1307\nmon Cyrl\n4616960\nmong1349\nkat Geor\n1004297\nkart1248\nnob Latn\n32869205\nindo1319\nhbs Cyrl\n4598073\nindo1319\npag Latn\n983637\naust1307\nrus Cyrl\n31787973\nindo1319\nhau Latn\n4368483\nafro1255\nloz Latn\n964418\natla1278\ndeu Latn\n31015993\nindo1319\nsna Latn\n4019596\natla1278\nfry Latn\n957422\nindo1319\ntur Latn\n29184662\nturk1311\nmsa Latn\n3929084\nmya Mymr\n945180\nsino1245\npan Guru\n29052537\nindo1319\nsom Latn\n3916769\nafro1255\nnds Latn\n944715\nindo1319\nmar Deva\n28748897\nindo1319\nsrp Cyrl\n3864091\nindo1319\nrun Latn\n943828\natla1278\npor Latn\n27824391\nindo1319\nmlg Latn\n3715802\npnb Arab\n899895\nindo1319\nnld Latn\n25061426\nindo1319\nzul Latn\n3580113\natla1278\nrar Latn\n894515\naust1307\nara Arab\n24524122\narz Arab\n3488224\nafro1255\nfij Latn\n887134\naust1307\nzho Hani\n24143786\nnya Latn\n3409030\natla1278\nwls Latn\n882167\naust1307\nita Latn\n23539857\nindo1319\ntam Taml\n3388255\ndrav1251\nckb Arab\n874441\nindo1319\nind Latn\n23018106\naust1307\nhat Latn\n3226932\nindo1319\nven Latn\n860249\natla1278\nell Grek\n22033282\nindo1319\nuzb Latn\n3223485\nturk1311\nzsm Latn\n859947\naust1307\nbul Cyrl\n21823004\nindo1319\nsot Latn\n3205510\natla1278\nchv Cyrl\n859863\nturk1311\nswe Latn\n20725883\nindo1319\nuzb Cyrl\n3029947\nturk1311\nlua Latn\n854359\natla1278\nces Latn\n20376340\nindo1319\ncos Latn\n3015055\nindo1319\nque Latn\n838486\nisl Latn\n19547941\nindo1319\nals Latn\n2954874\nindo1319\nsag Latn\n771048\natla1278\npol Latn\n19339945\nindo1319\namh Ethi\n2862985\nafro1255\nguw Latn\n767918\natla1278\nron Latn\n19190217\nindo1319\nsun Latn\n2586011\naust1307\nbre Latn\n748954\nindo1319\ndan Latn\n19174573\nindo1319\nwar Latn\n2584810\naust1307\ntoi Latn\n745385\natla1278\nhun Latn\n18800025\nural1272\ndiv Thaa\n2418687\nindo1319\npus Arab\n731992\nindo1319\ntgk Cyrl\n18659517\nindo1319\nyor Latn\n2392359\natla1278\nche Cyrl\n728201\nnakh1245\nsrp Latn\n18371769\nindo1319\nfao Latn\n2365271\nindo1319\npis Latn\n714783\nindo1319\nfas Arab\n18277593\nuzn Cyrl\n2293672\nturk1311\nkon Latn\n685194\nceb Latn\n18149215\naust1307\nsmo Latn\n2290439\naust1307\noss Cyrl\n683517\nindo1319\nheb Hebr\n18128962\nafro1255\nbak Cyrl\n2264196\nturk1311\nhyw Armn\n679819\nindo1319\nhrv Latn\n17882932\nindo1319\nilo Latn\n2106531\naust1307\niso Latn\n658789\natla1278\nglg Latn\n17852274\nindo1319\ntso Latn\n2100708\natla1278\nnan Latn\n656389\nsino1245\nfin Latn\n16730388\nural1272\nmri Latn\n2046850\naust1307\nlub Latn\n654390\natla1278\nslv Latn\n15719210\nindo1319\nhmn Latn\n1903898\nlim Latn\n652078\nindo1319\nvie Latn\n15697827\naust1305\nasm Beng\n1882353\nindo1319\ntuk Latn\n649411\nturk1311\nmkd Cyrl\n14717004\nindo1319\nhil Latn\n1798875\naust1307\ntir Ethi\n649117\nafro1255\nslk Latn\n14633631\nindo1319\nnso Latn\n1619354\natla1278\ntgk Latn\n636541\nindo1319\nnor Latn\n14576191\nindo1319\nibo Latn\n1543820\natla1278\nyua Latn\n610052\nmaya1287\nest Latn\n13600579\nkin Latn\n1521612\natla1278\nmin Latn\n609065\naust1307\nltz Latn\n12997242\nindo1319\nhye Armn\n1463123\nindo1319\nlue Latn\n599429\natla1278\neus Latn\n12775959\noci Latn\n1449128\nindo1319\nkhm Khmr\n590429\naust1305\nlit Latn\n12479626\nindo1319\nlin Latn\n1408460\natla1278\ntum Latn\n589857\natla1278\nkaz Cyrl\n12378727\nturk1311\ntpi Latn\n1401844\nindo1319\ntll Latn\n586530\natla1278\nlav Latn\n12143980\nindo1319\ntwi Latn\n1400979\natla1278\nekk Latn\n582595\nural1272\nbos Latn\n11014744\nindo1319\nkir Cyrl\n1397566\nturk1311\nlug Latn\n566948\natla1278\nepo Latn\n8737198\narti1236\npap Latn\n1360138\nindo1319\nniu Latn\n566715\naust1307\ncat Latn\n8648271\nindo1319\nnep Deva\n1317291\nindo1319\ntzo Latn\n540262\nmaya1287\ntha Thai\n7735209\ntaik1256\nazj Latn\n1315834\nturk1311\nmah Latn\n534614\naust1307\nukr Cyrl\n7462046\nindo1319\nbcl Latn\n1284493\naust1307\ntvl Latn\n521556\naust1307\ntgl Latn\n7411064\naust1307\nxho Latn\n1262364\natla1278\njav Latn\n516833\naust1307\nsin Sinh\n7293178\nindo1319\ncym Latn\n1244783\nindo1319\nvec Latn\n514240\nindo1319\ngle Latn\n7225513\nindo1319\ngaa Latn\n1222307\natla1278\njpn Jpan\n510722\njapo1237\nTable 5: List of languages of Glot500-c (Part I).\n16\nPreprint. Under review.\nLang\n∥Sent∥\nFamily\nLang\n∥Sent∥\nFamily\nLang\n∥Sent∥\nFamily\nlus Latn\n509250\nsino1245\nkmb Latn\n296269\natla1278\nncx Latn\n162558\nutoa1244\ncrs Latn\n508755\nindo1319\nzai Latn\n277632\notom1299\nqug Latn\n162500\nquec1387\nkqn Latn\n507913\natla1278\ngym Latn\n274512\nchib1249\nrmn Latn\n162069\nindo1319\nndo Latn\n496613\natla1278\nbod Tibt\n273489\nsino1245\ncjk Latn\n160645\natla1278\nsnd Arab\n488730\nindo1319\nnde Latn\n269931\natla1278\narb Arab\n159884\nafro1255\nyue Hani\n484700\nsino1245\nfon Latn\n268566\natla1278\nkea Latn\n158047\nindo1319\ntiv Latn\n483064\natla1278\nber Latn\n264426\nmck Latn\n157521\natla1278\nkua Latn\n473535\natla1278\nnbl Latn\n259158\natla1278\narn Latn\n155882\narau1255\nkwy Latn\n473274\natla1278\nkmr Latn\n256677\nindo1319\npdt Latn\n155485\nindo1319\nhin Latn\n466175\nindo1319\nguc Latn\n249044\naraw1281\nher Latn\n154827\natla1278\niku Cans\n465011\nmam Latn\n248348\nmaya1287\ngla Latn\n152563\nindo1319\nkal Latn\n462430\neski1264\nnia Latn\n247406\naust1307\nkmr Cyrl\n151728\nindo1319\ntdt Latn\n459818\naust1307\nnyn Latn\n241992\natla1278\nmwl Latn\n150054\nindo1319\ngsw Latn\n449240\nindo1319\ncab Latn\n240101\naraw1281\nnav Latn\n147702\natha1245\nmfe Latn\n447435\nindo1319\ntop Latn\n239232\ntoto1251\nksw Mymr\n147674\nsino1245\nswc Latn\n446378\natla1278\ntog Latn\n231969\natla1278\nmxv Latn\n147591\notom1299\nmon Latn\n437950\nmong1349\nmco Latn\n231209\nmixe1284\nhif Latn\n147261\nindo1319\nmos Latn\n437666\natla1278\ntzh Latn\n230706\nmaya1287\nwol Latn\n146992\natla1278\nkik Latn\n437228\natla1278\npms Latn\n227748\nindo1319\nsme Latn\n146803\nural1272\ncnh Latn\n436667\nsino1245\nwuu Hani\n224088\nsino1245\ngom Latn\n143937\nindo1319\ngil Latn\n434529\naust1307\nplt Latn\n220413\naust1307\nbum Latn\n141673\natla1278\npon Latn\n434522\naust1307\nyid Hebr\n220214\nindo1319\nmgr Latn\n138953\natla1278\numb Latn\n431589\natla1278\nada Latn\n219427\natla1278\nahk Latn\n135068\nsino1245\nlvs Latn\n422952\nindo1319\niba Latn\n213615\naust1307\nkur Arab\n134160\nindo1319\nsco Latn\n411591\nindo1319\nkek Latn\n209932\nmaya1287\nbas Latn\n133436\natla1278\nori Orya\n410827\nkoo Latn\n209375\natla1278\nbin Latn\n133256\natla1278\narg Latn\n410683\nindo1319\nsop Latn\n206501\natla1278\ntsz Latn\n133251\ntara1323\nkur Latn\n407169\nindo1319\nkac Latn\n205542\nsino1245\nsid Latn\n130406\nafro1255\ndhv Latn\n405711\naust1307\nqvi Latn\n205447\nquec1387\ndiq Latn\n128908\nindo1319\nluo Latn\n398974\nnilo1247\ncak Latn\n204472\nmaya1287\nsrd Latn\n127064\nlun Latn\n395764\natla1278\nkbp Latn\n202877\natla1278\ntcf Latn\n126050\notom1299\nnzi Latn\n394247\natla1278\nctu Latn\n201662\nmaya1287\nbzj Latn\n124958\nindo1319\ngug Latn\n392227\ntupi1275\nkri Latn\n201087\nindo1319\nudm Cyrl\n121705\nural1272\nbar Latn\n387070\nindo1319\nmau Latn\n199134\notom1299\ncce Latn\n120636\natla1278\nbci Latn\n384059\natla1278\nscn Latn\n199068\nindo1319\nmeu Latn\n120273\naust1307\nchk Latn\n380596\naust1307\ntyv Cyrl\n198649\nturk1311\nchw Latn\n119751\natla1278\nroh Latn\n377067\nindo1319\nina Latn\n197315\narti1236\ncbk Latn\n118789\nindo1319\naym Latn\n373329\nayma1253\nbtx Latn\n193701\naust1307\nibg Latn\n118733\naust1307\nyap Latn\n358929\naust1307\nnch Latn\n193129\nutoa1244\nbhw Latn\n117381\naust1307\nssw Latn\n356561\natla1278\nncj Latn\n192962\nutoa1244\nngu Latn\n116851\nutoa1244\nquz Latn\n354781\nquec1387\npau Latn\n190529\naust1307\nnyy Latn\n115914\natla1278\nsah Cyrl\n352697\nturk1311\ntoj Latn\n189651\nmaya1287\nszl Latn\n112496\nindo1319\ntsn Latn\n350954\natla1278\npcm Latn\n187594\nindo1319\nish Latn\n111814\natla1278\nlmo Latn\n348135\nindo1319\ndyu Latn\n186367\nmand1469\nnaq Latn\n109747\nkhoe1240\nido Latn\n331239\narti1236\nkss Latn\n185868\natla1278\ntoh Latn\n107583\natla1278\nabk Cyrl\n321578\nabkh1242\nafb Arab\n183694\nafro1255\nttj Latn\n106925\natla1278\nzne Latn\n318871\natla1278\nurh Latn\n182214\natla1278\nnse Latn\n105189\natla1278\nquy Latn\n311040\nquec1387\nquc Latn\n181559\nmaya1287\nhsb Latn\n104802\nindo1319\nkam Latn\n310659\natla1278\nnew Deva\n181427\nsino1245\nami Latn\n104559\naust1307\nbbc Latn\n310420\naust1307\nyao Latn\n179965\natla1278\nalz Latn\n104392\nnilo1247\nvol Latn\n310399\narti1236\nngl Latn\n178498\natla1278\napc Arab\n102392\nafro1255\nwal Latn\n309873\ngong1255\nnyu Latn\n177483\natla1278\nvls Latn\n101900\nindo1319\nuig Arab\n307302\nturk1311\nkab Latn\n176015\nafro1255\nmhr Cyrl\n100474\nural1272\nvmw Latn\n306899\natla1278\ntuk Cyrl\n175769\nturk1311\ndjk Latn\n99234\nindo1319\nkwn Latn\n305362\natla1278\nxmf Geor\n174994\nkart1248\nwes Latn\n98492\nindo1319\npam Latn\n303737\naust1307\nndc Latn\n174305\natla1278\ngkn Latn\n97041\natla1278\nseh Latn\n300243\natla1278\nsan Deva\n165616\nindo1319\ngrc Grek\n96986\nindo1319\ntsc Latn\n298442\natla1278\nnba Latn\n163485\natla1278\nhbo Hebr\n96484\nafro1255\nnyk Latn\n297976\natla1278\nbpy Beng\n162838\nindo1319\nswh Latn\n95776\natla1278\nTable 6: List of languages of Glot500-c (Part II).\n17\nPreprint. Under review.\nLang\n∥Sent∥\nFamily\nLang\n∥Sent∥\nFamily\nLang\n∥Sent∥\nFamily\nalt Cyrl\n95148\nturk1311\nmny Latn\n50581\natla1278\ncsy Latn\n34126\nsino1245\nrmn Grek\n94533\nindo1319\ngkp Latn\n50549\nmand1469\nazb Arab\n33758\nturk1311\nmiq Latn\n94343\nmisu1242\nkat Latn\n50424\nkart1248\ncsb Latn\n33743\nindo1319\nkaa Cyrl\n88815\nturk1311\nbjn Latn\n49068\naust1307\ntpm Latn\n33517\natla1278\nkos Latn\n88603\naust1307\nacr Latn\n48886\nmaya1287\nquw Latn\n33449\nquec1387\ngrn Latn\n87568\ndtp Latn\n48468\naust1307\nrmy Cyrl\n33351\nindo1319\nlhu Latn\n87255\nsino1245\nlam Latn\n46853\natla1278\nixl Latn\n33289\nmaya1287\nlzh Hani\n86035\nsino1245\nbik Latn\n46561\nmbb Latn\n33240\naust1307\najp Arab\n83297\nafro1255\npoh Latn\n46454\nmaya1287\npfl Latn\n33148\nindo1319\ncmn Hani\n80745\nsino1245\nphm Latn\n45862\natla1278\npcd Latn\n32867\nindo1319\ngcf Latn\n80737\nindo1319\nhrx Latn\n45716\nindo1319\ntlh Latn\n32863\narti1236\nrmn Cyrl\n79925\nindo1319\nquh Latn\n45566\nquec1387\nsuz Deva\n32811\nsino1245\nkjh Cyrl\n79262\nturk1311\nhyw Cyrl\n45379\nindo1319\ngcr Latn\n32676\nindo1319\nrng Latn\n78177\natla1278\nrue Cyrl\n45369\nindo1319\njbo Latn\n32619\narti1236\nmgh Latn\n78117\natla1278\neml Latn\n44630\nindo1319\ntbz Latn\n32264\natla1278\nxmv Latn\n77896\naust1307\nacm Arab\n44505\nafro1255\nbam Latn\n32150\nmand1469\nige Latn\n77114\natla1278\ntob Latn\n44473\nguai1249\nprk Latn\n32085\naust1305\nrmy Latn\n76991\nindo1319\nach Latn\n43974\nnilo1247\njam Latn\n32048\nindo1319\nsrm Latn\n76884\nindo1319\nvep Latn\n43076\nural1272\ntwx Latn\n32028\natla1278\nbak Latn\n76809\nturk1311\nnpi Deva\n43072\nindo1319\nnmf Latn\n31997\nsino1245\ngur Latn\n76151\natla1278\ntok Latn\n42820\narti1236\ncaq Latn\n31903\naust1305\nidu Latn\n75106\natla1278\nsgs Latn\n42467\nindo1319\nrop Latn\n31889\nindo1319\nyom Latn\n74818\natla1278\nlij Latn\n42447\nindo1319\ntca Latn\n31852\nticu1244\ntdx Latn\n74430\naust1307\nmyv Cyrl\n42147\nural1272\nyan Latn\n31775\nmisu1242\nmzn Arab\n73719\nindo1319\ntih Latn\n41873\naust1307\nxav Latn\n31765\nnucl1710\ncfm Latn\n70227\nsino1245\ntat Latn\n41640\nturk1311\nbih Deva\n31658\nzpa Latn\n69237\notom1299\nlfn Latn\n41632\narti1236\ncuk Latn\n31612\nchib1249\nkbd Cyrl\n67914\nabkh1242\ncgg Latn\n41196\natla1278\nkjb Latn\n31471\nmaya1287\nlao Laoo\n66966\ntaik1256\nful Latn\n41188\natla1278\nhne Deva\n31465\nindo1319\nnap Latn\n65826\nindo1319\ngor Latn\n41174\naust1307\nwbm Latn\n31394\naust1305\nqub Latn\n64973\nquec1387\nile Latn\n40984\narti1236\nzlm Latn\n31345\naust1307\noke Latn\n64508\natla1278\nium Latn\n40683\nhmon1336\ntui Latn\n31161\natla1278\note Latn\n64224\notom1299\nteo Latn\n40203\nnilo1247\nifb Latn\n30980\naust1307\nbsb Latn\n63634\naust1307\nkia Latn\n40035\natla1278\nizz Latn\n30894\natla1278\nogo Latn\n61901\natla1278\ncrh Cyrl\n39985\nturk1311\nrug Latn\n30857\naust1307\nabn Latn\n61830\natla1278\ncrh Latn\n39896\nturk1311\naka Latn\n30704\natla1278\nldi Latn\n61827\natla1278\nenm Latn\n39809\nindo1319\npxm Latn\n30698\nbook1242\nayr Latn\n61570\nayma1253\nsat Olck\n39614\naust1305\nkmm Latn\n30671\nsino1245\ngom Deva\n61140\nindo1319\nmad Latn\n38993\naust1307\nmcn Latn\n30666\nafro1255\nbba Latn\n61123\natla1278\ncac Latn\n38812\nmaya1287\nifa Latn\n30621\naust1307\naln Latn\n60989\nindo1319\nhnj Latn\n38611\nhmon1336\ndln Latn\n30620\nsino1245\nleh Latn\n59944\natla1278\nksh Latn\n38130\nindo1319\next Latn\n30605\nindo1319\nban Latn\n59805\naust1307\nikk Latn\n38071\natla1278\nksd Latn\n30550\naust1307\nace Latn\n59333\naust1307\nsba Latn\n38040\ncent2225\nmzh Latn\n30517\nmata1289\npes Arab\n57511\nindo1319\nzom Latn\n37013\nsino1245\nllb Latn\n30480\natla1278\nskg Latn\n57228\naust1307\nbqc Latn\n36881\nmand1469\nhra Latn\n30472\nsino1245\nary Arab\n56933\nafro1255\nbim Latn\n36835\natla1278\nmwm Latn\n30432\ncent2225\nhus Latn\n56176\nmaya1287\nmdy Ethi\n36370\ngong1255\nkrc Cyrl\n30353\nturk1311\nglv Latn\n55641\nindo1319\nbts Latn\n36216\naust1307\ntuc Latn\n30349\naust1307\nfat Latn\n55609\natla1278\ngya Latn\n35902\natla1278\nmrw Latn\n30304\naust1307\nfrr Latn\n55254\nindo1319\najg Latn\n35631\natla1278\npls Latn\n30136\notom1299\nmwn Latn\n54805\natla1278\nagw Latn\n35585\naust1307\nrap Latn\n30102\naust1307\nmai Deva\n54687\nindo1319\nkom Cyrl\n35249\nural1272\nfur Latn\n30052\nindo1319\ndua Latn\n53392\natla1278\nknv Latn\n35196\nkaa Latn\n30031\nturk1311\ndzo Tibt\n52732\nsino1245\ngiz Latn\n35040\nafro1255\nprs Arab\n26823\nindo1319\nctd Latn\n52135\nsino1245\nhui Latn\n34926\nnucl1709\nsan Latn\n25742\nindo1319\nnnb Latn\n52041\natla1278\nkpg Latn\n34900\naust1307\nsom Arab\n14199\nafro1255\nsxn Latn\n51749\naust1307\nzea Latn\n34426\nindo1319\nuig Latn\n9637\nturk1311\nmps Latn\n50645\ntebe1251\naoj Latn\n34349\nnucl1708\nhau Arab\n9593\nafro1255\nTable 7: List of languages of Glot500-c (Part III).\n18\nPreprint. Under review.\nB\nDetailed Results\nDetailed results of evaluation are shown in Tables 8-15 (NLL on Glot500-c), Tables 16-21\n(NLL on PBC), Tables 22-23 (ACC on SIB200), and Tables 24-29 (ACC on Taxi1500).\n19\nPreprint. Under review.\nLang\nLLaMA 2-7B\nmGPT-13B\nBLOOM-7B1\nXGLM-7.5B\nMALA-500\nabk Cyrl\n234.09\n249.16\n258.26\n231.44\n164.61\nabn Latn\n140.01\n197.81\n153.58\n152.90\n111.86\nace Latn\n235.15\n332.18\n244.00\n259.64\n168.79\nach Latn\n179.03\n227.84\n194.55\n197.05\n161.01\nacm Arab\n119.15\n153.09\n106.29\n101.35\n135.82\nacr Latn\n301.73\n399.80\n321.79\n316.49\n194.71\nada Latn\n132.76\n168.56\n150.19\n137.99\n103.17\nafb Arab\n134.03\n169.73\n112.55\n110.59\n152.58\nafr Latn\n52.43\n84.47\n73.24\n75.60\n64.25\nagw Latn\n228.22\n318.95\n246.48\n242.04\n152.59\nahk Latn\n229.45\n377.60\n245.81\n241.21\n163.96\najg Latn\n146.48\n185.41\n170.89\n155.21\n113.83\najp Arab\n153.34\n199.79\n129.62\n124.24\n164.80\naka Latn\n163.59\n223.13\n166.49\n185.41\n131.50\naln Latn\n191.62\n259.76\n218.75\n267.34\n143.64\nals Latn\n191.60\n271.51\n219.17\n260.14\n155.23\nalt Cyrl\n199.25\n220.77\n200.70\n215.71\n139.18\nalz Latn\n167.89\n214.64\n185.35\n171.34\n155.03\namh Ethi\n328.25\n834.56\n407.68\n550.50\n268.11\nami Latn\n122.67\n168.42\n131.77\n132.36\n109.13\naoj Latn\n318.62\n495.44\n340.07\n316.36\n196.64\napc Arab\n131.19\n153.97\n106.78\n109.24\n145.81\nara Arab\n111.05\n155.64\n80.72\n84.86\n140.73\narb Arab\n166.93\n318.76\n135.76\n137.80\n173.03\narg Latn\n173.62\n306.23\n171.32\n178.40\n160.08\narn Latn\n202.09\n292.40\n204.32\n216.04\n163.87\nary Arab\n198.80\n309.90\n184.82\n176.58\n173.37\narz Arab\n122.74\n248.72\n95.61\n100.43\n131.75\nasm Beng\n264.49\n409.59\n172.35\n311.81\n184.77\nast Latn\n208.41\n325.35\n184.93\n192.86\n178.77\naym Latn\n143.36\n183.42\n149.06\n154.45\n117.28\nayr Latn\n274.31\n342.40\n288.57\n293.48\n185.87\nazb Arab\n254.60\n293.24\n273.20\n285.61\n162.94\naze Latn\n156.58\n230.45\n195.32\n189.59\n110.56\nazj Latn\n168.12\n228.08\n212.31\n199.86\n126.98\nbak Cyrl\n274.50\n348.47\n288.93\n307.95\n169.00\nbak Latn\n191.06\n259.97\n196.98\n213.41\n152.50\nbam Latn\n195.29\n251.28\n203.50\n215.62\n171.51\nban Latn\n205.77\n297.97\n213.20\n213.89\n186.89\nbar Latn\n210.97\n287.33\n234.73\n208.66\n188.90\nbas Latn\n137.53\n172.78\n143.37\n147.13\n110.71\nbba Latn\n233.68\n286.30\n258.58\n238.94\n164.18\nbbc Latn\n172.78\n216.78\n181.59\n170.06\n148.89\nbci Latn\n176.81\n223.93\n190.52\n189.46\n171.00\nbcl Latn\n149.22\n209.44\n162.25\n174.40\n132.55\nbel Cyrl\n110.77\n174.19\n142.62\n147.27\n85.11\nbem Latn\n182.62\n222.50\n198.45\n150.51\n158.31\nben Beng\n92.79\n162.83\n50.33\n55.42\n73.86\nber Latn\n88.37\n120.03\n87.79\n101.52\n71.90\nbhw Latn\n186.42\n245.14\n194.41\n188.81\n155.12\nbih Deva\n248.12\n422.46\n176.37\n204.17\n180.31\nbik Latn\n151.63\n218.03\n173.42\n187.11\n137.28\nbim Latn\n229.29\n284.29\n244.21\n245.34\n166.16\nbin Latn\n137.28\n175.41\n152.32\n152.02\n109.51\nbis Latn\n165.83\n250.17\n179.61\n190.13\n130.32\nbjn Latn\n200.57\n302.58\n202.67\n199.15\n182.65\nbod Tibt\n437.54\n1690.09\n461.35\n80.21\n286.05\nbos Latn\n87.13\n175.82\n131.95\n149.85\n110.92\nbpy Beng\n251.20\n471.67\n154.31\n172.17\n155.64\nbqc Latn\n208.00\n266.53\n226.49\n205.65\n153.58\nbre Latn\n222.93\n276.71\n208.07\n260.44\n184.35\nbsb Latn\n236.62\n358.90\n275.10\n306.64\n204.50\nbts Latn\n214.80\n292.93\n232.31\n217.74\n156.31\nbtx Latn\n169.13\n227.44\n181.86\n174.25\n148.25\nbul Cyrl\n47.01\n90.81\n77.70\n42.90\n57.12\nbum Latn\n183.88\n237.35\n194.64\n195.91\n156.33\nbzj Latn\n167.62\n244.15\n188.25\n194.46\n137.81\nTable 8: Detailed results of NLL on Glot500-c (Part I).\n20\nPreprint. Under review.\nLang\nLLaMA 2-7B\nmGPT-13B\nBLOOM-7B1\nXGLM-7.5B\nMALA-500\ncab Latn\n222.05\n292.04\n234.53\n237.57\n168.63\ncac Latn\n293.47\n395.52\n310.33\n301.30\n192.22\ncak Latn\n295.24\n394.87\n317.52\n309.03\n200.69\ncaq Latn\n240.00\n323.71\n264.17\n257.49\n164.95\ncat Latn\n94.68\n212.17\n83.26\n86.26\n130.00\ncbk Latn\n143.05\n221.60\n145.69\n159.41\n137.96\ncce Latn\n178.45\n226.07\n190.01\n192.54\n152.70\nceb Latn\n136.44\n278.02\n164.94\n183.55\n123.31\nces Latn\n44.83\n98.77\n68.48\n76.15\n58.42\ncfm Latn\n240.20\n305.25\n252.92\n256.79\n185.94\ncgg Latn\n121.16\n160.92\n127.35\n129.19\n107.91\nche Cyrl\n199.15\n272.63\n203.57\n197.17\n158.57\nchk Latn\n189.52\n258.69\n201.19\n200.61\n145.98\nchv Cyrl\n246.19\n292.36\n252.81\n229.56\n157.91\nchw Latn\n139.07\n174.73\n142.88\n121.98\n121.16\ncjk Latn\n125.30\n158.06\n134.03\n128.75\n106.21\nckb Arab\n372.24\n437.95\n370.20\n521.30\n243.30\ncmn Hani\n52.17\n92.04\n40.75\n49.81\n62.30\ncnh Latn\n185.01\n242.39\n198.20\n198.57\n147.90\ncos Latn\n192.02\n323.30\n210.38\n211.96\n185.03\ncrh Cyrl\n236.43\n282.79\n239.67\n260.03\n141.08\ncrh Latn\n149.67\n240.28\n168.79\n157.01\n131.91\ncrs Latn\n153.11\n202.53\n153.34\n87.81\n129.39\ncsb Latn\n238.86\n336.99\n261.46\n294.41\n166.29\ncsy Latn\n226.53\n299.52\n249.53\n245.14\n172.03\nctd Latn\n210.45\n276.87\n227.39\n224.34\n158.35\nctu Latn\n216.90\n310.89\n226.68\n220.32\n157.27\ncuk Latn\n233.42\n325.97\n252.00\n247.83\n190.81\ncym Latn\n233.91\n369.64\n306.05\n332.89\n217.29\ndan Latn\n43.75\n84.32\n69.51\n66.96\n54.56\ndeu Latn\n37.46\n68.68\n49.65\n33.88\n53.45\ndhv Latn\n121.21\n170.85\n126.68\n128.57\n95.81\ndiq Latn\n174.75\n265.78\n180.00\n190.78\n147.56\ndiv Thaa\n314.55\n565.83\n314.34\n17.32\n153.76\ndjk Latn\n188.44\n249.39\n201.50\n207.16\n163.39\ndln Latn\n217.51\n288.73\n231.93\n238.10\n165.40\ndtp Latn\n267.22\n373.92\n279.80\n287.18\n184.75\ndua Latn\n131.20\n169.64\n136.03\n129.20\n109.86\ndyu Latn\n186.37\n237.65\n193.19\n205.47\n157.89\ndzo Tibt\n238.61\n842.40\n244.70\n47.40\n154.48\nefi Latn\n178.91\n251.07\n205.96\n203.93\n134.40\nekk Latn\n155.86\n223.64\n194.37\n89.18\n141.19\nell Grek\n52.85\n86.68\n67.98\n36.04\n54.45\neml Latn\n213.57\n278.33\n224.10\n225.17\n163.91\neng Latn\n30.45\n62.73\n31.32\n34.36\n48.60\nenm Latn\n79.08\n193.74\n108.20\n119.78\n87.78\nepo Latn\n68.89\n99.75\n79.80\n87.72\n70.22\nest Latn\n70.18\n100.28\n88.33\n40.53\n67.38\neus Latn\n79.07\n87.15\n48.33\n45.59\n70.49\newe Latn\n208.53\n269.62\n218.53\n195.99\n148.78\next Latn\n216.92\n338.22\n211.26\n231.30\n177.17\nfao Latn\n202.04\n284.61\n227.56\n263.89\n165.45\nfas Arab\n138.13\n193.21\n163.46\n166.76\n133.69\nfat Latn\n134.67\n180.66\n144.54\n144.50\n106.86\nfij Latn\n159.86\n219.85\n191.04\n137.83\n147.71\nfil Latn\n120.89\n206.21\n162.04\n161.84\n120.27\nfin Latn\n46.88\n86.18\n79.58\n35.79\n58.35\nfon Latn\n237.19\n295.74\n256.54\n262.29\n160.24\nfra Latn\n32.26\n63.71\n31.08\n32.74\n49.22\nfrr Latn\n192.91\n299.41\n206.26\n211.00\n144.13\nfry Latn\n191.87\n247.81\n205.02\n221.64\n168.86\nful Latn\n447.47\n550.03\n457.25\n511.87\n339.38\nfur Latn\n231.23\n313.99\n234.38\n250.02\n183.57\ngaa Latn\n188.66\n232.67\n222.71\n158.83\n146.37\ngcf Latn\n132.36\n173.10\n130.03\n91.07\n103.54\ngcr Latn\n113.22\n157.83\n115.02\n79.46\n94.40\ngil Latn\n175.92\n237.54\n187.79\n181.71\n154.60\ngiz Latn\n244.47\n332.32\n268.61\n266.29\n168.09\nTable 9: Detailed results of NLL on Glot500-c (Part II).\n21\nPreprint. Under review.\nLang\nLLaMA 2-7B\nmGPT-13B\nBLOOM-7B1\nXGLM-7.5B\nMALA-500\ngkn Latn\n223.58\n304.46\n253.54\n245.24\n167.81\ngkp Latn\n261.56\n358.97\n280.80\n270.48\n186.41\ngla Latn\n220.92\n382.20\n293.89\n315.23\n210.51\ngle Latn\n203.10\n345.45\n276.11\n299.80\n206.52\nglg Latn\n120.88\n204.76\n108.43\n122.45\n132.58\nglv Latn\n232.86\n326.69\n247.79\n265.04\n182.93\ngom Deva\n328.82\n462.17\n324.77\n358.50\n233.15\ngom Latn\n244.57\n318.36\n259.71\n257.90\n209.13\ngor Latn\n217.70\n326.26\n232.98\n239.37\n168.23\ngrc Grek\n126.86\n277.73\n181.00\n127.62\n141.80\ngrn Latn\n293.70\n382.11\n298.10\n316.62\n204.94\ngsw Latn\n180.67\n226.37\n199.03\n171.72\n157.34\nguc Latn\n241.99\n340.92\n257.19\n234.87\n183.29\ngug Latn\n197.04\n258.55\n201.92\n214.05\n158.39\nguj Gujr\n118.82\n291.38\n74.12\n194.71\n90.02\ngur Latn\n222.48\n311.22\n243.52\n233.99\n173.11\nguw Latn\n210.29\n215.37\n235.91\n246.28\n146.55\ngya Latn\n242.48\n350.56\n274.82\n258.26\n170.00\ngym Latn\n231.32\n324.92\n249.32\n191.13\n178.06\nhat Latn\n237.00\n341.48\n251.07\n150.39\n201.88\nhau Arab\n173.08\n330.75\n130.96\n129.69\n230.02\nhau Latn\n228.21\n300.72\n257.22\n265.65\n191.68\nhaw Latn\n190.18\n300.25\n217.54\n213.20\n174.30\nhbo Hebr\n140.73\n315.06\n194.19\n200.98\n155.08\nhbs Cyrl\n206.87\n503.80\n370.83\n417.41\n225.22\nhbs Latn\n209.02\n451.95\n333.95\n375.92\n223.11\nheb Hebr\n48.34\n63.09\n58.19\n63.73\n56.05\nher Latn\n140.31\n172.32\n146.72\n136.86\n109.29\nhif Latn\n396.80\n613.23\n471.65\n465.81\n371.92\nhil Latn\n145.89\n207.79\n161.39\n182.01\n126.09\nhin Deva\n142.07\n289.53\n105.86\n106.38\n166.12\nhin Latn\n150.11\n247.31\n166.34\n164.94\n176.00\nhmn Latn\n241.00\n375.11\n282.60\n284.95\n182.91\nhmo Latn\n165.38\n236.46\n178.12\n142.53\n133.19\nhne Deva\n201.66\n298.38\n171.37\n184.06\n161.30\nhnj Latn\n231.56\n324.18\n263.80\n278.94\n141.56\nhra Latn\n215.87\n271.46\n228.66\n229.46\n169.57\nhrv Latn\n43.03\n82.09\n63.02\n69.82\n54.08\nhrx Latn\n131.34\n182.33\n140.90\n135.17\n105.20\nhsb Latn\n182.90\n293.15\n211.79\n235.34\n127.71\nhui Latn\n297.34\n388.25\n319.23\n318.32\n197.57\nhun Latn\n45.03\n79.27\n75.21\n79.06\n59.30\nhus Latn\n247.96\n352.19\n260.90\n258.32\n180.85\nhye Armn\n286.18\n602.02\n372.75\n454.38\n202.92\nhyw Armn\n145.46\n263.04\n186.63\n213.52\n110.19\nhyw Cyrl\n162.17\n231.84\n171.73\n165.61\n117.61\niba Latn\n150.03\n192.62\n157.75\n151.54\n133.67\nibg Latn\n115.37\n152.94\n119.10\n122.19\n106.08\nibo Latn\n232.57\n333.59\n223.37\n296.17\n184.97\nido Latn\n140.94\n273.88\n153.94\n164.61\n121.37\nidu Latn\n153.00\n209.20\n162.53\n157.46\n106.21\nifa Latn\n252.33\n328.31\n270.66\n266.03\n172.20\nifb Latn\n257.92\n340.56\n278.23\n272.79\n183.83\nige Latn\n148.85\n199.02\n173.80\n176.02\n111.50\nikk Latn\n249.37\n330.44\n284.76\n310.26\n166.74\niku Cans\n261.21\n877.71\n343.18\n496.50\n174.80\nile Latn\n100.28\n199.76\n105.32\n115.20\n100.35\nilo Latn\n172.24\n227.41\n186.11\n208.36\n146.96\nina Latn\n209.38\n408.99\n230.14\n236.01\n201.92\nind Latn\n42.59\n69.80\n35.50\n36.82\n56.03\nish Latn\n126.54\n178.71\n144.92\n146.15\n101.29\nisl Latn\n103.40\n156.83\n127.49\n139.76\n83.51\niso Latn\n148.38\n175.75\n168.85\n167.42\n104.67\nita Latn\n39.35\n79.02\n49.94\n40.47\n53.36\nium Latn\n247.28\n361.48\n264.84\n266.46\n167.10\nixl Latn\n327.09\n506.74\n353.08\n348.23\n222.05\nizz Latn\n301.73\n400.14\n346.61\n361.39\n193.24\njam Latn\n204.69\n291.31\n223.99\n231.17\n157.87\nTable 10: Detailed results of NLL on Glot500-c (Part III).\n22\nPreprint. Under review.\nLang\nLLaMA 2-7B\nmGPT-13B\nBLOOM-7B1\nXGLM-7.5B\nMALA-500\njav Latn\n208.92\n275.29\n212.60\n220.31\n180.00\njbo Latn\n103.91\n200.82\n109.86\n112.61\n117.25\njpn Jpan\n136.26\n301.32\n197.23\n149.70\n150.43\nkaa Cyrl\n281.21\n363.07\n300.20\n317.13\n146.98\nkaa Latn\n284.60\n354.51\n292.04\n309.67\n192.43\nkab Latn\n192.58\n264.51\n185.56\n216.46\n161.31\nkac Latn\n210.47\n267.38\n223.77\n249.95\n166.44\nkal Latn\n240.15\n262.90\n259.85\n155.45\n182.71\nkam Latn\n153.84\n194.60\n156.10\n186.00\n115.78\nkan Knda\n216.22\n556.40\n146.43\n355.75\n175.17\nkat Geor\n302.53\n413.90\n435.47\n483.85\n239.51\nkat Latn\n184.94\n308.06\n217.07\n208.25\n184.20\nkaz Cyrl\n257.67\n341.78\n280.01\n297.13\n187.85\nkbd Cyrl\n212.12\n229.63\n198.20\n202.85\n146.86\nkbp Latn\n232.17\n306.53\n257.45\n246.16\n161.08\nkea Latn\n118.17\n159.93\n121.92\n122.29\n105.69\nkek Latn\n234.79\n332.19\n244.69\n228.87\n164.18\nkhm Khmr\n257.14\n815.56\n317.46\n437.56\n167.88\nkia Latn\n222.01\n298.21\n245.77\n236.71\n164.39\nkik Latn\n208.26\n277.92\n213.92\n237.26\n159.49\nkin Latn\n206.40\n237.66\n174.18\n234.91\n168.37\nkir Cyrl\n265.65\n308.15\n277.34\n313.50\n175.71\nkjb Latn\n263.79\n353.35\n280.16\n278.13\n179.76\nkjh Cyrl\n200.11\n251.59\n211.84\n217.34\n147.81\nkmb Latn\n132.84\n166.09\n137.48\n118.00\n112.99\nkmm Latn\n246.57\n330.77\n263.79\n266.44\n180.90\nkmr Cyrl\n224.23\n284.40\n226.51\n221.22\n154.70\nkmr Latn\n183.95\n220.51\n194.67\n215.02\n142.36\nknv Latn\n430.56\n581.45\n456.13\n427.27\n232.18\nkom Cyrl\n224.18\n302.71\n249.08\n213.41\n134.88\nkon Latn\n112.77\n131.61\n116.89\n119.41\n96.00\nkoo Latn\n132.73\n167.13\n144.33\n134.74\n111.26\nkor Hang\n129.20\n224.06\n180.21\n95.71\n151.37\nkos Latn\n146.15\n191.23\n153.05\n154.26\n123.85\nkpg Latn\n221.52\n321.94\n246.33\n245.73\n148.93\nkqn Latn\n125.33\n149.57\n128.12\n109.60\n106.08\nkrc Cyrl\n247.13\n292.86\n248.83\n267.39\n167.05\nkri Latn\n166.50\n240.92\n193.15\n192.19\n140.20\nksd Latn\n198.81\n269.96\n210.59\n212.57\n138.81\nksh Latn\n204.72\n261.51\n220.93\n218.50\n161.62\nkss Latn\n310.35\n477.02\n335.25\n300.31\n226.38\nksw Mymr\n210.34\n266.24\n226.59\n154.55\n124.78\nkua Latn\n179.05\n206.09\n187.92\n151.87\n140.72\nkur Arab\n402.78\n464.44\n400.97\n550.57\n253.61\nkur Latn\n633.22\n779.47\n678.30\n748.20\n424.98\nkwn Latn\n136.80\n170.23\n141.88\n111.31\n107.21\nkwy Latn\n131.93\n160.78\n137.77\n134.01\n110.55\nlam Latn\n209.07\n276.89\n228.12\n203.17\n176.61\nlao Laoo\n405.48\n978.35\n435.37\n583.11\n225.06\nlat Latn\n167.49\n274.19\n186.97\n210.22\n183.32\nlav Latn\n193.22\n257.06\n227.60\n252.31\n162.80\nldi Latn\n178.84\n230.26\n185.58\n191.19\n160.61\nleh Latn\n216.80\n273.56\n230.25\n201.57\n172.92\nlfn Latn\n232.59\n368.62\n246.45\n258.76\n187.82\nlhu Latn\n209.10\n365.95\n220.56\n219.50\n142.74\nlij Latn\n328.66\n483.81\n345.62\n348.28\n249.64\nlim Latn\n199.01\n290.80\n236.94\n239.44\n180.52\nlin Latn\n161.88\n173.63\n158.33\n180.17\n135.66\nlit Latn\n163.71\n220.62\n195.08\n225.98\n147.53\nllb Latn\n135.01\n180.06\n146.51\n135.39\n120.02\nlmo Latn\n222.22\n378.21\n247.54\n242.80\n182.01\nloz Latn\n179.54\n194.46\n185.77\n142.19\n147.86\nltz Latn\n190.70\n303.65\n202.02\n174.00\n169.36\nlua Latn\n126.47\n147.86\n131.94\n102.71\n102.36\nlub Latn\n136.45\n143.64\n140.96\n99.41\n111.01\nlue Latn\n128.48\n158.40\n135.27\n129.94\n103.72\nlug Latn\n225.72\n318.09\n221.56\n272.90\n196.21\nlun Latn\n135.96\n170.81\n142.71\n136.26\n113.31\nTable 11: Detailed results of NLL on Glot500-c (Part IV).\n23\nPreprint. Under review.\nLang\nLLaMA 2-7B\nmGPT-13B\nBLOOM-7B1\nXGLM-7.5B\nMALA-500\nluo Latn\n177.43\n224.04\n194.07\n187.72\n156.23\nlus Latn\n192.97\n251.35\n203.37\n212.72\n163.95\nlvs Latn\n154.85\n211.40\n185.87\n198.99\n138.63\nlzh Hani\n149.57\n215.19\n130.32\n153.38\n151.15\nmad Latn\n232.71\n325.38\n245.39\n249.29\n176.81\nmah Latn\n178.50\n246.26\n188.98\n183.17\n145.35\nmai Deva\n245.94\n389.84\n189.93\n223.00\n185.84\nmal Mlym\n96.92\n171.55\n57.45\n129.61\n72.46\nmam Latn\n232.38\n315.16\n247.28\n244.43\n189.28\nmar Deva\n85.13\n143.31\n55.38\n103.23\n70.08\nmau Latn\n186.46\n333.61\n204.91\n193.09\n161.45\nmbb Latn\n282.70\n410.99\n309.56\n307.47\n175.50\nmck Latn\n191.94\n244.49\n202.17\n191.28\n152.16\nmcn Latn\n207.28\n276.32\n220.35\n230.56\n158.99\nmco Latn\n271.45\n368.23\n281.55\n260.70\n206.54\nmdy Ethi\n306.26\n529.46\n293.68\n369.22\n166.26\nmeu Latn\n177.74\n235.19\n188.09\n168.43\n143.62\nmfe Latn\n147.50\n194.41\n143.47\n92.23\n129.23\nmgh Latn\n193.72\n257.45\n207.05\n200.68\n166.17\nmgr Latn\n183.96\n226.09\n194.25\n149.77\n160.18\nmhr Cyrl\n230.20\n298.73\n235.59\n236.71\n167.55\nmin Latn\n161.40\n266.18\n164.13\n170.30\n166.91\nmiq Latn\n207.63\n276.27\n228.42\n223.78\n160.37\nmkd Cyrl\n81.62\n144.52\n112.99\n98.33\n74.40\nmlg Latn\n185.23\n250.78\n189.32\n226.85\n148.82\nmlt Latn\n109.60\n184.08\n139.75\n146.69\n85.14\nmny Latn\n133.04\n170.16\n135.30\n126.14\n112.38\nmon Cyrl\n397.63\n535.59\n446.51\n555.16\n249.95\nmon Latn\n354.75\n411.54\n383.60\n383.02\n282.85\nmos Latn\n197.23\n229.14\n206.05\n212.55\n159.69\nmps Latn\n347.99\n496.26\n378.75\n366.78\n213.10\nmri Latn\n154.38\n247.38\n181.49\n179.85\n134.55\nmrw Latn\n235.11\n306.78\n250.41\n253.18\n169.69\nmsa Latn\n164.05\n261.28\n155.14\n151.77\n190.44\nmwl Latn\n275.26\n410.83\n270.47\n280.98\n202.89\nmwm Latn\n293.40\n430.46\n315.11\n294.17\n162.95\nmwn Latn\n131.84\n162.91\n138.48\n111.20\n123.37\nmxv Latn\n206.13\n324.92\n222.48\n222.86\n171.82\nmya Mymr\n383.74\n576.49\n472.04\n277.91\n252.84\nmyv Cyrl\n267.24\n357.29\n263.68\n276.10\n188.74\nmzh Latn\n257.70\n370.86\n285.03\n276.60\n169.96\nmzn Arab\n192.75\n263.60\n200.51\n204.50\n136.03\nnan Latn\n172.36\n311.98\n186.78\n200.62\n153.96\nnap Latn\n159.24\n246.36\n179.36\n167.94\n151.29\nnaq Latn\n195.43\n261.60\n207.68\n207.27\n150.47\nnav Latn\n258.40\n380.88\n284.18\n286.04\n181.13\nnba Latn\n123.68\n154.25\n130.25\n126.08\n99.29\nnbl Latn\n175.10\n238.64\n194.74\n211.98\n154.90\nnch Latn\n206.55\n287.53\n220.86\n221.43\n183.56\nncj Latn\n185.32\n260.91\n201.13\n196.79\n173.80\nncx Latn\n115.71\n168.08\n121.23\n122.70\n98.71\nndc Latn\n167.38\n222.72\n176.18\n184.45\n158.24\nnde Latn\n169.75\n235.54\n185.98\n211.96\n151.45\nndo Latn\n192.10\n227.02\n204.45\n150.28\n149.69\nnds Latn\n195.44\n272.44\n213.17\n204.47\n184.93\nnep Deva\n232.93\n425.83\n167.54\n291.83\n210.52\nnew Deva\n169.64\n330.40\n128.26\n135.07\n103.54\nngl Latn\n134.87\n177.05\n140.92\n115.46\n104.59\nngu Latn\n205.16\n282.39\n215.65\n213.78\n167.56\nnia Latn\n202.30\n269.59\n214.87\n196.19\n167.95\nniu Latn\n105.04\n142.53\n111.71\n113.36\n88.11\nnld Latn\n37.77\n65.47\n55.52\n51.54\n51.45\nnmf Latn\n222.98\n290.53\n242.04\n246.36\n167.31\nnnb Latn\n200.64\n248.60\n210.13\n212.23\n161.20\nnno Latn\n138.72\n234.11\n192.13\n199.51\n146.16\nnob Latn\n50.27\n96.43\n78.24\n73.64\n59.05\nnor Latn\n78.04\n146.26\n126.19\n123.99\n99.50\nnpi Deva\n212.50\n399.24\n143.71\n290.95\n166.12\nTable 12: Detailed results of NLL on Glot500-c (Part V).\n24\nPreprint. Under review.\nLang\nLLaMA 2-7B\nmGPT-13B\nBLOOM-7B1\nXGLM-7.5B\nMALA-500\nnse Latn\n176.20\n234.62\n184.77\n174.57\n161.52\nnso Latn\n170.49\n227.97\n170.96\n201.29\n142.72\nnya Latn\n203.45\n299.12\n222.51\n224.89\n175.69\nnyk Latn\n131.13\n166.47\n142.89\n138.50\n105.26\nnyn Latn\n174.51\n229.51\n189.05\n194.14\n149.17\nnyu Latn\n126.29\n172.40\n132.49\n127.67\n99.26\nnyy Latn\n215.07\n271.23\n234.37\n220.80\n168.74\nnzi Latn\n191.21\n256.55\n219.47\n209.42\n152.30\noci Latn\n202.93\n343.11\n207.95\n210.24\n185.26\nogo Latn\n134.14\n185.86\n149.15\n143.22\n118.08\noke Latn\n131.90\n166.07\n146.98\n149.55\n102.72\nori Orya\n323.51\n839.80\n179.33\n665.17\n203.94\norm Latn\n225.00\n334.29\n288.51\n313.08\n201.60\nory Orya\n232.83\n572.34\n134.20\n474.35\n164.65\noss Cyrl\n229.49\n279.89\n229.34\n227.24\n151.79\note Latn\n237.06\n362.46\n254.31\n241.61\n176.73\npag Latn\n173.32\n223.39\n184.05\n184.76\n157.30\npam Latn\n259.01\n373.98\n274.16\n280.47\n237.10\npan Guru\n242.88\n510.70\n153.50\n395.85\n180.54\npap Latn\n162.79\n213.88\n174.63\n173.16\n138.20\npau Latn\n176.42\n243.03\n188.84\n187.10\n150.24\npcd Latn\n144.96\n228.79\n143.18\n150.08\n140.39\npcm Latn\n159.00\n346.35\n182.00\n179.53\n147.50\npdt Latn\n192.69\n252.34\n199.07\n199.80\n144.40\npes Arab\n153.46\n199.83\n175.97\n179.97\n139.01\npfl Latn\n220.11\n315.47\n241.84\n225.74\n176.25\nphm Latn\n117.81\n162.32\n128.28\n125.57\n100.73\npis Latn\n153.04\n237.95\n173.91\n179.21\n130.98\npls Latn\n237.55\n350.88\n251.43\n251.35\n175.28\nplt Latn\n159.36\n220.84\n158.06\n193.44\n131.96\npms Latn\n132.94\n257.06\n137.39\n146.18\n106.52\npnb Arab\n345.25\n418.35\n279.85\n240.35\n237.22\npoh Latn\n389.80\n589.86\n417.71\n416.42\n230.35\npol Latn\n44.19\n82.29\n66.66\n71.91\n60.02\npon Latn\n177.92\n236.38\n190.47\n189.62\n149.49\npor Latn\n37.00\n66.01\n35.14\n33.91\n48.72\nprk Latn\n220.51\n301.85\n230.42\n238.15\n148.46\nprs Arab\n163.01\n218.38\n191.40\n195.64\n141.99\npus Arab\n259.45\n327.43\n277.81\n340.38\n203.38\npxm Latn\n299.37\n391.48\n317.99\n307.01\n180.85\nqub Latn\n210.38\n265.76\n222.82\n172.89\n152.70\nquc Latn\n248.16\n320.50\n271.51\n258.06\n187.13\nque Latn\n144.31\n170.69\n154.62\n96.53\n121.19\nqug Latn\n176.78\n225.11\n187.16\n136.85\n143.62\nquh Latn\n257.89\n293.32\n275.35\n187.44\n175.55\nquw Latn\n154.10\n205.67\n162.83\n142.63\n142.35\nquy Latn\n177.21\n202.67\n190.48\n125.92\n139.15\nquz Latn\n180.20\n211.40\n192.52\n123.67\n142.21\nqvi Latn\n178.08\n234.53\n188.58\n156.22\n145.79\nrap Latn\n204.53\n354.21\n219.29\n226.89\n158.90\nrar Latn\n169.22\n249.96\n191.91\n189.56\n168.88\nrmn Cyrl\n129.46\n181.44\n143.84\n137.02\n102.76\nrmn Grek\n135.82\n190.47\n141.78\n125.21\n92.56\nrmn Latn\n133.75\n175.58\n146.05\n143.75\n112.55\nrmy Cyrl\n135.65\n184.00\n147.87\n137.05\n109.18\nrmy Latn\n189.65\n244.12\n198.92\n205.44\n168.77\nrng Latn\n122.59\n150.36\n125.06\n129.81\n104.16\nroh Latn\n235.38\n312.78\n242.57\n253.77\n161.16\nron Latn\n44.70\n84.55\n68.14\n74.76\n54.82\nrop Latn\n233.05\n351.35\n257.34\n275.70\n155.36\nrue Cyrl\n223.89\n402.99\n299.90\n265.32\n179.38\nrug Latn\n257.50\n348.10\n277.13\n275.47\n169.94\nrun Latn\n184.59\n218.12\n161.96\n207.06\n157.49\nrus Cyrl\n65.34\n155.39\n116.17\n67.59\n84.56\nsag Latn\n162.87\n194.78\n175.45\n155.14\n149.65\nsah Cyrl\n383.55\n455.30\n382.36\n423.03\n218.84\nsan Deva\n182.35\n287.49\n189.83\n201.00\n186.46\nsan Latn\n242.46\n324.45\n278.75\n282.93\n199.18\nTable 13: Detailed results of NLL on Glot500-c (Part VI).\n25\nPreprint. Under review.\nLang\nLLaMA 2-7B\nmGPT-13B\nBLOOM-7B1\nXGLM-7.5B\nMALA-500\nsat Olck\n654.37\n3377.97\n667.66\n40.17\n311.96\nsba Latn\n272.45\n372.47\n303.48\n293.62\n167.13\nscn Latn\n236.20\n355.24\n263.10\n270.02\n191.69\nsco Latn\n147.94\n341.79\n193.24\n193.20\n170.39\nseh Latn\n173.46\n231.41\n177.38\n174.40\n138.70\nsgs Latn\n248.33\n313.78\n251.35\n277.73\n182.16\nsid Latn\n135.53\n180.29\n147.72\n139.17\n114.24\nsin Sinh\n82.29\n173.16\n114.00\n137.98\n70.77\nskg Latn\n128.02\n172.67\n131.34\n145.32\n116.16\nslk Latn\n62.89\n116.82\n86.67\n103.39\n63.93\nslv Latn\n42.18\n85.28\n64.75\n73.49\n55.26\nsme Latn\n288.98\n357.31\n301.64\n295.23\n205.46\nsmo Latn\n220.26\n338.16\n250.74\n252.76\n190.00\nsna Latn\n221.02\n311.60\n221.92\n258.38\n189.74\nsnd Arab\n209.83\n264.61\n217.96\n260.53\n163.07\nsom Arab\n230.91\n410.59\n192.88\n175.01\n265.88\nsom Latn\n235.21\n346.36\n286.69\n312.99\n212.51\nsop Latn\n176.17\n207.78\n188.41\n157.21\n167.90\nsot Latn\n200.82\n271.71\n205.51\n235.65\n157.18\nspa Latn\n37.28\n70.48\n34.26\n38.65\n53.39\nsqi Latn\n207.58\n295.58\n241.22\n296.90\n172.78\nsrd Latn\n228.12\n341.00\n242.74\n251.01\n179.87\nsrm Latn\n229.46\n318.77\n250.79\n246.75\n173.83\nsrn Latn\n161.18\n183.34\n171.30\n179.59\n132.77\nsrp Cyrl\n45.22\n100.88\n77.59\n81.95\n57.85\nsrp Latn\n33.66\n57.89\n43.91\n46.74\n42.31\nssw Latn\n194.10\n264.22\n212.99\n230.20\n165.70\nsun Latn\n220.72\n314.99\n228.18\n237.07\n203.32\nsuz Deva\n255.00\n400.13\n262.34\n257.16\n157.30\nswa Latn\n156.02\n208.21\n125.78\n94.55\n151.68\nswc Latn\n103.75\n133.69\n98.32\n71.66\n102.14\nswe Latn\n42.72\n82.20\n68.89\n60.92\n56.18\nswh Latn\n178.28\n223.65\n151.05\n97.98\n161.49\nsxn Latn\n243.81\n346.98\n263.76\n260.44\n183.47\nszl Latn\n132.77\n348.45\n156.37\n177.33\n111.32\ntah Latn\n114.41\n158.18\n124.60\n121.22\n101.40\ntam Taml\n231.12\n444.83\n152.34\n146.94\n205.53\ntat Cyrl\n251.96\n301.03\n256.66\n276.29\n159.44\ntat Latn\n248.71\n338.00\n261.10\n278.92\n186.84\ntbz Latn\n273.90\n352.17\n299.25\n281.11\n164.62\ntca Latn\n306.13\n452.15\n328.77\n316.81\n174.51\ntcf Latn\n133.72\n193.63\n138.67\n133.58\n102.94\ntdt Latn\n158.16\n217.96\n172.56\n182.04\n130.27\ntdx Latn\n125.88\n167.70\n130.54\n135.72\n113.29\ntel Telu\n94.93\n152.33\n54.92\n47.52\n72.02\nteo Latn\n193.42\n250.17\n206.10\n193.68\n159.90\ntgk Cyrl\n313.76\n369.08\n333.83\n342.42\n196.57\ntgk Latn\n296.86\n412.46\n342.18\n352.59\n248.69\ntgl Latn\n56.44\n94.00\n76.30\n77.15\n64.98\ntha Thai\n192.70\n331.25\n242.28\n116.12\n175.60\ntih Latn\n233.30\n329.24\n255.15\n254.70\n158.13\ntir Ethi\n267.84\n579.39\n319.29\n424.77\n189.73\ntiv Latn\n133.38\n168.19\n140.43\n126.42\n116.08\ntlh Latn\n163.23\n258.64\n183.94\n184.72\n111.43\ntll Latn\n138.57\n167.75\n152.44\n126.10\n105.23\ntob Latn\n299.95\n450.25\n316.77\n324.19\n182.95\ntog Latn\n127.47\n165.93\n133.37\n115.35\n102.88\ntoh Latn\n181.85\n238.80\n196.33\n194.76\n146.50\ntoi Latn\n185.04\n233.23\n194.93\n164.94\n165.33\ntoj Latn\n232.66\n311.24\n239.53\n236.11\n198.17\ntok Latn\n46.19\n61.55\n50.56\n43.88\n47.57\nton Latn\n172.88\n243.40\n178.94\n190.23\n141.17\ntop Latn\n221.27\n303.90\n232.90\n223.12\n212.88\ntpi Latn\n139.90\n209.92\n155.89\n170.67\n120.65\ntpm Latn\n214.33\n280.83\n241.97\n231.70\n154.99\ntsc Latn\n131.42\n150.62\n130.29\n132.42\n104.44\ntsn Latn\n209.69\n291.69\n203.77\n245.18\n169.85\ntso Latn\n182.87\n208.89\n176.69\n194.90\n142.27\nTable 14: Detailed results of NLL on Glot500-c (Part VII).\n26\nPreprint. Under review.\nLang\nLLaMA 2-7B\nmGPT-13B\nBLOOM-7B1\nXGLM-7.5B\nMALA-500\ntsz Latn\n183.82\n253.97\n200.16\n176.35\n153.98\nttj Latn\n133.35\n174.08\n142.10\n146.60\n112.98\ntuc Latn\n325.34\n444.23\n346.52\n291.84\n180.84\ntui Latn\n247.40\n330.20\n266.54\n265.71\n181.39\ntuk Cyrl\n196.40\n248.01\n210.45\n219.39\n143.19\ntuk Latn\n217.31\n235.95\n217.78\n238.66\n155.71\ntum Latn\n184.51\n236.91\n190.41\n153.36\n153.44\ntur Latn\n48.52\n66.76\n60.61\n34.71\n63.33\ntvl Latn\n114.81\n156.00\n123.30\n121.62\n97.96\ntwi Latn\n169.99\n229.39\n171.42\n190.50\n139.81\ntwx Latn\n123.52\n172.96\n130.82\n135.08\n106.56\ntyv Cyrl\n270.89\n314.09\n275.97\n304.11\n174.60\ntzh Latn\n195.49\n274.47\n208.05\n202.10\n162.63\ntzo Latn\n223.14\n324.35\n237.54\n228.92\n173.78\nudm Cyrl\n222.45\n277.14\n231.98\n219.71\n160.30\nuig Arab\n336.01\n432.84\n320.43\n463.38\n207.25\nuig Latn\n254.59\n292.36\n270.85\n285.12\n203.29\nukr Cyrl\n101.99\n240.89\n173.79\n160.03\n136.57\numb Latn\n129.60\n165.59\n135.06\n139.75\n100.07\nurd Arab\n77.96\n105.77\n53.61\n51.92\n81.62\nurh Latn\n145.52\n153.19\n164.21\n161.54\n108.55\nuzb Cyrl\n307.70\n353.00\n332.86\n314.77\n178.07\nuzb Latn\n307.44\n363.61\n357.04\n383.26\n220.74\nuzn Cyrl\n233.89\n270.06\n254.96\n247.92\n145.01\nvec Latn\n163.22\n261.93\n181.25\n168.76\n170.03\nven Latn\n190.45\n233.75\n198.94\n198.18\n151.65\nvep Latn\n316.12\n456.77\n326.76\n243.40\n192.08\nvie Latn\n108.65\n169.92\n86.74\n91.41\n138.89\nvls Latn\n200.17\n292.89\n242.66\n253.44\n171.13\nvmw Latn\n141.25\n176.25\n143.12\n107.10\n102.92\nvol Latn\n94.00\n260.01\n85.47\n87.18\n83.77\nwal Latn\n190.62\n261.79\n201.98\n177.73\n158.07\nwar Latn\n127.41\n249.86\n146.46\n166.29\n153.84\nwbm Latn\n222.06\n311.86\n234.78\n240.27\n150.33\nwes Latn\n64.78\n106.54\n73.37\n73.73\n86.61\nwls Latn\n114.80\n157.93\n125.63\n124.58\n99.38\nwol Latn\n197.17\n251.63\n171.70\n208.78\n173.01\nwuu Hani\n152.90\n283.11\n127.83\n152.82\n145.05\nxav Latn\n350.22\n619.11\n379.76\n371.80\n201.63\nxho Latn\n224.10\n315.12\n219.57\n265.57\n187.35\nxmf Geor\n260.61\n315.58\n316.49\n376.33\n170.15\nxmv Latn\n125.37\n168.73\n129.48\n139.97\n111.94\nyan Latn\n228.46\n314.62\n248.18\n243.68\n165.66\nyao Latn\n196.25\n253.72\n209.77\n198.91\n166.06\nyap Latn\n197.98\n274.54\n212.39\n209.00\n169.09\nyid Hebr\n437.75\n571.08\n480.37\n590.32\n295.70\nyom Latn\n176.11\n220.86\n184.62\n189.29\n150.95\nyor Latn\n233.75\n283.33\n193.55\n286.20\n185.60\nyua Latn\n195.86\n284.05\n208.08\n205.70\n161.16\nyue Hani\n74.79\n131.83\n62.91\n83.80\n74.28\nzai Latn\n170.49\n223.03\n179.18\n188.38\n148.03\nzea Latn\n174.18\n271.42\n212.95\n222.74\n155.52\nzho Hani\n57.89\n99.40\n48.19\n55.24\n70.80\nzlm Latn\n106.37\n176.09\n92.63\n93.81\n118.56\nzne Latn\n127.57\n167.13\n134.43\n115.53\n104.95\nzom Latn\n214.60\n277.57\n233.64\n228.48\n170.06\nzpa Latn\n127.29\n180.39\n129.07\n132.30\n107.04\nzsm Latn\n102.42\n171.64\n92.39\n94.59\n123.31\nzul Latn\n208.94\n340.58\n235.91\n257.18\n192.84\nall\n190.58\n282.46\n202.95\n205.07\n151.25\nTable 15: Detailed results of NLL on Glot500-c (Part VIII).\n27\nPreprint. Under review.\nLang\nLLaMA 2-7B\nmGPT-13B\nBLOOM-7B1\nXGLM-7.5B\nMALA-500\nace Latn\n137.43\n196.93\n144.50\n152.49\n97.89\nach Latn\n113.66\n152.08\n123.29\n125.31\n102.45\nacr Latn\n177.86\n233.22\n188.27\n182.33\n114.27\nafr Latn\n80.43\n132.25\n116.34\n129.21\n95.33\nagw Latn\n130.32\n186.58\n136.17\n138.27\n95.93\nahk Latn\n175.75\n291.31\n187.63\n179.54\n116.76\naka Latn\n98.41\n135.74\n99.46\n108.01\n78.20\naln Latn\n101.54\n147.77\n115.11\n139.73\n82.71\nals Latn\n93.47\n134.99\n106.68\n127.57\n78.53\nalt Cyrl\n122.23\n146.47\n125.04\n134.88\n90.21\nalz Latn\n107.41\n139.39\n116.48\n109.62\n102.48\namh Ethi\n100.60\n255.43\n121.36\n161.34\n98.24\naoj Latn\n175.25\n270.87\n185.66\n171.29\n114.19\narb Arab\n94.03\n186.47\n77.67\n78.12\n104.22\narn Latn\n141.08\n205.53\n143.63\n154.61\n113.59\nary Arab\n128.97\n212.49\n125.35\n118.25\n104.58\narz Arab\n80.59\n185.56\n64.91\n66.52\n92.22\nasm Beng\n123.16\n196.03\n79.80\n147.49\n101.89\nayr Latn\n149.96\n188.09\n154.45\n157.13\n106.66\nazb Arab\n134.00\n160.29\n139.49\n144.68\n93.09\naze Latn\n97.68\n131.80\n113.03\n106.38\n90.96\nbak Cyrl\n134.49\n169.96\n133.79\n150.46\n93.49\nbam Latn\n109.68\n147.72\n110.24\n118.13\n91.88\nban Latn\n138.98\n195.92\n147.93\n149.04\n111.51\nbar Latn\n114.49\n154.37\n121.74\n113.26\n108.65\nbba Latn\n132.00\n166.51\n146.31\n131.24\n96.87\nbbc Latn\n110.66\n143.87\n117.12\n107.02\n100.17\nbci Latn\n117.42\n156.47\n125.70\n124.26\n126.80\nbcl Latn\n101.39\n146.46\n109.03\n116.78\n88.46\nbel Cyrl\n92.30\n137.26\n110.12\n118.30\n88.89\nbem Latn\n125.52\n158.97\n135.60\n104.16\n107.57\nben Beng\n111.68\n194.50\n68.00\n77.83\n105.61\nbhw Latn\n124.94\n169.40\n130.40\n123.48\n101.65\nbim Latn\n124.64\n162.78\n132.77\n130.01\n96.33\nbis Latn\n126.46\n196.19\n136.72\n148.29\n95.85\nbod Tibt\n138.16\n525.70\n144.33\n30.40\n105.99\nbqc Latn\n113.18\n149.13\n122.07\n112.76\n91.11\nbre Latn\n120.49\n151.33\n111.97\n139.13\n105.99\nbts Latn\n111.90\n154.57\n120.61\n110.17\n89.16\nbtx Latn\n118.13\n163.25\n128.43\n125.76\n103.19\nbul Cyrl\n66.25\n124.78\n104.01\n42.33\n85.30\nbum Latn\n116.16\n153.66\n121.83\n121.74\n101.82\nbzj Latn\n115.75\n175.63\n128.70\n135.59\n93.15\ncab Latn\n164.07\n215.31\n172.22\n174.35\n123.20\ncac Latn\n169.42\n231.73\n176.29\n175.63\n116.03\ncak Latn\n185.42\n246.76\n193.62\n191.54\n123.65\ncaq Latn\n128.13\n174.12\n141.21\n138.17\n95.54\ncat Latn\n54.93\n118.69\n44.29\n45.98\n76.47\ncbk Latn\n103.50\n154.23\n105.08\n108.15\n91.19\ncce Latn\n124.20\n159.68\n133.40\n132.89\n106.02\nceb Latn\n99.37\n146.70\n113.69\n132.72\n94.43\nces Latn\n62.40\n133.26\n101.82\n114.59\n86.91\ncfm Latn\n138.07\n179.26\n142.58\n143.43\n107.20\nche Cyrl\n152.68\n188.52\n146.76\n148.42\n126.87\nchk Latn\n128.34\n180.14\n133.81\n134.01\n97.76\nchv Cyrl\n132.89\n166.12\n138.37\n128.58\n91.96\nckb Arab\n126.47\n155.90\n125.59\n164.22\n100.65\ncmn Hani\n63.67\n121.22\n51.49\n60.91\n76.95\ncnh Latn\n129.26\n175.83\n134.65\n139.53\n104.21\ncrh Cyrl\n128.56\n166.14\n128.91\n139.13\n82.61\ncrs Latn\n100.72\n139.95\n101.88\n57.70\n80.86\ncsy Latn\n125.81\n172.44\n138.22\n132.16\n100.90\nctd Latn\n120.85\n163.07\n128.99\n125.52\n92.79\nctu Latn\n156.04\n220.78\n162.45\n157.63\n112.41\ncuk Latn\n151.95\n213.08\n159.59\n156.10\n119.01\ncym Latn\n110.34\n165.10\n135.91\n147.72\n103.89\nTable 16: Detailed results of NLL on PBC (Part I).\n28\nPreprint. Under review.\nLang\nLLaMA 2-7B\nmGPT-13B\nBLOOM-7B1\nXGLM-7.5B\nMALA-500\ndan Latn\n63.65\n114.43\n97.95\n101.13\n86.68\ndeu Latn\n57.09\n109.69\n84.08\n54.00\n80.90\ndjk Latn\n143.19\n192.80\n147.13\n153.74\n120.66\ndln Latn\n113.43\n155.19\n118.82\n125.73\n92.37\ndtp Latn\n158.44\n222.01\n165.46\n169.63\n111.77\ndyu Latn\n122.24\n161.61\n126.53\n132.73\n103.04\ndzo Tibt\n157.37\n550.44\n162.42\n36.35\n99.22\nefi Latn\n121.73\n173.61\n139.58\n136.78\n90.15\nell Grek\n80.65\n169.16\n109.07\n57.11\n105.74\neng Latn\n28.40\n93.81\n40.01\n42.56\n46.91\nenm Latn\n45.43\n113.74\n62.99\n66.87\n55.22\nepo Latn\n79.83\n125.27\n88.81\n100.79\n85.24\nest Latn\n93.49\n128.66\n109.45\n45.04\n99.10\neus Latn\n133.89\n145.19\n101.06\n78.92\n150.43\newe Latn\n140.69\n190.49\n147.85\n133.36\n103.15\nfao Latn\n101.92\n150.02\n113.16\n134.84\n93.21\nfas Arab\n87.19\n121.18\n99.32\n104.15\n77.85\nfij Latn\n110.29\n158.90\n130.18\n97.89\n97.65\nfil Latn\n74.66\n130.09\n106.51\n109.03\n84.32\nfin Latn\n68.42\n125.52\n116.35\n38.52\n91.75\nfon Latn\n160.80\n210.76\n176.23\n178.40\n107.16\nfra Latn\n46.01\n105.73\n38.57\n44.16\n73.33\nfry Latn\n111.69\n146.88\n111.45\n123.28\n100.32\ngaa Latn\n128.54\n165.53\n145.88\n107.90\n100.68\ngil Latn\n125.22\n171.28\n131.71\n130.68\n106.24\ngiz Latn\n131.75\n183.07\n145.21\n143.35\n97.84\ngkn Latn\n151.99\n210.57\n167.40\n166.78\n116.75\ngkp Latn\n159.33\n219.00\n168.31\n166.05\n110.30\ngla Latn\n102.90\n174.06\n129.10\n138.42\n100.51\ngle Latn\n102.09\n161.80\n132.57\n146.14\n116.86\nglv Latn\n122.94\n172.06\n126.34\n134.37\n98.35\ngom Latn\n149.35\n199.59\n155.54\n159.44\n129.81\ngor Latn\n156.67\n215.11\n170.13\n167.89\n115.02\ngrc Grek\n64.91\n153.70\n93.39\n68.67\n81.49\nguc Latn\n193.75\n271.60\n202.31\n190.66\n138.75\ngug Latn\n139.06\n183.84\n146.45\n151.28\n114.14\nguj Gujr\n121.18\n329.05\n86.23\n202.19\n107.88\ngur Latn\n143.42\n208.51\n152.80\n148.18\n106.41\nguw Latn\n142.60\n155.00\n158.22\n166.16\n98.92\ngya Latn\n130.25\n197.61\n146.23\n137.31\n99.85\ngym Latn\n180.93\n262.58\n196.74\n161.03\n135.73\nhat Latn\n112.20\n159.68\n116.00\n48.45\n90.71\nhau Latn\n105.95\n146.45\n117.21\n127.18\n96.63\nhaw Latn\n91.42\n140.04\n102.87\n102.50\n91.03\nheb Hebr\n86.85\n197.96\n113.81\n125.21\n143.56\nhif Latn\n104.78\n161.10\n114.69\n116.63\n107.93\nhil Latn\n103.93\n151.84\n112.82\n130.28\n90.13\nhin Deva\n87.35\n175.19\n62.49\n63.21\n103.09\nhin Latn\n102.01\n144.04\n112.84\n112.96\n109.68\nhmo Latn\n119.64\n179.32\n128.46\n103.09\n91.86\nhne Deva\n124.72\n183.69\n106.59\n120.10\n94.27\nhnj Latn\n126.88\n186.09\n144.08\n149.87\n89.64\nhra Latn\n116.66\n151.27\n122.49\n122.14\n96.72\nhrv Latn\n62.52\n125.68\n96.82\n107.18\n73.96\nhui Latn\n151.46\n203.46\n161.05\n161.36\n108.54\nhun Latn\n69.17\n118.92\n117.60\n125.55\n94.04\nhus Latn\n170.91\n241.76\n179.70\n177.42\n120.81\nhye Armn\n111.94\n219.94\n141.97\n171.24\n89.75\niba Latn\n102.40\n135.32\n109.00\n102.90\n87.43\nibo Latn\n131.16\n189.15\n130.12\n172.79\n112.01\nifa Latn\n140.53\n194.86\n151.53\n148.37\n102.38\nifb Latn\n149.93\n198.42\n157.12\n156.49\n107.60\nikk Latn\n132.84\n186.95\n150.31\n163.16\n95.14\nilo Latn\n119.72\n162.55\n127.85\n146.58\n102.18\nind Latn\n66.39\n121.78\n58.14\n58.36\n80.77\nisl Latn\n92.39\n137.42\n113.54\n123.83\n94.12\nita Latn\n54.57\n116.50\n73.53\n52.57\n78.23\nTable 17: Detailed results of NLL on PBC (Part II).\n29\nPreprint. Under review.\nLang\nLLaMA 2-7B\nmGPT-13B\nBLOOM-7B1\nXGLM-7.5B\nMALA-500\nium Latn\n150.62\n222.39\n155.20\n157.52\n99.54\nixl Latn\n190.07\n299.20\n206.08\n202.92\n127.52\nizz Latn\n167.28\n228.45\n195.19\n198.57\n118.78\njam Latn\n119.85\n181.93\n134.52\n139.07\n96.42\njav Latn\n134.11\n171.16\n136.06\n140.01\n109.34\njpn Jpan\n67.67\n114.11\n84.64\n61.57\n88.53\nkaa Cyrl\n136.14\n179.48\n138.63\n153.33\n84.79\nkaa Latn\n134.02\n172.76\n135.14\n145.18\n99.15\nkab Latn\n137.81\n193.54\n129.87\n159.45\n117.96\nkac Latn\n141.33\n187.59\n150.24\n163.68\n110.99\nkal Latn\n120.90\n143.71\n134.44\n90.38\n109.58\nkan Knda\n128.60\n336.06\n93.77\n210.99\n110.09\nkat Geor\n103.81\n132.04\n144.43\n155.32\n93.39\nkaz Cyrl\n129.49\n166.60\n137.43\n150.12\n108.56\nkbp Latn\n151.83\n205.24\n166.76\n156.21\n105.09\nkek Latn\n161.79\n230.77\n168.62\n155.46\n110.43\nkhm Khmr\n141.48\n453.97\n161.21\n233.38\n100.53\nkia Latn\n122.81\n171.17\n136.22\n131.73\n95.76\nkik Latn\n141.34\n189.92\n143.91\n155.69\n106.53\nkin Latn\n110.75\n137.92\n101.14\n123.88\n99.96\nkir Cyrl\n125.74\n148.16\n127.29\n148.79\n94.02\nkjb Latn\n152.31\n205.47\n156.49\n160.88\n109.02\nkjh Cyrl\n133.84\n168.82\n142.31\n145.53\n97.43\nkmm Latn\n137.88\n185.46\n149.16\n145.91\n107.81\nkmr Cyrl\n139.23\n182.66\n137.99\n142.19\n103.56\nkmr Latn\n120.54\n149.31\n124.74\n136.78\n96.93\nknv Latn\n249.77\n346.55\n266.68\n245.87\n135.66\nkor Hang\n66.58\n119.14\n92.53\n42.28\n82.45\nkpg Latn\n128.18\n190.92\n139.68\n135.05\n90.65\nkrc Cyrl\n123.42\n149.60\n119.82\n130.97\n89.22\nkri Latn\n118.15\n172.62\n134.67\n131.33\n96.69\nksd Latn\n108.75\n155.22\n117.82\n116.91\n84.44\nkss Latn\n248.46\n385.70\n269.58\n224.81\n174.70\nksw Mymr\n145.34\n187.44\n155.38\n107.97\n94.71\nkua Latn\n118.00\n142.31\n125.97\n104.16\n99.83\nlam Latn\n145.51\n199.78\n154.91\n139.07\n115.48\nlao Laoo\n163.17\n414.25\n172.28\n234.46\n116.39\nlat Latn\n56.98\n102.85\n65.60\n73.77\n73.15\nlav Latn\n90.61\n119.37\n103.75\n114.03\n94.92\nldi Latn\n118.61\n161.07\n122.03\n124.26\n112.27\nleh Latn\n131.72\n169.51\n140.67\n124.57\n104.47\nlhu Latn\n147.32\n262.73\n152.94\n153.96\n100.83\nlin Latn\n113.81\n128.30\n110.20\n123.56\n92.52\nlit Latn\n92.16\n120.15\n107.63\n123.52\n97.69\nloz Latn\n119.93\n140.69\n125.00\n98.71\n99.46\nltz Latn\n114.62\n156.92\n114.49\n104.79\n96.89\nlug Latn\n117.59\n174.83\n115.12\n143.99\n107.58\nluo Latn\n118.37\n158.54\n129.88\n126.61\n108.96\nlus Latn\n122.17\n159.02\n125.37\n133.65\n103.21\nlzh Hani\n62.06\n88.07\n54.92\n60.19\n66.36\nmad Latn\n136.26\n192.90\n146.43\n145.94\n103.63\nmah Latn\n113.96\n159.42\n120.91\n110.27\n97.45\nmai Deva\n136.92\n209.91\n108.91\n126.23\n100.39\nmal Mlym\n111.12\n210.81\n72.27\n126.62\n105.00\nmam Latn\n173.35\n227.62\n181.33\n179.63\n138.57\nmar Deva\n105.80\n184.52\n83.30\n141.12\n106.37\nmau Latn\n139.06\n259.48\n153.06\n140.49\n148.96\nmbb Latn\n160.77\n237.84\n174.36\n171.35\n101.96\nmck Latn\n124.95\n161.95\n131.37\n123.87\n99.72\nmcn Latn\n110.95\n153.55\n120.44\n123.48\n96.39\nmco Latn\n203.59\n285.23\n205.92\n192.68\n159.16\nmdy Ethi\n164.72\n284.41\n157.66\n188.38\n92.89\nmeu Latn\n111.26\n152.92\n120.09\n103.47\n91.50\nmfe Latn\n99.68\n136.00\n98.60\n55.39\n80.86\nmgh Latn\n131.75\n181.11\n140.22\n136.00\n118.72\nmgr Latn\n126.60\n154.99\n129.40\n106.55\n108.42\nTable 18: Detailed results of NLL on PBC (Part III).\n30\nPreprint. Under review.\nLang\nLLaMA 2-7B\nmGPT-13B\nBLOOM-7B1\nXGLM-7.5B\nMALA-500\nmhr Cyrl\n122.42\n160.48\n119.36\n127.42\n100.09\nmin Latn\n139.41\n194.79\n136.22\n138.87\n133.30\nmiq Latn\n129.28\n182.98\n144.36\n141.12\n104.92\nmkd Cyrl\n85.29\n151.22\n112.67\n89.21\n89.46\nmlg Latn\n107.66\n135.73\n106.88\n128.75\n86.60\nmlt Latn\n108.58\n168.92\n134.24\n137.26\n107.12\nmos Latn\n129.97\n161.07\n135.30\n138.61\n112.98\nmps Latn\n196.29\n283.21\n212.92\n204.15\n126.56\nmri Latn\n87.56\n138.21\n103.82\n111.33\n88.68\nmrw Latn\n127.39\n174.88\n134.59\n133.06\n99.21\nmsa Latn\n104.71\n152.69\n97.60\n93.04\n113.32\nmwm Latn\n159.30\n238.34\n171.46\n159.27\n99.80\nmxv Latn\n146.98\n235.76\n162.84\n164.53\n126.52\nmya Mymr\n162.62\n248.51\n185.69\n84.78\n107.92\nmyv Cyrl\n148.95\n192.16\n140.65\n152.76\n110.76\nmzh Latn\n146.28\n217.81\n160.03\n153.09\n101.97\nnan Latn\n130.85\n204.44\n144.08\n138.29\n118.30\nnaq Latn\n126.47\n179.33\n139.25\n135.80\n100.90\nnav Latn\n167.01\n233.91\n176.25\n183.89\n119.97\nnbl Latn\n109.14\n148.07\n114.06\n127.75\n96.55\nnch Latn\n155.09\n212.74\n165.40\n171.74\n144.74\nncj Latn\n131.14\n184.55\n137.38\n140.86\n129.63\nndc Latn\n106.50\n151.16\n111.70\n117.94\n107.15\nnde Latn\n106.83\n152.97\n114.79\n133.97\n100.83\nndo Latn\n132.12\n162.83\n138.17\n107.11\n105.82\nnds Latn\n123.29\n166.55\n124.21\n125.62\n123.87\nnep Deva\n109.47\n199.05\n81.70\n141.10\n103.11\nngu Latn\n148.78\n204.17\n156.73\n156.92\n120.15\nnia Latn\n135.60\n192.37\n143.95\n130.11\n111.86\nnld Latn\n58.81\n114.31\n96.47\n97.28\n82.78\nnmf Latn\n122.39\n165.17\n130.43\n134.30\n98.07\nnnb Latn\n122.26\n163.28\n127.40\n131.83\n98.36\nnno Latn\n80.33\n133.43\n102.92\n112.53\n86.17\nnob Latn\n61.45\n126.38\n100.25\n98.89\n80.02\nnor Latn\n56.27\n104.11\n87.94\n86.18\n71.86\nnpi Deva\n115.63\n219.43\n78.62\n159.24\n96.97\nnse Latn\n116.86\n157.47\n124.34\n116.64\n109.55\nnso Latn\n116.55\n160.63\n114.34\n132.40\n97.49\nnya Latn\n112.30\n160.76\n116.85\n124.27\n101.20\nnyn Latn\n120.67\n159.71\n127.46\n131.05\n106.34\nnyy Latn\n153.10\n189.04\n164.69\n160.66\n121.06\nnzi Latn\n130.01\n179.62\n150.60\n141.28\n101.29\nori Orya\n148.25\n392.96\n91.33\n296.43\n98.06\nory Orya\n143.02\n352.28\n95.95\n282.70\n106.99\noss Cyrl\n140.22\n182.75\n141.83\n139.80\n97.09\note Latn\n160.20\n247.13\n175.42\n168.28\n119.40\npag Latn\n123.49\n163.26\n131.05\n133.56\n109.90\npam Latn\n117.54\n163.02\n121.69\n130.95\n103.78\npan Guru\n130.07\n286.36\n90.80\n208.67\n106.44\npap Latn\n110.09\n149.82\n118.80\n114.73\n92.08\npau Latn\n125.22\n178.67\n132.62\n131.85\n104.80\npcm Latn\n76.80\n127.05\n89.92\n91.28\n79.44\npdt Latn\n124.83\n175.03\n129.63\n126.61\n97.29\npes Arab\n91.68\n129.42\n105.39\n105.84\n84.63\npis Latn\n118.50\n180.76\n130.26\n133.14\n95.70\npls Latn\n147.97\n217.00\n152.42\n153.90\n104.29\nplt Latn\n113.52\n139.96\n112.22\n139.93\n89.18\npoh Latn\n240.95\n363.61\n257.65\n256.21\n140.88\npol Latn\n61.88\n111.24\n97.90\n107.87\n85.46\npon Latn\n123.40\n164.68\n131.48\n125.12\n105.87\npor Latn\n53.69\n106.83\n42.29\n45.85\n75.88\nprk Latn\n118.66\n167.68\n121.37\n128.55\n94.48\nprs Arab\n88.26\n123.81\n99.63\n105.09\n80.28\npxm Latn\n154.30\n207.27\n160.81\n160.27\n102.81\nqub Latn\n133.85\n172.77\n139.98\n107.69\n93.49\nquc Latn\n176.66\n222.18\n191.60\n178.35\n124.30\nTable 19: Detailed results of NLL on PBC (Part IV).\n31\nPreprint. Under review.\nLang\nLLaMA 2-7B\nmGPT-13B\nBLOOM-7B1\nXGLM-7.5B\nMALA-500\nqug Latn\n124.11\n158.66\n131.62\n95.23\n95.07\nquh Latn\n148.83\n174.81\n154.34\n107.04\n106.24\nquw Latn\n104.78\n139.63\n109.91\n95.69\n92.58\nquy Latn\n119.84\n140.49\n127.93\n85.16\n94.14\nquz Latn\n126.18\n149.08\n134.60\n85.68\n96.32\nqvi Latn\n134.03\n177.51\n139.73\n114.64\n100.81\nrap Latn\n139.27\n239.23\n152.39\n157.18\n100.81\nrar Latn\n136.30\n205.48\n152.87\n149.88\n123.36\nrmy Latn\n124.05\n164.59\n129.35\n132.97\n108.84\nron Latn\n71.75\n145.55\n113.22\n136.42\n92.16\nrop Latn\n141.24\n218.11\n152.37\n163.59\n93.46\nrug Latn\n144.21\n200.64\n155.21\n151.73\n99.72\nrun Latn\n111.11\n140.11\n101.95\n120.78\n99.61\nrus Cyrl\n57.09\n115.06\n85.44\n48.93\n78.66\nsag Latn\n118.57\n144.91\n123.32\n113.66\n101.74\nsah Cyrl\n140.83\n175.34\n139.86\n155.36\n99.78\nsan Deva\n120.11\n183.77\n128.71\n131.38\n123.45\nsan Latn\n133.78\n188.35\n151.17\n152.84\n112.82\nsba Latn\n147.44\n205.90\n167.36\n154.66\n98.05\nseh Latn\n116.71\n159.73\n123.08\n121.65\n100.51\nsin Sinh\n133.79\n283.43\n166.13\n228.72\n113.72\nslk Latn\n75.89\n141.01\n105.13\n123.74\n89.45\nslv Latn\n75.67\n140.40\n111.88\n127.31\n95.15\nsme Latn\n134.17\n166.51\n131.28\n132.85\n103.75\nsmo Latn\n113.64\n165.04\n126.68\n127.57\n96.65\nsna Latn\n107.03\n157.69\n112.48\n124.30\n99.14\nsnd Arab\n141.08\n183.48\n144.96\n173.65\n107.47\nsom Latn\n114.80\n163.60\n131.06\n149.83\n110.34\nsop Latn\n120.92\n148.69\n129.81\n113.62\n117.37\nsot Latn\n112.14\n155.55\n113.59\n127.04\n95.35\nspa Latn\n49.64\n107.41\n43.22\n48.95\n69.30\nsqi Latn\n106.17\n145.44\n116.56\n140.41\n92.13\nsrm Latn\n172.30\n242.13\n187.65\n185.42\n124.54\nsrn Latn\n112.06\n137.43\n113.65\n121.74\n91.24\nsrp Cyrl\n57.16\n129.53\n97.17\n99.06\n71.36\nsrp Latn\n61.53\n124.70\n95.02\n105.00\n71.54\nssw Latn\n120.48\n172.73\n132.69\n140.25\n104.17\nsun Latn\n123.92\n165.15\n124.81\n129.90\n111.93\nsuz Deva\n141.06\n222.01\n143.66\n139.17\n93.18\nswe Latn\n60.78\n124.59\n105.53\n99.90\n86.99\nswh Latn\n97.92\n131.52\n87.87\n54.27\n90.87\nsxn Latn\n173.04\n249.27\n188.33\n183.25\n124.96\ntam Taml\n109.64\n213.05\n70.91\n64.81\n100.45\ntat Cyrl\n136.52\n167.63\n136.15\n147.39\n94.42\ntbz Latn\n135.12\n176.55\n145.64\n137.50\n88.42\ntca Latn\n202.44\n294.68\n215.39\n207.66\n112.33\ntdt Latn\n114.70\n164.66\n123.50\n129.26\n93.86\ntel Telu\n122.18\n196.41\n91.03\n65.40\n115.17\nteo Latn\n115.64\n157.71\n122.24\n117.99\n99.95\ntgk Cyrl\n128.86\n144.47\n127.10\n140.25\n101.04\ntgl Latn\n74.71\n130.27\n109.14\n110.56\n85.29\ntha Thai\n107.69\n187.16\n134.02\n58.96\n101.09\ntih Latn\n129.95\n188.97\n139.91\n137.24\n89.82\ntir Ethi\n122.75\n258.00\n143.76\n190.93\n99.03\ntlh Latn\n87.59\n142.90\n97.02\n97.38\n62.55\ntob Latn\n179.90\n269.36\n189.99\n191.60\n107.12\ntoh Latn\n127.60\n171.62\n136.60\n136.06\n104.79\ntoi Latn\n124.75\n166.04\n133.32\n114.25\n114.54\ntoj Latn\n175.52\n237.75\n181.56\n177.72\n148.72\nton Latn\n120.61\n179.89\n125.92\n137.67\n98.37\ntop Latn\n165.19\n223.46\n174.82\n173.24\n164.19\ntpi Latn\n105.47\n161.38\n117.02\n128.35\n84.22\ntpm Latn\n120.52\n166.93\n131.95\n129.07\n89.91\ntsn Latn\n112.13\n163.36\n113.76\n129.32\n96.63\ntso Latn\n125.25\n155.16\n120.37\n134.66\n103.51\ntsz Latn\n129.96\n184.88\n142.29\n126.39\n110.66\ntuc Latn\n187.91\n261.93\n196.20\n166.43\n106.46\nTable 20: Detailed results of NLL on PBC (Part V).\n32\nPreprint. Under review.\nLang\nLLaMA 2-7B\nmGPT-13B\nBLOOM-7B1\nXGLM-7.5B\nMALA-500\ntui Latn\n135.41\n187.71\n146.21\n146.43\n107.57\ntuk Cyrl\n127.20\n168.02\n136.69\n145.86\n94.72\ntuk Latn\n123.72\n144.22\n124.67\n135.21\n97.42\ntum Latn\n127.49\n165.45\n130.05\n109.19\n102.35\ntur Latn\n76.97\n118.11\n102.96\n57.95\n99.22\ntwi Latn\n110.21\n159.12\n110.54\n122.43\n93.81\ntyv Cyrl\n165.82\n197.37\n164.25\n181.87\n107.33\ntzh Latn\n147.06\n205.37\n157.00\n148.16\n118.46\ntzo Latn\n166.45\n248.81\n178.03\n173.52\n122.42\nudm Cyrl\n138.00\n176.90\n140.39\n137.21\n102.56\nuig Arab\n166.57\n226.61\n157.03\n229.43\n114.04\nuig Latn\n145.11\n165.68\n156.02\n157.78\n121.77\nukr Cyrl\n68.45\n134.95\n101.40\n93.65\n92.95\nurd Arab\n99.74\n141.13\n74.20\n63.53\n110.49\nuzb Cyrl\n128.48\n149.94\n136.85\n135.60\n88.90\nuzb Latn\n118.83\n138.99\n132.96\n145.00\n95.19\nuzn Cyrl\n136.04\n160.00\n145.95\n142.34\n94.12\nven Latn\n131.18\n172.05\n138.68\n137.88\n104.82\nvie Latn\n74.42\n115.85\n56.37\n59.30\n91.10\nwal Latn\n129.99\n180.12\n134.43\n122.12\n105.68\nwar Latn\n111.26\n159.23\n118.85\n131.06\n113.74\nwbm Latn\n120.96\n174.48\n126.11\n128.99\n94.78\nwol Latn\n115.67\n154.99\n101.97\n127.09\n102.93\nxav Latn\n243.35\n430.83\n263.49\n257.10\n137.76\nxho Latn\n112.96\n155.63\n109.11\n135.39\n107.43\nyan Latn\n125.81\n179.37\n136.34\n131.31\n98.47\nyao Latn\n143.68\n187.96\n148.92\n143.16\n114.54\nyap Latn\n150.28\n207.86\n157.67\n157.91\n123.07\nyom Latn\n118.61\n155.58\n121.64\n126.20\n100.18\nyor Latn\n129.77\n166.68\n100.87\n155.88\n105.79\nyua Latn\n148.34\n218.12\n155.65\n156.40\n118.30\nyue Hani\n64.57\n122.47\n54.42\n62.71\n87.78\nzai Latn\n121.90\n161.31\n121.61\n129.12\n108.99\nzho Hani\n64.02\n115.19\n51.79\n63.22\n69.53\nzlm Latn\n57.83\n101.11\n48.96\n51.87\n64.76\nzom Latn\n119.86\n159.31\n128.06\n125.99\n98.96\nzsm Latn\n60.40\n110.60\n51.75\n52.51\n70.43\nzul Latn\n103.20\n157.55\n113.26\n130.35\n98.06\nall\n122.10\n180.54\n129.55\n131.31\n101.67\nTable 21: Detailed results of NLL on PBC (Part VI).\n33\nPreprint. Under review.\nLang\nLLaMA 2\n7B\nmGPT\n13B\nBLOOM\n7B1\nXGLM\n7.5B\nMaLA-500\n1-shot\n2-shot\n3-shot\n4-shot\n5-shot\n6-shot\n7-shot\n8-shot\n9-shot\n10-shot\nace Latn\n44.12\n47.55\n50.00\n36.76\n34.31\n52.94\n60.29\n60.78\n65.69\n67.65\n64.22\n65.20\n68.63\n71.57\nacm Arab\n52.45\n65.69\n69.12\n58.33\n32.35\n53.43\n59.31\n63.73\n63.73\n67.16\n66.67\n69.12\n66.67\n66.67\nafr Latn\n68.14\n55.39\n53.92\n40.20\n41.18\n62.25\n65.69\n69.12\n71.08\n74.02\n73.53\n74.51\n76.96\n78.92\najp Arab\n47.55\n64.22\n68.63\n53.43\n33.33\n56.86\n59.80\n59.80\n65.20\n63.73\n63.24\n69.12\n68.14\n66.67\nals Latn\n41.67\n46.57\n45.59\n28.43\n27.94\n51.96\n63.73\n62.25\n69.12\n71.08\n69.61\n72.06\n75.98\n77.45\namh Ethi\n15.69\n18.63\n16.67\n13.24\n25.00\n36.76\n45.59\n51.47\n51.96\n53.92\n51.96\n53.43\n54.90\n53.92\napc Arab\n46.57\n65.69\n68.14\n53.43\n31.37\n55.88\n60.29\n65.69\n65.69\n67.16\n65.20\n68.63\n67.65\n72.06\narb Arab\n53.43\n63.24\n68.14\n57.35\n32.35\n54.90\n60.29\n63.73\n65.20\n68.14\n67.16\n69.12\n68.63\n70.59\nary Arab\n45.10\n57.84\n69.12\n50.49\n26.47\n52.45\n55.39\n56.37\n60.29\n59.80\n64.22\n63.73\n59.31\n64.71\narz Arab\n50.98\n64.22\n68.14\n56.86\n30.88\n52.45\n59.31\n60.78\n64.22\n66.18\n68.14\n69.12\n66.67\n69.12\nasm Beng\n17.16\n49.02\n61.27\n37.25\n31.37\n53.43\n58.82\n65.20\n67.65\n67.65\n68.14\n67.65\n67.65\n67.65\nast Latn\n69.12\n60.78\n69.12\n55.39\n34.31\n65.69\n70.10\n70.59\n74.02\n75.00\n75.98\n77.94\n79.90\n79.90\nayr Latn\n25.00\n26.96\n32.35\n19.61\n20.10\n29.41\n38.24\n38.73\n38.24\n43.14\n40.20\n43.14\n44.61\n42.16\nazb Arab\n25.49\n41.67\n32.84\n24.51\n25.98\n41.18\n45.59\n45.10\n46.57\n49.51\n50.00\n49.02\n50.49\n49.51\nazj Latn\n34.80\n64.22\n37.25\n32.84\n30.88\n57.84\n64.71\n68.63\n64.22\n72.55\n69.61\n70.59\n74.02\n72.55\nbak Cyrl\n38.73\n61.27\n32.35\n32.35\n34.80\n51.47\n60.29\n61.27\n69.12\n68.63\n68.14\n68.63\n73.53\n70.10\nbam Latn\n25.49\n24.51\n29.41\n20.10\n22.55\n25.98\n34.80\n42.16\n43.14\n44.12\n45.10\n42.16\n46.08\n44.12\nban Latn\n58.82\n51.47\n58.82\n43.14\n28.92\n55.39\n63.24\n65.69\n66.67\n72.06\n72.06\n72.55\n72.06\n71.57\nbel Cyrl\n47.55\n59.80\n28.92\n30.39\n40.69\n60.29\n63.24\n66.18\n67.65\n70.10\n72.55\n72.06\n72.55\n73.04\nbem Latn\n31.37\n28.92\n38.24\n25.49\n21.08\n34.80\n43.14\n48.04\n50.49\n50.49\n53.43\n53.43\n52.45\n53.92\nben Beng\n25.49\n61.27\n64.22\n52.45\n31.37\n54.90\n63.24\n62.25\n67.65\n70.10\n70.10\n69.12\n66.18\n68.63\nbjn Latn\n48.53\n51.96\n61.76\n42.65\n32.35\n62.75\n66.18\n68.14\n71.57\n75.98\n73.04\n72.55\n75.00\n77.45\nbod Tibt\n15.20\n12.75\n15.69\n15.69\n22.06\n34.80\n37.75\n37.75\n38.73\n39.71\n39.71\n39.71\n41.67\n44.12\nbos Latn\n65.20\n64.71\n45.59\n33.82\n37.75\n65.20\n72.06\n70.10\n71.57\n75.98\n75.00\n76.47\n76.96\n77.45\nbul Cyrl\n66.18\n63.24\n38.73\n52.94\n45.10\n62.25\n67.65\n67.16\n68.14\n75.00\n71.57\n72.06\n73.53\n75.00\ncat Latn\n71.08\n60.78\n66.67\n60.29\n34.31\n59.31\n68.14\n68.63\n71.57\n72.55\n69.12\n73.04\n76.96\n76.47\nceb Latn\n50.49\n50.98\n49.02\n39.71\n39.22\n60.29\n66.67\n66.67\n68.63\n73.04\n72.06\n71.57\n74.51\n74.02\nces Latn\n69.12\n62.75\n47.55\n40.69\n39.22\n62.25\n69.61\n70.59\n72.55\n76.47\n72.55\n74.02\n80.88\n76.96\ncjk Latn\n27.94\n30.39\n34.31\n26.47\n22.55\n30.88\n31.86\n32.84\n38.24\n38.24\n38.24\n35.78\n39.22\n42.65\nckb Arab\n19.61\n22.55\n23.04\n12.25\n28.92\n53.43\n60.29\n57.35\n65.20\n65.20\n62.75\n65.69\n65.69\n70.59\ncmn Hani\n73.04\n65.20\n67.65\n54.90\n39.71\n69.12\n74.02\n72.06\n76.47\n77.45\n76.47\n75.98\n79.41\n76.47\ncrh Latn\n38.24\n56.37\n40.20\n36.76\n29.41\n51.96\n62.25\n61.76\n64.22\n69.12\n64.22\n70.10\n69.12\n71.08\ncym Latn\n39.22\n28.43\n34.80\n21.57\n28.43\n55.39\n62.75\n63.73\n66.18\n72.06\n74.02\n72.06\n75.00\n77.45\ndan Latn\n69.12\n64.22\n55.39\n44.12\n38.24\n54.41\n63.24\n65.20\n70.10\n71.08\n72.06\n71.57\n73.53\n74.51\ndeu Latn\n74.02\n60.29\n61.27\n55.39\n41.18\n63.73\n68.63\n71.57\n69.12\n75.00\n75.49\n76.47\n77.45\n77.45\ndyu Latn\n28.43\n28.92\n32.35\n20.10\n21.08\n29.90\n38.73\n39.71\n46.57\n44.12\n41.67\n47.06\n44.61\n43.63\ndzo Tibt\n14.71\n10.29\n13.73\n12.75\n21.57\n30.39\n36.76\n37.25\n39.71\n36.76\n39.22\n37.75\n43.14\n39.22\nell Grek\n47.55\n63.73\n28.43\n60.29\n43.63\n62.75\n69.61\n66.67\n69.12\n69.61\n70.59\n73.04\n72.06\n72.06\neng Latn\n71.57\n59.80\n71.08\n67.65\n48.04\n63.24\n70.59\n69.12\n69.12\n74.02\n73.04\n74.51\n76.96\n75.98\nepo Latn\n52.94\n50.49\n52.94\n43.63\n27.94\n49.51\n66.18\n66.67\n68.63\n72.55\n73.53\n71.57\n74.51\n75.98\nest Latn\n48.04\n54.90\n41.18\n57.35\n29.41\n55.88\n62.75\n66.67\n70.10\n72.06\n70.10\n69.12\n71.57\n73.04\neus Latn\n36.27\n59.80\n64.22\n55.88\n27.94\n52.45\n61.76\n66.18\n66.18\n73.53\n73.53\n72.55\n73.53\n75.49\newe Latn\n23.53\n23.53\n29.90\n17.16\n23.53\n28.43\n38.24\n35.29\n41.18\n43.63\n38.73\n44.61\n39.71\n43.63\nfao Latn\n41.18\n43.63\n38.73\n29.90\n35.29\n52.94\n56.86\n57.84\n62.25\n61.27\n62.25\n61.76\n64.22\n69.12\nfij Latn\n27.45\n27.45\n36.27\n24.02\n24.51\n37.75\n48.53\n47.55\n53.92\n48.53\n50.00\n51.47\n51.47\n53.92\nfin Latn\n67.65\n63.24\n38.73\n56.86\n36.76\n61.76\n70.10\n70.10\n72.55\n74.51\n72.06\n73.53\n75.00\n75.49\nfon Latn\n25.49\n22.06\n31.37\n19.12\n23.53\n29.90\n30.88\n37.25\n35.78\n38.24\n39.71\n38.24\n37.25\n46.57\nfra Latn\n72.06\n64.71\n66.18\n59.80\n36.76\n58.82\n71.08\n67.65\n71.08\n74.51\n71.57\n74.02\n77.45\n77.45\nful Latn\n27.45\n31.37\n32.84\n24.02\n21.57\n31.86\n36.76\n40.69\n43.14\n45.10\n44.12\n47.06\n47.06\n46.08\nfur Latn\n58.82\n50.00\n55.88\n38.73\n35.78\n53.92\n55.88\n62.25\n66.67\n68.63\n68.14\n67.16\n75.00\n72.55\ngla Latn\n37.75\n24.51\n27.45\n17.16\n24.51\n50.98\n55.88\n57.84\n57.35\n59.80\n63.24\n61.27\n62.75\n65.20\ngle Latn\n39.71\n25.49\n25.00\n17.16\n27.94\n53.43\n57.84\n63.24\n64.22\n67.65\n64.71\n63.24\n62.75\n73.53\nglg Latn\n68.63\n62.25\n64.22\n55.39\n29.41\n66.67\n70.10\n71.08\n74.02\n76.96\n73.53\n76.47\n77.45\n80.39\ngrn Latn\n42.16\n47.06\n48.53\n33.82\n25.98\n52.45\n62.75\n64.71\n62.25\n67.16\n64.22\n65.69\n61.76\n69.61\nguj Gujr\n15.20\n09.31\n62.25\n12.75\n28.43\n50.98\n54.90\n60.29\n63.73\n63.73\n64.22\n65.69\n62.75\n67.16\nhat Latn\n41.67\n38.73\n42.16\n45.10\n34.80\n57.35\n63.73\n62.25\n65.20\n72.06\n69.61\n70.10\n73.53\n73.04\nhau Latn\n25.49\n28.43\n29.90\n20.59\n28.43\n47.06\n55.39\n57.84\n61.27\n65.69\n62.75\n65.69\n66.18\n65.20\nheb Hebr\n37.75\n63.24\n20.59\n11.76\n26.47\n41.67\n47.06\n51.47\n54.90\n51.96\n50.98\n54.90\n53.92\n54.41\nhin Deva\n44.61\n62.75\n62.75\n51.96\n33.82\n55.39\n60.78\n66.67\n65.20\n69.61\n70.59\n74.02\n73.04\n72.06\nhne Deva\n37.75\n58.82\n59.80\n49.02\n28.43\n55.39\n55.88\n65.20\n62.75\n68.63\n66.18\n65.69\n68.14\n71.08\nhrv Latn\n66.18\n65.20\n44.12\n36.27\n40.69\n63.73\n73.04\n71.08\n73.53\n76.47\n72.06\n74.51\n78.43\n78.43\nhun Latn\n71.08\n63.24\n41.67\n27.94\n30.88\n60.78\n67.16\n70.59\n68.63\n75.49\n73.53\n74.02\n73.04\n76.47\nhye Armn\n20.59\n17.16\n13.73\n12.75\n32.84\n58.82\n59.80\n67.16\n65.20\n69.61\n68.14\n69.12\n69.12\n72.55\nibo Latn\n24.02\n26.47\n38.24\n19.12\n30.39\n51.47\n57.35\n63.73\n67.16\n69.12\n68.14\n69.12\n68.63\n72.06\nilo Latn\n45.10\n45.59\n48.04\n32.35\n27.45\n54.90\n61.76\n61.76\n68.14\n68.14\n70.10\n73.04\n73.53\n70.10\nind Latn\n74.02\n62.75\n70.10\n54.90\n40.69\n62.75\n68.63\n71.57\n70.59\n75.49\n75.49\n76.96\n80.39\n77.94\nisl Latn\n35.29\n36.76\n28.92\n24.51\n38.73\n55.88\n60.78\n58.33\n60.29\n64.71\n63.73\n63.24\n62.75\n65.20\nita Latn\n69.61\n62.25\n62.75\n57.84\n40.20\n64.22\n70.59\n70.59\n74.51\n77.94\n75.98\n76.96\n80.39\n76.96\njav Latn\n50.49\n52.94\n55.39\n38.24\n31.86\n53.43\n60.78\n64.22\n65.20\n72.55\n69.12\n73.04\n68.63\n73.04\njpn Jpan\n73.53\n60.29\n63.24\n55.88\n38.73\n67.16\n72.06\n75.49\n78.92\n79.41\n80.39\n78.92\n81.86\n81.37\nkab Latn\n16.18\n16.67\n20.10\n12.25\n20.59\n24.02\n22.55\n30.39\n31.86\n34.80\n28.43\n33.33\n32.35\n34.31\nkac Latn\n25.98\n24.51\n28.43\n20.59\n20.10\n24.02\n29.90\n35.78\n35.78\n43.14\n37.75\n37.25\n43.63\n39.71\nkam Latn\n26.96\n34.31\n34.80\n26.47\n22.06\n36.76\n38.73\n37.75\n40.69\n41.67\n46.57\n41.67\n42.16\n42.16\nkan Knda\n17.16\n11.27\n61.27\n11.27\n25.49\n50.98\n57.35\n60.29\n61.27\n65.20\n63.24\n64.22\n65.69\n67.16\nkat Geor\n29.41\n61.27\n18.14\n14.71\n32.84\n56.86\n60.78\n62.25\n65.20\n67.65\n70.59\n70.59\n70.10\n74.51\nkaz Cyrl\n37.75\n62.75\n29.90\n28.43\n34.31\n53.43\n57.35\n62.25\n65.69\n67.16\n65.20\n65.69\n69.12\n67.65\nkbp Latn\n24.51\n22.06\n30.39\n16.18\n21.08\n28.43\n36.76\n38.73\n40.69\n39.22\n40.69\n39.71\n38.73\n40.20\nkea Latn\n53.43\n51.96\n56.86\n39.71\n32.84\n56.86\n63.73\n65.20\n67.16\n69.61\n69.12\n71.57\n71.57\n72.06\nkhm Khmr\n27.45\n11.27\n25.49\n15.20\n39.22\n61.76\n67.16\n67.65\n68.63\n72.06\n73.53\n75.00\n76.47\n76.47\nkik Latn\n29.41\n32.84\n38.73\n26.96\n21.57\n37.75\n49.51\n50.49\n50.49\n56.86\n56.37\n56.37\n52.94\n56.86\nkin Latn\n26.47\n32.35\n50.49\n24.51\n27.45\n40.69\n49.02\n52.94\n57.84\n58.33\n60.78\n56.86\n59.31\n59.80\nkir Cyrl\n35.78\n60.78\n34.80\n27.45\n29.90\n45.59\n58.33\n60.78\n60.29\n65.20\n60.29\n64.71\n66.18\n66.18\nkmb Latn\n26.47\n28.43\n33.82\n25.00\n21.08\n31.86\n35.29\n39.71\n38.24\n41.18\n41.67\n37.25\n41.67\n44.61\nkmr Latn\n29.41\n33.82\n33.33\n21.57\n25.98\n37.75\n47.06\n47.55\n52.45\n52.45\n54.90\n54.41\n58.82\n61.76\nkon Latn\n33.33\n33.82\n40.69\n32.35\n22.06\n39.71\n46.57\n51.96\n53.92\n64.71\n64.22\n60.78\n64.22\n65.20\nkor Hang\n67.65\n63.24\n43.14\n56.37\n45.10\n63.24\n67.65\n69.12\n71.57\n73.04\n70.59\n75.98\n76.96\n76.47\nlao Laoo\n24.02\n14.22\n26.47\n16.67\n39.71\n55.39\n62.25\n63.73\n68.63\n70.59\n70.10\n68.63\n70.59\n70.59\nlij Latn\n55.88\n53.43\n56.37\n44.61\n37.25\n58.82\n67.65\n66.67\n69.61\n71.57\n71.08\n74.02\n76.47\n74.02\nTable 22: Detailed results on SIB200 (Part I). For previous LLMs, 3-shot results are presented.\n34\nPreprint. Under review.\nLang\nLLaMA 2\n7B\nmGPT\n13B\nBLOOM\n7B1\nXGLM\n7.5B\nMaLA-500\n1-shot\n2-shot\n3-shot\n4-shot\n5-shot\n6-shot\n7-shot\n8-shot\n9-shot\n10-shot\nlim Latn\n60.78\n50.00\n50.00\n33.82\n32.84\n56.37\n60.78\n63.24\n66.67\n67.16\n69.12\n72.55\n70.59\n72.06\nlin Latn\n36.76\n40.20\n43.14\n34.31\n23.04\n38.24\n47.06\n53.92\n57.84\n61.27\n56.86\n60.29\n62.75\n65.69\nlit Latn\n40.20\n60.29\n41.18\n30.39\n32.35\n55.39\n62.75\n65.20\n64.71\n70.59\n68.14\n66.67\n69.61\n73.04\nlmo Latn\n57.84\n50.98\n55.88\n41.67\n34.80\n59.31\n65.69\n66.18\n70.10\n71.57\n70.59\n70.59\n75.00\n75.49\nltz Latn\n55.88\n47.06\n52.94\n39.22\n39.22\n56.37\n65.20\n61.27\n70.59\n68.14\n71.08\n70.59\n71.08\n74.02\nlua Latn\n32.35\n33.33\n39.22\n28.43\n20.10\n33.82\n40.20\n42.65\n49.02\n51.96\n50.00\n50.00\n49.51\n50.00\nlug Latn\n27.94\n25.00\n33.82\n19.61\n22.06\n35.78\n40.20\n43.63\n48.04\n51.47\n47.06\n43.63\n49.02\n49.02\nluo Latn\n28.43\n28.43\n32.84\n25.49\n21.57\n31.37\n37.25\n42.65\n48.04\n49.51\n46.57\n51.47\n49.51\n51.47\nlus Latn\n43.63\n42.16\n49.02\n31.37\n25.49\n45.59\n51.96\n53.92\n52.45\n54.90\n57.84\n60.29\n58.33\n60.29\nlvs Latn\n43.14\n67.16\n43.63\n29.41\n31.37\n57.84\n65.20\n63.24\n68.14\n72.55\n67.65\n69.61\n71.08\n72.55\nmai Deva\n40.69\n59.31\n60.29\n51.47\n33.33\n57.84\n61.76\n66.67\n67.65\n69.12\n69.12\n70.10\n71.57\n69.12\nmal Mlym\n20.10\n60.29\n64.71\n13.24\n25.98\n52.45\n59.31\n60.29\n62.75\n62.25\n65.69\n63.24\n63.73\n68.14\nmar Deva\n29.90\n56.86\n63.73\n37.75\n36.27\n51.96\n57.35\n64.22\n63.73\n63.73\n63.73\n66.67\n66.18\n68.14\nmin Latn\n48.04\n55.39\n59.80\n39.71\n31.37\n57.35\n69.12\n68.14\n68.63\n77.94\n72.06\n75.98\n75.49\n77.45\nmkd Cyrl\n60.78\n52.45\n32.84\n44.12\n44.12\n66.18\n68.63\n69.12\n68.63\n73.04\n73.04\n72.55\n73.04\n76.96\nmlt Latn\n49.51\n45.10\n46.08\n29.90\n35.78\n64.71\n67.16\n68.14\n67.16\n77.45\n75.49\n76.96\n77.45\n76.96\nmon Cyrl\n23.53\n54.90\n20.10\n18.63\n38.24\n50.00\n56.86\n55.88\n63.24\n64.22\n63.24\n63.24\n65.20\n67.16\nmos Latn\n25.49\n23.53\n29.90\n20.59\n20.59\n27.94\n36.76\n37.75\n37.75\n40.69\n41.67\n45.10\n41.67\n45.10\nmri Latn\n30.39\n24.02\n30.88\n17.65\n28.43\n44.12\n49.02\n51.47\n51.47\n57.84\n55.88\n58.82\n56.37\n58.33\nmya Mymr\n19.12\n60.29\n19.61\n60.29\n23.53\n38.73\n43.14\n53.43\n53.43\n50.98\n52.45\n54.90\n51.96\n54.90\nnld Latn\n70.10\n59.80\n55.88\n46.08\n45.59\n64.71\n69.12\n68.63\n73.04\n73.53\n75.49\n74.02\n79.41\n80.88\nnno Latn\n64.71\n61.76\n52.45\n45.59\n35.29\n52.94\n64.22\n62.75\n66.18\n68.63\n68.63\n70.10\n69.12\n73.04\nnpi Deva\n39.22\n51.96\n64.71\n40.69\n33.82\n57.84\n61.76\n68.14\n67.65\n67.65\n68.63\n70.10\n68.63\n75.49\nnso Latn\n27.94\n30.88\n33.33\n22.55\n21.08\n33.82\n43.14\n46.08\n49.02\n52.94\n51.96\n53.92\n54.90\n53.92\nnya Latn\n32.35\n34.31\n40.69\n27.94\n23.04\n35.29\n45.59\n49.02\n50.98\n51.47\n52.94\n53.92\n52.94\n58.33\noci Latn\n68.63\n56.37\n65.69\n48.53\n34.31\n60.29\n69.12\n65.20\n67.65\n73.04\n73.53\n71.57\n75.49\n76.47\norm Latn\n17.16\n18.14\n22.06\n16.67\n20.10\n30.39\n35.29\n41.18\n41.67\n47.55\n41.67\n44.12\n43.14\n51.47\nory Orya\n13.24\n13.73\n64.22\n11.76\n24.51\n45.10\n52.45\n57.84\n53.92\n61.27\n57.84\n56.86\n60.78\n60.78\npag Latn\n52.45\n49.51\n53.92\n40.20\n31.86\n54.90\n62.75\n60.78\n67.65\n64.71\n70.10\n70.10\n69.12\n69.61\npan Guru\n14.22\n11.27\n62.25\n11.76\n33.82\n54.90\n58.82\n63.73\n64.22\n67.65\n67.16\n66.67\n68.63\n67.16\npap Latn\n55.39\n50.00\n52.94\n38.24\n30.39\n56.86\n64.71\n66.67\n69.61\n74.51\n69.12\n73.53\n70.59\n75.49\npes Arab\n47.06\n58.82\n52.94\n32.84\n39.22\n61.27\n71.08\n63.73\n70.59\n72.55\n72.55\n73.53\n76.47\n76.47\nplt Latn\n28.43\n32.84\n37.25\n21.57\n29.41\n51.96\n58.82\n57.84\n59.31\n60.78\n60.29\n60.29\n64.22\n60.29\npol Latn\n74.51\n60.78\n47.06\n32.84\n36.76\n61.76\n68.63\n69.12\n71.08\n75.00\n74.02\n74.02\n77.45\n75.98\npor Latn\n70.10\n61.76\n65.20\n59.31\n36.76\n64.71\n72.06\n70.10\n74.51\n75.00\n76.96\n75.49\n78.43\n82.84\nprs Arab\n50.49\n55.39\n49.51\n33.33\n37.25\n60.78\n64.22\n67.16\n69.12\n72.55\n72.55\n73.53\n72.55\n75.49\npus Arab\n30.39\n34.80\n38.73\n21.08\n30.39\n47.06\n50.98\n52.45\n54.41\n53.92\n53.92\n55.88\n55.88\n57.84\nquy Latn\n32.84\n35.29\n40.69\n35.29\n22.06\n36.27\n44.12\n45.59\n49.02\n52.45\n49.51\n49.02\n50.98\n50.98\nron Latn\n69.12\n61.76\n57.84\n42.65\n41.18\n61.27\n70.10\n65.20\n70.10\n74.51\n73.53\n75.00\n78.92\n78.43\nrun Latn\n25.49\n27.94\n44.12\n25.49\n23.53\n37.25\n46.57\n50.49\n51.96\n59.31\n51.96\n56.37\n57.84\n60.29\nrus Cyrl\n71.57\n63.24\n53.43\n60.29\n38.73\n64.22\n65.20\n69.12\n72.06\n75.98\n75.00\n76.47\n75.49\n78.92\nsag Latn\n29.90\n27.94\n31.37\n21.08\n20.59\n30.88\n43.63\n47.06\n48.53\n55.88\n52.45\n54.41\n55.88\n58.82\nsan Deva\n27.94\n47.55\n54.90\n42.65\n24.51\n48.04\n60.29\n57.84\n62.25\n66.67\n65.20\n61.76\n66.18\n65.20\nscn Latn\n51.96\n50.00\n53.43\n40.69\n37.25\n63.73\n73.04\n70.59\n74.02\n77.45\n75.49\n75.00\n80.39\n76.47\nsin Sinh\n15.20\n10.78\n20.10\n12.75\n29.90\n56.37\n60.29\n65.20\n66.18\n68.14\n64.71\n66.67\n63.73\n67.16\nslk Latn\n68.14\n60.29\n47.55\n39.71\n34.31\n58.33\n68.63\n66.67\n70.59\n75.00\n70.59\n71.57\n74.51\n75.00\nslv Latn\n68.14\n60.78\n44.12\n32.84\n38.73\n63.24\n68.14\n68.14\n70.59\n73.53\n73.53\n74.51\n78.43\n76.47\nsmo Latn\n30.39\n25.00\n31.86\n18.14\n29.41\n52.45\n60.29\n62.25\n62.25\n65.69\n67.16\n65.20\n66.18\n69.61\nsna Latn\n28.43\n29.41\n36.27\n23.53\n24.51\n39.71\n44.61\n45.59\n44.61\n49.51\n45.59\n47.55\n47.06\n50.00\nsnd Arab\n27.94\n37.25\n39.22\n23.53\n27.94\n42.65\n47.06\n50.49\n52.45\n54.41\n54.41\n52.94\n55.88\n56.86\nsom Latn\n23.53\n25.49\n27.94\n17.16\n22.06\n36.27\n44.61\n47.55\n51.47\n52.94\n52.94\n53.92\n54.41\n55.39\nsot Latn\n29.41\n28.43\n33.82\n18.63\n22.55\n36.76\n43.14\n47.06\n50.49\n51.96\n52.45\n55.39\n54.41\n56.86\nspa Latn\n72.55\n58.33\n67.65\n56.37\n35.29\n64.22\n69.61\n72.06\n74.51\n74.02\n72.06\n76.47\n78.43\n78.43\nsrd Latn\n53.92\n52.45\n50.98\n37.25\n31.37\n60.29\n66.18\n68.63\n75.98\n74.02\n77.94\n77.45\n79.41\n79.41\nsrp Cyrl\n63.73\n55.39\n33.33\n39.22\n45.59\n65.20\n70.59\n69.61\n73.04\n76.47\n74.02\n75.00\n77.94\n79.41\nssw Latn\n29.41\n25.00\n31.37\n21.57\n24.02\n44.12\n46.57\n50.00\n52.94\n51.96\n53.92\n56.86\n53.92\n60.78\nsun Latn\n55.39\n59.31\n63.73\n44.61\n37.25\n60.29\n68.63\n70.10\n71.08\n73.53\n72.55\n73.53\n75.00\n75.98\nswe Latn\n71.08\n61.27\n52.94\n48.04\n33.82\n53.43\n60.29\n64.71\n64.71\n69.12\n70.10\n69.61\n72.55\n70.59\nswh Latn\n32.35\n63.24\n61.27\n56.86\n29.41\n50.49\n59.31\n58.82\n62.75\n60.29\n62.25\n68.63\n66.67\n66.67\nszl Latn\n56.86\n50.49\n45.59\n29.41\n30.88\n51.47\n59.80\n63.73\n64.71\n67.16\n69.61\n68.63\n71.08\n69.12\ntam Taml\n20.59\n63.24\n67.16\n58.82\n30.88\n50.49\n55.88\n62.75\n63.24\n63.73\n65.20\n69.61\n66.67\n68.63\ntat Cyrl\n37.75\n60.29\n35.29\n28.92\n33.33\n54.90\n64.22\n64.71\n65.69\n74.51\n70.10\n71.57\n71.57\n73.53\ntel Telu\n18.14\n60.78\n61.27\n59.80\n25.98\n50.00\n52.45\n59.80\n58.82\n63.73\n60.78\n60.29\n63.73\n61.76\ntgk Cyrl\n26.96\n57.84\n23.53\n17.16\n36.76\n54.90\n60.29\n60.78\n61.27\n69.12\n64.71\n66.18\n68.14\n70.59\ntgl Latn\n55.88\n58.33\n49.02\n40.20\n43.14\n64.22\n69.61\n64.71\n70.10\n75.49\n74.51\n77.45\n78.43\n77.45\ntha Thai\n44.61\n60.78\n23.53\n57.35\n41.18\n63.24\n67.16\n68.63\n70.10\n72.06\n70.59\n70.10\n72.06\n75.49\ntir Ethi\n13.24\n16.18\n16.18\n13.73\n21.57\n34.80\n39.22\n41.67\n47.06\n46.08\n45.59\n47.06\n45.59\n47.55\ntpi Latn\n63.24\n46.57\n56.86\n33.33\n31.86\n58.82\n65.69\n68.14\n70.10\n72.06\n74.51\n73.53\n75.98\n74.51\ntsn Latn\n28.92\n29.90\n32.35\n24.51\n24.51\n40.20\n44.12\n47.06\n47.06\n53.43\n51.96\n50.00\n50.49\n54.41\ntso Latn\n30.88\n31.37\n36.76\n28.92\n22.55\n35.29\n41.18\n45.10\n46.08\n48.04\n43.14\n43.14\n45.59\n49.02\ntuk Latn\n34.31\n46.08\n39.22\n27.45\n24.02\n45.59\n53.43\n58.82\n57.84\n63.73\n63.73\n66.18\n65.69\n66.18\ntum Latn\n26.96\n34.31\n33.82\n27.94\n21.57\n39.22\n43.14\n45.59\n46.08\n47.55\n44.61\n49.02\n49.51\n49.51\ntur Latn\n52.94\n62.75\n40.20\n52.94\n36.76\n60.78\n68.63\n70.10\n72.06\n74.02\n75.00\n76.47\n75.98\n76.96\nuig Arab\n18.63\n18.14\n20.10\n11.27\n21.08\n33.33\n36.76\n39.71\n43.14\n44.61\n43.14\n48.53\n47.55\n48.04\nukr Cyrl\n71.57\n63.73\n41.18\n43.63\n39.71\n60.29\n65.69\n66.18\n69.12\n71.08\n75.00\n73.53\n72.55\n75.00\numb Latn\n25.00\n26.47\n29.90\n23.04\n21.57\n30.88\n32.84\n36.76\n35.78\n40.20\n38.24\n34.80\n36.76\n35.29\nurd Arab\n38.73\n53.43\n63.24\n54.41\n36.27\n55.39\n62.75\n64.22\n64.22\n68.63\n65.20\n68.63\n67.16\n67.16\nuzb Latn\n30.39\n62.75\n35.78\n23.53\n22.06\n50.49\n56.37\n57.84\n63.24\n72.06\n63.73\n69.12\n72.55\n71.57\nvec Latn\n65.69\n59.80\n56.86\n52.45\n39.22\n62.25\n66.18\n69.61\n70.10\n69.12\n74.51\n75.98\n75.00\n76.47\nvie Latn\n67.65\n63.24\n67.16\n60.78\n39.71\n60.78\n67.16\n68.63\n74.51\n75.98\n76.47\n75.00\n78.43\n79.90\nwar Latn\n51.47\n51.47\n51.47\n37.25\n37.75\n61.27\n65.69\n65.20\n69.61\n73.04\n71.57\n71.57\n74.02\n74.02\nwol Latn\n32.35\n34.80\n43.14\n25.49\n23.53\n36.76\n42.16\n45.59\n48.53\n53.43\n47.55\n52.94\n54.90\n53.43\nxho Latn\n30.39\n29.90\n38.24\n22.55\n25.98\n46.57\n51.96\n56.37\n58.33\n60.78\n61.76\n60.29\n62.75\n64.22\nyid Hebr\n23.04\n22.06\n16.18\n12.25\n24.02\n34.80\n39.22\n39.71\n40.20\n46.57\n41.18\n40.69\n44.61\n45.10\nyor Latn\n21.57\n29.41\n47.55\n21.57\n26.47\n32.35\n41.18\n39.22\n41.67\n48.04\n42.16\n43.14\n44.61\n43.14\nyue Hani\n75.00\n64.71\n67.16\n55.88\n40.69\n69.12\n71.57\n76.47\n76.96\n81.37\n77.94\n79.41\n81.86\n79.41\nzsm Latn\n65.69\n61.27\n64.71\n50.00\n36.76\n60.29\n68.14\n69.12\n67.65\n73.53\n73.53\n76.96\n77.45\n75.00\nzul Latn\n25.00\n25.49\n35.29\n15.20\n25.49\n51.47\n49.02\n54.41\n56.37\n57.84\n60.29\n61.76\n59.31\n62.75\nall\n42.08\n45.34\n44.63\n34.36\n30.88\n50.71\n57.02\n58.95\n61.20\n64.04\n63.15\n64.13\n65.19\n66.32\nTable 23: Detailed results on SIB200 (Part II). For previous LLMs, 3-shot results are presented.\n35\nPreprint. Under review.\nLang\nLLaMA 2-7B\nmGPT-13B\nBLOOM-7B1\nXGLM-7.5B\nMaLA-500\nace Latn\n46.85\n47.75\n49.55\n41.44\n48.65\nach Latn\n45.05\n37.84\n41.44\n40.54\n36.04\nacr Latn\n47.75\n51.35\n50.45\n47.75\n50.45\nafr Latn\n54.05\n38.74\n51.35\n49.55\n55.86\nagw Latn\n48.65\n45.05\n41.44\n42.34\n49.55\nahk Latn\n43.24\n36.04\n36.04\n35.14\n45.05\naka Latn\n42.34\n32.43\n38.74\n42.34\n54.95\naln Latn\n34.23\n35.14\n36.94\n35.14\n44.14\nals Latn\n38.74\n36.94\n42.34\n42.34\n47.75\nalt Cyrl\n44.14\n44.14\n45.05\n51.35\n51.35\nalz Latn\n36.94\n35.14\n31.53\n28.83\n37.84\naoj Latn\n50.93\n37.96\n45.37\n46.3\n49.07\narb Arab\n43.24\n45.05\n49.55\n44.14\n50.45\narn Latn\n38.74\n42.34\n34.23\n36.04\n43.24\nary Arab\n32.43\n33.33\n38.74\n32.43\n44.14\narz Arab\n31.53\n39.64\n45.05\n36.94\n46.85\nasm Beng\n45.95\n42.34\n54.95\n40.54\n54.05\nayr Latn\n47.75\n37.84\n44.14\n44.14\n54.05\nazb Arab\n39.64\n40.54\n47.75\n45.05\n47.75\naze Latn\n45.05\n46.85\n45.05\n43.24\n49.55\nbak Cyrl\n45.05\n52.25\n49.55\n56.76\n56.76\nbam Latn\n42.34\n37.84\n49.55\n39.64\n47.75\nban Latn\n36.04\n41.44\n34.23\n34.23\n42.34\nbar Latn\n49.55\n46.85\n44.14\n48.65\n53.15\nbba Latn\n45.05\n32.43\n45.95\n46.85\n46.85\nbci Latn\n36.94\n35.14\n36.94\n33.33\n44.14\nbcl Latn\n42.34\n48.65\n39.64\n39.64\n54.95\nbel Cyrl\n47.75\n45.95\n48.65\n43.24\n57.66\nbem Latn\n47.75\n37.84\n42.34\n41.44\n51.35\nben Beng\n40.54\n41.44\n52.25\n51.35\n47.75\nbhw Latn\n37.84\n43.24\n41.44\n46.85\n47.75\nbim Latn\n38.74\n39.64\n33.33\n36.94\n45.05\nbis Latn\n44.14\n49.55\n44.14\n39.64\n48.65\nbqc Latn\n39.64\n36.04\n34.23\n33.33\n40.54\nbre Latn\n39.64\n36.04\n35.14\n36.04\n40.54\nbtx Latn\n49.55\n36.94\n42.34\n41.44\n43.24\nbul Cyrl\n45.05\n42.34\n48.65\n45.05\n54.95\nbum Latn\n42.34\n39.64\n37.84\n37.84\n44.14\nbzj Latn\n53.15\n46.85\n47.75\n50.45\n52.25\ncab Latn\n39.64\n38.74\n37.84\n36.94\n36.04\ncac Latn\n43.24\n37.84\n40.54\n38.74\n45.05\ncak Latn\n45.95\n35.14\n44.14\n40.54\n50.45\ncaq Latn\n39.64\n38.74\n38.74\n44.14\n37.84\ncat Latn\n52.25\n45.05\n46.85\n48.65\n52.25\ncbk Latn\n54.05\n40.54\n56.76\n54.05\n55.86\ncce Latn\n49.55\n45.05\n50.45\n48.65\n48.65\nceb Latn\n44.14\n42.34\n48.65\n45.05\n51.35\nces Latn\n44.14\n43.24\n45.05\n46.85\n51.35\ncfm Latn\n49.55\n41.44\n49.55\n53.15\n48.65\nche Cyrl\n37.84\n33.33\n36.94\n37.84\n38.74\nchk Latn\n45.05\n41.44\n41.44\n36.04\n45.95\nchv Cyrl\n43.24\n45.05\n45.05\n49.55\n58.56\nckb Arab\n44.14\n36.94\n45.05\n42.34\n51.35\ncmn Hani\n48.65\n45.05\n53.15\n48.65\n53.15\ncnh Latn\n46.85\n46.85\n46.85\n49.55\n46.85\ncrh Cyrl\n49.55\n40.54\n47.75\n54.95\n54.05\ncrs Latn\n52.25\n44.14\n49.55\n55.86\n59.46\ncsy Latn\n47.75\n41.44\n54.95\n53.15\n45.95\nctd Latn\n50.45\n48.65\n56.76\n53.15\n56.76\nctu Latn\n41.44\n35.14\n38.74\n40.54\n43.24\ncuk Latn\n42.34\n42.34\n38.74\n39.64\n37.84\ncym Latn\n39.64\n38.74\n39.64\n43.24\n41.44\ndan Latn\n53.15\n41.44\n39.64\n38.74\n54.95\ndeu Latn\n45.05\n36.04\n37.84\n38.74\n43.24\ndjk Latn\n42.34\n35.14\n42.34\n46.85\n40.54\ndln Latn\n48.65\n40.54\n51.35\n54.05\n47.75\nTable 24: Detailed results on Taxi1500 (Part I). 3-shot results are presented.\n36\nPreprint. Under review.\nLang\nLLaMA 2-7B\nmGPT-13B\nBLOOM-7B1\nXGLM-7.5B\nMaLA-500\ndtp Latn\n39.64\n35.14\n42.34\n46.85\n50.45\ndyu Latn\n41.44\n39.64\n42.34\n38.74\n46.85\ndzo Tibt\n45.05\n40.54\n41.44\n45.05\n45.05\nefi Latn\n39.64\n36.04\n38.74\n41.44\n45.95\nell Grek\n49.55\n45.95\n49.55\n48.65\n51.35\neng Latn\n55.86\n42.34\n58.56\n54.05\n59.46\nenm Latn\n50.45\n41.44\n56.76\n50.45\n54.95\nepo Latn\n49.55\n40.54\n47.75\n42.34\n49.55\nest Latn\n46.85\n42.34\n38.74\n52.25\n46.85\neus Latn\n38.74\n36.04\n36.94\n39.64\n39.64\newe Latn\n51.35\n43.24\n50.45\n46.85\n45.05\nfao Latn\n53.15\n44.14\n52.25\n53.15\n58.56\nfas Arab\n49.55\n50.45\n57.66\n51.35\n55.86\nfij Latn\n48.65\n43.24\n41.44\n43.24\n53.15\nfil Latn\n48.65\n41.44\n46.85\n51.35\n51.35\nfin Latn\n47.75\n45.05\n41.44\n45.95\n54.95\nfon Latn\n38.74\n35.14\n37.84\n40.54\n45.05\nfra Latn\n60.36\n51.35\n62.16\n52.25\n59.46\nfry Latn\n37.84\n33.33\n36.04\n27.03\n46.85\ngaa Latn\n41.44\n33.33\n37.84\n35.14\n40.54\ngil Latn\n36.7\n31.19\n41.28\n32.11\n41.28\ngiz Latn\n46.85\n44.14\n43.24\n38.74\n45.05\ngkn Latn\n38.74\n34.23\n34.23\n36.94\n41.44\ngkp Latn\n30.63\n33.33\n41.44\n29.73\n48.65\ngla Latn\n33.33\n39.64\n44.14\n45.05\n49.55\ngle Latn\n33.33\n35.14\n36.04\n34.23\n39.64\nglv Latn\n43.24\n41.44\n37.84\n38.74\n42.34\ngom Latn\n34.23\n31.53\n33.33\n40.54\n42.34\ngor Latn\n43.24\n34.23\n43.24\n40.54\n46.85\nguc Latn\n44.14\n36.04\n37.84\n41.44\n45.05\ngug Latn\n45.05\n44.14\n42.34\n41.44\n50.45\nguj Gujr\n45.95\n37.84\n52.25\n44.14\n56.76\ngur Latn\n45.95\n45.95\n44.14\n47.75\n48.65\nguw Latn\n45.05\n37.84\n47.75\n46.85\n48.65\ngya Latn\n37.84\n37.84\n41.44\n34.23\n42.34\ngym Latn\n41.44\n39.64\n39.64\n43.24\n50.45\nhat Latn\n50.45\n43.24\n44.14\n41.44\n56.76\nhau Latn\n44.14\n37.84\n41.44\n44.14\n48.65\nhaw Latn\n45.95\n39.64\n38.74\n34.23\n49.55\nheb Hebr\n38.74\n35.14\n34.23\n36.94\n44.14\nhif Latn\n42.34\n43.24\n49.55\n47.75\n48.65\nhil Latn\n49.55\n41.44\n40.54\n36.94\n54.95\nhin Deva\n51.35\n50.45\n49.55\n46.85\n56.76\nhmo Latn\n46.85\n45.05\n46.85\n45.05\n53.15\nhne Deva\n55.86\n54.05\n54.05\n58.56\n58.56\nhnj Latn\n48.65\n45.05\n53.15\n51.35\n60.36\nhra Latn\n49.55\n41.44\n43.24\n46.85\n45.95\nhrv Latn\n55.86\n51.35\n52.25\n54.95\n61.26\nhui Latn\n51.35\n40.54\n41.44\n45.05\n46.85\nhun Latn\n46.85\n44.14\n41.44\n43.24\n48.65\nhus Latn\n32.43\n32.43\n34.23\n37.84\n42.34\nhye Armn\n45.95\n40.54\n45.95\n49.55\n61.26\niba Latn\n49.55\n46.85\n51.35\n48.65\n55.86\nibo Latn\n38.74\n33.33\n43.24\n38.74\n44.14\nifa Latn\n36.04\n30.63\n35.14\n38.74\n43.24\nifb Latn\n34.23\n35.14\n39.64\n34.23\n53.15\nikk Latn\n43.24\n36.94\n39.64\n39.64\n43.24\nilo Latn\n39.64\n36.04\n41.44\n37.84\n43.24\nind Latn\n49.55\n50.45\n53.15\n53.15\n54.95\nisl Latn\n48.65\n44.14\n43.24\n48.65\n54.05\nita Latn\n50.45\n49.55\n56.76\n58.56\n54.95\nium Latn\n45.95\n44.14\n49.55\n50.45\n45.05\nixl Latn\n42.34\n39.64\n41.44\n43.24\n40.54\nizz Latn\n38.74\n47.75\n39.64\n42.34\n54.95\njam Latn\n41.44\n43.24\n53.15\n50.45\n61.26\njav Latn\n41.44\n47.75\n44.14\n37.84\n45.95\nTable 25: Detailed results on Taxi1500 (Part II). 3-shot results are presented.\n37\nPreprint. Under review.\nLang\nLLaMA 2-7B\nmGPT-13B\nBLOOM-7B1\nXGLM-7.5B\nMaLA-500\njpn Jpan\n46.85\n46.85\n47.75\n50.45\n51.35\nkaa Latn\n43.24\n53.15\n47.75\n51.35\n54.05\nkab Latn\n27.93\n36.04\n30.63\n34.23\n35.14\nkac Latn\n44.14\n34.23\n43.24\n42.34\n52.25\nkal Latn\n41.44\n37.84\n36.04\n35.14\n40.54\nkan Knda\n48.65\n37.84\n52.25\n45.95\n54.05\nkat Geor\n41.44\n41.44\n42.34\n46.85\n48.65\nkaz Cyrl\n49.55\n45.05\n51.35\n53.15\n55.86\nkbp Latn\n40.54\n35.14\n36.94\n31.53\n47.75\nkek Latn\n45.95\n42.34\n45.05\n44.14\n51.35\nkhm Khmr\n52.25\n38.74\n48.65\n49.55\n64.86\nkia Latn\n36.94\n36.04\n40.54\n41.44\n48.65\nkik Latn\n45.05\n43.24\n45.05\n44.14\n50.45\nkin Latn\n42.34\n37.84\n41.44\n38.74\n50.45\nkir Cyrl\n51.35\n46.85\n47.75\n63.06\n64.86\nkjb Latn\n48.65\n46.85\n44.14\n44.14\n48.65\nkjh Cyrl\n44.14\n41.44\n45.05\n41.44\n45.95\nkmm Latn\n45.95\n45.05\n47.75\n51.35\n45.95\nkmr Cyrl\n39.64\n35.14\n45.05\n42.34\n43.24\nknv Latn\n44.55\n44.55\n45.45\n42.73\n44.55\nkor Hang\n48.65\n48.65\n49.55\n51.35\n62.16\nkpg Latn\n44.14\n52.25\n51.35\n42.34\n54.95\nkrc Cyrl\n45.95\n36.04\n48.65\n48.65\n53.15\nkri Latn\n49.55\n48.65\n49.55\n51.35\n54.95\nksd Latn\n36.94\n33.33\n40.54\n33.33\n49.55\nkss Latn\n32.43\n28.83\n34.23\n29.73\n47.75\nksw Mymr\n44.14\n45.95\n42.34\n37.84\n52.25\nkua Latn\n41.44\n42.34\n36.94\n35.14\n40.54\nlam Latn\n43.24\n36.94\n45.95\n43.24\n40.54\nlao Laoo\n45.05\n39.64\n46.85\n50.45\n50.45\nlat Latn\n53.15\n41.44\n53.15\n56.76\n57.66\nlav Latn\n39.64\n33.33\n36.04\n39.64\n45.05\nldi Latn\n35.14\n32.43\n36.94\n34.23\n36.04\nleh Latn\n47.75\n37.84\n33.33\n32.43\n41.44\nlhu Latn\n27.93\n34.23\n34.23\n37.84\n42.34\nlin Latn\n47.75\n37.84\n39.64\n39.64\n48.65\nlit Latn\n42.34\n40.54\n44.14\n48.65\n49.55\nloz Latn\n45.95\n42.34\n36.04\n44.14\n40.54\nltz Latn\n46.85\n45.95\n47.75\n41.44\n49.55\nlug Latn\n40.54\n32.43\n39.64\n38.74\n45.95\nluo Latn\n40.54\n36.94\n34.23\n38.74\n40.54\nlus Latn\n39.64\n40.54\n42.34\n41.44\n50.45\nlzh Hani\n54.95\n48.65\n54.05\n43.24\n56.76\nmad Latn\n47.75\n52.25\n47.75\n47.75\n53.15\nmah Latn\n43.24\n36.04\n42.34\n45.95\n45.05\nmai Deva\n45.05\n41.44\n49.55\n54.05\n51.35\nmam Latn\n43.24\n33.33\n41.44\n45.05\n45.95\nmar Deva\n49.55\n44.14\n53.15\n45.95\n56.76\nmau Latn\n29.73\n29.73\n36.94\n37.84\n32.43\nmbb Latn\n44.14\n42.34\n38.74\n39.64\n49.55\nmck Latn\n40.54\n34.23\n36.04\n39.64\n49.55\nmcn Latn\n35.14\n27.93\n33.33\n33.33\n38.74\nmco Latn\n41.44\n33.33\n43.24\n33.33\n43.24\nmdy Ethi\n39.64\n46.85\n43.24\n43.24\n51.35\nmeu Latn\n53.15\n38.74\n45.05\n48.65\n52.25\nmfe Latn\n51.35\n48.65\n52.25\n50.45\n56.76\nmgh Latn\n42.34\n33.33\n41.44\n35.14\n38.74\nmgr Latn\n39.64\n34.23\n33.33\n41.44\n38.74\nmhr Cyrl\n47.27\n42.73\n45.45\n42.73\n48.18\nmin Latn\n37.84\n45.95\n53.15\n45.05\n53.15\nmiq Latn\n51.35\n46.85\n43.24\n54.95\n49.55\nmkd Cyrl\n52.25\n48.65\n56.76\n57.66\n66.67\nmlg Latn\n35.14\n36.04\n36.94\n37.84\n45.95\nmlt Latn\n37.84\n33.33\n42.34\n43.24\n46.85\nmos Latn\n39.64\n42.34\n39.64\n36.04\n36.04\nmps Latn\n47.75\n45.05\n42.34\n45.05\n51.35\nmri Latn\n45.05\n42.34\n38.74\n42.34\n44.14\nTable 26: Detailed results on Taxi1500 (Part III). 3-shot results are presented.\n38\nPreprint. Under review.\nLang\nLLaMA 2-7B\nmGPT-13B\nBLOOM-7B1\nXGLM-7.5B\nMaLA-500\nmrw Latn\n40.54\n39.64\n41.44\n37.84\n49.55\nmsa Latn\n44.14\n41.44\n45.95\n37.84\n46.85\nmwm Latn\n36.94\n31.53\n39.64\n38.74\n47.75\nmxv Latn\n33.33\n35.14\n39.64\n38.74\n40.54\nmya Mymr\n45.05\n48.65\n44.14\n44.14\n46.85\nmyv Cyrl\n39.64\n43.24\n40.54\n41.44\n45.05\nmzh Latn\n45.05\n45.95\n42.34\n40.54\n44.14\nnan Latn\n32.43\n35.14\n48.65\n49.55\n44.14\nnaq Latn\n36.94\n36.94\n37.84\n39.64\n41.44\nnav Latn\n27.03\n28.83\n30.63\n33.33\n38.74\nnbl Latn\n21.62\n18.02\n21.62\n25.23\n27.93\nnch Latn\n37.84\n34.23\n33.33\n40.54\n40.54\nncj Latn\n46.85\n45.95\n42.34\n41.44\n42.34\nndc Latn\n44.14\n36.04\n43.24\n36.94\n49.55\nnde Latn\n33.33\n29.73\n33.33\n36.04\n41.44\nndo Latn\n41.28\n34.86\n37.61\n33.94\n46.79\nnds Latn\n41.44\n38.74\n37.84\n34.23\n43.24\nnep Deva\n45.05\n49.55\n63.06\n51.35\n60.36\nngu Latn\n47.75\n39.64\n43.24\n42.34\n49.55\nnld Latn\n47.75\n39.64\n47.75\n43.24\n56.76\nnmf Latn\n44.14\n40.54\n42.34\n41.44\n44.14\nnnb Latn\n45.05\n42.34\n36.94\n44.14\n40.54\nnno Latn\n56.76\n46.85\n45.95\n52.25\n54.95\nnob Latn\n52.25\n41.44\n44.14\n45.95\n56.76\nnor Latn\n50.45\n35.14\n46.85\n47.75\n53.15\nnpi Deva\n51.35\n54.95\n55.86\n45.95\n54.95\nnse Latn\n38.74\n28.83\n39.64\n38.74\n42.34\nnso Latn\n45.05\n43.24\n45.05\n45.05\n50.45\nnya Latn\n48.65\n39.64\n44.14\n42.34\n54.95\nnyn Latn\n39.64\n33.33\n37.84\n36.94\n45.05\nnyy Latn\n43.24\n42.34\n43.24\n40.54\n47.75\nnzi Latn\n36.94\n32.43\n33.33\n32.43\n35.14\nori Orya\n43.24\n34.23\n51.35\n46.85\n45.95\nory Orya\n44.14\n44.14\n49.55\n46.85\n55.86\noss Cyrl\n49.55\n49.55\n49.55\n44.14\n54.05\note Latn\n34.23\n31.53\n34.23\n36.04\n49.55\npag Latn\n44.14\n48.65\n48.65\n42.34\n50.45\npam Latn\n45.95\n36.04\n44.14\n47.75\n45.05\npan Guru\n41.44\n33.33\n46.85\n40.54\n47.75\npap Latn\n50.45\n44.14\n52.25\n49.55\n53.15\npau Latn\n38.74\n45.05\n37.84\n36.94\n46.85\npcm Latn\n58.56\n47.75\n56.76\n53.15\n57.66\npdt Latn\n53.15\n45.95\n45.95\n48.65\n54.05\npes Arab\n50.91\n46.36\n59.09\n48.18\n53.64\npis Latn\n57.66\n47.75\n50.45\n45.95\n55.86\npls Latn\n43.24\n43.24\n43.24\n39.64\n45.95\nplt Latn\n36.94\n35.14\n37.84\n43.24\n47.75\npoh Latn\n42.34\n42.34\n45.05\n39.64\n48.65\npol Latn\n41.44\n43.24\n46.85\n55.86\n56.76\npon Latn\n45.95\n39.64\n43.24\n39.64\n42.34\npor Latn\n56.76\n54.95\n56.76\n54.05\n58.56\nprk Latn\n44.14\n43.24\n49.55\n40.54\n46.85\nprs Arab\n50.45\n51.35\n55.86\n56.76\n57.66\npxm Latn\n48.65\n44.14\n41.44\n41.44\n47.75\nqub Latn\n46.85\n44.14\n43.24\n48.65\n45.05\nquc Latn\n45.05\n41.44\n43.24\n38.74\n50.45\nqug Latn\n45.95\n46.85\n50.45\n45.05\n56.76\nquh Latn\n49.55\n49.55\n46.85\n42.34\n51.35\nquw Latn\n43.24\n36.94\n45.05\n44.14\n53.15\nquy Latn\n58.56\n48.65\n54.95\n50.45\n57.66\nquz Latn\n51.35\n38.74\n60.36\n54.95\n59.46\nqvi Latn\n46.79\n46.79\n49.54\n45.87\n47.71\nrap Latn\n43.24\n35.14\n41.44\n39.64\n46.85\nrar Latn\n40.54\n32.43\n31.53\n29.73\n45.95\nrmy Latn\n37.84\n37.84\n38.74\n40.54\n43.24\nron Latn\n45.05\n51.35\n44.14\n47.75\n57.66\nTable 27: Detailed results on Taxi1500 (Part IV). 3-shot results are presented.\n39\nPreprint. Under review.\nLang\nLLaMA 2-7B\nmGPT-13B\nBLOOM-7B1\nXGLM-7.5B\nMaLA-500\nrop Latn\n45.95\n45.05\n42.34\n42.34\n55.86\nrug Latn\n43.24\n38.74\n46.85\n44.14\n45.05\nrun Latn\n46.85\n40.54\n45.05\n40.54\n52.25\nrus Cyrl\n49.55\n41.44\n50.45\n47.75\n53.15\nsag Latn\n43.24\n43.24\n41.44\n40.54\n47.75\nsah Cyrl\n40.54\n35.14\n44.14\n44.14\n54.95\nsba Latn\n42.34\n43.24\n45.05\n40.54\n49.55\nseh Latn\n45.05\n35.14\n40.54\n42.34\n45.95\nsin Sinh\n39.64\n38.74\n39.64\n42.34\n45.95\nslk Latn\n53.15\n50.45\n44.14\n47.75\n53.15\nslv Latn\n47.75\n45.05\n55.86\n51.35\n49.55\nsme Latn\n45.95\n45.05\n42.34\n41.44\n48.65\nsmo Latn\n38.74\n40.54\n43.24\n44.14\n53.15\nsna Latn\n50.45\n30.63\n43.24\n45.95\n60.36\nsnd Arab\n44.14\n45.05\n56.76\n51.35\n56.76\nsom Latn\n33.33\n36.94\n35.14\n34.23\n39.64\nsop Latn\n40.54\n34.23\n40.54\n35.14\n35.14\nsot Latn\n47.75\n41.44\n40.54\n43.24\n49.55\nspa Latn\n51.35\n49.55\n51.35\n51.35\n56.76\nsqi Latn\n42.34\n43.24\n52.25\n52.25\n57.66\nsrm Latn\n35.14\n41.44\n39.64\n37.84\n45.05\nsrn Latn\n45.95\n53.15\n54.05\n48.65\n51.35\nsrp Latn\n59.46\n48.65\n58.56\n54.05\n58.56\nssw Latn\n38.74\n45.05\n36.94\n40.54\n48.65\nsun Latn\n43.24\n40.54\n45.05\n44.14\n48.65\nsuz Deva\n46.85\n42.34\n42.34\n43.24\n49.55\nswe Latn\n58.56\n48.65\n53.15\n54.95\n61.26\nswh Latn\n46.85\n49.55\n49.55\n48.65\n56.76\nsxn Latn\n42.34\n36.94\n44.14\n44.14\n46.85\ntam Taml\n44.14\n53.15\n59.46\n48.65\n60.36\ntat Cyrl\n47.75\n47.75\n45.95\n48.65\n54.05\ntbz Latn\n36.04\n35.14\n34.23\n35.14\n42.34\ntca Latn\n39.64\n40.54\n43.24\n41.44\n45.05\ntdt Latn\n40.54\n38.74\n48.65\n45.05\n52.25\ntel Telu\n33.33\n45.95\n50.45\n45.95\n49.55\nteo Latn\n33.33\n37.84\n26.13\n31.53\n41.44\ntgk Cyrl\n42.34\n44.14\n48.65\n49.55\n57.66\ntgl Latn\n48.65\n41.44\n46.85\n51.35\n51.35\ntha Thai\n43.24\n42.34\n43.24\n37.84\n47.75\ntih Latn\n43.24\n37.84\n40.54\n36.04\n54.05\ntir Ethi\n29.73\n36.94\n27.93\n34.23\n41.44\ntlh Latn\n51.35\n45.95\n45.95\n41.44\n53.15\ntob Latn\n44.55\n43.64\n41.82\n38.18\n50.00\ntoh Latn\n42.34\n39.64\n40.54\n40.54\n42.34\ntoi Latn\n44.14\n45.05\n34.23\n36.04\n45.05\ntoj Latn\n43.24\n40.54\n36.94\n43.24\n42.34\nton Latn\n42.34\n42.34\n42.34\n44.14\n52.25\ntop Latn\n46.85\n34.23\n37.84\n38.74\n36.94\ntpi Latn\n48.65\n44.14\n52.25\n48.65\n49.55\ntpm Latn\n37.84\n41.44\n38.74\n32.43\n42.34\ntsn Latn\n40.54\n36.04\n38.74\n34.23\n37.84\ntsz Latn\n37.84\n32.43\n37.84\n38.74\n46.85\ntuc Latn\n45.95\n44.14\n47.75\n44.14\n48.65\ntui Latn\n42.34\n38.74\n38.74\n37.84\n50.45\ntuk Latn\n36.04\n42.34\n45.05\n43.24\n50.45\ntum Latn\n47.75\n39.64\n46.85\n52.25\n50.45\ntur Latn\n46.79\n44.04\n40.37\n43.12\n45.87\ntwi Latn\n41.44\n43.24\n41.44\n37.84\n46.85\ntyv Cyrl\n38.74\n38.74\n43.24\n44.14\n45.05\ntzh Latn\n41.82\n36.36\n41.82\n41.82\n38.18\ntzo Latn\n39.64\n43.24\n34.23\n29.73\n41.44\nudm Cyrl\n36.94\n38.74\n42.34\n44.14\n47.75\nukr Cyrl\n52.25\n48.65\n51.35\n55.86\n53.15\nTable 28: Detailed results on Taxi1500 (Part V). 3-shot results are presented.\n40\nPreprint. Under review.\nLang\nLLaMA 2-7B\nmGPT-13B\nBLOOM-7B1\nXGLM-7.5B\nMaLA-500\nukr Cyrl\n52.25\n48.65\n51.35\n55.86\n53.15\nuzb Latn\n45.05\n49.55\n37.84\n46.85\n54.05\nuzn Cyrl\n45.95\n40.54\n45.05\n45.05\n49.55\nven Latn\n45.05\n44.14\n42.34\n41.44\n54.05\nvie Latn\n53.15\n45.95\n62.16\n45.95\n54.95\nwal Latn\n35.14\n33.33\n35.14\n35.14\n39.64\nwar Latn\n48.65\n39.64\n37.84\n45.05\n54.95\nwbm Latn\n48.65\n39.64\n46.85\n46.85\n48.65\nwol Latn\n36.04\n34.23\n32.43\n34.23\n36.94\nxav Latn\n50.45\n33.33\n46.85\n44.14\n45.95\nxho Latn\n43.24\n37.84\n40.54\n39.64\n46.85\nyan Latn\n45.05\n46.85\n52.25\n41.44\n53.15\nyao Latn\n42.34\n41.44\n43.24\n44.14\n48.65\nyap Latn\n38.74\n40.54\n35.14\n32.43\n41.44\nyom Latn\n35.14\n31.53\n33.33\n25.23\n36.94\nyor Latn\n41.44\n38.74\n39.64\n44.14\n47.75\nyua Latn\n41.44\n32.43\n43.24\n41.44\n36.04\nyue Hani\n43.24\n48.65\n53.15\n38.74\n57.66\nzai Latn\n45.05\n35.14\n40.54\n43.24\n44.14\nzho Hani\n47.75\n51.35\n51.35\n44.14\n58.56\nzlm Latn\n54.05\n49.55\n57.66\n56.76\n64.86\nzom Latn\n50.45\n42.34\n44.14\n43.24\n48.65\nzsm Latn\n58.56\n59.46\n63.96\n55.86\n66.67\nzul Latn\n46.85\n42.34\n46.85\n46.85\n51.35\nall\n44.07\n40.98\n43.98\n43.24\n48.89\nTable 29: Detailed results on Taxi1500 (Part VI). 3-shot results are presented.\n41\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2024-01-24",
  "updated": "2024-04-03"
}