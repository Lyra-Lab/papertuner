{
  "id": "http://arxiv.org/abs/2401.13912v1",
  "title": "A Survey of Deep Learning and Foundation Models for Time Series Forecasting",
  "authors": [
    "John A. Miller",
    "Mohammed Aldosari",
    "Farah Saeed",
    "Nasid Habib Barna",
    "Subas Rana",
    "I. Budak Arpinar",
    "Ninghao Liu"
  ],
  "abstract": "Deep Learning has been successfully applied to many application domains, yet\nits advantages have been slow to emerge for time series forecasting. For\nexample, in the well-known Makridakis (M) Competitions, hybrids of traditional\nstatistical or machine learning techniques have only recently become the top\nperformers. With the recent architectural advances in deep learning being\napplied to time series forecasting (e.g., encoder-decoders with attention,\ntransformers, and graph neural networks), deep learning has begun to show\nsignificant advantages. Still, in the area of pandemic prediction, there remain\nchallenges for deep learning models: the time series is not long enough for\neffective training, unawareness of accumulated scientific knowledge, and\ninterpretability of the model. To this end, the development of foundation\nmodels (large deep learning models with extensive pre-training) allows models\nto understand patterns and acquire knowledge that can be applied to new related\nproblems before extensive training data becomes available. Furthermore, there\nis a vast amount of knowledge available that deep learning models can tap into,\nincluding Knowledge Graphs and Large Language Models fine-tuned with scientific\ndomain knowledge. There is ongoing research examining how to utilize or inject\nsuch knowledge into deep learning models. In this survey, several\nstate-of-the-art modeling techniques are reviewed, and suggestions for further\nwork are provided.",
  "text": "A Survey of Deep Learning and Foundation Models for Time\nSeries Forecasting\nJOHN A. MILLER, MOHAMMED ALDOSARI, FARAH SAEED, NASID HABIB BARNA,\nSUBAS RANA, I. BUDAK ARPINAR, and NINGHAO LIU\nDeep Learning has been successfully applied to many application domains, yet its advantages have been slow\nto emerge for time series forecasting. For example, in the well-known Makridakis (M) Competitions, hybrids of\ntraditional statistical or machine learning techniques have only recently become the top performers. With the\nrecent architectural advances in deep learning being applied to time series forecasting (e.g., encoder-decoders\nwith attention, transformers, and graph neural networks), deep learning has begun to show significant\nadvantages. Still, in the area of pandemic prediction, there remain challenges for deep learning models: the\ntime series is not long enough for effective training, unawareness of accumulated scientific knowledge, and\ninterpretability of the model. To this end, the development of foundation models (large deep learning models\nwith extensive pre-training) allows models to understand patterns and acquire knowledge that can be applied\nto new related problems before extensive training data becomes available. Furthermore, there is a vast amount\nof knowledge available that deep learning models can tap into, including Knowledge Graphs and Large\nLanguage Models fine-tuned with scientific domain knowledge. There is ongoing research examining how to\nutilize or inject such knowledge into deep learning models. In this survey, several state-of-the-art modeling\ntechniques are reviewed, and suggestions for further work are provided.\nACM Reference Format:\nJohn A. Miller, Mohammed Aldosari, Farah Saeed, Nasid Habib Barna, Subas Rana, I. Budak Arpinar, and Ning-\nhao Liu. 2024. A Survey of Deep Learning and Foundation Models for Time Series Forecasting. 1, 1 (Janu-\nary 2024), 35 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn\n1\nINTRODUCTION\nThe experience with COVID-19 over the past four years has made it clear to organizations such as\nthe National Science Foundation (NSF) and the Centers for Disease and Prevention (CDC) that we\nneed to be better prepared for the next pandemic. COVID-19 has had huge impacts with 6,727,163\nhospitalizations and 1,169,666 deaths as of Saturday, January 13, 2024, in the United States alone\n(first US case 1/15/2020, first US death 2/29/2020). The next one could be more virulent with greater\nimpacts.\nThere were some remarkable successes such as the use of messenger RNA vaccines that could be\ndeveloped much more rapidly than prior approaches. However, the track record for detecting the\nstart of a pandemic and the forecasting of its trajectory leaves room for improvement.\nPandemic Preparedness encapsulates the need for continuous monitoring. Predicting rare events\nin complex, stochastic systems is very difficult. Transitions from pre-emergence to epidemic to\npandemic are easy to see only after the fact. Pandemic prediction using models is critically important\nAuthors’ address: John A. Miller, jamill@uga.edu; Mohammed Aldosari, maa25321@uga.edu; Farah Saeed, farah.saeed@ug\na.edu; Nasid Habib Barna, nasidhabib.barna@uga.edu; Subas Rana, subas.rana@uga.edu; I. Budak Arpinar, budak@uga.edu;\nNinghao Liu, ninghao.liu@uga.edu.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee\nprovided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the\nfull citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored.\nAbstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires\nprior specific permission and/or a fee. Request permissions from permissions@acm.org.\n© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM XXXX-XXXX/2024/1-ART\nhttps://doi.org/10.1145/nnnnnnn.nnnnnnn\n, Vol. 1, No. 1, Article . Publication date: January 2024.\narXiv:2401.13912v1  [cs.LG]  25 Jan 2024\n2 John A. Miller, Mohammed Aldosari, Farah Saeed, Nasid Habib Barna, Subas Rana, I. Budak Arpinar, and Ninghao Liu\nas well. Sophisticated models are used to predict the future of hurricanes due to their high impact\nand potential for loss of life. The impacts of pandemics are likely to be far greater.\nAs with weather forecasting, accurate pandemic forecasting requires three things: (1) a collection\nof models, (2) accurate data collection, and (3) data assimilation. If any of these three break down,\naccuracy drops. When accuracy drops, interventions and control mechanisms cannot be optimally\napplied, leading to frustration from the public.\nDuring the COVID-19 pandemic data were collected daily, but as seen in Figure 1, there is a\nvery strong weekly pattern that dominates the curve of new deaths that is an artifact of reporting\nprocesses. Also, notice how strong hospitalizations and the number of Intensive Care Unit (ICU)\npatients appear to be good leading indicators.\nFig. 1. Rescaled Plot of Daily Deaths during the Active Pandemic (note the weekly reporting pattern),\nHospitalization, and ICU Patients.\nDue to the saw-tooth pattern in the daily deaths, some modeling studies find it better to work off\nof weekly data. In the later stages of COVID-19, daily reporting stopped, and only weekly remains.\nUnfortunately, this means that there is much less data available for training deep learning models.\nModeling techniques applied were statistical, machine learning or theory-based compartmen-\ntal models that were extensions to Susceptible-Infected-Recovered (SIR) or Susceptible-Exposed-\nInfected-Recovered (SEIR) models. Transitions among these states are governed by differential\nequations with rate constants that can be estimated from data. Unfortunately, estimating the\npopulation of individuals that are in, for example, the exposed state can be very difficult.\nThe other two categories, statistical and machine learning (including deep learning and founda-\ntion models), one could argue are more adaptable to available data as they look for patterns that\nrepeat, dependency on the past, and leading indicators. Both can be formulated as Multivariate Time\nSeries (MTS) forecasting problems, although the related problems of MTS classification and anomaly\ndetection are also very important. Still, having a connection to theory is desirable and could lead to\nbetter forecasting in the longer term, as well as a greater understanding of the phenomena. This\nhas led to work in Theory-Guided Data Science (TGDS) [52, 82] and Physics-Informed Neural\nNetworks (PINN) [51].\nStatistical and machine learning techniques complement each other. For example, modeling\nstudies should have reliable baseline models that, from our studies, should include Random Walk\n(RW), Auto-Regressive (AR), and Seasonal, Auto-Regressive, Integrated, Moving Average with\neXogenous variables (SARIMAX). SARIMAX is typically competitive with Deep Learning models\nwhen training data are limited. If weekly data are used, the training data will be limited for much of\nthe early stages of the pandemic, just when the need for accurate forecasts is the greatest. Baselines\n, Vol. 1, No. 1, Article . Publication date: January 2024.\nA Survey of Deep Learning and Foundation Models for Time Series Forecasting\n3\nlike SARIMAX can also be helpful for hyper-parameter tuning, in that with sufficient data, one\nwould expect deep learning models to perform well; the SARIMAX results can help gauge this.\nFurthermore, SARIMAX has been used for data augmentation to help train deep learning models\n[42].\nLooking toward the future, this survey paper that extends [80] asks the question of how Artificial\nIntelligence (AI), particularly deep learning, can be used to improve pandemic preparedness and\nprediction, in terms of better deep learning models, more explainable models, access to scientific\nliterature using Large Language Models (LLM), development and use of knowledge bases and\nknowledge graphs, as well as better and ongoing assessment of pandemic interventions and\ncontrols.\nThe rest of this paper is organized as follows: Section 2 provides an overview of two waves of\nimprovement in MTS forecasting. Section 3 focuses on recent progress in MTS forecasting looking\nat Transformers and related modeling techniques. These modeling techniques increasingly strive to\nbetter capture temporal dynamics and tend to be the top performers for national-level COVID-19\nforecasting. Section 4 focuses on recent progress in MTS forecasting in the spatial-temporal domain,\nwhere various types of Graph Neural Networks have a natural appeal. These modeling techniques\ntend to be applied to state-level COVID-19 data. Foundation models, large pre-trained deep learning\nmodels, for time series forecasting are discussed in Section 5. Knowledge in various forms such as\nKnowledge Graphs is a natural complement to forecasting models, as discussed in Section 6. The\nknowledge can be used to improve forecasting accuracy, check the reasonableness for forecasts\n(especially a problem for long-term forecasting), guide the modeling process, and help explain the\nmodeling results. A meta-study comparing the effectiveness of several modeling techniques found\nin the current literature is given in section 7. Finally, a summary is given in Section 8 that includes\nlooking into a crystal ball to see where MTS might be heading in the future.\n2\nPROGRESS IN MTS FORECASTING\nThe history of time series forecasting dates way back as shown in Table 1. Note that some of the\nmodeling techniques (or model types) have general use other than time series forecasting, but the\ndate and paper reflect their use in forecasting for time series or sequence data. There were, however,\nperiods of rapid progress, for example, the one in the 1950s through the ’70s, and captured in the\nseminal book by Box and Jenkins [10]. Another period of substantial progress coincides with the\nadvancement of deep learning, in the 2010s.\nThere are several open source projects that support time series analysis across programming\nlanguages:\n• R: Time Series [https://cran.r-project.org/web/views/TimeSeries.html] consists of a large\ncollection of time series models.\n• Python: Statsmodels [https://www.statsmodels.org/stable/index.html] provides a basic col-\nlection of time series models. Sklearn [https://scikit-learn.org/stable] and a related package,\nSktime [https://www.sktime.net/en/stable], provide most of the models offered by Statsmod-\nels. PyTorch-Forecasting [https://github.com/jdb78/pytorch-forecasting?tab=readme-ov-file]\nincludes several types of Recurrent Neural Networks. TSlib [https://github.com/thuml/Time-\nSeries-Library] provides several types of Transformers used for time series analysis.\n• Scala: Apache Spark [https://spark.apache.org/] has a limited collection of time series models.\nScalaTion [https://cobweb.cs.uga.edu/~jam/scalation.html], [https://github.com/scalation]\n[81] supports most of the modeling techniques listed in Table 1. In addition, it supports\nseveral forms of Time-Series Regression, including recursive ARX, direct ARX_MV, quadratic,\nrecursive ARX_Quad, and quadratic, direct ARX_Quad_MV.\n, Vol. 1, No. 1, Article . Publication date: January 2024.\n4 John A. Miller, Mohammed Aldosari, Farah Saeed, Nasid Habib Barna, Subas Rana, I. Budak Arpinar, and Ninghao Liu\nTable 1. Types of Time-Series Forecasting Models with Key Initial Reference in Time-Series Context\nModel Type\nShort Description\nDateReference\nBaselineHA\nHistorical Average Baseline\n.\n.\nBaselineRW\nRandom Walk Baseline -\nguess the previous value\n.\n.\nRegression4TS\nTime-Series Regression\nwith Lagged Variables\n1949\n[19]\nARIMA\nAutoRegressive, Integrated, Moving-Average 1951\n[9, 125]\nSES\nSimple Exponential Smoothing\n1957\n[36]\nVAR\nVector AutoRegressive\n1957\n[96]\nVARMA\nVector AutoRegressive, Moving-Average\n1957\n[96]\nVECM\nVector Error Correction Model\n1957\n[96]\nES\nExponential Smoothing (Holt-Winters)\n1960\n[127]\nSARIMA\nSeasonal ARIMA\n1967\n[11]\nSARIMAX\nSARIMA, with eXogenous variables\n1970\n[10]\nNAR/NARX\nNonlinear AutoRegressive, Exogenous\n1978\n[46]\nNeural Network\nNeural Network\n1988\n[108]\nRNN\nRecurrent Neural Network\n1989\n[126]\nFDA/FTSA\nFunctional Data/Time-Series Analysis\n1991\n[98]\nCNN\nConvolutional Neural Network\n1995\n[60]\nSVR\nSupport Vector Regression\n1997\n[85]\nLSTM\nLong, Short-Term Memory\n1997\n[35]\nELM\nExtreme Learning Machine\n2007\n[107]\nGRU\nGated Recurrent Unit\n2014\n[16]\nEncoder-Decoder\nEncoder-Decoder with Attention\n2014\n[17]\nMGU\nMinimal Gated Unit\n2016\n[149]\nTCN\nTemporal Convolutional Network\n2016\n[59]\nGNN/GCN\nGraph Neural/Convolutional Network\n2016\n[56]\nvTRF\n(Vanilla) Transformer\n2017\n[118]\nForecasting the future is a very difficult thing to do and until recently, machine learning models\ndid not offer much beyond what statistical models provided, as born out in the M Competitions.\n2.1\nM Competitions\nThe Makridakis or M Competitions began in 1982 and have continued with M6 which took place\n2022-2023. In the M4 competition ending in May 2018, the purely ML techniques performed\npoorly, although a hybrid neural network (LSTM) statistical (ES) technique [109] was the winner\n, Vol. 1, No. 1, Article . Publication date: January 2024.\nA Survey of Deep Learning and Foundation Models for Time Series Forecasting\n5\n[74]. The rest of the top performers were combinations of statistical techniques. Not until the M5\ncompetition ended in June 2020, did Machine Learning (ML) modeling techniques become better than\nclassical statistical techniques. Several top performers included LightGBM in their combinations\n[75]. LightGBM is a highly efficient implementation of Gradient Boosting Decision/Regression\nTrees [53]. Multiple teams included neural networks in their combinations as well. In particular,\ndeep learning techniques such as DeepAR consisting of multiple LSTM units [104] and N-BEATS,\nconsisting of multiple Fully Connected Neural Network blocks [88] were applied. Many of them\nalso combined recursive (e.g., uses 𝑡+ 1 prior forecast for 𝑡+ 2 forecast) and direct (non-recursive)\nforecasting. “In M4, only two sophisticated methods were found to be more accurate than simple\nstatistical methods, where the latter occupied the top positions in the competition. By contrast,\nall 50 top-performing methods were based on ML in M5\". [75] The M6 competition involves both\nforecasting and investment strategies [76] with summaries of its results expected in 2024.\n2.2\nStatistical and Deep Learning Models for Time Series\nAs illustrated by the discussion of the M Competitions, machine learning techniques took some\ntime to reach the top of the competition. Neural Network models demonstrated highly competitive\nresults in many domains, but less so in the domain of time series forecasting, perhaps because\nthe patterns are more elusive and often changing. Furthermore, until the big data revolution, the\ndatasets were too small to train a neural network having a large number of parameters.\n2.2.1\nFirst Wave. From the first wave of progress, SARIMAX models have been shown to gen-\nerally perform well, as they can use past and forecasted values of the endogenous time series,\npast errors/shocks for the endogenous time series, and past values of exogenous time series. In\naddition, differencing of the endogenous variable may be used to improve its stationarity, and\nfurthermore, seasonal/periodic patterns can be utilized. As an aside, many machine learning papers\nhave compared their models to ARIMA, yet SARIMAX is still efficient and is often superior to\nARIMA. In addition, the M4 and M5 Competitions indicated that Exponential Smoothing can\nprovide simple and accurate models.\nThe most straightforward time series model for MTS is a Vector Auto-Regressive (VAR) model\nwith 𝑝lags over 𝑛variables VAR(𝑝,𝑛). For example, a 3-lag, bi-variate VAR(3, 2) model can be\nuseful in pandemic forecasting as new hospitalizations and new deaths are related variables for\nwhich time series data are maintained, i.e., 𝑦𝑡0 is the number of new hospitalizations at time 𝑡and\n𝑦𝑡1 is the number of new deaths at time 𝑡. The model (vector) equation may be written as follows:\ny𝑡= 𝜹+ Φ(0)y𝑡−1 + Φ(1)y𝑡−2 + Φ(2)y𝑡−3 + 𝝐𝑡\n(1)\nwhere 𝜹∈R2 is a constant vector, Φ(0) ∈R2×2 is the parameter matrix for the first lag, Φ(1) ∈R2×2\nis the parameter matrix for the second lag, Φ(2) ∈R2×2 is the parameter matrix for the third lag,\nand 𝝐𝑡∈R2 is the residual/error/shock vector. Some research has explored VARMA but has found\nthem to be only slightly more accurate than VAR models, but more complex, as they can weigh in\npast errors/shocks [3]. SARIMAX and VAR can both be considered models for multivariate time\nseries, the difference is that SARIMAX focuses on one principal variable, with the other being used\nas indicators for those variables, for example, using new_cases and hospitalizations time series to\nhelp forecast new_deaths. SARIMAX tends to suffer less from the compounding of errors than VAR.\nA SARIMAX model can be trimmed down to an ARX model (Auto-Regressive, eXogenous) to see\nthe essential structure of the model consisting of the first 𝑝lags of the endogenous variable along\nwith lags [𝑎,𝑏] of the exogenous variable ARX(𝑝, [𝑎,𝑏]). For example, the model equation for an\nARX(3, [2, 3]) may be written as follows:\n, Vol. 1, No. 1, Article . Publication date: January 2024.\n6 John A. Miller, Mohammed Aldosari, Farah Saeed, Nasid Habib Barna, Subas Rana, I. Budak Arpinar, and Ninghao Liu\n𝑦𝑡= 𝛿+ 𝜙0𝑦𝑡−1 + 𝜙1𝑦𝑡−2 + 𝜙2𝑦𝑡−3 + 𝛽0𝑥𝑡−2 + 𝛽1𝑥𝑡−3 + 𝜖𝑡\n(2)\nwhere ˆ𝑦𝑡, the forecasted value, is the same formula without 𝜖𝑡. The MTS in this case consists of one\nendogenous time series 𝑦𝑡and one exogenous time series 𝑥𝑡for 𝑡∈0..𝑚−1. All the parameters\n𝛿,𝜙0,𝜙1,𝜙2, 𝛽0 and 𝛽1 are now scalars. ARX models may have more than one exogenous times series\n(see [92] for a discussion of endogenous vs. exogenous variables). The specification of a SARIMAX\nmodel subsumes the ARX specification,\nSARIMAX(𝑝,𝑑,𝑞)×(𝑃, 𝐷,𝑄)𝑠[𝑎,𝑏]\n(3)\nwhere 𝑝is the number of Auto-Regressive (AR) terms/lagged endogenous values, 𝑑is the number of\nstride-1 differences (Integrations (I)) to take, 𝑞is the number of moving average (MA) terms/lagged\nshocks, 𝑃is the number of seasonal (stride-𝑠) Auto-Regressive (AR) terms/lagged endogenous\nvalues, 𝐷is the number of stride-𝑠differences to take, 𝑄is the number of seasonal (stride-𝑠) moving\naverage (MA) terms/lagged shocks, 𝑠is the seasonal period (e.g., week, month, or whatever time\nperiod best captures the pattern) and [𝑎,𝑏] is the range of exogenous (X) lags to include. Again,\nthere may be multiple exogenous variables. Although the optimal values for parameters of ARX\nmodels may be found very efficiently by solving a system of linear equations, for example, using\nmatrix factorization, inclusion of the MA terms makes the equation non-linear, so the loss function\n(e.g., Mean Squared Error (MSE) or Negative Log-Likelihood (NLL)) is usually minimized using a\nnon-linear optimizer such as the Limited-memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS)\nalgorithm [68].\n2.2.2\nSecond Wave. The story is not complete for the second wave. The M5 Competition showed\nthe value of LightGBM. Studies have shown that LSTM and GRU tend to perform similarly and\nusually outperform Feedforward Neural Networks (FNN).\nThe idea of a Recurrent Neural Network (RNN) is to take a weighted combination of an input\nvector x𝑡(recent history) and a hidden state vector (decaying long-term history) and activate it to\ncompute the new state h𝑡. A final layer converts this to a time series forecast(s) ˆ𝑦𝑡.\nx𝑡= [𝑦𝑡−𝑝, . . . ,𝑦𝑡−1]\ninput vector\nh𝑡= f(𝑈x𝑡+𝑊h𝑡−1 + 𝜷(ℎ))\nhidden state vector\nˆ𝑦𝑡= 𝑔(𝑉h𝑡+ 𝛽(𝑦))\noutput scalar\nwhere 𝑈, 𝑊, and 𝑉are learnable weight matrices, the 𝛽s are biases, and the variables are\n• x𝑡∈R𝑝holds the collected inputs (time series values from the recent past)\n• h𝑡∈R𝑛ℎholds the current hidden state\n• ˆ𝑦𝑡∈R𝑘holds the 1-step to 𝑘-steps ahead forecasts (here 𝑘= 1)\nBy adding sigmoid-activated gates, the flow of historical information (and thus the stability of\ngradients) can often be improved. A Gated Recurrent Unit (GRU) model adds two gates, while\na Long Short-Term Memory (LSTM) model adds three gates to an RNN. Removing 𝑊h𝑡−1 from\nthe second equation turns an RNN into a FNN. Adding additional layers (up to a point) tends to\nimprove forecasting accuracy.\nSubstantial gains seem to come from adopting an Encoder-Decoder architecture (used for\nSeq2Seq problems) where the encoder can concentrate on learning patterns from the past, while the\ndecoder concentrates on making accurate forecasts. Originally, this architecture used for forecasting\nhad LSTM/GRU units in both the encoder and decoder. Each step of the encoder produced a hidden\nstate vector, with the last one being fed into the decoder. For a long time series, feeding all hidden\n, Vol. 1, No. 1, Article . Publication date: January 2024.\nA Survey of Deep Learning and Foundation Models for Time Series Forecasting\n7\nstate vectors would be unlikely to help. What was needed was to weigh these hidden states by their\nimportance in making forecasts, a tall order to figure out a priori, but the weights could be learned\nduring the training process. This led to the development of attention mechanisms that opened the\ndoor for more substantial improvement in time series forecasting.\nSelf-attention, multi-head attention, cross attention along with positional encoding has led to\nthe replacement of LSTM/GRU units with encoder (and decoder) layers that use attention followed\nby feed-forward neural network layers. Using several such layers for the encoder and several for\nthe decoder, forms a Transformer. In general, a Transformer consists of multiple encoder and\ndecoder blocks. A simplified first encoder block is depicted in Figure 2, where input vectors [x𝑡]\nare passed into a self-attention layer, to which the input is added (via a skip connection) and then\nnormalized to obtain [z𝑡], after which the result is passed to a FNN, followed by another round of\nadd-back and normalization (see [118] for a more complete diagram). In addition, as self-attention\ndoes not consider order, one may use (combine with input) positional (based on the absolute or\nrelative order of x𝑡) and/or temporal (based on x𝑡’s date-time) encodings to make up for this.\n[x𝑡]\nattention (𝑄, 𝐾,𝑉)\n→add in input [x𝑡]\nnormalize\nfeed-forward neural network\n→add in [z𝑡]\nnormalize again\n[z𝑡]\nFig. 2. Transformer First Encoder Layer for a Single Head\nFor some time series datasets, transformers have been shown to be the top performers. In the\nrelated prediction problem, forecasting the next word in Natural Language Processing (NLP), the\npresent day Large Language Models (LLM) based on the Transformer architecture, represent a\nmajor breakthrough. Will the same happen for time series forecasting or will the progress be\nmore incremental (remember due to the stochastic nature of time series, e.g., pandemic data, there\nare limits to what can be known). Nevertheless, progress in NLP can be adapted for time series\nforecasting.\nA less explored line of research involves Temporal Convolutional Networks (TCN) that utilize\ncausal and dilated convolutions, to provide an expanded view of history, as well as skip connec-\ntions/residual blocks to utilize information from prior layers/maintain gradients [59]. Although,\n, Vol. 1, No. 1, Article . Publication date: January 2024.\n8 John A. Miller, Mohammed Aldosari, Farah Saeed, Nasid Habib Barna, Subas Rana, I. Budak Arpinar, and Ninghao Liu\n[6] shows advantages for TCN over LSTM for many sequence tasks, the evidence appears to be less\nclear for time series forecasting.\n3\nRECENT PROGRESS ON TRANSFORMERS FOR TIME SERIES\n3.1\nSparse Attention\nOver the last few years, there have been several papers that have examined sparse attention for\ntransformers. Rather than having each time point compared with every other time (quadratic\nattention), the focus is sharpened and the complexity of the attention is reduced [66, 123, 154].\nGiven a query matrix 𝑄, key matrix 𝐾and value matrix 𝑉, attention is computed as follows:\nattention(𝑄, 𝐾,𝑉) = softmax\n\u0014𝑄𝐾\n⊺\n√𝑑𝑘\n\u0015\n𝑉\n(4)\nAt a high level, sparse attention can be achieved by reducing the number of queries, or given\na query, reduce the number of keys it is compared to (i.e., attention scores/weights computed).\nQuery prototypes can stand in for several queries and thereby reduce the computation. Also, if\ntwo time points are distant from each other, setting their attention weight to zero, would be one\nway to narrow the focus. In detail, there are many ways to do this [66]. Some of the more popular\napproaches are listed in Table 2. Note that narrowing the focus, besides reducing computation time,\nmay result in improved forecasts (less distraction).\nTable 2. Types of Time-Series Forecasting Transformer Models\nModel Type\nShort Description\nDate Reference\nLogTrans\nLocal and LogSparse Attention\n2019\n[61]\nReformer\nOnly Similar Queries and Keys Are Compared 2020\n[57]\nInformer\nUses Selected Query Prototypes\n2021\n[150]\nAutoformer\nReplaces Self-Attention with Auto-Correlation 2021\n[130]\nPyraformer\nHierarchical/Pyramidal Attention\n2021\n[70]\nFEDformer\nSeries Decomposition and\nUse of Frequency Domain\n2022\n[152]\nNon-stationary TRF\nSeries Stationarization and\nUse of De-stationary Attention\n2022\n[72]\nTriformer\nTriangular Structure for Layer Shrinking\n2022\n[18]\nCrossFormer\nCross-channel Modeling\n2023\n[145]\nPatchTST\nReplaces timestep inputs with patches\n2023\n[87]\nDue to having multiple heads and multiple layers/blocks, transformer explainability/interpretability\nis challenging [15]. To a certain extent, attention weights can also be used for interpretability [65, 84].\nRelated research that can improve explainability/interpretability as well as reduce training time is\non simplifying transformer blocks [34].\n3.2\nMasking and Pre-Training\nThere is an open question of how well pre-trained transformers will work for time series forecasting.\nThe success of pre-training for NLP and Computer Vision (CV) problems is remarkable, but will it\ncarry over for MTS forecasting?\n, Vol. 1, No. 1, Article . Publication date: January 2024.\nA Survey of Deep Learning and Foundation Models for Time Series Forecasting\n9\nThere are ways this could be applied to pandemics. For example, transformers trained on past\npandemics as well as influenza, could be useful in future pandemics and avoid the problem of only\nbecoming proficient once the pandemic is past its worst peaks.\nUsing vectors created from words as input tokens to a transformer, is not the same as taking a\nsingle value from a time series, so patches may be used as they can capture a meaningful pattern\nfrom a sub-sequence of a time series. Given a univariate time series [𝑦𝑡: 𝑡∈0, . . .𝑚−1], it can be\ndivided into (possibly overlapping) sub-sequences of length 𝑝with a stride of 𝑠(when 𝑠= 𝑝there\nis no overlap).\npatch(𝑝,𝑠) = {[𝑦𝑡: 𝑡∈𝑠𝑘, . . .𝑠𝑘+ 𝑝] : 𝑘∈0, . . .𝑚/𝑠}\n(5)\nPatchTST [87] takes multivariate time series data and splits it into multiple univariate time series,\nconsidering them as independent channels. For each univariate time series, it creates patches, e.g.,\nof size 𝑝= 16 with stride 𝑠= 8. These patches are fed as tokens into a transformer. The patches\ncarry local semantic information and the patching reduces computation and memory usage while\nattending a longer history. Channel-independence allows for different attention maps in different\nchannels, promoting better handling of diverse temporal patterns. The results show that PatchTST\noutperforms state-of-the-art Transformer models in supervised and unsupervised learning tasks,\nincluding transfer learning. Notably, it excels in forecasting with longer look-back windows.\nAR-Transformer [2] also exhibits improved performance by combining the Vanilla Transformer\narchitecture with segment-based attention, teacher-forcing, both temporal and positional encoding,\nand auto-regressive (recursive) multi-horizon forecasting.\nThe papers [113] (MTSMAE), [142] (ExtraMAE), and [62] (Ti-MAE) discuss the use of Masked\nAutoEncoders (MAE) for multivariate time series forecasting. Some of the input patches are masked\nout from the encoder and the task is to train the model to essentially put them back. Clearly, this is\nharder than a regular AutoEncoder that maps an input signal to a latent representation (encoded)\nfrom which the signal is recovered (decoded). To succeed in recovering the patches, the MAE needs\nto more fully capture the temporal dependencies. With this enhanced capability, their forecasting\nability, in principle, should be improved. [113] highlights the challenges in applying MAE to time\nseries data and proposes a modified approach called MTSMAE. The MTSMAE model incorporates\nthe concept of Vision Transformer and leverages patching to improve feature extraction and reduce\nredundancy. In the pre-training phase, random patches from the input are masked, and the missing\npatches are recovered. In the fine-tuning phase, the encoder trained in the previous step is used, and\nthe input of the decoder is redesigned. In contrast to the Bidirectional Encoder Representations from\nTransformer (BERT) decoder, which consists of a single fully connected layer, the MTSMAE employs\ndistinct lightweight decoder levels based on different multivariate time series data types. When\ntesting this approach on various typical multivariate time series datasets (Electricity Consuming\nLoad, Electricity Transformer Temperature, Weather) from diverse domains and with varying\ncharacteristics, their experimental findings indicate significantly strong performance.\nTwo major challenges to the successful application of the pre-training, fine-tuning paradigm\nto time series forecasting are (1) the shortness of most time series, (2) the commonness of do-\nmain/distributional shifts. The authors of [45] propose to use domain adaptation techniques com-\nbined with self-attention to deal with these challenges. Other approaches include maintaining a\ncollection of time series from which whole series or segments are compared for similarity, either at\nthe raw-series level or at the representation level (see the next section).\n, Vol. 1, No. 1, Article . Publication date: January 2024.\n10John A. Miller, Mohammed Aldosari, Farah Saeed, Nasid Habib Barna, Subas Rana, I. Budak Arpinar, and Ninghao Liu\n3.3\nRepresentation Learning\nRather than working with the raw multivariate time series [y𝑡], representation learning can be\nused to transform the series into [z𝑡] that exist in a latent space (possibly higher dimensional)\nsuch that [z𝑡] captures the essential information in the time series in a manner that facilitates a\ntask like classification or forecasting. Representation learning can be viewed as a generalization of\nfactor analysis [131], and “a good representation is one that disentangles the underlying factors of\nvariation\" [7]. For forecasting, the general idea is to divide the time series into the past (up to time\n𝜏) and future (after time 𝜏) and use a function f (with parameters 𝑊) to encode 𝑌and 𝑌′ into 𝑍\nand 𝑍′, that serve as their richer latent representations.\n𝑌= [y1, y2, . . . y𝜏]\npast\n(6)\n𝑌′ = [y𝜏+1, y𝜏+2, . . . y𝑚]\nfuture\n(7)\n𝑍= f(𝑌,𝑊)\nencoded past\n(8)\n𝑍′ = f(𝑌′,𝑊)\nencoded future\n(9)\nThe goal is to minimize the difference (measured by a loss function) between 𝑌′ and g(𝑍′,𝑈) where\ng is a prediction function/network (with parameters 𝑈). f plays the role of an encoder function,\nwhile g plays the role of the decoder function. Both may have multiple layers, although some\nresearch uses, for example, regularized regression for the prediction function (the thought being\n[z𝑡] is a rich representation that captures enough essential information from the time series to\nmake prediction more straightforward).\nThere is also work to decompose time series to better capture this essential information, e.g.,\ninto trend, seasonal, and local variability components (in some papers, the latter two are combined).\nDue to the complex mixing of components, techniques for disentanglement have been developed.\nFrom the point of view of linear models, disentanglement may be seen as a generalization of\nmulti-collinearity reduction [58]. One way to improve a representation of time series that is likely\nto reduce noise is to project it onto a smooth function using orthogonal polynomials (e.g., Legendre,\nLaguerre, Chebyshev Polynomials [151]). Training can be enhanced by augmentation or masking.\nPretext training, for example, series reconstruction using an Auto-Encoder (AE) or Masked Auto-\nEncoder (MAE) may be used as well. More for time series classification, but also for forecasting,\ncontrastive learning has been used. Contrastive learning pairs up similar segments for positive\ncases, and dissimilar segments for negative cases, with the thought being prediction should be\npositively influenced by positive cases and negatively by negative cases.\nThere are very recent studies and models developed that demonstrate the effectiveness of\nrepresentation learning for MTS as reviewed in [78]. Table 3 highlights some of the recent work in\nthis area.\nTable 3. Representation Learning for Time-Series\nModel Type\nShort Description\nDate Reference\nTS2Vec\nLocal and LogSparse Attention\n2022\n[139]\nCoST\nOnly Similar Queries and Keys Are Compared 2022\n[128]\nFEAT\nUses Selected Query Prototypes\n2023\n[54]\nSimTS\nReplaces Self-Attention with Auto-Correlation 2023\n[148]\n, Vol. 1, No. 1, Article . Publication date: January 2024.\nA Survey of Deep Learning and Foundation Models for Time Series Forecasting\n11\nThe paper “TS2Vec: Towards Universal Representation of Time Series,\" uses contrastive learning\nwith a loss function that combines temporal and instance-based elements. In the paper, “CoST:\nContrastive Learning of Disentangled Seasonal-Trend Representations for Time Series Forecasting,\"\nthe authors “argue that a more promising paradigm for time series forecasting, is to first learn\ndisentangled feature representations, followed by a simple regression fine-tuning step,\" [128].\nContrastive learning with loss functions for the time (for trend) and frequency (for seasonal)\ndomains are used. A unique aspect of the paper, “FEAT: A General Framework for Feature-Aware\nMultivariate Time-Series Representation Learning,\" is that the framework utilizes an encoder per\nvariable/feature of a multivariate time series, as the time sequence for each variable can have\ndifferent characteristics that can be exploited usefully by the prediction function. The authors\nstate that, “FEAT learns representation for the first time in terms of three diversified perspectives:\nfeature-specific patterns, feature-agnostic temporal patterns, and dependency between multiple\nfeature-specific and temporal information\" [54]. The authors of the paper, “SimTS: Rethinking\nContrastive Representation Learning for Time Series Forecasting,\" argue that while fine for time\nseries classification, the contrastive learning approach described in many of the papers may not be\nideal for time series prediction. In particular, their “model does not use negative pairs to avoid false\nrepulsion\" [148].\nRecent work wishes to make the encoding [z𝑡] more interpretable [147] to increase user confi-\ndence in the model. Although post-hoc interpretation methods can used, having the main model\nbe interpretable itself is the ideal (i.e., the model is highly accurate, it makes sense, and there is\nan understanding of how variables/features affect the results). For example, how effective were\nvaccinations in the COVID-19 Pandemic?\nA few recent papers have shown good results with simpler architectures:\n• DLinear [140] combines series decomposition with a linear (regression) model. A univariate\ntime series 𝑦𝑡is first decomposed into a simple moving average 𝑠𝑡and the remainder 𝑟𝑡.\n𝑠𝑡= movingAverage(𝑦𝑡)\n(10)\n𝑟𝑡= 𝑦𝑡−𝑠𝑡\n(11)\nThen a linear model is applied to each part (𝑠𝑡and 𝑟𝑡) to make forecasts that are combined\ntogether.\n• TSMixer [22] is motivated by MLP-Mixer from computer vision that relies on blocks of MLPs\nand does not use convolutions or self-attention, making the architecture simpler and more\nefficient [115]. (A Multi-Layer Perceptron (MLP) is a fully connected FNN). TSMixer looks\nfor temporal and cross-variable dependencies in an interleaved fashion. This is less complex\nthan considering temporal and cross-variable dependencies simultaneously, although if there\nis a strong leading indicator, this useful information may be ignored.\n4\nRECENT PROGRESS ON GRAPH NEURAL NETWORKS FOR TIME SERIES\nAlthough Transformers are well-suited for temporal analysis, Graph Neural Networks are concep-\ntually well-suited for spatial-temporal analysis. With an Encoder-Decoder or Transformer handling\ntemporal dependencies, a Graph Neural Network (GNN) may be more adept at capturing inter-series\nor spatial dependencies.\n4.1\nNational Level COVID-19 Data\nAt the national level, the dataset may be represented as a matrix 𝑌= [𝑦𝑡𝑗] where 𝑡is time and\n𝑗is the variable. The strength of GNNs is that they can more closely model and examine the\n, Vol. 1, No. 1, Article . Publication date: January 2024.\n12John A. Miller, Mohammed Aldosari, Farah Saeed, Nasid Habib Barna, Subas Rana, I. Budak Arpinar, and Ninghao Liu\ndependencies between the multiple time series. Following this approach, each variable’s time series\ny:𝑗can be made into a node in the graph. Then relationship information between the nodes could\nbe maintained as edge properties, for example, based on cross-correlation, mutual information, etc.\nNote that the strength of the, for example, cross-correlation would depend on the lag (typically\nhospitalization leads death by several days). Furthermore, if the data are not stationary, the cross-\ncorrelation pattern may change over time.\n4.2\nState-by-State COVID-19 Data\nThe spread of COVID-19 in a state can affect the neighboring states over time. This is because\npeople travel, trade, and socialize across states. To predict how COVID-19 will spread in a particular\nstate, we need to consider how it is connected to other states. We can represent the connections\nbetween states as a graph. In this graph, each state is a node, and there is an edge between two\nnodes if there is a significant connection between the two states. Many existing graph neural\nnetwork (GNN)-based models [91, 121] for predicting the spread of diseases use mobility data or\nsocial connections to connect different regions and capture the spatial relationships between them.\nFor example, if there is a rapidly increasing curve/peak developing in the state of New York, surely\nthe forecasting for New Jersey could benefit from having this information.\nIn addition, the study [99] shows that states can also be connected if they are linearly or non-\nlinearly dependent on each other. They calculated the correlation and mutual information between\nthe features of the states and found that this approach led to promising results. For example, the\nstudy found that the number of deaths and confirmed cases of COVID-19 in Ohio and Illinois are\nhighly correlated. This means that there is a strong linear relationship between the two states. As\nthe number of deaths and confirmed cases increases in one state, it also tends to increase in the\nother state.\nOne of the issues making this more complex is the fifty-fold expansion of the dataset. The dataset\nmay now be represented as a 3D tensor Y = [𝑦𝑡𝑗𝑘] where 𝑡is time, 𝑗is the variable, and 𝑘is the\nstate. For COVID-19 weekly data, the number of time points is approximately 200, the number of\nvariables before any encoding is around 10, and the number of US states around 50 (depending on\nwhether DC and US territories are included). The tensor would therefore have 100,000 elements, so\nthe number of possible dependencies is very large.\n4.3\nTypes of Graph Neural Networks\nEarly work was in the spectral domain that utilized Fourier transforms [50]. ChebNets offered a\nmore efficient computation involving the graph Laplacian. A Graph Convolution Network (GCN)\nsimplifies the calculations even more, by directly applying the Laplacian (tends to capture graph\nproperties well). A graph Laplacian is computed based on the graph’s adjacency matrix 𝐴with 𝐼\nfor self-loops and degree matrix 𝐷(𝑑𝑖is in-degree of node 𝑖). The hidden states of the nodes (e.g.,\nh𝑖for node 𝑖) are updated by multiplying a normalized graph Laplacian by their previous values\nand a learnable weight matrix. A Message Passing Neural Network (MPNN) is more general in that\nedge features can be included in the node update calculation where a hidden state is updated based\non a combinations of its previous value h𝑖and the messages m𝑗𝑖from its neighbors (a function of\nboth nodes and the features of the connecting edge with learnable weights). Utilizing an attention\nmechanism to compute attention weights 𝑎𝑖𝑗, a Graph Attention Network (GAT) has the potential\nto better capture the dependencies between nodes. Table 4 lists six common types of GNNs with\nthe first three being the simplest.\nIn utilizing GNNs for MTS forecasting, researchers have tried various ways to define the under-\nlying graph structure for the GNN. A static graph 𝐺is easier to deal with, but then the question\nis whether it is given a priori or needs to be learned from data (graph structure learning). If the\n, Vol. 1, No. 1, Article . Publication date: January 2024.\nA Survey of Deep Learning and Foundation Models for Time Series Forecasting\n13\nTable 4. Types of GNN Models (Sums are over Neighborhoods)\nModel Type\nShort Description\nUpdate for h𝑖\nDate\nGCN [56]\nGraph Convolution Network\nf(Í\n𝑗(𝑑𝑖𝑑𝑗)−1\n2𝑊h𝑗)\n2016\nMPNN [27]\nMessage Passing\nNeural Network\nf(h𝑖, Í\n𝑗m𝑗𝑖)\n2017\nGAT [119]\nGraph Attention Network\nf(Í\n𝑗(𝑎𝑖𝑗𝑊h𝑗))\n2017\nGraphSage [32]\nNeighborhood Sampling\nalgorithm 1 in [32]\n2017\nGIN [133]\nGraph Isomorphism Network\nequation 4.1 in [133]\n2018\nTGN [103]\nTemporal Graph Network\n(dynamic)\nembedding equation in [103] 2020\ngraph is dynamic, its topological structure changes over time. For time series, the graph structure\n(nodes/edges) would typically change a discrete points in time, i.e., the graph at time 𝑡, 𝐺𝑡.\nSection 4 in a survey of GNNs for time series forecasting [43] mentions two types of dependencies\nthat models need to handle: (1) modeling spatial or inter-variable dependencies and (2) modeling\ntemporal dependencies. GNNs are ideally suited for (1), but for (2) are often combined with recurrent,\nconvolutional, or attention-based models.\nSeveral studies have leveraged Graph Neural Networks (GNNs) for COVID-19 forecasting. Kapoor\net al. [49] used a spatial-temporal GNN to incorporate mobility data, capturing disease dynamics at\nthe county level. Panagopoulos et al. [91] introduced MPNN-TL, a GNN for understanding COVID-\n19 dynamics across European countries, emphasizing the role of mobility patterns. Fritz et al. [23]\ncombined GNNs with epidemiological models, utilizing Facebook data for infection rate forecasting\nin German cities and districts. Cao et al. [13] developed StemGNN, employing the Graph Fourier\nTransform (GFT) and Discrete Fourier Transform (DFT) to capture temporal correlations in a graph\nstructure representing different countries and forecasting confirmed cases across multiple horizons.\ntemporal modeling within a single deep neural network (DNN) block, eliminating the need for\nseparate treatment of neighboring areas and capturing both spatial and temporal dependencies. It\nalso presents a flexible architectural design by concatenating multiple DNN blocks, allowing the\nmodel to capture spatial dynamics across varying distances and longer-term temporal dependencies.\nCombining Transformers and Graph Neural Networks can provide the advantages of both. For\nexample, SageFormer [146] uses a graph representation and GNNs to establish the connections\nbetween the multiple series and as such helps focus the attention mechanism.\n5\nFOUNDATION MODELS\nA foundation model serves as a basis for more general problem solving. The term foundation model\nappeared in [8] with the general notion predating this. For example, one could view transfer learning\nas a precursor to foundation models. This paper argues that even though foundation models are\nbased on deep learning and transfer learning, their large scale supports broader applications and\nemergent capabilities, i.e., homogenization and emergence.\nIn forecasting, whether it be a traditional statistical model or a deep learning model, the main idea\nis to train the model for a particular dataset, so that it can pick up its specific patterns. Unfortunately,\nin many cases, there are not enough data available to train a complex model that has many trainable\nparameters. Data augmentation techniques [42] can help on the margins. At the beginning of\npandemics, the problem is severe. Just when the need the accurate forecasts is the greatest, the\namount of data is inadequate. This is where foundation models can show their value.\n, Vol. 1, No. 1, Article . Publication date: January 2024.\n14John A. Miller, Mohammed Aldosari, Farah Saeed, Nasid Habib Barna, Subas Rana, I. Budak Arpinar, and Ninghao Liu\nFoundation models with billions of parameters have recently shown remarkable success in the\nareas of natural language and computer vision. Naturally, other domains are investigating how\nfoundation models can work for other modes of data as well as for multi-modal data.\nAs time series is a type of sequential data, as is natural language, one might expect foundation\nmodels for time series to work as well as Large Language Models (LLMs) do for natural language.\nA foundation model, having been extensively trained, should more readily pick up the pattern in a\nnew dataset. There are several reasons why the problem is more challenging in the times series\ndomain:\n• Variety. Time Series data are being collected for many domains, many of which will have their\nown unique characteristics. The patterns seen in stock market data will be much different\nfrom Electrocardiogram (EKG/ECG) signals.\n• Many Small Datasets. Before the era of big data, time series data mainly consisted of short\nsequences of data and this makes it hard for deep learning models to gain traction. This\ncharacteristic of time series data will remain to a lesser degree over time.\n• Lack of Lexicon, Grammar, and Semantics. Any sequence of numbers can form a time series.\nThis is not the case with natural language as only certain sequences of lexical units are mean-\ningful, i.e., there is more structure to the patterns. Although time series may be decomposed\ninto trend, seasonal, and local patterns, the structural restrictions are not comparable.\n5.1\nBackbone Model/Architecture\nA foundation model can be built by scaling up a (or a combination of) deep learning model(s).\nExactly how this is done is the secret sauce of today’s highly successful Large Language Models\n(LLMs), such as GPT, BART, T5, LLaMA, PaLM, and Gemini [120]. A comparison of two recent\nmultimodal LLMs, OpenAi’s Chat-GPT4 and Google’s, is given in [77]. The efficiency of LLMs is\ndiscussed in [120].\nMultiple backbone models/architectures have been considered for foundation models in time\nseries classification [137]. This paper compared LSTM, ResNet, GRU, and Transformers architectures\nwith the Transformer architecture showing the most promise. For multivariate time series, models\nwith focused or sparse attention have shown greater accuracy (smaller errors) than transformers\nusing full attention. Furthermore, transformer-based backbone models may follow the encoder-\ndecoder architecture or may replace either the encoder or decoder with simpler components. The\ndebate now mainly centers around whether to use an encoder-decoder or decoder-only architecture\n[24]. Both types of transformers are represented by current state-of-the-art LLMs: Generative Pre-\ntrained Transformer (GPT) is decoder-only, while Bidirectional and Auto-Regressive Transformers\n(BART) and Test-to-Text Transfer Transformer (T5) are encoder-decoders.\nThere are additional architectures that may be considered for a backbone: (1) Transformer++\narchitecture extends self-attention to include convolution-based heads that allow tokens/words to\nbe compared with context vectors representing multiple tokens [114]. Half the heads use scaled-dot\nproduct attention with the other half using convolutional attention. This allows additional temporal\nor semantic dependencies to be captured. (2) State-Space Models, for example, Mamba combines\nelements from MLPs (of Transformers), CNNs, and RNNs, as well as classical state space models\n[30] to provide a more efficient alternative to plain transformers.\nAnother issue relevant to time series is whether to use channel-independent or cross-channel mod-\neling [33]. As discussed, PatchTST successfully utilized channel-independence. One form of cross-\nchannel modeling would be to consider cross-correlations between different channels/variables\n(e.g., hospitalizations and new deaths). As shown in Figure 1, results are likely to be better if\nlagged cross-correlations are used. There is also an issue of which variables/channels to include as\n, Vol. 1, No. 1, Article . Publication date: January 2024.\nA Survey of Deep Learning and Foundation Models for Time Series Forecasting\n15\nincluding many variables may be counter productive (analogs to the problem of feature selection\nin VAR and SARIMAX models).\nFor the spatial-temporal domain, the current research on Graph Foundation Models (GFMs)\n[69] becomes more relevant. Foundation models are typically scaled-up transformers and work\nwell for sequence data such as natural language and time series. Other types of deep learning\nmodels may be useful for data having a spatial component. Convolution Neural Networks and\nGraph Neural Networks intuitively match this type of data. Two types of architectures serve as the\nmost popular backbones: Message-passing-based GNNs [27] and transformer-based [138]. Several\nstudies adopt GAT, GCN, GIN, and GraphSage as their backbone architectures with GIN being\nparticularly favored due to its high expressive power. These GNNs are often integrated with RNNs\nto capture temporal dependencies within the data, and they can be scaled up to form a foundation\nmodel.\nThe success of transformers has given rise to the second type of backbone which is a hybrid of\ntransformers and GNNs. This method improves upon traditional message-passing GNNs by having\nstrong expressive power and the ability to model long-range dependencies effectively. GROVER\n[101] uses a GNN architecture to capture the structural information of a molecular graph, which\nproduces the outputs in the form of queries, keys, and values for the Transformer encoder. For a\nheterogeneous graph, researchers commonly employ the Heterogeneous Graph Transformer (HGT)\n[40] as the encoder.\n5.2\nBuilding a Foundation Model for Time Series\nThere are at least four approaches for creating foundation models for time series:\n(1) Use the power of an existing Large Language Model. This would involve converting time\nseries segments or patches to words, using these to generate the words that follow and\nthen converting back to time series (i.e., the forecasts). The basis for this working would be\nthe existence of universal patterns in the two sequences (words and time series segments).\nHowever, without care, the time series converted to a sequence of words are likely to produce\nmeaningless sentences. The same might happen when the output words are converted to\nthe time series forecast. Fine-tuning a Large Language Model using time series data may\nimprove their forecasting capabilities.\n(2) Build a general-purpose Foundation Model for Time Series from scratch using a huge number\nof time series datasets. This would be a large undertaking to collect and pre-process the large\nvolume of time series data. High performance computing would also be needed for extensive\ntraining. Although comprehensive training in the time series domain is generally considered\nto be less demanding than the language domain.\n(3) Build a special purpose Foundation Model for Time Series from scratch using datasets related\nto disease progression. This alternative is more manageable in terms of the volume of training\ndata needed and the training resource requirements. Also, it is unknown whether there exists\nexploitable universality across time series domains. Would a foundation model trained on\nstock market data be useful for pandemic prediction?\n(4) Create a Multi-modal Foundational Model that contains textual and time series data. For\nexample, the text could be from news articles or social media about the COVID-19 pandemic\nand the time series data (weekly/daily) could be from the CDC or OWID. The two would\nneed to be synchronized based on timestamps and using techniques such as Dynamic Time\nWarping (DTW) for time series alignment [86].\nVery recently, there have been several efforts to create foundation models for time series fore-\ncasting, as indicated in Table 5. The model type indicates which of the above four approaches are\n, Vol. 1, No. 1, Article . Publication date: January 2024.\n16John A. Miller, Mohammed Aldosari, Farah Saeed, Nasid Habib Barna, Subas Rana, I. Budak Arpinar, and Ninghao Liu\ntaken. The backbone indicates the base deep learning technique from the foundation model is built\nup.\nTable 5. Foundation Models for Time Series\nModel Type\nModel\nShort Description\nBackbone\nDate\n2\nTimeCLR [136, 137]\nContrastive Learning pre-training\nTransformer Jun 2022\n2\nTimesNet [129]\n2D variation modeling\nTimeBlock Oct 2022\n1\nGPT4TS [153]\nLLM with patch token input\nGPT-2\nMay 2023\n1\nLLM4TS [14]\nTemporal Encoding with LLM\nGPT-2\nAug 2023\n4\nUniTime [71]\nInput domain instructions + time series\nGPT-2\nOct 2023\n2\nTimeGPT [26]\nAdded linear layer for forecasting\nTransformer Oct 2023\n4\nTime-LLM [44]\nInput context via prompt prefix\nLLaMA\nOct 2023\n2\nPreDct [20]\nPatch-based Decoder-only\nTransformer Oct 2023\n2\nLag-Llama [100]\nLag-based Decoder-only\nLLaMA\nOct 2023\n3\nAutoMixer [90]\nAdds AutoEncoder to TSMixer\nTSMixer\nOct 2023\n1\nTEMPO [12]\nPre-trained transformer + statistical analysis\nGPT-2\nOct 2023\n1\nPromptCast [134]\nText-like prompts for time series with LLM\nGPT-3.5\nDec 2023\n5.2.1\nType 1: Repurposed LLMs. This type utilizes large language foundation models and\nrepurposes them for time series data. The LLM architecture designed for textual data is suited for\ntime series due to its sequential nature. Due to being pre-trained on large datasets using billions\nof parameters, it has shown satisfactory results with fine-tuning for specific language processing\ntasks such as question answering, recommendation, and others. In addition, these can also be\nfine-tuned for time series forecasting tasks related to disease, weather, and energy consumption\nforecasting. However, the transfer of pre-trained LLM to time series data has several requirements.\nLLMs require tokens as inputs. While a single point can be used as input, it cannot cover semantic\ninformation. Therefore, most of these models divide the time series into patches of a certain length.\nThese patches are considered as tokens which can be used as input to an LLM. To mitigate the\ndistribution shift, GPT4TS [153], LLM4TS [14] use reversible instance normalization (RevIN) [55].\nWhile GPT4TS [153] uses patch tokens as input to GPT, other methods enhanced the tokens with\nfurther encodings. LLM4TS [14] encodes temporal information with each patch and considers\nthe temporal details of the initial time step in each patch. Temporal encoding of input tokens is\nimportant for domain-specific models used for disease forecasting as data for certain viral infections\nlike COVID-19 show rise in holiday seasons and cold weather. TEMPO [12] decomposes the time\nseries into seasonal, trend and residual components. Additionally, it uses a shared pool of prompts\nrepresenting time series characteristics. The decomposed seasonal, residual and trend components\nare individually normalized, patched, and embedded. The embedded patches are concatenated with\nthe retrieved prompts before passing as input to a GPT block. Different from methods tokenizing\ntime series data, PromptCast [134] frames the time series forecasting as a question answering task\nand represents the numerical values as sentences. It uses specific prompting templates to apply\ndata-to-text transformations of time series to sentences.\nThe models GPT4TS [153], LLM4TS [14], TEMPO [12] use GPT as their backbone network, which\nis decoder only. However, these models are non-autoregressive and use a flattened and linear head\n, Vol. 1, No. 1, Article . Publication date: January 2024.\nA Survey of Deep Learning and Foundation Models for Time Series Forecasting\n17\non the last raw hidden state from the decoder to estimate chances for likely outcomes in a horizon\nwindow. However, this does not allow forecasting with varying horizon lengths which could be\nimportant for making timely decisions related to pandemic and epidemic control.\n5.2.2\nType 2: Broadly Pre-trained on Time Series Datasets. This type designs a model specifi-\ncally targeted for time series data and utilizes pre-training from scratch. TimeGPT [26] is among the\ninitial foundation models that use an encoder-decoder architecture with multiple layers containing\nresidual connections and layer normalization. The linear layer maps the output of the decoder to a\nhorizon window to estimate the likely outcome.\nAs a foundation model, TimeCLR [137] was pre-trained for time series classification. TimeCLR\nutilizes self-supervised learning for pre-training with a transformer backbone. The method adopts\na contrastive learning pre-training method, building on the existing method, SimCLR. Several\ntime series data augmentation techniques were utilized including jittering, smoothing, magnitude\nwarping, time warping, circular shifting, adding slope, adding spikes, adding steps, masking, and\ncropping. These additional techniques enhance the pre-training process by allowing the model to\nlearn more invariance properties. The overall architecture consists of a backbone and projector.\nThe backbone has a transformer architecture and the projector consists of linear and ReLU layers.\nFor fine-tuning, a classifier model was added on top of the projector, and the backbone, projector,\nand classifier model were updated using cross-entropy loss.\nAnother pre-trained foundation model, PreDct [20] utilizes a decoder-only architecture in an\nautoregressive manner to allow estimation of likely outcomes for varying history lengths, horizon\nlengths and temporal granularities. The input is pre-processed into patches using residual layers.\nThe processed patches are added to positional encoding before being passed as input to the network.\nThe network consists of a stack of transformer layers where each transformer layer contains\nself-attention followed by a feed-forward network. It uses causal self-attention, that is each token\ncan only attend to tokens that come before it and trains in decoder-only mode. Therefore, each\noutput patch token only estimates for the time period following the last input patch corresponding\nto it. While these methods use a certain patch length, different patch lengths may be optimal\nfor different time series data. For example, COVID-19 and other viral infections show periodicity\nover extended time periods, whereas, weather data demonstrate daily variations. To identify this\nperiodicity, TimesNet [129] uses the Fast Fourier Transform to discover the optimal periods and\nstack these in a column-wise manner to represent the input in a 2D format. In the 2D format the\ninter-period and intra-period features may be estimated using 2D kernels/filters from existing\ndeveloped models such as ConvNext, CNN, DenseNet, and others.\nAnother model, Lag-Llama [100] is built using LLaMA that features accuracy with a reduced\nnumber of parameters, normalization with RMSNorm [143] and RoPE [112] as well as SwiGLU [106]\nactivation and optimization using AdamW [116]. (Root Mean Squared Norm (RMSNorm) divides by\nthe RMS and serves as a faster alternative to LayerNorm that subtracts the mean and divides by the\nstandard deviation. Rotary Positional Embedding (RoPE) is a form for relative position encoding\n(used for self-attention) that encodes the absolute position with a rotation matrix and incorporates\nthe explicit relative position dependency in self-attention formulation. SwiGLU is a smoother\nreplacement for the ReLU activation function that combines Swish (𝑥sigmoid(𝑥)) with Gated\nLinear Units. AdamW improves upon the Adam optimization algorithm by utilizing decoupled\nweight decay regularization.) The inputs are based on selected lags that may include, for example,\nseasonal effects. This makes it different from using patches. It performs probabilistic forecasting to\nestimate a distribution for the next value(s). Alternatively, one could focus on point and/or interval\nforecasting. A current limitation is that it only works on univariate time series (further innovations\nwill be needed for multivariate time series). Additionally, the longer context window reduces\n, Vol. 1, No. 1, Article . Publication date: January 2024.\n18John A. Miller, Mohammed Aldosari, Farah Saeed, Nasid Habib Barna, Subas Rana, I. Budak Arpinar, and Ninghao Liu\nefficiency and increases memory usage due to increased evaluations in attention modules. However,\nthe longer context window enables models to process more information, which is particularly\nuseful for supporting longer histories in case of diseases like influenza. A follow-up version, Llama\n2 [117], works towards improving efficiency. It is mostly like Llama 1 in terms of architecture and\nuses the standard transformer architecture and uses pre-normalization using RMSNorm, SwiGLU\nactivation function, and rotary positional embeddings. Its two primary differences include doubling\nthe context length and using grouped-query attention (GQA) [1] to improve inference scalability.\nThe context window is expanded for Llama 2 from 2048 tokens to 4096 tokens which enables\nmodels to process more information. For speeding up attention computation, the standard practice\nfor autoregressive decoding is to cache the key (K) and value (V) pairs for the previous tokens\nin the sequence. With doubled context length, the memory costs associated with the KV cache\nsize in attention models grow significantly. As KV cache size becomes a bottleneck, key and value\nprojections can be shared across attention heads without much degradation of performance. For\nthis purpose, a grouped-query attention variant with 8 KV projections is used. Grouped-query\nattention (GQA), a generalization of multi-query attention, uses an intermediate (more than one,\nless than the number of query heads) number of key-value heads to reduce memory usage.\n5.2.3\nType 3: Pre-trained on Domain-related Time Series Datasets. This type of model is pre-\ntrained on domain-related data. Each domain has different characteristics related to seasonality and\ntrend. For example, pandemic-related data show an increasing trend in the initial days of a disease\noutbreak, while energy consumption fluctuates greatly within a year. Therefore, pre-training on a\nspecific domain may provide improved performance. Among special purpose foundation models,\nAutoMixer [90] was trained for business and IT observability. Different from other foundation\nmodels, it poses channel compression as a pre-training task. It proposes to project the raw channels\nto compressed channel space where unimportant channels are pruned away, and only important\ncorrelations are compactly retained. RNN-based AutoEncoder (AE) handling variable input and\noutput sequence lengths is used for pre-training. For fine-tuning, the input is compressed using the\nencoder part of pre-trained AE. Afterwards, the compressed representation is passed to TSMixer\nwhich is trained from scratch. The output from TSMixer is passed as input to the decoder part of\nthe AE to get the results for the horizon window.\n5.2.4\nType 4: Multimodal with Text and Time Series. Previous work has shown how in-\nformation from news or social media can improve forecasting [89]. Most of this work required\nfeature engineering, for example, using sentiment analysis scores to improve sales or stock market\npredictions. However, multi-modal foundation models provide greater potential and increased\nautomation.\nType 4 models utilize both textual and time series data to improve forecasting accuracy and\nprovide greater potential for explainability. In the case of pandemic estimates, a model trained on\nboth disease outbreaks and additional textual information about vaccination development may\nenhance the results for future diseases. Time-LLM [44] introduces a multimodal framework utilizing\na pre-trained large language foundation model. The input time series was tokenized and embedded\nvia patching and a customized embedding layer. These patch embeddings are then reprogrammed\nwith condensed text prototypes to align two modalities. Additional prompt prefixes in textual\nformat representing information about input statistics are concatenated with the time series patches.\nThe output patches from the LLM are projected to generate the forecasts.\nUniTime [71] allows the use of domain instructions to offer explicit domain identification\ninformation to the model. This facilitates the model to utilize the source of each time series and\nadapt its forecasting strategy accordingly. Specifically, it takes the text information as input and\ntokenizes it as done in most language processing models. In addition, it masks the input time series\n, Vol. 1, No. 1, Article . Publication date: January 2024.\nA Survey of Deep Learning and Foundation Models for Time Series Forecasting\n19\nto alleviate the over-fitting problem. A binary indicator series is formed representing the masked\nand unmasked locations. Both the masked time series and binary indicator series are tokenized\nthrough patching, embedded into a hidden space, and fused through gated fusion (for optimized\nmixing of signals). The fused patch tokens and the text tokens are concatenated before passing\nas input to the language model decoder. The output tokens from the language model decoder\nare padded to a fixed sequence length. The padded result is then passed through a lightweight\ntransformer. To allow variable horizon lengths, a linear layer is afterward utilized with a maximum\nlength parameter to generate predictions. The model always outputs that number of values, which\nmay be truncated to get estimates for a certain horizon window.\n5.3\nPre-Training Foundation Models\nAs pointed out in [28], there is a paradigm shift in time series forecasting from having a model\ntrained for each dataset, to having a model that is useful for several datasets. This paradigm shift\nnaturally leads to foundation models for time series forecasting that are trained over a large number\nof datasets. As discussed in the next subsection, the accuracy of such models can be improved by\nfine-tuning.\nPre-training of a foundation model for time series is challenging due to the diversity of data, but\neasier in the sense that the volume and dimensionality are less than for LLMs. Finding enough\ndatasets is another problem to deal with. As a partial solution, the following repositories include\ntime series datasets from multiple domains:\n• The Monash Time Series Forecasting Repository [28] contains 30 dataset collections many\nof which contain a large number of time series (summing to a total of 414,502 time series).\nhttps://forecastingdata.org/\n• The University of California, Riverside (UCR) Time Series Classification Archive [21] contains\n128 datasets. Its focus is on univariate time series classification. https://www.cs.ucr.edu/~eam\nonn/time_-series_-data_-2018/\n• University of East Anglia (UEA) Repository [4] contains 30 datasets. Its focus is on multivariate\ntime series classification.\nIn the time series domain, Self-Supervised Learning [144] can be utilized for large scale training\nto deal with the lack of labeled data. As time series forecasting is not dependent on labels anyway,\nhow would self-supervised learning differ from standard training? It can enhance it: For example, as\na pretext subtask, a portion of a time series may be masked out and regenerated. The thought being\nis that doing so can help the model (especially a foundation model) make accurate forecasts. Careful\nuse of data augmentation may be beneficial, so long as it does not interrupt complex temporal\npatterns. Furthermore, adding and removing noise from the time series may help the model to\nsee the true patterns. Of course, self-supervised learning is essential for other time series subtasks\nsuch as time series classification and anomaly detection. Even if forecasting is the goal, training on\nrelated subtasks is thought to improve the overall capabilities for foundation models.\nNeural Scaling Laws [48] for LLMs indicate that error rates drop following a power law that\nincludes training set size and number of parameters in the model. As these get larger, the demand\nfor computing resources and time naturally increase. To reduce these demands, [110] has shown\nthat by using a good self-supervised pruning metric to reduce training samples, for example, one\ncould obtain a drop “from 3% to 2% error by only adding a few carefully chosen training examples,\nrather than collecting 10× more random ones.\" Exactly, how this translates to time series forecasting\nis still an open research problem.\n, Vol. 1, No. 1, Article . Publication date: January 2024.\n20John A. Miller, Mohammed Aldosari, Farah Saeed, Nasid Habib Barna, Subas Rana, I. Budak Arpinar, and Ninghao Liu\n5.4\nFine-Tuning Foundation Models\nAs foundation models have a large number of trainable parameters that are optimized using a huge\namount of data requiring high performance computing over extended periods of time, how can\nthey be generally useful for time series forecasting?\nThe idea of fine-tuning a foundation model is to make small adjustments to the parameters to\nimprove its performance for a particular sub-domain. For example, a foundation model trained on\ninfectious diseases could be fine-tuned using COVID-19 datasets.\nA foundation model that follows a transformer architecture has trainable parameters for its\nself-attention mechanism and in its multiple fully connected layers. One option would be to re-train\nthe final layers of the last few blocks of the transformer, freezing the rest of the parameters. One\nmay also re-train the attention weights. The best combination to provide efficient fine tuning is an\nongoing research problem. A lower cost solution is to only re-train the attention weights.\nThe goal of Parameter Efficient Fine-Tuning (PEFT) [37, 64] techniques is to increase the accuracy\nof the pre-trained model as efficiently as possible. Three common approaches are listed below.\n• Sparse Fine-Tuning provides a means for choosing which parameters to fine-tune based on,\nfor example, changes in parameter values or values of gradients. The majority of parameters\nremaining frozen.\n• Adapter Fine-Tuning adds new trainable weight matrices, for example, after each feed-forward\nneural network in a transformer, one (𝑊(𝑑𝑛)) to project down from the dimension of the\ntransformer model𝑑to a lower dimension 𝑟, and the other (𝑊(𝑢𝑝) restoring back to dimension\n𝑑. Given a hidden state vector h, it will be updated to the following value.\nh + 𝑓(h𝑊(𝑑𝑛))𝑊(𝑢𝑝)\n(12)\nFine-tuning only changes 𝑊(𝑑𝑛) and 𝑊(𝑢𝑝) with all the other parameters being frozen.\n• Low Rank Adaptation (LoRA) [38] is similar to adapter fine-tuning, but is integrated into\nexisting layers, e.g., given a linear layer with computation h𝑊, 𝑊(𝑑𝑛) and 𝑊(𝑢𝑝) are added\nas follows:\nh𝑊+ (h𝑊(𝑑𝑛))𝑊(𝑢𝑝)\n(13)\nThe advantage of LoRA is through pre-computation, its inference speed is the same as with\nfull fine-tuning. A limitation of this technique is that it cannot be applied to a unit having a\nnonlinear (activation) function.\nThree common ways of improving the accuracy of foundational models for particular domains\nare Fine-Tuning (FT), Retrieval Augmented Generation (RAG), and Prompt Engineering (PE). Used\nin combination the effect can be substantial [25], e.g., hallucinations in LLM can be reduced and the\ntimeliness of answers increased. Retrieval Augmented Generation is facilitated by maintaining rapid\naccess to continually updated sources of information for example stored in relational databases or\nknowledge graphs. Prompt Engineering supplements a query with relevant information to make\nthe foundation model aware of it. RAG can be used to support building the prompts. RAG can also\nhelp with and facilitate the fine-tuning of foundation models.\n6\nUSE OF KNOWLEDGE\nData-driven methods have made great strides of late, but may still benefit by using accumulated\nknowledge. Even the remarkably capable recent Large Language Models improve their responses\nusing knowledge. For Pandemic Prediction, knowledge about the disease process and induced from\nprevious studies can improve forecasting models. Knowledge about the future based on industrial\n, Vol. 1, No. 1, Article . Publication date: January 2024.\nA Survey of Deep Learning and Foundation Models for Time Series Forecasting\n21\nor governmental policies can be very useful in forecasting, e.g., schools or stores will open in two\nweeks, a mask mandate will start next week, etc.\nThe use of knowledge has been a goal that has been pursued for a long time for time series\nforecasting. For example [105] used 99 rules based on causal forces to select and weigh forecasts.\nThen, the causal forces (growth, decay, supporting, opposing, regressing) were specified by the\nanalyst (but could be learned today). Another direction for applying knowledge is situational\nawareness [93]. Knowledge can be useful in feature selection either for improved forecasting or\ngreater interpretability. It can be used in model checking, e.g., in a pandemic the calculated basic\nreproduction number 𝑅0 based on the forecast is outside the feasible range.\nTo improve the forecasting of fashion trends, the authors of [73] have developed the Knowledge\nEnhanced Recurrent Network (KERN) model and shown that the incorporation of knowledge into\nthe model has increased its forecasting accuracy. The base model follows an LSTM encoder-decoder\narchitecture to which internal knowledge and external knowledge are added. For example, they\ndevelop close-far similarity relationships for trend patterns as internal knowledge (alternatively\ncould be taken as a different view of the data) to create a regulation term to add to the loss function.\nAs external (or domain) knowledge, they utilize a fashion element ontology (taxonomy and\npart-of relationships). Then if sales of a dress part (e.g., peplum) go up, it would be likely that the\nsales of dresses would go up. This external knowledge is incorporated via the embedding of the\ninputs that are passed to the encoder. The authors note the improvements due to adding knowledge,\nparticularly for longer-term forecasting.\n6.1\nCOVID-19 Knowledge Graphs\nThere are many large Knowledge Graphs available that are built as either Resource Description\nFramework (RDF) Graphs or Labelled Property Graphs (LPG). The RDF graphs consist of triples\nof the form (subject, predicate, object) and LPG graphs can be mapped to triples. A Temporal\nKnowledge Graph (TKG) may be viewed as a collection of quads (𝑠, 𝑝,𝑜,𝑡) meaning that predicate\n𝑝applied to subject 𝑠and object 𝑜is true at time 𝑡. Intervals may be used as well (𝑠, 𝑝,𝑜, [𝑡1,𝑡2]).\nTable 6 lists some available knowledge graphs that contain information about COVID-19.\nSeveral utilize CORD-19. The Allen Institute for AI has made available a large collection of\nresearch papers on the COVID-19 Pandemic: COVID 2019 Open Research Dataset (CORD-19)\ndataset, or in RDF, CORD-19-on-FHIR: Linked Open Data version of CORD-19.\nTable 6. COVID-19 Knowledge Graphs\nKG\nType\nBase\nDate\nCOVID-19-Net [102]\nLPG/Neo4j\nmany\n2020\nCovid-on-the-Web [79]\nRDF/Virtuoso\nCORD-19\n2020\nCord19-NEKG [79]\nRDF/Virtuoso\nCORD-19\n2020\nCOVID-KG [111]\nRDF\nCORD-19\n2020\nCovidGraph [31]\nLPG/Neo4j\nmany\n2022\nCovidPubGraph [94]\nRDF/Virtuoso\nCORD-19\n2022\nCOVID-Forecast-Graph [155]\nRDF/OWL\nCOVID Forecast Hub\n2022\nSo far, there is little work on building Temporal Knowledge Graphs (TKGs) for COVID-19, even\nthough they match up well with forecasting. The approach taken by Temporal GNN with Attention\nPropagation (T-GAP) [47] could be used to build a TKG for COVID-19. T-GAP performs Temporal\n, Vol. 1, No. 1, Article . Publication date: January 2024.\n22John A. Miller, Mohammed Aldosari, Farah Saeed, Nasid Habib Barna, Subas Rana, I. Budak Arpinar, and Ninghao Liu\nKnowledge Graph Completion that can fill in missing information as well as make forecasts. The\nGNN uses information from the TKG based on the current query, as well as Attention Flow (multi-\nhop propagation with edge base attention scores), to make TKG completion more accurate This\napproach also improves the interpretability of the model.\n6.2\nTemporal Knowledge Graph Embedding\nTemporal Knowledge Graph Embedding (TKGE) can be used for link prediction and if time is in\nthe future, it involves forecasting. TKGE represents graph elements in latent vector spaces with\nrelationships (including temporal ones) determining the relative positions of the vectors. There is a\ngrowing list of TKG embedding techniques including, TAE, TTransE, Know-Evolve, TA-TransE, TA-\nDistMult, DE-SimplE, TEE, and ATiSE, with the last one including time series decomposition [132].\nIt is an open question as to what degree these vectors (as knowledge of temporal relationships) could\nimprove other deep learning forecasting models. The possible synergy between link prediction in\nTKGs and MTS forecasting needs further exploration.\n6.3\nIncorporation of Knowledge\nThere a several ways that knowledge can be incorporated into a deep learning model:\n(1) Composite Loss Function: e.g., (1 −𝜆)∥y −ˆy∥𝑝+ 𝜆∥z −ˆy∥𝑝where y is the actual time series,\nˆy are the predicted values and z are predictions from a theory-based or simulation model.\n(2) Applying Constraints: e.g., ∥y−ˆy∥𝑝+𝜆𝑓𝑐(ˆy) where 𝑓𝑐is a penalty function based on constraint\nviolation. Depending on the form of the constraint, it could be viewed as regularization.\n(3) Factored into Self-Attention Mechanism: From previous studies, controlled experiments, or\ntheory, the relevance [5] of 𝑦𝑡𝑗to 𝑦𝑡−𝑙,𝑘could be maintained, for example, in a temporal\nknowledge graph (variable 𝑗to 𝑘with lag 𝑙) and used to focus or modify self-attention\ncalculations.\n(4) Embedded and Combined with Input: A sub-graph of a COVID-19 (Temporal) Knowledge\nGraph would produce embedded vectors that would be combined (e.g., concatenated) with\nthe input multivariate time series (e.g., raw or patch level).\n(5) Injected into a Downstream Layer: Determining the ideal place to combine knowledge with\ninput data or latent representations thereof is challenging. For models based on represen-\ntation learning that map x𝑡to z𝑡, it could happen anywhere in the process before the final\nrepresentation is created.\n(6) Knowledge Influencing Architecture: A sub-graph of a COVID-19 (Temporal) Knowledge\nGraph could also be used as a draft architecture for GNN.\n6.4\nKnowledge Enhanced Transformers\nUse of future knowledge is exploited by Aliformer by modifying the transformer self-attention\nmechanism [95]. In the e-commerce domain, they consider two types of future knowledge: produce\nrelated and platform-related.\nThere is ongoing research on the use of knowledge for the improvement of Large Language\nModels. Pre-trained Language Models (PLM) are typically large transformers that are extensively\ntrained and then fine-tuned, and include BERT, Generative Pre-trained Transformer (GPT), Bidi-\nrectional and Auto-Regressive Transformers (BART), and Test-to-Text Transfer Transformer (T5)\n[83]. These models can be enhanced with knowledge: The survey in [135] discusses how symbolic\nknowledge in the form of entity descriptions, knowledge graphs, and rules can be used to improve\nPLMs. A key question is how to design an effective knowledge injection technique that is most\nsuitable for a PLM’s architecture.\n, Vol. 1, No. 1, Article . Publication date: January 2024.\nA Survey of Deep Learning and Foundation Models for Time Series Forecasting\n23\nMultivariate Time-series forecasting is a critical aspect of pandemic forecasting. As of yet, an\naccurate forecasting model may not be built solely using an LLM. Fine-tuning using pandemic\nliterature and prompt design can help LLM improve its forecasting capability. Still, it can be highly\nbeneficial for another model that specializes in capturing temporal patterns in MTS COVID-19\ndatasets to be applied. The LLM can be used to improve the MTS model or be used in conjunction\nwith it.\nJust as knowledge graphs can be used to enhance PLM’s performance on language tasks, they can\nalso be used to improve the accuracy and explainability of transformers used for MTS forecasting.\nRecent literature suggests that traditional attention may not be necessary for capturing temporal\ndependencies [63]. Therefore, we envision a multi-model approach to pandemic forecasting in which\nspecialized models on language, temporal pattern and knowledge understating and processing\ncooperate with each other to produce accurate MTS pandemic forecasting.\n6.5\nKnowledge Enhanced Graph Neural Networks\nOne way to add knowledge to a model is to incorporate forecasting results from science-based\nforecasting models. For example, [122] improved its Particulate Matter (PM2.5) GNN forecasting\nmodel by using weather forecasting results from the National Centers for Environmental Prediction\n(NCEP)’s Global Forecast System (GFS) and climate forecasts from the European Centre for Medium-\nRange Weather Forecasts (ECMWF)’s ERA5.\nThe Knowledge Enhanced Graph Neural Network (KeGNN) [124] illustrates another way to\napply knowledge is by using a logic language. It supports unary predicates for properties and binary\npredicates for relations. The logic is mapped into real-valued vectors and functions. A knowledge\nenhancement layer takes predictions (classification problem) from a GNN and produces updated\npredictions based on how well the logic is satisfied.\nThere is substantial research on using GNNs for Knowledge Graph Completion, but little work\nin the opposite direction, creating GNNs from Knowledge Graphs [67]. For example, our research\ngroup has a project to utilize knowledge graphs about COVID-19 to improve pandemic prediction.\nLarge Language Models can extract information from the scientific literature on COVID-19, e.g., the\nCOVID-19 Open Research Dataset Challenge (CORD-19). https://www.kaggle.com/datasets/allen-\ninstitute-for-ai/CORD-19-research-challenge. Knowledge Graph Embeddings (KGEs) can be\nused to transfer the knowledge into the Forecasting Transformer. Knowledge Embeddings may be\nconcatenated with the input or at some later stage of the transformer (referred to as knowledge\ninfusion) [39]. The self-attention mechanism of transformers can help to select the most useful\nknowledge.\n7\nMETA EVALUATION\nMulti-horizon forecasting is an important and challenging task, fraught with uncertainty. Forecasts\ntend to degrade the longer the forecasting horizon. The feasible horizon (the distance in the future\na model can see before it degrades to sheer speculation) varies with the domain and on what is\nbeing forecast. For example, forecasting the weather (e.g. high and low daily temperature) for\na particular city, ten years from now is nonsense, while forecasting the monthly Global Mean\nSurface Temperature (GMST) or Pacific/Atlantic Sea Surface Temperature (SST) with a horizon of\n120 months can be done with reasonable accuracy with climate models [29]. Many of the papers\naddress the problem of Long Sequence Time series Forecasting (LSTF).\n7.1\nForecast Quality Metrics\nThere are several metrics that can be used to evaluate the quality of a model’s forecasts as shown\nin Table 7. Given vectors y, the actual observed values, and ˆy, the forecasted values, the following\n, Vol. 1, No. 1, Article . Publication date: January 2024.\n24John A. Miller, Mohammed Aldosari, Farah Saeed, Nasid Habib Barna, Subas Rana, I. Budak Arpinar, and Ninghao Liu\npreliminary definitions are needed: (1) A measure of variability of a time series, var(y), (2) Random\nWalk (RW@h1) simply guesses the previous/latest value as the forecast and performs reasonably\nwell for horizon one forecasting, and thus can be used as a standard to measure other models.\nTable 7. Forecast Quality Metrics\nMetric\nMeaning\nFormula\nMSE\nMean Squared Error\nmean ((y −ˆy)2)\n𝑅2\nCoefficient of Determination\n1 −MSE / var (y)\nRMSE\nRoot Mean Squared Error\n√\nMSE\nNRMSE\nNormalized RMSE\nRMSE/mean (y)\nMAE\nMean Absolute Error\nmean (|y −ˆy|)\nMAPE\nMean Absolute Percentage Error\n100 mean\n\u0012 |y −ˆy|\n|y|\n\u0013\nsMAPE\nsymmetric MAPE\n200 mean\n\u0012 |y −ˆy|\n|y| + |ˆy|\n\u0013\nMASE\nMean Absolute Scaled Error\nMAE(model)\nMAE(RW@h1)\nNotes: In the MAPE formula the mean/sum of ratios of the absolute values is the same as the\nmean/sum of absolute values of ratios. MSE, RMSE, and MAE require knowledge of the domains\nand their units to be interpretable. NRMSE will blow up to infinity when the mean is zero, in\nwhich case the alternative definition of dividing by the range could be used (but it is strongly\naffected by outliers). 𝑅2 is thought to be less informative than it is for regression problems, due to\ntemporal dependencies and changing values for variability over different time intervals. MAPE\nhas a tendency to blow up to infinity due to an observed zero value. MAPE and sMAPE vary with\nunits, for example, changing from Celsius to Kelvins will make the errors look smaller. MASE [41]\nis a scale/unit invariant metric where a value of 1 means the model is par with RW@1, less than\n1 means it is better, and greater than 1 means it is worse (but a MASE = 2 for horizon 10 may be\nquite good).\n7.2\nTesting the Quality of Models\nBased on the paradigm of train and test, the ideal of 𝑘-fold cross-validation is not available for\ntime series due to temporal dependencies. One should train on a beginning portion of the data and\ntest points past the training portion. A naïve way to do this would be to divide the dataset/time\nseries, say 60%-40%, and train on the first 60% time points (to obtain values for model parameters)\nand use these parameters along with training-only data to make very long horizon forecasts, all\nthe way through the testing set. The forecasts will degrade because of staleness of (1) data and (2)\nparameter values. The first problem can be fixed, by having a forecast horizon ℎless than the size\nof the testing set. Once forecasting through a window from 1 to ℎis complete, move the window\nahead one time unit and redo the forecasts, maintaining all the forecasts in a matrix where the\nrow gives the time (e.g., the date) and the column gives the horizon (e.g., the number of units/days\nahead). The redo will add one data point from the testing set to the values available for forecasting,\nso the available information is freshened. This leaves the staleness of the parameters which can\nbe addressed by retraining, by establishing a retraining frequency, say for every 10𝑡ℎwindow, to\nretrain the parameters. The training set would typically drop its first value and add the first value\nfrom the testing set. For simple models, the retraining frequency can be high (e.g., up to every time\n, Vol. 1, No. 1, Article . Publication date: January 2024.\nA Survey of Deep Learning and Foundation Models for Time Series Forecasting\n25\nthe window rolls), but to reduce high computational costs for complex models, the frequency could\nbe set lower. Also, complex models may use incremental training to avoid training from scratch\nevery time. Such techniques that reduce the staleness of data and/or parameters are referred to as\nrolling validation.\nFor Foundation Models, more options are available as they come to the problem already pre-\ntrained. One option would be to maintain the same process for rolling validation and simply replace\ntraining with fine-tuning.\n7.3\nSummary of Quality Findings from the Literature\nA meta-evaluation summarizes and establishes a consensus from the results from relevant published\nliterature. This subsection summarizes results from recent papers that compare MTS deep learning\nand foundation models on the following datasets: Electricity Transformer Temperatures (ETT),\nInfluenza-Like Illness (ILI), and Electricity Demand, Weather, and Traffic.\n(1) ETTh1,2 contains reporting of six electrical power loads, such as High Useful Load (HUFL)\nand High Useless Load (HULL), and oil temperature of electricity transformers. The data are\nrecorded hourly for two transformers (1 and 2) in two counties in China over two years.\n(2) ETTm1,2 also contains reporting of power loads and oil temperature of electrical transformers\nrecorded every 15 minutes. Similar to the ETTh1,2, ETTm1 includes the data for the first\ntransformer, while ETTm2 contains the data for the second transformer.\n(3) ILI contains weekly reporting of patients with symptoms related to influenza. Each data\ntime point includes variables, such as the age group, the number of reporting providers, and\nweighted and unweighted ILI cases. The data are maintained by the Centers for Disease\nControl and Prevention (CDC) of the United States.\n(4) Electricity contains the hourly electricity consumption of 321 customers. The columns of the\nelectricity dataset represent the customers. Unlike the other datasets with multiple variables\nrecorded concurrently, the electricity dataset has only one variable: the hourly consumption\nrecorded hourly for each customer. The data are sourced from Elergone Energia in Portugal.\n(5) Weather contains weather-related data, such as moisture level, CO2 level, and amount of\nrain, among many other variables. The data are maintained by the Max Planck Institute and\ncollected by a weather station installed on the top roof of the Institute for Biogeochemistry\nin Jena, Germany.\n(6) Traffic contains a collection of road occupancy rates of a highway in San Francisco, California.\nSimilar to the electricity dataset, the columns of the traffic dataset represent the 862 sensors\ninstalled on the road; it does not include any other variables, such as weather conditions,\nwhich may be helpful. The occupancy rates are maintained by the Caltrans Performance\nMeasurement System (PeMS) and are collected every hour.\nTo help ensure a fair comparison, every new proposed model uses the codebase made public by\nthe Informer model [150] or one of the subsequent models based on it. This often involves using the\nsame data-loading, transformation, and evaluation experimental setup. One, however, can observe\nsome experimental or modeling differences, which can be summarized as follows:\n• Look-back window: Early works on utilizing the Transformer model for forecasting used a\nrelatively short look-back window, such as using a 96 look-back window to forecast 96, 192,\n336, and 720 horizons for ETT datasets. The authors of [140] explored increasing the look-back\nwindow for the Transformer-based models and found that the forecasting performance tends\nto degrade, falling short in utilizing the extended long temporal information. Alternatively,\nthey showed that the PatchTST model benefits from the longer look-back window.\n, Vol. 1, No. 1, Article . Publication date: January 2024.\n26John A. Miller, Mohammed Aldosari, Farah Saeed, Nasid Habib Barna, Subas Rana, I. Budak Arpinar, and Ninghao Liu\nFig. 3. Percent of Weekly Patient Visits Exhibiting Influenza-Like Illness (ILI): Training (red), Testing (blue),\nRW (orange)\n• Channel Mixing vs. Channel Independence: In channel mixing, the attention is applied to all\nthe features projected into a shared embedding space. In channel-independence, the attention\nis applied separately to time points (or patches) in each channel, as in PatchTST, GPT4TS, and\nTime-LLM. One argument in favor of channel-independence is that each channel behaves\ndifferently, which can harm the overall performance when mixing all the channels. In addition,\nthe authors of [145] explored using two-stage attention: one for temporal attention and the\nother for cross-channel attention.\n• Temporal Embeddings: For both cases (channel mixing and channel-independence), a model-\ning technique projects the features into an embedding space, and it can also use additional\npositional or temporal embeddings designed explicitly for each time frequency, such as\nweekly or hourly data. For instance, the Informer model utilizes three types of embeddings:\nfeature, positional, and temporal embeddings. The PatchTST model uses two types: feature\nand positional. The GPT4TS model utilizes only one: the feature embeddings.\n• Instance Normalization: Some modeling techniques also utilize the Reversible Instance Nor-\nmalization (RevIN) [55], mitigating the distribution shift between the look-back window\nand the forecasting horizons. This technique simply normalizes the look-back window by\nsubtracting the mean and dividing it by the standard deviation. Subsequently, a modeling\ntechnique is trained on those normalized look-back windows to generate the forecasting\nhorizons. These forecasts then go through a denormalizing step to get the final forecasts for\nevaluation. This simpler technique has been proven effective and contributes to much of the\ngain of the PatchTST model and GPT4TS. For interested readers, please refer to Table 11 in\nthe PatchTST paper for a detailed experiment on PatchTST with and without RevIN.\n• Layer Normalization vs. Batch Normalization: Most Transformer-based modeling techniques,\nsuch as the Informer or Time-LLM models, use Layer Normalization for the attention heads.\nThe PatchTST models, however, use batch normalization, which has been shown to improve\nthe forecasting performance of the time series transformers [141].\n• Residual Attention: In addition to the layer vs. batch normalization, the implementation of the\nattention block also varies across the Time Series Transformers. For example, the PatchTST\nmodel uses residual attention, which keeps attention scores and adds them to the attention\nscores in the next layer. The other models do not use any intermediate attention scores.\n, Vol. 1, No. 1, Article . Publication date: January 2024.\nA Survey of Deep Learning and Foundation Models for Time Series Forecasting\n27\n• Model Size: The modeling techniques under study vary substantially in the model size (model\ndimensions and layers) used for modeling. For example, the Informer uses two-layer attention\nblocks with the following parameters (dmodel=512, nheads=8, and dff=2048) for the ETTh1.\nPatchTST uses a three layers attention block with these parameters (dmodel=16, nheads=8, and\ndff=128). GPT4TS uses pre-trained GPT2 of the six layers with these parameters (dmodel=768,\nnheads=4, and dff=768) for the same dataset.\n• On Baselines: To some extent, the modeling techniques are limited in their evaluation since\nthey do not include simpler baselines that can sometimes be competitive, such as the Random\nWalk (RW), Mean Model (MM), or Simple Moving Average (SMA). These simple baselines\nserve as good starting points as they do not require any training, although SMA has a hyper-\nparameter (the window size or number of elements to average). For example, Figure 3 shows\nthe Weighted (by age) Percentage of Patients with an ILI (for 966 time points) that has been\ndivided into train (772) and test (194) sets. The forecasts (orange) are from the simple Random\nWalk (RW) baseline model and it tends to degrade with increasing forecasting horizons,\nunless it happened to match a cycle/period. In this case, the metrics for horizon = 24 weeks\nare [ N = 170, MSE = 10.046, MAE = 2.473, sMAPE = 89.490 ]. Note, these results are\non the original scale as sMAPE values are distorted when data are standardized (or normalized).\nA note on reproducibility, since the baseline results are often collected in each new proposed\nmodel without actually rerunning the other models, it has been observed that some results\nare not reproducible by the research community in the GitHub repository of the models under\nstudy (although the results (e.g., MAE) tend to be close). In addition to the reproducibility,\nsome researchers also observed a source of potential unfairness in the evaluations, as some\ntest time steps are omitted during the data loading where some models use different batch\nsizes (e.g., skip the last incomplete batch). Although the effects are small, it would simply\nfollow-on work if a standard could be followed. Furthermore, our testing has shown that\nsmall improvements in these models can be made by further tuning of the hyper-parameters\n(suggesting it might be useful to provide a standardized setting and improved setting).\nTable 8 presents a comparative analysis of the modeling techniques under study on eight bench-\nmark datasets. These models include LLM-based models (GPT4TS), Transformer-based models\n(PatchTST/42, FEDformer, Autoformer, Stationary, ETSformer, Informer, and Reformer), CNN-based\nmodel (TimesNet), and MLP-based models (NLinear, DLinear, and LightTS). The results are sourced\nfrom [140, 153] and are generally consistent across different papers where each newly proposed\nmodel does not run the baselines but collects results from a previous paper. We included the\nmodels for which the code and hyper-parameters are made public, omitting PatchTST/64 for which\nthe hyper-parameters for ILI are not available. The assessment employs MSE and MAE for the\nnormalized observed and forecasted values, averaged over all features and forecasting horizons.\nThe assessment shows that PatchTST consistently outperforms the other modeling techniques,\nsuch as GPT4TS which is based on a pre-trained LLM. PatchTST is also highly competitive with\nNLinear achieving comparable scores. Table 8 presents a rank of these models using the average\nMAE over all datasets and forecasting horizons.\nOne can observe that forecasting performance tends to be less variable on the Electricity and\nTraffic datasets for all the modeling techniques including the Informer and the Reformer models.\nHowever, the forecasting performance tends to be highly variable on the ETT and ILI datasets\nespecially with Informer and Reformer being the least favorable models.\n, Vol. 1, No. 1, Article . Publication date: January 2024.\n28John A. Miller, Mohammed Aldosari, Farah Saeed, Nasid Habib Barna, Subas Rana, I. Budak Arpinar, and Ninghao Liu\nTable 8. Comparison of models using different look-back windows for forecasting {24, 36, 48, 60} horizons for\nILI and {96, 192, 336, 720} for the other datasets. The evaluation metrics used are Mean Squared Error (MSE)\nand Mean Absolute Error (MAE) applied to the normalized observed and forecasted values. The lower the\nscores the better the forecast performance. The best scores are highlighted in bold .\nRank\n3\n2\n5\n1\n4\n8\n9\n6\n7\n10\n11\n12\nMethods\nGPT2(6)\nNLinear\nDLinear\nPatchTST/42\nTimesNet\nFEDformer Autoformer Stationary ETSformer\nLightTS\nInformer\nReformer\nMetrics\nMSE\nMAE\nMSE\nMAE\nMSE MAE\nMSE\nMAE\nMSE\nMAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE\nWeather\n96\n0.162\n0.212\n0.182\n0.232\n0.176 0.237\n0.152\n0.199\n0.172\n0.220 0.217 0.296 0.266 0.336 0.173 0.223 0.197 0.281 0.182 0.242 0.300 0.384 0.689 0.596\n192\n0.204\n0.248\n0.225\n0.269\n0.220 0.282\n0.197\n0.243\n0.219\n0.261 0.276 0.336 0.307 0.367 0.245 0.285 0.237 0.312 0.227 0.287 0.598 0.544 0.752 0.638\n336\n0.254\n0.286\n0.271\n0.301\n0.265 0.319\n0.249\n0.283\n0.280\n0.306 0.339 0.380 0.359 0.395 0.321 0.338 0.298 0.353 0.282 0.334 0.578 0.523 0.639 0.596\n720\n0.326\n0.337\n0.338\n0.348\n0.333 0.362\n0.320\n0.335\n0.365\n0.359 0.403 0.428 0.419 0.428 0.414 0.410 0.352 0.288 0.352 0.386 1.059 0.741 1.130 0.792\nAvg\n0.237\n0.270\n0.254\n0.287\n0.248 0.300\n0.229\n0.265\n0.259\n0.287 0.309 0.360 0.338 0.382 0.288 0.314 0.271 0.334 0.261 0.312 0.634 0.548 0.803 0.656\nETTh1\n96\n0.376\n0.397\n0.374\n0.394\n0.375 0.399\n0.375\n0.399\n0.384\n0.402 0.376 0.419 0.449 0.459 0.513 0.491 0.494 0.479 0.424 0.432 0.865 0.713 0.837 0.728\n192\n0.416\n0.418\n0.408\n0.415\n0.405 0.416\n0.414\n0.421\n0.436\n0.429 0.420 0.448 0.500 0.482 0.534 0.504 0.538 0.504 0.475 0.462 1.008 0.792 0.923 0.766\n336\n0.442\n0.433\n0.429\n0.427\n0.439 0.443\n0.431\n0.436\n0.491\n0.469 0.459 0.465 0.521 0.496 0.588 0.535 0.574 0.521 0.518 0.488 1.107 0.809 1.097 0.835\n720\n0.477\n0.456\n0.440\n0.453\n0.472 0.490\n0.449\n0.466\n0.521\n0.500 0.506 0.507 0.514 0.512 0.643 0.616 0.562 0.535 0.547 0.533 1.181 0.865 1.257 0.889\nAvg\n0.427\n0.426\n0.413\n0.422\n0.422 0.437\n0.417\n0.430\n0.458\n0.450 0.440 0.460 0.496 0.487 0.570 0.537 0.542 0.510 0.491 0.479 1.040 0.795 1.029 0.805\nETTh2\n96\n0.285\n0.342\n0.277\n0.338\n0.289 0.353\n0.274\n0.336\n0.340\n0.374 0.358 0.397 0.346 0.388 0.476 0.458 0.340 0.391 0.397 0.437 3.755 1.525 2.626 1.317\n192\n0.354\n0.389\n0.344\n0.381\n0.383 0.418\n0.339\n0.379\n0.402\n0.414 0.429 0.439 0.456 0.452 0.512 0.493 0.430 0.439 0.520 0.504 5.602 1.931 11.12 2.979\n336\n0.373\n0.407\n0.357\n0.400\n0.448 0.465\n0.331\n0.380\n0.452\n0.452 0.496 0.487 0.482 0.486 0.552 0.551 0.485 0.479 0.626 0.559 4.721 1.835 9.323 2.769\n720\n0.406\n0.441\n0.394\n0.436\n0.605 0.551\n0.379\n0.422\n0.462\n0.468 0.463 0.474 0.515 0.511 0.562 0.560 0.500 0.497 0.863 0.672 3.647 1.625 3.874 1.697\nAvg\n0.354\n0.394\n0.343\n0.389\n0.431 0.446\n0.330\n0.379\n0.414\n0.427 0.437 0.449 0.450 0.459 0.526 0.516 0.439 0.452 0.602 0.543 4.431 1.729 6.736 2.191\nETTm1\n96\n0.292\n0.346\n0.306\n0.348\n0.299 0.343\n0.290\n0.342\n0.338\n0.375 0.379 0.419 0.505 0.475 0.386 0.398 0.375 0.398 0.374 0.400 0.672 0.571 0.538 0.528\n192\n0.332\n0.372\n0.349\n0.375\n0.335 0.365\n0.332\n0.369\n0.374\n0.387 0.426 0.441 0.553 0.496 0.459 0.444 0.408 0.410 0.400 0.407 0.795 0.669 0.658 0.592\n336\n0.366\n0.394\n0.375\n0.388\n0.369 0.386\n0.366\n0.392\n0.410\n0.411 0.445 0.459 0.621 0.537 0.495 0.464 0.435 0.428 0.438 0.438 1.212 0.871 0.898 0.721\n720\n0.417\n0.421\n0.433\n0.422\n0.425 0.421\n0.420\n0.424\n0.478\n0.450 0.543 0.490 0.671 0.561 0.585 0.516 0.499 0.462 0.527 0.502 1.166 0.823 1.102 0.841\nAvg\n0.352\n0.383\n0.365\n0.383\n0.388 0.403\n0.357\n0.378\n0.352\n0.382 0.400 0.406 0.448 0.452 0.481 0.456 0.429 0.425 0.435 0.437 0.961 0.734 0.799 0.671\nETTm2\n96\n0.173\n0.262\n0.167\n0.255\n0.167 0.269\n0.165\n0.255\n0.187\n0.267 0.203 0.287 0.255 0.339 0.192 0.274 0.189 0.280 0.209 0.308 0.365 0.453 0.658 0.619\n192\n0.229\n0.301\n0.221\n0.293\n0.224 0.303\n0.220\n0.292\n0.249\n0.309 0.269 0.328 0.281 0.340 0.280 0.339 0.253 0.319 0.311 0.382 0.533 0.563 1.078 0.827\n336\n0.286\n0.341\n0.274\n0.327\n0.281 0.342\n0.278\n0.329\n0.321\n0.351 0.325 0.366 0.339 0.372 0.334 0.361 0.314 0.357 0.442 0.466 1.363 0.887 1.549 0.972\n720\n0.378\n0.401\n0.368\n0.384\n0.397 0.421\n0.367\n0.385\n0.408\n0.403 0.421 0.415 0.433 0.432 0.417 0.413 0.414 0.413 0.675 0.587 3.379 1.338 2.631 1.242\nAvg\n0.266\n0.326\n0.257\n0.315\n0.267 0.333\n0.257\n0.315\n0.291\n0.333 0.305 0.349 0.327 0.371 0.306 0.347 0.293 0.342 0.409 0.436 1.410 0.810 1.479 0.915\nILI\n24\n2.063\n0.881\n1.683\n0.858\n2.215 1.081\n1.522\n0.814\n2.317\n0.934 3.228 1.260 3.483 1.287 2.294 0.945 2.527 1.020 8.313 2.144 5.764 1.677 4.400 1.382\n36\n1.868\n0.892\n1.703\n0.859\n1.963 0.963\n1.430\n0.834\n1.972\n0.920 2.679 1.080 3.103 1.148 1.825 0.848 2.615 1.007 6.631 1.902 4.755 1.467 4.783 1.448\n48\n1.790\n0.884\n1.719\n0.884\n2.130 1.024\n1.673\n0.854\n2.238\n0.940 2.622 1.078 2.669 1.085 2.010 0.900 2.359 0.972 7.299 1.982 4.763 1.469 4.832 1.465\n60\n1.979\n0.957\n1.819\n0.917\n2.368 1.096\n1.529\n0.862\n2.027\n0.928 2.857 1.157 2.770 1.125 2.178 0.963 2.487 1.016 7.283 1.985 5.264 1.564 4.882 1.483\nAvg\n1.925\n0.903\n1.731\n0.879\n2.169 1.041\n1.538\n0.841\n2.139\n0.931 2.847 1.144 3.006 1.161 2.077 0.914 2.497 1.004 7.382 2.003 5.137 1.544 4.724 1.445\nElectricity\n96\n0.139\n0.238\n0.141\n0.237\n0.140 0.237\n0.130\n0.222\n0.168\n0.272 0.193 0.308 0.201 0.317 0.169 0.273 0.187 0.304 0.207 0.307 0.274 0.368 0.312 0.402\n192\n0.153\n0.251\n0.154\n0.248\n0.153 0.249\n0.148\n0.240\n0.184\n0.289 0.201 0.315 0.222 0.334 0.182 0.286 0.199 0.315 0.213 0.316 0.296 0.386 0.348 0.433\n336\n0.169\n0.266\n0.171\n0.265\n0.169 0.267\n0.167\n0.261\n0.198\n0.300 0.214 0.329 0.231 0.338 0.200 0.304 0.212 0.329 0.230 0.333 0.300 0.394 0.350 0.433\n720\n0.206\n0.297\n0.210\n0.297\n0.203 0.301\n0.202\n0.291\n0.220\n0.320 0.246 0.355 0.254 0.361 0.222 0.321 0.233 0.345 0.265 0.360 0.373 0.439 0.340 0.420\nAvg\n0.167\n0.263\n0.169\n0.268\n0.166 0.263\n0.162\n0.253\n0.192\n0.295 0.214 0.327 0.227 0.338 0.193 0.296 0.208 0.323 0.229 0.329 0.311 0.397 0.338 0.422\nTraffic\n96\n0.388\n0.282\n0.410\n0.279\n0.410 0.282\n0.367\n0.251\n0.593\n0.321 0.587 0.366 0.613 0.388 0.612 0.338 0.607 0.392 0.615 0.391 0.719 0.391 0.732 0.423\n192\n0.407\n0.290\n0.423\n0.284\n0.423 0.287\n0.385\n0.259\n0.617\n0.336 0.604 0.373 0.616 0.382 0.613 0.340 0.621 0.399 0.601 0.382 0.696 0.379 0.733 0.420\n336\n0.412\n0.294\n0.435\n0.290\n0.436 0.296\n0.398\n0.265\n0.629\n0.336 0.621 0.383 0.622 0.337 0.618 0.328 0.622 0.396 0.613 0.386 0.777 0.420 0.742 0.420\n720\n0.450\n0.312\n0.464\n0.307\n0.466 0.315\n0.434\n0.287\n0.640\n0.350 0.626 0.382 0.660 0.408 0.653 0.355 0.632 0.396 0.658 0.407 0.864 0.472 0.755 0.423\nAvg\n0.414\n0.294\n0.433\n0.289\n0.433 0.295\n0.396\n0.265\n0.619\n0.336 0.610 0.376 0.628 0.379 0.624 0.340 0.621 0.396 0.622 0.392 0.764 0.416 0.741 0.422\nAvg All\n0.516\n0.407\n0.490\n0.400\n0.562 0.436\n0.460\n0.391\n0.596\n0.433 0.701 0.489 0.757 0.511 0.633 0.465 0.662 0.473 1.303 0.616 1.836 0.871 2.081 0.954\n8\nSUMMARY\nThe second wave of progress on multivariate time series forecasting has been recent and appears to\nbe heating up with the use of advanced deep learning architectures. Work has started to establish\nfurther improvements using Knowledge Graphs and Large Language Models. Efforts have started\nto train them with scientific literature, such as the CORD-19 dataset for COVID-19. A complete\ninvestigation of how this can help with time series forecasting or related problems of time series\nclassification or anomaly detection will take some time. Analysis of the features/factors influencing\nthe course/time evolution of a pandemic may be conducted with the help of LLMs. Findings from\n, Vol. 1, No. 1, Article . Publication date: January 2024.\nA Survey of Deep Learning and Foundation Models for Time Series Forecasting\n29\nthe analysis of a dataset (e.g., the effectiveness of mask policies) can be quickly checked against\nprevious studies. New research can more quickly be suggested to fill gaps in existing knowledge.\nEstablished knowledge can be associated with knowledge graphs or temporal knowledge graphs.\nAs discussed, there are multiple ways in which knowledge can be used to improve forecasting.\nFoundation models are being created for several modalities of data and time series data are no\nexception. During the Fall of 2023, several foundation models for time series data were created and\ntested. They show promise for providing more accurate and robust forecasts as well as potential\nfor greater explainability. As discussed, there are multiple options for backbone models as well\nas many choices for architectural elements. Although successful elements from LLMs are a good\nstarting point, additional research is needed to optimize them for time series data.\nFurther potential is provided by multi-modal foundation models, where we emphasize the\nimportance of modeling time series data with textual data. It is challenging to build time series\nfoundation models that are comparable to existing large language models since: (1) unlike natural\nlanguages, existing time series data lacks the inherent semantic richness; (2) the semantics in time\nseries data are often heavily domain-specific (e.g., the modeling of electrocardiogram signals would\nprovide little help in predicting stock prices). There are several potential benefits of the multi-model\nmodeling: (1) textual data can provide essential context that is not captured in the raw time series\ndata; (2) textual semantics can provide domain-specific knowledge that enhances the model’s\ninterpretability; (3) textual data can introduce additional features and dimensions of variability,\nwhich can help in training more robust and generalizable models. In this way, text-enhanced time\nseries models can be more easily transferred across different tasks (e.g., classification, forecasting,\nanomaly detection) and domains. Recently, foundation models like Contrastive Language-Image\nPre-Training (CLIP) [97] can effectively learn visual concepts from natural language supervision,\nwhich shows the success of data augmentation with textual semantics.\nThe scalation/data GitHub repository https://github.com/scalation/data contains links to\ndatasets, knowledge graphs, models, code, and papers on time series forecasting. As the MSE and\nMAE values in this paper are based on normalized values and therefore hard to interpret (other than\nby looking at the relative scores), tables in scalation/data show MSE, MAE, and sMAPE quality\nmetrics for the same forecasting models on the original scale. Results for COVID-19 pandemic\nprediction are also given in this repository.\nREFERENCES\n[1] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit Sanghai. 2023. GQA:\nTraining Generalized Multi-Query Transformer Models from Multi-Head Checkpoints. arXiv preprint arXiv:2305.13245\n(2023).\n[2] Mohammed Aldosari and John A Miller. 2023. On Transformer Autoregressive Decoding for Multivariate Time Series\nForecasting. In European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning\n(Bruges, Belgium).\n[3] George Athanasopoulos and Farshid Vahid. 2008. VARMA versus VAR for macroeconomic forecasting. Journal of\nBusiness & Economic Statistics 26, 2 (2008), 237–252.\n[4] Anthony Bagnall, Hoang Anh Dau, Jason Lines, Michael Flynn, James Large, Aaron Bostrom, Paul Southam, and\nEamonn Keogh. 2018. The UEA multivariate time series classification archive, 2018. arXiv preprint arXiv:1811.00075\n(2018).\n[5] Jiangang Bai, Yujing Wang, Hong Sun, Ruonan Wu, Tianmeng Yang, Pengfei Tang, Defu Cao, Mingliang Zhang, Yunhai\nTong, Yaming Yang, et al. 2022. Enhancing self-attention with knowledge-assisted attention maps. In Proceedings of\nthe 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies. 107–115.\n[6] Shaojie Bai, J Zico Kolter, and Vladlen Koltun. 2018. An empirical evaluation of generic convolutional and recurrent\nnetworks for sequence modeling. arXiv preprint arXiv:1803.01271 (2018).\n[7] Yoshua Bengio, Aaron Courville, and Pascal Vincent. 2013. Representation learning: A review and new perspectives.\nIEEE transactions on pattern analysis and machine intelligence 35, 8 (2013), 1798–1828.\n, Vol. 1, No. 1, Article . Publication date: January 2024.\n30John A. Miller, Mohammed Aldosari, Farah Saeed, Nasid Habib Barna, Subas Rana, I. Budak Arpinar, and Ninghao Liu\n[8] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein,\nJeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. 2021. On the opportunities and risks of foundation models.\narXiv preprint arXiv:2108.07258 (2021).\n[9] George EP Box and Gwilym M Jenkins. 1962. Some statistical aspects of adaptive optimization and control. Journal of\nthe Royal Statistical Society: Series B (Methodological) 24, 2 (1962), 297–331.\n[10] George EP Box and Gwilym M Jenkins. 1970. Time Series Analysis Forecasting and Control. Technical Report. DTIC\nDocument.\n[11] George EP Box, Gwilym M Jenkins, and DW Bacon. 1967. Models for Forecasting Seasonal and Non-Seasonal Time\nSeries. Technical Report. Wisconsin Univ Madison Dept of Statistics.\n[12] Defu Cao, Furong Jia, Sercan O Arik, Tomas Pfister, Yixiang Zheng, Wen Ye, and Yan Liu. 2023. TEMPO: Prompt-based\nGenerative Pre-trained Transformer for Time Series Forecasting. arXiv preprint arXiv:2310.04948 (2023).\n[13] Defu Cao, Yujing Wang, Juanyong Duan, Ce Zhang, Xia Zhu, Congrui Huang, Yunhai Tong, Bixiong Xu, Jing Bai, Jie\nTong, et al. 2020. Spectral temporal graph neural network for multivariate time-series forecasting. Advances in neural\ninformation processing systems 33 (2020), 17766–17778.\n[14] Ching Chang, Wen-Chih Peng, and Tien-Fu Chen. 2023. Llm4ts: Two-stage fine-tuning for time-series forecasting\nwith pre-trained llms. arXiv preprint arXiv:2308.08469 (2023).\n[15] Hila Chefer, Shir Gur, and Lior Wolf. 2021. Transformer interpretability beyond attention visualization. In Proceedings\nof the IEEE/CVF conference on computer vision and pattern recognition. 782–791.\n[16] Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and\nYoshua Bengio. 2014. Learning phrase representations using RNN encoder-decoder for statistical machine translation.\narXiv preprint arXiv:1406.1078 (2014).\n[17] Jan Chorowski, Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. End-to-end continuous speech\nrecognition using attention-based recurrent NN: First results. arXiv preprint arXiv:1412.1602 (2014).\n[18] Razvan-Gabriel Cirstea, Chenjuan Guo, Bin Yang, Tung Kieu, Xuanyi Dong, and Shirui Pan. 2022. Triformer: Triangular,\nVariable-Specific Attentions for Long Sequence Multivariate Time Series Forecasting–Full Version. arXiv preprint\narXiv:2204.13767 (2022).\n[19] Donald Cochrane and Guy H Orcutt. 1949. Application of least squares regression to relationships containing\nauto-correlated error terms. Journal of the American statistical association 44, 245 (1949), 32–61.\n[20] Abhimanyu Das, Weihao Kong, Rajat Sen, and Yichen Zhou. 2023. A decoder-only foundation model for time-series\nforecasting. arXiv preprint arXiv:2310.10688 (2023).\n[21] Hoang Anh Dau, Anthony Bagnall, Kaveh Kamgar, Chin-Chia Michael Yeh, Yan Zhu, Shaghayegh Gharghabi,\nChotirat Ann Ratanamahatana, and Eamonn Keogh. 2019. The UCR time series archive. IEEE/CAA Journal of\nAutomatica Sinica 6, 6 (2019), 1293–1305.\n[22] Vijay Ekambaram, Arindam Jati, Nam Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. 2023. TSMixer:\nLightweight MLP-Mixer Model for Multivariate Time Series Forecasting. arXiv preprint arXiv:2306.09364 (2023).\n[23] Cornelius Fritz, Emilio Dorigatti, and David Rügamer. 2022. Combining graph neural networks and spatio-temporal\ndisease models to improve the prediction of weekly COVID-19 cases in Germany. Scientific Reports 12, 1 (2022), 3930.\n[24] Zihao Fu, Wai Lam, Qian Yu, Anthony Man-Cho So, Shengding Hu, Zhiyuan Liu, and Nigel Collier. 2023. Decoder-Only\nor Encoder-Decoder? Interpreting Language Model as a Regularized Encoder-Decoder. arXiv preprint arXiv:2304.04052\n(2023).\n[25] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang. 2023.\nRetrieval-augmented generation for large language models: A survey. arXiv preprint arXiv:2312.10997 (2023).\n[26] Azul Garza and Max Mergenthaler-Canseco. 2023. TimeGPT-1. arXiv preprint arXiv:2310.03589 (2023).\n[27] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. 2017. Neural message passing\nfor quantum chemistry. In International conference on machine learning. PMLR, 1263–1272.\n[28] Rakshitha Godahewa, Christoph Bergmeir, Geoffrey I Webb, Rob J Hyndman, and Pablo Montero-Manso. 2021.\nMonash time series forecasting archive. arXiv preprint arXiv:2105.06643 (2021).\n[29] Emily M Gordon, Elizabeth A Barnes, James W Hurrell, Maria Rugenstein, Charles Anderson, et al. 2023. When is the\nUnpredictable (Slightly More) Predictable? An Assessment of Opportunities for Skillful Decadal Climate Prediction\nUsing Explainable Neural Networks. (2023).\n[30] Albert Gu and Tri Dao. 2023. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint\narXiv:2312.00752 (2023).\n[31] Lea Gütebier, Tim Bleimehl, Ron Henkel, Jamie Munro, Sebastian Müller, Axel Morgner, Jakob Laenge, Anke Pachauer,\nAlexander Erdl, Jens Weimar, et al. 2022. CovidGraph: a graph to fight COVID-19. Bioinformatics 38, 20 (2022),\n4843–4845.\n[32] Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive representation learning on large graphs. Advances in\nneural information processing systems 30 (2017).\n, Vol. 1, No. 1, Article . Publication date: January 2024.\nA Survey of Deep Learning and Foundation Models for Time Series Forecasting\n31\n[33] Lu Han, Han-Jia Ye, and De-Chuan Zhan. 2023. The Capacity and Robustness Trade-off: Revisiting the Channel\nIndependent Strategy for Multivariate Time Series Forecasting. arXiv preprint arXiv:2304.05206 (2023).\n[34] Bobby He and Thomas Hofmann. 2023. Simplifying Transformer Blocks. arXiv preprint arXiv:2311.01906 (2023).\n[35] Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural computation 9, 8 (1997), 1735–1780.\n[36] C Holt Charles. 1957. Forecasting trends and seasonal by exponentially weighted averages. International Journal of\nForecasting 20, 1 (1957), 5–10.\n[37] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo,\nMona Attariyan, and Sylvain Gelly. 2019. Parameter-efficient transfer learning for NLP. In International Conference on\nMachine Learning. PMLR, 2790–2799.\n[38] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\n2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 (2021).\n[39] Linmei Hu, Zeyi Liu, Ziwang Zhao, Lei Hou, Liqiang Nie, and Juanzi Li. 2023. A Survey of Knowledge Enhanced\nPre-Trained Language Models. IEEE Transactions on Knowledge and Data Engineering (2023).\n[40] Ziniu Hu, Yuxiao Dong, Kuansan Wang, and Yizhou Sun. 2020. Heterogeneous graph transformer. In Proceedings of\nthe web conference 2020. 2704–2710.\n[41] Rob J Hyndman and Anne B Koehler. 2006. Another look at measures of forecast accuracy. International journal of\nforecasting 22, 4 (2006), 679–688.\n[42] Indrajeet Y Javeri, Mohammadhossein Toutiaee, Ismailcem B Arpinar, John A Miller, and Tom W Miller. 2021.\nImproving neural networks for time-series forecasting using data augmentation and AutoML. In 2021 IEEE Seventh\nInternational Conference on Big Data Computing Service and Applications (BigDataService). IEEE, 1–8.\n[43] Ming Jin, Huan Yee Koh, Qingsong Wen, Daniele Zambon, Cesare Alippi, Geoffrey I Webb, Irwin King, and Shirui\nPan. 2023. A survey on graph neural networks for time series: Forecasting, classification, imputation, and anomaly\ndetection. arXiv preprint arXiv:2307.03759 (2023).\n[44] Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y Zhang, Xiaoming Shi, Pin-Yu Chen, Yuxuan Liang, Yuan-Fang\nLi, Shirui Pan, et al. 2023. Time-llm: Time series forecasting by reprogramming large language models. arXiv preprint\narXiv:2310.01728 (2023).\n[45] Xiaoyong Jin, Youngsuk Park, Danielle Maddix, Hao Wang, and Yuyang Wang. 2022. Domain adaptation for time\nseries forecasting via attention sharing. In International Conference on Machine Learning. PMLR, 10280–10297.\n[46] David Alan Jones and David Roxbee Cox. 1978. Nonlinear autoregressive processes. Proceedings of the Royal Society\nof London. A. Mathematical and Physical Sciences 360, 1700 (1978), 71–95.\n[47] Jaehun Jung, Jinhong Jung, and U Kang. 2021. Learning to walk across time for interpretable temporal knowledge\ngraph completion. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining. 786–795.\n[48] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford,\nJeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361 (2020).\n[49] Amol Kapoor, Xue Ben, Luyang Liu, Bryan Perozzi, Matt Barnes, Martin Blais, and Shawn O’Banion. 2020. Examining\ncovid-19 forecasting using spatio-temporal graph neural networks. arXiv preprint arXiv:2007.03113 (2020).\n[50] Sergios Karagiannakos. 2021.\nBest Graph Neural Networks architectures: GCN, GAT, MPNN and more.\nhttps://theaisummer.com/gnn-architectures/. https://theaisummer.com/ (2021).\n[51] George Em Karniadakis, Ioannis G Kevrekidis, Lu Lu, Paris Perdikaris, Sifan Wang, and Liu Yang. 2021. Physics-\ninformed machine learning. Nature Reviews Physics 3, 6 (2021), 422–440.\n[52] Anuj Karpatne, Gowtham Atluri, James H Faghmous, Michael Steinbach, Arindam Banerjee, Auroop Ganguly, Shashi\nShekhar, Nagiza Samatova, and Vipin Kumar. 2017. Theory-guided data science: A new paradigm for scientific\ndiscovery from data. IEEE Transactions on Knowledge and Data Engineering 29, 10 (2017), 2318–2331.\n[53] Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu. 2017. Lightgbm:\nA highly efficient gradient boosting decision tree. Advances in neural information processing systems 30 (2017).\n[54] Subin Kim, Euisuk Chung, and Pilsung Kang. 2023. FEAT: A general framework for feature-aware multivariate\ntime-series representation learning. Knowledge-Based Systems 277 (2023), 110790.\n[55] Taesung Kim, Jinhee Kim, Yunwon Tae, Cheonbok Park, Jang-Ho Choi, and Jaegul Choo. 2021. Reversible instance\nnormalization for accurate time-series forecasting against distribution shift. In International Conference on Learning\nRepresentations.\n[56] Thomas N Kipf and Max Welling. 2016. Semi-supervised classification with graph convolutional networks. arXiv\npreprint arXiv:1609.02907 (2016).\n[57] Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. 2020. Reformer: The efficient transformer. arXiv preprint\narXiv:2001.04451 (2020).\n[58] William La Cava, Tilak Raj Singh, James Taggart, Srinivas Suri, and Jason H Moore. 2018. Learning concise represen-\ntations for regression by evolving networks of trees. arXiv preprint arXiv:1807.00981 (2018).\n, Vol. 1, No. 1, Article . Publication date: January 2024.\n32John A. Miller, Mohammed Aldosari, Farah Saeed, Nasid Habib Barna, Subas Rana, I. Budak Arpinar, and Ninghao Liu\n[59] Colin Lea, Rene Vidal, Austin Reiter, and Gregory D Hager. 2016. Temporal convolutional networks: A unified\napproach to action segmentation. In European Conference on Computer Vision. Springer, 47–54.\n[60] Yann LeCun, Yoshua Bengio, et al. 1995. Convolutional networks for images, speech, and time series. The handbook\nof brain theory and neural networks 3361, 10 (1995), 1995.\n[61] Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang, and Xifeng Yan. 2019. Enhancing\nthe locality and breaking the memory bottleneck of transformer on time series forecasting. Advances in neural\ninformation processing systems 32 (2019).\n[62] Zhe Li, Zhongwen Rao, Lujia Pan, Pengyun Wang, and Zenglin Xu. 2023. Ti-MAE: Self-Supervised Masked Time\nSeries Autoencoders. arXiv preprint arXiv:2301.08871 (2023).\n[63] Zhe Li, Zhongwen Rao, Lujia Pan, and Zenglin Xu. 2023. Mts-mixers: Multivariate time series forecasting via factorized\ntemporal and channel mixing. arXiv preprint arXiv:2302.04501 (2023).\n[64] Baohao Liao, Yan Meng, and Christof Monz. 2023. Parameter-Efficient Fine-Tuning without Introducing New Latency.\narXiv preprint arXiv:2305.16742 (2023).\n[65] Bryan Lim and Stefan Zohren. 2021. Time-series forecasting with deep learning: a survey. Philosophical Transactions\nof the Royal Society A 379, 2194 (2021), 20200209.\n[66] Tianyang Lin, Yuxin Wang, Xiangyang Liu, and Xipeng Qiu. 2022. A survey of transformers. AI Open (2022).\n[67] Xuan Lin, Zhe Quan, Zhi-Jie Wang, Tengfei Ma, and Xiangxiang Zeng. 2020. KGNN: Knowledge Graph Neural\nNetwork for Drug-Drug Interaction Prediction.. In IJCAI, Vol. 380. 2739–2745.\n[68] Dong C Liu and Jorge Nocedal. 1989. On the limited memory BFGS method for large scale optimization. Mathematical\nprogramming 45, 1-3 (1989), 503–528.\n[69] Jiawei Liu, Cheng Yang, Zhiyuan Lu, Junze Chen, Yibo Li, Mengmei Zhang, Ting Bai, Yuan Fang, Lichao Sun, Philip S\nYu, et al. 2023. Towards graph foundation models: A survey and beyond. arXiv preprint arXiv:2310.11829 (2023).\n[70] Shizhan Liu, Hang Yu, Cong Liao, Jianguo Li, Weiyao Lin, Alex X Liu, and Schahram Dustdar. 2021. Pyraformer:\nLow-complexity pyramidal attention for long-range time series modeling and forecasting. In International conference\non learning representations.\n[71] Xu Liu, Junfeng Hu, Yuan Li, Shizhe Diao, Yuxuan Liang, Bryan Hooi, and Roger Zimmermann. 2023. UniTime: A\nLanguage-Empowered Unified Model for Cross-Domain Time Series Forecasting. arXiv preprint arXiv:2310.09751\n(2023).\n[72] Yong Liu, Haixu Wu, Jianmin Wang, and Mingsheng Long. 2022. Non-stationary transformers: Exploring the\nstationarity in time series forecasting. Advances in Neural Information Processing Systems 35 (2022), 9881–9893.\n[73] Yunshan Ma, Yujuan Ding, Xun Yang, Lizi Liao, Wai Keung Wong, and Tat-Seng Chua. 2020. Knowledge enhanced\nneural fashion trend forecasting. In Proceedings of the 2020 international conference on multimedia retrieval. 82–90.\n[74] Spyros Makridakis, Evangelos Spiliotis, and Vassilios Assimakopoulos. 2018. The M4 Competition: Results, findings,\nconclusion and way forward. International Journal of Forecasting 34, 4 (2018), 802–808.\n[75] Spyros Makridakis, Evangelos Spiliotis, and Vassilios Assimakopoulos. 2022. M5 accuracy competition: Results,\nfindings, and conclusions. International Journal of Forecasting 38, 4 (2022), 1346–1364.\n[76] Spyros Makridakis, Evangelos Spiliotis, Ross Hollyman, Fotios Petropoulos, Norman Swanson, and Anil Gaba. 2023.\nThe M6 forecasting competition: Bridging the gap between forecasting and investment decisions. arXiv preprint\narXiv:2310.13357 (2023).\n[77] Timothy R McIntosh, Teo Susnjak, Tong Liu, Paul Watters, and Malka N Halgamuge. 2023. From Google Gemini to\nOpenAI Q*(Q-Star): A Survey of Reshaping the Generative Artificial Intelligence (AI) Research Landscape. arXiv\npreprint arXiv:2312.10868 (2023).\n[78] Qianwen Meng, Hangwei Qian, Yong Liu, Yonghui Xu, Zhiqi Shen, and Lizhen Cui. 2023. Unsupervised Representation\nLearning for Time Series: A Review. arXiv preprint arXiv:2308.01578 (2023).\n[79] Franck Michel, Fabien Gandon, Valentin Ah-Kane, Anna Bobasheva, Elena Cabrio, Olivier Corby, Raphaël Gazzotti,\nAlain Giboin, Santiago Marro, Tobias Mayer, et al. 2020. Covid-on-the-Web: Knowledge graph and services to advance\nCOVID-19 research. In The Semantic Web–ISWC 2020: 19th International Semantic Web Conference, Athens, Greece,\nNovember 2–6, 2020, Proceedings, Part II 19. Springer, 294–310.\n[80] John A. Miller, Nasid Habib Barna, Subas Rana, I. Budak Arpinar, and Ninghao Liu. 2023. Knowledge Enhanced Deep\nLearning: Application to Pandemic Prediction. In Proceedings of the 9th IEEE International Conference on Collaboration\nand Internet Computin. 1–10.\n[81] John A Miller, Jun Han, and Maria Hybinette. 2010. Using domain specific language for modeling and simulation:\nScalation as a case study. In The 2010 Winter Simulation Conference (WSC). ACM/IEEE, 741–752.\n[82] John A Miller, Hao Peng, and Michael E Cotterell. 2017. Adding Support for Theory in Open Science Big Data. In the\n6th IEEE International Congress on Big Data (BigData’17). IEEE, 251–255.\n[83] Bonan Min, Hayley Ross, Elior Sulem, Amir Pouran Ben Veyseh, Thien Huu Nguyen, Oscar Sainz, Eneko Agirre, Ilana\nHeintz, and Dan Roth. 2021. Recent advances in natural language processing via large pre-trained language models:\n, Vol. 1, No. 1, Article . Publication date: January 2024.\nA Survey of Deep Learning and Foundation Models for Time Series Forecasting\n33\nA survey. Comput. Surveys (2021).\n[84] Khalil Mrini, Franck Dernoncourt, Quan Tran, Trung Bui, Walter Chang, and Ndapa Nakashole. 2019. Rethinking\nself-attention: Towards interpretability in neural parsing. arXiv preprint arXiv:1911.03875 (2019).\n[85] K-R Müller, Alexander J Smola, Gunnar Rätsch, Bernhard Schölkopf, Jens Kohlmorgen, and Vladimir Vapnik. 1997.\nPredicting time series with support vector machines. In International Conference on Artificial Neural Networks. Springer,\n999–1004.\n[86] Meinard Müller. 2007. Dynamic time warping. Information retrieval for music and motion (2007), 69–84.\n[87] Yuqi Nie, Nam H Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. 2022. A time series is worth 64 words:\nLong-term forecasting with transformers. arXiv preprint arXiv:2211.14730 (2022).\n[88] Boris N Oreshkin, Dmitri Carpov, Nicolas Chapados, and Yoshua Bengio. 2019. N-BEATS: Neural basis expansion\nanalysis for interpretable time series forecasting. arXiv preprint arXiv:1905.10437 (2019).\n[89] Ping-Feng Pai and Chia-Hsin Liu. 2018. Predicting vehicle sales by sentiment analysis of twitter data and stock\nmarket values. IEEE Access 6 (2018), 57655–57662.\n[90] Santosh Palaskar, Vijay Ekambaram, Arindam Jati, Neelamadhav Gantayat, Avirup Saha, Seema Nagar, Nam H\nNguyen, Pankaj Dayama, Renuka Sindhgatta, Prateeti Mohapatra, et al. 2023. AutoMixer for Improved Multivariate\nTime-Series Forecasting on BizITOps Data. arXiv preprint arXiv:2310.20280 (2023).\n[91] George Panagopoulos, Giannis Nikolentzos, and Michalis Vazirgiannis. 2021. Transfer graph neural networks for\npandemic forecasting. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 35. 4838–4845.\n[92] Judea Pearl. 2011. Exogeneity and superexogeneity: a no-tear perspective. (2011).\n[93] Hao Peng, Nicholas Klepp, Mohammadhossein Toutiaee, I Budak Arpinar, and John A Miller. 2019. Knowledge\nand Situation-Aware Vehicle Traffic Forecasting. In 2019 IEEE International Conference on Big Data (Big Data). IEEE,\n3803–3812.\n[94] Svetlana Pestryakova, Daniel Vollmers, Mohamed Ahmed Sherif, Stefan Heindorf, Muhammad Saleem, Diego Mous-\nsallem, and Axel-Cyrille Ngonga Ngomo. 2022. Covidpubgraph: A fair knowledge graph of covid-19 publications.\nScientific Data 9, 1 (2022), 389.\n[95] Xinyuan Qi, Kai Hou, Tong Liu, Zhongzhong Yu, Sihao Hu, and Wenwu Ou. 2021. From known to unknown:\nKnowledge-guided transformer for time-series sales forecasting in Alibaba. arXiv preprint arXiv:2109.08381 (2021).\n[96] Maurice Henry Quenouille and MH Quenouille. 1957. The analysis of multiple time-series. Technical Report. Griffin\nLondon.\n[97] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda\nAskell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision.\nIn International conference on machine learning. PMLR, 8748–8763.\n[98] James O Ramsay and CJ Dalzell. 1991. Some tools for functional data analysis. Journal of the Royal Statistical Society:\nSeries B (Methodological) 53, 3 (1991), 539–561.\n[99] Subas Rana, Nasid Habib Barna, and John A Miller. 2023. Exploring the Predictive Power of Correlation and\nMutual Information in Attention Temporal Graph Convolutional Network for COVID-19 Forecasting. In International\nConference on Big Data. Springer, 18–33.\n[100] Kashif Rasul, Arjun Ashok, Andrew Robert Williams, Arian Khorasani, George Adamopoulos, Rishika Bhagwatkar,\nMarin Biloš, Hena Ghonia, Nadhir Vincent Hassen, Anderson Schneider, et al. 2023. Lag-llama: Towards foundation\nmodels for time series forecasting. arXiv preprint arXiv:2310.08278 (2023).\n[101] Yu Rong, Yatao Bian, Tingyang Xu, Weiyang Xie, Ying Wei, Wenbing Huang, and Junzhou Huang. 2020. Self-\nsupervised graph transformer on large-scale molecular data. Advances in Neural Information Processing Systems 33\n(2020), 12559–12571.\n[102] Peter Rose. 2020. Peter Rose: COVID-19-Net: Integrating Health, Pathogen and Environmental Data into a Knowledge\nGraph for Case Tracking, Analysis, and Forecasting. (2020).\n[103] Emanuele Rossi, Ben Chamberlain, Fabrizio Frasca, Davide Eynard, Federico Monti, and Michael Bronstein. 2020.\nTemporal graph networks for deep learning on dynamic graphs. arXiv preprint arXiv:2006.10637 (2020).\n[104] David Salinas, Valentin Flunkert, Jan Gasthaus, and Tim Januschowski. 2020. DeepAR: Probabilistic forecasting with\nautoregressive recurrent networks. International Journal of Forecasting 36, 3 (2020), 1181–1191.\n[105] J Scott Armstrong and Fred Collopy. 1993. Causal forces: Structuring knowledge for time-series extrapolation. Journal\nof Forecasting 12, 2 (1993), 103–115.\n[106] Noam Shazeer. 2020. Glu variants improve transformer. arXiv preprint arXiv:2002.05202 (2020).\n[107] Rampal Singh and S Balasundaram. 2007. Application of extreme learning machine method for time series analysis.\nInternational Journal of Intelligent Technology 2, 4 (2007), 256–262.\n[108] Jack W Smith, James E Everhart, WC Dickson, William C Knowler, and Robert Scott Johannes. 1988. Using the ADAP\nlearning algorithm to forecast the onset of diabetes mellitus. In Proceedings of the annual symposium on computer\napplication in medical care. American Medical Informatics Association, 261.\n, Vol. 1, No. 1, Article . Publication date: January 2024.\n34John A. Miller, Mohammed Aldosari, Farah Saeed, Nasid Habib Barna, Subas Rana, I. Budak Arpinar, and Ninghao Liu\n[109] Slawek Smyl. 2020. A hybrid method of exponential smoothing and recurrent neural networks for time series\nforecasting. International Journal of Forecasting 36, 1 (2020), 75–85.\n[110] Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, and Ari Morcos. 2022. Beyond neural scaling\nlaws: beating power law scaling via data pruning. Advances in Neural Information Processing Systems 35 (2022),\n19523–19536.\n[111] Bram Steenwinckel, Gilles Vandewiele, Ilja Rausch, Pieter Heyvaert, Ruben Taelman, Pieter Colpaert, Pieter Simoens,\nAnastasia Dimou, Filip De Turck, and Femke Ongenae. 2020. Facilitating the analysis of COVID-19 literature through\na knowledge graph. In International Semantic Web Conference. Springer, 344–357.\n[112] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. 2024. Roformer: Enhanced transformer\nwith rotary position embedding. Neurocomputing 568 (2024), 127063.\n[113] Peiwang Tang and Xianchao Zhang. 2022. MTSMAE: Masked Autoencoders for Multivariate Time-Series Forecasting.\nIn 2022 IEEE 34th International Conference on Tools with Artificial Intelligence (ICTAI). IEEE, 982–989.\n[114] Prakhar Thapak and Prodip Hore. 2020. Transformer++. arXiv preprint arXiv:2003.04974 (2020).\n[115] Ilya O Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung,\nAndreas Steiner, Daniel Keysers, Jakob Uszkoreit, et al. 2021. Mlp-mixer: An all-mlp architecture for vision. Advances\nin neural information processing systems 34 (2021), 24261–24272.\n[116] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste\nRozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models.\narXiv preprint arXiv:2302.13971 (2023).\n[117] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models.\narXiv preprint arXiv:2307.09288 (2023).\n[118] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin. 2017. Attention is all you need. Advances in neural information processing systems 30 (2017).\n[119] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, Yoshua Bengio, et al. 2017. Graph\nattention networks. stat 1050, 20 (2017), 10–48550.\n[120] Zhongwei Wan, Xin Wang, Che Liu, Samiul Alam, Yu Zheng, Zhongnan Qu, Shen Yan, Yi Zhu, Quanlu Zhang,\nMosharaf Chowdhury, et al. 2023. Efficient Large Language Models: A Survey. arXiv preprint arXiv:2312.03863 (2023).\n[121] Lijing Wang, Xue Ben, Aniruddha Adiga, Adam Sadilek, Ashish Tendulkar, Srinivasan Venkatramanan, Anil Vullikanti,\nGaurav Aggarwal, Alok Talekar, Jiangzhuo Chen, et al. 2020. Using mobility data to understand and forecast covid19\ndynamics. MedRxiv (2020).\n[122] Shuo Wang, Yanran Li, Jiang Zhang, Qingye Meng, Lingwei Meng, and Fei Gao. 2020. PM2. 5-GNN: A domain\nknowledge enhanced graph neural network for PM2. 5 forecasting. In Proceedings of the 28th international conference\non advances in geographic information systems. 163–166.\n[123] Qingsong Wen, Tian Zhou, Chaoli Zhang, Weiqi Chen, Ziqing Ma, Junchi Yan, and Liang Sun. 2022. Transformers in\ntime series: A survey. arXiv preprint arXiv:2202.07125 (2022).\n[124] Luisa Werner, Nabil Layaïda, Pierre Genevès, and Sarah Chlyah. 2023. Knowledge Enhanced Graph Neural Networks.\narXiv preprint arXiv:2303.15487 (2023).\n[125] Peter Whittle. 1951. Hypothesis testing in time series analysis. Vol. 4. Almqvist & Wiksells boktr.\n[126] Ronald J Williams and David Zipser. 1989. A learning algorithm for continually running fully recurrent neural\nnetworks. Neural computation 1, 2 (1989), 270–280.\n[127] Peter R Winters. 1960. Forecasting Sales by Exponentially Weighted Moving Averages. Management science 6, 3\n(1960), 324–342.\n[128] Gerald Woo, Chenghao Liu, Doyen Sahoo, Akshat Kumar, and Steven Hoi. 2022. CoST: Contrastive learning of\ndisentangled seasonal-trend representations for time series forecasting. arXiv preprint arXiv:2202.01575 (2022).\n[129] Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng Long. 2022. Timesnet: Temporal\n2d-variation modeling for general time series analysis. arXiv preprint arXiv:2210.02186 (2022).\n[130] Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. 2021. Autoformer: Decomposition transformers with\nauto-correlation for long-term series forecasting. Advances in Neural Information Processing Systems 34 (2021),\n22419–22430.\n[131] Jianwen Xie, Ruiqi Gao, Erik Nijkamp, Song-Chun Zhu, and Ying Nian Wu. 2020. Representation learning: A statistical\nperspective. Annual Review of Statistics and Its Application 7 (2020), 303–335.\n[132] Chengjin Xu, Mojtaba Nayyeri, Fouad Alkhoury, Hamed Shariat Yazdi, and Jens Lehmann. 2019. Temporal knowledge\ngraph embedding model based on additive time series decomposition. arXiv preprint arXiv:1911.07893 (2019).\n[133] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2018. How powerful are graph neural networks? arXiv\npreprint arXiv:1810.00826 (2018).\n, Vol. 1, No. 1, Article . Publication date: January 2024.\nA Survey of Deep Learning and Foundation Models for Time Series Forecasting\n35\n[134] Hao Xue and Flora D Salim. 2023. Promptcast: A new prompt-based learning paradigm for time series forecasting.\nIEEE Transactions on Knowledge and Data Engineering (2023).\n[135] Jian Yang, Gang Xiao, Yulong Shen, Wei Jiang, Xinyu Hu, Ying Zhang, and Jinghui Peng. 2021. A survey of knowledge\nenhanced pre-trained models. arXiv preprint arXiv:2110.00269 (2021).\n[136] Xinyu Yang, Zhenguo Zhang, and Rongyi Cui. 2022. Timeclr: A self-supervised contrastive learning framework for\nunivariate time series representation. Knowledge-Based Systems 245 (2022), 108606.\n[137] Chin-Chia Michael Yeh, Xin Dai, Huiyuan Chen, Yan Zheng, Yujie Fan, Audrey Der, Vivian Lai, Zhongfang Zhuang,\nJunpeng Wang, Liang Wang, et al. 2023. Toward a foundation model for time series data. In Proceedings of the 32nd\nACM International Conference on Information and Knowledge Management. 4400–4404.\n[138] Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, and Tie-Yan Liu. 2021.\nDo transformers really perform badly for graph representation? Advances in Neural Information Processing Systems\n34 (2021), 28877–28888.\n[139] Zhihan Yue, Yujing Wang, Juanyong Duan, Tianmeng Yang, Congrui Huang, Yunhai Tong, and Bixiong Xu. 2022.\nTS2Vec: Towards universal representation of time series. In Proceedings of the AAAI Conference on Artificial Intelligence,\nVol. 36. 8980–8987.\n[140] Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. 2023. Are transformers effective for time series forecasting?. In\nProceedings of the AAAI conference on artificial intelligence, Vol. 37. 11121–11128.\n[141] George Zerveas, Srideepika Jayaraman, Dhaval Patel, Anuradha Bhamidipaty, and Carsten Eickhoff. 2021.\nA\ntransformer-based framework for multivariate time series representation learning. In Proceedings of the 27th ACM\nSIGKDD conference on knowledge discovery & data mining. 2114–2124.\n[142] Mengyue Zha, SiuTim Wong, Mengqi Liu, Tong Zhang, and Kani Chen. 2022. Time series generation with masked\nautoencoder. arXiv preprint arXiv:2201.07006 (2022).\n[143] Biao Zhang and Rico Sennrich. 2019. Root mean square layer normalization. Advances in Neural Information Processing\nSystems 32 (2019).\n[144] Kexin Zhang, Qingsong Wen, Chaoli Zhang, Rongyao Cai, Ming Jin, Yong Liu, James Zhang, Yuxuan Liang, Guansong\nPang, Dongjin Song, et al. 2023. Self-Supervised Learning for Time Series Analysis: Taxonomy, Progress, and Prospects.\narXiv preprint arXiv:2306.10125 (2023).\n[145] Yunhao Zhang and Junchi Yan. 2022. Crossformer: Transformer utilizing cross-dimension dependency for multivariate\ntime series forecasting. In The Eleventh International Conference on Learning Representations.\n[146] Zhenwei Zhang, Xin Wang, and Yuantao Gu. 2023. Sageformer: Series-aware graph-enhanced transformers for\nmultivariate time series forecasting. arXiv preprint arXiv:2307.01616 (2023).\n[147] Ziqi Zhao, Yucheng Shi, Shushan Wu, Fan Yang, Wenzhan Song, and Ninghao Liu. 2023. Interpretation of Time-Series\nDeep Models: A Survey. arXiv preprint arXiv:2305.14582 (2023).\n[148] Xiaochen Zheng, Xingyu Chen, Manuel Schürch, Amina Mollaysa, Ahmed Allam, and Michael Krauthammer. 2023.\nSimTS: Rethinking Contrastive Representation Learning for Time Series Forecasting. arXiv preprint arXiv:2303.18205\n(2023).\n[149] Guo-Bing Zhou, Jianxin Wu, Chen-Lin Zhang, and Zhi-Hua Zhou. 2016. Minimal gated unit for recurrent neural\nnetworks. International Journal of Automation and Computing 13, 3 (2016), 226–234.\n[150] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. 2021. Informer:\nBeyond efficient transformer for long sequence time-series forecasting. In Proceedings of the AAAI conference on\nartificial intelligence, Vol. 35. 11106–11115.\n[151] Tian Zhou, Ziqing Ma, Qingsong Wen, Liang Sun, Tao Yao, Wotao Yin, Rong Jin, et al. 2022. FiLM: Frequency improved\nlegendre memory model for long-term time series forecasting. Advances in Neural Information Processing Systems 35\n(2022), 12677–12690.\n[152] Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. 2022. FEDformer: Frequency enhanced\ndecomposed transformer for long-term series forecasting. In International Conference on Machine Learning. PMLR,\n27268–27286.\n[153] Tian Zhou, Peisong Niu, Xue Wang, Liang Sun, and Rong Jin. 2023. One Fits All: Power General Time Series Analysis\nby Pretrained LM. arXiv preprint arXiv:2302.11939 (2023).\n[154] Qingbo Zhu, Jialin Han, Kai Chai, and Cunsheng Zhao. 2023. Time Series Analysis Based on Informer Algorithms: A\nSurvey. Symmetry 15, 4 (2023), 951.\n[155] Rui Zhu, Krzysztof Janowicz, Gengchen Mai, Ling Cai, and Meilin Shi. 2022. COVID-Forecast-Graph: An open\nknowledge graph for consolidating COVID-19 forecasts and economic indicators via place and time. AGILE: GIScience\nSeries 3 (2022), 21.\n, Vol. 1, No. 1, Article . Publication date: January 2024.\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2024-01-25",
  "updated": "2024-01-25"
}