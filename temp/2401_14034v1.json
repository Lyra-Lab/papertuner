{
  "id": "http://arxiv.org/abs/2401.14034v1",
  "title": "Unsupervised Spatial-Temporal Feature Enrichment and Fidelity Preservation Network for Skeleton based Action Recognition",
  "authors": [
    "Chuankun Li",
    "Shuai Li",
    "Yanbo Gao",
    "Ping Chen",
    "Jian Li",
    "Wanqing Li"
  ],
  "abstract": "Unsupervised skeleton based action recognition has achieved remarkable\nprogress recently. Existing unsupervised learning methods suffer from severe\noverfitting problem, and thus small networks are used, significantly reducing\nthe representation capability. To address this problem, the overfitting\nmechanism behind the unsupervised learning for skeleton based action\nrecognition is first investigated. It is observed that the skeleton is already\na relatively high-level and low-dimension feature, but not in the same manifold\nas the features for action recognition. Simply applying the existing\nunsupervised learning method may tend to produce features that discriminate the\ndifferent samples instead of action classes, resulting in the overfitting\nproblem. To solve this problem, this paper presents an Unsupervised\nspatial-temporal Feature Enrichment and Fidelity Preservation framework\n(U-FEFP) to generate rich distributed features that contain all the information\nof the skeleton sequence. A spatial-temporal feature transformation subnetwork\nis developed using spatial-temporal graph convolutional network and graph\nconvolutional gate recurrent unit network as the basic feature extraction\nnetwork. The unsupervised Bootstrap Your Own Latent based learning is used to\ngenerate rich distributed features and the unsupervised pretext task based\nlearning is used to preserve the information of the skeleton sequence. The two\nunsupervised learning ways are collaborated as U-FEFP to produce robust and\ndiscriminative representations. Experimental results on three widely used\nbenchmarks, namely NTU-RGB+D-60, NTU-RGB+D-120 and PKU-MMD dataset, demonstrate\nthat the proposed U-FEFP achieves the best performance compared with the\nstate-of-the-art unsupervised learning methods. t-SNE illustrations further\nvalidate that U-FEFP can learn more discriminative features for unsupervised\nskeleton based action recognition.",
  "text": "Unsupervised Spatial-Temporal Feature Enrichment and Fidelity Preservation Network for\nSkeleton based Action Recognition\nChuankun Lia, Shuai Lib,∗, Yanbo Gaob, Ping Chena, Jian Lia, Wanqing Lic\naSchool of Information and Communication Engineering, North University of China\nbSchool of Control Science and Engineering and School of Software, Shandong University, Jinan 250100, China.\ncAdvanced Multimedia Research Lab, University of Wollongong, Australia\nAbstract\nUnsupervised skeleton based action recognition has achieved remarkable progress recently. Existing unsupervised learning methods\nsuffer from severe overfitting problem, and thus small networks are used, significantly reducing the representation capability.\nTo address this problem, the overfitting mechanism behind the unsupervised learning for skeleton based action recognition is\nfirst investigated. It is observed that the skeleton is already a relatively high-level and low-dimension feature, but not in the\nsame manifold as the features for action recognition. Simply applying the existing unsupervised learning method may tend to\nproduce features that discriminate the different samples instead of action classes, resulting in the overfitting problem. To solve\nthis problem, this paper presents an Unsupervised spatial-temporal Feature Enrichment and Fidelity Preservation framework (U-\nFEFP) to generate rich distributed features that contain all the information of the skeleton sequence. A spatial-temporal feature\ntransformation subnetwork is developed using spatial-temporal graph convolutional network and graph convolutional gate recurrent\nunit network as the basic feature extraction network. The unsupervised Bootstrap Your Own Latent based learning is used to\ngenerate rich distributed features and the unsupervised pretext task based learning is used to preserve the information of the skeleton\nsequence. The two unsupervised learning ways are collaborated as U-FEFP to produce robust and discriminative representations.\nExperimental results on three widely used benchmarks, namely NTU-RGB+D-60, NTU-RGB+D-120 and PKU-MMD dataset,\ndemonstrate that the proposed U-FEFP achieves the best performance compared with the state-of-the-art unsupervised learning\nmethods. t-SNE illustrations further validate that U-FEFP can learn more discriminative features for unsupervised skeleton based\naction recognition.\nKeywords: Skeleton, Action recognition, Graph convolutional network, Unsupervised learning\n1. Introduction\nAction recognition using different modalities (Sun et al.,\n2022; ¨Ozyer et al., 2021) (e.g., video, skeleton) (Li et al., 2023;\nSong et al., 2021; Yang et al., 2021; Hu et al., 2020; Zhang\net al., 2020b; Wang et al., 2018) has been widely studied due to\nits wide use in many potential applications such as autonomous\ndriving and video surveillance. Compared with the conven-\ntional RGB video, 3D skeleton owning high-level representa-\ntion is light-weight and robust to both view differences and\ncomplicated background. Therefore, 3D skeleton based action\nrecognition has been widely investigated with methods based\non handcrafted features (Weng et al., 2017; Xia et al., 2012),\nConvolutional Neural Networks (CNNs) (Ke et al., 2017a,b; Li\net al., 2017; Hou et al., 2018; Li et al., 2019a; Xu et al., 2018;\nLi et al., 2022), Recurrent Neural Networks (RNNs) (Li et al.,\n2018, 2019b; Liu et al., 2018; Song et al., 2017) and Graph\nConvolutional Networks (GCNs) (Yan et al., 2018; Shi et al.,\n∗Corresponding author\nEmail addresses: chuankun@nuc.edu.cn (Chuankun Li),\nshuaili@sdu.edu.cn (Shuai Li), ybgao@sdu.edu.cn (Yanbo Gao),\nchenping@nuc.edu.cn (Ping Chen), lijian@nuc.edu.cn (Jian Li),\nwanqing@uow.edu.au (Wanqing Li)\n2019b; Ye et al., 2020; Zhang et al., 2020a; Shi et al., 2019a;\nKong et al., 2022; Gao et al., 2021; Liu et al., 2022; Peng et al.,\n2021). However, these methods are developed in a fully super-\nvised manner and require extensive annotated labels, which is\nexpensive and time-consuming. Learning general features from\nunlabelled data for 3D skeleton based action recognition is still\nan open problem and highly desired.\nThere are two main approaches for unsupervised skeleton\nbased action recognition. One is to utilize an encoder-decoder\nnetwork and generate useful features by pretext tasks such as\nauto-regression (Su et al., 2020), reconstruction (Zheng et al.,\n2018) and jigsaw puzzle (Lin et al., 2020). This approach ex-\nploits low-level feature representation, and the performance of\nthe downstream task is dependent on the design of pretext tasks.\nExisting methods (Su et al., 2020; Zheng et al., 2018; Lin et al.,\n2020) usually take advantage of the RNNs to encode the input\nskeleton sequence and then regressively predict them. The sec-\nond approach is to utilize the contrastive learning such as Boot-\nstrap Your Own Latent (BYOL) (Grill et al., 2020), Momentum\ncontrast (He et al., 2020) and exploit the discriminative features\namong samples in latent space. The methods (Rao et al., 2021;\nLi et al., 2021; Thoker et al., 2021; Wang et al., 2022) learn fea-\ntures by pulling or pushing the features of different samples as\nPreprint submitted to arXiv\nJanuary 26, 2024\narXiv:2401.14034v1  [cs.CV]  25 Jan 2024\npositive and negative pairs. Putting aside their individual prob-\nlems such as designing relevant task and differentiating positive\nand negative pairs, both approaches suffer from severe overfit-\nting. The existing networks used in supervised learning (Shi\net al., 2019b; Ye et al., 2020; Zhang et al., 2020a; Shi et al.,\n2019a; Kong et al., 2022; Gao et al., 2021; Liu et al., 2022;\nPeng et al., 2021) cannot work effectively in the unsupervised\nlearning due to this severe overfitting. Consequently, the exist-\ning unsupervised learning methods employ very simple models,\neither using only basic RNN models or using very small mod-\nels with fewer neurons (Zhang et al., 2022b,a). However, such\nsimple models with low-dimension features are not capable for\nthe high-level action recognition task, and thus cannot achieve\nhigh performance.\nCurrently, there is no investigation on the mechanism be-\nhind the severe overfitting problem in the unsupervised learn-\ning for skeleton based action recognition. In this paper, we\nfirst study the overfitting mechanism in the unsupervised skele-\nton based action recognition learning and show that the existing\nunsupervised learning method cannot effectively generate fea-\ntures that are highly relevant and useful for action recognition.\nWith skeleton sequences already being relatively high-level and\nlow-dimension representations, the encoder-decoder architec-\nture and the contrastive learning can easily generate features\nrepresenting or differentiating each skeleton sequence, but the\nfeatures may not be useful for the action recognition task. This\ncan be intuitively understood since the high-level skeleton is\nnot in the same manifold as the high-level features for action\nrecognition (considering the example that directly using one\nfully connected layer cannot produce high action recognition\nperformance). This is further illustrated in the following Moti-\nvation Section.\nTo address the above problem, we propose an Unsuper-\nvised spatial-temporal Feature Enrichment and Fidelity Preser-\nvation framework (U-FEFP). The proposed network generates\nrich distributed spatial-temporal features containing all infor-\nmation of the original skeleton. Our contributions can be sum-\nmarized as follows.\n• We investigate the mechanism behind the severe over-\nfitting problem in the unsupervised learning for skele-\nton based action recognition. It is found that features\nrepresenting each skeleton may not be aligned with the\nfeatures for action recognition, leading to the require-\nment of learning rich distributed features. To the best of\nour knowledge, this is the first research that investigates\nthe overfitting mechanism in the unsupervised skeleton\nbased action recognition.\n• Based on our observation on the overfitting mechanism,\nwe develop an unsupervised spatial-temporal feature en-\nrichment and fidelity preservation framework (U-FEFP),\nwhich can learn rich distributed features while preserving\nthe fidelity of the original skeleton sequence.\n• A spatial-temporal feature transformation subnetwork is\ndeveloped by combining the spatial-temporal graph con-\nvolution network (ST-GCN) and the graph convolutional\nGRU network (GConv-GRU). It can effectively learn the\nspatial-temporal features with a relatively small model,\nwhich further reduces the overfitting problem.\n• Exhaustive\nexperiments\non\nNTU-RGB+D-\n60\n(Shahroudy\net\nal.,\n2016),\nNTU-RGB+D-120\n(Liu et al., 2020a) and PKU-MMD (Liu et al., 2020b)\ndatasets verify the capacity of the representations learned\nby our U-FEFP. It achieves state-of-the-art results under\nboth the unsupervised and semi-supervised training.\nThe rest of this paper is organized as follows. Section 2\nbriefly describes the related work in the skeleton based action\nrecognition, including representative supervised and unsuper-\nvised methods. Section 3 illustrates the motivation of this paper,\nand the proposed method is shown in Section 4. Experimental\nresults are presented in Section 5 with detailed ablation study,\nand Section 6 draws the conclusion.\n2. Related works\nIn this section, the works related to the proposed method\nare briefly reviewed including supervised skeleton based ac-\ntion recognition and unsupervised skeleton based action recog-\nnition.\n2.1. Supervised Skeleton based Action Recognition\n2.1.1. Hand-Crafted Feature based Method\nThe hand-crafted skeleton features are widely used in early\naction recognition (Weng et al., 2017; Xia et al., 2012; Vemu-\nlapalli et al., 2014; Evangelidis et al., 2014; Wang et al., 2014).\nFor example, Weng et al.\n(Weng et al., 2017) used Spatio-\nTemporal Naive-Bayes Nearest-Neighbor to capture spatio-\ntemporal structure of skeleton joints. However, the general-\nization ability of hand-crafted skeleton features is weak and\nthese methods perform worse on large datasets such as NTU-\nRGB+D-60 (Shahroudy et al., 2016).\n2.1.2. Deep Learning based Method\nDepending on the type of network, it can be generally clas-\nsified into three categories: CNNs based, RNNs based and\nGCNs based.\nIn the category of CNNs based methods (Ke\net al., 2017a,b; Li et al., 2017; Caetano et al., 2019; Hou et al.,\n2018; Li et al., 2019a; Xu et al., 2018; Banerjee et al., 2021;\nXia et al., 2022; Zhu et al., 2020; Cao et al., 2019), skeleton\nsequence is mapped into a color image and fed into CNNs to\nrecognize action classes. For example, Hou et al. (Hou et al.,\n2018) drew skeleton joints with different colors to generate\nskeleton optical spectra image. Banerjee et al. (Banerjee et al.,\n2021) used distance feature, distance velocity feature, angle\nfeature and angle velocity feature to obtain four grayscale im-\nages. The fuzzy combination is used to fuse scores extracted\nfrom four grayscale images. Xia et al. (Xia et al., 2022) utilized\nconvolutions with attention mechanisms to generate local-and-\nglobal attention network. Zhu et al. (Zhu et al., 2020) designed\n2\na cuboid CNN model with attention mechanism for skeleton-\nbased action recognition, where a cuboid arranging strategy is\nused to organize new action representation between all body\njoints. Although the temporal information is explored to some\nextent by coding the temporal change into an image, its rep-\nresentation capability in temporal modelling is still relatively\nlimited.\nThe second category is to treat skeleton as a sequence and\nuse RNNs to extract spatial-temporal information. It focuses\nmore on the temporal information while the spatial informa-\ntion of skeleton joint is not fully explored. To enhance the cap-\nturing of spatial information, many methods (Li et al., 2018,\n2019b; Liu et al., 2018; Song et al., 2017; Jiang et al., 2020;\nZhang et al., 2018; Ng et al., 2022) have been proposed. For\nexample, Jiang et al. (Jiang et al., 2020) proposed a denoising\nsparse long short-term memory network to decrease the intra-\nclass diversity and extract more spatial-temporal information.\nNg et al. (Ng et al., 2022) proposed the multi-localized sensi-\ntive autoencoder-attention-LSTM to reduce negative variations\nsuch as performers and viewpoints and improve performance.\nZhang et al. (Zhang et al., 2018) selected a set of simple geo-\nmetric features to feed into a multi-stream LSTM architecture\nwith a new smoothed score fusion technique to improve recog-\nnition accuracy. However, these approaches cannot effectively\ncapture relationship among joints.\nIn order to solve this problem, the third approach uses\nGCNs (Yan et al., 2018; Shi et al., 2019b; Ye et al., 2020; Zhang\net al., 2020a; Shi et al., 2019a; Kong et al., 2022; Gao et al.,\n2021; Liu et al., 2022; Peng et al., 2021; Song et al., 2020;\nWu et al., 2021; Liu et al., 2021) to capture topological graph\nstructure of skeleton. For example, Yan et al. (Yan et al., 2018)\nproposed spatial-temporal graph convolutional networks (ST-\nGCN) to extract topological spatial-temporal features, where a\nstatic graph is used to capture relationship among joints. How-\never, a static graph is not suitable for all different actions and\ncannot extract dynamic features among spatial joints. In order\nto solve this problem, existing methods adaptively learned the\ntopology of skeleton through attention or other similar mech-\nanisms. For example, Liu et al. (Liu et al., 2021) proposed\na Graph Convolutional Networks-Hidden conditional Random\nField (GCN-HCRF) model to construct multi-stream frame-\nwork. Generally, GCNs based methods have achieved the state-\nof-art performance for supervised skeleton based action recog-\nnition.\nWhile the supervised learning based methods have greatly\nadvanced in the last few years and achieved good performance,\nthese methods require massive labels for training and cannot\neffectively work for unlabeled skeleton data. Therefore, unsu-\npervised skeleton based action recognition methods are highly\ndesired.\n2.2. Unsupervised Skeleton based Action Recognition\n2.2.1. Self-supervised learning based Method\nPretext tasks are designed to extract discriminative features\nin self-supervised learning based methods. Zheng et al. (Zheng\net al., 2018) used the encoder-decoder model and the Gener-\native Adversarial Network (GAN) to reconstruct the skeleton\nsequence. Su et al. (Su et al., 2020) designed an autoencoder\nstructure with a weak decoder using recurrent neural network\nto learn more robust features from skeleton sequence. Lin et\nal. (Lin et al., 2020) proposed two pretext tasks including mo-\ntion prediction and Jigsaw puzzle recognition to learn more\ngeneral representations. However these methods usually used\nRNNs to extract temporal features where the spatial informa-\ntion are not mined effectively.\n2.2.2. Contrastive Learning based Method\nRao et al. (Rao et al., 2021) used momentum LSTM with a\ndynamic updated memory bank as the model, and augmented\ninstances of the skeleton sequence are contrasted to learn fea-\nture representation.\nLi et al. (Li et al., 2021) designed a\ncross-view contrastive learning scheme and leveraged multi-\nview complementary supervision signal. Thoker et al. (Thoker\net al., 2021) designed several skeleton-specific spatial and tem-\nporal augmentations to construct skeleton intra-inter contrastive\nlearning.\nLin et al. (Lin et al., 2023) proposed a new ac-\ntionlet dependent contrastive learning by treating motion and\nstatic regions differently. Zeng et al. (Zeng et al., 2023) pro-\nposed a Cross Momentum Contrast (CrossMoCo) framework\nto learn local and global semantic features and used two in-\ndependent negative memory banks to improve high-quality of\nnegative samples. Gao et al. (Gao et al., 2023) proposed spatio-\ntemporal contrastive learning using different spatio-temporal\nobservation scenes to build contrastive proxy tasks. Shah et\nal. (Shah et al., 2023) proposed Hallucinate Latent Positives for\ncontrastive learning to generate new positives and improve per-\nformance. These methods need to design different positive and\nnegative pairs to improve performance. Zhang et al. (Zhang\net al., 2022b) proposed a skeleton-based relation consistency\nlearning scheme to expand the contrastive objects from individ-\nual instance to the relation distribution between instances, and\ntarget at pursuing the relationship consistency learning between\ndifferent instances.\nZhang et al. (Zhang et al., 2022a) used\nBarlow Twins’ objective function to minimize the redundancy\nand keep similarity of different skeleton augmentations. How-\never, these methods cannot effectively capture robust and dis-\ncriminative features for action recognition. Moreover, both the\nself-supervised learning based methods and contrastive learn-\ning based methods suffer from severe overfitting problem and\nonly very small networks can be used, leading to reduced rep-\nresentation capability.\n3. Motivation\nAs mentioned in the Introduction and Related Work sec-\ntions, existing unsupervised learning for skeleton based action\nrecognition methods suffer from severe overfitting problem. As\nshown in Fig. 5, the test performance is much worse than the\ntraining performance when using the existing models. Detailed\ndescriptions on the experimental setup and analysis are shown\nin the following Subsection 5.3. Considering that unsupervised\nlearning naturally can use much more data without label than\nthe supervised one, this work focuses more from the perspective\n3\n(a) P&C (Su et al., 2020)\n(b) ASCAL (Rao et al., 2021)\n(c) Adaptive GCN (Shi et al., 2019b) with unsuper-\nvised learning\n(d) Proposed U-FEFP\n(e) Adaptive GCN (Shi et al., 2019b) with supervised\nlearning\nFigure 1: t-SNE visualization of the learned features of different methods on the cross-subject of NTU-RGB+D-60. 60 samples are selected for each class on the\ndataset. (a) Unsupervised learning based on pretext task, P&C (Su et al., 2020). (b) Unsupervised contrastive learning with the momentum LSTM, ASCAL (Rao\net al., 2021). (c) Unsupervised contrastive learning with the adaptive GCN (Shi et al., 2019b). (d) Proposed U-FEFP. (e) Supervised learning with the adaptive\nGCN (Shi et al., 2019b).\nof constructing a new model less prone to overfitting instead\nof data augmentation and regulation (Shorten and Khoshgof-\ntaar, 2019; Li et al., 2020; Zhong et al., 2020; Lim et al., 2019;\nYoo et al., 2020; Loshchilov and Hutter, 2017; Lu et al., 2021).\nSpecifically, we study why one model working well in super-\nvised learning leads to overfitting in unsupervised learning for\nskeleton based action recognition, and this behaviour has not\noccurred in the image related unsupervised learning. Instead of\ndirectly reducing the number of parameters used in the network\n(Zhang et al., 2022b,a) which in turn reduces the capability of\nthe network, this paper first investigates the mechanism behind\nthis severe overfitting problem. Then based on the observa-\ntion and this overfitting mechanism, our learning framework,\nU-FEFP, is proposed.\nFirst, to illustrate the differences between the features\nlearned with unsupervised learning and supervised learning,\nt-SNE (van der Maaten and Hinton, 2008) is used to visual-\nize their embedding clustering. The visual illustration shows\nhow the embedding of the same class of actions form clusters\nwhile different classes of actions are separated. Features from\nthree unsupervised learning methods, including unsupervised\nlearning based on pretext task (P&C (Su et al., 2020)), unsu-\npervised contrastive learning with the momentum LSTM (AS-\nCAL (Rao et al., 2021)), unsupervised contrastive learning with\nthe adaptive GCN (Shi et al., 2019b), are illustrated, and fea-\ntures from the supervised learning with the adaptive GCN (Shi\net al., 2019b) are used for comparison. The t-SNE comparison\nis shown in Fig. 1 using 60 samples from each action class.\nFirst, by visualizing the t-SNE illustration of the supervised\nGCN in Fig. 1(e), it can be seen that the samples are clustered\n4\nwell according to different actions, leading to the good classi-\nfication results with supervised learning. On the contrary, the\nt-SNE illustrations of the unsupervised learning with different\nmethods in Figs. 1(a), 1(b) and 1(c) show that the samples are\nalso grouped to some extent, but not according to their action\nclasses, thus producing poor results. Especially comparing the\nt-SNE illustrations of the adaptive GCN under supervised and\nunsupervised learning in Figs. 1(e) and 1(c), it can be clearly\nseen that with the same network, the features are learned com-\npletely differently, in terms of their clustering behaviour to the\naction classes. The features of the supervised learning GCN are\ngrouped according to the action classes while the features with\nthe unsupervised learning GCN are also grouped to some ex-\ntent, but not strongly related to the action classes. Intuitively,\nthis can be analyzed as the choice of the negative samples not\nhighly related to action recognition, leading to the difficulty in\nchoosing negative samples in unsupervised learning. Moreover,\nconsidering the samples are also grouped to some extent in un-\nsupervised learning, this demonstrates that the skeletons are\nalso high-level features that can be discriminated easier than\naction classes.\nAs a matter of fact, the skeleton sequences are already rel-\natively high-level and low-dimension representations. In such\na case, unsupervised learning tends to produce features that di-\nrectly discriminate or reconstruct samples, and such features\nmay not be useful for action recognition. In other words, the\nfeatures learned in the unsupervised way distribute in a high-\nlevel manifold that is not aligned with the high-level feature\nmanifold of the action recognition. Accordingly, the loss in the\nunsupervised learning can be very small while the loss of the\naction recognition is very high, leading to the overfitting prob-\nlem. To overcome this problem, we propose to generate rich\ndistributed spatial-temporal features containing all information\nof the original skeleton in unsupervised learning. The rich dis-\ntributed spatial-temporal features contain distributed features\nthat can be useful for action recognition, instead of pushing\nfeatures to discriminate certain samples that may narrow the\nrepresentation capability of the features. Constraining the fea-\ntures to be containing all information of the original skeleton\nencourages the network to preserve all useful information. To\nthe best of our knowledge, this is the first research that clearly\npoints out to learn such features in the unsupervised skeleton\nbased action recognition. Based on this observation, we pro-\npose a U-FEFP learning framework, which is explained in the\nnext section. The t-SNE illustration of our U-FEFP learning\nframework is shown in Fig. 1(d). Compared to the other unsu-\npervised learning methods shown in Figs. 1(a), 1(b) and 1(c),\nour U-FEFP clearly produces features that are better aligned\nwith the action classes. Although certain samples may deviate\nfrom the action centers, they are also away from other action\ncenters, making them easier for recognition.\n4. Proposed Method\nThe framework of the proposed unsupervised spatial-\ntemporal feature enrichment and fidelity preservation network\n(U-FEFP) is shown in Fig. 2, consisting of two parts: unsuper-\nvised BYOL based feature enrichment learning and unsuper-\nvised pretext task based fidelity preservation learning. The un-\nsupervised BYOL based feature enrichment learning, including\nthe online network and target network, is developed for spatial-\ntemporal feature transformation, in order to obtain a rich dis-\ntributed spatial-temporal feature representation. The unsuper-\nvised pretext task based fidelity preservation learning, including\nthe online network and the decoding network, is developed for\nspatial-temporal feature fidelity preservation, in order to keep\nthe original skeleton information. The details of the proposed\nU-FEFP are presented in the following.\n4.1. Unsupervised BYOL based Learning for Spatial-temporal\nFeature Enrichment\n4.1.1. Spatial-temporal Feature Transformation Network\nA spatial-temporal graph convolution subnetwork (ST-\nGCN) followed by a graph convolutional GRU network\n(GConv-GRU) is developed as the basic architecture of the\nonline network and target network used in our unsupervised\nBYOL based learning, to produce the spatial-temporal features.\nST-GCN takes advantage of the graph convolution to extract\nthe spatial-temporal feature of the skeleton. With the expres-\nsive power of graph convolution in processing non-grid data\nlike skeleton, it can obtain rich spatial features. Moreover, con-\nsidering the temporal change of each joint among frames is also\nimportant in characterizing the spatial features of a skeleton to\nbe representative and discriminative against others, ST-GCN is\nused to obtain spatial and short-term temporal features. In each\nST-GCN block, it consists of one spatial graph convolution ex-\ntracting the spatial information and one temporal convolution\nmining the short temporal information. The basic structure of\nthe graph convolution is the same as (Yan et al., 2018), which\nis not further detailed here. Four ST-GCN blocks are used and\nthe numbers of convolution kernels are 32, 64, 128 and 512,\nrespectively. In order to reduce the computation, the stride of\ntemporal convolution is set to 2 in the fourth ST-GCN block,\nwhich halves the length of the temporal features.\nFor capturing the long-term temporal features, a GConv-\nGRU network is used, which aggregates the spatial and short-\nterm temporal features from ST-GCN in order to achieve long-\nterm information. On one hand, it is found that using ST-GCN\nfor complete spatial-temporal feature extraction is easily over-\nfitting, since multiple ST-GCN layers are needed to extract the\nlong-term temporal features, making the network complex. By\ncontrast, in our method, as shown in Fig. 2, only four ST-GCN\nblocks (versus ten blocks in the conventional ST-GCN meth-\nods (Yan et al., 2018)) are used to reduce overfitting. On the\nother hand, GConv-GRU, due to its recurrent structure, is more\nsuitable for decoding features in the following temporal pretext\ntask (described in the next subsection). Therefore, a sequential\narchitecture combining the ST-GCN and GConv-GRU is used\nin this paper.\nFor the GConv-GRU, while the recurrent structure aggre-\ngates the temporal information, its sequential processing also\nincurs great computation if the processing of each step is com-\n5\nS1\nSt\nSf\nS1\nSt\nSf\nx\nfξ\nfθ \nST-GCN\ngθ \ngξ\nST-GCN\nTarget Network\nSf\nSt\nS2\nh1 \nht \nhf/2 \nLoss for Pretext task\nGConv-GRU\nGConv-GRU\ny\nOnline Network\nxf\nxt\nx1\nxf\nxt\nx1\n̅S1\n̅St \n̅Sf \nST-GCN block\nGConv-GRU\nMLP\nConv\nsg\nLoss for BYOL\nx1\nxt\nxf\ny1\nyt\nyf\nSkeleton Sequence\nhf/2 \n(\n)\nq\nz\n\n\n(\n)\ng\nz\n\n\nI\nGCN\nBYOL\nPretext task\nqθ \nFigure 2: The framework of the proposed U-FEFP, with unsupervised BYOL based feature enrichment learning and unsupervised pretext task based fidelity\npreservation learning. It consists of an online network (in green), a target network (in blue) and a reversed prediction network (in beige). The online network is\ntrained to learn rich representations and the target network is slowly updated by the exponential moving average of the online network to make them asynchronous.\nThe reversed prediction network is used to reconstruct the skeleton sequence with the features generated by the online network. The BYOL based contrastive\nlearning (within the black dash box) and the reversed prediction (pretext task) based learning (within the red dash box) are used to keep similarity of different\nskeleton augmentations at feature and instance level, respectively.\nputationally expensive. Here, considering the features are cap-\ntured via the ST-GCN with graph convolution, the spatial struc-\nture information is already contained in the input features to the\nGConv-GRU. Therefore, in order to reduce computation, gen-\neral convolution, with 1*1 kernel for per-joint processing, is\nused in the recurrent update of each GRU step. To make the in-\nput processing consistent with the recurrent processing, general\nconvolution is also used in the input processing. The hidden\noutput of the GConv-GRU is further enhanced with graph con-\nvolution, which can be computed in parallel over all time steps\nwith less computation complexity and enhancing the temporal\nfeatures with the spatial structure. The update of the GConv-\nGRU is shown in Fig. 3.\n4.1.2. BYOL based Feature Enrichment Learning\nAs mentioned in the Motivation, for unsupervised learning,\nrich distributed features are highly desired. It is required to pro-\nduce a rich set of distributed high-level representation features\nthat is useful to discriminate different samples and useful for the\ndownstream high-level task, i.e., action recognition. Naively\nwe can generate a set of features with a network of random\nparameters. However, this cannot provide view-invariant (shift-\ninvariant, pose-invariant, etc.) high-level features. Thanks to\nthe BYOL (Grill et al., 2020) based feature learning, rich dis-\ntributed features can be learned with two asymmetric networks,\ni.e., an online network and a target network as shown in Fig. 2,\nby pulling together the features of different augmented versions\nX\nσ  \ntanh\n1-\nσ  \nX\nX\n+\nh0\nc1\nh1\nGraph \nconvolution\nGraph \nconvolution\no1\not\nParameter shared\nConv\nConv\nConv\nX\nσ  \ntanh\n1-\nσ  \nX\nX\n+\nht-1\nct\nht\nConv\nConv\nConv\nFigure 3: The structure of the GConv-GRU\nof one sample.\nAs shown in the Fig. 2, data augmentation is first used to\nenhance a skeleton sequence to different views. Suppose that\nan original skeleton sequence S = (S1, · · ·, Sf) contains f con-\nsecutive skeleton frames, where Si ∈RN×3 is 3D coordinates of\nN skeleton joints. The data augmentation strategy in (Thoker\net al., 2021) (i.e., spatial augmentation and temporal augmen-\ntation) and rotation are used to transform S into its augmented\nversions x and y. For spatial augmentation, pose augmentation\nand joint jittering are randomly selected. For temporal augmen-\n6\ntation, temporal crop-resize by randomly selecting a starting\nframe and a sub-sequence length, and then resizing the sub-\nsequence to a fixed length is used. For rotation, an random axis\nwith random rotation is selected. The augmented skeleton se-\nquence does not change the graph structure of a skeleton and\nthus same graph convolution can be used.\nThe two different views of the samples are then processed\nby the online network and target network, generating the\nspatial-temporal features. Then two nonlinear projectors gθ and\ngξ are used to project the hidden features into a new feature\nspace. The two nonlinear projectors use same structure and are\nupdated in the same way with online network and target net-\nwork. The nonlinear projector consists of two fully connected\nlayers. The first fully connected layer is of 1024 neurons fol-\nlowed by a batch normalization layer and a relu activation layer.\nThe second one is of 512 neurons generating features without\nthe normalization and activation. This up-projects the features\nback to 512 channels to enrich the representation.\nAs in BYOL framework, asymmetric architecture is used\nand a predictor qθ using the same network as the nonlinear pro-\njector gθ is added only to the online branch to produce the pre-\ndiction qθ(zθ), where zθ is output of the projector gθ. For the\ntarget network, the stop-gradient operation is used after the non-\nlinear projector gξ and obtains feature gξ(zξ), where zξ is output\nof the target network. Then qθ(zθ) and gξ(zξ) are normalized\nwith the ℓ2-norm separately as\n¯qθ(zθ)\n∆= qθ(zθ)/∥qθ(zθ)∥2\n(1)\n¯gξ(zξ)\n∆= gξ(zξ)/\n\r\r\rgξ(zξ)\n\r\r\r2\n(2)\nFinally, Mean Square Error (MSE) objective function be-\ntween ¯qθ(zθ) and ¯gξ(zξ) is used to construct the self-supervised\nloss and can be expressed as:\nLθ,ξ =\n\r\r\rqθ(zθ) −gξ(zξ)\n\r\r\r2\n2\n(3)\nwhich can be further transformed by substituting ¯qθ(zθ) and\n¯gξ(zξ) with Eqs. (1) and (2), respectively, to the following:\nLθ,ξ = 2 −2 ·\nD\nqθ(zθ), gξ(zξ)\nE\n∥qθ(zθ)∥2 ·\n\r\r\rgξ(zξ)\n\r\r\r2\n(4)\nThe loss is symmetrized and a symmetric loss Lθ,ξ\n′ can be ob-\ntained by feeding the x and y into target network and online\nnetwork, respectively. Finally, the learning loss is obtained as\nLBYOL = Lθ,ξ + Lθ,ξ\n′.\nIn the training process, weights ξ of target network are up-\ndated using the exponential moving average of the online net-\nwork weight θ which follows τξ + (1 −τ)θ →ξ. This allows\nthe online network and target network to be always asymmetric.\nThe online and target GConv-GRU network produce a temporal\nfeature with half the time steps of the skeleton sequence. While\nthe features of all time steps can be processed as above, in this\npaper for simplicity, a global pooling over the temporal dimen-\nsion is used to generate the final 256 dimension feature and then\nprocessed.\nGCN\nSf\nSt\nS2\n̅S1\n̅S2\n̅S3\n̅S4\n̅S5\n̅Sf-1\n̅Sf\nConv\nGConv-GRU\nhf/2\nRandom \ninitial \nskeleton\nReversed \nskeleton \ninput\nI\nx'f\nx't-1\nx'1\nx'f-1\nGConv-GRU\nGConv-GRU\nGConv-GRU\nGConv-GRU\nGConv-GRU\nGConv-GRU\nFigure 4: The structure of the decoder in the unsupervised pretext task based\nlearning.\nThis BYOL based feature learning enables the online net-\nwork to generate rich distributed high-level features. Simply\nspeaking (as an extreme case for intuitive understanding), the\ntarget network produces a rich set of randomly combined fea-\ntures, while the learning updates the online network and tar-\nget network to produce view-invariant high-level features. The\nasymmetric updating avoids them to generate the trivial so-\nlutions of zero or other fixed representations. Therefore, the\nonline network produces rich distributed high-level features,\nwhich is further constrained by the pretext task (described in\nthe following subsection) and used for the final action recogni-\ntion.\n4.2. Unsupervised Pretext Task based Learning for Spatial-\ntemporal Feature Fidelity Preservation\nWhile the above BYOL based learning generates rich dis-\ntributed features that can keep the similarity of augmented\ndifferent-view skeleton data at instance level, it cannot ensure\nthe capability of the generated features to be able to classify all\nactions. In other words, the representation space of the gen-\nerated features may be reduced since there is no constraint in\ndiscriminating different samples or actions. Therefore, to gen-\nerate features that are not only rich distributed but also full and\ncontain as much information of the original skeleton as possi-\nble, a fidelity preservation constraint is required.\nIn this paper, an unsupervised pretext task based learning is\ndesigned using reversed prediction. Motivated by the pretext\ntask using encoder-decoder network (Su et al., 2020; Zheng\net al., 2018), the feature of the online network is used as the\nencoder feature, and a decoder is used to predict the skeleton\nsequence. To be specific, the skeleton sequence in the reverse\norder is used with each skeletal joint predicted. In this way, the\nhidden state obtained from the encoder (which is at the time\nstep of the last skeleton) can be directly used for predicting\nthe skeleton at its corresponding time step. The decoder used\nin this paper consists of one GCN, one GConv-GRU and one\nConvolution as shown in Fig. 4. At each time step, the previous\n7\nskeleton (also in the reverse order) is first fed into GCN to gen-\nerate features d as input, and with the recurrent hidden feature\nfrom GConv-GRU at the previous step, the network predicts\nthe skeleton. For the first time step, the encoder (online net-\nwork) feature is used as the initial recurrent hidden feature and\na randomly initialized skeleton is used as the input. The update\nprocess can be expressed as\n(ht, St) =\n(\nΘ(hf/2, d1)\nt = 1\nΘ(ht−1, dt−1)\nt > 1\n(5)\nwhere Θ(·) denotes the decoder GConv-GRU. hf/2 is the en-\ncoder feature from the online network, i.e., the hidden state at\nthe last time-stamp. When the decoding is initialized (t=1), hf/2\nand randomly initialized d1 is provided to the decoder GConv-\nGRU, producing the recurrent hidden features h1, and the out-\nput feature S1. Then S1 is processed with convolution to pro-\nduce the first skeleton (in the reverse order). For the follow-\ning time steps, dt−1, the feature of the skeleton at the previous\ntime step, is used as the input and ht−1 is used as the recur-\nrent input. Finally, MSE between the output sequence x′ and\nx (reversed sequence of x) is used as loss of the pretext task\nbased unsupervised learning, LP =\n\r\r\rx′ −x\n\r\r\r2\n2. The number of\nconvolutional kernel is the same with spatial GCN of the fourth\nST-GCN block in the online network.\nThis unsupervised pretext task based learning enforces the\nfeatures generated from the online network to contain all the\ninformation of the skeleton so as to predict the original skele-\nton. Together with the BYOL based learning, the proposed U-\nFEFP learning framework is jointly trained by using the total\nloss L = LBYOL + LP, to generate rich distributed and fidelity\npreserved features. The designed spatial-temporal feature trans-\nformation network of combining ST-GCN and GConv-GRU\nmakes the features rich of spatial-temporal information useful\nfor action recognition.\n5. Experimental Results\n5.1. Datasets\nThree widely used datasets, including the NTU RGB+D-60\ndataset, NTU-RGB+D-120 dataset and PKU-MMD dataset, are\nused for evaluating the proposed method.\nNTU RGB+D-60 dataset (NTU-60): NTU-60 (Shahroudy\net al., 2016) is one of the largest indoor-captured dataset for\nhuman action recognition task, which has been currently widely\nused. It is performed by 40 persons with different ages. This\ndataset contains 4 million frames and 56880 skeleton sequences\ncaptured by the Microsoft Kinect v2, and it consists of two side\nviews, front view and left, right 45 degree views. We adopt the\nsame training and testing protocols including the cross-subject\n(X-sub) and the cross-view (X-view) settings, as in (Shahroudy\net al., 2016), which is not further detailed here.\nNTU-RGB+D-120 dataset (NTU-120):\nNTU-120 (Liu\net al., 2020a) is an extended version of NTU-60. It consists of\n114480 action clips that are captured from 106 distinct human\nsubjects. The action samples are captured from 155 different\ncamera viewpoints. The subjects in this dataset are in a wide\nTable 1: Comparison of different feature transformation networks as online\nnetwork\nMethod\nX-Sub (%) X-View (%)\nMS-G3D (Liu et al., 2020c)+ BYOL\n50.16\n54.92\nCTR-GCN (Chen et al., 2021)+ BYOL\n51.25\n55.68\n2s-AGCN (Shi et al., 2019b) + BYOL\n53.68\n56.89\nST-GCN (Yan et al., 2018)+ BYOL\n75.42\n79.50\nonline network v1 + BYOL\n74.88\n80.26\nonline network v2 + BYOL\n77.85\n82.68\nonline network v3 + BYOL\n71.22\n76.98\nonline network v4 + BYOL\n78.12\n82.80\nonline network v5 + BYOL\n79.31\n84.20\nProposed online network + BYOL\n80.55\n85.62\nTable 2: Comparison of different modules in the proposed method\nMethod\nX-Sub (%) X-View (%)\nProposed online network + BYOL\n80.55\n85.62\nProposed online network + pretext task\n67.88\n73.96\nU-FEFP\n82.50\n87.52\nrange of age distribution (from 10 to 57) and from different cul-\ntural backgrounds (15 countries), which brings very realistic\nvariation to the quality of actions. We use cross-subject (X-\nSub) and cross-setup (X-Set) adopted in (Liu et al., 2020a) to\nevaluate the proposed method.\nPKU-MMD dataset:\nPKU-MMD dataset (Liu et al.,\n2020b) has nearly 20000 action clips in 51 action categories.\nTwo subsets PKU-MMD I and PKU-MMD II are used in the ex-\nperiments. PKU-MMD II is more challenging than PKU-MMD\nI due to higher level of noise. Experiments are conducted on the\ncross subject (X-Sub) benchmark for both subsets.\n5.2. Implementation Details\nUnsupervised Pre-training: We use the PyTorch frame-\nwork to implement the proposed U-FEFP and run it on four\nTesla A100 GPUs.\nLARS (Zhang et al., 2022b) is selected\nas optimizer and trained for 1500 epochs with a cosine decay\nschedule. The learning rate starts at 0 and is linearly increased\nto 2.0 in the first 25 epochs of training and then decreased to\n0.001 by a cosine decay schedule. We follow the paper (Li\net al., 2021) to downsample 50 frames for each skeleton se-\nquence. Target decay rate τ used in the BYOL based learning\nis set to 0.99, which is verified in the following ablation study.\nLinear Evaluation Protocol: The online network is frozen,\nand a fully connected layer is appended to online network and\ntrained for action recognition task. Cross Entropy loss of action\nrecognition is used as the objective function.\n5.3. Ablation Study\nIn this section, the effectiveness of our proposed U-FEFP\nis validated from five aspects: evaluation of the proposed on-\nline network, combining the BYOL learning and pretext task\n8\n0\n200\n400\n600\n800\n1000\n1200\n1400\nEpoch\n0.0\n0.1\n0.2\n0.3\n0.4\nloss\nUnsupervised Pre-training\nProposed U-FEFP\n2s-AGCN\n0\n20\n40\n60\n80\n100\nEpoch\n20\n30\n40\n50\n60\n70\n80\nAccuracy\nLinear Evaluation Protocol\nProposed U-FEFP Finetuning\nProposed U-FEFP Test\n2s-AGCN Finetuning\n2s-AGCN Test\n(a)\n(b)\nFigure 5: Comparisons of the proposed U-FEFP and 2s-AGCN in the process of unsupervised pre-training (a) and fine-tuning/test in the linear evaluation protocol\n(b), respectively.\nbased learning, different target decay rates, different batch sizes\nand semi-supervised learning. The experiments on the NTU-60\ndataset are used for all the ablation studies.\nEvaluation of the proposed online network: In order to\nvalidate the proposed spatial-temporal feature transformation\nnetwork as the online network, the BYOL scheme is used with\ndifferent models as online network for comparison, including\nMS-G3D (Liu et al., 2020c), CTR-GCN (Chen et al., 2021), 2s-\nAGCN (Shi et al., 2019b) and ST-GCN (Yan et al., 2018). The\nconfigurations are listed as follows and all configurations are\ntrained from scratch in the same way as the proposed method.\n• MS-G3D (Liu et al., 2020c)+ BYOL: MS-G3D (Liu\net al., 2020c) network is used as the online and target\nnetworks for the BYOL based learning.\n• CTR-GCN\n(Chen\net\nal.,\n2021)+\nBYOL:\nCTR-\nGCN (Chen et al., 2021) network is used as the\nonline and target networks for the BYOL based learning.\n• 2s-AGCN (Shi et al., 2019b) + BYOL: 2s-AGCN (Shi\net al., 2019b) network is used as the online and target\nnetworks for the BYOL based learning.\n• ST-GCN (Yan et al., 2018) + BYOL: ST-GCN (Yan et al.,\n2018) network is used as the online and target networks\nfor the BYOL based learning.\nThe results of comparison are listed in Table 1. From the\nresults, it can be seen that ST-GCN (Yan et al., 2018) performs\nbetter than other supervised methods (Liu et al., 2020c; Chen\net al., 2021; Shi et al., 2019b), and the proposed online network\nfurther outperforms ST-GCN (Yan et al., 2018) and achieves\nthe best performance in the BYOL based learning. Moreover,\nablation experiment on different structural compositions of our\nfeature transformation network is also conducted. Different lay-\ners of ST-GCN and GConv-GRU are used to construct different\nversions of the online network, including\n• v1: 8 ST-GCN layers + 1 GConv-GRU layer\n• v2: 6 ST-GCN layers + 1 GConv-GRU layer\n• v3: 2 ST-GCN layers + 1 GConv-GRU layer\n• v4: 4 ST-GCN layers + 0 GConv-GRU layer (temporal\npooling instead)\n• v5: 4 ST-GCN layers + 2 GConv-GRU layer\n• Proposed: 4 ST-GCN layers + 1 GConv-GRU layer\nThe results of comparison are also listed in Table 1. It can\nbe seen that the proposed online network with 4 ST-GCN layers\n+ 1 GConv-GRU layer performs the best. It can also be seen\nthat when increasing the layers of ST-GCN or GConv-GRU\nover the proposed one, the performance can no longer be im-\nproved. This behaves differently to the supervised learning net-\nworks such as the ST-GCN (Yan et al., 2018) with deep layers,\nindicating that it tends to be overfitting for unsupervised skele-\nton action recognition learning as described in Section 3. More-\nover, it cannot extract effective features with too few layers of\nST-GCN such as online network v3. This validates the effec-\ntiveness of our feature transformation network in extracting the\nspatial-temporal features and in reducing the overfitting (with\nless parameters than ST-GCN (Yan et al., 2018)). Also from\nthe perspective of computation, in practical use, only the pro-\nposed online network and the final output layer for recognition\nis needed and thus takes less complexity than the supervised\nST-GCN (Yan et al., 2018). Therefore, four ST-GCN layer and\none GConv-GRU layer is used for the proposed online network\nin the following experiments.\n9\nEvaluation of combining BYOL based learning and pre-\ntext task based learning: As discussed in the Motivation, rich\ndistributed spatial-temporal features containing all information\nof the original skeleton need to be generated in unsupervised\nlearning. In order to validate this, the proposed U-FEFP is com-\npared with the two separate modules, proposed online network\nwith BYOL based learning and proposed online network with\npretext task based learning. The results are shown in Table 2.\nThe proposed U-FEFP combining the BYOL and pretext task\nbased learning outperforms the two separate modules, validat-\ning they can complement each other. Moreover, the results of\nBYOL based learning significantly outperforms the pretext task\nbased learning, validating our argument in Motivation that rich\ndistributed features in unsupervised learning matters the most\nsince skeleton is already high-level and low-dimension features.\nEvaluation of the overfitting under different methods:\nIn order to illustrate the overfitting problem described in Sec-\ntion 3, the unsupervised pre-training, fine-tuning and test pro-\ncesses of the proposed U-FEFP and 2s-AGCN are visualized\nas shown in Fig.\n5.\nThe fine-tuning and test processes re-\nfer to the fine-tuning and test in the linear evaluation proto-\ncol where only one last fully connected layer is trained. In\nFig. 5, the unsupervised pre-training process is characterized\nin terms of loss while the fine-tuning and test processes are in\nterms of accuracy and the test accuracy is achieved for each\nepoch in the fine-tuning process. By comparing the unsuper-\nvised pre-training and fine-tuning/test in Fig. 5, it can be seen\nthat while 2s-AGCN achieves much smaller loss in the unsu-\npervised pre-training stage, it performs significantly worse than\nthe proposed U-FEFP in the fine-tuning/test in the linear eval-\nuation protocol stage, validating its overfitting. Moreover, by\ncomparing the fine-tuning and test in Fig. 5, it can be seen that\nthe accuracy gap between fine-tuning and test of 2s-AGCN is\nmuch larger that of the proposed U-FEFP. While the 2s-AGCN\nis fine-tuned to a relatively better performance, the gap is also\nbecoming much larger. By contrast, the performance of the pro-\nposed U-FEFP is significantly better than 2s-AGCN with a very\nsmall gap between fine-tuning and test, and only gets slightly\nincreased in the fine-tuning process. This demonstrates that the\nproposed U-FEFP is much less prone to overfitting compared\nto the existing methods.\nEvaluation of different target decay rates: The proposed\nU-FEFP is also evaluated with different target decay rates used\nin the BYOL based learning, and results are shown in Table 3.\nIt can be seen that proposed U-FEFP performs better when τ is\n0.99. Therefore, target decay rate is set to 0.99 in our experi-\nments.\nTable 3: Comparison of different target decay rates\nτ\nX-Sub (%)\nX-View (%)\n0.9\n80.84\n85.92\n0.99\n82.50\n87.52\n0.999\n80.86\n86.14\nEvaluation of different batch sizes: The proposed U-\nTable 4: Comparison of different batch sizes\nBatch-size\nX-Sub (%)\nX-View (%)\n64\n79.72\n84.54\n128\n80.31\n85.92\n256\n82.10\n87.04\n512\n82.50\n87.52\n1024\n82.63\n87.66\nTable 5: Comparison of different methods in the semi-supervised setting on\nNTU RGB+D 60 dataset.\nMethod\nLabel\nfraction(%)\nX-Sub\n(%)\nX-View\n(%)\nLongT GAN (Zheng et al., 2018)\n1\n35.20\n-\nMS2L (Lin et al., 2020)\n1\n33.10\n-\nISC (Thoker et al., 2021)\n1\n35.70\n38.10\nSKT (Zhang et al., 2022a)\n1\n43.20\n44.90\nSRCL (Zhang et al., 2022b)\n1\n52.60\n53.30\nU-FEFP\n1\n56.20\n57.90\nLongT GAN (Zheng et al., 2018)\n10\n62.00\n-\nMS2L (Lin et al., 2020)\n10\n65.20\n-\nISC (Thoker et al., 2021)\n10\n65.90\n72.50\nSKT (Zhang et al., 2022a)\n10\n67.60\n71.30\nSRCL (Zhang et al., 2022b)\n10\n69.30\n76.20\nU-FEFP\n10\n73.80\n79.40\nFEFP is also evaluated with different batch sizes and results\nare shown in Table 4. It can be seen that using larger batch size\nperforms better, because the variation in a large batch size may\nimprove the BYOL based learning performance. However, the\nimprovement is limited using 1024 batch-size which requires\nlarge GPU memory. Therefore, batch size is set to 512 in our\nexperiments.\nEvaluation of semi-supervised learning: The proposed\nU-FEFP is also verified in the semi-supervised learning way.\nFirstly, the online network is trained by unsupervised manner\nand fine-tuned with 1% and 10% labeled data, respectively. The\nresults compared with the existing methods are shown in Ta-\nble 5. It can be seen that compared with other unsupervised\nlearning methods (in the same semi-supervised learning set-\nting), the proposed U-FEFP also achieves better performance.\nThis validates that the proposed U-FEFP can work in different\ntraining settings and perform better than others.\n5.4. Comparison with the State-of-the-Art Methods\nThe proposed U-FEFP is compared with existing state-of-\nthe-art methods on the different datasets. The comparison re-\nsults on the NTU-60 are shown in Table 6. It can be seen that the\nproposed U-FEFP outperforms the existing unsupervised meth-\nods (Su et al., 2020; Zheng et al., 2018; Lin et al., 2020; Rao\net al., 2021; Li et al., 2021; Thoker et al., 2021; Zhang et al.,\n2022b,a; Gao et al., 2023; Zeng et al., 2023). The proposed\nU-FEFP only using joint is even better than method (Li et al.,\n2021) using joint, bone and motion data (3S) together. The\n10\n器\nFigure 6: t-SNE visualization of embedding for U-FEFP on the NTU-60 X-\nView task\nproposed U-FEFP even performs better than some supervised\nlearning based methods (Vemulapalli et al., 2014; Du et al.,\n2015; Yan et al., 2018; Shahroudy et al., 2016; Song et al., 2017;\nLiu et al., 2017c; Li et al., 2018).\nFurthermore, t-SNE (van der Maaten and Hinton, 2008) is\nused to visualize the embedding clustering produced by the pro-\nposed U-FEFP on the NTU-60 X-view task using all the data.\nThe t-SNE illustration is shown in Fig. 6. It can be seen that\nproposed U-FEFP can learn more discriminative latent space.\nAlthough some samples may deviate from their action class\ncenters, they are also away from other action classes, making\nthem easier to be discriminated. This forms the difference be-\ntween the features produced by the unsupervised learning and\nsupervised learning, and demonstrates the importance of rich\ndistributed features where different distributed features may be\nused to discriminate different samples in unsupervised learn-\ning. Moreover, the confusion matrix for the proposed U-FEFP\non the NTU-60 X-View is shown in Fig. 7. It can be seen that\nmost of the action classes are recognized with high accuracy,\nbut actions with similar small gesture motion can be confused\nsuch as “writing” and “reading”.\nThe result comparison of the proposed U-FEFP against the\nexisting methods on the NTU-120 dataset is shown in Table 7.\nThe proposed U-FEFP (3S) obtains 77.56% and 79.66% on X-\nSub and X-Set, respectively, and achieves the state-of-the-art\nperformance. U-FEFP using joint, bone and motion data is bet-\nter than ST-GCN using supervised way, which demonstrates the\neffectiveness of U-FEFP.\nThe result comparison on the PKU-MMD dataset is shown\nin Table 8. From the table, it can be seen that U-FEFP (3S)\nachieves 92.30% and 57.80% on PKU-MMD I and PKU-MMD\nTable 6: Experimental results (accuracy) on the NTU-60.\nMethod\nTrain\nmanner\nX-Sub\n(%)\nX-View\n(%)\nLie group (Vemulapalli et al., 2014) supervised\n50.10\n52.80\nH-RNN (Du et al., 2015)\nsupervised\n59.10\n64.00\nPA-LSTM (Shahroudy et al., 2016)\nsupervised\n62.90\n70.30\nST-LSTM+TS (Liu et al., 2017a)\nsupervised\n69.20\n77.70\nSTA-LSTM (Song et al., 2017)\nsupervised\n73.40\n81.20\nVisualize CNN (Liu et al., 2017c)\nsupervised\n76.00\n82.60\nC-CNN+MTLN (Ke et al., 2017b)\nsupervised\n79.60\n87.70\nVA-LSTM (Zhang et al., 2017)\nsupervised\n79.20\n88.30\nIndRNN (Li et al., 2018)\nsupervised\n81.80\n88.00\nST-GCN (Yan et al., 2018)\nsupervised\n81.50\n88.30\nLongT GAN (Zheng et al., 2018)\nunsupervised 39.10\n48.10\nPCRP (Xu et al., 2020)\nunsupervised 53.90\n63.50\nASCAL (Rao et al., 2021)\nunsupervised 58.50\n64.80\nMS2L (Lin et al., 2020)\nunsupervised 52.60\n-\nP&C (Su et al., 2020)\nunsupervised 50.70\n76.30\nCRRL (Wang et al., 2022)\nunsupervised 67.60\n73.80\nSKT (Zhang et al., 2022a)\nunsupervised 72.60\n77.10\nISC (Thoker et al., 2021)\nunsupervised 76.30\n85.20\nCrossSCLR (Li et al., 2021)\nunsupervised 72.90\n79.90\nCrossSCLR (3S) (Li et al., 2021)\nunsupervised 77.80\n83.40\nSRCL (Zhang et al., 2022b)\nunsupervised 77.30\n82.50\nSRCL (3S) (Zhang et al., 2022b)\nunsupervised 80.90\n85.60\nST-CL (Gao et al., 2023)\nunsupervised 68.10\n69.40\nCrossMoCo (Zeng et al., 2023)\nunsupervised 78.40\n84.90\nHaLP (Shah et al., 2023)\nunsupervised 79.70\n86.80\n3s-ActCLR (Lin et al., 2023)\nunsupervised 84.30\n88.80\nU-FEFP\nunsupervised 82.50\n87.52\nU-FEFP (3S)\nunsupervised 86.92\n91.44\nII, respectively. Compared with the previous best method (i.e.,\nSRCL (Zhang et al., 2022b)), the proposed method improves by\n4.10% and 4.60% on PKU-MMD I and PKU-MMD II, respec-\ntively. The performance is significantly improved compared\nwith the supervised ST-GCN (Yan et al., 2018), demonstrating\nthe effectiveness of the proposed U-FEFP.\n6. Conclusion\nIn this paper, we propose the U-FEFP learning framework\nfor unsupervised skeleton based action recognition.\nThe U-\nFEFP produces rich distributed features containing all infor-\nmation of the skeleton sequence, which is vital for the unsu-\npervised skeleton based action recognition. A relatively small\nspatial-temporal feature transformation subnetwork combining\nST-GCN and GConv-GRU is proposed to effectively capture\nthe skeleton sequence features. Based on this subnetwork, the\nunsupervised BYOL based feature enrichment learning and un-\nsupervised pretext task based fidelity preservation learning is\ncombined to formulate our U-FEFP, in order to produce the de-\nsired features. t-SNE is used to illustrate the features of the pro-\nposed U-FEFP, which demonstrates its advantage over the exist-\n11\nFigure 7: The confusion matrix for U-FEFP on the NTU-60 X-View.\nTable 7: Experimental results (accuracy) on the NTU-120.\nMethod\nTrain\nmanner\nX-Sub\n(%)\nX-Set\n(%)\nPA-LSTM (Shahroudy et al., 2016) supervised\n25.50\n26.30\nSkeMotion (Liu et al., 2017b)\nsupervised\n67.70\n66.90\nMulti CNN (Ke et al., 2018)\nsupervised\n62.20\n61.80\nTSRJI (Caetano et al., 2019)\nsupervised\n67.90\n62.80\nST-GCN (Yan et al., 2018)\nsupervised\n70.70\n73.20\nASCAL (Rao et al., 2021)\nunsupervised\n48.60\n48.60\nCRRL (Wang et al., 2022)\nunsupervised\n56.20\n57.00\nSKT (Zhang et al., 2022a)\nunsupervised\n62.60\n64.30\nISC (Thoker et al., 2021)\nunsupervised 67.90 9.66 67.10\nCrossSCLR (3S) (Li et al., 2021)\nunsupervised\n67.90\n66.70\nSRCL (Zhang et al., 2022b)\nunsupervised\n67.206\n67.90\nSRCL (3S) (Zhang et al., 2022b)\nunsupervised\n71.80\n72.90\nST-CL (Gao et al., 2023)\nunsupervised\n54.20\n55.60\nHaLP (Shah et al., 2023)\nunsupervised\n71.10\n72.20\n3s-ActCLR (Lin et al., 2023)\nunsupervised\n74.30\n75.70\nU-FEFP\nunsupervised\n73.85\n74.80\nU-FEFP (3S)\nunsupervised\n77.56\n79.66\ning methods. Extensive experiments are conducted on NTU-60,\nNTU-120 and PKU-MMD dataset, where the proposed U-FEFP\noutperforms the current state-of-the-art methods and achieves\nthe best performance. Ablation study on the proposed modules\nis also performed and validates their effectiveness.\nTable 8: Experimental results (accuracy) on the PKU-MMD.\nMethod\nTrain\nmanner\npart I\n(%)\npart II\n(%)\nST-GCN (Yan et al., 2018)\nsupervised\n84.10 48.20\nLongT GAN (Zheng et al., 2018) unsupervised 67.70 27.00\nMS2L (Lin et al., 2020)\nunsupervised 64.90 27.60\nISC (Thoker et al., 2021)\nunsupervised 80.90 36.00\nCRRL (Wang et al., 2022)\nunsupervised 82.10 41.80\nCrossSCLR (3S) (Li et al., 2021) unsupervised 84.90\n-\nSRCL (Zhang et al., 2022b)\nunsupervised 87.40 48.10\nSRCL (3S) (Zhang et al., 2022b) unsupervised 88.20 53.20\n3s-ActCLR (Lin et al., 2023)\nunsupervised 90.00 55.90\nHaLP (Shah et al., 2023)\nunsupervised\n-\n43.50\nU-FEFP\nunsupervised 90.72 53.12\nU-FEFP (3S)\nunsupervised 92.30 57.80\n7. Acknowledgements\nThis work was partly supported by the National Natural Sci-\nence Foundation of China (No. 62101512, 62001429,62271453\nand 62271290), Fundamental Research Program of Shanxi\nProvince (20210302124031) and Shanxi Scholarship Council\nof China (2023-131).\nReferences\nBanerjee, A., Singh, P.K., Sarkar, R., 2021. Fuzzy integral-based cnn classifier\nfusion for 3d skeleton action recognition. IEEE Transactions on Circuits and\nSystems for Video Technology 31, 2206–2216.\n12\nCaetano, C.A., Br´emond, F., Schwartz, W.R., 2019. Skeleton image represen-\ntation for 3d action recognition based on tree structure and reference joints,\nin: SIBGRAPI Conference on Graphics, Patterns and Images, pp. 16–23.\nCao, C., Lan, C., Zhang, Y., Zeng, W., Lu, H., Zhang, Y., 2019. Skeleton-\nbased action recognition with gated convolutional neural networks. IEEE\nTransactions on Circuits and Systems for Video Technology 29, 3247–3257.\nChen, Y., Zhang, Z., Yuan, C., Li, B., Deng, Y., Hu, W., 2021. Channel-wise\ntopology refinement graph convolution for skeleton-based action recogni-\ntion, in: IEEE International Conference on Computer Vision (ICCV), pp.\n13339–13348.\nDu, Y., Wang, W., Wang, L., 2015. Hierarchical recurrent neural network for\nskeleton based action recognition, in: IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), pp. 1110–1118.\nEvangelidis, G., Singh, G., Horaud, R., 2014. Skeletal quads: Human action\nrecognition using joint quadruples, in: International Conference on Pattern\nRecognition (ICPR), pp. 4513–4518.\nGao, J., He, T., Zhou, X., Ge, S., 2021. Skeleton-based action recognition with\nfocusing-diffusion graph convolutional networks. IEEE Signal Processing\nLetters 28, 2058–2062.\nGao, X., Yang, Y., Zhang, Y., Li, M., Yu, J.G., Du, S., 2023.\nEfficient\nspatio-temporal contrastive learning for skeleton-based 3-d action recogni-\ntion. IEEE Transactions on Multimedia 25, 405–417.\nGrill, J.B., Strub, F., Altch’e, F., Tallec, C., Richemond, P.H., Buchatskaya, E.,\nDoersch, C., Pires, B. ´A., Guo, Z.D., Azar, M.G., Piot, B., Kavukcuoglu, K.,\nMunos, R., Valko, M., 2020. Bootstrap your own latent: A new approach to\nself-supervised learning. ArXiv abs/2006.07733.\nHe, K., Fan, H., Wu, Y., Xie, S., Girshick, R.B., 2020. Momentum contrast\nfor unsupervised visual representation learning, in: IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), pp. 9726–9735.\nHou, Y., Li, Z., Wang, P., Li, W., 2018. Skeleton optical spectra-based ac-\ntion recognition using convolutional neural networks. IEEE Transactions on\nCircuits and Systems for Video Technology 28, 807–811.\nHu, G., Cui, B., Yu, S., 2020. Joint learning in the spatio-temporal and fre-\nquency domains for skeleton-based action recognition. IEEE Transactions\non Multimedia 22, 2207–2220.\nJiang, X., Xu, K., Sun, T., 2020. Action recognition scheme based on skeleton\nrepresentation with ds-lstm network. IEEE Transactions on Circuits and\nSystems for Video Technology 30, 2129–2140.\nKe, Q., An, S., Bennamoun, Sohel, F., Boussa¨ıd, F., 2017a. Skeletonnet: Min-\ning deep part features for 3-d action recognition. IEEE Signal Processing\nLetters 24, 731–735.\nKe, Q., Bennamoun, An, S., Sohel, F., Boussa¨ıd, F., 2017b. A new representa-\ntion of skeleton sequences for 3d action recognition, in: IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), pp. 4570–4579.\nKe, Q., Bennamoun, An, S., Sohel, F., Boussaid, F., 2018. Learning clip repre-\nsentations for skeleton-based 3d action recognition. IEEE Transactions on\nImage Processing 27, 2842–2855.\nKong, J., Bian, Y., Jiang, M., 2022. Mtt: Multi-scale temporal transformer\nfor skeleton-based action recognition. IEEE Signal Processing Letters 29,\n528–532.\nLi, C., Hou, Y., Wang, P., Li, W., 2017.\nJoint distance maps based action\nrecognition with convolutional neural networks. IEEE Signal Processing\nLetters 24, 624–628.\nLi, C., Hou, Y., Wang, P., Li, W., 2019a. Multiview-based 3-d action recogni-\ntion using deep networks. IEEE Transactions on Human-Machine Systems\n49, 95–104.\nLi, F., Zhu, A., Li, J., Xu, Y., Zhang, Y., Yin, H., Hua, G., 2022. Frequency-\ndriven channel attention-augmented full-scale temporal modeling network\nfor skeleton-based action recognition. Knowl. Based Syst. 256, 109854.\nURL: https://api.semanticscholar.org/CorpusID:252104191.\nLi, L., Wang, M., Ni, B., Wang, H., Yang, J., Zhang, W., 2021. 3d human action\nrepresentation learning via cross-view consistency pursuit, in: IEEE Confer-\nence on Computer Vision and Pattern Recognition (CVPR), pp. 4739–4748.\nLi, S., Li, W., Cook, C., Gao, Y., 2019b. Deep independently recurrent neural\nnetwork (indrnn). ArXiv abs/1910.06251.\nLi, S., Li, W., Cook, C., Zhu, C., Gao, Y., 2018. Independently recurrent neural\nnetwork (indrnn): Building a longer and deeper rnn, in: IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), pp. 5457–5466.\nLi, S., Xiang, X., Fang, J., Zhang, J., Cheng, S., Wang, K., 2023. Exploring in-\ncomplete decoupling modeling with window and cross-window mechanism\nfor skeleton-based action recognition. Knowl. Based Syst. 281, 111074.\nURL: https://api.semanticscholar.org/CorpusID:264134579.\nLi, W., Dasarathy, G., Berisha, V., 2020. Regularization via structural label\nsmoothing, in: International Conference on Artificial Intelligence and Statis-\ntics, p. 1453–1463.\nLim, S., Kim, I., Kim, T., Kim, C., Kim, S., 2019. Fast autoaugment, in: Neural\nInformation Processing Systems, p. 6665–6675.\nLin, L., Song, S., Yang, W., Liu, J., 2020. Ms2l: Multi-task self-supervised\nlearning for skeleton based action recognition, in: ACM International Con-\nference on Multimedia (ACM MM), pp. 2490–2498.\nLin, L., Zhang, J., Liu, J., 2023. Actionlet-dependent contrastive learning for\nunsupervised skeleton-based action recognition, in: IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), pp. 2363–2372.\nLiu, J., Shahroudy, A., Perez, M., Wang, G., yu Duan, L., Kot, A.C., 2020a. Ntu\nrgb+d 120: A large-scale benchmark for 3d human activity understanding.\nIEEE Transactions on Pattern Analysis and Machine Intelligence 42, 2684–\n2701.\nLiu, J., Shahroudy, A., Xu, D., Kot, A.C., Wang, G., 2017a. Skeleton-based\naction recognition using spatio-temporal lstm network with trust gates. IEEE\nTransactions on Pattern Analysis and Machine Intelligence 40, 3007–3021.\nLiu, J., Shahroudy, A., Xu, D., Kot, A.C., Wang, G., 2018. Skeleton-based\naction recognition using spatio-temporal lstm network with trust gates. IEEE\nTransactions on Pattern Analysis and Machine Intelligence 40, 3007–3021.\nLiu, J., Song, S., Liu, C., Li, Y., Hu, Y., 2020b.\nA benchmark dataset\nand comparison study for multi-modal human action analytics, in: ACM\nTransactions on Multimedia Computing, Communications, and Applica-\ntions (TOMM), pp. 1–24.\nLiu, J., Wang, G., Hu, P., yu Duan, L., Kot, A.C., 2017b. Global context-aware\nattention lstm networks for 3d action recognition, in: IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), pp. 3671–3680.\nLiu, K., Gao, L., Khan, N.M., Qi, L., Guan, L., 2021. A multi-stream graph\nconvolutional networks-hidden conditional random field model for skeleton-\nbased action recognition. IEEE Transactions on Multimedia 23, 64–76.\nLiu, K., Li, Y., Xu, Y., Liu, S., Liu, S., 2022. Spatial focus attention for fine-\ngrained skeleton-based action tasks.\nIEEE Signal Processing Letters 29,\n1883–1887.\nLiu, M., Liu, H., Chen, C., 2017c. Enhanced skeleton visualization for view\ninvariant human action recognition. Pattern Recognit. 68, 346–362.\nLiu, Z., Zhang, H., Chen, Z., Wang, Z., Ouyang, W., 2020c. Disentangling and\nunifying graph convolutions for skeleton-based action recognition, in: IEEE\nConference on Computer Vision and Pattern Recognition (CVPR), pp. 140–\n149.\nLoshchilov, I., Hutter, F., 2017. Decoupled weight decay regularization, in:\nInternational Conference on Learning Representations.\nLu, Z., Xu, C., Du, B., Ishida, T., Zhang, L., Sugiyama, M., 2021. Localdrop:\nA hybrid regularization for deep neural networks. IEEE Transactions on\nPattern Analysis and Machine Intelligence 44, 3590–3601.\nvan der Maaten, L., Hinton, G.E., 2008. Visualizing data using t-sne. Journal\nof Machine Learning Research 9, 2579–2605.\nNg, W., Zhang, M., Wang, T., 2022. Multi-localized sensitive autoencoder-\nattention-lstm for skeleton-based action recognition. IEEE Transactions on\nMultimedia 24, 1678–1690.\n¨Ozyer, T., Selin, A.D., Alhajj, R., 2021. Human action recognition approaches\nwith video datasets - a survey. Knowl. Based Syst. 222, 106995. URL:\nhttps://api.semanticscholar.org/CorpusID:233649424.\nPeng, W., Shi, J., Zhao, G., 2021. Spatial temporal graph deconvolutional net-\nwork for skeleton-based human action recognition. IEEE Signal Processing\nLetters 28, 244–248.\nRao, H., Xu, S., Hu, X., Cheng, J., Hu, B., 2021. Augmented skeleton based\ncontrastive action learning with momentum lstm for unsupervised action\nrecognition. Inf. Sci. 569, 90–109.\nShah, A., Roy, A., Shah, K., Mishra, S., Jacobs, D., Cherian, A., Chellappa,\nR., 2023.\nHalp: Hallucinating latent positives for skeleton-based self-\nsupervised learning of actions, in: IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), pp. 18846–18856.\nShahroudy, A., Liu, J., Ng, T.T., Wang, G., 2016. NTU RGB+ D: A large scale\ndataset for 3D human activity analysis, in: IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), pp. 1010–1019.\nShi, L., Zhang, Y., Cheng, J., Lu, H., 2019a. Skeleton-based action recognition\nwith directed graph neural networks, in: IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), pp. 7904–7913.\nShi, L., Zhang, Y., Cheng, J., Lu, H., 2019b. Two-stream adaptive graph con-\n13\nvolutional networks for skeleton based action recognition, in: IEEE Con-\nference on Computer Vision and Pattern Recognition (CVPR), pp. 12018–\n12027.\nShorten, C., Khoshgoftaar, T.M., 2019. A survey on image data augmentation\nfor deep learning. Journal of Big Data 6, 1–48.\nSong, S., Lan, C., Xing, J., Zeng, W., Liu, J., 2017. An end-to-end spatio-\ntemporal attention model for human action recognition from skeleton data,\nin: Association for the Advance of Artificial Intelligence (AAAI), pp. 4263–\n4270.\nSong, S., Liu, J., Lin, L., Guo, Z., 2021. Learning to recognize human ac-\ntions from noisy skeleton data via noise adaptation. IEEE Transactions on\nMultimedia 24, 1152–1163.\nSong, Y., Zhang, Z., Shan, C., Wang, L., 2020. Richly activated graph convo-\nlutional network for robust skeleton-based action recognition. IEEE Trans-\nactions on Circuits and Systems for Video Technology 31, 1915–1925.\nSu, K., Liu, X., Shlizerman, E., 2020. Predict & cluster: Unsupervised skeleton\nbased action recognition, in: IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), pp. 9628–9637.\nSun, Z., Ke, Q., Rahmani, H., Bennamoun, M., Wang, G., Liu, J., 2022. Hu-\nman action recognition from various data modalities: A review. IEEE Trans-\nactions on Pattern Analysis and Machine Intelligence , 1–20doi:10.1109/\nTPAMI.2022.3183112.\nThoker, F.M., Doughty, H., Snoek, C.G.M., 2021. Skeleton-contrastive 3d ac-\ntion representation learning, in: ACM International Conference on Multi-\nmedia (ACM MM), pp. 1655–1663.\nVemulapalli, R., Arrate, F., Chellappa, R., 2014. Human action recognition by\nrepresenting 3d skeletons as points in a lie group, in: IEEE Conference on\nComputer Vision and Pattern Recognition, pp. 588–595.\nWang, P., Li, W., Li, C., Hou, Y., 2018. Action recognition based on joint trajec-\ntory maps with convolutional neural networks. Knowledge-Based Systems\n158, 43–53.\nWang, P., Li, W., Ogunbona, P., Gao, Z., Zhang, H., 2014. Mining mid-level\nfeatures for action recognition based on effective skeleton representation,\nin: International Conference on Digital Image Computing: Techniques and\nApplications (DICTA), pp. 1–8.\nWang, P., Wen, J., Si, C., tao Qian, Y., Wang, L., 2022. Contrast-reconstruction\nrepresentation learning for self-supervised skeleton-based action recogni-\ntion. IEEE Transactions on Image Processing 31, 6224–6238.\nWeng, J., Weng, C., Yuan, J., 2017.\nSpatio-temporal naive-bayes nearest-\nneighbor (st-nbnn) for skeleton-based action recognition, in: IEEE Confer-\nence on Computer Vision and Pattern Recognition (CVPR), pp. 445–454.\nWu, C., Wu, X., Kittler, J., 2021.\nGraph2net: Perceptually-enriched graph\nlearning for skeleton-based action recognition. IEEE Transactions on Cir-\ncuits and Systems for Video Technology 32, 2120–2132.\nXia, L., Chen, C.C., Aggarwal, J.K., 2012. View invariant human action recog-\nnition using histograms of 3d joints, in: IEEE Computer Society Conference\non Computer Vision and Pattern Recognition Workshops (CVPRW), pp. 20–\n27.\nXia, R., Li, Y., Luo, W., 2022. Laga-net: Local-and-global attention network\nfor skeleton based action recognition. IEEE Transactions on Multimedia 24,\n2648–2661.\nXu, S., Rao, H., Hu, X., Cheng, J., Hu, B., 2020. Prototypical contrast and\nreverse prediction: Unsupervised skeleton based action recognition. IEEE\nTransactions on Multimedia 25, 624–634.\nXu, Y., Cheng, J., Wang, L., Xia, H., Liu, F., Tao, D., 2018. Ensemble one-\ndimensional convolution neural networks for skeleton-based action recogni-\ntion. IEEE Signal Processing Letters 25, 1044–1048.\nYan, S., Xiong, Y., Lin, D., 2018. Spatial temporal graph convolutional net-\nworks for skeleton-based action recognition, in: Association for the Ad-\nvance of Artificial Intelligence (AAAI), pp. 7444–7452.\nYang, J., Liu, W., Yuan, J., Mei, T., 2021. Hierarchical soft quantization for\nskeleton-based human action recognition. IEEE Transactions on Multimedia\n23, 883–898.\nYe, F., Pu, S., Zhong, Q., Li, C., Xie, D., Tang, H., 2020.\nDynamic gcn:\nContext-enriched topology learning for skeleton-based action recognition,\nin: ACM International Conference on Multimedia (ACM MM), pp. 55–63.\nYoo, J., Ahn, N., ah Sohn, K., 2020. Rethinking data augmentation for im-\nage super-resolution: A comprehensive analysis and a new strategy, in:\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp.\n8372–8381.\nZeng, Q., Liu, C., Liu, M., Chen, Q., 2023. Contrastive 3d human skeleton\naction representation learning via crossmoco with spatiotemporal occlusion\nmask data augmentation. IEEE Transactions on Multimedia 25, 1564–1574.\nZhang, H., Hou, Y., Zhang, W., 2022a. Skeletal twins: Unsupervised skeleton-\nbased action representation learning, in: IEEE International Conference on\nMultimedia and Expo (ICME), pp. 1–6.\nZhang, P., Lan, C., Xing, J., Zeng, W., Xue, J., Zheng, N., 2017. View adaptive\nrecurrent neural networks for high performance human action recognition\nfrom skeleton data, in: IEEE International Conference on Computer Vision\n(ICCV), pp. 2136–2145.\nZhang, P., Lan, C., Zeng, W., Xue, J., Zheng, N., 2020a. Semantics-guided\nneural networks for efficient skeleton-based human action recognition, in:\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp.\n1109–1118.\nZhang, S., Yang, Y., Xiao, J., Liu, X., Yang, Y., Xie, D., Zhuang, Y., 2018.\nFusing geometric features for skeleton-based action recognition using mul-\ntilayer lstm networks. IEEE Transactions on Multimedia 20, 2330–2343.\nZhang, T., Zheng, W., Cui, Z., Zong, Y., Li, C., Zhou, X., Yang, J., 2020b.\nDeep manifold-to-manifold transforming network for skeleton-based action\nrecognition. IEEE Transactions on Multimedia 22, 2926–2937.\nZhang, W., Hou, Y., Zhang, H., 2022b. Unsupervised skeleton-based action\nrepresentation learning via relation consistency pursuit. Neural Computing\nand Applications 34, 20327–20339.\nZheng, N., Wen, J., Liu, R., Long, L., Dai, J., Gong, Z., 2018.\nUnsuper-\nvised representation learning with long-term dynamics for skeleton based\naction recognition, in: Association for the Advance of Artificial Intelligence\n(AAAI), pp. 2644–2651.\nZhong, Z., Zheng, L., Kang, G., Li, S., Yang, Y., 2020.\nRandom erasing\ndata augmentation, in: Association for the Advance of Artificial Intelligence\n(AAAI).\nZhu, K., Wang, R., Zhao, Q., Cheng, J., Tao, D., 2020. A cuboid cnn model\nwith an attention mechanism for skeleton-based action recognition. IEEE\nTransactions on Multimedia 22, 2977–2989.\n14\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2024-01-25",
  "updated": "2024-01-25"
}