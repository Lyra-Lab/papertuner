{
  "id": "http://arxiv.org/abs/2401.14194v3",
  "title": "Parameter-Efficient Conversational Recommender System as a Language Processing Task",
  "authors": [
    "Mathieu Ravaut",
    "Hao Zhang",
    "Lu Xu",
    "Aixin Sun",
    "Yong Liu"
  ],
  "abstract": "Conversational recommender systems (CRS) aim to recommend relevant items to\nusers by eliciting user preference through natural language conversation. Prior\nwork often utilizes external knowledge graphs for items' semantic information,\na language model for dialogue generation, and a recommendation module for\nranking relevant items. This combination of multiple components suffers from a\ncumbersome training process, and leads to semantic misalignment issues between\ndialogue generation and item recommendation. In this paper, we represent items\nin natural language and formulate CRS as a natural language processing task.\nAccordingly, we leverage the power of pre-trained language models to encode\nitems, understand user intent via conversation, perform item recommendation\nthrough semantic matching, and generate dialogues. As a unified model, our\nPECRS (Parameter-Efficient CRS), can be optimized in a single stage, without\nrelying on non-textual metadata such as a knowledge graph. Experiments on two\nbenchmark CRS datasets, ReDial and INSPIRED, demonstrate the effectiveness of\nPECRS on recommendation and conversation. Our code is available at:\nhttps://github.com/Ravoxsg/efficient_unified_crs.",
  "text": "Parameter-Efficient Conversational Recommender System\nas a Language Processing Task\nMathieu Ravaut1, Hao Zhang1, Lu Xu2, Aixin Sun1, Yong Liu1\n1School of Computer Science and Engineering, Nanyang Technological University, Singapore\n2Singapore University of Technology and Design\nmathieuj001@e.ntu.edu.sg\nAbstract\nConversational recommender systems (CRS)\naim to recommend relevant items to users by\neliciting user preference through natural lan-\nguage conversation. Prior work often utilizes\nexternal knowledge graphs for items’ seman-\ntic information, a language model for dialogue\ngeneration, and a recommendation module for\nranking relevant items. This combination of\nmultiple components suffers from a cumber-\nsome training process, and leads to semantic\nmisalignment issues between dialogue genera-\ntion and item recommendation. In this paper,\nwe represent items in natural language and for-\nmulate CRS as a natural language processing\ntask. Accordingly, we leverage the power of\npre-trained language models to encode items,\nunderstand user intent via conversation, per-\nform item recommendation through semantic\nmatching, and generate dialogues. As a uni-\nfied model, our PECRS (Parameter-Efficient\nCRS), can be optimized in a single stage, with-\nout relying on non-textual metadata such as a\nknowledge graph. Experiments on two bench-\nmark CRS datasets, ReDial and INSPIRED,\ndemonstrate the effectiveness of PECRS on\nrecommendation and conversation. Our code\nis available at:\nhttps://github.com/\nRavoxsg/efficient_unified_crs.\n1\nIntroduction\nConversational recommender systems (CRS) have\nbecome an active research topic, which leverages\nboth natural language processing and recommen-\ndation techniques to provide high-quality recom-\nmendations through interactive conversations with\nusers (Jannach et al., 2021; Gao et al., 2021;\nPramod and Bafna, 2022).\nCRS consists of two sub-tasks: 1) generating nat-\nural language responses to interact with user (con-\nversation); and 2) recommending desirable items to\nuser based on dialogue context (recommendation).\nAn example of CRS data and model prediction is\nshown in Figure 1. In general, CRS represents a\nFigure 1: An example of dialogue from ReDial (Li et al.,\n2018), where blue color denotes the movie items.\nsignificant advancement in the field of recommen-\ndation, which could be applied to various possible\nuse cases, such as e-commerce, entertainment and\ncontent platforms.\nExisting CRS methods can be roughly catego-\nrized into attribute-based and generation-based\nmethods. The attribute-based methods (Lei et al.,\n2020; Ren et al., 2020; Zou et al., 2020) focus on\ncollecting user preferences on item attributes to\nnarrow down recommendation space to items with\ndesired properties. The generation-based meth-\nods (Zhou et al., 2020a, 2022; Wang et al., 2022c)\naim to acquire feedback from users, generate natu-\nral responses, and establish a comprehensive under-\nstanding of conversation to recommend the most\ndesirable items to user. In this work, we focus on\ngeneration-based CRS, which was greatly facili-\ntated with the rise of task-specific CRS datasets\nlike ReDial (Li et al., 2018), INSPIRED (Hayati\net al., 2020), TG-ReDial (Zhou et al., 2020b) and\nDuRecDial (Liu et al., 2020).\nThe key challenge of CRS methods consists in\nhow to jointly model language generation and item\nrecommendation, which are tasks of entirely differ-\nent natures. Early approaches (Chen et al., 2019;\nZhou et al., 2020a; Zhang et al., 2022; Zhou et al.,\n2022) mainly model conversation and recommen-\ndation tasks separately by incorporating external\narXiv:2401.14194v3  [cs.CL]  25 Feb 2024\nknowledge graphs (KG) for item semantics and\ndesigning auxiliary strategies to enhance the inter-\nactions between two tasks. They generally treat\nitems as nodes, which neglects the affluent textual\ninformation of items. They also sustain semantic\nmisalignment issue due to inconsistent item and\nword representations, because conversation and\nrecommendation modules are separately learned.\nRecent approaches (Wang et al., 2022a,b,c; Yang\net al., 2022) explore to seamlessly integrate con-\nversation and recommendation modules for bet-\nter knowledge sharing and semantic alignment via\nunified frameworks. However, due to the natural\ngap between recommendation and conversation,\nthey still require multiple training phases (Wang\net al., 2022c) and/or additional modules (Wang\net al., 2022a; Yang et al., 2022) to integrate the two\ntasks, failing to reach desired level of integration.\nWith the rapid development of language models\n(LMs), LMs for recommendation has gained sig-\nnificant attention. Based on LMs, recent work (Wu\net al., 2023; Lin et al., 2023) also shows a growing\ncorrelation between recommendation and language\ntasks. Thus, instead of applying structured KGs,\nwe stick to using item text descriptions together\nwith dialogue contexts for CRS, which formulates\nthe CRS directly as a natural language processing\ntask. Specifically, we devise a Parameter-Efficient\nConversational Recommender System (PECRS),\nwhich jointly solves recommendation and conver-\nsation by training a single model once, to bypass\nthe shortcomings of prior work in CRS. PECRS\nonly relies on a frozen pre-trained LM as backbone\nand employs a parameter-efficient plugin module\nto unify response generation and item recommen-\ndation in a simple yet flexible manner. Besides, we\ndesign a shared negative sampling strategy to sam-\nple negative items across subtasks and data points\nwithin the same mini-batch to boost both train-\ning efficiency and model performance. Moreover,\nthanks to the parameter-efficient plugin module,\nPECRS can easily scale up to larger LM backbones\nwithout significantly increasing training parame-\nters. In brief, our contributions are the following:\n• To the best of our knowledge, this is the first work\nsolving CRS by optimizing a single model in a\nsingle training phase and bypassing the need for\neither KGs or additional item encoders.\n• We demonstrate how to jointly generate response\nand learn item representations using a single\nand frozen language model. Through parameter-\nefficient fine-tuning techniques, our method is\nwith low computation cost, and can easily scale\nto larger backbones for higher performance.\n• Experiments on two benchmark datasets, ReDial\nand INSPIRED, demonstrate the effectiveness of\nour proposed PECRS method, which is competi-\ntive with SOTA.\n2\nRelated Work\nExisting conversational recommender systems\n(CRS) can be roughly categorized into attribute-\nbased and generation-based CRS methods. The\nattribute-based CRS methods utilize predefined\nactions to interact with users and target on ac-\ncomplishing the recommendation task with fewer\nturns (Christakopoulou et al., 2016; Sun and Zhang,\n2018; Lei et al., 2020; Ren et al., 2020; Zou et al.,\n2020; Hu et al., 2022a). Our work belongs to the\ngeneration-based CRS, which focuses on develop-\ning natural language based approaches to make\nhigh-quality recommendation and generate human-\nlike responses simultaneously (Li et al., 2018; Hay-\nati et al., 2020; Zhou et al., 2020b; Liu et al., 2020).\nGeneration-based CRS methods usually devise a\nrecommendation module and a conversation mod-\nule to implement item recommendation and re-\nsponse generation, respectively. Li et al. (2018) pro-\npose the first CRS dataset named ReDial, and solve\nit via encoder-decoder-based dialogue generator\nand autoencoder-based recommender. Subsequent\nwork commonly adopts external resources to incor-\nporate sufficient contextual information for better\nperformance. Numerous works (Chen et al., 2019;\nZhou et al., 2020a, 2021; Ma et al., 2020; Zhang\net al., 2022; Liang et al., 2021; Li et al., 2022;\nLiu et al., 2023; Zhang et al., 2023b) use knowl-\nedge graphs (KG) (Auer et al., 2007; Speer et al.,\n2017) coupled with graph networks (Schlichtkrull\net al., 2018) to enhance the items and user pref-\nerence understanding by designing sophisticated\nsemantic alignment strategies. RevCore (Lu et al.,\n2021) and C2-CRS (Zhou et al., 2022) further in-\ncorporate movie reviews to enrich the contextual\nknowledge via cross-attention (Lu et al., 2021) and\ncontrastive learning (Zhou et al., 2022). Despite\nconsecutive improvements, these works rely on\ndifferent architectures for conversation and recom-\nmendation, making them difficult to be effectively\nintegrated for end-to-end training and knowledge\nsharing. Consequently, they still suffer from a mis-\nmatch between conversation and recommendation\nmodules as well as inferior efficiency.\nTo remedy the aforementioned issues, recent\napproaches explore to jointly learn both conver-\nsation and recommendation tasks by pre-trained\nLMs. UniCRS (Wang et al., 2022c) adopts the\nDialoGPT (Zhang et al., 2020) for both con-\nversation and recommendation by tuning soft\nprompts (Lester et al., 2021) dedicated to each\ntask. Nevertheless, UniCRS requires three rounds\nof optimization, i.e., semantic fusion pre-training,\nconversation tuning, and recommendation tuning.\nUniMIND (Deng et al., 2023) follows the Uni-\nCRS paradigm with BART (Lewis et al., 2020)\nas the backbone, which unifies multi-goal CRS, i.e.,\nmulti-tasks, using prompting strategy with multiple\ntraining stages. RecInDial (Wang et al., 2022a) aug-\nments items into DialoGPT vocabulary and designs\na pointer mechanism for dynamic word and item\nprediction to achieve single multi-tasking process.\nSimilarly, BARCOR (Wang et al., 2022b) utilizes\nBART to recommend items with encoder and gener-\nate responses with decoder concurrently. Instead of\nusing KG, MESE (Yang et al., 2022) encodes item\nrepresentations using metadata and fuses them into\ndialogue for joint conversation and recommenda-\ntion learning using GPT-2 (Radford et al., 2019) as\nthe backbone. Although these methods attempt to\nintegrate conversation and recommendation tasks\nfor joint optimization, they rely on extra modules\n(e.g., R-GCN (Schlichtkrull et al., 2018) and Distil-\nBERT (Sanh et al., 2019)) for either item encoding\nor semantic fusion, and multi-round training stages.\nIn contrast, our goal is to design a framework to\nunify the CRS training under a single model opti-\nmized in a single training stage.\nOur work also employs parameter-efficient fine-\ntuning (PEFT) strategies. PEFT, including prompt\ntuning (Lester et al., 2021), Adapters (Houlsby\net al., 2019), and LoRA (Hu et al., 2022b), is a se-\nries of techniques to adapt (large) LMs with fewer\nparameters and low computation costs to achieve\nsame or even better performance comparing to the\nstandard fine-tuning on downstream tasks. PEFT\nhas shown great promise in various natural lan-\nguage (Zhang et al., 2023a; Dettmers et al., 2023),\ncomputer vision (He et al., 2022; Chen et al., 2023),\nand recommendation (Fu et al., 2023) tasks, but re-\nmains underexplored in CRS area. In this work,\nwe aim to train CRS via PEFT plugins without\ntouching the parameters of the backbone LM.\n3\nMethodology\nIn this section, we first describe the problem state-\nment of conversational recommendation systems\n(CRS). Then we present the proposed Parameter-\nEfficient Conversational Recommender System\n(PECRS) method in detail. The overall architec-\nture of PECRS is shown in Figure 2.\n3.1\nProblem Formulation\nLet I = {I1, I2, . . . , INitem} represent the item\ndatabase, which contains Nitem unique items, and\nD = {D1, D2, . . . , DNdial} denote a CRS dataset\nwith Ndial dialogues. Each dialogue D consists of\nnutt utterances denoted by D = {ut}nutt\nt=1, where ut\nrepresents the utterance at the t-th turn and each\nutterance ut = {wj}n\nj=1 contains a sequence of\nn words. The task of CRS is to generate the re-\nsponse and recommend desirable items based on\nthe given dialogue history and item database. To\nbe specific, given the dialogue history up to the\nt-th turn Dt = {ui}t−1\ni=1 and the item database I,\nthe CRS needs to recommend a set of candidate\nitems It from I, and generate the response ut\nwhich includes the items It. The recommended\ncandidate items set It could be empty when no\nrecommendation is needed, or contain one or more\nitems depending on the responses.\nIn this work, we apply our method to the movie\nrecommendation (i.e., I denotes a movie items\nset), but the process would be identical with other\ntypes of items. We follow prior work (Wang et al.,\n2022c; Yang et al., 2022) to adjust data samples\nand predict response with a single recommended\nmovie per utterance.\n3.2\nModel Input\nIn PECRS, items are represented by their textual de-\nscriptions, hence both input streams are modeled as\ntext. Nevertheless, we design a few special tokens\nto distinguish the various elements in PECRS.\nSpecial Tokens.\nOur PECRS is built upon a pre-\ntrained LM under the decoder-only style, param-\neterized by θ (e.g, GPT-2). However, LMs gen-\nerally do not have the capacity for recommenda-\ntion task. Thus, we define four special tokens, i.e.,\n“[ITEM]”, “[SEP]”, “[REC]” and “[REC_END]”,\nand add them into the LM’s vocabulary to guide\nthe model’s understanding of recommended items.\nItem Metadata.\nPrior work (Zhou et al., 2020a;\nZhang et al., 2022; Wang et al., 2022a; Zhou et al.,\n......\nSeeker: You really like the movies you suggest. Any\nother Will Ferrell movies you can suggest?\nRecommender: Yes, otherwise I would be wasting\nyour time.\nSeeker: True.\n------------------------------------------------------------------\nRecommender: I recommend you to check [MOVIE].\nItem DB\n...\nThe Other Guys [SEP] actors\n[SEP] genre [SEP] plot ...\nBatman Returns [SEP] actors\n[SEP] genre [SEP] plot ...\nBlack Panther [SEP] actors\n[SEP] genre [SEP] plot ...\nNegative\nSampling\nDial Context\nPooling\n&\nMLP\n...\nQuery\nStop Gradient \nMLP\n...\n...\nPre-trained Language Model\n...\nI recommend you ...\nNegative and Positive\nItem Representations\nGround-truth Item\nMLP\n[REC]\nFrozen weights\nLearnable weights\nVector representations\nPre-trained\nLanguage Model\n(Frozen)\nPEFT plugins\n(Trainable)\nFigure 2: The overall architecture of the proposed Parameter-efficient Conversation Recommendation System (PECRS), where\nthe PEFT denotes the parameter-efficient fine-tuning. Instead of fine-tuning backbone model, we inject PEFT plugins into\nbackbone model and fine-tune the PEFT weights (see the figure in the right).\n2022; Wang et al., 2022c) usually exploits external\nKG to encode item representations. They generally\nregard items as nodes and model relations among\nitems through R-GCN (Schlichtkrull et al., 2018),\nbut neglect the rich textual descriptions of the items.\nIn contrast, similar to Yang et al. (2022), we explore\nto use the static textual metadata of items. Item de-\nscriptions can be fed into a language model directly,\nhence bypassing the semantic misalignment issue.\nTo be specific, each item Ij is represented by afflu-\nent relevant information of the item rather than just\nits title. For movie recommendation, we use the\nfollowing format “Movie title [SEP] Actors [SEP]\nDirector(s) [SEP] Genre(s) [SEP] Plot” to describe\na movie item, where [SEP] is used to mark the sepa-\nration among different fields. Note this process can\nbe directly generalized to other domains by using\nthe meta information of items in the target domain.\nFormally, let Ij = {cj,k}l\nk=1 denotes the j-th item\ntextual data with l tokens, its output from LM is\nIj = [cj,1, . . . , cj,l]. We further adopt a MLP layer\nhitem with learnable pooling weight w to aggregate\nthe item representation as:\nvj = hitem(wT · Ij).\n(1)\nDialogue Context.\nThe dialogue context is made\nof all utterances up to the current t-th utterance:\nDt = {ui}t−1\ni=1. The word embeddings of the i-th ut-\nterance are denoted as ui = [ci,1, . . . , ci,n]. If any\nutterance ui contains an item, it will be replaced by\n“[ITEM]” token and its item representation is also\nconcatenated to the left side of the utterance’s word\nembeddings. Otherwise, it remains unchanged. Let\nvsep, vrec and vrec_end denote the token represen-\ntations of “[SEP]”, “[REC]” and “[REC_END]”,\nrespectively. Suppose the i-th utterance contains\nan item, if it is from seeker, its token embeddings\nare represented as ˜ui = [vsep, vj, vsep, ui]; if it\nis from recommender, its token embeddings are\n˜ui = [vrec, vj, vrec_end, ui]. Thus, the token em-\nbedding sequences of dialogue context are the con-\ncatenation of all utterances with vrec representation:\nDt = [¯u1, . . . , ¯ut−1, vrec],\n(2)\nwhere ¯ui = ˜ui if the utterance contains items,\notherwise ¯ui = ui.\n3.3\nRecommendation\nThe recommendation module contains two pro-\ncesses: retrieval and re-ranking. The retrieval pro-\ncess is to select candidate items relevant to dialogue\ncontext from item database. The re-ranking process\nfurther re-ranks the selected candidate items after\naggregating knowledge from the dialogue context.\nRetrieval.\nWe use the movie item in the response\nto be predicted as the ground-truth item, and sam-\nple M negative items from item database. Then,\nwe use their textual descriptions to encode item\nrepresentations via Equation (1) and derive ground-\ntruth item vp and negative items {v′\nj}M\nj=1. As the\ndialogue context is ended with “[REC]” token (ref.\nEquation (2)) and decoder-only LM can aggregate\nall contextual information via causal self-attention,\nwe utilize LM’s output of “[REC]” token, denoted\nas dt, to represent query representation of dialogue\ncontext. We adopt a noise-contrastive estimation\n(NCE) (Gutmann and Hyvärinen, 2012; Mnih and\nTeh, 2012; Mnih and Kavukcuoglu, 2013) objective\nto bring together the query dt with the positive key\nvp and push apart M negative (query, key) pairs\nformed by the set N = {(dt, v′\nj)}M\nj=1.\nThe NCE objective is written as:\nEDt =\nef(dt)⊤⊙vp\nef(dt)⊤⊙vp +\nP\n(dt,v′\nj)∼N\nef(dt)⊤⊙v′\nj ,\n(3)\nwhere f is a projection head with two-layer\nMLP and ReLU activation; ⊙denotes the angu-\nlar distance,\np\n2(1 −cos(a, b)), which measures\nthe similarity between two vectors, a and b. The\nrecall loss for retrieval process is defined as:\nLrecall = −1\n|D|\nX\nDt∈D\nlog(EDt).\n(4)\nNote we stop the gradients of LM and only op-\ntimize the pooling and MLP layers for item repre-\nsentations encoding during training (ref. Figure 2)\nto accelerate the learning process. The item repre-\nsentations will be reused in re-ranking process and\nthe LM will be optimized at this stage accordingly.\nRe-ranking.\nThe item representations derived\nfrom retrieval process are reused in the re-ranking\nprocess to aggregate the knowledge of dialogue\ncontext.\nTo be specific, given both positive\nand negative items, we concatenate them with\nthe token embeddings of dialogue context as\n[Dt, vp, v′\n1, . . . , v′\nM] and feed into LM then MLP\nf to compute the context-aware item representa-\ntions [qp, q1, . . . , qM]. Note that we adopt a spe-\ncial attention mask to enforce that each item vj\nonly attends to tokens from Dt, and positional\nembeddings are removed for item tokens to avoid\nany position leakage. Then another MLP layer\ng is applied to compute the final item scores as\nr = [rp, r1, . . . , rM].\nThe training objective of re-ranking process is:\nLrerank =\n1\n|D|\nX\nDt∈D\nfXE(r, Y ),\n(5)\nwhere Y = [1, 0, . . . , 0] and fXE denotes cross-\nentropy loss. Note we shuffle r and Y jointly to\navoid the positional bias of ground-truth labels. If\na data point has no recommended item in the re-\nsponse, we set Lrecall = Lrerank = 0.\n3.4\nResponse Generation\nThe response generation aims to predict the cur-\nrent utterance ut = {wj}n\nj=1 by giving the dia-\nlogue context. During training, if the ut contains\nan item to be recommended, the representations\nof the ground-truth item is appended to the corre-\nsponding dialogue context to guarantee that the LM\ngenerates the response relevant to the item. Then,\nthe input for response generation is:\n˜Dt = [¯u1, . . . , ¯ut−1, vrec, vp, vrec_end].\n(6)\nOtherwise, the input for response generation\nstays as ˜\nDt = [¯u1, . . . , ¯ut−1]. In general, the\nresponse generation is optimized by the standard\nnext-token prediction objective as:\nLgen = −1\n|D|\nP\nDt∈D\n1\nn\nnP\nj=1\nlog(pθ(wj|w1:(j−1), ˜Dt).\n(7)\n3.5\nParameter-Efficient Learning\nWe exploit parameter-efficient fine-tuning (PEFT)\ntechniques for training. PEFT can achieve compara-\nble performance to standard fine-tuning (Hu et al.,\n2023) with higher training efficiency and avoid\nthe catastrophic forgetting issue of LM. Specifi-\ncally, we leverage the LoRA (Hu et al., 2022b)\nmethod, which incorporates low-rank weight ma-\ntrices into transformer layers to adapt LM to down-\nstream tasks by fine-tuning the injected weights\nonly. In addition to LoRA layers, we also fine-tune\nthe task-specific MLP layers f, g and hitem and\nthe token embeddings of the four special tokens.\nPECRS only updates a small proportion (around\n5%) of the total number of parameters in the model.\n3.6\nTraining and Inference\nThe PECRS is trained in a singe-stage end-to-end\nmanner by minimizing the following loss:\nL = α × Lrecall + β × Lrerank + γ × Lgen,\n(8)\nwhere α, β and γ are hyperparameters to balance\nthe three losses. During training, we randomly sam-\nple Mtrain negative items and share them for com-\nputing the Lrecall and Lrerank losses. Besides, we\nshare the negative samples across batch elements\nand ensure that none of them is a positive for the\ndialogue contexts within a batch.\nDuring inference, we first use PLM to encode the\nrepresentations of all items in the database, which\nDataset\nUnique\nitems\nDialogues Utterances Recommender\nutterances\nRec. utt.\nw/o rec.\nRec. utt.\nw/ rec.\nReDial\n6,637\n11,348\n139,557\n73,999\n31,119\n42,880\nINSPIRED 1,546\n999\n21,124\n10,122\n7,243\n2,879\nTable 1: Statistics on ReDial and INSPIRED datasets, com-\nbined over train, dev and test sets.\nare reused for all dialogue contexts. Then the top-\nMinference items with highest similarities to the dia-\nlogue context query are retrieved via f(dt)⊤⊙vj\n(see Equation (3)). We further re-rank the Minference\nitems to obtain the top-1 item as the recommenda-\ntion output. In practice, we set Mtrain < Minference.\nWe show that M yields an important trade-off be-\ntween efficiency and recommendation performance\nboth during training and inference in Section 5.2.\nMoreover, the predicted item is appended at the\nend of the dialogue context rather than the ground\ntruth in Equation (6) in order to prompt the model\nfor response generation. To determine whether a\nmovie should be recommended at inference, we\ncheck whether the “[ITEM]” token is present in the\ngenerated response.\n4\nExperiments\n4.1\nExperimental Settings\nDatasets.\nWe conduct experiments on two com-\nmonly used datasets, i.e., ReDial (Li et al., 2018)\nand INSPIRED (Hayati et al., 2020).\nReDial1\ncontains 11, 348 conversations (10, 006 for train\nand 1, 342 for test) about movie recommendation\nbetween seeker and recommender, which is con-\nstructed through crowd-sourcing workers on Ama-\nzon Mechanical Turk. INSPIRED2 is also about\nmovie recommendation with smaller size of 999\n(801 for train, 99 for development and 99 for test)\nand more flexibility given to workers. The statistics\nof both datasets are summarized in Table 1.\nEvaluation Metrics.\nWe follow the common\npractices (Yang et al., 2022; Wang et al., 2022c)\nto evaluate PECRS on both recommendation per-\nformance and response generation quality. For rec-\nommendation subtask, we measure recall with Re-\ncall@K (R@K) metric, taking K ∈{1, 10, 50}. In\norder to assess the recommendation coverage, we\nalso report the number of different items predicted\nby the model over the test set, denoted as Unique.\nReDial and INSPIRED contain 6,637 and 1,546\n1https://redialdata.github.io/website/\n2https://github.com/sweetpeach/Inspired\nunique items in total (Table 1) and 1,872 and 264\nitems in the test set, respectively.\nWe use both reference-based and reference-free\nmetrics to evaluate response generation quality.\nFor reference-based metrics, we adopt ROUGE@K\n(RG-K) (Lin, 2004) with K ∈{1, 2}. To verify\nwhether the model could correctly predict a movie\nin response when required, we inspect the pres-\nence of the “[ITEM]” token in generated responses\nw.r.t. ground truth requirement of movie prediction\nvia F-1 score. For reference-free metrics, we use\nPerplexity (PPL) to assess the text fluency and Dis-\ntinct@K (Dist@K) with K ∈{2, 3, 4} to measure\nthe diversity of generated responses.\nImplementation.\nWe choose GPT-2 (Radford\net al., 2019) as the backbone LM, and experiment\nwith two different model sizes, i.e., GPT-2 small\nand GPT-2 medium, which enable us to compare\nagainst popular CRS approaches. Accordingly, we\nhave PECRS-small and PECRS-medium. We\nhighlight that PECRS is flexible and can support\nother choices of decoder-only LMs. We use the\npublic pre-trained checkpoints from HuggingFace\ntransformers library (Wolf et al., 2020). We set\nMtrain = 150 for training and Minfer = 700 for\ninference. For ReDial, we train for 10 epochs\nwith effective batch size 8; while for INSPIRED,\nwe train for 20 epochs with an effective batch\nsize of 2. Parameter optimization is performed\nby AdamW (Loshchilov and Hutter, 2019) with\nlinear learning rate warmup strategy. We set maxi-\nmum learning rate as 3e −5 for PECRS-small and\nPECRS-medium and warmup for 1 epoch. Dur-\ning training, we balance losses with α = 0.15,\nβ = 0.85, and γ = 1.0. We cap dialogue context\nlength at 256 tokens and response length at 64 to-\nkens. We use checkpoint with the highest mean\nof R@1, R@10 and R@50 for inference. PECRS\ngenerates the response with top-k sampling, using\nk = 50. The movie item metadata is obtained from\nThe Movie Database through tmdbv3api library3.\n4.2\nComparison with State-of-the-Art\nThe results on recommendation task are summa-\nrized in Table 2. Note that RevCore (Lu et al.,\n2021) and C2CRS (Zhou et al., 2022) are not di-\nrectly comparable to our method as they use addi-\ntional movie review information. PECRS gener-\nally outperforms the baselines using KG and extra\n3https://github.com/AnthonyBloomer/tmdbv3api\nModel\nMetadata\nModel Properties\nReDial\nINSPIRED\nKG Reviews Description Extra Model PEFT Rounds R@1 R@10 R@50 Unique R@1 R@10 R@50 Unique\nReDial (Li et al., 2018)\n✗\n✗\n✗\n✓\n✗\n3\n2.4\n14.0\n32.0\n_\n_\n_\n_\n_\nKBRD (Chen et al., 2019)\n✓\n✗\n✗\n✓\n✗\n2\n3.0\n16.3\n33.8\n_\n_\n_\n_\n_\nKGSF (Zhou et al., 2020a)\n✓\n✗\n✗\n✓\n✗\n3\n3.9\n18.3\n37.8\n_\n_\n_\n_\n_\nKECRS (Zhang et al., 2022)\n✓\n✗\n✗\n✓\n✗\n2\n2.3\n15.7\n36.6\n_\n_\n_\n_\n_\nBARCOR (Wang et al., 2022b)\n✓\n✗\n✗\n✓\n✗\n1\n2.5\n16.2\n35.0\n_\n_\n_\n_\n_\nUniCRS (Wang et al., 2022c)\n✓\n✗\n✗\n✓\n✓\n3\n5.1\n22.4\n42.8\n_\n9.4\n25.0\n41.0\n_\nRecInDial (Wang et al., 2022a)\n✓\n✗\n✗\n✓\n✗\n1\n3.1\n14.0\n27.0\n_\n_\n_\n_\n_\nVRICR (Zhang et al., 2023b)\n✓\n✗\n✗\n✓\n✗\n3\n5.7\n25.1\n41.6\n_\n_\n_\n_\n_\nRevCore (Lu et al., 2021)\n✓\n✓\n✗\n✓\n✗\n2\n6.1\n23.6\n45.4\n_\n_\n_\n_\n_\nC2-CRS (Zhou et al., 2022)\n✓\n✓\n✗\n✓\n✗\n2\n5.3\n23.3\n40.7\n_\n_\n_\n_\n_\nMESE (Yang et al., 2022)\n✗\n✗\n✓\n✓\n✗\n1\n5.6\n25.6\n45.5\n_\n4.8\n13.5\n30.1\n_\nPECRS-small\n✗\n✗\n✓\n✗\n✓\n1\n4.7\n20.8\n40.5\n463\n5.4\n16.1\n33.3\n34\nPECRS-medium\n✗\n✗\n✓\n✗\n✓\n1\n5.8\n22.5\n41.6\n634\n5.7\n17.9\n33.7\n72\nTable 2: Results of the recommendation task compared with the state-of-the-art on ReDial and INSPIRED. Results are taken\nfrom respective papers. Best numbers are in bold, second best underlined.\nModel\nReference-based\nReference-free\nRG-1 RG-2\nF-1\nPPL Dist@2 Dist@3 Dist@4\nC2-CRS\n_\n_\n_\n_\n0.163\n0.291\n0.417\nUniCRS\n_\n_\n_\n_\n0.492\n0.648\n0.832\nRecInDial\n_\n_\n_\n_\n0.518\n0.624\n0.598\nMESE\n_\n_\n_\n12.9\n0.822\n1.152\n1.313\nPECRS-small\n36.28 14.77 86.04 9.89\n0.745\n1.462\n2.132\nPECRS-medium 36.86 15.27 86.36 8.98\n0.820\n1.552\n2.154\nTable 3: Results of conversation task compared with the\nstate-of-the-art on ReDial.\nAspect\nMESE\nPECRS-small\nTie\nFluency\n28.00 (1.63)\n46.67 (5.91)\n25.33 (6.24)\nRelevancy\n26.33 (2.62)\n46.00 (0.82)\n27.67 (2.87)\nTable 4: Human evaluation on 100 random ReDial test data\npoints. We show the average scores for three human raters,\nwith standard deviation in parenthesis.\nmodel, such as KGSF (Zhou et al., 2020a) and Uni-\nCRS (Wang et al., 2022c), on both datasets. Com-\npared to the baselines with single training stage,\nPECRS surpasses BARCOR (Wang et al., 2022b)\nand RecInDial (Wang et al., 2022a). MESE (Yang\net al., 2022) also uses the item descriptions and em-\nploys two additional modules to encode items. In\ncontrast, our PECRS is simpler and more straight-\nforward, and it is the first approach without using\neither KG or supplementary module, but only rely-\ning on the pre-trained LM. PECRS-medium outper-\nforms MESE for Recall@1 on ReDial, achieving\nSOTA, and largely surpasses MESE for all met-\nrics on INSPIRED. Besides, PECRS-medium is\nsuperior to -small on all metrics, which demon-\nstrates that fine-tuning a larger LM brings more\ngains thanks to its stronger representation ability.\nTable 3 summarizes the results on conversation\ntask, where PECRS achieves promising perfor-\nmance on both types of metrics. Both PECRS-\nsmall and -medium surpass all baselines over\nModel\nTime/\nbatch (s)\nRec.\nConv.\nR@50 Unique RG-1 Dist@2\nPECRS-small\n6.1\n40.5\n463\n36.28\n0.745\nw/o Recall loss\n6.1\n19.3\n21\n37.67\n0.678\nw/o Rerank loss\n6.1\n12.2\n87\n36.50\n0.745\nw/o Generation loss\n6.1\n39.2\n451\n7.76\n11.907\nw/o Neg. sharing (batch)\n8.6\n39.8\n291\n36.40\n0.747\nw/o Neg. sharing (tasks)\n9.1\n40.8\n434\n35.98\n0.727\nw/o Item pooling\n6.1\n39.6\n530\n36.60\n0.748\nw/o Item head\n6.1\n37.9\n453\n36.33\n0.726\nw/o Metadata (just title)\n4.2\n35.8\n384\n36.38\n0.765\nTable 5: Models comparison with different modules and\noptimization strategies on ReDial with PECRS-small.\nRemoved None Title Actor(s) Director(s) Genre(s) Plot\nR@50\n33.3\n29.8\n26.9\n32.5\n30.5\n20.7\nTable 6: Effect of pruning fields of items metadata at infer-\nence on INSPIRED with PECRS-small.\nDist@3 and Dist@4. Comparing PECRS-small\nand -medium shows that Dist@K improvements\ncan be achieved by scaling up the backbone model.\nThus, we believe that larger LMs can bring better\nresults, and fine-tuning them with plugin style to\nacquire CRS capability is a promising research di-\nrection. A human evaluation (Table 4) for fluency\nand relevancy on ReDial test set with three vol-\nunteer graduate students with professional English\nproficiency confirms a preference for PECRS-small\ngenerated text over MESE outputs.\n4.3\nAblation Study\nWe also conduct ablative experiments to analyze\nthe architecture and optimization design of PECRS.\nReported in Table 5, all the components and train-\ning strategies contribute to the performance gains\non both recommendation and conversation tasks.\nIn particular, recommendation collapses without ei-\nther loss from its two-stage processes, i.e., retrieval\nand re-ranking ; and suffers without the genera-\nModel\nRec.\nConv.\nR@1 R@10 R@50 Unique RG-1 RG-2\nPECRS-small\n5.4\n16.1\n33.3\n34\n29.72\n8.26\nLlama-2-7B-chat\n9.3\n9.3\n9.3\n26\n19.88\n2.88\nVicuna-1.5-7B\n8.2\n8.2\n8.2\n23\n21.18\n3.50\nTable 7: Comparison between PECRS-small and two popular\nLLMs in zero-shot on INSPIRED test set.\nDecoding Strategy\nReference-based\nReference-free\nRG-1\nRG-2\nDist@2 Dist@3 Dist@4\nGreedy decoding\n38.54\n16.25\n0.208\n0.311\n0.390\nBeam search\n38.23\n16.83\n0.235\n0.353\n0.444\nDiverse beam search (diversity=0.5) 39.94\n17.30\n0.190\n0.287\n0.361\nDiverse beam search (diversity=1.0) 40.29\n17.40\n0.179\n0.264\n0.320\nDiverse beam search (diversity=1.5) 40.07\n17.23\n0.172\n0.246\n0.290\nTop-k sampling (k=25)\n33.54\n14.40\n0.593\n1.177\n1.806\nTop-k sampling (k=50)\n33.37\n14.17\n0.647\n1.300\n1.989\nTop-k sampling (k=75)\n33.48\n14.15\n0.644\n1.303\n1.992\nNucleus sampling (p=0.90)\n36.35\n16.04\n0.329\n0.555\n0.760\nNucleus sampling (p=0.95)\n36.44\n16.02\n0.351\n0.594\n0.804\nNucleus sampling (p=0.99)\n36.60\n16.07\n0.352\n0.593\n0.809\nTable 8: The conversation performance of PECRS-small\nwith different decoding strategies on ReDial. Except Greedy\ndecoding, all other techniques use a beam width of 4.\ntion loss. Sharing negative samples across batch\nelements and tasks leads to significant improve-\nments on training efficiency and marginal gains on\nrecommendation performance.\nIn Table 6, we conduct a further ablation on the\ntextual fields within items description. We observe\nthat every field contributes to the recommendation\nperformance, especially the plot. This suggests that\nricher metadata would yield even more recall gains.\n4.4\nComparison with Large Language Models\nLastly, we compare our fine-tuning approach with\nLarge Language Models (LLMs).\nInstruction-\ntuned LLMs have brought a seismic shift in NLP\nrecently, due to their ability to seamlessly conduct\nmany tasks in a zero-shot fashion through prompts,\nby-passing the need for task-specific supervised\nfine-tuning (Sanh et al., 2021; Wei et al., 2021;\nOuyang et al., 2022), including in recommender\nsystems (Hou et al., 2023).\nWe use two popular LLMs: Llama-2-7B-chat4\n(Touvron et al., 2023b), and Vicuna-1.5-7B5 (Chi-\nang et al., 2023). For each model, we condition on\nthe context, and prompt the LLM to predict the Rec-\nommender response, which should include a movie\nname. We infer in bfloat16, decode with greedy de-\ncoding, and check if the ground-truth movie name\nis included in the generated response. As seen in\nTable 7, the conversational recommendation capa-\nbility of LLMs in zero-shot is very promising, as\n4https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\n5https://huggingface.co/lmsys/vicuna-7b-v1.5\n150\n300\n400\n500\n600\n700\nMinference\n36\n37\n38\n39\n40\n41\nR@50\nMtrain = 50\nMtrain = 100\nMtrain = 150\nFigure 3: The R@50 results of PECRS-small using the dif-\nferent Mtrain and Minference pairs on ReDial dataset.\nthey outperform PECRS-small in Recall@1 on IN-\nSPIRED. However, due to the lack of a dedicated\nrecommendation module, LLMs used in this fash-\nion cannot suggest a full list of items, hence their\nrecall plateaus at the Recall@1 value. They also\ntend to recommend fewer different movies (lower\nUnique). Exploring the ranking of a larger list\nof recommended items with LLMs is a promising\nfuture research avenue.\n5\nAnalysis\nIn this section, we provide more detailed insights\nabout the behavior of PECRS.\n5.1\nConversation Evaluation\nWe first study the effects of different LM’s de-\ncoding strategies on conversational performance\nover Dist@K metric.\nSpecifically, we ana-\nlyze the greedy decoding, beam search, diverse\nbeam search (Vijayakumar et al., 2018), top-k\nsampling (Fan et al., 2018) and nucleus sam-\npling (Holtzman et al., 2020) strategies on PECRS-\nsmall. Reported in Table 8, reference-based metrics\n(RG-K) show much less variance on different de-\ncoding strategies compared to the reference-free\nmetrics (Dist@K). Meanwhile, the correlation be-\ntween reference-based and reference-free metrics\nis weak under different decoding strategies. More-\nover, PECRS without training for generation can\nachieve 11.907 on Dist@2 metric (see w/o Genera-\ntion loss in Table 5), but merely 7.76 on RG-1 met-\nric. This observation implies that Dist@K metrics\nare not reliable to evaluate the quality of response\ngeneration. Since Dist@K metrics have become\nthe most popular choice in evaluating conversation\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10 >10\nConversation Turns\n30\n35\n40\n45\n50\nR@50\nPECRS-small\nPECRS-medium\nFigure 4: R@50 of PECRS on ReDial per number of conver-\nsation turns prior to the CRS response.\nperformance of CRS (Zhou et al., 2022; Wang et al.,\n2022c; Yang et al., 2022), we advocate for applying\nother metrics, in particular reference-based metrics\nincluding n-gram overlap like ROUGE or semantic\nsimilarity like BERTScore (Zhang et al., 2019), to\nprovide more accurate evaluation on the response\ngeneration of CRS.\n5.2\nNegative Sampling\nNow we analyze how the hyper-parameters of neg-\native sampling, i.e., Mtrain and Minference, affect\nthe recommendation performance. Figure 3 illus-\ntrates the results of different choices of Mtrain and\nMinference pairs. In general, Mtrain and Minference\nhave significant impacts on the recommendation\nperformance, and larger Mtrain and Minference lead\nto better results. However, increasing M will re-\nduce the training and inference efficiency. Thus,\nthere is a trade-off between efficiency and recom-\nmendation performance for the selection of M.\n5.3\nConversation Turns\nLastly, we investigate how robust is PECRS with\nregards to the richness of dialogue context. In Fig-\nure 4, we group data points by number of utterances\nhappening before the CRS response. We observe\nthat PECRS performs well in recommendation for a\nwide range of context length, with only a moderate\ndrop when there is only one prior utterance.\n6\nConclusion\nIn this work, we formulate conversational recom-\nmendation as a language processing task and pro-\npose a unified parameter-efficient CRS (PECRS)\nframework to solve it in a single-stage end-to-end\nmanner. PECRS effectively addresses the infe-\nrior training efficiency via parameter-efficient fine-\ntuning techniques and semantic misalignment is-\nsues via joint conversation and recommendation\nmodeling. Through experiments, we show that\nPECRS achieves performance competitive with\nSOTA on both recommendation and response gen-\neration on benchmark datasets. Moreover, for re-\nsponse evaluation, we reveal the commonly used\nDist@K metrics are not reliable, and advocate for\nreference-based metrics (e.g ROUGE) for more ac-\ncurate evaluation. Generally, we show that it is\npromising to explore unified framework for CRS\nunder the natural language paradigm via language\nmodel and rich textual items data.\nLimitations\nOur work adheres to standard practices for dataset\nconstruction and model evaluation. However, we\nacknowledge three limitations: (1) Recommender\nutterances containing multiple items are separated\ninto individual data points, which is sub-optimal as\nthe model may only be accurate for the top-ranked\nitem in each data point. (2) If we train PECRS to\npredict multiple items within the same utterance,\nit is challenging to compare with current methods,\nas they do not make simultaneous predictions. (3)\nAll items mentioned by the recommender are con-\nsidered recommendations, although some may be\nreferences to previous discussions or express dis-\nlikes rather than actual recommendations.\nThe maximum context length for the backbone\nLM is another limitation. We have demonstrated\nthat increasing Minference yields better recommen-\ndation performance (ref. Section 5.2). However,\nwe are constrained by the maximum input length\nof 1024 for GPT-2, which limits the candidate set\nsize after concatenating with dialogue context. The\npotential extensions may involve performing infer-\nence with multiple forward passes to score batches\nof Minference items, or using a backbone that sup-\nports longer input lengths, albeit at a higher com-\nputational cost. We only experiment with relatively\nsmall backbone, i.e., GPT2-small and -medium,\ndue to resource limitation. However, PECRS is\nflexible and can be seamlessly applied to larger\nbackbones like LLaMA (Touvron et al., 2023a).\nReferences\nSören Auer, Christian Bizer, Georgi Kobilarov, Jens\nLehmann, Richard Cyganiak, and Zachary Ives. 2007.\nDbpedia: A nucleus for a web of open data. page\n722–735. Springer-Verlag.\nQibin Chen, Junyang Lin, Yichang Zhang, Ming Ding,\nYukuo Cen, Hongxia Yang, and Jie Tang. 2019. To-\nwards knowledge-based recommender dialog system.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 1803–\n1813, Hong Kong, China. Association for Computa-\ntional Linguistics.\nZhe Chen, Yuchen Duan, Wenhai Wang, Junjun He,\nTong Lu, Jifeng Dai, and Yu Qiao. 2023. Vision\ntransformer adapter for dense predictions. In The\nEleventh International Conference on Learning Rep-\nresentations.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E Gonzalez, et al.\n2023. Vicuna: An open-source chatbot impressing\ngpt-4 with 90%* chatgpt quality. See https://vicuna.\nlmsys. org (accessed 14 April 2023).\nKonstantina Christakopoulou, Filip Radlinski, and Katja\nHofmann. 2016.\nTowards conversational recom-\nmender systems. In Proceedings of the 22nd ACM\nSIGKDD International Conference on Knowledge\nDiscovery and Data Mining, page 815–824. Associa-\ntion for Computing Machinery.\nYang Deng, Wenxuan Zhang, Weiwen Xu, Wenqiang\nLei, Tat-Seng Chua, and Wai Lam. 2023. A unified\nmulti-task learning framework for multi-goal con-\nversational recommender systems. ACM Trans. Inf.\nSyst., 41(3).\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and\nLuke Zettlemoyer. 2023. Qlora: Efficient finetuning\nof quantized llms. ArXiv, abs/2305.14314.\nAngela Fan, Mike Lewis, and Yann Dauphin. 2018. Hi-\nerarchical neural story generation. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 889–898. Association for Computational Lin-\nguistics.\nJunchen Fu, Fajie Yuan, Yu Song, Zheng Yuan,\nMingyue Cheng, Shenghui Cheng, Jiaqi Zhang, Jie\nWang, and Yunzhu Pan. 2023. Exploring adapter-\nbased transfer learning for recommender systems:\nEmpirical studies and practical insights.\nArXiv,\nabs/2305.15036.\nChongming Gao, Wenqiang Lei, Xiangnan He, M. de Ri-\njke, and Tat-Seng Chua. 2021. Advances and chal-\nlenges in conversational recommender systems: A\nsurvey. AI Open, 2:100–126.\nMichael U. Gutmann and Aapo Hyvärinen. 2012. Noise-\ncontrastive estimation of unnormalized statistical\nmodels, with applications to natural image statistics.\nJ. Mach. Learn. Res., 13:307–361.\nShirley Anugrah Hayati, Dongyeop Kang, Qingxi-\naoyang Zhu, Weiyan Shi, and Zhou Yu. 2020. IN-\nSPIRED: Toward sociable recommendation dialog\nsystems. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 8142–8152, Online. Association for\nComputational Linguistics.\nXuehai He, Chunyuan Li, Pengchuan Zhang, Jianwei\nYang, and Xin Eric Wang. 2022. Parameter-efficient\nmodel adaptation for vision transformers.\nArXiv,\nabs/2203.16329.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2020. The curious case of neural text de-\ngeneration. In International Conference on Learning\nRepresentations.\nYupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu,\nRuobing Xie, Julian McAuley, and Wayne Xin\nZhao. 2023. Large language models are zero-shot\nrankers for recommender systems. arXiv preprint\narXiv:2305.08845.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin De Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly. 2019.\nParameter-efficient transfer learning for NLP.\nIn\nProceedings of the 36th International Conference on\nMachine Learning, volume 97, pages 2790–2799.\nChenhao Hu, Shuhua Huang, Yansen Zhang, and Yubao\nLiu. 2022a. Learning to infer user implicit preference\nin conversational recommendation. In Proceedings\nof the 45th International ACM SIGIR Conference on\nResearch and Development in Information Retrieval,\npage 256–266. Association for Computing Machin-\nery.\nEdward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu\nChen. 2022b. LoRA: Low-rank adaptation of large\nlanguage models. In International Conference on\nLearning Representations.\nZhiqiang Hu, Yihuai Lan, Lei Wang, Wanyu Xu, Ee-\nPeng Lim, Roy Ka-Wei Lee, Lidong Bing, Xing Xu,\nand Soujanya Poria. 2023. Llm-adapters: An adapter\nfamily for parameter-efficient fine-tuning of large\nlanguage models. ArXiv, abs/2304.01933.\nDietmar Jannach, Ahtsham Manzoor, Wanling Cai, and\nLi Chen. 2021. A survey on conversational recom-\nmender systems. ACM Comput. Surv., 54(5).\nWenqiang Lei, Xiangnan He, Yisong Miao, Qingyun\nWu, Richang Hong, Min Yen Kan, and Tat Seng Chua.\n2020. Estimation–action–reflection: Towards deep\ninteraction between conversational and recommender\nsystems. In WSDM 2020 - Proceedings of the 13th\nInternational Conference on Web Search and Data\nMining, pages 304–312. Association for Computing\nMachinery, Inc.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3045–3059, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 7871–7880, Online. Association for Computa-\ntional Linguistics.\nRaymond Li, Samira Kahou, Hannes Schulz, Vincent\nMichalski, Laurent Charlin, and Chris Pal. 2018.\nTowards deep conversational recommendations. In\nProceedings of the 32nd International Conference\non Neural Information Processing Systems, page\n9748–9758. Curran Associates Inc.\nShuokai Li, Ruobing Xie, Yongchun Zhu, Xiang Ao,\nFuzhen Zhuang, and Qing He. 2022. User-centric\nconversational recommendation with multi-aspect\nuser modeling. In Proceedings of the 45th Inter-\nnational ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval, page 223–233.\nAssociation for Computing Machinery.\nZujie Liang, Huang Hu, Can Xu, Jian Miao, Yingy-\ning He, Yining Chen, Xiubo Geng, Fan Liang,\nand Daxin Jiang. 2021. Learning neural templates\nfor recommender dialogue system. arXiv preprint\narXiv:2109.12302.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out, pages 74–81, Barcelona, Spain.\nAssociation for Computational Linguistics.\nJianghao Lin, Xinyi Dai, Yunjia Xi, Weiwen Liu,\nBo Chen, Xiangyang Li, Chenxu Zhu, Huifeng Guo,\nYong Yu, Ruiming Tang, and Weinan Zhang. 2023.\nHow can recommender systems benefit from large\nlanguage models: A survey. ArXiv, abs/2306.05817.\nZeming Liu, Haifeng Wang, Zheng-Yu Niu, Hua Wu,\nWanxiang Che, and Ting Liu. 2020. Towards conver-\nsational recommendation over multi-type dialogs. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 1036–\n1049, Online. Association for Computational Linguis-\ntics.\nZeming Liu, Ding Zhou, Hao Liu, Haifeng Wang,\nZheng-Yu Niu, Hua Wu, Wanxiang Che, Ting Liu,\nand Hui Xiong. 2023. Graph-grounded goal plan-\nning for conversational recommendation.\nIEEE\nTransactions on Knowledge and Data Engineering,\n35(5):4923–4939.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization. ArXiv, abs/1711.05101.\nYu Lu, Junwei Bao, Yan Song, Zichen Ma, Shuguang\nCui, Youzheng Wu, and Xiaodong He. 2021.\nRevCore: Review-augmented conversational recom-\nmendation. In Findings of the Association for Com-\nputational Linguistics: ACL-IJCNLP 2021, pages\n1161–1173, Online. Association for Computational\nLinguistics.\nWenchang Ma, Ryuichi Takanobu, and Minlie Huang.\n2020. Cr-walker: Tree-structured graph reasoning\nand dialog acts for conversational recommendation.\narXiv preprint arXiv:2010.10333.\nAndriy Mnih and Koray Kavukcuoglu. 2013. Learning\nword embeddings efficiently with noise-contrastive\nestimation. In Advances in Neural Information Pro-\ncessing Systems, volume 26. Curran Associates, Inc.\nAndriy Mnih and Yee Whye Teh. 2012. A fast and\nsimple algorithm for training neural probabilistic lan-\nguage models. In Proceedings of the 29th Interna-\ntional Coference on International Conference on Ma-\nchine Learning, page 419–426.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback.\nAdvances in Neural\nInformation Processing Systems, 35:27730–27744.\nDhanya Pramod and Prafulla Bafna. 2022. Conversa-\ntional recommender systems techniques, tools, accep-\ntance, and adoption: A state of the art review. Expert\nSyst. Appl., 203(C).\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nXuhui Ren, Hongzhi Yin, Tong Chen, Hao Wang,\nNguyen Quoc Viet Hung, Zi Huang, and Xiangliang\nZhang. 2020. Crsal: Conversational recommender\nsystems with adversarial learning. ACM Trans. Inf.\nSyst., 38(4).\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. ArXiv,\nabs/1910.01108.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Teven Le Scao, Arun\nRaja, et al. 2021. Multitask prompted training en-\nables zero-shot task generalization. arXiv preprint\narXiv:2110.08207.\nMichael Schlichtkrull, Thomas N. Kipf, Peter Bloem,\nRianne van den Berg, Ivan Titov, and Max Welling.\n2018. Modeling relational data with graph convolu-\ntional networks. In The Semantic Web, pages 593–\n607. Springer International Publishing.\nRobyn Speer, Joshua Chin, and Catherine Havasi. 2017.\nConceptnet 5.5: An open multilingual graph of gen-\neral knowledge. In Proceedings of the AAAI Confer-\nence on Artificial Intelligence.\nYueming Sun and Yi Zhang. 2018. Conversational rec-\nommender system. In The 41st International ACM\nSIGIR Conference on Research & Development in\nInformation Retrieval, page 235–244. Association\nfor Computing Machinery.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aur’elien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023a. Llama: Open\nand efficient foundation language models. ArXiv,\nabs/2302.13971.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023b.\nLlama 2: Open founda-\ntion and fine-tuned chat models.\narXiv preprint\narXiv:2307.09288.\nAshwin K Vijayakumar, Michael Cogswell, Ram-\nprasath R. Selvaraju, Qing Sun, Stefan Lee, David\nCrandall, and Dhruv Batra. 2018.\nDiverse beam\nsearch: Decoding diverse solutions from neural se-\nquence models. In Proceedings of the AAAI Confer-\nence on Artificial Intelligence.\nLingzhi Wang, Huang Hu, Lei Sha, Can Xu, Daxin\nJiang, and Kam-Fai Wong. 2022a. RecInDial: A uni-\nfied framework for conversational recommendation\nwith pretrained language models. In Proceedings of\nthe 2nd Conference of the Asia-Pacific Chapter of the\nAssociation for Computational Linguistics and the\n12th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers), pages\n489–500, Online only. Association for Computational\nLinguistics.\nTing-Chun Wang, Shang-Yu Su, and Yun-Nung Chen.\n2022b. Barcor: Towards a unified framework for\nconversational recommendation systems.\nArXiv,\nabs/2203.14257.\nXiaolei Wang, Kun Zhou, Ji-Rong Wen, and Wayne Xin\nZhao. 2022c. Towards unified conversational rec-\nommender systems via knowledge-enhanced prompt\nlearning. In Proceedings of the 28th ACM SIGKDD\nConference on Knowledge Discovery and Data Min-\ning, page 1929–1937. Association for Computing\nMachinery.\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M Dai, and Quoc V Le. 2021. Finetuned lan-\nguage models are zero-shot learners. arXiv preprint\narXiv:2109.01652.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nLikang Wu, Zhilan Zheng, Zhaopeng Qiu, Hao Wang,\nHongchao Gu, Tingjia Shen, Chuan Qin, Chen Zhu,\nHengshu Zhu, Qi Liu, Hui Xiong, and Enhong Chen.\n2023. A survey on large language models for recom-\nmendation. ArXiv, abs/2305.19860.\nBowen Yang, Cong Han, Yu Li, Lei Zuo, and Zhou\nYu. 2022. Improving conversational recommenda-\ntion systems’ quality with context-aware item meta-\ninformation. In Findings of the Association for Com-\nputational Linguistics: NAACL 2022, pages 38–48,\nSeattle, United States. Association for Computational\nLinguistics.\nRenrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu,\nShilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and\nYu Qiao. 2023a. Llama-adapter: Efficient fine-tuning\nof language models with zero-init attention. ArXiv,\nabs/2303.16199.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q\nWeinberger, and Yoav Artzi. 2019. Bertscore: Eval-\nuating text generation with bert.\narXiv preprint\narXiv:1904.09675.\nTong Zhang, Yong Liu, Boyang Li, Peixiang Zhong,\nChen Zhang, Hao Wang, and Chunyan Miao. 2022.\nToward knowledge-enriched conversational recom-\nmendation systems. In Proceedings of the 4th Work-\nshop on NLP for Conversational AI, pages 212–217,\nDublin, Ireland. Association for Computational Lin-\nguistics.\nXiaoyu Zhang, Xin Xin, Dongdong Li, Wenxuan Liu,\nPengjie Ren, Zhumin Chen, Jun Ma, and Zhaochun\nRen. 2023b. Variational reasoning over incomplete\nknowledge graphs for conversational recommenda-\ntion. In Proceedings of the Sixteenth ACM Interna-\ntional Conference on Web Search and Data Mining.\nACM.\nYizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen,\nChris Brockett, Xiang Gao, Jianfeng Gao, Jingjing\nLiu, and Bill Dolan. 2020. DIALOGPT : Large-scale\ngenerative pre-training for conversational response\ngeneration. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics:\nSystem Demonstrations, pages 270–278, Online. As-\nsociation for Computational Linguistics.\nJinfeng Zhou, Bo Wang, Ruifang He, and Yuexian\nHou. 2021. CRFR: Improving conversational rec-\nommender systems via flexible fragments reason-\ning on knowledge graphs. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 4324–4334, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nKun Zhou, Wayne Xin Zhao, Shuqing Bian, Yuanhang\nZhou, Ji-Rong Wen, and Jingsong Yu. 2020a. Im-\nproving conversational recommender systems via\nknowledge graph based semantic fusion.\nIn Pro-\nceedings of the 26th ACM SIGKDD International\nConference on Knowledge Discovery and Data Min-\ning, page 1006–1014. Association for Computing\nMachinery.\nKun Zhou, Yuanhang Zhou, Wayne Xin Zhao, Xi-\naoke Wang, and Ji-Rong Wen. 2020b.\nTowards\ntopic-guided conversational recommender system. In\nProceedings of the 28th International Conference\non Computational Linguistics, pages 4128–4139,\nBarcelona, Spain (Online). International Committee\non Computational Linguistics.\nYuanhang Zhou, Kun Zhou, Wayne Xin Zhao, Cheng\nWang, Peng Jiang, and He Hu. 2022. C²-crs: Coarse-\nto-fine contrastive learning for conversational recom-\nmender system. In Proceedings of the Fifteenth ACM\nInternational Conference on Web Search and Data\nMining, page 1488–1496. Association for Computing\nMachinery.\nJie Zou, Yifan Chen, and Evangelos Kanoulas. 2020.\nTowards question-based recommender systems. In\nProceedings of the 43rd International ACM SIGIR\nConference on Research and Development in Infor-\nmation Retrieval, page 881–890. Association for\nComputing Machinery.\nA\nSystem Outputs\nWe show an example from PECRS-medium on the\nINSPIRED dataset, in the same format as Figure 1.\nFigure 5:\nAn example of dialogue from IN-\nSPIRED (Hayati et al., 2020), where blue color denotes\nthe movie items.\nGenre\nItems (%)\nTest set\nRecommendation (%)\nCorrectly\nPredicted (%)\nComedy\n24.48\n23.74\n46.37\nAction\n21.88\n30.67\n57.65\nDrama\n17.89\n13.74\n32.93\nAdventure\n6.18\n6.02\n29.72\nHorror\n5.82\n7.94\n46.95\nCrime\n5.66\n4.50\n20.56\nAnimation\n5.40\n6.71\n62.38\nBiography\n4.11\n2.50\n12.61\nDocumentary\n3.27\n0.76\n22.22\nFantasy\n1.00\n0.61\n6.90\nThriller\n0.67\n0.46\n31.82\nFamily\n0.62\n0.38\n0.00\nMystery\n0.47\n0.57\n7.41\nRomance\n0.46\n0.04\n0.00\nTV\n0.43\n0.08\n0.00\nMusic\n0.26\n0.20\n0.00\nWestern\n0.25\n0.04\n0.00\nScience\n0.23\n0.13\n0.00\nShort\n0.23\n0.11\n0.00\nWar\n0.21\n0.11\n0.00\nSci-fi\n0.20\n0.06\n0.00\nHistory\n0.11\n0.00\n_\nMusical\n0.10\n0.23\n9.09\nFilm-noir\n0.05\n0.08\n0.00\nAdult\n0.02\n0.02\n0.00\nTable 9: Accuracy w.r.t genre prediction on ReDial test\nset broken down by movie genre.\nB\nGenre Analysis\nIn this section, we conduct a fine-grained analysis\nof PECRS top-1 recommendation. We investigate\nhow the model performs on several types of items.\nTo categorize items, we use the first genre tag in\nthe Genre(s) field in the items metadata, yielding\na partition of the movies set into 25 unique genres\nfor ReDial, 22 genres for INSPIRED. We report\nthe fraction of data points where the model outputs\na top-1 movie of the correct genre per genre on\nReDial and INSPIRED in Table 9 and Table 10,\nrespectively.\nAs we can see, there is wide variance in gen-\nres accuracy. Among wrong movie predictions,\nPECRS-medium outputs the correct genre 41.20%\ntimes on ReDial and 30.04% on INSPIRED. Ran-\ndom performance would yield 16.26% and 19.39%\naccuracy, respectively. The performance is much\nhigher on highly represented genres such as Com-\nedy, Action, or Horror, where it can surpass a ratio\nof correctly predicted genre of 50%, but quickly\nfalls to 0 for rare genres such as Romance. Future\nwork may focus on better handling the long tail dis-\ntribution in items variety, for instance through data\naugmentation techniques crafted for rare genres\nmovies.\nC\nPackages\nOur framework was implemented in Python 3.8.0.\nWe used the following Python package versions to\nGenre\nItems (%)\nTest set\nRecommendation (%)\nCorrectly\nPredicted (%)\nAction\n24.01\n36.20\n50.50\nComedy\n22.66\n17.92\n52.00\nDrama\n17.67\n13.98\n10.26\nHorror\n7.45\n9.68\n14.81\nAdventure\n4.86\n2.15\n66.67\nAnimation\n4.86\n4.66\n7.69\nCrime\n4.86\n6.09\n23.53\nBiography\n4.50\n2.15\n0.00\nDocumentary\n3.20\n1.79\n0.00\nThriller\n0.92\n0.36\n0.00\nFantasy\n0.86\n0.36\n0.00\nRomance\n0.80\n0.36\n0.00\nMystery\n0.62\n0.00\n_\nTV\n0.37\n0.00\n_\nShort\n0.37\n0.00\n_\nScience\n0.31\n0.72\n0.00\nMusic\n0.25\n0.00\n_\nSci-fi\n0.25\n0.36\n0.00\nWar\n0.12\n0.00\n_\nWestern\n0.12\n0.00\n_\nMusical\n0.06\n0.00\n_\nReality-TV\n0.06\n0.00\n_\nTable 10: Accuracy w.r.t genre prediction on INSPIRED\ntest set broken down by movie genre.\nconduct all experiments:\n• numpy 1.24.3\n• torch 1.9.1\n• transformers 4.33.2\n• rouge-score 0.1.2\n• nltk 3.8.1\n• peft 0.1.0\n• spacy 3.6.0\nAll packages and datasets used are freely avail-\nable and open-source, and were used for research\npurpose only. We refer to the specific papers for\nmore details on the use of each dataset.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2024-01-25",
  "updated": "2024-02-25"
}