{
  "id": "http://arxiv.org/abs/2401.14242v1",
  "title": "Improving Natural Language Capability of Code Large Language Model",
  "authors": [
    "Wei Li",
    "Daoguang Zan",
    "Bei Guan",
    "Ailun Yu",
    "Xiaolin Chen",
    "Yongji Wang"
  ],
  "abstract": "Code large language models (Code LLMs) have demonstrated remarkable\nperformance in code generation. Nonetheless, most existing works focus on\nboosting code LLMs from the perspective of programming capabilities, while\ntheir natural language capabilities receive less attention. To fill this gap,\nwe thus propose a novel framework, comprising two modules: AttentionExtractor,\nwhich is responsible for extracting key phrases from the user's natural\nlanguage requirements, and AttentionCoder, which leverages these extracted\nphrases to generate target code to solve the requirement. This framework\npioneers an innovative idea by seamlessly integrating code LLMs with\ntraditional natural language processing tools. To validate the effectiveness of\nthe framework, we craft a new code generation benchmark, called MultiNL-H,\ncovering five natural languages. Extensive experimental results demonstrate the\neffectiveness of our proposed framework.",
  "text": "Improving Natural Language Capability of Code Large Language Model\nWei Li1,2∗, Daoguang Zan1,2∗†, Bei Guan2,4†, Ailun Yu3, Xiaolin Chen1,2, Yongji Wang2,4\n1Cooperative Innovation Center, Institute of Software, Chinese Academy of Sciences\n2University of Chinese Academy of Sciences; 3Peking University;\n4Integrative Innovation Center, Institute of Software, Chinese Academy of Sciences\n{liwei@,daoguang@,guanbei@,chenxiaolin2019@,ywang@itechs.}iscas.ac.cn\nAbstract\nCode large language models (Code LLMs)\nhave demonstrated remarkable performance\nin code generation. Nonetheless, most exist-\ning works focus on boosting code LLMs from\nthe perspective of programming capabilities,\nwhile their natural language capabilities re-\nceive less attention. To fill this gap, we thus\npropose a novel framework, comprising two\nmodules: AttentionExtractor, which is respon-\nsible for extracting key phrases from the user’s\nnatural language requirements, and Attention-\nCoder, which leverages these extracted phrases\nto generate target code to solve the requirement.\nThis framework pioneers an innovative idea by\nseamlessly integrating code LLMs with tradi-\ntional natural language processing tools. To\nvalidate the effectiveness of the framework, we\ncraft a new code generation benchmark, called\nMultiNL-H, covering five natural languages.\nExtensive experimental results demonstrate the\neffectiveness of our proposed framework.\n1\nIntroduction\nCode Large Language Models (code LLMs) have\ndemonstrated remarkable proficiency in code gen-\neration tasks (Zan et al., 2023; Zhang et al., 2023b;\nZheng et al., 2023b), which can transform users’\nnatural language requirements into target code us-\ning various programming languages. Numerous\nmethods have emerged to enhance the performance\nof code LLMs, achieving significant advancements\nby constructing high-quality training data (Luo\net al., 2023; Shen et al., 2023; Name, 2023) or well-\ndesigned prompt (Wei et al., 2023; Shinn et al.,\n2023). However, previous works predominantly\nput efforts into enhancing code LLMs from the per-\nspective of programming capabilities, rarely direct-\ning attention to the natural language understanding\nabilities. The precondition for generating correct\ncode is that the model can accurately comprehend\n∗Equal contribution\n†Corresponding authors\nEnglish Chinese Spanish French Japanese\n50\n55\n60\n65\n70\n75\nPass@1(%)\nFigure 1: Pass@1 of OpenAI’s GPT-3.5-turbo evaluated\non HumanEval across multiple natural languages.\nthe programming task, which commonly involves\nnatural language understanding. The practice of\ntraining code LLMs on a corpus comprising both\ncode and natural language training instances, which\nleads to a powerful model CodeLlama (Rozière\net al., 2023), implies the important role of natu-\nral language understanding abilities in code LLMs.\nTherefore, this paper will delve into how to help\ncode LLMs comprehend natural language context\nmore accurately, thereby boosting the code genera-\ntion capabilities of code LLMs.\nAfter conducting preliminary experiments on\nHumanEval (Chen et al., 2021) across multiple nat-\nural languages, we find that code LLMs exhibit\nnotable variations between different natural lan-\nguages in code generation performance. Figure 1\ndemonstrates a significant drop in code generation\nperformance for other languages compared to En-\nglish. This highlights the need for more exploration\nof code generation in various natural languages. To\nmeet this challenge, we propose a framework to\nguide code LLMs to focus on critical information\nembedded in the given natural language context,\nhelping code LLMs to comprehend the task well.\nFrom the perspective of cognitive science, the cog-\nnitive load theory (Sweller, 1988, 2011; Kalyuga,\n2011) shows that humans inherently prioritize key\ninformation during handling new scenarios. In-\nspired by this theory, our framework highlights\nimportant words, phrases, and sentences in the task\narXiv:2401.14242v1  [cs.CL]  25 Jan 2024\ndescription for code LLMs to facilitate the mod-\nels in more easily capturing the key information\nin given programming requirements. This frame-\nwork comprises two modules: AttentionExtrac-\ntor, which identifies pivotal segments of natural-\nlanguage task description as Attention (e.g., words,\nphrases, or sentences), and AttentionCoder, which\nutilizes the extracted Attention to generate the tar-\nget code by harnessing the capabilities of off-the-\nshelf pre-trained models.\nIn order to comprehensively evaluate our ap-\nproach, we craft a multi-lingual benchmark,\nnamely\nMultiNL-H,\nadapted\nfrom\nthe\nHu-\nmanEval (Chen et al., 2021) which describes code\ntasks only in English.\nMultiNL-H comprises\ncode generation tasks described with multiple natu-\nral languages including English, Chinese, French,\nJapanese, and Spanish. On MultiNL-H, we con-\nduct extensive experiments to validate whether our\nframework can boost code LLMs in task under-\nstanding across different natural languages. The\nresults show that our approach exhibits obvious\nadvantages over its baselines. For example, by\nequipping code LLMs with AttentionExtractor and\nAttentionCoder, we boost OpenAI’s GPT-3.5-turbo\nin code generation for Chinese tasks by more than\nan absolute improvement of 10% on pass@1.\nIn summary, the contributions of this work are\nthreefold:\n• We propose a simple yet effective framework,\ncomposed of the AttentionExtractor and At-\ntentionCoder modules, to boost the code gen-\neration capabilities of code LLMs across mul-\ntiple natural languages. This framework can\ninspire researchers to solve tasks by integrat-\ning code LLMs with traditional natural lan-\nguage processing (NLP) analysis tools.\n• We craft a new benchmark, namely MultiNL-\nH, which extends the HumanEval (English)\nbenchmark into four natural languages. It can\nevaluate the code generation capabilities of\ncode LLMs from the perspective of natural\nlanguages.\n• We perform extensive experiments to prove\nthe effectiveness and superiority of our ap-\nproach, as well as to derive some valuable in-\nsights. Our work including all data and code\ncan be seen on https://github.com/\nNL2Code/AttentionCoder.\nAttention A \nContext x\nTarget \nCode y\nAttentionCoder\nEnglish\nChinese\nSpanish\nAttentionExtractor\nMC\nME\n......\nFigure 2: The overview of our proposed framework.\n2\nFramework\nWe would define the code generation task. Given\na context that contains the programming problem,\nthis task aims to output the target code to solve\nit. This paper leverages code LLM (defined as M)\nto address this task. Formally, let x represent the\ncontext. Given x, the code generation task can be\nformulated as y = M(x), where y represents the\ntarget code.\nIn this paper, we propose a framework to solve\nthis task, and it includes two modules: Atten-\ntionExtractor (ME) and AttentionCoder (MC),\nas depicted in Figure 2. AttentionExtractor first\nidentifies and extracts key information A (e.g.,\nwords, phrases, or sentences) from the program-\nming problem in the given context x. Subsequently,\nAttentionCoder focuses on generating the target\ncode y by leveraging A and x. Therefore, the\ncode generation task can be also reformulated as\ny = M(x, A). Specifically, the entire process of\nour framework can be formalized as A = ME(x)\nand y = MC(x, A). Note that A is called “Atten-\ntion” in our experiments.\n3\nMethodology\nThis section will implement our proposed frame-\nwork to enhance the multiple natural language ca-\npabilities of code LLMs, including AttentionEx-\ntractor and AttentionCoder.\n3.1\nAttentionExtractor\nThe goal of this module is to extract key informa-\ntion (alias “Attention”) from natural language de-\nscription. In the code generation scenario, as shown\nin Figure 3, AttentionExtractor first extracts the\ncode comment in the given context x using regular\nexpression. Subsequently, AttentionExtractor tok-\nenizes the extracted code comment and performs\npart-of-speech(POS) tagging using spaCy model1.\n1https://spacy.io\ndef count_upper(s):\n    \"\"\"\n    给定字符串s，统计偶数下标中大写元音的个\n数。\n    \"\"\"\nContext x\nAttentionExtractor ME\nAttention A\nAttentionCoder MC\nNatural Language \nDescription\nPrompt\nTemplate\ndef count_upper(s):\n    count = 0\n    for i in range(len(s)):\n        if i % 2 == 0 and s[i] in ['A', 'E', 'I', 'O', 'U']:\n            count += 1\n    return count\nTarget Code y\nTraditional NLP \nAnalysis Tools\n大写元音\n字符串\n偶数下标\nCode\nLLMs\nBelow is an instruction that describes a task. Write \na response that appropriately completes the \nrequest.\n### Instruction:\n{Context x}\n### KeyWords:\nPay attention to these below KeyWords in \ninstruction:\n{Attention A}\n### Response:\nrelevance \nPrompt Template\nFigure 3: The implementation details of our framework: AttentionExtractor and AttentionCoder.\nBased on these outcomes, AttentionExtractor em-\nploys the key phrase extraction algorithm such as\nTextRank (Mihalcea and Tarau, 2004) implemented\nby PKE package2 to extract multiple phrases3 as\nAttention and ranks them according to their rele-\nvance in the text network. For instance, Attention\nextracted from the code comment are three Chi-\nnese phases in Figure 3. These extracted Attention\nwill aid AttentionCoder in generating more accu-\nrate code. Besides using the automatic extraction\nmethod above, we also implement AttentionExtrac-\ntor by inviting five participants to manually extract\nkey phrases for each problem.\n3.2\nAttentionCoder\nAttentionExtractor has successfully extracted At-\ntention for a programming problem, and then At-\ntentionCoder aims to leverage these Attention to\ngenerate target code. We make use of the most\nstraightforward way for AttentionCoder: consider-\ning both these extracted Attention A and the context\nx when prompting models. Formally, the Attention-\nCoder can be written as y = MC(T (x, A)), where\nT (, ) denotes the prompt template that can concate-\nnate x and A. The detailed templates can be found\nin Figure 3. In our experiments, the off-the-shelf\ncode generation models, such as OpenAI’s GPT-\n4 (OpenAI et al., 2023), GPT-3.5-turbo (Ouyang\net al., 2022), WizardCoder (Luo et al., 2023), and\n2https://github.com/boudinfl/pke.git\n3The POS patterns designed for PKE package is\n“NP: <NOUN|PROPN|ADJ>*<NOUN|PROPN>; VP:\n<VERB><NP>“ defined by using regular expression grammar\nand POS tag.\nCodeLlama (Rozière et al., 2023), can be applied\ndirectly to implement our AttentionCoder.\n4\nBenchmark Construction\nTo comprehensively evaluate the efficacy of our\nframework, we carefully adapt HumanEval (Chen\net al., 2021) from English to four other natu-\nral languages, including Chinese, French, Span-\nish, and Japanese, and craft a new benchmark\ncalled MultiNL-H (Multiple Natural Language\nHumanEval). Specifically, we invite four native\nspeakers to translate the code comment from En-\nglish into four other languages. After careful check-\ning, we ensure the coherence and fluency of the\ntranslated content, as well as its consistency with\nthe original content. Furthermore, we also invite\nfour Python developers who are native speakers\nof the corresponding natural language to solve the\nprogramming problems in translated HumanEval.\nUpon their feedback, we further polish and refine\nthis benchmark. Overall, our meticulous transla-\ntion process ensures a high-quality benchmark for\nmulti-natural-language-oriented code generation.\nWe hope that this benchmark will contribute to the\nrapid advancement of the field.\n5\nExperiments\nIn this section, we evaluate our framework by an-\nswering three research questions (RQs):\n• RQ1 (Effectiveness Validation): How is the\nperformance of our framework on multiple-\nnatural-languages-oriented code generation?\n• RQ2 (Ablation Study): What factors influ-\nence the effectiveness of our framework?\n• RQ3 (Transferability Evaluation): Whether\nour framework is transferable to other tasks in-\nvolving natural-language context, by reusing\nAttentionExtractor?\nWe answer RQ1 by applying our framework to\nvarious models and comparing their performance\nbefore and after the application of Attention. To\naddress RQ2, we conduct extensive ablation stud-\nies to examine the impact of various factors on\nour framework’s effectiveness. We answer RQ3\nby extending our framework to other downstream\ntasks, including multiple-programming-languages-\noriented code generation, code translation, and\nmathematical reasoning. Specifically, we reuse\nthe task-agnostic AttentionExtractor to adapt to\ndifferent downstream tasks.\n5.1\nExperimental Setup\nBenchmark\nWe evaluate our framework on our\nnewly proposed MultiNL-H as described in Sec-\ntion 4. For each evaluation instance, MultiNL-H\nprovides five different language versions of the\ncode prompt (English, Chinese, French, Spanish,\nand Japanese) as well as the corresponding canoni-\ncal solution and test cases.\nBaselines\nWe choose several existing code LLMs\nas our base models, including close-source (Ope-\nnAI’s GPT-4-0613 and GPT-3.5-turbo-0613) and\nopen-source ones (WizardCoder 1B, 3B, 7B, and\n15B; CodeLlama 7B and 13B). We directly prompt\nthese models with different natural languages pro-\nvided by MultiNL-H, obtaining their performance\nwithout Attention as baseline results. Additionally,\nwe compare other competitive prompt engineering-\nbased approaches:\n• Chain of Thought (CoT) (Wei et al., 2023;\nLi et al., 2023a): CoT improves the capability\nof code LLMs by prompting them to think\nstep by step.\n• Reflexion (Shinn et al., 2023): Reflexion aug-\nments code LLMs by fostering reflective anal-\nysis in prompts, thereby enhancing task com-\nprehension and code generation precision.\nEvaluation Metrics\nFollowing the approach out-\nlined by Codex (Chen et al., 2021), we employ\nthe pass@1 metric to assess the performance of all\nmodels. Each model is tasked with generating a\nsingle code solution to every programming prob-\nlem using greedy decoding. The generated solution\nis then tested against given test cases. A program-\nming problem is deemed to be successfully solved\nonly if the generated code solution passes all the\ntest cases. Under this, pass@1 is defined as |Ps|\n|P| ,\nwhere |P| signifies the total count of programming\nproblems, and |Ps| indicates the number of chal-\nlenges successfully resolved. Thus, the pass@1\nmetric also can be served as the accuracy in our\nevaluation.\nImplementation Details\nIn the implementa-\ntion of AttentionExtractor, we treat all extracted\nnoun and verb phrases as Attention.\nWe im-\nplement AttentionCoder using existing models,\nemploy the greedy decoding strategy, and set\nmax_new_tokens to 1024. Furthermore, post-\nprocessing is applied to extract relevant code from\nchat-style responses to ensure a fair evaluation.\nFor details on reproducing the CoT and Reflexion,\nplease see Appendix B.\n5.2\nEffectiveness Validation\nTo find out whether our framework can boost\ncode LLMs on code generation in multiple-natural-\nlanguage-oriented code generation scenario, we\ncompare the performance of the base models on\nMultiNL-H in the three following settings: (1)\nNo Attention: prompting models without the in-\ncorporation of extracted key phrases (Attention),\n(2) AttentionCoderauto: prompting models with\nkey phrases extracted automatically through tradi-\ntional NLP analysis tools, (3) AttentionCoderhuman:\nprompting models with key phrases extracted man-\nually by human experts.\nTable 1 shows the performance of baseline mod-\nels on MultiNL-H under the aforementioned three\nsettings. By comparing the results under No At-\ntention setting and the other two settings, it can\nbe deduced that prompting Attention on existing\ncode LLMs is capable of leading to better perfor-\nmance. Particularly, with Chinese code comment,\nthe AttentionCoderauto setting of GPT-3.5 achieves\na large absolute improvement of 7.93% on pass@1,\ncompared with the No Attention setting. This ob-\nservation suggests that highlighting the extracted\nAttention in prompt can unlock the potential of code\nLLMs to better understand programming problem.\nFurthermore, by comparing the results under the\nAttentionCoderauto and AttentionCoderhuman set-\nModel\nSettings\nEnglish\nChinese\nFrench\nSpanish\nJapanese\nWizardCoder 1B\nNo Attention\n23.78\n22.56\n17.68\n14.63\n17.07\nAttentionCodeauto\n23.78+0.00\n21.34-1.22\n17.68+0.00\n14.63+0.00\n16.46-0.61\nAttentionCoderhuman\n23.78+0.00\n22.56+0.00\n17.68+0.00\n15.24+0.61\n16.46-0.61\nWizardCoder 3B\nNo Attention\n32.32\n23.17\n23.78\n21.95\n25.61\nAttentionCodeauto\n34.76+2.44\n26.83+3.66\n25.00+1.22\n20.73-1.22\n25.00-0.61\nAttentionCoderhuman\n34.14+1.82\n26.22+3.05\n25.00+1.22\n20.73-1.22\n25.00-0.61\nWizardCoder 7B\nNo Attention\n51.22\n41.46\n39.02\n38.41\n35.37\nAttentionCodeauto\n52.44+1.22\n46.34+4.88\n40.85+1.83\n40.85+2.44\n41.46+6.09\nAttentionCoderhuman\n52.44+1.22\n43.90+2.44\n40.85+1.83\n40.24+1.83\n42.07+6.70\nWizardCoder 15B\nNo Attention\n56.10\n46.34\n43.29\n40.85\n42.07\nAttentionCodeauto\n59.15+3.05\n52.44+6.10\n45.73+2.44\n42.68+1.83\n43.29+1.22\nAttentionCoderhuman\n59.15+3.05\n51.83+5.49\n45.73+2.44\n43.29+2.44\n42.68+0.61\nCodeLlama 7B\nNo Attention\n32.93\n28.05\n25.00\n23.78\n21.95\nAttentionCodeauto\n37.20+4.27\n32.93+4.87\n32.93+7.93\n32.93+9.15\n28.66+6.71\nAttentionCoderhuman\n37.80+4.87\n33.54+5.49\n32.93+7.93\n33.54+9.76\n26.22+4.27\nCodeLlama 13B\nNo Attention\n40.85\n39.02\n35.37\n34.15\n32.32\nAttentionCodeauto\n44.51+3.66\n42.07+3.05\n37.80+2.43\n38.41+4.26\n34.15+1.83\nAttentionCoderhuman\n45.12+4.27\n42.07+3.05\n37.80+2.43\n37.80+3.66\n35.37+3.05\nGPT-3.5\nNo Attention\n71.34\n66.46\n62.20\n64.02\n62.20\nCoT Prompting\n68.29-3.05\n60.98-5.48\n56.71-5.49\n64.63+0.61\n55.49-6.71\nReflexion\n65.17-6.17\n60.37-6.09\n55.44-6.76\n56.10-7.92\n59.15-2.05\nAttentionCodeauto\n75.00+3.66\n74.39+7.93\n66.46+4.26\n67.68+3.66\n62.80+0.60\nAttentionCoderhuman\n79.27+7.93\n76.83+10.37\n70.12+7.92\n70.73+6.71\n69.21+7.01\nGPT-4\nNo Attention\n81.20\n76.82\n70.73\n70.19\n73.17\nCoT Prompting\n84.15+2.95\n81.10+4.28\n75.61+4.88\n75.00+4.81\n73.17+0.00\nReflexion\n79.53-1.67\n80.49+3.67\n75.00+4.27\n72.56+2.37\n73.78+0.61\nAttentionCodeauto\n82.32+1.12\n78.05+1.23\n76.83+6.10\n75.00+4.81\n76.83+3.66\nAttentionCoderhuman\n82.32+1.12\n78.05+1.23\n77.44+6.71\n75.00+4.81\n76.83+3.66\nTable 1: Pass@1(%) results on the MultiNL-H: blue background is No Attention setting (baseline).\nting, it can be observed that humans in the loop\ncan usually further improve the code generation\nperformance of code LLMs. For example, for GPT-\n3.5, the human setting can lead to an absolute im-\nprovement of 10.37% on pass@1 compared to its\nbaseline under the Chinese scenario. A plausible\nexplanation for this observation is that humans, un-\nlike traditional NLP tools, excel in identifying and\nextracting pivotal phrases from natural language,\nthereby enabling code LLMs to better grasp the\nessence of the given natural language input. We\nalso compare two prompt engineering approaches\nincluding CoT (Wei et al., 2023; Li et al., 2023a)\nand Reflexion (Shinn et al., 2023) with our frame-\nwork on GPT-3.5 and GPT-4. And we observe that\nour approach consistently surpasses them across\nall languages. We hypothesize that this is primar-\nily due to the erroneous cascading within CoT and\nReflexion, further leading to a worse performance\nthan our framework. Interestingly, we observe that\nour baselines, CoT and Reflexion, show a declin-\ning trend on GPT-3.5 while exhibiting an upward\ntrend on the more powerful GPT-4 model. This\nsuggests that they demand more stringent founda-\ntional capabilities from the models. Overall, all\nthe above results prove the effectiveness of our ap-\nproach to promote model understanding of code\nrequirements.\n5.3\nAblation Study\nTo delve into the effectiveness of our framework,\nwe are keen on what factors can influence its per-\nWord\nPhrase\nSentence\n60\n65\n70\n75\nPass@1(%)\n(a) Attention Granularity Levels\nText\nRank\nTopic\nRank\nMultiPar\ntiteRank\nSingle\nRank\nPosition\nRank\nLLMs\nExtraction\n60.0\n62.5\n65.0\n67.5\n70.0\n72.5\n75.0\nPass@1(%)\n(b) Ranking Algorithms\nAverage\nEnglish\nChinese\nFrench\nSpanish\nJapanese\nTop-0\nTop-1\nTop-3\nTop-5\nAll\n60.0\n62.5\n65.0\n67.5\n70.0\n72.5\n75.0\nPass@1(%)\nAverage\n(c) Attention Count\nFigure 4: The result of influence factors on GPT-3.5: Attention Granularity Levels, Ranking Algorithms, Attention\nCount; “Top-x” in (c) means providing LLMs with x key phrases extracted by AttentionExtractor, “Top-0” donates\nbaseline (No Attention), “All” means providing LLMs all extracted Attention.\nformance. We initially explore the impact of differ-\nent Attention granularity levels on performance,\nincluding words, phrases, and sentences.\nFol-\nlowing that, we investigate the impact of various\nranking algorithms on performance, encompass-\ning TextRank (Mihalcea and Tarau, 2004), Top-\nicRank (Bougouin et al., 2013), MultiPartiteR-\nank (Boudin, 2018), PositionRank (Florescu and\nCaragea, 2017), SingleRank (Wan and Xiao, 2008),\nand LLMs extraction. Subsequently, we analyze\nthe impact of extracted phrase count on perfor-\nmance. Finally, we assess the effectiveness of our\nframework on models with varying parameter sizes.\nOwing to space limitations, more detailed analysis\nare provided in Appendix C.\nAttention Granularity Levels\nTechnically, atten-\ntion can be applied at different levels of granularity,\nincluding words, phrases, or sentences. Therefore,\nwe compare these levels and the results are pre-\nsented in Figure 4 (a). It is observed that the per-\nformance of phrase-based attention surpasses that\nof word-based and sentence-based attention. A rea-\nsonable conjecture is that phrases more effectively\nhighlight the key information, in contrast to words\nand sentences which may miss these focal points\ndue to being overly brief or excessively lengthy.\nConsequently, for our default experiment, we adopt\nphrase-based Attention.\nRanking Algorithms\nExtracting key phrases\nfrom natural language and ranking them in impor-\ntance is a very important step in our framework.\nTherefore, we explore the effect of different rank-\ning algorithms on MultiNL-H. Specifically, these\ninclude TextRank, TopicRank, MultiPartiteRank,\nSingleRank, and PositionRank. The results of these\nalgorithms are shown in Figure 4 (b). We find that\nthese rank algorithms perform almost the same.\nThis shows that our framework is insensitive to\nranking algorithms. In addition to using traditional\nNLP analysis tools, we also use OpenAI’s GPT-3.5\nto automatically extract and rank the attention. De-\ntails about the prompt can be seen in Appendix A.2.\nThe results show that LLMs extraction is less effec-\ntive, further demonstrating the necessity and good\nprospect of combining with traditional NLP tools\nfor LLMs.\nAttention Count\nWe would like to explore how\nmany phrases should be prompted to LLMs for ap-\npropriate Attention, and the results are displayed in\nFigure 4 (c). The results show that for almost all\nlanguages, performance initially declines and then\nincreases as the count of phrases increases, peaking\nwhen all phrases are prompted. This suggests that\nprompting with too few phrases might prevent the\nLLM from grasping the key information. Under\nthis context, we are intrigued to determine whether\nusing the entire programming problem as Attention,\nwithout extracting phrases, could improve perfor-\nmance. Results in Figure 4 (a) show that this setting\n(“Sentence”) leads to a decrease in performance,\nfurther underscoring the necessity of using phrases\nas Attention.\nModel Size\nWe display the performance of Wiz-\nardCoder and CodeLlama of various sizes in Fig-\nure 5. We observe that our framework has a thresh-\nold for model size. For instance, WizardCoder 1B\nachieves no improvement in pass@1 across all lan-\nguages, while WizardCoder 7B brought a 3.4% av-\nerage improvement. This suggests that by prompt-\ning Attention, the powerful models can obtain ob-\nvious improvement, while small models may not.\n1B\n3B\n7B\n13B\n15B\nModel Size (# parameters in billions)\n20\n25\n30\n35\n40\n45\n50\nPass@1 (%)\nNo Attention\n(WizardCoder)\nAttentionCoder\n(WizardCoder)\nNo Attention\n(CodeLlama)\nAttentionCoder\n(CodeLlama)\nFigure 5: The results of model size: ordinate value is\nthe average of five natural language results on Pass@1.\n5.4\nTransferability Evaluation\nIntuitively, as a means to assist the model in un-\nderstanding natural language tasks, our framework\nhas the potential to excel in various tasks involv-\ning natural-language context. Consequently, we\nare curious about whether Attention helps in dif-\nferent tasks across various domains. To declare\nour framework’s transferability, we extend it to\nthree other tasks: (1) code generation of multi-\nple programming languages, evaluated using the\nMultiPL-E (Cassano et al., 2022) benchmark. (2)\ncode translation, evaluated using the HumanEval-\nX (Zheng et al., 2023a) benchmark, from which\nwe specifically select two sub-tasks related to code\ntranslation: py2java and py2c++. (3) mathemati-\ncal reasoning, evaluated using the GSM8k (Cobbe\net al., 2021) benchmark. For these tasks, we imple-\nment our framework by reusing the AttentionEx-\ntractor and landing AttentionCoder with versatile\nOpenAI’s GPT-3.5. As for the three different down-\nstream tasks, we utilize different templates to im-\nport Attention and prompt the model. More details\nabout these templates can be seen in Appendix A.3.\nThe evaluation results on the three tasks are pre-\nsented in Table 2. For code generation of multi-\nple programming languages, our framework im-\nplemented based on GPT-3.5 obtains different de-\ngrees of improvement on different programming\nlanguages. For example, it leads to an absolute\nimprovement of 6.2% on pass@1 of the JavaScript\ncode generation tasks compared to the performance\nof GPT-3.5 without our framework. This indicates\nthe efficiency of our framework in code genera-\ntion of multiple programming languages. For code\ntranslation tasks, with our framework, GPT-3.5\nachieves a comprehensive enhancement on both\nof the tasks, which further shows the effectiveness\nof our framework. For instance, our framework pro-\nduces an absolute pass@1 improvement of 3.05%\nfor py2java and that of 11.58% for py2c++. It im-\nEvaluation Setting\nBaseline\nAttentionCoder\nMultiPL-E\ngo\n27.92\n31.17\njs\n50.93\n57.14\njava\n48.10\n49.37\nHumanEval-X\npy2java\n76.83\n79.88\npy2c++\n51.83\n63.41\nGSM8K\n-\n74.60\n74.90\nTable 2: Pass@1(%) results on GPT-3.5 of different\ntasks.\nplies that the improvement brought about by our\nframework in code translation depends greatly on\nthe involved programming languages. Actually,\nthis observation is also consistent with the results\non code generation of multiple programming lan-\nguages, where the improvement also varies accord-\ning to the programming language used to gener-\nate codes. For mathematical reasoning, our frame-\nwork only derives a tiny improvement on GSM8K.\nOne possible reason is that GSM8K’s task descrip-\ntions, characterized by structured and simple sen-\ntences, differ significantly in complexity from those\nin HumanEval, MultiNL-H, HumanEval-X, and\nMultiPL-E, which tend to be more intricate. Thus,\nour framework may excel in handling relatively\ncomplex natural language scenarios, with less pro-\nnounced advantages in simpler tasks. In summary,\nour framework consistently demonstrates advan-\ntages across various tasks, showing the potential of\ntransferability.\nOf note, the experiments to evaluate the transfer-\nability of our framework only focus on the English\nversion. However, we believe the improvement\nbrought about by our framework could be extended\nto other natural languages, especially considering\nthe conclusions already drawn from RQ1.\n5.5\nCase Study\nAfter extensive experiments, we can conclude that\nintroducing Attention can help LLMs to better un-\nderstand the programming context. One practical\ncase is displayed in Figure 6. For the baseline with-\nout Attention, the generated code cannot pass all\ntest cases. The primary reason is that it ignores the\n“an array of the words” specified in the\nrequirements. But when we add this phase into the\nprompt to highlight, code LLMs could pay more\nattention to the return type, thereby, generating the\ncorrect code.\ndef words_string(s):\n    \"\"\"\n     你会得到一串用逗号或空格分隔的单词。你的任务是\n将字符串拆分为单词并返回单词数组。\n(You will be given a string of words separated by commas \nor spaces. Your task is\\n    to split the string into words \nand return an array of the words)\n    \"\"\"\nBaseline (GPT-3.5)\nCode:\ndef words_string(s):\n    words = s.split(\",\") if \",\" in s else s.split(\" \")\n    words = [word.strip() for word in words]\n    return words\nTest Cases:\ndef check(candidate):\n    assert True, \"This prints if this assert fails 1 (good for \ndebugging!)\"\n    assert candidate(\"Hi, my name is John\") == [\"Hi\", \n\"my\", \"name\", \"is\", \"John\"]\n    assert candidate(\"One, two, three, four, five, six\") == \n[\"One\", \"two\", \"three\", \"four\", \"five\", \"six\"]\n    assert candidate(\"Hi, my name\") == [\"Hi\", \"my\", \n\"name\"]\ndef words_string(s):\n    \"\"\"\n    你会得到一串用逗号或空格分隔的单词。你的任务是\n将字符串拆分为单词并返回单词数组。\n(You will be given a string of words separated by commas \nor spaces. Your task is\\n    to split the string into words \nand return an array of the words)\n    \"\"\"\nAttentionCoder (GPT-3.5)\nCode:\ndef words_string(s):\n    words = s.split(\",\") \n    res = []\n    for word in words:\n        res.extend(word.split())  \n    return res\nTest Cases:\ndef check(candidate):\n    assert True, \"This prints if this assert fails 1 (good for \ndebugging!)\"\n    assert candidate(\"Hi, my name is John\") == [\"Hi\", \"my\", \n\"name\", \"is\", \"John\"]\n    assert candidate(\"One, two, three, four, five, six\") == \n[\"One\", \"two\", \"three\", \"four\", \"five\", \"six\"]\n    assert candidate(\"Hi, my name\") == [\"Hi\", \"my\", \n\"name\"]\nAttention: 单词数组( an array of the words)\nresult must be \nan array of the \nwords!\nFailed: result may \nnot be an array of \nthe words!\nFigure 6: One concrete case from the MultiNL-H benchmark.\n6\nRelated Work\n6.1\nCode Large Language Model\nCode large language models (Chen et al., 2023; Zan\net al., 2022b; Liu et al., 2023; Zan et al., 2022a;\nZhang et al., 2023a) have attracted widespread at-\ntention in both industry and academia. As a mile-\nstone, Codex (Chen et al., 2021) demonstrates great\ncode generation abilities. After that, a plenty of\ncode LLMs released, such as PolyCoder (Xu et al.,\n2022), CodeParrot (Huggingface, 2021), Alpha-\nCode (Li et al., 2022), PaLM-Coder (Christopoulou\net al., 2022), StarCoder (Li et al., 2023b), CodeL-\nlama (Rozière et al., 2023), WizardCoder (Luo\net al., 2023). Some efforts (Zheng et al., 2023a;\nCassano et al., 2022) have evaluated code LLMs\nfrom the perspective of multiple programming lan-\nguages. However, few works focus on multiple-\nnatural-language-oriented code generation. In this\npaper, we would like to explore the natural lan-\nguage capabilities of code LLMs by proposing a\nprompt engineering framework to unleash the po-\ntential of code LLMs.\n6.2\nPhrase Extraction\nPhrase extraction is a traditional NLP task, which\ninvolves extracting key phrases (Turney, 2000)\nfrom given natural language text. Many approaches\nhave been proposed, which can be divided into\ntwo types: supervised manners, such as deep\nkey phrase generation (Meng et al., 2017) and\nLTR (Burges et al., 2005), and unsupervised ones,\nsuch as TextRank (Mihalcea and Tarau, 2004),\nTopicRank (Bougouin et al., 2013), MultiPartit-\neRank (Boudin, 2018), SingleRank (Wan and Xiao,\n2008), and PositionRank (Florescu and Caragea,\n2017). In this paper, we regard these unsuper-\nvised methods are easy yet effective to extract key\nphrases from given natural language code descrip-\ntions. The extracted key phrases can be prompted\nto code LLMs to enhance natural language compre-\nhension.\n7\nConclusion\nThis paper proposes a simple yet effective frame-\nwork, including AttentionExtractor and Attention-\nCoder, for improving the natural language capabili-\nties of code LLMs. The essence of this framework\nlies in its seamless integration of LLMs with tra-\nditional NLP tools. To thoroughly evaluate our\nframework, we craft a multiple-natural-language-\noriented benchmark, namely MultiNL-H. Finally,\nextensive experiments demonstrate the effective-\nness and transferability of our framework. In future\nwork, we would like to investigate how to imple-\nment our framework during the pre-training and\nfine-tuning phase, not just at inference.\nLimitations\nOur framework also encompasses some limitations:\n(1) Technically, our framework can be applied to\nany task requiring natural language understand-\ning. However, validating our framework across all\ndownstream tasks is impractical. Therefore, in Sec-\ntion 5.4, we select three popular downstream tasks\nfor detailed analysis. Similarly, our framework\nis designed to process various natural languages.\nHowever, it is impractical to craft a dataset for ev-\nery natural language in the world, considering a\ntotal of upwards to 7, 0004. Hence, we choose five\nrepresentative languages (English, Chinese, French,\nSpanish, and Japanese) for which we create bench-\nmarks in code generation. (2) Our framework is\nbased on prompt engineering, and it costs a large\namount of computing resources. Therefore, we will\nopen source all our code and data to alleviate this\nissue.\nAcknowledgments\nThis research was supported by the National Key\nResearch and Development Program of China, un-\nder Grant No. 2022ZD0120201 - “Unified Repre-\nsentation and Knowledge Graph Construction for\nScience Popularization Resources”.\nReferences\nFlorian Boudin. 2018.\nUnsupervised keyphrase ex-\ntraction with multipartite graphs.\narXiv preprint\narXiv:1803.08721.\nAdrien Bougouin, Florian Boudin, and Béatrice Daille.\n2013.\nTopicrank: Graph-based topic ranking for\nkeyphrase extraction.\nIn International joint con-\nference on natural language processing (IJCNLP),\npages 543–551.\nChris Burges, Tal Shaked, Erin Renshaw, Ari Lazier,\nMatt Deeds, Nicole Hamilton, and Greg Hullender.\n2005. Learning to rank using gradient descent. In\nProceedings of the 22nd international conference on\nMachine learning, pages 89–96.\nFederico Cassano, John Gouwar, Daniel Nguyen,\nSy Duy Nguyen, Luna Phipps-Costin, Donald Pinck-\nney, Ming-Ho Yee, Yangtian Zi, Carolyn Jane An-\nderson, Molly Q. Feldman, Arjun Guha, Michael\nGreenberg, and Abhinav Jangda. 2022. A scalable\nand extensible approach to benchmarking nl2code for\n18 programming languages. ArXiv, abs/2208.08227.\n4https://en.wikipedia.org/wiki/Lists_\nof_languages\nBei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan,\nZeqi Lin, Jian-Guang Lou, and Weizhu Chen. 2023.\nCodeT: Code generation with generated tests. In\nThe Eleventh International Conference on Learning\nRepresentations.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde, Jared Kaplan, Harrison Ed-\nwards, Yura Burda, Nicholas Joseph, Greg Brockman,\nAlex Ray, Raul Puri, Gretchen Krueger, Michael\nPetrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin,\nBrooke Chan, Scott Gray, Nick Ryder, Mikhail\nPavlov, Alethea Power, Lukasz Kaiser, Moham-\nmad Bavarian, Clemens Winter, Philippe Tillet, Fe-\nlipe Petroski Such, David W. Cummings, Matthias\nPlappert, Fotios Chantzis, Elizabeth Barnes, Ariel\nHerbert-Voss, William H. Guss, Alex Nichol, Igor\nBabuschkin, S. Arun Balaji, Shantanu Jain, Andrew\nCarr, Jan Leike, Joshua Achiam, Vedant Misra, Evan\nMorikawa, Alec Radford, Matthew M. Knight, Miles\nBrundage, Mira Murati, Katie Mayer, Peter Welinder,\nBob McGrew, Dario Amodei, Sam McCandlish, Ilya\nSutskever, and Wojciech Zaremba. 2021. Evaluat-\ning large language models trained on code. ArXiv,\nabs/2107.03374.\nFenia Christopoulou, Gerasimos Lampouras, Milan\nGritta, Guchun Zhang, Yinpeng Guo, Zhong-Yi Li,\nQi Zhang, Meng Xiao, Bo Shen, Lin Li, Hao Yu,\nLi yu Yan, Pingyi Zhou, Xin Wang, Yu Ma, Igna-\ncio Iacobacci, Yasheng Wang, Guangtai Liang, Jia\nWei, Xin Jiang, Qianxiang Wang, and Qun Liu. 2022.\nPanGu-Coder: Program synthesis with function-level\nlanguage modeling. ArXiv, abs/2207.11280.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nNakano, et al. 2021. Training verifiers to solve math\nword problems. arXiv preprint arXiv:2110.14168.\nCorina Florescu and Cornelia Caragea. 2017.\nPosi-\ntionrank: An unsupervised approach to keyphrase\nextraction from scholarly documents. In Proceed-\nings of the 55th annual meeting of the association for\ncomputational linguistics (volume 1: long papers),\npages 1105–1115.\nHuggingface. 2021.\nTraining CodeParrot from\nScratch.\nhttps://huggingface.co/blog/\ncodeparrot.\nYongmin Li Jia Li, Ge Li and Zhi Jin. 2023. Structured\nchain-of-thought prompting for code generation.\nSlava Kalyuga. 2011. Cognitive load theory: How many\ntypes of load does it really need? Educational Psy-\nchology Review, 23:1–19.\nJia Li, Ge Li, Yongmin Li, and Zhi Jin. 2023a. Struc-\ntured chain-of-thought prompting for code genera-\ntion. arXiv preprint arXiv:2305.06599.\nRaymond Li, Loubna Ben Allal, Yangtian Zi, Niklas\nMuennighoff, Denis Kocetkov, Chenghao Mou, Marc\nMarone, Christopher Akiki, Jia Li, Jenny Chim,\nQian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo,\nThomas Wang, Olivier Dehaene, Mishig Davaadorj,\nJoel Lamy-Poirier, João Monteiro, Oleh Shliazhko,\nNicolas Gontier, Nicholas Meade, Armel Zebaze,\nMing-Ho Yee, Logesh Kumar Umapathi, Jian Zhu,\nBenjamin Lipkin, Muhtasham Oblokulov, Zhiruo\nWang, Rudra Murthy, Jason Stillerman, Siva Sankalp\nPatel, Dmitry Abulkhanov, Marco Zocca, Manan Dey,\nZhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya,\nWenhao Yu, Swayam Singh, Sasha Luccioni, Paulo\nVillegas, Maxim Kunakov, Fedor Zhdanov, Manuel\nRomero, Tony Lee, Nadav Timor, Jennifer Ding,\nClaire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri\nDao, Mayank Mishra, Alex Gu, Jennifer Robinson,\nCarolyn Jane Anderson, Brendan Dolan-Gavitt, Dan-\nish Contractor, Siva Reddy, Daniel Fried, Dzmitry\nBahdanau, Yacine Jernite, Carlos Muñoz Ferrandis,\nSean Hughes, Thomas Wolf, Arjun Guha, Leandro\nvon Werra, and Harm de Vries. 2023b. StarCoder:\nmay the source be with you!\nYujia Li, David H. Choi, Junyoung Chung, Nate Kush-\nman, Julian Schrittwieser, Rémi Leblond, Tom, Ec-\ncles, James Keeling, Felix Gimeno, Agustin Dal\nLago, Thomas Hubert, Peter Choy, Cyprien de,\nMasson d’Autume, Igor Babuschkin, Xinyun Chen,\nPo-Sen Huang, Johannes Welbl, Sven Gowal,\nAlexey, Cherepanov, James Molloy, Daniel Jaymin\nMankowitz, Esme Sutherland Robson, Pushmeet\nKohli, Nando de, Freitas, Koray Kavukcuoglu, and\nOriol Vinyals. 2022. Competition-level code genera-\ntion with alphacode. Science, 378:1092 – 1097.\nYan Liu, Xiaokang Chen, Yan Gao, Zhe Su, Fengji\nZhang, Daoguang Zan, Jian-Guang Lou, Pin-Yu\nChen, and Tsung-Yi Ho. 2023.\nUncovering and\nquantifying social biases in code generation. arXiv\npreprint arXiv:2305.15377.\nZiyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xi-\nubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma,\nQingwei Lin, and Daxin Jiang. 2023. WizardCoder:\nEmpowering code large language models with evol-\ninstruct. arXiv preprint arXiv:2306.08568.\nRui Meng, Sanqiang Zhao, Shuguang Han, Daqing He,\nPeter Brusilovsky, and Yu Chi. 2017. Deep keyphrase\ngeneration. arXiv preprint arXiv:1704.06879.\nRada Mihalcea and Paul Tarau. 2004. Textrank: Bring-\ning order into text. In Proceedings of the 2004 con-\nference on empirical methods in natural language\nprocessing, pages 404–411.\nAuthor’s Name. 2023. Code llama beats gpt-4: A deep\ndive. Accessed: 2023-10-31.\nOpenAI, :, Josh Achiam, Steven Adler, Sandhini Agar-\nwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Ale-\nman, Diogo Almeida, Janko Altenschmidt, Sam Alt-\nman, Shyamal Anadkat, Red Avila, Igor Babuschkin,\nSuchir Balaji, Valerie Balcom, Paul Baltescu, Haim-\ning Bao, Mo Bavarian, Jeff Belgum, Irwan Bello,\nJake Berdine, Gabriel Bernadett-Shapiro, Christo-\npher Berner, Lenny Bogdonoff, Oleg Boiko, Made-\nlaine Boyd, Anna-Luisa Brakman, Greg Brockman,\nTim Brooks, Miles Brundage, Kevin Button, Trevor\nCai, Rosie Campbell, Andrew Cann, Brittany Carey,\nChelsea Carlson, Rory Carmichael, Brooke Chan,\nChe Chang, Fotis Chantzis, Derek Chen, Sully Chen,\nRuby Chen, Jason Chen, Mark Chen, Ben Chess,\nChester Cho, Casey Chu, Hyung Won Chung, Dave\nCummings, Jeremiah Currier, Yunxing Dai, Cory\nDecareaux, Thomas Degry, Noah Deutsch, Damien\nDeville, Arka Dhar, David Dohan, Steve Dowl-\ning, Sheila Dunning, Adrien Ecoffet, Atty Eleti,\nTyna Eloundou, David Farhi, Liam Fedus, Niko\nFelix, Simón Posada Fishman, Juston Forte, Is-\nabella Fulford, Leo Gao, Elie Georges, Christian\nGibson, Vik Goel, Tarun Gogineni, Gabriel Goh,\nRapha Gontijo-Lopes, Jonathan Gordon, Morgan\nGrafstein, Scott Gray, Ryan Greene, Joshua Gross,\nShixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse\nHan, Jeff Harris, Yuchen He, Mike Heaton, Jo-\nhannes Heidecke, Chris Hesse, Alan Hickey, Wade\nHickey, Peter Hoeschele, Brandon Houghton, Kenny\nHsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu\nJain, Shawn Jain, Joanne Jang, Angela Jiang, Roger\nJiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie\nJonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser,\nAli Kamali, Ingmar Kanitscheider, Nitish Shirish\nKeskar, Tabarak Khan, Logan Kilpatrick, Jong Wook\nKim, Christina Kim, Yongjik Kim, Hendrik Kirch-\nner, Jamie Kiros, Matt Knight, Daniel Kokotajlo,\nŁukasz Kondraciuk, Andrew Kondrich, Aris Kon-\nstantinidis, Kyle Kosic, Gretchen Krueger, Vishal\nKuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan\nLeike, Jade Leung, Daniel Levy, Chak Ming Li,\nRachel Lim, Molly Lin, Stephanie Lin, Mateusz\nLitwin, Theresa Lopez, Ryan Lowe, Patricia Lue,\nAnna Makanju, Kim Malfacini, Sam Manning, Todor\nMarkov, Yaniv Markovski, Bianca Martin, Katie\nMayer, Andrew Mayne, Bob McGrew, Scott Mayer\nMcKinney, Christine McLeavey, Paul McMillan,\nJake McNeil, David Medina, Aalok Mehta, Jacob\nMenick, Luke Metz, Andrey Mishchenko, Pamela\nMishkin, Vinnie Monaco, Evan Morikawa, Daniel\nMossing, Tong Mu, Mira Murati, Oleg Murk, David\nMély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak,\nArvind Neelakantan, Richard Ngo, Hyeonwoo Noh,\nLong Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex\nPaino, Joe Palermo, Ashley Pantuliano, Giambat-\ntista Parascandolo, Joel Parish, Emy Parparita, Alex\nPassos, Mikhail Pavlov, Andrew Peng, Adam Perel-\nman, Filipe de Avila Belbute Peres, Michael Petrov,\nHenrique Ponde de Oliveira Pinto, Michael, Poko-\nrny, Michelle Pokrass, Vitchyr Pong, Tolly Pow-\nell, Alethea Power, Boris Power, Elizabeth Proehl,\nRaul Puri, Alec Radford, Jack Rae, Aditya Ramesh,\nCameron Raymond, Francis Real, Kendra Rimbach,\nCarl Ross, Bob Rotsted, Henri Roussez, Nick Ry-\nder, Mario Saltarelli, Ted Sanders, Shibani Santurkar,\nGirish Sastry, Heather Schmidt, David Schnurr, John\nSchulman, Daniel Selsam, Kyla Sheppard, Toki\nSherbakov, Jessica Shieh, Sarah Shoker, Pranav\nShyam, Szymon Sidor, Eric Sigler, Maddie Simens,\nJordan Sitkin, Katarina Slama, Ian Sohl, Benjamin\nSokolowsky, Yang Song, Natalie Staudacher, Fe-\nlipe Petroski Such, Natalie Summers, Ilya Sutskever,\nJie Tang, Nikolas Tezak, Madeleine Thompson, Phil\nTillet, Amin Tootoonchian, Elizabeth Tseng, Pre-\nston Tuggle, Nick Turley, Jerry Tworek, Juan Fe-\nlipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya,\nChelsea Voss, Carroll Wainwright, Justin Jay Wang,\nAlvin Wang, Ben Wang, Jonathan Ward, Jason Wei,\nCJ Weinmann, Akila Welihinda, Peter Welinder, Ji-\nayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner,\nClemens Winter, Samuel Wolrich, Hannah Wong,\nLauren Workman, Sherwin Wu, Jeff Wu, Michael\nWu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qim-\ning Yuan, Wojciech Zaremba, Rowan Zellers, Chong\nZhang, Marvin Zhang, Shengjia Zhao, Tianhao\nZheng, Juntang Zhuang, William Zhuk, and Barret\nZoph. 2023. Gpt-4 technical report.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida,\nCarroll L. Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex\nRay, John Schulman, Jacob Hilton, Fraser Kelton,\nLuke E. Miller, Maddie Simens, Amanda Askell, Pe-\nter Welinder, Paul Francis Christiano, Jan Leike, and\nRyan J. Lowe. 2022. Training language models to\nfollow instructions with human feedback.\nArXiv,\nabs/2203.02155.\nBaptiste Rozière, Jonas Gehring, Fabian Gloeckle,\nSten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi\nAdi, Jingyu Liu, Tal Remez, Jérémy Rapin, Artyom\nKozhevnikov, Ivan Evtimov, Joanna Bitton, Manish\nBhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wen-\nhan Xiong, Alexandre Défossez, Jade Copet, Faisal\nAzhar, Hugo Touvron, Louis Martin, Nicolas Usunier,\nThomas Scialom, and Gabriel Synnaeve. 2023. Code\nLlama: Open foundation models for code.\nBo Shen, Jiaxin Zhang, Taihong Chen, Daoguang Zan,\nBing Geng, An Fu, Muhan Zeng, Ailun Yu, Jichuan\nJi, Jingyang Zhao, Yuenan Guo, and Qianxiang Wang.\n2023. PanGu-Coder2: Boosting large language mod-\nels for code with ranking feedback.\nNoah Shinn, Federico Cassano, Edward Berman, Ash-\nwin Gopinath, Karthik Narasimhan, and Shunyu Yao.\n2023. Reflexion: Language agents with verbal rein-\nforcement learning.\nJohn Sweller. 1988. Cognitive load during problem\nsolving: Effects on learning.\nCognitive science,\n12(2):257–285.\nJohn Sweller. 2011. Cognitive load theory. In Psychol-\nogy of learning and motivation, volume 55, pages\n37–76. Elsevier.\nPeter D Turney. 2000.\nLearning algorithms for\nkeyphrase extraction. Information retrieval, 2:303–\n336.\nXiaojun Wan and Jianguo Xiao. 2008. Single document\nkeyphrase extraction using neighborhood knowledge.\nIn AAAI, volume 8, pages 855–860.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and\nDenny Zhou. 2023. Chain-of-thought prompting elic-\nits reasoning in large language models.\nFrank F. Xu, Uri Alon, Graham Neubig, and Vincent J.\nHellendoorn. 2022. A systematic evaluation of large\nlanguage models of code. Proceedings of the 6th\nACM SIGPLAN International Symposium on Ma-\nchine Programming.\nDaoguang Zan, Bei Chen, Zeqi Lin, Bei Guan, Yongji\nWang, and Jian-Guang Lou. 2022a. When language\nmodel meets private library. In Conference on Em-\npirical Methods in Natural Language Processing.\nDaoguang Zan, Bei Chen, Dejian Yang, Zeqi Lin,\nMinsu Kim, Bei Guan, Yongji Wang, Weizhu Chen,\nand Jian-Guang Lou. 2022b. CERT: Continual pre-\ntraining on sketches for library-oriented code genera-\ntion. In International Joint Conference on Artificial\nIntelligence.\nDaoguang Zan, Bei Chen, Fengji Zhang, Dianjie Lu,\nBingchao Wu, Bei Guan, Wang Yongji, and Jian-\nGuang Lou. 2023.\nLarge language models meet\nNL2Code: A survey. In Proceedings of the 61st An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 7443–\n7464, Toronto, Canada. Association for Computa-\ntional Linguistics.\nFengji Zhang, Bei Chen, Yue Zhang, Jin Liu, Daoguang\nZan, Yi Mao, Jian-Guang Lou, and Weizhu Chen.\n2023a. RepoCoder: Repository-level code comple-\ntion through iterative retrieval and generation.\nZiyin Zhang, Chaoyu Chen, Bingchang Liu, Cong Liao,\nZi Gong, Hang Yu, Jianguo Li, and Rui Wang. 2023b.\nUnifying the perspectives of nlp and software engi-\nneering: A survey on language models for code.\nQinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan-\nshan Wang, Yufei Xue, Zi-Yuan Wang, Lei Shen,\nAndi Wang, Yang Li, Teng Su, Zhilin Yang, and\nJie Tang. 2023a. CodeGeeX: A pre-trained model\nfor code generation with multilingual evaluations on\nhumaneval-x. ArXiv, abs/2303.17568.\nZibin Zheng, Kaiwen Ning, Yanlin Wang, Jingwen\nZhang, Dewu Zheng, Mingxi Ye, and Jiachi Chen.\n2023b. A survey of large language models for code:\nEvolution, benchmarking, and future trends.\nA\nPrompt Template\nA.1\nAttentionCoder\nThe prompt template of AttentionCoder can be di-\nvided into two types: one-chat and two-chat style.\nOne-chat style is aimed to prompt LLMs by di-\nrectly concatenating code context x and extracted\nAttention A in one chat, while two-chat style means\ninitially prompting the model to generate a solution\nfor x in the first chat round, and then prompting\nA in the subsequent chat round to refine the initial\nresponse. These two types of templates are listed\nin Table 3.\nA.2\nLLMs Extraction\nWe apply LLMs to extract key phrases of the code\ndescription to compare the extraction ability be-\ntween LLMs and traditional NLP analysis tools.\nWe conduct this experiment on GPT-3.5 with the\nprompt shown in Figure 7. We post-process the\ngenerated result and acquire the extracted Atten-\ntion.\nA.3\nThree other tasks of transferability\nevaluation\nWe apply our framework to other tasks includ-\ning multiple-programming-language-oriented code\ngeneration, code translation, and mathematical\nreasoning.\nThe prompt template for the task\nof multiple-programming-language-oriented code\ngeneration is the same as the two-chat style tem-\nplate of the multiple-natural-language-oriented\ncode generation task, as shown in Table 3. Prompt\nTemplates of code translation and mathematical rea-\nsoning are shown in Figure 9 and Figure 10, which\nare created by doing some task-related tweaks\nbased on two-chat style prompt templates of code\ngeneration. It is worth noting that we extracted the\nfunction call as the Attention for the origin code\nin the code translation task since it doesn’t have a\nnatural language description.\nB\nCoT and Reflexion Reproduction\nThe Chain of Thought (CoT) method was initially\nproposed for mathematical reasoning and has sub-\nsequently been extended to encompass various\nprompt engineering tasks.\nWe reproduce CoT\nmethod exactly based on the implementation pro-\ncess of Jia Li and Jin (2023). A concrete case of\ngenerating and leveraging solving process proce-\ndures is respectively shown in Figure 11 and 12.\nWe reproduce the Reflexion, using the open source\nproject5 of Shinn et al. (2023). We follow the\ninstructions of the project and do not change its\nsource code. We only set the benchmark path to\nMultiNL-H to get the result of the framework run-\nning on multiple natural languages.\n5https://github.com/noahshinn/\nreflexion\nC\nPart of Speech of Attention\nThe part of speech of Attention can be a noun or\na verb. We analyze the performance of these two\nparts of speech and the mixed setting (noun and\nverb) in Figure 8.\nThe results show that noun\nphrases are better than verb phrases. After care-\nful analysis, we find that the main reason is that\nnoun phrases encompass more details that models\nare prone to err on and need to pay attention to. For\nexample, given “Given a string s, count the number\nof uppercase vowels in even indices.”, Attention-\nCoder can extract verb phrases (“given a string s”,\n“count the number”) and noun phrases (“uppercase\nvowels”, “even indices”). Moreover, from the re-\nsults, we can find that the mixed setting is better\nthan verb or noun since either verb or noun is mean-\ningful for solving different problems. Therefore,\nby default, this paper regards the mixed setting as\nAttention to prompt code LLMs.\nType\nPrompt Template\nExample\nOne-chat\nBelow is an instruction that describes\na task. Write a response that appropri-\nately completes the request.\n### Instruction:\nCreate a Python script for this problem:\n{prompt with Attention putted into code\ncomment}\n### Response:\nBelow is an instruction that describes a task, Write a response that\nappropriately completes the request.\n### Instruction:\nCreate a Python script for this problem:\nfrom typing import List\ndef has_close_elements(numbers: List[float], threshold: float) -\nbool:\n\" Check if in given list of numbers, are any two numbers closer\nto each other than\ngiven threshold.\nKey Words: any two numbers, given list, given threshold\nhas_close_elements([1.0, 2.0, 3.0], 0.5) False\nhas_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3) True\n\"\n### Response:\nTwo-chat\nQuestion:\n{origin prompt}\nAnswer:\n{generated code}\nQuestion:\n1. Check out if the target code gener-\nated before is correct to match the Code\nDescription\n2. If not, pay attention to these Key\nWords in Code Description and rewrite\nthe code to be correct\n### Key Words\n{Attention}\n### Code Description\n{code comment}\nQuestion:\nfrom typing import List\ndef has_close_elements(numbers: List[float], threshold: float) -\nbool:\n\" Check if in given list of numbers, are any two numbers closer\nto each other than\ngiven threshold.\nKey Words: any two numbers, given list, given threshold\nhas_close_elements([1.0, 2.0, 3.0], 0.5) False\nhas_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3) True\n\"\nAnswer:\n{generated code}\nQuestion:\n1. Check out if the target code generated before is correct to match\nthe Code Description\n2. If not, pay attention to these Key Words in Code Description and\nrewrite the code to be correct\n### Key Words\nany two numbers, given list, given threshold\n### Code Description\nCheck if in given list of numbers, are any two numbers closer to\neach other than given threshold.\nTable 3: Prompt Templates of AttentionCoder.\nFor the following code problem description, extract the key phrases that need to be \nconsidered in focus to solve the problem and rank them by relevance of problem:\nCheck if in given list of numbers, are any two numbers closer to each other than given \nthreshold.    \nFigure 7: A specific prompt for extracting Attention using LLMs: code comment of context is in gray background\nVerb\nPhrase\nNoun\nPhrase\nBoth\nTypes\n57.5\n60.0\n62.5\n65.0\n67.5\n70.0\n72.5\n75.0\nPass@1(%)\nChinese\nEnglish\nFrench\nSpanish\nJapanese\nFigure 8: The result of Part of Speech of Attention: verb\nphrases, noun phrases, and both types.\n1. Check out if the target code generated before is correct when transform Origin Code \ninto another programming language of Target Function\n2. If not, pay attention to these Key Parts in Origin Code and complete the Target \nFunction of corresponding programming language correctly\n### Key Parts\nhas_close_elements, enumerate, abs\n### Origin Code\nfrom typing import List\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\n    for idx, elem in enumerate(numbers):\n        for idx2, elem2 in enumerate(numbers):\n            if idx != idx2:\n                distance = abs(elem - elem2)\n                if distance < threshold:\n                    return True\n    return False\n### Target Function\n#include<stdio.h>\n#include<vector>\n#include<math.h>\nusing namespace std;\n#include<algorithm>\n#include<stdlib.h>\nbool has_close_elements(vector<float> numbers, float threshold){\nFigure 9: One example of code translation on python2c++.\n1. Check out if the answer is right for the Math Problem before\n2. If not, pay attention to these KeyWords in Math Problem and rewrite the right answer\n### KeyWords\nany two numbers, given list, given threshold\n### Math Problem\nJanet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes \nmuffins for her friends every day with four. She sells the remainder at the farmers' \nmarket daily for $2 per fresh duck egg. How much in dollars does she make every day at \nthe farmers' market?\nFigure 10: One concrete prompt of mathematical reasoning.\nQuestion:\nfrom typing import List\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\n    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\n    given threshold.\n    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\n    False\n    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\n    True\n    \"\"\"\nPlease understand the requirement and write a rough solving process. The necessary \ndetails should be written in natural languages.\nAnswer:\n1. Define a function named `has_close_elements` that takes two parameters: `numbers` \n(a list of floats) and `threshold` (a float).\n2. Initialize a variable `n` to store the length of the `numbers` list.\n3. Iterate through the range of `n` using a for loop with the variable `i`.\n4. Inside the loop, iterate through the range of `n` again using a nested for loop with the \nvariable `j`.\n5. Inside the nested loop, check if `i` is not equal to `j` to avoid comparing the same \nnumber with itself.\n6. Calculate the absolute difference between `numbers[i]` and `numbers[j]` using the \n`abs()` function and store it in a variable `diff`.\n7. Check if `diff` is less than the given `threshold`.\n8. If the condition is satisfied, return True.\n9. After the loops, return False to indicate that no pair of numbers satisfy the condition.\nFigure 11: A case of generating a rough solving process in natural language: the solving process description is in\ngray background.\nfrom typing import List\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\n    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\n    given threshold.\n    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\n    False\n    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\n    True\n    1. Define a function named `has_close_elements` that takes two parameters: `numbers` (a list \nof floats) and `threshold` (a float).\n    2. Initialize a variable `n` to store the length of the `numbers` list.\n    3. Iterate through the range of `n` using a for loop with the variable `i`.\n    4. Inside the loop, iterate through the range of `n` again using a nested for loop with the \nvariable `j`.\n    5. Inside the nested loop, check if `i` is not equal to `j` to avoid comparing the same number \nwith itself.\n    6. Calculate the absolute difference between `numbers[i]` and `numbers[j]` using the `abs()` \nfunction and store it in a variable `diff`.\n    7. Check if `diff` is less than the given `threshold`.\n    8. If the condition is satisfied, return True.\n    9. After the loops, return False to indicate that no pair of numbers satisfy the condition.\n    \"\"\"\n    # Please understand the requirement and write a rough solving process. The necessary details \nshould be written in natural languages.\nFigure 12: A case of generating target code using CoT method: the solving process description is in gray background.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2024-01-25",
  "updated": "2024-01-25"
}