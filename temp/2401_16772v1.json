{
  "id": "http://arxiv.org/abs/2401.16772v1",
  "title": "Extrinsicaly Rewarded Soft Q Imitation Learning with Discriminator",
  "authors": [
    "Ryoma Furuyama",
    "Daiki Kuyoshi",
    "Satoshi Yamane"
  ],
  "abstract": "Imitation learning is often used in addition to reinforcement learning in\nenvironments where reward design is difficult or where the reward is sparse,\nbut it is difficult to be able to imitate well in unknown states from a small\namount of expert data and sampling data. Supervised learning methods such as\nBehavioral Cloning do not require sampling data, but usually suffer from\ndistribution shift. The methods based on reinforcement learning, such as\ninverse reinforcement learning and Generative Adversarial imitation learning\n(GAIL), can learn from only a few expert data. However, they often need to\ninteract with the environment. Soft Q imitation learning (SQIL) addressed the\nproblems, and it was shown that it could learn efficiently by combining\nBehavioral Cloning and soft Q-learning with constant rewards. In order to make\nthis algorithm more robust to distribution shift, we propose more efficient and\nrobust algorithm by adding to this method a reward function based on\nadversarial inverse reinforcement learning that rewards the agent for\nperforming actions in status similar to the demo. We call this algorithm\nDiscriminator Soft Q Imitation Learning (DSQIL). We evaluated it on MuJoCo\nenvironments.",
  "text": "IEICE TRANS. ??, VOL.Exx–??, NO.xx XXXX 200x\n1\nPAPER\nExtrinsicaly Rewarded Soft Q Imitation Learning with\nDiscriminator\nRyoma FURUYAMAa), Daiki KUYOSHIb), and Satoshi YAMANEc),\nSUMMARY\nImitation learning is often used in addition to reinforce-\nment learning in environments where reward design is difficult or where the\nreward is sparse, but it is difficult to be able to imitate well in unknown states\nfrom a small amount of expert data and sampling data. Supervised learn-\ning methods such as Behavioral Cloning do not require sampling data, but\nusually suffer from distribution shift. The methods based on reinforcement\nlearning, such as inverse reinforcement learning and Generative Adversarial\nimitation learning (GAIL), can learn from only a few expert data. However,\nthey often need to interact with the environment. Soft Q imitation learning\n(SQIL) addressed the problems, and it was shown that it could learn effi-\nciently by combining Behavioral Cloning and soft Q-learning with constant\nrewards. In order to make this algorithm more robust to distribution shift,\nwe propose more efficient and robust algorithm by adding to this method\na reward function based on adversarial inverse reinforcement learning that\nrewards the agent for performing actions in status similar to the demo. We\ncall this algorithm Discriminator Soft Q Imitation Learning (DSQIL). We\nevaluated it on MuJoCo environments.\nkey words: Artificial Intelligence, Machine Learning, Deep Reinforcement\nLearning, Imitation Learning, Inverse Reinforcement Learning\n1.\nIntroduction\nRecent developments in the field of deep reinforcement\nlearning have made it possible to learn diverse behaviors\nfor high-dimensional input. However, there are still some\nproblems. Among them, we focus on the efficiency of learn-\ning and the difficulty of designing a reward function [1]. For\nexample, when using reinforcement learning for artificial\nintelligence of autonomous driving, it is necessary to deal\nwith many unexpected phenomena such as various terrain\nand people coming out [2]. If we design a reward function\nto solve this problem, the program may become enormous.\nAlso, incomplete reward function design may promote unex-\npected behavior. In addition, it is necessary to explore with\nmany random actions until the agent obtains a reward in an\nenvironment with sparse rewards.\nIn such setting of problems, imitation learning is often\nused instead of reinforcement learning. Behavioral Cloning\n[3], which is the classical imitation learning, is a simple\nsupervised learning algorithm that maximizes the likelihood\nof the actions taken by an expert in a certain state. It shows\ngood results for simple tasks, but it requires a large dataset\nof the pairs of state and action, and it sometimes behaves\nManuscript received January 1, 2015.\nManuscript revised January 1, 2015.\n†The author is with the\na) E-mail: rfuruyama@csl.ec.t.kanazawa-u.ac.jp\nb) E-mail: dkuyoshi@csl.ec.t.kanazawa-u.ac.jp\nc) E-mail: syamane@is.t.kanazawa-u.ac.jp\nDOI: 10.1587/trans.E0.??.1\nstrangely in a state is not in the dataset. In order to overcome\nthese disadvantages, Inverse Reinforcement Learning (IRL)\n[4] performs a two-step learning process in which it estimates\na reward function instead of expert actions, and it performs\nreinforcement learning based on the reward function. This\nalgorithm helps to learn to behave in unexpected situations.\nHowever, IRL has the disadvantage of being unstable\ndue to two stages of learning. Therefore, the methods of\nlearning the behavior of the expert directly by Generative\nAdversarial Networks (GANs) [5] without explicitly finding\na reward function were proposed such as Generative Ad-\nversarial imitation Learning (GAIL) [6]. This methods can\nlearn efficiently with small amounts of data. Furthermore,\nAdversarial Inverse Reinforcement Leaning (AIRL) [7] has\nbeen proposed for IRL, which outperforms IRL and GAIL\nby improving generalizability through the use of GAN. Thus,\nimitation learning has been greatly advanced by GANs, and\nmethods and applications based on this technology continue\nto be studied [8], [9].\nAlthough Behavioral Cloning was no longer considered\nuseful, Reddy et al. [10] proposed Soft Q Imitation Learning\n(SQIL), which addressed state distribution shift by the com-\nbination of Behavioral Cloning and reinforcement learning.\nIt has been reported that the learning has been performed ef-\nficiently with less training steps than in previous adversarial\nimitation learning. In this paper, we propose more efficient\nand robust algorithm by adding to this method a reward func-\ntion that rewards the agent for performing actions in status\nsimilar to the demo. We evaluate it with three environments\nof MuJoCo [11], and we show the strong and weak points.\n2.\nBackground\nWe consider problems that satisfy the definition of Markov\nDecision Process (MDP) [12].\nIn continuing tasks, the\nreturns for a trajectory 𝜏= (𝑠𝑡, 𝑎𝑡)∞\n𝑡=0 are defined as\n𝑟𝑡= Í∞\n𝑘=𝑡𝛾𝑘−𝑡𝑅(𝑠𝑘, 𝑎𝑘) , where 𝛾is a discount factor.\nIn order to use the same notation for episodic tasks, we\ncan define a set of absorbing state 𝑠𝑎.\nWhen we define\nthe reward 𝑅(𝑠𝑎, ·) = 0, we can define returns simply as\n𝑟𝑡= Í𝑇\n𝑘=𝑡𝛾𝑘−𝑡𝑅(𝑠𝑘, 𝑎𝑘).\nIn reinforcement learning like\nActor Critic (AC) [13] or Q-learning [14], we would like to\nlearn a policy 𝜋that maximizes expected returns.Therefore,\nthe objective function is\n𝐽(𝜋) =\n𝑇\n∑︁\n𝑡=0\nE(𝑠𝑡,𝑎𝑡)∼𝜌𝜋[𝑟(𝑠𝑡, 𝑎𝑡)] .\n(1)\nCopyright © 200x The Institute of Electronics, Information and Communication Engineers\narXiv:2401.16772v1  [cs.LG]  30 Jan 2024\n2\nIEICE TRANS. ??, VOL.Exx–??, NO.xx XXXX 200x\nRecently, various methods have been studied, including\nAC-based Proximal Policy Optimization (PPO) [15] and Q-\nlearning-based Recurrent Replay Distributed DQN (R2D2)\n[16]. One of them is Soft Actor Critic (SAC) [17], [18] pro-\nposed by Haarnoja et al. SAC is a maximum entropy rein-\nforcement learning and has shown excellent performance, es-\npecially in complex environments. Maximum entropy rein-\nforcement learning significantly improves explorability and\nrobustness. is one of the methods. The objective function is\nEquation 1 plus an entropy maximization term 𝐻(𝜋(·|𝑠𝑡)),\n𝐽(𝜋) =\n𝑇\n∑︁\n𝑡=0\nE(𝑠𝑡,𝑎𝑡)∼𝜌𝜋[𝑟(𝑠𝑡, 𝑎𝑡) + 𝛼𝐻(𝜋(·|𝑠𝑡))] ,\n(2)\nwhere 𝛼is the temperature parameter, which determines\nthe relative importance of the entropy term to the reward\nand controls the stochasticity of the optimal policy. SAC\nuses a soft Q function [19] to learn measures that maximize\nestimates while improving their accuracy. This method op-\ntimizes stochastic policies off-policy, and this algorithm is\nsample efficient.\n3.\nRelated works\n3.1\nBehavioral Cloning\nBehavioral Cloning [3] is a classical imitation learning al-\ngorithm. It is a method of supervised learning that takes the\nstate 𝑠𝐸in the expert data as input and regards the action\n𝑎𝐸of the expert as the label.When the set of expert state\nand action pairs (𝑠𝐸, 𝑎𝐸) is 𝛽𝑑𝑒𝑚𝑜, the loss function for the\nparameter 𝜃is the following equation.\n𝑙𝐵𝐶(𝜃) =\n∑︁\n(𝑠,𝑎)∈𝛽𝑑𝑒𝑚𝑜\n−log 𝜋𝜃(𝑎| 𝑠),\n(3)\nwhere 𝜋𝜃is a measure with parameter 𝜃, and the objective is\nto find 𝜃that minimizes Equation 3. Although this method\nis simple, it is easy to overfit to the expert data because it\ndoes not learn the result of the action, and it suffers from the\nstate distribution shift. As a result, it has the disadvantage\nof not being able to make good decisions for unseen states\n[20].\n3.2\nSoft Q Imitation Learning\nTo cover the shortcomings of behavioral cloning, Reddy et\nal [10] proposed Soft Q Imitation Learning (SQIL). SQIL\ncombines behavioral cloning and reinforcement learning to\naddress shifts in the state distribution. This method is built on\nsoft Q learning [19], where experts are assumed to follow a\npolicy 𝜋that maximizes reward 𝑅(𝑠, 𝑎) in an infinite horizon\nMarkov decision process (MDP) with continuous state space\nS and discrete action space A. The policy 𝜋(𝑎| 𝑠) forms a\nBoltzmann distribution for action,\n𝜋(𝑎| 𝑠) ≜\nexp (𝑄(𝑠, 𝑎))\nÍ\n𝑎′∈𝐴exp (𝑄(𝑠, 𝑎′)) ,\n(4)\nwhere 𝑄is soft Q function and is defined by the following\nequation.\n𝑄(𝑠, 𝑎) ≜𝑅(𝑠, 𝑎) + 𝛾E𝑠′\n\"\nlog\n ∑︁\n𝑎′∈𝐴\nexp 𝑄(𝑠′, 𝑎′)\n!#\n(5)\n𝑠′ is the state when action 𝑎is taken in state 𝑠. If we assume\nthat the behavior of our agent follows the policy, we can\ndefine the loss function by Equation 3.\n𝑙𝐵𝐶(𝜃) ≜\n∑︁\n(𝑠,𝑎)∈𝛽𝑑𝑒𝑚𝑜\n−(𝑄𝜃(𝑠, 𝑎)\n−log\n ∑︁\n𝑎′∈𝐴\nexp (𝑄𝜃(𝑠, 𝑎))\n!!\n(6)\nSQIL aims to learn by considering the trajectory by reg-\nularizing this loss function with squared soft Bellman er-\nror 𝛿2 (𝛽, 𝑟), called Regularized Behavioral Cloning (RBC)\n[21].\n𝑙𝑅𝐵𝐶(𝜃) ≜𝑙𝐵𝐶(𝜃) + 𝜆𝛿2 \u0000𝛽𝑑𝑒𝑚𝑜∪𝛽𝑠𝑎𝑚𝑝, 0\u0001\n(7)\n𝛿2 (𝛽, 𝑟) ≜\n1\n| 𝛽|\n∑︁\n(𝑠,𝑎,𝑠′)∈𝛽\n(𝑄𝜃(𝑠, 𝑎)\n−\n \n𝑟+ 𝛾log\n ∑︁\n𝑎′∈𝐴\nexp (𝑄𝜃(𝑠′, 𝑎′))\n!!!2\n,\n(8)\nwhere 𝜆∈𝑅≥0 is a hyperparameter that determines the rel-\native importance of Behavioral Cloning versus soft Q learn-\ning. In addition, 𝛽𝑑𝑒𝑚𝑜and 𝛽𝑠𝑎𝑚𝑝are a replay buffer of\ndemonstration data by an expert and sampling data from an\nenvironment by an agent respectively.\nFurthermore, we can rewrite the gradient of 𝑙𝑅𝐵𝐶(𝜃)\nas simple form.\n▽𝜃𝑙𝑅𝐵𝐶(𝜃) ∝▽𝜃( 𝛿2 (𝛽𝑑𝑒𝑚𝑜, 1) + 𝜆𝑠𝑎𝑚𝑝𝛿2 \u0000𝛽𝑠𝑎𝑚𝑝, 0\u0001\n+ log\n ∑︁\n𝑎∈𝐴\nexp (𝑄𝜃(𝑠0, 𝑎))\n!!\n,\n(9)\nwhere 𝑠0 is the initial state. The update equation for 𝜃in the\nSQIL algorithm is defined by Equation 9.\nFURUYAMA et al.: EXTRINSICALY REWARDED SOFT Q IMITATION LEARNING WITH DISCRIMINATOR\n3\n𝜃←𝜃−𝜂▽𝜃\n\u0010\n𝛿2 (𝛽𝑑𝑒𝑚𝑜, 1) + 𝜆𝑠𝑎𝑚𝑝\n\u0000𝛽𝑠𝑎𝑚𝑝, 0\u0001\u0011\n(10)\nIn original paper, 𝜆𝑠𝑎𝑚𝑝= 1. Importantly, we can recognize\nthat the SQIL agent sets the rewards of all demonstration\ndata to 1 and the rewards of all sampling data to 0.\nSQIL can be implemented because it requires only mi-\nnor modifications to standard Q-learning implementations;\nSQIL can also be extended to MDPs with continuous action\nspaces by simply replacing Q-learning with an off-policy\nactor-critic method such as SAC Given the difficulty of cor-\nrectly implementing deep RL algorithms [22], this flexibility\nis an advantage and enhances the utility of SQIL because it\ncan be built on top of existing implementations of deep RL\nalgorithms.\n3.3\nGenerative Adversarial Networks\nGenerative Adversarial Networks (GANs) [5] is one of the\nmethods used for image generation.\nIn this method, two\nmodels are prepared: a generator 𝐺and a discriminator\n𝐷. In the generator 𝐺, the distribution of the input noise\nvariable 𝑝𝑧is predefined and the prepared data is trained.\nThe objective of this learning is to generate data similar to\nthe training example from the noise variables. The objective\nof 𝐷, on the other hand, is to discriminate between the data\nin the training example and the data generated in 𝐺. In this\nway, 𝐺and 𝐷learn to compete with each other. In other\nwords, 𝐺and 𝐷are playing a mini-max game based on the\nvalue function 𝑉(𝐺, 𝐷).\nmin\n𝐺max\n𝐷𝑉(𝐺, 𝐷) =E𝑥∼𝑝𝑑𝑎𝑡𝑎(𝑥) [log 𝐷(𝑥)]\n+ E𝑧∼𝑝𝑧(𝑧) [log (1 −𝐷(𝐺(𝑧)))] ,\n(11)\nwhere 𝑝𝑑𝑎𝑡𝑎is the distribution of the data in the train\nexample.Although GANs are image generators, they are\nwell suited for reinforcement learning and have contributed\ngreatly to the advancement of that technology, especially\nin imitation learning; GAIL [6] is one such example. An\noverview of the architecture of GANS is shown in Figure 1.\n4.\nProposal of Discriminator Soft Q Imitation Learning\n4.1\nDiscriminator Soft Q Imitation Learning\nSQIL [10] shows that soft Q learning, which provides a\npositive constant reward for expert data, can efficiently mimic\nan agent. However, the method of determining this constant\nreward is not always a good method. For example, after some\nprogress in learning, the agent learns as reward 0 even if the\nsample data obtained from the environment is similar to the\nexpert’s data. This may become noise in the learning process.\nTherefore, we propose a method that uses the discriminator D\nof the GAN as the reward function. This method is expected\nto reduce the above problem as well as to learn efficiently\nwith less expert data. We call this method Discriminator Soft\nFig. 1: Generative Adversarial Network Architecture.\nQ Imitation Learning (DSQIL). DSQIL’s algorithm is shown\nin Algorithm 1, and an overall view of DSQIL is shown in\nFigure 2. As with SQIL, the agent can be used flexibly, for\nexample, Q-Learning [14] for discrete value control tasks\nand SAC [17] for continuous value control tasks.\nAlgorithm 1 Discriminator Soft Q Imitation Learning\n(DSQIL)\nRequire: Replay buffer of demonstaration data 𝛽𝑑𝑒𝑚𝑜,\n𝜆𝑑𝑒𝑚𝑜= 1, 𝜆𝑠𝑎𝑚𝑝= 1 in our experiment.\n1: Initialize replay buffer of sample data 𝛽𝑠𝑎𝑚𝑝←∅\n2: for 𝑛= 1, 2, ... do\n3:\nwhile 𝑒≠𝑇𝑟𝑢𝑒do\n4:\n𝜏= (𝑠, 𝑎, ., 𝑠′, 𝑒) with 𝜋𝜃\n5:\n𝛽𝑠𝑎𝑚𝑝←𝛽𝑠𝑎𝑚𝑝∪𝜏\n6:\n𝑀𝑑𝑒𝑚𝑜=\n\b\u0000𝑠𝑡, 𝑎𝑡, ., 𝑠′\n𝑡, 𝑒𝑡\n\u0001\t𝑚\n𝑡=1 ∼𝛽𝑑𝑒𝑚𝑜\n7:\n𝑀𝑠𝑎𝑚𝑝=\n\b\u0000𝑠𝑡, 𝑎𝑡, ., 𝑠′\n𝑡, 𝑒𝑡\n\u0001\t𝑚\n𝑡=1 ∼𝛽𝑠𝑎𝑚𝑝\n8:\nCalcurate the loss of 𝐷\n9:\nUpdate 𝐷with GAN\n10:\nfor 𝑖= 1, 2, ..., 𝑀𝑑𝑒𝑚𝑜do\n11:\n𝑅\u0000𝛽𝑑𝑒𝑚𝑜𝑖\n\u0001 ←𝐷(𝑠𝑖,𝑎𝑖)\n2\n+\n1\n2𝜆𝑑𝑒𝑚𝑜\n12:\nend for\n13:\nfor 𝑗= 1, 2, ..., 𝑀𝑠𝑎𝑚𝑝do\n14:\n𝑅\n\u0010\n𝛽𝑠𝑎𝑚𝑝𝑗\n\u0011\n←\n𝐷\n\u0010\n𝑠𝑗,𝑎𝑗\n\u0011\n2\n15:\nend for\n16:\nUpdate 𝜃{See Equation 12}\n17:\nend while\n18: end for\n4.2\nUpdate equation\nWe set up the DSQIL update equation based on Equation 10.\n𝜃←𝜃−𝜂▽𝜃(𝜆𝑑𝑒𝑚𝑜𝛿2\n\u0012\n𝛽𝑑𝑒𝑚𝑜, 𝑅(𝛽𝑑𝑒𝑚𝑜) +\n1\n2𝜆𝑑𝑒𝑚𝑜\n\u0013\n+𝜆𝑠𝑎𝑚𝑝𝛿2 \u0000𝛽𝑠𝑎𝑚𝑝, 𝑅\u0000𝛽𝑠𝑎𝑚𝑝\n\u0001\u0001\u0011\n,\n4\nIEICE TRANS. ??, VOL.Exx–??, NO.xx XXXX 200x\nFig. 2: The overall of DSQIL algorithm.\n(12)\nwhere 𝜆𝑑𝑒𝑚𝑜∈R≥0, 𝜆𝑠𝑎𝑚𝑝∈R≥0 are hyperparameter, and\n𝛿2 denotes the squared soft Bellman error defined in Equation\n8. We added to the rewards, in addition to the fixed values,\na function that indicates the probability of similarity of the\nexpert data using discriminator 𝐷from GANs [5].\nThis\nchange enabled more efficient learning by rewarding sample\ndata similar to the expert data.\nThe update equation was derived from the following\nloss function defined with reference to Equation 7.\n𝑙𝐷𝑆𝑄𝐼𝐿(𝜃) ≜𝑙𝐵𝐶(𝜃) +𝜆𝛿2 \u0000𝛽𝑑𝑒𝑚𝑜∪𝛽𝑠𝑎𝑚𝑝, 𝑅\u0001 , (13)\nwhere, 𝑅is reward function.In addition, the soft value func-\ntion is defined as follows.\n𝑉(𝑠) ≜log\n ∑︁\n𝑎∈𝐴\nexp (𝑄𝜃(𝑠, 𝑎))\n!\n(14)\nIn Equation 13, dividing the soft Bellman squared error\nterm by demonstration data and sample data, and further\nusing Equation 6,\n▽𝑙𝐷𝑆𝑄𝐼𝐿(𝜃) =\n∑︁\n𝜏∈𝛽𝑑𝑒𝑚𝑜\n𝑇−1\n∑︁\n𝑡=0\n−(▽𝑄𝜃(𝑠𝑡, 𝑎𝑡) −▽𝑉(𝑠𝑡))\n+ 𝜆𝑑𝑒𝑚𝑜\n∑︁\n𝜏∈𝛽𝑑𝑒𝑚𝑜\n𝑇−1\n∑︁\n𝑡=0\n▽(𝑄𝜃(𝑠𝑡, 𝑎𝑡) −(𝑅+ 𝛾𝑉(𝑠𝑡+1)))2\n+ 𝜆𝑠𝑎𝑚𝑝▽𝛿2 \u0000𝛽𝑠𝑎𝑚𝑝, 𝑅\u0001\n=\n∑︁\n𝜏∈𝛽𝑑𝑒𝑚𝑜\n𝑇−1\n∑︁\n𝑡=0\n(𝑉(𝑠𝑡) −𝛾𝑉(𝑠𝑡+1))\n+ 𝜆𝑑𝑒𝑚𝑜▽𝛿2\n\u0012\n𝛽𝑑𝑒𝑚𝑜, 𝑅+\n1\n2𝜆𝑑𝑒𝑚𝑜\n\u0013\n+ 𝜆𝑠𝑎𝑚𝑝▽𝛿2 \u0000𝛽𝑠𝑎𝑚𝑝, 𝑅\u0001 .\n(15)\nAssuming 𝛾≜1, the inner product of the first term is a\ntelescoping sum.\n▽𝑙𝐷𝑆𝑄𝐼𝐿(𝜃) =\n∑︁\n𝜏∈𝛽𝑑𝑒𝑚𝑜\n(▽𝑉(𝑠0) −▽𝑉(𝑠𝑇))\n+ 𝜆𝑑𝑒𝑚𝑜▽𝛿2\n\u0012\n𝛽𝑑𝑒𝑚𝑜, 𝑅+\n1\n2𝜆𝑑𝑒𝑚𝑜\n\u0013\n+ 𝜆𝑠𝑎𝑚𝑝▽𝛿2 \u0000𝛽𝑠𝑎𝑚𝑝, 𝑅\u0001\n(16)\nFrom the assumption that 𝑠𝑇is absorbed, 𝑉(𝑠𝑇) = 0. There-\nfore,\nFURUYAMA et al.: EXTRINSICALY REWARDED SOFT Q IMITATION LEARNING WITH DISCRIMINATOR\n5\n▽𝑙𝐷𝑆𝑄𝐼𝐿(𝜃) =\n∑︁\n𝜏∈𝛽𝑑𝑒𝑚𝑜\n▽𝑉(𝑠0)\n+ 𝜆𝑑𝑒𝑚𝑜▽𝛿2\n\u0012\n𝛽𝑑𝑒𝑚𝑜, 𝑅+\n1\n2𝜆𝑑𝑒𝑚𝑜\n\u0013\n+ 𝜆𝑠𝑎𝑚𝑝▽𝛿2 \u0000𝛽𝑠𝑎𝑚𝑝, 𝑅\u0001 .\n(17)\nIn our experiments, all demo rollouts start from the same\ninitial state s0. Thus,\n▽𝑙𝐷𝑆𝑄𝐼𝐿(𝜃) ∝▽\n\u0012\n𝜆𝑑𝑒𝑚𝑜𝛿2\n\u0012\n𝛽𝑑𝑒𝑚𝑜, 𝑅+\n1\n2𝜆𝑑𝑒𝑚𝑜\n\u0013\n+𝜆𝑠𝑎𝑚𝑝𝛿2 \u0000𝛽𝑠𝑎𝑚𝑝, 𝑅\u0001 + 𝑉(𝑠0)\n\u0011\n.\n(18)\nThus, the gradient of the loss function is similar to the up-\ndated equation shown in Equation 12 plus the soft value at\n𝑠0.\n4.3\nReward function\nIn the update equation, we had placed the reward function\n𝑅. We used the discriminator 𝐷with reference to GANs\n[5] as the reward function R.The discriminator 𝐷was used\nas the reward function.\nThis discriminator 𝐷is trained\nto discriminate between expert data and sample data, and\nthe probability of being expert data is expressed as a value\nbetween 0 and 1. In other words, the value is close to 1 if\nit discriminates the data as expert data and close to 0 if it\njudges the data as other than expert data. To optimize the\ndiscriminator 𝐷with weight 𝜙, We minimize the following\nloss function,\n𝑙(𝜙) ≜−E(𝑠,𝑎)∼𝛽𝑑𝑒𝑚𝑜\n\u0002\nlog \u0000𝐷𝜙(𝑠, 𝑎)\u0001\u0003\n−E(𝑠,𝑎)∼𝛽𝑠𝑎𝑚𝑝\n\u0002\nlog \u00001 −𝐷𝜙(𝑠, 𝑎)\u0001\u0003\n(19)\nUsing this discriminator as a reward function, we can reward\nwhen data similar to the expert data is obtained as sample\ndata, thus enabling efficient learning.\nThe algorithm based on the rewards and hyperparam-\neters used in the experiment is shown in Algorithm 1. The\nhyperparameters were set to 𝜆𝑑𝑒𝑚𝑜= 1, 𝜆𝑠𝑎𝑚𝑝= 1, and\n𝑅= 𝐷(𝑠,𝑎)\n2\nwith the reward as half of the trained discrimi-\nnator’s output, based on the settings that performed best in\nSQIL, 1 reward for expert data and 0 reward for demo data.\nThis setting allows us to limit the reward to between 0 and 1,\nclose to the reward in SQIL. In this way, we can expect simi-\nlar performance to SQIL in the early stages when learning is\ninsufficient, and allow learning from sample data after some\nlearning has progressed. Therefore, performance similar to\nor better than SQIL can be expected.\n5.\nExperimental Evaluation\n5.1\nOutline of experimentation\nTo evaluate DSQIL, we compared DSQIL to SQIL using\nTable 1: expert policy performance to provide expert data.\nEnvironments\nExpert Performance\nHopper-V3\n3308.3 ± 26.7\nWalker2D-V3\n3897.7 ± 31.6\nHalfCheetah-V3\n5303.3 ± 75.4\nTable 2: SAC hyperparameters.\nParameters\nValue\nOptimizer\nAdam\nDiscount rate (𝛾)\n0.99\nHyperparameters\n𝛼initial\n0.2\nActor learning rate\n3𝑒−4\nCritic learning rate\n3𝑒−4\n𝛼learning rate\n3𝑒−4\nTarget update rate\n5𝑒−3\nMini batch size\n64\nActor hidden dim\n256\nActor hidden layers\n3\nArchitecture\nActor activation function\nReLU\nCritic hidden dim\n256\nCritic hidden layers\n3\nCritic activation function\nReLU\nTable 3: Discriminator hyperparameters.\nParameters\nValue\nOptimizer\nAdam\nlearning rate\n3𝑒−4\nHyperparameters\nMini batch size\n512\nLoss function\nBinary Cross Entropy\nreplay buffer size(𝛽𝑠𝑎𝑚𝑝)\n1e+6\nwarm up\n1024\nHidden dim\n128\nArchitecture\nHidden layers\n3\nActivation function\nTanh\nLast activation\nSigmoid\nsome empirical data: we evaluated Hopper-v2, Walker2d-\nv2, and HalfCheetah-v2 in three MuJoCo [11] environ-\nments. Since the environment used in the experiment was\na continuous-value control task, SAC [17] was used as the\nagent in this case.\nFirst, the policy is learned using SAC to obtain expert\nperformance. The expert policy is used to generate a set of\nexpert data to be stored in the replay buffer. The obtained\nexpert policy performance is shown in Table 1.\nAfter the expert data is obtained, each method is trained.\nTo study the effect of learning on the amount of expert data,\nthe algorithms are trained on sets of {2, 4, 8, 16, 32} as seen\nin [23]. In all environments, learning is performed in 500\nsteps, with one step acquiring one episode of sample data\nand storing it in the replay buffer. During training, sample\nand expert data are randomly extracted from the replay buffer\nat a ratio of 1:1 to be used for the discriminator in order to\nobtain a reward. At this time, the discriminator is trained\nsimultaneously. All reported results correspond to perfor-\nmance measures obtained after testing the learner policy on\n50 episodes. For agent we used SAC [17]: a 3 layer MLP\nwith ReLU activations and 256 hidden units. The discrimi-\nnator is a 3 layer MLP with a sigmoid at the end in addition\n6\nIEICE TRANS. ??, VOL.Exx–??, NO.xx XXXX 200x\nTable 4: Performance (The maximum value for each trajectory is shown in bold.)\nEnvironments\ntrajectory\nBC\nSQIL\nDSQIL(ours)\nHopper-V3\n2\n2766.7 ± 385.6\n3001.6 ± 422.4\n2886.9 ± 741.3\n4\n3010.8 ± 460.3\n3113.9 ± 365.6\n3202.3 ± 187.1\n8\n3051.5 ± 294.5\n3156.9 ± 351.7\n3262.9 ± 3.3\n16\n3164.7 ± 269.8\n3274.9 ± 6.1\n3209.8 ± 14.5\n32\n3241.1 ± 141.0\n3287.6 ± 1.9\n3272.6 ± 1.8\nWalker2D-V3\n2\n797.1 ± 75.1\n2112.4 ± 688.6\n3520.8 ± 551.3\n4\n2001.0 ± 1360.0\n3799.2 ± 214.8\n3987.4 ± 64.4\n8\n2110.0 ± 1398.1\n3989.7 ± 24.8\n3929.9 ± 15.5\n16\n3571.7 ± 588.6\n3911.9 ± 18.3\n3907.1 ± 19.4\n32\n3756.5 ± 236.2\n3905.7 ± 11.1\n3921.9 ± 41.9\nHalfCheetah-V3\n2\n381.9 ± 334.1\n113.9 ± 104.0\n99.0 ± 103.1\n4\n599.3 ± 306.9\n143.5 ± 115.1\n1607.5 ± 682.8\n8\n2622.4 ± 342.0\n1874.4 ± 2150.9\n4515.0 ± 289.9\n16\n3228.4 ± 671.2\n4361.4 ± 86.8\n4499.4 ± 105.2\n32\n3350.5 ± 491.4\n4636.8 ± 161.9\n4713.5 ± 69.3\nto tanh for activity and 128 hidden units. We trained all\nnetworks with the Adam optimizer [24].\nWe show other hyperparameters in Table 2 and Table 3.\nFor each task, we compare the following three algo-\nrithms:\n1. BC\n2. SQIL based on SAC\n3. DSQIL based on SAC (ours)\n5.2\nComparing the scores\nEvaluation results are shown in Table 4. In each environ-\nment, DSQIL outperforms BC. For relatively simple tasks\nsuch as the Hopper and Walker2d environments, SQIL and\nDSQIL have comparable performance if sufficient expert\ndata is available. On the other hand, in complex environ-\nments with more information required, such as the HalfChee-\ntah environment, DSQIL performs better than SQIL regard-\nless of the amount of expert data.\nThe performance dif-\nference is especially large when there is less expert data.\nHowever, for simple tasks such as the Hopper environment,\nthere is no performance difference depending on the amount\nof expert data, and DSQIL shows lower performance than\nSQIL with a small amount of expert data.\n5.3\nComparing the speed of learning\nThe training for DSQIL and SQIL is shown in Figure 3. It\nshows the learning for each environment given 32 episodes\nof expert data. At the end of an epoch, 5 episodes are tested\nusing the network learned up to that point, and the average of\nthe results is obtained. Figure 3 shows the evolution of these\nvalues obtained with SQIL and DSQIL. It can be seen that in\nthe relatively simple Hopper and Walker2d environments, the\nSQIL learns faster than the DSQIL. This is due to the fact that\ntraining a discriminator requires a certain number of steps.\nIf the discriminator is not well trained, it is possible that\nthe learning could be affected by giving unjustified rewards\nto the sample data.\nOn the other hand, in the complex\nHalfCheetah environment, DSQIL learns faster than SQIL.\nThis indicates that in complex environments, the effect of\nbeing able to learn from sample data is greater than the loss\nincurred during the training period of the discriminator.\n5.4\nComparing the rewards\nA comparison between DSQIL and SQIL for each reward\ntransition for the sample and expert data in each environment\nis shown in Figure 4. This shows the evolution of rewards\nduring step-by-step learning when 32 episodes of expert data\nare given as in Chapter 5.3. As we designed, SQIL continues\nto give fixed values, while DSQIL shows variable rewards. In\nparticular, in the HalfCheetah environment, it is noticeable\nthat the reward for the sample data increases as the learning\nprogresses. It can be seen that the rewards are given to data\nthat is close to the expert data found in the sample data and\nused for learning. On the other hand, we observed a decrease\nin the reward for expert data. Although no effect of this was\nobserved in the experiment, it is possible that the accuracy\nof the data decreases when the number of learning epochs is\nincreased, or that the data behaves differently from the expert\ndata.\n6.\nConclusion\nIn this paper, we propose Discriminator Soft Q Imitation\nLearning (DSQIL) as a data-efficient imitation learning\nmethod. In contrast to the conventional method SQIL, we\nshow that DSQIL can provide more detailed rewards for\nstate-action pairs by using a reward function instead of a con-\nstant reward. The method incorporates the idea of a GAN\ndiscriminator in the reward function and was evaluated in\nthree experiments with MuJoCo.\nThe experiments confirm that DSQIL performs as well\nas or better than conventional imitation learning. Especially\nin complex environments, DSQIL outperforms SQIL in both\ndata efficiency and learning efficiency. On the other hand,\nin certain environments, the rewards for both expert and\nsample data tended to converge to similar values as learning\nprogressed. This can be an advantage when seeking higher\nperformance from the expert data, but a disadvantage when\nFURUYAMA et al.: EXTRINSICALY REWARDED SOFT Q IMITATION LEARNING WITH DISCRIMINATOR\n7\nseeking performance comparable to the expert data.\nBased on the above considerations, it is necessary to\nexamine the extent to which discriminator accuracy affects\nlearning. It is also worth continuing research in terms of\nverifying performance in more complex environments and\ndesigning reward functions that improve performance in sim-\nple environments.\nReferences\n[1] S.P. Boyd and L. Vandenberghe, Convex optimization, Cambridge\nuniversity press, 2004.\n[2] U. Sumanth, N.S. Punn, S.K. Sonbhadra, and S. Agarwal, “Enhanced\nbehavioral cloning-based self-driving car using transfer learning,”\nin Data Management, Analytics and Innovation: Proceedings of\nICDMAI 2021, Volume 2, pp.185–198, Springer, 2021.\n[3] D.A. Pomerleau, “Efficient training of artificial neural networks for\nautonomous navigation,” Neural computation, vol.3, no.1, pp.88–97,\n1991.\n[4] A.Y. Ng, S. Russell, et al., “Algorithms for inverse reinforcement\nlearning.,” Icml, p.2, 2000.\n[5] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,\nS. Ozair, A. Courville, and Y. Bengio, “Generative adversarial nets,”\nAdvances in neural information processing systems, vol.27, 2014.\n[6] J. Ho and S. Ermon, “Generative adversarial imitation learning,”\nAdvances in neural information processing systems, vol.29, 2016.\n[7] J. Fu,\nK. Luo,\nand S. Levine,\n“Learning robust rewards\nwith adversarial inverse reinforcement learning,” arXiv preprint\narXiv:1710.11248, 2017.\n[8] S. Choi, J. Kim, and H. Yeo, “Trajgail:\nGenerating urban ve-\nhicle trajectories using generative adversarial imitation learning,”\nTransportation Research Part C: Emerging Technologies, vol.128,\np.103091, 2021.\n[9] Q. Wu, L. Li, and Z. Yu, “Textgail: Generative adversarial imitation\nlearning for text generation,” Proceedings of the AAAI Conference\non Artificial Intelligence, pp.14067–14075, 2021.\n[10] S. Reddy, A.D. Dragan, and S. Levine, “Sqil:\nImitation learn-\ning via reinforcement learning with sparse rewards,” arXiv preprint\narXiv:1905.11108, 2019.\n[11] E. Todorov, T. Erez, and Y. Tassa, “Mujoco: A physics engine for\nmodel-based control,” 2012 IEEE/RSJ international conference on\nintelligent robots and systems, pp.5026–5033, IEEE, 2012.\n[12] R.S. Sutton and A.G. Barto, Reinforcement learning: An introduc-\ntion, MIT press, 2018.\n[13] V. Konda and J. Tsitsiklis, “Actor-critic algorithms,” Advances in\nneural information processing systems, vol.12, 1999.\n[14] C.J. Watkins and P. Dayan, “Q-learning,” Machine learning, vol.8,\npp.279–292, 1992.\n[15] J.\nSchulman,\nF.\nWolski,\nP.\nDhariwal,\nA.\nRadford,\nand\nO. Klimov, “Proximal policy optimization algorithms,” arXiv\npreprint arXiv:1707.06347, 2017.\n[16] S. Kapturowski, G. Ostrovski, J. Quan, R. Munos, and W. Dabney,\n“Recurrent experience replay in distributed reinforcement learning,”\nInternational conference on learning representations, 2018.\n[17] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, “Soft actor-critic:\nOff-policy maximum entropy deep reinforcement learning with a\nstochastic actor,” International conference on machine learning,\npp.1861–1870, PMLR, 2018.\n[18] T. Haarnoja, A. Zhou, K. Hartikainen, G. Tucker, S. Ha, J. Tan, V. Ku-\nmar, H. Zhu, A. Gupta, P. Abbeel, et al., “Soft actor-critic algorithms\nand applications,” arXiv preprint arXiv:1812.05905, 2018.\n[19] T. Haarnoja, H. Tang, P. Abbeel, and S. Levine, “Reinforcement\nlearning with deep energy-based policies,” International conference\non machine learning, pp.1352–1361, PMLR, 2017.\n[20] S. Ross, G. Gordon, and D. Bagnell, “A reduction of imitation learn-\ning and structured prediction to no-regret online learning,” Proceed-\nings of the fourteenth international conference on artificial intelli-\ngence and statistics, pp.627–635, JMLR Workshop and Conference\nProceedings, 2011.\n[21] B. Piot, M. Geist, and O. Pietquin, “Boosted and reward-regularized\nclassification for apprenticeship learning,” Proceedings of the 2014\ninternational conference on Autonomous agents and multi-agent sys-\ntems, pp.1249–1256, 2014.\n[22] P. Henderson, R. Islam, P. Bachman, J. Pineau, D. Precup, and\nD. Meger, “Deep reinforcement learning that matters,” Proceedings\nof the AAAI conference on artificial intelligence, 2018.\n[23] G. Papagiannis and Y. Li, “Imitation learning with sinkhorn dis-\ntances,” Joint European Conference on Machine Learning and\nKnowledge Discovery in Databases, pp.116–131, Springer, 2022.\n[24] D.P. Kingma and J. Ba, “Adam: A method for stochastic optimiza-\ntion,” arXiv preprint arXiv:1412.6980, 2014.\nRyoma Furuyama\nHe received a B.S. de-\ngree from Kanazawa University in 2022.\nHe\nis now a M.S. student studying reinforcement\nlearning.\nDaiki Kuyoshi\nHe received a M.S. degree\nfrom Kanazawa University in 2022. He is inter-\nested in reinforcement learning\nSatoshi Yamane\nHe received B.S.,M.S and\nPh.D. degrees from Kyoto University. Now he\nis a professor of Kanazawa University.\nHe is\ninterested in formal verification of real-time and\ndistributed computing.\n8\nIEICE TRANS. ??, VOL.Exx–??, NO.xx XXXX 200x\n(a) Hopper-v3\n(b) Walker2d-v3\n(c) HalfCheetah-v3\nFig. 3: Comparison of the average score per epoch for each environment for 32 episodes of expert data.\nFURUYAMA et al.: EXTRINSICALY REWARDED SOFT Q IMITATION LEARNING WITH DISCRIMINATOR\n9\n(a) Expert data rewards in Hopper-v3\n(b) Sample data rewards in Hopper-v3\n(c) Expert data rewards in Walker2d-v3\n(d) Sample data rewards in Walker2d-v3\n(e) Expert data rewards in HalfCheetah-v3\n(f) Sample data rewards in HalfCheetah-v3\nFig. 4: Comparison of expert data and sample data rewards for each environment with 32 episodes of expert data.\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "I.2.6"
  ],
  "published": "2024-01-30",
  "updated": "2024-01-30"
}