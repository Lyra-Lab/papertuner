{
  "id": "http://arxiv.org/abs/2402.03988v3",
  "title": "REBORN: Reinforcement-Learned Boundary Segmentation with Iterative Training for Unsupervised ASR",
  "authors": [
    "Liang-Hsuan Tseng",
    "En-Pei Hu",
    "Cheng-Han Chiang",
    "Yuan Tseng",
    "Hung-yi Lee",
    "Lin-shan Lee",
    "Shao-Hua Sun"
  ],
  "abstract": "Unsupervised automatic speech recognition (ASR) aims to learn the mapping\nbetween the speech signal and its corresponding textual transcription without\nthe supervision of paired speech-text data. A word/phoneme in the speech signal\nis represented by a segment of speech signal with variable length and unknown\nboundary, and this segmental structure makes learning the mapping between\nspeech and text challenging, especially without paired data. In this paper, we\npropose REBORN,Reinforcement-Learned Boundary Segmentation with Iterative\nTraining for Unsupervised ASR. REBORN alternates between (1) training a\nsegmentation model that predicts the boundaries of the segmental structures in\nspeech signals and (2) training the phoneme prediction model, whose input is\nthe speech feature segmented by the segmentation model, to predict a phoneme\ntranscription. Since supervised data for training the segmentation model is not\navailable, we use reinforcement learning to train the segmentation model to\nfavor segmentations that yield phoneme sequence predictions with a lower\nperplexity. We conduct extensive experiments and find that under the same\nsetting, REBORN outperforms all prior unsupervised ASR models on LibriSpeech,\nTIMIT, and five non-English languages in Multilingual LibriSpeech. We\ncomprehensively analyze why the boundaries learned by REBORN improve the\nunsupervised ASR performance.",
  "text": "REBORN: Reinforcement-Learned Boundary\nSegmentation with Iterative Training\nfor Unsupervised ASR\nLiang-Hsuan Tseng‚àó\nEn-Pei Hu‚àó\nCheng-Han Chiang\nYuan Tseng\nHung-yi Lee\nLin-shan Lee\nShao-Hua Sun\nNational Taiwan University\nAbstract\nUnsupervised automatic speech recognition (ASR) aims to learn the mapping\nbetween the speech signal and its corresponding textual transcription without the\nsupervision of paired speech-text data. A word/phoneme in the speech signal\nis represented by a segment of speech signal with variable length and unknown\nboundary, and this segmental structure makes learning the mapping between speech\nand text challenging, especially without paired data. In this paper, we propose\nREBORN, Reinforcement-Learned Boundary Segmentation with Iterative Training\nfor Unsupervised ASR. REBORN alternates between (1) training a segmentation\nmodel that predicts the boundaries of the segmental structures in speech signals\nand (2) training the phoneme prediction model, whose input is the speech feature\nsegmented by the segmentation model, to predict a phoneme transcription. Since\nsupervised data for training the segmentation model is not available, we use re-\ninforcement learning to train the segmentation model to favor segmentations that\nyield phoneme sequence predictions with a lower perplexity. We conduct extensive\nexperiments and find that under the same setting, REBORN outperforms all prior\nunsupervised ASR models on LibriSpeech, TIMIT, and five non-English languages\nin Multilingual LibriSpeech. We comprehensively analyze why the boundaries\nlearned by REBORN improve the unsupervised ASR performance.\n1\nIntroduction\nAutomatic speech recognition (ASR) systems convert speech signals into their transcription texts.\nMost state-of-the-art ASR systems are trained with dense supervision using a large amount of labeled\nspeech-text paired data [10, 52]. However, more than 80% of languages in the world have limited\naccess to speech-text paired data [51], and collecting such paired data requires intensive labor and\ncosts, fundamentally limiting the applicability of supervised ASR systems to these languages. This\nleads to significant efforts devoted to developing unsupervised ASR (UASR) systems, which aim to\nlearn the mapping between speech signals and textual transcriptions (words or phonemes) without\nany speech-text pairs [4, 11, 18, 30, 33].\nUASR models learn to align the distribution of input speech signal and output text without paired\ndata. Learning to match two distributions of sequences unsupervisedly has been extensively studied\nin unsupervised neural machine translation (NMT) [1], where the aim is to learn a neural network\nthat can translate text in a source language to text in a target language without paired data [26, 27].\nPrior works show that adversarial training [20] can be used to learn such mappings [26, 27], which\nemploys a generator learning to translate, and a discriminator learning to distinguish generated text\nfrom the real text in the target language.\n‚àóEqual contribution.\nCorrespondence to: Shao-Hua Sun <shaohuas@ntu.edu.tw>\n38th Conference on Neural Information Processing Systems (NeurIPS 2024).\narXiv:2402.03988v3  [eess.AS]  15 Nov 2024\nCan we adopt such an adversarial training scheme for UASR? In unsupervised NMT, text can be\neasily tokenized into sub-word tokens, so unsupervised NMT only needs to learn the mapping\nbetween the source and target language‚Äôs token embeddings. However, in UASR, a phoneme or word\nis represented by a variable-length segment in the speech signal whose boundaries are unknown.\nMoreover, the length of a speech signal is much longer than the length of its textual transcription.\nThe above characteristics of speech make learning the mapping between the segmental structures in\nspeech and textual transcription challenging. Existing works in unsupervised ASR rely on handcrafted\nrules or separately learned modules to obtain the boundaries of the segmental structures [4, 33, 64].\nYet, such handcrafted boundaries are often sub-optimal, bottlenecking the performance of UASR.\nThis paper focuses on learning better segmental boundaries to improve the mapping between seg-\nmented speech and textual transcription. We propose REBORN (Reinforcement-Learned Boundary\nSegmentation with Iterative Training), an unsupervised ASR framework with a segmentation model\nand a phoneme prediction model. The segmentation model determines segmental structure boundaries\nin speech signals, while the phoneme prediction model assigns a phoneme to each segmental structure.\nAfter properly initializing the phoneme prediction model, we use an iterative algorithm that alternates\nbetween two stages to train the segmentation model and refine the phoneme prediction model. The\nfirst stage trains the segmentation model. Since we do not have the ground truth segmental bound-\naries, we use reinforcement learning to train the segmentation model to favor segmentations that\nyield phoneme sequence predictions with a lower perplexity. We experiment with various learning\nobjectives and implement them as reward functions for learning the segmentation model. In the\nsecond stage, based on the boundaries predicted by the segmentation model learned in the previous\nstage, we use an adversarial loss similar to generative adversarial networks (GANs) [20] to train the\nphoneme prediction model.\nWe conduct extensive experiments on LibriSpeech [45], TIMIT [19], and Multilingual LibriSpeech\n(MLS) [50] to compare the phoneme/phone error rate (PER) and word error rate (WER) with prior\nworks. We show that our method outperforms all prior UASR methods on LibriSpeech, TIMIT, and\nfive languages in MLS when using the same amount of training data. We perform thorough ablation\nstudies to show that iterative training and the rewards we design are critical to the performance of\nREBORN. By analyzing the segmental structure obtained by our segmentation model, we find that the\nsegmental structures are acoustic units smaller than phonemes, which helps the phoneme prediction\nmodel predict more accurate transcriptions. To facilitate further research and replication, our code\nand models are available at https://github.com/andybi7676/reborn-uasr.\n2\nRelated Work\nAn ASR model takes speech signals as the input and predicts the textual transcriptions. Unsupervised\nASR (UASR) aims to train an ASR model without access to paired speech-text data. Instead, the only\navailable training data is unlabeled speech data and unlabeled text data, while the correspondence\nbetween speech and text is not available. In this paper, we follow prior UASR works [4, 33] to predict\nphoneme transcriptions from speech signals. To achieve this, a lexicon is required to transform the\nunlabeled text into phoneme sequences. Learning the mapping between speech signals and phoneme\nsequences can be formulated as a distribution matching problem, where we want to learn an ASR\nmodel whose output phoneme sequences match the distribution of real phoneme sequences.\nYeh et al. [64] address this distribution matching problem using an unsupervised loss function based\non empirical output distribution matching [34], which guides the ASR model to produce phoneme\nsequences with statistical distributions close to real phoneme sequences. Wang et al. [60] further\nextend this approach by matching the N-skipgram and positional unigram distributions.\nLiu et al. [33] propose to use the generative adversarial network (GAN) [20] for UASR. The GAN\nsolves the distribution matching problem by training a generator whose output distribution resembles\na target distribution, and a discriminator is trained to distinguish the outputs of the generator and the\nsamples from the target distribution. When using GAN to train unsupervised ASR, the generator\nis the phoneme prediction model. The phoneme prediction model takes in speech features of the\nsegmental structures in the speech signal, and outputs a phoneme transcription. Prior works rely on\nhand-crafted or separately trained unsupervised phoneme segmentation models to find the boundary\nof the segmental structures [11, 33, 61].\n2\n(a) Stage 1: Segmentation model training\n...\n1\n0\n0\n1\n0\n0\n1\n0\n1\n0\n0\n0\n...\n...\nPhoneme Predictor (Generator)¬†üî•\nSegment-wise\nmean-pooling\nDiscriminator¬†üî•\n...\nSegmentation Model¬†‚ùÑÔ∏è\n...¬† r¬†¬†¬†i¬† ¬†…™¬† ¬†n¬† ...\n(b) Stage 2: Phoneme prediction model training\nIterative¬†\nTraining\n...\n1\n0\n0\n1\n0\n0\n1\n0\n1\n0\n0\n0\n...\n...\nPhoneme Predictor (Generator)¬†‚ùÑÔ∏è\nSegment-wise\nmean-pooling\nLanguage model¬†‚ùÑÔ∏è\n...\nSegmentation Model¬†üî•\n...¬† r¬†¬†¬†i¬† ¬†…™¬† ¬†n¬† ...\n,¬†\n1\n0\n1\n0\n0\n1\n1\n0\n1\n0\n1\n0\n...\n...\nPhoneme Predictor (Generator)¬†‚ùÑÔ∏è\nSegmentation Model¬†‚ùÑÔ∏è\nr\ni\ni\nn\nn\nf\n1\n0\n1\n0\n0\n0\n1\n0\n0\n0\n1\n0\nMerged boundary\nOriginal boundary\n(c) Boundary merging\n(a) Stage 1: Segmentation model training\n...\n1\n0\n0\n1\n0\n0\n1\n0\n1\n0\n0\n0\n...\n...\nPhoneme Predictor (Generator)¬†üî•\nSegment-wise\nmean-pooling\nDiscriminator¬†üî•\n...\nSegmentation Model¬†‚ùÑÔ∏è\n...¬† r¬†¬†¬†i¬† ¬†…™¬† ¬†n¬† ...\n(b) Stage 2: Phoneme prediction model training\nIterative¬†\nTraining\n...\n1\n0\n0\n1\n0\n0\n1\n0\n1\n0\n0\n0\n...\n...\nPhoneme Predictor (Generator)¬†‚ùÑÔ∏è\nSegment-wise\nmean-pooling\nLanguage model¬†‚ùÑÔ∏è\n...\nSegmentation Model¬†üî•\n...¬† r¬†¬†¬†i¬† ¬†…™¬† ¬†n¬† ...\n,¬†\n1\n0\n1\n0\n0\n1\n1\n0\n1\n0\n1\n0\n...\n...\nPhoneme Predictor (Generator)¬†‚ùÑÔ∏è\nSegmentation Model¬†‚ùÑÔ∏è\nr\ni\ni\nn\nn\nf\n1\n0\n1\n0\n0\n0\n1\n0\n0\n0\n1\n0\nMerged boundary\nOriginal boundary\n(c) Boundary merging\nFigure 1: (a) and (b): REBORN iterates between using RL to train the segmentation model and\nusing adversarial training to train the phoneme prediction model. (c): An illustration of the seg-\nmentation/boundary merging. 1 means the start of a segment while 0 is not. Given the original\nsegmentation and the predicted phoneme sequence, we merge the segments that result in the same\nphoneme prediction into the same segment, yielding the merged boundary.\nWith the emergence of foundation models, wav2vec-U [4] shows that UASR performance can greatly\nbenefit from using speech foundation models as the feature extractor. However, wav2vec-U still\nrelies on the two-stage feature preprocessing step based on k-means to find the boundaries of the\nsegmental structure in the speech signal. In our paper, we call the segmentation boundaries obtained\nby the feature preprocessing in wav2vec-U as \"k-means-based segmentation\". EURO [18] changes\nthe decoding algorithm used to decode the phoneme prediction sequences and explores other speech\nfoundation models for feature extraction, including HuBERT [23] and WavLM [12].\nIn these prior methods, the segmentation is either non-trainable or learned separately from the\nphoneme prediction model. REBORN differs from prior works by introducing a trainable segmenta-\ntion model tailored for the phoneme prediction model, and the segmentation model and phoneme\nprediction model can be iteratively polished. wav2vec-U 2.0 [30] simplifies the feature preprocessing\nstep in wav2vec-U and does not explicitly consider the segmentation structure in their model. RE-\nBORN has performance better than wav2vec-U 2.0 on almost all datasets, showing that learning the\nsegmentation boundary for feature preprocessing is important for the performance of UASR.\nAlthough recent UASR is originally to be a cross-modality distribution matching problem, it is also\nrelated to representation learning. For example, the speech features might be extracted from self-\nsupervised speech foundation models; and the segmentation problem is also commonly investigated\nin tasks like acoustic unit discovery. We discuss these two related topics further in Appendix D.\n3\nMethod: REBORN\nThe difficulty of mapping speech signals to their corresponding phoneme transcriptions lies in the\nsegmental structures in speech whose boundaries are unknown. To tackle this challenge, we propose\nREBORN, which trains a UASR system with unpaired speech and text data. REBORN contains a\nsegmentation model and a phoneme prediction model. The segmentation model takes the speech\nfeature as input and determines the boundaries of the segmental structures in the speech signal, and\nthe phoneme prediction model predicts a phoneme for each segmental structure. We will use the\nterms segment and segmental structure interchangeably. In our paper, we do not use the term segment\nin the exact sense as in linguistics, where segment refers to discrete units that can be identified in\nthe stream of speech [15]. We use segment to broadly refer to a span in the speech, which may be a\nmeaningful unit (e.g., a phone or word) or a span in the speech identified by the segmentation model.\nThe overall training process of REBORN is outlined in Figure 1. First, we initialize the phoneme\nprediction model using wav2vec-U (Section 3.3). Next, the training of REBORN iterates through\ntwo stages: Stage 1 (Figure 1(a), Section 3.1): Training the segmentation model to learn a better\nsegmentation until the segmentation model converges while fixing the phoneme prediction model.\nStage 2 (Figure 1(b), Section 3.2): Training the phoneme predictor based on the segment boundaries\npredicted by the segmentation model until the phoneme prediction model converges. The iterative\nprocess ceases when the UASR performance does not improve over the previous iteration. In UASR,\n3\nwe cannot use PER as a validation metric to select the best model or determine when to stop training.\nWe use an unsupervised evaluation metric to achieve it instead, detailed in Appendix C.4 and C.5.\n3.1\nStage 1: Training the Segmentation Model\n3.1.1\nSegmentation Model\nGiven an input speech signal, we use a self-supervised feature extractor model to extract the speech\nfeatures X = [x1, ¬∑ ¬∑ ¬∑ , xT ] from the waveform. The segmentation model takes the speech features\nand predicts the boundary of the segmental structure in the speech features. For each feature xi, the\nsegmentation model assigns a binary value ÀÜbi, indicating whether xi is the first frame of a segment.\nWe split X into segments based on ÀÜb = ÀÜb1:T and mean-pool the features within the same segment. The\nmean-pooled features [s1, ¬∑ ¬∑ ¬∑ , sT ‚Ä≤] are forwarded through the phoneme prediction model to obtain\nthe phoneme prediction [y1, ¬∑ ¬∑ ¬∑ , yT ‚Ä≤] using greedy decoding. The resulting phoneme prediction will\nbe de-duplicated into [y‚Ä≤\n1, ¬∑ ¬∑ ¬∑ , y‚Ä≤\nM] by removing the same consecutive phoneme.\nThe segmentation model is a one-dimensional CNN, denoted by œÄŒ∏ and parametrized by Œ∏. Given\n[x1, ¬∑ ¬∑ ¬∑ , xT ], œÄŒ∏ predicts a probability œÄŒ∏[i], ‚àÄi ‚àà[1, ¬∑ ¬∑ ¬∑ , T]. Let ÀÜBi ‚àºBernoulli(œÄŒ∏[i]) be a\nBernoulli random variable representing whether xi is the beginning of a segment. ÀÜBi = 1 means xi is\nthe beginning of a segment; ÀÜBi = 0 means xi is not the beginning of a segment. During training, we\nsample ÀÜbi from Bernoulli(œÄŒ∏[i]); during inference, we take ÀÜbi = 1 if œÄŒ∏[i] ‚â•0.5, otherwise ÀÜbi = 0.\n3.1.2\nTraining the Segmentation Model with RL\nTo help the phoneme prediction model predict phonemes, we want to train the segmentation model\nto capture the segmental structure in speech. However, the optimal segmentation boundary is not\navailable for training. Recognizing that the segmentation quality directly affects the phoneme\nprediction of the phoneme prediction model, we estimate the quality of the phoneme prediction\nto guide the segmentation model training. The phoneme prediction model is from the previous\nREBORN iteration or initialized from wav2vec-U in the first REBORN iteration, and it is fixed when\ntraining the segmentation model. Given that the feature segmentation based on boundary decision is\ninherently non-differentiable, we leverage RL to train the segmentation model. While related works\nmay employ techniques like soft monotonic alignment or straight-through estimator to approximate\ngradients [7, 60], we find that RL provides a natural and suitable approach for our scenario, where the\nestimated quality of the phoneme sequence across different segmentations serves as the RL reward.\nWe train the segmentation model using the policy gradient method [57] based on the REINFORCE\nalgorithm [63]. For each utterance, we calculate an utterance-wise reward R (defined in Eq. 4 in\nthe next subsection) from the de-duplicated phoneme prediction to train the segmentation model.\nBased on the policy gradient method, the segmentation model œÄŒ∏ is optimized using the following\ngradient: EÀÜb‚àºœÄŒ∏\nh\n‚àáŒ∏ log œÄŒ∏(ÀÜb|X)R\ni\n, where E is taken over ÀÜb sampled from œÄŒ∏ and approximated\nwith the mean of a batch of training sequences.\n3.1.3\nReward Designs\nGiven an utterance in the training set, the utterance-wise reward R is the weighted sum of perplexity\ndifference reward Rppl, edit-distance reward Redit, and length difference reward Rlen.\nWe introduce some notations: Given an utterance, we use the currently trained segmentation model\nœÄŒ∏ for segmentation and the phoneme prediction model trained in the last REBORN iteration to obtain\nthe de-duplicated phoneme prediction sequence Y ‚Ä≤\nŒ∏. For the same utterance, we use the segmentation\nmodel from the previous REBORN iteration œÄŒ∏‚àí1 for features segmentation and the same phoneme\nprediction model to obtain another de-duplicated phoneme prediction sequence, denoted as Y ‚Ä≤\nŒ∏‚àí1. In\nthe first REBORN iteration, Y ‚Ä≤\nŒ∏‚àí1 is the de-duplicated phoneme prediction from wav2vec-U.\nPerplexity Difference Reward. The perplexity difference reward is designed to favor phoneme\nsegmentation better than the segmentation learned in the previous iteration. Intuitively, a better\nphoneme segmentation prediction ÀÜb1:T should yield a more reasonable phoneme prediction y‚Ä≤\n1:M. We\nuse perplexity (PPL), the negative likelihood of a phoneme sequence scored by a phoneme language\nmodel (LM), to evaluate how reasonable a phoneme sequence is. Perplexity measures how likely a\n4\nphoneme sequence can be observed in the real world; a phoneme sequence with a lower perplexity\nmeans it is more likely to be observed in real-world phoneme datasets. We use a 4-gram phoneme LM\ntrained on the phonemicized unlabeled text corpora. To guide the segmentation model to generate a\nsegmentation with a better phoneme prediction than the phoneme predictions obtained in the previous\niteration, we define perplexity difference reward as follows:\nRppl = PPLŒ∏‚àí1 ‚àíPPLŒ∏,\n(1)\nwhere PPLŒ∏‚àí1 is the perplexity of Y ‚Ä≤\nŒ∏‚àí1 and PPLŒ∏ is the perplexity of Y ‚Ä≤\nŒ∏. The perplexity difference\nreward guides the segmentation model to produce segmentation that results in a phoneme sequence\nwith lower perplexity comparing with the previous iteration.\nWhen only using Rppl, the segmentation model learns some segmentation method that leads to\nphoneme predictions with lower perplexity but does not correspond to better phoneme prediction\nresults. To prevent such undesirable behaviors, we design two regularization rewards, the edit distance\nreward and length difference reward, to ensure that the policy learned by the segmentation model\ndoes not drastically alter the phoneme predictions between iterations. The two regularization rewards\npunish the segmentation model if Y ‚Ä≤\nŒ∏ is too different from Y ‚Ä≤\nŒ∏‚àí1. Crucially, regularization works only\nif the Y ‚Ä≤\nŒ∏‚àí1 in the first iteration is good enough. As previously mentioned, we ensure this by using the\npredictions from a trained wav2vec-U as Y ‚Ä≤\nŒ∏‚àí1 in the first iteration.\nEdit distance reward. We use Levenshtein distance dLev as the edit distance to calculate the\ndifference between Y ‚Ä≤\nŒ∏‚àí1 and Y ‚Ä≤\nŒ∏. We take the normalized negative edit distance as the reward, which\nis defined in Eq. 2\nLength difference reward. We use the length difference reward Rlen to guide the segmentation\nmodel to predict segmentation such that the length of Y ‚Ä≤\nŒ∏ does not differ significantly from the length\nof Y ‚Ä≤\nŒ∏‚àí1. The length difference reward for an utterance is defined in Eq. 3, where |Y ‚Ä≤| is the length of\nY ‚Ä≤.\nRedit = ‚àídLev(Y ‚Ä≤\nŒ∏‚àí1, Y ‚Ä≤\nŒ∏)\n|Y ‚Ä≤\nŒ∏‚àí1|\n,\n(2)\nRlen = 1 ‚àí\n\f\f|Y ‚Ä≤\nŒ∏| ‚àí|Y ‚Ä≤\nŒ∏‚àí1|\n\f\f\n|Y ‚Ä≤\nŒ∏‚àí1|\n(3)\nThe final utterance-wise reward R is the weighted sum of Rppl, Redit, and Rlen.\nR = cppl ¬∑ Rppl + cedit ¬∑ Redit + clen ¬∑ Rlen,\n(4)\nwhere cppl, cedit and clen are the weighting coefficients. During training, Rppl, Redit and Rlen are\nnormalized within each batch. Appendix C.5 discusses how the coefficients are determined.\n3.1.4\nInitializing œÄŒ∏ with Behavior Cloning\nBefore training the segmentation model œÄŒ∏ with RL, we initialize it with behavior cloning (BC) [47,\n48]. BC uses the supervised objective to train the segmentation model to predict the boundaries in\nspeech features. Given speech features x1:T , the segmentation model is trained to predict the 0/1\nboundary ÀÜbi by using some boundaries bi as the target. In REBORN, the target boundary for BC\nis the merged boundary (Section 3.1.5) obtained using œÄŒ∏‚àí1, the segmentation model learned in\nthe previous iteration. We then frame BC as a binary classification task and optimize it with the\ncross-entropy loss. Note that in the first REBORN iteration, we use the k-means-based boundary from\nwav2vec-U as the BC target.\n3.1.5\nBoundary Merging\nAfter training the segmentation model until convergence, we use the segmentation model to predict\nthe boundaries of the whole dataset and perform boundary merging. The process uses the phoneme\nprediction model trained in the previous iteration to refine the boundary predictions ÀÜbi. Even if some\nconsecutive speech features are split into two segments by the segmentation model, the mean-pooled\nfeatures of the two segments may yield the same phoneme prediction. In this case, the two segments\nwill be merged into one segment. An illustration of boundary merging is shown in Figure 1(c).\nBoundary merging differs from phoneme sequence de-duplication: boundary merging modifies the\nsegmentation prediction ÀÜbi, while de-duplication modifies the phoneme sequence.\n5\n3.2\nStage 2: Training the Phoneme Prediction Model\nThe phoneme predictor takes the mean-pooled features of segmental structures S = [s1, s2, ¬∑ ¬∑ ¬∑ , sT ‚Ä≤]\nand predicts a phoneme sequence Y = [y1, y2, ¬∑ ¬∑ ¬∑ , yT ‚Ä≤]. S are obtained by mean-pooling the features\nX in the same segmental structure based on [ÀÜb\n‚Ä≤\n1, ¬∑ ¬∑ ¬∑ ,ÀÜb\n‚Ä≤\nT ], where ÀÜb\n‚Ä≤\ni is the merged boundaries. We\nfind that it is effective to perform boundary merging to stabilize the training in this stage.\nAfter obtaining the merged phoneme boundaries, we train the phoneme predictor model using GAN\ntraining. The phoneme prediction model is the generator in GAN training. The generator aims to\noutput phoneme predictions that look like real phoneme sequences to fool the discriminator. The\ndiscriminator takes in a phoneme sequence, which can be the output of the generator or a phoneme\nsequence from the unpaired text corpora, and the goal of the discriminator is to distinguish whether\nthe input phoneme sequences are outputs of the generator. The generator and discriminator are\nupdated using the loss in GAN training. In this stage, the parameters of the segmentation model are\nnot updated, and the generator is initialized from the previous iteration. We discuss the effect of\napplying boundary merging and parameter initialization in Appendix A.1\n3.3\nInitialization of REBORN\nIn Stage 1, the segmentation model depends on the phoneme prediction when calculating the rewards.\nAs a result, REBORN cannot work without properly initializing the phoneme prediction model. We\nuse wav2vec-U to train a phoneme prediction model and use it as the initialization for the phoneme\nprediction model in REBORN. We briefly introduce wav2vec-U in Appendix B.\n4\nExperiment Setup\n4.1\nDataset\nWe use three datasets commonly used in ASR to evaluate the performance of REBORN.\nLibriSpeech [45] is an English speech recognition corpora that contains 960 hours of training data.\nFollowing EURO [18], we use 100 hours of audio from the train-clean-100 set as the unlabeled\nspeech data. The unlabeled text data is derived from the remaining 860 hours, which does not overlap\nwith the transcription of the unlabeled speech data.\nTIMIT [19] is another English speech recognition with the human-labeled phone boundary. We\nfollow the matched setting, which is more broadly used [4, 18, 33], where the speech and text data\ncome from the same set of utterances.\nMultilingual LibriSpeech (MLS) [50] is an ASR dataset including German (de), Dutch (nl), French\n(fr), Spanish (es), Italian (it), and Portuguese (pt). Following [4], we randomly sample and use 100\nhours of speech data for each language and use the LM data provided by the dataset as unpaired text.\n4.2\nEvaluation Metrics\nWe use phoneme error rate (PER) and word error rate (WER) to evaluate the ASR performance. If not\nspecified, we use greedy decoding to obtain phoneme-level results. For decoding word-level outputs,\nwe perform WFST decoding [38, 39] using PyKaldi [9]. We leave the details in Appendix C.7.\n4.3\nImplementation Details\nFor the English datasets, we use wav2vec 2.0 [3] to extract speech features from speech signals; for\nMLS, we use XLSR-53 [14] as the feature extractor. Both of them can be found in fairseq [44]. The 4-\ngram phoneme LM is derived using KenLM [22]. We describe more details about the data preparation\nand LM formulation in Appendix C.1, which basically follows wav2vec-U [4]. All the trainable\nmodels, including the segmentation model, the phoneme prediction model, and the discriminator\nin GAN training, are composed of one-dimensional CNN layers, as detailed in Appendix C.3. For\nLibriSpeech and TIMIT, we train REBORN for two iterations. For MLS, we only train one iteration\nsince we do not find the performance to improve in the second iteration.\n6\nTable 1: PER/WER on LibriSpeech using 100 hours\nspeech data. ‚Ä†: Our reproduction. (wav2vec-U and\nwav2vec-U 2.0 only report results of using 960 hours\nof unlabeled speech). HMM ST indicates HMM self-\ntraining (Appendix C.2).\nApproach\nPER/WER (%) ‚Üì\ndev-clean dev-other test-clean test-other\nWITH ORACLE BOUNDARY\nTrain from oracle\n6.3/12.8\n9.7/16.3\n6.4/12.7\n10.0/16.8\nBASELINE\nEURO (HuBERT)\n15.2/23.1 20.7/29.3 15.1/22.8 21.1/29.8\nwav2vec-U‚Ä†\n19.3/20.4 22.9/25.6 19.3/21.0 23.2/25.8\nwav2vec-U 2.0‚Ä†\n12.2/17.2 16.3/21.7 12.6/17.7 16.3/22.2\nwav2vec-U 2.0 + HMM ST‚Ä† 10.0/15.4 13.1/19.0 10.3/16.0 13.1/19.6\nOUR METHOD\nREBORN\n8.3/12.5\n11.9/17.6\n8.9/13.1\n12.5/18.7\nREBORN + HMM ST\n5.2/9.3\n8.5/13.5\n5.4/9.6\n8.5/13.7\nTable 2: PER results on TIMIT. The cross-\nmark (‚úó) in the greedy-decoding column indi-\ncates that an additional LM (4-gram) is used\nduring decoding. REBORN reaches the best\nperformance with no LM used for decoding,\nshowing that REBORN can benefit from the\nexternal LM via RL.\nApproach\nGreedy\ndecoding\nPER (%)\ncore-dev core-test all-test\n(a) Train from oracle\n‚úì\n9.1\n10.3\n9.2\n(b) wav2vec-U (reproduced)\n‚úì\n20.2\n22.2\n20.3\n(c) wav2vec-U+WFST\n‚úó\n17.1\n17.8\n16.8\n(d) EURO (wav2vec 2.0)\n‚úó\n18.5\n19.8\n-\n(e) EURO (WavLM)\n‚úó\n14.3\n14.6\n-\n(f) REBORN\n‚úì\n12.4\n13.5\n12.4\n5\nResults\n5.1\nMain Results\nWe show the PER and WER of LibriSpeech, TIMIT, and MLS in Table 1, Table 2, and Table 3.\nWe compare REBORN with several prior UASR works: wav2vec-U [4], wav2vec-U 2.0 [30], and\nEURO [18]. We have the following observations:\nREBORN significantly outperforms all prior methods on LibriSpeech. Our experiment on\nLibriSpeech follows Gao et al. [18] to use the 100-hour training split of LibriSpeech as the unlabeled\nspeech data. Compared with all the baselines in Table 1, REBORN achieves the lowest PER and WER.\nwav2vec-U and EURO both use hand-crafted rules to obtain the segmental boundaries, while wav2vec-\nU 2.0 removes the feature segmentation steps. The superior performance of REBORN illustrates\nthe importance of learning the segmental boundaries tailored for the phoneme prediction model.\nNotably, without using HMM self-training, REBORN already has PER/WER lower than wav2vec-U\n2.0 with HMM self-training, which is the prior state-of-the-art (SoTA) method on LibriSpeech.\nIn Appendix A.4, we further apply the REBORN pipeline with speech foundation models other\nthan wav2vec 2.0. We find that REBORN yields notable performance improvement when using\nHuBERT [23] or WavLM [12] as the feature extractor, illustrating the generalizability of our method.\nSelf-training further improves PER/WER of REBORN. Self-training uses the phoneme predictor‚Äôs\nprediction as the pseudo-label and trains a new phoneme prediction model using the pseudo-label as\nthe training data. It is commonly used in UASR to boost the performance [4, 30]. In Table 1, we show\nthat REBORN can be integrated with Hidden Markov Models (HMM) self-training (Appendix C.2)\nto further lower the PER/WER. We reach a new SoTA on LibriSpeech under the setting of 100-\nhour speech data, outperforming the prior SoTA in PER and WER by 5% and 6%, respectively.\nSurprisingly, REBORN with HMM self-training outperforms training wav2vec-U with the oracle\nboundary. This shows that REBORN can be combined with self-training to improve performance\neffectively. Due to limited computation resources, we only conduct self-training on LibriSpeech.\nREBORN outperforms all prior UASR methods on TIMIT. Table 2 shows the PER of TIMIT,\nand we again find that REBORN achieves the lowest PER compared with all prior UASR methods.\nThis indicates that REBORN not only works on large datasets like LibriSpeech but also works on\nsmall datasets like TIMIT, which only contains about 3 hours of audio data for training. Even using\ngreedy decoding only, REBORN outperforms the prior best-performing UASR model (row (e)), which\nrelies on prefix decoding. EURO shows that replacing the feature extractor with WavLM [12] (row\n(e)) outperforms using wav2vec 2.0 (row (d)) as the speech feature extractor. Since REBORN using\nwav2vec 2.0 already outperforms EURO with WavLM, we leave changing the feature extractor in\nREBORN on TIMIT as future work.\n7\nTable 3: WER on MLS. ‚Ä†: Results from Baevski\net al. [4]. ‚àó: Our reproduction of wav2vec-U, used\nas the initialization of the phoneme prediction\nmodel in REBORN. ‚Ä°: Results from Liu et al.\n[30].\nApproach\nWER (%) ‚Üì\nde\nnl\nfr\nes\nit\npt\nAvg.\nwav2vec-U‚Ä†\n32.5 40.2 39.8 33.3 58.1 59.8 44.0\nwav2vec-U‚àó\n33.9 38.1 37.7 33.1 51.8 59.4 42.3\nwav2vec-U 2.0‚Ä° 23.5 35.1 35.7 25.8 46.9 48.5 35.9\nREBORN\n20.9 26.9 28.2 24.7 39.9 51.5 32.0\nTable 4: Boundary evaluation results of different\nsegmentation methods on LibriSpeech test-clean\nsplit. The second-last column (Freq.) is the num-\nber of segments per second. All the methods share\nthe same phoneme prediction model trained with\nthe k-means-based segmentation of wav2vec-U.\nBoundary method\nPrec. Rec. F1\nFreq. PER%\n(a) Oracle\n1.0\n1.0\n1.0 11.89\n13.8\n(b) k-means-based\n0.64 0.77 0.70 14.27\n19.3\n(c) Strgar and Harwath [56] 0.75 0.75 0.75 12.26\n23.2\n(d) REBORN\n0.57 0.78 0.65 16.29\n12.9\nREBORN performs well on languages other than English. The results on MLS are presented in\nTable 3. We find out that a single iteration is enough for performance convergence on MLS. Recall\nthat REBORN is initialized from wav2vec-U, but REBORN outperforms wav2vec-U by a large margin\nin all six languages. This large performance gap underlines the importance of learning the boundaries\nof the segmental boundaries. Moreover, REBORN outperforms wav2vec-U 2.0 except for Portuguese,\nand the average WER on the six languages is 3.9% lower than wav2vec-U 2.0.\n5.2\nBoundary Analysis\nThe core of REBORN is the segmentation model that learns the segmental structure‚Äôs boundaries.\nIn this section, we look at how different segmental boundaries affect the performance of phoneme\nprediction. We compare the boundaries obtained by four methods: (a) the oracle phoneme boundaries\nobtained by forced alignment [36], which requires paired speech-text data to learn the alignment; (b)\nthe k-means-based segmentation boundary in wav2vec-U; (c) the phoneme boundary obtained by the\nSoTA unsupervised phoneme segmentation method [56] on LibriSpeech; (d) the boundaries learned\nby the segmentation model of REBORN in the first iteration before boundary merging.\nFirst, we focus on the PER in Table 4 when using different segmentation methods. The PER is\nobtained by taking the four different segmented features to the identical phoneme prediction model:\nthe phoneme prediction model trained using the k-means-based segmentation of wav2vec-U. Note\nthat the phoneme prediction model here is unsupervised and non-ideal. We find that simply replacing\nthe k-means-based boundary with the oracle phoneme boundary reduces the PER by 5.5%, showing\nthe imperfect hand-crafted k-means-based segmentation is the bottleneck of UASR. Next, even when\nusing the boundary predicted by the SoTA unsupervised phoneme segmentation [56], the PER does\nnot reduce. Conversely, the boundary learned by REBORN achieves the lowest PER. This is because\nthe boundary learned by REBORN is directly tailored for the phoneme prediction model.\nNext, we discuss the phoneme boundary results in Table 4. The phoneme boundary results is evaluated\nwith boundary precision, recall, and F1 with a 20ms tolerance window, following the harsh scheme\nfrom Strgar and Harwath [56]. We leave the discussion about the different schemes for boundary\nevaluation in Appendix E. We also report the average number of segments per second (Freq. in\nTable 4). Interestingly, the boundary F1s of the oracle boundary (row (a)) and Strgar and Harwath\n[56] (row (c)) are much higher than the boundary F1 of REBORN (row (d)), but the PER of REBORN\nis much better. This is because REBORN‚Äôs segmentation model predicts more segments than the\nnumber of phonemes in the utterance (before boundary merging), indicating that it learns some\nsegmental structures smaller than the phones. This can be seen from the lower precision (0.57) and\nhigher frequency (16.29) compared to oracle boundaries. However, segmenting the speech feature\ninto smaller units is not problematic because even if the consecutive speech features of the same\nphonemes are split into two segments by the segmentation model, as long as they have the same\nphoneme prediction, the duplicated phoneme prediction will be removed in the de-duplication step,\nand thus not affect the PER.\n8\nTable 5: We compare different reward functions\nand the effect of BC initialization with first itera-\ntion results on LibriSpeech test-clean. Row (c)\nis REBORN.\nAblation\nPER (%)\nLM PPL\n(Stage 1)\nStage 1\nStage 2\n(a). Rppl\n14.7\n12.9\n10.0\n(b). Rppl + Redit\n13.8\n12.1\n10.8\n(c). Rppl + Redit + Rlen\n12.9\n11.9\n11.2\n(d). REBORN w/o BC\n14.9\n14.2\n13.8\nFigure 2: PER across training epochs on the\ntest-clean split of LibriSpeech. BC pretraining\nspeeds up convergence and raises performance.\n0\n8\n16\n24\n32\n40\nEpoch\n15\n20\n25\n30\n35\nPER (%)\nBC pretrained\nfrom scratch\n5.3\nAblation Study\n5.3.1\nReward Design and Behavior Cloning\nWe verify the effectiveness of our three proposed reward functions and behavior cloning initialization\nin Table 5. While the perplexity difference reward alone is sufficient to guide the segmentation model,\nadding edit distance and length difference rewards improves performance further. As previously\nmentioned, using only the perplexity difference reward will make the segmentation model predict\nsegments overly focusing on lowering PPL. And a phoneme sequence with a lower perplexity\nmay not always be a better transcription. To deal with such overfitting problem, we design the\ntwo regularization rewards, namely, Redit and Rlen, to make the model learn to segment for lower\nperplexity while grounded on a reasonable transcription from the previous iteration. Our results in\nTable 5 further evidence the effectiveness of the two rewards. Additionally, we observe that removing\nBC leads to a decline in PER compared to using BC (see row (d) vs. row (c)). To deepen this analysis,\nwe present the PER curve on the LibriSpeech test-clean set with and without BC. The results in\nFigure 2 indicate that BC helps enhance performance and accelerate convergence.\n5.3.2\nIterative Training\nIn Figure 3, we show the PER on LibriSpeech test-clean split during the iterative training process\nof REBORN. We observe that after training the segmentation model in the first iteration, the PER\ndrastically drops by 6.4% compared with wav2vec-U, which is used to initialize the phoneme\nprediction model in iteration one. This shows that the quality of the segmental structure‚Äôs boundaries\nis important to the performance of UASR. Afterward, the PER of REBORN decreases steadily across\niterative training, showing that training the segmentation model and the phoneme prediction model is\ncritical to improving the performance of REBORN. We find that the PER does not drop significantly\nafter the fifth iteration. Last but not least, we provide more evidence in Appendix A.3 to showcase\nthat the phoneme predictors are iteratively polished.\nFigure 3: PER of each stage during REBORN‚Äôs two-\nstage iterative training on the test-clean split of Lib-\nriSpeech. St.: stage; w2vu: wav2vec-U.\nInit. (w2vu)\nIter-1 (St. 1)\nIter-1 (St. 2)\nIter-2 (St. 1)\nIter-2 (St. 2)\nIter-3 (St. 1)\nIter-3 (St. 2)\nIter-4 (St. 1)\nIter-4 (St. 2)\nIter-5 (St. 1)\nIter-5 (St. 2)\n9\n12\n15\n18\nPER (%)\nReborn\nEURO\nwav2vec-U\nwav2vec-U 2.0\nTable 6: The boundary evaluation results\non TIMIT. REBORN achieves better bound-\nary evaluation scores than the original k-\nmeans-based method, and is comparable\nwith Strgar and Harwath [56]‚Äôs after bound-\nary merging. ‚àó: from literature. All the\nmetrics are calculated on the test-all split.\nBoundary method\nPrec. Rec. F1 R-val. PER%\nk-means-based\n0.62 0.75 0.68 0.68\n20.3\nStrgar and Harwath [56]‚àó0.85 0.79 0.82 0.84\n-\nREBORN\n0.61 0.83 0.71 0.62\n12.4\n+ boundary merging\n0.80 0.78 0.79 0.82\n-\n9\n5.3.3\nBoundary Evaluation on TIMIT\nIn Section 5.2, we demonstrate that REBORN segmentation delivers fine-grained segments, achieving\nthe highest performance gain among segmentation methods given the same non-ideal phoneme\nprediction model. Here, we extend our boundary evaluation results to TIMIT, a smaller dataset with\nhuman-annotated phone boundaries, to provide more comprehensive boundary insights. Table 6\nshows that the initial boundaries learned by REBORN, optimized explicitly for the phoneme prediction\nmodel, already achieve a high recall. With boundary merging, i.e., consecutive segments with the\nsame phoneme prediction are merged as illustrated in Figure 1-(c), REBORN achieves results close to\nStrgar and Harwath [56]‚Äôs. In line with the previous context, we emphasize that REBORN achieves\nsubstantial performance improvement in PER by tailoring segmentation to the non-ideal unsupervised\nphoneme prediction model, rather than solely aiming for higher boundary evaluation scores. This\nrepresents a critical and intriguing finding in our work.\n6\nDiscussion\nIn unsupervised ASR, learning the mapping between the segmental structures in speech and their\ncorresponding transcription is challenging, especially without paired data. To tackle this, we pro-\npose REBORN, an iterative training method for unsupervised ASR. REBORN iteratively trains the\nsegmentation model and the phoneme prediction model to improve the ASR performance. The\nsegmentation model is trained using RL with carefully designed rewards to guide the segmentation\nmodel to find the boundaries of the segmental structures in speech signals. Extensive experiments\non three datasets spanning seven languages show that REBORN outperforms prior best-performing\nunsupervised ASR models in all datasets except one language in MLS. We conduct comprehensive\nablation studies to show that each component in REBORN is critical to the performance. We also\nexplain the effectiveness of REBORN by analyzing its segmentation patterns and find that REBORN\ntends to produce segmental structures smaller than phones, which helps the generators predict the\nphoneme transcription better.\nLimitations. Recent advancements in unsupervised ASR are still in the developmental stages, and\nwe have not yet implemented REBORN in more realistic scenarios, such as low-resource languages or\nnoisy environments. Additionally, the iterative training paradigm may amplify any existing biases in\nthe dataset, an aspect that remains unexplored in this study. Furthermore, if the phoneme prediction\nmodel is poorly initialized, REBORN may not be able to provide huge performance improvements.\nIn this work, lexicons are required to perform phonemicization on the unpaired text. Consequently,\nlearning the unsupervised ASR system directly from speech to word-level tokens remains an open\nproblem, where recent works aim to tackle [30, 41].\nBroader impacts. Unsupervised automatic speech recognition (UASR) - predicting the textual\ntranscriptions for the given speech signals using only unpaired speech and text data - is highly\nattractive. The existence of thousands of low-resourced languages spoken globally is the major\nreason. This paper presents REBORN, a novel unsupervised ASR training algorithm. We foresee that\nREBORN has great potential to make low-resource languages more accessible to train ASR systems,\nmaking speech technology more accessible and boundaryless.\nAcknowledgment\nWe thank the National Center for High-performance Computing (NCHC) of the National Applied\nResearch Laboratories (NARLabs) in Taiwan for providing computational and storage resources.\nShao-Hua Sun was supported by the Yushan Fellow Program by the Ministry of Education, Taiwan.\nWe also extend our gratitude to Alexei Baevski, Wei-Ning Hsu, and Alexander H. Liu, authors of\nwav2vec-U and wav2vec-U 2.0, for providing valuable insights into their work.\nReferences\n[1] Mikel Artetxe, Gorka Labaka, Eneko Agirre, and Kyunghyun Cho. Unsupervised neural\nmachine translation. In International Conference on Learning Representations, 2018.\n10\n[2] Alexei Baevski, Steffen Schneider, and Michael Auli. vq-wav2vec: Self-supervised learning\nof discrete speech representations. Proceedings of Inter national Conference on Learning\nRepresentations, 2020.\n[3] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: A\nframework for self-supervised learning of speech representations. In Advances in Neural\nInformation Processing Systems, 2020.\n[4] Alexei Baevski, Wei-Ning Hsu, Alexis Conneau, and Michael Auli. Unsupervised speech\nrecognition. Advances in Neural Information Processing Systems, 2021.\n[5] Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, and Michael Auli.\nData2vec: A general framework for self-supervised learning in speech, vision and language. In\nInternational Conference on Machine Learning, 2022.\n[6] Mathieu Bernard and Hadrien Titeux. Phonemizer: Text to phones transcription for multiple\nlanguages in python. Journal of Open Source Software, 2021.\n[7] Saurabhchand Bhati, Jes√∫s Villalba, Piotr ÀôZelasko, Laureano Moro-Vel√°zquez, and Najim\nDehak. Segmental Contrastive Predictive Coding for Unsupervised Word Segmentation. In\nInterspeech, 2021.\n[8] LFM ten Bosch, X Wang, and LCW Pols. Duration modeling with hidden markov models. The\nJournal of the Acoustical Society of America, 1993.\n[9] Dogan Can, Victor R Martinez, Pavlos Papadopoulos, and Shrikanth S Narayanan. Pykaldi: A\npython wrapper for kaldi. In IEEE International Conference on Acoustics, Speech and Signal\nProcessing, 2018.\n[10] Guoguo Chen, Shuzhou Chai, Guan-Bo Wang, Jiayu Du, Wei-Qiang Zhang, Chao Weng, Dan\nSu, Daniel Povey, Jan Trmal, Junbo Zhang, Mingjie Jin, Sanjeev Khudanpur, Shinji Watanabe,\nShuaijiang Zhao, Wei Zou, Xiangang Li, Xuchen Yao, Yongqing Wang, Zhao You, and Zhiyong\nYan. Gigaspeech: An evolving, multi-domain ASR corpus with 10, 000 hours of transcribed\naudio. In Interspeech, 2021.\n[11] Kuan-Yu Chen, Che-Ping Tsai, Da-Rong Liu, Hung-Yi Lee, and Lin-Shan Lee. Completely\nUnsupervised Phoneme Recognition by a Generative Adversarial Network Harmonized with\nIteratively Refined Hidden Markov Models. In Interspeech, 2019.\n[12] Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li,\nNaoyuki Kanda, Takuya Yoshioka, Xiong Xiao, et al. Wavlm: Large-scale self-supervised pre-\ntraining for full stack speech processing. IEEE Journal of Selected Topics in Signal Processing,\n2022.\n[13] Chung-Cheng Chiu, James Qin, Yu Zhang, Jiahui Yu, and Yonghui Wu. Self-supervised\nlearning with random-projection quantizer for speech recognition. In International Conference\non Machine Learning, 2022.\n[14] Alexis Conneau, Alexei Baevski, Ronan Collobert, Abdelrahman Mohamed, and Michael Auli.\nUnsupervised Cross-Lingual Representation Learning for Speech Recognition. In Interspeech,\n2021.\n[15] David Crystal. A dictionary of linguistics and phonetics. John Wiley & Sons, 2011.\n[16] Santiago Cuervo, Adrian Lancucki, Ricard Marxer, Pawe≈Ç Rychlikowski, and Jan K Chorowski.\nVariable-rate hierarchical cpc leads to acoustic unit discovery in speech. Advances in Neural\nInformation Processing Systems, 35:34995‚Äì35006, 2022.\n[17] Siyuan Feng, Piotr ÀôZelasko, Laureano Moro-Vel√°zquez, and Odette Scharenborg. Unsupervised\nAcoustic Unit Discovery by Leveraging a Language-Independent Subword Discriminative\nFeature Representation. In Proc. Interspeech 2021, 2021.\n[18] Dongji Gao, Jiatong Shi, Shun-Po Chuang, Leibny Paola Garcia, Hung-yi Lee, Shinji Watan-\nabe, and Sanjeev Khudanpur. Euro: Espnet unsupervised asr open-source toolkit. In IEEE\nInternational Conference on Acoustics, Speech and Signal Processing, 2023.\n11\n[19] John S Garofolo. Timit acoustic phonetic continuous speech corpus. Linguistic Data Consortium,\n1993, 1993.\n[20] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil\nOzair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in Neural\nInformation Processing Systems, 2014.\n[21] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville.\nImproved training of wasserstein gans. Advances in Neural Information Processing Systems,\n2017.\n[22] Kenneth Heafield. Kenlm: Faster and smaller language model queries. In Proceedings of the\nSixth Workshop on Statistical Machine Translation, 2011.\n[23] Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov,\nand Abdelrahman Mohamed.\nHubert: Self-supervised speech representation learning by\nmasked prediction of hidden units. IEEE/ACM Transactions on Audio, Speech, and Language\nProcessing, 2021.\n[24] J. Kahn, M. Rivi√®re, W. Zheng, E. Kharitonov, Q. Xu, P.E. Mazar√©, J. Karadayi, V. Liptchin-\nsky, R. Collobert, C. Fuegen, T. Likhomanenko, G. Synnaeve, A. Joulin, A. Mohamed, and\nE. Dupoux. Libri-light: A benchmark for asr with limited or no supervision. In IEEE Interna-\ntional Conference on Acoustics, Speech and Signal Processing, 2020.\n[25] Jacob Kahn, Ann Lee, and Awni Hannun. Self-training for end-to-end speech recognition. In\nICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing\n(ICASSP), 2020.\n[26] Guillaume Lample, Alexis Conneau, Ludovic Denoyer, and Marc‚ÄôAurelio Ranzato. Unsuper-\nvised machine translation using monolingual corpora only. In International Conference on\nLearning Representations, 2018.\n[27] Guillaume Lample, Alexis Conneau, Marc‚ÄôAurelio Ranzato, Ludovic Denoyer, and Herv√© J√©gou.\nWord translation without parallel data. In International Conference on Learning Representations,\n2018.\n[28] Chia-ying Lee and James Glass. A nonparametric bayesian approach to acoustic model discovery.\nIn Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics,\n2012.\n[29] Shaoshi Ling, Yuzong Liu, Julian Salazar, and Katrin Kirchhoff. Deep contextualized acoustic\nrepresentations for semi-supervised speech recognition. In ICASSP 2020-2020 IEEE Interna-\ntional Conference on Acoustics, Speech and Signal Processing (ICASSP), 2020.\n[30] Alexander H. Liu, Wei-Ning Hsu, Michael Auli, and Alexei Baevski. Towards end-to-end\nunsupervised speech recognition. In IEEE Spoken Language Technology Workshop, 2022.\n[31] Andy T Liu, Shu-wen Yang, Po-Han Chi, Po-chun Hsu, and Hung-yi Lee. Mockingjay:\nUnsupervised speech representation learning with deep bidirectional transformer encoders. In\nICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing\n(ICASSP), 2020.\n[32] Andy T Liu, Shang-Wen Li, and Hung-yi Lee. Tera: Self-supervised learning of transformer\nencoder representation for speech. IEEE/ACM Transactions on Audio, Speech, and Language\nProcessing, 2021.\n[33] Da-Rong Liu, Kuan-Yu Chen, Hung-Yi Lee, and Lin-Shan Lee. Completely Unsupervised\nPhoneme Recognition by Adversarially Learning Mapping Relationships from Audio Embed-\ndings. In Interspeech, 2018.\n[34] Yu Liu, Jianshu Chen, and Li Deng. Unsupervised sequence classification using sequential\noutput statistics. Advances in Neural Information Processing Systems, 2017.\n12\n[35] Vimal Manohar, Hossein Hadian, Daniel Povey, and Sanjeev Khudanpur. Semi-supervised\ntraining of acoustic models using lattice-free mmi. In 2018 IEEE international conference on\nacoustics, speech and signal processing (ICASSP), 2018.\n[36] Michael McAuliffe, Michaela Socolof, Sarah Mihuc, Michael Wagner, and Morgan Sonderegger.\nMontreal forced aligner: Trainable text-speech alignment using kaldi. Interspeech, 2017.\n[37] Abdelrahman Mohamed, Hung-yi Lee, Lasse Borgholt, Jakob D Havtorn, Joakim Edin, Christian\nIgel, Katrin Kirchhoff, Shang-Wen Li, Karen Livescu, Lars Maal√∏e, et al. Self-supervised\nspeech representation learning: A review. IEEE Journal of Selected Topics in Signal Processing,\n2022.\n[38] Mehryar Mohri. Finite-state transducers in language and speech processing. Computational\nLinguistics, 1997.\n[39] Mehryar Mohri, Fernando Pereira, and Michael Riley. Weighted finite-state transducers in\nspeech recognition. Computer Speech & Language, 2002.\n[40] Tu Anh Nguyen, Maureen de Seyssel, Patricia Roz√©, Morgane Rivi√®re, Evgeny Kharitonov,\nAlexei Baevski, Ewan Dunbar, and Emmanuel Dupoux. The zero resource speech benchmark\n2021: Metrics and baselines for unsupervised spoken language modeling. arXiv preprint\narXiv:2011.11588, 2020.\n[41] Junrui Ni, Liming Wang, Yang Zhang, Kaizhi Qian, Heting Gao, Mark Hasegawa-Johnson, and\nChang D Yoo. Towards unsupervised speech recognition without pronunciation models. arXiv\npreprint arXiv:2406.08380, 2024.\n[42] Lucas Ondel, Luk√°≈° Burget, and Jan ÀáCernock`y. Variational inference for acoustic unit discovery.\nProcedia Computer Science, 2016.\n[43] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive\npredictive coding. arXiv preprint arXiv:1807.03748, 2018.\n[44] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier,\nand Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of\nNAACL-HLT 2019: Demonstrations, 2019.\n[45] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: an asr\ncorpus based on public domain audio books. In IEEE International Conference on Acoustics,\nSpeech and Signal Processing, 2015.\n[46] Jongseok Park, Kyubyong & Kim. g2pe. https://github.com/Kyubyong/g2p, 2019.\n[47] Dean A. Pomerleau. Alvinn: An autonomous land vehicle in a neural network. In Advances in\nNeural Information Processing Systems, 1988.\n[48] Dean A. Pomerleau. Efficient training of artificial neural networks for autonomous navigation.\nNeural Computation, 1991.\n[49] Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Lukas Burget, Ondrej Glembek, Nagendra\nGoel, Mirko Hannemann, Petr Motlicek, Yanmin Qian, Petr Schwarz, et al. The kaldi speech\nrecognition toolkit. In IEEE 2011 workshop on automatic speech recognition and understanding,\n2011.\n[50] Vineel Pratap, Qiantong Xu, Anuroop Sriram, Gabriel Synnaeve, and Ronan Collobert. MLS:\nA Large-Scale Multilingual Dataset for Speech Research. In Interspeech, 2020.\n[51] Vineel Pratap, Andros Tjandra, Bowen Shi, Paden Tomasello, Arun Babu, Sayani Kundu, Ali\nElkahky, Zhaoheng Ni, Apoorv Vyas, Maryam Fazel-Zarandi, et al. Scaling speech technology\nto 1,000+ languages. arXiv preprint, 2023.\n[52] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya\nSutskever. Robust speech recognition via large-scale weak supervision. In International\nConference on Machine Learning, 2023.\n13\n[53] Thomas Schatz, Vijayaditya Peddinti, Francis Bach, Aren Jansen, Hynek Hermansky, and\nEmmanuel Dupoux. Evaluating speech features with the minimal-pair abx task: Analysis\nof the classical mfc/plp pipeline. In INTERSPEECH 2013: 14th Annual Conference of the\nInternational Speech Communication Association, 2013.\n[54] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region\npolicy optimization. In Proceedings of the 32nd International Conference on Machine Learning,\n2015.\n[55] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal\npolicy optimization algorithms. CoRR, 2017.\n[56] Luke Strgar and David Harwath. Phoneme segmentation using self-supervised speech models.\nIn IEEE Spoken Language Technology Workshop, 2022.\n[57] Richard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient meth-\nods for reinforcement learning with function approximation. Advances in Neural Information\nProcessing Systems, 1999.\n[58] Zheng-Hua Tan, Najim Dehak, et al. rvad: An unsupervised segment-based robust voice activity\ndetection method. Computer speech & language, 2020.\n[59] Karel Vesel√Ω, Luk√°≈° Burget, and Jan ÀáCernock√Ω. Semi-Supervised DNN Training with Word\nSelection for ASR. In Proc. Interspeech 2017, 2017.\n[60] Liming Wang, Mark Hasegawa-Johnson, and Chang D Yoo. Unsupervised speech recognition\nwith n-skipgram and positional unigram matching. In IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP), 2024.\n[61] Yu-Hsuan Wang, Hung-yi Lee, and Lin-shan Lee. Segmental audio word2vec: Representing\nutterances as sequences of vectors with applications in spoken term detection. In 2018 IEEE\nInternational Conference on Acoustics, Speech and Signal Processing (ICASSP), 2018.\n[62] Shu wen Yang, Po-Han Chi, Yung-Sung Chuang, Cheng-I Jeff Lai, Kushal Lakhotia, Yist Y. Lin,\nAndy T. Liu, Jiatong Shi, Xuankai Chang, Guan-Ting Lin, Tzu-Hsien Huang, Wei-Cheng Tseng,\nKo tik Lee, Da-Rong Liu, Zili Huang, Shuyan Dong, Shang-Wen Li, Shinji Watanabe, Abdel-\nrahman Mohamed, and Hung yi Lee. SUPERB: Speech Processing Universal PERformance\nBenchmark. In Proc. Interspeech 2021, 2021.\n[63] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforce-\nment learning. Machine learning, 1992.\n[64] Chih-Kuan Yeh, Jianshu Chen, Chengzhu Yu, and Dong Yu. Unsupervised speech recognition\nvia segmental empirical output distribution matching. In International Conference on Learning\nRepresentations, 2018.\n14\nAppendix\nA\nAdditional Results\nA.1\nStability Analysis\nIn unsupervised ASR, stability has always been one of the major concerns since the unlabeled\nscenario makes the problem difficult and the training unstable. In Table 7, we demonstrate that\nREBORN is stable by calculating the mean PER and the corresponding standard deviations, as well\nas the converged rate. We introduce that a converged experimental run is categorized by its result\nyielding PER < 40%, following Baevski et al. [4]. As represented in the table, REBORN generates\nsignificantly better performance while being stable with a 100% converged rate in both stages.\nNext, we focus on the ablations of REBORN stage 2 GAN training in Table 7. When the boundary\nmerging is not applied, the GAN training on the original REBORN-segmented features is still better\nthan the initial stage. However, the average PER and the standard deviation are larger. The observation\nindicates that the training is slightly unstable, but does not diverge too much since it is still fully\nconverged. On the other hand, when we train the next-iteration GAN with completely random\ninitialized parameters, the performance degrades even if we train the model for more steps. These\ntwo ablations directly demonstrate the effectiveness of adopting boundary merging and parameter\ninitialization from the previous iteration, while the GAN training can still work on the original\nboundary or with randomly initialized parameters.\nFurthermore, we find out that the instability might be highly correlated to the boundary frequency\n(the last column in Table 7), and this observation also corresponds with the prior works [4, 30]. Just\nas we performed in Table 1, we show the topline result of training GAN from the oracle boundary\nin the last row. We contribute the good result to the disappearance of the length mismatch issue\nbetween the segmented feature and its textual transcription. In the INITAIL STAGE, when we use\nonly the k-means for segmentation instead of the two-stage k-means + adjacent segmentation (or\n\"k-means-based segmentation\" in the main text) in wav2vec-U, the length mismatch issue arises and\nwe can not obtain reasonable results.\nAs for REBORN, the boundary without merging has a higher frequency compared with the wav2vec-U\nsegmentation. However, using the original boundary for GAN training can still yield fully converged\nresults with lower PER. We contribute this result to the improved quality of REBORN boundary,\nshowing the importance and effectiveness of our segmentation learning via RL.\nTable 7: The table for stability analysis. Each method is trained for 5 runs on LibriSpeech and\nevaluated on the test-clean split. REBORN performs steadily well while being fully converged in both\nof the stages. The ablations show the effectiveness of applying boundary merging and parameter\ninitialization from the previous iteration for the stage 2 GAN training. The abbreviation \"adj.\"\nindicates adjacent pooling, which is the 2nd stage of pooling in wav2vec-U (see Appendix B). The\n\"Freq.\" in the last column is the number of boundaries per second.\nAblation\nType\nmean PER ¬± std (%)\n%-converged (‚Üë)\nFreq. (Hz)\nINITIAL STAGE\nwav2vec-U (k-means-only)\nGAN\n> 100\n0%\n28.5\nwav2vec-U (k-means + adj.)\nGAN\n20.1 ¬± 0.6\n100%\n14.3\nITERATION 1\nREBORN stage 1\nRL\n12.9 ¬± 0.7\n100%\n16.3\nREBORN stage 2\nGAN\n12.0 ¬± 0.1\n100%\n11.6\nw/o boundary merging\nGAN\n13.9 ¬± 2.1\n100%\n16.3\nw/ random initialization\nGAN\n14.1 ¬± 0.4\n100%\n11.6\nTOPLINE\nTrain from oracle boundary\nGAN\n6.64 ¬± 0.2\n100%\n11.9\n15\nTable 8: Ablation study results of various configurations on the unsupervised metric assessed using\nvalidation set and phoneme error rates evaluated on the LibriSpeech dataset.\nConfiguration\nStage 1\nStage 2\nUnsupervised\nDev-clean\nTest-clean\nUnsupervised\nDev-clean\nTest-clean\nMetric (‚Üì)\nPER (%)\nPER (%)\nMetric (‚Üì)\nPER (%)\nPER (%)\nRppl\n256621.6\n14.0\n14.7\n206312.79\n12.2\n12.9\nRppl + Redit\n251476.6\n13.3\n13.8\n198447.93\n11.7\n12.1\nRppl + Redit + Rlen\n243419.4\n12.7\n12.9\n191043.37\n11.7\n11.9\nREBORN w/o BC\n252302.3\n14.8\n14.9\n237658.23\n14.1\n14.2\nA.2\nReward Design Details\nWhen training reinforcement learning, various methods have demonstrated that incorporating regu-\nlarization terms can enhance model training stability [54, 55]. In response to this insight, we have\nintegrated both edit-distance and length rewards as forms of regularization in our training regime.\nTo identify the most effective reward configuration, we employed an unsupervised metric (Eq. 10)\nfor selection. Detailed unsupervised metric scores and further evaluation results of our reward\nconfigurations are presented in Table 8.\nOur findings, as shown in Table 5, indicate that using perplexity (PPL) alone as a reward yields the\nlowest perplexity scores upon model convergence. However, this approach also results in higher\nscores on the unsupervised metric, implying a decline in model performance. Further analysis\nreveals that the PER increases when the PPL reward is the sole metric. This suggests that, without\nregularization, the model tends to minimize perplexity by producing more lengthy and less accurate\ntranscriptions, which do not align well with the expected outputs.\nA.3\nIteratively Polished Phoneme Predictors\nIn REBORN, we introduce an iterative paradigm that can gradually polish the segmentation model and\nthe phoneme predictor in turn. In Table 9, we wish to provide more evidence to directly demonstrate\nthat the phoneme predictors are actually improved through the iterations. We carry this out by feeding\nthe identical oracle-segmented features to the phoneme predictors in different stages. Given the same\nideally-segmented features, a better phoneme predictor should yield a better performance as well. In\nTable 9, we show that the phoneme predictor in the initial stage gives 13.8% PER when evaluating\non the oracle-segmented features. REBORN‚Äôs iterative learning paradigm gradually improves the\nphoneme predictor, and the performance finally achieves the best in the 4th iteration by 7.4% PER.\nThe relative performance gain compared with the initial stage is over 45%, as presented in the table.\nThe result is even 1% close to the topline listed in the bottom row, indicating the effectiveness of\nREBORN‚Äôs iterative training.\nTable 9: We demonstrate that the REBORN phoneme predictors are gradually improved though our\niterative training. Each of the phoneme predictors takes the same oracle-segmented features as input.\nPhoneme Predictor\nPER (%)\nRelative\n(evaluate on oracle)\nGain (‚Üë)\nINITIAL STAGE\nwav2vec-U\n13.8\n0%\nREBORN\nIteration 1\n10.9\n21%\nIteration 2\n9.9\n28%\nIteration 3\n7.6\n45%\nIteration 4\n7.4\n46%\nIteration 5\n8.7\n37%\nTOPLINE\nTrain from oracle\n6.4\n54%\n16\nTable 10: We implement REBORN across different speech foundation models on LibriSpeech. The\nresults are evaluated on test-clean. REBORN has exhibited strong generalizability by providing\nsubstantial performance improvements across different speech foundation models. We extract the\n15th layer representations from HuBERT and WavLM following EURO [18].\nFeature extractor\nPER on test-clean (‚Üì)\nPER diff.\nInitial stage\nIter.1-stage1\nIter.1-stage2\nafter Iter.1\nwav2vec 2.0 [3]\n19.3%\n12.9%\n11.9%\n-7.4%\nHuBERT [23]\n20.1%\n12.8%\n10.4%\n-9.7%\nWavLM [12]\n18.7%\n14.7%\n12.7%\n-6.0%\nA.4\nGeneralizability across Different Speech Foundation Models\nWe evaluate REBORN‚Äôs generalizability across different speech foundation models in Table 10.\nSpecifically, in Table 10, we replace the feature extractor from wav2vec 2.0 with HuBERT [23] or\nWavLM [12]. As presented in the table, REBORN can yield performance improvements regardless of\nwhich speech foundation model is used and how well the initial stage performed. The results further\nstrengthen the robustness and the generalizability of our method. Due to the computational resource\nlimitation, we only conduct the ablation for one iteration.\nB\nwav2vec-U\nwav2vec-U [4] is a popular UASR framework whose variants [18, 30] have achieved SoTA perfor-\nmance on multiple UASR benchmarks. Following prior works [33], wav2vec-U also uses adversarial\ntraining to train the phoneme prediction model. The wav2vec-U framework takes the raw waveform\nas the input and predicts phoneme transcription via three stages: (1) the feature extraction stage, (2)\nthe feature preprocessing stage, and (3) the phoneme prediction stage.\nStage (1) extracts speech features using a self-supervised speech foundation model, wav2vec 2.0 [3].\nFor a spoken utterance, we denote the extracted feature as X = [x1, x2, ..., xT ] ‚ààRT √ód, where\nT is the time dimension and d is the feature dimension. Stage (2) is the feature preprocessing\nstage. The dimension of the extracted speech features is first reduced using PCA from d = 1024 to\nd‚Ä≤ = 512. Next, k-means clustering and additional heuristics, namely, adjacent pooling, are adopted\nsequentially to define the boundary of the segmental structures in the speech features. The features\nin the same segmental structure are mean-pooled over the two stages into one feature embedding to\nreduce sequence length from T to T ‚Ä≤ to match phoneme sequences better. In the main text, we call\nthis process \"k-means-based segmentation\". We denote the output from the feature preprocessing\nstage as S = [s1, s2, ..., sT ‚Ä≤] ‚ààRT ‚Ä≤√ód‚Ä≤. In Stage (3), the phoneme prediction model takes the\npooled speech feature as input and predicts the phoneme sequence Y = [y1, y2, ..., yT ‚Ä≤]. Last, a\nde-duplication step removes the consecutive duplicate phoneme predictions from Y , resulting in a\nshortened phoneme sequence Y ‚Ä≤ = [y‚Ä≤\n1, y‚Ä≤\n2, ..., y‚Ä≤\nM] without consecutive duplicate phonemes.\nDuring training, the generator G takes the preprocessed speech feature S as input and generates\nthe logits of Y . We can shorten the logits of Y based on its argmax prediction, and the shortened\nlogits (denoted as G = G(S)) are then taken as input by the discriminator D for GAN training. The\nadversarial learning loss is defined as follows:\nLGAN = min\nG max\nD\nE\nZ‚àºZ[log(D(Z))] + E\nS‚àºS[log(1 ‚àíD(G(S)))]\n(5)\n, where Z is a tokenized phoneme sequence sampled from the phonemicized text corpora Z and\nS is the preprocessed speech feature sampled from the preprocessed speech corpora S. Besides\nthe min-max objective in the adversarial training, wav2vec-U [4] also adopts the three regularized\nobjectives for better convergence and performance. First, the gradient penalty Lgp [21] is applied on\nthe discriminator D to stabilize training, where the input of D is the linear combination of a generated\n17\nsample G and a real sample Z.\nLgp =\nE\nG,Z,Œ±‚àºU(0,1)\nh\n(‚à•‚àáD(Œ±G + (1 ‚àíŒ±)Z)‚à•2 ‚àí1)2i\n(6)\nNext, the smoothness penalty Lsp is computed across consecutive segments of a generated logit\nsequence, promoting the generation of closely aligned neighboring outputs by the model.\nLsp =\nX\n(gt,gt+1)‚ààG\n‚à•gt+1 ‚àígt‚à•2\n(7)\nFurthermore, the phoneme diversity loss Lpd is applied to encourage higher vocabulary usage of the\ngenerated sequences. As shown in Eq. 8, the entropy of the averaged softmax distribution of the\ngenerator outputs (HG(G(S))) is maximized over the phoneme vocabulary across a batch B.\nLpd =\n1\n|B|\nX\nS‚ààB\n‚àíHG(G(S))\n(8)\nFinally, we describe the overall training objectives L by weighting with coefficients Œª, Œ≥, and Œ∑,\nrespectively.\nL = LGAN ‚àíŒªLgp + Œ≥Lsp + Œ∑Lpd\n(9)\nIn this work, we utilize the same training objective from wav2vec-U to perform GAN training (¬ß 3.2).\nC\nImplementation Details\nC.1\nData Preparation and Phoneme LM Formulation\nOur data preparation process mainly follows wav2vec-U2 for fair comparisons. Specifically, we\nuse wav2vec 2.0 large pretrained on LibriLight [24] as the feature extractor in English3; and the\nmultilingual version of wav2vec 2.0 (XLSR-53) for the other langauges4. Before the feature extraction\nstage, we attempt to remove most of the silence through an unsupervised voice activity detection\n(VAD) method [58]5. However, the method is not completely perfect. To stabilize the GAN training, a\nspecial <silence> token is introduced to the text data. The <silence> token is inserted in the front and\nthe end of each sentence, and also between words by a probability of 0.25 according to wav2vec-U.\nThen, we phonemicize all the words in the text data along with the previously inserted <silence>\ntokens for GAN training. The two off-the-shelf phonemizers are used to perform phonemicization.\nFor LibriSpeech, we use the G2P phonemeizer [46], and the numerical stress makers are removed\nto reduce the phoneme set, which is shown to be beneficial for the performance [4, 18, 30]. As for\nMultilingual LibriSpeech, we use Phonemizer [6]6, a toolkit that supports a wide variety of languages\nother than English. The language-switching labels are removed as well as the phonemes appear\nless than 1000 times in the text data. For the Italian, we isolate the double consonant symbol in the\nlexicon, which is crucial for the initial stage to converge. Note that for TIMIT we do not adopt any\nof the above data preparations since it originally contains its phone inventories including silence.\nWe use the 39-phone inventory which can be easily found in Kaldi [49] for training and evaluation7,\nfollowing prior UASR works [4, 18, 33].\nAfter the data preparation, we directly use the phonemicized text with <silence> token to train\nthe phoneme LM. More specifically, we utilize KenLM [22]8, to build the 4-gram LM mentioned\n2https://github.com/facebookresearch/fairseq/tree/main/examples/wav2vec/\nunsupervised\n3https://dl.fbaipublicfiles.com/fairseq/wav2vec/wav2vec_vox_new.pt\n4https://dl.fbaipublicfiles.com/fairseq/wav2vec/xlsr_53_56k.pt\n5https://github.com/zhenghuatan/rVAD\n6https://github.com/bootphon/phonemizer\n7https://github.com/kaldi-asr/kaldi/blob/master/egs/timit/s5/conf/phones.60-48-39.\nmap\n8https://github.com/kpu/kenlm\n18\nin 3.1.3. Since in the GAN training stage, we are actually matching the two distributions between the\nsegmented speech features and phonemicized text with <silence>, we naturally build our external\nLM for RL segmentation learning from the same preprocessed text. The LM is also utilized for the\nunsupervised cross-validation described in C.5.\nC.2\nSelf-Training with Hidden Markov Models\nOriginated from semi-supervised learning, self-training aims at providing a trivial approach to utilize\nthe unpaired or unlabeled data effectively [25, 35, 59]. More specifically, given a supervised ASR\nsystem MASR learned from the labeled speech-text data Dlabeled, we can use MASR to generate pseudo\nlabels of a significant but unlabeled speech data Dunlabeled. Now, we can utilize the speech data along\nwith the pseudo labels to perform supervised learning or re-tuning [59] to obtain a new ASR system\nM‚Ä≤ASR. Generally, given that the pseudo label is good enough, we may expect that M‚Ä≤ASR performs\nbetter than MASR.\nIn the unsupervised setup, the core concept of self-training is suited as well. By simply using an\nunsupervised ASR system MUASR to perform pseudo labeling, we can train a new ASR model M‚Ä≤UASR\nby the supervised objective. Since no labeled data is used (we are just using the pseudo labels),\nthe new ASR system does not violate the unsupervised setup. More specifically, we use Hidden\nMarkov Models (HMMs; Bosch et al. [8])as the backbone of the new ASR system. This can be dated\nback to [11] and is also adopted by recent UASR [4, 30]. Furthermore, to give fair comparisons and\nincrease reproducibility, we follow the publicly available code provided by wav2vec-U for HMM\nself-training9. Our results in Table 1 indicate that with the high-quality pseudo labels generated by\nREBORN, the new ASR model based on HMMs performs better than the original UASR system and\nis the best among all the other methods.\nC.3\nModel Architecture\nWe parametrize the segmentation model œÄŒ∏ using a two-layer 1-dimensional convolutional neural\nnetwork (CNN). This choice of architecture allows us to efficiently extract relevant information from\nadjacent speech features, making it particularly well-suited for our segmentation task. The two con-\nvolutional layers in our CNN have different kernel sizes, namely 7 and 3. This is a lightweight model,\nmaking it computationally efficient for applications. The phoneme prediction model (generator)\nand the discriminator in GAN training are parametrized by a single-layer 1-dimensional CNN and a\ntwo-layer 1-dimensional CNN, respectively, following wav2vec-U. Note that the phoneme prediction\nmodel generates the phoneme sequences non-autoregressively due to its architecture.\nC.4\nHyperparameter Search\nWe search the optimal hyperparameters of our phoneme predictor and segmentation models with\nunsupervised validation metrics. For the phoneme predictor, we directly adopt the search space\nof the hyperparameters and the unsupervised validation metric from wav2vec-U [4].We search the\nhyperparameters indicated as Œª, Œ≥, and Œ∑ in Appendix B only during the initialization stage of the\nphoneme predictor (¬ß 3.3). We directly adopt the same hyperparameters in the following iterations of\nGAN training. As for the segmentation model, we take the idea from Baevski et al. [4] and derive a\nsimilar unsupervised metric.\nC.5\nUnsupervised Validation Metric for Segmentation Model Selection\nEach segmentation model, denoted as œÄŒ∏, undergoes unsupervised performance evaluation by pre-\ndicting the transcriptions of the validation set. The phoneme predictions generated by the model are\nprocessed to form a de-duplicated set, denoted as Y ‚Ä≤\nŒ∏ = [y‚Ä≤\n1,Œ∏, ¬∑ ¬∑ ¬∑ , y‚Ä≤\nM,Œ∏], where consecutive duplicate\nphonemes are removed.\nThe unsupervised metric places significant emphasis on two key aspects: Language Model (LM)\nnegative log-likelihood (NLL) and vocabulary usage. Vocabulary usage is calculated as U(Y ‚Ä≤\nŒ∏) =\n1\n|V |\nP\nv‚ààV I(v ‚ààY ‚Ä≤\nŒ∏), where V denotes the entire vocabulary set. The validation metric seeks to\n9https://github.com/facebookresearch/fairseq/tree/main/examples/wav2vec/\nunsupervised/kaldi_self_train\n19\nminimize the NLL while simultaneously maximizing vocabulary usage. The NLL metric reflects\nthe model‚Äôs proficiency in generating sensible sentences consistent with the dataset distribution,\nwhile vocabulary usage quantifies the diversity of phonemes employed by the model. This combined\napproach ensures that our unsupervised ASR system‚Äôs predictions align with the expected LM\ndistribution and exhibit a wide linguistic range, thus mitigating the risk of favoring repetitive, high-\nprobability sentences.\nAdditionally, the lengths of transcriptions are taken into account. We employ NLL without length\nnormalization, thereby favoring predictions that score high under the language model but without an\nundue preference for overly lengthy transcriptions. The optimization objective is formally expressed\nas:\nŒ∏‚àó= arg min\nŒ∏\n \n‚àíPM\ni=1 log pLM(y‚Ä≤\ni,Œ∏)\nU(Y ‚Ä≤\nŒ∏)\n!\n(10)\nC.6\nTraining Details\nFor the training of our segmentation model, we initially employ behavioral cloning, followed by\nupdates using policy gradient methods. In the BC phase, we configure our model with a batch size of\n128 and a learning rate of 0.0005, training for 20 epochs. We also assign cross-entropy weights of\n[1, 5] for boundary classification, to address the imbalance where the class labeled 0 significantly\noutnumbers the class labeled 1.\nDuring the policy gradient update phase, the model is trained with the same batch size of 128, but\nwith a reduced learning rate of 0.0001 and a weight decay of 0.0001. We utilize the AdamW optimizer\nin conjunction with the CosineAnnealingLR scheduler. For the LS and MLS datasets, the training\nproceeds for 40 epochs, while for the TIMIT dataset, owing to its smaller size, we extend the training\nto 500 epochs to ensure convergence.\nTo refine the performance of our reinforcement learning framework, we have tuned the reward\ncoefficients, leveraging the validation metric (Eq. 10), to secure optimal outcomes across varied\ndatasets. Specifically, the coefficient for the PPL difference reward, cppl, is fixed at 1.0 to prioritize the\nminimization of perplexity in generated sequences. The coefficient for the edit-distance reward, cedit,\nis set at 0.2. And the coefficient for the length difference reward, clen, is selected from a predefined\nset of values within the range [0.0, 0.2, 0.4, 0.6, 0.8]. These values are carefully chosen to balance\nthe importance of length consistency with other factors, with the optimal reward configuration for\neach dataset detailed in Table 11.\nIt is noteworthy that, as the iterations progress, the length constraints become less critical. This\nobservation suggests that the model gradually learns to generate sequences of appropriate length,\nreducing the need for explicit length-based regularizations.\nAs for the Stage 2 GAN training, we inherit the three weighted coefficients in Eq. 9 from the previous\niteration. The initial training of wav2vec-U takes 150000 steps following the original paper. After\nthe initialization stage, we find out that optimizing with 20000 steps for each iteration is enough for\nconverging, taking advantage of initializing the parameters from the previous stage.\nAll of the experiments can be done using a single NVIDIA V100 GPU. The initial stage of wav2vec-U\nGAN training and the REBORN stage 1 reinforcement learning takes about 12 hours of training on\nTable 11: Best reward configurations obtained through hyperparameter searches on each dataset.\nDataset\nLibriSpeech\nTIMIT\nMLS\niter. 1\niter. 2-5\niter. 1-3\nde\nnl\nfr\nes\nit\npt\ncppl\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\ncedit\n0.2\n0.2\n0.2\n0.2\n0.2\n0.2\n0.2\n0.2\n0.2\nclen\n0.2\n0.0\n0.0\n0.4\n0.2\n0.8\n0.4\n0.6\n0.4\n20\nan NVIDIA V100 GPU for each run. As for REBORN stage 2, since the model is not randomly\ninitialized, it only takes about 1.5 hours of training time.\nC.7\nWFST Decoding for Obtaining Word-Level Outputs\nIn REBORN, phoneme-level outputs can be generated directly with greedy decoding. However,\nobtaining word-level outputs requires additional modules such as WFST or external language mod-\nels [4, 18, 34]. In this work, we incorporate REBORN with WFST to generate word-level transcriptions.\nOur decoding procedure mainly follows Baevski et al. [4], which builds the WFST using PyKaldi [9],\nand additional self-loops are introduced to mimic the CTC behavior. Moreover, two hyperparameters:\nthe acoustic scale a, and the additive blank weight v are added during the WFST decoding process.\nIn wav2vec-U, the search interval of a is [0, 8] and v is tuned within [‚àí3, 8]. As for REBORN, we\nfind that using a much smaller search space for both of the hyperparameters is enough for obtaining\nreasonable outputs. More specifically, We tune the acoustic scale in [0.4, 1.2] with step 0.1 and the\nblank weight in [‚àí5, 2] with step 1.\nIt is worth mentioning that we utilize the original boundary generated by the segmentation model\ninstead of the merged one (¬ß 3.1.5) for WFST decoding. We find that the increased amount of\nsegments directly leads to longer generator outputs, which is beneficial for WFST to generate higher-\nquality outputs. The phenomenon is also discovered by the prior works [4, 30]. In wav2vec-U [4], the\nsegmental feature used for WFST decoding is the one before adjacent pooling; while in wav2vec-U\n2.0 [30], they simply lower the decoding stride to generate more lengthy outputs for WFST decoding.\nD\nSpeech Self-supervised Learning and Acoustic Unit Discovery\nSelf-supervised learning (SSL) aims to explore effective ways for representation learning without\nlabeled data. Recent speech SSL models can be categorized into the three classes based on the\npretraining objective [37], including generative [29, 31, 32], contrastive [2, 3, 43], and predictive [5,\n13, 23]. The SSL representation can be utilized for many different downstream tasks, such as phoneme\nrecognition, speaker identification, or emotion recognition [62]. Recent UASR also takes advantage\nfrom speech representations from SSL models [4, 18].\nExtended from self-supervised learning, acoustic unit discovery mainly focuses on deriving phoneme\nor word-like units or learning speech representations that retain only linguistically relevant infor-\nmation in an unsupervised manner [16, 17, 28, 40, 42]. The task is highly related to unsupervised\nphoneme/word segmentation, and the standard evaluation protocols include the phoneme/word\nboundary F1, frame-level ABX score [53] on ZeroSpeech Challenge [40], or phoneme accuracy\nwith supervised learned linear prediction head [16]. Although both acoustic unit discovery and\nunsupervised ASR are learned without supervision, they differ a lot in their learning objectives. As an\nextended topic of representation learning, acoustic unit discovery targets on learning better-segmented\nrepresentations; while unsupervised ASR directly aims at solving the distribution matching problem,\nand may utilize high-quality speech representations to reach the goal.\nE\nDifferent Schemes for Boundary Evaluation\nAccording to Strgar and Harwath [56], the original protocol for evaluating phoneme boundaries\ncontains some ambiguity. The issue arises from double counting when both the ground truth\nboundaries and the predicted boundaries fall within the same tolerance window. To address this\nissue, they propose a harsh (or strict) evaluation protocol that prevents double counting. The\noriginal protocol is referred to as the lenient boundary evaluation protocol. It can be assumed that\nstudies conducted before Strgar and Harwath [56] used the lenient evaluation method, which often\noverestimated the quality of the predicted boundaries. We encourage interested readers to refer to the\noriginal paper for more detailed explanations.\n21\n",
  "categories": [
    "eess.AS",
    "cs.CL",
    "cs.SD"
  ],
  "published": "2024-02-06",
  "updated": "2024-11-15"
}