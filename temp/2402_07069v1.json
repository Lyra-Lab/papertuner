{
  "id": "http://arxiv.org/abs/2402.07069v1",
  "title": "Using Large Language Models to Automate and Expedite Reinforcement Learning with Reward Machine",
  "authors": [
    "Shayan Meshkat Alsadat",
    "Jean-Raphael Gaglione",
    "Daniel Neider",
    "Ufuk Topcu",
    "Zhe Xu"
  ],
  "abstract": "We present LARL-RM (Large language model-generated Automaton for\nReinforcement Learning with Reward Machine) algorithm in order to encode\nhigh-level knowledge into reinforcement learning using automaton to expedite\nthe reinforcement learning. Our method uses Large Language Models (LLM) to\nobtain high-level domain-specific knowledge using prompt engineering instead of\nproviding the reinforcement learning algorithm directly with the high-level\nknowledge which requires an expert to encode the automaton. We use\nchain-of-thought and few-shot methods for prompt engineering and demonstrate\nthat our method works using these approaches. Additionally, LARL-RM allows for\nfully closed-loop reinforcement learning without the need for an expert to\nguide and supervise the learning since LARL-RM can use the LLM directly to\ngenerate the required high-level knowledge for the task at hand. We also show\nthe theoretical guarantee of our algorithm to converge to an optimal policy. We\ndemonstrate that LARL-RM speeds up the convergence by 30% by implementing our\nmethod in two case studies.",
  "text": "Using Large Language Models to Automate and Expedite Reinforcement Learning\nwith Reward Machine\nShayan Meshkat Alsadat1 , Jean-Rapha¨el Gaglione2 , Daniel Neider 3 , Ufuk Topcu2 and Zhe Xu1\n1Arizona State University\n2University of Texas at Austin\n3Technical University of Dortmund\n{smeshka1, xzhe1}@asu.edu, {jr.gaglione, utopcu}@utexas.edu, daniel.neider@cs.tu-dortmund.de\nAbstract\nWe present LARL-RM (Large language model-\ngenerated Automaton for Reinforcement Learning\nwith Reward Machine) algorithm in order to\nencode high-level knowledge into reinforcement\nlearning using automaton to expedite the rein-\nforcement learning. Our method uses Large Lan-\nguage Models (LLM) to obtain high-level domain-\nspecific knowledge using prompt engineering in-\nstead of providing the reinforcement learning al-\ngorithm directly with the high-level knowledge\nwhich requires an expert to encode the automa-\nton. We use chain-of-thought and few-shot meth-\nods for prompt engineering and demonstrate that\nour method works using these approaches. Addi-\ntionally, LARL-RM allows for fully closed-loop re-\ninforcement learning without the need for an expert\nto guide and supervise the learning since LARL-\nRM can use the LLM directly to generate the re-\nquired high-level knowledge for the task at hand.\nWe also show the theoretical guarantee of our algo-\nrithm to converge to an optimal policy. We demon-\nstrate that LARL-RM speeds up the convergence\nby 30% by implementing our method in two case\nstudies.\n1\nIntroduction\nLarge Language Models (LLM) encode various types of in-\nformation including domain-specific knowledge which en-\nables them to be used as a source for extracting specific infor-\nmation about a domain. This knowledge can be used for plan-\nning and control of various systems such as autonomous sys-\ntems by synthesizing the control policies [Seff et al., 2023].\nThese policies are essentially dictating the action of the agent\nin the environment in a high-level format.\nIn general, the extraction of the domain-specific knowledge\nmay not be a straightforward task since prompting the LLM\nwithout a proper prompt technique may result in hallucina-\ntions [Jang et al., 2023], even with proper techniques it is\nalso known LLMs can hallucinate. There are several meth-\nods suggested for prompt engineering ranging from zero-shot\n[Kojima et al., 2022] all the way to training the pre-trained\nlanguage model on a domain-specific dataset (fine-tuning)\n[Rafailov et al., 2023] to obtain more accurate responses.\nThe latter approach is more involved and resource intensive\nsince it requires a dataset related to the domain as well as\nenough computation power to train the pre-trained language\nmodel on the dataset [Ding et al., 2023]. Moreover, not all of\nthe language models are available for fine-tuning due to their\nproviders’ policy. Hence, fine-tuning is a less desirable ap-\nproach as long as the other prompt engineering methods can\nproduce accurate responses.\nOur algorithm LARL-RM (large language model-based\nautomaton for reinforcement learning) uses prompt engineer-\ning methods to extract the relevant knowledge from the LLM.\nThis knowledge is then incorporated into the reinforcement\nlearning (RL) algorithm in the form of high-level knowledge\nto expedite the RL and guide it to reach the optimal pol-\nicy faster. We integrate the LLM-encoded high-level knowl-\nedge into the RL using deterministic finite automata (DFA).\nWe have the following contributions to the field: (a) LARL-\nRM allows for integration of the high-level knowledge from\na specific domain into the RL using LLM outputs directly.\n(b) Our proposed method is capable of adjusting the instruc-\ntions of the LLM by closing the loop in learning and updat-\ning the prompt based on the chain-of-thought method to gen-\nerate the automata when counterexamples are encountered\nduring the policy update. (c) LARL-RM is capable of learn-\ning the reward machine and expediting the RL by providing\nLLM-generated DFA to the RL algorithm. We also show that\nLARL-RM speeds up the RL by 30% by implementing it in\ntwo case studies.\nRelated work. Generative language models (GLM) such as\nthe GPT models have been shown to encode world knowl-\nedge and be capable of generating instructions for a task\n[Hendrycks et al., 2020]. Researchers in [West et al., 2021]\nhave demonstrated the advances in GLM allow to construct\ntask-relevant knowledge graphs.\nGenerating automaton\nfrom GLM. Generating automaton from a generative lan-\nguage model for a specific task is shown in [Yang et al.,\n2022]; however, the method proposed by the authors called\nGLM2FSA uses verb and verb phrases to and maps them onto\na predefined set of verbs in order to convert the instructions\ngiven by the GLM to an automaton which in turn reduces\nthe generality of the method to be applied to any other do-\nmain and still requires an expert to pre-define this set. GLMs\nare also used to extract task-relevant semantic knowledge and\narXiv:2402.07069v1  [cs.LG]  11 Feb 2024\ngenerate plans to complete a task in robotics [Vemprala et al.,\n2023]. Learning reward machine. Researchers in [Neider\net al., 2021] have shown a method called AdvisoRL for RL\nwhere the algorithm is capable of learning the reward ma-\nchine; however, an expert is required to guide the learning\nprocess through provision of so-called advice. In our method\nthis is replaced by using LLM.\n2\nPreliminaries\nIn this section, we define our notations and introduce the nec-\nessary background for reinforcement learning, reward ma-\nchines, and finite deterministic automata (DFA).\nDefinition 1 (labeled Markov decision process). A la-\nbeled\nMarkov\ndecision\nprocess\nis\na\ntuple\nM\n=\n⟨S, sI, A, p, R, γ, P, L⟩where S is a finite set of states, sI ∈\nS is the agent’s initial state, A is a finite set of actions,\np : S ×A×S 7→[0, 1], reward function R : S ×A×S 7→R,\ndiscount factor γ ∈[0, 1], a finite set of propositions P, and\na labeling function L : S × A × S 7→2P that determines the\nhigh-level events that agent encounters in the environment.\nA policy is a function mapping states S to actions in A\nwith a probability distribution, meaning the agent at state\ns ∈S will choose action a ∈A with probability π(s, a)\nusing the policy π that leads to a new state s′. The prob-\nability of the state s′ is defined by p(s, a, s′).\nTrajectory\ns0a0s1 . . . skaksk+1 where k ∈N is the resulting sequence\nof states and actions during this stochastic process. Its corre-\nsponding label sequence is l0l1 . . . lk where L(si, ai, si+1) =\nli ∀i\n≤\nk.\nTrace is the pair of labels and rewards\n(λ, ρ) := (l1l2 . . . lk, r1r2 . . . rk) where the payoff of the\nagent is P\ni γiri.\nWe exploit deterministic finite automaton (DFA) to encode\nthe high-level knowledge obtained from the LLM to rein-\nforcement learning in order to speed up the process of ob-\ntaining the optimal policy π∗. A DFA consists of an input\nand output alphabet which we can use to steer the learning\nprocess.\nDefinition 2 (Deterministic finite automata). A DFA is a tuple\nD = ⟨H, hI, Σ, δ, F⟩where H is the finite set of states, ho is\nthe initial state, and Σ is the input alphabet (here we consider\nΣ = 2P), δ : H × Σ 7→H is the transition function between\nthe states, and F ⊆H is the set of accepting states.\nWe use deterministic finite state automata to model the\nLLM-generated DFA. The main idea of the LLM-generated\nDFA is to assist the algorithm with the learning of the reward\nmachines through the provision of information about which\nlabel sequence may result in a positive reward and which one\nwill definitely result in a non-positive reward [Neider et al.,\n2021].\nReward machines are a type of Mealy machine that is used\nto encode a non-Markovian reward function. They are au-\ntomatons that receive a label from the environment and re-\nspond with a reward as well as transition to their next state.\nTheir input alphabet is a set of propositional variables (from\nthe set of P) and their output alphabet is a set of real numbers.\nDefinition 3 (Reward machine). A reward machine is shown\nby A =\n\u0000V, νI, 2P, M, δ, σ\n\u0001\nwhere V is a finite set of states,\nνI ∈V is the initial state, input alphabet 2P, output alphabet\nM ⊆R, transition function δ : V × 2P 7→V , and output\nfunction σ : V × 2P 7→M.\nApplying the reward machine A on a sequence of la-\nbels l1l2 . . . lk ∈(2P)∗will create a sequence of states\nν0(l1, r1)ν1(l2, r2) . . . νk−1(lk, rk)νk where ν0 = νI for all\ni ∈{0, . . . , k −1}. Transition to next state of the reward ma-\nchine δ(νi, li+1) = νi+1 results in a reward of σ(νi, li+1) =\nri+1.\nHence, a label sequence of l1l2 . . . lk corresponds\nto a sequence of rewards produced by reward machine\nA(l1l2 . . . lk) = r1r2 . . . rk. Therefore, reward function R\nof an MDP process for every trajectory s0a0s1 . . . skaksk+1\ncorresponds to a label sequence of l1l2 . . . lk that encodes the\nreward machine A, agent then receives reward sequence of\nA(l1l2 . . . lk).\nJ\nb\ne\nd\nz\nFigure 1: An autonomous car (agent) must first go to intersection J\nand then make a right turn to go to intersection b. Agent must check\nfor the green traffic light g, car c, and pedestrian p.\nMotivating Example.\nWe consider an autonomous\ncar on American roads with the set of actions A\n=\n{up, down, right, left, stay} (see Figure 1) where the agent\nmust reach intersection J then making a right turn when the\ntraffic light g is green and if there is no car c on the left\nand no pedestrian p on the right. Then the agent reaches the\nintersectionb where it needs to go straight when the traffic\nlight is green if there are no cars and pedestrians. The corre-\nsponding DFA for this motivating example is shown in Figure\n2.\nh0\nstart\nh1\nh2\nh3\nh4\nJ\n¬J\ng ∧¬c ∧¬p\n¬g ∨¬c ∨¬p\n¬b\nb\n¬g ∨c\ng ∧¬c\n⊤\nFigure 2: DFA of the motivating example where the agent must\nreach intersection J while avoiding cars and pedestrians.\n3\nGenerating Domain Specific Knowledge\nUsing LLM\nLarge Language Models (LLM) encode rich world and\ndomain-specific knowledge [Yang et al., 2023] which allows\nfor the extraction of the relevant information through appro-\npriate prompting and has shown that can even surpass fine-\ntuning LLMs in some cases [Wei et al., 2022]. Hence, we use\nprompt engineering to extract the relevant domain knowledge\ninformation from the LLM and provide it in the form of LLM-\ngenerated DFA [Neider et al., 2021] to expedite the reinforce-\nment learning. An LLM-generated DFA can be thought of\nas a means to provide the process of learning reward ma-\nchine with information about which label sequence could be\npromising in such a way that it results in a reward.\nApplying a DFA D on a sequence λ = l1l2 . . . ln ∈Σ⋆\nwill result in a sequence of states h0h1 . . . hn where h0 = hI\nand hi+1 = δ(hi, li) ∀i ∈{1, . . . , n}. We define L(D) =\n{l1l2 . . . ln ∈Σ⋆|hn ∈F}, also commonly referred to as the\nformal language accepted by D.\nA LLM-generated DFA\ncould result in a positive reward, meaning that DFA D can\nbe considered as a LLM-generated DFA if the label sequence\nλ ∈L(D) could result in a positive reward; while, the la-\nbel sequences λ /∈L(D) should not receive a positive re-\nward; therefore, it is a binary classifier that points out the\npromising explorations [Neider et al., 2021]. Our proposed\nalgorithm uses the LLM-generated DFAs to narrow down the\nsearch space for learning the ground truth reward machine.\nHence, expediting the reinforcement learning; however, in\nour method, the LLM-generated DFA is generated automat-\nically for a specific domain using the LLM. It is known that\nthe LLMs hallucinate; therefore, our proposed algorithm is\ncapable of ignoring the LLM-generated DFA D if it is incom-\npatible the underlying ground truth reward machine.\nDefinition 4 (LLM-generated DFA compatibility). An LLM-\ngenerated DFA D is compatible with the ground truth reward\nmachine A if for all label sequences l1l2 . . . lk ∈(2P)∗with\nreward sequence A(l1l2 . . . lk) = r1r2 . . . rk it holds that\nrk > 0 implies l1l1 . . . lk ∈L(D).\nWe demonstrate an LLM-generated DFA for the motivat-\ning example in Figure 3.\nh1\nstart\nh2\n¬g ∨c ∨p\n⊤\ng ∧¬c ∧¬p\nFigure 3: An LLM-generated DFA for our motivating example.\nIn our proposed method we generate the domain-specific\nknowledge using the GPT series [Liu et al., 2023]. There\nare several methods of prompting LLMs such as zero-shot\nmethod [Kojima et al., 2022] where the prompt is using a\nstep-by-step approach to obtain better results or the few-shot\nmethod [Brown et al., 2020] where the prompt is given sim-\nilar examples encouraging the LLMs to produce similar rea-\nsoning to reach the correct answer. We use a combination of\nthese methods to encourage the GPT to produce the correct\nFigure 4: Prompting GPT-3.5-Turbo to adopt a persona as an\nexpert in traffic rules (GPT response and prompt).\nFigure 5: Mapping the output of the LLM to a specific set of propo-\nsitions.\ninformation. First, we adopt a persona for the domain that we\nplan to extract relevant information from; in our example, the\npersona is an expert in American road traffic rules. Adopt-\ning a persona allows for more coherent and higher-quality re-\nsponses from the language model. Figure 4 demonstrates the\nGPT response to a question for making a right turn at a traffic\nlight while an expert persona is adopted.\nFigure 4 demonstrates that we can obtain the knowl-\nedge for a specific domain using the appropriate prompting\nmethod. In general, LLMs are text-based which means that\ntheir output is text and it cannot directly be used in an RL\nalgorithm; therefore we require a method to convert this tex-\ntual output to an automata. The next step is to ask the LLM\nto convert the provided steps to relevant propositions P so it\ncan be used for the generation of the automata. We can per-\nform this by mapping the steps into the set of propositions as\nillustrated in Figure 5.\nWe use the mapped steps (instructions) to generate the DFA\nsince actions and propositions are available; however, transi-\ntions and states are yet to be defined. We can use the labels for\ntransitions between the states and to obtain the states we can\ncircle back to the task and use the instructions to obtain the\nstates. Therefore, we use the task description and the mapped\nsteps to obtain the transitions and states, Figure 6 shows the\nprompting used to create the states and transitions.\nWe can construct a DFA by having the states hi, proposi-\ntions P, and transition function δ(hi, li); therefore, we can\nnow use this information to generate the DFA for the task and\nuse it to expedite convergence to optimal policy π∗(s, a).\nAlgorithm 1 can be updated to not even require proposi-\ntions P and actions A to obtain the DFA. Generating DFA\nFigure 6: We use the mapped instructions to propositions and the\ntask description to obtain the states and transitions.\nAlgorithm 1 PromptLLMforDFA: Constructing task DFA\nfor a domain specific task using LLM\nInput: prompt f\nParameter: temperature T, Top p p (OpenAI LLM parame-\nters)\nOutput: DFA\n1: output ←promptLLM(f)\n2: for step ∈output do\n3:\nMprop ←MapToProp(step)\n4:\nWaction ←MapToActions(step)\n5: end for\n6: output ←UpdatePrompt(f, steps, Mprop, Waction)\n7: H ←{h0}\n8: δ ←{δ0}\n9: P ←{P0}\n10: i ←0\n11: for instruction in output do\n12:\ni ←i + 1\n13:\nhi ←GetStates(instruction)\n14:\nδi ←GetTransition(instruction)\n15:\nPi ←GetProposition(instruction)\n16:\nH ←H ∪{hi}\n17:\nδ ←δ ∪{δi}\n18:\nP ←P ∪{Pi}\n19: end for\n20: return ⟨H, h0, 2P, δ, {hi}⟩\nthis method requires a different prompting technique which\ncombines two methods of few-shots and chain-of-thought we\ncall MixedR (mixed-reasoning). We exploit this approach to\nobtain tasks that are more involved since obtaining the propo-\nsitions and actions requires more information about the sys-\ntem; however, the MixedR requires the task description and\nexamples of DFAs which do not require to even be relevant\nto the task as long as the descriptions are provided for the\nexample DFAs even in high-level since the LLMs are capa-\nble of extracting complex patterns from textual data. We use\nMixedR to generate the DFA. Figure 7 shows the prompt tem-\nplate used for MixedR.\nCounterexample \nof RM found?\nUser\nPrompt\nLLM\nLLM-generated DFA\nRL algorithm\nLearned RM\nPrompt update using \ncounterexample\nYes\nCounterexample of \nLLM-generated DFA \nfound?\nLearning the RM\nNo\nNo\nYes\nFigure 8: LARL-RM uses the LLM to generate the automata. The\nLLM-generated automata will be used by the RL algorithm to expe-\ndite the convergence to optimal policy.\nFigure 7: We use the MixedR to obtain the DFAs that are more\ncomplex.\nWe can use the motivating example and the MixedR\nprompt technique to obtain the relevant DFA for a smaller\nexample of the problem. We can then use the DFA obtained\nfrom the MixedR and provide it as LLM-generated DFA for\nthe RL algorithm to use in order to learn the ground truth re-\nward machine in a smaller search space.\n4\nExpediting Reinforcement Learning Using\nLLM-generated DFA\nWe use the LLM output as instructions to expedite the\nreinforcement learning convergence to the optimal policy\nπ∗(s, a) using the language models. We use the LLMs to\nnarrow down the search space for the RL algorithm since in\na typical reinforcement learning algorithm the search space\nis considerable leading to cases where the training requires\nsignificant time and resources. Hence, using the pre-trained\nLLMs we can reduce this search space and converge to the\noptimal policy faster. One main issue to address is that LLMs\ntypically generate outputs that are not aligned with facts;\ntherefore, it is necessary to close the loop in the RL algorithm\nso that if a counterexample is met then the algorithm should\nbe capable of adjusting the prompt, and updating it to create\nan updated LLM-generated DFA that could be used by the\nRL algorithm. Figure 8 illustrates the LARL-RM algorithm.\nOur proposed method LARL-RM uses the prompt pro-\nvided by the user to run the RL algorithm for a specific task\nthat requires domain knowledge; however, by using the LLM\nthe need for an expert is minimized and the algorithm itself\nAlgorithm 2 LARL-RM algorithm for incorporating high-\nlevel domain-specific knowledge from LLM into RL\nInput: prompt f, Episode length Episodelength\nParameter:\nlearning rate α, discount factor γ, epsilon-\ngreedy ϵ, temperature T, Top p p, LLM query budget J ∈N\n1: X ←∅(empty sample)\n2: D ←PromptLLMforDFA(f)\n3: A ←InitializeRewardMachine() (compatible\nwith D)\n4: q(s, v, a)\n←\nInitializeQFunction(), Q\n=\n{qq|q ∈V }\n5: for episode in 1, . . . , Episodelength do\n6:\nXinit ←X\n7:\nDinit ←D\n8:\n(λ, ρ, Q) ←QRM-episode(A, Q)\n9:\nif A(λ) ̸= ρ then\n10:\nX ←X ∪{(λ, ρ)}\n11:\nend if\n12:\nif D ̸= Dinit orX ̸= Xinit then\n13:\nif J ≥0 and ∃(λ′, ρ′) ∈X, ρ′ > 0 and λ′ /∈L(D)\nthen\n14:\nf ←UpdatePrompt(f, λ′)\n15:\nD ←PromptLLMforDFA(f)\n16:\nJ ←J −1\n17:\nend if\n18:\nA\n←\nLearnRewardMachine() (compatible\nwith D and X)\n19:\nq(s, v, a) ←InitializeQFunction()\n20:\nend if\n21: end for\ncan update the prompt using the counterexamples to update\nits DFA. LARL-RM first initializes an empty set for the trace\nX, then uses the prompt to generate the relevant DFA for\nthe RL (Lines 1-2). The algorithm then initializes the reward\nmachine based on the LLM-generated DFA obtained from the\nLLM and also initializes the QRM (q-learning for reward ma-\nchines) before starting the episode (Lines 3-4). LARL-RM\nstores the trace X and DFA D in order to reinitialize the re-\nward machines and q-values if counterexamples are met, then\nthe QRM is called to update the q-values and if there are\nany counterexamples then that trace is stored so that it can\nbe used to update the reward machine and the DFA (Lines\n5-10). If the trace met by the agent is not compatible with\nthe ground truth reward machine then it is removed from the\nLLM-generated DFA set D and then the algorithm uses this\nincompatible DFA and the initial prompt f to obtain an up-\ndated prompt f using the counterexample label sequence\n, LARL-RM also uses an LLM query budget J to ensure\nthat if the responses are not compatible with the ground truth\nreward machine then it will not get stuck in a loop and eventu-\nally after the budget J is depleted then it can start to learn the\nground truth reward machine without LLM-generated DFA\n(Lines: 13-16, this also highlights the importance of prompt\nengineering further). Afterward, LARL-RM uses the stored\nDFA and trace sets to reinitialize the reward machine and q-\nvalues (Lines 18-19).\n4.1\nUsing LLM-generated DFA to Learn the\nReward Machine\nLARL-RM uses the counterexamples to create a minimal\nreward machine while being guided by the LLM-generated\nDFA. Our method uses a similar approach as of the [Nei-\nder et al., 2021] such that it maintains a finite sample set\nX ⊂2P ×R and a finite set of DFAs D = {D1, . . . , Dl}. We\nassume that the LLM-generated DFA is compatible with the\nsample X such that the (l1 . . . lk, r1 . . . rk) ∈X with positive\nreward rk > 0 where l1 . . . lk ∈L(D) and if this criterion is\nnot fulfilled then it gets ignored by the LARL-RM.\nThe RM learning which relies on the LLM-generated DFA\nperforms the generation of the minimal reward machine A\nwhich is consistent with the trace X and compatible with D.\nThe minimal requirement for the reward machine to be mini-\nmal is important to the convergence of the LARL-RM to the\noptimal policy. LARL-RM uses SAT-based automata learn-\ning to verify the parametric systems since the learning task\ncan be broken down into a series of satisfiability checks for\npropositional logic formulas. Essentially we construct and\nsolve the propositional formulas ΦX,D\nn\nwhere the values of\nn > 0 [Neider et al., 2021][Neider, 2014].\nWe construct the formula ΦX,D\nn\nbased on a propositional\nset X = {x, y, z, . . .} using the Boolean connectives ¬, ∨, ∧,\nand 7→, i.e., and interpretation is the mapping from proposi-\ntional formulas to Boolean values such that I : X 7→{0, 1}.\nIf the the propositional formula is satisfied then I |= Φ which\ninterprets to I satisfies formula Φ. Using this approach we\ncan obtain a minimal reward machine similar to [Neider et\nal., 2021]. Using the LLM we are capable of constructing an\nLLM-generated DFA in a format that is compatible with SAT-\nbased automata learning methods. LARL-RM is also capable\nof adjusting the initial prompt f so that in case it is not com-\npatible with the ground truth reward machine then it could be\nupdated f. Updated prompt f uses the counterexample label\nsequence λ′ to call the LLM and obtain an updated DFA D\nwhich is compatible with the ground truth reward machine.\n4.2\nRefinement of Prompt and DFA\nWe use prompt f to generate the DFA D = ⟨H, hI, Σ, δ, F⟩\nfor a specific domain. Transition exists for a proposition if\nδ(hi, li) = 1, meaning if the LLM-generated DFA generated\nby the prompt f is incorrect then it cannot have the correct\ntransitions and trajectory which leads to a counterexample λ′.\nTherefore, we use the counterexample λ′ = l1l2 . . . lk to\nupdate the prompt in order to generate an updated DFA.\nHence, the updated DFA has a chance of becoming com-\npatible with the counterexample. This process continues and\neach time the algorithm encounters a counterexample (λ′)\nuses it to update the prompt f again.\n5\nConvergence to Optimal Policy\nThe learned reward machine will ultimately be equivalent to\nthe ground truth reward machine under the condition that any\nlabel sequence is admissible by the underlying MDP, i.e., that\nare physically allowed. If this assumption does not hold, it\nis still ensured that learned reward machine and the ground\ntruth reward machine will agree on every admissible label se-\nquences.\nDue to the fact that we use LLM to generate the DFA D and\nthe fact that the LLMs are known to produce outputs that are\nnot factual, we face an issue. We cannot make any assumption\non the quality of the output of the LLM. For the purpose of\nproving guaranties of LARL-RM, we need to consider the\nLLM as adversarial, that is, considering the worst case.\nLemma 1. Let M be a labeled MDP, A the ground\ntruth\nreward\nmachine\nencoding\nthe\nrewards\nof\nM,\nand\nD⋆\n=\n{D1, . . . , Dm}\nthe\nset\nof\nall\nLLM-\ngenerated DFAs that are added to D during the run\nof LARL-RM. Additionally, let nmax\n=\nmaxD∈D⋆{|D|}\nand m = max\nn\n2|M| · (|A| + 1) · nmax, |M| (|A| + 1)2o\n.\nThen, LARL-RM with Episodelength ≥m almost surely\nlearns a reward machine that is equivalent to A.\nLARL-RM provides us with an upper bound for the\nepisode length that needs to be explored.\nAdditionally,\nLARL-RM algorithm correctness follows the Lemma 1 and\nthe correctness of QRM algorithm [Icarte et al., 2018]. Using\nthe QRM algorithm guarantee we can now show the conver-\ngence to optimal policy\nTheorem 1. Let M, A, D⋆, and m be as in Lemma 1. Then,\nLARL-RM will converge to an optimal policy almost surely if\nEpisodelength ≥m.\nTheorem 1 guarantees the convergence of the LARL-RM\nto an optimal policy if sufficient episode length is given for\nexploration. It also provides an upper bound for convergence\nas shown in Lemma 1.\n6\nCase Studies\nWe implement the LARL-RM using the GPT series LLM,\nspecifically we focus on model GPT-3.5-Turbo. We can\nset up a chat with either of the models using the provided\nAPIs from OpenAI. For our example we consider the LARL-\nRM applied to an autonomous car example similar to the ex-\nample found in [Xu et al., 2020a].\nIn our example, the autonomous car must navigate to reach\na destination while avoiding any pedestrians, but obeying\ntraffic laws. It is worth mentioning that our algorithm is capa-\nble of running even without any LLM-generated DFA since in\nthis case it can learn the ground truth reward machine with-\nout LLM-generated DFA. We consider two case studies for\nour motivating example to demonstrate the LARL-RM capa-\nbilities to expedite the RL. In both cases, we demonstrate the\neffect of incompatible LLM-generated DFA (not compatible\nwith the ground truth reward machine) on the algorithm con-\nvergence to the optimal policy.\n6.1\nCase Study 1\nThe agent in our traffic example has the following set of ac-\ntions A = {up, down, right, left, stay}. The layout of the en-\nvironment is shown in Figure 1\nWe use the LLM-generated DFA D to guide the RL pro-\ncess. If the advice from the LLM is not compatible with the\nground truth reward machine then it will be ignored by the al-\ngorithm. Prompt is updated and fed back to the LLM in order\nto obtain a new DFA that has a chance of becoming com-\npatible with the counterexample. We show the ground truth\nreward machine for Case Study 1 in Figure 9.\nv0\nstart\nv1\nv2\nv3\nv4\n(g ∧¬c ∧¬p, 0)\n(¬g ∨c ∨p, 0)\n(¬J ∨¬g ∨c ∨p, 0)\n(J ∧g ∧¬c ∧¬p, 0)\n(¬b ∨¬g ∨c ∨p, 0)\n(b ∧g ∧¬c ∧¬p, 0)\n(¬z, 0)\n(z, 1)\n(⊤, 0)\nFigure 9: Ground truth reward machine for Case Study 1. Agent\nmust first navigate to intersection J then b.\nThe LLM-generated DFA generated by the prompt f for\nthis case study is compatible with the ground truth reward\nmachine (Figure 9) and helps the algorithm to converge to\nthe optimal policy faster. The generated DFA using the GPT\nis shown in Figure 10.\nh1\nstart\nh2\nJ ∧g ∧¬c ∧¬p\n¬J ∨¬g ∨c ∨p\n⊤\nFigure 10: LLM-generated DFA D, {J ∧g ∧¬c ∧¬p} generated\nby LLM for Case Study 1.\nWe use the DFA (10) to guide the learning process. The\nreward obtained by the LARL-RM algorithm shows that it\ncan reach the optimal policy faster if the LLM-generated DFA\nexists in comparison to the case when there is none.\n0\n5000\n10000\n15000\n20000\n25000\nStep\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nReward\nWithout LLM-generated DFA\nCompatible LLM-generated DFA\nIncompatible LLM-generated DFA\nFigure 11: LARL-RM uses the LLM-generated DFA D to obtain a\nreward machine that is aligned with the ground truth reward machine\nin order to converge to an optimal policy faster.\nWe average the results of 5 independent runs for using a\nv0\nstart\nv1\nv2\nv3\nv4\nv5\n(g ∧¬c ∧¬p, 0)\n(¬g ∨c ∨p, 0)\n(b ∧g ∧¬c ∧¬p, 0)\n(¬b ∨¬g ∨c ∨p, 0)\n(¬e ∨¬g ∨p ∨c, 0)\n(e ∧g ∧¬p ∧¬c, 0)\n(B, 0)\n(¬z, 0)\n(¬b ∨¬g ∨p ∨c, 0)\n(z, 1)\n(⊤, 0)\nFigure 12: Ground truth reward machine for Case Study 2. Agent\nmust reach the destination no matter the route (B = b∧g∧¬p∧¬c).\nrolling mean with a window size of 10. Figure 11 demon-\nstrates that the algorithm reaches the convergence policy us-\ning the LLM-generated DFA. In this example, the prompt f\nis one time compatible and one time incompatible with the\nground truth reward machine, but even if it is not compatible\nLARL-RM can use the counterexample to update the prompt\nand generate a new DFA which is more likely to be compati-\nble with the counterexample.\n6.2\nCase Study 2\nIn Case Study 2 we show that if the suggested DFA D is not\ncompatible with the ground truth reward machine then the\nLARL-RM uses the counterexample to adjust the prompt f\nand update it to obtain new DFA which is compatible with\nthe counterexample. In this case study we update the MDP\nenvironment to allow for multiple reward machines as well as\nDFAs to be considered as equivalent. This way the output of\nthe LLM may not necessarily match the ground truth reward\nmachine; however, the LARL-RM can still use this DFA as\nlong as it is not incompatible with the ground truth reward\nmachine, but is equivalent to it.\nIn this environment, we extend the autonomous car exam-\nple and provide it with more options to reach the destinations\nand the ground truth reward machine does not consider a spe-\ncific route, but rather emphasizes the target destination; how-\never, the LLM-generated DFA might specify a certain route\nover another which is not incompatible with the ground truth\nreward machine, but rather equivalent. We show the ground\ntruth reward machine for Case Study 2 in Figure 12.\nThe LLM-generated DFA D, {J ∧g ∧¬c ∧¬p, b ∧g ∧\n¬c ∧¬p} for this case study specifies a route which leads\nto the target and it is equivalent to the ground truth reward\nmachine since this solution DFA can be considered a subset\nof the larger DFA D which is compatible with the ground\ntruth reward machine. Figure 13 shows the LLM-generated\nDFA for Case Study 2.\nWe run the LARL-RM for the Case Study 2 using the\nprompt f which generates the DFA D, {J, b ∧g ∧¬c ∧¬p}\nwhich is incompatible with the ground truth reward machine,\nbut the LARL-RM takes this counterexample and generates\nh1\nstart\nh2\nh3\nJ ∧g ∧¬c ∧¬p\nb ∧g ∧¬c ∧¬p\n¬J ∨¬g ∨c ∨p\n¬b ∨¬g ∨c ∨p\n⊤\nFigure 13: LLM-generated DFA D contains {b ∧g ∧¬c ∧¬p}\ngenerated by LLM for Case Study 2.\nan updated DFA D, {b∧g∧¬c∧¬p} which is compatible with\nthe ground truth reward machine. Figure 14 demonstrates the\nreward of LARL-RM using the DFA D.\n0\n5000\n10000\n15000\n20000\n25000\n30000\n35000\n40000\nStep\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nReward\nWithout LLM-generated DFA\nCompatible LLM-generated DFA\nIncompatible LLM-generated DFA\nFigure 14: LARL-RM updates the prompt f in order to obtain a\nDFA that is compatible with the ground truth reward machine.\nFigure 14 shows the reward obtained by LARL-RM for\nCase Study 2 using the prompt f which generates the DFA\nD. The reward is for 5 independent runs averaged at each\n10th step. For both cases of compatible and incompatible\nLLM-generated DFA the LARL-RM converges to the opti-\nmal policy faster than when there is no LLM-generated DFA.\n7\nConclusion\nWe proposed a novel algorithm, LARL-RM, that uses a\nprompt to obtain an LLM-generated DFA to expedite rein-\nforcement learning. LARL-RM uses counterexamples to au-\ntomatically generate a new prompt and consequently, a new\nDFA that is compatible with the counterexamples to close the\nloop in RL. We showed that LARL-RM is guaranteed to con-\nverge to an optimal policy using the LLM-generated DFA. We\nshowed that RL can be expedited using the LLM-generated\nDFA and in case the output of the LLM is not compatible\nwith the ground truth reward machine, LARL-RM is capable\nof adjusting the prompt to obtain a more accurate DFA. In fu-\nture work, we plan to extend the proposed framework to RL\nfor multi-agent systems.\nAcknowledgments\nThis research is partially supported by the National Science\nFoundation under grant NSF CNS 2304863 and the Office of\nNaval Research under grant ONR N00014-23-1-2505.\nReferences\n[Brown et al., 2020] Tom Brown, Benjamin Mann, Nick Ry-\nder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,\nAmanda Askell, et al.\nLanguage models are few-shot\nlearners. Advances in neural information processing sys-\ntems, 33:1877–1901, 2020.\n[Ding et al., 2023] Ning Ding, Yujia Qin, Guang Yang,\nFuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu,\nYulin Chen, Chi-Min Chan, Weize Chen, et al. Parameter-\nefficient fine-tuning of large-scale pre-trained language\nmodels.\nNature Machine Intelligence, 5(3):220–235,\n2023.\n[Hendrycks et al., 2020] Dan\nHendrycks,\nCollin\nBurns,\nSteven Basart, Andy Zou, Mantas Mazeika, Dawn Song,\nand Jacob Steinhardt. Measuring massive multitask lan-\nguage understanding. arXiv preprint arXiv:2009.03300,\n2020.\n[Icarte et al., 2018] Rodrigo Toro Icarte, Toryn Klassen,\nRichard Valenzano, and Sheila McIlraith. Using reward\nmachines for high-level task specification and decompo-\nsition in reinforcement learning.\nIn International Con-\nference on Machine Learning, pages 2107–2116. PMLR,\n2018.\n[Jang et al., 2023] Joel Jang, Seonghyeon Ye, and Minjoon\nSeo. Can large language models truly understand prompts?\na case study with negated prompts. In Transfer Learning\nfor Natural Language Processing Workshop, pages 52–62.\nPMLR, 2023.\n[Kojima et al., 2022] Takeshi Kojima, Shixiang Shane Gu,\nMachel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large\nlanguage models are zero-shot reasoners.\nAdvances in\nneural information processing systems, 35:22199–22213,\n2022.\n[Liu et al., 2023] Yiheng Liu, Tianle Han, Siyuan Ma, Jiayue\nZhang, Yuanyuan Yang, Jiaming Tian, Hao He, Antong Li,\nMengshen He, Zhengliang Liu, et al. Summary of chatgpt-\nrelated research and perspective towards the future of large\nlanguage models. Meta-Radiology, page 100017, 2023.\n[Neider et al., 2021] Daniel Neider, Jean-Raphael Gaglione,\nIvan Gavran, Ufuk Topcu, Bo Wu, and Zhe Xu. Advice-\nguided reinforcement learning in a non-markovian envi-\nronment. In Proceedings of the AAAI Conference on Arti-\nficial Intelligence, volume 35, pages 9073–9080, 2021.\n[Neider, 2014] Daniel Neider.\nApplications of automata\nlearning in verification and synthesis. PhD thesis, Aachen,\nTechn. Hochsch., Diss., 2014, 2014.\n[Rafailov et al., 2023] Rafael Rafailov, Archit Sharma, Eric\nMitchell, Stefano Ermon, Christopher D Manning, and\nChelsea Finn. Direct preference optimization: Your lan-\nguage model is secretly a reward model. arXiv preprint\narXiv:2305.18290, 2023.\n[Seff et al., 2023] Ari Seff, Brian Cera, Dian Chen, Mason\nNg, Aurick Zhou, Nigamaa Nayakanti, Khaled S Refaat,\nRami Al-Rfou, and Benjamin Sapp.\nMotionlm: Multi-\nagent motion forecasting as language modeling. In Pro-\nceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 8579–8590, 2023.\n[Shallit, 2008] Jeffrey O. Shallit. A Second Course in Formal\nLanguages and Automata Theory. Cambridge University\nPress, 2008.\n[Vemprala et al., 2023] Sai\nVemprala,\nRogerio\nBonatti,\nArthur Bucker, and Ashish Kapoor. Chatgpt for robotics:\nDesign principles and model abilities. Microsoft Auton.\nSyst. Robot. Res, 2:20, 2023.\n[Wei et al., 2022] Jason Wei, Xuezhi Wang, Dale Schuur-\nmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le,\nDenny Zhou, et al.\nChain-of-thought prompting elicits\nreasoning in large language models. Advances in Neural\nInformation Processing Systems, 35:24824–24837, 2022.\n[West et al., 2021] Peter West, Chandra Bhagavatula, Jack\nHessel, Jena D Hwang, Liwei Jiang, Ronan Le Bras,\nXiming Lu, Sean Welleck, and Yejin Choi.\nSymbolic\nknowledge distillation: from general language models to\ncommonsense models. arXiv preprint arXiv:2110.07178,\n2021.\n[Xu et al., 2020a] Zhe Xu, Ivan Gavran, Yousef Ahmad, Ru-\npak Majumdar, Daniel Neider, Ufuk Topcu, and Bo Wu.\nJoint inference of reward machines and policies for rein-\nforcement learning. In Proceedings of the International\nConference on Automated Planning and Scheduling, vol-\nume 30, pages 590–598, 2020.\n[Xu et al., 2020b] Zhe Xu, Ivan Gavran, Yousef Ahmad, Ru-\npak Majumdar, Daniel Neider, Ufuk Topcu, and Bo Wu.\nJoint inference of reward machines and policies for re-\ninforcement learning. In ICAPS, pages 590–598. AAAI\nPress, 2020.\n[Yang et al., 2022] Yunhao Yang, Jean-Rapha¨el Gaglione,\nCyrus Neary, and Ufuk Topcu.\nAutomaton-based rep-\nresentations of task knowledge from generative language\nmodels. arXiv preprint arXiv:2212.01944, 2022.\n[Yang et al., 2023] Yunhao Yang, Neel P Bhatt, Tyler In-\ngebrand, William Ward, Steven Carr, Zhangyang Wang,\nand Ufuk Topcu. Fine-tuning language models using for-\nmal methods feedback. arXiv preprint arXiv:2310.18239,\n2023.\nSupplementary Materials\nA\nComparison of closed-loop and Open-loop\nLARL-RM\nOur proposed algorithm can be modified to perform in an\nopen-loop or closed-loop manner with respect to LLM-\ngenerated DFA, meaning that if a counterexample is found\nLARM-RM has the capability to either update the LLM-\ngenerated DFA D or keep the original LLM-generated DFA.\nLARL-RM has two options either continuing the learning\nprocess of the reward machine without updating the LLM-\ngenerated DFA, referred to as open-loop.\nIt can also call\nthe LLM again and generate a new DFA based on the coun-\nterexample, referred to as the closed loop. We show our pro-\nposed algorithm’s convergence to optimal policy under both\noptions. We demonstrate the open-loop configuration in Fig-\nure 15.\nCounterexample \nof RM found?\nUser\nPrompt\nLLM\nLLM-generated DFA\nRL algorithm\nLearned RM\nPrompt update using \ncounterexample\nYes\nCounterexample of \nLLM-generated DFA \nfound?\nLearning the RM\nNo\nNo\nYes\nFigure 15: LARL-RM open-loop configuration, counterexample of\nLLM-generated DFAs are not used to update the prompt (dashed\narrow).\nA.1\nCase Study 1\nWe investigate the two open-loop and closed-loop methods\nin the Case Study 1.\nFirst, we demonstrate the effect of\nincompatible LLM-generated DFA with compatible LLM-\ngenerated DFA in open-loop conditions. Figure 16 illustrates\nthe convergence to optimal policy for open-loop configura-\ntion.\n0\n2500\n5000\n7500\n10000\n12500\n15000\n17500\n20000\nStep\n0.2\n0.4\n0.6\n0.8\n1.0\nReward\nWithout LLM-generated DFA\nOpen-loop W/ compatible LLM-generated DFA\nClose-loop W/ compatible LLM-generated DFA\nFigure 16: LARL-RM open-loop and closed-loop configurations’\naveraged rewards for Case Study 1 when the LLM-generated DFA\nis compatible with the ground truth reward machine. Rewards are\nfor 5 independent runs, averaged at each 20 step.\nFigure 16 demonstrates that in LARL-RM open-loop con-\nfiguration if the LLM-generated DFA is compatible it con-\nverges to the optimal policy faster than when there is no\nLLM-generated DFA. However, if the LLM-generated DFA\nis incompatible then it might take longer to converge to the\noptimal policy. Now we consider the closed-loop configura-\ntion so that the DFA if incompatible then be used to update\nthe prompt f. Figure 17 demonstrates the LARL-RM conver-\ngence to the optimal policy when the closed-loop configura-\ntion is applied.\n0\n2500\n5000\n7500\n10000\n12500\n15000\n17500\n20000\nStep\n0.2\n0.4\n0.6\n0.8\n1.0\nReward\nWithout LLM-generated DFA\nOpen-loop W/ incompatible LLM-generated DFA\nClose-loop W/ incompatible LLM-generated DFA\nFigure 17: LARL-RM open-loop and closed-loop configurations’\naveraged rewards for Case Study 1 when the LLM-generated DFA\nis incompatible with the ground truth reward machine. Rewards are\nfor 5 independent runs, averaged at each 20 step.\nFigure 17 demonstrates that the closed-loop configuration\nis capable of updating the prompt f considering the coun-\nterexample such that LARL-RM has a better chance of con-\nverging faster than when there is no LLM-generated DFA.\nA.2\nCase Study 2\nWe further investigate the effect of open-loop and closed-loop\nconfigurations. The same configuration for the open-loop as\nillustrated in Figure 15 is applied here. The open-loop con-\nfiguration as shown in Figure 18 is converging to the optimal\npolicy faster than the case with no LLM-generated DFA if\nthe LLM-generated DFA is compatible with the ground truth\nreward machine.\n0\n10000\n20000\n30000\n40000\n50000\nStep\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nReward\nWithout LLM-generated DFA\nOpen-loop W/ compatible LLM-generated DFA\nClose-loop W/ compatible LLM-generated DFA\nFigure 18: LARL-RM open-loop and closed-loop configurations re-\nward for Case Study 2 when the LLM-generated DFA is compatible\nwith the ground truth reward machine. Rewards are for 5 indepen-\ndent runs, averaged at each 20 step.\nIf we apply the closed-loop configuration to Case Study 2,\nthen as expected it can converge to the optimal policy faster\nthan when there is no LLM-generated DFA and even if the\ninitial LLM-generated DFA is incompatible with the ground\ntruth reward machine. Figure 19 demonstrates the closed-\nloop configuration.\n0\n10000\n20000\n30000\n40000\n50000\nStep\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nReward\nWithout LLM-generated DFA\nOpen-loop W/ incompatible LLM-generated DFA\nClose-loop W/ incompatible LLM-generated DFA\nFigure 19: LARL-RM open-loop and closed-loop configurations re-\nward for Case Study 2 when the LLM-generated DFA is incompat-\nible with the ground truth reward machine. Rewards are for 5 inde-\npendent runs, averaged at each 20 step.\nAs illustrated in Figure 19, if the closed-loop configura-\ntion is applied then the LARL-RM has a better chance to\nconverge to the optimal policy faster than when there is no\nLLM-generated DFA.\nB\nTheoretical Guarantee of the LARL-RM\nHere we demonstrate the convergence of the proposed algo-\nrithm to the optimal policy.\nProof of Lemma 1. We first show that the set D remains sta-\ntionary after a finite amount of time (i.e., that no new LLM-\ngenerated DFA is added or removed), assuming that all tra-\njectories will be visited infinitely often. To this end, we ob-\nserve the following: if an LLM-generated DFA added to D is\ncompatible, then it will remain in D indefinitely. The reason\nis that there is no counterexample contradicting the LLM-\ngenerated DFA, meaning that the check-in Line 12 is never\ntriggered. On the other hand, if an LLM-generated DFA is\nnot consistent, then the algorithm eventually detects a trajec-\ntory witnessing this fact. Once this happens, it removes the\ncorresponding LLM-generated DFA from the set D.\nObserve now that the algorithm decrements J by one ev-\nery time it adds an LLM-generated DFA to D, and it only\ndoes so as long as J > 0. Thus, the total number of LLM-\ngenerated DFA that are generated during the run of the algo-\nrithm is bounded by J . Consequently, the set D no longer\nchanges after a finite period, because the algorithm has ei-\nther identified the true reward machine or all incompatible\nLLM-generated DFAs, have been removed and the budget J\nis exhausted.\nOnce D becomes stationary, an argument analogous to that\nin the proof of Lemma 1 in Neider et al.’s work [Neider et al.,\n2021] shows that the algorithm will eventually learn the true\nreward machine. Intuitively, as long as the current hypothesis\nis not equivalent to the true reward machine, there exists a\n“short” trajectory witnessing the fast. The episode length is\nchosen carefully such that such a witness will eventually be\nencountered and added to X. In the worst case, all trajectories\nof Episodelength will eventually be added to X, at which\npoint the learning algorithm is guaranteed to learn the true\nreward machine.\nSince our proposed algorithm learns the minimal reward\nmachine that is consistent with X and compatible with D ∈\nD, we need to use an automata learning method based on SAT\nwhich is used to verify parametric systems [Neider, 2014].\nThe underlying idea is to reduce the learning task to a series\nof satisfiability checks of formulas for propositional logic. In\nother words, we build and solve a sequence of propositional\nformulas ΦX,D\nn\nfor n > 0 such that the following properties\nhold [Neider et al., 2021]:\n• the satisfiability of ΦX,D\nn\nis contingent upon the exis-\ntence of a reward machine with n states, such that it is\nconsistent with X and compatible with D ∈D. This\ncondition holds true if and only if such a reward machine\nexists.\n• if ΦX,D\nn\nis satisfiable, it implies that a satisfying assign-\nment contains enough information to construct a reward\nmachine with n states that is both consistent and com-\npatible.\nWe can now build and solve ΦX,D\nn\nstarting from n = 1\nand increasing n until it becomes satisfiable.\nUsing this\nmethod our proposed algorithm constructs a minimal reward\nmachine that is consistent with X and compatible with the\nLLM-generated DFA D.\nTo ease the notation in the remainder, we use A: p\nw\n−→q\nto abbreviate a run of the reward machine A on the input-\nsequence w that starts in p and leads to q. By definition, we\nhave A: p\nε−→p for the empty sequence ε and every state p.\nWe later also use this notation for DFAs.\nTo encode the reward machine A = ⟨V, VI, 2P, RX, δ, σ⟩\nin propositional logic where set V of states and initial state\nvI is fixed, the reward machine A is uniquely determined by\nthe transitions δ and the output function σ. The size of states\n|V | = n and the initial state vI is fixed, then to encode the\ntransition function and the set representing the final states,\nwe will introduce two propositional variables namely dp,l,v\nwhere p, v ∈V and l ∈2P and op,l,r for p ∈V , l ∈2P, and\nr ∈RX. We apply the constraints in (1) and (2) to ensure that\nthe variables dp,l,v and op,l,r encode deterministic functions.\n^\np∈V\n^\nl∈2P\n\u0014\u0002 _\nq∈V\ndp,l,q\n\u0003\n∧\n\u0002\n^\nq̸=q′∈V\n¬dp,l,q ∨¬dp,l,q′\u0003\u0015\n(1)\n^\np∈V\n^\nl∈2P\n\u0014\u0002 _\nr∈RX\nop,l,r\n\u0003\n∧\n\u0002\n^\nr̸=r′∈RX\n¬op,l,r ∨¬op,l,r′\u0003\u0015\n(2)\nWe denote the conjunction of constraints (1) and (2) by\nΦRM\nn\nwhich we later on use to show the consistency of\nthe learned reward machine with X. To apply constraints\nfor consistency with samples in propositional logic, we pro-\npose auxiliary variables as xλ,ρ for every (λ, ρ) ∈Pref (X)\nwhere q ∈V , and for a trace τ = (l1 . . . lk, r1 . . . rk) ∈\n(2P)∗× R∗we define the set prefixes of τ by Pref (τ) =\n{(l1 . . . li, r1 . . . ri) ∈(2P)∗× R∗| 0 ≤i ≤k} ( (ε, ε) ∈\nPref (τ) is always correct). The value of xλ,ρ is set to true\nif and only if the prospective reward machine reaches states\nq after reading λ. To obtain the desired meaning, we add the\nfollowing constraints:\nxε,qI ∧\n^\np∈V \\{qI}\n¬xε,p\n(3)\n^\n(λl,ρr)∈Pref (X)\n^\np,q∈V\n(xλ,p ∧dp,l,q) →xλl,q\n(4)\n^\n(λl,ρr)∈Pref (X)\n^\np∈V\nxλ,p →op,l,r\n(5)\nIn order to ensure that the prospective reward machine AI\nand the LLM-generated DFA D are synchronized we add\nauxiliary variables yD\nv,v′ for v ∈V and v′ ∈VD. IF there\nis a label sequence λ where AI : vI\nλ−→q and D : qI,D\nλ−→q′\nis set to true. We denote the conjunction of constraints (3)-(5)\nby ΦX\nn which we later on use to show the consistency of the\nlearned reward machine with X. To obtain this behavior, we\nadd the following constraints:\nyD\nqI,q′\nl,D\n(6)\n^\np,q∈V\n^\nl∈2P\n^\nδD(p′,l)=q′\n(yD\np,p′ ∧dp,l,q) →yD\nq,q′\n(7)\n^\np∈V\n^\nδD(p′,l)=q′\nq′ /∈FD\nyD\np,p′ →¬\n_\nr∈RX\nr>0\nop,l,r\n(8)\nWe denote the conjunction of constraints (6)-(8) by ΦX\nn\nwhich we later use to show the consistency of the learned\nreward machine with X. Theorem 2 demonstrates the consis-\ntency of the LLM-generated DFA with the Algorithm 2. W e\ndenote the conjunction of Formulas (3), (4), and (5) by ΦD\nn .\nTheorem 2. Consider X ⊂(2P)+×R+ be sample and D be\na finite set of LLM-generated DFAs that are compatible with\nX. Then, the following holds:\n1. If we have I |= ΦX,D\nn\n, then the reward machine AI\nis consistent with X and compatible with the LLM-\ngenerated DFA D ∈D.\n2. If there exists a reward machine with n states that is con-\nsistent with X as well as compatible with each D ∈D,\nthen ΦX,D\nn\nis satisfiable.\nProof for Theorem 2 will follow in Section C after discus-\nsion of prerequisites.\nC\nLearning Reward Machines\nIn this section, we prove the correctness of our SAT-based\nalgorithm for learning reward machines. To this end, we show\nthat the propositional formula ΦX\nn and ΦD\nn have the desired\nmeaning. We begin with the formula ΦX\nn , which is designed\nto enforce that the learned reward machine is consistent with\nthe sample X.\nLemma 2. Let I |= ΦRM\nn\n∧ΦX\nn and AI the reward machine\ndefined above. Then, AI is consistent with X (i.e., AI(λ) =\nρ for each (λ, ρ) ∈X).\nProof. Let I |= ΦRM\nn\n∧ΦX\nn and AI the reward machine\nconstructed as above. To prove Lemma 2, we show the fol-\nlowing, more general statement by induction over the length\nof prefixes (λ, ρ) ∈Pref (X): if AI : qI\nλ−→q, then\n1. I(xλ,q) = 1; and\n2. AI(λ) = ρ.\nLemma 2 then follows immediately from Part 2 since X ⊆\nPref (X).\nBase case: Let (ε, ε) ∈Pref (X). By definition of runs,\nwe know that AI : qI\nε−→qI is the only run of A on\nthe empty word. Similarly, Formula (3) guarantees that\nthe initial state qI is the unique state q ∈V for which\nI(xε,q) = 1 holds.\nBoth observations immediately\nprove Part 1. Moreover, A(ε) = ε holds by definition\nthe semantics of reward machines, which proves Part 2.\nInduction step: Let (λl, ρr) ∈Pref (X).\nMoreover, let\nAI : qI\nλ−→p\nl−→q be the unique run of A on λ. By\napplying the induction hypothesis, we then obtain that\nboth I(xλ,p) = 1 and A(λ) = ρ hold.\nTo prove Part 1, note that AI contains the transition\nδ(p, l) = q since this transition was used in the last step\nof the run on λ. By construction of AI, this can only be\nthe case if I(dp,l,q) = 1. Then, however, Formula (4)\nimplies that I(xλl,q) = 1 because I(xλ,p) = 1 (which\nholds by induction hypothesis). This proves Part 1.\nTo prove Part 2, we exploit Formula (5). More precisely,\nFormula (5) guarantees that if I(xλ,p) = 1 and the next\ninput is l, then I(op,l,r) = 1. By construction of AI,\nthis means that σ(q, l) = r. Hence, AI outputs r in the\nlast step of the run on λl. Since AI(λ) = ρ (which holds\nby induction hypothesis), we obtain AI(λl) = ρr. This\nproves Part 2.\nThus, AI is consistent with X.\nNext, we show that the formula ΦD\nn ensures that the learned\nreward machine is compatible with the LLM-generated DFA\nD.\nLemma 3. Let I\n|=\nΦRM\nn\n∧ΦD\nn and AI the reward\nmachine defined above.\nThen, AI is compatible with D\n(i.e., AI(ℓ1ℓ2 . . . ℓk) = r1r2 . . . rk and rk > 0 implies\nℓ1ℓ2 . . . ℓk\n∈\nL(D) for every nonempty label sequence\nℓ1ℓ2 . . . ℓk).\nProof. Let I |= ΦRM\nn\n∧ΦD\nn and AI the reward machine de-\nfined above. We first show that AI : qI\nλ−→q and D: qI,D\nλ−→\nq′ imply I(yD\nq,q′) = 1 for all label sequences λ ∈2P ∗. The\nproof of this claim proceeds by induction of the length of la-\nbel sequences, similar to Part 2 in the proof of Lemma 2.\nBase case: Let λ = ε. By definition of runs, the only runs\non the empty label sequence are AI : qI\nε−→qI and\nD: qI,D\nε−→qI,D. Moreover, Formula (6) ensures that\nI(yD\nqI,qI,D) = 1, which proves the claim.\nInduction step: Let λ = λ′′l. Moreover, let AI : qI\nλ′′\n−−→\np\nl−→q and D: qI,D\nλ′′\n−−→p′\nl−→q′ be the runs of AI and\nD on λ = λ′′l, respectively. By induction hypothesis,\nwe then know that I(yD\np,p′) = 1. Moreover, AI contains\nthe transition δ(p, l) = q because this transition was\nused in the last step of the run of AI on λ. By construc-\ntion of AI, this can only be the case if I(dp,l,q) = 1.\nIn this situation, Formula 7 ensures I(yD\nq,q′) = 1 (since\nalso δD(p′, l) = q′), which proves the claim.\nLet now λ = ℓ1ℓ2 . . . ℓk be a nonempty label sequence\n(i.e., k ≥1). Moreover, let AI : qI\nl1...lk−1\n−−−−−→p\nlk\n−→q be the\nrun of AI on λ and D: qI,D\nl1...lk−1\n−−−−−→p′\nlk\n−→q′ the run of\nD on λ. Our induction shows that I(yp,p′) = 1 holds in this\ncase.\nTowards a contradiction, assume that AI(ℓ1ℓ2 . . . ℓk) =\nr1r2 . . . rk, rk > 0, and ℓ1ℓ2 . . . ℓk /∈L(D). In particu-\nlar, this means q′ /∈FD.\nSince δD(p′, lk) = q′ (which\nwas used in the last step in the run of D on ℓ1ℓ2 . . . ℓk) and\nI(yD\np,p′) = 1 (due to the induction above), Formula (8) en-\nsures that I(op,l,r) = 0 for all r ∈RX with r > 0. However,\nFormula (2) ensures that there is exactly one r ∈RX with\nI(op,l,r) = 1. Thus, there has to exist an r ∈RX such\nthat r ≤0 and I(op,l,r) = 1. By construction of AI, this\nmeans that the last output rk of AI on reading lk must have\nbeen rk ≤0. However, our assumption was rk > 0, which\nis a contradiction. Thus, AI is compatible with the LLM-\ngenerated DFA D.\nWe can now prove Theorem 2 (i.e., the correctness of our\nSAT-based learning algorithm for reward machines).\nProof of Theorem 2. The proof of Part 1 follows immediately\nfrom Lemma 2 and Lemma 3.\nTo prove Part 2, let A = (V, qI, 2P, M, δ, σ) be a reward\nmachine with n states that is consistent with X and compat-\nible with each D ∈D. From this reward machine, we can\nderive a valuation I for the variables dp,l,q and op,l,r in a\nstraightforward way (e.g., setting I(dp,l,q) = 1 if and only\nis δ(p, l) = q). Moreover, we obtain a valuation for the vari-\nables xλ,p from the runs of (prefixes) of traces in the sample\nX, and valuations for the variables yD\np,p′ from the synchro-\nnized runs of A and D for each D ∈D. Then, I indeed\nsatisfies ΦX,D\nn\n.\nD\nConvergence to an Optimal Policy\nIn this section, we prove that LARM-RM almost surely con-\nverges to an optimal policy in the limit. We begin by defining\nattainable trajectories—trajectories that can possibly appear\nin the exploration of an agent.\nDefinition 5. Let\nM = (S, sI, A, p, R, γ, P, L) be a labeled MDP and\nm ∈N a natural number. A trajectory ζ = s0a1s1 . . . aksk ∈\n(S × A)∗× S is said to be m-attainable if k ≤m and\np(si−1, ai, si) > 0 for each i ∈{1, . . . , k}. Moreover, a\ntrajectory ζ is called attainable if there exists an m ∈N such\nthat ζ is m-attainable. Analogously, we call a label sequence\nλ = l1 . . . lk (m-)attainable if there exists an (m-)attainable\ntrajectory s0a1s1 . . . sk−1aksk such that li = L(si−1, ai, si)\nfor each i ∈{1, . . . , k}\nAn induction shows that LARM-RM almost surely ex-\nplores every attainable trajectory in the limit (i.e., with prob-\nability 1 when the number of episodes goes to infinity. We\nrefer the reader to the extended version of [Xu et al., 2020b]1\nfor a detailed proof.\nLemma 4. Given m ∈N, LARM-RM with episodelength ≥\nm almost surely explores every m-attainable trajectory in the\nlimit.\nAs an immediate consequence of Lemma 4, we obtain that\nLARM-RM almost sure explores every (m-)attainable label\nsequence in the limit as well.\n1The extended version of [Xu et al., 2020b] can be found on\narXiv at https://arxiv.org/abs/1909.05912\nCorollary 1. Given m ∈N, LARM-RM with episodelength ≥\nm almost surely explores every m-attainable label sequence\nin the limit.\nIn this paper, we assume that all possible label sequences\nare attainable. We relax this restriction later as in [Neider et\nal., 2021] .\nTo prove Lemma 1, we show a series of intermediate re-\nsults. We begin by reminding the reader of a well-known\nfact from automata theory, which can be found in most text-\nbooks [Shallit, 2008].\nTheorem 3. Let A1 and A2 be two DFAs with L(A1) ̸=\nL(A2).\nThen, there exists a word w of length at most\n|A1|+|A2|−1 such that w ∈L(A1) if and only if w /∈L(A2).\nWe now observe that every reward machine A can be\ntranslated into a DFA AA that is “equivalent” to the re-\nward machine [Xu et al., 2020b]. This DFA operates over\nthe combined alphabet 2P × M and accepts a sequence\n(ℓ1, r1) . . . (ℓk, rk) if and only if A outputs the reward se-\nquence r1r2 . . . rk on reading the label sequence ℓ1ℓ2 . . . ℓk.\nLemma 5 ([Xu et al., 2020b]). Given a reward machine A =\n(V, qI, 2P, M, δ, σ), one can construct a DFA AA with |A|+1\nstates such that L(AA) = {(l1, r1) . . . (lk, rk) ∈(2P × R) |\nA(l1 . . . lk) = r1 . . . rk}.\nProof. Let A = (QA, qI,A, 2P, R, δA, σA) be a reward ma-\nchine. Then, we define a DFA AA = (Q, qI, 2P × R, δ, F)\nby\n• Q = QA ∪{⊥}, where ⊥/∈QA;\n• qI = qI,A;\n• δ\n\u0000p, (ℓ, r)\n\u0001\n=\n\u001aq\nif δA(p, ℓ) = q and σA(p, ℓ) = r;\n⊥\notherwise\n• FA = QA.\nIn this definition, ⊥is a new sink state to which AA moves\nif its input does not correspond to a valid input-output pair\nproduced by A. A straightforward induction over the length\nof inputs to AA shows that it indeed accepts the desired lan-\nguage. In total, AA has |A| + 1 states.\nNext, we show that if two reward machines are semanti-\ncally different, then we can bound the length of a trace wit-\nnessing this fact.\nLemma 6. Let A1 and A2 be two reward machines. If A1 ̸=\nA2, then there exists a label sequence λ of length at most\n|A1| + |A2| + 1 such that A1(λ) ̸= A2(λ).\nProof of Lemma 6. Consider the DFAs AA1 and AA2 ob-\ntained from Lemma 5.\nThese DFAs have |A1| + 1 and\n|A2| + 1 states, respectively. If A1 ̸= A2, then L(AA1) ̸=\nL(AA2). Thus, applying Theorem 3 yields a sequence w =\n(l1, r1) . . . (lk, rk) with\nk ≤|A1| + 1 + |A2| + 1 −1 = |A1| + |A2| + 1\nsuch that w ∈L(AA1) if and only if w /∈L(AA2). By\nLemma 5, the label sequence λ = l1 . . . lk has the desired\nproperty.\nSimilar to Lemma 6, we can show that if an LLM-\ngenerated DFA is not compatible with a reward machine, we\ncan also bound the length of a label sequence witnessing this\nfact.\nLemma 7. Let A be a reward machine and D an LLM-\ngenerated DFA. If D is not compatible with A, then there\nexists a label sequence l1 . . . lk with k ≤2(|A|+1)·|D| such\nthat A(l1 . . . lk) = r1 . . . rk, rk > 0, and l1 . . . lk /∈L(D).\nProof. Let A be a reward machine and R ⊂R the fi-\nnite set of rewards that A can output.\nMoreover, let\nD = (VD, vI,D, 2P, δD, FD) be an LLM-generated DFA.\nOur proof proceeds by constructing four DFAs and a subse-\nquent analysis of the final DFA to derive the desired bound.\nFirst, we construct the DFA AA\n=\n(VA, vI,A, 2P ×\nR, δA, FA)\naccording\nto\nLemma\n5.\nRecall\nthat\n(l1, r1) . . . (lk, rk) ∈L(AA) if and only if A(l1 . . . lk) =\nr1 . . . rk. Moreover, AA has |A| + 1 states.\nSecond, we modify the DFA AA such that it only ac-\ncepts sequences (l1, r1) . . . (lk, rk) with rk > 0.\nTo this\nend, we augment the state space with an additional bit b ∈\n{0, 1}, which tracks whether the most recent reward was 0\nor greater than 0. More formally, we define a DFA A′\nA =\n(V ′\nA, v′\nI,A, 2P × R, δ′\nA, F ′\nA) by\n• V ′\nA = VA × {0, 1};\n• v′\nI,A = (vI,A, 0);\n• δ′\nA\n\u0000(v, b), (l, r)\n\u0001\n=\n\u0000δA(v, (l, r)), b\n\u0001\nwhere b = 1 if and\nonly if r > 0; and\n• F ′\nA = FA × {1}.\nIt is not hard to verify that A′\nA indeed has the desired property.\nMoreover, by Lemma 5, A′\nA has 2(|A| + 1) states.\nThird, we apply “cylindrification” to the LLM-generated\nDFA D, which works over the alphabet 2P, to match the in-\nput alphabet 2P × R of A′\nA. Our goal is to construct a new\nDFA D′ = (V ′\nD, v′\nI,D, 2P × R(X), δ′\nD, F ′\nD) that accepts a\nsequence (l1, r1) . . . (lk, rk) if and only if l1 . . . lk ∈L(D)\nfor every reward sequence r1 . . . rk (cylindrification can be\nthought of as the inverse of the classical projection operation).\nWe achieve this by replacing each transition δD(v, l) = v′\nin D with |R(X)| many transitions of the form δ′\nD\n\u0000v, (l, r)\n\u0001\nwhere r ∈R. Formally, we define the DFA D′ by\n• V ′\nD = VD;\n• v′\nI,D = vI,D;\n• δ′\nD\n\u0000v, (l, r)\n\u0001\n= δD(v, l) for each r ∈R; and\n• F ′\nD = FD.\nIt is not hard to verify that D′ has indeed the desired property\nand its size is |D|.\nFourth, we construct the simple product DFA of A′\nA and\nD′. This DFA is given by A = (V, vI, 2P × R(X), δ, F)\nwhere\n• V = V ′\nA × V ′\nD;\n• vI = (v′\nI,A, v′\nI,D);\n• δ\n\u0000(v1, v2), (l, r)\n\u0001\n=\n\u0000δ′\nA(v1, (l, r)), δ′\nD(v2, (l, r))\n\u0001\n; and\n• F = F ′\nA × (Q′\nD \\ F ′\nD).\nBy construction of A′\nA and D′, is is not hard to see\nthat A accepts a sequence (l1, r1) . . . (lk, rk) if and only if\nA(l1 . . . lk) = r1 . . . rk with rk > 0 and l1 . . . lk /∈L(D)—\nin other words, L(A) contains all sequences that witness that\nD is not compatible with A. Moreover, A has 2(|A|+1)·|D|\nstates.\nIt is left to show that if D is not compatible with A, then\nwe can find a witness with the desired length. To this end, it\nis sufficient to show that if L(A) ̸= ∅, then there exists a se-\nquence (l1, r1) . . . (lk, rk) ∈L(A) with k ≤2(|A| + 1) · |D|.\nThis fact can be established using a simple pumping argu-\nment. To this end, assume that (l1, r1) . . . (lk, rk) ∈L(A)\nwith k > 2(|A|+1)·|D|. Then, there exists a state v ∈V such\nthat the unique accepting run of A on (l1, r1) . . . (lk, rk) vis-\nits v twice, say at the positions i, j ∈{0, . . . k} with i < j. In\nthis situation, however, the DFA A also accepts the sequence\n(l1, r1) . . . (li, ri)(lj+1, rj+1) . . . (lk, rk), where we have re-\nmoved the “loop” between the repeating visits of v. Since\nthis new sequence is shorter than the original sequence, we\ncan repeatedly apply this argument until we arrive at a se-\nquence (l′\n1, r′\n1) . . . (l′\nℓ, r′\nℓ) ∈L(A) with ℓ≤2(|A| + 1) · |D|.\nBy construction of A, this means that A(l′\n1 . . . l′\nℓ) = r′\n1 . . . r′\nℓ,\nrℓ> 0, and l′\n1 . . . l′\nℓ/∈L(D), which proves the claim.\nWith these intermediate results at hand, we are now ready\nto prove Lemma 1.\nProof of Lemma 1. Let (X0, D0), (X1, D1), . . . be the se-\nquence of samples and sets of LLM-generated DFAs that\narise in the run of LARM-RM whenever a new counterex-\nample is added to X (in Lines 9 and 10 of Algorithm 2) or\nan LLM-generated DFA is removed from the set D (Lines 12\nand 13 of Algorithm 2). Moreover, let A0, A1, . . . be the cor-\nresponding sequence of reward machines that are computed\nfrom (Xi, Di). Note that constructing a new reward machine\nis always possible because LARM-RM makes sure that all\nLLM-generated DFAs in the current set D are compatible\nwith the traces in the sample X.\nWe first observe three properties of these sequences:\n1. The true reward machine A (i.e., the one that encodes\nthe reward function R) is consistent with every sample\nXi that is generated during the run of LARM-RM. This\nis due to the fact that each counterexample is obtained\nfrom an actual exploration of the MDP and, hence, cor-\nresponds to the “ground truth”.\n2. The sequence X0, X1, . . . grows monotonically (i.e.,\nX0 ⊆X1 ⊆· · · ) because LARM-RM always adds\ncounterexamples to X and never removes them (Lines 9\nand 10).\nIn fact, whenever a counterexample (λ′, ρ)\nis added to Xi to form Xi+1, then (λ′, ρ) /∈Xi (i.e.,\nXi ⊊Xi+1). To see why this is the case, remember\nthat LARM-RM always constructs hypotheses that are\nconsistent with the current sample (and the current set\nof LLM-generated DFAs). Thus, the current reward ma-\nchine Ai is consistent with Xi, but the counterexample\n(λ′, ρ) was added because Ai(λ′) ̸= ρ. Thus, (λ′, ρ)\ncannot have been an element of Xi.\n3. The sequence D0, D1, . . . decreases monotonically (i.e.,\nD0 ⊇D1 ⊇· · · ) because LARM-RM always re-\nmoves LLM-generated DFAs from D and never adds\nany (Lines 12 and 13).\nThus, there exists a position\ni⋆∈N at which this sequence becomes stationary, im-\nplying that Di = Di+1 for i ≥i⋆.\nSimilar to Property 1, we now show that each LLM-\ngenerated DFA in the set Di, i ≥i⋆, is compatible with the\ntrue reward machine A. Towards a contradiction, let D ∈Di\nbe an LLM-generated DFA and assume that D is not compat-\nible with A. Then, Lemma 7 guarantees the existence of a\nlabel sequence l1 . . . lk with\nk ≤2(|A| + 1) · |D|\n≤2(|A| + 1) · nmax\nsuch that A(l1, . . . lk) = r1 . . . rk,rk > 0, and l1 . . . lk /∈\nL(D). Since we assume all label sequences to be attainable\nand have chosen episodelength ≥2(|A| + 1) · nmax, Corol-\nlary 1 guarantees that LARM-RM almost surely explores this\nlabel sequence in the limit. Once this happens, LARM-RM\nremoves D from the set D (Lines 12 and 13), which is dic-\ntion to the fact that the sequence Di⋆, Di⋆+1, . . . is stationary\n(Property 3). Hence, we obtain the following:\n4. Every LLM-generated DFA in Di, i ≥i⋆, is compatible\nwith the true reward machine A.\nNext, we establish the three additional properties about the\nsub-sequence Ai⋆, Ai⋆+1 of hypotheses starting at position\ni⋆:\n5. The size of the true reward machine A is an upper bound\nfor the size of Ai⋆(i.e., |Ai⋆| ≤|A|). This is due to the\nfact that A is consistent with every sample Xi (Prop-\nerty 1), every LLM-generated DFA in Di, i ≥i⋆, is\ncompatible with A (Property 4), and LARM-RM always\ncomputes minimal consistent reward machines.\n6. We have |Ai| ≤|Ai+1| for all i ≥i⋆. Towards a con-\ntradiction, assume that |Ai| > |Ai+1|. Since LARM-\nRM always computes consistent reward machines and\nXi ⊊Xi+1 if i ≥i⋆(see Property 2), we know that\nAi+1 is not only consistent with Xi+1 but also with Xi\n(by definition of consistency). Moreover, LARM-RM\ncomputes minimal consistent reward machines. Hence,\nsince Ai+1 is consistent with Xi and |Ai+1| < |Ai|, the\nreward machine Ai is not minimal, which is a contradic-\ntion.\n7. We have Ai ̸= Aj for i ≥i⋆and j ∈{i⋆, . . . , i}—\nin other words, the reward machines generated during\nthe run of LARM-RM after the i⋆-th recomputation\nare semantically distinct. This is a consequence of the\nfacts that (λ′\nj, ρj) was a counterexample to Aj (i.e.,\nAj(λ′\nj) ̸= ρj) and that LARM-RMalways constructs\nconsistent reward machines (which implies Ai(λ′\ni) =\nρi).\nProperties 5 and 6 now provide |A| as an upper bound on\nthe size of any reward machine that LARM-RM constructs\nafter the i⋆-th recomputation. Since there are only finitely\nmany reward machines of size at most |A|, Property 7 im-\nplies that there exists a j⋆≥i⋆after which no new reward\nmachine is learned. Hence, it is left to show that Aj⋆= A\n(i.e., Aj⋆(λ) = A(λ) for all label sequences λ).\nTowards a contradiction, let us assume that Aj⋆̸= A.\nThen, Lemma 6 guarantees the existence of a label sequence\nλ = l1 . . . lk with\nk ≤|Aj⋆| + |A| + 1\n≤2|A| + 1\n≤2(|A| + 1) · nmax\nsuch that Aj⋆(λ) ̸= A(λ).\nSince we assume all label sequences to be attainable and\nhave chosen episodelength ≥2(|A| + 1) · nmax, Corollary 1\nguarantees that LARM-RM almost surely explores this label\nsequence in the limit. Thus, the trace (λ, ρ), where ρ = A(λ),\nis almost surely returned as a new counterexample, resulting\nin a new sample Xj⋆+1. This, in turn, causes the construction\nof a new reward machine, which contradicts the assumption\nthat no further reward machine is generated. Thus, the reward\nmachine Aj⋆is equivalent to the true reward machine A.\nLet us finally turn to the proof of Theorem 1 (i.e., that\nLARM-RM almost surely converges to the optimal policy in\nthe limit). The key idea is to construct the product of the given\nMDP M and the true reward machine A. In this new MDP\nM′, the reward function is in fact Markovian since the reward\nmachine A encodes all necessary information in its states\n(and, hence, in the product M′). Thus, the fact that classi-\ncal Q-learning almost surely converges to an optimal policy\nin the limit also guarantees the same for LARM-RM. We re-\nfer the reader to the extended version of [Xu et al., 2020b] for\ndetailed proof.\nE\nUnattainable Label Sequences\nLet us now consider the case that not all labels can be ex-\nplored by the agent (e.g., because certain label sequences are\nnot possible in the MDP). In this situation, two complications\narise:\n1. It is no longer possible to uniquely learn the true reward\nmachine A. In fact, any reward machine that agrees with\nA on the attainable traces becomes a valid solution.\n2. The notion of compatibility needs to reflect the attain-\nable traces. More precisely, the condition rk > 0 implies\nl1 . . . lk ∈L(D) now has to hold only for attainable la-\nbel sequences.\nBoth complications add a further layer of complexity, which\nmakes the overall learning problem—both Q-learning and the\nlearning of reward machines—harder. In particular, we have\nto adapt the episode length and also the size of the SAT en-\ncoding grows. In total, we obtain the following result.\nTheorem 4. Let M be a labeled MDP where all label se-\nquences are attainable and A the true reward machine encod-\ning the rewards of M. Moreover, let D = {D1, . . . , Dℓ} be\na set of LLM-generated DFAs and nmax = max1≤i≤ℓ{|Di|}.\nThen, LARM-RM with\nepisodelength ≥max\n\b\n2|M| · (|A| + 1) · nmax, |M|(|A| + 1)2\t\nalmost surely converges to an optimal policy in the limit. The\nsize of the formula ΦX,D\nn\ngrows linearly in |M|.\nIn order to prove Theorem 4, specifically the bound on the\nlength of episodes, we first need to introduce the nondeter-\nministic version of DFAs. Formally, a nondeterministic fi-\nnite automaton is a tuple A = (Q, qI, Σ, ∆, F) where Q, qI,\nΣ, F are as in DFAs and ∆⊂Q × Σ × Q is the transition\nrelation. Similar to DFAs, a run of an NFA A on a word\nu = a1 . . . ak is a sequence q0, . . . qk such that qo = qI and\n(qi−1, ai, qi) ∈∆for each i ∈{1, . . . , k}. In contrast to\nDFAs, however, NFAs permit multiple runs on the same in-\nput or even no run. Accepting runs and the language of NFAs\nare defined analogously to DFAs. Note that we use NFAs in-\nstead of DFAs because the former can be exponentially more\nsuccinct than the latter.\nSimilar to Lemma 5, one can translate an MDP M into an\nNFA AM with |M| states that accepts exactly the attainable\ntraces of an MDP M.\nLemma 8. Given a labeled MDP M, one can construct an\nNFA AM with at most 2|M| states that accept exactly the\nadmissible label sequences of M.\nThis construction is similar to Remark 1 of the extended\nversion of [Xu et al., 2020b] as proceeds as follows.\nProof. Given a labeled MDP M = (S, sI, A, p, R, γ, P, L),\nwe construct an NFA AM = (Q, qI, 2P, ∆, F) by\n• Q = S;\n• qI = sI;\n• (s, ℓ, s′) ∈∆if and only if there exists an action a ∈A\nwith L(s, a, s′) = ℓand p(s, a, s) > 0; and\n• F = S.\nA straightforward induction shows that λ ∈L(AM) holds if\nand only if λ is an attainable label of M.\nWe are now ready to prove Theorem 4.\nProof of Theorem 4. We first modify Lemma 7 slightly.\nMore precisely, we add another fifth step, where we build\nthe product of A with the cylindrification of the DFA AM. It\nis not hard to verify that this results in an NFA that accepts a\ntrace (l1 . . . lk, r1 . . . rk) if and only if l1 . . . lk is admissible\nand not in the language L(D) , A(l1 . . . lk) = r1 . . . rk, and\nrk > 0. This NFA has 2|M|(|A| + 1) · |D| states. A sim-\nilar pumping argument as in the proof of Lemma 7 can now\nbe used to show the existence of a witness of length at most\n2|M|(|A| + 1) · |D|.\nAnalogously, we can modify Lemma 6. We build the input-\nsynchronized product of the two DFAs AA1 and AA2 and the\nDFA AM. This results in an NFA with (|A1| + 1) · (|A2| +\n1) · |M| states. If A1 ̸= A2 on an attainable label sequence,\nthen we can find a sequence such that leads in this product to\na state where AM accepts (the label sequence is attainable),\nbut exactly one of A1 and A2 accepts. Using a pumping ar-\ngument as above, we can bound the length of such an input to\nat most (|A1| + 1) · (|A2| + 1) · |M|.\nMoreover, we have to modify Formula ΦX,D\nn\nto account\nfor the new situation.\nGiven AM = (QAM, qI,AM, 2P ×\nR, ∆AM, FAM), we first introduce new auxiliary variables\nzD\np,p′,p′′ where p′′ ∈QAM; we use these variables instead\nof the variables yp, p′D. Second, we replace Formulas (6),\n(7), and (8) with the following three formulas:\nzD\nqI,q′\nI,D,qI,AM\n(9)\n^\np,q∈Q\n^\nl∈2P\n^\nδD(p′,l)=q′\n^\n(p′′,l,q′′)∈∆AM\n(zD\np,p′,p′′ ∧dp,l,q) →zD\nq,q′,q′′\n(10)\n^\np∈Q\n^\nδD(p′,l)=q′\nq′ /∈FD\n^\n(p′′,l,q′′)∈∆AM\nq′′∈FAM\nzD\np,p′,p′′ →¬\n_\nr∈RXr>0\nop,l,r\n(11)\nClearly, the size of ΦX,D\nn\ngrows linearly in |M| as compared\nto the case that all label sequences are attainable.\nMore-\nover, it is not hard to verify that ΦX,D\nn\nhas indeed the de-\nsired meaning. More precisely, we obtain a result analogous\nto Lemma 1, but for the case that not all label sequences are\nattainable.\nOnce we have established that our learning algorithm\nlearns consistent and compatible reward machines, the over-\nall convergence of LARM-RM to an optimal policy and the\nbound on the length of episodes can then be proven analo-\ngously to Theorem 1.\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.CL"
  ],
  "published": "2024-02-11",
  "updated": "2024-02-11"
}