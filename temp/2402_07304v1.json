{
  "id": "http://arxiv.org/abs/2402.07304v1",
  "title": "Insights into Natural Language Database Query Errors: From Attention Misalignment to User Handling Strategies",
  "authors": [
    "Zheng Ning",
    "Yuan Tian",
    "Zheng Zhang",
    "Tianyi Zhang",
    "Toby Li"
  ],
  "abstract": "Querying structured databases with natural language (NL2SQL) has remained a\ndifficult problem for years. Recently, the advancement of machine learning\n(ML), natural language processing (NLP), and large language models (LLM) have\nled to significant improvements in performance, with the best model achieving\n~85% percent accuracy on the benchmark Spider dataset. However, there is a lack\nof a systematic understanding of the types, causes, and effectiveness of\nerror-handling mechanisms of errors for erroneous queries nowadays. To bridge\nthe gap, a taxonomy of errors made by four representative NL2SQL models was\nbuilt in this work, along with an in-depth analysis of the errors. Second, the\ncauses of model errors were explored by analyzing the model-human attention\nalignment to the natural language query. Last, a within-subjects user study\nwith 26 participants was conducted to investigate the effectiveness of three\ninteractive error-handling mechanisms in NL2SQL. Findings from this paper shed\nlight on the design of model structure and error discovery and repair\nstrategies for natural language data query interfaces in the future.",
  "text": "Insights into Natural Language Database Query Errors: From\nAttention Misalignment to User Handling Strategies\nZHENG NING∗, University of Notre Dame, USA\nYUAN TIAN∗, Purdue University, USA\nZHENG ZHANG, University of Notre Dame, USA\nTIANYI ZHANG, Purdue University, USA\nTOBY JIA-JUN LI, University of Notre Dame, USA\nQuerying structured databases with natural language (NL2SQL) has remained a difficult problem for years.\nRecently, the advancement of machine learning (ML), natural language processing (NLP), and large language\nmodels (LLM) have led to significant improvements in performance, with the best model achieving ∼85%\npercent accuracy on the benchmark Spider dataset. However, there is a lack of a systematic understanding of\nthe types, causes, and effectiveness of error-handling mechanisms of errors for erroneous queries nowadays.\nTo bridge the gap, a taxonomy of errors made by four representative NL2SQL models was built in this work,\nalong with an in-depth analysis of the errors. Second, the causes of model errors were explored by analyzing\nthe model-human attention alignment to the natural language query. Last, a within-subjects user study with\n26 participants was conducted to investigate the effectiveness of three interactive error-handling mechanisms\nin NL2SQL. Findings from this paper shed light on the design of model structure and error discovery and\nrepair strategies for natural language data query interfaces in the future.\nCCS Concepts: • Human-centered computing →Empirical studies in interaction design.\nAdditional Key Words and Phrases: Empirical study, human-computer interaction, error handling, database\nsystems\nACM Reference Format:\nZheng Ning, Yuan Tian, Zheng Zhang, Tianyi Zhang, and Toby Jia-Jun Li. 2023. Insights into Natural Language\nDatabase Query Errors: From Attention Misalignment to User Handling Strategies. 1, 1 (February 2023),\n32 pages.\n1\nINTRODUCTION\nData querying is an indispensable step in data analysis, sensemaking, and decision-making pro-\ncesses∗∗. However, traditional data query interfaces require users to specify their queries in a formal\nlanguage such as SQL, leading to significant learning barriers for non-expert users who have little\nprogramming experience [64, 68]. This problem becomes increasingly important in the Big Data\nera, given the rising needs for end users in many key domains including business, healthcare, public\n∗These authors contributed equally to this work.\n∗∗This work extends our previous research [53] published at IUI with new experiments on large language models (GPT-4)\nand a root cause analysis of the model attention and human attention.\nAuthors’ addresses: Zheng Ning, University of Notre Dame, Notre Dame, IN, USA, zning@nd.edu; Yuan Tian, Purdue\nUniversity, West Lafayette, IN, USA, tian211@purdue.edu; Zheng Zhang, University of Notre Dame, Notre Dame, IN,\nUSA, zzhang37@nd.edu; Tianyi Zhang, Purdue University, West Lafayette, IN, USA, tianyi@purdue.edu; Toby Jia-Jun Li,\nUniversity of Notre Dame, Notre Dame, IN, USA, toby.j.li@nd.edu.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee\nprovided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the\nfull citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored.\nAbstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires\nprior specific permission and/or a fee. Request permissions from permissions@acm.org.\n© 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nXXXX-XXXX/2023/2-ART $15.00\nhttps://doi.org/\n, Vol. 1, No. 1, Article . Publication date: February 2023.\narXiv:2402.07304v1  [cs.HC]  11 Feb 2024\n2\nZheng Ning, Yuan Tian, Zheng Zhang, Tianyi Zhang, and Toby Jia-Jun Li\npolicy, scientific research, etc. To address this problem, natural language (NL) data query interfaces\nallow users to express data queries in natural language. For example, a semantic parser can map\nthe user’s natural language query into a formal data query language such as SQL (NL2SQL). Such\nNL interfaces have shown the potential to lower the bar for data querying and support flexible data\nexploration for end users [5, 21, 75, 83].\nHowever, achieving robust NL2SQL parsing in realistic scenarios is difficult because of the\nambiguity in natural language and the complex structures (e.g., nested queries, joined queries) in\nthe target queries. For example, in Spider [90], a large-scale complex and cross-domain dataset for\nNL2SQL parsing, the accuracy of state-of-the-art models remained low in the 20% to 30% range for\nquite some time until 2019. In the past three years, advances in deep learning and large language\nmodels have brought us closer than ever to achieving useful performance on this important task—\nwith the use of state-of-the-art end-to-end models such as [23, 24, 31, 57], the accuracy quickly\nincreased to about 85%. However, the development in model performance appears to have stagnated\nin the 85% range recently, suggesting a bottleneck in model-only methods for NL2SQL.\nThis work focuses on the flip side of the 85% accuracy—the 15% erroneous queries. We started\nby understanding “What errors current NL2SQL models make.” Then, we investigate “How NL2SQL\nmodels made these errors” and “How users handle these errors” with different studies.\nWe first performed a comprehensive analysis of SQL errors made by representative state-of-the-\nart NL2SQL models and developed an axial taxonomy of those errors. We reproduced four represen-\ntative high-performing models with various structures from the Spider leaderboard∗—DIN-SQL +\nGPT-4 [58], SmBop + GraPPa (SmBop) [61], BRIDGE v2 + BERT (BRIDGE) [47], and GAZP + BERT\n(GAZP) [95]. For each model, we collected all model-generated queries for the Spider dataset whose\nexecution results varied from the ground truth results. Four authors conducted multiple rounds of\nqualitative coding and refinement on these errors to derive a taxonomy of the errors. The error anal-\nysis reveals that, despite having different model architectures and performance, NL2SQL errors origi-\nnate from a common set of queries and demonstrate a similar distribution across various error types.\nGiven the error distribution, we further investigate the potential reasons behind the error. Inspired\nby recent work [6, 32, 46], which showed that attention alignment can improve the performance\nin code summarization, machine translation, and visual feature representation, we hypothesize\nthat SQL errors generated by NL2SQL models derive from the misalignment between the model’s\nattention and human’s attention toward the natural language query. To validate this hypothesis,\ntwo authors (SQL experts) manually annotated important words (human attention) in the NL queries\nwhen they try to understand the query. The model-focused words (model attention) were obtained by\ncalculating the weight of each word that contributed to the model’s prediction using a perturbation-\nbased method [48]. The attention alignment was measured by computing the overlap between the\nhuman-focused words and the model-focused words. The results showed a significant difference\nin attention alignment between erroneous queries and correctly predicted queries, implying that\nNL2SQL errors are highly correlated with attention misalignment. The findings suggest the promise\nof future work in aligning model attention with human attention to improve model performance.\nTo support NL2SQL models in real-world deployment, it is also important to provide effective\nerror-handling mechanisms for users, which enable human users to discover and repair errors. In\nboth human-computer interaction (HCI) and natural language processing (NLP) communities, we\nhave seen efforts using different approaches. In general, there are three representative paradigms.\nFirst, following the task decomposition paradigm, strategies such as DIY [52] decompose a generated\nSQL statement into a sequence of incremental subqueries and provide step-by-step explanations to\nhelp users identify errors. Second, based on query visualization, approaches such as QueryVis [39],\n∗https://yale-lily.github.io/spider\n, Vol. 1, No. 1, Article . Publication date: February 2023.\n3\nQUEST [9], and SQLVis [51] seek to improve user understanding of the generated SQL statements\nby visualizing the structures and relations among entities and tables in a query. Third, using\nconversational agents, works such as [29, 84, 88] implement chatbots to communicate the model’s\ncurrent state with users and update the results with user feedback through dialogs. These interactive\napproaches help the model and humans to synchronize their attention. Consequently, humans gain\nconfidence in the decisions generated by the model, while the models can make better decisions\nunder human guidance.\nAlthough these approaches were shown to be useful in different contexts in individual evaluations,\nit is unclear how effective each approach is for users with various SQL expertise. Furthermore, as\nmost of these methods were evaluated with only simple NL2SQL errors, it is unclear how well\nthey will perform on errors made by state-of-the-art NL2SQL models on complex datasets such\nas Spider. Therefore, we selected three representative models and conducted a controlled user\nstudy (𝑁= 26) to investigate the effectiveness and efficiency of representative error discovery\nand repair approaches. Specifically, we selected (i) an explanation- and example-based approach\nthat supports fixing the SQL query through entity mapping between the natural language (NL)\nquestion and the generated query and discovering the error through a step-by-step NL explanation\napproach (DIY [52]), (ii) an explanation-based SQL visualization approach (SQLVis [51]), and (iii)\na conversational dialog approach [88]. The study reveals that these error-handling mechanisms\nhave limited impacts on increasing the efficiency and accuracy of error discovery and repair for\nerrors made by state-of-the-art NL2SQL models. Finally, we discussed the implications for future\nerror-handling mechanisms in NL query interfaces.\nTo conclude, this paper presents the following four contributions:\n• We developed a taxonomy of error types for three representative state-of-the-art NL2SQL\nmodels through iterative and axial coding procedures.\n• We conducted a comprehensive analysis that compared model attention to human attention.\nThe result shows that NL2SQL errors are highly correlated with attention misalignment\nbetween humans and models.\n• We conducted a controlled user study that investigated the effectiveness and efficiency of\nthree representative NL2SQL error discovery and repair methods.\n• We discussed the implications for designing future error-handling mechanisms in natural\nlanguage query interfaces.\n2\nRELATED WORK\n2.1\nNL2SQL techniques\nSupporting natural language queries for relational databases is a long-standing problem in both\nthe database (DB) and NLP communities. Given a relational database 𝐷and a natural language\nquery 𝑞𝑛𝑙to 𝐷, an NL2SQL model aims to find an equivalent SQL statement 𝑞𝑠𝑞𝑙to answer 𝑞𝑛𝑙. The\nearly methods of mapping 𝑞𝑛𝑙to 𝑞𝑠𝑞𝑙depend mainly on the development of intermediate logical\nrepresentation [27, 85] or mapping rules [5, 40, 56, 62, 87]. In the former case, 𝑞𝑛𝑙is first parsed\ninto logical queries independent of the underlying database schema, which are then converted\ninto queries that can be executed on the target database [34]. On the contrary, rule-based methods\ngenerally assume that there is a one-to-one correspondence between the words in 𝑞𝑛𝑙and a subset\nof database keywords/entities [34]. Therefore, the NL2SQL mapping can be achieved by directly\napplying the syntactic parsing and semantic entity mapping rules to 𝑞𝑛𝑙. Although both strategies\nhave achieved significant improvement over time, they have two intrinsic limitations. First, they\nrequire significant effort to create hand-crafted mapping rules for translation [34]. Second, the\n, Vol. 1, No. 1, Article . Publication date: February 2023.\n4\nZheng Ning, Yuan Tian, Zheng Zhang, Tianyi Zhang, and Toby Jia-Jun Li\ncoverage of these methods is limited to a definite set of semantically tractable natural language\nqueries [34, 56].\nThe recent development of deep learning (DL) based methods aims to achieve flexible NL2SQL\ntranslation through a data-driven approach [10, 28, 33, 47, 61, 95, 97]. From large-scale datasets,\nDL-based models learn to interpret NL queries conditioned on a relational DB via SQL logic [47].\nMost NL2SQL models use the encoder-decoder architecture [47, 95, 97], where the encoder models\nthe input 𝑞𝑛𝑙into a sequence of hidden representations along time steps. The decoder then maps\nthe hidden representations into the corresponding SQL statement. Recently, Transformer-based\narchitecture [47, 82] and pre-training techniques [30, 65, 89] have become popular as the backbone\nof NL2SQL encoders. At the same time, many decoders have been used to optimize SQL generation,\nsuch as autoregressive bottom-up decoding [61] and the LSTM-based pointer-generator [47].\nHowever, those DL-based models are usually “black-boxes” due to the lack of explainability [34].\nThe lack of transparency makes it difficult for users to figure out how to fix the observed errors\nwhen using DL-based NL2SQL models.\nThe evaluation of these DL-based models is based mainly on objective benchmarks such as\nSpider [90] and WikiSQL [97]. For example, Spider requires models to generalize well not only\nto unseen NL queries but also to new database schemas, in order to encourage NL interfaces\nto adapt to cross-domain databases. The performance of a model is evaluated using multiple\nmeasures, including component matching, exact matching, and execution accuracy. However, these\nbenchmarks only involve quantitative analysis of NL2SQL models, giving little clue about what\ntypes of errors a model tends to fall into.\nAn aim of our work is to develop a taxonomy of error types of errors made by state-of-the-art\nNL2SQL models and report the corresponding descriptive statistics to complement the quantitative\nbenchmark with the qualitative analysis of those NL2SQL models.\n2.2\nDetecting and repairing errors for NL2SQL\nNatural language interfaces for NL2SQL face challenges in language ambiguity, underspecification,\nand model misunderstanding [18, 52]. Previous work has explored ways to support error detec-\ntion and repair for NL2SQL systems through human-AI collaboration. NL2SQL error detection\nmethods can be mainly divided into categories of natural language-explanation-based, visualization-\nbased, and conversation-based approaches. NL2SQL error repair methods consist mainly of direct\nmanipulation and conversational error fixing approaches.\nFor error detection, a popular method is to explain the query and its answer in natural language\n[18, 69, 72, 77, 88]. For example, DIY [52] and STEPS [77] show step-by-step NL explanations and\nquery results by applying templates to subqueries, helping users understand SQL output in an\nincremental way; similarly, SOVITE [44] allows users to fix agent breakdowns in understanding\nuser instructions by revealing the agent’s current state of understanding of the user’s intent and\nsupporting direct manipulation for the user to repair the detected errors; NaLIR [41] explains the\nmapping relations between entities in the input query and those in the database schema; Ioannidis\net al. [69] introduced a technique to describe structured database content and SQL translation\ntextually to support user sensemaking of the model output. Visualizations have also been widely\nused to explain a SQL query and its execution [8, 9, 39, 51]. For example, QueryVis [39] produces\ndiagrams of SQL queries to capture their logical structures; QUEST [9] visualizes the connection\nbetween the matching entities from the input query and their correspondences in the database\nschema; SQLVis [51] introduced visual query representations to help SQL users understand the\ncomplex structure of SQL queries and verify their correctness.\n, Vol. 1, No. 1, Article . Publication date: February 2023.\n5\nMost of the previous work employed direct manipulation to repair and disambiguate queries.\nNaLIR [41], DIY [52] and DataTone [25] allow users to modify the entity mappings through drop-\ndowns; Eviza [66] and Orko [71] enable users to modify quantitative values in queries through range\nsliders. In addition to direct manipulation, several other prior interaction mechanisms enable users\nto give feedback to NL2SQL models through dialogs in natural language. For example, MISP [88]\nmaintains the state of the current parsing process and asks for human feedback to improve SQL\ntranslation through human-AI conversations. Elgohary et al. [18, 19] investigated how to enable\nusers to correct NL2SQL parsing results with natural language feedback in conversation.\nWith the many error-handling mechanisms that have been proposed, there is a gap in evaluating\nhow effective and efficient these mechanisms are to address different types of NL2SQL errors and\nwhat specific limitations they have. These types of information are critical to inform the effective\nchoice and design of NL2SQL error handling mechanisms in different use scenarios and to inspire\nthe use of ensemble mechanisms to handle different usage contexts of NL2SQL. Our work bridges\nthis gap by investigating these questions through controlled user studies, whose findings could\nguide the future design of NL2SQL error handling systems.\n2.3\nError handling via human-AI collaboration\nHandling errors made by AI models in human-AI collaboration faces many key challenges. First,\nmany state-of-the-art AI models lack transparency in their decision-making process, making it\ndifficult for users to understand exactly what leads to incorrect predictions [63]. Although there\nare some attempts to explain the state of the AI model using methods such as heatmap [60, 98],\nsearch traces [92], and natural language explanations [13, 17], they only allow users to peek at\nthe AI model’s reasoning at certain stages instead of exposing the holistic states of the model.\nSecond, it is difficult for users to develop a correct mental model for complex AI models due to the\nrepresentational mismatch in which “humans can create a mental model in terms of features that are\nnot identical to those used by AI models” [7]. Lastly, error handling usually requires multiple turns\nof interactions [36, 44]. However, maintaining coherent multi-turn interactions between AI and\nhumans is challenging [1]. It requires AI to closely maintain and update the context history, evolve\nits contextual understanding, and behave appropriately based on user’s timely responses [2, 3, 99].\nOur work contributes to the knowledge of how users handle errors in their collaborations with\nNL2SQL models by studying how users utilize existing error-handling mechanisms to inspect and\nfix errors made by NL2SQL models and how they perceive the usefulness of these mechanisms.\nOur findings of user challenges also echo the identified challenges in human-AI collaborations in\nother domains (e.g., programming [76, 79, 93], data annotation [26], QA generation [94], interactive\ntask learning [43, 45]), showing that users need help comprehending the state of AI models and\ndeveloping a proper mental model in AI-based interactive data-centric tools to understand and\nassess their recommendations.\n3\nAN ANALYSIS AND TAXONOMY OF NL2SQL ERRORS\nIn this section, we describe the development of the taxonomy of NL2SQL errors of four represen-\ntative NL2SQL models and the corresponding error analysis. The structure of this section is as\nfollows. Section 3.1 introduces the models we selected for analyzing the erroneous SQL queries.\nWe also discussed the discrepancy in the workflow of different models. Section 3.2 summarizes\nthe methodology used for building the dataset; Section 3.3 explains the axial and iterative coding\nprocedure we used to derive the error taxonomy; Section 3.4 describes the developed taxonomy of\nNL2SQL errors; Section 3.5 presents an analysis of erroneous queries in the dataset based on the\ntaxonomy.\n, Vol. 1, No. 1, Article . Publication date: February 2023.\n6\nZheng Ning, Yuan Tian, Zheng Zhang, Tianyi Zhang, and Toby Jia-Jun Li\nModel Index\nModel Names\nErr. queries\nRetrained Acc.\nOriginal Acc.\nM1\nSmBop [61]\n431\n81.2%\n71.1%\nM2\nBRIDGE [47]\n853\n62.7%\n68.3%\nM3\nGAZP [96]\n1062\n53.6%\n53.5%\nM4\nDIN-SQL+GPT-4 [58]\n304\n86.7%\n85.3%\nTable 1. Descriptive statistics and the accuracy of each model we reproduced\n3.1\nModel selection\nWe selected four representative NL2SQL models from the official Spider leader board, the informa-\ntion of which is shown in Table 1.\nWhile most models generate the SQL query in a top-down decoding procedure, SmBop (M1)\nimproves the speed and accuracy by adopting a bottom-up structure. Specifically, it gradually\ncombines smaller components to form a syntactically larger SQL component. BRIDGE (M2) is\na sequential architecture that models the dependencies between natural language queries and\nrelational databases. It combines a BERT-based [15] encoder with a sequential pointer-generator for\nend-to-end cross-DB NL2SQL semantic parsing. In comparison, GAZP (M3) combines a semantic\nparser with an utterance generator. When given a new database, it synthesizes data and selects\ninstances that are consistent with the schema to adapt an existing semantic parser to this new\ndatabase. DIN-SQL+GPT-4 (M4) has the best accuracy among all four models. It improves the\nperformance of the large language model (GPT-4 [55]) on text-to-SQL tasks by employing task\ndecomposition, adaptive prompting, linking schema to prompt, and self-correction. All these\nmodels employ different NL2SQL task-solving strategies at different stages, including decoding\n(M1), encoding (M2), finetuning (M3), and task preprocessing (M4).\n3.2\nErroneous queries dataset collection\nWe adopted the Spider [90] dataset to train and evaluate the models and to collect a set of erroneous\nSQL queries for the taxonomy. Spider is the most popular benchmark to evaluate NL2SQL models\nwith complex and cross-domain semantic parsing problems. It consists of around 10,000 queries in\nnatural language on multiple databases across different domains (e.g., “soccer”, “college”). In the\noriginal Spider dataset, the difficulty of the queries is divided into four levels: “Easy”, “Medium”,\n“Hard”, and “Extra Hard”, depending on the complexity of their structures and the SQL keywords\ninvolved. We demonstrate an example NL-SQL pair for each difficulty level in Table 2. In this\nwork, we focus only on the first three difficulty levels as state-of-the-art models have significantly\nlower accuracy on the “extra hard” queries — the best model we reproduced, DIN-SQL+GPT-4, only\nachieved less than 50% in accuracy, indicating that NL2SQL for “extra hard” queries remains less\nfeasible at this point.\nSince the held-out test set in Spider is not publicly available, we created our own test set by re-\nsplitting the public training and development sets from Spider. The ratios of the three difficulty levels\nin the new training and testing sets were close to those in the original training and developing sets.\nIn addition, we ensured that there was no overlap in the databases used between our training and\ntesting sets. Table 3 shows the distribution of our training and test data compared to the original pub-\nlic Spider dataset. We re-trained model M1-M3 with their officially released code using our training\nset. Since M4 was using few-shot learning, we did not retrain this model separately. The models used\ndifferent core structures and showed close to SOTA performance at the time we conducted the study.\nThe erroneous queries are those that have different execution results from the correct ones\n(with values). The queries generated by these models on the test set were manually analyzed to\ndevelop the taxonomy of SQL generation errors. Table 1 shows the total number of erroneous\n, Vol. 1, No. 1, Article . Publication date: February 2023.\n7\nNL query\nSQL query\nEasy\nWhat is the abbreviation\nfor airline ``JetBlue Airways” ?\nSELECT Abbreviation FROM AIRLINES\nWHERE Airline = ``JetBlue Airways”\nMedium\nWhat are the codes of countries\nwhere Spanish is spoken by the\nlargest percentage of people?\nSELECT CountryCode , MAX(Percentage) FROM\ncountrylanguage WHERE language= ``Spanish”\nGROUP BY CountryCode\nHard\nWhat are the first names of the students\nwho live in Haiti permanently or\nhave the cell phone number 09700166582?\nSELECT T1.first_name FROM students AS T1\nJOIN addresses AS t2 ON T1.permanent_address_id = T2.address_id\nWHERE T2.country = ’haiti’ OR T1.cell_mobile_number = ’09700166582’\nExtra\nHard\nWhat is the series name and country of\nall TV channels that are playing cartoons\ndirected by Ben Jones and cartoons\ndirected by Michael Chang?\nSELECT T1.series_name , T1.country FROM TV_Channel AS T1\nJOIN cartoon AS T2 ON T1.id = T2.Channel WHERE T2.directed_by =\n’Michael Chang’ INTERSECT SELECT T1.series_name , T1.country\nFROM TV_Channel AS T1 JOIN cartoon AS T2 ON T1.id = T2.Channel\nWHERE T2.directed_by = ’Ben Jones’\nTable 2. NL-SQL pairs with different difficulty levels in the Spider dataset\nEasy\nMedium\nHard\nExtra\nTotal\nOriginal\nTrain\n1983\n2999\n1921\n1755\n8658\nDev\n248\n446\n174\n166\n1034\nRe-split\nTrain\n1604\n2363\n1516\n0\n5483\nTest\n627\n1082\n579\n0\n2288\nTable 3. Descriptive statistics of the original Spider dataset and our sampled dataset\nqueries generated by each model. The accuracy of each model on our test set is close to the reported\nperformance of these models on the private held-out test set, indicating that our reproduction of\nthese models are consistent with the original implementations.\n3.3\nThe coding procedure\nAfter curating the dataset of erroneous queries, we followed the established open, axial and iterative\ncoding process [4, 11, 37] to develop a taxonomy of NL2SQL errors. The detail of the process is as\nfollows.\n3.3.1\nStep 1: Open coding. To begin with, we randomly sampled 40 erroneous SQL queries to\ndevelop the preliminary taxonomy. Four authors with in-depth SQL knowledge performed open\ncoding [4, 11, 37] on this subset of erroneous SQL queries. They were instructed to code to answer\nthe following questions: (1) What are the errors in the generated SQL query in comparison to the\nground truth? (2) What SQL component does each error reside at? (3) Have all the errors in the\nincorrect SQL query been covered?. Once finishing the first round of coding, the coded query pairs\n(the generated query and the ground truth) were put line by line in a shared spreadsheet. The\nannotators sat together to discuss the codes and reached a consensus on the preliminary version of\nthe codebook.\n3.3.2\nStep 2: Iterative refinement of the codebook. After creating the preliminary codebook, four\nannotators conducted iterative refinements of the established codes. Each iteration consisted of the\nfollowing three steps. First, the annotators coded a new sample batch of 40 unlabeled erroneous\nqueries using the codebook from the last iteration. If there is a new error not covered by the current\ncodebook, annotators would write a short description of it. Second, we computed the inter-rater\nreliability between coders [50] (Fleiss’ Kappa and Krippendorff’s Alpha) at the end of each iteration.\nLastly, annotators exchanged opinions about adding, merging, removing, or reorganizing codes\nand updated the codebook accordingly. Annotators completed three refinement iterations until the\n, Vol. 1, No. 1, Article . Publication date: February 2023.\n8\nZheng Ning, Yuan Tian, Zheng Zhang, Tianyi Zhang, and Toby Jia-Jun Li\nFig. 1. The user interface that we used for NL2SQL error annotation\ncodebook became stable and the inter-rater reliability scores were substantial. At the end of the\nfinal refinement iteration, the Fleiss’ Kappa was 0.69 and the Krippendorff’s Alpha was 0.67.\n3.3.3\nStep 3: Coding the remaining dataset. We then proceeded to code the remaining dataset using\nthe codebook from the final refinement iteration. Because the inter-rater reliability scores stabilized\namong annotators, two annotators participated in this step. The Fleiss’ Kappa and Krippendorff’s\nAlpha of the full dataset annotation between those two annotators were 0.76 and 0.78 respectively,\nindicating substantial agreement [4, 20, 35].\n3.3.4\nThe annotation interface. We implemented an error annotation system to annotate the er-\nroneous queries. The front-end UI is shown in Figure 1. It consists of three components: (A): A\nquery display section that presents the natural language query, the corresponding ground truth,\nand the model-generated SQL query. (B): An error annotation section where the annotator first\ndecided which part(s) of the generated SQL is wrong by clicking on the corresponding selection\nbutton. After that, the annotator was supposed to choose error types from the checkbox below.\nIf the error type was not included, the system provided an input box to accept open feedback. The\nerror types would be updated after each batch of coding described in Section 3.3.2. (C): A results\ndisplay section. The tables involved in the pairs and the execution result were shown in this section\nto help annotators identify the error types.\n3.4\nThe Taxonomy of NL2SQL Errors\nTable 4 shows the finalized taxonomy of NL2SQL errors. Specifically, we categorized the error types\nalong two dimensions: (1) the syntactic dimension shows which parts of the SQL query an error oc-\ncurs in, categorized by SQL keywords such as WHERE and JOIN; (2) the semantic dimension indicates\nwhich aspects of the NL description that the model misunderstands, such as misunderstanding\na value or the name of a table. For each type of error, the uppercase letter refers to the syntactic\ncategory, and the lowercase letter refers to the semantic category. Note that there may be multiple\n, Vol. 1, No. 1, Article . Publication date: February 2023.\n9\nmanifestations of a semantic error in a syntactic error category. For example, the table error has\ntwo different forms in the “JOIN” clause, including “Miss a table to JOIN” (Ba1) and “JOIN\nthe wrong table” (Ba2). An erroneous query may also have multiple error types associated with\nit.\nError categories\nError types\nSmBop\nBRIDGE\nGAZP\nDIN-SQL+\nGPT-4\nSyntactic errors\nA: WHERE error\nAa1:\nUse a wrong table in WHERE\n21\n68\n73\n10\nAb1:\nUse a wrong column in WHERE\n19\n36\n23\n12\nAc1:\nRedundant WHERE clause\n14\n16\n27\n13\nAc2:\nMissing WHERE clause\n15\n21\n61\n7\nAd1:\nOther wrong value in WHERE clause\n51\n52\n91\n18\nAd2:\nValue case error in WHERE clause\n62\n69\n82\n36\nAd3:\nValue plurality error in WHERE clause\n8\n6\n16\n0\nAd4:\nValue synonym error in WHERE clause\n35\n40\n45\n35\nAe1:\nWrong comparator (<, >, =, !=, etc)\n8\n13\n14\n3\nAe2:\nWrong boolean operator (AND, OR etc.)\n4\n15\n9\n3\nB: JOIN error\nBa1:\nMiss a table to JOIN\n35\n106\n101\n15\nBa2:\nJOIN the wrong table\n24\n89\n78\n3\nBb1:\nUse a wrong column in JOIN\n13\n79\n69\n7\nBc1:\nRedudant JOIN clause\n17\n82\n113\n36\nC: ORDER BY error\nCb1:\nUse a wrong column to sort\n3\n26\n26\n9\nCc1:\nMiss a ORDER BY clause\n12\n22\n20\n4\nCc2:\nRedundant sorting\n3\n1\n3\n0\nCe1:\nWrong sorting direction\n6\n27\n23\n15\nD: SELECT error\nDa1:\nUse a wrong table in SELECT\n59\n117\n106\n26\nDb1:\nReturn a wrong column in SELECT\n21\n56\n78\n33\nDb2:\nReturn a redundant column in SELECT\n10\n19\n36\n13\nDb3:\nMiss returning column(s) in SELECT\n20\n34\n59\n11\nDf1:\nUse wrong aggretation function\n7\n43\n11\n9\nDf2:\nMiss aggregation function\n5\n19\n14\n6\nE: GROUP BY error\nEb1:\nUse a wrong column in GROUP BY\n6\n10\n18\n12\nEc1:\nMiss a GROUP BY clause in the SQL query\n10\n33\n47\n8\nEc2:\nRedudant GROUP BY clause\n6\n7\n16\n9\nF: HAVING error\nFc1:\nMiss HAVING clause\n1\n5\n12\n1\nFc2:\nRedundant HAVING clause\n0\n2\n6\n1\nFe1:\nWrong condition in HAVING\n1\n2\n3\n6\nG: LIKE error\nGc1:\nMiss LIKE clause\n1\n3\n9\n4\nGe1:\nWrong LIKE condition\n1\n8\n22\n2\nH: LIMIT error\nHc1:\nRedudant LIMIT clause\n1\n2\n6\n0\nHc2:\nMiss LIMIT clause\n0\n1\n3\n1\nI: INTERSECT error\nIe1:\nWrong INTERSECT condition\n8\n8\n9\n11\nJ: DISTINCT error\nJc1\nMiss a DISTINCT keyword\n7\n18\n96\n12\nJc2\nRedundant DISTINCT keyword\n4\n15\n0\n11\nK: EXCEPT error\nKc1\nWrong EXCEPT clause\n14\n27\n24\n13\nL: NOT error\nLc1\nMiss NOT keyword\n7\n9\n7\n0\nM: UNION\nMe1\nWrong UNION condition\n9\n9\n8\n10\nSemantic errors\na: Table error\n97\n279\n274\n53\nb: Column error\n81\n237\n251\n94\nc: Miss/redundant Clause/keyword error\n107\n250\n432\n101\nd: Value error\n153\n162\n230\n82\ne: Condition error\n37\n82\n88\n49\nf: Aggregation function error\n12\n62\n25\n15\nTable 4. The taxonomy of NL2SQL errors with the count of each error type for the models\n3.5\nNL2SQL error analysis\nBased on the error taxonomy, we further analyzed the erroneous queries to explore the following\nthree questions:\nRQ1 How are the erroneous queries distributed among different models? Do models tend to\nstumble on the same queries or make mistakes on distinct queries? Furthermore, for those\n, Vol. 1, No. 1, Article . Publication date: February 2023.\n10\nZheng Ning, Yuan Tian, Zheng Zhang, Tianyi Zhang, and Toby Jia-Jun Li\noverlapping erroneous queries, do models tend to make similar types of error on them or\nnot?\nRQ2 How do error types spread along the syntactic and semantic dimensions? How different are\nthe distributions of error types among the three models?\nRQ3 How far are the erroneous queries from their corresponding ground truths?\n3.5.1\nThe distribution of erroneous queries among models. Figure 2 shows the overlap of erroneous\nqueries among the best three models in a Venn diagram (DIN-SQL+GPT-4, SmBop, and BRIDGE).\nEach circle represents the queries on which the model made errors. The size of each circle is\nproportional to the number of erroneous queries of its corresponding model in the sampled dataset\n(Table 1).\nFig. 2. The overlap of erroneous queries generated by DIN-SQL+GPT-4, SmBop, and BRIDGE\nFurthermore, we found that there were 129 queries appeared in all four models, we sampled\nthree of them and presented the queries and error types in Table 6. Additionally, we found 92.1%\n(280 out of 304) of DIN-SQL+GPT-4’s; 82.4% (355 out of 431) of SmBop’s; 84.4% (720 out of 853) of\nBRIDGE’s; and 70.6% (750 out of 1062) of GAZP’s incorrect queries also confounded other models,\nindicating that new models are making errors on a limited number of new queries each time. The\nstatistics are also presented in Table 5. The results imply that different models tend to make\nerrors on the same subset of queries in NL2SQL.\nModel Names\nOverlapped Queries Percentage\nDIN-SQL+GPT-4\n92.1%\nSmBop\n82.4%\nBRIDGE\n84.4%\nGAZP\n70.6%\nTable 5. The percentage of erroneous queries for each model that also appeared in the other three models\n𝐽𝑎𝑐𝑐𝑎𝑟𝑑𝑐𝑜𝑒𝑓𝑓𝑖𝑐𝑖𝑒𝑛𝑡= |𝐸𝐷𝐼𝑁−𝑆𝑄𝐿+𝐺𝑃𝑇−4 ∩𝐸𝑆𝑚𝐵𝑜𝑝∩𝐸𝐵𝑅𝐼𝐷𝐺𝐸∩𝐸𝐺𝐴𝑍𝑃|\n|𝐸𝐷𝐼𝑁−𝑆𝑄𝐿+𝐺𝑃𝑇−4 ∪𝐸𝑆𝑚𝐵𝑜𝑝∪𝐸𝐵𝑅𝐼𝐷𝐺𝐸∪𝐸𝐺𝐴𝑍𝑃|\n(1)\nTo understand whether models make similar types of errors in those overlapped queries, for each\nquery, we measured the similarity of the syntactic and semantic error types respectively among\n, Vol. 1, No. 1, Article . Publication date: February 2023.\n11\n1\n2\n3\nNL query\nHow many games are\nplayed for all students?\nWhat are the different\nmembership levels?\nFind the package choice and series name of the\nTV channel that has high definition TV.\nCorrect\nquery\nSELECT sum(gamesplayed)\nFROM Sportsinfo\nSELECT count(DISTINCT level)\nFROM member\nSELECT package_option, series_name FROM\nTV_Channel WHERE high_definition_TV = “yes”\nModel\nGenerated query\nError\ntypes\nGenerated query\nError\ntypes\nGenerated query\nError\ntypes\nDIN-SQL\n+GPT-4\nSELECT COUNT(GameID)\nFROM Plays_Games\nDa1, Db1,\nDf1\nSELECT DISTINCT\nLevel FROM member\nDf2\nSELECT package_option, series_name\nFROM TV_Channel WHERE\nhigh_definition_TV = “Yes”\nAd2\nSmBop\nSELECT COUNT( * ) , stuid\nFROM plays_games\nDa1, Db1,\nDb2, Df1\nSELECT Level\nFROM member\nDf2, Jc1\nSELECT package_option, series_name\nFROM TV_Channel WHERE\nhigh_definition_TV = 1\nAd1\nBRIDGE\nSELECT COUNT(*)\nFROM Plays_Games\nDa1, Db1,\nDf1\nSELECT DISTINCT\nlevel FROM member\nDf2\nSELECT package_option, series_name\nFROM TV_Channel WHERE\nhigh_definition_TV = “t”\nAd4\nGAZP\nSELECT count ( * )\nFROM Plays_Games\nDa1, Db1,\nDf1\nSELECT level\nFROM member\nDf2, Jc1\nSELECT package_option, series_name\nFROM TV_Channel WHERE\nhigh_definition_TV = “definition”\nAd1\nTable 6. Sampled erroneous NL-SQL pairs and their error types\nthe models. Noticeably, there are multiple distance metrics such as Jaccard distance [38], Hamming\ndistance, and Euclidean distance. Hamming distance usually works for sets with the same length,\nwhile Euclidean distance measures the distance between two points in an Euclidean space. In our\ncase, these two different error sets are challenging to map to the space. Thus, we choose the Jaccard\ndistance as it can be adapted to sets of different lengths and explicitly considers the difference of\neach error type in the set. The Jaccard coefficient is measured using Equation 1, where 𝐸𝑚means\nthe set of error types that the model 𝑚made on the target query. For syntactic error types, 27.9% of\noverlapped queries have a Jaccard coefficient of 0 among the four models, which implies that the\nmodels did not all make the same syntactic error type in these queries. Regarding the semantic\nerror types, 24.8% of these queries have a Jaccard coefficient of 0. On the other hand, only 7.8% of\nthe overlapped queries have the same syntactic error type from the four models, and even fewer\nof them (6.2%) have the same semantic error type from all four models. These results show that\nalthough the models tend to make errors in the same set of queries, the types of errors in\neach query tend to be different. We provide three examples in Table 6 to illustrate the similarity\nand disparity of error types in the same queries.\n3.5.2\nError frequency. In this section, we investigate the distribution of error types among models.\nSpecifically, we report the following three measures for each model:\n(1) Syntactic error rate (𝑆𝑌𝑁𝐸𝑅𝑚𝑠): Given the model 𝑚and a syntactic error type 𝑠, 𝑆𝑌𝑁𝐸𝑅𝑚𝑠\nis the number of queries in which the model 𝑚made the syntactic error 𝑠divided by the\nnumber of ground truth queries in the entire development set that has the corresponding\nsyntax. (Table 7). It tells us how likely a syntactical part of a query will produce errors.\n(2) Syntactic error percentage (distribution) (𝑆𝑌𝑁𝐸𝑃𝑚𝑠): Given the model 𝑚and a syntactic\nerror type 𝑠, 𝑆𝑌𝑁𝐸𝑃𝑚𝑠is the number of queries in which the model 𝑚made the syntactic\nerror 𝑠divided by the total number of erroneous queries made by the model 𝑚. (Table 7). It\nmeasures the percentage of queries that contain a specific type of syntactic error among all\nerroneous queries.\n(3) Semantic error percentage (distribution) (𝑆𝐸𝑀𝐸𝑃𝑚𝑠): Given the model𝑚and the semantic\nerror rate 𝑠, 𝑆𝐸𝑀𝐸𝑃𝑚𝑠is the number of queries in which the model 𝑚made the semantic\nerror 𝑠divided by the total number of erroneous queries made by the model 𝑚. (Table 8). It\nmeasures the percentage of queries that contain a specific type of semantic error among all\nerroneous queries.\n, Vol. 1, No. 1, Article . Publication date: February 2023.\n12\nZheng Ning, Yuan Tian, Zheng Zhang, Tianyi Zhang, and Toby Jia-Jun Li\nAs shown in Table 7, the distributions of syntactic error type are similar among all four models.\nNote that a model can produce a query with multiple types of errors. Notably, the error percentages\nof WHERE, JOIN, and SELECT are significantly higher than that of other syntactic error types for all\nthe models. However, comparing it with the syntactic error rate, we see that a higher frequency\nof errors (in all queries) does not equate to a higher error rate when a specific type of keyword\nis encountered. For example, although UNION errors only account for fewer than 4% of erroneous\nqueries among all models, it has an error rate of more than 50% (i.e., when the correct query should\ncontain a UNION clause, the model has a high probability of making errors there). The top 5 syntactic\nparts that have the highest error rates are shown in Table 9.\nError Percentage\nError Rate\nError type\nSmBop\nBRIDGE\nGAZP\nDIN-SQL+\nGPT-4\nSmBop\nBRIDGE\nGAZP\nDIN-SQL+\nGPT-4\nA: WHERE error\n47.80%\n35.05%\n30.89%\n42.43%\n18.86%\n27.38%\n30.04%\n11.81%\nB: JOIN error\n17.87%\n28.14%\n31.83%\n19.74%\n10.13%\n31.58%\n44.47%\n7.89%\nC: ORDER BY error\n4.64%\n7.39%\n5.74%\n9.87%\n4.58%\n14.42%\n13.96%\n6.86%\nD: SELECT error\n23.20%\n25.79%\n25.80%\n30.92%\n4.37%\n9.62%\n11.98%\n4.11%\nE: GROUP BY error\n5.10%\n5.86%\n7.63%\n9.54%\n4.50%\n10.22%\n16.56%\n5.93%\nF: HAVING error\n0.46%\n1.06%\n1.98%\n2.63%\n1.44%\n6.47%\n15.11%\n5.76%\nG: LIKE error\n0.46%\n1.29%\n2.92%\n1.97%\n2.86%\n15.71%\n44.29%\n8.57%\nH: LIMIT error\n0.23%\n0.35%\n0.85%\n0.33%\n0.41%\n1.24%\n3.73%\n0.41%\nI: INTERSECT error\n1.86%\n0.94%\n0.85%\n3.62%\n18.60%\n18.60%\n20.93%\n25.58%\nJ: DISTINCT error\n2.55%\n3.87%\n9.04%\n7.57%\n4.78%\n14.35%\n41.74%\n1.74%\nK: EXCEPT error\n3.25%\n3.17%\n2.26%\n4.28%\n20.29%\n39.13%\n34.78%\n18.84%\nL: NOT error\n1.62%\n1.06%\n0.66%\n0.00%\n13.46%\n17.31%\n13.46%\n0.00%\nM: UNION error\n2.09%\n1.06%\n0.75%\n3.29%\n56.25%\n56.25%\n50.00%\n62.5%\nTable 7. The error percentage and error rate of each syntactic error type\nCompared to syntactic errors, the distribution of semantic errors is more varied between models\n(Figure 8). We found that d: Value error and a: Table error are the most frequent error types for\nSmBop (35.5%) and BRIDGE (32.71%), respectively. While c: Miss/redundant clause/keyword\nerror appeared most frequently in GAZP (40.68%) and DIN-SQL+GPT-4 (33.22%). It indicates\nthat the semantic challenges of the investigated NL2SQL models are more varied than the\nsyntactic challenges they faced.\nError type\nSmBop\nBRIDGE\nGAZP\nDIN-SQL\n+GPT-4\na: Table error\n22.51%\n32.71%\n25.80%\n17.43%\nb: Column error\n18.79%\n27.78%\n23.63%\n30.92%\nc: Miss/redundant Clause/keyword error\n24.83%\n29.31%\n40.68%\n33.22%\nd: Value error\n35.50%\n18.99%\n21.66%\n26.97%\ne: Condition error\n8.58%\n9.61%\n8.29%\n16.12%\nf: Aggregation function error\n2.78%\n7.27%\n2.35%\n4.93%\nTable 8. The error percentage of each semantic error type\n3.5.3\nDistance between erroneous and ground truth queries. Lastly, we used Levenshtein distance\nto measure the distance between erroneous and ground truth queries. The Levenshtein distance\nbetween two queries is defined as the minimum number of word-level (split by space) edits\n(insertions, deletions, or substitutions) required to transform the model-generated query into the\nground truth query. Before computing the distance, we first pre-process the ground truth and\n, Vol. 1, No. 1, Article . Publication date: February 2023.\n13\nModel\nTop 1\nTop 2\nTop 3\nTop 4\nTop 5\nSmBop\nUNION 56.25%\nEXCEPT 20.29%\nWHERE 18.86%\nINTERSECT 18.60%\nNOT 13.46%\nBRIDGE\nUNION 56.25%\nEXCEPT 39.13%\nJOIN 31.58%\nWHERE 27.38%\nINTERSECT 18.6%\nGAZP\nUNION 50.00%\nJOIN 44.47%\nLIKE 44.29%\nDISTINCT 41.74%\nEXCEPT 34.78%\nDIN-SQL+GPT-4\nUNION 62.5%\nINTERSECT 25.58%\nEXCEPT 18.84%\nWHERE 11.81%\nLIKE 8.57%\nTable 9. The top 5 error-prone syntactic parts of a SQL query for the selected models\nFig. 3. The distribution of Levenshtein distances between erroneous queries and ground truth queries for\neach model\npredicted queries for each pair to unify the SQL format. Specifically, we i): ignore the differences in\nthe upper or lower case letters except for values; ii) extract the table names and the alias for each\ncolumn used in the query and prefix the column names with the identified table names. Noticeably,\nthere may be other cases where the predicted query does not need to be revised to exactly the form\nof the ground truth query in order to get the correct result; therefore, the Levenshtein distances we\nobtained may be larger than the actual ones.\nFigure 3 shows the distribution of the Levenshtein distances of errors made by each model. It is\nworth noting that all four distributions have a long tail. Specifically, by looking at the Levenshtein\ndistance in three different groups: 0–5, 6—10, and more than 10; we found that a large portion of\nerroneous SQL queries for all four models can be fixed in a small number of edits. Especially for\nthe best model we reproduced, DIN-SQL+GPT-4, 19.1% (58/304) of the erroneous queries can be\nfixed in one step. In particular, 19.6% (208/1062) queries in GAZP only need changes in one token;\nthe percentage of erroneous queries requiring only changes in one token for BRIDGE and SmBop\nis 3.9% (34/853) and 10.7% (46/431), respectively.\n, Vol. 1, No. 1, Article . Publication date: February 2023.\n14\nZheng Ning, Yuan Tian, Zheng Zhang, Tianyi Zhang, and Toby Jia-Jun Li\n4\nANALYSIS ON NL2SQL HUMAN-MODEL ATTENTION ALIGNMENT\nAfter understanding the error types and distribution in NL2SQL, we aim to further investigate\nthe possible cause of SQL errors. Previous research [6, 32, 46] has shown that the discrepancy\nbetween model attention and human attention is correlated with poor performance in code sum-\nmarization, machine translation, and visual feature representation. We hypothesize that such\nmisalignment between model attention and human attention is also a source of error in NL2SQL.\nIn other words, NL2SQL models return incorrect columns or tables in part because they do not pay\nattention to words that humans pay attention to. However, the lack of empirical research limits\nour understanding in this area.\nTo investigate whether and to what extent SQL errors derive from such attention misalignment,\nwe compare human-labeled attention with a representative model’s attention on NL queries.\n4.1\nData preparation\n4.1.1\nDifferent attention calculation methods. There are different methods to obtain the model’s\nattention. For example, some work [12, 22, 42, 80] leveraged the self-attention mechanism of\ntransformer [81] to obtain attention, while some work [14, 67, 70, 74] used the gradients of the\nmodel predictions with respect to the input features to calculate the model’s attention.\nPerturbation-based methods follow a two-step process: they first mutate the input and then\ncalculate the model’s attention based on the differences in the output. For instance, LIME [59]\ngenerates a local explanation by approximating the specific model predictions using a simpler\nmodel, such as a linear classifier. SHAP [49] improves on LIME by perturbing the input based on\ngame theory and using the Shapely value to estimate the importance of different tokens. However, a\nlimitation of both methods is that they often require a large number of perturbed samples to ensure\nan accurate estimation. Furthermore, LIME and SHAP only mutate an input by deleting tokens,\nwhich may significantly alter the meaning or structure of the input. To address this limitation, more\nrecent perturbation-based methods opt to replace tokens with similar or semantically related tokens\nin the context [48, 86]. These methods often utilize a masked language model such as BERT [16] to\npredict similar or semantically related tokens to replace existing tokens in an input. In this study,\nwe selected a perturbation-based method optimized by BERT. The reasons for this choice will be\nexplained in the following paragraphs.\n4.1.2\nNL2SQL model selection. We selected SmBoP [61], which achieves the second-highest perfor-\nmance among the models we used in the previous study. Although DIN-SQL + GPT-4 [58] achieves\nthe highest performance, GPT-4 is not open-source, therefore, we cannot directly calculate its\nattention based on self-attention or gradient. Additionally, by using the perturbation-based method,\nthe GPT-4 API has to be called millions of times, which is not practical and affordable.\n4.1.3\nAttention calculation method selection. Methods leveraging either self-attention or gradient\nare not suitable for a model with multiple components other than a transformer [81] since attention\nor gradient does not represent the attention for the entire model. The decoding process of SmBoP\nstarts by generating SQL components and gradually combines them from the bottom up to create\na complete SQL statement. This process involves contextualization based on the transformer’s\nself-attention mechanism and the use of predefined rules to filter out invalid queries.\nDue to the complexity of SmBoP architecture, the attention in transformer headers or simply\nusing gradients cannot accurately represent the attention of the entire model. Therefore, we choose\nthe perturbation-based method, which regards the SmBoP model as a black box without considering\nthe inner details of its architecture.\n, Vol. 1, No. 1, Article . Publication date: February 2023.\n15\n4.2\nHuman attention labeling\nTo obtain human attention on NL2SQL tasks, two annotators, who are proficient in SQL, conducted\niterative refinements of attention annotation. We first randomly sampled 200 tasks from the Spider\ndataset, where 140 tasks can be correctly solved by the experiment model (SmBoP), and 60 tasks\non which the model makes errors. We intentionally balanced the sampled tasks according to the\nperformance of SmBoP (69.5% on the Spider test set).\nEach iteration consisted of the following three steps. First, two annotators separately annotated\na new sample batch of 25 tasks. For each task, the annotators reviewed the natural language (NL)\nquestion and the ground truth SQL query. Then each annotator individually identified important NL\nwords (those that contribute to the query) and marked their attention weight as 1 while marking the\nattention weight of the remaining unimportant words (e.g., all, the, of) as 0. Second, we computed\nthe inter-rater reliability between annotators [50] (Fleiss’ Kappa and Krippendorff’s Alpha) at the\nend of each iteration. Lastly, annotators discussed how they judge the importance of a certain word.\nAnnotators completed three refinement iterations until the human-labeled attention became stable\nand the inter-rater reliability scores were substantial. At the end of the final refinement iteration,\nFleiss’ Kappa was 0.67 and Krippendorff’s Alpha was 0.58.\n4.3\nMeasuring attention alignment between human and model\nWe measure the alignment between the attention of the models and the annotators using the\nkeyword coverage rate. Specifically, we select the Top 𝐾model-focused words with the highest\nattention as 𝑤𝑜𝑟𝑑𝑚, where 𝐾equals the number of keywords selected by human annotators.\nThen we calculate the percentage of human-labeled words 𝑤𝑜𝑟𝑑ℎcovered in 𝑤𝑜𝑟𝑑𝑚, as shown in\nEquation 2.\n𝑟𝑎𝑡𝑒= |𝑤𝑜𝑟𝑑𝑚∩𝑤𝑜𝑟𝑑ℎ|\n|𝑤𝑜𝑟𝑑ℎ|\n(2)\nNevertheless, the attention scores calculated by this method cannot be directly compared to\nwords annotated by human labelers. This is because human labelers annotate attention on each\nindividual word, while machine attention is calculated and distributed on specific tokenization\nof the model. For example, the word “apple” can be tokenized into two tokens: “ap” and “ple”\nthrough byte pair encoding. To bridge the gap, we developed a method to map model tokens back\nto individual words and recalculate the attention distribution. Suppose an input text includes 𝑁\nwords {𝑤1,𝑤2, ...,𝑤𝑛}, while the model’s tokenizer splits the text into 𝑀tokens {𝑡1,𝑡2, ...,𝑡𝑚}. We\ncalculate the model’s attention on 𝑖th word 𝑤𝑖as the sum of all tokens that overlap with 𝑤𝑖, as\nshown in Equation 4.\n𝑎𝑡𝑡𝑒𝑛𝑡𝑖𝑜𝑛𝑤𝑖=\n∑︁\n𝑡𝑗∩𝑤𝑖≠∅\n𝑎𝑡𝑡𝑒𝑛𝑡𝑖𝑜𝑛𝑡𝑗\n(3)\n4.4\nResults\nIn this section, we aim to answer the following research questions:\nRQ1 To what extent model attention is aligned with human attention?\nRQ2 Can the attention misalignment explain the errorous queries generated by NL2SQL models?\nRQ3 Which error types are highly related to the attention misalignment?\nIn our 200 sampled tasks from the Spider dataset, the average number of words in the NL queries\nis 12. Accordingly, we calculate the Keyword Coverage Rate by experimenting with different 𝐾(i.e.,\n, Vol. 1, No. 1, Article . Publication date: February 2023.\n16\nZheng Ning, Yuan Tian, Zheng Zhang, Tianyi Zhang, and Toby Jia-Jun Li\nthe top 𝐾words with the highest attention to be considered in the calculation) from 1 to 20. Since\nthe value of 𝐾may exceed the number of words in the NL query, the actual number of words\nconsidered is equal to the minimum of 𝐾and the number of words in the NL query.\nK\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nAlignment\n0.138\n0.231\n0.302\n0.368\n0.433\n0.491\n0.545\n0.601\n0.649\n0.698\nK\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\nAlignment\n0.740\n0.776\n0.804\n0.832\n0.851\n0.868\n0.880\n0.888\n0.894\n0.898\nTable 10. Average Keyword Coverage Rate for all 200 tasks\nFig. 4. The distribution of alignment for correctly and incorrectly solved tasks considering different numbers\nof keywords.\nF1: The model’s attention partially aligns with human attention, which is consistent with\nthe model’s performance. Table 10 shows the average Keyword Coverage Rate for all 200 tasks\nin different settings of 𝐾. For example, when considering the top 12 words (the average number\nof words), there is an overlap of around 77.6% between human-focused and model-focused words.\nThis result is consistent with the performance of SmBoP, which achieves 70% accuracy on the\nsampled dataset.\nF2: Attention alignment is higher when the model correctly solves the task, suggesting\nthat SQL errors are correlated with attention misalignment. To examine the relationship\nbetween attention alignment and the model’s performance, we compare the Keyword Coverage Rate\nof correctly solved tasks with that of incorrectly solved tasks. Figure 4 shows the average keyword\ncoverage rate with different 𝐾values for correct and incorrect queries generated by SmBoP. When\nSmBoP generates a correct SQL query, the attention alignment is significantly higher than when it\ngenerates an erroneous SQL for all 𝐾values, with all 𝑝-values being less than 0.05.\n, Vol. 1, No. 1, Article . Publication date: February 2023.\n17\nTake 𝐾= 6 (half of the number of words) as an example, Figure 5 shows the comparison\nof attention alignment distributions between correctly and incorrectly generated queries. Each\ndistribution approximately follows a Gaussian distribution. However, the mean of the distribution\nfor erroneous queries is significantly lower than that of correct queries, with a 𝑝-value of 1.5𝑒−4.\nFurthermore, when generated queries are correct, all the rates are no more than 0.67, suggesting\nthat a low attention alignment contributes to generating an erroneous SQL.\nFig. 5. A comparison of attention alignment between correctly and incorrectly solved tasks.\nF3: Attention misalignment is not specific to a certain error type. To further investigate\nwhich types of errors are more correlated with attention misalignment, we labeled the error types\nof all 40 incorrectly solved tasks using the same procedure discussed in Section 3.3. Next, we\ncalculated the average keyword coverage rate associated with each type of error. Specifically, for\na certain error type, we summed up the attention alignment of all tasks that included that error.\nThen, we divided the summed attention alignment score by the number of tasks that included this\nerror, as shown in Equation 2.\n𝑎𝑙𝑖𝑔𝑛𝐸𝑖=\nÍ\n𝐸𝑖∈𝑡𝑎𝑠𝑘𝑗𝑎𝑙𝑖𝑔𝑛𝑡𝑎𝑠𝑘𝑗\n\f\f{task𝑖| task𝑗contains error 𝐸𝑖}\n\f\f\n(4)\nTable 11 shows the average alignment for each type of error. The results indicate no significant\ndifference in attention alignment across different types of error.\n5\nTHE USER STUDY OF INTERACTIVE ERROR DISCOVERY & REPAIR MECHANISMS\nIn the past few years, we have seen a growing interest in interactive mechanisms for users to\ndetect and repair NL2SQL errors [9, 29, 39, 51, 52, 84, 88], regardless of users’ domain expertise of\n, Vol. 1, No. 1, Article . Publication date: February 2023.\n18\nZheng Ning, Yuan Tian, Zheng Zhang, Tianyi Zhang, and Toby Jia-Jun Li\nTable 11. Average attention alignment for different types of error.\nSyntactic error type\n𝐴\n𝐵\n𝐶\n𝐷\n𝐸\n𝐹\n𝐺\nAttention alignment\n0.26\n0.32\n0.22\n0.30\n0.25\n0.27\n0.34\nSyntactic error type\n𝐻\n𝐼\n𝐽\n𝐾\n𝐿\n𝑀\nAttention alignment\n0.19\n0.28\n0.33\n0.24\n0.21\n0.26\nusing SQL language. To understand the performance and usage of these mechanisms by users, we\nconducted a controlled user study to examine the effectiveness of different error discovery and\nrepair mechanisms for NL2SQL†. Specifically, we investigated the following research questions.\nRQ1. How effective and efficient are the different error-handling mechanisms and interaction\nstrategies in NL2SQL?\nRQ2. What are the user preferences and perceptions of different mechanisms and strategies?\nRQ3. What are the gaps between the capabilities of existing approaches and user needs?\n5.1\nExperiment conditions\nIn this study, we used four conditions shown in Table 12. In the baseline condition, no interactive\nsupport was provided for error discovery and repair. Users had to examine the correctness of a\ngenerated SQL query by directly checking the query result and manually editing a generated SQL\nquery to fix an error.\nIn addition to the baseline, we selected three experimental conditions based on three repre-\nsentative approaches for error discovery and repair. The first experimental condition exemplifies\nan explanation- and example-based approach (DIY [52]) that displays intermediate results by\ndecomposing a long SQL query into shorter queries and generating natural language explanations\nfor each step. Meanwhile, it allows users to fix the mapping between words in the NL description\nand their corresponding entities in the generated SQL query from a drop-down menu. The second\nexperimental condition uses an explanation-based visualization approach (SQLVis [51]). The\ntechnique uses a graph-based visualization for the generated SQL query to illustrate the explicit\nand implicit relationship among different SQL components such as the selected columns, tables,\nprimary and foreign keys. The third experimental condition exemplifies a conversational dia-\nlog approach (MISP [88]). It allows users to correct an erroneous SQL query through multiple\nrounds of conversation in natural language (Table 12). We replicated the core functionalities of\nthe DIY mechanism used in experimental condition #1 as described in the paper [52] because the\nofficial source code was not publicly released. For experimental condition #2, we used the official\nimplementation‡ provided by the authors. For the dialog system under experimental condition\n#3, we implemented an interactive widget based on the open-sourced command-line tool and an\ninteractive graphical user interface based on the React-Chatbot-Kit§ for the study.\n5.2\nParticipants\nWe recruited 26 participants from the campus community of a private university in the midwest of\nthe United States through mailing lists and social media. Participants included 15 men and 11 women\naged 20 to 30 years. Nine participants were novice SQL users who had either no experience in using\n†The protocol of the study has been reviewed and approved by the IRB at our institution.\n‡https://github.com/Giraphne/sqlvis\n§https://www.npmjs.com/package/react-chatbot-kit\n, Vol. 1, No. 1, Article . Publication date: February 2023.\n19\nCondition\nError Discovery and Repair Mechanisms\nBaseline\nDirect SQL query editing\nExp. Cond. #1\nStep-by-step SQL query explanation & NL-SQL entity mapping (DIY [52])\nExp. Cond. #2\nGraph-based SQL query visualization (SQLViz [51])\nExp. Cond. #3\nConversational dialog system (MISP [88])\nTable 12. The list of conditions used in the user study\nSQL or had seen SQL queries before but were not familiar with the syntax. 10 participants were in-\ntermediate SQL users who had either taken an introductory database course or understood the SQL\nsyntax. The remaining 7 were experienced users who were familiar with SQL queries or had signifi-\ncant experience working with SQL. Each participant was compensated with $15 USD for their time.\n5.3\nStudy procedure\nIn our study, each participant experienced the four conditions described in Section 5.1. As the goal of\nthis study is to investigate the error discovery and repair behavior of users, the example SQL queries\nfor each participant were randomly selected from the dataset of incorrect queries generated by the\nthree NL2SQL models used in the error analysis study. Each query that a participant encountered\nwas also randomly assigned to one of the experimental conditions or the baseline condition.\nTo facilitate the user experiment, we implemented a web application that can automatically\nselect SQL tasks and assign conditions to study participants. After finishing one SQL query, users\ncan click the “Next” button on the application, and it will randomly select the next query and assign\na condition to it. Both the query assignment and the condition assignment were randomized. For\neach query, the web application renders the task description, the database and its tables, and the\nassigned error-handling mechanisms.\nEach experiment session began with the informed consent process. Then, each participant\nwatched a tutorial video about how to interact with the system to solve an SQL task and fix\nNL2SQL errors under different conditions. Then, each participant was given a total of 45 minutes\nto solve as many SQL tasks as possible. On average, each participant completed 22.0 SQL tasks\nin 45 minutes (5.5 in each condition). After each experiment session, the participant completed a\npost-study questionnaire. This questionnaire asked participants to rate their overall experience,\nthe usefulness of interactive tool support under different conditions, and their preferences in Likert\nscale questions. We ended each experiment session with a 10-minute semi-structured interview. In\nthe interview, we asked follow-up questions about their responses to the post-study questionnaire,\nif they encountered any difficulties with interaction mechanisms under the conditions, and which\nparts they found useful. We also asked participants about the general workflow as they approached\nthe task and the features they wished they had when handling NL2SQL errors. All user study\nsessions were video recorded with the consent of the participants.\nFollowing established open coding methods [11, 37], an author conducted a thematic analysis\nof the interview transcripts to identify common themes about user experiences and challenges\nthey encountered while using the different error handling mechanisms, as well as their suggestions\nfor new features. Specifically, the coder went through and coded the transcripts of the interview\nsessions using an inductive approach. For user quotes that did not include straightforward key\nterms, the coder assigned researcher-denoted concepts as the code.\n, Vol. 1, No. 1, Article . Publication date: February 2023.\n20\nZheng Ning, Yuan Tian, Zheng Zhang, Tianyi Zhang, and Toby Jia-Jun Li\nConditions\nAvg. Acc. (𝜇= 0.56)\nSD (𝜇= 0.50)\nAvg. ToC (𝜇= 116.7)\nSD (𝜇= 89.6)\nB1\n0.55\n0.48\n109.7\n95.8\nC1\n0.56\n0.51\n110.9\n101.0\nC2\n0.60\n0.50\n115.9\n96.5\nC3\n0.53\n0.51\n128.5\n61.7\nTable 13. The average accuracy and ToC (in seconds) for different conditions\n5.4\nData collection\nFor each SQL task, we collected three types of data from the participant: (1) the updated SQL\nquery after their repair; (2) the starting and ending time; (3) the user’s interaction log with the\nerror handling mechanism (e.g., clicking to view the sampled table, opening the drop-down menu,\ninteracting with the chatbot).\nWe cleaned up the data from the participants through the following steps. First, we filtered out\nthe queries that are skipped by the participants (i.e., the user clicking on “Next” without making\nany changes to the query), which consist of less than 10% of the total data. Second, if the participant\ndid not utilize the interaction mechanism associated with the experimental condition at all (e.g.,\nthe user inspected the query without using any assistance and modified the query manually), the\ntask was deemed to be solved using the baseline method.\n5.5\nResults\nIn this section, we report the key findings on the efficiency, effectiveness, and usability of different\nerror handling mechanisms and their user experiences. For each condition in a statistical test, the\ndata is sampled evenly and randomly.\nF1: The error handling mechanisms do not significantly improve the accuracy of fixing\nerroneous SQL queries. To start with, we conducted a one-way ANOVA test (𝛼=0.05) among tasks\nthat used different error handling mechanisms. The p-value for the accuracy was 0.82, indicating\nthat there were no significant differences between the different error handling methods. The average\naccuracy and standard deviation among the participants are shown in Table 13.\nWe then analyzed the effect of different mechanisms on the accuracy of fixing specific error\ntypes, including five common syntactic error types (A: WHERE error; B: JOIN error; C: ORDER\nBY error; D: SELECT error; E: GROUP BY error, as well as six semantic errors shown in\nTable 7. Using the same statistical test, we found that the p-values for all types of error were higher\nthan the 0.05 threshold, indicating that there were no significant differences in accuracy when the\nuser used different error-handling mechanisms (Table 14).\nSyntactic types\nSemantic types\nA\nB\nC\nD\nE\na\nb\nc\nd\ne\nf\nB1\n0.42\n0.40\n0.38\n0.67\n0.29\n0.40\n0.58\n0.52\n0.45\n0.32\n0.25\nC1\n0.40\n0.44\n0.27\n0.56\n0.24\n0.42\n0.58\n0.53\n0.32\n0.42\n0.28\nC2\n0.40\n0.42\n0.31\n0.60\n0.29\n0.30\n0.53\n0.42\n0.28\n0.32\n0.32\nC3\n0.62\n0.33\n0.31\n0.60\n0.38\n0.33\n0.57\n0.52\n0.28\n0.27\n0.22\nAvg. Acc.\n0.46\n0.40\n0.32\n0.61\n0.30\n0.36\n0.57\n0.50\n0.33\n0.33\n0.27\nSD\n0.50\n0.49\n0.47\n0.49\n0.46\n0.48\n0.50\n0.50\n0.47\n0.47\n0.44\np-value\n0.10\n0.73\n0.73\n0.76\n0.58\n0.51\n0.94\n0.57\n0.17\n0.37\n0.64\nTable 14. The accuracy of error handling for different types of errors under each condition.\n, Vol. 1, No. 1, Article . Publication date: February 2023.\n21\nFurthermore, we found that different error-handling mechanisms did not significantly influence\nthe accuracy of SQL query error handling at various difficulty levels (Table 15). These findings\nsuggest that existing interaction mechanisms are not very effective for handling NL2SQL errors\nthat state-of-the-art deep learning NL2SQL models make on complex datasets like Spider. We\nfurther discuss the reasons behind these results and their implications in the rest of Section 5.5 and\nSection 6.\nDifficulty levels\nEasy\nMedium\nHard\nB1\n0.64\n0.64\n0.21\nC1\n0.71\n0.64\n0.36\nC2\n0.79\n0.71\n0.36\nC3\n0.79\n0.50\n0.29\nAvg. Acc\n0.73\n0.63\n0.30\nSD\n0.45\n0.49\n0.46\np-value\n0.81\n0.71\n0.83\nTable 15. The error-handling of different difficulty levels under each condition\nF2: The error handling mechanisms do not significantly impact the overall time of\ncompletion. To study the impact of different error handling mechanisms on time usage, we\nanalyzed the time of completion (ToC) of the query that was solved correctly by the participants.\nWe used the same ANOVA test as applied in the previous analysis to test the mean difference\namong ToC using various error handling mechanisms (Table 13), no significant significance was\nfound among the groups (𝑝= 0.52).\nSimilarly, we analyzed the impact of different error-handling mechanisms on the selected error\ntypes. In general, the baseline method was more efficient in solving a task, while the conversational\ndialog system took more time compared with other methods. The results are shown in Table 16.\nAdditionally, the results of experiments on SQL queries of various levels of difficulty revealed\ndifferences among the error-handling mechanisms tested in the case of easy queries (𝑝= 0.04).\nSpecifically, direct editing was found to be the fastest method when the query was easy, followed\nby the explanation and example-based approach (C1), the explanation-based visualization approach\n(C2), and the conversational dialog system (C3).\nSyntactic types\nSemantic types\nA\nB\nC\nD\nE\na\nb\nc\nd\ne\nf\nB1\n112.0\n98.5\n97.5\n109.7\n109.6\n104.0\n93.8\n116.0\n130.0\n121.7\n103.0\nC1\n103.3\n103.8\n91.1\n104.6\n101.7\n96.1\n103.9\n110.1\n107.0\n97.9\n83.4\nC2\n117.9\n108.2\n86.2\n96.8\n93.0\n99.5\n119.0\n129.7\n108.7\n105.2\n95.0\nC3\n129.2\n110.3\n123.6\n125.8\n133.8\n118.4\n125.0\n148.6\n116.2\n125.6\n125.0\nAvg. ToC\n115.6\n105.2\n99.6\n109.2\n109.5\n104.5\n110.4\n126.1\n115.5\n106.3\n101.6\nSD\n33.2\n68.8\n48.5\n45.0\n73.1\n53.3\n37.5\n77.2\n73.6\n49.1\n59.3\np-value\n0.87\n0.99\n0.39\n0.60\n0.70\n0.82\n0.24\n0.71\n0.91\n0.17\n0.48\nTable 16. The average ToC of different error types under each condition.\nF3: Users perform better on error types with fewer variants. We analyzed the impact of error\ntypes on task accuracy and ToC, and reported the results in Table 18. The results revealed that\namong the syntactic error types, A: WHERE errors and E: GROUP BY errors had high accuracy,\nwhile for semantic error types, d: Value error and e: Condition error had high accuracy.\n, Vol. 1, No. 1, Article . Publication date: February 2023.\n22\nZheng Ning, Yuan Tian, Zheng Zhang, Tianyi Zhang, and Toby Jia-Jun Li\nDifficulty levels\nEasy*\nMedium\nHard\nB1\n31.8\n124.4\n133.6\nC1\n55.8\n110.4\n154.2\nC2\n79.0\n110.5\n199.1\nC3\n95.7\n137.7\n125.3\nAvg. ToC\n65.6\n120.7\n153.1\nSD\n50.9\n96.2\n97.7\np-value\n0.04\n0.60\n0.39\nTable 17. The average ToC of different difficulty levels under each condition. *statistically significant difference\n(𝑝< 0.05)\nAs shown in the error taxonomy (Table 4), value errors occur only in the WHERE clauses, and\nthose errors usually require fewer steps to fix and have little relationship with the other syntactic\nparts in an SQL query. Similarly, condition errors such as wrong sorting directions and wrong\nboolean operator (AND, OR, etc.) are relatively independent components in a query. The\nbetter user performance on those error types may indicate that users face challenges in handling\nsemantically complicated errors, such as joining tables and selecting columns from multiple tables,\nbut are more successful in discovering and repairing error types where the error is more local (i.e.,\nwith little interdependency with other parts of the query). This conclusion is also evidenced in the\nuser interview, which we will analyze in the following section.\nSyntactic types\nAvg. Acc. (𝜇= 0.53)\nSD (𝜇= 0.50)\nAvg. ToC (𝜇= 128.8)\nSD (𝜇= 91.4)\nA\n0.56\n0.51\n132.3\n105.5\nB\n0.48\n0.50\n147.2\n90.4\nC\n0.53\n0.51\n128.2\n75.6\nD\n0.53\n0.47\n111.5\n55.5\nE\n0.55\n0.51\n125.1\n72.4\nSemantic types\nAvg. Acc. (𝜇= 0.54)\nSD (𝜇= 0.50)\nAvg. ToC (𝜇= 123.1) (N=26)\nSD (𝜇= 80.53)\na\n0.47\n0.49\n123.0\n67.4\nb\n0.54\n0.51\n123.3\n68.2\nc\n0.50\n0.51\n128.7\n68.7\nd\n0.61\n0.47\n118.1\n96.5\ne\n0.60\n0.49\n116.7\n83.2\nf\n0.51\n0.51\n128.1\n66.8\nTable 18. The average accuracy and ToC (in seconds) for different error types.\nF4: The explanation- and example-based methods are more useful for non-expert users.\nWhen participants were asked to rate their preferences among the different interaction mecha-\nnisms (shown in Table 19), we found that the explanation- and example-based approach (C1) is\nthe most preferred, while the explanation-based visualization approach (C2) was rated similarly\nto the baseline method (B1). In contrast, the conversational dialog system (C3) was generally rated\nas less useful than the others.\nWe found that the user’s level of expertise significantly impacts their adoption rate of different\nerror-handling mechanisms. The adoption rate measures when a mechanism was available, and\nhow likely that a user will use the mechanism (instead of just using the baseline method) to handle\nthe error. We calculated the adoption rate for each condition (C1, C2, and C3) for different levels of\n, Vol. 1, No. 1, Article . Publication date: February 2023.\n23\nMost useful\n2nd most useful\n3rd most useful\nleast useful\nB1\n7\n4\n9\n6\nC1\n13\n10\n3\n0\nC2\n5\n8\n9\n4\nC3\n1\n4\n5\n16\nTable 19. The participants’ ranked preferences for different error handling mechanisms\nexpertise by dividing the number of SQL queries in which the participant used the provided error-\nhandling mechanism by the total number of queries provided with the corresponding mechanism\nin the participant’s study session. The result is shown in Table 20.\nExpertise levels\nC1 (𝜇= 0.74)\nC2 (𝜇= 0.74)\nC3 (𝜇= 0.41)\nExpert\n0.53\n0.43\n0.41\nIntermediate\n0.84\n0.90\n0.44\nNovice\n0.86\n0.88\n0.38\nTable 20. The adoption rate of each mechanism among different expertise levels\nThe primary factor contributing to the lower level of interest in using error handling mechanisms\namong expert participants under the experimental conditions was their ability to efficiently identify\nand repair errors independently. For example, P2 stated that “It (the step-by-step execution function in\nC1) is very redundant and time-consuming to break down the SQL queries and execute the sub-queries,\nsince most errors can be found at first glance.” Another reason why expert users were less interested\nin using the error handling mechanisms was that they were not confident in the intermediate\nresults they provided. P3, for example, noted that “Though the chatbot is capable of revising the\nerroneous SQL queries, I found it sometimes gives an incorrect answer and provides no additional clues\nfor me to validate the new query.” Therefore, several expert participants chose to repair the original\nSQL query instead of validating and repairing the newly generated query.\nThe study also showed that the conversational dialog system (C3) was the least preferred\nmechanism among users at all levels of expertise. One reason for this is the relatively low accuracy\nof the model in recognizing user intents from the dialog and automatically repairing the errors in\nthe query. For example, P3 stated that “Though it sometimes predicts the correct query, for most of\nthe times, the prediction is still erroneous.” In addition, the chatbot did not provide explanations for\nits suggestions, so users had to spend significant effort to validate and repair the newly generated\nSQL queries. Furthermore, while the chatbot allowed manual input from users to intervene in the\nprediction process, such as pointing out erroneous parts and providing correct answers, it often\nintroduced new errors while predicting the SQL. As noted by P7: “In one example, when I asked the\nchatbot to change the column name that was in SELECT, it somehow changes the column in JOIN as\nwell.” As a result, many users quickly became frustrated after using it for a few SQL queries.\nF5: The explanation- and example-based methods are more effective in helping users\nidentify errors in the SQL query than in repairing errors. In the post-study questionnaire, we\nasked participants to evaluate the usefulness of each condition in terms of its ability to help (1)\nidentify and (2) repair errors, respectively (Fig. 6). The results indicate that most of the participants\nfound C1 to be effective in identifying incorrect parts of the SQL query, while half of them thought\nit was not useful for repairing errors. Meanwhile, a notable proportion of participants (12 out of\n26) affirmed C2’s effectiveness in identifying the errors, but it was helpful for repairing the errors.\n, Vol. 1, No. 1, Article . Publication date: February 2023.\n24\nZheng Ning, Yuan Tian, Zheng Zhang, Tianyi Zhang, and Toby Jia-Jun Li\nIn terms of C3, a significant number of participants (16 and 18) had a negative perception of its\neffectiveness in both identifying and repairing errors within the SQL query.\nFig. 6. The result of the post-study questionnaire\nFurthermore, we learned that the recursive natural language explanations might help reduce the\nunderstanding barrier for a long and syntactic-complicated SQL query. For example, P8 stated that\n“By looking at the shorter sentences first at the beginning, I could finally understand the meaning that\nthe original long sentence were trying to convey.” P17 also mentioned that: “Those shorter sentences\nusually did not have complex grammatical structures and confusing inter-table relationships, so that the\nproblems were easier to be spotted.” Additionally, executing the subquery and displaying the results\nwere deemed helpful for localizing the erroneous parts in the original SQL query. For example, P23\nstated: “When I noticed that the retrieved result was empty, I realized that some problems should\nexist in the current query.” In terms of C2, participants affirmed the effectiveness of graph-based SQL\nvisualization in helping them better understand the relationship between the syntactical components\nof a query. The learning barrier of this approach was also the lowest among all experimental\nconditions: users could view the connections to a table by simply clicking the widget in the canvas.\nThen, we investigated why the participants were less satisfied with the effectiveness of repairing\nerrors in a SQL query for C1. There were two main factors. First, the repair strategies supported\nby the error-handling mechanisms were limited. Specifically, participants could only replace the\nincorrect parts with their correct substitutions using the drop-down menu of entity mappings, but\nfor queries that require the addition, deletion, or reorganization of clauses, users had to manually\nedit the query. This limitation led to frustration among participants and ultimately resulted in them\nnot prioritizing using this error-handling mechanism for future tasks. Second, the current approach\nprovided little assistance for users in validating their edits. As a result, one participant stated that:\n“I did not trust my own edits nor the suggested changes from the approach.” (P20).\n6\nDISCUSSION AND IMPLICATIONS\n6.1\nImproving NL2SQL model evaluations through the error taxonomy\nCurrently, the evaluation of NL2SQL models emphasizes their accuracy from benchmark datasets.\nThough it is fair and effective in indicating the overall performance of the model, it fails in evaluating\nthe model at a more fine-grained level, which impedes the development of error-handling strategies\nand the model’s real-world application.\n, Vol. 1, No. 1, Article . Publication date: February 2023.\n25\nThe error taxonomy we contributed helps us understand the types of syntactic and semantic\nerrors that a particular NL2SQL model tends to make in addition to only the overall accuracy.\nThis information can provide model developers with specific information to improve the model’s\nrobustness against certain error types, thereby developing more accurate NL2SQL models. Future\nwork can focus on the technical solutions on how to design better NL2SQL models based on the\ntaxonomy. Second, this taxonomy allowed us to make fine-grained comparisons between models\nbeyond the accuracy metrics. By comparing the error distributions of different models, we can\nidentify not only the relative advantages of individual models but also the common errors that\ncurrent models are prone to make.\nOur work delves deep into the errors of the models, revealing that though model architectures\nand performances differ, they all exhibit high error rates in particular error types. On the other\nhand, some models, though have a relatively low overall accuracy, are capable of handling particular\ntypes of errors. Future work could focus on studying one of those high error-rate error types to\nincrease the model performance.\n6.2\nDesign opportunities for NL2SQL error handling mechanisms\nThe result of our empirical study suggests that existing error handling mechanisms do not per-\nform as well on errors made by state-of-the-art deep-learning-based NL2SQL models on complex\ncross-domain tasks, despite the promising results reported in the respective evaluations of these\nmechanisms. We think the main reason could be that our study used a much more challenging\ndataset than what was used in prior studies. We used queries from Spider [90] (which is complex and\ncross-domain) that the state-of-the-art of NL2SQL models (instead of the earlier NL2SQL models,\nwhich would start to make errors on simpler SQL queries) failed on. The dataset used in our study\nmore accurately represents realistic error scenarios that users encounter in natural language data\nqueries. Here, we identified several design opportunities for more effective NL2SQL error-handling\nmechanisms.\n6.2.1\nEnabling effective mixed-initiative collaboration between users and error handling tools. Our\nfindings indicate that the current error-handling tools for NL2SQL models do not provide suffi-\ncient feedback to users when they attempt to modify SQL queries. While existing error-handling\nmechanisms, such as the conversational dialog approach (C3), have focused on predicting correct\nmodifications using static forms of user input, they have not adequately addressed the need for\nmechanisms to elicit useful human feedback to guide model prediction. For example, in C3, users pro-\nvide input in the form of multiple-choice options for the recommended locations of potential errors,\nwhich was considered confusing and not useful by some participants, particularly when “none of the\nrecommended options made sense” (P15) or “the errors existed in multiple places and cannot be fixed\nby only selecting one answer” (P24). Therefore, we suggest that future work should focus on the de-\nvelopment of effective mixed initiative mechanisms that allow both users and error-handling tools to\ndevelop a mutual understanding of the model’s current state of understanding and the user’s intent.\n6.2.2\nImplementing interactive approaches based on attention alignment. Our study and analysis\non attention have proved the correlation between erroneous queries and attention misalignment,\nsuggesting that a potential way of designing an error-handling mechanism is to enable users to\ncorrect the misalignment. However, a majority of existing work focuses on automated attention\nalignment in the decoding process of an NL2SQL model without involving humans. For example,\nsome work [6, 46, 91] uses an external model trained on a dataset of human attention to adjust\nthe model attention, while some works may use a statistical [54] approach. Nevertheless, aligning\nattention automatically has limitations such as requiring the design of a model-specific alignment\n, Vol. 1, No. 1, Article . Publication date: February 2023.\n26\nZheng Ning, Yuan Tian, Zheng Zhang, Tianyi Zhang, and Toby Jia-Jun Li\nmechanism and the preparation of a dataset of human attention, which can not be generalized\nefficiently.\nOur study of model attention provides a supportive theory for designing interactive approaches\nfor NL2SQL models. In fact, existing interactive mechanisms can also be viewed as the imple-\nmentation of attention alignment between the user and the model. For instance, MISP [88] asks\nclarification questions to users when uncertain about a generated token. This QA procedure aims\nto force the model’s attention to align with the user’s attention when the model’s attention is very\nlikely to be deviated. Our study also highlights the possibility of designing an attention-based error-\nhandling mechanism. For example, the interactive approach can enable users to comprehend and\ndirectly manipulate the model’s attention. Various approaches may employ different mechanisms\nat different layers of attention. But as long as humans are included in the loop, the model has an\nopportunity to align its attention with the human’s attention through the model’s explanation and\nuser feedback.\n6.2.3\nComprehending the generated queries and inspecting how queries operate on data complement\neach other. The results of the study suggested that, to support effective NL2SQL error handling for\nusers, it is important to help users (1) interpret the meaning of the generated SQL query, untangle\nits structures, and explain how it corresponds to the user’s NL query; and (2) inspect the behaviors\nof the query on example data and examine whether they match user intents and expectations. The\ntwo parts are interdependent on each other. In practice, the user’s preferences for these different\napproaches may vary depending on their expertise. For example, in our study, non-users and\nnovice SQL users appreciated the explanation-based visualization mechanism (in SQLVis [51])\nand the NL explanations in step-by-step execution of the generated queries (in DIY [52]), because\nthese mechanisms lower the barrier to understanding the generated SQL queries for users who\nare unfamiliar with SQL syntax and structures. This preference was also reflected in their use of\ndifferent mechanisms in the study. Experienced SQL users, on the contrary, did not use mechanisms\nfor explaining the meanings of the generated SQL queries as often. However, they found the entity\nmapping feature and the example tables (in DIY) useful for discovering NL2SQL errors.\n6.2.4\nOpportunities for adaptive strategies. Lastly, the results of our user study suggest that the\nmost effective error-handling strategy to use depends on many factors such as user expertise, query\ntype, and possible error types. For example, expert users may require less sense-making strategies\n(e.g., step-by-step NL explanation), while they may expect an intuitive execution result preview\nor an efficient validation of the updated answer. In contrast, intermediate or novice users may\nneed more mixed-initiative guides to facilitate error discovery and repair. Meanwhile, as discussed\nin Section 5.5, the length, syntactical components, and potential error types of a query would\nresult in different barriers to users when repairing errors. For example, for queries with more\ncomplicated syntactical structures, a visualization-based approach might be useful to reduce the\nbarrier to understanding the structure of the query. Therefore, we recommend that future work\nin this area consider the development of adaptive error-handling strategies. An effective NL2SQL\nsystem could adapt its interface features, models, and interaction strategies according to the use\ncase and context. Specifically, it could consider the semantic and syntactic characteristics of the\nquery, whether the error is local (i.e., on a specific entity in the query) or global (i.e., regarding the\noverall query structure), and the user’s preferences and level of expertise.\n7\nLIMITATIONS AND FUTURE WORK\nThe current study has several limitations. First, the total number is unbalanced for each error type\n(as shown in Section 3.5.1), which may cause bias in the study of error-handling mechanisms in\nSection 5. Despite the fact that Spider is already a large-scale dataset, there were only a small\n, Vol. 1, No. 1, Article . Publication date: February 2023.\n27\nnumber of example errors in some rare error types. Therefore, we have to exclude these error types\nin our analysis. The problem could be addressed by conducting larger-scale user studies with more\nparticipants and erroneous query data.\nSecond, despite that we reproduced four representative NL2SQL models based on the model\narchitecture, it is hard to cover all due to the lack of open-source implementation or the engineering\nchallenges in adapting them to our analysis pipeline. In addition, all the models used in our study\nare “black-box” models that do not provide much transparency into the process, which limits the\nselection of error-handling mechanisms. Interactive models [19, 73], on the other hand, provide\nthe transparency that could allow additional error handling mechanisms, such as modifying the\nintermediate results of the model predictions. In future work, we will expand the scope of our\nresearch to include additional types of representative NL2SQL models.\nThird, in the cause analysis study, we only explored one of the possible causes — attention\nmisalignment, a more comprehensive analysis could be conducted to build up the factors that\ncontribute to the model’s performance. Additionally, we only tested one model in our study due to\nthe computational resource and time limit. In future work, it could be valuable to explore recently\nemerged large open-sourced foundational models such as LlaMa-2 [78].\nLastly, while the example SQL queries were real erroneous queries made by NL2SQL models\non realistic databases and natural language queries, the setting of our study is still quite artificial,\nlacking the real-world task context in the actual usage scenarios of NL2SQL systems. In the future,\nit will be useful to study user error handling behaviors through a field study to better understand\nthe impact of task-specific contexts on user behavior and the effectiveness of user handling of\nNL2SQL errors.\n8\nCONCLUSION\nIn this paper, we i) presented an empirical study to understand the error types in the SQL query\ngenerated by NL2SQL models; ii) explored a possible cause of model errors by measuring the\nalignment of model-user attention on the NL query, and iii) conducted a controlled user experiment\nwith 26 participants to measure the effectiveness and efficiency of representative NL2SQL error\nhandling mechanisms. The error taxonomy summarized 48 types of errors and revealed their\ndistribution characteristics. Our study also demonstrated a strong correlation between the cause of\nmodel errors and the misalignment of attention between humans and models. The results of the user\nexperiment revealed challenges and limitations of existing NL2SQL error handling mechanisms on\nerrors made by state-of-the-art deep-learning-based NL2SQL models on complex cross-domain\ntasks. Based on the results, we identified several research opportunities and design implications for\nmore effective and efficient mechanisms for users to discover and repair errors in natural language\ndatabase queries.\nACKNOWLEDGMENTS\nThis research was supported in part by an AnalytiXIN Faculty Fellowship, an NVIDIA Academic\nHardware Grant, a Google Cloud Research Credit Award, NSF grant CCF-2211428, and NSF grant\nITE-2333736. Any opinions, findings, or recommendations expressed here are those of the authors\nand do not necessarily reflect the views of the sponsors.\nREFERENCES\n[1] Mohammad Aliannejadi, Manajit Chakraborty, Esteban Andrés Ríssola, and Fabio Crestani. 2020. Harnessing Evolution\nof Multi-Turn Conversations for Effective Answer Retrieval. In Proceedings of the 2020 Conference on Human Information\nInteraction and Retrieval (Vancouver BC, Canada) (CHIIR ’20). Association for Computing Machinery, New York, NY,\nUSA, 33–42. https://doi.org/10.1145/3343413.3377968\n, Vol. 1, No. 1, Article . Publication date: February 2023.\n28\nZheng Ning, Yuan Tian, Zheng Zhang, Tianyi Zhang, and Toby Jia-Jun Li\n[2] James Allen, Nathanael Chambers, George Ferguson, Lucian Galescu, Hyuckchul Jung, Mary Swift, and William\nTaysom. 2007. PLOW: A Collaborative Task Learning Agent. In Proceedings of the 22nd National Conference on Artificial\nIntelligence - Volume 2 (Vancouver, British Columbia, Canada) (AAAI’07). AAAI Press, 1514–1519.\n[3] James F. Allen, Bradford W. Miller, Eric K. Ringger, and Teresa Sikorski. 1996. A Robust System for Natural Spoken\nDialogue. In Proceedings of the 34th Annual Meeting on Association for Computational Linguistics (Santa Cruz, California)\n(ACL ’96). Association for Computational Linguistics, USA, 62–70. https://doi.org/10.3115/981863.981872\n[4] Axel Antoine, Sylvain Malacria, Nicolai Marquardt, and Géry Casiez. 2021. Interaction Illustration Taxonomy:\nClassification of Styles and Techniques for Visually Representing Interaction Scenarios. In Proceedings of the 2021 CHI\nConference on Human Factors in Computing Systems. 1–22.\n[5] Christopher Baik, Hosagrahar V Jagadish, and Yunyao Li. 2019. Bridging the semantic gap with SQL query logs in\nnatural language interfaces to databases. In 2019 IEEE 35th International Conference on Data Engineering (ICDE). IEEE,\n374–385.\n[6] Aakash Bansal, Bonita Sharif, and Collin McMillan. 2023. Towards Modeling Human Attention from Eye Movements\nfor Neural Source Code Summarization. Proc. ACM Hum.-Comput. Interact. 7, ETRA, Article 167 (may 2023), 19 pages.\nhttps://doi.org/10.1145/3591136\n[7] Gagan Bansal, Besmira Nushi, Ece Kamar, Walter S Lasecki, Daniel S Weld, and Eric Horvitz. 2019. Beyond accuracy: The\nrole of mental models in human-AI team performance. In Proceedings of the AAAI Conference on Human Computation\nand Crowdsourcing, Vol. 7. 2–11.\n[8] Jonathan Berant, Daniel Deutch, Amir Globerson, Tova Milo, and Tomer Wolfson. 2019. Explaining queries over web\ntables to non-experts. In 2019 IEEE 35th International Conference on Data Engineering (ICDE). IEEE, 1570–1573.\n[9] Sonia Bergamaschi, Francesco Guerra, Matteo Interlandi, Raquel Trillo Lado, Yannis Velegrakis, et al. 2013. QUEST: a\nkeyword search system for relational data based on semantic and machine learning techniques. (2013).\n[10] Ben Bogin, Matt Gardner, and Jonathan Berant. 2019. Representing schema structure with graph neural networks for\ntext-to-SQL parsing. arXiv preprint arXiv:1905.06241 (2019).\n[11] Virginia Braun and Victoria Clarke. 2006. Using thematic analysis in psychology. Qualitative research in psychology 3,\n2 (2006), 77–101.\n[12] Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning. 2019. What Does BERT Look at? An\nAnalysis of BERT’s Attention. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural\nNetworks for NLP. Association for Computational Linguistics, Florence, Italy, 276–286. https://doi.org/10.18653/v1/W19-\n4828\n[13] Felipe Costa, Sixun Ouyang, Peter Dolog, and Aonghus Lawlor. 2018. Automatic generation of natural language\nexplanations. In Proceedings of the 23rd international conference on intelligent user interfaces companion. 1–2.\n[14] Misha Denil, Alban Demiraj, and Nando de Freitas. 2014. Extraction of Salient Sentences from Labelled Documents.\nCoRR abs/1412.6815 (2014). arXiv:1412.6815 http://arxiv.org/abs/1412.6815\n[15] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional\nTransformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). ACL,\nMinneapolis, Minnesota, 4171–4186. https://doi.org/10.18653/v1/N19-1423\n[16] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional\nTransformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). Association\nfor Computational Linguistics, Minneapolis, Minnesota, 4171–4186. https://doi.org/10.18653/v1/N19-1423\n[17] Upol Ehsan, Brent Harrison, Larry Chan, and Mark O Riedl. 2018. Rationalization: A neural machine translation\napproach to generating natural language explanations. In Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics,\nand Society. 81–87.\n[18] Ahmed Elgohary, Saghar Hosseini, and Ahmed Hassan Awadallah. 2020. Speak to your parser: Interactive text-to-SQL\nwith natural language feedback. arXiv preprint arXiv:2005.02539 (2020).\n[19] Ahmed Elgohary, Christopher Meek, Matthew Richardson, Adam Fourney, Gonzalo A. Ramos, and Ahmed Hassan\nAwadallah. 2021. NL-EDIT: Correcting Semantic Parse Errors through Natural Language Interaction. In NAACL.\n[20] Joseph L Fleiss. 1971. Measuring nominal scale agreement among many raters. Psychological bulletin 76, 5 (1971), 378.\n[21] Han Fu, Chang Liu, Bin Wu, Feifei Li, Jian Tan, and Jianling Sun. 2023. CatSQL: Towards Real World Natural Language\nto SQL Applications. Proceedings of the VLDB Endowment 16, 6 (2023), 1534–1547.\n[22] Andrea Galassi, Marco Lippi, and Paolo Torroni. 2021. Attention in Natural Language Processing. IEEE Transactions on\nNeural Networks and Learning Systems 32, 10 (oct 2021), 4291–4308. https://doi.org/10.1109/tnnls.2020.3019893\n[23] Yujian Gan, Xinyun Chen, Jinxia Xie, Matthew Purver, John R Woodward, John Drake, and Qiaofu Zhang. 2021. Natural\nSQL: making SQL easier to infer from natural language specifications. arXiv preprint arXiv:2109.05153 (2021).\n, Vol. 1, No. 1, Article . Publication date: February 2023.\n29\n[24] Dawei Gao, Haibin Wang, Yaliang Li, Xiuyu Sun, Yichen Qian, Bolin Ding, and Jingren Zhou. 2023. Text-to-SQL\nEmpowered by Large Language Models: A Benchmark Evaluation. arXiv:2308.15363 [cs.DB]\n[25] Tong Gao, Mira Dontcheva, Eytan Adar, Zhicheng Liu, and Karrie G Karahalios. 2015. Datatone: Managing ambiguity\nin natural language interfaces for data visualization. In Proceedings of the 28th annual acm symposium on user interface\nsoftware & technology. 489–500.\n[26] Simret Araya Gebreegziabher, Zheng Zhang, Xiaohang Tang, Yihao Meng, Elena Glassman, and Toby Jia-Jun Li. 2023.\nPaTAT: Human-AI Collaborative Qualitative Coding with Explainable Interactive Rule Synthesis. In Proceedings of the\n2023 CHI Conference on Human Factors in Computing Systems (CHI ’23). ACM.\n[27] Barbara Grosz. 1983. Team: A transportable natural language interface system. In Proceedings of the Conference on\nApplied Natural Language Processing (1983). Association for Computational Linguistics.\n[28] Jiaqi Guo, Zecheng Zhan, Yan Gao, Yan Xiao, Jian-Guang Lou, Ting Liu, and Dongmei Zhang. 2019. Towards complex\ntext-to-sql in cross-domain database with intermediate representation. arXiv preprint arXiv:1905.08205 (2019).\n[29] Izzeddin Gür, Semih Yavuz, Yu Su, and Xifeng Yan. 2018. Dialsql: Dialogue based structured query generation.\nIn Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers).\n1339–1349.\n[30] Jonathan Herzig, Paweł Krzysztof Nowak, Thomas Müller, Francesco Piccinno, and Julian Martin Eisenschlos. 2020.\nTaPas: Weakly supervised table parsing via pre-training. arXiv preprint arXiv:2004.02349 (2020).\n[31] Junyang Huang, Yongbo Wang, Yongliang Wang, Yang Dong, and Yanghua Xiao. 2021.\nRelation Aware Semi-\nautoregressive Semantic Parsing for NL2SQL. arXiv preprint arXiv:2108.00804 (2021).\n[32] Siteng Huang, Min Zhang, Yachen Kang, and Donglin Wang. 2020. Attributes-Guided and Pure-Visual Attention\nAlignment for Few-Shot Recognition.\nArXiv abs/2009.04724 (2020).\nhttps://api.semanticscholar.org/CorpusID:\n221586045\n[33] Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, Jayant Krishnamurthy, and Luke Zettlemoyer. 2017. Learning a neural\nsemantic parser from user feedback. arXiv preprint arXiv:1704.08760 (2017).\n[34] Hyeonji Kim, Byeong-Hoon So, Wook-Shin Han, and Hongrae Lee. 2020. Natural language to SQL: where are we\ntoday? Proceedings of the VLDB Endowment 13, 10 (2020), 1737–1750.\n[35] Klaus Krippendorff. 2011. Computing Krippendorff’s alpha-reliability. (2011).\n[36] Shaopeng Lai, Qingyu Zhou, Jiali Zeng, Zhongli Li, Chao Li, Yunbo Cao, and Jinsong Su. 2022. Type-Driven Multi-Turn\nCorrections for Grammatical Error Correction. arXiv preprint arXiv:2203.09136 (2022).\n[37] Jonathan Lazar, Jinjuan Heidi Feng, and Harry Hochheiser. 2017. Research methods in human-computer interaction.\nMorgan Kaufmann.\n[38] Michael Levandowsky and David Winter. 1971. Distance between sets. Nature 234, 5323 (1971), 34–35.\n[39] Aristotelis Leventidis, Jiahui Zhang, Cody Dunne, Wolfgang Gatterbauer, HV Jagadish, and Mirek Riedewald. 2020.\nQueryVis: Logic-based diagrams help users understand complicated SQL queries faster. In Proceedings of the 2020 ACM\nSIGMOD International Conference on Management of Data. 2303–2318.\n[40] Fei Li and Hosagrahar V Jagadish. 2014. Constructing an interactive natural language interface for relational databases.\nProceedings of the VLDB Endowment 8, 1 (2014), 73–84.\n[41] Fei Li and Hosagrahar V Jagadish. 2014. NaLIR: An Interactive Natural Language Interface for Querying Relational\nDatabases. In Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data (Snowbird, Utah,\nUSA) (SIGMOD ’14). Association for Computing Machinery, New York, NY, USA, 709–712. https://doi.org/10.1145/\n2588555.2594519\n[42] Jiwei Li, Will Monroe, and Dan Jurafsky. 2016. Understanding Neural Networks through Representation Erasure. CoRR\nabs/1612.08220 (2016). arXiv:1612.08220 http://arxiv.org/abs/1612.08220\n[43] Toby Jia-Jun Li, Amos Azaria, and Brad A. Myers. 2017. SUGILITE: Creating Multimodal Smartphone Automation by\nDemonstration. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems (CHI ’17). ACM,\nNew York, NY, USA, 6038–6049. https://doi.org/10.1145/3025453.3025483\n[44] Toby Jia-Jun Li, Jingya Chen, Haijun Xia, Tom Michael Mitchell, and Brad A. Myers. 2020. Multi-Modal Repairs\nof Conversational Breakdowns in Task-Oriented Dialogs. Proceedings of the 33rd Annual ACM Symposium on User\nInterface Software and Technology (2020).\n[45] Toby Jia-Jun Li, Marissa Radensky, Justin Jia, Kirielle Singarajah, Tom M. Mitchell, and Brad A. Myers. 2019. PUMICE: A\nMulti-Modal Agent that Learns Concepts and Conditionals from Natural Language and Demonstrations. In Proceedings\nof the 32nd Annual ACM Symposium on User Interface Software and Technology (UIST 2019). ACM. https://doi.org/10.\n1145/3332165.3347899\n[46] Xintong Li, Guanlin Li, Lemao Liu, Max Meng, and Shuming Shi. 2019. On the Word Alignment from Neural Machine\nTranslation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Association for\nComputational Linguistics, Florence, Italy, 1293–1303. https://doi.org/10.18653/v1/P19-1124\n, Vol. 1, No. 1, Article . Publication date: February 2023.\n30\nZheng Ning, Yuan Tian, Zheng Zhang, Tianyi Zhang, and Toby Jia-Jun Li\n[47] Xi Victoria Lin, Richard Socher, and Caiming Xiong. 2020. Bridging textual and tabular data for cross-domain text-to-sql\nsemantic parsing. arXiv preprint arXiv:2012.12627 (2020).\n[48] Shusen Liu, Zhimin Li, Tao Li, Vivek Srikumar, Valerio Pascucci, and Peer-Timo Bremer. 2019. NLIZE: A Perturbation-\nDriven Visual Interrogation Tool for Analyzing and Interpreting Natural Language Inference Models. IEEE Transactions\non Visualization and Computer Graphics 25, 1 (jan 2019), 651–660. https://doi.org/10.1109/TVCG.2018.2865230\n[49] Scott M. Lundberg and Su-In Lee. 2017. A Unified Approach to Interpreting Model Predictions. In Proceedings of the\n31st International Conference on Neural Information Processing Systems (Long Beach, California, USA) (NIPS’17). Curran\nAssociates Inc., Red Hook, NY, USA, 4768–4777.\n[50] Nora McDonald, Sarita Schoenebeck, and Andrea Forte. 2019. Reliability and Inter-Rater Reliability in Qualitative\nResearch: Norms and Guidelines for CSCW and HCI Practice. Proc. ACM Hum.-Comput. Interact. 3, CSCW, Article 72\n(nov 2019), 23 pages. https://doi.org/10.1145/3359174\n[51] Daphne Miedema and George Fletcher. 2021. SQLVis: Visual Query Representations for Supporting SQL Learners. In\n2021 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC). IEEE.\n[52] Arpit Narechania, Adam Fourney, Bongshin Lee, and Gonzalo Ramos. 2021. DIY: Assessing the correctness of natural\nlanguage to sql systems. In 26th International Conference on Intelligent User Interfaces. 597–607.\n[53] Zheng Ning, Zheng Zhang, Tianyi Sun, Yuan Tian, Tianyi Zhang, and Toby Jia-Jun Li. 2023. An Empirical Study of\nModel Errors and User Error Discovery and Repair Strategies in Natural Language Database Queries. In Proceedings\nof the 28th International Conference on Intelligent User Interfaces (Sydney, NSW, Australia) (IUI ’23). Association for\nComputing Machinery, New York, NY, USA, 633–649. https://doi.org/10.1145/3581641.3584067\n[54] Franz Josef Och and Hermann Ney. 2003.\nA Systematic Comparison of Various Statistical Alignment\nModels.\nComputational Linguistics 29, 1 (03 2003), 19–51.\nhttps://doi.org/10.1162/089120103321337421\narXiv:https://direct.mit.edu/coli/article-pdf/29/1/19/1797914/089120103321337421.pdf\n[55] OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL]\n[56] Ana-Maria Popescu, Alex Armanasu, Oren Etzioni, David Ko, and Alexander Yates. 2004. Modern natural language\ninterfaces to databases: Composing statistical parsing with semantic tractability. In COLING 2004: Proceedings of the\n20th International Conference on Computational Linguistics. 141–147.\n[57] Mohammadreza Pourreza and Davood Rafiei. 2023. DIN-SQL: Decomposed In-Context Learning of Text-to-SQL with\nSelf-Correction. arXiv:2304.11015 [cs.CL]\n[58] Mohammadreza Pourreza and Davood Rafiei. 2023. DIN-SQL: Decomposed In-Context Learning of Text-to-SQL with\nSelf-Correction. arXiv preprint arXiv:2304.11015 (2023).\n[59] Marco Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. “Why Should I Trust You?”: Explaining the Predictions of\nAny Classifier. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational\nLinguistics: Demonstrations. Association for Computational Linguistics, San Diego, California, 97–101. https://doi.org/\n10.18653/v1/N16-3020\n[60] Laura Rieger and Lars Kai Hansen. 2020. A simple defense against adversarial attacks on heatmap explanations. arXiv\npreprint arXiv:2007.06381 (2020).\n[61] Ohad Rubin and Jonathan Berant. 2020. SmBoP: Semi-autoregressive bottom-up semantic parsing. arXiv preprint\narXiv:2010.12412 (2020).\n[62] Diptikalyan Saha, Avrilia Floratou, Karthik Sankaranarayanan, Umar Farooq Minhas, Ashish R Mittal, and Fatma Özcan.\n2016. ATHENA: an ontology-driven system for natural language querying over relational data stores. Proceedings of\nthe VLDB Endowment 9, 12 (2016), 1209–1220.\n[63] Wojciech Samek, Thomas Wiegand, and Klaus-Robert Müller. 2017. Explainable artificial intelligence: Understanding,\nvisualizing and interpreting deep learning models. arXiv preprint arXiv:1708.08296 (2017).\n[64] Mark S. Schlager and William C. Ogden. 1986. A cognitive model of database querying: a tool for novice instruction.\nIn CHI ’86.\n[65] Torsten Scholak, Nathan Schucher, and Dzmitry Bahdanau. 2021. PICARD: Parsing Incrementally for Constrained\nAuto-Regressive Decoding from Language Models. In Proceedings of the 2021 Conference on Empirical Methods in\nNatural Language Processing. Association for Computational Linguistics, Online and Punta Cana, Dominican Republic,\n9895–9901. https://doi.org/10.18653/v1/2021.emnlp-main.779\n[66] Vidya Setlur, Sarah E Battersby, Melanie Tory, Rich Gossweiler, and Angel X Chang. 2016. Eviza: A natural language\ninterface for visual analysis. In Proceedings of the 29th annual symposium on user interface software and technology.\n365–377.\n[67] Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. 2017. Learning Important Features through Propagating\nActivation Differences. In Proceedings of the 34th International Conference on Machine Learning - Volume 70 (Sydney,\nNSW, Australia) (ICML’17). JMLR.org, 3145–3153.\n[68] Huda Salim Al Shuaily and Karen Vera Renaud. 2016. A Framework for SQL Learning: Linking Learning Taxonomy,\nCognitive Model and Cross Cutting Factors. World Academy of Science, Engineering and Technology, International\n, Vol. 1, No. 1, Article . Publication date: February 2023.\n31\nJournal of Social, Behavioral, Educational, Economic, Business and Industrial Engineering 10 (2016), 3095–3101.\n[69] Alkis Simitsis and Yannis Ioannidis. 2009. DBMSs should talk back too. arXiv preprint arXiv:0909.1786 (2009).\n[70] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. 2014. Deep Inside Convolutional Networks: Visualising\nImage Classification Models and Saliency Maps. arXiv:1312.6034 [cs.CV]\n[71] Arjun Srinivasan and John Stasko. 2017. Orko: Facilitating multimodal interaction for visual exploration and analysis\nof networks. IEEE transactions on visualization and computer graphics 24, 1 (2017), 511–521.\n[72] Yu Su, Ahmed Hassan Awadallah, Madian Khabsa, Patrick Pantel, Michael Gamon, and Mark Encarnacion. 2017.\nBuilding natural language interfaces to web apis. In Proceedings of the 2017 ACM on Conference on Information and\nKnowledge Management. 177–186.\n[73] Yu Su, Ahmed Hassan Awadallah, Miaosen Wang, and Ryen W. White. 2018. Natural Language Interfaces with\nFine-Grained User Interaction: A Case Study on Web APIs. In The 41st International ACM SIGIR Conference on Research\n& Development in Information Retrieval (Ann Arbor, MI, USA) (SIGIR ’18). Association for Computing Machinery, New\nYork, NY, USA, 855–864. https://doi.org/10.1145/3209978.3210013\n[74] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017.\nAxiomatic Attribution for Deep Networks.\narXiv:1703.01365 [cs.LG]\n[75] Valentin Tablan, Danica Damljanovic, and Kalina Bontcheva. 2008. A natural language query interface to structured\ninformation. In European Semantic Web Conference. Springer, 361–375.\n[76] Ningzhi Tang, Meng Chen, Zheng Ning, Aakash Bansal, Yu Huang, Collin McMillan, and Toby Jia-Jun Li. 2023. An\nEmpirical Study of Developer Behaviors for Validating and Repairing AI-Generated Code. In 13th Annual Workshop at\nthe Intersection of PL and HCI (PLATEAU 2023).\n[77] Yuan Tian, Zheng Zhang, Zheng Ning, Toby Li, Jonathan K. Kummerfeld, and Tianyi Zhang. 2023. Interactive Text-to-\nSQL Generation via Editable Step-by-Step Explanations. In Proceedings of the 2023 Conference on Empirical Methods\nin Natural Language Processing, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational\nLinguistics, Singapore, 16149–16166. https://doi.org/10.18653/v1/2023.emnlp-main.1004\n[78] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste\nRozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume\nLample. 2023. LLaMA: Open and Efficient Foundation Language Models. arXiv:2302.13971 [cs.CL] https://arxiv.org/\nabs/2302.13971\n[79] Priyan Vaithilingam, Tianyi Zhang, and Elena L Glassman. 2022. Expectation vs. experience: Evaluating the usability\nof code generation tools powered by large language models. In Chi conference on human factors in computing systems\nextended abstracts. 1–7.\n[80] Shikhar Vashishth, Shyam Upadhyay, Gaurav Singh Tomar, and Manaal Faruqui. 2019. Attention Interpretability\nAcross NLP Tasks. CoRR abs/1909.11218 (2019). arXiv:1909.11218 http://arxiv.org/abs/1909.11218\n[81] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia\nPolosukhin. 2017. Attention Is All You Need. CoRR abs/1706.03762 (2017). arXiv:1706.03762 http://arxiv.org/abs/1706.\n03762\n[82] Bailin Wang, Richard Shin, Xiaodong Liu, Oleksandr Polozov, and Matthew Richardson. 2019. Rat-sql: Relation-aware\nschema encoding and linking for text-to-sql parsers. arXiv preprint arXiv:1911.04942 (2019).\n[83] Xieyang Wang, Mengyi Liu, Jianqiu Xu, and Hua Lu. 2023. NALMO: Transforming Queries in Natural Language for\nMoving Objects Databases. GeoInformatica (2023), 1–34.\n[84] Xiaxia Wang, Sai Wu, Lidan Shou, and Ke Chen. 2021. An Interactive NL2SQL Approach with Reuse Strategy. In\nInternational Conference on Database Systems for Advanced Applications. Springer, 280–288.\n[85] David H.D. Warren and Fernando C.N. Pereira. 1982. An Efficient Easily Adaptable System for Interpreting Natural\nLanguage Queries. American Journal of Computational Linguistics 8, 3-4 (1982), 110–122. https://aclanthology.org/J82-\n3002\n[86] Zhiyong Wu, Yun Chen, Ben Kao, and Qun Liu. 2020. Perturbed Masking: Parameter-free Probing for Analyzing\nand Interpreting BERT. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics.\nAssociation for Computational Linguistics, Online, 4166–4176. https://doi.org/10.18653/v1/2020.acl-main.383\n[87] Navid Yaghmazadeh, Yuepeng Wang, Isil Dillig, and Thomas Dillig. 2017. SQLizer: query synthesis from natural\nlanguage. Proceedings of the ACM on Programming Languages 1, OOPSLA (2017), 1–26.\n[88] Ziyu Yao, Yu Su, Huan Sun, and Wen-tau Yih. 2019. Model-based Interactive Semantic Parsing: A Unified Framework\nand A Text-to-SQL Case Study. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). Association for\nComputational Linguistics, Hong Kong, China, 5447–5458. https://doi.org/10.18653/v1/D19-1547\n[89] Pengcheng Yin, Graham Neubig, Wen-tau Yih, and Sebastian Riedel. 2020. TaBERT: Pretraining for joint understanding\nof textual and tabular data. arXiv preprint arXiv:2005.08314 (2020).\n, Vol. 1, No. 1, Article . Publication date: February 2023.\n32\nZheng Ning, Yuan Tian, Zheng Zhang, Tianyi Zhang, and Toby Jia-Jun Li\n[90] Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle\nRoman, Zilin Zhang, and Dragomir Radev. 2018. Spider: A Large-Scale Human-Labeled Dataset for Complex and\nCross-Domain Semantic Parsing and Text-to-SQL Task. In Proceedings of the 2018 Conference on Empirical Methods in\nNatural Language Processing. ACL, Brussels, Belgium, 3911–3921. https://doi.org/10.18653/v1/D18-1425\n[91] Jingyi Zhang and Josef van Genabith. 2021. A Bidirectional Transformer Based Alignment Model for Unsupervised\nWord Alignment. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language Processing (Volume 1: Long Papers). Association for Computational\nLinguistics, Online, 283–292. https://doi.org/10.18653/v1/2021.acl-long.24\n[92] Tianyi Zhang, Zhiyang Chen, Yuanli Zhu, Priyan Vaithilingam, Xinyu Wang, and Elena L Glassman. 2021. Interpretable\nprogram synthesis. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. 1–16.\n[93] Tianyi Zhang, London Lowmanstone, Xinyu Wang, and Elena L Glassman. 2020. Interactive program synthesis by\naugmented examples. In Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology.\n627–648.\n[94] Zheng Zhang, Ying Xu, Yanhao Wang, Bingsheng Yao, Daniel Ritchie, Tongshuang Wu, Mo Yu, Dakuo Wang, and\nToby Jia-Jun Li. 2022. StoryBuddy: A Human-AI Collaborative Chatbot for Parent-Child Interactive Storytelling with\nFlexible Parental Involvement. In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems\n(New Orleans, LA, USA) (CHI ’22). Association for Computing Machinery, New York, NY, USA, Article 218, 21 pages.\nhttps://doi.org/10.1145/3491102.3517479\n[95] Victor Zhong, Mike Lewis, Sida I. Wang, and Luke Zettlemoyer. 2020. Grounded Adaptation for Zero-shot Executable\nSemantic Parsing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP).\nAssociation for Computational Linguistics, Online, 6869–6882. https://doi.org/10.18653/v1/2020.emnlp-main.558\n[96] Victor Zhong, Mike Lewis, Sida I Wang, and Luke Zettlemoyer. 2020. Grounded adaptation for zero-shot executable\nsemantic parsing. arXiv preprint arXiv:2009.07396 (2020).\n[97] Victor Zhong, Caiming Xiong, and Richard Socher. 2017. Seq2sql: Generating structured queries from natural language\nusing reinforcement learning. arXiv preprint arXiv:1709.00103 (2017).\n[98] Bolei Zhou, Yiyou Sun, David Bau, and Antonio Torralba. 2018. Interpretable basis decomposition for visual explanation.\nIn Proceedings of the European Conference on Computer Vision (ECCV). 119–134.\n[99] Xiangyang Zhou, Lu Li, Daxiang Dong, Yi Liu, Ying Chen, Wayne Xin Zhao, Dianhai Yu, and Hua Wu. 2018. Multi-turn\nresponse selection for chatbots with deep attention matching network. In Proceedings of the 56th Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Papers). 1118–1127.\n, Vol. 1, No. 1, Article . Publication date: February 2023.\n",
  "categories": [
    "cs.HC"
  ],
  "published": "2024-02-11",
  "updated": "2024-02-11"
}