{
  "id": "http://arxiv.org/abs/2402.07376v2",
  "title": "Unsupervised Discovery of Object-Centric Neural Fields",
  "authors": [
    "Rundong Luo",
    "Hong-Xing Yu",
    "Jiajun Wu"
  ],
  "abstract": "We study inferring 3D object-centric scene representations from a single\nimage. While recent methods have shown potential in unsupervised 3D object\ndiscovery from simple synthetic images, they fail to generalize to real-world\nscenes with visually rich and diverse objects. This limitation stems from their\nobject representations, which entangle objects' intrinsic attributes like shape\nand appearance with extrinsic, viewer-centric properties such as their 3D\nlocation. To address this bottleneck, we propose Unsupervised discovery of\nObject-Centric neural Fields (uOCF). uOCF focuses on learning the intrinsics of\nobjects and models the extrinsics separately. Our approach significantly\nimproves systematic generalization, thus enabling unsupervised learning of\nhigh-fidelity object-centric scene representations from sparse real-world\nimages. To evaluate our approach, we collect three new datasets, including two\nreal kitchen environments. Extensive experiments show that uOCF enables\nunsupervised discovery of visually rich objects from a single real image,\nallowing applications such as 3D object segmentation and scene manipulation.\nNotably, uOCF demonstrates zero-shot generalization to unseen objects from a\nsingle real image. Project page: https://red-fairy.github.io/uOCF/",
  "text": "Published in Transactions on Machine Learning Research (02/2025)\nUnsupervised Discovery of Object-Centric Neural Fields\nRundong Luo∗†\nrundongluo@cs.cornell.edu\nCornell University\nHong-Xing Yu∗\nkoven@cs.stanford.edu\nStanford University\nJiajun Wu\njiajunwu@cs.stanford.edu\nStanford University\nReviewed on OpenReview: https: // openreview. net/ forum? id= ScEv13W2f1\nAbstract\nWe study inferring 3D object-centric scene representations from a single image. While\nrecent methods have shown potential in unsupervised 3D object discovery, they are limited\nin generalizing to unseen spatial configurations. This limitation stems from the lack of\ntranslation invariance in their 3D object representations. Previous 3D object discovery\nmethods entangle objects’ intrinsic attributes like shape and appearance with their 3D\nlocations. This entanglement hinders learning generalizable 3D object representations. To\ntackle this bottleneck, we propose the unsupervised discovery of Object-Centric neural Fields\n(uOCF), which integrates translation invariance into the object representation. To allow\nlearning object-centric representations from limited real-world images, we further introduce\na learning method that transfers object-centric prior knowledge from a synthetic dataset. To\nevaluate our approach, we collect four new datasets, including two real kitchen environments.\nExtensive experiments show that our approach significantly improves generalization and\nsample efficiency, and enables unsupervised 3D object discovery in real scenes. Notably,\nuOCF demonstrates zero-shot generalization to unseen objects from a single real image. The\nproject page is available at https://red-fairy.github.io/uOCF/.\n1\nIntroduction\nCreating factorized, object-centric 3D scene representations is a fundamental ability in human vision and\na long-standing topic of interest in computer vision and machine learning. Some recent work has explored\nunsupervised learning of 3D factorized scene representations from images alone (Stelzner et al., 2021; Yu\net al., 2022; Smith et al., 2023; Jia et al., 2023). These methods have delivered promising results in 3D object\ndiscovery and reconstruction from a simple synthetic image.\nHowever, existing methods fail to generalize to unseen spatial configurations and objects. A fundamental\nbottleneck is that their representations lack the invariance to the 3D positions of the objects. In particular,\nexisting methods represent 3D objects as implicit functions in the viewer’s coordinate frame, so that any\nchange related the coordinate frame (e.g., slight changes in an object’s location or subtle camera movements)\nmay lead to significant changes in the object representation even if the object remains the same. Therefore,\nexisting methods do not generalize when an object appears at an unseen location during inference.\nTo address this fundamental bottleneck, we propose the unsupervised discovery of Object-Centric neural\nFields (uOCF). Unlike existing methods, uOCF explicitly infers an object’s 3D location, disentangling it from\nthe object’s latent representation. This design builds translation invariance into the object representation, so\n∗Equal Contribution.\n†Work done while R. Luo was a visiting student at Stanford University.\n1\narXiv:2402.07376v2  [cs.CV]  14 Feb 2025\nPublished in Transactions on Machine Learning Research (02/2025)\nReconstruction (ours)\nUnseen\ninput image\nRearrangement (ours)\nReconstruction (uORF)\nInsertion (ours)\nFigure 1: We propose the unsupervised discovery of Object-Centric neural Fields (uOCF), which infers factorized 3D\nscene representations from an unseen real image, thus enabling scene reconstruction and manipulation from novel\nviews. We compare uOCF with the state-of-the-art method, uORF (Yu et al., 2022).\nthat the object’s latent only represents the intrinsics of the object (e.g., shape and appearance). This design\nsignificantly improves generalization. As showcased in Figure 1, uOCF can generalize to unseen real-world\nscenes. We train uOCF on sparse multi-view images without object annotations. During inference, uOCF\ntakes in a single image and generates a set of object-centric neural radiance fields (NeRFs) (Mildenhall et al.,\n2020) and a background NeRF.\nAnother advantage of our translation-invariant 3D object representation is that it facilitates learning 3D\nobject priors from simple scenes and generalizes to more complex scenes with unseen spatial configurations\nand objects. This further boosts sample efficiency and thus it is particularly beneficial when we deal with\nreal scenes where training data is often limited. We introduce an object prior learning method to this end.\nTo evaluate our approach, we introduce new challenging datasets for 3D object discovery, including two\nreal kitchen datasets and two synthetic room datasets. The two real datasets feature real-world kitchen\nbackgrounds and objects from multiple categories. The synthetic room datasets feature furniture with diverse,\nrealistic shapes and textures. Across all these datasets, uOCF yields high-fidelity discovery of object-centric\nneural fields, allowing applications such as unsupervised 3D object segmentation and scene manipulation from\na real image. uOCF shows strong generalization to unseen spatial configurations and high sample efficiency,\nand we showcase that it even allows zero-shot 3D object discovery on a few simple real scenes with unseen\nobjects. In summary, our contributions are threefold:\n• First, we highlight the overlooked role of translation invariance in unsupervised 3D object discovery. We\ninstantiate the idea by proposing the unsupervised discovery of Object-Centric neural Fields (uOCF),\nwhich builds translation invariance to the object representation.\n• Second, we introduce a 3D object prior learning method, which leverages uOCF’s translation-invariant\nproperty to learn category-agnostic object priors from simple scenes and generalize to different object\ncategories and scene layouts.\n• Lastly, we collect four challenging datasets, Room-Texture, Room-Furniture, Kitchen-Matte, and Kitchen-\nShiny, and show that uOCF significantly outperforms existing methods on these datasets, unlocking\nzero-shot, single-image object discovery. All code and data will be made public.\n2\nRelated Works\nUnsupervised 2D object discovery. Before the advent of deep learning, traditional methods for object\ndiscovery (often referred to as co-segmentation) primarily focused on locating visually similar objects across\n2\nPublished in Transactions on Machine Learning Research (02/2025)\nInput image\nFeature map\nDINO ViT\nConv\nInput view rotation\nQuery point position\nVolume Rendering\nDensity-Weighted Comp.\nNovel view\nReconstruction\n(a) Encoder\n(c) Object NeRF Decoder\nRelative PE\nCross-Attn.\nGlobal residual\nLift to 3D\n(b) Latent Inference Module\nLearnable \nobject queries\nInferred \nobject latents\nMLP\nMomentum Update\nFigure 2: With a single forward pass, uOCF processes a single image input to infer a set of object-centric radiance\nfields along with their 3D locations and background radiance field. uOCF is trained on sparse multi-view images from\na collection of scenes and uses a single image as input during inference.\na collection of images (Sivic et al., 2005; Russell et al., 2006), where objects are defined as visual words\nor clusters of patches (Grauman & Darrell, 2006; Joulin et al., 2010). This clustering concept was later\nincorporated into deep learning techniques for improved grouping results (Li et al., 2019; Vo et al., 2020).\nThe incorporation of deep probabilistic inference propelled the field towards factorized scene representation\nlearning (Eslami et al., 2016). These methods decompose a visual scene into several components, where\nobjects are often modeled as latent codes that can be decoded into image patches (Kosiorek et al., 2018;\nCrawford & Pineau, 2019; Jiang et al., 2020; Lin et al., 2020), phase values (Löwe et al., 2022; Gopalakrishnan\net al., 2024), scene mixtures (Greff et al., 2016; 2017; 2019; Burgess et al., 2019; Engelcke et al., 2019; Locatello\net al., 2020; Biza et al., 2023; Didolkar et al., 2023), or layers (Monnier et al., 2021). Notably, Seitzer et al.\n(2023) leveraged DINO features to extract robust image representations, while Daniel & Tamar (2022; 2023)\nintroduced approaches to disentangle object latents into 2D position and appearance attributes. Despite their\nefficacy in scene decomposition, they do not model the objects’ 3D nature.\nUnsupervised 3D object discovery. To capture the 3D nature of scenes and objects, recent works have\nexplored learning 3D-aware representations from point clouds (Wang et al., 2022), videos (Henderson &\nLampert, 2020), and multi-view images of either single scenes (Liang et al., 2022) or large datasets for\ngeneralization (Eslami et al., 2018; Chen et al., 2020; Sajjadi et al., 2022). More recent research focuses on\ninferring object-centric factorized scene representations from single images (Stelzner et al., 2021; Yu et al.,\n2022; Smith et al., 2023). Among these, Yu et al. (2022) proposed a method for the unsupervised discovery of\nobject radiance fields (uORF) from single images. Follow-up works (Smith et al., 2023) improved rendering\nefficiency by replacing NeRF with light fields (Sitzmann et al., 2021) or enhanced segmentation accuracy\nthrough bi-level query optimization (Jia et al., 2023). However, these representations often lack translation\ninvariance, which limits their robustness and generalization capabilities. In this work, we address this\nlimitation by incorporating translation invariance, resulting in significant improvements in both generalization\nand sample efficiency. For a more comprehensive review of related methods, we refer readers to Villa-Vásquez\n& Pedersoli (2024).\nObject-centric 3D reconstruction. Decomposing visual scenes on an object-by-object basis and estimating\ntheir semantic/geometric attributes has been explored in several recent works (Wu et al., 2017; Yao et al., 2018;\nKundu et al., 2018; Ost et al., 2021). Some approaches, such as AutoRF (Müller et al., 2022), successfully\nreconstruct specific objects (e.g., cars) from annotated images. Others decompose visual scenes into the\nbackground and individual objects represented by neural fields (Yang et al., 2021; Wu et al., 2022). Our work\ndiffers because of its emphasis on unsupervised learning. Another line of recent work focuses on lifting 2D\nsegmentation to reconstructed 3D scenes (Fan et al., 2022; Cen et al., 2023a;b). In contrast, our work aims at\nsingle-image inference, whereas these studies concentrate on multi-view reconstruction.\nGenerative neural fields. Neural fields have revolutionized 3D scene modeling. Early works have shown\npromising geometric representations (Sitzmann et al., 2019; Park et al., 2019). The seminal work on neural\n3\nPublished in Transactions on Machine Learning Research (02/2025)\nradiance fields (Mildenhall et al., 2020) has opened up a burst of research on neural fields. We refer the reader\nto recent survey papers (Tewari et al., 2020; Xie et al., 2022) for a comprehensive overview. In particular,\ncompositional generative neural fields such as GIRAFFE (Niemeyer & Geiger, 2021) and others (Nguyen-\nPhuoc et al., 2020; Wang et al., 2023b) also allow learning object representations from image collections. Yet,\nthey target unconditional generation and cannot tackle inference.\n3\nApproach\nGiven a single input image, our goal is to infer object-centric radiance fields (i.e., each discovered object is\nrepresented in its local object coordinate rather than the world or the viewer coordinates) and the objects’\n3D locations. The object-centric design not only boosts generalizability due to representation invariance, but\nalso allows learning object priors from scenes with different spatial layouts and compositional configurations.\nThe following provides an overview of our approach and then introduces the technical details.\n3.1\nModel Overview\nAs shown in Figure 2, uOCF consists of an encoder, a latent inference module, and a decoder.\nEncoder. From an input image I, the encoder extracts a feature map f ∈RN·C, where N = H · W\nis the spatial size of the feature map and C represents the number of channels. We set it as a frozen\nDINOv2-ViT (Oquab et al., 2023) followed by two convolutional layers.\nLatent inference module. The latent inference module infers the latent representation and position\nof the objects in the underlying 3D scene from the feature map. We assume that the scene is composed\nof a background environment and no more than K foreground objects. Therefore, the output includes a\nbackground latent zb ∈R1×D and a set of foreground object latent zf = [zfT\n1\nzfT\n2\n· · · zfT\nK ]T ∈RK×D with\ntheir corresponding positions {pwd\ni }K\ni=1 , where pwd\ni\n∈R3 denotes a position in the world coordinate. Note\nthat some object latent may be empty when the scene has < K objects.\nDecoder. Our decoder employs the conditional NeRF formulation g(x|z), which takes the 3D location x\nand the latent z as input and generates the radiance color and density for rendering. We use two MLPs, gb\nand gf, for the background environment and the foreground objects, respectively.\n3.2\nObject-Centric 3D Scene Modeling\nObject-centric latent inference. Our Latent Inference Module (LIM) aims at binding a set of learnable\nobject queries ( qf = [qfT\n1\nqfT\n2\n· · · qfT\nK ]T ∈RK×D) to the visual features of each foreground object, and\nanother query to the background features (qb ∈R1×D). The binding is modeled via the cross-attention\nmechanism with learnable linear functions Kb, Kf, Qb, Qf, Vb, Vf:\nAi,j =\nexp(Mi,j)\nP\nk exp(Mi,k),\nwhere M =\n1\n√\nD\n\u0014\nQb(qb) · Kb(f)T\nQf(qf) · Kf(f)T\n\u0015T\n∈RN×(K+1).\n(1)\nWe then calculate the update signals for queries via an attention-weighted mean of the input:\nub = (W(:,1))T · Vb(f) ∈R1×D;\nuf = (W(:,2:))T · Vf(f) ∈RK×D,\n(2)\nwhere Wi,j =\nAi,j\nP\nl Al,j is the normalized attention map. Queries are then updated by:\nqb ←qb + ub, qf ←qf + uf;\nqb ←qb + tb(qb), qf ←qf + tf(qf),\n(3)\nwhere tb and tf are MLPs. We repeat this procedure for T iterations, followed by concatenating the updated\nobject queries with the corresponding attention-weighted mean of the input feature map f (global residual),\nfinally delivering the background latent zb and foreground latent {zf\ni}K\ni=1.\nOur LIM is related to the Slot Attention (Locatello et al., 2020) while differs in several critical aspects. We\ndiscuss their relationship in Appendix C.1.\nObject location inference. To infer objects’ position along with their latent representation, we assign\na normalized image position pimg\ni\n∈[−1, 1]2 initialized as zero to each foreground object query, then\n4\nPublished in Transactions on Machine Learning Research (02/2025)\nStage 1: Learn 3D object prior \nfrom synthetic scenes with simple composition\nStage 2: Learn to discover objects from scenes\nwith diverse object category and spatial layout\nFigure 3: Our object-centric design allows learning 3D object priors that generalize across different scene configurations.\nWe first train our model to learn 3D object priors on simple synthetic scenes (e.g., single synthetic object), and then\nwe leverage the 3D object priors to learn to discover objects in more complex scenes with different object categories\nand spatial layouts. Note that no object annotation is needed in either stage.\niteratively update them by momentum m with the attention-weighted mean over the normalized 2D grid\nEabs ∈[−1, 1]N×2:\npimg\ni\n←(W(:,i+1))T · Eabs · (1 −m) + pimg\ni\n· m.\n(4)\nTo incorporate the inferred positions, we adopt the relative positional encoding (Biza et al., 2023) Epos\ni\n:=\nconcat([Eabs −pimg\ni\n, pimg\ni\n−Eabs]) ∈RN×4, i ∈{1, 2, · · · , K}, and Epos\n0\n:= concat([Eabs, −Eabs]), where\nconcat is the concatenation along the last dimension. Then, we re-write M in Eq. (1) as:\nM =\n1\n√\nD\n\n\nQb(qb) · Kb(f + h1(Epos\n0\n))T\nQf(qf\n1) · Kf(f + h1(Epos\n1\n))T\n· · ·\nQf(qf\nK) · Kf(f + h1(Epos\nK ))T\n\n\nT\n,\n(5)\nwhere h1 : R4 →RD is a linear function.\nOverall, LIM achieves a gradual binding between the queries and the objects in the scene through an iterative\nupdate of the queries and their locations. To address potential issues of duplicate object identification, we\ninvalidate one of two similar object queries with high similarity and positional proximity by the start of\nthe last iteration. Finally, a small bias term is added to the position to handle potential occlusion, i.e.,\npimg\ni\n←pimg\ni\n+ tanh(h2((W(:,i+1))T )) · α , where scaling hyperparameter α = 0.2 and h2 : RN →R2 is a linear\nfunction.\nThe 2D positions pimg\ni\nare then unprojected into the 3D world coordinate to obtain pwd\ni . To do this, we\nextend the rays by depth d · si, where d is the depth estimated by a monocular depth estimator (Ranftl et al.,\n2022) and {si}K\ni=1 are scaling terms predicted by a linear layer using the camera parameters and object latent\nas input.\nCompositional neural rendering. The object positions allow us to put objects in their local coordinates\nrather than the viewer or world coordinates, thereby obtaining object-centric neural fields. Technically, for each\n3D point x in the world coordinate, we transform it to the ith object’s local coordinate by xi = R · (x −pwd\ni ),\nwhere R denotes the input camera rotation matrix. We then retrieve the color and density of x in the\nforeground radiance fields as (ci, σi) = gf(xi|zf\ni) and in the background radiance field as (c0, σ0) = gb(x|zb).\nThese values are aggregated into the scene’s composite density and color (c, σ) using density-weighted means:\nσ =\nX\ni≥0\nωiσi, c =\nX\ni≥0\nωici, where ωi =\nσi\nP\nj≥0 σj\n.\n(6)\nFinally, we compute the pixel color by volume rendering. Our pipeline is trivially differentiable, allowing\nbackpropagation through all parameters simultaneously.\nDiscussion on extrinsics disentanglement. An object’s canonical orientation is ambiguous without\nassuming its category (Wang et al., 2019). Thus, we choose not to disentangle objects’ orientation since\nwe target category-agnostic object discovery.\nFurther, we observe that uOCF has learned meaningful\nrepresentations that can smoothly interpolate an object’s scale and orientation. Please refer to Appendix B\nfor visualization and analysis.\n5\nPublished in Transactions on Machine Learning Research (02/2025)\n(d) Kitchen-Shiny\n(a) Room-Texture\n(c) Kitchen-Matte\n(b) Room-Furniture\nFigure 4: Samples from our collected datasets, where Room-Texture and Room-Furniture consist of synthetic images,\nand Kitchen-Matte and Kitchen-Shiny consist of real photos.\n3.3\nObject Prior Learning\nUnsupervised discovery of 3D objects in complex scenes is inherently difficult due to multiple challenging\nambiguities. A major ambiguity is what defines an object. While existing methods define objects via visual\nappearance similarity (Yu et al., 2022) or priors from 2D segments (Chen et al., 2024), they suffer from\nunder-segmentation due to visual cluttering (Yu et al., 2022) or over-segmentation inherited from the 2D\nsupervision (Chen et al., 2024).\nWe explore addressing this challenge by learning 3D object priors from synthetic data. Existing methods\nhave difficulties learning generalizable 3D object priors, as their object representation is sensitive to spatial\nconfigurations: a minor shift in camera pose or object location, rather than the object itself, can lead to\ndrastic changes in the object representation. Thus, such learned object priors do not generalize when there\nare unseen spatial configurations.\nOur 3D object-centric representation mitigates this issue by translation invariance. In particular, we introduce\n3D object prior learning. We show an illustration in Figure 3. The main idea is to pre-train uOCF on\nsynthetic scenes that are constructed with a single object to ease the learning, similar to curriculum learning.\nAfter the pre-training stage, we proceed to training uOCF on the more complex scenes that may have different\nobject categories and spatial layouts. Note that either training stage does not require any object annotation.\nThe pre-training synthetic single-object dataset can be easily scaled up.\n3.4\nModel Training\nObject-centric sampling. To improve the reconstruction quality, we leverage an object’s local coordinates\nto concentrate the sampled points in proximity to the object. Specifically, we start dropping distant samples\nfrom the predicted object positions after a few training epochs when the model has learned to distinguish the\nforeground objects and predict their positions. This approach enables us to quadruple the number of samples\nwith the same amount of computation, leading to significantly improved robustness and visual quality.\nIn both training stages, we train our model across scenes, each with calibrated sparse multi-view images.\nFor each training step, the model receives an image as input, infers the objects’ latent representations and\npositions, renders multiple views from the input and reference poses, and compares them to the ground\ntruth images to calculate the loss. Model supervision consists of the MSE reconstruction loss ℓrecon and the\nperceptual loss ℓperc (Johnson et al., 2016) between the reconstructed and ground truth images. In addition,\nwe incorporate the depth ranking loss (Wang et al., 2023a) with pre-trained monocular depth estimators and\nbackground occlusion regularization (Yang et al., 2023) to minimize common floating artifacts in few-shot\nNeRFs.\nThe overall loss function is thus formulated as follows:\nL = ℓrecon + λpercℓperc + λdepthℓdepth + λoccℓocc.\n(7)\nWe leave further architectural details and illustrations in Appendix C.1.\n4\nExperiments\nWe evaluate our method on three tasks: unsupervised object segmentation in 3D, novel view synthesis,\nand scene manipulation in 3D. Below, we briefly describe the data collection process and experimental\nconfigurations, with additional details provided in Appendices C.2 and C.3. Sample code and data are\nincluded in the supplementary material, and we plan to release the full code and datasets for public use.\n6\nPublished in Transactions on Machine Learning Research (02/2025)\nTable 1: Object segmentation and view synthesis on Room-Texture and Room-Furniture.\nRoom-Texture\nRoom-Furniture\nMethod\nObject segmentation\nNovel view synthesis\nObject segmentation\nNovel view synthesis\nARI↑FG-ARI↑NV-ARI↑PSNR↑SSIM↑LPIPS↓ARI↑FG-ARI↑NV-ARI↑PSNR↑SSIM↑LPIPS↓\nuORF (Yu et al., 2022)\n0.670\n0.093\n0.578\n24.23\n0.711\n0.254\n0.686\n0.497\n0.556\n27.49\n0.780\n0.258\nBO-QSA (Jia et al., 2023)\n0.697\n0.354\n0.604\n25.26\n0.739\n0.215\n0.682\n0.479\n0.579\n27.29\n0.774\n0.261\nCOLF (Smith et al., 2023) 0.235\n0.532\n0.011\n22.98\n0.670\n0.504\n0.514\n0.458\n0.439\n28.73\n0.781\n0.386\nuOCF (ours)\n0.785\n0.563\n0.704\n28.85\n0.798\n0.136\n0.861\n0.739\n0.808\n29.77\n0.830\n0.127\nGT image\nN/A\nN/A\nGT segment.\nBO-QSA\nuORF\nCOLF\nuOCF (ours)\nInput view\nNovel view\nInput view\nNovel view\nFigure 5: Scene segmentation qualitative results. Novel view images are for reference only.\nData. We collect two synthetic datasets and two real-world datasets to evaluate our method. Examples of\nthese datasets are shown in Figure 4.\nRoom-Texture. Room-Texture features 324 object models from the “armchair” category of the ABO(Collins\net al., 2022) dataset. Each scene includes 2–4 objects arranged on backgrounds randomly selected from a\ncollection of floor textures. The dataset comprises 5,000 scenes for training and 100 for evaluation. Each\nscene is rendered from four viewpoints centered on the scene.\nRoom-Furniture. In Room-Furniture, objects are selected from 1,425 ABO (Collins et al., 2022) models\nspanning seven categories, including “bed”, “cabinet”, “chair”, “dresser”, “ottoman”, “sofa”, and “plant pot”.\nOther configurations match that of Room-Texture.\nKitchen-Matte. This dataset includes scenes featuring single-color matte dinnerware set against two types\nof backgrounds: a plain tabletop or a complex kitchen environment. The dataset contains 735 scenes for\ntraining and 102 for evaluation. Each scene includes 3–4 objects positioned randomly and is captured from\nthree viewpoints (for tabletop scenes) or two viewpoints (for kitchen backdrops).\nKitchen-Shiny. This dataset contains scenes with textured, shiny dinnerware. Similar toKitchen-Matte, the\nfirst half of the dataset features a plain tabletop, while the latter half includes a kitchen background. The\ndataset consists of 324 scenes for training and 56 for evaluation.\n7\nPublished in Transactions on Machine Learning Research (02/2025)\nTable 2: Novel view synthesis on Kitchen-Shiny and Kitchen-Matte.\nMethod\nKitchen-Shiny\nKitchen-Matte\nPSNR↑\nSSIM↑\nLPIPS↓\nPSNR↑\nSSIM↑\nLPIPS↓\nuORF (Yu et al., 2022)\n19.23\n0.602\n0.336\n26.07\n0.808\n0.092\nBO-QSA (Jia et al., 2023)\n19.78\n0.639\n0.318\n27.36\n0.832\n0.067\nCOLF (Smith et al., 2023)\n18.30\n0.561\n0.397\n20.68\n0.643\n0.236\nuOCF (ours)\n28.58\n0.862\n0.049\n29.40\n0.867\n0.043\nTable 3:\nNovel view synthesis\non Kitchen-Shiny with a larger num-\nber of object queries K.\nPSNR↑\nSSIM↑\nLPIPS↓\nK = 4\n28.58\n0.862\n0.049\nK = 5\n28.28\n0.846\n0.059\nK = 6\n28.04\n0.848\n0.058\nK = 10\n28.20\n0.840\n0.065\nInput view\nNovel view\nuORF\nGT\nuOCF (ours)\nCOLF\nBO-QSA\nInput view\nNovel view\nFigure 6: Novel view synthesis qualitative results on Kitchen-Shiny (top) and Room-Furniture (bottom).\nImplementation details. To learn object priors, we generate a synthetic dataset of over 8,000 scenes. Each\nscene contains one object, sampled from a high-quality subset of Objaverse-LVIS (Deitke et al., 2023), placed\nagainst a room background. These objects span over 100 categories. The synthetic dataset is easy to generate\nand scalable, making it ideal for learning object priors for all our experiments.\nIn the second stage, the number of foreground object queries is set to K = 4. We initialize the model with the\npre-trained weights from the object prior learning stage and train it on multi-object scenes. Once trained, our\nmodel can perform direct inference on images with spatial configurations that differ from those seen during\ntraining. Additionally, our model can adapt to unseen environments through efficient test-time optimization\n(see Sec. 4.2 for details).\nBaselines. We compare our method against uORF (Yu et al., 2022), BO-QSA (Jia et al., 2023), and\nCOLF (Smith et al., 2023). To ensure a fair comparison, we increase the latent dimensions and training\niterations for all methods. For baseline models, we retain their original implementation without incorporating\nour proposed object-centric learning stage to maintain consistency. Details on baselines enhanced with our\nobject-centric learning stage can be found in Appendix D.\nMetrics. We report the PSNR, SSIM, and LPIPS metrics for novel view synthesis. For scene segmentation,\nwe use three variants of the Adjusted Rand Index (ARI): the conventional ARI (calculated on all input image\npixels), the Foreground ARI (FG-ARI, calculated on foreground input image pixels), and the Novel View\nARI (NV-ARI, calculated on novel view pixels). All scores are computed on images of resolution 128 × 128.\n8\nPublished in Transactions on Machine Learning Research (02/2025)\nTable 4: Scene manipulation results on the Room-Texture dataset.\nMethod\nObject Translation\nObject Removal\nPSNR↑\nSSIM↑\nLPIPS↓\nPSNR↑\nSSIM↑\nLPIPS↓\nuORF (Yu et al., 2022)\n23.65\n0.654\n0.284\n23.81\n0.664\n0.282\nBO-QSA (Jia et al., 2023)\n25.21\n0.700\n0.226\n24.58\n0.698\n0.247\nuOCF (ours)\n27.66\n0.774\n0.156\n28.99\n0.802\n0.136\nObject translation\nInput image\nInput view\nuOCF (ours)\nuORF\nBO-QSA\nuORF\nuOCF (ours)\nBO-QSA\nObject removal\nNovel view\nFigure 7: Qualitative results of single-image 3D scene manipulation on the Kitchen-Shiny dataset.\n4.1\nBaseline Comparison on Multiple Tasks\nUnsupervised object segmentation in 3D. We evaluate the object discovery quality by object segmentation\nin 3D. We render a density map di for each latent i and assign each pixel p a segmentation label sp =\narg maxK\ni=0di\np in the input view and novel views. We show our results in Table 1 and examples in Figure 5.\nFrom Table 1, we see that our uOCF outperforms all existing methods in all metrics. From Figure 5, we\nobserve that no prior method can produce reasonable segmentation results in real-world Kitchen-Shiny scenes.\nSpecifically, uORF binds all objects to the background, resulting in empty object segmentation; BO-QSA fails\nto distinguish different object instances; COLF produces meaningless results on novel views. A fundamental\nissue in these methods is that they lack appropriate object priors to handle the ambiguity in disentangling\nmultiple objects. In contrast, uOCF can discover objects in real-world scenes. Moreover, uOCF can handle\nscenes where objects occlude each other. We provide more visualization results in Appendix D.\nNovel view synthesis. We evaluate the scene and object reconstruction quality by novel view synthesis.\nFor each test scene, we use a single image as input and other views as references. We show our results in\nTable 2 and examples in Figure 6. We also show additional results in Appendix D. Our method significantly\nsurpasses the baselines in all metrics. Importantly, while previous methods often fail to distinguish foreground\nobjects and thus produce blurry reconstruction of objects, our approach consistently produces high-fidelity\nscene and object reconstruction and novel view synthesis results.\nScene manipulation in 3D. We further evaluate object discovery by single-image 3D scene manipulation.\nSince uOCF explicitly infers 3D locations of discovered objects, it readily supports: 1) object translation by\nmodifying an object’s position, and 2) object removal by excluding objects during compositional rendering.\nFor quantitative evaluation, we create a test set by randomly selecting an object in each of the Room-\nTexture scenes, and shift its position (object translation) or remove it (object removal). During inference,\nwe determine the object to manipulate by selecting the object with the highest IoU score with the ground\ntruth mask. As shown in Table 4, uOCF outperforms baselines across all metrics in both object translation\nand object removal due to its better performance in object discovery. We further show qualitative examples\nfrom the Kitchen-Shiny dataset in Figure 7. We observe that uORF merges all objects into the background,\nand thus the manipulation results are identical to the original reconstruction; BO-QSA fails to distinguish\nforeground objects, resulting in blurry manipulation results (we show more visualization in Appendix D).\n9\nPublished in Transactions on Machine Learning Research (02/2025)\nInput image\nReconstruction\nObjects\nNovel view\n324 training scenes\n100 training scenes\n50 training scenes\n10 training scenes\nFigure 8: Qualitative results on sample efficiency. With fewer training scenes, our uOCF can still produce reasonable\nobject discovery thanks to the object-centric modeling and learned object priors.\nuOCF (ours)\nBO-QSA\nuORF\nGT\nuOCF (ours)\nBO-QSA\nuORF\n(a) Room-Texture à HM3D\n(b) Room-Texture à Phone capture\nGT\nN/A\nN/A\nFigure 9: Zero-shot generalization results. We load the model trained on one dataset and test it on an image\nfrom another dataset after a fast test-time optimization on the input view only. First/second/third row: scene\nreconstruction/novel view/objects.\nIn contrast, our uOCF delivers much higher-quality manipulation results. We show additional visualization\nresults in the supplementary video.\n4.2\nGeneralization Analysis\nIn the experiments above, all test scenes have unseen novel spatial configurations, where uOCF shows strong\ngeneralization. We further evaluate the sample efficiency on spatial generalization, and we showcase the\ngeneralization to unseen objects.\n10\nPublished in Transactions on Machine Learning Research (02/2025)\nInput view\nNovel view\nGT\nuOCF (ours)\nuORF\nuOCF (variant 1)\nuOCF (variant 2)\nuOCF (variant 3)\nFigure 10: Ablation on translation invariance and object prior learning. Three variants: (1) without both translation\ninvariance and object prior learning, (2) without translation invariance, (3) without object prior learning.\nSample efficiency. We train uOCF with a small subset of (e.g., only 10) the training scenes, and test it\non the test set. As shown by the qualitative example in Figure 8, even when we only have a few training\nscenes, uOCF still demonstrates a good generalization ability to discover objects. This is mainly due to the\ntranslation invariance and learned object priors, which reduce the dependence on massive training scenes.\nGeneralization to unseen objects. We evaluate the zero-shot generalization ability of uOCF by training it\non one dataset and test it on a single image of unseen background and objects. Specifically, we test our model\non five real-world examples (one from the HM3D dataset (Ramakrishnan et al., 2021) and four captured with\na phone) using a model trained solely on the synthetic Room-Texture dataset. As shown in Figures 9 and ,\nexisting methods struggle to adapt to novel objects in unseen settings. In contrast, uOCF demonstrates\nremarkable generalizability, requiring only a lightweight single-image test-time optimization for 1000 iterations.\nThis process takes approximately 3 minutes, a fraction of the 6 days needed for the full training of the\nmodel. These results highlight uOCF ’s ability to adapt effectively from synthetic training data to real-world\nscenarios. Details on this experiment can be found in Appendix C.3.\n4.3\nAblation Study\nKey technical contributions. We conduct ablation studies to analyze the impact of our key technical\ncontributions: the translation-invariant design and object prior learning.\nAs shown in Table 5 and Figure 10, incorporating translation invariance dramatically reduces the LPIPS\nmetric from 0.186 to 0.049. Similarly, leveraging object prior learning significantly decreases LPIPS from\n0.125 to 0.049. These results demonstrate that both contributions are not only individually impactful but\nalso complementary. Removing either one severely degrades performance, justifying their importance in\nachieving high-quality results.\nOther technical improvements. We also evaluate the impact of other technical improvements through\nablation studies. As shown in Table 6, the inclusion of DINO ViT and standard attention enhances overall\nperformance. However, these components contribute relatively modestly; for instance, removing DINO or\nstandard attention slightly increases LPIPS from 0.049 to 0.060 or 0.062, respectively. Excluding depth and\nocclusion losses also degrades visual quality, leading to a noticeable drop in performance. Additionally, remov-\ning the object-centric sampling strategy slightly reduces overall reconstruction quality, further highlighting\nthe significance of these improvements.\nDifferent K values. We evaluate the effectiveness of different K values and different scales of data used for\n3D object prior learning. We evaluate our method’s robustness to different K values, and we show results in\nTable 3 and Figure 11. From Table 3, we can see that even when we set K = 10 which is much higher than\nthe number of possible maximal objects (i.e., 4), our model is robust and gives comparable results. From\nFigure 11, we observe that even if there are more object queries than the number of objects in the scene,\nuOCF learns to generate “empty” object queries instead of over-segmenting the objects.\n11\nPublished in Transactions on Machine Learning Research (02/2025)\nTable 5: Ablation studies on our key technical contributions\non Kitchen-Shiny.\nMethod\nPSNR ↑\nSSIM ↑\nLPIPS ↓\nw/o trans. invar. or object prior learning\n20.68\n0.645\n0.303\nw/o trans. invar.\n23.70\n0.724\n0.186\nw/o object prior learning\n26.81\n0.806\n0.125\nuOCF (ours)\n28.58\n0.862\n0.049\nTable 6: Ablation study on model architecture\nand loss function on Kitchen-Shiny.\nMethod\nPSNR↑\nSSIM↑\nLPIPS↓\nw/o DINO\n26.25\n0.831\n0.060\nw/o standard attention\n27.82\n0.844\n0.062\nw/o object-centric sampling\n27.31\n0.852\n0.072\nw/o ℓdepth and ℓocc\n26.79\n0.819\n0.081\nuOCF (ours)\n28.58\n0.862\n0.049\nInput image\nK=10\nObject 5\nObject 6+\nReconstruction\nObject 3\nObject 4\nObject 1\nObject 2\nN/A\nK=4\nN/A\nFigure 11: Qualitative results of uOCF on scenes with larger object queries K. The order of the object reconstructions\nis rearranged for better visualization.\n5\nConclusion\nWe study the importance of translation invariance for unsupervised 3D object discovery, instantiated as\nour model for the unsupervised discovery of Object-Centric neural Fields (uOCF). Our results show that\nour translation-invariant design and the 3D object prior learning can substantially improve the spatial\ngeneralization and sample efficiency. Our results demonstrate that unsupervised 3D object discovery can be\nextended to real scenes while obtaining satisfactory performances.\nLimitations. Although uOCF shows promising unsupervised 3D object discovery results, it is currently\nlimited to simple real scenes such as the kitchen scenes. Extending to more complex real scenes with complex\nspatial layouts and a large number of objects from different categories is an important future direction. We\nleave more discussion on technical limitations in Appendix E.\nAcknowledgments\nThis work is in part supported by NSF RI #2211258 and #2338203, ONR MURI N00014-22-1-2740, and\nONR YIP N00014-24-1-2117.\nReferences\nJonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, and Pratul P\nSrinivasan. Mip-nerf: A multiscale representation for anti-aliasing neural radiance fields. In IEEE/CVF\nInternational Conference on Computer Vision (ICCV), 2021.\nOndrej Biza, Sjoerd van Steenkiste, Mehdi SM Sajjadi, Gamaleldin F Elsayed, Aravindh Mahendran, and\nThomas Kipf. Invariant slot attention: Object discovery with slot-centric reference frames. In International\nConference on Machine Learning (ICML), 2023.\nChristopher P Burgess, Loic Matthey, Nicholas Watters, Rishabh Kabra, Irina Higgins, Matt Botvinick, and\nAlexander Lerchner. Monet: Unsupervised scene decomposition and representation. arXiv:1901.11390,\n2019.\nJiazhong Cen, Jiemin Fang, Chen Yang, Lingxi Xie, Xiaopeng Zhang, Wei Shen, and Qi Tian. Segment any\n3d gaussians. arXiv:2312.00860, 2023a.\n12\nPublished in Transactions on Machine Learning Research (02/2025)\nJiazhong Cen, Zanwei Zhou, Jiemin Fang, Wei Shen, Lingxi Xie, Xiaopeng Zhang, and Qi Tian. Segment\nanything in 3d with nerfs. In Advances in Neural Information Processing Systems (NeurIPS), 2023b.\nChang Chen, Fei Deng, and Sungjin Ahn. Learning to infer 3d object models from images. arXiv:2006.06130,\n2020.\nHonglin Chen, Wanhee Lee, Hong-Xing Yu, Rahul Mysore Venkatesh, Joshua B Tenenbaum, Daniel Bear,\nJiajun Wu, and Daniel LK Yamins. Unsupervised 3d scene representation learning via movable object\ninference. Transactions on Machine Learning Research (TMLR), 2024.\nJasmine Collins, Shubham Goel, Kenan Deng, Achleshwar Luthra, Leon Xu, Erhan Gundogdu, Xi Zhang,\nTomas F Yago Vicente, Thomas Dideriksen, Himanshu Arora, Matthieu Guillaumin, and Jitendra Malik.\nAbo: Dataset and benchmarks for real-world 3d object understanding. In IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), 2022.\nEric Crawford and Joelle Pineau. Spatially invariant unsupervised object detection with convolutional neural\nnetworks. In AAAI Conference on Artificial Intelligence (AAAI), 2019.\nTal Daniel and Aviv Tamar. Unsupervised image representation learning with deep latent particles. In\nInternational Conference on Machine Learning (ICML), 2022.\nTal Daniel and Aviv Tamar. Ddlp: Unsupervised object-centric video prediction with deep dynamic latent\nparticles. Transactions on Machine Learning Research (TMLR), 2023.\nMatt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt,\nKiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: A universe of annotated 3d objects. In\nIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023.\nAniket Didolkar, Anirudh Goyal, and Yoshua Bengio.\nCycle consistency driven object discovery.\narXiv:2306.02204, 2023.\nMartin Engelcke, Adam R Kosiorek, Oiwi Parker Jones, and Ingmar Posner. Genesis: Generative scene\ninference and sampling with object-centric latent representations. arXiv:1907.13052, 2019.\nSM Eslami, Nicolas Heess, Theophane Weber, Yuval Tassa, David Szepesvari, Koray Kavukcuoglu, and\nGeoffrey E Hinton. Attend, infer, repeat: Fast scene understanding with generative models. In Advances\nin Neural Information Processing Systems (NeurIPS), 2016.\nSM Ali Eslami, Danilo Jimenez Rezende, Frederic Besse, Fabio Viola, Ari S Morcos, Marta Garnelo, Avraham\nRuderman, Andrei A Rusu, Ivo Danihelka, Karol Gregor, et al. Neural scene representation and rendering.\nScience, 2018.\nZhiwen Fan, Peihao Wang, Yifan Jiang, Xinyu Gong, Dejia Xu, and Zhangyang Wang. Nerf-sos: Any-view\nself-supervised object segmentation on complex scenes. arXiv:2209.08776, 2022.\nAnand Gopalakrishnan, Aleksandar Stanić, Jürgen Schmidhuber, and Michael Curtis Mozer. Recurrent\ncomplex-weighted autoencoders for unsupervised object discovery. In Advances in Neural Information\nProcessing Systems (NeurIPS), 2024.\nKristen Grauman and Trevor Darrell. Unsupervised learning of categories from sets of partially matching\nimage features. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2006.\nKlaus Greff, Antti Rasmus, Mathias Berglund, Tele Hotloo Hao, Jürgen Schmidhuber, and Harri Valpola.\nTagger: Deep unsupervised perceptual grouping. In Advances in Neural Information Processing Systems\n(NeurIPS), 2016.\nKlaus Greff, Sjoerd Van Steenkiste, and Jürgen Schmidhuber. Neural expectation maximization. In Advances\nin Neural Information Processing Systems (NeurIPS), 2017.\n13\nPublished in Transactions on Machine Learning Research (02/2025)\nKlaus Greff, Raphaël Lopez Kaufman, Rishabh Kabra, Nick Watters, Christopher Burgess, Daniel Zoran,\nLoic Matthey, Matthew Botvinick, and Alexander Lerchner. Multi-object representation learning with\niterative variational inference. In International Conference on Machine Learning (ICML), 2019.\nPaul Henderson and Christoph H Lampert. Unsupervised object-centric video generation and decomposition\nin 3d. In Advances in Neural Information Processing Systems (NeurIPS), 2020.\nYicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui,\nand Hao Tan. Lrm: Large reconstruction model for single image to 3d. arXiv preprint arXiv:2311.04400,\n2023.\nBaoxiong Jia, Yu Liu, and Siyuan Huang. Improving object-centric learning with query optimization. In\nInternational Conference on Learning Representations (ICLR), 2023.\nJindong Jiang, Sepehr Janghorbani, Gerard de Melo, and Sungjin Ahn. Scalor: Generative world models with\nscalable object representations. In International Conference on Learning Representations (ICLR), 2020.\nJustin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-\nresolution. In European Conference on Computer Vision (ECCV), 2016.\nArmand Joulin, Francis Bach, and Jean Ponce. Discriminative clustering for image co-segmentation. In\nIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2010.\nAdam R Kosiorek, Hyunjik Kim, Ingmar Posner, and Yee Whye Teh. Sequential attend, infer, repeat:\nGenerative modelling of moving objects. In Advances in Neural Information Processing Systems (NeurIPS),\n2018.\nAbhijit Kundu, Yin Li, and James M Rehg. 3d-rcnn: Instance-level 3d object reconstruction via render-and-\ncompare. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2018.\nBo Li, Zhengxing Sun, Qian Li, Yunjie Wu, and Anqi Hu. Group-wise deep object co-segmentation with co-\nattention recurrent neural network. In IEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), 2019.\nShengnan Liang, Yichen Liu, Shangzhe Wu, Yu-Wing Tai, and Chi-Keung Tang. Onerf: Unsupervised 3d\nobject segmentation from multiple views. arXiv:2211.12038, 2022.\nZhixuan Lin, Yi-Fu Wu, Skand Vishwanath Peri, Weihao Sun, Gautam Singh, Fei Deng, Jindong Jiang,\nand Sungjin Ahn. Space: Unsupervised object-oriented scene representation via spatial attention and\ndecomposition. In International Conference on Learning Representations (ICLR), 2020.\nFrancesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold, Jakob\nUszkoreit, Alexey Dosovitskiy, and Thomas Kipf. Object-centric learning with slot attention. In Advances\nin Neural Information Processing Systems (NeurIPS), 2020.\nSindy Löwe, Phillip Lippe, Maja Rudolph, and Max Welling. Complex-valued autoencoders for object\ndiscovery. Transactions on Machine Learning Research (TMLR), 2022.\nBen Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren\nNg. Nerf: Representing scenes as neural radiance fields for view synthesis. In European Conference on\nComputer Vision (ECCV), 2020.\nTom Monnier, Elliot Vincent, Jean Ponce, and Mathieu Aubry. Unsupervised layered image decomposition\ninto object prototypes. In IEEE/CVF International Conference on Computer Vision (ICCV), 2021.\nNorman Müller, Andrea Simonelli, Lorenzo Porzi, Samuel Rota Bulo, Matthias Nießner, and Peter\nKontschieder. Autorf: Learning 3d object radiance fields from single view observations. In IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR), 2022.\n14\nPublished in Transactions on Machine Learning Research (02/2025)\nThu H Nguyen-Phuoc, Christian Richardt, Long Mai, Yongliang Yang, and Niloy Mitra. Blockgan: Learning\n3d object-aware scene representations from unlabelled images. In Advances in Neural Information Processing\nSystems (NeurIPS), 2020.\nMichael Niemeyer and Andreas Geiger. Giraffe: Representing scenes as compositional generative neural\nfeature fields. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021.\nMaxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre\nFernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual\nfeatures without supervision. arXiv:2304.07193, 2023.\nJulian Ost, Fahim Mannan, Nils Thuerey, Julian Knodt, and Felix Heide. Neural scene graphs for dynamic\nscenes. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021.\nJeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. Deepsdf: Learning\ncontinuous signed distance functions for shape representation. In IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), 2019.\nSanthosh K Ramakrishnan, Aaron Gokaslan, Erik Wijmans, Oleksandr Maksymets, Alex Clegg, John Turner,\nEric Undersander, Wojciech Galuba, Andrew Westbury, Angel X Chang, et al. Habitat-matterport 3d\ndataset (hm3d): 1000 large-scale 3d environments for embodied ai. arXiv:2109.08238, 2021.\nRené Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun.\nTowards robust\nmonocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. Transactions on Pattern\nAnalysis and Machine Intelligence (TPAMI), 2022.\nBryan C Russell, William T Freeman, Alexei A Efros, Josef Sivic, and Andrew Zisserman. Using multiple\nsegmentations to discover objects and their extent in image collections. In IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), 2006.\nMehdi SM Sajjadi, Daniel Duckworth, Aravindh Mahendran, Sjoerd van Steenkiste, Filip Pavetic, Mario\nLucic, Leonidas J Guibas, Klaus Greff, and Thomas Kipf. Object scene representation transformer. In\nAdvances in Neural Information Processing Systems (NeurIPS), 2022.\nMaximilian Seitzer, Max Horn, Andrii Zadaianchuk, Dominik Zietlow, Tianjun Xiao, Carl-Johann Simon-\nGabriel, Tong He, Zheng Zhang, Bernhard Schölkopf, Thomas Brox, et al. Bridging the gap to real-world\nobject-centric learning. In International Conference on Learning Representations (ICLR), 2023.\nVincent Sitzmann, Michael Zollhöfer, and Gordon Wetzstein. Scene representation networks: Continuous\n3d-structure-aware neural scene representations. In Advances in Neural Information Processing Systems\n(NeurIPS), 2019.\nVincent Sitzmann, Semon Rezchikov, Bill Freeman, Josh Tenenbaum, and Fredo Durand. Light field networks:\nNeural scene representations with single-evaluation rendering. In Advances in Neural Information Processing\nSystems (NeurIPS), 2021.\nJosef Sivic, Bryan C Russell, Alexei A Efros, Andrew Zisserman, and William T Freeman. Discovering objects\nand their location in images. In IEEE International Conference on Computer Vision (ICCV), 2005.\nCameron Smith, Hong-Xing Yu, Sergey Zakharov, Fredo Durand, Joshua B Tenenbaum, Jiajun Wu, and\nVincent Sitzmann. Unsupervised discovery and composition of object light fields. Transactions on Machine\nLearning Research (TMLR), 2023.\nKarl Stelzner, Kristian Kersting, and Adam R Kosiorek. Decomposing 3d scenes into objects via unsupervised\nvolume segmentation. arXiv:2104.01148, 2021.\nAyush Tewari, Ohad Fried, Justus Thies, Vincent Sitzmann, Stephen Lombardi, Kalyan Sunkavalli, Ricardo\nMartin-Brualla, Tomas Simon, Jason Saragih, Matthias Nießner, et al. State of the art on neural rendering.\nComputer Graphics Forum, 2020.\n15\nPublished in Transactions on Machine Learning Research (02/2025)\nJosé-Fabian Villa-Vásquez and Marco Pedersoli. Unsupervised object discovery: A comprehensive survey and\nunified taxonomy. arXiv preprint arXiv:2411.00868, 2024.\nHuy V Vo, Patrick Pérez, and Jean Ponce. Toward unsupervised, multi-object discovery in large-scale image\ncollections. In European Conference on Computer Vision (ECCV), 2020.\nGuangcong Wang, Zhaoxi Chen, Chen Change Loy, and Ziwei Liu. Sparsenerf: Distilling depth ranking\nfor few-shot novel view synthesis. In IEEE/CVF International Conference on Computer Vision (ICCV),\n2023a.\nHe Wang, Srinath Sridhar, Jingwei Huang, Julien Valentin, Shuran Song, and Leonidas J Guibas. Normalized\nobject coordinate space for category-level 6d object pose and size estimation. In IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), 2019.\nQian Wang, Yiqun Wang, Michael Birsak, and Peter Wonka. Blobgan-3d: A spatially-disentangled 3d-aware\ngenerative model for indoor scenes. arXiv:2303.14706, 2023b.\nTianyu Wang, Miaomiao Liu, and Kee Siong Ng. Spatially invariant unsupervised 3d object-centric learning\nand scene decomposition. In European Conference on Computer Vision (ECCV), 2022.\nJiajun Wu, Joshua B Tenenbaum, and Pushmeet Kohli. Neural scene de-rendering. In IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), 2017.\nQianyi Wu, Xian Liu, Yuedong Chen, Kejie Li, Chuanxia Zheng, Jianfei Cai, and Jianmin Zheng. Object-\ncompositional neural implicit surfaces. In European Conference on Computer Vision (ECCV), 2022.\nYiheng Xie, Towaki Takikawa, Shunsuke Saito, Or Litany, Shiqin Yan, Numair Khan, Federico Tombari,\nJames Tompkin, Vincent Sitzmann, and Srinath Sridhar. Neural fields in visual computing and beyond.\nComputer Graphics Forum, 2022.\nBangbang Yang, Yinda Zhang, Yinghao Xu, Yijin Li, Han Zhou, Hujun Bao, Guofeng Zhang, and Zhaopeng\nCui. Learning object-compositional neural radiance field for editable scene rendering. In IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR), 2021.\nJiawei Yang, Marco Pavone, and Yue Wang. Freenerf: Improving few-shot neural rendering with free frequency\nregularization. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023.\nShunyu Yao, Tzu Ming Harry Hsu, Jun-Yan Zhu, Jiajun Wu, Antonio Torralba, William T Freeman, and\nJoshua B Tenenbaum. 3d-aware scene manipulation via inverse graphics. In Advances in Neural Information\nProcessing Systems (NeurIPS), 2018.\nAlex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelnerf: Neural radiance fields from one or\nfew images. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021.\nHong-Xing Yu, Leonidas J Guibas, and Jiajun Wu. Unsupervised discovery of object radiance fields. In\nInternational Conference on Learning Representations (ICLR), 2022.\n16\nPublished in Transactions on Machine Learning Research (02/2025)\nA\nAppendix Overview\nThis supplementary document is structured as follows: We begin with the proof of concept in Appendix B\nand provide the implementation details in Appendix C. Then, we discuss the limitations of our approach in\nAppendix E and present additional qualitative results in Appendix D. Accompanying this document is our\nproject page with an overview video attached in the supplementary file.\nB\nProof of Concept\nWe conduct a toy experiment (Figure 12) to demonstrate that our model has successfully learned object\nposition, rotation, and scale. In this experiment, we begin with two images (input 1 and input 2) of a chair\nplaced at the scene’s center, exhibiting different sizes (on the left) or rotation angles (on the right), all\ncaptured from the same viewing direction.\nWe extract the object latents from these images, interpolate them, and then send the interpolated latents to\nthe decoder. As shown between the two input images, we observe a smooth transition in object size and\nrotation, indicating that the latent representation has effectively captured the scale and rotation of objects.\nIn the second row, we placed the chairs in different positions. As shown on the right, we obtained a smooth\ntransition again, proving that our model could disentangle object positions from the latent representation.\nInput 1\nInput 2\nLatent interpolation \nsize: big →small\nrotation:  0 →𝜋/2\nInput 1\nRotation: 𝜋/2\nPosition≠(0, 0)\nInput 2\nRotation: 0\nPosition≠(0, 0)\nLatent linear interpolation & position translation\nInput 1\nInput 2\nLatent interpolation \nFigure 12: Proof of concept. We demonstrate that uOCF has effectively learned objects’ scale and orientation along\nwith the translation-invariant object representation by interpolating the representation of two identical objects with\ndifferent orientations and scales to obtain transitional results.\nC\nImplementation\nC.1\nModel Architecture\nEncoder. Our encoder module consists of a frozen DINO encoder and two convolutional layers. We illustrate\nits architecture in Figure 13(a).\nLatent inference module. While motivated by the background-aware slot attention module proposed\nby (Yu et al., 2022), our latent inference module exhibits three key differences: (1) The object queries are\ninitialized with learnable embeddings instead of being sampled from learnable Gaussians, which enhances\ntraining stability; (2) We jointly extract object positions and their latent representations and add object-\nspecific positional encoding to utilize the extracted position information; (3) We remove the Gated Recurrent\nUnit (GRU) and replace it with the transformer architecture to smooth the gradient flow.\nC.2\nData Collection\nThis section introduces the details of our datasets.\n17\nPublished in Transactions on Machine Learning Research (02/2025)\nResize to 876*876\n(Frozen) DINOv2 ViT\n(876*876*3à64*64*768)\n3*3 conv, stride=1, ReLU\n(64*64*768à64*64*64)\nI\nFeature map 𝑓\n(a) Encoder architecture\n3*3 conv, stride=1\n(64*64*768à64*64*64)\nDropped samples\nRemaining samples\nObject bounding box\n(b) Object-centric sampling\nFigure 13: (a) Architecture of our encoder module. (b) Object-centric sampling: We drop the samples distant from\nthe predicted object position for efficient sampling..\nRoom-Texture. In Room-Texture, objects are chosen from 324 ABO objects (Collins et al., 2022) from the\n“armchair” category. The single-object subset contains four scenes for each object instance, resulting in 1296\nscenes in total. The multiple-object subset includes 5, 000 scenes for training and 100 for evaluation, with\neach scene containing 2-4 objects set against a background randomly chosen from a collection of floor textures.\nEach scene is rendered from 4 directions toward the center.\nRoom-Furniture. In Room-Furniture, objects are chosen from 1, 425 ABO (Collins et al., 2022) object models,\nspanning across seven categories, including “bed”, “cabinet”, “chair”, “dresser”, “ottoman”, “sofa”, and “plant\npot”. Each scene contains 2-4 objects set against a background randomly chosen from a collection of floor\ntextures. We render 5000 scenes for training and 100 scenes for evaluation.\nKitchen-Matte. In Kitchen-Matte, objects are diffuse and have no texture. The dataset comprises 16 objects\nand 6 tablecloths in total. We captured 3 images for each tabletop scene and 2 for each kitchen scene. This\ndataset contains 735 scenes for training and 102 for evaluation, each containing 3-4 objects. We calibrate the\ncameras using the OpenCV library.\nKitchen-Shiny. In Kitchen-Shiny, objects are specular, and the lighting is more complex.\nThe dataset\ncomprises 12 objects and 6 tablecloths, and the other settings are identical to Kitchen-Matte. This dataset\ncontains 324 scenes for training and 56 for evaluation, each containing 4 objects.\nC.3\nTraining Configuration\nThis section discusses the training configuration of uOCF.\nWe employ Mip-NeRF (Barron et al., 2021) as our NeRF backbone and estimate the depth maps by\nMiDaS (Ranftl et al., 2022). An Adam optimizer with default hyper-parameters and an exponential decay\nscheduler is used across all experiments. The initial learning rate is 0.0003 for the first stage and 0.00015 for\nthe second stage. Loss weights are set to λperc = 0.006, λdepth = 1.5, and λocc = 0.1. The position update\nmomentum m is set to 0.5, and the latent inference module lasts T = 6 iterations. Most hyperparameters\nare inherited from Yu et al. (2022), while the loss weights of our proposed losses are chosen to ensure a\nsimilar scale between all loss terms. All experiments are run on a single RTX-A6000 GPU. Training lasts\napproximately 6 days, including 1.5 days for object prior learning and another 4.5 days for training on\nmulti-object scenes.\nCoarse-To-Fine progressive training. We employ a coarse-to-fine strategy in our second training stage\nto facilitate training at higher resolutions. Reference images are downsampled to a lower resolution (64 × 64)\n18\nPublished in Transactions on Machine Learning Research (02/2025)\nduring the coarse training stage and replaced by image patches with the same size as the low-resolution\nimages randomly cropped from the high-resolution (128 × 128) input images during the fine training stage.\nLocality constraint and object-centric sampling. We employ the locality constraint (a bounding box\nfor foreground objects in the world coordinate) proposed by (Yu et al., 2022) in both training stages but\nonly adopt it before starting object-centric sampling. The number of samples along each ray before and\nafter starting object-centric sampling is set to 64 and 256, respectively. We provide an illustration of our\nobject-centric sampling strategy in Figure 13(b).\nTraining configuration on Room-Texture. During stage 1, we train the model for 100 epochs directly\non images of resolution 128 × 128. We start with the reconstruction loss only, add the perceptual 10th epoch,\nand start the object-centric sampling at the 20th epoch. During stage 2, we train the model for 60 epochs\non the coarse stage and another 60 on the fine stage. We start with the reconstruction loss only, add the\nperceptual loss at the 10th epoch, and start the object-centric sampling from the 20th epoch.\nTraining configuration on Kitchen-Matte and Kitchen-Shiny. Both kitchen datasets share the same\ntraining configuration with Room-Texture in stage 1. During stage 2, we train the model for 750 epochs,\nwhere the fine stage starts at the 250th epoch. We add the perceptual loss at the 50th epoch and start the\nobject-centric sampling from the 150th epoch.\nTraining configuration for zero-shot generalization. For the test-time adaptation, we fine-tune our\nmodel on the input view only using our proposed loss function (Eq. 7) for 1000 iterations on resolution\n128 × 128. We use the Adam optimizer and the learning rate is set to 1 × 10−4. This optimization takes\nabout 3 minutes on a single A6000 gpu.\nD\nAdditional Experiments\nAdditional real-world dataset. We introduce an additional real-world dataset, named “Planters,” which\nfeatures tabletop scenes containing four plant pots or vases arranged on tablecloths. The dataset includes\n745 scenes for training and 140 scenes for evaluation, with each scene captured from three different camera\nposes. As shown in the quantitative results in Table 7 and the qualitative results in Figure 17, our method\nsignificantly outperforms existing approaches. It achieves superior scene reconstruction and novel view\nsynthesis, delivering results with noticeably higher visual quality.\nMethod\nPSNR↑\nSSIM↑\nLPIPS↓\nuORF\n24.49\n0.748\n0.163\nuORF-BO-QSA\n28.09\n0.847\n0.108\nCOLF\n19.22\n0.588\n0.464\nuOCF (ours)\n29.00\n0.864\n0.062\nTable 7: Quantitative results on the Planters dataset.\nBaseline performance with object-centric learning. To ensure fair comparisons, we conducted additional\nexperiments where object prior learning was incorporated into the baseline methods. The results in Table 8\ndemonstrate that even with the incorporation of object prior learning, uOCF significantly outperforms\nexisting methods due to its translation-invariant object representation, which enhances generalization and\ndata efficiency.\nAdditional zero-shot evaluation. We evaluate our method on two additional phone-captured scenes to\nfurther demonstrate the generalizability of uOCF. To ensure fair comparisons, we incorporate object prior\nlearning into the baseline methods. However, as shown in Figure 15, the baseline methods still struggle\ngeneralizing to unseen environments. In contrast, our method produces accurate object segmentation and\nnovel view synthesis results, further validating its effectiveness.\nVisualization on discovered objects. We visualize the discovered objects in Figure 16. Notably, uORF (Yu\net al., 2022) puts all objects within the background, whereas BO-QSA (Jia et al., 2023) binds the same object\nto all queries, resulting in identical foreground reconstruction. In contrast, uOCF accurately differentiates\nbetween the foreground objects and the background.\n19\nPublished in Transactions on Machine Learning Research (02/2025)\nMethod\nLPIPS ↓\nSSIM ↑\nPSNR ↑\nuORF\n0.336\n0.602\n19.23\nuORF + object prior learning\n0.193\n0.714\n22.78\nBO-QSA\n0.318\n0.639\n19.78\nBO-QSA + object prior learning\n0.129\n0.766\n24.00\nCOLF\n0.397\n0.561\n18.30\nCOLF + object prior learning\n0.290\n0.709\n21.66\nuOCF (ours)\n0.049\n0.862\n28.58\nTable 8: Baseline performance with object-centric learning. Our method maintains its superiority even when baselines\nmethods employ our proposed object-prior learning approach due to its translation-invariance representation.\nVisualization on object segmentation in 3D. We show scene segmentation results on the kitchen datasets\nin Figure 18. Unlike compared methods that yield cluttered results, uOCF consistently yields high-fidelity\nsegmentation results.\nAdditional novel view synthesis results. We show more qualitative results for novel view synthesis in\nFigures 18, 19, and 20. Our method produces much better results than compared methods regarding visual\nquality.\nE\nLimitations Analysis\nLimitation on reconstruction quality. Scene-level generalizable NeRFs (Yu et al., 2021; Sajjadi et al.,\n2022; Yu et al., 2022) commonly face challenges in accurately reconstructing detailed object textures. Our\napproach also has difficulty capturing extremely high-frequency details. As shown in Figure 14(a), our method\nfails to replicate the mug’s detailed texture. Future research may benefit from stronger object priors learned\nfrom larger-scale datasets, such as Large Reconstruction Models (Hong et al., 2023).\nFailure in position prediction. Our two-stage training pipeline, despite its robustness in many situations,\nis not immune to errors, particularly in object position prediction. Due to the occlusion between objects, using\nthe attention-weighted mean for determining object positions can sometimes lead to inaccuracies. Although\na bias term can rectify this in most instances (Figure 6), discrepancies persist under a few conditions, as\ndepicted in Figure 14(b).\nTraining instability. Like other slot-based object discovery methods, our approach also faces challenges\nwith training instability. For example, the model occasionally collapses within the first few training epochs,\neven when using identical hyperparameters. To address this, we perform multiple trials for each experiment,\nterminating early if signs of collapse appear during the initial training stages. We observed that approximately\nhalf of the experiments fail at this stage. However, once the model progresses beyond this critical phase, the\nfinal results remain consistent across different trials. For baseline methods that struggle on our proposed\ncomplex datasets, we conduct at least five trials to ensure that their failure stems from limitations in their\ndesign rather than issues with random initialization.\n20\nPublished in Transactions on Machine Learning Research (02/2025)\nInput view\n(a) Failure on reconstruct texture\n(b) Error in position prediction\nGT\nReconstruction\nGT\nReconstruction\nNovel view\nFigure 14: Failure case visualizations. Our method may fail to reconstruct intricate object texture or predict biased\nobject position.\nuOCF (ours)\nuORF\nBO-QSA\nuOCF (ours)\nuORF\nBO-QSA\nInput image\nInput image\nReconstruction Segmentation\nObject 1\nObject 2\nObject 3\nObject 4\nNovel view\nFigure 15: Qualitative zero-shot generalization results.\n21\nPublished in Transactions on Machine Learning Research (02/2025)\nInput image\nInput image\nuORF\nBO-QSA\nBackground\nObject 1\nuOCF (ours)\nObject 3\nObject 2\nObject 4\nuORF\nBO-QSA\nuOCF (ours)\nFigure 16: Visualization on discovered objects on Kitchen-Shiny.\n22\nPublished in Transactions on Machine Learning Research (02/2025)\nGT\nuOCF (ours)\nuORF\nCOLF\nBO-QSA\nInput view\nNovel  view\nInput view\nNovel  view\nInput view\nNovel  view\nInput view\nNovel  view\nFigure 17: Qualitative comparison results on the Planters dataset.\n23\nPublished in Transactions on Machine Learning Research (02/2025)\nGT\nuORF\nCOLF\nBO-QSA\nuOCF (ours)\nInput view\nNovel  view\nInput view\nNovel  view\nInput view\nNovel  view\nInput view\nNovel  view\nFigure 18: Additional segmentation and view synthesis results on the Room-Texture dataset.\n24\nPublished in Transactions on Machine Learning Research (02/2025)\nGT\nuORF\nCOLF\nBO-QSA\nuOCF (ours)\nInput view\nNovel  view\nInput view\nNovel  view\nInput view\nNovel  view\nInput view\nNovel  view\nFigure 19: Additional view synthesis results on the Kitchen-Matte dataset.\n25\nPublished in Transactions on Machine Learning Research (02/2025)\nInput view\nNovel  view\nGT\nuORF\nBO-QSA\nCOLF\nuOCF (ours)\nInput view\nNovel  view\nInput view\nNovel  view\nInput view\nNovel  view\nFigure 20: Additional view synthesis results on the Kitchen-Shiny dataset.\n26\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2024-02-12",
  "updated": "2025-02-14"
}