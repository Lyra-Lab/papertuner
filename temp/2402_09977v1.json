{
  "id": "http://arxiv.org/abs/2402.09977v1",
  "title": "Fast Vocabulary Transfer for Language Model Compression",
  "authors": [
    "Leonidas Gee",
    "Andrea Zugarini",
    "Leonardo Rigutini",
    "Paolo Torroni"
  ],
  "abstract": "Real-world business applications require a trade-off between language model\nperformance and size. We propose a new method for model compression that relies\non vocabulary transfer. We evaluate the method on various vertical domains and\ndownstream tasks. Our results indicate that vocabulary transfer can be\neffectively used in combination with other compression techniques, yielding a\nsignificant reduction in model size and inference time while marginally\ncompromising on performance.",
  "text": "Fast Vocabulary Transfer for Language Model Compression\nLeonidas Gee\nExpert.ai, Italy\nlgee@expert.ai\nAndrea Zugarini\nExpert.ai, Italy\nazugarini@expert.ai\nLeonardo Rigutini\nExpert.ai, Italy\nand\nUniversity of Siena\nlrigutini@expert.ai\nPaolo Torroni\nDepartment of Computer Science and\nEngineering,\nUniversity of Bologna, Italy\npaolo.torroni@unibo.it\nAbstract\nReal-world business applications require a\ntrade-off between language model performance\nand size. We propose a new method for model\ncompression that relies on vocabulary transfer.\nWe evaluate the method on various vertical do-\nmains and downstream tasks. Our results indi-\ncate that vocabulary transfer can be effectively\nused in combination with other compression\ntechniques, yielding a significant reduction in\nmodel size and inference time while marginally\ncompromising on performance.\n1\nIntroduction\nIn the last few years, many NLP applications have\nbeen relying more and more on large pre-trained\nLanguage Models (LM) (Devlin et al., 2018; Liu\net al., 2019; He et al., 2020). Because larger LMs,\non average, exhibit higher accuracy, a common\ntrend has been to increase the model’s size. Some\nLMs like GPT-3 (Brown et al., 2020) and BLOOM1\nhave reached hundreds of billion parameters. How-\never, these models’ superior performance comes\nat the cost of a steep increase in computational\nfootprint, both for development and for inference,\nultimately hampering their adoption in real-world\nbusiness use-cases. Besides models that only a few\nhi-tech giants can afford, like GPT-3, even smaller\nLMs with hundreds of million parameters could\nbe too expensive or infeasible for certain products.\nFor one thing, despite being tremendously cheaper\nthan their bigger cousins, fine-tuning, deploying\nand maintaining large numbers of such models (one\nfor each downstream task) soon becomes too ex-\npensive. Furthermore, latency and/or hardware re-\nquirements may limit their applicability to specific\n1https://bigscience.huggingface.co/blog/bloom\nuse-cases. For all these reasons, significant efforts\n– in both academic and industry-driven research –\nare oriented towards the designing of solutions to\ndrastically reduce the costs of LMs.\nRecently, several attempts have been made to\nmake these models smaller, faster and cheaper,\nwhile retaining most of their original performance\n(Gupta et al., 2015; Shen et al., 2020).\nNo-\ntably, Knowledge Distillation (KD) (Hinton et al.,\n2015) is a teacher-student framework, whereby the\nteacher consists of a pre-trained large model and\nthe student of a smaller one. The teacher-student\nframework requires that both the teacher and the\nstudent estimate the same probability distribution.\nWhile the outcome is a smaller model, yet, this\nprocedure constrains the student to operate with\nthe same vocabulary as the teacher in the context\nof Language Modeling.\nIn this work, we explore a method for further\nreducing an LM’s size by compressing its vocab-\nulary through the training of a tokenizer in the\ndownstream task domain. The tokenizer (Sennrich\net al., 2016; Schuster and Nakajima, 2012; Kudo\nand Richardson, 2018) is a crucial part of modern\nLMs. In particular, moving from word to subword-\nlevel, the tokenization solves two problems: vocab-\nulary explosion and unknown words. Moreover,\nthe capability to tokenize text effectively in any do-\nmain is key for the massive adoption of pre-trained\ngeneral-purpose LMs fine-tuned on downstream\ntasks. Indeed, tokenizers are still able to process\nout-of-distribution texts at the cost of producing\nfrequent word splits into multiple tokens.\nHowever, the language varies significantly in\nvertical domains or, more generally, in different\ntopics. Hence, ad-hoc tokenizers, trained on the\ndomain statistics, may perform a more efficient to-\narXiv:2402.09977v1  [cs.CL]  15 Feb 2024\nFigure 1: Sketch of the VT procedure. First, the vocabulary is constructed on the in-domain data, then an embedding\nis assigned to each token, transferring information from the pre-trained representations of the general-purpose\nlanguage model.\nkenization, reducing on average the length of the\ntokenized sequences. This is important since com-\npact and meaningful inputs could reduce computa-\ntional costs, while improving performance. Indeed,\nmemory and time complexity of attention layers\ngrows quadratically with respect to the sequence\nlength (Vaswani et al., 2017). Furthermore, a ver-\ntical tokenizer may require a smaller vocabulary,\nwhich also affects the size of the embedding matrix,\nhence further reducing the model’s size.\nFollowing this intuition, we propose a Vocab-\nulary Transfer (VT) technique to adapt LMs to\nin-domain, smaller tokenizers, in order to further\ncompress and accelerate them. This technique is\ncomplementary to the aforementioned model com-\npression methods and independent of the type of\ntokenizer. As a matter of fact, we apply it in com-\nbination with KD.\nOur experiments show that VT achieves an infer-\nence speed-up between x1.07 and x1.40, depend-\ning on the downstream task, with a limited perfor-\nmance drop, and that a combination of VT with\nKD yields an overall reduction up to x2.76.\nThe paper is organized as follows. After re-\nviewing related works in Section 2, we present\nthe methodology in Section 3, we then outline the\nexperiments in Section 4 and draw our conclusions\nin Section 5.\n2\nRelated Works\nThe goal of Model Compression is to shrink and\noptimize neural architectures, while retaining most\nof their initial performance. Research on LM com-\npression has been carried out following a variety\nof approaches like quantization (Gupta et al., 2015;\nShen et al., 2020), pruning (Zhu and Gupta, 2017;\nMichel et al., 2019) knowledge distillation (Sanh\net al., 2019; Jiao et al., 2020; Wang et al., 2020),\nand combinations thereof (Polino et al., 2018).\nA most popular distillation approach in NLP\nwas proposed by Sanh et al. (2019). The obtained\nmodel, called DistilBERT, is a smaller version of\nBERT, with the same architecture but half the lay-\ners, trained to imitate the full output distribution\nof the teacher (a pre-trained BERT model). Dis-\ntilBERT has a 40% smaller size than BERT and\nretains 97% of its language understanding capabil-\nities. This enables a 60% inference-time speedup.\nFurther compression was achieved by Jiao et al.\n(2020) by adding transformer-layer, prediction-\nlayer and embedding-layer distillation. The result-\ning model, TinyBERT, is 10 times smaller than\nBERT, with only four layers and reduced embed-\ndings sizes. Related methods were proposed (Sun\net al., 2020; Wang et al., 2020), achieving simi-\nlar compression rates. All these works focus on\nthe distillation of general-purpose language models.\nGordon and Duh (2020) investigated the interaction\nbetween KD and Domain Adaptation.\nLittle focus has been devoted thus far to the role\nof tokenization in the context of model compres-\nsion.\nEven in domain adaptation (Gordon and\nDuh, 2020), the vocabulary was kept the same.\nBoth the versatility of the subword-level tokeniza-\ntion, and the constraints imposed by the teacher-\nstudent framework (same output distribution), dis-\ncouraged such investigations. Recently, Samenko\net al. (2021) presented an approach for transfer-\nring the vocabulary of an LM into a new vocabu-\nlary learned from new domain, with the purpose of\nboosting the performance of the fine-tuned model.\nTo the best of our knowledge, we are the first to\nstudy VT in the scope of model compression.\n3\nVocabulary Transfer\nLet us consider a LM, trained on a general-purpose\ndomain Dgen and associated with a vocabulary\nVgen. Such a vocabulary is used by the LM’s tok-\nenizer in order to produce an encoding of the input\nstring via an embedding matrix Egen defined on\nVgen. More specifically, a tokenizer is a function\nthat maps a textual string into a sequence of sym-\nbols of a given vocabulary V. Let T be a tokenizer\nassociated with a vocabulary V and a string s, we\nhave T : s →(t1, . . . , tn), ti ∈V, ∀i = 1, . . . , n.\nHence, the vocabulary of the tokenizer determines\nhow words in a text are split, whether as words, sub-\nwords, or even characters. These symbols, which\ndefine the LM’s vocabulary, are statistically deter-\nmined by training the tokenizer to learn the distri-\nbution of a dataset.\nNow, let us consider a vertical domain Din, also\nreferred as in-domain. For the reasons discussed\nearlier, a vocabulary Vin specialized on Din itself\nbetter fits the language distribution than Vgen. Un-\nfortunately, with a new vocabulary, embedding\nrepresentations associated with the tokens of Vgen\nwould be lost. Thus, VT aims to initialize Vin by\nre-using most of the information learned from the\nLM pre-trained on Dgen. Once the new tokenizer\nTin has been trained on the in-domain dataset Din\nusing a given vocabulary size, Tin will be differ-\nent from the LM’s tokenizer Tgen. However, the\ntwo tokenizers’ vocabularies Vgen and Vin may still\nhave a large portion of their symbols in common.\nOur objective is to transfer most of the information\nfrom Vgen into Vin. To this end, we first define\na mapping between each symbol in Vin and a set\nof symbols in Vgen. Then, we define an assign-\nment criterion, based on the mapping, to obtain the\nembeddings for the tokens of Tin.\nOne such criterion, called Vocabulary Initializa-\ntion with Partial Inheritance (VIPI), was defined\nby Samenko et al. (2021). Whenever a token is in\nVin but not in Vgen, VIPI calculates all the parti-\ntions of the new token with tokens from Vgen, then\ntakes the minimal partitions and finally averages\nthem to obtain an embedding for the new token.\nDifferently, we define a simplified implementation\nof VIPI called FVT for Fast Vocabulary Transfer.\nInstead of calculating all tokenizations, FVT uses\na straightforward assignment mechanism, whereby\neach token ti ∈Vin is partitioned using Tgen. If ti\nbelongs to both vocabularies, ti ∈Vin ∩Vgen, then\nTgen(ti) = ti and the in-domain LM embedding\nInput:\nHe was initially treated with interferon alfa.\nTgen:\nHe, was, initially, treated, with, inter,##fer,\n##on, al, ##fa, .\nT100:\nHe, was, initially, treated, with, interferon,\nalfa, .\nFigure 2: Example of different tokenizations using a\npre-trained or an adapted tokenizer. In the latter case,\ndomain-specific words are not broken down into multi-\nple word pieces.\nDataset\nTgen\nT100\nT75\nT50\nT25\nADE\n31\n21\n22\n23\n26\nLEDGAR\n155\n131\n131\n132\n135\nCoNLL03\n19\n17\n17\n18\n20\nTable 1: Average sequence length on the three datasets\nwith different tokenizers. Tgen is the generic tokenizer\n(BERT cased), the same in each corpus, while T% are the\ntokenizers trained in the vertical domain itself, where %\nindicates the percentage of the original vocabulary size\nthat has been set for training it.\nEin(ti) is the same as the embedding in the general\nLM:\nEin(ti) = Egen(ti).\n(1)\nIf instead ti ∈Vin \\ Vgen, then the in-domain em-\nbedding is the average of the embeddings associ-\nated with the tokens produced by Tgen:\nEin(ti) =\n1\n|Tgen(ti)| ·\nX\ntj∈Tgen(ti)\nEgen(tj).\n(2)\nPlease notice that Equation 2 is a generalization\nof Equation 1. Indeed, in case ti ∈Vin ∩Vgen,\nEquation 2 falls back to Equation 1.\nOnce embeddings are initialized with FVT, we\nadjust the model’s weights by training it with MLM\non the in-domain data before fine-tuning it on the\ndownstream task. MLM eases adaptation and has\nalready been found to be beneficial in (Samenko\net al., 2021). We observed this trend as well during\npreliminary experiments, therefore we kept such a\ntuning stage in all our experiments.\nAs a baseline model, we also implement a\nmethod called Partial Vocabulary Transfer (PVT),\nwhereby only the tokens belonging to both vocab-\nularies ti ∈Vin ∩Vgen are initialized with pre-\ntrained embeddings, while unseen new tokens are\nrandomly initialized.\nTransfer\nADE\nLEDGAR\nCoNLL03\nTgen\n90.80\n80.93\n89.43\nT100 + FVT\n90.77\n80.60\n87.87\nT75 + FVT\n90.40\n80.93\n87.90\nT50 + FVT\n90.07\n80.93\n86.87\nT25 + FVT\n90.27\n81.03\n86.17\nT100 + PVT\n82.57\n80.07\n84.53\nT75 + PVT\n82.47\n80.33\n84.63\nT50 + PVT\n83.07\n80.23\n84.43\nT25 + PVT\n83.57\n80.20\n83.47\nTable 2: F1 results on the three benchmarks. A pre-\ntrained language model fine-tuned on the task (Tgen)\nis compared with models having differently sized\nin-domain tokenizers (T100, T75, T50, T25) adapted by\ntransferring information with FVT or PVT.\n3.1\nDistillation\nVT can be combined with other model compression\nmethods like quantization, pruning and KD. For\nsome of the methods, the combination is trivial,\nsince they have no impact on the vocabulary. KD,\nhowever, requires the vocabularies of the student\nand teacher to be aligned. Hence, its integration\nwith VT is non-trivial. Accordingly, we set up a\nKD procedure with VT, in order to determine the\neffects of applying both VT and KD to an LM.\nOur distillation consists of two steps. In the first\nstep, we replicate the distillation process used in\n(Sanh et al., 2019) for DistilBERT, in which the\nnumber of layers of the encoder is halved and a\ntriple loss-function is applied: a distillation loss, a\nMLM loss, and a cosine embedding loss. However,\nunlike the original setup, we do not remove the\ntoken-type embeddings and pooler. Inspired by\nGordon and Duh (2020), after distilling the student\non Dgen, we further distil the student using Din.\nHowever, instead of adapting the teacher before the\nsecond distillation, we simply distil the student a\nsecond time on the in-domain dataset. Finally, we\napply VT using either FVT or PVT and fine-tune\nthe student model on the in-domain datasets.\nOur choice of applying VT after KD is based on\nfindings by Kim and Hassan (2020), that different\ninput embedding spaces will produce different out-\nput embedding spaces. This difference in spaces\nis not conducive to knowledge transfer during dis-\ntillation. Hence, if VT were to be applied first to\nthe student, its input embedding space would differ\ngreatly from that of the pre-trained teacher during\ndistillation.\nDistillation\nTransfer\nADE\nLEDGAR\nCoNLL03\nTgen\n90.47\n78.37\n86.90\nT100 + FVT\n89.47\n78.33\n84.63\nT75 + FVT\n88.57\n78.90\n84.23\nT50 + FVT\n88.43\n79.30\n83.80\nT25 + FVT\n88.23\n78.10\n83.13\nT100 + PVT\n79.13\n76.97\n81.13\nT75 + PVT\n78.87\n76.93\n81.40\nT50 + PVT\n76.30\n77.37\n81.63\nT25 + PVT\n77.90\n77.33\n79.50\nTable 3: F1 results on the three benchmarks. A distilled\nlanguage model fine-tuned on the task (Tgen) is com-\npared with models having differently sized in-domain\ntokenizers (T100, T75, T50, T25) adapted by transferring\ninformation with FVT or PVT.\n4\nExperiments\nIn the experiments we measure the impact of FVT\non three main KPIs: quality (F1 score), size of the\nmodels and speedup in inference.\n4.1\nExperimental Setup\nWe consider for all our experiments the pre-trained\ncased version of BERTbase (Devlin et al., 2018) as\nour pre-trained language model. Its tokenizer is\ncomposed of 28996 wordpieces. We then define\nfour vocabulary sizes for retraining our tokenizers.\nSpecifically, we take the original vocabulary size\nand define it as a vocabulary size of 100%. We sub-\nsequently reduce this size to 75%, 50%, and 25%.\nFrom now on, we will refer to such tokenizers as\nT100, T75, T50, T25 respectively, while the original\nvocabulary will be called Tgen.\nModels are fine-tuned for 10 epochs with early\nstopping on the downstream task. We set the initial\nlearning rate to 3 · 10−5 and batch size to 64 for\neach task. The sequence length is set to 64 for ADE\nand CoNLL03 and 128 for LEDGAR. Each config-\nuration is repeated 3 times with different random\ninitializations. MLM is performed for one epoch.\n4.2\nDatasets\nTo best assess the effectiveness of VT, we apply\nit on three different tasks from three heteroge-\nneous linguistic domains: medical (ADE), legal\n(LEDGAR) and news (CoNLL03). Table 4 reports\nthe dataset statistics.\nADE.\nThe Adverse Drug Events (ADE) corpus\n(Gurulingappa et al., 2012) is a binary sentence\nFigure 3: Sequence length distribution of each tokenizer on ADE, LEDGAR and CoNLL03 (left to right).\nclassification dataset in the medical domain. This\ndomain is particularly suitable for investigating the\nbenefits of VT, since documents are characterized\nby the presence of frequent technical terms, such\nas drug and disease names, that are usually rare\nin common language. Domain-specific words are\nusually split into multiple tokens, yielding longer\nsequences and breaking the semantics of a word\ninto multiple pieces. An example is shown in Fig-\nure 2.\nLEDGAR.\nLEDGAR (Tuggener et al., 2020) is a\ndocument classification corpus of legal provisions\nin contracts from the US Securities and Exchange\nCommission (SEC). The dataset is annotated with\n100 different mutually-exclusive labels. It is also\npart of LexGLUE (Chalkidis et al., 2022), a bench-\nmark for legal language understanding.\nCoNLL03.\nCoNLL03 (Tjong Kim Sang and\nDe Meulder, 2003) is a popular Named Entity\nRecognition (NER) benchmark. It is made of news\nstories from the Reuters corpus. We chose this cor-\npus because, differently from ADE and LEDGAR,\nthe news domain typically uses a more standard\nlanguage, hence we expect its distribution to differ\nless from the one captured by a general-purpose\ntokenizers in the web. Statistics in Table 1 con-\nfirms this hypothesis. We can observe that the\nsequence compression gain obtained with domain-\nspecific tokenizers is less significant with respect\nto LEDGAR and ADE.\nDataset\nTrain\nValidation\nTest\nADE\n16716\n3344\n836\nLEDGAR\n60000\n10000\n10000\nCoNLL03\n14042\n3251\n3454\nTable 4: Number of examples of each dataset.\n4.3\nResults\nWe report an extensive evaluation of FVT on differ-\nent setups and perspectives.\nFigure 4: F1-score vs model size of VT with or without\nKD on ADE. VT and KD together can further compress\na LM’s size in exchange for a limited performance drop.\nFVT is better than PVT. A smaller vocabulary size does\nnot always imply a lower performance.\nIn-domain Tokenization.\nBy retraining the tok-\nenizer on the in-domain dataset, the average num-\nber of tokens per sequence decreases since the\nlearned distribution reduces the number of word\nsplits, as shown in Table 1. In the medical domain,\nwhich is particularly specialized, we notice a re-\nmarkable 32% reduction of the average number of\ntokens per sequence. We expect this to yield a no-\nticeable impact on inference time speedup. Further-\nmore, we can notice in Figure 3 that the sequence\nlength distribution shifts to the left for the learned\ntokenizers. It can also be observed that by reduc-\ning the vocabulary size of the in-domain tokenizer,\nthe sequence length distribution will begin to shift\nback to the right. Indeed, with fewer tokens in its\nvocabulary, the tokenizer will need to break down\nwords more frequently into subwords.\nVocabulary Transfer.\nFrom the results shown\nin Tables 2 and 3, we note a few interesting find-\nings. First, FVT vectors initialization method con-\nsistently outperforms the baseline PVT, which con-\nfirms the positive contribution of Equation 2. Sec-\nond, transferring vocabulary with FVT causes lim-\nited drops in performance, especially in LEDGAR\n(the largest one), where F1 slightly increases de-\nspite a 75% vocabulary reduction. As a matter of\nTransfer\nADE\nLEDGAR\nCoNLL03\n∆F1\n∆Size\nSpeedup\n∆F1\n∆Size\nSpeedup\n∆F1\n∆Size\nSpeedup\nTgen\n90.80 433.32\n1.00\n80.93 433.62\n1.00\n89.43 430.98\n1.00\nT100 + FVT\n-0.04\n0.00\n1.40\n-0.41\n0.00\n1.21\n-1.75\n0.00\n1.07\nT75 + FVT\n-0.44\n-5.14\n1.35\n0.00\n-5.14\n1.21\n-1.71\n-5.17\n1.07\nT50 + FVT\n-0.81\n-10.28\n1.32\n0.00\n-10.27\n1.10\n-2.87\n-10.33\n1.02\nT25 + FVT\n-0.59\n-15.42\n1.20\n0.12\n-15.41\n1.09\n-3.65\n-15.50\n0.99\nDistil + T100 + FVT\n-1.47\n-39.26\n2.76\n-3.21\n-39.24\n2.38\n-5.37\n-39.48\n2.11\nDistil + T75 + FVT\n-2.46\n-44.40\n2.64\n-2.51\n-44.37\n2.38\n-5.81\n-44.64\n2.11\nDistil + T50 + FVT\n-2.61\n-49.54\n2.59\n-2.02\n-49.51\n2.16\n-6.30\n-49.81\n2.01\nDistil + T25 + FVT\n-2.83\n-54.68\n2.37\n-3.50\n-54.64\n2.14\n-7.04\n-54.98\n1.96\nTable 5: The first row (Tgen) reports absolute values of the LM fine-tuned on the downstream task without VT or\nKD. The rows below show values relative to Tgen.\nfact, the effects of FVT on model performance do\nnot have a steadily decreasing trend as might be ex-\npected when reducing the vocabulary size, as also\nevident from Figure 4. In some cases, somewhat\nsurprisingly, reducing the vocabulary size yields\nbetter model performance. In other cases, a 50%\nvocabulary size reduction yields better results than\na full scale reduction or no reduction. Hence, vo-\ncabulary size should be considered as a hyperpa-\nrameter.\nVocabulary Transfer and Distillation.\nThe re-\nsults summarized in Table 3 clearly indicate that\nKD is complementary to VT: there is no harm in\napplying them together, in terms of performance\non the downstream task. Crucially, this guarantees\na full exploitation of FVT in the scope of language\nmodel compression.\nCompression and Efficiency.\nAfter showcasing\nthat VT has limited impact on performance, we\nanalyze and discuss its effects on efficiency and\nmodel compression. Table 5 reports the relative F1\ndrop on the downstream task with respect to the\noriginal LM (∆F1), the relative reduction in model\nsize (∆Size) and the speedup gained by FVT alone\nand by FVT combined with KD for varying vocab-\nulary sizes. Either way, FVT achieves a remarkable\n15%+ reduction with respect to BERT’s learnable\nparameters, with almost no loss in F1.\nFurthermore, the reduced input length enabled\nby in-domain tokenization brings a reduction in\ninference time. The more a language is specialized,\nthe higher is the speedup with in-domain tokeniz-\ners. This is also confirmed by the experiments,\nwhere the major benefits are obtained on the med-\nical domain, with a x1.40 speedup. In CoNLL03\ninstead where language is much less specialized,\nspeedup reduces and even disappears with T25. Dis-\ntillation further pushes compression and speedup\nin any benchmark and setup, up to about 55% (of\nwhich 15% due to VT) and x2.75 respectively.\nIn summary, depending on the application needs,\nVT enables a strategic trade-off between compres-\nsion rate, inference speed and accuracy.\n5\nConclusion\nThe viability and success of industrial NLP applica-\ntions often hinges on a delicate trade-off between\ncomputational requirements, responsiveness and\noutput quality. Hence, language model compres-\nsion methods are an active area of research whose\npractical ramifications are self-evident. One of the\nfactors that greatly contribute to a model’s infer-\nence speed and memory footprint is vocabulary\nsize. VT has been recently proposed for improving\nperformance, but never so far in the scope of model\ncompression. In this work, we run an extensive ex-\nperimental study on the application of a lightweight\nmethod for VT, called FVT. An analysis conducted\non various downstream tasks, application domains,\nvocabulary sizes and on its possible combination\nwith knowledge distillation indicates that FVT en-\nables a strategic trade-off between compression\nrate, inference speed and accuracy, especially, but\nnot only, in more specialized domains. Importantly,\nFVT appears to be orthogonal to other model com-\npression methods.\nIn the future, we plan to fully integrate Vocabu-\nlary Transfer within Knowledge Distillation during\nthe learning process in order to maximize the infor-\nmation transfer.\nReferences\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nIlias Chalkidis, Abhik Jana, Dirk Hartung, Michael\nBommarito, Ion Androutsopoulos, Daniel Katz, and\nNikolaos Aletras. 2022. LexGLUE: A benchmark\ndataset for legal language understanding in English.\nIn Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 4310–4330, Dublin, Ireland.\nAssociation for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. arXiv preprint arXiv:1810.04805.\nMitchell Gordon and Kevin Duh. 2020. Distill, adapt,\ndistill: Training small, in-domain models for neural\nmachine translation. In Proceedings of the Fourth\nWorkshop on Neural Generation and Translation,\npages 110–118, Online. Association for Computa-\ntional Linguistics.\nSuyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan,\nand Pritish Narayanan. 2015. Deep learning with lim-\nited numerical precision. In International conference\non machine learning, pages 1737–1746. PMLR.\nHarsha Gurulingappa, Abdul Mateen Rajput, Angus\nRoberts, Juliane Fluck, Martin Hofmann-Apitius, and\nLuca Toldo. 2012. Development of a benchmark\ncorpus to support the automatic extraction of drug-\nrelated adverse effects from medical case reports.\nJournal of Biomedical Informatics, 45(5):885 – 892.\nText Mining and Natural Language Processing in\nPharmacogenomics.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. 2020. DeBERTa: Decoding-enhanced\nBERT with disentangled attention. arXiv preprint\narXiv:2006.03654.\nGeoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. 2015.\nDistilling the knowledge in a neural network. arXiv\npreprint arXiv:1503.02531, 2(7).\nXiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang,\nXiao Chen, Linlin Li, Fang Wang, and Qun Liu.\n2020. TinyBERT: Distilling BERT for natural lan-\nguage understanding. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n4163–4174.\nYoung Jin Kim and Hany Hassan. 2020.\nFast-\nFormers: Highly efficient transformer models for\nnatural language understanding.\nIn Proceedings\nof SustaiNLP: Workshop on Simple and Efficient\nNatural Language Processing, pages 149–158, On-\nline. Association for Computational Linguistics.\nTaku Kudo and John Richardson. 2018. Sentencepiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing.\narXiv preprint arXiv:1808.06226.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized BERT pretraining\napproach. arXiv preprint arXiv:1907.11692.\nPaul Michel, Omer Levy, and Graham Neubig. 2019.\nAre sixteen heads really better than one? Advances\nin neural information processing systems, 32.\nAntonio Polino, Razvan Pascanu, and Dan Alistarh.\n2018. Model compression via distillation and quanti-\nzation. arXiv preprint arXiv:1802.05668.\nIgor Samenko, Alexey Tikhonov, Borislav Kozlovskii,\nand Ivan P Yamshchikov. 2021.\nFine-tuning\ntransformers: Vocabulary transfer. arXiv preprint\narXiv:2112.14569.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. DistilBERT, a distilled version\nof BERT: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108.\nMike Schuster and Kaisuke Nakajima. 2012. Japanese\nand\nkorean\nvoice\nsearch.\nIn\n2012\nIEEE\ninternational conference on acoustics, speech and\nsignal processing (ICASSP), pages 5149–5152.\nIEEE.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In Proceedings of the 54th Annual\nMeeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1715–\n1725.\nSheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei\nYao, Amir Gholami, Michael W Mahoney, and Kurt\nKeutzer. 2020. Q-BERT: Hessian based ultra low\nprecision quantization of BERT. In Proceedings of\nthe AAAI Conference on Artificial Intelligence, vol-\nume 34, pages 8815–8821.\nZhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu,\nYiming Yang, and Denny Zhou. 2020. MobileBERT:\na compact task-agnostic BERT for resource-limited\ndevices. arXiv preprint arXiv:2004.02984.\nErik F. Tjong Kim Sang and Fien De Meulder.\n2003. Introduction to the CoNLL-2003 shared task:\nLanguage-independent named entity recognition. In\nProceedings of the Seventh Conference on Natural\nLanguage Learning at HLT-NAACL 2003, pages\n142–147.\nDon Tuggener, Pius von Däniken, Thomas Peetz, and\nMark Cieliebak. 2020. Ledgar: A large-scale multi-\nlabel corpus for text classification of legal provisions\nin contracts. In Proceedings of the 12th Language\nResources and Evaluation Conference, pages 1235–\n1241.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017.\nAttention is\nall you need.\nAdvances in neural information\nprocessing systems, 30.\nWenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan\nYang, and Ming Zhou. 2020. Minilm: Deep self-\nattention distillation for task-agnostic compression\nof pre-trained transformers.\nAdvances in Neural\nInformation Processing Systems, 33:5776–5788.\nMichael Zhu and Suyog Gupta. 2017. To prune, or not\nto prune: exploring the efficacy of pruning for model\ncompression. arXiv preprint arXiv:1710.01878.\n",
  "categories": [
    "cs.CL",
    "cs.AI",
    "cs.LG"
  ],
  "published": "2024-02-15",
  "updated": "2024-02-15"
}