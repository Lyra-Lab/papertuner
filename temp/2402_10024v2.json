{
  "id": "http://arxiv.org/abs/2402.10024v2",
  "title": "Self-Augmented In-Context Learning for Unsupervised Word Translation",
  "authors": [
    "Yaoyiran Li",
    "Anna Korhonen",
    "Ivan Vulić"
  ],
  "abstract": "Recent work has shown that, while large language models (LLMs) demonstrate\nstrong word translation or bilingual lexicon induction (BLI) capabilities in\nfew-shot setups, they still cannot match the performance of 'traditional'\nmapping-based approaches in the unsupervised scenario where no seed translation\npairs are available, especially for lower-resource languages. To address this\nchallenge with LLMs, we propose self-augmented in-context learning (SAIL) for\nunsupervised BLI: starting from a zero-shot prompt, SAIL iteratively induces a\nset of high-confidence word translation pairs for in-context learning (ICL)\nfrom an LLM, which it then reapplies to the same LLM in the ICL fashion. Our\nmethod shows substantial gains over zero-shot prompting of LLMs on two\nestablished BLI benchmarks spanning a wide range of language pairs, also\noutperforming mapping-based baselines across the board. In addition to\nachieving state-of-the-art unsupervised BLI performance, we also conduct\ncomprehensive analyses on SAIL and discuss its limitations.",
  "text": "Self-Augmented In-Context Learning for Unsupervised Word Translation\nYaoyiran Li\nAnna Korhonen\nIvan Vuli´c\nLanguage Technology Lab, TAL, University of Cambridge\n{yl711,alk23,iv250}@cam.ac.uk\nAbstract\nRecent work has shown that, while large lan-\nguage models (LLMs) demonstrate strong word\ntranslation or bilingual lexicon induction (BLI)\ncapabilities in few-shot setups, they still can-\nnot match the performance of ‘traditional’\nmapping-based approaches in the unsupervised\nscenario where no seed translation pairs are\navailable, especially for lower-resource lan-\nguages. To address this challenge with LLMs,\nwe propose self-augmented in-context learning\n(SAIL) for unsupervised BLI: starting from a\nzero-shot prompt, SAIL iteratively induces a\nset of high-confidence word translation pairs\nfor in-context learning (ICL) from an LLM,\nwhich it then reapplies to the same LLM in the\nICL fashion. Our method shows substantial\ngains over zero-shot prompting of LLMs on\ntwo established BLI benchmarks spanning a\nwide range of language pairs, also outperform-\ning mapping-based baselines across the board.\nIn addition to achieving state-of-the-art unsu-\npervised BLI performance, we also conduct\ncomprehensive analyses on SAIL and discuss\nits limitations.\n1\nIntroduction and Motivation\nThe task of word translation (WT), also known\nas bilingual lexicon induction (BLI), aims to auto-\nmatically induce lexica of words with the same or\nsimilar meaning in different languages, thus bridg-\ning the lexical gap between languages. Even in\nthe era of large language models (LLMs), BLI still\nhas wide applications in machine translation and\ncross-lingual transfer learning (Sun et al., 2021;\nZhou et al., 2021; Wang et al., 2022; Ghazvinine-\njad et al., 2023; Jones et al., 2023). A particular\nBLI setup, termed (fully) unsupervised BLI, is es-\npecially compelling because it is not only more\ntechnically challenging but is also used as a pivotal\ncomponent towards unsupervised machine trans-\nlation (Lample et al., 2018; Artetxe et al., 2018b;\nMarchisio et al., 2020; Chronopoulou et al., 2021).\nUntil recently, BLI approaches have predomi-\nnantly relied on learning cross-lingual word em-\nbedding (CLWE) mappings:\nthese are known\nas MAPPING-BASED approaches and are developed\nbased on static or decontextualised word embed-\ndings (WEs) (Patra et al., 2019; Grave et al., 2019;\nLi et al., 2022a; Yu et al., 2023). Meanwhile, au-\ntoregressive LLMs have become the cornerstone\nof modern NLP techniques (Brown et al., 2020;\nOuyang et al., 2022; Touvron et al., 2023a) with\nsuccess in many real-world tasks (Kasneci et al.,\n2023; Wu et al., 2023; Thirunavukarasu et al., 2023;\nLi et al., 2024). Given this trend, recent BLI re-\nsearch has also started to shift towards exploring\nLLMs. Li et al. (2023) first show that prompting\nLLMs with gold-standard WT pairs as in-context\nexamples (few-shot in-context learning: ICL) out-\nperforms all existing BLI approaches in the super-\nvised and semi-supervised BLI setups (where typi-\ncally 1K∼5K gold-standard WT pairs are available\nfor training or ICL), while zero-shot prompting still\nfalls behind traditional MAPPING-BASED approaches\nin the fully unsupervised BLI setup, especially for\nlower-resource languages.\nIn this work, we thus aim at improving unsuper-\nvised BLI with LLMs. To this end, we analyze the\nlimitations of zero-shot prompting and propose a\nnovel self-augmented in-context learning (SAIL)\nmethod for unsupervised BLI with LLMs. The\nkey idea is to first retrieve a set of high-confidence\nWT pairs by zero-shot prompting LLMs, then it-\neratively refine the high-confidence dictionary and\nfinally use the gradually refined bilingual lexicon\nfor BLI inference in an ICL fashion (§2). Our\nextensive experiments show that SAIL establishes\nnew state-of-the-art unsupervised BLI performance\non two standard BLI benchmarks. We also con-\nduct thorough analyses on our approach, provid-\ning further insights into its inner workings (§3-\n§4).\nOur code is publicly available at https:\n//github.com/cambridgeltl/sail-bli.\narXiv:2402.10024v2  [cs.CL]  5 Jun 2024\n2\nMethodology\nUnsupervised BLI: Task Preliminaries. We as-\nsume a pair of two languages: a source language\nLx with its vocabulary X and a target language\nLy with vocabulary Y. In a typical, standard BLI\nsetup the vocabulary of each language contains\nthe most frequent 200, 000 word types in the lan-\nguage (Glavaš et al., 2019; Li et al., 2022a). Given\na source word wx ∈X, the unsupervised BLI task\nthen aims to infer its translation in Ly, without any\nword-level parallel data (i.e., seed translation pairs\nfrom a lexicon) available for training or ICL.1\nZero-Shot Prompting. Li et al. (2023) have pro-\nposed to prompt autoregressive LLMs for the BLI\ntask, where the input word wx is embedded into\na predefined text template. We adopt the pool of\ntemplates provided by Li et al. (2023) and conduct\ntemplate search for each LLM on a randomly cho-\nsen language pair. As an example, the zero-shot\ntemplate for LLAMA-27B is as follows:2\n‘The Lx word wx in Ly is:’,\nwhere Lx, Ly, and wx are placeholders for the\nsource language, target language, and the query\nword in the source language (e.g., Lx = Hungarian,\nwx = macska, Ly = Catalan).\nThe deterministic beam search (with beam size\nof n as a hyper-parameter) is adopted to generate\nn output text pieces in the final beam, ranked by\ntheir sequence scores.3 For each of the n outputs,\nthe first word in the generated output following the\ninput sequence is extracted as a candidate answer.\nAfter filtering out those candidate answers not in\nY, the candidate Ly word with the highest associ-\nated sequence score is returned as the final word\ntranslation prediction.\nLimitations of Zero-Shot Prompting. The above\nzero-shot approach for unsupervised BLI, proposed\nby Li et al. (2023), comes with several limitations.\nFirst, the template does not stipulate the output for-\nmat and thus parsing the output text may not be as\nstraightforward as expected. Put simply, LLM’s\nprediction may not be the first word in the gener-\nated sequence. Second, the LLM may not fully\n‘understand’ the input template and sometimes may\n1Following prior work, when wx has multiple ground truth\ntranslations in Ly, a prediction is considered correct if it is\nany of the ground truth answers.\n2The full list of templates used for other LLMs are pre-\nsented in Appendix C.\n3We use n = 5 following Li et al. (2023).\ntend not to generate words in the target language\nespecially for lower-resource languages. For the\nsupervised BLI setup, where a dictionary of gold\nstandard translation pairs is assumed and available,\nfew-shot in-context learning can substantially im-\nprove final BLI performance (Li et al., 2023), since\nit not only provides examples of the desired out-\nput format but also helps LLMs ‘understand’ the\nBLI task. However, the availability of such a seed\ndictionary is not assumed in the unsupervised BLI\ntask variant, and the key idea of this work is to\nderive and iteratively refine a seed dictionary by\nprompting LLMs.\nSAIL: Self-Augmented In-Context Learning for\nUnsupervised BLI. We thus propose to facili-\ntate and improve unsupervised BLI by S1) using\nzero-shot prompting to retrieve Dh, a set of high-\nconfidence translation pairs, and then S2) leverag-\ning these pairs as ‘self-augmented’ in-context ex-\namples for few-shot prompting to further iteratively\nrefine Dh (across 0 to Nit −1 iterations, where Nit\nis a hyper-parameter denoting total times of Dh\ninference in S1 and S2), and finally S3) conducting\nfew-shot learning with the final, Nit-th self-created\nseed lexicon Dh for BLI inference on the test set.\nDeriving High-Confidence Pairs. For both steps\nS1 and S2 outlined above, we start with the most\nfrequent Nf words in Lx since representations of\nless frequent words are considered to be much nois-\nier in general (Artetxe et al., 2018a). For each wx,\nwe conduct Lx →Ly translation: we refer to this\npredicted word as ˆwy.4 We then propose to con-\nduct word back-translation, translating ˆwy from\nLy back into Lx. The word pair (wx, ˆwy) is con-\nsidered a high-confidence pair only if wx is also\nthe output word of the back-translation step.5 We\ndenote the set of all high-confidence pairs from the\nLx words as Dx\nh. Likewise, we also start from the\nmost frequent Nf words in Ly and symmetrically\nderive Dy\nh. Finally, we update the high-confidence\ndictionary with Dh = Dx\nh ∪Dy\nh.6\nFew-Shot Prompting with High-Confidence\nPairs. Step S1 of SAIL relies on zero-shot prompt-\ning, but all the subsequent iterations in S2 and\n4We do not require ˆwy to be one of the most frequent Nf\nwords in Ly.\n5Earlier MAPPING-BASED approaches have retrieved high-\nconfidence pairs through ranking cross-lingual word simi-\nlarity scores (e.g., cosine similarity) to refine CLWE map-\npings (Artetxe et al., 2018a; Li et al., 2022a); in a sense, our\nwork renovates and revitalises the idea with LLMs.\n6Therefore, |Dx\nh| ≤Nf, |Dy\nh| ≤Nf, and |Dh| ≤2×Nf.\nS3 apply few-shot prompting/ICL with the ‘self-\naugmented’ high-confidence translation pairs Dh.\nFollowing Li et al. (2023), we adopt 5-shot prompt-\ning, and again conduct template search on the\nBLI task with a single, randomly selected lan-\nguage pair.7 The in-context examples, (wx\ni , wy\ni ) ∈\nDh, 1 ≤i ≤5, are retrieved where the wx\ni words\nare the nearest neighbours of the input word wx in\nLx’s static word embedding space. The few-shot\ntemplate for LLAMA-27B is then as follows:\n‘The Lx word wx\n1 in Ly is wy\n1. The\nLx word wx\n2 in Ly is wy\n2. ... The Lx\nword wx in Ly is’.\n3\nExperimental Setup\nBLI Data and LLMs. We adopt two standard BLI\nbenchmarks: 1) 5 languages from XLING (Glavaš\net al., 2019) including German (DE), English (EN),\nFrench (FR), Italian (IT), and Russian (RU), their\ncombinations resulting in 20 BLI directions; 2)\n3 lower-resource languages including Bulgarian\n(BG), Catalan (CA), and Hungarian (HU) from\nPanLex-BLI (Vuli´c et al., 2019), which result in\n6 BLI directions.8 For both benchmarks, a test\nset of 2K WT pairs is provided for each BLI di-\nrection.\nWe experiment with four open-source\nLLMs: LLAMA 7B, LLAMA-27B, LLAMA 13B, and\nLLAMA-213B (Touvron et al., 2023a,b). Li et al.\n(2023) found that 4 other families of LLMs, includ-\ning mT5, mT0, mGPT and XGLM, underperform\nLLAMA; we thus skip these LLMs in our work.\nImplementation Details and BLI Evaluation. As\nmentioned in §2, our hyper-parameter and tem-\nplate search are conducted on a single, randomly\nselected language pair, which is DE-FR, follow-\ning Li et al. (2023). Batch size is set to 1. We\nadopt Nit = 1, Nf = 5, 000 in our main experi-\nments (§4.1) and then investigate their influence\non BLI performance and the effectiveness of our\nproposed word back-translation in our further anal-\nyses (§4.2). Half-precision floating-point format\n(torch.float16) is adopted for all our SAIL and\nzero-shot experiments. Since our method does not\nimply any randomness, all results are from single\nruns. For evaluation, we adopt the standard top-1\naccuracy as prior work.\n7The decoding and output parsing strategy is the same as\nin zero-shot prompting.\n8The two datasets are also used in many recent BLI\nworks (Sachidananda et al., 2021; Aboagye et al., 2022; Li\net al., 2022a,b; Vuli´c et al., 2020, 2023; Li et al., 2023).\nBaselines. We adopt two established MAPPING-\nBASED baselines. 1) VECMAP is a representative\nunsupervised BLI approach and features a self-\nlearning mechanism that refines linear maps for\nderiving CLWEs (Artetxe et al., 2018a). 2) CON-\nTRASTIVEBLI learns CLWEs with a two-stage con-\ntrastive learning framework and is the strongest\nMAPPING-BASED approach for supervised and semi-\nsupervised BLI tasks on our two benchmarks (Li\net al., 2022a); however, it does not support unsuper-\nvised setup. We extend CONTRASTIVEBLI to unsu-\npervised BLI by initialising the initial map with the\nunsupervised VECMAP method. The CONTRASTIVE-\nBLI C1 variant based on static WEs and its stronger\nC2 variant combining static and decontextualised\nWEs are both used as our baselines. We adopt\nCross-domain Similarity Local Scaling (CSLS) re-\ntrieval (Lample et al., 2018) for all MAPPING-BASED\napproaches as recommended in the baselines. In\naddition, we report 3) ZERO-SHOT prompting with\neach of our LLMs as baselines following the previ-\nous findings of Li et al. (2023).\n4\nResults and Discussion\n4.1\nMain Results\nResults on the Two BLI Benchmarks are sum-\nmarised in Tables 1 and 2 respectively, with full\nBLI scores per each individual language pair in Ta-\nbles 8 and 9 in Appendix F. As the main findings, 1)\nour SAIL shows consistent gains against ZERO-SHOT\nprompting for each of the 4 LLMs, showing the\neffectiveness of the proposed approach; 2) while\nZERO-SHOT prompting still lags behind MAPPING-\nBASED approaches on PanLex-BLI’s lower-resource\nlanguages, applying SAIL outperforms MAPPING-\nBASED baselines across the board. The only excep-\ntion is that CONTRASTIVEBLI (C2) still has a slight\nedge over SAIL with the weakest LLM overall,\nLLAMA 7B. 3) Among the 4 LLMs, LLAMA-213B\npresents the strongest BLI capability.\nVariance and Statistical Significance. The whole\nSAIL method does not imply any variance due to\nrandomness: it does not rely on any actual LLM\nfine-tuning; we adopt deterministic beam search;\nthe deterministic nearest neighbour retrieval is\nused for deriving in-context examples. Here, we\nreport the statistical significance with χ2 tests.\nWhen comparing SAIL and ZERO-SHOT prompting\n(both with LLAMA-213B), the p-value is 1.1e-251\non 20 XLING BLI directions and 2.7e-109 on 6\nPanLex-BLI BLI directions. We then compare\n[Unsupervised BLI]\nDE\nEN\nFR\nIT\nRU\nAVG.\nMAPPING-BASED\nVECMAP\n44.14\n51.7\n51.51\n51.03\n34.36\n46.55\nCONTRASTIVEBLI (C1)\n44.72\n52.12\n52.29\n51.77\n35.5\n47.28\nCONTRASTIVEBLI (C2)\n46.02\n53.32\n53.26\n52.99\n37.26\n48.57\nZERO-SHOT\nLLAMA 7B\n41.94\n50.16\n48.25\n46.91\n40.04\n45.46\nLLAMA-27B\n43.91\n52.7\n50.68\n48.23\n42.8\n47.66\nLLAMA 13B\n45.39\n53.35\n52.39\n50.58\n41.74\n48.69\nLLAMA-213B\n47.12\n55.02\n51.31\n52.02\n43.09\n49.71\nSAIL (Ours)\nLLAMA 7B\n51.39\n61.92\n58.92\n56.94\n50.7\n55.97\nLLAMA-27B\n53.81\n64.12\n61.09\n59.96\n53.77\n58.55\nLLAMA 13B\n55.35\n64.84\n62.49\n61.27\n54.5\n59.69\nLLAMA-213B\n57.69\n67.0\n64.11\n63.18\n57.04\n61.8\nTable 1: Main results on the 20 XLING BLI directions.\nFor each language, the average accuracy scores over 8\nBLI directions (i.e., going from and going to other 4\nlanguages) is reported. See also Appendix F.\n[Unsupervised BLI]\nBG\nCA\nHU\nAVG.\nMAPPING-BASED\nVECMAP\n37.22\n36.27\n36.89\n36.8\nCONTRASTIVEBLI (C1)\n36.7\n35.86\n37.82\n36.79\nCONTRASTIVEBLI (C2)\n38.87\n38.48\n40.54\n39.3\nZERO-SHOT\nLLAMA 7B\n27.9\n28.87\n27.18\n27.98\nLLAMA-27B\n28.2\n27.21\n26.92\n27.45\nLLAMA 13B\n27.49\n30.61\n28.2\n28.77\nLLAMA-213B\n29.08\n32.38\n30.53\n30.66\nSAIL (Ours)\nLLAMA 7B\n37.02\n37.63\n36.29\n36.98\nLLAMA-27B\n40.06\n40.51\n40.22\n40.27\nLLAMA 13B\n41.71\n42.76\n42.07\n42.18\nLLAMA-213B\n45.4\n46.26\n44.88\n45.51\nTable 2: Main results on 6 PanLex-BLI BLI directions.\nFor each language, the average accuracy scores over 4\nBLI directions (i.e., going from and going to other 2\nlanguages) is reported. See also Appendix F.\nSAIL (with LLAMA-213B) against CONTRASTIVEBLI\n(C2) which is our strongest MAPPING-BASED base-\nline: the p-values are 3.1e-300 and 7.8e-20 respec-\ntively. These show that our findings are strongly\nstatistically significant.9\n4.2\nFurther Analyses\nInspection of High-Confidence Dictionaries. To\nprovide additional insight into our SAIL approach,\nwe present statistics on the size of high-confidence\ndictionaries derived in our main experiments\n9Usually p < 0.05 or p < 0.001 is considered to indicate\nstatistical significance.\nLLM (SAIL)\n|Dh|: XLING\n|Dh|: PanLex-BLI\nMEAN\nMIN∼MAX\nMEAN\nMIN∼MAX\nLLAMA 7B\n2471\n1731∼3180\n1735\n1363∼2095\nLLAMA-27B\n3019\n2086∼3824\n1873\n1690∼2183\nLLAMA 13B\n2850\n2064∼3579\n2005\n1548∼2351\nLLAMA-213B\n2612\n1577∼3362\n1737\n1184∼2049\nTable 3: Statistics on |Dh| for each LLM over 20\nXLING BLI directions and 6 PanLex-BLI BLI direc-\ntions respectively.\n(Nit = 1, Nf = 5, 000, and with word back-\ntranslation) over 20 XLING BLI directions and 6\nPanLex-BLI BLI directions respectively for each of\nour four LLMs in Table 3. The values indicate that\n|Dh| of higher-resource languages (XLING) is typ-\nically greater than that of lower-resource languages\n(PanLex-BLI). In addition to the dictionary size,\nit is also worth investigating the quality of high-\nconfidence dictionaries. However, to directly eval-\nuate the quality of the ‘silver standard’ generated\ndictionaries is difficult since we do not have ground\ntruth dictionaries for comparison. As a preliminary\ninvestigation, we randomly sample 50 translation\npairs from the EN-DE LLAMA-213B-augmented dic-\ntionary and compare them with answers derived\nfrom Google Translate10 (EN→DE). We found that\n40 out of the 50 pairs in our augmented dictionary\nare the same as the results from Google Translate.\nAlthough these results from Google Translate are\nalso not ‘gold standard’ ground truth, it does point\nin the direction of reliability of extracted WT pairs.\nImpact of Nit. Figure 1 shows the influence of\nthe number of iterations Nit on the average BLI\nscores on XLING. When Nit = 1, where only step\nS1 is executed (see §2), SAIL already approaches\n(almost) its optimal performance. Further refining\nthe Dh for more iterations (step S2) only leads to\nsmall fluctuations in BLI performance, which we\ndeem not worth the increased computational cost.\nFigure 3 (Appendix B) with results on PanLex-BLI\nshows a similar trend.\nImpact of Nf. We then study the impact of the\nfrequency threshold Nf on the average BLI perfor-\nmance with a subset of XLING spanning DE-FR,\nEN-RU and RU-FR, each in both directions. The re-\nsults in Figure 2 reveal that even with Nf = 1, 000,\nthe BLI performance is boosted substantially when\ncompared against the ZERO-SHOT baseline (i.e.,\nwhen Nf = 0). When we further increase Nf, the\n10https://translate.google.com/\n0\n1\n2\n3\n4\nNit\n45\n50\n55\n60\nAccuracy×100%\nLLaMA-213B\nLLaMA13B\nLLaMA-27B\nLLaMA7B\nFigure 1: Top-1 accuracy (×100%) averaged over 20\nXLING BLI directions with respect to Nit. Nit = 0\nyields the ZERO-SHOT baseline.\n0\n1000 2000 3000 4000 5000 6000 7000 8000 9000 10000\nNf\n45\n50\n55\n60\nAccuracy×100%\nLLaMA-213B\nLLaMA-27B\nFigure 2: Top-1 accuracy on a subset of XLING with\nrespect to Nf. Nf = 0 yields the ZERO-SHOT baseline.\nLLM\nZERO-SHOT\nSAIL (w/o back-translation)\nSAIL\nLLAMA-27B\n45.36\n52.9\n56.12\nLLAMA-213B\n46.26\n55.1\n59.31\nTable 4: BLI results on XLING, demonstrating the use-\nfulness of back-translation when constructing Dh. Top-1\naccuracy (×100%) scores.\naccuracy score still increases slowly, and the gain\nseems negligible with Nf ≥5000: i.e., increasing\nNf again may not be worth the extra computation.\nImpact of Word Back-Translation. The back-\ntranslation step aims to improve the quality of Dh.\nHere, we experiment with the ablated version of\nSAIL without back-translation on the same XLING\nsubset (DE-FR, EN-RU and RU-FR) as before. The\nresults in Table 4 clearly demonstrate the effec-\ntiveness of proposed word back-translation: the\np-values (χ2 tests) are 8.8e-7 and 1.0e-10 respec-\ntively for LLAMA-27B and LLAMA-213B when com-\nparing SAIL variants with and without the back-\ntranslation mechanism.\nCHATGPT for BLI? We additionally report GPT-\n3.5 (OpenAI, 2022) and GPT-4 (Achiam et al.,\n2023) results on DE-FR, EN-RU and RU-FR with\nZERO-SHOT prompting (see Appendix E for ex-\nBLI Direction\nLLAMA-213B\nGPT-3.5\nGPT-4\nLLAMA-213B\nZERO-SHOT\nSAIL\nDE→FR\n46.64\n59.52\n62.6\n61.5\nFR→DE\n50.8\n58.41\n60.63\n56.29\nEN→RU\n47.6\n55.85\n55.9\n63.75\nRU→EN\n51.44\n59.93\n60.35\n59.93\nRU→FR\n41.17\n59.77\n61.39\n60.29\nFR→RU\n39.94\n46.82\n49.35\n54.11\nAvg.\n46.26\n56.72\n58.37\n59.31\nTable 5: Comparisons with GPT models.\nperimental details). Note that the procedure of\ninstruction-tuning of LLMs usually covers large-\nscale parallel data for machine translation. There-\nfore, leveraging CHATGPT models, even with ZERO-\nSHOT prompting, is not in line with the motivation\nof unsupervised BLI and leads to unfair compar-\nisons with the results of our main experiments and\nbaselines.11 Here, we report CHATGPT results as\nan upper bound for ZERO-SHOT prompting. Our\nresults in Table 5 show that 1) as expected, the\ninstruction-tuned CHATGPT models outperform pre-\ntrained LLAMA-213B by a large margin in the ZERO-\nSHOT setup, but 2) our SAIL method with the same\npretrained LLAMA-213B outperforms both GPT-3.5\nand the state-of-the-art GPT-412 in terms of the\naverage performance, even for the selected higher-\nresource languages, again demonstrating the effec-\ntiveness of the proposed SAIL approach.\n5\nConclusion\nWe proposed Self-Augmented In-Context Learning\n(SAIL) to improve unsupervised BLI with LLMs.\nThe key idea is to iteratively retrieve a set of high-\nconfidence word translation pairs by prompting\nLLMs and then leverage the retrieved pairs as in-\ncontext examples for unsupervised BLI. Our exper-\niments on two standard BLI benchmarks showed\nthat the proposed SAIL method substantially outper-\nforms established MAPPING-BASED and ZERO-SHOT\nBLI baselines. We also conducted a series of in-\ndepth analyses on the high-confidence dictionary,\nkey hyper-parameters, and the back-translation\nmechanism, and we additionally show that our\nSAIL approach with LLAMA-213B can even outper-\nform ZERO-SHOT prompting with the state-of-the-art\nGPT-4 model.\n11The four LLAMA models used in our main experi-\nments are pretrained LLMs without instruction-tuning (see\nAppendix D); our MAPPING-BASED baselines adopt static\nWEs derived from monolingual corpora of respective lan-\nguages and our CONTRASTIVEBLI (C2) baseline additionally\nleverages pretrained mBERT (Devlin et al., 2019).\n12We adopt the strong ‘gpt-4-turbo-2024-04-09’ model\nwhich ranked 1st on the LMSYS Chatbot Arena Leaderboard\nat the time of experimentation (May 12, 2024).\nLimitations\nThe main limitation of this work, inherited from\nprior work as well (Li et al., 2023) is that the\nscope of our languages is constrained to the lan-\nguages supported (or ‘seen’) by the underlying\nLLMs. For example, LLAMA-2 is reported to sup-\nport only around 27 natural languages (Touvron\net al., 2023b). This limitation could be mitigated if\nmore advanced LLMs that support more languages\nare available in the future. It might also be fea-\nsible to adapt existing LLMs to more languages\nby fine-tuning on their monolingual corpora poten-\ntially combined with modern cross-lingual transfer\nlearning techniques, whereas such adaptations of\nLLMs to unseen languages extend way beyond this\nwork focused on the BLI task.\nIn addition, compared to the ZERO-SHOT base-\nline, our SAIL framework organically requires more\ncomputational time and budget, as reported in Ta-\nble 7 of Appendix D.\nMoreover, the SAIL framework is proposed and\nevaluated for the unsupervised BLI task. This work\ndoes not discuss if and how adapted variants of\nSAIL could also be applied to other NLP tasks\nbeyond BLI. Further, the SAIL method should be\nequally applicable in weakly supervised BLI se-\ntups (Vuli´c et al., 2019) where a tiny set of available\nseed word translations (e.g., 50-500 word pairs)\ncan be assumed to seed the iterative procedure. We\nleave this to future work.\nAcknowledgements\nWe thank the anonymous reviewers for their valu-\nable feedback. Yaoyiran Li is supported by Grace\n& Thomas C. H. Chan Cambridge International\nScholarship. Anna Korhonen is supported by the\nUK Research and Innovation (UKRI) Frontier Re-\nsearch Grant EP/Y031350/1 (the UK government’s\nfunding guarantee for ERC Advanced Grants). Ivan\nVuli´c is supported by a personal Royal Society Uni-\nversity Research Fellowship Inclusive and Sustain-\nable Language Technology for a Truly Multilingual\nWorld’ (no 221137).\nReferences\nPrince Osei Aboagye, Jeff Phillips, Yan Zheng, Junpeng\nWang, Chin-Chia Michael Yeh, Wei Zhang, Liang\nWang, and Hao Yang. 2022. Normalization of lan-\nguage embeddings for cross-lingual alignment. In\nInternational Conference on Learning Representa-\ntions.\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama\nAhmad, Ilge Akkaya, Florencia Leoni Aleman,\nDiogo Almeida, Janko Altenschmidt, Sam Altman,\nShyamal Anadkat, et al. 2023. Gpt-4 technical report.\narXiv preprint arXiv:2303.08774.\nAnonymous. 2023. Dm-bli: Dynamic multiple sub-\nspaces alignment for unsupervised bilingual lexicon\ninduction. OpenReview Preprint.\nMikel Artetxe, Gorka Labaka, and Eneko Agirre. 2018a.\nA robust self-learning method for fully unsupervised\ncross-lingual mappings of word embeddings. In Pro-\nceedings of the 56th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 789–798, Melbourne, Australia. As-\nsociation for Computational Linguistics.\nMikel Artetxe, Gorka Labaka, Eneko Agirre, and\nKyunghyun Cho. 2018b. Unsupervised neural ma-\nchine translation. In International Conference on\nLearning Representations.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching word vectors with\nsubword information. Transactions of the Associa-\ntion for Computational Linguistics, 5:135–146.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners.\nIn Ad-\nvances in Neural Information Processing Systems,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nAlexandra Chronopoulou, Dario Stojanovski, and\nAlexander Fraser. 2021. Improving the lexical abil-\nity of pretrained language models for unsupervised\nneural machine translation. In Proceedings of the\n2021 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 173–180, Online.\nAssociation for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nMarjan Ghazvininejad, Hila Gonen, and Luke Zettle-\nmoyer. 2023. Dictionary-based phrase-level prompt-\ning of large language models for machine translation.\narXiv preprint arXiv:2302.07856.\nGoran Glavaš, Robert Litschko, Sebastian Ruder, and\nIvan Vuli´c. 2019. How to (properly) evaluate cross-\nlingual word embeddings: On strong baselines, com-\nparative analyses, and some misconceptions. In Pro-\nceedings of the 57th Annual Meeting of the Associa-\ntion for Computational Linguistics, pages 710–721,\nFlorence, Italy. Association for Computational Lin-\nguistics.\nEdouard Grave, Armand Joulin, and Quentin Berthet.\n2019. Unsupervised alignment of embeddings with\nwasserstein procrustes. In Proceedings of the Twenty-\nSecond International Conference on Artificial Intel-\nligence and Statistics, volume 89 of Proceedings\nof Machine Learning Research, pages 1880–1890.\nPMLR.\nAlex Jones, Isaac Caswell, Ishank Saxena, and Orhan\nFirat. 2023. Bilex rx: Lexical data augmentation for\nmassively multilingual machine translation. arXiv\npreprint arXiv:2303.15265.\nEnkelejda Kasneci, Kathrin Sessler, Stefan Küche-\nmann, Maria Bannert, Daryna Dementieva, Frank\nFischer, Urs Gasser, Georg Groh, Stephan Günne-\nmann, Eyke Hüllermeier, Stephan Krusche, Gitta\nKutyniok, Tilman Michaeli, Claudia Nerdel, Jür-\ngen Pfeffer, Oleksandra Poquet, Michael Sailer, Al-\nbrecht Schmidt, Tina Seidel, Matthias Stadler, Jochen\nWeller, Jochen Kuhn, and Gjergji Kasneci. 2023.\nChatgpt for good? on opportunities and challenges of\nlarge language models for education. Learning and\nIndividual Differences, 103:102274.\nGuillaume Lample, Alexis Conneau, Marc’Aurelio Ran-\nzato, Ludovic Denoyer, and Hervé Jégou. 2018.\nWord translation without parallel data. In Proceed-\nings of the International Conference on Learning\nRepresentations.\nYaoyiran Li, Anna Korhonen, and Ivan Vuli´c. 2023.\nOn bilingual lexicon induction with large language\nmodels. In Proceedings of the 2023 Conference on\nEmpirical Methods in Natural Language Processing,\npages 9577–9599, Singapore. Association for Com-\nputational Linguistics.\nYaoyiran Li, Fangyu Liu, Nigel Collier, Anna Korhonen,\nand Ivan Vuli´c. 2022a. Improving word translation\nvia two-stage contrastive learning. In Proceedings\nof the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 4353–4374, Dublin, Ireland. Association for\nComputational Linguistics.\nYaoyiran Li, Fangyu Liu, Ivan Vuli´c, and Anna Korho-\nnen. 2022b. Improving bilingual lexicon induction\nwith cross-encoder reranking. In Findings of the As-\nsociation for Computational Linguistics: EMNLP\n2022, pages 4100–4116, Abu Dhabi, United Arab\nEmirates. Association for Computational Linguistics.\nYaoyiran Li, Xiang Zhai, Moustafa Alzantot, Keyi Yu,\nIvan Vuli´c, Anna Korhonen, and Mohamed Hammad.\n2024. Calrec: Contrastive alignment of generative\nllms for sequential recommendation. arXiv preprint\narXiv:2405.02429.\nSasha Luccioni, Victor Schmidt, Alexandre Lacoste,\nand Thomas Dandres. 2019. Quantifying the carbon\nemissions of machine learning. In NeurIPS 2019\nWorkshop on Tackling Climate Change with Machine\nLearning.\nKelly Marchisio, Kevin Duh, and Philipp Koehn. 2020.\nWhen does unsupervised machine translation work?\nIn Proceedings of the Fifth Conference on Machine\nTranslation, pages 571–583, Online. Association for\nComputational Linguistics.\nOpenAI. 2022. Openai: Introducing chatgpt. URL\nhttps://openai.com/blog/chatgpt.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul F Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback. In Advances in Neural Information\nProcessing Systems, volume 35, pages 27730–27744.\nCurran Associates, Inc.\nBarun Patra, Joel Ruben Antony Moniz, Sarthak Garg,\nMatthew R. Gormley, and Graham Neubig. 2019.\nBilingual lexicon induction with semi-supervision\nin non-isometric embedding spaces. In Proceedings\nof the 57th Annual Meeting of the Association for\nComputational Linguistics, pages 184–193, Florence,\nItaly. Association for Computational Linguistics.\nVin Sachidananda, Ziyi Yang, and Chenguang Zhu.\n2021. Filtered inner product projection for crosslin-\ngual embedding alignment. In International Confer-\nence on Learning Representations.\nJimin Sun, Hwijeen Ahn, Chan Young Park, Yulia\nTsvetkov, and David R. Mortensen. 2021. Cross-\ncultural similarity features for cross-lingual transfer\nlearning of pragmatically motivated tasks. In Pro-\nceedings of the 16th Conference of the European\nChapter of the Association for Computational Lin-\nguistics: Main Volume, pages 2403–2414, Online.\nAssociation for Computational Linguistics.\nArun James Thirunavukarasu, Darren Shu Jeng Ting,\nKabilan Elangovan, Laura Gutierrez, Ting Fang Tan,\nand Daniel Shu Wei Ting. 2023. Large language\nmodels in medicine. Nature medicine, 29(8):1930–\n1940.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, et al. 2023a.\nLlama:\nOpen and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023b.\nLlama 2: Open founda-\ntion and fine-tuned chat models.\narXiv preprint\narXiv:2307.09288.\nIvan Vuli´c, Goran Glavaš, Fangyu Liu, Nigel Collier,\nEdoardo Maria Ponti, and Anna Korhonen. 2023.\nProbing cross-lingual lexical knowledge from mul-\ntilingual sentence encoders. In Proceedings of the\n17th Conference of the European Chapter of the As-\nsociation for Computational Linguistics, pages 2089–\n2105, Dubrovnik, Croatia. Association for Computa-\ntional Linguistics.\nIvan Vuli´c, Goran Glavaš, Roi Reichart, and Anna Ko-\nrhonen. 2019. Do we really need fully unsupervised\ncross-lingual embeddings?\nIn Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 4407–4418, Hong Kong,\nChina. Association for Computational Linguistics.\nIvan Vuli´c, Edoardo Maria Ponti, Robert Litschko,\nGoran Glavaš, and Anna Korhonen. 2020. Probing\npretrained language models for lexical semantics. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 7222–7240, Online. Association for Computa-\ntional Linguistics.\nXinyi Wang, Sebastian Ruder, and Graham Neubig.\n2022. Expanding pretrained models to thousands\nmore languages via lexicon-based adaptation. In Pro-\nceedings of the 60th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 863–877, Dublin, Ireland. Association\nfor Computational Linguistics.\nZongxiao Wu, Yizhe Dong, Yaoyiran Li, and Baofeng\nShi. 2023. Unleashing the power of text for credit\ndefault prediction: Comparing human-generated and\nai-generated texts. Available at SSRN 4601317.\nShenglong Yu, Wenya Guo, Ying Zhang, and Xiaojie\nYuan. 2023. Cd-bli: Confidence-based dual refine-\nment for unsupervised bilingual lexicon induction. In\nNatural Language Processing and Chinese Comput-\ning, pages 379–391, Cham. Springer Nature Switzer-\nland.\nYucheng Zhou, Xiubo Geng, Tao Shen, Wenqiang\nZhang, and Daxin Jiang. 2021. Improving zero-shot\ncross-lingual transfer for multilingual question an-\nswering over knowledge graph. In Proceedings of\nthe 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 5822–5834,\nOnline. Association for Computational Linguistics.\nA\nLanguages\nFamily\nLanguage\nCode\nGermanic\nEnglish\nEN\nGerman\nDE\nRomance\nCatalan\nCA\nFrench\nFR\nItalian\nIT\nSlavic\nBulgarian\nBG\nRussian\nRU\nUralic\nHungarian\nHU\nTable 6: Languages used in our experiments with their\nISO 639-1 codes.\nB\nImpact of Nit with PanLex-BLI\n0\n1\n2\n3\n4\nNit\n25\n30\n35\n40\n45\nAccuracy×100%\nLLaMA-213B\nLLaMA13B\nLLaMA-27B\nLLaMA7B\nFigure 3: Top-1 accuracy (×100%) averaged over 6\nPanLex-BLI BLI directions with respect to Nit. Nit =\n0 yields the ZERO-SHOT baseline.\nC\nTemplates\nLi et al. (2023) provide the suggested (carefully\nsearched) templates for LLAMA 7B and LLAMA 13B,\nwhich we directly adopt in our work. For LLAMA-\n27B and LLAMA-213B, we conduct template search\nfollowing Li et al. (2023) on a single language pair\nDE-FR in both directions. For CHATGPT models\nused in §4.2, details about their templates are pro-\nvided in Appendix E.\nZero-Shot Template. LLAMA 7B, LLAMA-27B and\nLLAMA-213B share the same zero-shot template as\nintroduced in §2. LLAMA 13B’s zero-shot template\nis as follows:\n‘Translate from Lx to Ly: wx=>’.\nFew-Shot Template.. We have introduced the few-\nshot template of LLAMA-27B in §2. The remaining\nthree LLMs happen to share the same few-shot\ntemplate, given as follows:\n‘The Lx word 'wx\n1' in Ly is wy\n1. The\nLx word 'wx\n2' in Ly is wy\n2. ... The Lx\nword 'wx' in Ly is’.\nD\nReproducibility Checklist\n• Source Code: our code is publicly available at\nhttps://github.com/cambridgeltl/sail-b\nli.\n• Hyper-Parameter Search: Nit is selected from\n{1, 2, 3, 4} and Nf from {1000, 2000, 3000, 4000,\n5000, 6000, 7000, 8000, 9000, 10000}.\n• Software: Python 3.9.7, PyTorch 1.10.1, Trans-\nformers 4.28.1, OpenAI 1.28.1.\n• Computing Infrastructure: we run our codes\non Wilkes3, a GPU cluster hosted by the University\nof Cambridge. Each run makes use of a single\nNvidia 80GB A100 GPU and 32× CPU cores.\n• Half-Precision Floating-Point Format: as in-\ntroduced in §3, our BLI inference relies on\ntorch.float16 for both our SAIL and the ZERO-\nSHOT baseline. We have verified that fp16 can\naccelerate our computation with only negligible\nimpact on the absolute BLI performance. Note that\nLi et al. (2023) did not specify torch.float16 in\ntheir ZERO-SHOT experiments with LLAMA 7B and\nLLAMA 13B, so the BLI scores reported are slightly\ndifferent from ours.\n• Data, WEs, LLMs: all the BLI data, WEs,\nLLMs (excluding CHATGPT models) and baseline\ncodes are open-source and publicly available. The\nWEs for retrieving in-context examples are fastText\nWEs (Bojanowski et al., 2017) trained on monolin-\ngual corpora of respective languages: the version\npretrained on Wikipedia13 is used for XLING and\nthe version pretrained with Wikipedia plus Com-\nmon Crawl14 is used for PanLex-BLI, as recom-\nmended by XLING and PanLex-BLI, respectively.\nThe same WEs are used for our MAPPING-BASED\nbaselines.\nThe LLMs used in our main exper-\niments (LLAMA models) are summarised in Ta-\nble 7. Note that we only adopt pretrained versions\nof LLAMA (e.g., ‘meta-llama/Llama-2-7b-hf’)\nrather than the instruction-tuned models (e.g.,\n‘meta-llama/Llama-2-7b-chat-hf’).\nThe de-\ntails of CHATGPT models used in §4.2 are provided\nin Appendix E.\n13https://fasttext.cc/docs/en/pretrained-vecto\nrs.html\n14https://fasttext.cc/docs/en/crawl-vectors.h\ntml\n• Baselines: for every baseline, we use its rec-\nommended setup for unsupervised BLI and make\nsure the recommended setup achieves its own (near-\n)optimal performance. As introduced in §3, we\nextend CONTRASTIVEBLI to the unsupervised BLI\nsetup. Specifically, we adopt the set of its hyper-\nparameters recommended for the weakly super-\nvised BLI setup, which we found can also achieve\nstrong unsupervised BLI performance.\n• Parameter Count and Runtime: we report the\nnumber of parameters of each LLM and the GPU\nruntime for BLI inference on a single BLI direction\nDE→FR, which contains circa 2K word pairs, in\nTable 7.\n• Carbon Footprint: our work consumes about\n750 A100 GPU hours in total. We estimate that\nour experiments causes the emission of circa 90kg\nCO2 equivalents according to a publicly available\n‘machine learning emissions calculator’ (Luccioni\net al., 2019)15.\nE\nDetails of CHATGPT Experiments\nWe run our CHATGPT experiments introduced\nin §4.2 with the OpenAI API.16 The model\nID\nfor\nGPT-3.5\nis\n‘gpt-3.5-turbo-0125’.\nFor\nGPT-4,\nwe\nadopt\nthe\nstate-of-the-art\n‘gpt-4-turbo-2024-04-09’ model which ranked\n1st on the LMSYS Chatbot Arena Leaderboard at\nthe time of experimentation (May 12, 2024).\nOur input to CHATGPT consists of two types of\ninput messages: a system message followed by a\nuser message. For the user message, we adopt the\nfollowing template for both GPT-3.5 and GPT-4 as\nrecommended in Anonymous (2023):\n‘Translate the Lx word wx into Ly:’,\nwhich is also selected from the template pool of Li\net al. (2023). We additionally adopt the following\nsystem message which is not used in Anonymous\n(2023) or Li et al. (2023):\n‘Please complete the following\nsentence and only output the target\nword.’.\nIn our preliminary investigation, we find that our\nsystem message can considerably improve the BLI\nperformance of both CHATGPT models.\n15https://mlco2.github.io/impact/#compute\n16https://platform.openai.com/docs/overview\nThere are two hyper-parameters used in our API\ncalls: temperature = 0 and max_tokens = 5.\nLike our main experiments, we also extract the\nfirst word in the generated output sequence as the\nprediction for the target word. But different from\nour LLAMA experiments, we only derive a single\noutput sequence from the CHATGPT API for each\nprompt. The code for our CHATGPT experiments is\nalso provided in our GitHub repository.\nF\nFull BLI Results\nTable 8 shows detailed BLI scores for each BLI di-\nrection in the XLING dataset. Similarly, individual\nper-direction results on PanLex-BLI are presented\nin Table 9.\nLLM\nModel ID\nParameter Count\nRuntime: ZERO-SHOT\nRuntime: SAIL\nLLAMA 7B\n‘huggyllama/llama-7b’\n6, 738, 415, 616\n5 min\n40 min\nLLAMA-27B\n‘meta-llama/Llama-2-7b-hf’\n6, 738, 415, 616\n5 min\n40 min\nLLAMA 13B\n‘huggyllama/llama-13b’\n13, 015, 864, 320\n6 min\n49 min\nLLAMA-213B\n‘meta-llama/Llama-2-13b-hf’\n13, 015, 864, 320\n6 min\n49 min\nTable 7: LLMs adopted in our work with their huggingface.co model IDs, parameter count, and GPU runtime on\na single BLI direction for ZERO-SHOT prompting and SAIL respectively.\n[Unsupervised BLI]\nVECMAP\nCONTRASTIVEBLI (C1)\nCONTRASTIVEBLI (C2)\nLLAMA 7B\nLLAMA-27B\nLLAMA 13B\nLLAMA-213B\nLLAMA 7B\nLLAMA-27B\nLLAMA 13B\nLLAMA-213B\nMAPPING-BASED\nZERO-SHOT\nSAIL (Ours)\nDE→FR\n48.98\n50.39\n51.8\n42.46\n44.44\n47.37\n46.64\n54.67\n54.77\n58.37\n61.5\nFR→DE\n43.97\n43.61\n44.9\n43.2\n45.47\n48.11\n50.8\n50.08\n54.16\n54.47\n56.29\nDE→IT\n48.41\n49.77\n50.23\n42.78\n42.78\n46.06\n48.51\n53.36\n54.25\n57.38\n59.05\nIT→DE\n44.03\n43.93\n45.43\n38.6\n41.55\n44.39\n45.27\n46.15\n51.63\n52.2\n52.92\nDE→RU\n25.67\n28.22\n31.09\n30.41\n35.32\n32.76\n36.62\n45.12\n46.9\n48.98\n51.59\nRU→DE\n39.13\n40.02\n41.33\n43.53\n44.68\n43.11\n42.12\n46.83\n50.55\n50.65\n53.9\nEN→DE\n48.4\n47.45\n47.4\n52.0\n52.1\n54.35\n59.85\n59.55\n61.75\n62.8\n65.05\nDE→EN\n54.51\n54.36\n55.97\n42.57\n44.91\n46.95\n47.16\n55.35\n56.44\n57.96\n61.24\nEN→FR\n60.15\n61.05\n61.25\n57.6\n62.65\n62.65\n61.75\n72.6\n73.8\n75.85\n76.35\nFR→EN\n61.25\n62.34\n63.58\n54.58\n55.56\n57.27\n53.03\n63.68\n65.13\n65.29\n66.63\nEN→IT\n57.4\n57.6\n58.75\n58.95\n60.85\n60.4\n65.8\n71.7\n73.0\n74.25\n77.6\nIT→EN\n60.83\n62.02\n63.46\n47.39\n50.08\n54.94\n53.54\n60.1\n64.08\n64.13\n65.43\nEN→RU\n24.55\n25.45\n26.1\n42.05\n44.6\n40.1\n47.6\n57.4\n60.25\n61.05\n63.75\nRU→EN\n46.52\n46.67\n50.03\n46.15\n50.81\n50.13\n51.44\n54.95\n58.51\n57.41\n59.93\nIT→FR\n64.75\n65.12\n65.89\n51.42\n54.47\n57.36\n55.3\n61.91\n65.58\n65.94\n68.17\nFR→IT\n63.37\n63.94\n64.61\n57.32\n55.98\n60.01\n61.87\n64.72\n66.22\n69.22\n69.53\nRU→FR\n45.31\n46.78\n47.93\n43.58\n48.04\n47.77\n41.17\n54.79\n57.62\n57.52\n60.29\nFR→RU\n24.26\n25.09\n26.07\n35.8\n38.8\n38.59\n39.94\n48.94\n51.42\n53.29\n54.11\nRU→IT\n43.95\n44.89\n46.15\n47.3\n47.15\n45.99\n49.45\n53.54\n56.26\n56.31\n59.25\nIT→RU\n25.48\n26.87\n29.35\n31.52\n33.02\n35.45\n36.38\n44.03\n48.63\n50.75\n53.49\nAvg.\n46.55\n47.28\n48.57\n45.46\n47.66\n48.69\n49.71\n55.97\n58.55\n59.69\n61.8\nTable 8: Full BLI results on 20 XLING BLI directions.\n[Unsupervised BLI]\nVECMAP\nCONTRASTIVEBLI (C1)\nCONTRASTIVEBLI (C2)\nLLAMA 7B\nLLAMA-27B\nLLAMA 13B\nLLAMA-213B\nLLAMA 7B\nLLAMA-27B\nLLAMA 13B\nLLAMA-213B\nMAPPING-BASED\nZERO-SHOT\nSAIL (Ours)\nBG→CA\n39.6\n38.08\n39.66\n32.83\n29.79\n32.77\n33.47\n40.19\n42.23\n42.52\n47.9\nCA→HU\n34.09\n34.2\n36.85\n23.7\n23.2\n24.42\n30.17\n32.27\n35.25\n38.34\n39.83\nHU→BG\n36.46\n38.36\n40.44\n28.28\n27.71\n26.5\n26.73\n38.19\n41.47\n43.89\n46.66\nCA→BG\n33.6\n31.39\n33.94\n26.35\n27.2\n27.03\n28.39\n36.54\n38.47\n42.27\n45.67\nHU→CA\n37.79\n39.77\n43.45\n32.62\n28.66\n38.23\n37.51\n41.53\n46.09\n47.91\n51.65\nBG→HU\n39.24\n38.95\n41.44\n24.13\n28.12\n23.67\n27.72\n33.16\n38.08\n38.14\n41.38\nAvg.\n36.8\n36.79\n39.3\n27.98\n27.45\n28.77\n30.66\n36.98\n40.27\n42.18\n45.51\nTable 9: Full BLI results on 6 PanLex-BLI BLI directions.\n",
  "categories": [
    "cs.CL",
    "cs.AI",
    "cs.IR",
    "cs.LG"
  ],
  "published": "2024-02-15",
  "updated": "2024-06-05"
}