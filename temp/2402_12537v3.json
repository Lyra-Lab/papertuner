{
  "id": "http://arxiv.org/abs/2402.12537v3",
  "title": "ADEPT: Hierarchical Bayes Approach to Personalized Federated Unsupervised Learning",
  "authors": [
    "Kaan Ozkara",
    "Bruce Huang",
    "Ruida Zhou",
    "Suhas Diggavi"
  ],
  "abstract": "Statistical heterogeneity of clients' local data is an important\ncharacteristic in federated learning, motivating personalized algorithms\ntailored to the local data statistics. Though there has been a plethora of\nalgorithms proposed for personalized supervised learning, discovering the\nstructure of local data through personalized unsupervised learning is less\nexplored. We initiate a systematic study of such personalized unsupervised\nlearning by developing algorithms based on optimization criteria inspired by a\nhierarchical Bayesian statistical framework. We develop adaptive algorithms\nthat discover the balance between using limited local data and collaborative\ninformation. We do this in the context of two unsupervised learning tasks:\npersonalized dimensionality reduction and personalized diffusion models. We\ndevelop convergence analyses for our adaptive algorithms which illustrate the\ndependence on problem parameters (e.g., heterogeneity, local sample size). We\nalso develop a theoretical framework for personalized diffusion models, which\nshows the benefits of collaboration even under heterogeneity. We finally\nevaluate our proposed algorithms using synthetic and real data, demonstrating\nthe effective sample amplification for personalized tasks, induced through\ncollaboration, despite data heterogeneity.",
  "text": "Hierarchical Bayes Approach to Personalized Federated\nUnsupervised Learning\nKaan Ozkara\nUniversity of California, Los Angeles\nkaan@ucla.edu\nBruce Huang\nUniversity of California, Los Angeles\nbrucehuang@ucla.edu\nRuida Zhou\nUniversity of California, Los Angeles\nruida@ucla.edu\nSuhas Diggavi\nUniversity of California, Los Angeles\nsuhas@ee.ucla.edu\nAbstract\nStatistical heterogeneity of clients’ local data is an important characteristic in federated learning,\nmotivating personalized algorithms tailored to the local data statistics. Though there has been a plethora\nof algorithms proposed for personalized supervised learning, discovering the structure of local data through\npersonalized unsupervised learning is less explored. We initiate a systematic study of such personalized\nunsupervised learning by developing algorithms based on optimization criteria inspired by a hierarchical\nBayesian statistical framework. We develop adaptive algorithms that discover the balance between\nusing limited local data and collaborative information. We do this in the context of two unsupervised\nlearning tasks: personalized dimensionality reduction and personalized diffusion models. We develop\nconvergence analyses for our adaptive algorithms which illustrate the dependence on problem parameters\n(e.g., heterogeneity, local sample size). We also develop a theoretical framework for personalized diffusion\nmodels, which shows the benefits of collaboration even under heterogeneity. We finally evaluate our\nproposed algorithms using synthetic and real data, demonstrating the effective sample amplification for\npersonalized tasks, induced through collaboration, despite data heterogeneity.\n1\nIntroduction\nOne of the goals of unsupervised learning is to discover the underlying structure in data and use this for\ntasks such as dimensionality reduction and generating new samples from the data distribution. We might\nwant to perform this task on local data of a client, e.g., data collected from personal sensors or other\ndevices; such data could have heterogeneous statistics across clients. The desired personalized task should\nbe tailored to the particular distribution of the local data, and hence to discover this structure one might\nneed a significant amount of local data. There might be insufficient local samples for the task, motivating\ncollaboration between clients. Moreover, as argued in the federated learning (FL) paradigm, we would like to\nleverage data across clients without explicit data sharing [26, 16]. In this paper, we initiate a systematic\nstudy of personalized federated unsupervised learning, where clients collaborate to discover personalized\nstructure in their (heterogeneous) local data.\nThere has been a plethora of personalized learning models proposed in the literature, mostly for supervised\nfederated learning [11, 8, 24, 30, 28]. These methods were motivated by the statistical heterogeneity of local\ndata, causing a single “global” model to perform poorly on local data. The different personalized federated\nsupervised learning algorithms were unified in [28] using an empirical/hierarchical Bayes statistical model\n[10], which also suggested new supervised learning algorithms. However, there has been much less work\non personalized federated unsupervised learning. We will build on the statistical approach studied in [28]\nfor supervised learning, applying it to personalized unsupervised learning. This leads to new federated\n1\narXiv:2402.12537v3  [cs.LG]  24 Jan 2025\nalgorithms for personalized dimensionality reduction and personalized diffusion-based generative models for\nheterogeneous data.\nA question is how to use local data and learn from others despite heterogeneity. The hierarchical Bayes\nframework [10] suggests using an estimated population distribution effectively as a prior. However, the\nchallenge is to efficiently estimate it, enabling each client to combine local data and collaborative information\nfor the unsupervised learning task. We do this by simultaneously learning a global model (a proxy for the\npopulation model) and learning a local model by adaptively estimating its discrepancy to balance the global\nand local information. In Section 2, we define this through a loss function, making it amenable to a standard\ndistributed gradient descent approach. Our main contributions are as follows.\nUnsupervised collaboration learning criteria: Section 2 develops and uses the hierarchical Bayes\nframework for personalized dimensionality reduction and personalized (generative) diffusion models. We\ndevelop an Adaptive Distributed Emperical-Bayes based Personalized Training (ADEPT) criterion which embeds\nthe balance between local data and collaboration for these tasks (see below). As far as we are aware, these\nare the first explicit criteria for such personalized federated unsupervised learning tasks.\nPersonalized dimensionality reduction: Section 3 develops adaptive personalized algorithms for linear\n(PCA) (ADEPT-PCA) and non-linear (auto-encoders) (ADEPT-AE) for dimensionality reduction.\nWe also\ndemonstrate its convergence in Theorems 3.3 and 3.12.\nIn Remark 3.13, we see that these allow us\nto theoretically examine the impact of heterogeneity, number of local samples and number of clients,\non optimization performance. We believe that these are the first adaptive algorithms for personalized\ndimensionality reduction and their convergence analyses. Finally in Section 5 1, we evaluate ADEPT-PCA and\nADEPT-AE on synthetic and real data showing the benefits of adaptive collaboration. For example, Table 2\nshows effective amplification of as much as 20× in local sample complexity through collaboration.\nPersonalized diffusion models: Section 4 develops an adaptive personalized diffusion generative model\n(ADEPT-DGM) to generate novel samples for local data statistics. We believe that ADEPT-DGM is the first\nalgorithm for federated personalized diffusion. We develop a theory for such personalized federated diffusion\nmodels through our statistical framework, and use this to demonstrate, in Theorem 4.5, conditions when\ncollaboration can improve performance, despite statistical heterogeneity. Finally in Section 5, we evaluate\nADEPT-DGM, demonstrating the value of adaptive collaboration despite heterogeneity, as well as significant\nperformance benefits for “worst” clients.\nRelated work: As mentioned earlier, there have been several recent works on personalized supervised learning\n(see next paragraph). There has been much less attention given to personalized federated unsupervised\nlearning. The closest work to ours on personalized dimensionality reduction is [35, 29] which study personalized\nPCA algorithms. [35] has a restrictive assumption that principal components for global and local models are\nnon-overlapping. [29] uses the hierarchical Bayes statistical model to develop a criterion for personalized PCA;\nhowever, the authors assume heterogeneity of the setting is known; in contrast, ADEPT-PCA learns and adapts\nthe solution accordingly. There is some literature on specific tasks such as training recommender systems\n[21, 32], grouping clients based on latent representations [44], generating data to improve performance of\npersonalized supervised learning [3, 31]. However, our approach to developing personalized dimensionality\nreduction for heterogeneous data is distinct from them. We are not aware of any FL work for personalized\ndiffusion generative models. There is work using pre-trained diffusion models and then fine-tuning them\n[34, 46, 27, 23]. However, these do not fit into the federated learning paradigm, and require data collection\nfrom clients to obtain the pre-trained model. One way to view our approach is to simultaneously build such\n“global” models (akin to pre-trained models) and individual (personalized) models, while not sharing data.\nBeyond these above works, some more related works are as follows: Personalized Federated Learning (FL) has\nseen recent advances with diverse approaches for learning personalized models. These approaches encompass\nmeta-learning-based methods [11, 1, 17], regularization techniques [7, 24, 13], clustered FL [47, 24, 12, 25],\nknowledge distillation strategies [22, 30], multi-task learning [8, 36, 43, 45], and the utilization of common\nrepresentations [9, 40, 6]. Additionally, recently there have been works on using a hierarchical Bayesian view to\nderive novel personalized supervised FL algorithms [28, 4, 20]. Adaptation is also considered in [28, 4]. In [4],\nvariance estimation is performed for supervised learning to estimate the heterogeneity within the local models\nand the estimated variance is used to form an initialization for each local model to be trained on. However,\nif we apply variance estimation to our criterion, we observe an early stopping issue during the training. In\n1Our code is available at https://github.com/kazkara/adept.\n2\nFigure 1: Hierarchical Bayesian model of data distribution\ncontrast, ours is based on a standard gradient descent. Moreover, they do not examine convergence which we\ndo in our unsupervised algorithms. In [28], the authors consider an adaptation for supervised learning based\non the KL-divergence criterion and do not have hyper prior. Moreover, a convergence analysis is not explored\nin their algorithm. In that case, the σ is inclined to smaller values or even vanishes during the training,\nand our ADEPT criterion resolves the issue by introducing the hyper prior. [42] investigated local features of\nglobally trained diffusion models through FedAvg, but they do not consider personalized generation.\n2\nProblem Formulation\nWe present a hierarchical Bayesian framework for the personalized unsupervised learning, using it to develop\noptimization criteria for federated personalized dimensionality reduction and personalized diffusion models.\nFirst we state some preliminaries for notation. For the notations used in this paper:\n• We use bold lowercase letters (such as u, v) to denote vectors and we use bold uppercase letters (such\nas U, V ) to denote matrices.\n• Given a composite function f(u, v), we denote ∇f(u, v) or ∇(u,v)f(u, v) as the gradient; ∇uf(u, v)\nand ∇vf(u, v) as the partial gradients with respect to u and v.\n• For a vector u, ∥u∥denotes the ℓ2-norm ∥u∥2. For a matrix U, ∥U∥2 and ∥U∥op both denote the\nℓ2-norm (operator norm) of the matrix and ∥U∥, ∥U∥F denotes the Frobenius norm of the matrix.\n• We use {U i}m\ni=1 or {U i} (when the context is clear) to denote the collection {U1, . . . , Um}. When there\nare multiple indices, we use {U i,t}i := {U 1,t, . . . , U m,t} to denote the collection over index i.\n2.1\nHierarchical Bayes for personalized learning\nThe statistical model based on hierarchical Bayes, suitable for federated unsupervised learning, is illustrated\nin Figure 1. There are m clients. Each client i is associated with a parameter θi and has a local dataset Xi\nconsisting of n data points (xi1, . . . , xin) i.i.d. from distribution p(x|θi). The parameters obey a population\ndistribution (a.k.a. prior distribution in the Bayesian model) p(θ|Γ) parameterized by Γ. We have a (carefully\ndesigned) hyper prior distribution π over Γ, i.e., Γ ∼π to prevent ill-posedness. This statistical model defines\nthe joint distribution\npΓ,{θi},{Xi}(Γ, θ1, . . . , θm, X1, . . . , Xm) = π(Γ)\nm\nY\ni=1\np(θi|Γ)\nm\nY\ni=1\nn\nY\nj=1\np(xij|θi).\n(1)\n3\nWe learn the distribution parameters Γ, {θi} from data {Xi} by maximizing the joint distribution (a.k.a.\nmaximum a posteriori), by minimizing the ADEPT loss function:\nf({θi}, Γ; {Xi}) = 1\nm\nm\nX\ni=1\nfi(θi; Xi) + R({θi}, Γ),\nwhere fi(θi; Xi) := −Pn\nj=1 log(p(xij|θi)) is the local loss function for client i reflecting the likelihood\nof the local dataset, and R({θi}, Γ) = −1\nm log π(Γ) −1\nm\nPm\ni=1 log p(θi|Γ) is a regularization allowing the\ncollaboration among clients. When the likelihood function is not easy to optimize, we leverage surrogates\nsuch as evidence lower bound (ELBO) instead.\nIn this work, we focus on a Gaussian population distribution over an dθ-dimensional normed metric space\n(Θ, ∥· ∥)2. Specifically, Γ = (µ, σ), the parameters θ, µ ∈Θ and p(θ|Γ) =\n1\n(2πσ2)dθ/2 exp(−∥θ−µ∥2\n2σ2\n), where d is\nthe dimensionality of θ and µ has the same dimension as θ. We assume an improper (non-informative) hyper\nprior π over Γ = (µ, σ), as µ ∼N(0, ∞· Idθ) and σ2 follows an inverse gamma distribution parameterized by\na hyper-parameter ξ, i.e., π(µ, σ) ∝exp( mξ\nσ2 ) 3. We thus have the regularization\nR({θi}, Γ) = −1\nm log π(Γ) −1\nm\nm\nX\ni=1\nlog p(θi|Γ) = 1\nm\nm\nX\ni=1\n2ξ + ∥µ −θi∥2\n2σ2\n+ dθ log σ.\n(2)\n2.2\nPersonalized Dimensionality Reduction\nLinear Dimensionality Reduction:\nThe linear dimensionality reduction is equivalent to PCA. We\nextend a personalized PCA formulation that was previously studied in [29] by introducing adaptivity i.e.\noptimizing the loss over σ. In this setting, the dataset from client i is Xi ∈Rd×n containing n samples of d\ndimensional vectors. Let us denote Si = 1\nnXiX⊤\ni as the sample covariance matrix of client i. For notational\nconsistency with canonical PCA notation, we set the parameters θi = U i ∈Rd×r. Similar to [29], we adopt\nthe probabilistic view of PCA [41],\nxij = U izij + ϵij,\n(3)\nwhere zi1, . . . , zin\ni.i.d.\n∼\nN(0, Ir) and ϵi1, . . . , ϵin\ni.i.d.\n∼\nN(0, σ2\nϵ Id). This results in the likelihood function\np(Xi|θ = U i) = N(0, U iU ⊤\ni + σ2\nϵ I). Recall that the prior parameter is Γ = (µ, σ), here for notational\nconsistency we use V = µ. The underlying metric space (Θ, ∥· ∥) containing the parameters U and V in\nthe Steifel manifold where St(d, r) := {U ∈Rd×r|U ⊤U = I}. The metric is d(V , U) = ∥PTV (U)∥where\nPTV (U) = U −1\n2V (V ⊤U + U ⊤V ) is the projection of U onto the tangent space at V . Because computing\nthe (geodesic) distance on St(d, r) is hard, the defined metric first projects a matrix to the tangent space\nof another matrix and then computes the Frobenius norm. The personalized unsupervised learning is then\nminimizing the ADEPT-PCA loss function:\nmin\n{U i},V ,σ f pca({U i}, V , σ; {Xi}) := 1\nm\n\u0010 m\nX\ni=1\nn\n2 (log(|W i|) + tr(W −1\ni Si)) + 2ξ + d2(V , U i)\n2σ2\n\u0011\n+ dθ log σ\n(4)\ns.t. V ⊤V = I, U ⊤\ni U i = I\n∀i ∈[m]; where W i = (U iU ⊤\ni + σ2\nϵ I).\nNon-linear dimensionality reduction:\nWhile PCA is a good starting point for dimensionality reduction,\nit cannot capture non-linear relations between the latent variable and the observed space. Hence, one can\nextend (3) to model non-linearity as follows,\nXij = ψθd\ni (zij) + ϵij,\n(5)\n2Note that this general framework can be applied to any general (parametric) population distribution.\n3Inverse-Gamma distribution is conjugate prior distribution for the variance term.\n4\nwhere θd\ni parameterizes a non-linear decoding map from the latent space to the observed space. We can\nparameterize the encoder structure such that zij = gθe\ni(Xij). Using Gaussian distribution we get the\nADEPT-AE criterion:\narg min\n{θi},µ,σ\nf ae({θi}, µ, σ) = 1\nm\nm\nX\ni=1\n\u0010\n∥Xi −ψθd\ni (gθe\ni(Xi))∥2\nF + 2ξ + ∥µ −θi∥2\n2σ2\n\u0011\n+ dθ log σ\n(6)\nwhere µ is the global model and {θi} are the autoencoders of the clients and viewed as the concatenation of\nθe\ni and θd\ni .\n2.3\nPersonalized Generation through Diffusion Models\nThe denoising diffusion model has attracted attention recently due to its capability of generating high-quality\nimages and the theoretical foundation of the stochastic differential equations [33, 37]. The mathematical\nmodel of a diffusion model is the following stochastic differential equation [39]\ndxt = dwt\n(variance exploding process),\n(7)\nwhere wt is a standard d-dimensional Brownian motion. For time t ∈[0, T], its time-reversed process is\ndx←\nt\n= ∇ln pxT −t(x←\nt )dt + dw←\nt ,\n(8)\nwhere pxt is the probability density function of xt, w←\nt\nis a standard Wiener process. It can be verified that\nX←\nt\nfollows the same distribution as xT −t. More strongly, suppose X←\n0\nhas the same distribution as xT , and\nthen the two processes {xT −t}t∈[0,T ] and {x←\nt }t∈[0,T ] have the same distribution.\nSuppose that the score function ∇ln pxt(x) can be represented by a neural network ϕ(x; θ, t) ∈Rd for some\nparameter θ. The data generation is then integration over the time-revised process (8) with the drift function\nϕ(x; θ, t) and the starting distribution x←\n0 ∼N(0, σ2\n0Id). The generated distribution p(x|θ) is then implicitly\ndefined without a closed form. [18, Eq (4)] shows that\nln p(x|θ) ≥ELBOθ(x) = −1\n2\nZ T\nt=0\nE∥ϕ(xt; θ, t) −∇xt ln pxt|x0(xt|x)∥2dt + C\n(9)\nwhere xt ∼N(x, Id) and C is some constant factor independent of θ. Accordingly, we use ELBO 4\nas a surrogate function for the negative log-likelihood; thus, the personalized loss function is fi(θi; Xi) =\n−ELBOθi(Xi) = −Pn\nj=1 ELBOθi(xij). The personalized data generation is then minimizing the ADEPT-DGM\nloss function\nmin\n{θi},µ,σ2 f df({θi}, µ, σ; {Xi}) := 1\nm\nm\nX\ni=1\n\u0012\n−ELBOθi(Xi) + 2ξ + ∥µ −θi∥2\n2σ2\n\u0013\n+ dθ log σ.\n(10)\n3\nPersonalized Federated Dimensionality Reduction\nIn this section, we discuss our algorithms and convergence results on personalized adaptive dimensionality\nreduction.\n3.1\nPersonalized Adaptive PCA: ADEPT-PCA\nWe introduce ADEPT-PCA (Algorithm 1) to train adaptive personalized PCA. On line 7 we update the\nadaptivity parameter σi,t, on lines 8, 10 we compute the projected gradients for personalized and global\nparameters. On lines 9, 16 we update the personalized and global parameters through projected gradient\ndescent and retraction. In Algorithm 1, we use the polar retraction to map the updated V and U i back to\nthe Steifel manifold. We define the polar retraction in the next section in detail.\nTo show the convergence of the algorithm we need the following standard assumption and naturally occurring\nlower bound on σ.\n4Unlike the discrete case, for the continuous case, there are several definitions used for ELBO e.g. in [38, 19]; [18] uses the\nsimplified definition in (9) to unify different ELBO definitions.\n5\nAlgorithm 1 ADEPT-PCA Algorithm\nInput: Number of iterations T and learning rates (η1, η2, η3).\n1: Initialize local PCs {U i,0}m\ni=1, global PC V 0, and σ0.\n2: Broadcast V 0, σ0 to the clients\n3: for t = 1 to T do\n4:\nOn the clients:\n5:\nfor i = 1 to m: do\n6:\nReceive V t−1, σt−1\n7:\nσi,t = σt−1 −η3\n∂\n∂σt−1 f pca\ni\n(U i,t−1, V t−1, σt−1)\n8:\ngU\ni,t = PTUi,t−1(∇U i,t−1f pca\ni\n(U i,t−1, V t−1, σt−1))\n9:\nU i,t ←RU i,t−1(−η1gU\ni,t)\n10:\ngV\ni,t = PTV t−1(∇V t−1f pca\ni\n(U i,t, V t−1, σt−1))\n11:\nV i,t ←V t−1 −η2gV\ni,t\n12:\nSend V i,t, σi,t to the server\n13:\nend for\n14:\nAt the server:\n15:\nReceive {V i,t}m\ni=1 and {σi,t}m\ni=1\n16:\nV t = RV t−1\n\u0000 1\nm\nPm\ni=1 V i,t −V t−1\n\u0001\n17:\nσt = 1\nm\nPm\ni=1 σi,t\n18:\nBroadcast V t, σt to the clients\n19: end for\nOutput: Personalized PCs {U 1,T , . . . , U m,T }.\nAssumption 3.1. For each client i, the operator and Frobenius norms of Si are bounded by\n∥Si∥F ≤Gi,F\nand\n∥Si∥op ≤Gi,op,\nand Gmax,F := maxi∈[m] Gi,F ,Gmax,op := maxi∈[m] Gi,op. The assumption implies the Lipschitz smoothness\nproperties of the loss function w.r.t. personalized PCs {U i}.\nLemma 3.2 (A lower bound on σt). Given any ω ∈(0, 1). Let the learning rate η3 ≤(1 −ω) 2ξ\nd2\nθ and the\ninitialization σ0 ≥ω\nq\n2ξ\ndθ . Then, for all t ∈[T], we have σt ≥ω\nq\n2ξ\ndθ .\nSee Appendix A.1.1 for the proof. We will fix some ω ∈(0, 1) for the rest of the paper and initialize σ0\naccordingly so that we can utilize the lower bound in Lemma 3.2. The bound is due to ξ to guarantee that the\nloss does not explode due to vanishing σ. Let us now define gU\ni,t = PTUi,t−1 (∇U i,t−1f pca\ni\n(U i,t−1, V t−1, σt−1)),\ngV\nt\n=\n1\nm\nPm\ni=1 PTV t−1(∇V t−1f pca\ni\n(U i,t, V t−1, σt−1)), and gσ\nt\n=\n∂\n∂σt−1 f pca({U i,t−1}i, V t−1, σt−1).\nThen,\nAlgorithm 1 has the following convergence upper bound for finding a first-order stationary point.\nTheorem 3.3 (Convergence of ADEPT-PCA Algorithm 1). Let Gt =\n\u0010\n1\nm\nPm\ni=1\n\r\rgU\ni,t\n\r\r2\u0011\n+\n\r\rgV\nt\n\r\r2 + (gσ\nt )2. By\nchoosing η1 = min{\n1\n3Cη1 , 1}, η2 = min{\n1\n3Cη2 , 1}, and η3 = min\nn\nη1\n3(L(σ)\nU )2 ,\nη2\n3(L(σ)\nV\n)2 ,\n1\nLσ\no\n, we have\n1\nT\nT\nX\nt=1\nGt ≤\n3∆pca\nT\nT min{η1, η2, η3},\nwhere T is number of total iterations, ∆pca\nT\n= f pca({U i,0}i, V 0, σ0) −f pca({U i,T }i, V T , σT ), and η1, η2, η3\nare the learning rates for updating U i, V , and σ respectively. The constants are defined such that f pca(·)\nisLσ-smooth w.r.t. σ and gU\ni,t, gV\nt are L(σ)\nU , L(σ)\nV\ncontinuous w.r.t. σ respectively. Cη1, Cη2 are defined in\ndetail in the next section and depend on smoothness w.r.t. U, V .\nWe provide a detailed comment on the factors impacting the convergence rate in Remark 3.13.\n6\n3.1.1\nProof Outline for Theorem 3.3\nFor any point U ∈St(d, r), a retraction at a point U ∈St(d, r) is a map RU : TU →St(d, r) that\ninduces local coordinates on the Stiefel manifold. In this work, we use polar retraction that is defined as\nRU(V ) = (U + V )(I + V ⊤V )−1\n2 . The polar retraction is a second-order retraction that approximates the\nexponential mapping up to second-order terms. Consequently, it possesses the following non-expansiveness\nproperty and we state the Lemmas and properties that we use throughout the proof, detailed proof of Lemmas\ncan be found in Appendix A.\nLemma 3.4 (Non-expansiveness of polar retraction [5]). Let V ∈St(d, r), for any point U ∈TV with\nbounded norm, ∥U∥F ≤M, there exists C ∈R such that\n∥RV (U) −(V + U)∥F ≤C∥U∥2\nF .\n(11)\nLemma 3.5 (Lipschitz type inequality [5]). Let U, V ∈St(d, r). If a function ψ is L-Lipschitz smooth in\nRd×r, the following inequality holds:\n|ψ(V )−(ψ(U)+⟨PTU (∇ψ(U)), V −U⟩)| ≤Lg\n2 ∥V −U∥2\nF\nwhere Lg = L + G with G := maxU∈St(d,r) ∥∇ψ(U)∥2.\nUsing Assumption 3.1 and Lemma 3.2 we introduce the following Lemmas to be used in the proof.\nLemma 3.6 (Lipschitz smoothness and bounded gradients with respect to σ). For all i ∈[m] and U i, V ∈\nSt(d, r), within the domain σ ∈[ω\nq\n2ξ\nd , ∞], the function f pca\ni\n(U i, V , σ) is Lσ-Lipschitz smooth with respect\nto σ with constants\nLσ :=\nd2\n2ξω2 + 3d2\n2ξω4 + 3d2\nξ2ω4 .\nLemma 3.7 (Lipschitz smoothness and bounded gradients with respect to U i). The function f pca\ni\n(U i, V , σ)\nis LU-Lipschitz smooth with respect to U i and ∥∇f pca\ni\n(U i, V , σ)∥2 ≤GU for all i ∈[m] with constants\nLU := n\n2\n\u0012 1\nσ2ϵ\n+ Gmax,op\nσ4ϵ\n+\n\u0012\n1 + 2Gmax,op\nσ2ϵ\n\u0013 2\nσ4ϵ\n\u0013\n+\nd\nξω2 ,\nGU := n\n2\n\u0012Gmax,op\nσ4ϵ\n+ 1\nσ2ϵ\n\u0013\n+\nd\nξω2 .\nLemma 3.8 (Lipschitz smoothness and bounded gradients with respect to V ). The function f pca({U i}i, V , σ)\nis LV -Lipschitz smooth with respect to V and ∥∇f pca({U i}i, V , σ)∥2 ≤GV with constants\nLV := 12d\nξω2 ,\nGV := 3d\nξω2 .\nLemma 3.9 (Lipschitz continuity of\n∂\n∂σf pca\ni\n(U, V , σ) with respect to U, V ). The function\n∂\n∂σf pca\ni\n(U, V , σ)\nis L(σ)\nU -Lipschitz continuous with respect to U and L(σ)\nV -Lipschitz continuous with respect to V with\nL(σ)\nU\n=\n√\n2d3\nω3p\nξ3 ,\nL(σ)\nV\n=\n2\n√\nd3\nω3p\n2ξ3 .\n7\nBefore showing the convergence results, we define the following terms. Let\nCη1 = C1G1 + (LU + GU)(C2\n1G2\n1 + 1)\n2\n,\nCη2 = C2G2 + (LV + GV )(C2\n2G2\n2 + 1)\n2\n,\nG1 = 2GU\n√\nd,\nG2 = 2GV\n√\nd.\n(12)\nwith some constants C1, C2 given by Lemma 3.4 and GU, GV given in Lemma 3.7, 3.8. Given the above\nresults, we can obtain overall sufficient decrease as follows,\nLemma 3.10 (Sufficient Decrease). At any iteration t, we have\nf pca({U i,t}i, V t, σt) −f pca({U i,t−1}i, V t−1, σt−1)\n≤\n\u0010\n−η1 + Cη1η2\n1 + η3(L(σ)\nU )2\u0011 1\nm\nm\nX\ni=1\n∥PTUi,t−1\n\u0000∇U i,t−1f pca\ni\n(U i,t−1, V t−1, , σt−1)\n\u0001\n∥2\nF\n+\n\u0010\n−η2 + Cη2η2\n2 + η3(L(σ)\nV )2\u0011\n∥PTV t−1\n\u0000∇V t−1f pca({U i,t}i, V t−1, σt−1)\n\u0001\n∥2\nF\n+\n\u0012−η3 + η2\n3Lσ\n2\n\u0013 \u0014\n∂\n∂σt−1\nf pca({U i,t−1}i, V t−1, σt−1)\n\u00152\n.\nProof outline of Theorem 3.3. We show that a lower bound on σt is obtained with a proper initialization of\nσ0 and we can further derive the Lipschitz constants of the loss function (4). The sufficient decrease of the\nloss function w.r.t. U i, V , and σ (Lemma 3.10) are derived individually using non-expansiveness of polar\nretraction (Lemma 3.4) and Lipschitz type inequality (Lemma 3.5). After the sufficient decrease, we show\nthat by choosing η1 = min{\n1\n3Cη1 , 1}, η2 = min{\n1\n3Cη2 , 1}, and η3 = min\nn\nη1\n3(L(σ)\nU )2 ,\nη2\n3(L(σ)\nV\n)2 ,\n1\nbLσ\no\n, we have. The\nproofs of Lemmas can be found in Appendix A. Technical challenges in this proof are to control the error due\nto projections, utilize the lower bound on σ to avoid non-smoothness, and use the Lipschitz continuity of the\ngradients to combine updates on PCs with the update on σ.\n3.2\nPersonalized Adaptive AEs: ADEPT-AE\nAlgorithm 2 shows the alternating gradient descent training procedure for personalized adaptive AEs. At the\nbeginning of local iterations (line 8) the clients receive the global model and σ5 terms then do local updates\non θi (line 10). At the end of local iterations, the client updates the global model and variance term using\nits personalized model lines 12,13 and sends them to the server where it is aggregated and broadcast again\n(lines 21-23).\nAssumption 3.11. The loss function f (ae)\ni\n({θi}, µ, σ) is Lθ-smooth w.r.t. individual {θi}, Lµ-smooth w.r.t.\nµ and Lσ-smooth w.r.t. σ. Note that only the first one is an assumption, second and third ones are derived\nfrom the fact that σ is lower bounded when initialized properly (Appendix B).\nDefine gθ\ni,t = ∇θi,t−1f ae\ni (θi,t−1, µt−1, σt−1), gµ\nt = ∇µt−1f ae({θi,t}i,µt−1, σt−1), gσ\nt =\n∂f({θi,t}i,µt−1,σt−1)\n∂σt−1\n. For\nAlgorithm 2, we obtain the following convergence upper bound for finding a first-order stationary point.\nTheorem 3.12 (Convergence of ADEPT-AE (Algorithm 2)). Let us define Gt =\n\u0000 1\nm\nPm\ni=1 ∥gθ\ni,t∥2\u0001\n+ ∥gµ\nt ∥2 +\n(gσ\nt )2. Then, by choosing η1 =\n1\nLθ , η2 =\n1\nLσ+L(µ)\nσ\n2 , and η3 = min{1,\n1\nLµ }, we have\nmin\nt∈[T ],τ|t{Gt} ≤max{Lθ, Lσ + L(µ)\nσ\n2, Lµ, 1}∆ae\nT\nR\n,\nwhere R = T/τ is the number of communication rounds, ∆ae\nT = f ae({θi,0}i, µ0, σ0) −f ae({θi,T }i, µT , σT ), and\nthe constants can be found in lemma 3.15.\n5In the experiments we use individual variance terms for each weight that is σ ∈Θ which only has a constant effect on\nconvergence.\n8\nAlgorithm 2 ADEPT-AE Algorithm\nInput: Number of iterations T, learning rates (η1, η2, η3), number of local iterations τ\n1: Init local models {θi,0}m\ni=1, global model µ0, and σ0.\n2: On server:\n3: Broadcast µ0, σ0 to all clients\n4: for t = 1 to T do\n5:\nOn Clients:\n6:\nfor i = 1 to m do\n7:\nif τ divides t −1 then\n8:\nReceive µt−1, σt−1\n9:\nend if\n10:\nθi,t=θi,t−1−η1∇θi,t−1f ae\ni (θi,t−1, µt−1, σt−1)\n11:\nif τ divides t then\n12:\nµi,t = µt−1 −η2∇µt−1f ae\ni (θi,t, µt−1, σt−1)\n13:\nσi,t = σt−1 −η3∇σt−1f ae\ni (θi,t, µt−1, σt−1)\n14:\nSend µi,t, σi,t to server\n15:\nelse\n16:\nµt = µt−1, σt = σt−1\n17:\nend if\n18:\nend for\n19:\nAt the Server:\n20:\nif τ divides t then\n21:\nReceive {µi,t}m\ni=1 and {σi,t}m\ni=1\n22:\nµt = 1\nm\nPm\ni=1 µi,t, σt = 1\nm\nPm\ni=1 σi,t\n23:\nBroadcast µt, σt to all clients\n24:\nend if\n25: end for\nOutput: Personalized autoencoders {θ1,T , . . . , θm,T }.\nRemark 3.13. By examining the multiplicative constants within the bounds specified in Theorems 3.3 and 3.12,\nwe note a consistent observation: σ exhibits an inverse relationship with convergence speed. A higher σ,\nwhether resulting from a large value of ξ or inherent heterogeneity in the setting, can expedite the convergence\nprocess. Essentially, a large σ diminishes collaboration, allowing the model to fit quickly due to a reduced\neffective number of samples. Conversely, a smaller σ promotes collaboration and may augment the effective\nsample count. Observe that faster convergence does not necessarily imply a superior generalization error.\nConsequently, in our experiments, opting for a high value of σ0 facilitates fast convergence in the initial\nstages, while still allowing flexibility for adjustments to yield a superior generalization error.\n3.2.1\nProof outline of Theorem 3.12\nHere we present a proof outline for Theorem 3.12, detailed proof of lemmas and intermediate steps is provided\nin Appendix B. We start with an assumption that is necessary to have smoothness w.r.t. σ, and a lemma\nindicating Lipschitz properties of the loss function. The assumption is equivalent to having an upper bound\nover the gradient w.r.t. σ, which is a standard assumption for model parameters.\nAssumption 3.14. Assume that there exists some B > 0 such that for the weights of the autoencoders, we\nhave ∥µ∥≤B and ∥θi∥≤B for all i ∈[m].\nNow we show the Lipschitzness properties of the loss function.\n9\nLemma 3.15. The loss function f ae\ni (θi, µ, σ) is Lµ-smooth w.r.t. µ and Lσ-smooth w.r.t. σ with\nLµ =\ndθ\n2ξω2\nLσ = 3ξd2\nθ\n2ξ2ω4 + 3d2\nθB2\nξ2ω4 +\nd2\nθ\n2ξω2 .\nAlso, we have\n∥∇µf ae\ni (θ, µ, σ1) −∇µf ae\ni (θ, µ, σ2)∥≤L(µ)\nσ |σ1 −σ2|\nwith\nL(µ)\nσ\n= B\np\nd3\nθ\nω3p\n2ξ3 .\nSufficient decrease when τ divides t\nAt communication round time steps we have the following decrease\nproperty,\nf ae({θi,t}, µt, σt) −f ae({θi,t−1}, µt−1, σt−1) ≤\n\u0012\n−η1 + η2\n1\nLθ\n2\n\u0013  \n1\nm\nm\nX\ni=1\n\r\r∇θi,t−1f ae\ni (θi,t−1, µt−1, σt−1)\n\r\r2\n!\n+\n\n−η2 + η2\n2\nLσ + L(µ)\nσ\n2\n2\n\n(gσ\nt )2 +\n\u0012\n−η3\n2 + Lµη2\n3\n2\n\u0013\n∥gµ\nt ∥2.\n(13)\nSufficient decrease when τ does not divide t\nAt time steps that are not communication rounds we\nsimply have decreased due to the updates of {θi}, that is,\nf ae({θi,t}, µt, σt) −f ae({θi,t−1}, µt−1, σt−1) = f ae({θi,t}, µt−1, σt−1) −f ae({θi,t−1}, µt−1, σt−1)\n= 1\nm\nm\nX\ni=1\n(f ae\ni (θi,t, µt−1, σt−1) −f ae\ni (θi,t−1, µt−1, σt−1))\n≤\n\u0012\n−η1 + η2\n1\nLθ\n2\n\u0013  \n1\nm\nm\nX\ni=1\n\r\r∇θi,t−1f ae\ni (θi,t−1, µt−1, σt−1)\n\r\r2\n!\n.\nBy choosing, η1 =\n1\nLθ , η2 =\n1\nLσ+L(µ)\nσ\n2 , η3 = min{1,\n1\nLµ }, and by averaging over time steps while combining\ntwo type of decrease, we obtain the final bound.\nProof outline of Theorem 3.12. Since the form of the adaptation in the loss function is similar to the PCA\nloss function, the lower bound on sigma (Lemma 3.2) holds for ADEPT-AE as well. We utilize the lower bound\nto derive the Lipschitz smoothness constants with respect to µ and σ (Lemma 3.15), and the Lipschitz\nsmoothness constant with respect to θ is stated in Assumption 3.11. Then, we derive the sufficient decrease\nwith respect to θ, µ, and σ with the Lipschitz constants. However, we have multiple local iterations for one\ncommunication round in ADEPT-AE. Thus, we have to deal with the sufficient decrease separately depending\non whether the round is a communication round. With careful derivation under the two cases, we can combine\nthem in the end and get to our Theorem 3.12.\nRemark 3.16. In the experiments for ADEPT-AE, we treat sigma as a vector σ ∈Rdθ so that each weight in\nthe models can learn its own σj instead of sharing one σ across all the weights. The convergence for the\nmodified algorithm is almost identical to the proof here. Moreover, it can be shown that for the Lipschitz\nsmoothness constant Lσ, the dependence on the dimension in the numerators becomes dθ instead of d2\nθ. This\nis because our lower bound in Lemma 3.2 will not depend on dθ in this case.\n10\nAlgorithm 3 Personalized Adaptive Diffusion Model: ADEPT-DGM\nInput: Number of iterations T, learning rates (η2, η1, η3), number of local iterations τ, sample corruption\nrange γ ∈Z+\n1: Init local models {θi,0}m\ni=1, global model µ0, and σ0.\n2: On server:\n3: Broadcast µ0, σ0 to all clients\n4: for t = 1 to T do\n5:\nOn Clients:\n6:\nfor i = 1 to m do\n7:\nif τ divides t −1 then\n8:\nReceive µt−1, σt−1\n9:\nend if\n10:\nSample noise amount αi ∈Uniform[1, . . . , γ] independently for each sample and construct α =\n[α1, . . . , αn]\n11:\nθi,t = θi,t−1 −η1∇θi,t−1f df\ni,α(θi,t−1, µt−1, σt−1)\n12:\nif τ divides t then\n13:\nµi,t = µt−1 −η2∇µt−1f df\ni,α(θi,t, µt−1, σt−1)\n14:\nσi,t = σt−1 −η3\n∂\n∂σt−1 f df\ni,α(θi,t, µt−1, σt−1)\n15:\nSend µi,t, σi,t to server\n16:\nelse\n17:\nµt = µt−1, σt = σt−1\n18:\nend if\n19:\nend for\n20:\nAt the Server:\n21:\nif τ divides t then\n22:\nReceive {µi,t}m\ni=1 and {σi,t}m\ni=1\n23:\nµt = 1\nm\nPm\ni=1 µi,t, σt = 1\nm\nPm\ni=1 σi,t\n24:\nBroadcast µt, σt to all clients\n25:\nend if\n26: end for\nOutput: Personalized autoencoders {θ1,T , . . . , θm,T }.\n4\nPersonalized Generation through Adaptive Diffusion Models:\nADEPT-DGM\nIn Algorithm 3, the main change compared to ADEPT-AE is that we input a corrupted sample to the network\nduring the forward pass (line 11) to train it as a denoiser. Accordingly, f df\ni,α(θi, µ, σ) := ∥ϕ(X(1 −α) +\nZα; θi, α) −X∥2 + 2ξ+∥µ−θi∥2\n2σ2\n+ d log σ, where α is the randomly sampled noise amount.\nRemark 4.1. It is easy to see that one can prove a convergence result identical to Theorem 3.12 for ADEPT-DGM\nAlgorithm 3.\nAs an illustration of the effectiveness of personalized diffusion model learning, we analyze a simple personalized\nGaussian distribution generation problem, i.e., client-i’s target distribution is a Gaussian distribution\np(x|θi) = N(x; θi, σ2\n0Id). In the following, we first introduce some details of the canonical denoising diffusion\nmodel for Gaussian target distribution and analyze the improvement of collaboration in the proposed\npersonalized algorithm.\nDiffusion model with Gaussian target distribution:\nWhen the desired distribution is N(x; θ, σ2\n0Id),\nthe diffusion process (7) is a Gaussian process and the drift term for the reverse-time process (8) is a linear\nfunction, i.e., ∇ln pxT −t(x←\nt ) = −x←\nt −θ\nσ2\n0+t . The time-reversed process is\ndx←\nt\n= ∇ln pxT −t(x←\nt )dt + dw←\nt , x←\n0 ∼N(θ, (σ2\n0 + T)Id).\n(14)\n11\nWithout the knowledge of θ, we approximate the score function by a neural network of ϕ(x; ˆθ, t) = −x−ˆθ\nσ2\n0+t\nand approximate the initial distribution of the time-reversed process N(θ, (σ2\n0 + T)Id) by N(0, (σ2\n0 + T)Id).\nThe learned/approximated time-reversed process is then\ndx←\nt\n= −x −ˆθ\nσ2\n0 + tdt + dw←\nt , x←\n0 ∼N(0, (σ2\n0 + T)Id).\n(15)\nThe following lemma characterizes the difference between the generation distribution and the target\ndistribution.\nLemma 4.2. The output distribution px←\nT |ˆθ of the learned reversed-time process (15) satisfies,\nDKL(px|θ||px←\nT |ˆθ) =\n\r\r\r\rθ −ˆθ +\nσ2\n0\nσ2\n0 + T\nˆθ\n\r\r\r\r\n2\n,\n(16)\nwhere DKL is the Kullback-Leibler divergence and px|θ = N(x; θi, σ2\n0Id) is the target distribution.\nThe lemma shows that the KL-divergence between target distribution and the learned distribution is\nmeasured by the difference between θ and ˆθ a bias term\nσ2\n0\nσ2\n0+T ˆθ due to the initial distribution mismatch of\nthe time-reversed process.\nIt is straightforward to verify that for ϕ(x; ˆθ, t) = −x−ˆθ\nσ2\n0+t, training with dataset X = (x1, . . . , xn) by\nmaximizing ELBO (9) has a closed-form sample-mean solution, i.e., ˆθ = arg maxθ ELBOθ(X) =\nPn\nj=1 xj\nn\n.\nSince the data are i.i.d. sampled from N(θ, σ2\n0Id), we have E[DKL(px|θ||px←\nT |ˆθ)] = σ2\n0\nn , when omitting the\ninitial distribution bias by T →∞. It can be viewed as personalized training without collaboration and in\nthe following, we show that the proposed personalized training with collaboration can improve over this.\nPersonalized denoising diffusion model with Gaussian targets:\nThe local dataset Xi of client-i\nsampled i.i.d. from a target Gaussian distribution P(x|θi) = N(x; θi, σ2\n0Id), and the population distribution\nis also Gaussian with P(θ|Γ∗) = N(θ; µ∗, σ2\n∗Id).\nNote that we conduct analysis for a fixed unknown\npopulation distribution parameterized by Γ∗= (µ∗, σ∗), though the proposed loss function and algorithm\nfollows a Hierarchical Bayesian model as in Section 2.3.\nLemma 4.3 (Personalized estimation). For the parameterized score function ϕ(x; θ, t) = −x−θ\nσ2\n0+t, the optimal\nsolution to (10) is\nˆµ =\nPm\ni=1\nPn\nj=1 xij\nmn\nˆθi =\nnαˆσ2\nnαˆσ2 + 1\nPn\nj=1 xij\nn\n+\nˆµ\nnαˆσ2 + 1,\nwhere α =\n1\nσ2\n0 −\n1\nσ2\n0+T and ˆσ2 satisfies ˆσ2 = 2ξ\nd + s2(\nnαˆσ2\nnαˆσ2+1)2 with s2 =\nPm\ni=1∥ˆµ−1\nn\nPn\nj=1 xij∥\n2\nmd\n.\nRemark 4.4. We note that the global optimum model is the average of average samples of each client. The\npersonalized optimum model is interpolation of local estimate and the true global model. The interpolation\ncoefficient depends on the heterogeneity (large σ skews the result towards the local estimate and low σ\nskews it towards true global model). A large amount of local samples n decreases the reliance on µ These\nobservations are in parallel with findings in [28] on mean estimation.\nTheorem 4.5 (Condition for performance improvement). Consider an asymptotic regime that m →∞\nand T →∞.\nCompared to training without collaboration, the solution of ({ˆθi}, ˆµ, ˆσ2) in Lemma 4.3\nimproves the averaged KL-divergence\n1\nm\nPm\ni=1 DKL(px|θi||px←\nT |ˆθi) by a factor of\n\u0010\n2ˆσ2+σ2\n0/n−σ2\n∗\nˆσ2+σ2\n0/n\n\u0011\nσ2\n0/n\nˆσ2+σ2\n0/n\nσ2\n0\nn ,\nwhen ˆσ2 > σ2\n∗\n2 −σ2\n0\n2n.\nCorollary 4.6. Under the same setting as in Theorem 4.5, choosing ξ ≥3dσ2\n0\n2n\nguarantees strict improvement\nof collaboration for any population distribution N(µ∗, σ∗Id).\n12\nRemark 4.7. Note that if our estimate ˆσ2 of the population variance is accurate, i.e., ˆσ2 = σ2\n∗, then\ncollaboration always improves over only using local data for personalized generation; and in fact the\ncollaboration gain is the largest. In this case, the gain is larger when the number of local samples is relatively\nsmall. However, if the estimate is inaccurate, one could, in principle be better off without collaboration.\nHowever, by setting the hyperparameter ξ ≥3dσ2\n0\n2n , we can ensure that our estimate ˆσ2 ≥1\n2\n\u0000σ2\n∗−σ2\n0/n\n\u0001\n,\nensuring that collaboration is useful.\n4.1\nProofs\nProof of Lemma 4.2. Let βt =\n1\nT −t+σ2\n0 . For stochastic differential equation\ndx←\nt + βt(x←\nt −ˆθ)dt = dwt,\nx←\n0 ∼N(0, (σ2\n0 + T)Id),\nwe have\nd(e\nR t\n0 βsds(x←\nt −ˆθ)) = e\nR t\n0 βsdsdx←\nt + e\nR t\n0 βsdsβt(x←\nt −ˆθ)dt\n= e\nR t\n0 βsds \u0010\ndx←\nt + βt(x←\nt −ˆθ)dt\n\u0011\n= e\nR t\n0 βsdsdwt.\nNote that\ne\nR t\n0 βsds = e\nR t\n0\n1\nT −s+σ2\n0\nds = eln(T +σ2\n0)−ln(T −t+σ2\n0) =\nT + σ2\n0\nT −t + σ2\n0\nand\nZ T\n0\ne2\nR t\n0 βsdsdt =\nZ T\n0\n(T + σ2\n0)2\n(T + σ2\n0 −t)2 dt = (T + σ2\n0)2\n\u0012 1\nσ2\n0\n−\n1\nT + σ2\n0\n\u0013\n=\n\u0012\n1 + T\nσ2\n0\n\u0013\nT.\nIt then follows that\ne\nR T\n0 βsds(x←\nT −ˆθ) −(x←\n0 −ˆθ) ∼N\n \n0,\nZ T\n0\ne2\nR t\n0 βsdsdt\n!\n= N\n\u0012\n0,\n\u0012\n1 + T\nσ2\n0\n\u0013\nT\n\u0013\n,\nand equivalently\nx←\nT = ˆθ +\nσ2\n0\nσ2\n0 + T (x←\n0 −ˆθ) +\ns\nσ2\n0T\nσ2\n0 + T ϵ,\nwhere ϵ ∼N(0, Id).\nSince x←\n0\n∼N(0, (σ2\n0 + T)Id), we have px←\nT |ˆθ = N\n\u0010\nˆθ −\nσ2\n0\nσ2\n0+T ˆθ, σ2\n0Id\n\u0011\n. The KL-divergence between the\ntarget distribution px|θ = N(θ, σ2\n0Id) and px←\nT |ˆθ can then be calculated as\n\r\r\rθ −ˆθ +\nσ2\n0\nσ2\n0+T ˆθ\n\r\r\r\n2\n.\nProof of Lemma 4.3. The parameterized score function is ϕ(x; θ, t) = −x−θ\nσ2\n0+t. Note that ∇xt ln pxt|x0(xt|x0) =\n−xt−x0\nt\nsince xt|x0 ∼N(x0, tId). The training loss of (10) can then be written as\n1\nm\nm\nX\ni=1\nn\nX\nj=1\nZ T\nt=0\nEϵ∼N(0,Id)\n\u00141\n2\n\r\r\rϕ(xij +\n√\ntϵ; θj, t) + ϵ/\n√\nt\n\r\r\r\n2\u0015\ndt +\nm\nX\nj=1\n2ξ + ∥θj −µ∥2\n2σ2\n+ d\n2 ln σ2.\nNote that\nZ T\nt=0\nEϵ∼N(0,Id)[∥ϕ(xij +\n√\ntϵ; θ, t) + ϵ/\n√\nt∥2]dt =\nZ T\nt=0\nEϵ∼N(0,Id)\n\"\r\r\r\r−xij +\n√\ntϵ −θ\nσ2\n0 + t\n+ ϵ\n√\nt\n\r\r\r\r\n2#\ndt\n=\nZ T\n0\nEϵ∼N(0,Id)\n\"\r\r\r\r\nθ −xij\nσ2\n0 + t +\nσ2\n0\n√\nt(σ2\n0 + t)ϵ\n\r\r\r\r\n2#\ndt =\n Z T\n0\n1\n(σ2\n0 + t)2 dt\n!\n∥θ −xij∥2 + const.\n13\nSince\nR T\n0\n1\n(σ2\n0+t)2 dt =\n1\nσ2\n0 −\n1\nσ2\n0+T , the optimization of minimizing the training loss is equivalent to\nmin\nθ1:m,θ,σ2\n1\nm\nm\nX\ni=1\n\n\nn\nX\nj=1\nα\n2 ∥θj −xij∥2 + 2ξ + ∥θj −µ∥2\n2σ2\n\n+ d\n2 ln σ2,\nwhere α =\n1\nσ2\n0 −\n1\nσ2\n0+T . By the KKT condition that\nn\nX\nj=1\nα(θi −xij) + θi −µ\nσ2\n= 0,\n∀i = 1, 2, . . . , m,\nµ = 1\nm\nm\nX\ni=1\nθi,\n−1\nm\nm\nX\ni=1\n2ξ + ∥θj −µ∥2\n2σ4\n+\nd\n2σ2 = 0,\nWe thus have\nˆµ =\nPm\ni=1\nPn\nj=1 xij\nmn\nˆθi =\nnαˆσ2\nnαˆσ2 + 1\nPn\nj=1 xij\nn\n+\nˆµ\nnαˆσ2 + 1,\nwhere ˆσ2 satisfies ˆσ2 = 2ξ\nd + s2(\nnαˆσ2\nnαˆσ2+1)2 with s2 =\nPm\ni=1∥ˆµ−1\nn\nPn\nj=1 xij∥\n2\nmd\n.\nProof of Theorem 4.5. When m →∞, s2 →σ2\n0\nn + σ2\n∗and ˆµ →µ∗, a.s.. ˆσ2 satisfies\nˆσ2 = 2ξ\nd + (σ2\n0\nn + σ2\n∗)(\nˆσ2\nˆσ2 + 1/(nα))2.\nSince θi are sampled i.i.d. from a population distribution N(µ∗, σ2\n∗Id). α = 1/σ2\n0 since T →∞. Let\nx ∼N(θ, σ2\n0\nn Id), θ ∼N(µ, σ2Id), by Lemma 4.2, we have\n1\nm\nm\nX\ni=1\nDKL(px|θi||px←\nT |ˆθi) = 1\nm\nm\nX\ni=1\n\r\r\r\rθi −ˆθi +\nσ2\n0\nσ2\n0 + T\nˆθi\n\r\r\r\r\n2\n= E\n\"\r\r\r\rθ −x −\n1\nnαˆσ2 + 1(µ −x)\n\r\r\r\r\n2#\n= (\nσ2\n0/n\nˆσ2 + σ2\n0/n)2E\nh\n∥θ −µ∥2i\n+ (\nˆσ2\nˆσ2 + σ2\n0/n)2E\nh\n∥θ −x∥2i\n= (\nσ2\n0/n\nˆσ2 + σ2\n0/n)2σ2\n∗+ (\nˆσ2\nˆσ2 + σ2\n0/n)2σ2\n0/n\n= σ2\n0\nn + (σ2\n∗+ σ2\n0/n)σ2\n0/n\n(ˆσ2 + σ2\n0/n)2\nσ2\n0\nn −2\nσ2\n0/n\nˆσ2 + σ2\n0/n\nσ2\n0\nn\n= σ2\n0\nn −\n\u00122ˆσ2 + σ2\n0/n −σ2\n∗\nˆσ2 + σ2\n0/n\n\u0013\nσ2\n0/n\nˆσ2 + σ2\n0/n\nσ2\n0\nn\nwhere the expectation is taken w.r.t. x, θ.\nSince without collaboration, the training of maximizing ELBOi for client-i leads to parameter ˆθi =\nPn\nj=1 xij\nn\nand the KL-divergence between the target distribution and the output distribution is σ2\n0\nn , it follows that\ncollaboration improves the performance as long as ˆσ2 > σ2\n∗\n2 −σ2\n0\n2n.\nThe improvement is\n\u0010\n2ˆσ2+σ2\n0/n−σ2\n∗\nˆσ2+σ2\n0/n\n\u0011\nσ2\n0/n\nˆσ2+σ2\n0/n\nσ2\n0\nn and achieves the maximum when ˆσ2 = σ2\n∗, i.e., the learned\nˆσ2 = σ2\n∗\n14\nProof of Corollary 4.6. Under the same setting as in Theorem 4.5, by Lemma 4.3, we have ˆσ2 =\n2ξ\nd +\n( σ2\n0\nn + σ2\n∗)(\nˆσ2\nˆσ2+σ2\n0/n)2. Taking ξ > 2ξ\nd = 3dσ2\n0\n2n\ngives that ˆσ2 ≥3σ2\n0\nn , and thus ˆσ2 > ( σ2\n0\nn + σ2\n∗)(\nˆσ2\nˆσ2+σ2\n0/n)2 ≥\n9\n16( σ2\n0\nn + σ2\n∗) > σ2\n∗\n2 −σ2\n0\n2n, which guarantees strictly improvement by Theorem 4.5.\n5\nExperiments\n5.1\nExperimental Setting\nIn our experiments, our goal is to compare our adaptive personalized unsupervised algorithms with global\ntraining (FedAvg, FedAvg+fine-tuning), local training (training individual models without collaboration),\nand competitive baselines in terms of testing performance under different heterogeneous scenarios. For all\nexperiments, we use 50 clients (m = 50) and initialize ξ = 1e−6.\nADEPT-PCA:\nWe use synthetic datasets for the experiments of ADEPT-PCA:. In the dataset, we first sample a\nglobal PC, V ∗∈St(d, r), uniformly on the Steifel manifold. We sample { ˆU\n∗\ni }m\ni=1 where the entries of each\nˆU\n∗\ni follows Gaussian distribution with mean being V ∗and variance σ∗. Then, we let U ∗\ni = RV ∗(PTV ∗( ˆU\n∗\ni ))\nso that it is in the Steifel manifold. Data on each client are then generated by x = U ∗\ni z + ϵ.\nFigure 2: Ratio of the reconstruction error of different methods to the true model w.r.t. different values of\nσ∗. We have d = 100, r = 20, m = 10, and n = 20.\nADEPT-AE:\nFor AEs we do synthetic and real data experiments. In the synthetic experiments from a 0\nmean σµ = 0.1 standard deviation Gaussian, we sample weights for a one-layer decoder µd,∗with 5 latent\ndimensionality and 64 output dimensionality. Then by perturbing the weights with another zero-mean\nGaussian with σ∗we obtain true personalized decoders, which is used as in (5) to generate 10 local samples\nacross 50 clients. Heterogeneity among clients will depend on σ∗and can be quantified in terms of signal-to-\nnoise ratio as 20 log10\nσµ\nσ∗dB. For the real data experiments, we use MNIST, Fashion MNIST, and CIFAR-10.\nTo introduce heterogeneity, we distribute the samples such that each client has access to samples from a\nsingle class which simulates distinct data distributions for each client (commonly referred as pathological\nheterogeneity [26]). For MNIST and Fashion MNIST, each client has access to 120 training samples; and for\nCIFAR-10, 250 samples.\nModels.\nFor the synthetic experiments we use a two layer fully connected AE, for MNIST and Fashion\nMNIST we also use a two layer AE with 784 input dimension with 10 and 20 latent dimensions depending\non experiment. For CIFAR-10 we use a symmetric convolutional AE whose input and output layers are\nconvolutional layers with 16 channels, 3 kernel size, 2 stride and no padding; the intermediate layers are fully\nconnected layers that maps 3600 dimensions to latent dimensions. We use 10 and 50 latent dimensionality\n15\nTable 1: Total energy captured % averaged over samples and clients in the synthetic experiments.\nMethod\nHeterogeneity (std of noise and SNR)\n0.05(6dB)\n0.025(12dB)\n0.01(20dB)\nBaseline\n88.3 ± 0.5\n95.4 ± 0.8\n98.6 ± 0.1\nADEPT-AE\n87.3 ± 1.1\n95.9 ± 0.1\n98.7 ± 0.1\nGlobal Training\n81.3 ± 0.1\n94.3 ± 0.2\n98.4 ± 0.4\nLocal Training\n83.2 ± 1.9\n83.2 ± 1.9\n83.2 ± 1.9\ndepending on the experiment. We use ReLU activation function after the first layer and sigmoid after the\nlast layer.\nTraining and hyper-parameters.\nFor the synthetic experiments we don’t do local iterations and just do\ndistributed training. For the other experiments, we do 20 local iterations per communication round, and\nevery communication round corresponds to an epoch, i.e. we use 300 global batch size for MNIST and 750 for\nCIFAR-10. For all datasets and methods, we use SGD with a constant learning rate of 0.01 after individually\ntuning in the set {0.5, 0.1, 0.05, 0.01, 0.005, 0.001} and momentum coefficient of 0.9. For our method, we\nchoose η2 = 0.01, η3 = 0.001 and use SGD without momentum. For synthetic, MNIST, and Fashion MNIST\ndatasets we train for 150 epochs/comm. rounds and for CIFAR-10 for 250 epochs. We initialize σ = 1 in\nMNIST and Fashion MNIST experiments, and σ = 0.4 in synthetic experiments, and do not update σ for\nthe first two epochs. For CIFAR-10 we initialize σ = 0.2 and we do lazy updates that is we start updates\nafter 200 epochs. We observed lazy updates with relatively small initial σ works better for deeper models,\nwhereas simpler models do not require it. To improve the empirical performance and have a more stable\ntraining we make a few changes to Algorithm 2. Namely, we keep individual σ for each scalar weight, for\npersonalized and global models we clip the ℓ∞norm of the gradients by 1, and for σ by 10. We update σ at\nthe first iteration instead of the last one. We include the global model in the local iterations.\nTable 2: Total energy captured % averaged over samples and clients in the real dataset experiments.\nDataset\nMethod\nLatent dimensionality\nLow\nHigh\nMNIST\nBaseline\n78.9 ± 0.5\n85.7 ± 0.4\nADEPT-AE\n70.8 ± 0.5\n77.7 ± 0.1\nFedAvg\n66.2 ± 0.9\n75.9 ± 0.7\nLocal Training\n67.0 ± 0.8\n69.1 ± 1.1\nF. MNIST\nBaseline\n88.5 ± 0.2\n91.3 ± 0.1\nADEPT-AE\n83.9 ± 0.2\n85.6 ± 0.2\nFedAvg\n81.2 ± 0.2\n84.9 ± 0.2\nLocal Training\n76.9 ± 2.0\n77.1 ± 0.5\nCIFAR-10\nBaseline\n88.7 ± 0.5\n93.3 ± 0.1\nADEPT-AE\n88.4 ± 0.5\n93.3 ± 0.2\nFedAvg\n87.4 ± 0.2\n91.2 ± 0.1\nLocal Training\n87.7 ± 0.2\n92.2 ± 0.2\nADEPT-DGM:\nFor diffusion experiments we use a 6-layer U-Net from Hugging Face. We let every client to\nhave access to 1200 or 600 samples from one class depending on the experiment. Instead of FedAvg, we\ncompare to FedAvg+fine-tuning, as FedAvg cannot exclusively generate samples from the client’s target\ndistribution.\n16\nFigure 3: Randomly chosen samples (Left:ADEPT-DGM, noise σ = 0.024; Middle:FedAvg+fine-tuning,noise\nσ = 0.028; Right:Local training, noise σ = 0.032) (models are trained and samples are chosen with the same\nseed across runs) from generated dataset for a client with data from ’0’ class.\nTraining and hyper-parameters.\nWe train using 20 local iterations per communication round and epoch.\nWe use Adam optimizer with 1e−3 for all methods. For our method, we use Adam with 0.01 learning rate\nfor the updates of the global model and SGD with 0.001 lr for σ. We do 100 epochs/comm. rounds in total.\nWe initialize σ = 0.8 and do not update for the first 2 epochs. We multiply the learning rates η1, η2 by 0.1 at\n75th epoch. We employ the same changes in Algorithm 2 in Algorithm 3 as well. For a simpler demonstration,\nwe use a variance preserving SDE (as in Algorithm 3) instead of variance exploding (as in Section 4). We do\nthe same to modify Algorithm 3 as we did for ADEPT-DGM.\n5.2\nResults\nTable 3: Diffusion model generation quality for generating MNIST samples using U-Net model (lower is\nbetter).\nMethod\nMetric\nFID\nKID\nBaseline\n72.3 ± 2.2\n0.062 ± 0.003\nHigh Number of Samples\nADEPT-DGM\n80.0 ± 2.3\n0.067 ± 0.003\nFedAvg+fine-tuning\n88.5 ± 3.8\n0.082 ± 0.004\nLocal Training\n84.1 ± 0.8\n0.075 ± 0.001\nLow Number of Samples\nADEPT-DGM\n84.2 ± 1.5\n0.069 ± 0.002\nFedAvg+fine-tuning\n95.9 ± 7.6\n0.090 ± 0.009\nLocal Training\n91.8 ± 2.0\n0.083 ± 0.003\nADEPT-PCA:\nWe compare the reconstruction error between Algorithm 1, local training, and global training.\nIn the global training setting, we train a single global model with the average of local gradients in each\niteration. In Figure 2, the value in the y-axis is the ratio of the reconstruction error of each training method\nto the true error, which is evaluated by {U ∗\ni }m\ni=1. When σ∗is small, the heterogeneity of the data among the\nclients is low, and thus global training benefits from the sample size and performs better than pure local\ntraining. Our algorithm also makes use of the sample size and achieves an even smaller reconstruction error\nwith the personalized models. When σ∗is large, the heterogeneity of the data among the clients is high and\nthus training a single global model for each client does not work well. In this case, our algorithm learns a\n17\nlarger σ and performs more like local training. In the scenario between the two cases, our algorithm also\noutperforms both global and local training.\nADEPT-AE: The results are in terms of percentage of total energy captured per sample which is, for a sample\nx, equal to 100(1 −∥x −ˆx∥2/∥x∥2). The results are averaged over 3 runs and reported together with the\nstandard deviation.\nSynthetic data. In Table 1, baseline denotes that each client trains a personalized AE whose decoder\npart is the true data generating decoder. Our method outperforms local and global training and even the\ncompetitive baseline when heterogeneity is smaller. The result is similar to Figure 2, showing our method\noutperforms both local and global training in the regimes where they are strong alternatives.\nReal data.\nThe competitive baseline in (Table 2) is when 10 clients maintain all the training data from\ntheir corresponding class (n = 5000 per client). Remarkably, our method (n = 250 per client) matches the\nbaseline on CIFAR-10 (for high latent dimensions), indicating that adaptive personalized collaboration results\nin ×20 effective sample size. For other datasets, our method consistently outperforms FedAvg and local\ntraining by an important margin regardless of latent dimensionality. Our method reduces reconstruction\nerror by as much as ∼35% and ∼25% compared to local training and FedAvg respectively.\nFigure 4: Violin plot of KID values of clients.\nADEPT-DGM:\nFor Diffusion models we use FID [14] and KID [2] metrics to quantitatively measure the\ngenerated dataset quality (see Table 3). At the end of training, each client generates 200 samples (using\nthe model with the lowest validation loss) and compares it to the local test dataset to compute the metrics.\nOur method consistently results in better quality generated samples and improves upon other methods by\n5% −22%. Moreover, in Figure 4 we depict the resulting KID values of clients. We see that our method\nbrings equity, that is, the worst-performing client is much better compared to the worst clients of other\nmethods; and the performance variance of clients is lower. We also illustrate randomly chosen sample images\nin Figure 3 and estimate the amount of noise using [15]. Compared to ADEPT-DGM, in images obtained using\nFedAvg+fine-tuning we observe missing features and inconsistent hallucinations. On the other hand, local\ntraining outputs images with significantly more background noise as apparent in images (e.g. 1st from the\nlast row and 2nd from the first row) and from the estimated noise standard deviation which indicates a 1.5×\nincrease in noise level in terms of noise standard deviation (σ = 0.032 for local training vs σ = 0.024 for\nadaptive personalized method).\n6\nConclusion\nWe developed, ADEPT, a hierarchical Bayes framework for personalized federated unsupervised learning; leading\nto new criteria for linear (ADEPT-PCA), non-linear (ADEPT-AE) dimensionality reduction, and personalized\nfederated diffusion models (ADEPT-DGM). Each of our algorithms included adaptation for the heterogeneity\nduring training which resulted in novel theoretical interpretations and superior empirical performance. Open\nquestions include extensions with information constraints such as communication and privacy.\n18\nReferences\n[1] Durmus Alp Emre Acar, Yue Zhao, Ruizhao Zhu, Ramon Matas, Matthew Mattina, Paul Whatmough,\nand Venkatesh Saligrama. Debiasing model updates for improving personalized federated training. In\nInternational Conference on Machine Learning, pages 21–31. PMLR, 2021.\n[2] Miko laj Bi´nkowski, Dougal J. Sutherland, Michael Arbel, and Arthur Gretton. Demystifying MMD\nGANs. In International Conference on Learning Representations, 2018.\n[3] Xingjian Cao, Gang Sun, Hongfang Yu, and Mohsen Guizani. Perfed-gan: Personalized federated learning\nvia generative adversarial networks. IEEE Internet of Things Journal, 10(5):3749–3762, 2023.\n[4] Huili Chen, Jie Ding, Eric William Tramel, Shuang Wu, Anit Kumar Sahu, Salman Avestimehr, and Tao\nZhang. Self-aware personalized federated learning. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave,\nand Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022.\n[5] Shixiang Chen, Alfredo Garcia, Mingyi Hong, and Shahin Shahrampour. Decentralized riemannian\ngradient descent on the stiefel manifold. In International Conference on Machine Learning, pages\n1594–1605. PMLR, 2021.\n[6] Liam Collins, Hamed Hassani, Aryan Mokhtari, and Sanjay Shakkottai. Exploiting shared representations\nfor personalized federated learning. In Marina Meila and Tong Zhang, editors, International Conference\non Machine Learning (ICML), volume 139 of Proceedings of Machine Learning Research, pages 2089–2099.\nPMLR, 2021.\n[7] Yuyang Deng, Mohammad Mahdi Kamani, and Mehrdad Mahdavi. Adaptive personalized federated\nlearning. arXiv preprint arXiv:2003.13461, 2020.\n[8] Canh T. Dinh, Nguyen H. Tran, and Tuan Dung Nguyen. Personalized federated learning with moreau\nenvelopes. In Advances in Neural Information Processing Systems, 2020.\n[9] Simon Shaolei Du, Wei Hu, Sham M. Kakade, Jason D. Lee, and Qi Lei. Few-shot learning via learning\nthe representation, provably. In International Conference on Learning Representations, 2021.\n[10] Bradley Efron. Large-Scale Inference: Empirical Bayes Methods for Estimation, Testing, and Prediction.\nInstitute of Mathematical Statistics Monographs. Cambridge University Press, 2010.\n[11] Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar. Personalized federated learning: A meta-learning\napproach. In Advances in Neural Information Processing Systems, 2020.\n[12] Avishek Ghosh, Jichan Chung, Dong Yin, and Kannan Ramchandran. An efficient framework for\nclustered federated learning. In Advances in Neural Information Processing Systems, 2020.\n[13] Filip Hanzely and Peter Richt´arik. Federated learning of a mixture of global and local models. arXiv\npreprint arXiv:2002.05516, 2020.\n[14] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans\ntrained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural\ninformation processing systems, 30, 2017.\n[15] John Immerkær. Fast noise variance estimation. Computer Vision and Image Understanding, 64(2):300–\n302, 1996.\n[16] Peter Kairouz, H Brendan McMahan, Brendan Avent, Aur´elien Bellet, Mehdi Bennis, Arjun Nitin\nBhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances and\nopen problems in federated learning. Foundations and Trends® in Machine Learning, 14(1–2):1–210,\n2021.\n[17] Mikhail Khodak, Maria-Florina F Balcan, and Ameet S Talwalkar. Adaptive gradient-based meta-learning\nmethods. In Advances in Neural Information Processing Systems, 2019.\n19\n[18] Diederik P Kingma and Ruiqi Gao. Understanding diffusion objectives as the ELBO with simple data\naugmentation. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.\n[19] Diederik P Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. On density estimation with diffusion\nmodels. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, Advances in Neural\nInformation Processing Systems, 2021.\n[20] Nikita Yurevich Kotelevskii, Maxime Vono, Alain Durmus, and Eric Moulines. Fedpop: A bayesian\napproach for personalised federated learning. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and\nKyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022.\n[21] Qi Le, Enmao Diao, Xinran Wang, Ali Anwar, Vahid Tarokh, and Jie Ding. Personalized federated\nrecommender systems with private and partially federated autoencoders, 2022.\n[22] Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith.\nFederated optimization in heterogeneous networks. In Proceedings of Machine Learning and Systems\n2020, MLSys, 2020.\n[23] Jian Ma, Junhao Liang, Chen Chen, and Haonan Lu. Subject-diffusion: Open domain personalized\ntext-to-image generation without test-time fine-tuning. arXiv preprint arXiv:2307.11410, 2023.\n[24] Yishay Mansour, Mehryar Mohri, Jae Ro, and Ananda Theertha Suresh. Three approaches for personal-\nization with applications to federated learning. arXiv preprint arXiv:2002.10619, 2020.\n[25] Othmane Marfoq, Giovanni Neglia, Aur´elien Bellet, Laetitia Kameni, and Richard Vidal. Federated\nmulti-task learning under a mixture of distributions. Advances in Neural Information Processing Systems,\n34, 2021.\n[26] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.\nCommunication-efficient learning of deep networks from decentralized data. In Artificial Intelligence and\nStatistics, pages 1273–1282. PMLR, 2017.\n[27] Taehong Moon, Moonseok Choi, Gayoung Lee, Jung-Woo Ha, and Juho Lee. Fine-tuning diffusion\nmodels with limited data. In NeurIPS 2022 Workshop on Score-Based Methods, 2022.\n[28] Kaan Ozkara, Antonious Girgis, Deepesh Data, and Suhas Diggavi.\nA statistical framework for\npersonalized federated learning and estimation: Theory, algorithms, and privacy. In International\nConference on Learning Representations, 2023.\n[29] Kaan Ozkara, Bruce Huang, and Suhas Diggavi. Personalized PCA for federated heterogeneous data. In\n2023 IEEE International Symposium on Information Theory (ISIT), pages 168–173. IEEE, 2023.\n[30] Kaan Ozkara, Navjot Singh, Deepesh Data, and Suhas Diggavi. Quped: Quantized personalization via\ndistillation with applications to federated learning. Advances in Neural Information Processing Systems,\n34, 2021.\n[31] Chao Peng, Yiming Guo, Yao Chen, Qilin Rui, Zhengfeng Yang, and Chenyang Xu. Fedgm: Heterogeneous\nfederated learning via generative learning and mutual distillation. In Euro-Par 2023: Parallel Processing:\n29th International Conference on Parallel and Distributed Computing, Limassol, Cyprus, August 28 –\nSeptember 1, 2023, Proceedings, page 339–351, Berlin, Heidelberg, 2023. Springer-Verlag.\n[32] Mirko Polato. Federated variational autoencoder for collaborative filtering. In 2021 International Joint\nConference on Neural Networks (IJCNN), pages 1–8, 2021.\n[33] Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In International\nconference on machine learning, pages 1530–1538. PMLR, 2015.\n[34] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman.\nDreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22500–22510, 2023.\n20\n[35] Naichen Shi and Raed Al Kontar. Personalized PCA: Decoupling shared and unique features. arXiv\npreprint arXiv:2207.08041, 2022.\n[36] Virginia Smith, Chao-Kai Chiang, Maziar Sanjabi, and Ameet S. Talwalkar. Federated multi-task\nlearning. In Advances in Neural Information Processing Systems, pages 4424–4434, 2017.\n[37] Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised\nlearning using nonequilibrium thermodynamics. In Proceedings of the 32nd International Conference\non Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, volume 37 of JMLR Workshop and\nConference Proceedings, pages 2256–2265. JMLR.org, 2015.\n[38] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International\nConference on Learning Representations, 2021.\n[39] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole.\nScore-based generative modeling through stochastic differential equations. In International Conference\non Learning Representations, 2021.\n[40] Yonglong Tian, Yue Wang, Dilip Krishnan, Joshua B Tenenbaum, and Phillip Isola. Rethinking few-shot\nimage classification: a good embedding is all you need? In European Conference on Computer Vision,\npages 266–282. Springer, 2020.\n[41] Michael E Tipping and Christopher M Bishop. Probabilistic principal component analysis. Journal of\nthe Royal Statistical Society: Series B (Statistical Methodology), 61(3):611–622, 1999.\n[42] Ye Lin Tun, Chu Myaet Thwal, Ji Su Yoon, Sun Moo Kang, Chaoning Zhang, and Choong Seon\nHong. Federated learning with diffusion models for privacy-sensitive vision tasks. In 2023 International\nConference on Advanced Technologies for Communications (ATC), pages 305–310. IEEE, 2023.\n[43] Paul Vanhaesebrouck, Aur´elien Bellet, and Marc Tommasi. Decentralized collaborative learning of\npersonalized models over networks. In Artificial Intelligence and Statistics, pages 509–517. PMLR, 2017.\n[44] Lei Yang, Jiaming Huang, Wanyu Lin, and Jiannong Cao. Personalized federated learning on non-iid\ndata via group-based meta-learning. ACM Transactions on Knowledge Discovery from Data, 17(4):1–20,\n2023.\n[45] Valentina Zantedeschi, Aur´elien Bellet, and Marc Tommasi.\nFully decentralized joint learning of\npersonalized models and collaboration graphs. In International Conference on Artificial Intelligence and\nStatistics, pages 864–874. PMLR, 2020.\n[46] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion\nmodels. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3836–3847,\n2023.\n[47] Michael Zhang, Karan Sapra, Sanja Fidler, Serena Yeung, and Jose M. Alvarez. Personalized federated\nlearning with first order model optimization. In International Conference on Learning Representations,\n2021.\nAppendix\nA\nProofs for Adaptive PCA: ADEPT-PCA\nTheorem A.1. By choosing η1 = min{\n1\n3Cη1 , 1}, η2 = min{\n1\n3Cη2 , 1}, and η3 = min\nn\nη1\n3(L(σ)\nU )2 ,\nη2\n3(L(σ)\nV\n)2 ,\n1\nbLσ\no\n,\nwe have\nT\nX\nt=1\n  \n1\nm\nm\nX\ni=1\n∥gU\ni,t∥2\nF\n!\n+ ∥gV\nt ∥2\nF + (gσ\nt )2\n!\n≤\n3∆T\nmin{η1, η2, η3}\n21\nwhere\ngV\nt = PTV t−1\n\u0000∇V t−1f pca({U i,t}i, V t−1, σt−1)\n\u0001\n,\ngU\ni,t = PTUi,t−1\n\u0000∇U i,t−1f pca\ni\n(U i,t−1, V t−1, , σt−1)\n\u0001\n,\ngσ\nt =\n∂\n∂σt−1\nf pca({U i,t−1}i, V t−1, σt−1),\n∆T = f pca({U i,0}i, V 0, σ0) −f pca({U i,T }i, V T , σT ).\nA.1\nProofs\nFact A.2. The gradients of the local loss function with respect to the local and global PC’s and σ are given\nas\n∇U if pca\ni\n(U i, V , σ) = −n\n2 (W −1\ni SiW −1\ni U i−W −1\ni U i)+PTV (U i)\nσ2\n,\n∇V f pca\ni\n(U i, V ) = −PTV (U i)(U ⊤\ni V + V ⊤U i)\n2σ2\n,\n∂\n∂σ f pca\ni\n(U i, V , σ) = d\nσ −2ξ + d2(V , U i)\nσ3\n,\n∇U i\n\u0012 ∂\n∂σ f pca\ni\n(U i, V , σ)\n\u0013\n= −2PTV (U i)\nσ3\n,\n∇V\n\u0012 ∂\n∂σ f pca\ni\n(U i, V , σ)\n\u0013\n= PTV (U i)(U ⊤\ni V + V ⊤U i)\nσ3\n.\nFact A.3. For two matrices A ∈Ra×b and B ∈Rb×c, we have\n∥AB∥F ≤∥A∥op∥B∥F and ∥AB∥F ≤∥A∥F ∥B∥op.\nFact A.4. For matrix to matrix functions, {gi}k\ni=1, with bounded output operator norms, maxX ∥gi(X)∥op ≤\nMi, we have\n∥\nk\nY\ni=1\ngi(X) −\nk\nY\ni=1\ngi(Y )∥F ≤\nk\nY\nj=1\nMj\n k\nX\ni=1\n∥gi(X) −gi(Y )∥F\n!\nA.1.1\nProof of Lemma 3.2\nProof. We use mathematical induction to proof the lemma. For the base case, it is given that σ0 ≥ω\nq\n2ξ\nd .\nAssume that for all τ ∈{0, 1, . . . , t},\nστ ≥ω\nr\n2ξ\nd .\nThen, we consider the following two cases. First, if σt ∈\n\u0014\nω\nq\n2ξ\nd ,\nq\n2ξ\nd\n\u0015\n, we have\nσt ≤\nr\n2ξ\nd\n⇒\nd ≤2ξ\nσ2\nt\n⇒\nd\nσt\n−2ξ\nσ3\nt\n≤0\n⇒\n∀i ∈[m] :\n∂\n∂σt\nf pca\ni\n(U i,t, V t, σt) = d\nσt\n−2ξ + d2(V t, U i,t)\nσ3\nt\n≤d\nσt\n−2ξ\nσ3\nt\n≤0\n⇒\n∀i ∈[m] :\nσi,t+1 = σt −η3\n∂\n∂σt\nf pca\ni\n(U i,t, V t, σt) ≥σt ≥ω\nr\n2ξ\nd\n⇒\nσt+1 = 1\nm\nm\nX\ni=1\nσi,t+1 ≥ω\nr\n2ξ\nd .\n22\nOtherwise, if we have σt >\nq\n2ξ\nd , we have\nσi,t+1 = σt −η3\n∂\n∂σt\nf pca\ni\n(U i,t, V t, σt) = σt −η3\n\u0012 d\nσt\n−2ξ + d2(V t, U i,t)\nσ3\nt\n\u0013\n≥σt −η3\nd\nσt\n≥\nr\n2ξ\nd −(1 −ω)2ξ\nd2 ·\nd\np\n2ξ/d\n= ω\nr\n2ξ\nd\nand thus\nσt+1 = 1\nm\nm\nX\ni=1\nσi,t+1 ≥ω\nr\n2ξ\nd .\nThus, by mathematical induction, we have\n∀t ∈N ∀i ∈[m] :\nσt ≥ω\nr\n2ξ\nd\nand\nσi,t ≥ω\nr\n2ξ\nd .\nA.1.2\nProof of Lemma 3.6\nProof. For the Lipschitz smoothness, we have\n\f\f\f\f\n∂2\n∂σ2 f pca\ni\n(U i, V , σ)\n\f\f\f\f =\n\f\f\f\f\n6ξ + 3d2(V , U i)\nσ4\n−d\nσ2\n\f\f\f\f\n≤\n\f\f\f\f\n6ξ + 3d2(V , U i)\nσ4\n\f\f\f\f +\n\f\f\f\f\nd\nσ2\n\f\f\f\f\n≤6ξ + 12\n4ξ2ω4/d2 +\nd\n2ξω2/d\n= 3d2\n2ξω4 + 3d2\nξ2ω4 +\nd2\n2ξω2\n= Lσ\nfor any V , U i, and σ ≥ω\nq\n2ξ\nd .\nA.1.3\nProof of Lemma 3.7\nProof. For the bound on the gradient,\n\r\r\r\r−n\n2 (W −1\ni SiW −1\ni U i −W −1\ni U i)+PTV (U i)\nσ2\n\r\r\r\r\nop\n≤∥−n\n2 (W −1\ni SiW −1\ni U i −W −1\ni U i)∥op + 2\nσ2\n≤n\n2 (∥W −1\ni SiW −1\ni U i∥op + ∥W −1\ni U i∥op) + 2\nσ2\n≤n\n2\n\u0012Gmax,op\nσ4ϵ\n+ 1\nσ2ϵ\n\u0013\n+\nd\nξω2 ,\nwhere in the last inequality we use ∥W −1\ni ∥op ≤\n1\nσ2ϵ . Therefore, we find that norm of the gradient is bounded\nby GU := n\n2 ( Gmax,op\nσ4\nϵ\n+\n1\nσ2\nϵ ) +\nd\nξω2 . For the Lipschitz continuity of the gradient, we omit the client index i and\nuse U 1 and U 2 to denote two arbitrary points on St(d, r) for simplicity. For any client i, we focus on the\nfirst term of the gradient,\n∥W −1\n1 SiW −1\n1 U 1−W −1\n1 U 1−W −1\n2 SiW −1\n2 U 2+W −1\n2 U 2∥F\n23\n≤\n\u0010 1\nσ2ϵ\n+ Gmax,op\nσ4ϵ\n+\n\u0010\n1 + 2Gmax,op\nσ2ϵ\n\u0011 2\nσ4ϵ\n\u0011\n∥U 2 −U 1∥F ,\n(17)\nFor the second part of the gradient we have\n1\nσ2 ∥PTV (U 1) −PTV (U 2)∥F\n= 1\nσ2 ∥U 1−U 2−1\n2V (V ⊤(U 1−U 2)+(U ⊤\n1 −U ⊤\n2 )V )∥F\n≤2\nσ2 ∥U 1 −U 2∥F ,\n≤\nd\nξω2 ∥U 1 −U 2∥F ,\nwhere in the last inequality we use Fact A.3. As a result, we find that the gradient is Lipschitz continuous\nwith LU := n\n2\n\u0010\n1\nσ2ϵ + Gmax,op\nσ4ϵ\n+\n\u0010\n1 + 2Gmax,op\nσ2ϵ\n\u0011\n2\nσ4ϵ\n\u0011\n+\nd\nξω2 .\nA.1.4\nProof of Lemma 3.8\nProof. For the Lipschitz constant\n2\nσ2 ∥PTV 1(U i)sym(U ⊤\ni V 1)−PTV2 (U i)sym(U ⊤\ni V 2)∥F\n= 2\nσ2 ∥U iU ⊤\ni (V 1 −V 2) + U i(V 1 −V 2)U ⊤\ni −1\n2(V 1(V ⊤\n1 U i + U ⊤\ni V 1) −V 2(V ⊤\n2 U i + U ⊤\ni V 2))∥F\n≤24\nσ2 ∥V 1 −V 2∥F\n≤12d\nξω2 ,\nwhere sym(U ⊤\ni V ) = U ⊤\ni V +V ⊤U i and we used Fact A.4, hence LV = 12d\nξω2 . For the gradient bound, is it\nstraightforward to see that\n∥∇V f pca\ni\n(U, V , σ)∥2 ≤4\nσ2 ≤2d\nξω2 .\nA.1.5\nProof of Lemma 3.9\nProof. Using Fact A.2, we have\n∥∇U i\n\u0012 ∂\n∂σ f pca\ni\n(U i, V , σ)\n\u0013\n∥2 ≤4\nσ3 ≤\n√\n2d3\nω3p\nξ3\nand\n∥∇V\n\u0012 ∂\n∂σ f pca\ni\n(U i, V , σ)\n\u0013\n∥2 ≤8\nσ3 ≤2\n√\n2d3\nω3p\nξ3\nA.1.6\nProof of Lemma 3.10\nProof. We have\nf pca({U i,t}i, V t, σt) −f pca({U i,t−1}i, V t−1, σt−1)\n24\n= [f pca({U i,t}i, V t, σt) −f pca({U i,t}i, V t, σt−1)]\n+ [f pca({U i,t}i, V t, σt−1) −f pca({U i,t}i, V t−1, σt−1)]\n+ [f pca({U i,t}i, V t−1, σt−1) −f pca({U i,t−1}i, V t−1, σt−1)]\nWith a similar proof as Lemma 5 in [29], we have\nf pca({U i,t}i, V t−1, σt−1)−f pca({U i,t−1}i, V t−1, σt−1)\n≤(−η1+Cη1η2\n1)\nm\nm\nX\ni=1\n∥PTUi,t−1(∇U i,t−1f pca\ni\n(U i,t−1, V t−1, σi,t))∥2\nF ,\nf pca({U i,t}i, V t, σt−1)−f pca({U i,t}i, V t−1, σt−1)\n≤(−η2 + Cη2η2\n2)∥PTV t−1(∇V t−1f pca({U i,t}i, V t−1, σt−1))∥2\nF\nwhere\nCη1 = C1G1 + Lgu(C2\n1G2\n1 + 1)\n2\n,\nCη2 = C2G2 + Lgv(C2\n2G2\n2 + 1)\n2\n,\nG1 = 2GU\n√\nd,\nG2 = 2GV\n√\nd\nwith some constants C1, C2 given by Lemma 3.4 and GU, GV given in Lemma 3.7, 3.8. For the sufficient\ndecrease with respect to σ, we have\nf pca({U i,t}i, V t, σt) −f pca({U i,t}i, V t, σt−1)\n≤\n∂\n∂σt−1\nf pca({U i,t}i, V t, σt−1)(σt −σt−1) + Lσ\n2 (σt −σt−1)2\n=\n\u0014\n∂\n∂σt−1\nf pca({U i,t}i, V t, σt−1)\n\u0015 \u0014\n−η3\n∂\n∂σt−1\nf pca({U i,t−1}i, V t−1, σt−1)\n\u0015\n+ η2\n3Lσ\n2\n\u0014\n∂\n∂σt−1\nf pca({U i,t−1}i, V t−1, σt−1)\n\u00152\n(by the update rule of σt)\n= (−η3)\n\u0014\n∂\n∂σt−1\nf pca({U i,t}i, V t, σt−1) −\n∂\n∂σt−1\nf pca({U i,t−1}i, V t−1, σt−1) +\n∂\n∂σt−1\nf pca({U i,t−1}i, V t−1, σt−1)\n\u0015\n·\n\u0014\n∂\n∂σt−1\nf pca({U i,t−1}i, V t−1, σt−1)\n\u0015\n+ η2\n3Lσ\n2\n\u0014\n∂\n∂σt−1\nf pca({U i,t−1}i, V t−1, σt−1)\n\u00152\n= (−η3)\n\u0014\n∂\n∂σt−1\nf pca({U i,t}i, V t, σt−1) −\n∂\n∂σt−1\nf pca({U i,t−1}i, V t−1, σt−1)\n\u0015 \u0014\n∂\n∂σt−1\nf pca({U i,t−1}i, V t−1, σt−1)\n\u0015\n−\n\u0012\nη3 −η2\n3Lσ\n2\n\u0013 \u0014\n∂\n∂σt−1\nf pca({U i,t−1}i, V t−1, σt−1)\n\u00152\n≤η3\n2\n\u0014\n∂\n∂σt−1\nf pca({U i,t}i, V t, σt−1) −\n∂\n∂σt−1\nf pca({U i,t−1}i, V t−1, σt−1)\n\u00152\n+ η3\n2\n\u0014\n∂\n∂σt−1\nf pca({U i,t−1}i, V t−1, σt−1)\n\u00152\n−\n\u0012\nη3 −η2\n3Lσ\n2\n\u0013 \u0014\n∂\n∂σt−1\nf pca({U i,t−1}i, V t−1, σt−1)\n\u00152\n(since 2ab ≤a2 + b2 for any a, b ∈R)\n= η3\n2\n\u0014\n∂\n∂σt−1\nf pca({U i,t}i, V t, σt−1) −\n∂\n∂σt−1\nf pca({U i,t−1}i, V t−1, σt−1)\n\u00152\n−\n\u0012η3\n2 −η2\n3Lσ\n2\n\u0013 \u0014\n∂\n∂σt−1\nf pca({U i,t−1}i, V t−1, σt−\n= η3\n2\n\"\n∂\n∂σt−1\nf pca({U i,t}i, V t, σt−1) −\n∂\n∂σt−1\nf pca\ni\n(V t, {U i,t−1}i, σt−1)\n+\n∂\n∂σt−1\nf pca({U i,t−1}i, V t, σt−1) −\n∂\n∂σt−1\nf pca({U i,t−1}i, V t−1, σt−1)\n#2\n25\n−\n\u0012η3\n2 −η2\n3Lσ\n2\n\u0013 \u0014\n∂\n∂σt−1\nf pca({U i,t−1}i, V t−1, σt−1)\n\u00152\n= η3\n2\n\"  \n1\nm\nm\nX\ni=1\n∂\n∂σt−1\nf pca\ni\n(U i,t, V t, σt−1) −\n∂\n∂σt−1\nf pca\ni\n(U i,t−1, V t, σt−1)\n!\n+\n∂\n∂σt−1\nf pca({U i,t−1}i, V t, σt−1) −\n∂\n∂σt−1\nf pca({U i,t−1}i, V t−1, σt−1)\n#2\n−\n\u0012η3\n2 −η2\n3Lσ\n2\n\u0013 \u0014\n∂\n∂σt−1\nf pca({U i,t−1}i, V t−1, σt−1)\n\u00152\n≤η3\n2\n\" \n1\nm\nm\nX\ni=1\nL(σ)\nU ∥U i,t −U i,t−1∥F\n!\n+ L(σ)\nV ∥V i,t −V i,t−1∥F\n#2\n−\n\u0012η3 −η2\n3Lσ\n2\n\u0013 \u0014\n∂\n∂σt−1\nf pca({U i,t−1}i, V t−1, σt−1)\n\u00152\n(from Lemma 3.9)\n≤η3\n2\n\n2\n \n1\nm\nm\nX\ni=1\nL(σ)\nU ∥U i,t −U i,t−1∥F\n!2\n+ 2\n\u0010\nL(σ)\nV ∥V i,t −V i,t−1∥F\n\u00112\n\n−\n\u0012η3 −η2\n3Lσ\n2\n\u0013 \u0014\n∂\n∂σt−1\nf pca({U i,t−1}i, V t−1, σt−1)\n(since (a + b)2 ≤2a2 + 2b2)\n≤η3\n\"\n1\nm\nm\nX\ni=1\n\u0010\nL(σ)\nU ∥U i,t −U i,t−1∥F\n\u00112\n+\n\u0010\nL(σ)\nV ∥V i,t −V i,t−1∥F\n\u00112\n#\n−\n\u0012η3 −η2\n3Lσ\n2\n\u0013 \u0014\n∂\n∂σt−1\nf pca({U i,t−1}i, V t−1, σt−1)\n\u00152\n(Cauchy–Schwarz inequality)\n= η3(L(σ)\nU )2 1\nm\n m\nX\ni=1\n∥PTUi,t−1\n\u0000∇U i,t−1f pca(U i,t−1, V t−1, σt−1)\n\u0001\n∥2\nF\n!\n+ η3(L(σ)\nV )2∥PTV t−1\n\u0000∇V t−1f pca({U i,t}i, V t−1, σt−1)\n\u0001\n∥2\nF\n−\n\u0012η3 −η2\n3Lσ\n2\n\u0013 \u0014\n∂\n∂σt−1\nf pca({U i,t−1}i, V t−1, σt−1)\n\u00152\n.\nThus, we have\nf pca({U i,t}i, V t, σt) −f pca({U i,t−1}i, V t−1, σt−1)\n= [f pca({U i,t}i, V t−1, σt−1) −f pca({U i,t−1}i, V t−1, σt−1)]\n+ [f pca({U i,t}i, V t, σt−1) −f pca({U i,t}i, V t−1, σt−1)]\n+ [f pca({U i,t}i, V t, σt) −f pca({U i,t}i, V t, σt−1)]\n≤\n\u0010\n−η1 + Cη1η2\n1 + η3(L(σ)\nU )2\u0011 1\nm\nm\nX\ni=1\n∥PTUi,t−1\n\u0000∇U i,t−1f pca\ni\n(U i,t−1, V t−1, , σt−1)\n\u0001\n∥2\nF\n+\n\u0010\n−η2 + Cη2η2\n2 + η3(L(σ)\nV )2\u0011\n∥PTV t−1\n\u0000∇V t−1f pca({U i,t}i, V t−1, σt−1)\n\u0001\n∥2\nF\n+\n\u0012−η3 + η2\n3Lσ\n2\n\u0013 \u0014\n∂\n∂σt−1\nf pca({U i,t−1}i, V t−1, σt−1)\n\u00152\n.\nBy choosing η1 = min{\n1\n3Cη1 , 1}, η2 = min{\n1\n3Cη2 , 1}, and η3 = min\nn\nη1\n3(L(σ)\nU )2 ,\nη2\n3(L(σ)\nV\n)2 ,\n1\n6Lσ\no\n, we have\n−η1 + Cη1η2\n1 + η3(L(σ)\nU )2 ≤η1\n\u0012\nCη1η1 −1\n3\n\u0013\n−2η1\n3\n+ η1\n3 = −η1\n3 ,\n−η2 + Cη2η2\n2 + η3(L(σ)\nV )2 ≤η2\n\u0012\nCη2η2 −1\n3\n\u0013\n−2η2\n3\n+ η2\n3 = −η2\n3 ,\n−η3 + η2\n3Lσ\n2\n= η3\n\u0012\nLση3 −1\n6\n\u0013\n−η3\n3 ≤−η3\n3 .\n26\nTherefore, we obtain\nf pca({U i,t}i, V t, σt)−f pca({U i,t−1}i, V t−1, σt−1) ≤−η1\n3\n \n1\nm\nm\nX\ni=1\n∥PTUi,t−1\n\u0000∇U i,t−1f pca\ni\n(U i,t−1, V t−1, , σt−1)\n\u0001\n∥2\nF\n!\n−η2\n3 ∥PTV t−1\n\u0000∇V t−1f pca({U i,t}i, V t−1, σt−1)\n\u0001\n∥2\nF −η3\n3\n\u0014\n∂\n∂σt−1\nf pca({U i,t−1}i, V t−1, σt−1)\n\u00152\n.\nA.1.7\nProof of Theorem 3.3\nProof. Following Lemma 3.10, by telescoping across the iterations, we have\n1\nT\nT\nX\nt=1\n\"\n∥PTV t−1\n\u0000∇V t−1f pca({U i,t}i, V t−1, σt−1)\n\u0001\n∥2\nF\n+\n \n1\nm\nm\nX\ni=1\n∥PTUi,t−1\n\u0000∇U i,t−1f pca\ni\n(U i,t−1, V t−1, , σt−1)\n\u0001\n∥2\nF\n!\n+\n\u0014\n∂\n∂σt−1\nf pca({U i,t−1}i, V t−1, σt−1)\n\u00152 #\n≤\n1\nT min{ η1\n3 , η2\n3 , η3\n3 }\nT\nX\nt=1\nf pca({U i,t−1}i, V t−1, σt−1) −f pca({U i,t}i, V t, , σt)\n= 3 (f pca({U i,0}i, V 0, σ0) −f pca({U i,T }i, V T , σT ))\nT min{η1, η2, η3}\n.\nB\nProofs for Adaptive AEs: ADEPT-AE\nB.1\nProofs\nB.1.1\nProof of Lemma 3.15\nProof. Following the same proof in Lemma 3.2, we have the same lower bound on σt if we initialized it in the\nsame way.\nThe gradient w.r.t. µ is\n∇µf ae\ni (θ, µ, σ) = µ −θ\n2σ2 .\nThus, we have\n\r\r\r\r\nµ1 −θ\n2σ2\n−µ2 −θ\n2σ2\n\r\r\r\r =\n\r\r\r\r\nµ1 −µ2\n2σ2\n\r\r\r\r ≤\ndθ\n2ξω2 ∥µ1 −µ2∥≤Lµ∥µ1 −µ2∥.\nFor Lσ, we have\n\f\f\f\f\n∂2\n∂σ2 f ae\ni (θ, µ, σ)\n\f\f\f\f =\n\f\f\f\f\n6ξ + 3∥µ −θ∥2\nσ4\n−dθ\nσ2\n\f\f\f\f\n≤\n\f\f\f\f\n6ξ + 3∥µ −θ∥2\nσ4\n\f\f\f\f +\n\f\f\f\f\ndθ\nσ2\n\f\f\f\f\n≤6ξ + 3(2B)2\n4ξ2ω4/d2\nθ\n+\nd2\nθ\n2ξω2\n= 3ξd2\nθ\n2ξ2ω4 + 3d2\nθB2\nξ2ω4 +\nd2\nθ\n2ξω2\n= Lσ.\n27\nFor L(µ)\nσ , we have\n∥∇µf ae\ni (θ, µ, σ1) −∇µf ae\ni (θ, µ, σ2)∥=\n\r\r\r\r\nµ −θ\n2σ2\n1\n−µ −θ\n2σ2\n2\n\r\r\r\r\n=\n\f\f\f\f\n1\nσ2\n1\n−1\nσ2\n2\n\f\f\f\f\n∥µ −θ∥\n2\n= |σ1 −σ2|\n\f\f\f\f\n1\nσ2\n1σ2\n+\n1\nσ1σ2\n2\n\f\f\f\f\n∥µ −θ∥\n2\n≤2\n \nω\nr\n2ξ\ndθ\n!−3\nB|σ1 −σ2|\n= B\np\nd3\nθ\nω3p\n2ξ3 |σ1 −σ2|\n= L(µ)\nσ |σ1 −σ2|.\nB.1.2\nProof of Theorem 3.12\nProof. Since µt and σt are updated only when τ divides t, we consider the two cases separately.\nWhen τ divides t\nFirst, for the sufficient decrease of θt,i, we have\nf ae\ni (θi,t, µt−1, σt−1) −f ae\ni (θi,t−1, µt−1, σt−1)\n≤\n\n∇θi,t−1f ae\ni (θi,t−1, µt−1, σt−1), θi,t −θi,t−1\n\u000b\n+ Lθ\n2 ∥θi,t −θi,t−1∥2\n=\n\n∇θi,t−1f ae\ni (θi,t−1, µt−1, σt−1), −η1∇θi,t−1f ae\ni (θi,t−1, µt−1, σt−1)\n\u000b\n+ Lθ\n2\n\r\r−η1∇θi,t−1f ae\ni (θi,t−1, µi,t−1, σt−1)\n\r\r2\n≤\n\u0012\n−η1 + η2\n1\nLθ\n2\n\u0013 \r\r∇θi,t−1f ae\ni (θi,t−1, µt−1, σt−1)\n\r\r2 .\nSum over the clients and we have\nf ae({θi,t}, µt−1, σt−1) −f ae({θi,t−1}, µt−1, σt−1) ≤\n\u0012\n−η1 + η2\n1\nLθ\n2\n\u0013  \n1\nm\nm\nX\ni=1\n\r\r∇θi,t−1f ae\ni (θi,t−1, µt−1, σt−1)\n\r\r2\n!\n.\n(18)\nSecond, for the sufficient decrease of σt, define\ngσ\nt = 1\nm\nm\nX\ni=1\n∂\n∂σt−1\nf ae\ni (θi,t, µt−1, σt−1).\nThus, σt = σt−1 −η2gσ\nt and\nf ae\ni (θi,t, µt−1, σt) −f ae\ni (θi,t, µt−1, σt−1) ≤\n\u0012\n∂\n∂σt−1\nf ae\ni (θi,t, µt−1, σt−1)\n\u0013\n(σt −σt−1) + Lσ\n2 (σt −σt−1)2\n=\n\u0012\n∂\n∂σt−1\nf ae\ni (θi,t, µt−1, σt−1)\n\u0013\n(−η2gσ\nt ) + Lσ\n2 (−η2gσ\nt )2 .\nSum over the clients and we have\nf ae({θi,t}, µt−1, σt) −f ae({θi,t}, µt−1, σt−1) ≤−η2(gσ\nt )2 + η2\n2\nLσ\n2 (gσ\nt )2 =\n\u0012\n−η2 + η2\n2\nLσ\n2\n\u0013\n(gσ\nt )2.\n(19)\n28\nThen, for the sufficient decrease of µt, define\ngµ\ni,t = ∇µt−1f ae\ni (θi,t, µt−1, σt−1),\ngµ\nt = 1\nm\nm\nX\ni=1\ngµ\ni,t,\n˜gµ\ni,t = ∇µt−1f ae\ni (θi,t, µt−1, σt),\n˜gµ\nt = 1\nm\nm\nX\ni=1\n˜gµ\ni,t.\nWe have\nf ae({θi,t}, µt, σt) −f ae({θi,t}, µt−1, σt) ≤\n\n˜gµ\nt , µt −µt−1\n\u000b\n+ Lµ\n2 ∥µt −µt−1∥2\n= −η3 ⟨˜gµ\nt −gµ\nt + gµ\nt , gµ\nt ⟩+ Lµη2\n3\n2\n∥gµ\nt ∥2\n=\n\u0012\n−η3 + Lµη2\n3\n2\n\u0013\n∥gµ\nt ∥2 + η3 ⟨gµ\nt −˜gµ\nt , gµ\nt ⟩\n≤\n\u0012\n−η3 + Lµη2\n3\n2\n\u0013\n∥gµ\nt ∥2 + η3\n2 ∥gµ\nt −˜gµ\nt ∥2 + η3\n2 ∥gµ\nt ∥2\n≤\n\u0012\n−η3\n2 + Lµη2\n3\n2\n\u0013\n∥gµ\nt ∥2 + η3L(µ)\nσ\n2\n2\n(σt −σt−1)2\n≤\n\u0012\n−η3\n2 + Lµη2\n3\n2\n\u0013\n∥gµ\nt ∥2 + η3η2\n2L(µ)\nσ\n2\n2\n(gσ\nt )2\n≤\n\u0012\n−η3\n2 + Lµη2\n3\n2\n\u0013\n∥gµ\nt ∥2 + η2\n2L(µ)\nσ\n2\n2\n(gσ\nt )2.\n(20)\nFinally, we have the overall decrease when τ divides t by summing equation (19), (18), and (20),\nf ae({θi,t}, µt, σt) −f ae({θi,t}, µt−1, σt−1) ≤\n\u0012\n−η1 + η2\n1\nLθ\n2\n\u0013  \n1\nm\nm\nX\ni=1\n\r\r∇θi,t−1f ae\ni (θi,t−1, µt−1, σt−1)\n\r\r2\n!\n+\n\n−η2 + η2\n2\nLσ + L(µ)\nσ\n2\n2\n\n(gσ\nt )2 +\n\u0012\n−η3\n2 + Lµη2\n3\n2\n\u0013\n∥gµ\nt ∥2.\nWhen τ does not divide t\nAt time steps that are not communication rounds we simply have decrease\ndue to the updates of {θi}, that is,\nf ae({θi,t}, µt, σt) −f ae({θi,t−1}, µt−1, σt−1) = f ae({θi,t}, µt−1, σt−1) −f ae({θi,t−1}, µt−1, σt−1)\n= 1\nm\nm\nX\ni=1\n(f ae\ni (θi,t, µt−1, σt−1) −f ae\ni (θi,t−1, µt−1, σt−1))\n≤\n\u0012\n−η1 + η2\n1\nLθ\n2\n\u0013  \n1\nm\nm\nX\ni=1\n\r\r∇θi,t−1f ae\ni (θi,t−1, µt−1, σt−1)\n\r\r2\n!\n.\nThe final bound\nBy choosing, η1 =\n1\nLθ , η2 =\n1\nLσ+L(µ)\nσ\n2 , η3 = min{1,\n1\nLµ }, and by averaging over time steps\nwhile combining two type of decrease, we obtain\n1\nT\nT\nX\nt=1\n \n1\nm\nm\nX\ni=1\n\r\r∇θi,t−1f ae\ni (θi,t−1, µt−1, σt−1)\n\r\r2\n!\n+ 1\nT\nT\nX\nt=1\nt%τ=0\n∥gµ\nt ∥2 + 1\nT\nT\nX\nt=1\nt%τ=0\n(gσ\nt )2 ≤max{Lθ, Lσ + L(µ)\nσ\n2, Lµ, 1}∆T\nT\n,\n29\nwhere ∆T = f ae({θi,0}, µ0, σ0) −f ae({θi,T }, µT , σT ). Given that τ is a finite constant let us denote R = T/τ\nas the number of communication rounds. Then we have,\n1\nT\nT\nX\nt=1\n \n1\nm\nm\nX\ni=1\n\r\rgθ\ni,t\n\r\r2\n!\n+ 1\nT\nT\nX\nt=1\nt%τ=0\n∥gµ\nt ∥2 + 1\nT\nT\nX\nt=1\nt%τ=0\n(gσ\nt )2\n= R\nT\n\n\n1\nR\nT\nX\nt=1\n \n1\nm\nm\nX\ni=1\n\r\rgθ\ni,t\n\r\r2\n!\n+ 1\nR\nT\nX\nt=1\nt%τ=0\n∥gµ\nt ∥2 + 1\nR\nT\nX\nt=1\nt%τ=0\n(gσ\nt )2\n\n\n\n≥R\nT\n\n\n1\nR\nT\nX\nt=1\nt%τ=0\n \n1\nm\nm\nX\ni=1\n\r\rgθ\ni,t\n\r\r2\n!\n+ 1\nR\nT\nX\nt=1\nt%τ=0\n∥gµ\nt ∥2 + 1\nR\nT\nX\nt=1\nt%τ=0\n(gσ\nt )2\n\n\n\n≥R\nT\nmin\nt∈[T ],τ|t\nn\r\rgθ\ni,t\n\r\r2 + ∥gµ\nt ∥2 + (gσ\nt )2o\n.\nFinally this yields,\nmin\nt∈[T ],τ|t\nn\r\rgθ\ni,t\n\r\r2 + ∥gµ\nt ∥2 + (gσ\nt )2o\n≤max{Lθ, Lσ + L(µ)\nσ\n2, Lµ, 1}∆ae\nT\nR\n,\nwhere ∆ae\nT = f ae({θi,0}i, µ0, σ0) −f ae({θi,T }i, µT , σT ).\n30\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2024-02-19",
  "updated": "2025-01-24"
}