{
  "id": "http://arxiv.org/abs/2402.13296v1",
  "title": "Evolutionary Reinforcement Learning: A Systematic Review and Future Directions",
  "authors": [
    "Yuanguo Lin",
    "Fan Lin",
    "Guorong Cai",
    "Hong Chen",
    "Lixin Zou",
    "Pengcheng Wu"
  ],
  "abstract": "In response to the limitations of reinforcement learning and evolutionary\nalgorithms (EAs) in complex problem-solving, Evolutionary Reinforcement\nLearning (EvoRL) has emerged as a synergistic solution. EvoRL integrates EAs\nand reinforcement learning, presenting a promising avenue for training\nintelligent agents. This systematic review firstly navigates through the\ntechnological background of EvoRL, examining the symbiotic relationship between\nEAs and reinforcement learning algorithms. We then delve into the challenges\nfaced by both EAs and reinforcement learning, exploring their interplay and\nimpact on the efficacy of EvoRL. Furthermore, the review underscores the need\nfor addressing open issues related to scalability, adaptability, sample\nefficiency, adversarial robustness, ethic and fairness within the current\nlandscape of EvoRL. Finally, we propose future directions for EvoRL,\nemphasizing research avenues that strive to enhance self-adaptation and\nself-improvement, generalization, interpretability, explainability, and so on.\nServing as a comprehensive resource for researchers and practitioners, this\nsystematic review provides insights into the current state of EvoRL and offers\na guide for advancing its capabilities in the ever-evolving landscape of\nartificial intelligence.",
  "text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n1\nEvolutionary Reinforcement Learning: A Systematic\nReview and Future Directions\nYuanguo Lin, Fan Lin, Guorong Cai, Hong Chen∗, Lixin Zou, Pengcheng Wu\nAbstract—In response to the limitations of reinforcement\nlearning and evolutionary algorithms (EAs) in complex problem-\nsolving, Evolutionary Reinforcement Learning (EvoRL) has\nemerged as a synergistic solution. EvoRL integrates EAs and\nreinforcement learning, presenting a promising avenue for train-\ning intelligent agents. This systematic review firstly navigates\nthrough the technological background of EvoRL, examining the\nsymbiotic relationship between EAs and reinforcement learning\nalgorithms. We then delve into the challenges faced by both\nEAs and reinforcement learning, exploring their interplay and\nimpact on the efficacy of EvoRL. Furthermore, the review\nunderscores the need for addressing open issues related to\nscalability, adaptability, sample efficiency, adversarial robustness,\nethic and fairness within the current landscape of EvoRL.\nFinally, we propose future directions for EvoRL, emphasizing\nresearch avenues that strive to enhance self-adaptation and self-\nimprovement, generalization, interpretability, explainability, and\nso on. Serving as a comprehensive resource for researchers and\npractitioners, this systematic review provides insights into the\ncurrent state of EvoRL and offers a guide for advancing its\ncapabilities in the ever-evolving landscape of artificial intelligence.\nIndex Terms—Evolutionary reinforcement learning, Evolution-\nary algorithms, Reinforcement learning, Policy search, Evolution\nstrategy\nI. INTRODUCTION\nR\nEINFORCEMENT learning utilizes agents for au-\ntonomous decision-making, focusing on long-term\naction strategies, particularly effective in tasks like in-\ndustrial automation and personalized recommendation systems\n[1, 2, 3]. However, reinforcement learning faces challenges\nsuch as parameter sensitivity and sparse rewards, leading\nto issues in learning efficiency and adaptability [4, 5]. On\nthe other hand, evolutionary algorithms (EAs), inspired by\nDarwin’s natural selection, excel in solving complex, multi-\nobjective problems in large solution spaces [6, 7]. Despite their\nrobust search capabilities, EAs are limited by hyper-parameter\nY. Lin is with the School of Computer Engineering, Jimei University, China,\nand also with the School of Informatics, Xiamen University, China. Email:\nxdlyg@jmu.edu.cn.\nF. Lin is with the School of Informatics, Xiamen University, China. Email:\niamafan@xmu.edu.cn.\nG. Cai is with the School of Computer Engineering, Jimei University, China.\nEmail: guorongcai.jmu@gmail.com.\nH. Chen is with the Institute of Systems Science, National University of\nSingapore, Singapore. Email: chenhong@u.nus.edu.\nL. Zou is with the school of Cyber Science and Engineering, Wuhan\nUniversity, Wuhan, China. Email: zoulixin@whu.edu.cn.\nP. Wu is with the Joint NTU-UBC Research Centre of Excellence in Active\nLiving for the Elderly (LILY), Nanyang Technological University, Singapore.\nEmail: pengchengwu@ntu.edu.sg.\n∗Corresponding author\nsensitivity and struggle in high-dimensional environments,\naffecting their optimization efficacy [8, 9].\nHence, Evolutionary Reinforcement Learning (EvoRL)\nwhich integrates EAs with reinforcement learning, to address\nthe limitations of each method [10]. EvoRL maintains mul-\ntiple policies within a population and utilizes evolutionary\noperations like crossover and mutation to refine these policies,\nenhancing the policy-making process inherent in reinforcement\nlearning. Simultaneously, EvoRL leverages the global search\ncapabilities of EAs for exploring the policy space and opti-\nmizing various components like agents and actions. EvoRL’s\ncore mechanism, combining the precision of policy gradients\nwith EAs’ global search, enables effective solutions in com-\nplex, high-dimensional environments [11, 12]. Additionally,\napproaches like EGPG and CERL within EvoRL focus on\ncollaborative efforts of multiple agents, boosting performance\nin intricate tasks [13].\nEvoRL has been applied in various domains, demonstrating\nits versatility and effectiveness. For instance, EvoRL enhances\nsample efficiency in reinforcement learning, a crucial aspect\nfor practical applications [14, 15]. In embodied intelligence,\nEvoRL fosters complex behavior through the integration of\nlearning and evolution, offering new perspectives in this field\n[16]. Another significant application of EvoRL lies in qual-\nity diversity for neural control, contributing to the advance-\nment of neural network-based control systems [17]. EvoRL’s\nintegration with Deep Reinforcement Learning (DRL) has\nbeen instrumental in promoting novelty search, expanding\nthe boundaries of exploration in reinforcement learning [12].\nFurthermore, the SUPER-RL approach within EvoRL, which\napplies genetic soft updates to the actor-critic framework,\nhas been shown to improve the searching efficiency of DRL\n[18]. Early research highlighted the importance of meta-\nparameter selection in reinforcement learning, a concept that\nremains relevant in current EvoRL applications [19]. Lastly,\nthe combination of population diversity from EAs and policy\ngradient methods from reinforcement learning has led to\nnovel approaches like differentiable quality diversity, further\nenhancing the gradient approximation capabilities in EvoRL\n[20].\nRecent advances in EvoRL [21, 22] have demonstrated\nsignificant improvements in both reinforcement learning and\nEAs. From the perspective of reinforcement learning, EvoRL\nhas been shown to notably enhance sample efficiency and\nexpand exploratory capabilities, which are essential in address-\ning reinforcement learning’s limitations in complex and high-\ndimensional problem spaces [12, 15]. In terms of EAs, the\nintegration with reinforcement learning techniques has resulted\narXiv:2402.13296v1  [cs.NE]  20 Feb 2024\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n2\nin more adaptive and precise evolutionary strategies [18, 19].\nThis review aims to underscore the importance of EvoRL in\novercoming the inherent challenges of using reinforcement\nlearning and EA independently, highlighting its integrated\napproach in complex problem-solving scenarios.\nIn the field of EvoRL, the surveys by [23, 24] provide\nrelated insights. [23] categorizes over 45 EvoRL algorithms,\nprimarily developed after 2017, focusing on the integration\nof EAs with reinforcement learning techniques, which em-\nphasizes the mechanisms of combination rather than the ex-\nperimental results. This classification provides a structured\napproach to understanding the field’s current methodologies.\n[24] further explores this domain, examining the intersection\nof EAs and reinforcement learning, and highlights its ap-\nplications in complex environments. Although these surveys\noffer valuable insights for EvoRLs, they might benefit from\nincorporating a broader range of perspectives and analytical\napproaches, ensuring a more diverse understanding of EvoRL’s\npotential and challenges in future explorations.\nOur contribution In this review, the contribution made by\nus can be demonstrated by following four points:\n• Multidimensional classification We have taken a com-\nprehensive look at the EvoRL field, categorizing different\napproaches and strategies in detail. This detailed clas-\nsification covers not only EvoRL’s multiple techniques\nand strategies, such as genetic algorithms and policy\ngradients, but also their application to complex problem-\nsolving.\n• Multifaceted issue comments We provide an in-depth\nanalysis of the challenges and limitations of current\nresearch, contains issues encountered by reinforcement\nlearning and EAs and their corresponding EvoRL out-\ncome methods, and provide critical insights into existing\nresearch methods.\n• Constructive Open Issues This review presents a sum-\nmary of emerging topics, including scalability to high-\ndimensional spaces, adaptability to dynamic environ-\nments, sample efficiency and data efficiency, adversarial\nrobustness, ethic and fairness in EvoRL.\n• Promising Future Directions We identify key research\ngaps and propose future directions for EvoRL, high-\nlighting the need of meta-evolutionary strategies, self-\nadaptation and self-improvement mechanisms, transfer\nlearning and generalization, interpretability and explain-\nability, as well as incorporating EvoRL within large\nlanguage models.\nThe remainder of this review It begins with an overview\nof the technological background used in EvoRL, followed by\nan investigation of EvoRL’s applications and algorithms based\non existing literature. The review then analyses the challenges\nfaced by both EAs and reinforcement learning, along with the\nsolutions provided by EvoRL. Finally, it concludes with the\ndiscussion on the open issues and future directions in the field\nof EvoRL.\nII. BACKGROUND\nIn the literal sense, EvoRL combines EAs with reinforce-\nment learning. To offer a comprehensive description of EvoRL,\nthis section will provide brief introductions to both EAs and\nreinforcement learning.\nA. Evolutionary Algorithms\nEvolutionary algorithms do not refer to one specific al-\ngorithm, but the generic term of a series of sub-algorithms\nthat are inspired by natural selection and the principle of\ngenetics. Those sub-algorithms solve complex problems by\nimitating the process of biological evolution. Therefore, EAs\ncan be considered as a general concept, that consists of\nmultiple optimization algorithms which utilize the mechanism\nof biological evolution [25].\nIn this paper, five areas of EAs will be introduced, which are\nevolutionary strategy, genetic algorithm, cross-entropy method\n(CEM), population-based training (PBT) and other EAs.\nEvolutionary strategy was first proposed by Rechenberg\n[26, 27]. Evolutionary strategy optimize the solution through\npopulation generation and mutation based on principles of bio-\nlogical evolution. There is an adaptive adjustment of mutation\nstep size in this algorithm, which enables the method to search\neffectively in the solution space. Following is core formula of\nevolutionary strategy, which is applied on the adjustment of\nmutation step size:\nσ′\nj = σj · e(N(0,1)−Nj(0,1)),\n(1)\nwhere σ′\nj, σj denote the mutation step size after and before\nadjustment respectively, e(N(0,1)−Nj(0,1)) represents the adjust\nfactor of normal distribution random number, which used for\nincreasing or decreasing step size. Evolutionary strategy is\nespecially suitable for solving issues of sparse reward and\npolicy search in reinforcement learning due to its strong\ncapability of searching on large scale[24].\nGenetic algorithm is one of the most famous EAs. There\nare three essential operators which are selection, mutation,\nand crossover [28]. Population is also a core concept in\ngenetic algorithm, each individual in it represents a potential\nsolution, which will be evaluated through fitness function[29].\nDifferent from evolutionary strategy, genetic algorithm focuses\non the crossover as the main mechanism of exploration, while\nevolutionary strategy tends to rely on mutation[26]. Genetic\nalgorithm was usually applied to deal with the problem of\nhyper-parameter tuning in reinforcement learning.\nCross-entropy method defines an accurate mathematical\nframework, that explores optimal solutions based on advanced\nsimulation theory. It starts by randomizing the issue using a\nseries of probability density functions and solves the associ-\nated stochastic problem through adaptive updating [30]. The\nkey concept of CEM contains elite solution-based probability\ndensity function’s parameters updating and global searching\nability enhancement by applying mutation. Following is the\ncore formula of CEM [31]:\nv∗= arg max\nv\nEWIS(X)≥γ log f(X; v) f(X; u)\nf(X; w),\n(2)\nwhere S(X) is a performance function used to evaluate ran-\ndom variable X, f(X; v) denotes a parameterized probability\ndensity function, v is a parameter. This equation aims to\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n3\nfind a v∗to maximize the logarithmic likelihood expectation\nwhen the event S(X) ≥γ happens, γ is a threshold. That\nis, seek a v∗to make a high-performance event easier to\nhappen. This is the core idea of CEM, which is by minimizing\ncross-entropy, the sampling distribution is gradually close to\nthe high-performance region so that the high performance is\nmore easily sampled [31]. CEM offers a robust solution to\nthe challenges of local optima and computational efficiency in\nreinforcement learning [32].\nPopulation-based training can be viewed as a type of\nparallel EAs, it optimizes weights and hyper-parameters of\na neural network’s population simultaneously [33]. The algo-\nrithm starts with randomly initializing a population of models,\neach model individual will optimize their weights indepen-\ndently (similar to mutation) in training iteration. The prepared\nindividual needs to get through two stages which are ”exploit”\nand ”explore” at the population level. The former is similar\nto ”selection”, that is, replacing original model weights and\nhyper-parameters with a better performance model. Actually,\nit is a parameter transfer inside the population. The latter one\nis similar to ”mutation”, realizing the exploration of hyper-\nparameters space by adding noise, which provides diversity for\nfollowing training [33]. Due to its characteristics, PBT shows\nan excellent ability to solve issues of hyper-parametric sensi-\ntivity and sparse rewards in reinforcement learning [24, 28].\nApart from four algorithms introduced previously, there are\nseveral other EAs, e.g., genetic programming, evolutionary\ncomputing, and random search. Although these algorithms\nhave their own characteristics, they basically share some\ncommon principles like population-based search, iterative op-\ntimization, simulation of natural selection, and genetic mech-\nanisms. Genetic programming is mainly applied to optimizing\npolicy search [34], evolutionary computing focuses on fitness\nfunction problem [35] while random search could be used\nfor improving calculation efficiency of reinforcement learning\n[36].\nB. Reinforcement Learning\nAs one of the most popular machine learning methods,\nreinforcement learning has attracted a lot of attention in recent\nyears. It has shown great potential in everything from gaming\nto autonomous driving. Given that reinforcement learning has\nbeen extensively discussed and studied, we will not repeat\nits basic concepts here, but directly introduce its three main\nmethods.\nValue function approach evaluates each state s or state-\naction pair (s, a) by learning a value function vπ(s, a), and\nguides decision making according to this [37], which can also\nbe demonstrated as:\nπ(s) = arg max\na\nvπ(s, a),\n(3)\nwhere π(s) denotes the policy under state s. It can be also\ncalled as greedy policy [38]. The core concept of this approach\nis to optimize the value function to predict future rewards.\nPolicy gradient method directly optimizes the policy\nπ(a|s) itself, aiming to maximize the expected return [39].\nThis method optimizes the policy by adjusting policy parame-\nters but not the estimated value function. The policy gradient\nmethod provides a formula related to updating the policy\nparameters θ by gradient ascent on the expected return [38]:\nJ(πθ) = Eτ∼πθ[r(τ)] =\nZ\nπθ(τ)r(τ) dτ,\n(4)\nwhere J(πθ) denotes the expected return of the policy, τ is\na trajectory, r(τ) represents cumulative reward after operating\ntrajectory τ, while Eτ∼πθ demonstrates the expectation over\ntrajectories sampled from the policy πθ [38].\nActor-critic algorithm is a reinforcement learning tech-\nnique used for learning optimal policies in unknown en-\nvironments. Its core concept revolves around guiding the\npolicy improvement of the actor based on the value function\nestimations provided by the critic [37, 39]. In this algorithm,\nthe updating rule of policy parameter θ is defined as:\n∆θ = α∇log πθ(s, a)(r + γV (s′) −V (s)),\n(5)\nwhere α represents the learning rate, πθ(s, a) denotes the\npolicy, V (s) and V (s′) are the estimation of value function\nunder current and next state, respectively. r is the reward,\nwhile γ represents the discount factor towards future rewards\n[38].\nIII. METHODS OF EVOLUTIONARY REINFORCEMENT\nLEARNING\nThis section aims to provide an in-depth discussion of\nthe core mechanism in EvoRL algorithms, focusing on the\nEAs they incorporate. In this section, the components of\nreinforcement learning and EAs will be described in detail,\nto analyze their effects and advancements in the decision-\nmaking process. Besides, we will also discuss the evaluation\nmetrics used for assessing EvoRL’s performance and the\ncorresponding benchmarks. An overview of literature is shown\nas Table I.\nA. Evolutionary Strategy\nIn the expansive field of reinforcement learning, the value\nfunction method has always been one of the core research\ndirections, mainly focusing on how to effectively estimate and\noptimize the expected return under a given policy. Against this\nbackground, the natural parameter search mechanism of evo-\nlutionary strategy provides a unique approach by simulating\nthe processes of natural selection and heritable variation.\nMutation is one of the most essential operations in the evo-\nlutionary strategy, it provides the algorithm with new solution\nspace. The operation empowers the corresponding EvoRL to\neffectively adapt complex learning environments. [40] employs\na simple but effective EvoRL algorithm called AEES, which\ncontains two distinct, coexisting mutation strategies. Each two\nstrategies is connected with their population subsets. That is,\neach subset mutates in accordance with one related mutation\nstrategy. AEES applies cumulative return and convergence\nrate as evaluation metrics, and shows a better performance\nof the proposed model compared to A2C, SAC, and other\ndeep reinforcement learning (DRL) methods. Compared to\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n4\nTABLE I: Overview of Evolutionary Reinforcement Learning Algorithms with Evaluation Metrics and Benchmarks\nEvolutionary Algorithm\nRL Algorithm\nMethod\nEvaluation Metric\nCompared Algorithm\nEvolutionary Strategy\nValue Function\nAEES [40]\nConvergence, Cumulative return\nA2C,SAC,DDPG,TRPO\nOHT-ES [41]\nAsymptotic performance,learning speed\nTD3\nPolicy Gradient\nR-R1-ES [42]\nReward\nOpenAI-ES,NS-ES\nCCNCS [43]\nTime budget\nPPO,A3C\nActor-Critic\nGRL [44]\nMakespan,Computation complexity\nPPO,L2D\nZOSPI [45]\nReward\nSAC,TD3,OAC,OURS\nEPG [46]\nReturn, KL\nPPO\nES-TD3 [47]\nMean,std,Median\nTD3,CEM-RL\nGenetic Algorithm\nValue Function\nA-MFEA-RL [48]\nTime budget\nSAC,PPO\nMAERL [49]\nMAE,RMSE,MME\nTD3\nERLGA [50]\nReturn\nNA\nCOE-RL [51]\nTime budget,episode\nNA\nPolicy Gradient\nAMODE-DRL [52]\nTime budget\nDDPG,DDQN\nMERL [53]\nConvergence\nPPO,IPG\nMetaPG [54]\nEntropy,Return\nSAC\nActor-Critic\nSI-LFC [55]\nFrequency deviation, Generation cost\nMADDPG,MATD3\nECRL [56]\nConstraint, Reward\nIPO\nPDERL [57]\nReward\nTD3,PPO\nSERL [58]\nStep\nDDPG\nQD-PG-PF [59]\nTraining curve\nDPG\nERL-Re [60]\nReturn\nTD3\nCross-Entropy Method\nValue Function\nQT-OPT [61]\nSuccess rate\nNA\nPolicy Gradient\nGRAC [62]\nReturn\nTD3,SAC,DDPG,TRPO,CEM\nActor-Critic\nCEM-RL [63]\nTraining curve\nTD3\nPGPS [64]\nReturn\nCEM,PPO,DDPG,CERL,SAC\nCSPC [65]\nReturn\nSAC,PPO,CEM\nSAC-CEPO [66]\nReturn\nSAC\nDEPRL [67]\nReturn\nCEM,TD3\nPopulation-Based Training\nPolicy Gradient\nSBARL [4]\nNA\nTD3\nEARL [68]\nNPV\nSAC\nActor-Critic\nEAS-RL [69]\nReturn\nTD3\nPS3-TD3 [70]\nReward\nTD3\nARAC [71]\nReturn\nCERL,TD3\nSOS-PPO [72]\nReward\nPPO\nMERL [73]\nSuccess rate\nMADDPG,MATD3\nEPC-MARL [74]\nReward\nMADDPG\nEMOGI [75]\nWin rate,Duration,Distance\nA3C\nOthers\nValue Function\nECRL [76]\nReward\nNA\nAGPRL [77]\nQ-value\nNA\nTDDQN [78]\nQ-value\nDQN\nDDQN-RS [79]\nMLHP\nDDQN\nPolicy Gradient\nEGPRL [80]\nReward\nNA\nGPRL [34]\nError\nNA\nGPFDM [81]\nReward\nNA\nActor-Critic\nFiDi-RL [36]\nNA\nNA\n[40], the OHT-ES algorithm in [41] more focuses on adjusting\nkey parameters of the reinforcement learning method by\nevolutionary strategy, hence improving the adaptability and\nefficiency of the algorithm. [41] proves that OHT-ES performs\nbetter than traditional DRL (e.g., TD3) in learning speed.\nDifferent from value function, the introduction of evolu-\ntionary strategy in policy gradient method provides a brand\nnew perspective. The realization idea of R-R1-ES [42] is\ndifferent from [40], R-R1-ES put special emphasis on the\ndirect optimization of the policy itself, which applies the Gaus-\nsian distribution model N(θt, σ2Ct) and restart mechanism\nto update the searching direction, where θt ∈Rn represents\ndistribution mean, σt > 0 denotes mutation strength, Ct is a\nn-dimensional covariance matrix at t iteration. The update rule\nof Ct is given as:\nCt = (1 −ccov)I + ccovptpT\nt ,\n(6)\nwhere ccov ∈(0, 1) is the changing rate of covariance matrix,\nI denotes unit matrix, pt is a vector which represents primary\nsearch direction. The model performs better than NS-ES\n(Novelty Search-Evolutionary Strategy) according to reward\nevaluation.\nBesides, the ZOSPI model [45] reveals the potential of\ncombining evolutionary strategy and actor-critic algorithm.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n5\nCompared to R-R1-ES [42], ZOSPI chooses to optimize\npolicy from global and local aspects, which both exploit the\nadvantages of the global value function and the accuracy of\npolicy gradient to the full.\n∇ϕJ = E[∇aQw(st, a)|a = πθt(st) + πϕt(st)∇ϕπϕt(st)],\n(7)\nwhere ∇ϕJ is the gradient of object function J to perturbation\nnetwork’s parameter ϕ, ∇aQw(st, a) represents the gradient of\nvalue function to action a, πθt(st) denotes the action selected\nby current policy, πϕt(st) is the output of perturbation network\nunder time step t and status st, ∇ϕπϕt(st) denotes the gradient\nof perturbation network to its parameter ϕ. The approach not\nonly improves sample efficiency but expands the possibility\nof multi-modal policy learning, paving a novel trajectory for\nactor-critic algorithm.\nB. Genetic Algorithm\nGenetic algorithm-based EvoRLs are different from tradi-\ntional methods, they focus on applying genetic diversity to\nthe policy search process, which makes the algorithm find an\neffective and stable policy in a highly complex environment.\nOne of the valuable contributions of genetic algorithms is\nthe variety of parameters and strategies provided by crossover\noperation. [49] introduces a MAERL method that mainly\nfocuses on parameter optimization in the processing industry.\nThe method consists of multi-agent reinforcement learning,\nGraph Neural Networks (GNNs), and genetic algorithms. The\nERLGA [50] method discusses the combination of genetic\nalgorithm and off-policy reinforcement learning, finally reach-\ning a better performance on return than existing methods.\nMoreover, [54] proposes a MetaPG algorithm to optimize\ndifferent reinforcement learning targets by multi-objective\nsearching standards, and consider individual reinforcement\nlearning targets via Non-dominated Sorting Genetic Algorithm\nII (NSGA-II).\nLperf\nπ\n= E(st,at,st+1) D[log(min(π(˜at+1|st+1)γ))\n−min Qi(st, ˜at)],\n(8)\nwhere π(˜at+1|st+1) denotes the policy probability of taking\naction ˜at+1 given a next state st+1, γ represents discount\nfactor, Qi(st, ˜at) is action value function. The formula in [54]\ndenotes the policy loss function. MetaPG is able to improve\nabout 3% and 7% in performance and generalization compared\nto Soft Actor-Critic (SAC) by adjusting the loss function.\nSimultaneously, genetic algorithms play a crucial role in fine-\ntuning the intricacies of policy evolution, [57] develops a\nPDERL method to solve the scalability issues caused by\nsimple genetic encoding in traditional algorithms. The PDERL\napplies the following formula to define the proximal mutation\noperator:\ns =\n1\n|A|NM\nX\ni\n∥∇θµθ(si)∥2,\n(9)\nwhere s represents the sensitivity of action to weight pertur-\nbation, |A| is the size of action space, NM denotes the sample\nsize used for calculating sensitivity, ∇θµθ(si) is the gradient\nof policy network to its parameter θ, to evaluate the sensitivity\nof policy changes to parameter under state si.\nIn addition, genetic algorithms can be applied to tackle com-\nplex reinforcement learning problems that demand extensive\ninteraction with the environment, [58] proposes a SERL which\ncontains a Surrogate-assisted controller module. The module\ncombines genetic algorithm and actor-critic algorithm, where\nthe genetic algorithm here is mainly used for evaluating the\nfitness of the genetic population. The method applies surrogate\nmodels to predict the environmental performance of individ-\nuals, which decreases the requirements of direct interaction\nwith the environment and leads to a lower computational cost.\nC. Cross-Entropy Method\nIn the field of multiple EvoRLs research, CEM as a core\ntechnique, mainly focuses on selecting elites to update policy\ndistribution, so that policy evolves in a better direction. The\nkey concept of CEM-based EvoRL is it does not rely on com-\nplex gradient calculation, but processes iterative optimization\nof policies by statistical methods.\nThe CEM-RL [63] is a typical example, it combines CEM\nwith policy gradient, to balance exploration and exploitation\n[63]. [66] proposes a SAC-CEPO method combining CEM and\nSAC. More specifically, SAC-CEPO samples optimal policy\ndistribution iteratively and applies it as the target of policy\nnetwork update. The key formula of SAC-CEPO is:\nJ(π) = Eπ\n\"X\nt\nγtr(st, at) + αH(π(·|st))\n#\n,\n(10)\nwhere J(π) represents the performance of policy π, Eπ is\nthe expectation under the policy π, γ denotes discount factor,\nr(st, at) indicates the reward of action at under state st, α\nstands for the model parameter.\nNot only can CEM optimize the policy network effectively,\nbut it can also enhance the overall decision quality and algo-\nrithmic efficiency through the statistical evolution of the value\nfunction. [61] introduces the QT-OPT method, which applies\nCEM to optimize the value function of reinforcement learning.\nQT-OPT shows a good ability on success rate evaluation\ncompared to existing algorithms. [62] develops an algorithm\nthat exploits CEM to seek the optimal action with maximal\nQ-value, called GRAC. The combination of EAs and value\nfunction makes GRAC exceed TD3, SAC, and other popular\nDRLs in OpenAI gym’s six continuous control missions.\nCEM also demonstrates a robust capability in guiding pop-\nulation evolution and iteratively optimizing the entire policy\nspace, thereby expanding its application in the field of rein-\nforcement learning. [64] proposes a PGPS that considers CEM\nas its core component. In PGPS, CEM is applied to generate\nthe next population based on current evaluation results, to\ncreate a higher return (defined as the sum of instant rewards\nwithin a certain number of steps). PGPS performs better than\nmultiple DRLs such as DDPG, PPO, SAC, etc. in several\nMuJoCo environments.\nD. Population-Based Training\nPBT-based EvoRL shows the potential in multiple research\nfields. The core concept of it is to adjust parameters and\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n6\nhyper-parameters of the algorithm dynamically in the training\nprocess, so that realizes a more effective and flexible learning\nin complicated environments.\nTherefore, [4] introduces a parameter control strategy train-\ning method for EA and swarm intelligence (SI) algorithms\ncalled SBARL. PBT in SBARL is applied to evolve parameters\nand hyper-parameters in reinforcement learning, the results\nof the experiment demonstrate that SBARL performs better\nthan traditional DRL TD3. EARL [68] is a framework that\ncombines EA (more specifically, PBT) and reinforcement\nlearning. The key concept of EARL is the collaborative work\nof EA and reinforcement learning could facilitate the learning\nand evolving process. [73] proposes a MERL algorithm, that\nrealizes individual and team objectives by combining gradient-\nless and gradient-based optimizers. Among them, The policy\nof gradient-based will be added to the population evolving\nregularly, which enables EAs to leverage skills learned through\ntraining on individual-specific rewards to optimize team goals\nwithout relying on reward shaping. Besides the policy space,\nPBT can influence the action space in reinforcement learning,\nemphasizing the importance of optimizing action decisions.\n[69] proposes an EAS-RL method, it exploits actions selected\nby reinforcement learning’s policy to generate a population,\nand processes particle swarm optimization to evolve the pop-\nulation iteratively. The key concept of EAS-RL is choosing to\noptimize action space but not policy space, the definition of\nloss function in action space is:\nLevo(θ, A) = E(si,ae\ni )∼A[∥µθ(si) −ae\ni∥2],\n(11)\nwhere the state si and the evolutionary action ai in this formula\nare sampled respectively from the archive A, while θ denotes\nthe learning parameters in the reinforcement learning policy\nµθ. The proposed model behaves better than TD3 in MuJoCo\nenvironments.\nBeyond optimizing parameters and actions, PBT places\nspecific emphasis on automatically adjusting reward functions,\ne.g., [75] focuses on game-AI generation. Therefore, they\npropose a PBT-based EvoRL, EMOGI framework. EMOGI\nconsiders the reward function as a part of candidate objects,\nrealizing the auto-adjustment of parameters by EAs. EMOGI\napplies multi-objective optimization to select policies with\ndistinct behaviors, to ensure population diversity. The key\nprocess of initialization of EMOGI, that is, randomly initialize\na candidate population consisting of policy parameters and\nreward weights:\nP = {πθ1Rw1, ..., πθnRwn},\n(12)\nwhere P denotes the population of candidates, each candidate\nconsists of two parts, policy parameter πθ, and weight of\nthe reward Rw. The size of the population is decided by n,\nrepresenting the amount of candidates in the population.\nE. Other EAs\nThe preceding chapters covered classical algorithms based\non evolutionary strategies, genetic algorithms, CEM, and PBT,\neach demonstrating notable results in their respective applica-\ntion scenarios. Additionally, there are other promising EvoRL\nmethods that, while less commonly employed, deserve atten-\ntion. These include random search-based approaches designed\nto enhance efficiency by streamlining the search process,\ngenetic programming methods that optimize strategies through\nsimulation of biological genetic processes, and evolutionary\ncomputing algorithms that emphasizes the use of principles of\nevolutionary theory to improve the learning process.\nRandom search simplifies the parameter optimization pro-\ncess while maintaining a certain level of exploration ability.\nThis approach can efficiently find solutions in complex tasks.\nDDQN-RS [79] applies random search to randomly sample\nindividuals from the population by Gaussian distribution. Eval-\nuate their fitness according to the reward got from one round\nof running in the environment. Compared to Double Deep Q-\nNetwork (DDQN), the proposed model performs better than it\ndoes in the mission of keeping the vehicle close to the center\nof the lane for the longest distance.\nCompared to the efficient parameter optimization discussed\nin [79], genetic programming in [78] demonstrates a capability\nfor in-depth optimization of computation graphs. In this frame-\nwork, genetic programming is applied to search in computation\ngraph space, and these graphs compute the minimal objective\nfunction required by the agent. Genetic programming improves\nthese computation graphs by simulating the evolving process\nof creatures. The proposed model applies the loss function\nfrom Deep Q-Network (DQN) as a key component:\nLDQN = (Q(st, at) −(rt + γ · max\na\nQtarg(st+1, a)))2, (13)\nwhere Q(st, at) indicates the Q-value under current state\nst and action at, rt represents instant reward, γ denotes\ndiscount factor, maxa Qtarg(st+1, a) stands for the maximal\nexpectation of Q-value under next state st+1. By applying this\nloss function, the proposed method more focuses on a more\naccurate estimation of the Q-value, to solve the overestimate\nissue. The experiment result shows that the DQN modified by\ngenetic programming behaves better than the original DQN in\nQ-value estimation [78].\nSimilarly, [80] proposes an EGPRL that applies genetic\nprogramming to search on computation graph space, finally\nminimizing the objective function. EGPRL allows agents could\noperate under multiple environments including OpenAI Gym’s\nClassic Control Suite. The experiment result shows that the\nproposed model owns a competitive generalization ability\nand efficiency. Different from [78] which considers accurate\nestimation of Q-value as its core concept, EGPRL more\nfocuses on the hierarchical structure of memory coding and\nmultitasking.\nBesides, ECRL [76] applies evolutionary computing to opti-\nmize parameters in reinforcement learning. More specifically,\nevolutionary computing evaluates the fitness function of each\nset of parameters, to find out the optimal solution in parameter\nspace iteratively. The fitness function in ECRL is defined as:\nfitness function = −2(average reward) + error, (14)\nthe formula combines two indexes which are average reward\nand error, to maximize the performance and stability of the\nreinforcement learning algorithm.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n7\nPopulation\n(Policies, parameters)\nPopulation \nInitialization\nMutation\nSelection\nEnvironment\nReplace\nPolicy \nNetwork\nValue \nNetwork\nExperience \nReplay\nState Information\nAgents\nLearning/Fill\nFill\nInteract\nGuiding\nEvolutionary Strategy\nReinforcement Learning\nUpdate\nEvaluation\nFig. 1: A General Workflow of ES-based EvoRL.\nThe core evolutionary operation here is the mutation,\nwhich enhances the diversity of the population.\nPopulation\n(Policies, parameters)\nPopulation \nInitialization\nCrossover\nEnvironment\nReplace\nPolicy \nNetwork\nValue \nNetwork\nMemory\nState Information\nAgents\nLearning/Fill\nFill\nInteract\nGuiding\nGenetic Algorithm\nReinforcement Learning\nUpdate\nSelection\nMutation\nEvaluation\nStore\nFig. 2: A General Workflow of GA-based EvoRL.\nThe EvoRL retains good genes through crossover to\nproduce better individuals.\nWe have discussed the core mechanism of EvoRL in the\nabove contents, EvoRL applies EAs to optimize the decision\nprocess of reinforcement learning by simulating the opera-\ntion of natural evolution. In this paper, evolutionary strategy,\ngenetic algorithm, CEM and PBT primarily emphasize and\nexplore their potential to develop effective and robust strategies\nin challenging environments. The general workflows of the\nmost frequently used EvoRL methods, ES-based and GA-\nbased, are depicted in Fig.1 and Fig.2, respectively. Evolu-\ntionary strategy primarily emphasizes the direct optimization\nof policies and behaviors through the simulation of natural\nselection and genetic variation. In contrast, Genetic algo-\nrithm focuses on searching for effective and stable policies\nthrough genetic diversity and crossover operations. The CEM\niteratively optimizes strategies through statistical methods,\nwith a particular emphasis on elite selection to guide policy\ndevelopment. The PBT demonstrates flexibility in adjusting\nparameters and hyper-parameters, particularly in the automatic\nadjustment of reward functions. Random search improves\nexploration efficiency by simplifying the search process. In\ncontrast, genetic programming exhibits a deeper complexity\nin the optimization of computation graphs. While these algo-\nrithms differ in methods and emphasis, they share a common\ncore objective: to enhance the reinforcement learning process\nby applying principles from evolutionary theory, thereby im-\nproving the performance and adaptability of policies.\nIV. CHALLENGES IN EVOLUTIONARY REINFORCEMENT\nLEARNING\nIn this chapter, we delve into the challenges encountered\nwhen employing reinforcement learning and EAs indepen-\ndently. We specifically analyze how these challenges under-\nscore the importance and advantages of combining reinforce-\nment learning and EA into EvoRL. While reinforcement learn-\ning and EA each exhibit significant strengths in addressing\ncomplex problems, they also have evident limitations. For\ninstance, reinforcement learning commonly faces challenges\nsuch as parameter sensitivity, reward sparsity, susceptibility to\nlocal optima, multitask processing difficulties, policy search\ncomplexity, and computational efficiency issues. Similarly,\nwhen applied in isolation, EA encounters challenges such as\nhyper-parameter sensitivity, multi-objective optimization com-\nplexities, computational efficiency, and the design of fitness\nfunctions. To comprehensively understand these issues, this\nchapter is divided into two subsections, addressing the chal-\nlenges faced when using reinforcement learning and EA indi-\nvidually. Furthermore, we explore how EvoRL, as a holistic\napproach, overcomes these limitations by integrating the deci-\nsion optimization capabilities of reinforcement learning with\nthe natural evolutionary simulation of EA, thereby achieving a\nmore comprehensive, efficient, and adaptive problem-solving\nmethodology. An overview of scientific problems encountered\nby both of algorithms and their corresponding resolutions is\nshown as Table II and Table III.\nA. Issues Encountered by Reinforcement Learning\nWhen applying reinforcement learning independently, we\nface many of the scientific problems mentioned above. These\nchallenges limit the scope and efficiency of reinforcement\nlearning in complex environments, suggesting that further\nmethodological innovations and technological advances are\nneeded to overcome these limitations [95]. EA, as an algorithm\nthat simulates biological evolutionary genetics, can help with\nreinforcement learning to some extent.\n1) Parameters Sensitivity\nThe performance of reinforcement learning is highly related\nto the settings of its parameters including learning rate, dis-\ncount factor, parameters of policy networks, parameters of\nreward function, etc. The adjustments and optimizations of\nthese parameters are critical to the realization of an effective\nand stable reinforcement learning process. The unreasonable\nsetting of parameters may lead to an unstable process of\ntraining, low speed of convergence, or not being able to\nfind useful policy. EAs mainly dynamically adjust parameters\nin reinforcement learning through a series of evolutionary\noperations, or increase the diversity of parameter space to find\nthe optimal combination of parameters.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n8\nTABLE II: Scientific Issues Encountered by Reinforcement Learning and Corresponding EAs Solution\nScientific Issue\nResolution\nMethod\nRL Algorithm\nEvo Algorithm\nParameters Sensitivity\nEA dynamically adjusts parameters\nSBARL [4]\nPolicy gradient\nPopulation-based training\nMAEyS [35]\nPolicy gradient\nEvolutionary computing\nEA enhances parameter space diversity\nGA-DRL [82]\nActor-critic\nGenetic algorithms\nCCNCS [43]\nPolicy gradient\nEvolutionary strategy\nNS-MERL [83]\nActor-critic\nEvolutionary strategy\nSparse Reward\nEA accelerates the search process\nPS3-TD3 [70]\nActor-critic\nPopulation-based training\nPNS-RL [5]\nActor-critic\nEvolutionary strategy\nEA enhances policy space diversity\nGEATL [84]\nActor-critic\nGenetic algorithm\nRACE [85]\nActor-cirtic\nGenetic algorithm\nLocal Optima\nEA enhances policy space diversity\nEARL [68]\nPolicy gradient\nPopulation-based training\nG2AC [86]\nActor-critic\nGenetic algorithm\nDEPRL [67]\nActor-critic\nCross-entropy method\nMulti-task\nEA enhances parameter space diversity\nA-MFEA-RL [48]\nValue function\nGenetic algorithm\nEA enhances policy combination and synergy\nEGPRL [80]\nPolicy gradient\nGenetic programming\nPolicy Search\nEA-integrated process for policy generation\nAGPRL [77]\nValue function\nGenetic programming\nGPRL [34]\nPolicy gradient\nGenetic programming\nGPFDM [81]\nPolicy gradient\nGenetic programming\nComputational Efficiency\nEA-integrated process for policy generation\nFiDi-RL [36]\nActor-critic\nRandom search\nCERM-ACER [87]\nActor-critic\nCross-entropy method\nCGP [88]\nActor-critic\nCross-entropy method\nTABLE III: Scientific Issues Encountered by Evolutionary Algorithms and Corresponding Reinforcement Learning Solution\nScientific Issue\nResolution\nMethod\nRL Algorithm\nEvo Algorithm\nHyper-parameters Sensitivity\nRL guides the selection of evolutionary operators\nESAC [8]\nAcor-critic\nEvolutionary strategy\nMulti-objective\nRL guides the selection of evolutionary operators\nAMODE-DRL [52]\nPolicy gradient\nGenetic algorithm\nRL-MOEA [9]\nValue function\nGenetic algorithm\nRL reduces the optimal solution search space\nMOEADRL [89]\nAcor-critic\nGenetic algorithm\nRL applies action to optimize the network\nMERL [53]\nPolicy gradient\nGenetic algorithm\nRL optimizes evolution process\nMOPDERL [90]\nActor-critic\nGenetic algorithm\nComputational Efficiency\nRL applies action to optimize the network\nBNEP [91]\nValue function\nGenetic programming\nRL guides the selection of evolutionary operators\nRL-GA [92]\nValue function\nGenetic algorithm\nFitness Function\nRL applies the surrogate model to assist calculation\nSMB-NE [93]\nPolicy gradient\nEvolutionary strategy\nRL optimizes evolution process\nX-DDPG [94]\nActor-critic\nGenetic algorithm\nAccording to [4], the quality of final results in parameter\ncontrol methods for metaheuristics with reinforcement learn-\ning is highly correlated with the values of these parame-\nters. Therefore, [4] introduces the SBARL method, aiming\nto dynamically adjust and evolve these parameters while\nmaintaining a static configuration. Specifically, the evaluation\nof PBT workers in SBARL aligns with their average reward in\neach training period. The less well-performing workers adopt\nthe parameter settings of the better-performing workers as a\nreference for optimization and evolution. Similar to SBARL,\n[35] dynamically selects the optimal local skills to adapt\nto the varying requirements of a multi-agent environment,\neffectively addressing the parameter sensitivity issue in re-\ninforcement learning. The proposed MAEDyS [35] not only\nutilizes policy gradient methods to learn multiple local skills\nbut also enhances its capability to handle parameter sensitivity\nin complex multi-agent settings through the dynamic selection\nand optimization of these local skills.\nBesides dynamic parameter adjustment, [82] employs a ge-\nnetic algorithm to directly search and optimize parameters. The\napproach explores the parameter space through the selection,\nmutation, and crossover operations of the genetic algorithm,\nenhancing the diversity of the parameter space. This methodol-\nogy aids in finding the optimal parameter combination for the\nproposed method. [82] presents Polyak-averaging coefficient\nupdates as shown below:\nθQ′ ←τθQ + (1 −τ)θQ′\nθµ′ ←τθµ + (1 −τ)θµ′,\n(15)\nwhere θQ′ and θµ′ are the parameters of the target Q-network\nand target policy network, respectively. θQ and θµ denote the\nparameters of the current Q-network and policy network, τ is\nthe Polyak-averaging coefficient, a factor usually close to but\ngreater than zero, used to blend the current and target network\nparameters.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n9\nLikewise, [43] focuses on optimizing parameters by en-\nhancing the diversity of the parameter space. The approach\nemployed by CCNCS [43] utilizes a framework known as\nCooperative Coevolution (CC) to break down large-scale\noptimization problems into smaller, more manageable sub-\nproblems. CC enables the independent evolution of different\nparameter combinations while maintaining their interactions,\nfacilitating the search for an optimal parameter solution.\n2) Sparse Reward\nThe sparse reward problem is a significant challenge in\nreinforcement learning. As highlighted by [5], agents face\ndifficulty obtaining sufficient reward signals to guide an effec-\ntive learning process when exploring environments with sparse\nrewards. In such scenarios, the decision-making process of\nagents may become inefficient due to the lack of immediate\nfeedback, thereby impacting the performance and training\nspeed [5].\nTo address the issue, [70] exploits the global searching\nability of EA, particularly PBT, to expedite the search process\nthrough cooperation and information sharing among multi-\nagents. [70] present the augmented loss function for an agent\ni as:\n˜L(ϕi) = L(ϕi) + 1i=bβEs∼DD(πϕi, πϕb),\n(16)\nwhere L(ϕi) is the original loss function for agent i, β is a\nconstant, Es∼D is the expectation over states sampled from\ndistribution D, D(πϕi, πϕi) is a distance function measuring\ndivergence between policies of agent i and a baseline agent b,\nand 1{i=b} is an indicator function. More specifically, the pro-\nposed model accelerates the learning process by incorporating\ninformation on the best strategy, effectively guiding the agent\nto discover an effective strategy in sparse reward environments.\nIn addition to expediting the search process, [84] introduces\na GEATL method that employs a genetic algorithm to foster\npolicy diversity through exploration in the parameter space.\nThis policy exploration aids in uncovering solutions effective\nin sparse reward environments, as it doesn’t depend on fre-\nquent or immediate reward feedback.\nSimilarly, [85] also resolve the sparse reward of multi-agent\nreinforcement learning through improving the diversity of\npolicy space. [85] proposes a RACE method which considers\neach agent as a population, explores new policy space by the\ncrossover and mutation of individuals in the population. The\nevolutionary operations are able to help reinforcement learning\nto generate diverse behavior modes. This kind of diversity\nis quite essential for those effective policy that only appears\nunder certain conditions in sparse reward environment. [85]\nalso introduces random perturbation, which is shown as:\nW ′\ni, W ′\nj = ((Wi −W di\ni ) ∪W di\nj , (Wj −W dj\nj ) ∪W dj\ni )\n= Crossover(Wi, Wj),\nW ′\nj = (Wj −W dj\nj ) ∪P(W dj\nj\n= Mutation(Wj),\n(17)\nwhere Wi and Wj stand as the chosen teams, di and dj\ndepict subsets of agent indices 1, ..., N selected at random.\nThe perturbation function, denoted as P, introduces Gaussian\nnoise to specific parameters or resets them. Wd is utilized to\ndenote the subset of policy representations corresponding to\nthe team characterized by indices d.\n3) Local Optima\nThe challenge of local optima is primarily attributed to the\nvanishing gradient during policy updating, as highlighted by\n[67]. This issue may hinder effective exploration of better\npolices in complex environments. To handle this problem, [67]\nintroduces the DEPRL method, employing CEM to enhance\npolicy diversity and improve exploration efficiency. This ap-\nproach shows a significant improvement in continuous control\ntasks and effectively reduces the risk of getting trapped in local\noptima.\nSimilarly, [68] introduces an approach that enhances policy\ndiversity by integrating reinforcement learning and EAs within\na unified framework. In the proposed EARL [68], the crucial\nconcept lies in the exchange of information between rein-\nforcement learning agents and EA populations. Reinforcement\nlearning agents acquire diverse exploration experiences from\nthe EA population, while the EA population regularly receives\ngradient information from reinforcement learning agents. This\nreciprocal interaction fosters strategy diversity, enhancing the\nstability and robustness of the algorithm. The formula of loss\nfunction in EARL is shown as:\nJπ(ϕ) = Est∼D,at∼πϕ(·|st)[α log πϕ(at|st)−min\nj=1,2Qθj(st, at)],\n(18)\nwhere Est∼D,at∼πϕ(·|st) represents the expectation over states\nst sampled from a dataset D and actions at sampled from the\npolicy πϕ given the state st.\nFor actor-critic algorithm, the diversity of policy space\nbrought by EAs can improve sample efficiency and perfor-\nmance. To this end, [86] proposes a G2AC approach that\ncombines gradient-independent and gradient-dependent opti-\nmization by integrating genetic algorithms in hidden layers of\nneural networks. The policy update gradient of G2AC is given\nas following formula:\n∇θJ(θ) = Eπ[∇θ log πθ(s, a)Aπ(s, a)],\n(19)\nwhere Eπ denotes the expectation under the policy π pa-\nrameterized by θ, ∇θ log πθ(s, a) stands for the gradient of\nthe logarithm of the policy πθ, evaluated at a specific state-\naction pair (s, a), Aπ(s, a) signifies the advantage function\nunder policy π. This approach allows models to diversify their\nexploration in the solution space and jump around as they find\nbetter areas, and G2AC increases the diversity of policies in\nthis way.\n4) Multi-task\nThe multi-task challenges in reinforcement learning arise\nprimarily from the dynamics and complexity of the real\nworld, requiring agents to handle various tasks within dis-\ntinct environments. As emphasized by [80], agents may need\nto navigate environments with both discrete and continuous\naction spaces, often characterized by partial observability. To\ntackle this issue, [80] proposes the EGPRL method, combining\nmultiple independently adapted agents to realize a synergistic\neffect among different policies, thereby enhancing multi-task\nperformance. This is further supported by the Tangled Program\nGraph (TPG) framework in [80], which leverages hierarchical\nstructures and modular memory to effectively encode and man-\nage environmental information in partially observable settings.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n10\nSimilarly, [48] introduces the A-MFEA-RL method. This\napproach employs a unified search space and genetic manipu-\nlation (e.g., crossover and mutation) to enhance the diversity of\nthe parameter space, aiding reinforcement learning in explor-\ning broader solutions in multi-task environments. Additionally,\nA-MFEA-RL utilizes adaptive knowledge transfer strategies\nto optimize the learning process across tasks, effectively\nbalancing information exchange to prevent negative transfer\nwhile promoting beneficial synergies [48].\nWhile the two mentioned articles propose distinct solutions\nto the multi-task challenge in reinforcement learning, they\nboth employ EAs as a common strategy to enhance policies.\nWhether aiming to boost policy synergy or increase param-\neter space diversity for more effective policy search, both\napproaches demonstrate their efficacy in effectively addressing\nthe multi-task issue.\n5) Policy Search\nIn reinforcement learning, the policy search problem re-\nvolves around determining the optimal policy to maximize\nthe cumulative reward for an agent interacting with its environ-\nment. A proficient policy search is pivotal for efficient learning\nand preventing potentially sub-optimal or erroneous behavior.\nAs highlighted in [34], conventional reinforcement learning\napproaches face challenges when confronted with intricate and\nhigh-dimensional state spaces. These difficulties can impede\nthe learning process, resulting in subpar strategy quality.\nTherefore, [34] proposes a GPRL method which is able\nto autonomously learn the policy equation. In this method,\ngenetic programming is used to generate the basic algebraic\nequations that form the reinforcement lraning strategy from\nthe pre-existing default state-action trajectory sample. The\nkey to GPRL is that it learns interpretable and moderately\ncomplex policy representations from the data in the form of\nbasic algebraic equations.\nIn a similar vein, genetic programming is applied in a two-\nstage process in another study [77]. Initially, programs are gen-\nerated in a simulated environment using genetic programming,\nserving as candidate solutions for various tasks. Subsequently,\nthese actions, derived from genetic programming, are adapted\nto the operational characteristics of specific real robots through\nreinforcement learning, particularly Q-learning. The pivotal\naspect of this approach is that the programs created by\ngenetic programming provide an effective starting point for\nreinforcement learning, thereby accelerating and enhancing the\nprocess of policy search and adaptation.\nExpanding on this concept, another study [81] introduces\na genetic programming-based method aimed at automating\nfeature discovery in reinforcement learning. Central to this\napproach is the use of genetic programming to generate a\nset of features that significantly enhance the efficiency of\nreinforcement learning algorithms in learning strategies. The\nkey lies in utilizing genetic programming to automatically\nunearth useful features from an agent’s observations, capturing\nthe intricate non-linear mappings between states and actions.\nThis, in turn, improves both the efficiency and effectiveness\nof the policy search process [81].\n6) Computational Efficiency\nTraditional reinforcement learning methods often grapple\nwith the high cost and inefficiency of calculating the derivative\nof the optimal target, leading to poor stability and robustness\nin complex tasks [36]. This challenge is compounded when\nemploying complex neural networks (NNs) as control strate-\ngies in most current approaches. These deep NNs, despite\ntheir potential for enhanced performance, complicate param-\neter tuning and computation. In response, [36] introduces\nFiDi-RL, a novel method that integrates DRL with finite-\ndifference (FiDi) policy search. By combining DDPG and\nAugmented Random Search (ARS), FiDi-RL enhances ARS’s\ndata efficiency. Empirical results validate that FiDi-RL not\nonly boosts ARS’s performance and stability but also stands\ncompetitively among existing DRL methods.\nComplementing this, the CERM-ACER algorithm [87] ad-\ndresses computational efficiency in reinforcement learning\nthrough an EA perspective, blending the CEM with the actor-\ncritic with experiential replay (ACER). This synergistic ap-\nproach enables policy parameters to make substantial jumps\nin the parameter space, allowing for more assertive updates\nper iteration. Consequently, CERM-ACER not only stabilizes\nthe algorithm but also diminishes the necessity for extensive\nsample collection in complex environments, thus boosting\ncomputational efficiency.\nSimilarly, the CGP algorithm [88] enhances the computa-\ntional efficiency of Q-learning in continuous action domains.\nBy fusing CEM with deterministic neural network strategies,\nCGP employs heuristic sampling for Q function training\nwhile simultaneously training a policy network to emulate\nCEM. This is mathematically represented by the L2 regression\nobjective:\nJ(ϕ) = Es∼ρπCEM (∇πϕ∥πϕ(st) −πCEM(st)∥2),\n(20)\nwhere J(ϕ) denotes the objective function for training the\npolicy network πϕ, st represents a state in the state space,\nπCEM is the policy generated by CEM, and ρπCEM is the\ndistribution over states as determined by the CEM policy.\nThis strategy eliminates the need for costly sample iterations\nduring inference, significantly accelerating inference speed and\nreducing computational demands. CGP’s efficacy in execution\nefficiency makes it particularly suited for real-time, compute-\nsensitive tasks.\nIn summary, these approaches demonstrate how EAs can\nrevolutionize reinforcement learning’s computational effi-\nciency. Starting from policy generation, these methods adeptly\nnavigate the complexities of reinforcement learning, offering\nmore efficient, stable, and robust solutions.\nB. Issues Encountered by Evolutionary Algorithm\nEven though EAs has many advantages, its own shortcom-\nings can not be ignored. Small changes of hyper-parameters\nmay cause huge fluctuations in performance. Multi-objective\noptimization needs to strike a balance among conflicting\nobjectives, and the limitation of computational efficiency is a\nproblem that cannot be ignored. Designing a fitness function\nwhich can reflect the nature of the problem and is easy to\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n11\ncalculate is one of the complex problems. The addition of\nreinforcement learning comes to the fore in these challenges,\noffering promising solutions to problems. Through adaptive\nlearning, reinforcement learning assists EAs to find balance\nand optimize decision-making process in multi-objective and\ndynamic environment. Reinforcement learning shows high\nflexibility and efficiency in hyper-parameter adjustment and\nfitness function design. Therefore, EvoRL plays a key role in\nimproving performance and application effectiveness.\n1) Hyper-parameters Sensitivity\nHyper-parameter sensitivity is a key challenge where the\nalgorithm’s performance hinges on hyper-parameter settings.\nEven slight adjustments can cause notable variations in results,\ncomplicating tuning and limiting the algorithm’s adaptability\nacross tasks. Designers must invest significant time and re-\nsources in parameter tuning to find the best configuration [8].\nTherefore, [8] introduces the ESAC method, a combination\nof evolutionary strategy and SAC, to address hyper-parameter\nsensitivity. The reinforcement learning aspect in ESAC assists\nEA in overcoming this issue through its adaptive mechanism.\nSpecifically, ESAC incorporates Automatic Mutation Tuning\n(AMT) to maximize the mutation rate, diminishing the need\nfor precise hyper-parameter settings. The SmoothL1 loss func-\ntion in AMT is defined as:\nSmoothL1(xi, yi) =\n\u001a 0.5(xi −yi)2,\nif|xi −yi| < 1\n|xi −yi| −0.5,\notherwise\n(21)\nThe SmoothL1 loss function, a combination of absolute loss\nand square loss, aims to mitigate the impact of abnormal\nvalues on model training. Simultaneously, ESAC employs evo-\nlutionary strategies to explore the strategy in weight space and\nleverages the gradient-based knowledge of the SAC framework\nto utilize policies. This dual approach helps maintain a balance\nbetween exploration and exploitation, reducing reliance on a\nsingle hyper-parameter setting.\n2) Multi-objective\nMulti-objective optimization problem refers to how to effec-\ntively balance multiple conflicting objectives in an uncertain\nenvironment. For example, in some production environments\n[9], the two goals of reducing costs and increasing production\nefficiency need to be considered simultaneously, and these\ngoals often contradict each other, which increases the com-\nplexity of problem solving. The characteristic of this kind of\nmulti-objective problem is that a way must be found to balance\nthe interests of different objectives in order to achieve the\noptimal combined effect. This not only challenges algorithm\ndesigners, but also requires the effectiveness and feasibility of\npractical application scenarios.\nReinforcement learning offers a multi-faceted solution to\naddress multi-objective problems in Evolutionary Algorithms\n(EAs), as outlined by [9]. The proposed RL-MOEA, built upon\nthe NSGA-II framework, introduces reinforcement learning\ncapabilities. The algorithm’s core mechanism dynamically\nselects the most suitable crossover and mutation operators in\neach iteration, employing a strategy based on Q-learning. The\ncentral formula of the Q-learning strategy is:\nnewQs,a = (1 −α) · Qs,a + α · (Rs,a + γ · max ˜Qs,a), (22)\nwhere newQs,a and Qs,a represent the Q value after and\nbefore updating under state s and action a. max ˜Qs, a denotes\nthe maximal Q value of the next state, and Rs, a stands for\nthe reward of taking action a under state s. In multi-objective\noptimization, different crossover and mutation operators have\nvaried effects on different targets. RL -MOEA utilizes Q-\nlearning strategies to choose the most appropriate operator in\neach iteration, effectively balancing and optimizing individual\nobjectives based on historical and current evaluation informa-\ntion.\nIn the realm of reinforcement aiding EAs in selecting evo-\nlutionary operators, [52] proposes an AMODE-DRL method.\nThis method integrates DRL into Multi-Objective Differential\nEvolution (MODE). Through this integration, DRL guides\nMODE in selecting suitable mutation operators and parame-\nters, resulting in improved solutions compared to other multi-\nobjective evolution and adaptive MODE algorithms.\nBuilding upon the concept of reinforcement learning aiding\nevolutionary algorithms in selecting evolutionary operators, as\nexemplified by [52] with their AMODE-DRL method, [89]\ntake a different approach. In their study, the integration of the\nactor-critic from DRL into evolutionary algorithms specifically\ntargets the challenge of multi-objective optimization. By strate-\ngically focusing on non-zero decision variables and employing\na reward function that compares HyperVolume (HV) indica-\ntors between states, the actor-critic model effectively narrows\ndown the search space. This method not only streamlines the\noptimization process but also ensures that the evolutionary\nalgorithm is directed towards the most relevant and impactful\nsolutions, enhancing both efficiency and outcome quality in\nmulti-objective scenarios.\nDifferent from optimizing solution search space, [53] uti-\nlizes reinforcement learning to refine neural network policy\nparameters. This approach targets a dual-objective issue in\nfinancial cloud services, focusing on load imbalance and server\nidle time. Their reinforcement learning model, involving rout-\ning decisions across servers, eschews the need for intermediate\nrewards by employing cumulative objectives as the reward\nmetric. This strategy effectively narrows down the optimal\nsolution search space, enhancing efficiency in tackling multi-\nobjective optimization challenges in EAs.\nReinforcement learning can also address the multi-objective\nissue through optimizing the evolution process. In [90], the\nproblem of multi-objective optimization can be defined as:\nmax\nπ F(π) = max\nπ [f1(π), f2(π), ..., fm(π)],\n(23)\nwhere m denotes the number of objectives, π represents the\npolicy, fi(π) = Jπ\ni . The study uses a two-stage framework\nin evolutionary algorithms, where reinforcement learning op-\ntimizes both network parameters and evolutionary operator\nselection. This approach reduces the search space in multi-\nobjective optimization. Crucially, the NSGA-II algorithm is\nemployed for selection and sorting, enhancing solution diver-\nsity and quality.\n3) Computational Efficiency\nIn [93], a significant challenge in EAs is their computa-\ntional inefficiency, particularly when the problem requires a\nlarge number of parallel evaluations and is fast to evaluate.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n12\nTo address this, the study integrates reinforcement learning\nwith gene programming. The advantage calculation in these\nmethods is given by:\nAϕ(st, at) = Rt(st, at) −Vϕ(st),\n(24)\nwhere Rt(st, at) represents the reward function, Vϕ(st), is\nthe state value estimated by the critic. This targeted approach\nenhances the efficiency of finding optimal solutions in EAs,\nparticularly in complex environments with large search spaces.\nIn [92], the study transitions from optimizing networks in\nreinforcement learning to guiding the selection of evolutionary\noperators in EAs to enhance computational efficiency. They\nadopt a Q-learning method in reinforcement learning to direct\nthe evolutionary process. This approach addresses the compu-\ntational inefficiency of EAs, particularly evident when multiple\nparallel evaluations are required. By utilizing Q-learning, the\nstudy effectively selects mutation operators for optimization,\nwhere the specific operation for each individual’s evolution is\ndetermined based on Q-values. This method ensures a more\nfocused and efficient search within the evolutionary process.\n4) Fitness Function\n[93] points out that one disadvantage of EAs is that it\ncan require a lot of functional evaluation and does not fully\nutilize the information available in each fitness evaluation,\nespecially when fitness evaluation is costly. Therefore, [93]\nproposes a SMB-NE method, which applies reinforcement\nlearning to deal with this issue. The proposed method is to\npartially replace the expensive fitness function by using a\nSurrogate Model-Based Optimization (SMBO). These proxy\nmodels utilize data-driven models to simulate fitness functions,\nthereby reducing evaluation costs. In MoutainCar mission\nmentioned in [93], the key formula is:\nfitness : y(x) = −(maxHeightepisode + Rewardepisode\n100\n),\n(25)\nit signifies the altered fitness function, where y(x) denotes the\noutput of neural networks, maxHeightepisode represents the\nmaximal height reached in one attempt, Rewardepisode stands\nfor the sum of reward obtained from this attempt.\nAnother article also pointed out the fitness function problem\nin EAs, especially genetic algorithm. [94] considers traditional\ngenetic algorithm cannot effectively solve machine learning\nproblems when the fitness function has a high variance. This\nresults in solutions generated by genetic algorithm that are\noften less robust and perform poorly, which is known as the\n”generalization limitation” of genetic algorithm. To address the\nissue, [94] introduces a X-DDPG algorithm, which combines\ngenetic algorithm and DDPG. At the heart of this hybrid\napproach is the effect of ”gradient bias,” which tilts the\nevolutionary process in favor of more robust solutions by\nperiodically injecting agents trained with DDPG into genetic\npopulations.\nV. OPEN ISSUES AND FUTURE DIRECTIONS\nAfter a thorough review of EvoRL algorithms, it is evident\nthat their current application does not stand out as a remarkable\nachievement but rather necessitates further refinement. In this\nsection, we put forward some emerging topics for considera-\ntion.\nA. Open Issues\n1) Scalability to High-Dimensional Spaces\nThe challenge lies in extending EvoRL methodologies to\neffectively handle complex, high-dimensional action and state\nspaces commonly encountered in real-world applications such\nas autonomous vehicles [96], Unmanned aerial vehicle [97],\nand large-scale industrial systems. Overcoming this hurdle\nentails the development of EvoRL algorithms capable of effi-\nciently exploring and exploiting these expansive spaces while\nmaintaining computational tractability. Furthermore, ensuring\nthe scalability of EvoRL necessitates the implementation of\ninnovative techniques to handle the curse of dimensionality,\nfacilitate effective knowledge transfer [98] across related tasks,\nand enable the discovery of meaningful solutions amidst the\ninherent complexity of high-dimensional environments.\n2) Adaptability to Dynamic Environments\nAdaptability to dynamic environments stands out as a sig-\nnificant open issue in EvoRL. EvoRL systems usually face\nchallenges in rapidly adjusting their policies to keep pace with\nchanges in the environment, where the optimal strategy may\nevolve over time. As real-world applications often involve\ndynamic and uncertain conditions, resolving the challenge\nof adaptability is essential for making EvoRL systems ro-\nbust and versatile in handling the complexities of changing\nenvironments. To this end, it requires the development of\nalgorithms that can dynamically adapt to shifting conditions.\nEvolutionary algorithms with dynamic parameter adaptation\n[99], such as Adaptive Evolution Strategies [100], represent\none avenue of exploration. These methods allow the algorithm\nto autonomously adjust parameters based on environmental\nchanges. Additionally, research might delve into the integra-\ntion of memory mechanisms (e.g., Long Short-Term Memory\nnetwork [101]) or continual learning approaches to retain\ninformation from past experiences, enabling EvoRL agents to\nadapt more effectively to evolving scenarios.\n3) Sample Efficiency and Data Efficiency\nA notable open issue in EvoRL pertains to sample efficiency\nand data efficiency, where EvoRL algorithms often require a\nsubstantial number of interactions with the environment to\nlearn effective policies. Addressing this challenge involves\nexploring innovative algorithms to enhance the efficiency of\nlearning processes. One feasible solution is data expansion. For\nexample, the dual training [102] can be adopted to get different\nbehavior distribution by updating the trajectory generator.\nThus, the sampled trajectories are repeatedly used to optimize\nthe policy. Moreover, the exploration of transfer learning\ntechniques [103] might improve the ability of EvoRL agents to\nleverage knowledge gained from previous tasks, thus reducing\nthe demand for extensive data collection. The ongoing pursuit\nof algorithms that strike a balance between exploration and\nexploitation, combined with approaches emphasizing effective\nknowledge transfer, remains pivotal in advancing the sample\nand data efficiency of EvoRL to make it more practical and\napplicable to real-world scenarios.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n13\n4) Adversarial Robustness in EvoRL\nHow to ensure that EvoRL agents maintain resilience in the\nface of intentional perturbations or adversarial interventions, is\nanother open issue in EvoRL. Unlike traditional adversarial at-\ntacks in DRL, the unique characteristics of EvoRL algorithms\nintroduce a set of challenges that demand tailored solutions\n[104]. Addressing this issue involves developing algorithms\nthat can evolve policies capable of withstanding adversarial\nmanipulations, ultimately leading to more reliable and secure\ndecision in dynamic and uncertain environments. To this\nend, we may focus on training EvoRL agents with diverse\nadversarial examples, which promote transferable defenses\nthat can withstand perturbations across different environments.\nBesides, it is worth designing evolutionary algorithms that\nemphasize safe exploration, aiming to guide the learning\nprocess towards policies that are less prone to adversarial\nmanipulation.\n5) Ethic and Fairness\nAnother open issue in EvoRL that demands attention is the\nethic and fairness of evolved policies. As EvoRL applications\nbecome more pervasive, ensuring that the learned policies\nalign with ethical standards and exhibit fairness is crucial.\nEthical concerns may arise if evolved agents exhibit biased\nbehavior or inadvertently learn strategies that have undesirable\nsocietal implications. To address this issue, researchers need\nto explore algorithms that incorporate fairness-aware objec-\ntives during the evolutionary process. Techniques inspired by\nfairness-aware machine learning, such as federated adversar-\nial debiasing [105] or reweighted optimization [106], could\nbe adapted to the EvoRL context. Additionally, integrating\nhuman-in-the-loop approaches to validate and guide the evo-\nlutionary process may contribute to more ethically aligned\npolicies. As EvoRL continues to impact diverse domains, it\nbecomes imperative to develop algorithms that not only opti-\nmize for performance but also adhere to ethical considerations\nand ensure fairness in decision-making processes.\nB. Future Directions\n1) Meta-Evolutionary Strategies\nMeta-evolutionary strategies involve the evolution of the\nparameters guiding the evolutionary process or even the evo-\nlution of entire learning algorithms. This approach enables\nEvoRL agents to adapt their behaviors across different tasks\nand environments, making it inherently more versatile. Tech-\nniques inspired by meta-learning, such as Model-Agnostic\nMeta-Learning algorithm [107] applied to evolutionary al-\ngorithms, hold promise for enhancing the ability of agents\nto generalize knowledge across various tasks. By evolving\nstrategies that can rapidly adapt to new challenges, the future\nof EvoRL lies in creating agents that not only excel in specific\nenvironments but can dynamically adjust and learn efficiently\nin diverse and evolving scenarios.\n2) Self-Adaptation and Self-Improvement Mechanisms\nIn the future, EvoRL is likely to witness significant progress\nin the incorporation of self-adaptation and self-improvement\nmechanisms, reflecting a paradigm shift towards more au-\ntonomous and adaptive learning systems. Researchers are\nexploring algorithms that enable EvoRL agents to dynamically\nadjust their strategies and parameters without external inter-\nvention. Evolutionary algorithms with self-adaptation mecha-\nnisms, such as Self-Adaptive Differential Evolution [108] or\nhybrid differential evolution based on adaptive Q-Learning\n[109], exemplify this trend. These algorithms allow the opti-\nmization process to autonomously adapt to the characteristics\nof the problem at hand, enhancing efficiency and robustness.\nAdditionally, the integration of self-improvement mechanisms,\ninspired by principles of continual learning, may empower\nEvoRL agents to accumulate knowledge over time and refine\ntheir policies iteratively. As self-adaptive and self-improving\nalgorithms become integral to the EvoRL landscape, the future\nholds the promise of more resilient, efficient, and increasingly\nautonomous learning systems capable of thriving in complex\nand dynamic environments.\n3) Transfer Learning and Generalization\nIt is poised to witness significant strides in the realms\nof transfer learning and generalization, essential for adapting\nEvoRL agents to a broader range of tasks and environments.\nTechniques inspired by transfer learning [110], such as meth-\nods leveraging meta-learning for adaptation [111], are likely\nto be at the forefront of this development. The goal is to equip\nEvoRL agents with the ability to generalize learned knowledge\nand skills, enabling them to solve tasks more effectively and\nrapidly in novel scenarios. As real-world applications demand\ngreater flexibility and adaptability, the integration of transfer\nlearning and generalization mechanisms will be pivotal in\nestablishing EvoRL as a robust and versatile learning paradigm\ncapable of addressing diverse challenges.\n4) Heterogeneous Networks and Multi-agent Systems\nAs we look ahead, one key area of development involves\nextending EvoRL methodologies to address the complexities\nof diverse, heterogeneous environments where agents exhibit\nvarying capabilities, goals, and behaviors. Embracing this\nheterogeneity requires evolving EvoRL algorithms that can\nadapt to different agent types, preferences, and constraints,\nthus enabling the emergence of more robust and adaptive\ncollective behavior [112]. Additionally, the advancement of\nEvoRL in multi-agent systems will entail exploring algorithms\ncapable of learning effective coordination and cooperation\nstrategies among diverse agents [113], fostering the evolution\nof sophisticated group behaviors while considering emer-\ngent properties and system-level objectives. This evolution in\nEvoRL will likely contribute significantly to addressing real-\nworld challenges across domains such as autonomous systems,\nsmart cities, and decentralized networks, paving the way for\nmore resilient, scalable, and adaptable multi-agent ecosystems.\n5) Interpretability and Explainability\nThe future trajectory of EvoRL is poised to place a height-\nened emphasis on interpretability and explainability, acknowl-\nedging the growing importance of transparent decision-making\nin artificial intelligence systems. One potential avenue involves\nthe incorporation of symbolic reasoning [114] during the\nevolutionary process, facilitating the generation of policies\nthat are not only effective but also comprehensible to humans.\nBesides, hybrid approaches, merging EvoRL with rule-based\nmethods [115], may offer a synergistic solution, ensuring the\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n14\nemergence of policies that align with domain-specific knowl-\nedge and are more readily understandable. The integration of\nexplainable meta-learning techniques [116] could also play\na role, enabling EvoRL agents to adapt their strategies to\ndifferent tasks while maintaining trustworthy.\n6) Incorporating Large Language Models\nIncorporating EvoRL within large language models [117],\nsuch as GPT-4 [118], holds tremendous potential. For instance,\nwe can leverage EvoRL to facilitate the evolution and adap-\ntation of language model architectures that can effectively\ncomprehend, generate, and respond to human language. By in-\ntegrating EvoRL with large language models, we can anticipate\nadvancements in training methods that enable these models\nto not only learn from explicit rewards but also understand\nimplicit signals within human interactions. This fusion could\nlead to the development of language models with enhanced\ncontextual understanding, ethical reasoning capabilities, and\nthe capacity to engage in more meaningful and empathetic\ninteractions with users. Furthermore, the synergy between\nEvoRL and large language models may pave the way for novel\napplications in conversational artificial intelligence, personal-\nized content generation, and context-aware decision-making\nsystems. For instance, we can extend EvoRL to incorporate\nmulti-modal information from large language models [119],\naiming to shape more intuitive and socially aware artificial\nintelligence interfaces.\nVI. CONCLUSION\nAs an emerging technology, EvoRL holds considerable\npromise across diverse tasks. In light of this, our paper\npresents a comprehensive review of various EvoRL algorithms.\nInitially, we provide a background on EAs and reinforcement\nlearning, serving as the foundation for EvoRLs. Subsequently,\nwe categorize existing EvoRLs, highlighting the corresponding\nEAs they employ. Within each subdomain, we organize them\nbased on different reinforcement learning methods, establish-\ning a systematic classification rule for EvoRL. Furthermore,\nwe explore challenges faced by EAs and reinforcement learn-\ning independently. Within this context, we examine potential\nsolutions proposed by EvoRLs. To a certain extent, these\nchallenges impede the performance of EAs and reinforcement\nlearning, and EvoRLs offer related solutions that exhibit supe-\nrior adaptability and problem-solving capabilities under spe-\ncific circumstances. Finally, we put forward several emerging\ntopics, including constructive open issues and promising future\ndirections for the development of EvoRL, aiming to enhance\nits performance across an expanding array of scenarios.\nREFERENCES\n[1] L. P. Kaelbling, M. L. Littman, and A. W. Moore, “Re-\ninforcement learning: A survey,” Journal of artificial\nintelligence research, vol. 4, pp. 237–285, 1996.\n[2] J. Morimoto and K. Doya, “Robust reinforcement learn-\ning,” Neural computation, vol. 17, no. 2, pp. 335–359,\n2005.\n[3] X. Zhao, L. Xia, L. Zhang, Z. Ding, D. Yin, and\nJ. Tang, “Deep reinforcement learning for page-wise\nrecommendations,” in Proceedings of the 12th ACM\nconference on recommender systems, 2018, pp. 95–103.\n[4] M. G. P. de LACERDA, F. B. de Lima Neto, T. B.\nLudermir, and H. Kuchen, “Out-of-the-box parameter\ncontrol for evolutionary and swarm-based algorithms\nwith distributed reinforcement learning,” Swarm Intel-\nligence, pp. 1–45, 2023.\n[5] Q. Liu, Y. Wang, and X. Liu, “Pns: Population-guided\nnovelty search for reinforcement learning in hard explo-\nration environments,” in 2021 IEEE/RSJ International\nConference on Intelligent Robots and Systems (IROS).\nIEEE, 2021, pp. 5627–5634.\n[6] D. E. Goldberg, “Cenetic algorithms in search,” Opti-\nmization, Machine Learning, 1989.\n[7] C. A. C. Coello, Evolutionary algorithms for solving\nmulti-objective problems.\nSpringer, 2007.\n[8] K. Suri, X. Q. Shi, K. N. Plataniotis, and Y. A.\nLawryshyn,\n“Maximum\nmutation\nreinforcement\nlearning\nfor\nscalable\ncontrol,”\narXiv\npreprint\narXiv:2007.13690, 2020.\n[9] Z. Zhang, Q. Tang, M. Chica, and Z. Li, “Rein-\nforcement learning-based multiobjective evolutionary\nalgorithm for mixed-model multimanned assembly line\nbalancing under uncertain demand,” IEEE Transactions\non Cybernetics, 2023.\n[10] O. Nilsson and A. Cully, “Policy gradient assisted map-\nelites,” in Proceedings of the Genetic and Evolutionary\nComputation Conference, 2021, pp. 866–875.\n[11] S. Khadka and K. Tumer, “Evolution-guided policy\ngradient in reinforcement learning,” Advances in Neural\nInformation Processing Systems, vol. 31, 2018.\n[12] L. Shi, S. Li, Q. Zheng, M. Yao, and G. Pan, “Efficient\nnovelty search through deep reinforcement learning,”\nIEEE Access, vol. 8, pp. 128 809–128 818, 2020.\n[13] S. Khadka, S. Majumdar, T. Nassar, Z. Dwiel, E. Tumer,\nS. Miret, Y. Liu, and K. Tumer, “Collaborative evolu-\ntionary reinforcement learning,” in International confer-\nence on machine learning.\nPMLR, 2019, pp. 3341–\n3350.\n[14] S. L¨u, S. Han, W. Zhou, and J. Zhang, “Recruitment-\nimitation mechanism for evolutionary reinforcement\nlearning,” Information Sciences, vol. 553, pp. 172–188,\n2021.\n[15] J. K. Franke, G. K¨ohler, A. Biedenkapp, and F. Hutter,\n“Sample-efficient automated deep reinforcement learn-\ning,” arXiv preprint arXiv:2009.01555, 2020.\n[16] A. Gupta, S. Savarese, S. Ganguli, and L. Fei-Fei,\n“Embodied intelligence via learning and evolution,”\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n15\nNature communications, vol. 12, no. 1, p. 5721, 2021.\n[17] T. Pierrot, V. Mac´e, G. Cideron, N. Perrin, K. Beguir,\nand O. Sigaud, “Sample efficient quality diversity for\nneural continuous control,” 2020.\n[18] E. Marchesini, D. Corsi, and A. Farinelli, “Genetic\nsoft updates for policy evolution in deep reinforcement\nlearning,” in International Conference on Learning Rep-\nresentations, 2020.\n[19] A. Eriksson, G. Capi, and K. Doya, “Evolution of meta-\nparameters in reinforcement learning algorithm,” in\nProceedings 2003 IEEE/RSJ International Conference\non Intelligent Robots and Systems (IROS 2003)(Cat. No.\n03CH37453), vol. 1.\nIEEE, 2003, pp. 412–417.\n[20] B. Tjanaka, M. C. Fontaine, J. Togelius, and S. Niko-\nlaidis, “Approximating gradients for differentiable qual-\nity diversity in reinforcement learning,” in Proceedings\nof the Genetic and Evolutionary Computation Confer-\nence, 2022, pp. 1102–1111.\n[21] A. Y. Majid, S. Saaybi, V. Francois-Lavet, R. V. Prasad,\nand C. Verhoeven, “Deep reinforcement learning versus\nevolution strategies: a comparative survey,” IEEE Trans-\nactions on Neural Networks and Learning Systems,\n2023.\n[22] Y. Wang, K. Xue, and C. Qian, “Evolutionary diversity\noptimization with clustering-based selection for rein-\nforcement learning,” in International Conference on\nLearning Representations, 2021.\n[23] O. Sigaud, “Combining evolution and deep reinforce-\nment learning for policy search: a survey,” ACM Trans-\nactions on Evolutionary Learning, vol. 3, no. 3, pp.\n1–20, 2023.\n[24] H. Bai, R. Cheng, and Y. Jin, “Evolutionary reinforce-\nment learning: A survey,” Intelligent Computing, vol. 2,\np. 0025, 2023.\n[25] P. A. Vikhar, “Evolutionary algorithms: A critical re-\nview and its future prospects,” in 2016 International\nconference on global trends in signal processing, infor-\nmation computing and communication (ICGTSPICC).\nIEEE, 2016, pp. 261–265.\n[26] F. Hoffmeister and T. B¨ack, “Genetic algorithms and\nevolution strategies: Similarities and differences,” in\nInternational conference on parallel problem solving\nfrom nature.\nSpringer, 1990, pp. 455–469.\n[27] W.\nVent,\n“Rechenberg,\ningo,\nevolutionsstrate-\ngie—optimierung technischer systeme nach prinzipien\nder\nbiologischen\nevolution.\n170\ns.\nmit\n36\nabb.\nfrommann-holzboog-verlag. stuttgart 1973. broschiert,”\n1975.\n[28] Q. Zhu, X. Wu, Q. Lin, L. Ma, J. Li, Z. Ming, and\nJ. Chen, “A survey on evolutionary reinforcement learn-\ning algorithms,” Neurocomputing, vol. 556, p. 126628,\n2023.\n[29] A. Slowik and H. Kwasnicka, “Evolutionary algorithms\nand their applications to engineering problems,” Neu-\nral Computing and Applications, vol. 32, pp. 12 363–\n12 379, 2020.\n[30] S. L. Ho and S. Yang, “The cross-entropy method and\nits application to inverse problems,” IEEE transactions\non magnetics, vol. 46, no. 8, pp. 3401–3404, 2010.\n[31] Z. I. Botev, D. P. Kroese, R. Y. Rubinstein, and\nP. L’Ecuyer, “The cross-entropy method for optimiza-\ntion,” in Handbook of statistics. Elsevier, 2013, vol. 31,\npp. 35–59.\n[32] K. Huang, S. Lale, U. Rosolia, Y. Shi, and A. Anand-\nkumar, “Cem-gd: Cross-entropy method with gradient\ndescent planner for model-based reinforcement learn-\ning,” arXiv preprint arXiv:2112.07746, 2021.\n[33] M.\nJaderberg,\nV.\nDalibard,\nS.\nOsindero,\nW.\nM.\nCzarnecki,\nJ.\nDonahue,\nA.\nRazavi,\nO.\nVinyals,\nT. Green, I. Dunning, K. Simonyan et al., “Popula-\ntion based training of neural networks,” arXiv preprint\narXiv:1711.09846, 2017.\n[34] D. Hein, S. Udluft, and T. A. Runkler, “Interpretable\npolicies for reinforcement learning by genetic program-\nming,” Engineering Applications of Artificial Intelli-\ngence, vol. 76, pp. 158–169, 2018.\n[35] E. Sachdeva, S. Khadka, S. Majumdar, and K. Tumer,\n“Maedys: Multiagent evolution via dynamic skill selec-\ntion,” in Proceedings of the Genetic and Evolutionary\nComputation Conference, 2021, pp. 163–171.\n[36] L. Shi, S. Li, L. Cao, L. Yang, G. Zheng, and G. Pan,\n“Fidi-rl: Incorporating deep reinforcement learning with\nfinite-difference policy search for efficient learning of\ncontinuous control,” arXiv preprint arXiv:1907.00526,\n2019.\n[37] Y. Lin, H. Chen, W. Xia, F. Lin, P. Wu, Z. Wang,\nand Y. Li, “A comprehensive survey on deep learning\ntechniques in educational data mining,” arXiv preprint\narXiv:2309.04761, 2023.\n[38] X. Chen, L. Yao, J. McAuley, G. Zhou, and X. Wang, “A\nsurvey of deep reinforcement learning in recommender\nsystems: A systematic review and future directions,”\narXiv preprint arXiv:2109.03540, 2021.\n[39] Y. Lin, Y. Liu, F. Lin, L. Zou, P. Wu, W. Zeng, H. Chen,\nand C. Miao, “A survey on reinforcement learning for\nrecommender systems,” IEEE Transactions on Neural\nNetworks and Learning Systems, 2023.\n[40] O. S. Ajani and R. Mallipeddi, “Adaptive evolution\nstrategy with ensemble of mutations for reinforce-\nment learning,” Knowledge-Based Systems, vol. 245, p.\n108624, 2022.\n[41] Y. Tang and K. Choromanski, “Online hyper-parameter\ntuning in off-policy learning via evolutionary strate-\ngies,” arXiv preprint arXiv:2006.07554, 2020.\n[42] Z. Chen, Y. Zhou, X. He, and S. Jiang, “A restart-based\nrank-1 evolution strategy for reinforcement learning.” in\nIJCAI, 2019, pp. 2130–2136.\n[43] P. Yang, H. Zhang, Y. Yu, M. Li, and K. Tang, “Evo-\nlutionary reinforcement learning via cooperative co-\nevolutionary negatively correlated search,” Swarm and\nEvolutionary Computation, vol. 68, p. 100974, 2022.\n[44] C. Su, C. Zhang, D. Xia, B. Han, C. Wang, G. Chen,\nand L. Xie, “Evolution strategies-based optimized graph\nreinforcement learning for solving dynamic job shop\nscheduling problem,” Applied Soft Computing, vol. 145,\np. 110596, 2023.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n16\n[45] H. Sun, Z. Xu, Y. Song, M. Fang, J. Xiong, B. Dai,\nand B. Zhou, “Zeroth-order supervised policy improve-\nment,” arXiv preprint arXiv:2006.06600, 2020.\n[46] R. Houthooft, Y. Chen, P. Isola, B. Stadie, F. Wol-\nski, O. Jonathan Ho, and P. Abbeel, “Evolved policy\ngradients,” Advances in Neural Information Processing\nSystems, vol. 31, 2018.\n[47] A. Callaghan, K. Mason, and P. Mannion, “Evolutionary\nstrategy guided reinforcement learning via multibuffer\ncommunication,”\narXiv\npreprint\narXiv:2306.11535,\n2023.\n[48] A. D. Martinez, J. Del Ser, E. Osaba, and F. Herrera,\n“Adaptive multifactorial evolutionary optimization for\nmultitask reinforcement learning,” IEEE Transactions\non Evolutionary Computation, vol. 26, no. 2, pp. 233–\n247, 2021.\n[49] W. Li, S. He, X. Mao, B. Li, C. Qiu, J. Yu, F. Peng, and\nX. Tan, “Multi-agent evolution reinforcement learning\nmethod for machining parameters optimization based\non bootstrap aggregating graph attention network simu-\nlated environment,” Journal of Manufacturing Systems,\nvol. 67, pp. 424–438, 2023.\n[50] B. Zheng and R. Cheng, “Rethinking population-\nassisted\noff-policy\nreinforcement\nlearning,”\narXiv\npreprint arXiv:2305.02949, 2023.\n[51] S. Elfwing, E. Uchibe, K. Doya, and H. I. Christensen,\n“Co-evolution of shaping rewards and meta-parameters\nin reinforcement learning,” Adaptive Behavior, vol. 16,\nno. 6, pp. 400–412, 2008.\n[52] T. Li, Y. Meng, and L. Tang, “Scheduling of continuous\nannealing with a multi-objective differential evolution\nalgorithm based on deep reinforcement learning,” IEEE\nTransactions on Automation Science and Engineering,\n2023.\n[53] P. Yang, L. Zhang, H. Liu, and G. Li, “Reducing\nidleness in financial cloud via multi-objective evolution-\nary reinforcement learning based load balancer,” arXiv\npreprint arXiv:2305.03463, 2023.\n[54] J. J. Garau-Luis, Y. Miao, J. D. Co-Reyes, A. Parisi,\nJ. Tan, E. Real, and A. Faust, “Multi-objective evolution\nfor generalizable policy gradient algorithms,” in ICLR\n2022 Workshop on Generalizable Policy Learning in\nPhysical World, 2022.\n[55] J. Li and T. Zhou, “Evolutionary multi agent deep meta\nreinforcement learning method for swarm intelligence\nenergy management of isolated multi area microgrid\nwith internet of things,” IEEE Internet of Things Jour-\nnal, 2023.\n[56] C. Hu, J. Pei, J. Liu, and X. Yao, “Evolving con-\nstrained reinforcement learning policy,” arXiv preprint\narXiv:2304.09869, 2023.\n[57] C. Bodnar, B. Day, and P. Li´o, “Proximal distilled\nevolutionary reinforcement learning,” in Proceedings of\nthe AAAI Conference on Artificial Intelligence, vol. 34,\nno. 04, 2020, pp. 3283–3290.\n[58] Y. Wang, T. Zhang, Y. Chang, X. Wang, B. Liang, and\nB. Yuan, “A surrogate-assisted controller for expensive\nevolutionary reinforcement learning,” Information Sci-\nences, vol. 616, pp. 539–557, 2022.\n[59] T.\nPierrot,\nV.\nMac´e,\nF.\nChalumeau,\nA.\nFlajolet,\nG. Cideron, K. Beguir, A. Cully, O. Sigaud, and\nN. Perrin-Gilbert, “Diversity policy gradient for sample\nefficient quality-diversity optimization,” in Proceedings\nof the Genetic and Evolutionary Computation Confer-\nence, 2022, pp. 1075–1083.\n[60] J. Hao, P. Li, H. Tang, Y. Zheng, X. Fu, and Z. Meng,\n“Erl-re2: Efficient evolutionary reinforcement learning\nwith shared state representation and individual policy\nrepresentation,” arXiv preprint arXiv:2210.17375, 2022.\n[61] D. Kalashnikov, A. Irpan, P. Pastor, J. Ibarz, A. Herzog,\nE. Jang, D. Quillen, E. Holly, M. Kalakrishnan, V. Van-\nhoucke et al., “Qt-opt: Scalable deep reinforcement\nlearning for vision-based robotic manipulation,” arXiv\npreprint arXiv:1806.10293, 2018.\n[62] L. Shao, Y. You, M. Yan, S. Yuan, Q. Sun, and J. Bohg,\n“Grac: Self-guided and self-regularized actor-critic,” in\nConference on Robot Learning. PMLR, 2022, pp. 267–\n276.\n[63] A. Pourchot and O. Sigaud, “Cem-rl: Combining evolu-\ntionary and gradient-based methods for policy search,”\narXiv preprint arXiv:1810.01222, 2018.\n[64] N. Kim, H. Baek, and H. Shin, “Pgps: Coupling policy\ngradient with population-based search,” 2020.\n[65] H. Zheng, P. Wei, J. Jiang, G. Long, Q. Lu, and\nC. Zhang, “Cooperative heterogeneous deep reinforce-\nment learning,” Advances in Neural Information Pro-\ncessing Systems, vol. 33, pp. 17 455–17 465, 2020.\n[66] Z.\nShi\nand\nS.\nP.\nSingh,\n“Soft\nactor-critic\nwith\ncross-entropy\npolicy\noptimization,”\narXiv\npreprint\narXiv:2112.11115, 2021.\n[67] J. Liu and L. Feng, “Diversity evolutionary policy\ndeep reinforcement learning,” Computational Intelli-\ngence and Neuroscience, vol. 2021, pp. 1–11, 2021.\n[68] Z.-Z. Wang, K. Zhang, G.-D. Chen, J.-D. Zhang, W.-\nD. Wang, H.-C. Wang, L.-M. Zhang, X. Yan, and\nJ. Yao, “Evolutionary-assisted reinforcement learning\nfor reservoir real-time production optimization under\nuncertainty,” Petroleum Science, vol. 20, no. 1, pp. 261–\n276, 2023.\n[69] Y. Ma, T. Liu, B. Wei, Y. Liu, K. Xu, and W. Li,\n“Evolutionary action selection for gradient-based policy\nlearning,” in International Conference on Neural Infor-\nmation Processing.\nSpringer, 2022, pp. 579–590.\n[70] W. Jung, G. Park, and Y. Sung, “Population-guided\nparallel policy search for reinforcement learning,” arXiv\npreprint arXiv:2001.02907, 2020.\n[71] T. Doan, B. Mazoure, M. Abdar, A. Durand, J. Pineau,\nand R. D. Hjelm, “Attraction-repulsion actor-critic\nfor continuous control reinforcement learning,” arXiv\npreprint arXiv:1909.07543, 2019.\n[72] E. Marchesini, D. Corsi, and A. Farinelli, “Exploring\nsafer behaviors for deep reinforcement learning,” in\nProceedings of the AAAI Conference on Artificial In-\ntelligence, vol. 36, no. 7, 2022, pp. 7701–7709.\n[73] S. Majumdar, S. Khadka, S. Miret, S. McAleer, and\nK. Tumer, “Evolutionary reinforcement learning for\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n17\nsample-efficient multiagent coordination,” in Interna-\ntional Conference on Machine Learning. PMLR, 2020,\npp. 6651–6660.\n[74] Q. Long, Z. Zhou, A. Gupta, F. Fang, Y. Wu, and\nX. Wang, “Evolutionary population curriculum for scal-\ning multi-agent reinforcement learning,” arXiv preprint\narXiv:2003.10423, 2020.\n[75] R. Shen, Y. Zheng, J. Hao, Z. Meng, Y. Chen, C. Fan,\nand Y. Liu, “Generating behavior-diverse game ais\nwith evolutionary multi-objective deep reinforcement\nlearning.” in IJCAI, 2020, pp. 3371–3377.\n[76] F. C. Fernandez and W. Caarls, “Parameters tuning\nand optimization for reinforcement learning algorithms\nusing evolutionary computing,” in 2018 International\nConference on Information Systems and Computer Sci-\nence (INCISCOS).\nIEEE, 2018, pp. 301–305.\n[77] S. Kamio and H. Iba, “Adaptation technique for inte-\ngrating genetic programming and reinforcement learn-\ning for real robots,” IEEE Transactions on Evolutionary\nComputation, vol. 9, no. 3, pp. 318–333, 2005.\n[78] J. D. Co-Reyes, Y. Miao, D. Peng, E. Real, S. Levine,\nQ. V. Le, H. Lee, and A. Faust, “Evolving reinforcement\nlearning algorithms,” arXiv preprint arXiv:2101.03958,\n2021.\n[79] A. AbuZekry, I. Sobh, M. Hadhoud, and M. Fayek,\n“Comparative study of neuroevolution algorithms in\nreinforcement learning for self-driving cars,” European\nJournal of Engineering Science and Technology, vol. 2,\nno. 4, pp. 60–71, 2019.\n[80] S. Kelly, T. Voegerl, W. Banzhaf, and C. Gondro,\n“Evolving hierarchical memory-prediction machines in\nmulti-task reinforcement learning,” Genetic Program-\nming and Evolvable Machines, vol. 22, pp. 573–605,\n2021.\n[81] S. Girgin and P. Preux, “Feature discovery in reinforce-\nment learning using genetic programming,” in European\nconference on genetic programming.\nSpringer, 2008,\npp. 218–229.\n[82] A. Sehgal, H. La, S. Louis, and H. Nguyen, “Deep\nreinforcement learning using genetic algorithm for pa-\nrameter optimization,” in 2019 Third IEEE International\nConference on Robotic Computing (IRC). IEEE, 2019,\npp. 596–601.\n[83] A. A. Aydeniz, R. Loftin, and K. Tumer, “Novelty\nseeking multiagent evolutionary reinforcement learn-\ning,” in Proceedings of the Genetic and Evolutionary\nComputation Conference, 2023, pp. 402–410.\n[84] S. Zhu, F. Belardinelli, and B. G. Le´on, “Evolutionary\nreinforcement learning for sparse rewards,” in Pro-\nceedings of the Genetic and Evolutionary Computation\nConference Companion, 2021, pp. 1508–1512.\n[85] P. Li, J. Hao, H. Tang, Y. Zheng, and X. Fu, “Race:\nimprove multi-agent reinforcement learning with repre-\nsentation asymmetry and collaborative evolution,” in In-\nternational Conference on Machine Learning.\nPMLR,\n2023, pp. 19 490–19 503.\n[86] S. Chang, J. Yang, J. Choi, and N. Kwak, “Genetic-\ngated networks for deep reinforcement learning,” Ad-\nvances\nin\nneural\ninformation\nprocessing\nsystems,\nvol. 31, 2018.\n[87] Y. Tang, “Guiding evolutionary strategies with off-\npolicy actor-critic.” in AAMAS, 2021, pp. 1317–1325.\n[88] R. Simmons-Edler, B. Eisner, E. Mitchell, S. Se-\nung, and D. Lee, “Q-learning for continuous actions\nwith cross-entropy guided policies,” arXiv preprint\narXiv:1903.10605, 2019.\n[89] M. Gao, X. Feng, H. Yu, and X. Li, “An efficient\nevolutionary algorithm based on deep reinforcement\nlearning for large-scale sparse multiobjective optimiza-\ntion,” Applied Intelligence, pp. 1–24, 2023.\n[90] H.-L. Tran, L. Doan, N. H. Luong, and H. T. T. Binh, “A\ntwo-stage multi-objective evolutionary reinforcement\nlearning framework for continuous robot control,” in\nProceedings of the Genetic and Evolutionary Compu-\ntation Conference, 2023, pp. 577–585.\n[91] J. Stork, M. Zaefferer, N. Eisler, P. Tichelmann,\nT. Bartz-Beielstein, and A. Eiben, “Behavior-based neu-\nroevolutionary training in reinforcement learning,” in\nProceedings of the Genetic and Evolutionary Compu-\ntation Conference Companion, 2021, pp. 1753–1761.\n[92] Y. Song, L. Wei, Q. Yang, J. Wu, L. Xing, and Y. Chen,\n“Rl-ga: A reinforcement learning-based genetic algo-\nrithm for electromagnetic detection satellite schedul-\ning problem,” Swarm and Evolutionary Computation,\nvol. 77, p. 101236, 2023.\n[93] J. Stork, M. Zaefferer, T. Bartz-Beielstein, and A. Eiben,\n“Surrogate models for enhancing the efficiency of neu-\nroevolution in reinforcement learning,” in Proceedings\nof the genetic and evolutionary computation conference,\n2019, pp. 934–942.\n[94] F. Espositi and A. Bonarini, “Gradient bias to solve\nthe generalization limit of genetic algorithms through\nhybridization with reinforcement learning,” in Machine\nLearning, Optimization, and Data Science: 6th Interna-\ntional Conference, LOD 2020, Siena, Italy, July 19–23,\n2020, Revised Selected Papers, Part I 6.\nSpringer,\n2020, pp. 273–284.\n[95] Y. Li, “Reinforcement learning in practice: Opportuni-\nties and challenges,” arXiv preprint arXiv:2202.11296,\n2022.\n[96] A. Rasouli and J. K. Tsotsos, “Autonomous vehicles\nthat interact with pedestrians: A survey of theory and\npractice,” IEEE transactions on intelligent transporta-\ntion systems, vol. 21, no. 3, pp. 900–918, 2019.\n[97] Y. Bai, H. Zhao, X. Zhang, Z. Chang, R. J¨antti, and\nK. Yang, “Towards autonomous multi-uav wireless net-\nwork: A survey of reinforcement learning-based ap-\nproaches,” IEEE Communications Surveys & Tutorials,\n2023.\n[98] J.-Y. Li, Z.-H. Zhan, K. C. Tan, and J. Zhang, “A\nmeta-knowledge transfer-based differential evolution for\nmultitask optimization,” IEEE Transactions on Evolu-\ntionary Computation, vol. 26, no. 4, pp. 719–734, 2021.\n[99] A. Aleti and I. Moser, “A systematic literature review\nof adaptive parameter control methods for evolutionary\nalgorithms,” ACM Computing Surveys (CSUR), vol. 49,\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n18\nno. 3, pp. 1–35, 2016.\n[100] Z.-H. Zhan, Z.-J. Wang, H. Jin, and J. Zhang, “Adaptive\ndistributed differential evolution,” IEEE transactions on\ncybernetics, vol. 50, no. 11, pp. 4633–4647, 2019.\n[101] S. Hochreiter and J. Schmidhuber, “Long short-term\nmemory,” Neural computation, vol. 9, no. 8, pp. 1735–\n1780, 1997.\n[102] H. Shi, B. Zhou, H. Zeng, F. Wang, Y. Dong, J. Li,\nK. Wang, H. Tian, and M. Q.-H. Meng, “Reinforce-\nment learning with evolutionary trajectory generator: A\ngeneral approach for quadrupedal locomotion,” IEEE\nRobotics and Automation Letters, vol. 7, no. 2, pp.\n3085–3092, 2022.\n[103] K. Weiss, T. M. Khoshgoftaar, and D. Wang, “A survey\nof transfer learning,” Journal of Big data, vol. 3, no. 1,\npp. 1–40, 2016.\n[104] L. A. Ajao and S. T. Apeh, “Secure edge computing\nvulnerabilities in smart cities sustainability using petri\nnet and genetic algorithm-based reinforcement learn-\ning,” Intelligent Systems with Applications, vol. 18, p.\n200216, 2023.\n[105] J. Hong, Z. Zhu, S. Yu, Z. Wang, H. H. Dodge, and\nJ. Zhou, “Federated adversarial debiasing for fair and\ntransferable representations,” in Proceedings of the 27th\nACM SIGKDD Conference on Knowledge Discovery &\nData Mining, 2021, pp. 617–627.\n[106] A. Petrovi´c, M. Nikoli´c, S. Radovanovi´c, B. Delibaˇsi´c,\nand M. Jovanovi´c, “Fair: Fair adversarial instance re-\nweighting,” Neurocomputing, vol. 476, pp. 14–37, 2022.\n[107] C. Finn, P. Abbeel, and S. Levine, “Model-agnostic\nmeta-learning for fast adaptation of deep networks,” in\nInternational conference on machine learning. PMLR,\n2017, pp. 1126–1135.\n[108] S. M. Elsayed, R. A. Sarker, and D. L. Essam, “An\nimproved self-adaptive differential evolution algorithm\nfor optimization problems,” IEEE Transactions on In-\ndustrial Informatics, vol. 9, no. 1, pp. 89–99, 2012.\n[109] L. Peng, Z. Yuan, G. Dai, M. Wang, and Z. Tang,\n“Reinforcement learning-based hybrid differential evo-\nlution for global optimization of interplanetary trajec-\ntory design,” Swarm and Evolutionary Computation, p.\n101351, 2023.\n[110] S. Niu, Y. Liu, J. Wang, and H. Song, “A decade survey\nof transfer learning (2010–2020),” IEEE Transactions\non Artificial Intelligence, vol. 1, no. 2, pp. 151–166,\n2020.\n[111] D. Lian, Y. Zheng, Y. Xu, Y. Lu, L. Lin, P. Zhao,\nJ. Huang, and S. Gao, “Towards fast adaptation of neu-\nral architectures with meta learning,” in International\nConference on Learning Representations, 2019.\n[112] T. Zhang, L. Yu, D. Yue, C. Dou, X. Xie, and\nL. Chen, “Coordinated voltage regulation of high\nrenewable-penetrated distribution networks: an evolu-\ntionary curriculum-based deep reinforcement learning\napproach,” International Journal of Electrical Power &\nEnergy Systems, vol. 149, p. 108995, 2023.\n[113] X. Yang, N. Chen, S. Zhang, X. Zhou, L. Zhang,\nand T. Qiu, “An evolutionary reinforcement learning\nscheme for iot robustness,” in 2023 26th International\nConference on Computer Supported Cooperative Work\nin Design (CSCWD).\nIEEE, 2023, pp. 756–761.\n[114] L. Malandri, F. Mercorio, M. Mezzanzanica, and\nA. Seveso, “Model-contrastive explanations through\nsymbolic reasoning,” Decision Support Systems, p.\n114040, 2023.\n[115] S. Porebski, “Evaluation of fuzzy membership functions\nfor linguistic rule-based classifier focused on explain-\nability, interpretability and reliability,” Expert Systems\nwith Applications, vol. 199, p. 117116, 2022.\n[116] X. Shao, H. Wang, X. Zhu, F. Xiong, T. Mu, and\nY. Zhang, “Effect: Explainable framework for meta-\nlearning in automatic classification algorithm selection,”\nInformation Sciences, vol. 622, pp. 211–234, 2023.\n[117] B. Min, H. Ross, E. Sulem, A. P. B. Veyseh, T. H.\nNguyen, O. Sainz, E. Agirre, I. Heintz, and D. Roth,\n“Recent advances in natural language processing via\nlarge pre-trained language models: A survey,” ACM\nComputing Surveys, 2021.\n[118] E. Y. Chang, “Examining gpt-4: Capabilities, implica-\ntions and future directions,” in The 10th International\nConference on Computational Science and Computa-\ntional Intelligence, 2023.\n[119] S. Zhong, Z. Huang, W. Wen, J. Qin, and L. Lin, “Sur-\nadapter: Enhancing text-to-image pre-trained diffusion\nmodels with large language models,” in Proceedings of\nthe 31st ACM International Conference on Multimedia,\n2023, pp. 567–578.\n",
  "categories": [
    "cs.NE"
  ],
  "published": "2024-02-20",
  "updated": "2024-02-20"
}