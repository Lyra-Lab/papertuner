{
  "id": "http://arxiv.org/abs/2402.13740v1",
  "title": "From Text to CQL: Bridging Natural Language and Corpus Search Engine",
  "authors": [
    "Luming Lu",
    "Jiyuan An",
    "Yujie Wang",
    "Liner yang",
    "Cunliang Kong",
    "Zhenghao Liu",
    "Shuo Wang",
    "Haozhe Lin",
    "Mingwei Fang",
    "Yaping Huang",
    "Erhong Yang"
  ],
  "abstract": "Natural Language Processing (NLP) technologies have revolutionized the way we\ninteract with information systems, with a significant focus on converting\nnatural language queries into formal query languages such as SQL. However, less\nemphasis has been placed on the Corpus Query Language (CQL), a critical tool\nfor linguistic research and detailed analysis within text corpora. The manual\nconstruction of CQL queries is a complex and time-intensive task that requires\na great deal of expertise, which presents a notable challenge for both\nresearchers and practitioners. This paper presents the first text-to-CQL task\nthat aims to automate the translation of natural language into CQL. We present\na comprehensive framework for this task, including a specifically curated\nlarge-scale dataset and methodologies leveraging large language models (LLMs)\nfor effective text-to-CQL task. In addition, we established advanced evaluation\nmetrics to assess the syntactic and semantic accuracy of the generated queries.\nWe created innovative LLM-based conversion approaches and detailed experiments.\nThe results demonstrate the efficacy of our methods and provide insights into\nthe complexities of text-to-CQL task.",
  "text": "From Text to CQL: Bridging Natural Language and\nCorpus Search Engine\nLuming Lu1∗Jiyuan An1∗Yujie Wang2* Liner yang1† Cunliang Kong3 Zhenghao Liu4\nShuo Wang3 Haozhe Lin3 Mingwei Fang1 Yaping Huang2 Erhong Yang1\n1Beijing Language and Culture University, China\n2Beijing Jiaotong University, China\n3Tsinghua University, China\n4Northeastern University, China\nAbstract\nNatural Language Processing (NLP) technolo-\ngies have revolutionized the way we interact\nwith information systems, with a significant fo-\ncus on converting natural language queries into\nformal query languages such as SQL. How-\never, less emphasis has been placed on the\nCorpus Query Language (CQL), a critical tool\nfor linguistic research and detailed analysis\nwithin text corpora. The manual construction\nof CQL queries is a complex and time-intensive\ntask that requires a great deal of expertise,\nwhich presents a notable challenge for both re-\nsearchers and practitioners. This paper presents\nthe first text-to-CQL task that aims to auto-\nmate the translation of natural language into\nCQL. We present a comprehensive framework\nfor this task, including a specifically curated\nlarge-scale dataset and methodologies leverag-\ning large language models (LLMs) for effec-\ntive text-to-CQL task. In addition, we estab-\nlished advanced evaluation metrics to assess\nthe syntactic and semantic accuracy of the gen-\nerated queries. We created innovative LLM-\nbased conversion approaches and detailed ex-\nperiments. The results demonstrate the efficacy\nof our methods and provide insights into the\ncomplexities of text-to-CQL task.\n1\nIntroduction\nNatural Language Processing (NLP) technologies\nhave significantly improved our interaction with\ninformation systems, enabling a more intuitive and\neffective interface to communicate with computers.\nAmong these advances, the conversion of natural\nlanguage queries into query languages, such as\nStructured Query Language (SQL) for databases,\nhas been a focal point of research. The exploration\nof linguistic corpora has benefitted significantly\nfrom advances in query languages, enabling re-\nsearchers and practitioners to navigate and analyze\n* Equal contribution.\n† Corresponding author.\ntext corpora efficiently. While several query lan-\nguages, such as SQL for databases and various\nDomain-Specific Languages (DSLs) for other appli-\ncations, have seen extensive study and application,\nthe focus on Corpus Query Language (CQL) has\nbeen relatively less pronounced. Existing research\nhas extensively explored Text-to-SQL(Zhong et al.,\n2017; Liu et al., 2022; Yu et al., 2018a; Li et al.,\n2023a; Gao et al., 2023; Pourreza and Rafiei, 2023;\nDong et al., 2023; Li et al., 2023b) and Text-to-\nDSL(Wang et al., 2023; Staniek et al., 2023) tasks,\ndemonstrating the feasibility and efficiency of trans-\nlating natural language instructions into formal\nquery statements to interact with databases and\ninformation systems.\nCQL is vital for linguistic research as it offers a\nnuanced approach to querying annotated text cor-\npora, allowing for sophisticated searches based on\nlinguistic features. This capability is crucial for\nconducting detailed linguistic analysis and sup-\nports a wide range of research activities in NLP\nand computational linguistics. However, crafting\nCQL queries manually is a time-consuming and\nerror-prone process that requires a high level of ex-\npertise in both the query language and the specific\nannotations of the corpus being queried.\nThis work introduces the task of text-to-CQL.\nThis task aims to bridge the gap between natural\nlanguage descriptions and their corresponding CQL\nrepresentations, facilitating more accessible and ef-\nficient interactions with linguistic corpora with rich\nlinguistic annotations. However, unlike its coun-\nterparts in text-to-CQL task, the text-to-CQL task\nfaces unique challenges, including a scarcity of\ndedicated training data and the intricate syntax and\nsemantics of CQL, which are not easily handled by\neven the most advanced generative models, such as\nGPT-4, without specialized training and adaptation.\nModels need not only to understand the semantics\nof natural language descriptions, but also to navi-\ngate the complexities of linguistic annotations and\narXiv:2402.13740v1  [cs.CL]  21 Feb 2024\nSearch for instances where there exists a sentence (as indicated by <s/>)\nin which there are two entities, A and B. B is a word with a lemma of 'be'\nand A is any word. These two entities should be separated by zero or\nmore arbitrary words (indicated by []*). The query looks for instances\nwhere the part of speech (pos) of A is not equal to the part of speech of\nB.\nUser Input\nA:[] []* B:[lemma = 'be'] within <s/> :: A.pos != B.pos\n1. ... the altered scale , altered dominant scale , or Super\nLocrian scale ( Locrian 4 scale ) is  a seven ...\n2.  ... the three irreducibly essential tones that define a\ndominant seventh chord , which are  root , major third ...\n3. ... known under the Latinized name Henricus Institor , was  a\nGerman churchman and inquisitor ... \n...\nCorpus Query Language \nQuery Execution Results\nFigure 1: Example of text-to-CQL . Given any input nat-\nural language query description, the model is expected\nto convert it into the corresponding Corpus Query Lan-\nguage (CQL) and the generated CQL should be able\nto be accurately executed by the Corpus Engine. The\nCQL uses symbols (in green) with a small number of\nCQL uses symbols (green) and a small number of key-\nwords (purple) to construct queries, and allows to spec-\nify names (blue) for tokens to constrain relationships\nbetween tokens. The corpus search engine will return a\nquery execution result.\nthe specific query constructs of CQL.\nThis work aims to address these challenges by\nproposing a comprehensive framework for the text-\nto-CQL task. We introduce a novel dataset specifi-\ncally curated for this task, along with methodolo-\ngies and evaluation metrics tailored to the unique\nrequirements of CQL query generation. Our contri-\nbutions include the creation of a large-scale dataset\nthat encompasses a wide range of linguistic phe-\nnomena and query types, the development of mod-\nels that adapt large language models and introduce\nnew LLM-based approaches for text-to-CQL task,\nand the establishment of evaluation metrics that go\nbeyond traditional measures to assess the syntactic\nvalidity and semantic correctness of the generated\nqueries.\nIn summary, our key contributions are as fol-\nlows:\n• A large-scale, diverse dataset for text-to-CQL\ntask, providing a benchmark for model evalu-\nation.\n• A series of LLM-based text-to-CQL method-\nologies, including both prompt engineering\nand fine-tuning pretrained language models.\n• New evaluation metrics designed to accurately\nreflect the complexities of the text-to-CQL\ntask, focusing on syntactic validity and se-\nmantic correctness.\n• Comprehensive experiments and analysis that\nhighlight the effectiveness of our proposed\nmethods and offer insights into the challenges\nof text-to-CQL conversion.\nWe will release all our code and datasets for\nresearch purposes on Github.\n2\nBackground\nCorpus Query Language (CQL) is a query language\nspecifically used to query a corpus with the lin-\nguistic features required by users. The CQL uti-\nlized in this article is mainly related to the Black-\nLab (de Does et al., 2017)1 corpus Query Lan-\nguage2.\nNumerous corpus search tools endorse and em-\nploy CQL, including well-known platforms such as\nCQPweb3, Sketch Engine4, and BlackLab, among\nothers. Some examples of CQL are shown in Table\n1.\n2.1\nCQL Statementes\nPourreza and Rafiei (2023) categorized SQL into\nsimple, complex, and nesting classes, delineating\ndistinctions in sentence structure complexity within\nthe Text-to-SQL task. Analogously, CQL can be\nclassified into three categories based on keywords,\nas distinct keywords induce alterations in the CQL\nstructure.\nSimpe Query.\nIn CQL, users possess the\nability to formulate queries that target the de-\nsired corpus by using sequential associations\namong the tokens. For example, to retrieve in-\nstances of research categorized as nouns within\nthe corpus, a user can use the following CQL:\n[word='research' & pos='NN']\nIn the above example, we employed a corpus\naligned with the Penn Treebank (PTB) (Taylor\net al., 2003) part-of-speech system. The model’s ca-\npacity to accurately associate the lexical properties\n1https://github.com/INL/BlackLab\n2The existing systems for Corpus Query Languages\nare offshoots of the Corpus Query Language Processor\n(CQP) (Hardie, 2012) query language, which is a suite of\nlanguages designed for the retrieval of lexical information.\n3https://cwb.sourceforge.io/cqpweb.php\n4https://www.sketchengine.eu/\nType\nCQL\nNL\nSimple\n[lemma=\"teapot\"]\nFind the lemma teapot.\nWithin\n[pos=\"N.*\"] within [pos=\"VB.*\"]\n[]{0,5} [pos=\"VB.*\"]\nSearches for nouns that appear be-\ntween two verbs to be, the verbs are at\na distance of max. 5 tokens from each\nother.\nCondition\n1:[] 2:[] :: 1.pos = 2.pos\nFind any two tokens whose tag is the\nsame.\nTable 1: Example of the Corpus Query Language. The above examples and explanations are all from the Sketch\nEngine documentation.\nof natural language representations with the ap-\npropriate lexical labels represents a potential chal-\nlenge.\nWithin Query.\nAs shown in Table 1, the\n\"within\" syntax serves to partition a CQL statement\ninto two sub-queries, restricting the target retrieved\nin the initial portion to the scope delineated in the\nsubsequent segment. Typically, \"within\" is accom-\npanied by a subquery with a larger maximum target\nlength or certain XML structure.\nCondition Query. A condition statement is em-\nployed to compare the tokens with each other and\nto impart additional options to individual tokens.\nAll attributes of a token are eligible for comparison\nwithin a condition.\n3\nDataset Construction\nGiven a parallel dataset of natural language descrip-\ntions and CQL queries D = {(Xi, Yi)}N\ni=1, where\nXi = {w1, w2, . . . , wn1} is a query described in\nnatural language and Yi = {t1, t2, . . . , tn2} is the\nCQL corresponding to the NL. n1 represents the\nlength of the natural language description while\nn2 represents the length of the CQL query. The\ngoal of the text-to-CQL task is to train a model that\nconverts a natural language query description into\na query language. This also means that the model\nneeds to have the ability to extract key information\nfrom a natural language description and combine\nit into CQL with the correct syntax. For this pur-\npose, constructing a dataset from natural language\nto CQL is necessary.\n3.1\nCorpus Collection\nWe employed two distinct corpora for our study,\none in Chinese and the other in English. Both\ncorpora were annotated using Stanford Corenlp.\nTCFL Textbook. We collected the main teach-\ning materials to teach Chinese as a foreign language\non the market and constructed the TCFL Textbook.\nDataset\nSentences\nTokens\nTCFL Textbook\n578.4 k\n7.7 M\nEnWiki\n138.6 M\n3.1 B\nTable 2: The token and sentiment size of the corpus we\nused.\nEnWiki. We use the EnWiki (Denoyer and Gal-\nlinari, 2006)5 corpus and clean and extract the text\nin it using wiki-extractor6(Attardi, 2015). In Table\n2, We show the size of the cleaned dataset.\nOur text-to-CQL dataset is divided into two parts:\nChinese NL-CQL pairs based on the TCFL text-\nbooks and English NL-CQL pairs based on Wiki.\n3.2\nCQL Generation Strategies\nCertain conventional dataset construction methods,\nsuch as the data mining techniques employed in\nWikiTable(Bhagavatula et al., 2013), are precluded\ndue to the limited availability of pertinent informa-\ntion on the Internet. Consequently, we develop a\nnovel data collection approach grounded in Chi-\nnese collocation extraction.\n3.2.1\nCollocation Extraction\nOur approach to data augmentation is based on Chi-\nnese Collocation Extraction(Hu and Xiao, 2019).\nAn example of Chinese collocation extraction is\nshown in the Appendix. The collocation set extrac-\ntion methodology takes advantage of both surface\nand dependency relation knowledge, along with\nstatistical methods. Furthermore, we incorporate\nthe enhanced Chinese dependency(Yu et al., 2022)\nto improve the efficacy of collocation extraction.\nSpecifically, our process involves initially employ-\ning Stanford CoreNLP(Manning et al., 2014) for\n5https://dumps.wikimedia.org/\n6https://github.com/attardi/wikiextractor\nQuery Type\nCQL\nW\n[word='book']\nP\n[pos='NN']\nWOP\n[word='book'|pos='NN']\nWAP\n[word='book'&pos='NN']\nWW\n[word='book'|word=\n'notebook']\nWWP\n[(word='book'|word=\n'notebook')&pos='NN']\nTable 3: An example of a CQL Token Queries trans-\nformed from Token extracted from a corpus, containing\n6 random transformations: simple Word Query (W),\nSimple Pos Query (P), Word and Pos Query (WAP),\nWord or Pos Query (WOP), Word or Word query (WW)\nand Words with Pos Query (WWP)\nthe dependency analysis of sentences within the\ncorpus. Subsequently, enhanced dependencies are\nintroduced, and collocations are extracted from the\nentire corpus. Finally, a random selection is made\nfrom the extracted collocations and applied in clas-\nsified CQL templates.\n3.2.2\nCQL Template\nAs depicted in Table 1, the CQL queries can be\ncategorized into three distinct types: simple, within,\nand condition. In alignment with these three types\nof CQL, we established distinct templates that are\ntailored for each type.\nSimple Statements. Given that the collocations\nextracted from the corpus consist of word combi-\nnations that exceed two words, our post-extraction\nprocedure involves traversing the word sequence.\nInitially, we randomly assign a set of conditions\nfor each token. For any token (such as the noun\n’book’), we randomly convert it into the following\nforms:\n1. Simple word query (W).\n2. Simple part-of-speech query (P).\n3. Query its word and POS at the same time. The\nlogical relationship between the two conditions is\nrandomly chosen from AND (WAP) or OR (WOP).\n4. Query two words at the same time and there\nis an OR relationship between them (WW). In this\ncase, we also randomly restrict its part of speech\nwith an AND relation (WWP). Another candidate\nword is selected based on the synonyms specified\nin the synonym forest.\nExamples are shown in Table 3. After convert-\ning the word in each collocation to CQL, we ran-\ndomly add empty tokens to it as shown in Algo-\nrithm 1, where the mutate function refers to the\nprocess of converting the collocation word to a\nCQL token as described in this section, and the\ninsert_null_token method randomly adds an un-\nrestricted token to the end of a CQL and assigns it\na random number of repetitions or quantifiers using\nregular expressions.\nAlgorithm 1 Generation of simple CQL\nInput Collocation = {w1, w2, . . . , wn}\nOutput CQL\nCQL←None\nwhile i ̸= 0 do\nif freq(wi) ≤5 then\nAbandon()\nelse\nCQL.append(Mutate(wi))\nif wi+1 = ”X” then\nCQL.insert_null_token()\ni ←i + 1\nelse\nif random_number < 0.5 then\nCQL.insert_null_token()\nend if\nend if\ni ←i + 1\nend if\nend while\nWithin Statements. Two potential subqueries\nare permissible following the keyword within:\n1) Simple CQL Subquery. This causes the cor-\npus searching engine to search for CQL before\nthe within keyword within the specified subquery\nscope. 2) Structure. One may utilize XML Struc-\nture to confine the query scope to the specified\nXML domain, with strict prohibition on extending\nbeyond the boundaries defined by the tags.\nFor both cases, we randomly apply one of them.\nOn the one hand, two CQLs are generated through\ncollocation analysis, in which the maximum token\nlength they can reference is examined. Then the\nshorter one is placed preceding the within key-\nword. On the other hand, we randomly specify a\ncertain level of XML format and place it after the\nwithin.\nWe also generate muiti \"within\" statements.\nHowever, nesting multiple levels of queries may\nlack meaningful interpretation and pose challenges\nin natural language description. Furthermore, XML\nstructures are commonly segmented at the sentence\nlevel and beyond, rendering queries across XML\nstructures practically insignificant. Consequently,\nwe restrict the generation of nested queries to those\ncomprising two \"within\" keywords, with the XML\nstructure query positioned at the end of the query\n(representing the highest-priority decision). An ex-\nample of our generated within CQL is shown in the\nAppendix.\nCondition Statements. we extract the analyt-\nical outcomes of all sentences within the corpus.\nFrom these results, we identify token pairs within\nsentences where parts of speech or words share\nequality. Subsequently, sentences containing such\ntoken pairs are randomly selected, and CQL with\ncondition syntax is generated based on these equiv-\nalence relationships. Given that the collocation-\nbased method is no longer applicable to CQL with\nequivalence relationships, our consideration is lim-\nited to scenarios that involve the embedding of\nCQL within the XML structure in condition state-\nments. An instance of our generated condition CQL\nis shown in the Appendix.\n3.3\nAnnotation\nWe create the text-to-CQL dataset TCQL with\nmanual annotation. To ensure clarity and precision\nin natural language descriptions, we implemented\na training and selection process for our annotators.\nOf the initial pool of 14 recruited annotators, we\nassessed their abilities and ultimately retained eight\nannotators for subsequent annotation tasks. These\nannotators have undergraduate degrees and are fa-\nmiliar with both computer science and linguistics.\nWe perform 4 rounds of labeling for each dataset.\nFirst, for the CQLs that have been generated, we\nperform the initial labeling using the OpenAI GPT-\n4 API (OpenAI et al., 2023). We ask GPT-4 to gen-\nerate the demand text based on a given CQL and\nprompt it with the necessary information. Then, we\nask the annotator to perform 2 rounds of re-labeling\nto revise the errors in the results of the initial anno-\ntation. Finally, two reviewers who are well-versed\nin CQL syntax are responsible for reviewing the\nannotation results again. The size of the labeled\ndata set is shown in Table 4.\n4\nMethodology\nIn the construction of the text-to-CQL dataset, we\nimplemented five distinct methodologies, encom-\npassing approaches based on the In Context Learn-\ning (ICL) method and approaches using pretraining\nor fine-tuning pretrained language models.\nTrain\nDev\nTest\nSimple\n5,631\n805\n1,609\nWithin\n2,332\n334\n667\nCondition\n1,399\n199\n401\nAll\n9,362\n1,328\n2,677\nTable 4: Combined Classification Statistics for TCQL\ndatasets.\n4.1\nIn-Context Learning (ICL) Methods\nWe investigate three classifications of Large Lan-\nguage Model (LLM) prompt methods to assess the\nefficacy of LLMs in text-to-CQL tasks.\nDocumentation Prompt (DP). Furnish the\nLLM with a CQL tutorial created by human ex-\nperts, derived from tutorials accessible in Sketch\nEngine(Kilgarriff et al., 2008, 2014) and Black-\nlab(de Does et al., 2017) Documentation. Within\nthe tutorial, we elucidate the syntax of CQL using\nnatural language and furnish illustrative instances,\nsourced from the tutorial documentation.\nFew-shot ICL. We adhere to the methodology\noutlined by Sun et al. (2023) and three sets of ex-\nperiments with different numbers of examples were\nset up. In each group of examples, we set an ex-\nample for each of the three types of CQL. 1-Shot\nLearning (1SL) and 3-Shot Learning (SL) mean\nthat we embed one or three groups of examples\nin the prompt. Prompt details can be found in the\nAppendix.\n4.2\nFine-tuning PLM Methods.\nModels equipped with an encoder-decoder archi-\ntecture are aptly suited the text-to-CQL task. This\ncategory encompasses several models, including\nBERT(Devlin et al., 2018)，T5(Raffel et al., 2020),\nBART(Lewis et al., 2019), and GPT(Radford et al.,\n2019), among others. this study prioritizes BART\ndue to its integrated encoder-decoder architecture.\nFurthermore, BART’s foundation on a denoising\nautoencoder pre-training paradigm potentially en-\nhances its proficiency in natural language compre-\nhension and structured query generation, as evi-\ndenced by preliminary experimental findings.\nFor the generation of CQL queries from Chinese\ntexts, this research employed the BART-Chinese\nmodel (Shao et al., 2021). We leverage the most\nexpansive ‘Large’ size model available. respec-\ntively. Two distinct methodologies were applied for\nthe fine-tuning of the pre-trained language model:\nPrefix-tuning and Full Model Fine-tuning. The find-\nings indicate that, within the context of the Chinese\ntext-to-CQL task, prefix-tuning yielded superior\nresults. Conversely, for the English text-to-CQL\ntask, full model fine-tuning demonstrated enhanced\nperformance. Appendix gives more results of PLM\nperformance on this task.\n4.3\nMetrics\nIn this section, we draw upon prior research in the\ndomain of Text-to-SQL, as well as relevant Text-\nto-Code evaluation metrics, to introduce the four\nevaluation metrics employed in our study.\n4.3.1\nExact Match (EM)\nExact Match (EM) is used to evaluate whether the\ngenerated SQL query matches exactly the human-\nannotated standard query. Specifically, the EM\nmetric measures whether the generated SQL query\nagrees with the reference query without any differ-\nences. However, execution accuracy may create\nfalse positives for CQL queries that are semanti-\ncally identical but have different forms(Yu et al.,\n2018b; Deng et al., 2022).\n4.3.2\nValid Accuracy (VA)\nWe introduce the Valid Accuracy (VA) metric,\nwhich is designed to assess the syntactic correct-\nness of the generated code concerning the CQL\ngrammar. The VA metric provides insights into\nthe model’s ability to generate syntactically sound\ncode structures.\n4.3.3\nExecution Accuracy (EX)\nExecution Accuracy (EX) metrics are used to evalu-\nate how well the generated CQL query executes on\nthe corpus Engine. It determines whether the gen-\nerated SQL query executes correctly and returns\nthe desired result 7.\n4.3.4\nCQLBLEU\nInspired by Ren et al. (2020), we propose new CQL-\nBLEU metrics. This metric is used to assess the\nsimilarity between the CQL generated by the model\nand the reference CQL. Specifically, CQLBLEU is\na combination of BLEU(Papineni et al., 2002) and\nsemantic similarity metrics. Given an candidate\nCQL Qc and a reference CQL Qr, CQLBLEU is\n7In our experimental setup, we employ BlackLab as the\nexecution engine for CQL and ascertain the congruence of the\ncorpus retrieval results.\ndefined as:\nCQLBLEU(Qc, Qr) =α · BLEU(Qc, Qr)\n+ β · TS(Qc, Qr)\n(1)\nwhere BLEU stands for BLEU metrics and TS\nstands for the AST tree similarity which can be\na semantic similarity metric. The metric is com-\nputed based on the AST generated after syntactic\nparsing:\nTc = Parse(Qc)\n(2)\nTr = Parse(Qr)\n(3)\nTS(Qc, Qr) = Sim(Tc, Tr)\n(4)\nwhere Tc and Tr are the CQL AST of Qc and Qr\nparsed by Blacklab. The Sim function compares\neach node in the AST of Qc by itself and its direct\nchildren for the presence of Qr:\nSim(Tc, Tr) =\nP\nnc∈N(Tc) Match(nc, Tr)\n|N(Tc)|\n(5)\nwhere N(T) represents the set of non-leaf nodes\nin tree T, and Match(nc, Tr) is a function that re-\nturns 1 if a node nc from Tc and its direct children\nhave a matching signature in Tr, and 0 otherwise.\nThe matching criterion for a node nc with signa-\nture s(ni) = (N(nc), C(nc), K(nc)) against Tr is\ndefined as follows:\nMatch(ni, Tr) =\n(\n1,\nif ∃nr ∈N(Tr) : s(ni) = s(nr)\n0,\notherwise\n(6)\nThe coefficients α and β in the definition of\nCQLBLEU allow for balancing the contribution\nof syntactic similarity, as measured by BLEU, and\nsemantic similarity, as measured by TS, to the\noverall metric. In our work, we choose α = 0.5\nand β = 0.5.\n5\nAnalysis\n5.1\nLLM capability assessment\nIn our LLM-based ICL experiments, we found\nthree significant features of LLM for this task:\nLLM by itself is almost incapable of writing\nCQL correctly. In our early test, LLM shows low\nperformance of several methods for each of the\nthree CQL classifications: simple, within, and con-\nditional. LLM did not perform better than PLM\neven when Documentation Prompt (DP) was pro-\nvided. This may be due to the fact that the training\nModel\nSettings\nTCFL Textbook\nEnWiki\nEM\nVA\nEX\nCQLBLEU\nEM\nVA\nEX\nCQLBLEU\nBART-Chinese\n-\n46.52\n80.46\n50.95\n72.95\n-\n-\n-\n-\nBART-English\n-\n-\n-\n-\n-\n37.58\n81.74\n44.30\n82.13\nGPT-4\nDP\n35.17\n77.52\n51.79\n74.95\n14.93\n75.37\n24.49\n67.63\nGPT-4\n1SL\n47.81\n81.84\n62.71\n82.22\n43.31\n82.24\n51.87\n82.93\nGPT-4\n3SL\n67.49\n90.28\n77.85\n91.83\n58.24\n89.74\n65.53\n89.93\nTable 5: Experiment results. The table contains the results of four evaluation metrics: Exact Match (EM), Valid\nAccuracy (VA), Execution Accuracy(EX), and CQLBLEU. We choose the BART-Large model and use its Chinese\nbranch to fit our Chinese dataset.\ndata that LLM was exposed to may have contained\nfewer CQL examples, and these examples were\nmostly focused on simple classification, and not\nmuch on the other two classifications, which are\nmore flexible and broader in application scenarios.\nLLM is much better at learning from exam-\nples. We experimented with having LLM learn\nCQL knowledge from documents written by hu-\nman experts (DP) and having LLM learn from ex-\namples given CQL-NL pairs (1SL and 3SL). In\nthe DP approach, there is still a large gap between\nLLM and finetuned PLM, which may mean that\nLLM is not as efficient at reading documents that\nare more easily understood by humans. The results\nshow that CQL can effectively understand the syn-\ntax of the query language in fewer samples. This is\nconsistent with Staniek et al. (2023)’s conclusion.\nLLM understands the semantics expressed in\nhuman language. In most cases, LLMs achieve\nhigh CQLBLEU scores even if they are not given\ndetailed hints about the CQL syntax or if their exe-\ncution results do not meet expectations. This means\nthat LLM writes answers that are closer to human\nanswers in terms of semantic similarity and text.\nThis ability of LLM can continue to be enhanced\nwith more hints or examples. This also confirms\nthat LLM learns not only formal knowledge from\nexamples but also semantic information.\n5.2\nPLM Performance Analysis\n5.2.1\nPerformance of PLM on different\nlanguages\nBased on the experimental results described in the\nprevious section, the performance of the same large-\nsized BART model shows differences in the text-to-\nCQL tasks for both English and Chinese languages.\nBeyond the differences in model performance due\nto the language used for fine-tuning, we believe\na more significant reason is the addition of the\n\"lemma\" attribute in English CQL compared to\nChinese. In addition to \"word\" and \"pos\" in Chi-\nnese queries, English queries also include \"lemma,\"\nrequiring the model to learn an additional attribute\nname. Furthermore, the forms of words and their\nlemmas are quite similar in natural language expres-\nsion and are often mixed in actual human queries.\nThe model exhibits similar behavior, where the pre-\ndicted CQL queries differ from the gold standard\nonly in the attribute names \"word\" and \"lemma,\"\nwhich is a very common type of error occurrence.\n5.2.2\nPerformance of PLM on different query\ndifficulties\nTo better assess the performance of our proposed\nmodel, we categorized CQL queries into three lev-\nels of difficulty based on human habits in writing\nCQL queries. According to our intuition, the diffi-\nculty of generating CQL queries from text for the\nmodel should follow the order: Simple < Within <\nCondition. However, the model’s performance in\nsome cases deviated from our expectations, show-\ning a significantly better performance on condi-\ntion type than on within type (for example, when\nusing the DP method). To elucidate the reasons\nbehind this phenomenon, we provide detailed sta-\ntistical data from the dataset, as shown in the Ap-\npendix. We found that in terms of the character\nlength of CQL queries and the number of con-\nstraint conditions, Within type far exceeds Condi-\ntion type, implying that natural language inputs of\nthe Within type lead to the generation of the longest\nCQL queries with the most constraint conditions,\nwhich typically signifies a higher probability of\nerrors. Conversely, condition type demonstrated\nmore complex query logic, but since it involves\nmore non-constraint word queries, the primary chal-\nlenge it poses to the model is the understanding of\nthe logic in the natural language input rather than\nlonger CQL queries and more constraint condi-\ntions.\n6\nRelated Work\n6.1\nText-to-SQL\nText-to-SQL task, a key research area, involves\ntranslating natural language questions into SQL\nqueries. Seq2SQL (Zhong et al., 2017) is a notable\nmodel in this field, utilizing policy-based reinforce-\nment learning to accurately generate SQL queries,\nparticularly focusing on the unordered nature of\nquery conditions. It excelled in both execution and\nlogical form accuracy on the WikiSQL dataset. In\nthe same data set, TAPEX (Liu et al., 2022), an\nexecution-centric table pretraining approach that\nlearns a neural SQL executor over a synthetic cor-\npus, achieved the state-of-the-art results.\nThe Spider (Yu et al., 2018a) dataset furthered\nText-to-SQL research by presenting a complex,\ncross-domain semantic parsing challenge. It fea-\ntures varied SQL queries and databases, pushing\nmodels to adapt to new structures and databases.\nRESDSQL (Li et al., 2023a) introduced a ranking-\nenhanced encoding and skeleton-aware decoding\nframework that effectively decouples schema link-\ning and skeleton parsing, demonstrating improved\nparsing performance and robustness on the Spider\ndataset and its variants.\nRecent advances in large language models\n(LLMs), such as GPT-4 (OpenAI et al., 2023) and\nClaude-2, have also shown impressive results in\nthis domain (Gao et al., 2023; Pourreza and Rafiei,\n2023; Dong et al., 2023). To our knowledge, most\nprevious benchmarks, including Spider and Wik-\niSQL, focused on database schemas with limited\nrows, creating a gap between academic studies\nand real-world applications. To bridge this gap,\nthe BIRD (Li et al., 2023b) benchmark was in-\ntroduced, providing a comprehensive text-to-SQL\ndataset that emphasizes the challenges of dealing\nwith dirty and noisy database values, grounding\nexternal knowledge, and ensuring SQL efficiency\nin massive databases.\nHowever, adapting these methods from Text-to-\nSQL to text-to-CQL isn’t straightforward, primar-\nily because of the scarcity of training data for text-\nto-CQL. This challenge motivated the proposal of\nthis paper.\n6.2\nText-to-DSL\nThe field of generating Domain-Specific Lan-\nguages (DSLs) from natural language, known as\nText-to-DSL, has seen a surge in interest, pri-\nmarily due to the emergence of LLMs capable\nof understanding and generating structured lan-\nguages. A notable approach in this area is Grammar\nPrompting (Wang et al., 2023), which leverages\nBackus–Naur Form (BNF) grammars to provide\nLLMs with domain-specific constraints and exter-\nnal knowledge. This method has shown promise\nacross various DSL generation tasks, including se-\nmantic parsing and molecule generation.\nText-to-OverpassQL (Staniek et al., 2023) fo-\ncused on generating Overpass queries from natural\nlanguage. This task is particularly challenging due\nto the complex and open-vocabulary nature of the\nOverpass Query Language (OverpassQL). Staniek\net al. (2023) proposed the OverpassNL dataset and\nestablished task-specific evaluation metrics.\n7\nConclusion\nIn this paper, we introduce a novel task, text-to-\nCQL, aimed at converting natural language input\ninto Corpus Query Language (CQL). This task\nholds significant relevance for corpus development\nand research, sharing certain similarities with exist-\ning text-to-query language tasks. The text-to-CQL\ntask, however, presents distinctive challenges ow-\ning to its unique syntax and limited availability of\npublicly accessible resources. To support research\nin this domain, we propose TCQL—a template-\nbased generation approach for creating text-to-CQL\ndatasets. To ensure the authenticity of the dataset,\nwe build the data based on NLP tasks such as collo-\ncation extraction and lexical labeling. We use this\ndataset to test the performance of several state-of-\nthe-art models, propose a new evaluation metric,\nCQLBLEU, based on N-gram similarity and AST\nsimilarity, and build a baseline for the tasks with\nreference to several commonly used metrics in Text-\nto-SQL.We evaluate the results in detail, revealing\nthe strengths and weaknesses of the considered\nlearning strategies. We hope that this contribution\nwill positively impact corpus development and ap-\nplications, advancing technology in both the realms\nof NLP and linguistics.\nLimitations\nThis work introduces a novel approach to convert-\ning natural language queries into Corpus Query\nLanguage (CQL) expressions. Despite its poten-\ntial to significantly advance research in corpus lin-\nguistics and natural language processing, several\nlimitations must be acknowledged:\n• Currently, the construction of the TCQL\ndataset used in this paper is based on auto-\nmatically generated and manually labeled due\nto the lack of a large amount of raw CQL data\ngenerated from real human queries. Despite\nthe fact that we have used a variety of methods\nto enhance its authenticity, it is still possible\nto generate queries that are not meaningful\nenough.\n• The scalability of the proposed solution to\nlonger text queries and its dependency on com-\nputational resources are concerns that may\nlimit its applicability in resource-constrained\nsettings.\nFuture research is encouraged to address these\nlimitations, exploring the method’s applicability to\na wider range of languages, enhancing its scalabil-\nity, and reducing its computational requirements.\nReferences\nGiusepppe Attardi. 2015. Wikiextractor.\nhttps://\ngithub.com/attardi/wikiextractor.\nChandra Sekhar Bhagavatula, Thanapon Noraset, and\nDoug Downey. 2013. Methods for exploring and min-\ning tables on wikipedia. In Proceedings of the ACM\nSIGKDD workshop on interactive data exploration\nand analytics, pages 18–26.\nJess de Does, Jan Niestadt, and Katrien Depuydt.\n2017. Creating research environments with blacklab.\nCLARIN in the Low Countries, pages 245–257.\nNaihao Deng, Yulong Chen, and Yue Zhang. 2022. Re-\ncent advances in text-to-SQL: A survey of what we\nhave and what we expect. In Proceedings of the\n29th International Conference on Computational Lin-\nguistics, pages 2166–2187, Gyeongju, Republic of\nKorea. International Committee on Computational\nLinguistics.\nLudovic Denoyer and Patrick Gallinari. 2006.\nThe\nwikipedia xml corpus. In ACM SIGIR Forum, vol-\nume 40, pages 64–69. ACM New York, NY, USA.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nXuemei Dong, Chao Zhang, Yuhang Ge, Yuren Mao,\nYunjun Gao, lu Chen, Jinshu Lin, and Dongfang Lou.\n2023. C3: Zero-shot text-to-sql with chatgpt.\nDawei Gao, Haibin Wang, Yaliang Li, Xiuyu Sun,\nYichen Qian, Bolin Ding, and Jingren Zhou. 2023.\nText-to-sql empowered by large language models: A\nbenchmark evaluation.\nAndrew Hardie. 2012. Cqpweb—combining power,\nflexibility and usability in a corpus analysis tool. In-\nternational journal of corpus linguistics, 17(3):380–\n409.\nRenfen Hu and Hang Xiao. 2019. The construction\nof chinese collocation knowledge bases and their\napplication in second language acquisition. Applied\nLinguistics, (1):135–144.\nAdam Kilgarriff, Vít Baisa, Jan Bušta, Miloš Jakubíˇcek,\nVojtˇech Kováˇr, Jan Michelfeit, Pavel Rychl`y, and Vít\nSuchomel. 2014. The sketch engine: ten years on.\nLexicography, 1(1):7–36.\nAdam Kilgarriff, Pavel Rychly, Pavel Smrz, and David\nTugwell. 2008. The sketch engine. Practical Lexi-\ncography: a reader, pages 297–306.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2019.\nBART: denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. CoRR, abs/1910.13461.\nHaoyang Li, Jing Zhang, Cuiping Li, and Hong Chen.\n2023a.\nResdsql: decoupling schema linking and\nskeleton parsing for text-to-sql.\nIn Proceedings\nof the Thirty-Seventh AAAI Conference on Artifi-\ncial Intelligence and Thirty-Fifth Conference on In-\nnovative Applications of Artificial Intelligence and\nThirteenth Symposium on Educational Advances in\nArtificial Intelligence, AAAI’23/IAAI’23/EAAI’23.\nAAAI Press.\nJinyang Li, Binyuan Hui, Ge Qu, Binhua Li, Jiaxi\nYang, Bowen Li, Bailin Wang, Bowen Qin, Rongyu\nCao, Ruiying Geng, et al. 2023b. Can llm already\nserve as a database interface? a big bench for large-\nscale database grounded text-to-sqls. arXiv preprint\narXiv:2305.03111.\nQian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi\nLin, Weizhu Chen, and Jian-Guang Lou. 2022.\nTAPEX: Table pre-training via learning a neural SQL\nexecutor. In International Conference on Learning\nRepresentations.\nChristopher D Manning, Mihai Surdeanu, John Bauer,\nJenny Rose Finkel, Steven Bethard, and David Mc-\nClosky. 2014. The stanford corenlp natural language\nprocessing toolkit. In Proceedings of 52nd annual\nmeeting of the association for computational linguis-\ntics: system demonstrations, pages 55–60.\nOpenAI, :, Josh Achiam, Steven Adler, Sandhini Agar-\nwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Ale-\nman, Diogo Almeida, Janko Altenschmidt, Sam Alt-\nman, Shyamal Anadkat, Red Avila, Igor Babuschkin,\nSuchir Balaji, Valerie Balcom, Paul Baltescu, Haim-\ning Bao, Mo Bavarian, Jeff Belgum, Irwan Bello,\nJake Berdine, Gabriel Bernadett-Shapiro, Christo-\npher Berner, Lenny Bogdonoff, Oleg Boiko, Made-\nlaine Boyd, Anna-Luisa Brakman, Greg Brockman,\nTim Brooks, Miles Brundage, Kevin Button, Trevor\nCai, Rosie Campbell, Andrew Cann, Brittany Carey,\nChelsea Carlson, Rory Carmichael, Brooke Chan,\nChe Chang, Fotis Chantzis, Derek Chen, Sully Chen,\nRuby Chen, Jason Chen, Mark Chen, Ben Chess,\nChester Cho, Casey Chu, Hyung Won Chung, Dave\nCummings, Jeremiah Currier, Yunxing Dai, Cory\nDecareaux, Thomas Degry, Noah Deutsch, Damien\nDeville, Arka Dhar, David Dohan, Steve Dowl-\ning, Sheila Dunning, Adrien Ecoffet, Atty Eleti,\nTyna Eloundou, David Farhi, Liam Fedus, Niko\nFelix, Simón Posada Fishman, Juston Forte, Is-\nabella Fulford, Leo Gao, Elie Georges, Christian\nGibson, Vik Goel, Tarun Gogineni, Gabriel Goh,\nRapha Gontijo-Lopes, Jonathan Gordon, Morgan\nGrafstein, Scott Gray, Ryan Greene, Joshua Gross,\nShixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse\nHan, Jeff Harris, Yuchen He, Mike Heaton, Jo-\nhannes Heidecke, Chris Hesse, Alan Hickey, Wade\nHickey, Peter Hoeschele, Brandon Houghton, Kenny\nHsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu\nJain, Shawn Jain, Joanne Jang, Angela Jiang, Roger\nJiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie\nJonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser,\nAli Kamali, Ingmar Kanitscheider, Nitish Shirish\nKeskar, Tabarak Khan, Logan Kilpatrick, Jong Wook\nKim, Christina Kim, Yongjik Kim, Hendrik Kirch-\nner, Jamie Kiros, Matt Knight, Daniel Kokotajlo,\nŁukasz Kondraciuk, Andrew Kondrich, Aris Kon-\nstantinidis, Kyle Kosic, Gretchen Krueger, Vishal\nKuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan\nLeike, Jade Leung, Daniel Levy, Chak Ming Li,\nRachel Lim, Molly Lin, Stephanie Lin, Mateusz\nLitwin, Theresa Lopez, Ryan Lowe, Patricia Lue,\nAnna Makanju, Kim Malfacini, Sam Manning, Todor\nMarkov, Yaniv Markovski, Bianca Martin, Katie\nMayer, Andrew Mayne, Bob McGrew, Scott Mayer\nMcKinney, Christine McLeavey, Paul McMillan,\nJake McNeil, David Medina, Aalok Mehta, Jacob\nMenick, Luke Metz, Andrey Mishchenko, Pamela\nMishkin, Vinnie Monaco, Evan Morikawa, Daniel\nMossing, Tong Mu, Mira Murati, Oleg Murk, David\nMély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak,\nArvind Neelakantan, Richard Ngo, Hyeonwoo Noh,\nLong Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex\nPaino, Joe Palermo, Ashley Pantuliano, Giambat-\ntista Parascandolo, Joel Parish, Emy Parparita, Alex\nPassos, Mikhail Pavlov, Andrew Peng, Adam Perel-\nman, Filipe de Avila Belbute Peres, Michael Petrov,\nHenrique Ponde de Oliveira Pinto, Michael, Poko-\nrny, Michelle Pokrass, Vitchyr Pong, Tolly Pow-\nell, Alethea Power, Boris Power, Elizabeth Proehl,\nRaul Puri, Alec Radford, Jack Rae, Aditya Ramesh,\nCameron Raymond, Francis Real, Kendra Rimbach,\nCarl Ross, Bob Rotsted, Henri Roussez, Nick Ry-\nder, Mario Saltarelli, Ted Sanders, Shibani Santurkar,\nGirish Sastry, Heather Schmidt, David Schnurr, John\nSchulman, Daniel Selsam, Kyla Sheppard, Toki\nSherbakov, Jessica Shieh, Sarah Shoker, Pranav\nShyam, Szymon Sidor, Eric Sigler, Maddie Simens,\nJordan Sitkin, Katarina Slama, Ian Sohl, Benjamin\nSokolowsky, Yang Song, Natalie Staudacher, Fe-\nlipe Petroski Such, Natalie Summers, Ilya Sutskever,\nJie Tang, Nikolas Tezak, Madeleine Thompson, Phil\nTillet, Amin Tootoonchian, Elizabeth Tseng, Pre-\nston Tuggle, Nick Turley, Jerry Tworek, Juan Fe-\nlipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya,\nChelsea Voss, Carroll Wainwright, Justin Jay Wang,\nAlvin Wang, Ben Wang, Jonathan Ward, Jason Wei,\nCJ Weinmann, Akila Welihinda, Peter Welinder, Ji-\nayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner,\nClemens Winter, Samuel Wolrich, Hannah Wong,\nLauren Workman, Sherwin Wu, Jeff Wu, Michael\nWu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qim-\ning Yuan, Wojciech Zaremba, Rowan Zellers, Chong\nZhang, Marvin Zhang, Shengjia Zhao, Tianhao\nZheng, Juntang Zhuang, William Zhuk, and Barret\nZoph. 2023. Gpt-4 technical report.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th annual meeting of the Association for Computa-\ntional Linguistics, pages 311–318.\nMohammadreza Pourreza and Davood Rafiei. 2023.\nDin-sql: Decomposed in-context learning of text-\nto-sql with self-correction.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. The Journal of Machine Learning Research,\n21(1):5485–5551.\nShuo Ren, Daya Guo, Shuai Lu, Long Zhou, Shujie Liu,\nDuyu Tang, Neel Sundaresan, Ming Zhou, Ambrosio\nBlanco, and Shuai Ma. 2020. Codebleu: a method\nfor automatic evaluation of code synthesis.\nYunfan Shao, Zhichao Geng, Yitao Liu, Junqi Dai,\nFei Yang, Li Zhe, Hujun Bao, and Xipeng Qiu.\n2021. Cpt: A pre-trained unbalanced transformer\nfor both chinese language understanding and genera-\ntion. arXiv preprint arXiv:2109.05729.\nMichael Staniek, Raphael Schumann, Maike Züfle, and\nStefan Riezler. 2023. Text-to-overpassql: A natural\nlanguage interface for complex geodata querying of\nopenstreetmap.\nShuo Sun, Yuchen Zhang, Jiahuan Yan, Yuze Gao,\nDonovan Ong, Bin Chen, and Jian Su. 2023. Battle\nof the large language models: Dolly vs llama vs vi-\ncuna vs guanaco vs bard vs chatgpt – a text-to-sql\nparsing comparison.\nAnn Taylor, Mitchell Marcus, and Beatrice Santorini.\n2003. The penn treebank: an overview. Treebanks:\nBuilding and using parsed corpora, pages 5–22.\nBailin Wang, Zi Wang, Xuezhi Wang, Yuan Cao, Rif A.\nSaurous, and Yoon Kim. 2023. Grammar prompting\nfor domain-specific language generation with large\nlanguage models.\nJingsi Yu, Shi Jialu, Liner Yang, Dan Xiao, and Erhong\nYang. 2022. Transformation of enhanced dependen-\ncies in chinese. In Proceedings of the 21st Chinese\nNational Conference on Computational Linguistics,\npages 99–109.\nTao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga,\nDongxu Wang, Zifan Li, James Ma, Irene Li, Qingn-\ning Yao, Shanelle Roman, Zilin Zhang, and Dragomir\nRadev. 2018a. Spider: A large-scale human-labeled\ndataset for complex and cross-domain semantic pars-\ning and text-to-SQL task. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 3911–3921, Brussels, Bel-\ngium. Association for Computational Linguistics.\nTao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga,\nDongxu Wang, Zifan Li, James Ma, Irene Li, Qingn-\ning Yao, Shanelle Roman, et al. 2018b. Spider: A\nlarge-scale human-labeled dataset for complex and\ncross-domain semantic parsing and text-to-sql task.\narXiv preprint arXiv:1809.08887.\nVictor Zhong, Caiming Xiong, and Richard Socher.\n2017. Seq2sql: Generating structured queries from\nnatural language using reinforcement learning.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2024-02-21",
  "updated": "2024-02-21"
}