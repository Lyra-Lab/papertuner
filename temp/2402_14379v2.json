{
  "id": "http://arxiv.org/abs/2402.14379v2",
  "title": "Novi jezički modeli za srpski jezik",
  "authors": [
    "Mihailo Škorić"
  ],
  "abstract": "The paper will briefly present the development history of transformer-based\nlanguage models for the Serbian language. Several new models for text\ngeneration and vectorization, trained on the resources of the Society for\nLanguage Resources and Technologies, will also be presented. Ten selected\nvectorization models for Serbian, including two new ones, will be compared on\nfour natural language processing tasks. Paper will analyze which models are the\nbest for each selected task, how does their size and the size of their training\nsets affect the performance on those tasks, and what is the optimal setting to\ntrain the best language models for the Serbian language.",
  "text": "Научни рад\nНови jезички модели за српски jезик\nУДК\nСАЖЕТАК: У раду ће укратко бити\nприказан\nисториjат\nразвоjа\njезичких\nмодела за српски jезик коjи су засновани\nна\nтрансформерскоj\nархитектури.\nБиће,\nтакође,\nпредстављено\nнеколико\nнових\nмодела\nза\nгенерисање\nи\nвекторизациjу\nтекста, обучених на ресурсима Друштва\nза jезичке ресурсе и технологиjе. Десет\nодабраних модела за векторизациjу српског\njезика, међу коjима су и два нова модела,\nбиће упоређена на четири задатка обраде\nприродног jезика. Биће анализирано коjи\nмодели су наjбољи за изабране задатке,\nкако величина модела и величина скупа за\nобучавање утичу на њихове перформансе\nна тим задацима и шта jе потребно за\nобучавање\nнаjбољих\nмодела\nза\nсрпски\njезик.\nКЉУЧНЕ РЕЧИ: jезички модели,\nсрпски jезик, векторизациjа, обрада\nприродног jезика.\nРАД ПРИМЉЕН:\n27. jануар 2024.\nРАД ПРИХВАЋЕН:\n24. фебруар 2024.\nМихаило Шкорић\nmihailo.skoric@rgf.bg.ac.rs\nORCID: 0000-0003-4811-8692\nУниверзитет у Београду\nРударско-геолошки факултет\nБеоград, Србиjа\n1.\nУвод\nПочетком двадесет и првог века, дошло jе наjпре до наглог пораста\nколичине доступних текстуалних података, а потом и до наглог раста\nрачунарске моћи, што jе покренуло талас истраживања заснованих на\nидеjи дубоког учења (deep learning) (LeCun, Bengio, and Hinton 2015).\nУ случаjу обраде природних jезика, истраживања кулминираjу поjавом\nархитектуре трансформера (Vaswani et al. 2017), коjа се базира на\nупотреби енкодера, чиjа jе главна намена анализа текста, и декодера,\nкоjи су задужени за синтезу текста. Први изразито популаран модел\nовог типа био jе BERT 1 (Devlin et al. 2018), заснован искључиво на\n1. Bidirectional Encoder Representations from Transformers\n– двосмерно\nкодирање репрезентациjе из трансформера\nИнфотека, год. 24, бр. 1, фебруар 2024.\n1\narXiv:2402.14379v2  [cs.CL]  23 Feb 2024\nШкорић М., Нови jезички модели за српски jезик, стр. 1–22\nтрансформерском енкодеру. Оваj модел jе направио велики помак у\nобради природних jезика, пре свега на задацима коjи се засниваjу на\nвекторизациjи текста. Његове вариjациjе, RoBERTa2 (Liu et al. 2019)\nи DeBERTa3 (He et al. 2020) и данас постижу наjбоље резултате\nна задацима угњежђивања речи (word embedding), анотациjе речи\n(нпр. обележавање врсте речи и препознавање именованих ентитета) и\nкласификациjе реченица и докумената. Са друге стране, поjављивање\nмодела GPT\n(генеративни предобучени трансформер) (Radford et\nal. 2018) и GPT-2 (Radford et al. 2019) jе популаризовало jезичке\nмоделе засноване на траснформерском декодеру, а ова група модела\nсе данас наjбрже развиjа. Модели коjи комбинуjу употребу енкодера и\nдекодера, као што су, на пример BART (Lewis et al. 2020) и T5 (Raffel et\nal. 2020), остаjу недовољно запажени упркос изванредним резултатима\nкоjе постижу на задацима трансформациjе текста, као што су машинско\nпревођење, сумaризациjа и прилагођавање стила.\n1.1\nПреглед обjављених модела за српски jезик\nJезички\nмодели\nзасновани\nна\nархитектури\nтрансформера\nсу\nнаправили продор у српски jезик путем вишеjезичних модела, наjпре\nкроз MBERT 4 (Devlin et al. 2018), а потом и кроз XLM-RoBERTa\nмодел5 (Conneau et al. 2019), за чиjе обучавање jе коришћено око 4\nмилиjарде токена из тесктова писаних на српском или другом блиско-\nсродном jезику (хрватски, босански). Потоњи модел обjављен jе у\nдецембру 2019. године у две вариjанте, base (279 милиона параметара)\nи large (561 милион параметара). И данас се, као jедан од наjвећих\nмодела за векторизациjу, употребљава у обради српског jезика и притом\nостваруjе добре резултате, поготово након дообучавања.\nПочетком 2021. године, на платформи Huggingface6 обjављен jе модел\nпод називом БЕРТић (classla/bcms-bertic) (Ljubeˇsi´c and Lauc 2021),\nбазиран на архитектури ELECTRA (Clark et al. 2020), са 110 милиона\nпараметара, обучаван на корпусу од преко 8 милиjарди токена, босанског\n(800 милиона), хрватског (5.5 милиjарди), црногорског (80 милиона) и\nсрпског jезика (2 милиjарде).\n2. Robustly Optimized BERT – Робусно оптимизовани BERT\n3. Decoding-Enhanced BERT – BERT проширен декодирањем\n4. Multilingual BERT – вишеjезични BERT\n5. Cross-lingual Language Model – Међуjезички jезички модел\n6. Huggingface, наjвеће веб чвориште за обjављивање jезичких модела.\n2\nИнфотека, год. 24, бр. 1, фебруар 2024.\nНаучни рад\nКасниjе те исте године, направљени су и обjављени први специфични\nмодели за српски jезик у оквиру jедног ширег jезичког истраживања за\nмакедонски jезик (Dobreva et al. 2022). Прецизниjе, обjављена jе српска\nверзиjа RoBERTa-base модела, macedonizer/sr-roberta-base (120 милиона\nпараметара) и српска верзиjа GPT2-small модела, macedonizer/sr-gpt2\n(130 милиона параметара). Оба модела су обучавана на корпусу српске\nВикипедиjе и подржаваjу само ћирилично писмо.\nНедуго потом предузет jе сличан подухват, при коjем jе обучено пет\nRoBERTa-base модела за српски jезик (Cveji´c 2022). Инициjални модел,\nAndrija/SRoBERTa, имао jе 120 милиона параметара и обучаван jе на\nмалом корпусу од 18 милиона токена познатом под именом Leipzig (Bie-\nmann et al. 2007), док су потоња четири модела имала по 80 милиона\nпараметара, при чему jе сваки обучаван на све већем корпусу. За модел\nAndrija/SRoBERTa-base додат jе корпус OSCAR (Su´arez, Sagot, and Ro-\nmary 2019) (220 милиона токена), за модел Andrija/SRoBERTa-L jе поред\nњега додат и srWAc (Ljubeˇsi´c and Klubiˇcka 2014) (490 милиона токена), за\nмодел Andrija/SRoBERTa-XL jе уз претходне додат и део корпуса cc100-\nhr (21 милиjарда токена) и cc100-sr (5.5 милиjарди токена) (Wenzek et\nal. 2020), док су за модел Andrija/SRoBERTa-F сви поменути корпуси\nкоришћени у целости.\nКраjем 2022. године обjављена су три експериментална генеративна\nмодела за српски jезик (ˇSkori´c 2023). Контролни модел procesaur/gpt2-\nsrlat jе био поново заснован на GPT2-small архитектури, имао jе 138\nмилиона параметара и био jе обучен на исечку корпуса Друштва за\njезичке ресурсе и технологиjе (260 милиона токена) (Krstev and Stankovi´c\n2023). Друга два модела, procesaur/gpt2-srlat-sem\nи procesaur/gpt2-\nsrlat-synt, настала су дообучавањем контролног модела коришћењем\nдва специjално припремљена корпуса са циљем засебног моделовања\nсемантике, односно, синтаксе текста. Три модела су потом употребљена\nза експеримент комбиновања jезичких модела на задатку класификациjе\nреченица (ˇSkori´c, Utvi´c, and Stankovi´c 2023).\nПочетком наредне године, истраживачи са Универзитета у Нишу\nобjавили су модел JelenaTosic/SRBerta (75 милиона параметара) коjи\njе такође заснован на RoBERTa-base архитектури, а коjи jе обучаван\nпомоћу корпуса OSCAR (Su´arez, Sagot, and Romary 2019).Занимљиво jе\nда jе оваj модел, као и његова друга верзиjа (nemanjaPetrovic/SRBerta,\n120 милиона параметара), пре обjављивања дообучен над текстовима из\nдомена права (Bogdanovi´c, Koci´c, and Stoimenov 2024). У периоду између\nобjављивања ова два модела, обjављен jе и aleksahet/xlm-r-squad-sr-\nИнфотека, год. 24, бр. 1, фебруар 2024.\n3\nШкорић М., Нови jезички модели за српски jезик, стр. 1–22\nlat (Cvetanovi´c and Tadi´c 2023), први модел за одговарање на питања на\nсрпском jезику, настао прилагођавањем XLM-RoBERTа модела помоћу\nскупа података SQuAD (Rajpurkar, Jia, and Liang 2018), преведеног на\nсрпски jезик.\nСредином 2023. године обjављена су jош два генеративна модела\nзаснована на ГПТ архитектури. Оба модела су обучавана над истим\nскупом\nподатака:\nнад\nкорпусима\nДруштва\nза\njезичке\nресурсе\nи\nтехнологиjе (Krstev and Stankovi´c 2023), докторским дисертациjама\nпреузетим са платформе НАРДУС,7 корпусу jавног дискурса српског\njезика Института за српски jезик САНУ под називом PDRS (Wasser-\nscheidt 2023) и додатним jавно доступним корпусима са веба, као што\nсу већ поменути srWAc (Ljubeˇsi´c and Klubiˇcka 2014) и cc100-sr (Wen-\nzek et al. 2020). Укупан броj токена у овом скупу података броjи око\n4 милиjарде токена. Већи модел, jerteh/gpt2-orao,8 броjи 800 милиона\nпараметара, заснован jе на архитектури GPT2-large и представља\nтренутно наjвећи доступни модел предобучен за српски jезик. Мањи\nмодел, jerteh/gpt2-vrabac,9 броjи 136 милиона параметара и заснован\njе на архитектури GPT2-small. Оба модела су обучавана коришћењем\nрачунарских ресурса Националне платформе за вештачку интелигенциjу\nСрбиjе. Осим корпуса за обучавање, ова два модела деле и речник токена\nи токенизатор, специjално опремљен да упаруjе ћирилична и латинична\nслова, омогућуjући њихову равноправну подршку.\nНакон обjављивања генеративног модела од 800 милиона (jerteh/gpt2-\norao) параметара, фокус се полако помера на дообучавање великих\nмодела коришћењем текстова на српском jезику. Тако су обjављена\nдва модела заснована на архитектури Alpaca\n(Taori et al. 2023),\ndatatab/alpaca-serbian-3b-base (3 милиjарде параметара) и datatab/alpaca-\nserbian-7b-base (7 милиjарди параметара), a наjављено jе и обjављивање\njош jедног модела исте величине, заснованог на архитектури Mistral-\n7b (Jiang et al. 2023), коjи jе обучаван на хрватским, босанским и\nсрпским текстовима коjи броjе 11,5 милиjарди токена. Исти корпус од\n11,5 милиjарди токена коришћен jе и за дообучавање XLM-RoBERTa-\nlarge модела у циљу поређења перформанси дообучаваних модела у\nодносу на моделе коjи су обучавани од почетка (од нуле). Нови модел jе\n7. НАРДУС – Национални репозиториjум докторских дисертациjа са свих\nуниверзитета у Србиjи.\n8. jerteh/gpt2-orao\n9. jerteh/gpt2-vrabac\n4\nИнфотека, год. 24, бр. 1, фебруар 2024.\nНаучни рад\nобjављен под именом classla/xlm-r-bertic и броjи 561 милион параметара,\nколико има и оригинални XLM модел.\nКоначно, на скупу података над коjим су обучавани jerteh/gpt2-\norao и jerteh/gpt2-vrabac, обучена су jош два модела за векторизациjу\nтекста. Већи модел, jerteh/Jerteh-355 10, заснован jе на RoBERTa-large\nархитектури и броjи 355 милиона параметара, док jе мањи модел,\njerteh/Jerteh-81,11 заснован на RoBERTa-base архитектури и броjи 81\nмилион параметара. Као и код модела jerteh/gpt2-orao, циљ jе био\nда се модели обуче на што квалитетниjем корпусу текстова. У овом\nраду биће представљено испитивање перформанси ова два модела и\nњихово поређење са перформансама других одабраних модела како\nби се установило њихово место у хиjерархиjи jезичких модела за\nвекторизациjу текста на српском jезику.\n1.2\nПоставка експеримента\nУ\nпретходном\nодељку\njе\nуказано\nна\nпостоjање\nвећег\nброjа\nвишеjезичних модела коjи, у мањоj или већоj мери, подржаваjу обраду\nсрпског jезика, као и на двадесетак модела коjи су припремљени\nспециjално за обраду српског jезика. Обjављени модели се међусобно\nразликуjу према неколико особина: породици (архитектури) модела и\nброjу његових параметара, речнику, односно токенизатору, на коjем се\nзасниваjу, скупу коjи jе коришћен за њихово обучавање, задатку на\nкоjем jе модел обучаван и дужини обучавања. Треба напоменути да\nнеке од ових информациjа недостаjу за неке од модела, али и да су неке\nинформациjе коjе су доступне (пре свега особине скупа коjи jе коришћен\nза обучавање) непроверљиве.\nУ наставку, рад ће се фокусирати на десет одабраних модела за\nвекторизациjу (општег типа). Основне информациjе о тим моделима\nбиће\nпредстављене\nу\nодељку\n2.,\nексперимент\nпоређења\nњихових\nперформанси\nна\nчетири\nприпремљена\nзадатка\nбити\nприказан\nу\nодељку 3., а резултати експеримената ће бити приказани и размотрени\nу одељку 4. Напослетку, у одељку 5., биће предложен процес обучавања\nнових модела за српски jезик. Оваj рад се неће фокусирати на\nгенеративне моделе услед недостатка поузданог (али аутоматског)\nмеханизма за мерење њихових перформанси. Jош увек ниjе обjављен\nниjедан енкодер-декодер модел специjално развиjен за српски jезик.\n10. jerteh/Jerteh-355\n11. jerteh/Jerteh-81\nИнфотека, год. 24, бр. 1, фебруар 2024.\n5\nШкорић М., Нови jезички модели за српски jезик, стр. 1–22\n2.\nОдабрани модели за векторизациjу текста\nЗа потребе овог рада, од претходно поменутих модела (одељак 1.)\nодабрано jе десет коjи ће бити детаљниjе анализирани. У тих десет\nулазе наjпре четири SRoBERTa модела, коjи су, услед тога што се\nразликуjу искључиво по скупу података за обучавање, врло погодни\nза оваj експеримент. Даље, ту су наjстариjи модел, classla/bcms-\nbertic и наjновиjи модел, classla/xlm-r-bertic, коjе jе обjавио центар за\njужнословенске jезике CLASSLA, као и два наjпопуланриjа вишеjезична\nмодела xlm-roberta-base и xlm-roberta-large. Коначно, ту су два модела\nкоjа се први пут представљаjу у овом раду, модели jerteh-81 и jerteh-355,\nкоjи су обучени над ресурсима Друштва за jезичке ресурсе и технологиjе.\nОсновне карактеристике ових десет модела приказане су у Табели 1.\nУ приложеноj табели, као и из описа модела у претходном одељку,\nвиди се да jе наjпопуларниjа архитектура RoBERTa (6 од 10 одабраних\nмодела), a додатна три модела заснована су на блискоj, XLM-RoBERTa\nархитектури. Преостали модел, bcms-bertic, заснива се на ELECTRA\nархитектури и jедини jе од одабраних коjи ниjе обучаван на задатку\nмоделовања маскираног jезика (предвиђања делова текста маскираних\nиза неке специjалне етикете).\nВеличина одабраних модела варира од 80 (за четири SRoBERTa\nмодела) па до преко 560 милиона параметара (за моделе засноване\nна XLM-RoBERTa-large). Величина скупа за обучавање варира од 500\nмилиона за модел 1 (SRoBERTa-base), па до чак 11,5 милиjарди токена\nза модел 6 (classla/xlm-r-bertic), при чему треба напоменути да он ниjе\nобучаван од нуле, већ jе у питању xlm-roberta-large модел дообучен\nза хрватски, босански и српски jезик. Само четири од десет модела\nсу обучавана искључиво над корпусом српских текстова. У питању су\nмодели 1, 2, 9 и 10, тj. прва два SRoBERTa модела, jerteh/jerteh-81 и\njerteh/jerteh-355.\nБитно jе напоменути и да десет приказаних модела користе само\nчетири различита речника токена, односно токенизатора:\nX1 SRoBERTa токенизатор - прва 4 модела;\nX2 bertic токенизатор - модел броj 5;\nX3 XLM-R токенизатор - модели 6 до 8;\nX4 jerteh токенизатор - последња 2 модела (9 и 10).\n6\nИнфотека, год. 24, бр. 1, фебруар 2024.\nНаучни рад\nредни броj\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nИдентификатор\nAndrija/SRoBERTa-base\nAndrija/SRoBERTa-L\nAndrija/SRoBERTa-XL\nAndrija/SRoBERTa-F\nclassla/bcms-bertic\nclassla/xlm-r-bertic\nxlm-roberta-base\nxlm-roberta-large\njerteh/jerteh-81\njerteh/jerteh-355\nРечник токена\nSRoBERTa\nbertic\nXLM-R\njerteh\nАрхитектура\nRoBERTa\nELE.\nXLM-R\nRoBERTa\nВеличина модела\n80\n110\n561\n279\n561\n81\n355\nВеличина скупа\n500\n1000 3750 5700\n8400 11500\n4000*\n4000\nСрпски\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\nХрватски\n✓\n✓\n✓\n✓\n✓\n✓\nБосански\n✓\n✓\n✓\n✓\nЦрногорски\n✓\n✓\n✓\nТабела 1. Десет одабраних модела за векторизациjу текста на српском\njезику и њихове особине: речник токена на коjем су засновани, архитектура\nмодела, величина модела изражена у милионима параметара и величина\nскупа изражена у милионима токена. Подаци су преузети са платформе Hug-\ngingFace. *Величина скупа за обучавање код модела 7 и 8 (xlm-roberta-base,\nxlm-roberta-large) односи се на део скупа на српском, хрватском или другом\nсродном jезику. Доњи део табеле приказуjе на коjем од ових jезика су модели\nобучавани.\nИнфотека, год. 24, бр. 1, фебруар 2024.\n7\nШкорић М., Нови jезички модели за српски jезик, стр. 1–22\n3.\nПоставка евалуациje перформанси модела\nДесет одабраних модела jе евалуирано на четири засебна задатка\nкако би се упоредиле њихове перформансе:\nT1 Моделовање маскираног jезика (погађање недостаjућих токена);\nT2 Израчунавање (семантичке) сличности између реченица;\nT3 Обележавање врстом речи;\nT4 Препознавање именованих ентитета.\nПрва два задатка припадаjу групи такозваних узводних задатака (up-\nstream), то jест задатака коjи користе моделе у њиховом основном стању,\nдок друга два задатка припадаjу групи низводних задатака (down-\nstream) jер захтеваjу да се модели фино подесе (fine-tune) и тестираjу\nна специjално припремљеном скупу података.\n3.1\nЕвалуациjа модела на узводним задацима\nКао што jе већ поменуто, за узводне задатке ниjе неопходно\nприлагођавање модела, тако да jе потребно само де се припреме скупове\nза тестирање.\nКако би се спровела евалуациjа модела на задатку моделовања\nмаскираног jезика (T1) наjпре jе припремљен специjалан скуп података\nу виду текстова у коjима су у свакоj реченици по jедан насумично\nизабрани токен маскирани – по jедан токен jе сакривен, на пример иза\nмаске <MASK>. За текстуалну грађу су коришћена четири извора:\nY1 Дечко, српски превод романа Подросток од Достоjевског;\nY2 Младић, алтернативни превод Дечка;\nY3 Пут око света за 80 дана, српски превод романа Жила Верна;\nY4 Пут око свиjета у 80 дана, хрватски превод романа Жила Верна.\nПрва два извора нису коришћена за обучавање ниjедног модела, док\nсу друга два већ дуго доступни на вебу (Vitas et al. 2008) и стога су\nвероватно коришћени за обучавање већине, ако не и свих, наведених\nмодела. Како неки модел не би био у посебноj предности, текстови су\nтокенизовани коришћењем сва четири токенизатора (X1 до X4), потом\nмаскирани, а онда jе пред сваким од модела био задатак да одмаскира\nсвих шеснаест припремљених текстова (четири извора токенизована и\nмаскирана на четири начина). У свакоj реченици био jе маскиран по\n8\nИнфотека, год. 24, бр. 1, фебруар 2024.\nНаучни рад\njедан токен, а модели су током евалуациjе за његово место нудили по\nтри кандидата. За процену резултата теста узимана jе мера тачности на\nовом задатку, при чему се као погодак рачунало свако одмаскирање код\nкога jе маскирани, то jест тражени, токен био у скупу кандидата коjе jе\nмодел понудио за задату реченицу.\nЗа потребе евалуациjе на задатку израчунавања сличности између\nреченица (T2), коришћене су реченице из истих романа, то jест два пара\nпаралелизованих романа (Y1 и Y2, односно Y3 и Y4), али припремљене као\nтриплети. С обзиром на то да су романи претходно паралелизовани на\nнивоу реченице, било jе лако направити парове реченица коjе имаjу исто\nзначење. Сваки триплет jе употпуњавала додатна реченица из романа\nпарњака, коjа дели што jе више могуће токена са контролном реченицом\nи има сличну дужину, али jе повучена из неког другог места у тексту.\nПример триплета:\n1. \"Zaista, ko ne bi obiˇsao svet i za manju cenu?\" (контролна реченица,\nY3: Пут око света за 80 дана)\n2. \"Doista, nije li i za manje od toga vrijedno izvrˇsiti put oko svijeta?\"\n(парњак, Y4: Пут око свиjета у 80 дана)\n3. \"He! he! pa konaˇcno zaˇsto ne bi uspio?\" (лажни парњак, Y4: Пут око\nсвиjета у 80 дана)\nЗадатак модела jе био да у задатим триплетима препознаjу правог\nпарњака (сличност између прве и друге реченице треба да буде већа него\nизмеђу прве и треће), а за процену резултата теста jе узимана тачност на\nтом задатку. Да би се израчунао реченични вектор, модел наjпре додели\nвекторске вредности сваком токену у реченици, а потом се вредност\nтих вектора усредњава како би се добила векторска репрезентациjа\nреченице. Сличност између две реченице се израчунава као разлика\nброjа 1 и косинусне удаљености израчунатих реченичних вектора.\n3.2\nЕвалуациjа модела на низводним задацима\nЗа потребе евалуациjе модела преостала два предвиђена задатка,\nмодели су дообучени и тестирани на посебним скуповима података. За\nобележавање врстом речи (T3) коришћен jе jавно доступни скуп Srp-\nKor4Tagging (Stankovi´c et al. 2020) (триста педесет хиљада обележених\nтокена), док jе за препознавање именованих ентитета (T4) коришћен\nдруги jавно доступни скуп SrpELTeC-gold (Todorovi´c et al. 2021). У\nоба случаjа, модели су дообучени на 90% обележених реченица из\nИнфотека, год. 24, бр. 1, фебруар 2024.\n9\nШкорић М., Нови jезички модели за српски jезик, стр. 1–22\nсваког скупа и тестирани на преосталих 10%. Како се ради о проблему\nвишекласне класификациjе, за процену резултата ова два задатка узете\nсу F1-мере остварене приликом класификациjе над реченицама из скупа\nза тестирање.\n4.\nРезултати евалуациjе\nРезултати првог теста (T1), то jест просечна тачност одабраних\nмодела на задатку допуњавања недостаjућег токена у сваком од\nшеснаест припремљених текстова, приказани су у табели 2.\nУ приложеним резултатима jе може се уочити супериорност новог\nмодела, jerteh-355, коjи остваруjе бољи резултат од осталих модела\nу тринаест од шеснаест случаjева, односно бољи или исти резултат\n(±1%) у петнаест од шеснаест случаjева. Осим тога, у девет од дванаест\nслучаjева модел jerteh-355 надмашуjе друге моделе чак и када обрађуjе\nтекст маскиран токенизатором тих модела. Jедини модел коjи успева\nда га надмаши у два случаjа jе SRoBERTa-F, коjи се наjчешће показуjе\nкао наjбољи у обради извора (Y4) писаног на хрватском jезику, коjи jе\nу великом проценту био укључен у његов скуп за обучавање. Ипак, у\nпросеку, његова тачност на овом тесту jе нижа и од оне коjу остваруjе\nдруги нови модел, jerteh-81. Модел 5 (classla/bcms-bertic), коjи, за\nразлику од осталих, ниjе обучаван на задатку моделовања маскираног\njезика, ниjе био укључен у евалуациjу на овом задатку, jер би био у\nнеповољном положаjу.\nРезултати теста израчунавања сличности између реченица (T2)\nприказани су у табели 3. Вредности приказуjу тачност модела при\nпрепознавању реченица са истим/сличним значењем у триплетима\nекстрахованим из два српска превода истог романа, Y1 i Y2 (први ред\nвредности), из српског и хрватског превода истог романа, Y3 и Y4 (други\nред), и просечну тачност (трећи ред).\nРезултати за први низ триплета су веома добри за неколико\nмодела: SRoBERTa-L, SRoBERTa-XL, SRoBERTa-F, jerteh-81 и jerteh-\n355 остваруjу сличну тачност од преко 95%. Модел SRoBERTa-XL\nостваруjе наjбоље резултате, уз малу маргину, али и наjбољи резултат\nза други низ триплета коjи садржи реченице на хрватском jезику (92%\nтачности), па самим тим има и наjбољи просечни резултат на овом\nзадатку. Jедини други модел коjи остваруjе тачност од преко 90% за\nдруги низ триплета jе SRoBERTa-F, што jе и очекивано jер су ова два\nмодела обучавана на хрватским текстовима.\n10\nИнфотека, год. 24, бр. 1, фебруар 2024.\nНаучни рад\nредни броj\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nAndrija/SRoBERTa-base\nAndrija/SRoBERTa-L\nAndrija/SRoBERTa-XL\nAndrija/SRoBERTa-F\nclassla/bcms-bertic\nclassla/xlm-r-bertic\nxlm-roberta-base\nxlm-roberta-large\njerteh/jerteh-81\njerteh/jerteh-355\nX1-Y1\n0.43\n0.63\n0.66\n0.70\n/\n0.43\n0.46\n0.51\n0.70\n0.75\nX1-Y2\n0.43\n0.62\n0.64\n0.69\n/\n0.42\n0.46\n0.50\n0.69\n0.73\nX1-Y3\n0.37\n0.56\n0.59\n0.63\n/\n0.34\n0.38\n0.43\n0.66\n0.72\nX1-Y4\n0.36\n0.55\n0.64\n0.68\n/\n0.34\n0.38\n0.42\n0.58\n0.63\nX2-Y1\n0.36\n0.47\n0.51\n0.54\n/\n0.47\n0.50\n0.55\n0.57\n0.60\nX2-Y2\n0.37\n0.48\n0.51\n0.54\n/\n0.46\n0.50\n0.54\n0.56\n0.59\nX2-Y3\n0.31\n0.41\n0.45\n0.48\n/\n0.42\n0.45\n0.50\n0.50\n0.54\nX2-Y4\n0.31\n0.42\n0.47\n0.51\n/\n0.42\n0.46\n0.50\n0.47\n0.51\nX3-Y1\n0.37\n0.49\n0.52\n0.54\n/\n0.48\n0.50\n0.55\n0.57\n0.60\nX3-Y2\n0.37\n0.48\n0.51\n0.54\n/\n0.46\n0.50\n0.54\n0.57\n0.59\nX3-Y3\n0.30\n0.41\n0.44\n0.47\n/\n0.41\n0.45\n0.49\n0.50\n0.54\nX3-Y4\n0.31\n0.42\n0.47\n0.51\n/\n0.41\n0.46\n0.50\n0.47\n0.50\nX4-Y1\n0.42\n0.60\n0.63\n0.67\n/\n0.43\n0.47\n0.51\n0.73\n0.78\nX4-Y2\n0.41\n0.58\n0.61\n0.65\n/\n0.41\n0.45\n0.49\n0.71\n0.75\nX4-Y3\n0.35\n0.53\n0.55\n0.60\n/\n0.33\n0.38\n0.42\n0.69\n0.76\nX4-Y4\n0.34\n0.50\n0.58\n0.62\n/\n0.33\n0.37\n0.41\n0.62\n0.66\nпросек\n0.36\n0.51\n0.55\n0.59\n/\n0.41\n0.45\n0.49\n0.60\n0.64\nТабела 2. Тачност модела у погађању токена (из три покушаjа) на задатку\nмоделовања маскираног jезика над шеснаест припремљених маскираних\nтекстова и њихова просечна тачност. Текстови су обележени (на почетку\nсваког реда) jединственим комбинациjама ознака токенизатора X и извора\nY . У сваком реду наjбољи резултат (±1%) обележен jе подебљањем.\nРезултати коjи су модели остварили на низводним задацима T3\n(обележавање врсте речи) и T4 (препознавање именованих ентитета)\nприказани су у виду F1-мера у Табели 4.\nИнфотека, год. 24, бр. 1, фебруар 2024.\n11\nШкорић М., Нови jезички модели за српски jезик, стр. 1–22\nредни броj\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nAndrija/SRoBERTa-base\nAndrija/SRoBERTa-L\nAndrija/SRoBERTa-XL\nAndrija/SRoBERTa-F\nclassla/bcms-bertic\nclassla/xlm-r-bertic\nxlm-roberta-base\nxlm-roberta-large\njerteh/jerteh-81\njerteh/jerteh-355\nY1-Y2\n0.93\n0.95\n0.96\n0.96\n0.92\n0.76\n0.90\n0.87\n0.95\n0.95\nY3-Y4\n0.83\n0.89\n0.92\n0.91\n0.79\n0.66\n0.78\n0.71\n0.89\n0.83\nпросек\n0.88\n0.92\n0.94\n0.93\n0.85\n0.71\n0.84\n0.79\n0.92\n0.89\nТабела 3. Резултати одабраних модела на задатку препознавања реченица\nса истим значењем у триплетима екстрахованим из превода Достоjевског (Y1-\nY2), Верна (Y3-Y4), као и у просеку. У сваком реду наjбољи резултат (±1%)\nобележен jе подебљањем.\nр.б.\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nAndrija/SRoBERTa-base\nAndrija/SRoBERTa-L\nAndrija/SRoBERTa-XL\nAndrija/SRoBERTa-F\nclassla/bcms-bertic\nclassla/xlm-r-bertic\nxlm-roberta-base\nxlm-roberta-large\njerteh/jerteh-81\njerteh/jerteh-355\nT3\n0.974\n0.980\n0.982\n0.982\n0.986 0.987\n0.984\n0.986\n0.985\n0.986\nT4\n0.908\n0.922\n0.929\n0.935\n0.942 0.942\n0.933\n0.935\n0.928\n0.928\nТабела 4. F1-мера коjу су модели остварили на задацима T3 (обележавање\nврсте речи) и T4 (препознавање именованих ентитета). У сваком реду наjбољи\nрезултат (±0.1%) обележен jе подебљањем.\n12\nИнфотека, год. 24, бр. 1, фебруар 2024.\nНаучни рад\nИз резултата приказаних у табели 4 види се да на задатку T3\n(обележавање врстом речи) девет од десет модела остваруjе jако добре\nрезултате (преко 98%), при чему се резултати коjе остваруjу четири\nнаjбоља модела (classla/bcms-bertic, classla/xlm-r-bertic, xlm-roberta-large\nи jerteh/jerteh-355) разликуjу за мање од 0.02%, указуjући да се модели\nполако приближаваjу горњим границама перформанси када jе у питању\nоваj задатак.\nКада су у питању резултати остварени на последњем задатку T4\n(препознавање именованих ентитета), наjбоље перформансе приказали\nсу модели classla/bcms-bertic и classla/xlm-r-bertic, при чему су разлике у\nF1-мерама нешто више него на задатку T3 (∼4% за задатак T4 у односу\nна ∼1% за задатак T3), али и даље знатно ниже него што су разлике на\nузводним задацима (чак ∼28% за задатак T1).\nУ наредном одељку биће разматрани резултати коjе су модели\nостварили као и разлози коjи су довели до тих резултат, са циљем да\nсе установе наjповољниjи услови за обучавање модела за српски jезик у\nбудућности.\n5.\nДискусиjа\nПретходно приказани резултати евалуациjе модела (табеле 2–4),\nпоказуjу да не постоjи jедан модел, или група модела, коjи су наjбољи\nу општем случаjу, већ су се различити модели показали као бољи\n(или лошиjи) на различитим задацима. У наставку, сваки од задатака\nће бити посматран поjединачно, пре свега у светлу односа остварених\nперформанси према величини модела, величини скупова за њихово\nобучавање и квалитету тих скупова.\n5.1\nМоделовање маскираног jезика\nРезултати евалуациjе модела на задатку моделовања маскираног\njезика (табела 2) показуjу убедљиву предност модела jerteh/jerteh-355, са\nтим да добре резултате остваруjе и jerteh/jerteh-81, коjи jе други наjбољи\nмодел за оваj задатак када се посматраjу просечне вредности. Како ова\nдва модела користе исти скуп података за обучавање, то указуjе да би\nуправо оваj скуп могао бити разлог добрих резултата.\nПросечна тачност модела на задатку T1 према њиховоj величини,\nодносно према величини скупа коjи jе коришћен за њихово обучавање\nприказан jе на слици 1. На први поглед, приметни су неки истакнути\nИнфотека, год. 24, бр. 1, фебруар 2024.\n13\nШкорић М., Нови jезички модели за српски jезик, стр. 1–22\nСлика 1. Тачност модела на задатку моделовања маскираног jезика према\nвеличини тих модела (лево), као и према величини скупова за обучавање\nмодела (десно). Приказана крива тренда одговара логаритамскоj функциjи.\nизузеци, пре свега модели засновани на XLM-R архитектури, коjи\nостваруjу неке од наjлошиjих резултата на овом задатку. Уколико се\nњихови резултати уклоне, поjављуjу се нови трендови (Слика 2). Дакле,\nкада посматрамо само RoBERTa моделе, чини се да већи модел (не баш\nубедљиво) и већи скуп за његово обучавање (врло убедљиво) повољно\nутичу на перформансе модела.\nСлика 2. Тачност модела на задатку моделовања маскираног jезика према\nвеличини тих модела (лево), као и према величини скупова за обучавање\nмодела (десно), при чему су уклоњени резултати модела заснованих на XLM-\nR архитектури. Приказана крива тренда одговара логаритамскоj функциjи.\n14\nИнфотека, год. 24, бр. 1, фебруар 2024.\nНаучни рад\n5.2\nИзрачунавање сличности између реченица\nПриликом евалуациjе задатка T2 установљено jе да наjбоље резултате\nостваруjу модели SRoBERTa-L, SRoBERTa-XL, SRoBERTa-F, jerteh-81 и\njerteh-355 када jе у питању препознавање сличних реченица на српском\njезику, а SRoBERTa-XL и SRoBERTa-F када jе у питању препознавање\nсличних реченица између српског и хрватског jезика (табела 2). Ово\nуказуjе да jе кључ за добро угњежђивање реченица претходно обучавање\nмодела за jезике коjи се испитуjу.\nДакле, за угњежђивање реченица на српском jезику наjбољи су\nмодели коjи су претходно обучени на довољно великом скупу реченица\nсрпског jезика, али ако се обрађуjу и реченице на хрватском, модели\nкоjи су обучавани и на српском и на хрватском имаjу предност. Са\nдруге стране, модели засновани на XLM-R архитектури коjи су унапред\nобучени на сто светских jезика поново показуjу наjлошиjе резултате,\nвероватно због великог шума коjи разнолики скуп за обучавање\nпроизводи.\nСлика 3. Тачност модела на задатку угњежђивања према величини тих\nмодела (лево), као и према величини скупова за обучавање модела (десно).\nПриказана крива тренда одговара логаритамскоj функциjи.\nНа слици 3 приказано jе какав jе утицаj величине модела и скупова\nза обучавање на перформансе на овом задатку, а линиjе тренда указуjу\nда се са повећањем и модела и скупа за обучавање перформансе смањуjу.\nУтицаj величине скупа може донекле бити приписан претходно описаном\nфеномену коjи погађа XLM-R архитектуру. Ипак, када jе у питању\nИнфотека, год. 24, бр. 1, фебруар 2024.\n15\nШкорић М., Нови jезички модели за српски jезик, стр. 1–22\nутицаj величине модела, постоjе додатни индикатори да су мањи модели\nбољи за оваj задатак када jе у питању обрада вишеjезичних текстова.\nТако jerteh/jerteh-81 надмашуjе jerteh/jerteh-355 на задатку утврђивања\nсличности између реченица на српском и хрватском jезику. Разлог може\nбити то што jе мањи модел, услед недостатка у величини, слабиjе\nприлагођен српском jезику (model underfit), али зато има предност\nприликом генерализациjе.\n5.3\nНизводни задаци\nЗа разлику од евалуациjе на узводним задацима, на низводним\nзадацима су резултати коjе постижу модели много сличниjи. На\nзадатку обележавања врстом речи (T3) готово сви модели остваруjу\nдобре резултате (табела 4), укључуjући и оне засноване на XLM-R\nархитектури. Штавише, classla/xlm-r-bertic и xlm-roberta-large су два од\nчетири модела коjи остваруjу наjбоље резултате (друга два модела су\nclassla/bcms-bertic и jerteh/jerteh-355).\nЗаjедничко за ова четири модела jе да су то или наjвећи модели или\nмодели коjи су обучавани на наjвећим скуповима података. Позитивна\nкорелациjа између перформанси и величине модела, као и између\nперформанси и величине скупова за обучавање уочава се и слици 4.\nСлика 4. Перформансе модела на задатку обележавања врстом речи према\nвеличини тих модела (лево), као и према величини скупова за обучавање\nмодела (десно). Приказана крива тренда одговара логаритамскоj функциjи.\n16\nИнфотека, год. 24, бр. 1, фебруар 2024.\nНаучни рад\nКорелациjа између величине скупа за обучавање jош jе очигледниjа у\nслучаjу препознавања именованих ентитета (слика 5). Наjбоље резултате\nна овом задатку (задатку T4) су остварила два модела са наjвећим\nскуповима за обучавање, док jе поново приметна и успешност XLM-R\nмодела, као и код претходног задатака. Величина модела такође показуjе\nтек незнатну позитивну корелациjу.\nСлика 5. Перформансе модела на задатку препознавања именованих ентитета\nпрема величини тих модела (лево), као и према величини скупова за обучавање\nмодела (десно). Приказана крива тренда одговара логаритамскоj функциjи.\n5.4\nЗакључак\nКада jе у питању моделирање маскираног jезика, чини се да развоj\nнових модела за српски jезик иде у правом правцу. Модел jerteh/jerteh-\n355\nостваруjе убедљиво наjбоље резултате, бар када jе у питању\nрад са високо-квалитетним текстовима, чак и када су они маскирани\nтокенизаторима других модела (табела 1). Премда повећање скупа\nподатака за обучавање повољно утиче на перформансе модела (слика 2),\nне треба занемарити ни квалитет скупа, jер модели jerteh/jerteh-\n355\nи\njerteh/jerteh-81\nнадмашуjу\nмоделе\nAndrija/SRoBERTa-F\nи\nAndrija/SRoBERTa-XL\nкоjи\nсу\nобучени\nна\nвећим\nскуповима\nза\nобучавање, указуjући да веб-корпуси можда нису увек довољни за\nобучавање добрих модела. Оваj закључак jе у складу са закључком\nиз jедног другог недавног истраживања (Li et al. 2023); ипак, ново\nИнфотека, год. 24, бр. 1, фебруар 2024.\n17\nШкорић М., Нови jезички модели за српски jезик, стр. 1–22\nистраживање би требало да у скуп за евалуациjу уврсти и нелитерарне\nизворе, како би се добила свеобухватниjа слика стања.\nНа\nзадатку\nизрачунавања\nсличности\nизмеђу\nреченица,\nто\njест\nугњежђивања\nреченица,\nистакли\nсу\nсе\nмодели\nкао\nшто\nсу\nAndrija/SRoBERTa-F и Andrija/SRoBERTa-XL, за коjима сасвим мало\nзаостаjу Andrija/SRoBERTa-L, jerteh/jerteh-81 и jerteh/jerteh-355, када\nсу у питању реченице на српском jезику (табела 3). Оно што издваjа ове\nмоделе jе да су они мањи у односу на друге моделе и да су обучавани\nна већем скупу, па изгледа да jе за оваj задатак кључна генерализациjа,\nдакле, већи скупови података (или можда мањи модели). Такође, када jе\nу питању обрада реченица ширег jезичког спектра (нпр. jужнословенски\njезици) било би потребно да се реченице из комплетног спектра укључе\nу скуп за обучавање или, jош боље, да се речник прилагоди за мапирање\nширег спектра токена, па самим тим и за правилну векторизациjу ових\nреченица. Ново истраживање на ову тему би требало да истражи и\nмодерниjи начин векторизациjе реченица, на пример, коришћењем\nархитектуре трансформера реченица sentence transformers (Reimers and\nGurevych 2019).\nУ случаjу оба узводна задатка, тачност коjу остваруjу модели\nзасновани на XLM-R архитектури jе знатно нижа у односу на тачност\nмодела заснованих на RoBERTa архитектури. У случаjу првог задатка\n(T1) то се може обjаснити њиховим знатно већим речником токена\n(па jе самим тим и избор одговараjућег токена тежи). Ипак, такво\nобjашњење не би било адекватно и за други задатак (T2). Са друге\nстране модели засновани на XLM-R архитектури су се показали као\nнаjбољи (уз малу маргину) на низводним задацима, пре свега на задатку\nпрепознавања именованих ентитета (T4). Чини се да jе за успешно\nрешавање овог задатка наjбоље да се модел током обучавања сусретне\nса наjразличитиjим броjем токена, али додатна побољшања доноси и\nдодатно обучавање на српским текстовима. Изгледа да би за унапређење\nперформанси тренутно било оптимално дообучавање XLM-RoBERTa-\nlarge модела коришћењем што већег и квалитетниjег скупа текстова на\nсрпском jезику. Када jе у питању обележавање врстом речи, изгледа да\nби било коjи нови модел био адекватан за решавање овог задатка.\n18\nИнфотека, год. 24, бр. 1, фебруар 2024.\nНаучни рад\nЗахвалница\nНаjважниjе скупове података за обучавање модела GPT2-orao,\nGPT2-vrabac, jerteh-81 и jerteh-355 обезбедило jе Друштво за jезичке\nресурсе и технологиjе.12\nРачунарске ресурсе за обучавање модела GPT2-orao\nи GPT2-\nvrabac обезбедила je Национална платформа за вештачку интелигенциjу\nСрбиjе.\nРачунарске ресурсе за обучавање модела jerteh-81\nи jerteh-355\nобезбедио jе Рударско-геолошки факултет Универзитета у Београду.\nИстраживање jе спроведено уз подршку Фонда за науку Републике\nСрбиjе, #7276, Text Embeddings – Serbian Language Applications\n–\nTESLA.\nХвала!\nЛитература\nBiemann, Chris, Gerhard Heyer, Uwe Quasthoff, and Matthias Richter. 2007.\n“The Leipzig corpora collection-monolingual corpora of standard size.”\nProceedings of corpus linguistic 2007.\nBogdanovi´c, Miloˇs, Jelena Koci´c, and Leonid Stoimenov. 2024. “SRBerta-A\nTransformer Language Model for Serbian Cyrillic Legal Texts.” Infor-\nmation 15 (2). issn: 2078-2489. https://doi.org/10.3390/info15020074.\nClark, Kevin, Minh-Thang Luong, Quoc V Le, and Christopher D Manning.\n2020. “Electra: Pre-training text encoders as discriminators rather than\ngenerators.” arXiv preprint arXiv:2003.10555.\nConneau, Alexis, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary,\nGuillaume Wenzek, Francisco Guzm´an, Edouard Grave, Myle Ott, Luke\nZettlemoyer, and Veselin Stoyanov. 2019. “Unsupervised cross-lingual\nrepresentation learning at scale.” arXiv preprint arXiv:1911.02116.\nCveji´c, Andrija. 2022. “Prepoznavanje imenovanih entiteta u srpskom jeziku\npomo´cu transformer arhitekture.” Zbornik radova Fakulteta tehniˇckih\nnauka u Novom Sadu 37 (02): 310–315.\n12. JeRTeh\nИнфотека, год. 24, бр. 1, фебруар 2024.\n19\nШкорић М., Нови jезички модели за српски jезик, стр. 1–22\nCvetanovi´c, Aleksa, and Predrag Tadi´c. 2023. “Synthetic Dataset Creation\nand Fine-Tuning of Transformer Models for Question Answering in Ser-\nbian.” In 2023 31st Telecommunications Forum (TELFOR), 1–4. IEEE.\nDevlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018.\n“Bert: Pre-training of deep bidirectional transformers for language un-\nderstanding.” arXiv preprint arXiv:1810.04805.\nDobreva, Jovana, Tashko Pavlov, Kostadin Mishev, Monika Simjanoska,\nStojancho Tudzarski, Dimitar Trajanov, and Ljupcho Kocarev. 2022.\n“MACEDONIZER-The Macedonian Transformer Language Model.” In\nInternational Conference on ICT Innovations, 51–62. Springer.\nHe, Pengcheng, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. 2020. “DE-\nBERTA: Decoding-Enhanced BERT with Disentangled Attention.” In\nInternational Conference on Learning Representations.\nJiang, Albert Q, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,\nDevendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna\nLengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. “Mistral 7B.”\narXiv preprint arXiv:2310.06825.\nKrstev, Cvetana, and Ranka Stankovi´c. 2023. “Language Report Serbian.”\nIn European Language Equality: A Strategic Agenda for Digital Lan-\nguage Equality, edited by Georg Rehm and Andy Way, 203–206. Cham:\nSpringer International Publishing. isbn: 978-3-031-28819-7. https://doi.\norg/10.1007/978-3-031-28819-7_32.\nLeCun, Yann, Yoshua Bengio, and Geoffrey Hinton. 2015. “Deep learning.”\nnature 521 (7553): 436–444. https://doi.org/10.1038/nature14539.\nLewis, Mike, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrah-\nman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. “BART: Denoising Sequence-to-Sequence Pre-training for Natural\nLanguage Generation, Translation, and Comprehension.” In Proceedings\nof the 58th Annual Meeting of the Association for Computational Lin-\nguistics, 7871–7880.\nLi, Yuanzhi, S´ebastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gu-\nnasekar, and Yin Tat Lee. 2023. “Textbooks are all you need ii: phi-1.5\ntechnical report.” arXiv preprint arXiv:2309.05463.\n20\nИнфотека, год. 24, бр. 1, фебруар 2024.\nНаучни рад\nLiu, Yinhan, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi\nChen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.\n2019. “Roberta: A robustly optimized BERT pretraining approach.”\narXiv preprint arXiv:1907.11692.\nLjubeˇsi´c, Nikola, and Filip Klubiˇcka. 2014. “{bs, hr, sr} wac-web corpora of\nBosnian, Croatian and Serbian.” In Proceedings of the 9th web as corpus\nworkshop (WaC-9), 29–35.\nLjubeˇsi´c, Nikola, and Davor Lauc. 2021. “BERTi´c–The Transformer Lan-\nguage Model for Bosnian, Croatian, Montenegrin and Serbian.” arXiv\npreprint arXiv:2104.09243.\nRadford, Alec, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al.\n2018. “Improving language understanding by generative pre-training.”\nRadford, Alec, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya\nSutskever, et al. 2019. “Language models are unsupervised multitask\nlearners.”\nRaffel, Colin, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,\nMichael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. “Exploring\nthe limits of transfer learning with a unified text-to-text transformer.”\nThe Journal of Machine Learning Research 21 (1): 5485–5551.\nRajpurkar, Pranav, Robin Jia, and Percy Liang. 2018. “Know what you\ndon’t know: Unanswerable questions for SQuAD.” arXiv preprint\narXiv:1806.03822.\nReimers, Nils, and Iryna Gurevych. 2019. “Sentence-BERT: Sentence Em-\nbeddings using Siamese BERT-Networks.” In Proceedings of the 2019\nConference on Empirical Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP), 3982–3992.\nˇSkori´c,\nMihailo.\n2023.\n“Композитне\nпсеудограматике\nзасноване\nна\nпаралелним\njезичким\nмоделима\nсрпског\njезика.”\nДокторска\nдисертациjа. PhD diss., Универзитет у Београду.\nˇSkori´c, Mihailo, Miloˇs Utvi´c, and Ranka Stankovi´c. 2023. “Transformer-\nBased Composite Language Models for Text Evaluation and Classifi-\ncation.” Mathematics 11 (22): 4660.\nИнфотека, год. 24, бр. 1, фебруар 2024.\n21\nШкорић М., Нови jезички модели за српски jезик, стр. 1–22\nStankovi´c, Ranka, Branislava ˇSandrih, Cvetana Krstev, Miloˇs Utvi´c, and\nMihailo ˇSkori´c. 2020. “Machine learning and deep neural network-based\nlemmatization and morphosyntactic tagging for serbian.” In Proceedings\nof the Twelfth Language Resources and Evaluation Conference, 3954–\n3962.\nSu´arez, Pedro Javier Ortiz, Benoˆıt Sagot, and Laurent Romary. 2019. “Asyn-\nchronous pipeline for processing huge corpora on medium to low re-\nsource infrastructures.” In 7th Workshop on the Challenges in the Man-\nagement of Large Corpora (CMLC-7). Leibniz-Institut f¨ur Deutsche\nSprache.\nTaori, Rohan, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen\nLi, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. 2023.\n“Alpaca: A strong, replicable instruction-following model.” Stanford\nCenter for Research on Foundation Models. https://crfm. stanford.\nedu/2023/03/13/alpaca. html 3 (6): 7.\nTodorovi´c, Branislava ˇSandrih, Cvetana Krstev, Ranka Stankovi´c, and Milica\nIkoni´c Neˇsi´c. 2021. “Serbian ner&beyond: The archaic and the modern\nintertwinned.” In Proceedings of the International Conference on Recent\nAdvances in Natural Language Processing (RANLP 2021), 1252–1260.\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez,  Lukasz Kaiser, and Illia Polosukhin. 2017. “Attention\nis all you need.” Advances in neural information processing systems 30.\nVitas, Duˇsko, Svetla Koeva, Cvetana Krstev, and Ivan Obradovi´c. 2008.\n“Tour du monde through the dictionaries.” In Actes du 27eme Colloque\nInternational sur le Lexique et la Gammaire, 249–256.\nWasserscheidt, Philipp. 2023. Serbian Web Corpus PDRS 1.0. Slovenian lan-\nguage resource repository CLARIN.SI. http://hdl.handle.net/11356/\n1752.\nWenzek, Guillaume, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaud-\nhary, Francisco Guzm´an, Armand Joulin, and Edouard Grave. 2020.\n“CCNet: Extracting High Quality Monolingual Datasets from Web\nCrawl Data” [in English]. In Proceedings of the 12th Language Re-\nsources and Evaluation Conference, 4003–4012. Marseille, France: Eu-\nropean Language Resources Association, May. isbn: 979-10-95546-34-4.\nhttps://www.aclweb.org/anthology/2020.lrec-1.494.\n22\nИнфотека, год. 24, бр. 1, фебруар 2024.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2024-02-22",
  "updated": "2024-02-23"
}