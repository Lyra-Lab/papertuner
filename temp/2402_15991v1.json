{
  "id": "http://arxiv.org/abs/2402.15991v1",
  "title": "$C^3$: Confidence Calibration Model Cascade for Inference-Efficient Cross-Lingual Natural Language Understanding",
  "authors": [
    "Taixi Lu",
    "Haoyu Wang",
    "Huajie Shao",
    "Jing Gao",
    "Huaxiu Yao"
  ],
  "abstract": "Cross-lingual natural language understanding (NLU) is a critical task in\nnatural language processing (NLP). Recent advancements have seen multilingual\npre-trained language models (mPLMs) significantly enhance the performance of\nthese tasks. However, mPLMs necessitate substantial resources and incur high\ncomputational costs during inference, posing challenges for deployment in\nreal-world and real-time systems. Existing model cascade methods seek to\nenhance inference efficiency by greedily selecting the lightest model capable\nof processing the current input from a variety of models, based on model\nconfidence scores. Nonetheless, deep models tend to exhibit overconfidence, and\nconfidence distributions vary across languages. This leads to the emission of\nconfident but incorrect predictions by smaller models, hindering their ability\nto generalize effectively across test languages. In this study, we introduce a\nconfidence calibration model cascade ($C^3$) method. This approach, simple yet\neffective, involves calibration prior to cascade inference, thereby enhancing\ncascade accuracy through more reliable predictions. Extensive experiments\nconducted on three cross-lingual benchmarks demonstrate that $C^3$\nsignificantly outperforms all state-of-the-art baselines.",
  "text": "C3: Confidence Calibration Model Cascade for Inference-Efficient\nCross-Lingual Natural Language Understanding\nTaixi Lu1∗, Haoyu Wang2∗, Huajie Shao3, Jing Gao2, Huaxiu Yao1\n1UNC-Chapel Hill, 2Purdue University, 3College of William and Mary\ntaixi@email.unc.edu, wang5346@purdue.edu, huaxiu@cs.unc.edu\nAbstract\nCross-lingual natural language understanding\n(NLU) is a critical task in natural language\nprocessing (NLP). Recent advancements have\nseen multilingual pre-trained language mod-\nels (mPLMs) significantly enhance the perfor-\nmance of these tasks. However, mPLMs ne-\ncessitate substantial resources and incur high\ncomputational costs during inference, posing\nchallenges for deployment in real-world and\nreal-time systems.\nExisting model cascade\nmethods seek to enhance inference efficiency\nby greedily selecting the lightest model capable\nof processing the current input from a variety\nof models, based on model confidence scores.\nNonetheless, deep models tend to exhibit over-\nconfidence, and confidence distributions vary\nacross languages. This leads to the emission of\nconfident but incorrect predictions by smaller\nmodels, hindering their ability to generalize ef-\nfectively across test languages. In this study,\nwe introduce Confidence Calibration Cascade\n(C3), a simple yet effective method that in-\nvolves calibration prior to cascade inference,\nthereby enhancing cascade accuracy through\nmore reliable predictions. Our evaluation of C3,\nusing both encoder-only and decoder-only lan-\nguage models, across five cross-lingual bench-\nmarks covering classification and generation\ntasks, shows that C3 markedly surpasses all\nleading baselines.\n1\nIntroduction\nPre-trained language models (PLMs), such as\nBERT (Devlin et al., 2019), RoBERTa (Liu et al.,\n2020b), T5 (Raffel et al., 2020), and Llama (Tou-\nvron et al., 2023), have exhibited remarkable perfor-\nmance across various natural language processing\n(NLP) tasks. Notably, their multilingual versions\nhave demonstrated impressive zero-shot transfer\ncapabilities in cross-lingual settings (Pires et al.,\n2019; Conneau et al., 2019). In these scenarios,\n∗equal contribution\nPLMs are fine-tuned on English data, often with\nlimited or even without data from other languages,\nyet they acquire the proficiency to handle tasks in\ndifferent languages. However, multilingual PLMs\nare typically constructed using stacked transformer\nlayers or their variants, employing self-attention\nmechanisms to capture diverse and distant depen-\ndencies among tokens. The use of self-attention\nintroduces significant computational complexity.\nConsequently, the inference complexity of multi-\nlingual PLMs has become a bottleneck, limiting\ntheir deployment on devices sensitive to latency\nand constrained by computational resources.\nTo fulfill the stringent requirements for efficient\ninference in applications, various methods have\nbeen proposed to accelerate Pre-trained Language\nModel (PLM) inference. These methods include\nmodel compression (Sanh et al., 2019; Jiao et al.,\n2020; Sun et al., 2020, 2019), early exiting (Xin\net al., 2020; Zhou et al., 2020; Liao et al., 2021),\nand model cascading (Li et al., 2020; Wang et al.,\n2022). Among these, model cascading methods are\nparticularly appealing for several reasons: 1) They\ndo not depend on specific hardware support, such\nas custom chips and GPUs. 2) They eliminate the\nneed to train an inference-efficient model from\nscratch on the pre-training corpora. 3) They of-\nfer flexibility to adapt to the latest, incrementally\npowerful PLMs. Model cascading methods involve\nthe aggregation of multiple PLMs with different\nsizes. Confidence scores are computed sequen-\ntially, ranging from small to large size models, to\ndetermine the appropriate model to employ. Once\na confidence score surpasses a threshold, the cor-\nresponding model is selected, and the inference\nprocess ends.\nCascade-based models, however, exhibit notable\nlimitations in cross-lingual scenarios. The confi-\ndence score, which measures the probability of the\ncurrent prediction being correct in cascade-based\nmodels, is determined by the maximum output\narXiv:2402.15991v1  [cs.CL]  25 Feb 2024\nPremise: One of our number will carry out your instructions minutely.\nHypothesis: A member of my team will execute your orders with immense precision.\n0: entailment         1: neutral       2: contradiction\n0.7\n0.2\n0.1\n0\n1\n2\nLogit \nNormalization\nTraining\n0.5\n0.2\n0.3\n0\n1\n2\nTemperature \n     Scaling\nInference\n0.3 0.4 0.3\n0\n1\n2\n0.4 > λ\nLogit \nNormalization\nTraining\n0.5\n0.15\n0.35\n0\n1\n2\nTemperature \n     Scaling\nInference\n0.55\n0.3 0.15\n0\n1\n2\n0.55 > λ\n…\nLogit \nNormalization\nTraining\n0.8\n0.1\n0.1\n0\n1\n2\nTemperature \n     Scaling\nInference\n0.75\n0.15 0.1\n0\n1\n2\nEncoder\n Model \nClassifier\nLanguage Model 1\nEncoder\n Model \nClassifier\nLanguage Model 2\nEncoder\n Model \nClassifier\nLanguage Model N\n0.9\n0.040.06\n0\n1\n2\n0.6\n0.3 0.1\n0\n1\n2\nFigure 1: An illustration of our C3 framework (for clas-\nsification task) for speeding up natural language infer-\nence yet retain the most accuracy, especially in OOD\ndata. We leverage Logit Normalization at training time\nand Temperature Scaling at inference time to calibrate\neach model so that the model will yield more reliable\nconfidence score for cascade decisions. For Large Lan-\nguage Model (e.g., GPT-4, Llama) inference where there\nis no training involved, we simply remove the training\nmodule. The λ represents the confidence score.\nprobability, the mean of the output probability, or\nthe entropy of the output probability. Unfortunately,\nneural networks often generate unreliable confi-\ndence scores, particularly in out-of-distribution\n(OOD) scenarios (Guo et al., 2017; Wei et al., 2022;\nHan et al., 2024; Choi et al., 2023; Liang et al.,\n2022). The cross-lingual task represents a typical\nOOD setting, where the training data exhibits sig-\nnificantly different distributions compared to the\ntesting data. Consequently, the distribution of con-\nfidence scores on the training data differs from that\non the testing data. Applying a threshold derived\nfrom the training set, based on pre-defined infer-\nence budgets, may prove either too conservative\nor radical for the testing data, leading to a perfor-\nmance or efficiency drop.\nTo tackle the challenges outlined earlier, we in-\ntroduce a Confidence Calibration Cascade (C3)\nframework for efficient cross-lingual inference.\nThe motivation behind our proposed approach is to\ncalibrate the confidence of Multilingual Pre-trained\nLanguage Models (mPLMs), allowing the thresh-\nold determined on English data to be applicable to\nother languages. Specifically, we introduce a plug-\nin calibration step at the base of mPLMs. Initially,\nwe normalize the logits to alleviate over-confidence\nduring model fine-tuning. Subsequently, we im-\nplement a temperature scaling step to adjust the\nlogits with a learnable scalar parameter. The pro-\nposed framework calibrates each individual model\nin the cascade, providing more reliable confidence\nscores. This, in turn, enhances the model’s per-\nformance and generalization capabilities, leading\nto consistent improvements in efficiency and accu-\nracy across different languages. Importantly, the\nproposed framework only requires an extra calibra-\ntion module at the base of mPLMs, preserving the\noriginal architectures of mPLMs. Hence, it demon-\nstrates flexibility to accommodate the latest models\nwith minimal additional training overhead.\nThe primary contributions of this paper is C3,\na flexible and effective framework for enhancing\nefficiency in cross-lingual inference. To the best\nof our knowledge, this is the first work dedicated\nto the design of inference-efficient models specif-\nically tailored for cross-lingual scenarios. Based\non the observation of a notable overconfidence phe-\nnomenon in both encoder-only PLMs and decoder-\nonly PLMs in cross-lingual scenarios, and consid-\nering that the extent of overconfidence appears\nto be correlated with linguistic distance, we in-\ntroduce a plug-in calibration module to address\nthis issue. Extensive experiments on five cross-\nlingual benchmarks, including XNLI, PAWS-X,\nQAM, GSM8k, and TabMWP, where the first three\nare text classification datasets and the later two are\ngeneration datasets, indicate that the proposed C3\noutperforms baselines significantly and achieves a\ngood efficiency-accuracy trade-off, e.g., preserving\n98.10% of BERT’s performance and 95.28% of\nLlama-2’s performance on classification task and\nan average of 74.3% performance on generation\ntask only half the computation costs.\n2\nRelated Work\n2.1\nInference Acceleration for PLMs\nExisting approaches for accelerating the inference\nof pre-trained language models (PLMs) can be cat-\negorized into two types: 1) model compression-\nbased methods and 2) dynamic network-based\nmethods. The proposed method is more relevant\nto dynamic network-based methods, which are dis-\ncussed in more detail as follows. The discussion\nabout the model compression-based methods can\nbe found in the Appendix A.\nDynamic network-based methods leverage mod-\nels with different sizes based on input data when in-\nference. Early existing methods and cascade meth-\nods are two typical dynamic networks.\nEarly exiting Methods. There are a number of\nearly exiting methods that accelerate model infer-\nence by emitting predictions from an inner layer.\nFastBERT (Liu et al., 2020a), DeeBERT (Xin et al.,\n2020), and SDN (Kaya et al., 2019), are score-\nbased methods, using the entropy of the prediction\nprobability and the maximum of the predicted dis-\ntribution as the score for exit decision. BERxiT\n(Xin et al., 2021) is another type of early-exiting\nmethod that learns to exit. Also, patience-based\nmethods, such as PABEE (Zhou et al., 2020), SEN-\nTEE (Li et al., 2021), and LeeBERT (Zhu, 2021),\nensemble scores from multiple layers to make de-\ncision. However, the problem with early existing\nmethods is that the model loses high-level semantic\nunderstanding without going into the higher layers.\nCascade Methods. The main difference between\ncascade and early existing methods is cascade meth-\nods choose models with different sizes to make\npredictions while early existing methods choose\na layer to exit with respect to a specific model.\nSpecifically, cascades combine different sizes of\nmodels and go through each model sequentially\nfrom the smallest one to the largest one. Once the\nprediction from one model is confident enough, the\nprediction is emitted. Many previous works incor-\nporate model cascade for faster inference on certain\ntasks. For example, Viola and Jones (2001) built\na cascade of classifiers with increasing complex-\nity to speed up facial recognition; CascadeBERT\nensembles two models prediction and regularizes\nthem with a difficulty-awareness regularization (Li\net al., 2020); Window-Based Cascade caches out-\nput from multiple levels and compare them to a\nwindow threshold (Xia and Bouganis, 2023).\n2.2\nModel Calibration\nAs models become larger, they also tend to be less\ncalibrated, meaning the confidence output by the\nmodel does not reflect the true probability. In fact,\nlarge models are found to be easily overconfident\nin a lot of scenarios. We found that the problem of\nmiscalibration is essentially important in cascades,\nwhere confidence is directly used as a metric to\ndetermine whether the prediction from the smaller\nmodels are reliable. There are various calibration\nmethods in previous works (Tian et al., 2023). For\nexample, Temperature Scaling divides the logits\nby a learned parameter from the validation set to\ncalibrate the model after training (Guo et al., 2017);\nLogit Normalization incorporates a modified loss\nfunction to learn to produce logits with smaller\nnorms (Wei et al., 2022); AugMax uses data aug-\nmentation techniques to calibrate models (Wang\net al., 2021).\n3\nPreliminaries: Model Cascade\nIn this section, we introduce the overall flow of the\nvanilla model cascade. The model cascade consists\nof n PLMs, denoted as {Mi}n\ni=1, with the sizes\nof these PLMs arranged in ascending order, i.e.\n|Mi| < |Mj| for i < j where | · | represents the\nsize of the model. The cascade model leverages\neach PLM for inference sequentially and computes\nthe confidence score based on prediction logits of\ncurrent PLM. If the confidence score exceeds a pre-\ndefined threshold, the cascade model terminates its\ninference. Specifically, for an input text sequence t\nand the current PLM Mi, the output logits can be\nrepresented as\nL = Mi(t),\n(1)\nwhere L = {lj}q\nj=1 and q is the number of classes\nin the label. Subsequently, it computes the confi-\ndence score based on the logits L, such as by using\naveraged probability or maximum probability. We\nillustrate the calculation using maximum probabil-\nity as an example:\nC = arg max\nj\nexp lj\nP\nk exp(lk).\n(2)\nIf it has traveled all models or C > λ, where λ\nis a threshold, the inference ends and returns the\ncurrent logits L.\n4\nConfidence Calibration Cascade\nGiven n pre-trained language models (PLMs) with\nascending sizes {Mi}n\ni=1 and source language (En-\nglish in our paper) training set D = {(xi, yi)}N\ni=1,\nwhere xi is the input sequence, yi is the correspond-\ning label and N is the number of training data, the\naim is to fine-tune these PLMs and select the most\nsuitable PLM that enhances the inference efficiency\nwhile preserving accuracy on both source and tar-\nget languages with respect to the largest PLM Mn.\n4.1\nOverview\nTo enhance model inference efficiency in cross-\nlingual scenarios,\nwe propose a confidence\ncalibration model cascade method (C3), as illus-\ntrated in Fig. 1. The motivation behind this ap-\nproach is to select the most lightweight model for\neach input based on the model confidence scores.\nTo ensure the reliability of the confidence score,\nwe introduce an additional calibration module in-\ntegrated into vanilla cascade methods, which in-\ncludes logit normalization and temperature scaling.\nWe present the proposed C3 with encoder-only lan-\nguage models in Section 4.2 and decoder-only lan-\nguage models in Section 4.3.\n4.2\nConfidence Calibration for Language\nModel Cascade\nIn the vanilla model cascade method, a sample se-\nquentially progresses through models of increasing\nsize. A prediction is made when a model’s confi-\ndence score exceeds a predetermined threshold, λ,\nor when the final model in the sequence is reached.\nOur proposed C3 approach to cascading lan-\nguage models introduces two additional steps to\nthe standard model cascade procedure. These steps\naim to align the confidence distributions across var-\nious languages and models of different sizes. The\nfirst step involves integrating a Logit Normaliza-\ntion layer during the fine-tuning of each language\nmodel. Consider n language models {M1, .., Mn}.\nEach model is fine-tuned using a task-specific loss\nfunction ℓ(x, y), where the input logits are normal-\nized. This can be expressed as:\nℓ(x, y) = −log\nexp( ly\nτ∥l∥)\nPq\ni=1 exp(\nli\nτ∥l∥)\n,\n(3)\nwhere q is the number of classes, l = [l1, ..., lq]\nis the logits, ∥l∥is the norm of the logits, and τ\nis a hyper-parameter to modulate the model’s out-\nput logits. The rationale for Logit Normalization\nis to counteract the tendency of the cross-entropy\nloss function, which often encourages the model\nto produce increasingly large logits during train-\ning, leading to overconfident Softmax scores. By\nmaintaining a constant ℓ2 norm of logits, Logit\nNormalization seeks to address this issue.\nThe second step involves applying temperature\nscaling to each fine-tuned model. Temperature scal-\ning adjusts the model output logits by scaling them\nwith a temperature parameter learned from the val-\nidation data. More precisely, we learn the tempera-\nture parameter Ti for each model Mi, and then we\nscale the model logits l by the temperature when\nPrompt: \nDo (Sentence1, The NBA season \nof 1975 – 76 was the 30th season \nof the National Basketball \nAssociation.) and (Sentnece2, The \n1975 – 76 season of the National \nBasketball Association was the \n30th season of the NBA.) have \ndifferent meaning?\nAnswer: No\nDo (Sentence1: It is the seat of \nZerendi District in Akmola \nRegion.) and (Sentence2: It is the \nseat of the district of Zerendi in \nAkmola region.) have different \nmeaning?\nAnswer:\nAnswer:\nNo\nUser\nLlama2\nFigure 2: Example C3 one-shot prompt on Llama-2\nfor PAWS-X task. The answer candidates set would\nbe {Yes, No}, and the embedding set would be {y, n},\nwhere y is the Llama embedding of the word \"Yes,\" and\nn is the Llama embedding for the word \"No.\"\ncalculating the confidence score C.\nCi = arg max\nj\nexp(lj/Ti)\nP\nk exp(lk/Ti)\n(4)\nThese enhancements aim to calibrate model pre-\ndictions and enhance the reliability of confidence\nscores, a critical factor in model selection within a\ncascade framework.\n4.3\nConfidence Calibration for Large\nLanguage Model Cascade\nIn this section, we illustrate the application of\nour proposed method C3 to large language mod-\nels (LLMs). As with C3 for encoder-only LMs,\nwe compile a collection of n LLMs of varying\nsizes. However, in contrast to the full fine-tuning\napproach described in Section 4.2, the adaptation\nof LLMs to specific downstream tasks is more ef-\nfectively achieved through zero-shot or few-shot\nin-context learning. This is due to the substan-\ntial number of parameters in LLMs and their in-\nherent capacity for generalization. Following this\napproach, we incorporate the task description and\nexamples in English into a prompt, as detailed in\nFigure 2. The LLM Mi then generates logits for\nthe next token from the vocabulary, which can be\nrepresented as:\nl = LogitsMi(Prompt).\n(5)\nFor cross-lingual NLU tasks, particularly classifica-\ntion scenarios, we extract the logit corresponding\nto each class from l using the token IDs for each\nclass. This yields the logits for each class, denoted\nas {lcj}q\nj=1, where cj is the j-th class. We then\napply temperature scaling to calibrate each model’s\noutput using a temperature Ti same as Section 4.2\nto achieve model confidence. Finally, we determine\na threshold λ for model selection during inference.\nThe complete framework is outlined in Alg. 1 in\nthe appendix.\nFurthermore, more generally, for generation\ntasks, unlike classification tasks where a set of\nlogits per class can be obtained for temperature\nscaling, we employ token-level relevance (Duan\net al., 2023) for calibration. Specifically, we com-\npute the entropy for each token in the generated\nsequence, i.e.\nE(Zi) = −log(p(Zi)),\n(6)\nwhere Zi is the ith token in the generated sequence.\nFor each token Zi, we remove it from the sequence\nand utilize a pre-trained sentence similarity model\nto calculate the similarity, denoted as Ri. The\nentropy of the entire sequence is determined by\nE(Z) = 1\nn\nn\nX\ni=0\nE(Zi)(1 −Ri).\n(7)\nFinally, we establish a threshold, λ, for Zi to man-\nage the cascading of models effectively.\n5\nExperiment\nIn this section, we evaluate the performance of C3\non three cross-lingual benchmarks, aiming to an-\nswer the following questions: (1) How does C3\nperform compared to state-of-the-art baselines? (2)\nIs the proposed calibration method effective to re-\nduce the calibration error? (3) How does the per-\nformance change with varing hyper-parameter τ in\nLogit Normalization?\n5.1\nExperimental Setup\nDatasets\nTo evaluate the performance of our pro-\nposed C3 on classification tasks, we conduct com-\nprehensive experiments on three widely-used cross-\nlingual benchmarks: XNLI (Conneau et al., 2018;\nWang et al., 2023), PAWS-X (Yang et al., 2019),\nand QAM (Liang et al., 2020). These datasets\ncover natural language inference, paraphrase iden-\ntification, and question-answering matching tasks,\nrespectively. Model fine-tuning is exclusively per-\nformed using the English training set, with subse-\nquent evaluation conducted directly on testing data\nin other languages. Additionally, to evaluate the\nperformance of our proposed C3 on generation\ntasks, we also use GSM8k (Cobbe et al., 2021)\nand TabMWP (Lu et al., 2023) datasets. These\ntwo datasets both evaluate a model’s mathematical\nreasoning capabilities, featuring questions as math-\nematical problems and answers in free-form text.\nDue to limited computational resources for running\nLarge Language Models, we randomly selected 500\nsamples from the test set of each dataset for evalu-\nation. Subsequently, we utilized the Google Trans-\nlate API to translate these examples into five other\nlanguages: Spanish, German, Chinese, Japanese,\nand Korean. Detailed dataset statistics are pre-\nsented in Table 6 in the appendix.\nBaselines\nWe adopt the following state-of-the-art\nmethods as baselines: (1) PABEE. The early exit-\ning method PABEE emits predictions from an inner\nlayer of a model if the prediction remains consis-\ntent for a certain number of consecutive instances\n(Zhou et al., 2020); (2) DeeBERT, which is an-\nother early exiting method that utilizes off-ramps\nlayers to determine whether the prediction is confi-\ndent enough at an inner layer (Xin et al., 2020); (3)\nCasacdeBERT, which is a cascade-based method\nthat performs inference based on complete models\nwith early stopping criteria and a difficulty-aware\nregularization (Li et al., 2020); (4) Cascade, which\nis also a cascade-based method that goes through\neach model sequentially until the stopping restric-\ntion is met (Wang et al., 2022).\n5.2\nPerformance Comparison\nIn this section, we vary the threshold for each\nmethod and compare their accuracy on the test set,\nmaintaining an identical speed-up ratio to answer\nRQ1. The speed-up ratio (S) is computed by di-\nviding the number of Floating Point Operations\n(FLOPs) that the largest model (XLM-RoBERTa-\nlarge and Llama2-70b-chat-hf) would require by\nthe number of FLOPs of the current method. Con-\nsistent with previous work, we also consider three\ndistinct speed-up ratios, i.e. 2, 3, and 4, for a com-\nprehensive comparison.\n5.2.1\nEncoder-only Language Model\nWe fine-tune encoder-only language models,\nincluding\nDistilBERT,\nmBERT-base,\nXLM-\nRoBERTa-base, and XLM-RoBERTa large, on\nthe XNLI, PAWS-X, and QAM datasets, as\nillustrated in tables from Table 1 to Table 3.\nAccording to these tables, the proposed method\n(C3) outperforms existing methods. Notably, C3\nexhibits superior performance compared to other\nTable 1: Result comparison for cascading BERT, multilingual BERT base, XLM-RoBERTa base, and XLM-\nRoBERTa large on XNLI. The blue rows represents a speed-up ration of 2. The red rows represents a speed-up\nratio of 3. And the yellow rows represents a speed-up ration of 4.\nMethod\nar\nbg\nde\nel\nen\nes\nfr\nhi\nru\nsw\nth\ntr\nur\nvi\nzh\nAvg.\nPABEE\n60.03\n65.23\n70.65\n82.97\n82.97\n76.44\n75.47\n69.06\n74.75\n57.84\n68.12\n71.11\n62.49\n73.87\n70.62\n69.92\nDeeBERT\n38.32\n61.47\n46.13\n59.70\n58.90\n55.73\n56.27\n51.08\n48.42\n51.52\n45.71\n57.21\n55.05\n49.48\n48.42\n52.23\nCascadeBERT\n71.36\n75.12\n77.06\n72.62\n86.72\n80.36\n79.18\n63.65\n74.10\n56.89\n56.76\n68.57\n62.48\n70.35\n73.59\n71.25\nCascades\n75.51\n79.64\n79.64\n78.24\n86.72\n82.11\n81.31\n72.01\n77.98\n63.31\n70.71\n74.25\n68.04\n77.30\n75.90\n76.18\nC3\n75.75\n81.00\n80.74\n79.94\n87.31\n81.76\n81.50\n73.81\n78.30\n66.99\n74.09\n75.47\n68.86\n77.68\n75.99\n77.28\nPABEE\n52.89\n52.89\n62.07\n61.25\n76.03\n70.09\n68.8\n57.86\n62.99\n46.70\n60.56\n62.98\n53.21\n65.09\n63.89\n61.15\nDeeBERT\n39.90\n40.48\n40.58\n40.02\n49.50\n48.56\n49.20\n44.91\n42.47\n39.50\n41.86\n41.12\n39.48\n41.60\n45.19\n42.96\nCascadeBERT\n68.61\n71.26\n70.52\n68.51\n84.66\n76.99\n75.69\n57.58\n69.48\n51.66\n46.97\n63.07\n58.44\n64.69\n70.36\n66.56\nCascades\n71.32\n76.39\n77.22\n74.39\n86.48\n80.19\n79.02\n66.78\n74.91\n59.24\n59.50\n69.70\n64.43\n74.49\n73.59\n72.51\nC3\n71.44\n78.44\n78.60\n75.87\n86.07\n79.68\n79.46\n68.38\n76.67\n62.16\n67.76\n72.04\n63.35\n75.99\n73.53\n73.96\nPABEE\n48.62\n48.62\n54.95\n53.58\n73.45\n63.39\n63.43\n51.85\n58.08\n44.27\n54.59\n57.28\n48.74\n60.92\n59.08\n56.06\nDeeBERT\n38.92\n37.52\n39.76\n37.02\n43.39\n40.36\n41.04\n37.88\n40.12\n37.30\n38.48\n36.23\n35.91\n41.60\n39.58\n39\nCascadeBERT\n63.49\n67.74\n69.40\n63.35\n83.64\n76.10\n74.86\n54.93\n68.89\n48.50\n42.43\n59.2\n55.63\n61.38\n68.44\n63.86\nCascades\n68.48\n73.11\n74.39\n70.58\n85.14\n78.82\n77.26\n62.23\n72.14\n54.45\n54.59\n65.01\n60.85\n70.23\n71.75\n69.26\nC3\n69.88\n75.07\n75.51\n72.42\n85.45\n78.60\n78.28\n64.17\n74.49\n56.49\n58.22\n66.99\n60.82\n72.30\n70.98\n70.64\nTable 2: Result comparison for cascading BERT, multilingual BERT base, XLM-RoBERTa base, and XLM-\nRoBERTa large on PAWS-X. The blue rows represents a speed-up ration of 2. The red rows represents a speed-up\nratio of 3. And the yellow rows represents a speed-up ration of 4.\nMethod\nen\nfr\nes\nde\nzh\nja\nko\nAvg.\nPABEE\n90.35\n87.75\n85.75\n86.2\n79.4\n74.45\n73.90\n82.54\nDeeBERT\n75.9\n71.7\n70.6\n64.7\n64.8\n59.2\n62.4\n67.04\nCascadeBERT\n92.25\n88.3\n87.7\n87.8\n79.75\n75.6\n75.88\n83.89\nCascades\n95.6\n91.45\n90.85\n90.2\n84.4\n80.4\n79.85\n87.53\nC3\n95.6\n90.85\n91.5\n90.45\n84.5\n81.7\n81.6\n88.03\nPABEE\n63.55\n63.3\n57.6\n58.25\n58.35\n62.25\n59.2\n60.36\nDeeBERT\n67.45\n64.8\n63.9\n60.75\n60.4\n56.8\n57.4\n61.64\nCascadeBERT\n94.2\n87.35\n87.85\n85.7\n77.85\n75.15\n72.6\n82.96\nCascades\n95.2\n90.8\n90.2\n90.05\n81.65\n77.7\n76.75\n86.05\nC3\n95.6\n89.95\n90.85\n90.25\n82.45\n78.55\n78.75\n86.63\nPABEE\n60.3\n63.2\n57.25\n57.4\n57.75\n56.8\n58.1\n58.69\nDeeBERT\n60\n61.25\n60.15\n61\n58.15\n56.1\n56.65\n59.04\nCascadeBERT\n94.25\n85.55\n85.7\n84.05\n75.9\n73.1\n70.5\n81.29\nCascades\n95.2\n89.75\n89.3\n88.3\n80.3\n75.7\n74.75\n84.76\nC3\n95.5\n88.8\n89.45\n88.45\n80.55\n77.35\n76.55\n85.24\ncascade-based methods, especially in languages\ndistant from English. For instance, on Thai, C3\nachieves an accuracy of 67.76%, surpassing the\nbaseline Cascade by 8.26% at a speed-up ratio of\n3.\n5.2.2\nDecoder-only Language Model\nWe also conduct experiments using Llama2 as the\nbackbone on PAWS-X to investigate the general-\nization capability of the proposed method (C3) for\nlarge-scale decoder-only language models on clas-\nsification task. The results are presented in Table\n4. Notably, the framework exhibits only a marginal\n3% decrease in accuracy when subjected to a two-\nfold speed-up, another approximately 3% decrease\nfor a three-fold speed-up, and less than an addi-\ntional 2% decrease for a four-fold speed-up. It’s\nnoteworthy that our findings lack accuracy data for\nthe baseline Cascade. This absence is due to the\nremarkably concentrated confidence scores of Cas-\ncade, predominantly clustering around the value\n1, resulting from the overconfident predictions by\nLlama-2. Consequently, establishing a threshold to\nachieve a specific speedup ratio becomes unfeasi-\nble, marking a limitation of Cascade.\nWe further present the results of our experiments\nusing Llama2 as the backbone architecture on the\nGSM8k and TabMWP datasets in Table 4, demon-\nstrating the generalizability of our method to gen-\neration tasks. In the majority of languages within\nthese datasets, our method surpasses the Cascade\nmethod by margins ranging from 2% to 6.2%. In\nother instances, the two methods exhibit compara-\nble performance.\n5.3\nAnalysis\nIn this section, we study the effectiveness of the\nproposed calibration module to answer RQ2. We\ncompute the calibration error of baseline Cascade\nand the proposed C3 on XNLI with a 2x speedup\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy in bin\nCalibration Error:\n0.40\nDistlBERT (Uncalibrated)\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCalibration Error:\n0.09\nDistlBERT (Calibrated)\n0.2\n0.4\n0.6\n0.8\n1.0\nConfidence\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy in bin\nCalibration Error:\n0.26\nCascade\n0.2\n0.4\n0.6\n0.8\n1.0\nConfidence\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCalibration Error:\n0.16\nC³\n(a) Thai.\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy in bin\nCalibration Error:\n0.13\nDistlBERT (Uncalibrated)\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCalibration Error:\n0.12\nDistlBERT (Calibrated)\n0.2\n0.4\n0.6\n0.8\n1.0\nConfidence\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy in bin\nCalibration Error:\n0.15\nCascade\n0.2\n0.4\n0.6\n0.8\n1.0\nConfidence\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCalibration Error:\n0.11\nC³\n(b) Spanish.\nFigure 3: ECE comparison between Cascade (in orange) and C3 (in blue). In each subfigure, the top two figures\nare the ECE of the smallest model in these two methods, and the bottom two figures are the overall ECE.\nTable 3: Result comparison for cascading BERT,\nmultilingual BERT base, XLM-RoBERTa base, and\nXLM-RoBERTa large on QAM. The blue rows repre-\nsents a speed-up ration of 2. The red rows represents a\nspeed-up ratio of 3. And the yellow rows represents a\nspeed-up ration of 4.\nMethod\nen\nde\nfr\nAvg.\nPABEE\n64.3\n64.12\n65.75\n64.72\nDeeBERT\n63.83\n56.82\n63.58\n61.41\nCascadeBERT\n65.56\n64.7\n64.22\n64.83\nCascades\n71.41\n68.4\n68.83\n69.55\nC3\n72.45\n70.7\n70.19\n71.11\nPABEE\n60.6\n59.54\n61.64\n60.59\nDeeBERT\n57.8\n53.32\n57.36\n56.16\nCascadeBERT\n66.28\n62.98\n64.18\n64.48\nCascades\n69.9\n66.59\n67.35\n67.95\nC3\n70.95\n67.54\n68.55\n69.01\nPABEE\n59.41\n57.94\n58.5\n58.62\nDeeBERT\n56.38\n51.99\n55.67\n54.68\nCascadeBERT\n66.13\n61.86\n62.94\n63.64\nCascades\n68.52\n64.64\n65.73\n66.3\nC3\n69.36\n64.45\n66.3\n66.7\nratio. We show the results on four languages, in-\ncluding Thai in Fig. 3a, Spanish in Fig. 3b. See\nAppendix D for ECE of English and Swahili. Based\non figures, we have following findings.\nFirst, we observe that the proposed calibration\nmethod effectively reduces the calibration error for\neach individual model. In Fig. 3a, the expected\ncalibration error (ECE) of the uncalibrated Dis-\ntilBERT model on Thai data is notably high at\napproximately 0.4. Furthermore, we note a pat-\ntern of overconfidence in the uncalibrated model,\nwith around 50% of data having a confidence level\nexceeding 0.8, yet their actual accuracy is only\naround 40%. This discrepancy is represented in\nthe color depth of the histogram. In contrast, our\nproposed calibration method mitigates this over-\nconfidence and miscalibration, as evidenced by the\nsignificantly reduced ECE of 0.09 for the calibrated\nDistilBERT, marking a substantial 78% decrease.\nCalibration contributes to more reliable model con-\nfidence. A similar trend is observed in the case of\nSpanish data, as shown in Fig. 3b, where the ECE\nfor the uncalibrated DistilBERT is 12.62, and after\ncalibration, it decreases to 11.88.\nSecond, the entire model benefits from the cali-\nbration process. As depicted in Fig. 3a, the ECE for\nuncalibrated model cascade is 25.81, whereas the\nECE for calibrated cascade is reduced to 15.75, rep-\nresenting a substantial decrease of approximately\n40% in ECE. Examining accuracy, the calibrated\ncascade achieves an accuracy of 74.09%, which is\n3.38% higher than that of the uncalibrated cascade.\nSimilar trends are observed in Spanish data, where\nthe ECE for the uncalibrated cascade is 15.31, and\nthe ECE for the calibrated cascade is 11.10.\n5.4\nSensitivity w.r.t. Hyper-parameters\nWe explore the sensitivity of our method concern-\ning the parameter τ in logit normalization. In Fig. 4,\nwe present the average accuracy of our model on\nthe PAWS-X dataset with varying values of τ. With\na two-fold speed-up, τ exerts minimal influence on\ncascade accuracy. However, when a higher speed-\nup ratio is required, we observe that a τ value of\n0.04 corresponds to the peak accuracy.\nTable 4: Zero-shot esult comparison for Llama2 on PAWS-X (Classification), GSM8k (Generation), and\nTabMWP (Generation). Cascade method accuracy around 2, 3, 4 times speed-up on PAWS-X cannot be measured\nin our experiment because the concentrated confidence scores of Cascade, predominantly clustering around the\nvalue 1, resulting from the overconfident predictions by Llama-2. For example, the Cascade model is bounded at the\nceiling at a 4.89 speed-up ratio. Any ratio smaller than that number cannot be measured as we alter the threshold.\nDataset\nSpeed-up\nModel\nen\nfr\nes\nde\nzh\nja\nko\nAvg.\nPAWS-X\n1x\nLLama-2-70B\n78.2\n72.05\n71.15\n72.8\n67.6\n65.9\n64.7\n70.34\n∼2x\nCascade\n-\n-\n-\n-\n-\n-\n-\n-\nC3\n77.35\n69.45\n67.85\n65.95\n65.05\n61.05\n62.45\n67.02\n∼3x\nCascade\n-\n-\n-\n-\n-\n-\n-\n-\nC3\n70.05\n68.05\n66.45\n63.9\n62.7\n59.85\n60.05\n64.44\n∼4x\nCascade\n-\n-\n-\n-\n-\n-\n-\n-\nC3\n67\n67.15\n65.1\n62.95\n61.3\n59.35\n59.6\n63.21\nGSM8k\n1x\nLLama-2-70B\n54.4\n-\n42.6\n38.2\n26.6\n23.4\n19.6\n34.13\n∼2x\nCascade\n30.4\n-\n21.8\n18.2\n15.4\n11.8\n11.8\n18.23\nC3\n33.6\n-\n26.4\n22\n13.4\n14.6\n10.4\n20.06\n∼3x\nCascade\n22.4\n-\n15.8\n11.4\n12.2\n8.4\n8.4\n13.1\nC3\n25.4\n-\n20.8\n15.2\n11.4\n14.6\n8.6\n16\n∼4x\nCascade\n18.8\n-\n13.4\n10.2\n10.4\n7.6\n7\n11.23\nC3\n22.2\n-\n16.8\n12.8\n9.8\n9\n7.2\n12.96\nTabMWP\n1x\nLLama-2-70B\n65.2\n-\n51.8\n39.8\n35.2\n38\n40.2\n45.03\n∼2x\nCascade\n64.8\n-\n46.4\n39.4\n29.8\n32.4\n30.8\n40.6\nC3\n57.8\n-\n45.4\n39.6\n33.4\n34.4\n32.2\n40.47\n∼3x\nCascade\n53.8\n-\n44\n37.4\n28.8\n29.4\n26.4\n36.7\nC3\n55\n-\n44.4\n38\n31.8\n31.8\n29.2\n38.3\n∼4x\nCascade\n54\n-\n42\n36.2\n27.8\n30.6\n25.6\n36.03\nC3\n53.2\n-\n42.2\n38.4\n30.8\n30.6\n28\n37.2\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n84\n85\n86\n87\n88\nAccuracy (%)\n~2x\n~3x\n~4x\nFigure 4: C3 accuracy on PAWS-X w.r.t. τ values.\n5.5\nCase Study\nIn this section, we present concrete cases to illus-\ntrate the functioning of the proposed C3. We ex-\namine several examples across different languages,\nincluding English, Bulgarian, German, Thai, and\nChinese, where calibration rectifies predictions, as\ndemonstrated in Figure 5 in the appendix. These\ncases reveal that vanilla cascade models frequently\nexhibit overconfidence, potentially leading to the\nselection of smaller models in error. In contrast, the\nproposed C3 adeptly chooses appropriate models\nfor inference. For instance, when presented with\nthe premise \"They said we’re providing accommo-\ndations for your stay\" and the hypothesis \"They’re\ncovering housing costs,\" the cascade model dis-\nplays unwavering confidence in the prediction from\nthe second model, while C3 selects the largest\nmodel. This highlights the prevalence of miscali-\nbration, which often results in excessive overcon-\nfidence, causing the model to emit a prediction\nwithout progressing to larger models. Calibration\nplays a pivotal role in mitigating this issue, a pattern\nconsistently observed across various languages.\n6\nConclusion\nIn this paper, we propose a simple, yet effective\nmethod, Confidence Calibration Cascade (C3) that\nenhances cross-lingual inference accuracy through\nmore reliable confidence scores. We introduce cali-\nbration methods for both Language Model cascade\nand Large Language Model cascade. We conduct\nextensive experiment on cross-lingual benchmarks.\nBy comparing with state-of-the-art methods, the\nresults demonstrates the effectiveness of C3. Fur-\nthermore, C3 also demonstrates strong calibration\nresults compared to vanilla cascade methods.\nLimitations\nThe proposed C3 framework depends on a hyperpa-\nrameter τ at a tuning process, which might require\nadditional effort to tune. Fortunately, our experi-\nment shows that the C3 model is not sensitive to\nthis hyperparameter, and thus this issue can be eas-\nily addressed.\nAcknowledgement\nWe thank Google Cloud Research Credits program\nfor supporting our computing needs.\nReferences\nHaoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jing\nJin, Xin Jiang, Qun Liu, Michael Lyu, and Irwin\nKing. 2020. Binarybert: Pushing the limit of bert\nquantization. arXiv preprint arXiv:2012.15701.\nTianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia\nLiu, Yang Zhang, Zhangyang Wang, and Michael\nCarbin. 2020. The lottery ticket hypothesis for pre-\ntrained bert networks. Advances in neural informa-\ntion processing systems, 33:15834–15846.\nCaroline Choi, Fahim Tajwar, Yoonho Lee, Huaxiu Yao,\nAnanya Kumar, and Chelsea Finn. 2023. Conserva-\ntive prediction via data-driven confidence minimiza-\ntion. arXiv preprint arXiv:2306.04974.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nNakano, Christopher Hesse, and John Schulman.\n2021. Training verifiers to solve math word prob-\nlems. arXiv preprint arXiv:2110.14168.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2019. Unsupervised\ncross-lingual representation learning at scale. arXiv\npreprint arXiv:1911.02116.\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Ad-\nina Williams, Samuel R. Bowman, Holger Schwenk,\nand Veselin Stoyanov. 2018. Xnli: Evaluating cross-\nlingual sentence representations. In Proceedings of\nthe 2018 Conference on Empirical Methods in Natu-\nral Language Processing. Association for Computa-\ntional Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nJinhao Duan, Hao Cheng, Shiqi Wang, Chenan Wang,\nAlex Zavalny, Renjing Xu, Bhavya Kailkhura, and\nKaidi Xu. 2023. Shifting attention to relevance: To-\nwards the uncertainty estimation of large language\nmodels. arXiv preprint arXiv:2307.01379.\nJonathan Frankle and Michael Carbin. 2018. The lottery\nticket hypothesis: Finding sparse, trainable neural\nnetworks. arXiv preprint arXiv:1803.03635.\nYuxian Gu, Li Dong, Furu Wei, and Minlie Huang.\n2023. Knowledge distillation of large language mod-\nels. arXiv preprint arXiv:2306.08543.\nChuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Wein-\nberger. 2017. On calibration of modern neural net-\nworks. In International conference on machine learn-\ning, pages 1321–1330. PMLR.\nZongbo Han, Yifeng Yang, Changqing Zhang, Linjun\nZhang, Joey Tianyi Zhou, Qinghua Hu, and Huaxiu\nYao. 2024. Selective learning: Towards robust cali-\nbration with dynamic regularization. arXiv preprint\narXiv:2402.08384.\nXiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao\nChen, Linlin Li, Fang Wang, and Qun Liu. 2020.\nTinyBERT: Distilling BERT for natural language un-\nderstanding. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2020, pages 4163–\n4174, Online. Association for Computational Lin-\nguistics.\nYigitcan Kaya, Sanghyun Hong, and Tudor Dumitras.\n2019. Shallow-deep networks: Understanding and\nmitigating network overthinking. In International\nconference on machine learning, pages 3301–3310.\nPMLR.\nLei Li, Yankai Lin, Deli Chen, Shuhuai Ren, Peng Li,\nJie Zhou, and Xu Sun. 2020. Cascadebert: Acceler-\nating inference of pre-trained language models via\ncalibrated complete models cascade. arXiv preprint\narXiv:2012.14682.\nXiaonan Li, Yunfan Shao, Tianxiang Sun, Hang Yan,\nXipeng Qiu, and Xuanjing Huang. 2021. Accelerat-\ning bert inference for sequence labeling via early-exit.\narXiv preprint arXiv:2105.13878.\nChen Liang, Simiao Zuo, Qingru Zhang, Pengcheng\nHe, Weizhu Chen, and Tuo Zhao. 2023. Less is\nmore: Task-aware layer-wise distillation for language\nmodel compression. In International Conference on\nMachine Learning, pages 20852–20867. PMLR.\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris\nTsipras, Dilara Soylu, Michihiro Yasunaga, Yian\nZhang, Deepak Narayanan, Yuhuai Wu, Ananya Ku-\nmar, et al. 2022. Holistic evaluation of language\nmodels. arXiv preprint arXiv:2211.09110.\nYaobo Liang, Nan Duan, Yeyun Gong, Ning Wu, Fenfei\nGuo, Weizhen Qi, Ming Gong, Linjun Shou, Daxin\nJiang, Guihong Cao, Xiaodong Fan, Ruofei Zhang,\nRahul Agrawal, Edward Cui, Sining Wei, Taroon\nBharti, Ying Qiao, Jiun-Hung Chen, Winnie Wu,\nShuguang Liu, Fan Yang, Daniel Campos, Rangan\nMajumder, and Ming Zhou. 2020. Xglue: A new\nbenchmark dataset for cross-lingual pre-training, un-\nderstanding and generation. arXiv, abs/2004.01401.\nKaiyuan Liao, Yi Zhang, Xuancheng Ren, Qi Su,\nXu Sun, and Bin He. 2021. A global past-future\nearly exit method for accelerating inference of pre-\ntrained language models. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 2013–2023, Online.\nAssociation for Computational Linguistics.\nWeijie Liu, Peng Zhou, Zhe Zha, Zhiruo Wang, Haotang\nDeng, and Qi Ju. 2020a. FastBERT: a self-distilling\nbert with adaptive inference time. In Proceedings of\nACL 2020.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2020b.\nRoberta: A robustly optimized bert pretraining ap-\nproach. In International Conference on Learning\nRepresentations.\nZejian Liu, Fanrong Li, Gang Li, and Jian Cheng.\n2021. Ebert: Efficient bert inference with dynamic\nstructured pruning. In Findings of the Association\nfor Computational Linguistics: ACL-IJCNLP 2021,\npages 4814–4823.\nPan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu,\nSong-Chun Zhu, Tanmay Rajpurohit, Peter Clark,\nand Ashwin Kalyan. 2023. Dynamic prompt learning\nvia policy gradient for semi-structured mathematical\nreasoning. In International Conference on Learning\nRepresentations (ICLR).\nGunho Park, Baeseong Park, Se Jung Kwon, Byeong-\nwook Kim, Youngjoo Lee, and Dongsoo Lee. 2022.\nnuqmm: Quantized matmul for efficient inference\nof large-scale generative language models. arXiv\npreprint arXiv:2206.09557.\nTelmo Pires, Eva Schlinger, and Dan Garrette. 2019.\nHow multilingual is multilingual BERT? In Proceed-\nings of the 57th Annual Meeting of the Association for\nComputational Linguistics, pages 4996–5001, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108.\nKumar Shridhar, Alessandro Stolfo, and Mrinmaya\nSachan. 2023. Distilling reasoning capabilities into\nsmaller language models. In Findings of the Associa-\ntion for Computational Linguistics: ACL 2023, pages\n7059–7073.\nSiqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. 2019.\nPatient knowledge distillation for bert model com-\npression. arXiv preprint arXiv:1908.09355.\nZhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu,\nYiming Yang, and Denny Zhou. 2020. MobileBERT:\na compact task-agnostic BERT for resource-limited\ndevices. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 2158–2170, Online. Association for Computa-\ntional Linguistics.\nHanlin Tang, Xipeng Zhang, Kai Liu, Jianchen Zhu,\nand Zhanhui Kang. 2022. Mkq-bert: Quantized bert\nwith 4-bits weights and activations. arXiv preprint\narXiv:2203.13483.\nKatherine Tian, Eric Mitchell, Allan Zhou, Archit\nSharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn,\nand Christopher D Manning. 2023. Just ask for cali-\nbration: Strategies for eliciting calibrated confidence\nscores from language models fine-tuned with human\nfeedback. arXiv preprint arXiv:2305.14975.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023.\nLlama 2:\nOpen founda-\ntion and fine-tuned chat models.\narXiv preprint\narXiv:2307.09288.\nPaul Viola and Michael Jones. 2001. Rapid object de-\ntection using a boosted cascade of simple features. In\nProceedings of the 2001 IEEE computer society con-\nference on computer vision and pattern recognition.\nCVPR 2001, volume 1, pages I–I. Ieee.\nHaotao Wang, Chaowei Xiao, Jean Kossaifi, Zhiding\nYu, Anima Anandkumar, and Zhangyang Wang. 2021.\nAugmax: Adversarial composition of random aug-\nmentations for robust training. Advances in neural\ninformation processing systems, 34:237–250.\nHaoyu Wang, Yaqing Wang, Huaxiu Yao, and Jing Gao.\n2023. Macedon: Minimizing representation coding\nrate reduction for cross-lingual natural language un-\nderstanding. In The 2023 Conference on Empirical\nMethods in Natural Language Processing.\nXiaofang Wang, Dan Kondratyuk, Eric Christiansen,\nKris M Kitani, Yair Alon, and Elad Eban. 2022.\nWisdom of committees: An overlooked approach\nto faster and more accurate models. In International\nConference on Learning Representations.\nHongxin Wei, Renchunzi Xie, Hao Cheng, Lei Feng,\nBo An, and Yixuan Li. 2022. Mitigating neural net-\nwork overconfidence with logit normalization. In In-\nternational Conference on Machine Learning, pages\n23631–23644. PMLR.\nGuoxuan Xia and Christos-Savvas Bouganis. 2023.\nWindow-based early-exit cascades for uncertainty\nestimation: When deep ensembles are more effi-\ncient than single models.\nIn Proceedings of the\nIEEE/CVF International Conference on Computer\nVision (ICCV).\nMengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi\nChen. 2023. Sheared llama: Accelerating language\nmodel pre-training via structured pruning.\narXiv\npreprint arXiv:2310.06694.\nGuangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu,\nJulien Demouth, and Song Han. 2023. Smoothquant:\nAccurate and efficient post-training quantization for\nlarge language models. In International Conference\non Machine Learning, pages 38087–38099. PMLR.\nJi Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, and\nJimmy Lin. 2020. DeeBERT: Dynamic early exiting\nfor accelerating BERT inference. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 2246–2251, Online.\nAssociation for Computational Linguistics.\nJi Xin, Raphael Tang, Yaoliang Yu, and Jimmy Lin.\n2021. Berxit: Early exiting for bert with better fine-\ntuning and extension to regression. In Proceedings\nof the 16th conference of the European chapter of\nthe association for computational linguistics: Main\nVolume, pages 91–104.\nYinfei Yang, Yuan Zhang, Chris Tar, and Jason\nBaldridge. 2019. PAWS-X: A Cross-lingual Adver-\nsarial Dataset for Paraphrase Identification. In Proc.\nof EMNLP.\nOfir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe\nWasserblat. 2019. Q8bert: Quantized 8bit bert. In\n2019 Fifth Workshop on Energy Efficient Machine\nLearning and Cognitive Computing-NeurIPS Edition\n(EMC2-NIPS), pages 36–39. IEEE.\nChunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao\nSun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu,\nLili Yu, et al. 2023. Lima: Less is more for alignment.\narXiv preprint arXiv:2305.11206.\nWangchunshu Zhou, Canwen Xu, Tao Ge, Julian\nMcAuley, Ke Xu, and Furu Wei. 2020. Bert loses\npatience: Fast and robust inference with early exit. In\nAdvances in Neural Information Processing Systems,\nvolume 33, pages 18330–18341. Curran Associates,\nInc.\nWei Zhu. 2021. Leebert: Learned early exit for bert\nwith cross-level optimization. In Proceedings of the\n59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Vol-\nume 1: Long Papers), pages 2968–2980.\nA\nRelated Work\nModel Compression-based Methods. There are a\nnumber of model compression methods which can\nbe applied to accelerate PLM inference, such as\nknowledge distillation, pruning, and weight quan-\ntization. Knowledge distillation is to leverage a\npowerful teacher model to guide the learning of\na lightweight student model. The lightweight stu-\ndent can support efficient inference. For example,\nDistilBERT (Sanh et al., 2019), TinyBERT (Jiao\net al., 2020), MobileBERT (Sun et al., 2020), and\nPKD (Sun et al., 2019) use knowledge distilla-\ntion to learn a lightweight BERT. (Liang et al.,\n2023), (Zhou et al., 2023), (Gu et al., 2023), (Gu\net al., 2023) and (Shridhar et al., 2023) apply\nknowledge distillation to train a small generative\nmodel. Pruning focuses on finding redundant pa-\nrameters and sets them as zero to achieve highly\nsparse neural network. For example, EBERT (Liu\net al., 2021) and (Chen et al., 2020) propose a\ndynamic structured pruning method and a lot-\ntery ticket hypothesis (Frankle and Carbin, 2018)\nfor BERT, and Sheared Llama (Xia et al., 2023)\ndesigns a structured pruning method for genera-\ntive pre-trained language models. Weight quan-\ntization is to map the model weights into low-\nprecision integers or floating-point numbers, which\nare hardware-friendly and efficient for matrix com-\nputation.\nSpecifically, Q8BERT (Zafrir et al.,\n2019) proposes 8-bit quantization for BERT; MKQ-\nBERT (Tang et al., 2022) designs 4-bit quantiza-\ntion method for BERT; BinaryBERT (Bai et al.,\n2020) explore how to quantize BERT in 1 bit;\nSmoothquant (Xiao et al., 2023) and nuQmm (Park\net al., 2022) study how to apply quantization for\ngenerative language models. However, these meth-\nods often need to train compressed models from\nscratch, which is expensive. Pruning and quantiza-\ntion rely on specialized hardware support, which is\nnot flexible.\nB\nPrompt Design\nWe include the prompt we use for Llama-2 in our\nexperiment on PAWS-X in this section. Detailed\nprompt is shown in Table 5.\nC\nAlgorithm\nD\nECE Figures\nWe report the ECE figures for comparison between\nthe Cascade (in orange) and C3 (in blue) on Swahili\nand English in Figure 7 and Figure 6.\nThey said we’re providing \naccommodations for your stay.\nThey’re covering housing costs.\nEntailment\n4\n3\nТака\nче\nне\nсъм\nмного\nсигурен защо.\nНе знам защо се е случило \nтова.\nEntailment\n4\n2\nEs gibt so viel was ich darüber\nerzählen könnte, ich überspringe\ndas einfach.\nIch werde nicht darüber reden,\nobwohl es viel zu decken gibt.\nEntailment\n4\n2\nเธอกล่าวว่ามีน ้าตาไหลออกมาจากตาของเธอ\nและเธอกล่าวว่าโจมาปรากฏตัวที่ชานบ้าน\nเธอร้องไห้ที่ได้เจอJoe เพ\nราะเธอมีความสุขมาก\nNeutral\n4\n3\n在这个赤裸城市里有许多故事。\n我还没听说过这些故事。\nContradiction\n3\n2\nen\nbg\nde\nth\nzh\nPremise\nHypothesis\nLabel\nModel chosen \nby 𝑪𝟑 \nModel chosen \nby 𝑪𝒂𝒔𝒄𝒂𝒅𝒆\nFigure 5: Case study of five languages.\nAlgorithm 1: Calibration Cascade Existing\nInput: Models {M1, ..., Mn}, threshold λ,\ndata x\nfor i ←1 to n do\n// get the logits after temperature scaling\nlogits = Mi(x) / temperature(Mi)\n// get the probability for each class label\nPr = softmax(logits)\n// get the confidence\nconfidence = max(Pr)\nif confidence > λ or i == n then\nreturn logits\nE\nExperiment\nE.1\nImplementation Details\nIn this section, we introduce the implemtation de-\ntails of our C3 method in two different settings:\nlanguage model cascade (fine-tuning) and large\nlanguage model cascade (zero/few-shot learning).\nLanguage Model Cascade We initialize four en-\ncoder models from pretrained multilingual Distil-\nBERT, multilingual BERT base, XLM-RoBERTa\nbase, and XLM-RoBERTa large respectively. Each\npretrained model is taken from the checkpoint pub-\nlished on Huggingface model hub. We then train\neach model independently on the English split in\nthe training set for {4, 4, 5, 5} epoches with batch\nsizes of {32, 32, 16, 16} and learning rates of {3e-5,\n2e-5, 1e-5, 1e-5} on 4 Nvidia A6000 GPUs.\nLarge Language Model Cascade We initialize\nthree casual language models from pretrained\nLlama-2-7b-chat-hf, Llama-2-13b-chat-hf, and\nLlama-2-70b-chat-hf published on Hugginface\nmodel hub. Without any training, we apply few-\nshot learning by giving the model three examples\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy in bin\nCalibration Error:\n0.10\nDistlBERT (Uncalibrated)\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCalibration Error:\n0.14\nDistlBERT (Calibrated)\n0.2\n0.4\n0.6\n0.8\n1.0\nConfidence\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy in bin\nCalibration Error:\n0.11\nCascade\n0.2\n0.4\n0.6\n0.8\n1.0\nConfidence\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCalibration Error:\n0.08\nC³\nFigure 6: ECE comparison between Cascade and C3 in\nEnglish language.\nfrom the English split in the prompt.\nE.2\nDatasets\nE.3\nCase Study\nUser\nDo (Sentence1, Example1: Sentence1) and\n(Sentnece2, Example1: Sentence2) have\ndifferent meaning?\nAnswer: Example1: Answer.\nDo (Sentence1: Example2: Sentence1) and\n(Sentence2: Example2: Sentence1) have\ndifferent meaning?\nAnswer: Example2: Answer.\nDo (Sentence1: Example2: Sentence1) and\n(Sentence2: Example3: Sentence1) have\ndifferent meaning?\nAnswer: Example3: Answer.\nDo (Sentence1: Question: Sentence1) and\n(Sentence2: Question: Sentence2) have dif-\nferent meaning?\nAnswer:\nAssistant\nAnswer\nTable 5: C3 prompt on Llama-2 for PAWS-X task.\nThe answer candidates set would be {Yes, No}, and the\nembedding set would be {y, n}, where y is the Llama\nembedding of the word \"Yes,\" and n is the Llama em-\nbedding for the word \"No.\"\nTable 6: Dataset we incorparate in our experiment.\nDataset\n# Train\n# Dev\n# Test\nLanguages\n(English)\n(per langauge)\n(per langauge)\nXNLI\n393k\n2.49k\n5.01k\n15\nPAWS-X\n49.4k\n2k\n2k\n7\nQAM\n100k\n10k\n10k\n3\nGSM8k\n-\n-\n500\n6\n(translated)\nTabMWP\n-\n-\n500\n6\n(translated)\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy in bin\nCalibration Error:\n0.33\nDistlBERT (Uncalibrated)\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCalibration Error:\n0.05\nDistlBERT (Calibrated)\n0.2\n0.4\n0.6\n0.8\n1.0\nConfidence\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy in bin\nCalibration Error:\n0.31\nCascade\n0.2\n0.4\n0.6\n0.8\n1.0\nConfidence\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCalibration Error:\n0.21\nC³\nFigure 7: ECE comparison between Cascade and C3 in\nSwahili language.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2024-02-25",
  "updated": "2024-02-25"
}