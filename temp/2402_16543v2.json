{
  "id": "http://arxiv.org/abs/2402.16543v2",
  "title": "Model-based deep reinforcement learning for accelerated learning from flow simulations",
  "authors": [
    "Andre Weiner",
    "Janis Geise"
  ],
  "abstract": "In recent years, deep reinforcement learning has emerged as a technique to\nsolve closed-loop flow control problems. Employing simulation-based\nenvironments in reinforcement learning enables a priori end-to-end optimization\nof the control system, provides a virtual testbed for safety-critical control\napplications, and allows to gain a deep understanding of the control\nmechanisms. While reinforcement learning has been applied successfully in a\nnumber of rather simple flow control benchmarks, a major bottleneck toward\nreal-world applications is the high computational cost and turnaround time of\nflow simulations. In this contribution, we demonstrate the benefits of\nmodel-based reinforcement learning for flow control applications. Specifically,\nwe optimize the policy by alternating between trajectories sampled from flow\nsimulations and trajectories sampled from an ensemble of environment models.\nThe model-based learning reduces the overall training time by up to $85\\%$ for\nthe fluidic pinball test case. Even larger savings are expected for more\ndemanding flow simulations.",
  "text": "Model-based deep reinforcement learning for\naccelerated learning from flow simulations\nAndre Weiner1*† and Janis Geise1†\n1*Institute of Fluid Mechanics, TU Dresden, George-B¨ahr-Str. 3c ,\nDresden, 01069, Saxony, Germany.\n*Corresponding author(s). E-mail(s): andre.weiner@tu-dresden.de;\nContributing authors: janis.geise@tu-dresden.de;\n†These authors contributed equally to this work.\nAbstract\nIn recent years, deep reinforcement learning has emerged as a technique to solve\nclosed-loop flow control problems. Employing simulation-based environments in\nreinforcement learning enables a priori end-to-end optimization of the control sys-\ntem, provides a virtual testbed for safety-critical control applications, and allows\nto gain a deep understanding of the control mechanisms. While reinforcement\nlearning has been applied successfully in a number of rather simple flow control\nbenchmarks, a major bottleneck toward real-world applications is the high com-\nputational cost and turnaround time of flow simulations. In this contribution, we\ndemonstrate the benefits of model-based reinforcement learning for flow control\napplications. Specifically, we optimize the policy by alternating between trajecto-\nries sampled from flow simulations and trajectories sampled from an ensemble of\nenvironment models. The model-based learning reduces the overall training time\nby up to 85% for the fluidic pinball test case. Even larger savings are expected\nfor more demanding flow simulations.\nKeywords: closed-loop flow control, model ensemble proximal policy optimization,\nfluidic pinball\n1 Introduction\nAccording to the 2022 IPCC report, smart control technologies are a key enabler in\nmitigating the impact of global warming and climate change [1]. Control tasks in which\nthe manipulation of fluid flows can significantly reduce the carbon dioxide footprint\n1\narXiv:2402.16543v2  [physics.flu-dyn]  10 Apr 2024\nare ubiquitous in society and industries, e.g., reducing aerodynamic forces acting on\nvehicles [2], maximizing the coefficient of performance of heat pumps [3], or adjusting\nthe operating conditions of process engineering systems in response to the availability\nof renewable energy resources [4], to name a few. To achieve optimal control in the\naforementioned example applications, it is necessary that the controller can respond to\nchanges in the ambient conditions. However, the design and implementation of closed-\nloop active flow control (AFC) systems is highly complex. As an example, consider the\nflow past a truck. At highway speeds, such a flow is fully 3D and comprises turbulent\nboundary layers, flow separation, reattachment, and large turbulent wakes [5, 6]. To\nimplement closed-loop AFC, this complexity must be captured with a limited amount\nof sensors placed on the vehicle’s surface. Similarly, the actuators must be designed and\nplaced sensibly to control the dominant flow structures. Moreover, a suitable control\nlaw must be derived, i.e., a mapping from the sensor reading to the optimal actuation.\nFinally, the nonlinear flow dynamics create a tight coupling between sensors, actuators,\nand control law. Hence, the AFC system should be designed and optimized as a whole.\nRecently, reinforcement learning (RL), and in particular deep RL (DRL), started\nto emerge as a technology in fluid mechanics with the potential to tackle the complex-\nity of closed-loop AFC. Viquerat et al. [7] provide an extensive account of DRL-based\napplications in fluid mechanics. DRL solves control problems through trial-and-error\ninteractions with an environment, where environment is the DRL term for the system\nto be controlled. The unique advantage of learning from simulation-based environ-\nments is the possibility of end-to-end optimization of the full control system, i.e.,\nsensor locations, control law, and actuator positions [8–10]. However, a critical chal-\nlenge of simulation-based RL is the high computational cost and turnaround time of\nflow simulations. Even for relatively simple flow control benchmark problems, state-\nof-the-art RL algorithms require O(100) episodes to converge [7], which is far from\npractical for real-world AFC applications.\nTo emphasize the need for sample-efficient learning, consider again the application\nof DRL-based closed-loop AFC to vehicles. The current gold standard for accurate and\nfast simulations of vehicle aerodynamics are hybrid approaches like improved delayed\ndetached eddy simulations (IDDES) [11]. Computing converged mean force coefficients\nfor the DrivAer test case at Re = 768000, required approximately two days on 700\nCPU cores in 2016 [11]. It is conceivable that roughly five sequential IDDES can be\nperformed per day because it is not necessary to obtain converged statistics in every\nrun. Moreover, hardware and scalability have improved significantly over the past\nyears. Assuming that 10 simulations are performed in parallel per episode to generate\nmore data, each simulation being run on 1000 CPU cores, the computational cost for\n100 episodes is approximately 5M CPU hours, and the training lasts 20 days. Cloud\nproviders charge 0.01-0.05 euros per CPU hour, which puts a price tag of 0.5M-2.5M\neuros on a single training at a single flow condition. Additional hyperparameter opti-\nmization, sensor/actuator variants, and an extension to multiple flow conditions can\neasily increase the cost by a factor of 10-100. This cost, combined with a turnaround\ntime of 20 days, is far from practical for the majority of potential users.\nSeveral remedies exist to reduce the computational cost and turnaround time,\ne.g., by exploiting invariances [12, 13] or by pre-training on simulations with very\n2\ncoarse meshes [14]. While effective, the applicability of the aforementioned techniques\nstrongly depends on the control problem, i.e., the problem must exhibit invariances,\nand the mesh coarsening must not change the flow characteristics. A more general\napproach to improve data efficiency is model-based DRL (MBDRL) [15]. The main\nidea is to substitute the expensive simulation-based environment with one or more\nsurrogate models. The surrogate models are optimized based on data coming from the\nhigh-fidelity environment. Additional data to optimize the control law can be created\nwith little effort by querying the optimized surrogate model. A plethora of MBDRL\nalgorithms exist that differ in the type of surrogate model and how the control law is\nderived from the model. Moerland et al. [15] provide an extensive overview.\nKey challenges in MBDRL are i) the efficient creation of accurate surrogate models\nand ii) dealing with the presence of model error. Due to their flexibility, neural net-\nworks are a common choice for building data-driven surrogate models. However, the\nmodels must be created on the fly based on data that is unavailable for prior tests,\nso the optimization pipeline must be robust and efficient. Moreover, the models are\nauto-regressive. Hence, long-term stability is a persistent challenge. If the prediction\naccuracy is not sufficiently high, querying the models repeatedly leads to vanishing\nor exploding solutions. Finally, even if the model creation workflow is robust and effi-\ncient, there is always epistemic uncertainty due to the alternating iterative updates of\nenvironment models and control law. With each update of the control law, previously\nunknown actuations and states will emerge, and consequently, the environment models\nwill eventually become too inaccurate. Estimating whether the surrogate models are\nstill reliable or should be updated with high-fidelity data from the true environment\nis essential to making MBDRL robust.\nIn this contribution, we present a modified version of the model ensemble trust\nregion policy optimization (METRPO) [16] algorithm and demonstrate the benefits of\nMBDRL for fluid-mechanical applications. Specifically, we compare model-free (MF)\nand model-based (MB) learning on the flow past a circular cylinder [17, 18] and the\nfluidic pinball configuration [9, 19]. The appendix describes various implementation\ndetails and serves as a reference for the abbreviations used throughout the text. More-\nover, we provide a complimentary code repository with instructions to reproduce the\nnumerical experiments and data analyses presented hereafter [20]. A persistent record\nof all scientific results, including a snapshot of the code repository, is publicly accessible\n[21].\n2 Theory\nThis section starts with a very short introduction to essential RL concepts and then\ndescribes the MF algorithm, proximal policy optimization (PPO), employed to solve\nthe flow control problems. Thereafter, we explain the creation of model ensembles,\nwhich emulate the simulation outputs, and how these ensembles are employed in an\nMBDRL variant of PPO. Figure 1 shows how the different algorithmic components\nare connected within one episode of model-based PPO.\n3\nenvironment\nmodel(s) available\nand  reliable?\n(section 2.4) \nsample trajectories\nfrom simulations\nsample trajectories from\nmodel or model ensemble\n(section 2.4)\ncreate/update\nenvironment model(s)\n(section 2.3)\nfinal number\nof episodes\nreached? \nPPO update of value\nand policy network\n(section 2.2)\nnext episode\nno\nyes\ntraining\ncomplete\nno\nyes\none episode of model-based PPO\nFig. 1 High-level overview of one model-based PPO episode.\n2.1 Reinforcement learning\nThere are two main entities in RL: the agent and the environment. The agent con-\ntains the control logic, while the environment represents the simulation or experiment\nthat shall be controlled. The environment at a discrete time step n ∈N is character-\nized by a state Sn ∈S, where S is the space of all possible states. Formally, there is\na difference between the system’s state and a partial observation of the state, since\nthe full state is often inaccessible. Nonetheless, we follow standard practice and refer\nto partial observations and full states alike simply as states. In fluid mechanics, Sn\ncould represent instantaneous pressure or velocity values at one or more sensor loca-\ntions in the region of interest. The agent can manipulate the state via an action\nAn ∈A, where A is the space of all possible actions, upon which the environment\ntransitions to a new state Sn+1. Common actions (means of actuation) for flows are\nsuction, blowing, rotation, or heating. The control objective in RL is expressed by\nthe reward Rn+1 ∈R, where R is the space of all possible rewards, and is assessed\nbased on the transition from Sn to Sn+1. For example, if the control objective is\ndrag reduction, the reward would be defined such that large drag forces yield low\nrewards while small drag forces yield high rewards. The combination of Sn, An, Rn+1,\nand Sn+1 is called an experience tuple [22]. The aggregation of experience tuples\nover N interactions between agent and environment forms a so-called trajectory [22],\nτ = [(S0, A0, R1, S1), . . . , (SN−1, AN−1, RN, SN)]. Trajectories form the basis for opti-\nmizing the policy π(An = a|Sn = s) (the control law), which predicts the probability\nof taking some action a given that the current state is s. Optimality in RL is defined in\nterms of cumulative rewards, the so-called return Gn = PN\ni=n+1 γiRi, because the sys-\ntem’s response to an action may be delayed. The discount factor γ ∈(0, 1] introduces\na notion of urgency, i.e., the same reward counts more if achieved early in a trajectory.\nGiven the same policy π and the same state Sn, the return Gn might vary between\n4\ntrajectories. Incomplete knowledge of the state is one common source of uncertainty.\nTo consider uncertainty in the optimization, RL ultimately aims to find the policy that\nmaximizes the expected return vπ(s) under policy π over all possible states s ∈S [22]:\nv∗\nπ(s) = max\nπ\nEπ[Gn|Sn = s]\n|\n{z\n}\nvπ(s)\n,\n(1)\nwhere Eπ[·] denotes the expected value under policy π, e.g., the mean value for a\nnormally distributed random variable, and v∗\nπ(s) is the optimal state-value function.\nEquation (1) forms the basis for all RL algorithms.\n2.2 Policy learning\nExpression (1) implicitly defines the optimal policy π∗(a|s), which is the policy associ-\nated with the optimal value function v∗\nπ(s). However, additional algorithmic building\nblocks are required to derive a workable policy. PPO is the most common algorithm to\nsolve problem (1) for fluid-mechanical systems [7]. PPO employs deep neural networks\nas ansatz for the value function vπ(s) and the policy π(a|s). Hence, PPO belongs to the\ngroup of actor-critic DRL algorithms. PPO is relatively straightforward to implement\nand enables accelerated learning from multiple trajectories, i.e., multiple trajectories\nmay be sampled and processed in parallel rather than sequentially to produce new\ntraining data T = [τ1, τ2, . . . τK]. It is important to note that there are many PPO\ndetails that vary across different implementations. Many of these details are not thor-\noughly described in the standard PPO reference article [23], but they are nonetheless\nof utmost importance. Andrychowicz et al. [24] provide guidelines based on compre-\nhensive test results for a multitude of algorithmic variants. Our implementation is\nbased on the textbook by Morales [25]. We outline only those elements needed for the\nMBDRL version.\nBased on the trajectories, the free parameters θv of the parametrized value network\nvθv(s) are optimized to predict the expected return over all trajectories. The value\nfunction’s subscript π has been dropped to simplify the notation. The loss function\nincludes some additional clipping to limit the change between two updates [25]:\nLval =\n1\nKN\nK\nX\nk=1\nN−1\nX\nn=0\nmax\nn\u0000Gk\nn −vθv(Sk\nn)\n\u00012 ,\n\u0000Gk\nn −V k\nclip,n\n\u00012o\n,\n(2)\nV k\nclip,n = vold\nθv (Sk\nn) + clip\n\b\nvθv(Sk\nn) −vold\nθv (Sk\nn), −δ, δ\n\t\n,\nwhere vold\nθv\nis the value network’s state before optimization, and the clip function is\ndefined as:\nclip(x, −δ, δ) =\n\n\n\n\n\nx,\nfor −δ ≤x ≤δ\n−δ,\nfor x < −δ\nδ,\nfor x > δ\n.\n(3)\nSimilar to the value function network, the policy network πθπ(a|s) parametrizes a\nmultivariate Beta distribution over possible actions. During exploration, actions are\n5\nsampled at random from this distribution, i.e., An ∼πθπ(Sn). We do not consider a\npotential cross-correlation between actions for exploration. To evaluate if a sampled\naction performs better than expected, the reward is compared to the expected reward,\ni.e. [23, 25]:\nδn = Rn −(vθv(Sn) −γvθv(Sn+1)),\n(4)\nwhere δn is called the 1-step advantage estimate at time step n. Actions with a positive\nadvantage estimate should be favored. Rather than looking one step ahead, one could\nalso look l steps ahead to consider delayed effects. The generalized advantage estimate\nˆA balances between short-term and long-term effects using the hyperparameter λ ∈\n(0, 1] [23, 25]:\nˆAn =\nN−n\nX\nl=0\n(γλ)lδn+l.\n(5)\nFinally, the free parameters of the policy network, θπ, are optimized to make favorable\nactions ( ˆAn > 0) more likely and poor actions ( ˆAn ≤0) less likely. This concept is\ncombined with a clipping mechanism that restricts policy changes [25]:\nLpol = −1\nKN\nK\nX\nk=1\nN−1\nX\nn=0\nmin\n(\nπθπ(Ak\nn|Sk\nn)\nπold\nθπ (Akn|Skn)\nˆAk\nn, clip\n(\nπθπ(Ak\nn|Sk\nn)\nπold\nθπ (Akn|Skn), −ε, ε\n)\nˆAk\nn\n)\n−\nβ\nKN\nK\nX\nk=1\nN−1\nX\nn=0\nE\n\u0002\nπθπ(Sk\nn)\n\u0003\n(6)\nwhere πθπ/πold\nθπ is the probability ratio between the current and old (before optimiza-\ntion) policy, ε is the clipping parameter, E(π) is the policy’s entropy [25], and β is\nthe entropy’s weighting factor. The additional entropy term avoids a premature stop\nof the exploration [15].\nOne sequence of generating trajectories, updating the value network according to\nequation (2), and updating the policy according to equation (6) is called an episode.\nSince the optimization starts with randomly initialized value and policy networks,\nmultiple episodes are required to find the optimal policy.\n2.3 Model learning\nThe type of environment model employed here is a simple feed-forward neural network\nwith weights θm that maps from the last d + 1 states and a given action to the next\nstate and the received reward:\nmθm : (Sn−d, . . . , Sn−1, Sn, An) →(Sn+1, Rn+1).\n(7)\nTo simplify the notation, we introduce the current extended state ˆSn, which comprises\nthe last d + 1 visited states:\nˆSn = [Sn−d, . . . , Sn−1, Sn] .\n(8)\n6\nArranging the current extended state and the current action into a feature vector,\nxn = [ ˆSn, An], and the next state and the received reward into a label vector, yn =\n[Sn+1, Rn+1], the model weights θm can be optimized employing a mean squared error\n(MSE) loss of the form:\nLm =\n1\n|D|\n|D|\nX\ni\n(yi −mθm(xi))2,\n(9)\nwhere D is a set of feature-label-pairs constructed from high-fidelity trajectories, and\n|D| is the overall number of feature-label-pairs. A single trajectory of length N yields\n|D| = N −(d + 1) such pairs. To obtain the model weights based on equation (9), we\nemploy state-of-the-art deep learning techniques, e.g., normalization of features and\nlabels, layer normalization, batch training, learning rate schedule, and early stopping.\nMore details about the model training are provided in appendix A.\n2.4 Model-based proximal policy optimization\nGiven a trained model of the form (7), it becomes straightforward to sample new (fic-\ntitious) trajectories from the model by employing the model recursively. This process\nis described in detail in algorithm 1. The initial extended state ˆS0 is selected from the\nexisting high-fidelity trajectories. Note that the action sampling makes the trajectory\nsampling stochastic, so even when starting from the same ˆS0, executing algorithm 1\nK times generates K different trajectories. For efficiency, the sampling of multiple\ntrajectories should be performed in parallel.\nAlgorithm 1 Sampling of a trajectory from an environment model.\nInput: ˆS0, πθπ, mθm\n▷initial history, policy, env. model\nResult: τ\n▷sampled model trajectory\nτ ←[]\n▷initialize empty trajectory\nˆSn ←ˆS0\n▷initialize current extended state\nwhile i < N do\nAn ∼πθπ(Sn)\n▷sample an action from the current policy\n[Sn+1, Rn+1] ←mθm( ˆSn, An)\n▷predict the next state and the reward\nτ ←τ ∪[[Sn, An, Rn+1, Sn+1]]\n▷append experience tuple to trajectory\nˆSn ←[Sn−d+1, . . . , Sn, Sn+1]\n▷overwrite current extended state\nend while\nIn principle, MBDRL can work by sampling trajectories from a single model.\nHowever, the policy changes with each episode, which leads to newly explored states\nand actions. Naturally, the environment model’s prediction error increases with each\nepisode, and it becomes increasingly challenging to sample meaningful trajectories\nfrom the model. Therefore, it is vital to monitor the model’s confidence in a prediction\nto decide at which point new high-fidelity episodes should be generated. There are two\n7\npathways to include model uncertainty: one could train a fully probabilistic environ-\nment model, i.e., a model that predicts a distribution over the next possible states,\nor one could train an ensemble of regular models [15]. Here, we follow the METRPO\nalgorithm [16] and train an ensemble of Nm simple environment models. Each mem-\nber in the ensemble M = [m1, m2, . . . , mNm] has a different set of weights θi, which\nwe drop from the index to simplify the notation. The ensemble approach introduces\na new hyperparameter, namely the number of models, but training the ensemble is\ntypically easier than training a fully probabilistic network. Moreover, it is straightfor-\nward to optimize the models in parallel. Each model is trained as described before,\ni.e., employing loss function (9). Even though the training procedure stays the same,\nthe optimized models differ from one another for several reasons:\n1. The dataset D is split randomly into Nm subsets, and each model is trained on a\ndifferent subset.\n2. Each model has a different set of randomly initialized weights. Since the optimiza-\ntion is nonlinear, the optimized weights likely differ, even if the same dataset is\nused for training.\n3. The gradient descent algorithm updates the model weights multiple times per epoch\n(iteration) based on batch gradients. The mini-batches are chosen at random from\nthe training data (batch training).\nThere are several options to generate new trajectories from the model ensemble.\nThe most obvious one is to repeat algorithm 1 once per model. In contrast, the original\nMETRPO algorithm mixes the models within the trajectory sampling to improve\nrobustness [16]. The authors state that different models are likely to have different\nmodel biases due to the training on a different subset of the data. When mixing the\nmodels, the different biases can partially cancel out. The alternation between models\nis achieved by sampling a different model for each step from a categorical distribution\nPM over Nm classes, where each class i has an equal selection probability of pi = 1/Nm.\nThe modified sampling strategy, which we also employ here, is depicted in algorithm 2.\nAlgorithm 2 Sampling of a trajectory from an ensemble of environment models.\nDifferences compared to algorithm 1 are marked in bold.\nInput: ˆS0, πθπ, M\n▷initial history, policy, model ensemble\nResult: τ\n▷sampled model trajectory\nτ ←[]\n▷initialize empty trajectory\nˆSn ←ˆS0\n▷initialize current extended state\nwhile i < N do\nAn ∼πθπ(Sn)\n▷sample an action from the current policy\nmn ∼PM\n▷randomly select a model from the ensemble\n[Sn+1, Rn+1] ←mn( ˆSn, An)\n▷predict the next state and the reward\nτ ←τ ∪[[Sn, An, Rn+1, Sn+1]]\n▷append experience tuple to trajectory\nˆSn ←[Sn−d+1, . . . , Sn, Sn+1]\n▷overwrite current extended state\nend while\n8\nBesides the robust generation of trajectories, the ensemble allows to quantify pre-\ndiction uncertainty. More precisely, we would like to determine, at which point the\nensemble becomes unsuitable to generate new trajectories. Therefore, in each model-\nbased episode, K trajectories are generated from the ensemble employing algorithm\n2, and Nm additional trajectories, one for each model, are generated employing algo-\nrithm 1. The ensemble trajectories are used to update policy and value networks.\nThe model-specific trajectories are used to evaluate the policy loss (6) individually\nfor each model to obtain a scalar value expressing the model’s quality (not for gradi-\nent descent). Comparing the ith model’s loss values of the current episode, i.g., Lnew\npol,i,\nwith the loss of the same model obtained in the previous episode, i.e., Lold\npol,i, allows\nassessing the quality of the policy update in the current episode. The loss trends for\nall models in the ensemble are combined by evaluating:\nNpos =\nNm\nX\ni=1\nH\n\u0000Lold\npol,i −Lnew\npol,i\n\u0001\n,\n(10)\nwhere H is the Heaviside step function. In simple terms, Npos is the number of models\nin the ensemble for which the policy loss improves after an update of the policy.\nValues of Npos/Nm close to zero indicate that the models’ generalization capabilities\nare exhausted. Consequently, new high-fidelity data should be generated to update\nthe ensemble. We define the condition Npos ≥Nthr to switch between model-based\nand simulation-based trajectory sampling. Values of Nthr/Nm close to zero encourage\nlearning from the models at the risk of decreased robustness, whereas values close to\none ensure high model quality at the risk of decreased computational efficiency. It is\nconceivable that Nthr/Nm ≈0.5 might be a suitable compromise between efficiency and\ncontrol performance. The final model ensemble PPO (MEPPO) workflow is depicted\nin algorithm 3.\nAlgorithm 3 Model-based proximal policy optimization (MEPPO) algorithm.\nInput: vθv, πθπ, M\n▷initial value and policy networks, model ensemble\nResult: θ∗\nπ\n▷optimal policy weights\ne ←0\n▷initialize episode counter\nwhile e < emax do\nif Npos < Nthr or e < 2 then\n▷sample high-fidelity data\nT ←GenerateCFDTrajectories(πθπ)\n▷run K simulations\nM ←UpdateModelEnsemble(M, T)\n▷loss function (9)\nNpos ←Nm\n▷all models trustworthy\nelse\n▷sample from model ensemble\nT ←GenerateEnsembleTrajectories(πθπ, M)\n▷use algorithm 2 K times\nNpos ←EvaluateModels(πθπ, M)\n▷algorithm 1, equation (10)\nend if\nπθπ ←UpdatePolicyNet(πθπ, vθv, T)\n▷loss function (6)\nvθv ←UpdateValueNet(vθv, T)\n▷loss function (2)\nend while\n9\n3 Results\nWe demonstrate the MEPPO algorithm on two flow configurations, namely a rotating\ncylinder in channel flow and the fluidic pinball. The numerical setups are described\nin section 3.1. In section 3.2, we compare MF and MB trainings and investigate the\ninfluence of ensemble size Nm and switching criterion Nthr. Section 3.3 presents a\nshort discussion of the optimal policies found for each test case. The numerical simu-\nlations are performed with the OpenFOAM-v2206 toolbox [26]. The orchestration of\nsimulations and the DRL logic are implemented in the drlFoam package [27]. Further\nimplementation details are available in the complementary code repository [20].\n3.1 Flow control problems\n3.1.1 Cylinder flow\nThe flow past a circular cylinder has become an established AFC benchmark. Orig-\ninally, the setup was introduced by Sch¨afer et al. [28] as a benchmark for numerical\nmethods. Rabault et al. [17] were the first to adopt the setup for DRL-based AFC.\nSince then, many variants with different means of actuation, sensor positions, and\nReynolds numbers have been investigated [7].\nx\ny\ninlet\noutlet\nwall\nwall\n22d\n1.5d\nd\n4.1d\n1.6d\nω\nFig. 2 AFC setup of the flow past a cylinder; based on [17, 18, 28].\nThe setup used in the present study is depicted in figure 2. The inlet velocity profile\nis parabolic [28], while the velocity vector at the upper and lower domain boundary\nis zero. At the outlet boundary, the velocity gradient is set to zero. For the pressure,\na fixed reference value is applied at the outlet, while the gradient is set to zero on\nall other boundaries. The Reynolds number based on the mean inlet velocity Uin, the\ncylinder diameter d and the kinematic viscosity ν is Re = Uind/ν = 100. To solve\nthe incompressible Navier Stokes equations, we employ the pimpleFoam solver with\nresidual control. The pressure-velocity coupling is stopped once the initial residuals\nfor pressure and momentum equations drop below 10−4. Since the flow is laminar,\nno additional turbulence modeling is necessary. The mesh consists of approximately\n10\n5.3×103 hexahedral cells and is created using the blockMesh utility. For completeness,\nwe note that the mesh has one cell layer in the third spatial direction of depth ∆z =\n0.1d, even though the simulation is 2D. The extension in the third direction is a\ntechnical requirement of 2D simulations in OpenFOAM. To discretize convective and\ndiffusive fluxes, we employ pure linear interpolation. An implicit first-order Euler\nmethod is used for the temporal discretization.\nThe state (observation) used as input for control is formed by 12 pressure probes\nplaced in the cylinder’s wake. The flow is actuated by rotating the cylinder. The control\naims to minimize the forces acting on the cylinder. Given the density-normalized\nintegral force vector F = [Fx, Fy]T , the corresponding force coefficients along each\ncoordinate are:\ncx =\n2Fx\nU 2\ninAref\n,\ncy =\n2Fy\nU 2\ninAref\n,\n(11)\nwhere the reference area is Aref = d∆z. The instantaneous reward at time step n is\ncomputed as [18]:\nRn = 3 −(cx,n + 0.1|cy,n|) .\n(12)\nNote that we do not use time or window-averaged coefficients but the instantaneous\nvalues ci,n. The magical constants in equation (12) simply scale the reward to a value\nrange near zero, which avoids additional tuning of PPO hyperparameters.\nGiven the convective time scale tconv = d/Uin, the control starts once the quasi-\nsteady state is reached, which is at 40tconv. The policy is queried once every 20\nnumerical time steps, which corresponds to a control time step of ∆tc = 0.1tconv.\nWithin the control interval, the angular velocity linearly transitions from ωn to ωn+1\n[18]. The value range of the normalized angular velocity, ω∗= ωd/Uin, is limited to\nω∗∈[−0.5, 0.5]. Overall, N = 400 experience tuples are generated within one trajec-\ntory, which corresponds to a duration of 40tconv. The generation of a single trajectory\ntakes approximately 4min on two MPI ranks.\n3.1.2 Fluidic pinball\nNoack et al. [29] introduced the fluidic pinball, which is a triangular configuration of\nthree rotating cylinders. The unforced flow undergoes several different vortex shedding\nregimes, namely symmetric, asymmetric, and chaotic vortex shedding. The cylinder-\nbased Reynolds number of the present setup is Re = 100, which places the flow in the\nasymmetric vortex shedding regime.\nThe setup is depicted in figure 3. To reduce the initial transient simulation phase,\nwe apply a step function velocity profile at the inlet with ε = 0.01Uin. A constant\nvelocity vector is set on the lower and upper domain boundaries, i.e., 1.01Uin and\n0.99Uin, respectively. The velocity gradient at the outlet boundary is set to zero.\nSimilar to the cylinder setup, the pressure is fixed at the outlet, and the pressure\ngradient on all other boundaries is set to zero. The remaining details of the setup are\nlargely the same as those presented in section 3.1.1. Due to the more complex geometry,\nthe mesh consists of approximately 5.3×104 control volumes. For the convective term\nof the momentum equation, we employ a limitedLinear scheme with a coefficient of\n1.0.\n11\nω1\nω3\nω2\nUin + ε\nUin −ε\nx\ny\nd\n1.5d\n6d\n6d\n22d\n1.3d\n6d\ninlet\noutlet\nFig. 3 AFC setup of the fluidic pinball; based on [29].\nA total of 14 pressure sensors form the state. The positions depicted in figure 3\nare loosely inspired by a preliminary optimal sensor placement study [9]. The precise\nnumerical coordinates of each sensor can be inferred from the setup files in the comple-\nmentary code repository. The policy parametrizes a trivariate Beta distribution over\nthe angular velocities ωi, i ∈{1, 2, 3}. As the control objective, we aim to minimize\nthe cumulative forces acting on all three cylinders. Given the density-normalized inte-\ngral force Fi = [Fx,i, Fy,i]T of the ith cylinder, the corresponding force coefficients are\ndefined as:\ncx,i =\n2Fx,i\nU 2\ninAref\n,\ncy,i =\n2Fy,i\nU 2\ninAref\n,\n(13)\nwhere the reference area is the projected area of all three cylinders in x-direction,\nnamely Aref = 2.5d∆z. Definition (13) ensures that the sum over all cylinders recovers\nthe correct total force coefficients:\ncx =\n3\nX\ni=1\ncx,i,\ncy =\n3\nX\ni=1\ncy,i.\n(14)\nBased on the cumulative force coefficients, the instantaneous reward is computed as:\nRn = 1.5 −(cx,n + 0.5|cy,n|).\n(15)\nThe reward definition is very similar to the one used for the single cylinder. Note that\nthere is no particular reason for this definition other than that it was employed in a\nprevious study [19]. Alternatively, we could have also summed up the magnitudes of the\n12\nindividual force coefficients. However, the focus of this work is on the demonstration\nof MBDRL. The magical constants in equation 15 have the same purpose as before,\nnamely normalizing the reward values.\nKeeping the same definition of the convective time scale as before, the control\nstarts at the end of the initial transient regime, which is at 200tconv. The policy is\nqueried every 50 numerical time steps, corresponding to a control interval of ∆tc =\n0.5tconv. The trajectory extends over 100tconv such that N = 200 experience tuples are\ngenerated. The dimensionless angular velocities are limited to the range ω∗\ni ∈[−5, 5].\nGenerating a single trajectory with eight MPI ranks requires approximately 40min.\nThe main parameters relevant to AFC are summarized in table 1 for both test cases.\nTable 1 Characteristic parameters of the two AFC setups; ranks refers to the number\nof MPI ranks; CV is the number of control volumes, and Ttr is the time required to\nexecute one simulation (to sample one trajectory).\ncase\nRe\ntconv\n∆tc/tconv\nω∗range\nN\nranks\nCV\nTtr\ncylinder\n100\n0.1s\n0.1\n[−0.5, 0.5]\n400\n2\n5.3 × 103\n≈4min\npinball\n100\n1s\n0.5\n[−5, 5]\n200\n8\n5.3 × 104\n≈40min\n3.2 Training performance\n3.2.1 General remarks about the training\nWe compare the control performance and the computational cost between MF and\nMB trainings. Each training is executed in isolation on a dedicated compute node\nwith 96 CPU cores. To discuss the schematics presented later on, it is necessary to\noutline several implementation details. In each episode, exactly ten trajectories are\ngenerated in parallel before updating the policy and value networks. The buffer size\nand the parallel trajectory generation are the same for CFD and MB trajectories. We\nperform a very rough check of the force coefficients in the MB trajectories and discard\ntrajectories that are obviously extremely inaccurate. Specifically, we only keep an MB\ntrajectory for the cylinder flow if all coefficients fulfill the criteria 2.85 ≤cx ≤3.5\nand |cy| ≤1.3. The analogous bounds for pinball trajectories are −3.5 ≤cx,i ≤\n3.5 and |cy,i| ≤3.5. Note that these bounds are really generous and filter out only\nextreme prediction errors. If one or more trajectories are discarded, the sampling is\nrepeated until ten valid trajectories are available. The training is stopped after 200\nand 150 episodes for the cylinder and pinball cases, respectively. Finally, we note that\nall parameter configurations for the cylinder flow are repeated with five different seed\nvalues. All results presented in section 3.2.2 are averaged over these five runs. Due to\nthe computational cost of the fluidic pinball, we execute only a single training run per\nparameter configuration.\n13\n3.2.2 Cylinder flow\nFigure 4 shows the episode-wise mean rewards received with different training con-\nfigurations. Within 200 episodes almost all configurations achieve a similar maximum\nreward. The maximum reward in the MF training is slightly lower, but the reward\nstill keeps increasing moderately toward the end of the training. Surprisingly, all MB\ntrainings reach the maximum reward earlier than the MF training. We noticed that\nthe MB trajectories display less small-scale variance than the CFD trajectories. Pre-\nsumably, this indirect filtering performed by the environment models affects the policy\nand value network updates positively. In general, The MB trainings display a higher\nvariance in the mean reward, especially within the first 50 episodes, where the changes\nin the reward are the strongest. This behavior is related to individual MB trajectories\nwith poor prediction quality. Luckily, these trajectories seem to have a limited impact\non the learning.\n0\n100\n200\n−0.3\n−0.2\n−0.1\n0.0\nmodel-free\n0\n100\n200\nNm = 1\n0\n100\n200\nNm = 5\nNthr = 3\nNthr = 2\n0\n100\n200\nNm = 10\nNthr = 5\nNthr = 3\ne\nR\nFig. 4 Cylinder flow: episode-wise mean reward R for different training configurations; the shaded\narea encloses one standard deviation below and above the mean; mean and standard deviation are\ncomputed over all trajectories of all seeds; for the MB training, the markers indicate CFD-based\ntrajectory sampling.\nThe markers in the MB training in figure 4 indicate CFD trajectories. For the MB\ntraining with one model, we switch back to simulation-based sampling every fourth\nepisode, so 75% of all episodes use the environment model. The same ratio results\nin the MEPPO training with Nm = 5 and Nthr = 3. Interestingly, the automated\nswitching between CFD and model sampling leads to a relatively regular alternation.\nThe same number of models with a lowered threshold value of Nthr = 2 reduces\nthe amount of CFD episodes to 15%, notably without performance degradation. The\ntraining with Nm = 10 and Nthr = 5 also employs the model ensemble in 75% of\nthe episodes. Reducing the threshold value in the latter case to Nthr = 3 reduces\nthe number of CFD episodes to 6%. However, the optimization becomes significantly\nless stable and does not reach optimal control performance. Note that the amount\nof training data per model decreases as the number of models increases. Therefore,\n14\nincreasing the number of sampled trajectories per episode or allowing overlapping\ntraining data could potentially stabilize trainings with a lowered threshold value. The\nincreased amount of training data when employing only a single model could explain\nthe quick convergence of the corresponding training after approximately 100 episodes.\nNm = 1\nNm = 5\nNthr = 3\nNm = 5\nNthr = 2\nNm = 10\nNthr = 6\nNm = 10\nNthr = 5\nNm = 10\nNthr = 3\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\nTMB / TMF\n62.32%\n28.50%\n7.08%\n76.41%\n11.72%\n9.52%\n67.21%\n11.29%\n17.76%\n79.71%\n8.94%\n9.15%\n75.84%\n9.75%\n11.93%\n49.86%\n7.17%\n36.63%\nCFD sampling\nMB sampling\nmodel update\nPPO update\nother\nFig. 5 Cylinder flow: composition of the total MB training time TMB normalized with the MF\ntraining time TMF ≈14h.\nAssuming that the creation of environment models is reliable and much less costly\nthan a CFD simulation, the more CFD episodes can be replaced with MB episodes, the\nlower the overall training time. This trend is reflected by the normalized MB training\ntime depicted in figure 5. Remarkably, all configurations reduce the training time by\nat least 65%, even though the computational cost to simulate the cylinder flow is fairly\nsmall. To better understand differences in the overall training time, figure 5 also shows\nthe time spent on the most relevant parts of the training. For all configurations, the\nsimulation runs remain the most time-consuming element. The time spent updating\nthe networks for environment, policy, and value function remains fairly constant.\nA noticeable increase can be observed in the time spent to sample trajectories\nfrom a single environment model. This increase, as well as other variations in the sam-\npling time, are related to the removal of obviously corrupted trajectories, as explained\nin section 3.2.1. Figure 6 shows the number of discarded trajectories per training.\nClearly, the ensemble models lead to a significantly more robust trajectory sampling.\nMoreover, ensembles with more models produce fewer invalid samples. It is notewor-\nthy to mention that the majority of invalid trajectories result in the initial training\nphase, e < 50, presumably because of the strong exploration and policy changes. In\n15\nour implementation, trajectories are sampled in parallel. However, the constraint of\nexecuting the PPO update with exactly ten valid trajectories leads to repeated sam-\npling in the case of discarded trajectories. Of course, this extra time could be avoided\nby sampling more trajectories than required in parallel or by loosening the constraint.\nNm = 1\nNm = 5\nNthr = 3\nNm = 5\nNthr = 2\nNm = 10\nNthr = 6\nNm = 10\nNthr = 5\nNm = 10\nNthr = 3\n0\n20\n40\n60\nNd\nFig. 6 Cylinder flow: number of discarded trajectories Nd per training; the box plot shows the\noutcome of five independent runs (seed values); the orange line indicates the median value, and the\nmarkers indicate extreme outcomes.\nWe note that there are also small variations in the measured execution times that\nare not straightforward to explain. One means to reduce these variations would be exe-\ncuting additional training runs with varying seeds. However, the main trends in terms\nof control performance and training time are clearly visible and explainable. More-\nover, there are nontrivial coupling effects between simulations, environment models,\nand PPO updates. As discussed before, policy updates differ between MF and MB\ntrainings and also depend to some degree on ensemble size and switching criterion.\nThe policy employed in a simulation impacts the execution time, e.g., by subsequent\nchanges in the pressure-velocity-coupling iterations, or the linear solver iterations.\n3.2.3 Fluidic pinball\nDue to the computational cost of the pinball setup, we do not perform multiple train-\ning runs and investigate fewer hyperparameter configurations. Figure 7 shows the\nepisode-wise mean rewards of all trainings. The final reward is similar for all tested\nconfigurations. The MF training reaches the maximum reward after approximately\n60 episodes. Thereafter, the control performance drops slightly. The highest overall\nreward is achieved by the MB training with Nm = 10. For the MB training with a\nsingle model, we switch every fourth episode to CFD trajectories. Also the automated\nswitching criterion in the training with Nm = 10 and Nthr = 5 leads to 25% CFD\nepisodes. The smaller ensemble with five models switches approximately every fifth\nepisode back to the high-fidelity samples.\nFigure 8 shows the main contributions to the normalized training time. Even\nthough the relative amount of model trajectories is similar to the cylinder flow, the\n16\n0\n50\n100\n−1.0\n−0.5\n0.0\n0.5\n1.0\nmodel-free\n0\n50\n100\nNm = 1\n0\n50\n100\nNthr = 2\nNm = 5\n0\n50\n100\nNthr = 5\nNm = 10\ne\nR\nFig. 7 Fluidic pinball: episode-wise mean reward R for different training configurations; the shaded\narea encloses one standard deviation below and above the mean; mean and standard deviation are\ncomputed over all trajectories; for the MB training, the markers indicate CFD-based trajectory\nsampling.\nruntime reduction is even more significant due to the higher computational cost of per-\nforming the pinball simulation. Remarkably, all configurations reduce the training time\nby at least 80%. The remaining CFD episodes dominate the overall time consumption.\nAs for the cylinder test case, the time spent on sampling model trajectories increases\nwhen using only a single model. Ten trajectories were discarded during the training\nand had to be re-sampled. In the MEPPO runs, no invalid trajectories occurred. The\nincreased time spent on simulations in the training with ten models is a result of the\nnon-trivial interaction between policy learning and the numerical properties of the\nsimulation, as discussed before.\n3.3 Analysis of the best policies\n3.3.1 Cylinder flow\nIn this section, we compare the final policies of the MF and MB trainings that achieved\nthe highest mean reward. Note that the achieved force reduction and the corresponding\ncontrol are by no means new [7]. However, we would like to point out a few subtle\ndifferences in the final MF and MB policies. It is noteworthy that all MB policies\nachieved a similar force reduction, even though we present only results for the MEPPO\ntraining with ten models.\nFigure 9 shows the optimal control and the corresponding force coefficients. Both\nMF and MB policies reduce the drag force by approximately 7%. The second force\ncoefficient remains near zero after 20 convective time units. Both the MF and MB\npolicies perform a very similar actuation within the first five convective time units.\nThereafter, the MB policy rotates the cylinder slightly less and archives an almost\nperfectly steady force balance. Small fluctuations remain in the angular velocity and\nthe side force when employing the MF policy.\n17\nNm = 1\nNm = 5\nNthr = 2\nNm = 10\nNthr = 5\n0.00\n0.05\n0.10\n0.15\n0.20\nTMB / TMF\n88.18%\n8.26%\n94.49%\n95.19%\nCFD sampling\nMB sampling\nmodel update\nPPO update\nother\nFig. 8 Fluidic pinball: composition of the total MB training time TMB normalized with the MF\ntraining time TMF ≈130h.\nThe cylinder rotation steadies and extends the wake. The pressure drop over the\ncylinder decreases, as can be inferred visually in figure 10. The figure also shows the\nsignificant suppression of vortex shedding. While small pressure fluctuations remain\nwhen employing the MF policy, the MB policies leads to an almost perfectly steady flow\nfield. It is conceivable that the MF training could achieve a similar control performance\nwith more training episodes and additional PPO hyperparameter tuning. However, the\nMB runs achieve the presented control performance with even less than 200 episodes.\n3.3.2 Fluidic pinball\nSeveral established control mechanisms exist for the fluidic pinball, e.g., boat tailing\nand base bleeding [30]. The strategy learned by the policies in the present work is\nreferred to as boat tailing. Figure 11 compares the control and the resulting force\ncoefficients of the best-performing MF and MB policies. Both policies follow a similar\nstrategy, i.e., rotating cylinders 2 and 3 with maximum speed in opposite directions\nwhile keeping the cylinder in the front nearly still. This actuation leads to boat tail-\ning and reduces the drag coefficient by approximately 87%. Of course, the reduction\nstrongly depends on the allowed maximum rate of rotation and would be smaller for\nsmaller rates of rotation.\nWhile the drag reduction is fairly similar, the MF policy introduces a nonzero net\nforce in y-direction. This asymmetry is caused by a subtle difference in the absolute\nvalues of ω∗\n2 and ω∗\n3, i.e., the absolute value of ω∗\n2 is marginally larger. The same\nsmall difference between ω∗\n2 and ω∗\n3 is present in the MB policy. However, the MB\n18\n2.5\n3.0\n3.5\ncx\ncontrolled →\n−1\n0\n1\ncy\n30\n40\n50\n60\n70\n80\n90\n100\nt∗\n−0.5\n0.0\n0.5\nω∗\nuncontrolled\nMB, Nm = 10, Nthr = 5\nMF\nFig. 9 Cylinder flow: angular velocity and force coefficients resulting from the best MF and MB\npolicies; the dimensionless time is t∗= t/tconv.\nFig. 10 Temporal mean and standard deviation of the pressure fields with and without control;\nboth mean and standard deviation are normalized with the minimum (blue) and maximum (yellow)\nvalues of the uncontrolled case; the coordinates are normalized with the diameter.\n19\npolicy applies a small positive spin to the cylinder in the front, which balances the\nside forces to net zero. The reader might be wondering if the same drag reduction and\nnet zero force in y-direction could be achieved by setting ω∗\n1 = 0 and ω∗\n2 = −ω∗\n3 = 5.\nThe short answer is yes. However, the latter control leads to relatively strong cy\nfluctuations whose amplitude decays only slowly. Approximately 150 convective time\nunits are necessary to reach the steady state with open-loop control, which is longer\nthan the prescribed trajectory length employed in the training. The asymmetric control\nperformed by the MB policy achieves the same force reduction in about 20 convective\ntime units.\n0\n1\n2\nP\ni cx,i\ncontrolled →\n−0.5\n0.0\n0.5\nP\ni cy,i\n200\n210\n220\n230\n240\n250\nt∗\n−5\n0\n5\nω∗\ni\nω∗\n1\nω∗\n2\nω∗\n3\nuncontrolled\nMB, Nm = 10, Nthr = 5\nMF\nFig. 11 Fluidic pinball: angular velocities and summed force coefficients resulting from the best MF\nand MB policies; the dimensionless time is t∗= t/tconv.\nFinally, we want to shed some light on the local flow dynamics created by the\nspinning cylinders. Figure 12 shows the velocity field in the vicinity of the cylinders. As\nmentioned before, the uncontrolled state is asymmetric, as can be observed by looking\nat the wakes of the two cylinders in the back. In the controlled flow, the rotation of\ncylinders two and three pushes fluid between them in upstream direction. This suction\nsignificantly reduces the extent of the wake and the cumulative pressure drop over\nthe cylinders. The fluid then passes through the gaps between the rear cylinders and\nthe front cylinder. For the MF policy, the fluxes in positive and negative y-direction\nare fairly balanced. Instead, the MB policy hinders the flux between cylinders one\nand two and diverts more fluid to the gap between cylinders one and three. This\nsmall imbalance is enough to compensate for the different rotation speeds of the rear\ncylinders.\n4 Conclusion\nDRL-based AFC is a promising approach to enable smart control technology. A current\nlimitation of simulation-based DRL is the high computational cost and turnaround\n20\nFig. 12 Comparison of instantaneous velocity fields with and without control; the velocity field is\nnormalized with the inlet velocity.\ntime associated with the optimization. The idea of MBDRL is the creation of low-\ncost environment models that partially replace the costly sampling of high-fidelity\ntrajectories from simulations. On two common benchmark AFC problems, we demon-\nstrate that the MEPPO algorithm adopted here can tremendously reduce the training\ncost and time while achieving optimal control performance. The relative reduction of\ntraining time increases with the cost of the CFD simulation. Therefore, we expect the\nMEPPO algorithm, or variants thereof, to be a key enabler in performing DRL-based\nAFC on realistic, 3D simulations at an industrial scale. Of course, further tests on\nmore complex flow configurations are required to consolidate this hypothesis.\nThere are a number of promising options to reduce the training cost even further.\nCreating accurate auto-regressive environment models on the fly posed a significant\nchallenge. Automating the model creation, e.g., by means of Bayesian hyperparam-\neter optimization, could be beneficial for the models’ accuracy and adaptability to\nnew control problems. The more accurate the models can extrapolate, the more sav-\nings are possible. More advanced recurrent network architectures or transformers [31]\nmight be capable of achieving higher accuracy than the simple, fully connected net-\nworks employed here. However, such advanced networks are typically also harder to\ntrain. Alternatively, more straightforward approaches to creating reduced-order mod-\nels might be worthwhile to explore, too. For example, dynamic mode decomposition\nwith control [32] could be a drop-in replacement for the models employed here.\nWe also noticed a strong dependency of the learning progress on the PPO hyper-\nparameters. Due to the high training cost, automated tuning of these parameters\nis infeasible for complex simulations. However, recent improvements in the PPO\nalgorithm, e.g., the PPO-CMA variant [33], reduce the number of hyperparameters,\n21\nimprove robustness, and accelerate learning. Such improvements would be extremely\nbeneficial both for MF and MB learning.\n22\nSupplementary information.\nAcknowledgements.\nThe\nauthors\ngratefully\nacknowledge\nthe\nDeutsche\nForschungsgemeinschaft DFG (German Research Foundation) for funding this work\nin the framework of the research unit FOR 2895 under the grant WE 6948/1-1. The\ncomputational resources to conduct the numerical experiments were kindly provided\nby Amazon Web Services. Finally, the authors gratefully acknowledge the organizing\ncommittee of the 18th OpenFOAM Workshop for the support provided during the\npreparation of this manuscript.\nDeclarations\nConflict of interest\nThe authors have no conflict of interest to declare.\nEthical approval\nNo experiments on humans or animals have been conducted.\nFunding\nThis work has been funded by the German Research Foundation (DFG) under the\ngrant WE 6948/1-1.\nAvailability of data and materials\nThe full research data and instructions to reproduce the numerical experiments and\nanalyses are available at:\nhttps://doi.org/10.23728/b2share.85ab8f3f68724372b83babbdaca85910\nAppendix A\nEnvironment models\nWe use a standard deep learning workflow to train the environment models. The\ndifferent optimization techniques are described in detail by Raff [31]. The activation\nfunction, the number of hidden layers, and the number of neurons per layer have been\ndetermined by employing a simple grid search. Table A1 provides an overview of the\nmost important hyperparameters. Since the amount of training data is fairly small,\nwe employ 8 CPU cores to train a single model. Note that the models predict the\nindividual force coefficients rather than the full reward. Consequently, the output layer\nhas 12+2 and 14+6 neurons for the cylinder and pinball test cases, respectively. The\ntraining is stopped once the maximum number of epochs (gradient descent steps) is\nreached, the absolute loss value falls below a threshold value, or the relative change\nof the loss averaged over 40 epochs becomes too small.\n23\nTable A1 Hyperparameters for the training of the environment models.\nparameter\ncylinder\npinball\nhidden layers\n3\n3\nlayer normalization\nLayerNorm\nLayerNorm\nneurons per hidden layer\n100\n100\ntime delays d\n30\n30\ninput neurons\n450\n690\noutput neurons\n14\n20\nactivation function\nleaky ReLU\nleaky ReLU\nmax. number of epochs\n2500\n2500\nbatch size\n25\n25\ntrain/validation split\n75/25%\n75/25%\nabsolute stopping loss\n10−6\n10−6\nrelative stopping loss\n10−7\n10−7\nloss function\nMSELoss\nMSELoss\noptimizer\nAdamW\nAdamW\nlearning rate schedule\nReduceLROnPlateau\nReduceLROnPlateau\ninit./min. learning rate\n10−2/10−4\n10−2/10−4\nAppendix B\nPPO hyperparameters\nMost PPO hyperparameters employed here are standard values suggested in the litera-\nture [23, 25]. In contrast to the original implementation, we use two separate networks\nfor policy and value function. The two networks are optimized sequentially by two dif-\nferent optimizers. Since the optimization is inexpensive, we allocate only 5 CPU cores.\nAs suggested in [25], the policy optimization is stopped if the difference between old\nand new policy, measured in terms of KL divergence, exceeds a prescribed threshold.\nHowever, we noticed that this criterion is rarely triggered.\nTable B2 PPO hyperparameters.\nparameter\ncylinder\npinball\nhidden layers\n2\n2\nneurons per hidden layer\n64\n512\ninput neurons\n12\n14\noutput neurons\n2\n6\nactivation function\nReLU\nReLU\ndiscount factor γ\n0.99\n0.99\nadvantage smoothing λ\n0.97\n0.97\nlearning rate policy net.\n10−3\n10−5\nlearning rate value net.\n5 × 10−4\n10−5\nmax. number of epochs\n100\n100\nbatch size\nfull\nfull\nvalue and policy clipping\n0.1\n0.1\noptimizer\nAdamW\nAdamW\nKL divergence\n0.2\n0.2\nentropy weight β\n0.01\n0.01\n24\nAFC\nactive flow control\nCFD\ncomputational fluid dynamics\nDRL\ndeep reinforcement learning\nIDDES\nimproved delayed detached eddy simulation\nMB\nmodel-based\nMEPPO\nmodel ensemble proximal policy optimization\nMETRPO\nmodel ensemble trust region policy optimization\nMF\nmodel-free\nRL\nreinforcement learning\nTable C3 Abbreviations used throughout the article.\nAppendix C\nList of abbreviations\nReferences\n[1] IPCC: Climate Change 2022: Impacts, Adaptation and Vulnerability. Summary\nfor Policymakers, pp. 3–33. Cambridge University Press, Cambridge, UK and\nNew York, USA (2022)\n[2] Seifert, A., Shtendel, T., Dolgopyat, D.: From lab to full scale active flow control\ndrag reduction: How to bridge the gap? Journal of Wind Engineering and Indus-\ntrial Aerodynamics 147, 262–272 (2015) https://doi.org/10.1016/j.jweia.2015.09.\n012\n[3] Rohrer, T., Frison, L., Kaupenjohann, L., Scharf, K., Hergenr¨other, E.: Deep\nreinforcement learning for heat pump control. In: Arai, K. (ed.) Intelligent\nComputing, pp. 459–471. Springer, Cham (2023). https://doi.org/10.1007/\n978-3-031-37717-4 29\n[4] Esche, E., Repke, J.-U.: Dynamic process operation under demand response – a\nreview of methods and tools. Chemie Ingenieur Technik 92(12), 1898–1909 (2020)\nhttps://doi.org/10.1002/cite.202000091\n[5] Hucho, W., Sovran, G.: Aerodynamics of road vehicles. Annual Review of Fluid\nMechanics 25(1), 485–537 (1993) https://doi.org/10.1146/annurev.fl.25.010193.\n002413\n[6] Choi,\nH.,\nLee,\nJ.,\nPark,\nH.:\nAerodynamics\nof\nheavy\nvehicles.\nAnnual\nReview of Fluid Mechanics 46(1), 441–468 (2014) https://doi.org/10.1146/\nannurev-fluid-011212-140616\n[7] Viquerat, J., Meliga, P., Larcher, A., Hachem, E.: A review on deep reinforcement\nlearning for fluid mechanics: An update. Physics of Fluids 34(11), 111301 (2022)\nhttps://doi.org/10.1063/5.0128446\n[8] Paris, R., Beneddine, S., Dandois, J.: Robust flow control and optimal sensor\nplacement using deep reinforcement learning. Journal of Fluid Mechanics 913, 25\n(2021) https://doi.org/10.1017/jfm.2020.1170\n25\n[9] Krogmann, T.: Optimal sensor placement for active flow control with deep\nreinforcement learning (2023). https://doi.org/10.5281/zenodo.7636959\n[10] Paris, R., Beneddine, S., Dandois, J.: Reinforcement-learning-based actuator\nselection method for active flow control. Journal of Fluid Mechanics 955, 8 (2023)\nhttps://doi.org/10.1017/jfm.2022.1043\n[11] Ashton, N., West, A., Lardeau, S., Revell, A.: Assessment of rans and des methods\nfor realistic automotive models. Computers and Fluids 128, 1–15 (2016) https:\n//doi.org/10.1016/j.compfluid.2016.01.008\n[12] Belus, V., Rabault, J., Viquerat, J., Che, Z., Hachem, E., Reglade, U.: Exploiting\nlocality and translational invariance to design effective deep reinforcement learn-\ning control of the 1-dimensional unstable falling liquid film. AIP Advances 9(12),\n125014 (2019) https://doi.org/10.1063/1.5132378\n[13] Vignon, C., Rabault, J., Vasanth, J., Alc´antara-´Avila, F., Mortensen, M.,\nVinuesa, R.: Effective control of two-dimensional Rayleigh–B´enard convection:\nInvariant multi-agent reinforcement learning is all you need. Physics of Fluids\n35(6), 065146 (2023) https://doi.org/10.1063/5.0153181\n[14] Dixit, A., Elsheikh, A.H.: Robust optimal well control using an adaptive multi-\ngrid reinforcement learning framework. Mathematical Geosciences 55(3), 345–375\n(2023) https://doi.org/10.1007/s11004-022-10033-x\n[15] Moerland, T.M., Broekens, J., Jonker, C.M.: Model-based reinforcement learning:\nA survey. CoRR abs/2006.16712 (2020) 2006.16712\n[16] Kurutach, T., Clavera, I., Duan, Y., Tamar, A., Abbeel, P.: Model-ensemble trust-\nregion policy optimization. CoRR abs/1802.10592 (2018) 1802.10592\n[17] Rabault, J., Kuchta, M., Jensen, A., R´eglade, U., Cerardi, N.: Artificial neural\nnetworks trained through deep reinforcement learning discover control strategies\nfor active flow control. Journal of Fluid Mechanics 865, 281–302 (2019) https:\n//doi.org/10.1017/jfm.2019.62\n[18] Tokarev, M., Palkin, E., Mullyadzhanov, R.: Deep reinforcement learning control\nof cylinder flow using rotary oscillations at low reynolds number. Energies 13(22),\n5920 (2020) https://doi.org/10.3390/en13225920\n[19] Holm, M.: Using reinforcement learning for active flow control. DUO Research\nArchive (2020). http://hdl.handle.net/10852/79212\n[20] Geise, J., Weiner, A.: Git repository accompanying the article. https://github.\ncom/JanisGeise/MB DRL for accelerated learning from CFD (2024)\n[21] Weiner, A., Geise, J.: Model-based deep reinforcement learning for accelerated\n26\nlearning from flow simulations. https://b2share.eudat.eu (2024). https://doi.org/\n10.23728/B2SHARE.85AB8F3F68724372B83BABBDACA85910\n[22] Sutton, R.S., Barto, A.G.: Reinforcement Learning, Second Edition: An Intro-\nduction. Adaptive Computation and Machine Learning series. MIT Press, ???\n(2018)\n[23] Schulman, J., Wolski, F., Dhariwal, P., Radford, A., Klimov, O.: Proximal policy\noptimization algorithms. CoRR abs/1707.06347 (2017) 1707.06347\n[24] Andrychowicz, M., Raichuk, A., Stanczyk, P.M., Orsini, M., Girgin, S., Marinier,\nR., Hussenot, L., Geist, M., Pietquin, O., Michalski, M., Gelly, S., Bachem, O.F.:\nWhat matters for on-policy deep actor-critic methods? a large-scale study. In:\nICLR (2021). https://openreview.net/pdf?id=nIAxjsniDzg\n[25] Morales, M.: Grokking Deep Reinforcement Learning. Manning Publications, ???\n(2020)\n[26] OpenFOAM version 2206 (2022). https://www.openfoam.com/news/main-news/\nopenfoam-v2206\n[27] drlFoam - deep reinforcement learning with OpenFOAM. https://github.com/\nOFDataCommittee/drlfoam (2022)\n[28] Sch¨afer, M., Turek, S., Durst, F., Krause, E., Rannacher, R.: In: Hirschel, E.H.\n(ed.) Benchmark Computations of Laminar Flow Around a Cylinder, pp. 547–566.\nVieweg+Teubner Verlag, Wiesbaden (1996)\n[29] Noack, B.R., Stankiewicz, W., Morzy´nski, M., Schmid, P.J.: Recursive dynamic\nmode decomposition of transient and post-transient wake flows. Journal of Fluid\nMechanics 809, 843–872 (2016) https://doi.org/10.1017/jfm.2016.678\n[30] Raibaudo, C., Zhong, P., Noack, B.R., Martinuzzi, R.J.: Machine learning strate-\ngies applied to the control of a fluidic pinball. Physics of Fluids 32(1), 015108\n(2020) https://doi.org/10.1063/1.5127202\n[31] Raff, E.: Inside Deep Learning: Math, Algorithms, Models. Manning, ??? (2022)\n[32] Proctor, J.L., Brunton, S.L., Kutz, J.N.: Dynamic mode decomposition with\ncontrol. SIAM Journal on Applied Dynamical Systems 15(1), 142–161 (2016)\nhttps://doi.org/10.1137/15M1013857\n[33] H¨am¨al¨ainen, P., Babadi, A., Ma, X., Lehtinen, J.: Ppo-cma: Proximal policy\noptimization with covariance matrix adaptation. In: 2020 IEEE 30th International\nWorkshop on Machine Learning for Signal Processing (MLSP), pp. 1–6 (2020).\nhttps://doi.org/10.1109/MLSP49062.2020.9231618\n27\n",
  "categories": [
    "physics.flu-dyn",
    "cs.CE",
    "cs.LG"
  ],
  "published": "2024-02-26",
  "updated": "2024-04-10"
}