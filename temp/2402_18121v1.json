{
  "id": "http://arxiv.org/abs/2402.18121v1",
  "title": "Saving the legacy of Hero Ibash: Evaluating Four Language Models for Aminoacian",
  "authors": [
    "Yunze Xiao",
    "Yiyang Pan"
  ],
  "abstract": "This study assesses four cutting-edge language models in the underexplored\nAminoacian language. Through evaluation, it scrutinizes their adaptability,\neffectiveness, and limitations in text generation, semantic coherence, and\ncontextual understanding. Uncovering insights into these models' performance in\na low-resourced language, this research pioneers pathways to bridge linguistic\ngaps. By offering benchmarks and understanding challenges, it lays groundwork\nfor future advancements in natural language processing, aiming to elevate the\napplicability of language models in similar linguistic landscapes, marking a\nsignificant step toward inclusivity and progress in language technology.",
  "text": "Saving the legacy of Hero Ibash: Evaluating Four Language Models for\nAminoacian\nYunze Xiao∗\nCarnegie Mellon University\nyunzex@andrew.cmu.edu\nYiyang Pan∗\nUniversity of Rochester\nypan36@u.rochester.edu\nAbstract\nThis study assesses four cutting-edge language\nmodels in the underexplored Aminoacian lan-\nguage. Through evaluation, it scrutinizes their\nadaptability, effectiveness, and limitations in\ntext generation, semantic coherence, and con-\ntextual understanding. Uncovering insights into\nthese models’ performance in a low-resourced\nlanguage, this research pioneers pathways to\nbridge linguistic gaps. By offering benchmarks\nand understanding challenges, it lays ground-\nwork for future advancements in natural lan-\nguage processing, aiming to elevate the appli-\ncability of language models in similar linguistic\nlandscapes, marking a significant step toward\ninclusivity and progress in language technol-\nogy.\n1\nIntroduction\nThe Aminoac people, a civilization that thrived\naround the Caspian Sea from the sixth century BCE,\nspoke the unique Aminoas language, setting it apart\nfrom more common languages. This language ex-\nhibits complexity akin to Chinese, featuring six\ncontent word classes (nouns, verbs, adjectives, nu-\nmerals, classifiers, pronouns), and six function\nword types (adverbs, prepositions, conjunctions,\ninterjections, auxiliary words, onomatopoeia). Its\nsentence structure follows the rare OVS pattern, po-\nsitioning the subject at the sentence’s end. The lan-\nguage’s tonal system comprises four primary tones\nin the standard dialect, and remnants of an ancient\nscript hint at potential ideographic origins. This\nlanguage was pivotal in shaping the ancient civi-\nlization’s identity and cultural narrative, preserv-\ning their heritage despite transitioning to the Latin\nscript in modern writing systems(Bilibili, 2024a).\nExploring the representation of Aminoac lan-\nguages across Llama2(Touvron et al., 2023), Chat-\nGPT(OpenAI et al., 2023), Mistral(Jiang et al.,\n2023), and Ernie-bot is crucial for understanding\n∗Equal contribution\ntheir linguistic nuances and cultural significance.\nAssessing these languages’ availability on various\nmodels entails testing their performance in machine\ntranslation to Chinese, question answering tasks,\nand entailment recognition. Such evaluation en-\ndeavors to uncover how effectively these models\ncapture the complexity of Aminoac languages, en-\nsuring a better understanding of their syntax, se-\nmantics, and contextual comprehension.\nBy scrutinizing these models’ capabilities in\ntranslating Aminoac languages into Chinese, we\naim to gauge their proficiency in capturing the\nessence and subtleties of these languages when\nreaching a broader linguistic audience. Evaluating\nquestion answering tasks helps gauge the models’\nability to comprehend and respond accurately to\nqueries posed in Aminoac languages. Furthermore,\nexamining entailment tasks enables us to under-\nstand how well these models can infer logical con-\nnections between sentences in Aminoac languages,\na crucial aspect for accurate language understand-\ning and representation.\nThis investigation intends to identify the\nstrengths and limitations of each model in repre-\nsenting Aminoac languages, paving the way for\nimprovements. Enhancing the representation of\nunderrepresented languages like Aminoac within\nlanguage models will not only enrich their digital\npresence but also contribute significantly to foster-\ning linguistic diversity and cultural inclusiveness\nin language technologies.\n2\nBackground\n2.1\nAminoacian and Their Languages\nThe Aminoac people, an ancient civilization origi-\nnating around the 6th century BC along the shores\nof the Caspian Sea, represent a cultural tapestry wo-\nven with unique linguistic characteristics.(Bilibili,\n2024a) Their language, Aminoas, stands out amidst\nmore common languages globally, displaying in-\narXiv:2402.18121v1  [cs.CL]  28 Feb 2024\nFigure 1: The Amnioac Empire Flag\ntriguing idiosyncrasies in its grammar, syntax, and\nphonetics. Aminoas, akin to isolated languages, ex-\nhibits a rich variety of word classes, encompassing\nnouns, verbs, adjectives, numerals, pronouns, and\nan array of functional words(Bilibili, 2024b). In-\nterestingly, the language lacks grammatical gender\nor noun inflections based on case or number, dis-\ntinguishing it from many others. Additionally, the\nstructure of Aminoas sentences, unlike the more\ntypical SVO or SOV configurations, adopts the rare\nOVS arrangement, placing the subject at the sen-\ntence’s conclusion—a linguistic feature that sets it\napart from more widely known languages. More-\nover, Aminoas speech is melodic, characterized\nby distinct tonal patterns, comprising four primary\ntones in its standard form. This language’s phonetic\npeculiarity, commencing predominantly with vow-\nels and concluding with consonants, contributes to\nits soft and euphonious pronunciation, distinguish-\ning it from languages relying heavily on explosive\nconsonants.\nThe linguistic uniqueness of Aminoas is inter-\ntwined with the rich historical tapestry and mytho-\nlogical narrative of the Aminoac people. Their\norigin stories revolve around the Aminoas star, be-\nlieved to be the cradle and destiny of all Aminoac\nindividuals.\nLegend has it that their ancestors,\nled by the valorous warrior Ibash, arrived on\nEarth from the Aminoas star, overcoming celes-\ntial calamities and uniting the northern nomadic\nAmi and southern agrarian Noas tribes. This union\nbirthed the foundational Aminoac kingdom, char-\nacterized by economic prosperity and cultural de-\nvelopment. As the language evolved, so did the so-\nciety, maintaining a semi-agrarian, semi-nomadic\nlifestyle that became a hallmark of their identity.\nThe Aminoac language stands apart for its\nunique grammar, mirroring Mandarin Chinese with\na diverse array of word classes—six types for con-\ntent words such as nouns, verbs, adjectives, numer-\nals, classifiers, and pronouns, and another six for\nfunction words like adverbs, prepositions, conjunc-\ntions, interjections, particles, and onomatopoeic\nwords. Its isolating nature, lacking noun case, gen-\nder, or number distinctions and verbs void of person\nor tense, distinguishes Aminoac from conventional\nstructures. Moreover, its uncommon OVS sentence\nstructure places the subject at the sentence end,\nenriching expressions through modifiers and com-\nplements.\nAminoac’s tonal quality—four primary tones in\nstandard form—alongside its preference for vowel-\nstarting and nasal-ending words yield a melodi-\nous pronunciation. The language’s history hints\nat logographic scripts for recording due to homo-\nphonic challenges, yet societal hierarchies barred\ncommoners from learning these scripts, leading to\ntheir gradual disappearance. As a result, modern\nAminoac speakers adopted the Latin script during\nexternal interactions, leveraging its phonetics to\ncreate the contemporary Aminoac-Latin writing\nsystem, marking a shift from the original script.\n2.2\nLow-resourced Language\nLow-resourced languages, often marginalized or\nunderrepresented, reflect extensive linguistic diver-\nsity globally (Amano et al., 2014; Li et al., 2021;\nGorenflo et al., 2012; Gorter, 2013; Chu et al.,\n2012). Despite their cultural richness and histor-\nical significance, these languages confront chal-\nlenges in the digital sphere due to limited resources\nand inadequate technological support (Gorenflo\net al., 2012; Gorter, 2013; Protassova, 2021; imp,\n2021; Xiao, 2022). With roughly 7,000 languages\nspoken worldwide, many lack robust digital in-\nfrastructure, impeding their integration into ad-\nvancements in natural language processing and dig-\nital tools (Li et al., 2021; Gorenflo et al., 2012;\nGorter, 2013; Gören, 2017),such as role-play chat-\nbots(Wang et al., 2024).\nBeyond mere linguistic diversity, these lan-\nguages encapsulate unique cultural, historical, and\ntraditional knowledge vital for global heritage\npreservation (Kamwendo and Seretse, 2014; Bau-\nmann et al., 2018; Rey, 2017). Often serving as\nthe primary mode of communication for communi-\nties, they facilitate cultural expression, identity, and\nintergenerational knowledge transfer (Joshi et al.,\n2020; Rózsa et al., 2015). Neglecting these lan-\nguages exacerbates digital disparities, hindering\naccess to education, healthcare, and information\nfor speakers of these marginalized tongues (Amano\net al., 2014; Li et al., 2021; Gorenflo et al., 2012;\nChu et al., 2012). Recognizing and empowering\nthese languages within digital domains not only\nfosters inclusivity but also unlocks substantial po-\ntential for social, economic, and cultural empow-\nerment within these communities (Gorter, 2013;\nProtassova, 2021; Xiao and Alam, 2023).\n3\nMethodology\nWe sought to evaluate these models in crucial NLP\ntasks that could measure their availability in un-\nderstanding the Aminoac language. The tasks are\nmachine translation, Entailment and contextual un-\nderstanding.\n3.1\nMachine Translation\nFor evaluating the models’ proficiency in Aminoac\nlanguage understanding through machine transla-\ntion, we designed a rigorous experiment focusing\non several crucial aspects:\n3.1.1\nExperimental Design\n• Dataset Selection: We curated a diverse\ndataset consisting of Aminoac text paired with\ntranslations in awidely spoken language- Chi-\nnese.\n• Model Selection:\nSeveral state-of-the-art\nLLMs were chosen for comparison, including\nbut not limited to Transformer-based architec-\ntures and recurrent neural networks.\n• Evaluation Metrics: We employed standard\nmetrics like BLEU score, METEOR, and\nROUGE to quantitatively assess the quality\nof translations generated by each model.\n3.1.2\nExperimental Procedure\nThe experiment was conducted in several phases:\n1. Data Preprocessing: The Aminoac dataset\nwas preprocessed to remove noise, tokenize\nsentences, and align with their corresponding\ntranslations.\n2. Evaluation:\nThe LLMs were evaluated\nagainst a held-out test set using standard eval-\nuation metrics, comparing their translations\nagainst human-generated references.\n3.2\nExperimental Result\n• Model Performance: In the realm of ma-\nchine translation, our experiment presented a\nsignificant challenge to the leading language\nFigure 2: The Performance of Different Models in Trans-\nlation Experiment\nmodels Llama2, ChatGPT, Mistral, and Ernie-\nbot. Despite their advanced architectures and\nprevious successes in various NLP tasks, none\nof the models demonstrated proficiency in\ntranslating the Aminoac language. The evalua-\ntions, conducted under stringent standards us-\ning metrics like BLEU(Papineni et al., 2002),\nMETEOR(Banerjee and Lavie, 2005), and\nROUGE(Lin, 2004), yielded uniformly null\nresults across all models. This was an unex-\npected outcome, considering the models’ ca-\npability to process and generate complex lan-\nguage structures in other contexts. The failure\npoints towards a peculiar complexity and nu-\nance in the Aminoac language that these state-\nof-the-art models are currently unable to grasp.\nIt’s noteworthy that the Aminoac dataset, with\nits intricate syntax and unique semantic struc-\ntures, posed a challenge that went beyond the\nprocessing abilities of even the most sophisti-\ncated current LLMs. This result underlines a\ncritical gap in the field of machine translation,\nhighlighting the need for further research and\ndevelopment to build models that can under-\nstand and translate less commonly spoken and\nstructurally complex languages like Aminoac.\nAs the quest for truly universal language mod-\nels continues, the Aminoac translation task\nstands as a testament to the complexity and\ndiversity of human languages, and the substan-\ntial work still required in the field of NLP.\n4\nFuture Work\nThe unexpected results of this study open up nu-\nmerous avenues for future research in the field\nof Natural Language Processing, particularly in\nthe domain of language translation for less com-\nmonly spoken languages like Aminoac. Future\nwork should focus on enhancing the adaptability\nand learning mechanisms of Language Learning\nModels (LLMs) to better handle the intricacies\nof such unique languages. This includes devel-\noping more robust training datasets that encompass\nthe rich linguistic diversity and complex syntactic\nstructures of Aminoac. Additionally, exploring and\nintegrating novel neural network architectures or\nhybrid models that combine different approaches\nof language processing might offer breakthroughs\nin this area. Research should also delve into the\nrealm of unsupervised and semi-supervised learn-\ning techniques, which could enable LLMs to learn\nmore effectively from limited or unlabelled data, a\ncommon challenge in dealing with rare languages.\nCross-linguistic transfer learning, where models\ntrained on widely spoken languages are adapted to\nunderstand less prevalent ones, is another promis-\ning area. Moreover, there is a pressing need for\ninterdisciplinary collaboration, bringing together\nlinguists, computer scientists, and cultural experts,\nto ensure that the nuances of language and cul-\nture are adequately captured and represented. Ul-\ntimately, these efforts will not only advance the\nfield of machine translation but also contribute to\nthe preservation and understanding of linguistic\ndiversity across the globe.\nLimitation\nThis study is not without its limitations. Firstly, the\ntarget language under investigation suffers from\na scarcity of data, which may limit the generaliz-\nability of our findings.Secondly, our exploration\nwas confined to a select number of models. Fu-\nture work should consider a broader array of mod-\nels to provide a more comprehensive understand-\ning of the performance spectrum in translation\ntasks.Lastly,the importance of aminoac language\nlies in reading it reversely using Chinese.\nReferences\n2021. Impact of quality of healthcare services on con-\nsumer’s satisfaction at primary healthcare centers.\nMedico-Legal Update.\nT. Amano, B. Sandel, H. Eager, E. Bulteau, J. Svenning,\nB. Dalsgaard, C. Rahbek, R. Davies, and W. Suther-\nland. 2014. Global distribution and drivers of lan-\nguage extinction risk. Proceedings of the Royal Soci-\nety B Biological Sciences, 281:20141574.\nSatanjeev Banerjee and Alon Lavie. 2005. METEOR:\nAn automatic metric for MT evaluation with im-\nproved correlation with human judgments. In Pro-\nceedings of the ACL Workshop on Intrinsic and Ex-\ntrinsic Evaluation Measures for Machine Transla-\ntion and/or Summarization, pages 65–72, Ann Arbor,\nMichigan. Association for Computational Linguis-\ntics.\nA. Baumann, T. Matzinger, and N. Ritt. 2018. Linguis-\ntic and non-linguistic correlates in the evolution of\nphonotactic diversity.\nBilibili. 2024a. A comprehensive exploration of aminos:\nHistorical, religious, and cultural dimensions. Bili-\nbili Inc. Chapter or Paper within Proceedings. Ac-\ncessed on January 1, 2024.\nBilibili. 2024b. Exploring the heritage of aminos: Tra-\nditional folk songs. In Proceedings of International\nConference on Historical and Cultural Studies. Bili-\nbili Inc. Paper on Aminos’ traditional folk songs .\nAccessed on January 1, 2024.\nP. Chu, E. Józsa, A. Komlodi, and K. Hercegfi. 2012.\nAn exploratory study on search behavior in different\nlanguages.\nL. Gorenflo, S. Romaine, R. Mittermeier, and K. Walker-\nPainemilla. 2012. Co-occurrence of linguistic and\nbiological diversity in biodiversity hotspots and high\nbiodiversity wilderness areas. Proceedings of the\nNational Academy of Sciences, 109:8032–8037.\nD. Gorter. 2013. Linguistic landscapes in a multilingual\nworld. Annual Review of Applied Linguistics, 33:190–\n212.\nE. Gören. 2017. Consequences of linguistic distance for\neconomic growth. Oxford Bulletin of Economics and\nStatistics, 80:625–658.\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Men-\nsch, Chris Bamford, Devendra Singh Chaplot, Diego\nde las Casas, Florian Bressand, Gianna Lengyel, Guil-\nlaume Lample, Lucile Saulnier, Lélio Renard Lavaud,\nMarie-Anne Lachaux, Pierre Stock, Teven Le Scao,\nThibaut Lavril, Thomas Wang, Timothée Lacroix,\nand William El Sayed. 2023. Mistral 7b.\nP. Joshi, S. Santy, A. Budhiraja, K. Bali, and M. Choud-\nhury. 2020. The state and fate of linguistic diversity\nand inclusion in the nlp world.\nG. Kamwendo and T. Seretse. 2014. Linguistic and\nreligious diversity and inclusivity in the botswana\nschool curriculum. Journal of Asian and African\nStudies, 50:533–541.\nX. Li, J. Li, J. Yao, A. Black, and F. Metze. 2021. Phone\ndistribution estimation for low resource languages.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out, pages 74–81, Barcelona, Spain.\nAssociation for Computational Linguistics.\nOpenAI, :, Josh Achiam, Steven Adler, Sandhini Agar-\nwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Ale-\nman, Diogo Almeida, Janko Altenschmidt, Sam Alt-\nman, Shyamal Anadkat, Red Avila, Igor Babuschkin,\nSuchir Balaji, Valerie Balcom, Paul Baltescu, Haim-\ning Bao, Mo Bavarian, Jeff Belgum, Irwan Bello,\nJake Berdine, Gabriel Bernadett-Shapiro, Christo-\npher Berner, Lenny Bogdonoff, Oleg Boiko, Made-\nlaine Boyd, Anna-Luisa Brakman, Greg Brockman,\nTim Brooks, Miles Brundage, Kevin Button, Trevor\nCai, Rosie Campbell, Andrew Cann, Brittany Carey,\nChelsea Carlson, Rory Carmichael, Brooke Chan,\nChe Chang, Fotis Chantzis, Derek Chen, Sully Chen,\nRuby Chen, Jason Chen, Mark Chen, Ben Chess,\nChester Cho, Casey Chu, Hyung Won Chung, Dave\nCummings, Jeremiah Currier, Yunxing Dai, Cory\nDecareaux, Thomas Degry, Noah Deutsch, Damien\nDeville, Arka Dhar, David Dohan, Steve Dowl-\ning, Sheila Dunning, Adrien Ecoffet, Atty Eleti,\nTyna Eloundou, David Farhi, Liam Fedus, Niko\nFelix, Simón Posada Fishman, Juston Forte, Is-\nabella Fulford, Leo Gao, Elie Georges, Christian\nGibson, Vik Goel, Tarun Gogineni, Gabriel Goh,\nRapha Gontijo-Lopes, Jonathan Gordon, Morgan\nGrafstein, Scott Gray, Ryan Greene, Joshua Gross,\nShixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse\nHan, Jeff Harris, Yuchen He, Mike Heaton, Jo-\nhannes Heidecke, Chris Hesse, Alan Hickey, Wade\nHickey, Peter Hoeschele, Brandon Houghton, Kenny\nHsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu\nJain, Shawn Jain, Joanne Jang, Angela Jiang, Roger\nJiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie\nJonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser,\nAli Kamali, Ingmar Kanitscheider, Nitish Shirish\nKeskar, Tabarak Khan, Logan Kilpatrick, Jong Wook\nKim, Christina Kim, Yongjik Kim, Hendrik Kirch-\nner, Jamie Kiros, Matt Knight, Daniel Kokotajlo,\nŁukasz Kondraciuk, Andrew Kondrich, Aris Kon-\nstantinidis, Kyle Kosic, Gretchen Krueger, Vishal\nKuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan\nLeike, Jade Leung, Daniel Levy, Chak Ming Li,\nRachel Lim, Molly Lin, Stephanie Lin, Mateusz\nLitwin, Theresa Lopez, Ryan Lowe, Patricia Lue,\nAnna Makanju, Kim Malfacini, Sam Manning, Todor\nMarkov, Yaniv Markovski, Bianca Martin, Katie\nMayer, Andrew Mayne, Bob McGrew, Scott Mayer\nMcKinney, Christine McLeavey, Paul McMillan,\nJake McNeil, David Medina, Aalok Mehta, Jacob\nMenick, Luke Metz, Andrey Mishchenko, Pamela\nMishkin, Vinnie Monaco, Evan Morikawa, Daniel\nMossing, Tong Mu, Mira Murati, Oleg Murk, David\nMély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak,\nArvind Neelakantan, Richard Ngo, Hyeonwoo Noh,\nLong Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex\nPaino, Joe Palermo, Ashley Pantuliano, Giambat-\ntista Parascandolo, Joel Parish, Emy Parparita, Alex\nPassos, Mikhail Pavlov, Andrew Peng, Adam Perel-\nman, Filipe de Avila Belbute Peres, Michael Petrov,\nHenrique Ponde de Oliveira Pinto, Michael, Poko-\nrny, Michelle Pokrass, Vitchyr Pong, Tolly Pow-\nell, Alethea Power, Boris Power, Elizabeth Proehl,\nRaul Puri, Alec Radford, Jack Rae, Aditya Ramesh,\nCameron Raymond, Francis Real, Kendra Rimbach,\nCarl Ross, Bob Rotsted, Henri Roussez, Nick Ry-\nder, Mario Saltarelli, Ted Sanders, Shibani Santurkar,\nGirish Sastry, Heather Schmidt, David Schnurr, John\nSchulman, Daniel Selsam, Kyla Sheppard, Toki\nSherbakov, Jessica Shieh, Sarah Shoker, Pranav\nShyam, Szymon Sidor, Eric Sigler, Maddie Simens,\nJordan Sitkin, Katarina Slama, Ian Sohl, Benjamin\nSokolowsky, Yang Song, Natalie Staudacher, Fe-\nlipe Petroski Such, Natalie Summers, Ilya Sutskever,\nJie Tang, Nikolas Tezak, Madeleine Thompson, Phil\nTillet, Amin Tootoonchian, Elizabeth Tseng, Pre-\nston Tuggle, Nick Turley, Jerry Tworek, Juan Fe-\nlipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya,\nChelsea Voss, Carroll Wainwright, Justin Jay Wang,\nAlvin Wang, Ben Wang, Jonathan Ward, Jason Wei,\nCJ Weinmann, Akila Welihinda, Peter Welinder, Ji-\nayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner,\nClemens Winter, Samuel Wolrich, Hannah Wong,\nLauren Workman, Sherwin Wu, Jeff Wu, Michael\nWu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qim-\ning Yuan, Wojciech Zaremba, Rowan Zellers, Chong\nZhang, Marvin Zhang, Shengjia Zhao, Tianhao\nZheng, Juntang Zhuang, William Zhuk, and Barret\nZoph. 2023. Gpt-4 technical report.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 311–318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nE. Protassova. 2021. Interculturality in the modern rus-\nsian linguistic landscape. Philological Class, 26:52–\n67.\nC. Rey. 2017. Linguistic genocide or superdiversity:\nnew and old language diversities. Language and\nIntercultural Communication, 18:464–467.\nG. Rózsa, A. Komlodi, and P. Chu. 2015. Online search-\ning in english as a foreign language.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, Dan Bikel, Lukas Blecher, Cristian Canton\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu,\nJude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\nCynthia Gao, Vedanuj Goswami, Naman Goyal, An-\nthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\nIsabel Kloumann, Artem Korenev, Punit Singh Koura,\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-\nana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-\ntinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-\nbog, Yixin Nie, Andrew Poulton, Jeremy Reizen-\nstein, Rashi Rungta, Kalyan Saladi, Alan Schelten,\nRuan Silva, Eric Michael Smith, Ranjan Subrama-\nnian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-\nlor, Adina Williams, Jian Xiang Kuan, Puxin Xu,\nZheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,\nMelanie Kambadur, Sharan Narang, Aurelien Ro-\ndriguez, Robert Stojnic, Sergey Edunov, and Thomas\nScialom. 2023. Llama 2: Open foundation and fine-\ntuned chat models.\nXintao Wang, Yunze Xiao, Jen tse Huang, Siyu Yuan,\nRui Xu, Haoran Guo, Quan Tu, Yaying Fei, Ziang\nLeng, Wei Wang, Jiangjie Chen, Cheng Li, and\nYanghua Xiao. 2024. Incharacter: Evaluating per-\nsonality fidelity in role-playing agents through psy-\nchological interviews.\nYunze Xiao. 2022. A transformer-based attention flow\nmodel for intelligent question and answering chat-\nbot. In 2022 14th International Conference on Com-\nputer Research and Development (ICCRD), pages\n167–170.\nYunze Xiao and Firoj Alam. 2023. Nexus at ArAIEval\nshared task: Fine-tuning Arabic language models for\npropaganda and disinformation detection. In Pro-\nceedings of ArabicNLP 2023, pages 576–582, Singa-\npore (Hybrid). Association for Computational Lin-\nguistics.\n",
  "categories": [
    "cs.CL",
    "cs.AI"
  ],
  "published": "2024-02-28",
  "updated": "2024-02-28"
}