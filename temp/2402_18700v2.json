{
  "id": "http://arxiv.org/abs/2402.18700v2",
  "title": "Learning to Compress Prompt in Natural Language Formats",
  "authors": [
    "Yu-Neng Chuang",
    "Tianwei Xing",
    "Chia-Yuan Chang",
    "Zirui Liu",
    "Xun Chen",
    "Xia Hu"
  ],
  "abstract": "Large language models (LLMs) are great at processing multiple natural\nlanguage processing tasks, but their abilities are constrained by inferior\nperformance with long context, slow inference speed, and the high cost of\ncomputing the results. Deploying LLMs with precise and informative context\nhelps users process large-scale datasets more effectively and cost-efficiently.\nExisting works rely on compressing long prompt contexts into soft prompts.\nHowever, soft prompt compression encounters limitations in transferability\nacross different LLMs, especially API-based LLMs. To this end, this work aims\nto compress lengthy prompts in the form of natural language with LLM\ntransferability. This poses two challenges: (i) Natural Language (NL) prompts\nare incompatible with back-propagation, and (ii) NL prompts lack flexibility in\nimposing length constraints. In this work, we propose a Natural Language Prompt\nEncapsulation (Nano-Capsulator) framework compressing original prompts into NL\nformatted Capsule Prompt while maintaining the prompt utility and\ntransferability. Specifically, to tackle the first challenge, the\nNano-Capsulator is optimized by a reward function that interacts with the\nproposed semantics preserving loss. To address the second question, the\nNano-Capsulator is optimized by a reward function featuring length constraints.\nExperimental results demonstrate that the Capsule Prompt can reduce 81.4% of\nthe original length, decrease inference latency up to 4.5x, and save 80.1% of\nbudget overheads while providing transferability across diverse LLMs and\ndifferent datasets.",
  "text": "Learning to Compress Prompt in Natural Language Formats\nYu-Neng Chuang∗\nRice University\nynchuang@rice.edu\nTianwei Xing\nSamsung Research America\nt.xing@samsung.com\nChia-Yuan Chang\nTexas A&M University\ncychang@tamu.edu\nZirui Liu\nRice University\nzl105@rice.edu\nXun Chen\nSamsung Research America\nxun.chen@samsung.com\nXia Hu\nRice University\nxia.hu@rice.edu\nAbstract\nLarge language models (LLMs) are excel at\nprocessing multiple natural language process-\ning tasks, but their abilities are constrained by\ninferior performance with long context, slow\ninference speed, and the high cost of comput-\ning the results. Deploying LLMs with precise\nand informative context helps users process\nlarge-scale datasets more effectively and cost-\nefficiently. Existing works rely on compress-\ning long prompt contexts into soft prompts.\nHowever, soft prompt compression encoun-\nters limitations in transferability across differ-\nent LLMs, especially API-based LLMs. To\nthis end, this work aims to compress lengthy\nprompts in the form of natural language with\nLLM transferability.\nThis poses two chal-\nlenges: (i) Natural Language (NL) prompts\nare incompatible with back-propagation, and\n(ii) NL prompts lack flexibility in imposing\nlength constraints.\nIn this work, we pro-\npose a Natural Language Prompt Encapsula-\ntion (Nano-Capsulator) framework compress-\ning original prompts into NL formatted Cap-\nsule Prompt while maintaining the prompt util-\nity and transferability. Specifically, to tackle\nthe first challenge, the Nano-Capsulator is opti-\nmized by a reward function that interacts with\nthe proposed semantics preserving loss. To ad-\ndress the second question, Nano-Capsulator is\noptimized by a reward function featuring length\nconstraints. Experimental results demonstrate\nthat the Capsule Prompt can reduce 81.4% of\nthe original length, decrease inference latency\nup to 4.5×, and save 80.1% of budget over-\nheads while providing transferability across di-\nverse LLMs and different datasets.\n1\nIntroduction\nLarge Language Models (LLMs) have demon-\nstrated substantial proficiency across a variety of\nnatural language processing tasks. Despite their\nsignificant potential and broad adoption, LLMs are\n∗Work done as an intern at Samsung Research America.\n \n−459.67 ℉\n \n−459.67 ℉\nPrompt:\nPrompt:\nPrompt \n  Compression\nQ: What is Absolute Zero in ℉ ?\nQ: What is Absolute Zero in ℉ ?\nUtility\nPreservation\nLLMs\nLLMs\nFigure 1: An example of successful prompt compres-\nsion with NL formats. The compressed NL-formatted\nprompt (green) aims to obtain a shorter length and main-\ntain transferability and utility of the long prompt (red).\nfundamentally limited by the long context length\ninput, which impairs their capability to understand\nlengthy documents and affects their efficiency dur-\ning inference (Touvron et al., 2023; Brown et al.,\n2020; Yang et al., 2023; Jin et al., 2024). As the\ndemand for processing millions of tokens increases,\nit is progressively crucial to deploy LLMs that are\nadept at comprehending extended lengths while\nminimizing budgetary requirements.\nTo help LLMs better process long context knowl-\nedge, recent advancements have focused on com-\npressing long prompt contexts into concise soft\nprompts. This approach effectively transforms the\noriginal extensive prompt into a manageable se-\nries of short-length soft prompt tokens. Gener-\nally, compression-oriented soft prompts are learned\nwith the guarantee of semantics through self-\ninformation (Chevalier et al., 2023), instruction\nfinetuning (Ge et al., 2023; Ren et al., 2023), and\nthe performance alignment via knowledge distilla-\ntion (Wingate et al., 2022; Mu et al., 2023). How-\never, with the rapid evolution and the growth of\nAPI-based accessibility of LLMs, soft prompts\npose significant limitations in terms of transfer-\nability across different LLMs, implying that well-\ntrained soft prompts can only be effectively adapted\nto the specific LLMs for which they were designed.\narXiv:2402.18700v2  [cs.CL]  2 Apr 2024\nThis situation creates a critical need to achieve\nboth transferability and utility effectively. A natu-\nral question is raised: Can we compress lengthy\nprompts in a natural language format, yet still\npreserve utility and ensure transferability among\nvarious LLMs?\nCompressing extended prompts into a shorter,\nnatural language (NL) format continues to be a\nchallenging and unresolved issue. As depicted in\nFigure 1, effective prompt compression entails pre-\nserving essential semantic information in a con-\nstrained length with successful performance preser-\nvation. However, unlike soft prompts, which can\nbe directly optimized with a fixed length, compress-\ning prompts into shorter NL prompts is challenging\nfor several reasons: (i) NL prompts are incompati-\nble with back-propagation, as the gradient cannot\nbackward to a discrete raw text; (ii) NL prompts\nlack flexibility on imposing strict length constraints,\nwhere overly stringent limitations on generation\nlength may lead to performance degradation. Thus,\nit is nontrivial to compress lengthy prompts into\nshorter NL ones.\nTo tackle the aforementioned problems, we pro-\npose a Natural Language Prompt Encapsulation\n(Nano-Capsulator) framework to effectively com-\npress original prompts into a Capsule Prompt with\nthe aid of a rewarding technique. Our proposed\nNano-Capsulator aims to encapsulate long prompts\ninto shorter ones under specific generation length\nconstraints, maintaining performance through an\nexplicit semantic preservation objective with re-\nward scores. Specifically, we compress our prompt\nby employing a semantics-preserving summariza-\ntion, and then monitor the optimization process\nusing reward scores that reflect the remaining infor-\nmation relevant to the downstream task. Notably,\nshorter Capsule Prompts, characterized by their\nconcise NL formatting, preserve transferability and\nutility across diverse LLMs. Capsule Prompt en-\nables two advantages: the preservation of prompt\ntransferability and utility across different LLMs,\nand the reduction of inference time and budget\noverheads. Additionally, Nano-Capsulator can be\ndirectly applied to unseen datasets without any fur-\nther training, provided these new datasets encom-\npass downstream tasks with similar domains.\nTo assess the effectiveness of Nano-Capsulator,\nwe conduct compression experiments with two dif-\nferent prompt types: few-shot demonstration chain-\nof-thoughts (CoT) and passage prompts of reading\ncomprehension (i.e., content passages). Capsule\nPrompt exhibits strong transferability across dif-\nferent LLMs and similar but unseen downstream\ndatasets. This enables effective adaptation without\nretraining the prompt compressor.\nOur main contributions are concluded as follows:\nFirst, we introduce and formalize Nano-Capsulator\nframework, which can effectively generate high-\nquality Capsule Prompt with prominent transfer-\nability across multiple LLMs and unseen datasets\nwith similar downstream tasks. Second, we ef-\nfectively reduce the original prompts to 81.4%\nof their initial length and transform them into\nNL-formatted Capsule Prompt, which retains the\nprompt’s transferability and utility across various\nLLMs. Our compression mechanism significantly\ndecreases up to 4.5× of the inference latency and\nsaves 80.1% of the budget overheads for input se-\nquences. Third, experimental results demonstrate\nthat Capsule Prompt can efficiently perform across\ndiverse LLMs, which is applicable to both few-shot\ndemonstration CoT and input contextual prompts.\n2\nRelated Work\n2.1\nSoft Prompt Compression\nIn the realm of prompt compression for LLMs,\nmost of the existing work aims to compress the\nprompt into soft prompts. Soft prompts are train-\nable vectors that are optimized in conjunction with\na designated LLM, which embeds the original con-\ntent information of the long hard prompts into low-\ndimensional vectors.\nThe first line of work (Wingate et al., 2022) lever-\nages the knowledge distillation object to extract the\ninformation from hard prompts to soft prompts.\nThe compressed soft prompts are expected to cap-\nture high-level concepts and preserve the fluency\nfrom the original hard prompts. The second line of\nwork (Chevalier et al., 2023) employs the summa-\nrization capabilities of LLMs to condense lengthy\nand complex prompts into soft prompts. The pro-\ncess involves dividing the input prompts into mul-\ntiple segments and sequentially compressing the\ninformation from the original prompt into smaller\nsegments of soft prompts, where they assemble\nfrom these individual fragments of soft prompts to\nform the final soft prompts. Another work, Gist\nToken (Mu et al., 2023), condenses instruction\nprompts into customized prefix soft prompts by\ntraining a virtual soft prompt predictor.\nNevertheless, the transferability of soft prompt-\nbased compression across diverse LLMs is con-\nstrained, necessitating the re-training of soft\nprompts with each change in the specified LLMs.\nThis means that the soft prompts generated are\nspecifically tailored to work only with that particu-\nlar LLM, which falls short in maintaining transfer-\nability across different LLMs, especially applied\non API-based LLMs, such as Claude2 (Anthropic,\n2023) and PaLM (Chowdhery et al., 2023).\n2.2\nContext Distillation for Compression\nBesides directly compressing hard prompts into\nsoft prompt vectors, recent advancement (Li et al.,\n2023; Jiang et al., 2023) involves computing the\nself-information scores or perplexity of the given\ninput context prompt to shorten the prompt length.\nThis process includes filtering out words with\nlower scores from the input prompt, resulting in\na more concise input during the inference stage.\nThe primary distinction between our work and\nthese recent studies is that they operate prompt\ncompression without considering any information\nfrom downstream tasks, resulting in inferior per-\nformance while directly applying to downstream\ntasks or transferring between similar but unseen\ndownstream datasets.\n3\nLong Prompt Encapsulation\nWe systematically introduce the Nano-Capsulator\nframework in this section. Figure 2 illustrates the\noverall pipeline of Nano-Capsulator. In particu-\nlar, our pipeline initially compresses text into NL-\nformatted Capsule Prompt and concurrently opti-\nmizes their utility using the proposed rewarding\nmethod. The design of the NL-formatted compres-\nsion aims to maintain prompt utility and preserve\ntransferability among different LLMs.\n3.1\nPrompt Encapsulation\nThe primary aim of Nano-Capsulator is to preserve\nthe inherent utility of the original pre-compressed\ntext and ensure the compressed prompt closely\nreaches the designated length constraint. Specif-\nically, the prompt encapsulation mechanism in-\ncludes two components to effectively generate\nCapsule Prompt: (1) NL-formatted prompt com-\npression and (2) prompt utility preservation. The\nlearning of Nano-Capsulator involves integrating\ntwo components and optimizing them concurrently,\nthereby assuring the compressed prompts are suffi-\ncient to preserve their inherent utility.\nAlgorithm 1 Algorithm of Nano-Capsulator\nInput: Original Long Prompt K, Compression Instructions\nTRep and TSumm, and pre-trained frozen LLMs G∗(·), Sam-\npled set of downstream task questions Q.\nOutput: NL-formatted Capsule Prompts C.\n1: Initialize F(· | θC) and G∗(·) with pre-trained weights\n2: while not convergence do\n3:\nGenerate C from F(K | TRep, TSumm, θC)\n4:\nRandomly sample a set of questions Q\n5:\nReceive reward scores from Rcap(G∗(·), Q, C, K)\n6:\nF(· | θC) ←minimizing with LNano(·)\n7: end while\n3.1.1\nNL-formatted Prompt Compression\nWe adopt an unsupervised training approach fea-\nturing semantic preservation loss, motivating the\nmodel to compress contexts while retaining simi-\nlar semantic content. In this work, we shorten the\nlong prompts by summarizing their context and\napplying our proposed semantics loss LComp to en-\nsure maximal preservation of semantic meaning.\nHere, semantics refers to the logical thinking pro-\ncess from the few-shot demonstration CoT and the\nbeneficial content from context passages.\nGiven the original prompt K = {k1, · · · kn} con-\nsisted of n tokens to the Capsule Prompt C =\n{c1, · · · cm} with m tokens, where n ≫m. Our\nsemantics loss aims to ensure the maximal seman-\ntics preservation by measuring the similarity be-\ntween the hidden state embedding of C and K of\nNano-Capsulator F(· | θC). To obtain the hidden\nstate embedding of K, we instruct F(· | θC) to\nreplicate the input prompt K, which aids in better\npreserving and embedding the semantic meanings\nof K. Specifically, d-dimensional hidden state em-\nbedding of K and C can be generated by eK ∼\nPF(K | θC, TRep) and eC ∼PF(C | θC; TSumm),\nwhere TRep and TSumm denote a replicating instruc-\ntion and a summarizing instruction, respectively.\nWith the aid of TRep, we compel F(· | θC) to repli-\ncate K under the model parameter θC, ensuring\nthat eK ∈Rd accurately represents the embedding\nof K. Following the criterion, F(· | θC) essentially\nminimizes the semantics loss as follows:\nLComp = EC\n\u0002\nDdist(eK || eC)\n\u0003\n(1)\nwhere Ddist(· || ·) can be any suitable distance mea-\nsurement in metric space. In this work, we leverage\nmean square error as the distance function to mea-\nsure the similarity between eK and eC.\n3.1.2\nPrompt Utility Preservation\nTo impose a constraint on the generated length\nwhile preserving utility, we establish a reward func-\nCompression LLM\n(Nano-Capsulator)\nLong Prompt\nCapsulate Prompt\nSemantic Loss\nEq. (1)\nFrozen\nDownstream LLM \nQ: 22 + 4 =? \nQuestions\nLong Prompt\nCapsulate Prompt\n26\n26\nAnswer_Cap Answer_Long\nReward Score Eq. (2)\nFigure 2: The illustration of Nano-Capsulator training framework. Nano-Capsulator compress the long prompt with\nthe action of semantic (Equation 1) and utility preservation (Equation 2). Questions are sampled from the training\nset to develop the reward scores for utility preservation.\ntion Rcap(·) featuring a strict cut-off mechanism\nΦ(·) for restricting the generated length of Capsule\nPrompt. The high-level idea of the reward func-\ntion is to calculate the score changes of the down-\nstream task question based on leveraging the origi-\nnal prompt K and the Capsule Prompt C. Notably,\nthe reward function employs a truncation strategy\nto limit the C to a predetermined length before\nproceeding to compute the scores using the reward\nfunction. In this manner, the Capsule Prompt that\nsurpasses the specified length threshold could be\nassigned a lower reward score as a result of the\ncut-off mechanism.\nFormally, given K and C along with arbitrary\npre-trained frozen LLMs G∗(·) and a sampled set of\ndownstream task questions Q, the reward function\nRcap(·) can be defined as follows:\nRcap = EQ\n\u0002\nI{ G\n\u0000Φ(Ci) ⊕Qi\n\u0001\n|| G\n\u0000Ki ⊕Qi\n\u0001\n}\n\u0003\n(2)\nwhere I(· || ·) denotes an arbitrary reward metric\nfor yielding the reward score., and ⊕represents\nconcatenation of prompts and questions. In this\nstudy, we calculate the reward scores using the\nmean square error between the hidden state embed-\nding from G∗(·). It’s noteworthy that I(· || ·) can\nbe replaced by other metrics, such as accuracy and\nGPT4Eval Scores (Liu et al., 2023), facilitating its\npotential application to API-based LLMs.\n3.1.3\nCompression with Reward\nUpon receiving the reward scores from Rcap as per\nEquation 2, we synchronize these scores with the\nsemantic loss LComp to maintain utility. Formally,\nthe ultimate objective function of Nano-Capsulator\ncan be indicated as:\nLNano = LComp(·|θC) ∗Rcap(·|θ∗)\n(3)\nwhere θ∗denotes the frozen model parameters of\nG∗(·) and θC is the trainable parameters of Nano-\nCapsulator. The fundamental principle of LNano(·)\nis to impose penalties when shorter versions of\nCapsule Prompt exhibit inferior performance. This\nimplies that if a Capsule Prompt receives a low\nreward score from Equation 2, its semantic loss\nwill be composed by a high penalty value, resulting\nin a substantial semantic loss value as punishment\nduring the training phase of Nano-Capsulator.\n3.2\nAlgorithm of Nano-Capsulator\nThe framework of Nano-Capsulator is detailed in\nAlgorithm 1. Nano-Capsulator adheres to Equa-\ntion 1 for the preservation of semantic meaning and\nintegrates the rewarding function from Equation 2\nto maintain the utility of compressed NL-formatted\nprompts. The two elements are then aligned, as\ndepicted in Equation 3, and optimized simultane-\nously with the goal of obtaining compressed NL-\nformatted prompts of high utility. In the inference\nphase, Nano-Capsulator is solely required to pro-\nduce the compressed version of Capsule Prompt\nfrom the provided long input prompt.\n4\nExperiments\nIn this section, we conduct experiments to evaluate\nthe performance of Nano-Capsulator, aiming to\nanswer the following three research questions:\n• RQ1: How does Nano-Capsulator perform in\nterms of the efficacy and transferability among\ndifferent LLMs and datasets?\n• RQ2: How do the two components of Nano-\nCapsulator contribute to the compression perfor-\nmance for utility preservation?\n• RQ3: What are the inference latency and impact\nfactors of Capsule Prompt?\n4.1\nDataset\nWe conduct compression experiments with two dif-\nferent prompt types: few-shot CoT and passage\nprompts of reading comprehension. The details of\nthe datasets are provided as follows:\nFew-shot CoT Dataset. We choose two reasoning\ndatasets to evaluate the proposed framework.\n• CommonsenseQA (Talmor et al., 2019): The\nCommonsenseQA (CSQA) dataset is a publicly\naccessible collection of multiple choice questions\nwith 1221 samples for the commonsense reason-\ning task. CSQA presents questions characterized\nby intricate semantics, typically demanding rea-\nsoning grounded in pre-existing knowledge.\n• GSM8K (Cobbe et al., 2021): The GSM8K is\na dataset containing 1319 samples of graduate\nschool math questions. Each question is collected\nfrom the Math World Problem Repository (Roy\nand Roth, 2015) with a numerical answer.\nReading Comprehension Dataset.\n• MultiRC (Khashabi et al., 2018; DeYoung\net al.): MultiRC (Multi-Sentence Reading Com-\nprehension) comprises a collection of brief para-\ngraphs paired with multi-sentence questions,\nwhere the answers can be derived from the para-\ngraph’s content. The dataset obtains 24029 sam-\nples for training, 3214 samples for validating,\nand 4848 samples for testing.\n• TriviaQA LongBench (Joshi et al., 2017): Triv-\niaQA LongBench (TriviaQA-Long) is a reading\ncomprehension dataset featuring 300 question-\nanswer-evidence triples collected from the Long-\nbench dataset. It includes question-answer pairs\ncreated by trivia enthusiasts, along with indepen-\ndently sourced evidence documents, offering ro-\nbust supervision for responding to the questions.\n4.2\nExperiment Settings\nIn this part, we introduce the experimental set-\ntings and metrics for evaluating Nano-Capsulator.\nTwo distinct types of transferability evaluations are\ntaken into account. To verify the model transfer-\nability, the compression models are trained on one\ndownstream LLM, and tested on different down-\nstream LLMs. The evaluation is performed on the\nsame dataset, with a division of 70% allocated for\ntraining and validation and 30% designated as the\ntesting set. To assess data transferability, we train\nthe compression models using one seen dataset and\nthen test them on unseen datasets that the mod-\nels have not previously encountered with the same\ndownstream tasks. The considered compression\nsettings and implementation details are shown as\nfollows. Two types of prompt compression tasks\nare focused on.\nFew-shot CoT Compression Task. For the few-\nshot CoT compression task, we randomly compile\nseven examples from the CSQA dataset and eight\nfrom the GSM8K dataset following (Wei et al.,\n2022), all selected from their respective training\nsets, to construct the few-shot CoT. During the\ntraining phase, a total of 1,000 CoT samples are\nthen gathered to serve as the training data for Nano-\nCapsulator. During the inference stage, we aim\nto compress the manual few-shot CoT proposed\nin (Wei et al., 2022), where the demonstrations in\nmanual CoT are eliminated from any training set.\nThe primary evaluation metric used in both CSQA\nand GSM8K datasets is accuracy, implying that the\nmodel scores only when it provides answers that\nexactly match the expected responses.\nReading Comprehension Compression Task. For\nthe reading compression task, we aim to compress\nthe reading paragraphs from the question-answer\ntriplets. Due to the limitation of GPU memory, we\neliminate the paragraphs that exceed 2k tokens in\nTriviaQA from the LongBench dataset, resulting\nin an average length of 900 tokens, while MultiRC\nremains to utilize the all paragraphs in the dataset.\nThroughout the training phase, we select 2,000\nquestion-answer-paragraph triplets in the MultiRC\ndataset to serve as training data and leverage all\ntraining data in the TriviaQA-Long dataset. Our\nframework is evaluated on the entire set of testing\ndata, using accuracy as the metric of assessment.\nImplementation Details. In primary experiments,\nwe utilize Vicuna-7B (Chiang et al., 2023) as\nthe initial compression model F(· | θC) in Nano-\nCapsulator. The pre-trained LLMs G∗(·) is given\nas Vicuna-7B with frozen weights. We train Nano-\nCapsulator using Vicuna-7B and then assess the\ngenerated Capsule Prompt with various LLMs\nother than Vicuna-7B, in order to evaluate its trans-\nferability. To reduce memory consumption dur-\ning training, we utilize LoRA1 and train the Nano-\nCapsulator using two NVIDIA A40 GPUs of 48GB\nmemory. We employ the Adam optimizer for the\nfine-tuning process, with a learning rate set at 5e-6\n1PEFT: https://github.com/huggingface/peft\nCSQA\n|\nGSM8K\n|\nMultiRC\n|\nTriviaQA-Long\nManual\nZero-shot\nOurs\nManual\nZero-shot\nOurs\nOriginal\nOurs\nOriginal\nOurs\nVicuna-13B\n60.4\n44.6\n58.8\n34.4\n25.3\n31.9\n57.3\n57.1\n86.0\n88.8\nPaLM\n73.7\n67.5\n75.5\n62.8\n56.8\n59.5\n72.7\n72.2\n78.9\n78.8\nClaude2\n76.6\n69.4\n74.6\n85.6\n52.7\n84.9\n59.4\n58.2\n95.0\n92.3\nLength (# of Token)\n831\n–\n154\n751\n–\n231\n378.39\n95.66\n915.7\n422.6\nCompress Rate (%)\n–\n–\n81.4%\n–\n–\n69.3%\n–\n74.71%\n–\n53.84%\nTable 1: Evaluation of Nano-Capsulator among different LLMs. The results show that Nano-Capsulator compress\nup to 81.4% of the original long prompt and save up to 80.1% of the expense on requesting for LLM API calls.\nClaude2 (Anthropic, 2023)\nCost($)\nOriginal\nCapsule Prompt\nSaved\nCSQA\n15.03\n3.30\n-77.9%\nGSM8K\n5.22\n1.88\n-63.9%\nMultiRC\n45.91\n13.01\n-71.6%\nTrivaQA-Long\n2.14\n0.42\n-80.1%\nTable 2: API cost comparison of Capsule Prompt and\noriginal prompt, where Capsule Prompt save up to\n80.1% of the original cost.\nunder the gradient clipping of 0.8, depending on the\ndatasets. The instructions that leverage for prompt\nencapsulation TRep and TSumm are listed in Table 6\nfrom Appendix F.\n4.3\nMain Results (RQ1)\nModel Transferability. To assess the effective-\nness and transferability, we compress the origi-\nnal input prompts into the Capsule Prompt by\nNano-Capsulator. We then evaluate the transfer-\nability and utility of these compressed prompts\nacross three different LLMs not included in the\npre-training of Nano-Capsulator: Vicuna-13B (Chi-\nang et al., 2023), PaLM (Chowdhery et al., 2023),\nand Claude2 (Anthropic, 2023). The main findings\nare presented in Table 1. In the table, \"Manual\"\nrefers to the manually created few-shot CoT pro-\nposed by (Wei et al., 2022), \"Zero-shot\" denotes\nthe zero-shot CoT followed (Kojima et al., 2022),\nand \"Original\" indicates the original paragraphs\nused in the reading comprehension tasks.\nIn the primary experiment, we establish a com-\npression constraint limiting to a maximum of 150\ntokens for the CSQA and MultiRC datasets; and a\nmaximum of 350 and 500 tokens for the GSM8K\nand TriviaQA-Long dataset, where 150 tokens are\nnot sufficient for preserving the logic of GSM8K\nand TriviaQA-Long dataset. We observe that the\nNano-Capsulator obtains up to 81.4% of the com-\npression rate and saves up to 80.1% of the Claude2\nAPI cost compared to the original input prompts,\nas displayed in Table 2. The cost of PaLM API\nVicuna-13B\nClaude2\n0.2\n0.4\n0.6\n0.8\nAccuracy\nOriginal Prompt\nw/ Training\nw/o Training\nFigure 3:\nEvaluation of transferability on Nano-\nCapsulator across unseen datasets.\ncan be found in Appendix C. For utility preserva-\ntion, Nano-Capsulator retains the original perfor-\nmance on the CSQA, GSM8k, and TriviaQA-Long\ndatasets mostly among three LLMs. Remarkably,\nNano-Capsulator maintains almost identical per-\nformance to that achieved with non-compressed\nprompts in MultiRC datasets. The significant com-\npression rate can advantageously impact the LLMs\nby allowing for a higher tolerance in batch infer-\nence, accompanied by reduced latency and cost.\nDataset Transferability. We previously assessed\nthe effectiveness of Nano-Capsulator within the\nsame datasets, where the testing set was derived\nfrom the same training domain. In this section, we\ninvestigate the transferability of Nano-Capsulator\nacross unseen datasets (i.e., not in training data)\nwith the same downstream tasks. We train Nano-\nCapsulator on the MultiRC dataset (seen dataset)\nand test on BoolQ (Clark et al., 2019) (unseen\ndataset) without any further training, where BoolQ\nis also a reading comprehension dataset, under\nVicuna-13B and Claude2. The results are demon-\nstrated in Figure 3. We see a competitive perfor-\nmance with only a slight accuracy drop compared\nto the training version of Capsule Prompt. While\nCapsule Prompt yields better performance with\ntraining, the results indicate that Nano-Capsulator\npossesses a great property of data transferability.\n4.4\nContributions of Utility Preservation\n(RQ2)\nIn this section, we explore the effectiveness of\ncomponents from Nano-Capsulator. Specifically,\nVicuna-13B\nClaude2\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nAccuracy\nZero-shot Summ\nCapsule Prompt\nVicuna-13B\nClaude2\n0.400\n0.425\n0.450\n0.475\n0.500\n0.525\n0.550\n0.575\n0.600\nAccuracy\nZero-shot Summ\nCapsule Prompt\nFigure 4: Comparison results of Capsule Prompt and\nZero-shot Summarization on GSM8K dataset (left) and\nMultiRC dataset (right).\nCSQA\nGSM8K\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nAccuracy\nGPT-3.5-Turbo\nCapsule Prompt\nVicuna-13B\nClaude2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nOriginal Prompt\nw/ Reward\nw/o Reward\nFigure 5: Ablation studies of comparison with Capsule\nPrompt and GPT-35-Turbo Summarization on CSQA\ndataset and GSM8K dataset (left); and of the contribu-\ntion of Reward Function from Equation 2 (right).\nwe conduct ablation studies from two perspectives.\nFirst, we evaluate the efficacy of semantic preserva-\ntion. We compare Nano-Capsulator with in-context\nzero-shot summarization generated by Vicuna-7B,\nas Vicuna-7B is the initial model weight of Nano-\nCapsulator for prompt compression. The results are\ndemonstrated in Figure 4 with the comprehensive\ncomparison of three LLMs, including Vicuna-13B\nand Claude2. We observe that Capsule Prompt\nyielded by Nano-Capsulator outperforms in all sce-\nnarios, which means that our Nano-Capsulator can\nsignificantly preserve more semantic information\nand preserve prompt utility. Additionally, we assess\nthe performance by directly employing GPT-3.5-\nTurbo to summarize the provided prompts. Figure 5\n(left) illustrates that Nano-Capsulator maintains a\nhigher level of prompt utility compared to GPT-3.5-\nTurbo, resulting in enhanced performance on both\nthe CSQA and GSM8K tasks.\nSecondly, we carry out ablation studies to es-\ntablish the effectiveness of the reward function\nin Nano-Capsulator. These studies are conducted\non Vicuna-13B and Claude2 using the TriviaQA-\nLong dataset. As shown in Figure 5 (right), the\nresults indicate a degradation in performance for\ndownstream tasks when the reward function is\nnot utilized. Note that \"w/ reward\" means Nano-\nCapsulator trained with the reward function, while\n150\n200\n250\n350\n500\nLength\n0.5\n0.6\n0.7\n0.8\n0.9\nAccuracy\nZero-shot Summ\nCapsule Prompt\n150\n200\n250\n350\n500\nLength\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\nAccuracy\nZero-shot Summ\nCapsule Prompt\nFigure 6: Impact of prompt length on Vicuna-13B (left)\nand Claude2 (right) on TriviaQA dataset.\n200\n400\n600\nLength\n0.40\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\nAccuracy\nMultiRC (Selective)\nMultiRC (Ours)\nCSQA (Selective)\nCSQA (Ours)\n200\n400\n600\nLength\n0.55\n0.60\n0.65\n0.70\n0.75\nAccuracy\nCSQA (Random)\nCSQA (Ours)\nFigure 7: The comparison results of Capsule Prompt\nand text dropping methods, including Selective Context\n(left) and random demonstration elimination (right).\n\"w/o reward\" denotes a Nano-Capsulator trained\nwithout the reward function. We further present\ncase studies of Capsule Prompt on GSM8K to\nshowcase the logic preservation, as illustrated in\nFigure 10 from Appendix E. These studies clearly\ndemonstrate that Capsule Prompt retains more se-\nmantic meanings by preserving complete logical\nstructures, suggesting that the utility of prompts is\nbetter maintained.\n4.5\nExploration of Impact Factors (RQ3)\nIn this section, we explore our proposed compres-\nsion mechanism in greater detail, examining vari-\nous factors that influence its performance.\nImpact of Capsule Prompt Length. In the main\nexperiment, the length constraint is fixed to 150 or\n300 tokens for the prompt compression. We fur-\nther explore how the compression rate affects the\npreservation of utility. The results are displayed\nin Figure 6, obtained from experiments conducted\nusing the TriviaQA dataset with Vicuna-13B and\nClaude2. We observe that the length of Capsule\nPrompt can impact its utility on different LLMs.\nWhile longer prompts might capture more useful\ninformation for downstream tasks, they can also\nintroduce certain noise or misinformation to the\nLLMs. This can result in suboptimal performance\nwhen specific LLMs interact with the compressed\n2\n4\n6\nBatch Size\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nLatency (Sec.)\nCapsule Prompt\nOriginal Prompt\n2\n4\n6\nBatch Size\n0\n1\n2\n3\n4\n5\nLatency (Sec.)\nCapsule Prompt\nOriginal Prompt\nFigure 8: Inference Latency of Vicuna-13B on CSQA\ndataset (left) and TrivaQA-Long dataset (right), where\n✖indicates out of memory.\nprompts. The situation can be observed from the\nresults of Capsule Prompt and Zero-shot Summ\nPrompt, where the length of 150 outperforms other\nlength settings under Claude2 and the length of\n200 outperforms other length settings Vicuna-13B.\nWe notice that the desired length settings of Nano-\nCapsulator can be observed from the performance\nof the Zero-shot Summ Prompt, as they share simi-\nlar performance trends.\nImpact of Discrete Text Elimination. In addition\nto compressing prompts using Nano-Capsulator,\nwe acknowledge that prompt length can also be\nreduced by employing methods like random drop-\nping or rule-based selection. To this end, we have\nconducted studies comparing the performance of\nstraightforward text dropping with our proposed\nframework. We consider two baseline methods un-\nder Claude2: a naive random demonstration elimi-\nnation on the CSQA dataset and Selective Context\nas described in (Li et al., 2023), in which Selec-\ntive Context eliminated the word according to the\nself-information values, on both the CSQA and\nMultiRC datasets. The outcomes of these compar-\nisons are showcased in Figure 7. We observe that\nCapsule Prompt outperforms the other two base-\nlines. Particularly, Capsule Prompt achieves better\nperformance when the length is similar to the base-\nlines. This again demonstrates the effectiveness of\nCapsule Prompt in preserving the utility.\n4.6\nLatency of Nano-Capsulator (RQ3)\nThe configuration of the computational infrastruc-\nture is given in Appendix A. We conducted the\nefficiency experiments on two publicly available\nLLMs: OPT-2.7B (Zhang et al., 2022) and Vicuna-\n13B (Chiang et al., 2023), under different batch\nsize settings with the generated length of 200 to-\nkens. Due to the limited GPU memory, we set\nVicuna-13B as bfloat16 to accommodate a single\n4\n8\n16\nBatch Size\n0.0\n0.5\n1.0\n1.5\n2.0\nLatency (Sec.)\nCapsule Prompt\nOriginal Prompt\n4\n8\n16\nBatch Size\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\nLatency (Sec.)\nCapsule Prompt\nOriginal Prompt\nFigure 9: Inference Latency of OPT-2.7B on CSQA\ndataset (left) and TrivaQA-Long dataset (right), where\n✖indicates out of memory.\nGPU, while OPT-2.7B remains its inherent version.\nAs demonstrated in Figure 8 and Figure 9, we ob-\nserve that Nano-Capsulator framework achieves\nmuch lower computational latency compared to\nthe case while inputting the original prompt on\nboth LLMs. Specifically, Capsule Prompt obtains\nmostly of its original performance while reducing\n2.1× ∼4.5× of execution latency. As depicted in\nFigure 9, Capsule Prompt is capable of being ac-\ncommodated within OPT-2.7B under a larger batch\nsize, whereas the use of the original longer prompt\nleads to an out-of-memory issue (i.e., when batch\nsize = 16). Additionally, we notice that Capsule\nPrompt achieves considerable benefits in speeding\nup the inference process as the batch size increases.\nUnder OPT-2.7B, Capsule Prompt accelerates the\nprocess by up to 4.5×, and under Vicuna-13B, it\nachieves a speed increase of 4.1× compared to the\noriginal input prompt. This indicates that Capsule\nPrompt allows for a larger batch size while reduc-\ning the time required for the inference process.\n5\nConclusion\nOur work introduces Natural Language Prompt En-\ncapsulation (Nano-Capsulator), a framework for\neffectively compressing long prompts for LMs\nwhile preserving essential information.\nNano-\nCapsulator alleviates the context length limitations\nof LLMs, enhancing processing efficiency and\ncost-effectiveness. Our results show that Nano-\nCapsulator reduces prompt lengths by 81.4%, de-\ncreases inference latency by up to 4.5×, and cuts\nbudget overheads by 80.1% with almost identical\naccuracy and relevance. This demonstrates its sig-\nnificant potential for improving LLM efficiency\nacross various applications that utilize long input\ndocuments. Future research will focus on refining\nNano-Capsulator for broader domain applications\nand exploring its usage in data-intensive fields.\nReferences\nAnthropic. 2023. Claude.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nAlexis Chevalier, Alexander Wettig, Anirudh Ajith,\nand Danqi Chen. 2023.\nAdapting language\nmodels to compress contexts.\narXiv preprint\narXiv:2305.14788.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\nsource chatbot impressing gpt-4 with 90%* chatgpt\nquality.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebas-\ntian Gehrmann, et al. 2023. Palm: Scaling language\nmodeling with pathways. Journal of Machine Learn-\ning Research, 24(240):1–113.\nChristopher Clark, Kenton Lee, Ming-Wei Chang,\nTom Kwiatkowski, Michael Collins, and Kristina\nToutanova. 2019. Boolq: Exploring the surprising\ndifficulty of natural yes/no questions. arXiv preprint\narXiv:1905.10044.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nNakano, Christopher Hesse, and John Schulman.\n2021. Training verifiers to solve math word prob-\nlems. arXiv preprint arXiv:2110.14168.\nJay DeYoung, Sarthak Jain, Nazneen Fatema Rajani,\nEric Lehman, Caiming Xiong, Richard Socher, and\nByron C. Wallace. Eraser: A benchmark to evaluate\nrationalized nlp models.\nTao Ge, Jing Hu, Xun Wang, Si-Qing Chen, and Furu\nWei. 2023. In-context autoencoder for context com-\npression in a large language model. arXiv preprint\narXiv:2307.06945.\nHuiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing\nYang, and Lili Qiu. 2023. Llmlingua: Compressing\nprompts for accelerated inference of large language\nmodels. arXiv preprint arXiv:2310.05736.\nHongye Jin, Xiaotian Han, Jingfeng Yang, Zhimeng\nJiang, Zirui Liu, Chia-Yuan Chang, Huiyuan Chen,\nand Xia Hu. 2024. Llm maybe longlm: Self-extend\nllm context window without tuning. arXiv preprint\narXiv:2401.01325.\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke\nZettlemoyer. 2017. triviaqa: A Large Scale Distantly\nSupervised Challenge Dataset for Reading Compre-\nhension. arXiv e-prints, page arXiv:1705.03551.\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth,\nShyam Upadhyay, and Dan Roth. 2018. Looking\nbeyond the surface:a challenge set for reading com-\nprehension over multiple sentences. In Proceedings\nof North American Chapter of the Association for\nComputational Linguistics (NAACL).\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. Advances in\nneural information processing systems, 35:22199–\n22213.\nYucheng Li, Bo Dong, Chenghua Lin, and Frank Guerin.\n2023. Compressing context to enhance inference\nefficiency of large language models. arXiv preprint\narXiv:2310.06201.\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang,\nRuochen Xu, and Chenguang Zhu. 2023. G-eval:\nNlg evaluation using gpt-4 with better human align-\nment, may 2023. arXiv preprint arXiv:2303.16634,\n6.\nJesse Mu, Xiang Lisa Li, and Noah Goodman. 2023.\nLearning to compress prompts with gist tokens.\narXiv preprint arXiv:2304.08467.\nSiyu Ren, Qi Jia, and Kenny Q Zhu. 2023. Context\ncompression for auto-regressive transformers with\nsentinel tokens. arXiv preprint arXiv:2310.08152.\nSubhro Roy and Dan Roth. 2015. Solving general arith-\nmetic word problems. In Proceedings of the 2015\nConference on Empirical Methods in Natural Lan-\nguage Processing. Association for Computational\nLinguistics.\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and\nJonathan Berant. 2019. CommonsenseQA: A ques-\ntion answering challenge targeting commonsense\nknowledge. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4149–4158, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. 2022. Chain-of-thought prompting elicits rea-\nsoning in large language models. Advances in Neural\nInformation Processing Systems, 35:24824–24837.\nDavid Wingate, Mohammad Shoeybi, and Taylor\nSorensen. 2022.\nPrompt compression and con-\ntrastive conditioning for controllability and toxic-\nity reduction in language models. arXiv preprint\narXiv:2210.03162.\nJingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian\nHan, Qizhang Feng, Haoming Jiang, Bing Yin, and\nXia Hu. 2023. Harnessing the power of llms in prac-\ntice: A survey on chatgpt and beyond. arXiv preprint\narXiv:2304.13712.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-\nhaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel\nSimig, Punit Singh Koura, Anjali Sridhar, Tianlu\nWang, and Luke Zettlemoyer. 2022. Opt: Open pre-\ntrained transformer language models.\nAppendix\nA\nComputation Infrastructure\nFor a fair comparison of testing algorithmic\nthroughput, the experiments are conducted based\non the following physical computing infrastructure\nin Table 3.\nDevice Attribute\nSpec\nComputing infrastructure\nGPU\nGPU model\nNvidia-A40\nGPU number\n1\nGPU Memory\n46068 MB\nTable 3: Computing infrastructure for the experiments.\nB\nAdditional Experiments of Comparison\nto Soft Prompt Baselines\nTo evaluate our proposed framework against exist-\ning soft prompt methods, we conduct experiments\nwith AutoCompressors (Chevalier et al., 2023) on\nthe GSM8K dataset, as shown in Table 4. Our\nCapsule Prompt is utilized for predictions using\nthe Llama-2-7B model, which is identical to the\npre-trained model used by AutoCompressor. As\nwe can see, AutoCompressor does not preserve es-\nsential information in the compressed soft prompts,\nleading to a considerable performance drop in the\nGSM8K task.\nGSM8K\nAutoCompressors\nOurs\nAccuracy\n3.79\n19.7\nTable 4: Computing infrastructure for the experiments.\nPaLM (Chowdhery et al., 2023)\nCost($)\nOriginal\nCapsule Prompt\nSaved\nCSQA\n0.156\n0.034\n-77.9%\nGSM8K\n0.054\n0.019\n-63.9%\nMultiRC\n0.478\n0.135\n-71.6%\nTrivaQA-Long\n0.022\n0.004\n-80.1%\nTable 5: API cost comparison of Capsule Prompt and\noriginal prompt on PaLM, where Capsule Prompt save\nup to 80.1% of the original cost.\nC\nAPI Cost of PaLM\nWe here provide the API cost of PaLM during\nthe evaluation of Nano-Capsulator. We can ob-\nserve that Capsule Prompt generated by Nano-\nCapsulator save up to 80.1% of its original cost\non PaLM. The results further underscore the excel-\nlent cost-efficiency attributes of Capsule Prompt.\nD\nTraining Costs of Nano-Capsulator\nIn this section, we discuss the training time the\nmemory cost of Nano-Capsulator in this section.\nAll datasets are trained using the initial weights\nof Vicuna-7B. Training time and memory require-\nments are different with the volume and types of\ntraining data. In our work, the training time for\nthe few-shot CoT compression task is approxi-\nmately 8 hours, and for the reading comprehension\ncompression task, it is about 4 hours. Once the\nNano-Capsulator completes the training process,\nwe can directly derive the compressed hard prompts\nthrough a single forward pass of Nano-Capsulator.\nE\nThe Case Studies of Capsule Prompt\nWe demonstrate the case study of Capsule Prompt\non the GSM8K and MultiRC dataset. The results,\ndepicted in Figure 10 and Figure 11, show that\nNano-Capsulator can better obtain the semantic\nmeanings of math logic from the original CoT\nprompt. We observe that in the absence of Nano-\nCapsulator, the Zero-shot Summarization approach\nstruggles to compress long prompts effectively.\nF\nInstruction Usage in Inference LLMs\nWe provide a list of the instruction utilized in train-\ning our Nano-Capsulator framework in Table 6,\nincluding TRep replicating instruction and summa-\nrizing instruction TSumm.\n○Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. \nHow many trees did the grove workers plant today?\nA: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The \nanswer is 6.\n○Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\nA:There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\n○Q: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\nA: Originally, Leah had 32 chocolates. Her sister had 42. So in total they had 32 + 42 = 74. After eating 35, they had 74 - 35 = 39. \nThe answer is 39.\n○Q: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to \nDenny?\nA: Jason started with 20 lollipops. Then he had 12 after giving some to Denny. So he gave Denny 20 - 12 = 8. The answer is 8.\n○Q: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\nA: Shawn started with 5 toys. If he got 2 toys each from his mom and dad, then that is 4 more toys. 5 + 4 = 9. The answer is 9.\n○Q: There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How \nmany computers are now in the server room?\nA: There were originally 9 computers. For each of 4 days, 5 more computers were added. So 5 * 4 = 20 computers were added. 9 \n+ 20 is 29. The answer is 29.\"\n○Q: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at \nthe end of wednesday?\nA: Michael started with 58 golf balls. After losing 23 on tuesday, he had 58 - 23 = 35. After losing 2 more, he had 35 - 2 = 33 golf \nballs. The answer is 33.\n○Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\nA: Olivia had 23 dollars. 5 bagels for 3 dollars each will be 5 x 3 = 15 dollars. So she has 23 - 15 dollars left. 23 - 15 is 8. The \nanswer is 8.\nOriginal Prompt\n○There were 15 trees in the grove, and grove workers planted 6 more trees, so there are now 21 trees in the grove.\n○In the parking lot, there were 3 cars, and 2 more cars arrived, so there are now 5 cars in the parking lot.\n○Leah and her sister had 32 and 42 chocolates, respectively, and they ate 35 chocolates, so they have 39 chocolates left in total.\n○Jason had 20 lollipops and gave 8 lollipops to Denny, so he now has 12 lollipops.\n○Shawn had 5 toys and got 2 toys each from his mom and dad, so he now has 9 toys.\n○There were 9 computers in the server room, and 20 more computers were installed, so there are now 29 computers in the server \nroom.\n○Michael had 58 golf balls and lost 23 on tuesday and 2 more on wednesday, so he has 33 golf balls left.\n○Olivia had $23 and bought 5 bagels for $3 each, so she has $8 left.\n○Grove workers planted 6 trees in the grove.\n○There are now 5 cars in the parking lot.\n○Leah and her sister have 39 pieces of chocolates left in total.\n○Jason gave Denny 8 lollipops.Shawn has 9 toys now.\n○There are 29 computers in the server room.\n○Michael had 33 golf balls at the end of wednesday.\n○Olivia has 8 dollars left.\nCapsule Prompt\nZero-shot Summarization Prompt\nFigure 10: A case study on GSM8K dataset. The results are the Capsule Prompt and in-context summarization\nprompt generated Vicuna-7B, following the settings in RQ2.\nCompression Tasks\nTRep (Replicating Instruction)\nTSumm (Summarizing Instruction)\nFew-shot CoT\nRepeat the following main input.\nPlease summarize each question-answer pair in one sen-\ntence within less than {word count} words. Make sure\nnot to repeat the input question-answer pair.\nReading Comprehension\nRepeat the following main input.\nPlease summarize the passage within less than {word\ncount} words. Make sure not to repeat the passage.\nTable 6: Instructions used in training Nano-Capsulator.\n○The narrator left the tower and took a candle with them to search the ground floor. The daylight was creeping in through the \nbarred windows by the time the narrator finished searching the ground floor. The narrator hesitated to search the cellars but \ndecided to do so because they did not want to be seen as cowardly and because the cellars were the most unlikely place to find \nanything dangerous.\nCapsule Prompt\n○The narrator left the tower and took a candle with them, searching the ground floor of the house. The daylight was creeping in \nthrough the barred windows, but the search showed nothing new. The narrator hesitated before deciding to search the cellars, \nwhich were large and eerie, but ultimately decided to face their fear and complete the task.\nZero-shot Summarization Prompt\nOriginal Prompt\n○The day was just breaking , as I left the tower ; though it was still too dark in the house to be able to see without a light , and I \ntook one of the study candles with me on my ' round . By the time I had finished the ground floor , the daylight was creeping in \n, wanly , through the barred windows . My search had shown me nothing fresh . Everything appeared to be in order , and I was \non the point of extinguishing my candle , when the thought suggested itself to me to have another glance ' round the cellars . I \nhad not , if I remember rightly , been into them since my hasty search on the evening of the attack . For , perhaps , the half of a \nminute, I hesitated . I would have been very willing to forego the task -- as , indeed , I am inclined to think any man well might \n-- for of all the great , awe - inspiring rooms in this house , the cellars are the hugest and weirdest . Great , gloomy caverns of \nplaces , unlit by any ray of daylight . Yet , I would not shirk the work . I felt that to do so would smack of sheer cowardice . \nBesides , as I reassured myself , the cellars were really the most unlikely places in which to come across anything dangerous ; \nconsidering that they can be entered , only through a heavy oaken door , the key of which , I carry always on my person.\nFigure 11: A case study on MultiRC dataset. The results are the Capsule Prompt and in-context summarization\nprompt generated Vicuna-7B, following the settings in RQ2.\n",
  "categories": [
    "cs.CL",
    "cs.AI",
    "cs.LG"
  ],
  "published": "2024-02-28",
  "updated": "2024-04-02"
}