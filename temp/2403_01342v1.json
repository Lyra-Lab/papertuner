{
  "id": "http://arxiv.org/abs/2403.01342v1",
  "title": "LM4OPT: Unveiling the Potential of Large Language Models in Formulating Mathematical Optimization Problems",
  "authors": [
    "Tasnim Ahmed",
    "Salimur Choudhury"
  ],
  "abstract": "In the rapidly evolving field of natural language processing, the translation\nof linguistic descriptions into mathematical formulation of optimization\nproblems presents a formidable challenge, demanding intricate understanding and\nprocessing capabilities from Large Language Models (LLMs). This study compares\nprominent LLMs, including GPT-3.5, GPT-4, and Llama-2-7b, in zero-shot and\none-shot settings for this task. Our findings show GPT-4's superior\nperformance, particularly in the one-shot scenario. A central part of this\nresearch is the introduction of `LM4OPT,' a progressive fine-tuning framework\nfor Llama-2-7b that utilizes noisy embeddings and specialized datasets.\nHowever, this research highlights a notable gap in the contextual understanding\ncapabilities of smaller models such as Llama-2-7b compared to larger\ncounterparts, especially in processing lengthy and complex input contexts. Our\nempirical investigation, utilizing the NL4Opt dataset, unveils that GPT-4\nsurpasses the baseline performance established by previous research, achieving\nan F1-score of 0.63, solely based on the problem description in natural\nlanguage, and without relying on any additional named entity information.\nGPT-3.5 follows closely, both outperforming the fine-tuned Llama-2-7b. These\nfindings not only benchmark the current capabilities of LLMs in a novel\napplication area but also lay the groundwork for future improvements in\nmathematical formulation of optimization problems from natural language input.",
  "text": "arXiv:2403.01342v1  [cs.CL]  2 Mar 2024\nLM4OPT: Unveiling the Potential of Large Language Models in Formulating\nMathematical Optimization Problems\nTasnim Ahmed, Salimur Choudhury\nSchool of Computing, Queen’s University\nKingston, Ontario K7L 2N8, Canada\n{tasnim.ahmed, s.choudhury}@queensu.ca\nAbstract\nIn the rapidly evolving ﬁeld of natural language processing,\nthe translation of linguistic descriptions into mathematical\nformulation of optimization problems presents a formidable\nchallenge, demanding intricate understanding and process-\ning capabilities from Large Language Models (LLMs). This\nstudy compares prominent LLMs, including GPT-3.5, GPT-\n4, and Llama-2-7b, in zero-shot and one-shot settings for this\ntask. Our ﬁndings show GPT-4’s superior performance, par-\nticularly in the one-shot scenario. A central part of this re-\nsearch is the introduction of ‘LM4OPT,’ a progressive ﬁne-\ntuning framework for Llama-2-7b that utilizes noisy em-\nbeddings and specialized datasets. However, this research\nhighlights a notable gap in the contextual understanding ca-\npabilities of smaller models such as Llama-2-7b compared\nto larger counterparts, especially in processing lengthy and\ncomplex input contexts. Our empirical investigation, utilizing\nthe NL4Opt dataset, unveils that GPT-4 surpasses the base-\nline performance established by previous research, achiev-\ning an F1-score of 0.63, solely based on the problem de-\nscription in natural language, and without relying on any ad-\nditional named entity information. GPT-3.5 follows closely,\nboth outperforming the ﬁne-tuned Llama-2-7b. These ﬁnd-\nings not only benchmark the current capabilities of LLMs in a\nnovel application area but also lay the groundwork for future\nimprovements in mathematical formulation of optimization\nproblems from natural language input.\nIntroduction\nNumerous practical challenges originating from diverse do-\nmains such as operations, economics, engineering, and com-\nputer science can be articulated as optimization problems\n(AhmadiTeshnizi, Gao, and Udell 2023). Standard optimiza-\ntion algorithms, including the simplex (Nash 2000) and\ninterior-point methods (Karmarkar 1984), can efﬁciently ad-\ndress these problems. Nevertheless, the translation of a real-\nworld situation into a mathematical formulation necessi-\ntates specialized knowledge. This expertise barrier hinders\nmany individuals from utilizing optimization algorithms,\neven when these could substantially enhance their opera-\ntions. The advancement of automating problem formulation,\nwhich involves translating natural language descriptions into\ndecision variables, constraints, and objective functions, has\nthe potential to make these processes more accessible to in-\ndividuals beyond just operations research experts. Conse-\nquently, optimization modeling would become accessible to\nindividuals who cannot afford experts to augment efﬁciency\nusing optimization techniques. Provided the problem is cor-\nrectly formulated, it can be readily solved by transcribing it\ninto an algebraic modeling language interpretable by solvers\n(Ramamonjison et al. 2023).\nThe ﬁeld of Natural Language Processing (NLP) presents\na potent avenue for enhancing the accessibility and efﬁ-\nciency of optimization problem formulation. From the in-\nception of word embeddings to the evolution of language\nmodels, NLP has undergone transformative progress over\nthe years. Especially with the emergence of pre-trained lan-\nguage models (Devlin et al. 2019), these models have at-\ntained state-of-the-art results on a multitude of NLP tasks\nsuch as natural language inference (NLI), question answer-\ning, summarization, collaborative writing, etc., with min-\nimal task-speciﬁc ﬁne-tuning (Laskar, Hoque, and Huang\n2021). The recent advancements in LLMs, including GPT\n(OpenAI 2023), and Llama (Touvron et al. 2023), have sig-\nniﬁcantly reshaped the NLP landscape and practices. These\nLLMs, with parameter sizes exceeding several billions, and\neven reaching hundreds of billions, have exhibited remark-\nable generalization abilities in zero-shot and few-shot set-\ntings through prompting. Furthermore, these LLMs have\nshown exceptional ﬁne-tuning capabilities, even when ﬁne-\ntuned on datasets signiﬁcantly smaller than those used by\ntheir predecessors.\nTo\nthis\nend,\nformal\nassessment\nof\nthis\nspeciﬁc\ntask−mathematical\nformulation\nof\noptimization\nprob-\nlems from natural language descriptions using the latest\ndevelopments from the GPT series models, namely GPT-3.5\nand GPT-4, which have garnered widespread recognition,\nremains an uncharted territory. Additionally, this research\naims to investigate the capabilities and limitations of a\nsmaller Large Language Model (LLM), Llama-2-7b, when\nﬁne-tuned on this task. Consequently, this study offers the\nfollowing contributions:\n• Comprehensive analysis of GPT-3.5, GPT-4, and Llama-\n2-7b in mathematical formulation of optimization prob-\nlems from natural language description.\n• Evaluation in zero-shot and one-shot settings to under-\nstand the impact of few-shot prompt engineering and\nlearning adaptations of the models.\n• Empirical study using the NL4Opt (Ramamonjison et al.\n2023) dataset, demonstrating the superior performanceof\nGPT-4, followed by GPT-3.5.\n• Exploration of utilizing the LM4OPT framework to ﬁne-\ntune Llama-2-7b, revealing signiﬁcant performance en-\nhancements.\nRelated Work\nEfforts to simplify combinatorial optimization using LLMs\nhave seen diverse approaches, aiming to make the process\nuser-friendly for laypersons. The NL4Opt (Ramamonjison\net al. 2023) competition stands out, exploring the trans-\nformation of natural language into structured optimization\nmodels. In Task 1 which is described in (Dakle et al. 2023),\nthe aim is to accurately identify and label the components\nof optimization models—such as objectives, variables, and\nconstraints—within natural language texts. Researchers ap-\nproached this by using classical NER techniques that rely on\nthe morphological and grammatical properties of the text.\nAdditionally, modern methods were employed, involving\nthe use of pre-trained LLMs like BERT and GPT, which\nwere further ﬁne-tuned on optimization-speciﬁc datasets to\nbetter understand the unique language of optimization prob-\nlems. Task 2 required building mathematical representations\nfrom these elements, a more complex step involving deeper\nmodel comprehension. The methodologies here included the\nuse of sequence-to-sequencemodels, which are adept at han-\ndling such translation tasks.\nThe former two-step approach to generate mathemati-\ncal formulation from optimization problem description re-\nquires training and dependency on two separate models. To\nbridge the research gap, Tsouros et al. (Tsouros et al. 2023)\nproposed an all-in-one LLM-based model that creates opti-\nmization models directly from prompts, showing early po-\ntential on the dataset described in NL4Opt but without es-\ntablished benchmarks for comparison. Advancing this ap-\nproach, Teshinizi et al. (AhmadiTeshnizi, Gao, and Udell\n2023) presented a novel framework named OptiMUS, which\nutilizes LLMs (pre-trained GPT) to formulate and solve\nMixed Integer Linear Programming (MILP) problems from\nnatural language descriptions. They introduced a dataset,\nNLP4LP, containing linear programming and MILP prob-\nlems to benchmark OptiMUS, which shows signiﬁcant im-\nprovement over basic LLM prompting strategies. OptiMUS\nintegrates mathematical modeling, Gurobi solver code gen-\neration, automated testing, and debugging in a cohesive sys-\ntem that streamlines the optimization problem-solving pro-\ncess. The goal of this study is to democratize access to opti-\nmization techniques across various domains, thereby broad-\nening the use of optimization tools beyond expert circles.\nFurthermore, Yang et al. (Yang et al. 2023) introduced an-\nother prompt-based framework, OPRO, which uses LLMs\nto optimize problems without needing traditional solvers.\nOPRO works by iteratively improving solutions using a\n‘meta-prompt’ that incorporates both the problem descrip-\ntion and feedback from previous solutions. It aims to learn\ncontinuously as it updates the meta-prompt with new infor-\nmation. To ensure stable results, OPRO generates several\nsolutions at each iteration, balancing the need to explore\ndifferent options with reﬁning existing ones. The authors\ndemonstrated encouraging preliminary outcomes when ap-\nplying their methods to the GSM8K (Cobbe et al. 2021) and\nBBH (Suzgun et al. 2022) datasets, in addition to tasks such\nas linear regression and the traveling salesman problem. The\neffectiveness of OPRO for complex optimization tasks is\nyet to be fully determined. In a recent study focused on\npractical applications, researchers introduced the OptiGuide\nframework (Li et al. 2023), a novel integration of combi-\nnatorial optimization technology with advanced Large Lan-\nguage Models (LLMs), such as GPT-4, aimed at augment-\ning decision-making processes within supply chain man-\nagement. This framework transforms user queries into in-\ncontext learning (ICL) queries for LLM processing, gener-\nating code that is vetted for accuracy and reliability. Upon\nvalidation, this code interfaces with speciﬁc components\nlike optimization solvers and databases to derive solutions.\nThe results, converted into understandable explanations by\nthe LLM, simplify complex supply chain optimizations for\nnon-technical users, fostering trust in automated decisions.\nIn practical deployments, such as Microsoft Azure’s supply\nchain, OptiGuide has exhibited promising outcomes, achiev-\ning an average accuracy of 93% with GPT-4, highlighting its\neffectiveness in real-world settings. A summary of the recent\nworks in the ﬁeld of Optimization and Language Models is\nshown in Table 1.\nDespite these strides, a gap persists−an end-to-end sys-\ntem that allows users the ﬂexibility to verify and mod-\nify mathematical problem formulation, independent of the\nsolver or programming language used. Addressing this, our\nresearch identiﬁes a niche for benchmarking popular pre-\ntrained LLMs on the speciﬁc task of optimization problem\nformulation and developing a tailored ﬁne-tuning approach\nto enhance LLM speciﬁcity for this nuanced application.\nThis work endeavors to bridge the research gap, offering\na robust benchmark and a novel ﬁne-tuning strategy that\ncould signiﬁcantly beneﬁt the scientiﬁc community’s pursuit\nof democratizing optimization modeling.\nTask Formulation\nThis research investigates a generative task in the ﬁeld of\nnatural language processing, concentrating on the genera-\ntion of mathematical formulations for optimization prob-\nlems derived from textual descriptions. Our objective is to\nderive structured representations - encompassing variables,\nconstraints, and the objective function based on given natu-\nral language descriptions. We utilize a dataset, denoted as S,\ncomprising a series of problem descriptions, and C, repre-\nsenting their corresponding formulations in canonical math-\nematical form. At the core of our methodology is the in-\ntroduction of an intermediate representational set, R, which\nencapsulates the essential components of optimization prob-\nlems (variables, constraints, and objective functions) in an\nequation-centric format, as opposed to the ﬁnal matrix form\ndepicted in C. For a given problem description s ∈S, the\nprimary goal of an LLM is to predict an intermediate rep-\nresentation r ∈R. Finally, the predicted intermediate rep-\nresentation, r, undergoes a systematic conversion into the\nResearch Work\nDataset\nInput\nFramework\nObjective\nProblem Type in Natural Language\nHuman-in-the-loop\nMultiple LLMs\nFine-tuning\nPrompt Engineering\nNER4Opt\nNL4Opt\nOptimization\n×\n×\n✓\n×\nIdentifying named entitties\nNL4Opt Competition\nNL4Opt\nOptimization\n×\n✓\n✓\n×\nMathematical Formulation\nHoly Grail 2.0\n−\nOptimization\n−\n−\n−\n−\nMathematical Formulation\nOPRO\nGSM8K, BBH\nMath word, Common-sense, Optimization\n×\n×\n×\n✓\nProblem Solution\nOptimus\nNLP4LP\nOptimization\n✓\n✓\n×\n✓\nProblem Solution\nOptiguide\nPrivate\nSupply chain management\n×\n×\n×\n✓\nProblem Solution (QA Session)\nLM4OPT (ours)\nNL4Opt\nOptimization\n×\n×\n✓\n✓\nMathematical Formulation\nTable 1: Recent works in the ﬁeld of Optimization and Language Models\ncanonical formulation, denoted as c ∈C, to facilitate a com-\nprehensive evaluation of the performance of LLM. This pro-\ncess is exempliﬁed in Figure 1, where an example of a prob-\nlem description along with the corresponding intermediate\nrepresentation and canonical form is provided. It should be\nnoted that the constraints are transformed into a format em-\nbodying ‘less than or equal to’ conditions, and the objective\nfunction is reformulated into a minimization paradigm.\nMethodology\nIn contemporary research, language models are conceptual-\nized as functions that accept a textual input context and yield\na corresponding textual output. This paradigm is predomi-\nnantly instantiated through the use of transformer-based ar-\nchitectures, a concept introduced by Vaswani et al. (Vaswani\net al. 2017) in 2017, which has since revolutionized the\nﬁeld of NLP. The quintessential aspect of transformer lan-\nguage models is their reliance on self-attention mechanisms.\nThese mechanisms are designed to encode input contexts\nby weighing the importance of different parts of the input\ntext relative to each other. However, these models face a no-\ntable limitation in processing long text sequences due to the\nquadratic increase in computational complexity with longer\ninputs (Devlin et al. 2019). This leads to a restricted context\nwindow during pre-training, limiting the model’s ability to\nmaintain and utilize long-term dependencies and integrate\ninformation from distant text segments. Consequently, this\nimpacts the model’s effectiveness in tasks requiring exten-\nsive contextual understanding (Brown et al. 2020). To this\nend, our experiments investigate the performance of LLMs\nin zero-shot and one-shot pre-trained settings, alongside a\nsmaller LLM, speciﬁcally ﬁne-tuned for the task of mathe-\nmatical formulation of optimization problems.\nFor this purpose, we evaluate GPT-3.5, GPT-4, and\nLlama-2-7b models. As ﬁne-tuning is not a prerequisite for\ninference in these LLMs, our approach centers on the de-\nvelopment of optimal prompt instructions for both zero-shot\nand one-shot settings. This development is guided by the\nprompt optimization techniques delineated in (Yang et al.\n2023). Additionally, to explore the impact of ﬁne-tuning on\na task-speciﬁc dataset, we selected the Llama-2-7b model,\nprimarily due to its comparatively lower resource demands.\nThis model was ﬁne-tuned using the NL4Opt dataset, allow-\ning for an in-depth analysis of ﬁne-tuning effects on model\nperformance within this speciﬁc context. Optimized instruc-\ntions for ﬁne-tuning, zero-shot, and one-shot prompts are\nprovided in Figure 2.\nAdvanced Tuning of Llama-2-7b via LM4OPT\nA progressive ﬁne-tuning strategy was employed for the\nLlama-2-7b model, enabling it to initially adapt to a broader\ndomain context related to the ﬁnal task. This preliminary\nadaptation phase is crucial in enhancing the model’s com-\nprehension and performance capabilities. Following this, the\nmodel undergoes further ﬁne-tuning on a specialized, task-\nspeciﬁc dataset, where it applies the knowledge acquired in\nthe initial phase to achieve improved performance and gen-\neralization on the target task. Prior to its ﬁne-tuning on the\nNL4Opt dataset, the model was ﬁne-tuned on GSM8K−a\ndataset comprising high-quality, linguistically diverse grade\nschool math word problems crafted by human problem writ-\ners (Cobbe et al. 2021). This sequential ﬁne-tuning approach\neffectively leverages the broader contextual understanding\ngained from GSM8K, thereby reﬁning the model’s perfor-\nmance on the NL4Opt tasks.\nIn the ﬁne-tuning phase, a methodological approach in-\ntegrating Low-Rank Adaptations (LoRA) (Hu et al. 2021)\nwith Parameter-Efﬁcient Fine-Tuning (PEFT) (Liu et al.\n2022) was employed. The ﬁne-tuning process involved care-\nfully adjusting the low-rank matrices introduced by LoRA,\nensuring minimal yet strategic changes to the pre-existing\nweights. This method preserves the general linguistic under-\nstanding gained from pre-training while efﬁciently steering\nit toward the specialized task of mathematical problem for-\nmulation. The effectiveness of this approach is evident in the\nimproved ability to parse and translate complex natural lan-\nguage descriptions into structured mathematical representa-\ntions, a crucial requirement for the NL4Opt dataset. PEFT,\non the other hand, extends this concept by focusing on selec-\ntively ﬁne-tuning a small subset of the parameters. By adopt-\ning PEFT, the ﬁne-tuning process becomes computationally\nless demanding and more feasible on standard hardware,\nwhile still achieving performance comparable to full-model\nﬁne-tuning. The synergy between LoRA and PEFT in ﬁne-\ntuning Llama-2-7b is particularly effective in addressing the\nchallenges of large model adaptation to speciﬁc tasks.\nFurthermore, the inclusion of Noisy Embedding Instruc-\ntion Fine Tuning (NEFTune) (Jain et al. 2023) further aug-\nmented the ﬁne-tuning process. NEFTune, by integrating\ncontrolled random noise into the embedding vectors during\ntraining prevents the model from overﬁtting to the speciﬁcs\nof the training dataset, such as formatting details and exact\nwording. Instead, it encourages the model to generate re-\nsponses that are more coherent, longer, and more diverse. A\ndetailed conﬁguration of our experimental setup is described\nin the following subsection.\nProblem Description\nA hotel employs cleaners and receptionists.\nCleaners earn $500 per week and reception-\nists earn $350 per week. The hotel requires\na minimum of 100 workers of whom at least\n20 must be receptionists. To keep the hotel\nclean and running smoothly, the number of\nreceptionists should be at least a third of the\nnumber of cleaners. The hotel wants to keep\nthe weekly wage bill below $30000. Formu-\nlate an LP to minimize the wage bill.\nIntermediate Representation\nVariables: cleaners, receptionists\nConstraints:\n(−1.0) ∗cleaners + (−1.0) ∗receptionists ≤−100.0\n(−0.0) ∗cleaners + (−1.0) ∗receptionists ≤−20.0\n(0.33) ∗cleaners + (−1.0) ∗receptionists ≤−0.0\n(500.0) ∗cleaners + (350.0) ∗receptionists ≤30000.0\nObjective Function:\nminimize(500.0) ∗cleaners + (350.0) ∗receptionist\nCanonical Form\n[[-1.0, -1.0, -100.0],\n[0.0, -1.0, -20.0],\n[0.33, -1.0, 0.0],\n[500.0, 350.0, 30000]],\n[500.0, 350.0]\nFigure 1: Task Representation\nFine-tuning Instruction\nImagine you are a combinatorial optimization problem solver. I will give you a problem description. Your task is to ﬁnd the variables,\nconstraints, and objective functions from that description. In your response, all the constraints must be in the less than or equal to format.\nYour response must contain only these 3 parts: - Variables, Constraints, and Objective Function. There must be no extra strings before or\nafter it.\nZero-shot Instruction\nImagine you are a combinatorial optimization problem solver. I will give you a problem description. Your task is to ﬁnd the variables,\nconstraints, and objective functions from the description. I am giving you an example response format; your output should be formatted\nlike this. Example Response:\n“Variables: cleaners, receptionists\nConstraints:\n(−1.0) ∗cleaners + (−1.0) ∗receptionists ≤−100.0\n(−0.0) ∗cleaners + (−1.0) ∗receptionists ≤−20.0\n(0.33) ∗cleaners + (−1.0) ∗receptionists ≤−0.0\n(500.0) ∗cleaners + (350.0) ∗receptionists ≤30000.0\nObjective Function:\nminimize(500.0) ∗cleaners + (350.0) ∗receptionist”.\nNow, below is the actual problem description that you have to solve. In your response, all the constraints must be in the less than or equal\nto format. Your response must contain only these 3 parts: Variables, Constraints, and Objective Function. There must be no extra strings\nbefore or after it. Problem description to solve:\nOne-shot Instruction\nImagine you are a combinatorial optimization problem solver. I will give you a problem description. Your task is to ﬁnd the variables,\nconstraints, and objective functions from that description. Before that, I am giving you an example problem description and response for\nyour understanding; Your response should be formatted like this. Example Problem Description:\n“A hotel employs cleaners and receptionists. Cleaners earn $500 per week and receptionists earn $350 per week. The hotel requires\na minimum of 100 workers of whom at least 20 must be receptionists. To keep the hotel clean and running smoothly, the number of\nreceptionists should be at least a third of the number of cleaners. The hotel wants to keep the weekly wage bill below $30000. Formulate\nan LP to minimize the wage bill.”\nExample Response for the given example problem:\n“Variables: cleaners, receptionists\nConstraints:\n(−1.0) ∗cleaners + (−1.0) ∗receptionists ≤−100.0\n(−0.0) ∗cleaners + (−1.0) ∗receptionists ≤−20.0\n(0.33) ∗cleaners + (−1.0) ∗receptionists ≤−0.0\n(500.0) ∗cleaners + (350.0) ∗receptionists ≤30000.0\nObjective Function: minimize(500.0) ∗cleaners + (350.0) ∗receptionist”.\nNow, below is the actual problem description that you have to solve. In your response, all the constraints must be in the less than or equal\nto format. Your response must contain only these 3 parts: Variables, Constraints, and Objective Function. There must be no extra strings\nbefore or after it. Problem description to solve:\nFigure 2: Instruction set for the Prompts to LLMs\nThe incorporation of methodologies such as progressive\nﬁne-tuning, LoRA, PEFT, and NEFTune into the conven-\ntional ﬁne-tuning framework of Large Language Models\n(LLMs) has notably augmented the inferential efﬁcacy of\nthe Llama-2-7b model. This strategic enhancement is partic-\nularly salient for a generative language model of this scale,\nwith a parameter count of only 7 billion, especially in intri-\ncate tasks that challenge even more extensive models like\nGPT-3.5 and GPT-4 in their capacity to comprehend and\nmaintain prolonged and complex contexts.\nExperimental Setup\nThe ﬁne-tuning of the Llama-2-7b model was conducted on\nan NVIDIA A40 GPU, equipped with 48 GB of VRAM,\nover a span of 7 epochs. This process leveraged the dataset\ndivision suggested by the authors of NL4Opt (Ramamonji-\nson et al. 2023), segregating it into training, validation, and\nevaluation subsets. A batch size of 4 was employed, cou-\npled with a gradient accumulation step of 1, and the AdamW\n(Loshchilov and Hutter 2017) optimizer was utilized. The\ninitial learning rate was set at 3e −4, with a weight de-\ncay factor of 0.001. A random noisy embedding strength\nof 5 provided the most satisfactory results during the ﬁne-\ntuning process. A maximum response sequence length of\n200 was designated, under the premise that model outputs\nwould not exceed this threshold for this speciﬁc task. Fur-\nthermore, the implementation of Gradient Checkpointing\n(Chen et al. 2016) facilitated a more resource-efﬁcient ﬁne-\ntuning framework.\nAn additional aspect of this research involved estimating\nthe carbon footprint associated with the ﬁne-tuning phase,\nguided by the methodology proposed by Lannelongue et\nal. (Lannelongue, Grealey, and Inouye 2021). This analy-\nsis revealed that each ﬁne-tuning session of the Llama-2-7b\nmodel produced approximately 23.52 grams of CO2 emis-\nsions. Notably, this ﬁnding underscores the relatively mod-\nest environmental impact of ﬁne-tuning the model for spe-\ncialized tasks.\nResult and Discussion\nA comprehensive assessment of various LLMs was con-\nducted, focusing on their capability in formulating optimiza-\ntion problems. This evaluation was based on prompt-based\nzero-shot and one-shot learning experiments. The perfor-\nmances of these LLMs were meticulously compared against\nthe established baseline provided by Ramamonjison et al.\n(Ramamonjison et al. 2023), as detailed in Table 2. For a\nconsistent and objective assessment, the same scoring mech-\nanism employed in the baseline evaluation by Ramamonji-\nson et al. was adopted. This approach ensures a fair and di-\nrect comparison of the performance of LLMs relative to the\nexisting benchmark in this task.\nThe baseline performance in Table 2 is derived from a\nﬁne-tuned BART (Lewis et al. 2019) model, which oper-\nates under different input conditions compared to the LLMs.\nWhile LLMs like Llama-2 and GPT receive instruction\nprompts and problem descriptions in natural language, the\nbaseline BART model is also provided with named en-\ntity information extracted from the natural language prob-\nlem descriptions. This additional data potentially contributes\nto the baseline’s competitive F1-score of 0.61. The GPT-4\nmodel, especially in the one-shot setting, outperforms oth-\ners, including the baseline and GPT-3.5, with an F1-score of\nLanguage Model\nk-Shot\nF1-score\nBaseline (Ramamonjison et al. 2023)\n-\n0.610\nLlama-2-7b\n0\n0.1259\nLlama-2-7b\n1\n0.1022\nGPT-3.5\n0\n0.4381\nGPT-3.5\n1\n0.4928\nGPT-4\n0\n0.6072\nGPT-4\n1\n0.6330\nTable 2: Performance evaluation of LLMs for opti-\nmization problem formulation. The best performance\nin terms of F1-score is highlighted in bold. GPT-3.5\n(gpt-3.5-turbo-0613) and GPT-4 (gpt-4-0613)\nmodels are accessed through OpenAI api1on November 1,\n2023. Llama-2-7b model is ﬁne-tuned using the proposed\nLM4OPT framework.\nModel\nk-Shot\nFine-tune\nNEFTune\nF1-Score\n0\n×\n×\n0.0036\n0\nN\n×\n0.0617\n1\nN\n×\n0.0581\nLlama-2-7b\n0\nN\n✓\n0.0770\n1\nN\n✓\n0.0693\n0\nP\n✓\n0.1259\n1\nP\n✓\n0.1022\nTable 3: Performance comparison of ﬁne-tuned Llama-\n2-7b. ‘N’ in the ‘Fine-tune’ column represents non-\nprogressive ﬁne-tuning, whereas, ‘P’ refers to progressive\nﬁnetuning. The best performance is highlighted in bold.\n0.6330. This superior performance can be attributed to GPT-\n4’s advanced architecture and larger dataset training, as sug-\ngested by recent studies emphasizing the enhanced contex-\ntual understanding and response accuracy in more extensive\nmodels (OpenAI 2023). Conversely, Llama-2-7b, despite be-\ning a smaller model, shows notable performance improve-\nments in the zero-shot setting compared to one-shot, which\naligns with the ﬁndings that smaller models might struggle\nwith longer context prompts.\nTable 3 showcases the performance comparison of the\nLlama-2-7b model under various ﬁne-tuning conditions. It\nassesses the F1-Score across different conﬁgurations, in-\ncluding zero-shot and one-shot settings (k-Shot), with and\nwithout ﬁne-tuning, and the application of Noisy Embed-\ndings Fine-tuning (NEFTune). Notably, progressive ﬁne-\ntuning using the LM4OPT framework (P), especially in the\nzero-shot setting, signiﬁcantly enhances the performance,\nachieving the highest F1-Score of 0.1259. This indicates the\nefﬁcacy of progressive ﬁne-tuning combined with NEFTune\nin improving the ability to understand and solve optimiza-\ntion problems, as opposed to non-progressive ﬁne-tuning\n(N) and the baseline without any ﬁne-tuning.\nA notable observation from Table 3 is the superior out-\n1https://platform.openai.com/docs/models\ncomes in zero-shot settings compared to their one-shot coun-\nterparts across all conﬁgurations. This phenomenon could be\nattributed to the hypothesis that a smaller model like Llama-\n2-7b struggles with longer contexts. The data suggests that\nin scenarios involving extended contexts, the model tends to\nexhibit behavior indicative of hallucinations and produces\nrepetitive responses that lack coherence with the broader\ncontext. Such patterns reinforce the notion that smaller mod-\nels may face challenges in maintaining consistency and rel-\nevance in responses as the prompt length increases, a critical\nconsideration in optimizing model performance for complex\ntasks.\nEffect of Progressive Fine-tuning\nAs shown in Table 3, ﬁne-tuning speciﬁcally for instruc-\ntion processing signiﬁcantly enhanced the performance of\nthe Llama-2-7b model. Initially, the pre-trained Llama-2-\n7b, in both zero-shot and one-shot settings, exhibited sub-\nstantial hallucination. A notable example of this was the\nmodel generating two distinct sets of variables within a sin-\ngle response, and its output format often did not align with\nthe given prompt instructions, as demonstrated in Figure 3.\nHowever, the performance signiﬁcantly improved after pro-\ngressively ﬁne-tuning the model. As it is evident from the\nresponse samples, the performance of the ﬁne-tuned Llama-\n2-7b model signiﬁcantly declined due to its inability to con-\nsistently maintain a speciﬁc response format. It is hypoth-\nesized that involving human evaluators or a human-in-the-\nloop approach for minor modiﬁcations to the outputs could\nsigniﬁcantly improve its efﬁciency. Such interventions could\npotentially bring the performance of a smaller model like\nLlama-2-7b closer to that of some of the larger models.\nDoes Increased Instruction Length Always\nEnhance Performance?\nUpon a thorough examination of the results and the out-\nputs from both GPT and Llama models, it became evi-\ndent that longer instructions do not universally enhance re-\nsponses across all models. The study noted that extended,\ndetailed instructions were beneﬁcial for larger models like\nGPT-3.5 and GPT-4. Longer instructions aided GPT-3.5\nand GPT-4 in resolving parsing issues common in scenar-\nios where multiple formulations are correct but are scored\ndifferently due to manual parsing in the scoring mecha-\nnism. By specifying the solution format, these larger mod-\nels were guided to respond in a particular way. For in-\nstance, GPT-3.5 in a zero-shot setting produced the equa-\ntion largeships ≤smallships, whereas in a one-shot set-\nting, it generated largeships −smallships ≤0. The latter\nformulation, after parsing, yielded a higher score. However,\nwith Llama-2-7b, a smaller model, longer instructions led\nto issues such as repetition and hallucination, particularly\nnoticeable in one-shot settings. As illustrated in Figure 4,\nLlama-2-7b not only repeated parts of the instructions but\nalso generated nonexistent variables such as x1, x2, x3, de-\nviating from the original problem description.\nLimitations\nIn this study, certain limitations have been identiﬁed that\nbear on the research outcomes. A noticeable constraint\nwithin the dataset utilized for this research is its composi-\ntion of straightforward, formally structured samples replete\nwith speciﬁc optimization domain terminologies like ‘for-\nmulate an LP.’ This framework diverges from our overarch-\ning aim to assess the efﬁcacy of LLMs in interpreting and\nformulating optimization problems as presented in natural\nlanguage by individuals unversed in domain-speciﬁc jargon.\nIt is posited that this dataset limitation might yield a dis-\ncrepancy between the documented performance of LLMs\nand their practical application by domain-agnostic users.\nMoreover, resource constraints impeded the exploration of\nprogressive ﬁne-tuning effects on larger LLMs, such as\nLlama-2-70b and GPT-3.5, which may have offered addi-\ntional insights. Furthermore, the adoption of a rule-based ap-\nproach for converting intermediate representations to canon-\nical forms has its drawbacks. Upon meticulous review, it\nwas observed that some LLM-generated intermediate repre-\nsentations were inaccurately formatted, leading to canonical\nforms that diverged from the ground truth. While these dis-\ncrepancies inﬂuenced the LLMs’ performance metrics, it is\nconjectured that such nuances would be within human inter-\npretive capabilities, suggesting that a collaborative human-\nmodel approach might counterbalance the observed perfor-\nmance degradation linked to format conversions. The inter-\naction between what the model produces and how humans\nunderstand it highlights an important area for future studies.\nIt emphasizes the need to harmonize machine precision with\nhuman judgment.\nConclusion\nIn this study, we undertook a comprehensive evaluation of\nLLMs such as GPT-3.5, GPT-4, and Llama-2-7b, focusing\non their ability to translate natural language descriptions into\nmathematical formulation of optimization problems. The re-\nsearch highlights that while GPT-4 exhibits superior perfor-\nmance in both zero-shot and one-shot scenarios, there is a\nnotable capability gap with smaller models like Llama-2-7b,\nparticularly in handling complex contexts. Progressive ﬁne-\ntuning of Llama-2-7b, especially with noisy embeddings and\nspecialized datasets using our proposed LM4OPT frame-\nwork, signiﬁcantly enhances its performance. These ﬁndings\ncontribute to understanding the capabilities and limitations\nof LLMs in a novel application area, paving the way for\nfuture improvements in the ﬁeld of optimization and OR.\nDrawing upon the foundational research by Teshnizi et al.\n(AhmadiTeshnizi, Gao, and Udell 2023), this study lays the\ngroundwork for future extensions, wherein the intermediate\nmathematical representations derived from natural language\ndescriptions in this research could serve as precursors for\nLLMs to generate ILP solver code in Python.\nReferences\nAhmadiTeshnizi, A.; Gao, W.; and Udell, M. 2023. Opti-\nMUS: Optimization Modeling Using mip Solvers and large\nlanguage models. ArXiv, abs/2310.06116.\nPre-trained\n### Solution\n“‘\nVariables: thin, stubby\nConstraints:\n(50.0) ∗thin + (90.0) ∗baking <= 3000.0\n(30.0) ∗thin + (150.0) ∗baking <= 4000.0\n(5.0) ∗thin + (9.0) ∗baking <= 0.0\n(5.0) ∗thin + (9.0) ∗baking <= 0.0\nObjective Function:\nmaximize(5.0) ∗thin + (9.0) ∗baking\n“‘\n### Solution\n“‘\nVariables: cleaners, receptionists\nConstraints:\n(−1.0) ∗cleaners + (−1.0) ∗receptionists <= −100.0\n(−0.0) ∗cleaners + (−1.0\nFine-tuned\nExample Response: “Variables: thinjar, stubbyjar\nConstraints:\n(50.0) ∗thinjar + (30.0) ∗stubbyjar <= 3000.0\n(90.0) ∗thinjar + (150.0) ∗stubbyjar <= 4000.0\nObjective Function:\nmaximize(5.0) ∗thinjar + (9.0) ∗stubbyjar”\nFigure 3: Sample response from pre-trained and ﬁne-tuned Llama-2-7b. Hallucinating behavior is highlighted in red.\nLooping\nResponse:“Variables: apple, pear\nConstraints:\n(−1.0) ∗apple + (−0.0) ∗pear ≤−5.0\n(−0.0) ∗apple + (−1.0) ∗pear ≤−10.0\n(1.0) ∗apple + (−0.0) ∗pear ≤−0.0\n(1.0) ∗apple + (2.0) ∗pear ≤0.0\nObjective Function: maximize(2.0) ∗apple + (4.0) ∗pear”.\n## Part 2: An easy LP In this part, you are given a problem description and you have to\nﬁnd the variables and constraints. Your response must be in the following format:\nHallucination\nVariables: x1, x2, x3\nConstraints:\n(−1.0) ∗x1 + (−0.0)∗\nFigure 4: Looping Behavior and Hallucination for few-shot instructions in Llama-2-7b. The repetitive and hallucinating\nbehaviors are highlighted in red.\nBrown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; Agarwal, S.; Herbert-Voss, A.; Krueger, G.; Henighan,\nT. J.; Child, R.; Ramesh, A.; Ziegler, D. M.; Wu, J.; Winter,\nC.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.;\nChess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford,\nA.; Sutskever, I.; and Amodei, D. 2020. Language Models\nare Few-Shot Learners. ArXiv, abs/2005.14165.\nChen, T.; Xu, B.; Zhang, C.; and Guestrin, C. 2016. Train-\ning Deep Nets with Sublinear Memory Cost.\nArXiv,\nabs/1604.06174.\nCobbe, K.; Kosaraju, V.; Bavarian, M.; Chen, M.; Jun, H.;\nKaiser, L.; Plappert, M.; Tworek, J.; Hilton, J.; Nakano, R.;\nHesse, C.; and Schulman, J. 2021.\nTraining Veriﬁers to\nSolve Math Word Problems. ArXiv, abs/2110.14168.\nDakle, P.; Kadio˘glu, S.; Uppuluri, K.; Politi, R.; Ragha-\nvan, P.; Rallabandi, S. K.; and Srinivasamurthy, R. S. 2023.\nNer4Opt: Named Entity Recognition for Optimization Mod-\nelling from Natural Language. In Integration of AI and OR\nTechniques in Constraint Programming.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.\nBERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding. In North American Chapter of the\nAssociation for Computational Linguistics.\nHu, J. E.; Shen, Y.; Wallis, P.; Allen-Zhu, Z.; Li, Y.; Wang,\nS.; and Chen, W. 2021. LoRA: Low-Rank Adaptation of\nLarge Language Models. ArXiv, abs/2106.09685.\nJain, N.; yeh Chiang, P.; Wen, Y.; Kirchenbauer, J.;\nChu, H.-M.; Somepalli, G.; Bartoldson, B.; Kailkhura, B.;\nSchwarzschild, A.; Saha, A.; Goldblum, M.; Geiping, J.; and\nGoldstein, T. 2023. NEFTune: Noisy Embeddings Improve\nInstruction Finetuning. ArXiv, abs/2310.05914.\nKarmarkar, N. 1984. A new polynomial-time algorithm for\nlinear programming. Combinatorica, 4: 373–395.\nLannelongue, L.; Grealey, J.; and Inouye, M. 2021. Green\nalgorithms: quantifying the carbon footprint of computation.\nAdvanced science, 8(12): 2100707.\nLaskar, M. T. R.; Hoque, E.; and Huang, J. 2021.\nDo-\nmain Adaptation with Pre-trained Transformers for Query-\nFocused Abstractive Text Summarization. Computational\nLinguistics, 48: 279–320.\nLewis, M.; Liu, Y.; Goyal, N.; Ghazvininejad, M.; rahman\nMohamed, A.; Levy, O.; Stoyanov, V.; and Zettlemoyer, L.\n2019. BART: Denoising Sequence-to-SequencePre-training\nfor Natural Language Generation, Translation, and Compre-\nhension. In Annual Meeting of the Association for Compu-\ntational Linguistics.\nLi, B.; Mellou, K.; qing Zhang, B.; Pathuri, J.; and Men-\nache, I. 2023. Large Language Models for Supply Chain\nOptimization. ArXiv, abs/2307.03875.\nLiu, H.; Tam, D.; Muqeeth, M.; Mohta, J.; Huang, T.;\nBansal, M.; and Raffel, C. 2022.\nFew-Shot Parameter-\nEfﬁcient Fine-Tuning is Better and Cheaper than In-Context\nLearning. ArXiv, abs/2205.05638.\nLoshchilov, I.; and Hutter, F. 2017. Decoupled Weight De-\ncay Regularization. In International Conference on Learn-\ning Representations.\nNash, J. C. 2000. The (Dantzig) simplex method for linear\nprogramming. Comput. Sci. Eng., 2: 29–31.\nOpenAI. 2023.\nGPT-4 Technical Report.\nArXiv,\nabs/2303.08774.\nRamamonjison, R.; Yu, T. T.; Li, R.; Li, H.; Carenini,\nG.; Ghaddar, B.; He, S.; Mostajabdaveh, M.; Banitalebi-\nDehkordi, A.; Zhou, Z.; and Zhang, Y. 2023.\nNL4Opt\nCompetition: Formulating Optimization Problems Based\non\nTheir\nNatural\nLanguage\nDescriptions.\nArXiv,\nabs/2303.08233.\nSuzgun, M.; Scales, N.; Scharli, N.; Gehrmann, S.; Tay, Y.;\nChung, H. W.; Chowdhery, A.; Le, Q. V.; hsin Chi, E. H.;\nZhou, D.; and Wei, J. 2022. Challenging BIG-Bench Tasks\nand Whether Chain-of-Thought Can Solve Them. In Annual\nMeeting of the Association for Computational Linguistics.\nTouvron, H.; Martin, L.; Stone, K. R.; Albert, P.; Alma-\nhairi, A.; Babaei, Y.; Bashlykov, N.; Batra, S.; Bhargava,\nP.; Bhosale, S.; Bikel, D. M.; Blecher, L.; Ferrer, C. C.;\nChen, M.; Cucurull, G.; Esiobu, D.; Fernandes, J.; Fu, J.; Fu,\nW.; Fuller, B.; Gao, C.; Goswami, V.; Goyal, N.; Hartshorn,\nA. S.; Hosseini, S.; Hou, R.; Inan, H.; Kardas, M.; Kerkez,\nV.; Khabsa, M.; Kloumann, I. M.; Korenev, A. V.; Koura,\nP. S.; Lachaux, M.-A.; Lavril, T.; Lee, J.; Liskovich, D.; Lu,\nY.; Mao, Y.; Martinet, X.; Mihaylov, T.; Mishra, P.; Moly-\nbog, I.; Nie, Y.; Poulton, A.; Reizenstein, J.; Rungta, R.;\nSaladi, K.; Schelten, A.; Silva, R.; Smith, E. M.; Subrama-\nnian, R.; Tan, X.; Tang, B.; Taylor, R.; Williams, A.; Kuan,\nJ. X.; Xu, P.; Yan, Z.; Zarov, I.; Zhang, Y.; Fan, A.; Kam-\nbadur, M.; Narang, S.; Rodriguez, A.; Stojnic, R.; Edunov,\nS.; and Scialom, T. 2023. Llama 2: Open Foundation and\nFine-Tuned Chat Models. ArXiv, abs/2307.09288.\nTsouros, D. C.; Verhaeghe, H.; Kadiouglu, S.; and Guns, T.\n2023. Holy Grail 2.0: From Natural Language to Constraint\nModels. ArXiv, abs/2308.01589.\nVaswani, A.; Shazeer, N. M.; Parmar, N.; Uszkoreit, J.;\nJones, L.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017.\nAttention is All you Need. In Neural Information Process-\ning Systems.\nYang, C.; Wang, X.; Lu, Y.; Liu, H.; Le, Q. V.; Zhou, D.;\nand Chen, X. 2023. Large Language Models as Optimizers.\nArXiv, abs/2309.03409.\n",
  "categories": [
    "cs.CL",
    "cs.IR"
  ],
  "published": "2024-03-02",
  "updated": "2024-03-02"
}