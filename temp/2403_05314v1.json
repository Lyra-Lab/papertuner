{
  "id": "http://arxiv.org/abs/2403.05314v1",
  "title": "Advances of Deep Learning in Protein Science: A Comprehensive Survey",
  "authors": [
    "Bozhen Hu",
    "Cheng Tan",
    "Lirong Wu",
    "Jiangbin Zheng",
    "Jun Xia",
    "Zhangyang Gao",
    "Zicheng Liu",
    "Fandi Wu",
    "Guijun Zhang",
    "Stan Z. Li"
  ],
  "abstract": "Protein representation learning plays a crucial role in understanding the\nstructure and function of proteins, which are essential biomolecules involved\nin various biological processes. In recent years, deep learning has emerged as\na powerful tool for protein modeling due to its ability to learn complex\npatterns and representations from large-scale protein data. This comprehensive\nsurvey aims to provide an overview of the recent advances in deep learning\ntechniques applied to protein science. The survey begins by introducing the\ndevelopments of deep learning based protein models and emphasizes the\nimportance of protein representation learning in drug discovery, protein\nengineering, and function annotation. It then delves into the fundamentals of\ndeep learning, including convolutional neural networks, recurrent neural\nnetworks, attention models, and graph neural networks in modeling protein\nsequences, structures, and functions, and explores how these techniques can be\nused to extract meaningful features and capture intricate relationships within\nprotein data. Next, the survey presents various applications of deep learning\nin the field of proteins, including protein structure prediction,\nprotein-protein interaction prediction, protein function prediction, etc.\nFurthermore, it highlights the challenges and limitations of these deep\nlearning techniques and also discusses potential solutions and future\ndirections for overcoming these challenges. This comprehensive survey provides\na valuable resource for researchers and practitioners in the field of proteins\nwho are interested in harnessing the power of deep learning techniques. By\nconsolidating the latest advancements and discussing potential avenues for\nimprovement, this review contributes to the ongoing progress in protein\nresearch and paves the way for future breakthroughs in the field.",
  "text": "SCIENCE CHINA\nInformation Sciences\n. REVIEW .\nAdvances of Deep Learning in Protein Science: A\nComprehensive Survey\nBozhen HU1,2, Cheng TAN2, Lirong WU2, Jiangbin ZHENG2, Jun XIA2,\nZhangyang GAO2, Zicheng LIU2, Fandi WU3, Guijun ZHANG4 & Stan Z. LI2*\n1Zhejiang University, Hangzhou 310058, China;\n2AI Division, School of Engineering, Westlake University, Hangzhou 310030, China;\n3Tencent AI Lab, Shenzhen 518054, China;\n4Zhejiang University of Technology, Hangzhou 310014, China\nAbstract\nProtein representation learning plays a crucial role in understanding the structure and function\nof proteins, which are essential biomolecules involved in various biological processes. In recent years, deep\nlearning has emerged as a powerful tool for protein modeling due to its ability to learn complex patterns and\nrepresentations from large-scale protein data. This comprehensive survey aims to provide an overview of the\nrecent advances in deep learning techniques applied to protein science. The survey begins by introducing\nthe developments of deep learning based protein models and emphasizes the importance of protein repre-\nsentation learning in drug discovery, protein engineering, and function annotation. It then delves into the\nfundamentals of deep learning, including convolutional neural networks, recurrent neural networks, attention\nmodels, and graph neural networks in modeling protein sequences, structures, and functions, and explores\nhow these techniques can be used to extract meaningful features and capture intricate relationships within\nprotein data. Next, the survey presents various applications of deep learning in the field of proteins, in-\ncluding protein structure prediction, protein-protein interaction prediction, protein function prediction, etc.\nFurthermore, it highlights the challenges and limitations of these deep learning techniques and also discusses\npotential solutions and future directions for overcoming these challenges. This comprehensive survey provides\na valuable resource for researchers and practitioners in the field of proteins who are interested in harnessing\nthe power of deep learning techniques. It is a hands-on guide for researchers to understand protein science,\ndevelop powerful protein models, and tackle challenging problems for practical purposes. By consolidating\nthe latest advancements and discussing potential avenues for improvement, this review contributes to the\nongoing progress in protein research and paves the way for future breakthroughs in the field.\nKeywords\nprotein representation learning, structure prediction, sequence and structure, function, graph\nneural network\nCitation\n1\nIntroduction\nProteins are the workhorses of life, playing an essential role in a broad range of applications ranging from\ntherapeutics to materials. They are built from twenty different basic chemical building blocks (called\namino acids), which fold into complex ensembles of three-dimensional (3D) structures that determine\ntheir functions and orchestrate the biological processes of cells [1]. Protein modeling is a vital field in\nbioinformatics and computational biology, aimed at understanding the structure, function, and interac-\ntions of proteins. With the rapid advancement of deep learning techniques, there has been a significant\nimpact on the field of protein [2], enabling more accurate predictions and facilitating breakthroughs in\nvarious areas of biological research.\nProtein structures determine their interactions with other molecules and their ability to perform spe-\ncific tasks. However, predicting protein structure from amino acid sequence is challenging because small\nperturbations in the sequence of a protein can drastically change the protein’s shape and even render it\n* Corresponding author (email: Stan.ZQ.Li@westlake.edu.cn)\narXiv:2403.05314v1  [q-bio.BM]  8 Mar 2024\nSci China Inf Sci\n2\nProtein models\nRecurrent models\nAttention models\nSupervised \nUnsupervised\nResidue\nembedding\nPair\nembedding\nProtein representation\n...\nDownstream tasks\n(pre-) Training tasks\nGNNs\nDatabases\n...\nFine-tune\nPSP\nPPI\nData\ncollection and\nprocessing\nCNNs\nFigure 1\nA general framework for deep learning models applied in protein, learning protein representations for various applica-\ntions.\nuseless, and the polypeptide is flexible and can fold into a staggering number of different shapes [3,4]. One\nway to find out the structure of a protein is to use an experimental approach, including X-ray crystallogra-\nphy, Nuclear Magnetic Resonance (NMR) Spectroscopy [5], and cryo-electron microscopy (cryo-EM) [6].\nUnfortunately, laboratory approaches for structure determination are expensive and cannot be used on\nall proteins. Therefore, protein sequences vastly outnumber available structures and annotations [7]. For\nexample, there are about 190K (thousand) structures in the Protein Data Bank (PDB) [8] versus over\n500M (million) sequences in UniParc [9] and only approximately 5M Gene Ontology (GO) term triplets\nin ProteinKG25 [10], including about 600K protein, 50K attribute terms.\nIn recent years, there has been a growing interest in applying deep learning techniques to proteins.\nResearchers have recognized the potential of deep learning models to learn complex patterns and extract\nmeaningful features from large-scale protein data, which includes information from protein sequences,\nstructures, functions, and interactions. One particular area of active research is protein representation\nlearning (PRL), which draws inspiration from approaches used in natural language processing (NLP)\nand aims to learn representations that can be utilized for various downstream tasks [11]. However, a\nmajor challenge in protein research is the scarcity of labeled data. Labeling proteins often requires time-\nconsuming and resource-intensive laboratory experiments, making it difficult to obtain sufficient labeled\ndata for training deep learning models. To address this issue, researchers have adopted a pre-train and\nfine-tune paradigm, similar to what has been performed in NLP. This approach involves pre-training a\nmodel on a pre-training task, where knowledge about the protein data is gained, and then fine-tuning the\nmodel on a downstream task with a smaller amount of labeled data. Self-supervised learning methods are\ncommonly employed during the pre-training phase to learn protein representations. One popular pretext\ntask is predicting masked tokens, where the model is trained to reconstruct corrupted tokens given the\nsurrounding sequence. Several well-known pre-trained protein encoders have been developed, including\nProtTrans [12], ESM models [13,14] and GearNet [15]. These pre-trained models have demonstrated their\neffectiveness in various protein tasks and have contributed to advancements in protein research. Figure 1\nillustrates the comprehensive pipeline of deep learning based protein models utilized for various tasks.\nDeep learning models for proteins are widely used in various applications such as protein structure\nprediction (PSP), property prediction, and protein design. One of the key challenges is predicting the\n3D structure of proteins from their sequences. Computational methods have traditionally taken two ap-\nproaches: focusing on (a) physical interactions or (b) evolutionary principles [16]. (a) The physics-based\napproach simulates the folding process of the amino acid chain using molecular dynamics or fragment as-\nsembly based on the potential energy of the force field. This approach emphasizes physical interactions to\nform a stable 3D structure with the lowest free energy state. However, it is highly challenging to apply this\nSci China Inf Sci\n3\n1. Introduction\n2. Definitions, notations and terms\n3. Baisc Neural networks\n4. Protein Foundations\nCNNs\nRNNs\nLSTM\nAtten-\ntion,\nTrans-\nformer\nGNNs\nBERT,\nGPT\nProtein\nand\nlangu-\nage\nPhysico-\nchemical\nproper-\nties\nMotifs,\nregions,\ndomains\nGeo-\nmetries\nStruc-\nture\nproper-\nties\n5. Deep learning based protein models \n6. Pretext task\nProtein language models\nProtein structure models\nProtein multimodal models\nSelf-supervised\nSupervised\n7. Downstream tasks\nProtein structure prediction\nProtein design\nProtein properties prediction\n8. Insights and future outlooks\nFunc-\ntions\nFigure 2\nA general diagram of the organization of this paper.\napproach to moderately sized proteins due to the computational complexity of molecular simulation, the\nlimited accuracy of fragment assembly, and the difficulty in accurately modeling protein physics [17,18].\n(b) On the other hand, recent advancements in protein sequencing have resulted in a large number\nof available protein sequences [19, 20], enabling the generation of multiple sequence alignments (MSAs)\nfor homologous proteins. With the availability of these large-scale datasets and the development of deep\nlearning models, evolutionary-based models such as AlphaFold2 (AF2) [16] and recent works [21–24] have\nachieved remarkable success in PSP. As researchers continue to explore the potential of these models,\nthey are now focusing on developing even deeper models to address more challenging problems that have\nyet to be solved.\nIn the following sections, we provide definitions, commonly used terms, and explanations of various\ndeep learning architectures that have been employed in protein research. These architectures include\nconvolutional neural networks (CNNs), recurrent neural networks (RNNs), transformer models, and graph\nneural networks (GNNs). Although deep learning models have been increasingly applied in the field\nof protein research, there is still a need for a systematic summary of this fast-growing field. Existing\nsurveys related to protein research focus mainly on biological applications [25–27], without delving deeper\ninto other important aspects, such as comparing different pre-trained protein models. We explore how\nthese architectures have been adapted to be used as protein models, summarize and contrast the model\narchitectures used for learning protein sequences, structures, and functions. Besides, the models optimized\nfor protein-related tasks are discussed, such as PSP, protein-protein interaction (PPI) prediction, and\nprotein property prediction, with their innovations and differences being highlighted. Furthermore, a\ncollection of resources is also provided, including deep protein methods, pre-training databases, and\npaper lists1)2). Finally, this survey presents the limitations and unsolved problems of existing methods\nand proposes possible future research directions. An overview of this paper’s organization is shown in\nFigure 2.\nTo the best of our knowledge, this is the first comprehensive survey for proteins, specifically focusing on\nlarge-scale pre-training models and their connections, contrasts, and developments. Our goal is to assist\nresearchers in the field of protein and artificial intelligence (AI) in developing more suitable algorithms\n1) https://github.com/bozhenhhu/A-Review-of-pLMs-and-Methods-for-Protein-Structure-Prediction\n2) https://github.com/LirongWu/awesome-protein-representation-learning\nSci China Inf Sci\n4\nand addressing essential, challenging, and urgent problems.\n2\nDefinitions, Notations, and Terms\n2.1\nMathematical Definitions\nThe sequence of amino acids can be folded into a stable 3D structure, which can be represented by a\n3D graph as G = (V, E, X, E), where V = {vi}i=1,...,n and E = {εij}i,j=1,...,n denote the vertex and edge\nsets with n residues, respectively, and P = {Pi}i=1,...,n is the set of position matrices, where Pi ∈Rki×3\nrepresents the position matrix for node vi. We treat each amino acid as a graph node for a protein, then\nki depends on the number of atoms in the i-th amino acid. The node and edge feature matrices are\nX = [xi]i=1,...,n and E = [eij]i,j=1,...,n, the feature vectors of node and edge are xi ∈Rd1 and eij ∈Rd2,\nd1 and d2 are the initial feature dimensions. The goal of protein graph representation learning is to form\na set of low-dimensional embeddings z for each protein, which is then applied in various downstream\ntasks.\nN-terminus DIVLTQSPSS...SFNRNEC C-terminus\n...\nD\nV\nI\nN\nE\nC\nPrimary structure\nSecondary structure\nTertiary structure\nQuaternary structure\nAlpha helix\nPleated sheet\nFigure 3\nFour different levels of protein structures [28,29].\n2.2\nNotations and Terms\n• Sequence/primary structure: The linear sequence of amino acids in a peptide or protein [30].\nAny sequence of polypeptides is reported starting from the single amine (N-terminus) end to car-\nboxylic acid (C-terminus) [31] (refer to Figure 3).\n• Secondary structure (SS): The 3D form of local segments of proteins. The two most common\nsecondary structural elements are α-helix (H) and β-strand (E); 3-state SS includes H, E, C (coil\nregion); 8 fine-grained states include three types for helix (G for 310-helix, H for α-helix, and I for\nπ-helix), two types for strand (E for β-strand and B for β-bridge), and three types for coil (T for\nβ-turn, S for high curvature loop, and L for irregular) [32].\n• Tertiary structure: The 3D arrangement of its polypeptide chains and their component atoms.\n• Quaternary structure: The 3D arrangement of the subunits in a multisubunit protein [33].\n• Multiple sequence alignment (MSA): The result of the alignment of three or more biological\nsequences (protein or nucleic acid).\n• Sequence homology: The biological homology between sequences (proteins or nucleic acids) [34].\nMSA assumes all the sequences to be aligned may share recognizable evolutionary homology [35] and\nis used to indicate which regions of each sequence are homologous.\n• Coevolution: The interdependence between the evolutionary changes of two entities [36] plays\nan important role at all biological levels, which is evident between protein residues (see Figure 7(a)).\n• Templates: The homologous 3D structures of proteins.\n• Contact map: A two-dimensional binary matrix represents the residue-residue contacts of a\nprotein within a distance threshold [37].\n• Protein structure prediction (PSP): The prediction of the 3D structure of a protein from\nits amino acid sequence.\n• Orphan proteins: Proteins without any detectable homology [38] (MSAs of homologous pro-\nteins are not available).\n• Antibody: A Y-shaped protein is produced by the immune system to detect and neutralize\nSci China Inf Sci\n5\nharmful substances, such as viruses and pathogenic bacteria.\n• Ribonucleic acid (RNA): A polymeric molecule essential in various biological roles, including\ntranscription, translation, and gene regulation, most often single-stranded.\n• Protein complex: A form of quaternary structure associated with two or more polypeptide\nchains.\n• Protein conformation: The spatial arrangement of its constituent atoms that determines the\noverall shape [39].\n• Protein energy function: Proteins fold into 3D structures in a way that leads to a low-energy\nstate. Protein-energy functions are used to guide PSP by minimizing the energy value.\n• Gene Ontology (GO): GO is a widely used bioinformatics resource that provides a stan-\ndardized vocabulary to describe the functions, processes, and cellular locations of genes and gene\nproducts, organizing biological knowledge into three main domains: molecular function (MF), cellular\ncomponent (CC), and biological process (BP).\n• Monte Carlo methods: A class of computational mathematical algorithms that use repeated\nrandom sampling to estimate the possible outcomes of an uncertain event.\n• Supervised learning: The use of labeled input-output pairs to learn a function that can\nclassify data or predict outcomes accurately.\n• Unsupervised learning: Models are trained without a labeled dataset and encouraged to\ndiscover hidden patterns and insights from the given data.\n• Natural language processing (NLP): The ability of computer programs to process, analyze,\nand understand the text and spoken words in much the same way humans can.\n• Language model (LM): A LM is a statistical model that is trained to predict the probability\nof a sequence of words in a given language.\n• Embedding: An embedding is a low-dimensional, learned continuous vector representation of\ndiscrete variables into which you can translate high-dimensional and real-valued vectors (words or\nsentences) [40].\n• Convolution neural networks (CNNs): A class of neural networks that consist of convolu-\ntional operations to capture the local information.\n• Recurrent neural networks (RNNs): A class of neural networks where connections between\nnodes form a directed or undirected graph along a temporal sequence.\n• Attention models: A class of neural network architectures that are able to focus their com-\nputation on specific parts of their input or memory [41].\n• Graph neural networks (GNNs): A type of deep learning model specifically designed to\noperate on graph-structured data.\n• Tansfer learning: A machine learning method where a model developed for one task is reused\nfor a model to solve a different but related task [42, 43], which has two major activities, i.e., pre-\ntraining and fine-tuning.\n• Pre-training: A strategy in AI refers to training a model with one task to help it form\nparameters that can be used in other tasks.\n• Fine-tuning: A method that takes the weights of a pre-trained neural network, which are used\nto initialize a new model being trained on the same domain.\n• Autoregressive language model: A feed-forward model predicts the future word from a set\nof words given a context [44].\n• Masked language model: A LM masks some of the words in a sentence and predicts which\nwords should replace those masks.\n• Bidirectional language model: A LM learns to predict the probability of the next token in\nthe past and future directions [45].\n• Multi-task learning: A machine learning paradigm in which multiple tasks are solved simul-\ntaneously while exploiting commonalities and differences across tasks [46].\n• Sequence-to-Sequence (Seq2Seq): A family of machine learning approaches train models\nto convert sequences from one domain to sequences in another domain.\n• Knowledge distillation: The process of transferring the knowledge from a large model or set\nof models to a single smaller model [47].\n• Multi-modal learning: Training models by combining information obtained from more than\none modality [48,49].\nSci China Inf Sci\n6\nFigure 4\nAn illustration of CNN.\n• Residual neural network: A neural network in which skip connections or shortcuts are used\nto jump over some layers, e.g., the deep residual network, ResNet [50].\n3\nBasic Neural Networks in Protein Modeling\nThis section initiates by introducing fundamental deep learning architectures, delineated into four pri-\nmary categories: CNNs, RNNs, attention mechanisms, and GNNs. Notably, attention models like trans-\nformers [51] and message passing mechanisms receive particular emphasis. Subsequently, two frequently\nemployed LMs, Bidirectional encoder representations from transformers (BERT) [52] and Generative pre-\ntrained transformer (GPT) [53–55], are outlined. These foundational neural networks typically serve as\nbuilding blocks for constructing intricate models in the realm of protein research.\n3.1\nConvolution Neural Networks\nCNNs are a type of deep learning algorithm that has revolutionized the field of computer vision [56].\nInspired by the visual cortex of the human brain [57], CNNs are particularly effective at analyzing and\nextracting features from images and other grid-like data.\nThe key idea behind CNNs is the use of convolutional layers, which apply filters or kernels to input\ndata to extract local patterns. These filters are small matrices that slide over the input data, performing\nelement-wise multiplications and summations to produce feature maps [58]. Convolution is a mathemati-\ncal operation that involves the integration of the product of two functions, where one function is reversed\nand shifted. In the context of deep learning, the convolution of two functions, denoted as f and g, can\nbe expressed as:\nConvolution: (f ∗g)(t) =\nZ\nτ∈Ω\ng(τ)f(t + τ)dτ,\n(1)\nhere, Ωrepresents a neighborhood in a given space. In deep learning applications, f(t) typically represents\nthe feature at position t, denoted as f(t) = ft ∈Rd1×1, where d1 refers to the number of input feature\nchannels. On the other hand, g(τ) ∈Rd×d1 is commonly implemented as a parametric kernel function,\nwith d representing the number of output feature channels [59].\nBy stacking multiple convolutional layers, CNNs can learn increasingly complex and abstract features\nfrom the input data. In addition to convolutional layers, CNNs typically include pooling layers, which\nreduce the spatial dimensions of the feature maps while preserving the most important information. Pool-\ning helps to make the network more robust to variations in the input data and reduces the computational\ncomplexity.\nCNNs also incorporate fully connected layers at the end of the network, which perform\nclassification or regression tasks based on the extracted features. Figure 4 shows a framework of deep\nCNNs. The versatility and effectiveness of CNNs have made them a fundamental tool in the field of deep\nlearning and have contributed to significant advancements in various domains.\n3.2\nRecurrent Neural Networks and Long Short-term Memory\nThe first example of LM was studied by Andrey Markov, who proposed the Markov chain in 1913 [60,61].\nAfter that, some machine learning methods, particularly hidden Markov models and their variants, have\nbeen described and applied as fundamental tools in many fields, including biological sequences [62]. The\ngoal is to recover a data sequence that is not immediately observable [63–65].\nSci China Inf Sci\n7\nSince the 2010s, neural networks have started to produce superior results in various NLP tasks [66].\nRNNs allow previous outputs to be used as inputs while having hidden states to exhibit temporal dynamic\nbehaviors. Therefore, RNNs can use their internal states to process variable-length sequences of inputs,\nwhich are useful and applicable in NLP tasks [67]. In a recent development, Google DeepMind introduced\nHawk, an RNN featuring gated linear recurrences, alongside Griffin, a hybrid model blending gated linear\nrecurrences with local attention mechanisms [68], which match the hardware efficiency of transformers [51]\nduring training.\nOutput layer\nHidden layers\nInput layer\nUnfold\n...\n...\n(a) RNNs\nNeural\nnetwork layer\nPointwise\noperation\n(b) LSTM cell\nFigure 5\nGraphical explanation of RNNs and LSTM.\nRNNs are typically shown in Figure 5(a). In each timestep t, the input xt ∈Rd1, hidden ht ∈Rd and\noutput state vectors ot ∈Rd, where the superscripts d1 and d refer to the number of input features and\nthe number of hidden units, respectively, are formulated as follows:\nht = g (Wxxt + Whht−1 + bh)\n(2)\not = g (Wyht + by)\n(3)\nwhere Wx ∈Rd×d1, Wh ∈Rd×d and Wy ∈Rd×d are the weights associated with the input, hidden and\noutput vectors in the recurrent layer, and bh ∈Rd, by ∈Rd are the bias, which are shared temporally,\ng(·) is the activation function.\nIn order to deal with the vanishing gradient problem [69] that can be encountered when training\ntraditional RNNs, LSTM networks are developed to process sequences of data. They present superior\ncapabilities in learning long-term dependencies [70] with various applications such as time series predic-\ntion [71], protein homology detection [72], drug design [73], etc. Unlike standard LSTM, bidirectional\nLSTM (BiLSTM) adds one more LSTM layer, reversing the information flow direction. This means it is\ncapable of utilizing information from both sides and is also a powerful tool for modeling the sequential\ndependencies between words and phrases in a sequence [74].\nThe LSTM architecture aims to provide a short-term memory that can last more timesteps, shown in\nFigure 5(b), Sigmoid(·) and tanh(·) represent the sigmoid and tanh layer. Forget gate layer in the LSTM\nis to decide what information is going to be thrown away from the cell state at timestep t, xt ∈Rd1,\nht ∈(−1, 1)d and ft ∈(0, 1)d are the input, hidden state vectors and forget gate’s activation vector.\nft = Sigmoid(Wfxt + Ufht−1 + bf)\n(4)\nThen, the input gate layer decides which values should be updated, and a tanh layer creates a vector\nof new candidate values, ˜Ct ∈(−1, 1)d that could be added to the state, it ∈(0, 1)d is the input gate’s\nactivation vector.\nit = Sigmoid(Wixt + Uiht−1 + bi)\n(5)\n˜Ct = tanh(Wcxt + Ucht−1 + bc)\n(6)\nNext, we combine old state Ct−1 ∈Rd and new candidate values ˜Ct ∈(−1, 1)d to create an update to\nthe new state Ct ∈Rd.\nCt = ft ⊙Ct−1 + it ⊙˜Ct\n(7)\nFinally, the output gate layer decides what parts of the cell state to be outputted, ot ∈(0, 1)d.\not = Sigmoid(Woxt + Uoht−1 + bo)\n(8)\nSci China Inf Sci\n8\nGlobal\nattention\nLocal\nattention\n(a) Attention mechanism\nInput\nInput\nembedding\nPositional\nencoding\nOutput\nOutput\nembedding\nMulti-head\nattention\nAdd & Norm\nFeed-forward\nAdd & Norm\nMasked Multi-\nhead attention\nAdd & Norm\nMulti-head\nattention\nAdd & Norm\nFeed-forward\nAdd & Norm\nMult-head attention\nV\nK\nQ\nScale Dot-Product\nAttention\nLinear\nLinear\nLinear\nh\nN\nConcat\nLinear\nScale Dot-Product\nAttention\nMatMul\nK\nQ\nScale\nMask\n(Optional)\nSoftmax\nMatMul\nV\n(b) The Transformer\nFigure 6\nGraphical explanation of attention mechanism and the architecture of Transformer.\nht = ot ⊙tanh (Ct)\n(9)\nwhere {Wf, Wi, Wc, Wo} ∈Rd×d1, {Uf, Ui, Uc, Uo} ∈Rd×d and {bf, bi, bc, bo} ∈Rd are weight matrices\nand bias vector parameters in the LSTM cell, ⊙means the pointwise multiplication.\n3.3\nAttention Mechanism and Transformer\nTraditional Sequence-to-Sequence (Seq2Seq) models typically use RNNs or LSTMs as encoders and de-\ncoders [75] to process sequences and extract features for various tasks.\nHowever, these models have\nlimitations, such as the final state of the RNNs or LSTMs needing to hold information for the entire\ninput sequence, which can lead to information loss. To overcome these limitations, attention mecha-\nnisms [76,77] have been introduced, which can be divided into two categories, local attention, and global\nattention (refer to Figure 6(a)). They allow models to focus on specific parts of the input sequence that\nare relevant to the task at hand. The basic idea behind attention is to assign weights to different elements\nof the input sequence based on their relevance to the current step of the output sequence generation.\nAttention mechanisms are first applied in machine translation [76] and have gradually replaced traditional\nRNNs and LSTMs.\nThe attention layer in a model can access all previous states and learn their importance by assigning\nweights to them. In 2017, Google Brain introduced the Transformer architecture [51], which completely\neliminates recurrence and convolutions.\nThis breakthrough leads to the development of pre-trained\nmodels such as BERT and GPT, which are trained on large language datasets. Unlike RNNs, as shown\nin Figure 6(b), the Transformer processes the entire input simultaneously using N stacked self-attention\nlayers for both the encoder and decoder. Each layer consists of a multi-head attention module followed\nby a feed-forward module with a residual connection and normalization. The basic attention mechanism\nused in Transformer is called ”Scaled Dot-Product Attention” and operates as follows:\nAtt(Q, K, V ) = Softmax\n\u0012QKT\n√\nd\n\u0013\nV\n(10)\nwhere Q, K, V ∈Rl×d are d-dimensional vector representations of l words in sequences of queries, keys\nand values, respectively. The multi-head attention mechanism allows the model to attend to different\nrepresentation subspaces in parallel. The multi-head attention module can be defined as follows:\nMultiHead (Q, K, V ) = Concat (head1, . . . , headh) W O\nwhere headi = Att\n\u0010\nQW Q\ni , KW K\ni , V W V\ni\n\u0011\nSci China Inf Sci\n9\nwhere the projections are parameter matrices W Q\ni\n∈Rd×di, W K\ni\n∈Rd×di, W V\ni\n∈Rd×di and W O ∈\nRhdi×d, di = d/h, there are h parallel attention layers or heads. Additionally, the Transformer architecture\nincludes position-wise feed-forward networks, which consist of two linear transformations with a ReLU\nactivation in between. Positional encoding is also added to the embedding at the bottom of the encoder\nand decoder stacks to incorporate the order of the sequence.\n3.4\nGraph Neural Networks\nUnlike traditional neural networks, which operate on grid-like data structures such as images or se-\nquences, GNNs are specifically designed to handle data represented as graphs. In a graph, data entities\nare represented as nodes, and the relationships between these entities are captured by edges. This flex-\nible and expressive representation makes GNNs well-suited for a wide range of applications, including\nsocial network analysis [78], recommendation systems [79], drug discovery [80], and knowledge graph\nreasoning [81].\nThe key idea behind GNNs is to learn node representations by aggregating information from their\nneighboring nodes. This is achieved through a series of message passing steps, where each node updates its\nrepresentation by incorporating information from its neighbors. By iteratively propagating and updating\ninformation across the graph, GNNs can capture complex dependencies and patterns in the data. Given\na protein 3D graph as G = (V, E, X, E) as presented in Subsection 2.1, a message passing layer can be\nexpressed as follows:\nhi = ϕ\n\nxi,\nM\nvj∈N(vi)\nψ (xi, xj, eij)\n\n\n(11)\nwhere L is a permutation invariant aggregation operator (e.g., element-wise sum), ϕ(·) and ψ(·) are\ndenoted as update and message functions, and N(vi) means the neighbors of node vi.\nThe variation of GNN models, including GCN [82], GAT [83], and GraphSAGE [84], differ in aggrega-\ntion strategies, attention mechanisms, and propagation rules, but they all share the fundamental idea of\nlearning node representations through the message passing mechanism. GNNs have the ability to capture\nboth local and global information, providing a powerful framework for understanding graph-structured\ndata.\n3.5\nLanguage Models\nIn order to effectively train deep neural models that can store knowledge for specific tasks using limited\nhuman-annotated data, the approach of transfer learning has been widely adopted.\nThis involves a\ntwo-step process: pre-training and fine-tuning [43,77,85].\nOver the past few years, there has been remarkable progress in the development of pre-trained LMs,\nwhich have found extensive applications in various domains such as NLP and computer vision. Among\nthese models, the transformer architecture has emerged as a standard neural architecture for both nat-\nural language understanding and generation. Notably, BERT and GPT are two landmark models that\nhave opened the doors to large-scale pre-training LMs. GPT [53] is designed to optimize autoregressive\nlanguage modeling during pre-training. It utilizes a transformer to model the conditional probability of\neach word, making it proficient in predicting the next token in a sequence. On the other hand, BERT [52]\nemploys a multi-layer bidirectional transformer encoder as its architecture. During the pre-training phase,\nBERT utilizes next-sentence prediction and masked language modeling strategies to understand sentence\nrelationships and capture contextual information.\nFollowing the introduction of GPT and BERT, numerous improvements and variants have been pro-\nposed by researchers. One notable trend has been the increase in model size and dataset size [86, 87].\nLarge transformer models have become the de facto standard in NLP, driven by scaling laws that govern\nthe relationship between overfitting, model size, and dataset size within a given computational bud-\nget [88, 89]. In November 2022, OpenAI released ChatGPT [90], which garnered significant attention\nfor its ability to understand human language, answer questions, write code, and even generate novels.\nIt stands as one of the most successful applications of large-scale pre-trained LMs. Moreover, as pre-\ntrained LMs have demonstrated their effectiveness and efficiency in various domains, they have gradually\nexpanded beyond NLP into fields such as finance, computer vision, and biomedicine [91–95].\nSci China Inf Sci\n10\n4\nProtein Foundations\nThis section delves into the fundamental aspects of proteins, exploring their intricate connections with\nhuman languages, physicochemical properties, structural geometries, and biological insights. The dis-\ncussion aims to unveil the foundational elements that contribute to a comprehensive understanding of\nproteins and their roles in various contexts.\n4.1\nProtein and Language\nLMs are increasingly being utilized in the analysis of large-scale protein sequence databases to acquire\nembeddings [96–98]. One significant reason for this trend is the shared characteristics between human\nlanguages and proteins. For instance, both exhibit a hierarchical organization [66, 99], where the four\ndistinct levels of protein structures (as depicted in Figure 3) can be analogized to letters, words, sentences,\nand texts in human languages. This analogy illustrates that proteins and languages consist of modular\nelements that can be reused and rearranged. Additionally, principles governing protein folding, such as\nthe hydrophilicity and hydrophobicity of amino acids, the principle of minimal frustration [100], and the\nfolding funnel landscapes of proteins [101], bear a resemblance to language grammars in linguistics.\nMSA\nResidue contact\nStatistical  inference\nConstraint\nCoevolution\nCoevolved positions\nQuagga differed from other\nzebras mainly in color – it had\nthe typical stripes on the head\nand neck only.\n(a) Protein\n(b) Sentence\nFigure 7\nComparisons of protein and language. (a) Relationship between a MSA and the residue contact of one protein in the\nalignment.\nThe positions that coevolved are highlighted in red and light blue.\nResidues within these positions where changes\noccurred are shown in blue.\nGiven such a MSA, one can infer correlations statistically found between two residues that these\nsequence positions are spatially adjacent, i.e., they are contacts [36, 102, 103]. (b) One grammatically complex sentence contains\nlong-distance dependencies (shown in bold).\nFigure 7(a) demonstrates the statistical inference of residue contacts in a protein based on a MSA. It\nhighlights the existence of long-range dependencies between two residues, where they may be distant in the\nsequence but spatially close, indicating coevolution. A similar phenomenon of long-distance dependencies\nis observed in human languages as well. Figure 7(b) provides an example of language grammar rules\nthat require agreement between words that are far apart [104]. These similarities suggest that successful\nmethods from NLP can be applied to analyze protein data. However, it is important to note that proteins\nare distinct from human languages, despite these shared characteristics. For instance, training LMs often\nnecessitates a vast corpus, which requires tokenization, i.e., breaking down the text into individual tokens\nor using words directly as tokens. This serves computational purposes and ideally aligns with linguistic\ngoals in NLP [97,98,105–107]. In contrast, protein tokenization methods are still at a rudimentary stage\nwithout a well-defined and biologically meaningful algorithm.\n4.2\nProtein Physicochemical Properties\nPhysicochemical properties of proteins refer to the characteristics and behaviors of proteins that are\ndetermined by their chemical and physical properties, playing a crucial role in protein structure, stability,\nfunction, and interactions. Only when the environment is suitable for the physicochemical properties of\na protein can it remain alive and play its role [108]. Understanding the physicochemical properties of\nproteins is crucial for developing new protein drugs. Certain physicochemical characteristics of proteins\nresemble those of amino acids, including amphoteric ionization, isoelectric point, color reaction, salt\nreaction, and more. However, there are also differences between proteins and amino acids in terms of\nSci China Inf Sci\n11\nproperties such as high molecular weight, colloid behavior, denaturation, and others. Several studies have\nutilized amino acid related physicochemical properties to gain insights into the biochemical nature of each\namino acid [109,110]. These properties include steric parameter, hydrophobicity, volume, polarizability,\nisoelectric point, helix probability, and sheet probability.\nThe importance of these features at the residue level has been calculated and visualized by HIGH-\nPPI [111]. They have found that features such as isoelectric point, polarity, and hydrogen bond acceptor\nfunction in protein-protein interaction interfaces. By analyzing the changes in evaluation scores before\nand after dropping each individual feature dimension from the model, they have identified topological\npolar surface area and octanol-water partition coefficient as dominant features for PPI interface charac-\nterization.\n4.3\nMotifs, Regions, and Domains\nMotifs, regions, and domains are commonly used as additional information for training deep learning\nmodels. A motif refers to a short, conserved sequence pattern or structural feature that is found in\nmultiple proteins or nucleic acids. It represents a functional or structural unit that is often associated\nwith a specific biological activity [112]. Motifs can be used as signatures to identify potential binding\nsites for ligands, substrates, or other interacting molecules. On the other hand, the term “region” denotes\na specific region of interest within a sequence. In contrast, a domain is an independent unit within a\nprotein or nucleic acid sequence, both structurally and functionally [113]. Domains can fold into stable\n3D structures and often perform specific functions. Figure 8 illustrates various data categories in the\nprotein field, providing an example of motifs, regions, and domains.\nHu et al. [114] have collected\nmultiple datasets that include 1364 categories of motifs, 3383 categories of domains, and 10628 categories\nof regions. It is important to note that these categories exhibit long-tailed distributions. Therefore, when\nworking with protein-related tasks, it is crucial to consider the multimodal property and long-tail effects\nof protein data.\n3D structure\nProtein sequence\nGo terms\nRegion\nDomain\nMotif\nDIVLTQSPSS...SFNRNEC \nGO:0032355;\nGO:0045471;\nGO:0033591;\nIMP binding;\nUDP-GlcNAc binding;\nInteraction with tRNA;\nProtein Kinase;\ntr-type G;\nHelicase ATP-binding;\nHIGH region;\nKMSKS region;\nNuclear localization signal;\nIELTQSPSSLSASLGGKVTITCKASQDIKKYIGWYQHK\nPGKQPRLLIHYTSTLLPGRGSGSGRDYSFSISNLEPEDI\nATYYNLRNVKWKIDGSERQNGVLNSWTDQDSKDST\nMotif 1\nDomain 1\nRegion 1\nMotif 2\nFigure 8\nAn overview of the multimodal dataset of proteins [114], including sequences, structures, GO terms, regions, domains,\nand motifs.\n4.4\nProtein Structure Geometries\nWang et al. [115] emphasize the importance of effectively utilizing multi-level structural information for\naccurate protein function prediction, where the four distinct levels are shown in Figure 3. They propose\nincorporating the PPI task during the pre-training phase to capture quaternary structure information.\nIn addition to considering multiple levels of protein structures, deep learning models can leverage hier-\narchical relationships to process tertiary structure information. For instance, ProNet [116] focuses on\nrepresentation learning for proteins with 3D structures at various levels, such as the amino acid, back-\nbone, or all-atom levels. At the amino acid level, ProNet considers the Cα positions of the structures.\nSci China Inf Sci\n12\nAt the backbone level, it incorporates the information of all backbone atoms (Cα, C, N, O). Finally, at\nthe all-atom level, ProNet processes the coordinates of both backbone and side chain atoms.\n(a)\nresidue \nresidue \n(b)\nFigure 9\nProtein structure geometries [119]. (a) The local coordinate system, Pi,Cα is the coordinate of Cα in residue i. (b)\nInterresidue geometries, including the distance (dij,Cβ ), three dihedral angles (ωij, θij, θji) and two planar angles (φij, φji).\nLocal Coordinate System\nThe locally informative features are developed from the local coordinate\nsystem (LCS) [117], shown in Figure 9(a), which is defined as:\nQi = [bi\nni\nbi × ni]\n(12)\nwhere ui =\nPi,Cα−Pi−1,Cα\n∥Pi,Cα−Pi−1,Cα∥, bi =\nui−ui+1\n∥ui−ui+1∥, ni =\nui×ui+1\n∥ui×ui+1∥, bi is the negative bisector of the angle\nbetween the rays (Pi−1,Cα −Pi,Cα) and (Pi+1,Cα −Pi,Cα), Pi,Cα represent the coordinate of atom Cα in\nnode vi, and ∥·∥denotes the l2-norm. It is clear to see that LCS is defined at the amino acid level. The\nspatial edge features e(1)\nij can be obtained considering the distance, direction, and orientation by LCS,\ne(1)\nij = Concat(∥dij,Cα∥, QT\ni ·\ndij,Cα\n∥dij,Cα∥, QT\ni · Qj)\n(13)\nwhere · is the matrix multiplication, and dij,Cα = Pi,Cα −Pj,Cα. This implementation obtains complete\nrepresentations at the amino acid level; as if we have Qi, the LCS Qj can be easily obtained by e(1)\nij .\nThus, LCS is widely utilized in protein design, antibody design, and PRL [117–119].\ntrRosetta Interresidue Geometries\nWe introduce the relative rotations and distances in trRosetta [120],\nincluding the distance (dij,Cβ), three dihedral angles (ωij, θij, θji) and two planar angles (φij, φji), as\nshown in Figure 9(b), where dij,Cβ = dji,Cβ, ωij = ωji, but θ and φ values depend on the order of residues.\nThese interresidue geometries define the relative locations of the backbone atoms of two residues in all\ntheir details [120], because the torsion angles of Ni −Cαi and Cαi −Ci do not influence their positions.\nTherefore, these six geometries are complete for amino acids at the backbone level for the radius graph,\nwhich are commonly used in PSP and protein model quality assessment [120–124]. The edge features e(2)\nij\ncan be obtained by these interresidue geometries, which contain the relative spatial information between\nany two neighboring amino acids.\ne(2)\nij = Concat(dij,Cβ, (sin ∧cos)(ωij, θij, φij))\n(14)\nBackbone Torsion Angles\nThe peptide bond exhibits partial double-bond character due to reso-\nnance [125], indicating that the three non-hydrogen atoms comprising the bond are coplanar, as shown\nin Figure 10, the free rotation about the bond is limited due to the coplanar property. The Ni −Cαi and\nCαi −Ci bonds, are the two bonds in the basic repeating unit of the polypeptide backbone. These single\nbonds allow unrestricted rotation until sterically restricted by side chains [126,127]. The coordinates of\nSci China Inf Sci\n13\npeptide\nplane\n1.33Å\n1.45Å\n1.52Å\n1.23Å\nbackbone\nFigure 10\nThe polypeptide chain depicting the characteristic backbone bond lengths, angles, and torsion angles (Ψi, Φi, Ωi).\nThe planar peptide groups are denoted as shaded gray regions, indicating that the peptide plane differs from the geometric plane\ncalculated from 3D positions [119].\nbackbone atoms based on these rigid bond lengths and angles are able to be determined with the remain-\ning degree of the backbone torsion angles Φi, Ψi, Ωi. The omega torsion angle around the C −N peptide\nbond is typically restricted to nearly 180◦(trans) but can approach 0◦(cis) in rare instances. Other than\nthe bond lengths and angles presented in Figure 10, all the H bond lengths measure approximately 1 ˚A.\nEuler Angles\nDifferent from the trRosetta interresidue geometries, Wang et al. [116] propose to use Eu-\nler angles to capture the rotation between two backbone planes. Unlike this protein design method [117],\nProNet [116] defines its local coordinate system for an amino acid i as yi = rN\ni −rCα\ni\n, ti = rC\ni −rCα\ni\n,\nand zi = ti × yi, xi = yi × zi, where the ri represents the position vector of the i-th amino acid in a\nprotein, this coordinate system is shown in Figure 11(a). The three Euler angles τ 1\nij, τ 2\nij and τ 3\nij between\ntwo backbone coordinate systems can be computed as shown in Figure 11(b), where n = zi × zj, is the\nintersection of two planes. τ 1\nij is the signed angle between n and xi, τ 2\nij is the angle between zi and\nzj, and τ 3\nij is the angle from n to xj. The relative rotations for any two amino acids i and j can be\ndetermined by these three Euler angles [116].\n(a)\n(b)\nFigure 11\nIllustration of Euler angles [116]. (a) The backbone coordinate system for an amino acid. (b) The three Euler angles\nbetween the backbone coordinate system for amino acids i and j.\nIn summary, the backbone torsion angles Φi, Ψi, Ωi, trRosetta interresidue geometries and Euler angles\nare defined at the backbone level, while the LCS is defined at the amino acid level. When considering the\npositions of all atoms, following the implementations in AF2 [16], the first four torsion angles χ1, χ2, χ3, χ4,\nare usually considered for side chain atoms, as only the amino acid arginine has five side chain torsion\nangles, and the fifth angle is close to 0 [116].\n4.5\nStructure Properties\nProteins can move in the 3D space through translations and rotations. These properties, such as trans-\nlation and rotation invariance, improve the accuracy, reliability, and usefulness of models in different\nprotein-related tasks. One example is GVP-GNN [128], which can process both scalar features and vec-\ntors, enabling the inclusion of detailed geometric information at nodes and edges without oversimplifying\nit into scalar values that may not fully represent complex geometry.\nSci China Inf Sci\n14\nInvariance and Equivariance\nWe examine affine transformations that maintain the distance between\nany two points, known as the isometric group SE(3) in Euclidean space. This group, denoted as the\nsymmetry group, encompasses 3D translations and the 3D rotation group SO(3) [129,130].\nThe collection of 4 × 4 real matrices of the SE(3) is shown as:\n\"\nR t\n0 1\n#\n=\n\n\nr11 r12 r13 t1\nr21 r22 r23 t2\nr31 r32 r33 t3\n0\n0\n0\n1\n\n\n,\n(15)\nwhere R ∈SO(3) and t ∈R3, SO(3) is the 3D rotation group. R satisfying RT R = I and det(R) = 1.\nGiven the function f : Rd →Rd′, assuming the given symmetry group G acts on Rd and Rd′, f is\nconsidered G-equivariant if it satisfies the following condition:\nf(Tgx) = Sgf(x), ∀x ∈Rd, g ∈G\n(16)\nhere, Tg and Sg represent the transformations. For the SE(3) group, when d\n′ = 1 and the output of f is\na scalar, we have\nf(Tgx) = f(x), ∀x ∈Rd, g ∈G\n(17)\nthus f is SE(3)-invariant [131].\nBy maintaining SE(3)-equivariance in structural geometries, protein models demonstrate the ability\nto recognize and interpret protein structures irrespective of transformations in 3D space. This enables\nthem to learn from a diverse array of protein structures and seamlessly apply that knowledge to predict\nthe functions of novel proteins.\nThe pivotal capability of generalization plays a critical role in PSP\nand protein design [132]. Notably, a considerable number of proteins exhibit symmetrical properties,\nsuch as recurring motifs or symmetric domains. SE(3)-equivariant models can effectively capture and\nleverage these symmetrical properties, thereby enhancing their comprehension of protein structures and\nfunctions [133].\nComplete Geometries\nA geometric transformation F(·) is complete if for two 3D graphs G1 =\n(V, E, P1) and G2 = (V, E, P2), there exists Tg ∈SE(3) such that the representations,\nF(G1) = F(G2) ⇐⇒P 1\ni = Tg(P 2\ni ), for i = 1, . . . n\n(18)\nthe operation Tg would not change the 3D conformation of a 3D graph [116,134,135]. P = {Pi}i=1,...,n is\nthe set of position matrices, F(G) ⇐⇒P, means positions can generate geometric representations, which\ncan also be recovered from them.\nGlobal completeness enhances the robustness of statistical analyses applied to protein structure data.\nProteins with SE(3) equivalence share identical 3D conformations but may differ in orientations and\npositions. To discern diverse conformers, it is essential to comprehensively model entire protein struc-\ntures. Solely focusing on local regions would overlook substantial long-range effects arising from subtle\nconformational changes occurring at a distance [119].\n4.6\nBiology Knowledge\nThe biology knowledge can enhance deep learning based protein models to understand the protein\nstructure-function relationships and enable various applications in bioinformatics.\nZhang et al. [10]\nhave constructed ProteinKG25, which provides large-scale biology knowledge facts aligned with pro-\ntein sequences, as shown in Figure 12(a). The attributes and relation terms are described using natural\nlanguages, which are extracted from, GO3), the world’s largest source of information on the functions of\ngenes and gene products (e.g., protein). In order to enhance protein sequences with text descriptions of\ntheir functions, Xu et al. [136] describe protein in four fields: protein name, functions, the location in a\ncell, and protein families that a protein belongs to (refer to Figure 12(b)).\n3) https://www.uniprot.org/uniprotkb\nSci China Inf Sci\n15\nQ1408\nPlasma\nMembrane\nCilium\nAssembly\nChannel\nActivity\nCilium\nAssembly\nCation\nTransport\nTerminal\nBouton\nABCA4\nSRRM1\nEnables\npart_of\nEnables\nbinding\npart_of\ninvolved_in\ninvolved_in\nPROTEIN NAME: 14-3-3-like protein GF14-F.\nFUNCTION: Is associated with a DNA binding complex that binds\nto the G box, a well-characterized cis-acting DNA regulatory\nelement found in plant genes.\nSUBCELLULAR LOCATION: Cytoplasm. Nucleus.\nSIMILARITY: Belongs to the 14-3-3 family.\n(a)\n(b)\nFigure 12\nExamples of proteins with biology knowledge. (a) A sub-graph in ProteinKG25 with proteins, GO terms, and re-\nlations [10]. Yellow nodes are proteins and blue nodes are GO entities with biological descriptions. (b) An example of protein\nproperty descriptions, including protein name, function texts, subcellular, and similarity [136].\n5\nDeep Learning based Protein Models\nIn this section, we summarize some commonly used deep learning models for processing protein data,\nincluding sequences, structures, functions or hybrid of them.\n5.1\nProtein Language Models\nDue to the inherent similarities between proteins and human languages, protein sequences, which are\nrepresented as strings of amino acid letters, naturally lend themselves to LMs.\nLMs are capable of\ncapturing complex dependencies among these amino acids [99]. Consequently, protein LMs have emerged\nas promising approaches for learning protein sequences, with the ability to handle both single sequences\nand MSAs as input.\nSingle Sequences\nTo begin, we introduce methods that mainly take a single sequence as input. Early\ndeep learning methods often utilized CNNs, LSTM, or their combinations [137–139], to predict protein\nstructural features and properties. Examples of such methods include DeepPrime2Sec [140], SPOT-1D-\nSingle [141]. Additionally, the Variational Auto-Encoder (VAE) [142, 143] has been employed to learn\ninteractions between positions within a protein. Given protein sequential data X and latent variables Z,\nthe protein VAE model aims to learn the joint probability p(X, Z) = p(Z)p(X|Z). However, computing\np(Z|X) from observed data necessitates evaluating the evidence term for each data point:\np(X) =\nZ\np(X|Z)p(Z)dZ\n(19)\ndirect computation of this integral is intractable. Consequently, VAE models typically adopt an Evidence\nLower BOund (ELBO) [142] to approximate p(X).\nTable 1\nLMs used in ProtTrans [12]\nModel\nNetwork\nPretext Task\n#Params.\nComments\nBERT [52]\nTransformer\nMasked Language Modeling\n340M\na commonly-used LM for predicting masked tokens\nALBERT [146]\nBERT\nMasked Language Modeling\n223M\na lite version of BERT\nTransformer-XL [147] Transformer Autoregressive Language Modeling\n257M\nenabling learn dependencies beyond a fixed length\nXLNet [87]\nBERT\nAutoregressive Language Modeling\n340M\nmore training data, integrates ideas from Transformer-XL\nELECTRA [148]\nBERT\nReplaced Token Detection\n335M\nfor token detection\nT5 [149]\nTransformer\nMasked Language Modeling\n11B\na text-to-text transfer learning framework\nAll examples report the largest model of their public series. Network displays high-level backbone models preferentially if they are used to\ninitialize parameters. #Param. means the number of parameters; M, millions; B, billions.\nSci China Inf Sci\n16\nSecondly, the pre-training of protein LMs in the absence of structural or evolutionary data has been\nexplored. Alley et al. [105] employ multiplicative long-/short-term memory (mLSTM) [144] to condense\narbitrary protein sequences into fixed-length vectors. Notably, TAPE [145] has introduced a benchmark\nfor protein models, including LSTM, Transformer, ResNet, among others, by means of self-supervised\npre-training and subsequent evaluation on a set of five biologically relevant tasks. Elnaggar et al. [12]\nsuccessfully trained six LMs (BERT [52], ALBERT [146], Transformer-XL [147], XLNet [87], ELEC-\nTRA [148] and T5 [149], show in Table 1) on protein sequences encompassing a staggering 393B amino\nacids, leveraging extensive computational resources (5616 GPUs and one TPU Pod). ESM-1b [14], on\nthe other hand, consists of a deep Transformer architecture (illustrated in Figure 13(a)) and a masking\nstrategy to construct intricate representations that incorporate contextual information from across the\nentire sequence. The outcomes of ProtTrans [12] and ESM-1b suggest that large-scale protein LMs pos-\nsess the ability to learn the underlying grammar of proteins, even without explicit utilization of apparent\nevolutionary information. With the support of large-scale databases, training resources, researchers have\nstarted exploring the boundaries of protein LMs by constructing billion-level models [13, 166, 169]. For\nexample, ProGen2 [166] demonstrates that large protein LMs can generate libraries of viable sequences,\nexpanding the sequence and structural space of natural proteins, obtaining the results suggest the scale of\nthe model size can be continued. Moreover, a large-scale protein LM, ESM-2 [13] with trainable parame-\nters up to 15B (billion), has achieved impressive results on the PSP task, surpassing smaller ESM models\nin terms of validation perplexity and TM-score [175]. Chen et al. have proposed a unified protein LM,\nxTrimoPGLM [170], to handle protein understanding and generation tasks concurrently, which involves\n100B parameters and 1 trillion training tokens. These pre-trained large-scale protein LMs showcase their\neffectiveness on various protein-related tasks. A summary of the pre-trained protein LMs and structure\nmodels is listed in Table 2.\nMSA\nRow\nAttention\nLayer-\nNorm\nLayer-\nNorm\nColumn \nAttention\nLayer-\nNorm\nFeed\nForward\nLayer-\nNorm\nSelf-\nAttention\nLayer-\nNorm\nFeed\nForward\nSequence\n(a) ESM-1b\n(b) MSA Transformer\nFigure 13\nCore modules of ESM-1b and MSA Transformer.\nMSA Sequences\nAs depicted in Figure 7(a), the inference of residue contact maps from MSA se-\nquences has been a long-standing practice in computational biology [176]. This approach has been relied\nupon in the early stages, as large protein LMs were not developed to extract implicit coevolutionary infor-\nmation from individual sequences. It is evident that additional information, such as MSAs, can enhance\nprotein embeddings. MSA Transformer [160] extends transformer-based LMs to handle sets of sequences\nas inputs by employing alternating attention over rows and columns, as illustrated in Figure 13(b). The\ninternal representations of MSA Transformer enable high-quality unsupervised structure learning with\nsignificantly fewer parameters compared to contemporary protein LMs. Additionally, AF2 [16] has lever-\naged row-wise and column-wise self-attentions to capture rich information within MSA representations.\n5.2\nProtein Structure Models\nProtein structures contain extremely valuable information that can be used for understanding biological\nprocesses and facilitating important interventions like the development of drugs based on structural\ncharacteristics or targeted genetic modifications [16]. In addition to sequence-based encoders, structure-\nbased encoders have been developed to leverage the 3D structural information of proteins.\nSci China Inf Sci\n17\nTable 2\nList of representative pre-trained protein LMs and structure models\nModel and Repository\nInput\nNetwork\n#Embedding #Param.\nPretext Task\nPre-training Dataset\nYear\nUniRep [105]\nSeq\nmLSTM [144]\n1900\n18.2M\nNext Amino Acid Prediction\nUniRef50\n2019\nTAPE [145]\nSeq\nLSTM, Transformer, ResNet\n-\n38M\nMasked Language Modeling\nPfam\n2019\nNext Amino Acid Prediction\nSeqVec [150]\nSeq\nELMo (LSTM) [151]\n1024\n93.6M\nNext Amino Acid Prediction\nUniRef50\n2019\nUDSMProt [152]\nSeq\nLSTM\n400\n24M\nNext Amino Acid Prediction\nSwiss-Prot\n2020\nCPCProt [153]\nSeq\nGRU [154], LSTM\n1024, 2048\n1.7M\nContrastive Predictive Coding\nPfam\n2020\nMuPIPR [155]\nSeq\nGRU, LSTM\n64\n-\nNext Amino Acid Prediction\nSTRING [156]\n2020\nProfile Prediction [157]\nMSA\nTransformer\n-\n-\nAlignment Profiles Prediction\nPfam\n2020\nPRoBERTa [158]\nSeq\nTransformer\n768\n44M\nMasked Language Modeling\nSwiss-Prot\n2020\nESM-1b [14]\nSeq\nTransformer\n1280\n650M\nMasked Language Modeling\nUniParc\n2021\nProtTXL [12]\nSeq\nTransformer-XL\n1024\n562M\nMasked Language Modeling\nBFD100, UniRef100\n2021\nProtBert [12]\nSeq\nBERT\n1024\n420M\nMasked Language Modeling\nBFD100, UniRef100\n2021\nProtXLNet [12]\nSeq\nXLNet\n1024\n409M\nMasked Language Modeling\nUniRef100\n2021\nProtAlbert [12]\nSeq\nALBERT\n4096\n224M\nMasked Language Modeling\nUniRef100\n2021\nProtElectra [12]\nSeq\nELECTRA\n1024\n420M\nMasked Language Modeling\nUniRef100\n2021\nProtT5 [12]\nSeq\nT5\n1024\n11B\nMasked Language Modeling\nUniRef50, BFD100\n2021\nPMLM [159]\nSeq\nTransformer\n1280\n715M\nMasked Language Modeling\nUniRef50\n2021\nMSA Transformer [160]\nMSA\nTransformer\n768\n100M\nMasked Language Modeling\nUniRef50, UniClust30\n2021\nProteinLM [161]\nSeq\nBERT\n-\n3B\nMasked Language Modeling\nPfam\n2021\nPLUS-RNN [162]\nSeq\nRNN\n2024\n59M\nMasked Language Modeling\nPfam\n2021\nSame-Family Prediction\nCARP [163]\nSeq\nCNN\n1280\n640M\nMasked Language Modeling\nUniRef50\n2022\nAminoBERT [22]\nSeq\nTransformer\n3072\n-\nMasked Language Modeling\nUniParc\n2022\nOmegaPLM [164]\nSeq\nGAU [165]\n1280\n670M\nMasked Language Modeling\nUniRef50\n2022\nSpan and Sequential Masking\nProGen2 [166]\nSeq\nTransformer\n4096\n6.4B\nMasked Language Modeling\nUniRef90, BFD30, BFD90 2022\nProtGPT2 [167]\nSeq\nGPT-2 [168]\n1280\n738M\nNext Amino Acid Prediction\nUniRef50\n2022\nRITA [169]\nSeq\nGPT-3 [55]\n2048\n1.2B\nNext Amino Acid Prediction\nUniRef100\n2022\nESM-2 [13]\nSeq\nTransformer\n5120\n15B\nMasked Language Modeling\nUniRef50\n2022\nxTrimoPGLM [170]\nSeq\nTransformer\n10240\n100B\nMasked Language Modeling\nUniref90\n2023\nSpan Tokens Prediction\nColAbFoldDB\nReprogBERT [171]\nSeq\nBERT\n768\n110M\nMasked Language Modeling\nEnglish Wikipedia\n2023\nBookCorpus\nPoET [172]\nMSA\nTransformer\n1024\n201M\nNext Amino Acid Prediction\nUniRef50\n2023\nCELL-E2 [173]\nSeq\nTransformer\n480\n35M\nMasked Language Modeling\nHuman Protein Atlas [174] 2023\nGraphMS [187]\nStruct\nGCN\n-\n-\nMultiview Contrast\nNeoDTI [188]\n2021\nCRL [189]\nStruct\nIEConv [186]\n2048\n36.6M\nMultiview Contrast\nPDB\n2022\nSTEPS [190]\nStruct\nGNN\n1280\n-\nDistance and Dihedral Prediction\nPDB\n2022\nExamples report the largest model of their public series. Seq: sequence, Struct: Structure. #Embedding means the dimension of embeddings; #Param.,\nthe number of parameters of network; M, millions; B, billions. Some models are linked with the GitHub repositories.\nInvariant Geometries\nFirstly, we introduce the modeling methods of distance and contact maps. A\ndistance map of proteins, also referred to as a residue-residue distance map, is a graphical representation\nthat shows the distances between pairs of amino acid residues in a protein structure. It provides valuable\ninformation about the spatial proximity between residues within the protein. Typically, the distance\nmap is calculated based on the positions of the Cα atoms, denoted as dij,Cα = Pi,Cα −Pj,Cα, where\nPi,Cα denotes the 3D position of Cα in the i-th residue. From a distance map, a contact map can be\nderived by assigning a value to each element representing the distance between two atoms. In practice,\ntwo residues (or amino acids) i and j of the same protein are considered in contact if their Euclidean\ndistance is below a specific threshold value, often set at 8 ˚A [177]. An example of a distance map and\ncontact map can be seen in Figure 14. CNNs, such as ResNet, are commonly employed to process these\nfeature maps [179], generating more accurate maps [180] or protein embeddings. Additionally, 3D CNNs\nhave been utilized to identify interaction patterns on protein surfaces [181]. It is worth noting that these\nfeature maps are often used in conjunction with protein sequences to provide supplementary information.\nFor instance, ProSE [46] incorporates structural supervision through residue-residue contact loss, along\nwith sequence masking loss, to better capture the semantic organization of proteins, leading to improved\nprotein function prediction. Furthermore, the recently proposed Struct2GO model [182] transforms the\nprotein’s 3D structure into a protein contact graph and utilizes amino acid-level embeddings as node\nrepresentations, enhancing the accuracy of protein function prediction.\nIn addition to distance and contact maps, there are other invariant features that can be used to cap-\nSci China Inf Sci\n18\n(a) Distance map\n(b) Contact map\nFigure 14\nAn example of protein distance and contact maps, visualized by MapPred [178].\nture the geometric properties of proteins. These include backbone torsion angles, trRosetta interresidue\ngeometries, and Euler angles, etc., which are described in Subsection 4.4. The simple approach to achieve\nSE(3) group symmetry in molecule geometry is through invariant modeling. Invariant modeling focuses\non capturing only the invariant features or type-0 features [131]. These type-0 features remain unchanged\nregardless of rotation and translation. To model the geometric relationships between amino acids, GNNs\nare commonly employed, including methods such as GCN [82,183,184], GAT [83], and GraphSAGE [84].\nTo represent the protein structures as graphs, the 1D and 2D features are used as node and edge features,\nsuch as the edge features e(1)\nij and e(2)\nij .\nIn the field of protein research, there are three common graph construction methods: sequential graph,\nradius graph, and k-Nearest Neighbors (kNN) graph [15]. Here, given a graph G = (V, E, X, E), with\nvertex and edge sets V = {vi}i=1,...,n and E = {εij}i,j=1,...,n, the sequential graph is defined based on\nthe sequence. If ∥i −j∥< lseq, an edge εij exists, where lseq is a hyperparameter. For the radius graph,\nan edge exists between nodes vi and vj if ∥dij,Cα∥< r, where r is a pre-defined radius. The kNN graph\nconnects each node to its k closest neighbors, with k commonly set to 30 [119, 185]. An illustration of\nthese graph construction methods is shown in Figure 15.\ni-1\ni\ni+1\ni+2\ni-2\ni+3\ni+4\nSequential edge\nRadius edge\nkNN edge\nFigure 15\nRelational protein residue graphs with sequential edges, radius edges, and kNN edges for residue i.\nTo leverage these invariant geometric features effectively, Hermosilla et al. [186] introduced protein\nconvolution and pooling techniques to capture the primary, secondary, and tertiary structures of proteins\nby incorporating intrinsic and extrinsic distances between nodes and atoms.\nFurthermore, Wang et\nal. [116] introduced complete geometric representations and a complete message passing scheme that\ncovers protein geometries at the amino acid, backbone, and all-atom levels. In Subsection 4.5, we provide\ndefinitions for complete geometries, which generally refer to 3D positions that can generate geometric\nrepresentations and can be recovered from them.\nSuch representations are considered complete.\nBy\nSci China Inf Sci\n19\nincorporating complete geometric representations into the commonly-used message passing framework\n(Eq. 11), a complete message passing scheme can be achieved [116,134,135].\nEquivariant Geometries\nInvariant modeling only captures the type-0 features, although protein can\nbe represented as a graph naturally, it remains under-explored mainly due to the significant challenges.\nFor example, it is challenging to extract and preserve multi-level rotation and translation equivariant\ninformation and effectively leverage the input spatial representations to capture complex geometries across\nthe spatial dimension. Thus higher-order particles include type-1 features like coordinates and forces in\nmolecular conformation are important to be considered [131]. An equivariant message passing paradigm\non proteins embeded into existing GNNs has been developed [191], like GBPNet [192], showing superior\nversatility in maintaining equivariance. AtomRefine [193] uses a SE(3)-equivariant graph transformer\nnetwork to refine protein structures, where each block in the SE(3) transformer network consists of one\nequivariant GCN attention block. Specifically, jing et al. [128] introduce geometric vector perceptrons\n(GVP) and operate directly on both scalar and vector features under a global coordinate system (refer\nto Figure 16).\nWe have introduced the definitions of invariance and equivariance in Subsection 4.5. Here, we introduce\nthe mechanisms of equivariant GNNs (EGNNs [194]) applied in protein. We consider a 3D graph as\nG = (V, E, X, E), Pi,Cα is the position of Cα in node vi, coordinate embeddings P(l)\nCα = {P (l)\ni,Cα}i=1,...,n.\nThe node and edge embeddings are H(l) = [h(l)\ni ]i=1,...,n and E = [eij]i,j=1,...,n, H(0) = X, the following\nequations can define the l-th equivariant message passing layer:\nmij = ψe\n\u0000hl\ni, hl\nj,\n\r\rP l\ni,Cα −P l\nj,Cα\n\r\r , eij\n\u0001\nP l+1\ni,Cα = P l\ni,Cα + C\nX\nvj∈N(vi)\n\u0000P l\ni,Cα −P l\nj,Cα\n\u0001\nψx (mij)\nmi =\nX\nvj∈N(vi)\nmij\nhl+1\ni\n= ϕ\n\u0000hl\ni, mi\n\u0001\n(20)\nhere, C is a constant number, ψe and ψx are the message functions, and ϕ is the update function.\nThe coordinate embeddings P(l)\nCα are updated by the weighted sum of all relative neighbors’ differences\n(P l\ni,Cα −P l\nj,Cα). The coordinate embeddings are also used to update the invariant node embeddings by\nthe l2-norm (∥·∥). These operations maintain the equivariance in GNNs.\nVector channel\nFC\nFC\nScale\nScalar channel\nFC\nNonlinear\nInput\nOutput\nFigure 16\nSchematic of the geometric vector perceptron in GVP-GNN [128]. FC: The linear weight in a fully connected layer.\n5.3\nProtein Multimodal Methods\nAs mentioned in Subsection 4.3, protein data encompasses various types of information, such as sequences,\nstructures, GO annotations, motifs, regions, domains, and more. To gain a comprehensive understanding\nof proteins, it is crucial to consider and integrate these multimodal sources of information.\nSci China Inf Sci\n20\nTable 3\nList of representative pre-trained protein multimodal models\nModel and Repository\nInput\nNetwork\n#Embedding #Param.\nPretext Task\nPre-training Dataset\nYear\nSSA [195]\nSeq, Struct\nBiLSTM\n100, 512\n-\nContact and Similarity Prediction\nPfam, SCOP\n2019\nLM-GVP [196]\nSeq, Struct\nProtBert, GVP\n1024\n420M\n-\nUniRef100\n2021\nDeepFRI [197]\nSeq, Struct\nLSTM, GCN\n512\n6.2M\nGO Term Prediction\nPfam\n2021\nHJRSS [198]\nSeq, Struct\nSE(3) Transformer\n128\n16M\nMasked Language and Graph Modeling\ntrRosetta2 [199]\n2021\nGraSR [200]\nSeq, Struct\nLSTM, GCN\n32\n-\nMomentum Contrast [202]\nSCOPe\n2022\nCPAC [203]\nSeq, Struct\nRNN, GAT\n128\n-\nMasked Language and Graph Modeling\nPfam\n2022\nMIF-ST [204]\nSeq, Struct\nCNN, GNN\n256\n640M\nMasked Language Modeling\nUniRef50\n2022\nPeTriBERT [293]\nSeq, Struct\nBERT\n3072\n40M\nNext Amino Acid Prediction\nAlphaFoldDB\n2022\nGearNet [15]\nSeq, Struct\nGNN\n512\n42M\nDistance, Angle and Residue Type Prediction\nAlphaFoldDB\n2022\nMultiview Contrast\nESM-GearNet [205]\nSeq, Struct\nESM, GearNet\n512\n692M\nDistance, Angle and Residue Type Prediction\nAlphaFoldDB\n2023\nMultiview Contrast, SiamDiff [206]\nSaProt [207]\nSeq, Struct\nESM-2\n1280\n650M\nMasked Language Modeling\nAlphaFoldDB, PDB\n2023\nProGen [208]\nSeq, Func\nTransformer\n1028\n1.2B\nNext Amino Acid Prediction\nUniparc, UniProtKB, Swiss-Prot 2020\nPfam, TrEMBL, NCBI [209]\nProteinBERT [210]\nSeq, Func\nTransformer\n512\n16M\nMasked Language Modeling\nUniRef90\n2021\nOntoProtein [10]\nSeq, Func\nProtBert, BERT\n1024\n-\nMasked Language Modeling\nProteinKG25 [10]\n2022\nEmbedding Contrast\nKeAP [211]\nSeq, Func\nBERT, PubMedBERT [213]\n1024\n520M\nMasked Language Modeling\nProteinKG25\n2023\nProtST [136]\nSeq, Func\nProtBert, ESM, PubMedBERT\n1024\n750M\nMasked Language Modeling\nProtDescribe [136]\n2023\nMASSA [114]\nSeq, Struct\nESM-MSA, GVP-GNN\n-\n-\nMasked Language Modeling\nUniProtKB, Swiss-Prot\n2023\nFunc\nTransformer, GraphGO\nAlphaFoldDB, RCSB PDB [214]\nProteinINR [212]\nSeq, Struct\nESM-1b, GearNet\n-\n-\nMultiview Contrast\n20 Species,\n2024\nSurface\nTransformer\nSwiss-Prot\nExamples report the largest model of their public series. Seq: sequence, Struct: Structure, Func: Function. #Embedding means the dimension of embeddings; #Param., the\nnumber of parameters of network; M, millions; B, billions. Some models are linked with the GitHub repositories.\nTable 4\nInformation of Protein Databases\nDataset\n#Proteins Disk Space\nDescription\nLink\nUniProtKB/Swiss-Prot [9]\n500K\n0.59GB\nknowledgebase\nhttps://www.uniprot.org/uniprotkb?query=*\nUniProtKB/TrEMBL [9]\n229M\n146GB\nknowledgebase\nhttps://www.uniprot.org/uniprotkb?query=*\nUniRef100 [215]\n314M\n76.9GB\nclustered sets of sequences\nhttps://www.uniprot.org/uniref?query=*\nUniRef90 [215]\n150M\n34GB\n90% identity\nhttps://www.uniprot.org/uniref?query=*\nUniRef50 [215]\n53M\n10.3GB\n50% identity\nhttps://www.uniprot.org/uniref?query=*\nUniParc [9]\n528M\n106GB\nsequence\nhttps://www.uniprot.org/uniparc?query=*\nPDB [8]\n180K\n50GB\n3D structure\nhttps://www.wwpdb.org/ftp/pdb-ftp-sites\nCATH4.3 [216]\n-\n1073MB\nhierarchical classification\nhttps://www.cathdb.info/\nBFD [217]\n2500M\n272GB\nsequence profile\nhttps://bfd.mmseqs.com/\nPfam [218]\n47M\n14.1GB\nprotein families\nhttps://www.ebi.ac.uk/interpro/entry/pfam/\nAlphaFoldDB [219]\n214M\n23 TB\npredicted 3D structures\nhttps://alphafold.ebi.ac.uk/\nESM Metagenomic Atlas [13]\n772M\n-\npredicted metagenomic protein structures\nhttps://esmatlas.com/\nColAbFoldDB [170]\n950M\n-\nan amalgamation of various metagenomic databases https://colabfold.mmseqs.com/\nProteinKG25 [10]\n5.6M\n147MB\na knowledge graph dataset with GO\nhttps://drive.google.com/file/d/1iTC2-zbvYZCDhWM wxRufCvV6vvPk8HR\nUniclust30 [220]\n-\n6.6GB\nclustered protein sequences\nhttps://uniclust.mmseqs.com/\nSCOP [221]\n-\n-\nstructural classification\nhttp://scop.mrc-lmb.cam.ac.uk/\nSCOPe [222]\n-\n86MB\nan extended version of SCOP\nhttp://scop.berkeley.edu\nOpenProteinSet [223]\n16M\n-\nMSAs\nhttps://dagshub.com/datasets/openproteinset/\nK, thousand; M, million, disk space is in GB or TB (compressed storage as text), which is estimated data influenced by the compressed format.\nSequence-structure Modeling\nIn recent years, there has been a growing focus on sequence-structure\nco-modeling methods, which aim to capture the intricate relationships between protein sequences and\nstructures. Rather than treating protein sequences and structures as separate entities, these methods\nleverage the complementary information from both domains to improve modeling performance.\nOne prevailing approach involves using pre-trained protein LMs, such as ESM-1b and ProtTrans, to\nobtain embeddings for sequences. For example, SPOT-1D-LM [224,225] and BIB [226] utilize pre-trained\nLMs to generate embeddings for contact map prediction and biological sequence design. To incorporate\nstructural information into protein LMs, LM-GVP [196] proposes a novel fine-tuning procedure that\nexplicitly injects inductive bias from complex structure information using GVP. This method enhances\nthe representation capability of protein LMs by considering both sequence and structure information,\nleading to improved performance in downstream tasks. Another approach, GearNet [15], simultaneously\nencodes sequential and spatial information by incorporating different types of sequential or structural\nedges. It performs node-level and edge-level message passing, enabling it to capture both local and global\ndependencies in protein sequences and structures. This comprehensive modeling approach has shown\npromising results in various applications.\nFoldseek [227] employs a VQ-VAE [228] model to encode\nprotein structures into informative tokens. These tokens combine residue and 3D geometric features,\neffectively representing both primary and tertiary structures. By representing protein structures as a\nSci China Inf Sci\n21\nDomain i\nGO j\nDomain\nembedding\nGO\nembedding\nHadamard\nproduct\np(GO j| Domain i)\n(a)\n(b)\nFigure 17\nIllustration of diverse protein modalities. (a) The method of calculating the conditional probability of a protein that\ncontains domain i having the GO j function in DomainPFP [230]. (b) The protein surface at different time steps in a molecular\ndynamics trajectory [232].\nsequence of these novel tokens, Foldseek seamlessly integrates the foundational models like BERT and\nGPT to process protein sequences and structures simultaneously, SaProt [207] is also an example of\nsuch integration, where it combines these tokens using general-purpose protein LMs. Without using the\nstructure information as input, Bepler and Berger [46] carry out multi-task with structural supervision,\nleading to an even better-organized embedding space compared with a single task.\nSequence-function Modeling\nProtein Sequence-function Modeling is a research field dedicated to\ncomprehending the intricate relationships between protein sequences and their functional properties.\nGO annotations provide valuable structured information about protein functions, enabling researchers\nto systematically analyze and compare protein functions across different species and experimental stud-\nies [229]. The function information is typically derived from prior biological knowledge, which can be\neasily incorporated into sentences, as depicted in Figure 12. Consequently, the study of protein sequences\nand functions often goes hand in hand, with LMs being commonly employed. One notable model in this\ndomain is ProteinBERT [210], which undergoes pre-training on protein sequences and GO annotations\nusing two interconnected BERT-like encoders.\nBy leveraging large-scale biology knowledge datasets,\nProteinKG25 [10], OntoProtein [10], on the other hand, focuses on reconstructing masked amino acids\nwhile simultaneously minimizing the embedding distance between contextual representations of proteins\nand associated knowledge terms. In comparison, KeAP [211] aims to explore the relationships between\nproteins and knowledge at a more granular level than OntoProtein.\nDiverse Modalities Modeling\nThere are models that leverage deep learning techniques to incorporate\ndiverse modalities, enabling a more comprehensive understanding of protein structure, function, and\ndynamics. For instance, MASSA [114] is an advanced multimodal deep learning framework designed to\nincorporate protein sequence, structure, and functional annotation. It employs five specific pre-training\nobjectives to enhance its performance, including masked amino acid and GO, placement capture of motifs,\ndomains, and regions. Domain-PFP [230], on the other hand, utilizes a self-supervised protocol to derive\nfunctionally consistent representations for protein domains. It achieves this by learning domain-GO co-\noccurrences and associations by calculating the probabilities of domains and GO terms as illustrated in\nFigure 17(a), resulting in an improved understanding of domain functionality. BiDock [231] is a robust rigid\ndocking model that effectively integrates MSAs and structural information. By employing a cross-modal\ntransformer through bi-level optimization, BiDock enhances the accuracy of protein docking predictions.\nTo capture protein dynamics, Sun et al. [232] focus on representing protein surfaces using implicit neural\nnetworks. This approach is particularly well-suited for modeling the dynamic shapes of proteins, which\noften exhibit complex and varied conformations, as shown in Figure 17(b). Representative pre-trained\nprotein multimodal models are listed in Table 3, and relative common datasets are presented in Table 4.\n5.4\nAssessment of Pre-training Methods for Protein Representation Learning\nIn this section, we enumerate various types of deep protein models, with a particular focus on PRL\nmethods commonly employed in diverse scenarios.\nThe evaluation encompasses several widely used\nSci China Inf Sci\n22\nTable 5\nAccuracy (%) of fold classification and enzyme reaction classification. The best results are shown in bold.\nMethod\nInput\nParam.\nPre-training\nFold Classification\nEnzyme\nDataset (Used)\nFold SuperFamily Family Reaction\nESM-1b [14]\nSeq\n650M\nUniRef50 (24M)\n26.8\n60.1\n97.8\n83.1\nProtBert-BFD [12]\nSeq\n420M\nBFD (2.1B)\n26.6\n55.8\n97.6\n72.2\nIEConv [186]\nStruct\n36.6M\nPDB (180K)\n50.3\n80.6\n99.7\n88.1\nDeepFRI [197]\nSeq, Struct\n6.2M\nPfam (10M)\n15.3\n20.6\n73.2\n63.3\nGearNet (Multiview Contras) [15]\nSeq, Struct\n42M\nAlphaFoldDB (805K) 54.1\n80.5\n99.9\n87.5\nGearNet (Residue Type Prediction) [15] Seq, Struct\n42M\nAlphaFoldDB (805K) 48.8\n71.0\n99.4\n86.6\nGearNet (Distance Prediction) [15]\nSeq, Struct\n42M\nAlphaFoldDB (805K) 50.9\n73.5\n99.4\n87.5\nGearNet (Angle Prediction) [15]\nSeq, Struct\n42M\nAlphaFoldDB (805K) 56.5\n76.3\n99.6\n86.8\nGearNet (Dihedral Prediction) [15]\nSeq, Struct\n42M\nAlphaFoldDB (805K) 51.8\n77.8\n99.6\n87.0\nSeq: sequence, Struct: Structure, Func: Function. Param., means the number of trainable parameters (B: billion; M:\nmillion; K: thousand). The dataset used here does not exceed the reported size as shown in Table 4.\npre-trained PRL methods applied to four distinct downstream tasks: (a) protein fold classification,\n(b) enzyme reaction classification, (c) GO term prediction, and (d) enzyme commission (EC) number\nprediction.\nFor protein fold classification (a), we adhere to the methodology outlined by Hermosilla et al. [186].\nThis dataset [222] comprises 16,712 proteins distributed across 1,195 fold classes, featuring three provided\ntest sets: Fold (excluding proteins from the same superfamily during training), SuperFamily (omitting\nproteins from the same family during training), and Family (including proteins from the same family\nin the training set). Enzyme reaction classification (b) is treated as a protein function prediction task\nbased on enzyme-catalyzed reactions defined by the four levels of enzyme commission numbers [233].\nThe dataset, compiled by Hermosilla et al. [186], encompasses 29,215 training proteins, 2,562 validation\nproteins, and 5,651 test proteins. For GO term prediction (c), the objective is to predict whether a\ngiven protein should be annotated with a specific GO term. The GO term prediction dataset includes\n29,898/3,322/3,415 proteins for training/validation/test, respectively. In EC number prediction (d), the\ngoal is to predict 538 EC numbers at the third and fourth levels of hierarchies for different proteins,\nfollowing the methodology of DeepFRI [197]. The training/validation/test datasets comprise a total of\n15,550/1,729/1,919 proteins. For both GO term and EC number prediction, the test sets only include\nPDB chains with a sequence identity no greater than 95% to the training set [59]. Similar settings are\nalso employed in LM-GVP [196], GearNet [15], ProtST [136], etc. These results are presented in Table 5\nand Table 6.\nWe can see that the protein multimodal modeling methods, such as GearNet [15], SaProt [207], and\nESM-GearNet-INR-MC [212], consistently deliver superior results across various tasks, showcasing the\neffectiveness of leveraging both sequence and structure information. This highlights the effectiveness of\nintegrating both sequence and structure information in protein modeling. Notably, methods with a higher\nnumber of trainable parameters, such as ESM-1b and ProtBERT-BFD, exhibit competitive performance,\nemphasizing the significance of model complexity in specific scenarios. These observations underscore\nthe pivotal role played by the pre-training dataset and model architecture choices in attaining superior\nperformance across various facets of protein modeling.\n6\nPretext Task\nIn pre-training, models are exposed to a large amount of unlabeled data to learn general representations\nbefore being fine-tuned on specific downstream tasks. The pretext task is typically designed to encourage\nthe model to learn useful features or patterns from the input data. Thus, the pretext task refers to a\nspecific objective or task that a machine learning model is trained on as part of a pre-training phase. We\nhave listed the pretext tasks in the Table 1-Table 3.\nSci China Inf Sci\n23\nTable 6\nFmax [15] of GO term prediction and EC number prediction. The best results are shown in bold.\nMethod\nInput\nParam.\nPre-training\nGO\nEC\nDataset (Used)\nBP\nMF\nCC\nESM-1b [14]\nSeq\n650M\nUniRef50 (24M)\n0.470\n0.657\n0.488\n0.864\nProtBERT-BFD [12]\nSeq\n420M\nBFD (2.1B)\n0.279\n0.456\n0.408\n0.838\nESM-2 [13]\nSeq\n650M\nUniRef50 (24M)\n0.472\n0.662\n0.472\n0.874\nIEConv [186]\nStruct\n36.6M\nPDB (180K)\n0.468\n0.661\n0.516\n-\nDeepFRI [197]\nSeq, Struct\n6.2M\nPfam (10M)\n0.399\n0.465\n0.460\n0.631\nLM-GVP [196]\nSeq, Struct\n420M\nUniRef100 (216M)\n0.417\n0.545 0.527 0.664\nGearNet (Multiview Contrast) [15]\nSeq, Struct\n42M\nAlphaFoldDB (805K)\n0.490\n0.654\n0.488\n0.874\nGearNet (Residue Type Prediction) [15]\nSeq, Struct\n42M\nAlphaFoldDB (805K) 0.430\n0.604\n0.465\n0.843\nGearNet (Distance Prediction) [15]\nSeq, Struct\n42M\nAlphaFoldDB (805K) 0.448\n0.616\n0.464\n0.839\nGearNet (Angle Prediction) [15]\nSeq, Struct\n42M\nAlphaFoldDB (805K) 0.458\n0.625\n0.473\n0.853\nGearNet (Dihedral Prediction) [15]\nSeq, Struct\n42M\nAlphaFoldDB (805K) 0.458\n0.626\n0.465\n0.859\nESM-GearNet [205]\nSeq, Struct\n692M\nAlphaFoldDB (805K) 0.488\n0.681\n0.464\n0.890\nSaProt [207]\nSeq, Struct\n650M\nAlphaFoldDB (805K) 0.356\n0.678\n0.414\n0.884\nKeAP [211]\nSeq, Func\n520M\nProteinKG25 (5M)\n0.466\n0.659\n0.470\n0.845\nProtST-ESM-1b [136]\nSeq, Func\n759M\nProtDescribe (553K)\n0.480\n0.661\n0.488\n0.878\nProtST-ESM-2 [136]\nSeq, Func\n759M\nProtDescribe (553K)\n0.482\n0.668\n0.487\n0.878\nESM-GearNet-INR-MC [212]\nSeq, Struct, Surface\n-\nSwiss-Prot\n0.518 0.683 0.504 0.896\nSeq: sequence, Struct: Structure, Func: Function. Param., means the number of trainable parameters (B: billion; M:\nmillion; K: thousand). The dataset used here does not exceed the reported size as shown in Table 4.\n6.1\nSelf-supervised Pretext Task\nThe self-supervised pretext tasks leverage the available training data as supervision signals, eliminating\nthe requirement for additional annotations [234].\nPredictive Pretext Task\nThe goal of predictive methods is to generate informative labels directly\nfrom the data itself, which are then utilized as supervision to establish and manage the relationships\nbetween data and their corresponding labels.\nAs we have stated in Subsection 3.5, GPT is developed to enhance the performance of autoregressive\nlanguage modeling in the pre-training phase. Formally, given an unsupervised corpus of tokens X =\n{x0, x1, . . . , xn, xn+1}, GPT employs a standard language modeling objective to maximize the likelihood:\nLnext(X) =\nn+1\nX\ni=1\nlog P (xi | xi−k, . . . , xi−1; ΘP )\n(21)\nhere, k represents the size of the context window, and the conditional probability P is modeled using a\nnetwork decoder with parameters ΘP . The next token prediction is a fundamental aspect of autoregressive\nlanguage modeling. The model learns to generate coherent and meaningful sequences by estimating the\nprobability distribution over the vocabulary and selecting the most likely next token.\nFor the masked language modeling in BERT, formally, given a corpus of tokens X = {x0, x1, . . . , xn, xn+1},\nBERT maximizes the likelihood as follows:\nLmasked(X) = P\nx∈mask(X) log P(x | ˜\nX; ΘP )\n(22)\nmask(X) represents the masked tokens, ˜\nX is the result obtained after masking certain tokens in X, and\nthe probability P is modeled by the transformer encoder with parameters ΘP .\nBidirectional language modeling is to model the probability of a token based on both the preceding\nand following tokens. Formally, given a corpus of tokens X = {x0, x1, . . . , xn, xn+1}, k is the size of the\ncontext window, and xi denotes the i-th token in the sequence:\nLbi(X) =\nn+1\nX\ni=1\n[log P(xi | xi−k, . . . , xi−1; ΘP ) + log P(xi | xi+1, . . . , xi+k; ΘP )]\n(23)\nSci China Inf Sci\n24\nSTART\nD\nI\nV\nL\nT\nQ\nEND\nAutoregressive language modeling\nSTART\nD\nI\nV\nL\nT\nQ\nEND\nBidirectional language modeling\nSTART\nD\nI\nL\nT\nQ\nEND\nMasked language modeling\nV\nFigure 18\nDiagram of language modeling approaches, including the autoregressive, masked and bidirectional language modeling\nstrategies [46].\nInstead of predicting the likelihood of an individual amino acid, PMLM [159] suggests modeling the\nlikelihood of a pair of masked amino acids. Besides, there are span masking and sequential masking\nstrategies used in the training of OmegaPLM [164]. For the span masking, the span length is sampled\nfrom Poisson distribution and clipped at 5 and 8, the protein sequence is masked consecutively according\nto the span length. For the sequential masking, the protein sequence is masked in either the first half or\nthe second half. Moreover, xTrimoPGLM [170] employs two types of pre-training objectives, each with\nits specific indicator tokens, to ensure both understanding and generative capacities. One is predicting\nrandomly masked tokens, and the other is to predict span tokens, i.e., recovering short spans in an\nautoregressive manner.\nSimilar to the masked language modeling methods, which involve masking and predicting tokens, there\nare also predictive pretext tasks designed for GNNs in the context of protein analysis. In these tasks, the\npseudo labels are derived from protein features. For example, Chen et al. [190] employ a GNN model\nthat takes the masked protein structure as input and aims to reconstruct the pairwise residue distances\nand dihedral angles. These masked protein features commonly include Cα distances, backbone dihedral\nangles, and residue types, as indicated in Table 3 [15,190].\nContrastive Pretext Task\nThe contrastive pretext task involves training a model to learn meaningful\nrepresentations by contrasting similar and dissimilar examples. Hermosilla et al. [189] propose contrastive\nlearning for protein structures, addressing the challenge of limited annotated datasets. Multiview con-\ntrastive learning aims to learn meaningful representations from multiple views of the same data. For\ninstance, GearNet [15] aligns representations from different views of the same protein while minimizing\nsimilarity with representations from different proteins (Multiview Contrast). Diverse views of a protein\nare generated using various data augmentation strategies, such as sequence cropping and random edge\nmasking. In terms of momentum contrast in GraSR [200], there are two GNN-based encoders, denoted\nas fq and fk, which take the query protein and key protein as inputs, respectively. These proteins have\nsimilar structures and are considered positive pairs. fq and fk share the same architecture but have\ndifferent parameter sets (θq and θk). θq is updated through back-propagation, while θk is updated using\nthe following equation:\nθk ←mθk + (1 −m)θq\n(24)\nhere, m ∈(0, 1] is a momentum coefficient.\nThe ProteinINR [212] employs a continual pre-training\napproach [201] to effectively model the embeddings across various modalities. Specifically, sequence data\nundergo initial pre-training using a sequence encoder, with the resultant encodings serving as inputs\nfor the subsequent structure encoder phase. This structure encoder is then pre-trained on surface data,\nutilizing the weights from this phase as the foundational weights for further pre-training focused on\nstructural details. The process culminates in the structure encoder undergoing additional pre-training\non the architectural aspects through a multi-view contrastive learning method.\n6.2\nSupervised Pretext Task\nDuring the pre-training phase, a supervised pretext task is commonly employed to provide auxiliary\ninformation and guide the model in learning enhanced representations. Min et al. [162] introduces a\nprotein-specific pretext task called Same-Family Prediction, where the model is trained to predict whether\na given pair of proteins belongs to the same protein family. This task helps the model learn meaningful\nprotein representations. In addition, Bepler and Berger [46] utilize a masked language modeling objective,\ndenoted as Lmasked, for training on sequences. They predict intra-residue contacts by employing a bilinear\nprojection of the sequence embeddings, which is measured by the negative log-likelihood of the true\nSci China Inf Sci\n25\ncontacts. They also introduce a structural similarity prediction loss to incorporate structural information\ninto the LM. However, Wu et al. [235] argue that not all external knowledge is beneficial for downstream\ntasks. It is crucial to carefully select appropriate tasks to avoid task interference, especially when dealing\nwith diverse tasks. Task interference is a common problem that needs to be considered [115].\n7\nDownstream Tasks\nIn the field of protein analysis, there are several downstream tasks that aim to extract valuable informa-\ntion and insights from protein data. These tasks play a crucial role in understanding protein structure,\nfunction, interactions, and their implications in various biological processes, including PSP, protein prop-\nerty prediction, and protein engineering and design, etc.\n7.1\nProtein Structure Prediction\nStructural features can be categorized into 1D and 2D representations. The 1D features encompass various\naspects such as secondary structure, solvent accessibility, torsion angles, contact density, and more. On\nthe other hand, the 2D features include contact and distance maps. For example, RaptorX-Contact [236]\nintegrates sequence and evolutionary coupling information using two deep residual neural networks to\npredict contact maps. This approach significantly enhances contact prediction performance. Other than\nthe contact and distance maps, Yang et al. [120], Li and Xu [237] focus on studying inter-atom distance\nand inter-residue orientation using a ResNet network. They utilize the predicted means and deviations\nto construct 3D structure models, leveraging the constraints provided by PyRosetta.\nWhile it is true that the prediction of structural features may not have significant practical value\nin the presence of highly accurate 3D structure data from AF2.\nIt still holds relevance and utility\nin several aspects.\nFrom one perspective, predicting structural features can serve as a reference for\nevaluating and comparing the results of various proposed methods [238]. Furthermore, these predictions\ncan contribute to the exploration of relationships between protein sequence, structure, and function\nby uncovering additional protein grammars and patterns.\nFor instance, DeepHomo [239] focuses on\npredicting inter-protein residue-residue contacts across homo-oligomeric protein interfaces. By integrating\nmultiple sources of information and removing potential noise from intra-protein contacts, DeepHomo\naims to provide insights into the complex interactions within protein complexes. Another example is\nGeoformer [164], which refines contact prediction to address the issue of triangular inconsistency.\nThe introduction of the highly accurate model, AF2 [16] has significantly influenced the development\nof end-to-end models for PSP. As depicted in Figure 19, AF2 consists of an encoder and decoder. The\ncore module of AF2 is Evoformer (encoder), which utilizes a variant of axial attention, including row-wise\ngated self-attention and column-wise gated self-attention, to process the MSA representation. To ensure\nconsistency in the embedding space, triangle multiplicative update, and triangle self-attention blocks\nare designed. The former combines information within each triangle of graph edges, while the latter\noperates self-attention around graph nodes.\nThe structure module (decoder) consists of eight layers\nwith shared weights. It takes the pair and first row MSA representations from the Evoformer as input.\nEach layer updates the single representation and the backbone frames using Euclidean transforms. The\nstructure module includes the Invariant Point Attention (IPA) module, a form of attention that acts on\na set of frames and is invariant under global Euclidean transformation. Another notable PSP method,\nRoseTTAFold [240] is a three-track model with attention layers that facilitate information flow at the\n1D, 2D, and 3D levels. It comprises seven modules. The MSA features are processed by attention over\nrows and columns, and the resulting features are aggregated using the outer product to update pair\nfeatures, which are further refined via axial attention. The MSA features are also updated based on\nattention maps derived from pair features, which exhibit good agreement with the true contact maps.\nA fully connected graph, built using the learned MSA and pair features as node and edge embeddings,\nis employed with a graph transformer to estimate the initial 3D structure. New attention maps derived\nfrom the current structure are used to update MSA features. Finally, the 3D coordinates are refined by\nSE(3)-Transformer [129] based on the updated MSA and pair features.\nHowever, AF2 and RosettaFold may face challenges when predicting structures for proteins with low or\nno evolutionary information, such as orphan proteins [241]. In such cases, OmegaFold [164] utilizes a LM\nto obtain the residue and pairwise embeddings from a single sequence. These embeddings are then fed\ninto Geoformer for accurate predictions on orphan proteins. ESMFold [13] tackles this issue by training\nSci China Inf Sci\n26\nSequence\nDatabase\nsearch\nMSA\nPair\nPairing\nFolding\ntrunk\n(Evoformer)\nStructure\nmodule\nPair\nrepresentation\nRecycling \nProtein LM\n(ESM-2/OmegaPLM)\nReplace\nESMFold\n/OmegaFold\nAlphaFold2\nSingle\nrepresentation\nFigure 19\nComparison of schematic architectures of AF2 and single sequence structure prediction methods.\nlarge-scale protein LMs (ESM-2) to learn more structural and functional properties, replacing the need\nfor explicit MSAs, as shown in Figure 19, which can reduce training resources and time. Thus, ESMFold\nachieves inference speeds 60 times faster than AF2.\nThe remarkable success in PSP has led to research in various related areas, including protein-peptide\nbinders identification [242], antibody structure prediction [243], protein complex structure prediction [244,\n245], RNA structure prediction [246], and protein conformation prediction [247], etc. Representative\nmethods for PSP are summarized in Figure 20.\n7.2\nProtein Design\nProtein design is a field of research that focuses on engineering and creating novel proteins with specific\nstructures and functions. It involves modifying existing proteins or designing entirely new ones to perform\ndesired tasks, such as enzyme catalysis, drug binding, molecular sensing, or protein-based materials.\nIn recent years, significant advancements have been made in the field of protein design through the\napplication of deep learning techniques. We divide the major works into three catogories.\nThe first approach involves pre-training a model on a large dataset of sequences. The pre-trained model\ncan be utilized to generate novel homologous sequences sharing sequence features with that protein family,\ndenoted as MSA generation, which is used to enhance the model’s ability to predict structures [260].\nProGen [208] is trained on sequences conditioned on a set of protein properties, like function or affiliation\nwith a particular organism, to generate desired sequences.\nAnother crucial problem is to design protein sequences that fold into desired structures, namely\nstructure-based protein design [117, 118, 128, 271, 272, 282, 291–296]. Formally, it is to find the amino\nacid sequence S = {si}i=1,...,n can fold into the desired structure P = {Pi}i=1,...,n, where Pi ∈R1×3, n is\nthe number of residues and the natural proteins are composed by 20 types of amino acids. It is to learn\na deep network having a function fθ:\nFθ : P 7→S\n(25)\nThere are also some works [291] combining the 3D structural encoder and 1D sequence decoder, where\nthe protein sequences are generated in an autoregressive way:\np(S | P; θ) =\nn\nY\ni=1\np (si | s<i, P; θ)\n(26)\nThirdly, generating a novel protein satisfying specified structural or functional properties is the task\nof de novo protein design [273, 274, 297, 298]. For example, in order to design protein sequences with\ndesired biological function, such as high fitness, Song et al. [273] develop a Monte Carlo Expectation-\nMaximization method to learn a latent generative model, augmented by combinatorial structure features\nfrom a separate learned Markov random fields. PROTSEED [270] learns the joint generation of residue\ntypes and 3D conformations of a protein with n residues based on context features as input to encourage\ndesigned proteins to have desired structural properties. These context features can be secondary structure\nannotations, binary contact features between residues, etc.\nSci China Inf Sci\n27\nApplication\nStructure Prediction\nProtein\nRGN [248], AF2 [16], RoseTTAFold [240], EigenFold [262],\nESMFold [13], OmegaFold [164], DMPfold2 [263], RFAA [264]\nAntibody\nEquiFold [255], IgFold [243]\nComplex\nAlphaFold-Multimer [244], ColAttn [257], GAPN [258],\nRoseTTAFoldNA [259], BiDock [286], PromptMSP [287]\nRNA\nE2Efold-3D [246]\nConformation\nSPEACH AF [247], Atom Transformer [249],\nMultiSFold [250], DiffMD [251], Str2Str [252]\nFolding Path\nPAthreader [253], Pathfinder [254]\nRefinement\nEquiFold [255], DeepACCNet [256], GNNRefine [185],\nATOMRefine [193]\nAssessment\nQDistance [122], DeepUMQA [124]\nDesign\nMSA Generation\nEvoGen [260], MSA-Augmenter [261], PoET [281]\nProtein Design\nPiFold [118], PROTSEED [270], LM-DESIGN [271], Genie [272],\nIsEM-Pro [273], FrameDiff [274], GraDe IF [282], NOS [289]\nAntibody Design\ndyMEAN [275], AbODE [276], AntiDesigner [277], HTP [278]\nDNA/RNA\nBIB [279], RDesign [280]\nProtein Pocket\nFAIR [288], RFdiffusionAA [264]\nProperty Prediction\nSubstructure\nRaptorX-Contact [236], DeepDist [266]\nFunction\nRaptorX-Property [265], Domain-PFP [230], Struct2GO [182]\nMutation Effect\nHotProtein [267], SidechainDiff [268], RDE [269],\nMutate Everything [290], Tranception [299], PPIRef [300]\nInteraction\nFABind [283], NERE DSM [284], MAPE-PPI [285]\nFigure 20\nTaxonomy of representative methods for different protein applications. The model name is linked with the official\nGitHub or server page.\nIn addition to protein sequence and structure design, some research focuses on designing antibody\nand DNA sequences [275,279]. A critical yet challenging task in this domain is the design of the protein\npocket, the cavity region of the protein where the ligand binds [288]. The pocket is essential for molecular\nrecognition and plays a key role in protein function. Effective pocket design can enable control over\nselectivity and affinity towards desired ligands.\n7.3\nProtein Property Prediction\nProtein property prediction aims to predict various properties and characteristics of proteins, such as\nsolvent accessibility, functions, subcellular localization, fitness, etc. The structural properties, like sec-\nondary structure and contact maps, are useful in other tasks [237]. For the function prediction, there\nis a group of PRL methods enabling inference about biochemical, cellular, systemic or phenotypic func-\ntions [15, 59].\nMany proteins contain intrinsic fluorescent amino acids like tryptophan and tyrosine.\nMutations can alter the microenvironment of these residues and change the emitted fluorescence upon\nexcitation [145]. Stability landscape prediction estimates the impact of mutations on the overall ther-\nmodynamic stability of protein structures. Stability is quantified by ∆G, the Gibbs free energy change\nbetween folded and unfolded states. Mutations disrupting key interactions can undesirably destabilize\nthe native state [145,299,301,302], leading to changes in protein properties. Thus, there are models are\ndeveloped to evaluate the mutational effects on PPI identifications [268, 269, 303]. The protein fitness\nlandscape refers to the mapping between genotype (e.g., the amino acid sequence) and phenotype (e.g.,\nprotein function), which is a fairly broad concept. Models that learn the protein fitness landscape are\nexpected to be effective at predicting the effects of mutations [303].\nSci China Inf Sci\n28\n8\nDiscussion: Insights and Future Outlooks\nBased on a comprehensive review of fundamental deep learning techniques, protein fundamentals, protein\nmodel architectures, pretext tasks, and downstream applications, we aim to provide deeper perspectives\ninto protein models.\nTowards a Generalizable Large-Scale Biological Model\nWhile breakthroughs like ChatGPT [90]\nhave demonstrated remarkable success across various domains, there is still a need to develop large-scale\nmodels tailored to biological data encompassing proteins, DNA, RNA, antibodies, enzymes, and more\nin order to address existing challenges. Roney and Ovchinnikov [304] find that AlphaFold has learned\nan accurate biophysical energy function and can identify low-energy conformations using co-evolutionary\ninformation [305]. However, some studies have indicated AlphaFold may not reliably predict mutation\nimpacts on proteins [306–308]. Performance on structure-based tasks doesn’t directly transfer to other\npredictive problems [166, 302].\nData-driven methods have attracted more attention, but there is no\nsingular optimal model architecture or pretext task that generalizes across all data types and downstream\napplications. Continued efforts are imperative to develop versatile, scalable, and interpretable models\nintegrating both physical and data sciences for comprehensively tackling biomolecular modeling and\ndesign across contexts [92].\nCase by Case\nWith ever-expanding protein sequence databases now containing millions of entries,\nprotein models have witnessed a commensurate growth in scale, with parameter counts in the billions (e.g.,\nESM-2 [13] and xTrimoPGLM [170]). However, training such enormous deep learning models currently\nremains accessible only to large corporations with vast computational resources. For instance, DeepMind\nutilized 128 TPU v3 cores over one week to train AF2. The requirements pose a challenge for academic\nresearch groups to learn protein representations from scratch and also raise environmental sustainability\nconcerns. Given these constraints, increased focus on targeted, problem-centric formulations may be\nprudent. The choice of appropriate model architecture and self-supervised learning scheme should match\ndataset attributes and application objectives. This demands careful scrutiny of design choices dependent\non available inputs and intended predictive utility. Furthermore, the vast potential of large pre-trained\nmodels remains underexplored from the lens of effectively utilizing them under specific problem contexts\nwith the integration of prior knowledge.\nExpanding Multimodal Representation Learning\nESM-2 [13] analysis indicates that model per-\nformance saturates quickly with increasing size for high evolutionary depth, while gains continue at low\ndepths with larger models. This exemplifies that appropriately incorporating additional modalities like\nbiological and physical priors, MSAs, etc., can reduce model scale and improve metrics. As evident in\nTable 6, combining sequence with structure or functional data into models like LM-GVP, GearNet and\nProtST-ESM-2 confers improvements over ESM-2 alone. More extensive exploration into multimodal\nrepresentation learning is imperative [46].\nInterpretability\nProteins and languages share similarities but also key differences, as discussed pre-\nviously. Most deep learning models currently lack interpretability, obstructing insights into underlying\nprotein folding mechanisms. Models like AlphaFold cannot furnish detailed characterizations of molec-\nular interactions and chemical principles imperative for mechanistic studies and structure-based drug\ndesign. Interpretable models that reveal grammar-like rules governing proteins would inform impactful\nbiomedical applications. Hence, conceptualizing methodologies tailored to the nuances of protein data is\nan urgent priority. Visualization tools that capture folding dynamics and functional conformational tran-\nsitions can powerfully address these needs, which would grant researchers the ability to visually traverse\nthe atomic trajectories of proteins. Such molecular recordings would waveguide principles discovered to\nbe harnessed towards materials and therapeutic innovation.\nPractical Utility Across Domains\nAs depicted in Figure 20, researchers have progressed towards\ntackling more complex challenges such as predicting structures from single sequences, modeling complex\nassemblies, elucidating folding mechanisms, and characterizing protein-ligand interactions [13, 164, 259,\n283]. Since mutations can precipitate genetic diseases, modeling their functional effects provides insights\nSci China Inf Sci\n29\ninto how sequence constraints structure. Thus, accurately predicting robust structures and mutation\nimpacts is imperative [268, 269]. Drug design represents a promising avenue for expeditious and eco-\nnomical compound discovery. Recent years have witnessed innovations in AI-driven methodologies for\nidentifying candidate molecules from huge libraries to bind specific pockets [309–311]. Going further,\ngenerating enhanced out-of-distribution sequences with desirably tuned attributes (like stability) remains\nan engaging prospect [312,313]. For groups equipped with wet-lab capabilities, synergistic combinations\nof computational predictions and experiments offer traction for multifarious problems at the interface of\ndeep learning and biology.\nUnified Benchmarks and Evaluation Protocols\nAmidst an influx of emerging work, comparative\nassessments are often impeded by inconsistencies in datasets, architectures, metrics, and other evaluation\nfactors. To enable healthy progress, there is a pressing need to establish standardized benchmarking pro-\ntocols across tasks. Unified frameworks for fair performance analysis will consolidate disjoint efforts and\nclarify model capabilities to distill collective progress [234]. Considering factors like data leakage, bias\nand ethics, some groups develop new datasets tailored to their needs [314]; On the other hand, construct-\ning rigorous, reliable and equitable benchmarks remains essential for evaluating models and promoting\nimpactful methods. Initiatives like TAPE [145], ProteinGym [303], ProteinShake [315], PEER [316], Pro-\nteinInvBench [317], ProteinWorkshop [318], exemplify the critical role of comprehensive benchmarking\nin furthering innovation. Moreover, the Critical Assessment of Protein Structure Prediction (CASP) ex-\nperiments act as a crucial platform for evaluating the most recent advancements in PSP within the field,\nwhich has been conducted 15 times by mid-December 2022. Research groups from all over the world\nparticipate in CASP to objectively test their structure prediction methods. By categorizing different\nthemes, like quality assessment, domain boundary prediction, and protein complex structure prediction,\nCASPers can identify what progress has been made and highlight the future efforts that may be most\nproductively focused on.\nProtein Structure Prediction in Post-AF2 Era\nAF2 marked a significant advancement in protein\nstructure prediction, yet opportunities for enhancement persist. Several key strategies can be employed\nto refine the conventional AF2 model. These include diversifying approaches or expanding the database\nto generate more comprehensive MSAs, optimizing template utilization, and integrating distances and\nconstraints derived from the AF2 model with alternative methods [27]. Notably, spatial constraints like\ncontact, distance, and hydrogen-bond networks have been incorporated into I-TASSER [319] to predict\nfull-length protein structures, achieving superior performance. In CASP15, the performance of the top\nmodels from server groups closely approached and, in some cases, even surpassed that of the human\ngroups. This indicates that AI models have reached a stage where they can effectively assimilate and\napply human knowledge in the field. However, when benchmarked on the human proteome, only 36%\nof residues fall within its highest accuracy tier [320]. While approximately 35% of AF2’s predictions\nrival experimental structures, challenges remain in enhancing coverage, and at least 40 teams surpassed\nAF2 in accuracy in CASP15. Although models completely replacing AF2 have not yet emerged in the\npast two or three years, it has revealed some limitations. For instance, AF2’s prediction accuracy on\nmultidomain proteins is not as robust as its accuracy for individual domains [27]. Addressing challenges\nposed by multidomain proteins, including protein complexes, multiple conformational states, and folding\npathways, may be crucial research directions in the field of PSP, though there have appeared works\nattempting to tackle these problems, as shown in Figure 20.\nBoundary of Large Language Models in Proteins\nLarge language models (LLMs), e.g., Chat-\nGPT [90], have prevailed in NLP [52,55]. Demis Hassabis, CEO and co-founder of DeepMind, has stated\nthat biology can be thought of as an information processing system, albeit an extraordinarily complex\nand dynamic one, just as mathematics turned out to be the right description language for physics, biology\nmay turn out to be the perfect type of regime for the application of AI. Like the rules of grammar would\nemerge from training an LLM on language samples, the limitations dictated by evolution would arise\nfrom training the system on samples of proteins. There are essentially two families in LLMs, BERT and\nGPT, which have different training objectives and processing methods, as we have stated above [168].\nBased on the two base models, different LLMs are proposed in protein, like ProtGPT2 [167], ESM-2 [13],\nand ProtChatGPT [321], etc. Researchers have tried a range of LLM sizes and found some intriguing\nSci China Inf Sci\n30\nfacts, for example, the ESM-2 [13] model can get better results when increasing the resources, but it is\nstill not clear when it would max out [322]. It would be interesting to explore the boundaries of LLMs\nin proteins.\n9\nConclusion\nThis paper presents a systematic overview of pertinent terminologies, notations, network architectures,\nand protein fundamentals, spanning CNNs, LSTMs, GNNs, LMs, physicochemical properties, sequence\nmotifs and structural regions, etc. Connections between language and protein domains are elucidated,\nexemplifying model utility across applications. Through a comprehensive literature survey, current pro-\ntein models are reviewed, analyzing architectural designs, self-supervised objectives, training data and\ndownstream use cases. Limitations and promising directions are discussed, covering aspects like multi-\nmodal learning, model interpretability, and knowledge integration. Overall, this survey aims to orient\nmachine learning and biology researchers towards open challenges for enabling the next generation of\ninnovations. By condensing progress and perspectives, we hope to crystallize collective headway while\ncharting fruitful trails for advancing protein modeling and design, elucidating molecular mechanisms, and\ntranslating insights into functional applications for the benefit of science and society.\nReferences\n1\nPleiss J, Fischer M, Peiker M, et al. Lipase engineering database: understanding and exploiting sequence–structure–function\nrelationships. Journal of Molecular Catalysis B: Enzymatic, 2000, 10: 491-508\n2\nGromiha M M. Protein bioinformatics: from sequence to function. academic press, 2010\n3\nKryshtafovych A, Schwede B, Topf M, et al. Critical assessment of methods of protein structure prediction (CASP)—Round\nXIII. Proteins: Structure, Function, and Bioinformatics, 2019, 87: 1011–1020\n4\nSenior A W, Evans R, Jumper J, et al. Improved protein structure prediction using potentials from deep learning. Nature,\n2020, 577: 706–710\n5\nIkeya T, G¨untert P, Ito Y. Protein structure determination in living cells. International Journal of Molecular Sciences, 2019,\n20: 2442\n6\nGauto D F, Estrozi L F, Schwieters C D, et al. Integrated NMR and cryo-EM atomic-resolution structure determination of\na half-megadalton enzyme complex. Nature, 2019, 10: 1-12\n7\nAshburner M, Ball C A, Blake J A, et al. Gene ontology: tool for the unification of biology. Nature genetics, 2000, 25: 25-9\n8\nProtein Data Bank: the single global archive for 3D macromolecular structure data.\nNucleic Acids Research, 2019, 47:\nD520–D528\n9\nUniProt Consortium. Update on activities at the Universal Protein Resource (UniProt) in 2013. Nucleic Acids Research,\n2013, 41: D43-D47\n10\nZhang N, Bi Z, Liang X, et al. Ontoprotein: Protein pretraining with gene ontology embedding. arXiv, January 2022\n11\nUnsal S, Atas H, Albayrak M, et al.\nLearning functional properties of proteins with language models.\nNature Machine\nIntelligence, 2022, 4: 227-45\n12\nElnaggar A, Heinzinger M, Dallago C, et al. Prottrans: Towards cracking the language of lifes code through self-supervised\ndeep learning and high performance computing. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021\n13\nLin Z, Akin H, Rao R, et al.\nLanguage models of protein sequences at the scale of evolution enable accurate structure\nprediction. bioRxiv, 2022\n14\nRives A, Meier J, Sercu T, et al. Biological structure and function emerge from scaling unsupervised learning to 250 million\nprotein sequences. In: Proceedings of the National Academy of Sciences, 2021. 118\n15\nZhang Z, Xu M, Jamasb A, et al. Protein representation learning by geometric structure pretraining. arXiv, March 2022\n16\nJumper J M, Evans R O, Pritzel A, et al. Highly accurate protein structure prediction with AlphaFold. Nature, 2021, 596:\n583-589\n17\nAlQuraishi M. End-to-end differentiable learning of protein structure. Cell systems, 2019, 292–301\n18\nSusanty M, Rajab T E, Hertadi R. A review of protein structure prediction using deep learning. In: BIO Web of Conferences,\n2021. 41\n19\nMa B, Johnson R. De novo sequencing and homology searching. Molecular & Cellular Proteomics, 2012, 11: 2\n20\nMa B. Novor: real-time peptide de novo sequencing software. Journal of the American Society for Mass Spectrometry, 2015,\n26: 1885-1894\n21\nZheng W, Li Y, Zhang C, et al. Protein structure prediction using deep learning distance and hydrogen-bonding restraints\nin CASP14. Proteins: Structure, Function, and Bioinformatics, 2021, 89: 1734-1751\n22\nChowdhury R, Bouatta N, Biswas S, et al. Single-sequence protein structure prediction using a language model and deep\nlearning. Nature Biotechnology, 2022, 40: 1617-1623\n23\nWu F, Xu J. Deep template-based protein structure prediction. PLoS computational biology, 2021, 17(5): e1008954\n24\nWu F, Jing X, Luo X. et al. Improving protein structure prediction using templates and sequence embedding. Bioinformatics,\n2023, 39: btac723\n25\nIuchi H, Matsutani T, Yamada K, et al. Representation learning applications in biological sequence analysis. Computational\nand Structural Biotechnology Journal, 2021, 19: 3198-3208\n26\nHu L, Wang X, Huang Y A, et al. A survey on computational models for predicting protein–protein interactions. Briefings\nin bioinformatics, 2021, 22: bbab036\n27\nPeng C X, Liang F, Xia Y H, et al. Recent Advances and Challenges in Protein Structure Prediction. Journal of Chemical\nInformation and Modeling, 2023\n28\nIhm Y. A threading approach to protein structure prediction: studies on TNF-like molecules, Rev proteins, and protein\nkinases. Iowa State University, 2004\nSci China Inf Sci\n31\n29\nPatel M, Shah H. Protein secondary structure prediction using support vector machines (svms). In: International Conference\non Machine Intelligence and Research Advancement, IEEE, 2013. 594-598\n30\nSanger F. The arrangement of amino acids in proteins. Advances in protein chemistry, 1952, 7: 1-67\n31\nHao X H, Zhang G J, Zhou X G. Conformational space sampling method using multi-subpopulation differential evolution\nfor de novo protein structure prediction. IEEE Transactions on NanoBioscience, 2017, 16: 618-33\n32\nWang S, Peng J, Ma J, et al. Protein secondary structure prediction using deep convolutional neural fields. Scientific reports,\n2016, 6: 1-11\n33\nChou K C, Cai Y D. Predicting protein quaternary structure by pseudo amino acid composition.\nProteins: Structure,\nFunction, and Bioinformatics, 2003, 53: 282–289\n34\nKoonin E V. Orthologs, paralogs, and evolutionary genomics. Annual review of genetics, 2005, 39: 309–338\n35\nWang Y, Wu H, Cai Y. A benchmark study of sequence alignment methods for protein clustering. BMC bioinformatics,\n2018, 19: 95–104\n36\nOchoa D, Pazos F. Practical aspects of protein co-evolution. Frontiers in cell and developmental biology, 2014, 2: 14\n37\nEmerson I A, Amala A. Protein contact maps: a binary depiction of protein 3D structures. Physica A: Statistical Mechanics\nand its Applications, 2017, 465: 782-91\n38\nBasile W, Sachenkova O, Light S, et al.\nHigh gc content causes orphan proteins to be intrinsically disordered.\nPLoS\ncomputational biology, 2017, 13: 3\n39\nBlackstock J C. Guide to Biochemistry. Butterworth-Heinemann, 2014\n40\nLi S, Chua T S, Zhu J, et al. Generative topic embedding: a continuous representation of documents. In: Proceedings of\nthe 54th Annual Meeting of the Association for Computational Linguistics, 2016. 666–675\n41\nLin Z, Feng M, Santos C N, et al. A structured self-attentive sentence embedding. arXiv, March 2017\n42\nWeiss K, Khoshgoftaar T M, Wang D. A survey of transfer learning. Journal of Big data, 2016, 3: 1-40\n43\nPan S J, Yang Q. A survey on transfer learning. IEEE Transactions on knowledge and data engineering, 2010, 22: 1345-1359\n44\nBond-Taylor S, Leach A, Long Y, et al. Deep generative modelling: A comparative review of VAEs, GANs, normalizing\nflows, energy-based and autoregressive models. arXiv, March 2021\n45\nJahan M S, Khan H U, Akbar S, et al.\nBidirectional Language Modeling: A Systematic Literature Review.\nScientific\nProgramming, 2021\n46\nBepler T, Berger B. Learning the protein language: Evolution, structure, and function. Cell systems, 2021, 12: 654-669\n47\nGou J, Yu B, Maybank SJ, et al. Knowledge distillation: A survey. International Journal of Computer Vision, 2021, 129:\n1789-1819\n48\nSkocaj D, Leonardis A, Kruijff GJ. Cross-modal learning. Encyclopedia of the Sciences of Learning, 2012, 861-864\n49\nWang H, Zhang J, Chen Y, et al. Uncertainty-aware multi-modal learning via cross-modal random network prediction. arxiv,\nJuly 2022\n50\nHe K, Zhang X, Ren S, et al. Identity mappings in deep residual networks. In: European conference on computer vision,\nSpringer, Cham, 2016. 630-645\n51\nVaswani A, Shazeer N, Parmar N, et al. Attention is all you need. Advances in neural information processing systems. 2017,\n30\n52\nDevlin J, Chang M W, Lee K, et al. Bert: Pre-training of deep bidirectional transformers for language understanding. arxiv,\nOctober 2018\n53\nRadford A, Narasimhan K, Salimans T, et al. Improving language understanding by generative pre-training. 2018\n54\nRadford A, Wu J, Child R, et al. Language models are unsupervised multitask learners. OpenAI blog. 2019, 1: 9\n55\nBrown T, Mann B, Ryder N, et al.\nLanguage models are few-shot learners.\nAdvances in neural information processing\nsystems, 2020, 33: 1877-1901\n56\nLindsay, G W. Convolutional neural networks as a model of the visual system: Past, present, and future. Journal of cognitive\nneuroscience, 2021, 33: 2017-2031\n57\nKriegeskorte N. Deep neural networks: a new framework for modeling biological vision and brain information processing.\nAnnual review of vision science, 2015, 1: 417-446\n58\nChartrand G, Cheng P M, Vorontsov E, et al. Deep learning: a primer for radiologists. Radiographics, 2017, 37: 2113-2131\n59\nFan H, Wang Z, Yang Y, et al. Continuous-Discrete Convolution for Geometry-Sequence Modeling in Proteins. In: The\nEleventh International Conference on Learning Representations, 2022\n60\nHayes B. First links in the Markov chain. American Scientist, 2013, 101: 252\n61\nLi H. Language models: past, present, and future. Communications of the ACM, 2022, 65: 56-63\n62\nBishop M, Thompson E A. Maximum likelihood alignment of dna sequences. Journal of molecular biology, 1986, 190: 159–165\n63\nChiu J T, Rush A M. Scaling hidden markov language models. arxiv, November 2020\n64\nStigler J, Ziegler F, Gieseke A, et al.\nThe complex folding network of single calmodulin molecules.\nScience, 2011, 334:\n512–516\n65\nWong K C, Chan T M, Peng C, et al. DNA motif elucidation using belief propagation. Nucleic Acids Research, 2013, 41:\ne153–e153\n66\nFerruz N, H¨ocker B, Controllable protein design with language models. Nature Machine Intelligence, 2022, 1-12\n67\nAGMLS F, Bunke R, Schmiduber J. A novel connectionist system for improved unconstrained handwriting recognition. IEEE\nTransactions on Pattern Analysis Machine Intelligence, 2009, 31: 5\n68\nDe S, Smith S L, Fernando A, et al. Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language\nModels. arxiv, February 2024\n69\nHochreiter S. Untersuchungen zu dynamischen neuronalen netzen. Diploma, Technische Universit¨at M¨unchen, 1991, 91: 1\n70\nLample G, Ballesteros M, Subramanian S, et al. Neural architectures for named entity recognition. arxiv, March 2016\n71\nSchmidhuber J, Wierstra D, Gomez F J. Evolino: Hybrid neuroevolution/optimal linear search for sequence prediction. In:\nProceedings of the 19th International Joint Conferenceon Artificial Intelligence (IJCAI), 2005\n72\nHochreiter S, Heusel M, Obermayer K. Fast model-based protein homology detection without alignment. Bioinformatics,\n2007, 23: 1728–1736\n73\nGupta A, M¨uller A T, Huisman B J, et al. Generative recurrent networks for de novo drug design. Molecular informatics,\n2018, 37: 1700111\n74\nMa C, Dai G, Zhou J. Short-term traffic flow prediction for urban road sections based on time series analysis and\nLSTM BILSTM method. IEEE Transactions on Intelligent Transportation Systems, 2021\n75\nRadford A, Jozefowicz R, Sutskever I. Learning to generate reviews and discovering sentiment. arxiv, April 2017\nSci China Inf Sci\n32\n76\nBahdanau D, Cho K, Bengio Y. Neural machine translation by jointly learning to align and translate. arxiv, September 2014\n77\nHan X, Zhang Z, Ding N, et al. Pre-trained models: Past, present and future. AI Open, 2021, 2: 225-50.\n78\nTan Q, Liu N, Hu X. Deep representation learning for social network analysis. Frontiers in big Data, 2019, 2: 2\n79\nWu S, Sun F, Zhang W, et al. Graph neural networks in recommender systems: a survey. ACM Computing Surveys, 2022,\n55: 1-37\n80\nDeng J, Yang Z, Ojima I, et al. Artificial intelligence in drug discovery: applications and techniques. Briefings in Bioinfor-\nmatics, 2022, 23: bbab430\n81\nFensel D, S¸im¸sek U, Angele K, et al. Introduction: what is a knowledge graph?. Knowledge Graphs, Springer, Cham, 2020,\n1-10\n82\nKipf T N, Welling M. Semi-supervised classification with graph convolutional networks. arxiv, September 2016\n83\nVeliˇckovi´c P, Cucurull G, Casanova A, et al. Graph attention networks. arxiv, October 2017\n84\nOh J, Cho K, Bruna J. Advancing graphsage with a data-driven node sampling. arxiv, April 2019\n85\nThrun S, Pratt L. Learning to learn: Introduction and overview. In: Learning to learn, 1998. 3-17\n86\nLiu Y, Ott M, Goyal N, et al. Roberta: A robustly optimized bert pretraining approach. arxiv, July 2019\n87\nYang Z, Dai Z, Yang Y, et al. Xlnet: Generalized autoregressive pretraining for language understanding. Advances in neural\ninformation processing systems. 2019, 32\n88\nKaplan J, McCandlish S, Henighan T, et al. Scaling laws for neural language models. arxiv, January 2020\n89\nHoffmann J, Borgeaud S, Mensch A, et al. Training Compute-Optimal Large Language Models. arxiv, March 2022\n90\nPerlman AM. The Implications of OpenAI’s Assistant for Legal Services and Society. Available at SSRN. 2022\n91\nAraci D. Finbert: Financial sentiment analysis with pre-trained language models. arxiv, August 2019\n92\nWang B, Xie Q, Pei J, et al. Pre-trained language models in biomedical domain: A systematic survey. arxiv, October 2021\n93\nBengio Y, Ducharme R, Vincent P. A neural probabilistic language model.\nAdvances in neural information processing\nsystems, 2000, 13\n94\nMikolov T, Chen K, Corrado G S, et al.\nEfficient estimation of word representations in vector space.\nIn: International\nconference on learning representations, 2013\n95\nLee J, Yoon W, Kim S, et al. Biobert: a pre-trained biomedical language representation model for biomedical text mining.\nBioinformatics, 2020, 36: 1234–1240\n96\nYoung T, Hazarika D, Poria S, et al. Recent trends in deep learning based natural language processing. IEEE Computational\nintelligenCe magazine, 2018, 13: 55-75\n97\nYang K K, Wu Z, Bedbrook C N, et al.\nLearned protein embeddings for machine learning.\nBioinformatics, 2018, 34:\n2642-2648\n98\nAsgari E, Mofrad M R. Continuous distributed representation of biological sequences for deep proteomics and genomics.\nPloS one, 2015, 10: 11\n99\nOfer D, Brandes N, Linial M. The language of proteins: NLP, machine learning & protein sequences. Computational and\nStructural Biotechnology Journal, 2021, 19:1750-1758\n100\nBryngelson J D, Onuchic J N, Socci N D, et al. Funnels, pathways, and the energy landscape of protein folding: a synthesis.\nProteins: Structure, Function, and Bioinformatics, 1995, 21: 167-95\n101\nLeopold P E, Montal M, Onuchic J N. Protein folding funnels: a kinetic approach to the sequence-structure relationship.\nIn: Proceedings of the National Academy of Sciences, 1992, 8721–8725\n102\nZerihun MB, Schug A. Biomolecular Structure Prediction via Coevolutionary Analysis: A Guide to the Statistical Framework.\nIn: NIC Symposium, 2018. FZJ-2018-02966\n103\nVorberg S. Bayesian statistical approach for protein residue-residue contact prediction. Doctoral dissertation, lmu, 2017\n104\nChoshen L, Abend O. Automatically Extracting Challenge Sets for Non local Phenomena in Neural Machine Translation.\narxiv, September 2019\n105\nAlley EC, Khimulya G, Biswas S, et al. Unified rational protein engineering with sequence-based deep representation learning.\nNature methods. 2019, 16: 1315-1322\n106\nMadani A, Krause B, Greene ER, et al. Deep neural language modeling enables functional protein generation across families.\nbioRxiv, 2021\n107\nOfer D, Brandes N, Linial M. The language of proteins: NLP, machine learning & protein sequences. Computational and\nStructural Biotechnology Journal, 2021, 19: 1750-1758\n108\nJia J, Liu Z, Xiao X, et al. Identification of protein-protein binding sites by incorporating the physicochemical properties\nand stationary wavelet transforms into pseudo amino acid composition. Journal of Biomolecular Structure and Dynamics,\n2016, 34: 1946-1961\n109\nXu G, Wang Q, Ma J. OPUS-Rota4: a gradient-based protein side-chain modeling framework assisted by deep learning-based\npredictors. Briefings in Bioinformatics, 23: bbab529\n110\nHanson J, Paliwal K, Litfin T, et al. Improving prediction of protein secondary structure, backbone angles, solvent accessi-\nbility and contact numbers by using predicted contact maps and an ensemble of recurrent and residual convolutional neural\nnetworks. Bioinformatics, 2019, 35: 2403–2410\n111\nGao Z, Jiang C, Zhang J, et al. Hierarchical graph learning for protein–protein interaction. Nature Communications, 2023,\n14: 1093\n112\nJohansson M U. Defining and searching for structural motifs using DeepView/Swiss-PdbViewer. BMC Bioinformatics, 2012,\n13: 173\n113\nXu D, Nussinov R. Favorable domain size in proteins. Folding & Design, 1998, 3: 11–7\n114\nHu F, Hu Y, Zhang W, et al.\nA Multimodal Protein Representation Framework for Quantifying Transferability Across\nBiochemical Downstream Tasks. Advanced Science, 2023, 2301223\n115\nWang Z, Zhang Q, Shuang-Wei H U, et al. Multi-level Protein Structure Pre-training via Prompt Learning. In: The Eleventh\nInternational Conference on Learning Representations, 2022\n116\nWang L, Liu H, Liu Y, et al. Learning hierarchical protein representations via complete 3d graph networks. In: The Eleventh\nInternational Conference on Learning Representations, 2022\n117\nIngraham J, Garg V, Barzilay R, et al. Generative models for graph-based protein design. Advances in neural information\nprocessing systems, 2019, 32\n118\nGao Z, Tan C, Li S Z. PiFold: Toward effective and efficient protein inverse folding. In: ICLR, 2023\n119\nHu B, Tan C, Xia J, et al. Learning Complete Protein Representation by Deep Coupling of Sequence and Structure. bioRxiv,\n2023\nSci China Inf Sci\n33\n120\nYang J, Anishchenko I, Park H, et al. Improved protein structure prediction using predicted inter-residue orientations. In:\nProceedings of the National Academy of Sciences of the United States of America, 2019\n121\nDu Z, Su H, Wang W, et al. The trRosetta server for fast and accurate protein structure prediction. Nature protocols, 2021,\n16: 5634-5651\n122\nYe L, Wu P, Peng Z, et al. Improved estimation of model quality using predicted inter-residue distance. Bioinformatics,\n2021, 37: 3752-3759\n123\nTischer D, Lisanza S, Wang J, et al. Design of proteins presenting discontinuous functional sites using deep learning. bioRxiv,\n2020\n124\nGuo S S, Liu J, Zhou X G, et al. DeepUMQA: ultrafast shape recognition-based protein model quality assessment using\ndeep learning. Bioinformatics, 2022, 38: 1895-1903\n125\nGross E H, Meienhofer J. The Peptide Bond. Major Methods of Peptide Bond Formation: The Peptides Analysis, Synthesis,\nBiology, 2014, 1: 1\n126\nNelson D L, Lehninger A L, Cox M M. Lehninger principles of biochemistry. Macmillan. 2008\n127\nVollhardt K P C, Schore N E. Organic chemistry: structure and function. Macmillan. 2003\n128\nJing B, Eismann S, Suriana P, et al. Learning from protein structure with geometric vector perceptrons. arxiv, September\n2020\n129\nFuchs F, Worrall D, Fischer V, et al. Se (3)-transformers: 3d roto-translation equivariant attention networks. Advances in\nNeural Information Processing Systems, 2020, 33: 1970-1981\n130\nDu W, Zhang H, Du Y. et al. SE (3) Equivariant Graph Neural Networks with Complete Local Frames. In: International\nConference on Machine Learning, 2022. 5583–5608\n131\nLiu S, Du W, Li Y, et al. Symmetry-Informed Geometric Representation for Molecules, Proteins, and Crystalline Materials.\narxiv, June 2023\n132\nLiu D, Chen S, Zheng S, et al. SE (3) Equivalent Graph Attention Network as an Energy-Based Model for Protein Side\nChain Conformation. bioRxiv, 2022\n133\nKrapp L F, Abriata L A, Cort´es Rodriguez F, et al. PeSTo: parameter-free geometric deep learning for accurate prediction\nof protein binding interfaces. Nature Communications, 2023, 14: 2175\n134\nLiu Y, Wang L, Liu M, et al. Spherical message passing for 3d molecular graphs. In: International Conference on Learning\nRepresentations, 2021\n135\nWang L, Liu Y, Lin Y, et al. ComENet: Towards Complete and Efficient Message Passing for 3D Molecular Graphs. arxiv,\nJune 2022\n136\nXu M, Yuan X, Miret S, et al. Protst: Multi-modality learning of protein sequences and biomedical texts. arxiv, January\n2023\n137\nKlausen M S, Jespersen M C, Nielsen H, et al. Netsurfp-2.0: improved prediction of protein structural features by integrated\ndeep learning. Proteins, 2018\n138\nHeffernan R, Paliwal K, Lyons J, et al. Single-sequence-based prediction of protein secondary structures and solvent acces-\nsibility by deep whole-sequence learning. Journal of computational chemistry, 2018, 39: 2210-2216\n139\nAlmagro Armenteros J J, Johansen A R, Winther O, et al. Language modelling for biological sequences–curated datasets\nand baselines. bioRxiv, 2020\n140\nAsgari E, Poerner N, McHardy A C, et al. Deepprime2sec: Deep learning for protein secondary structure prediction from\nthe primary sequences. bioRxiv, 2019\n141\nSingh J, Litfin T, Paliwal K, et al. SPOT-1D-Single: improving the single-sequence-based prediction of protein secondary\nstructure, backbone angles, solvent accessibility and half-sphere exposures using a large training set and ensembled deep\nlearning. Bioinformatics, 2021, 37:3464-3472\n142\nSinai S, Kelsic E, Church G M, et al. Variational auto-encoding of protein sequences. arxiv, December 2017\n143\nDing X, Zou Z, Brooks III C L. Deciphering protein evolution and fitness landscapes with latent space models.\nNature\ncommunications, 2019, 10: 5644\n144\nKrause B, Lu L, Murray I, et al. Multiplicative LSTM for sequence modelling. arxiv, September 2016\n145\nRao R, Bhattacharya N, Thomas N, et al. Evaluating protein transfer learning with TAPE. Advances in neural information\nprocessing systems, 2019, 32\n146\nLan Z, Chen M, Goodman S, et al.\nAlbert: A lite bert for self-supervised learning of language representations.\narxiv,\nSeptember 2019\n147\nDai Z, Yang Z, Yang Y, et al. Transformer-xl: Attentive language models beyond a fixed-length context. arxiv, January\n2019\n148\nClark K, Luong M T, Le Q V, et al. Electra: Pre-training text encoders as discriminators rather than generators. arxiv,\nMarch 2020\n149\nRaffel C, Shazeer N, Roberts A, et al.\nExploring the limits of transfer learning with a unified text-to-text transformer.\nJournal of Machine Learning Research, 2020, 21: 1–67\n150\nHeinzinger M, Elnaggar A, Wang Y, et al. Modeling the language of life-deep learning protein sequences. bioRxiv, 2019\n151\nPeters ME, Neumann M, Iyyer M, et al. Deep contextualized word representations. North American chapter of the association\nfor computational linguistics, 2018\n152\nStrodthoff N, Wagner P, Wenzel M, et al. UDSMProt: universal deep sequence models for protein classification. Bioinfor-\nmatics, 2020, 36: 2401–2409\n153\nLu A X, Zhang H, Ghassemi M, et al. Self-supervised contrastive learning of protein representations by mutual information\nmaximization. bioRxiv, 2020\n154\nDey R, Salem F M. Gate-variants of gated recurrent unit (GRU) neural networks. In: 2017 IEEE 60th international midwest\nsymposium on circuits and systems (MWSCAS), IEEE, 2017. 1597-1600\n155\nZhou G, Chen M, Ju C J, et al.\nMutation effect estimation on protein–protein interactions using deep contextualized\nrepresentation learning. NAR genomics and bioinformatics, 2020, 2: lqaa015\n156\nSzklarczyk D, Gable A L, Lyon D, et al.\nSTRING v11:\nprotein–protein association networks with increased coverage,\nsupporting functional discovery in genome-wide experimental datasets. Nucleic acids research, 2019, 47: D607-D613\n157\nSturmfels P, Vig J, Madani A, et al. Profile prediction: An alignment-based pre-training task for protein sequence models.\narxiv, December 2020\n158\nNambiar A, Heflin M, Liu S, et al. Transforming the language of life: transformer neural networks for protein prediction\ntasks.\nIn:\nProceedings of the 11th ACM international conference on bioinformatics, computational biology and health\nSci China Inf Sci\n34\ninformatics, 2020. 1-8\n159\nHe L, Zhang S, Wu L, et al.\nPre-training co-evolutionary protein representation via a pairwise masked language model.\narxiv, October 2021\n160\nRao R M, Liu J, Verkuil R, et al. Msa transformer. In: International Conference on Machine Learning, 2021. 8844-8856\n161\nXiao Y, Qiu J, Li Z, et al. Modeling protein using large-scale pretrain language model. arxiv, August 2021\n162\nMin S, Park S, Kim S, et al. Pre-training of deep bidirectional protein sequence representations with structural information.\nIEEE Access, 2021, 9: 123912-123926\n163\nYang K K, Fusi N, Lu A X. Convolutions are competitive with transformers for protein sequence pretraining. bioRxiv, 2022\n164\nWu R, Ding F, Wang R, et al. High-resolution de novo structure prediction from primary sequence. bioRxiv, 2022\n165\nHua W, Dai Z, Liu H, et al. Transformer quality in linear time. In: International Conference on Machine Learning, PMLR,\n2022. 9099-9117\n166\nNijkamp E, Ruffolo J, Weinstein E N, et al. ProGen2: exploring the boundaries of protein language models. arxiv, June\n2022\n167\nFerruz N, Schmidt S, H¨ocker B. A deep unsupervised language model for protein design. bioRxiv, 2022\n168\nRadford A, Wu J, Child R, et al. Language models are unsupervised multitask learners. OpenAI blog, 2019, 1: 9\n169\nHesslow D, Zanichelli N, Notin P, et al. Rita: a study on scaling up generative protein sequence models. arxiv, May 2022\n170\nChen B, Cheng X, Geng Y A, et al. xTrimoPGLM: unified 100B-scale pre-trained transformer for deciphering the language\nof protein. bioRxiv, 2023\n171\nMelnyk I, Chenthamarakshan V, Chen P Y, et al. Reprogramming pretrained language models for antibody sequence infilling.\nIn: International Conference on Machine Learning, PMLR, 2023. 24398-24419\n172\nTruong, T F, Bepler T. PoET: A generative model of protein families as sequences-of-sequences. arxiv, June 2023\n173\nEmaad K, Yun S S, Aaron A, et al. CELL-E2: Translating Proteins to Pictures and Back with a Bidirectional Text-to-Image\nTransformer. In: Thirty-seventh Conference on Neural Information Processing Systems, 2023\n174\nAndreas D, Cecilia L. The Human Protein AtlasSpatial localization of the human proteome in health and disease. Protein\nScience, 2021, 30: 218–233\n175\nZhang Y, Skolnick J. Tm-align: a protein structure alignment algorithm based on the tm-score. Nucleic Acids Research,\n2005\n176\nThomas J, Ramakrishnan N, Bailey-Kellogg, C. Graphical models of residue coupling in protein families. In: Proceedings of\nthe 5th international workshop on Bioinformatics, 2005. 12-20\n177\nVassura M, Margara L, Di Lena P, et al.\nReconstruction of 3D Structures From Protein Contact Maps.\nIEEE/ACM\nTransactions on Computational Biology and Bioinformatics, 2008, 5: 357–367\n178\nWu Q, Peng Z, Anishchenko I, et al.\nProtein contact prediction using metagenome sequence data and residual neural\nnetworks. Bioinformatics, 2020, 36: 41-48\n179\nDerevyanko G, Grudinin S, Bengio Y, et al. Deep convolutional networks for quality assessment of protein folds. Bioinfor-\nmatics, 2018, 34: 4046-4053\n180\nMaddhuri Venkata Subramaniya S R, Terashi G, Jain A, et al.\nProtein contact map refinement for improving structure\nprediction using generative adversarial networks. Bioinformatics, 2021, 37: 3168-3174\n181\nSverrisson F, Feydy J, Correia B E, et al. Fast end-to-end learning on protein surfaces. In: Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, 2021. 15272-15281\n182\nJiao P, Wang B, Wang X, et al. Struct2GO: protein function prediction based on graph pooling algorithm and AlphaFold2\nstructure information. Bioinformatics, 2023, 39: btad637\n183\nGelman S, Fahlberg S A, Heinzelman P, et al. Neural networks to learn protein sequence–function relationships from deep\nmutational scanning data. Proceedings of the National Academy of Sciences, 2021, 118: e2104878118\n184\nXia T, Ku W S. Geometric Graph Representation Learning on Protein Structure Prediction. In: Proceedings of the 27th\nACM SIGKDD Conference on Knowledge Discovery & amp; Data Mining, 2021\n185\nJing X, Xu J. Fast and effective protein model refinement using deep graph neural networks. Nature computational science,\n2021, 1: 462-469\n186\nHermosilla P, Sch¨afer M, Lang M, et al. Intrinsic-extrinsic convolution and pooling for learning on 3d protein structures.\nIn: ICLR, 2021\n187\nCheng S, Zhang L, Jin B, et al. GraphMS: Drug Target Prediction Using Graph Representation Learning with Substructures.\nAppl. Sci. 2021, 11: 3239\n188\nWan F, Hong L, Xiao A, et al.\nNeoDTI: neural integration of neighbor information from a heterogeneous network for\ndiscovering new drug–target interactions. Bioinformatics, 2019, 35: 104-111\n189\nHermosilla P, Ropinski T. Contrastive representation learning for 3d protein structures. arxiv, May 2022\n190\nChen C, Zhou J, Wang F, et al. Structure-aware protein self-supervised learning. arxiv, April 2022\n191\nLi J, Luo S, Deng C. Directed weight neural networks for protein structure representation learning. arxiv, January 2022\n192\nAykent S, Xia T. Gbpnet: Universal geometric representation learning on protein structures. In: Proceedings of the 28th\nACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2022. 4-14\n193\nWu T, Cheng J. Atomic protein structure refinement using all-atom graph representations and SE (3)-equivariant graph\nneural networks. bioRxiv, 2022\n194\nSatorras V G, Hoogeboom E, Welling M. E (n) equivariant graph neural networks. In: International conference on machine\nlearning. PMLR, 2021. 9323-9332\n195\nBepler T, Berger B. Learning protein sequence embeddings using information from structure. arxiv, February 2019\n196\nWang Z, Combs S A, Brand R, et al. LM-GVP: A Generalizable Deep Learning Framework for Protein Property Prediction\nfrom Sequence and Structure. bioRxiv, 2021\n197\nGligorijevi´c V, Renfrew P D, Kosciolek T, et al.\nStructure-based protein function prediction using graph convolutional\nnetworks. Nature communications, 2021, 12: 3168\n198\nMansoor S, Baek M, Madan U, et al. Toward more general embeddings for protein design: Harnessing joint representations\nof sequence and structure. bioRxiv, 2021\n199\nAnishchenko I, Baek M, Park H, et al. Protein tertiary structure prediction and refinement using deep learning and Rosetta\nin CASP14. Proteins: Structure, Function, and Bioinformatics, 2021, 89: 1722-1733\n200\nXia C, Feng S H, Xia Y, et al. Fast protein structure comparison through effective representation learning with contrastive\ngraph neural networks. PLoS computational biology, 2022, 18: e1009986\n201\nKe Z, Shao Y, Lin H, et al. Continual Pre-training of Language Models. In: ICLR, 2023\nSci China Inf Sci\n35\n202\nHe K, Fan H, Wu Y, et al. Momentum contrast for unsupervised visual representation learning. In: Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020\n203\nYou Y, Shen Y. Cross-modality and self-supervised protein embedding for compound–protein affinity and contact prediction.\nBioinformatics, 2022, 38: ii68-ii74\n204\nYang K K, Zanichelli N, Yeh H. Masked inverse folding with sequence transfer for protein representation learning. bioRxiv,\n2022\n205\nZhang, Z, Wang C, Xu, M, et al. A Systematic Study of Joint Representation Learning on Protein Sequences and Structures.\narxiv, March 2023\n206\nZhang Z, Xu M, Lozano A, et al.\nPre-Training Protein Encoder via Siamese Sequence-Structure Diffusion Trajectory\nPrediction. In: Annual Conference on Neural Information Processing Systems, 2023\n207\nSu J, Han C, Zhou Y, et al. SaProt: Protein Language Modeling with Structure-aware Vocabulary. bioRxiv, 2023\n208\nMadani A, McCann B, Naik N, et al. Progen: Language modeling for protein generation. arxiv, April 2020\n209\nFederhen, S. The NCBI Taxonomy database. Nucleic Acids Research, 2012, 40: D136–D143\n210\nBrandes N, Ofer D, Peleg Y, et al.\nProteinBERT: A universal deep-learning model of protein sequence and function.\nBioinformatics, 2022, 38: 2102-10\n211\nZhou H Y, Fu Y, Zhang Z, et al. Protein Representation Learning via Knowledge Enhanced Primary Structure Modeling.\nbioRxiv, 2023\n212\nLee Y, Yu H, Lee J, et al. Pre-training Sequence, Structure, and Surface Features for Comprehensive Protein Representation\nLearning. In: ICLR, 2024\n213\nGu Y, Tinn R, Cheng H, et al. Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing.\nACM Transactions on Computing for Healthcare, 2022: 1–23\n214\nRose P W, Prli´c A, Altunkaya A, et al. The RCSB protein data bank: integrative view of protein, gene and 3D structural\ninformation. Nucleic acids research, 2016: gkw1000\n215\nSuzek B E, Wang Y, Huang H, et al. UniRef clusters: a comprehensive and scalable alternative for improving sequence\nsimilarity searches. Bioinformatics, 2015, 31: 926-932\n216\nOrengo C A, Michie A D, Jones S, et al. CATH–a hierarchic classification of protein domain structures. Structure, 1997, 5:\n1093-1109\n217\nSteinegger M, S¨oding J. Clustering huge protein sequence sets in linear time. Nature communications, 2018, 9: 1-8\n218\nEl-Gebali S, Mistry J, Bateman A, et al. The Pfam protein families database in 2019. Nucleic Acids Research, 2019, 47:\nD427-D432\n219\nVaradi M, Anyango S, Deshpande M, et al.\nAlphaFold Protein Structure Database: massively expanding the structural\ncoverage of protein-sequence space with high-accuracy models. Nucleic acids research, 2022, 50: D439-D444\n220\nMirdita M, Von Den Driesch L, Galiez C, et al. Uniclust databases of clustered and deeply annotated protein sequences and\nalignments. Nucleic Acids Research, 2017, 45: D170-D176\n221\nLo Conte L, Ailey B, Hubbard T J, et al. SCOP: a structural classification of proteins database. Nucleic Acids Research.\n2000, 28: 257-259\n222\nChandonia J M, Fox N K, Brenner S E. SCOPe: manual curation and artifact removal in the structural classification of\nproteins–extended database. Journal of molecular biology, 2017, 429: 348-355\n223\nAhdritz G, Bouatta N, Kadyan S, et al. OpenProteinSet: Training data for structural biology at scale. arxiv, August 2023\n224\nSingh J, Paliwal K, Litfin T,et al. Reaching alignment-profile-based accuracy in predicting protein secondary and tertiary\nstructural properties without alignment. Scientific reports, 2022, 12: 1-9\n225\nSingh J, Litfin T, Singh J, et al. SPOT-Contact-Single: Improving Single-Sequence-Based Prediction of Protein Contact\nMap using a Transformer Language Model. bioRxiv, 2021\n226\nChen C, Zhang Y, Liu X, et al. Bidirectional learning for offline model-based biological sequence design. OpenProteinSet:\nTraining data for structural biology at scale, January 2023\n227\nvan Kempen M, Kim S S, Tumescheit C, et al. Fast and accurate protein structure search with Foldseek. Nature Biotech-\nnology, 2023, 1-4\n228\nOord A, Vinyals O, et al. Neural Discrete Representation Learning. Advances in neural information processing systems,\n2017, 30\n229\nHu B, Tan C, Wu L R, et al. Multimodal Distillation of Protein Sequence, Structure, and Function. arxiv, 2023\n230\nIbtehaz N, Kagaya Y, et al. Domain-PFP allows protein function prediction using function-aware domain embedding repre-\nsentations. Communications Biology, 2023, 6, 1103\n231\nWang R, Sun Y, Luo Y, et al. Injecting Multimodal Information into Rigid Protein Docking via Bi-level Optimization. In:\nThirty-seventh Conference on Neural Information Processing Systems, 2023\n232\nSun D, Huang H, Li Y, et al. DSR: Dynamical Surface Representation as Implicit Neural Networks for Protein. In: Thirty-\nseventh Conference on Neural Information Processing Systems, 2023\n233\nOmelchenko M V, Galperin M Y, Wolf Y I, et al. Non-homologous isofunctional enzymes: a systematic analysis of alternative\nsolutions in enzyme evolution. Biology direct, 2010, 5: 1-20\n234\nWu L, Huang Y, Lin H, et al. A survey on protein representation learning: Retrospect and prospect. arxiv, January 2022\n235\nLiu W, Zhou P, Zhao Z, et al. K-bert: Enabling language representation with knowledge graph. In: Proceedings of the\nAAAI Conference on Artificial Intelligence, 2020. 2901-2908\n236\nWang S, Sun S, Li Z, et al.\nAccurate de novo prediction of protein contact map by ultra-deep learning model.\nPLoS\ncomputational biology, 2017, 13: e1005324\n237\nLi J, Xu J. Study of real-valued distance prediction for protein structure prediction with deep learning.\nBioinformatics,\n2021, 37:3197-3203\n238\nHu B, Zang Z, Tan C, etc. Deep Manifold Transformation for Protein Representation Learning. ICASSP, 2024\n239\nYan Y, Huang S Y. Accurate prediction of inter-protein residue–residue contacts for homo-oligomeric protein complexes.\nBriefings in bioinformatics, 2021, 22: bbab038\n240\nBaek M, DiMaio F, Anishchenko I, et al.\nAccurate prediction of protein structures and interactions using a three-track\nneural network. Science, 2021, 373: 871-876\n241\nJing X, Wu F, Luo X, et al. RaptorX-Single: single-sequence protein structure prediction by integrating protein language\nmodels. bioRxiv, 2023\n242\nChang L, Perez A. AlphaFold encodes the principles to identify high affinity peptide binders. bioRxiv, 2022\n243\nRuffolo J A, Gray J J. Fast, accurate antibody structure prediction from deep learning on massive set of natural antibodies.\nSci China Inf Sci\n36\nBiophysical Journal, 2022\n244\nEvans R, O’Neill M, Pritzel A, et al. Protein complex prediction with AlphaFold-Multimer. bioRxiv, 2022\n245\nBryant P, Pozzati G, Elofsson A. Improved prediction of protein-protein interactions using alphafold2 and extended multiple-\nsequence alignments. bioRxiv, 2021\n246\nShen T, Hu Z, Peng Z, et al.\nE2Efold-3D: End-to-End Deep Learning Method for accurate de novo RNA 3D Structure\nPrediction. arxiv, July 2022\n247\nStein RA, Mchaourab HS. SPEACH AF: Sampling protein ensembles and conformational heterogeneity with Alphafold2.\nPLOS Computational Biology, 2022, 18: e1010483\n248\nAlQuraishi M. End-to-end differentiable learning of protein structure. Cell systems, 2019, 8: 292-301\n249\nDu Y, Meier J, Ma J, et al. Energy-based models for atomic-resolution protein conformations. arxiv, April 2020\n250\nHou M, Jin S, Cui X, et al. Protein multiple conformations prediction using multi-objective evolution algorithm. bioRxiv,\n2023\n251\nWu F, Li S Z. DIFFMD: a geometric diffusion model for molecular dynamics simulations. In: Proceedings of the AAAI\nConference on Artificial Intelligence, 2023. 37: 5321-5329\n252\nLu J, Zhong B, Zhang Z, et al. Str2Str: A Score-based Framework for Zero-shot Protein Conformation Sampling. In: ICLR,\n2024\n253\nZhao K, Xia Y, Zhang F, et al. Protein structure and folding pathway prediction based on remote homologs recognition\nusing PAthreader. Communications Biololy, 2023, 6: 243\n254\nHuang Z, Cui X, Xia Y, et al. Pathfinder: protein folding pathway prediction based on conformational sampling. bioRxiv,\n2023\n255\nLee J H, Yadollahpour P, Watkins A, et al. Equifold: Protein structure prediction with a novel coarse-grained structure\nrepresentation. bioRxiv, 2022\n256\nHiranuma N, Park H, Baek M, et al. Improved protein structure refinement guided by deep learning based accuracy estima-\ntion. Nature communications, 2021, 12: 1-11\n257\nChen B, Xie Z, Xu J, Qiu J, Ye Z, Tang J. Improve the Protein Complex Prediction with Protein Language Models. bioRxiv,\n2022\n258\nFeng T, Gao Z, You J, et al. Deep Reinforcement Learning for Modelling Protein Complexes. In: ICLR, 2024\n259\nBaek M, McHugh R, Anishchenko I, et al.\nAccurate prediction of nucleic acid and protein-nucleic acid complexes using\nRoseTTAFoldNA. bioRxiv, 2022\n260\nZhang J, Liu S, Chen M, et al.\nFew-shot learning of accurate folding landscape for protein structure prediction.\narxiv,\nAugust 2022\n261\nZhang L, Chen J, Shen T, et al.\nEnhancing the Protein Tertiary Structure Prediction by Multiple Sequence Alignment\nGeneration. arxiv, June 2023\n262\nJing B, Erives E, Pao-Huang P, et al. EigenFold: Generative Protein Structure Prediction with Diffusion Models. arxiv,\nApril 2023\n263\nKandathil S M, Greener J G, Lau A M. et al. Ultrafast end-to-end protein structure prediction enables high-throughput\nexploration of uncharacterized proteins. Proceedings of the National Academy of Sciences, 2022, 119: e2113348119\n264\nKrishna R, Wang J, Ahern W, et al. Generalized biomolecular modeling and design with RoseTTAFold All-Atom. bioRxiv,\n2023\n265\nWang S, Li W, Liu S, et al. RaptorX-Property: a web server for protein structure property prediction. Nucleic acids research,\n2016, 44: W430-W435\n266\nWu T, Guo Z, Hou J, et al. DeepDist: real-value inter-residue distance prediction with deep residual convolutional network.\nBMC bioinformatics, 2021, 22: 1-17\n267\nChen T, Gong C, Diaz D J, et al. HotProtein: A novel framework for protein thermostability prediction and editing. In:\nICLR, 2023\n268\nLiu S, Zhu T, Ren M, et al. Predicting mutational effects on protein-protein binding via a side-chain diffusion probabilistic\nmodel. In: NeurIPS, 2023\n269\nLuo S, Su Y, Wu Z, et al. Rotamer Density Estimator is an Unsupervised Learner of the Effect of Mutations on Protein-\nProtein Interaction. In: ICLR, 2023\n270\nShi C, Wang C, Lu J, et al. Protein Sequence and Structure Co-Design with Equivariant Translation. In: ICLR, 2023\n271\nZheng Z, Deng Y, Xue D, et al. Structure-informed Language Models Are Protein Designers. In: ICML, 2023\n272\nLin Y, AlQuraishi M. Generating novel, designable, and diverse protein structures by equivariantly diffusing oriented residue\nclouds. In: ICML, 2023\n273\nSong Z, Li L. Importance Weighted Expectation-Maximization for Protein Sequence Design. In: ICML, 2023\n274\nYim J, Tripple B L, Bortoli V D, et al. SE(3) diffusion model with application to protein backbone generation. In: ICML,\n2023\n275\nKong X, Huang W, Liu Y, et al. End-to-End Full-Atom Antibody Design. In: ICML, 2023\n276\nVerma Y, Heinonen M, Garg V, et al. AbODE: Ab Initio Antibody Design using Conjoined ODEs. In: ICML, 2023\n277\nTan C, Gao Z, Li S Z. Cross-Gate MLP with Protein Complex Invariant Embedding is A One-Shot Antibody Designer.\narxiv, May 2023\n278\nWu F, Li S Z. A Hierarchical Training Paradigm for Antibody Structure-sequence Co-design. arxiv, November 2023\n279\nChen C, Zhang Y, Liu X, et al. Bidirectional Learning for Offline Model-based Biological Sequence Design. In: ICML, 2023\n280\nTan, Zhang Y, Gao Z, et al. RDesign: Hierarchical Data-efficient Representation Learning for Tertiary Structure-based RNA\nDesign. In: ICLR, 2024\n281\nTruong Jr T F, Bepler T. PoET: A generative model of protein families as sequences-of-sequences. In: NeurIPS, 2023\n282\nYi K, Zhou B, Shen Y, et al. Graph Denoising Diffusion for Inverse Protein Folding. In: NeurIPS, 2023\n283\nPei Q, Gao K, Wu L, et al. FABind: Fast and Accurate Protein-Ligand Binding. In: NeurIPS, 2023\n284\nJin W, Sarzikova S, Chen X, et al. Unsupervised Protein-Ligand Binding Energy Prediction via Neural Euler’s Rotation\nEquation. In: NeurIPS, 2023\n285\nWu L, Tian Y, Huang Y, et al.\nMAPE-PPI: Towards Effective and Efficient Protein-Protein Interaction Prediction via\nMicroenvironment-Aware Protein Embedding. In: ICLR, 2024\n286\nWang R, Sun Y, Luo Y, et al. Injecting Multimodal Information into Rigid Protein Docking via Bi-level Optimization. In:\nNeurIPS, 2023\n287\nGao Z, Sun X, Liu Z, et al. Protein Multimer Structure Prediction via Prompt Learning. In: ICLR, 2024\nSci China Inf Sci\n37\n288\nZhang Z, Lu Z, Hao Z, et al. Full-Atom Protein Pocket Design via Iterative Refinement. In: NeurIPS, 2023\n289\nGruver N, Stanton S, Frey N, et al. Protein Design with Guided Discrete Diffusion. In: NeurIPS, 2023\n290\nZhang J O, Diaz D J, Klivans A R, et al. Predicting a Protein ’ s Stability under a Million Mutations. In: NeurIPS, 2023\n291\nHsu C, Verkuil R, Liu J, et al. Learning inverse folding from millions of predicted structures. In: International Conference\non Machine Learning, 2022. 8946-8970\n292\nCao Y, Das P, Chenthamarakshan V, et al. Fold2seq: A joint sequence (1d)-fold (3d) embedding-based generative model for\nprotein design. In: International Conference on Machine Learning, 2021. 1261-1271\n293\nDumortier B, Liutkus A, Carr´e C,et al. PeTriBERT: Augmenting BERT with tridimensional encoding for inverse protein\nfolding and design. bioRxiv, 2022\n294\nDauparas J, Anishchenko I, Bennett N, et al. Robust deep learning–based protein sequence design using ProteinMPNN.\nScience, 2022, 378: 49-56\n295\nGao Z, Tan C, Li S Z. AlphaDesign: A graph protein design method and benchmark on AlphaFoldDB. arxiv, February 2022\n296\nTan C, Gao Z Y, Xia J, et al. Generative de novo protein design with global context. arxiv, April 2022\n297\nKorendovych I V, DeGrado W F. De novo protein design, a retrospective. Quarterly reviews of biophysics, 2020, 53, e3\n298\nMao W, Sun Z, Zhu M, et al. De novo Protein Design Using Geometric Vector Field Networks. In: ICLR, 2024\n299\nNotin P, Dias M, Frazer J, et al. Tranception: protein fitness prediction with autoregressive transformers and inference-time\nretrieval. In: International Conference on Machine Learning, 2022. 16990-17017\n300\nBushuiev A, Bushuiev R, Kouba P, et al. Learning to design protein-protein interactions with enhanced generalization. In:\nICLR, 2024\n301\nMeier J, Rao R, Verkuil R, et al. Language models enable zero-shot prediction of the effects of mutations on protein function.\nAdvances in Neural Information Processing Systems, 2021, 34:29287-303\n302\nHu M, Yuan F, Yang K K, et al. Exploring evolution-based &-free protein language models as protein function predictors.\narxiv, June 2022\n303\nNotin P, Kollasch A W, Ritter D, et al. ProteinGym: Large-Scale Benchmarks for Protein Design and Fitness Prediction.\nIn: NeurIPS. 2023\n304\nRoney JP, Ovchinnikov S. State-of-the-Art estimation of protein model accuracy using AlphaFold. bioRxiv, 2022\n305\nRoney J. Evidence for and Applications of Physics-Based Reasoning in AlphaFold. Doctoral dissertation, 2022\n306\nAkdel M, Pires D E, Pardo E P, et al.\nA structural biology community assessment of alphafold2 applications.\nNature\nStructural & Molecular Biology, 2022\n307\nPak M A, Markhieva K A, Novikova M S, et al. Using alphafold to predict the impact of single mutations on protein stability\nand function. bioRxiv, 2021\n308\nBuel G R, Walters K J. Can AlphaFold2 predict the impact of missense mutations on structure?.\nNature Structural &\nMolecular Biology, 2022, 29: 1-2\n309\nZheng S, Li Y, Chen S, et al. Predicting drug–protein interaction using quasi-visual question answering system. Nature\nMachine Intelligence, 2023, 2: 134–140\n310\nZhang Z, Liu Q. Learning Subpocket Prototypes for Generalizable Structure-based Drug Design. In: ICML, 2023\n311\nGao B, Qiang B, Tan H, et al. DrugCLIP: Contrastive Protein-Molecule Representation Learning for Virtual Screening. In:\nNeurIPS, 2023\n312\nPadmakumar V, Pang R Y, He H, et al. Extrapolative Controlled Sequence Generation via Iterative Refinement. In: ICML,\n2023\n313\nLee S, Jo J, Hwang S J. Exploring Chemical Space with Score-based Out-of-distribution Generation. In: ICML, 2023\n314\nAhdritz G, Bouatta N, Kadyan S, et al. OpenProteinSet: Training data for structural biology at scale. In: NeurIPS, 2023\n315\nKucera T, Oliver C, Chen D, et al. ProteinShake: Building datasets and benchmarks for deep learning on protein structures.\nIn: NeurIPS, 2023\n316\nXu M, Zhang Z, Lu J, et al. PEER: A Comprehensive and Multi-Task Benchmark for Protein Sequence Understanding.\narxiv, June 2022\n317\nGao Z, Tan C, Zhang Y, et al.\nProteinInvBench: Benchmarking Protein Inverse Folding on Diverse Tasks, Models, and\nMetrics. In: NeurIPS, 2023\n318\nJamasb A R, Morehead A, Zhang Z, et al. Evaluating Representation Learning on the Protein Structure Universe. In: ICLR,\n2024\n319\nRoy A, Kucukural A, Zhang Y. I-TASSER: a unified platform for automated protein structure and function prediction.\nNature protocols, 2010, 5: 725-738\n320\nTunyasuvunakool K, Adler J, Wu Z, et al. Highly accurate protein structure prediction for the human proteome. Nature,\n2021, 596: 590-596\n321\nUnder review as a conference paper at ICLR 2024. ProtChatGPT: Towards Understanding Proteins with Large Language\nModels, 2023\n322\nLin Z, Akin H, Rao R, et al. Evolutionary-scale prediction of atomic-level protein structure with a language model. Science,\n2023, 379: 1123-1130\n",
  "categories": [
    "q-bio.BM"
  ],
  "published": "2024-03-08",
  "updated": "2024-03-08"
}