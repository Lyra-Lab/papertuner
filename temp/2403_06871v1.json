{
  "id": "http://arxiv.org/abs/2403.06871v1",
  "title": "On the Generalization Ability of Unsupervised Pretraining",
  "authors": [
    "Yuyang Deng",
    "Junyuan Hong",
    "Jiayu Zhou",
    "Mehrdad Mahdavi"
  ],
  "abstract": "Recent advances in unsupervised learning have shown that unsupervised\npre-training, followed by fine-tuning, can improve model generalization.\nHowever, a rigorous understanding of how the representation function learned on\nan unlabeled dataset affects the generalization of the fine-tuned model is\nlacking. Existing theoretical research does not adequately account for the\nheterogeneity of the distribution and tasks in pre-training and fine-tuning\nstage. To bridge this gap, this paper introduces a novel theoretical framework\nthat illuminates the critical factor influencing the transferability of\nknowledge acquired during unsupervised pre-training to the subsequent\nfine-tuning phase, ultimately affecting the generalization capabilities of the\nfine-tuned model on downstream tasks. We apply our theoretical framework to\nanalyze generalization bound of two distinct scenarios: Context Encoder\npre-training with deep neural networks and Masked Autoencoder pre-training with\ndeep transformers, followed by fine-tuning on a binary classification task.\nFinally, inspired by our findings, we propose a novel regularization method\nduring pre-training to further enhances the generalization of fine-tuned model.\nOverall, our results contribute to a better understanding of unsupervised\npre-training and fine-tuning paradigm, and can shed light on the design of more\neffective pre-training algorithms.",
  "text": "On the Generalization Ability of Unsupervised Pretraining\nYuyang Deng\nJunyuan Hong\nJiayu Zhou\nMehrdad Mahdavi\nPenn State University\nMichigan state university\nMichigan state university\nPenn State University\nAbstract\nRecent advances in unsupervised learning\nhave shown that unsupervised pre-training,\nfollowed by fine-tuning, can improve model\ngeneralization. However, a rigorous under-\nstanding of how the representation function\nlearned on an unlabeled dataset affects the\ngeneralization of the fine-tuned model is lack-\ning. Existing theoretical research does not ad-\nequately account for the heterogeneity of the\ndistribution and tasks in pre-training and fine-\ntuning stage. To bridge this gap, this paper\nintroduces a novel theoretical framework that\nilluminates the critical factor influencing the\ntransferability of knowledge acquired during\nunsupervised pre-training to the subsequent\nfine-tuning phase, ultimately affecting the gen-\neralization capabilities of the fine-tuned model\non downstream tasks. We apply our theo-\nretical framework to analyze generalization\nbound of two distinct scenarios: Context En-\ncoder pre-training with deep neural networks\nand Masked Autoencoder pre-training with\ndeep transformers, followed by fine-tuning on\na binary classification task. Finally, inspired\nby our findings, we propose a novel regulariza-\ntion method during pre-training to further en-\nhances the generalization of fine-tuned model.\nOverall, our results contribute to a better\nunderstanding of unsupervised pre-training\nand fine-tuning paradigm, and can shed light\non the design of more effective pre-training\nalgorithms.\n1\nIntroduction\nUnsupervised representation learning has achieved re-\nmarkable success in various domains, including com-\nProceedings of the 27th International Conference on Artifi-\ncial Intelligence and Statistics (AISTATS) 2024, Valencia,\nSpain. PMLR: Volume 238. Copyright 2024 by the au-\nthor(s).\nputer vision and natural language processing, as ev-\nidenced by a rapidly increasing number of empirical\nstudies [Coates and Ng, 2012, Radford et al., 2015,\nSun et al., 2019, Dosovitskiy et al., 2020, Feichtenhofer\net al., 2022, He et al., 2020, 2022, Devlin et al., 2018,\nChen et al., 2020]. In this learning paradigm, the goal\nis to learn a representation function on a large, possibly\nunlabeled dataset by optimizing a carefully designed un-\nsupervised learning objective. Then, using the learned\nrepresentation, a task-specific classifier, such as the\nhead of a neural network, is trained on a small in-house\ndataset during the fine-tuning stage. This two-stage\nparadigm addresses the issue of small dataset size in\ndownstream tasks. While unsupervised pre-training\nfor transfer learning has experienced significant em-\npirical growth, a comprehensive understanding of the\nfundamental factors that influence the generalization\nperformance of fine-tuned models lags considerably be-\nhind what has been empirically observed [Neyshabur\net al., 2020].\nMost existing generalization bounds primarily rely on\nnotions such as distance between the weights of the\npre-trained and fine-tuned models [Li and Zhang, 2021,\nShachaf et al., 2021] or data-dependent measurements\nsuch as Hessian [Ju et al., 2022] through PAC-Bayesian\nanalysis [Arora et al., 2018, Neyshabur et al., 2018] to\nexamine the performance of fine-tuned model. These\nresults inform the design of effective regularization\nmethods [Li and Zhang, 2021, Ju et al., 2022] or in-\ncorporating consistent losses [Ju et al., 2022] in fine-\ntuning stage to improve the generalization of fine-tuned\nmodel by mitigating issues such as overfitting caused\nby fine-tuning a large model on a small training set\nor instability due to label noise. These generalization\nbounds, however, do not explicitly incorporate other\nkey factors that may govern the success of fine-tuning\nsuch as similarity between the pre-training (on which\na model is pre-trained) and target tasks [Shachaf et al.,\n2021] or task diversity [Tripuraneni et al., 2020], the\nnumber of training samples and complexity of model\nspaces utilized in each stage in a unified bound. For\nexample, in real-world learning tasks, the pre-training\nand fine-tuning tasks may be conducted on completely\ndifferent domains, and we usually employ some kind of\narXiv:2403.06871v1  [cs.LG]  11 Mar 2024\nOn the Generalization Ability of Unsupervised Pretraining\ntransformation on the pre-training data (i.e., adding\nnoise, rotating or masking), which further exacerbates\nthe data heterogeneity. Consequently, a well-designed\ngeneralization theory is expected to take the data het-\nerogeneity into account [Yang et al., 2020]. In modern\ntransfer learning, different tasks can be conducted in\nthe pre-training and fine-tuning stages. For example,\nin a Masked Autoencoder (MAE) [He et al., 2022], a\nregression task utilized during pre-training, while a\nclassification task used for fine-tuning. Therefore, a\ndesired theory should allow for flexibility in choosing di-\nverse types of tasks in the pre-training and fine-tuning\nstages which poses a challenge in formalizing the de-\nsired guarantees.\nMotivated by the above observations, we aim at formal-\nizing and establishing general generalization bounds\non unsupervised pre-training and fine-tuning paradigm\nthat captures aforementioned factors in a unified man-\nner. We introduce the notion of representation trans-\nferrability to quantify how much knowledge can be\ntransferred from unsupervised representation learning\nstage to fine-tuned model, in the presence of task het-\nerogeneity. We then establish a bound on the gen-\neralization capability of fine-tuned model composed\nwith pre-trained representation model that highlights\nhow representation-induced complexity and distribu-\ntion mismatch affects the generalization of fine-tuned\nmodel. We instantiate our theory to the scenario of\nContext Encoder [Pathak et al., 2016] with deep neural\nnetworks and Masked Autoencoder [Devlin et al., 2018,\nHe et al., 2022] with deep Transformer architectures\nwhich highlights the relative merits of learning repre-\nsentations. From a technical perspective, we establish\ngeneralization bounds for multi-layer transformers, by\nderiving the worst case covering number of hypothe-\nsis space by expanding upon the machinery that was\ndeveloped in [Edelman et al., 2022].\nSince our theoretical analysis reveals the representation-\ninduced Rademacher complexity as one of the key fa-\ncotors governing the capacity of the transfer learning,\nit naturally motivates itself to be incorporated as a\nregularizer during pre-training. Inspired by this obser-\nvation, we propose a novel Rademacher representation\nregularized algorithm, dubbed as RadReg, to enhance\nthe generalization capability of the fine-tuned model.\nWe show that by utilizing unlabeled data from the\ndownstream task, we can effectively regularize the pre-\ntrained model to learn representations that entail better\ngeneralization after fine-tuning. We propose an efficient\nalgorithm to optimize the new objective and establish\nits convergence on smooth nonconvex losses.\nContributions. Our main contributions are summa-\nrized as follows:\n• (Theory) We introduce a formal framework to\nstudy the utility of unsupervised representation\nlearning and fine-tuning paradigm (Section 3)\nand derive the generalization bound for fine-\ntuned model based on a pre-trained representa-\ntion function (Section 4). We discover that the\ngeneralization capability of model depends on\nfour key factors: Representation transferrability,\nrepresentation-induced Rademacher complexity,\ndomain heterogeneity, and generalization of the\npre-training task.\n• (Applications) We apply our theory to derive gener-\nalization bound of the pre-training with a context\nencoder (CE) and a masked autoencoder (MAE)\nwith a transformer followed by a binary classifica-\ntion fine-tuning task (Section 5). We show that,\nthe pre-training tasks defined by regression loss\nare provably transferrable to downstream binary\nclassification task. In doing so, to our best knowl-\nedge, we establish the first generalization analysis\nof multi-layer transformer models with residual\nblock.\n• (Algorithm) Inspired by our generalization bounds,\nwe propose a novel Rademacher Representation\nRegularized algorithm, RadReg, for improved pre-\ntraining and provide convergence guarantees for\nnonconvex objectives (Section 6). The experimen-\ntal results show that RadReg can learn better rep-\nresentation than ℓ2 norm regularized training on\ndownstream tasks with a small dataset (Section 7).\n2\nAdditional Related Works\nTheory of Transfer Learning A significant body\nof work [Tripuraneni et al., 2020, Du et al., 2020] fo-\ncuses on the theoretical aspects of transfer learning\nparadigm, trying to answer the question: why transfer\nlearning can work and what factors affect the learning\nperformance? Tripuraneni et al. [2020] give the first\nrisk bound capturing task diversity among pre-training\nand fine-tuning stages as the key quantity affecting\ngeneralization which is also reflected in our generaliza-\ntion analysis. Xu and Tewari [2021] follow the setup\nin [Tripuraneni et al., 2020], and show that even though\nthe model architectures used in representation learn-\ning and fine-tuning are different, the task diversity\nremains bounded.\nDu et al. [2020] study few-shot\nrepresentation learning, where it considers pre-training\na linear representation function by solving the regres-\nsion problem with squared loss (OLS) on a give large\ndataset, and then fine-tuning another linear predictor\non some target dataset. It shows that the generaliza-\ntion will depend on the number of pre-training data\nand fine-tuning data. Similar dependencies appear in\nthe generalization bound obtained in our main theo-\nrem. Zhang et al. [2023] study the general supervised\nYuyang Deng, Junyuan Hong, Jiayu Zhou, Mehrdad Mahdavi\npretraining, and highlight the trade-off between the\nintra and inter class diversity.\nTheory of Modern Unsupervised Representa-\ntion Learning.\nRecently, due to the rise of con-\ntrastive learning [Chen et al., 2020] and masked train-\ning [Devlin et al., 2018, He et al., 2022], a line of\nstudies are devoted to understanding the generaliza-\ntion capability or smaple complexity of these learn-\ning paradigms [HaoChen et al., 2021, Arora et al.,\n2019b, Wang and Isola, 2020, Lee et al., 2021, Ge et al.,\n2023, Gouk et al., 2020, Ju et al., 2022]. Arora et al.\n[2019b] presents a theoretical framework for studying\ncontrastive learning, and shows that it provably reduces\nthe sample complexity of downstream tasks. HaoChen\net al. [2021] consider contrastive learning and estab-\nlishes the theory without conditional independence of\npositive data pairs. Wang and Isola [2020] prove that\ncontrastive learning optimizes for alignment and uni-\nformity asymptotically. Zhang et al. [2022] establish\nthe connection of masked pre-training with contrastive\nlearning over bipartite graphs. Lee et al. [2021] also\nconsider a masking pre-training scenario, but contrary\nto the present work, it assumes the labels are generated\nby a function of masked data plus Gaussian noise, and\nonly focuses on the ERM model as a representation\nfunction. A recent work [Ge et al., 2023] also examine\nthe unsupervised pre-training framework, but the dif-\nference to ours, they consider a maximum likelihood\nestimation as pre-training method, while we start from\ngeneral pre-training task and instantiate it in modern\nmachine learning scenario such as Context Encoder\nand MAE. Gouk et al. [2020] study the end-to-end\nfinetuing scenario, and find that the generalization of\nfinetuned model will depend on the distance that neural\nnetwork weights traveled away from pretrained model.\nThey hence propose a distance regularization finetun-\ning algorithm and achieve better performance. Ju et al.\n[2022] also study the entire model finetuning paradigm,\nand derive a Hessian based generalization bound via\nPAC-Bayesian analysis.\n3\nA Formal Framework\nIn this section we formalize unsupervised pre-training\nfollowed by supervised fine-tuning problem that will\nenable us to study the relative merits of various un-\nsupervised representation learning approaches and ex-\namine their utility on the generalization capability of\ndownstream tasks. In the scenario of unsupervised\nrepresentation pre-training and fine-tuning on a down-\nstream task, we are given two datasets: one, possibly\nlarge, unlabeled pre-training dataset and a small la-\nbeled data set. The goal is to learn model f ◦h which is\ncomposed of task-specific function f ∈F and represen-\ntation function h ∈H, where F and H are model spaces\nfor fine-tuned and representation models, respectively.\nUnsupervised pre-training. We assume access to\na raw pre-training data {˜xi}N\ni=1 drawn from an un-\nknown, arbitrary distribution D over an instance do-\nmain X such as images.\nTo learn representations,\none first transforms (e.g., masking, adding noise, ro-\ntating, or other geometric transformation) unlabeled\ndata into ˜zi = T1(˜xi) ∈X and (self-generated) la-\nbel ˜yi = T2(˜xi) ∈Z using suitable transformers\nT1 : X 7→X and T2 : X 7→Z to generate the pre-\ntraining dataset bU = {(˜zi, ˜yi)}N\ni=1. For example, in\npre-training with masking, our augmented data are\nmasked sentence/image, and self-generated labels are\nthe masked part of data. We denote the transformed\ndistribution over X × Z as U.\nWe note that that\nmarginal distribution UX of U over instance space X is\nnot necessarily same as D of raw data due to random-\nness in data transformation T1. To learn the represen-\ntations, we consider a class of decoding and encoding\npairs, which is closely inspired by [Hazan and Ma, 2016],\nand minimize the following empirical risk\nmin\ng∈G,h∈H L b\nU(g ◦h) := 1\nN\nX\n(˜zi,˜yi)∈b\nU ℓ(g ◦h(˜zi), ˜yi),\n(1)\nwhere G ⊆{I 7→Z} and H ⊆{X 7→I} are the model\nspaces for encoder and decoder, respectively, where\nI denotes the latent space of representations, and ℓ\nis the loss function used for pre-training, e.g., ℓ(g ◦\nh(˜zi), ˜yi) = ∥g ◦h(˜zi) −˜yi∥2\n2. Let ˆg and ˆh denote the\ndecoder and encoder (representation function) obtained\nby solving (1). We define the following excess risk for\npre-training task:\nEU(ˆg, ˆh) := LU(ˆg ◦ˆh) −\nmin\ng∈G,h∈H LU(g ◦h)\nwhere LU(g ◦h) := E(˜z,˜y)∼U[ℓ(f ◦h(˜z), ˜y)] denotes\nthe generalization ability of pre-training task realized\nby distribution U. We note that the learned decoder\nfunction ˆg may be discarded after pre-training. We\nuse h∗\nU = arg minh∈H ming∈G LU(g ◦h) ∈H to denote\noptimal encoder for pre-training task.\nSupervised fine-tuning. In fine-tuning stage, we\nassume access to a labeled downstream dataset bT =\n{xi, yi}n\ni=1 where feature vector xi is sampled based an\nunknown, arbitrary distribution T (possibly different\nfrom D) on domain X, and its label yi is generated\nbased on a labeling function yi = y(xi). The goal is\nto utilize the representation function ˆh obtained by\nsolving (1) to perform fine-tuning on the downstream\ndataset bT to learn a prediction model ˆf from a function\nclass F:\nmin\nf∈F R b\nT (f ◦ˆh) := 1\nn\nX\n(xi,yi)∈b\nT ϕ(f ◦ˆh(xi), yi), (2)\nOn the Generalization Ability of Unsupervised Pretraining\nwhere ϕ is the loss function which is not necessarily\nthe same as the pre-training loss.\nOur goal is to rigorously analyze the generalization\ncapability of the final model which is the composition\nof two functions, i.e., ˆf ◦ˆh where ˆf is the solution of\n(2), by bounding the excess risk\nET ( ˆf, ˆh) = RT ( ˆf ◦ˆh) −\nmin\nf∈F,h∈H RT (f ◦h).\n(3)\nHere RT (f ◦h) := Ex∼T [ϕ(f ◦h(x), y(x))] denotes the\ntrue risk on downstream task realized by distribution\nT over X and underlying labeling function y(·).\n4\nOn the Utility of Unsupervised\nRepresentation Learning\nWe now turn to establishing the generalization bound\nof fine-tuned models given a pre-trained representation\nfunction, and discuss its implications. Before, we first\nintroduce two key notions. We start by introducing\nthe notion of Rademacher complexity of a hypothesis\nspace when individual models are composed with a\nfixed representation function (similar measures appear\nin [Tripuraneni et al., 2020, Xu and Tewari, 2021]).\nDefinition 1 (Representation-induced Rademacher com-\nplexity). For a hypothesis space F of set of real (vector)-\nvalued functions defined over input space X and label\nspace Y, a loss function ϕ : Y ×Y 7→R+, and a dataset\nbT = {xi, yi}n\ni=1, the empirical Representation-induced\nRademacher complexity of F with respect to ϕ and bT ,\nfor a given representation function ˆh, is defined as\nR b\nT (ϕ ◦F ◦ˆh)\n:= Eε∈{±1}n\n\"\nsup\nf∈F\n1\nn\nXn\ni=1 εiϕ(f ◦ˆh(xi), yi)\n#\n,\nwhere ε1, . . . , εn are i.i.d. Rademacher random variables\nwith P{εi = 1} = P{εi = −1} = 1/2.\nThe following definition, relates the generalization of\nfine-tuned and representation models.\nDefinition 2 (Represnetation transferability). Given\ntwo representation functions h, h′ ∈H and a distri-\nbution U for pre-training data, we say a pre-training\ntask and fine-tuning task satisfies (Cβ, β) transferability\nfor some constant 0 < Cβ < ∞, β > 0 on h, h′, if the\nfollowing statement holds:\nmin\nf∈F RT (f ◦h) −min\nf ′∈F RT (f ′ ◦h′)\n≤Cβ\n\u0012\nmin\ng∈G LU(g ◦h) −min\ng′∈G LU(g′ ◦h′)\n\u0013β\nwhere RUX (f ◦h) := Ex∼UX [ϕ(f ◦h(x), y(x))] denotes\nthe risk realized by pre-training marginal data distribu-\ntion UX and downstream labeling function y(·).\nWe note that a similar notation is proposed in the\nanalysis of multi-task learning [Hanneke and Kpotufe,\n2022, Definition 4], to characterize the transferrability\nfrom one task to another task. We emphasize that\nrepresnetation transferability is the key to transfer\nthe generalizability of pre-training model to fine-tuned\nmodel. Unlike the transfer ratio defined in previous\nworks [Tripuraneni et al., 2020, Ge et al., 2023, Zhang\net al., 2023], we have an exponent variable β, which\nallows the transferrability from losses with different\norder, e.g., from a quadratic loss to non-quadratic loss.\nLater on, we show that condition holds essentially under\nrealistic assumptions on suitable data transformations\nto generate pre-training data and model spaces, such\nas pre-training with a inpainting autoencoder and a\nmasked autoencoder with a transformer, where both\nare fine-tuned on a classification task.\nThe next theorem establishes the generalization bound\nof the fine-tuned model on a downstream dataset, given\na pre-trained representation function ˆh.\nTheorem 1. Assume ˆh and ˆg are the pre-trained rep-\nresentation function and its associated decoder function,\nand real valued non-negative loss ϕ to be Gϕ Lipschitz\nand bounded by Bϕ.\nAssume pre-training and fine-\ntuning task admit (Cβ, β) representation transferrabil-\nity on ˆh and h∗\nU. If we solve (2) to get ˆf, then with\nprobability at least 1 −ν, the following statement holds\nET ( ˆf, ˆh) ≤CβEU(ˆg, ˆh)β + 4GϕR b\nT (F ◦ˆh)\n+ 4Bϕ\nr\nlog(1/ν)\nn\n+ 4Bϕ ∥T −UX ∥TV + min\nf∈F ET (f, h∗\nU)\nwhere h∗\nU\n=\narg minh∈H ming∈G LU(g ◦h) is the\noptimal pre-training representation function,\nand\n∥P −Q∥TV = supA∈Ω|P(A) −Q(A)| denotes total\nvariation distance between two distributions.\nThe proof of Theorem 1 is deferred to Appendix B.1.\nTheorem 1 shows that the generalization of the fine-\ntuned model depends on four quantities: i) Repre-\nsentation transferrability, ii) Representation-induced\nRademacher complexity, iii) domain heterogeneity and\niv) generalization of the pre-training task.\nRepresentation transferrability is the key to connect\ndownstream generaliztion with pre-training generaliza-\ntion. It is analogous to task diversity notion in the\nmulti-task learning works [Tripuraneni et al., 2020, Xu\nand Tewari, 2021], since they all measure how well the\nknowledge can be transferred across different learning\nstage. However, our notion is more powerful since we\nneither assume the pre-training and fine-tuning stage\nshare the same type of task, e.g., both being regression,\nnor assume a generic nonlinear feature representation\nis shared across all tasks [Tripuraneni et al., 2020]. As\nYuyang Deng, Junyuan Hong, Jiayu Zhou, Mehrdad Mahdavi\nwe will see in the later section, with the help of rep-\nresentation transferrability, we can show that encoder\nlearnt by regression pre-training can be transferred to\ndownstream classification task. The representation-\ninduced Rademacher complexity will play a key role in\nreflecting how well the learnt representation and F are\ncoupled. Notice that this complexity is defined over\ndownstream data, and only over class F, which means\nthat in fine-tuning stage we only suffer from a smaller\ncomplexity in learning. The price for learning with\npotential more complex encoder class H is paid in pre-\ntraining task. This observation is consistent with a line\nof multi-task or transfer learning works [Tripuraneni\net al., 2020, Du et al., 2020, Xu and Tewari, 2021, Ge\net al., 2023].\nThe domain heterogeneity term ∥T −UX ∥TV charac-\nterizes the statistical heterogeneity between the pre-\ntraining task and the fine-tuning task. Generalization\nof the pre-training task also appears in the bound\nwhich depends on the convergence of the optimization\nalgorithm, and the complexity of the representation\nfunction classes G and H for decoder and encoder,\nrespectively. The last term minf∈F ET (f, h∗\nU) is the\ndownstream risk evaluated with optimal representation\nmodel h∗\nU, which characterize the task heterogeneity be-\ntween pre-training and downstream stages. If the two\ntasks are well aligned, i.e., they share similar optimal\nrepresentation, then this quantity is ignorable.\n5\nThe Power of Unsupervised\nRepresentation Learning\nWe now proceed to establish generalization bounds\nin two distinct settings by refining the generic result\npresented in the previous section.\n5.1\nPre-training with Context Encoder\nThe setting. We start by applying our theory to\nthe setting where inpainting task is considered as pre-\ntraining task and binary classification as downstream\ntask. This learning paradigm is also known as Con-\ntext Encoder (CE) [Pathak et al., 2016], where in pre-\ntraining stage, a deep neural network is trained by\nreconstructing a random transformation of raw data\n(e.g, rotating, scaling, adding Gaussian noise or mask-\ning) of a given image:\nmin\ng∈G,h∈H L b\nU(g ◦h) := 1\nN\nN\nX\ni=1\n∥g(h(˜zi)) −zi∥2\n(4)\nto learn ˆg and ˆh. Then, we discard the decoder ˆg, and\nuse the rest layers as an encoder. A linear projection\nhead is added on top of encoder in fine-tuning stage,\non the downstream binary classification task with data\nx1, . . . , xn using the learnt encoder ˆh:\nmin\nf∈F R b\nT (f ◦ˆh) = 1\nn\nn\nX\ni=1\nϕ(f(ˆh(xi)), yi).\n(5)\nThe encoder-decoder architecture is defined as follows:\nencoder:\nh(x) = σ (WL · · · σ (W1x)) ,\ndecoder:\ng(h(x)) = WL+1h(x),\nwhere W1 ∈Rm×d, W2, . . . , WL ∈Rm×m, and\nWL+1 ∈Rd×m. In fine-tuning stage, we add a linear\nhead on top of encoder function, i.e., f(h(x)) = θ⊤h(x).\nThe hypothesis class for encoder is then defined as:\nH :=\n(\nx 7→σ (WL · · · σ (W1x)) : ∥Wl∥≤W(l),\n∥Wl∥2,1 ≤B(l)\n)\nwhere W(l) and B(l) are upper bound on spectral and\n(2, 1) norms of weight matrices, respectively.\nThe decoder class is defined as:\nG :=\n(\nx 7→WL+1x : ∥WL+1∥≤W(L + 1),\n∥WL+1∥2,1 ≤B(L + 1)\n)\n.\nGeneralization bound.\nThe following lemma es-\ntablishes the representation transferrability of CE pre-\ntraining to binary classification task. We need to make\nthe following assumption\nAssumption 1 (Realizability). There exists g∗∈G\nand h∗\nU ∈H such that LU(g∗◦h∗\nU) = 0.\nRemark 1. In Assumption 1 we assume that there\nexist optimal encoder and decoder that can perfectly\nrealize pre-training task. This is reasonable if we con-\nsider overparameterized model, e.g., deep neural net-\nwork. For example, in masked image reconstruction\npre-training, at the most cases, the remaining part of\nimage is enough for deep model to reconstruct the raw\nimage [Pathak et al., 2016, He et al., 2022].\nLemma 1. Under Assumption 1, CE pre-training ad-\nmits an\n\u0000Ω(1) , 1\n2\n\u0001\nrepresentation transferrability to\nbinary classification task.\nThe proof of Lemma 1 is deferred to Appendix C.1.\nThis lemma shows that generalization of a pre-training\nregression task can be effectively transferred to a down-\nstream binary classification task. Here the transfer\nexponent is 1\n2, which implies that the generalization\nrisk of downstream task will be roughly square root\nof pre-training generalization. To get the excess risk\nrate of downstream task, we need to derive the general-\nization risk of neural network regression. The existing\nworks [Cao and Gu, 2019, 2020, Arora et al., 2019a]\nOn the Generalization Ability of Unsupervised Pretraining\nmainly focus on classification task where the loss func-\ntion is Lipschitz, which is not the case in regression\nloss. Our technique is to generalize the seminal analy-\nsis in [Srebro et al., 2010] for smooth losses and scalar\nvalued hypothesis classes to a vector valued hypothesis\nclass, i.e., neural network class in our case and borrow\nthe standard neural network covering number result\nfrom [Bartlett et al., 2017] to conclude the proof.\nTheorem 2. Assume ˆh and ˆf are the pre-trained rep-\nresentation function and its associated decoder function\nobtained by solving (4) and (5). Let ˜Z = [˜z1; . . . ; ˜zN]\nand X = [x1; . . . ; xN] be pre-training and downstream\ndata, then under Assumption 1 with probability at least\n1 −ν, the following statement holds:\nET ( ˆf, ˆh)\n≤˜O\n\n\n\n\nq\nsL+1 ∥X∥2\nn\n+\nv\nu\nu\nt\n\r\r\r˜Z\n\r\r\r\n2\nsL+1\n\u0010PL+1\nl=1 ρl\n\u00113\nN\n\n\n\n\n+ 4Bϕ\n\n\ns\nlog( 1\nν )\nn\n+ ∥T −UX ∥TV\n\n+ min\nf∈F ET (f, h∗\nU),\nwhere sl = QL+1\nl=1 W 2(l), ρl = B(l)/W(l) .\nThe proof of Theorem 2 is deferred to Appendix C.3.\nHere we achieve roughly O( C(F)\n√n + C(G◦H)\n√\nN\n) bound for\ndownstream task where C(·) denotes the complexity\nof the set. The cost of learning the complex heavy-\nweight encoder is incurred during the pre-training task,\nwhereas in the fine-tuning stage, we only endure the\ncomplexity of learning a lightweight classification head.\n5.2\nPre-training with masked autoencoder\nwith tranformer models\nThe setting. Here we apply our theory to explain the\nempirical success of masked autoencoder pre-training\nmethods [He et al., 2022].\nIn masked autoencoder\npre-training, taking vision tasks for example, we draw\na large set of images Z1, ..., ZN ∈RK×d, and then\nrandomly mask some patches of each image to get\n˜Z1, . . . , ˜ZN ∈RK×d. Then an encoder-decoder model\nis trained by recovering the missing patches:\nmin\ng∈G,h∈H L b\nU(g ◦h) := 1\nN\nN\nX\ni=1\n\r\r\rg(h(˜Zi)) −Zi\n\r\r\r\n2\nF\n(6)\nto get ˆg and ˆh. Finally, we discard the decoder and\nonly fine-tune a new head (e.g., linear projection layer)\non the downstream binary classification task with data\nX1, . . . , Xn using the encoder:\nmin\nf∈F R b\nT (f ◦ˆh) = 1\nn\nn\nX\ni=1\nϕ(f(ˆh(Xi)), yi).\n(7)\nWe consider an L-layer transformer as the pre-training\nencoder model, and a linear projection layer as the\npre-train decoder model, and a linear projection layer\nfor binary classification as fine-tune model.\nencoder:\nh(X) = SAWL (SAWL−1 (· · · SAW1 (X))) ,\ndecoder:\ng(h(X)) = (h(X)) WD,\nwhere SAW(·) is a self-attention module parameter-\nized by W = (WV , WQ, WK, WFC1, WFC2), which is\ndefined as\nSAW(X) = α2σ (ZWFC1) WFC2 + Z,\nZ = (α1A + X) ,\nA = softmax\n\u0012\n1\n√dK\nXWK(XWQ)⊤\n\u0013\nXWV ,\nwhere α1, α2 are some small constant, as used in prac-\ntice [Noci et al., 2022]. We assume lth layer’s weights’\nspectral norm is bounded by W(l), and (2, 1) norm\nis bounded by B(l). In downstream task, we aggre-\ngate (sum) over all patches from encoder, add a linear\nprojection head θ on top of h(X) to make a scalar\noutput:\ndownstream:\nf(h(X)) = (1⊤h(X))θ.\nGeneralization bound. The following lemma estab-\nlishes the representation transferrability of MAE with\na transformer pre-training to binary classification task.\nLemma 2. MAE pre-training admits an\n\u0000Ω(1) , 1\n2\n\u0001\nrepresentation transferrability to binary classification\ntask\nThe proof of Lemma 2 is deferred to Appendix D.1.\nThis implies that MAE pre-training with a multi-layer\ntransformer can be transfered to binary classification\ntask, with a constant factor. The exponent is also 1/2.\nTo derive the excess risk bound of downstream task, we\nneed to find the generalization risk of transformer re-\ngression, which is characterized by the following lemma.\nLemma 3 (Generalization of MAE pre-training task).\nLet ˆg, ˆh be the solution of (6), and ˜Z[N] = [˜Z1; . . . ; ˜ZN]\nis the concatenated pre-training data. Then under As-\nsumption 1 with probability at least 1 −ν the following\nstatement holds:\nEU(ˆg, ˆh) ≤O\n \ns2\nL\n\r\r\r˜Z[N]\n\r\r\r\n2 L+1\nX\nl=1\nρl\nN + log( 1\nν )\nN\n!\n,\nYuyang Deng, Junyuan Hong, Jiayu Zhou, Mehrdad Mahdavi\nwhere\nsl :=\nlY\nj=1\n\u0000α2W 2(j) + 1\n\u0001 \u0000W 2(j)α1K + 1\n\u0001\n,\nρl := O\n\u0000(α1α2W 2(l) + α1)2B2(l) ln(2d2)\n\u0001\n×\n \nK2 + α1W 4(l)\n\u0000sl−1 maxi∈[N] ∥Xi∥\n\u00014\ndK\n!\n+ O\n\u0000α2\n2W 2(l)B2(l)(1 + α2\n1K2W 2(l)) ln(2dm)\n\u0001\n.\nThe proof of Lemma 3 is deferred to Appendix D.2.\nThe proof idea is similar to analysis of CE pre-training,\nwhere we connect local Rademacher complexity to the\ncovering number of model class, and then use tech-\nniques from [Srebro et al., 2010] to establish the gener-\nalization of a smooth loss, i.e. MSE. At the heart of our\nproof is to carefully control the norm of stacked output\nof transformer on N samples, so that the final covering\nnumber does not scale with N, but only depends on\nspectral norm of concatenated samples. We note that\na similar study [Edelman et al., 2022] also establishes\nthe capacity of transformers, but they do not consider\nresidual blocks.\nWe now proceed to determine the excess risk associ-\nated with fine-tuning a transformer model on a binary\nclassification task.\nTheorem 3. Assume ˆh and ˆf are the pre-trained rep-\nresentation function and its associated decoder function\nobtained by solving (6) and (7). Let ˜Z[N] = [˜Z1; ...˜ZN]\nand X[N] = [X1; ...XN] be pre-training and downstream\ndata, then under Assumption 1, with probability at least\n1 −ν, the following statement holds:\nET ( ˆf, ˆh)\n≤O\n\n\n\n\nq\nsL\n\r\rX[N]\n\r\r2\nn\n+\nv\nu\nu\nts2\nL\n\r\r\r˜Z[N]\n\r\r\r\n2 PL\nl=1 ρl\nN\n\n\n\n\n+ 4Bϕ\n\n\ns\nlog( 1\nν )\nn\n+ ∥T −UX ∥TV\n\n+ min\nf∈F ET (f, h∗\nU),\nwhere sl, ρl are constants as defined in Lemma 3.\nThe proof of Theorem 3 is deferred to Appendix D.3.\nOur observations are similar to those in the CE scenario:\nsince we train the encoder only on the pre-training\ndataset, during downstream learning, we mainly con-\ntend with the intricacies of a smaller model class, which\nresults in the generalization bound of O( C(F)\n√n + C(G◦H)\n√\nN\n).\nMeanwhile, it’s worth noting that the introduction of\nmasking may potentially reduce the norm of the data,\ndenoted as\n\r\r\r˜Z[N]\n\r\r\r\n2\n, thereby diminishing the influence\nof the second term. On the other hand, it could also\namplify the domain discrepancy, i.e., ∥T −UX ∥TV.\n6\nEffective Learning via Rademacher\nRepresentation Regularization\nAs shown in Theorem 1, a significant quantity that\naffects the generalization risk of downstream task is\nR b\nT (ϕ ◦F ◦ˆh), the Rademacher complexity of F given\nlearnt representation function ˆh. Here we devise an\nalgorithm to leverage the unlabeled downstream data in\nthe pre-training stage, to regularize the representation\nfunction and further improve the accuracy of fine-tuned\nmodel.\nLet us first consider binary classification case with\nbinary label yi ∈{−1, +1}.\nThe idea is that, in\nthe binary classification setting, the Rademacher com-\nplexity is independent of labels, and hence it can be\nprecisely estimated by only unlabeled downstream\ndataset.\nIf we assume ϕ is Gϕ Lipschitz, then ac-\ncording to the contraction property of Rademacher\ncomplexity [Ledoux and Talagrand, 2013], we have:\nR b\nT (ϕ ◦F ◦ˆh) ≤GϕEσ\nh\nsupf∈F\n1\nn\nPn\ni=1 σif(ˆh(xi))\ni\n,\nwhere we can see that, due to the randomness of\nRademacher variables, the upper bound of Rademacher\ncomplexity can be estimated without knowing the ac-\ntual labels {yi}n\ni=1. Hence, we can leverage the unla-\nbeled downstream data in the pre-training stage, to\nregularize the representation function. This can be cast\nas the following problem:\nmin\ng∈H,h∈H L b\nU(g ◦h) + λEσ\n\"\nsupf∈F\n1\nn\nn\nX\ni=1\nσif(h(xi))\n#\n,\nwhere λ is the regularization coefficient.\nThe idea\ncan also be generalized to multi-class classification,\nwhere we use a vector contraction lemma to esti-\nmate the upper bound of this complexity which we\ndiscuss in Section 6.1. To estimate the expectation,\nwe sample B configurations of Rademacher variables\n{σj = [σj\n1, ..., σj\nn]}B\nj=1. If we assume f and h are pa-\nrameterized by v ∈V and w ∈W, respectively, we\nhave\nmin\nw∈W L b\nU(w) + λ\nB\nB\nX\nj=1\n\"\nmax\nvj∈V\n1\nn\nn\nX\ni=1\nRj(vj, w; xi)\n#\n(8)\nwhere Rj(v, w; xi) := σj\ni fv(hw(xi)).\nOptimization method. To solve the aforementioned\noptimization problems, we adapt the celebrated SGDA\nalgorithm [Lin et al., 2019] (Algorithm 1).\nAt the\nbeginning of each iteration, we first sample a batch of\nn′ pre-training data {zt\ni}n′\ni=1, and then do mini-batch\nOn the Generalization Ability of Unsupervised Pretraining\nAlgorithm 1: RadReg: Rademacher Regularized\nPre-training\nInput: Number of iterations T; regularization\nparameter λ\nSample B configurations of Rademacher variables\n{σ1, . . . , σB},\nInitialize v0\nj = 0, ∀j ∈[B]\nfor t = 0, . . . , T −1 do\nSample a batch of data from pre-training\ndataset {˜zt\n1, . . . , ˜zt\nn′}\nSample a batch of data from downstreaming\ndataset {˜xt\n1, ..., ˜xt\nn′}\nfor j = 1, ..., B do\nvt+1\nj\n= vt\nj + γ 1\nn′\nPn′\ni=1 ∇vRj(vt\nj, wt; ˜xt\ni).\n# Dual variable update\nend\nwt+1 = wt −η 1\nn′\nPn′\ni=1 ∇L b\nU(wt; ˜zt\ni)\n−ηλ 1\nB\nPB\nj=1\nh\n1\nn′\nPn′\ni=1 ∇wRj(vt\nj, wt; ˜xt\ni)\ni\n# Representation model update\nend\nOutput: ˆw uniformly sampled from {wt}T\nt=1.\nstochastic gradient descent:\nwt+1 = wt −η 1\nn′\nXn′\ni=1 ∇L b\nU(wt; ˜zt\ni)\n−ηλ 1\nB\nXB\nj=1\n\u0014 1\nn′\nXn′\ni=1 ∇wRj(vt\nj, wt; ˜xt\ni)\n\u0015\n.\nTo solving the inner max problem, we sample an-\nother batch of n′ downstream (unlabeled) data, and\nthen we do one step mini-batch stochastic gradient\nascent:vt+1\nj\n= vt\nj + γ 1\nn′\nPn′\ni=1 ∇vRj(vt\nj, wt; ˜xt\ni).\nConvergence\nanalysis\nof\nRadReg.\nTo estab-\nlish the convergence of RadReg on (8), we consider\nthe following primal function:Ψ(w)\n:=\nL b\nU(w) +\nλ 1\nB\nPB\nj=1\n\u0002\nmaxvj∈V 1\nn\nPn\ni=1 Rj(vj, w; xi)\n\u0003\n. Then we\nfollow [Lin et al., 2020] and consider the following\nMoreau envelope function:\nDefinition 3 (Moreau Envelope). A function Ψρ(w)\nis the ρ-Moreau envelope of a function Ψ if Ψρ(w) :=\nminw′∈W{Ψ(w′) + 1\n2ρ∥w′ −w∥2}.\nTheorem 4 (Convergence of RadReg with Linear Top\nLayer, Informal). RadReg (Algorithm 1 converge to ϵ-\nstationary point of Ψ1/4L(w) with gradient complexity\nbounded by O\n\u0000B/ϵ8\u0001\n.\nThe formal version of Theorem 4 as well as the proof is\ndeferred to Appendix E. We can see that the proposed\noptimization algorithm can find an ϵ-stationary point\nwith at most O\n\u0000B/ϵ8\u0001\nstochastic gradient evaluations.\nGiven that complexity increases with respect to B, it\nbecomes crucial to have an appropriately sized sample\nof Rademacher variable.\n6.1\nMulti-class\nThe proposed regularization idea can also be general-\nized to multi-class classification. If the model f(·) is a\nvector-valued function, i.e., in multi-class classification,\nf(h(x)) : X 7→Ro, we can apply the following vector-\nvalued contraction lemma of Rademacher complexity:\nLemma 4. [Maurer, 2016] Let ϕ(·, ·) : Ro 7→R be Gϕ-\nLipschitz in the first argument, and f(·) : Rd′ 7→Ro\nbe vector-valued function. Then, the following facts\nhold true for Rademacher complexity over F and any\nh : X 7→Rd′:\nEε\n\"\nsup\nf∈F\n1\nn\nn\nX\ni=1\nεiϕ(f(h(xi)), yi)\n#\n≤\n√\n2GϕEεi\n\"\nsup\nf∈F\n1\nn\nn\nX\ni=1\nεif(h(xi))\n#\n,\nwhere εi ∈{−1, +1}o is Rademacher vector.\nNow the empirical minimization problem becomes:\nmin\nw∈W L ˆU(w) + λ 1\nB\nB\nX\nj=1\n\"\nmax\nV∈V\n1\nn\nn\nX\ni=1\n(σj\ni )⊤f(h(xi))\n#\n.\nSpecifically, if the top layer is a linear projection layer,\ni.e., f(h(x)) = Vh(x), the objective is equivalent to:\nmin\nw∈W L ˆ\nU(w) + λ 1\nB\nB\nX\nj=1\n\"\nmax\nV∈V tr\n \nV 1\nn\nn\nX\ni=1\nh(xi)(σj\ni )⊤\n!#\n,\nwhere tr(·) denotes the trace of a matrix. Here the\ninner problem is convex and easy to solve with simple\n(stochastic) gradient ascent.\n7\nExperiments\nIn this section, we empirically evaluate the proposed\nregularization method in improving the generalization\nof unsupervised pre-training to downstream tasks. We\nutilize the Masked AutoEncoder (MAE) [He et al.,\n2022] as the base unsupervised pre-training method.\nWe conduct experiments using 50,000 images from CI-\nFAR10 dataset [Krizhevsky et al., 2009] for pre-training\nand 4,096 few-shot STL [Coates et al., 2011] samples\nfor finetuning. Since our regularization requires un-\nlabeled data from the downstream task, but L2 and\nnon-regularization methods cannot leverage those data,\nfor a fair comparison, we incorporate the fine-tuning\ndata into a separate unsupervised loss with the same\nYuyang Deng, Junyuan Hong, Jiayu Zhou, Mehrdad Mahdavi\nFigure 1: Testing and training accuracy by epochs, averaged by three repetitions.\nReg.\nλ\nFinal Acc Best Acc Train Acc\nNone\n-\n70.8 (0.2)\n70.9 (0.3)\n100 (0.)\n10−4\n70.7 (0.3)\n70.7 (0.3)\n100 (0.)\nL2\n10−3\n70.8 (0.6)\n70.9 (0.5)\n100 (0.)\n10−2\n70.7 (0.6)\n70.9 (0.5)\n100 (0.)\nRadReg\n10−5\n71.5 (0.2) 71.8 (0.3)\n100 (0.)\n10−4\n71.5 (0.4)\n71.6 (0.7)\n100 (0.)\n10−3\n69.6 (0.6)\n69.6 (0.5)\n100 (0.)\n(a) End-to-end fine-tuning\nReg.\nλ\nFinal Acc Best Acc Train Acc\nNone\n-\n55.3 (0.1)\n55.5 (0.0)\n65.7 (0.2)\nL2\n10−5\n55.3 (0.1)\n55.5 (0.0)\n57.5 (0.1)\n10−4\n55.9 (0.1)\n56.0 (0.1)\n58.2 (0.1)\n10−3\n54.4 (0.0)\n54.5 (0.0)\n58.2 (0.1)\nRadReg 10−5\n56.8 (0.0) 56.9 (0.1)\n65.7 (0.2)\n10−4\n26.8 (0.0)\n27.0 (0.1)\n40.6 (0.1)\n(b) Linear fine-tuning\nTable 1: Evaluation of MAE. Average fine-tuning accu-\nracy is reported with its standard deviations in brack-\nets.\nformulation as the MAE loss:\nmin\ng,h L b\nU(g ◦h) + α · L b\nD(g ◦h) + λR b\nT (F ◦h) (RadReg),\nmin\ng,h L b\nU(g ◦h) + α · L b\nD(g ◦h) + λ ∥W∥2\n(L2),\nmin\ng,h L b\nU(g ◦h) + α · L b\nD(g ◦h)\n(Non-regularized),\nwhere we assume h is parameterized by W and α is\nfixed as 0.01. Our proposed regularization will fur-\nther leverage the data to control the complexity of\nlearned representations. The details of experiments are\nincluded in Appendix F.\nIn Table 1, we compare our method to non-regularized\nMAE training and the one with L2 regularization. We\nrepeat the fine-tuning three times by randomly selecting\n4096 samples from the preset STL10 training set, and\nreport the mean and standard deviations. We vary the\ncoefficient for our and L2 regularization and compare\nthe best test accuracy on fine-tuning. We observe that\nour method can effectively improve the downstream\nperformance as early as in the pre-training stage with-\nout using any labels. Compared to L2 regularization,\nour method can achieve higher test accuracy.\nIn Figure 1, we show the learning curves by different\nregularization strategies. Due to the large capacity\nof the pre-trained ViT encoder, all methods can suffi-\nciently fit the training set approaching 100% training\naccuracy, but the testing accuracy reaches the ceiling.\nOur method can improve the best test accuracy by\nlimiting the representation complexity as early as the\npre-training stage. Our method also improves the con-\nvergence rate at fine-tuning, when our method reaches\nthe 71% test accuracy at epoch 80 but the best baseline\nreaches the same accuracy after 200 epochs.\n8\nConclusion\nThis paper establishes a generic learning bound in un-\nsupervised representation pre-training and fine-tuning\nparadigm.\nWe discover that the generalization de-\npends on representation transferrbaility, representation-\ninduced Rademacher complexity, task heterogeneity\nand generalization of pre-training task. We apply our\ntheory to analyze the generalization of CE and MAE\npre-training.\nMotivated by our theory, we propose\nRademacher representation regularization, with a prov-\nable convergence guarantee. The experiments validate\nthe superiority of our algorithm. As a future direction,\nit would be interesting to expand our analysis to end-\nto-end model fine-tuning, where task specific head and\nencoder are jointly updated in fine-tuning stage.\nAcknowledgement\nThe work of YD and MM was partially supported by\nNSF CAREER Award #2239374 and NSF CNS Award\n#1956276. JZ was supported by NSF #IIS-2212174\nand IIS-1749940 and NIA #1RF1AG072449.\nOn the Generalization Ability of Unsupervised Pretraining\nReferences\nSanjeev Arora, Rong Ge, Behnam Neyshabur, and\nYi Zhang. Stronger generalization bounds for deep\nnets via a compression approach. In International\nConference on Machine Learning, pages 254–263.\nPMLR, 2018.\nSanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and\nRuosong Wang. Fine-grained analysis of optimization\nand generalization for overparameterized two-layer\nneural networks.\nIn International Conference on\nMachine Learning, pages 322–332. PMLR, 2019a.\nSanjeev Arora, Hrishikesh Khandeparkar, Mikhail Kho-\ndak, Orestis Plevrakis, and Nikunj Saunshi. A the-\noretical analysis of contrastive unsupervised repre-\nsentation learning. arXiv preprint arXiv:1902.09229,\n2019b.\nPeter L Bartlett, Dylan J Foster, and Matus J Telgar-\nsky. Spectrally-normalized margin bounds for neural\nnetworks. Advances in neural information processing\nsystems, 30, 2017.\nOlivier Bousquet. Concentration inequalities and em-\npirical processes theory applied to the analysis of\nlearning algorithms. 01 2002.\nYuan Cao and Quanquan Gu. Generalization bounds of\nstochastic gradient descent for wide and deep neural\nnetworks. Advances in neural information processing\nsystems, 32, 2019.\nYuan Cao and Quanquan Gu.\nGeneralization er-\nror bounds of gradient descent for learning over-\nparameterized deep relu networks. In Proceedings\nof the AAAI Conference on Artificial Intelligence,\nvolume 34, pages 3349–3356, 2020.\nTing Chen, Simon Kornblith, Mohammad Norouzi, and\nGeoffrey Hinton. A simple framework for contrastive\nlearning of visual representations. In International\nconference on machine learning, pages 1597–1607.\nPMLR, 2020.\nAdam Coates and Andrew Y Ng. Learning feature\nrepresentations with k-means. In Neural networks:\nTricks of the trade, pages 561–580. Springer, 2012.\nAdam Coates, Andrew Ng, and Honglak Lee. An anal-\nysis of single-layer networks in unsupervised feature\nlearning. In Proceedings of the fourteenth interna-\ntional conference on artificial intelligence and statis-\ntics, pages 215–223. JMLR Workshop and Conference\nProceedings, 2011.\nDamek Davis and Dmitriy Drusvyatskiy. Stochastic\nmodel-based minimization of weakly convex func-\ntions. SIAM Journal on Optimization, 29(1):207–239,\n2019.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. Bert: Pre-training of deep bidi-\nrectional transformers for language understanding.\narXiv preprint arXiv:1810.04805, 2018.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Un-\nterthiner, Mostafa Dehghani, Matthias Minderer,\nGeorg Heigold, Sylvain Gelly, et al. An image is\nworth 16x16 words: Transformers for image recog-\nnition at scale. arXiv preprint arXiv:2010.11929,\n2020.\nSimon S Du, Wei Hu, Sham M Kakade, Jason D Lee,\nand Qi Lei. Few-shot learning via learning the repre-\nsentation, provably. arXiv preprint arXiv:2002.09434,\n2020.\nBenjamin L Edelman, Surbhi Goel, Sham Kakade, and\nCyril Zhang. Inductive biases and variable creation in\nself-attention mechanisms. In International Confer-\nence on Machine Learning, pages 5793–5831. PMLR,\n2022.\nChristoph Feichtenhofer, Haoqi Fan, Yanghao Li, and\nKaiming He. Masked autoencoders as spatiotemporal\nlearners. arXiv preprint arXiv:2205.09113, 2022.\nJiawei Ge, Shange Tang, Jianqing Fan, and Chi Jin. On\nthe provable advantage of unsupervised pretraining.\narXiv preprint arXiv:2303.01566, 2023.\nHenry Gouk, Timothy Hospedales, et al. Distance-\nbased regularisation of deep networks for fine-tuning.\nIn International Conference on Learning Represen-\ntations, 2020.\nSteve Hanneke and Samory Kpotufe. A no-free-lunch\ntheorem for multitask learning. The Annals of Statis-\ntics, 50(6):3119–3143, 2022.\nJeff Z HaoChen, Colin Wei, Adrien Gaidon, and Tengyu\nMa. Provable guarantees for self-supervised deep\nlearning with spectral contrastive loss. Advances\nin Neural Information Processing Systems, 34:5000–\n5011, 2021.\nElad Hazan and Tengyu Ma. A non-generative frame-\nwork and convex relaxations for unsupervised learn-\ning.\nAdvances in Neural Information Processing\nSystems, 29, 2016.\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and\nRoss Girshick. Momentum contrast for unsupervised\nvisual representation learning.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and\npattern recognition, pages 9729–9738, 2020.\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li,\nPiotr Dollár, and Ross Girshick. Masked autoen-\ncoders are scalable vision learners. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 16000–16009, 2022.\nYuyang Deng, Junyuan Hong, Jiayu Zhou, Mehrdad Mahdavi\nHaotian Ju, Dongyue Li, and Hongyang R Zhang. Ro-\nbust fine-tuning of deep neural networks with hessian-\nbased generalization guarantees. In International\nConference on Machine Learning, pages 10431–10461.\nPMLR, 2022.\nAlex Krizhevsky, Geoffrey Hinton, et al.\nLearning\nmultiple layers of features from tiny images. 2009.\nMichel Ledoux and Michel Talagrand. Probability in\nBanach Spaces: Isoperimetry and Processes. Springer\nScience & Business Media, 2013.\nJason D Lee, Qi Lei, Nikunj Saunshi, and Jiacheng\nZhuo. Predicting what you already know helps: Prov-\nable self-supervised learning. Advances in Neural\nInformation Processing Systems, 34:309–323, 2021.\nDavid A Levin and Yuval Peres. Markov chains and\nmixing times, volume 107. American Mathematical\nSoc., 2017.\nDongyue Li and Hongyang Zhang. Improved regulariza-\ntion and robustness for fine-tuning in neural networks.\nAdvances in Neural Information Processing Systems,\n34:27249–27262, 2021.\nTianyi Lin, Chi Jin, and Michael I Jordan. On gradi-\nent descent ascent for nonconvex-concave minimax\nproblems. arXiv preprint arXiv:1906.00331, 2019.\nTianyi Lin, Chi Jin, and Michael Jordan. On gradi-\nent descent ascent for nonconvex-concave minimax\nproblems. In International Conference on Machine\nLearning, pages 6083–6093. PMLR, 2020.\nAndreas Maurer. A vector-contraction inequality for\nrademacher complexities. In International Confer-\nence on Algorithmic Learning Theory, pages 3–17.\nSpringer, 2016.\nBehnam Neyshabur, Srinadh Bhojanapalli, and Nathan\nSrebro.\nA pac-bayesian approach to spectrally-\nnormalized margin bounds for neural networks. In\nInternational Conference on Learning Representa-\ntions, 2018.\nBehnam Neyshabur, Hanie Sedghi, and Chiyuan Zhang.\nWhat is being transferred in transfer learning? Ad-\nvances in neural information processing systems, 33:\n512–523, 2020.\nLorenzo Noci, Sotiris Anagnostidis, Luca Biggio, Anto-\nnio Orvieto, Sidak Pal Singh, and Aurelien Lucchi.\nSignal propagation in transformers: Theoretical per-\nspectives and the role of rank collapse. arXiv preprint\narXiv:2206.03126, 2022.\nDeepak Pathak, Philipp Krahenbuhl, Jeff Donahue,\nTrevor Darrell, and Alexei A Efros. Context encoders:\nFeature learning by inpainting. In Proceedings of\nthe IEEE conference on computer vision and pattern\nrecognition, pages 2536–2544, 2016.\nAlec Radford, Luke Metz, and Soumith Chintala. Un-\nsupervised representation learning with deep con-\nvolutional generative adversarial networks. arXiv\npreprint arXiv:1511.06434, 2015.\nHassan Rafique, Mingrui Liu, Qihang Lin, and Tianbao\nYang. Weakly-convex concave min-max optimization:\nProvable algorithms and applications in machine\nlearning. arXiv preprint arXiv:1810.02060, 2018.\nAxel Ruhe. Perturbation bounds for means of eigen-\nvalues and invariant subspaces.\nBIT Numerical\nMathematics, 10:343–354, 1970. URL https://api.\nsemanticscholar.org/CorpusID:122004897.\nGal Shachaf, Alon Brutzkus, and Amir Globerson. A\ntheoretical analysis of fine-tuning with linear teachers.\nAdvances in Neural Information Processing Systems,\n34:15382–15394, 2021.\nNathan Srebro, Karthik Sridharan, and Ambuj Tewari.\nSmoothness, low noise and fast rates. Advances in\nneural information processing systems, 23, 2010.\nChen Sun, Austin Myers, Carl Vondrick, Kevin Murphy,\nand Cordelia Schmid. Videobert: A joint model for\nvideo and language representation learning. In Pro-\nceedings of the IEEE/CVF international conference\non computer vision, pages 7464–7473, 2019.\nNilesh Tripuraneni, Michael Jordan, and Chi Jin. On\nthe theory of transfer learning: The importance of\ntask diversity. Advances in neural information pro-\ncessing systems, 33:7852–7862, 2020.\nTongzhou Wang and Phillip Isola. Understanding con-\ntrastive representation learning through alignment\nand uniformity on the hypersphere. In International\nConference on Machine Learning, pages 9929–9939.\nPMLR, 2020.\nKan Wu, Jinnian Zhang, Houwen Peng, Mengchen\nLiu, Bin Xiao, Jianlong Fu, and Lu Yuan. Tinyvit:\nFast pretraining distillation for small vision trans-\nformers. In 17th European Conference Computer\nVision–ECCV 2022:, pages 68–85. Springer, 2022.\nZiping Xu and Ambuj Tewari. Representation learning\nbeyond linear prediction functions. Advances in Neu-\nral Information Processing Systems, 34:4792–4804,\n2021.\nFan Yang, Hongyang R Zhang, Sen Wu, Weijie J\nSu, and Christopher Ré.\nAnalysis of informa-\ntion transfer from heterogeneous sources via pre-\ncise high-dimensional asymptotics. arXiv preprint\narXiv:2010.11750, 2020.\nJieyu Zhang, Bohan Wang, Zhengyu Hu, Pang Wei\nKoh, and Alexander Ratner. On the trade-off of\nintra-/inter-class diversity for supervised pre-training.\narXiv preprint arXiv:2305.12224, 2023.\nOn the Generalization Ability of Unsupervised Pretraining\nQi Zhang, Yifei Wang, and Yisen Wang. How mask mat-\nters: Towards theoretical understandings of masked\nautoencoders.\narXiv preprint arXiv:2210.08344,\n2022.\nYuyang Deng, Junyuan Hong, Jiayu Zhou, Mehrdad Mahdavi\nChecklist\n1. For all models and algorithms presented, check if\nyou include:\n(a) A clear description of the mathematical set-\nting, assumptions, algorithm, and/or model.\n[Yes]\n(b) An analysis of the properties and complexity\n(time, space, sample size) of any algorithm.\n[Yes]\n(c) (Optional) Anonymized source code, with\nspecification of all dependencies, including\nexternal libraries. [Not Applicable]\n2. For any theoretical claim, check if you include:\n(a) Statements of the full set of assumptions of\nall theoretical results. [Yes]\n(b) Complete proofs of all theoretical results.\n[Yes]\n(c) Clear explanations of any assumptions. [Yes]\n3. For all figures and tables that present empirical\nresults, check if you include:\n(a) The code, data, and instructions needed to re-\nproduce the main experimental results (either\nin the supplemental material or as a URL).\n[Yes]\n(b) All the training details (e.g., data splits, hy-\nperparameters, how they were chosen). [Yes]\n(c) A clear definition of the specific measure or\nstatistics and error bars (e.g., with respect to\nthe random seed after running experiments\nmultiple times). [Yes]\n(d) A description of the computing infrastructure\nused. (e.g., type of GPUs, internal cluster, or\ncloud provider). [No]\n4. If you are using existing assets (e.g., code, data,\nmodels) or curating/releasing new assets, check if\nyou include:\n(a) Citations of the creator If your work uses\nexisting assets. [Yes]\n(b) The license information of the assets, if appli-\ncable. [Not Applicable]\n(c) New assets either in the supplemental material\nor as a URL, if applicable. [Not Applicable]\n(d) Information\nabout\nconsent\nfrom\ndata\nproviders/curators. [Not Applicable]\n(e) Discussion of sensible content if applicable,\ne.g., personally identifiable information or of-\nfensive content. [Not Applicable]\n5. If you used crowdsourcing or conducted research\nwith human subjects, check if you include:\n(a) The full text of instructions given to partici-\npants and screenshots. [Not Applicable]\n(b) Descriptions of potential participant risks,\nwith links to Institutional Review Board (IRB)\napprovals if applicable. [Not Applicable]\n(c) The estimated hourly wage paid to partici-\npants and the total amount spent on partici-\npant compensation. [Not Applicable]\nOn the Generalization Ability of Unsupervised Pretraining\nOrganization\nThe appendix is organized as follows. In Appendix A we will introduce some helper inequalities\nthat we will be utilized in our proofs and prove the main generalization theorem in Appendix B. In Appendices C\nand D we will provide the proofs in Section 5.1 (generalization of pre-training with a context encoder) and\nSection 5.2 (generalization of pre-training with masked autoencoder with a transformer), respectively.\nIn\nAppendix E, we provide the proof of convergence of the proposed algorithm in Section 6. At last, in Appendix F\nwe will provide the details of setup for our experiments.\nA\nBasic Inequalities\nIn this section, we provide some general technical results that will be used in our proofs.\nProposition 1 (Total variation distance and L1 distance).\n[Levin and Peres, 2017, Proposition 4.2] Given two\nprobability measures P and Q defined over instance space X, the following inequality holds:\n∥P −Q∥TV = 1\n2\nX\nx∈X\n|P(x) −Q(x)|.\nProposition 2 (Ruhe’s trace inequality).\n[Ruhe, 1970] If A and B are positive semidefinite Hermitian matrices\nwith eigenvalues,\na1 ≥... ≥an ≥0, b1 ≥... ≥bn ≥0,\n(9)\nrepsectively, then\nn\nX\ni=1\naibn−i+1 ≤tr (AB) ≤\nn\nX\ni=1\naibi.\nB\nProof of Main Generalization Theorem\nIn this section we provide the proof of main result on generalization of fine-tuned model composed with an\nunsupervised pre-trained model stated in Theorem 1. For readability purposes, we re-state the theorem here:\nTheorem 5 (Theorem 1 restated). Assume ˆh and ˆg are the pre-trained representation function and its associated\ndecoder function, and real valued non-negative loss ϕ to be Gϕ Lipschitz and bounded by Bϕ. Assume pre-training\nand fine-tuning task admit a (Cβ, β) representation transferrability on ˆh and h∗\nU . If we solve (2) to get ˆf, then\nwith probability at least 1 −ν, the following statement holds\nET ( ˆf, ˆh) ≤Cβ\n\u0010\nEU(ˆg, ˆh)\n\u0011β\n+ 4GϕR b\nT (F ◦ˆh) + 4Bϕ\nr\nlog(1/ν)\nn\n+ 4Bϕ ∥T −UX ∥TV + min\nf∈F ET (f, h∗\nU),\nwhere h∗\nU = arg minh∈H ming∈G LU(g ◦h) is the optimal pre-training representation function, and ∥P −Q∥TV =\nsupA∈Ω|P(A) −Q(A)| denotes total variation distance between two distributions.\nB.1\nProof of Theorem 1\nProof. For the ease of presentation we define\nf ∗\nT (h) = arg min\nf∈F RT (f ◦h) := Ex∼T [ϕ(f ◦h(x), y(x))].\nThat is, the optimal fine-tuned risk minimizer in function class F w.r.t. distribution T over domain, given a\nrepresentation function h, which denotes the optimal risk minimizer for downstream task with labeling function\ny(·), for a given representation function. Also, recall h∗\nU = arg minh∈H ming∈G LU(g ◦h) denotes the optimal\npre-training representation function.\nBy standard risk decomposition we have:\nYuyang Deng, Junyuan Hong, Jiayu Zhou, Mehrdad Mahdavi\nET ( ˆf, ˆh) = RT ( ˆf ◦ˆh) −\nmin\nf∈F,h∈H RT (f ◦h)\n= RT ( ˆf ◦ˆh) −min\nf∈F RT (f ◦ˆh) + min\nf∈F RT (f ◦ˆh) −\nmin\nf∈F,h∈H RT (f ◦h)\n= RT ( ˆf ◦ˆh) −min\nf∈F RT (f ◦ˆh)\n|\n{z\n}\nI\n+ min\nf∈F RUX (f ◦ˆh) −min\nf∈F RUX (f ◦h∗\nU)\n|\n{z\n}\nII\n+\n\u0012\nmin\nf∈F RT (f ◦ˆh) −min\nf∈F RUX (f ◦ˆh)\n\u0013\n|\n{z\n}\nIII\n−\n\u0012\nmin\nf∈F,h∈H RT (f ◦h) −min\nf∈F RUX (f ◦h∗\nU)\n\u0013\n|\n{z\n}\nIV\nWe now turn to bounding each term in RHS of above inequality.\nBounding I. The term I can be bounded by following standard results in uniform convergence and noting the\nfact that ˆf is empirical risk minimizer of downstream task by fixing the pre-training representation function ˆh:\nI = RT ( ˆf ◦ˆh) −min\nf∈F RT (f ◦ˆh)\n= RT ( ˆf ◦ˆh) −R b\nT ( ˆf ◦ˆh) + R b\nT ( ˆf ◦ˆh) −R b\nT (f ∗\nT (ˆh) ◦ˆh)\n|\n{z\n}\n≤0\n+R b\nT (f ∗\nT (ˆh) ◦ˆh) −min\nf∈F RT (f ◦ˆh)\n≤4R b\nT (ϕ ◦F ◦ˆh) + 4Bϕ\nr\nlog(1/ν)\nn\n.\nBounding III. To bound III, we define f ∗\nU(h) = arg minf∈F RUX (f ◦ˆh), where RUX (f ◦h) := Ex∼UX [ϕ(f ◦\nh(x), y(x))] denotes the risk realized by pre-training marginal data distribution UX and downstream labeling\nfunction y(·) (Definition 2). We have:\nIII = min\nf∈F RT (f ◦ˆh) −min\nf∈F RUX (f ◦ˆh) ≤RT (f ∗\nU(ˆh) ◦ˆh) −RUX (f ∗\nU(ˆh) ◦ˆh)\n= Ex∼T [ϕ(f ∗\nU(ˆh) ◦ˆh(x), y)] −Ex∼U[ϕ(f ∗\nU(ˆh) ◦ˆh(x), y)]\n=\nX\nx∈X\n|T (x) −UX (x)| · ϕ(f ∗\nU(ˆh) ◦ˆh(x), y)\n≤Bϕ\nX\nx∈X\n|T (x) −UX (x)|\n≤2Bϕ ∥T −UX ∥TV .\nwhere the last step follows from Proposition 1.\nBounding IV. For IV, recalling that f ∗\nT (h) = arg minf∈F RT (f ◦h), and we have\nIV = min\nf∈F RUX (f ◦h∗\nU) −\nmin\nf∈F,h∈H RT (f ◦h)\n= min\nf∈F RUX (f ◦h∗\nU) −min\nf∈F RT (f ◦h∗\nU) + min\nf∈F RT (f ◦h∗\nU) −\nmin\nf∈F,h∈H RT (f ◦h)\n≤RUX (f ∗\nT (h∗\nU) ◦h∗\nU) −RT (f ∗\nT (h∗\nU) ◦h∗\nU) + min\nf∈F RT (f ◦h∗\nU) −\nmin\nf∈F,h∈H RT (f ◦h)\n≤2Bϕ ∥T −UX ∥TV + min\nf∈F ET (f, h∗\nU)\nOn the Generalization Ability of Unsupervised Pretraining\nwhere at last step we use the same reasoning we used in bounding III, and the definition of ET (·).\nBounding II. It remains to bound II. Under the representation transferability assumption, we know\nII = min\nf∈F RUX (f ◦ˆh) −min\nf∈F RUX (f ◦h∗\nU)\n≤Cβ\n\u0012\nmin\ng∈G LU(g ◦ˆh) −min\ng∈G LU(g ◦h∗\nU)\n\u0013β\n≤Cβ\n\u0012\nLU(ˆg ◦ˆh) −min\ng∈G LU(g ◦h∗\nU)\n\u0013β\n= Cβ\n\u0012\nLU(ˆg ◦ˆh) −\nmin\ng∈G,h∈H LU(g ◦h)\n\u0013β\n= Cβ\n\u0010\nEU(ˆg, ˆh)\n\u0011β\n.\nwhere the last step follows from the definition of EU(·).\nPutting pieces I-IV together yields:\nET ( ˆf, ˆh) ≤Cβ\n\u0010\nEU(ˆg, ˆh)\n\u0011β\n+ 4GϕR b\nT (F ◦ˆh) + 4Bϕ\nr\nlog(1/ν)\nn\n+ 4Bϕ ∥T −UX ∥TV + min\nf∈F ET (f, h∗\nU),\nthus leading to the desired generalization bound stated in Theorem 1.\nAs mentioned earlier, to instantiate Theorem 1 to a particular application, we need to establish bounds on\nrepresentation transferrability, generalization of pre-training task, and representation-induced Rademacher\ncomplexity as we demonstrate on two specific pre-training tasks. We note that similar notions to representation\ntransferrability were proposed in [Tripuraneni et al., 2020, Ge et al., 2023, Du et al., 2020, Zhang et al., 2023], but\nthey do not have exponent in definition, so cannot capture the transferrability when pre-training and downstream\ntask losses are not homogeneous. The term minf∈F ET (f, h∗\nU) characterizes how well the optimal pre-training task\nencoder is when applied on downstream task. It will depend on specific pre-training and downstream distribution.\nSince we do not make distributional assumption, analyzing this term is beyond the scope of this paper.\nC\nProof of Generalization for Pre-training with Context Encoder\nIn this section we prove the results on generalization of pre-training with Context Encoder (CE) and fine-tuning\non binary classification as downstream task provided in Subsection 5.1. Recall during pre-training, we draw a\nset of unlabeled data,e.g., images {z1, ..., zN}, and corrupt these data to make {˜z1, ..., ˜zN}, then a deep neural\nnetwork is trained by reconstructing the corrupted pixel of a given image. The encoder-decoder architecture is\ndefined as follows:\nencoder:\nh(x) = σ (WL · · · σ (W1x)) ,\ndecoder:\ng(h(x)) = WL+1h(x).\nwhere W1 ∈Rm×d, W2, ..., WL ∈Rm×m, and WL+1 ∈Rd×m (for simplicity we assume the hiddent layers\nshare the same dimension m). We assume each layer’s weight is with bounded norm: ∥Wl∥≤W(l), ∥Wl∥2,1 ≤\nB(l), ∀l ∈[L + 1]. The hypothesis class for encoder is then defined as:\nH :=\nn\nx 7→σ (WL · · · σ (W1x)) : ∥Wl∥≤W(l), ∥Wl∥2,1 ≤B(l), ∀l ∈[L]\no\nand decoder class is defined as:\nG :=\nn\nx 7→WL+1x : ∥WL+1∥≤W(L + 1), ∥WL+1∥2,1 ≤B(L + 1)\no\n.\nYuyang Deng, Junyuan Hong, Jiayu Zhou, Mehrdad Mahdavi\nIn pre-training stage we optimize the following empirical unsupervised losses:\nmin\ng∈G,h∈H L b\nU(g ◦h) := 1\n2\nN\nX\ni=1\n∥g(h(ezi)) −zi∥2 ,\n(10)\nwhere ˜zi = T1(zi), and T1 : X 7→X is some random transformation, e.g, rotating, scaling, adding Gaussian noise\nor masking pixels.\nAfter pre-training, we discard the top layer of the network, and use the rest layers as an encoder. A linear\nprojection head is added on top of encoder in downstream training:\ndownstream model:\nf(ˆh(x)) = θ⊤ˆh(x),\nwith ∥θ∥≤R, and we assume that only the linear head is trainable during fine-tune stage. We optimize a binary\nclassification task with Lipschitz loss function as fine-tuning task:\nmin\n∥θ∥≤R R b\nT (θ ◦ˆh) = 1\nn\nXn\ni=1 ϕ(θ⊤h(xi), yi),\nto get ˆf, where yi ∈{−1, +1} is binary labeling function for downstream task.\nRoadmap. We will provide proof of Theorem 2 in the following subsections.\nThe roadmap is as follows:\nin Appendix C.1 we first show that the CE pre-training admits bounded representation transferrability to\ndownstream task (the proof of Lemma 1), and then in Appendix C.2 we prove the generalization of CE pre-\ntraining task (Lemma 5), and finally in Appendix C.3 we conclude the proof for Theorem 2 by showing that the\nrepresentation-induced Rademacher complexity is bounded.\nC.1\nProof of Transferability\nIn this subsection we provide the proof of Lemma 1. For notational convenience we define the following quantities:\n∆ft\nU (ˆh, h∗\nU) = min\n∥θ∥≤R E(˜z,z)∼U[ϕ(θ⊤ˆh(˜z))] −min\n∥eθ∥≤R\nE˜z∼UX [ϕ(˜θ⊤h∗\nU(˜z))],\n∆pt\nU (ˆh, h∗\nU) = min\nWL+1 E(˜z,z)∼U\n\r\r\rWL+1ˆh(˜z) −z\n\r\r\r\n2\n−min\nf\nWL+1\nE˜z∼UX\n\r\r\rf\nWL+1h∗\nU(˜z) −z\n\r\r\r\n2\nTo prove Lemma 1, we are going to show ∆ft\nU (ˆh, h∗\nU) ≤Cβ\n\u0010\n∆pt\nU (ˆh, h∗\nU)\n\u0011β\nholds for some Cβ, β.\nUpper bounding ∆ft\nU (ˆh, h∗\nU):\nWe examine ∆ft\nU (ˆh, h∗\nU) first. We define the optimal head for classification task\non distribution UX under represetation h∗\nU as ˜θ∗= arg min∥˜θ∥≤R E˜z∼UX [ϕ(˜θ⊤h∗\nU(˜z))].\n∆ft\nU (ˆh, h∗) = min\n∥θ∥≤R Ex∼U[ϕ(θ⊤ˆh(˜z))] −Ex∼U[ϕ(˜θ∗⊤h∗\nU(˜z))]\n≤min\n∥θ∥≤R E(˜z,z)∼U\n\f\f\fθ⊤ˆh(˜z) −˜θ∗⊤h∗\nU(˜z)\n\f\f\f\n≤min\n∥θ∥≤R\nr\nE(˜z,z)∼U\n\u0010\nθ⊤ˆh(˜z) −˜θ∗⊤h∗\nU(˜z)\n\u00112\n= min\n∥θ∥≤R\nr\nθ⊤E\nh\nˆh(˜z)ˆh⊤(˜z)\ni\nθ −2θ⊤E\nh\nˆh(˜z)h∗\nU\n⊤(˜z)\ni\n˜θ∗+ ˜θ∗⊤E\nh\nh∗\nU(˜z)h∗\nU\n⊤(˜z)\ni\n˜θ∗\nSince\np\nf(x) and f(x) attain the minimum at the same point, we examine the minimum of θ⊤E\nh\nˆh(˜z)ˆh⊤(˜z)\ni\nθ −\n2θ⊤E\nh\nˆh(˜z)h∗⊤\nU (˜z)\ni\n˜θ∗+ ˜θ∗⊤E\nh\nh∗\nU(˜z)hU\n∗⊤(˜z)\ni\n˜θ∗over θ. Under unconstrained setting, the minimum of above\nstatement is ˜θ∗⊤Λ˜θ∗\nOn the Generalization Ability of Unsupervised Pretraining\nwhen θ =\n\u0010\nE\nh\nˆh(˜z)ˆh⊤(˜z)\ni\u0011†\nE\nh\nˆh(˜z)h∗\nU\n⊤(˜z)\ni\n˜θ∗, and\nΛ = E\nh\nˆh(˜z)ˆh⊤(˜z)\ni\n−E\nh\nh∗\nU(˜z)ˆh⊤(˜z)\ni \u0010\nE\nh\nh∗\nU(˜z)h∗\nU\n⊤(˜z)\ni\u0011†\nE\nh\nˆh(˜z)h∗\nU\n⊤(˜z)\ni\n.\nHence we have\n∆ft\nU (ˆh, h∗\nU) ≤\np\n˜θ∗⊤Λ˜θ∗=\nq\ntr(Λ˜θ∗⊤˜θ∗) ≤\nq\ndσmax(Λ)σmax(˜θ∗⊤˜θ∗),\n(11)\nwhere we applied Ruhe’s Trace Inequalities at last step (Proposition 2):\ntr(AB) ≤Pd\ni=1 σi(A)σi(B) ≤\ndσmax(A)σmax(B).\nFinally, we choose large enough R so that we can attain the optimum.\nLower bounding ∆pt\nU (ˆh, h∗)\nNow we switch to lower bounding ∆pt\nU (ˆh, h∗). We have:\n∆pt\nU (ˆh, h∗\nU) =\nmin\nWL+1:∥W∥≤W (L+1) E(˜z,z)∼U\n\r\r\rWL+1ˆh(˜z) −z\n\r\r\r\n2\n−E(˜z,z)∼U\n\r\rW∗\nL+1h∗\nU(˜z) −z\n\r\r2\n=\nmin\nWL+1:∥W∥≤W (L+1) E(˜z,z)∼U\n\r\r\rWL+1ˆh(˜z) −W∗\nL+1h∗\nU(˜z)\n\r\r\r\n2\nwhere the last step is due to our realizability Assumption 1, the optimal encoder-decoder exists in the hypothesis\nclass which can perfectly recover masked data.\nHence\n∆pt\nU (ˆh, h∗\nU) =\nmin\nWL+1∈Rd×m E(˜z,z)∼U\n\r\r\rWL+1ˆh(˜z) −W∗\nL+1h∗(˜z)\n\r\r\r\n2\n=\nmin\nwr∈Rm,r∈[d] E(˜z,z)∼U\nd\nX\nr=1\n\r\r\rw⊤\nr ˆh(˜z) −w∗\nr\n⊤h∗(˜z)\n\r\r\r\n2\n≥\nd\nX\nr=1\nmin\nwr∈Rm E(˜z,z)∼U\n\r\r\rw⊤\nr ˆh(˜z) −w∗\nr\n⊤h∗(˜z)\n\r\r\r\n2\n≥\nd\nX\nr=1\nmin\nwr∈Rm E(˜z,z)∼U\n\u0010\nw⊤\nr ˆh(˜z)ˆh(˜z)\n⊤wr −2w⊤\nr ˆh(˜z)h∗\nU(˜z)⊤w∗\nr + w∗\nr\n⊤h∗\nU(˜z)h∗\nU(˜z)⊤w∗\nr\n\u0011\n.\nAccording to similar reasoning in the proof of upper bound, with Λ defined in the same way as (11), we have\n∆pt\nU (ˆh, h∗\nU) ≥\nd\nX\nr=1\nw∗\nr\n⊤Λw∗\nr\n= tr\n \nΛ\nm\nX\nr=1\nw∗\nrw∗\nr\n⊤\n!\n≥σmax(Λ)σmin\n d\nX\nr=1\nw∗\nrw∗\nr\n⊤\n!\nwhere at last step we apply Ruhe’s trace inequality (Proposition 2)): tr (AB) ≥σmax(A)σmin(B). Therefore, we\ncan conclude that\n∆ft\nU (ˆh, h∗\nU)\n\u0010\n∆pt\nU (ˆh, h∗\nU)\n\u00111/2 ≤O\n\n\n\n\nq\ndσmax(˜θ∗˜θ∗⊤)\nr\nσmin\n\u0010Pd\nr=1 w∗rw∗r\n⊤\u0011\n\n\n\n,\nYuyang Deng, Junyuan Hong, Jiayu Zhou, Mehrdad Mahdavi\nwhich indicates that Context Encoder pretraining admits an\n \nΩ\n \n√\ndσmax( ˜θ∗˜θ∗⊤)\nq\nσmin(\nPd\nr=1 w∗rw∗r ⊤)\n!\n, 1\n2\n!\nrepresentation\ntransferrability to binary classification task. In the main paper Lemma 1 we omit the constant dependency for\nease of exposition.\nC.2\nProof of generalization of CE pretraining task\nIn this section we are going to derive generalization bound of the CE pre-training. The generalization is given in\nthe following lemma:\nLemma 5 (Generalization of pre-training task). Let ˆg, ˆh be the solution of (4), and ˜Z = [˜z1; . . . ; ˜zN] is the\nconcatenated pre-training data. Then with probability at least 0.99 the following statement holds:\nEU(ˆg, ˆh) ≤O\n\n\n\n\n\u0012\r\r\r˜Z\n\r\r\r\n2\nln(2m2)\n\u0013 \u0010QL+1\nl=1 W 2(l)\n\u0011 \u0010PL+1\nl=1 ( B(l)\nW (l))\n2\n3\n\u00113\nN\n\n\n\n.\nTo prove Lemma 5, we first introduce the following worst case covering number quantity:\nDefinition 4 (L2 covering number). Given a hypothesis class H and a set of data S = {x1, ..., xN}, let\nh(X) = [h(x1); ...; h(xN)] denote the concatenated output of N points.The the covering number N(H(S), ϵ, ∥·∥) is\nthe least cardinality of set C, such that for every h ∈H, there exists a hϵ ∈C, and ensures that\n∥h(X) −hϵ(X)∥≤ϵ.\nDefinition 5 (L∞covering number). Given a hypothesis class H and a set of data S = {x1, ..., xN}, the worst\ncase covering number N∞(H(S), ϵ, ∥·∥) is the least cardinality of set C, such that for every h ∈H, there exists a\nhϵ ∈C, and ensures that\nmax\ni∈[N] ∥h(xi) −hϵ(xi)∥≤ϵ.\nThe following result will relate the Rademacher complexity of the local loss class induced by a hypothesis class H,\nto the L∞covering number of H.\nTheorem 6 ([Srebro et al., 2010, Theorem 1]). Given a non-negative H-smooth loss ℓbounded by b and a set of\ndata pairs bS = {(xi, yi)}N\ni=1, Define a local loss class L(r) =\n\b\n(x, y) 7→ℓ(h(x), y) : h ∈H, L b\nS(h) ≤r\n\t\nfor some\n0 ≤r < ∞. Then , for all f ∈F simultaneously\nR b\nS (L(r)) ≤inf\nα\n\nα\n√\nN\n+\nZ √\nbr\nα\ns\nln N∞(H,\nϵ\n√\n12Hr, ∥·∥)\nN\ndϵ\n\n\nwhere the empirical Rademacher complexity of loss class is defined as\nR b\nS (L(r)) = Eε\n\"\nsup\nh∈H,L b\nS(h)≤r\n\f\f\f\f\f\n1\nn\nn\nX\ni=1\nεiℓ(h(xi), yi)\n\f\f\f\f\f\n#\n.\n(ε1, . . . , εn\niid\n∼unif{±1})\nThe above theorem relates the complexity of loss class to the worst case spectral covering number of function\nclass, in our case, vector valued neural networks. Hence, it remains to find worst case (L∞) covering number of\nour encoder class\nG ◦H := {x 7→WL+1σ (WL · · · σ(W1x)) : ∥Wl∥≤W(l), ∥Wl∥2,1 ≤B(l) ∀l ∈[L + 1]} .\n(12)\nLemma 6 (Implication of [Bartlett et al., 2017, Theorem 3.3]). Given a set of data pairs bS = {˜zi}N\ni=1, and\nhypothesis class defined in (12), then the following statement holds:\nln N∞(G ◦H(S), ϵ, ∥·∥) ≤ln N(G ◦H(S), ϵ, ∥·∥) ≤\n\n\n\n\r\r\r˜Z\n\r\r\r\n2\nln(2m2)\nϵ2\n\n\n\n L+1\nY\nl=1\nW 2(l)\n!  L+1\nX\nl=1\n( B(l)\nW(l))\n2\n3\n!3\n.\nwhere ˜Z = [˜z1; ...; ˜zN].\nOn the Generalization Ability of Unsupervised Pretraining\nProof. We define g ◦h(X) = [g(h(x1)); ...; g(h(xN))] ∈RN×d. Notice the fact that 2-norm of a row of a matrix, is\nalways less than the spectral norm of the matrix:\nmax\ni∈[N] ∥g ◦h(xi) −g′ ◦h′(xi)∥≤max\n∥a∥≤1\n\r\r(g ◦h(X) −g′ ◦h′(X))⊤a\n\r\r = ∥g ◦h(X) −g′ ◦h′(X)∥,\nhence we can have the following fact for covering numbers:\nln N∞(G ◦H(S), ϵ, ∥·∥) ≤ln N(G ◦H(S), ϵ, ∥·∥).\n(13)\nAt last plugging the bound for ln N(G ◦H(S), ϵ, ∥·∥) from [Bartlett et al., 2017] concludes the proof.\nEquipped with above results, we are ready to show the local Rademacher complexity of loss class induced by\nencoder-decoder function class G ◦H:\nLemma 7. Given a hypothesis class H, if the logarithm of its L∞covering number ln N∞(G ◦H(S), ϵ, ∥·∥) is\nbounded by\nc\nϵ2 , then the following bound for local Rademcaher complexity holds true:\nR b\nS (L(r)) ≤10\nr\ncHr\nN\n+ 10\nr\ncHr\nN\n \nln\n√\nbr −ln\n \n5\n2\nr\ncHr\nN\n!!\n.\nProof. According to Theorem 6 we have\nR b\nS (L(r)) ≤4α + 10\nZ √\nbr\nα\ns\nln N∞(H,\nϵ\n√\n12Hr, N)\nN\ndϵ\n≤4α + 10\nZ √\nbr\nα\nr\ncHr\nNϵ2 dϵ\n≤4α + 10\nr\ncHr\nN (ln\n√\nbr −ln(α)).\nChoosing α = 5\n2\nq\nB2cHr\nN\n=\n5\n2\n√\nN\n√\ncHr will minimize above bound, and yields:\nR b\nS (L(r)) ≤10\nr\ncHr\nN\n+ 10\nr\ncHr\nN\n \nln\n√\nbr −ln\n \n5\n2\nr\nHr · c\nN\n!!\n.\nThe following theorem connects local Rademacher complexity to population risk.\nTheorem 7. [Bousquet, 2002, Theorem 6.1] Given a loss class L(r), let ϕ(r) be the function such that\nR b\nS (L(r)) ≤ϕ(r).\nthen with probability at least 1 −exp(−ν),\nLS(h) ≤L b\nS(h) + 45r∗+\np\nLS(h)\n \np\n8r∗n +\nr\n4b(log(1/ν) + 6 log log N)\nN\n!\n+ 20b(ν + 6 log log N)\nN\nwhere r∗is the largest solution such that ϕ(r) = r.\nC.2.1\nProof of Lemma 5\nProof. First we evoke Lemma 7 with c = 12\n\r\r\r˜Z\n\r\r\r\n2\nln(2m2)\n\u0010QL+1\nl=1 W 2(l)\n\u0011 \u0010PL+1\nl=1 ( B(l)\nW (l))\n2\n3\n\u00113\nR b\nS (L(r)) ≤10\nr\nHr · c\nN\n+ 10\nr\ncHr\nN\n \nln\n√\nbr −ln\n \n5\n2\nr\nHr · c\nN\n!!\n= 10\nr\nHr · c\nN\n+ 10\nr\ncHr\nN\nln\n \n2\n5\nr\nbN\nHc\n!\nYuyang Deng, Junyuan Hong, Jiayu Zhou, Mehrdad Mahdavi\nWe set ϕ(r) = 10\nq\nHr·c\nN\n· max\n\u001a\n1, ln\n\u0012\n2\n5\nq\nbN\nHc\n\u0013\u001b\n. Solving the follwoing equation to get r∗\nϕ(r) = 10\nr\nHr · c\nN\n· max\n(\n1, ln\n \n2\n5\nr\nbN\nHc\n!)\n= r,\n⇐⇒r∗= 100H · c\nN\n· max\n(\n1, ln\n \n2\n5\nr\nbN\nHc\n!)2\nNow, according to Theorem 7, and the fact that\nA ≤B + C\n√\nA =⇒A ≤B + C2 +\n√\nBC,\nwe have\nLU(g ◦h) ≤L b\nU(g ◦h) + 45r∗+\n \n√\n8r∗+\nr\n4b(log(1/ν) + 6 log log N)\nN\n!2\n+ 20b(ν + 6 log log N)\nN\n+\nr\nL b\nU(g ◦h) + 45r∗+ 20b(ν + 6 log log N)\nN\n \n√\n8r∗+\nr\n4b(log(1/ν) + 6 log log N)\nN\n!\n.\nPlugging r∗, and empirical risk minimizers ˆg, ˆh will conclude the proof.\nC.3\nProof of Theorem 2\nProof. Recall that in Theorem 1, the generalization bound is given by\nET ( ˆf, ˆh) ≤Cβ\n\u0010\nEU(ˆg, ˆh) + µ\n\u0011β\n+ 4GϕR b\nT (F ◦ˆh) + 4Bϕ\nr\nlog(1/ν)\nn\n+ 4Bϕ ∥T −UX ∥TV + min\nf∈F ET (f, h∗\nU).\nSince in the previous subsection we prove the bounded transferrability and generalization of pre-training task, it\nremains to show the upper bound of representation-induced Rademacher complexity. To this end, we have\nR b\nT (ϕ ◦F ◦ˆh) = Eε∈{±1}n\n\"\nsup\nθ:∥θ∥≤R\n1\nn\nXn\ni=1 εiϕ(θ⊤ˆh(xi), yi)\n#\n≤RGϕEε∈{±1}n\n\"\nsup\nθ:∥θ∥≤R\n1\nn\nXn\ni=1 εiθ⊤ˆh(xi)\n#\n= RGϕ\nn\nEε\n\r\r\r\nXn\ni=1 εiˆh(xi)\n\r\r\r\n≤RGϕ\nn\nr\nEε\n\r\r\r\nXn\ni=1 εiˆh(xi)\n\r\r\r\n2\n= RGϕ\nn\nrXn\ni=1\n\r\r\rˆh(xi)\n\r\r\r\n2\nwhere at first inequality we apply Ledoux-Talagrand’s inequality to peel off Lipschitz loss ϕ(·), and at last\ninequality we use the fact that εi are i.i.d. with zero mean, so that the cross terms disappear. For each\n\r\r\rˆh(xi)\n\r\r\r\n2\n,\nwe have:\n\r\r\rˆh(xi)\n\r\r\r\n2\n≤\nL+1\nY\nl=1\nW 2(l) ∥xi∥2 ,\nhence we arrive at\nR b\nT (ϕ ◦F ◦ˆh) ≤\nRGϕ\nqQL+1\nl=1 W 2(l) Pn\ni=1 ∥xi∥2\nn\n.\nPlugging Lemmas 1 and 5 back into Theorem 1 as well as above bound will complete the proof of Theorem 2.\nOn the Generalization Ability of Unsupervised Pretraining\nD\nProof of Pre-training with Masked Autoencoder with Tranformer Models\nWe turn to proving the generalization of pretraining with masked autoencoder (MAE) with tranformer models\n(Section 5.2).\nRecall, in MAE pre-training for vision tasks as an example, we draw a large set of images Z1, ..., ZN ∈RK×d,\nand then randomly mask some patches of each image to get ˜Z1, ..., ˜ZN ∈RK×d. Then an encoder-decoder model\nis trained by recovering the missing patches (e.g., by utilizing MSE loss ℓ(ˆZ, Z) =\n\r\r\rˆZ −Z\n\r\r\r\n2\nF as pre-training loss).\nWe will consider L-layer transformer as the pre-train encoder model, a single self-attention layer transformer as\nthe pre-train decoder model, and a linear projection layer for binary classification as fine-tune model.\nEncoder Architecture\nIn a L-layer transformer, given a input X, the lth layer’s output is define as:\nXl =\n(\nX,\nl = 0\nSAWl(Xl−1),\nl = [L],\nwhere SAWl(·) is the l-layer self attention module given a collection of weight matrices Wl\n=\n\u0000Wl\nV , Wl\nK, Wl\nQ, Wl\nFC1, Wl\nFC2\n\u0001\n∈Rd×d × Rd×dK × Rd×dK × Rd×m × Rm×d defined as:\nSAWl(Xl−1) = α2σ\n\u0000ZlWl\nFC1\n\u0001\nWl\nFC2 + Zl,\nZl =\n\u0000α1Al + Xl−1\u0001\n,\nAl = softmax\n\u0012\n1\n√dK\nXWl\nK(XWl\nQ)⊤\n\u0013\nXWl\nV ,\nwhere α1, α2 are some small constant, as used in practice [Noci et al., 2022]. We use the Lth layer’s output as the\nfinal output of encoder, i.e., h(X) = XL.\nencoder:\nh(X) = XL.\nThe hypothesis class of encoder is defined as:\nH =\n\n\n\n\n\n\n\nX 7→SAWL (SAWL−1...SAW1(X)) :\n\r\rWl\nFC1\n\r\r ,\n\r\rWl\nFC2\n\r\r ,\n\r\rWl\nK\n\r\r ,\n\r\rWl\nQ\n\r\r ,\n\r\rWl\nV\n\r\r ≤W(l),\n\r\rWl\nFC1\n\r\r\n2,1 ,\n\r\rWl\nFC2\n\r\r\n2,1 ,\n\r\rWl\nK\n\r\r\n2,1 ,\n\r\rWl\nQ\n\r\r\n2,1 ,\n\r\rWl\nV\n\r\r\n2,1 ≤B(l), ∀l ∈[L]\n\n\n\n\n\n\n\n.\n(14)\nDecoder Architecture\nWhen encoder finished processing masked sequence, we will send the encoder output\nh(˜Z) to decoder. The decoder is a simple linear projection layer:\ndecoder:\ng(h(˜Z)) = h(˜Z)WD,\nTo learn the representation model, we solve the following1:\nmin\ng∈G,h∈H L b\nU(g ◦h) := 1\n2\nN\nX\ni=1\n\r\r\rg(h(eZi)) −Zi\n\r\r\r\n2\nF ,\n(15)\nto get representation ˆh.\nThen, in the fine-tuning stage for a binary classification tasks with labels yi ∈{−1, +1}, we consider a linear\nmodel parameterized by θ\ndownstream model:\nf(h(X)) = 1⊤ˆh(Xi)θ,\n1In some implementation of MAE pre-training, the MSE loss is not computed on full patches, but only the masked\npatches. It can be adapted by changing our objective to 1\n2\nPN\ni=1\n\r\r\rA ⊙(g(h(eZi)) −Zi)\n\r\r\r\n2\nF where A ∈RK×d is the indicator\nmatrix with j row to be 1 if jth patch is masked, otherwise 0. This adaptation will not affect our analysis significantly.\nYuyang Deng, Junyuan Hong, Jiayu Zhou, Mehrdad Mahdavi\nwith classification loss ϕ(·, ·) and optimize:\nmin\n∥θ∥2≤R R b\nT (θ ◦ˆh(X)) = 1\nn\nn\nX\ni=1\nϕ(1⊤ˆh(Xi)θ, yi),\nto get ˆf (or ˆθ in this setting), the aggregated patch over all patches is used for linear projection in classification\ntask.\nRoadmap. We will provide proof of Theorem 3 in the following subsections. The roadmap is that in Appendix D.1\nwe first show the MAE pre-training admits bounded representation transferrability to downstream task (Lemma 2),\nand then in Appendix D.2 we prove the generalization of MAE pre-training task (Lemma 3). The heart of the\nproof in this part is to derive worst case covering number of transformer class. Finally in Appendix D.3 we\nconclude the proof for Theorem 3 by showing that the representation-induced Rademacher complexity is bounded.\nD.1\nProof of Task Transferability of MAE\nSimilar to proof of DAE transferability, we define the following quantity:\n∆ft\nU (ˆh, h∗\nU) = min\n∥θ∥≤R E(˜Z,Z)∼U[ϕ(θ⊤(1⊤ˆh(˜Z))⊤)] −min\n∥eθ∥≤R\nE(˜Z,Z)∼U[ϕ(˜θ⊤(1⊤h∗\nU(˜Z))⊤)],\n∆pt\nU (ˆh, h∗\nU) = min\nWD∈R E(˜Z,Z)∼U\n\r\r\rˆh(˜Z)WD −Z\n\r\r\r\n2\nF −E(˜Z,Z)∼U\n\r\r\rh∗\nU(˜Z)WD∗−Z\n\r\r\r\n2\nF\nwhere 1 = [1, 1, 1, ...] ∈RK.\nUpper bounding ∆ft\nU (ˆh, h∗\nU)\nWe examine ∆ft\nU (ˆh, h∗\nU) first. Similar to DAE proof, We define the optimal head\nfor classification task on distribution UX under representation h∗\nU as ˜θ∗= arg min∥˜θ∥≤R E˜Z∼UX [ϕ(˜θ⊤(1⊤h∗\nU(˜Z)))].\n∆ft\nU (ˆh, h∗\nU) = min\n∥θ∥≤R E(˜Z,Z)∼U[ϕ(θ⊤(1⊤ˆh(˜Z))⊤)] −E(˜Z,Z)∼U[ϕ(˜θ⋆⊤(1⊤h∗\nU(˜Z))⊤)]\n≤min\n∥θ∥≤R GϕE(˜Z,Z)∼U|(θ⊤(1⊤ˆh(˜Z))⊤) −(˜θ⋆⊤(1⊤h∗\nU(˜Z))⊤)|\n≤min\n∥θ∥≤R Gϕ\nr\nE(˜Z,Z)∼U\n\u0010\nθ⊤(1⊤ˆh(˜Z))⊤−˜θ⋆⊤(1⊤h∗\nU(˜Z))⊤\n\u00112\n= min\n∥θ∥≤R\nGϕ\nr\nθ⊤E\nh\n(1⊤ˆh(˜Z))⊤(1⊤ˆh(˜Z))\ni\nθ −2θ⊤E\nh\n(1⊤ˆh(˜Z))⊤(1⊤h∗\nU(˜Z))\ni\n˜θ⊤+ ˜θ⋆⊤E\nh\n(1⊤h∗\nU(˜Z))⊤(1⊤ˆh∗\nU(˜Z))\ni\n˜θ\nSince\np\nf(x) and f(x) attain the minimum at the same point, we examine the minimum of above state-\nment over θ without square root.\nUnder unconstrained setting, the minimum of above statement is\n˜θ⋆⊤Λ\n\u0010\n1⊤ˆh(˜Z), 1⊤h∗\nU(˜Z)\n\u0011\n˜θ∗when θ∗=\n\u0010\nE\nh\n(1⊤ˆh(X))⊤(1⊤ˆh(X))\ni\u0011†\nE\nh\n(1⊤ˆh(X))⊤(1⊤h∗\nU(X))\ni\n˜θ∗. Hence we\nhave\n∆ft\nU (ˆh, h∗\nU) ≤\nr\n˜θ⋆⊤Λ\n\u0010\n1⊤ˆh(˜Z), 1⊤h∗\nU(˜Z)\n\u0011\n˜θ∗=\nr\ntr(Λ\n\u0010\n1⊤ˆh(˜Z), 1⊤h∗\nU(˜Z)\n\u0011\n˜θ∗˜θ⋆⊤)\n≤\nr\ndσmax(Λ\n\u0010\n1⊤ˆh(˜Z), 1⊤h∗\nU(˜Z)\n\u0011\n)σmax(˜θ∗˜θ⋆⊤),\nwhere\nΛ\n\u0010\n1⊤ˆh(˜Z), 1⊤h∗\nU(˜Z)\n\u0011\n= E\nh\n(1⊤h∗\nU(˜Z))⊤(1⊤h∗\nU(˜Z))\ni\n−E\nh\n(1⊤ˆh(˜Z))⊤(1⊤h∗\nU(˜Z))\ni \u0010\nE\nh\n(1⊤ˆh(˜Z))⊤(1⊤ˆh(˜Z))\ni\u0011†\nE\nh\n(1⊤ˆh(˜Z))⊤(1⊤h∗\nU(˜Z))\ni\n.\nAt last, by choosing a properly large R, we can guarantee the optimum can be attained.\nOn the Generalization Ability of Unsupervised Pretraining\nLower bounding ∆pt\nU (ˆh, h∗\nU)\nSimilar to CE proof, we have:\n∆pt\nU (ˆh, h∗\nU)\n=\nmin\nWD∈Rd×d E(˜Z,Z)∼U\n\r\r\rˆh(eZ)WD −Z\n\r\r\r\n2\nF −E(˜Z,Z)∼U\n\r\r\rh∗\nU(eZ)WD∗−Z\n\r\r\r\n2\nF\n=\nmin\nWD∈Rd×d E(˜Z,Z)∼U\n\r\r\rˆh(eZ)WD −h∗\nU(eZ)WD∗\r\r\r\n2\nF\n≥\nd\nX\ni=1\nmin\nwr∈Rd E(˜Z,Z)∼U\n\r\r\rˆh(eZ)wr −h∗\nU(eZ)w∗\nr\n\r\r\r\n2\n=\nd\nX\ni=1\nmin\nwr∈Rd E(˜Z,Z)∼U\n\u0010\nw⊤\nr ˆh(eZ)⊤ˆh(eZ)wr −2w⊤\nr ˆh(eZ)⊤h∗\nU(eZ)w∗\nr + w∗\nr\n⊤h∗\nU(eZ)⊤h∗\nU(eZ)w∗\nr\n\u0011\nwhere the second step is due to our realizability Assumption 1, wr ∈Rd represents rth colum of WD and so is\nw∗\nr ∈Rd represents rth colum of WD∗.\nSimilar to Context Encoder proof, we define Schur complement as:\nΛ\n\u0010\nˆh(˜Z), h∗\nU(˜Z)\n\u0011\n= E\nh\n(h∗\nU(˜Z))⊤h∗\nU(˜Z)\ni\n−E\nh\n(h∗\nU(˜Z))⊤(ˆh(˜Z))\ni \u0010\nE\nh\n(h∗\nU(˜Z))⊤h∗\nU\n⊤(˜Z)\ni\u0011†\nE\nh\n(ˆh(˜Z))⊤h∗\nU(˜Z)\ni\n.\nBy computing the closed form solution of quadratic form we arrived at:\n∆pt\nU (ˆh, h∗\nU) ≥\nd\nX\nr=1\nw∗⊤\nr Λ\n\u0010\nˆh(˜Z), h∗(˜Z)\n\u0011\nw∗\nr\n≥tr\n\nΛ\n\u0010\nˆh(˜Z), h∗(˜Z)\n\u0011\nd\nX\nj=1\nw∗\njw∗⊤\nj\n\n\n≥σmax\n\u0010\nΛ\n\u0010\nˆh(˜Z), h∗(˜Z)\n\u0011\u0011\nσmin\n\n\nd\nX\nj=1\nw∗\njw∗⊤\nj\n\n.\nRecall that\n∆ft\nU (ˆh, h∗\nU) ≤\nr\ndσmax\n\u0010\nΛ\n\u0010\n1⊤ˆh(˜Z), 1⊤h∗\nU(˜Z)\n\u0011\u0011\nσmax(˜θ∗˜θ⋆⊤).\nHence, we can conclude that\n∆ft\nU (ˆh, h∗\nU)\n\u0010\n∆pt\nU (ˆh, h∗\nU)\n\u00111/2 ≤O\n\n\n\n\nr\nσmax\n\u0010\nΛ\n\u0010\n1⊤ˆh(˜Z), 1⊤h∗\nU(˜Z)\n\u0011\u0011\nr\nσmax\n\u0010\nΛ\n\u0010\nˆh(˜Z), h∗\nU(˜Z)\n\u0011\u0011\nv\nu\nu\nt\ndσmax(˜θ∗˜θ⋆⊤)\nσmin\n\u0010Pd\nj=1 w∗\njw∗⊤\nj\n\u0011\n\n\n\n,\nwhich indicates that MAE pre-training admits an\n\n\n\nΩ\n\n\n\n\nr\nσmax\n\u0010\nΛ\n\u0010\n1⊤ˆh(˜Z), 1⊤h∗\nU(˜Z)\n\u0011\u0011\nr\nσmax\n\u0010\nΛ\n\u0010\nˆh(˜Z), h∗\nU(˜Z)\n\u0011\u0011\nv\nu\nu\nt\ndσmax(˜θ∗˜θ⋆⊤)\nσmin\n\u0010Pd\nj=1 w∗\njw∗⊤\nj\n\u0011\n\n\n\n, 1\n2\n\n\n\n\nrepresentation transferrability to binary classification task. Notice that the transfer constant Cβ mainly depends\non the Schur complement of ˆh(˜Z), h∗\nU(˜Z), and 1⊤ˆh(˜Z), 1⊤h∗\nU(˜Z). In the main paper Lemma 2 we omit this\nconstant dependency.\nYuyang Deng, Junyuan Hong, Jiayu Zhou, Mehrdad Mahdavi\nD.2\nProof of Generalization of MAE Pre-training Task\nIn this section we are going to derive generalization bound of the masking pre-training with Transformer. In\npursuit of optimal generalization bound of pretraining task, i.e., regression with deep transformer, we again need\nto employ the framework we introduced in CE analysis (Appendix C.2). Hence, we need to upper bound the\nworst case L2 covering number of deep transformer class. The following result establishes the worst case spectral\ncovering number of L-layer self-attention transformer defined in 14.\nLemma 8 (Covering number of transformer class). Let X[N] = [X1; ...XN] ∈RNK×d denotes the concatenated\ndata matrix. Then the worst case covering number of L-layer transformer class H defined in 14 is bounded as\nfollows:\nln N∞(H(S), ϵ, ∥·∥) ≤O\n \ns2\nL\n\r\rX[N]\n\r\r2\nL\nX\nl=1\nρl\nϵ2\n!\n,\nwhere\nsl :=\nlY\nj=1\n\u0000α2W 2(j) + 1\n\u0001 \u0000W 2(j)α1K + 1\n\u0001\n,\nρl := O\n \nα2\n1(α2W 2(l) + 1)2B2(l) ln(2d2)\n \nK2 + α1W 2(l) (sl−1 ∥X∗∥)2\ndK\n!!\n+ O\n\u0000α2\n2W 2(l)B2(l)(W 2(l) + α2\n1K2W 2(l)) ln(2dm)\n\u0001\n,\n∥X∗∥:= max\ni∈[N] ∥Xi∥.\nRoughly speaking, ρl is the price for covering the parameter of lth self-attention layer, and extending the\ncover to the whole model yields the sum over l. Notice that to ensure a L∞cover, it suffices to ensure that\nPN\ni=1 ∥h(Xi) −hϵ(Xi)∥2 ≤ϵ2. However, if we trivially cover each individual loss ∥h(Xi) −hϵ(Xi)∥2 with ϵ2/N\nradius, the final covering number will be N times larger, which make the later generalization bound vacuous,\ni.e., greater than 1. To avoid this N factor, we directly consider the cover over concatenated data matrix X[N],\nand consider the covering\n\r\r\rˆh(X[N]) −h(X[N])\n\r\r\r\n2\n≤ϵ2. Using the fact that matrix covering bound is independent\nof dimension of X[N], but only depends the spectral norm of X[N], the final covering number will only have\nlogrithmic dependency on N.\nTo prove Lemma 8, first we introduce the following matrix covering number bound from [Bartlett et al., 2017].\nLemma 9. [Bartlett et al., 2017, Lemma 3.2] Let conjugate exponents (p, q) and (r, s) be given with p ≤2, as\nwell as positive reals (a, b, ϵ) and positive integer m. Let matrix X ∈RNK×d be given with ∥X∥p ≤b. Then\nln N\n\u0000\b\nXW : W ∈Rd×m, ∥W∥q,s ≤a\n\t\n, ϵ, ∥· ∥2\n\u0001\n≤\n\u0018a2b2m2/r\nϵ2\n\u0019\nln(2dm).\nLemma 10 (Covering number of attention matrix). Given a set of data S = {X1, ..., XN} and the attention\nmatrix class:\nHS(S) =\n\n\n\n\n\n\n\n\n\n\n\nS =\n\n\nS1, 0, ..., 0,\n...\n0, ..., 0, SN\n\n: Si = softmax\n\u0012\n1\n√dK\nXiWK(XiWQ)⊤\n\u0013\n: ∥WK∥, ∥WQ∥≤W,\n∥WK∥2,1 , ∥WQ∥2,1 ≤B,\n\n\n\n\n\n\n\n\n\n\n\nthe following covering number bound holds true:\nln N(HS(S), ϵ, ∥·∥) ≤O\n \nKW 2B2 ∥X∗∥4\ndKϵ2\nln(2d2)\n!\n.\nOn the Generalization Ability of Unsupervised Pretraining\nProof. we define set K =\nn\nXWK : ∥WK∥≤W, ∥WK∥2,1 ≤B\no\n, Q =\nn\nXWQ : ∥WQ∥≤W, ∥WQ∥2,1 ≤B\no\n.\nWe define ϵK cover of K as CK, and ϵQ cover of Q as CQ. We construct the following set:\nCS =\n\n\n\n\n\nS =\n\n\nS1, 0, ..., 0,\n...\n0, ..., 0, SN\n\n: Si = softmax\n\u0012\n1\n√dK\nXiWK(XiWQ)⊤\n\u0013\n: WK ∈CK, WQ ∈CQ\n\n\n\n\n\nNext we will show that CS is a cover of HS(S) with some radius. For any S[N] ∈HS, we can find ˆS[N] ∈CS such\nthat:\n\r\r\rS[N] −ˆS[N]\n\r\r\r ≤max\ni∈[N]\n\r\r\rSi −ˆSi\n\r\r\r\n= max\ni∈[N]\n\r\r\r\rsoftmax\n\u0012\n1\n√dK\nXiWK(XiWQ)⊤\n\u0013\n−softmax\n\u0012\n1\n√dK\nXi ˆ\nWK(Xi ˆ\nWQ)⊤\n\u0013\r\r\r\r\n≤\n√\nK\n√dK\nmax\ni∈[N]\n\r\r\rXiWK(XiWQ)⊤−Xi ˆ\nWK(Xi ˆ\nWQ)⊤\r\r\r\n≤\n√\nK\n√dK\nmax\ni∈[N]\n\r\r\r(XiWK −Xi ˆ\nWK)(XiWQ)⊤\r\r\r\n+\n√\nK\n√dK\nmax\ni∈[N]\n\r\r\rXi ˆ\nWK(XiWQ)⊤−Xi ˆ\nWK(Xi ˆ\nWQ)⊤\r\r\r\n≤W\n√\nK\n√dK\n(ϵK + ϵQ) max\ni∈[N] ∥Xi∥,\nwhere the first inequality is due to the property of block diagonal matrices. We define ∥X∗∥= maxi∈[N] ∥Xi∥.\nTo ensure above bound is less than ϵ, we choose ϵK = ϵQ =\n√dK\n2W\n√\nK∥X∗∥ϵ. According to Lemma 9, we know:\nln |CS| ≤ln |CK| + ln |CQ| ≤O\n \nKW 2B2 ∥X∗∥4\ndKϵ2\nln(2d2)\n!\n.\nProposition 3 (Covering number of single self-attention layer). Consider the following function class of\nself-attention module:\nHSA :=\n\n\n\n\n\n\n\n\n\nX 7→σ (ZWFC1) WFC2 + Z :Z = (A + X) , A = softmax\n\u0012\n1\n√dK\nXWK(XWQ)⊤\n\u0013\nXWV\n∥WFC1∥, ∥WFC2∥, ∥WK∥, ∥WQ∥, ∥WV ∥≤W,\n∥WFC1∥2,1 , ∥WFC2∥2,1 , ∥WK∥2,1 , ∥WQ∥2,1 , ∥WV ∥2,1 ≤B,\n\n\n\n\n\n\n\n\n\nthen the following bound holds for its covering number:\nln N(HSA(S), ϵ, ∥·∥) ≤O\n \n(α1α2W 2 + α1)2B2 \r\rX[N]\n\r\r2\nϵ2\nln(2d2)\n!  \nK2 + α1W 2 ∥X∗∥2\ndK\n!\n+ O\n \nα2\n2W 2B2(W 2 \r\rX[N]\n\r\r2 + α2\n1K2W 2 \r\rX[N]\n\r\r2)\nϵ2\nln(2dm)\n!\n.\nProof. Recall that X[N] ∈RNK×d is the concatenated data matrix, and we shall use h(X[N]) ∈RNK×d to denote\nthe concatenated encoder output, i.e., h(X[N]) = [h(X1); ..., h(XN)]. Our goal is to find the cardinality of a cover\nsuch that for any h ∈H we can find a hϵ ∈CSA such that\n\r\rh(X[N]) −hϵ(X[N])\n\r\r ≤ϵ.\nYuyang Deng, Junyuan Hong, Jiayu Zhou, Mehrdad Mahdavi\nI: Covering number of input layer by value matrix\nLet CV\nto be ϵV\ncover of set HV (S) =\nn\nX[N]WV : ∥WV ∥≤W, ∥WV ∥2,1 ≤B\no\n, then evoking Lemma 9 we have:\nln N(HV , ϵV , ∥·∥) ≤O\n \nB2 \r\rX[N]\n\r\r2\nϵ2\nV\nln(2dm)\n!\n.\nII: Covering number of Attention layer\nNext, consider the set of attention matrix\nHS(S) =\n\n\n\n\n\n\n\n\n\n\n\nS =\n\n\nS1, 0, ..., 0,\n...\n0, ..., 0, SN\n\n: Si = softmax\n\u0012\n1\n√dK\nXiWK(XiWQ)⊤\n\u0013\n: ∥WK∥, ∥WQ∥≤W,\n∥WK∥2,1 , ∥WQ∥2,1 ≤B,\n\n\n\n\n\n\n\n\n\n\n\nFrom Lemma 10 we know its covering number can be bounded as:\nln N (HS(S), ϵ, ∥·∥) ≤ln N (H ˜S, ϵS, ∥·∥) ≤O\n \nKW 2B2 ∥X∗∥4\ndKϵ2\nS\nln(d2)\n!\n.\nNow we can proceed to bounding the covering number of following set:\nHA(S) =\n\n\n\n\n\nα1softmax\n\u0012\n1\n√dK\nX[N]WK(X[N]WQ)⊤\n\u0013\nX[N]WV : ∥WK∥, ∥WQ∥, ∥WV ∥≤W,\n∥WK∥2,1 , ∥WQ∥2,1 , ∥WV ∥2,1 ≤B,\n\n\n\n\n\nFor every element ˆV[N] ∈CV , we construct the set α1HS(S) ◦ˆV[N] :=\nn\nα1S[N] ˆV[N] : S[N] ∈HS(S)\no\n. Then we\ndefine ϵA-covering of HS ◦ˆV as C(HS ◦ˆV, ϵA, ∥·∥). To construct HS ◦ˆV as C(HS ◦ˆV, ϵA, ∥·∥), we consider CS.\nFor any S[N] ˆV[N] ∈HS(S) ◦ˆV[N], we can find ˆS[N] ∈CS, such that\n\r\r\rα1S[N] ˆV[N] −α1ˆS[N] ˆV[N]\n\r\r\r ≤α1\n\r\r\rS[N] −ˆS[N]\n\r\r\r\n\r\r\r ˆV[N]\n\r\r\r\n≤α1ϵS\n\r\rX[N]\n\r\r W.\nSetting ϵS =\nϵA\nα1∥X[N]∥W we can conclude that C(HS ◦ˆV, ϵA, ∥·∥) actually ϵA covers HS ◦ˆV and the following\nfact holds for the covering number\nln |C(HS(S) ◦ˆV[N], ϵA, ∥·∥)| ≤\nsup\nˆV[N]∈CV\nln N(HS(S) ◦ˆV[N], ϵA, ∥·∥) ≤O\n \nα2\n1KB2W 4 ∥X∗∥4 \r\rX[N]\n\r\r2\ndKϵ2\nA\nln(2d2)\n!\n.\nThen we construct a cover CA for HA by:\nCA =\n[\nˆV[N]∈CV\nC(α1HS(S) ◦ˆV[N])\nIt is not hard to verify the cardinality of this cover:\nln |CA| ≤ln |CV | +\nsup\nˆV[N]∈CV\nln |C(α1HS(S) ◦ˆV[N], ϵA, ∥·∥)|\n≤O\n \nB2 \r\rX[N]\n\r\r2\nϵ2\nV\nln(2dm)\n!\n+ O\n \nα2\n1KB2W 4 ∥X∗∥4 \r\rX[N]\n\r\r2\ndKϵ2\nA\nln(2d2)\n!\n.\nOn the Generalization Ability of Unsupervised Pretraining\nIII: Covering number of fully-connected layer 1\nBy similar reasoning, we can show that the covering\nnumber of\nHFC1(S) =\nn\nZ[N]WFC1 : Z[N] = α1A[N] + X[N], A[N] ∈HA(S), ∥WFC1∥≤W, ∥WFC1∥2,1 ≤B\no\nFor every element ˆA[N] ∈CA, we define set\nˆA[N] ◦WFC1 =\nn\n(α1 ˆA[N] + X[N])WFC1, ∥WFC1∥≤W, ∥WFC1∥2,1 ≤B\no\nWe denote ϵFC1-cover of ˆA[N] ◦WFC1 as C( ˆA[N] ◦WFC1, ϵFC1, ∥·∥), and the covering number of ˆA[N] ◦WFC1 is\nbounded by:\nln |C( ˆA[N] ◦WFC1, ϵFC1, ∥·∥)| ≤\nsup\nˆA[N]∈CA\nln N( ˆA[N] ◦WFC1, ϵFC1, ∥·∥)\n=\nsup\nˆS[N]∈CS\nO\n\n\n\nB2(\n\r\rX[N]\n\r\r2 + α2\n1W 2 \r\rX[N]\n\r\r2 \r\r\rˆS[N]\n\r\r\r\n2\n)\nϵ2\nFC1\nln(dm)\n\n\n\n≤O\n \nB2(\n\r\rX[N]\n\r\r2 + α2\n1K2W 2 \r\rX[N]\n\r\r2)\nϵ2\nFC1\nln(dm)\n!\nNow, we construct the ϵFC1-cover of HFC1(S) as\nCFC1 =\n[\nˆA[N]∈CA\nC( ˆA[N] ◦WFC1, ϵFC1, ∥·∥)\nAnd the covering number is bounded:\nln |CFC1| ≤ln |CA| +\nsup\nˆA[N]∈CA\nln |C( ˆA[N] ◦WFC1, ϵFC1, ∥·∥)|\n≤O\n \nB2 \r\rX[N]\n\r\r2\nϵ2\nV\nln(2dm)\n!\n+ O\n \nα2\n1KB2W 4 ∥X∗∥4 \r\rX[N]\n\r\r2\ndKϵ2\nA\nln(2d2)\n!\n+ O\n \nB2(\n\r\rX[N]\n\r\r2 + α2\n1K2W 2 \r\rX[N]\n\r\r2)\nϵ2\nFC1\nln(dm)\n!\n.\nIV: Covering number of fully-connected layer 2\nThe analysis this part is almost identical to III. We try\nto find the covering number of the set HSA. For every element ˆF[N] ∈CFC1 and ˆA[N] ∈CA, define the set\nα2 ˆF[N] ◦WFC2 + ˆZ[N] =\nn\nα2σ(ˆF[N])WFC2 + ˆZ[N] : ˆZ[N] = α1 ˆA[N] + X[N], ∥WFC2∥≤W, ∥WFC2∥2,1 ≤B\no\n.\nWe denote ϵFC2-cover of α2 ˆF[N] ◦WFC2 + ˆZ as C(α2 ˆF[N] ◦WFC2 + ˆZ, ϵFC2, ∥·∥), and the cardinality of this set is\nbounded by:\nln |C(α2 ˆF[N] ◦WFC2 + ˆZ[N], ϵFC2, ∥·∥)| ≤\nsup\nˆF[N]∈CFC1, ˆA∈CA\nln N(ˆF[N] ◦WFC2 + ˆZ[N], ϵFC2, ∥·∥)\n= O\n\n\nα2\n2B2 \u0010\r\rX[N]\n\r\r2 + α2\n1K2W 2 \r\rX[N]\n\r\r2\u0011\nW 2\nϵ2\nFC2\nln(2dm)\n\n\nYuyang Deng, Junyuan Hong, Jiayu Zhou, Mehrdad Mahdavi\nNow, we construct the ϵFC2-cover of HFC2 as\nCFC2 =\n[\nˆF[N]∈HFC1, ˆA∈HA\nC(ˆF[N] ◦WFC2 + ˆZ[N], ϵFC2, ∥·∥)\nAnd the covering number is bounded:\nln |CFC2| ≤ln |CA| + ln |CFC1| + max\nˆA∈CA\nln |C( ˆA[N] ◦WFC1, ˜ϵ, ∥·∥)|\n≤O\n \nB2 \r\rX[N]\n\r\r2\nϵ2\nV\nln(2dm)\n!\n+ O\n \nα2\n1KB2W 4 ∥X∗∥4 \r\rX[N]\n\r\r2\ndKϵ2\nA\nln(2d2)\n!\n+ O\n \nB2(\n\r\rX[N]\n\r\r2 + α2\n1K2W 2 \r\rX[N]\n\r\r2)\nϵ2\nFC1\nln(dm)\n!\n+ O\n\n\nα2\n2B2 \u0010\r\rX[N]\n\r\r2 + α2\n1K2W 2 \r\rX[N]\n\r\r2\u0011\nW 2\nϵ2\nFC2\nln(2dm)\n\n.\nV: Verification of CFC2 being an ϵ cover of HSA\nIt remains to verify CFC2 is an ϵ cover of HSA. Given any\nH[N] ∈HSA, we can find a ˆH[N] ∈CFC2 such that\n\r\r\rH[N] −ˆH[N]\n\r\r\r =\n\r\r\rα2σ(Z[N]WFC1)WFC2 + Z[N] −α2σ(ˆZ[N] ˆ\nWFC1) ˆ\nWFC2 −ˆZ[N]\n\r\r\r\n≤α2\n\r\r\rσ(Z[N]WFC1)WFC2 −σ(ˆZ[N] ˆ\nWFC1)WFC2\n\r\r\r\n+\n\r\r\rα2σ(ˆZ[N] ˆ\nWFC1)WFC2 + Z[N] −α2σ(ˆZ[N] ˆ\nWFC1) ˆ\nWFC2 −ˆZ[N]\n\r\r\r\n≤α2W\n\r\r\rσ(Z[N]WFC1) −σ(ˆZ[N] ˆ\nWFC1)\n\r\r\r + ϵFC2 +\n\r\r\rZ[N] −ˆZ[N]\n\r\r\r .\nWe bound\n\r\r\rσ(Z[N]WFC1) −σ(ˆZ[N] ˆ\nWFC1)\n\r\r\r first as follows:\n\r\r\rσ(Z[N]WFC1) −σ(ˆZ[N] ˆ\nWFC1)\n\r\r\r ≤\n\r\r\r(α1A[N] + X[N])WFC1 −(α1 ˆA[N] + X[N]) ˆ\nWFC1\n\r\r\r\n≤\n\r\r\r(α1A[N] + X[N])WFC1 −(α1 ˆA[N] + X[N])WFC1\n\r\r\r\n+\n\r\r\r(α1 ˆA[N] + X[N])WFC1 −(α1 ˆA[N] + X[N]) ˆ\nWFC1\n\r\r\r\n≤α1W\n\r\r\rA[N] −ˆA[N]\n\r\r\r + ϵFC1.\nFor\n\r\r\rA[N] −ˆA[N]\n\r\r\r, we have\n\r\r\rA[N] −ˆA[N]\n\r\r\r =\n\r\r\rS[N]X[N]WV −ˆS[N]X[N] ˆ\nWV\n\r\r\r\n≤\n\r\r\rS[N]X[N]WV −S[N]X[N] ˆ\nWV\n\r\r\r +\n\r\r\rS[N]X[N] ˆ\nWV −ˆS[N]X[N] ˆ\nWV\n\r\r\r\n≤K\n\r\r\rX[N]WV −X[N] ˆ\nWV\n\r\r\r + ϵA\n≤KϵV + ϵA.\nPutting pieces together yields:\n\r\r\rσ(ZWFC1) −σ(ˆZ ˆ\nWFC1)\n\r\r\r ≤α1W(KϵV + ϵA) + ϵFC1.\nOn the Generalization Ability of Unsupervised Pretraining\nNow we switch to bounding\n\r\r\rZ[N] −ˆZ[N]\n\r\r\r:\n\r\r\rZ[N] −ˆZ[N]\n\r\r\r = α1\n\r\r\rA[N] −ˆA[N]\n\r\r\r ≤α1(KϵV + ϵA)\nHence we know:\n\r\r\rH[N] −ˆH[N]\n\r\r\r ≤α2W (α1W(KϵV + ϵA) + ϵFC1) + α1(KϵV + ϵA) + ϵFC2\n= (α1α2W 2K + α1K)ϵV + (α1α2W 2 + α1)ϵA + α2WϵFC1 + ϵFC2\nTo make sure RHS is less than ϵ, we set\nϵV =\nϵ\n4(α1α2W 2K + α1K), ϵA =\nϵ\n4(α1α2W 2 + α1), ϵFC1 =\nϵ\n4α2W , ϵFC2 = ϵ\n4.\nRecall that\nln |CFC2| ≤ln |CA| + ln |CFC1| + max\nˆA∈CA\nln |C( ˆA[N] ◦WFC1, ˜ϵ, ∥·∥)|\n≤O\n \nB2 \r\rX[N]\n\r\r2\nϵ2\nV\nln(2dm)\n!\n+ O\n \nα2\n1KB2W 4 ∥X∗∥4 \r\rX[N]\n\r\r2\ndKϵ2\nA\nln(2d2)\n!\n+ O\n \nB2(\n\r\rX[N]\n\r\r2 + α2\n1K2W 2 \r\rX[N]\n\r\r2)\nϵ2\nFC1\nln(dm)\n!\n+ O\n\n\nα2\n2B2 \u0010\r\rX[N]\n\r\r2 + α2\n1K2W 2 \r\rX[N]\n\r\r2\u0011\nW 2\nϵ2\nFC2\nln(2dm)\n\n.\nHence we can upper bound the covering number of HSA as follows:\nln N(HSA, ϵ, ∥·∥) ≤O\n \n(α1α2W 2 + α1)2K2B2 \r\rX[N]\n\r\r2\nϵ2\nln(2dm)\n!\n+ O\n \n(α1α2W 2 + α1)2α2\n1KB2W 4 ∥X∗∥4 \r\rX[N]\n\r\r2\ndKϵ2\nln(2d2)\n!\n+ O\n \nα2\n2W 2B2(\n\r\rX[N]\n\r\r2 + α2\n1K2W 2 \r\rX[N]\n\r\r2)\nϵ2\nln(2dm)\n!\n= O\n \n(α1α2W 2 + α1)2B2 \r\rX[N]\n\r\r2\nϵ2\nln(2d2)\n!  \nK2 + α1W 4 ∥X∗∥4\ndK\n!\n+ O\n \nα2\n2W 2B2(\n\r\rX[N]\n\r\r2 + α2\n1K2W 2 \r\rX[N]\n\r\r2)\nϵ2\nln(2dm)\n!\n.\nProposition 4 (Contraction mapping of self-attention layer). For a single attention layer parameterized by W,\nwith ∥W∥≤W, the following statement holds:\n\r\r\rSAW(X) −SAW( ˆX)\n\r\r\r ≤(α2W 2 + 1) (α1KW + 1)\n\r\r\rX −ˆX\n\r\r\r .\nYuyang Deng, Junyuan Hong, Jiayu Zhou, Mehrdad Mahdavi\nProof. The proof follows by definition and simple algebraic manipulation:\n\r\r\rSAW(X) −SAW( ˆX)\n\r\r\r ≤\n\r\r\rα2σ (ZWFC1) WFC2 + Z −βσ\n\u0010\nˆZWFC1\n\u0011\nWFC2 −ˆZ\n\r\r\r\n≤α2W 2 \r\r\rZ −ˆZ\n\r\r\r +\n\r\r\rZ −ˆZ\n\r\r\r\n≤(α2W 2 + 1)\n\r\r\rA + X −ˆA −ˆX\n\r\r\r\n≤(α2W 2 + 1)\n\u0010\r\r\rα1SXWV −α1S ˆXWV\n\r\r\r +\n\r\r\rX −ˆX\n\r\r\r\n\u0011\n≤(α2W 2 + 1) (α1KW + 1)\n\r\r\rX −ˆX\n\r\r\r .\nD.2.1\nProof of Lemma 8\nProof. We first examine the norm of each self-attention layer’s output:\n\r\rXl\ni\n\r\r ≤α2W 2(l)\n\r\rZl\ni\n\r\r +\n\r\rZl\ni\n\r\r\n≤\n\u0000α2W 2(l) + 1\n\u0001 \u0000α\n\r\rAl\ni\n\r\r +\n\r\rXl−1\ni\n\r\r\u0001\n≤\n\u0000α2W 2(l) + 1\n\u0001 \u0000W 2(l)αK + 1\n\u0001 \r\rXl−1\r\r\n≤\nlY\nj=1\n\u0000α2W 2(j) + 1\n\u0001 \u0000W 2(j)α1K + 1\n\u0001\n∥Xi∥\n(16)\nand grouped output\n\r\r\rXl\n[N]\n\r\r\r ≤α2W 2(l)\n\r\r\rZl\n[N]\n\r\r\r +\n\r\r\rZl\n[N]\n\r\r\r\n≤\n\u0000α2W 2(l) + 1\n\u0001 \u0010\nα\n\r\r\rAl\n[N]\n\r\r\r +\n\r\r\rXl−1\n[N]\n\r\r\r\n\u0011\n≤\n\u0000α2W 2(l) + 1\n\u0001 \u0000W 2(l)αK + 1\n\u0001 \r\r\rXl−1\n[N]\n\r\r\r\n≤\nlY\nj=1\n\u0000α2W 2(j) + 1\n\u0001 \u0000W 2(j)α1K + 1\n\u0001 \r\rX[N]\n\r\r .\nFor the ease of presentation, we define the class of lth layer output:\nHl =\n\n\n\n\n\nSAl \u0010\nSAl−1...SA1(X)\n\u0011\n:\n\r\r\rWj\nFC1\n\r\r\r ,\n\r\r\rWj\nFC2\n\r\r\r ,\n\r\r\rWj\nK\n\r\r\r ,\n\r\rWl\nQ\n\r\r ,\n\r\r\rWj\nV\n\r\r\r ≤W(j),\n\r\r\rWj\nFC1\n\r\r\r\n2,1 ,\n\r\r\rWj\nFC2\n\r\r\r\n2,1 ,\n\r\r\rWj\nK\n\r\r\r\n2,1 ,\n\r\r\rWj\nQ\n\r\r\r\n2,1 ,\n\r\r\rWj\nV\n\r\r\r\n2,1 ≤B(j), ∀j ∈[l]\n\n\n\n\n\n,\nand it will be useful to define set of weight matrices at lth layer:\nWl =\n\bW : ∥W∥≤W(l), ∥W∥2,1 ≤B(l).\t\n.\nWe shall construct the cover with certain radius for each Hl, l ∈[L].\nFor base case l = 1: we create ϵ1 cover of SA1(X[N])\nC1 = C(SA1(X[N]), ϵ1, ∥·∥).\nFor 1 < l + 1 ≤L, for each element ˆXl ∈Cl, we construct the ϵl+1-cover of the following set:\nSAl+1( ˆXl) :=\nn\nSAWl+1( ˆXl), Wl+1 ∈Wl+1\no\nOn the Generalization Ability of Unsupervised Pretraining\nand we denote the cover as C\n\u0010\nSAl+1( ˆXl), ϵl+1, ∥·∥\n\u0011\n. We first examine the cardinality of this cover as follows:\nln\n\f\f\fC\n\u0010\nSAl+1(Xl\n[N]), ϵl+1, ∥·∥\n\u0011\f\f\f\n≤max\nXl∈Cl\nO\n\u0012(α1α2W 2 + α1)2B2\nϵ2\nln(2d2)\n\u0013  \nK2 + α1W 4 \u0000maxi∈[N]\n\r\rXl\ni\n\r\r\u00014\ndK\n! \r\r\rXl\n[N]\n\r\r\r\n2\n+ O\n\u0012α2\n2W 2(l + 1)B2(l + 1)(1 + α2\n1K2W 2(l + 1))\nϵ2\nln(2dm)\n\u0013 \r\r\rXl\n[N]\n\r\r\r\n2\n≤O\n\u0012(α1α2W 2(l + 1) + α1)2B2(l + 1)\nϵ2\nln(2d2)\n\u0013  \nK2 + α1W 4(l + 1) (sl ∥X∗∥)4\ndK\n!\ns2\nl\n\r\rX[N]\n\r\r2\n+ O\n\u0012α2\n2W 2(l + 1)B2(l + 1)(1 + α2\n1K2W 2(l + 1))\nϵ2\nln(2dm)\n\u0013\ns2\nl\n\r\rX[N]\n\r\r2\n:= ln Nl+1\nwhere sl := Ql\nj=1\n\u0000α2W 2(j) + 1\n\u0001 \u0000W 2(j)α1K + 1\n\u0001\n.\nWe then construct cover for Hl+1 as:\nCl+1 =\n[\nXl∈Cl\nC\n\u0010\nSAl+1(Xl), ϵl+1, ∥·∥\n\u0011\n.\nIt is not hard to check the cardinality of Cl+1\n|Cl+1| =\n\f\f\f\f\f\f\n[\nXl∈Cl\nC\n\u0010\nSAl+1(Xl), ϵl+1, ∥·∥\n\u0011\n\f\f\f\f\f\f\n≤|Cl|Nl+1 ≤\nl+1\nY\nl′=1\nNl′\nLet\nρl := O\n\u0000(α1α2W 2(l) + α1)2B2(l) ln(2d2)\n\u0001\n \nK2 + α1W 4(l) (sl−1 ∥X∗∥)4\ndK\n!\n+ O\n\u0000α2\n2W 2(l)B2(l)(1 + α2\n1K2W 2(l)) ln(2dm)\n\u0001\n,\nwe have\nln |Cl+1| ≤\nl+1\nX\nl′=1\nln Nl′ ≤\nl+1\nX\nl′=1\nρl′\nϵ2\nl′ s2\nl′\n\r\rX[N]\n\r\r2.\nNow it remains to verify CL is a cover of HL. For any XL ∈HL we can find a ˆXL ∈CL such that\n\r\r\rXL −ˆXL\r\r\r =\n\r\r\rSAWL(XL−1) −SA ˆ\nWL( ˆXL−1)\n\r\r\r\n≤\n\r\r\rSAWL(XL−1) −SAWL( ˆXL−1)\n\r\r\r +\n\r\r\rSAWL( ˆXL−1) −SA ˆ\nWL( ˆXL−1)\n\r\r\r\n≤(α2W 2(L) + 1) (α1KW(L) + 1)\n\r\r\rXL−1 −ˆXL−1\r\r\r + ϵL\n≤\nL\nX\nl=0\nL\nY\nj=l+1\n(α2W 2(j) + 1) (α1KW(j) + 1) ϵl\nWe choose ϵj\n=\n\u0010\nL QL\nj=l+1(α2W 2(j) + 1) (α1KW(j) + 1)\n\u0011−1\nϵ, and let sl+17→L\n:=\nQL\nj=l+1(α2W 2(j) +\n1) (α1KW(j) + 1). Hence we have:\nρl\nϵ2\nl\n=ρls2\nl+17→L\nϵ2\n\r\rX[N]\n\r\r2\nYuyang Deng, Junyuan Hong, Jiayu Zhou, Mehrdad Mahdavi\nand conclude the covering number of HL as follows:\nln N(HL, ϵ, ∥·∥) = ln |CL| ≤\nL\nX\nl=1\nρl\nϵ2\nl\n\u0000sl\n\r\rX[N]\n\r\r\u00012\n= ln |CL| ≤O\n \ns2\nL\n\r\rX[N]\n\r\r2\nL\nX\nl=1\nρl\nϵ2\n!\n.\nFinally according to covering number fact (13),\nln N∞(G ◦H(S), ϵ, ∥·∥) ≤ln N(G ◦H(S), ϵ, ∥·∥),\n(17)\nwe can conclude the proof.\nNow, equipped with covering number bound for the transformer, we are ready to show the generalization of MAE\npre-training task.\nD.2.2\nProof of Lemma 3\nProof. Similar to the proof in CE section, we evoke Lemma 7 with c = O\n\u0012\ns2\nL+1\n\r\r\r˜Z[N]\n\r\r\r\n2 PL+1\nl=1 ρl\n\u0013\n, where sl, ρl\nare defined in Lemma 8.\nR b\nS (L(r)) ≤10\nr\nHr · c\nN\n+ 10\nr\ncHr\nN\n \nln\n√\nbr −ln\n \n5\n2\nr\nHr · c\nN\n!!\n= 10\nr\nHr · c\nN\n+ 10\nr\ncHr\nN\nln\n \n2\n5\nr\nbN\nHc\n!\n.\nWe set ϕ(r) = 10\nq\nHr·c\nN\n· max\n\u001a\n1, ln\n\u0012\n2\n5\nq\nbN\nHc\n\u0013\u001b\n. Solving the follwoing equation to get r∗\nϕ(r) = 10\nr\nHr · c\nN\n· max\n(\n1, ln\n \n2\n5\nr\nbN\nHc\n!)\n= r,\n⇐⇒r∗= 100H · c\nN\n· max\n(\n1, ln\n \n2\n5\nr\nbN\nHc\n!)2\nNow, according to Theorem 7, and the fact that\nA ≤B + C\n√\nA =⇒A ≤B + C2 +\n√\nBC,\nwe have\nLU(g ◦h) ≤L b\nU(g ◦h) + 45r∗+\n \n√\n8r∗+\nr\n4b(log(1/ν) + 6 log log N)\nN\n!2\n+ 20b(ν + 6 log log N)\nN\n+\nr\nL b\nU(g ◦h) + 45r∗+ 20b(ν + 6 log log N)\nN\n \n√\n8r∗+\nr\n4b(log(1/ν) + 6 log log N)\nN\n!\nPlugging r∗and empirical risk minimizers ˆg, ˆh will conclude the proof.\nOn the Generalization Ability of Unsupervised Pretraining\nD.3\nProof of Theorem 3\nProof. Again, recall in Theorem 1, the generalization bound of downstream task is given by\nET ( ˆf, ˆh) ≤Cβ\n\u0010\nEU(ˆg, ˆh)\n\u0011β\n+ 4GϕR b\nT (F ◦ˆh) + 4Bϕ\nr\nlog(1/ν)\nn\n+ 4Bϕ ∥T −UX ∥TV\n+ min\nf∈F ET (f, h∗\nU).\nSince in the previous subsection we prove the bounded transferrability and generalization of pre-training task, it\nremains to show the upper bound of representation-induced Rademacher complexity.\nR b\nT (ϕ ◦F ◦ˆh) = Eε∈{±1}n\n\"\nsup\nθ:∥θ∥≤R\n1\nn\nXn\ni=1 εiϕ(θ⊤(1⊤ˆh(Xi))⊤, yi)\n#\n≤GϕEε∈{±1}n\n\"\nsup\nθ:∥θ∥≤R\n1\nn\nXn\ni=1 εiθ⊤(1⊤ˆh(Xi))⊤\n#\n= RGϕ\nn\nEε\n\r\r\r\nXn\ni=1 εi(1⊤ˆh(Xi))⊤\r\r\r\n≤RGϕ\nn\nr\nEε\n\r\r\r\nXn\ni=1 εi(1⊤ˆh(Xi))⊤\n\r\r\r\n2\n≤RGϕ\nn\nrXn\ni=1\n\r\r\r1⊤ˆh(Xi)\n\r\r\r\n2\n≤RGϕ\nn\nrXn\ni=1 K\n\r\r\rˆh(Xi)\n\r\r\r\n2\nwhere at first inequality we apply Ledoux-Talagrand’s inequality to peel of Lipschitz loss ϕ(·), and at last inequality\nwe use the fact that εi are i.i.d. with zero mean, so that the cross terms disappear. For each\n\r\r\rˆh(Xi)\n\r\r\r\n2\n, evoking\n(16) we have:\n\r\r\rˆh(Xi)\n\r\r\r ≤\nlY\nj=1\n\u0000α2W 2(j) + 1\n\u0001 \u0000W 2(j)α1K + 1\n\u0001\n∥Xi∥,\nhence we arrive at\nR b\nT (ϕ ◦F ◦ˆh) ≤\nRGϕ\nqQl\nj=1 (α2W 2(j) + 1)2 (W 2(j)α1K + 1)2 Pn\ni=1 ∥Xi∥2\nn\n.\nPlugging Lemmas 2 and 3 as well as above bound will complete the proof.\nE\nProof of Convergence RadReg Algorithm\nIn this section we provide the missing proofs from Section 6. Then we provide the proof of convergence.\nE.1\nConvergence result of RadReg\nIn this section we provide formal version of convergence results for RadReg. First let us introduce the following\nMoreau envelope concept.\nDefinition 6 (Moreau Envelope). A function Ψρ(w) is the ρ-Moreau envelope of a function Ψ if Ψρ(w) :=\nminw′∈W{Ψ(w′) + 1\n2ρ∥w′ −w∥2}.\nWe have the following property of the Moreau Envelope of a nonsmooth function:\nLemma 11. [Davis and Drusvyatskiy, 2019] Let ˆw = arg minw′∈W Ψ(w′) +\n1\n2ρ∥w′ −w∥2, then we have the\nfollowing facts: ∥ˆw −w∥≤ρ∥∇Φρ(w)∥, ming∈∂Ψ( ˆw) ∥g∥≤∥∇Φρ(w)∥.\nYuyang Deng, Junyuan Hong, Jiayu Zhou, Mehrdad Mahdavi\nLemma 11 shows that, if we find a w such that ∥∇Ψρ(w)∥is small, then we can demonstrate that w is near some\npoint ˆx which is a near-stationary point of Ψ. We will use 1/4L-Moreau envelope of Ψ, following the setting in\n[Lin et al., 2020, Rafique et al., 2018], and state the convergence rate in terms of ∥∇Ψ1/4L(w)∥. We also define\nquantity ˆ∆Ψ1/4L = Ψ1/4L(w0) −minw∈W Ψ1/4L(w) that will be used in stating the convergence rates.\nAssumption 2 (Bounded Variance).\nLet ˜z and ˜x be uniformly sampled from ˆU and ˆD. Then, the variance of\nstochastic gradients is bounded:\nE\n\r\r∇L ˆU(w; ˜z) −∇L ˆU(w)\n\r\r2 ≤δ2,\nE\n\r\r\r\r∇Rj(v, w; ˜x) −1\nn\nXn\ni=1 ∇Rj(v, w; xi)\n\r\r\r\r\n2\n≤δ2.\nAssumption 3 (Smooth and Bounded Linear Head).\nL ˆU and Rj(v, w′; x) are L smooth w.r.t. w, ∀j ∈[B],\nand x ∈X:\n\r\r∇L ˆU(w) −∇L ˆU(w′)\n\r\r ≤L ∥w −w′∥,\n∥∇wRj(v, w; x) −∇wRj(v, w′; x)∥≤L ∥w −w′∥.\nAlso, we assume Rj(v, w; x) is linear in v, and maxv∈V ∥v∥≤D.\nAssumption 4 (Lipschitzness).\nL ˆU and Rj(v, w′; x) are G Lipschitz w.r.t. w, ∀v ∈V, j ∈[B], and x ∈X,\ni.e.,\n\r\rL ˆU(w) −L ˆU(w′)\n\r\r ≤G ∥w −w′∥,\n∥Rj(v, w; x) −Rj(v, w′; x)∥≤G ∥w −w′∥.\nWe are now ready to state the formal version of Theorem 4 as follows.\nTheorem 8 (Convergence of RadReg with Linear Top Layer). Under Assumptions 2 and 3, if we use RadReg\n(Algorithm 1 with one step update) to optimize (8), by choosing η = Θ\n\u0010\nϵ6\nL3D2G\n\u0011\nand γ = Θ\n\u0010\nϵ2\nLδ2\n\u0011\nit holds that:\n1\nT + 1\nXT\nt=0 E\n\r\rΨ1/4L(wt)\n\r\r2 ≤ϵ2,\nwith the gradient complexity bounded by:\nO\n \nBL3(G2 + δ2\nn′ )D2 δ2\nn′ ∆Ψ1/4L\nϵ8\n!\n.\nWe can see that the proposed optimization algorithm can find an ϵ stationary point with at most O\n\u0000 B\nϵ8\n\u0001\nstochastic\ngradient evaluations. Since the complexity grows in terms of B, a proper sampling size of Rademacher variable is\ncrucial.\nIn the rest of this section, we prove the convergence rate of RadReg. The proof idea mainly follows the framework\ndeveloped in Lin et al. [2019]. But before we state a few intermediate results that the main proof relies on.\nLemma 12. Under the conditions of Theorem 8, the following one iteration recursion relation holds true:\nηE\n\r\r∇Ψ(wt−1)\n\r\r2 = E[Ψ1/2L(wt−1)] −E[Ψ1/2L(wt)]\n+ 4ηL 1\nB\nB\nX\nj=1\nE\n\u0002\nRj(v∗\nj(wt−1), wt−1) −Rj(vt−1\nj\n, wt−1)\n\u0003\n+ η2L(G2 + δ2\nn′ ),\nwhere v∗\nj(w) := arg maxv∈V Rj(v, wt−1).\nProof. Recall the definition of Ψ and Rj:\nΨ(w) := L ˆU(w) + λ 1\nB\nB\nX\nj=1\n\"\nmax\nV∈V v⊤\n \n1\nn\nn\nX\ni\nσj\ni hw(xi)\n!#\n,\n(18)\nRj(v, w) := v⊤\n \n1\nn\nn\nX\ni\nσj\ni hw(xi).\n!\n(19)\nOn the Generalization Ability of Unsupervised Pretraining\nAlso recall the definition of Ψ’s Moreau Envelope:\nΨ1/4L(w) := min\nw′∈W Ψ(w′) + 2L ∥w −w′∥2 .\nWe define the proximal solution as:\nˆwt := arg min\nw′∈W Ψ(w′) + 2L\n\r\rwt −w′\r\r2 .\nWith all aforementioned definitions are in place, we proceed to proving the lemma. First, since ˆwt−1 is not\nminimizer of Ψ(·) + 2L ∥· −wt∥2 we have\nE[Ψ1/4L(wt)] ≤E[Ψ( ˆwt−1)] + 2L\n\r\r ˆwt−1 −wt\r\r2\nRecall the updating rule for w and vj as stated below:\nwt+1 = wt −η\n\n1\nn′\nn′\nX\ni=1\n∇L(wt; ˜xt\ni) + λ 1\nB\nB\nX\nj=1\n1\nn′\nn′\nX\ni=1\n∇wRj(vt\nj, wt; ˜xt\ni)\n\n,\nvt+1\nj\n= vt\nj + γλ 1\nB\nB\nX\nj=1\n1\nn′\nn′\nX\ni=1\n∇vRj(vt\nj, wt; ˜xt\ni).\nHence we can get the following relation by completing the square trick:\nE\n\r\r ˆwt−1 −wt\r\r2 = E\n\r\r ˆwt−1 −wt−1\r\r2\n+ 2ηE\n*\nˆwt−1 −wt−1, ∇L(wt−1) + λ 1\nB\nB\nX\nj=1\n∇vRj(vt−1\nj\n, wt−1)\n+\n+ η2(G2 + δ2\nn′ ).\nAccording to L-smoothness of L and Rj, we can re-write the inner product term as:\nE\n*\nˆwt−1 −wt−1, ∇L(wt−1) + λ 1\nB\nB\nX\nj=1\n∇vRj(vt−1\nj\n, wt−1)\n+\n≤E\n\nL( ˆwt−1) −L(wt−1) + λ 1\nB\nB\nX\nj=1\n\u0000Rj(vt−1\nj\n, ˆwt−1) −Rj(vt−1\nj\n, wt−1)\n\u0001\n\n+ L\n\r\r ˆwt−1 −wt−1\r\r\nNotice the following fact about Ψ, Ψ1/4L:\nL( ˆwt−1) + λ 1\nB\nB\nX\nj=1\nRj(vt−1\nj\n, ˆwt−1) ≤Ψ( ˆwt−1) ≤Ψ1/4L(wt−1) −2L\n\r\r ˆwt−1 −wt−1\r\r2 .\nThe last inequality is because ˆwt−1 is the minimizer of Ψ(·) + 2L\n\r\r· −wt−1\r\r2. As a result, the inner product is\nbounded by:\nE\n*\nˆwt−1 −wt−1, ∇L(wt−1) + λ 1\nB\nB\nX\nj=1\n∇vRj(vt−1\nj\n, wt−1)\n+\n≤E\n\u0002\nΨ(wt−1) −F(vt−1\nj\n, wt−1)\n\u0003\n−LE\n\r\r ˆwt−1 −wt−1\r\r .\nFinally, putting pieces together and using the fact that ∇Ψ1/4L(wt−1) =\n\r\r ˆwt−1 −wt−1\r\r /4L will conclude the\nproof:\nE[Ψ1/2L(wt)] ≤E[Ψ1/2L(wt−1)] + 4ηLE\n\u0002\nΨ(wt−1) −F(vt−1\nj\n, wt−1)\n\u0003\n−4ηLE\n\r\r ˆwt−1 −wt−1\r\r + 2η2L(G2 + δ2\nn′ )\n= E[Ψ1/2L(wt−1)] + 4ηL 1\nB\nB\nX\nj=1\nE\n\u0002\nRj(v∗(wt−1), wt−1) −Rj(vt−1\nj\n, wt−1)\n\u0003\n−4ηL2E\n\r\r ˆwt−1 −wt−1\r\r + 2η2L(G2 + δ2\nn′ ).\nYuyang Deng, Junyuan Hong, Jiayu Zhou, Mehrdad Mahdavi\nLemma 13 (Lemma D4 in [Lin et al., 2019]). If Rj(v, w) is convex and smooth in v, L smooth and G Lipschitz\nin w, then under the dynamic of stochastic gradient descent ascent on v, we have the following statement holding:\nE[Rj(v∗(wt−1), wt−1) −Rj(vt−1\nj\n, wt−1)]\n≤ηG\np\nG2 + δ2/n′(2t −2s −1) + 1\n2γ\n\u0010\nE\n\r\rv∗(ws) −vt−1\nj\n\r\r2 −E\n\r\rv∗(ws) −vt\nj\n\r\r2\u0011\n+ E[Rj(vt, wt) −Rj(vt−1, wt−1)] + γδ2\n2n′\nand\n1\nT + 1\nT\nX\nt=0\nE[Rj(v∗(wt), wt) −Rj(vt\nj, wt)] ≤ηGS2p\nG2 + σ2 + D2\n2Sγ + γδ2\n2n′ + maxv R(v, w0) −R(v0, w0)\nT + 1\n.\nwhere D = maxv∈V ∥v∥,\nE.2\nProof of Theorem 8\nNow we are ready to present proof of Theorem 8 by putting the above results together.\nProof. Summing Lemma 12 from t = 0 to T yields:\n1\nT + 1\nT\nX\nt=0\nE\n\r\r∇Ψ(wt)\n\r\r2 = E[Ψ1/2L(w0)] −E[Ψ1/2L(wT )]\nη(T + 1)\n+ 4L\n\u0012\nηGS\np\nG2 + σ2 + D2\n2Sγ + γδ2\n2\n\u0013\n+ 4 1\nB\nB\nX\nj=1\nL(maxv Rj(v, w0) −Rj(v0, w0))\nT + 1\n+ 4ηL\np\nG2 + δ2/n′.\nSetting S = D\n2\nq\n1\nηγG√\nG2+δ2/n′ yields:\n1\nT + 1\nT\nX\nt=0\nE\n\r\r∇Ψ(wt)\n\r\r2 = O\n \nE[Ψ1/2L(w0)] −E[Ψ1/2L(wT )]\nη(T + 1)\n!\n+ O\n\nLD\ns\nηG\n√\nG2 + δ2\nγ\n+ Lγδ2\n2\n\n\n+ O\n\u0012L(maxv R(v, w0) −R(v0, w0))\nT + 1\n+ ηL\np\nG2 + δ2\n\u0013\n.\nFinally, by choosing η = Θ\n\u0010\nϵ6\nL3D2G\n\u0011\nand γ = Θ\n\u0010\nϵ2\nLδ2\n\u0011\n, we can guarantee the stationary of past iterates:\n1\nT + 1\nT\nX\nt=0\nE\n\r\rΨ1/4L(wt)\n\r\r ≤ϵ,\nwith the gradient complexity bounded by\nO\n \nBL3(G2 + δ2/n′)D2δ2∆Ψ1/4L\nϵ8\n!\n.\nas stated.\nF\nExperiment Details\nRecall we utilize the Masked AutoEncoder (MAE) [He et al., 2022] as the base unsupervised pre-training method.\nFor models, we use the Tiny Vision Transform (TinyViT) [Wu et al., 2022] as the backbone for pre-training\nOn the Generalization Ability of Unsupervised Pretraining\nand use a 10-way linear classifier on top of the encoder for fine-tuning. The encoder h sequentially contains\none convolutional layer, 12 192-head attention blocks, and one layer-normalization layer. The decoder g for\nreconstructing images in MAE includes 4 192-head attention blocks followed by one linear layer.Details of\nhyperparameters for the experiments reported in Table 1 are included in Table 2. For RadReg, we sample σ for 50\ntimes and solve the inner maximization by Adam optimizer with a learning rate of 0.001 and a weight decay of\n5 × 10−4.\nConfig\nValue\nOptimizer\nAdamW\nBase learning rate\n1.5 × 10−4\nOptimizer momentum\nβ = 0.9, 0.95\nBatch size\n4096\nLearning rate schedule\ncosine decay\nWarmup epochs\n200\nAugmentation\nRandomResizedCrop\nMasking ratio\n75%\nPre-training epochs\n2000\nFine-tuning epochs\n300\nTable 2: Pre-training setting. Fine-tuning follows the same setting except for the number of epochs.\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2024-03-11",
  "updated": "2024-03-11"
}