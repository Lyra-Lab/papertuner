{
  "id": "http://arxiv.org/abs/2403.07569v1",
  "title": "Exploring Challenges in Deep Learning of Single-Station Ground Motion Records",
  "authors": [
    "Ümit Mert Çağlar",
    "Baris Yilmaz",
    "Melek Türkmen",
    "Erdem Akagündüz",
    "Salih Tileylioglu"
  ],
  "abstract": "Contemporary deep learning models have demonstrated promising results across\nvarious applications within seismology and earthquake engineering. These models\nrely primarily on utilizing ground motion records for tasks such as earthquake\nevent classification, localization, earthquake early warning systems, and\nstructural health monitoring. However, the extent to which these models\neffectively learn from these complex time-series signals has not been\nthoroughly analyzed. In this study, our objective is to evaluate the degree to\nwhich auxiliary information, such as seismic phase arrival times or seismic\nstation distribution within a network, dominates the process of deep learning\nfrom ground motion records, potentially hindering its effectiveness. We perform\na hyperparameter search on two deep learning models to assess their\neffectiveness in deep learning from ground motion records while also examining\nthe impact of auxiliary information on model performance. Experimental results\nreveal a strong reliance on the highly correlated P and S phase arrival\ninformation. Our observations highlight a potential gap in the field,\nindicating an absence of robust methodologies for deep learning of\nsingle-station ground motion recordings independent of any auxiliary\ninformation.",
  "text": "1\nExploring Challenges in Deep Learning of\nSingle-Station Ground Motion Records\n¨Umit Mert C¸ a˘glar∗, Baris Yilmaz∗, Melek T¨urkmen∗, Erdem Akag¨und¨uz∗, Salih Tileylioglu†\n∗Dept. of Modeling and Simulation, Graduate School of Informatics,\nMiddle East Technical University, Ankara, T¨urkiye\n†Dept. of Civil Engineering, Kadir Has University, Istanbul, T¨urkiye\n{mert.caglar, yilmaz.baris 01, turkmen.melek, akaerdem}@metu.edu.tr, salih.tileylioglu@khas.edu.tr\nAbstract—Contemporary deep learning models have demon-\nstrated promising results across various applications within\nseismology and earthquake engineering. These models rely\nprimarily on utilizing ground motion records for tasks such\nas earthquake event classification, localization, earthquake early\nwarning systems, and structural health monitoring. However, the\nextent to which these models effectively learn from these complex\ntime-series signals has not been thoroughly analyzed. In this\nstudy, our objective is to evaluate the degree to which auxiliary\ninformation, such as seismic phase arrival times or seismic station\ndistribution within a network, dominates the process of deep\nlearning from ground motion records, potentially hindering its\neffectiveness. We perform a hyperparameter search on two deep\nlearning models to assess their effectiveness in deep learning\nfrom ground motion records while also examining the impact of\nauxiliary information on model performance. Experimental results\nreveal a strong reliance on the highly correlated P and S phase\narrival information. Our observations highlight a potential gap in\nthe field, indicating an absence of robust methodologies for deep\nlearning of single-station ground motion recordings independent\nof any auxiliary information.\nIndex Terms—Deep Learning, ground motion records, High-\nPerformance Computing, Epicentral Distance Prediction\nI. INTRODUCTION\nAt the core of deep learning lies a defining element: deep\nfeatures. These abstract representations, automatically extracted\nthrough multiple layers of a neural network, enable the\nmodel to discern intricate patterns and properties within a\ncomplex input signal. The depth of the network allows it\nto learn increasingly complex and nuanced representations,\nempowering it to make more informed and accurate predictions.\nIn essence, deep features serve as the bedrock of the remarkable\ncapabilities exhibited by deep learning models in tasks such as\nimage recognition [4], natural language processing [20], and\nvarious other domains, including seismology and earthquake\nengineering [14].\nEarthquakes occur when faults rupture underground. The\nlocation where the rupture occurs is termed as the focus\nof the earthquake. Once a fault ruptures, waves propagate\nthrough the rocks, traversing soils along their trajectory, until\nthey eventually reach the ground surface. The first waves\nto reach the ground surface are the body waves (P and S\nwaves, respectively). These waves are then followed by the\nsurface waves, known as Rayleigh and Love waves. The waves\nthat reach the surface create the ground motions felt during\nearthquakes. To study earthquakes and their impacts, seismic\nstations are established with the purpose of recording ground\nmotions resulting from seismic activity. Ground motions caused\nby an earthquake are generally defined by three components\nperpendicular to each other. Seismic stations generally include\na sensor usually capable of sensing the signal in three directions\nas well as a recorder. Depending on the size of the earthquake\nand proximity to the fault, ground motion amplitudes caused\nby an earthquake can range from a few nanometers to several\nmeters, and thus, it becomes a challenge to record the wide\nrange of signals with a single type of instrument [1]. Earthquake\nmeasuring instruments come in various types, each tailored for\nspecific purposes, characterized by distinct dynamic ranges and\nbandwidths. Stations consisting of seismographs, which serve\nas the primary data sources for this study, play an important\nrole in determining an earthquake’s location.\nThe location of an earthquake is commonly defined by its\nepicenter, which is the projection of the focus of the earthquake\non the ground surface. The epicentral distance, the focal point\nof this study, refers to the distance between the epicenter of an\nearthquake and a specific location, typically a seismic station.\nThis distance is typically estimated by measuring the time\ndifference between the arrival of P waves and S waves at a\nstation.\nThe convergence of deep learning with seismology and\nearthquake engineering is a recent development in the literature.\nBased on a rather small-scale dataset consisting of roughly\n∼3k events, [19] established the first DL-based earthquake\nengineering model in 2018. In the short period that followed this\nseminal study, there have been various studies in seismology\nand earthquake engineering that used deep learning for different\npurposes, such as in event detection for early warning [9],\n[11], [14], [16], [25], [2], event classification [7], [17], ground\nresponse estimation [5], earthquake (EQ) phase picking [18],\n[14], magnitude estimation [6], [24], [16], [29], [21], [22],\n[3], EQ origin time estimation [12], [22], epicenter location\nclassification [9], [11], [8], [22], epicentral distance estimation\n[21], [27], soil parameter estimation [26], depth prediction\n[8], [12], [24], [29], [21], [22], [2], and epicenter coordinates\nprediction [28], [12], [24], [29], [2].\nA large-scale dataset is essential to any deep learning study,\nespecially when it comes to creating a high-level (i.e., deep)\narXiv:2403.07569v1  [eess.SP]  12 Mar 2024\n2\nFig. 1: A sample recorded event from the STEAD, belonging\nto station at 38.034, -120.38, Columbia (California). Primary\n(solid) and Secondary wave (dashed) arrival time indices are\ndepicted as dashed lines.\nrepresentation of the input signal. However, in the field of\nAI-based seismology and earthquake engineering, despite its\nimportance, such a corpus containing hundreds of thousands of\nEQ records did not exist until very recently. This necessity led\nto the publication of the Stanford Earthquake Dataset (STEAD)\nby [15], which contained more than 1M seismic waveforms.\nThe same group used a subset of this dataset in their subsequent\nstudies, first using a Bayesian DNN to detect P-arrival times\nand localize epicenters [12], and then using a transformer and\nLSTM-included architecture for event and phase picking [14].\nSTEAD was further investigated by another group [21] that\nemployed a complex CNN to address earthquake localization\nand magnitude estimation problems. The significance of [21]\nlies in their distinct approach to predicting epicentral distances\nwithout relying on auxiliary data such as P/S phase arrival\ninformation, in contrast to [12] which utilizes this auxiliary\ndata in their method. However, [21]’s pretrained model remains\nundisclosed to the public. Although their code repository is\naccessible, the authors have provided limited information about\nthe subset of the STEAD set used in their experiments, which\nposes challenges for reproducing their results.\nThe historical evolution of deep learning unfolds a recurring\ntheme that starts with crafting an encoder [23]. From raw\nsignals, this encoder can extract deep and nontransparent\nrepresentations. The recent developments in the field show\nthat the encoder then becomes an off-the-shelf tool, ready\nfor deployment in subsequent downstream tasks once this\nmilestone is reached, such as ResNet [4] for vision. As far\nas seismology and earthquake engineering are concerned, an\nimportant question arises: “are there any off-the-shelf encoders\nFig. 2: Station locations extracted from the benchmarked subset\nof the STEAD utilized in [12]. The circled area indicates the\nlocal subset focused on a 300km radius centered around the\nCalifornia state.\nthat can extract transferable seismic deep representations”?\nTo address this question, we take a step back to re-implement\nthe most promising deep learning architectures from the\nliterature, namely the Residual Networks (ResNet) [4] and\nTemporal Convolutional Networks (TCN) [10] and conduct\nexperiments on STEAD to catch the state-of-the-art results\nin this domain. We then apply an ablation study to find the\neffects of different hyperparameters and most importantly, the\neffects of auxiliary information, such as the primary/secondary\n(P/S) phase signal arrival times (P/S phase information from\nhereafter). [9]. Previous studies that attempted to localize\nepicenters from single station ground motion data [12], [13]\nused P/S phase information with ground motion records.\nAlternatively, some deep-learning studies use multiple station\ninformation [8] [28] [24] [29] [2] when processing the ground\nmotion records for different down-stream tasks. In this paper,\nwe conduct a set of experiments to observe “how deeply” we\ncould learn from only ground motion records without using\nauxiliary information such as P/S phase arrival times.\nWe hypothesize that deep learning methods may fit well\ninto this highly correlated information, potentially displaying\na notable dependency on it when predicting the epicentral\ndistance. Consequently, it raises the question of whether we\ncan effectively deep-learn from accelerometer records. In\norder to minimize the effect of auxiliary information such\nas phase arrival times or distribution of stations within a\nnetwork, we focus on the problem of epicenter localization\nusing accelerometer records obtained from a single station.\nWe train our models on the STEAD dataset, with different\nhyperparameters to find the best-performing combination. To\nenhance clarity and reproducibility, in addition to our code\nrepository, we also make our entire experiment history available\nthrough an online public project framework.\nThe organization of the paper is as follows: our approach is\ndetailed in the methodology section with details of the dataset\nwe have used, and the utilized deep learning models. Then, we\nproceed with the experimental setup, how we apply the ablation\nstudy, and our experiment tracking capabilities. We also provide\na dashboard for the experiments conducted in this work as well\nas summary reports via a tracking tool. Following this step, we\npresent the best-performing combinations of hyperparameters\nand models, followed by a summary of the results from\n3\nFig. 3: Overall system architecture with optional P-S channel in addition the 3-channel earthquake waveform, variable size\ndense layers, ResNet or TCN for the encoder.\nour experiments. We then proceed with a discussion of the\ncorrelation between the P-S arrival times and the distance of the\nearthquake epicenters from the seismic stations. Furthermore,\nwe present additional visualizations illustrating the outputs\nof our epicenter prediction model, alongside training plots\nshowcasing the best models identified through our rigorous\nhyperparameter search. We conclude the paper with a summary\nof our findings.\nII. METHODOLOGY\nA. Dataset\nSTanford EArthquake Dataset (STEAD): A Global Data\nSet of Seismic Signals for AI [15] contains 60 seconds long\nseismic activities associated with roughly 450,000 earthquakes.\nThe dataset contains roughly 1 million events between January\n1984 and August 2018. The records have 6000 samples for each\nevent corresponding to 100 Hz recording frequency. STEAD\ndataset is the main dataset that forms the baseline for this\nwork. A sample that represents an event of an earthquake is\nprovided in Figure 1. It consists of ground motion records in\nNorth-South, East-West, and Up-Down dimensions, as well\nas labels for the Primary (P) and Secondary (S) signal arrival\ntime.\nIn the original dataset, there are 2613 stations; however, in\nthe studies associated with the dataset [12], authors filter out\nwaveforms that are generated by earthquakes further than 110\nkm or signal-to-noise (SNR) ratio of 25dB and lower. They\nhave also omitted the stations where north-south and east-\nwest components are not aligned to their correct geographic\norientations. With these filters, 147,195 records can be used\nfor epicenter distance estimation from single-station records.\nWe kept the same filtering approach as in [12] and utilized the\naforementioned subset with 743 stations that record earthquakes\nall over the globe, as depicted in Figure 2. Utilizing this\nSTEAD subset as our foundation, which we refer to as the\n“global” subset in this paper, we further refine our analysis\nby establishing a more localized subset focused on a 300km\nradius centered around California, which we term the “local”\nsubset (depicted as a circle in Figure 2). We conduct distinct\nexperiments on these subsets to discern differences in the\ndepth of features extracted from each. We chose to utilize the\nidentical training set (80%) and test set (20%) as described in\n[12]. The rationale behind this decision lies in the fact that [12]\nrepresents one of the few large-scale deep learning experiments\nthat rely solely on single station records without incorporating\nnetwork topology information. By adopting the same subset of\nSTEAD, we aim to conduct an ablation study focusing on a\ncontrolled auxiliary factor, specifically the phase information.\nB. Deep Learning Models\nIn this study, we benchmark two fundamental CNN architec-\ntures. The first one is the ResNet [4], which is the most widely\nused CNN encoder architecture that proved to be effective on\nvarious data and several down-stream tasks in the literature.\nResNet architecture enables deeper networks to be trained\nfor more complex signals that require deeper understanding\nvia residual connections. These residual connections enable\nthe flow of gradients throughout the network, effectively\neliminating the problematic deep learning phenomena of\nvanishing gradients, which disables deeper networks from being\ntrained. We also experiment on the Temporal Convolutional\nNetwork (TCN) [10], which employs a convolutional model\nin a sequential (time-series) manner. Convolutional networks\nare known for their efficient performance on data with spatial\ninformation, such as images where pixels in close proximity\nare highly correlated. Similarly, in time-series signals, such as\nearthquake waveforms, the value of the waveform at time ”t” is\ncorrelated with the neighboring time steps. Furthermore, TCN\nhas been used in experiments in [12], which address the same\nproblem definition as ours. The general model architectures\nare depicted in Figure 3.\nIII. EXPERIMENTS\nWe utilized exhaustive hyperparameter search experiments\nto identify the best-performing deep learning scenario, both\nwith and without the inclusion the P/S phase information. For\nthis purpose, we have used a grid search method with two\ndifferent datasets, three different learning rates, two different\ngamma values, and three different architectural parameters on\ntwo different deep learning models, resulting in at least 144\ndistinct experiments. For distance prediction, mean absolute\nerror (L1 loss) is chosen and directly calculated using the\nhaversine distance in kilometers between the epicenter and the\nrecording station.\n4\nA. Ablation\nThe ablation study of this work aims to evaluate the\nimportance of the P/S phase arrival time information, which\nis highly correlated with the distance of the epicenter from\nthe station. This investigation aims to determine whether deep\nlearning models can learn from only the three-channel sensor\nsignals or rely heavily on the P-S arrival time difference to\nextract the distance. We posit that rather than learning from only\nground motion records, the models may predominantly rely\non the highly correlated P-S arrival times for the extraction of\nrelative epicentral distances. To achieve this objective, our\nablation study incorporates a fundamental distinction: the\ninclusion of the P/S phase information. We created and trained\ntwo identical setups, differing solely in the presence or absence\nof the P/S signal information.\nB. Experiment Tracking\nThe experiments conducted in this work were monitored\nthrough Weights and Biases (WANDB) framework. The exper-\niments can be followed through a publicly available and online\n[31]. The report includes details such as the total runtime of\nexperiments, parameter importance concerning the network\nmodel (TCN-ResNet), learning rate, sizes of fully connected\nlayers, gamma values, and most notably, the presence of the\nP/S phase information as input.\nC. Resources and Experimental Costs\nThe reproducible experiments in this work takes 360 Hours\nto complete on an A100 GPU with 80 GB of VRAM, 16 cores\nof AMD 7742 server CPUs with 200 GB RAM. With a rated\npeak power of A100 of 400Watts and 225 Watts for the average\nCPU, our experiments had one-fourth CPU cores resulting in\n55 Watts of average CPU usage and mean GPU power usage\nstatistics recorded as 225 Watts, resulting in 280 Watts for only\nGPU and CPU power consumption. Other components such\nas RAM and SSD or hard drives would cost roughly 20 Watts\nfor our experiments, resulting in a total of 300 Watts. With the\ntotal amount of training of 268 hours for TCN experiments\nand 92 hours for ResNet, the grand total power consumption\nof our work was 108 KWh. We believe that our experiments,\nmodels, and approach are sustainable and have minimal impact\non the environment due to the compact nature of our deep\nlearning models and optimized training structure. The best-\nperforming model (TCN) we have trained throughout this work\ntakes about 300KB of memory and trains in 100 minutes for\nthe local subset and 400 minutes for the global dataset.\nIV. RESULTS\nTo structure our findings effectively, we have categorized\nour experimental results into two primary subgroups: ResNet\nand TCN experiments. In the following, we first provide a\ncomprehensive summary of each experiment and proceed to\nshowcase our prediction charts and learning plots from the\nexperiments conducted both with and without the inclusion of\nthe P/S phase information as input. Moreover, we explore the\ncorrelation between P/S phase arrival differences and epicentral\ndistances to enrich our analyses.\nFig. 4: Test set average loss values for different hyperpa-\nrameters, with and without the inclusion of the P/S phase\ninformation, on the ResNet model using the local subset.\nFig. 5: Test set average loss values for different hyperpa-\nrameters, with and without the inclusion of the P/S phase\ninformation, on the ResNet model using the global subset.\nFig. 6: Test set average loss values for different hyperpa-\nrameters, with and without the inclusion of the P/S phase\ninformation, on the TCN model using the local subset.\nA. Experiments with the ResNet Architecture\nExperimental results for the ResNet model trained with the\nlocal subset centered around California, with and without using\nP/S phase information, are given in Table I. The best-performing\nhyperparameter combination is shown in bold fonts and has\nan L1 Loss score of 13.33 km without phase information and\n4.47 km with it. The same results are depicted in Figure 4.\nThe results for the ResNet model trained with, this time,\nthe global dataset with and without the inclusion of the P/S\nphase information as input are given in Table II. Again, the\nbest-performing hyperparameter combination is shown in bold\nfont and has L1 Loss score of 14.82 km without the phase\n5\nFig. 7: Test set average loss values for different hyperpa-\nrameters, with and without the inclusion of the P/S phase\ninformation, on the TCN model using the global subset.\nResNet Local\nSize\nγ\nlr\nNo PS\nPS\n256\n0.5\n0.001\n51.74\n16.55\n256\n0.5\n0.0001\n49.21\n15.45\n256\n0.5\n0.00001\n48.81\n13.17\n256\n0.9\n0.001\n47.08\n11.55\n256\n0.9\n0.0001\n47.06\n9.79\n256\n0.9\n0.00001\n24.93\n9.51\n128\n0.5\n0.001\n23.57\n8.05\n128\n0.5\n0.0001\n22.28\n7.49\n128\n0.5\n0.00001\n19.9\n7.1\n128\n0.9\n0.001\n19.86\n6.85\n128\n0.9\n0.0001\n19.78\n6.51\n128\n0.9\n0.00001\n19.58\n6.28\n64\n0.5\n0.001\n19.35\n5.97\n64\n0.5\n0.0001\n19.06\n5.62\n64\n0.5\n0.00001\n18.44\n5.46\n64\n0.9\n0.001\n18.06\n4.64\n64\n0.9\n0.0001\n15.99\n4.51\n64\n0.9\n0.00001\n13.33\n4.47\nBest:\n4.47\nµ ± σ :\n27.67 ± 13.74\n8.28 ± 3.69\nTABLE I: Experimental results for the Resnet model trained\nwith and without P/S phase information on the local subset\ninformation and 4.31 km with it. The results of the experiments\nare also depicted in Figure 5.\nResNet Global\nSize\nγ\nlr\nNo PS\nPS\n256\n0.5\n0.001\n31.23\n58.99\n256\n0.5\n0.0001\n27.49\n28.56\n256\n0.5\n0.00001\n26\n19.41\n256\n0.9\n0.001\n25.17\n17.36\n256\n0.9\n0.0001\n24.32\n14.18\n256\n0.9\n0.00001\n23.49\n14.09\n128\n0.5\n0.001\n21.13\n11.15\n128\n0.5\n0.0001\n20.96\n8.69\n128\n0.5\n0.00001\n20.92\n8.49\n128\n0.9\n0.001\n20.38\n7.94\n128\n0.9\n0.0001\n20.28\n6.73\n128\n0.9\n0.00001\n20.01\n6.7\n64\n0.5\n0.001\n19.21\n5.96\n64\n0.5\n0.0001\n18.92\n5.75\n64\n0.5\n0.00001\n18.23\n5.51\n64\n0.9\n0.001\n18.19\n5.34\n64\n0.9\n0.0001\n17.67\n5.11\n64\n0.9\n0.00001\n14.82\n4.31\nBest:\n4.31\nµ ± σ :\n21.58 ± 4.02\n13.02 ± 13.11\nTABLE II: Experimental results for the Resnet model trained\nwith and without P/S phase information on the global dataset\nThe experiments in this section agree with our hypothesis\nthat the inclusion of the P-S phase arrival time information as an\nadditional input channel shows a direct effect on the capability\nof the model on predicting the epicentral distance from single\nstation recordings. With the inclusion of the auxiliary P/S\nphase information, we obtain the best model L1 Loss scores\nof 4.47 km and 4.31 km, for local and global sets, respectively.\nThe same respective results obtained without using P/S phase\ninformation are 13.33 km and 14.82 km, which are significantly\nhigher in all iterations.\nB. Experiments with the TCN Architecture\nThe TCN model trained with the local dataset centered\naround California, with and without using P-S phase infor-\nmation, are provided in Table III. Again, the best-performing\nhyperparameter combination is shown in bold fonts and has\nL1 Loss score of 7 km for without the P/S phase information\nand 1.74 km with its inclusion as input. The same results are\ndepicted in Figure 6.\nTCN Local\nSize\nγ\nlr\nNo PS\nPS\n256\n0.5\n0.001\n15.71\n1.74\n256\n0.5\n0.0001\n13.02\n1.88\n256\n0.5\n0.00001\n13.03\n1.98\n256\n0.9\n0.001\n11.93\n1.99\n256\n0.9\n0.0001\n12.4\n2.01\n256\n0.9\n0.00001\n7\n2.09\n128\n0.5\n0.001\n14.96\n2.16\n128\n0.5\n0.0001\n15.72\n2.17\n128\n0.5\n0.00001\n15.19\n2.17\n128\n0.9\n0.001\n11.68\n2.2\n128\n0.9\n0.0001\n15.4\n2.29\n128\n0.9\n0.00001\n15.74\n2.31\n64\n0.5\n0.001\n16.65\n3.6\n64\n0.5\n0.0001\n15.42\n3.79\n64\n0.5\n0.00001\n8.61\n3.8\n64\n0.9\n0.001\n19.58\n3.86\n64\n0.9\n0.0001\n8.66\n3.87\n64\n0.9\n0.00001\n12.76\n3.94\nBest:\n1.74\nµ ± σ :\n13.53 ± 3.18\n2.66 ± 0.85\nTABLE III: Experimental results for TCN model trained with\nand without P/S phase information on the local subset\nFinally, the TCN model trained on the global STEAD dataset,\nwith and without using P-S phase information, are given\nin Table IV. Consistent with our previous finding, the best-\nperforming hyperparameter combinations are shown in bold\nfonts and have an L1 Loss score of 3.14 km without P/S\ninformation as the input, and 2.64 km with its inclusion. The\nsame results are depicted in Figure 7.\nThe experiments in this section again agree with our\nhypothesis that P/S phase information has a distinct effect on\nthe performance of deep learning models for epicenter distance\nprediction from single station recordings. For the TCN models,\nwe have achieved the best L1 Loss scores of 1.74 km and 2.64\nkm, for local and global sets, respectively. We observe that the\ninclusion of the P/S phase information significantly drops the\nloss score by up to four times when compared to 7 km (local\nset) and 3.14 km (global set) L1 Loss scores obtained without\nthe P/S phase information.\n6\nTCN Global\nSize\nγ\nlr\nNo PS\nPS\n256\n0.5\n0.001\n10.8\n3.51\n256\n0.5\n0.0001\n10.18\n3.48\n256\n0.5\n0.00001\n5.35\n3.45\n256\n0.9\n0.001\n4.16\n3.45\n256\n0.9\n0.0001\n3.02\n3.44\n256\n0.9\n0.00001\n2.88\n3.43\n128\n0.5\n0.001\n7.73\n2.96\n128\n0.5\n0.0001\n6.8\n2.94\n128\n0.5\n0.00001\n5.52\n2.93\n128\n0.9\n0.001\n5.12\n2.91\n128\n0.9\n0.0001\n3.21\n2.9\n128\n0.9\n0.00001\n3.16\n2.88\n64\n0.5\n0.001\n12.56\n2.86\n64\n0.5\n0.0001\n9.67\n2.84\n64\n0.5\n0.00001\n9.32\n2.81\n64\n0.9\n0.001\n8.55\n2.74\n64\n0.9\n0.0001\n3.67\n2.7\n64\n0.9\n0.00001\n3.14\n2.64\nBest:\n2.64\nµ ± σ :\n6.38 ± 3.15\n3.05 ± 0.31\nTABLE IV: Experimental results for TCN model trained with\nand without P/S phase information on the global dataset.\nFig. 8: Epicentral Distances (km) vs P/S phase arrival differ-\nences (sec.). A clear correlation between P/S phase information\nand the epicentral distances can be observed.\nC. Experimentation Summary\nOverall, we used two deep learning models based on two\nfundamental convolutional architectures, namely RNN and\nTCN, both global and local subsets of STEAD. P/S phase\ninformation was included as an additional (i.e. 4th) input\nchannel for ablation experiments. The results demonstrate a\nstrong correlation between P/S phase information and the\nepicentral distances. Based on the model and dataset, the\ntraining time increases by about 6%, and the losses improve\nbetween 114% and 343% when P/S information is included\nas an input channel. A summary of all results are provided\nin Table V. In Figure 10b, where the best-performing model\nobtained without the incorporation of P/S phase information is\ndepicted, we observe a partial diagonal scatter. Because this\nexperiment is carried out on the global set with the better\nperforming model, TCN, this underscores the significance of\nhaving access to large-scale data for the effectiveness of deep\nlearning methodologies.\nSignal\nModel\nData\nTrain\nVal\nTest\nRuntime\nP-S\nTCN\nLocal\n1.85\n1.65\n1.74\n85\nP-S\nTCN\nGlobal\n2.83\n2.78\n2.64\n344\nP-S\nResNet\nLocal\n1.42\n2.16\n4.47\n31\nP-S\nResNet\nGlobal\n2.54\n4.03\n4.31\n128\nNo P-S\nTCN\nLocal\n5.88\n5.45\n7.00\n82\nNo P-S\nTCN\nGlobal\n3.13\n3.05\n3.02\n338\nNo P-S\nResNet\nLocal\n5.02\n10.97\n13.33\n29\nNo P-S\nResNet\nGlobal\n5.10\n12.96\n14.82\n118\nTABLE V: Summary of the experiments with best results\nin each category. Train and Validation and Test performance\nmetrics are in L1 Loss, the runtime is in minutes.\nD. Correlation between the P/S Phase Information and the\nEpicentral Distance\nAs stated earlier, the epicentral distance is typically estimated\nby measuring the time difference between the arrival of P\nwaves and S waves at a station. The greater the time difference\nbetween the waves, the farther the epicenter is from that station.\nThe effects of the inclusion the P/S phase information on the\nperformance metrics for different models, subsets and various\nhyperparameters can be observed in Table V.We utilize both\nPearson’s linear correlation and Spearman’s rank correlation\ntechniques to compute the correlation between epicentral\ndistances and P-to-S phase intervals, yielding coefficients of\n0.956 and 0.926, respectively. The epicentral distance and P-to-\nS phase intervals for each individual event of the global set are\nvisualized in Figure 8, which indicates a linear relationship.\nE. Distance Prediction Capability and Training Plots\nThe effects of different hyperparameters, especially the\ninclusion of P/S phase information, is visualized in Figure 9, for\nthe best-performing models in each category. The prediction\nvs. ground truth graphs show the training (red), validation\n(green) and test (test) results for each event, with mostly a\ndiagonal-like scatter. Compared to the experiments without P/S\ninformation inputs, as shown in Figure 10, the predictions are\nobservable better for a majority of events. Learning curves for\nthe corresponding experiments are given for the experiments\nwith and without the P/S phase information, in Figures 11\nand 12, respectively. The training plots for deep learning\nmodels highlight an important difference between the presence\nand absence of P-S signal for the aforementioned models.\nThe models trained without P-S signal tend to have higher\ndiscrepancy of the epicentral distance estimation performance\nfor validation and training sets. We conclude that the presence\nof P-S signal prevent the overfitting of deep learning models\nand enable more robust feature extraction, resulting in better\ngeneralizability.\n7\n(a) TCN, Local, L1 Loss 2.66 ±\n0.85 km\n(b) TCN, Global, L1 Loss 3.05 ±\n0.31 km\n(c) ResNet, Local, L1 Loss 8.28±\n3.69 km\n(d) ResNet, Global, L1 Loss\n13.02 ± 13.11 km\nFig. 9: Overall summary of the models trained with the P/S phase information in the input. The best model has an L1 Loss\nscore of 2.03km (Train: Star, Test: Triangle, Validation: Circle)\n(a) TCN, Local, L1 Loss 13.53 ±\n3.18 km\n(b) TCN, Global, L1 Loss 6.38 ±\n3.15 km\n(c)\nResNet,\nLocal,\nL1\nLoss\n27.67 ± 13.74 km\n(d) ResNet, Global, L1 Loss\n21.58 ± 4.02 km\nFig. 10: Overall summary of the models trained without the P/S phase information in the input. The best model has an L1 Loss\nscore of 3.02km (Train: Star, Test: Triangle, Validation: Circle).\n(a) TCN, Local, L1 Loss 1.74\n(b) TCN, Global, L1 Loss 2.64\n(c) ResNet, Local, L1 Loss 4.47 (d) ResNet, Global, L1 Loss 4.31\nFig. 11: Overall learning curves of the models trained with P/S phase information. Best model Loss score: 1.74.\n(a) TCN, Local, L1 Loss 6.91\n(b) TCN, Global, L1 Loss 3.02 (c) ResNet, Local, L1 Loss 13.33 (d) ResNet, Global, L1 Loss 14.82\nFig. 12: Overall learning curves of the models trained without the P/S phase information. Best model Loss score: 3.02 km.\n8\nV. CONCLUSION\nIn this study, we conduct a hyperparameter search on two\ndeep learning models ,TCN and ResNet, to explore their\nefficacy in learning from ground motion records, while also\ninvestigating the influence of auxiliary information on model\nperformance. We found that by incorporating arrival times of\nPrimary (P) and Secondary (S) earthquake waves, known as\nthe P/S phases, models demonstrated improved performance.\nFurthermore, our analyses reveal a strong correlation between\nthe epicentral distances and P/S phase information, supported\nby Pearson’s and Spearman’s correlation coefficients of 95.6%\nand 92.6%, respectively. Our findings indicate a challenge in\ndeep learning from accelerometer signals. The models tend to\nrely heavily on the highly correlated P/S phase information.\nResNet models tend to have higher oscillations in the validation\nscores, TCN models usually have validation scores on par with\ntraining scores and with much less oscillations. This oscillating\nbehaviour in training plots, is also evident with the increased\nstandard deviation for models trained without P/S phase\ninformation or for ResNet models in general. Thanks to our\nextensive model training approach, we can conclude that TCN\nmodels are better suited for this task than ResNet models, and\ninclusion of P/S phase information improves training capability\nof models, decreasing the overfitting behaviour of deep learning\nmodels, and improves the overall model performance, achieving\na better performance and generalizability.\nTo the best of our knowledge, there is a notable absence\nof deep learning studies exploring the impact of auxiliary\ninformation on model performance in seismic data analysis.\nIn our study, we investigated the influence of P/S phase\ninformation. However, auxiliary information could extend to\nfactors such as the distribution of seismic stations within a\nnetwork when multiple station locations are incorporated into\nthe network, which is why we work on records from single\nstations. While deep learning in seismology and earthquake\nengineering methodologies show promising results, our study\nsuggests that models may struggle to capture the nuanced\ncharacteristics of ground motion recordings.\nOur study underscores the need for future research to explore\nalternative architectures and experimental designs tailored to\nlocalized seismic data. The disparity in performance between\nglobal and localized subsets suggests avenues for further\ninvestigation, potentially leveraging datasets such as local and\ndense ground motion recordings. By refining our methodologies,\nwe aim to advance our understanding of seismic phenomena\nand enhance the efficacy of earthquake epicentral distance\nestimation techniques. Ultimately, our goal is to develop models\ncapable of extracting high-level features from ground motion\nrecords, thus contributing to the advancement of AI-based\nseismic data analysis.\nACKNOWLEDGMENTS\nThis work is supported by The Scientific and Technological\nResearch Council of Turkey (T ¨UB˙ITAK) as a part of the\nongoing T ¨UB˙ITAK 1001 funded project, Project No.121M732,\ntitled ”Deep Learning and Machine Learning Based Dynamic\nSoil and Earthquake Parameter Estimation Using Strong\nGround Motion Station Records”.\nThe numerical calculations reported in this paper were fully\nperformed at TUBITAK ULAKBIM, High Performance and\nGrid Computing Center (TRUBA).\nThe authors would like to thank Dr. S. Mostafa Mousavi,\nwho is the first author of [12] and a contributor to the STEAD,\nfor sharing the details of their experimental setup, so that an\naccurate benchmark could be conducted.\nCOMPUTER CODE AVAILABILITY\nThe computer code used for the experiments conducted\nin this work is named as MAGE after Middle East Technical\nUniversity (METU), Graduate School of Informatics Applied In-\ntelligence Research Laboratory (AIRLab), Geoinformatics and\nArtificial Intelligence Applications Group (GAIA), Epicenter\ndistance estimation with deep learning. All authors have\ncontributed to the code in software development tasks or\nsupervised the processes. Corresponding Author hosts the code\npublicly in GitHub repository [30] and for further inquiries can\nbe contacted. The experiments can be reproduced faithfully to\nthis work by installing the required libraries and packages for\nPython, as detailed in requirements file in the repository. The\nexperiments reported in this work is also available online at\nWANDB project report [31]. The size of the code, excluding\nthe required open-source libraries, is less than 200 KB. For\nproof of concept, consumer grade resources, Intel i5-13600KF\nCPU, NVIDIA 3070 GPU with 8GB of VRAM and 32 GB\nRAM were employed and the experiments were conducted\nwith NVIDIA A100 GPUs with 80 GB of VRAM, AMD 7742\nserver CPUs with 200 GB RAM.\nREFERENCES\n[1] Gerardo Alguacil and Jens Havskov. Seismic Accelerometers, pages\n2504–2519. Springer Berlin Heidelberg, Berlin, Heidelberg, 2015.\n[2] Muhammad Atif Bilal, Yanju Ji, Yongzhi Wang, Muhammad Pervez\nAkhter, and Muhammad Yaqub. Early earthquake detection using batch\nnormalization graph convolutional neural network (bngcnn). Applied\nSciences, 12(15), 2022.\n[3] Stefan Bloemheuvel, Jurgen van den Hoogen, Dario Jozinovi´c, Alberto\nMichelini, and Martin Atzmueller. Graph neural networks for multivariate\ntime series regression with application to seismic data. International\nJournal of Data Science and Analytics, Aug 2022.\n[4] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep\nResidual Learning for Image Recognition.\nIn Proceedings of 2016\nIEEE Conference on Computer Vision and Pattern Recognition, CVPR\n’16, pages 770–778. IEEE, June 2016.\n[5] Seokgyeong Hong, Huyen-Tram Nguyen, Jongwon Jung, and Jaehun\nAhn. Seismic ground response estimation based on convolutional neural\nnetworks (cnn). Applied Sciences, 11(22), 2021.\n[6] Dario Jozinovi´c, Anthony Lomax, Ivan ˇStajduhar, and Alberto Michelini.\nRapid prediction of earthquake ground shaking intensity using raw\nwaveform data and a convolutional neural network. Geophysical Journal\nInternational, 222(2):1379–1389, 05 2020.\n[7] Gwantae Kim, Bonhwa Ku, Jae-Kwang Ahn, and Hanseok Ko. Graph\nconvolution networks for seismic events classification using raw wave-\nform data from multiple stations. IEEE Geoscience and Remote Sensing\nLetters, 19:1–5, 2022.\n[8] Marius Kriegerowski, Gesa M. Petersen, Hannes Vasyura-Bathke, and\nMatthias Ohrnberger.\nA Deep Convolutional Neural Network for\nLocalization of Clustered Earthquakes Based on Multistation Full\nWaveforms. Seismological Research Letters, 90(2A):510–516, 12 2018.\n[9] H. Serdar Kuyuk and Ohno Susumu.\nReal-time classification of\nearthquake using deep learning. Procedia Computer Science, 140:298–\n305, 2018. Cyber Physical Systems and Deep Learning Chicago, Illinois\nNovember 5-7, 2018.\n9\n[10] Colin Lea, Rene Vidal, Austin Reiter, and Gregory D Hager. Temporal\nconvolutional networks: A unified approach to action segmentation. In\nComputer Vision–ECCV 2016 Workshops: Amsterdam, The Netherlands,\nOctober 8-10 and 15-16, 2016, Proceedings, Part III 14, pages 47–54.\nSpringer, 2016.\n[11] Anthony Lomax, Alberto Michelini, and Dario Jozinovi´c. An Inves-\ntigation of Rapid Earthquake Characterization Using Single-Station\nWaveforms and a Convolutional Neural Network. Seismological Research\nLetters, 90(2A):517–529, 02 2019.\n[12] S. Mostafa Mousavi and Gregory C. Beroza. Bayesian-deep-learning\nestimation of earthquake location from single-station observations. IEEE\nTransactions on Geoscience and Remote Sensing, 58(11):8211–8224,\n2020.\n[13] S. Mostafa Mousavi, Gregory C. Beroza, Tapan Mukerji, and Majid\nRasht-Behesht. Applications of deep neural networks in exploration\nseismology: A technical survey. GEOPHYSICS, 89(1):WA95–WA115,\n2024.\n[14] S. Mostafa Mousavi, William L. Ellsworth, Weiqiang Zhu, Lindsay Y.\nChuang, and Gregory C. Beroza. Earthquake transformer—an attentive\ndeep-learning model for simultaneous earthquake detection and phase\npicking. Nature Communications, 11(1):3952, Aug 2020.\n[15] S. Mostafa Mousavi, Yixiao Sheng, Weiqiang Zhu, and Gregory C.\nBeroza. Stanford earthquake dataset (stead): A global data set of seismic\nsignals for ai. IEEE Access, 7:179464–179476, 2019.\n[16] Jannes M¨unchmeyer, Dino Bindi, Ulf Leser, and Frederik Tilmann. The\ntransformer earthquake alerting model: a new versatile approach to\nearthquake early warning. Geophysical Journal International, 225(1):646–\n656, 12 2020.\n[17] Masaru Nakano and Daisuke Sugiyama. Discriminating seismic events\nusing 1d and 2d cnns: applications to volcanic and tectonic datasets.\nEarth, Planets and Space, 74(1):134, Sep 2022.\n[18] Esteban Pardo, Carmen Garfias, and Norberto Malpica. Seismic phase\npicking using convolutional networks. IEEE Transactions on Geoscience\nand Remote Sensing, 57(9):7086–7092, 2019.\n[19] Thibaut Perol, Micha¨el Gharbi, and Marine Denolle. Convolutional\nneural network for earthquake detection and location. Science Advances,\n4(2):e1700578, 2018.\n[20] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel\nGoh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin,\nJack Clark, et al. Learning transferable visual models from natural\nlanguage supervision. In International conference on machine learning,\npages 8748–8763. PMLR, 2021.\n[21] Nicolae-C˘at˘alin Ristea and Anamaria Radoi. Complex neural networks\nfor estimating epicentral distance, depth, and magnitude of seismic waves.\nIEEE Geoscience and Remote Sensing Letters, 19:1–5, 2022.\n[22] Omar M. Saad, Ali G. Hafez, and M. Sami Soliman. Deep learning\napproach for earthquake parameters classification in earthquake early\nwarning system.\nIEEE Geoscience and Remote Sensing Letters,\n18(7):1293–1297, 2021.\n[23] Iqbal H Sarker. Deep learning: a comprehensive overview on techniques,\ntaxonomy, applications and research directions. SN Computer Science,\n2(6):420, 2021.\n[24] M. P. A. van den Ende and J.-P. Ampuero.\nAutomated seismic\nsource characterization using deep graph neural networks. Geophysical\nResearch Letters, 47(17):e2020GL088690, 2020.\ne2020GL088690\n10.1029/2020GL088690.\n[25] Keisuke Yano, Takahiro Shiina, Sumito Kurata, Aitaro Kato, Fumiyasu\nKomaki, Shin’ichi Sakai, and Naoshi Hirata.\nGraph-partitioning\nbased convolutional neural network for earthquake detection using\na seismic array.\nJournal of Geophysical Research: Solid Earth,\n126(5):e2020JB020269, 2021. e2020JB020269 2020JB020269.\n[26] B. Yilmaz, M. Turkmen, S. Meral, E. Akag¨und¨uz, and S. Tileylioglu.\nDeep learning-based average shear wave velocity prediction using\naccelerometer records. In Proceedings of 2024 World Conference on\nEarthquake Engineering, July 2024.\n[27] N´estor Becerra Yoma, Jorge Wuth, Andr´es Pinto, Nicol´as de Celis, Jorge\nCelis, Fernando Huenupan, and Ivo Janos Fustos-Toribio. End-to-end\nlstm based estimation of volcano event epicenter localization. Journal\nof Volcanology and Geothermal Research, 429:107615, 2022.\n[28] Xiong Zhang, Jie Zhang, Congcong Yuan, Sen Liu, Zhibo Chen, and\nWeiping Li. Locating induced earthquakes with a network of seismic\nstations in oklahoma via a deep learning method. Scientific Reports,\n10(1):1941, Feb 2020.\n[29] Xiong Zhang, Miao Zhang, and Xiao Tian.\nReal-time earthquake\nearly warning with deep learning: Application to the 2016 m 6.0\ncentral apennines, italy earthquake.\nGeophysical Research Letters,\n48(5):2020GL089394, 2021. 2020GL089394 2020GL089394.\n[30] Umit Mert C¸ a˘glar. Github codebase for this work. https://github.com/\ncaglarmert/mage, 2024. Accessed: 2024-03-11.\n[31] Umit Mert C¸ a˘glar. Weights and biases (wandb) project report. https:\n//api.wandb.ai/links/caglarmert/rdvjvsyu, 2024. Accessed: 2024-03-11.\n",
  "categories": [
    "eess.SP",
    "cs.CV",
    "cs.LG"
  ],
  "published": "2024-03-12",
  "updated": "2024-03-12"
}