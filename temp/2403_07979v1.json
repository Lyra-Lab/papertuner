{
  "id": "http://arxiv.org/abs/2403.07979v1",
  "title": "Do Agents Dream of Electric Sheep?: Improving Generalization in Reinforcement Learning through Generative Learning",
  "authors": [
    "Giorgio Franceschelli",
    "Mirco Musolesi"
  ],
  "abstract": "The Overfitted Brain hypothesis suggests dreams happen to allow\ngeneralization in the human brain. Here, we ask if the same is true for\nreinforcement learning agents as well. Given limited experience in a real\nenvironment, we use imagination-based reinforcement learning to train a policy\non dream-like episodes, where non-imaginative, predicted trajectories are\nmodified through generative augmentations. Experiments on four ProcGen\nenvironments show that, compared to classic imagination and offline training on\ncollected experience, our method can reach a higher level of generalization\nwhen dealing with sparsely rewarded environments.",
  "text": "Do Agents Dream of Electric Sheep?:\nImproving\nGeneralization in Reinforcement Learning through\nGenerative Learning\nGiorgio Franceschelli\ngiorgio.franceschelli@unibo.it\nDepartment of Computer Science and Engineering\nUniversity of Bologna, Italy\nMirco Musolesi\nm.musolesi@ucl.ac.uk\nDepartment of Computer Science\nUniversity College London, United Kingdom\nDepartment of Computer Science and Engineering\nUniversity of Bologna, Italy\nAbstract\nThe Overfitted Brain hypothesis (Hoel, 2021) suggests dreams happen to allow gen-\neralization in the human brain. Here, we ask if the same is true for reinforcement\nlearning agents as well. Given limited experience in a real environment, we use\nimagination-based reinforcement learning to train a policy on dream-like episodes,\nwhere non-imaginative, predicted trajectories are modified through generative aug-\nmentations. Experiments on four ProcGen environments show that, compared to\nclassic imagination and offline training on collected experience, our method can\nreach a higher level of generalization when dealing with sparsely rewarded environ-\nments.\n1\nIntroduction\nDeep Reinforcement Learning (RL) has emerged as a very effective mechanism for dealing with\ncomplex and intractable AI tasks of different nature. Model-free methods that essentially learn by\ntrial and error have solved challenging games (Mnih et al., 2015), performed simulated physics tasks\n(Lillicrap et al., 2016), and aligned large language models with human values (Ouyang et al., 2022).\nHowever, RL commonly requires an incredible amount of collected experience, especially compared\nto the one required by humans (Tsividis et al., 2017), limiting its applications to real-world tasks.\nModel-based RL (Sutton & Barto, 2017) constitutes a promising direction towards sample efficiency.\nLearning a world model capable of predicting the next states and rewards conditioned on actions\nallows the agent to plan (Schrittwieser et al., 2020) or build additional training trajectories (Ha &\nSchmidhuber, 2018). In particular, recent imagination-based methods (Hafner et al., 2020; 2021;\n2023; Micheli et al., 2023) have shown remarkable performance simply by learning from imagined\nepisodes within a learned latent space.\nSuch imagined trajectories are commonly mentioned as\ndreams. However, these dreams are nothing like human dreams, as they essentially try to mimic\nreality as best as possible.\nAccording to the Overfitted Brain hypothesis (Hoel, 2021), dreams\nhappen to allow generalization in the human brain. In particular, it is by providing hallucinatory\nand corrupted content (Hoel, 2019) that are far from the limited daily experiences (i.e., the training\nset) that dreaming helps prevent overfitting. We build on this intuition and ask: can human-like\n“dreams” help RL agents generalize better when dealing with limited experience?\nIn this paper, we explore whether this type of experience augmentation based on dream-like generated\ntrajectories helps generalization and, consequently, improves learning. In particular, we consider the\nsituation in which only a limited amount of real experience (analogously to “daylight activities”\nfor humans) is available, and we question whether building a world model upon it and leveraging\nit to generate dream-like experiences improves the agent’s generalization capabilities. To simulate\n1\narXiv:2403.07979v1  [cs.LG]  12 Mar 2024\nthe hallucinatory and corrupted nature of dreams, we propose to transform the classic imagined\ntrajectories with generative augmentations, i.e., through interpolation with random noise (Wang\net al., 2020), DeepDream (Mordvintsev et al., 2015), or critic’s return optimization (similar to class\nvisualization; Simonyan et al. (2014)). We evaluate them on four ProcGen environments (Cobbe\net al., 2020), a standard suite for generalization in RL (Kirk et al., 2023). Our experiments1 show that\nfor sparsely rewarded environments our method can reach higher levels of generalization compared\nwith classic imagination and offline training.\nThe main contributions of this paper can be summarized as follows:\n• We leverage existing world models (Section 3) learned from limited data to construct imag-\nined trajectories from randomly generated states.\n• We define three novel types of experience augmentation based on hallucination and corrup-\ntion of the trajectories to improve generalization, making them closer to human-like dreams\nin a sense (Section 4).\n• We evaluate the generalization capabilities of our methods against standard imagination and\noffline training over collected experience using ProcGen, showing how dream-like trajectories\ncan help generalize better (Section 5).\n2\nRelated Work\n2.1\nImagination-Based RL\nWorld models were first introduced by Dyna (Sutton, 1991) and then extensively studied in model-\nbased RL for planning (Chua et al., 2018; Gal et al., 2016; Hafner et al., 2019; Henaff et al., 2017;\nSchrittwieser et al., 2020). It has been shown that they can also help model-free agents by reducing\ntheir state-space dimensionality (Banijamali et al., 2018; Watter et al., 2015), by guiding their\ndecisions through the provision of additional information (Buesing et al., 2018; Racanière et al.,\n2017), and by constructing imagined trajectories to be used in place of real (expensive) experience.\nWhile it is possible to directly work on highly-dimensional observations (Kaiser et al., 2020), the\nmain line of research consists of learning a compact latent representation of the environment with a\nposterior model to encode current observation and a prior model to predict the encoded state. The\nprior model is then used to construct imagined trajectories and can be implemented with (Ha &\nSchmidhuber, 2018; Hafner et al., 2019; 2020) or without (Lee et al., 2020a; Zhang et al., 2019) a\nrecurrent layer that keeps track of the episode history. Igl et al. (2018) also train the world model on\nthe agent objective. Instead of working on states, Nagabandi et al. (2018) model the environment’s\ndynamics through the difference between consecutive states. Hafner et al. (2021; 2023) replace the\nclassic continuous latent space with discrete representations. Sekar et al. (2020) use an intrinsic\nreward based on ensemble disagreement to guide imagined exploration. Zhu et al. (2020) employ\nlatent overshooting to train the dynamics-agent pair together. Mu et al. (2021) construct imagined\ntrajectories not only from real states but also from derived states whose features are randomly\nmodified. Transformers (Vaswani et al., 2017) can be used to represent and learn the world model\nin place of the recurrent layer (Chen et al., 2022) or all predictive components (Micheli et al., 2023).\nIn general, all these methods only produce trajectories that adhere as close as possible to the real\nones, lacking the divergent aspect of dreaming that helps humans avoid overfitting.\n2.2\nGeneralization in RL\nDifferent techniques have been proposed to approach generalization in RL. A first strategy is to\nlearn an environment representation decoupled from the specific policy optimization (Jaderberg\net al., 2017; Stooke et al., 2021). Another is to adopt techniques used in supervised learning to avoid\noverfitting, e.g., dropout, batch normalization, and specific convolutional architectures (Cobbe et al.,\n1We plan to release the code soon.\n2\n2019; Farebrother et al., 2018; Igl et al., 2019). An alternative is to improve the agent’s architecture,\nthe training process, or the experience replay sampling technique (Jiang et al., 2021). For example,\nRaileanu & Fergus (2021) propose to train the value function with an auxiliary loss that encourages\nthe model to be invariant to task-irrelevant properties. Also, post-training distillation may help\nimprove generalization to new data (Lyle et al., 2022), as well as learning an embedding in which\nstates are close when their optimal policies are similar (Agarwal et al., 2021) or interpolating between\ncollected observations (Wang et al., 2020). Another strategy is to use data augmentation to increase\nthe quantity and variability of training data (Cobbe et al., 2019; Laskin et al., 2020; Lee et al.,\n2020b; Tobin et al., 2017; Yarats et al., 2021; Ye et al., 2020). The most appropriate augmentation\ntechnique can even be learned and not selected a priori (Raileanu et al., 2021). Finally, Ghosh\net al. (2021) propose to deal with the epistemic uncertainty introduced by generalization through an\nensemble-based technique, with multiple policies trained on different subsets of the distribution and\nthen combined. Our approach is conceptually close to data augmentation, but with augmentations\nbased on the learned world model itself, thus providing semantically richer transformations.\n3\nPreliminaries\n3.1\nModeling Latent Dynamics\nWorld models represent a compact and learned version of the environment capable of predicting\nimagined future trajectories (Sutton, 1991). When the inputs are high-dimensional observations ot\n(i.e., images), Dreamer (Hafner et al., 2020; 2021; 2023) represents the current state of the art due\nto its ability to learn compact latent states zt. In general, Dreamer world model consists of the\nfollowing components:\nRecurrent model:\nht = fθ(ht−1, zt−1, at−1)\nEncoder model:\nzt ∼qθ(zt|ht, ot)\nTransition predictor:\nˆzt ∼pθ(ˆzt|ht)\nReward predictor:\nˆrt ∼pθ(ˆrt|ht, zt)\nContinue predictor:\nˆct ∼pθ(ˆct|ht, zt)\nDecoder model:\nˆot ∼pθ(ˆot|ht, zt)\nThe deterministic recurrent state ht is predicted by a Gated Recurrent Unit (GRU) (Cho et al.,\n2014), while the encoder and decoder models use convolutional neural networks for visual observa-\ntions. Overall, the Recurrent State-Space Model (Hafner et al., 2019), an architecture that contains\nrecurrent, encoder, and transition components, learns to predict the next state only from the current\none and the action, while also allowing for correct reward, continuation bit, and image reconstruc-\ntions.\nWhile zt was originally parameterized through a multivariate normal distribution, more recent works\n(Hafner et al., 2021; 2023) consider a discrete latent state. In particular, they use a vector of C one-\nhot encoded categorical variables (i.e., a very sparse binary vector). Hafner et al. (2023) parameterize\nthis categorical distribution as a mixture of 1% uniform and 99% neural network output. Moreover,\ninstead of regressing the rewards via squared error, they propose a learning scheme based on two\ntransformations: first, the rewards are symlog-transformed (Webber, 2012); then, they are two-hot\nencoded, i.e., converted into a vector of K values where K −2 are 0, and the remaining, consecutive\ntwo are positive weights whose sum is 1. The K values correspond to equally spaced buckets, so\nthat by multiplying the vector with the bucket values we reconstruct the original reward. We adopt\nthis solution: this helps learning, especially in environments with very sparse rewards.\nOverall, given a sequence of inputs {o0:T −1, a0:T −1, r1:T , c1:T }, the world model is trained to minimize\nthe following loss:\n3\nL(θ) = Eqθ\n\" T\nX\nt=1\n(Lpred(θ) + β1Ldyn(θ) + β2Lrep(θ))\n#\n(1)\nwhere Lpred trains the decoder model via mean squared error loss, the reward predictor via cate-\ngorical cross-entropy loss, and the continue predictor via binary cross-entropy loss; while Ldyn and\nLrep consider the same Kullback-Leibler (KL) divergence between qθ(zt|ht, ot) and pθ(ˆzt|ht), but\nusing the stop-gradient operator on the former for the first loss and on the latter for the second loss.\nMoreover, free bits (Kingma et al., 2016) are employed to clip the KL divergence below the value of\n1 nat. Finally, β1 and β2 are scaling factors necessary to encourage learning an accurate prior over\nincreasing posterior entropy (Hafner et al., 2021).\n3.2\nLearning through Imagination\nIn general, leveraging the world model detailed in Section 3.1, a policy πϕ(at|st) can be learned by\nacting only in the latent space of imagination: given a compact latent state ˆsim\nt\n=\n\u0000him\nt , ˆzim\nt\n\u0001\n, the\nagent selects action at, returns it to the world model, and receives ˆrt+1, ˆct+1, and ˆsim\nt+1. Furthermore,\na critic vϕ(vt|st) is simultaneously learned to predict the state-value function vt. This process is\nrepeated until a fixed imagination horizon is reached and the policy can be learned from the imagined\nexperience as it would have done by acting in the real environment. The agent can be trained on\nthe collected trajectories either by direct reward optimization (leveraging the differentiability of the\ntrajectory construction and back-propagating through the reward model; Hafner et al. (2020)) or by\nusing a model-free policy gradient method, e.g., REINFORCE (Williams, 1992).\n4\nDream to Generalize\nBy starting from the models presented in Section 3, our method proposes first to learn a latent\nworld model from real experience; to augment the imagined trajectories to resemble human dreams\n(Section 4.1); and then to exploit such new trajectories to learn policies that are more keen to\ngeneralize (Section 4.2).\n4.1\nGenerating Human-Like Dreams\nGiven a trained world model, we can use it to construct imagined trajectories as detailed in Section\n3.2. Crucially, instead of starting each trajectory from a real collected state (as is commonly done\nin the literature), we start from randomly generated states ˆsim\n0\n=\n\u0000him\n0 , ˆzim\n0\n\u0001\nwith\nhinit ∼N(0, I) ,\nˆzinit = one-hot(u1:C) , uc ∼U(0, J −1) for c = 1...C,\nhim\n0\n= fθ(hinit, ˆzinit, ainit) ,\nˆzim\n0\n∼pθ\n\u0000him\n0\n\u0001\n,\n(2)\nwhere J is the number of classes each of the C categorical variables can assume, one-hot(·) transforms\na list of categorical variables into a vector of one-hot encoded vectors, and ainit is a zero vector.\nIn addition, to obtain more human-like dreams, we leverage the world model to propose three\nperturbation strategies (see Figure 1 for a summary of the process):\n• Random swing, i.e., interpolation between the current state ˆsim\nt\n=\n\u0000him\nt , ˆzim\nt\n\u0001\nand a\nrandom noise state (similar to Wang et al. (2020)). In particular, we perturb the hidden\nstate him\nt\nby adding a random vector hrand ∼N(0, I). Instead, our transformation over the\nlatent state ˆzim\nt\ncan be formalized as:\n4\nFigure 1: At imagination time, we start from a random latent state and then we only leverage\nthe predicting capabilities of our world model to obtain future latent states (the concatenation of\na discrete latent vector and a recurrent hidden state), rewards and termination bits given\nthe actions from the agent. To introduce a dream-like transformation, we modify the current\nlatent state with a small probability by doing one of three operations: interpolate it with random\nnoise; DeepDream its corresponding observation from the decoder by maximizing the activation\nof the encoder last convolution layer; optimize it to maximize the absolute value of critic output.\nˆzim\nt\n= one-hot\n\u0000λ · reverse-one-hot\n\u0000ˆzim\nt\n\u0001\n+ (1 −λ) · u1:C\n\u0001\n,\nλ ∼Bin(C, pswing) ,\nuc ∼U(0, J −1) for c = 1...C\n(3)\nwhere reverse-one-hot(·) inverts the one-hot encoding, i.e., recovers the list of categorical\nvariables, and pswing = 0.5 is the probability of making a swing.\nIn other words, each\ncategorical variable is changed into a randomly sampled class with probability pswing. This\nsimulates the corruption of dream content and the sudden visual changes we commonly\nexperience during REM sleep (Andrillon et al., 2015).\n• DeepDream, i.e., by iteratively adjusting the image reconstructed from the state to maxi-\nmize the firing of a model layer (Mordvintsev et al., 2015). Specifically, we consider the last\nconvolutional layer of the encoder, which should learn the building elements of real images.\nGiven qLC\nθ\n(·) as the activation of the last convolutional layer of dimension D, we transform\nthe hidden state him\nt\nand the latent state ˆzim\nt\nvia gradient ascent over the following objective:\ngdd = ∇him\nt\n,ˆzim\nt\nPD\ni=1 qLC\nθ\n\u0000pθ\n\u0000him\nt , ˆzim\nt\n\u0001\u0001\ni\nD\n.\n(4)\nThis simulates the hallucinatory nature of dreams.\n• Value diversification, i.e., by iteratively adjusting the state ˆsim\nt\nto maximize the squared\ndifference between the value of the critic prediction at iteration τ and iteration 0.\nWe\nperform a gradient ascent over the following objective:\n5\n(a) Original.\n(b) Reconstruction.\n(c) Random swing.\n(d) DeepDream.\n(e) Value div.\nFigure 2: An example of the three generative augmentations on a state from Plunder environment.\ngvd = ∇him\nt\n,ˆzim\nt\n\u0010\nvϕ\n\u0000him\nt , ˆzim\nt\n\u0001\n−vϕ(hinp\nt\n, ˆzinp\nt\n)\n\u00112\n,\n(5)\nwhere ˆsinp\nt\n= (hinp\nt\n, ˆzinp\nt\n) is the state before optimization. The squared difference is con-\nsidered to optimize for both positive and negative changes in the critic’s prediction. In\naddition, at each iteration, ˆzim\nt\nis transformed to keep it as a vector of one-hot categorical\nvariables. The value diversification transformation suddenly introduces or removes goals or\nobstacles, simulating the narrative content and the fact that dreams commonly resemble\ndaily aspects that are significant to us, especially threatening events (Revonsuo, 2000). In\nfact, simulating negative experiences might allow an agent to learn what to avoid in practice.\nFigure 2 reports a visual example of the three transformations. We alter each state ˆsim\nt\nwith a\nsmall probability ϵdream = 1\nH with H imagination horizon. In this way, each trajectory includes, on\naverage, one transformed state.\n4.2\nLearning by Day and Night\nOur method can be divided into two stages. During the first, our agent plays a limited number of real\nepisodes (the day experience), which are used to train both the world model and the agent in an E2C-\nlike setting (Watter et al., 2015), where the agent receives the encoding of the real observation by the\nworld model as the state. We then leverage the world model to generate additional dreamed episodes\n(the night experience), which are used to keep on training the agent. Algorithm 1 summarizes the\nentire learning process.\nThe latent world model is trained as detailed in Section 3.1.\nAs far as the agent is concerned,\nfollowing Hafner et al. (2023), we adopt an actor-critic architecture that works on the latent state\nst = (ht, zt). However, instead of using REINFORCE, we train it during both day and night stages\nthrough Proximal Policy Optimization (PPO; Schulman et al. (2017)), which we find helpful to\nobtain a more stable training. The overall loss is defined as follows:\nLt(ϕ) =\nT −1\nX\nt=0\nEπϕ,vϕ,pθ\n\u0002\n−LCLIP\nt\n(ϕ) + cvLV F\nt\n(ϕ) −ceH[πϕ](st)\n\u0003\n,\n(6)\nwhere\nLCLIP\nt\n(ϕ) = ˆEt\n\u0014\nmin\n\u0012 πϕ(at|st)\nπϕold(at|st)\nˆAt, clip\n\u0012 πϕ(at|st)\nπϕold(at|st), 1 −ϵ, 1 + ϵ\n\u0013\nˆAt\n\u0013\u0015\n(7)\nis the clipped surrogate objective that modifies the policy in the right direction while preventing\nlarge changes, and H[πϕ] is an entropy bonus scaled by ce coefficient. Instead, the value function\nis modeled as the reward (see Section 3.1): the critic network vϕ produces a softmax distribution\n6\nAlgorithm 1 Learning to generalize by day and by night\nRequire S number of seed episodes, Ed day epochs, En night epochs, Uw update steps per day\nepoch, Ua update steps per night epoch, Bw world batch size, Ba agent batch size, L sequence\nlength, H imagination horizon, Td steps in environment per day epoch.\nInitialize neural network parameters θ and ϕ randomly.\nInitialize dataset D with S random seed episodes.\no1 ←env.reset()\nfor day epoch ed = 1...Ed do\n▷Day experience\nfor update step u = 1...Uw do\nDraw Bw data sequences {(ot, at, rt+1, ct+1)}k+L\nt=k ∼D.\nUpdate θ through representation learning (Equation 1).\nend for\nfor day step t = 1...Td do\nCompute st = (ht, zt), ht = fθ(ht−1, zt−1, at−1), zt ∼qθ(ht, ot).\nCompute at ∼πϕ(at|st).\not+1, rt+1, ct+1 ←env.step(at).\nend for\nUpdate ϕ through PPO (Equation 6) using collected experience.\nAdd collected experience to dataset D ←D ∪\nn\n(ot, at, rt+1, ct+1)Td−1\nt=0\no\n.\nEvaluate πϕ on test_env.\nend for\nfor night epoch en = 1...En do\n▷Night experience\nSample Ba · Ua random states ˆsim\n0\naccording to Equation 2.\nDream trajectories\n\b\u0000ˆsim\nτ , aτ, ˆrτ+1, ˆcτ+1\n\u0001\tH−1\nτ=0 from each ˆsim\n0 .\nUpdate ϕ through PPO (Equation 6) using generated experience.\nEvaluate πϕ on test_env.\nend for\nacross bins, where each one represents a partition of the potential value range. Therefore, its loss\nfunction is defined as follows:\nLV F\nt\n(ϕ) = −sg\n\u0000two-hot\n\u0000symlog\n\u0000V target\nt\n\u0001\u0001\u0001T ln(vϕ(·|st)) ,\n(8)\nwhere sg(·) stops the gradient, and two-hot(·) and symlog(·) transform the target discounted return\nV target\nt\n= ˆAt + vϕ(st) into its two-hot encoded, symlog-transformed version. Finally, the advantage\nis estimated following Schulman et al. (2016):\nˆAt =\nT −t\nX\ni=0\n(γλ)i (rt+i+1 + γvϕ(st+i+1) −vϕ(st+i)) ,\n(9)\nand normalized according to the scheme proposed in Hafner et al. (2023), i.e., by dividing it by\nmax(1, P) with P as the exponentially decaying average of the range from their 5th to their 95th\nbatch percentile.\nThis improves exploration under sparse rewards.\nFinally, we also adopt two\nadditional mechanisms to improve the learning process: we penalize non-successful completion of\nthe tasks in sparsely rewarded environments, i.e., we associate a negative reward to a non-positive\ntermination state; and we prioritize sampling of non-zero rewarded sequences during training, which\nhelps learn a meaningful reward predictor.\n7\n5\nExperiments\nIn the following, we present experiments on the generalization capabilities of our proposed approach\nusing ProcGen (Cobbe et al., 2020), a simple yet rich set of environments for RL generalization\nevaluation.\n5.1\nSetup\nProcGen is a suite of 16 procedurally generated game-like environments. To benchmark the gener-\nalization capabilities of our approach, only a small subset of the distribution of levels (N = 200) is\nused to train the agent and the full distribution to test it. Due to resource constraints, our experi-\nments consider the ProcGen suite in easy mode; we limit the collected real experience to 1M steps,\nfar below the suggested 25M. We evaluate our method across four ProcGen environments, each\npresenting unique and challenging properties, namely Caveflyer (open-world navigation with sparse\nrewards), Chaser (grid-based game with highly dense rewards), CoinRun (left-to-right platformer\nwith highly sparse rewards), and Plunder (war game with dense rewards).\nAfter training the world model and the agent on the collected steps, we keep on training the agent\nusing dream-like trajectories from the fixed world model for another 1M steps. All the implementa-\ntion details of the proposed solution in terms of architecture and training setup are reported in the\nSupplementary Material.\n5.2\nBaselines\nWe compare three variants of our method (which we refer to as RndDreamer, DeepDreamer, and\nValDreamer since they use random swing, DeepDream, and value diversification respectively) with\ntwo baselines: Dreamer-like training and Offline training. In this way, we study whether dream-like\ntrajectories can improve generalization performances against classic imagination (without any trans-\nformation) and against further offline training over collected real experience. For a fair comparison,\nwe use the same hyperparameters and use an offline adaptation of PPO (Queeney et al., 2021) for\noffline training. Note that, unlike the original implementation, our Dreamer baseline considers ran-\ndomly generated initial states instead of collected ones. As reported in Figure 3, we find that it\nhelps obtain better generalization scores even without any further transformation.\n0.0\n0.5\n1.0\n1.5\nstep\n1e6\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\nepisodic return\ncaveflyer\n0.0\n0.5\n1.0\n1.5\nstep\n1e6\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nchaser\n0.0\n0.5\n1.0\n1.5\nstep\n1e6\n0\n1\n2\n3\n4\n5\n6\ncoinrun\n0.0\n0.5\n1.0\n1.5\nstep\n1e6\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\nplunder\nDay training\nDreamer with random initial states\nDreamer with collected initial states\nFigure 3: Total rewards received on all possible levels by classic Dreamer varying the source of initial\nstates for imagination (randomly generated or collected from real environments). The vertical line\nseparates the day training (common to all methods) from the night training. Results report average\nand confidence intervals across 5 seeds.\n5.3\nResults\nFigure 4 summarizes the results for all the four environments. As far as sparsely rewarded envi-\nronments are concerned (i.e., Caveflyer and Coinrun), our variants consistently increase the rewards\nreceived by the agent, showing how the night training is crucial to complement the day training\n8\n(reported in blue). Offline training (in light blue) only provides around 50% of the improvement;\nwhile the variants of our proposed solution exceed standard imagination with random initial states\n(in green) by a very small margin. This again proves the importance of starting from generated ini-\ntial states (see Figure 3). On the contrary, results from densely rewarded environments (i.e., Chaser\nand Plunder) suggest that our method and in general imagination are of little help and can even\ncause catastrophic forgetting. Such different performances suggest that dream-like imagination is\nwell-suited to complement the scarce information provided by a sparse environment when a limited\namount of real experience is available. We also experiment with a mixture of the generative transfor-\nmations without observing any further improvements (full results are reported in the Supplementary\nMaterial).\n0.0\n0.5\n1.0\n1.5\nstep\n1e6\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\nepisodic return\ncaveflyer\n0.0\n0.5\n1.0\n1.5\nstep\n1e6\n0.2\n0.4\n0.6\n0.8\n1.0\nchaser\n0.0\n0.5\n1.0\n1.5\nstep\n1e6\n0\n1\n2\n3\n4\n5\n6\ncoinrun\n0.0\n0.5\n1.0\n1.5\nstep\n1e6\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\nplunder\nDay training\nOffline PPO\nDreamer\nRndDreamer\nValDreamer\nDeepDreamer\nFigure 4: Total rewards received on all possible levels by our variants and by the two baselines. The\nvertical line separates the day training (common to all methods) from the night training. Results\nreport average and confidence intervals across 5 seeds.\n6\nConclusion\nIn this paper, we have introduced a method for improving generalization in RL agents in case\nof limited training experience. Inspired by the Overfitted Brain hypothesis, we have proposed to\naugment agent training through dream-like imagination. In particular, we have discussed a method\nbased on generating diverse imagined trajectories starting from randomly generated latent states\nand modifying intermediate ones with a set of state transformations. Our method has demonstrated\nsuperior generalization capabilities when compared to traditional imagination-based and offline RL\ntechniques, particularly in scenarios characterized by very sparse rewards.\nOur research agenda\nencompasses analyzing the scalability of our approach to tackle increasingly complex and diverse\nenvironments, as well as devising and assessing additional methods to generate even richer and more\ninformative dream-like experiences.\nReferences\nRishabh Agarwal, Marlos C. Machado, Pablo Samuel Castro, and Marc G. Bellemare.\nCon-\ntrastive behavioral similarity embeddings for generalization in reinforcement learning, 2021.\narXiv:2101.05265 [cs.LG].\nThomas Andrillon, Yuval Nir, Chiara Cirelli, Giulio Tononi, and Itzhak Fried. Single-neuron activity\nand eye movements during human REM sleep and awake vision. Nature Communications, 6(1):\n7884, 2015.\nErshad Banijamali, Rui Shu, mohammad Ghavamzadeh, Hung Bui, and Ali Ghodsi. Robust locally-\nlinear controllable embedding. In Proceedings of the 21st International Conference on Artificial\nIntelligence and Statistics (AISTATS’18), 2018.\n9\nLars Buesing, Theophane Weber, Sebastien Racaniere, S. M. Ali Eslami, Danilo Rezende, David P.\nReichert, Fabio Viola, Frederic Besse, Karol Gregor, Demis Hassabis, and Daan Wierstra. Learning\nand querying fast generative models for reinforcement learning, 2018. arXiv:1802.03006 [cs.LG].\nChang Chen, Yi-Fu Wu, Jaesik Yoon, and Sungjin Ahn. TransDreamer: Reinforcement learning\nwith transformer world models, 2022. arXiv:2202.09481 [cs.LG].\nKyunghyun Cho, Bart van Merrienboer, Dzmitry Bahdanau, and Yoshua Bengio. On the properties\nof neural machine translation: Encoder-decoder approaches. In Proceedings of the 8th Workshop\non Syntax, Semantics and Structure in Statistical Translation (SSST-8), 2014.\nKurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep reinforcement learn-\ning in a handful of trials using probabilistic dynamics models. In Advances in Neural Information\nProcessing Systems (NIPS’18), 2018.\nKarl Cobbe, Oleg Klimov, Chris Hesse, Taehoon Kim, and John Schulman. Quantifying generaliza-\ntion in reinforcement learning. In Proceedings of the 36th International Conference on Machine\nLearning (ICML’19), 2019.\nKarl Cobbe, Christopher Hesse, Jacob Hilton, and John Schulman. Leveraging procedural generation\nto benchmark reinforcement learning.\nIn Proceedings of the 37th International Conference on\nMachine Learning (ICML’20), 2020.\nJesse Farebrother, Marlos C. Machado, and Michael Bowling. Generalization and regularization in\ndqn, 2018. arXiv:1810.00123 [cs.LG].\nYarin Gal, Rowan McAllister, and Carl Edward Rasmussen. Improving PILCO with Bayesian neural\nnetwork dynamics models. In ICML’16 Workshop in Data-Efficient Machine Learning, 2016.\nDibya Ghosh, Jad Rahme, Aviral Kumar, Amy Zhang, Ryan P Adams, and Sergey Levine. Why gen-\neralization in RL is difficult: Epistemic POMDPs and implicit partial observability. In Advances\nin Neural Information Processing Systems (NeurIPS’21), 2021.\nDavid Ha and Jürgen Schmidhuber. Recurrent world models facilitate policy evolution. In Advances\nin Neural Information Processing Systems (NIPS’18), 2018.\nDanijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James\nDavidson. Learning latent dynamics for planning from pixels. In Proceedings of the 36th Interna-\ntional Conference on Machine Learning (ICML’19), 2019.\nDanijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to Control: Learning\nBehaviors by Latent Imagination. In Proceedings of the 8th International Conference on Learning\nRepresentations (ICLR’20), 2020.\nDanijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba.\nMastering Atari with\nDiscrete World Models. In Proceedings of the 9th International Conference on Learning Repre-\nsentations (ICLR’21), 2021.\nDanijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering Diverse Domains\nthrough World Models, 2023. arXiv:2301.04104 [cs.AI].\nMikael Henaff, William F. Whitney, and Yann LeCun. Model-based planning with discrete and\ncontinuous actions, 2017. arXiv:1705.07177 [cs.AI].\nErik Hoel. Enter the supersensorium: The neuroscientific case for art in the age of netflix. The\nBaffler, 45, 2019. https://thebaffler.com/salvos/enter-the-supersensorium-hoel.\nErik Hoel. The overfitted brain: Dreams evolved to assist generalization. Patterns, 2(5):100244,\n2021.\n10\nMaximilian Igl, Luisa Zintgraf, Tuan Anh Le, Frank Wood, and Shimon Whiteson. Deep variational\nreinforcement learning for POMDPs.\nIn Proceedings of the 35th International Conference on\nMachine Learning (ICML’18), 2018.\nMaximilian Igl, Kamil Ciosek, Yingzhen Li, Sebastian Tschiatschek, Cheng Zhang, Sam Devlin,\nand Katja Hofmann. Generalization in reinforcement learning with selective noise injection and\ninformation bottleneck. In Advances in Neural Information Processing Systems (NeurIPS’19),\n2019.\nMax Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul, Joel Z. Leibo, David\nSilver, and Koray Kavukcuoglu. Reinforcement learning with unsupervised auxiliary tasks. In\nProceedings of the 5th International Conference on Learning Representations (ICLR’17), 2017.\nMinqi Jiang, Edward Grefenstette, and Tim Rocktäschel. Prioritized level replay. In Proceedings of\nthe 38th International Conference on Machine Learning (ICML’21), 2021.\nLukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy H. Campbell, Konrad\nCzechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, Afroz Mohiuddin,\nRyan Sepassi, George Tucker, and Henryk Michalewski. Model based reinforcement learning for\natari. In Proceedings of the 8th International Conference on Learning Representations (ICLR’20),\n2020.\nDurk P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling. Im-\nproved variational inference with inverse autoregressive flow. In Advances in Neural Information\nProcessing Systems (NIPS’16), 2016.\nRobert Kirk, Amy Zhang, Edward Grefenstette, and Tim Rocktäschel. A survey of zero-shot gener-\nalisation in deep reinforcement learning. Journal of Artificial Intelligence Research, 76:64, 2023.\nMisha Laskin, Kimin Lee, Adam Stooke, Lerrel Pinto, Pieter Abbeel, and Aravind Srinivas. Rein-\nforcement learning with augmented data. In Advances in Neural Information Processing Systems\n(NeurIPS’20), 2020.\nAlex X. Lee, Anusha Nagabandi, Pieter Abbeel, and Sergey Levine. Stochastic latent actor-critic:\nDeep reinforcement learning with a latent variable model. In Advances in Neural Information\nProcessing Systems (NeurIPS’20), 2020a.\nKimin Lee, Kibok Lee, Jinwoo Shin, and Honglak Lee. Network randomization: A simple tech-\nnique for generalization in deep reinforcement learning. In Proceedings of the 8th International\nConference on Learning Representations (ICLR’20), 2020b.\nTimothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,\nDavid Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In Pro-\nceedings of the 4th International Conference on Learning Representations (ICLR’16), 2016.\nClare Lyle, Mark Rowland, Will Dabney, Marta Kwiatkowska, and Yarin Gal. Learning dynam-\nics and generalization in deep reinforcement learning. In Proceedings of the 39th International\nConference on Machine Learning (ICML’22), 2022.\nVincent Micheli, Eloi Alonso, and François Fleuret. Transformers are sample-efficient world models.\nIn Proceedings of the 11th International Conference on Learning Representations (ICLR’23), 2023.\nPaulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia,\nBoris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed\nprecision training. In Proceedings of the 6th International Conference on Learning Representations\n(ICLR’18), 2018.\n11\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Belle-\nmare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen,\nCharles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wier-\nstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning.\nNature, 518(7540):529–533, 2015.\nAlexander Mordvintsev, Christopher Olah, and Mike Tyka. Inceptionism: Going deeper into neural\nnetworks, 2015. Google Research Blog.\nYao Mu, Yuzheng Zhuang, Bin Wang, Guangxiang Zhu, Wulong Liu, Jianyu Chen, Ping Luo,\nShengbo Li, Chongjie Zhang, and Jianye Hao. Model-based reinforcement learning via imagination\nwith derived memory. In Advances in Neural Information Processing Systems (NeurIPS’21), 2021.\nAnusha Nagabandi, Gregory Kahn, Ronald S. Fearing, and Sergey Levine. Neural network dynamics\nfor model-based deep reinforcement learning with model-free fine-tuning. In Proceedings of the\n2018 IEEE International Conference on Robotics and Automation (ICRA’18), 2018.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kel-\nton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano, Jan Leike,\nand Ryan Lowe. Training language models to follow instructions with human feedback. In Ad-\nvances in Neural Information Processing Systems (NeurIPS’22), 2022.\nJames Queeney, Ioannis Paschalidis, and Christos Cassandras. Generalized proximal policy opti-\nmization with sample reuse. In Advances in Neural Information Processing Systems (NeurIPS’21),\n2021.\nSébastien Racanière, Theophane Weber, David Reichert, Lars Buesing, Arthur Guez, Danilo\nJimenez Rezende, Adrià Puigdomènech Badia, Oriol Vinyals, Nicolas Heess, Yujia Li, Razvan Pas-\ncanu, Peter Battaglia, Demis Hassabis, David Silver, and Daan Wierstra. Imagination-augmented\nagents for deep reinforcement learning. In Advances in Neural Information Processing Systems\n(NIPS’17), 2017.\nRoberta Raileanu and Rob Fergus. Decoupling value and policy for generalization in reinforcement\nlearning. In Proceedings of the 38th International Conference on Machine Learning (ICML’21),\n2021.\nRoberta Raileanu, Maxwell Goldstein, Denis Yarats, Ilya Kostrikov, and Rob Fergus. Automatic\ndata augmentation for generalization in reinforcement learning. In Advances in Neural Information\nProcessing Systems (NeurIPS’21), 2021.\nAntti Revonsuo. The reinterpretation of dreams: An evolutionary hypothesis of the function of\ndreaming. Behavioral and Brain Sciences, 23(6):877–901, 2000.\nJulian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon\nSchmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, Timothy Lillicrap, and\nDavid Silver. Mastering Atari, Go, chess and shogi by planning with a learned model. Nature,\n588:604–609, 2020.\nJohn Schulman, Philipp Moritz, Sergey Levine, Michael I. Jordan, and Pieter Abbeel.\nHigh-\ndimensional continuous control using generalized advantage estimation.\nIn Proceedings of the\n4th International Conference on Learning Representations (ICLR’16), 2016.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\noptimization algorithms, 2017. arXiv:1707.06347 [cs.LG].\nRamanan Sekar, Oleh Rybkin, Kostas Daniilidis, Pieter Abbeel, Danijar Hafner, and Deepak Pathak.\nPlanning to explore via self-supervised world models. In Proceedings of the 37th International\nConference on Machine Learning (ICML’20), 2020.\n12\nKaren Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks: Vi-\nsualising image classification models and saliency maps. In Proceedings of the ICLR’14 Workshop,\n2014.\nAdam Stooke, Kimin Lee, Pieter Abbeel, and Michael Laskin. Decoupling representation learning\nfrom reinforcement learning. In Proceedings of the 38th International Conference on Machine\nLearning (ICML’21), 2021.\nR. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. The MIT Press, 2017.\nRichard S. Sutton. Dyna, an integrated architecture for learning, planning, and reacting. In Working\nNotes of the 1991 AAAI Spring Symposium, 1991.\nJosh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. Do-\nmain randomization for transferring deep neural networks from simulation to the real world. In\nProceedings of the 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems\n(IROS’17), 2017.\nPedro Tsividis, Thomas Pouncy, Jaqueline L. Xu, Joshua B. Tenenbaum, and Samuel J. Gershman.\nHuman learning in atari. In Proceedings of the 2017 AAAI Spring Symposium Series, Science of\nIntelligence: Computational Principles of Natural and Artificial Intelligence, 2017.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information\nProcessing Systems (NIPS’17), 2017.\nKaixin Wang, Bingyi Kang, Jie Shao, and Jiashi Feng. Improving generalization in reinforcement\nlearning with mixture regularization.\nIn Advances in Neural Information Processing Systems\n(NeurIPS’20), 2020.\nManuel Watter, Jost Springenberg, Joschka Boedecker, and Martin Riedmiller. Embed to control:\nA locally linear latent dynamics model for control from raw images.\nIn Advances in Neural\nInformation Processing Systems (NIPS’15), 2015.\nJ Beau W Webber. A bi-symmetric log transformation for wide-range data. Measurement Science\nand Technology, 24(2):027001, 2012.\nRonald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement\nlearning. Machine Learning, 8:229–256, 1992.\nDenis Yarats, Ilya Kostrikov, and Rob Fergus. Image augmentation is all you need: Regularizing\ndeep reinforcement learning from pixels. In Proceedings of the 9th International Conference on\nLearning Representations (ICLR’21), 2021.\nChang Ye, Ahmed Khalifa, Philip Bontrager, and Julian Togelius. Rotation, translation, and crop-\nping for zero-shot generalization, 2020. arXiv:2001.09908 [cs.LG].\nMarvin Zhang, Sharad Vikram, Laura M. Smith, Pieter Abbeel, Matthew J. Johnson, and Sergey\nLevine.\nSOLAR: deep structured representations for model-based reinforcement learning.\nIn\nProceedings of the 36th International Conference on Machine Learning (ICML’19), 2019.\nGuangxiang Zhu, Minghao Zhang, Honglak Lee, and Chongjie Zhang. Bridging imagination and\nreality for model-based deep reinforcement learning. In Advances in Neural Information Processing\nSystems (NeurIPS’20), 2020.\n13\nSupplementary Material\nImplementation Details for Reproducibility\nWe adopt small model sizes for our neural networks as in Hafner et al. (2023); Table 1 reports the\nfull list of network hyperparameters. Table 2 reports all the training parameters. In addition, we\nalso leverage mixed precision (Micikevicius et al., 2018) to reduce resource consumption.\nParameter\nValue\nCategoricals C\n32\nClasses J\n32\nRNN hidden units\n512\nConvolution filters\n[32, 64, 128, 256]\nConvolution kernel size\n4\nConvolution strides\n2\nDeconvolution filters\n[128, 64, 32, 3]\nDeconvolution kernel size\n4\nDeconvolution strides\n2\nLinear units\n512\nMLP layers\n2\nNormalization\nLayer\nActivation\nswish\nLearning rate during day\n5e-4\nLearning rate at night\n1e-4\nOptimizer\nAdam\nReward/return bins K\n255\nBins extremes\n-20, +20\nDynamics loss factor β1\n0.5\nRepresentation loss factor β2\n0.1\nCritic loss factor cv\n0.5\nEntropy loss factor ce\n0.001\nγ parameter during day (GAE)\n0.99\nγ parameter at night (GAE)\n1 - 1/H\nλ parameter (GAE)\n0.95\nPPO clip factor ϵ\n0.2\nPPO gradient clip factor\n0.5\nPPO iterations\n4\nTable 1: Network hyperparameters.\n14\nParameter\nValue\nSeed episodes S\n5\nDay epochs Ed\n200\nWorld model update steps Uw\n20\nDay steps per epoch Td\n5000\nWorld batch size Bw\n100\nSequence length L\n25\nNight epochs En\n200\nImagination update steps Ua\n26\nImagination batch size Ba\n12\nImagination horizon H\n16\nTest repetition\n5\nParallelized environments\n5\nPenalty for non-successful episodes (coinrun and caveflyer)\n-10\nPenalty for non-successful episodes (chaser and plunder)\n0\nReward scaling factor (chaser only)\n25\nDeepDream optimization steps\n10\nDeepDream step size\n0.1\nValue maximization optimization steps\n10\nValue maximization step size\n0.5\nTable 2: Training parameters.\n15\nAdditional Results\nIn addition to the results presented in Section 5, we also experiment with a mixture of the generative\ntransformations, i.e., by randomly applying the three transformations with equal probability to the\ncurrent state. We refer to this variant as FullDreamer. As reported in Figure 5, we do not observe\nany significant improvement compared with the cases in which the transformations are applied\nseparately. This is probably due to the fact that one of them has a prominent effect on the learning\nperformance.\n0.0\n0.5\n1.0\n1.5\nstep\n1e6\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\nepisodic return\ncaveflyer\n0.0\n0.5\n1.0\n1.5\nstep\n1e6\n0.2\n0.4\n0.6\n0.8\n1.0\nchaser\n0.0\n0.5\n1.0\n1.5\nstep\n1e6\n0\n1\n2\n3\n4\n5\n6\ncoinrun\n0.0\n0.5\n1.0\n1.5\nstep\n1e6\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\nplunder\nDay training\nFullDreamer\nRndDreamer\nValDreamer\nDeepDreamer\nFigure 5: Total rewards received on all the levels by our variants considering the transformations\nseparately and together with random uniform probability. The vertical line separates the day training\n(common to all methods) from the night training. Results report average and confidence intervals\nacross 5 seeds.\n16\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2024-03-12",
  "updated": "2024-03-12"
}