{
  "id": "http://arxiv.org/abs/2403.08194v1",
  "title": "Unsupervised Learning of Hybrid Latent Dynamics: A Learn-to-Identify Framework",
  "authors": [
    "Yubo Ye",
    "Sumeet Vadhavkar",
    "Xiajun Jiang",
    "Ryan Missel",
    "Huafeng Liu",
    "Linwei Wang"
  ],
  "abstract": "Modern applications increasingly require unsupervised learning of latent\ndynamics from high-dimensional time-series. This presents a significant\nchallenge of identifiability: many abstract latent representations may\nreconstruct observations, yet do they guarantee an adequate identification of\nthe governing dynamics? This paper investigates this challenge from two angles:\nthe use of physics inductive bias specific to the data being modeled, and a\nlearn-to-identify strategy that separates forecasting objectives from the data\nused for the identification. We combine these two strategies in a novel\nframework for unsupervised meta-learning of hybrid latent dynamics (Meta-HyLaD)\nwith: 1) a latent dynamic function that hybridize known mathematical\nexpressions of prior physics with neural functions describing its unknown\nerrors, and 2) a meta-learning formulation to learn to separately identify both\ncomponents of the hybrid dynamics. Through extensive experiments on five\nphysics and one biomedical systems, we provide strong evidence for the benefits\nof Meta-HyLaD to integrate rich prior knowledge while identifying their gap to\nobserved data.",
  "text": "Unsupervised Learning of Hybrid Latent Dynamics:\nA Learn-to-Identify Framework\nYubo Ye1, Sumeet Vadhavkar2, Xiajun Jiang2, Ryan Missel2, Huafeng Liu1, and Linwei\nWang2\n1Zhejiang University, China\n2College of Computing and Information Sciences, Rochester Institute of Technology,\nUSA\nMarch 14, 2024\nAbstract\nModern applications increasingly require unsu-\npervised learning of latent dynamics from high-\ndimensional time-series. This presents a significant\nchallenge of identifiability: many abstract latent\nrepresentations may reconstruct observations, yet\ndo they guarantee an adequate identification of\nthe governing dynamics? This paper investigates\nthis challenge from two angles: the use of physics\ninductive bias specific to the data being modeled,\nand a learn-to-identify strategy that separates fore-\ncasting objectives from the data used for the iden-\ntification. We combine these two strategies in a\nnovel framework for unsupervised meta-learning\nof hybrid latent dynamics (Meta-HyLaD) with: 1)\na latent dynamic function that hybridize known\nmathematical expressions of prior physics with neu-\nral functions describing its unknown errors, and 2)\na meta-learning formulation to learn to separately\nidentify both components of the hybrid dynam-\nics. Through extensive experiments on five physics\nand one biomedical systems, we provide strong evi-\ndence for the benefits of Meta-HyLaD to integrate\nrich prior knowledge while identifying their gap to\nobserved data.\n1\n1\nIntroduction\nLearning the dynamics underlying observed time-\nseries is at the heart of many applications, such\nas health monitoring and autonomous driving. As\nthe quality and diversity of observation data con-\ntinue to improve, modern applications increasingly\nrequire deep learning capabilities to extract latent\n1It’s a preprint under review\ndynamics from high-dimensional observations (e.g.,\nimages), without label access to the latent variables\nbeing modeled. This unsupervised learning of la-\ntent dynamics presents a fundamental challenge of\nidentifiability that is distinct from supervised mod-\neling at the data space: different latent abstrac-\ntion may be learned to reconstruct an observed\ntime-series, but do they all guarantee an adequate\nidentification of the governing dynamic functions?\nRecent works have shown that integrating in-\nductive bias, in the form of known physics-based\ndynamic functions at the latent space, enables iden-\ntification of the latent state variables and even pa-\nrameters of the dynamic function with simple un-\nsupervised objectives of data reconstruction, as il-\nlustrated in Fig. 1A [35, 25, 17]. These approaches\nhowever require the physics of the latent dynamic\nfunction to be accurately known, which is not al-\nways possible in practice.\nThis limitation has been recognized in emerg-\ning hybrid models that combines physics-based\ndynamic functions with neural networks describ-\ning potentially unknown components in the prior\nphysics [16, 38, 10, 13, 29]. The existing hybrid\nmodels, however, are all learned at the data space\nwith direct supervision on the state variables being\nmodeled by the dynamic function. There are no\nexisting solutions supporting unsupervised learning\nof hybrid dynamic functions at the latent space.\nIn this paper, we investigate the identifiability\nof neural latent dynamic functions and show that\n– unlike physics-based functions – a neural latent\ndynamic function without physics inductive bias\ncannot be adequately identified with an unsuper-\nvised reconstruction objective. Indeed, there has\nbeen a variety of purely neural-network based la-\ntent dynamic models which, as illustrated in 1B,\nare learned with an unsupervised forecasting objec-\n1\narXiv:2403.08194v1  [cs.LG]  13 Mar 2024\nFigure 1: Overview of Meta-HyLaD vs. existing frameworks for unsupervised learning of physics-based\nor neural latent dynamic functions.\ntive to use several initial frames to predict a longer\nsequence [32, 1, 2]. These latent dynamic functions,\nhowever, are optimized globally for the entire train-\ning data: i.e., the training process results in the\nidentification of a single dynamic function with-\nout retraining. Adopted directly in hybrid latent\ndynamics, this will result in a strong assumption\nthat the gap between the prior physics and ob-\nserved data are globally shared across all samples.\nThis challenge may be further aggravated when the\nphysics-based and neural components – both het-\nerogeneous across samples – need to be separately\nidentified.\nTo this end, we introduce the first solution\nto unsupervised learning of hybrid latent dynam-\nics (Meta-HyLaD). As illustrated in Fig. 1C, in\ncontrast to existing purely physics-based or neu-\nral approaches (Fig. 1A-B), Meta-HyLaD has two\nkey innovations. First, inspired by [29], we for-\nmulate the latent dynamic function in the form\nof a universal differential equation (UDE), dzt\ndt =\nf(zt; cp, cn) = fPHY(zt; cp) + fNNϕ(zt; cn), where\nfPHY(zt, cp) describes prior physics specific to the\ndata under study and fNNϕ(zt; cn) is a neural net-\nwork describing potential errors in the prior physics,\neach with parameter cp and cn to be identified. Sec-\nond, we present a novel learn-to-identify solution\nwhere, instead of learning/identifying a single fNNϕ\nthat can only describe a gloal discrepancy between\nfPHY and the true governing dynamics, we train\nfeedforward meta-models to learn to identify fNNϕ:\nwe will show that this strategy – asking latent dy-\nnamic functions to forecast for samples different\nfrom those used to identify them – is critical to\ntheir successful identifications.\nWe evaluate Meta-HyLaD on a spectrum of\ndynamic systems ranging from relatively simple\nbenchmark physics to tracer kinetics underlying dy-\nnamic positron emission tomography (PET) imag-\ning [27]. We first demonstrate that the unsuper-\nvised reconstruction objectives cannot guarantee\nthe adequate identification of a neural latent dy-\nnamic function, and that this can be overcome\neither by the incorporation of a physics-based dy-\nnamic function or – at the absence of such physics\ninductive bias – the presented learn-to-identify for-\nmulation (Section 4.2). We then demonstrate the\nclear improvements in system identification and\ntime-series forecasting obtained by Meta-HyLaD\nin comparison to purely physics-based, purely neu-\nral, or hybrid models with a global neural compo-\nnent (Section 4.3). Finally, we demonstrate the\nadvantages of Meta-HyLaD in comparison to a\nwide variety of existing works – both physics- and\nneural-based latent dynamic models and their ex-\ntensions to meta-learning formulations – across\ndifferent types of errors in the prior physics and\nacross the datasets considered (Section 4.4). The\nresults provide strong evidence for the benefits\nof hybrid dynamics to integrate rich prior knowl-\nedge while allowing for their errors, as well as the\nimportance of properly identifying these hybrid\ncomponents.\n2\nRelated Works\nUnsupervised learning of latent dynamics:\nThere has been a surge of interests in learning\nthe governing dynamics of high-dimensional time-\nseries, mostly in an encoder/decoder formulation\nwith the goal to identify a dynamic function and\nits resulting state variables at the latent space.\nPhysics-based (white-box) latent dynam-\nics: Several methods have proposed to integrate\nphysics-based dynamic equations into the latent\nspace as prior knowledge specific to the data un-\nder study [35, 25, 17]. They however require the\nphysics of the latent dynamics to be accurately\nknown (with only some physics parameters un-\nknown), which is not always possible in practice.\nWe will show that – at the presence of different\ntypes of errors in the prior physics – the perfor-\n2\nmance of this type of white-box modeling quickly\ndeteriorates. How to leverage physics-based func-\ntions while addressing their errors remains an open\nquestion in learning latent dynamics.\nNeural\n(black-box)\nlatent\ndynamics:\nThere has also been an explosion of neural-network\nbased latent dynamic models, using for instance\nLSTMs [4, 23, 8, 19] and neural ODEs [36, 2] at\nthe latent space. Different from white-box dynamic\nfunctions with physically-meaningful latent state\nvariables and parameters, black-box dynamic func-\ntions are associated with abstract latent states\nand functions. It thus sees an increased challenge\nof identifiability, especially when learning across\nheterogeneous dynamics.\nAmong recent efforts,\nthere has been a growing body of works explor-\ning ways to effectively identify different neural dy-\nnamic functions from heterogeneous data samples\n[37, 33, 21, 18], among which meta-learning has\nemerged to be a promising solution [21, 18].\nAll of these existing works, however, are based\non black-box dynamic functions without the ability\nto leverage potentially valuable prior knowledge.\nSome recent works, such as Hamiltonian and La-\ngrangian neural network [24, 12, 6, 11, 39, 32, 14,\n28, 40, 30, 3] encode physical laws as priors to con-\nstrain the behavior of the neural latent dynamic\nfunction, blurring the boundary between black- and\nwhite-box modeling. The latent dynamic functions,\nhowever, are still in the form of neural networks.\nFurthermore, these approaches encompass broader\npriors rather than richer knowledge specific to the\nproblem/data.\nMeta-HyLaD represents the first hybrid solu-\ntion to learning latent dynamics. Moreover, we\ndive deep into the identifiability of physics-based\nversus neural components, and show that – for la-\ntent dynamic functions without physics inductive\nbias – successful reconstruction of observed time-\nseries does not guarantee a successful identification:\na non-trivial challenge inadequately discussed in\nthe literature.\nHybrid (grey-box) dynamics: When the\ndynamic function is directly supervised, a number\nof hybrid models has emerged to combine physics-\nbased functions with neural networks to compen-\nsate for unknown components in the known physics.\nMost approaches [16, 38, 10] use a neural function\nto describe the residual between the simulated and\nthe measured states. NeuralSim [13] takes it a\nstep further and includes neural functions as dif-\nferent components within a physics-based function.\nUDEs define a differential equation with known\nmathematical expressions combined with neural\nnetworks [29].\nThese hybrid models, however, are learned at\nthe data space and directly supervised by mea-\nsured state variables of the dynamics.\nThe ab-\nsence of such supervision substantially increases\nthe difficulty to identify the dynamic function, and\nMeta-HyLaD represents the first step towards a\nsolution.\n3\nMethods\nFig. 1C outlines the key elements in Meta-HyLaD:\n1) hybrid latent dynamic functions, and 2) learn-\nto-identify strategies.\n3.1\nHybrid Latent Dynamic Func-\ntions as a UDE\nConsidering high-dimensional time-series x1:T =\n[x0, · · · , xt · · · , xT ], we model its generation pro-\ncess as:\nzt = F(z<t);\nxt = g(zt)\n(1)\nwhere function F(z<z) describes the latent dynam-\nics of the state variables zt, and function g(zt)\ndescribes the emission of the latent state variables\nto the observed data.\nLatent dynamic functions: While Meta-\nHyLaD is agnostic to the type of dynamic func-\ntions used in Equation (1), we choose an ordinary\ndifferential equation (ODE) to describe the latent\ndynamics as a continuous process that can be emit-\nted to the observation space only when needed.\nMore specifically, we describe the latent dynamics\nas a UDE:\ndzt\ndt = fϕ(zt; cz) = fPHY(zt; cp) + fNNϕ(zt; cn)\n(2)\nwhere fPHY(zt; cp) represents the known physics\nequation governing the data, and cp its unknown\nparameters. fNNϕ(zt; cn) represents the potential\nerrors in the prior physics, modeled by a neural\nnetwork with weight parameters ϕ. Instead of iden-\ntifying a single neural function fNNϕ(zt) that will\nonly model a global discrepancy between fPHY and\nthe true governing dynamics, we further allow it to\nchange with a parameter cn that can be identified\nfrom data: more specifically, we use cn to gener-\nate the weight parameter ϕ of fNNϕ via a hyper\nnetwork ϕ = hθ(cn).\nEmission functions: The emission function\nxt = g(zt), i.e., the decoder, bridges the mapping\nfrom the latent state space to the observed data\nspace. In most existing works considering latent\nphysics-based functions, it has been considered crit-\nical for this emission function to be physics-based\n[25, 17]. Similarly, when the latent dynamic func-\ntion is purely neural, it is customary to adopt a\n3\nneural function as the emission function [2]. As a\nsecondary objective of this paper, we will investi-\ngate the use of a physics-based vs. neural decoder\nfor different types of latent dynamic functions. We\ndenote them generally as gψ where ψ is known if\ng is physics-based, and unknown if g is a neural\nnetwork parameterized with ψ.\nWith the pair of functions described in Equa-\ntions (1-2), both state variables z0:T and data x0:T\ncan be generated once the initial condition z0 and\nthe parameters of the hybrid components – cp and\ncn – are properly identified.\n3.2\nLearn-to-Identify\nvia\nMeta-\nLearning\nGiven\nan\nobserved\ntime-series\nx0:T\n=\n[x0, · · · , xt · · · , xT ]\nin\ntraining\ndata\nD,\na\ncommon approach to identifying its latent dy-\nnamic process is to infer its time-varying latent\nstate z0:T\n= [z0, · · · , zt · · · , zT ] to reconstruct\nx0:T [23, 19, 8, 35].\nWe will show that – for\nneural functions with abstract latent states zt –\nthis reconstruction does not guarantee a correct\nidentification of the dynamic function governing\nzt: an evidence is when the identified dynamic\nfunction, given different initial conditions, could\nnot generate the correct time series governed by\nthe same dynamics.\nWe propose that a proper identification of the\nlatent dynamic functions requires us to go a step\nfurther: one must further separate z0:T into two\nkey ingredients generating it – the initial latent\nstate z0 that is specific to the time-series sample,\nand parameters cz for the governing latent dynamic\nfunction. In a neural network where the learned\nfunctions can be arbitrary and the latent states\nabstract without direct supervision, this separation\nis not trivial.\nBelow, we describe our learning\nstrategies to facilitate this separation, and discuss\nhow such identification strategies may be different\nfor the physics-based versus neural functions.\nIdentifying initial latent states: The initial\nlatent state z0 of an observed time series x0:T is\nspecific to that series and can be simply identified\nfrom the first several frames of observations x0:l,\nwhere l ≪T:\nˆz0 = Eϕz(x0:l)\n(3)\nwhere Eϕz is a neural encoder with weight parame-\nters ϕz.\nIdentifying latent dynamics:\nTo identify cn\nand cp for Equation (2), a simple reconstruction\nobjective would be:\nˆcz = [ˆcp, ˆcn] = Eϕc(x0:T )\n(4)\nˆzt+1 = ˆzt +\nZ t+1\nt\nfϕ(ˆzt; ˆcz)dz,\nt = 0, · · · , T −1\n(5)\nˆΘ = arg min\nΘ\n1\n|D|\nX\nx0:T ∈D\n∥x0:T −gψ(ˆz0:T )∥∥2\n2 (6)\nwhere Θ = {ϕz, ϕc} and ψ if gψ is neural. Eϕc is\nneural encoder parameterized by ϕc. We refer to\nthis as a reconstruction objective, as illustrated in\nFig. 2A. Although commonly used, we will show\nthat this learning strategy – while deceivingly able\nto provide good reconstruction performance on a\ngiven time series – will not be able to separately\nidentify the three components of the latent dynamic\nfunction.\nIdentifying the neural component: Instead\nof the reconstruction of an observed time-series,\nwe present a novel learn-to-identify solution to ad-\ndress the identifiability challenge associated with\nthe neural component fNNϕ of the latent dynamic\nfunction. The fundamental intuition is that, to\nseparately identify time-varying latent states from\ntime-invariant governing dynamic functions, one\ncan leverage the statistical strength that the gov-\nerning latent dynamics might be shared across mul-\ntiple samples. Therefore, we can attempt to learn\nto identify cn for fNNϕ from one or more samples,\nand ask the identified function to be applicable\nfor disjoint samples sharing the same dynamics.\nThis is the basis for the presented learn-to-identify\nframework.\nFormally, we cast this into a meta-learning\nformulation.\nConsider a dataset D of high-\ndimensional time-series with M similar but distinct\nunderlying dynamics: D = {Dj}M\nj=1. For each Dj,\nwe consider disjoint few-shot context time-series\nsamples Ds\nj =\nn\nxs,1\n0:T , xs,2\n0:T , . . . , xs,k\n0:T\no\nand query sam-\nples Dq\nj =\nn\nxq,1\n0:T , xq,2\n0:T , . . . , xq,d\n0:T\no\nwhere k ≪d. In-\nstead of the reconstruction objective in Equations\n(4-6), we formulate a meta-objective to learn to\nidentify cn from k-shot context time-series sam-\nples Ds\nj, such that the identified hybrid dynamic\nfunction is able to forecast for any query time\nseries in Dq\nj given only an estimate of its initial\nstate ˆzq\n0,j. More specifically, we have a feedforward\nmeta-model Mζn(Ds\nj) to learn to identify cn,j for\ndynamics j as:\nˆcn,j = Mζn(Ds\nj) = 1\nk\nP\nxs\n0:T ∈Ds\nj Eζn(xs\n0:T )\n(7)\nwhere an embedding is extracted from each individ-\nual context time-series via a meta-encoder Eζn and\ngets aggregated across Ds\nj to extract knowledge\nshared by the set. k is the size of the context set,\nand its value can be fixed or variable which we will\ndemonstrate in the ablation study.\n4\nFigure 2: HyLaD with alternative identification\nstrategies\nIdentifying the physics-component: The\nparameter cp for the physics-based fPHY(zt; cp) of\nthe latent dynamic functions can be inferred in a\nsimilar fashion. However, as the structural form\nof the function defines specific physics meaning for\nthe latent state variable zt, we will show that this\nphysics inductive bias alone allows it to be identi-\nfied with a simple reconstruction objective similar\nto Equations (4-6). In another word, cp for the\nphysics-based dynamic function can be identified\nin two alternative formulations:\nˆcp,j =\n\n\n\n1\nk\nP\nxs\n0:T ∈Ds\nj Eζp(xs\n0:T ),\nFig. 2B\nEζp(xq\n0:T ),\nxq\n0:T ∈Dq\nj\nFig. 2C\n(8)\n(9)\nLearn-to-identify meta-objectives: Given\ndata with M different dynamics D = {Dj}M\nj=1, for\nall query samples xq\n0:T ∈Dq\nj, we have its initial\nlatent state ˆzq\n0 identified from its own initial frames\nxq\n0:l (Equation (3)), and its hybrid latent dynamic\nfunctions identified with ˆcn,j and ˆcp,j as described\nin Equations (7-8) (Fig. 2B, Meta-HyLaD), or in\nEquations (7) and (9) (Fig. 2C, Mixed-HyLaD).\nGiven the inferred ˆzq\n0, ˆcn,j, and ˆcp,j, we minimize\nthe forecasting accuracy on the query time-series\nwith Θ = {ϕz, ζp, ζn} and ψ if gψ is neural:\nˆΘ = arg min\nΘ\nM\nX\nj=1\nX\nxq\n0:T ∈Dq\nj\n∥xq\n0:T −ˆxq\n0:T ∥2\n2\n(10)\n4\nExperiments and Results\nWe first investigate the two key contributions of\nMeta-HyLaD: 1) its identification strategy in com-\nparison to reconstruction objectives in addressing\nthe identifiability of physics-based vs. neural latent\ndynamic functions (Section 4.2), and 2) its model-\ning strategy vs. purely physics-based, purely neu-\nral, or hybrid models with a global neural compo-\nnent (Section 4.3). We then evaluate Meta-HyLaD\nwith a variety of existing physics-based and neu-\nral latent dynamic models [35, 2] along with their\nextensions into the presented meta-learning frame-\nworks, considering both physics-based and neural\ndecoders (Section 4.4). Finally, we test its feasibil-\nity on identifying radiotracer kinetics in dynamic\nPET (Section 4.5). All experiments are repeated\nwith three random seeds.\n4.1\nData & Experimental Settings\nWe consider common benchmarks including three\nsimple physics systems of Pendulum [2], Mass\nSpring [8], and Bouncing Ball (under gravity) [2],\nand two more complex physics systems of Double\nPendulum [2] and Rigid Rody [25]. To demonstrate\nfeasibility towards more complex systems, we fur-\nther consider an additional dataset of dynamic\nPET [27]. For each of the five physics system, we\nrandomly sample the initial states and parameters\nof the governing dynamic function, and generate\ntime-series of system states and the corresponding\nimage observations by physical rendering in [25].\nWe refer to the dynamic functions used for the gen-\neration of data as full physics functions, and design\npartial physics functions to represent our imperfect\nprior knowledge about the observed data reflecting\na variety of potentially additive and multiplicative\nerrors as summarized in Table 1: in Pendulum\nand Mass Spring, partial physics lacks the knowl-\nedge about damping (additive error), in Bouncing\nBall and Rigid Body, partial physics only considers\ngravity/force in one direction/plane (multiplica-\ntive error); in Double Pendulum, partial physics\nignores the impact of the mass difference between\ntwo pendulums (multiplicative error).\nTo introduce heterogeneity in the underlying\ndynamics, in data generation we vary the parame-\nters of the full physics equations in the components\nboth present (blue) and absent (red) in the prior\nphysics, with their training and test distributions\nand number of samples summarized in Table 1.\nMore details on each dataset are in Appendix A.\nWe leave descriptions of the dynamic PET dataset\nin Section 4.5.\n4.2\nIdentifiability of Physics.\nvs.\nNeural Dynamics\nWe first evaluate the reconstruction vs. learn-to-\nidentify objectives for identifying physics-based vs.\nneural latent dynamic functions, on the Pendulum\ndataset (see Table 1).\nModels: Here we consider the three identifi-\ncation schemes for learning hybrid latent dynamic\nfunctions as described in Section 3.2 (Fig. 2). All\nthree learning strategies share the same backbone\narchitectures, with their architectural and imple-\nmentation details provided in appendix G. We\n5\nTable 1: Summary of experimental settings.\nDataSet\nPhysics\nEquation\nConfigurantion\nDataSize\nPendulum\nFull\nd\ndt\n\u0014φ\n˙φ\n\u0015\n=\n\u0014\n˙φ\n−G\nL sin(φ) −β ˙φ\n\u0015\nG ∈[5.0, 15.0]\nβ ∈(0.0, 1.0] ID\nβ ∈(1.0, 1.2] OOD\n15390\nPartial\nd\ndt\n\u0014\nφ\n˙φ\n\u0015\n=\n\u0014\n˙φ\n−G\nL sin(φ)\n\u0015\nMass Spring\n˜x = x1 −x2\n˜v = v1 −v2\nFull\nd\ndt\n\u0014v1\nv2\n\u0015\n=\n\"\n−˜x\n|˜x|\nk\nm1 (|˜x| −l0) −\nβ\nm1 ˜v\n˜x\n|˜x|\nk\nm2 (|˜x| −l0) +\nβ\nm2 ˜v\n#\nk ∈[5.0, 15.0]\nβ ∈(0.0, 1.0] ID\nβ ∈(1.0, 1.5] OOD\n12960\nPartial\nd\ndt\n\u0014v1\nv2\n\u0015\n=\n\"\n−˜x\n|˜x|\nk\nm1 (|˜x| −l0)\n˜x\n|˜x|\nk\nm2 (|˜x| −l0)\n#\nBouncing Ball\nFull\nd\ndt\n\u0014vx\nvy\n\u0015\n=\n\u0014G cos φ\nG sin φ\n\u0015\n, φ = i\n8π, i = −8, −7, . . . , 6, 7\nG ∈[0.0, 5.0]\ni ∈[−6 : −1] ∪[2 : 7] ID\ni ∈[−8, −7, 0, 1] OOD\n21060\nPartial\nd\ndt\n\u0014vx\nvy\n\u0015\n=\n\u0014 0\n−G\n\u0015\nRigid Body\n\u0014m\n0\n0\nI\n\u0015 \u0014 ˙v\n˙ω\n\u0015\n=\n\u0014\nf\nr × f\n\u0015\n−\n\u0014\n0\nω × Iω\n\u0015\nψ ∈[−π, π)\nφ ∈( π\n8 , π\n2 ] ID\nφ ∈[0, π\n8 ] OOD\n4800\nFull\nf = f · [cos ψ sin φ, sin ψ sin φ, cos φ]\nPartial\nf = f · [cos ψ, sin ψ, 0]\nDouble Pendulum\n˜m = m1/m2\nϕ1 = cos(φ1 −φ2)\nϕ2 = sin(φ1 −φ2)\nFull\nd\ndt\n\u0014 ˙φ1\n˙φ2\n\u0015\n=\n\n\nG sin φ2ϕ1−ϕ2(L1 ˙φ2\n1ϕ1+L2 ˙φ2\n2)−( ˜m+1)G sin φ1\nL1( ˜m+ϕ2\n2)\n( ˜m+1)(L1 ˙φ2\n1ϕ2−G sin φ2+G sin φ1ϕ1)+L2 ˙φ2\n2ϕ1ϕ2\nL2( ˜m+ϕ2\n2)\n\n\nG ∈[5.0, 15.0]\n˜m ∈[0.5, 1.5] ID\n˜m ∈(1.5, 2.0] OOD\n12705\nPartial\nd\ndt\n\u0014 ˙φ1\n˙φ2\n\u0015\n=\n\n\nG sin φ2ϕ1−ϕ2(L1 ˙φ2\n1ϕ1+L2 ˙φ2\n2)−2G sin φ1\nL1(1+ϕ2\n2)\n2(L1 ˙φ2\n1ϕ2−G sin φ2+G sin φ1ϕ1)+L2 ˙φ2\n2ϕ1ϕ2\nL2(1+ϕ2\n2)\n\n\nDynamic PET\nFull\n\u0014 ˙CEi(t)\n˙CMi(t)\n\u0015\n=\n\u0014−k2 −k3\nk4\nk3\n−k4\n\u0015 \u0014CEi(t)\nCMi(t)\n\u0015\n+\n\u0014k1\n0\n\u0015\nCP (t)\nSee Appendix E\n2000\nPartial\n˙CT i(t) = −k2CT i(t) + k1CP (t)\nTable 2: Comparison of HyLAD obtained from different identification strategies in reconstruction and\nprediction performance.\nRecontruction Task\nPrediction Task\nStrategy\nMSE\nxt(e−4) ↓\nzt(e−3) ↑\ncp(e−1) ↓\ncn(e−3) ↓\nxt(e−4)\nzt(e−3)\ncp(e−1)\ncn(e−3)\nA – Recon-HyLaD\n3.95(0.27)\n4.34(0.74)\n2.05(0.31)\n1.13(0.26)\n7.17(0.50)\n10.59(1.06)\n1.83(0.15)\n3.72(0.69)\nB – Meta-HyLaD (k=1)\n4.39(0.19)\n5.19(0.85)\n1.70(0.10)\n1.16(0.18)\n4.40(0.37)\n6.70(1.31)\n1.46(0.13)\n2.23(0.28)\nC – Mixed-HyLaD (k=1)\n3.95(0.58)\n4.15(0.97)\n1.70(0.07)\n0.45(0.06)\n4.11(0.58)\n6.85(2.48)\n1.54(0.50)\n0.44(0.06)\nB – Meta-HyLaD (k=7)\n/\n/\n/\n/\n2.86(0.13)\n2.05(0.09)\n0.28(0.00)\n0.45(0.05)\nC – Mixed-HyLaD (k=7)\n/\n/\n/\n/\n2.62(0.11)\n1.93(0.12)\n0.32(0.07)\n0.39(0.12)\nconsider k = 1 and k = 7 for learn-to-identify\nstrategies, to assess potential impact associated\nwith the number of samples used to identify the\ndynamic functions.\nMetrics: A successfully-identified latent dy-\nnamic function should be able to forecast a time\nseries that is different from those used to identify\nit (but shares the same dynamics). To this end,\nwe test the three learning strategies for two tasks:\n1) a reconstruction task to identify cp and cn from\na test time-series and reconstruct the seires itself,\nand 2) a prediction task where cp and cn are iden-\ntified from a test time-series but used to predict\nanother time-series that follows the same dynamics\nbut different initial conditions.\nFor both tasks, we measured the mean squared\nerror (MSE) between the ground-truth and esti-\nmated time series at both the observation x0:T\nand the state variable z0:T level .\nIn addition,\nwe consider two metrics to separately evaluate\nhow well cp and cn are each identified: for cp,\nwe directly measure the MSE between its identified\nand ground-truth value; for cn, because we do not\nexpect its value to be physically meaningful, we\ninstead measure the fit between the identified neu-\nral dynamic function fNNθ(zt; cn) and the ground\ntruth residual function −β ˙φ by their MSE.\nResults:\nAs summarized in Table 2, all three\nlearning strategies results in strong performance\nto reconstruct the same time series used to iden-\ntify the latent dynamic functions. However, latent\ndynamic functions identified with the reconstruc-\ntion objective (A) fail when attempting to predict\ndifferent time series sharing the same dynamics.\nA deeper dive reveals something interesting: the\nphysics component (cp) is well identified by all\nidentification strategies (A-C); in contrast, cn is\nonly successfully identified by the learn-to-identify\nstrategy (B-C). Moreover, the mixed identification\nstrategy (C) seems to facilitate a better identifica-\ntion of cn when k = 1, although both Meta- (B)\nand Mixed-HyLaD (C) achieve similar performance\nwhen k is increased to 7. Interestingly, increasing\nk for identifying cn also improves the identifica-\n6\nFigure 3: Comparison of alternative strategies for modeling the latent dynamic functions (purely\nphysics, purely neural, Global-HyLaD, and Meta-HyLaD). MSE metrics are the lower the better, and\nVPT metrics are the higher the better.\ntion of cp in Mixed-HyLaD (C). We provide visual\nexamples of identified fNNϕ in appendix B.\nThese experiments provide two important in-\nsights: 1) a good reconstruction is not sufficient\nevidence that the underlying latent dynamics is\ncorrectly identified, and 2) the inclusion of physics\ninductive bias or the learn-to-identify strategy are\ntwo effective solutions to address the issue of iden-\ntifiability.\n4.3\nBenefits\nof\nHybrid\nDynamic\nFunctions\nWe now evaluate the benefits of hybrid latent dy-\nnamic functions on the five physics systems de-\nscribed in Section 4.1.\nModels: We now fix the learn-to-identify strat-\negy to that of Fig. 2B, and compare four alterna-\ntives of latent dynamic functions: using only the\npartial physics function (purely physics), using\nonly a neural network (purely neural), hybrid as\ndefined in Equation (2) but with a global fNNϕ\n(Global-HyLaD), and Meta-HyLaD. We also ob-\ntain results on a latent dynamic function utilizing\nthe full physics, setting a performance reference\nwhen prior knowledge is perfect.\nMetrics: We consider MSE for both the ob-\nserved images x0:T and state variables z0:T , and\nValid Prediction Time (VPT) that measures how\nlong the predicted object’s trajectory remains close\nto the ground truth trajectory based on the MSE\n[2]. We separately evaluate these metrics for test\ntime-series with parameters within (in distribution\n/ ID) or outside (out of distribution / OOD) those\nused in the training data, as summarized in Table\n1.\nResults: Fig. 3 summarizes the results on three\ndatasets, with complete results and visual exam-\nples provided in appendix C: note that the purely\nneural dynamic functions (yellow), due to the ab-\nstract nature of its latent state variables, results in\npoor MSE/VPT metrics on z0:T at a a magnitude\nlarger than alternative approaches; their values\nare thus omitted from Fig. 3 but can be found in\nappendix C. As shown, purely physics-based ap-\nproaches, when given perfect knowledge, achieve\nexcellent performance as a reference (dashed blue\nline). Their accuracy, however, deteriorates sub-\nstantially given imperfect knowledge (aqua), often\nto a level on par with purely neural approaches (yel-\nlow). The use of hybrid functions helps, although\nto a limited extent if the neural components are\nglobally optimized (green). Meta-HyLaD (red),\nwith an ability to identify the neural components\nfrom data, substantially improves over all three\nalternatives – its performance is on par with, and\nsometimes better than, the performance achieved\nwith perfect physics.\n4.4\nComparison with Existing Base-\nlines\nOn the five physics systems, we then compare Meta-\nHyLaD with existing unsupervised latent dynamic\nmodels. Note that there is no existing hybrid latent\ndynamic models.\nModels: We consider 1) ALPS [35] which\nconsiders a physics-based latent dynamic function\n(here the partial physics) and a reconstruction-\nbased learning objective, and 2) three neural latent\ndynamic models ranging from those with minimal\nphysics prior (LSTM and neural ODE / NODE)\nand strong prior (Hamiltonian generative network /\nHGN) [2]. We consider their original formulations\n[2] that optimize global latent dynamic functions\nas illustrated in Fig. 1B, as well as their extension\nto the presented meta-learning framework repre-\nsentative of recent works in meta-learning latent\ndynamics [18, 21]. Architecture details of all base-\nlines are provided in appendix G. This provides\na comprehensive coverage of prior arts in their\nchoices of latent dynamic functions and identifica-\ntions strateiges.\nWe further note that different decoders were\n7\nFigure 4: Comparison of Meta-HyLAD and baselines with neural and physics-based (blue-shaded\nbackground) decoders.\nused in these original baselines: the neural latent\ndynamic models utilize a neural network as the de-\ncoder (neural decoder) [2]; the latent-HGN utilizes\na neural decoder but specifically uses the position\nlatent state as the input (neural decoder with prior)\n[2]; ALPS uses both this and a physics-based de-\ncoder [35]. To isolate the effect of decoders, we\nfurther evaluate each of these baselines using each\nof the three different decoders.\nMetrics: We consider MSE and VPT for x0:T ,\nand omit metrics on z0:T due to its abstract nature\nin most baselines.\nResults: Fig. 4 summarizes the results on three\ndatasets for physics-based vs. neural decoders, with\nthe complete quantitative results in appendix D.\nNotably, across all datasets, Meta-HyLaD in gen-\neral demonstrates a significant margin of improve-\nments over all baselines including those utilizing\nmeta-learning. This improvement is the most sig-\nnificant when used in combination of a physics-\nbased decoder while, with a neural decoder with\nor without prior, Meta-HyLaD sometimes becomes\ncomparable with the meta-extension of HGN – this\nagain demonstrates the benefits of physics induc-\ntive bias in both models. Note that, as we will\nshow in Section 4.5, Meta-HyLaD is more generally\napplicable beyond Hamiltonian systems for which\nHGN is designed.\nInterestingly, Meta-HyLaD improves with using\na physics-based decoder, while the other baselines\ndeteriorates. The effect of using a prior with the\ndecoder (appendix C) is less consistent and varies\nwith the dataset for all models.\n4.5\nFeasibility on Biomedical Sys-\ntems: Dynamic PET\nFinally, we consider recovering the spatial distri-\nbution and kinetics of radiotracer-labeled biolog-\nical substrates from dynamic PET images. The\nregional tracker kinetics underlying PET can be\ndescribed by the widely used compartment mod-\nels in Table 1.\nThe total concentration of ra-\ndioactivity in all tissues, CTi(t), on the scanning\ntime interval [tk−1, tk] is measured as: xik(t) =\n1\ntk−tk−1\nR tk\ntk−1 CT i(t)e−t\nτ dt at each voxel i. The ac-\ntivity image xk is obtained by lexicographic order-\ning of xik at different voxels. While raw PET data\nare sinograms, in this proof-of-concept we consider\nx0:T as the observed image series, with the goal\nto identify the compartment models. More details\nare in appendix E.\nData: We use the two-tissue compartment\nmodel listed in Table 1 to generate x0:T of a brain\nwith 5 regions of interesting for 0-40 minutes with\na scanning period of 0.5 minutes, resulting in 80\nframes per time series. 2000 time-series samples\nare generated with training/test and ID/OOD split\nof parameter ranges provided in appendix E.\nWe then assume the one-tissue compartment\nmodel to be our prior physics in Meta-HyLaD.\nCompared to the additive and multiplicative er-\nrors considered earlier, this setup provides a more\nchallenging scenario where the prior physics rep-\nresents a crude approximation, i.e., CT i(t) =\nCEi(t) + CMi(t) , for the data-generating latent\ndynamics.\nModel & Metrics: We compare Meta-HyLaD\nwith purely physics, purely neural, and Global-\nHyLaD similar to Section 4.3. We consider MSE\nand peak signal-to-noise-ratio (PSNR) on x0:T , and\nMSE and VPT on z0:T , i.e., CT (0 : T).\nResults: Fig. 5A summarize the quantitative\nmetrics along with visual examples, with the full re-\nsults and examples of tracer kinetics in appendix E\n. The same observations in the physics systems\nstill hold: Meta-HyLaD is the only model that is\nable to approach the reference performance (where\ndata-generating physics is used in the identifica-\ntion), significantly outperforming the alternatives.\nThis leaves exciting future real-world opportunities\nfor Meta-HyLaD.\n8\nFigure 5: A: Quantitative metrics and examples in dynamic PET. B: Ablation with context-size k.\n5\nConclusions & Discussion\nWe present Meta-HyLaD as a first solution to unsu-\npervised learning of hybrid latent dynamics, demon-\nstrating the use of physics inductive bias combined\nwith the learning-to-identify strategy to effectively\nleverage prior knowledge while identifying its un-\nknown gap to observed data.\nThe effect of the size of context set:\nFig. 5B shows that, while increasing the context-\nset size increases the performance of Meta-HyLaD,\nat k = 1 it still significantly outperforms the vari-\nous baselines. It is also minimally affected when\ntrained with variable size of k, enhancing its abil-\nity to accommodate varying availability of context\nsamples at test time.\nIf there is no knowledge\nabout which samples share the same dynamics,\nfuture solutions may be to replace the averaging\nfunction in context-set embedding with attention\nmechanisms, to learn to extract similar context\nsamples. Additional ablation results are provided\nin appendix F.\nGenerality and failure modes:\nIn ap-\npendix F.3, we provide examples that Meta-HyLaD\ncan accommodate general design choices of fNNϕ\nand maintains its strong identification and forecast-\ning performance. We also probe the potential fail-\nure modes for Meta-HyLaD and its use of decoders.\nAs shown in appendix F.4, if fp representing prior\nphysics becomes too weak, e.g., modeling only the\ndampening effect on Pendulum, Meta-HyLaD may\ndegenerate to a performance similar to purely neu-\nral approaches for forecasting xt, although still\nwith a substantial gain in the accuracy of zt. Fi-\nnally, while neural decoders provide competitive\nperformance for all models considered, they could\nfail if the data-generating decoder function is not\nglobal. In this setting, we show that the same\nlearn-to-identify strategy can be extended to adapt\nthe decoder (appendix F.5), leaving another future\navenue of Meta-HyLaD to be pursued.\nAcknowledgements\nThis work is supported in part by the National Key\nResearch and Development Program of China(No:\n2020AAA0109502); the Talent Program of Zhejiang\nProvince (No: 2021R51004); NIH NHLBI grant\nR01HL145590 and NSF OAC-2212548. This paper\npresents work whose goal is to advance the field\nof Machine Learning. There are many potential\nsocietal consequences of our work, none which we\nfeel must be specifically highlighted here.\nReferences\n[1] Allen-Blanchette, C., Veer, S., Majumdar,\nA., Leonard, N.E.: Lagnetvip: A lagrangian\nneural network for video prediction. arXiv\npreprint arXiv:2010.12932 (2020)\n[2] Botev,\nA.,\nJaegle,\nA.,\nWirnsberger,\nP.,\nHennes, D., Higgins, I.: Which priors mat-\nter? benchmarking models for learning latent\ndynamics (2021)\n[3] Choudhary, A., Lindner, J.F., Holliday, E.G.,\nMiller, S.T., Sinha, S., Ditto, W.L.: Forecast-\ning hamiltonian dynamics without canonical\ncoordinates. Nonlinear Dynamics 103, 1553–\n1562 (2021)\n[4] Chung, J., Kastner, K., Dinh, L., Goel, K.,\nCourville, A.C., Bengio, Y.: A recurrent latent\nvariable model for sequential data. Advances\nin neural information processing systems 28\n(2015)\n[5] Chung, J., Kastner, K., Dinh, L., Goel, K.,\nCourville, A.C., Bengio, Y.: A recurrent latent\nvariable model for sequential data. Advances\nin neural information processing systems 28\n(2015)\n[6] Cranmer, M., Greydanus, S., Hoyer, S.,\nBattaglia, P., Spergel, D., Ho, S.: Lagrangian\nneural networks. In: ICLR 2020 Workshop\non Integration of Deep Neural Models and\nDifferential Equations (2020)\n[7] Feng, D., Huang, S.C., Wang, X.: Models\nfor computer simulation studies of input func-\ntions for tracer kinetic modeling with positron\n9\nemission tomography. International journal of\nbio-medical computing 32(2), 95–110 (1993)\n[8] Fraccaro, M., Kamronn, S., Paquet, U.,\nWinther, O.: A disentangled recognition and\nnonlinear dynamics model for unsupervised\nlearning. Advances in neural information pro-\ncessing systems 30 (2017)\n[9] Girin, L., Leglaive, S., Bie, X., Diard, J.,\nHueber, T., Alameda-Pineda, X.: Dynamical\nvariational autoencoders: A comprehensive\nreview. Foundations and Trends in Machine\nLearning 15(1-2), 1–175 (2021)\n[10] Golemo, F., Taiga, A.A., Courville, A.,\nOudeyer, P.Y.:\nSim-to-real transfer with\nneural-augmented robot simulation. In: Con-\nference on Robot Learning. pp. 817–828.\nPMLR (2018)\n[11] Greydanus, S., Dzamba, M., Yosinski, J.:\nHamiltonian neural networks. Advances in\nneural information processing systems 32\n(2019)\n[12] Gupta, J.K., Menda, K., Manchester, Z.,\nKochenderfer, M.J.:\nA general framework\nfor structured learning of mechanical systems.\narXiv preprint arXiv:1902.08705 (2019)\n[13] Heiden, E., Millard, D., Coumans, E., Sheng,\nY., Sukhatme, G.S.: Neuralsim: Augmenting\ndifferentiable simulators with neural networks.\nIn: 2021 IEEE International Conference on\nRobotics and Automation (ICRA). pp. 9474–\n9481. IEEE (2021)\n[14] Higgins, I., Wirnsberger, P., Jaegle, A., Botev,\nA.: Symetric: Measuring the quality of learnt\nhamiltonian dynamics inferred from vision.\nAdvances in Neural Information Processing\nSystems 34, 25591–25605 (2021)\n[15] Hong, J., Brendel, M., Erlandsson, K., Sari,\nH., Lu, J., Clement, C., Bui, N.V., Meindl, M.,\nZiegler, S., Barthel, H., et al.: Forecasting the\npharmacokinetics with limited early frames in\ndynamic brain pet imaging using neural ordi-\nnary differential equation. IEEE Transactions\non Radiation and Plasma Medical Sciences\n(2023)\n[16] Hwangbo, J., Lee, J., Dosovitskiy, A., Bel-\nlicoso, D., Tsounis, V., Koltun, V., Hutter,\nM.: Learning agile and dynamic motor skills\nfor legged robots. Science Robotics 4(26),\neaau5872 (2019)\n[17] Jaques, M., Burke, M., Hospedales, T.:\nPhysics-as-inverse-graphics:\nUnsupervised\nphysical parameter estimation from video. In:\nEighth International Conference on Learning\nRepresentations. pp. 1–16 (2020)\n[18] Jiang, X., Missel, R., Li, Z., Wang, L.: Sequen-\ntial latent variable models for few-shot high-\ndimensional time-series forecasting. In: The\nEleventh International Conference on Learn-\ning Representations (2022)\n[19] Karl, M., Soelch, M., Bayer, J., Van der\nSmagt, P.: Deep variational bayes filters: Un-\nsupervised learning of state space models from\nraw data. arXiv preprint arXiv:1605.06432\n(2016)\n[20] Karl, M., Soelch, M., Bayer, J., van der Smagt,\nP.: Deep variational bayes filters: Unsuper-\nvised learning of state space models from raw\ndata. In: International Conference on Learn-\ning Representations (2016)\n[21] Kirchmeyer, M., Yin, Y., Don`a, J., Baskiotis,\nN., Rakotomamonjy, A., Gallinari, P.: Gen-\neralizing to new physical systems via context-\ninformed dynamics model. In: International\nConference on Machine Learning. pp. 11283–\n11301. PMLR (2022)\n[22] Klushyn, A., Kurle, R., Soelch, M., Cseke, B.,\nvan der Smagt, P.: Latent matters: Learning\ndeep state-space models. Advances in Neural\nInformation Processing Systems 34, 10234–\n10245 (2021)\n[23] Krishnan, R., Shalit, U., Sontag, D.: Struc-\ntured inference networks for nonlinear state\nspace models. In: Proceedings of the AAAI\nConference on Artificial Intelligence. vol. 31\n(2017)\n[24] Lutter, M., Ritter, C., Peters, J.: Deep la-\ngrangian networks: Using physics as model\nprior for deep learning. In: International Con-\nference on Learning Representations (ICLR\n2019). OpenReview. net (2019)\n[25] Murthy, J.K., Macklin, M., Golemo, F., Vo-\nleti, V., Petrini, L., Weiss, M., Considine,\nB., Parent-L´evesque, J., Xie, K., Erleben, K.,\net al.: gradsim: Differentiable simulation for\nsystem identification and visuomotor control.\nIn: International conference on learning rep-\nresentations (2020)\n[26] Oreshkin, B.N., Carpov, D., Chapados, N.,\nBengio, Y.: Meta-learning framework with ap-\nplications to zero-shot time-series forecasting.\n10\nIn: Proceedings of the AAAI Conference on\nArtificial Intelligence. vol. 35, pp. 9242–9250\n(2021)\n[27] Phelps, M.E.: Molecular imaging and its bi-\nological applications. Eur J Nucl Med Mol\nImaging 31, 1544 (2004)\n[28] Queiruga, A., Erichson, N.B., Hodgkinson,\nL., Mahoney, M.W.: Stateful ode-nets using\nbasis function expansions. Advances in Neural\nInformation Processing Systems 34, 21770–\n21781 (2021)\n[29] Rackauckas, C., Ma, Y., Martensen, J.,\nWarner, C., Zubov, K., Supekar, R., Skinner,\nD., Ramadhan, A., Edelman, A.: Universal\ndifferential equations for scientific machine\nlearning. arXiv preprint arXiv:2001.04385\n(2020)\n[30] Saemundsson, S., Terenin, A., Hofmann, K.,\nDeisenroth, M.: Variational integrator net-\nworks for physically structured embeddings.\nIn:\nInternational Conference on Artificial\nIntelligence and Statistics. pp. 3078–3087.\nPMLR (2020)\n[31] Sung, F., Yang, Y., Zhang, L., Xiang, T.,\nTorr, P.H., Hospedales, T.M.: Learning to\ncompare: Relation network for few-shot learn-\ning. In: Proceedings of the IEEE conference\non computer vision and pattern recognition.\npp. 1199–1208 (2018)\n[32] Toth, P., Rezende, D.J., Jaegle, A., Racani`ere,\nS.,\nBotev,\nA.,\nHiggins,\nI.:\nHamilto-\nnian generative networks. arXiv preprint\narXiv:1909.13789 (2019)\n[33] Wang, R., Walters, R., Yu, R.: Meta-learning\ndynamics forecasting using task inference. Ad-\nvances in Neural Information Processing Sys-\ntems 35, 21640–21653 (2022)\n[34] Wang, R., Walters, R., Yu, R.: Meta-learning\ndynamics forecasting using task inference. Ad-\nvances in Neural Information Processing Sys-\ntems 35, 21640–21653 (2022)\n[35] Yang, T.Y., Rosca, J., Narasimhan, K., Ra-\nmadge, P.J.: Learning physics constrained\ndynamics using autoencoders. Advances in\nNeural Information Processing Systems 35,\n17157–17172 (2022)\n[36] Yildiz, C., Heinonen, M., Lahdesmaki, H.:\nOde2vae: Deep generative second order odes\nwith bayesian neural networks. Advances in\nNeural Information Processing Systems 32\n(2019)\n[37] Yin, Y., Ayed, I., de B´ezenac, E., Baskiotis,\nN., Gallinari, P.: Leads: Learning dynamical\nsystems that generalize across environments.\nAdvances in Neural Information Processing\nSystems 34, 7561–7573 (2021)\n[38] Zeng, A., Song, S., Lee, J., Rodriguez, A.,\nFunkhouser, T.:\nTossingbot:\nLearning to\nthrow arbitrary objects with residual physics.\nIEEE Transactions on Robotics 36(4), 1307–\n1319 (2020)\n[39] Zhong, Y.D., Dey, B., Chakraborty, A.: Sym-\nplectic ode-net: Learning hamiltonian dynam-\nics with control. In: International Conference\non Learning Representations (2019)\n[40] Zhong, Y.D., Leonard, N.:\nUnsupervised\nlearning of lagrangian dynamics from images\nfor prediction and control. Advances in Neu-\nral Information Processing Systems 33, 10741–\n10752 (2020)\n11\nA\nData Details and Experimental Settings of Physics Systems\nHere we provide complete data details and experimental settings of physics systems and the overview\nof physics systems can be seen in Fig. 6. The ratio of training samples:ID testing samples:OOD testing\nsamples is roughly 6:2:2 in all datasets.\nA.1\nData Details\n(1) Pendulum: The dynamic equation is:\nd\ndt\n\u0014φ\n˙φ\n\u0015\n=\n\u0014\n˙φ\n−G\nL sin(φ) −β ˙φ\n\u0015\n, where φ is the angle, ˙φ is\nthe angular velocity, G, β are the gravitational constant and the damping coefficient, and L is the\nlength of the pendulum. We fix L = 2.0, then sample φ0 ∈[−1\n2π, 1\n2π], ˙φ0 ∈[−2.0, 2.0], G ∈[5.0, 15.0],\nID:β ∈(0.0, 1.0] and OOD: β ∈(1.0, 1.25]. We generate a 25 step in time domain [0.0s, 5.0s] following\nthe dynamic equation, and render corresponding 32 by 32 by 1 pixel observation snapshots. In total,\nwe generate 15390 training and test sequences.\n(2) Mass Spring: The dynamic equation is:\nd\ndt\n\u0014v1\nv2\n\u0015\n=\n\"\n−˜x\n|˜x|\nk\nm1 (|˜x| −l0) −\nβ\nm1 ˜v\n˜x\n|˜x|\nk\nm2 (|˜x| −l0) +\nβ\nm2 ˜v\n#\n, where ˜x = x1 −x2,\n˜v = v1 −v2, x1, x2 are the positions of node1 and node2, v1, v2 are the velocities of node1 and\nnode2 and k, β are the stiffness and damping coefficient of the spring and damper.\nWe fix\nl0 = 6.0, m1 = m2 = 1.0, v10 = v20 = 0, then sample x10 ∈[−4.0, −2.0], x2.0 ∈[2.0, 4.0], k ∈[5.0, 15.0],\nID: β ∈(0.0, 1.0] and OOD: β ∈(1.0, 1.5]. We generate a 25 step in time domain [0.0s, 5.0s] following\nthe dynamic equation, and render corresponding 32 by 32 by 3 pixel observation snapshots. In total,\nwe generate 12960 training and test sequences.\n(3) Bouncing Ball:\nThe dynamic equation is:\nd\ndt\n\u0014vx\nvy\n\u0015\n=\n\u0014G cos φ\nG sin φ\n\u0015\n, where G, φ are the am-\nplitude and direction of gravity. When there is a collision, vx = −vx or vy = −vy. We set the\ncollision position at x = 5.0, x = −5.0, y = 5.0, y = −5.0.\nWe fix vx0 = vy0 = 0, then sample\nx0 ∈[−4.0, 4.0], y0 ∈[−4.0, 4.0], G ∈(0.0, 5.0], φ = i\n8π, ID: i = [−6, −5, −4, −3, −2, −1, 2, 3, 4, 5, 6, 7]\nand OOD: i = [−8, −7, 0, 1]. We generate a 25 step in time domain [0.0s, 2.5s] following the dynamic\nequation, and render corresponding 32 by 32 by 1 pixel observation snapshots. In total, we generate\n21060 training and test sequences.\n(4) Rigid Body:\nWe represent the state of a 3D cube as q = [x, r] consisting of a position\nx ∈R3 and a quaternion r ∈R4.\nThe generalized velocity of the cube is u = [v, ω] and its\ndynamic equation is:\n\u0014m\n0\n0\nI\n\u0015 \u0014 ˙v\n˙ω\n\u0015\n=\n\u0014\nf\nr × f\n\u0015\n−\n\u0014\n0\nω × Iω\n\u0015\n, where m, I are the mass and inertia matrix,\nf = f · [cos ψ sin φ, sin ψ sin φ, cos φ] and f, ψ, φ are the magnitude of the force, its direction in the xy-\nplane and its direction with respect to the z-axis. We fix z0 = 0, r = [0, 0, 0, 1], v = [0, 0, 0], ω = [0, 0, 0]\nm = 8 and f = 10, then sample x0 ∈[−1.0, 1.0], y0 ∈[−1.0, 1.0], ψ ∈[−π, π), ID: φ ∈( 1\n8π, 1\n2π] and\nOOD: φ ∈[0, 1\n8π]. We generate a 30 step in time domain [0.0s, 1.0s] following the dynamic equation,\nand render corresponding 64 by 64 by 3 pixel observation snapshots. In total, we generate 4800 training\nand test sequences.\n(5)\nDouble\nPendulum:\nThe\ndynamic\nequation\nis:\nd\ndt\n\u0014 ˙φ1\n˙φ2\n\u0015\n=\n\n\nG sin φ2ϕ1−ϕ2(L1 ˙φ2\n1ϕ1+L2 ˙φ2\n2)−( ˜m+1)G sin φ1\nL1( ˜m+ϕ2\n2)\n( ˜m+1)(L1 ˙φ2\n1ϕ2−G sin φ2+G sin φ1ϕ1)+L2 ˙φ2\n2ϕ1ϕ2\nL2( ˜m+ϕ2\n2)\n\n, where ˜m = m1/m2, ϕ1 = cos(φ1 −φ2), ϕ2 = sin(φ1 −φ2)\nand m1, m2 are the mass of two pendulums. We fix L1 = L2 = 1.5, ˙φ10 = ˙φ10 = 0, then sample\nφ10 ∈[−1\n2π, 1\n2π], φ20 ∈[−1\n8π, 1\n8π], G ∈[5.0, 15.0], ID: ˜m ∈[0.5, 1.5] and OOD: ˜m ∈(1.5, 2.0]. We\ngenerate a 40 step in time domain [0.0s, 4.0s] following the dynamic equation, and render corresponding\n32 by 32 by 3 pixel observation snapshots. In total, we generate 12705 training and test sequences.\nA.2\nExperimental Settings for section 4.3\nHere we provide specific forms of different latent dynamic functions in section 4.3 for each datasets.\n12\nFigure 6: The overview of five physics systems\nA.2.1\nPendulum\nTable 3: Experimental Setting of Pendulum\nDynamics\nEquation\nFull Physics\nd\ndt\n\u0014φ\n˙φ\n\u0015\n=\n\u0014\n˙φ\n−G\nL sin(φ) −β ˙φ\n\u0015\nPurely Physics\nd\ndt\n\u0014φ\n˙φ\n\u0015\n=\n\u0014\n˙φ\n−G\nL sin(φ)\n\u0015\nPurely Neural\nd\ndt\n\u0014φ\n˙φ\n\u0015\n= fNNϕ(φ, ˙φ; cn)\nGlobal-HyLaD\nd\ndt\n\u0014φ\n˙φ\n\u0015\n=\n\u0014\n˙φ\n−G\nL sin(φ)\n\u0015\n+ fNNϕ( ˙φ)\nMeta-HyLaD\nd\ndt\n\u0014φ\n˙φ\n\u0015\n=\n\u0014\n˙φ\n−G\nL sin(φ)\n\u0015\n+ fNNϕ( ˙φ; cn)\nA.2.2\nMass Spring\nTable 4: Experimental Setting of Mass Spring\nDynamics\nEquation\nFull Physics\nd\ndt\n\u0014v1\nv2\n\u0015\n=\n\"\n−˜x\n|˜x|\nk\nm1 (|˜x| −l0) −\nβ\nm1 ˜v\n˜x\n|˜x|\nk\nm2 (|˜x| −l0) +\nβ\nm2 ˜v\n#\nPurely Physics\nd\ndt\n\u0014\nv1\nv2\n\u0015\n=\n\"\n−˜x\n|˜x|\nk\nm1 (|˜x| −l0)\n˜x\n|˜x|\nk\nm2 (|˜x| −l0)\n#\nPurely Neural\nd\ndt\n\u0014v1\nv2\n\u0015\n= fNNϕ(x1, x2, v1, v2; cn)\nGlobal-HyLaD\nd\ndt\n\u0014v1\nv2\n\u0015\n=\n\"\n−˜x\n|˜x|\nk\nm1 (|˜x| −l0)\n˜x\n|˜x|\nk\nm2 (|˜x| −l0)\n#\n+ fNNϕ(v1, v2)\nMeta-HyLaD\nd\ndt\n\u0014\nv1\nv2\n\u0015\n=\n\"\n−˜x\n|˜x|\nk\nm1 (|˜x| −l0)\n˜x\n|˜x|\nk\nm2 (|˜x| −l0)\n#\n+ fNNϕ(v1, v2; cn)\nA.2.3\nBouncing Ball\n13\nTable 5: Experimental Setting of Bouncing Ball\nDynamics\nEquation\nFull Physics\nd\ndt\n\u0014\nvx\nvy\n\u0015\n=\n\u0014\nG cos φ\nG sin φ\n\u0015\nPurely Physics\nd\ndt\n\u0014vx\nvy\n\u0015\n=\n\u0014 0\n−G\n\u0015\nPurely Neural\nd\ndt\n\u0014vx\nvy\n\u0015\n= fNNϕ(G; cn)\nGlobal-HyLaD\nd\ndt\n\u0014vx\nvy\n\u0015\n=\n\u0014 0\n−G\n\u0015\n+ fNNϕ(G)\nMeta-HyLaD\nd\ndt\n\u0014vx\nvy\n\u0015\n=\n\u0014 0\n−G\n\u0015\n+ fNNϕ(G; cn)\nA.2.4\nRigid Body\nTable 6: Experimental Setting of Rigid Body\nDynamics\nEquation\nFull Physics\nf = f · [cos ψ sin φ, sin ψ sin φ, cos φ]\nPurely Physics\nf = f · [cos ψ, sin ψ, 0]\nPurely Neural\nf = fNNϕ(ψ; cn)\nGlobal-HyLaD\nf = f · [cos ψ, sin ψ, 0] + fNNϕ(ψ)\nMeta-HyLaD\nf = f · [cos ψ, sin ψ, 0] + fNNϕ(ψ; cn)\nA.2.5\nDouble Pendulum\nTable 7: Experimental Setting of Double Pendulum\nDynamics\nEquation\nFull Physics\nd\ndt\n\u0014\n˙φ1\n˙φ2\n\u0015\n=\n\n\nG sin φ2ϕ1−ϕ2(L1 ˙φ2\n1ϕ1+L2 ˙φ2\n2)−( ˜m+1)G sin φ1\nL1( ˜m+ϕ2\n2)\n( ˜m+1)(L1 ˙φ2\n1ϕ2−G sin φ2+G sin φ1ϕ1)+L2 ˙φ2\n2ϕ1ϕ2\nL2( ˜m+ϕ2\n2)\n\n\nPurely Physics\nd\ndt\n\u0014 ˙φ1\n˙φ2\n\u0015\n=\n\n\nG sin φ2ϕ1−ϕ2(L1 ˙φ2\n1ϕ1+L2 ˙φ2\n2)−2G sin φ1\nL1(1+ϕ2\n2)\n2(L1 ˙φ2\n1ϕ2−G sin φ2+G sin φ1ϕ1)+L2 ˙φ2\n2ϕ1ϕ2\nL2(1+ϕ2\n2)\n\n\nPurely Neural\nd\ndt\n\u0014\n˙φ1\n˙φ2\n\u0015\n= fNNϕ(φ1, φ2, ˙φ1, ˙φ2; cn)\nGlobal-HyLaD\nd\ndt\n\u0014 ˙φ1\n˙φ2\n\u0015\n=\n\n\nG sin φ2ϕ1−ϕ2(L1 ˙φ2\n1ϕ1+L2 ˙φ2\n2)−2G sin φ1\nL1(1+ϕ2\n2)\n2(L1 ˙φ2\n1ϕ2−G sin φ2+G sin φ1ϕ1)+L2 ˙φ2\n2ϕ1ϕ2\nL2(1+ϕ2\n2)\n\n+ fNNϕ(φ1, φ2, ˙φ1, ˙φ2)\nMeta-HyLaD\nd\ndt\n\u0014\n˙φ1\n˙φ2\n\u0015\n=\n\n\nG sin φ2ϕ1−ϕ2(L1 ˙φ2\n1ϕ1+L2 ˙φ2\n2)−2G sin φ1\nL1(1+ϕ2\n2)\n2(L1 ˙φ2\n1ϕ2−G sin φ2+G sin φ1ϕ1)+L2 ˙φ2\n2ϕ1ϕ2\nL2(1+ϕ2\n2)\n\n+ fNNϕ(φ1, φ2, ˙φ1, ˙φ2; cn)\nB\nExamples of Identified Neural Dynamic Functions (Addi-\ntioanl Results for section 4.2)\nWe also visualize the neural dynamic function fNNθ(zt; cn) of three learning strategies and ground\ntruth −β ˙φ, see Fig 7. From figure, we can clearly see that all three learning strategies perform\nwell on reconstruction task, but Recon-HyLaD will fail on prediction task, which shows that a good\nreconstruction is not sufficient evidence that the underlying latent dynamics is correctly identified and\nthe learn-to-identify strategy is an effective solution to address this issue of identifiability.\n14\nFigure 7: Visualization of different learning strategies\nC\nAdditional Results of section 4.3: Benefits of Hybrid Dy-\nnamics\nHere we provide complete experimental results and some visual examples of section 4.3.\nC.1\nPendulum\nTable 8: Results of Pendulum\nDynamics\nData\nMSE xt (e−3)↓\nMSE zt (e−2) ↓\nVPT xt ↑\nVPT of zt ↑\nFull Physics\n(oracle)\nID\n0.31(0.05)\n0.24(0.06)\n0.99(0.00)\n0.98(0.01)\nOOD\n0.62(0.06)\n0.48(0.08)\n0.97(0.01)\n0.88(0.03)\nPurely Physics\nID\n4.38(0.00)\n13.09(0.18)\n0.13(0.19)\n0.05(0.01)\nOOD\n4.94(0.07)\n19.57(0.55)\n0.05(0.00)\n0.01(0.00)\nPurely Neural\nID\n3.36(0.52)\n109.96(54.82)\n0.53(0.07)\n0.00(0.00)\nOOD\n4.02(0.54)\n78.47(35.18)\n0.43(0.03)\n0.00(0.00)\nGlobal-HyLaD\nID\n2.26(0.05)\n4.01(0.05)\n0.60(0.01)\n0.41(0.01)\nOOD\n2.99(0.06)\n6.50(0.02)\n0.38(0.02)\n0.17(0.00)\nMeta-HyLaD\nID\n0.30(0.01)\n0.22(0.00)\n0.99(0.00)\n0.98(0.00)\nOOD\n0.25(0.02)\n0.17(0.02)\n0.99(0.00)\n0.95(0.01)\n15\nFigure 8: Visual examples of Pendulum\nC.2\nMass Spring\nTable 9: Results of Mass Spring\nDynamics\nData\nMSE xt (e−4)↓\nMSE zt (e−2) ↓\nVPT xt ↑\nVPT of zt ↑\nFull Physics\n(oracle)\nID\n0.12(0.01)\n0.09(0.02)\n1.00(0.00)\n1.00(0.00)\nOOD\n0.40(0.03)\n0.46(0.07)\n0.98(0.01)\n0.88(0.02)\nPurely Physics\nID\n6.32(0.09)\n19.01(0.56)\n0.44(0.09)\n0.14(0.00)\nOOD\n8.83(0.03)\n27.93(0.21)\n0.17(0.00)\n0.05(0.00)\nPurely Neural\nID\n1.86(0.13)\n393.47(75.06)\n0.91(0.01)\n0.05(0.00)\nOOD\n2.51(0.39)\n348.02(93.33)\n0.77(0.07)\n0.05(0.00)\nGlobal-HyLaD\nID\n1.53(0.01)\n3.20(0.13)\n0.92(0.01)\n0.70(0.03)\nOOD\n2.45(0.05)\n4.68(0.16)\n0.76(0.02)\n0.28(0.02)\nMeta-HyLaD\nID\n0.13(0.03)\n0.09(0.02)\n1.00(0.00)\n1.00(0.00)\nOOD\n0.19(0.07)\n0.12(0.06)\n1.00(0.00)\n0.98(0.01)\n16\nFigure 9: Visual examples of Mass Spring\nC.3\nBouncing Ball\nTable 10: Results of Boucing Ball\nDynamics\nData\nMSE xt (e−2)↓\nMSE zt (e−1) ↓\nVPT xt ↑\nVPT of zt ↑\nFull Physics\n(oracle)\nID\n0.05(0.02)\n0.03(0.02)\n1.00(0.01)\n0.96(0.04)\nOOD\n0.14(0.06)\n0.14(0.09)\n0.93(0.04)\n0.84(0.06)\nPurely Physics\nID\n3.20(0.10)\n15.54(0.16)\n0.09(0.02)\n0.04(0.02)\nOOD\n3.67(0.13)\n17.73(0.70)\n0.03(0.01)\n0.01(0.00)\nPurely Neural\nID\n1.26(0.01)\n2.55(0.01)\n0.10(0.01)\n0.02(0.00)\nOOD\n1.92(0.23)\n7.26(2.98)\n0.12(0.02)\n0.03(0.01)\nGlobal-HyLaD\nID\n2.15(0.04)\n8.33(0.19)\n0.21(0.03)\n0.12(0.02)\nOOD\n3.24(0.04)\n15.21(0.32)\n0.06(0.00)\n0.03(0.00)\nMeta-HyLaD\nID\n0.06(0.02)\n0.03(0.01)\n1.00(0.00)\n0.94(0.03)\nOOD\n0.16(0.05)\n0.17(0.07)\n0.92(0.02)\n0.80(0.05)\n17\nFigure 10: Visual examples of Bouncing Ball\nC.4\nRigid Body\nTable 11: Results of Rigid Body\nDynamics\nData\nMSE xt (e−3)↓\nMSE zt (e−2) ↓\nVPT xt ↑\nVPT of zt ↑\nFull Physics\n(oracle)\nID\n0.37(0.19)\n0.02(0.01)\n1.00(0.01)\n1.00(0.00)\nOOD\n1.26(0.02)\n0.09(0.00)\n0.94(0.03)\n0.98(0.02)\nPurely Physics\nID\n6.46(0.08)\n2.56(0.02)\n0.59(0.00)\n0.59(0.00)\nOOD\n18.23(0.23)\n10.36(0.24)\n0.29(0.00)\n0.38(0.00)\nPurely Neural\nID\n5.01(0.17)\n15.92(0.93)\n0.58(0.01)\n0.00(0.00)\nOOD\n12.29(0.27)\n19.56(0.39)\n0.36(0.03)\n0.00(0.00)\nGlobal-HyLaD\nID\n2.90(0.33)\n0.48(0.08)\n0.79(0.02)\n0.86(0.01)\nOOD\n14.41(0.92)\n5.28(1.00)\n0.34(0.02)\n0.46(0.02)\nMeta-HyLaD\nID\n0.74(0.34)\n0.05(0.04)\n0.98(0.02)\n0.99(0.01)\nOOD\n1.74(0.12)\n0.13(0.01)\n0.90(0.01)\n0.97(0.00)\n18\nFigure 11: Visual examples of Rigid Body\nC.5\nDouble Pendulum\nTable 12: Results of Double Pendulum\nDynamics\nData\nMSE xt (e−3)↓\nMSE zt (e−1) ↓\nVPT xt ↑\nVPT of zt ↑\nFull Physics\n(oracle)\nID\n0.18(0.01)\n0.13(0.01)\n1.00(0.00)\n0.99(0.00)\nOOD\n0.92(0.11)\n2.14(0.37)\n0.95(0.01)\n0.95(0.00)\nPurely Physics\nID\n3.93(0.01)\n8.37(0.89)\n0.47(0.01)\n0.36(0.01)\nOOD\n3.03(0.08)\n4.08(0.51)\n0.56(0.03)\n0.59(0.02)\nPurely Neural\nID\n3.41(0.02)\n35.73(0.00)\n0.61(0.00)\n0.13(0.01)\nOOD\n4.10(0.17)\n37.35(0.00)\n0.57(0.04)\n0.14(0.01)\nGlobal-HyLaD\nID\n1.64(0.01)\n2.79(0.03)\n0.86(0.00)\n0.78(0.00)\nOOD\n2.84(0.17)\n6.55(0.26)\n0.72(0.03)\n0.75(0.01)\nMeta-HyLaD\nID\n0.52(0.05)\n0.38(0.03)\n0.98(0.00)\n0.93(0.00)\nOOD\n1.10(0.06)\n2.97(0.19)\n0.94(0.01)\n0.95(0.01)\n19\nFigure 12: Visual examples of Double Pendulum\nD\nComplete Results of section 4.4: Comparison with Baselines\nHere we provide complete experimental results of section 4.4.\n20\nTable 13: Results of different baselines and Meta-HyLaD 1\nPendulum\nPhysics-based Decoder\nNeural Decoder with prior\nNeural Decoder no prior\nModel\nData\nMSE(e−3)\nVPT\nMSE(e−3)\nVPT\nMSE(e−3)\nVPT\nLSTM\nID\n7.38(1.90)\n0.09(0.05)\n/\n/\n4.75(0.46)\n0.18(0.12)\nOOD\n5.36(0.74)\n0.10(0.03)\n/\n/\n4.09(0.19)\n0.19(0.12)\nMeta-LSTM\nID\n7.13(1.13)\n0.04(0.02)\n/\n/\n5.13(0.01)\n0.04(0.00)\nOOD\n5.16(0.49)\n0.04(0.01)\n/\n/\n3.68(0.01)\n0.04(0.00)\nNODE\nID\n8.56(0.14)\n0.06(0.01)\n/\n/\n4.21(0.02)\n0.28(0.02)\nOOD\n5.16(0.44)\n0.07(0.01)\n/\n/\n3.85(0.11)\n0.30(0.03)\nMeta-NODE\nID\n8.52(0.05)\n0.05(0.00)\n/\n/\n2.88(0.01)\n0.55(0.00)\nOOD\n5.35(0.11)\n0.07(0.00)\n/\n/\n3.50(0.10)\n0.43(0.01)\nHGN\nID\n6.47(0.17)\n0.11(0.03)\n3.53(0.02)\n0.48(0.01)\n2.99(0.03)\n0.62(0.01)\nOOD\n5.64(0.27)\n0.09(0.03)\n3.69(0.15)\n0.37(0.02)\n3.10(0.17)\n0.47(0.03)\nMeta-HGN\nID\n9.69(0.15)\n0.03(0.00)\n3.04(0.17)\n0.54(0.01)\n0.67(0.08)\n0.98(0.00)\nOOD\n6.24(0.14)\n0.04(0.00)\n3.69(0.08)\n0.42(0.01)\n0.72(0.05)\n0.91(0.02)\nALPS\nID\n3.56(0.03)\n0.16(0.01)\n2.70(0.01)\n0.28(0.01)\n2.68(0.02)\n0.27(0.02)\nOOD\n4.17(0.00)\n0.04(0.00)\n3.28(0.01)\n0.06(0.00)\n3.25(0.03)\n0.06(0.00)\nMeta-HyLaD\nID\n0.30(0.02)\n0.99(0.00)\n0.29(0.03)\n0.99(0.00)\n0.25(0.01)\n0.99(0.00)\nOOD\n0.31(0.06)\n0.99(0.00)\n0.25(0.02)\n0.99(0.00)\n0.20(0.04)\n1.00(0.00)\nMass Spring\nPhysics-based Decoder\nNeural Decoder with prior\nNeural Decoder no prior\nModel\nData\nMSE(e−3)\nVPT\nMSE(e−3)\nVPT\nMSE(e−3)\nVPT\nLSTM\nID\n16.59(12.95)\n0.01(0.01)\n/\n/\n0.40(0.03)\n0.56(0.06)\nOOD\n16.51(13.11)\n0.01(0.01)\n/\n/\n0.20(0.02)\n0.67(0.09)\nMeta-LSTM\nID\n19.48(8.84)\n0.00(0.00)\n/\n/\n0.40(0.03)\n0.59(0.06)\nOOD\n19.44(8.94)\n0.00(0.00)\n/\n/\n0.20(0.01)\n0.60(0.02)\nNODE\nID\n0.80(0.01)\n0.15(0.01)\n/\n/\n0.38(0.01)\n0.62(0.02)\nOOD\n0.38(0.01)\n0.25(0.01)\n/\n/\n0.19(0.01)\n0.67(0.03)\nMeta-NODE\nID\n0.69(0.09)\n0.21(0.05)\n/\n/\n0.27(0.02)\n0.77(0.01)\nOOD\n0.26(0.10)\n0.44(0.16)\n/\n/\n0.24(0.01)\n0.69(0.02)\nHGN\nID\n10.33(7.47)\n0.00(0.00)\n0.27(0.02)\n0.84(0.02)\n0.15(0.01)\n0.95(0.01)\nOOD\n10.11(7.60)\n0.23(0.03)\n0.11(0.01)\n0.78(0.04)\n0.11(0.01)\n0.85(0.03)\nMeta-HGN\nID\n5.96(3.90)\n0.00(0.00)\n0.18(0.02)\n0.85(0.03)\n0.08(0.02)\n1.00(0.00)\nOOD\n5.66(3.96)\n0.00(0.00)\n0.18(0.03)\n0.76(0.02)\n0.06(0.01)\n0.99(0.00)\nALPS\nID\n17.52(7.60)\n0.00(0.00)\n2.59(0.80)\n0.03(0.03)\n1.40(0.59)\n0.10(0.10)\nOOD\n17.67(7.64)\n0.00(0.00)\n2.25(1.07)\n0.05(0.05)\n1.16(0.61)\n0.13(0.11)\nMeta-HyLaD\nID\n0.01(0.00)\n1.00(0.00)\n0.03(0.00)\n1.00(0.00)\n0.04(0.01)\n1.00(0.00)\nOOD\n0.02(0.01)\n1.00(0.00)\n0.03(0.00)\n1.00(0.00)\n0.03(0.01)\n1.00(0.00)\n21\nTable 14: Results of different baselines and Meta-HyLaD 2\nBouncing Ball\nPhysics-based Decoder\nNeural Decoder with prior\nNeural Decoder no prior\nModel\nData\nMSE(e−3)\nVPT\nMSE(e−3)\nVPT\nMSE(e−3)\nVPT\nLSTM\nID\n62.35(7.22)\n0.00(0.00)\n/\n/\n30.74(1.94)\n0.00(0.00)\nOOD\n60.29(5.78)\n0.00(0.00)\n/\n/\n34.25(0.29)\n0.00(0.00)\nMeta-LSTM\nID\n79.03(9.22)\n0.01(0.00)\n/\n/\n26.37(0.93)\n0.00(0.00)\nOOD\n76.66(11.14)\n0.01(0.00)\n/\n/\n35.84(2.29)\n0.00(0.00)\nNODE\nID\n38.74(0.17)\n0.03(0.00)\n/\n/\n22.33(1.13)\n0.00(0.00)\nOOD\n42.17(0.73)\n0.01(0.00)\n/\n/\n33.37(1.18)\n0.00(0.00)\nMeta-NODE\nID\n12.64(0.14)\n0.10(0.01)\n/\n/\n10.16(0.28)\n0.00(0.00)\nOOD\n19.21(2.33)\n0.12(0.02)\n/\n/\n18.04(7.94)\n0.10(0.00)\nHGN\nID\n33.92(3.69)\n0.01(0.00)\n15.65(7.04)\n0.06(0.02)\n11.41(3.04)\n0.15(0.05)\nOOD\n39.46(5.69)\n0.01(0.01)\n18.88(4.98)\n0.07(0.01)\n21.40(1.72)\n0.07(0.02)\nMeta-HGN\nID\n18.70(6.67)\n0.02(0.01)\n3.87(0.27)\n0.70(0.04)\n3.48(0.97)\n0.76(0.11)\nOOD\n25.85(4.39)\n0.01(0.01)\n14.69(7.79)\n0.50(0.13)\n32.09(6.54)\n0.31(0.04)\nALPS\nID\n34.15(2.46)\n0.04(0.01)\n21.40(0.20)\n0.03(0.00)\n15.15(0.63)\n0.03(0.01)\nOOD\n33.95(1.22)\n0.01(0.00)\n24.92(1.12)\n0.02(0.01)\n23.54(4.06)\n0.02(0.01)\nMeta-HyLaD\nID\n0.56(0.19)\n1.00(0.00)\n2.13(1.37)\n0.94(0.08)\n4.05(0.71)\n0.68(0.06)\nOOD\n1.60(0.50)\n0.92(0.02)\n4.46(1.25)\n0.81(0.04)\n5.92(1.22)\n0.61(0.02)\nRigid Body\nPhysics-based Decoder\nNeural Decoder with prior\nNeural Decoder no prior\nModel\nData\nMSE(e−3)\nVPT\nMSE(e−3)\nVPT\nMSE(e−3)\nVPT\nLSTM\nID\n21.43(3.99)\n0.02(0.03)\n/\n/\n10.40(0.11)\n0.35(0.01)\nOOD\n19.63(5.15)\n0.02(0.04)\n/\n/\n7.60(0.46)\n0.43(0.02)\nMeta-LSTM\nID\n18.80(4.85)\n0.06(0.05)\n/\n/\n3.97(0.69)\n0.72(0.11)\nOOD\n16.66(6.27)\n0.08(0.07)\n/\n/\n5.58(1.77)\n0.60(0.15)\nNODE\nID\n17.73(6.42)\n0.22(0.18)\n/\n/\n9.29(0.01)\n0.42(0.00)\nOOD\n15.55(8.24)\n0.27(0.23)\n/\n/\n6.33(0.07)\n0.50(0.00)\nMeta-NODE\nID\n4.65(1.56)\n0.60(0.11)\n/\n/\n1.51(0.08)\n0.99(0.00)\nOOD\n5.00(1.98)\n0.58(0.15)\n/\n/\n1.70(0.10)\n0.96(0.02)\nHGN\nID\n11.76(3.42)\n0.32(0.01)\n2.65(0.13)\n0.85(0.02)\n6.36(0.66)\n0.61(0.02)\nOOD\n10.38(1.61)\n0.35(0.08)\n3.87(0.47)\n0.73(0.04)\n11.59(1.97)\n0.47(0.02)\nMeta-HGN\nID\n6.77(1.56)\n0.61(0.02)\n1.44(0.04)\n1.00(0.00)\n5.45(0.22)\n0.62(0.03)\nOOD\n7.30(2.10)\n0.98(0.02)\n1.62(0.03)\n0.98(0.01)\n5.53(1.03)\n0.62(0.08)\nALPS\nID\n5.03(0.57)\n0.54(0.05)\n2.53(0.18)\n0.92(0.03)\n5.77(1.13)\n0.54(0.12)\nOOD\n3.57(0.14)\n0.64(0.02)\n2.81(0.15)\n0.85(0.03)\n10.62(2.91)\n0.42(0.14)\nMeta-HyLaD\nID\n0.74(0.34)\n0.98(0.02)\n1.75(0.10)\n0.99(0.00)\n5.11(0.22)\n0.63(0.02)\nOOD\n1.74(0.13)\n0.90(0.01)\n1.80(0.07)\n0.99(0.01)\n7.69(2.08)\n0.47(0.08)\nDouble Pendulum\nPhysics-based Decoder\nNeural Decoder with prior\nNeural Decoder no prior\nModel\nData\nMSE(e−3)\nVPT\nMSE(e−3)\nVPT\nMSE(e−3)\nVPT\nLSTM\nID\n3.92(0.41)\n0.48(0.02)\n/\n/\n2.79(0.25)\n0.68(0.11)\nOOD\n4.64(0.28)\n0.43(0.01)\n/\n/\n3.05(0.25)\n0.60(0.08)\nMeta-LSTM\nID\n3.57(0.25)\n0.57(0.02)\n/\n/\n2.32(0.05)\n0.83(0.02)\nOOD\n4.25(0.15)\n0.53(0.02)\n/\n/\n2.59(0.06)\n0.76(0.02)\nNODE\nID\n5.68(1.53)\n0.30(0.14)\n/\n/\n1.89(0.13)\n0.90(0.01)\nOOD\n6.03(1.46)\n0.26(0.14)\n/\n/\n2.42(0.08)\n0.78(0.01)\nMeta-NODE\nID\n4.40(0.12)\n0.40(0.01)\n/\n/\n1.37(0.06)\n0.97(0.00)\nOOD\n4.87(0.35)\n0.36(0.01)\n/\n/\n2.05(0.05)\n0.84(0.01)\nHGN\nID\n3.88(0.24)\n0.48(0.02)\n1.61(0.03)\n0.91(0.01)\n1.16(0.04)\n0.96(0.01)\nOOD\n4.57(0.33)\n0.44(0.01)\n2.25(0.10)\n0.78(0.01)\n1.91(0.16)\n0.84(0.03)\nMeta-HGN\nID\n5.29(1.82)\n0.33(0.16)\n1.50(0.13)\n0.94(0.03)\n0.64(0.03)\n1.00(0.00)\nOOD\n5.89(1.55)\n0.30(0.15)\n2.09(0.08)\n0.82(0.01)\n1.18(0.02)\n0.95(0.01)\nALPS\nID\n2.98(0.04)\n0.42(0.00)\n1.87(0.05)\n0.84(0.02)\n1.17(0.27)\n0.98(0.02)\nOOD\n2.47(0.10)\n0.67(0.01)\n1.99(0.06)\n0.83(0.04)\n1.66(0.27)\n0.91(0.03)\nMeta-HyLaD\nID\n0.52(0.05)\n0.98(0.00)\n1.63(0.11)\n0.92(0.03)\n1.15(0.11)\n0.98(0.01)\nOOD\n1.10(0.06)\n0.94(0.01)\n2.24(0.06)\n0.80(0.00)\n1.77(0.14)\n0.89(0.02)\n22\nE\nData Details and Experimental Settings of Biomedical Sys-\ntem: Dynamic PET‘\nIn our experiment, we also consider a biomedical system: dynamic PET, which experiment can show\nthat hybrid latent dynamics can learn a complex model with the help of a simple model. Here we\nprovide more details of it.\nE.1\nBackground\nDynamic positron emission tomography (PET) imaging can provide measurements of spatial and\ntemporal distribution of radiotracer-labeled biological substrates in living tissue[27]. Since Dynamic\nPET reconstruction is an ill-conditioned problem, the significance of incorporating prior knowledge\ninto statistical reconstruction is well appreciated. The temporal kinetics of underlying physiological\nprocesses is an important physical prior in dynamic PET and various models have been proposed to\nmodel the tracer kinetics , which convert the radiotracer concentrations reconstructed from PET data\ninto measures of the physiological processes.\nCompartment model:\nDue to the simple implementation and biological plausibility, compartment\nmodels have been widely employed to quantitatively describe regional tracer kinetics in PET imaging\nwhere one need to postulate a linear or nonlinear structure in a number of compartments and their\ninterconnections, and resolve them from the measurement data. Depending on the number of com-\npartments and the complexity of the model, compartment model can be categorized into one-tissue\ncompartment model, two-tissue compartment model and so on. In our experiment, we only consider\none-tissue compartment model and two-tissue compartment model, which are written as:\none-tissue compartment model: ˙CT i(t) = −k2CT i(t) + k1CP (t)\ntwo-tissue compartment model:\n\u0014 ˙CEi(t)\n˙CMi(t)\n\u0015\n=\n\u0014−k2 −k3\nk4\nk3\n−k4\n\u0015 \u0014CEi(t)\nCMi(t)\n\u0015\n+\n\u0014k1\n0\n\u0015\nCP (t)\nwhere CP (t): a space-invariant tracer delivery, CEi(t), CMi(t): the concentration of radioactivity for\nany voxel i(i = 1, . . . , N) in different tissues(compartments) and CT i(t): the total concentration of\nradioactivity in all tissues. For computational simplicity and without losing generality, we consider\nthat CT i(t) = CEi(t) + CMi(t) in two-tissue compartment model. And we assume the input CP (t) is\nknown here, since CP (t) can be measured directly in practice.\nImaging model:\nDynamic PET imaging involves a sequence of contiguous acquisition and a time\nseries of activity images need to be reconstructed from the measurement data. For voxel i(i = 1, . . . , N),\nthe kth scan k = 1, . . . , K attempts to measure the mean of the total concentration of radioactivity on\nthe scanning time interval [tk−1, tk], so the measured activity in scan k for voxel i is expressed as:\nxik(t) =\n1\ntk −tk−1\nZ tk\ntk−1\nCT i(t)e−t\nτ dt\n(11)\nwhere e−t\nτ is the attenuation factor of radiotracer, The activity image of the kth scan xk is obtained by\nlexicographic ordering of the integrated radioactivity at different voxels xik. The raw data obtained by\nPET imaging device is sinogram yk = Dxk + gk, where D is the imaging matrix and gk is measurement\nerrors. In our experiment, we simplify the original PET reconstruction problem by considering xk as\nthe system observation and CT (t) as the system state zt.\nFigure 13: One-tissue and two-tissue compartment model in Dynamic PET\n23\nE.2\nData Details\nIn experiment, we use the model proposed in [7] for the input CP (t).\nCP (t) = (A1t −A2 −A3)eλ1t + A2eλ2t + A3eλ3t\n(12)\nwhere λ1, λ2, and λ3 (in min−1) are the eigenvalues of the model and A1 (in µCi/ml/min), A2 and A3\n(in µCi/ml) are the coefficients of the model. In experiment, we fix A2 = 21.8798, A3 = 20.8113, λ1 =\n4.1339, λ2 = 0.1191, λ3 = 0.0104 and τ = 110, then sample A1 in range [100, 200]. And for two-tissue\ncompartment model, we sample k1 in range [0.1, 0.3], k2 in range [0.01, 0.3], k3 in range [0.01, 0.1] and\nk4 in range [0.01, 0.05] according to [15]. We generate concentration images CT (t) of a brain with 5\nregions of interesting (ROIs) in time domain 0-40 min and we assume the pixels (voxels) in the same\nROI share the same k1, k2, k3, k4. For activity images o(t), the scanning period is 0.5 min, thus k = 80.\nIn total, we generate 12705 training and test sequences.\nE.3\nExperimental Settings\nDynamics\nEquation\nFull Physics\n\u0014 ˙CEi(t)\n˙CMi(t)\n\u0015\n=\n\u0014\n−k2 −k3\nk4\nk3\n−k4\n\u0015 \u0014\nCEi(t)\nCMi(t)\n\u0015\n+\n\u0014\nk1\n0\n\u0015\nCP (t)\nPurely Physics\n˙CT i(t) = −k2CT i(t) + k1CP (t)\nPurely Neural\n˙CT i(t) = fNNϕ(CT i(t); cn)\nGlobal-HyLaD\n˙CT i(t) = −k2CT i(t) + k1CP (t) + fNNϕ(CT i(t))\nMeta-HyLaD\n˙CT i(t) = −k2CT i(t) + k1CP (t) + fNNϕ(CT i(t); cn)\nE.4\nExperimental Results\nTable 15: Results of Dynamic PET\nDynamics\nMSE of ot ↓\nMSE of xt ↓\nPSNR of ot ↑\nVPT-MSE of xt ↑\nFull Physics\n9.34(1.66)e-3\n1.07(0.26)e-1\n58.00(0.83)\n0.99(0.01)\nPurely Physics\n1.08(0.01)e0\n5.36(0.06)e0\n36.84(0.25)\n0.00(0.00)\nPurely Neural\n2.77(0.35)e-1\n1.50(0.32)e0\n43.32(0.58)\n0.30(0.08)\nGloba-HyLaD\n8.83(0.75)e-2\n4.66(0.50)e-1\n48.26(0.41)\n0.90(0.05)\nMeta-HyLaD\n1.37(0.15)e-2\n1.13(0.18)e-1\n56.39(0.50)\n1.00(0.00)\n24\nFigure 14: Dynamic PET\nF\nAblation Study\nF.1\nThe effect of the size of context set\nWe test the effect of k on k-shot context set on two cases: 1) training and testing on the same and\nfixed k. 2) training on the variable k, but testing on fixed or variable k.\nTable 16: Training and Testing on the same and fixed k\nK\nMSE of xt(e-4) ↓\nMSE of zt(e-3) ↓\nVPT-MSE of xt ↑\nVPT-MSE of zt ↑\n1\n4.90(0.41)\n6.08(0.12)\n0.98(0.00)\n0.96(0.01)\n3\n3.76(0.21)\n3.85(0.92)\n0.98(0.00)\n0.97(0.00)\n5\n3.35(0.21)\n2.53(0.09)\n0.99(0.00)\n0.98(0.00)\n7\n2.99(0.20)\n2.24(0.20)\n0.99(0.00)\n0.98(0.00)\n9\n3.09(0.41)\n2.21(0.38)\n0.99(0.00)\n0.99(0.00)\nTable 17: Training on variable k between [1, 9], but testing on fixed or variable k\nK\nMode\nMSE of xt(e-4) ↓\nMSE of zt(e-3) ↓\nVPT-MSE of xt ↑\nVPT-MSE of zt ↑\n1\nFixed\n7.14(1.17)\n8.72(2.24)\n0.95(0.01)\n0.91(0.02)\n3\nFixed\n4.79(0.39)\n4.97(0.72)\n0.98(0.01)\n0.96(0.01)\n5\nFixed\n3.98(0.38)\n3.37(0.35)\n0.98(0.00)\n0.97(0.01)\n7\nFixed\n3.43(0.35)\n2.64(0.22)\n0.99(0.00)\n0.98(0.00)\n9\nFixed\n3.12(0.28)\n2.27(0.19)\n0.99(0.00)\n0.98(0.00)\nVariable\n3.88(0.23)\n3.50(0.14)\n0.99(0.00)\n0.97(0.00)\nF.2\nThe robustness to different level of noise\nWe test the rubustness of Meta-HyLaD to different level of noise on observed time-series. The SNR of\nobserved time-series is in range [5dB, 30dB].\nF.3\nGenerality of Meta-HyLaD\nWe investigate different choices of modeling the neural component fNNϕ in Meta-HyLaD. As an example,\non Pendulum, we compare two types of Meta-HyLaD: Meta-HyLaD with prior where the input to fNNϕ\nis selected to be the state variable ˙φ as informed by physics, and a more general version of Meta-HyLaD\n25\nTable 18: The robustness to different level of noise\nSNR\nMSE of xt(e-4) ↓\nMSE of zt(e-3) ↓\nVPT-MSE of xt ↑\nVPT-MSE of zt ↑\n5 dB\n4.02(0.17)\n3.33(0.16)\n0.98(0.00)\n0.96(0.00)\n10 dB\n3.89(0.47)\n3.09(0.47)\n0.99(0.00)\n0.97(0.01)\n15 dB\n3.44(0.06)\n2.66(0.07)\n0.99(0.00)\n0.98(0.00)\n20 dB\n3.51(0.55)\n2.61(0.54)\n0.99(0.00)\n0.98(0.01)\n30 dB\n3.41(0.04)\n2.59(0.01)\n0.99(0.00)\n0.99(0.00)\nTable 19: Pendulum\nMSE xt(e−4) ↓\nMSE φt(e−3) ↓\nMSE ˙φt(e−3) ↓\nMeta-HyLaD with prior\n2.96(0.15)\n1.05(0.12)\n3.40(0.19)\nMeta-HyLaD no prior\n3.19(0.11)\n1.07(0.10)\n3.74(0.03)\n(Meta-HyLaD no prior) where the complete state variables are input to fNNϕ.\nMeta-HyLaD with prior: d\ndt\n\u0014φ\n˙φ\n\u0015\n=\n\u0014\n˙φ\n−G\nL sin(φ)\n\u0015\n+ fNNϕ( ˙φ; cn)\nMeta-HyLaD no prior: d\ndt\n\u0014\nφ\n˙φ\n\u0015\n=\n\u0014\n˙φ\n−G\nL sin(φ)\n\u0015\n+ fNNϕ(φ, ˙φ; cn)\n(13)\nSimilar examples are tested on Mass Spring:\nMeta-HyLaD with prior: d\ndt\n\u0014v1\nv2\n\u0015\n=\n\"\n−˜x\n|˜x|\nk\nm1 (|˜x| −l0)\n˜x\n|˜x|\nk\nm2 (|˜x| −l0)\n#\n+ fNNϕ(v1, v2; cn)\nMeta-HyLaD no prior: d\ndt\n\u0014v1\nv2\n\u0015\n=\n\"\n−˜x\n|˜x|\nk\nm1 (|˜x| −l0)\n˜x\n|˜x|\nk\nm2 (|˜x| −l0)\n#\n+ fNNϕ(x1, x2, v1, v2; cn)\n(14)\nResults below show that the forecasting and identification performance of Meta-HyLaD are minimally\naffected by a more general expression of fNNϕ.\nF.4\nFailure Modes for Meta-HyLaD\nHere we probe the potential failure models for Meta-HyLaD, in particularly concerning the strength\nof the prior physics. On Pendulum, we compare two Meta-HyLaD formulations: one with stronger\nprior physics as used in the main experiments, and one with weaker prior physics that is only aware of\nthe dampening effect in the true physics. The results show that, when the prior physics is too weak,\nMeta-HyLaD will approach the performance of a fully neural model at the data space, but still with\nsignificantly better results for the latent state variables.\nMeta-HyLaD with strong physics: d\ndt\n\u0014φ\n˙φ\n\u0015\n=\n\u0014\n˙φ\n−G\nL sin(φ)\n\u0015\n+ fNNϕ( ˙φ; cn)\nMeta-HyLaD with weak physics: d\ndt\n\u0014φ\n˙φ\n\u0015\n=\n\u0014\n˙φ\n−β ˙φ\n\u0015\n+ fNNϕ(φ; cn)\n(15)\nF.5\nFailure Modes for Neural Decoder & Mitigation by Meta-HyLaD\nGiven the strong performance of neural decoder as observed in section 4.4, here we probe its potential\nfailure modes, especially considering the possibility that the true emission functions are not global but\nalso change with each data sample.\nAs an example, we consider another experimental setting of Pendulum:\nd\ndt\n\u0014φ\n˙φ\n\u0015\n=\n\u0014\n˙φ\n−G\nL sin(φ) −β ˙φ\n\u0015\n,\nwe fix G = 10 and ample L ∈[1.0, 3.0] with other details identical appendix A.1. Because the true\nemission function relies on parameter L, this creates a scenario where a global decoder can fail – as\nshown in the results below from the standard neural decoder.\n26\nTable 20: Mass Spring\nMSE xt(e−5) ↓\nMSE φt(e−4) ↓\nMSE ˙φt(e−3) ↓\nMeta-HyLaD with prior\n1.29(0.31)\n1.21(0.37)\n1.66(0.41)\nMeta-HyLaD no prior\n1.19(0.16)\n1.20(0.00)\n1.74(0.13)\nTable 21: Results\nMSE xt(e−4) ↓\nMSE zt(e−2) ↓\nVPT xt ↑\nVPT zt ↑\nMeta-HyLaD with strong physics\n2.96(0..06)\n2.16(0.03)\n0.99(0.00)\n0.98(0.00)\nMeta-HyLaD with weak physics\n33.32(0.58)\n44.94(1.08)\n0.48(0.01)\n0.39(0.00)\nPurely Neural\n33.55(5.18)\n167.06(106.23)\n0.53(0.07)\n0.00(0.00)\nInterestingly, if we simply provide the estimated physics parameter cp (here representing L) to the\ndecoder (adaptive neural decoder), this challenge can be significantly reduced. While not perfectly\naddressing the issue, this leaves an interesting future avenue for Meta-HyLaD and its learn-to-identify\nlearning strategies.\nStandard neural decoder : ˆxt = gψ( ˙φ, L)\nAdaptive neural decoder : ˆxt = gψ( ˙φ)\n(16)\nG\nImplementation Details\nIn this section, we give more implementation details on each experiment over all modules and models.\nAll experiments were run on NVIDIA TITAN RTX with 24 GB memory. We use Adam as optimizer\nwith learning rate 1 × 10−3.\nG.1\nArchitecture for Meta-HyLaD\nAs shown in section 3, Meta-HyLaD has following basic modules: 1) Neural Dynamic Function\nfNNϕ:\ndzt\ndt = fPHY(zt; cp) + fNNϕ(zt; cn); 2) Hyper Network: ϕ = hθ(cn); 3) Initial Encoder Eϕz:\nˆz0 = Eϕz(x0:l); 4) Neural Context Encoder Eζn: ˆcn,j =\n1\nk\nP\nxs\n0:T ∈Ds\nj Eζn(xs\n0:T ); 5) Physical Context\nEncoder Eζp: ˆcp,j = 1\nk\nP\nxs\n0:T ∈Ds\nj Eζp(xs\n0:T ) or Eζp(xq\n0:T ); 6) Neural Decoder gζD: ˆxt = gζD(ˆzt). All these\nmodules are composed of convolutional layers(CNN) and Fully Connected Layers(FNN) Here we\nprovide more details about them.\n(1) Neural Dynamic Function fNNϕ:\nnn . Sequential (\nnn . Linear ( input dim ,\n8) ,\nnn . SiLU () ,\nnn . Linear (8 ,\n8) ,\nnn . SiLU () ,\nnn . Linear (8 ,\n8) ,\nnn . SiLU () ,\nnn . Linear (8 ,\n8) ,\nt]\nTable 22: Results\nMSE xt(e−4) ↓\nMSE zt(e−2) ↓\nVPT xt ↑\nVPT zt ↑\nPhysics-based decoder\n1.93(0.31)\n0.17(0.03)\n1.00(0.00)\n0.97(0.01)\nStandard neural decoder\n39.24(2.95)\n50.57(2.26)\n0.27(0.01)\n0.01(0.00)\nAdaptive neural decoder\n7.45(3.93)\n10.88(2.79)\n0.89(0.05)\n0.62(0.05)\n27\nnn . SiLU () ,\nnn . Linear (8 ,\noutput dim ))\n(2) Hyper Network: ϕ = hθ(cn):\nnn . Sequential (\nnn . Linear ( input dim ,\n16) ,\nnn . SiLU () ,\nnn . Linear (16 ,\n16) ,\nnn . SiLU () ,\nnn . Linear (16 ,\n16) ,\nnn . SiLU () ,\nnn . Linear (16 ,\n8) ,\nnn . SiLU () ,\nnn . Linear (8 ,\noutput dim ))\n(3) Initial Encoder Eϕz:\nnn . Sequential (\nnn . Conv2d( time steps ,\nnum filters ,\nk e r n e l s i z e =5,\ns t r i d e =2, padding =(2 ,\n2)) ,\nnn . BatchNorm2d( n u m f i l t e r s ) ,\nnn . LeakyReLU ( 0 . 1 ) ,\nnn . Conv2d( num filters ,\nn u m f i l t e r s\n∗2 ,\nk e r n e l s i z e =5,\ns t r i d e =2, padding =(2 ,\n2)) ,\nnn . BatchNorm2d( n u m f i l t e r s\n∗2) ,\nnn . LeakyReLU ( 0 . 1 ) ,\nnn . Conv2d( n u m f i l t e r s\n∗2 ,\nnum filters ,\nk e r n e l s i z e =5,\ns t r i d e =2, padding =(2 ,\n2)) ,\nnn . Tanh () ,\nFlatten ( ) )\nnn . Linear ( n u m f i l t e r s ∗16 , output dim )\n(4) Neural Encoder Eζn:\nc l a s s\nSpatialTemporalBlock (nn . Module ) :\ndef\ni n i t\n( s e l f ,\nt in ,\nt out ,\nn in ,\nn out ,\nl a s t )\ns e l f . conv = nn . Conv2d( n in ,\nn out ,\nk e r n e l s i z e =5,\ns t r i d e =2, padding =(2 ,2))\ns e l f . bn = nn . BatchNorm2d( n out )\ns e l f . act = nn . LeakyReLU ( 0 . 1 )\ns e l f . l i n t = nn . Linear ( t in ,\nt out )\nnn . Sequential (\nSpatialTemporalBlock ( time steps ,\ntime steps //2 ,\n1 ,\nnum filters ,\nFalse ) ,\nSpatialTemporalBlock ( time steps //2 ,\ntime steps //4 ,\nnum filters ,\nn u m f i l t e r s ∗2 ,\nFalse\nSpatialTemporalBlock ( time steps //4 ,\n1 ,\nn u m f i l t e r s ∗2 ,\nnum filters ,\nTrue ) ,\nFlatten ( ) )\nnn . Linear ( n u m f i l t e r s ∗16 , output dim )\n(5) Physical Context Encoder Eζp:\nnn . Sequential (\nSpatialTemporalBlock ( time steps ,\ntime steps //2 ,\n1 ,\nnum filters ,\nFalse ) ,\nSpatialTemporalBlock ( time steps //2 ,\ntime steps //4 ,\nnum filters ,\nn u m f i l t e r s ∗2 ,\nFalse\nSpatialTemporalBlock ( time steps //4 ,\n1 ,\nn u m f i l t e r s ∗2 ,\nnum filters ,\nTrue ) ,\nFlatten ( ) )\nnn . Linear ( n u m f i l t e r s ∗16 ,\nlatent dim )\n(6) Neural Decoder gζD: ˆxt = gζD(ˆzt):\nnn . Sequential (\nnn . Linear ( input dim ,\n32) ,\nnn .ReLU() ,\nnn . Linear (32 ,\n32∗4) ,\nnn .ReLU() ,\nnn . Linear (32∗4 ,\n32∗16) ,\n28\nnn .ReLU() ,\nnn . Linear (32∗16 ,\noutput dim∗output dim ))\nG.2\nArchitecture for Other Baselines\nG.2.1\nLSTM, Meta-LSTM, NODE, and Meta-NODE\nLSTM, Meta-LSTM, NODE, and Meta-NODE are modified from the code in [18]. Their modules are\nalmost the same as Meta-HyLaD, only the neural dynamic function are different.\n(1) Neural Dynamic Function of LSTM and Meta-LSTM:\nnn . Linear ( input dim ,\nlatent dim )\nnn . LSTMCell( i n p u t s i z e=latent dim ,\nhidden size=transition dim )\nnn . Linear ( transition dim ,\noutput dim )\n(2) Neural Dynamic Function of NODE and Meta-NODE:\nnn . Sequential (\nnn . Linear ( input dim ,\n8) ,\nnn . SiLU () ,\nnn . Linear (8 ,\n8) ,\nnn . SiLU () ,\nnn . Linear (8 ,\n8) ,\nnn . SiLU () ,\nnn . Linear (8 ,\n8) ,\nnn . SiLU () ,\nnn . Linear (8 ,\noutput dim ))\nG.2.2\nHGN and Meta-HGN\nHGN and Meta-HGN are modified from the code in [32]. They have HamiltonNet in addition to\nMeta-HyLaD and neural dynamics are followed Hamiltonian mechanics.\n(1) HamiltonNet:\nnn . Sequential (\nnn . Linear ( input dim ,\n64) ,\nnn . functional . s o f t p l u s () ,\nnn . Linear (64 ,\n64) ,\nnn . functional . s o f t p l u s () ,\nnn . Linear (64 ,\noutput dim ))\n(2) Neural dynamics followed Hamiltonian mechanics:\nenergy = HamiltonNet (q , p)\ndq dt = torch . autograd . grad ( energy , p ,\ncreate graph=True ,\nretain graph=True ,\ngrad outputs=torch . o n e s l i k e ( energy ) ) [ 0 ]\ndp dt = −torch . autograd . grad ( energy , q ,\ncreate graph=True ,\nretain graph=True ,\ngrad outputs=torch . o n e s l i k e ( energy ) ) [ 0 ]\nG.2.3\nALPS\nALPS are modified from the code in [35] and it has the following basic modules: 1) Initial Encoder; 2)\nPhysical Context Encoder; 3) State Encoder; 4) Neural Decoder.\n(1) Initial Encoder:\nnn . Sequential (\nnn . Conv2d( time steps ,\nnum filters ,\nk e r n e l s i z e =5,\ns t r i d e =2, padding =(2 ,\n2)) ,\nnn . BatchNorm2d( n u m f i l t e r s ) ,\nnn . LeakyReLU ( 0 . 1 ) ,\nnn . Conv2d( num filters ,\nn u m f i l t e r s\n∗2 ,\nk e r n e l s i z e =5,\ns t r i d e =2, padding =(2 ,\n2)) ,\n29\nnn . BatchNorm2d( n u m f i l t e r s\n∗2) ,\nnn . LeakyReLU ( 0 . 1 ) ,\nnn . Conv2d( n u m f i l t e r s\n∗2 ,\nnum filters ,\nk e r n e l s i z e =5,\ns t r i d e =2, padding =(2 ,\n2)) ,\nnn . Tanh () ,\nFlatten ( ) )\nnn . Linear ( n u m f i l t e r s ∗16 , output dim )\n(2) Physical Context Encoder:\nnn . Sequential (\nnn . Linear ( input dim ,\n64) ,\nnn .ReLU() ,\nnn . Linear (64 ,\n64) ,\nnn .ReLU() ,\nnn . Linear (64 ,\n32)\nnn .ReLU() ,\nnn . Linear (32 ,\noutput dim ))\n(3) State Encoder:\nc l a s s\nSpatialTemporalBlock (nn . Module ) :\ndef\ni n i t\n( s e l f ,\nt in ,\nt out ,\nn in ,\nn out ,\nl a s t )\ns e l f . conv = nn . Conv2d( n in ,\nn out ,\nk e r n e l s i z e =5,\ns t r i d e =2, padding =(2 ,2))\ns e l f . bn = nn . BatchNorm2d( n out )\ns e l f . act = nn . LeakyReLU ( 0 . 1 )\ns e l f . l i n t = nn . Linear ( t in ,\nt out )\nnn . Sequential (\nSpatialTemporalBlock ( time steps ,\ntime steps ∗2 ,\n1 ,\nnum filters ,\nFalse ) ,\nSpatialTemporalBlock ( time steps ∗2 ,\ntime steps ∗2 ,\nnum filters ,\nn u m f i l t e r s ∗2 ,\nFalse ) ,\nSpatialTemporalBlock ( time steps ∗2 ,\ntime steps ,\nn u m f i l t e r s ∗2 ,\nnum filters ,\nTrue ))\nnn . Linear ( n u m f i l t e r s ∗16 , output dim )\n(4) Neural Decoder:\nnn . Sequential (\nnn . Linear ( input dim ,\n32) ,\nnn .ReLU() ,\nnn . Linear (32 ,\n32∗4) ,\nnn .ReLU() ,\nnn . Linear (32∗4 ,\n32∗16) ,\nnn .ReLU() ,\nnn . Linear (32∗16 ,\noutput dim∗output dim ))\n30\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2024-03-13",
  "updated": "2024-03-13"
}