{
  "id": "http://arxiv.org/abs/2403.09718v1",
  "title": "Comprehensive Implementation of TextCNN for Enhanced Collaboration between Natural Language Processing and System Recommendation",
  "authors": [
    "Xiaonan Xu",
    "Zheng Xu",
    "Zhipeng Ling",
    "Zhengyu Jin",
    "ShuQian Du"
  ],
  "abstract": "Natural Language Processing (NLP) is an important branch of artificial\nintelligence that studies how to enable computers to understand, process, and\ngenerate human language. Text classification is a fundamental task in NLP,\nwhich aims to classify text into different predefined categories. Text\nclassification is the most basic and classic task in natural language\nprocessing, and most of the tasks in natural language processing can be\nregarded as classification tasks. In recent years, deep learning has achieved\ngreat success in many research fields, and today, it has also become a standard\ntechnology in the field of NLP, which is widely integrated into text\nclassification tasks. Unlike numbers and images, text processing emphasizes\nfine-grained processing ability. Traditional text classification methods\ngenerally require preprocessing the input model's text data. Additionally, they\nalso need to obtain good sample features through manual annotation and then use\nclassical machine learning algorithms for classification. Therefore, this paper\nanalyzes the application status of deep learning in the three core tasks of NLP\n(including text representation, word order modeling, and knowledge\nrepresentation). This content explores the improvement and synergy achieved\nthrough natural language processing in the context of text classification,\nwhile also taking into account the challenges posed by adversarial techniques\nin text generation, text classification, and semantic parsing. An empirical\nstudy on text classification tasks demonstrates the effectiveness of\ninteractive integration training, particularly in conjunction with TextCNN,\nhighlighting the significance of these advancements in text classification\naugmentation and enhancement.",
  "text": "SPIE Proceedings Publications \n \n \n \nComprehensive Implementation of TextCNN for Enhanced Collaboration between \nNatural Language Processing and System Recommendation \n \nXiaonan Xu1,* Zheng Xu2, Zhipeng Ling3, Zhengyu Jin4, ShuQian Du5 \n1*Independent  Researcher,Northern Arizona University,Flagstaff, USA \n2Computer Engineering,Stevens Institute of Technology,hoboken, NJ,USA \n3Computer Science,University of Sydney,Sydney, Australia \n4Informatics,Univeristy of California, Irvine,Irvine ,CA,USA \n5Information Studies, Trine University, Phoenix, Arizona, AZ,USA \n*Corresponding author :xiaonanxu5@gmail.com \n \nABSTRACT Natural Language Processing (NLP) is an important branch of artificial intelligence \nthat studies how to enable computers to understand, process, and generate human language. Text \nclassification is a fundamental task in NLP, which aims to classify text into different predefined \ncategories. Text classification is the most basic and classic task in natural language processing, and most \nof the tasks in natural language processing can be regarded as classification tasks. In recent years, deep \nlearning has achieved great success in many research fields, and today, it has also become a standard \ntechnology in the field of NLP, which is widely integrated into text classification tasks. Unlike numbers \nand images, text processing emphasizes fine-grained processing ability. Traditional text classification \nmethods generally require preprocessing the input model's text data. Additionally, they also need to \nobtain good sample features through manual annotation and then use classical machine learning \nalgorithms for classification. Therefore, this paper analyzes the application status of deep learning in the \nthree core tasks of NLP (including text representation, word order modeling, and knowledge \nrepresentation). This content explores the improvement and synergy achieved through natural language \nprocessing in the context of text classification, while also taking into account the challenges posed by \nadversarial techniques in text generation, text classification, and semantic parsing. An empirical study on \ntext classification tasks demonstrates the effectiveness of interactive integration training, particularly in \nconjunction with TextCNN, highlighting the significance of these advancements in text classification \naugmentation and enhancement. \n \nKeyword list Natural Language Processing (NLP);Text classification-CNN;Deep learning;Semantic \nparsing \n \nIntroduction \nNatural language processing (NLP), an important branch of artificial intelligence, has \ndeveloped rapidly in recent years, driven by deep learning. Deep neural networks, such as \nrecurrent neural networks (RNNS) and convolutional neural networks (CNNS), have \ndemonstrated excellent performance in semantic understanding, speech recognition and machine \ntranslation, but they also face security risks such as anti-sample and poison attacks. In many \nSPIE Proceedings Publications \n \n \n \ncases, the diversity, complexity and distribution of information in text form makes traditional \nmanual analysis methods ineffective, forcing people to look for new solutions. Intelligence texts \ncover a wide range of media and sources, including news reports, spy reports, battlefield reports, \ngovernment documents, social media posts and more. These texts can contain information from \nall parts of the world and at all times, making them highly timely and valuable. However, this \ndiversity also presents significant challenges, as texts from different sources may use different \nformats, languages and phrases, leading to fragmentation of information and increased difficulty \nin analysis. In addition, intelligence texts themselves are often highly specialised, containing a \nlarge number of technical terms, acronyms and domain-specific languages, which places higher \nprofessional demands on analysts and therefore requires a more intelligent and efficient approach \nto this problem. \nTraditional text classification methods generally need to pre-process the text data of the \ninput model, in addition to obtaining good sample features through manual annotation, and then \nuse classical machine learning algorithms to classify them. Similar methods include Naive Bayes \n(NB), K-Nearest Neighbour (KNN), Support Vector Machine (SVM), etc. The level of feature \nextraction has an even greater impact on text classification than on image classification, and \nfeature engineering in text classification is often time consuming and computationally expensive. \nAfter 2010, the method of text classification gradually shifted to deep learning model. Deep \nlearning applied to text classification maps feature engineering directly to the output by learning \na series of nonlinear transformation patterns, thus integrating feature engineering into the process \nof model fitting, and has achieved great success once applied. Therefore, this paper delves into \nthe current state of applying deep learning to the fundamental tasks of Natural Language \nProcessing (NLP), which encompass text representation, word order modeling, and knowledge \nrepresentation. It further explores strategies for enhancing natural language processing \ntechnology, specifically through the utilization of text classification algorithms, in response to \nthe challenges posed by attack methodologies in text generation, text classification, and semantic \nparsing. The effectiveness of an integrated interactive training approach is corroborated through \nempirical studies conducted on text classification tasks. This comprehensive analysis integrates \ninsights from \"Classification Augmentation and Synergy through TextCNN and Natural \nLanguage Processing,\" underscoring the significance of these advancements in the NLP field. \n2. Application of Deep Learning in Natural Language Processing \nWith the continuous development of computer and artificial intelligence, Natural Language \nProcessing (NLP) technology, as one of the research directions, has been applied in more and \nmore fields, including text processing, speech recognition and translation, semantic \nunderstanding and knowledge-based text mining. \n2.1 Natural language processing concepts \nIn September 2019, the Allen Institute for Artificial Intelligence (AI2) released a computer \nprogram called Aristo that correctly answered more than 90 per cent of the questions on an \neighth-grade science test. Passing a high school exam may sound trivial, but it's complicated for \na computer. Aristo used natural language processing (NLP) to find answers from billions of \ndocuments. NLP is a branch of computer science and artificial intelligence that enables computer \nsystems to extract meaning from unstructured text. While computers are still a long way from \nSPIE Proceedings Publications \n \n \n \nbeing able to understand and use human language, NLP is already key to many applications we \nuse every day, including digital assistants, web search, email and machine translation. \n2.2 Natural language - text preprocessing \nText preprocessing is a very important step in NLP, its goal is to convert raw text into a \nform that a computer can process. Common preprocessing operations include word segmentation, \nstop and stop words, part-of-speech tagging and so on. Word segmentation refers to the division \nof sentences into single words, the elimination of words that have a high frequency in the text but \nhave no meaning, such as \"the\", \"a\", etc., part of speech tagging refers to the assignment of each \nword to a part of speech, such as nouns, verbs, adjectives, etc. These actions can help the \ncomputer better process text data. \nPre-trained language models can effectively learn global semantic representation and \nsignificantly improve the effectiveness of NLP tasks, including text classification. It typically \nuses an unsupervised approach to automatically mine semantic knowledge and then builds pre-\ntrained targets that allow the machine to learn to understand the semantics. As shown in Figure 1, \nit is assumed that the differences between Embeddingfrom Language Model (ELMo), OpenAI \nGPT, and BERT are given. ELMo is a deeply contextualized word representation model that can \nbe easily integrated into models. \n \nFigure 1: Classification of natural language processing models \nAmong them, preprocessed text can model the complex features of words and learn \ndifferent representations for various linguistic environments. It learns the embeddings of each \nword based on the context of the bidirectional LSTM. GPT employs supervised fine-tuning and \nunsupervised pre-training to learn general representations that transfer to many NLP tasks with \nlimited adaptations. In addition, the domain of the target dataset does not need to be similar to \nthe domain of the unlabeled dataset. The training process of GPT algorithm usually includes two \nstages. First, the initial parameters of the neural network model are learned by modeling \nobjectives on an unlabeled data set. \n2.3 Text classification-CNN \nText classification is an important problem in NLP, where the goal is to classify a given text \ninto predefined categories. Common algorithms include naive Bayes, support vector machines, \ndeep learning, and so on. Naive Bayes is a classification algorithm based on probability statistics, \nwhich assumes that features are independent of each other; support vector machine is a \nclassification algorithm based on maximum boundary separation, which can map nonlinear \nseparable data to high-dimensional space for classification; Deep learning is a classification \nalgorithm based on neural networks, which can automatically extract features and classify them. \nSPIE Proceedings Publications \n \n \n \nTextCNN is an algorithm that uses Convolutional Neural Networks to classify text. It was \nproposed by Yoon Kim in 2014 in the article Convolutional Neural Networks for Sentence \nClassification. The detailed schematic diagram is as follows. \n \nFigure 2: Schematic diagram of text-CNN Text classification model \nThe input dimensions are: [batch_size, seq_len, emb_dim]. In the example above, batch_size=1, \nseq_len=7, emb_dim=5; \nIn the convolution, it is actually a single channel, the width of the convolution kernel is the same \nas the dimension of the word vector, which is emb_dim, and the height of the convolution kernel \nis specified by itself; \nBecause the width is emb_dim, the convolution can only move up and down, not left and right. \nIn the figure above, there are two convolution kernels of high dimension 4, two convolution \nkernels of high dimension 3, and two convolution kernels of height 2. \nThe feature map after each convolution is downsampled with maximum pooling. \nIn the example above, it is equivalent to having 3 convolution layers, each of which has 2 \nconvolution nuclei, then the convolution layers with sizes 2, 3 and 4 have the following \ndimensions: [batch_size, 2, seq_len-1, 1], [batch_size, 2, seq_len-2, 1], [batch_size, 2, seq_len-3, \n1]; \nThe dimensionality of each layer after convolution pooling is: [batch_size, 2], where 2 is the \nnumber of convolution nuclei; \nThe results after maximum pooling are splicing in the following dimensions: [batch_size, \nlen(filter_sizes) * num_filters], and fed into the full connection for classification. \nfilter_sizes are the heights of the convolution cores and num_filters are the number of \nconvolution cores. \n2.3 Bi-LSTM text classification application \nLSTM: Long-term short-term memory network (LSTM) can remember the information \nbefore and after long sentences, solving the problem of RNN (when the time interval is large, the \nnetwork will forget the previous information, resulting in the problem of gradient disappearance, \nwhich will form a long-term dependency problem) and avoiding long-term dependency problems. \nSPIE Proceedings Publications \n \n \n \nTherefore, the combination of Bi-LSTM and TextCNN has several advantages for \nemotional text classification. First, Bi-LSTM (Bidirectional Long Short-Term Memory) can \neffectively capture contextual information in the text because it considers both forward and \nbackward sequences of the text to better understand the relationship and context between words \nin the text. The ability of the model to capture the long-range dependency relationship is \nimproved. Compared to traditional recurrent neural networks (RNN), Bi-LSTM mitigates the \ngradient disappearance problem and makes the model easier to train. \n(1) \nWhere :ω and is the weight coefficient; b is a biased value. Compared with a single LSTM \nmodel, the Bi-LSTM model not only takes into account the forward correlation information \nbetween the time series data, but also takes into account the reverse correlation information \nbetween the time series data, so it shows superior performance in the classification of sequence \ndata. \nIn this paper, TextCNN is used to embed words, which are then fed into the Bi-LSTM \nnetwork for fine tuning to complete parameter optimisation for text classification tasks. Bi-\nLSTM reads the text sequence X from both directions and computes the hidden state of each \nword, then concatenates the hidden states in both directions to obtain the final hidden \nrepresentation of the ith word: \n(2) \nTherefore, the process by which the Bi-LSTM network obtains the final feature vector \noutput by learning BERT input data can be expressed as: \n(3) \nWhere: indicates the mapping relationship; s represents the text sentences, TextCNN and \nparameters of the Bi-LSTM network :D represents the dimensions of the hidden layer in the Bi-\nLSTM network. \nTherefore, TextCNN (Convolutional Neural Network for Text) has the ability to perform \nconvolutional operations on different window sizes, allowing the model to capture features at \ndifferent scales in the text, from local to global. This helps to extract key information and \nfeatures from the text, making the model more robust and able to adapt to text of different \nlengths and structures. \nIn summary, by combining Bi-LSTM and TextCNN, we can take full advantage of their \nrespective strengths and effectively tackle emotional text classification tasks. Bi-LSTM is \nresponsible for capturing the contextual information of the text, and TextCNN is responsible for \nextracting features at different scales. The two complement each other, improving the \nperformance and generalisation ability of the model to achieve better results in emotional text \nSPIE Proceedings Publications \n \n \n \nclassification. This combined approach has already achieved remarkable success in several \nnatural language processing tasks. \n2.4 TextCNN Procedure \nAccording to the TextCNN principle architecture in Figure 2, it can be seen, \n(1) \nThe first layer is input layer.  \n(4) \nIn the formula, it is assumed that the dimension of the input data is 1, the length is 8, the number \nof channels is 1, and the dimension and number of filter are 1 and the length is 5.The input layer \nis an n× k matrix where n is the number of words in a sentence and k is the dimension of the \nword vector corresponding to each word. That is, each row of the input layer is the K-\ndimensional word vector corresponding to a word. In addition, padding is applied to the original \nsentence to make the length of the vectors consistent. \n(2) The second layer is the convolution layer, and the third layer is the pooling layer. First, we \nshould note the differences between convolution operations in computer vision (CV) and \nNLP. In CV, the convolution kernel is usually square, such as the 3×3 convolution kernel, \nand then the convolution kernel moves along the height and width of the entire image to \ncarry out the convolution operation. Different from CV, the \"image\" of the input layer in \nNLP is a word matrix composed of word vectors, and the width of the convolution kernel is \nthe same as the width of the word matrix, which is the size of the word vector, and the \nconvolution kernel only moves in the height direction.  \n(5) \n(6) \nTherefore, the position that the convolution kernel slides through each time is a complete word, \nand the one-minute \"vector\" of several words is not convolved. \n(3) Finally, there is the pooling layer. The network shown in the figure adopts 1-Max pooling, \nthat is, a maximum feature is selected from the feature vectors generated by each sliding \nwindow, and then these features are spliced together to form a vector representation. You can \nalso choose K-MAX pooling (select the largest K features in each feature vector), or average \npooling (average each dimension in the feature vector), etc., the effect is to obtain a fixed-\nlength vector representation of sentences of different lengths through pooling. \nIn summary, TextCNN (Convolutional Neural Network for Text Classification) has \nsignificant advantages in natural language processing, which are mainly reflected in the \nfollowing aspects: First, TextCNN can effectively capture local features and patterns in text and \nautomatically extract key information through convolutional operations, which makes it perform \nwell in text classification tasks.  \nSPIE Proceedings Publications \n \n \n \n3. Methodology \nSentiment analysis has become particularly important in the age of social media and the \nInternet, allowing us to gain insight into the emotions, opinions and attitudes of the public in \norder to better understand social trends and public opinion. Tweets on social media platforms, as \none of the main ways for users to express their emotions and opinions, contain a lot of valuable \ninformation. As a result, sentiment analysis of tweets has become an area of intense research, \nhelping governments, businesses and research institutions to better understand the public's \nemotional leanings on specific issues. \n3.1 Experimental introduction \nIn this context, this study aims to use TextCNN (Convolutional Neural Networks for Text \nClassification) for sentiment analysis of tweets. Our research question is: How can the TextCNN \nmodel be used effectively to analyse sentiment in tweets and how well does it perform? To \nanswer this question, we will use a massive tweet dataset containing tweets from different social \nmedia platforms, covering a variety of topics and emotional categories. \nTextCNN is a deep learning model known for its excellent performance in text \nclassification tasks. It can capture local features and patterns in text, has a shallow network \nstructure, is easy to train and deploy, and is suitable for processing text data of unlimited length. \nIn this study, TextCNN is chosen as the main model to efficiently analyse the emotional \ntendency in large-scale tweet data. \nIn the following sections, we will detail key steps such as experimental design, data pre-\nprocessing, model training, and evaluation indicators to demonstrate our research methods and \nresults. Our experimental hypothesis is that TextCNN can accurately analyse tweet sentiment, \nthus providing valuable insights for further research and application in the field of sentiment \nanalysis. \n3.2 Data preprocessing \nLabeled sentiment analysis tasks can be treated as text subtasks. The dataset came from a \ncourse assignment SI650-Sentiment Classification at the University of Michigan and a dataset of \ntweets collected by Niek Sanders, with a total of about 157W tweets with a target variable of 0 or \n1 indicating negative and positive emotions, and was a binary classification task for a large \ndataset. The paper mentioned that the naive Bayes classifier could achieve 75% accuracy. To \nverify this, I tested all the datasets using NB and SVM models respectively, and the accuracy of \nNB was 77.5% and that of SVM was 73%. \nCode：from sklearn.feature_extraction.text import CountVectorizer \nfrom sklearn.feature_selection import SelectKBest, chi2 \nfrom sklearn.linear_model import LogisticRegressionCV \nfrom sklearn.pipeline import Pipeline \nfrom sklearn.model_selection import train_test_split \nSPIE Proceedings Publications \n \n \n \n3.4 Text model building \nSteps: \n① Use two layers of convolution \n② Use more convolution nuclei, more scale convolution nuclei \n③ BatchNorm is used \n④ Two layers of full connection are used in the classification \n \n \nTable : Model data set \nConcat \nMaxPool \n \n \nMaxPool \n \n \nMaxPool \n \n \nMaxPool \n \n \nMaxPool \nReLU \nReLU \nReLU \nReLU \nReLU \nBatchNorm \nBatchNorm \nBatchNorm \nBatchNorm \nBatchNorm \nconv(1) \nconv(2) \nconv(3) \nconv(4) \nconv(5) \nReLU \nReLU \nReLU \nReLU \nReLU \nBatchNorm \nBatchNorm \nBatchNorm \nBatchNorm \nBatchNorm \nconv(1) \nconv(2) \nconv(3) \nconv(4) \nconv(5) \n \nCode：model.compile(loss=‘binary_crossentropy’, \noptimizer=‘adam’, \nmetrics=[‘accuracy’]) \nhistory = model.fit(x_train_padded_seqs, y_train, \nbatch_size=32, \nepochs=5, \nvalidation_data=(x_test_padded_seqs, y_test)) \n \nSPIE Proceedings Publications \n \n \n \nFigure 3: TextCNN result ROC curve \n \n3.4 LSTM text model \nRNN models can handle sequence problems, and LSTM is better at capturing long sequence \nrelationships. Because of the existence of gate, LSTM can learn and grasp the dependencies in \nsequence well, so it is more suitable for dealing with long sequence NLP problems. The model \nstructure is as follows: \n \nFigure 4: LATM text classification architecture diagram \nAmong them, the sentence is first word embedding, passed into the LSTM sequence for \ntraining, the last hidden state of LSTM is taken out, and the final output is obtained by adding the \nfull connection layer. \nWhen analyzing text classification under the LSTM model, it should be noted that in \nembeddings, unlike word vector summation in DNN, LSTM does not need to sum word vectors, \nbut directly learns word vectors themselves. Whether it is summing or averaging, the convergent \noperation will lose some information \nIn the model, we first construct the LSTM unit and add dropout to prevent overfitting; After \nexecuting dynamic_rnn, we will get the final state of lstm, which is a tuple structure containing \nthe cell state and hidden state (the result of passing through the output gate), we will only take \nthe hidden state output here. lstm_state.h, concatenates this vector, and finally produces the \noutput result. \nSPIE Proceedings Publications \n \n \n \n \nFigure 5： ROC curve of LATM text classification results \n3.5 Experimental result \nSo far, we have completed the task of handling sentence classification using RNNS and \nCNNS respectively. Among them, the accuracy of DNN and RNN on test is almost the same, \nwhile the accuracy of CNN on test is 1%~2% higher, and the accuracy of multi-channels CNN \non test is as high as 76.93%, and the training times are also less. \nOur models are relatively simple, but generally speaking, these models have achieved good \naccuracy, which is largely due to the pre-trained word embedding, which shows the importance \nof word embedding in NLP models. The accuracy of multi-channels CNN is improved by adding \ntask-specific information. \nConclusion \nThis paper discusses in detail the application of deep learning in natural language \nprocessing, especially in the field of text classification. Through the application of TextCNN and \nother models, it successfully realises the efficient processing of core tasks such as sentiment \nanalysis. Deep learning has brought significant improvements to NLP technology, making the \ntask of processing text data more flexible and efficient. The experimental results show that \nTextCNN and other models perform well in text classification tasks, especially under the pre-\ntrained word embeddedness, and achieve satisfactory performance. In addition, this paper also \nfocuses on the technical challenges of text generation, text classification and semantic parsing, \nand proposes an integrated interactive training method to effectively address these challenges, \nwhich provides an important reference for further research and application in the field of NLP. \nAs one of the most important tasks in natural language processing, text classification is \nconstantly evolving and expanding. In the future, with the continuous development and \nimprovement of deep learning technology, text classification will bring more opportunities and \nchallenges. Some of these future directions are \n1. Multimodal text classification: Combining text with multimodal information such as \nimages and audio for classification, so that text classification can better meet the processing \nneeds of multimedia content. \nSPIE Proceedings Publications \n \n \n \n2. Cross-lingual text classification: Develop a text classification model that can handle \nmultiple languages, and promote the analysis of multilingual social media and international \ninformation. \n3. Continuous learning: Develop text classification models that can continuously learn and \nadapt to new data and domains to cope with the changing information environment. \n4. Privacy protection: Strengthen the protection of user privacy in text classification to \nprevent the misuse of personal information. \nIn summary, text classification has a wide range of application prospects in natural language \nprocessing, which is not only of great value in public opinion analysis, information retrieval and \nother fields, but also plays an important role in everyday life, such as social media and intelligent \nassistants. As technology continues to advance, we can expect text classification to play a greater \nrole in improving natural language processing techniques and solving practical problems. \nReference \n[1] BRA P D, HOUBEN G J,KORNATZKY Y,et al. Information retrieval indistributed \nhypertexts [C ]// 4th International Conference on IntelligentMultimedia Information \nRetrievalSystems and Management,1994:481-493. \n[2] Yu, Liqiang, et al. “Research on Machine Learning With Algorithms and \nDevelopment”. Journal of Theory and Practice of Engineering Science, vol. 3, no. 12, \nDec. 2023, pp. 7-14, doi:10.53469/jtpes.2023.03(12).02. \n[3] Liu, Bo, et al. \"Integration and Performance Analysis of Artificial Intelligence and \nComputer Vision Based on Deep Learning Algorithms.\" arXiv preprint \narXiv:2312.12872 (2023). \n[4] MIKOLOV T,SUTSKEVER I,CHEN K,et al. Distributed representa-tions of words \nand phrases and their compositionality[C //Proceedings ofthe 26th International \nConference on Neural Information Processing Systems,2013:3111-3119. \n[5] Yu, L., Liu, B., Lin, Q., Zhao, X., & Che, C. (2024). Semantic Similarity Matching for \nPatent Documents Using Ensemble BERT-related Model and Novel Text Processing \nMethod. arXiv preprint arXiv:2401.06782. \n[6] Huang, J., Zhao, X., Che, C., Lin, Q., & Liu, B. (2024). Enhancing Essay Scoring with \nAdversarial Weights Perturbation and Metric-specific AttentionPooling. arXiv preprint \narXiv:2401.05433. \n[7] CHOUDHARY K,BENIWAL R. Xplore word embedding using CBOWmodel and \nskip-gram model[C ]//2021 7th International Conference onSignal Processing and \nCommunication ,2021 :267-270.YAN X, GUO J, LAN Y, et al. A biterm topic model \nfor short texts[C ]//Proceedings of the 22nd linternational Conference on World Wide \nWeb,2013:1445-1456 \n[8] Tianbo, Song, Hu Weijun, Cai Jiangfeng, Liu Weijia, Yuan Quan, and He Kun. \"Bio-\ninspired Swarm Intelligence: a Flocking Project With Group Object Recognition.\" In \n2023 3rd International Conference on Consumer Electronics and Computer \nEngineering (ICCECE), pp. 834-837. IEEE, 2023.DOI: 10.1109/mce.2022.3206678 \n[9] Liu, B., Zhao, X., Hu, H., Lin, Q., & Huang, J. (2023). Detection of Esophageal Cancer \nLesions Based on CBAM Faster R-CNN. Journal of Theory and Practice of \nEngineering Science, 3(12), 36–42. https://doi.org/10.53469/jtpes.2023.03(12).06 \nSPIE Proceedings Publications \n \n \n \n[10] Liu, Bo, et al. \"Integration and Performance Analysis of Artificial Intelligence and \nComputer Vision Based on Deep Learning Algorithms.\" arXiv preprint \narXiv:2312.12872 (2023). \n \n \n",
  "categories": [
    "cs.CL",
    "cs.AI"
  ],
  "published": "2024-03-12",
  "updated": "2024-03-12"
}