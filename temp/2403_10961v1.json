{
  "id": "http://arxiv.org/abs/2403.10961v1",
  "title": "Energy-Based Models with Applications to Speech and Language Processing",
  "authors": [
    "Zhijian Ou"
  ],
  "abstract": "Energy-Based Models (EBMs) are an important class of probabilistic models,\nalso known as random fields and undirected graphical models. EBMs are\nun-normalized and thus radically different from other popular self-normalized\nprobabilistic models such as hidden Markov models (HMMs), autoregressive\nmodels, generative adversarial nets (GANs) and variational auto-encoders\n(VAEs). Over the past years, EBMs have attracted increasing interest not only\nfrom the core machine learning community, but also from application domains\nsuch as speech, vision, natural language processing (NLP) and so on, due to\nsignificant theoretical and algorithmic progress. The sequential nature of\nspeech and language also presents special challenges and needs a different\ntreatment from processing fix-dimensional data (e.g., images). Therefore, the\npurpose of this monograph is to present a systematic introduction to\nenergy-based models, including both algorithmic progress and applications in\nspeech and language processing. First, the basics of EBMs are introduced,\nincluding classic models, recent models parameterized by neural networks,\nsampling methods, and various learning methods from the classic learning\nalgorithms to the most advanced ones. Then, the application of EBMs in three\ndifferent scenarios is presented, i.e., for modeling marginal, conditional and\njoint distributions, respectively. 1) EBMs for sequential data with\napplications in language modeling, where the main focus is on the marginal\ndistribution of a sequence itself; 2) EBMs for modeling conditional\ndistributions of target sequences given observation sequences, with\napplications in speech recognition, sequence labeling and text generation; 3)\nEBMs for modeling joint distributions of both sequences of observations and\ntargets, and their applications in semi-supervised learning and calibrated\nnatural language understanding.",
  "text": "Foundations and Trends® in Signal Processing\nEnergy-Based Models with\nApplications to Speech and\nLanguage Processing\nSuggested Citation: Zhijian Ou (2022), “Energy-Based Models with Applications to\nSpeech and Language Processing”, Foundations and Trends® in Signal Processing: Vol.\nxx, No. xx, pp 1–200. DOI: 10.1561/XXXXXXXXX.\nZhijian Ou\nTsinghua University, Beijing, China\nozj@tsinghua.edu.cn\nThis article may be used only for the purpose of research, teaching,\nand/or private study. Commercial use or systematic downloading (by\nrobots or other automatic processes) is prohibited without explicit\nPublisher approval.\nBoston — Delft\narXiv:2403.10961v1  [cs.LG]  16 Mar 2024\nContents\n1\nIntroduction\n3\n1.1\nThe probabilistic approach\n. . . . . . . . . . . . . . . . .\n3\n1.1.1\nGenerative models and discriminative models\n. . .\n6\n1.1.2\nConditional models\n. . . . . . . . . . . . . . . . .\n7\n1.2\nFeatures of EBMs . . . . . . . . . . . . . . . . . . . . . .\n8\n1.3\nOrganization of this monograph\n. . . . . . . . . . . . . .\n10\n2\nBasics for EBMs\n13\n2.1\nProbabilistic graphical models (PGMs) . . . . . . . . . . .\n13\n2.1.1\nDirected graphical models . . . . . . . . . . . . . .\n15\n2.1.2\nUndirected graphical models\n. . . . . . . . . . . .\n19\n2.2\nEBM model examples . . . . . . . . . . . . . . . . . . . .\n24\n2.2.1\nIsing model in statistical physics\n. . . . . . . . . .\n24\n2.2.2\nRestricted Boltzmann Machines (RBMs) . . . . . .\n26\n2.2.3\nEBMs parameterized by neural networks . . . . . .\n29\n2.3\nLearning EBMs by maximum likelihood . . . . . . . . . . .\n32\n2.3.1\nMarkov Chain Monte Carlo (MCMC) . . . . . . . .\n35\n2.3.2\nImportance sampling\n. . . . . . . . . . . . . . . .\n42\n2.3.3\nStochastic approximation methods . . . . . . . . .\n43\n2.3.4\nVariational methods with auxiliary models . . . . .\n49\n2.3.5\nNon-MLE methods for learning EBMs . . . . . . .\n60\n2.4\nLearning EBMs by noise-contrastive estimation (NCE)\n. .\n61\n2.4.1\nDynamic noise-contrastive estimation (DNCE) . . .\n64\n2.5\nGeneration from EBMs\n. . . . . . . . . . . . . . . . . . .\n66\n3\nEBMs for sequential data with applications in language mod-\neling\n71\n3.1\nAutoregressive language model (ALM) . . . . . . . . . . .\n71\n3.2\nEnergy-based language model (ELM) . . . . . . . . . . . .\n73\n3.2.1\nGlobally-normalized ELM (GN-ELM) . . . . . . . .\n73\n3.2.2\nTrans-dimensional random field (TRF) LMs . . . .\n74\n3.2.3\nComparison between GN-ELM and TRF-LM . . . .\n77\n3.3\nELMs for speech recognition\n. . . . . . . . . . . . . . . .\n78\n3.3.1\nArchitectures of energy functions . . . . . . . . . .\n78\n3.3.2\nIntegrating discrete and neural features\n. . . . . .\n86\n3.3.3\nResidual ELMs . . . . . . . . . . . . . . . . . . . .\n88\n3.3.4\nTraining methods . . . . . . . . . . . . . . . . . .\n89\n3.4\nEnergy-based cloze models for representation learning over\ntext . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n89\n4\nConditional EBMs with applications\n94\n4.1\nCRFs as conditional EBMs\n. . . . . . . . . . . . . . . . .\n94\n4.1.1\nLinear-chain CRFs . . . . . . . . . . . . . . . . . .\n95\n4.1.2\nLabel bias and exposure bias . . . . . . . . . . . .\n97\n4.1.3\nTraining of CRFs\n. . . . . . . . . . . . . . . . . . 104\n4.2\nCRFs for speech recognition . . . . . . . . . . . . . . . . . 108\n4.2.1\nConnectionist Temporal Classification (CTC)\n. . . 108\n4.2.2\nCRF-based acoustic modeling with CTC topology . 115\n4.3\nCRFs for sequence labeling in NLP . . . . . . . . . . . . . 123\n4.3.1\nRNN-Transducer (RNN-T) . . . . . . . . . . . . . 124\n4.3.2\nFrom RNN-T to CRF transducer . . . . . . . . . . 125\n4.4\nEBMs for conditional text generation . . . . . . . . . . . . 130\n4.4.1\nResidual energy-based models . . . . . . . . . . . . 130\n4.4.2\nControlled text generation from pre-trained language\nmodels . . . . . . . . . . . . . . . . . . . . . . . . 135\n5\nJoint EBMs with applications\n142\n5.1\nBasics for semi-supervised learning . . . . . . . . . . . . . 142\n5.1.1\nDiscriminative SSL\n. . . . . . . . . . . . . . . . . 143\n5.1.2\nGenerative SSL\n. . . . . . . . . . . . . . . . . . . 144\n5.2\nUpgrading EBMs to Joint EBMs (JEMs) for fixed-dimensional\ndata\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147\n5.3\nUpgrading CRFs to Joint random fields (JRFs) for sequential\ndata\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149\n5.4\nJEMs and JRFs for semi-supervised learning . . . . . . . . 156\n5.4.1\nPre-training via EBMs for SSL . . . . . . . . . . . 157\n5.4.2\nJoint-training via EBMs for SSL\n. . . . . . . . . . 158\n5.4.3\nComparison of joint-training and pre-training\n. . . 160\n5.5\nJRFs for calibrated natural language understanding\n. . . . 164\n6\nConclusion\n169\n6.1\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . 169\n6.2\nFuture challenges and directions\n. . . . . . . . . . . . . . 170\nAcknowledgements\n173\nAppendices\n174\nA Notations and definitions\n175\nA.1\nNotations\n. . . . . . . . . . . . . . . . . . . . . . . . . . 175\nA.2\nDefinitions . . . . . . . . . . . . . . . . . . . . . . . . . . 176\nB Background material\n178\nB.1\nMaximum entropy models . . . . . . . . . . . . . . . . . . 178\nB.2\nFisher equality . . . . . . . . . . . . . . . . . . . . . . . . 179\nC Open-source toolkits related to EBMs\n181\nReferences\n182\nIndex\n200\nList of Algorithms\n1\nMetropolis-Hastings Algorithm . . . . . . . . . . . . . .\n36\n2\nGibbs sampler . . . . . . . . . . . . . . . . . . . . . . . .\n38\n3\nA naive algorithm of learning EBMs by Monte Carlo\nmethods . . . . . . . . . . . . . . . . . . . . . . . . . . .\n44\n4\nThe general stochastic approximation (SA) algorithm . .\n45\n5\nSA with multiple moves . . . . . . . . . . . . . . . . . .\n47\n6\nStochastic maximum likelihood for fitting an EBM . . .\n48\n7\nThe inclusive-NRF algorithm for learning EBMs for con-\ntinuous data with latent-variable auxiliary models\n. . .\n56\n8\nSampling in the augmented space defined by pθ(x)qϕ(h|x) 59\n9\nNCE for fitting an unnormalized model\n. . . . . . . . .\n63\n10\nTop-k sampling for the residual EBM . . . . . . . . . . . 133\n1\nList of Figures\n1.1\nThe probabilistic approach . . . . . . . . . . . . . . . . .\n4\n1.2\nOutline of this monograph . . . . . . . . . . . . . . . . .\n11\n2.1\n(a) A simple directed graphical model with four vari-\nables (x1, x2, x3, x4). (b) A simple undirected graphical\nmodel with four variables (x1, x2, x3, x4). For both types\nof graphs, V denotes the set of nodes and E the set of\nedges. If both ordered pairs (α, β) and (β, α) belong to E,\nwe say that we have an undirected edge between α and\nβ. A nice introduction of graph theory in the context of\ngraphical models could be found in Chapter 4 of Cowell\net al., 1999. . . . . . . . . . . . . . . . . . . . . . . . . .\n14\n2.2\nGraphical model representation of a hidden Markov\nmodel (HMM). . . . . . . . . . . . . . . . . . . . . . . .\n17\n2.3\nNeural network based classifier. (a) GM representation;\n(b) Computational graph representation. . . . . . . . . .\n18\n2.4\nIllustration of the global Markov property in UGMs. . .\n21\n2.5\nIsing model: (a) The undirected graph representation,\n(b) A sample. . . . . . . . . . . . . . . . . . . . . . . . .\n24\n2.6\nSample states of square Ising models with J = 1, H =\n0, kB = 1, N = 4096 at a sequence of temperatures\nT = 5, 2.5, 2.4, 2.3, 2. (MacKay, 2003) . . . . . . . . . . .\n26\n2\nLIST OF FIGURES\n3\n2.7\nRestricted Boltzmann Machine. The top layer represents\na vector of stochastic binary hidden variables h and the\nbottom layer represents a vector of stochastic binary\nvisible variables v. (Salakhutdinov, 2009) . . . . . . . . .\n27\n2.8\n(a) Restricted Boltzmann machine (RBM), (b) Sigmoid\nbelief network (SBN).\n. . . . . . . . . . . . . . . . . . .\n28\n2.9\nPotential functions in EBMs can be flexibly parameter-\nized by neural networks for images, natural languages\nand so on.\n. . . . . . . . . . . . . . . . . . . . . . . . .\n30\n2.10 Overview of the inclusive-variational approach for learn-\ning EBMs for continuous data. Two neural networks are\nused to define the EBM’s potential function Uθ(x) and\nthe auxiliary generator gϕ(h) respectively. The param-\neters of both networks, θ and ϕ, are updated by using\nthe revised samples (x, h) in the augmented space, which\nare obtained by revising the samples (x′, h′) proposed by\nthe auxiliary generator, according to the stochastic gra-\ndients defined by both the target EBM and the auxiliary\ngenerator. (Song and Ou, 2018) . . . . . . . . . . . . . .\n54\n3.1\nExample of discrete features.\n. . . . . . . . . . . . . . .\n80\n3.2\nHidden2Scalar: a deep CNN architecture used to define\nthe potential function Uθ(x). Shadow areas denote the\npadded zeros. (Wang and Ou, 2017)\n. . . . . . . . . . .\n82\n3.3\nHidden2Scalar: a bidirectional LSTM on top of CNN\nused to define the potential function Uθ(x). (Wang and\nOu, 2018b)\n. . . . . . . . . . . . . . . . . . . . . . . . .\n83\n3.4\nSumInnerProduct: a bidirectional LSTM used to define\nthe potential function Uθ(x). (Wang and Ou, 2018a) . .\n83\n3.5\nThe WER curves of the three TRF-LMs during the first\n100 training epochs are plotted. (Gao et al., 2020)\n. . .\n87\n4\nLIST OF FIGURES\n3.6\nComparison of BERT and Electric. Both model the condi-\ntional probability of a token given its surrounding context.\nBERT produces normalized conditional distribution for\nmasked positions, while Electric calculates unnormalized\nconditional probabilities for all input tokens. (Clark et al.,\n2020b) . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n90\n4.1\nGraphical model representation of a conditional random\nfield (CRF). . . . . . . . . . . . . . . . . . . . . . . . . .\n95\n4.2\nState transitions resulting from estimating an autoregres-\nsive language model from training data - “Tom likes tea”,\n“John likes tea”, and “Alice like tea”. For some transitions\nnot appeared in the training data, the transition proba-\nbilities are smoothed to take small values ϵ. We pad the\nbeginning and the end of a sentence with special tokens,\n⟨s⟩and ⟨/s⟩, respectively (Chen and Goodman, 1999). .\n99\n4.3\nEstimating a globally-normalized energy-based language\nmodel (ELM) from training data - “Tom likes tea”, “John\nlikes tea”, and “Alice like tea”. The bi-gram features used\nby the ELM are similar to those used in the bigram ALM,\nand so can also be illustrated by a graph. The estimated\nparameters are shown over the edges, which represent\nthe corresponding bi-gram features. . . . . . . . . . . . . 100\n4.4\nIllustration of exposure bias. y: real, ˆy: predicted. . . . . 103\n4.5\nDifferent units of labels can be used in speech recognition.109\n4.6\nState transitions in HMMs for speech recognition are\nconstrained by a number of knowledge sources. . . . . . 110\n4.7\nOverview of CTC architecture.\n. . . . . . . . . . . . . . 112\n4.8\nIllustration of the lattice, which contains all the possible\nalignments between the acoustic sequence and the label\nsequence ‘CAT’. Also illustration of the forward-backward\nalgorithm. Black circles represent ordinary labels, and\nwhite circles represent blanks. Arrows signify allowed\ntransitions. (Graves et al., 2006)\n. . . . . . . . . . . . . 113\nLIST OF FIGURES\n5\n4.9\nGraphical model representation of the CTC model (a)\nand the CTC-CTF model (b). Note that the edge poten-\ntial does not involve exactly n consecutive nodes for a\nn-gram LM of labels, as detailed in the text of Section\n4.2.2. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118\n4.10 Graphical model representations of different ASR models:\n(a) HMM, defined in Eq. (2.2), (b) CTC, defined in Eq.\n(4.17), (c) RNN-T, (d) AED, (e) CTC-CRF, defined in\nEq. (4.21). . . . . . . . . . . . . . . . . . . . . . . . . . . 121\n4.11 Graphical model representations of (a) a linear-chain\nCRF, (b) a RNN-T for the aligned setting, and (c) a\nCRF transducer. Notably, the graphical representation\nof the RNN-T for the aligned setting, as defined in Eq.\n(4.28), is different from that of the usual RNN-T as shown\nin Figure 4.10(c). . . . . . . . . . . . . . . . . . . . . . . 126\n4.12 The architecture of a CRF transducer. . . . . . . . . . . 127\n4.13 Overview of mix-and-match LM. The Lego pieces show\ndifferent experts that can be used to form the energy\nLM and help control different features in the generated\ntext. The right side shows the i-th step in the the Gibbs\nsampling chain, where a proposal is made by the MLM,\nand then it is accepted/rejected based on the energy\nscore. (Mireshghallah et al., 2022)\n. . . . . . . . . . . . 137\n5.1\nAn overview of SSL and a general categorization of gen-\nerative SSL methods. Examples are mainly chosen from\nNLP. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145\n5.2\nOverview of the JRF model. The node and edge po-\ntentials define a JRF (a joint distribution over xl and\nyl). Inducing the conditional and the marginal from the\njoint yields a CRF and a TRF respectively. A JRF can\nbe trained from labeled data (acting like a CRF) and\nalso from unlabeled data (acting like a TRF). In prac-\ntice, the node potentials are calculated from the logits\noi, i = 1, · · · , l, from the NN, and the edge potential\nfollows a linear-chain definition.\n. . . . . . . . . . . . . 153\n6\nLIST OF FIGURES\n5.3\nIllustration of EBM based semi-supervised image classifi-\ncation. (a) Pre-training, (b) Fine-tuning, (c) Joint-training.157\n5.4\nIllustration of EBM based sequence labeling. (a) Pre-\ntraining, (b) Fine-tuning, (c) Joint-training. . . . . . . . 159\n5.5\nComparison of the scalar and the hidden variants of\nenergy functions. The modules introduced for EBM are\nshaded in green. (He et al., 2021) . . . . . . . . . . . . . 165\n5.6\nThe entropy of the posterior (pθ(·|x)) versus energy value\nˆEθ(x) for SST-2 test-set samples. (He et al., 2021)\n. . . 168\nList of Tables\n2.1\nA survey of different sampling methods used in gener-\nating text from EBMs. The target model is the EBM,\nwhile a proposal is required for both MCMC and IS. Dif-\nferent proposals are used in different applications. Short-\nhands: ALMs (autoregressive language model), MLM\n(masked language model), SNIS (self-normalized impor-\ntance sampling), ASR (automatic speech recognition),\nCTG (controlled text generation), CTGAP (conditional\ntext generation after prefix). . . . . . . . . . . . . . . . .\n68\n3.1\nThe development of TRF-LMs. . . . . . . . . . . . . . .\n76\n3.2\nFeature definition in TRF LMs (Wang et al., 2018) . . .\n81\n4.1\nA general classification of sequence models, with some\ncommon examples. . . . . . . . . . . . . . . . . . . . . .\n98\n7\n8\nLIST OF TABLES\n4.2\nComparison of different models for ASR. HMM topol-\nogy denotes that labels (including silence) are modeled\nby multiple states with left-to-right transitions, possible\nself-loops and skips. CTC topology denotes the special\nstate transitions used in CTC (including blank). Lo-\ncally/globally normalized denotes the formulation of the\nmodel distribution. In defining the joint distribution of a\nmodel, locally normalized models use conditional proba-\nbility functions, while globally normalized models use un-\nnormalized potential functions. SS-LF-MMI is classified\nas globally normalized, though it is cast as MMI-based\ndiscriminative training of a pseudo HMM and the HMM\nmodel is locally normalized. AED does not use states to\nalign label sequence y and observation sequence x. . . . 120\n4.3\nModel comparison and connection. . . . . . . . . . . . . 125\n5.1\nApplications of EBMs across different domains: compari-\nson and connection (See text for details).\n. . . . . . . . 156\n5.2\nSSL for image classification over CIFAR-10 with 4,000\nlabels for a full training set of 50K images. The up-\nper/lower blocks show the generative/discriminative SSL\nmethods respectively. The means and standard deviations\nare calculated over ten independent runs with randomly\nsampled labels. . . . . . . . . . . . . . . . . . . . . . . . 161\n5.3\nRelative improvements by joint-training EBMs compared\nto the supervised baseline (abbreviated as sup.) and\nthe pre-training+fine-tuning EBMs (abbreviated as pre.)\nrespectively. The evaluation metric is accuracy for POS\nand F1 for chunking and NER. “Labeled” denotes the\namount of labels in terms of the proportions w.r.t. the\nfull set of labels. “U/L” denotes the ratio between the\namount of unlabeled and labeled data. . . . . . . . . . . 163\nEnergy-Based Models with\nApplications to Speech and\nLanguage Processing\nZhijian Ou1\n1Tsinghua University, Beijing, China; ozj@tsinghua.edu.cn\nABSTRACT\nEnergy-Based Models (EBMs) are an important class of\nprobabilistic models, also known as random fields and undi-\nrected graphical models. EBMs are un-normalized and thus\nradically different from other popular self-normalized prob-\nabilistic models such as hidden Markov models (HMMs),\nautoregressive models, generative adversarial nets (GANs)\nand variational auto-encoders (VAEs). During these years,\nEBMs have attracted increasing interests not only from core\nmachine learning but also from application domains such\nas speech, vision, natural language processing (NLP) and\nso on, with significant theoretical and algorithmic progress.\nTo the best of our knowledge, there are no review papers\nabout EBMs with applications to speech and language pro-\ncessing. The sequential nature of speech and language also\npresents special challenges and needs treatment different\nfrom processing fix-dimensional data (e.g., images).\nThe purpose of this monograph is to present a systematic\nintroduction to energy-based models, including both algo-\nrithmic progress and applications in speech and language\nprocessing, which is organized into four chapters. First, we\nZhijian Ou (2022), “Energy-Based Models with Applications to Speech and Language\nProcessing”, Foundations and Trends® in Signal Processing: Vol. xx, No. xx, pp\n1–200. DOI: 10.1561/XXXXXXXXX.\n©2024 Zhijian Ou\n2\nLIST OF TABLES\nwill introduce basics for EBMs, including classic models,\nrecent models parameterized by neural networks, sampling\nmethods, and various learning methods from the classic\nlearning algorithms to the most advanced ones. The next\nthree chapters will present how to apply EBMs in three\ndifferent scenarios, i.e., for modeling marginal, conditional\nand joint distributions, respectively. 1) EBMs for sequen-\ntial data with applications in language modeling, where we\nare mainly concerned with the marginal distribution of a\nsequence itself; 2) EBMs for modeling conditional distribu-\ntions of target sequences given observation sequences, with\napplications in speech recognition, sequence labeling and\ntext generation; 3) EBMs for modeling joint distributions of\nboth sequences of observations and targets, and their appli-\ncations in semi-supervised learning and calibrated natural\nlanguage understanding. In addition, we will introduce some\nopen-source toolkits to help the readers to get familiar with\nthe techniques for developing and applying energy-based\nmodels.\n1\nIntroduction\n1.1\nThe probabilistic approach\nAs a community we seem to have embraced the fact that dealing with\nuncertainty is crucial for machine intelligence tasks such as speech recog-\nnition and understanding, speech synthesis, natural language labeling,\nmachine translation, text generation, computer vision, signal denoising,\ndecision making, and so on. Uncertainty arises because of limitations\nin our ability to observe the world, limitations in our ability to model\nit, and possibly even because of innate nondeterminism (Koller and\nFriedman, 2009). In face of such uncertainty, we use probabilistic models\nto describe the random phenomena. Indeed, many tasks in intelligent\nsignal processing and machine learning are solved in the probabilistic\napproach, which generally involves probabilistic modeling, inference and\nlearning, as shown in Figure 1.1. Such probabilistic approach has been\nintroduced in textbooks with sufficient details (Koller and Friedman,\n2009; Murphy, 2012; Bishop, 2006; Hastie et al., 2009), and thus in this\npaper we only give a brief overview as the background material.\nA probabilistic model is, in mathematical terms, a distribution over a\nset of random variables, which are assumed to characterise the random\nphenomena in the specific task. The set of variables can generally\n3\n4\nIntroduction\nFigure 1.1: The probabilistic approach\nbe divided into observations x and (optionally) hidden variables h,\naccording to their roles in the task. Hidden variables, or called latent\nvariables, are variables that are part of the model, but which we do\nnot observe, and are therefore not part of the data. Remarkably, the\nobservability of some variables may change, depending on what phase\n(training or testing) the model is used. A most common example is the\ntarget variable in prediction tasks, such as the class label in classification\nor the response variable in regression, which is observed in training\nbut becomes unknown in testing. To avoid clutter in this paper, such\nvariable is viewed as part of the hidden variables and usually denoted\nby y.\nWe will typically denote a variable by a lower case letter such as\nx, h and y. Whether x denotes the value that the variable takes or\nrepresents the variable itself would be clear from the context. Further,\nfor notational simplicity, we also use lower case letter (e.g., x) to denote\na set of random variables, i.e., flattened and concatenated such that the\nset is represented as a single vector. So if x is a vector or a sequence,\nits components can be accessed by subscripts xi. Here, we are using\nthe terminology distribution or density loosely, typically denoted by\np. Our notation p should be understood as a mass function (density\nwith respect to counting measure) in the discrete case, and a density\nfunction with respect to Lebesgue measure in the continuous case. See\nSection A.1 for more on notations.\nGiven the form of the probabilistic model, namely the distribution\n1.1. THE PROBABILISTIC APPROACH\n5\npθ(x, h) with parameters θ, there are two crucial problems that must\nbe solved in applying the model in real-world tasks:\n• Inference: how to reason in the presence of uncertainty;\n• Learning: how to learn from experience.\nThe former problem is often referred to probabilistic inference with\na fully-specified model, or inference for short; and the later problem\nsometimes referred to statistical inference (or more often to say, learning\nin machine learning terminology) for model parameters (Neal, 1993).\nPut in a more straightforward way, learning is to find the most\nappropriate model with parameters, using both data and human knowl-\nedge. Human knowledge is implicitly employed to specify the family of\nparametric distributions, and data are used to estimate the parameters.\nGiven a fully-specified model, i.e., fully-determined with fixed parame-\nters, inference is to infer the unknown from the observation x. There\nare several typical classes of inference problems:\n• Computing conditional probabilities, e.g., pθ(h|x). This amounts\nto computing the posterior probability of some variables given the\nvalues of other variables (i.e., given evidence on others).\n• Computing marginal probabilities, including the likelihood pθ(x).\n• Computing modes, e.g., arg maxh pθ(h|x).\n• Sampling from the model (Neal, 1993; Liu, 2001).\nWe provide two more points for readers to appreciate the impor-\ntance of the inference problems. First, the inference problems themselves\nare often taken as the means to use the model. For example, speech\nrecognition is generally to find the mode of the posterior distribution\non state sequences given observed speech. Second, learning algorithms\noften make use of some inference problem as a subroutine. For exam-\nple, algorithms that maximize likelihood for learning latent variable\nmodels, e.g., the expectation-maximization (EM) algorithm (Dempster\net al., 1977), call the calculation of pθ(h|x) as a subroutine. Seeking\ncomputational efficient algorithms to solve these inference problems for\n6\nIntroduction\nincreasingly complex models has been an enduring challenge for our\nresearch community.\n1.1.1\nGenerative models and discriminative models\nOne major division in the probabilistic approach is generative versus\ndiscriminative modeling. In generative modeling, one aims to learn\nthe joint distribution pθ(x, h) over all the variables. In discriminative\nmodeling, one only models the conditional distribution pθ(h|x) over the\ntarget variable (denoted by h for convenience) given the observation x.\nIn discriminative modeling, the observation and the target variable are\nalso called the input and output, respectively.\nThe generative-discriminative distinction has received much atten-\ntion in machine learning (Ng and Jordan, 2001; Liang and Jordan, 2008).\nWhen a discriminative model follows the induced form of the condi-\ntional distribution pθ(h|x) from a generative model pθ(x, h), the two\nmodels are called a generative-discriminative pair (i.e., under the same\nparametric family of models) (Ng and Jordan, 2001). For example, naive\nBayes classifier and logistic regression, hidden Markov model (HMM)\n(Rabiner, 1989) and conditional random field (CRF) (Lafferty et al.,\n2001; Sutton, McCallum, et al., 2012), form Generative-Discriminative\npairs, respectively. To compare generative and discriminative learning,\nit seems natural to focus on such pairs. Basically, there are different\nregimes of performance as the training set size in increased. Taking naive\nBayes and logistic regression as a case study, it is shown in (Ng and\nJordan, 2001) that “while discriminative learning has lower asymptotic\nerror, a generative classifier may also approach its (higher) asymptotic\nerror much faster”. The comparison of HMM and CRF is further studied\nin (Liang and Jordan, 2008), and it is found that generative modeling\n(modeling more of the data) tends to reduce asymptotic variance, but\nat the cost of being more sensitive to model misspecification. These\nprevious results, including (Ng and Jordan, 2001; Liang and Jordan,\n2008), to name a few, strengthen our basic intuitions about generative-\ndiscriminative distinction.\nGiven that the generative and discriminative estimators are comple-\nmentary, one natural question is how to interpolate between the two\n1.1. THE PROBABILISTIC APPROACH\n7\nto get the benefits of both. There have studies for hybrid generative-\ndiscriminative methods (see Bouchard, 2007 and the references therein).\nNotably, those hybrid models have been applied for semi-supervised\nlearning (SSL), where one may have few labeled examples and many\nmore unlabeled examples, but mostly based on traditional generative\nmodels like naive Bayes.\nIn recent years, generative modeling techniques have been greatly\nadvanced by inventing new models with new learning algorithms under\nthe umbrella of deep generative models (DGMs), which are character-\nized by using multiple layers of stochastic or deterministic variables in\nmodeling and are much more expressive than classic generative mod-\nels such as naive Bayes and HMM. See (Ou, 2018) for a systematic\nintroduction to DGMs from perspective of graphical modeling. The\ngenerative-discriminative discussion continues with new points, when\nmore types of generative models have constantly emerged and become\nstudied. Here we provide two examples with the new points.\n• A type of DGMs, variational autoencoders (VAEs) (Kingma,\nWelling, et al., 2019), has been successfully applied in the setting\nof semi-supervised learning.\n• It is concurrently shown in (Song and Ou, 2018; Grathwohl et al.,\n2020) that a standard discriminative classifier pθ(y|x) can be used\nto directly define an energy-based model (EBM) for the joint dis-\ntribution pθ(x, y). It is shown in (Song and Ou, 2018) that energy-\nbased semi-supervised training of the joint distribution produces\nstrong classification results on par with state-of-art DGM-based\nsemi-supervised methods. It is demonstrated in (Grathwohl et al.,\n2020) that energy based training of the joint distribution improves\ncalibration, robustness, and out-of-distribution detection while\nalso generating samples rivaling the quality of recent generative\nadversarial network (GAN) (Goodfellow et al., 2014) approaches.\n1.1.2\nConditional models\nDiscriminative models are a kind of conditional models for discrimina-\ntive tasks. However, conditional modeling is a more general modeling\n8\nIntroduction\nconcept than discriminative modeling. Basically, a conditional model is,\nin probability terms, a conditional distribution of a random variable of\ninterest, when another variable c is known to take a particular value.\nIn this case, c is often called the input of the model. The variable of\ninterest generally can still consist of observable and (optionally) hidden\ncomponents, denoted by x and h respectively. Thus, a conditional model\ncan generally be denoted by pθ(x, h|c).\nMany real-world applications are solved by conditional modeling.\nSome examples from discriminative tasks are as follows.\n• First, by abuse of notation, discriminative modeling of image\nclassification involves the conditional model pθ(y|x), where x is\nthe input image and y is the images’s class.\n• A more complicated example is the recurrent neural network\ntransducer (RNN-T) model (Graves, 2012) for speech recognition.\nLet x denote the input speech, y the label sequence (e.g., word\ntranscription), and π the hidden state sequence (or say, a path)\nwhich realizes the alignment of x and y. Then the RNN-T model\ninvolves the conditional model pθ(y, π|x). See Section 4.3.1 for\nmore details on RNN-T.\nApart from discriminative tasks, conditional models can also be\nused for conditional generation tasks. One example is the reverse of the\nimage classification problem: prediction of a distribution over images,\nconditioned on the class label.\nImportantly, one should keep in mind that the learning and inference\nmethods introduced in unconditional modeling are in theory equally\napplicable to conditional models. So the basics introduced in Chapter\n2 lay the foundation for both (unconditional) EBMs in Chapter 3 and\nconditional EBMs in Chapter 4. On the other hand, the unconditional\nand conditional settings have their own characteristics, and thus needs\ndifferent treatments, as we will detail in Chapter 3 and 4 respectively.\n1.2\nFeatures of EBMs\nIn the probabilistic approach, the family of models chosen in real-world\napplications clearly plays a crucial role. In terms of graphical modeling\n1.2. FEATURES OF EBMS\n9\nterminology (Koller and Friedman, 2009), probabilistic models can be\nbroadly classified into two classes - directed and undirected.\n• In directed graphical models (DGMs), also known as (a.k.a.)Bayesian\nnetworks (BNs) or called locally-normalized models, the distri-\nbution is factorized into a product of local conditional density\nfunctions.\n• In contrast, in undirected graphical models (UGMs), also known\nas Markov random fields (MRFs) or energy-based models (EBMs)\nor called globally-normalized models, the distribution is defined to\nbe proportional to the product of local potential functions. The\nthree terms, UGMs, MRFs and EBMs, are exchangeable in this\nmonograph.\nSimply speaking, an easy way to tell an undirected model from a directed\nmodel is that an undirected model is un-normalized and involves the\nnormalizing constant (also called the partition function in physics),\nwhile the directed model is self-normalized.\nIn general, directed models and undirected models make different\nassertions of conditional independence. Thus, there are families of prob-\nability distributions that are captured by a directed model and are\nnot captured by any undirected model, and vice versa (Pearl, 1988).\nTherefore, undirected models, though less explored, provide an impor-\ntant complementary choice to directed models for various real-world\napplications.\nDuring these years, EBMs have attracted increasing interests not\nonly from core machine learning but also from application domains such\nas speech, vision, natural language processing and so on, with significant\ntheoretical and algorithmic progress. There have emerged a dedicated\nworkshop at ICLR 2021, which is a broad forum about EBM researches,\nand a tutorial at CVPR 2021, which focuses on computer vision tasks.\n• ICLR2021 Workshop - Energy Based Models: Current Perspec-\ntives, Challenges, and Opportunities, https://sites.google.com/\nview/ebm-workshop-iclr2021\n• CVPR 2021 Tutorial: Theory and Application of Energy-Based\nGenerative Models, https://energy-based-models.github.io/\n10\nIntroduction\nTo the best of our knowledge, there are no review papers about EBMs\nwith applications to speech and language processing. The sequential\nnature of speech and language also presents special challenges and needs\ntreatment different from processing fix-dimensional images that was\ndescribed in the CVPR 2021 tutorial. The aim of this monograph is\nto present a systematic introduction to energy-based models, including\nboth algorithmic progress and applications in speech and language\nprocessing. We hope it will also be of general interests to the artificial\nintelligence and signal processing communities.\nBefore delving into the specific content, we first point out five key\nfeatures of EBMs, which may motivate you to pursue the study and\napplication of EBMs.\n• Flexibility in modeling. Compared to modeling a self-normalized\ndensity function, learning EBMs relaxes the normalization con-\nstraint and thus allows much greater flexibility in the parameteri-\nzation of the energy function. Moreover, undirected modeling is\nmore natural for certain domains, where fixing the directions of\nedges is awkward in a graphical model.\n• Computation efficiency in likelihood evaluation, since the negative\nlog likelihood of an EBM (by ignoring an additive constant)\ncan be easily evaluated, without incurring any calculation for\nnormalization.\n• Naturally overcoming label bias and exposure bias suffered by\nlocally-normalized models (Section 4.1.2).\n• Superiority for hybrid generative-discriminative and semi-supervised\nlearning (Section 5).\n• Challenge in model training. Both computation of the exact likeli-\nhood and exact sampling from EBMs are generally intractable,\nwhich makes training especially difficult.\n1.3\nOrganization of this monograph\nThe rest of the monograph is organized as follows.\n1.3. ORGANIZATION OF THIS MONOGRAPH\n11\nFigure 1.2: Outline of this monograph\nIn Chapter 2, we present basics for EBMs. We start with a brief\nintroduction to probabilistic graphical models (PGMs), because basically\nwe introduce EBMs as undirected graphical models (UGMs). Then, we\npresent EBM model examples, including both classic ones (such as\nIsing model and restricted Boltzmann machines) and modern ones\nparameterized by neural networks. Next, basic algorithms for learning\nEBMs are described, which covers the two most widely used classes of\nmethods - Monte Carlo based maximum likelihood methods and noise-\ncontrastive estimation (NCE) methods. Finally, we present a dedicated\nsection to introduce how to sample/generate from EBMs, since sampling\nis not only a critical step in maximum likelihood learning of EBMs, but\nalso itself forms as an important class of applications in speech and\nlanguage processing.\nThe basics for inference and learning with EBMs are general for both\ndiscrete and continuous data modeling. Remarkably, most applications\ncovered in this monograph is discrete data modeling (text in natural\nlanguage processing, discrete labels in speech recognition), but in some\nplaces, we also present examples and applications in images. For example,\nIsing model is introduced for readers to get the abstract concepts\nconveyed by EBMs. EBM based joint-training for semi-supervised image\nclassification is a fixed-dimensional counterpart of the more complicated\nsequence setting, which is for semi-supervised natural language labeling.\nThe next three chapters are devoted to introduce how to develop\n12\nIntroduction\nEBMs in three different scenarios respectively.\n• Note that the sequential nature of speech and language presents\nspecial challenges and needs treatment different from processing\nfix-dimensional data (e.g., images). In Chapter 3, we introduce\nEBMs for sequential data with applications in language model-\ning. In this scenario, we are mainly concerned with learning the\n(marginal) distribution of an observation sequence x itself, e.g., a\nnatural language sentence as in language modeling.\n• In Chapter 4, we introduce EBMs for modeling conditional distri-\nbutions of target sequences given observation sequences. Condi-\ntional EBMs have been successfully applied in speech recognition,\nsequence labeling in natural language processing (NLP), and var-\nious forms of conditional text generation (e.g., controlled text\ngeneration, factual error correction).\n• In Chapter 5, we introduce EBMs for modeling joint distributions\nof both sequences of observations and targets. We first introduce\nthe fixed-dimensional case, then move on to the sequential case,\nand finally present the applications in semi-supervised natural\nlanguage labeling and calibrated natural language understanding.\nFinally, conclusions are given in Chapter 6 to summarize the mono-\ngraph and to discuss future challenges and directions.\nWe visualize the content of this monograph in Figure 1.2. At the\ncenter is the basic knowledge for EBM modeling and learning. The\nbasic theory can be applied to model different types of distributions –\nthe distribution of the observation itself, the conditional distribution,\nand the joint distribution. In different applications or scenarios, we are\nconcerned with different types of distributions. In Chapter 3, 4, and\n5, we in fact show how to develop EBMs for the three different types\nof distributions in three different scenarios, respectively, as described\nabove.\nThis monograph contains the material expanded from the tutorial\nthat the author gave at ICASSP 2022 in May 2022. Substantial updates\nhave been made to incorporate more recent work and cover wider areas\nof research activities.\n2\nBasics for EBMs\nBasically we introduce EBMs as undirected graphical models. We begin\nwith background on probabilistic graphical models, which would be\nbeneficial for readers to intuitively appreciate the differences between\ndirected graphical models (DGMs) and undirected graphical models\n(UGMs).\n2.1\nProbabilistic graphical models (PGMs)\nProbabilistic graphical models provide a general framework for describ-\ning and applying probabilistic models in the probabilistic approach.\nMany ideas developed in the probabilistic approach can be understood,\nunified, and generalized within the formalism of graphical models.\n“A graphical model is a family of probability distributions\ndefined in terms of a directed or undirected graph. The nodes\nin the graph are identified with random variables, and joint\nprobability distributions are defined by taking products over\nfunctions defined on connected subsets of nodes.” (Jordan,\n2004)\nConsider a graph G = (V, E) where V is a set of vertices (also\n13\n14\nBasics for EBMs\nFigure 2.1: (a) A simple directed graphical model with four variables (x1, x2, x3, x4).\n(b) A simple undirected graphical model with four variables (x1, x2, x3, x4). For both\ntypes of graphs, V denotes the set of nodes and E the set of edges. If both ordered\npairs (α, β) and (β, α) belong to E, we say that we have an undirected edge between\nα and β. A nice introduction of graph theory in the context of graphical models\ncould be found in Chapter 4 of Cowell et al., 1999.\ncalled nodes) and the set of edges E is a subset of the set V × V .\nSee Figure 2.1 for an illustration of these concepts from graph theory.\nLet1 xV ≜{xv : v ∈V } be a collection of random variables indexed\nby the nodes of the graph. A graphical model in terms of G describes\na family of probability distributions p(xV ) over the variables xV . A\nvariable can either be scalar- or vector-valued, where in the latter case\nthe vector variable implicitly corresponds to a sub-graphical model over\nthe elements of the vector.\nThe edges E specifies the connections between the nodes and, ac-\ncording to the graph semantics (see below), plays a crucial role in\ndefining the graphical model distribution. One view is that the edges E,\ndepending on the graph semantics, determines a particular factorized\nform of the distribution. Another view is that the edges E, of course\nstill depending on the graph semantics, determines a particular set of\nconditional independence (CI) assumptions over the random variables.\nIn both views, the properties, either the factorized form or the CI\nproperties, implied by the graphical model are true for all members of\n1As described in Section A.1, we allow sets of indices to appear wherever a single\nindex appears.\n2.1. PROBABILISTIC GRAPHICAL MODELS (PGMS)\n15\nits associated distribution family. As we will see in Section 2.1.1 and\nSection 2.1.2, the two views are, in a strong sense, equivalent.\nGraphical models can be defined over different types of graphs,\ndirected, undirected or mixed, each with differing semantics. The se-\nmantics specifies what a graphical model means and tells how the family\nof distributions is defined (Russell and Norvig, 2010; Koller and Fried-\nman, 2009). The set of CI properties specified by a particular graphical\nmodel, and therefore the family of probability distributions it represents,\nwill be different depending on the type of graphical model currently\nbeing considered.\nThe two most common forms of graphical model (GM) are directed\ngraphical models (DGMs) and undirected graphical models (UGMs),\nbased on directed acylic graphs and undirected graphs, respectively. In\ngeneral, directed graphs and undirected graphs make different assertions\nof conditional independence. Thus, there are families of probability dis-\ntributions that are captured by a directed graph and are not captured by\nany undirected graph, and vice versa (Pearl, 1988; Koller and Friedman,\n2009).\n2.1.1\nDirected graphical models\nLet us begin with the directed case. Continuing with the notations in\nSection 2.1, let G = (V, E) be a directed acyclic graph (DAG), and xV\nbe a collection of random variables indexed by the nodes of G. For each\nnode v ∈V , let pa(v) denote the subset of indices of its parents; thus\nxpa(v) denotes the vector of random variables indexed by the parents of\nv.\nDefinition 2.1 (DGM). A directed graphical model in terms of G consists\nof a family of distributions that factorize in the following way:\np(xV ) =\nY\nv∈V\np(xv|xpa(v))\n(2.1)\nWe then also say that p(xV ) has the directed factorization property\n(DF) according to G, or simply, p(xV ) factorizes according to G.\nRemarkably, the notation in Eq. (2.1) is self-consistent, because it\ncan be verified that the joint distribution p(xV ) defined by the factor-\n16\nBasics for EBMs\nization Eq. (2.1) has {p(xv|xpa(v))} as its conditionals. For the simple\ndirected graphical model shown in Figure 2.1(a), the joint distribution\nthat it describes is:\np(x1, x2, x3, x4) = p(x1)p(x2|x1)p(x3|x2)p(x4|x1, x3)\nGeneral speaking, the nodes in a graphical model correspond to\nthe random variables, and the edges indicate some direct probabilistic\ninteractions between the nodes. In directed graphical models, every edges\nis directed and, intuitively, correspond to direct influence of the parent\nnode on the child node. Thus, DGMs are suitable for modeling clear\ninfluence relationships between random variables, which are expressed\nthrough conditional distributions.\nFactorization and Markov properties in directed graphical models\nThe factorization in Eq. (2.1) implies a set of conditional independence\nstatements among the variables xV . In an opposite way, we could define\na set of conditional independence statements in terms of G, which is\noften referred to as a Markov property over G. A range of Markov\nproperties could be defined, relative to G, such as the directed global\nMarkov property (DG), the directed local Markov property (DL), the\ndirected ordered Markov property (DO) (See Section 5.3 of Cowell et al.,\n1999). It can be shown that the three Markov properties, (DG), (DL)\nand (DO), are equivalent, and further, they are equivalent to the (DF)\nproperty. These properties collectively characterize a graphical model\n(i.e., a family of distributions defined in terms of a graph). The Markov\nproperties of a distribution are precisely what allow it to be expressed\ncompactly in a factorized form. Conversely, a particular factorization of\na distribution guarantees that certain independencies hold.\nDGM example - HMM\nMany classic probabilistic models in speech and language processing\ncan be easily understood in terms of graphical models. Figure 2.2 shows\nthe graphical model representation of an hidden Markov model (HMM)\n(Rabiner, 1989), which has been widely used in speech recognition and\nvarious natural language processing tasks.\n2.1. PROBABILISTIC GRAPHICAL MODELS (PGMS)\n17\nFigure 2.2: Graphical model representation of a hidden Markov model (HMM).\nIn an HMM, there is an underlying hidden Markov chain, corre-\nsponding to the state sequence π1:T ≜π1 · · · πT . At each time frame,\ndepending on the state πt, the model probabilistically emit an output\nxt, which can be observed. The joint distribution is given by\np(π1:T , x1:T ) = p(π1)\nT−1\nY\nt=1\np(πt+1|πt)\nT\nY\nt=1\np(xt|πt)\n(2.2)\nwhere p(πt+1|πt) and p(xt|πt) are often called state-transition distri-\nbution and state-output distribution, respectively. It is easy to verify\nthat the graphical model as shown in Figure 2.2 exactly describes the\njoint distribution Eq. (2.2), by following the DGM semantics - the joint\ndistribution is defined as the product of local conditionals of each variable\ngiven its parents. Through this example, readers can appreciate the\nnaturalness of the graphical model approach in formulating probabilistic\nmodels of complex phenomena.\nDGM example - Neural network based classifier\nTraditionally, each conditional probability distribution p(xv|xpa(v)) is\nparameterized as a lookup table or a linear model (Koller and Friedman,\n2009). A more flexible way to parameterize such conditional distributions\nis with neural networks. In this case, a neural networks takes as input the\nparents of a variable in a directed graph, and produces the distributional\nparameters over the variable.\nη = NeuralNet(xpa(v))\np(xv|xpa(v)) = PDF(xv|η)\n18\nBasics for EBMs\nFigure 2.3: Neural network based classifier. (a) GM representation; (b) Computa-\ntional graph representation.\nwhere we use NeuralNet(·) and PDF(·|η) to generally denote a neural\nnetwork (NN) function and a probability density function (PDF) param-\neterized by η, respectively. For example, if xv is a continuous variable,\nη could denote the mean and variance parameters; if xv is a discrete\nvariable, η could denote the logits (as explained below).\nBasically, we can employ the above concept to build arbitrary di-\nrected graphical models, which is not the main focus of this monograph.\nFor illustration, let us examine the widely used NN based classifier,\nwhich could be cast as a simple two-node directed model for observation\nx ∈RD and class label y ∈{1, · · · , K}, as shown in Figure 2.3. It is\nimportant to differentiate the GM representation and computational\ngraph representation.\nThe classic multi-class logistic regression (Bishop, 2006; Murphy,\n2012) basically is to use a single linear layer to obtain the logits zk’s,\nwhich are then fed to a softmax layer to calculate the class posterior:\np(y = k|x) =\nexp(zk)\nPK\nj=1 exp(zj) ≜softmax(z1:K)k\n(2.3)\nwhere\nzk = wT\nk x + bk, k = 1, · · · , K\n(2.4)\nare often called the logits2, and wk ∈RD, bk ∈R denote the weight\nvector and bias of the linear layer. A simple notation to describe the\n2Presumably because the argument of the sigmoid function is often called the logit,\nso analogously, the argument of the softmax function (as a multi-class generalization\nof the sigmoid) is also called the logit.\n2.1. PROBABILISTIC GRAPHICAL MODELS (PGMS)\n19\nlinear layer Eq. (2.4) is\nzk = Linear(x|wk, bk)\nor denoted as zk = Linear(x) when the parameters are suppressed.\nA recent advance in deep learning is that we can use a multi-layer\nneural network, often referred to as a deep neural network (DNN), to\ncalculate the logits, and then still use the softmax function to obtain\nthe probability vector from the logits. In this way, the multi-layer\nNN could be viewed as a non-linear feature extractor, which hopefully\ncan be trained to extract features, more discriminative than the raw\nobservation x. Here in describing an NN based classifier, we show\na directed model which has shallow stochastic connections but use\ndeep deterministic layers in implementing the conditional distributions.\nVariational autoencoder (VAE) (Kingma, Welling, et al., 2019) is also\nsuch a model. Further, directed models with deep stochastic connections\nhave also been examined such as such as Sigmoid belief Networks (SBNs)\n(Neal, 1992; Saul et al., 1996), Helmholtz machines (HMs) (Hinton et al.,\n1995; Dayan et al., 1995).\n2.1.2\nUndirected graphical models\nLet us now consider the undirected case. Given an undirected graph\nG = (V, E), we again let xV be a collection of random variables indexed\nby the nodes of the graph and let C denote the set of cliques3 of the\ngraph. Associated with each clique C ∈C, let ϕC(xC) denote a potential\nfunction, which is a non-negative function of its arguments.\nDefinition 2.2 (UGM). With the above notation, an undirected graphical\nmodel in terms of G consists of a family of distributions that factorize\nas:\np(xV ) = 1\nZ\nY\nC∈C\nϕC(xC)\n(2.5)\nwhere Z is the normalizing constant (also known as the partition func-\ntion) given by\nZ =\nX\nxV\nY\nC∈C\nϕC(xC)\n(2.6)\n3A subset of nodes C is called a clique, if every pair of nodes in C is joined.\n20\nBasics for EBMs\nWe then also say that p(xV ) has the factorization property (F) according\nto G, or simply, p(xV ) factorizes according to G.\nFor the simple undirected graphical model shown in Figure 2.1(b),\nthe joint distribution that it describes is4:\np(x1, x2, x3, x4) = 1\nZ ϕ(x1, x2)ϕ(x2, x3)ϕ(x3, x4)ϕ(x1, x4)\nRemarkably, the potentials do not need to be self-normalized, only\nrequired to be non-negative functions. In contrast, the conditionals\nin directed models are required to be normalized. Thus, undirected\nmodels generally offer more flexibility in modeling than directed models.\nMoreover, undirected models do not require us to specify edge orienta-\ntions, and are well suited to be used in problems in which there is little\ndirectional structure to guide the construction of a directed graph.\nFactorization and Markov properties in undirected graphical models\nWe have discussed the equivalence of factorization and conditional\nindependence in directed models in Section 2.1.1. Similarly, a range of\nMarkov properties could be defined, relative to a undirected graph G,\nsuch as the global Markov property (G), the pairwise Markov property\n(P), the local Markov property (L) (See Section 5.2 of Cowell et al.,\n1999). However, in contrast to the discussion in the case of directed\ngraphical models, the four properties, (F), (G), (L) and (P), are different\nin general. In general, we have (F)⇒(G)⇒(L)⇒(P). In the case where\np(xV ) has a positive density (i.e., never zero or negative for any value of\nxV ), it can be shown that (P) implies (F), and thus the four properties\nbecome equivalent. This result is known as the Hammersley-Clifford\ntheorem.\nWhen we use the term undirected graphical model without further\nqualification, we shall always mean one that factorizes, hence satisfies all\nof the properties. In the following, we will detail the (G) property, which\n4Note that factorization over the set of cliques can be easily shown to be equivalent\nto factorization over the set of maximal cliques, i.e., the set of all cliques that are not\nproperly contained within any other clique. Therefore, the joint distribution in this\nexample can be written as the product of 4 potentials over the 4 maximum cliques,\ndivided by the normalizing constant.\n2.1. PROBABILISTIC GRAPHICAL MODELS (PGMS)\n21\nFigure 2.4: Illustration of the global Markov property in UGMs.\nis important because it enable us to easily decide when two groups of\nvariables A and B are conditionally independent give a third group of\nvariables S in an undirected model.\nA probability distribution p(xV ) is said to obey the global Markov\nproperty (G), relative to a undirected graph G , if for any triple (A, B, S)\nof disjoint subsets of V such that S separates A from B5, we have\nxA ⊥xB|xS. Simply put, the (G) property means: separation between\nnodes in the graph implies CI between variables in the distribution, as\nillustrated in Figure 2.4.\nEnergy-based models and Gibbs distributions\nDefinition 2.3 (EBM). When we are restricted to potential functions\nwhich are strictly positive, it is convenient to express them as exponen-\ntials, so that\nϕC(xC) = exp[−EC(xC)]\nwhere EC(xC) is called an energy function. Hence, negative log-potential\nis often called energy, and high probability states correspond to low\nenergy configurations. Distributions of this exponential form are called\nenergy-based models (EBMs), also known as the Gibbs (or Boltzmann)\ndistributions, originating from statistical physics:\np(xV ) = 1\nZ exp\n\"X\nC∈C\n−EC(xC)\n#\n(2.7)\nA benefit of the form of EBMs is that unlike the potential functions,\nthe log-potential functions (or after negating, the energy functions) are\n5We say S separates A from B if all trails from A to B intersect S.\n22\nBasics for EBMs\nnot constrained to be non-negative and can be very flexibly parameter-\nized.\nLog-linear models and maximum entropy models\nDefinition 2.4 (Log-linear model). A classic approach to implement\nlog-potentials is to define them as a linear function of the parameters:\nlog ϕC(xC) = θT\nCfC(xC)\nwhere fC(xC) is a feature vector derived from (the values of) the variable\nxC, θC is the associated feature weight vector. The resulting distribution\nhas the form\np(xV ) =\n1\nZ(θ) exp\n\"X\nC\nθT\nCfC(xC)\n#\n(2.8)\nwhere θ = {θC | C ∈C} collectively denotes the model parameters and\nwe explicit the dependence of the normalizing constant on θ. This is\nknown as a log-linear model or an exponential-family model (Wainwright,\nJordan, et al., 2008; Murphy, 2012).\nExample 2.1 (Word morphology). As an illustrative example, suppose\nwe are interested in making a probabilistic model of English spelling.\nThis is known as the word morphology problem, which aims to model\nEnglish words as letter sequences, x1, x2, · · · , by assigning probabilities\n(Wang et al., 2018; Pietra et al., 1997; Murphy, 2012). Since certain\nletter combinations occur together quite frequently (e.g., “ing”), we will\nneed higher order cliques to capture this. Suppose we limit ourselves\nto letter trigrams. Since the variables in the sequence here are discrete,\nwe can represent the potential functions as tables of (non-negative)\nnumbers, which are often called tabular potentials. A tabular potential\nhas 263 = 17, 576 parameters in it. However, most of these triples will\nnever occur. An alternative approach is to define indicator functions\n(as feature functions) that look for certain special triples, such as “ing”,\n“qu-”, etc. Then we can define the potential at position t as follows:\nϕt(xt−1, xt, xt+1) = exp\n X\nk\nθkfk(xt−1, xt, xt+1)\n!\n2.1. PROBABILISTIC GRAPHICAL MODELS (PGMS)\n23\nwhere k indexes the different features, corresponding to “ing”, “qu-”,\netc., and fk is the corresponding binary feature function6. By tying the\nparameters across positions, we can define the probability of a word of\nany length T using\np(x1:T |θ) = exp\n T\nX\nt=1\nX\nk\nθkfk(xt−1, xt, xt+1)\n!\n(2.9)\nIt has been shown in (Wang et al., 2018) that a (trans-dimensional)\nlog-linear model outperforms a traditional n-gram model, using the\nsame set of features.\nModeling with Eq. (2.9) raises the question of where these feature\nfunctions come from. Traditionally, the features fC are mainly hand-\ncrafted to reflect domain knowledge, and people take efforts for feature\nengineering. In fact, log-linear models with careful feature engineering\nwere mainstream in NLP before the revival of neural approaches. Re-\ncently, neural networks have been successfully used to implement more\ngeneral form of EBMs, where, unlike in classic log-linear models, the\nlog-potentials are defined by NN-based non-linear functions, as we will\nshow later in Section 2.2.3. Notably, if NN-based log-potential uses a\nlinear final layer, these NN-based EBMs could in some sense still be\nviewed as log-linear models, but on top of the learned features extracted\nby the trainable neural networks.\nInterestingly, log-linear models are closely connected to maximum\nentropy (maxent) models. A classic conclusion is that the maxent dis-\ntribution (i.e., the distribution with the maximum entropy subject to\nthe constraints that empirical expectation of features equal to model\nexpectation of features) is the same as the maximum likelihood distri-\nbution from the closure of the set of log-linear distributions (MacKay,\n2003; Pietra et al., 1997) (see Appendix B.1 for details). Hence EBMs,\nwhich could be viewed as log-linear models as we describe above, also\nenjoy such a connection to maxent models. Remarkably, the EBM dis-\ntribution p(x) is only known up to a normalizing constant Z and the\npotentials are not probabilities, it may be hard for us to understand\n6For example, f“ing”(xt−1, xt, xt+1) equals to 1, if xt−1 = “i”, xt = “n”, xt+1 =\n“g”, and equals to 0, otherwise.\n24\nBasics for EBMs\nEBM models. Such maximum entropy property of EBM models allows\nus to gain intuitive understanding of EBM models.\n2.2\nEBM model examples\nIn this section, we first introduce classic EBM model examples including\nIsing model and restricted Boltzmann machine, to familiarize readers\nwith basic concepts. Then, we focus on modern ones parameterized by\nneural networks.\n2.2.1\nIsing model in statistical physics\n(a)\n(b)\nFigure 2.5: Ising model: (a) The undirected graph representation, (b) A sample.\nA well-known example of EBMs is the Ising model, which arose from\nstatistical physics, and has been a classic probabilistic model for binary\nimages. It was originally used for modeling the behavior of magnets.\nIn particular, the spin of an atom can either be spin down or up. The\nspins are arranged in a lattice, allowing each spin to interact with its\nneighbors. Neighboring spins that agree have a lower energy than those\nthat disagree; the system tends to the lowest energy but heat disturbs\nthis tendency, thus creating the possibility of different structural phases.\nThe two-dimensional square-lattice Ising model is one of the simplest\nstatistical models to show a phase transition.\nConsider a lattice of variables x1:N, each component xi taking values\n−1/+1 to model a spin down/up or a pixel black/white, as shown in\n2.2. EBM MODEL EXAMPLES\n25\nFigure 2.5. This\n√\nN ×\n√\nN lattice, as an undirected graph, defines an\nIsing model p(x1:N) ∝exp[−E(x1:N)] with the energy function\nE(x1:N) = −β\n\nX\ni∼j\nJxixj +\nX\ni\nHxi\n\n\n(2.10)\nwhere i ∼j denotes that two spins i and j are neighbours.\nThe energy for an Ising model includes two contributions: the inter-\naction between neighboring spins and the effect of an applied external\nmagnetic field on each individual spin. Consider the case of ferromag-\nnetism. The interaction between neighboring spins tends to induce\nparallel alignment of the neighbors, so it should be favorable (negative\nenergy) when the neighbors are both +1 or both −1, and unfavorable\n(positive energy) when the neighbors are +1 next to −1. Hence, for\neach pair of neighbors i and j, the interaction energy can be written as\n−Jxixj, where J is a positive coefficient giving the interaction strength.\nIf the applied magnetic field is pointing up, it favors each spin pointing\nup; if the field is pointing down, it favors each spin pointing down.\nHence, for each site i, the field energy can be written as −Hxi, where H\ndenotes the magnetic moment of the field. Putting these pieces together,\nthe total energy for the system becomes Eq. (2.10).\nIt is usual to include in Eq. (2.10) the inverse temperature parameter\nβ =\n1\nkBT , where kB is Boltzmann’s constant, and T the temperature. β\nmeasures how much neighboring spins take identical values is favored.\nThe larger β (equivalently the lower temperature T) is, the more favor-\nable of neighboring spins to take identical values. Figure 2.6 shows a\nsequence of typical samples from the simulation of N = 4096 spins at a\nsequence of decreasing temperatures. At infinite temperature (β = 0),\neach spin is completely independent of any other, and if typical states\nat infinite temperature are plotted, they look like television snow. For\nhigh, but not infinite temperature, there are small correlations between\nneighboring positions, the snow tends to clump a little bit, but the\nscreen stays randomly looking. When the temperature decreases (β\nincreases), it is more favored for neighboring spins to take identical\nvalues, so large patches of black or white become to appear.\nThrough this example, we could get some sense of the characteristics\nof EBM modeling - EBMs are natural for modeling interactions (mutual\n26\nBasics for EBMs\nFigure 2.6: Sample states of square Ising models with J = 1, H = 0, kB = 1, N =\n4096 at a sequence of temperatures T = 5, 2.5, 2.4, 2.3, 2. (MacKay, 2003)\ninfluences), where the directions of edges cannot be clearly defined. For\nexample, here it is hard to say one pixel determines another pixel, even\nin a probabilistic sense. It is better to use undirected edges to model\ninteractions between pixels through energy functions. Particularly in\nthis example, for each pair of nodes connected by an edge in the lattice,\nthere is an clique potential, which is implemented as follow:\nϕij(xi, xj) =\n(\neβ,\nxi = xj = ±1\ne−β,\nxi = −xj = ±1\nwhere we set J = 1, H = 0, kB = 1. So when the two neighboring pixels\ntake the same value, it will contribute eβ to the un-normalized density;\notherwise, contribute e−β.\n2.2.2\nRestricted Boltzmann Machines (RBMs)\nRestricted Boltzmann machines (RBMs) are main building blocks of\ndeep belief networks (DBNs) (Hinton et al., 2006), which ignite deep\nlearning. A RBM is a classic undirected graphical model with hidden\nvariables. It is defined over a bipartite graph, in which the visible,\nbinary stochastic variables v ∈{0, 1}D are connected to hidden binary\nstochastic variables h ∈{0, 1}H, as shown in Figure 2.7. The energy of\nthe state {v, h} is defined over cliques7:\nEθ(v, h) = −vT Wh −bT v −aT h\n= −\nD\nX\ni=1\nH\nX\nj=1\nviWijhj −\nD\nX\ni=1\nbivi −\nH\nX\nj=1\najhj\n7Each node is a clique, and for each edge connecting vi and hj, there is a clique.\n2.2. EBM MODEL EXAMPLES\n27\nFigure 2.7: Restricted Boltzmann Machine. The top layer represents a vector of\nstochastic binary hidden variables h and the bottom layer represents a vector of\nstochastic binary visible variables v. (Salakhutdinov, 2009)\nwhere θ = {W, b, a} are the model parameters: Wij represents the\nsymmetric interaction term between visible unit i and hidden unit j; bi\nand aj are bias terms. The joint distribution is:\npθ(v, h) =\n1\nZ(θ) exp [−Eθ(v, h)]\nZ(θ) = −\nX\nv\nX\nh\nexp [−Eθ(v, h)]\nwhere Z(θ) is the normalizing constant.\nDue to the special bipartite structure of RBMs, given v, different\nhidden units hj’s are separated and thus are conditionally independent,\naccording to the Markov property of UGMs. Therefore, the conditional\ndistribution of h given v is factored:\npθ(h|v) =\nY\nj\npθ(hj|v)\npθ(hj|v) ∝exp\n X\ni\nviWijhj + ajhj\n!\nfrom which we could easily obtain the conditional probability of a single\nunit hj, expressed by the sigmoid function σ(·):\npθ(hj = 1|v) = σ\n X\ni\nWijvi + aj\n!\n(2.11)\nSimilarly, the conditional distribution of v given h is also factored\n28\nBasics for EBMs\nand given by:\npθ(v|h) =\nY\ni\npθ(vi|h)\npθ(vi|h) ∝exp\n\nX\nj\nviWijhj + bivi\n\n\npθ(vi = 1|h) = σ\n\nX\nj\nWijhj + bi\n\n\n(2.12)\nRemarkably, it can be seen from Eq. (2.11) that an RBM is related to\na stochastic version of a neural network, also known as a sigmoid belief\nnetwork (SBN) (Neal, 1992; Saul et al., 1996). To see this8, imagine\nthat the nodes v1:D and the edges of an RBM as shown in Figure\n2.7 are viewed as the input layer and the synaptic connections of a\ntwo-layer SBN; the output layer at h1:H fires, hj taking 0 or 1, j =\n1, · · · , H, stochastically from a sigmoid activation function. Therefore,\nthe conditional distribution pθ(h|v) induced from an RBM can be viewed\nas implementing a two-layer SBN, and a two-layer SBN is very similar to\nan ordinary two-layer feedfoward neural network, except it stochastically\nfires instead of outputting activations to the next layer (see Figure 2.8).\nThis resemblance between RBMs and NNs is the underlying intuition\nthat a stack of RBMs can be trained as pre-training for a multi-layer\nneural network (Salakhutdinov, 2009).\n(a)\n(b)\nFigure 2.8: (a) Restricted Boltzmann machine (RBM), (b) Sigmoid belief network\n(SBN).\n8Such relationship could also seen from Eq. (2.12) from an opposite direction.\n2.2. EBM MODEL EXAMPLES\n29\n2.2.3\nEBMs parameterized by neural networks\nClassic EBM models employ simple energy functions, e.g., the energy\nfunctions in both the Ising model and the RBM model are bilinear.\nRecently, beyond the classic EBM models, there have emerged a bundle\nof deep EBM models (deep undirected generative models), which are\ncharacterized by using multiple layers of stochastic or deterministic\nvariables.\nThose deep EBM models with multiple stochastic hidden layers such\nas deep belief networks (DBNs) (Hinton et al., 2006) and deep Boltz-\nmann machines (DBMs) (Salakhutdinov and Hinton, 2009) involve very\ndifficult inference and learning, which severely limits their applications\nbeyond of the form of pre-training. Another type of deep EBM models,\nwhich appear to be more successfully applied, is to directly define the\nenergy function through a multi-layer neural network. In this case, the\nlayers of the network do not represent latent variables but rather are\ndeterministic transformations of input observations.\nFor simplicity, we will first introduce unconditional models in the\nfollowing. It is relatively straightforward to extend to models with\nconditioning variables, which will be detailed in Chapter 4.\nDefinition 2.5 (EBMs parameterized by neural networks). Generally,\nconsider an EBM to define a probability distribution for a collection of\nrandom variables x ∈X with parameter θ in the form:\npθ(x) =\n1\nZ(θ) exp [Uθ(x)]\n(2.13)\nwhere X denotes the space of all possible values of x, and Z(θ) denotes\nthe normalizing constant:\nZ(θ) =\nZ\nexp [Uθ(x)] dx\n(2.14)\nUθ(x) : X →R denotes the (log) potential function9 which assigns a\nscalar value to each configuration of x in X and can be very flexibly\n9In the literature, log potential function is sometimes also referred to as potential\nfunction. Whether taking log or not should be clear from the context, although\nwith abuse of nomenclature. Moreover, reversing the potential function will obtain\nthe energy function, and vise versa. So an EBM could be equivalently defined by an\nenergy function or a potential function.\n30\nBasics for EBMs\nFigure 2.9: Potential functions in EBMs can be flexibly parameterized by neural\nnetworks for images, natural languages and so on.\nparameterized through neural networks of different architectures. For\ndifferent applications, X could be discrete or continuous, and x could be\nfix-dimensional or trans-dimensional (i.e., sequences of varying lengths).\nFor example, images are fix-dimensional continuous data (i.e., X =\nRD), and natural languages are sequences taking discrete tokens (i.e.,\nX = S\nl Vl where V is the vocabulary of tokens). The general idea is\nto parameterize Uθ(x) by a neural network, taking multi-variate x as\ninput and outputting scalar Uθ(x), so that we can take advantage of\nthe representation power of neural networks, as shown in Figure 2.9.\nHistorically, this type of EBMs has been studied several times in\ndifferent contexts. They are once called deep energy models (DEMs) in\n(Ngiam et al., 2011; Kim and Bengio, 2016), generative ConvNet in (Xie\net al., 2016), descriptive models in (Guo et al., 2003; Xie et al., 2018),\nneural trans-dimensional random field language models in (Wang and\nOu, 2017; Wang and Ou, 2018b; Wang and Ou, 2018a; Gao et al., 2020),\nneural random fields (NRFs) in (Song and Ou, 2018). There are some\nspecific differences between implementation (or say, parameterization)\nof the potential functions in these deep EBM models.\n• The potential function in (Ngiam et al., 2011; Kim and Bengio,\n2016) uses the form of a product of experts (Hinton, 2002) and is\ncomposed of linear and squared terms and the aggregated (i.e.,\n2.2. EBM MODEL EXAMPLES\n31\nthe sum of) logistic regression responses of a set of weak classifier\n(“expert”) for images x:\nUθ(x) = bT x −1\nσ2 xT x +\nX\ni\nlog(1 + ewT\ni fθ(x)+ci)\n(2.15)\nThe first two terms capture the mean and the global variance, and\nthe last term comes from a set of experts over the feature data\nspace fθ(x). fθ(x) is the output of a feedforward neural network.\nOne can allow activations other than logitic sigmoid as shown in\nEq. (2.15).\n• In (Wang and Ou, 2017; Wang and Ou, 2018b; Wang and Ou,\n2018a; Gao et al., 2020; Deng et al., 2020), deep EBM models are\ndefined over sequences (natural language sentences).\n• In (Xie et al., 2016; Guo et al., 2003; Xie et al., 2018; Wang and\nOu, 2017; Deng et al., 2020), the EBM is defined in the form of\nexponential tilting of a reference distribution q(x):\npθ(x) =\n1\nZ(θ)q(x) exp [Uθ(x)]\n(2.16)\nIn (Xie et al., 2016; Guo et al., 2003; Xie et al., 2018) for image\nmodeling, a Gaussian white noise distribution is used as q(x). In\n(Wang and Ou, 2017; Deng et al., 2020) for language modeling,\nautoregressive language models based on LSTM or Transformer\nnetworks are often used as q(x).\nDespite the different parameterizations of the potential function\nin various deep EBM models, these differences would not affect much\nwhen presenting learning algorithms, as we will introduce immediately\nin the next section.\nComparison between classic undirected graphical models and modern\nEBM models parameterized by neural networks\nAs we introduced before, classic undirected graphical models such as the\nIsing model and the RBM model employ simple potential functions like\nlinear or bilinear, while modern EBM models utilize neural networks to\n32\nBasics for EBMs\nparameterize potential functions. From this apparent difference, there\nis a remarkable implication, which will be described below.\nIn graphical modeling terminology, without loss of generality, let\neach component of x indexed by a node in an undirected graph. The\nEBM distribution pθ(x) is defined to be decomposed over cliques, as\nshown in Eq. (2.7). Such decomposition reduces the complexity of model\nrepresentation but maybe at the sacrifice of model expressive capacity.\nIn an EBM model parameterized by a neural network as introduced\nabove, the model essentially becomes defined over a fully-connected\nundirected graph and captures interactions in x to the largest order,\nsince the neural potential function U(x) involves all the components in\nx. In this manner, hopefully we can take advantage of the representation\npower of neural networks for modeling. As shown above, we can define\nvery flexible potential functions Uθ(x), by utilizing neural networks\nof various architectures to define the densities Eq. (2.13). For this\nreason, it is often assumed in theoretical analysis that pθ(x) has infinite\ncapacity (sometime called in the non-parametric setting). In practice, the\nperformances of the models largely depend on how they are optimized\nin model learning.\n2.3\nLearning EBMs by maximum likelihood\nThe de facto standard for learning probabilistic models from IID (in-\ndependent and identically distributed) data is maximum likelihood\nestimation (MLE) . Let pθ(x), as defined in Eq. (2.13), be an EBM\nmodel parameterized by θ, and pemp(x) ≜1\nN\nPN\ni=1 δ(x −xi) denote the\nempirical distribution for a training dataset consisting of N IID data\npoints {x1, · · · , xN}. We can fit pθ(x) to data by maximizing the scaled\nlog-likelihood of the data, defined by\nL(θ) ≜1\nN\nN\nX\ni=1\nlog pθ(xi) =\n\"\n1\nN\nN\nX\ni=1\nUθ(xi)\n#\n−log Zθ\n(2.17)\nas a function of θ.\nMaximizing likelihood is equivalent to minimizing the (inclusive)\n2.3. LEARNING EBMS BY MAXIMUM LIKELIHOOD\n33\nKL divergence between pemp(x) and pθ(x), because\nKL[pemp(x)||pθ(x)] = Ex∼pemp(x)[log pemp(x)] −Ex∼pemp(x)[log pθ(x)]\n= constant −L(θ),\nwhere the second equality holds because Ex∼pemp(x)[log pemp(x)] does\nnot depend on θ.\nTaking the derivative of log-likelihood with respect to (w.r.t.) θ, the\nfirst term of the gradient is a sum over data points and can be written\nas an expectation under the empirical distribution:\n1\nn\nN\nX\ni=1\n∇θUθ(xi) = Epemp(x) [∇θUθ(x)]\nThe second term involves taking the derivative of the log normalizing\nconstant, which, as shown below, can also be written as an expectation\nbut under model distribution pθ(x):\n∇θ log Zθ = 1\nZθ\n∇θZθ = 1\nZθ\nZ\n∇θ exp [Uθ(x)] dx\n= 1\nZθ\nZ\nexp [Uθ(x)] ∇θUθ(x)dx\n=\nZ exp [Uθ(x)]\nZθ\n∇θUθ(x)dx\n=\nZ\npθ(x)∇θUθ(x)dx\n= Epθ(x) [∇θUθ(x)]\n(2.18)\nCombining the two terms, we obtain the core formula in learning EBMs:\n∇θL(θ) = Epemp(x) [∇θUθ(x)] −Epθ(x) [∇θUθ(x)]\n(2.19)\nThe maximum likelihood estimate of θ is obtained as a solution to\n∇θL(θ) = 0. Obviously, the challenge is in calculating the expectation\nunder model distribution, which is often intractable to compute exactly\nand approximated by Monte Carlo averaging.\nSuppose that we can draw random samples from the EBM pθ(x),\ndenoted as x(1), · · · , x(M) ∼pθ(x), then we can obtain an unbiased\n34\nBasics for EBMs\nestimate of the second term in the log-likelihood gradient10:\nEpθ(x) [∇θUθ(x)] ≈1\nM\nM\nX\nj=1\n∇θUθ(x(j))\n(2.20)\nAdditionally, note that the computational cost of Eq. (2.19) is linear in\nN (the number of training data points). So when N is large, we often\napply minibatching as follows. Through random drawing a minibatch\nof κ1, · · · , κB from {1, · · · , N}, we can obtain an unbiased estimate of\nthe first term in the log-likelihood gradient:\nEpemp(x) [∇θUθ(x)] ≈1\nB\nB\nX\nj=1\n∇θUθ(xκj)\n(2.21)\nCombining Eq. (2.20) and Eq. (2.21) allows us to optimize the parame-\nters with stochastic gradient ascent. See further introduction in Section\n2.3.3.\nWe have shown above the basic idea of applying Monte Carlo meth-\nods in maximum likelihood learning of EBMs. As long as we can draw\nrandom samples from the model pθ(x), we have access to an unbiased\nestimate of the log-likelihood gradient, allowing us to optimize the\nparameters with stochastic gradient ascent. So a critical step in\nlearning EBMs by Monte Carlo methods is the simulation (sam-\npling) from the EBM distribution pθ(x), as defined in Eq. (2.13) in\ngeneral.\nIn some applications, directly generating independent samples from\nan distribution is not feasible. There are two broad classes of strategies\nfor sampling from high-dimensional distributions. The first is the MCMC\nstrategy, which produces statistically dependent samples based on the\ntheory of Markov chains. The second is the importance sampling (IS)\nstrategy, in which independent samples are generated from a trial\ndistribution (a.k.a. a proposal distribution) and then weighted according\nto the importance weight. The two methods will be introduced as follows\nin Section 2.3.1 and Section 2.3.2 respectively. Further introduction can\n10Here we suppose that the samples are direct samples from pθ(x). As will\nbe described in Section 2.3.3, such assumption can be relaxed in the stochastic\napproximation methodology, which allows us to use Markov chain samples.\n2.3. LEARNING EBMS BY MAXIMUM LIKELIHOOD\n35\nbe found in monographs (Neal, 1993; Liu, 2001) or general textbooks on\nmachine learning (Koller and Friedman, 2009; Murphy, 2012; Bishop,\n2006).\n2.3.1\nMarkov Chain Monte Carlo (MCMC)\nLet pθ(x) be the target distribution under investigation. The basic\nidea of Markov Chain Monte Carlo (MCMC) is to construct a Markov\nchain in the state space of x, denoted by X, so that the limiting (or\nsay, stationary or equilibrium) distribution of this chain is the target\ndistribution pθ. Roughly speaking, this means that the fraction of time\nspent in each state along the chain in a long run is equal to pθ.\nMetropolis–Hastings algorithm\nA classic MCMC method is the Metropolis-Hastings (MH) algorithm,\nwhich is described in Algorithm 1. Starting with any configuration x(0),\nthe MH algorithm proceeds by iterating “propose” and “accept/reject”,\nas shown in Line 3 and Line 4 respectively. A single run of “propose”\nand “accept/reject” is often called a MH transition, which defines a\nparticular Markov chain.\n• First, we propose to move from the previous state x(t−1) to a\nnew state x′ with probability q(x′|x(t−1)), where q is called the\nproposal distribution.\n• Having proposed a move to x′, we then decide whether to accept\nthis proposal or not according to some formula, which ensures that\nthe limiting distribution of this chain is the target distribution pθ.\nIf the proposal is accepted, the new state is x′, otherwise the new\nstate is the same as the previous state, x(t−1) (i.e., we repeat the\nsample).\nAt the end of the iterations, we obtain a realization (or say, a single\nrun) of the Markov chain, x(1), · · · , x(T). It can be shown that this\nparticular Markov chain leave pθ invariant. Note that theoretically, only\nthe limiting distribution of the chain follows pθ. So it is necessary to\ndiscard a few initial samples until the Markov chain has burned in,\n36\nBasics for EBMs\nAlgorithm 1 Metropolis-Hastings Algorithm\nInput: A target distribution pθ(x), a proposal distribution q(x′|x)\n1: Randomly initialize x(0);\n2: for t = 1 to T do\n3:\nGenerate x′ from the proposal q(x′|x(t−1));\n4:\nAccept x(t) = x′ with probability min\nn\n1,\npθ(x′)q(x(t−1)|x′)\npθ(x(t−1))q(x′|x(t−1))\no\n,\notherwise set x(t) = x(t−1);\n5: end for\n6: Return: {x(1), · · · , x(T)}\nor entered its stationary distribution. The remained samples can then\nbe used for Monte Carlo averaging such as in Eq. (2.20) to estimate\nexpectations.\nIn practice, the accept/reject step is taken by drawing U ∼Uni[0, 1],\ncalculate the acceptance probability\nr = min\n(\n1,\npθ(x′)q(x(t−1)|x′)\npθ(x(t−1))q(x′|x(t−1))\n)\n(2.22)\nand update\nx(t) =\n(\nx′,\nif U ≤r\nx(t−1),\notherwise.\nCritically, the MH algorithm only needs to know the target distri-\nbution up to a normalization constant. In particular, consider the EBM\ndistribution pθ(x) =\n1\nZθ exp [Uθ(x)], then MH ratio\npθ(x′)q(x(t−1)|x′)\npθ(x(t−1))q(x′|x(t−1)) =\n1\nZθ exp [Uθ(x′)] q(x(t−1)|x′)\n1\nZθ exp\n\u0002Uθ(x(t−1))\n\u0003 q(x′|x(t−1))\nin which the Zθ’s cancel. Hence, we can sample from pθ(x) even if the\nnormalizing constant Zθ is unknown.\nRemarkably, the user is free to use any kind of proposal they want,\nsubject to some theoretical conditions. This makes MH quite a flexible\nmethod. We introduce two special algorithms that are instances of the\ngeneral MH algorithm.\n2.3. LEARNING EBMS BY MAXIMUM LIKELIHOOD\n37\nThe Metropolis algorithm.\nIf the proposal transition function is\nsymmetric, so q(x′|x) = q(x|x′), the acceptance probability is given by\nthe following formula:\nr = min\n\u001a\n1,\npθ(x′)\npθ(x(t−1))\n\u001b\nWe see that if x′ is more probable than x, we definitely move there\n(since\npθ(x′)\npθ(x(t−1)) > 1), but if x′ is less probable, we may still move there\nanyway, depending on the relative probabilities. So instead of greedily\nmoving to only more probable states, we occasionally allow “downhill”\nmoves to less probable states.\nThe Metropolis independence sampler (MIS).\nAnother special choice\nof the transition function is in the form of q(x′) = q(x′|x); that is, the\nproposed move x′ is generated independent of the previous state x(t−1).\nIn MIS, the acceptance probability becomes:\nr = min\n\u001a\n1,\nw(x′)\nw(x(t−1))\n\u001b\nwhere w(x) = pθ(x)\nq(x) is the usual importance weight.\nGibbs sampling\nThe Gibbs sampler is conceptually the simplest of the Markov chain\nsampling method, and as we introduce below, could be viewed as the\nMCMC analog of coordinate descent.\nSuppose we wish to sample from the joint distribution for x =\n(x1, · · · , xn) given by p(x1, · · · , xn), where the range of the xi may be\neither continuous or discrete. The Gibbs sampler does this by repeatedly\nreplacing each component, say xi, with a value picked from the full\nconditional for variable xi, i.e., the distribution of xi conditional on the\ncurrent values of all other components (x1, · · · , xi−1, xi+1, · · · , xn) ≜x\\i.\nThis process can be seen as generating a realization of a Markov chain\nthat is built from a set of base transition probabilities Bi, for i = 1, · · · , n.\nBi leaves all the components except xi unchanged, and draws a new xi\nfrom its full conditional, which is assumed to be a feasible operation.\n38\nBasics for EBMs\nAlgorithm 2 Gibbs sampler\nInput: A target distribution p(x) for x = (x1, · · · , xn)\n1: Randomly initialize x(0) = (x(0)\n1 , · · · , x(0)\nn );\n2: for t = 1 to T do\n3:\nPick\nx(t)\n1\nfrom\nthe\ndistribution\nfor\nx1\ngiven\nx(t−1)\n2\n, x(t−1)\n3\n, · · · , x(t−1)\nn\n;\n4:\nPick x(t)\n2\nfrom the distribution for x2 given x(t)\n1 , x(t−1)\n3\n, · · · , x(t−1)\nn\n;\n5:\n...\n6:\nPick\nx(t)\ni\nfrom\nthe\ndistribution\nfor\nxi\ngiven\nx(t)\n1 , · · · , x(t)\ni−1, x(t−1)\ni+1 , · · · , x(t−1)\nn\n;\n7:\n...\n8:\nPick x(t)\nn from the distribution for xn given x(t)\n1 , x(t)\n2 , · · · , x(t)\nn−1;\n9: end for\n10: Return: {x(1), · · · , x(T)}\nThe Gibbs sampling algorithm can be described as simulating a\nhomogeneous Markov chain, x(0), x(1), x(2), · · · , with transition matrix\nP = B1 × B2 × · · · × Bn, as shown in Algorithm 2. Generating x(t) from\nx(t−1), i.e., from Line 3 to Line 8, is called a sweep. Note that the new\nvalue for xi−1 is used immediately when picking the new value for xi.\nStarting from the Gibbs sampler, we provide three useful points.\n• Constructing a Markov chain from base transitions. In\nsampling application, our goal is to find an ergodic Markov chain\nthat converges to the target invariant distribution p(x), at as fast\na rate as possible. The Gibbs sampling embodies a useful, general\nmethod to construct such a Markov chain, as described below.\nConsider to construct the transition probabilities for such a chain\nfrom a set of base transition probabilities, given by B1, · · · , Bs11,\neach of which leaves the target distribution invariant. It can be\nshown that when the base transitions are applied in sequence, if\n11Generally, the number of base transitions s is not necessarily equal to n, the\ndimensionaliy of x.\n2.3. LEARNING EBMS BY MAXIMUM LIKELIHOOD\n39\na distribution is invariant with respect to all the base transitions,\nthen it is also invariant with respect to P = B1 × B2 × · · · × Bs\n(Neal, 1993). We show in the next point that each base transition\nin Gibbs sampler leaves the target distribution invariant, so that\nwe can understand why the Gibbs sampler works.\n• Gibbs sampler is a special case of MH, and thus leaves\nthe target distribution invariant. Each base transition in Gibbs\nsampler is equivalent to using MH with a proposal of the form\nq(x′|x) = p(x′\ni|x\\i)1(x′\n\\i = x\\i)\nThat is, we move to a new state where xi is sampled from its\nfull conditional, but x\\i is left unchanged. It turns out that the\nacceptance rate of such proposal is 1, because the MH ratio\np(x′)q(x|x′)\np(x)q(x′|x) =\np(x′\ni|x′\n\\i)p(x′\n\\i)p(xi|x′\n\\i)\np(xi|x\\i)p(x\\i)p(x′\ni|x\\i)\n= p(x′\ni|x\\i)p(x\\i)p(xi|x\\i)\np(xi|x\\i)p(x\\i)p(x′\ni|x\\i) = 1\nwhere we exploited that fact that x′\n\\i = x\\i, and that q(x′|x) =\np(x′\ni|x\\i). So every time the Gibbs sampler draws a new value\nfrom the full conditional of a component and always accepts it.\n• MH within Gibbs sampling. Gibbs sampling assumes that\ndrawing from the full conditional of each component is tractable.\nWhen sampling from the full conditionals of a certain component\nis intractable, we can replace the exact sampling of this compo-\nnent by a MH sampling step, i.e., a single run of “propose” and\n“accept/reject”. The resulting algorithm is thus called MH within\nGibbs sampling.\nGradient guided MCMC\nFor continuous distribution, MCMC samplers leveraging continuous dy-\nnamics (namely continuous-time Markov processes described by stochas-\ntic differential equations), such as Langevin dynamics (LD) and Hamil-\ntonian Monte Carlo (HMC) (Neal, 2011), are known to be efficient in\n40\nBasics for EBMs\nexploring the continuous state space. Simulating the continuous dy-\nnamics leads to the target distribution as the stationary distribution.\nIn practice, a discretization of the continuous-time system is needed\nnecessitating a Metropolis-Hastings (MH) correction, though still with\nhigh acceptance probability. Recently, stochastic gradient variants of\ncontinuous-dynamic samplers have emerged, showing that adding the\n“right amount” of noise to stochastic gradient ascent iterates leads to\nsamples from the target posterior as the step size is annealed (Welling\nand Teh, 2011; Chen et al., 2014). In either manners, the Markov transi-\ntion kernel defined by the continuous dynamical system usually involves\nusing the gradients of the target distribution w.r.t the data vector x.\nRemarkably, the gradient of log-density of an EBM model w.r.t. the\ndata vector x is easy to calculate:\n∇x log pθ(x) = ∇xUθ(x) −∇x log Z(θ)\n|\n{z\n}\n=0\n= ∇xUθ(x)\nwhich does not require the calculation of the normalizing constant.\nIn the following, we mainly introduce LD and SGLD. For HMC and\nstochastic gradient Hamiltonian Monte Carlo (SGHMC), readers can\nrefer to (Neal, 2011; Chen et al., 2014; Ma et al., 2015).\nLangevin dynamics (LD) sampler.\nGiven current sample x0, a new\nobservation is proposed as\nx 1\n2 = x0 + σ2\n2 ∇xUθ(x0) + σε,\n(2.23)\nwhere ε ∼N(0, I) is a Gaussian noise, and σ is a step size. The next\nsample x1 may directly be the proposal x 1\n2 , in which case the Markov\ntransition from x0 to x1 does not strictly leave pθ invariant, but the\nsampling bias may usually be small for σ ≈0. To allow large σ, a\ncorrection can be achieved by accepting or rejecting the proposal x 1\n2 ,\ni.e., setting x1 = x 1\n2 or x0, with the Metropolis-Hastings probability.\nLangevin sampling with rejection is known as the Metropolis-Adjusted\nLangevin Algorithm (MALA) (Besag, 1994; Roberts and Tweedie, 1996).\nStochastic gradient Langevin dynamics (SGLD).\nRecently, stochas-\ntic gradient samplers have emerged in simulating posterior samples\n2.3. LEARNING EBMS BY MAXIMUM LIKELIHOOD\n41\nin large-scale Bayesian inference, such as SGLD (stochastic gradient\nLangevin dynamics) (Welling and Teh, 2011) and SGHMC (Stochas-\ntic Gradient Hamiltonian Monte Carlo) (Chen et al., 2014). To il-\nlustrate, consider the posterior p(θ|D) of model parameters θ given\nthe observed dataset D, with abuse of notation. We have p(θ|D) ∝\nexp [P\nx∈D log pθ(x) + log p(θ)], which is taken as the target distribution.\nInstead of using full-data gradients\n∂\n∂θ log p(θ|D), which needs a sweep\nover the entire dataset, these stochastic gradient samplers subsample the\ndataset and use stochastic gradients\n∂\n∂θ\n\u0014\n| ˜\nD|\n|D|\nP\nx∈˜D log pθ(x) + log p(θ)\n\u0015\nin the dynamic simulation, where ˜D ⊂D is a subsampled data subset.\nIn this manner, the computation cost is significantly reduced in each\niteration and such Bayesian inference methods scale to large datasets.\nIn practice, sampling is based on a discretization of the continuous\ndynamics. Despite the discretization error and the noise introduced by\nthe stochastic gradients, it can be shown that simulating the discretized\ndynamics with stochastic gradients also leads to the target distribution\nas the stationary distribution, when the step sizes are annealed to zero\nat a certain rate12. The convergence of SGLD and SGHMC can be\nobtained from (Sato and Nakagawa, 2014; Chen et al., 2014; Ma et al.,\n2015). We summarize in Theorem 2.1 for SGLD.\nTheorem 2.1. Denote the target density as p(z; λ) with given λ. Assume\nthat one can compute a noisy, unbiased estimate ∆(z; λ) (a stochastic\ngradient) to the gradient\n∂\n∂z log p(z; λ). For a sequence of asymptotically\nvanishing time-steps {δl, l ≥1} (satisfying P∞\nl=1 δl = ∞and P∞\nl=1 δ2\nl <\n∞), the SGLD algorithm iterates as follows, starting from z(0):\nz(l) =z(l−1) + δl∆(z(l−1); λ) +\np\n2δlη(l),\nη(l) ∼N(0, I), l = 1, · · ·\n(2.24)\nThe iterations of Eq. (2.24) lead to the target distribution p(z; λ) as\nthe stationary distribution.\n12A Metropolis-Hastings (MH) correction can be applied, when it is hard to check\nthe annealing condition is satisfied or not.\n42\nBasics for EBMs\n2.3.2\nImportance sampling\nOne of the principal reasons for wishing to sample from complicated\ndistributions is to be able to estimate expectations of the form Eq.\n(2.20). The technique of importance sampling (IS) provides a framework\nfor approximating expectations directly.\nSuppose, generally, one is interested in estimating\nEpθ(x) [g(x)] =\nZ\npθ(x)g(x)dx\n(2.25)\nImportance sampling is based on the use of a proposal distribution q(x)\nfrom which it is easy to draw samples, say, x(1), · · · , x(M) ∼q(x). We\ncan then express the expectation by Monte Carlo averaging, i.e., in the\nform of a finite sum over samples {x(j)} drawn from q(x):\nEpθ(x) [g(x)] =\nZ\nq(x)pθ(x)\nq(x) g(x)dx\n≈1\nM\nM\nX\nj=1\npθ(x(j))\nq(x(j)) g(x(j))\n(2.26)\nwhich is an unbiased estimate of the expectation in Eq. (2.25). The\nquantities w(j) = pθ(x(j))\nq(x(j)) are known as importance weights, and they\ncorrect the bias that {x(j)} are drawn from the proposal distribution\nrather than from the target distribution.\nIt will often be the case that the distribution pθ(x) can only be\nevaluated up to a normalization constant (e.g., in EBMs), so that\npθ(x) = ˜pθ(x)/Zp where ˜pθ(x) can be evaluated easily, whereas Zp\ndenotes the unknown normalizing constant. Generally, we may wish to\nuse a proposal q(x) = ˜q(x)/Zq, which is also in the un-normalized form.\nWe then have\nEpθ(x) [g(x)] = Zq\nZp\nZ\nq(x) ˜pθ(x)\n˜q(x) g(x)dx\n≈Zq\nZp\n1\nM\nM\nX\nj=1\n˜pθ(x(j))\n˜q(x(j)) g(x(j))\n(2.27)\n2.3. LEARNING EBMS BY MAXIMUM LIKELIHOOD\n43\nWe can use the same samples to evaluate the ratio Zq\nZp with the result:\nZp\nZq\n= 1\nZq\nZ\n˜pθ(x)dx =\nZ\nq(x) ˜pθ(x)\n˜q(x) dx\n≈1\nM\nM\nX\nj=1\n˜pθ(x(j))\n˜q(x(j))\n(2.28)\nwhich is an unbiased estimate of Zp\nZq . When ˜q(x) is self-normalized (i.e.,\nZq = 1), Eq. (2.28) shows a way of using importance sampling to\nestimate the normalizing constant Zp.\nCombining Eq. (2.27) and Eq. (2.28) and letting ˜w(j) = ˜pθ(x(j))\n˜q(x(j)) , we\ncan approximate the expectation by\nEpθ(x) [g(x)] ≈˜w(1)g(x(1)) + · · · + ˜w(M)g(x(M))\n˜w(1) + · · · + ˜w(M)\n(2.29)\nwhich turns to be a biased estimate of the expectation in Eq. (2.25).\nFurther, by defining normalized importance weights ω(j) =\n˜w(j)\nPM\nj=1 ˜w(j) ,\nEq. (2.29) can be re-written in a simpler form:\nEpθ(x) [g(x)] ≈\nM\nX\nj=1\nω(j)g(x(j))\n(2.30)\nwhich is often referred to as self-normalized importance sampling (SNIS),\nfor example, in (Parshakova et al., 2019). A major advantage of using\nthe biased estimate Eq. (2.30) instead of the unbiased estimate Eq.\n(2.27) is that in using the former (although biased), we need only to\nknow the ratio pθ(x)\nq(x) up to a multiplicative constant; whereas in the\nlatter, the ratio needs to be known exactly.\nRemarkably, the success of the importance sampling approach de-\npends crucially on how well the proposal distribution q(x) matches the\ntarget distribution pθ(x).\n2.3.3\nStochastic approximation methods\nIn the above, we introduce the basics of some classic Monte Carlo\nmethods and the general idea of applying them in maximum likelihood\n44\nBasics for EBMs\nAlgorithm 3 A naive algorithm of learning EBMs by Monte Carlo\nmethods\nInput: A target EBM distribution pθ(x)\nfor each minibatch of size B do\nObtain empirical expectations by Eq. (2.21);\nfor j = 1 to M do\nDraw x(j) with pθ(x) as the target distribution;\nend for\nObtain model expectations by Eq. (2.20);\nUpdate parameter θ by gradient Eq. (2.19);\nend for\nlearning of EBMs. We show in Eq. (2.19) that the log-likelihood gra-\ndient for learning EBMs is equal to the difference between empirical\nexpectation and model expectation, and in Eq. (2.20), that the model\nexpectation is approximated by Monte Carlo sampling from EBM distri-\nbution pθ(x). Combining Eq. (2.19), Eq. (2.20) and Eq. (2.21), we could\nobtain a naive algorithm of learning EBMs by Monte Carlo methods,\nas shown in Algorithm 3.\nTypically we use MCMC to generate the samples x(1), · · · , x(M),\nfor each minibatch. For EBM distribution pθ(x), which can only be\nevaluated up to a normalization constant, using the unbiased IS estimate\nEq. (2.27) is intractable. Using the biased IS estimate Eq. (2.30) will\nproduce biased gradient estimates, which were used in some prior studies\n(Parshakova et al., 2019).\nIn learning EBMs by Monte Carlo methods, at first thought (as\nshown in Algorithm 3), there are two loops. The outer loop iterates\nover minibatches of training data. The inner loop iterates to generate\nsamples via MCMC (e.g., MH), but running MCMC sufficiently long\n(with large M) to approaching convergence at the inner loop would\nbe extremely slow. Fortunately, it was shown by (Younes, 1989) that\nwe can start the MCMC chain at its previous value from the outer\nloop, and just take a few Markov moves in the inner loop (i.e., using\nsmall M). So in this way, the Markov chain evolves persistently across\nouter loops. This algorithm, called stochastic maximum likelihood (SML)\n2.3. LEARNING EBMS BY MAXIMUM LIKELIHOOD\n45\nAlgorithm 4 The general stochastic approximation (SA) algorithm\nfor t = 1, 2, · · · do\nMonte Carlo sampling: Draw a sample z(t) with a Markov transi-\ntion kernel Kλ(t−1)(z(t−1), ·), which starts with z(t−1) and admits\npλ(t−1)(·) as the invariant distribution.\nSA updating: Set λ(t) = λ(t−1) + γtFλ(t−1)(z(t)), where γt is the\nlearning rate.\nend for\n(Younes, 1989), along with its variants appeared in the literature, turn\nout to be application of the more general stochastic approximation (SA)\nmethodolgy to learning EBMs. See further introduction in Section 2.3.3.\nNote.\nThe above cognition of the EBM learning by Monte Carlo\nis very important. Many people may think that learning with Monte\nCarlo methods is very slow. But since we do not need to run MCMC\nto convergence at the inner loop, but just a few steps. Learning with\nMCMC is not so expensive as people might think.\nIntroduction to stochastic approximation (SA) methodology\nStochastic approximation methods are an important family of iterative\nstochastic optimization algorithms, introduced in (Robbins and Monro,\n1951) and extensively studied (Benveniste et al., 1990; Chen, 2002).\nBasically, stochastic approximation provides a mathematical framework\nfor stochastically solving a root finding problem, which has the form of\nexpectations being equal to zeros. Suppose that the objective is to find\nthe solution λ∗of f(λ) = 0 with\nf(λ) = Ez∼pλ(·)[Fλ(z)],\n(2.31)\nwhere λ is a d-dimensional parameter vector in Λ ⊂Rd, and z is an obser-\nvation from a probability distribution pλ(·) depending on λ. Fλ(z) ∈Rd\nis a function of z, providing d-dimensional stochastic measurements of\nthe so-called mean-field function f(λ). Intuitively, we solve a system of\nsimultaneous equations, f(λ) = 0, which consists of d constraints, for\ndetermining d-dimensional λ.\n46\nBasics for EBMs\nGiven some initialization λ(0) and z(0), a general SA algorithm\niterates Monte Carlo sampling and parameter updating, as shown in\nAlgorithm 4. The convergence of SA has been established under condi-\ntions (Benveniste et al., 1990; Andrieu et al., 2005; Song et al., 2014),\nincluding a few technical requirements for the mean-field function f(λ),\nthe Markov transition kernel Kλ(t−1)(z(t−1), ·) and the learning rates.\nParticularly, when f(λ) corresponds to the gradient of some objective\nfunction, then λ(t) will converge to local optimum, driven by stochastic\ngradients Fλ(z). For completeness, we provide a short summary on the\nconvergence of {λt, t ≥1} in Algorithm 4, based on Theorem 1 in (Song\net al., 2014).\nTheorem 2.2. Let {γt} be a monotone non-increasing sequence of\npositive numbers such that13 P∞\nt=1 γt = ∞and P∞\nt=1 γ2\nt < ∞. Assume\nthat Λ is compact and the Lyapunov condition on f(λ) and the drift\ncondition on the transition kernel Kλ(·|·) hold. Then we have: d(λt, L) →\n0 almost surely as t →∞, where L = {λ : f(λ) = 0} and d(λ, L) =\ninfλ′∈L ||λ −λ′||.\nRemarkably, Algorithm 4 shows stochastic approximation with\nMarkovian perturbations (Benveniste et al., 1990). It is more gen-\neral than the non-Markovian SA which requires exact sampling z(t) ∼\npλ(t−1)(·) at each iteration and in some tasks can hardly be realized. In\nnon-Markovian SA, we check that Fλ(z) is unbiased estimates of f(λ),\nwhile in SA with Markovian perturbations, we check the ergodicity\nproperty of the Markov transition kernel.\nTo speed up convergence, during each SA iteration, it is possible\nto generate a set of multiple observations z by performing the Markov\ntransition repeatedly and then use the average of the corresponding\nvalues of Fλ(z) for updating λ, which is known as SA with multiple\nmoves (Wang et al., 2018), as shown in Algorithm 5.\nNote I.\nPerhaps the most familiar application of SA in machine learn-\ning literature is the stochastic gradient descent (SGD) technique, partic-\nularly the minibatching technique. When the objective (and therefore its\n13In practice, we can set a large learning rate at the early stage of learning and\ndecrease to 1/t for convergence.\n2.3. LEARNING EBMS BY MAXIMUM LIKELIHOOD\n47\nAlgorithm 5 SA with multiple moves\nfor t = 1, 2, · · · do\n1.\nMonte Carlo sampling: Set z(t,0) = z(t−1,K). For k from\n1\nto\nK,\ngenerate\nz(t,k)\n∼\nKλ(t−1)(z(t,k−1), ·),\nwhere\nKλ(t−1)(z(t,k−1), ·) is a Markov transition kernel that admits\npλ(t−1)(·) as the invariant distribution.\n2.\nSA updating: Set λ(t) = λ(t−1) + γt{ 1\nK\nP\nz∈B(t) Fλ(t−1)(z)},\nwhere B(t) = {z(t,k)|k = 1, · · · , K}.\nend for\ngradient) is a sum of many terms that can be computed independently,\nSGD samples one term at a time and follows one noisy estimate of\nthe gradient with a decreasing step size. Furthermore, it can be easily\nseen that SGD training with minibatches is an application of SA with\nmultiple moves.\nNote II.\nGenerally, SA represents an iterative methodology to find the\nroot of an expectation. Each iteration consists of a sampling step and a\nparameter updating step. Basically, we use MCMC to simulate the noisy\nmeasurements to approximate the expectation. A keypoint is that we do\nnot need to wait the chain to converge, but use a decaying learning rate\nto guarantee the convergence. Intuitively, as the learning rate becomes\nsufficiently small compared to the mixing rate of the Markov chain, the\nchain will stay close to the stationary distribution, even if it only runs\nfor one Markov move per parameter update.\nApplication of SA to learning EBMs\nIt can be easily seen that the EBM gradients ∇θL(θ) in Eq. (2.19)\nexactly follows the form of Eq. (2.31), as summarized in Theorem 2.3.\nSo the problem of maximum likelihood estimate of EBM parameters\ncan then be solved by setting the gradients to zeros and applying the SA\nalgorithm to finding the root for the resulting system of simultaneous\nequations.\n48\nBasics for EBMs\nAlgorithm 6 Stochastic maximum likelihood for fitting an EBM\nfor t = 1, 2, · · · do\nSampling: Draw xκ from training data, and simulate a sample x(t)\nwith a Markov transition kernel Kθ(t−1)(x(t−1), ·), which starts with\nx(t−1) and admits pθ(t−1)(·) as the invariant distribution.\nUpdating: Update θ by gradient ascent as:\nθ(t) = θ(t−1) + γt{∇θUθ(xκ) −∇θUθ(x(t))}|θ=θ(t−1)\n(2.32)\nend for\nTheorem 2.3. Consider an EBM distribution pθ(x) parameterized with\nθ as shown in Eq. (2.13), and a training dataset consisting of IID data\npoints {x1, · · · , xN}. Introduce an index variable κ which is uniformly\ndistributed over {1, · · · , N}. The log-likelihood gradients w.r.t. θ as\nshown in Eq. (2.19) can be recast in the expectation form of Eq. (2.31)\n(i.e. as expectation of stochastic gradients), by letting λ ≜θ, z ≜(κ, x)T ,\npλ(z) ≜1\nN pθ(x), f(λ) ≜∇θL(θ), and\nFλ(z) ≜∇θUθ(xκ) −∇θUθ(x)\nProof. This can be readily seen by rewriting Eq. (2.19) as:\n∇θL(θ) = Eκ∼Uni[1,N],x∼pθ(x) [∇θUθ(xκ) −∇θUθ(x)]\nand applying the independence between κ and x.\n■\nCombining Theorem 2.3 and general SA (Algorithm 4), the particu-\nlar resulting pseudocode for learning EBMs is shown in Algorithm 6,\nwhich is often known as stochastic maximum likelihood (SML) (Younes,\n1989). Algorithm 6 corresponds to SA with single move. Further, by\napplying SA with multiple moves (Algorithm 5), at each iteration, we\ncan draw a minibatch from training data (minibatching), say draw-\ning κ1, · · · , κB from {1, · · · , N}. At each iteration, we could directly\ndraw x(t,1), · · · , x(t,M) ∼pθ(t−1)(x) when it is tractable, or run multiple\nsteps of a single chain, or multiple parallel chains, or a combination\nof both, to draw multiple samples, say obtaining x(t,1), · · · , x(t,M) that\nadmit pθ(t−1)(x) as the invariant distribution. Then, at each iteration,\n2.3. LEARNING EBMS BY MAXIMUM LIKELIHOOD\n49\nparameter updating in Eq. (2.32) can be replaced by:\nθ(t) = θ(t−1) + γt\n\n\n\n1\nB\nB\nX\nj=1\n∇θUθ(xκj) −1\nM\nM\nX\nj=1\n∇θUθ(x(t,j))\n\n\n\n\f\f\f\f\f\f\nθ=θ(t−1)\nHistorical comments.\nThe general stochastic approximation method-\nology was originally proposed in (Robbins and Monro, 1951). The\nstochastic maximum likelihood method, proposed in (Younes, 1989),\nturns out to be application of the more general SA methodology to\nlearning EBMs. The same idea was applied to training RBMs in (Tiele-\nman, 2008), which is called persistent contrastive divergence (PCD)\nto emphasize that the Markov chain is not reset between parameter\nupdates. The regular contrastive divergence (CD) method, proposed in\n(Hinton, 2002), restarts the Markov chain at the training data rather\nthan at the previous state. This will not converge to MLE. As com-\nmented in (Salakhutdinov, 2009), “Clearly, the widely used practice\nof CD1 learning is a rather poor “substitute” for maximum likelihood\nlearning.”\n2.3.4\nVariational methods with auxiliary models\nAs introduced before, Monte Carlo sampler is a crucial component\nwhich affects maximum likelihood learning of EBMs. A recent progress\nas studied in (Kim and Bengio, 2016; Wang and Ou, 2017; Kuleshov\nand Ermon, 2017; Xie et al., 2018) is to pair the target EBM pθ(x) with\nan auxiliary directed generative model (often called generator) qϕ(x)\nparameterized by ϕ, which approximates sampling from the target EBM.\nLearning is performed by maximizing the log-likelihood of training data\nunder pθ or some bound of the log-likelihood, and simultaneously mini-\nmizing some divergence between the target EBM pθ and the auxiliary\n50\nBasics for EBMs\ngenerator qϕ14:\n(\nMaximize over θ the log-likelihood itself or some bound\nMinimize over ϕ some divergence between pθ and qϕ.\nDifferent learning methods mainly differ in the objective functions used\nin the joint training of pθ and qϕ, and thus have different computational\nand statistical properties. There are also other factors that distinguish\ndifferent studies in learning EBMs with auxiliary models, e.g. modeling\ndiscrete or continuous data, different model choices of the target EBM\nand the auxiliary generator.\nMany methods in learning EBMs with auxiliary models are related to\nvariational methods. Variational methods provide an optimization-\nbased principle to inference and learning (Jordan et al., 1999; Frey\nand Jojic, 2005). A classic application of variational methods is in\nBayesian inference, called variational inference (VI). VI posits a family\nof approximating distributions q and then finds the member of that\nfamily that is closest to the target posterior distribution p, mostly by\nminimizing the exclusive-divergence KL(q||p). Variational methods have\nalso been widely used in the context of maximum likelihood parameter\nestimation, which is often called variational learning. In particular, (Neal\nand Hinton, 1998) shows a link between variational bound and maximum\nlikelihood parameter estimation via the Expectation-Maximization (EM)\nalgorithm. Variational learning in early days is called variational EM\n(Frey and Jojic, 2005).\nRemarkably, classic variational methods mostly optimize the exclu-\nsive divergence, and hence could be classified as the exclusive-variational\napproach. Recently, there have emerged some variational methods that\noptimize the inclusive divergence KL(p||q), which has good statistical\nproperties that makes it more appropriate for certain inference and\nlearning problems. These studies include joint stochastic approxima-\ntion (JSA) (Xu and Ou, 2016; Ou and Song, 2020), Markovian score\nclimbing (MSC) (Naesseth et al., 2020), parallel Markov chain score\n14Such optimization using two objectives has also been employed in training other\ntypes of models apart from learning EBMs, such as learning GAN with logD trick\n(Goodfellow et al., 2014), the wake-sleep algorithm (Hinton et al., 1995) for learning\nHelmholtz Machines.\n2.3. LEARNING EBMS BY MAXIMUM LIKELIHOOD\n51\nascent (pMCSA) (Kim et al., 2022), and transport score climbing (TSC)\n(Zhang et al., 2022b) for learning latent-variable models (belonging to\ndirected graphical models), AugSA plus JSA (Wang and Ou, 2017) and\ninclusive-NRF (Song and Ou, 2018) for learning EBMs (belonging to\nundirected graphical models). We could refer these studies collectively as\nthe inclusive-variational approach. See (Ou, 2018) for more introduction\non variational methods and the two approaches.\nIn practice, the performance of learning EBMs with auxiliary models\noften performs better than without auxiliary models. In the following,\nwe first follow (Song and Ou, 2018) to give a short literature review of\nthis class of studies, and then mainly detail the inclusive-variational\napproach for learning EBMs.\nRelated work in MLE of EBMs with auxiliary models\nLet pθ(x) denote the target EBM as defined in Eq. (2.13), qϕ(x) the\nauxiliary generator which allows efficient sampling and approximates\nsampling from the target EBM, and pemp(x) the empirical distribution\nfor training data.\n• It is shown in (Song and Ou, 2018) that we have the following\nevidence upper bound (EUBO) w.r.t. θ for EBMs:\nEUBO(x; θ, ϕ) = log pθ(x) + KL(qϕ(x)||pθ(x))\n= Uθ(x) −log Z(θ) −(Eqϕ(x)[pθ(x)] + H[qϕ(x)])\n= Uθ(x) −(Eqϕ(x)[Uθ(x)] + H[qϕ(x)])\n≥log pθ(x)\nIt is further shown in (Song and Ou, 2018) that learning in (Kim\nand Bengio, 2016) amounts to maximizing the EUBO bound w.r.t.\nθ, while simultaneously minimizing the gap, i.e., the exclusive-\ndivergence KL[qϕ||pθ] w.r.t. ϕ:\n\n\n\n\n\nmax\nθ\nEx∼pemp(x)EUBO(x; θ, ϕ)\nmin\nϕ KL [qϕ(x)||pθ(x)]\n(2.33)\n52\nBasics for EBMs\nRemarkably, the EUBO bound involves the intractable entropy\nterm H [qϕ] and tends to enforce the generator to seek modes,\nyielding missing modes. In Eq. (2.33), we optimize the exclusive-\ndivergence w.r.t. an auxiliary distribution to approximate a target\ndistribution pθ(x). Hence we classify Eq. (2.33) as the exclusive-\nvariational approach, which is called exclusive-NRF in (Song and\nOu, 2018).\n• Learning in (Wang and Ou, 2017; Song and Ou, 2018) minimizes\nthe inclusive-divergence KL[pθ||qϕ] w.r.t. ϕ, which could be clas-\nsified as the inclusive-variational approach for learning EBMs.\nThe main idea is to perform maximum likelihood learning of pθ\nand simultaneously minimize the inclusive-divergence between the\ntarget EBM pθ and the auxiliary generator qϕ by\n\n\n\n\n\nmin\nθ\nKL [pemp(x)||pθ(x)]\nmin\nϕ KL [pθ(x)||qϕ(x)]\n(2.34)\nThe first line of Eq. (2.34) is equivalent to maximum likelihood\nfitting of the target EBM pθ under the empirical distribution pemp,\nwhich requires sampling from pθ. Simultaneously, the second line\noptimizes the generator qϕ to be close to pθ so that qϕ becomes a\ngood proposal for sampling from pθ.\nCompared to the exclusive-variational approach, the inclusive-\nvariational approach shown in Eq. (2.34) has several advantages.\nFirst, minimizing inclusive-divergence avoids the annoying entropy\nterm, which is suffered by minimizing the exclusive-divergence. Sec-\nond, inclusive-divergence minimization tends to drive the auxiliary\ngenerator, acting like an adaptive proposal in adaptive MCMC\n(Andrieu and Thoms, 2008; Roberts and Rosenthal, 2009), to\ncover modes of the target density pθ. Mode-covering is a desirable\nproperty for proposal design in MCMC. In contrast, minimizing\nexclusive-divergence leads to variational approximations that seek\nmodes and underestimate uncertainty. The auxiliary model qϕ(x)\nand the sampler for pθ(x) can be very flexibly designed, depending\non the nature of data x, discrete or continuous.\n2.3. LEARNING EBMS BY MAXIMUM LIKELIHOOD\n53\n– (Wang and Ou, 2017) mainly studies neural random field lan-\nguage models, using LSTM generators (autoregressive with\nno latent variables) and employing Metropolis independence\nsampler (MIS) - applicable for discrete data (natural sen-\ntences). The learning algorithm proposed in (Wang and Ou,\n2017), called AugSA plus JSA, is an instance of the inclusive-\nvariational approach for learning EBMs over discrete data.\n– (Song and Ou, 2018) mainly designs neural random field\nmodels (NRFs) for continuous data (e.g., images), choosing\nlatent-variable generators and developing SGLD (stochastic\ngradient Langevin dynamics)/SGHMC (stochastic gradient\nHamiltonian Monte Carlo) samplers to exploit noisy gradients\nin the continuous space. The learning algorithm proposed in\n(Song and Ou, 2018), called inclusive-NRF, is an instance\nof the inclusive-variational approach for learning EBMs over\ncontinuous data.\n• In (Xie et al., 2018) (CoopNet), motivated by interweaving maxi-\nmum likelihood training of the EBM pθ(x) and the latent-variable\ngenerator qϕ(h, x), a joint training method is introduced to train\nEBMs. There are clear differences that distinguish the inclusive-\nvariational approach. First, CoopNet uses LD (Langevin dynamics)\nsampling to generate samples, but two LD sampling steps are\nintuitively interleaved according to\n∂\n∂x log pθ(x) (with Lx steps)\nand\n∂\n∂h log qϕ(h, x) (with Lh steps) separately, not aiming to draw\nsamples from pθ(x)qϕ(h|x). This is different from the stochastic\ngradient sampler in the augmented space in inclusive-NRF, which\nmoves (x, h) jointly. Second, according to theoretical understand-\ning in (Xie et al., 2018), Coopnet considers the following joint\noptimization problem:\n\n\n\n\n\nmin\nθ\n{KL [pemp(x)||pθ(x)] −KL [r(h, x)||pθ(x)]}\nmin\nϕ KL [r(h, x)||qϕ(h, x)]\nwhere r(h, x) denotes the distribution of (x(Lx), h(Lh)), resulting\nfrom the CoopNet sampler. This objective is also clearly differ-\n54\nBasics for EBMs\nFigure 2.10: Overview of the inclusive-variational approach for learning EBMs\nfor continuous data. Two neural networks are used to define the EBM’s potential\nfunction Uθ(x) and the auxiliary generator gϕ(h) respectively. The parameters of\nboth networks, θ and ϕ, are updated by using the revised samples (x, h) in the\naugmented space, which are obtained by revising the samples (x′, h′) proposed by\nthe auxiliary generator, according to the stochastic gradients defined by both the\ntarget EBM and the auxiliary generator. (Song and Ou, 2018)\nent from inclusive-NRF, which aims to minimize the inclusive-\ndivergence KL[pθ||qϕ] w.r.t. ϕ. It is found in (Song and Ou, 2018)\nthat inclusive-NRF with SGLD outperforms CoopNet in image\ngeneration.\n• Learning in (Kuleshov and Ermon, 2017) minimizes the χ2-divergence\nχ2[qϕ||pθ] ≜\nR (pθ−qϕ)2\nqϕ\nw.r.t. ϕ, which also tends to drive the gen-\nerator to cover modes. But this approach is severely limited by\nthe high variance of the gradient estimator w.r.t. ϕ, and is only\ntested on the simpler MNIST and Omniglot.\n• Learning in (Han et al., 2019) further extends CoopNet and\nintroduces an inference model, apart from the target EBM and\nthe latent-variable generator, and jointly optimizes the three\nmodels under a divergence triangle.\nThe inclusive-variational approach for learning EBMs\nThe basic idea of using the inclusive-variational approach for learning\nEBMs is described in Eq. (2.34). The auxiliary model qϕ(x) and the\n2.3. LEARNING EBMS BY MAXIMUM LIKELIHOOD\n55\nsampler for pθ(x) can be very flexibly designed, depending on the\nnature of data x, discrete or continuous. In the following, we mainly\nintroduce the inclusive-variational approach for learning EBMs for\ncontinuous data, which is called inclusive-NRF in (Song and Ou, 2018)\nand illustrated in Figure 2.10. The important features of inclusive-\nNRF is that the auxiliary model qϕ(x) is a latent-variable model, and\nstochastic gradient guided samplers (SGLG/SGHMC) are developed,\nwhich is particularly useful for sampling from a continuous distribution\npθ(x). For the inclusive-variational approach for learning EBMs for\ndiscrete data, readers could see (Wang and Ou, 2017).\nThe NRF model.\nEBMs parameterized by neural network, as defined\nin Eq. (2.13), are called Neural random fields (NRFs) in (Song and Ou,\n2018), which are usually denoted by pθ(x). The potential Uθ(x) : Rdx →\nR is realized by a neural network, which takes the multi-dimensional\nx ∈Rdx as input and outputting the scalar uθ(x) ∈R.\nAn inclusive-divergence minimized auxiliary generator\nqϕ(x) is in-\ntroduced to approximate sampling from the target EBM, particularly\nfor fixed-dimensional continuous observations x ∈Rdx (e.g. images).\nWe use a directed generative model, qϕ(x, h) ≜q(h)qϕ(x|h), for the\nauxiliary generator, which is defined as follows15:\nh ∼N(0, Ih),\nx = gϕ(h) + ϵ, ϵ ∼N(0, σ2Iϵ).\n(2.35)\nHere gϕ(h) : Rdh →Rdx is implemented as a neural network with\nparameter ϕ, which maps the latent code h to the observation space.\nIh and Iϵ denote the identity matrices, with dimensionality implied by\nh and ϵ respectively. Drawing samples from the generator qϕ(x, h) is\nsimple as it is just ancestral sampling (Murphy, 2012) from a 2-variable\ndirected graphical model.\nBy using Fisher equality (Appendix B.2), we have the following\ngradients for θ and ϕ respectively, where in the first equation, we use ˜x\n15Note that during training, σ2 is absorbed into the learning rates and does not\nneed to be estimated.\n56\nBasics for EBMs\nAlgorithm 7 The inclusive-NRF algorithm for learning EBMs for\ncontinuous data with latent-variable auxiliary models\nrepeat\nSampling: Draw a minibatch M =\n\b(˜xi, xi, hi), i = 1, · · · |M|\n\t\nfrom pemp(˜x)pθ(x)qϕ(h|x) (see Algorithm 8);\nUpdating:\nUpdate θ by ascending:\n1\n|M|\nP\n(˜x,x,h)∼M [∇θuθ(˜x) −∇θuθ(x)];\nUpdate ϕ by ascending:\n1\n|M|\nP\n(˜x,x,h)∼M ∇ϕ log qϕ(x, h);\nuntil convergence\nand x to differentiate samples from the empirical distribution pemp(x)\nand those from the model distribution pθ(x).\nProposition 1. The gradients for optimizing the two objectives in Eq.\n(2.34) can be derived as follows:\n\n\n\n\n\n\n\n−∂\n∂θKL [pemp(x)||pθ(x)] = Epemp(˜x) [∇θuθ(˜x)] −Epθ(x) [∇θuθ(x)]\n−∂\n∂ϕKL [pθ(x)||qϕ(x)] = Epθ(x)qϕ(h|x) [∇ϕlogqϕ(x, h)]\n(2.36)\nBy Proposition 1, we can obtain the gradients w.r.t. θ and ϕ (to be\nascended). In practice, we apply minibatch based stochastic gradient\ndescent (SGD) to solve the optimization problem Eq. (2.34), as shown\nin Algorithm 7.\nIdeally, the learning of θ could be conducted without ϕ, by using an\nMCMC sampler (e.g. LD) to draw samples from pθ(x). But the chain\noften mixes between modes so inefficiently that severely slow down the\nlearning of θ especially when the target density pθ(x) is multimodal.\nThis is the main difficulty that hinders the effective training of NRFs.\nIntroducing auxiliary generator qϕ to approximate the target NRF pθ\nis inspired by and related to two advanced MCMC ideas - auxiliary\nvariable MCMC (Neal, 2011) and adaptive MCMC (Andrieu and Thoms,\n2008; Roberts and Rosenthal, 2009).\n• The classic example of adaptive MCMC is adaptive scaling of the\nvariance of the step-size in random-walk Metropolis (Roberts and\n2.3. LEARNING EBMS BY MAXIMUM LIKELIHOOD\n57\nRosenthal, 2009). In inclusive-NRF, the auxiliary generator acts\nlike an adaptive proposal, updated by using samples from the\ntarget density16.\n• Further to be detailed next, the target density is extended to\nbe pθ(x)qϕ(h|x), which leaves the original target as the marginal,\nbut sampling in the augmented space (x, h) can be easier (more\nefficiently), with the help of the adaptive proposal qϕ(x, h). This\nfollows the basic idea of auxiliary variable MCMC (Neal, 2011) -\nsampling in an augmented space could be more efficient.\nDeveloping stochastic gradient samplers for EBMs for continuous\ndata.\nIn Algorithm 7, we need to draw samples (x, h) ∈Rdx+dh in the\naugmented space defined by the target joint distribution pθ(x)qϕ(h|x)\ngiven current θ and ϕ. Gradient guided samplers (Section 2.3.1), such as\nLangevin dynamics (LD) and Hamiltonian Monte Carlo (HMC) (Neal,\n2011), are known to be efficient in exploring the continuous state space.\nThe gradients of the target distribution can be derived as follows:\n\n\n\n\n\n\n\n∂\n∂x log [pθ(x)qϕ(h|x)] = ∂\n∂x [log pθ(x) + log qϕ(h, x) −log qϕ(x)]\n∂\n∂h log [pθ(x)qϕ(h|x)] = ∂\n∂h log qϕ(h, x)\n(2.37)\nIt can be seen that it is straightforward to obtain the gradient w.r.t. h\nand the first two terms in the gradient w.r.t. x. However, calculating the\nthird term\n∂\n∂x log qϕ(x) in the gradient w.r.t. x is intractable. Therefore\nwe are interested in developing stochastic gradient variants of those\nsamplers, which rely on using noisy estimate of\n∂\n∂x log qϕ(x).\nBy considering z ≜(x, h), p(z; λ) ≜pθ(x)qϕ(h|x), λ ≜(θ, ϕ)T , and\nEq. (2.37), we can use Theorem 2.1 to develop the sampling step for\nAlgorithm 7, as presented in Algorithm 8. For the gradient w.r.t. x, the\nintractable term\n∂\n∂x log qϕ(x) is estimated by a stochastic gradient.\n16Minimizing the inclusive-divergence tends to drive the generator (the proposal)\nto have higher entropy than the target density, which is a desirable property for\nproposal design in MCMC.\n58\nBasics for EBMs\nProposition 2. Given qϕ(h, x), we have\n∂\n∂x log qϕ(x) = Eh∗∼qϕ(h∗|x)\n\u0014 ∂\n∂x log qϕ(h∗, x)\n\u0015\n.\n(2.38)\nProof. By using Fisher equality (Appendix B.2).\n■\nMotivated by Proposition 2, ideally we draw h∗∼qϕ(h∗|x) and then\nuse\n∂\n∂x log qϕ(h∗, x) as an unbiased estimator of\n∂\n∂x log qϕ(x). In practice,\nat step l, given x(l−1) and starting from h(l−1), we run one step of LD\nsampling over h targeting qϕ(h|x(l−1)), to obtain h(l−1)∗and calculate\n∂\n∂x(l−1) log qϕ(h(l−1)∗, x(l−1)). This gives a biased but tractable estimator\nto\n∂\n∂x log qϕ(x). It is empirically found in experiments in (Song and Ou,\n2018) that more steps of this inner LD sampling do not significantly\nimprove the performance for NRF learning.\nSo instead of using the exact gradient\n∂\n∂z log p(z; λ) as shown in\nEq. (2.37), Song and Ou, 2018 developed a tractable biased stochastic\ngradient ∆(z; λ) as follows:\n∆(z; λ) ≜\n \n∂\n∂x [log pθ(x) + log qϕ(h, x) −log qϕ(h∗, x)]\n∂\n∂h log qϕ(h, x)\n!\n,\n(2.39)\nwhere h∗is an approximate sample from qϕ(h∗|x) obtained by running\none step of LD from (h, x). Remarkably, as shown in Algorithm 8, the\nstarting point (h(0), x(0)) for the SGLD/SGHMC recursions is obtained\nfrom an ancestral sampling from qϕ(h, x). Thus at step l = 1, h(0) is\nalready a sample from qϕ(h|x(0)) given x(0), and we can directly use h(0)\nas h(0)∗without running the inner LD sampling. Afterwards, for l > 1,\nthe conditional distribution of h(l−1) given x(l−1) is close to qϕ(h|x(l−1)),\nthough strictly not. One or more steps of LD could be run to obtain\nh(l−1)∗to reduce the bias in the stochastic gradient estimator.\nWith the above stochastic gradients in Eq. (2.39), the sampling\nstep in Algorithm 7 can be performed by running |M| parallel chains,\neach chain being executed by running finite steps of SGLD/SGHMC\nwith tractable gradients w.r.t. both x and h, as shown in Algorithm 8.\nIntuitively, the auxiliary generator first gives a proposal (x′, h′), and\nthen the system follows the gradients of pθ(x) and qϕ(h, x) (w.r.t. x\nand h respectively) to revise (x′, h′) to (x, h). The gradient terms pull\n2.3. LEARNING EBMS BY MAXIMUM LIKELIHOOD\n59\nsamples moving to low energy region of the random field and adjust the\nlatent code of the generator, while the noise term brings randomness. In\nthis manner, we obtain Markov chain samples in the augmented space\ndefined by pθ(x)qϕ(h|x).\nAlgorithm 8 Sampling in the augmented space defined by pθ(x)qϕ(h|x)\n1. Conduct ancestral sampling from the auxiliary generator qϕ(x, h),\ni.e. first draw h′ ∼q(h′), and then draw x′ ∼qϕ(x′|h′);\n2. Starting from (x′, h′) = z(0), run finite steps of SGLD (l = 1, · · · , L)\nto obtain (x, h) = z(L), which we call sample revision, according to\nEq. (2.24).\nIn particular, the SGLD recursions are conducted as follows:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nx(l) = x(l−1) + δl\n∂\n∂x(l−1)\nh\nlog pθ(x(l−1)) + log qϕ(h(l−1), x(l−1))\n−log qϕ(h(l−1)∗, x(l−1))\ni\n+\np\n2δlη(l)\nx ,\nh(l) = h(l−1) + δl\n∂\n∂h(l−1) log qϕ(h(l−1), x(l−1)) +\np\n2δlη(l)\nh ,\nη(l) ≜(η(l)\nx , η(l)\nh )T ∼N(0, I)\n(2.40)\nwhere, for l > 1, h(l−1)∗, which is an approximate sample from\nqϕ(h|x(l−1)) given x(l−1), is obtained from running one step of LD as\nfollows, starting from h(l−1):\nh(l−1)∗= h(l−1) + δ∗\nl\n∂\n∂h(l−1) log qϕ(h(l−1), x(l−1)) +\nq\n2δ∗\nl η(l)∗\nh\n,\nη(l)∗\nh\n∼N(0, I);\n(2.41)\nfor l = 1, we directly use h(0) as h(0)∗, since, by initialization, h(0) is\nan exact sample from qϕ(h|x(0)) given x(0).\nReturn (x, h), i.e. z(L).\n60\nBasics for EBMs\n2.3.5\nNon-MLE methods for learning EBMs\nMaximum likelihood estimation (MLE) has been the most widely used\nobjective for learning probabilistic models. When training an EBM\nwith MLE, we need to sample from the EBM per training iteration.\nThe training efficiency of the MLE method highly depends on the\nmixing efficiency of the Markov chain. In high-dimensional problems,\nit is very challenging to design Markov chains with fast mixing rates.\nThus, MLE training of EBMs may converge slowly. As alternatives\nto MLE training, non-MLE methods for learning EBMs (as a kind of\nunnormalized models) have been explored, such as noise-contrastive\nestimation (Gutmann and Hyvärinen, 2010; Gutmann and Hyvärinen,\n2012) and score matching (Hyvärinen and Dayan, 2005).\nAnother motivation to pursue non-MLE methods is that the op-\ntimization criterion used has profound effect on the behavior of the\noptimized model (Theis et al., 2016). Maximizing likelihood is equivalent\nto minimizing the KL divergence between pora(x) and pθ(x), because\nKL[pora(x)||pθ(x)] = −Ex∼pora(x)[log pθ(x)] + Ex∼pora(x)[log pora(x)]\n= −Ex∼pora(x)[log pθ(x)] + constant\n≈−Ex∼pemp(x)[log pθ(x)] + constant\n(2.42)\n= −L(θ) + constant\nwhere pora(x) denote the the underlying (oracle) data distribution, and\nL(θ) is the log-likelihood in Eq. (2.17). Eq. (2.42) is an unbiased Monte\nCarlo integration since the expectation under pora(x) is approximated\nby empirical samples {xi}N\ni=1.\nIt is known that the KL divergence is asymmetric, and optimizing\nwhich direction of the KL divergence leads to different trade-offs. The KL\napproximation covers the data distribution while reverse-KL has more\nof a mode-seeking behavior (Minka, 2005). The basic score matching\n(SM) objective minimizes a discrepancy between two distributions called\nthe Fisher divergence:\nDF (pora(x)||pθ(x)) = Ex∼pora(x)\n\u00141\n2||∇x log pora(x) −∇x log pθ(x)||2\n\u0015\nLearning under most criteria is provably consistent given infinite\nmodel capacity and data. Most of the methods are firstly examined over\n2.4. LEARNING EBMS BY NOISE-CONTRASTIVE ESTIMATION\n(NCE)\n61\ncontinuous data, with few on discrete data. The score matching method\nis based on minimizing the expected squared distance of the score\nfunction17 of the data distribution and the score function given by the\nmodel, and thus is not applicable to training EBMs over discrete data\nsuch as natural languages. In contrast, the noise-contrastive estimation\nmethod has no such limitation, which will be detailed below.\n2.4\nLearning EBMs by noise-contrastive estimation (NCE)\nNoise-contrastive estimation (NCE) is proposed in (Gutmann and\nHyvärinen, 2010; Gutmann and Hyvärinen, 2012), as a typical non-\nMLE method, for learning unnormalized statistical models. Its basic\nidea is “learning by comparison”, i.e. to perform nonlinear logistic re-\ngression to discriminate between data samples drawn from the data\ndistribution pora(x) and noise samples drawn from a known noise dis-\ntribution q(x). An advantage of NCE is that the normalizing constant\ncan be treated as a normal parameter and updated together with the\nmodel parameters.\nDenote the target unnormalized model by:\npθ(x) = 1\nZθ\n˜pθ(x)\n(2.43)\nwhere we highlight that the model, parameterized by θ, is unnormalized,\nand the normalizing constant Zθ =\nR ˜pθ(x)dx. To apply NCE, we\ntreat log Zθ as an additional parameter ζ and rewrite Eq. (2.43) in the\nfollowing form:\npθ,ζ(x) = exp [−ζ + log ˜pθ(x)]\n(2.44)\nAs shown below, model parameters (θ, ζ) will be jointly estimated in\nNCE.\nIntroduce a fixed, known noise distribution denoted by q(x), and\nconsider a binary classification. There are two classes of samples, with\nprior probabilities P(C = +) and P(C = −). For class +, the sample\nis drawn from the data distribution pora; for class −, the sample is\n17The gradient of log-density with respect to the data vector x is called the score\nfunction.\n62\nBasics for EBMs\ndrawn from the noise distribution q. This defines a generation process\nof samples in NCE.\nGiven a sample x from such a generation process, the class posterior\nprobabilities are defined as follows:\np(C = +|x) =\np(C = +)p(x|C = +)\np(C = +)p(x|C = +) + p(C = −)p(x|C = −)\np(C = −|x) = 1 −p(C = +|x)\nBy our design of the binary classification experiment, p(x|C = −) is\nthe noise distribution q(x). The (unknown) class-conditional density for\nclass + is assumed to be modeled by the target model pθ,ζ(x). Let the\nratio between the prior probabilities p(C=−)\np(C=+) be ν (i.e., the ratio of noise\nsample size to real sample size). Then the posterior probabilities can\nbe parameterized as follows:\np(C = +|x; θ, ζ) =\npθ,ζ(x)\npθ,ζ(x) + νq(x)\np(C = −|x; θ, ζ) = 1 −p(C = +|x; θ, ζ)\n(2.45)\nNCE estimates the model parameters (θ, ζ) by optimizing the two-\nclass classification through maximizing the following conditional log-\nlikelihood:\nJNCE(θ, ζ) = Ex∼pora(x) [log p(C = +|x; θ, ζ)]+νEx∼q(x) [log p(C = −|x; θ, ζ)]\n(2.46)\nThe objective function J(θ, ζ) is a sum of two expectations. The first\nis the expectation w.r.t. the data distribution pora(x), which can be\napproximated by randomly drawing samples from training data, namely\napproximating pora(x) by pemp(x). The second is the expectation w.r.t.\nthe noise distribution q(x), which can be approximated by drawing\nsamples from the noise distribution.\nSetting to zeros the gradients of J(θ, ζ) w.r.t. (θ, ζ), we can apply the\nSA algorithm to find its root and thus solve the optimization problem\nin Eq. (2.46). The pseudocode of NCE is shown in Algorithm 9. The\nrelevant gradients can be further simplified as follows:\n∇θ,ζ log p(C = +|x; θ, ζ) = p(C = −|x; θ, ζ)∇θ,ζ log pθ,ζ(x)\n∇θ,ζ log p(C = −|x; θ, ζ) = −p(C = +|x; θ, ζ)∇θ,ζ log pθ,ζ(x)\n(2.47)\n2.4. LEARNING EBMS BY NOISE-CONTRASTIVE ESTIMATION\n(NCE)\n63\nAlgorithm 9 NCE for fitting an unnormalized model\nrepeat\nSampling: Draw an empirical minibatch D ∼pora(x) and a noise\nminibatch B ∼q(x), satisfying ν = |B|/|D|;\nUpdating: Update (θ, ζ) by ascending:\n1\n|D|\nX\nx∼D\n∇θ,ζ log p(C = +|x; θ, ζ)+ ν\n|B|\nX\nx∼B\n∇θ,ζ log p(C = −|x; θ, ζ)\nuntil convergence\nIt is shown in (Gutmann and Hyvärinen, 2012) that under the ideal\nsituation of infinite amount of data and infinite model capacity, we have\nthe following theorem (nonparametric estimation). It is further shown\n(Gutmann and Hyvärinen, 2012) that the NCE estimator is consistent.\nTheorem 2.4 (Nonparametric estimation). JNCE(θ, ζ) attains its maxi-\nmum at pθ,ζ(x) = pora(x). There are no other extrema if the noise density\nq(x) is chosen such that it is nonzero whenever pora(x) is nonzero.\nThe noise distribution q and the ratio ν have an influence on the\naccuracy of the NCE estimate of model parameters (θ, ζ). A natural\nquestion to ask in applying NCE is what, from a statistical standpoint,\nthe best choice of q and ν is, to get estimates with a small estimation\nerror. This question is discussed in the original paper of NCE (Gutmann\nand Hyvärinen, 2012), which give the following suggestions:\n1. Choose noise for which an analytical expression for log q is avail-\nable.\n2. Choose noise that can be sampled easily.\n3. Choose noise that is in some aspect, for example with respect to\nits covariance structure, similar to the data.\n4. Make the noise sample size as large as computationally possible.\n64\nBasics for EBMs\n2.4.1\nDynamic noise-contrastive estimation (DNCE)\nRemarkably, there exist two problems in applying NCE learning. First,\nreliable NCE needs a large ν, especially when the noise distribution is\nnot close to the data distribution. And the time and memory cost for\ngradient calculation are almost linearly increased with ν. Second, the\nexpectation w.r.t. the data distribution pora in Eq. (2.46) is approxi-\nmated by the expectation w.r.t. the empirical distribution pemp (namely\nthe training data), which is rather sparse for high-dimensionality data\nmodeling. The model estimated by NCE is thus easily overfitted to the\nempirical distribution. Dynamic noise-contrastive estimation (DNCE)\nwas proposed in (Wang and Ou, 2018a) to address the above problems,\nwith two modifications.\nFirst, instead of using a fixed noise distribution, a dynamic noise\ndistribution qϕ(x) with parameter ϕ is introduced in DNCE. In addi-\ntion to maximizing w.r.t. (θ, ζ) the NCE objective function JNCE(θ, ζ),\nDNCE simultaneously performs maximum likelihood optimization of ϕ\nover training data:\nmax\nϕ\nEx∼pora(x) [log qϕ(x)]\nThe motivation is to push the noise distribution to be close to the data\ndistribution, so that we can achieve reliable model estimation even\nusing a small ν. Theoretically, one can optimize the noise distribution\nbeforehand and then use a fixed noise density in NCE. It is found that\ndynamic noise distribution helps optimization, by gradually increasing\nthe difficulty of the two-class discrimination task (Wang and Ou, 2018a).\nIf the noise distribution qϕ is too different from the data distribution\npora, the two-class discrimination problem might be too easy and would\nnot require the system to learn much about the structure of the data.\nBut if qϕ is too close to pori from the beginning, the discrimination\nproblem might be too difficult to proceed.\nSecond, instead of using the standard NCE objective function\nJNCE(θ, ζ) in Eq. (2.46), a modified objective function is proposed\n2.4. LEARNING EBMS BY NOISE-CONTRASTIVE ESTIMATION\n(NCE)\n65\nas follows18:\nJDNCE(θ, ζ) = Ex∼pint(x) [log p(C = +|x; θ, ζ)]+νEx∼qϕ(x) [log p(C = −|x; θ, ζ)]\n(2.48)\nwhere\npint(x) = αpora(x) + (1 −α)qϕ(x)\ndenotes an interpolation of the data distribution and the noise distri-\nbution, and 0 < α < 1 is the interpolating factor. p(C = +|x; θ, ζ) and\np(C = −|x; θ, ζ) are defined the same as in Eq. (2.45), except that the\nfixed noise distribution q(x) is replaced by the dynamic noise distribu-\ntion qϕ(x). Intuitively, as the noise distribution qϕ converges to the data\ndistribution, using the interpolated distribution pint will increase the\nnumber of data-like samples by adding samples drawn from the noise\ndistribution. This could avoid the model to be overfitted to the sparse\ntraining set.\nPutting the two modifications together, DNCE conducts the follow-\ning joint optimization,\n\n\n\n\nmax\nθ,ζ JDNCE(θ, ζ)\nmax\nϕ\nEx∼pora(x) [log qϕ(x)]\nwhich can be solved by applying minibatch-based stochastic gradient\nascent.\nAt each iteration, a set of data samples, denoted by D, is sampled\nfrom pora, with the number of samples in D denoted as |D|. Additionally,\ntwo sets of noise samples are drawn from the noise distribution qϕ,\ndenoted by B1 and B2, whose sizes satisfy |B1| = 1−α\nα |D| and |B2| = ν\nα|D|\nrespectively. As a result, the union of D and B1 can be viewed as samples\ndrawn from the interpolated distribution pint, with |D∪B1| = |D|\nα . Model\nparameters (θ, ζ) are updated by ascending the following stochastic\ngradients:\nα\n|D|\nX\nx∈D∪B1\np(C = −|x; θ, ζ)∇θ,ζ log pθ,ζ(x)\n−α\n|D|\nX\nx∈B2\np(C = +|x; θ, ζ)∇θ,ζ log pθ,ζ(x)\n18In JDNCE(θ, ζ) and pint(x), we suppress their dependency on ϕ, since, as will be\nshown later, the optimization of JDNCE(θ, ζ) is taken only over (θ, ζ) while fixing ϕ.\n66\nBasics for EBMs\nNoise model parameter ϕ are updated by ascending the following stochas-\ntic gradient:\n1\n|D|\nX\nx∈D\n∇ϕ log qϕ(x)\nA final remark is that the theoretical consistency of DNCE learning\nin the nonparametric limit can be shown by the following theorem.\nTheorem 2.5. Suppose that an arbitrarily large number of data samples\ncan be drawn from pora, and the model distribution pθ(x) and the noise\ndistribution qϕ(x) have infinite capacity. Then we have\n(i) The KL divergence KL(pora||qϕ) can be minimized to attain zero;\n(ii) If KL(pori||qϕ) attains zero at ϕ∗, and the conditional log-likelihood\nEq. (2.48) attains a maximum at (θ∗, ζ∗), then we have\npθ∗(x) = qϕ∗(x) = pora(x)\nProof.\n(i) This conclusion can be easily seen by consistencey of MLE,\nsince minimizing KL(pora||qϕ) is equivalent to MLE of qϕ.\n(ii) From KL(pori||qϕ∗) = 0, we have qϕ∗= pora.\nBy Theorem 2.4, with fixed ϕ∗, Eq. (2.48) has the only extremum\nat pθ∗(x) = pint(x)|ϕ=ϕ∗= αpora(x) + (1 −α)qϕ∗(x).\nThe conclusion is clear from combining the above two equations.\n■\n2.5\nGeneration from EBMs\nGiven an EBM, an important inference task is sampling from the model,\ni.e., drawing or generating samples from the model. Sampling is not\nonly a critical step in maximum likelihood learning of EBMs (as we\nintroduce in Section 2.3), but also itself forms as an important class\nof applications. Generating text, images, speech, or other media has\nreceived increasing interests, and recently has been collectively referred\nto as generative AI19.\nTransformer-based (Vaswani et al., 2017) autoregressive language\nmodels (ALMs), generating text sequentially from left to right, have\n19https://en.wikipedia.org/wiki/Generative_artificial_intelligence\n2.5. GENERATION FROM EBMS\n67\nbeen the dominant approach for text generation (Radford et al., 2018).\nKey to their success is local normalization, i.e. they are defined in\nterms of a product of conditional distributions, one for each token in\nthe sequence. These models can be trained efficiently via maximum\nlikelihood teacher-forcing, and sampling from ALMs is straightforward\nby ancestral sampling . Unfortunately, local normalization also brings\nsome drawbacks for these locally-normalized sequence models, when\ncompared to globally-normalized sequence models (namely EBM based\nsequence models)20. As will be detailed in Section 4.1.2 and Section\n4.4.1, the drawbacks include:\n• Discrepancy between training and inference (related to exposure\nbias);\n• Limitation in long-range coherency due to only left-to-right mod-\neling and decoding (related to label bias);\n• Inflexibility in controlled generation (e.g., satisfying hard lexical\nconstraints and/or soft topical constraints in text generation).\nThere are similar concerns for generating other sequence media (e.g.,\nspeech). There have been studies for speech synthesis by EBMs (Sun\net al., 2023) and also by some non-autoregressive models, such as\nFastSpeech 2 (Ren et al., 2020) and diffusion models (Popov et al.,\n2021).\nRemarkably, given a learned EBM, some applications need likeli-\nhood evaluation (e.g., in language modeling for speech recognition),\nwhile other tasks require generation from the learned generative model\n(e.g., in many NLP tasks such as summarization, dialog, and machine\ntranslation). Generation from generative models basically is sampling\nfrom them. In practice, generating from locally-normalized sequence\nmodels, sometimes also referred to as decoding, can be readily realized\nby greedy decoding (beam search) or nucleus sampling (Holtzman et al.,\n20There are some studies, which do not involve EBM modeling, but use a masked\nlanguage modeling objective to train the model. This approach, referred to as\nnon-autoregressive generation, performs iterative decoding, i.e., generating non-\nautoregressively, then masking out and regenerating, and so cycles for a number of\niterations (Ghazvininejad et al., 2019).\n68\nBasics for EBMs\nTable 2.1: A survey of different sampling methods used in generating text from\nEBMs. The target model is the EBM, while a proposal is required for both MCMC\nand IS. Different proposals are used in different applications. Shorthands: ALMs (au-\ntoregressive language model), MLM (masked language model), SNIS (self-normalized\nimportance sampling), ASR (automatic speech recognition), CTG (controlled text\ngeneration), CTGAP (conditional text generation after prefix).\nSampling method\nProposal\nApplication\nMH within\nGibbs sampling\nConditional\nof word class\nASR (Wang et al., 2015; Wang\net al., 2018)\nALM\nASR (Wang and Ou, 2017)\nMLM\nCTG (Miao et al., 2019; Goyal\net al., 2022; Mireshghallah et\nal., 2022) (see Section 4.4.2)\nSNIS\nALM\nCTG (Parshakova et al., 2019;\nKhalifa et al., 2021); CTGAP\n(Deng et al., 2020) (see Section\n4.4.1)\nLangevin dynamics\n-\nCTG (Qin et al., 2022)\n2019) as engineering variants of ancestral sampling. There is no easy\nmethods for sampling from EBMs. Basically we have to resort to Monte\nCarlo methods, such as MCMC and importance sampling (IS), which is\nusually not as computational efficient as ancestral sampling. Perhaps\nthis is the most challenging practical limitation of EBMs for their ap-\nplications in generation, and the dominant approach to text generation\nis still based on large neural auto-regressive models.\nTheoretically, sampling from EBMs can be performed by the MCMC\nand importance sampling methods, which are introduced in Section 2.3.1\nand Section 2.3.2 respectively. Gradient-based MCMC methods (Section\n2.3.1), such as Langevin dynamics, are good choices for sampling for\ncontinuous data (e.g., images), by using the gradient of the potential\n∇xUθ(x). However, since text is discrete, the gradient is not well-defined,\nmaking it non-trivial to apply gradient-based MCMC methods for\nsampling text from EBMs.\n2.5. GENERATION FROM EBMS\n69\nVarious MCMC and IS methods have been explored in applications\nof EBMs for generating text. In Table 2.1, we survey some recent\nstudies, which provide concrete examples. Remarkably, both MCMC\nand IS methods need proposal distributions, and the design of proposal\ndistributions heavily depends on specific applications. We comment on\nthe particular proposal distributions used in different applications in\nTable 2.1. some further discussions are as follows.\nMH within Gibbs sampling.\nSuppose we use Gibbs sampling to gener-\nate a sequence of n tokens, x = (x1, · · · , xn), from an EBM distribution\np(x1, · · · , xn). Exact Gibbs sampling needs to calculate the conditional\ndistribution p(xi|x\\i) of token xi for each position i, given all the other\ntokens x\\i ≜(x1, · · · , xi−1, xi+1, · · · , xn). For modern EBMs developed\nfor text, such as in (Wang et al., 2015; Wang et al., 2018; Wang and Ou,\n2017) and so on, this is computational expensive, because calculating\np(xi|x\\i) needs to enumerate all possible values of xi from V and to\ncompute the joint probability p(xi, x\\i) for each possible value, where V\ndenotes the vocabulary. Metropolis-Hastings (MH) within Gibbs sam-\npling has been explored, with a proposal, to draw MCMC samples from\np(xi|x\\i).\nIn (Wang et al., 2015; Wang et al., 2018), word classing is introduced\nto accelerate sampling, which means that each word is assigned to a\nsingle class. Through applying MH within Gibbs sampling, we first\nsample the class by using a reduced model as the proposal, which\nincludes only the features that depend on xi through its class21, and\nthen sample the word. This reduces the computational cost from |V| to\n|C| + |V|/|C| on average, where |C| denotes the number of classes.\nThe computation reduction in using word classing in EBMs with\nneural features (i.e., parameterized by neural networks) is not as signifi-\ncant as in EBMs with discrete features, because EBMs parameterized\nby neural networks involve a much larger context, which makes the\nsampling computation with the reduced model still expensive. In later\nwork in (Wang and Ou, 2017), an ALM (autoregressive language model)\n21This is possible, since features used in (Wang et al., 2015; Wang et al., 2018)\nare discrete features (n-gram features). (See introduction in Section 3.3.1)\n70\nBasics for EBMs\nis introduced to propose for p(xi|x\\i), and in (Miao et al., 2019; Goyal\net al., 2022; Mireshghallah et al., 2022), a MLM (masked language mod-\nels) is used as the proposal. The proposal model can be jointly trained\nwith the EBM, as in (Wang and Ou, 2017), or pre-trained language\nmodels can be directly used for the proposal (Miao et al., 2019; Goyal\net al., 2022; Mireshghallah et al., 2022).\nSelf-normalized importance sampling (SNIS).\nThe basics are intro-\nduced in Section 2.3.2. See Section 4.4.1 for details in applications.\nLangevin dynamics.\nThe basics are introduced in Section 2.3.1. See\nSection 4.4.2 for details in applications.\n3\nEBMs for sequential data with applications in\nlanguage modeling\nIn this chapter, we are mainly concerned with learning the (marginal)\ndistribution of observation x itself by EBMs. Considering the sequential\nnature of speech and language, we show how to develop EBMs for\nsequential data, or more generally, for trans-dimensional data.\nEBMs are mostly developed in fixed-dimensional settings, for ex-\nample, in the modeling of fixed-size images. Trans-dimensional setting\nmeans that the observations can be of different dimensions. A familiar\ncase is temporal modeling of sequential data, where each observation is\na sequence of a random length. Language modeling falls exactly in this\ntrans-dimensional setting, where an observation sequence x is a natural\nlanguage sentence (i.e., a token sequence).\n3.1\nAutoregressive language model (ALM)\nLanguage modeling involves determining the joint probability p(x) of a\nsentence x, which can be denoted as a pair x = (l, xl), where l is the\nlength and xl = (x1, . . . , xl) is a sequence of l tokens. Currently, the\ndominant approach to language modeling is the locally-normalized or\nconditional modeling, which decomposes the joint probability of xl into\n71\n72\nEBMs for sequential data with applications in language modeling\na product of conditional probabilities by using the chain rule,\np(x1, . . . , xl) =\nlY\ni=1\np(xi|x1, . . . , xi−1).\n(3.1)\nLanguage models (LMs) in the form of Eq. (3.1) is known as autore-\ngressive language models (ALMs). Remarkably, for an ALM to make\nthe sum of the probabilities of all sequences equal to 1, it is necessary\nto place a special token ⟨EOS⟩at the end of sentences and to include\nthis in the product of Eq. (3.1) (Chen and Goodman, 1999). Otherwise,\nthe sum of the probabilities of all sequences of a given length is 1, and\nthe sum of the probabilities of all sequences is then infinite.\nIn early days before the deep learning era, the history of xi, denoted\nas (x1, · · · , xi−1), is often reduced to equivalence classes through a\nmapping ϕ(x1, · · · , xi−1) with the assumption\np(xi|x1, · · · , xi−1) ≈p(xi|ϕ(x1, · · · , xi−1)).\nA classic example is the traditional n-gram language models (LMs) with\nϕ(x1, · · · , xi−1) = (xi−n+1, . . . , xi−1),\nassuming that current token xi depends on history only through the\nprevious n−1 tokens, i.e., the (n−1)-order Markov assumption. Various\nsmoothing techniques have been used for parameter estimation, and\nparticularly, the modified Kneser-Ney (KN) smoothed n-gram LMs are\nstill widely used because of its simplicity and good performance (Chen\nand Goodman, 1999).\nRecently, neural network LMs have begun to surpass the traditional\nn-gram LMs, and also follow the locally-normalized approach. The\nmapping ϕ(x1, · · · , xi−1) condenses the history into a hidden vector\nhi ∈RD through a neural network (NN), which can be a feedforward NN\n(Schwenk, 2007), a recurrent NN (Mikolov et al., 2011; Sundermeyer\net al., 2012), or more recently, a Transformer NN (Vaswani et al.,\n2017). Specifically, in either manners, the neural network calculates the\nconditional probability at each position as follows:\np(xi = k|x1, · · · , xi−1) =\nexp(zk)\nP|V|\nj=1 exp(zj)\n(3.2)\n3.2. ENERGY-BASED LANGUAGE MODEL (ELM)\n73\nwhich is in the form of a multi-class logistic regression, as introduced in\nEq. (2.3). The logits zk = wT\nk hi + bk, k = 1, · · · , K are calculated from\na linear layer on top of hidden vector hi. wk ∈RD, bk ∈R denote the\nweight vector and bias of the linear layer, respectively. V denotes the\nvocabulary of all possible tokens.\nDrawbacks of ALMs.\nBoth the classic n-gram LMs and the recent\nneural network LMs are autoregressive language models, which are\nlocally normalized. Unfortunately, local normalization in ALMs brings\nsome drawbacks. As will be detailed in Section 4.1.2, ALMs are prone\nto exposure bias (Wiseman and Rush, 2016; Ranzato et al., 2016) and\nlabel bias (Lafferty et al., 2001; Andor et al., 2016).\n3.2\nEnergy-based language model (ELM)\n3.2.1\nGlobally-normalized ELM (GN-ELM)\nEnergy-based language models (ELMs) parameterize an unnormalized\ndistribution for natural sentences and are radically different from au-\ntoregressive language models. Let x be a natural sentence (i.e., a token\nsequence). An energy-based language model (ELM) is defined as follows\npθ(x) = exp(Uθ(x))\nZ(θ)\n(3.3)\nwhere Uθ(x) denotes the potential function with parameter θ, Z(θ) =\nP\nx exp(Uθ(x)) is the normalizing constant, and pθ(x) is the probability\nof sentence x. For reasons to be clear below (mainly to be differentiated\nfrom TRF-LM), the model in Eq. (3.3) is called globally-normalized\nELM (GN-ELM).\nELMs potentially address the drawbacks of ALMs introduced above,\nas they do not require any local normalization. Early attempts on\nbuilding energy-based language models are GN-ELMs and date back\nto (Rosenfeld et al., 2001), which proposes whole-sentence maximum\nentropy (WSME) language models1. Specifically, a WSME model has\n1Due to the connection between log-linear model and maxent model as we\nintroduced before in Section 2.1.2, this model is called WSME.\n74\nEBMs for sequential data with applications in language modeling\nthe log-linear form\np(x; λ) =\n1\nZ(λ)eλT f(x).\n(3.4)\nHere f(x) is a vector of features, which are computable functions of x\nsuch as n-grams conventionally used, λ is the corresponding parameter\nvector, and Z(λ) = P\nx eλT f(x) is the global normalizing constant.\nThere has been little work on WSME-LMs, mainly in (Rosenfeld et\nal., 2001; Amaya and Benedi, 2001; Ruokolainen et al., 2010). Although\nthe whole-sentence approach has the potential advantage of being able\nto flexibly integrate a richer set of features, the empirical results of\nprevious WSME-LMs are not satisfactory, almost the same as traditional\nn-gram LMs. After incorporating lexical and syntactic information, a\nmere relative improvement of 1% and 0.4% respectively in perplexity\nand in WER (word error rate) was reported for the resulting WSME-LM\n(Rosenfeld et al., 2001). Subsequent studies of using WSME LMs with\ngrammatical features, as in (Amaya and Benedi, 2001) and (Ruokolainen\net al., 2010), reported perplexity improvement above 10% but no WER\nimprovement when using WSME LMs alone.\nIn recent years, there are encouraging progresses in both theories and\napplications of ELMs. A new class of ELMs, called trans-dimensional\nrandom fields (TRFs), have been developed, which are different from\nGN-ELMs and present the first strong empirical evidence supporting the\npower of using energy-based approach to language modeling (Wang et al.,\n2015; Wang et al., 2018). Applications of ELMs have covered computa-\ntion of sentence likelihoods (up to a constant) for speech recognition\n(Wang et al., 2015; Wang et al., 2018; Wang and Ou, 2017; Wang and\nOu, 2018b; Wang and Ou, 2018a; Gao et al., 2020) (to be introduced in\nthe next section), text generation (Deng et al., 2020) (to be covered in\nSection 4.4.1), language model pre-training (Clark et al., 2020b) (to be\ncovered in Section 3.4), calibrated natural language understanding (He\net al., 2021) (to be covered in Section 5.5), and so on.\n3.2.2\nTrans-dimensional random field (TRF) LMs\nTo describe trans-dimensional observations in general, a new energy-\nbased probabilistic model, called the trans-dimensional random field\n3.2. ENERGY-BASED LANGUAGE MODEL (ELM)\n75\n(TRF) model, has been proposed in (Wang et al., 2015; Wang et al.,\n2018), which explicitly mixes a collection of random fields in sample\nspaces of different dimensions. A GN-ELM is globally-normalized over\nall sequences of all lengths. In contrast, a TRF-LM is a collection\nof random fields, each normalized on subspaces of different lengths\nseparately and weighted by empirical length probabilities.\nSuppose that it is of interest to build random field models for\nmultiple sets of observations of different dimensions, such as images\nof different sizes or sentences of different lengths. Denote by xj an\nobservation in a sample space X j of dimension j, ranging from 1 to\nm. The space of all observations is then the union X = ∪m\nj=1X j. To\nemphasize the dimensionality, each observation in X can be represented\nas a pair x = (j, xj), even though j is identified as the dimension of xj.\nBy abuse of notation, write f(x) = f(xj) for features of x.\nFor j = 1, . . . , m, assume that the observations xj are distributed\nfrom a random field in the form\npj(xj; λ) =\n1\nZj(λ)eUθ(xj),\nUθ(xj) : X j →R denotes the potential function which assigns a scalar\nvalue to each configuration of x in X and can be very flexibly param-\neterized through linear functions or nonlinear functions with neural\nnetworks of different architectures, to be explained in Section 3.3.1. θ\ndenotes the set of parameters, and Zj(θ) is the normalizing constant:\nZj(θ) =\nX\nxj\neUθ(xj),\nj = 1, . . . , m.\nMoreover, assume that dimension j is associated with a probability πj\nfor j = 1, . . . , m with Pm\nj=1 πj = 1. Therefore, the pair (j, xj) is jointly\ndistributed as\np(j, xj; π, θ) = πj pj(xj; θ) =\nπj\nZj(θ)eUθ(xj),\n(3.5)\nwhere π = (π1, . . . , πm)T .\nHere we actually define a mixture of random fields for joint modeling\nsentences of different dimensions (namely lengths). There is a random\nfield for each length. By maximum likelihood, the mixture weights can\nbe estimated to be the empirical length probabilities.\n76\nEBMs for sequential data with applications in language modeling\nTable 3.1: The development of TRF-LMs.\nWork\nContribution\nWang et al., 2015;\nWang et al., 2018\n• Discrete features\n• Augmented stochastic approximation (AugSA)\nfor model training\nWang and Ou,\n2017\n• Potential function as a deep CNN\n• Model training by AugSA plus JSA (joint\nstochastic approximation)\nWang and Ou,\n2018b\n• Potential function in the form of exponential\ntilting, revisited in residual EBMs (Deng et al.,\n2020)\n• Use LSTM on top of CNN\n• NCE is introduced to train TRF-LMs\nWang and Ou,\n2018a\n• Simplify the potential definition by using only\nbidirectional LSTM\n• Propose Dynamic NCE for improved model\ntraining\nGao et al., 2020\n• Mixed-feature TRFs, by integrating discrete\nand neural features\nLiu and Ou, 2023\n• Pre-trained language models (PLMs) are used\nas the backbones of energy functions, noise dis-\ntributions in NCE and proposal distributions in\nMonte Carlo\nAs outlined in Table 3.1, there are a series of works in the de-\nvelopment of TRF-LMs, each with its own contribution in different\nparameterizations of potential functions and model training methods.\nBefore expanding introductions around Table 3.1, let us first recognize\nthe differences between GN-ELM and TRF-LM.\n3.2. ENERGY-BASED LANGUAGE MODEL (ELM)\n77\n3.2.3\nComparison between GN-ELM and TRF-LM\nThe following comment on the connection and difference between GN-\nELM and TRF-LM models is adapted from the comparison between\nWSME and TRF models in (Wang et al., 2018). Suppose that we add\nthe dimension features in the GN-ELM model Eq. (3.3) and obtain\np(j, xj; λ, ν) =\n1\nZ(θ, ν)eνT δ(j)+Uθ(xj),\n(3.6)\nwhere δ(j) = (δ1(j), · · · , δm(j))T denotes the dimension features such\nthat δl(j) = 1(j = l). ν = (ν1, . . . , νm)T denotes the corresponding\nparameter vector, and Z(θ, ν) is the global normalizing constant\nZ(θ, ν) =\nm\nX\nj=1\nX\nxj∈X j\neνT δ(j)+Uθ(xj) =\nm\nX\nj=1\neνjZj(θ).\n(3.7)\nSimilar to Proposition 1 in (Wang et al., 2018), it can be seen that\nwhen both fitted by maximum likelihood estimation, model Eq. (3.6)\nis equivalent to model Eq. (3.5) but with different parameterization.\nThe parameters in model Eq. (3.6) are (θ, ν), whereas the parameters\nin model Eq. (3.5) are (π, θ). Therefore, an important distinction of\nTRF-LM from GN-ELM lies in the use of dimension features, which has\nsignificance consequences in both model definition and model learning.\nFirst, it is clear that model Eq. (3.5) is a mixture of random fields on\nsubspaces of different dimensions, with mixture weights explicitly as free\nparameters. Hence model Eq. (3.5) will be called a trans-dimensional\nrandom field (TRF). Moreover, by maximum likelihood, the mixture\nweights can be estimated to be the empirical dimension probabilities.\nSecond, it is instructive to point out that model Eq. (3.3) is essen-\ntially also a mixture of random fields, but the mixture weights implied\nare fixed to be proportional to the normalizing constants Zj(θ):\np(j, xj; θ) = Zj(θ)\nZ(λ) ·\n1\nZj(θ)eUθ(xj),\n(3.8)\nwhere Z(θ) = P\nx eUθ(x) = Pm\nj=1 Zj(θ). Typically the unknown mixture\nweights in Eq. (3.8) may differ from the empirical length probabilities\nand also from each other by orders of magnitudes, e.g. 1040 or more in\n78\nEBMs for sequential data with applications in language modeling\nthe experiments of (Wang et al., 2018). As a result, it is very difficult\nto efficiently sample from model Eq. (3.3), in addition to the fact that\nthe length probabilities are poorly fitted for model Eq. (3.3). Setting\nmixture weights to the known, empirical length probabilities enables\nus to develop an effective learning algorithm, as introduced in (Wang\net al., 2018). Basically, the empirical weights serve as a control device to\nimprove sampling from multiple distributions (Liang et al., 2007; Tan,\n2017).\n3.3\nELMs for speech recognition\nAs an important application, ELMs have been successfully used as a\nmeans for calculating sentence scores in automatic speech recognition\n(ASR). The design of energy function and the optimization of parameters\nare central research questions in applying ELMs, which are introduced\nin the following two subsections respectively.\n3.3.1\nArchitectures of energy functions\nOne is generally free to choose the energy function in ELMs, as long as\nit assigns a scalar energy to every sentence, no matter for TRF-LMs\nor GN-ELMs. A subtle difference between defining the energy function\n−Uθ(x) in Eq. (3.3) for GN-ELMs and defining −Uθ(xj) in Eq. (3.5)\nfor TRF-LMs is whether to include the special token ⟨EOS⟩or not. We\ndo not need to include ⟨EOS⟩at the end of xj in calculating Uθ(xj) for\nTRF-LMs. For GN-ELMs, we often include ⟨EOS⟩at the end of x in\ncalculating Uθ(x), hoping to help modeling of sentence lengths. Except\nthis difference, the architectures of energy functions for TRF-LMs can\nbe readily used for GN-ELMs, and vise versa.\nBroadly speaking, there are three types of energy functions.\n• Early ELMs are log-linear models using discrete features, including\nRosenfeld et al., 2001 (WSME-LM) and Wang et al., 2015; Wang\net al., 2018 (TRF-LM). These ELMs could thus be referred to as\ndiscrete ELMs.\n• Later, based on CNN and LSTM networks, ELMs using neural net-\nwork based energy functions (neural ELMs) have been developed\n3.3. ELMS FOR SPEECH RECOGNITION\n79\n(Wang and Ou, 2017; Wang and Ou, 2018b; Wang and Ou, 2018a),\noutperforming neural ALMs with similar model sizes. ELMs using\nneural network based energy functions could be regarded as using\nneural features, by following the discussion in Section 2.1.2.\n• By integrating discrete and neural features, mixed-feature TRFs\nhave also been proposed (Gao et al., 2020), demonstrating the\nadvantage of energy-based models in integrating discrete and\nneural features.\nRecently, based on Transformer networks (Vaswani et al., 2017) and\nlarge pre-trained lanugage models (PLMs) such as BERT (Devlin et al.,\n2018) and GPT2 (Radford et al., 2019), neural ELMs have been further\nadvanced (Liu and Ou, 2023). Extensive experiments are conducted on\ntwo datasets, AISHELL-1 (Bu et al., 2017) and WenetSpeech (Zhang\net al., 2022a). The results show that the best ELM achieves competitive\nresults with the finetuned GPT2 and performs significantly better than\nthe finetuned BERT. Further analysis show that the ELM obtains better\nconfidence estimate performance than the finetuned GPT2.\nIn the following, we summarize the architectures used to define Uθ(x)\nin the literature, in roughly chronological order. We first introduce the\nclassic log-linear energy function using discrete features. Then, a suite of\nnonlinear energy functions are shown, which are called Hidden2Scalar,\nSumInnerProduct, SumTargetLogit, SumMaskedLogit and SumTokenLogit,\nrespectively.\nNotably, although one energy architecture is proposed in one context\nof either TRF-LMs or GN-ELMs, it can be applied in both TRF-LMs\nand GN-ELMs. Let x = {xi}i=1...|x|, where xi ∈{1, · · · , V } is the i-th\ntoken in x. |x| denotes the length of sentence x in tokens. V denotes\nthe size of token vocabulary.\nLinear energy using discrete features\nLinear energy functions using discrete features basically corresponds to\nlog-linear models, as illustrated in Example 2.1, i.e., defining\nUλ(x) = λT f(x)\n(3.9)\n80\nEBMs for sequential data with applications in language modeling\nFigure 3.1: Example of discrete features.\nHere we follow the notations in (Wang et al., 2018), where f(x) =\n(f1(x), f2(x), . . . , fd(x))T is a feature vector, λ = (λ1, λ2, . . . , λd)T is the\ncorresponding parameter vector (instead of using θ).\nA feature fi(x), i = 1, . . . , d, can be any computable function of\nthe input x. For various applications such as language modeling, the\nparameters of local potential functions across locations are often tied\ntogether (Pietra et al., 1997). Each feature fi(x) is often defined in\nthe form fi(x) = P\nk fi(x, k), where fi(x, k) is a binary function of x\nevaluated at position k. In a trigram example, fi(x, k) equals to 1 if three\nspecific words appear at positions k to k + 2 and k ≤j −2. The binary\nfeatures fi(x, k) share the same parameter λi for different positions\nk and dimensions j, so called position-independent and dimension-\nindependent. Hence, the feature fi(x) indicates the count of nonzero\nfi(x, k) over k in the observation xj and takes values as non-negative\nintegers. Intuitively, fi(x) returns the count of a specific phrase (often\ncalled a n-gram feature) observed in the input sentence x, as shown in\nFigure 3.1.\nThe energy-based language modeling approach allows a very flexible\nuse of features, not limited to ordinary n-gram features. In (Wang et al.,\n2018), a variety of features as shown in Table 3.2 are used, mainly\nbased on word and class information. Each word is deterministically\nassigned to a single class, by running the automatic clustering algorithm\nproposed in (Martin et al., 1998) on the training dataset. In Table\n3.2, wi, ci, i = 0, −1, . . . , −5, denote the word and its class at different\nposition offset i, e.g., w0, c0 denotes the current word and its class. The\nword/class n-gram features “w”/“c”, skipping n-gram features “ws”/“cs”\n(Goodman, 2001), higher-order skipping features “wsh”/“csh”, and the\n3.3. ELMS FOR SPEECH RECOGNITION\n81\nTable 3.2: Feature definition in TRF LMs (Wang et al., 2018)\nType\nFeatures\nw\n(w−3w−2w−1w0)(w−2w−1w0)(w−1w0)(w0)\nc\n(c−3c−2c−1c0)(c−2c−1c0)(c−1c0)(c0)\nws\n(w−3w0)(w−3w−2w0)(w−3w−1w0)(w−2w0)\ncs\n(c−3c0)(c−3c−2c0)(c−3c−1c0)(c−2c0)\nwsh\n(w−4w0) (w−5w0)\ncsh\n(c−4c0) (c−5c0)\ncpw\n(c−3c−2c−1w0) (c−2c−1w0)(c−1w0)\ntied\n(c−9:−6, c0) (w−9:−6, w0)\ncrossing features “cpw” (meaning class-predict-word) are introduced in\n(Wang et al., 2015). In (Wang et al., 2018), the tied long-skip-bigram\nfeatures “tied” (Shazeer et al., 2015) are further introduced, in which\nthe skip-bigrams with skipping distances from 6 to 9 share the same\nparameter. In this way we can leverage long distance contexts without\nincreasing the model size. All the features f(x) in model Eq. (3.9) are\nconstructed from Table 3.2 in a position-independent and dimension-\nindependent manner, and only the features observed in the training\ndata are used. It is shown in (Wang et al., 2018) that the TRF-LM using\nfeatures “w+c+ws+cs+wsh+csh+tied” outperforms the KN 5-gram\nLM significantly with 10% relative error reduction.\nNon-linear energy: Hidden2Scalar\nGenerally speaking, like in (Wang and Ou, 2017; Wang and Ou, 2018b;\nDeng et al., 2020; He et al., 2021), we can use a text encoder to encode x\nand denote the encoder output (hidden vectors) by encθ(x). At position\ni, we have encθ(x)[i]. Then, the potential function can be defined as\nUθ(x) = Linear\n\n\n|x|\nX\ni=1\nencθ(x)[i]\n\n\n(3.10)\nwhere Linear(·) denotes a trainable linear layer whose output is a scalar.\nThis energy function is obtained by transforming neural hidden vectors\ninto a scalar, hence it is named by Hidden2Scalar.\n82\nEBMs for sequential data with applications in language modeling\nFigure 3.2: Hidden2Scalar: a deep CNN architecture used to define the potential\nfunction Uθ(x). Shadow areas denote the padded zeros. (Wang and Ou, 2017)\n3.3. ELMS FOR SPEECH RECOGNITION\n83\nFigure 3.3: Hidden2Scalar: a bidirectional LSTM on top of CNN used to define the\npotential function Uθ(x). (Wang and Ou, 2018b)\nWord embedding\nForward LSTM\n𝑒1\n𝑒3\n𝑒4\n𝑒5\n𝑒2\nBackward LSTM\nℎ𝑓,1\nℎ𝑓,2\nℎ𝑓,3\nℎ𝑓,4\nℎ𝑓,5\nℎ𝑏,1\nℎ𝑏,2\nℎ𝑏,3\nℎ𝑏,4\nℎ𝑏,5\nFigure 3.4: SumInnerProduct: a bidirectional LSTM used to define the potential\nfunction Uθ(x). (Wang and Ou, 2018a)\n84\nEBMs for sequential data with applications in language modeling\nThe text encoder can be based on a fully CNN architecture (Wang\nand Ou, 2017) as shown in Figure 3.2, or a bidirectional LSTM (BLSTM)\nstacked on top of CNN as shown in Figure 3.3. Recently, BERT based\ntext encoders have been used in (Deng et al., 2020; He et al., 2021; Liu\nand Ou, 2023). In (Deng et al., 2020), in the final layer of RoBERTa (Liu\net al., 2019), the mean-pooled hidden states are projected to a scalar\nenergy value. In (He et al., 2021), three variants of energy functions\n(scalar, hidden, and sharp-hidden) are defined on top of a RoBERTa\nbased text encoder, which will be detailed in Section 5.5.\nNon-linear energy: SumInnerProduct\nIn (Wang and Ou, 2018a), a bidirectional LSTM based potential function\nis defined as follows, illustrated in Figure ??. First, each word xi\n(i = 1, . . . , |x|) in a sentence is mapped to an embedded vector ei ∈Rd.\nThen the word embedding vectors are fed into a bidirectional LSTM\nto extract the long-range sequential features from the forward and\nbackward contexts. Denote by hf,i, hb,i ∈Rd the hidden vectors of the\nforward and backward LSTMs respectively at position i. Finally, we\ncalculate the inner product of the hidden vector of the forward LSTM\nat current position and the embedding vector at the next position, and\ncalculate the inner product of the hidden vector of the backward LSTM\nat current position and the embedding vector at the pervious position\n(dash line in Figure ??). The potential function ϕ(xl; θ) is computed by\nsumming all the inner products, hence named by SumInnerProduct,\nUθ(x) =\n|x|−1\nX\ni=1\nhT\nf,iei+1 +\n|x|\nX\ni=2\nhT\nb,iei−1\n(3.11)\nwhere θ denotes all the parameters in the neural network. The SumInnerProduct\nenergy provides a theoretical-solid framework to incorporate the bidi-\nrectional LSTM features.\nNon-linear energy: SumTargetLogit\nThe SumInnerProduct energy uses a bidirectional network. In order\nto exploit pre-trained language models, which mostly are ALMs and\n3.3. ELMS FOR SPEECH RECOGNITION\n85\nunidirectional, we could consider energy definition tailored to ALMs\n(Liu and Ou, 2023). Given history x1:i−1, let the output logits to predict\nthe next token be denoted by zθ(x1:i−1), whose dimension is equal to V .\nThe k-th logit is denoted by zθ(x1:i−1)[k]. Then, the potential is defined\nas\nUθ(x) =\n|x|\nX\ni=1\nzθ(x1:i−1)[xi]\n(3.12)\nThis potential function sums the logits corresponding to the target token\n(next token) at each position, hence it is named by SumTargetLogit. In\ncontrast, the ALM applies local normalization (softmax) to the logits\nzθ(x1:i−1) to obtain the conditional probability of xi given history x1:i−1.\nNon-linear energy: SumMaskedLogit\nFor masked language model (MLM), e.g., BERT, pseudo-log-likelihood\n(PLL) is introduced for scoring sentences (Wang and Cho, 2019). Inspired\nby this, we can define the potential function as follows (Liu and Ou,\n2023):\nUθ(x) =\n|x|\nX\ni=1\ngθ(mask(x, i))[i][xi]\n(3.13)\nwhere gθ denotes the MLM, whose output, at each position, is the\nlogits before softmax. gθ(mask(x, i)) means masking the i-th token\nin x and sending the masked sequence into the MLM for a forward\npass. At position i, the logit corresponding to the masked token xi is\ndenoted as gθ(mask(x, i))[i][xi]. In Eq. (3.13), the potential is defined by\nsumming the logits corresponding to masked tokens, hence it is named\nby SumMaskedLogit. Notably, this architecture is much time-consuming\nthan others, since it requires |x| forward passes to calculate the energy\nof one sentence, therefore this architecture is primarily for stimulating\nideas rather than conducting experiments.\nNon-linear energy: SumTokenLogit\nTo overcome the deficiency of SumMaskedLogit, a simplication is pro-\nposed (Liu and Ou, 2023), i.e., omitting the masking step and feeding x\n86\nEBMs for sequential data with applications in language modeling\ndirectly to the MLM, so that the logits at all positions can be calculated\nin parallel. The potential is defined as:\nUθ(x) =\n|x|\nX\ni=1\ngθ(x)[i][xi]\n(3.14)\nComparison of non-linear energy architectures\nFor comparing the different non-linear energy architectures, experi-\nments in (Liu and Ou, 2023) find that the bi-directional architectures\n(Hidden2Scalar and SumTokenLogit) based on BERT are generally\nbetter than the unidirectional architecture (SumTargetLogit).\n3.3.2\nIntegrating discrete and neural features\nThere has been a long recognition that discrete features (n-gram fea-\ntures) and neural network based features have complementary strengths\nfor language models (LMs). Generally, LMs with neural features (e.g.\nneural ALMs, neural TRF-LMs) outperform LMs with discrete features\n(e.g. KN n-gram LMs, discrete TRF-LMs), but interpolation between\nthem usually gives further improvement. This suggests that discrete\nand neural features have complementary strengths. Presumably, the\nn-gram features mainly capture local lower-order interactions between\nwords, while the neural features particularly defined by LSTMs can\nlearn higher-order interactions. Additionally, by embedding words into\ncontinuous vector spaces, neural LMs are good at learning smoothed reg-\nularities, while discrete LMs may be better suited to handling symbolic\nknowledges or idiosyncrasies in human language, as noted in (Ostendorf,\n2016). Currently, model interpolation, either linear or log-linear (Chen\net al., 2019; Wang et al., 2016), is often a second step, after the discrete\nand neural models are separately trained beforehand. The interpola-\ntion weights are ad-hoc fixed or estimated over held-out data (different\nfrom the training data in the first step). This two-step integration is\nsub-optimal.\nThe ELM approach can provide a unified and simplified approach\nin integrating both discrete and neural features, based on its capability\nin flexibly integrating a rich set of features. Basically, with the ELM\n3.3. ELMS FOR SPEECH RECOGNITION\n87\n0\n20\n40\n60\n80\n100\nepoch\n7\n7.5\n8\n8.5\n9\n9.5\n10\nWER\nDiscrete TRF full\nNeural TRF\nMixed TRF\nFigure 3.5: The WER curves of the three TRF-LMs during the first 100 training\nepochs are plotted. (Gao et al., 2020)\napproach, one is free to define the potential function in any sensible\nway with much flexibility. It is straightforward to define a mixed-feature\nTRF-LM (Gao et al., 2020), in which the potential function is a sum of\na linear potential using discrete features and a nonlinear potential using\nneural features. Mixed-feature TRF-LMs can be trained by applying\nthe dynamic noise-contrastive estimation (DNCE) method (Wang and\nOu, 2018a), as introduced in Section 2.4.1.\nMixed-feature TRF-LMs represent the first single LM model that\nincorporates both discrete and neural features without relying on a\nsecond-step interpolation. Apart from naturally integrating discrete\nand neural features, another bonus from using mixed-feature TRF-LMs\nis that faster training convergence and shorter training time can be\nachieved, using only 58% training epochs when compared to training\nneural TRF-LMs alone (see Figure 3.5). Notably, the log-likelihood of the\ntraining data with respect to (w.r.t.) the parameters of discrete features\nis concave. This helps to reduce the non-convexity of the optimization\nproblem for maximum likelihood training. Also, after incorporating\nthe linear potential, the nonlinear potential only needs to capture the\nresidual interactions between tokens. This may also explain the faster\ntraining convergence of mixed-feature TRF models.\n88\nEBMs for sequential data with applications in language modeling\nIn (Gao et al., 2020), various LMs are trained over the Wall Street\nJournal (WSJ) portion of Penn Treebank (PTB) (Marcus et al., 1993)\nand Google one-billion-word corpus2, and evaluated in N-best list rescor-\ning experiments for speech recognition. Among all single LMs (i.e. with-\nout model interpolation), the mixed-feature TRF-LMs perform the best,\nimproving over both discrete TRF-LMs and neural TRF-LMs alone, and\nalso being significantly better than LSTM ALMs. Compared to interpo-\nlating two separately trained models with discrete and neural features\nrespectively, the performance of mixed-feature TRF-LMs matches the\nbest interpolated model, and with simplified one-step training process\nand reduced training time.\n3.3.3\nResidual ELMs\nAs mentioned previously in this section, there are three types of energy\nfunctions based on discrete, neural and mixed features, respectively.\nOrthogonal to this3, there is another architecture for EBMs, i.e., in the\nform of exponential tilting of a reference distribution. In the fields of\nlanugage modeling, this model formulation dates back to Rosenfeld et al.,\n2001, Wang and Ou, 2018b, and recently Deng et al., 2020. Specifically,\nthe probability of a sentence x is defined as follows:\npθ(x) =\n1\nZ(θ)q(x)eUθ(x)\n(3.15)\nwhere a reference distribution q(x) is introduced as the baseline distri-\nbution; Uθ(x) denotes the residual potential function with parameter\nθ; Z(θ) = P\nx q(x) exp(Uθ(x)) is the normalizing constant. Hence, Eq.\n(3.15) is called a residual ELM, after Deng et al., 2020.\nThe role of residual energy −Uθ(x) is to fit the difference between\nthe data distribution and the reference distribution. If q(x) is a good\napproximation of the data distribution, such as the LSTM based ALMs\nused in (Wang and Ou, 2018b), the Transformer based ALMs in (Deng\net al., 2020), fitting the difference between the data distribution and the\nreference distribution q(x) shall be much simpler than fitting the data\n2https://github.com/ciprian-chelba/1-billion-word-language-modeling-benchmark\n3It means that the energy architectures described previously can be applied to\nUθ(x) in residual ELMs.\n3.4. ENERGY-BASED CLOZE MODELS FOR REPRESENTATION\nLEARNING OVER TEXT\n89\ndistribution directly. Moreover, when the reference distribution q(x) is\nused as the proposal distribution in Monte Carlo methods for learning\na residual EBM, we have the importance weight in the following simple\nform:\npθ(x)\nq(x) ∝eUθ(x)\nSimilarly, when the reference distribution q(x) is used as the noise\ndistribution in NCE methods for learning a residual EBM, we will have\na simple form of Eq. (2.45).\n3.3.4\nTraining methods\nDifferent types of ELMs, including GN-ELMs Eq. (3.3), TRF-LMs Eq.\n(3.5) and residual ELMs Eq. (3.15), all obeys the general from of general\nEBMs Eq. (2.13). Basically, we can use the methods introduced in\nSection 2.3 (MLE) and Section 2.4 (NCE) for learning general EBMs\nto train ELMs. A GN-ELM Eq. (3.3) has the same form as a EBM Eq.\n(2.13), so those methods can be seamlessly applied. In TRF-LMs Eq.\n(3.5), length probabilities are introduced and normalization is conducted\nfor each length separately, some special care is needed.\nFor more details about MLE for learning TRF-LMs, refer to (Wang\net al., 2018; Wang and Ou, 2017). For more details about DNCE for\nlearning TRF-LMs, refer to (Wang and Ou, 2018a; Gao et al., 2020).\n3.4\nEnergy-based cloze models for representation learning over text\nMotivation\nIn this section, we show the capability of EBMs for representation\nlearning over text (Clark et al., 2020b). The cloze task of predicting\nthe identity of a token given its surrounding context has proven highly\neffective for representation learning over text. BERT (Devlin et al.,\n2018), as a representative masked language model (MLM), implements\nthe cloze task by replacing input tokens with a special placeholder token\n[MASK], but there are some drawbacks with the BERT approach:\n• It suffers from the drawback in efficiency (only 15% of tokens are\nmasked out at a time);\n90\nEBMs for sequential data with applications in language modeling\nFigure 3.6: Comparison of BERT and Electric. Both model the conditional probabil-\nity of a token given its surrounding context. BERT produces normalized conditional\ndistribution for masked positions, while Electric calculates unnormalized conditional\nprobabilities for all input tokens. (Clark et al., 2020b)\n• It introduces a pre-train/fine-tune mismatch where BERT sees\n[MASK] tokens in training but not in fine-tuning;\n• In principle, it does not produce log-likelihoods for sentences\n(even up to an additive constant) and, in this sense, does not\ndefine a language model. The pseudo-log-likelihood (PLL) has\nbeen introduced for scoring sentences by BERT (Wang and Cho,\n2019), but calculating the PLL for a sentence x requires |x| passes\nof the transformer (once with each token masked out), and is thus\ncomputationally expensive.\nSome details are as follows. BERT and related masked language\nmodels (Devlin et al., 2018; Liu et al., 2019) train a large neural network\nto perform the cloze task. In contrast to the standard language modeling\ntask to learn the joint probability pora(x), these models try to learn the\nconditional probabilities of masked tokens occurring in their surrounding\ncontext. Specifically, multiple positions (e.g., 15%) in the input sentence\nx = (x1, · · · , xl) are randomly drawn, denoted by R = {t1, · · · , tk},\n1 ≤tj ≤l, j = 1, · · · , k, and those positions are replaced by [MASK]\n(i.e., masked). The masked sentence, denoted by mask(x, R), is encoded\ninto vector representations by a Transformer network (Vaswani et al.,\n2017). Then the vector representation at position tj is passed into a\nsoftmax layer to calculate a distribution over the vocabulary for position\ntj, j = 1, · · · , k. The conditional probabilities are maximized for learning\nthe model parameters θ:\nk\nX\nj=1\nlog pθ(xtj|mask(x, R))\n3.4. ENERGY-BASED CLOZE MODELS FOR REPRESENTATION\nLEARNING OVER TEXT\n91\nThe Electric model\nThe new model proposed in (Clark et al., 2020b), called Electric, is\nclosely related to the Electra pre-training method (Clark et al., 2020a),\nand implements the cloze task based on using conditional EBMs.\nSpecifically, Electric does not use masking or a softmax layer. Elec-\ntric first maps the unmasked input x = (x1, · · · , xl) into contextualized\nvector representations h(x) = (h1, · · · , hl) using a Transformer net-\nwork. Then, the conditional probability of a token xt occurring in the\nsurrounding context x\\t = (x1, · · · , xt−1, xt+1, · · · , xl) is modeled by a\nconditional EBM:\npθ(xt|x\\t) =\n1\nZθ(x\\t) exp(−wT ht), 1 ≤t ≤l\n(3.16)\nwhere w is learnable weight vector, and ht is the vector representation at\nposition t. A comparison of BERT and Electric is illustrated in Figure\n3.6.\nTraining of the Electric model\nThe conditional EBMs defined in Eq. (3.16) can be trained using NCE\n(Section 2.4), and more specifically its conditional version (Section 4.1.3).\nFirst, we define the un-normalized output\nˆpθ(xt|x\\t) = exp(−wT ht)\nDenote the training dataset by D. In NCE, a binary classifier is trained\nto distinguish positive token xt vs negative token ˆxt, with k negatives\nand l positives (e.g., k = ⌈0.15l⌉) for a sentence x of length l. Denote\nthe random positions by R = {t1, · · · , tk}, 1 ≤tj ≤l, j = 1, · · · , k.\nFormally, referring to Eq. (2.46), the NCE loss L(θ) is as follows:\nl · Ex∼D,t∼Uni(1,l)\n\"\n−log\nl · ˆpθ(xt|x\\t)\nl · ˆpθ(xt|x\\t) + k · q(xt|x\\t)\n#\n+k · Ex∼D,t∼R,\nˆxt∼q(ˆxt|x\\t)\n\"\n−log\nk · q(ˆxt|x\\t)\nl · ˆpθ(ˆxt|x\\t) + k · q(ˆxt|x\\t)\n#\n(3.17)\nwhere q(xt|x\\t) denotes the noise distribution, which was realized by a\ntwo-tower cloze model (Clark et al., 2020b). Specifically, the noise model\n92\nEBMs for sequential data with applications in language modeling\nruns two transformers TLTR and TRTL over the input sequence. These\ntransformers apply causal masking so one processes the sequence left-\nto-right (LTR) and the other operates right-to-left (RTL). The model’s\npredictions come from a softmax layer applied to the concatenated\nstates of the two transformers:\n−→h = TLTR(x), ←−h = TRTL(x)\nq(xt|x\\t) = softmax(W[−→h t−1, ←−h t+1])xt\nThe noise distribution is trained simultaneously with Electric using\nstandard maximum likelihood estimation over the data. Thus, the\ntraining method used by Electric is in fact DNCE (Section 2.4.1).\nNote that the NCE loss Eq. (3.17) does not introduce additional\nparameters for normalizing constants Zθ(x\\t). In the setting of non-\nparametric estimation (Theorem 2.4), the NCE loss is minimized when\nˆpθ(xt|x\\t) matches the oracle density. Thus, we assume that the model\nis of sufficient capacity and the model can learn to be self-normalized\nsuch that Zθ(x\\t) = 1.\nA further examination of Eq. (3.17) reveals that its optimization\nis computationally expensive to run. It requires k + 1 forward passes\nthrough the Transformer to compute the ˆpθ’s, once for the positive\nsamples xt|x\\t and once for every negative sample ˆxt|x\\t. So a modified\nalgorithm, allowing more efficient calculation, is to replace the k random\npositions simultaneously by negative tokens ˆxt ∼q(xt|x\\t). The resulting\nnoised sequence is denoted by\nxnoised = replace(x, R, (ˆxt1, · · · , ˆxtk))\nTo apply this efficiency trick, Electric assumes ˆpθ(·|x\\t) = ˆpθ(·|xnoised\n\\t\n),\nfor “·” taking xt or ˆxt, i.e., assuming that extra noise replacing does\nnot change the conditional distribution much, for both positive and\nnegative tokens. The modified loss for a sentence x of length l becomes\n3.4. ENERGY-BASED CLOZE MODELS FOR REPRESENTATION\nLEARNING OVER TEXT\n93\nas follows4:\nl · Ex∼D,t∼Uni(1,l)\n\"\n−log\n(l −k) · ˆpθ(xt|xnoised\n\\t\n)\n(l −k) · ˆpθ(xt|xnoised\n\\t\n) + k · q(xt|x\\t)\n#\n+k · Ex∼D,t∼R,\nˆxt∼q(ˆxt|x\\t)\n\"\n−log\nk · q(ˆxt|x\\t)\n(l −k) · ˆpθ(ˆxt|xnoised\n\\t\n) + k · q(ˆxt|x\\t)\n#\n(3.18)\nCalculating Eq. (3.18) requires just one pass through the Transformer\nfor k noise sample and l −k data samples. This brings significant\ncomputation reduction.\nPerformance of the Electric model\nExperiments in (Clark et al., 2020b) on GLUE natural language under-\nstanding benchmark (Wang et al., 2019) and SQuAD question answering\ndataset (Rajpurkar et al., 2016) show that Electric substantially outper-\nforms BERT but slightly under-performs ELECTRA. However, Electric\nis particularly useful in its ability to efficiently produce pseudo-log-\nlikelihood (PLL) scores for text, as defined below.\nPLL(x) =\nl\nX\nt=1\nlog ˆpθ(xt|x\\t) =\nl\nX\nt=1\n−wT ht\n(3.19)\nExperiments on the 960-hour LibriSpeech corpus (Panayotov et al.,\n2015) show Electric is better at re-ranking the outputs of a speech\nrecognition system than GPT2 (Radford et al., 2019) and is much faster\nat re-ranking than BERT because it scores all input tokens simultane-\nously rather than having to be run multiple times with different tokens\nmasked out. It appears that EBMs are a promising alternative to the\nstandard ALMs and MLMs currently used for language representation\nlearning.\n4To compare with Eq. (3.17), this modified loss is written for l −k positives and\nk negatives, while Eq. (3.17) is for l positives and k negatives.\n4\nConditional EBMs with applications\nIn Chapter 3, we mainly introduce EBMs for modeling the marginal\ndistribution of natural language sentences, with applications to language\nmodeling for speech recognition and language representation learning.\nIn this chapter, we introduce EBMs for modeling conditional dis-\ntributions, i.e., using EBMs for conditional models (Section 1.1.2). In\nSection 4.1, we present basics for conditional EBMs, or equivalently,\nconditional random fields (CRFs). In the following sections, we show\nhow they can be applied in not only discriminative tasks such as speech\nrecognition (Section 4.2) and natural language labeling (Section 4.3)\nbut also conditional text generation tasks (Section 4.4).\n4.1\nCRFs as conditional EBMs\nAs introduced in (Section 1.1.2), many real-world applications are solved\nby conditional models. Conditional random fields (CRFs) (Lafferty et\nal., 2001; Sutton, McCallum, et al., 2012) have been known to be\none of the most successful conditional models, especially for sequence\nlabeling. A CRF is basically a conditional distribution p(y|x) defined as\na random field, or equivalently, an undirected graphical model. A formal\ndefinition is as follows. Similar to the equivalent meaning of random\n94\n4.1. CRFS AS CONDITIONAL EBMS\n95\nFigure 4.1: Graphical model representation of a conditional random field (CRF).\nfields and EBMs in unconditional setting, conditional random fields and\nconditional EBMs are exchangeable terms.\nDefinition 4.1 (Conditional random field). Consider probability distribu-\ntions p(y|x), where x represents an observation corresponding to the\ninput variable, and y is the output variable that we wish to predict. A\nvariable can either be scalar- or vector-valued. Suppose y = y1, · · · , yT .\nIn an undirected graph consisting of x and (y1, · · · , yT ) (e.g., as shown\nin Figure 4.1), let C denote the set of cliques in the subgraph induced\nby y. Associated with each clique C ∈C, let ϕC(yC, x) denote a (log)\npotential function. An conditional random field (CRF) in terms of this\nundirected graph consists of a family of distributions that factorize as:\np(y|x) =\n1\nZ(x)\nY\nC∈C\nϕC(yC, x)\n(4.1)\nwhere Z(x) is the normalizing constant given by\nZ(x) =\nX\ny\nY\nC∈C\nϕC(yC, x)\n(4.2)\nEq. (4.1) actually means that for every assignment x, p(y|x) fac-\ntorizes according to the subgraph induced by y. Since x is given, x is\ntreated as a constant, and included in the definition of potentials.\n4.1.1\nLinear-chain CRFs\nLinear-chain CRFs are a class of CRFs, particularly useful for sequence\nlabeling in the area of natural language processing (NLP), such as part-of-\nspeech (POS) tagging (Collobert et al., 2011), named entity recognition\n96\nConditional EBMs with applications\n(NER) (Lample et al., 2016; Ma and Hovy, 2016), chunking (Søgaard\nand Goldberg, 2016) and syntactic parsing (Durrett and Klein, 2015),\nand also in other areas such as bioinformatics (Sato and Sakakibara,\n2005; Peng et al., 2009). Formally, in sequence labeling, the input x is a\nsequence with the same length of the output y. Thus, given a sequence\nof observations x = (x1, · · · xT ) = x1:T , the task of sequence labeling is\nto predict a sequence of labels y = (y1, · · · yT ) = y1:T , with one label\nfor one observation in each position. yi ∈{1, · · · , K} denotes the label\nat position i.\nA linear-chain CRF defines a conditional distribution for label\nsequence y given observation sequence x in the following form:\np(y|x) ∝exp\n( T\nX\nt=1\nϕt(yt, x) +\nT\nX\nt=1\nψt(yt−1, yt, x)\n)\n.\n(4.3)\nHere the labels yt’s are structured to form a chain, giving the term\nlinear-chain. Over the linear-chain, we define node potentials and edge\npotentials.\n• ϕt(yt, x) is often called the node potential at position t. Tradition-\nally, people use discrete features, as introduced in Example 2.1 and\nSection 3.3.1, to define node potentials. The feature functions (or\nsimply, the features) are usually hand-crafted indicator functions,\nwhich we think are useful for labeling, as shown below.\nf1(yt, x) =δ(yt = prep, xt = on)\nf2(yt, x) =δ(yt = adv, xt ends in ly)\n· · ·\nThen, the potential function at position t can be defined as follows:\nϕt(yt, x) =\nX\ni\nλifi(yt, x)\n(4.4)\nwhere i indexes the different features. Notably, the potential\nfunctions ϕt(·, x) defined above at different positions take identical\nforms, i.e., position-independent.\n• A recent progress is the development of Neural CRFs (NCRFs),\nwhich combines the sequence-level discriminative ability of CRFs\n4.1. CRFS AS CONDITIONAL EBMS\n97\nand the representation ability of neural networks (NNs). In differ-\nent studies, these models are called conditional neural field (Peng\net al., 2009), neural CRF (Artieres et al., 2010), recurrent CRF\n(Mesnil et al., 2015), and LSTM-CRF (Lample et al., 2016; Ma\nand Hovy, 2016). Though there are detailed differences between\nthese existing models, generally they are all defined by using NNs\n(of different network architectures) to implement the non-linear\nnode potentials in CRFs, while still keeping the linear-chain hid-\nden structure, i.e., using a bigram table as the edge potential (to\nbe detailed below). Suppose that the input sequence x1:T is trans-\nformed into vector representations h1:T = (h1, · · · hT ) ∈RT×D\nvia an appropriate neural network. The vector representations\nh1:T can be viewed as features, and the neural network is referred\nas a feature extractor, as discussed previously in Section 2.1.1.\nThen, the node potential can be calculated via a linear layer on\ntop of the vector representations:\nϕt(yt = k, x) = wT\nk ht + bk ≜ϕk\nt , k = 1, · · · , K\n(4.5)\nwhere wk ∈RD, bk ∈R denote the weight vector and bias of the\nlinear layer, independent of t. ϕk\nt denotes the potential value at\npositon t for label k.\n• ψt(yt−1, yt, x) is the edge potential defined on the edge connecting\nyt−1 and yt. It is mostly implemented as a transition matrix A:\nψt(yt−1 = j, yt = k, x) = Aj,k\n(4.6)\n4.1.2\nLabel bias and exposure bias\nIt is known that locally-normalized sequence models are prone to la-\nbel bias (Lafferty et al., 2001; Andor et al., 2016) and exposure bias\n(Wiseman and Rush, 2016; Ranzato et al., 2016). Depending on locally-\nnormalized or globally-normalized, unconditional or conditional, there\nare four classes of models, as overviewed in Table 4.1.\n• The label bias problem was raised initially in the conditional\nsetting (Lafferty et al., 2001). Later in this section, we will show\nthat the label bias problem also exist in the unconditional setting.\n98\nConditional EBMs with applications\nTable 4.1: A general classification of sequence models, with some common examples.\nUnconditional\nConditional\nLocally-normalized\nALM\nseq2seq\nGlobally-normalized\nELM\nCRF/Conditional EBM\n• The exposure bias is related to the manner of model training,\nrather than caused by the model itself; so it exist in both uncon-\nditional and conditional settings.\nIn summary, the problems of label bias and exposure bias exist in\nboth unconditional and conditional settings. An advantage of globally-\nnormalized sequence models is that they avoid the problems of label\nbias and exposure bias, as explained below.\nThe label bias problem\nConditional, locally-normalized sequence models such as maximum\nentropy Markov models (MEMMs) (McCallum et al., 2000) and sequence-\nto-sequence models (seq2seq) (Sutskever et al., 2014) are potential\nvictims of the label bias problem. In general, a conditional, locally-\nnormalized sequence model is described by the conditional probability\np(y|x) with the following decomposition, where x = (x1, · · · xT ) = x1:T\nis an input sequence and y = (y1, · · · yL) = y1:L is its corresponding\noutput sequence whose length L may differ from T.\np(y1:L|x1:T ) =\nL\nY\ni=1\np(yi|x1:T , y1, · · · , yi−1).\n(4.7)\nThe ouput probabilities at each time-step, p(yi|x1:T , y1, · · · , yi−1), are\nlocally normalized, so successors of incorrect histories receive the same\nmass as do the successors of the true history (Wiseman and Rush, 2016).\nSo locally normalized sequence models often have a very weak ability\nto revise earlier decisions (Andor et al., 2016). This problem has been\ncalled the label bias problem.\nThe label bias problem is more severe when the output probability\nof yi can only depend on a partial input sequence x1:t(i), where t(i)\n4.1. CRFS AS CONDITIONAL EBMS\n99\nFigure 4.2: State transitions resulting from estimating an autoregressive language\nmodel from training data - “Tom likes tea”, “John likes tea”, and “Alice like tea”.\nFor some transitions not appeared in the training data, the transition probabilities\nare smoothed to take small values ϵ. We pad the beginning and the end of a sentence\nwith special tokens, ⟨s⟩and ⟨/s⟩, respectively (Chen and Goodman, 1999).\ndenotes the available length of the partial input when producing yi.\nFor example, in streaming speech recognition, each token yi must be\nrecognized shortly after it was spoken. It is found in (Variani et al.,\n2022) that by switching from a locally normalized model to a globally\nnormalized model, the streaming speech recognition performance can\nbe significantly improved.\nIn the case of using p(yi|x1:t(i), y1, · · · , yi−1), the model becomes:\np(y1:L|x1:T ) =\nL\nY\ni=1\np(yi|x1:t(i), y1, · · · , yi−1).\n(4.8)\nThe label bias problem can be understood by considering the in-\ndependence assumption made by the model. In this case, we have\ny1:i ⊥xt(i)+1:T |x1:t(i), i.e.,\np(y1:i|x1:T ) = p(y1:i|x1:t(i))\nThus, observations from later in the input sequence xt(i)+1:T has no\neffect on the posterior probability of history input y1:i. Intuitively, we\nwould like the model to be able to revise an earlier decision made during\nsearch, when later evidence becomes available that rules out the earlier\ndecision as incorrect (Andor et al., 2016).\nThe above discussion of the label bias problem is for conditional,\nlocally-normalized sequence models. In the following, we will show,\n100\nConditional EBMs with applications\nFigure 4.3: Estimating a globally-normalized energy-based language model (ELM)\nfrom training data - “Tom likes tea”, “John likes tea”, and “Alice like tea”. The\nbi-gram features used by the ELM are similar to those used in the bigram ALM, and\nso can also be illustrated by a graph. The estimated parameters are shown over the\nedges, which represent the corresponding bi-gram features.\nthrough an empirical example1, that the label bias problem causes\ntrouble not only in conditional, locally-normalized sequence models, but\nalso in unconditional, locally-normalized sequence models; and can be\nnaturally overcome in globally-normalized sequence models. Suppose\nthat we would like to build a model for sentences with the following\ntraining data, which consist of three sentences:\nTom likes tea\nJohn likes tea\nAlice like tea\nSince the training data are often noisy, there exist some samples with\nsome incorrect, infrequent patterns such as in “Alice like tea”. We\nestimate different models over these training data, apply them to score\ntest data\nAlice likes tea\nTom like tea\nand examine the performance of different models.\nLet us first consider an autoregressive language model (ALM), which\nis a typical unconditional, locally-normalized sequence model, trained\n1This English example is adapted from a Chinese example in Section 3.4.1 of\n(Wang, 2018)’s.\n4.1. CRFS AS CONDITIONAL EBMS\n101\nover these training data. We estimate a smoothed bi-gram ALM, i.e., we\nsmooth the transition probabilities for unseen bi-grams. The resulting\nALM with estimated transition probabilities are shown in Figure 4.2. We\ncan see that transitions from the wrong history (“like”) and the correct\nhistory (“likes”) to “tea” get the same score, due to local normalization\nof conditional probabilities. Consequently, when scoring test data -\n“Alice likes tea” and “Tom like tea”, the bi-gram ALM cannot score\nthem correctly - the two test samples are scored as being of the same\nprobability:\nP(Alice likes tea) = P(Alice|⟨s⟩)P(likes|Alice)P(tea|likes)P(⟨s⟩|tea)\n= 0.33 × ϵ × 1.0 × 1.0 = 0.33ϵ\nP(Tom like tea) = P(Tom|⟨s⟩)P(like|Tom)P(tea|like)P(⟨s⟩|tea)\n= 0.33 × ϵ × 1.0 × 1.0 = 0.33ϵ\nIn fact, “Alice likes tea” should be more likely than “Tom like tea”,\nconsidering that the correct form of verb appears more often than\nthe incorrect form in the training data. We can see that the bi-gram\nALM seems not to be a good model choice for this example, due to\nlocal normalization. To be more precise, this label bias problem is\ncaused by the improper local normalization following incorrect histories.\nIncorrect histories occur in training data, as part of noise. Successors\nafter incorrect histories are in fact biased labels, even they appear in\ntraining data. This issue should be somehow fixed in order to better\nmodel the regularities in the data2. However, it is worthwhile to remark\nthat how adverse this label bias issue will cause depends on the particular\ndata and model under investigation. Further theoretical analysis will\nbe interesting future work.\nNext, let us consider a globally-normalized energy-based language\nmodel (GN-ELM) (Section 3.2.1), trained over the same training data,\nbut with a different model assumption from ALMs. The globally-\nnormalized model, which is defined as follows, also uses the bi-gram\n2We usually do not assume that the data generating distribution is exactly the\nsame as the empirical distribution, but rather the modeling task is to seek a smoothed\ndistribution based on the empirical distribution (See discussion of the concept of\nlearning in Section 1.1).\n102\nConditional EBMs with applications\nfeatures, similar to those used in the bi-gram ALM:\nP(x1, x2, x3) = 1\nZ exp\n 9\nX\nk=1\nλkfk(x1, x2, x3)\n!\n(4.9)\nwhere the bi-gram features are indicator functions as follows:\nf1 = δ(x1 = Tom),\nf2 = δ(x1 = John),\nf3 = δ(x1 = Alice),\nf4 = δ(x1 = Tom, x2 = likes)),\nf5 = δ(x1 = John, x2 = likes),\nf6 = δ(x1 = Alice, x2 = like),\nf7 = δ(x2 = likes, x3 = tea),\nf8 = δ(x2 = like, x3 = tea),\nf9 = δ(x3 = tea).\nMaximum likelihood estimate (MLE) of Eq. (4.9) can be performed by\nthe stochastic approximation (SA) method or the improved iterative\nscaling (IIS) method, as introduced in (Wang et al., 2018). The estimated\nparameters, {λk, k = 1, · · · , 9}, are shown in Figure 4.3. We show the\nparameters over the edges, which represent the corresponding bi-gram\nfeatures. The estimated ELM model can be used to score test data -\n“Alice likes tea” and “Tom like tea”:\nlog P(Alice likes tea) = 2.53 + 5.21 + 5.18 −log Z = −7.06\nlog P(Tom like tea) = 2.88 + 4.12 + 5.18 −log Z = −7.79\nwhere the log normalizing constant, in this simple example, can be\ndirectly calculated as follows:\nlog Z = log\nX\nx1∈{Tom,John,Alice},\nx2∈{likes, like},x3=tea\nexp\n 9\nX\nk=1\nλkfk(x1, x2, x3)\n!\n= 19.98\nInterestingly, according to the ELM modeling, “Alice likes tea” is nat-\nurally scored as being more probable than “Tom like tea”, without\n4.1. CRFS AS CONDITIONAL EBMS\n103\nFigure 4.4: Illustration of exposure bias. y: real, ˆy: predicted.\nrelying on any other ad-hoc tricks. It seems that the ELM model is\nsmoothed more correctly (not disturbed by the noise in the training\ndata) - this is a desirable result. Compare to the ALM modeling, the\nELM modeling abstains from using local normalization. There is no\nimproper local normalization following incorrect histories. In this sense,\nwe could say that the label bias issue is avoided.\nThe exposure bias problem\nLocally-normalized sequence models, whether conditional or uncon-\nditional, suffers from the exposure bias problem. Locally-normalized\nsequence models are usually trained in a manner called teacher forcing\n(Williams and Zipser, 1989). Take the training of a ALM as an example.\n• In training, the model maximizes the likelihood of each successive\ntarget word, conditioned on the gold history of the target word.\nAs shown in Figure 4.4, in training, the model is only exposed\nto real data, which predicts ˆyi given the real data of the history,\ny1, · · · , yi−1.\n104\nConditional EBMs with applications\n• In testing, the model predicts the next step ˆyi, using its own\npredicted words in testing, i.e., ˆy1, · · · , ˆyi−1.\nSuch mismatch between training (teacher forcing) and testing (predic-\ntion) of locally-normalized sequence models has been called the exposure\nbias problem. The model is never exposed to its own errors during\ntraining, and so the inferred histories at test-time do not resemble the\ngold training histories (Wiseman and Rush, 2016).\nRemarkably, exposure bias results from training in a certain way,\nwhich maybe alleviated by some ad-hoc methods such as scheduled\nsampling (Bengio et al., 2015). In contrast, label bias results from\nproperties of the model itself. Thus, it may be more difficult to overcome\nlabel bias than to avoid exposure bias.\n4.1.3\nTraining of CRFs\nTo introduce the training methods for CRFs/conditional EBMs, we\nwrite the model in Definition 4.1 in a simpler form, with input x and\noutput y:\npθ(y|x) =\n1\nZθ(x) exp [Uθ(x, y)]\n(4.10)\nUθ(x, y) : X ×Y →R denotes the (log) potential function, which assigns\na scalar value to each combined configuration of x in X and y in Y, and\ncan be very flexibly parameterized through neural networks of different\narchitectures. X and Y denotes the space of all possible values of x and\ny, respectively. Normalizing is taken only over Y, and Zθ(x) denotes\nthe normalizing constant:\nZθ(x) =\nX\ny∈Y\nexp [Uθ(x, y)]\n(4.11)\nLearning CRFs by conditional maximum likelihood (CML)\nSuppose we have a training dataset consisting of N independent and\nidentically distributed (IID) data points D = {(xi, yi), i = 1, · · · , N}.\nWe can fit pθ(x) to data by maximizing the log conditional likelihood\n4.1. CRFS AS CONDITIONAL EBMS\n105\nof training data, defined by\nL(θ) ≜1\nN\nN\nX\ni=1\nlog pθ(yi|xi) = 1\nN\nN\nX\ni=1\n{Uθ(xi, yi) −log Zθ(xi)}\n(4.12)\nas a function of θ.\nTaking the derivative of the log conditional likelihood with respect\nto θ and making use of Eq. (2.18) about the derivative of the log\nnormalizing constant, we obtain the core formula in learning conditional\nEBMs:\n∇θL(θ) = 1\nN\nN\nX\ni=1\nn\n∇θUθ(xi, yi) −Epθ(y|xi) [∇θUθ(xi, y)]\no\n(4.13)\nThe maximum likelihood estimate of θ is obtained as a solution to\n∇θL(θ) = 0. It can be easily seen that the gradients ∇θL(θ) in Eq. (4.13)\nin learning conditional EBMs exactly follows the form of Eq. (2.31),\nas summarized in Theorem 2.3. So the problem of learning conditional\nEBMs by conditional maximum likelihood (CML) can then be solved by\nsetting the gradients to zeros and applying the SA algorithm to finding\nthe root for the resulting system of simultaneous equations. Techniques\nin learning (unconditional) EBMs are applicable in learning conditional\nEBMs here. See Section 2.3.3 for details. For example:\n• Minibatching from training data when N is large;\n• When calculating the model expectation Epθ(y|xi) [∇θUθ(xi, y)] is\nintractable, we can resort to Monte Carlo methods and conduct\nMonte Carlo averaging.\n• The model expectation can be exactly calculated under some\nlimited circumstances, mostly in low tree-width3 random fields\n(e.g., chain-structured) with moderately sized state spaces4.\n3The tree-width of a graph is defined as the minimum width over all possible\ntree decompositions of the graph, which measures roughly how close the graph is to\na tree. The tree-width of a tree is 1.\n4The state space of a multivariate model is the set of all possible values for each\ncoordinate of a multivariate observation.\n106\nConditional EBMs with applications\nCML training of neural linear-chain CRFs\nHere we provide more details about CML training of a neural linear-\nchain CRF, for which the model expectation can be exactly calculated.\nContinue with the notations in Section 4.1.1, and consider a neural\nlinear-chain CRF as follows, with input x = (x1, · · · xT ) = x1:T , output\ny = (y1, · · · yT ) = y1:T . Combining Eq. (4.3), Eq. (4.5) and Eq. (4.6),\nwe obtain\npθ(y|x) =\n1\nZθ(x) exp [Uθ(x, y)]\nUθ(x, y) =\nT\nX\nt=1\n\u0002ϕyt\nt + Ayt−1,yt\n\u0003\n(4.14)\nwhere the parameters θ consists of the network parameters and the\ntransition matrix A.\nSuppose the state space of y1:T is {1, · · · , K}, i.e., yi ∈{1, · · · , K}.\nThus, at each position t = 1, · · · , T, there are K node potential values,\nϕk\nt , k = 1, · · · , K, which are calculated by a neural network, as defined\nin Eq. (4.5). The network is trained to optimize the log conditional\nlikelihood Eq. (4.12). The gradient of the log conditional likelihood\nw.r.t. θ for a single data point (xi, yi) is:\n∂log pθ(yi|xi)\n∂θ\n= ∂Uθ(xi, yi)\n∂θ\n−Epθ(y|xi)\n\u0014∂Uθ(xi, y)\n∂θ\n\u0015\n(4.15)\nNote that log pθ(yi|xi) depends on the network parameters through\nϕyt\nt . Thus, an important quantity in calculating Eq. (4.15) for the\nnetwork parameters is the gradient w.r.t. to the potential values, which\ncan be derived from Eq. (4.14) as follows:\n∂log pθ(yi|xi)\n∂ϕk\nt\n= δ(yt = k) −Epθ(y|xi) [δ(yt = k)]\n= δ(yt = k) −p(yt = k)\nThis difference between the empirical count and the expected count\nis the error signal received by the NN based feature extractor during\ntraining. The expected count p(yt = k) is often known as the posterior\nstate occupation probability, which can be calculated using the alpha-\nbeta variables from the forward-backward algorithm (Rabiner, 1989).\n4.1. CRFS AS CONDITIONAL EBMS\n107\nThe gradients for the network parameters can be calculated from the\ngradient w.r.t. to the potential values based on the back-propagation\nprocedure.\nLearning CRFs by conditional NCE\nA theoretical analysis of NCE in the estimation of conditional unnor-\nmalized models in the form of Eq. (4.10) has been given in (Ma and\nCollins, 2018). A subtle but important question when generalizing NCE\nto the conditional case is raised, i.e., the unconditional EBM in Eq.\n(2.13) has a single partition function, which is estimated as a parameter\nof the model, whereas the conditional model in Eq. (4.10) has a separate\npartition function Zθ(x) for each value of x.\nIntroduce the unnormalized density ˜pθ(y|x) = exp [Uθ(x, y)] and\nrewrite Eq. (4.10) as follows:\npθ(y|x) =\n1\nZθ(x) ˜pθ(y|x)\n(4.16)\nwhere, for each value of x, the partition function is defined by\nZθ(x) =\nX\ny∈Y\n˜pθ(y|x)\nApplying NCE in estimating conditional unnormalized models yields\nthe conditional NCE method. Suppose we have a training dataset\nconsisting of N independent and identically distributed (IID) data\npoints D = {(xi, yi), i = 1, · · · , N}. The basic idea of NCE is to perform\nnonlinear logistic regression to discriminate between data samples drawn\nfrom the data and noise samples drawn from a known noise distribution\nq(x|y). Formally, referring to Eq. (2.46), conditional NCE estimates the\nmodel parameters θ by maximizing the following objective function:\nJ(θ) = Eκ∼Uni(1,N)\n\u001a\nlog\n˜pθ(yκ|xκ)\n˜pθ(yκ|xκ) + νq(yκ|xκ)\n+ νEy∼q(y|xκ)\n\u0012\nlog\nνq(y|xκ)\n˜pθ(y|xκ) + νq(y|xκ)\n\u0013\u001b\nwhere ν represents the ratio of noise sample size to real sample size.\nIt is shown in (Ma and Collins, 2018) that the conditional NCE\nmethod gives consistent parameter estimates under the assumption that\n108\nConditional EBMs with applications\nZθ(x) is constant with respect to x. Equivalently, the conditional NCE\nmethod is consistent under the assumption that the function ˜pθ(y|x)\nis powerful enough to incorporate Zθ(x). Thus, under the assumption\nthat the model is of sufficient capacity, the model can learn to be self-\nnormalized such that Zθ(x) = 1. Remarkably, the discussion in (Ma and\nCollins, 2018) is in line with Theorem 2.4 about NCE (nonparametric\nestimation), since in the setting of nonparametric estimation, the NCE\nloss is minimized when ˜pθ(y|x) matches the oracle density and thus is\nself-normalized.\nAs a final note, it can be easily seen that training of the Electric\nmodel in Section 3.4 is exactly an instance of the conditional NCE\nmethod.\n4.2\nCRFs for speech recognition\nIn this section, we show the development of CRFs for automatic speech\nrecognition (ASR). First, we introduce some background knowledge\nin ASR, particularly the connectionist temporal classification (CTC)\nmethod for recent end-to-end ASR. Then, we elaborate on the recently\ndeveloped approach of CRF-based single-stage acoustic modeling with\nCTC topology (Xiang and Ou, 2019; An et al., 2020).\nASR basically is a sequence discriminative problem. Given acoustic\nobservations x = (x1, · · · xT ) = x1:T , the task of ASR is to find the most\nlikely labels y = (y1, · · · yL) = y1:L. Acoustic observations are usually\nspectral feature vectors. Different units can be used for labels, such as\nphone (namely monophone), triphone, character, wordpiece and word,\nas shown in Figure 4.5.\n4.2.1\nConnectionist Temporal Classification (CTC)\nThe DNN-HMM hybrid approach\nThe history of speech recognition dates back to 1970s (Jelinek, 1976)\nand even earlier. The classic model for speech recognition is HMMs,\nas we review in Section 2.1.1. In building an HMM for speech recogni-\ntion, consisting of acoustic sequence x = (x1, · · · xT ) = x1:T and state\n4.2. CRFS FOR SPEECH RECOGNITION\n109\nFigure 4.5: Different units of labels can be used in speech recognition.\nsequence π = (π1, · · · πT ) = π1:T , we need to specify two distributions,\nthe state-transition distribution and the state-output distribution.\nThe state sequence π forms a Markov chain. In terminology of\nMarkov chains, state transitions in a Markov chain are determined by\nits state topology, or say, its state transition graph. The state transition\ngraph is determined by combining some knowledge sources, as illustrated\nin Figure 4.6. For example, we need a language model to model how\nwords are connected to form a sentence. We need a lexicon to model\nhow phones are connected to form a word. Depending on left and right\nphonetic contexts, we can define different context-dependent phones,\ne.g., triphones. Each context-dependent phone can be decomposed into\nseveral phonetic states, which intuitively correspond to the beginning,\nsteady, closing phases of a phone. The resulting state transition graph,\noften represented by a weighted finite-state transducer (WFST), encodes\ncommon constraints in human speech5. In practice, each knowledge\nsource can be represented by a component WFST, and the combination\nof several knowledge sources can be easily realized by applying the\ncomposition and some compression operations to the component WFSTs\nand producing a single integrated WFST (Mohri et al., 2008).\nFormally, a WFST represents a weighted relation between sequences\nof input symbols and sequences of output symbols (Mohri et al., 2008).\n5For subtle differences between the two graphs (WFSTs and state transition\ngraphs), readers can refer to (Ou and Xiao, 2010).\n110\nConditional EBMs with applications\nFigure 4.6: State transitions in HMMs for speech recognition are constrained by a\nnumber of knowledge sources.\nA state sequence π for an utterance of T frames can be seen as a path\ntraversing in the integrated WFST for T steps, with π = (π1, · · · πT )\nmatching the input symbols and the emitted output symbols corre-\nsponding to y = (y1, · · · yL). It can be seen that in this way, a path π\nuniquely determines a label sequence y, but not vice versa. Thus, based\non the integrated WFST, a many-to-one mapping, BHMM : π →y, is\ndefined. The state topology basically determines the mapping. We also\nsay that the mapping represents the state topology.\nFor state-output distribution p(xt|πt), Gaussian mixture models\n(GMMs) were commonly used in the so-called GMM-HMM approach,\nbefore the deep learning era. Recently, deep neural networks (DNNs) of\nvarious architectures have become dominantly used for modeling the\nstate-output distributions. Based on the following Bayesian formula,\nthe state likelihood p(xt|πt) can be calculated from the state posteriori\np(πt|xt), divided by the state prior p(πt), while the marginal likelihood\np(xt), being a constant, can be ignored.\np(xt|πt) = p(πt|xt)p(xt)\np(πt)\nState prior probabilities are estimated from the training data. State\nposterior probabilities are calculated from the DNN, but we need frame-\nlevel alignments in training of the DNN.\nIn summary, the approach described above is often referred to as\nthe DNN-HMM hybrid approach for ASR (Dahl et al., 2012). It is\nfeatured by using the frame-level loss (cross-entropy) to train the DNN\nto estimate the posterior probabilities of HMM states.\n4.2. CRFS FOR SPEECH RECOGNITION\n111\nEnd-to-end ASR\nNotably, building DNN-HMM systems for ASR runs in multi-stages. A\nGMM-HMM training is firstly needed to obtain frame-level alignments\nand then the DNN-HMM is trained. The hybrid approach usually\nconsists of an DNN-HMM based acoustic model (AM), a state-tying\ndecision tree for context-dependent phone modeling, a pronunciation\nlexicon and a language model (LM), which can be compactly combined\ninto a weighted finite-state transducer (WFST). The WFST not only\noperationally represents the state topology, but also is useful as a\ncompact data structure for efficient decoding.\nA recent trend in ASR is to develop end-to-end ASR models (Graves\net al., 2006; Miao et al., 2015; Graves, 2012; Chorowski et al., 2014). The\nend-to-end approach is characterized by eliminating the construction\nof GMM-HMMs and phonetic decision-trees, training the DNN from\nscratch (in single-stage) and, even ambitiously, removing the need for a\npronunciation lexicon and training the acoustic and language models\njointly rather than separately. For developing end-to-end ASR models,\nthere are two main issues.\n• The first is how to handle alignment between x and y, since the\ntwo sequences generally differ in length (T ̸= L).\n• The second is how to obtain p(y|x), the posteriori probability of\nthe label sequence y given the acoustic sequence x. To be end-to-\nend, we need a differentiable sequence-level loss of mapping the\nacoustic sequence x to the label sequence y.\nThree widely-used end-to-end models are based on connectionist\ntemporal classification (CTC) (Graves et al., 2006), RNN-transducer\n(RNN-T) (Graves, 2012), and attention based encoder-decoder (AED)\n(Chorowski et al., 2014) respectively. The three models are featured by\ndifferent losses. In CTC and RNN-T, the alignment is handled explicitly\nby introducing hidden state sequence π, while AED implements implicit,\nsoft alignment via the attention mechanism.\nRemarkably, all the three models are (conditional) locally normalized\nsequence models, according to the classification of sequence models\nas shown in Table 4.1. In Section 4.2.2, we will introduce a recently\n112\nConditional EBMs with applications\nFigure 4.7: Overview of CTC architecture.\ndeveloped (conditional) globally normalized sequence model, called\nCTC-CRF, for end-to-end ASR.\nThe CTC method\nThe motivation of CTC (Graves et al., 2006) is to enable the training\nof p(y|x) without requiring the frame-level alignments between the\nacoustics x and the transcripts y. To this end, CTC introduces a blank\nsymbol <b> in addition to the ordinary labels, and further introduces a\nstate sequence π = (π1, · · · πT ) = π1:T , which aids the aligning between\nx1:T and y1:L. See Figure 4.7 for an overview of CTC architecture.\nHandle alignment.\nGiven acoustic sequence x1:T , at each frame t, the\npossible values that πt can freely take is Y ∪<b>, where Y denotes\nthe the alphabet of ordinary labels. Suppose the alphabet size is K,\nthen number of possible values that πt can freely take is K + 1. When\ngiven y1:L, the state transitions followed by π is constrained by the state\ntopology, so that the output sequence is y1:L. CTC defines a special\nstate topology, which is enforced by a special many-to-one mapping\nBCTC : π1:T →y1:L. The mapping BCTC is defined by reducing repetitive\nsymbols in π to a single symbol, and removing all blank symbols, e.g.,\nBCTC(−CC −−AA −T−) = CAT\nIt can be seen that for given acoustic sequence x1:T and label sequence\ny1:L, all possible alignments between them can be organized in a lattice,\n4.2. CRFS FOR SPEECH RECOGNITION\n113\nFigure 4.8: Illustration of the lattice, which contains all the possible alignments\nbetween the acoustic sequence and the label sequence ‘CAT’. Also illustration of\nthe forward-backward algorithm. Black circles represent ordinary labels, and white\ncircles represent blanks. Arrows signify allowed transitions. (Graves et al., 2006)\nas shown in Figure 4.8. A path from the top left to the bottom right in the\nlattice represents an alignment between x and y, which, in terminology\nof CTC (Graves et al., 2006), is denoted by π1:T ∈{Y ∪<b>}T .\nObtain p(y|x).\nWe can use any kind of neural network (NN) to\ncalculate the high-level feature vectors h1:T = (h1, · · · hT ) ∈RD×T\nfrom the raw spectral features x1:T , or simply say, we encode x into\nh. For each frame t, we obtain ht. The vector representations h1:T can\nbe viewed as features, and the neural network is referred as a feature\nextractor as discussed previously in Section 2.1.1, or an acoustic encoder\nin the context of ASR.\nThen, we can apply a linear layer followed by a softmax layer to\ncalculate the posteriori distribution of πt, as follows:\nzt = W T ht + b ∈RK+1\np(πt = k|x) =\nexp(zk\nt )\nPK\nj=1 exp(zj\nt )\n≜pk\nt , k = 1, · · · , K + 1\nwhere W ∈R(K+1)×D, b ∈RK+1 denote the weight matrix and bias\n114\nConditional EBMs with applications\nvector of the linear layer, respectively. pk\nt represents the the probability\nof observing label k at time t. The un-normalized outputs zt are often\ncalled logits, and zk\nt denotes the k-th logit corresponding to label k.\nCTC assumes the conditional independence between states in a path\nπ1:T , and defines the path posteriori as follows:\np(π|x) =\nT\nY\nt=1\np(πt|x)\n(4.17)\nFinally, we use the CTC topology to calculate the posteriori probability\nof the label sequence y, by summing over all possible paths, which map\nto y:\np(y|x) =\nX\nπ:BCTC(π)=y\np(π|x)\n(4.18)\nCTC model training.\nIn the previous introduction of the CTC model,\nwe suppress the model parameters in formula. In the following, we make\nexplicit of the model parameters θ, which parameterizes the acoustic\nencoder network.\nThe network is trained to optimize the log conditional likelihood\nEq. (4.18). According to Fisher equality Eq. (B.8), the gradient of the\nlog conditional likelihood w.r.t. θ for a single data point (x, y) is:\n∂log pθ(y|x)\n∂θ\n= Epθ(π|x,y)\n\u0014∂log pθ(π, y|x)\n∂θ\n\u0015\n= Epθ(π|x,y)\n\u0014∂log pθ(π|x)\n∂θ\n\u0015\n(4.19)\nwhere the second line holds because π deterministically determines y\nwith BCTC.\nNote that log pθ(y|x) depends on the network parameters through\nlogits zk\nt ’s. Thus, an important quantity in calculating Eq. (4.19) for\nthe network parameters is the gradient w.r.t. to the logits, which can\n4.2. CRFS FOR SPEECH RECOGNITION\n115\nbe derived as follows:\n∂log pθ(y|x)\n∂zk\nt\n= Epθ(π|x,y)\n\"\nlog pθ(π|x)\n∂zk\nt\n#\n(∵Fisher equality Eq.(B.8))\n= Epθ(π|x,y)\n\"\nlog pπt\nt\n∂zk\nt\n#\n(∵Eq. (4.17))\n= Epθ(π|x,y)\nh\nδ(πt = k) −pk\nt\ni\n= pθ(πt = k|x, y) −pk\nt\n(4.20)\nThis difference between the posteriori probability and the prior probabil-\nity pk\nt (without observing y) is the error signal received by the NN based\nfeature extractor during training. pθ(πt = k|x, y) is often known as the\nposterior state occupation probability, which can be calculated using\nthe alpha-beta variables from the forward-backward algorithm (Rabiner,\n1989). The gradients for the network parameters can be calculated\nfrom the gradient w.r.t. to the logits based on the back-propagation\nprocedure.\n4.2.2\nCRF-based acoustic modeling with CTC topology\nMotivation\nFrom Eq. (4.17), we see that the CTC model assumes the conditional\nindependence between states in a path. To overcome this drawback, the\nRNN-T model (RNN-transducer) and the CTC-CRF model (CRF with\nCTC topology) have been developed in (Graves, 2012) and (Xiang and\nOu, 2019), respectively.\nA second motivation is that we are interested in bridging the hybrid\nand the end-to-end approaches for ASR, trying to inherit the data-\nefficiency of the hybrid approach and the simplicity of the end-to-\nend approach. Remarkably, when comparing the hybrid and end-to-\nend approaches (modularity versus a single neural network, separate\noptimization versus joint optimization), it is worthwhile to note the\npros and cons of each approach.\n• The end-to-end approach aims to subsume the acoustic, pronun-\nciation, and language models into a single neural network and\n116\nConditional EBMs with applications\nperform joint optimization. This appealing feature comes at a cost,\ni.e. the end-to-end ASR systems are data hungry, which require\nabove thousands of hours of labeled speech to be competitive with\nthe hybrid systems (Lüscher et al., 2019; Chiu et al., 2018; Tüske\net al., 2019).\n• In contrast, the modularity of the hybrid approach permits training\nthe AM and LM independently and on different data sets. A\ndecent acoustic model can be trained with around 100 hours of\nlabeled speech, whereas the LM can be trained on text-only data,\nwhich is available in vast amounts for many languages. In this\nsense, modularity promotes data efficiency. Due to the lack of\nmodularity, it is difficult for an end-to-end model to exploit the\ntext-only data, though there are recent efforts to alleviate this\ndrawback (Toshniwal et al., 2018; Zheng et al., 2022).\nThe CTC-CRF model\nThe CTC-CRF approach consists of separable AM and LM, which meets\nthe principle to be data efficient by keeping necessary modularity. In\nthe following, we mainly describe the CTC-CRF based AM. Different\ntypes of LMs, whether autoregressive LMs or energy-based LMs, can\nbe used with the CTC-CRF based AM. Different schemes are available\nfor decoding, such as WFST based decoding with n-gram LMs, one-\npass decoding with shallow fusion of AM and LM scores, or two-pass\ndecoding with LM rescoring. More details can be found in the toolkit\n(An et al., 2020).\nContinue with the notations in CTC, and note that the core formula\nin establishing the CTC model is Eq. (4.17) and Eq. (4.18), while Eq.\n(4.17) makes the conditional independence assumption. The main idea\nof CTC-CRF is that we can still use the CTC topology to define the\nposteriori probability of the label sequence y, pθ(y|x), from the path\nposteriori pθ(π|x), as defined in Eq. (4.18), but we define the path\nposteriori as a globally normalized sequence model, or say, a condition\nrandom field (CRF), as follows:\npθ(π|x) =\nexp(ϕθ(π, x))\nP\nπ′ exp(ϕθ(π′, x))\n(4.21)\n4.2. CRFS FOR SPEECH RECOGNITION\n117\nHere ϕθ(π, x) denotes the potential function of the CRF, defined as:\nϕθ(π, x) = log p(B(π)) +\nT\nX\nt=1\nlog pθ(πt|x)\n(4.22)\nPT\nt=1 log pθ(πt|x) defines the node potential, which is calculated from\nthe acoustic encoder network with parameters θ. log p(B(π)) defines the\nedge potential, realized by an n-gram LM of labels.\nRemarkably, regular CTC suffers from the conditional independence\nbetween the states in π. In contrast, by incorporating log p(B(π)) into\nthe potential function in CTC-CRF, this drawback is naturally avoided.\nThe difference between the CTC model and the CTC-CRF model can\nbe clearly seen from their graphical model representations, as shown\nin Figure 4.9. Note that the n-gram LM of labels means the transition\nstructure between labels is of (n-1)-th order. The transition structure\nbetween πt’s, when represented by a WFST, is determined by the\ncomposition of two component WFSTs, i.e., the WFST representation\nof the CTC topology and the WFST representation of the n-gram LM\nof labels. For reasons to be clear in the following, the resulting WFST is\nreferred to as the denominator WFST. Due to the composition operation,\nthe order of the transition structure between states (πt’s) is larger than\n(n-1)-th order. Thus, as a reminder for reading Figure 4.9(b), the edge\npotential does not involve exactly n consecutive nodes for a n-gram LM\nof labels. The graphical model representation of CTC-CRF in Figure\n4.9(b) is mainly for concept illustration. In practice, in training of a\nCTC-CRF model, the forward-backward algorithm involving log p(B(π))\ncan be conducted in the denominator WFST (to be detailed in the\nfollowing).\nFinally, note that we may use a n-gram LM of words in decoding. It\nis reminded not to confuse the n-gram LM of labels used in defining the\npotential in CTC-CRF and the n-gram LM of words used in decoding.\nTraining of the CTC-CRF model\nNote that only the parameters from the acoustic encoder network,\ndenoted by θ, need to be trained in the CTC-CRF model, since the\n118\nConditional EBMs with applications\nFigure 4.9: Graphical model representation of the CTC model (a) and the CTC-\nCTF model (b). Note that the edge potential does not involve exactly n consecutive\nnodes for a n-gram LM of labels, as detailed in the text of Section 4.2.2.\nn-gram LM of labels is estimated from the training transcripts and\nfixed in estimating θ.\nThe network is trained to optimize the log conditional likelihood Eq.\n(4.18). Making use of the Fisher equality Eq. (B.8) and the gradient of\nCRF’s log conditional likelihood Eq. (4.13), the gradient of CTC-CRF’s\nlog conditional likelihood w.r.t. θ for a single data point (x, y) is:\n∂log pθ(y|x)\n∂θ\n= Epθ(π|x,y)\n\u0014log pθ(π|x)\n∂θ\n\u0015\n(∵Eq.(B.8), similar to CTC)\n= Epθ(π|x,y)\n\u0014∂ϕθ(π, x)\n∂θ\n−Epθ(π′|x)\n\u0014∂ϕθ(π′, x)\n∂θ\n\u0015\u0015\n= Epθ(π|x,y)\n\u0014∂ϕθ(π, x)\n∂θ\n\u0015\n−Epθ(π′|x)\n\u0014∂ϕθ(π′, x)\n∂θ\n\u0015\n(4.23)\nwhere the second term in the last line does not depend on π and thus\ncan be moved out of the expectation w.r.t. π. Notably, pθ(π′|x) denotes\nthe model distribution, as defined in Eq. (4.21) but with π′ to represent\nthe path. As commonly found in estimating CRFs, the above gradient\nis the difference between empirical expectation and model expectation.\nThe two expectations are similar to the calculations using the numerator\ngraph and denominator graph in LF-MMI respectively (Povey et al.,\n2016).\nRemarkably, Eq. (4.23) can be derived in another way, which reveals\nthe concept of numerator and denominator. Combining Eq. (4.18) and\n4.2. CRFS FOR SPEECH RECOGNITION\n119\nEq. (4.21) yields CTC-CRF’s log conditional likelihood:\nlog pθ(y|x) = log\nP\nπ:BCTC(π)=y exp(ϕθ(π, x))\nP\nπ′ exp(ϕθ(π′, x))\n(4.24)\nIt can be easily seen that the gradient of the above objective function\ninvolves two gradients calculated from the numerator and denominator\nrespectively, which can be shown to be equal to the two terms in Eq.\n(4.23).\nDenote log p(πt = k|x) = ϕk\nt , and note that log pθ(y|x) depends on\nthe network parameters through potential values ϕk\nt , 1 ≤t ≤T, 1 ≤\nk ≤K + 1. Thus, an important quantity in calculating Eq. (4.23) for\nthe network parameters θ is the gradient w.r.t. to the potential values,\nwhich can be derived as follows:\n∂log pθ(y|x)\n∂ϕk\nt\n= Epθ(π|x,y)\n\"\n∂ϕθ(π, x)\n∂ϕk\nt\n#\n−Epθ(π′|x)\n\"\n∂ϕθ(π′, x)\n∂ϕk\nt\n#\n= Epθ(π|x,y) [δ(πt = k)] −Epθ(π′|x)\n\u0002δ(π′\nt = k)\n\u0003\n(4.25)\nThis difference is the error signal received by the NN based feature\nextractor during training. The gradients for the network parameters θ\ncan be calculated from the gradient w.r.t. to the potential values based\non the back-propagation procedure.\nBoth terms in Eq. (4.25) can be obtained via the forward-backward\n(FB) algorithm.\n• Calculating the first term, often referred to the numerator cal-\nculation, amounts to running the FB algorithm over the WFST\ndetermined by y, which is similar to conducting the FB algorithm\nin calculating the first term of Eq. (4.20) in CTC.\n• Calculating the second term, often referred to the denominator\ncalculation, involves running the forward-backward algorithm over\nthe denominator WFST Tden. Tden is an composition of the CTC\ntopology WFST and the WFST representation of the n-gram\nLM of labels. The n-gram LM of labels is thus often called the\ndenominator n-gram LM, to be differentiated from the word-level\nLM in decoding.\n120\nConditional EBMs with applications\nTable 4.2: Comparison of different models for ASR. HMM topology denotes that\nlabels (including silence) are modeled by multiple states with left-to-right transitions,\npossible self-loops and skips. CTC topology denotes the special state transitions used\nin CTC (including blank). Locally/globally normalized denotes the formulation of the\nmodel distribution. In defining the joint distribution of a model, locally normalized\nmodels use conditional probability functions, while globally normalized models use\nun-normalized potential functions. SS-LF-MMI is classified as globally normalized,\nthough it is cast as MMI-based discriminative training of a pseudo HMM and the\nHMM model is locally normalized. AED does not use states to align label sequence\ny and observation sequence x.\nModel\nState\nTraining\nLocally/globally\ntopology\nobjective\nnormalized\nHMM\nHMM\np(x|y)\nlocal\nCTC\nCTC\np(y|x)\nlocal\nSS-LF-MMI\nHMM\np(y|x)\nglobal\nCTC-CRF\nCTC\np(y|x)\nglobal\nRNN-T\nRNN-T\np(y|x)\nlocal\nAED\n-\np(y|x)\nlocal\nRelated work\nIn Table 4.2, we give a brief review of existing models in ASR, de-\npending on state topologies, training objectives and whether the model\ndistribution is locally or globally normalized. We differentiate HMM\ntopology and CTC topology, though the later may be interpreted as\na special HMM topology (Zeyer et al., 2017). The two differ not only\nin the state transition structure but also in the label inventory used\n(which affects not only the definition of the whole state space but also\nthe estimation of the denominator LM).\nFurther, graphical model representations of existing ASR models\nare plotted in Figure 4.10, which clearly show the differences between\nthose models. An ASR model involves an acoustic observations x =\n(x1, · · · xT ) = x1:T and a label sequence y = (y1, · · · yL) = y1:L. HMM,\nCTC and CTC-CRF are defined in Eq. (2.2), Eq. (4.17), and Eq. (4.21),\nrespectively. RNN-transducer (RNN-T) (Graves, 2012) is defined by\np(π1:T+L|x1:T ) =\nT+L\nY\nj=1\np(πj|π1:j−1)\n(4.26)\n4.2. CRFS FOR SPEECH RECOGNITION\n121\nFigure 4.10: Graphical model representations of different ASR models: (a) HMM,\ndefined in Eq. (2.2), (b) CTC, defined in Eq. (4.17), (c) RNN-T, (d) AED, (e)\nCTC-CRF, defined in Eq. (4.21).\nHere π1:T+L = (π1, · · · πT+L) denote the state sequences, or say, the\npath with T blanks and L labels in RNN-T, such that removing the\nblanks in π1:T+L yields y1:L (see Section 4.3.1 for details). Attention\nbased encoder-decoder (AED) (Chorowski et al., 2014) is defined by\np(y1:L|x1:T ) =\nL\nY\ni=1\np(yi|x, y1, · · · , yi−1)\n(4.27)\nIn summary, from Table 4.2 and Figure 4.10, we can cleary see that CTC-\nCRF is fundamentally different from those prior models. For comparison\nbetween CTC-CRF and single-stage (SS) lattice-free maximum-mutual-\ninformation (LF-MMI) (Hadian et al., 2018), readers can refer to (Xiang\nand Ou, 2019).\nRelation to CRF-based acoustic models.\nASR is a sequence transduc-\ntion problem in that the input and output sequences differ in lengths,\nand both lengths are variable. An idea in applying CRFs to ASR is to\nintroduce a (hidden) state sequence π to align the label sequence y and\nobservation sequence x, and define a CRF p(π|x) over the (hidden) state\nsequence π. As shown in Eq. (4.18), deriving p(y|x) based on p(π|x)\ndepends on the mapping between π and y, which is determined by the\nstate topology that allows for different choices, e.g., CTC topology or\n122\nConditional EBMs with applications\nHMM topology. This kind of hidden CRFs was explored in (Gunawar-\ndana et al., 2005) for phone classification, using zero, first and second\norder features. Generally speaking, (hidden) CRFs using neural features\nfor ASR are underappreciated. The CTC-CRF model proposed in (Xi-\nang and Ou, 2019) represents the first exploration of CRFs with CTC\ntopology and advances the CRF-based approach with strong empirical\nresults. Segmental CRFs (Lu et al., 2016) provide another solution to\nthe alignment problem.\nPerformance of the CTC-CRF model\nThe CTC-CRF model inherits the data-efficiency of the hybrid approach\nand the simplicity of the end-to-end approach. CTC-CRF eliminates the\nconditional independence assumption in CTC and performs significantly\nbetter than CTC on a wide range of benchmarks, including WSJ (80-h),\nAISHELL (170-h Chinese), Switchboard (260-h), Librispeech (1000-h),\nand Fisher-Switchboard (2300-h) (the numbers in the parentheses are\nthe size of training data in hours) (Xiang and Ou, 2019; An et al., 2020).\nIt has been also shown (Xiang and Ou, 2019; An et al., 2020) that\nCTC-CRF is on par with other state-of-the-art end-to-end models, like\nRNN-T and AED.\nThe CTC-CRF models have also been used in a variety of tasks in\nASR, which show their great potential.\n• Streaming ASR, particularly the Chunking, Simulating Future\nContext and Decoding (CUSIDE) approach (An et al., 2022);\n• Neural architecture search (Zheng et al., 2021a);\n• Children Speech Recognition (Yu et al., 2021);\n• Modeling with wordpieces and Conformer architectures (Zheng\net al., 2021b);\n• Multilingual and crosslingual speech recognition, particularly the\nJoinAP (Joining of Acoustics and Phonology) approach (Zhu et\nal., 2021).\n4.3. CRFS FOR SEQUENCE LABELING IN NLP\n123\n4.3\nCRFs for sequence labeling in NLP\nConditional random fields (CRFs) have been shown to be one of the most\nsuccessful approaches to sequence labeling. Various linear-chain neural\nCRFs (NCRFs) have been developed, as introduced in Section 4.1.1. The\nnode potential modeling is improved by using NNs, but the linear-chain\nstructure is still kept, i.e., using a bigram table as the edge potential.\nNCRFs represent an extension from conventional CRFs, where both node\npotentials and edge potentials are implemented as linear functions using\ndiscrete indicator features. However, linear-chain NCRFs capture only\nfirst-order6 interactions and neglect higher-order dependencies between\nlabels, which can be potentially useful in real-world sequence labeling\napplications, e.g., as shown in (Zhang et al., 2018) for chunking and\nNER. How can we improve CRFs to capture long-range dependencies\nin the label sequence (preferably non-Markovian)?\nRelated work.\nExtending CRFs to model higher-order interactions\nthan pairwise relationships between labels is an important research\nproblem for sequence labeling. There are some prior studies, e.g. higher-\norder CRFs (Chatzis and Demiris, 2013), semi-Markov CRFs (Sarawagi\nand Cohen, 2004) and latent-dynamic CRFs (Morency et al., 2007),\nbut not using NNs. Using NNs to enhance the modeling of long-range\ndependencies in CRFs is under-appreciated in the literature. A related\nwork is structured prediction energy networks (SPENs) (Belanger and\nMcCallum, 2016), which use neural networks to define energy func-\ntions that potentially can capture long-range dependencies between\nstructured outputs/labels. SPENs depend on relaxing labels from dis-\ncrete to continuous and use gradient descent for test-time inference,\nwhich is time-consuming. Training and inference with SPENs are still\nchallenging, though with progress (Tu and Gimpel, 2018).\nOutside of the globally normalized sequence models, where CRFs\nrepresent a typical class, attention-based encoder-decoder (AED) and\nRNN-T exploit non-Markovian dependences between labels, but both\nare locally normalized sequence models and thus suffer from the label\n6Fixed n-th order can be cast as first-order.\n124\nConditional EBMs with applications\nbias and exposure bias problems, as described in Section 4.1.2. The\nwork in (Wiseman and Rush, 2016) extends AED, by removing the final\nsoftmax in the RNN decoder to learn global sequence scores, but cast as\na non-probabilistic variant of the sequence-to-sequence model. A recent\nwork in (Cui et al., 2021) aims to reducing exposure bias in training\nRNN-T.\nIn this section, we mainly introduce a progress made by CRF trans-\nducers (Hu et al., 2019)7, which introduce a LSTM-RNN to implement a\nnew edge potential so that long-range dependencies in the label sequence\nare captured and modeled in CRFs. So there are two LSTM-RNNs in a\nCRF transducer, one extracting features from observations to define the\nnode potential and the other capturing (theoretically infinite) long-range\ndependencies between labels to define the edge potential. In this view,\na CRF transducer is similar to a RNN-transducer (RNN-T) (Graves,\n2012), which also uses two LSTM-RNNs.\nIn the following, we firstly briefly introduce RNN-T, and then\ndescribe CRF transducer in details. We continue with the notations in\nSection 4.1.1 for sequence labeling. Given a sequence of observations\nx = (x1, · · · xT ) = x1:T , the task of sequence labeling is to predict\na sequence of labels y = (y1, · · · yT ) = y1:T , with one label for one\nobservation in each position. yi ∈{1, · · · , K} denotes the label at\nposition i.\n4.3.1\nRNN-Transducer (RNN-T)\nRNN Transducers (RNN-T) are originally developed for general sequence-\nto-sequence learning (Graves, 2012), which do not assume that the input\nand output sequences are of equal lengths and aligned, e.g., in speech\nrecognition. In the following, we introduce RNN transducers in a simple\nform for applications in sequence labeling, i.e., for the aligned setting -\none label for one observation at each position. To this end, we define\np(y|x) =\nT\nY\ni=1\np(yi|y0:i−1, x)\n(4.28)\n7Reproducible code is at https://github.com/thu-spmi/SPMISeq\n4.3. CRFS FOR SEQUENCE LABELING IN NLP\n125\nTable 4.3: Model comparison and connection.\nModel\nGlobally\nLong-range dependencies\nnormalized\nbetween labels\nLinear-chain CRF\n√\n×\nRNN Transducer\n×\n√\nCRF transducer\n√\n√\nand implement p(yi|y0:i−1, x) through two networks - transcription\nnetwork F and prediction network G as follows:\np(yi = k|y0:i−1, x) =\nexp(fk\ni + gk\ni )\nPK\nk′=1 exp(fk′\ni + gk′\ni )\n(4.29)\nHere F scans the observation sequence x and outputs the transcription\nvector sequence f = (f1, · · · fT ) = f1:T . G scans the label sequence\ny0:T−1 and outputs the prediction vector sequence g = (g1, · · · gT ) =\ng1:T . y0 denotes the beginning symbol (<bos>) of the label sequence.\nFor a sequence labeling task with K possible labels, fi and gi are K\ndimensional vectors. Superscript k is used to denote the k-th element of\nthe vectors. Remarkably, the prediction network G can be viewed as a\nlanguage model of labels, capable of modeling long-range dependencies in\ny, which is exactly the motivation to introducing G in RNN transducers.\nTo ease comparison, we will also refer to the network below the\nCRF layer in linear-chain NCRFs as a transcription network, which also\nimplement ϕt(yt = k, x) as fk\nt .\n4.3.2\nFrom RNN-T to CRF transducer\nIn the following, we introduce CRF transducers, which combine the\nadvantages of linear-chain NCRFs (globally normalized, using LSTM-\nRNNs to implement node potentials) and of RNN transducers (capable\nof capturing long-range dependencies in labels), and meanwhile overcome\ntheir drawbacks, as illustrated in Table 4.3. Further, graphical model\nrepresentations of different models are plotted in Figure 4.11, which\nclearly show the differences between those models.\n126\nConditional EBMs with applications\nFigure 4.11: Graphical model representations of (a) a linear-chain CRF, (b) a\nRNN-T for the aligned setting, and (c) a CRF transducer. Notably, the graphical\nrepresentation of the RNN-T for the aligned setting, as defined in Eq. (4.28), is\ndifferent from that of the usual RNN-T as shown in Figure 4.10(c).\nModel definition.\nA CRF transducer defines a globally normalized,\nconditional distribution p(y|x; θ) as follows:\np(y|x; θ) = exp {u(y, x; θ)}\nZ(x; θ)\n.\nwhere Z(x; θ) = P\ny′∈DT exp {u(y′, x; θ)} is the global normalizing term\nand DT is the set of allowed label sequences of length T. The total\npotential u(y, x; θ) is decomposed as follows:\nu(y, x; θ) =\nT\nX\ni=1\n{ϕi(yi, x; θ) + ψi(y0:i−1, yi; θ)} .\nwhere ϕi(yi, x; θ) is the node potential at position i, ψi(y0:i−1, yi; θ) is\nthe clique potential involving labels from the beginning up to position\ni. Thus the underlying undirected graph for the label sequence y is\nfully-connected, which potentially can capture long-range dependencies\nfrom the beginning up to each current position.\nNeural network architectures.\nLike in RNN transducers, we introduce\ntwo networks in CRF transducers, as shown in Figure ??. The tran-\nscription network F implements the node potential ϕi(yi, x; θ), which\nrepresents the score for yi based on observations x. In the experiments\non NLP sequence labeling, each word xi is represented by a concatena-\ntion of a pre-trained word embedding vector and another embedding\nvector obtained from a character-level CNN (Hu et al., 2019). The\ntranscription network F is a bidirectional RNN (Rf) that scans the se-\nquence of the concatenated vectors for words to generate hidden vectors\n4.3. CRFS FOR SEQUENCE LABELING IN NLP\n127\n𝒈𝟏\n𝒉𝟏\n𝒇\n𝒉𝟐\n𝒇\n𝒉𝒏\n𝒇\nObservations\nBi-RNN (𝑹𝒇)\n𝒉𝟏\n𝒈\n𝒉𝟐\n𝒈\n𝒉𝒏\n𝒈\nLabels\nRNN (𝑹𝒈)\nPrediction \nnetwork (𝑮)\nTranscription \nnetwork (𝑭)\n𝒚𝒏\n𝒚𝟏\n𝒚𝒏−𝟏\n𝒚𝟎\n𝒈𝟏\n𝒇𝟏\n𝒈𝟐\n𝒈𝒏\n𝒇𝟐\n𝒇𝒏\nPotentials\n𝒙1\n𝒙𝟐\n𝒙𝒏\nFigure 4.12: The architecture of a CRF transducer.\nhf\ni = [\n−→\nhf\ni ;\n←−\nhf\ni ], which are then fed to a linear layer with output size of\nK to generate fi ∈RK.\nThe prediction network G implements the clique potential ψi(y0:i−1, yi; θ),\nwhich represents the score for yi by taking account of dependencies\nbetween yi and previous labels y0:i−1. In the experiments, each label\nyi is represented by a label embedding vector, initialized randomly. G\nis a unidirectional RNN (Rg) that accepts the label sequence y and\ngenerates hidden vectors hg\ni =\n−→\nhg\ni , which are then fed to a linear layer\nwith output size of K to generate gi ∈RK.\nIt can be seen from above that a CRF transducer is similar to a RNN\ntransducer. The difference is that a RNN transducer is locally normalized\nthrough softmax calculations as shown in Eq. (4.29), while a CRF\ntransducer is globally normalized, locally producing (un-normalized)\npotential scores.\nPotential design.\nBased on fi and gi, there are two possible designs\nto implement the potentials ϕi and ψi, which can be chosen empirically\nin the experiments. The first design is:\nϕi(yi = k, x; θ) = fk\ni\nψi(y0:i−1, yi = k; θ) = gk\ni\n(4.30)\n128\nConditional EBMs with applications\nThe second design is:\nϕi(yi = k, x; θ) = log\nexp(fk\ni )\nPK\nk′=1 exp(fk′\ni )\nψi(y0:i−1, yi = k; θ) = log\nexp(gk\ni )\nPK\nk′=1 exp(gk′\ni )\n(4.31)\nDecoding and training.\nCRF transducers break the first-order Markov\nassumption in the label sequence as in linear-chain NCRFs and thus do\nnot admit dynamic programming for decoding. Instead, beam search\ncan be used to approximately find the most probable label sequence:\nˆy = arg max\ny′∈DT\np(y′|x; θ) = arg max\ny′∈DT\nu(y′, x; θ).\nTraining data consists of inputs x paired with oracle label sequences\ny∗. Stochastic gradient descent (SGD) on the negative log-likelihood of\nthe training data is conducted:\nL(y∗; θ) = −u(y∗, x; θ) + log Z(x; θ).\nIt is easy to calculate the gradient of the first term. However, the\ngradient of the log normalizing term involves model expectation:\n∇θ log Z(x; θ) = Ep(y′|x;θ)\n\u0002∇θu(y′, x; θ)\n\u0003\nThe calculations of the normalizing term and the model expectation\ncan be exactly performed for linear-chain NCRFs (via the forward and\nbackward algorithm), but are intractable for CRF transducers. It is\nempirically found in the experiments (Hu et al., 2019) that the method\nof beam search with early updates (Collins and Roark, 2004) marginally\noutperforms Monte Carlo based methods for training CRF transducers.\nThe basic idea is that we run beam search and approximate the\nnormalizing term by summing over the paths in the beam. Early updates\nrefer to that as the training sequence is being decoded, we keep track\nof the location of the oracle path in the beam; If the oracle path falls\nout of the beam at step j, a stochastic gradient step is taken on the\nfollowing objective:\nL(y∗\n1:j; θ) = −u(y∗\n1:j; θ) + log\nX\ny′∈Bj\nexp\nn\nu(y′\n1:j; θ)\no\n4.3. CRFS FOR SEQUENCE LABELING IN NLP\n129\nwhere u(y1:j; θ) = Pj\ni=1 {ϕi(yi, x; θ) + ψi(y0:i−1, yi; θ)} denotes the par-\ntial potential (with abuse of the notation of u). The set Bj contains all\npaths in the beam at step j, together with the oracle path prefix y∗\n1:j.\nPerformance of CRF transducers.\nDifferent sequence labeling meth-\nods are evaluated over POS tagging, chunking and NER (English, Dutch)\nin (Hu et al., 2019). Experiment results show that CRF transducers\nachieve consistent improvements over linear-chain NCRFs and RNN\ntransducers across all the four tasks, and can improve state-of-the-art\nresults.\n130\nConditional EBMs with applications\n4.4\nEBMs for conditional text generation\n4.4.1\nResidual energy-based models\nMotivation\nText generation is ubiquitous in many NLP tasks, from summariza-\ntion, to dialogue and machine translation. In this section, we will\nfurther introduce residual energy-based language models, as briefly\ncovered in Section 3.3.3, and show their application in (conditional)\ntext generation. The dominant approach to text generation is based on\nautoregressive language models (ALMs), especially recent large ALMs\nparameterized by large neural networks (Radford et al., 2018), which\nare locally-normalized. Unfortunately, local normalization also brings\nsome drawbacks, as described in (Deng et al., 2020) and listed below.\nFirst, the designer of the model needs to specify the order in which\ntokens are generated. Second, at training time the model is conditioned\non ground truth context while at test time it is conditioned on its own\ngenerations, a discrepancy referred to as exposure bias (Section 4.1.2).\nFinally, while heuristics like beam search somewhat help rescore at the\nsequence level, generation generally lacks long-range coherency because\nit is produced by the greedy selection of one token at a time without\nlookahead8.\nIn principle, EBMs potentially address all these issues, as they do not\nrequire any local normalization. EBMs are not prone to exposure bias\nand label bias, and they can score the whole input at once. EBMs may\nenable generation of large chunks of text, which should help improve\ncoherency. However, a difficulty of applying EBMs in text generation is\nthat sampling from EBMs is intractable, and so is maximum likelihood\ntraining. A recent work in (Deng et al., 2020) develops residual EBMs\nfor text generation and shows impressive results, which will be detailed\nbelow.\n8This drawback of generation without lookahead is related to the label bias\nproblem of locally-normalized sequence models, namely being weak in revising earlier\ndecisions (Section 4.1.2).\n4.4. EBMS FOR CONDITIONAL TEXT GENERATION\n131\nThe residual EBM model\nThe formulation of residual EBMs, as introduced in Section 3.3.3,\nhas multi-fold benefits for text generation (Deng et al., 2020). First,\nby incorporating an autoregressive language model in the residual\nformulation, we can leverage recent advancements in autoregressive\nlanguage modeling. Second, the autoregressive language model provides\na natural proposal distribution for training of residual EBMs, and the\ntraining can be made efficient by using the conditional noise contrastive\nestimation objective (Section 4.1.3). Lastly, this formulation enables\nefficient evaluation and generation via importance sampling, as we\nshall detail in the following. In some sense, this last point is perhaps\nthe central contribution of (Deng et al., 2020), as it allows estimating\nperplexity of the residual EBM, and thus allows these EBMs to be\ncompared in a standard way to other models.\nDeng et al., 2020 investigates an EBM trained on the residual of a\npre-trained autoregressive LM, particularly for conditional generation of\ndiscrete sequences. Given a conditioning prefix x1, · · · , xc with xj ∈V\nwhere V is the vocabulary, the probability of generating a sequence of\ntotal length T > c is defined as follows:\npθ(xc+1, · · · , xT |x1, · · · , xc)\n=pLM(xc+1, · · · , xT |x1, · · · , xc) exp(−Eθ(x1, · · · , xc, xc+1, · · · , xT ))\nZθ(x1, · · · , xc)\n= ˜pθ(xc+1, · · · , xT |x1, · · · , xc)\nZθ(x1, · · · , xc)\n(4.32)\nwhere pLM denotes the pre-trained autoregressive LM and is fixed\nthroughout training, Eθ(·) is the residual energy function parameterized\nby θ. In the experiment of (Deng et al., 2020), Eθ is initialized with\nBERT/RoBERTa; in the final layer the mean-pooled hidden states\nare projected to a scalar energy value. pθ is called the joint model.\nZθ(x1, · · · , xc) is the normalizing factor known as partition function. ˜pθ\ndenotes the un-normalized probability.\n132\nConditional EBMs with applications\nTraining of the residual EBM\nThe conditional EBMs defined in Eq. (4.32) can be trained using NCE\n(Section 2.4), and more specifically its conditional version (Section\n4.1.3). For the sake of reducing clutter in the notation, we will drop\nthe conditioning variables x1, · · · , xc and use x to denote a target token\nsequence (namely xc+1, · · · , xT ) in the following discussion. Denote the\ntraining dataset by D.\nNCE requires two distributions: the model distribution and a noise\ndistribution. Here the model distribution is the joint model of Eq. (4.32),\npθ, while the noise distribution is the pre-trained LM, pLM. NCE then\ntrains a binary classifier on the difference of log-probability scores of\nthese two models. Since the joint model is the product of the energy\nfunction (whose parameters we want to learn) with the pre-trained\nLM, the difference reduces to: log ˜pθ −log pLM = −Eθ. Therefore, under\nthese modeling assumptions of residual learning and noise model, the\nNCE objective function becomes:\nJθ =\nE\nx+∼D log\n˜pθ(x+)\n˜pθ(x+) + pLM(x+) +\nE\nx−∼pLM log\npLM(x−)\n˜pθ(x−) + pLM(x−)\n=\nE\nx+∼D log\n1\n1 + exp(Eθ(x+)) +\nE\nx−∼pLM log\n1\n1 + exp(−Eθ(x−))\n(4.33)\nwhere x+ denotes the positive sequence taken from the human generated\ntraining set, and x−the negative sequence drawn from the pre-trained\nLM (for a given ground truth prefix). Again, we see that NCE training of\nthe energy function reduces to training a binary classifier to discriminate\nbetween real text and text generated by a pre-trained autoregressive\nlanguage model. The experiment of (Deng et al., 2020) uses a prefix of\nsize 120 tokens and generates the following 40 tokens; with the notation\nof Eq. (4.32), c = 120 and T = 160. For NCE training, for efficiency 16\nsamples per prefix for CC-News (Bakhtin et al., 2019) were generated\noffline, and sampled uniformly from those samples at training time.\n4.4. EBMS FOR CONDITIONAL TEXT GENERATION\n133\nAlgorithm 10 Top-k sampling for the residual EBM\nInput: Number of samples n drawn from pLM, value of k in top-k\n// Get a set of samples from pLM\nSample n samples {x1, ..., xn} from pLM with top-k sampling\nCalculate energies si = Eθ(xi) for each xi ∈{x1, ..., xn}\n// Resample from the set of LM samples\nSample x = xi with probability\nexp(−si)\nPn\nj=1 exp(−sj)\nReturn: x\nGeneration from the residual EBM\nIn order to generate from the residual EBM model Eq. (4.32) efficiently,\nDeng et al., 2020 uses self-normalized importance sampling (SNIS)\n(Section 2.3.2). Under the assumptions that the model from which we\nwish to draw samples is the joint model, which is the product of the\nautoregressive model and the energy function, and that the proposal\ndistribution is the autoregressive model itself, sampling proceeds simply\nby: a) sampling from the autoregressive language model, followed by b)\nresampling according to the energy function. The algorithm is shown\nin Algorithm 10, where a top-k constraint on the pre-trained language\nmodel is introduced to improve the quality of samples in the set. Without\nthe top-k constraint, as the number of samples goes to infinity, we would\nrecover exact samples from the joint model distribution.\nEvaluation of the residual EBM\nA commonly used protocol for evaluating generative sequence models,\nespecially language models, is perplexity (PPL), which is equal to\nPPL = 2−\n1\nT −c\nPT\ni=c+1 log2 pθ(xi|x1,··· ,xi−1)\nPPL can be interpreted as the average number of tokens the model\nis uncertain of at every time step. For the residual EBM model, the\nlog-likelihood required by PPL relies on estimating the partition function\nZθ =\nX\nx\npLM(x) exp(−Eθ(x)) = Ex∼pLM exp(−Eθ(x)).\n134\nConditional EBMs with applications\nBased on (Nowozin, 2018), two estimators are derived in (Deng et\nal., 2020) for the lower and upper bounds of the partition function\nrespectively, as shown in the following theorem.\nTheorem 4.1. Denote Tn as the empirical estimate of log Ex∼pLM exp(−Eθ(x))\nwith n samples xi ∼PLM, i = 1, · · · , n: Tn = log 1\nn\nPn\ni=1 exp(−E(xi)),\nthen ∀ϵ > 0, ∃N > 0 such that ∀n > N we have\nZθ −ϵ < E[Tn] < Zθ < E[(2n −1)Tn −2(n −1)Tn−1] < Zθ + ϵ. (4.34)\nSimilarly to locally normalized models, we can also factorize the\nprobabilities of an entire sequence step by step, pθ(x) = QT\nt=1 pθ(xt|x<t).\nBy marginalizing over the future, the following step-wise probabilities\nare derived in (Deng et al., 2020):\npθ(xt|x<t) = pLM(xt|x<t)\nEx′\nt+1:T ∼pLM(·|x≤t)[exp(−Eθ(x≤t, x′\nt+1:T ))]\nEx′\nt:T ∼pLM(·|x≤t−1)[exp(−Eθ(x≤t−1, x′\nt:T ))]\n(4.35)\nNote that both the numerator and the denominator in Eq. (4.35) take\nthe same form as the partition function, we can also use Eq. (4.34) to\nestimate the upper and lower bounds. For example, the lower bound of\nlog pθ(xt|x<t) can be obtained by using the lower bound of the numerator\nand the upper bound of the denominator. Remarkably, for t = T, we\ncan calculate the log probability by exhaustive enumeration. This gives\nus an idea of the true performance of the model at the last step, and it\nalso provides a sanity-check of the tightness of the estimators.\nPerformance of the residual EBM\nIn (Deng et al., 2020), experiments on two datasets, CC-News (Bakhtin\net al., 2019) and the Toronto Book Corpus (Zhu et al., 2015) show\nthat residual EBMs demonstrated improved generation ability against\nstrong autoregressive baselines, both in terms of estimated perplexity\nand through human evaluation.\n4.4. EBMS FOR CONDITIONAL TEXT GENERATION\n135\n4.4.2\nControlled text generation from pre-trained language models\nMotivation\nWhile large transformer-based autoregressive language models trained\non massive amounts of data exhibit exceptional capabilities to generate\nnatural language text, effective methods for generating text that sat-\nisfy global constraints and possess holistic desired attributes remains\nan active area of research (Khalifa et al., 2021; Mireshghallah et al.,\n2022). For instance, we may want to avoid toxic content; or steer gen-\nerations towards a certain topic or style. Much of the prior work has\napproached controlled generation via either training domain-conditioned\nneural language models or finetuning/modifying an underlying large\npre-trained base model for attribute sensitive generation (see references\nin Mireshghallah et al., 2022). Not only do these approaches involve\ncomputational overhead and estimation errors associated with the train-\ning of language models, but they are also dependent on access to a large\namount of attribute-specific language data which can be impractical in\nmany scenarios and exacerbate privacy concerns.\nIn order to address these limitations, the study in (Mireshghallah\net al., 2022) thus aims to eschew training and focuses on generation-time\ncontrol from pre-trained modules. The mix-and-match method proposed\nin (Mireshghallah et al., 2022), as shown in Figure 4.13, draws samples\nfrom a test-time combination of pre-trained black-box experts that each\nscores a desired property of output text - for example, fluency, attribute\nsensitivity, or faithfulness to the context. Specifically, the product of\nthese black-box experts is viewed as a EBM, and sampling is performed\n(without further training or fine-tuning), using a specialized Gibbs\nsampler with a Metropolis-Hastings (MH) correction step (Goyal et al.,\n2022), which is basically MH within Gibbs sampling, as introduced in\nSection 2.3.1.\nA related work in (Khalifa et al., 2021) proposes a distributional\napproach for addressing controlled text generation from pre-trained\nlanguage models (PLMs). Both “pointwise” constraints (hard require-\nments on each individual) and “distributional” constraints (collective\nstatistical requirements such as moment constraints) are permitted to\nadded in the target LM, while minimizing KL divergence from the\n136\nConditional EBMs with applications\ninitial pre-trained LM distribution. The optimal target distribution is\nalso uniquely determined as a residual EBM model and can be trained\nthrough a variant of policy gradient based on importance sampling.\nAlso for controlled text generation, the work in (Qin et al., 2022)\nis motivated by the need to enrich decoding algorithms that can work\ndirectly with pre-trained language models (PLMs) without task-specific\nfine-tuning, and support complex combinations of hard and soft con-\nstraints to control the generated text on the fly. Previous studies use\nMH within Gibbs sampling or SNIS, as surveyed in Table 2.1. A new\ndecoding method, called Constrained decoding with Langevin dynamics\n(COLD), is developed in (Qin et al., 2022), which introduces Langevin\ndynamics to text-based EBMs for efficient gradient-based sampling.\nSpecifically, the COLD based text generation performs sampling by\niteratively updating a continuous relaxation of text using gradients of\nthe energy function. The resulting continuous text samples are then\nmapped back to the discrete space with a simple guided discretiza-\ntion approach, yielding text sequences that are fluent and adhere to\nthe constraints. Experiments are conducted in three text generation\napplications - lexically-constrained generation, abductive reasoning,\nand counterfactual reasoning and show the effectiveness of the COLD\napproach, both in terms of automatic and human evaluation.\nThe mix-and-match language model\nIn (Mireshghallah et al., 2022), the problem of performing controlled\ngeneration is framed as a problem of sampling from a specialized energy\nbased (or globally normalized) sequence model that defines a probability\ndistribution that satisfies the desired constraints we wish to impose in\nthe controlled generation setting.\nAs described below, this energy based model is composed of pre-\ntrained components and does not require any further optimization.\nSpecifically, an energy-based sequence model defines the probability\ndistribution over the space of possible sequences X as:\npθ(X) =\ne−Eθ(X)\nP\nX′∈X e−Eθ(X′)\n4.4. EBMS FOR CONDITIONAL TEXT GENERATION\n137\nFigure 4.13: Overview of mix-and-match LM. The Lego pieces show different\nexperts that can be used to form the energy LM and help control different features\nin the generated text. The right side shows the i-th step in the the Gibbs sampling\nchain, where a proposal is made by the MLM, and then it is accepted/rejected based\non the energy score. (Mireshghallah et al., 2022)\nwhere Eθ(X) refers to the scalar energy of a sequence X that is\nparametrized by θ. Lower energy corresponds to the higher likelihood\nof X. In contrast to the common autoregressive sequence models, ex-\nact likelihood computation and efficient sampling from EBM models\nis challenging. Despite these challenges, the EBM paradigm offers in-\ncreased flexibility via sequence-level features and constraints. As we\ndiscuss next, this capability lets us easily define expressive functions for\ncontrolled generation of sequences which is not readily offered by the\nautoregressive modeling paradigm.\nNote that the task of controlled generation requires concentrating\nprobability mass over a small subspace of sequences in X that satisfies\nvarious constraints pertaining to fluency, target attributes, and other\ncontrol variables. The EBM based mix-and-match language model, as\ndefined below, involves a linear combination of various black-box experts\nin order to obtain a distribution whose samples satisfy the requirements\nof a desired controlled generation task:\nEM&M(X) =\nk\nX\ni=1\nαiEi(X)\n(4.36)\nwhere the mix-and-match (M&M) energy is composed of k expert\nenergy components, which are weighted by scalar hyperparameters α.\n138\nConditional EBMs with applications\nThe following black-box experts are used in (Mireshghallah et al., 2022):\n• Emlm(X): Masked language models (MLMs) like BERT (Devlin\net al., 2018) are used as a black-box to model the form and fluency\nof sentences. Particularly, Emlm(X) is defined as the negative\nof the sum of unnormalized logits iteratively computed at each\nposition obtained via the forward pass of the MLM after masking\nthe corresponding position (Goyal et al., 2022).\n• Edisc(X): This particular expert refers to the energy obtained\nvia the discriminator for the attributes of interest. What this\nmodule returns is the raw logits of the discriminator, for the target\nattribute. For instance, if we have a sentiment classifier, and want\nto produce positive sentiment, then Edisc(X) = −log p(+|X).\n• Ehamm(X, X′): For a given sequence X′, this quantity refers to the\nhamming distance between the sequence X and X′. This penalizes\ntoken level deviation from X′ which is useful if we are interested\nin only making minor edits to X′.\n• Efuzzy(X, X′): Similar to the hamming distance, this quantity\nrefers to the BertScore (Zhang et al., 2020a) computed between\nX and X′ which can be viewed as a fuzzy hamming distance that\ntakes semantic similarity into account.\nSampling from the mix-and-match model\nIn (Mireshghallah et al., 2022), Metropolis-Hastings (MH) based MCMC\nscheme (Section 2.3.1) is used to sample from the M&M model. The\nproposal distribution is implemented by a masked language model\n(MLM) like BERT. At each MH step, we mask out a token at a random\nposition i in current sequence X, propose a new token by sampling from\nthe MLM conditional softmax at the masked position9, and obtain the\nnew sequence X. The new sequence is accepted with the probability\nmin\n(\n1, exp(−EM&M(X))pmlm(Xi|X\\i)\nexp(−EM&M(X))pmlm(Xi|X\\i)\n)\n9Note that the proposed move Xi is generated independent of the previous state\nXi, thus the sampling algorithm used here is in fact MIS with Gibbs sampling.\n4.4. EBMS FOR CONDITIONAL TEXT GENERATION\n139\nIn experiments in (Mireshghallah et al., 2022), a MCMC chain is run\nfor sentence generation for more than 8 epochs, where an epoch refers\nto one masking cycle over all positions of the sequence.\nThe performance of the mix-and-match model\nTwo kinds of controlled generation tasks are performed in (Mireshghallah\net al., 2022).\nPrompted generation.\nThis task focuses on generating well-formed\nsentences that start with a specified prompt and also satisfy a target\nattribute for which we have access to a discriminator. An example task\nwould be to generate positive sentiment sequences starting with This\nmovie. The energy function takes the form:\nE(X) = Emlm(X) + αEdisc(X)\nwhere α is a hyperparameter that controls the tradeoff between the\nMLM score and the discriminator’s influence.\nControlled text revision.\nThis task aims to edit a source sequence\nX′ to generate sequence X to satisfy the desired target attributes. The\nenergy function for this task is :\nE(X) = Emlm(X) + αEdisc(X) + βEhamm(X, X′) + γEfuzzy(X, X′)\nThis energy function, in addition to valuing well-formedness and satis-\nfying target attribute requirements, also focuses on maintaining faith-\nfulness to the source sequence X′.\nIn (Mireshghallah et al., 2022), the effectiveness of the mix-and-\nmatch approach has been shown on controlled generation tasks (with\nsentiment or topic) and style-based text revision tasks (controllable\ndebiasing, style transfer), by outperforming recently proposed methods\nthat involve extra training, fine-tuning, or restrictive assumptions over\nthe form of models.\n140\nConditional EBMs with applications\nConstrained decoding with Langevin dynamics (COLD)\nConstrained text generation aims to produce text samples y that satisfy\na set of constraints (usually conditioned on an input x omitted for\nbrevity). Let y = (y1, · · · , yT ) denote a discrete sequence where each yt\nis a token from a vocabulary V. Assume each constraint can be captured\nby a constraint function fi(y) ∈R, where higher values of fi mean that\nthe text y better satisfies the constraint. Generating text under the\nconstraints can be seen as sampling from a energy-based distribution\ny ∼p(y):\np(y) ∝exp {U(y)} , U(y) =\nX\ni\nλifi(y)\nEBMs are flexible - any differentiable function that outputs a goodness\nscore of text can be used as a constraint function, as long as it reflects the\nrequirements of the target task. Three example constraints are shown\nin (Qin et al., 2022): soft fluency constraint, future-token prediction\nconstraint, and N-gram similarity constraint.\nFor efficient sampling from p(y), we want to use Langevin dynamics,\nwhich makes use of the gradient ∇y log p(y) = ∇yU(y). However, in\ntext generation, y is a discrete sequence and the gradient ∇y log p(y) is\nnot well-defined. To address this problem, (Qin et al., 2022) proposed\na new sampling/decoding method, called Constrained decoding with\nLangevin dynamics (COLD), which consists of three steps - continuous\nrelaxation of text, performing Langevin dynamics with an energy defined\non a sequence of continuous “soft” token vectors, and finally, guided\ndiscretization.\nContinuous relaxation of text.\nInstead of defining the energy function\non discrete tokens, the energy function is defined on a sequence of\ncontinuous vectors ˜y = (˜y1, · · · , ˜yT ), which is called a soft sequence.\nEach position in the soft sequence is a vector of logits ˜yt ∈R|V|, each\nelement corresponding to the logit of a word in the vocabulary. Taking\nthe softmax of ˜yt yields a distribution over the vocabulary for position\nt. An EBM model can be defined on the soft sequence ˜y as follows:\np(˜y) ∝exp {U(˜y)}\n(4.37)\n4.4. EBMS FOR CONDITIONAL TEXT GENERATION\n141\nLangevin dynamics over soft tokens.\nIn the continuous space, we\ncan perform gradient guided MCMC (Section 2.3.1) such as Langevin\ndynamics to generate samples ˜y(1), ˜y(2), · · · , which ends up generating\nsamples from the distribution induced by the energy function Eq. (4.37).\nFrom soft tokens to discrete text.\nAfter receiving a soft sequence\nsample ˜y from running Langevin dynamics, we map the soft sequence\nto a discrete text sequence, which we consider as the output of COLD\ndecoding. A simple method would be selecting the most-likely token at\neach position t,\nyt = arg max\nv∈V\n˜yt(v)\nHowever, the resulting text suffers from fluency issues even if the soft\nfluency constraint is used, due to competing constraints that sacrifice\nfluency. To overcome this, COLD uses the underlying pre-trained lan-\nguage model (PLM) as a “guardian” for obtaining the discrete sequence.\nSpecifically, at each position t, we first use the PLM to produce the\ntop-k most-likely candidate tokens based on its generation distribution\nconditioning on preceding tokens, which we denote as Vk\nt . We then\nselect from the top-k candidates the most likely token based on the soft\nsample ˜y:\nyt = arg max\nv∈Vk\nt\n˜yt(v)\nWe refer to this method as “top-k filtering”. The resulting text tends to\nbe fluent because each token is among the top-k most probable tokens\nfrom the PLM.\n5\nJoint EBMs with applications\nIn this chapter, we introduce EBMs for modeling joint distributions for\nboth fixed-dimensional and sequential data, with the applications for\nsemi-supervised learning, training more calibrated models, and improved\nlanguage modeling with additional relevant linguistic tags (e.g., part-of-\nspeech tags). First, we present basics for semi-supervised learning. Then,\nwe introduce the fixed-dimensional case of joint EBMs (Section 5.2),\nthen move on to the sequential case (Section 5.3). Finally, we present the\napplication of EBM-based joint modeling for semi-supervised learning\nand calibrated natural language understanding in Section 5.4 and Section\n5.5, respectively.\n5.1\nBasics for semi-supervised learning\nAs we have witnessed, supervised learning from large amounts of la-\nbeled data, particularly with deep neural networks (DNNs), has achieved\ntremendous success in various intelligence tasks, spanning speech process-\ning, computer vision, and natural language processing (NLP). However,\ncollecting labeled data is difficult and expensive, but there are often\neasily-available unlabeled data. This has motivated the community to\ndevelop semi-supervised learning (SSL). SSL aims to leverage both la-\n142\n5.1. BASICS FOR SEMI-SUPERVISED LEARNING\n143\nbeled and unlabeled data to train a conditional model for inference (for\neither discriminative or generation tasks), which, basically, is to find the\nposteriori of label y given observation x. A plethora of semi-supervised\nlearning methods have emerged to train deep neural networks (DNNs)\n(Miyato et al., 2018; Laine and Aila, 2017; Tarvainen and Valpola, 2017;\nSohn et al., 2020; Chen et al., 2020; Clark et al., 2018), spanning over\nvarious domains such as image classification, natural language labeling\nand so on. People may wonder how unlabeled observations x’s may\nhelp finding the posterior. A common belief is that the the information\ncontained in the unlabeled observations can provide some kind of priors,\nor alternatively say, some regularizations or inductive biases, for finding\nthe posterior p(y|x).\nRoughly speaking, recent SSL methods with DNNs can be distin-\nguished by the priors they adopt, and, can be divided into two classes1\n- based on generative models or discriminative models, which are re-\nferred to as generative SSL and discriminative SSL, respectively. An\nintuitive way to differentiate between generative and discriminative\nSSL is that generative SSL typically requires modeling the marginal\ndistribution of the data, while discriminative SSL only involves modeling\nthe conditional distribution.\n5.1.1\nDiscriminative SSL\nDiscriminative SSL methods often assume that the outputs from the\ndiscriminative classifier are smooth with respect to local and random\nperturbations of the inputs. Examples include virtual adversarial train-\ning (VAT) (Miyato et al., 2018) and a number of recently developed\nconsistency-regularization based methods such as temporal ensembling\n(Laine and Aila, 2017), mean teachers (Tarvainen and Valpola, 2017),\nFixMatch (Sohn et al., 2020) and contrastive learning based methods\nsuch as SimCLR (Chen et al., 2020).\nDiscriminative SSL methods thus heavily rely on domain-specific\ndata augmentations, e.g., RandAugment (Cubuk et al., 2020), which are\ntuned intensively for images and lead to impressive performance in some\n1We mainly discuss the SSL methods for using DNNs. General discussion of SSL\ncan be referred to (Zhu, 2006).\n144\nJoint EBMs with applications\nimage domains. But discriminative SSL is often less successful for other\ndomains, where these augmentations are less effective (e.g., medical\nimages and text). For instance, random input perturbations are more\ndifficult to apply to discrete data like text (Clark et al., 2018). Although\nthere are some efforts to use data-independent model noises, e.g., by\ndropout (Srivastava et al., 2014), domain-specific data augmentation is\nindispensable.\n5.1.2\nGenerative SSL\nGenerative SSL methods exploit unsupervised learning of generative\nmodels over unlabeled data, which inherently does not require data\naugmentations and generally can be applied to a wider range of do-\nmains. Generative SSL usually involves blending unsupervised learning\nand supervised learning. These methods make fewer domain-specific\nassumptions and tend to be domain-agnostic. The performance compar-\nisons between generative and discriminative SSL methods are mixed.\nIt is found that consistency based discriminative SSL methods often\noutperform generative SSL methods in image domain. However, in text\ndomain, the generative SSL methods such as those based on pre-trained\nword vectors (Mikolov et al., 2013; Pennington et al., 2014) and pre-\ntrained language models (PLMs) (Radford et al., 2019; Raffel et al.,\n2020) are more successful and widely used.\nConsidering observation x and label y, there exist two different\nmethods for the generative SSL approach - joint-training (Larochelle\net al., 2012; Kingma et al., 2014) and pre-training (Hinton et al., 2006).\n• In joint-training, a joint model of p(x, y) is defined. When we have\nlabel y, we maximize p(y|x) (the supervised objective), and when\nthe label is unobserved, we marginalize it out and maximize p(x)\n(the unsupervised objective). Semi-supervised learning over a mix\nof labeled and unlabeled data is formulated as maximizing the\n(weighted) sum of log p(y|x) and log p(x). Given infinite model\ncapacity and data, the joint-learning based SSL is consistent and\ncan find the oracle p(y|x)2.\n2This is because that the maximum likelihood estimator is consistent.\n5.1. BASICS FOR SEMI-SUPERVISED LEARNING\n145\nFigure 5.1: An overview of SSL and a general categorization of generative SSL\nmethods. Examples are mainly chosen from NLP.\n• In pre-training, we perform unsupervised representation learn-\ning on unlabeled data, which is followed by supervised training\n(called fine-tuning) on labeled data. Thus, pre-training is usually\nbased on a model that only defines the marginal distribution p(x)\nwithout the need to involve y. Its empirical effectiveness is mostly\nunderstood from the perspective of representation learning. There\nis no theoretical guarantee that this kind of pre-training will lead\nto finding the oracle p(y|x). This method of pre-training followed\nby fine-tuning has been widely used in natural language processing\n(Radford et al., 2018)3.\nA categorization of generative SSL methods\nis shown in Figure\n5.1. For either of joint-training and pre-training, we can use directed\nmodels (locally-normalized) or undirected models (globally-normalized).\nSo there are four main classes for generative SSL. The models used in\njoint-training could be latent-variable model (LVM) such as in LABES\n(Zhang et al., 2020b), or JRF (Song et al., 2020) or say JEM (Zhao\net al., 2020). Pre-training could be based on masked language models\n3But, more recently in NLP, an approach of jointly modeling the input, the\noutput, and the task description, has emerged to gain more attention and achieved\nsuperior performances (Radford et al., 2019). The input, the output, and the task\ndescription can all be specified as a sequence of tokens, and a language model is\ntrained for estimating natural language sequences so that p(task, input, output) for\nvarious tasks, inputs and outputs are implicitly trained.\n146\nJoint EBMs with applications\n(Devlin et al., 2018), autoregressive language models (Radford et al.,\n2018), or random-field language models (Wang et al., 2018). Further,\ntwo approaches of pre-training and joint-training could be combined or\ncompared to each other. So there are many open questions in designing\nsemi-supervised methods for particular tasks.\nEBM based SSL.\nAmong existing generative SSL methods, energy-\nbased models (EBMs), as an important class of generative models,\nhave been shown with promising results for semi-supervised learning\nacross various domains. Early studies date back to the pre-training of\nrestricted Boltzmann machines (RBMs) (Hinton et al., 2006) (which\nare a simple type of EBMs) and the joint-training with classification\nRBMs (Larochelle et al., 2012).\nRecently, it is shown in (Song and Ou, 2018) that joint-training via\nEBMs produces state-of-the-art SSL results on images (MNIST, SVHN\nand CIFAR-10), compared to previous generative SSL methods based on\nVariational AutoEncoders (VAEs) and Generative Adversarial Networks\n(GANs). It is also shown in (Zhao et al., 2020) that joint-training\nvia EBMs outperforms VAT (virtual adversarial training) (Miyato et\nal., 2018) on tabular data from the UCI dataset repository. Further,\njoint-training via EBMs has been extended in (Song et al., 2020) to\nmodeling sequences and consistently outperforms conditional random\nfields (CRFs) (the supervised baseline) and self-training (the classic semi-\nsupervised baseline) on natural language labeling tasks such as POS\n(part-of-speech) tagging, chunking and NER (named entity recognition).\nNote that although both joint-training and pre-training of EBMs\nhave been used for SSL in the literature, very few studies evaluated\nand compared the two methods. The results from previous individual\nworks are often not directly comparable to each other, since they are not\nevaluated in a common experimental setup. In (Song et al., 2021), a suite\nof experiments are conducted to systematically compare joint-training\nand pre-training for EBM-based SSL. Both the amounts of labeled\nand unlabeled data are varied to give a realistic whole picture of the\nperformances of the two methods for SSL (Oliver et al., 2018). It is found\nthat joint-training EBMs outperform pre-training EBMs marginally\nbut nearly consistently, presumably because the optimization of joint-\n5.2. UPGRADING EBMS TO JOINT EBMS (JEMS) FOR\nFIXED-DIMENSIONAL DATA\n147\ntraining is directly related to the targeted task, while pre-training does\nnot.\nIn the remainder of this chapter, we will detail EBM based SSL,\nincluding both pre-training and joint-training. Note that pre-training\nonly involves the marginal distribution p(x), so the basics for EBM\nbased pre-training can be referred to Section 3 for learning with discrete\nx such as natural languages, and for learning with continuous x such as\nimages, be referred to (Song and Ou, 2018). We will mainly introduce\nthe basics for EBM based joint-training, i.e., establishing EBMs for\nmodeling joint distributions, first in the fixed-dimensional case (Section\n5.2) and then in the sequential case (Section 5.3). Although speech and\nlanguage processing is primarily concerned with the sequential case, the\nintroduction of the fixed-dimensional case can lay the foundation for\nunderstanding the more complicated, sequential case.\n5.2\nUpgrading EBMs to Joint EBMs (JEMs) for fixed-dimensional\ndata\nMotivation.\nOriginally, EBMs are established for modeling the marginal\ndistribution p(x) of observations x, as introduced in Section 3. Recently,\na kind of EBM, called semi-supervised EBM, for modeling the joint dis-\ntribution of observation x and label y has been developed and used for\nsemi-supervised classification (Song and Ou, 2018). In (Grathwohl et al.,\n2020), a similar kind of EBM has been studied, which is called joint EBM\n(JEM). It is established by reinterpreting a standard discriminative clas-\nsifier of p(y|x) as an energy based model for the joint distribution p(x, y).\nIt is demonstrated that energy based training of the joint distribution\nimproves calibration, robustness, and out-of-distribution detection while\nalso enabling sample generation which rivals the quality of recent GAN\napproaches. Hereafter, these EBMs are collectively referred to as JEMs.\nModel definition.\nNote that different models are needed in unsuper-\nvised and semi-supervised learning, because SSL needs to additionally\nconsider labels apart from observations.\nIn semi-supervised tasks, we consider the following EBM for joint\n148\nJoint EBMs with applications\nmodeling of observation x ∈Rdx and class label y ∈{1, · · · , K}:\npθ(x, y) =\n1\nZ(θ) exp [Uθ(x, y)] .\n(5.1)\nThis is different from Eq. (2.13) for unsupervised learning which only\nmodels x without labels. To implement the potential function Uθ(x, y),\nwe consider a neural network Ψθ(x) : Rdx →RK, with x as the input\nand the output size being equal to the number of class labels, K. Then\nwe define\nUθ(x, y) = onehot(y)T Ψθ(x)\nwhere onehot(y) represents the one-hot encoding vector for the label y.\nIn this manner, the conditional density pθ(y|x) is the classifier, defined\nas follows:\npθ(y|x) = pθ(x, y)\npθ(x)\n=\nexp [Uθ(x, y)]\nP\ny′ exp [Uθ(x, y′)]\n(5.2)\nwhich acts like multi-class logistic regression using K logits calculated\nfrom x by the neural network Ψθ(x). And we do not need to calculate\nZ(θ) for classification.\nWith the definition the joint density in Eq. (5.1), it can be shown\nthat, with abuse of notation, the marginal density is\npθ(x) =\n1\nZ(θ) exp [Uθ(x)]\n(5.3)\nwhere Uθ(x) ≜log P\ny′ exp [Uθ(x, y′)].\nModel learning.\nSuppose that among the data D = {x1, · · · , xn},\nonly a small subset of the observations, for example the first m ob-\nservations, have class labels, m ≪n. Denote these labeled data as\nL = {(x1, y1), · · · , (xm, ym)}. Let pemp denote the empirical distribu-\ntion over D. Then we can formulate the semi-supervised learning as\noptimizing\nmin\nθ\n\n\nKL [pemp(x)||pθ(x)] −αd\nX\n(x,y)∼L\nlog pθ(y|x)\n\n\n\n(5.4)\nwhich are defined by hybrids of generative and discriminative criteria,\nsimilar to (Zhu, 2006; Larochelle et al., 2012; Kingma et al., 2014). The\n5.3. UPGRADING CRFS TO JOINT RANDOM FIELDS (JRFS) FOR\nSEQUENTIAL DATA\n149\nhyper-parameter αd controls the relative weight between generative and\ndiscriminative criteria.\nOnce the JEM model and the training objective are established, the\ninclusive-NRF training algorithm (Algorithm 7) developed in (Song and\nOu, 2018) can be applied. Other algorithms for training EBMs, which\ncan achieve proper density estimation such as those recently developed\nin (Zhang et al., 2023), can also be employed.\n5.3\nUpgrading CRFs to Joint random fields (JRFs) for sequential\ndata\nMotivation.\nProbabilistic generative modeling is a principled method-\nology that promisingly can learn from data of various forms (whether\nlabeled or unlabeled) to benefit downstream tasks, which, however, is\nparticularly challenging for sequence data. Two important sequence\ntasks are sequence modeling and sequence labeling.\nA basic problem of sequence modeling is to determine the prob-\nabilities of sequences. An example is language modeling (Chen and\nGoodman, 1999), which, as described in Section 3.2, is a crucial com-\nponent in many speech and language processing tasks. For sequences\nof length l, xl ≜x1, x2, ..., xl, this amounts to calculate p(l, xl), where\nwe make explicit the role of the length l. Ideally, this density modeling\ncan be improved with additional relevant labels. e.g. incorporating part-\nof-speech (POS) tags for language modeling. There are some previous\nstudies in (Rosenfeld et al., 2001; Sarikaya et al., 2010). The difficulty\nis that the labels (e.g. POS) usually are not available in testing, so\na standalone POS tagger is needed to provide hypothesized labels in\ntesting.\nThe task of sequence labeling, as described in Section 4.3, is, given\nobservation sequence xl, to predict the label sequence yl ≜y1, y2, ..., yl,\nwith one label for one observation at each position. Sequence labeling has\nbeen widely applied in various tasks, e.g., POS labeling (Collobert et al.,\n2011; Ling et al., 2015), named entity recognition (NER) (Huang et al.,\n2015; Lample et al., 2016; Ma and Hovy, 2016), and chunking (Huang\net al., 2015; Søgaard and Goldberg, 2016). As introduced in Section\n5.1.2, it is desirable for the labeling model to leverage both labeled\n150\nJoint EBMs with applications\ndata (namely pairs of xl and yl) and unlabeled data (namely xl without\nlabels), i.e., to conduct semi-supervised learning (SSL). Pre-training\nhas proved to be effective (Collobert et al., 2011; Devlin et al., 2018),\nwhich, however, is task-independent followed by task-dependent fine-\ntuning. Besides pre-training, it is worthwhile to explore task-dependent\nsemi-supervised learning (SSL) in the manner of joint-training, which\nlearns for a task on a mix of labeled and unlabeled data. Self-training\nis such a method with limited success (Scudder, 1965).\nAs introduced in Section 4.1, conditional random fields (CRFs)\n(Lafferty et al., 2001) have been shown to be one of the most successful\napproaches to sequence labeling. A CRF is a discriminative model,\nwhich directly defines a conditional distribution p(yl|xl), and thus\nmainly depends on supervised learning with abundant labeled data.\nIt is proposed in (Song et al., 2020) to upgrade CRFs and obtain a\njoint generative model of xl and yl, p(l, xl, yl), called joint random\nfields (JRFs). Specifically, the potential function U(xl, yl) in the original\nCRF p(yl|xl) is borrowed as the potential function that defines a joint\ndistribution p(xl, yl). This upgrading, operated in the sequential setting,\nis similar to upgrade EBMs to JEMs in the fixed-dimensional setting\n(Song and Ou, 2018; Grathwohl et al., 2020). Similar to the fixed-\ndimensional setting, the conditional distribution of yl given xl induced\nfrom this joint distribution is exactly the original conditional distribution\n- the CRF p(yl|xl)4; and the marginal distribution of p(l, xl) induced\nfrom the joint distribution is a trans-dimensional random field (TRF)\nlanguage model (Wang et al., 2018; Wang and Ou, 2018a), as described\nin Section 3.2.2.\nThis development from CRFs to JRFs benefits both modeling and\nlabeling of sequence data. For sequence modeling, the marginal like-\nlihood p(l, xl) can be efficiently calculated by JRFs, without the step\nof producing hypothesized labels by a standalone POS tagger. For se-\nquence labeling, JRFs admit not only supervised learning from labeled\ndata by maximizing the conditional likelihood p(yl|xl) (which is like the\ntraining of a CRF), but also unsupervised learning from unlabeled data\n4So writing the JRF as p(l, xl, yl) and the CRF as p(yl|xl) is correct, not an\nabuse of notation.\n5.3. UPGRADING CRFS TO JOINT RANDOM FIELDS (JRFS) FOR\nSEQUENTIAL DATA\n151\nby maximizing the marginal likelihood p(l, xl) (which is like the train-\ning of a TRF LM), thereby achieving task-dependent semi-supervised\nlearning.\nModel definition.\nWe will first briefly review linear-chain CRFs, as\ndescribed in Section 4.1.1, but with different notations, which facilitate\nthe introduction of JRFs. A linear-chain CRF defines a conditional\ndistribution with parameters θ for label sequence yl given observation\nsequence xl of length l:\npθ(yl|xl) =\n1\nZθ(xl) exp(Uθ(xl, yl))\n(5.5)\nHere the potential function\nUθ(xl, yl) =\nl\nX\ni=1\nϕi(yi, xl) +\nl\nX\ni=1\nψi(yi−1, yi, xl),\n(5.6)\nis defined as a sum of node potentials and edge potentials, and Zθ(xl) =\nP\nyl exp(Uθ(xl, yl)) is the normalizing constant. ϕi(yi, xl) is the node\npotential defined at position i, which, in recently developed neural CRFs\n(Collobert et al., 2011; Huang et al., 2015; Lample et al., 2016; Ma\nand Hovy, 2016) is implemented by using features generated from a\nneural network (NN) of different network architectures. ψi(yi−1, yi, xl)\nis the edge potential defined on the edge connecting yi−1 and yi, often\nimplemented as a matrix A, ψi(yi−1 = j, yi = k, xl) = Aj,k, thereby\ndefines a linear-chain CRF. In linear-chain CRFs, there are efficient\nalgorithms for training (the forward-backward algorithm) and decoding\n(the Viterbi algorithm).\nInspired from the idea of jointly modeling fixed-dimensional ob-\nservations (e.g. images) and labels via JEMs in (Song and Ou, 2018;\nGrathwohl et al., 2020), CRFs can be similarly upgraded and a joint\ndistribution over sequential observations and labels can be established,\ncalled JRFs (Song et al., 2020). The keypoint is that we can use Uθ(xl, yl)\nin Eq. (5.6) from the original CRF to define a joint distribution pθ(xl, yl):\npθ(l, xl, yl) = πlpθ(xl, yl; l) =\nπl\nZθ(l) exp(Uθ(xl, yl))\n(5.7)\n152\nJoint EBMs with applications\nwhere πl is the empirical prior probability for length l. Notably, a CRF is\na conditional distribution, normalizing over label sequences. In contrast,\na JRF is a joint distribution, normalizing over both observation and\nlabel sequences, with the normalizing constant for length l defined as\nZθ(l) =\nX\nxl,yl\nexp(Uθ(xl, yl)).\n(5.8)\nInterestingly, it can be easily seen that the conditional distribution\nof yl given xl induced from the JRF’s joint distribution Eq. (5.7) is\nexactly the original CRF Eq. (5.5). Further, by marginalizing out yl,\nthe marginal distribution of p(l, xl) induced from the joint distribution\nis:\npθ(l, xl) =\nπl\nZθ(l)\nX\nyl\nexp(Uθ(xl, yl)) =\nπl\nZθ(l) exp(Uθ(xl))\nwhich acts like a trans-dimensional random field (TRF) language model\n(Wang et al., 2018; Wang and Ou, 2018a), with the potential defined by\nUθ(xl) = log\nX\nyl\nexp(Uθ(xl, yl)).\nNotably this marginal potential Uθ(xl) is exactly the normalizing con-\nstant log Zθ(xl) Eq. (5.8) from the CRF. It can be calculated via the\nforward algorithm from the linear-chain potential Uθ(xl, yl).\nModel learning.\nGiven different data resources (labeled or unlabeled),\nJRF can be trained under different settings (supervised, unsupervised,\nor semi-supervised) and applied in different downstream tasks (sequence\nmodeling or labeling), as illustrated in Fig. 5.2. Note that the embedded\nCRF and TRF inside a JRF share all parameters θ, which is different\nfrom multi-task learning where only bottom-level parameters are shared\n(Argyriou et al., 2007).\nSupervised learning of JRFs amounts to the training of the embedded\nCRF with the following supervised objective, given labeled data in the\nform of empirical distribution plab(xl, yl),\nmax\nθ\nJsup(θ) = E(xl,yl)∼plab(xl,yl)[log pθ(yl|xl)]\n(5.9)\n5.3. UPGRADING CRFS TO JOINT RANDOM FIELDS (JRFS) FOR\nSEQUENTIAL DATA\n153\nFigure 5.2: Overview of the JRF model. The node and edge potentials define a\nJRF (a joint distribution over xl and yl). Inducing the conditional and the marginal\nfrom the joint yields a CRF and a TRF respectively. A JRF can be trained from\nlabeled data (acting like a CRF) and also from unlabeled data (acting like a TRF).\nIn practice, the node potentials are calculated from the logits oi, i = 1, · · · , l, from\nthe NN, and the edge potential follows a linear-chain definition.\n154\nJoint EBMs with applications\nwhich can be solved by applying minibatch-based stochastic gradient\ndescent (SGD). At each iteration, a minibatch of sentences and labels is\nsampled from plab(xl, yl), denoted by Dlab, and the stochastic gradients\nare:\n∂Jsup(θ)\n∂θ\nV\n=\n1\n|Dlab|\nX\n(xl,yl)∈Dlab\n\"\n∂Uθ(xl, yl)\n∂θ\n−∂Uθ(xl)\n∂θ\n#\nUnsupervised learning of JRFs amounts to the training the embedded\nTRF, by applying the dynamic noise-contrastive estimation (DNCE)\nalgorithm developed in (Wang and Ou, 2018a). Given unlabeled data\n(e.g. sentences) in the form of empirical distribution punl(l, xl), DNCE\njointly optimizes over a JRF and a noise distribution pϕ(l, xl) (generally\na LSTM language model) parameterized by ϕ:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmax\nθ E\n(l,xl)∼\npunl(l,xl)+pϕ(l,xl)\n2\n\"\nlog\npθ(l, xl)\npθ(l, xl) + pϕ(l, xl)\n#\n+\nE(l,xl)∼pϕ(l,xl)\n\"\nlog\npϕ(l, xl)\npθ(l, xl) + pϕ(l, xl)\n#\n≜Juns(θ)\nmin\nϕ KL\nh\npunl(l, xl)||pϕ(l, xl)\ni\n(5.10)\nThanks to optimization of DNCE, the annoying normalizing constants\nZθ(l) in JRFs can be jointly estimated along with the parameter es-\ntimation. Specifically, we introduce parameters ζl for log Zθ(l) and\nζ = (ζ1, ζ2, ..., ζL), where L is a pre-defined maximum length. Hereafter,\nwe denote by ξ = (θ, ζ) all the parameters in the JRF and rewrite pθ(·)\nas pξ(·).\nAt each iteration, a minibatch of sentences are sampled from punl(l, xl),\ndenoted by Dunl, two minibatches of sentences sampled from pϕ(l, xl),\ndenoted by B1, B2 (|B2| = 2|B1| = 2|Dunl|), and the stochastic gradients\n5.3. UPGRADING CRFS TO JOINT RANDOM FIELDS (JRFS) FOR\nSEQUENTIAL DATA\n155\nare:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n∂Junl(ξ)\n∂ξ\nV\n=\n1\n|B2|\nX\n(l,xl)∈B2\npξ(l, xl)\npξ(l, xl) + pϕ(l, xl)g(l, xl; ξ)\n+\n1\n|Dunl| + |B1|\nX\n(l,xl)∈Dunl∪B1\npϕ(l, xl)\npξ(l, xl) + pϕ(l, xl)g(l, xl; ξ)\n∂KL(punl||pϕ)\n∂ϕ\nV\n= −\n1\n|Dunl|\nX\n(l,xl)∈Dunl\n∂log pϕ(l, xl)\n∂ϕ\nwhere g(l, xl; ξ) denotes the gradient of log pξ(l, xl) w.r.t. ξ = (θ, ζ),\nand the two gradient components w.r.t. θ and ζ are ∂Uθ(xl)/∂θ and\n−(δ(l = 1), ..., δ(l = L)) respectively.\nSemi-supervised learning of JRFs over a mix of labeled and unlabeled\ndata amounts to combining the above supervised and unsupervised\ntraining with the following semi-supervised objective:\n\n\n\n\n\nmax\nξ J(ξ) = Jsup(ξ) + αJuns(ξ)\nmin\nϕ KL\nh\npunl(l, xl)||pϕ(l, xl)\ni\n(5.11)\nwhere α is the trade-off weight between supervised and unsupervised\nlearning, and ξ = (θ, ζ) is defined before.\nThe performance of the JRF model.\nThe benefits of JRFs to sequence\nmodeling and labeling are demonstrated through two sets of experiments\nin (Song et al., 2020). First, various traditional language models (LMs)\nsuch as Kneser-Ney (KN) smoothed n-gram LM (Chen and Goodman,\n1999), LSTM LM (Zaremba et al., 2014) and TRF LM (Wang and\nOu, 2018a) are trained on Wall Street Journal (WSJ) portion of Penn\nTreebank (PTB) English dataset (without using POS tags). JRF LMs\nare trained by using POS tags. These models are then used to rescore\nthe 1000-best list generated from the WSJ’92 test set, with similar\nexperimental setup as in (Wang and Ou, 2018a). The JRF model is\neffective in incorporating POS tags and performs the best with the\nlowest rescoring word error rate (WER). Second, experiments on three\nsequence labeling tasks - POS tagging, chunking and NER, with Google\n156\nJoint EBMs with applications\nTable 5.1: Applications of EBMs across different domains: comparison and connec-\ntion (See text for details).\nImage classification\nNatural language labeling\nObservation\nx ∈RD\nx ∈S\nl Vl\ncontinuous, fixed-dimensional\ndiscrete, sequence\nLabel\ny ∈{1, 2, · · · , K}\ny ∈S\nl{1, 2, · · · , K}l\nPre-training\nUθ(x) = wT h\nUθ(x) in Eq. (5.12)\nJoint-training\nUθ(x, y) = Ψθ(x)[y]\nUθ(x, y) in Eq. (5.16)\none-billion-word dataset (Chelba et al., 2014) as the unlabeled data\nresource, are conducted. It is found that the JRF based SSL consistently\noutperforms the CRF baseline and self-training.\n5.4\nJEMs and JRFs for semi-supervised learning\nNow we have introduced the basics for establishing JEMs and JRFs for\nfixed-dimensional and sequential data in Section 5.2 and Section 5.3,\nrespectively. As introduced in Section 5.1.2, there exist two different\nmethods for EBM based SSL - pre-training and joint-training. Pre-\ntraining of RBMs once received attention in the early stage of training\nDNNs (Hinton et al., 2006). Encouraging SSL results have been shown\nrecently for EBM based joint-training. In (Song and Ou, 2018; Zhao\net al., 2020; Song et al., 2020), state-of-the-art SSL results are reported\nbased on EBMs and across different data modalities (images, natural\nlanguages, an protein structure prediction and year prediction from the\nUCI dataset repository) and in different data settings (fix-dimensional\nand sequence data).\nSo EBM-based SSL can be applied across different data modalities\n(fix-dimensional and sequence data), by either of pre-training and joint-\ntraining. Thus, there are four cases. In this section, we systematically\nintroduce these four cases, as summarized in Table 5.1, where we take\nimage classification and natural language labeling as representative tasks.\nWe continue with the notations in Definition 2.5 of EBMs parameterized\nby neural networks.\n5.4. JEMS AND JRFS FOR SEMI-SUPERVISED LEARNING\n157\nFigure 5.3: Illustration of EBM based semi-supervised image classification. (a)\nPre-training, (b) Fine-tuning, (c) Joint-training.\n5.4.1\nPre-training via EBMs for SSL\nPre-training via EBMs for SSL consists of two stages. The first stage is\npre-training an EBM on unlabeled data. It is followed by a fine-tuning\nstage, where we can easily use the pre-trained EBM to initialize a\ndiscriminative model and further train over labeled data.\nPre-training of an EBM for semi-supervised image classification\nessentially involves estimating pθ(x) as defined in Eq. (2.13) from unla-\nbeled images. For the potential function Uθ(x), we can use a multi-layer\nfeed-forward neural network Φθ(x) : RD →R, which, in the final layer,\ncalculates a scalar via a linear layer, Uθ(x) = wT h, as shown in Figure\n5.3(a). Here h ∈RH denotes the activation from the last hidden layer\nand w ∈RH the weight vector in the final linear layer. For simplicity,\nwe omit the bias in describing linear layers throughout Section 5.4.\nIn fine-tuning, as shown in Figure 5.3(b), we throw away w and fed\nh into a new linear output layer, followed by softmax(Wh), to predict\ny, where W ∈RK×H denotes the new trainable weight parameters and\ny ∈{1, · · · , K} the class label. It can be seen that pre-training aims to\n158\nJoint EBMs with applications\nlearn representations that may be useful for multiple downstream tasks,\nand information about the labels is not utilized until the fine-tuning\nstage.\nPre-training of an EBM for semi-supervised natural language labeling\n(e.g., POS tagging)\nis similar to the above procedure of pre-training\nof an EBM for semi-supervised image classification. In pre-training,\nwe estimate an EBM-based language model pθ(x) from unlabeled text\ncorpus. Neural networks with different architectures can be used to\nimplement the potential function Φθ(x) : Vl →R given length l. With\nabuse of notation, here x = (x1, . . . , xl) denotes a token sequence of\nlength l, and xi ∈V, i = 1, · · · , l. For example, as shown in Figure\n5.4(a), we can use the bidirectional LSTM based potential function in\n(Wang and Ou, 2018a) as follows:\nUθ(x) =\nl−1\nX\ni=1\nhT\nf,iei+1 +\nl\nX\ni=2\nhT\nb,iei−1\n(5.12)\nwhere ei, hf,i and hb,i are of the same dimensions, denoting the output\nembedding vector, the last hidden vectors of the forward and backward\nLSTMs respectively at position i.\nIn fine-tuning, as shown in Figure 5.4(b), we add a CRF, as the dis-\ncriminative model, on top of the extracted representations, (hf,i, hb,i), i =\n1, · · · , l, to do sequence labeling, i.e., to predict a sequence of labels\ny = (y1, . . . , yl) with one label for one token at each position, where\nyi ∈{1, · · · , K} denotes the label at position i. Specifically, we concate-\nnate hf,i and hb,i, add a linear output layer to define the node potential,\nand add a matrix A ∈RK×K to define the edge potential, as in recent\nneural CRFs (Lample et al., 2016; Ma and Hovy, 2016). The parameters\nto be fine-tuned are the weights in the linear output layer and the edge\npotential matrix A.\n5.4.2\nJoint-training via EBMs for SSL\nThe above pre-training via EBMs for SSL considers the modeling of\nonly observations x without labels y. The joint-training refers to the\n5.4. JEMS AND JRFS FOR SEMI-SUPERVISED LEARNING\n159\nFigure 5.4: Illustration of EBM based sequence labeling. (a) Pre-training, (b)\nFine-tuning, (c) Joint-training.\njoint modeling of x and y:\npθ(x, y) =\n1\nZ(θ) exp [Uθ(x, y)]\n(5.13)\nThen, it can be easily seen, as detailed in Section 5.2, that the conditional\ndensity pθ(y|x) implied by the joint density Eq. (5.13) is:\npθ(y|x) = pθ(x, y)\npθ(x)\n=\nexp(Uθ(x, y))\nP\ny′ exp(Uθ(x, y′))\n(5.14)\nAnd the implied marginal density is\npθ(x) =\n1\nZ(θ) exp(Uθ(x))\n(5.15)\nwhere, with abuse of notation, Uθ(x) ≜log P\ny exp [Uθ(x, y)]. Different\nfrom pre-training, the unsupervised objective pθ(x) depends on the\ntargeted task. The key for EBM based joint-training for SSL is to choose\nsuitable Uθ(x, y) such that both pθ(y|x) and pθ(x) can be tractably\noptimized.\nJoint-training of an EBM for semi-supervised image classification\nconsiders a neural network Ψθ(x) : RD →RK, which, as shown in Figure\n5.3(c), accepts the image x and outputs an vector, whose size is equal\nto the number of class labels, K. Then we define Uθ(x, y) = Ψθ(x)[y],\nwhere [y] denotes the y-th element of a vector. With the above potential\n160\nJoint EBMs with applications\ndefinition, it can be easily seen that the implied conditional density\npθ(y|x) is exactly a standard K-class softmax based classifier, using the\nK logits calculated by the neural network Ψθ(x) from the input x. And\nwe do not need to calculate Z(θ) for classification. Therefore, we can\nconduct SSL over a mix of labeled and unlabeled data by maximizing the\n(weighted) sum of log pθ(y|x) and log pθ(x), where both optimizations\nare tractable as detailed in (Song and Ou, 2018).\nJoint-training of an EBM for semi-supervised natural language la-\nbeling\nis similar to the above procedure of joint-training of an EBM\nfor semi-supervised image classification, with x = (x1, . . . , xl) and\ny = (y1, . . . , yl), xi ∈V, yi ∈{1, · · · , K} , i = 1, · · · , l. As shown in\nFigure 5.4(c), we consider a neural network Ψθ(x) : Vl →Rl×K and\ndefine\nUθ(x, y) =\nl\nX\ni=1\nΨθ(x)[i, yi] +\nl\nX\ni=1\nA[yi−1, yi]\n(5.16)\nwhere [·, ·] denotes the element of a matrix and A ∈RK×K models the\nedge potential for adjacent labels. With the above potential definition,\nit can be easily seen, as detailed in Section 5.3, that the conditional\ndensity pθ(y|x) implied by the joint density Eq.(5.13) is exactly a CRF\nwith node potentials Ψθ(x)[i, yi] and edge potentials A[yi−1, yi], and the\nimplied marginal density pθ(x) is exactly a trans-dimensional random\nfield (TRF) language model (Wang et al., 2018; Wang and Ou, 2017;\nWang and Ou, 2018b). Training of both models are tractable as detailed\nin (Song et al., 2020; Wang and Ou, 2018a).\n5.4.3\nComparison of joint-training and pre-training\nIn (Song et al., 2021), a suite of SSL experiments are conducted on\nstandard benchmark datasets in different domains, including the CIFAR-\n10 and SVHN datasets (Song and Ou, 2018) for image classification\nand the POS, chunking and NER datasets (Hu et al., 2019; Song et al.,\n2020) for natural language labeling. It is revealed that joint-training\nEBMs outperform pre-training EBMs marginally but nearly consistently.\nPresumably, this is because that the optimization of joint-training is\n5.4. JEMS AND JRFS FOR SEMI-SUPERVISED LEARNING\n161\nTable 5.2: SSL for image classification over CIFAR-10 with 4,000 labels for\na full training set of 50K images. The upper/lower blocks show the genera-\ntive/discriminative SSL methods respectively. The means and standard deviations\nare calculated over ten independent runs with randomly sampled labels.\nMethods\nerror (%)\nCatGAN (Springenberg, 2016)\n19.58±0.46\nLadder network (Rasmus et al., 2015)\n20.40±0.47\nImproved-GAN (Salimans et al., 2016)\n18.63±2.32\nBadGAN (Dai et al., 2017)\n14.41±0.30\nSobolev-GAN (Mroueh et al., 2018)\n15.77±0.19\nSupervised baseline\n25.72±0.44\nPre-training+fine-tuning EBM\n21.40±0.38\nJoint-training EBM\n15.12±0.36\nResults below this line cannot be directly compared to those above.\nVAT small (Miyato et al., 2018)\n14.87\nTemporal Ensembling (Laine and Aila, 2017)\n12.16±0.31\nMean Teacher (Tarvainen and Valpola, 2017)\n12.31±0.28\ndirectly related to the targeted task, but pre-training is not aware of\nthe labels for the targeted task.\nFull detailed experimental results are referred to (Song et al., 2021).\nHere we present some results for illustration. In (Song et al., 2021), the\nstandard data split for training and testing is used. When changing the\namount of labeled and unlabeled data for training, varying proportions\n(e.g., 10%, 100%) of labels are selected from the original full set of\nlabeled data. Hereafter, the amount of labels is thus described in terms\nof proportions. “100% labeled” means 50K images for CIFAR-10, and\n56K, 7.4K, 14K sentences for POS, chunking and NER, respectively.\nSSL for Image Classification.\nIn (Song et al., 2021), different genera-\ntive SSL methods are compared over CIFAR-10. As in previous works,\n4,000 labeled images are randomly sampled for training. The remaining\nimages are treated as unlabeled. It can be seen from Table 5.2 that semi-\nsupervised EBMs, especially the joint-training EBMs, produce strong\n162\nJoint EBMs with applications\nresults on par with state-of-art generative SSL methods5. Furthermore,\njoint-training EBMs outperform pre-training+fine-tuning EBMs by a\nlarge margin in this task. Note that some discriminative SSL methods,\nas listed in the lower block in Table 5.2, also produce superior results\nbut heavily utilize domain-specific data augmentations, and thus are\nnot directly compared to the generative SSL methods.\nSSL for Natural Language Labeling.\nIn this experiment, different SSL\nmethods are evaluated for natural language labeling over three tasks -\nPOS tagging, chunking and NER. The following benchmark datasets\nare used - PTB POS tagging, CoNLL-2000 chunking and CoNLL-2003\nEnglish NER, as in (Ma and Hovy, 2016; Clark et al., 2018; Hu et al.,\n2019; Song et al., 2020). Varying proportions of labels are sampled\nas labeled training data and use the Google one-billion-word dataset\n(Chelba et al., 2014) as the large pool of unlabeled sentences. A large\nscale of experiments are conducted, covering the labeling proportions\nof 2%, 10% and 100% with “U/L” (the ratio between the amount of\nunlabeled and labeled) of 50, 250 and 500 for three tasks, which consist\nof a total of 27 settings. The network architectures in (Song et al.,\n2020) is used. After some empirical search, hyper-parameters (tuned\nseparately for different methods) are fixed, which are used for all the 27\nsettings.\nTable 5.3 only show the relative numerics, absolute numerics are\nreferred to (Song et al., 2021). The main observations are as follows.\n• The joint-training EBMs outperform the supervised baseline in 25\nout of the 27 settings. Since we perform one run for each setting,\nthis indicates 2 outliers.\n• The effects of increasing the labeling sizes on the improvements\nof the joint-training EBMs over the supervised baseline with\na fixed “U/L” are mixed. For POS/chunking/NER, the largest\nimprovements are achieved under 2%/10%/100% labeled, respec-\ntively. It seems that the working point where an SSL method\n5As discussed in (Song and Ou, 2018), Bad-GANs could hardly be classified as a\ngenerative SSL method.\n5.4. JEMS AND JRFS FOR SEMI-SUPERVISED LEARNING\n163\nTable 5.3: Relative improvements by joint-training EBMs compared to the su-\npervised baseline (abbreviated as sup.) and the pre-training+fine-tuning EBMs\n(abbreviated as pre.) respectively. The evaluation metric is accuracy for POS and\nF1 for chunking and NER. “Labeled” denotes the amount of labels in terms of the\nproportions w.r.t. the full set of labels. “U/L” denotes the ratio between the amount\nof unlabeled and labeled data.\njoint over sup.\njoint over pre.\nLabeled\nU/L\nPOS\nChunking\nNER\nPOS\nChunking\nNER\n2%\n50\n7.9\n16.5\n-2.7\n4.7\n3.4\n3.7\n250\n12.6\n16.6\n1.5\n4.2\n0.9\n0.1\n500\n15.1\n20.3\n4.5\n4.1\n-0.3\n-1.5\n10%\n50\n5.6\n18.0\n0.9\n3.8\n3.0\n5.0\n250\n6.0\n18.3\n-1.2\n3.8\n9.4\n-0.7\n500\n8.5\n21.8\n1.0\n5.2\n3.7\n-4.1\n100%\n50\n3.1\n10.3\n6.5\n3.5\n5.3\n1.1\n250\n5.0\n13.6\n8.3\n3.5\n7.4\n3.6\n500\n6.2\n14.0\n8.4\n4.3\n6.4\n2.5\nbrings the largest improvement over the supervised baseline is\ntask dependent. Suppose that the working point is indicated by\nthe performance of the supervised baseline, then the SSL method\nbrings the largest effect when the performance of the supervised\nbaseline is moderate, i.e., neither too low nor too high.\n• Joint-training EBMs outperform pre-training EBMs in 23 out\nof the 27 settings marginally but nearly consistently. A possible\nexplanation is that pre-training is not aware of the labels for the\ntargeted task and is thus weakened for representation learning.\nIn contrast, the marginal likelihood optimized in joint-training is\ndirectly related to the targeted task.\n• It seems that the degrees of improvements of the joint-training\nEBMs over the pre-training EBMs are not much affected when\nvarying the labeling sizes and the “U/L” ratios.\n164\nJoint EBMs with applications\n5.5\nJRFs for calibrated natural language understanding\nMotivation\nIn this section, we describe how the joint EBMs (JEMs), as introduced\nin Section 5.2, can be used for calibrated natural language understand-\ning. Calibration refers to how well a classification model’s confidence\n(reflected by its output posterior probability) aligns with its actual\naccuracy. As deep learning models achieve amazing accuracy in com-\nputer vision and natural language processing (NLP), more research\nattention has been drawn to the calibration aspect of these models. As\nshown by Guo et al., 2017, the high accuracy from deep models does\nnot always lead to better calibration. This motivates an important line\nof works attempting to achieve a better trade-off between accuracy and\ncalibration.\nMost existing calibration methods (see references in He et al., 2021)\ngenerally rescale the posterior distribution predicted from the classi-\nfier after training. Such post-processing methods require a held-out\ndevelopment set with a decent number of samples to be available. In\nanother line of work, Grathwohl et al., 2020 shows that one can train\na joint EBM together with the standard training of a neural classifier.\nAlthough calibration is not explicitly addressed during EBM training,\nthe calibration of the resulting classifier is shown to be greatly improved.\nHowever, the training framework proposed by Grathwohl et al., 2020\nis designed for image classifiers, and it can not be readily applied to\ndiscrete text data. He et al., 2021 investigates JEM training during the\nfinetuning of pre-trained text encoders (e.g., BERT or RoBERTa) for\nnatural language understanding (NLU) tasks.\nThe JEM model for NLU\nHe et al., 2021 considers the finetuning of pre-trained text encoder on\nNLU tasks. Assume samples from the data distribution pdata are in\nthe form of (x, y) pairs, where x usually refers to a single or a pair\nof sentences, and y refers to the corresponding label. The number of\nclasses are denoted by |Y|.\nGiven input x, a text encoder model (e.g., BERT or RoBERTa) is\n5.5. JRFS FOR CALIBRATED NATURAL LANGUAGE\nUNDERSTANDING\n165\nFigure 5.5: Comparison of the scalar and the hidden variants of energy functions.\nThe modules introduced for EBM are shaded in green. (He et al., 2021)\nfirstly used to encode x, and the resulting embedding is denoted by\nenc(x). For the target classification task, a classifier fCLS, which could\nbe a simple linear transform or a multi-layer perception (MLP), will\nbe applied to enc(x). The output logits are denoted as fCLS(enc(x)),\nwhose dimension is equal to the number of possible classes |Y|. The y-th\nlogit is denoted by fCLS(enc(x))[y]. The posterior distribution pθ(y|x)\nis obtained by applying a softmax operation to the logits, where θ refers\nto the parameters in the model.\nIn standard finetuning, the cross-entropy (CE) loss and gradient\nbased optimizers are used to train the classifier:\nLCE = E(x,y)∼pdata(−log pθ(y|x)).\n(5.17)\nIn the following, we will introduce how a JEM model can be defined on\ntop of the text encoder for calibrated NLU.\nHe et al., 2021 considers a JEM model in a residual form, similar to\nSection 3.3.3 and Section 4.4.1. The energy function for modeling the\nmarginal distribution of x is defined as follows:\nEθ(x) = −log pN(x) + ˆEθ(x)\n(5.18)\nwhere pN(x) is the base distribution, which will also be used as the\nnoise distribution during NCE training. He et al., 2021 examines three\nvariants of the residual energy function ˆEθ(x).\n• Variant scalar. Another linear layer gS is introduced to define\nthe energy function, whose output is a scalar:\nˆEθ(x) = gS(enc(x)).\n166\nJoint EBMs with applications\n• Variant hidden. Similar to (Grathwohl et al., 2020; Song and\nOu, 2018), starting from a softmax based classifier, an EBM can\nbe defined with the logits as follows:\nˆEθ(x) = −LogSumExp|Y|\ny=1(fCLS(enc(x))[y]).\nDifferent from the scalar variant, here the energy function directly\nuses the logits for prediction (visualized in Figure 5.5). Hence the\nimpact on the model’s classification behavior could be larger.\n• Variant sharp-hidden. The hidden variant has a potential weak-\nness that, the correlation between input x and the prediction\ny is not addressed because the energy is distributed among all\nthe logits. Motivated by this potential issue, the following sharp\nvariant is proposed, which can be viewed as an approximation\nto the hidden variant, and is found to work well in practice (He\net al., 2021).\nˆEθ(x) = −max\ny\nfCLS(enc(x))[y].\nNCE training of the JEM model\nSimilar to the discussion in Section 4.4.1, NCE is applied to train the\nresidual JEM model. NCE trains the model to discriminate between real\nsamples from pdata and noise samples from a given noise distribution\npN. The NCE loss is the same as Eq. (4.33):\nLNCE = −\nE\nx+∼pdata log\n1\n1 + ν exp(Eθ(x+))−ν\nE\nx−∼pN log\n1\n1 + exp(−Eθ(x−))/ν\nwhere ν is the ratio of noise samples to real samples.\nIn (He et al., 2021), LCE and LNCE are jointly optimized during\ntraining, that is:\nLjoint = LCE + LNCE\nFor constructing the noise distribution pN(x), (He et al., 2021)\nfinetunes the GPT-2 language model (Radford et al., 2019) with samples\nfrom the target training set. However during NCE training, the energy\nmodel is found to easily discriminate between data samples and noise\n5.5. JRFS FOR CALIBRATED NATURAL LANGUAGE\nUNDERSTANDING\n167\nsamples, which makes training ineffective (He et al., 2021). To alleviate\nthis issue, an objective similar to the masked language model (MLM)\nloss (Devlin et al., 2018) is adopted during the finetuning of the noise\nmodel. With a given mask ratio M (e.g., 0.4), He et al., 2021 randomly\nmasks part of x, and trains the model to complete it:\nLMLM = −\nE\nx∼pdata,xm∼mask(x,M) log pN(x|xm)\nDuring noise sample generation, adopting the same mask ratio M, a\nmasked xm is fed to pN (x is from the training set), and the generated\nsample are used as the noise sample. In this way, the noise distribution\nis made closer to the data distribution.\nEvaluation of the Calibration performance\nTo measure calibration performance, expected calibration error (ECE)\nmetric is used, following (Guo et al., 2017; Grathwohl et al., 2020). Given\nan input sample x, for each label y, we say that the model predicts\nthat x belongs to label y with confidence pθ(y|x). Assuming the test-set\ncontains n samples, we will have n × |Y| predictions.\nECE first partitions all predictions into B equally-spaced bins by its\nconfidence. Following (Grathwohl et al., 2020), B = 20 is used, which\nmeans the width of each bin is 0.05. For example, the first bin contains\nall predictions that have confidence in the range of [0, 0.05). Then for\neach bin ECE computes how the average of confidence is different from\nits actual accuracy:\nECE = 1\n|Y|\n|Y|\nX\ny=1\nB\nX\nb=1\n|Byb|\nn\n|acc(Byb) −conf(Byb)|\n(5.19)\nwhere n is the number of test samples. acc(Byb) is the ratio of samples\n(x) whose true label is indeed y in the bin Byb, and conf(Byb) is the\naverage confidence in that bin.\nHe et al., 2021 uses the RoBERTa-base model as the text encoder\nand finetune it on eight GLUE tasks (Wang et al., 2019). It is found\nthat EBM training is able to reach a better trade-off between accuracy\nand calibration. In most tasks, all three EBM variants get substantial\n168\nJoint EBMs with applications\nFigure 5.6: The entropy of the posterior (pθ(·|x)) versus energy value ˆEθ(x) for\nSST-2 test-set samples. (He et al., 2021)\nimprovement in ECE with little or no loss in accuracy comparing to\nthe (strong) baseline methods.\nHow does the model get better calibration? Figure 5.6 from He et al.,\n2021 give some analysis. Figure 5.6 shows the energy value ˆEθ(x) versus\nthe entropy of the posterior distribution\nH(pθ(·|x)) = −\n|Y|\nX\ny=1\npθ(y|x) log pθ(·|x)\nfor samples in the SST-2 test set. It is shown that models trained with\nthe hidden and sharp-hidden variants tend to assign more conservative\npredictions (reflected by higher entropy) for higher-energy (less likely)\nsamples. It is hypothesized that this is due to the strong coupling\nbetween the energy function and the classification logits. However, this\ninteresting trend (Figure 5.6) is not observed in all datasets (e.g., QNLI).\n6\nConclusion\n6.1\nSummary\nEnergy-based models (EBMs) form an important aspect of modern arti-\nficial intelligence and signal processing. Unlike most other probabilistic\nmodels which are self-normalized, EBMs specify probability density\nfunctions up to an unknown normalizing constant, and thus are also\nreferred to as non-normalized probabilistic models. Such model defini-\ntion of not imposing normalization enables a great power and flexibility\nto the modeling process. One is generally free to choose any nonlin-\near regression function as the energy function, as long as it remains\nnormalizable in principle. Accompanied with such flexibility, we have\nalso shown the advantages of EBMs in naturally overcoming label bias\nand exposure bias suffered by locally-normalized models (Section 4.1.2)\nand in hybrid generative-discriminative and semi-supervised learning\n(Section 5).\nOn the other hand, although the flexibility of EBMs can provide\nsignificant modeling advantages, both computation of the exact likeli-\nhood and exact sampling from these models are generally intractable,\nwhich makes training of EBMs especially difficult and limits their appli-\ncations. Moreover, the sequential nature of speech and language also\n169\n170\nConclusion\npresents special challenges and needs treatment different from processing\nfix-dimensional data (e.g., images).\nWe are making progress. This monograph presents a systematic in-\ntroduction to energy-based models, including both algorithmic progress\nand applications in speech and language processing, which is organized\ninto four chapters.\nIn Chapter 2, we provide a self-contained introduction to basics\nfor EBMs, including model definitions, learning algorithms, and sam-\npling/generation algorithms. There are two remarkable features in our\nintroduction. First, we starts with the framework of probabilistic graphi-\ncal models (PGMs). The PGM framework enables readers to appreciate\nEBMs from the perspective of undirected graphical models and easily\nunderstand the differences between undirected graphical models and\ndirected graphical models, and between globally-normalized and locally-\nnormalized. Graphical models provide a natural tool for formulating\nvariations on classical methods, as well as for exploring entirely new\nmodels. Second, our introduction to the stochastic approximation (SA)\nmethodology is very useful for readers to develop new algorithms for\nlearning EBMs, particularly learning with Monte Carlo sampling. The\nSA methodology is more general than the ordinary stochastic gradient\ndescent (SGD) technique, while SGD is only one instance of SA.\nThe next three chapters are dedicated to present how to apply EBMs\nin three different scenarios, i.e., for modeling marginal, conditional\nand joint distributions, respectively. As visualized in Figure 1.2, our\nsuch organization is comprehensive and distinctive. A wide range of\napplications in speech and language processing are covered, including\nlanguage modeling, representation learning over text, speech recognition,\nsequence labeling in NLP, text generation, semi-supervised learning,\nand calibrated natural language understanding.\n6.2\nFuture challenges and directions\nEBM based methods represent an important class for the probabilistic\napproach to many fields. Despite the progress achieved in these years,\nmuch more work needs to be carried out.\n6.2. FUTURE CHALLENGES AND DIRECTIONS\n171\n• Applications of EBMs are still mainly limited by lack of\neffective and efficient training techniques. Training tech-\nniques are crucial to problem solving with EBMs, and will remain\nan active direction for future research. EBMs have been especially\ndifficult to train. We mainly introduce maximum likelihood train-\ning with MCMC sampling, noise-contrastive estimation (NCE)\nand briefly covers score matching (SM). There has been persis-\ntent and ongoing interest in developing effective modeling and\ntraining techniques for learning EBMs (see Zhang et al., 2023\nand its references), but those methods are mostly first studied\nfor fix-dimensional data. There is vast room to improve training\ntechniques for EBMs, especially for sequential data such as speech\nand language.\nMoreover, as we have discussed, learning EBMs over discrete\ndata are more challenging than over continuous data, due to the\ncombinatorial explosion of the state space. Langevin sampling,\na particular MCMC sampling method which utilizes gradients,\nhas been dominantly used in training EBMs over continuous data.\nA recent progress in (Grathwohl et al., 2021; Qin et al., 2022)\nbegins to use gradients of the likelihood function with respect to\nits discrete inputs to propose updates in a Metropolis-Hastings\nsampler. More further research is needed.\n• More downstream applications of EBMs are worthwhile\nfor exploration. Considering the potential advantages of EBMs\nin modeling flexibility, overcoming label bias and exposure bias,\nand hybrid generative-discriminative and semi-supervised learning,\nthe results reviewed in this monograph are only preliminary. There\nare many interesting tasks which could be approached by EBMs.\nThe applications of EBMs require more skill and experience, apart\nfrom using the mainstream deep learning toolkits. To help the\nreaders to get familiar with the techniques for developing and\napplying energy-based models, we summarize some open-source\ntoolkits in Appendix C.\n• We look forward to more foundational and interdisci-\n172\nConclusion\nplinary research around EBMs. There is an opportunity for\ndeveloping physics-based methods that could address the difficulty\nof calculating or sampling the partition function of EBMs (Huem-\nbeli et al., 2022). There are emergent areas of research at the\ninterfaces of machine learning, quantum computing, many-body\nphysics, and optimization (see references in Huembeli et al., 2022).\nMany recent biologically plausible algorithms utilize the framework\nof energy-based models (Scellier and Bengio, 2017; Millidge et\nal., 2023). Biologically plausible algorithms compute gradients\nthat approximate those computed by back-propagation (BP), and\noperate in ways that more closely satisfy the constraints imposed\nby neural circuitry. We anticipate that progress in neuroscience\nand EBM based machine learning will benefit from an interplay\nbetween both fields.\nAcknowledgements\nThanks to collaborators and students: Zhiqiang Tan, Bin Wang, Hongyu\nXiang, Yunfu Song, Kai Hu, Keyu An, Huahuan Zheng, Silin Gao, Hong\nLiu, Junlan Feng, and Yi Huang.\nThanks for funding supports from\n• NSFC (National Science Foundation of China) through No. 60402029,\nNo. 61075020, No. 61473168, and No. 61976122;\n• Ministry of Education and China Mobile joint funding through\nNo. MCM20170301;\n• Joint Institute of Tsinghua University - China Mobile Communi-\ncations Group Co. Ltd.;\n• Beijing National Research Center for Information Science and\nTechnology;\n• Tsinghua Initiative through No. 20121088069 and No. 20141081250;\n• Toshiba Corporation;\n• Apple Corporation.\n173\nAppendices\nA\nNotations and definitions\nA.1\nNotations\nExample\nDescription\nzi:j\nFor any generic sequence {zn}, we shall use zi:j to\ndenote zi, zi+1, · · · zj. Similarly, wherever a collection\nof indices appears in the subscript, we refer to the\ncorresponding collection of indexed variables, e.g.,\ncl,1:H ≜{cl,1, cl,2, · · · cl,H}.\nx\nx generally denotes a random variable, which can ei-\nther be scalar- or vector-valued, and often denotes the\nobservation variable. For simplicity, we also use the\nsame notation x to denote the values taken by the\nrandom variable x, e.g., in the argument of its density\nfunction, which should be clear from the context.\nh\nThe hidden variable.\ny\nThe class label, or the output variable.\n|B|\nThe cardinality/size of a set B\nxT , AT\nA superscript T denotes the transpose of a vector x or\nmatrix A\n∆K\nThe K-dimensional probability simplex.\n175\n176\nNotations and definitions\nP\nx f(x)\nThe summation over x is a shorthand, which should be\nan appropriate combination of summation and integra-\ntion, depending on the components of x being discrete\nvariables, continuous variables, or a combination of the\ntwo.\npora(·)\nThe (unknown) oracle density, sometimes also known\nas the data distribution and denoted as pdata(·).\npemp(·)\nThe empirical density. For a training dataset consisting\nof n independent and identically distributed (IID) data\npoints {x1, · · · , xN}, we have\npemp(x) ≜1\nN\nN\nX\ni=1\nδ(x −xi)\npθ(·), p(·; θ)\nThe (target) model density, parameterized by θ.\nqϕ(·), q(·; ϕ)\nThe auxiliary density introduced in training, parame-\nterized by ϕ.\nUni[a, b]\nUniform distribution for a continuous variable over\ninterval [a, b], or for a discrete variable over integers\nfrom a to b.\nA.2\nDefinitions\nTerm\nDescription\nσ(v)\nThe sigmoid function, σ(v) ≜\n1\n1+e−v , also called\nthe logistic sigmoid function. It is also known as a\nsquashing function, since it maps the whole real line\nto [0, 1], which is necessary for the output to be\ninterpreted as a probability.\nA.2. DEFINITIONS\n177\nlogit(σ)\nThe logit function, logit(σ) ≜log(\nσ\n1−σ) for 0 < σ <\n1, also known as the inverse of the sigmoid function.\nIt represents the log of the ratio of probabilities for\ntwo classes, also known as the log odds.\nsoftmax(z1:K)\nThe\nsoftmax\nfunction,\nsoftmax(z1:K)k\n≜\nexp(zk)\nPK\nj=1 exp(zj), which realizes normalization from RK\nto ∆K (the K-dimensional probability simplex). It\nis also known as the normalized exponential and\ncan be regarded as a multiclass generalization of the\nlogistic sigmoid.\nδ(x = a)\nAn indicator function of x which takes the value 1\nwhen x = a and 0 otherwise.\nH[q]\nThe entropy is defined as H[q] ≜−\nR qlogq.\nKL[p||q]\nThe inclusive KL-divergence between two distri-\nbutions p(·) and q(·) is defined as KL[p||q] ≜\nR plog\n\u0010\np\nq\n\u0011\n, which by default is called the KL-\ndivergence, and is sometimes also referred to as the\nforward KL-divergence, relative entropy.\nKL[q||p]\nThe exclusive KL-divergence is defined as KL[q||p] ≜\nR qlog\n\u0010\nq\np\n\u0011\n, which is also referred to as the reverse\nKL-divergence.\nB\nBackground material\nB.1\nMaximum entropy models\nTheorem B.1. When confronted by a probability distribution p(x)\nabout which only a few facts are known, the maximum entropy principle\n(maxent) offers a rule for choosing a distribution that satisfies those\nconstraints (Cover, 1999; MacKay, 2003). According to maxent, one\nshould select the p(x) that maximizes the entropy\nH(p) = −\nX\nx\np(x) log p(x)\n(B.1)\nsubject to the constraints. When there is a reference distribution q(x),\none should select the p(x) that minimizes the relative entropy or\nKullback-Leibler divergence1\nKL(p||q) =\nX\nx\np(x) log p(x)\nq(x)\n(B.2)\nAssuming the constraints assert that the averages of certain functions\nfk(x) are known, i.e.,\nEp(x) [fk(x)] = Fk, k = 1, 2, · · ·\n(B.3)\nThen, it can be shown that by introducing Lagrange multipliers (one\nfor each constraint, including normalization),\n• The distribution that maximizes the entropy has the following\nform\np∗(x) = 1\nZ exp\n X\nk\nwkfk(x)\n!\n(B.4)\n1When q(x) is uniform, this is the same as maximizing the entropy.\n178\nB.2. FISHER EQUALITY\n179\n• The distribution that minimizing relative entropy relative to q(x),\nhas the following form\np∗(x) = 1\nZ q(x) exp\n X\nk\nwkfk(x)\n!\n(B.5)\nwhere {wk} are set such that the constraints Eq. (B.3) are satisfied, and\nZ is the normalizing constant. The two forms in Eq. (B.4) and Eq. (B.5)\nare often collectively referred to as maximum entropy distributions.\nTheorem B.1 gives the form of maximum entropy distributions\nthat satisfy certain moment constraints. In an opposite way, when\ngiven that a distribution satisfies the form of Eq. (B.4) or Eq. (B.5),\nthe following theorem establish the connection between the maximum\nentropy distribution and the maximum likelihood distribution.\nTheorem B.2. Assume that a variable x comes from a probability\ndistribution of the form in Eq. (B.4) or Eq. (B.5), where the functions\nfk(x) are given, and the parameters {wk} are not known. A dataset\n{x(n)} is supplied. Then, it can be shown that by differentiating the log\nlikelihood, the maximum-likelihood (ML) parameters wML satisfy\nEp(x) [fk(x)] = 1\nN\nX\nn\nfk(x(n)), k = 1, 2, · · ·\n= Epemp(x) [fk(x)]\n(B.6)\nwhere the left-hand is the model average under the fitted model, the\nright-hand the empirical average over the training data points, and\npemp(·) denotes the empirical density over the training data points.\nCombining the above two theorems, we can easily see that maximum\nentropy fitting with Fk’s being set as the empirical averages is equivalent\nto maximum likelihood fitting of a log-linear distribution (MacKay, 2003;\nPietra et al., 1997).\nB.2\nFisher equality\nFormally, for any density function pθ(x), the partial derivative w.r.t.\nθ of the log density function,\n∂\n∂θlogpθ(x), is called the “score”. Under\n180\nBackground material\ncertain regularity conditions, the expectation of the score w.r.t. the\ndensity itself is 0. This formula is often referred in presenting Fisher\ninformation2, so we call it Fisher equality, which, is frequently used in\nthis monograph.\nEpθ(x)\n\u0014 ∂\n∂θ log pθ(x)\n\u0015\n= 0.\n(B.7)\nFurther, based on the above basic Fisher equality, we have the\nfollowing very useful theorem.\nTheorem B.3. Consider any latent-variable model pθ(x, h), which con-\nsisting of observation x and latent variable h, then we have\n∂\n∂θ log pθ(x) = Epθ(h|x)\n\u0014 ∂\n∂θ log pθ(x, h)\n\u0015\n(B.8)\nwhich means that the gradient of the log marginal likelihood is equal to\nthe expected log joint likelihood, where the expectation is taken over\nthe posteriori distribution.\nProof.\n∂\n∂θ log pθ(x) = Epθ(h|x)\n\u0014 ∂\n∂θ log pθ(x)\n\u0015\n= Epθ(h|x)\n\u0014 ∂\n∂θ log pθ(x, h) −∂\n∂θ log pθ(h|x)\n\u0015\n= Epθ(h|x)\n\u0014 ∂\n∂θ log pθ(x, h)\n\u0015\nwhere in the second line, according to Fisher equality, we have\nEpθ(h|x)\n\u0014 ∂\n∂θ log pθ(h|x)\n\u0015\n= 0,\nand thus we obtain the final line. For simplicity, Eq. (B.8) is also referred\nto as Fisher equality.\n■\n2https://en.wikipedia.org/wiki/Fisher_information\nC\nOpen-source toolkits related to EBMs\n• Trans-dimensional random field (TRF) LMs: https://github.com/\nthu-spmi/SPMILM\n• Energy-based cloze models for representation learning over text\n(Electric): https://github.com/google-research/electra\n• CRF-based ASR Toolkit (CAT): https://github.com/thu-spmi/\nCAT\n• Neural CRF Transducers for Sequence Labeling: https://github.\ncom/thu-spmi/SPMISeq\n• Controlled text generation from pre-trained language models (mix-\nand-match): https://github.com/mireshghallah/mixmatch\n• Learning neural random fields with inclusive auxiliary generators:\nhttps://github.com/thu-spmi/Inclusive-NRF\n• JEMs and JRFs for semi-supervised learning: https://github.com/\nthu-spmi/semi-EBM\n181\nReferences\nAmaya, F. and J. M. Benedi. (2001). “Improvement of a whole sentence\nmaximum entropy language model using grammatical features”. In:\nProc. Ann. Meeting of the Association for Computational Linguistics\n(ACL).\nAn, K., H. Xiang, and Z. Ou. (2020). “CAT: A CTC-CRF based ASR\nToolkit Bridging the Hybrid and the End-to-end Approaches towards\nData Efficiency and Low Latency”. In: INTERSPEECH.\nAn, K., H. Zheng, Z. Ou, H. Xiang, K. Ding, and G. Wan. (2022).\n“Cuside: Chunking, simulating future context and decoding for\nstreaming ASR”. In: INTERSPEECH.\nAndor, D., C. Alberti, D. Weiss, A. Severyn, A. Presta, K. Ganchev,\nS. Petrov, and M. Collins. (2016). “Globally Normalized Transition-\nBased Neural Networks”. In: Proceedings of the 54th Annual Meeting\nof the Association for Computational Linguistics (Volume 1: Long\nPapers).\nAndrieu, C., É. Moulines, and P. Priouret. (2005). “Stability of stochastic\napproximation under verifiable conditions”. SIAM Journal on control\nand optimization. 44(1): 283–312.\nAndrieu, C. and J. Thoms. (2008). “A tutorial on adaptive MCMC”.\nStatistics and computing. 18(4): 343–373.\nArgyriou, A., T. Evgeniou, and M. Pontil. (2007). “Multi-task feature\nlearning”. In: NIPS.\n182\nREFERENCES\n183\nArtieres, T. et al. (2010). “Neural conditional random fields”. In: AIS-\nTATS.\nBakhtin, A., S. Gross, M. Ott, Y. Deng, M. Ranzato, and A. Szlam.\n(2019). “Real or fake? learning to discriminate machine from human\ngenerated text”. arXiv preprint arXiv:1906.03351.\nBelanger, D. and A. McCallum. (2016). “Structured Prediction Energy\nNetworks”. In: ICML.\nBengio, S., O. Vinyals, N. Jaitly, and N. Shazeer. (2015). “Scheduled\nsampling for sequence prediction with recurrent neural networks”.\nAdvances in neural information processing systems.\nBenveniste, A., M. Métivier, and P. Priouret. (1990). Adaptive algorithms\nand stochastic approximations. New York: Springer.\nBesag, J. E. (1994). “Comments on “Representations of knowledge in\ncomplex systems” by U. Grenander and M.I. Miller”. Journal of the\nRoyal Statistical Society: Series B. 56: 549–581.\nBishop, C. M. (2006). Pattern recognition and machine learning. Springer.\nBouchard, G. (2007). “Bias-variance tradeoff in hybrid generative-\ndiscriminative models”. In: International Conference on Machine\nLearning and Applications (ICMLA).\nBu, H., J. Du, X. Na, B. Wu, and H. Zheng. (2017). “AISHELL-1:\nAn open-source Mandarin speech corpus and a speech recognition\nbaseline”. In: 2017 20th Conference of the Oriental Chapter of the\nInternational Coordinating Committee on Speech Databases and\nSpeech I/O Systems and Assessment (O-COCOSDA).\nChatzis, S. P. and Y. Demiris. (2013). “The Infinite-Order Conditional\nRandom Field Model for Sequential Data Modeling”. IEEE Trans-\nactions on Pattern Analysis and Machine Intelligence.\nChelba, C., T. Mikolov, M. Schuster, Q. Ge, T. Brants, P. Koehn, and\nT. Robinson. (2014). “One Billion Word Benchmark for Measuring\nProgress in Statistical Language Modeling”. In: INTERSPEECH.\nChen, H. (2002). Stochastic approximation and its applications. Springer\nScience & Business Media.\nChen, S. F. and J. Goodman. (1999). “An empirical study of smoothing\ntechniques for language modeling”. Computer Speech & Language.\n13(4): 359–394.\n184\nREFERENCES\nChen, T., E. Fox, and C. Guestrin. (2014). “Stochastic gradient Hamil-\ntonian Monte Carlo”. In: ICML.\nChen, T., S. Kornblith, M. Norouzi, and G. Hinton. (2020). “A sim-\nple framework for contrastive learning of visual representations”.\narXiv:2002.05709.\nChen, X., X. Liu, Y. Wang, A. Ragni, J. H. Wong, and M. J. Gales.\n(2019). “Exploiting Future Word Contexts in Neural Network Lan-\nguage Models for Speech Recognition”. IEEE/ACM Transactions\non Audio, Speech, and Language Processing. 27(9): 1444–1454.\nChiu, C.-C., T. N. Sainath, Y. Wu, R. Prabhavalkar, P. Nguyen, Z. Chen,\nA. Kannan, R. J. Weiss, K. Rao, E. Gonina, et al. (2018). “State-of-\nthe-art speech recognition with sequence-to-sequence models”. In:\nICASSP.\nChorowski, J., D. Bahdanau, K. Cho, and Y. Bengio. (2014). “End-to-\nend continuous speech recognition using attention-based recurrent\nNN: First results”. arXiv preprint arXiv:1412.1602.\nClark, K., M.-T. Luong, Q. V. Le, and C. D. Manning. (2020a). “Electra:\nPre-training text encoders as discriminators rather than generators”.\nIn: International Conference on Learning Representations (ICLR).\nClark, K., M.-T. Luong, Q. V. Le, and C. D. Manning. (2020b). “Pre-\nTraining Transformers as Energy-Based Cloze Models”. Conference\non Empirical Methods in Natural Language Processing (EMNLP).\nClark, K., M.-T. Luong, C. D. Manning, and Q. Le. (2018). “Semi-\nSupervised Sequence Modeling with Cross-View Training”. In: EMNLP.\nCollins, M. and B. Roark. (2004). “Incremental Parsing with the Per-\nceptron Algorithm”. In: ACL.\nCollobert, R., J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P.\nKuksa. (2011). “Natural language processing (almost) from scratch”.\nJournal of machine learning research. 12(Aug): 2493–2537.\nCover, T. M. (1999). Elements of information theory. John Wiley &\nSons.\nCowell, R. G., A. P. Dawid, S. L. Lauritzen, and D. J. Spiegelhalter.\n(1999). Probabilistic Networks and Expert Systems. Springer-Verlag.\nCubuk, E. D., B. Zoph, J. Shlens, and Q. V. Le. (2020). “RandAug-\nment: Practical automated data augmentation with a reduced search\nspace”. In: CVPR.\nREFERENCES\n185\nCui, X., B. Kingsbury, G. Saon, D. Haws, and Z. Tuske. (2021). “Reduc-\ning exposure bias in training recurrent neural network transducers”.\nIn: INTERSPEECH.\nDahl, G. E., D. Yu, L. Deng, and A. Acero. (2012). “Context-dependent\npre-trained deep neural networks for large-vocabulary speech recogni-\ntion”. IEEE Transactions on audio, speech, and language processing.\n20(1): 30–42.\nDai, Z., Z. Yang, F. Yang, W. W. Cohen, and R. R. Salakhutdinov.\n(2017). “Good semi-supervised learning that requires a bad GAN”.\nIn: NIPS.\nDayan, P., G. E. Hinton, R. M. Neal, and R. S. Zemel. (1995). “The\nhelmholtz machine”. Neural computation. 7(5): 889–904.\nDempster, A. P., N. M. Laird, and D. B. Rubin. (1977). “Maximum\nlikelihood from incomplete data via the EM algorithm”. Journal of\nthe Royal Statistical Society. 39.\nDeng, Y., A. Bakhtin, M. Ott, A. Szlam, and M. Ranzato. (2020).\n“Residual energy-based models for text generation”. In: ICLR.\nDevlin, J., M.-W. Chang, K. Lee, and K. Toutanova. (2018). “Bert:\nPre-training of deep bidirectional transformers for language under-\nstanding”. arXiv preprint arXiv:1810.04805: 4171–4186.\nDurrett, G. and D. Klein. (2015). “Neural CRF Parsing”. In: ACL.\nFrey, B. J. and N. Jojic. (2005). “A comparison of algorithms for\ninference and learning in probabilistic graphical models”. IEEE\nTrans. Pattern Analysis and Machine Intelligence (PAMI). 27(9):\n1392–1416.\nGao, S., Z. Ou, W. Yang, and H. Xu. (2020). “Integrating Discrete\nand Neural Features Via Mixed-Feature Trans-Dimensional Random\nField Language Models”. In: ICASSP.\nGhazvininejad, M., O. Levy, Y. Liu, and L. Zettlemoyer. (2019). “Mask-\npredict: Parallel decoding of conditional masked language models”.\narXiv preprint arXiv:1904.09324.\nGoodfellow, I., J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S.\nOzair, A. Courville, and Y. Bengio. (2014). “Generative adversarial\nnets”. In: NIPS.\nGoodman, J. (2001). “A bit of progress in language modeling”. Computer\nSpeech & Language. 15: 403–434.\n186\nREFERENCES\nGoyal, K., C. Dyer, and T. Berg-Kirkpatrick. (2022). “Exposing the\nImplicit Energy Networks behind Masked Language Models via\nMetropolis–Hastings”. In: International conference on learning rep-\nresentations.\nGrathwohl, W., K. Swersky, M. Hashemi, D. Duvenaud, and C. Maddi-\nson. (2021). “Oops I took a gradient: Scalable sampling for discrete\ndistributions”. In: International Conference on Machine Learning.\nGrathwohl, W., K.-C. Wang, J. Jacobsen, D. Duvenaud, M. Norouzi,\nand K. Swersky. (2020). “Your classifier is secretly an energy based\nmodel and you should treat it like one”. In: ICLR.\nGraves, A. (2012). “Sequence transduction with recurrent neural net-\nworks”. arXiv preprint arXiv:1211.3711.\nGraves, A., S. Fernández, F. Gomez, and J. Schmidhuber. (2006). “Con-\nnectionist Temporal Classification: Labelling Unsegmented Sequence\nData with Recurrent Neural Networks”. In: ICML.\nGunawardana, A., M. Mahajan, A. Acero, and J. C. Platt. (2005).\n“Hidden conditional random fields for phone classification”. In: Ninth\nEuropean Conference on Speech Communication and Technology\n(EUROSPEECH).\nGuo, C. E., S. C. Zhu, and Y. N. Wu. (2003). “Modeling Visual Patterns\nby Integrating Descriptive and Generative Methods.” International\nJournal of Computer Vision. 53(1): 5–29.\nGuo, C., G. Pleiss, Y. Sun, and K. Q. Weinberger. (2017). “On Cali-\nbration of Modern Neural Networks”. In: Proceedings of the 34th\nInternational Conference on Machine Learning.\nGutmann, M. and A. Hyvärinen. (2010). “Noise-contrastive estimation:\nA new estimation principle for unnormalized statistical models”. In:\nAISTATS.\nGutmann, M. U. and A. Hyvärinen. (2012). “Noise-Contrastive Esti-\nmation of Unnormalized Statistical Models, with Applications to\nNatural Image Statistics.” Journal of machine learning research.\n13(2).\nHadian, H., H. Sameti, D. Povey, and S. Khudanpur. (2018). “Flat-\nStart Single-Stage Discriminatively Trained HMM-Based Models for\nASR”. IEEE/ACM Transactions on Audio, Speech, and Language\nProcessing. 26(11): 1949–1961.\nREFERENCES\n187\nHan, T., E. Nijkamp, X. Fang, M. Hill, S.-C. Zhu, and Y. N. Wu.\n(2019). “Divergence Triangle for Joint Training of Generator Model,\nEnergy-based Model, and Inferential Model”. In: CVPR.\nHastie, T., R. Tibshirani, J. H. Friedman, and J. H. Friedman. (2009).\nThe elements of statistical learning: data mining, inference, and\nprediction. Vol. 2. Springer.\nHe, T., B. McCann, C. Xiong, and E. Hosseini-Asl. (2021). “Joint\nenergy-based model training for better calibrated natural language\nunderstanding models”. preprint arXiv:2101.06829.\nHinton, G. E. (2002). “Training products of experts by minimizing\ncontrastive divergence”. Neural computation. 14(8): 1771–1800.\nHinton, G. E., P. Dayan, B. J. Frey, and R. M. Neal. (1995). “The\nwake-sleep algorithm for unsupervised neural networks.” Science.\n268(5214): 1158–1161.\nHinton, G. E., S. Osindero, and Y. W. Teh. (2006). “A fast learning\nalgorithm for deep belief nets”. Neural Computation. 18(7): 1527–\n1554.\nHoltzman, A., J. Buys, L. Du, M. Forbes, and Y. Choi. (2019). “The cu-\nrious case of neural text degeneration”. In: International Conference\non Learning Representations (ICLR).\nHu, K., Z. Ou, M. Hu, and J. Feng. (2019). “Neural CRF transducers\nfor sequence labeling”. In: ICASSP.\nHuang, Z., W. Xu, and K. Yu. (2015). “Bidirectional LSTM-CRF models\nfor sequence tagging”. arXiv:1508.01991.\nHuembeli, P., J. M. Arrazola, N. Killoran, M. Mohseni, and P. Wittek.\n(2022). “The physics of energy-based models”. Quantum Machine\nIntelligence. 4(1): 1.\nHyvärinen, A. and P. Dayan. (2005). “Estimation of non-normalized\nstatistical models by score matching”. Journal of Machine Learning\nResearch. 6(4).\nJelinek, F. (1976). “Continuous speech recognition by statistical meth-\nods”. Proceedings of the IEEE. 64(4): 532–556.\nJordan, M. I., Z. Ghahramani, T. S. Jaakkola, and L. K. Saul. (1999).\n“An introduction to variational methods for graphical models”. Ma-\nchine learning. 37: 183–233.\n188\nREFERENCES\nJordan, M. I. (2004). “Graphical models”. Statistical science. 19(1):\n140–155.\nKhalifa, M., H. Elsahar, and M. Dymetman. (2021). “A distributional\napproach to controlled text generation”. In: International conference\non learning representations.\nKim, K., J. Oh, J. Gardner, A. B. Dieng, and H. Kim. (2022). “Markov\nchain score ascent: A unifying framework of variational inference with\nMarkovian gradients”. Advances in Neural Information Processing\nSystems (NeurIPS).\nKim, T. and Y. Bengio. (2016). “Deep directed generative models with\nenergy-based probability estimation”. In: ICLR Workshop.\nKingma, D. P., M. Welling, et al. (2019). “An introduction to varia-\ntional autoencoders”. Foundations and Trends® in Machine Learn-\ning. 12(4): 307–392.\nKingma, D. P., D. J. Rezende, S. Mohamed, and M. Welling. (2014).\n“Semi-Supervised Learning with Deep Generative Models”. In: NIPS.\nKoller, D. and N. Friedman. (2009). Probabilistic graphical models:\nprinciples and techniques. MIT press.\nKuleshov, V. and S. Ermon. (2017). “Neural Variational Inference and\nLearning in Undirected Graphical Models”. In: NIPS.\nLafferty, J., A. McCallum, and F. C. Pereira. (2001). “Conditional\nrandom fields: Probabilistic models for segmenting and labeling\nsequence data”. In: International conference on Machine learning\n(ICML).\nLaine, S. and T. Aila. (2017). “Temporal Ensembling for Semi-Supervised\nLearning”. In: ICLR.\nLample, G., M. Ballesteros, S. Subramanian, K. Kawakami, and C.\nDyer. (2016). “Neural Architectures for Named Entity Recognition”.\nIn: NAACL-HLT.\nLarochelle, H., M. I. Mandel, R. Pascanu, and Y. Bengio. (2012). “Learn-\ning algorithms for the classification restricted Boltzmann machine”.\nJournal of Machine Learning Research. 13(1): 643–669.\nLiang, F., C. Liu, and R. J. Carroll. (2007). “Stochastic approximation\nin Monte Carlo computation”. Journal of the American Statistical\nAssociation. 102(477): 305–320.\nREFERENCES\n189\nLiang, P. and M. I. Jordan. (2008). “An asymptotic analysis of genera-\ntive, discriminative, and pseudolikelihood estimators”. In: Interna-\ntional conference on Machine learning (ICML). 584–591.\nLing, W., C. Dyer, A. W. Black, I. Trancoso, R. Fermandez, S. Amir, L.\nMarujo, and T. Luis. (2015). “Finding Function in Form: Composi-\ntional Character Models for Open Vocabulary Word Representation”.\nIn: EMNLP.\nLiu, H. and Z. Ou. (2023). “Exploring Energy-based Language Mod-\nels with Different Architectures and Training Methods for Speech\nRecognition”. In: INTERSPEECH.\nLiu, J. S. (2001). Monte Carlo strategies in scientific computing. Vol. 10.\nSpringer.\nLiu, Y., M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,\nL. Zettlemoyer, and V. Stoyanov. (2019). “RoBERTa: A Robustly\nOptimized BERT Pretraining Approach”. ArXiv. abs/1907.11692.\nLu, L., L. Kong, C. Dyer, N. A. Smith, and S. Renals. (2016). “Segmental\nRecurrent Neural Networks for End-to-End Speech Recognition”.\nIn: INTERSPEECH.\nLüscher, C., E. Beck, K. Irie, M. Kitza, W. Michel, A. Zeyer, R. Schlüter,\nand H. Ney. (2019). “RWTH ASR Systems for LibriSpeech: Hybrid\nvs Attention”. In: INTERSPEECH.\nMa, Y.-A., T. Chen, and E. Fox. (2015). “A complete recipe for stochas-\ntic gradient MCMC”. In: NIPS.\nMa, X. and E. Hovy. (2016). “End-to-end Sequence Labeling via Bi-\ndirectional LSTM-CNNs-CRF”. In: ACL.\nMa, Z. and M. Collins. (2018). “Noise contrastive estimation and neg-\native sampling for conditional models: Consistency and statistical\nefficiency”. EMNLP.\nMacKay, D. J. (2003). Information theory, inference and learning algo-\nrithms. Cambridge university press.\nMarcus, M., B. Santorini, and M. A. Marcinkiewicz. (1993). “Building\na large annotated corpus of English: The Penn Treebank”.\nMartin, S., J. Liermann, and H. Ney. (1998). “Algorithms for bigram\nand trigram word clustering”. Speech Communication. 24: 19–37.\n190\nREFERENCES\nMcCallum, A., D. Freitag, and F. Pereira. (2000). “Maximum entropy\nMarkov models for information extraction and segmentation.” In:\nICML.\nMesnil, G., Y. Dauphin, K. Yao, Y. Bengio, L. Deng, D. Z. Hakkani-Tür,\nX. He, L. P. Heck, G. Tür, D. Yu, and G. Zweig. (2015). “Using\nRecurrent Neural Networks for Slot Filling in Spoken Language Un-\nderstanding”. IEEE/ACM Trans. on Audio, Speech, and Language\nProcessing. 23: 530–539.\nMiao, N., H. Zhou, L. Mou, R. Yan, and L. Li. (2019). “CGMH: Con-\nstrained sentence generation by metropolis-hastings sampling”. In:\nProceedings of the AAAI Conference on Artificial Intelligence.\nMiao, Y., M. Gowayyed, and F. Metze. (2015). “EESEN: End-to-end\nspeech recognition using deep RNN models and WFST-based de-\ncoding”. In: ASRU.\nMikolov, T., S. Kombrink, L. Burget, J. H. Cernocky, and S. Khudanpur.\n(2011). “Extensions of recurrent neural network language model”. In:\nInternational Conference on Acoustics, Speech and Signal Processing\n(ICASSP).\nMikolov, T., I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. (2013).\n“Distributed representations of words and phrases and their compo-\nsitionality”. In: Advances in neural information processing systems.\n3111–3119.\nMillidge, B., Y. Song, T. Salvatori, T. Lukasiewicz, and R. Bogacz.\n(2023). “Backpropagation at the infinitesimal inference limit of\nenergy-based models: unifying predictive coding, equilibrium propa-\ngation, and contrastive hebbian learning”. In: International Confer-\nence on Machine Learning.\nMinka, T. (2005). “Divergence measures and message passing”. Microsoft\nResearch Technical Report.\nMireshghallah, F., K. Goyal, and T. Berg-Kirkpatrick. (2022). “Mix and\nMatch: Learning-free Controllable Text Generation using Energy\nLanguage Models”. In: Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Papers).\nREFERENCES\n191\nMiyato, T., S.-i. Maeda, M. Koyama, and S. Ishii. (2018). “Virtual\nadversarial training: a regularization method for supervised and\nsemi-supervised learning”. IEEE transactions on pattern analysis\nand machine intelligence. 41(8): 1979–1993.\nMohri, M., F. Pereira, and M. Riley. (2008). “Speech recognition with\nweighted finite-state transducers”. In: Springer Handbook of Speech\nProcessing. Springer. 559–584.\nMorency, L.-P., A. Quattoni, and T. Darrell. (2007). “Latent-Dynamic\nDiscriminative Models for Continuous Gesture Recognition”. In:\nCVPR.\nMroueh, Y., C.-L. Li, T. Sercu, A. Raj, and Y. Cheng. (2018). “Sobolev\nGAN”. In: ICLR.\nMurphy, K. P. (2012). Machine learning: a probabilistic perspective. MIT\npress.\nNaesseth, C., F. Lindsten, and D. Blei. (2020). “Markovian score climb-\ning: Variational inference with KL (p|| q)”. Advances in Neural\nInformation Processing Systems (NeurIPS).\nNeal, R. M. (1993). Probabilistic inference using Markov chain Monte\nCarlo methods. Department of Computer Science, University of\nToronto, Canada.\nNeal, R. M. (2011). “MCMC using Hamiltonian dynamics”. Handbook\nof Markov Chain Monte Carlo.\nNeal, R. M. and G. E. Hinton. (1998). “A view of the EM algorithm\nthat justifies incremental, sparse, and other variants”. In: Learning\nin graphical models. Springer. 355–368.\nNeal, R. M. (1992). “Connectionist Learning of Belief Networks”. Arti-\nficial Intelligence. 56: 71–113.\nNg, A. and M. Jordan. (2001). “On discriminative vs. generative classi-\nfiers: A comparison of logistic regression and naive bayes”. Advances\nin neural information processing systems. 14.\nNgiam, J., Z. Chen, P. W. Koh, and A. Y. Ng. (2011). “Learning deep\nenergy models”. In: International conference on machine learning\n(ICML).\nNowozin, S. (2018). “Debiasing evidence approximations: On importance-\nweighted autoencoders and jackknife variational inference”. In: In-\nternational conference on learning representations.\n192\nREFERENCES\nOliver, A., A. Odena, C. Raffel, E. D. Cubuk, and I. J. Goodfellow.\n(2018). “Realistic evaluation of semi-supervised learning algorithms”.\nIn: ICLR.\nOstendorf, M. (2016). “Continuous-Space Language Processing: Beyond\nWord Embeddings”. In: International Conference on Statistical Lan-\nguage and Speech Processing.\nOu, Z. (2018). “A review of learning with deep generative models from\nperspective of graphical modeling”. arXiv preprint arXiv:1808.01630.\nOu, Z. and Y. Song. (2020). “Joint stochastic approximation and its ap-\nplication to learning discrete latent variable models”. In: Conference\non Uncertainty in Artificial Intelligence. PMLR. 929–938.\nOu, Z. and J. Xiao. (2010). “A study of large vocabulary speech recog-\nnition decoding using finite-state graphs”. In: The 7th International\nSymposium on Chinese Spoken Language Processing.\nPanayotov, V., G. Chen, D. Povey, and S. Khudanpur. (2015). “Lib-\nrispeech: an asr corpus based on public domain audio books”. In:\nIEEE international conference on acoustics, speech and signal pro-\ncessing (ICASSP).\nParshakova, T., J.-M. Andreoli, and M. Dymetman. (2019). “Global\nautoregressive models for data-efficient sequence learning”. arXiv\npreprint arXiv:1909.07063.\nPearl, J. (1988). Probabilistic reasoning in intelligent systems: networks\nof plausible inference. Morgan kaufmann.\nPeng, J., L. Bo, and J. Xu. (2009). “Conditional Neural Fields”. In:\nNIPS.\nPennington, J., R. Socher, and C. Manning. (2014). “Glove: Global vec-\ntors for word representation”. In: Conference on empirical methods\nin natural language processing (EMNLP). 1532–1543.\nPietra, S. D., V. D. Pietra, and J. Lafferty. (1997). “Inducing Features\nof Random Fields”. IEEE Trans. Pattern Analysis and Machine\nIntelligence (PAMI). 19: 380–393.\nPopov, V., I. Vovk, V. Gogoryan, T. Sadekova, and M. Kudinov. (2021).\n“Grad-TTS: A diffusion probabilistic model for text-to-speech”. In:\nInternational Conference on Machine Learning. PMLR. 8599–8608.\nREFERENCES\n193\nPovey, D., V. Peddinti, D. Galvez, P. Ghahremani, V. Manohar, X. Na,\nY. Wang, and S. Khudanpur. (2016). “Purely Sequence-Trained\nNeural Networks for ASR Based on Lattice-Free MMI”. In: INTER-\nSPEECH.\nQin, L., S. Welleck, D. Khashabi, and Y. Choi. (2022). “Cold decoding:\nEnergy-based constrained text generation with langevin dynamics”.\nAdvances in Neural Information Processing Systems (NeurIPS).\nRabiner, L. R. (1989). “A tutorial on hidden Markov models and selected\napplications in speech recognition”. Proceedings of the IEEE. 77(2):\n257–286.\nRadford, A., K. Narasimhan, T. Salimans, and I. Sutskever. (2018).\n“Improving language understanding by generative pre-training”.\nRadford, A., J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et\nal. (2019). “Language models are unsupervised multitask learners”.\nOpenAI blog. 1(8): 9.\nRaffel, C., N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,\nY. Zhou, W. Li, and P. J. Liu. (2020). “Exploring the Limits of\nTransfer Learning with a Unified Text-to-Text Transformer”. Journal\nof Machine Learning Research. 21(140): 1–67.\nRajpurkar, P., J. Zhang, K. Lopyrev, and P. Liang. (2016). “Squad:\n100,000+ questions for machine comprehension of text”. In: EMNLP.\nRanzato, M., S. Chopra, M. Auli, and W. Zaremba. (2016). “Sequence\nlevel training with recurrent neural networks”. In: International\nConference on Learning Representations (ICLR).\nRasmus, A., H. Valpola, M. Honkala, M. Berglund, and T. Raiko. (2015).\n“Semi-supervised learning with Ladder networks”. In: NIPS.\nRen, Y., C. Hu, X. Tan, T. Qin, S. Zhao, Z. Zhao, and T.-Y. Liu. (2020).\n“Fastspeech 2: Fast and high-quality end-to-end text to speech”.\narXiv preprint arXiv:2006.04558.\nRobbins, H. and S. Monro. (1951). “A stochastic approximation method”.\nThe Annals of Mathematical Statistics: 400–407.\nRoberts, G. O. and J. S. Rosenthal. (2009). “Examples of adaptive\nMCMC”. Journal of Computational and Graphical Statistics. 18(2):\n349–367.\n194\nREFERENCES\nRoberts, G. O. and R. L. Tweedie. (1996). “Exponential convergence of\nLangevin distributions and their discrete approximations”. Bernoulli.\n2: 341–363.\nRosenfeld, R., S. F. Chen, and X. Zhu. (2001). “Whole-sentence exponen-\ntial language models: a vehicle for linguistic-statistical integration”.\nComputer Speech & Language. 15: 55–73.\nRuokolainen, T., T. Alumae, and M. Dobrinkat. (2010). “Using Depen-\ndency Grammar Features in Whole Sentence Maximum Entropy\nLanguage Model for Speech Recognition.” In: Baltic HLT.\nRussell, S. and P. Norvig. (2010). Artificial intelligence: a modern\napproach (3rd). Upper Saddle River, Prentice-Hall.\nSalakhutdinov, R. and G. Hinton. (2009). “Deep Boltzmann Machines”.\nJournal of Machine Learning Research. 5(2): 1967–2006.\nSalakhutdinov, R. (2009). “Learning Deep Generative Models”. Ph.D.\nthesis, University of Toronto.\nSalimans, T., I. Goodfellow, W. Zaremba, V. Cheung, A. Radford,\nand X. Chen. (2016). “Improved techniques for training GANs”. In:\nNIPS.\nSarawagi, S. and W. W. Cohen. (2004). “Semi-Markov Conditional\nRandom Fields for Information Extraction”. In: NIPS.\nSarikaya, R., S. F. Chen, A. Sethy, and B. Ramabhadran. (2010). “Im-\npact of word classing on shrinkage-based language models”. In:\nEleventh Annual Conference of the International Speech Communi-\ncation Association.\nSato, I. and H. Nakagawa. (2014). “Approximation Analysis of Stochas-\ntic Gradient Langevin Dynamics by using Fokker-Planck Equation\nand Ito Process”. In: ICML.\nSato, K. and Y. Sakakibara. (2005). “RNA secondary structural align-\nment with conditional random fields”. Bioinformatics. 21: 237–42.\nSaul, L. K., T. Jaakkola, and M. I. Jordan. (1996). “Mean field theory for\nsigmoid belief networks”. Journal of artificial intelligence research.\n4(1): 61–76.\nScellier, B. and Y. Bengio. (2017). “Equilibrium propagation: Bridg-\ning the gap between energy-based models and backpropagation”.\nFrontiers in computational neuroscience. 11: 24.\nREFERENCES\n195\nSchwenk, H. (2007). “Continuous space language models”. Computer\nSpeech & Language. 21: 492–518.\nScudder, H. (1965). “Probability of error of some adaptive pattern-\nrecognition machines”. IEEE Transactions on Information Theory.\n11(3): 363–371.\nShazeer, N., J. Pelemans, and C. Chelba. (2015). “Sparse Non-negative\nMatrix Language Modeling For Skip-grams”. In: INTERSPEECH.\nSøgaard, A. and Y. Goldberg. (2016). “Deep multi-task learning with\nlow level tasks supervised at lower layers”. In: ACL. 231–235.\nSohn, K., D. Berthelot, C.-L. Li, and et al. (2020). “FixMatch: Sim-\nplifying semi-supervised learning with consistency and confidence”.\narXiv:2001.07685.\nSong, Q., M. Wu, and F. Liang. (2014). “Weak convergence rates of\npopulation versus single-chain stochastic approximation MCMC\nalgorithms”. Advances in Applied Probability. 46(4): 1059–1083.\nSong, Y. and Z. Ou. (2018). “Learning neural random fields with inclu-\nsive auxiliary generators”. arXiv preprint arXiv:1806.00271.\nSong, Y., Z. Ou, Z. Liu, and S. Yang. (2020). “Upgrading CRFs to JRFs\nand its Benefits to Sequence Modeling and Labeling”. In: ICASSP.\nSong, Y., H. Zheng, and Z. Ou. (2021). “An empirical comparison of\njoint-training and pre-training for domain-agnostic semi-supervised\nlearning via energy-based models”. In: IEEE International Workshop\non Machine Learning for Signal Processing (MLSP).\nSpringenberg, J. T. (2016). “Unsupervised and Semi-supervised Learning\nwith Categorical Generative Adversarial Networks”. In: ICML.\nSrivastava, N., G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhut-\ndinov. (2014). “Dropout: a simple way to prevent neural networks\nfrom overfitting”. The journal of machine learning research.\nSun, W., Z. Tu, and A. Ragni. (2023). “Energy-Based Models For Speech\nSynthesis”. arXiv preprint arXiv:2310.12765.\nSundermeyer, M., R. Schlüter, and H. Ney. (2012). “LSTM Neural\nNetworks for Language Modeling.” In: INTERSPEECH. 194–197.\nSutskever, I., O. Vinyals, and Q. V. Le. (2014). “Sequence to sequence\nlearning with neural networks”. Advances in neural information\nprocessing systems. 27.\n196\nREFERENCES\nSutton, C., A. McCallum, et al. (2012). “An introduction to conditional\nrandom fields”. Foundations and Trends® in Machine Learning. 4(4):\n267–373.\nTan, Z. (2017). “Optimally adjusted mixture sampling and locally\nweighted histogram analysis”. Journal of Computational and Graph-\nical Statistics. 26: 54–65.\nTarvainen, A. and H. Valpola. (2017). “Mean teachers are better\nrole models: Weight-averaged consistency targets improve semi-\nsupervised deep learning results”. In: NIPS.\nTheis, L., A. V. Den Oord, and M. Bethge. (2016). “A note on the\nevaluation of generative models”. In: ICLR.\nTieleman, T. (2008). “Training restricted Boltzmann machines using\napproximations to the likelihood gradient”. In: ICML.\nToshniwal, S., A. Kannan, and et al. (2018). “A comparison of tech-\nniques for language model integration in encoder-decoder speech\nrecognition”. In: SLT.\nTu, L. and K. Gimpel. (2018). “Learning Approximate Inference Net-\nworks for Structured Prediction”. In: ICLR.\nTüske, Z., K. Audhkhasi, and G. Saon. (2019). “Advancing sequence-\nto-sequence based speech recognition”. In: INTERSPEECH.\nVariani, E., K. Wu, M. D. Riley, D. Rybach, M. Shannon, and C.\nAllauzen. (2022). “Global normalization for streaming speech recog-\nnition in a modular framework”. Advances in Neural Information\nProcessing Systems. 35: 4257–4269.\nVaswani, A., N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nŁ. Kaiser, and I. Polosukhin. (2017). “Attention is all you need”.\nAdvances in neural information processing systems.\nWainwright, M. J., M. I. Jordan, et al. (2008). “Graphical models,\nexponential families, and variational inference”. Foundations and\nTrends® in Machine Learning. 1(1–2): 1–305.\nWang, A. and K. Cho. (2019). “BERT has a Mouth, and It Must Speak:\nBERT as a Markov Random Field Language Model”. In: Proceedings\nof the Workshop on Methods for Optimizing and Evaluating Neural\nLanguage Generation.\nREFERENCES\n197\nWang, A., A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman.\n(2019). “GLUE: A multi-task benchmark and analysis platform for\nnatural language understanding”. In: International Conference on\nLearning Representations (ICLR).\nWang, B. (2018). “Statistical Language Models Based on Trans-dimensional\nRandom Fields”. Ph.D. thesis, Tsinghua University.\nWang, B. and Z. Ou. (2017). “Language modeling with neural trans-\ndimensional random fields”. In: IEEE Automatic Speech Recognition\nand Understanding Workshop (ASRU).\nWang, B. and Z. Ou. (2018a). “Improved training of neural trans-\ndimensional random field language models with dynamic noise-\ncontrastive estimation”. In: IEEE Spoken Language Technology\nWorkshop (SLT).\nWang, B. and Z. Ou. (2018b). “Learning neural trans-dimensional\nrandom field language models with noise-contrastive estimation”.\nIn: ICASSP.\nWang, B., Z. Ou, Y. He, and A. Kawamura. (2016). “Model interpolation\nwith trans-dimensional random field language models for speech\nrecognition”. arXiv preprint arXiv:1603.09170.\nWang, B., Z. Ou, and Z. Tan. (2015). “Trans-dimensional random\nfields for language modeling”. In: Proceedings of the 53rd Annual\nMeeting of the Association for Computational Linguistics and the\n7th International Joint Conference on Natural Language Processing\n(Volume 1: Long Papers). 785–794.\nWang, B., Z. Ou, and Z. Tan. (2018). “Learning trans-dimensional\nrandom fields with applications to language modeling”. IEEE trans-\nactions on pattern analysis and machine intelligence. 40(4): 876–\n890.\nWelling, M. and Y. W. Teh. (2011). “Bayesian learning via stochastic\ngradient Langevin dynamics”. In: ICML.\nWilliams, R. J. and D. Zipser. (1989). “A learning algorithm for contin-\nually running fully recurrent neural networks”. Neural computation.\n1(2): 270–280.\nWiseman, S. and A. M. Rush. (2016). “Sequence-to-sequence learning\nas beam-search optimization”. In: EMNLP.\n198\nREFERENCES\nXiang, H. and Z. Ou. (2019). “CRF-based Single-stage Acoustic Model-\ning with CTC Topology”. In: ICASSP. 5676–5680.\nXie, J., Y. Lu, R. Gao, S.-C. Zhu, and Y. N. Wu. (2018). “Cooperative\ntraining of descriptor and generator networks”. IEEE transactions\non pattern analysis and machine intelligence. 42(1): 27–45.\nXie, J., Y. Lu, S.-C. Zhu, and Y. Wu. (2016). “A theory of generative\nconvnet”. In: ICML.\nXu, H. and Z. Ou. (2016). “Joint stochastic approximation learning of\nhelmholtz machines”. In: ICLR Workshop Track.\nYounes, L. (1989). “Parametric inference for imperfectly observed Gibb-\nsian fields”. Probability Theory and Related Fields. 82: 625–645.\nYu, F., Z. Yao, X. Wang, K. An, L. Xie, Z. Ou, B. Liu, X. Li, and G.\nMiao. (2021). “The SLT 2021 children speech recognition challenge:\nOpen datasets, rules and baselines”. In: IEEE Spoken Language\nTechnology Workshop (SLT).\nZaremba, W., I. Sutskever, and O. Vinyals. (2014). “Recurrent neural\nnetwork regularization”. arXiv:1409.2329.\nZeyer, A., E. Beck, R. Schlüter, and H. Ney. (2017). “CTC in the context\nof generalized full-sum HMM training”. In: INTERSPEECH.\nZhang, B., H. Lv, P. Guo, Q. Shao, C. Yang, L. Xie, X. Xu, H. Bu, X.\nChen, C. Zeng, D. Wu, and Z. Peng. (2022a). “WENETSPEECH: A\n10000+ Hours Multi-Domain Mandarin Corpus for Speech Recogni-\ntion”. In: International Conference on Acoustics, Speech, and Signal\nProcessing (ICASSP).\nZhang, L., D. M. Blei, and C. A. Naesseth. (2022b). “Transport score\nclimbing: Variational inference using forward KL and adaptive neural\ntransport”. arXiv preprint arXiv:2202.01841.\nZhang, T., V. Kishore, F. Wu, K. Q. Weinberger, and Y. Artzi. (2020a).\n“BERTScore: Evaluating Text Generation with BERT”. In: Interna-\ntional Conference on Learning Representations.\nZhang, X., Z. Tan, and Z. Ou. (2023). “Persistently Trained, Diffusion-\nassisted Energy-based Models”. Stat.\nZhang, Y., X. Sun, S. Ma, Y. Yang, and X. Ren. (2018). “Does Higher\nOrder LSTM Have Better Accuracy for Segmenting and Labeling\nSequence Data?” In: COLING.\nREFERENCES\n199\nZhang, Y., Z. Ou, M. Hu, and J. Feng. (2020b). “A Probabilistic End-To-\nEnd Task-Oriented Dialog Model with Latent Belief States towards\nSemi-Supervised Learning”. In: Proc. of the Conference on Empirical\nMethods in Natural Language Processing (EMNLP).\nZhao, S., J. Jacobsen, and W. Grathwohl. (2020). “Joint Energy-Based\nModels for Semi-Supervised Classification”. In: ICML Workshop on\nUncertainty and Robustness in Deep Learning.\nZheng, H., K. An, and Z. Ou. (2021a). “Efficient neural architecture\nsearch for end-to-end speech recognition via straight-through gradi-\nents”. In: 2021 IEEE Spoken Language Technology Workshop (SLT).\nZheng, H., K. An, Z. Ou, C. Huang, K. Ding, and G. Wan. (2022).\n“An Empirical Study of Language Model Integration for Transducer\nbased Speech Recognition”. In: INTERSPEECH.\nZheng, H., W. Peng, Z. Ou, and J. Zhang. (2021b). “Advancing CTC-\nCRF based end-to-end speech recognition with wordpieces and\nconformers”. arXiv preprint arXiv:2107.03007.\nZhu, C., K. An, H. Zheng, and Z. Ou. (2021). “Multilingual and crosslin-\ngual speech recognition using phonological-vector based phone em-\nbeddings”. In: IEEE Automatic Speech Recognition and Understand-\ning Workshop (ASRU).\nZhu, X. (2006). “Semi-supervised learning literature survey”. Technical\nreport, University of Wisconsin-Madison.\nZhu, Y., R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba,\nand S. Fidler. (2015). “Aligning books and movies: Towards story-\nlike visual explanations by watching movies and reading books”.\nIn: Proceedings of the IEEE international conference on computer\nvision. 19–27.\nIndex\nAcoustic model (AM), 111\nAncestral sampling, 55, 58, 67\nAttention based\nencoder-decoder\n(AED), 121\nAugSA plus JSA algorithm, 53\nAutomatic speech recognition\n(ASR), 108\nAutoregressive language model\n(ALM), 66, 72\nBayesian network (BN), 9\nBurned in, 35\nCalibration, 164\nClique, 19\nConditional generation, 8\nConditional maximum\nlikelihood (CML), 104\nConditional model, 8\nConditional NCE, 107\nConditional random field\n(CRF), 94\nConstrained decoding with\nLangevin dynamics\n(COLD), 136, 140\nContional EBM, 94\nContrastive divergence (CD),\n49\nCoopNet, 53\nCRF transducer, 124\nData efficiency, 116\nData hungry, 116\nDeep belief network (DBN), 29\nDeep Boltzmann machine\n(DBM), 29\nDeep generative model (DGM),\n7\nDeep neural network (DNN),\n19, 110, 142\nDirected graphical model\n(DGM), 9, 15\n200\nINDEX\n201\nDiscrete feature, 79, 96\nDiscriminative SSL, 143\nDNN-HMM hybrid, 108, 110\nDynamic noise-contrastive\nestimation (DNCE), 64\nEBMs parameterized by neural\nnetworks, 29\nEdge potential, 97\nElectric model, 91\nEnergy function, 21\nEnergy-based language model\n(ELM), 73\nEnergy-based model (EBM), 7,\n9, 21\nEntropy, 168\nEvidence upper bound (EUBO),\n51\nExclusive-NRF algorithm, 52\nExclusive-variational approach,\n50\nExpectation-Maximization\n(EM) algorithm, 50\nExpected calibration error\n(ECE), 167\nExponential tilting, 88\nExposure bias, 97\nFeature extractor, 19, 97\nFeature function, 22\nFisher equality, 180\nFull conditional, 37\nGenerative adversarial network\n(GAN), 7\nGenerative AI, 66\nGenerative SSL, 143, 144\nGenerative-discriminative pair,\n6\nGibbs sampling, 37\nGlobally-normalized ELM\n(GN-ELM), 73\nGlobally-normalized model, 9\nGlobally-normalized sequence\nmodel, 98\nGraphical model, 14\nHamiltonian Monte Carlo\n(HMC), 39\nHelmholtz machine (HM), 19\nHidden Markov model (HMM),\n16\nHybrid generative-\ndiscriminative, 7\nImportance sampling (IS), 42\nImportance weight, 37, 42\nInclusive-NRF algorithm, 53,\n149\nInclusive-variational approach,\n51\nIsing model, 24\nJoint EBM (JEM), 147\nJoint random field (JRF), 150\nJoint stochastic approximation\n(JSA), 50\nJoint-training for generative\nSSL, 144\nLabel bias, 97\nLangevin dynamics (LD), 40\nLanguage model (LM), 72\n202\nINDEX\nLatent-variable model (LVM),\n145\nLearning, 5\nLinear layer, 19\nLinear-chain CRF, 95\nLocally-normalized model, 9\nLocally-normalized sequence\nmodel, 67, 97\nLog-linear model, 22\nLogits, 18\nMarkov Chain Monte Carlo\n(MCMC), 35\nMarkov random field (MRF), 9\nMarkovian score climbing\n(MSC), 50\nMasked language model\n(MLM), 85, 89\nMaximum entropy Markov\nmodel (MEMM), 98\nMaximum entropy model\n(maxent), 23\nMaximum likelihood estimation\n(MLE), 32\nMetropolis algorithm, 37\nMetropolis independence\nsampler (MIS), 37\nMetropolis-Hastings (MH)\nalgorithm, 35\nMH ratio, 36\nMH within Gibbs sampling, 39\nMinibatching, 34, 46, 48\nMix-and-match language model,\n136\nMonte Carlo averaging, 33\nMulti-class logistic regression,\n18\nNatural language processing\n(NLP), 12, 142\nNeural CRF (NCRF), 96, 123\nNeural random fields (NRFs),\n30, 53\nNode potential, 96\nNoise-contrastive estimation\n(NCE), 61\nNon-autoregressive generation,\n67\nNon-normalized probabilistic\nmodel, 169\nPart-of-speech (POS), 149\nPath in CTC, 113\nPerplexity (PPL), 133\nPersistent contrastive\ndivergence (PCD), 49\nPre-trained language model\n(PLM), 70, 84, 133,\n135, 144\nPre-training for generative SSL,\n144\nProbabilistic approach, 3\nProbabilistic inference, 5\nProbabilistic model, 3\nProposal distribution, 35, 42\nPseudo-log-likelihood (PLL), 85\nRecurrent neural network\ntransducer (RNN-T), 8,\n120\nResidual ELM, 88, 130\nRestricted Boltzmann machine\n(RBM), 26\nINDEX\n203\nScheduled sampling, 104\nScore function, 61\nScore matching (SM), 60\nSelf-normalized importance\nsampling (SNIS), 43,\n133\nSemi-supervised learning (SSL),\n7, 142\nSequence labeling, 95, 123, 149\nSequence modeling, 149\nSequence-to-sequence model\n(seq2seq), 98\nSigmoid belief network (SBN),\n19, 28\nSoftmax, 18\nState space, 105\nState topology of a Markov\nchain, 109\nState transition graph of a\nMarkov chain, 109\nStatistical inference, 5\nStochastic approximation (SA),\n43\nStochastic gradient descent\n(SGD), 46\nStochastic gradient Langevin\ndynamics (SGLD), 40\nStochastic maximum likelihood\n(SML), 48\nStreaming speech recognition,\n99\nTabular potential, 22\nTarget variable, 4\nTeacher forcing, 103\nTrans-dimensional random field\n(TRF), 74\nTree-width, 105\nTrigram, 80\nUndirected graphical model\n(UGM), 9, 19\nVariational autoencoder (VAE),\n7, 19\nVariational inference (VI), 50\nVariational learning, 50\nVariational methods, 50\nWeighted finite-state transducer\n(WFST), 109\nWhole-sentence maximum\nentropy (WSME), 73\nWord error rate (WER), 74\nWord morphology, 22\n",
  "categories": [
    "cs.LG",
    "cs.CL",
    "cs.SD",
    "eess.AS"
  ],
  "published": "2024-03-16",
  "updated": "2024-03-16"
}