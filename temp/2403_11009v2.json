{
  "id": "http://arxiv.org/abs/2403.11009v2",
  "title": "DIALECTBENCH: A NLP Benchmark for Dialects, Varieties, and Closely-Related Languages",
  "authors": [
    "Fahim Faisal",
    "Orevaoghene Ahia",
    "Aarohi Srivastava",
    "Kabir Ahuja",
    "David Chiang",
    "Yulia Tsvetkov",
    "Antonios Anastasopoulos"
  ],
  "abstract": "Language technologies should be judged on their usefulness in real-world use\ncases. An often overlooked aspect in natural language processing (NLP) research\nand evaluation is language variation in the form of non-standard dialects or\nlanguage varieties (hereafter, varieties). Most NLP benchmarks are limited to\nstandard language varieties. To fill this gap, we propose DIALECTBENCH, the\nfirst-ever large-scale benchmark for NLP on varieties, which aggregates an\nextensive set of task-varied variety datasets (10 text-level tasks covering 281\nvarieties). This allows for a comprehensive evaluation of NLP system\nperformance on different language varieties. We provide substantial evidence of\nperformance disparities between standard and non-standard language varieties,\nand we also identify language clusters with large performance divergence across\ntasks. We believe DIALECTBENCH provides a comprehensive view of the current\nstate of NLP for language varieties and one step towards advancing it further.\nCode/data: https://github.com/ffaisal93/DialectBench",
  "text": "DIALECTBENCH:\nA NLP Benchmark for Dialects, Varieties, and Closely-Related Languages\nFahim Faisalα,*\nOrevaoghene Ahiaβ,*\nAarohi Srivastavaγ\nKabir Ahujaβ\nDavid Chiangγ\nYulia Tsvetkovβ\nAntonios Anastasopoulosα, δ\nαGeorge Mason University\nβUniversity of Washington\nγUniversity of Notre Dame\nδArchimedes Research Unit, RC Athena, Greece\n{ffaisal,antonis}@gmu.edu\n{oahia,kahuja,yuliats}@cs.washington.edu\n{asrivas2,dchiang}@nd.edu\nAbstract\nLanguage technologies should be judged on\ntheir usefulness in real-world use cases. An\noften overlooked aspect in natural language\nprocessing (NLP) research and evaluation is\nlanguage variation in the form of non-standard\ndialects or language varieties (hereafter, vari-\neties). Most NLP benchmarks are limited to\nstandard language varieties. To fill this gap, we\npropose DIALECTBENCH, the first-ever large-\nscale benchmark for NLP on varieties, which\naggregates an extensive set of task-varied vari-\nety datasets (10 text-level tasks covering 281 va-\nrieties). This allows for a comprehensive evalu-\nation of NLP system performance on different\nlanguage varieties. We provide substantial evi-\ndence of performance disparities between stan-\ndard and non-standard language varieties, and\nwe also identify language clusters with larger\nperformance divergence across tasks. We be-\nlieve DIALECTBENCH provides a comprehen-\nsive view of the current state of NLP for lan-\nguage varieties and one step towards advancing\nit further. 1\n1\nIntroduction\nBenchmarking is important for tracking the\nprogress the field of natural language processing\n(NLP) has made in various tasks. In the past few\nyears, large-scale multilingual benchmarks like\nXTREME (Hu et al., 2020), XTREME-R (Ruder\net al., 2021), and XGLUE (Liang et al., 2020) have\nplayed a pivotal role in evaluating the multilingual\ncapabilities of NLP models. These efforts have\nsought to make model evaluation more accessible\nto researchers and representative of a variety of lan-\nguages (Song et al., 2023). However, most bench-\nmarks have focused on the standard varieties of\nlanguages, largely neglecting non-standard dialects\nand language varieties (Blasi et al., 2022).\n*Equal contribution.\n1More information can be found at the following:\nCode/data: https://github.com/ffaisal93/DialectBench\nWebsite: https://fahimfaisal.info/DialectBench.io\nFigure 1: DIALECTBENCH Evaluation Suite.\nWe refer to non-standard dialects and language\nvarieties simply as varieties, and sometimes in-\nclude low-resource related languages, writing sys-\ntem variants, and other kinds of variation. Varieties\ncontain subtle but notable variations in vocabulary,\npronunciation, orthography and grammar, reflect-\ning regional, social, and cultural differences (Cham-\nbers and Trudgill, 1998). The non-standard nature\nof these language varieties oftentimes contributes\nto the scarcity of substantial datasets that accu-\nrately capture these variations (Hedderich et al.,\n2021). As a result, they have often been absent\nfrom widely adopted benchmarks, even from ad-\nmirable efforts like XTREME-up (Ruder et al.,\n2023), GLUECoS (Khanuja et al., 2020) and Cre-\noleVal (Lent et al., 2023), which focus on under-\nresourced, code-switched, and creole languages,\nrespectively. It is currently challenging to accu-\nrately test the robustness of multilingual models\non a large suite of varieties without establishing\nan NLP evaluation framework covering multiple\nlanguage clusters (each of which contains standard\nlanguages alongside its closely related varieties).\nTo this end, we create DIALECTBENCH, a large-\nscale benchmark covering 40 language clusters\nwith 281 varieties, spanning 10 NLP tasks. We ob-\nserve that the performance disparity between differ-\nent varieties of the same language cluster becomes\narXiv:2403.11009v2  [cs.CL]  7 Jul 2024\nTotal\narabic\nhigh german\nitalian romance\nbasque\nanglic\nsinitic\ncommon turkic\nsw shift. romance\ngreek\ngallo-rhaetian\nnorwegian\nneva\nbengali\ngallo-italian\nkurdish\nkomi\nserb.-croa.-bosnian\ntupi-guarani.\nmodern dutch\neastern romance\nfrisian\nswahili\nOther\nLanguage Clusters\nDEP.\nPOS.\nNER\nEQA\nMRC\nNLI\nTC\nSA\nDId\nMT\nTotal\nTask\n40\n3\n2\n4\n3\n3\n4\n3\n3\n1\n3\n3\n8\n51\n6\n2\n4\n2\n3\n5\n3\n3\n8\n1\n3\n3\n8\n85\n2\n8\n4\n4\n6\n4\n5\n2\n6\n3\n5\n2\n2\n4\n3\n3\n3\n19\n24\n7\n11\n2\n2\n2\n11\n6\n1\n2\n2\n38\n9\n2\n2\n1\n3\n3\n4\n1\n2\n3\n2\n1\n5\n38\n9\n2\n2\n1\n3\n3\n4\n1\n2\n3\n2\n1\n5\n9\n9\n49\n26\n4\n3\n4\n6\n6\n114\n25\n23\n20\n21\n8\n1\n2\n3\n5\n2\n4\n281\n42\n31\n26\n21\n19\n13\n12\n11\n11\n8\n8\n8\n6\n5\n5\n4\n4\n3\n3\n3\n3\n3\n32\nFigure 2: DIALECTBENCH language clusters with their\nvariety counts per task. \"Other\" encompasses 18 clusters\n(full cluster list in Appendix Table 8).\nmore pronounced when we shift from zero-shot\nevaluation to fine-tuning on variety data, because\nof uneven data availability across varieties. Cer-\ntain language clusters exhibit varying performance\nacross downstream tasks within the same category,\ndue to low-resource limitations. Additionally, we\nimprove the dialectal task coverage for natural lan-\nguage inference by constructing a translate-test\nevaluation dataset. Putting these all together, DI-\nALECTBENCH serves as a comprehensive suite that\nattains a multifaceted purpose: identifying broader\nlimitations in dialectal NLP, while reflecting on\npotential areas for improvement.\n2\nDIALECTBENCH\nDIALECTBENCH is a benchmark created to unify\ndialectal datasets across different tasks to foster\nresearch on language varieties and non-standard\ndialects. Below we describe the design choices\nwe undertook to achieve this goal. This includes\nour language variety and task selection procedures,\ndata collation methods, and evaluation principles.\nVariety Selection\nWe first looked through pa-\npers published in the ACL Anthology2 from the\nlast 10 years to find usable language resources,\nas well as commonly used online data reposito-\nries (Littauer, n.d.). We selected languages that\nhave well-established, high-resourced varieties. Va-\nrieties may vary by location, ethnicity, or other\nfactors. We also found instances where varieties\nare classified by writing system or even by genre\n(e.g., Twitter). When varying by location, vari-\neties may be classified by different datasets at\ndifferent levels of granularity, sometimes coun-\ntry, region, or city.\nIn some cases, we found\nresources with two or more varieties within one\ndataset (e.g., the UD_Portuguese-Bosque depen-\n2https://aclanthology.org\ndency treebank (Rademaker et al., 2017) includes\nexamples from both European and Brazilian Por-\ntuguese variants. To incorporate all these cases\nunder one paradigm, we formulate a cluster-variety\nmapping procedure.\nCluster-Variety Mapping\nWe construct sev-\neral language clusters comprising of both high-\nresourced varieties and their low-resourced coun-\nterparts.\nWe use the Glottolog language\ndatabase (Nordhoff, 2012) to define clusters and\nassign varieties as outlined in Figure 1. This design\nchoice enables us to keep varieties that are closely\nrelated in terms of either mutual intelligibility, phy-\nlogenetic similarity or geographic proximity within\nthe same cluster. Hence, all cluster varieties always\nroot back to the closest common linguistic ancestor\nand the whole cluster maps to an established phylo-\ngenetic subtree. For example, Fiji Hindi and Hindi,\nwith Hindustani3 as their closest common ancestor,\nare placed in the Hindustani cluster.\nWe primarily use the Glottocode language identi-\nfication scheme (Hammarström and Forkel, 2022),\nensuring a standardized naming scheme across\nall varieties.\nFor instance, AAE variety from\nTwitterAAE (Blodgett et al., 2018) dependency\nparsing dataset is renamed as African American\nVernacular English with a corresponding Glot-\ntocode afri1276. In cases where Glottocodes are\nunavailable, like for spoken English from South\nIndia, we substitute with the immediate ancestor\nGlottocode (indi1255) and further categorize the\nvarieties using the following metadata identifiers:\n1. Area (a): the region where the variety is spo-\nken or where its dataset was collected.\n2. Language register (r): frozen, formal, consul-\ntative, casual, and intimate.\n3. Language mode (m): written, spoken, and\nsigned language.\n4. Orthography (o): In DIALECTBENCH this is\nonly specific to Sinitic varieties. This could\nbe either traditional or simplified.\n5. Identifier (i): Dataset-specific metadata, could\nbe domain (eg. twitter).\nWe encapsulate all this information, into a nam-\ning convention, and use the template: {Glottocode\nname}-(a:{},r:{},m:{},o:{},i:{}). 4\n3https://glottolog.org/resource/languoid/id/\nhind1270\n4For example, mandarin chinese (a:mainland, o:simplified\nrefers to Mandarin Chinese (mand1415) spoken in Mainland\nChina and written in simplified characters.\nCategory\nTask\nMetric\nSource Dataset\nStructured\nDEP parsing\nUAS\nUniversal Dependency (Zeman et al., 2021), TwitterAAE (Blodgett et al., 2018), Singlish (Wang et al., 2017)\nPrediction\nPOS tagging\nF1\nUniversal Dependency (Zeman et al., 2021), Singlish (Wang et al., 2017), Noisy Dialects (Blaschke et al., 2023)\nNER\nF1\nWikiann (Pan et al., 2017; Rahimi et al., 2019), Norwegian NER (Johansen, 2019)\nClassification\nDId\nF1\nMADAR (Bouamor et al., 2018), DMT (Jauhiainen et al., 2019), Greek (Sababa and Stassopoulou, 2018), DSL-TL (Zampieri et al.,\n2023), Swiss Germans (Scherrer et al., 2019)\nSA\nF1\nTSAC (Medhaffar et al., 2017), TUNIZI (Fourati et al., 2021), DzSentiA (Abdelli et al., 2019), SaudiBank (Alqahtani et al., 2022),\nMAC (Garouani and Kharroubi, 2022), ASTD (Nabil et al., 2015), AJGT (Alomari et al., 2017), OCLAR (Al Omari et al., 2019)\nTC\nF1\nSIB-200 (Adelani et al., 2023)\nNLI\nF1\nXNLI (Conneau et al., 2018) translate-test\nQuestion\nMRC\nF1\nBelebele (Bandarkar et al., 2023)\nAnswering\nEQA\nSpan F1\nSDQA (Faisal et al., 2021)\nGeneration\nMT\nBLEU\nCODET (Alam et al., 2023), TIL-MT (Mirzakhalov, 2021)\nTable 1: The tasks and data sources of DIALECTBENCH (Detailed discussion: Appendix B).\nCluster Representative\nEach cluster will often\nhave a high-resourced variety usually with the\nlargest speaker population. We choose this high-\nresourced variety as the cluster representative. This\nselection might vary across downstream tasks de-\npending on the data availability. We primarily uti-\nlize this representative variety to evaluate the per-\nformance gap across cluster varieties, and also rely\non it for transfer-learning in resource-scarce set-\ntings. Sometimes, the members of a cluster are\nconsidered closely related languages, and some-\ntimes dialects; to avoid making this distinction, we\nrefer to all the members of a cluster simply as vari-\neties of the cluster representative.\nTask and Dataset Selection\nIn selecting tasks,\nwe maintain a balanced approach, promoting task\ndiversity while also including tasks that require\ndiverse levels of textual understanding. In the end,\nour complete list of tasks are as follows:\n1. Dependency parsing (DEP parsing)\n2. Parts-of-speech tagging (POS tagging)\n3. Named entity recognition (NER)\n4. Dialect identification (DId)\n5. Sentiment analysis (SA)\n6. Topic classification (TC)\n7. Natural language inference (NLI)\n8. Multiple-choice machine reading comprehen-\nsion (MRC)\n9. Extractive question answering (EQA)\n10. Machine translation (MT)\nIn Table 1, we present the task and dataset de-\ntails. We mostly keep the datasets in their orig-\ninally published form (except for varieties renam-\ning). For NLI, we use the existing English test set\nof XNLI (Conneau et al., 2018) and construct a\nmultilingual dialect-focused translated evaluation\ndataset. We refer to this as translate-test NLI.\nEvaluation Principles\nOn the ground level, we\nevaluate existing NLP systems on text-based tasks\nusing standard evaluation metrics (e.g., UAS for\nparsing, F1 for classification tasks, BLEU for trans-\nlation). At a global level, we believe a sustainable\nNLP system should be user-focused while provid-\ning substantial (i) linguistic utility and (ii) demo-\ngraphic utility (Song et al., 2023; Blasi et al., 2022).\nBlasi et al. (2022) defined the utility of a task and\nlanguage, as the corresponding performance nor-\nmalized by the best possible performance (usually\nhuman-level performance). Demographic utility\nconsiders the demand for a language technology\nwithin a specific language, where the demand is\nproportional to the number of speakers of that lan-\nguage. Linguistic utility, on the other hand, asserts\nthat “all languages are created equal” regardless of\nthe number of speakers, and hence all languages in\nthe world should receive identical weights.\nOverall, we want to capture the performance gap\nbetween language clusters (e.g., Anglic vs. Italian\nRomance) as well as within language clusters (e.g.,\nNorwegian Bokmål vs. Nynorsk). To attain this,\nwe define the performance gap metrics in §3.3 . We\nalso vary the experimental settings in §3.2, includ-\ning zero-shot and few-shot cross-lingual transfer, as\nwell as fine-tuning with similar high-resource lan-\nguages. This is essential given that we lack clean\nannotated data in many varieties.\n3\nExperiments\nHere, we report the entire process involved in creat-\ning and evaluating baselines for all of the tasks and\nvarieties in DIALECTBENCH. Additionally, we de-\nfine a dialectal gap metric to analyze performance\ndisparities within and across clusters.\n3.1\nModels\nWe evaluate using two multilingual models:\nmBERT (Devlin et al., 2019) and XLM-R (Con-\nneau et al., 2020) for all tasks except MT. For MT,\nwe do zero-shot evaluation with NLLB (NLLB\nTeam et al., 2022), using both the 600M and 1.3B\nvariants. In addition, we use Mistral 7B (Jiang et al.,\n2023) to evaluate the current capability of LLMs\non multilingual and dialectal understanding tasks.\nOur main goal is collating dialectal data across dif-\nferent languages and tasks under a single platform,\nhence we do not optimize for the best model per-\nformance. Rather, we focus on understanding and\nreporting the current state of performance on all\nDIALECTBENCH varieties.\n3.2\nTraining and Evaluation\nTraining and evaluation procedures are largely de-\ntermined by the availability of training or evalua-\ntion data for each task.\nFor any cluster C, let ¯C be the highest-resourced\nvariety (which is usually the cluster representative)\nof C. In addition, for any variety v ∈C, we write\n¯v for the highest-resourced variety, that is, ¯v =\n¯C. For any varieties t and v, let St(v) be the raw\nevaluation score of a system fine-tuned on t and\ntested on v (higher is better).\nWe use five general approaches for task-specific\nmodel training:\n1. In-variety fine-tuning (Sv(v)):\nIn cases\nwhere there is available training data for a\nvariety v, we fine-tune the base model on v.\nThis primarily applies to tasks such as POS\ntagging and dependency parsing.\n2. In-cluster fine-tuning (S¯v(v)): In-variety\nfine-tuning is quite resource-intensive when\nwe have a large number of varieties within a\ncluster C. In such cases, we fine-tune the base\nmodel on ¯C. Then we evaluate this model on\neach variety v ∈C. This is the most com-\nmon setting in our experiments, as it allows\nus to evaluate the dialectal performance gap\nwithout increasing the computation cost. For\nthe DId task, we typically use a dataset of\nsentences annotated with variety labels to fine-\ntune one dialect-identification model for each\nlanguage cluster.\n3. Combined fine-tuning (SL(v)):\nUnlike\nin the previous two methods, where each\ntraining set contains data from a single\nlanguage cluster, we fine-tune our baseline\nquestion-answering (EQA and MRC) models\nusing the SD-QA (Faisal et al., 2021) and\nBelebele (Bandarkar et al., 2023) datasets\nrespectively, both of which contain training\ndata in multiple standard varieties only and\ntest data in other varieties.\nThe SD-QA\nTask\nIn-variety\nFT\nIn-cluster\nFT\nCombined\nFT\nZero-\nshot\nNo\nref-\nerence\nIn-context\nlearning\nDEP\n✓\n✓\nPOS\n✓\n✓\nNER\n✓\n✓\nEQA\n✓\n✓\n✓\nMRC\n✓\nNLI\n✓\nTC\n✓\n✓\nSA\n✓\n✓\nDId\n✓\nMT\n✓\n✓\nTable 2: Task-specific training and evaluation proce-\ndure.\ntraining data (L in the notation) contains\nquestions in 9 standard varieties (L\n=\n{eng, ara, ben, fin, ind, swa, kor, rus, tel}),\nwhile Belebele assembles data from 6 distinct\nmultiple-choice QA datasets in standard\nEnglish (L = {eng}).\n4. Zero-shot evaluation (Seng(v)): For certain\nvarieties, obtaining training data even for in-\ncluster fine-tuning can be a challenge. Fortu-\nnately, English training data is always avail-\nable for the datasets we study, so we use En-\nglish to fill the gaps when we lack in-variety,\nin-cluster, or combined training data. At the\nsame time, we aim to assess the feasibility of\nusing this zero-shot cross-lingual transfer in\nreducing any existing performance gap across\nvarieties.\nSo we perform zero-shot cross-\nlingual transfer from English to each variety\nfor 6 tasks in total. We only leave out those\ntasks such as dialect identification that explic-\nitly require in-cluster training data.\n5. In-context learning (Sicl(v)): When evalu-\nating large language models, we do not fine-\ntune them but instead rely on prompting and\nin-context learning. For this, we provide in-\nstructions and 5 examples in English as exem-\nplars, followed by a prompt for predicting the\ntest examples. Employing Mistral 7B (Jiang\net al., 2023), we assess the present effective-\nness of a close-to-state-of-the-art LLM on lan-\nguage varieties. The task-specific example\nprompts are reported in Appendix I.\nTable 2 summarizes the task-specific training pro-\ncedures that we employ based on data availability.\nNote that, for MT, we perform zero-shot evaluation\nspecifically in the translation direction, (standard\nvariety to English) tested on (dialectal variety to\nEnglish). But evaluation is a challenge because\nhuman-created reference translations into or out\nof non-standard varieties are usually very limited.\nTherefore, we adopt an evaluation protocol from\nprevious work (Alam et al., 2023) that uses pseudo-\nreferences. Given x, a sentence in a variety and ¯x,\nthe translation of x into the standard variety, let y\nbe the output of the MT system on input x and ¯y be\nthe output on input ¯x. Then we measure the quality\nof y (using, e.g., BLEU) compared against ¯y as a\npseudo-reference.\n3.3\nQuantifying the Dialectal Gap\nTo quantify the performance disparity across var-\nious resource-specific settings, language clusters\nand varieties, we introduce a dialect performance\ngap metric Gt(u, v): the relative decrease in perfor-\nmance of a system fine-tuned on variety t, tested\non variety v compared to a baseline variety u:\nGt(u, v) = St(u) −St(v)\nSt(u)\nwith a special case for in-variety fine-tuning:\nGin-variety(u, v) = Su(u) −Sv(v)\nSu(u)\n.\nFor the baseline score, we use either the score on\nthe standard variety for each cluster (u = ¯v), or,\nin the zero-shot setting, the score on the language\nused for fine-tuning, namely English (u = t =\neng). Rather than computing an absolute gap, we\nopt for a relative gap (i.e., dividing by the base-\nline score). We also indicate whether the training\nsetting is zero-shot (t = eng) or fine-tuned on in-\nvariety, in-cluster, or assembled data. Putting all\nthese together, we compute the following three vari-\nations of dialectal inequality.\n1. Geng(eng, v): We calculate this metric to get a\ncomprehensive measurement of global dispar-\nity across all varieties in a resource-limited en-\nvironment (zero-shot transfer from English).\n2. Geng(¯v, v): Using this variation, we keep the\nsetting fixed as zero-shot and calculate the\ngap between the representative variety and\nany other variety.\n3. Gt(¯v, v): The two aforementioned metrics\nshed light on the extent of the variety per-\nformance gap in a resource-limited setting.\nTo gain a more comprehensive perspective,\nwe additionally compute another metric, this\ntime utilizing the availability of resources.\nThe computation approach remains as straight-\nforward as before. We just use fine-tuning\non a variety t instead of zero-shot transfer\nfrom Standard English: t = in-variety for\nin-variety fine-tuning, t = ¯v for in-cluster\nfine-tuning, or t is some set of varieties for\ncombined fine-tuning.\nFor all three G metrics, we compute them at the\nvariety level and then average them at the cluster\nlevel:\nGt(u, C) =\n1\n|C|\nX\nv∈C\nGt(u, v)\nGin-variety(u, C) =\n1\n|C|\nX\nv∈C\nGin-variety(u, v).\n4\nResults\nWe, first of all, discuss results by highlighting the\nhighest possible score per variety, aka the maxi-\nmum obtainable evaluation scores regardless of\nevaluation method or training data. Next, we ex-\ntend our discussion further by reporting the existing\ndialectal disparity across clusters and varieties.\n4.1\nMaximum Obtainable Scores\nHere we provide key findings from our evaluation\non each task. A task-specific summary is reported\nin Table 3. Detailed results comprising all tasks,\nmodels, language clusters and varieties are reported\nin Tables 10 to 20 in Appendix E.\nStructured prediction\nWe present visualizations\nfor the task-specific maximum scores. We show the\none for Dependency Parsing in Fig. 3, where we\nobserve that low-resource varieties from language\nclusters such as Tupi-Guarani (indigenous South\nAmerican cluster), Saami and Komi (low-resource\nUralic language clusters) have the lowest perfor-\nmance compared to Standard English and other\nclosely related Germanic and Romance clusters.\nThese low-resource varieties are also not included\nin the pretraining stage of our base language mod-\nels (eg. mBERT). Furthermore, this trend is evi-\ndent across all three structured prediction tasks. On\nthe other hand, high-resource Indo-European lan-\nguages such as Portuguese, French, and Norwegian\nusually perform better.\nSequence classification\nFor DId and SA, we gen-\nerally collate different datasets for each language\ncluster and therefore, report the comparative clas-\nsification results together. As a result, the locality\nlevel (e.g. city/region/country) also varies across\nclusters. For example, we report city-level DId re-\nsults for Arabic and High German but country-level\nnum. num.\navg.\nCategory\nTask\ncl.\nvar.\nscore\nMax-score cluster/variety\nMin-score cluster/variety\nStructured prediction\nDEP parsing\n16\n40\n64.3\nsw. shifted romance/brazilian portuguese\n94.4\ntupi-guarani sg./mbyá guaraní (a:brazil)\n9.0\nPOS tagging\n17\n51\n72.1\nnorwegian/norwegian bokmål (m:written)\n98.7\ntupi-guarani sg./mbyá guaraní (a:brazil)\n1.9\nNER\n27\n85\n70.1\neastern romance/romanian\n94.2\nanglic/jamaican creole english\n0.0\nSequence classification\nNLI\n15\n38\n64.2\nanglic/english\n83.4\nsotho-tswana (s.30)/southern sotho\n34.6\nTC\n15\n38\n77.7\nsinitic/cmm. sinitic (o:traditional)\n89.8\nkurdish/central kurdish\n19.4\nDId\n6\n49\n67.0\nsinitic/mandarin chinese (a:taiwan, o:simp.)\n98.6\nsw. shifted romance/portuguese (m:written)\n17.4\nSA\n1\n9\n80.3\narabic/tunisian arabic\n94.6\narabic/south levantine arabic\n58.9\nQuestion Answering\nMRC\n4\n11\n40.9\nanglic/english\n53.4\nsotho-tswana (s.30)/southern sotho\n29.0\nEQA\n5\n24\n74.2\narabic/arabic (a:saudi-arabia)\n77.9\nswahili/swahili (a:tanzania)\n63.5\nGeneration\nMT-dialect\n12\n73\n25.2\narabic/gulf arabic (a:riy)\n43.1\ncommon turkic/sakha\n2.5\nMT-region\n2\n41\n33.0\nhigh german/central alemannic (a:ur)\n44.1\nitalian romance/italian (a:sardegna)\n13.0\nTable 3: Task specific result summary using Maximum Obtainable Score. The varieties with the minimum scores\nexhibit a noticeable lag in performance across various tasks when compared to the average task performance.\nm. guaraní (a:brazil)\nm. guaraní (a:paraguay)\nold guarani\nkomi-zyrian (m:writ.)\nkomi-permyak\nkomi-zyrian (m:spoken)\numbrian\nwest low german\nligurian\nskolt saami\nnorth saami\ncentral alemannic (a:zh)\ngerman\ngheg albanian\nalbanian\ncont. south. ita.\nita.\nita. (r:ca., m:writ., i:tweet)\nita. (r:for., m:writ., i:essay)\nclassical chinese\ncmm sinitic (a:HK, o:trad.)\ncmm sinitic (o:simp.)\nsouth levantine arabic\nnorth african arabic\nstandard arabic\neastern armenian\nwestern armenian\nAAVE\nsinglish\nenglish\nnor. nynorsk (m:writ., i:old)\nnor. nynorsk (m:writ.)\nnor. bokmål (m:writ.)\nfrench (a:paris)\nold french (842-ca. 1400)\nfrench\nportuguese (a:european)\nportuguese (m:writ.)\nportuguese (i:mix)\nbrazilian portuguese\nVariety\n0\n20\n40\n60\n80\n100\nUAS\ntupi-guarani\nsubgroup\ni.a\nkomi\nsabellic\nwest low german\ngallo-italian\nsaami\nhigh german\nalbanian\nitalian romance\nsinitic\narabic\neastern-western\narmenian\nanglic\nnorwegian\ngallo-rhaetian\nsouthwestern\nshifted\nromance\nFigure 3: Maximum scores (max. UAS) in Dep. Parsing task. Yellow-shaded region: Komi is the only cluster having\nno varieties seen during mBERT pertaining. Colored bars with diagonal stripes: the cluster representative variety.\nLow-resourced cluster varieties score lower compared to high-resource Germanic clusters.\nresults for Portuguese, Spanish and English. In the\ncase of SA, we have region/country-level results\nfor Arabic varieties. For TC and NLI, we have\nthe same set of clusters and varieties. However, we\nonly report the zero-shot transfer performance from\nStandard English for NLI using the newly created\ntranslate-test NLI dataset.\nFor TC and NLI, we observe the largest in-\ncluster disparity in the Kurdish cluster, with North-\nern Kurdish outperforming all others. The Sotho\nvarieties consistently perform significantly lower\ncompared to other clusters. For all three sequence\nclassification tasks, we generally find the Chinese\ncluster performing on par with high-resource Latin\ncounterparts.\nQuestion answering\nWe generally do not see\nlarge gaps in performance within varieties in each\nlanguage cluster. In EQA zero-shot experiments,\nEnglish and its varieties have the highest perfor-\nmance overall and Korean varieties score the low-\nest. Combined fine-tuning boosts performance on\nall language clusters except in English. It’s im-\nportant to note that these EQA scores primarily\nindicate the model’s robustness to accent-level dif-\nferences and transcription noise, rather than broad\ndialectal robustness. However, further investiga-\ntion is needed to determine whether this robustness\nspecifically applies to both accent-level differences\nand transcription noise, or to any character-level\nvariation up to a certain threshold.\nFor the MRC task, the performance peaked at\n53.4 for Standard English, while the lowest score\nwas 29 for Southern Sotho. More detailed results\nare presented in Tables 15 and 19.\nMachine translation\nThe performance gap here\nvaries widely across and within language varieties.\nPerformance is similar within the Swiss-German\ncluster, with higher performance (see Figure 5)\nacross regions in Northern Switzerland, which is\ngeographically closer to Germany.\nThe perfor-\nmance gap for Norwegian dialects (Figure 10b)\nis surprising as we perform zero-shot transfer from\nTask\nGap metric\nAvg. val\ncluster (max)\ncluster (min)\nDEP parsing\nGeng(eng, C)\n34.0\narabic, 50.8\nsw. shift. romance, 19.4\nGeng(¯v, C)\n15.7\nanglic, 34.2\nsw. shift. romance, −0.9\nGin-variety(¯v, C)\n26.4\narabic, 93.8\nitalian romance, 0.6\nPOS tagging\nGeng(eng, C)\n27.4\narabic, 58.5\nnorwegian, 14.7\nGeng(¯v, C)\n6.7\nanglic, 20.9\new. armenian, −2.1\nGin-variety(¯v, C)\n6.2\narabic, 29.1\nneva, −0.5\nNER\nGeng(eng, C)\n31.7\nkurdish, 77.1\nmodern dutch, 12.3\nGeng(¯v, C)\n22.3\nkurdish, 78.2\nhindustani, −23.6\nG¯v(¯v, C)\n−28.6\nkurdish, 91.5\nsorbian, -1162.7\nTC\nGeng(eng, C)\n22.4\nkurdish, 74.2\nsinitic, 0.4\nGeng(¯v, C)\n12.5\nkurdish, 60.6\nnorwegian, −2.2\nG¯v(¯v, C)\n−1.9\nlatvian, 21.0\nkurdish, −61.3\nTable 4: Comparative cluster-level dialectal gap across\ntasks. In general, the average disparity is larger for zero-\nshot transfer Geng(eng, C). However, when we move\nfrom zeroshot to finetune (i.e. Geng →G¯v/in-variety) and\ncompute the distance from a cluster representative ¯v, we\nobserve increased dialectal disparity |G(¯v, C)|.\nNorwegian Nynorsk (a Western dialect) but obtain\nbetter performance on the Eastern dialect. Within\nArabic, Riyadh is the highest performer while Sfax\nperforms the worst. For the Bengali cluster, Jes-\nsore has the highest performance –this is not sur-\nprising since it is one of the dialects from which\nstandard Bengali originated (Alam et al., 2023).\nThe Ethiopian variety of Tigrinya exhibits a higher\nperformance than the Eritrean one, even though\nTigrinya is more commonly spoken in Eritrea 5.\nAmongst the clusters within the Basque cluster,\nBarkoxe and Maule have the lowest score while\nAzkaine scores the highest.\n4.2\nDialectal Gap Across Language Clusters\nIn Fig. 4, we plot the zero-shot dialectal gap for\nthree tasks. In the x-axis, we report the aggregated\ncluster-level gap Geng(eng, v), compared against\nthe fine-tuning variety (standard English) while\nin y-axis we report Geng(¯v, v), the gap compared\nagainst the representative variety of a cluster. In\nan ideal scenario, we would want both of these\ngap values to be close to zero. However, this is\ncertainly not the case. The general observed trend\nis that the low-resource clusters have higher gaps\nof both Geng(eng, C) and Geng(¯v, C), whereas high-\nresource Germanic and Sinitic language clusters\nconsistently exhibit low dialectal gaps. That said,\ncertain specific high-resource varieties, such as\nStandard German and its dialectal counterparts like\nSwiss German, showcase significant within-cluster\ndialectal gaps (Fig. 4a).\nWe primarily report dialectal gaps using zero-\n5https://en.wikipedia.org/wiki/Tigrinya_\nlanguage\nzero-shot\nfew-shot / FT\nTask\nmBERT\nXLM-R\nmBERT\nXLM-R\nDEP. Parsing\n61.6\n61.3\n76.2\n64.3\nPOS Tagging\n69.5\n69.7\n89.8\n89.1\nNER\n59.7\n57.8\n65.8\n61.4\nNLI\n56.9\n62.5\n—\n—\nTC\n72.3\n71.4\n73.1\n68.9\nMRC\n—\n—\n39.4\n40.3\nEQA\n53.9\n51.9\n69.2\n67.2\nSA\n—\n—\n78.8\n80.1\nDId\n—\n—\n65.8\n59.3\nwin\n4/6\n2/6\n6/8\n2/8\nTable 5: Base model comparison. We found mBERT\nwas easier to fine-tune using the default hyperparameter\nsetting thus, resulting in a higher winning rate.\nshot transfer because the finetuning data available\nacross task and cluster is very disproportionate. Of-\nten the in-cluster/variety data is not good enough\nin terms of data quality and quantity. For exam-\nple, we have 37 varieties in 13 clusters for depen-\ndency parsing but out of these, only 20 varieties\nhave data available for in-variety fine-tuning. This\nlacking becomes more apparent when we compare\nthe statistics of two types of within-cluster dialec-\ntal gaps: zero-shot Geng(¯v, C) against fine-tuning\nG¯v/in-variety(¯v, C) in Table 4. In general, the within-\ncluster dialectal disparity is smaller for zero-shot\ntransfer (i.e.\nGeng(¯v, C) ≤|G¯v/in-variety(¯v, C)|).\nHere, the in-cluster/variety fine-tuning results in a\nhigher performance deviation primarily due to the\ninconsistent variety-specific finetuning data quality.\n5\nDiscussion\nHigh resource vs. low resource varieties\nThe\nhighest-performing varieties are mostly standard\nhigh-resource languages and a few high-resource\ndialects (Norwegian dialects) whereas, the majority\nof the lowest-performing language variants are low-\nresourced varieties. This clear distinction of lan-\nguage varieties points towards the large existence\nof in-cluster dialectal gaps. Furthermore, this find-\ning correlates with language script differences. We\nobserve that 77.2% of top-10 varieties in terms of\nmaximum obtainable score are written with Latin\nscript. Another finding is the performance insta-\nbility of low-resource varieties across tasks. For\ninstance, Old Guarani performs better in DEP pars-\ning whereas, Mbyá Guarani (Paraguay) surpasses\nit in POS tagging even though the dataset remains\nthe same (i.e. UD). For more detailed comparisons,\nin Appendix Table 21, we report the top-10 highest\nand lowest-scoring varieties across different tasks.\n0\n20\n40\n60\n80\neng(eng, C)\n0\n10\n20\n30\n40\n50\neng(v, C)\nalbanian\nanglic\narabic\new armenian\ngal-rhaetian\nh. german\nita. romance\nkomi\nnorwegian\nsaami\nsinitic\nsw shift. romance\ntup--guarani sub.\n(a) Dependency parsing\n0\n10\n20\n30\n40\n50\n60\n70\neng(eng, C)\n0\n10\n20\n30\n40\n50\n60\neng(v, C)\nanglic\narabic\ncommon turkic\ngal-italian\nh. german\nita. romance\nkurdish\nlatvian\nnorwegian\nsinitic\nsotho-tswana (s.30)\nsw shift. romance\n(b) Topic classification\n0\n20\n40\n60\n80\neng(eng, C)\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\neng(v, C)\nanglic\narabic\nbengali\nkorean\nswahili\n(c) Extractive question answering\nFigure 4: Dialectal gap visualization for language clusters utilizing zero-shot cross-lingual transfer from Standard\nEnglish. In the x-axis, values far from zero have a larger performance gap from English whereas, in the y-axis,\nvalues far from zero have a larger within cluster gap. Ideally, we want both of them to be close to zero.\nModel hyperparameter tuning\nIn Table 5, we\ncompare the baseline multilingual models. mBERT\nwas comparatively easier to train than XLM-R us-\ning the default learning rates reported in the earlier\ntask experiments. Often for tasks such as MRC,\nwe needed to tune hyperparameters (e.g. learn-\ning rate, max. sequence length) in case of XLM-\nR. However, once we identify a hyperparameter\nconfiguration that converges in a zero-shot set-\nting, we use the same to train all the language-\nvariant training for that specific task.\nAs a re-\nsult, for some low-resource languages, XLM-R\ndoes not converge compared to mBERT. For ex-\nample, XLM-R dependency parsing UAS score for\nNorwegian-NynorskLIA is 56.08 (zeroshot) and\n8.25 (in-variety FT) whereas, we get 78.39 for in-\nvariety mBERT fine-tuning. We suspect this hyper-\nparameter tuning issue is one of the contributing\nfactors toward a lower winning rate of XLM-R in\nfew-shot / fine-tuning settings. However, this could\nbe improved further with an extensive parameter\ngrid search and settings specifically tailored for\neach language cluster.\nPositive zero-shot transfer for Latin varieties\nLow-resource varieties written in Latin script re-\nceive greater benefit in zero-shot because of effec-\ntive transfer from high-resource Standard English.\nWith the presence of In-cluster/variety finetuning\ndata, we effectively diminish this script effect to\nsome extent. For example in the Hindi cluster, Fiji\nperforms better than its Latin non-standard counter-\nparts with in-cluster finetuning for NER (Table 12).\nIn summary, if the standard variety of a cluster is\nnon-latin but high-resource, then the success rate\nof in-cluster/variety fine-tuning tends to be higher.\nHowever, where all dialects are low-resource, Latin\nscript varieties utilizing zero-shot transfer, eventu-\nally surpass others in the performance hierarchy.\nAs an example, we report the zero-shot NER in-\nstances where the low-resource varieties perform\nbetter than the representative ones in Appendix Ta-\nble 22 (most of these use Latin script).\nLLM evaluation via In-context learning\nFor\nSA and EQA tasks, we have in-context learning\nresults using the Mistral7B LLM. Comparing the\nperformance against zero-shot and fine-tuning us-\ning our encoder-based models, we find dialect per-\nformance of LLM is better than zero-shot transfer\nbut falls behind the finetuned results. On top of\nthat, data contamination (Ahuja et al., 2023) dur-\ning LLM evaluation is another existing issue while\nconsidering these few available dialectal resources.\nCreating translation-based comparable data might\nbe a solution to perform a fair benchmarking of\nLLMs on low-resource varieties.\nMisinterpreting evaluation metrics\nWe also re-\nport the cluster-level population-weighted average\n(i.e. demographic utility) which rewards a system\nmore when it provides increased linguistic util-\nity (eg. raw F1 score) for varieties, spoken by a\nlarger population compared to varieties spoken by a\nsmaller population. Alone, this metric could be mis-\nleading if we consider the fact that the performance\ngap among all varieties from a particular cluster\nshould be minimal. On the other hand, solely look-\ning into the linguistic utility average does not give\na clear picture either (e.g. often overshadows the\nlarger performance deviation of certain varieties\nhaving extreme scores). So for all clusters and\ntasks, we report the linguistic utility average as\nwell as the demographic utility average, the mini-\nmum score of a cluster, and the standard deviation\nin Tables 23 to 30.\n6\nConclusion\nWe propose DIALECTBENCH, the first-ever inclu-\nsive Dialectal NLP benchmark reporting perfor-\nmance evaluation and disparity across standard and\nnon-standard varieties. This is one step towards the\neffort of bringing more and more language under\nthe paradigm of NLP technology. We would like to\nfurther improve the benchmark, constructing high-\nquality comparable data and expanding the task\ncoverage to speech-based NLP technologies.\nLimitations\nThe data quality and quantity, variety cover-\nage vary significantly across tasks because of\ndata scarcity issues. We avoid full-scale LLM-\nevaluation consciously because of the uncertain\ndata-contamination issue and their well-known\nlower performance threshold compared to smaller\nmasked-language-modeling-based fine-tuned mod-\nels. In addition, we focus on text-based NLP tasks\nfor this current iteration. Moreover, we do not\nclaim the representative varieties of each language\nclusters to be any kind of superior or standard-\nized forms over the other varieties. These varieties\nare chosen to perform a well-informed compari-\nson among the perceived well-resourced linguistic\nvariety and its counterparts having lesser data avail-\nability. At the same time, the mutual intelligibility\nand phylogenetic similarity of the similar cluster\nvarieties also vary across cluster and this was not\nselected in a numerically quantifiable manner.\nEvaluation Limitations\nTo further improve the\nevaluation fairness of the current version of DI-\nALECTBENCH, we need (i) Parallel corpus utiliza-\ntion to prepare task-specific data (ii) Translation-\nbased task data generation to perform compara-\nble analysis (iii) Quantifying the resource-supply\nand demand as well as population-coverage (Song\net al., 2023) to identify where a variety stands in\nthe global landscape of linguistic utility. Here, we\nhave accumulated data for diverse varieties across\ntasks that vary significantly in terms of quality, ex-\nample count, and domain. However, to perform\na perfectly fair comparison of dialectal inequality,\nwe should consider high-quality comparable data\n(e.g. parallel corpus, similar varieties across tasks)\nwhich is not available at this point.\nContinuity of DIALECTBENCH\nDespite our\nbest effort, this benchmark does not include every\none of the already published task-specific dialectal\ndatasets. So, our next steps on this project involve\nhosting the benchmark on the website that displays\nthe current statistics of the datasets in DIALECT-\nBENCH. We will also encourage researchers to add\nnew and existing datasets for tasks, language clus-\nters that might be currently missing alongside the\nrespective baselines.\nSpace Limitations\nOur study encompasses a\nlarge set of evaluation result tables, their corre-\nsponding visualizations and findings analysis. Due\nto space limitations, we have to move the detailed\nreports (Appendix E) and the rest of the visual-\nizations (Appendix D) in the Appendix. To better\nassist, we include an Appendix Table of Content\n(Table 6) at the introductory section of Appendix\n(Section 6).\nEthics Statement\nThis work is a compilation of existent dialectal\ndatasets across different tasks, including structured\nprediction and generative tasks. Our experiments\ndo not particularly optimize for the best model\nperformance of these tasks. Therefore we acknowl-\nedge that for some of the tasks, the baseline models\nmight not be robust enough to handle dialectal text\nhence resulting in wrong predictions and genera-\ntions. We believe that this underscores the need\nfor building models robust to different language\nvariations and future work should focus on this.\nAcknowledgements\nThis material is based upon work supported by\nthe US National Science Foundation under Grants\nNo. IIS-2125466, IIS-2125948, CAREER Grant\nNo. IIS2142739, as well as NSF Grants No. IIS-\n2203097 and IIS-2125201. We gratefully acknowl-\nedge support from Alfred P. Sloan Foundation Fel-\nlowship. This research is also supported in part\nby the Office of the Director of National Intelli-\ngence (ODNI), Intelligence Advanced Research\nProjects Activity (IARPA), via the HIATUS Pro-\ngram contract #2022-22072200004. The views and\nconclusions contained herein are those of the au-\nthors and should not be interpreted as necessarily\nrepresenting the official policies, either expressed\nor implied, of ODNI, IARPA, or the U.S. Gov-\nernment. The U.S. Government is authorized to\nreproduce and distribute reprints for governmental\npurposes notwithstanding any copyright annotation\ntherein.\nReferences\nAdel Abdelli, Fayçal Guerrouf, Okba Tibermacine, and\nBelkacem Abdelli. 2019. Sentiment analysis of Ara-\nbic Algerian dialect using a supervised method. In\n2019 International Conference on Intelligent Systems\nand Advanced Computing Sciences (ISACS), pages\n1–6.\nMuhammad Abdul-Mageed, AbdelRahim Elmadany,\nand El Moatez Billah Nagoudi. 2021. ARBERT &\nMARBERT: Deep bidirectional transformers for Ara-\nbic. In Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the\n11th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers), pages\n7088–7105, Online. Association for Computational\nLinguistics.\nDavid Ifeoluwa Adelani, Hannah Liu, Xiaoyu Shen,\nNikita Vassilyev, Jesujoba O. Alabi, Yanke Mao, Hao-\nnan Gao, and Annie En-Shiun Lee. 2023. SIB-200:\nA simple, inclusive, and big evaluation dataset for\ntopic classification in 200+ languages and dialects.\nSanchit Ahuja, Divyanshu Aggarwal, Varun Gumma,\nIshaan Watts, Ashutosh Sathe, Millicent Ochieng,\nRishav Hada, Prachi Jain, Maxamed Axmed, Ka-\nlika Bali, and Sunayana Sitaram. 2023. Megaverse:\nBenchmarking large language models across lan-\nguages, modalities, models and tasks.\nMarwan Al Omari, Moustafa Al-Hajj, Nacereddine\nHammami, and Amani Sabra. 2019. Sentiment classi-\nfier: Logistic regression for Arabic services’ reviews\nin Lebanon. In 2019 International Conference on\nComputer and Information Sciences (ICCIS), pages\n1–5.\nMd Mahfuz Ibn Alam, Sina Ahmadi, and Antonios\nAnastasopoulos. 2023. CODET: A benchmark for\ncontrastive dialectal evaluation of machine transla-\ntion.\nKhaled Mohammad Alomari, Hatem M. ElSherif, and\nKhaled Shaalan. 2017. Arabic tweets sentimental\nanalysis using machine learning. In Advances in Ar-\ntificial Intelligence: From Theory to Practice, pages\n602–610, Cham. Springer International Publishing.\nDhuha Alqahtani, Lama Alzahrani, Maram Bahareth,\nNora Alshameri, Hend Al-Khalifa, and Luluh Ald-\nhubayi. 2022. Customer sentiments toward Saudi\nbanks during the Covid-19 pandemic. In Proceed-\nings of the 5th International Conference on Natural\nLanguage and Speech Processing (ICNLSP 2022),\npages 251–257, Trento, Italy. Association for Com-\nputational Linguistics.\nLucas Bandarkar, Davis Liang, Benjamin Muller, Mikel\nArtetxe, Satya Narayan Shukla, Donald Husa, Naman\nGoyal, Abhinandan Krishnan, Luke Zettlemoyer, and\nMadian Khabsa. 2023. The Belebele benchmark: a\nparallel reading comprehension dataset in 122 lan-\nguage variants. arXiv preprint arXiv:2308.16884.\nVerena Blaschke, Hinrich Schütze, and Barbara Plank.\n2023.\nDoes manipulating tokenization aid cross-\nlingual transfer? A study on POS tagging for non-\nstandardized languages. In Proceedings of the Tenth\nWorkshop on NLP for Similar Languages, Varieties\nand Dialects, pages 40–54, Dubrovnik, Croatia. As-\nsociation for Computational Linguistics.\nDamian Blasi, Antonios Anastasopoulos, and Gra-\nham Neubig. 2022. Systematic inequalities in lan-\nguage technology performance across the world’s\nlanguages. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 5486–5505, Dublin,\nIreland. Association for Computational Linguistics.\nSu Lin Blodgett, Johnny Wei, and Brendan O’Connor.\n2018.\nTwitter Universal Dependency parsing for\nAfrican-American and mainstream American En-\nglish. In Proceedings of the 56th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 1415–1425, Melbourne,\nAustralia. Association for Computational Linguistics.\nHouda Bouamor, Nizar Habash, Mohammad Salameh,\nWajdi Zaghouani, Owen Rambow, Dana Abdul-\nrahim, Ossama Obeid, Salam Khalifa, Fadhl Eryani,\nAlexander Erdmann, and Kemal Oflazer. 2018. The\nMADAR Arabic dialect corpus and lexicon. In Pro-\nceedings of the Eleventh International Conference on\nLanguage Resources and Evaluation (LREC 2018),\nMiyazaki, Japan. European Language Resources As-\nsociation (ELRA).\nJack K. Chambers and Peter Trudgill. 1998. Dialectol-\nogy. Cambridge University Press.\nJonathan H. Clark, Eunsol Choi, Michael Collins, Dan\nGarrette, Tom Kwiatkowski, Vitaly Nikolaev, and\nJennimaria Palomaki. 2020. TyDi QA: A benchmark\nfor information-seeking question answering in typo-\nlogically diverse languages. Transactions of the As-\nsociation for Computational Linguistics, 8:454–470.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Adina\nWilliams, Samuel Bowman, Holger Schwenk, and\nVeselin Stoyanov. 2018. XNLI: Evaluating cross-\nlingual sentence representations. In Proceedings of\nthe 2018 Conference on Empirical Methods in Nat-\nural Language Processing, pages 2475–2485, Brus-\nsels, Belgium. Association for Computational Lin-\nguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nAbdelRahim Elmadany, ElMoatez Billah Nagoudi, and\nMuhammad Abdul-Mageed. 2023. ORCA: A chal-\nlenging benchmark for Arabic language understand-\ning. In Findings of the Association for Computa-\ntional Linguistics: ACL 2023, pages 9559–9586,\nToronto, Canada. Association for Computational Lin-\nguistics.\nFahim Faisal, Sharlina Keshava, Md Mahfuz Ibn Alam,\nand Antonios Anastasopoulos. 2021. SD-QA: Spo-\nken dialectal question answering for the real world.\nIn Findings of the Association for Computational\nLinguistics: EMNLP 2021, pages 3296–3315, Punta\nCana, Dominican Republic. Association for Compu-\ntational Linguistics.\nChayma Fourati, Hatem Haddad, Abir Messaoudi,\nMoez BenHajhmida, Aymen Ben Elhaj Mabrouk,\nand Malek Naski. 2021. Introducing a large Tunisian\nArabizi dialectal dataset for sentiment analysis. In\nProceedings of the Sixth Arabic Natural Language\nProcessing Workshop, pages 226–230, Kyiv, Ukraine\n(Virtual). Association for Computational Linguistics.\nMoncef Garouani and Jamal Kharroubi. 2022. MAC:\nAn open and free Moroccan Arabic corpus for sen-\ntiment analysis. In Innovations in Smart Cities Ap-\nplications Volume 5, pages 849–858, Cham. Springer\nInternational Publishing.\nMika Hämäläinen, Khalid Alnajjar, Niko Partanen, and\nJack Rueter. 2021.\nFinnish dialect identification:\nThe effect of audio and text. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 8777–8783, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nHarald Hammarström and Robert Forkel. 2022. Glot-\ntocodes: Identifiers linking families, languages and\ndialects to comprehensive reference information. Se-\nmantic Web Journal, 13(6):917–924.\nMichael A. Hedderich, Lukas Lange, Heike Adel, Jan-\nnik Strötgen, and Dietrich Klakow. 2021. A survey\non recent approaches for natural language process-\ning in low-resource scenarios. In Proceedings of\nthe 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 2545–2568,\nOnline. Association for Computational Linguistics.\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Gra-\nham Neubig, Orhan Firat, and Melvin Johnson.\n2020. XTREME: A massively multilingual multi-\ntask benchmark for evaluating cross-lingual gener-\nalisation. In Proceedings of the 37th International\nConference on Machine Learning, volume 119 of\nProceedings of Machine Learning Research, pages\n4411–4421. PMLR.\nTommi Jauhiainen, Heidi Jauhiainen, and Krister\nLindén. 2022. Italian language and dialect identi-\nfication and regional French variety detection using\nadaptive naive Bayes. In Proceedings of the Ninth\nWorkshop on NLP for Similar Languages, Varieties\nand Dialects, pages 119–129, Gyeongju, Republic of\nKorea. Association for Computational Linguistics.\nTommi Jauhiainen, Krister Lindén, and Heidi Jauhi-\nainen. 2019. Discriminating between Mandarin Chi-\nnese and Swiss-German varieties using adaptive lan-\nguage models. In Proceedings of the Sixth Work-\nshop on NLP for Similar Languages, Varieties and\nDialects, pages 178–187, Ann Arbor, Michigan. As-\nsociation for Computational Linguistics.\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Men-\nsch, Chris Bamford, Devendra Singh Chaplot, Diego\nde las Casas, Florian Bressand, Gianna Lengyel,\nGuillaume Lample, Lucile Saulnier, Lélio Re-\nnard Lavaud, Marie-Anne Lachaux, Pierre Stock,\nTeven Le Scao, Thibaut Lavril, Thomas Wang, Timo-\nthée Lacroix, and William El Sayed. 2023. Mistral\n7B. arXiv:2310.06825.\nBjarte Johansen. 2019. Named-entity recognition for\nNorwegian. In Proceedings of the 22nd Nordic Con-\nference on Computational Linguistics, NoDaLiDa.\nAnjali Kantharuban, Ivan Vuli´c, and Anna Korhonen.\n2023. Quantifying the dialect gap and its correlates\nacross languages.\nSimran Khanuja, Sandipan Dandapat, Anirudh Srini-\nvasan, Sunayana Sitaram, and Monojit Choudhury.\n2020.\nGLUECoS: An evaluation benchmark for\ncode-switched NLP. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 3575–3585, Online. Association\nfor Computational Linguistics.\nHeather Lent, Kushal Tatariya, Raj Dabre, Yiyi Chen,\nMarcell Fekete, Esther Ploeger, Li Zhou, Hans Erik\nHeje, Diptesh Kanojia, Paul Belony, Marcel Boll-\nmann, Loïc Grobol, Miryam de Lhoneux, Daniel\nHershcovich, Michel DeGraff, Anders Søgaard, and\nJohannes Bjerva. 2023. CreoleVal: Multilingual mul-\ntitask benchmarks for creoles.\nYaobo Liang, Nan Duan, Yeyun Gong, Ning Wu, Fenfei\nGuo, Weizhen Qi, Ming Gong, Linjun Shou, Daxin\nJiang, Guihong Cao, Xiaodong Fan, Ruofei Zhang,\nRahul Agrawal, Edward Cui, Sining Wei, Taroon\nBharti, Ying Qiao, Jiun-Hung Chen, Winnie Wu,\nShuguang Liu, Fan Yang, Daniel Campos, Rangan\nMajumder, and Ming Zhou. 2020. XGLUE: A new\nbenchmark dataset for cross-lingual pre-training, un-\nderstanding and generation. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 6008–6018,\nOnline. Association for Computational Linguistics.\nRichard Littauer. n.d.\nLow resource languages:\nResources for conservation,\ndevelopment,\nand\ndocumentation\nof\nlow\nresource\n(human)\nlan-\nguages.\nhttps://github.com/RichardLitt/\nlow-resource-languages.\n[Accessed 15-11-\n2023].\nSalima Medhaffar, Fethi Bougares, Yannick Estève, and\nLamia Hadrich-Belguith. 2017.\nSentiment analy-\nsis of Tunisian dialects: Linguistic ressources and\nexperiments. In Proceedings of the Third Arabic\nNatural Language Processing Workshop, pages 55–\n61, Valencia, Spain. Association for Computational\nLinguistics.\nJamshidbek Mirzakhalov. 2021. Turkic Interlingua: A\nCase Study of Machine Translation in Low-resource\nLanguages.\nPh.D. thesis, University of South\nFlorida.\nJamshidbek Mirzakhalov, Anoop Babu, Duygu Ataman,\nSherzod Kariev, Francis Tyers, Otabek Abduraufov,\nMammad Hajili, Sardana Ivanova, Abror Khaytbaev,\nAntonio Laverghetta Jr., Bekhzodbek Moydinboyev,\nEsra Onal, Shaxnoza Pulatova, Ahsan Wahab, Orhan\nFirat, and Sriram Chellappan. 2021a. A large-scale\nstudy of machine translation in Turkic languages.\nIn Proceedings of the 2021 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n5876–5890, Online and Punta Cana, Dominican Re-\npublic. Association for Computational Linguistics.\nJamshidbek Mirzakhalov, Anoop Babu, Duygu Ataman,\nSherzod Kariev, Francis Tyers, Otabek Abduraufov,\nMammad Hajili, Sardana Ivanova, Abror Khaytbaev,\nAntonio Laverghetta Jr, et al. 2021b. A large-scale\nstudy of machine translation in turkic languages.\nIn Proceedings of the 2021 Conference on Empir-\nical Methods in Natural Language Processing, pages\n5876–5890.\nMahmoud Nabil, Mohamed Aly, and Amir Atiya. 2015.\nASTD: Arabic sentiment tweets dataset. In Proceed-\nings of the 2015 Conference on Empirical Methods\nin Natural Language Processing, pages 2515–2519,\nLisbon, Portugal. Association for Computational Lin-\nguistics.\nNLLB Team, Marta R. Costa-jussà, James Cross, Onur\nÇelebi, Maha Elbayad, Kenneth Heafield, Kevin Hef-\nfernan, Elahe Kalbassi, Janice Lam, Daniel Licht,\nJean Maillard, Anna Sun, Skyler Wang, Guillaume\nWenzek, Al Youngblood, Bapi Akula, Loic Bar-\nrault, Gabriel Mejia Gonzalez, Prangthip Hansanti,\nJohn Hoffman, Semarley Jarrett, Kaushik Ram\nSadagopan, Dirk Rowe, Shannon Spruit, Chau\nTran, Pierre Andrews, Necip Fazil Ayan, Shruti\nBhosale, Sergey Edunov, Angela Fan, Cynthia\nGao, Vedanuj Goswami, Francisco Guzmán, Philipp\nKoehn, Alexandre Mourachko, Christophe Ropers,\nSafiyyah Saleem, Holger Schwenk, and Jeff Wang.\n2022. No Language Left Behind: Scaling human-\ncentered machine translation. arXiv:2207.04672.\nSebastian Nordhoff. 2012. Linked data for linguistic\ndiversity research: Glottolog/langdoc and asjp. In\nChristian Chiarcos, Sebastian Nordhoff, and Sebas-\ntian Hellmann, editors, Linked Data in Linguistics.\nRepresenting and Connecting Language Data and\nLanguage Metadata, pages 191–200. Springer, Hei-\ndelberg.\nXiaoman Pan, Boliang Zhang, Jonathan May, Joel Noth-\nman, Kevin Knight, and Heng Ji. 2017. Cross-lingual\nname tagging and linking for 282 languages. In Pro-\nceedings of the 55th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 1946–1958, Vancouver, Canada. As-\nsociation for Computational Linguistics.\nSungjoon Park, Jihyung Moon, Sungdong Kim, Won Ik\nCho, Ji Yoon Han, Jangwon Park, Chisung Song, Jun-\nseong Kim, Youngsook Song, Taehwan Oh, Joohong\nLee, Juhyun Oh, Sungwon Lyu, Younghoon Jeong,\nInkwon Lee, Sangwoo Seo, Dongjun Lee, Hyunwoo\nKim, Myeonghwa Lee, Seongbo Jang, Seungwon\nDo, Sunkyoung Kim, Kyungtae Lim, Jongwon Lee,\nKyumin Park, Jamin Shin, Seonghyun Kim, Lucy\nPark, Lucy Park, Alice Oh, Jung-Woo Ha (NAVER\nAI Lab), Kyunghyun Cho, and Kyunghyun Cho.\n2021. KLUE: Korean language understanding eval-\nuation. In Proceedings of the Neural Information\nProcessing Systems Track on Datasets and Bench-\nmarks, volume 1. Curran.\nAlexandre Rademaker, Fabricio Chalub, Livy Real,\nCláudia Freitas, Eckhard Bick, and Valeria de Paiva.\n2017. Universal Dependencies for Portuguese. In\nProceedings of the Fourth International Conference\non Dependency Linguistics (Depling), pages 197–\n206, Pisa, Italy.\nAfshin Rahimi, Yuan Li, and Trevor Cohn. 2019. Mas-\nsively multilingual transfer for NER. In Proceedings\nof the 57th Annual Meeting of the Association for\nComputational Linguistics, pages 151–164, Florence,\nItaly. Association for Computational Linguistics.\nAnthony Rios. 2020.\nFuzzE: Fuzzy fairness evalu-\nation of offensive language classifiers on African-\nAmerican English. Proceedings of the AAAI Confer-\nence on Artificial Intelligence, 34(01):881–889.\nSebastian Ruder, Jonathan H. Clark, Alexander Gutkin,\nMihir Kale, Min Ma, Massimo Nicosia, Shruti Rijh-\nwani, Parker Riley, Jean-Michel A. Sarr, Xinyi Wang,\nJohn Wieting, Nitish Gupta, Anna Katanova, Christo\nKirov, Dana L. Dickinson, Brian Roark, Bidisha\nSamanta, Connie Tao, David I. Adelani, Vera Ax-\nelrod, Isaac Caswell, Colin Cherry, Dan Garrette,\nReeve Ingle, Melvin Johnson, Dmitry Panteleev, and\nPartha Talukdar. 2023. XTREME-UP: A user-centric\nscarce-data benchmark for under-represented lan-\nguages.\nSebastian Ruder, Noah Constant, Jan Botha, Aditya Sid-\ndhant, Orhan Firat, Jinlan Fu, Pengfei Liu, Junjie\nHu, Dan Garrette, Graham Neubig, and Melvin John-\nson. 2021. XTREME-R: Towards more challenging\nand nuanced multilingual evaluation. In Proceedings\nof the 2021 Conference on Empirical Methods in\nNatural Language Processing, pages 10215–10245,\nOnline and Punta Cana, Dominican Republic. Asso-\nciation for Computational Linguistics.\nHanna Sababa and Athena Stassopoulou. 2018. A clas-\nsifier to distinguish between Cypriot Greek and Stan-\ndard Modern Greek. In 2018 Fifth International Con-\nference on Social Networks Analysis, Management\nand Security (SNAMS), pages 251–255.\nYves Scherrer, Tanja Samardži´c, and Elvira Glaser.\n2019. Digitising Swiss German: how to process\nand study a polycentric spoken language. Language\nResources and Evaluation, 53.\nHaitham Seelawi, Ibraheem Tuffaha, Mahmoud Gzawi,\nWael Farhan, Bashar Talafha, Riham Badawi, Zyad\nSober, Oday Al-Dweik, Abed Alhakim Freihat, and\nHussein Al-Natsheh. 2021. ALUE: Arabic language\nunderstanding evaluation.\nIn Proceedings of the\nSixth Arabic Natural Language Processing Workshop,\npages 173–184, Kyiv, Ukraine (Virtual). Association\nfor Computational Linguistics.\nYueqi Song, Catherine Cui, Simran Khanuja, Pengfei\nLiu, Fahim Faisal, Alissa Ostapenko, Genta Indra\nWinata, Alham Fikri Aji, Samuel Cahyawijaya, Yu-\nlia Tsvetkov, Antonios Anastasopoulos, and Gra-\nham Neubig. 2023.\nGlobalBench: A benchmark\nfor global progress in natural language processing.\nHongmin Wang, Yue Zhang, GuangYong Leonard Chan,\nJie Yang, and Hai Leong Chieu. 2017. Universal\nDependencies parsing for colloquial Singaporean En-\nglish. In Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 1732–1744, Vancouver,\nCanada. Association for Computational Linguistics.\nYizhong Wang, Swaroop Mishra, Pegah Alipoormo-\nlabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva\nNaik, Arjun Ashok, Arut Selvan Dhanasekaran,\nAnjana Arunkumar, David Stap, Eshaan Pathak,\nGiannis Karamanolakis, Haizhi Lai, Ishan Puro-\nhit, Ishani Mondal, Jacob Anderson, Kirby Kuznia,\nKrima Doshi, Kuntal Kumar Pal, Maitreya Patel,\nMehrad Moradshahi, Mihir Parmar, Mirali Purohit,\nNeeraj Varshney, Phani Rohitha Kaza, Pulkit Verma,\nRavsehaj Singh Puri, Rushang Karia, Savan Doshi,\nShailaja Keyur Sampat, Siddhartha Mishra, Sujan\nReddy A, Sumanta Patro, Tanay Dixit, and Xudong\nShen. 2022. Super-NaturalInstructions: Generaliza-\ntion via declarative instructions on 1600+ NLP tasks.\nIn Proceedings of the 2022 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n5085–5109, Abu Dhabi, United Arab Emirates. As-\nsociation for Computational Linguistics.\nBryan Wilie, Karissa Vincentio, Genta Indra Winata,\nSamuel Cahyawijaya, Xiaohong Li, Zhi Yuan Lim,\nSidik Soleman, Rahmad Mahendra, Pascale Fung,\nSyafri Bahar, and Ayu Purwarianti. 2020. IndoNLU:\nBenchmark and resources for evaluating Indonesian\nnatural language understanding. In Proceedings of\nthe 1st Conference of the Asia-Pacific Chapter of the\nAssociation for Computational Linguistics and the\n10th International Joint Conference on Natural Lan-\nguage Processing, pages 843–857, Suzhou, China.\nAssociation for Computational Linguistics.\nMarcos Zampieri, Kai North, Tommi Jauhiainen, Mar-\niano Felice, Neha Kumari, Nishant Nair, and Yash\nBangera. 2023. Language variety identification with\ntrue labels.\nDaniel Zeman, Joakim Nivre, Mitchell Abrams, Elia\nAckermann, Noëmi Aepli, Hamid Aghaei, Željko\nAgi´c,\nAmir Ahmadi,\nLars Ahrenberg,\nAjede,\nand et al. 2021.\nUniversal Dependencies 2.9.\nLINDAT/CLARIAH-CZ digital library at the Insti-\ntute of Formal and Applied Linguistics (ÚFAL), Fac-\nulty of Mathematics and Physics, Charles University.\nCaleb Ziems, Jiaao Chen, Camille Harris, Jessica An-\nderson, and Diyi Yang. 2022. VALUE: Understand-\ning dialect disparity in NLU. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n3701–3720, Dublin, Ireland. Association for Compu-\ntational Linguistics.\nCaleb Ziems, William Held, Jingfeng Yang, Jwala\nDhamala, Rahul Gupta, and Diyi Yang. 2023. Multi-\nVALUE: A framework for cross-dialectal English\nNLP. In Proceedings of the 61st Annual Meeting\nof the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 744–768, Toronto,\nCanada. Association for Computational Linguistics.\nAppendix\nIn this supplementary material, we provide the following: (i) Relevant literature review (ii) Overall results\nand dataset details that we could not fit into the main body of the paper.\nSection\nDesctiption\nAppendix A\nRelated Work\nAppendix B\nTasks of DIALECTBENCH\n• Translate-Test NLI Dataset Statistics (Table 7)\nAppendix C\nVarieties and Clusters of DIALECTBENCH\n• DIALECTBENCH Variety list (Table 8)\n• Language Clusters and Representative Varieties (Table 9)\nAppendix D\nResult Visualizations\nRegional maps with aggregated Machine Translation scores (Figs. 5 to 6)\n• Map of Switzerland with aggregated BLEU scores of Swiss-German variety per region (Fig. 5)\n• Map of Italy with aggregated BLEU scores of Italian variety per region (Fig. 6)\nTask Specific Plot for Maximum Scores (Figs. 7 to 10)\n• Parts-of-Speech Tagging (Fig. 7a)\n• Named entity recognition (Fig. 7b)\n• Topic classification (Fig. 7c)\n• Natural language inference (Fig. 8a)\n• Extractive question answering (Fig. 8b)\n• Multiple-choice machine reading comprehension (Fig. 8c)\n• Sentiment analysis (Fig. 9a)\n• Dialect identification (Fig. 9b)\n• Machine translation (MT-region) (Fig. 10a)\n• Machine translation (MT-dialect) (Fig. 10b)\nDialectal Gap visualization utilizing zero-shot cross-lingual transfer from Standard English.\n• Parts-of-Speech Tagging (Fig. 11a)\n• Named entity recognition (Fig. 11b)\n• Dialect identification (Fig. 11c)\nAppendix E\nTask Specific Evaluation Result Tables\n• Dependency parsing (Table 10)\n• Parts-of-Speech tagging (Table 11)\n• Named entity recognition (Table 12)\n• Natural language inference (Table 13)\n• Extractive question answering (Tables 14 to 15)\n• Dialect identification (Table 16)\n• Topic classification (Table 17)\n• Sentiment analysis (Table 18)\n• Multiple-choice machine reading comprehension (Table 19)\n• Machine translation (Table 20)\nAppendix F\nHighest performing and lowest performing varieties (Table 21)\nAppendix G\nLow-resource Variety performing better in zero-shot NER (Table 22)\nAppendix H\nCluster level Result Summaries with Demographic Utility and Standard Deviation report (Tables 23 to 31)\nAppendix I\nIn-Context Learning Details\nTable 6: Table of contents for supplementary material.\nA\nRelated Work\nThe majority of multilingual benchmarks (Hu et al., 2020; Ruder et al., 2023, 2021; Liang et al., 2020;\nWilie et al., 2020; Park et al., 2021) have heavily focused on dominant language varieties. However,\nthe performance disparity between standard languages and their dialectal counterparts has been studied\n(Kantharuban et al., 2023; Ziems et al., 2022; Rios, 2020; Ziems et al., 2023) and shown to be significantly\nlarge across different tasks. Still, the large-scale generalization of this finding comprising numerous task\ncategories is yet to be done. There have been previous efforts to bridge this gap though. Some work has\nfocused on curating dialectal datasets across several tasks within one language cluster, while others have\nfocused on single tasks across many language clusters. For instance, ORCA (Elmadany et al., 2023),\nARLUE (Abdul-Mageed et al., 2021) and ALUE (Seelawi et al., 2021) are dedicated natural language\nunderstanding benchmarks focusing on Arabic varieties alone. Multi-VALUE (Ziems et al., 2023) was\ndeveloped for benchmarking NLP tasks in English varieties. (Mirzakhalov et al., 2021a) is a suite of\nresources for benchmarking MT in several Turkic languages. There has also been a large body of work on\ndialect identification across several languages (Jauhiainen et al., 2022; Hämäläinen et al., 2021). Recently\nCODET (Alam et al., 2023) was released as a contrastive dialectal MT benchmark covering 882 different\nvariations from nine different languages. To the best of our knowledge, we are the first to do a large scale\naggregation of dialectal data across several language clusters and tasks.\nB\nTasks of DIALECTBENCH\nDIALECTBENCH includes 10 NLP tasks falling into four broader categories: structured prediction,\nsentence classification, question answering, and text generation. In Table 1, we present statistics for the\ndatasets for each task and briefly discuss each task below.\nDependency parsing\nFor the dependency parsing task, we include only those Universal Dependencies\n(UD) (Zeman et al., 2021) languages that have dialectal data. Beyond the data available in UD 2.12, we\nincorporate two additional datasets for African-American English (AAVE) Twitter data (Blodgett et al.,\n2018) and Singlish (Wang et al., 2017). To make these two datasets compatible with the UD processing\npipeline, we replace the original dependency labels with the labels corresponding to the official UD\nformalism.\nPart of speech (POS) tagging\nWe use the same UD languages for POS tagging that we used for\ndependency parsing. At the same time, we use the POS data instances from Singlish (Wang et al., 2017).\nMoreover, we include six Finnish dialects, four Arabic dialects and Occitan through the UPOS label\nstandardized pipeline proposed by Blaschke et al. (2023).\nNamed entity recognition (NER)\nWe use data from the 176-language version of the Wikiann dataset\nprocessed by Rahimi et al. (2019). All these languages provide both training and test data. In addition, we\ninclude dialectal data from the original Wikiann dataset (282 languages) (Pan et al., 2017) for evaluation.\nMoreover, we include three Norwegian dialects (Johansen, 2019) with train, test and validation datasets\nthat use a slightly different set of NER tags (GEO, ORG, OTH, PER) compared to the one we use in\nWikiann (LOC, ORG, PER). We leave these levels as it is and do not convert to the Wikiann tags.\nDialect identification\nWe include dialect identification experiments on Arabic, Greek, Portuguese,\nEnglish, Spanish, and Swiss German dialectal datasets. In these datasets, we find large variations in the\nlevel of granularity with which dialects are classified. For instance, the MADAR corpus differentiates\nArabic varieties at the city level (Bouamor et al., 2018), whereas our English and Spanish datasets are\nlabeled with country names (Zampieri et al., 2023).\nSentiment classification\nHere we include several different Arabic varieties. Like other dialectal\ndatasets, these datasets do not follow one standard labeling process. However, all datasets contain two\nmain sentiment types: positive and negative. A number of datasets contain additional labels such as\n“objective” or “neutral.” In our setting, we perform a further split of data to provide validation data for\neach dialect. However, we do not remove these extra labeled data for information preservation.\nTopic classification\nWe use the SIB-200 dataset (Adelani et al., 2023) for topic classification task\nSIB-200 was constructed from the FLORES-200 translation datasets. The authors annotated the English\ndataset of FLORES-200 with 6 topic labels and then further propagated these labels to the translated\ninstances for all other languages. For our case of benchmarking dialectal segments, we use the dialectal\nand regional varieties from SIB-200.\nVariety\n# Sentences\ncluster\nLanguage code\nanglic\neng_Latn\nenglish\n5010\narabic\nacm_Arab\nnorth mesopotamian arabic\n5010\nacq_Arab\nta’izzi-adeni arabic\n5010\naeb_Arab\ntunisian arabic\n5010\najp_Arab\nsouth levantine arabic\n5010\napc_Arab\nlevantine arabic (a:north)\n5010\narb_Arab\nstandard arabic\n5010\nars_Arab\nnajdi arabic\n5010\nary_Arab\nmoroccan arabic\n5010\narz_Arab\negyptian arabic\n5010\ncommon turkic\nazb_Arab\nsouth azerbaijani\n5010\nazj_Latn\nnorth azerbaijani\n5010\ntur_Latn\ncentral oghuz (m:spoken)\n5010\ngallo-italian\nlij_Latn\nligurian\n5010\nlmo_Latn\nlombard\n5010\nvec_Latn\nvenetian\n5010\ngallo-rhaetian\nfur_Latn\nfriulian\n5010\nhigh german\nlim_Latn\nlimburgan\n5010\nltz_Latn\nluxemburgish\n5010\nitalian romance\nita_Latn\nitalian\n5010\nscn_Latn\nsicilian\n5010\nkurdish\nckb_Arab\ncentral kurdish\n5010\nkmr_Latn\nnorthern kurdish\n5010\nlatvian\nltg_Latn\neast latvian\n5010\nlvs_Latn\nlatvian\n5010\nmodern dutch\nnld_Latn\ndutch\n5010\nnorwegian\nnno_Latn\nnorwegian nynorsk (m:written)\n5010\nnob_Latn\nnorwegian bokmål (m:written)\n5010\nsardo-corsican\nsrd_Latn\nsardinian\n5010\nsinitic\nyue_Hant\ncantonese\n5010\nzho_Hans\nclassical-middle-modern sinitic (o:simplified)\n5010\nzho_Hant\nclassical-middle-modern sinitic (o:traditional)\n5010\nsotho-tswana (s.30)\nnso_Latn\nnorthern sotho\n5010\nsot_Latn\nsouthern sotho\n5010\nsouthwestern shifted romance\nglg_Latn\ngalician\n5010\noci_Latn\noccitan\n5010\npor_Latn\nportuguese (a:european)\n5010\nspa_Latn\nspanish\n5010\nTable 7: Data statistics for newly created translate-test natural language inference (NLI) dataset. We prepare this\ntranslate-test NLI dataset by translating XNLI (Conneau et al., 2018) english evaluation dataset.\nNatural language inference\nFor the natural language inference task, there is no existing dataset\nwith varieties. So we use the existing English test set of XNLI (Conneau et al., 2018) and construct a\nmultilingual dialect-focused translated evaluation dataset. We use a state-of-the-art machine translation\nmodel (NLLB-200 3B) to translate the English test set to 12 language clusters encompassing 40 varieties\n(Complete data statistics are reported in Table 7). After that, we perform zero-shot cross-lingual transfer\nfrom the English finetuned NLI model. We refer to this setting as translate-test.\nMultiple-choice machine reading comprehension\nThis task aims to evaluate the capability of multiple-\nchoice question answering given a context passage. The question could be answered by understanding the\ncontext passage while the right answer is given at one of the multiple choices. We use the Chinese, Sotho\nand Arabic clusters data from the recently released Belebele MRC dataset (Bandarkar et al., 2023). This\nis an evaluation-only dataset.\nExtractive question answering\nThis task predicts the answer span given a question and context passage.\nWe use the SD-QA dialectal question-answering dataset (Faisal et al., 2021). SD-QA is an evaluation\ndataset built on top of TyDiQA (Clark et al., 2020), another well-known typologically diverse question-\nanswering dataset. SD-QA contains the spoken utterances and transcription of the original TyDiQA\nquestion from speakers of English, Bengali, Arabic, Korean, and Swahili. We only use the textual part\nthat contains the transcription of the dialectal spoken question matching the original TyDiQA question\ntext. Note that since the transcriptions of the questions are obtained through automatic speech recognition,\nthey may include both dialectal variations and noise due to ASR transcription errors.\nMachine translation\nWe evaluate variety translation using the CODET benchmark (Alam et al., 2023)\nand the TIL MT corpus (Mirzakhalov et al., 2021b). CODET contains a contrastive dataset of 882\ndifferent varieties from nine different languages. We evaluate dialects here at city level for all languages\nexcept Italian and Swiss-German and aggregate dialects at the region level for them. The TIL corpus\ncontains parallel translations across 22 Turkic languages, but in our evaluations we only include 8 turkic\nlanguages (Turkic, Sakha, Kazakh, Karakalpak, Bashkir, Azerbaijani, Kyrgyz) that have parallel English\ntranslations.\nC\nVarieties and Clusters of DIALECTBENCH\nC.1\nDIALECTBENCH Variety list\nTable 8: Varieties represented in DialectBench.\nLanguage cluster\nVariety name\nGlottocode\nmBERT seen\nUDP\nPOS\nNER\nSDQA\nRCMC\nNLI\nTC\nSC\nDI\nMT\nalbanian\nalbanian\nalba1267\n✓\n✓\n✓\n-\n-\n-\n-\n-\n-\n-\n-\ngheg albanian\ngheg1238\n-\n✓\n✓\n-\n-\n-\n-\n-\n-\n-\n-\nanglic\nafrican american vernacular\nenglish\nafri1276\n-\n✓\n-\n-\n-\n-\n-\n-\n-\n-\n-\naustralian english\naust1314\n-\n-\n-\n-\n✓\n-\n-\n-\n-\n-\n-\nenglish\nstan1293\n✓\n✓\n✓\n✓\n-\n✓\n✓\n✓\n-\n✓\n-\nenglish (a:scotland)\nstan1293\n-\n-\n-\n-\n✓\n-\n-\n-\n-\n-\n-\nenglish (a:uk)\nstan1293\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\n-\nenglish (o:controlled)\nstan1293\n-\n-\n-\n✓\n-\n-\n-\n-\n-\n-\n-\nindian english (a:north)\nindi1255\n-\n-\n-\n-\n✓\n-\n-\n-\n-\n-\n-\nindian english (a:south)\nindi1255\n-\n-\n-\n-\n✓\n-\n-\n-\n-\n-\n-\nirish english\niris1255\n-\n-\n-\n-\n✓\n-\n-\n-\n-\n-\n-\njamaican creole english\njama1262\n-\n-\n-\n✓\n-\n-\n-\n-\n-\n-\n-\nkenyan english\nkeny1281\n-\n-\n-\n-\n✓\n-\n-\n-\n-\n-\n-\nnew zealand english\nnewz1240\n-\n-\n-\n-\n✓\n-\n-\n-\n-\n-\n-\nnigerian english\nnige1260\n-\n-\n-\n-\n✓\n-\n-\n-\n-\n-\n-\nnorth american english\nnort3314\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\n-\nold english (ca. 450-1100)\nolde1238\n-\n-\n-\n✓\n-\n-\n-\n-\n-\n-\n-\nphilippine english\nphil1246\n-\n-\n-\n-\n✓\n-\n-\n-\n-\n-\n-\nsinglish\nsing1272\n-\n✓\n✓\n-\n-\n-\n-\n-\n-\n-\n-\nsoutheast american english\nsout3300\n-\n-\n-\n-\n✓\n-\n-\n-\n-\n-\n-\nsouthern african english\nsout3331\n-\n-\n-\n-\n✓\n-\n-\n-\n-\n-\n-\narabic\naleppo\nalep1241\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\n✓\nalgerian arabic\nalge1239\n-\n-\n-\n-\n✓\n-\n-\n-\n✓\n✓\n✓\narabian\npeninsula\narabic\n(a:yemen)\narab1393\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\n✓\narabic (a:bahrain)\n-\n-\n-\n-\n-\n✓\n-\n-\n-\n-\n-\n-\narabic (a:jordan)\n-\n-\n-\n-\n-\n✓\n-\n-\n-\n✓\n-\n-\narabic (a:saudi-arabia)\n-\n-\n-\n-\n-\n✓\n-\n-\n-\n✓\n-\n-\nContinued on next page\nTable 8: Varieties represented in DialectBench.\nLanguage clusters\nVariety name\nGlottocode\nmBERT seen\nUDP\nPOS\nNER\nSDQA\nRCMC\nNLI\nTC\nSC\nDI\nMT\negyptian arabic\negyp1253\n-\n-\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n-\n-\negyptian arabic (a:alx)\negyp1253\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\n✓\negyptian arabic (a:asw)\negyp1253\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\n✓\negyptian arabic (a:cai)\negyp1253\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\n✓\negyptian arabic (a:kha)\negyp1253\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\n✓\nfez. meknes\nfezm1238\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\n✓\ngilit mesopotamian arabic\nmeso1252\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\n✓\ngulf arabic\ngulf1241\n-\n-\n✓\n-\n-\n-\n-\n-\n-\n-\n-\ngulf arabic (a:doh)\ngulf1241\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\n✓\ngulf arabic (a:jed)\ngulf1241\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\n✓\ngulf arabic (a:mus)\ngulf1241\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\n✓\ngulf arabic (a:riy)\ngulf1241\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\n✓\nlevantine arabic\nnort3139\n-\n-\n✓\n-\n-\n-\n-\n-\n-\n-\n-\nlevantine arabic (a:north)\nnort3139\n-\n-\n-\n-\n-\n✓\n✓\n✓\n-\n-\n-\nlevantine arabic (a:north-dam)\nnort3139\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\n✓\nlibyan arabic\nliby1240\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nlibyan arabic (a:ben)\nliby1240\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\n✓\nmoroccan arabic\nmoro1292\n-\n-\n-\n-\n✓\n✓\n✓\n✓\n✓\n-\n-\nnajdi arabic\nnajd1235\n-\n-\n-\n-\n-\n✓\n✓\n✓\n-\n-\n-\nnorth african arabic\nnort3191\n-\n✓\n✓\n-\n-\n-\n-\n-\n-\n-\n-\nnorth mesopotamian arabic\nnort3142\n-\n-\n-\n-\n-\n✓\n✓\n✓\n-\n-\n-\nnorth mesopotamian arabic\n(a:bas)\nnort3142\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\n✓\nnorth mesopotamian arabic\n(a:mos)\nnort3142\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\n✓\nrabat-casablanca arabic\nraba1252\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\n✓\nsfax\nsfax1238\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\n✓\nsouth levantine arabic\nsout3123\n-\n✓\n✓\n-\n-\n-\n✓\n✓\n✓\n-\n-\nsouth\nlevantine\narabic\n(a:south-amm)\nsout3123\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\n✓\nsouth\nlevantine\narabic\n(a:south-jer)\nsout3123\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\n✓\nsouth\nlevantine\narabic\n(a:south-sal)\nsout3123\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\n✓\nstandard arabic\nstan1318\n✓\n✓\n✓\n✓\n-\n✓\n✓\n✓\n✓\n✓\n-\nsunni beiruti arabic\nsunn1238\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\n✓\nta’izzi-adeni arabic\ntaiz1242\n-\n-\n-\n-\n-\n-\n✓\n✓\n-\n-\n-\ntripolitanian arabic\ntrip1239\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\n✓\ntunisian arabic\ntuni1259\n-\n-\n-\n-\n✓\n-\n✓\n✓\n✓\n-\n-\ntunisian arabic (a:tun)\ntuni1259\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\n✓\ntunisian arabic (r:casual)\ntuni1259\n-\n-\n-\n-\n-\n-\n-\n-\n✓\n-\n-\nbasque\nbasque (a:amenduze)\nbasq1248\n✓\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\nbasque (a:azkaine)\nbasq1248\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\nbasque (a:baigorri)\nbasq1248\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\nbasque (a:barkoxe)\nbasq1248\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\nbasque (a:donibane)\nbasq1248\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\nbasque (a:garruze)\nbasq1248\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\nbasque (a:iholdi)\nbasq1248\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\nbasque (a:jatsu)\nbasq1248\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\nbasque (a:jutsi)\nbasq1248\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\nbasque (a:larzabale)\nbasq1248\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\nbasque (a:luhuso)\nbasq1248\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\nbasque (a:sara)\nbasq1248\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\nbasque (a:senpere)\nbasq1248\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\nbasque (a:suhuskune)\nbasq1248\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\nbasque (a:uharte)\nbasq1248\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\nnavarro-labourdin\nbasque\n(a:behorlegi)\nbasq1249\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\nnavarro-labourdin\nbasque\n(a:bidarrai)\nbasq1249\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\nnavarro-labourdin\nbasque\n(a:helete)\nbasq1249\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\nnavarro-labourdin\nbasque\n(a:mugerre)\nbasq1249\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\nnavarro-labourdin\nbasque\n(a:urruna)\nbasq1249\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\nsouletin (a:maule)\nbasq1250\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\nbengali\nvanga (a:barisal)\nvang1242\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\nvanga (a:dhaka)\nvang1242\n✓\n-\n-\n-\n✓\n-\n-\n-\n-\n-\n✓\nvanga (a:jessore)\nvang1242\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\nvanga (a:khulna)\nvang1242\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\nvanga (a:kushtia)\nvang1242\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\nvanga (a:west bengal)\nvang1242\n✓\n-\n-\n-\n✓\n-\n-\n-\n-\n-\n-\ncircassian\nadyghe\nadyg1241\n-\n-\n-\n✓\n-\n-\n-\n-\n-\n-\n-\nkabardian\nkaba1278\n-\n-\n-\n✓\n-\n-\n-\n-\n-\n-\n-\ncommon turkic\nbashkir\nbash1264\n✓\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\ncentral oghuz\nazer1255\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\ncentral oghuz (m:spoken)\nazer1255\n-\n-\n-\n-\n-\n-\n✓\n✓\n-\n-\n-\ncrimean tatar\ncrim1257\n-\n-\n-\n✓\n-\n-\n-\n-\n-\n-\n-\nkara-kalpak\nkara1467\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\nkazakh\nkaza1248\n✓\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\nkirghiz\nkirg1245\n✓\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\nnorth azerbaijani\nnort2697\n✓\n-\n-\n✓\n-\n-\n✓\n✓\n-\n-\n-\nsakha\nyaku1245\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\nsouth azerbaijani\nsout2697\n✓\n-\n-\n✓\n-\n-\n✓\n✓\n-\n-\n-\nturkish\nnucl1301\n✓\n-\n-\n✓\n-\n-\n-\n-\n-\n-\n✓\nuzbek\nuzbe1247\n✓\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\neastern romance\naromanian\narom1237\n-\n-\n-\n✓\n-\n-\n-\n-\n-\n-\n-\nmoldavian\nmold1248\n-\n-\n-\n✓\n-\n-\n-\n-\n-\n-\n-\nContinued on next page\nTable 8: Varieties represented in DialectBench.\nLanguage clusters\nVariety name\nGlottocode\nmBERT seen\nUDP\nPOS\nNER\nSDQA\nRCMC\nNLI\nTC\nSC\nDI\nMT\nromanian\nroma1327\n✓\n-\n-\n✓\n-\n-\n-\n-\n-\n-\n-\neastern-western armenianeastern armenian\nnucl1235\n✓\n✓\n✓\n-\n-\n-\n-\n-\n-\n-\n-\nwestern armenian\nhoms1234\n-\n✓\n✓\n-\n-\n-\n-\n-\n-\n-\n-\nfarsic\ndari\ndari1249\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\nfrisian\nems-weser frisian\nsate1242\n-\n-\n-\n✓\n-\n-\n-\n-\n-\n-\n-\nnorthern frisian\nnort2626\n-\n-\n-\n✓\n-\n-\n-\n-\n-\n-\n-\nwestern frisian\nwest2354\n✓\n-\n-\n✓\n-\n-\n-\n-\n-\n-\n-\ngallo-italian\nemiliano-romagnolo\nemil1243\n-\n-\n-\n✓\n-\n-\n-\n-\n-\n-\n-\nligurian\nligu1248\n-\n✓\n✓\n✓\n-\n-\n✓\n✓\n-\n-\n-\nlombard\nlomb1257\n✓\n-\n-\n✓\n-\n-\n✓\n✓\n-\n-\n-\npiemontese\npiem1238\n✓\n-\n-\n✓\n-\n-\n-\n-\n-\n-\n-\nvenetian\nvene1258\n-\n-\n-\n✓\n-\n-\n✓\n✓\n-\n-\n-\ngallo-rhaetian\nanglo-norman\nangl1258\n-\n-\n-\n✓\n-\n-\n-\n-\n-\n-\n-\narpitan\nfran1260\n-\n-\n-\n✓\n-\n-\n-\n-\n-\n-\n-\nfrench\nstan1290\n✓\n✓\n✓\n✓\n-\n-\n-\n-\n-\n-\n-\nfrench (a:paris)\nstan1290\n-\n✓\n✓\n-\n-\n-\n-\n-\n-\n-\n-\nfriulian\nfriu1240\n-\n-\n-\n✓\n-\n-\n✓\n✓\n-\n-\n-\nold french (842-ca. 1400)\noldf1239\n-\n✓\n✓\n-\n-\n-\n-\n-\n-\n-\n-\nromansh\nroma1326\n-\n-\n-\n✓\n-\n-\n-\n-\n-\n-\n-\nwalloon\nwall1255\n-\n-\n-\n✓\n-\n-\n-\n-\n-\n-\n-\ngreater panjabic\neastern panjabi\npanj1256\n✓\n-\n-\n✓\n-\n-\n-\n-\n-\n-\n-\nwestern panjabi\nwest2386\n✓\n-\n-\n✓\n-\n-\n-\n-\n-\n-\n-\ngreek\napulian greek\napul1237\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\ncretan\ncret1244\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\ncypriot greek\ncypr1249\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\ncypriot\ngreek\n(r:casual,\nm:written, i:fb)\ncypr1249\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\n-\ncypriot\ngreek\n(r:casual,\nm:written, i:other)\ncypr1249\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\n-\ncypriot\ngreek\n(r:casual,\nm:written, i:twitter)\ncypr1249\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\n-\nmodern greek\nmode1248\n✓\n-\n-\n✓\n-\n-\n-\n-\n-\n-\n-\nmodern\ngreek\n(r:casual,\nm:written, i:fb)\nmode1248\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\n-\nmodern\ngreek\n(r:casual,\nm:written, i:other)\nmode1248\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\n-\nmodern\ngreek\n(r:casual,\nm:written, i:twitter)\nmode1248\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\n-\npontic\npont1253\n-\n-\n-\n✓\n-\n-\n-\n-\n-\n-\n-\nhigh german\nbavarian\nbava1246\n✓\n-\n-\n✓\n-\n-\n-\n-\n-\n-\n-\ncentral alemannic\nswis1247\n-\n-\n-\n✓\n-\n-\n-\n-\n-\n-\n-\ncentral alemannic (a:ag)\nswis1247\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\ncentral alemannic (a:ai)\nswis1247\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\ncentral alemannic (a:ar)\nswis1247\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\ncentral alemannic (a:be)\nswis1247\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\n✓\ncentral alemannic (a:bl)\nswis1247\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\ncentral alemannic (a:bs)\nswis1247\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\n✓\ncentral alemannic (a:fr)\nswis1247\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\ncentral alemannic (a:gl)\nswis1247\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\ncentral alemannic (a:gr)\nswis1247\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\ncentral alemannic (a:lu)\nswis1247\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\n✓\ncentral alemannic (a:nw)\nswis1247\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\ncentral alemannic (a:ow)\nswis1247\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\ncentral alemannic (a:sg)\nswis1247\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\ncentral alemannic (a:sh)\nswis1247\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\ncentral alemannic (a:so)\nswis1247\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\ncentral alemannic (a:sz)\nswis1247\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\ncentral alemannic (a:tg)\nswis1247\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\ncentral alemannic (a:ur)\nswis1247\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\ncentral alemannic (a:vs)\nswis1247\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\ncentral alemannic (a:zg)\nswis1247\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\ncentral alemannic (a:zh)\nswis1247\n-\n✓\n✓\n-\n-\n-\n-\n-\n-\n✓\n✓\ncentral bavarian\ncent1967\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\ngerman\nstan1295\n✓\n✓\n✓\n✓\n-\n-\n-\n-\n-\n-\n-\nkölsch\nkols1241\n-\n-\n-\n✓\n-\n-\n-\n-\n-\n-\n-\nlimburgan\nlimb1263\n-\n-\n-\n✓\n-\n-\n✓\n✓\n-\n-\n-\nluxemburgish\nluxe1243\n✓\n-\n-\n✓\n-\n-\n✓\n✓\n-\n-\n-\npennsylvania german\npenn1240\n-\n-\n-\n✓\n-\n-\n-\n-\n-\n-\n-\npfaelzisch-lothringisch\npala1330\n-\n-\n-\n✓\n-\n-\n-\n-\n-\n-\n-\nupper saxon\nuppe1465\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\nhindustani\nfiji hindi\nfiji1242\n-\n-\n-\n✓\n-\n-\n-\n-\n-\n-\n-\nhindi\nhind1269\n✓\n-\n-\n✓\n-\n-\n-\n-\n-\n-\n-\ninuit\nalaskan inupiaq\ninup1234\n-\n-\n-\n✓\n-\n-\n-\n-\n-\n-\n-\nkalaallisut\nkala1399\n-\n-\n-\n✓\n-\n-\n-\n-\n-\n-\n-\nitalian romance\ncontinental southern italian\nneap1235\n-\n✓\n✓\n✓\n-\n-\n-\n-\n-\n-\n-\nitalian\nital1282\n✓\n✓\n✓\n✓\n-\n-\n✓\n✓\n-\n-\n-\nitalian (a:abruzzo)\nital1282\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\nitalian (a:basilicata)\nital1282\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\nitalian (a:calabria)\nital1282\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\nitalian (a:campania)\nital1282\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\nitalian (a:emilia-romagna)\nital1282\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\nitalian (a:friuli-venezia giulia)\nital1282\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\nitalian (a:lazio)\nital1282\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\nitalian (a:liguria)\nital1282\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\nitalian (a:lombardia)\nital1282\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\nContinued on next page\nTable 8: Varieties represented in DialectBench.\nLanguage clusters\nVariety name\nGlottocode\nmBERT seen\nUDP\nPOS\nNER\nSDQA\nRCMC\nNLI\nTC\nSC\nDI\nMT\nitalian (a:marche)\nital1282\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\nitalian (a:molise)\nital1282\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\nitalian (a:piemonte)\nital1282\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\nitalian (a:puglia)\nital1282\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\nitalian (a:sardegna)\nital1282\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\nitalian (a:sicilia)\nital1282\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\nitalian (a:toscana)\nital1282\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\nitalian\n(a:trentino-alto\nadi-\nge/südtirol)\nital1282\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\nitalian (a:umbria)\nital1282\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\nitalian (a:unknown)\nital1282\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\nitalian (a:veneto)\nital1282\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\nitalian (r:casual, m:written,\ni:tweet)\nital1282\n✓\n✓\n✓\n-\n-\n-\n-\n-\n-\n-\n-\nitalian (r:formal, m:written,\ni:essay)\nital1282\n✓\n✓\n✓\n-\n-\n-\n-\n-\n-\n-\n-\nitalian\nromance\n(a:apulia,\nm:spoken, i:tarantino)\n-\n-\n-\n-\n✓\n-\n-\n-\n-\n-\n-\n-\nsicilian\nsici1248\n✓\n-\n-\n✓\n-\n-\n✓\n✓\n-\n-\n-\nkomi\nkomi\nkomi1267\n-\n-\n-\n✓\n-\n-\n-\n-\n-\n-\n-\nkomi-permyak\nkomi1269\n-\n✓\n✓\n✓\n-\n-\n-\n-\n-\n-\n-\nkomi-zyrian (m:spoken)\nkomi1268\n-\n✓\n✓\n-\n-\n-\n-\n-\n-\n-\n-\nkomi-zyrian (m:written)\nkomi1268\n-\n✓\n✓\n-\n-\n-\n-\n-\n-\n-\n-\nkorean\nkorean\n(a:south-eastern,\nm:spoken)\nkore1280\n-\n-\n-\n-\n✓\n-\n-\n-\n-\n-\n-\nseoul (m:spoken)\nseou1239\n✓\n-\n-\n-\n✓\n-\n-\n-\n-\n-\n-\nkurdish\ncentral kurdish\ncent1972\n-\n-\n-\n✓\n-\n-\n✓\n✓\n-\n-\n-\nkurdish\nkurd1259\n-\n-\n-\n✓\n-\n-\n-\n-\n-\n-\n-\nnorthern kurdish\nnort2641\n-\n-\n-\n-\n-\n-\n✓\n✓\n-\n-\n-\nsine’i\nsine1239\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\nsorani\nsora1257\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\nlatvian\neast latvian\neast2282\n-\n-\n-\n✓\n-\n-\n✓\n✓\n-\n-\n-\nlatvian\nlatv1249\n✓\n-\n-\n✓\n-\n-\n✓\n✓\n-\n-\n-\nmari\neastern mari\neast2328\n-\n-\n-\n✓\n-\n-\n-\n-\n-\n-\n-\nwestern mari\nwest2392\n-\n-\n-\n✓\n-\n-\n-\n-\n-\n-\n-\nmodern dutch\ndutch\ndutc1256\n✓\n-\n-\n✓\n-\n-\n✓\n✓\n-\n-\n-\nwestern flemish\nvlaa1240\n-\n-\n-\n✓\n-\n-\n-\n-\n-\n-\n-\nzeeuws\nzeeu1238\n-\n-\n-\n✓\n-\n-\n-\n-\n-\n-\n-\nneva\ncentral and north pohjanmaa\n(a:ostrobothnian)\ncent1985\n-\n-\n✓\n-\n-\n-\n-\n-\n-\n-\n-\nestonian\nesto1258\n✓\n-\n✓\n-\n-\n-\n-\n-\n-\n-\n-\nfinnish\nfinn1318\n✓\n-\n✓\n-\n-\n-\n-\n-\n-\n-\n-\nhäme (a:tavastian)\nhame1240\n-\n-\n✓\n-\n-\n-\n-\n-\n-\n-\n-\nneva (a:south-west trans)\n-\n-\n-\n✓\n-\n-\n-\n-\n-\n-\n-\n-\nsavo (a:savonian)\nsavo1254\n-\n-\n✓\n-\n-\n-\n-\n-\n-\n-\n-\nsoutheastern finnish (a:south-\neast)\nsout1743\n-\n-\n✓\n-\n-\n-\n-\n-\n-\n-\n-\nsouthwestern finnish (a:south-\nwest)\nsout2677\n-\n-\n✓\n-\n-\n-\n-\n-\n-\n-\n-\nnorwegian\nnorwegian\nnorw1258\n✓\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nnorwegian (a:eastern)\nnorw1258\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\nnorwegian (a:setesdal)\nnorw1258\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\nnorwegian (a:southwestern)\nnorw1258\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\nnorwegian\n(m:written,\ni:samnorsk)\n-\n-\n-\n-\n✓\n-\n-\n-\n-\n-\n-\n-\nnorwegian\nbokmål\n(m:written)\nnorw1259\n✓\n✓\n✓\n✓\n-\n-\n✓\n✓\n-\n-\n-\nnorwegian\nnynorsk\n(m:written)\nnorw1262\n✓\n✓\n✓\n✓\n-\n-\n✓\n✓\n-\n-\n-\nnorwegian\nnynorsk\n(m:written, i:old)\nnorw1262\n-\n✓\n✓\n-\n-\n-\n-\n-\n-\n-\n-\nsaami\nnorth saami\nnort2671\n-\n✓\n✓\n-\n-\n-\n-\n-\n-\n-\n-\nskolt saami\nskol1241\n-\n✓\n✓\n-\n-\n-\n-\n-\n-\n-\n-\nsabellic\numbrian\numbr1253\n-\n✓\n✓\n-\n-\n-\n-\n-\n-\n-\n-\nsardo-corsican\ncorsican\ncors1241\n-\n-\n-\n✓\n-\n-\n-\n-\n-\n-\n-\nsardinian\nsard1257\n-\n-\n-\n✓\n-\n-\n✓\n✓\n-\n-\n-\nserbian-croatian-bosnian\nbosnian standard\nbosn1245\n✓\n-\n-\n✓\n-\n-\n-\n-\n-\n-\n-\ncroatian standard\ncroa1245\n✓\n-\n-\n✓\n-\n-\n-\n-\n-\n-\n-\nserbian standard\nserb1264\n✓\n-\n-\n✓\n-\n-\n-\n-\n-\n-\n-\nserbian-croatian-bosnian\nsout1528\n✓\n-\n-\n✓\n-\n-\n-\n-\n-\n-\n-\nsinitic\ncantonese\ncant1236\n-\n-\n-\n✓\n-\n-\n✓\n✓\n-\n-\n-\nclassical chinese\nlite1248\n-\n✓\n✓\n✓\n-\n-\n-\n-\n-\n-\n-\nclassical-middle-modern\nsinitic\n(a:hongkong,\no:traditional)\nclas1255\n-\n✓\n✓\n-\n-\n-\n-\n-\n-\n-\n-\nclassical-middle-modern\nsinitic (o:simplified)\nclas1255\n✓\n✓\n✓\n-\n-\n✓\n✓\n✓\n-\n-\n-\nclassical-middle-modern\nsinitic (o:traditional)\nclas1255\n✓\n-\n-\n-\n-\n✓\n✓\n✓\n-\n-\n-\nhakka chinese\nhakk1236\n-\n-\n-\n✓\n-\n-\n-\n-\n-\n-\n-\nmandarin chinese\nmand1415\n-\n-\n-\n✓\n-\n-\n-\n-\n-\n-\n-\nmandarin chinese (a:mainland,\no:simplified)\nmand1415\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\n-\nContinued on next page\nTable 8: Varieties represented in DialectBench.\nLanguage clusters\nVariety name\nGlottocode\nmBERT seen\nUDP\nPOS\nNER\nSDQA\nRCMC\nNLI\nTC\nSC\nDI\nMT\nmandarin chinese (a:mainland,\no:traditional, i:synthetic)\nmand1415\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\n-\nmandarin chinese (a:taiwan,\no:simplified)\nmand1415\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\n-\nmandarin chinese (a:taiwan,\no:traditional, i:synthetic)\nmand1415\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\n-\nmin nan chinese\nminn1241\n-\n-\n-\n✓\n-\n-\n-\n-\n-\n-\n-\nwu chinese\nwuch1236\n-\n-\n-\n✓\n-\n-\n-\n-\n-\n-\n-\nsorbian\nlower sorbian\nlowe1385\n-\n-\n-\n✓\n-\n-\n-\n-\n-\n-\n-\nupper sorbian\nuppe1395\n-\n-\n-\n✓\n-\n-\n-\n-\n-\n-\n-\nsotho-tswana (s.30)\nnorthern sotho\nnort3233\n-\n-\n-\n✓\n-\n✓\n✓\n✓\n-\n-\n-\nsouthern sotho\nsout2807\n-\n-\n-\n✓\n-\n✓\n✓\n✓\n-\n-\n-\nsouthwestern shifted\nbrazilian portuguese\nbraz1246\n-\n✓\n✓\n-\n-\n-\n-\n-\n-\n✓\n-\nromance\nextremaduran\nextr1243\n-\n-\n-\n✓\n-\n-\n-\n-\n-\n-\n-\ngalician\ngali1258\n✓\n-\n-\n✓\n-\n-\n✓\n✓\n-\n-\n-\nlatin american spanish\namer1254\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\n-\nmirandese\nmira1251\n-\n-\n-\n✓\n-\n-\n-\n-\n-\n-\n-\noccitan\nocci1239\n✓\n-\n✓\n✓\n-\n-\n✓\n✓\n-\n-\n✓\nportuguese (a:european)\nport1283\n-\n✓\n✓\n-\n-\n-\n✓\n✓\n-\n✓\n-\nportuguese (i:mix)\nport1283\n-\n✓\n✓\n-\n-\n-\n-\n-\n-\n-\n-\nportuguese (m:written)\nport1283\n✓\n✓\n✓\n-\n-\n-\n-\n-\n-\n✓\n-\nspanish\nstan1288\n✓\n-\n-\n✓\n-\n-\n✓\n✓\n-\n✓\n-\nspanish (a:europe)\nstan1288\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\n-\nswahili\nswahili\nswah1253\n✓\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nswahili (a:kenya)\nswah1253\n-\n-\n-\n-\n✓\n-\n-\n-\n-\n-\n-\nswahili (a:tanzania)\nswah1253\n-\n-\n-\n-\n✓\n-\n-\n-\n-\n-\n-\ntigrinya\ntigrinya (a:eritrea)\ntigr1271\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\ntigrinya (a:ethiopia)\ntigr1271\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\ntupi-guarani subgroup i.a\nmbyá guaraní (a:brazil)\nmbya1239\n-\n✓\n✓\n-\n-\n-\n-\n-\n-\n-\n-\nmbyá guaraní (a:paraguay)\nmbya1239\n-\n✓\n✓\n-\n-\n-\n-\n-\n-\n-\n-\nold guarani\noldp1258\n-\n✓\n✓\n-\n-\n-\n-\n-\n-\n-\n-\nwest low german\nwest low german\nwest2357\n✓\n✓\n✓\n✓\n-\n-\n-\n-\n-\n-\n-\nyoruba\nyoruba (a:central nigeria)\nyoru1245\n✓\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\nC.2\nLanguage Clusters and Representative Varieties\nTable 9: Language clusters and their standard varieties.\nTask\nCluster Name\nCluster Representative\nDEP. Parsing\nalbanian\nalbanian\narabic\nstandard arabic\neastern-western armenian\nwestern armenian\nsinitic\nclassical-middle-modern sinitic (o:simplified)\nanglic\nenglish\ngallo-rhaetian\nfrench\nhigh german\ngerman\ntupi-guarani subgroup i.a\nold guarani\nitalian romance\nitalian (r:formal, m:written, i:essay)\nkomi\nkomi-zyrian (m:spoken)\nnorwegian\nnorwegian bokmål (m:written)\nsouthwestern shifted romance\nbrazilian portuguese\nsaami\nnorth saami\nPOS Tagging\nalbanian\nalbanian\nanglic\nenglish\narabic\nstandard arabic\neastern-western armenian\nwestern armenian\ngallo-rhaetian\nfrench\nhigh german\ngerman\nitalian romance\nitalian (r:formal, m:written, i:essay)\nkomi\nkomi-zyrian (m:spoken)\nneva\nfinnish\nnorwegian\nnorwegian bokmål (m:written)\nsaami\nnorth saami\nsinitic\nclassical-middle-modern sinitic (o:simplified)\nsouthwestern shifted romance\nbrazilian portuguese\ntupi-guarani subgroup i.a\nmbyá guaraní (a:paraguay)\nNER\nanglic\nenglish\narabic\nstandard arabic\ncircassian\nadyghe\ncommon turkic\nturkish\neastern romance\nromanian\nfrisian\nwestern frisian\ngallo-italian\npiemontese\ngallo-rhaetian\nfrench\ngreater panjabic\nwestern panjabi\ngreek\nmodern greek\nhigh german\ngerman\nhindustani\nhindi\ninuit\nalaskan inupiaq\nitalian romance\nitalian\nkomi\nkomi\nkurdish\nkurdish\nContinued on next page\nTable 9: Language clusters and their clusters representatives.\nTask\nCluster Name\nCluster Representative\nlatvian\nlatvian\nmari\neastern mari\nmodern dutch\ndutch\nnorwegian\nnorwegian bokmål (m:written)\nsardo-corsican\nsardinian\nserbian-croatian-bosnian\ncroatian standard\nsinitic\nmandarin chinese\nsorbian\nlower sorbian\nsotho-tswana (s.30)\nsouthern sotho\nsouthwestern shifted romance\nspanish\nEQA\nanglic\nsoutheast american english\narabic\narabic (a:saudi-arabia)\nbengali\nvanga (a:dhaka)\nkorean\nseoul (m:spoken)\nswahili\nswahili (a:kenya)\nMRC\nanglic\nenglish\narabic\nstandard arabic\nsinitic\nclassical-middle-modern sinitic (o:simplified)\nsotho-tswana (s.30)\nnorthern sotho\nTC\nanglic\nenglish\narabic\nstandard arabic\ncommon turkic\nnorth azerbaijani\ngallo-italian\nvenetian\nhigh german\nluxemburgish\nitalian romance\nitalian\nkurdish\nnorthern kurdish\nlatvian\nlatvian\nnorwegian\nnorwegian bokmål (m:written)\nsinitic\nclassical-middle-modern sinitic (o:simplified)\nsotho-tswana (s.30)\nnorthern sotho\nsouthwestern shifted romance\nspanish\nSC\narabic\nstandard arabic\nMT-Dialect\narabic\ngulf arabic (a:riy)\nbasque\nbasque (a:azkaine)\nbengali\nvanga (a:dhaka)\nhigh german\ncentral bavarian\nkurdish\nsorani\nnorwegian\nnorwegian (a:eastern)\ntigrinya\ntigrinya (a:ethiopia)\ncommon turkic\nturkish\ngreek\ncretan\nMT-Region\nitalian romance\nitalian (a:umbria)\nhigh german\ncentral alemannic (a:zh)\nDId\nanglic\nenglish (a:uk)\narabic\nstandard arabic\ngreek\nmodern greek (r:casual, m:written, i:twitter)\nhigh german\ncentral alemannic (a:zh)\nsinitic\nmandarin chinese (a:mainland, o:simplified)\nsouthwestern shifted romance\nbrazilian portuguese\nNLI\nanglic\nenglish\narabic\nstandard arabic\ncommon turkic\nnorth azerbaijani\ngallo-italian\nvenetian\nhigh german\nluxemburgish\nitalian romance\nitalian\nkurdish\nnorthern kurdish\nlatvian\nlatvian\nnorwegian\nnorwegian bokmål (m:written)\nsinitic\nclassical-middle-modern sinitic (o:simplified)\nsotho-tswana (s.30)\nnorthern sotho\nsouthwestern shifted romance\nspanish\nD\nResult Visualizations\nD.1\nRegional maps with aggregated Machine Translation scores\n43.62\n42.27\n43.90\n42.51\n43.11\n42.95\n42.14\n43.90\n43.67\n42.86\n43.04\n42.30\n41.11\n43.62\n40.27\n43.24\n43.94\n44.08\n41.69\n43.05\n43.71\nFigure 5: Map of Switzerland with aggregated BLEU scores of Swiss-German variety per region\n13.64\n13.76\n20.11\n17.03\n13.86\n18.62\n39.03\n17.98\n16.23\n26.29\n17.73\n14.48\n16.50\n11.67\n22.91\n28.90\n23.40\n40.73\n29.38\nFigure 6: Map of Italy with aggregated BLEU scores of Italian variety per region\nD.2\nTask Specific Plot for Maximum Scores\nD.3\nDialectal Gap visualizations (zeroshot)\nE\nEvaluation results\ncluster\nvariety\nUD-code\nZeroshot\n(mBERT)\nZeroshot\n(XLM-R)\nFinetune\n(mBERT)\nFinetune\n(XLM-R)\nalbanian\nalbanian\nUD_Albanian-TSA\n81.78\n83.08\n-\n-\ngheg albanian\nUD_Gheg-GPS\n38.14\n43.50\n-\n-\nanglic\nenglish\nUD_English-EWT\n91.55\n91.10\n91.55\n91.1\nsinglish\nsinglish\n69.88\n68.55\n82.36\n11.0\nafrican american vernacular en-\nglish\nTwitterAAE\n50.53\n52.53\n-\n-\narabic\nstandard arabic\nUD_Arabic-PADT\n50.87\n55.91\n88.47\n2.48\nsouth levantine arabic\nUD_South_Levantine_Arabic-\nMADAR\n49.94\n45.75\n-\n-\nnorth african arabic\nUD_Maghrebi_Arabic_French-\nArabizi\n34.33\n28.04\n5.46\n54.7\neastern-western armenian\nwestern armenian\nUD_Western_Armenian-\nArmTDP\n54.63\n59.71\n88.84\n89.29\neastern armenian\nUD_Armenian-ArmTDP\n53.27\n62.63\n85.55\n87.01\ngallo-italian\nligurian\nUD_Ligurian-GLT\n50.22\n43.78\n13.06\n9.1\ngallo-rhaetian\nfrench\nUD_French-ParTUT\n80.87\n82.17\n93.93\n92.09\nfrench (a:paris)\nUD_French-ParisStories\n57.64\n61.36\n77.54\n77.54\nold french (842-ca. 1400)\nUD_Old_French-SRCMF\n56.12\n46.52\n91.51\n89.78\nhigh german\ngerman\nUD_German-LIT\n72.65\n76.50\n-\n-\ncentral alemannic (a:zh)\nUD_Swiss_German-UZH\n36.77\n34.70\n-\n-\nitalian romance\nitalian\nUD_Italian-PUD\n78.99\n78.58\n-\n-\nitalian (r:formal,\nm:written,\ni:essay)\nUD_Italian-MarkIT\n76.83\n79.38\n86.34\n83.81\nitalian\n(r:casual,\nm:written,\ni:tweet)\nUD_Italian-PoSTWITA\n61.88\n62.13\n85.82\n86.32\ncontinental southern italian\nUD_Neapolitan-RB\n30.00\n50.00\n-\n-\nkomi\nkomi-zyrian (m:spoken)\nUD_Komi_Zyrian-IKDP\n26.89\n32.14\n-\n-\nkomi-permyak\nUD_Komi_Permyak-UH\n26.12\n30.91\n-\n-\nkomi-zyrian (m:written)\nUD_Komi_Zyrian-Lattice\n21.01\n27.55\n-\n-\nnorwegian\nnorwegian bokmål (m:written)\nUD_Norwegian-Bokmaal\n79.55\n82.95\n93.57\n93.49\nnorwegian nynorsk (m:written)\nUD_Norwegian-Nynorsk\n76.45\n76.76\n93.15\n93.14\nnorwegian nynorsk (m:written,\ni:old)\nUD_Norwegian-NynorskLIA\n56.58\n56.08\n78.39\n8.25\nsaami\nnorth saami\nUD_North_Sami-Giella\n23.58\n16.87\n67.56\n5.96\nskolt saami\nUD_Skolt_Sami-Giellagas\n19.41\n28.28\n-\n-\nsabellic\numbrian\nUD_Umbrian-IKUVINA\n33.21\n28.75\n-\n-\nsinitic\nclassical-middle-modern\nsinitic\n(a:hongkong,\no:traditional)\nUD_Chinese-HK\n58.97\n61.78\n-\n-\nclassical-middle-modern\nsinitic (o:simplified)\nUD_Chinese-GSDSimp\n53.35\n52.21\n87.85\n87.37\nclassical chinese\nUD_Classical_Chinese-Kyoto\n46.72\n35.14\n19.65\n18.11\nsouthwestern shifted romance\nportuguese (a:european)\nUD_Portuguese-PUD\n76.67\n77.28\n-\n-\nportuguese (i:mix)\nUD_Portuguese-Bosque\n75.89\n77.96\n92.64\n92.15\nbrazilian portuguese\nUD_Portuguese-GSD\n73.30\n74.21\n94.37\n93.75\nportuguese (m:written)\nUD_Portuguese-CINTIL\n69.34\n72.94\n83.35\n84.31\ntupi-guarani subgroup i.a\nold guarani\nUD_Guarani-OldTuDeT\n22.58\n29.03\n-\n-\nmbyá guaraní (a:paraguay)\nUD_Mbya_Guarani-Thomas\n13.51\n11.15\n-\n-\nmbyá guaraní (a:brazil)\nUD_Mbya_Guarani-Dooley\n8.95\n4.23\n-\n-\nwest low german\nwest low german\nUD_Low_Saxon-LSDC\n40.75\n37.38\n-\n-\nTable 10: Dependency parsing evaluation report comprising zeroshot score and in-language finetuning. We report\nUAS as evaluation score. Zeroshot scores are evaluated using model finetuned on Standerd English. If training data\nis not available, we skip those languages (mentioned as ’-’) for in-language finetuning.\numbrian\nm. guaraní (a:brazil)\nold guarani\nm. guaraní (a:paraguay)\nkomi-zyrian (m:writ.)\nkomi-permyak\nkomi-zyrian (m:spoken)\nligurian\nwest low german\nskolt saami\nnorth saami\ngheg albanian\nalbanian\ncentral alemannic (a:zh)\ngerman\ncmm sinitic (a:HK, o:trad.)\nclassical chinese\ncmm sinitic (o:simp.)\neastern armenian\nwestern armenian\nlevantine arabic\ngulf arabic\negyptian arabic\nsouth levantine arabic\nnorth african arabic\nstandard arabic\ncont. south. ita.\nita.\nita. (r:for., m:writ., i:essay)\nita. (r:ca., m:writ., i:tweet)\nold french (842-ca. 1400)\nfrench (a:paris)\nfrench\nsinglish\nenglish\nneva (a:south-west trans)\nsouthwestern finnish (a:south-west)\nsoutheastern finnish (a:south-east)\nsavo (a:savonian)\ncnp. (a:ostrobothnian)\nhäme (a:tavastian)\nestonian\nfinnish\noccitan\nportuguese (a:european)\nportuguese (i:mix)\nportuguese (m:writ.)\nbrazilian portuguese\nnor. nynorsk (m:writ., i:old)\nnor. nynorsk (m:writ.)\nnor. bokmål (m:writ.)\nVariety\n0\n20\n40\n60\n80\n100\nF1\nsabellic\ntupi-guarani\nsubgroup\ni.a\nkomi\ngallo-italian\nwest low german\nsaami\nalbanian\nhigh german\nsinitic\neastern-western\narmenian\narabic\nitalian romance\ngallo-rhaetian\nanglic\nneva\nsouthwestern\nshifted\nromance\nnorwegian\n(a) Parts of Speech Tagging\nkomi-permyak\nkomi\neastern mari\nwestern mari\ncentral kurdish\nkurdish\neastern panjabi\nwestern panjabi\nkabardian\nadyghe\nnorthern sotho\nsouth. sotho\nupper sorbian\nlower sorbian\nkalaallisut\nalaskan inupiaq\nwest low german\nnorthern frisian\nems-weser frisian\nwestern frisian\nhakka chinese\nmin nan chinese\nclassical chinese\nwu chinese\ncantonese\nm. chinese\ncorsican\nsardinian\nemiliano-romagnolo\nligurian\nvenetian\nlombard\npiemontese\nfiji hindi\nhindi\njamaican creole english\nold english (ca. 450-1100)\nenglish\nenglish (o:controlled)\negyptian arabic\nstandard arabic\npennsylvania german\npfaelzisch-lothringisch\nkölsch\nbavarian\nlimburgan\nluxemburgish\ncentral alemannic\ngerman\nnor. nynorsk (m:writ.)\nnor. bokmål (m:writ.)\nnor. (m:writ., i:samnorsk)\nwalloon\nfriulian\nromansh\narpitan\nanglo-norman\nfrench\npontic\nmodern greek\nzeeuws\nwestern flemish\ndutch\nmirandese\nextremaduran\noccitan\ngalician\nspanish\ncont. south. ita.\nir. (a:apulia, m:spoken, i:tarantino)\nsicilian\nita.\nserbian standard\nserbian-croatian-bosnian\nbosnian standard\ncroatian standard\nsouth azerbaijani\ncrimean tatar\nnorth azerbaijani\nturkish\neast latvian\nlatvian\nmoldavian\naromanian\nromanian\nVariety\n0\n20\n40\n60\n80\n100\nF1\nkomi\nmari\nkurdish\ngreater\npanjabic\ncircassian\nsotho-tswana\n(s.30)\nsorbian\ninuit\nwest low german\nfrisian\nsinitic\nsardo-corsican\ngallo-italian\nhindustani\nanglic\narabic\nhigh german\nnorwegian\ngallo-rhaetian\ngreek\nmodern dutch\nsouthwestern\nshifted\nromance\nitalian romance\nserbian-croatian-bosnian\ncommon turkic\nlatvian\neastern romance\n(b) Named Entity Recognition\nnorthern sotho\nsouth. sotho\ncentral kurdish\nnorthern kurdish\nfriulian\nsardinian\nlombard\nligurian\nvenetian\nlimburgan\nluxemburgish\neast latvian\nlatvian\nsouth azerbaijani\nnorth azerbaijani\ncentral oghuz (m:spoken)\nmoroccan arabic\nlevantine arabic (a:north)\ntunisian arabic\nsouth levantine arabic\negyptian arabic\nnorth mesopotamian arabic\nta'izzi-adeni arabic\nstandard arabic\nnajdi arabic\nsicilian\nita.\noccitan\nspanish\ngalician\nportuguese (a:european)\nnor. bokmål (m:writ.)\nnor. nynorsk (m:writ.)\ndutch\nenglish\ncmm sinitic (o:simp.)\ncantonese\ncmm sinitic (o:trad.)\nVariety\n0\n20\n40\n60\n80\n100\nF1\nsotho-tswana\n(s.30)\nkurdish\ngallo-rhaetian\nsardo-corsican\ngallo-italian\nhigh german\nlatvian\ncommon turkic\narabic\nitalian romance\nsouthwestern\nshifted\nromance\nnorwegian\nmodern dutch\nanglic\nsinitic\n(c) Topic Classification\nFigure 7: Task specific plot of maximum obtainable score for all varieties. The yellow-shaded region represents\nlanguage clusters having no varieties seen during mBERT pertaining. The bars with colored stripes represent the\nstandard variety of a cluster\nsouth. sotho\nnorthern sotho\nfriulian\nsardinian\nlimburgan\nluxemburgish\ncentral kurdish\nnorthern kurdish\nligurian\nlombard\nvenetian\ncmm sinitic (o:trad.)\ncantonese\ncmm sinitic (o:simp.)\neast latvian\nlatvian\ntunisian arabic\nmoroccan arabic\nnorth mesopotamian arabic\nlevantine arabic (a:north)\nsouth levantine arabic\negyptian arabic\nta'izzi-adeni arabic\nnajdi arabic\nstandard arabic\nsouth azerbaijani\nnorth azerbaijani\ncentral oghuz (m:spoken)\ndutch\nsicilian\nita.\noccitan\ngalician\nspanish\nportuguese (a:european)\nnor. nynorsk (m:writ.)\nnor. bokmål (m:writ.)\nenglish\nVariety\n0\n20\n40\n60\n80\n100\nF1\nsotho-tswana\n(s.30)\ngallo-rhaetian\nsardo-corsican\nhigh german\nkurdish\ngallo-italian\nsinitic\nlatvian\narabic\ncommon turkic\nmodern dutch\nitalian romance\nsouthwestern\nshifted\nromance\nnorwegian\nanglic\n(a) Natural Language Inference\nkorean (a:south-eastern, m:spoken)\nseoul (m:spoken)\nswahili (a:tanzania)\nswahili (a:kenya)\nvanga (a:west bengal)\nvanga (a:dhaka)\nindian english (a:south)\nkenyan english\nnigerian english\nindian english (a:north)\nirish english\naustralian english\nphilippine english\nenglish (a:scotland)\nsouth. african english\nnew zealand english\nsoutheast american english\negyptian arabic\ntunisian arabic\nmoroccan arabic\narabic (a:jordan)\narabic (a:bahrain)\nalgerian arabic\narabic (a:saudi-arabia)\nVariety\n0\n20\n40\n60\n80\n100\nSpan F1\nkorean\nswahili\nbengali\nanglic\narabic\n(b) Extractive Question Answering\nsouth. sotho\nnorthern sotho\nmoroccan arabic\negyptian arabic\nnajdi arabic\nlevantine arabic (a:north)\nnorth mesopotamian arabic\nstandard arabic\ncmm sinitic (o:trad.)\ncmm sinitic (o:simp.)\nenglish\nVariety\n0\n20\n40\n60\n80\n100\nF1\nsotho-tswana\n(s.30)\narabic\nsinitic\nanglic\n(c) Multiple Choice Machine Reading Comprehension\nFigure 8: Task specific plot of maximum obtainable Linguistic Utility for all varieties. The yellow-shaded region\nrepresents language clusters having no varieties seen during mBERT pertaining. The bars with colored stripes\nrepresent the standard variety of a cluster. The dialect with the Rawlsian score in each cluster is that with the\nleftmost bar.\nsouth levantine arabic\negyptian arabic\nmoroccan arabic\ntunisian arabic (r:ca.)\narabic (a:saudi-arabia)\nstandard arabic\nalgerian arabic\narabic (a:jordan)\ntunisian arabic\nVariety\n0\n20\n40\n60\n80\n100\nF1\narabic\n(a) Sentiment Analysis\ncypriot greek (r:ca., m:writ., i:twitter)\ncypriot greek (r:ca., m:writ., i:other)\nmodern greek (r:ca., m:writ., i:twitter)\ncentral alemannic (a:be)\ncentral alemannic (a:lu)\ncentral alemannic (a:bs)\ncentral alemannic (a:zh)\nnorth american english\nenglish (a:uk)\nportuguese (m:writ.)\nspanish\nportuguese (a:european)\nlatin american spanish\nspanish (a:europe)\nbrazilian portuguese\nsouth levantine arabic (a:south-amm)\nlevantine arabic (a:north-dam)\ngulf arabic (a:mus)\nsouth levantine arabic (a:south-jer)\negyptian arabic (a:cai)\ngulf arabic (a:riy)\ngulf arabic (a:doh)\nnorth mesopotamian arabic (a:bas)\nlibyan arabic (a:ben)\negyptian arabic (a:asw)\nrabat-casablanca arabic\ngilit mesopotamian arabic\negyptian arabic (a:kha)\ntunisian arabic (a:tun)\ngulf arabic (a:jed)\nsunni beiruti arabic\naleppo\nsfax\nfez. meknes\nsouth levantine arabic (a:south-sal)\narabian peninsula arabic (a:yemen)\ntripolitanian arabic\nalgerian arabic\negyptian arabic (a:alx)\nnorth mesopotamian arabic (a:mos)\nstandard arabic\nm. chinese (a:mainland, o:trad., i:synthetic)\nm. chinese (a:taiwan, o:trad., i:synthetic)\nm. chinese (a:mainland, o:simp.)\nm. chinese (a:taiwan, o:simp.)\nVariety\n0\n20\n40\n60\n80\n100\nF1\ngreek\nhigh german\nanglic\nsouthwestern\nshifted\nromance\narabic\nsinitic\n(b) Dialect Identification\nFigure 9: Task specific plot of maximum obtainable Linguistic Utility for all varieties. The yellow-shaded region\nrepresents language clusters having no varieties seen during mBERT pertaining. The bars with colored stripes\nrepresent the standard variety of a cluster. The dialect with the Rawlsian score in each cluster is that with the\nleftmost bar.\nita. (a:sardegna)\nita. (a:abruzzo)\nita. (a:emilia-romagna)\nita. (a:basilicata)\nita. (a:piemonte)\nita. (a:puglia)\nita. (a:campania)\nita. (a:lombardia)\nita. (a:molise)\nita. (a:liguria)\nita. (a:calabria)\nita. (a:sicilia)\nita. (a:friuli-venezia giulia)\nita. (a:unknown)\nita. (a:trentino-alto adige/südtirol)\nita. (a:marche)\nita. (a:toscana)\nita. (a:veneto)\nita. (a:lazio)\nita. (a:umbria)\ncentral alemannic (a:so)\ncentral alemannic (a:sg)\ncentral alemannic (a:fr)\ncentral alemannic (a:ow)\ncentral alemannic (a:be)\ncentral alemannic (a:vs)\ncentral alemannic (a:ai)\ncentral alemannic (a:lu)\ncentral alemannic (a:nw)\ncentral alemannic (a:zg)\ncentral alemannic (a:bl)\ncentral alemannic (a:sz)\ncentral alemannic (a:bs)\ncentral alemannic (a:sh)\ncentral alemannic (a:ag)\ncentral alemannic (a:gr)\ncentral alemannic (a:ar)\ncentral alemannic (a:tg)\ncentral alemannic (a:gl)\ncentral alemannic (a:zh)\ncentral alemannic (a:ur)\nVariety\n0\n20\n40\n60\n80\n100\nBLEU\nitalian romance\nhigh german\n(a) Machine Translation (Region level aggregation)\nupper saxon\ncentral bavarian\nvanga (a:barisal)\nvanga (a:dhaka)\nvanga (a:kushtia)\nvanga (a:khulna)\nvanga (a:jessore)\nyoruba (a:central nigeria)\napulian greek\ncretan\ntigrinya (a:eritrea)\ntigrinya (a:ethiopia)\nbasque (a:barkoxe)\nsouletin (a:maule)\nbasque (a:iholdi)\nbasque (a:jatsu)\nnavarro-labourdin basque (a:bidarrai)\nnavarro-labourdin basque (a:urruna)\nbasque (a:baigorri)\nbasque (a:donibane)\nbasque (a:larzabale)\nbasque (a:uharte)\nbasque (a:suhuskune)\nbasque (a:jutsi)\nnavarro-labourdin basque (a:mugerre)\nnavarro-labourdin basque (a:helete)\nbasque (a:luhuso)\nbasque (a:sara)\nbasque (a:senpere)\nnavarro-labourdin basque (a:behorlegi)\nbasque (a:amenduze)\nbasque (a:garruze)\nbasque (a:azkaine)\noccitan\nnor. (a:setesdal)\nnor. (a:southwestern)\nnor. (a:eastern)\nsakha\nkara-kalpak\nkazakh\nkirghiz\ncentral oghuz\nbashkir\nturkish\nuzbek\nsine'i\nsorani\ndari\nsfax\ntunisian arabic (a:tun)\nalgerian arabic\nsunni beiruti arabic\nrabat-casablanca arabic\ntripolitanian arabic\naleppo\nlibyan arabic (a:ben)\nsouth levantine arabic (a:south-jer)\nlevantine arabic (a:north-dam)\ngulf arabic (a:jed)\ngulf arabic (a:doh)\nnorth mesopotamian arabic (a:bas)\nsouth levantine arabic (a:south-amm)\negyptian arabic (a:asw)\ngilit mesopotamian arabic\narabian peninsula arabic (a:yemen)\nnorth mesopotamian arabic (a:mos)\nfez. meknes\nsouth levantine arabic (a:south-sal)\negyptian arabic (a:kha)\negyptian arabic (a:cai)\negyptian arabic (a:alx)\ngulf arabic (a:mus)\ngulf arabic (a:riy)\nVariety\n0\n20\n40\n60\n80\n100\nBLEU\nhigh german\nbengali\nyoruba\ngreek\ntigrinya\nbasque\nsouthwestern\nshifted\nromance\nnorwegian\ncommon turkic\nkurdish\nfarsic\narabic\n(b) Machine Translation (Variety level)\nFigure 10: Task specific plot of maximum obtainable Linguistic Utility for all varieties. The yellow-shaded region\nrepresents language clusters having no varieties seen during mBERT pertaining. The bars with colored stripes\nrepresent the standard variety of a cluster. The dialect with the Rawlsian score in each cluster is that with the\nleftmost bar.\n0\n20\n40\n60\n80\neng(eng, C)\n0\n20\n40\n60\n80\neng(v, C)\nalbanian\nanglic\narabic\new armenian\ngal-rhaetian\nh. german\nita. romance\nkomi\nneva\nnorwegian\nsaami\nsinitic\nsw shift. romance\ntup--guarani sub.\n(a) Parts-of-speech (POS) tagging\n0\n10\n20\n30\n40\n50\n60\n70\n80\neng(eng, C)\n20\n0\n20\n40\n60\n80\neng(v, C)\nanglic\narabic\ncircassian\ncommon turkic\ne. romance\nfrisian\ngal-italian\ngal-rhaetian\ngreater panjabic\ngreek\nh. german\nhindustani\ninuit\nita. romance\nkomi\nkurdish\nlatvian\nmari\nm. dutch\nsardo-corsican\nserb-croa-bos.\nsinitic\nsorbian\nsotho-tswana (s.30)\nsw shift. romance\n(b) Named entity recognition (NER)\n0\n10\n20\n30\n40\n50\n60\neng(eng, C)\n10\n5\n0\n5\n10\n15\n20\neng(v, C)\nanglic\narabic\ncommon turkic\ngal-italian\nh. german\nita. romance\nkurdish\nlatvian\nnorwegian\nsinitic\nsotho-tswana (s.30)\nsw shift. romance\n(c) Natural language inference (NLI)\nFigure 11: Dialectal Gap visualization for language clusters utilizing zero-shot cross-lingual transfer from Standard\nEnglish. The x-axis is for cluster gap while comparing against Standard English variety. Values far from zero have a\nlarger performance gap from English. The y-axis is for aggregated cluster gap while comparing against standard\ncluster variety and values far from zero have larger within cluster gap. Ideally, we want both of them to be close to\nzero.\ncluster\nvariety\nDataset-code\ndataset\nZeroshot\n(mBERT)\nZeroshot\n(XLM-R)\nFinetune\n(mBERT)\nFinetune\n(XLM-R)\nalbanian\nalbanian\nUD_Albanian-TSA\nud\n75.80\n84.41\n-\n-\ngheg albanian\nUD_Gheg-GPS\nud\n48.96\n55.84\n-\n-\nanglic\nenglish\nUD_English-EWT\nud\n96.41\n97.16\n96.41\n97.16\nsinglish\nsinglish\nud\n76.27\n77.55\n87.38\n87.96\narabic\nsouth levantine arabic\nUD_South_Levantine_Arabic-MADAR\nud\n51.99\n61.84\n-\n-\nstandard arabic\nUD_Arabic-PADT\nud\n39.74\n56.67\n95.72\n96.11\ngulf arabic\ndar-glf\nnoisy\n38.84\n50.12\n-\n-\negyptian arabic\ndar-egy\nnoisy\n36.14\n51.39\n-\n-\nlevantine arabic\ndar-lev\nnoisy\n32.66\n43.76\n-\n-\nnorth african arabic\nUD_Maghrebi_Arabic_French-Arabizi\nud\n28.30\n26.01\n67.89\n59.29\neastern-western armenian\neastern armenian\nUD_Armenian-ArmTDP\nud\n71.78\n82.63\n91.31\n92.36\nwestern armenian\nUD_Western_Armenian-ArmTDP\nud\n70.27\n75.31\n94.86\n95.14\ngallo-italian\nligurian\nUD_Ligurian-GLT\nud\n58.90\n52.78\n13.16\n5.09\ngallo-rhaetian\nfrench\nUD_French-ParTUT\nud\n84.36\n85.47\n96.88\n96.42\nfrench (a:paris)\nUD_French-ParisStories\nud\n81.37\n82.77\n96.70\n96.76\nold french (842-ca. 1400)\nUD_Old_French-SRCMF\nud\n64.70\n59.41\n95.64\n95.64\nhigh german\ngerman\nUD_German-LIT\nud\n87.08\n88.36\n-\n-\ncentral alemannic (a:zh)\nUD_Swiss_German-UZH\nud\n62.56\n47.18\n-\n-\nitalian romance\nitalian\nUD_Italian-PUD\nud\n81.09\n83.12\n-\n-\nitalian (r:formal,\nm:written,\ni:essay)\nUD_Italian-MarkIT\nud\n80.00\n81.87\n93.35\n92.70\nitalian\n(r:casual,\nm:written,\ni:tweet)\nUD_Italian-PoSTWITA\nud\n73.71\n76.45\n95.80\n96.83\ncontinental southern italian\nUD_Neapolitan-RB\nud\n30.00\n57.14\n-\n-\nkomi\nkomi-zyrian (m:spoken)\nUD_Komi_Zyrian-IKDP\nud\n41.25\n46.66\n-\n-\nkomi-permyak\nUD_Komi_Permyak-UH\nud\n29.52\n43.67\n-\n-\nkomi-zyrian (m:written)\nUD_Komi_Zyrian-Lattice\nud\n20.40\n35.12\n-\n-\nneva\nfinnish\nUD_Finnish-TDT\nud\n81.29\n86.21\n96.05\n97.76\nestonian\nUD_Estonian-EDT\nud\n80.34\n85.17\n96.49\n97.20\nhame (a:tavastian)\nmurre-HAAM\nnoisy\n55.63\n70.08\n-\n-\ncentral and north pohjanmaa\n(a:ostrobothnian)\nmurre-POH\nnoisy\n55.09\n69.68\n-\n-\nsavo (a:savonian)\nmurre-SAV\nnoisy\n54.70\n69.42\n-\n-\nsoutheastern finnish (a:south-\neast)\nmurre-KAA\nnoisy\n51.68\n68.71\n-\n-\nsouthwestern finnish (a:south-\nwest)\nmurre-LVA\nnoisy\n49.80\n68.54\n-\n-\nneva (a:south-west trans)\nmurre-LOU\nnoisy\n42.57\n61.72\n-\n-\nnorwegian\nnorwegian bokmål (m:written)\nUD_Norwegian-Bokmaal\nud\n88.53\n89.55\n98.19\n98.67\nnorwegian nynorsk (m:written)\nUD_Norwegian-Nynorsk\nud\n85.06\n85.81\n97.83\n98.32\nnorwegian nynorsk (m:written,\ni:old)\nUD_Norwegian-NynorskLIA\nud\n73.25\n79.29\n95.47\n95.72\nsaami\nnorth saami\nUD_North_Sami-Giella\nud\n35.92\n32.13\n78.89\n71.50\nskolt saami\nUD_Skolt_Sami-Giellagas\nud\n20.26\n34.15\n-\n-\nsabellic\numbrian\nUD_Umbrian-IKUVINA\nud\n11.90\n5.44\n-\n-\nsinitic\nclassical-middle-modern\nsinitic\n(a:hongkong,\no:traditional)\nUD_Chinese-HK\nud\n68.99\n35.49\n-\n-\nclassical-middle-modern\nsinitic (o:simplified)\nUD_Chinese-GSDSimp\nud\n58.26\n30.92\n94.72\n95.02\nclassical chinese\nUD_Classical_Chinese-Kyoto\nud\n35.80\n20.85\n89.62\n89.49\nsouthwestern shifted romance\nportuguese (a:european)\nUD_Portuguese-PUD\nud\n80.08\n81.38\n-\n-\nbrazilian portuguese\nUD_Portuguese-GSD\nud\n78.63\n80.12\n98.19\n98.50\nportuguese (i:mix)\nUD_Portuguese-Bosque\nud\n78.48\n79.85\n97.71\n97.81\noccitan\nROci\nnoisy\n76.84\n65.80\n-\n-\nportuguese (m:written)\nUD_Portuguese-CINTIL\nud\n76.19\n78.76\n97.67\n97.87\ntupi-guarani subgroup i.a\nmbyá guaraní (a:paraguay)\nUD_Mbya_Guarani-Thomas\nud\n27.89\n28.77\n-\n-\nold guarani\nUD_Guarani-OldTuDeT\nud\n8.96\n10.30\n-\n-\nmbyá guaraní (a:brazil)\nUD_Mbya_Guarani-Dooley\nud\n1.94\n0.59\n-\n-\nwest low german\nwest low german\nUD_Low_Saxon-LSDC\nud\n69.65\n54.93\n-\n-\nTable 11: Parts-of-speech evaluation report comprising zeroshot score and in-language finetuning. We report F1 as\nevaluation score. Zeroshot scores are evaluated using model finetuned on Standerd English. If training data is not\navailable, we skip those languages (mentioned as ‘-’) for in-language finetuning.\nTable 12: Named entity recognition (NER) evaluation report comprising zeroshot score and in-group finetuning. We\nreport F1 as evaluation score. Zeroshot scores are evaluated using model finetuned on Standerd English. If training\ndata is not available, we skip those languages (mentioned as ‘-’) for in-language finetuning.\ncluster\nvariety\ntarget-\ncode\nsource\ndataset\nsupport Zeroshot\n(mBERT)\nZeroshot\n(XLM-R)\nFinetune\n(mBERT)\nFinetune\n(XLM-R)\nanglic\nenglish\n(o:controlled)\nsimple\nen\nwikiann\n1000\n89.07\n86.03\n89.07\n86.03\nenglish\nen\nen\nwikiann\n10000\n84.15\n82.11\n84.15\n82.11\nold english (ca. 450-\n1100)\nang\nen\nwikiann\n100\n54.41\n55.94\n54.41\n55.94\njamaican creole en-\nglish\njam\nen\nwikiann\n0\n0.00\n0.00\n0.00\n0.00\narabic\negyptian arabic\narz\nar\nwikiann\n100\n43.82\n50.58\n67.77\n73.33\nstandard arabic\nar\nar\nwikiann\n10000\n41.12\n41.76\n89.10\n87.85\ncircassian\nadyghe\nady\nen\nwikiann\n693\n67.33\n54.03\n-\n-\nkabardian\nkbd\nen\nwikiann\n1482\n47.51\n34.79\n-\n-\ncommon turkic\nturkish\ntr\ntr\nwikiann\n10000\n73.56\n75.71\n92.90\n91.80\nnorth azerbaijani\naz\naz\nwikiann\n1000\n67.31\n61.01\n89.17\n88.26\ncrimean tatar\ncrh\ntr\nwikiann\n100\n47.81\n40.57\n57.99\n52.67\nsouth azerbaijani\nazb\naz\nwikiann\n2567\n31.67\n11.16\n30.22\n22.14\neastern romance\nromanian\nro\nro\nwikiann\n10000\n74.62\n70.82\n94.17\n93.47\naromanian\nroa-rup\nel\nwikiann\n732\n64.78\n62.66\n68.92\n63.33\nmoldavian\nmo\nro\nwikiann\n345\n63.31\n56.53\n60.60\n64.56\nfrisian\nwestern frisian\nfy\nnl\nwikiann\n1000\n80.17\n71.96\n81.06\n78.21\nems-weser frisian\nstq\nnl\nwikiann\n1085\n59.45\n57.47\n64.55\n55.15\nnorthern frisian\nfrr\nnl\nwikiann\n100\n46.67\n46.22\n54.78\n52.63\ngallo-italian\npiemontese\npms\nit\nwikiann\n100\n79.53\n71.10\n87.88\n78.72\nlombard\nlmo\nit\nwikiann\n100\n72.49\n68.17\n79.37\n78.66\nvenetian\nvec\nit\nwikiann\n100\n62.71\n55.20\n73.73\n67.51\nligurian\nlij\nit\nwikiann\n100\n45.36\n34.75\n56.51\n47.79\nemiliano-\nromagnolo\neml\nit\nwikiann\n100\n33.55\n33.23\n42.80\n45.45\ngallo-rhaetian\nfrench\nfr\nfr\nwikiann\n10000\n79.16\n76.52\n90.96\n89.00\nanglo-norman\nnrm\nfr\nwikiann\n1281\n66.78\n88.56\n71.98\n71.92\narpitan\nfrp\nit\nwikiann\n2358\n63.30\n63.67\n68.38\n71.13\nromansh\nrm\nit\nwikiann\n100\n56.88\n55.19\n69.69\n67.58\nfriulian\nfur\nit\nwikiann\n100\n51.41\n50.75\n64.12\n56.30\nwalloon\nwa\nfr\nwikiann\n100\n46.33\n41.27\n45.19\n42.50\ngreater panjabic\nwestern panjabi\npnb\npa\nwikiann\n100\n64.46\n53.78\n17.82\n0.00\neastern panjabi\npa\npa\nwikiann\n100\n32.90\n45.25\n30.63\n0.00\ngreek\nmodern greek\nel\nel\nwikiann\n10000\n71.76\n72.96\n91.18\n90.68\npontic\npnt\nel\nwikiann\n291\n66.37\n69.79\n68.45\n72.58\nhigh german\ngerman\nde\nde\nwikiann\n10000\n79.08\n75.67\n89.97\n87.80\ncentral alemannic\nals\nde\nwikiann\n100\n75.36\n65.15\n84.84\n79.02\nluxemburgish\nlb\nnl\nwikiann\n1000\n71.61\n49.22\n79.22\n58.71\nlimburgan\nli\nnl\nwikiann\n100\n63.03\n63.72\n78.03\n73.15\nbavarian\nbar\nde\nwikiann\n100\n56.62\n55.96\n76.36\n68.84\nkölsch\nksh\nnl\nwikiann\n100\n54.80\n39.42\n62.50\n48.51\npfaelzisch-\nlothringisch\npfl\nde\nwikiann\n1092\n49.40\n47.19\n59.12\n50.14\npennsylvania\nger-\nman\npdc\nde\nwikiann\n100\n41.76\n41.79\n39.39\n38.76\nhindustani\nfiji hindi\nhif\nhi\nwikiann\n715\n81.29\n79.67\n74.28\n85.15\nhindi\nhi\nhi\nwikiann\n1000\n65.74\n65.77\n88.11\n85.83\ninuit\nalaskan inupiaq\nik\nen\nwikiann\n431\n76.27\n70.56\n-\n-\nkalaallisut\nkl\nen\nwikiann\n1403\n63.20\n60.20\n-\n-\nitalian romance\nitalian\nit\nit\nwikiann\n10000\n81.44\n78.35\n92.21\n90.22\nitalian\nromance\n(a:apulia, m:spoken,\ni:tarantino)\nroa-tara\nit\nwikiann\n3811\n62.97\n66.02\n60.34\n64.45\nsicilian\nscn\nit\nwikiann\n100\n60.26\n54.46\n72.25\n60.87\ncontinental southern\nitalian\nnap\nit\nwikiann\n100\n57.35\n54.81\n61.48\n58.33\nkomi\nkomi\nkv\nen\nwikiann\n2464\n56.78\n41.78\n-\n-\nkomi-permyak\nkoi\nen\nwikiann\n2798\n53.62\n47.80\n-\n-\nkurdish\nkurdish\nku\nku\nwikiann\n100\n31.67\n59.48\n13.70\n0.00\ncentral kurdish\nckb\nku\nwikiann\n1000\n6.90\n35.49\n1.17\n0.00\nlatvian\nlatvian\nlv\nlv\nwikiann\n10000\n69.36\n71.07\n93.24\n92.30\neast latvian\nltg\nlv\nwikiann\n1036\n48.27\n48.86\n50.10\n49.95\nContinued on next page\nTable 12: Named entity recognition (NER) evaluation report comprising zeroshot score and in-group finetuning. We\nreport F1 as evaluation score. Zeroshot scores are evaluated using model finetuned on Standerd English. If training\ndata is not available, we skip those languages (mentioned as ‘-’) for in-language finetuning.\ncluster\nvariety\ntarget-\ncode\nsource\ndataset\nsupport Zeroshot\n(mBERT)\nZeroshot\n(XLM-R)\nFinetune\n(mBERT)\nFinetune\n(XLM-R)\nmari\neastern mari\nmhr\nmhr\nwikiann\n100\n46.67\n40.94\n32.70\n0.00\nwestern mari\nmrj\nmhr\nwikiann\n6036\n38.29\n58.46\n5.51\n0.00\nmodern dutch\ndutch\nnl\nnl\nwikiann\n10000\n82.03\n80.42\n91.96\n90.51\nwestern flemish\nvls\nnl\nwikiann\n100\n73.36\n72.08\n77.66\n79.85\nzeeuws\nzea\nnl\nwikiann\n100\n65.98\n66.20\n79.55\n76.09\nnorwegian\nnorwegian nynorsk\n(m:written)\nnynorsk\nnynorsk\nnorwegian_ner1511\n87.58\n87.86\n87.58\n87.86\nnorwegian\n(m:written,\ni:samnorsk)\nsamnorsk\nsamnorsk\nnorwegian_ner3450\n86.96\n90.55\n86.96\n90.55\nnorwegian bokmål\n(m:written)\nbokmaal\nbokmaal\nnorwegian_ner1939\n85.82\n90.54\n85.82\n90.54\nsardo-corsican\nsardinian\nsc\nit\nwikiann\n917\n67.32\n65.26\n80.87\n81.17\ncorsican\nco\nit\nwikiann\n100\n56.65\n56.41\n70.59\n66.06\nserbian-croatian-bosnian\ncroatian standard\nhr\nhr\nwikiann\n10000\n77.59\n78.12\n92.40\n91.02\nbosnian standard\nbs\nhr\nwikiann\n1000\n69.93\n74.88\n87.50\n88.86\nserbian standard\nsr\nhr\nwikiann\n10000\n64.38\n60.71\n63.68\n65.86\nserbian-croatian-\nbosnian\nsh\nhr\nwikiann\n10000\n38.92\n69.14\n85.05\n85.43\nsinitic\nwu chinese\nwuu\nzh\nwikiann\n100\n71.89\n35.80\n74.15\n63.73\nmin nan chinese\nzh-min-\nnan\nzh\nwikiann\n100\n44.68\n47.30\n21.40\n15.08\ncantonese\nzh-yue\nzh\nwikiann\n10000\n43.73\n26.55\n76.19\n71.97\nmandarin chinese\nzh\nzh\nwikiann\n10000\n42.86\n24.71\n81.12\n77.22\nclassical chinese\nzh-\nclassical\nzh\nwikiann\n100\n28.03\n16.78\n67.28\n62.39\nhakka chinese\nhak\nzh\nwikiann\n100\n27.43\n31.84\n36.55\n40.00\nsorbian\nlower sorbian\ndsb\nhsb\nwikiann\n862\n71.04\n68.80\n3.03\n0.00\nupper sorbian\nhsb\nhsb\nwikiann\n100\n65.44\n65.48\n38.20\n0.00\nsotho-tswana (s.30)\nsouthern sotho\nst\nen\nwikiann\n339\n64.36\n69.26\n-\n-\nnorthern sotho\nnso\nen\nwikiann\n720\n19.08\n29.66\n-\n-\nsouthwestern shifted romance\ngalician\ngl\nes\nwikiann\n10000\n81.98\n80.27\n87.99\n86.14\nspanish\nes\nes\nwikiann\n10000\n72.80\n70.73\n92.17\n90.32\noccitan\noc\nit\nwikiann\n100\n72.00\n67.58\n78.50\n75.35\nmirandese\nmwl\nes\nwikiann\n100\n46.20\n44.07\n49.29\n42.81\nextremaduran\next\nes\nwikiann\n100\n44.83\n38.33\n61.82\n40.00\nwest low german\nwest low german\nnds\nde\nwikiann\n100\n80.29\n66.44\n79.40\n70.99\nTable 20: Zero-shot results for Machine Translation. We evaluate NLLB_600m and NLLB_1_3bn by translating\neach dialectal variety to English. For all languages without reference translations in English, we evaluate dialectal\ntranslations using the standard language’s translation as the reference. For varieties with parallel data like Yoruba,\nTurkish, Farsi, Tigrinya and Bengali we use the English reference provided in the dataset.\nlanguage_group\nvariety\nNLLB_600m-bleu\nNLLB_1_3bn-bleu\narabic\ngulf arabic (a:riy)\n43.07\n43.07\ngulf arabic (a:mus)\n40.32\n40.32\negyptian arabic (a:alx)\n38.90\n38.90\negyptian arabic (a:cai)\n38.71\n38.71\negyptian arabic (a:kha)\n38.10\n38.10\nsouth levantine arabic (a:south-sal)\n37.79\n37.79\nfez. meknes\n37.19\n37.19\nnorth mesopotamian arabic (a:mos)\n36.47\n36.47\narabian peninsula arabic (a:yemen)\n35.93\n35.93\ngilit mesopotamian arabic\n35.78\n35.78\negyptian arabic (a:asw)\n35.02\n35.02\nsouth levantine arabic (a:south-amm)\n34.88\n34.88\nnorth mesopotamian arabic (a:bas)\n34.41\n34.41\ngulf arabic (a:doh)\n34.20\n34.20\ngulf arabic (a:jed)\n34.10\n34.10\nlevantine arabic (a:north-dam)\n33.36\n33.36\nContinued on next page\nTable 20: Machine Translation\nlanguage_group\nvariety\nNLLB_600m-bleu\nNLLB_1_3bn-bleu\nsouth levantine arabic (a:south-jer)\n33.27\n33.27\nlibyan arabic (a:ben)\n32.45\n32.45\naleppo\n31.64\n31.64\ntripolitanian arabic\n31.21\n31.21\nrabat-casablanca arabic\n31.17\n31.17\nsunni beiruti arabic\n31.07\n31.07\nalgerian arabic\n27.25\n27.25\ntunisian arabic (a:tun)\n25.10\n25.10\nsfax\n23.24\n23.24\nbasque\nbasque (a:azkaine)\n25.61\n22.58\nnavarro-labourdin basque (a:helete)\n22.53\n22.03\nbasque (a:garruze)\n25.40\n22.00\nnavarro-labourdin basque (a:behorlegi)\n24.19\n21.35\nbasque (a:luhuso)\n22.58\n20.95\nbasque (a:amenduze)\n24.21\n20.86\nbasque (a:senpere)\n23.47\n20.35\nbasque (a:sara)\n23.04\n20.25\nbasque (a:jutsi)\n21.46\n19.99\nnavarro-labourdin basque (a:mugerre)\n22.00\n19.71\nbasque (a:baigorri)\n20.21\n18.80\nbasque (a:uharte)\n21.10\n18.57\nbasque (a:suhuskune)\n21.10\n18.57\nbasque (a:donibane)\n20.28\n18.03\nbasque (a:larzabale)\n20.37\n17.88\nnavarro-labourdin basque (a:bidarrai)\n17.94\n16.90\nnavarro-labourdin basque (a:urruna)\n18.50\n16.25\nbasque (a:iholdi)\n16.68\n15.01\nbasque (a:jatsu)\n17.01\n14.56\nbasque (a:barkoxe)\n10.93\n10.82\nsouletin (a:maule)\n11.55\n10.36\nbengali\nvanga (a:jessore)\n20.71\n21.44\nvanga (a:khulna)\n18.96\n19.73\nvanga (a:kushtia)\n17.36\n19.12\nvanga (a:dhaka)\n17.18\n17.85\nvanga (a:barisal)\n11.33\n12.68\ncommon turkic\nuzbek\n24.68\n28.82\nturkish\n22.24\n24.22\nbashkir\n20.88\n23.78\ncentral oghuz\n18.82\n22.65\nkirghiz\n18.68\n22.53\nkazakh\n19.56\n20.96\nkara-kalpak\n14.31\n18.23\nsakha\n2.53\n2.43\nfarsic\ndari\n37.02\n40.49\ngreek\ncretan\n25.03\n21.34\napulian greek\n3.92\n3.97\nhigh german\ncentral bavarian\n9.49\n12.01\nupper saxon\n11.99\n8.63\nkurdish\nsorani\n35.74\n34.88\nsine’i\n22.56\n22.60\nnorwegian\nnorwegian (a:eastern)\n24.00\n27.49\nnorwegian (a:southwestern)\n21.88\n15.81\nnorwegian (a:setesdal)\n2.24\n4.23\nsouthwestern shifted romance\noccitan\n20.73\n25.74\ntigrinya\ntigrinya (a:ethiopia)\n22.40\n25.36\ntigrinya (a:eritrea)\n18.64\n21.02\nyoruba\nyoruba (a:central nigeria)\n21.46\n18.10\nhigh german\ncentral alemannic (a:zh)\n43.71\n44.06\ncentral alemannic (a:gl)\n43.90\n43.96\nContinued on next page\nTable 20: Machine Translation\nlanguage_group\nvariety\nNLLB_600m-bleu\nNLLB_1_3bn-bleu\ncentral alemannic (a:tg)\n43.94\n43.62\ncentral alemannic (a:bs)\n42.94\n43.48\ncentral alemannic (a:gr)\n43.67\n43.31\ncentral alemannic (a:ag)\n43.62\n43.26\ncentral alemannic (a:sh)\n43.62\n43.01\ncentral alemannic (a:zg)\n43.05\n42.95\ncentral alemannic (a:ai)\n42.27\n42.72\ncentral alemannic (a:ar)\n43.90\n42.67\ncentral alemannic (a:sz)\n43.24\n42.57\ncentral alemannic (a:vs)\n41.69\n42.52\ncentral alemannic (a:ur)\n44.08\n42.41\ncentral alemannic (a:be)\n42.51\n42.36\ncentral alemannic (a:lu)\n42.86\n42.22\ncentral alemannic (a:nw)\n43.04\n41.89\ncentral alemannic (a:bl)\n43.11\n41.68\ncentral alemannic (a:fr)\n42.13\n41.05\ncentral alemannic (a:ow)\n42.29\n40.89\ncentral alemannic (a:sg)\n41.11\n40.55\ncentral alemannic (a:so)\n40.27\n39.39\nitalian romance\nitalian (a:umbria)\n40.73\n41.62\nitalian (a:lazio)\n39.03\n32.23\nitalian (a:veneto)\n29.38\n31.27\nitalian (a:toscana)\n28.90\n31.24\nitalian (a:marche)\n26.29\n30.22\nitalian (a:trentino-alto adige/südtirol)\n23.40\n25.56\nitalian (a:unknown)\n22.25\n24.54\nitalian (a:friuli-venezia giulia)\n18.62\n23.51\nitalian (a:sicilia)\n22.91\n23.00\nitalian (a:calabria)\n20.11\n20.48\nitalian (a:liguria)\n17.98\n20.22\nitalian (a:lombardia)\n16.23\n17.32\nitalian (a:campania)\n17.03\n17.03\nitalian (a:molise)\n17.73\n15.98\nitalian (a:puglia)\n16.50\n15.64\nitalian (a:piemonte)\n14.48\n15.54\nitalian (a:basilicata)\n13.76\n14.36\nitalian (a:emilia-romagna)\n13.86\n14.35\nitalian (a:sardegna)\n11.67\n12.98\nitalian (a:abruzzo)\n13.64\n11.02\nF\nHighest performing and lowest performing varieties\nG\nLow-resource variety performing better in zero-shot NER\nH\nCluster level Result Summaries with Demographic Utility and Standard Deviation\nreport\nI\nIn-Context Learning Details\nI.1\nPrompts\nWe adapt the prompts from Super-NaturalInstructions (Wang et al., 2022) for our in-context learning experiments.\nSentiment Analysis.\nFor sentiment analysis we provide 4 few-shot examples in the prompt. The prompt template is given\nbelow:\nIn this task, you are given a piece of text. Your task is to classify the sentiment of the text based\non its content.\nSentence: <Sentence Example 1>\nLabel: <Label for Example 1, Positive, negative, neutral>\n· · ·\nSentence: <Sentence Example k>\ncluster\nvariety\ntarget-code\nsrc\nmBERT (acc)\nXLM-R (acc)\nmBERT (F1)\nXLM-R (F1)\nanglic\nenglish\neng_Latn\neng_Latn\n81.90\n83.35\n81.95\n83.43\narabic\nstandard arabic\narb_Arab\neng_Latn\n65.57\n73.83\n65.57\n73.85\nnajdi arabic\nars_Arab\neng_Latn\n59.42\n69.02\n59.14\n68.94\nta’izzi-adeni arabic\nacq_Arab\neng_Latn\n58.84\n68.72\n58.64\n68.62\nmoroccan arabic\nary_Arab\neng_Latn\n54.65\n58.30\n54.61\n58.14\negyptian arabic\narz_Arab\neng_Latn\n54.53\n65.87\n53.86\n65.70\nsouth levantine ara-\nbic\najp_Arab\neng_Latn\n54.09\n64.13\n53.42\n63.81\nnorth mesopotamian\narabic\nacm_Arab\neng_Latn\n52.95\n58.94\n52.84\n58.75\nlevantine\narabic\n(a:north)\napc_Arab\neng_Latn\n52.14\n61.74\n51.40\n61.31\ntunisian arabic\naeb_Arab\neng_Latn\n47.54\n50.18\n47.42\n50.20\ncommon turkic\nnorth azerbaijani\nazj_Latn\neng_Latn\n59.76\n73.17\n59.20\n73.17\ncentral\noghuz\n(m:spoken)\ntur_Latn\neng_Latn\n59.14\n74.47\n58.37\n74.52\nsouth azerbaijani\nazb_Arab\neng_Latn\n46.05\n41.82\n44.58\n39.24\ngallo-italian\nvenetian\nvec_Latn\neng_Latn\n65.15\n68.52\n64.99\n68.55\nlombard\nlmo_Latn\neng_Latn\n59.38\n56.39\n59.34\n56.16\nligurian\nlij_Latn\neng_Latn\n57.82\n57.70\n56.70\n57.16\nhigh german\nluxemburgish\nltz_Latn\neng_Latn\n60.34\n47.33\n60.01\n46.21\nlimburgan\nlim_Latn\neng_Latn\n50.80\n59.88\n50.31\n59.75\nitalian romance\nitalian\nita_Latn\neng_Latn\n73.71\n78.08\n73.71\n78.19\nsicilian\nscn_Latn\neng_Latn\n62.69\n56.17\n62.66\n55.82\nkurdish\ncentral kurdish\nckb_Arab\neng_Latn\n40.98\n44.93\n37.40\n39.59\nnorthern kurdish\nkmr_Latn\neng_Latn\n39.10\n63.45\n33.93\n63.26\nlatvian\nlatvian\nlvs_Latn\neng_Latn\n60.14\n73.57\n59.95\n73.63\neast latvian\nltg_Latn\neng_Latn\n48.62\n54.19\n47.02\n53.54\nnorwegian\nnorwegian bokmål\n(m:written)\nnob_Latn\neng_Latn\n72.44\n79.44\n72.45\n79.51\nnorwegian nynorsk\n(m:written)\nnno_Latn\neng_Latn\n68.08\n71.16\n68.10\n71.06\nsinitic\nclassical-middle-\nmodern\nsinitic\n(o:simplified)\nzho_Hans\neng_Latn\n68.52\n72.61\n68.54\n72.57\nclassical-middle-\nmodern\nsinitic\n(o:traditional)\nzho_Hant\neng_Latn\n61.72\n64.89\n61.48\n64.49\ncantonese\nyue_Hant\neng_Latn\n60.44\n68.02\n60.27\n67.41\nsotho-tswana (s.30)\nnorthern sotho\nnso_Latn\neng_Latn\n39.38\n40.18\n35.06\n35.98\nsouthern sotho\nsot_Latn\neng_Latn\n39.36\n39.30\n34.62\n34.16\nsouthwestern shifted romance\nspanish\nspa_Latn\neng_Latn\n75.13\n79.00\n75.15\n79.09\nportuguese\n(a:european)\npor_Latn\neng_Latn\n73.73\n79.18\n73.73\n79.22\ngalician\nglg_Latn\neng_Latn\n73.39\n78.48\n73.39\n78.55\noccitan\noci_Latn\neng_Latn\n68.48\n63.09\n68.47\n62.96\nTable 13: Natural language inference (NLI) evaluation report using zeroshot cross-lingual transfer from Standard\nEnglish. We report F1 as evaluation score. NLI uses F1 as evaluation matric. We prepare a translate-train dataset to\nperform this evaluation.\ncluster\nvariety\ntarget-code\ncount\nFinetune (mBERT)\nFinetune (XLM-R)\nZeroshot (mBERT)\nZeroshot (XLM-R)\nanglic\nirish english\nenglish–irl\n494\n73.00\n67.98\n68.62\n62.45\nsoutheast american english\nenglish–usa\n494\n73.51\n67.95\n68.56\n62.97\nnew zealand english\nenglish–nzl\n494\n73.62\n68.50\n68.21\n63.05\nsouthern african english\nenglish–zaf\n494\n73.50\n68.06\n68.13\n63.19\nenglish (a:scotland)\nenglish–gbr\n494\n73.30\n67.57\n67.90\n62.49\naustralian english\nenglish–aus\n494\n73.23\n68.04\n67.71\n62.33\nnigerian english\nenglish–nga\n494\n72.53\n66.56\n67.39\n62.37\nphilippine english\nenglish–phl\n494\n73.75\n67.29\n67.35\n61.54\nindian english (a:south)\nenglish–ind_s\n494\n70.96\n66.04\n65.43\n61.72\nkenyan english\nenglish–kenya\n494\n70.63\n65.64\n65.30\n60.28\nindian english (a:north)\nenglish–ind_n\n494\n70.28\n65.32\n64.46\n60.84\narabic\nmoroccan arabic\narabic–mar\n324\n70.94\n65.14\n50.94\n49.56\ntunisian arabic\narabic–tun\n324\n71.36\n65.31\n50.86\n50.01\narabic (a:jordan)\narabic–jor\n324\n70.60\n65.77\n50.81\n49.56\narabic (a:bahrain)\narabic–bhr\n324\n70.96\n65.86\n49.88\n50.36\narabic (a:saudi-arabia)\narabic–sau\n324\n70.96\n65.58\n49.29\n49.63\negyptian arabic\narabic–egy\n324\n70.06\n65.34\n48.72\n48.71\nalgerian arabic\narabic–dza\n324\n68.63\n64.71\n48.71\n48.65\nbengali\nbengali (a:west bengal)\nbengali–ind\n107\n67.64\n70.89\n28.06\n38.24\nbengali (a:dhaka)\nbengali–dhaka\n107\n68.98\n66.84\n27.17\n37.32\nkorean\nseoul (m:spoken)\nkorean–korn\n60\n9.60\n28.08\n6.89\n20.24\nkorean (a:south-eastern, m:spoken)\nkorean–kors\n60\n9.27\n26.96\n6.89\n19.66\nswahili\nswahili (a:kenya)\nswahili–kenya\n1000\n72.06\n71.90\n46.20\n46.22\nswahili (a:tanzania)\nswahili–tanzania\n1000\n71.08\n70.08\n45.11\n46.00\nTable 14: Extractive dialectal question answering evaluation on SD-QA development set. We report span F1\nas evaluation score. Zeroshot scores are evaluated using model finetuned on Standerd English whereas, we use\ncombined training set for supervised finetuning\ncluster\nvariety\ntarget-code\ncount\nFinetune\n(mBERT)\nFinetune\n(XLM-R)\nZeroshot\n(mBERT)\nZeroshot\n(XLM-R)\nICL Mistral\nanglic\nenglish (a:scotland)\nenglish–gbr\n440\n76.38\n70.34\n71.82\n63.15\n70.18\nsouthern african english\nenglish–zaf\n440\n76.66\n71.18\n71.49\n63.87\n71.14\nnew zealand english\nenglish–nzl\n440\n76.71\n71.39\n71.22\n63.69\n70.95\naustralian english\nenglish–aus\n440\n75.66\n70.89\n71.20\n62.28\n69.23\nsoutheast american en-\nglish\nenglish–usa\n440\n77.26\n71.50\n71.17\n63.71\n71.76\nirish english\nenglish–irl\n440\n75.52\n70.73\n70.92\n62.15\n70.64\nphilippine english\nenglish–phl\n440\n76.37\n70.64\n70.47\n62.22\n70.11\nnigerian english\nenglish–nga\n440\n73.61\n68.33\n69.10\n61.27\n68.10\nindian english (a:north)\nenglish–ind_n\n440\n74.62\n68.03\n68.84\n61.25\n68.99\nkenyan english\nenglish–kenya\n440\n72.59\n66.68\n68.72\n58.64\n64.91\nindian english (a:south)\nenglish–ind_s\n440\n71.93\n66.88\n66.49\n60.36\n65.13\narabic\narabic (a:bahrain)\narabic–bhr\n921\n77.52\n72.11\n53.25\n53.28\n55.33\narabic (a:jordan)\narabic–jor\n921\n77.35\n71.29\n52.72\n53.72\n54.93\narabic (a:saudi-arabia)\narabic–sau\n921\n77.88\n72.11\n52.72\n53.24\n55.66\nalgerian arabic\narabic–dza\n921\n77.85\n72.34\n52.56\n53.52\n55.18\ntunisian arabic\narabic–tun\n921\n76.72\n71.64\n52.28\n52.94\n55.03\nmoroccan arabic\narabic–mar\n921\n76.73\n71.57\n51.86\n52.17\n53.92\negyptian arabic\narabic–egy\n921\n76.53\n70.75\n51.80\n51.99\n54.66\nbengali\nbengali (a:west bengal)\nbengali–ind\n113\n68.62\n73.27\n32.30\n36.39\n50.06\nbengali (a:dhaka)\nbengali–dhaka\n113\n67.37\n74.24\n31.79\n35.52\n51.73\nkorean\nseoul (m:spoken)\nkorean–korn\n276\n10.15\n31.91\n7.26\n19.62\n65.79\nkorean (a:south-eastern,\nm:spoken)\nkorean–kors\n276\n9.92\n31.01\n7.22\n20.08\n64.63\nswahili\nswahili (a:tanzania)\nswahili–tanzania\n472\n63.54\n62.30\n38.24\n39.38\n55.70\nswahili (a:kenya)\nswahili–kenya\n472\n72.25\n70.53\n37.97\n41.59\n58.71\nTable 15: Extractive dialectal question answering evaluation on SD-QA test set. We report span F1 as evaluation\nscore. Zeroshot scores are evaluated using model finetuned on Standerd English whereas, we use combined training\nset for supervised finetuning\ncluster\nvariety\ndialect-code\nsupport\nprecision\n(mBERT)\nprecision\n(XLM-R)\nrecall\n(mBERT)\nrecall\n(XLM-R)\nF1\n(mBERT)\nF1\n(XLM-\nR)\nanglic\nenglish (a:uk)\nenglish:en-gb\n249\n98.10\n89.57\n83.13\n71.59\n90.00\n79.58\nnorth american english\nenglish:en-us\n349\n93.27\n88.14\n83.38\n82.09\n88.05\n85.01\narabic\naleppo\narabic:ale\n200\n59.50\n58.50\n59.50\n48.35\n59.50\n52.94\nalgerian arabic\narabic:alg\n272\n79.00\n65.50\n58.09\n62.68\n66.95\n64.06\narabian peninsula arabic\n(a:yemen)\narabic:san\n177\n60.50\n55.50\n68.36\n56.63\n64.19\n56.06\negyptian arabic (a:alx)\narabic:alx\n192\n70.50\n74.50\n73.44\n66.82\n71.94\n70.45\negyptian arabic (a:asw)\narabic:asw\n221\n56.00\n52.00\n50.68\n45.02\n53.21\n48.26\negyptian arabic (a:cai)\narabic:cai\n130\n35.50\n36.50\n54.62\n72.28\n43.03\n48.50\negyptian arabic (a:kha)\narabic:kha\n244\n63.50\n55.50\n52.05\n44.05\n57.21\n49.12\nfez. meknes\narabic:fes\n196\n60.00\n59.50\n61.22\n56.40\n60.61\n57.91\ngilit mesopotamian arabic\narabic:bag\n203\n57.50\n47.50\n56.65\n49.48\n57.07\n48.47\ngulf arabic (a:doh)\narabic:doh\n205\n50.00\n43.50\n48.78\n45.55\n49.38\n44.50\ngulf arabic (a:jed)\narabic:jed\n196\n58.00\n46.00\n59.18\n40.89\n58.59\n43.29\ngulf arabic (a:mus)\narabic:mus\n178\n38.50\n49.50\n43.26\n42.67\n40.74\n45.83\ngulf arabic (a:riy)\narabic:riy\n311\n62.00\n56.50\n39.87\n37.92\n48.53\n45.38\nlevantine arabic (a:north-\ndam)\narabic:dam\n148\n37.50\n24.50\n50.68\n42.98\n43.10\n31.21\nlibyan arabic (a:ben)\narabic:ben\n238\n56.50\n47.50\n47.48\n52.78\n51.60\n50.00\nnorth mesopotamian ara-\nbic (a:bas)\narabic:bas\n186\n49.50\n39.00\n53.23\n49.68\n51.30\n43.70\nnorth mesopotamian ara-\nbic (a:mos)\narabic:mos\n188\n71.50\n70.00\n76.06\n69.31\n73.71\n69.65\nrabat-casablanca arabic\narabic:rab\n153\n50.00\n40.00\n65.36\n60.61\n56.66\n48.19\nsfax\narabic:sfx\n215\n62.50\n64.50\n58.14\n48.13\n60.24\n55.13\nsouth\nlevantine\narabic\n(a:south-amm)\narabic:amm\n177\n40.50\n35.00\n45.76\n35.53\n42.97\n35.26\nsouth\nlevantine\narabic\n(a:south-jer)\narabic:jer\n202\n48.50\n47.00\n48.02\n40.34\n48.26\n43.42\nsouth\nlevantine\narabic\n(a:south-sal)\narabic:sal\n167\n46.00\n66.50\n55.09\n59.11\n50.14\n62.59\nstandard arabic\narabic:msa\n244\n75.00\n98.00\n61.48\n95.61\n67.57\n96.79\nsunni beiruti arabic\narabic:bei\n192\n58.00\n52.50\n60.42\n68.18\n59.18\n59.32\ntripolitanian arabic\narabic:tri\n201\n66.00\n60.00\n65.67\n60.30\n65.84\n60.15\ntunisian arabic (a:tun)\narabic:tun\n164\n52.50\n37.00\n64.02\n56.49\n57.69\n44.71\ngreek\ncypriot greek (r:casual,\nm:written, i:other)\ngreek:cg_other\n81\n74.14\n84.48\n53.09\n56.32\n61.87\n67.59\ncypriot greek (r:casual,\nm:written, i:twitter)\ngreek:cg_twitter\n36\n51.11\n44.44\n63.89\n68.97\n56.79\n54.05\nmodern greek (r:casual,\nm:written, i:twitter)\ngreek:smg_twitter\n94\n89.83\n100.00\n56.38\n53.15\n69.28\n69.41\nhigh german\ncentral alemannic (a:be)\nswiss-\ndialects:be\n389\n72.89\n51.58\n71.21\n62.42\n72.04\n56.48\ncentral alemannic (a:bs)\nswiss-\ndialects:bs\n340\n73.50\n57.83\n75.88\n61.14\n74.67\n59.44\ncentral alemannic (a:lu)\nswiss-\ndialects:lu\n335\n72.91\n65.13\n75.52\n59.47\n74.19\n62.17\ncentral alemannic (a:zh)\nswiss-\ndialects:zh\n359\n78.84\n73.33\n75.77\n63.73\n77.27\n68.19\nsinitic\nmandarin\nchinese\n(a:mainland, o:simplified)\nmandarin_simplified:m 986\n97.90\n96.10\n99.29\n90.66\n98.59\n93.30\nmandarin\nchinese\n(a:mainland, o:traditional,\ni:synthetic)\nmandarin_traditional:m977\n96.80\n92.10\n99.08\n95.74\n97.93\n93.88\nmandarin\nchinese\n(a:taiwan, o:simplified)\nmandarin_simplified:t 1014\n99.30\n90.10\n97.93\n95.85\n98.61\n92.89\nmandarin\nchinese\n(a:taiwan,\no:traditional,\ni:synthetic)\nmandarin_traditional:t1023\n99.10\n95.90\n96.87\n92.39\n97.97\n94.11\nsouthwestern shifted romance\nbrazilian portuguese\nportuguese:pt-\nbr\n627\n96.94\n92.35\n90.91\n84.98\n93.83\n88.51\nlatin american spanish\nspanish:es-ar\n207\n81.06\n9.25\n88.89\n91.30\n84.79\n16.80\nportuguese (a:european)\nportuguese:pt-\npt\n349\n91.45\n83.64\n70.49\n63.92\n79.61\n72.46\nportuguese (m:written)\nportuguese:pt\n15\n9.70\n0.00\n86.67\n0.00\n17.45\n0.00\nspanish\nspanish:es\n290\n74.21\n74.53\n81.38\n47.69\n77.63\n58.16\nspanish (a:europe)\nspanish:es-es\n492\n90.99\n83.33\n82.11\n78.89\n86.32\n81.05\nTable 16: Dialect Identification evaluation using language cluster specific datasets. We finetune a classificaiton\nmodel using either mBERT or XLM-R and then evaluate on the test data.\ncluster\nvariety\ntarget-code\nsrc\nmBERT (acc)\nXLM-R (acc)\nmBERT (F1)\nXLM-R (F1)\narabic\nstandard arabic\narb_Arab\narb_Arab\n85.25\n83.96\n86.71\n82.27\nta’izzi-adeni arabic\nacq_Arab\narb_Arab\n84.96\n82.05\n86.44\n81.98\nnajdi arabic\nars_Arab\narb_Arab\n84.80\n84.39\n87.41\n83.33\nnorth mesopotamian arabic\nacm_Arab\narb_Arab\n82.97\n80.95\n84.77\n80.36\nsouth levantine arabic\najp_Arab\narb_Arab\n81.82\n80.16\n84.16\n79.05\nlevantine arabic (a:north)\napc_Arab\narb_Arab\n81.59\n80.15\n83.76\n79.88\negyptian arabic\narz_Arab\narb_Arab\n81.02\n76.38\n84.43\n81.03\ntunisian arabic\naeb_Arab\narb_Arab\n79.45\n72.88\n83.97\n77.33\nmoroccan arabic\nary_Arab\narb_Arab\n73.87\n79.14\n78.76\n78.55\ncommon turkic\nnorth azerbaijani\nazj_Latn\nazj_Latn\n80.46\n79.87\n82.00\n79.55\ncentral oghuz (m:spoken)\ntur_Latn\nazj_Latn\n79.10\n84.41\n80.61\n79.51\nsouth azerbaijani\nazb_Arab\nazj_Latn\n65.90\n67.08\n69.71\n68.37\ngallo-italian\nvenetian\nvec_Latn\nita_Latn\n76.72\n70.68\n75.07\n74.28\nlombard\nlmo_Latn\nita_Latn\n69.92\n59.90\n70.65\n64.56\nligurian\nlij_Latn\nlij_Latn\n66.81\n63.42\n74.03\n57.78\nhigh german\nluxemburgish\nltz_Latn\nnld_Latn\n74.74\n58.50\n77.86\n64.83\nlimburgan\nlim_Latn\nnld_Latn\n71.09\n65.83\n71.12\n65.73\nitalian romance\nitalian\nita_Latn\nita_Latn\n87.67\n84.92\n86.68\n85.83\nsicilian\nscn_Latn\nita_Latn\n75.22\n59.71\n72.70\n59.47\nkurdish\nnorthern kurdish\nkmr_Latn\nckb_Arab\n33.23\n68.21\n10.45\n5.71\ncentral kurdish\nckb_Arab\nckb_Arab\n13.10\n19.37\n16.86\n12.38\nlatvian\nlatvian\nlvs_Latn\nlvs_Latn\n76.35\n83.75\n80.63\n82.80\neast latvian\nltg_Latn\nlvs_Latn\n55.67\n65.02\n63.69\n67.42\nnorwegian\nnorwegian nynorsk (m:written)\nnno_Latn\nnob_Latn\n85.66\n79.94\n89.20\n79.06\nnorwegian bokmål (m:written)\nnob_Latn\nnob_Latn\n83.81\n82.90\n83.82\n84.14\nsinitic\nclassical-middle-modern\nsinitic (o:traditional)\nzho_Hant\nzho_Hans\n89.82\n86.80\n89.02\n86.39\ncantonese\nyue_Hant\nzho_Hans\n89.45\n86.46\n88.71\n87.64\nclassical-middle-modern\nsinitic (o:simplified)\nzho_Hans\nzho_Hans\n88.74\n86.38\n88.86\n89.15\nsotho-tswana (s.30)\nnorthern sotho\nnso_Latn\nnso_Latn\n35.62\n28.16\n34.86\n13.55\nsouthern sotho\nsot_Latn\nnso_Latn\n32.55\n32.31\n39.93\n19.08\nsouthwestern shifted romance\nportuguese (a:european)\npor_Latn\nspa_Latn\n88.13\n89.10\n88.10\n87.74\ngalician\nglg_Latn\nspa_Latn\n86.99\n89.00\n86.93\n87.83\nspanish\nspa_Latn\nspa_Latn\n86.74\n85.93\n84.87\n86.55\noccitan\noci_Latn\nlij_Latn\n84.12\n74.80\n78.53\n62.56\nTable 17: Topic classification evaluation using SIB-200 language data with dialectal presence. We report span F1\nas evaluation score. Zeroshot scores are evaluated using model finetuned on Standerd English whereas, we use\nin-group training for supervised finetuning\ndialect\nMBERT_Acc\nMBERT_F1\nXLMR_Acc\nXLMR_F1\nmistral7b\nlang-group\nvariety\narabic\ntunisian arabic\naeb_arab\n94.55\n94.56\n94.61\n94.62\n73.3\nalgerian arabic\narq_arab\n84.98\n85.00\n84.70\n84.69\n76.0\narabic (a:jordan)\njor_arab\n82.96\n82.90\n89.07\n89.00\n82.2\narabic (a:saudi-arabia)\nsau_arab\n81.38\n65.97\n83.40\n67.66\n79.8\ntunisian arabic (r:casual)\naeb_latn\n80.95\n65.65\n79.80\n63.70\n62.3\nstandard arabic\narb_arab\n80.63\n70.01\n83.96\n72.91\n65.7\nmoroccan arabic\nary_arab\n78.08\n61.50\n77.41\n55.55\n58.4\negyptian arabic\narz_arab\n67.03\n40.00\n69.03\n47.89\n50.0\nsouth levantine arabic\nar_lb\n58.38\n34.63\n58.90\n32.72\n0.0\nTable 18: Sentiment Analysis results. In addition to, using mBERT and XLM-R as the base models, we also perform\nin-context learning to evaluate the performance of large language models (Mistral-7B).\ndialect\nacc (mBERT)\nacc (XLM-R)\nmBERT (F1)\nXLM-R (F1)\nlang-group\nlanguage\nanglic\nenglish\neng_Latn\n52.22\n53.56\n51.97\n53.44\narabic\nstandard arabic\narb_Arab\n39.00\n43.78\n39.01\n43.78\nlevantine arabic (a:north)\napc_Arab\n38.89\n40.78\n38.64\n40.71\nnorth mesopotamian arabic\nacm_Arab\n38.11\n41.33\n37.99\n41.35\nmoroccan arabic\nary_Arab\n37.00\n37.67\n36.94\n37.61\negyptian arabic\narz_Arab\n36.22\n38.00\n36.21\n37.98\nnajdi arabic\nars_Arab\n36.00\n38.11\n36.05\n38.16\nsinitic\nclassical-middle-modern sinitic (o:simplified)\nzho_Hans\n50.11\n47.22\n49.79\n47.10\nclassical-middle-modern sinitic (o:traditional)\nzho_Hant\n47.00\n45.11\n46.88\n44.76\nsotho-tswana (s.30)\nnorthern sotho\nnso_Latn\n31.11\n29.78\n31.18\n29.72\nsouthern sotho\nsot_Latn\n28.56\n29.11\n28.52\n29.00\nTable 19: Multiple-choice machine reading comprehension evaluation using Belebele dataset languages with\ndialectal presence. We report span F1 as evaluation score. We use combined finetuning using the aggregated training\ndata provided with Belebele evaluationd data.\nLabel: <Label for Example k, Positive, negative, neutral>\nSentence: <Test Example Input>\nLabel:\nExtractive Question Answering.\nWe provide 2 few-shot examples i.e. k = 2 due to the long-form nature of text for\nthis task.\nThis task is about writing a correct answer for the reading comprehension task. Based on the information\nprovided in a given passage, you should identify the shortest continuous text span from the passage\nthat serves as an answer to the given question. Avoid answers that are incorrect or provides incomplete\njustification for the question.\nPassage: <Passage for Example 1>\nQuestion: <Question for Example 1>\nAnswer: <Answer for Example 1>\n· · ·\nPassage: <Passage for Example k>\nQuestion: <Question for Example k>\nAnswer: <Answer for Example k>\nPassage: <Passage for Test Example>\nQuestion: <Question for Test Example>\nAnswer:\nVarieties with Highest Performance\nVarieties with Lowest Performance\nTask (Dataset)\nDEP parsing (UD)\nanglic/english*\nitalian\nromance/italian\n(r:formal, m:written, i:essay)*\ntupi-guarani subgroup i.a/old\nguarani*\narabic/north african arabic\nalbanian/albanian*\nsouthwestern shifted romance/-\nportuguese (a:european)\nkomi/komi-zyrian\n(m:written)*\nitalian\nromance/continental\nsouthern italian\ngallo-rhaetian/french*\nnorwegian/norwegian nynorsk\n(m:written)*\nsaami/skolt saami*\nkomi/komi-zyrian\n(m:spoken)†\nnorwegian/norwegian bokmål\n(m:written)*\nsouthwestern shifted romance/-\nportuguese (i:mix)*\ntupi-guarani\nsubgroup\ni.a/mbyá guaraní (a:paraguay)\nkomi/komi-permyak†\nitalian romance/italian*\nsouthwestern\nshifted\nro-\nmance/brazilian portuguese*\ntupi-guarani\nsubgroup\ni.a/mbyá guaraní (a:brazil)*\nsaami/north saami*†\nEQA (SD-QA-test)\nanglic/english (a:scotland)\nanglic/irish english\nswahili/swahili (a:kenya)*\narabic/algerian arabic†\nanglic/southern african english\nanglic/philippine english\nbengali/vanga\n(a:west\nbengal)*†\narabic/tunisian arabic†\nanglic/new zealand english\nanglic/nigerian english\nbengali/vanga (a:dhaka)*†\narabic/moroccan arabic†\nanglic/australian english\nanglic/indian english (a:north)\nkorean/seoul (m:spoken)*†\narabic/egyptian arabic*†\nanglic/southeast american en-\nglish*\nanglic/kenyan english\nkorean/korean (a:south-eastern,\nm:spoken)*†\nswahili/swahili (a:tanzania)*\nTC (SIB-200)\nsinitic/classical-\nmiddle-modern\nsinitic\n(o:traditional)*†\nitalian romance/italian*\nlatvian/east latvian*\narabic/moroccan arabic†\nanglic/english*\nsouthwestern shifted romance/-\ngalician*\nsotho-tswana (s.30)/northern\nsotho*\nhigh german/limburgan\nsinitic/cantonese*†\nsouthwestern\nshifted\nro-\nmance/spanish*\nkurdish/northern kurdish†\ngallo-italian/lombard\nsinitic/classical-middle-\nmodern sinitic (o:simplified)*†\nnorwegian/norwegian nynorsk\n(m:written)*\nsotho-tswana (s.30)/southern\nsotho*\ngallo-italian/ligurian\nsouthwestern shifted romance/-\nportuguese (a:european)\narabic/standard arabic*†\nkurdish/central kurdish†\ncommon\nturkic/south\nazerbaijani†\nMRC (Belebele)\nanglic/english*\narabic/north\nmesopotamian\narabic†\narabic/moroccan arabic†\nsinitic/classical-middle-\nmodern sinitic (o:simplified)*†\nsinitic/classical-middle-\nmodern sinitic (o:simplified)*†\narabic/moroccan arabic†\narabic/egyptian arabic*†\nsinitic/classical-\nmiddle-modern\nsinitic\n(o:traditional)*†\nsinitic/classical-\nmiddle-modern\nsinitic\n(o:traditional)*†\narabic/egyptian arabic*†\narabic/najdi arabic†\narabic/standard arabic*†\narabic/standard arabic*†\narabic/najdi arabic†\nsotho-tswana (s.30)/northern\nsotho*\narabic/levantine\narabic\n(a:north)†\narabic/levantine\narabic\n(a:north)†\nsotho-tswana (s.30)/northern\nsotho*\nsotho-tswana (s.30)/southern\nsotho*\narabic/north\nmesopotamian\narabic†\nNER (Wikiann)\nanglic/english (o:controlled)*\nmodern dutch/dutch*\nsinitic/classical chinese†\nmari/western mari†\nnorwegian/norwegian nynorsk\n(m:written)*\nsouthwestern shifted romance/-\ngalician*\nsinitic/hakka chinese*†\ngallo-italian/emiliano-\nromagnolo\nnorwegian/norwegian\n(m:written, i:samnorsk)\nitalian romance/italian*\nsotho-tswana (s.30)/northern\nsotho*\ngreater\npanjabic/eastern\npanjabi*†\nnorwegian/norwegian bokmål\n(m:written)*\nhindustani/fiji hindi†\nkurdish/central kurdish†\ncommon\nturkic/south\nazerbaijani†\nanglic/english*\nfrisian/western frisian*†\nanglic/jamaican creole english\nkurdish/kurdish*†\nNLI (XNLI-translate-test)\nanglic/english*\nnorwegian/norwegian bokmål\n(m:written)*\ncommon\nturkic/south\nazerbaijani†\narabic/north\nmesopotamian\narabic†\nsouthwestern\nshifted\nro-\nmance/spanish*\nsinitic/classical-middle-\nmodern sinitic (o:simplified)*†\nkurdish/central kurdish†\narabic/levantine\narabic\n(a:north)†\nsouthwestern shifted romance/-\nportuguese (a:european)\nsouthwestern\nshifted\nro-\nmance/occitan\nsotho-tswana (s.30)/northern\nsotho*\nhigh german/limburgan\nitalian romance/italian*\nnorwegian/norwegian nynorsk\n(m:written)*\nsotho-tswana (s.30)/southern\nsotho*\narabic/tunisian arabic†\nsouthwestern shifted romance/-\ngalician*\narabic/standard arabic*†\nkurdish/northern kurdish†\nlatvian/east latvian*\nPOS tagging (UD)\nanglic/english*\ngallo-rhaetian/french (a:paris)\ntupi-guarani\nsubgroup\ni.a/mbyá guaraní (a:paraguay)\nsinitic/classical chinese†\nnorwegian/norwegian bokmål\n(m:written)*\nneva/finnish*\nkomi/komi-zyrian\n(m:written)*\narabic/levantine arabic†\nhigh german/german*\nitalian romance/italian*\nsaami/skolt saami*\nitalian\nromance/continental\nsouthern italian\nnorwegian/norwegian nynorsk\n(m:written)*\nneva/estonian*\ntupi-guarani subgroup i.a/old\nguarani*\nkomi/komi-permyak†\ngallo-rhaetian/french*\nsouthwestern shifted romance/-\nportuguese (a:european)\ntupi-guarani\nsubgroup\ni.a/mbyá guaraní (a:brazil)*\narabic/north african arabic\nwriting system (non-MT)\nLatin (77.2%)\nLatin (44.2%)\ndialect (non-MT)\nStandard (7%)\nStandard (4.5%)\nTable 21: Varieties with the highest and lowest performance (in terms of raw evaluation score) on various tasks in the\nzero-shot setting. On the left are the top 10. We find that most of these varieties are high-resource standard varieties,\nand only a few high-resource non-standard varieties. At right are the bottom 10, which are mainly low-resource,\nnon-standard varieties. We use the following notations for variety type and writing system quantification: * marks\nstandard varieties (for some clusters, here we consider multiple varieties as standard because of the substantial\nresource presence of both of the varieties; e.g. portuguese/european and portuguese/mix) and † notes mix and\nnon-Latin writing system.\nCluster\nHigh-resource variety\nScript\nLow-resource variety\nScript\nHindustani\nHindi\nDevanagari\nFiji hindi\nLatin\nGreater panjabic\nEastern panjabi\nDevanagari\nWestern panjabi\nArabic\nSotho-tswana (s.30)\nNorthern sotho\nLatin\nSouthern sotho\nLatin\nTable 22: NER cases where low-resource varieties perform better in zero-shot. The script plays a significant\nrole here as most of the cases, high-resource scripts such as Latin, and Arabic could uplift the performance of a\nlow-resource variety over a high-resource non-Latin script.\nTable 23: Language clusters with their linguistic utility, demographic utility, variety with minimum linguistic utility\nand standard deviation. Task: DEP Parsing\nclusters\nlinguistic\nutility\ndemographic\nutility\nvariety (minimum ling. u.)\nstandard deviation\nalbanian\n63.3\n69.1\n(gheg albanian, 43.5)\n28.0\nanglic\n75.5\n90.7\n(african american vernacular english, 52.5)\n20.4\narabic\n64.4\n83.3\n(south levantine arabic, 49.9)\n21.0\neastern-western armenian\n88.2\n87.5\n(eastern armenian, 87.0)\n1.6\ngallo-italian\n50.2\n50.2\n(ligurian, 50.2)\n-\ngallo-rhaetian\n87.7\n93.4\n(french (a:paris), 77.5)\n8.8\nhigh german\n56.6\n76.5\n(central alemannic (a:zh), 36.8)\n28.1\nitalian romance\n75.4\n76.5\n(continental southern italian, 50.0)\n17.3\nkomi\n30.2\n30.1\n(komi-zyrian (m:written), 27.6)\n2.4\nnorwegian\n88.4\n-\n(norwegian nynorsk (m:written, i:old), 78.4)\n8.6\nsaami\n47.9\n67.0\n(skolt saami, 28.3)\n27.8\nsabellic\n33.2\n-\n(umbrian, 33.2)\n-\nsinitic\n65.4\n-\n(classical chinese, 46.7)\n20.8\nsouthwestern shifted romance\n87.2\n91.6\n(portuguese (a:european), 77.3)\n7.9\ntupi-guarani subgroup i.a\n17.2\n22.0\n(mbyá guaraní (a:brazil), 9.0)\n10.5\nwest low german\n40.8\n40.8\n(west low german, 40.8)\n-\nTable 24: Language clusters with their linguistic utility, demographic utility, variety with minimum linguistic utility\nand standard deviation. Task: POS Tagging\nclusters\nlinguistic utility\ndemographic utility\nvariety (minimum ling. u.)\nstandard deviation\nalbanian\n70.1\n74.3\n(gheg albanian, 55.8)\n20.2\nanglic\n92.6\n97.1\n(singlish, 88.0)\n6.5\narabic\n61.9\n81.4\n(levantine arabic, 43.8)\n18.9\neastern-western\narme-\nnian\n93.8\n93.0\n(eastern armenian, 92.4)\n2.0\ngallo-italian\n58.9\n58.9\n(ligurian, 58.9)\n-\ngallo-rhaetian\n96.4\n96.9\n(old french (842-ca.\n1400),\n95.6)\n0.7\nhigh german\n75.5\n88.4\n(central alemannic (a:zh), 62.6)\n18.2\nitalian romance\n82.6\n80.9\n(continental southern italian,\n57.1)\n17.9\nkomi\n41.8\n41.6\n(komi-zyrian (m:written), 35.1)\n6.0\nneva\n75.4\n97.7\n(neva (a:south-west trans), 61.7)\n13.9\nnorwegian\n97.6\n-\n(norwegian nynorsk (m:written,\ni:old), 95.7)\n1.6\nsaami\n56.5\n78.3\n(skolt saami, 34.1)\n31.6\nsabellic\n11.9\n-\n(umbrian, 11.9)\n-\nsinitic\n84.5\n-\n(classical-middle-modern sinitic\n(a:hongkong,\no:traditional),\n69.0)\n13.7\nsouthwestern shifted ro-\nmance\n90.5\n95.7\n(occitan, 76.8)\n10.5\ntupi-guarani subgroup\ni.a\n13.7\n18.5\n(mbyá guaraní (a:brazil), 1.9)\n13.7\nwest low german\n69.7\n69.7\n(west low german, 69.7)\n-\nTable 25: Language clusters with their linguistic utility, demographic utility, variety with minimum linguistic utility\nand standard deviation. Task: NER\nclusters\nlinguistic utility\ndemographic utility\nvariety (minimum ling. u.)\nstandard deviation\nanglic\n57.3\n83.9\n(jamaican creole english, 0.0)\n40.9\narabic\n81.2\n86.0\n(egyptian arabic, 73.3)\n11.1\ncircassian\n57.4\n54.1\n(kabardian, 47.5)\n14.0\ncommon turkic\n67.9\n85.1\n(south azerbaijani, 31.7)\n28.8\neastern romance\n75.9\n79.3\n(moldavian, 64.6)\n16.0\nfrisian\n66.8\n80.4\n(northern frisian, 54.8)\n13.3\ngallo-italian\n68.6\n68.0\n(emiliano-romagnolo, 45.5)\n17.3\ngallo-rhaetian\n71.8\n90.3\n(walloon, 46.3)\n16.5\ngreater panjabic\n54.9\n53.2\n(eastern panjabi, 45.2)\n13.6\ngreek\n81.9\n90.1\n(pontic, 72.6)\n13.2\nhigh german\n71.5\n88.0\n(pennsylvania german, 41.8)\n15.9\nhindustani\n86.6\n88.1\n(fiji hindi, 85.2)\n2.1\ninuit\n69.7\n63.5\n(kalaallisut, 63.2)\n9.2\nitalian romance\n73.0\n88.4\n(continental southern italian, 61.5)\n13.6\nkomi\n55.2\n56.6\n(komi-permyak, 53.6)\n2.2\nkurdish\n47.5\n54.8\n(central kurdish, 35.5)\n17.0\nlatvian\n71.7\n88.7\n(east latvian, 50.1)\n30.5\nmari\n52.6\n47.4\n(eastern mari, 46.7)\n8.3\nmodern dutch\n83.8\n91.2\n(zeeuws, 79.5)\n7.1\nnorwegian\n89.7\n-\n(norwegian nynorsk (m:written), 87.9)\n1.5\nsardo-corsican\n75.9\n79.8\n(corsican, 70.6)\n7.5\nserbian-croatian-\nbosnian\n83.1\n81.0\n(serbian standard, 65.9)\n11.9\nsinitic\n64.3\n78.3\n(hakka chinese, 40.0)\n16.8\nsorbian\n68.3\n67.4\n(upper sorbian, 65.5)\n3.9\nsotho-tswana (s.30)\n49.5\n41.1\n(northern sotho, 29.7)\n28.0\nsouthwestern\nshifted romance\n74.0\n92.1\n(mirandese, 49.3)\n18.1\nwest low german\n80.3\n80.3\n(west low german, 80.3)\n-\nTable 26: Language clusters with their linguistic utility, demographic utility, variety with minimum linguistic utility\nand standard deviation. Task: NLI\nclusters\nlinguistic utility\ndemographic utility\nvariety (minimum ling. u.)\nstandard deviation\nanglic\n83.4\n83.4\n(english, 83.4)\n-\narabic\n63.3\n70.0\n(tunisian arabic, 50.2)\n7.1\ncommon turkic\n64.1\n62.3\n(south azerbaijani, 44.6)\n16.9\ngallo-italian\n61.7\n63.6\n(ligurian, 57.2)\n6.0\ngallo-rhaetian\n54.6\n54.6\n(friulian, 54.6)\n-\nhigh german\n59.9\n59.8\n(limburgan, 59.7)\n0.2\nitalian romance\n70.4\n77.1\n(sicilian, 62.7)\n11.0\nkurdish\n51.4\n55.5\n(central kurdish, 39.6)\n16.7\nlatvian\n63.6\n71.5\n(east latvian, 53.5)\n14.2\nmodern dutch\n76.5\n76.5\n(dutch, 76.5)\n-\nnorwegian\n75.3\n-\n(norwegian nynorsk (m:written),\n71.1)\n6.0\nsardo-corsican\n58.3\n58.3\n(sardinian, 58.3)\n-\nsinitic\n68.2\n67.4\n(classical-middle-modern sinitic\n(o:traditional), 64.5)\n4.1\nsotho-tswana (s.30)\n35.3\n35.6\n(southern sotho, 34.6)\n1.0\nsouthwestern shifted ro-\nmance\n76.3\n79.1\n(occitan, 68.5)\n5.2\nTable 27: Language clusters with their linguistic utility, demographic utility, variety with minimum linguistic utility\nand standard deviation. Task: TC\nclusters\nlinguistic utility\ndemographic utility\nvariety (minimum ling. u.)\nstandard deviation\nanglic\n89.7\n89.7\n(english, 89.7)\n-\narabic\n84.5\n85.7\n(moroccan arabic, 79.1)\n2.4\ncommon turkic\n78.7\n77.3\n(south azerbaijani, 69.7)\n7.9\ngallo-italian\n73.8\n73.7\n(lombard, 70.6)\n3.0\ngallo-rhaetian\n68.8\n68.8\n(friulian, 68.8)\n-\nhigh german\n74.5\n72.7\n(limburgan, 71.1)\n4.8\nitalian romance\n81.4\n86.8\n(sicilian, 75.2)\n8.8\nkurdish\n43.8\n52.3\n(central kurdish, 19.4)\n34.5\nlatvian\n75.6\n82.0\n(east latvian, 67.4)\n11.5\nmodern dutch\n89.6\n89.6\n(dutch, 89.6)\n-\nnorwegian\n86.7\n-\n(norwegian bokmål (m:written),\n84.1)\n3.6\nsardo-corsican\n71.0\n71.0\n(sardinian, 71.0)\n-\nsinitic\n89.5\n89.5\n(classical-middle-modern sinitic\n(o:simplified), 89.2)\n0.3\nsotho-tswana (s.30)\n37.8\n36.9\n(northern sotho, 35.6)\n3.1\nsouthwestern shifted romance\n87.2\n86.9\n(occitan, 84.1)\n2.3\nTable 28: Language clusters with their linguistic utility, demographic utility, variety with minimum linguistic utility\nand standard deviation. Task: DId\nclusters\nlinguistic utility\ndemographic utility\nvariety (minimum ling. u.)\nstandard deviation\nanglic\n89.0\n88.4\n(north american english, 88.0)\n1.4\narabic\n58.1\n89.0\n(south levantine arabic (a:south-\namm), 43.0)\n11.3\ngreek\n64.6\n-\n(cypriot\ngreek\n(r:casual,\nm:written, i:twitter), 56.8)\n6.8\nhigh german\n74.5\n-\n(central alemannic (a:be), 72.0)\n2.1\nsinitic\n98.3\n98.3\n(mandarin chinese (a:mainland,\no:traditional, i:synthetic), 97.9)\n0.4\nsouthwestern shifted romance\n73.3\n82.7\n(portuguese (m:written), 17.4)\n27.9\nTable 29: Language clusters with their linguistic utility, demographic utility, variety with minimum linguistic utility\nand standard deviation. Task: SA\nclusters\nlinguistic utility\ndemographic utility\nvariety (minimum ling. u.)\nstandard deviation\narabic\n80.3\n81.4\n(levantine/south, 58.9)\n10.7\nTable 30: Language clusters with their linguistic utility, demographic utility, variety with minimum linguistic utility\nand standard deviation. Task: MRC\nclusters\nlinguistic utility\ndemographic utility\nvariety (minimum ling. u.)\nstandard deviation\nanglic\n53.4\n53.4\n(english, 53.4)\n-\narabic\n39.9\n42.1\n(moroccan arabic, 37.6)\n2.4\nsinitic\n48.3\n-\n(classical-middle-modern sinitic\n(o:traditional), 46.9)\n2.1\nsotho-tswana (s.30)\n30.1\n30.5\n(southern sotho, 29.0)\n1.5\nTable 31: Language clusters with their linguistic utility, demographic utility, variety with minimum linguistic utility\nand standard deviation. Task: EQA\nclusters\nlinguistic utility\ndemographic utility\nvariety (minimum ling. u.)\nstandard deviation\nanglic\n75.2\n75.4\n(indian english (a:south), 71.9)\n1.8\narabic\n77.2\n77.0\n(egyptian arabic, 76.5)\n0.6\nbengali\n73.8\n-\n(vanga (a:west bengal), 73.3)\n0.7\nkorean\n65.2\n64.6\n(korean (a:south-eastern, m:spoken), 64.6)\n0.8\nswahili\n67.9\n64.1\n(swahili (a:tanzania), 63.5)\n6.2\n",
  "categories": [
    "cs.CL",
    "cs.AI"
  ],
  "published": "2024-03-16",
  "updated": "2024-07-07"
}