{
  "id": "http://arxiv.org/abs/2403.11266v1",
  "title": "A Dynamically Weighted Loss Function for Unsupervised Image Segmentation",
  "authors": [
    "Boujemaa Guermazi",
    "Riadh Ksantini",
    "Naimul Khan"
  ],
  "abstract": "Image segmentation is the foundation of several computer vision tasks, where\npixel-wise knowledge is a prerequisite for achieving the desired target. Deep\nlearning has shown promising performance in supervised image segmentation.\nHowever, supervised segmentation algorithms require a massive amount of data\nannotated at a pixel level, thus limiting their applicability and scalability.\nTherefore, there is a need to invest in unsupervised learning for segmentation.\nThis work presents an improved version of an unsupervised Convolutional Neural\nNetwork (CNN) based algorithm that uses a constant weight factor to balance\nbetween the segmentation criteria of feature similarity and spatial continuity,\nand it requires continuous manual adjustment of parameters depending on the\ndegree of detail in the image and the dataset. In contrast, we propose a novel\ndynamic weighting scheme that leads to a flexible update of the parameters and\nan automatic tuning of the balancing weight between the two criteria above to\nbring out the details in the images in a genuinely unsupervised manner. We\npresent quantitative and qualitative results on four datasets, which show that\nthe proposed scheme outperforms the current unsupervised segmentation\napproaches without requiring manual adjustment.",
  "text": "A Dynamically Weighted Loss Function for\nUnsupervised Image Segmentation\n1st Boujemaa Guermazi\nElectrical, Computer, and Biomedical Engineering\nToronto Metropolitan University\nToronto, Ontario\nbguermazi@ryerson.ca\n2nd Riadh Ksantini\nComputer Science\nUniversity of Bahrain\nZallaq, Bahrain\nrksantini@uob.edu.bh\n3rd Naimul Khan\nElectrical, Computer, and Biomedical Engineering\nToronto Metropolitan University\nToronto, Ontario\nn77khan@ryerson.ca\nAbstract—Image segmentation is the foundation of several\ncomputer vision tasks, where pixel-wise knowledge is a pre-\nrequisite for achieving the desired target. Deep learning has\nshown promising performance in supervised image segmentation.\nHowever, supervised segmentation algorithms require a massive\namount of data annotated at a pixel level, thus limiting their\napplicability and scalability. Therefore, there is a need to invest\nin unsupervised learning for segmentation. This work presents\nan improved version of an unsupervised Convolutional Neural\nNetwork (CNN) based algorithm that uses a constant weight\nfactor to balance between the segmentation criteria of feature\nsimilarity and spatial continuity, and it requires continuous\nmanual adjustment of parameters depending on the degree of\ndetail in the image and the dataset. In contrast, we propose a\nnovel dynamic weighting scheme that leads to a flexible update\nof the parameters and an automatic tuning of the balancing\nweight between the two criteria above to bring out the details\nin the images in a genuinely unsupervised manner. We present\nquantitative and qualitative results on four datasets, which show\nthat the proposed scheme outperforms the current unsupervised\nsegmentation approaches without requiring manual adjustment.\nIndex Terms—Image segmentation, unsupervised learning\nI. INTRODUCTION\nImage segmentation is fundamental to many application\ndomains such as medical imaging, surveillance, self-driving\ncars, and sports. Image classification allocates a category label\nto the entire image. In contrast, image segmentation generates\na category label for each input image pixel, dividing a whole\npicture into subgroups known as image segments. Although\nwe have made notable progress, the segmentation process is\nstill challenging due to various factors such as illumination\nvariation, occlusion, and background clutters.\nClassical pioneering segmentation techniques such as active\ncontour models (ACM) [1], k-means [2], and graph-based\nsegmentation method (GS) [3] impose global and local data\nand geometry constraints on the masks. As a result, these tech-\nniques become sensitive to initialization and require heuristics\nsuch as point resampling, making them unsuitable for modern\napplications.\nThis work was funded by the Natural Sciences and Engineering Research\nCouncil of Canada.\nCode\nfor\nthis\nproject\nis\navailable\nat:\nhttps://github.com/bijou-\nbijou/DynamicSeg\nThe current state-of-the-art models are deep learning meth-\nods based on convolutional neural networks (CNNs), espe-\ncially the Mask R-CNN framework [4], which has been\nsuccessfully put into practice for supervised semantic and\ninstance image segmentation. Even so, such methods require\na considerable amount of hand-labeled data, limiting their\napplicability in many areas. The problem becomes much more\nacute when it comes to pixel-wise classification, where the\nannotation cost per image is expensive. A possible solution to\nthis problem is unsupervised image segmentation, where the\nimage is automatically segmented into semantically similar\nregions. The task has been studied as a clustering problem\nin recent literature, reaching promising results. The Differen-\ntiable Feature Clustering [5] is a state-of-the-art CNN-based\nclustering algorithm that simultaneously optimizes the pixel\nlabels and feature representations [5]. It applies a combina-\ntion of feature similarity and spatial continuity constraints\nto backpropagate the model’s parameters. Feature similarity\ncorresponds to the constraint that pixels in the same cluster\nshould be similar to each other. Spatial continuity refers to\nthe constraint that pixels in the same cluster should be next\nto each other (continuous). However, to reach the desired\nsegmentation result, [5] applies a manual parameter tuning to\nfind the optimal balancing weight µ, which fails to achieve a\ngood balance between the two aforementioned constraints de-\npending on the degree of details in the image and dataset. This\nwork introduces a novel dynamic weighting scheme that leads\nto a flexible update of the parameters and an automatic tuning\nof the balancing weight µ. We achieve this by conditioning the\nvalue of µ to the number of predicted clusters and iteration\nnumber. We dynamically prioritize one of the constraints at\neach iteration to achieve a good balance. Experimental results\non four benchmark datasets show that our method achieves\nbetter quantitative metrics and qualitative segmentation results\nby striking a better balance between feature similarity and\nspatial continuity.\nII. RELATED WORKS\nMachine learning methods directly address the image seg-\nmentation problem by considering various features found in\nthe image, such as colour or pixel information. K-means\narXiv:2403.11266v1  [eess.IV]  17 Mar 2024\nFig. 1. The CNN Framework: a forward-backward process which is iterated T times to obtain the final prediction of the cluster labels cn.\nclustering [2], for instance, is a region-based method that\ndivides an image into K groups based on the discontinuity\nproperties of the extracted features. K-means is widely used\nfor unsupervised segmentation. However, the hard cluster-\ning that assumes sharp boundaries between clusters in K-\nmeans does not guarantee continuous areas. The graph-based\nsegmentation (GS) [3] is another region-based method that\nperforms segmentation based on pixel similarity, distance,\nor colour weights. As a result, GS preserves details in low\nvariability image regions while ignoring details in high vari-\nability regions, plus GS has a complex computation. On the\nother hand, The Invariant Information Clustering [6] is an\nedge-based method and is easy to implement and rigorously\ngrounded in information theory. The deep net IIC directly\ntrains a randomly initialized neural network into semantic\nclusters without the need for postprocessing to cluster the high-\ndimensional representations.\nMajority of the recent unsupervised segmentation models\nutilize deep learning. X. Xia et al. [7] tackles the problem\nof unsupervised image segmentation inspired by the concept\nof U-net architecture [8]. They join two U-net structures\ninto an auto-encoder followed by a post-processing phase to\nrefine its prime segmentation. The model merges segments\nusing a Hierarchical Segmentation method to form the final\nimage segments. It is a computationally expensive process\nand requires extensive hyperparameter tuning. L. Zhou et al.\nproposed another neural network-based algorithm; the Deep\nImage Clustering (DIC) [9]. DIC divided the image segmen-\ntation problem into two steps; A feature transformation sub-\nnetwork (FTS) to extract the features first, then a trainable\ndeep clustering sub-network (DCS) that groups the pixels\nto split the image into non-overlapping regions. The DIC\nmodel has proven to be less sensitive to varying segmentation\nparameters and has lower computation costs. However, it uses\nsuperpixels to optimize the model parameters, which results\nin pre-determined fixed boundaries for segmentation regions.\nIn contrast to the approaches mentioned above that use\nintermediate representations followed by post-processing, W.\nKim et al. propose utilizing a clustering algorithm that jointly\noptimizes the pixel labels and feature representations and up-\ndates their parameters using backpropagation [5]. Furthermore,\nin contrast to the superpixel-based refinement process [10],\nthe authors present a novel spatial continuity loss function\nto achieve dynamic segmentation boundaries as opposed to\nutilizing superpixels. It is a simple process that uses the\nbackpropagation of the feature similarity loss and a new\nspatial continuity loss that resolves the problem caused by\nsuperpixels in [10]. However, the weight coefficient for the\nloss function used must be adjusted each time according to\nthe datasets and the degree of detail of the image. Instead, we\npropose a dynamic method for adjusting the weighting factor\nautomatically, as described in the following sections.\nIII. PROPOSED METHOD\nA. The CNN framework\nFor a fair comparison, we use the same network architecture\nand training framework used in [5]. The model is shown in\nFigure (1). M convolutional components are used to produce\na p-dimensional feature map r. The CNN subnetwork consists\nof 2D Conv layers, Relu functions, batch normalizations, and\na final linear layer classifier that classifies the features of each\npixel into q′ classes. A batch normalization function is applied\nto the response map r to get a normalized map r′. Lastly,\nthe argmax function is used to select the dimension that has\nthe maximum value in r′\nn. Each pixel is assigned the corre-\nsponding cluster label cn, which is identical to allocating each\npixel to the closest point among the q′ representative points.\nDuring the backward propagation, first, the loss L (defined in\nthe next section) is calculated. Then, the convolutional filters’\nparameters and the classifier’s parameters are updated with\nstochastic gradient descent. This forward-backward process\nis iterated T times to obtain the final prediction of the\ncluster labels cn. The segmentation problem is handled in an\nunsupervised manner without knowing the exact number of\nclusters. The latter must be flexible according to the content\nof the image. Therefore, we must allocate a large number q\nto the initial cluster labels q’. After that, similar or spatially\nrelated pixels are iteratively integrated to update the number\nof clusters q′.\nB. Proposed dynamic weighting scheme\nThe loss function in [5] is designed to strike a balance\nbetween feature similarity and spatial continuity. The loss\nfunction L from [5] is shown in Equation (1).\nL = Lsim({r′\nn, cn}) + µLcon({r′\nn}),\n(1)\nwhere, µ: Weight for balancing; Lsim : feature similarity;\nLcon: spatial continuity; cn: cluster labels ; r′\nn: normalized\nresponse.\nThe loss function in equation (1) consists of two parts. The\nfirst part is the feature similarity loss Lsim which is the cross-\nentropy loss between the normalized response map r′\nn and the\ncluster labels cn. Minimizing this loss would reveal network\nweights that ease the extraction of more accurate attributes\nfor segmentation. Thus, Lsim ensures that pixels of similar\nfeatures should be assigned the same label. The second part\nis the Manhattan Distance L1 Norm of horizontal and vertical\ndifferences of the response map r′\nn as a spatial continuity\nloss which redresses the deficiency caused by superpixels\n[10]. This additional loss component Lcon has proven to be\nefficient in removing an excessive number of labels due to\ntheir complex patterns or textures and ensuring that continuous\npixels are assigned the same label.\nWhile the loss L as mentioned above can result in reason-\nably accurate unsupervised segmentation results as reported in\n[5], the segmentation results are susceptible to the balancing\nparameter µ. Figure (2) show examples of the sensitivity to\nthis parameter on an example image from the BSD500 dataset.\nAs can be seen, for µ = 50 and µ = 100, the segmentation\nis coarse, resulting in sky, buildings, and coastal regions.\nHowever, the image is further segmented with µ = 1 and\nµ = 5, where buildings are further segmented into glass\nbuildings, concrete buildings, and different floors. Although\nthe authors argue that the value of µ is proportional to the\ncoarseness of segmentation, We see that the results are not\nconsistent, e.g. the segmentation for µ = 50 appears coarser\nthan µ = 100.\nThis poses a problem in practice. Such high sensitivity to\nthe parameter means that for each dataset, this parameter has\nto be tuned extensively to obtain a result that is semantically\nmore meaningful.\nIn this work, we propose changing the weighting param-\neter’s value during training dynamically. Our observation is\nthat we can prioritize feature similarity during training at the\nearlier iterations and gradually shift focus to spatial continuity\n(or vice versa). We suggest a new dynamic loss function\nthat includes a continuous variable µ. The weight µ depends\ndirectly on the number of predicted clusters and iterations. We\nexamine two versions of the proposed weighting scheme:\n• Start by gathering continuous regions and shifting focus\nto feature similarity later. We call this approach Feature\nSimilarity Focus (FSF). In this case, the performed trials\nlead to a linear function of the number of clusters (q’)\nfor the new dynamic balancing weight µ′ = (q′/µ)\nas shown in equation(2). We tried other versions that\nFig. 2. Results for different µ values on a sample image from the BSD500\ndataset using the approach in [5].\nvary exponentially with the value of q′; However, such\nfunctions resulted in a rapid change in the value of µ′,\nwhich was not conducive to the balance we sought to\nachieve between the two constraints.\nLF SF = Lsim({r′\nn, cn}) + (q′/µ)Lcon({r′\nn})\n(2)\n• Start by prioritizing feature similarity criteria early in\ntraining and end with a higher spatial continuity weight.\nWe call this approach Spatial Continuity Focus (SCF). In\nthis case, the proposed dynamic weight would be the mul-\ntiplicative inverse of the number of clusters µ′ = (µ/q′)\nas shown in equation (3). Similar to FSF, we tried an\nexponential form, but the decay was too fast for it to be\neffective.\nLSCF = Lsim({r′\nn, cn}) + (µ/q′)Lcon({r′\nn})\n(3)\nIV. EXPERIMENTAL RESULTS\nA. Experiment Setup\nThe objective of the experiments is to show that our\nproposed dynamic weighting approach provides us with a\nmore semantically meaningful segmentation. We replicate the\nexperiments performed in [5]. For all the tests, we fixed the\nnumber of components in the feature extraction phase M to\n3. In addition, we set the dimension of the features space p\nequal to the dimension of cluster space q equal to 100. Finally,\nwe report the mean Intersection Over Union mIOU overall\nimages for the benchmark datasets. Ground truth is only used\nduring the assessment phase and has no bearing on the training\nprocess.\nBerkley Segmentation Dataset BSD500 [11] and PASCAL\nVisual Object Classes 2012 [12] are used to evaluate the\nsegmentation results quantitatively and qualitatively. BSD500\nconsists of 500 color and grayscale natural images. Following\nthe experimental setup in [5], we used the 200 color images\nof the BSD500 test-set to evaluate all the models. Since\nTABLE I\nCOMPARISON OF mIOU FOR UNSUPERVISED SEGMENTATION ON BSD500 AND PASCAL VOC2012. BEST SCORES ARE IN BOLD.\ndataset\nMethod\nBSD500 All\nBSD500 Fine\nBSD500 Coarse\nBSD500 Mean\nPASCAL VOC2012\nIIC [6]\n0.172\n0.151\n0.207\n0.177\n0.201\nk-means clustering\n0.240\n0.221\n0.265\n0.242\n0.238\nGraph-based Segmentation [3]\n0.313\n0.295\n0.325\n0.311\n0.286\nCNN-based + superpixels [10]\n0.226\n0.169\n0.324\n0.240\n***\nCNN-based + weighted loss, µ = 5 [5]\n0.305\n0.259\n0.374\n0.313\n0.288*\nSpatial Continuity Focus µ′ = 100/q′\n0.329\n0.288\n0.406\n0.341\n0.289\nSpatial Continuity Focus µ′ = 50/q′\n0.330\n0.290\n0.407\n0.342\n0.290\nFeature Similarity Focus µ′ = q′/10\n0.330\n0.297\n0.390\n0.339\n0.280\nFeature Similarity Focus µ′ = q′/15\n0.349\n0.307\n0.420\n0.359\n0.275\n* All the results are copied from [5], except for the results on Pascal VOC 2012, as there is no indication in [5] which images from the\ndataset were used for evaluation. Therefore, we chose 150 random images and re-produced the results for this dataset only.\nFig. 3. Qualitative Results on select BSD500 and PASCAL VOC2012 images. Same color corresponds to the pixels being assigned the same clustering label\nby the algorithm. Please read Section IV-C for discussion on these results.\nthe BSD500 dataset contains multiple types of ground truth,\nwe set three types of mIOU counting to assess the given\nresults; ”BSD500 All” takes into account all the ground truth\nfiles, ”BSD500 Fine” considers the only ground truth file per\nimage that has the most significant number of segments, and\n”BSD500 Coarse” takes only the ground truth file that contains\nthe smallest number of segments. We defined ”BSD500 Mean”\nas the average value of the above three measurements. For\nPASCAL VOC2012, we considered each segment an individ-\nual entity ignoring the object classification. VOC2012 is a\nlarge dataset containing 17,124 images with 2,913 images with\nsemantic segmentation annotations. We randomly chose 150\nof the semantic segmentation images to evaluate our method.\nFor the Icoseg [13] and Pixabay [14] datasets, select images\nare used to demonstrate additional qualitative results only. All\nof the aforementioned experimental settings are identical to\nthe settings from [5] for a fair comparison.\nB. Quantitative results\nTo demonstrate the effectiveness of the dynamic loss func-\ntion, we compare the results with The Invariant Information\nClustering (IIC) [6], the k-means clustering [2] and the graph-\nbased segmentation (GS) [3] in addition to the two CNN-\nbased methods in [10] and [5]. One important note is that\nwe report the same results as in [5], except for the results\nof the method [5] on Pascal VOC 2012, as there is no clue\nabout how the images were selected. In addition, the method\nFig. 4.\nQualitative Results on select Icoseg and Pixabay images. Same color corresponds to the pixels being assigned the same clustering label by the\nalgorithm. Please read Section IV-C for discussion on these results.\nis sensitive to the random initialization of the neural network\nweights, resulting in different mIOU scores every time we re-\nrun the experiment. Therefore, we could not exactly reproduce\nthe results reported in the paper using their provided code base.\nFor a fair comparison, we fixed the initialization to a single\nset of values for all methods, including ours.\nThe Differentiable Feature Clustering [5] uses a fixed value\nof µ. Yet five is the ideal value of µ for BSD500 [11], and\nPASCAL VOC 2012 datasets [12], as that achieved the best\nresults as reported in [5]. In contrast, for our proposed SCF\nand FSF methods, the old µ is still the only hyper-parameter\nthat needs to be tuned, but it is integrated into a dynamic loss\nfunction, as shown in equations (3) and (2). The best µ for\nSCF and FSF clustering were experimentally determined from\n{25, 45, 50, 55, 60, 75 100, 200} and {2, 10, 15, 25,50, 100},\nrespectively. We report results for µ = 10 and µ = 15 for FSF\nclustering ; µ = 100 and µ = 50 for SCF clustering.\nAs illustrated in Table I, the graph-based segmentation (GS)\nmethod outperformed the Differentiable Feature Clustering [5]\non “BSD500 all” and “BSD500 fine”. However, our proposed\nmethod outperforms [10], IIC, k-means, and, in particular, [5]\nand GS for both datasets obtaining the best mIOU scores.\nWhile FSF shows the best performance for the BSD500\ndataset, SCF achieves the best scores in the Pascal VOC2012\ndataset and the second-best score in the BSD500 dataset.\nC. Qualitative results\nWe also provide qualitative results on a few images as done\nin [5]. As shown in Figure (3), our model is more effective\nin bringing out segmentation regions that are semantically\nrelated. For example, For the ”Show Jumping” image (column\n5), the horse and the obstacle are classified as the same class\nby [5] (both yellow). However, for both FSF and SCF, the\nhorse and the obstacle are appropriately distinguished. For the\n”ship” image (column 2), [5] fails to differentiate between the\nsky and the body of the ship (both red), but both proposed\nSCF and FSF can do it successfully.\nFurther qualitative results are shown for select Icoseg [13]\nand Pixabay [14] datasets that were also used in [5]. These re-\nsults can be seen in Figure (4). The qualitative results on these\ndatasets are presented to further demonstrate that our proposed\napproaches do not require as much parameter tuning as [5]\ndoes. Figure (4) highlights that the baseline Differentiable\nFeature Clustering [5] is quite parameter sensitive. For each\ndataset, the weighting balance µ must be tuned extensively to\nobtain a more semantically meaningful result. For instance,\nPASCAL VOC 2012 and BSD500 datasets require a small\nbalancing value µ = 5. While, Icoseg [13] and Pixabay\n[14] datasets need a much larger balance value µ = 50 and\nµ = 100, respectively. On the other hand, As illustrated in\nFigures (3) and (4), our proposed method has proven effective\nin dealing with different datasets using the same weight for\nboth FSF and SCF. The proposed methods also bring out\ndetails in an unsupervised manner that is semantically more\nmeaningful. For example, using the Feature Similarity Focus\nmethod (FSF), the red car from the iCoseg dataset (column\n2 row 4 in Figure (4)) displays more detail on the tires and\nmore precise building outlines than the details extracted by\n[5] (column 2 row 2), where the car is partially blended into\nthe road. Similarly, for the peppers image (column 4 in Figure\n(4)), [5] was unable to identify the shapes of the individual\npeppers accurately. Both of our proposed method do a much\nbetter job, even with the same value of µ as the other images.\nComparing the proposed SCF and FSF, SCF shows potency\nin class segmentation, while FSF performs better in object\nsegmentation tasks. For instance, in the third row in Figure (3),\nthe SCF method segmented the image into the sky, dust, and\nmonuments. However, the FSF segmented the image into the\nsky, dust, pyramid, and Sphinx. Depending on the application\ncontext, one of these approaches could be more favorable.\nV. CONCLUSION\nIn this paper, we propose a dynamic weighting scheme for\nCNN-based unsupervised segmentation. Our proposed FSF\nand SCF approaches can strike a balance between feature\nsimilarity and spatial continuity for better segmentation. Qual-\nitative and quantitative results on four datasets show that\nour proposed approach outperforms classical methods for\nunsupervised image segmentation such as k-means clustering\nand a graph-based segmentation method, and outperforms the\nstate-of-the-art deep learning model, the Differentiable Feature\nClustering, [5] while producing semantically more meaningful\nresults.\nREFERENCES\n[1] Michael Kass, Andrew Witkin, and Demetri Terzopoulos,\n“Snakes:\nActive contour models,” International Journal of Computer Vision, vol.\n1, no. 4, pp. 321–331, jan 1988.\n[2] J.MacQueen et all, “Some methods for classification and analysis of\nmultivariate observations,” Proceedings of the fifth Berkeley symposium\non mathematical statistics and probability, vol. 1, no. 14, pp. 281–297,\n1967.\n[3] Pedro F. Felzenszwalb and Daniel P. Huttenlocher, “Efficient Graph-\nBased Image Segmentation,” International Journal of Computer Vision,\nvol. 59, no. 2, pp. 167–181, sep 2004.\n[4] Piotr Doll, Ross Girshick, and Facebook Ai, “Mask R-CNN,” CVPR,\n2017.\n[5] Wonjik Kim, Asako Kanezaki, and Masayuki Tanaka, “Unsupervised\nLearning of Image Segmentation Based on Differentiable Feature Clus-\ntering,” IEEE Transactions on Image Processing, vol. 29, pp. 8055–\n8068, 2020.\n[6] J.Henriques X.Ji and A.Vedaldi, “Invariant Information Clustering for\nUnsupervised Image Classification and Segmentation,” Proceedings of\nthe IEEE/CVF International Conference on Computer Vision (ICCV),\n2019.\n[7] Xide Xia and Brian Kulis, “W-Net: A Deep Model for Fully Unsuper-\nvised Image Segmentation,” arXiv, nov 2017.\n[8] Nassir Navab, Joachim Hornegger, William M. Wells, and Alejandro F.\nFrangi, Medical Image Computing and Computer-Assisted Intervention –\nMICCAI 2015, vol. 9351 of Lecture Notes in Computer Science, Springer\nInternational Publishing, Cham, 2015.\n[9] Lei Zhou and Weiyufeng Wei,\n“DIC: Deep Image Clustering for\nUnsupervised Image Segmentation,” IEEE Access, vol. 8, pp. 34481–\n34491, 2020.\n[10] Asako Kanezaki, “Unsupervised Image Segmentation by Backpropaga-\ntion,” Icassp, pp. 2–4, 2018.\n[11] Pablo Arbel´aez, Michael Maire, Charless Fowlkes, and Jitendra Malik,\n“Contour detection and hierarchical image segmentation,” IEEE Trans-\nactions on Pattern Analysis and Machine Intelligence, vol. 33, no. 5,\npp. 898–916, 2011.\n[12] M Everingham, M Everingham, A Zisserman, A Zisserman, C Williams,\nand C Williams, “The PASCAL Visual Object Classes Challenge 2006\n(VOC2006) results,” International Journal of Computer Vision, vol. 88,\nno. 2, pp. 303–338, 2010.\n[13] Dhruv Batra, Adarsh Kowdle, Devi Parikh, Jiebo Luo, and Tsuhan Chen,\n“iCoseg: Interactive co-segmentation with intelligent scribble guidance,”\nin 2010 IEEE Computer Society Conference on Computer Vision and\nPattern Recognition. jun 2010, pp. 3169–3176, IEEE.\n[14] Asako Kanezaki, “Unsupervised image segmentation by backpropaga-\ntion,” in 2018 IEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP), 2018, pp. 1543–1547.\n",
  "categories": [
    "eess.IV"
  ],
  "published": "2024-03-17",
  "updated": "2024-03-17"
}