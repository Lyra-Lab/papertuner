{
  "id": "http://arxiv.org/abs/2403.12116v4",
  "title": "Unsupervised End-to-End Training with a Self-Defined Target",
  "authors": [
    "Dongshu Liu",
    "Jérémie Laydevant",
    "Adrien Pontlevy",
    "Damien Querlioz",
    "Julie Grollier"
  ],
  "abstract": "Designing algorithms for versatile AI hardware that can learn on the edge\nusing both labeled and unlabeled data is challenging. Deep end-to-end training\nmethods incorporating phases of self-supervised and supervised learning are\naccurate and adaptable to input data but self-supervised learning requires even\nmore computational and memory resources than supervised learning, too high for\ncurrent embedded hardware. Conversely, unsupervised layer-by-layer training,\nsuch as Hebbian learning, is more compatible with existing hardware but does\nnot integrate well with supervised learning. To address this, we propose a\nmethod enabling networks or hardware designed for end-to-end supervised\nlearning to also perform high-performance unsupervised learning by adding two\nsimple elements to the output layer: Winner-Take-All (WTA) selectivity and\nhomeostasis regularization. These mechanisms introduce a \"self-defined target\"\nfor unlabeled data, allowing purely unsupervised training for both\nfully-connected and convolutional layers using backpropagation or equilibrium\npropagation on datasets like MNIST (up to 99.2%), Fashion-MNIST (up to 90.3%),\nand SVHN (up to 81.5%). We extend this method to semi-supervised learning,\nadjusting targets based on data type, achieving 96.6% accuracy with only 600\nlabeled MNIST samples in a multi-layer perceptron. Our results show that this\napproach can effectively enable networks and hardware initially dedicated to\nsupervised learning to also perform unsupervised learning, adapting to varying\navailability of labeled data.",
  "text": "Unsupervised End-to-End Training with a\nSelf-Defined Target\nDongshu Liu 1,∗, J´er´emie Laydevant 1, Adrien Pontlevy 1,\nDamien Querlioz 2 and Julie Grollier 1,∗\n1Laboratoire Albert Fert, CNRS, Thales, Universit´e Paris-Saclay, Palaiseau,\nFrance\n2Universit´e Paris-Saclay, CNRS, Centre de Nanosciences et de Nanotechnologies,\nPalaiseau, France\n* Corresponding author\nE-mail: dongshu.liu@cnrs.fr, julie.grollier@cnrs-thales.fr\nAbstract.\nDesigning algorithms for versatile AI hardware that can learn on the edge\nusing both labeled and unlabeled data is challenging. Deep end-to-end training\nmethods incorporating phases of self-supervised and supervised learning are\naccurate and adaptable to input data but self-supervised learning requires\neven more computational and memory resources than supervised learning, too\nhigh for current embedded hardware.\nConversely, unsupervised layer-by-layer\ntraining, such as Hebbian learning, is more compatible with existing hardware\nbut doesn’t integrate well with supervised learning. To address this, we propose\na method enabling networks or hardware designed for end-to-end supervised\nlearning to also perform high-performance unsupervised learning by adding two\nsimple elements to the output layer:\nWinner-Take-All (WTA) selectivity and\nhomeostasis regularization.\nThese mechanisms introduce a ’self-defined target’\nfor unlabeled data, allowing purely unsupervised training for both fully-connected\nand convolutional layers using backpropagation or equilibrium propagation on\ndatasets like MNIST (up to 99.2%), Fashion-MNIST (up to 90.3%), and SVHN\n(up to 81.5%).\nWe extend this method to semi-supervised learning, adjusting\ntargets based on data type, achieving 96.6% accuracy with only 600 labeled\nMNIST samples in a multi-layer perceptron. Our results show that this approach\ncan effectively enable networks and hardware initially dedicated to supervised\nlearning to also perform unsupervised learning, adapting to varying availability\nof labeled data.\nKeywords: Backpropagation, Equilibrium propagation, Hebbian learning, Edge AI\nhardware, Unsupervised learning, Semi-supervised learning, Local learning rule\narXiv:2403.12116v4  [cs.NE]  21 Nov 2024\nPreprint. Under review.\n2\n1. Introduction\nAutonomous AI devices operating at the edge would be much more adaptable and\nuseful if they could learn continuously, making use of both precise but usually scarce\nlabeled data as well as widely available unlabeled data (Xiao et al. 2022, Barhoush\net al. 2022, Shakarami et al. 2020). But for this, they need to be able to perform both\nsupervised and unsupervised learning.\nState-of-the-art (SOTA) deep learning methods for unsupervised learning\npredominantly rely on self-supervised representation learning methods (Zbontar et al.\n2021, Bardes et al. 2021, Siddiqui et al. 2022, Balestriero et al. 2023), where all layers\nare trained together through backpropagation in the “end-to-end” manner schematized\nin Figure 1(A). These methods can achieve high accuracy but necessitate gathering\nstatistics (such as variance and covariance) of model outputs across mini-batches,\ndemanding substantial computational and memory resources.\nThese requirements cannot be satisfied in the case of edge computing. Typically,\nedge devices have restricted computational power, limited memory capacity, and\nstringent energy consumption requirements (Cao et al. 2020, Chen & Ran 2019).\nTherefore, algorithms designed for edge computing must be resource-efficient to\noptimize power usage.\nIn parallel, unsupervised learning for edge AI hardware has explored bio-inspired\napproaches, such as Hebbian Learning (Do 1949, Azghadi et al. 2015) or Spike-Timing-\nDependent Plasticity (STDP) (Bi & Poo 1998, Bichler et al. 2012, Jo et al. 2010, Ishii\net al. 2019, Indiveri et al. 2006), which circumvent the need for extensive computational\nresources. These methods enable learning to be both local and “greedy”, meaning\nit occurs independently and sequentially at each layer as illustrated in Figure 1(B)\n(Bengio et al. 2006).\nRecent works have shown impressive (and often thought\nimpossible) results on “Hebbian Deep Learning” (Miconi 2021, Moraitis et al. 2022),\nreaching high accuracy on complex tasks (Journ´e et al. 2022). However, the absence\nof a global learning objective and the layer-specific nature of these training methods\nprecludes the possibility of supervised learning. Therefore, semi-supervised learning,\nwhich leverages both labeled and unlabeled data through alternating unsupervised\nand supervised learning phases, cannot be easily performed (Van Engelen & Hoos\n2020, Zhu & Goldberg 2022).\nIn this study, we introduce a simple method, illustrated in Figure 1(C), that\nallows a network designed for end-to-end supervised learning to also efficiently achieve\nunsupervised learning. Leveraging the network’s inherent dynamics, whether recurrent\nor feedforward, in response to input data we generate a self-defined target for each data\npoint using a Winner-Take-All mechanism in the last layer, derived from biological\nprinciples (Maass 2000).\nAdditionally, we apply homeostasis, another bio-inspired\nPreprint. Under review.\n3\napproach, to regulate the labeling process and prevent feature collapse, where the\nmodel outputs identical predictions for all inputs (Querlioz et al. 2013). These bio-\ninspired approaches, optimized through evolution, are inherently resource-efficient and\ntherefore particularly valuable for hardware design. On one hand, compared to self-\nsupervised learning, our unsupervised target depends only on the current network\noutput and the homeostasis term, requiring minimal memory and thus being more\ncompatible with edge AI hardware. On the other hand, we use a global objective that\naccommodates both global (Backpropagation) and local (Equilibrium Propagation)\nlearning rules, retaining the advantages of end-to-end supervised learning while also\nenabling online unsupervised and semi-supervised training of neural networks.\nWith our versatile and framework-agnostic we demonstrate the first results of\nunsupervised and semi-supervised training with Equilibrium Propagation (Scellier &\nBengio 2017, Ernoult et al. 2019, 2020, Laborieux et al. 2021, Laborieux & Zenke 2022,\nScellier et al. 2024). This algorithm, originally applied to supervised learning, shows\ngreat promise for edge AI implementations on analog hardware because it computes\nweight updates solely through the dynamics of the hardware and with local information\nonly - paving the way for implementations requiring less memory and circuitry (in Yi\net al. 2022, Dillavou et al. 2022, Kendall et al. 2020, Laydevant et al. 2023, Martin et al.\n2021). We also demonstrate the method’s effectiveness in multilayer and convolutional\nnetworks networks trained with conventional non-local backpropagation. The results\nfor the different datasets are summarized in Figure 2.\nFigure 1.\n(A) Self-supervised End-to-end training methods, here illustrated with a contrastive\nglobal loss as training objective. (B) Layer-wise training approaches typically used for unsupervised\nlearning algorithms with local learning rules. (C) Our approach to unsupervised end-to-end using an\nunsupervised global loss defined at the network output.\nMore specifically, the contributions of this work are the following:\n• Our innovation is a special objective function that takes inspiration from both\nSOTA Self-Supervised Learning methods and biological mechanisms. Information\nabout the input data in the representation vector (output of the model) is\nPreprint. Under review.\n4\nMNIST\n(One-layer)\nMNIST\n(Two-layer)\nMNIST\n(Conv)\nFashion-MNIST\n(Conv)\nSVHN\n(Conv)\nTest Accuracy (%)\n97.3\n97.6\n97.1\n97.6\n99.2\n90.3\n81.5\nUnsupervised EP\nUnsupervised BP\nFigure 2.\nTest accuracies achieved using our Unsupervised End-to-End Training Method for\nweight updates, utilizing Backpropagation and Equilibrium Propagation, applied to fully-connected\nand convolutional networks across the MNIST, Fashion-MNIST, and SVHN datasets.\nsparsified with a Winner-Take-All mechanism while the features are regularized\nthrough a homeostasis mechanism, which imposes the neurons to have a certain\nlevel of variance given different input data.\n• We first perform unsupervised learning on the MNIST dataset with a multilayer\nperceptron, within two different frameworks: a bidirectional recurrent network\nwith static inputs trained by Equilibrium-Propagation (EP) and a feed-forward\nnetwork trained by Backpropagation (BP). Using a fully-connected network with\na hidden layer, we achieve a test accuracy of 97.6% with both EP and BP (see\nFigure 2. The analysis of the classification accuracy and the quality of the learned\nfeatures reveal the improvement in performance of the network with a hidden layer\ncompared to a single-layer network.\n• We show that our Self-Defined Unsupervised Target method trains Convolutional\nNetworks in an unsupervised way through BP with competitive test accuracies\non MNIST (99.2%), Fashion-MNIST (90.3%) and the RGB color dataset SVHN\n(81.5%) (Figure 2).\n• We demonstrate that we can alternate unsupervised and supervised learning\nphases on the same network, by adapting the target type based on the availability\nof labels. Our two-layer network thus achieves a test accuracy of 96.6% training\nwith only 600 labeled samples from the MNIST dataset, outperforming semi-\nsupervised methods based on ’pseudo-labels’ (Lee 2013).\n2. Methods\n2.1. Self-defined Unsupervised Target\nWe introduce a self-defined target d to determine an unsupervised global loss from\nthe network output, by a direct modification of the supervised global loss. For this,\nPreprint. Under review.\n5\nwe equip the output layer of the neural network with two bio-mechanisms, Winner-\nTake-All (WTA) selectivity and homeostasis regularization, where WTA encourages\noutput neurons to respond differently to each input, and homeostasis ensures that the\naverage unsupervised target value across the examples is balanced among the output\nneurons (Querlioz et al. 2013, Bichler et al. 2012, Miconi 2021).\nFor each new unlabeled example presented at the input of the neural network,\nwe select the ’winners’ as the top k neurons in the output layer y with the highest\nresponse above a threshold defined by the homeostasis penalty term H.\nFor each\noutput neuron i, the target value of the ’winners’ di is set to ‘1’, while the target value\nof the other neurons is set to ‘0’:\ndi =\n(\n1\nif yi −Hi ∈{k largest elements of y −H},\n0\notherwise.\n(1)\nThe vectors y and H have the same length noutput, the number of output neurons.\nThe term H may take on positive or negative values to either encourage or suppress\nneuronal competition, respectively.\nThe homeostasis value is set to zero at the\nbeginning. In the pseudo-code Algorithm. 1, Eq. 1 is shortened as d = kWTA(y −H).\nThe WTA competitive environment enables neurons to specialize in specific data\npatterns.\nWhen data from different classes activate distinct output neurons, data\nclustering is obtained. However, without a regulatory mechanism for neuron activity\nduring training, dominant neurons may respond to all inputs, while the others become\nsilent.\nThe homeostasis penalty term H, introduced in Eq. 1, addresses the issue\nof silent neurons by ensuring that all output neurons achieve an equal ’winning\nfrequency.’\nConsequently, each neuron learns from, on average, the same number\nof images presented at the input. Following the proposal of Querlioz et al. (2013), the\nhomeostasis term H is updated as\n˙H ←γ(⟨d⟩−T),\n(2)\nwhere ⟨d⟩is the average of the unsupervised target across examples, T the desired\nwinning frequency defined as the ratio between the number of winner neurons k and\nthe number of output neurons noutput, and γ a hyperparameter. The average activity,\n⟨d⟩, is defined differently depending on whether the input data is batched or not. In\nsequential mode, where inputs are sent to the network one by one, it is defined as the\nexponential moving average of the unsupervised target. In mini-batch mode, where\ninputs are processed in batches, it is defined as the average unsupervised target across\nthe batch. The equations for these definitions (Eq. 11 and Eq. 12) and additional\ndetails can be found in the Appendices.\nOnce the unsupervised target is defined, we calculate the unsupervised loss L as\nthe Mean Squared Error (MSE) between the output y and unsupervised target d:\nL = 1\nN\nX\nn∈N\n(yn −dn)2,\n(3)\nPreprint. Under review.\n6\nwhere N designates the dataset with N samples, and yn and dn are respectively\nthe output value and the unsupervised target corresponding to the input n.\nFor\nthe training process, the weights of the network are updated by performing stochastic\ngradient descent on L. In the case of BP, the gradients are calculated directly by using\nthe chain rule; in the case of EP, they are approximated by local neural activities.\nThe complete unsupervised training process is described in Algorithm 1, where\nXtrain is the training dataset, Model the neural network model, W the training\nparameters, lr the learning rate, and N the epoch number.\nAlgorithm 1 Pseudo Code for Unsupervised End-to-End Training with Self-Defined\nTarget\n1: Inputs:\n2:\nXtrain, Model, W, γ, lr, N, L, T.\n3:\n4: for epoch n ∈N do\n5:\nInitialize the homeostasis term H ←0\n6:\nfor each image x ∈Xtrain do\n7:\nGet output : y ←Model(x)\n8:\nDefine unsupervised target : d ←kWTA(y −H)\n▷Target decision\n9:\nCalculate loss: l ←L(y, d)\n10:\nCalculate gradient: ∆W ←−∂l\n∂W\n▷Analytically (BP) or via local activities (EP)\n11:\nUpdate weight: W ←W + lr · ∆W\n12:\nUpdate the homeostasis term H ←H + γ(⟨d⟩−T)\n13:\nend for\n14: end for\nIn the pseudo-code and experiments presented here, the homeostasis term H is\ninitialized at each epoch, but we found that the training performance on MNIST is\nnot affected if it is not the case.\nTable 1. Notations used in our neural network models.\nTraining type\nNotation\nDefinition\nOne-layer network\nUnsupervised neural network consisting of a single synaptic\nlayer, directly linking the input to the output without any\nhidden layer.\nTwo-layer network\nUnsupervised neural network consisting of two processing\nlayers, linking the input to the output with one hidden layer.\nUnsupervised\nTraining\nUnsupervised\noutput\nlayer\nLast layer of the unsupervised neural network, where the\nunsupervised loss is defined.\nUnsupervised\nhidden\nlayer\nMiddle layer connecting the input and output layer in the\nunsupervised two-layer network.\nReadout layer\nLinear classifier layer added to the top of the unsupervised\nnetwork,\nintegrated\nsubsequent\nto\nthe\ncompletion\nof\nunsupervised training.\nMulti-layer\npercep-\ntron (MLP)\nFully connected neural network used for semi-supervised\ntraining, consisting of at least one hidden layer.\nSemi-\nsupervised\ntraining\nOutput layer\nLast layer of the MLP, where the unsupervised or supervised\nloss is defined. Neuron count corresponds to the number of\nclasses in the dataset.\nHidden layer\nThe middle layer between the input layer and output layer\nin the MLP.\nPreprint. Under review.\n7\n2.2. Training Methodology\nA comprehensive list of all layer and network structure notations is presented in\nTable 1.\n2.2.1. Unsupervised Learning\nWe train two distinct network architectures using our\nunsupervised learning framework: a one-layer network and a two-layer network. These\nnotations discount the input layer, as its values are clamped to the inputs.\nThe\nunsupervised loss is always computed at the final layer, designated as the unsupervised\noutput layer. In the two-layer network, the first layer is called the unsupervised hidden\nlayer.\nClass association is used exclusively for unsupervised training to evaluate\nclassification performance.\nThis procedure is implemented at the end of the\nunsupervised training phase, prior to the testing phase. During the association phase,\nthe weights of the trained unsupervised network are kept frozen. In our approach,\nwe utilize two distinct methods:\ndirect association and a linear classifier.\nBoth\nmethods require a subset of labeled data. However, at no point are the weights of\nthe unsupervised network updated using these labels.\nIn the direct association method, each output neuron of the neural network is\nassigned to the class to which it shows the strongest response.\nThis technique is\nwidely employed in unsupervised training (Querlioz et al. 2013, Diehl & Cook 2015,\nMoraitis et al. 2022). We expose the network to a subset of labeled data and record\nthe network response to each input. For every output neuron, we determine its average\nresponse to the different classes. The class that elicits the highest average response\nfrom a neuron is designated as that neuron’s represented class. During the testing\nphase, the class whose corresponding neurons exhibit the highest average response is\nthen identified as the predicted class for the given input.\nIn the linear classifier method, a linear classifier is trained in a supervised manner\non a subset of labeled data (Ferr´e et al. 2018, Miconi 2021, Moraitis et al. 2022). It is\nbuilt on top of the unsupervised network, using the unsupervised network’s output as\nits input. Upon completion of training, each class is represented by a distinct output\nneuron in the classifier.\nThe prediction of the class is determined by the output\nneuron that exhibits the maximum value. In our notation, the linear classifier added\nfor classification is not included in the total layer count of the network.\nFigure 3 illustrates the unsupervised network’s architecture and the training-\ntesting workflow, using a two-layer network as an example.\n2.2.2.\nSemi-supervised Learning For the semi-supervised learning framework, we\nemploy a multilayer perceptron (MLP) with a single hidden layer, trained using a\nlimited amount of labeled data. The MLP consists of two fully connected layers: a\nPreprint. Under review.\n8\nhidden layer and an output layer. Both the supervised and unsupervised losses\nare computed at the output layer.\nUnlike fully unsupervised learning, our semi-supervised framework does not\ninvolve a class association step. Training initially proceeds in a supervised manner,\nfollowed by unsupervised learning.\nTherefore, the output layer’s neuron count\ncorresponds to the number of classes in the dataset. Upon completion of the training\nphase, the network is directly subjected to the testing process. The structure of the\nsemi-supervised network, along with its configuration, is shown in Figure 8.\n2.3. Additional Regularization Methods\nFor unsupervised training, dropout (Nitish et al. 2014) is applied at both the input\nand output layers of our neural network architecture, with dropout probabilities set\nto 0.3 and 0.2. Applying dropout to the output layer helps to introduce randomness\nin the unsupervised target decision. This contrasts with supervised learning, where\ndropout is usually not used at the output layer, as the number of output neurons\ntypically matches the number of classes. For our semi-supervised training, where the\nnumber of output neurons also matches the number of classes, dropout is not employed\nat the output, but at the input and hidden layers, with probabilities of 0.3 and 0.5\nrespectively to prevent over-fitting.\nIn the case of EP training, we also use label smoothing (Pereyra et al. 2017) to\npenalize overconfidence in the unsupervised targets. Instead of pushing output neurons\nto the values of ‘1’ and ‘0’, the k winner neurons are pushed to a less confident value,\nwhile the other neurons receive a less strong inhibition value.\nThese two additional regularization methods are detailed in the Appendices.\n3. Results\n3.1. Unsupervised Learning\nIn this section, we present simulation results for the MNIST handwritten digits\ndataset (LeCun et al. 2010), utilizing both Backpropagation (BP) and Equilibrium\nPropagation (EP) to train the networks with our unsupervised global loss.\nThe\ngeneral network structure is shown in Figure 3(A). In practice, BP is adapted to\ntrain a standard feed-forward network, while training with EP requires a bidirectional\nnetwork. In this bidirectional recurrent network, hidden neurons receive information\nfrom both the input neurons and the output neurons, contrary to the feed-forward\nnetwork where hidden neurons receive information only from the input neurons. In\nthis convergent recurrent structure, neural values evolve towards stable values for a\nduration corresponding to their relaxation time after each given input is applied.\nPreprint. Under review.\n9\nWe implement both unsupervised EP and BP on two distinct networks: a one-\nlayer network and a two-layer network, following the notations in Table 1, and compare\ntheir performance. For the one-layer network, since there are no hidden neurons, there\nis no difference between the feed-forward and EP-used bidirectional networks. All the\nreported results represent the averaged values from ten simulations.\nFigure 3.\nUnsupervised learning: (A) Network architecture and (B) Training-Testing process\nThe sequence of training, class association, and testing steps, described in the\nMethods section, is illustrated in Figure 3(B).\nFigure 4 shows the test accuracy on MNIST obtained by training a one-layer\nneural network with our unsupervised method through EP and BP. The results are\ncompared to untrained (i.e., random weights) one-layer and two-layer networks. The\nresults are plotted as a function of the percentage of labeled data used for class\nassociation in the MNIST database, for direct association (A) and with a linear\nclassifier (B). For both association methods, the unsupervisedly trained networks\nexhibit a marked increase in accuracy compared to the untrained networks.\nThe\naccuracy obtained with EP and BP is comparable.\nThe results obtained with the two class association methods are different. For\ndirect association (Figure 4(A)), less than 5% of labels are needed to reach the\npeak accuracy of about 95.8%, beyond which the use of more labeled data does not\nsignificantly impact the accuracy.\nOn the contrary, as shown in Figure 4(B), the\nperformance of the linear classifier improves significantly with increasing amounts of\nlabeled data, underscoring the need for substantial labeled data to effectively train its\nextensive parameters in a supervised manner. The highest accuracy for the one-layer\nnetwork, close to 97.2% is obtained when all labels in the database are used.\nThe performance of the untrained networks in Figure 4(B) also strongly increases\nwith the amount of labeled data used to train the linear classifier, reaching the accuracy\nPreprint. Under review.\n10\nof the trained networks when all the labels are leveraged. With direct association in\nFigure 4(A) on the other hand, the gap between the trained and untrained networks\nremains large and constant versus the amount of labeled data.\nTherefore, while\nthe accuracy results are reduced with the direct association method, it provides a\nbetter framework than the linear classifier to evaluate the contribution of unsupervised\ntraining to the network accuracy, compared to untrained networks.\n92.5\n95\n97.5\n100 (A) Direct association\n1 5\n50\n100\n50\n55\n60\n65\nPercentage of Labels for Class Association (%)\nTest Accuracy (%)\n1 5\n50\n100\nPercentage of Labels for Class Association (%)\n80\n85\n90\n95\n97.5\n100 (B) Linear classifier\nUnsupervised EP one-layer\nUnsupervised BP one-layer\nUntrained one-layer\nUntrained two-layer\nFigure 4.\nTest accuracy on MNIST for unsupervised learning, as a function of the percentage\nof labeled data used for class association: (A) with direct association; (B) with a linear classifier.\nGreen dotted line: one-layer network trained by unsupervised BP, blue dotted line: one-layer network\ntrained by unsupervised EP. Pink lines: untrained networks, including one-layer network (dotted line)\nand two-layer network (solid line). The single-layer network has 2,000 output neurons, while the two-\nlayer version adds 2,000 hidden neurons.\nIn Figure 4(A) and (B), we see that the two-layer untrained network has a\ndecreased accuracy compared to the one-layer untrained network for both class\nassociation methods. We now evaluate in Figure 5 the impact of training the two-layer\nneural network with our method compared to a one-layer network. As previously, the\naccuracy is plotted against the amount of labeled data used in the class association\nprocess, for direct association in 5(A) and with the additional classifier in 5(B). In all\ncases, the trained two-layer networks achieve a higher accuracy than the trained one-\nlayer networks: The addition of a hidden layer consistently improves the classification\nability. The enhancement is stronger in the direct association method than with the\nlinear classifier: The optimization of the classifier’s parameters reduces the impact of\nunsupervised training on the overall accuracy.\nThe highest accuracy, of 97.6%, is achieved by the two-layer network trained\nwith EP or BP, using a linear classifier and all available labeled data for association\n(solid lines in Figure 5(B)). Overall, the enhancement in accuracy obtained by adding\na hidden layer in the networks indicates the effectiveness of the proposed end-to-\nPreprint. Under review.\n11\nend unsupervised training: In traditional unsupervised Hebbian or STDP learning\nmethods, adding a hidden layer in a fully connected network typically does not lead\nto improvements in accuracy (Zhou 2022, Journ´e et al. 2022).\n1 5\n50\n100\nPercentage of Labels for Class Association (%)\n94\n94.5\n95\n95.5\n96\n96.5\n97\n97.5\n98\nTest Accuracy (%)\n(A) Direct association\n1 5\n50\n100\nPercentage of Labels for Class Association (%)\n94\n94.5\n95\n95.5\n96\n96.5\n97\n97.5\n98\n(B) Linear classifier\nUnsupervised EP one-layer\nUnsupervised EP two-layer\nUnsupervised BP one-layer\nUnsupervised BP two-layer\nFigure 5.\nTest accuracy on MNIST for unsupervised training as a function of the amount of\nlabeled data used for class association: (A) with direct association; (B) with a linear classifier. The\ntwo-layer network trained by unsupervised BP (green solid line) and EP (blue solid line) is compared\nwith the one-layer unsupervised trained network (dotted lines). The single-layer network has 2,000\noutput neurons, while the two-layer version adds 2,000 hidden neurons.\n(A) Unsupervised one-layer network\n(B) Unsupervised two-layer network\n4\n3\n2\n1\n0\n1\n2\n3\n4\nFigure 6.\nMaximum activation images:\ninput images that elicit the strongest responses in\n20 selected (the two highest responses for each class) output neurons of the networks trained by\nunsupervised BP on the MNIST dataset.\n(A) One-layer network (B) Two-layer network.\nEach\nnetwork comprises 2,000 output neurons. The hidden layer of the two-layer network consists of 2,000\nneurons.\nWe use a visualization method to further study the impact of the hidden layer\nin our fully-connected network trained in an unsupervised way (Erhan et al. 2009).\nThis technique finds through optimization the input images that maximize neuronal\nresponses in the trained network, producing ’maximum activation images’.\nThese\nPreprint. Under review.\n12\nimages provide insights into the learned features of each neuron in the network. Figure\n6 showcases the maximum activation images for output neurons from two networks\ntrained via unsupervised BP: with a single unsupervised layer (Fig.6 (A)) and with\ntwo unsupervised layers (Fig.6 (B)). Compared to the single-layer network, the two-\nlayer network displays digit features with clearer outlines and sharper contrast against\nthe background. The single-layer approach tends to focus on the extreme values of\ndigit pixels, occasionally omitting parts of the digit features. In contrast, the two-layer\nunsupervised network captures the entirety of the digit structure. This observation\nprovides an explanation for the improved accuracy obtained by incorporating the\nhidden layer. The inclusion of a hidden layer results in clearer feature representation\nat the output layer and enhanced accuracy.\nWe further applied our algorithm to convolutional neural networks (CNNs) for\nthe MNIST dataset, as well as two more complex datasets: Fashion MNIST (Xiao\net al. 2017) and Street View House Numbers (SVHN) (Netzer et al. 2011). For SVHN\ntraining, we did not use the extra training set. As shown in Figure 7, our convolutional\nnetwork is composed of two convolutional layers followed by a fully connected layer.\nThe input data is passed through the convolutional layers, then flattened and sent\nto the fully connected layer. After each convolutional layer, max pooling is applied\nto downscale the image before passing it to the next layer. The unsupervised loss\nis calculated at the output of the fully connected layer. All details about the CNN\narchitecture can be found in the appendices. After unsupervised training, we trained\na linear classifier with full labels to evaluate the test accuracy.\nFor the MNIST dataset, our CNN achieved a test accuracy of 99.2%, which is 1.6%\nhigher than the result obtained with two unsupervised layers. This demonstrates that\nour method can be integrated into deeper networks and benefit from the convolutional\narchitecture. Our method also generalizes well to more complex datasets, achieving\n90.3% accuracy on Fashion-MNIST and 81.5% accuracy on the SVHN dataset. These\nresults compete with other fully unsupervised training methods across all datasets, as\ndetailed in Section 4.3 and in Table 4. They also approach those of purely supervised\nbackpropagation training results for the MNIST (99.2 % for our unsupervised BP vs.\n99.6% for supervised BP)and Fashion-MNIST datasets (90.3 % for our unsupervised\nBP vs. 93.1% for supervised BP). For the SVHN dataset, supervised backpropagation\nachieves a test accuracy of 93.4% using the same CNN architecture, indicating that\nfurther improvements are needed to enhance our method for more complex tasks.\nPreprint. Under review.\n13\nTable 2.\nTest accuracy of our convolutional network trained with unsupervised backpropagation\nacross three different datasets: MNIST, Fashion-MNIST, and SVHN\nDataset\nData type\nInput size\nTraining samples\nTest samples\nOur Test Accuracy\nMNIST\nGrey scale\n28x28\n60,000\n10,000\n99.17(± 0.05)%\nFashion-MNIST\nGrey scale\n28x28\n60,000\n10,000\n90.25(± 0.26)%\nSVHN\nRGB\n3x32x32\n73,257\n26,032\n81.53(± 1.15)%\nFigure 7. Architecture of the convolutional network, consisting of two convolutional layers followed\nby a flatten operation and a fully connected layer.\n3.2. Semi-supervised Learning\nIn this section, we address a scenario where only a limited amount of training data is\nlabeled, and the majority remains unlabeled, which is the case in practical situations.\nSemi-supervised learning serves as a pertinent strategy under these circumstances\n(Thomas 2009, Van Engelen & Hoos 2020).\nOn one hand, the inclusion of some\nlabeled data can substantially enhance accuracy for unsupervised clustering tasks. On\nthe other hand, the vast volume of unlabeled data, when aligned with the predictions\nof a supervised network, aids in generalizing the training outcomes.\nIn our approach, as our unsupervised global loss retains the same form as the\nsupervised loss, we can leverage the benefits of both labeled and unlabeled data by\nadapting the target decision. For analog devices suitable for supervised Equilibrium\nPropagation (EP) training (in Yi et al. 2022, Dillavou et al. 2022, Kendall et al. 2020,\nLaydevant et al. 2023), our method enables semi-supervised training by modifying the\ntarget decision without any hardware changes.\nFigure 8 outlines our semi-supervised learning methodology, introduced in\nMethods, which aligns with standard practices in machine learning (Yarowsky 1995,\nZhu & Goldberg 2022). Our method starts with the model undergoing training using\na select, small subset of labeled data in a phase known as supervised pre-training, for\nN1 epochs. The duration of N1 is set to a magnitude where extending the number\nPreprint. Under review.\n14\nFigure 8. Semi-supervised Learning: (A) Network architecture and (B) Training-Testing process.\nof epochs beyond this point yields no further enhancement in the network’s efficiency.\nStarting with supervised, rather than unsupervised learning allows matching the\nnumber of output neurons to the number of classes, thus significantly reducing their\nnumber, together with requirements on memory and computational resources. After\nthe initial supervised training, we transition to unsupervised learning with the self-\ndefined target. Following each unsupervised training epoch, we perform one epoch of\nsupervised training with the same set of labeled data as used initially. This supervised\nre-training ensures that the network retains its earlier supervised learning insights.\nThis iterative process persists for N2 epochs. The hyperparameters N1 and N2 are\nspecified in the Appendices.\nIn semi-supervised learning, the use of a homeostasis term is, in principle, not\ncompulsory in the self-defined target expression of Eq: 1: The supervised pre-training\nensures that each output neuron becomes specialized in one class, averting the issue\nof silent neurons. In the following, we compare the results obtained with and without\nhomeostasis.\nGiven that the number of output neurons corresponds to the number of classes\nin both methods, the number of winner neurons k is consistently set to 1.\nThe network architecture used in these experiments is a multi-layer perceptron\nwith 5,000 hidden neurons and 10 output neurons, each corresponding to a class\nprediction. Experiments were conducted using both BP and EP algorithms, and to\nmitigate overfitting, dropout was incorporated during both training phases.\nFigure 9 shows the test accuracy on the MNIST dataset as a function of the\nquantity of labeled training data, for BP (Figure 9(A) and EP (Figure 9(B)). Our\nanalysis encompasses five scenarios with different amounts of available labeled data:\n100, 300, 600, 1000, and 3000 labels. This labeled data is randomly selected from\nthe training dataset, ensuring an equal representation of each class. To account for\nvariability in labeled data selection, each scenario is subjected to multiple simulation\nPreprint. Under review.\n15\nruns: thirty for the 100-label scenario due to high variance in its results, fifteen for\nthe simulation with 300 labeled images, and ten for the others.\n100\n300\n600 1000\n3000\nTraining Labels Count\n75%\n80%\n85%\n90%\n95%\n97%\n100%\nTest Accuracy (%)\n(A) Semi-supervised BP\n100\n300\n600 1000\n3000\nTraining Labels Count\n75%\n80%\n85%\n90%\n95%\n97%\n100%\n(B) Semi-supervised EP\nSupervised\nSupervised + WTA\nSupervised + WTA + homeostasis\nFigure 9. Test accuracy (%) of semi-supervised methods on the MNIST dataset using (A) BP and\n(B) EP for loss optimization. The network is firstly trained by limited labeled data, then trained in\na unsupervised manner by two different unsupervised targets, shown as ’+ WTA’ and ’+ WTA +\nHomeostasis’ respectively.\nFigure 9 compares the accuracy of semi-supervised learning without and with\nhomeostasis against purely supervised learning. The addition of unsupervised training\nmarkedly enhances performance, mainly because supervised training struggles to\ngeneralize from limited labeled data, often leading to overfitting.\nUnsupervised\ntraining mitigates this by utilizing the wealth of unlabeled data. When fewer than 600\nlabels are available, the homeostasis regularization becomes crucial. This is logical,\nas limited labeled data renders the WTA-defined target, essentially the prediction\naccuracy from supervised training, less accurate. Thus, imposing a winning frequency\nbalance helps sharpen these predictions.\nAdditionally, EP surpasses BP in semi-\nsupervised settings, likely due to its quicker convergence and the augmented impact\nof label smoothing in recurrent networks.\nWe use the Uniform manifold approximation and projection for dimension\nreduction (UMAP) method (McInnes et al. 2018) to visually assess the influence\nof WTA selectivity and homeostasis by condensing the network output for MNIST\ntest data into a two-dimensional space.\nThe corresponding 2D embedded output\nafter initial supervised training on 100 labeled samples is showcased in Figure 10(A).\nSubsequent outputs, after unsupervised training with WTA and the combination of\nWTA with homeostasis, are depicted in sub-figures (B) and (C), respectively.\nAs\nwe introduce WTA and its combination with homeostasis, the network output grows\nincreasingly compact and more distinguishable: This visualization underscores that\nincorporating unlabeled data enhances class differentiation based on prior supervised\ntraining outcomes. Moreover, homeostasis demonstrates its benefits even within semi-\nPreprint. Under review.\n16\nFigure 10.\nUMAP 2D embedding of the semi-supervised network output on MNIST test data\n(McInnes et al. 2018). (A): Output of the initial supervised training on 100 labeled images; (B):\nOutput of unsupervised training with WTA selectivity after the initial supervised step; (C): Output\nunsupervised training with both WTA and homeostasis after the initial supervised step.\nsupervised training scenarios.\nOverall, this study demonstrates that unsupervised learning realized through our\nself-defined target can integrate effectively with supervised learning to enhance model\ngeneralization from abundant unlabeled data. This semi-supervised approach is not\nonly efficient but straightforward to implement. Compared to purely unsupervised\nlearning, the incorporation of even a small amount of labeled data in the training\nsignificantly reduces the number of output neurons and eliminates the need for a class\nassociation process, resulting in a more hardware-friendly solution for edge training.\n4. Discussion\n4.1. Single-layer Unsupervised Network: Comparison with Hebbian Learning\nThe scenario of a single-layer unsupervised network provides a straightforward\ncomparison between our proposed weight updating rule and the conventional Hebbian\nlearning rule. Consider the synaptic weight Wij connecting neuron i in the input layer\nto neuron j in the hidden layer. Let xi denote the input value for neuron i, and yj the\naggregated input for neuron j before the activation ρ, i.e., yj = P\ni wijxi. The weight\nupdate rule that we use for the one-layer network is\n∆wij = −η ∂L\n∂wij\n= ηρ′(yj)xi(dj −ρ(yj)).\n(4)\nη is the learning rate, and ρ′(yj) is the derivative of activation ρ.\nWe compare our learning rule with WTA based Hebbian learning, which\nintroduces competition among neurons, ensuring that each neuron learns to recognize\nPreprint. Under review.\n17\ndifferent patterns. Its classic formulation is (Lagani et al. 2023):\n∆wij = ηxirj;\nrj =\n\n\n\n1\nif yj = max(y),\n0\nif yj ̸= max(y);\n(5)\nwhere rj denotes the activation state of neuron j in the hidden layer following the\nWTA selection, and vector y contains all the aggregated input values in the hidden\nlayer. (xi and yj retain the same definition as in Eq. 4.)\nHebbian learning, therefore, interprets the WTA mechanism as a nonlinear\nactivation function for hidden neurons. In contrast, our approach utilizes WTA to\ndefine an unsupervised target, maintaining the original activation function ρ. This\nWTA definition is suitable for end-to-end training, allowing the entire network to be\ntrained simultaneously, whereas the previous WTA definition is appropriate for layer-\nwise training, which enables independent training of each layer. While the weight\nupdates in both our method and the Hebbian rule are proportional to the input\nvalue x, our approach introduces negative weight updates for non-winning neurons.\nAdditionally, as the output of winning neurons approaches the value ’1’, the magnitude\nof their positive weight updates diminishes.\nIn contrast, Hebbian learning keeps\nthe weights of non-winning neurons static, updating only those of winning neurons,\nsolely based on the input value x. Our weight update strategy effectively prevents\nunbounded growth in synaptic weights, promoting a highly sparse output in the last\nlayer’s neurons. However, this approach also increases the disparity between winning\nand non-winning neurons, making the use of homeostasis regularization critical for\neffective pattern recognition. This form of regularization is less critical in traditional\nHebbian learning, as discussed by Miconi (2021).\nIn Table 3, we compare the results obtained with our unsupervised approach\non the MNIST dataset to several Hebbian-like learning algorithms.\nThe table\ndifferentiates between methods that utilize direct association and those that employ\na linear classifier. As discussed in Results, the use of linear classifiers helps to further\nimprove the accuracy, but the direct association method gives a better appreciation\nof the intrinsic quality of unsupervised learning.\nOur result on MNIST outperforms the Spike-timing-dependent plasticity (STDP),\na Hebbian learning implementation on the spiking neural network (SNN). It also\nachieves close results to standard Hebbian learning (Hard WTA) and SoftHebb\nMoraitis et al. (2022). For the direct association method, our one-layer network is\n0.5% below SoftHebb, but our two-layer network reaches 96.6%, which is 0.3% above\nthe SoftHebb result. Our best result with the linear classifier association method is\n97.3% with the one-layer network, and 97.60% with the two-layer network, compared\nto 97.8% for SoftHebb and 98.5% for Krotov & Hopfield (2019).\nPreprint. Under review.\n18\nTable 3.\nComparison of test accuracy across unsupervised methods on the MNIST dataset for\nMLP SNNs and ANNs. Methods are categorized based on their class association approach: direct\nassociation (top) and linear classifier (bottom). Labeled data is exclusively utilized during the class\nassociation phase.\nAssociation\nArchitecture\nLearning rule\nLabels used\nAccuracy\nSNN: 784-100 (Nessler et al. 2013)\nSTDP\nnot mentioned\n80.1%\nSNN: 784-300 (Querlioz et al. 2013)\nSTDP\n1,000\n93.5%\nSNN: 784-6400 (Diehl & Cook 2015)\nSTDP\n60,000\n95.0%\nANN: 784-2000 Moraitis et al. (2022)\nHard WTA\n60,000\n96.2%\nDirect\nANN: 784-2000 Moraitis et al. (2022)\nSoftHebb\n60,000\n96.3%\nAssociation\nANN: 784-2000 (Ours)\nBP\n60,000\n95.78(±0.14)%\nANN: 784-2000 (Ours)\nEP\n60,000\n95.77(±0.08)%\nANN: 784-2000-2000 (Ours)\nBP\n60,000\n96.61(±0.07)%\nANN: 784-2000-2000 (Ours)\nEP\n60,000\n96.60(±0.15)%\nANN: 784-2000-10 (Moraitis et al. 2022)\nHard WTA\n60,000\n97.8%\nANN: 784-2000-10 (Moraitis et al. 2022)\nSoftHebb\n60,000\n97.8%\nANN: 784-2000-10 (Krotov & Hopfield 2019))\nHebbian-like\n60,000\n98.5%\nLinear\nANN: 784-2000-10 (Ours)\nBP\n60,000\n97.08(±0.11)%\nClassifier\nANN: 784-2000-10 (Ours)\nEP\n60,000\n97.25(±0.07)%\nANN: 784-2000-2000-10 (Ours)\nBP\n60,000\n97.60(±0.11)%\nANN: 784-2000-2000-10 (Ours)\nEP\n60,000\n97.57(±0.10)%\nSoftHebb, developed by Moraitis et al. (2022), combines Oja’s rule (Oja 1982)\nwith a soft WTA. Oja’s rule with the form ∆wij = ηyj(xi −yjwij) introduces a weight\nnormalization mechanism to prevent the uncontrolled growth of synaptic weights in\nclassical Hebbian learning. Moraitis et al. (2022) introduce WTA into Oja’s rule inside\nthe activation function of aggregated input yj:\n∆wij = ηrj(xi −yjwij);\nrj = softmax(yj) =\neyj\nP\nk eyk .\n(6)\nCompared to the Hard-WTA implementation described before, softmax activation\nbrings smoother weight changes for Hebbian learning, as all neurons can update their\nweights proportionally to their normalized exponential value, rather than the sparse\nweight update of the Hard-WTA version. Contrary to our method, neither Soft-WTA\nnor Hard-WTA includes negative weight updates for non-wining neurons (which is\nknown as the anti-Hebbian part).\nOur algorithm cannot directly benefit from the use of a soft-WTA: The Hard-\nWTA in our target definition ensures that the winner’s weights are strengthened,\nas the output neurons value is bounded between 0 and 1.\nUsing a softmax to\ndefine the target drives the output neurons to smoother and similar values, making\nthem indistinguishable.\nHowever, we have introduced in our simulations a Label\nSmoothing strategy (Pereyra et al. 2017) (see Appendices) that resembles the soft\nWTA mechanism, by setting the winner values to be slightly less than ’1’ and the\nvalues for non-winning neurons slightly higher than ’0’, to penalize an overconfidence\nin the unsupervised target decision. This method does help to train two-layer networks\nwith EP, but we have not obtained any improvement for training with BP. This may\nPreprint. Under review.\n19\nbe due to the different network architectures used for EP and BP. Indeed, in the\nbidirectional recurrent networks trained by EP, the output value and therefore the\ntarget value directly influences the hidden layer, even before the weight update.\nKrotov & Hopfield (2019) proposes a Hebbian-like plasticity, which performs very\nwell for the one-layer unsupervised learning case. They generalize the L2 norm weight\nnormalization in Oja’s rule to the Lp norm, and add an anti-Hebbian part for the least\nactive neurons:\nrj =\n\n\n\n\n\n\n\n\n\n1\nif j = K,\n−∆\nif j = K −k;\n0\notherwise.\n(7)\nIn the notations of Krotov & Hopfield (2019), j ranks the neurons, with j = 0\ncorresponding to the least-activated, and j = K the strongest-driven hidden unit.\nFor consistency with our notations, we rename the competitive learning mechanism\napplied to their activation function as r.\nKrotov & Hopfield (2019) keeps the hard K-WTA as the competition mechanism,\nbut adds negative weight changes to the k neurons ranked below the winner. They\nalso use the Rectified Power (RePU) activation function and gain about 6% accuracy\nby optimizing the power coefficient. While such activation function cannot be directly\ncombined with our unsupervised training method, it is possible that other choices of\nbounded activation functions at the output layer can increase the overall accuracy.\nThe Hebbian-like plasticity mechanism introduced by Krotov & Hopfield (2019)\nsurpasses our one-layer network by 1.2%.\nWe show in the next section that\nadding a hidden layer to our network improves the quality of feature representation,\nunderscoring the advantages of end-to-end training in unsupervised learning contexts.\nFurthermore, Hebbian learning struggles to integrate with supervised learning to\nfully exploit label information. The integration of Hebbian learning within a semi-\nsupervised framework will be further discussed in the Discussion section 4.4.\n4.2. Two-layer Unsupervised Network: Comparison with Supervised End-to-End\nTraining\nIn the Results section, we show that by adding a hidden layer, we achieve better\naccuracy and clearer learned features at the output layer.\nThis improvement\ndemonstrates the advantage of end-to-end training on the multi-layer network, even\nfor unsupervised training. In this part, we analyze the role of hidden layer in our\nunsupervised training method by comparing our results with supervised end-to-end\ntraining.\nWe use BP to train supervisedly a one-layer and a two-layer network with the\nsame architecture as used for unsupervised learning: 2,000 output neurons (and 2,000\nPreprint. Under review.\n20\n(A) Supervised one-layer network\n(B) Unsupervised one-layer network\n(C) Supervised two-layer network\n(D) Unsupervised two-layer network\n0.100\n0.075\n0.050\n0.025\n0.000\n0.025\n0.050\nFigure 11. Visualization of the weights trained by BP on the MNIST dataset for 20 neurons in\nthe first layer of (A) a supervised one-layer network, (B) an unsupervised one-layer network, (C) a\nsupervised two-layer network and (D) an unsupervised two-layer network. The one-layer network\ncomprises 2,000 output neurons, while the two-layer network includes an additional 2,000 hidden\nneurons.\nhidden neurons for two-layer network). In this structure, each class is represented by\n200 output neurons, that all have the same target ’0’ or ’1’ depending on the presented\nimage.\nFigures 11(A) and (B) illustrate the weight patterns of 20 randomly selected\nneurons from the output of a single layer network.\nIn the supervised training\nscenario, depicted in Figure 11(A), each neuron’s blurred digit representations indicate\nthat neurons learn to recognize all images within a class. Conversely, Figure 11(B)\ndemonstrates that in unsupervised training, neurons develop sharp, distinct patterns\nfor individual handwritten digits within a class. Compared to supervised training,\nunsupervised training results in neurons learning varied weights even when they\nrepresent the same class. This variation is due to the selection of only k strongest\nwinners for each image in unsupervised training, enabling neurons to learn different\nimages despite representing the same class.\nPreprint. Under review.\n21\nWe then examine in Figure 11(C) and (D) the learned weights in the first layer of\na two-layer network. Supervised end-to-end training allows hidden layers to establish a\nhierarchy of features, with lower layers capturing simple elements such as edges (Erhan\net al. 2009). In Figure 11(C), the supervised training of the hidden layer reveals weights\nwith significantly high or low values at the edges, facilitating the abstraction of simpler,\nmore analyzable features for subsequent layers. Similarly, the hidden layer weights in\nour unsupervised network exhibit patterns reminiscent of digit components, including\nnotably high values at edges. However, as shown in Figure 11(D), these weights display\nmore uniform patterns and less specificity across different edges than for supervised\nlearning. This observation can explain why the improvement in accuracy introduced\nby the hidden layer is lower in our unsupervised learning than in supervised end-to-\nend training. Incorporating regularization techniques, such as homeostasis, into the\nhidden layer could improve both accuracy and feature extraction capabilities.\n4.3. Unsupervised CNN Network: comparison with other purely unsupervised learning\nTable 4.\nComparison of test accuracy across unsupervised methods with a convolutional neural\nnetwork on different datasets: MNIST, Fashion-MNIST, and SVHN. All labeled data is used during\nthe class association phase.\nDataset\nMethod\nArchitecture\nAssociation\nAccuracy\nUnsupervised BP (Ours)\n32f5-128f3-3000\nLinear classifier\n99.17(± 0.05)%\nPL-AE (Bouayed et al. 2020)\n32f4-64f4-128f4-256f4-350\nLinear classifier\n99.3%\nWTA-based (Erko¸c & Eskıl 2023)\n97f5-117f5\nMLP\n99.18%\nMNIST\nHard WTA (Lagani et al. 2022)\n96f5-128f3-192f3\nLinear classifier\n98.6%\nSoftHebb (Journ´e et al. 2022)\n96f5-384f3-1536f3\nLinear classifier\n99.35%\nAE (Cavallari et al. 2018)\n4f5-392-392-128\nSVM\n94%\nUnsupervised BP (Ours)\n32f5-128f3-3000\nLinear classifier\n90.25(± 0.25)%\nWTA-based (Erko¸c & Eskıl 2023)\n92f3-48f3\nMLP\n90.11%\nFashion-MNIST\nAE (Cavallari et al. 2018)\n4f5-392-392-128\nSVM\n88%\nSiamese Network(Trosten & Sharma 2019)\n32f5-32f5-64\nSVM\n86%\nUnsupervised BP (Ours)\n32f5-128f3-3000\nLinear classifier\n81.53(±1.15)%\nVAE (Pihlgren et al. 2020)\n32f4-64f4-128f4-256f4-128\nMLP\n81.2%\nSVHN\nVAE (Pihlgren et al. 2020)\n32f4-64f4-128f4-256f4-128\nLinear classifier\n71.2%\nPL-AE (Bouayed et al. 2020)\n32f4-64f4-128f4-256f4-350\nLinear classifier\n76.5%\nSiamese Network(Trosten & Sharma 2019)\n32f5-32f5-64\nSVM\n78%\nHere, we demonstrate that our method can be generalized to CNNs and\nachieves high accuracy compared to other purely unsupervised methods, showing the\neffectiveness of unsupervised learning with a Self-Defined Target.\nTable 4 compares our test accuracy reached on MNIST, Fashion-MNIST, and\nSVHN with the results obtained through other purely unsupervised methods. These\nmethods include reconstructive strategies like Autoencoders (AE) and Variational\nAutoencoders (VAE), competitive learning techniques such as Competitive WTA\nand Hebbian learning, and similarity-based models like Siamese networks. All these\napproaches utilize the classification task as the benchmark for evaluating their training\noutcomes. As for the network architecture, the number of channels and filter sizes\ndefine each convolutional layer; for instance, ’32f5’ signifies 32 output channels with a\nPreprint. Under review.\n22\nfilter size of 5. A fully connected layer, like ’3000’, is characterized solely by its neuron\ncount, indicating 3000 neurons.\nWhile the pseudo-labelling auto-encoder (PL-AE) method (Bouayed et al. 2020)\nand the SoftHebb method surpass our results on MNIST (99.3% and 99.35%,\nrespectively, compared to our 99.17%), their network structures are more complex,\nwith four and three convolutional layers, respectively, compared to two in our\ncase. Furthermore, the SoftHebb method includes batch normalization before each\nconvolutional layer, which complicates the calculation.\nConversely, for the Fashion-MNIST and SVHN datasets, we obtain better\naccuracy than PL-AE (Bouayed et al. 2020), AE (Cavallari et al. 2018), VAE (Pihlgren\net al. 2020), Hard WTA (Lagani et al. 2022), and Siamese NetworkTrosten & Sharma\n(2019), demonstrating the effectiveness of our method for unsupervised CNN training.\nFor more complex datasets like CIFAR-10 (Krizhevsky et al. 2009), further\nexploration of network architectures and normalization techniques is necessary to\nachieve better performance (He et al. 2016, Ioffe 2015, Miconi 2021, Journ´e et al.\n2022). Our preliminary results on the CIFAR-10 dataset achieved a test accuracy\nof 71.5(±0.3)% with a 96f5-384f3-5000 architecture. We believe that optimizing the\nnetwork architecture and integrating advanced normalization techniques could further\nenhance accuracy in future work.\n4.4. Semi-supervised Approach for Neuromorphic Implementations\nSemi-supervised\nlearning\nis\nideally\nsuited\nfor\nneuromorphic\nedge\ncomputing\nenvironments, characterized by limited labeled data and a wealth of unlabeled\ndata. As demonstrated in the Results section, our algorithm is well suited to this\ncontext, leveraging the available labeled data and benefiting from the generalization\ncapabilities derived from unsupervised learning. This section compares our method\nwith alternative semi-supervised strategies within the context of edge neuromorphic\ncomputing.\nLayer-wise, bio-inspired algorithms like STDP and Hebbian learning, which\ntraditionally lack feedback mechanisms, encounter challenges in implementing efficient\nsupervised learning through solely forward processes. To address this, the concept of\nweakly supervised learning has been introduced, incorporating feedback limited to\nthe network’s last layer (Gupta et al. 2022, Mozafari et al. 2018). In this approach,\nthe backward error, defined as the difference between the network’s prediction and\nthe actual label, is utilized to adjust synaptic strengths of the last layer.\nLee\n& Sim (2023) combine this weakly supervised STDP with unsupervised STDP to\nrealize semi-supervised learning in a single-layer network for semi-supervised learning,\noutperforming purely unsupervised learning. Using 30% of the labels, they achieve a\n94.95% test accuracy on MNIST. Our approach appears more powerful in this context,\nPreprint. Under review.\n23\nas our semi-supervised EP training reaches 96.18(±0.57)% test accuracy with only\n0.5% of the labels (Figure 9). Furthermore, Lee & Sim (2023) obtained less than 60%\ntest accuracy when only weakly supervised STDP is used in the purely supervised\ncontext, indicating a limited efficiency compared to backpropagation, particularly with\nplentiful labeled data.\nAnother promising avenue explored in semi-supervised learning combines forward\nbio-inspired algorithms with end-to-end training through backpropagation (Lagani\net al. 2021, Furuya & Ohkubo 2021).\nThis hybrid method markedly improves\nthe performance of bio-inspired algorithms in multi-layer fully connected networks.\nHowever, applying this approach to on-chip semi-supervised training in neuromorphic\nhardware presents challenges.\nThe distinct nature of supervised and unsupervised\nphases necessitates separate circuits, significantly complicating chip design and\nimpacting compactness.\nIn our proposed approach, the neural network circuit\nmaintains a consistent configuration across both supervised and unsupervised phases.\nThe sole modification occurs in the small circuit segment responsible for computing\nthe target, which is determined by either the label or autonomously generated at the\nnetwork’s output.\nEnd-to-end training therefore offers a more cohesive approach to semi-supervised\nlearning compared to layer-wise training, given its capacity to define a global loss in\nboth supervised and unsupervised contexts. The method benefits from the precision\nof supervised learning through gradient descent weight updates.\nBy maintaining\na consistent global loss formula for both types of training and only substituting\nthe supervised target with an unsupervised one in the absence of labels, we draw\ninspiration from the Pseudo Label (PL) method introduced by (Lee 2013). The PL\nmethod starts with supervised training using a limited number of labels and then\nemploys the pre-trained network’s predictions as pseudo labels for further unsupervised\ntraining.\nSimilarly, our approach defines these pseudo labels, comparable to our\nunsupervised target, through Winner-Take-All (WTA) selectivity, especially when the\nwinner count, k, is set to one. Moreover, we incorporate the homeostasis mechanism\ninto our target definition, enhancing semi-supervised learning performance.\nTable 5. Test accuracy (%) of semi-supervised methods over MNIST dataset.\nLearning method\n100 labels\n600 labels\n1000 labels\n3000 labels\nsupervised BP + PL (Lee 2013)\n83.85%\n94.97%\n95.70%\n97.20%\nsupervised BP + WTA + homeostasis (Ours)\n88.43(±1.60)%\n94.88(±0.35)%\n95.66(±0.27)%\n96.76(±0.11)%\nsupervised EP + WTA + homeostasis (Ours)\n88.41(±3.13)%\n96.57(±0.22)%\n96.79(±0.12)%\n97.03(±0.17)%\nIn Table 5 we compare our semi-supervised learning results with those from the\nPseudo Label (PL) method. Our approach exhibits a notable advantage when the\ncount of labeled data is less than 1000.\nFor example, with 600 labels, our semi-\nsupervised method achieves approximately 96.6% test accuracy with Equilibrium\nPreprint. Under review.\n24\nPropagation (EP) training, compared to 95% for PL. This gain largely comes from the\nhomeostasis regularization incorporated in the unsupervised target decision, which is\nparticularly effective in scenarios with very few labels. This observation aligns with\nthe indispensable role of homeostasis during our unsupervised training. However, with\nmore than 3,000 labeled data, PL performance exceeds that of our WTA by a margin\nof 0.4%, achieving an error rate of 2.8%.\nThis performance gap can be linked to\ndifferences in the initial supervised training phase, where Lee (2013) surpassed our\napproach by about 0.8%. While we employ the Adam Optimizer with an exponential\nlearning rate scheduler, Lee (2013) uses a Stochastic gradient descent optimizer\ncombined with an exponentially decaying learning rate and momentum term. This\nweight adjustment strategy seems to offer an advantage when training in a supervised\nmanner with sparse data.\nLee (2013) also proposes to integrate layer-wise unsupervised training techniques,\nsuch as the Denoising Auto-Encoder (DAE), before the first stage of supervised\ntraining. In scenarios with fewer labels, DAE is effective, boosting results by 5.7% and\n1% for 100 and 600 labels case, respectively. Such layer-wise pre-training techniques\ncould complement our method in the future to improve performance, but require\nconsiderable circuit overhead for edge computing.\nEmerging semi-supervised training methods, inspired by the pseudo label concept,\nhave shown significant promise in complex class classification tasks within the machine\nlearning (ML) domain (Laine & Aila 2016, Tarvainen & Valpola 2017, Cascante-\nBonilla et al. 2020).\nThe approach of (Asano et al. 2019) which defines a target\nusing network dynamics, closely aligns with our method.\nThese innovative ML\nstrategies hold potential for enhancing our method’s effectiveness.\nHowever, their\ndeployment demands extensive on-chip memory and peripheral circuits, rendering\nthem less suitable for low-power, compact edge AI systems, unlike our approach.\n5. Conclusion\nIn this study, we use bio-inspired mechanisms to create a self-defined target, serving as\na simple drop-in replacement for standard loss functions during unsupervised learning\nphases. This method is suitable for both semi-supervised learning and unsupervised\nend-to-end training.\nBeing training-algorithm and architecture agnostic, it holds\npromise for rapid integration into various existing hardware platforms.\nWe show competitive results on MNIST with SOTA approaches in unsupervised\nand in semi-supervised learning with both global (BP) and local learning rules\n(EP). Our approach also benefits from the inclusion of deeper fully connected layers.\nFurthermore, we show that our method achieves high accuracy for CNNs applied to\nMNIST, Fashion-MNIST and SVHN datasets. These results represent a step forward\nPreprint. Under review.\n25\ntoward edge AI hardware that can learn by leveraging both labeled and unlabeled\ndata.\n6. Appendices\n6.1. WTA selectivity\nThe Winner-Take-All (WTA) mechanism is widely used in unsupervised learning\n(Ferr´e et al. 2018, Oster et al. 2009).\nWithin a network layer, neurons compete\nagainst one another, with only the neuron exhibiting the highest activation allowed\nto reinforce its synaptic connections. In our study, WTA selectivity is employed to\nobtain the unsupervised target, which helps to further calculate the unsupervised loss.\nThe unsupervised target for a strict WTA is defined as d, where di is the ith\nelement of d, and y is the output of the network:\ndi =\n(\n1\nif yi = max(y),\n0\nif yi ̸= max(y).\n(8)\nThis approach implicitly necessitates scaling the output values between 0 and 1,\nassigning the winning neuron a target of maximum activation value (’1’) and all other\nneurons a target of minimum activation value (’0’).\nFor a large number of output neurons, selecting a single winner among all the\noutput neurons is not the optimal choice. We thus use the k-winners-take-all (kWTA)\nmethod (Majani et al. 1988):\ndi =\n(\n1\nyi ∈{k largest elements of y},\n0\nOtherwise.\n(9)\nDuring the simulation, k is treated as a hyperparameter, which is found to be\nstrongly dependent on the number of output neurons and the dropout probability for\nthe output layer. Specifically, a higher number of output neurons leads to an increased\nselection of winners k.\nHowever, relying solely on WTA selectivity for determining unsupervised targets\nproves to be ineffective. Minimizing the difference between the output value and the\nunsupervised target determined solely by WTA leads to a scenario where the initial\n’winner’ neuron ends up learning all data classes, undermining the intended goal of\nachieving distinct classification. This issue arises because the weights of the ’winner’\nneuron are strengthened after processing initial images, while the weights of other\nneurons are weakened. As a result, the other non-winning neurons become increasingly\ndisadvantaged, and in some cases, may even become silent during subsequent learning\nphases.\nPreprint. Under review.\n26\n6.2. Homeostasis\nHomeostasis is a bio-inspired regulation method often employed in STDP algorithms,\nas referenced by (Diehl & Cook 2015, Querlioz et al. 2013). At its essence, homeostasis\naims to mitigate excessive learning by individual neurons. Within the realm of SNNs,\nthis equilibrium is maintained by modulating the neurons’ threshold levels, as done\nin (Querlioz et al. 2013); here, the adjustment is critical since a neuron’s learning is\ncontingent upon its ability to spike. In contrast, ANNs function based on continuous\nvalues rather than discrete spiking events. Consequently, in ANNs—and particularly\nin the context of end-to-end training—the mere magnitude of a neuron’s activation\ndoes not dictate the occurrence of learning. Instead, learning is determined by whether\na neuron’s activation outcompetes that of its peers. Therefore, in adapting the concept\nof neural activity from SNNs to ANNs, it is the frequency with which a neuron ’wins’\nthis competitive process that should be regulated. This approach ensures a balanced\ndistribution of learning opportunities across the network, effectively mirroring the\nprinciple of homeostasis.\nIn our approach, the homeostasis term is applied to each output neuron; if a\nparticular output neuron is consistently selected as the ’winner’ too many times, we\nincrement its homeostasis term H; otherwise, we decrement the term H. The update\nto the homeostasis term is performed after each weight update, following the rule\noutlined below:\n∆H = γ(A −T ).\n(10)\nHere, A is the mean activity, T is the target activity, and γ is a multiplicative constant.\nThe target activity T is defined by a fixed value\nk\nNoutput for all the output neurons.\nThe mean activity A is calculated as a moving average of the unsupervised target.\nIn the following, < d >P represents the moving average of target d over P examples.\nWe considered two types of moving average:\n• Exponential moving average :\nA =< d >P = ηd + (1 −η) < d >P −1 .\n(11)\nIn the exponential moving average (EMA), we take the average of all previously\npresented examples, which means that P represents the total number of presented\nimages.\nIn simulations employing EMA, this approach is referred to as the\nsequential mode, as the input data is provided one example at a time.\n• Classical moving average:\nA =< d >P =\nP\nP d\nP\n.\n(12)\nIn the classical moving average (CMA), we take the average of P examples, where\nP is a fixed number. This moving average calculation can be easily implemented in\nthe mini-batch mode computation and is referred as batch mode in the following.\nPreprint. Under review.\n27\nIn the simulations, both types of moving averages—exponential and classical—exhibit\nsimilar performances (Figure 12), sequential mode using the EMA has a slightly better\nperformance than the batch mode using the CMA. We opt for the classical moving\naverage to test our algorithm because the mini-batch mode facilitates parallelization\nof calculations.\nIn real-world applications, the choice of moving average can be\ncustomized to suit the specific requirements of practical problems.\n0\n10\n20\n30\n40\n50\n60\nEpochs\n6\n8\n10\n12\n14\nTest Error Rate (%)\n(A) Unsupervised BP\n0\n5\n10\n15\n20\n25\n30\nEpochs\n6\n8\n10\n12\n14\n(B) Unsupervised EP\nOne-layer sequential mode\nOne-layer batch mode\nTwo-layer sequential mode\nTwo-layer batch mode\nFigure 12.\nMNIST test error rate (%) comparison between batch and sequential mode for different\nnetworks trained with (A) unsupervised BP and (B) unsupervised EP. The one-layer network has 500\noutput neurons. A hidden layer with 1024 neurons is added in the two-layer unsupervised network.\nThe hyperparameter γ and η do not have a large influence on the accuracy. For\nboth EP and BP optimized γ is about 0.5.\n6.3. Influence of Output Neuron Count\nThe effectiveness of unsupervised learning, facilitated by WTA selectivity, is critically\ndependent on the number of output neurons participating in the competition (Querlioz\net al. 2013, Diehl & Cook 2015). Here, we examine how the count of output neurons\nimpacts classification accuracy.\nThe graph in Figure 13 demonstrates that the unsupervised learning performance\nis significantly influenced by the number of output neurons. Specifically, when the\nnumber of output neurons is limited to 10, the test accuracy achieved is approximately\n60%. This is attributed to inadequate class representation, where not all classes are\ndistinctly represented by the available neurons. As the number of output neurons\nincreases, each class is more likely to be uniquely represented, enhancing classification\naccuracy.\nFor example, with 500 output neurons, a test accuracy of 93.4% for\nunsupervised EP and 93.3% for unsupervised BP is achieved, which is only 2.4%\nand 2.5% lower, respectively, than the highest accuracy obtained with 2000 output\nPreprint. Under review.\n28\nDirect association\n10\n100\n500 1000 2000\nNumber of output neurons\n60\n70\n80\n90\n95\nTest Accuracy (%)\nUnsupervised EP 1layer\nUnsupervised BP 1layer\nFigure 13.\nPerformance comparison of a single unsupervised layer using different numbers of\nneurons at the output, trained either with EP or with BP. Following unsupervised training, a direct\nassociation is performed with 2% of the training data. Left: Structure of the single-layer network;\nRight: Test accuracy on the MNIST dataset as a function of the number of neurons in the output\nlayer.\nneurons.\n6.4. Dropout Regularization\nIn our study, we employed dropout regularization — a technique traditionally utilized\nin machine learning to counteract overfitting (Nitish et al. 2014) — and observed a\nmarginal enhancement in model performance. We treated the dropout probability as\na hyperparameter, optimizing its value through grid research between 0, 0.1, 0.2, 0.3,\n0.4 and 0.5.\nFor unsupervised training with BP and EP, the optimal dropout probabilities were\napproximately 0.2 for output neurons and zero for hidden neurons. These findings\nsuggest that within the context of unsupervised learning, the primary role of dropout\nmay be to balance neuron activity rather than to prevent overfitting. Following the\nmethodology used by Hinton et al. (2012), dropout is also employed on the input\nneurons with a fixed probability of 0.3.\nFor semi-supervised learning, dropout with probabilities of 0.3 and 0.5 was applied\nto the input and hidden neurons, respectively.\nGiven that the number of output\nneurons corresponds to the number of classes, we did not apply dropout to the output\nlayer in this case.\nPreprint. Under review.\n29\n6.5. Label Smoothing\nWe use label smoothing to penalize the overconfidence (Pereyra et al. 2017) of\nunsupervised targets :\n˜d = d ∗(1 −α) + T ∗α,\n(13)\nWe maintain the same notations as previously introduced:\nd represents the\nunsupervised target vector, and T denotes the target activity vector. Each element\nof T has the same value, given by\nk\nNoutput , for all output neurons. This technique\nmoderates the tendency of output neurons to assume extreme values of ’1’ or\n’0’.\nInstead, it adjusts k winner neurons towards a more moderated value, while\ndiminishing the inhibition on other neurons. The sum of the adjusted unsupervised\ntarget ˜d remains k. In our simulations, the smoothing factor α is set to 0.3.\n6.6. Hyperparameters\nIn our study, the hyperparameters are searched using Random Search with the Optuna\nframework (Akiba et al. 2019).\n6.6.1. Unsupervised Learning\nFor unsupervised learning, mean squared error (MSE)\nis taken as the loss function, and the gradients are updated by stochastic gradient\ndescent (SGD).\nFor activation functions:\n• In unsupervised EP, hardsigmoid is taken as the activation function for all the\nlayers.\n• In unsupervised BP, ReLu is the hidden layer activation function,\nand\nhardsigmoid is the output layer activation function.\nrelu(x)\n= max(0, x)\nhardsigm(x) = min(max(0, x), 1)\n(14)\nWe train separately a single unsupervised layer with 2,000 output neurons,\nand a two-layer unsupervised network with 2,000 added hidden neurons. Dropout\nregularization is applied to both the input and output layers with probabilities of\n0.3 and 0.2, respectively. To improve convergence during unsupervised training, we\nemploy a linear learning rate scheduler that starts with a ratio of 1 and decreases to\n5e-4. Other hyperparameters are shown in Table 6.\nPreprint. Under review.\n30\nTable 6.\nHyperparameters of one-layer / two-layer network trained by unsupervised EP/BP.\nHyperparameters\nEP one-layer\nEP two-layer\nBP one-layer\nBP two-layer\nT\n40\n60\n-\n-\nK\n10\n20\n-\n-\nβ\n0.2\n0.2\n-\n-\nWinners k\n5\n4\n6\n5\nγ\n0.3\n0.65\n0.4\n0.5\nBatch Size\n16\n32\n16\n16\nLR\n0.03\n0.01-0.015\n8\n2.5-5\nEpoch\n100\n100\n200\n200\nLabel Smoothing\nYes\nYes\nNo\nNo\nFollowing the unsupervised training, for the linear classifier association method,\nwe append a supplementary linear classifier to the terminal layer of the unsupervised\nnetwork, which enables the network to make predictions during inference.\nThe\ninput neuron count for this perceptron matches the output neuron count of the final\nunsupervised layer, while its output neuron count aligns with the number of classes.\nTo train the auxiliary linear classifier, we use Cross-Entropy loss for error\ncomputation and optimize it with the Adam algorithm. The hyperparameters required\nfor training classifiers exhibit significant differences, depending on whether BP or EP\nis used as the unsupervised learning algorithm. The classifier is trained for 50 epochs\nwhen the unsupervised network is trained using BP and 100 epochs when trained\nusing EP. A learning rate scheduler is employed during classifier training. For the BP-\ntrained unsupervised network, the classifier learning rate undergoes exponential decay\nat each epoch with a decay factor of 0.9. Conversely, for the EP-trained unsupervised\nnetwork, the classifier rate undergoes linear decay from a factor of 1 to 0.5 over the\ncourse of all training epochs.\nThe classifier learning rate for the different label numbers used for class\nassociation is detailed in Table 7.\nTable 7.\nLearning rate for the added linear classifier with different amount of available labels.\n1% labels\n5% labels\n50% labels\n100% labels\nBP unsupervisedly trained\n0.05\n0.065\n0.09\n0.1\nEP unsupervisedly trained\n0.0085\n0.008\n0.005\n0.0022\n6.6.2.\nConvolutional Layers The convolutional network architecture is shown in\nTable 8.\nNo pre-processing or normalization was applied to any of the MNIST,\nFashion-MNIST and SVHN. Random flip transformation is applied to the CIFAR-\n10 dataset.\nPreprint. Under review.\n31\nTable 8.\nConvolutional network architecture used in both supervised and unsupervised learning.\n#layer\nMNIST/Fashion-MNIST/SVHN\nCIFAR-10\n5x5 conv32 padding=2 stride=1\n5x5 conv96 padding=2 stride=1\n1\nRelu\nWTA\n4x4 Max/AvPool padding=1 stride=2\n4x4 MaxPool padding=1 stride=2\n3x3 conv128 padding=1 stride=1\n3x3 conv384 padding=1 stride=1\n2\nRelu\nWTA\n4x4 Max/AvPool padding=1 stride=2\n4x4 MaxPool padding=1 stride=2\nDropout=0.2\nDropout = 0.3\n3\nFC layer with 3000 neurons\nFC layer with 5000 neurons\nDropout=0.3\nDropout=0.5\nFor the MNIST dataset, we use average pooling after the convolutional layer,\nwhile for the other datasets, we use max pooling. The Adam optimizer is used to\nupdate the gradients, and no label smoothing is applied. For the CNNs training, the\nwinner number k is set at 1, and the homeostasis coefficient γ is set at 1 as well.\nFor CIFAR-10, we use a new activation function inspired by the Triangle method\n(Coates et al. 2011), referred to as ‘WTA’ in the table. This activation function first\nsubtracts the mean activation of each channel across all positions, rectifies negative\nvalues to 0, then subtracts the mean activation at each position across all channels,\nand rectifies negative values to 0 again. This procedure effectively performs two rounds\nof winner-take-all: first, across neurons within the same channel, and second, across\nneurons at the same position.\nThe weights of CNNs are initially pruned by 30% for all datasets except CIFAR-\n10, where 80% pruning is applied to introduce greater sparsity into the network,\nfollowing the approach in Miconi (2021). Dropout is applied both before and after\nthe fully connected layer, which serves as the output layer in the CNN. All the other\nhyperparameters for CNN training can be found in Table 9.\nTable 9.\nHyperparameters of CNNs trained by unsupervised BP.\nHyperparameters\nMNIST\nFashion-MNIST\nSVHN\nCIFAR10\nBatch Size\n16\n16\n16\n10\nLR\n(6e-7, 3e-8, 3e-7)\n(5e-7, 3e-8, 3e-6)\n(6e-9, 4e-9, 2e-7)\n(3e-6, 7e-7, 8e-5)\nEpoch\n30\n20\n40\n40\n6.6.3.\nSemi-supervised Learning In our semi-supervised learning approach, we\nemploy the cross-entropy loss function to compute the output error. Gradient updates\nare performed using the Adam optimizer.\nFor activation function:\n• In the EP paradigm, we use the hardsigmoid function as the activation for all\nlayers.\nPreprint. Under review.\n32\n• In the BP paradigm, the hidden layer utilizes the ReLU activation function, while\nthe sigmoid function is adopted for the output layer.\nOur training commences with N1 epochs of supervised learning, using limited\nlabeled datasets of sizes 100, 300, 600, 1000, and 3000, respectively, in four distinct\nscenarios.\nThis is followed by an amalgamation of supervised and unsupervised\ntraining over the subsequent N2 epochs.\nThe learning rate’s evolution, especially its ratio between supervised and\nunsupervised learning, plays a pivotal role in determining the final model accuracy,\nas discussed in Lee (2013). To regulate this, we employ an exponential learning rate\nscheduler during the initial N1 epochs of supervised pre-training.\nSubsequently, a\nlinear learning rate scheduler is utilized for the following N2 epochs, encompassing\nboth supervised and unsupervised training phases.\n• During supervised pre-training, the network is trained using a limited set of\nlabeled data for N1 epochs. The initial learning rate for this phase is referred\nto as the Pre-training LR in Table 10. This learning rate undergoes exponential\ndecay with a factor of 0.97 throughout the N1 epoch.\n• For the subsequent N2 epochs of alternating supervised and unsupervised\ntraining, two distinct linear learning rate schedulers are applied separately for\nsupervised and unsupervised training.\nThe initial learning rate for these N2\nepochs is referred to as the Semi-supervised LR in Table 10. During supervised\nre-training, the learning rate begins at 0.7 times the Semi-supervised LR and\ngradually decreases to a factor of 0.05. Concurrently, for unsupervised training,\nthe learning rate initiates at 0.001 times the Semi-supervised LR and concludes\nat a factor of 0.18.\nThe mini-batch size for supervised learning is 32, while for unsupervised learning\nis 256. Dropout is applied with 0.2 and 0.5 probabilities for the input layer and hidden\nlayer separately. These are applicable to both EP and BP learning methods.\nPreprint. Under review.\n33\nTable 10.\nHyperparameters of semi-supervised learning by EP/BP with different labels numbers.\nThere is always 5,000 neurons in the hidden unsupervised layer, and 10 neurons in the output layer.\nHyperparameters\n100 labels\n300 labels\n600 labels\n1000 labels\n3000 labels\nT\n60\n60\n60\n60\n60\nK\n10\n10\n10\n10\n10\nβ\n0.48\n0.4\n0.38\n0.4\n0.48\nγ\n0.1\n0.1\n0.005\n0.001\n0.001\nSemi-supervised EP\nEpoch N1\n100\n100\n100\n100\n100\nEpoch N2\n200\n200\n200\n200\n200\nPre-training LR\n0.001-0.0035\n0.001-0.0035\n0.001-0.0035\n0.001-0.0035\n0.001-0.0035\nSemi-supervised LR\n0.0003-0.009\n0.0004-0.008\n0.0002-0.001\n0.0002-0.006\n0.00015-0.009\nγ\n0.1\n0.1\n0.003\n0.001\n0.001\nEpoch N1\n200\n200\n200\n200\n200\nEpoch N2\n400\n400\n400\n400\n400\nSemi-supervised BP\nPre-training LR\n0.001-0.003\n0.001-0.003\n0.001-0.003\n0.001-0.003\n0.001-0.003\nSemi-supervised LR\n0.0006-0.0044\n0.0005-0.004\n0.0001-0.002\n0.00024-0.003\n0.0006-0.0008\nConflict of Interest Statement\nThe authors declare that the research was conducted in the absence of any commercial\nor financial relationships that could be construed as a potential conflict of interest.\nAuthor Contributions\nThe project was designed by JG and DL with the participation of DQ and JL. The\ncode and numerical simulations were all carried out by DL. AP contributed to semi-\nsupervised learning tests.\nDL, JG, JL and DQ participated to the writing of the\nmanuscript and all the authors discussed and reviewed the content.\nFunding\nThis work was funded by the European Research Council advanced grant GrenaDyn\n(reference: 101020684) and the ANR project SpinSpike ANR-20-CE24-0002-02.\nAcknowledgments\nThis project was supported with AI computing and storage resources by GENCI at\nIDRIS thanks to Grant 2023-AD010913993 on the supercomputer Jean Zay’s V100\npartition.\nThe text of the article was partially edited by a large language model\n(OpenAI ChatGPT). We greatly appreciate Dr. Xing Chen’s constructive suggestions\nregarding the CIFAR-10 training process.\nREFERENCES\n34\nSupplemental Data\nThe programming code utilized in this study has been made publicly available online\nunder an open-source license:\nhttps://github.com/neurophysics-cnrsthales/\nunsupervised-target.\nReferences\nAkiba, T., Sano, S., Yanase, T., Ohta, T. & Koyama, M. (2019), Optuna:\nA\nnext-generation hyperparameter optimization framework, in ‘Proceedings of the\n25th ACM SIGKDD International Conference on Knowledge Discovery and Data\nMining’, p. 2623–2631.\nAsano, Y. M., Rupprecht, C. & Vedaldi, A. (2019), ‘Self-labelling via simultaneous\nclustering and representation learning’, arXiv preprint arXiv:1911.05371 .\nAzghadi, M. R., Moradi, S., Fasnacht, D. B., Ozdas, M. S. & Indiveri, G. (2015),\n‘Programmable spike-timing-dependent plasticity learning circuits in neuromorphic\nvlsi architectures’, ACM Journal on Emerging Technologies in Computing Systems\n(JETC) 12(2), 1–18.\nBalestriero, R., Ibrahim, M., Sobal, V., Morcos, A., Shekhar, S., Goldstein, T., Bordes,\nF., Bardes, A., Mialon, G., Tian, Y., Schwarzschild, A., Wilson, A. G., Geiping,\nJ., Garrido, Q., Fernandez, P., Bar, A., Pirsiavash, H., LeCun, Y. & Goldblum, M.\n(2023), ‘A Cookbook of Self-Supervised Learning’.\nBardes, A., Ponce, J. & LeCun, Y. (2021), Vicreg: Variance-invariance-covariance\nregularization for self-supervised learning, in ‘10th International Conference on\nLearning Representations, ICLR 2022’, Vol. abs/2105.04906.\nBarhoush, M., Ayad, A. & Schmeink, A. (2022), Semi-supervised algorithms in\nresource-constrained edge devices: An overview and experimental comparison, in\n‘2022 IEEE International Conferences on Internet of Things (iThings) and IEEE\nGreen Computing & Communications (GreenCom) and IEEE Cyber, Physical\n& Social Computing (CPSCom) and IEEE Smart Data (SmartData) and IEEE\nCongress on Cybermatics (Cybermatics)’, pp. 555–559.\nBengio, Y., Lamblin, P., Popovici, D. & Larochelle, H. (2006), ‘Greedy layer-wise\ntraining of deep networks’, Advances in neural information processing systems 19.\nBi, G.-q. & Poo, M.-m. (1998), ‘Synaptic modifications in cultured hippocampal\nneurons: dependence on spike timing, synaptic strength, and postsynaptic cell type’,\nJournal of neuroscience 18(24), 10464–10472.\nBichler, O., Querlioz, D., Thorpe, S. J., Bourgoin, J.-P. & Gamrat, C. (2012),\n‘Extraction of temporally correlated features from dynamic vision sensors with\nspike-timing-dependent plasticity’, Neural networks 32, 339–348.\nREFERENCES\n35\nBouayed, A. M., Atif, K., Deriche, R. & Saim, A. (2020), ‘A pseudo-labelling auto-\nencoder for unsupervised image classification’, arXiv preprint arXiv:2012.03322 .\nCao, K., Liu, Y., Meng, G. & Sun, Q. (2020), ‘An overview on edge computing\nresearch’, IEEE Access 8, 85714–85728.\nCascante-Bonilla, P., Tan, F., Qi, Y. & Ordonez, V. (2020), ‘Curriculum labeling:\nRevisiting pseudo-labeling for semi-supervised learning’.\nCavallari, G. B., Ribeiro, L. S. & Ponti, M. A. (2018), Unsupervised representation\nlearning using convolutional and stacked auto-encoders: A domain and cross-domain\nfeature space analysis, in ‘2018 31st SIBGRAPI Conference on Graphics, Patterns\nand Images (SIBGRAPI)’, pp. 440–446.\nChen, J. & Ran, X. (2019), ‘Deep learning with edge computing:\nA review’,\nProceedings of the IEEE 107(8), 1655–1674.\nCoates, A., Ng, A. & Lee, H. (2011), An analysis of single-layer networks in\nunsupervised feature learning, in G. Gordon, D. Dunson & M. Dud´ık, eds,\n‘Proceedings of the Fourteenth International Conference on Artificial Intelligence\nand Statistics’, Vol. 15 of Proceedings of Machine Learning Research, PMLR, Fort\nLauderdale, FL, USA, pp. 215–223.\nDiehl, P. U. & Cook, M. (2015), ‘Unsupervised learning of digit recognition using\nspike-timing-dependent plasticity’, Frontiers in Computational Neuroscience 9.\nDillavou, S., Stern, M., Liu, A. J. & Durian, D. J. (2022), ‘Demonstration of\ndecentralized physics-driven learning’, Physical Review Applied 18(1), 014040.\nDo, H. (1949), ‘The organization of behavior’, New York .\nErhan, D., Bengio, Y., Courville, A. & Vincent, P. (2009), ‘Visualizing higher-layer\nfeatures of a deep network’, Technical Report, Univerist´e de Montr´eal .\nErko¸c, T. & Eskıl, M. T. (2023), ‘A novel similarity based unsupervised technique for\ntraining convolutional filters’, IEEE Access 11, 49393–49408.\nErnoult, M., Grollier, J., Querlioz, D., Bengio, Y. & Scellier, B. (2019), ‘Updates of\nequilibrium prop match gradients of backprop through time in an rnn with static\ninput’, Advances in neural information processing systems 32.\nErnoult, M., Grollier, J., Querlioz, D., Bengio, Y. & Scellier, B. (2020), ‘Equilibrium\npropagation with continual weight updates’, arXiv preprint arXiv:2005.04168 .\nFerr´e, P., Mamalet, F. & Thorpe, S. J. (2018), ‘Unsupervised feature learning with\nwinner-takes-all based stdp’, Frontiers in Computational Neuroscience 12, 1–12.\nFuruya, K. & Ohkubo, J. (2021), ‘Semi-supervised learning combining backpropaga-\ntion and stdp: Stdp enhances learning by backpropagation with a small amount of\nlabeled data in a spiking neural network’, Journal of the Physical Society of Japan\n90(7), 074802.\nREFERENCES\n36\nGupta, M., Modi, S. K., Zhang, H., Lee, J. H. & Lim, J. H. (2022), ‘Is bio-inspired\nlearning better than backprop? benchmarking bio learning vs. backprop’, arXiv\npreprint arXiv:2212.04614 .\nHe, K., Zhang, X., Ren, S. & Sun, J. (2016), Deep residual learning for image\nrecognition, in ‘Proceedings of the IEEE conference on computer vision and pattern\nrecognition’, pp. 770–778.\nHinton, G. E., Srivastava, N., Krizhevsky, A., Sutskever, I. & Salakhutdinov,\nR. R. (2012), ‘Improving neural networks by preventing co-adaptation of feature\ndetectors’, CoRR abs/1207.0580.\nin Yi, S., Kendall, J. D., Williams, R. S. & Kumar, S. (2022), ‘Activity-difference\ntraining of deep neural networks using memristor crossbars’, Nature Electronics .\nIndiveri, G., Chicca, E. & Douglas, R. (2006), ‘A vlsi array of low-power spiking\nneurons and bistable synapses with spike-timing dependent plasticity’, IEEE\nTransactions on Neural Networks 17(1), 211–221.\nIoffe, S. (2015), ‘Batch normalization: Accelerating deep network training by reducing\ninternal covariate shift’, arXiv preprint arXiv:1502.03167 .\nIshii, M., Kim, S., Lewis, S., Okazaki, A., Okazawa, J., Ito, M., Rasch, M., Kim,\nW., Nomura, A., Shin, U. et al. (2019), On-chip trainable 1.4 m 6t2r pcm synaptic\narray with 1.6 k stochastic lif neurons for spiking rbm, in ‘2019 IEEE International\nElectron Devices Meeting (IEDM)’, IEEE, pp. 14–2.\nJo, S. H., Chang, T., Ebong, I., Bhadviya, B. B., Mazumder, P. & Lu, W. (2010),\n‘Nanoscale memristor device as synapse in neuromorphic systems’, Nano letters\n10(4), 1297–1301.\nJourn´e, A., Rodriguez, H. G., Guo, Q. & Moraitis, T. (2022), ‘Hebbian deep learning\nwithout feedback’, arXiv preprint arXiv:2209.11883 .\nKendall, J., Pantone, R., Manickavasagam, K., Bengio, Y. & Scellier, B. (2020),\n‘Training end-to-end analog neural networks with equilibrium propagation’, arXiv\npreprint arXiv:2006.01981 .\nKrizhevsky, A., Nair, V. & Hinton, G. (2009), Learning multiple layers of features\nfrom tiny images, Technical report, University of Toronto.\nKrotov, D. & Hopfield, J. J. (2019), ‘Unsupervised learning by competing hidden\nunits’, Proceedings of the National Academy of Sciences of the United States of\nAmerica 116, 7723–7731.\nLaborieux, A., Ernoult, M., Scellier, B., Bengio, Y., Grollier, J. & Querlioz, D.\n(2021), ‘Scaling equilibrium propagation to deep convnets by drastically reducing\nits gradient estimator bias’, Frontiers in Neuroscience 15.\nREFERENCES\n37\nLaborieux, A. & Zenke, F. (2022), ‘Holomorphic equilibrium propagation computes\nexact gradients through finite size oscillations’, Advances in Neural Information\nProcessing Systems 35, 12950–12963.\nLagani, G., Falchi, F., Gennaro, C. & Amato, G. (2021), Evaluating hebbian learning\nin a semi-supervised setting, in ‘International Conference on Machine Learning,\nOptimization, and Data Science’, Springer, pp. 365–379.\nLagani, G., Falchi, F., Gennaro, C. & Amato, G. (2022), ‘Comparing the performance\nof hebbian against backpropagation learning using convolutional neural networks’,\nNeural Computing and Applications 34(8), 6503–6519.\nLagani, G., Falchi, F., Gennaro, C. & Amato, G. (2023), ‘Synaptic plasticity\nmodels and bio-inspired unsupervised deep learning: A survey’, arXiv preprint\narXiv:2307.16236 .\nLaine, S. & Aila, T. (2016), Temporal ensembling for semi-supervised learning, in\n‘International Conference on Learning Representations’, pp. 1–13.\nLaydevant, J., Markovic, D. & Grollier, J. (2023), ‘Training an ising machine with\nequilibrium propagation’, arXiv preprint arXiv:2305.18321 .\nLeCun, Y., Cortes, C. & Burges, C. (2010), ‘Mnist handwritten digit database’, ATT\nLabs [Online]. Available: http://yann.lecun.com/exdb/mnist 2.\nLee, D.-H. (2013), ‘Pseudo-label : The simple and efficient semi-supervised learning\nmethod for deep neural networks’,\nICML 2013 Workshop :\nChallenges in\nRepresentation Learning (WREPL) .\nLee, J. & Sim, D. (2023), ‘Semi-supervised learning for spiking neural networks based\non spike-timing-dependent plasticity’, IEEE Access 11, 35140–35149.\nMaass,\nW. (2000),\n‘On the computational power of winner-take-all’,\nNeural\ncomputation 12(11), 2519–2535.\nMajani, E., Erlanson, R. & Abu-Mostafa, Y. (1988), On the k-winners-take-all\nnetwork, in D. Touretzky, ed., ‘Advances in Neural Information Processing Systems’,\nVol. 1, Morgan-Kaufmann, pp. 634–642.\nMartin, E., Ernoult, M., Laydevant, J., Li, S., Querlioz, D., Petrisor, T. &\nGrollier, J. (2021), ‘Eqspike: spike-driven equilibrium propagation for neuromorphic\nimplementations’, Iscience 24(3).\nMcInnes, L., Healy, J. & Melville, J. (2018), ‘Umap: Uniform manifold approximation\nand projection for dimension reduction’, arXiv preprint arXiv:1802.03426 .\nMiconi, T. (2021), ‘Hebbian learning with gradients: Hebbian convolutional neural\nnetworks with modern deep learning frameworks’, arXiv preprint arXiv:2107.01729\n.\nREFERENCES\n38\nMoraitis, T., Toichkin, D., Journ´e, A., Chua, Y. & Guo, Q. (2022), ‘Softhebb:\nBayesian\ninference\nin\nunsupervised\nhebbian\nsoft\nwinner-take-all\nnetworks’,\nNeuromorphic Computing and Engineering 2(4), 044017.\nMozafari, M., Kheradpisheh, S. R., Masquelier, T., Nowzari-Dalini, A. & Ganjtabesh,\nM. (2018), ‘Bio-inspired digit recognition using reward-modulated stdp’, IEEE\ntransactions on neural networks and learning systems 30(3), 613–628.\nNessler, B., Pfeiffer, M., Buesing, L. & Maass, W. (2013), ‘Bayesian computation\nemerges in generic cortical microcircuits through spike-timing-dependent plasticity’,\nPLoS Computational Biology 9.\nNetzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B. & Ng, A. Y. (2011), Reading\ndigits in natural images with unsupervised feature learning, in ‘NIPS Workshop on\nDeep Learning and Unsupervised Feature Learning 2011’.\nNitish, S., Geoffrey, H., Alex, K., Ilya, S. & Ruslan, S. (2014), ‘Dropout: A simple\nway to prevent neural networks from overfitting’, The Journal of Machine Learning\nResearch 15, 1929–1958.\nOja, E. (1982), ‘Simplified neuron model as a principal component analyzer’, Journal\nof mathematical biology 15, 267–273.\nOster, M., Douglas, R. & Liu, S.-C. (2009), ‘Computation with spikes in a winner-\ntake-all network’, Neural Computation 21(9), 2437–2465. Publisher: MIT Press.\nPereyra, G., Tucker, G., Chorowski, J.,  Lukasz Kaiser & Hinton, G. (2017),\n‘Regularizing neural networks by penalizing confident output distributions’, 5th\nInternational Conference on Learning Representations, ICLR 2017 - Workshop\nTrack Proceedings .\nPihlgren, G. G., Sandin, F. & Liwicki, M. (2020), Improving image autoencoder\nembeddings with perceptual loss, in ‘2020 International Joint Conference on Neural\nNetworks (IJCNN)’, IEEE, pp. 1–7.\nQuerlioz, D., Bichler, O., Dollfus, P. & Gamrat, C. (2013), ‘Immunity to device\nvariations in a spiking neural network with memristive nanodevices’, IEEE\nTransactions on Nanotechnology 12(3), 288–295.\nScellier, B. & Bengio, Y. (2017), ‘Equilibrium propagation:\nBridging the gap\nbetween energy-based models and backpropagation’, Frontiers in Computational\nNeuroscience 11.\nScellier, B., Ernoult, M., Kendall, J. & Kumar, S. (2024), ‘Energy-based learning\nalgorithms for analog computing:\na comparative study’, Advances in Neural\nInformation Processing Systems 36.\nShakarami, A., Ghobaei-Arani, M. & Shahidinejad, A. (2020), ‘A survey on the\nREFERENCES\n39\ncomputation offloading approaches in mobile edge computing: A machine learning-\nbased perspective’, Computer Networks 182.\nSiddiqui, S. A., Krueger, D., LeCun, Y. & Deny, S. (2022), ‘Blockwise self-supervised\nlearning at scale’, arXiv preprint .\nTarvainen, A. & Valpola, H. (2017), ‘Mean teachers are better role models:\nWeight-averaged consistency targets improve semi-supervised deep learning results’,\nAdvances in neural information processing systems 30.\nThomas, P. (2009), ‘Semi-supervised learning edited by o. chapelle, b. sch¨olkopf and\na. zien’, IEEE Transactions on Neural Networks 20(3), 542.\nTrosten, D. J. & Sharma, P. (2019), Unsupervised feature extraction–a cnn-\nbased approach, in ‘Image Analysis: 21st Scandinavian Conference, SCIA 2019,\nNorrk¨oping, Sweden, June 11–13, 2019, Proceedings 21’, Springer, pp. 197–208.\nVan Engelen, J. E. & Hoos, H. H. (2020), ‘A survey on semi-supervised learning’,\nMachine learning 109(2), 373–440.\nXiao, H., Rasul, K. & Vollgraf, R. (2017), ‘Fashion-mnist: a novel image dataset for\nbenchmarking machine learning algorithms’.\nXiao, Z., Yan, B., Zhang, T., Huang, R. & Yang, Y. (2022), ‘Memristive devices based\nhardware for unlabeled data processing’, Neuromorphic Computing and Engineering\n2(2), 022003.\nYarowsky, D. (1995), Unsupervised word sense disambiguation rivaling supervised\nmethods, in ‘33rd annual meeting of the association for computational linguistics’,\npp. 189–196.\nZbontar, J., Jing, L., Misra, I., LeCun, Y. & Deny, S. (2021), Barlow twins:\nSelf-supervised learning via redundancy reduction, in ‘Proceedings of the 38th\nInternational Conference on Machine Learning’, PMLR, pp. 12310–12320. ISSN:\n2640-3498.\nZhou,\nH. (2022),\n‘Activation learning by local competitions’,\narXiv preprint\narXiv:2209.13400 .\nZhu, X. & Goldberg, A. B. (2022), Introduction to semi-supervised learning, Springer\nNature.\n",
  "categories": [
    "cs.NE",
    "cs.ET",
    "cs.LG"
  ],
  "published": "2024-03-18",
  "updated": "2024-11-21"
}