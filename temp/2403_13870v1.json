{
  "id": "http://arxiv.org/abs/2403.13870v1",
  "title": "ExMap: Leveraging Explainability Heatmaps for Unsupervised Group Robustness to Spurious Correlations",
  "authors": [
    "Rwiddhi Chakraborty",
    "Adrian Sletten",
    "Michael Kampffmeyer"
  ],
  "abstract": "Group robustness strategies aim to mitigate learned biases in deep learning\nmodels that arise from spurious correlations present in their training\ndatasets. However, most existing methods rely on the access to the label\ndistribution of the groups, which is time-consuming and expensive to obtain. As\na result, unsupervised group robustness strategies are sought. Based on the\ninsight that a trained model's classification strategies can be inferred\naccurately based on explainability heatmaps, we introduce ExMap, an\nunsupervised two stage mechanism designed to enhance group robustness in\ntraditional classifiers. ExMap utilizes a clustering module to infer\npseudo-labels based on a model's explainability heatmaps, which are then used\nduring training in lieu of actual labels. Our empirical studies validate the\nefficacy of ExMap - We demonstrate that it bridges the performance gap with its\nsupervised counterparts and outperforms existing partially supervised and\nunsupervised methods. Additionally, ExMap can be seamlessly integrated with\nexisting group robustness learning strategies. Finally, we demonstrate its\npotential in tackling the emerging issue of multiple shortcut\nmitigation\\footnote{Code available at \\url{https://github.com/rwchakra/exmap}}.",
  "text": "ExMap: Leveraging Explainability Heatmaps for Unsupervised Group\nRobustness to Spurious Correlations\nRwiddhi Chakraborty, Adrian Sletten, Michael C. Kampffmeyer\nDepartment of Physics and Technology, UiT The Arctic University of Norway\nfirstname[.middle initial].lastname@uit.no\nAbstract\nGroup robustness strategies aim to mitigate learned bi-\nases in deep learning models that arise from spurious cor-\nrelations present in their training datasets. However, most\nexisting methods rely on the access to the label distribution\nof the groups, which is time-consuming and expensive to\nobtain. As a result, unsupervised group robustness strate-\ngies are sought. Based on the insight that a trained model’s\nclassification strategies can be inferred accurately based on\nexplainability heatmaps, we introduce ExMap, an unsuper-\nvised two stage mechanism designed to enhance group ro-\nbustness in traditional classifiers. ExMap utilizes a cluster-\ning module to infer pseudo-labels based on a model’s ex-\nplainability heatmaps, which are then used during training\nin lieu of actual labels. Our empirical studies validate the\nefficacy of ExMap - We demonstrate that it bridges the per-\nformance gap with its supervised counterparts and outper-\nforms existing partially supervised and unsupervised meth-\nods. Additionally, ExMap can be seamlessly integrated with\nexisting group robustness learning strategies. Finally, we\ndemonstrate its potential in tackling the emerging issue of\nmultiple shortcut mitigation1.\n1. Introduction\nDeep neural network classifiers trained for classification\ntasks, have invited increased scrutiny from the research\ncommunity due to their overreliance on spurious correla-\ntions present in the training data [4, 5, 9, 31, 38]. This is\nrelated to the broader aspect of Shortcut Learning [10], or\nthe Clever Hans effect [15], where a model picks the path\nof least resistance to predict data, thus relying on shortcut\nfeatures that are not causally linked to the label. The con-\nsequence of this phenomenon is that, although such models\nmay demonstrate impressive mean accuracy on the test data,\nthey may still fail on challenging subsets of the data, i.e. the\ngroups [7, 8, 27]. As a result, group robustness is a natural\n1Code available at https://github.com/rwchakra/exmap\nobjective to be met to mitigate reliance on spurious corre-\nlations. Thus, instead of evaluating models based on mean\ntest accuracy, evaluating them on worst group accuracy has\nbeen the recent paradigm [12, 21, 25, 40], resulting in the\nemergence of group robustness techniques. By dividing a\ndataset into pre-determined groups of spurious correlations,\nclassifiers are then trained to maximize the worst group ac-\ncuracy - As a result, the spurious attribute that the model is\nmost susceptible to is considered the shortcut of interest.\nIn Figure 1, we illustrate the group robustness paradigm.\nGiven a dataset, a robustness strategy takes as input the\ngroup labels and retrains a base classifier (such as Expected\nRisk Minimization, i.e. ERM) to improve the worst group\naccuracy (G3 in this case). GroupDRO [28] was one of the\nearly influential works that introduced the group robustness\nparadigm. Further, it demonstrated a strategy that could in-\ndeed improve worst group accuracy. One limitation of this\napproach was the reliance on group labels in the training\ndata, which was replaced with the reliance on group labels\nin the validation data in successive works [13, 19]. How-\never, while these efforts have made strides in enhancing the\naccuracy of trained classifiers for underperforming groups,\nmany hinge on the assumption that the underlying groups\nare known apriori and that the group labels are available,\nwhich is often impractical in real-world contexts. An unsu-\npervised approach, as illustrated in Figure 1, would ideally\nestimate pseudo-labels that could be inputs to any robust-\nness strategy, leading to improved worst group robustness.\nAn example of such a fully unsupervised worst group ro-\nbustness approach is (GEORGE) [32]. GEORGE clusters\nthe penultimate layer features in a UMAP reduced space,\ndemonstrating impressive results on multiple datasets. In\nthis work, we instead show that clustering explainability\nheatmaps instead, is more beneficial in improving worst\ngroup robustness. Intuitively, this stems from the fact that a\npixel-attribution based explainability method in input space\nfocuses only on the relevant image features (pixel space) in\nthe task, discarding other entangled features irrelevant for\nthe final prediction.\nIn our work, we circumvent the need for a group labeled\narXiv:2403.13870v1  [cs.CV]  20 Mar 2024\n1\nG1\nG2\nG3\nG4\n97.4\n92.5\n92.5\n95.2\nOriginal Model – Group Accuracy\n1\nG1\nG2\nG3\nG4\n99.6\n89.6\n76.8\n95.6\nSupervised\n(a) Ground Truth\n(b) Feature-based\n(c) Explainability-\nbased\nUnsupervised\nPseudo-\nLabels\nLabels\nGroup Label Selection\nTraining Strategy\nGroup Robust Model\nFigure 1. To improve the original models worst group accuracy, most current approaches rely on supervised group labels (a), which\nrequires extensive annotation processes. Unsupervised approaches have relied on extracting pseudo labels based on the models feature\nrepresentations (b), where information can be highly entangled. ExMap instead infers group pseudo labels based on explainability heatmaps\n(c), leading to improved worst group performance.\ndataset by introducing ExMap, a novel two stage mech-\nanism: First, we extract explainability heatmaps from a\ntrained (base) model on the dataset of interest (we use the\nvalidation set without group labels). Next, we use a clus-\ntering module to produce pseudo-labels for the validation\ndata. The resulting pseudo-labels can then be used for any\noff-the-shelf group robustness learning strategy in use to-\nday. ExMap is also flexible in the choice of clustering al-\ngorithm. We show that attaching the ExMap mechanism to\nbaseline methods leads to improved performance over the\nunsupervised counterparts, and further closes the gap to su-\npervised and partially supervised counterparts. Addition-\nally, we demonstrate that ExMap is also useful in the re-\ncent multiple shortcut paradigm [18], where current popu-\nlar supervised approaches have been shown to struggle. We\nconclude with an extended analysis on why clustering ex-\nplainability heatmaps is more beneficial than raw features.\nIn summary, our contributions include:\n1. ExMap: A simple but efficient unsupervised, strategy ag-\nnostic mechanism for group robustness that leverages ex-\nplainability heatmaps and clustering to generate pseudo-\nlabels for underlying groups.\n2. An extended analysis that provides intuition and insight\ninto why clustering explainability heatmaps leads to su-\nperior results over other group-robustness baseline meth-\nods.\n3. Demonstrating the usefulness of ExMap in improving\nworst group robustness in both the single shortcut and\nmultiple shortcut settings.\n2. Related Work\nSingle shortcut mitigation with group labels\nThe\nparadigm of taking a frozen base model and proposing a\nshortcut mitigation strategy to maximise worst group accu-\nracy was introduced in Group-DRO (gDRO) [28]. How-\never, the requirement of group labels in both training and\nvalidation data motivated the proposal of mitigation strate-\ngies without training labels. This has resulted partially su-\npervised approaches [33] that only require a small set of\ngroup labels as well as in several methods that only re-\nquire the validation group labels [13, 19, 26]. One such\nexample is DFR[13], which re-trains the final layer of a\nbase ERM model on a balanced, reweighting dataset. Most\nrelevant to our work, GEORGE [32] proposes an unsuper-\nvised mechanism to generate pseudo-labels for retraining\nby clustering raw features, and can therefore be considered\nthe closest method to our proposed ExMap. We show that\nclustering heatmaps is a more beneficial and intuitive tech-\nnique for generating pseudo-labels, as attributing the model\nperformance on the input data pixels leads to a more in-\ntuitive interpretation of which features are relevant for the\ntask, and which are not. Our method, ExMap, leverages\nthis insight and clusters the heatmaps instead, leading to\nimproved performance over GEORGE and its two variants\n- GEORGE(gDRO) trained with the Group-DRO strategy,\nand GEORGE(DFR), trained with the DFR strategy.\nOther Strategies for Shortcut Mitigation\nThere are\nother extant works that mitigate spurious correlations with-\nout adopting the group-label based paradigm directly.\nMaskTune [2], for instance, learns a mask over discrimi-\nnatory features to reduce reliance on spurious correlations.\nCVar DRO [17] proposes an efficient robustness strategy us-\ning conditional value at risk (CVar). DivDis [16], on the\nother hand, proposes to train multiple functions on source\nand target data, identifying the most informative subset\nof labels in the target data.\nDiscover-and-Cure (DISC)\n[37] discovers spurious concepts using a predefined con-\ncept bank, then intervenes on the training data to mitigate\nthe spurious concepts, while ULA [35] uses a pretrained\nself-supervised model to train a classifier to detect and mit-\nigate spurious correlations. While these approaches do not\ndirectly adopt the group-label, we show that the proposed\nexplainability heatmap-based approach is more efficient in\nimproving the worst-group accuracy.\nMulti-Shortcut Mitigation\nThe single shortcut setting\nis a simpler benchmark as the label is spuriously corre-\nlated with only a single attribute.\nHowever, real world\ndatasets are challenging, and may contain multiple spuri-\nous attributes correlated with an object of interest. As a\nresult, when one spurious attribute is known, mitigating the\nreliance on this attribute may exacerbate the reliance on an-\nother. The recently introduced Whac-A-Mole [18] dilemma\nfor multiple shortcuts demonstrates this phenomenon with\ndatasets containing multiple shortcuts (e.g. background and\nco-occurring object). Single shortcut methods fail to miti-\ngate both shortcuts at once, leading to a spurious conserva-\ntion principle, where if one shortcut is mitigated, the other\nis exacerbated. The authors introduce Last Layer Ensem-\nble (LLE) to mitigate multiple shortcuts in their datasets,\nby training a separate classifier for each shortcut. How-\never, LLE’s reliance on apriori knowledge of dataset short-\ncuts is impractical in the real world. We evaluate ExMap in\nthis context and show that it is effective as an unsupervised\ngroup robustness approach to the multi-shortcut setting.\nHeatmap-based Explainability\nThe challenge of at-\ntributing learned features to the decision making of a model\nin the image space has a rich history. The techniques ex-\nplored can be differentiated on a variety of axes. LIME,\nSHAP, LRP [3, 11, 22] are early model-agnostic methods,\nwhile Grad-CAM and Integrated Gradients[30, 34] are gra-\ndient based attribution methods. We use LRP in this work\nowing to its popularity, but in principle, the heatmap extrac-\ntion module can incorporate any other method widely in use\ntoday. LRP is a backward propagation based technique re-\nlying on the relevance conservation principle across each\nneuron in each layer. The output is a set of relevance scores\nthat can be attributed to a pixel wise decomposition of the\ninput image. Heatmap-based explainability techniques have\nalso been used in conjunction with clustering, in the con-\ntext of discovering model strategies for classification, and\ndisparate areas such as differential privacy [6, 15, 29].\n3. Worst Group Robustness\nIn this section, we provide notation and brief background\nof the group robustness problem. We are given a dataset D\nwith image-label pairs being defined as D “ tpxi, yiquN\ni“1,\nwhere xi represents an image, yi is its corresponding la-\nbel, and N is the number of pairs in the dataset.\nThe\nmodel’s prediction for an image x is ypred “ ˆfpxq. The\ncross-entropy loss for true label y and predicted label ypred\nis given by Lpy, ypredq “ ´ řC\nc“1 yc logpypred,cq, where C\nis the number of classes. Then, an ERM classifier simply\nminimizes the average loss over the training data:\nˆf “ arg min\nf\n1\nN\nN\nÿ\ni“1\nLpyi, fpxiqq\n(1)\nwhere ˆf is the model obtained after training. Next, given\nthe validation data D, we assume that for the class label\nset L “ tc1, c2, ..., cku there exists a corresponding spuri-\nous attribute set A “ ta1, a2, ..., amu, such that the group\nlabel set G : L Ś A. For example, in CelebA, typically\na : Gender (Male/Female), and c: Blonde Hair (Blonde/Not\nBlonde). In this case L “ t0, 1u, and A “ t0, 1u. Then,\nthe optimization can be described as the worst-expected loss\nover the validation set, conditioned on the group labels and\nthe spurious attributes:\nˆf ˚ “ arg min\nf\nmax\npci,ajqPG Epx,yqPDrLpy, fpxqq|ci, ajs\n(2)\nAs discussed before, recent works aim to design strate-\ngies over the (base) trained model to minimize this objec-\ntive. For example, JTT collects an error set from the train-\ning data, and then upweights misclassified examples dur-\ning the second training phase. DFR reweights the features\nresponsible for misclassifications during the first phase in\nits finetuning stage. Note, however, that both these meth-\nods rely on the validation set group labels Gval to finetune\nthe network. We consider the case where Gtrain “ ϕ and\nGval “ ϕ. We do not have access to group labels, and must\ntherefore infer pseudo-labels in an unsupervised manner so\nthat existing group robustness methods can be used.\n4. Leveraging Explainability Heatmaps for\nGroup Robustness – ExMap\nIn this section, we describe ExMap, an intuitive and effi-\ncient approach for unsupervised group robustness to spuri-\nous correlations. ExMap is a two-stage method, illustrated\nin Figure 2.\nIn the first stage, we extract explainability\nheatmaps for the model predictions. In the second stage,\nwe cluster the heatmaps to generate pseudo-labels. These\npseudo-labels can then be used on any off-the-shelf group\nrobustness strategy in use today. In our work, we demon-\nstrate the strategy agnostic nature of ExMap by running it\non two popular strategies - JTT and DFR.\n4.1. Explainability Heatmaps\nWe use LRP [3] in this stage to generate pixel attributions in\nthe input space. This allows us to focus only on the relevant\nfeatures for the task. Specifically, given the validation data\nDval “ tpxi, yiquM\ni“1, we use pixel wise relevance score\nrx “ pLRPpxqq@x P Dval. Specifically, for each data\npoint x, the relevance score is defined on a per-neuron-per\nlayer basis. For an input neuron nk at layer k, and an output\nneuron nl at the following layer l, the relevance score Rk is\nintuitively a measure of how much this particular input nk\ncontributed to the output value nl:\nRk “\nÿ\nl:kÝÑl\nzkl\nzl ` ε ¨ signpzlq\n(3)\nExMap\n(B) Clustering\n(C) Pseudo-Labels\nInput Data\n(A) Heatmap Extraction\nFrozen ERM Model\nRetrain\nFigure 2. Our Proposed Method: ExMap facilitates group-robustness by extracting explainability heatmaps from the frozen base ERM\nmodel for the validation data (A). These heatmaps are then clustered (B) to obtain pseudo-labels for the underlying groups, which are\nsubsequently chosen for the retraining strategy (C).\nAlgorithm 1 Generating Pseudo-labels using G-ExMap\n1: Input: Dataset Dval, ERM Model M, DataLoader L\n2: Output: Pseudo-labels ˆG\n3: procedure GENERATEPSEUDOLABELS(D, M, L)\n4:\nR Ð H\nŹ Initialize heatmap set\n5:\nfor each batch x in L do\n6:\npred Ð arg maxi Mpxqi\n7:\nfor each layer k, l do\n8:\nCompute zl Ð nkwkl\n9:\nend for\n10:\nCompute LRP relevance rx for x using Eq. 3\n11:\nAdd rx to R\n12:\nend for\n13:\nCluster R using G-ExMap method:\n14:\nˆA Ð ClusterpRq\nŹ Estimated spurious labels\n15:\nCombine class labels L with ˆA\nˆG Ð L ˆ ˆA\n16:\nreturn Pseudo-labels ˆG\n17: end procedure\nwhere zl “ nkwkl for the weight connection wkl. Rk is\ncomputed for all the neurons at layer k, and backpropagated\nfrom the output layer to the input layer to generate pixel\nlevel relevance scores rx for each data input x. We can thus\nbuild the heatmap set R “ trx|x P Dvalu. This process is\nsummarized in Algorithm 1.\n4.2. Clustering\nIn the second stage, we cluster the LRP representations\nfrom the first stage. The intuition here is that over the data,\nthe heatmaps capture the different strategies undertaken by\nthe model for the classification task [15]. The clustering\nmodule helps identify dominant model strategies used for\nthe classification task. By identifying such strategies and\nresampling in a balanced manner, ExMap guides the model\nto be less reliant on the dominant features across the data,\ni.e. the spurious features. The heatmaps serve as an effec-\ntive proxy to describe model focus areas. We have two op-\ntions in choosing how to cluster: Local-ExMap (L-ExMap),\nwhere we cluster heatmaps on a per-class basis, and Global-\nExMap (G-ExMap), where we cluster all the heatmaps at\nonce, and segment by class labels. We present the G-ExMap\nresults in this paper, owing to better empirical results.\nSpecifically, given the Heatmap set R as described in\nAlgorithm 1, the estimated spurious labels are generated\nby the global clustering method, ˆA “ ClusterpRq where\nCluster (.) represents a clustering function.\nNow, given\nclass label set L and estimated spurious label set ˆA, we can\ngenerate our pseudo-group label set ˆG “ L Ś A by select-\ning each ai P ˆA, and each ck P L, to create tck, aiu @k, i.\nIn principle, it doesn’t matter what clustering method\nwe use, but that the clustering process itself outputs use-\nful pseudo-labels. For our work, we leverage spectral clus-\ntering with an eigengap heuristic, inspired by SPRAY [15].\nLater, we show that the choice of the clustering method does\nnot have a significant effect on the results. The outputs,\nwhich are the pseudo group labels for the validation data\nDval, can now be used as labels in lieu of ground truth la-\nbels to train any group robustness strategy in use. Note how\nin principle, any method that uses group labels (training or\nvalidation) would benefit from this approach. To apply this\nto the training set D, one would simply repeat Algorithm 1\non D. In this work, we apply ExMap to two common group\nrobustness strategies - JTT [19] and DFR [13]. Thus, we\ndemonstrate the strategy-agnostic nature of our approach\nthat can be applied to any off-the-shelf method using group\nlabels today.\n5. Experiments\nIn this section we first present the datasets, baselines, and\nexperimental setup. Next, we present the results and dis-\ncussion2.\n2A discussion of the limitations and societal impact can be found in the\nsupplementary material.\ny: <=4\ns: Red\ny: > 4\ns: Green\ny: Landbird\ns: Land\ny: Landbird\ns: Water\nC-MNIST\nWaterbirds\ny: Country Car\ns1: Country Background\ns2: Country Co-occuring \n                 Object\ny: Country Car\ns1: Country Background\ns2: Urban Co-occuring \n                 Object\nUrbanCars\ny: Blonde\ns: Woman\ny: Not Blonde\ns: Man\nCelebA\nFigure 3. The datasets used in our work, visualized with respect to the class labels, and the shortcuts s. For the complete list of datasets\nand more details, please refer to the supplementary material.\nDatasets\nWe use CelebA [20], Waterbirds [28, 36, 39],\nC-MNIST [1], and Urbancars [18]. In CelebA, the class la-\nbel to be predicted is hair colour (Blonde/Not Blonde), and\nthe spurious attribute is gender (Male/Female). For Water-\nbirds, the class label is the bird type (waterbird/landbird),\nand the spurious attribute is the background (land/water).\nIn C-MNIST, the class label is if the number is smaller\nthan or equal to four.\nAny number lesser than or equal\nto four is assigned blue, while all numbers greater than\nfour are assigned the color red, with a correlation of 99%.\nThus, the spurious attribute is the color. For Urbancars, the\nclass label is the car type (country/urban), and the spuri-\nous attributes are the background and co-occuring object\n(both country/urban). We create two variants of Urbancars:\nThe first variant is Urbancars (BG), where only the back-\nground object is the spurious attribute. The second vari-\nant is Urbancars (CoObj), where the co-occuring object is\nthe spurious attribute. We present single shortcut results\non CelebA, Waterbirds, C-MNIST, Urbancars (BG) and Ur-\nbancars (CoObj). For the multiple shortcut setting, we use\nthe original UrbanCars dataset with both shortcuts [18]. An\noverview over the considered datasets can be found in Fig-\nure 3. We present more dataset details in the supplementary.\nBaselines\nWe use the unsupervised approaches DivDis,\nMaskTune, and two variants of GEORGE (with gDRO and\nDFR) as the baselines in our work. We also adapt LfF, JTT,\nand CVar DRO to the unsupervised setting as additional\nbaselines. We train the ERM model using an Imagenet-\npretrained Resnet-50, and use the open source implemen-\ntations of the baselines to generate our results. Specifically,\nwe implement GEORGE(DFR), ExMap, and JTT. Remain-\ning results are reported from [2], [19], and [16].\nSetup\nWe make sure to use the same hyperparameters\nfrom the baseline papers to reproduce the results. We utilise\na composite of LRP rules to get the explainability heatmaps\nas recommended by [14, 23]. Following their recommenda-\ntions we use LRP-ϵ for the dense layers near the output of\nthe model with small epsilon (ϵ ! 1), followed by LRP-γ\nfor the convolutional layers.\nFor the spectral clustering, we use the affinity matrix,\nand cluster-QR [29] to perform the clustering. The eigen-\ngap heuristic is applied to the 10 smallest eigenvalues of the\nLaplacian matrix to select the number of significant clusters\nto use. We demonstrate later that using a simpler cluster-\ning approach such as k-means can also emit reasonable re-\nsults. For more details on the affinity matrix, clustering and\npseudo-label generation, please see the supplementary.\n5.1. Results: Single Shortcut\nIn Table 1 we present the single shortcut results for the\ndatasets. First, we note that with no supervision, ExMap\nbased DFR improves significantly upon ERM. Second, we\nnote the improved performance of ExMap based DFR over\nthe unsupervised baselines, including GEORGE, our clos-\nest baseline. Further, since the DFR-based GEORGE and\nExMap significantly outperforms the other baselines, we\npresent results comparing these two methods on C-MNIST,\nUrbancars (BG) and Urbancars (CoObj) in Table 2.\nIn\nboth tables, we demonstrate the superiority of clustering\nheatmaps to generate pseudo-labels instead of the raw fea-\ntures as in GEORGE. These results also show that the\ngroups inferred by ExMap are indeed useful for worst group\nrobustness to spurious correlations. Third, we note the gap\nin performance between DFR and ExMap based DFR. Since\nthe former uses validation labels, we expect an increased\naccuracy, but we can report better performance on Water-\nbirds, and within 3% 2% 8% and 6% of the DFR results on\nthe remaining datasets. On CelebA, our results are within\n5% of Group-DRO, which demonstrates the best overall re-\nsults. However, note that Group-DRO is a fully supervised\napproach, using labels from both the training and validation\nsets. For all datasets, we are able to outperform GEORGE,\nour closest baseline. As discussed before, while mean ac-\ncuracy is not the appropriate metric to track in the group\nrobustness setting (ERM has the best overall mean accu-\nracy but the worst overall worst group accuracy), we can\nstill confirm that ExMap based DFR does not witness sig-\nnificant drops in performance.\nMethods\nGroup Info\nWaterbirds\nCelebA\nTrain/Val\nWGA(%)Ò\nMean(%)\nWGA(%)Ò\nMean(%)\nBase (ERM)\n✗/✗\n76.8\n98.1\n41.1\n95.9\nGroup DRO\n✓/✓\n91.4\n93.5\n88.9\n92.9\nEIIL\n✓/✓\n87.3\n93.1\n81.3\n89.5\nBARACK\n✓/✓\n89.6\n94.3\n83.8\n92.8\nCVar DRO\n✗/✓\n75.9\n96.0\n64.4\n82.5\nLfF\n✗/✓\n78.0\n91.2\n77.2\n85.1\nJTT\n✗/✓\n86.7\n93.3\n81.1\n88.0\nDFR\n✗/✓\n92.1\n96.7\n86.9\n91.1\nGEORGE (gDRO)\n✗/✗\n76.2\n95.7\n53.7\n94.6\nCVar DRO\n✗/✗\n62.0\n96.0\n36.1\n82.5\nLfF\n✗/✗\n44.1\n91.2\n24.4\n85.1\nJTT\n✗/✗\n62.5\n93.3\n40.6\n88.0\nDivDis*\n✗/✗\n81.0\n-\n55.0\n-\nMaskTune\n✗/✗\n86.4\n93.0\n78.0\n91.3\nGEORGE (DFR)\n✗/✗\n91.7\n96.5\n83.3\n89.2\nDFR+ExMap (ours)\n✗/✗\n92.5\n96.0\n84.4\n91.8\nTable 1. Worst group and mean accuracy on the test sets of the different datasets. The Group Info column showcases for each method\nwhether group labels are used for that split of the data (✗= does not use group labels, ✓= uses group labels). We report the average results\nover 5 runs after hyperparameter tuning. Gray rows represent supervised approaches. *DivDis does not report mean test accuracy results.\nMethods\nGroup Info\nC-MNIST\nUrbancars (BG)\nUrbancars (CoObj)\nTrain/Val\nWGA(%)Ò\nMean(%)\nWGA(%)Ò\nMean(%)\nWGA(%)Ò\nMean(%)\nBase (ERM)\n✗/✗\n39.6\n99.3\n55.6\n90.2\n50.8\n92.7\nDFR\n✗/✓\n74.2\n93.7\n77.5\n81.0\n84.7\n88.2\nGEORGE (DFR)\n✗/✗\n71.7\n95.2\n69.1\n83.6\n76.9\n91.4\nDFR+ExMap (ours)\n✗/✗\n72.5\n94.9\n71.4\n93.2\n79.2\n93.2\nTable 2. Worst Group accuracy and mean accuracy on C-MNIST, Urbancars (BG), and Urbancars (CoObj). We use GEORGE as the\nbaseline, since both GEORGE and ExMap significantly outperform other unsupervised methods on Waterbirds and CelebA. Gray rows\nrepresent supervised approaches.\n5.2. Results: Multiple Shortcuts\nHere, we present the results on the UrbanCars data, which\ncontains multiple shortcuts in the images - the background\nand the co-occurring object in the image. This dataset was\nintroduced in the recent work on multiple shortcut miti-\ngation [18], where the authors show that mitigating one\nshortcut may lead to a reliance on another shortcut in the\ndata, rendering the single shortcut setting incomplete (the\nWhac-a-Mole problem). The authors introduce a new set\nof metrics for the task - The BG Gap, which is the drop\nin accuracy between mean and cases when only the back-\nground is uncommon, the CoObj Gap which is the drop\nin accuracy between mean and cases when only the co-\noccurring object is uncommon, and the BG+CoObj Gap,\nthe drop when both the background and the co-occurring\nobject are uncommon. A mitigation strategy should witness\na smaller drop from the original accuracy when compared\nto others. In Table 3, we present the ExMap based DFR re-\nsults with respect to DFR, ERM, and GEORGE(DFR). We\nalso present results of three variants of DFR: DFR (Both),\nwhich is retraining on the original UrbanCars data with both\nshortcuts. DFR(BG) retrains on UrbanCars with only the\nbackground shortcut, and DFR(CoObj) retrains with only\nthe co-occuring object shortcut.\nRed values indicate an\nincrease in gap when compared to ERM, which is unde-\nsirable (the Whac-A-Mole dilemma).\nNote that the first\nthree DFR methods have access to the group labels, while\nGEORGE and ExMap do not. Table 3 demonstrates some\nimportant results: First, that DFR + ExMap consistently\nposts lower drops than the base ERM model. Second, that\nExMap does not witness an increase in gap on any of the\nmetrics compared to ERM, unlike GEORGE(DFR), which\nMethod\nBG Gap Ò\nCoObj Gap Ò\nBG+CoObj Gap Ò\nERM\n-8.2\n-14.2\n-69.0\nDFR (Both)\n-4.6\n-5.4\n-14.2\nDFR (BG)\n-0.3\n-29.2 (ˆ 2.06)\n-33.2\nDFR (CoObj)\n-16.3 (ˆ 1.99)\n-0.5\n-19.1\nGEORGE (DFR)\n-7.0\n-15.4 (ˆ1.08)\n-63.4\nDFR+ExMap (ours)\n-5.9\n-9.9\n-30.7\nTable 3. Multiple Shortcuts on UrbanCars. Red values indicate\nthe Whac-A-Mole dilemma: Mitigating one shortcut exacerbates\nreliance on the other (compared to ERM). ExMap proves to be the\nmost robust in this setting, and outperforms GEORGE, its direct\nunsupervised counterpart.\nInput Data\nERM\nExMap\nFigure 4. ERM and ExMap Heatmaps - Left: The Input images.\nMiddle: ERM model explanations. Right: Improved group ro-\nbustness using ExMap. Our method helps improve the focus on\nrelevant attributes, in turn improving the pseudo-label estimation\nfor retraining.\nexhibits the Whac-A-Mole dilemma for the CoObj Gap. Fi-\nnally, the DFR variants exhibit the Whac-A-Mole dilemma:\nFor a DFR variant retrained on a particular shortcut, the re-\nliance on that shortcut is mitigated (e.g. DFR (BG) miti-\ngates the BG Gap), but the other shortcut reliance is exacer-\nbated (DFR (BG) exhibits a higher CoObj Gap than ERM).\nNote that DFR uses the validation group labels, and hence\nwill be more useful in mitigating shortcuts than our unsu-\npervised setting. In fact, as demonstrated in [18], training\nseparate classifiers for each shortcut is the best approach to\nmitigating multiple shortcuts, which explains DFR’s best\noverall results.\nHowever, this setting assumes availabil-\nity to the shortcut labels, which ExMap does not assume.\nYet, it demonstrates a robust performance for the multi-\nshortcut setting even in the unsupervised setting, outper-\nforming GEORGE, its closest unsupervised competitor.\n6. Analysis\nIn this section, we present analysis and ablations along five\naxes: First, we demonstrate how the clustering of heatmaps\nis more useful than the clustering of features.\nSecond,\nwe demonstrate the usefulness of the ExMap representa-\nGroup 1: (Non-Blonde/Female) \nGroup 2: (Blonde/Female) \nGroup 3: (Non-Blonde/Male) \nGroup 4: (Blonde/Male) \nFigure 5. ExMap Heatmaps on CelebA: Each entry represents a\ngroup. The positive and negative attributions help interpret which\nfeatures the model considers spurious (Blue), and which features\nare helpful (Red).\nMethods\nMean (FG-Only %)\nMean (%)\nDrop Ó\nERM\n44.2\n98.1\n53.9\nDFR\n64.7\n94.6\n29.9\nGEORGE (DFR)\n73.2\n96.5\n23.3\nDFR+ExMap (ours)\n78.5\n96.0\n17.5\nTable 4. Waterbirds (FG-Only). All methods exhibit a reliance\non the background shortcut in Waterbirds, but ExMap posts the\nlowest drop, demonstrating its robustness.\ntions when compared to ERM with respect to the classifi-\ncation task. Third, we provide more insight into what the\nlearned clusters by ExMap capture in the data. Fourth, we\ndemonstrate that ExMap is robust to the choice of clustering\nmethod, by performing an ablation on the clustering method\nusing k-means instead of spectral clustering.\nFinally, to\ndemonstrate that ExMap is strategy-agnostic, we use JTT\nas a retraining strategy using ExMap pseudo-labels, and are\nable to demonstrate robust performance with respect to JTT\ntrained on true validation labels.\n6.1. The benefit of heatmaps over features\nIn this section we add more insight into why leveraging\nheatmaps for worst group robustness is more useful over\nfeatures, as for example done in GEORGE. Specifically, we\nillustrate how heatmap based clustering mitigates reliance\non the image background, the spurious attribute in the Wa-\nterbirds dataset. The results illustrate a common intuition -\nExplainability heatmaps highlight only the features relevant\nfor prediction, ignoring those that are not.\nCircumventing background reliance\nHere, we present\nresults on Waterbirds with the spurious attribute, i.e. back-\nground, removed.\nWe call this variant Waterbirds (FG-\nOnly), following [13]. Please refer to the supplementary\nsection for examples. An effective group robustness method\nwould not witness a sharp drop in test accuracy if the model\ndoes not rely on the background. In Table 4, we present\nthese results.\nWe can clearly see that the heatmap clustering strategy\nMethods\nGroup Info\nWGA(%) Ò\nMean(%) Ò\nBase (ERM)\n✗/✗\n76.8\n98.1\nDFR\n✗/✓\n92.1\n96.7\nDFR+ExMap (SC)\n✗/✗\n92.6\n96.0\nDFR+ExMap (KMeans)\n✗/✗\n92.5\n95.9\nTable 5. Worst group accuracy and mean accuracy on Waterbirds\nwith two different clustering methods - kmeans and Spectral.\nmitigates the background reliance better than the feature\nbased clustering strategy of GEORGE (lowest drop among\nall methods). This is also intuitive as the heatmap attribu-\ntions focus on only the relevant features for prediction, dis-\ncarding the rest (see Figure 4).\n6.2. Qualitative Analysis\nExMap improves explanations upon retraining\nWe vi-\nsualize the heatmaps and predictions of ERM and Exmap\nbased DFR in Figure 4. This is an image of the Water-\nbirds dataset that ERM misclassifies. This is reflected on\nthe heatmap, as ERM fails to capture the relevant features.\nOn the other hand, ExMap based DFR correctly classifies\nthe image and focuses on the correct object region of inter-\nest (bird), instead of the spurious attribute (background).\nExMap improves Model Strategy\nIn Figure 5, each en-\ntry represents a particular group. The positive and negative\nrelevance scores correspond to the features that the model\nconsiders relevant and spurious respectively. ExMap un-\ncovers the strategy used to make the prediction: In all four\ngroups, we see ExMap helps the model uncover the hair\ncolor as a strategy.\nIn fact, in Group 4, the model also\nlearns that the facial features (Gender) are negatively asso-\nciated with the prediction task (Hair Color), which is what\nwe desire from our method. The model has learned the\nshortcut between man and not-blonde hair, hence ExMap\nuncovers the negative relevance in the face, effectively un-\ncovering this shortcut. These examples impart a notion of\ninterpretability to our results, as we are able to explain why\nthe model made a particular prediction, and what shortcuts\nare uncovered.\n6.3. Ablation Analysis\nRobustness to choice of clustering method\nOur pro-\nposed method does not depend on any particular clustering\nalgorithm. Although we used spectral clustering, one can\nalso use the simpler K-means [24] to capture the clusters for\npseudo-labelling. In Table 5, we present the results on Wa-\nterbirds. We are able to demonstrate that there is no signif-\nicant difference in the worst group robustness performance\nfor the clustering method we choose3. Both improve upon\nthe base ERM and DFR models, and hence, both are useful.\n3Note, empirical results illustrated that k-means results were robust to\nthe number of clusters, K, given that K was chosen sufficiently large.\nMethods\nGroup Info\nWGA(%) Ò\nMean(%) Ò\nBase (ERM)\n✗/✗\n41.1\n95.9\nDFR\n✗/✓\n92.1\n96.7\nDFR+ExMap (ours)\n✗/✗\n92.6\n96.0\nJTT\n✗/✓\n86.7\n93.3\nJTT+ExMap (ours)\n✗/✗\n86.9\n90.0\nTable 6. Worst group and mean accuracy on Waterbirds for two\ndifferent retraining strategies - JTT and DFR.\nThus, ExMap is more about demonstrating the usefulness of\na heatmap clustering pseudo-labelling module rather than\nthe specifics of the clustering method itself.\nRobustness to choice of learning strategy\nAll the results\npresented until now focus on the DFR backbone for shortcut\nmitigation. We mention previously that ExMap is strategy-\nagnostic, meaning that it can be applied to any off-the shelf\nmethod in use today. In Table 6, we show the results af-\nter applying ExMap to the JTT method on Waterbirds. We\ndemonstrate similar performance to using JTT (originally\nuses validation labels) simply by using the pseudo labels\nproposed by ExMap. Additionally, we are able to improve\nover ERM’s poor worst group accuracy as well.\n7. Conclusion\nThe group robustness paradigm for deep learning classifiers\nraises important questions for when deep learning models\nsucceed, but more importantly, when they fail. However,\nmost of current research focuses on the setting where group\nlabels are available. This assumption is impractical for real-\nworld scenarios, where the underlying spurious correlations\nin the data may not be known apriori. While recent work\ninvestigating unsupervised group robustness mechanisms\nhave shown promise, we show that further improvements\nare possible. In our work, we propose ExMap, where we\ncluster explainable heatmaps to generate pseudo-labels for\nthe validation data. These pseudo-labels are then used on\noff-the-shelf group robustness learning mechanisms in use\ntoday. In addition to showing why using heatmaps over raw\nfeatures is useful in this setting, our results demonstrate the\nefficacy of this approach on a range of benchmark datasets,\nin both the single and multi-shortcut settings. We are able to\nfurther close the gap to supervised counterparts, and outper-\nform partially supervised and unsupervised baselines. Fi-\nnally, ExMap opens up interesting avenues to further lever-\nage explainability heatmaps in group robust learning.\nAcknowledgements\nThis work was financially supported by the Research Coun-\ncil of Norway (RCN) FRIPRO grant no. 315029. It was\nfurther funded through the RCN Centre for Research-based\nInnovation funding scheme (Visual Intelligence, grant no.\n309439), and Consortium Partners.\nReferences\n[1] Mart´ın Arjovsky, L´eon Bottou, Ishaan Gulrajani, and\nDavid Lopez-Paz.\nInvariant risk minimization.\nArXiv,\nabs/1907.02893, 2019. 5\n[2] Saeid Asgari,\nAliasghar Khani,\nFereshte Khani,\nAli\nGholami, Linh Tran, Ali Mahdavi-Amiri, and Ghassan\nHamarneh. Masktune: Mitigating spurious correlations by\nforcing to explore. Advances in Neural Information Process-\ning Systems, 2022. 2, 5\n[3] Sebastian Bach, Alexander Binder, Gr´egoire Montavon,\nFrederick Klauschen, Klaus-Robert M¨uller, and Wojciech\nSamek. On pixel-wise explanations for non-linear classifier\ndecisions by layer-wise relevance propagation. PLOS ONE,\n10, 2015. 3\n[4] Hyojin Bahng, Sanghyuk Chun, Sangdoo Yun, Jaegul Choo,\nand Seong Joon Oh. Learning de-biased representations with\nbiased representations.\nInternational Conference on Ma-\nchine Learning, 2019. 1\n[5] Wieland Brendel and Matthias Bethge. Approximating cnns\nwith bag-of-local-features models works surprisingly well on\nimagenet. International Conference on Learning Represen-\ntations, 2018. 1\n[6] Bryant Chen, Wilka Carvalho, Nathalie Baracaldo, Heiko\nLudwig, Benjamin Edwards, Taesung Lee, Ian Molloy, and\nBiplav Srivastava. Detecting backdoor attacks on deep neu-\nral networks by activation clustering. Workshop on Artificial\nIntelligence Safety, 2019. 3\n[7] John\nDuchi,\nTatsunori\nHashimoto,\nand\nHongseok\nNamkoong.\nDistributionally robust losses for latent\ncovariate mixtures.\nOperations Research, 71(2):649–664,\n2023. 1\n[8] John C Duchi, Peter W Glynn, and Hongseok Namkoong.\nStatistics of robust optimization: A generalized empirical\nlikelihood approach. Mathematics of Operations Research,\n46(3):946–969, 2021. 1\n[9] Robert Geirhos,\nPatricia Rubisch,\nClaudio Michaelis,\nMatthias Bethge, Felix A Wichmann, and Wieland Brendel.\nImagenet-trained cnns are biased towards texture; increasing\nshape bias improves accuracy and robustness. International\nConference on Learning Representations, 2018. 1\n[10] Robert Geirhos, J¨orn-Henrik Jacobsen, Claudio Michaelis,\nRichard S. Zemel, Wieland Brendel, Matthias Bethge, and\nFelix Wichmann. Shortcut learning in deep neural networks.\nNature Machine Intelligence, 2:665 – 673, 2020. 1\n[11] Xiaojie Guo, Yu Li, and Haibin Ling. Lime: Low-light im-\nage enhancement via illumination map estimation.\nIEEE\nTransactions on Image Processing, 26:982–993, 2017. 3\n[12] Pavel Izmailov, Polina Kirichenko, Nate Gruver, and An-\ndrew G Wilson. On feature learning in the presence of spuri-\nous correlations. Advances in Neural Information Processing\nSystems, 35:38516–38532, 2022. 1\n[13] Polina Kirichenko, Pavel Izmailov, and Andrew Gordon Wil-\nson. Last layer re-training is sufficient for robustness to spu-\nrious correlations.\nInternational Conference on Learning\nRepresentations, 2022. 1, 2, 4, 7\n[14] Maximilian Kohlbrenner, Alexander Bauer, Shinichi Naka-\njima, Alexander Binder, Wojciech Samek, and Sebastian La-\npuschkin. Towards best practice in explaining neural net-\nwork decisions with lrp. International Joint Conference on\nNeural Networks, pages 1–7, 2020. 5\n[15] Sebastian\nLapuschkin,\nStephan\nW¨aldchen,\nAlexander\nBinder, Gr´egoire Montavon, Wojciech Samek, and Klaus-\nRobert M¨uller. Unmasking clever hans predictors and assess-\ning what machines really learn. Nature Communications, 10,\n2019. 1, 3, 4\n[16] Yoonho Lee, Huaxiu Yao, and Chelsea Finn. Diversify and\ndisambiguate: Out-of-distribution robustness via disagree-\nment.\nInternational Conference on Learning Representa-\ntions, 2023. 2, 5\n[17] Daniel Levy, Yair Carmon, John C Duchi, and Aaron Sid-\nford. Large-scale methods for distributionally robust opti-\nmization. Advances in Neural Information Processing Sys-\ntems, 33:8847–8860, 2020. 2\n[18] Zhiheng Li, I. Evtimov, Albert Gordo, Caner Hazirbas, Tal\nHassner, Cristian Cant´on Ferrer, Chenliang Xu, and Mark\nIbrahim. A whac-a-mole dilemma: Shortcuts come in multi-\nples where mitigating one amplifies others. IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n20071–20082, 2023. 2, 3, 5, 6, 7\n[19] Evan Z Liu, Behzad Haghgoo, Annie S Chen, Aditi Raghu-\nnathan, Pang Wei Koh, Shiori Sagawa, Percy Liang, and\nChelsea Finn. Just train twice: Improving group robustness\nwithout training group information. International Confer-\nence on Machine Learning, pages 6781–6792, 2021. 1, 2, 4,\n5\n[20] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.\nDeep learning face attributes in the wild. International Con-\nference on Computer Vision, 2015. 5\n[21] Vishnu Suresh Lokhande, Kihyuk Sohn, Jinsung Yoon,\nMadeleine Udell, Chen-Yu Lee, and Tomas Pfister.\nTo-\nwards group robustness in the presence of partial group la-\nbels. ICML 2022: Workshop on Spurious Correlations, In-\nvariance and Stability, 2022. 1\n[22] Scott M Lundberg and Su-In Lee.\nA unified approach to\ninterpreting model predictions. Advances in neural informa-\ntion processing systems, 30, 2017. 3\n[23] Gr´egoire Montavon,\nAlexander Binder,\nSebastian La-\npuschkin,\nWojciech Samek,\nand Klaus-Robert M¨uller.\nLayer-wise relevance propagation: an overview. Explainable\nAI: interpreting, explaining and visualizing deep learning,\npages 193–209, 2019. 5\n[24] Shi Na, Liu Xumin, and Guan Yong. Research on k-means\nclustering algorithm: An improved k-means clustering algo-\nrithm. Third International Symposium on Intelligent Infor-\nmation Technology and Security Informatics, pages 63–67,\n2010. 8\n[25] Junhyun Nam, Jaehyung Kim, Jaeho Lee, and Jinwoo Shin.\nSpread spurious attribute: Improving worst-group accuracy\nwith spurious attribute estimation. International Conference\non Learning Representations, 2021. 1\n[26] Jun Hyun Nam, Hyuntak Cha, Sungsoo Ahn, Jaeho Lee, and\nJinwoo Shin. Learning from failure: De-biasing classifier\nfrom biased classifier. Neural Information Processing Sys-\ntems, 2020. 2\n[27] Yonatan Oren, Shiori Sagawa, Tatsunori B Hashimoto, and\nPercy Liang.\nDistributionally robust language modeling.\nEmpirical Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural Language\nProcessing, pages 4227–4237, 2019. 1\n[28] Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and\nPercy Liang. Distributionally robust neural networks. Inter-\nnational Conference on Learning Representations, 2019. 1,\n2, 5\n[29] Lukas Schulth, Christian Berghoff, and Matthias Neu. De-\ntecting backdoor poisoning attacks on deep neural networks\nby heatmap clustering. ArXiv, abs/2204.12848, 2022. 3, 5\n[30] Ramprasaath R. Selvaraju, Abhishek Das, Ramakrishna\nVedantam, Michael Cogswell, Devi Parikh, and Dhruv Ba-\ntra. Grad-cam: Visual explanations from deep networks via\ngradient-based localization. International Journal of Com-\nputer Vision, 128:336 – 359, 2016. 3\n[31] Sahil Singla and Soheil Feizi.\nCausal imagenet:\nHow\nto discover spurious features in deep learning?\nCoRR,\nabs/2110.04301, 2021. 1\n[32] Nimit Sohoni, Jared Dunnmon, Geoffrey Angus, Albert\nGu, and Christopher R´e.\nNo subclass left behind: Fine-\ngrained robustness in coarse-grained classification problems.\nAdvances in Neural Information Processing Systems, 33:\n19339–19352, 2020. 1, 2\n[33] Nimit Sharad Sohoni, Maziar Sanjabi, Nicolas Ballas,\nAditya Grover, Shaoliang Nie, Hamed Firooz, and Christo-\npher Re. Barack: Partially supervised group robustness with\nguarantees. ICML 2022: Workshop on Spurious Correla-\ntions, Invariance and Stability, 2022. 2\n[34] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic\nattribution for deep networks. International Conference on\nMachine Learning, 2017. 3\n[35] Christos Tsirigotis, Joao Monteiro, Pau Rodriguez, David\nVazquez, and Aaron C Courville. Group robust classifica-\ntion without any group information. Advances in Neural In-\nformation Processing Systems, 36, 2024. 2\n[36] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie.\nCaltech-ucsd-birds-2011. Technical Report CNS-TR-2011-\n001, California Institute of Technology, 2011. 5\n[37] Shirley Wu, Mert Yuksekgonul, Linjun Zhang, and James\nZou. Discover and cure: Concept-aware mitigation of spu-\nrious correlation.\nInternational Conference on Machine\nLearning, 2023. 2\n[38] Michael Zhang, Nimit S Sohoni, Hongyang R Zhang,\nChelsea Finn, and Christopher Re. Correct-n-contrast: a con-\ntrastive approach for improving robustness to spurious cor-\nrelations. International Conference on Machine Learning,\n162:26484–26516, 2022. 1\n[39] Bolei Zhou, Agata Lapedriza, Antonio Torralba, and Aude\nOliva. Places: An image database for deep scene understand-\ning. Journal of Vision, 17(10):296–296, 2017. 5\n[40] Chunting Zhou, Xuezhe Ma, Paul Michel, and Graham Neu-\nbig. Examining and combating spurious features under dis-\ntribution shift. International Conference on Machine Learn-\ning, pages 12857–12867, 2021. 1\nExMap: Leveraging Explainability Heatmaps for Unsupervised Group\nRobustness to Spurious Correlations\nSupplementary Material\nIn this supplementary material, we present additional de-\ntails about the following:\n• The datasets used - C-MNIST, Waterbirds, CelebA, Ur-\nbancars, Urbancars single shortcut variants, Waterbirds\n(FG-Only).\n• Experimental Setup - The details on the heatmap extrac-\ntion and clustering phase in ExMap.\n• Additional results providing further intuition on how\nExMap captures underlying group information.\n• More results on the robustness of our method with stan-\ndard errors.\n• The connection between group robustness and fair clus-\ntering.\n• Limitations and Societal Impact.\n1. Datasets\nWe present the number of examples from each group for\nall the datasets, and the process of generating them. For C-\nMNIST, we used the same setup as in [2]. For Waterbirds\nand CelebA, we use the same setup as in [6, 8]. For Urban-\ncars we use the same setup as in [7].\n1.1. C-MNIST\nWe create a dataset where we have control of the number of\nelements in each group and what the spurious attribute is.\nThe Colored-MNIST dataset is a synthetic dataset based\non the well-known MNIST. The MNIST dataset is a collec-\ntion of several thousands of examples of handwritten digits\n(0-9). The images are single-channelled (black and white)\nand have a size of 28x28 pixels, and are accompanied by a\nlabel giving the ground truth.\nWe use the original data split, 60000 train and 10000\ntest. Since the original dataset does not have a validation\nset, we use the last 10000 images of the training set as the\nvalidation set.\nWe convert the dataset into a 2 class problem by modify-\ning the task. This is done by simply going over to classify\nthe numbers as smaller or equal to 4 (y “ 0 : value ă“ 4),\nand larger than 4 (y “ 1 : value ą 4). To create the spuri-\nous attributes we make use of colors. Red is used as the first\nspurious attribute (s “ 0 : RGB “ p255, 0, 0q), and green\nis used as the second spurious attribute (s “ 1 : RGB “\np0, 255, 0q). Naturally, the images will need to be made 3-\nchanneled to account for this change.\nAs we are interested in combating spurious correlations\nwe create the dataset in a way such that there are correla-\ntions between the classes and spurious attributes. We use\nSplit\nTotal Data\nGroups\nGroup\n0\n(y=0,\ns=0)\nGroup\n1\n(y=0,\ns=1)\nGroup\n2\n(y=1,\ns=0)\nGroup\n3\n(y=1,\ns=1)\nTrain\n50,000\n254\n25,284\n24,231\n231\nVal\n10,000\n45\n5,013\n4,893\n49\nTest\n10,000\n48\n5,091\n4,815\n46\nTable 1. Data splits in the Colored-MNIST dataset.\n99% correlation. That means that 99% of images from one\nclass will have the same colour, while the remaining 1%\nwill have the other colour. The amount of correlation was\ndeliberately chosen so that ERM worst group accuracy is\nlow. Table 1 shows the number of images in each group for\neach split.\n1.2. Waterbirds\nWaterbirds [10] is a synthetic dataset created with the pur-\npose of testing a model’s reliance on background.\nThe\ndataset consists of RGB images depicting different types\nof birds on different types of backgrounds. The different\ntypes of birds are divided into 2 classes, landbirds (y “ 0)\nand waterbirds (y “ 1). The different backgrounds are also\ndivided into 2 and represent the spurious attributes of this\ndataset: land background (s “ 0) and water background\n(s “ 1). The group distributions across the different splits\nare presented in Table 2.\nThe Waterbirds dataset is created by using 2 other\ndatasets, the Caltech-UCSD Birds-200-2011 (CUB) dataset\n[13] and the Places dataset [14]. The CUB dataset contains\nimages of birds labelled by species and their segmentation\nmasks. To construct the Waterbirds dataset the labels in the\nCUB dataset are split into 2 groups, where waterbirds are\nmade up of seabirds (albatross, auklet, cormorant, frigate-\nbird, fulmar, gull, jaeger, kittiwake, pelican, puffin, or tern)\nand waterfowls (gadwall, grebe, mallard, merganser, guille-\nmot, or Pacific loon), while the remaining classes are la-\nbelled as landbirds. The birds are cropped using the pixel-\nlevel segmentation masks and pasted onto a water back-\nground (categories: ocean or natural lake) or land back-\nground (categories: bamboo forest or broadleaf forest) from\nthe Places dataset.\nThe official train-test split of the CUB dataset is used,\nand 20% of the training set is used to create the validation\nset. The group distribution for the training set is such that\narXiv:2403.13870v1  [cs.CV]  20 Mar 2024\nSplit\nTotal Data\nGroups\nGroup\n0\n(y=0,\ns=0)\nGroup\n1\n(y=0,\ns=1)\nGroup\n2\n(y=1,\ns=0)\nGroup\n3\n(y=1,\ns=1)\nTrain\n4,795\n3,498\n184\n56\n1,057\nVal\n1,199\n467\n466\n133\n133\nTest\n5,794\n2,255\n2,255\n642\n642\nTable 2. Data splits in the Waterbirds dataset.\nmost images (95%) depict bird types with corresponding\nbackgrounds, to represent a distribution that may arise from\nreal-world data. This distribution turns the background into\na spurious feature. Take note that there is a distribution shift\nfrom the training split to the validation and test splits which\nare both more balanced, and include many more elements\nfor the minority group. The creators of the dataset argue\nthat they do this to more accurately gauge the performance\nof the minority groups, something that might be difficult if\nthere are too few examples. They also do this to allow for\neasier hyperparameter tuning.\n1.3. Celeb-A\nCelebA here is a reference to a part of the CelebA celebrity\nface dataset [9] that was introduced by [10] as a group\nrobustness dataset. From the original dataset, the feature\nBlond Hair is used as the class, meaning that the images\nare divided into people who are not blonde (y “ 0) and\nblonde (y “ 1). Meanwhile, as a spurious attribute, we use\nthe feature Male from the original dataset, which divides\ninto female (s “ 0) and male (s “ 1). The official train-\nval-test split of the CelebA dataset is used. Note in Table 3,\nthat the splits are likely randomly created, which results in\nequally group-distributed splits. Across all splits the group\n(blonde, male) is the smallest.\nThis dataset tests for model reliance on strongly corre-\nlated features in a real-world dataset. Observe in Table 3\nthat g3 “ py “ 1, s “ 1q which represents blonde males\nis severely underrepresented compared to the other groups,\nhence we expect the model to learn gender as a spurious\nfeature for the class blonde.\nSplit\nTotal Data\nGroup\n0\n(y=0,\ns=0)\nGroup\n1\n(y=0,\ns=1)\nGroup\n2\n(y=1,\ns=0)\nGroup\n3\n(y=1,\ns=1)\nTrain\n162,770\n71,629\n66,874\n22,880\n1,387\nVal\n19,867\n8,535\n8,276\n2,874\n182\nTest\n19,962\n9,767\n7,535\n2,480\n180\nTable 3. Data splits in the CelebA dataset.\n1.4. Urbancars\nWe use Urbancars, as proposed by [7]. There are 4000 im-\nages per target class, i.e. 8000 images in total. The target\nclass is the car type (country/urban), while the two shortcuts\nare the background type (country/urban), and co-occurring\nobject (country/urban). For the exact list of the cars, ob-\njects, and background, please see [7].\n1.5. Urbancars single shortcut variants\nThe original Urbancars data has eight group combinations\ndue to two classes, and two shortcuts (Background and\nCo-Occurring object). For the single shortcut variants, we\nmerge the 4 extra groups for one particular shortcut, to leave\n4 groups for the other.\nFor example: To create Urban-\ncars (BG), we merge the 4 groups from the other shortcut\n(CoObj), to create four groups containing the single short-\ncut of background for each of the two classes. A similar\nprocedure is adopted to create Urbancars (CoObj).\n1.6. Waterbirds (FG-Only)\nThis dataset is created to evaluate how well the trained\nmodels circumvent background reliance on the Waterbirds\ndataset, since background is the shortcut in the data. We re-\nmove the backgrounds in all the images only on the test set.\nIn Figure 1, we present some examples.\nOriginal Data\nForeground Only \nFigure 1. Waterbirds (FG Only)\n2. Experimental Setup\nIn this section, we present more details on the heatmap ex-\ntraction phase, clustering choice, and the hyperparameters\nused.\n2.1. Heatmap Extraction\nFollowing SPRAY [3], which reports good results across\ndifferent downsizing of heatmaps, we sweep predominantly\nMethods\nGroup Info\nC-MNIST\nUrbancars (BG)\nUrbancars (CoObj)\nTrain/Val\nWGA(%)Ò\nMean(%)\nWGA(%)Ò\nMean(%)\nWGA(%)Ò\nMean(%)\nBase (ERM)\n✗/✗\n39.6\n99.3\n55.6\n90.2\n50.8\n92.7\nGEORGE (DFR)\n✗/✗\n71.7˘0.1\n95.2˘0.3\n69.1˘0.9\n83.6˘1.0\n76.9˘0.9\n91.4˘1.0\nDFR+ExMap (ours)\n✗/✗\n72.5˘0.2\n94.9˘0.3\n71.4˘0.8\n93.2˘0.2\n79.2˘0.7\n93.2˘0.3\nTable 4. Group/mean test accuracy with std. Results over 5 runs.\nFigure 2. ExMap based misclassifications on challenging exam-\nples (Waterbirds): In each of these images, the object of interest\n(bird) is co-habited by dominant peripheral objects such as humans\nand other birds. These situations are challenging for the classifier\nto discern the relevant object from the irrelevant ones.\nover the following downsizings: [224, 112, 100, 56, 28, 14,\n7, 5, 3]. The downsizing of heatmaps additionally helps in\nspeeding up the clustering process and mitigating potential\nout-of-memory issues.\n2.2. Clustering choice\nSince ExMap if flexible to the choice of clustering algo-\nrithm, we experiment with spectral clustering, UMap re-\nduced KMeans [6], and KMeans.\nWe use the eigengap\nheuristic with spectral to automatically choose the number\nof clusters, and sweep over different cluster sizes for the\nKMeans based methods. We look for the largest gap in\namong the first 10 eigenvalues. Otherwise we test for 2-15\nclusters for kmeans (overclustering as practiced in [12]).\n2.3. Hyperparameters\nWe use the same hyperparameters for DFR and JTT as in\nthe original papers [6, 8].\nFor DFR, we perform the following steps:\n• Given (pseudo)group labels we create a retraining set by\nsubsampling each group to the size of the smallest group.\nThese are then used to retrain the last layer. After being\npassed through the feature extractor, each sample is nor-\nmalised based on the data used to retrain the last layer.\n• Similar to [6], we use logistic regression with L1-loss.\n• The strength of L1 is swept over [1.0, 0.7, 0.3, 0.1, 0.07,\n0.03, 0.01]. The sweep is performed by randomly split-\nting the retraining dataset in 2, and performing retrain-\ning with one half and evaluating the performance with\nthe other. This is performed 5 times with different splits\nand the best strength is chosen based on highest worst\n(pseudo)group accuracy.\nMethod\nAccuracy (%)\nWaterbirds\nCelebA\nWGA / Mean\nWGA / Mean\nBase (ERM)\n76.8 / 98.1\n41.1 / 95.9\nGEORGE (DFR) 91.7 ˘ 0.2 / 96.5 ˘ 0.1 83.3 ˘ 0.2 / 89.2 ˘ 0.2\nDFR+ExMap\n92.5 ˘ 0.1 / 96.0 ˘ 0.3 84.4 ˘ 0.5 / 91.8 ˘ 0.2\nTable 5. Group / mean test accuracy with std. Results over 5 runs.\n• When L1 strength has been selected, we retrain using the\nwhole retrain set. This is performed 20 times with differ-\nent subsamplings. The weights from each subsampling\nare averaged (this is viable according to DFR authors) to\nyield the final last layer weights. The normalisation of\ndata is also averaged across the 20 runs.\nFor the ERM model, we perform the following steps:\n• We use Resnet-18 for CMNIST, Resnet-50 for the others.\nWe start with imagenet-pretrained Resnet-50 similar to\nprevious work as it was observed to perform better. For\nall settings we replace the final fully connected layer to\nreflect the nature of our problems, i.e. 2 classes.\n• Learning rate: 3e-3, weight decay: 1e-4, cosine learning\nrate scheduler.\n• Batch size:We use batch size of 32 for Waterbirds and\nUrbancars, 100 for CelebA, and 128 for C-Mnist.\n• Epochs: We train for 100 epochs on Waterbirds and Ur-\nbancars, 20 for CelebA, and 10 for C-Mnist.\n• We use early stopping using the best mean (weighted) val-\nidation accuracy\nFor GEORGE, we perform the following steps:\n• Acquire feature extractor (base ERM) outputs.\n• Max normalise features.\n• Cluster features as exmap or using UMAP+kmeans. We\nuse 2 dimensions for the UMAP reduction, and high num-\nber of clusters (overclustering regime following [12]).\n3. Capturing of Group Information\nIn addition to why ExMap representations are better for\ndownstream group robustness over raw classifier features,\nwe are also interested in what kind of group information the\nExMap representations capture. The advantage of heatmaps\nare that they capture only the relevant features, while previ-\nous approaches that cluster in the feature space are prone\nto be effected by features that are irrelevant for the fi-\nnal prediction.\nTo further substantiate our findings, we\ngenerate additional results to demonstrate that ExMap in-\nMethods\nGroup Info\nWaterbirds\nCelebA\nTrain/Val\nWGA(%)Ò\nMean(%)\nWGA(%)Ò\nMean(%)\nBase (ERM)\n✗/✗\n76.8\n98.1\n41.1\n95.9\nBPA\n✗/✗\n71.3\n87.1\n83.3\n90.1\nDFR+ExMap (ours)\n✗/✗\n92.5\n96.0\n84.4\n91.8\nTable 6. Comparison with Fair Clustering: Worst group and mean accuracy on Waterbirds and CelebA.\nGroup 0 (Class 1)\nGroup 0 (Class 0)\nGroup 1 (Class 0)\nGroup 1 (Class 1)\nCluster 0\nCluster 1\nCluster 3\nCluster 5\nCluster 4\nCluster 2\nFigure 3. Groups in UrbanCars: (Left) Ground truth group la-\nbels per class. We observe minority groups (the spurious corre-\nlations) in the highlighted bottom right corner. (Right) Pseudo-\nlabels learned by ExMap based clustering reveals a similar over-\nall structure, conserving the dominant groups (green and yellow),\nwhile capturing the minority groups (blue cross and circle) as well.\ndeed captures the underlying group information. In Fig-\nure 3, we plot the pseudo-labels for UrbanCars (CoObj)\nafter ExMap based clustering. ExMap captures both the\ndominant groups and the minority groups in the dataset,\nas indicated by the pseudo-labels learned. We also note\nthat ExMap does not necessarily learn the same number of\ngroups as in the ground truth data, since this information\nis assumed unavailable. The key observation from this fig-\nure is that ExMap is successful in identifying the dominant\nand minority group structure in the data. The group robust\nlearner (such as DFR) can then sample across these groups\nin a balanced manner while retraining, leading to mitigation\nagainst spurious correlations.\n4. Robustness Analysis\nOur results in Table 1 and Table 2 in the main text are pre-\nsented as the average of five runs. To illustrate the robust-\nness of the compared approaches, we further provide the\nstandard deviation for the ExMap and the main competitor\nin Table 4 and Table 5. We observe that results are robust\nacross runs.\n5. Connections to Fair Clustering\nGiven the close relationship between group robustness and\nthe domain of fair clustering [1, 4, 5], we briefly com-\nment on their connection and the potential of the insights\nof ExMap in the fair clustering setting. The domains of fair\nclustering and group robustness differ slightly, with the for-\nmer aiming to improve mean accuracy independent of sensi-\ntive attributes, while the latter aim to maximize worst group\naccuracy. Therefore, there is a natural connection between\nthese two research areas. Sensitive attributes in fair cluster-\ning can be regarded as a special type of spurious correlation,\ncausally unrelated to the task. Recent work in fair clustering\nhas therefore adopted some of the insights from the field of\ngroup robustness [11]. However, these approaches adopt a\nGEORGE inspired approach (cluster in raw features space),\nwhich we demonstrate to be sub-optimal in the context of\ngroup robustness. While an in-depth exploration of this is\nout-of-scope for this work, it could present an interesting\navenue of future work. In Table 6, we present the ExMap re-\nsults on Waterbirds and CelebA with respect to the method\nintroduced in [11].\n6. Limitations and Societal Impact\nThere are certain intuitive failure cases where the ExMap\napproach is not as efficient. This occurs when the images\nthemselves are quite challenging to discern the objects of in-\nterest (the class), from other peripheral objects in the scene.\nIn Figure 2, we present some examples of misclassifications\nby ExMap based DFR. In these images, we can see that the\nobject of interest (bird), is co-habited by other dominant ob-\njects in the scene, such as humans and other birds. This cre-\nates an exceptionally challenging task for the classifier to\ndiscern the relevant features for the task. We recognise the\nneed for robustness across challenging examples in datasets\nas motivation for future work. With regard to social impact,\nwe recognise that model robustness to spurious correlations\nis an important first step in ensuring fair, transparent, and\nreliable AI that can be deployed in safety critical domains\nin the real world. Elucidating why models classify as they\ndo, and specific failure cases uncovers shortcomings in ex-\nclusively choosing mean test accuracy as a metric. As a\nresult, probing models for their weaknesses is as important\nas exemplifying their strengths.\nReferences\n[1] Sara Ahmadian, Alessandro Epasto, Ravi Kumar, and Mo-\nhammad Mahdian. Clustering without over-representation.\nInternational Conference on Knowledge Discovery & Data\nMining, pages 267–275, 2019. 4\n[2] Mart´ın Arjovsky, L´eon Bottou, Ishaan Gulrajani, and\nDavid Lopez-Paz.\nInvariant risk minimization.\nArXiv,\nabs/1907.02893, 2019. 1\n[3] Sebastian Bach, Alexander Binder, Gr´egoire Montavon,\nFrederick Klauschen, Klaus-Robert M¨uller, and Wojciech\nSamek. On pixel-wise explanations for non-linear classifier\ndecisions by layer-wise relevance propagation. PLOS ONE,\n10, 2015. 2\n[4] Ioana O Bercea, Martin Groß, Samir Khuller, Aounon Ku-\nmar, Clemens R¨osner, Daniel R Schmidt, and Melanie\nSchmidt.\nOn the cost of essentially fair clusterings.\nApproximation, Randomization, and Combinatorial Opti-\nmization. Algorithms and Techniques (APPROX/RANDOM\n2019), 2019. 4\n[5] Anshuman Chhabra, Karina Masalkovait˙e, and Prasant Mo-\nhapatra. An overview of fairness in clustering. IEEE Access,\n9:130698–130720, 2021. 4\n[6] Polina Kirichenko, Pavel Izmailov, and Andrew Gordon Wil-\nson. Last layer re-training is sufficient for robustness to spu-\nrious correlations.\nInternational Conference on Learning\nRepresentations, 2022. 1, 3\n[7] Zhiheng Li, I. Evtimov, Albert Gordo, Caner Hazirbas, Tal\nHassner, Cristian Cant´on Ferrer, Chenliang Xu, and Mark\nIbrahim. A whac-a-mole dilemma: Shortcuts come in multi-\nples where mitigating one amplifies others. IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n20071–20082, 2023. 1, 2\n[8] Evan Z Liu, Behzad Haghgoo, Annie S Chen, Aditi Raghu-\nnathan, Pang Wei Koh, Shiori Sagawa, Percy Liang, and\nChelsea Finn. Just train twice: Improving group robustness\nwithout training group information. International Confer-\nence on Machine Learning, pages 6781–6792, 2021. 1, 3\n[9] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.\nDeep learning face attributes in the wild. International Con-\nference on Computer Vision, 2015. 2\n[10] Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and\nPercy Liang. Distributionally robust neural networks. Inter-\nnational Conference on Learning Representations, 2019. 1,\n2\n[11] Seonguk Seo, Joon-Young Lee, and Bohyung Han. Unsu-\npervised learning of debiased representations with pseudo-\nattributes. IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 16742–16751, 2022. 4\n[12] Nimit Sohoni, Jared Dunnmon, Geoffrey Angus, Albert\nGu, and Christopher R´e.\nNo subclass left behind: Fine-\ngrained robustness in coarse-grained classification problems.\nAdvances in Neural Information Processing Systems, 33:\n19339–19352, 2020. 3\n[13] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie.\nCaltech-ucsd-birds-2011. Technical Report CNS-TR-2011-\n001, California Institute of Technology, 2011. 1\n[14] Bolei Zhou, Agata Lapedriza, Antonio Torralba, and Aude\nOliva. Places: An image database for deep scene understand-\ning. Journal of Vision, 17(10):296–296, 2017. 1\n",
  "categories": [
    "cs.CV",
    "cs.LG"
  ],
  "published": "2024-03-20",
  "updated": "2024-03-20"
}