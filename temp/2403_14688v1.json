{
  "id": "http://arxiv.org/abs/2403.14688v1",
  "title": "Kernel Alignment for Unsupervised Feature Selection via Matrix Factorization",
  "authors": [
    "Ziyuan Lin",
    "Deanna Needell"
  ],
  "abstract": "By removing irrelevant and redundant features, feature selection aims to find\na good representation of the original features. With the prevalence of\nunlabeled data, unsupervised feature selection has been proven effective in\nalleviating the so-called curse of dimensionality. Most existing matrix\nfactorization-based unsupervised feature selection methods are built upon\nsubspace learning, but they have limitations in capturing nonlinear structural\ninformation among features. It is well-known that kernel techniques can capture\nnonlinear structural information. In this paper, we construct a model by\nintegrating kernel functions and kernel alignment, which can be equivalently\ncharacterized as a matrix factorization problem. However, such an extension\nraises another issue: the algorithm performance heavily depends on the choice\nof kernel, which is often unknown a priori. Therefore, we further propose a\nmultiple kernel-based learning method. By doing so, our model can learn both\nlinear and nonlinear similarity information and automatically generate the most\nappropriate kernel. Experimental analysis on real-world data demonstrates that\nthe two proposed methods outperform other classic and state-of-the-art\nunsupervised feature selection methods in terms of clustering results and\nredundancy reduction in almost all datasets tested.",
  "text": "Kernel Alignment for Unsupervised Feature Selection via\nMatrix Factorization\nZiyuan Lin and Deanna Needell1\n1University of California, Los Angeles, Department of Mathematics\ne-mail: ziyuan.1.lin@gmail, deanna@math.ucla.edu\nMarch 13, 2024\nAbstract\nBy removing irrelevant and redundant features, feature selection aims to find a\ngood representation of the original features. With the prevalence of unlabeled data,\nunsupervised feature selection has been proven effective in alleviating the so-called\ncurse of dimensionality. Most existing matrix factorization-based unsupervised fea-\nture selection methods are built upon subspace learning, but they have limitations\nin capturing nonlinear structural information among features. It is well-known that\nkernel techniques can capture nonlinear structural information. In this paper, we\nconstruct a model by integrating kernel functions and kernel alignment, which can\nbe equivalently characterized as a matrix factorization problem. However, such an\nextension raises another issue: the algorithm performance heavily depends on the\nchoice of kernel, which is often unknown a priori. Therefore, we further propose\na multiple kernel-based learning method. By doing so, our model can learn both\nlinear and nonlinear similarity information and automatically generate the most ap-\npropriate kernel. Experimental analysis on real-world data demonstrates that the\ntwo proposed methods outperform other classic and state-of-the-art unsupervised\nfeature selection methods in terms of clustering results and redundancy reduction\nin almost all datasets tested.\nKeywords—Unsupervised feature selection, Kernel alignment, Matrix factorization, Multiple\nkernel learning\n1\nIntroduction\nHigh-dimensional data appears in multiple scientific fields like computer vision [1,2], bioinformat-\nics [3] and social networks [4]. Dealing with such high-dimensional data significantly amplifies\nthe complexity of data processing and computation, demanding increased storage space and\ncomputation time. Moreover, data with many dimensions often encompasses irrelevant, redun-\ndant, or even noisy features, which can adversely impact the performance of various learning\nalgorithms, affecting outcomes like clustering and classification [5,6]. As a result, the pursuit of\napproximating the original data using fewer but high-quality features has emerged as a critical\n1\narXiv:2403.14688v1  [cs.LG]  13 Mar 2024\nresearch focus, and dimensionality reduction algorithms provide a viable solution to address this\nchallenge.\nIn dimensionality reduction techniques, feature selection [7] and feature extraction [8] stand\nas the two primary methods. The former involves selecting a small subset of original features\nbased on certain criteria, while the latter seeks a projection to map the original high-dimensional\ndata into a lower-dimensional subspace. In other words, compared to feature extraction, feature\nselection algorithms retain the original features of the dataset, enhancing the interpretability of\nthe model’s outcomes. Moreover, in numerous data mining applications, sample labels remain\nunknown, rendering supervised and semi-supervised dimensionality reduction algorithms ineffec-\ntive. In contrast, unsupervised learning methods solely rely on the intrinsic properties of data\nwithout necessitating class labels, thus making the development of unsupervised feature selection\n(UFS) algorithms indispensable [9].\nSubspace learning is regarded as a successful approach in the realm of unsupervised fea-\nture selection [10–12]. It aims to discover a mapping that can effectively project the original\nhigh-dimensional space into a lower-dimensional subspace that is representative and relevant.\nSubspace learning allows the use of powerful tools such as matrix factorization and various\nregularization frameworks. These tools not only can compress the feature space into smaller\ndimensions, but also can efficiently remove noise and redundant features from the data [13].\nWithin subspace learning, the concept of subspace distance plays a critical role. As one\nof the evaluation criteria for feature selection, subspace distance directly influences the learned\nmapping of the algorithm, subsequently impacting the quality of the selected features. For in-\nstance, the Matrix Factorization Feature Selection (MFFS) algorithm proposed in [14] introduces\nthe concept of directional distance, measuring the distance between the original data and the\nselected feature subset projection back to the original space. It allows non-negative matrix fac-\ntorization tools to solve the feature selection model. The Regularized Self-representation (RSR)\nmethod [15] proposes self-representation distance, assuming that each feature of the original\ndata can be well approximated by a linear combination of related features. This approach also\ndiscerns redundant features and mitigates interference from outliers using the l2,1-norm. Fur-\nthermore, the Variance–Covariance Subspace Distance Feature Selection (VCSDFS) method [16]\nintroduces a subspace distance termed variance and covariance distance. It thoroughly considers\nthe variance of individual features and their correlations, thereby selecting informative features.\nIn addition, in subspace learning, regularization frameworks also play a crucial role in select-\ning effective features. For example, the feature selection based on Maximum Projection and\nMinimum Redundancy (MPMR) method [17] and the Regularized Matrix Factorization Feature\nSelection (RMFFS) method [18], both built upon the MFFS algorithm, respectively introduce a\nregularization framework based on linear correlation among a set of selected features and inner\nproduct regularization to reduce redundancy in the selected feature subset. The RSR method\nemploys l2,1-norm for sparse regularization of the feature selection matrix. Despite the good per-\nformance of these UFS methods, a crucial drawback is their exclusive focus on linear correlation\nbetween features. They do not capture the nonlinear relationships between features, leading to\nthe inability to select more discriminative features.\nIn this paper, we introduces a kernel alignment-based subspace distance as an evaluation\nmetric for subspace learning to construct an UFS model. This approach enables the consider-\nation of nonlinear relationships between features to jointly assess the quality of each feature.\nAdditionally, we employ non-negative matrix factorization techniques to generate efficient algo-\nrithms for model solution. Furthermore, a multiple kernel method for feature selection is also\nprovided and its algorithm is developed.\n2\n1.1\nOther Related Work\nThe notion of kernel alignment was first introduced by Cristianini et al. [19], which is used\nas a similarity measure between two kernel matrices. Cortes et al. [20] improved upon kernel\nalignment by proposing unnormalized centered kernel alignment, which can cancel the effect of\nunbalanced class distribution.\nIn the context of UFS tasks, where guidance from class labels is lacking, for two samples\nin a given dataset, it is assumed that the perspectives on similarity between these two samples\nprovided by two different kernels should be consistent. Therefore, maximizing the similarity\nbetween the two kernels, one computed from the original features and the other from the selected\nfeatures, which can ensure that the selected features maximally preserve the information of the\noriginal features.\nSeveral works have considered using kernel alignment in the design of UFS models. Wei\net al. [21] utilized unnormalized centered kernel alignment based on the Gaussian kernel to\ndevelop an UFS method. However, the derivation of their algorithm depends on the choice of\nkernel, making it challenging to generalize to other types of kernels. Karami et al. [16] proposed\nan UFS method based on variance-covariance subspace distance. We consider that, from the\nperspective of kernel alignment, minimizing the variance-covariance subspace distance can be\ninterpreted as maximizing the similarity between two linear kernels. Xing et al. [22], based on\nkernel alignment, investigated the fairness issue of unsupervised feature selection by introducing\nprotected attributes. The UFS method proposed by Palazzo et al. [23] is time-consuming as it\ninvolves training two models. The approach employs autoencoders to construct a target kernel\nand then utilizes kernel alignment to select features. In our work, we focus on accelerating the\nsolution of the kernel alignment based UFS model through matrix factorization. Additionally,\ntaking into account the limitations of single kernel models, we propose an UFS model based on\nmultiple kernel learning [24].\n1.2\nContribution\nIn this paper, we propose two novel UFS methods from the viewpoint of non-negative matrix\nfactorization, which we term Kernel Alignment Unsupervised Feature Selection (KAUFS) and\nMultiple Kernel Alignment Unsupervised Feature Selection (MKAUFS). The main highlights of\nthe proposed methods are summarized below:\n• To capture the nonlinear relationships among features, we innovatively introduce kernel\nalignment into the modeling of UFS methods based on subspace learning.\n• Many existing UFS algorithms based on non-negative matrix factorization cannot handle\ndatasets containing negative values. Our proposed algorithm can handle both nonnegative\nand negative values in the input data and the kernel matrix (Gram matrix), and we provide\na proof of convergence for the algorithm.\n• Considering that the performance of a single kernel model heavily depends on the choice\nof the kernel, which is usually unknown and time-consuming to determine. Inspired by the\nmultiple kernel learning approach [24], we extend the KAUFS method to the MKAUFS\nmethod by constructing several candidate kernels and merging them to form a consensus\nkernel to alleviate this issue, and it also enables us to better exploit the heterogeneous\nfeatures of real-world datasets.\n3\n1.3\nOrganization\nThe rest of this paper is structured as follows. The remainder of this section introduces the\nnotation used in this paper. In Section 2, we explain the details of the KAUFS method. In Section\n3, we derive an iterative update algorithm for the KAUFS method and prove its convergence. In\nSection 4, we extend KAUFS method to a multiple kernel version and design its algorithm. We\nalso provide the computational complexity analysis for both algorithms. Numerical experiments\nare presented and analyzed in Section 5. Finally, this paper is concluded in Section 6.\n1.4\nNotation\nWe denote scalars by lower or upper case letters. We denote matrices with boldface uppercase\nletters and vectors with boldface lowercase letters. Ik is used to denote the identity matrix in\nRk×k. The matrix 1d×k is in Rd×k so that all entries are equal to one. An indicator matrix\nmeans that each element in the matrix is either 1 or 0, and any row or column can have at most\none non-zero element. For a given matrix X, the Frobenius norm of X is defined as ∥X∥F . Tr (·)\nis the trace operator. XT denotes the transpose of X. For two vectors a and b, we denote their\ninner product as ⟨a, b⟩.\n2\nThe Framework of KAUFS\nIn this section, we will comprehensively explain the detailed information required to describe the\nstructure of the KAUFS method.\nFirst, we are given a high-dimensional input data, which is represented by an n × d matrix\nX = [x1; x2; . . . ; xn] = [f1, f2, . . . , fd], where xi ∈R1×d is the ith sample, fj ∈Rn×1 is the jth\nfeature and n and d represent the number of samples and features, respectively. The goal of a\nfeature selection framework is to choose a small subset of features from all features that capture\nthe most valuable information, thus approximating and representing all features effectively. More\nspecifically, given a fixed number k, where k < d, the objective of the KAUFS method is to select\na set of k features {fj1, fj2, . . . , fjk} from the dataset X. These selected features will form the\nselected sub-matrix XI = [fj1, fj2, . . . , fjk], such that XI can effectively approximate the original\nfeature matrix X = [f1, f2, . . . , fd] according to some metric. Moreover, we can express XI as\nXI = XW.\nwhere W ∈Rd×k is referred to as the feature weight matrix and is an indicator matrix.\nAfter selecting the feature subset, leveraging the self-representation property, which assumes\nthat linear combinations of related features can effectively approximate the features of the original\ndata, we can further represent the k features in XI as a linear combination to approximate the\nd features in the original dataset X, expressed as X ≈XIH. More specifically,\n\n\n\n\n\n\n\n\n\nf1 ≈XWh1 = H11fj1 + H21fj2 + · · · + Hk1fjk,\nf2 ≈XWh2 = H12fj1 + H22fj2 + · · · + Hk2fjk,\n...\nfd ≈XWhd = H1dfj1 + H2dfj2 + · · · + Hkdfjk,\n(1)\nwhere H ∈Rk×d is referred to as the representation matrix and its ith column denoted by hi,\nfor i = 1, . . . , d. The entries of H are denoted by Hij, for i = 1, . . . , k and j = 1, . . . , d.\nIn summary, we use the linear combinations of the selected features to approximate the\nhigh-dimensional input data matrix, i.e., X ≈XWH. This is the basis for the introduction of a\n4\nnovel UFS method, namely MFFS [14] which utilizes the Frobenius norm to evaluate the distance\nbetween the spaces spanned by X and XWH, ultimately transformed into a non-negative matrix\nfactorization problem as:\narg min\nW,H ∥X −XWH∥F\nsubject to W ≥0,\nH ≥0,\nW is an indicator matrix.\n(2)\nSome subsequent studies [17, 18, 25, 26] have made improvements within this framework.\nHowever, the Frobenius norm measuring the distance between the space formed by the original\nspace and the selected features fails to consider nonlinear relationships between features and is\nsensitive to noise and outliers [27]. Given these considerations, in this section, we propose a\nkernel alignment based learning method for unsupervised feature selection, termed KAUFS. The\nKAUFS method characterizes the similarity between two kernels, one associated with the original\nfeatures and the other with the selected features, enabling the selection of highly discriminative\nfeatures which consider the nonlinear relationships among features.\n2.1\nKernel Alignment\nWe briefly review several concepts related to centered kernel and kernel alignment following [20].\nSuppose that the data matrix X = [x1; x2; . . . ; xn] ∈Rn×d. A mapping function Φ : Rd →H\nwhich maps the data samples from the input space to a reproducing kernel Hilbert space H.\nThus, we have the transformed data sample x from the original space X to the kernel space is\nΦ (x). In addition, the mapping can be centered by subtracting its empirical expectation, that\nis forming it by Φ (x) −¯Φ, where ¯Φ = 1\nn\nPn\ni=1 Φ (xi).\nConsider a positive definite kernel function k : Rd × Rd →R which can be induced by\nthe mapping function Φ with entries, k (x, x′) = ⟨Φ (x) , Φ (x′)⟩= Φ (x)T Φ (x′). Centering the\npositive definite kernel function consists of centering all the mapping Φ associated with k.\nDefinition 1 (Centered Kernel Function). For any two samples x and x′ in the data matrix X,\nthe positive definite kernel function k (x, x′) after centering is\nkc (x, x′) =\n\u0000Φ (x) −¯Φ\n\u0001T \u0000Φ (x′) −¯Φ\n\u0001\n= k (x, x′) −1\nn\nn\nX\ni=1\nk (x, xi) −1\nn\nn\nX\ni=1\nk (xi, x′) + 1\nn2\nn\nX\ni=1\nn\nX\nj=1\nk (xi, xj).\nDefinition 2 (Centered Kernel Matrix). The positive semidefinite kernel matrix K, associated\nwith the positive definite kernel function k for all samples in the data matrix X, where the sample\nindex i, j ∈[1, n], after centering is\n[Kc]ij = Kij −1\nn\nn\nX\nj=1\nKij −1\nn\nn\nX\ni=1\nKij + 1\nn2\nn\nX\ni=1\nn\nX\nj=1\nKij.\nDenoting Λ = In −1\nn1n×n, the centered kernel matrix Kc can also be expressed as\nKc = ΛKΛ.\n(3)\nDefinition 3 (Centered Kernel Alignment). Let k and k′ denote two positive definite kernel\nfunctions defined over Rd×Rd, and K and K′ represent their kernel matrices, satisfying ∥Kc∥F ̸=\n5\n0 and ∥K′\nc∥F ̸= 0. Then, the alignment ρ (k, k′) between k and k′ and the alignment ˆρ (K, K′)\nbetween K and K′ are defined by\nρ (k, k′) =\nE [kck′\nc]\np\nE [k2c] E [k′2\nc ]\nand\nˆρ (K, K′) =\nTr (KcK′\nc)\n∥Kc∥F ∥K′c∥F\n.\nThe centered kernel matrix alignment ˆρ (K, K′) can be viewed as calculating the cosine sim-\nilarity between two kernel matrices. In addition, in many cases, incorporating the normalization\nterm ∥Kc∥F ∥K′\nc∥F typically adds complexity to the optimization problem. Hence, a more widely\nadopted approach to kernel alignment is its unnormalized version [21].\nDefinition 4 (Unnormalized Centered Kernel Alignment). Let k and k′ denote two positive\ndefinite kernel functions defined over Rd × Rd, and K and K′ represent their kernel matrices\nfor a sample of size n. Then, the unnormalized alignment ρu (k, k′) between k and k′ and the\nunnormalized alignment ˆρu (K, K′) between K and K′ are defined by\nρu (k, k′) = E [kck′\nc]\nand\nˆρu (K, K′) = 1\nn2 Tr (KcK′\nc) .\nIt is worthy to note that since the kernel alignment is only evaluated using positive semidef-\ninite kernel matrices, UFS methods proposed in this paper are not suitable for utilizing non-\npositive semidefinite kernel matrices.\nThe main idea of our proposed method is to seek a dataset composed of a concise set of\nfeatures that aligns best with the original dataset in the kernel space.\nThe objective is to\nmaximize the similarity between the kernel matrices corresponding to the original features and\nthe selected features, ensuring that the selected features retain the information from the original\nfeatures to the greatest extent. Specifically, given a high-dimensional input data matrix X =\n[x1; x2; . . . ; xn] ∈Rn×d and a positive definite kernel function k : Rd × Rd →R, we can obtain\nthe kernel matrix K corresponding to the original features:\nK =\n\n\n\nk (x1, x1)\n· · ·\nk (x1, xn)\n...\n...\n...\nk (xn, x1)\n· · ·\nk (xn, xn)\n\n\n.\nFor the dataset XWH formed by the selected features through the model, we compute the\nlinear kernel matrix corresponding to this dataset. We desire the linear kernel matrix of the\nselected features and the kernel matrix of the original features to exhibit similar perspectives on\nthe similarity among samples. Therefore, our objective is to select features that maximize the\nfollowing unnormalized centered kernel matrix alignment expression\n1\nn2 Tr\n\u0000ΛKΛΛXWHHT WT XT Λ\n\u0001\n= 1\nn2 Tr\n\u0000ΛKΛXWHHT WT XT \u0001\n,\n(4)\nwhere the second equation can be obtain by the fact that ΛΛ = Λ and the trace cyclic property.\nSimilar to the problem (2), according to the centered kernel matrix formula (3) and the\nkernel alignment function (4), the primary formulation of our feature selection method can be\nexpressed as the following matrix factorization problem:\narg min\nW,H −Tr\n\u0000KcXWHHT WT XT \u0001\nsubject to W ≥0,\nH ≥0,\nW is an indicator matrix.\n(5)\n6\n2.2\nKAUFS Model\nIn order for relaxing the feature weight matrix W from being an indicator matrix, Wang et\nal. [14] initially proposed that W should satisfy WT W = Ik.\nHowever, Saberi-Movahed et\nal. [28] pointed out that this indicator constraint on W may not necessarily yield an indicator\nmatrix. Moreover, such constraint may not perform effectively in identifying salient features.\nInspired by Han et al. [29], our proposed algorithm for matrix factorization-based feature selection\nincorporates a regularization method known as inner product regularization. This regularization\nis employed to characterize both sparsity and low redundancy simultaneously in the variables.\nThus, we introduce the inner product regularization for learning the feature weight matrix W,\ndefined as follows\nReg(W) =\nd\nX\ni=1\nd\nX\nj=1,j̸=i\n⟨wi, wj⟩= Tr\n\u00001d×dWWT \u0001\n−Tr\n\u0000WWT \u0001\n.\n(6)\nRecently, researchers have incorporated a regularization strategy termed dual sparsity inner\nproduct regularization into the feature selection model to enhance the selection of more prominent\nfeatures [28]. That is, the inner product regularization should not only apply to W but also to\nthe representation matrix H.\nThis consideration arises from the fact that, according to the\nequation group (1), we can obtain the i-th original feature in X approximation expression\nfi ≈XWhi = H1ifj1 + H2ifj2 + · · · + Hkifjk.\n(7)\nThe i-th column in H given in (7) plays an inevitable role in the self-representation property. In\nother words, when two original features fi and fj are highly dependent, it’s reasonable to expect\nthat their corresponding columns in the representation matrix which denoted as hi and hj should\nexhibit similar patterns. The inner product regularization applied to the representation matrix\nnot only induces sparsity in the columns of H but also shows beneficial in identifying redundant\nfeatures, preserving those with discriminative qualities. Thus, leveraging the advantages of inner\nproduct regularization, a regularization on H can be designed as\nReg(H) =\nd\nX\ni=1\nd\nX\nj=1,j̸=i\n⟨hi, hj⟩= Tr\n\u00001d×dHT H\n\u0001\n−Tr\n\u0000HT H\n\u0001\n.\n(8)\nIn summary, by substituting the indicator constraints for W with the inner product regu-\nlarization provided by (6), and integrating the regularization term on H given by (8) into the\nproblem (5), the objective function of our KAUFS method is formulated as:\narg min\nW,H −1\n2 Tr\n\u0000KcXWHHT WT XT \u0001\n+ α\n2\n\u0002\nTr\n\u00001d×dWWT \u0001\n−Tr\n\u0000WWT \u0001\u0003\n+ β\n2\n\u0002\nTr\n\u00001d×dHT H\n\u0001\n−Tr\n\u0000HT H\n\u0001\u0003\nsubject to W ≥0,\nH ≥0\n(9)\nwhere α and β are two regularization parameters to balance these three terms.\nAs a summary of what is discussed in this section, on one hand, in order to enable the\nmodel to consider nonlinear relationships among features, we utilize kernel alignment instead\nof traditional entry-wise matrix norms such as Frobenius norm, l2,1 norm, etc., as a subspace\n7\nlearning distance. On the other hand, in order to tackle the sparsity of the feature weight matrix\nW and characterize the correlation of features, we further apply inner product regularization\nto the feature weight matrix W and the representation matrix H.\nThe combination of the\nkernel alignment distance and the inner product regularization leads to a notable reduction in\nredundancy among the selected features.\n3\nAlgorithm and Convergence Analysis\nIn this section, we propose an iterative update algorithm to solve the KAUFS model and analyze\nits convergence.\n3.1\nAlgorithm\nIn order to solve the optimization problem (9), we utilize the Lagrange multiplier method. For\nthis purpose, we set Lagrange multipliers A ∈Rd×k and B ∈Rk×d for the constraints W ≥0\nand H ≥0. We construct the Lagrange function as follows\nL (W, H, A, B) = −1\n2 Tr\n\u0000KcXWHHT WT XT \u0001\n+ α\n2\n\u0002\nTr\n\u00001d×dWWT \u0001\n−Tr\n\u0000WWT \u0001\u0003\n+ β\n2\n\u0002\nTr\n\u00001d×dHT H\n\u0001\n−Tr\n\u0000HT H\n\u0001\u0003\n+ Tr\n\u0000AWT \u0001\n+ Tr\n\u0000BHT \u0001\n.\nCalculating the partial derivatives of L with respect to W and H respectively, we can obtain\n∂L\n∂W = −XT KcXWHHT + α1d×dW −αW + A\n(10)\nand\n∂L\n∂H = −WT XT KcXWH + βH1d×d −βH + B.\n(11)\nWe noticed that equations (10) and (11) include the term XT KcX.\nIf the data matrix\nX or the centered kernel matrix Kc contains negative values, this could result in the iterative\nupdate rules for W and H producing negative values, leading to W and H not satisfying the\nnon-negative constraint. To address this issue, we employ the technique proposed by Ding et\nal. [30] to further refine equations (10) and (11). That is, utilizing XT KcX to construct two\nnon-negative matrices, with each entry in these two matrices being\n\u0000XT KcX\n\u0001+\nij =\n\f\f\f\n\u0000XT KcX\n\u0001\nij\n\f\f\f +\n\u0000XT KcX\n\u0001\nij\n2\n,\n(12)\n\u0000XT KcX\n\u0001−\nij =\n\f\f\f\n\u0000XT KcX\n\u0001\nij\n\f\f\f −\n\u0000XT KcX\n\u0001\nij\n2\n.\n(13)\nIt can be verified that XT KcX =\n\u0000XT KcX\n\u0001+ −\n\u0000XT KcX\n\u0001−. Then, we have\n∂L\n∂W = −\n\u0000XT KcX\n\u0001+ WHHT +\n\u0000XT KcX\n\u0001−WHHT + α1d×dW −αW + A\n8\nand\n∂L\n∂H = −WT \u0000XT KcX\n\u0001+ WH + WT \u0000XT KcX\n\u0001−WH + βH1d×d −βH + B.\nFinally, exploiting the Karush-Kuhn–Tucker conditions AijW2\nij = 0 and BijH2\nij = 0 leads to\nthe following updating rules\nWij ←Wij\nv\nu\nu\nu\nu\nt\n\u0010\n(XT KcX)+ WHHT + αW\n\u0011\nij\n\u0010\n(XT KcX)−WHHT + α1d×dW\n\u0011\nij\n(14)\nand\nHij ←Hij\nv\nu\nu\nu\nu\nt\n\u0010\nWT (XT KcX)+ WH + βH\n\u0011\nij\n\u0010\nWT (XT KcX)−WH + βH1d×d\n\u0011\nij\n.\n(15)\nBased on the above analysis, the entire framework of the proposed KAUFS method is sum-\nmarized in Algorithm 1.\nAlgorithm 1 Kernel Alignment Unsupervised Feature Selection (KAUFS)\n1: Input Data matrix X ∈Rn×d, the centered kernel matrix Kc ∈Rn×n, the number\nof selected features k and two regularization parameters α and β;\n2: Initialize W ∈Rd×k and H ∈Rk×d;\n3: Compute\n\u0000XT KcX\n\u0001+ and\n\u0000XT KcX\n\u0001−using the rules (12) and (13);\n4: repeat\n5:\nUpdate Wij by the rule (14);\n6:\nUpdate Hij by the rule (15);\n7: until Convergence criterion has been satisfied;\n8: Output Sort all d features in descending order according to the value of\n\r\rwi\r\r\n2,\ni = 1, 2, . . . , d. The top k features of X are selected based on these top k highest\nl2-norm values of W to form the optimal feature subset, and the corresponding row\nindexes are regarded as the output of the KAUFS algorithm.\n3.2\nConvergence Analysis\nIn the following, we study the convergence of the proposed iterative update rules in Algorithm\n1.\nTheorem 1. For W, H ≥0, the values of the objective function given in (9) are non-increasing\nby employing the updating rules (14) and (15).\nProof. The objective function of the KAUFS method can be written as\nJ (W, H) = −1\n2 Tr\n\u0000KcXWHHT WT XT \u0001\n+ α\n2\n\u0002\nTr\n\u00001d×dWWT \u0001\n−Tr\n\u0000WWT \u0001\u0003\n+ β\n2\n\u0002\nTr\n\u00001d×dHT H\n\u0001\n−Tr\n\u0000HT H\n\u0001\u0003\n.\n(16)\n9\nProving this theorem consists of two steps. In the first step, we initially fix the matrix H\nand look for an appropriate auxiliary function [31] for J (W) according to the equation (16).\nSubsequently, we examine the descending behavior of J (W) utilizing the proposed auxiliary\nfunction. In the second step, we keep the matrix W fixed and perform a similar analysis on the\nupdating formula for H.\nLet the matrix H be fixed. By neglecting terms in equation (16) that remain constant or\nonly depend on the matrix H, we can reformulate the function J (W, H) into the following form\nJ (W) = −1\n2 Tr\n\u0000KcXWHHT WT XT \u0001\n+ α\n2\n\u0002\nTr\n\u00001d×dWWT \u0001\n−Tr\n\u0000WWT \u0001\u0003\n.\nLet A = XT KcX and B = HHT . Utilizing the trace cyclic property and formulas (12) and\n(13). The function J (W) can be further converted into\nJ (W) = −1\n2 Tr\n\u0000A+WBWT \u0001\n+ 1\n2 Tr\n\u0000A−WBWT \u0001\n+ α\n2 Tr\n\u00001d×dWWT \u0001\n−α\n2 Tr\n\u0000WWT \u0001\n.\nLet us define the following function\nG (W, W′) = −1\n2\nd\nX\ni=1\nd\nX\nj=1\nk\nX\nm=1\nk\nX\nn=1\nA+\nijW′\njmBmnW′\nin\n \n1 + log WjmWin\nW′\njmW′\nin\n!\n+ 1\n2\nd\nX\ni=1\nk\nX\nj=1\n(A−W′B)ij W2\nij\nW′\nij\n+ α\n2\nd\nX\ni=1\nk\nX\nj=1\n(1d×dW′)ij W2\nij\nW′\nij\n−α\n2\nd\nX\ni=1\nk\nX\nj=1\nW′2\nij\n \n1 + log W2\nij\nW′2\nij\n!\n,\nwhere W′ is a non-negative matrix in Rd×k. Our aim is to demonstrate that G (W, W′) is an\nauxiliary function for J (W). At first, it can be verified that when W′ = W, we have\nG (W, W) = −1\n2\nd\nX\ni=1\nd\nX\nj=1\nk\nX\nm=1\nk\nX\nn=1\nA+\nijWjmBmnWin\n+ 1\n2\nd\nX\ni=1\nk\nX\nj=1\n\u0000A−WB\n\u0001\nij Wij\n+ α\n2\nd\nX\ni=1\nk\nX\nj=1\n(1d×dW)ij Wij\n−α\n2\nd\nX\ni=1\nk\nX\nj=1\nW2\nij\n= −1\n2 Tr\n\u0000A+WBWT \u0001\n+ 1\n2 Tr\n\u0000A−WBWT \u0001\n+ α\n2 Tr\n\u00001d×dWWT \u0001\n−α\n2 Tr\n\u0000WWT \u0001\n,\n10\nwhich implies that G (W, W) = J (W). Then, we use the inequality z ≥1 + log z, which holds\nfor any z ≥0, and obtain\nTr\n\u0000A+WBWT \u0001\n=\nd\nX\ni=1\nd\nX\nj=1\nk\nX\nm=1\nk\nX\nn=1\nA+\nijWjmBmnWin\n≥\nd\nX\ni=1\nd\nX\nj=1\nk\nX\nm=1\nk\nX\nn=1\nA+\nijW′\njmBmnW′\nin\n \n1 + log WjmWin\nW′\njmW′\nin\n!\n(17)\nand\nTr\n\u0000WWT \u0001\n=\nd\nX\ni=1\nk\nX\nj=1\nW2\nij\n≥\nd\nX\ni=1\nk\nX\nj=1\nW′2\nij\n \n1 + log W2\nij\nW′2\nij\n!\n.\n(18)\nMoreover, by utilizing the Proposition 6 in [32], the following inequalities can be derived\nTr\n\u0000A−WBWT \u0001\n≤\nd\nX\ni=1\nk\nX\nj=1\n(A−W′B)ij W2\nij\nW′\nij\n(19)\nand\nTr\n\u00001d×dWWT \u0001\n≤\nk\nX\nj=1\n(1d×dW′)ij W2\nij\nW′\nij\n.\n(20)\nCombining the relations (17), (18), (19) and (20) yields that G (W, W′) ≥J (W).\nConsequently, by considering relations G (W, W) = J (W) and G (W, W′) ≥J (W), we\nconclude that G (W, W′) serves as an auxiliary function for J (W), which means that J (W)\nis non-increasing by the update rule arg minW G (W, W′). In order to determine the minima of\nG (W, W′), by taking the derivatives of G with respect to Wij, it turns out that\n∂G (W, W′)\n∂Wij\n= −\n\u0000A+W′B\n\u0001\nij\nW′ij\nWij\n+\n\u0000A−W′B\n\u0001\nij\nWij\nW′ij\n+ α (1d×dW′)ij\nWij\nW′ij\n−αW′\nij\nW′ij\nWij\n.\nBy setting ∂G (W, W′) /∂Wij to zero, and substituting\n\u0000XT KcX\n\u0001+,\n\u0000XT KcX\n\u0001−and HHT\ninto A+, A−and B respectively, we can derive\nWij = W′\nij\nv\nu\nu\nu\nu\nt\n\u0010\n(XT KcX)+ W′HHT + αW′\n\u0011\nij\n\u0010\n(XT KcX)−W′HHT + α1d×dW′\n\u0011\nij\n.\nThis implies that when the matrix H is fixed, the objective function of the KAUFS method\nexhibits non-increasing behavior with respect to the updating formula given by equation (14).\nThe rest of the proof regarding the decreasing behavior of the objective function of KAUFS,\nwith respect to the updating formula (15), follows a similar process as described above, and its\nexplanation is omitted.\n11\n4\nMutiple Kernel Method for KAUFS\nWe can apply the KAUFS model to learn the similarity information among data samples, thereby\nobtaining a discriminative subset of features. However, the performance of a kernel learning\nmodel typically depends on the choice of the kernel function, and the optimal kernel is typically\nunknown and computationally challenging to identify. Additionally, in cases where sample fea-\ntures contain heterogeneous information [33,34], the sample size is large [35,36], or samples are\nunevenly distributed in a high-dimensional feature space [37], using a single kernel function to\nmap all samples may not be appropriate.\nMultiple kernel learning has been proposed to address the above issues. It constructs several\ncandidate kernel matrices and combines them into a consensus kernel matrix [24]. This consensus\nkernel matrix helps accurately characterize the internal structure of the dataset and can achieve\nsuperior performance compared to a single kernel matrix or a combination of different kernels\nwith different parameters for the same kernel function. Moreover, the consensus kernel matrix\ndemonstrate enhanced robustness, minimizing the impact of noise or outliers and enhancing the\nstability of the model [38].\nIn this section, we extend the KAUFS model to a multiple kernel model and propose a\ncorresponding solving algorithm. By incorporating a sufficient number and diversity of kernel\nfunctions, the algorithm can jointly learns the optimal convex combination of kernel matrices\nalong with the feature weight matrix W and the representation matrix H. At the end of this\nsection, we provide the computational complexity analysis of the KAUFS algorithm and the\nMKAUFS algorithm respectively.\n4.1\nMKAUFS Model\nGiven a dataset X ∈Rn×d with n samples, each having d features, we suppose there are N dif-\nferent kernel functions {ki}N\ni=1 available for the unsupervised feature selection task. Accordingly,\nthere are N different kernel spaces. Typically, the most suitable kernel space is unknown. An in-\ntuitive way to use them is to concatenate all by concatenating all feature spaces into an augmented\nHilbert space and associating each feature space with a relevance weight ηi, i = 1, 2, . . . , N. More\nspecifically, we apply these N kernel functions to the dataset, generating N different kernel ma-\ntrices, and centralize them using (3). Subsequently, we consider a convex combination of these\ncentered kernel matrices\nn\nK(i)\nc\noN\ni=1, i.e.\nK =\nN\nX\ni=1\nηiK(i)\nc ,\nsubject to\nN\nX\ni=1\nηi = 1,\nηi ≥0.\n12\nWe substitute this relation into (9) to obtain the objective function for Multiple Kernel Alignment\nUnsupervised Feature Selection (MKAUFS):\narg min\nW,H −1\n2 Tr\n N\nX\ni=1\nηiK(i)\nc XWHHT WT XT\n!\n+ α\n2\n\u0002\nTr\n\u00001d×dWWT \u0001\n−Tr\n\u0000WWT \u0001\u0003\n+ β\n2\n\u0002\nTr\n\u00001d×dHT H\n\u0001\n−Tr\n\u0000HT H\n\u0001\u0003\n+ γ\n2 ∥η∥2\n2\nsubject to W ≥0,\nH ≥0,\nN\nX\ni=1\nηi = 1,\nηi ≥0.\n(21)\nWe denote the kernel weights as a vector η = [η1, . . . , ηN]T and further impose the l2-norm to it\nas the regularization term ∥η∥2\n2 to prevent the weights from overfitting to one kernel.\n4.2\nAlgorithm\nIn this subsection, we propose an algorithm to solve (21). Similar to the KAUFS method, we\nalso adopt an iterative strategy to alternately optimize (W, H) and η, while holding the other\nvariable as constant.\n(1) Optimizing with respect to (W, H) when η is fixed. The optimization problem\n(21) is reduced to\narg min\nW,H −1\n2 Tr\n\u0000KXWHHT WT XT \u0001\n+ α\n2\n\u0002\nTr\n\u00001d×dWWT \u0001\n−Tr\n\u0000WWT \u0001\u0003\n+ β\n2\n\u0002\nTr\n\u00001d×dHT H\n\u0001\n−Tr\n\u0000HT H\n\u0001\u0003\nsubject to W ≥0,\nH ≥0\nwhere K = PN\ni=1 ηiK(i)\nc . Similar to the optimization of W and H of KAUFS method, we have\nfollowing rules to update W and H respectively.\nWij ←Wij\nv\nu\nu\nu\nu\nt\n\u0010\n(XT KX)+ WHHT + αW\n\u0011\nij\n\u0010\n(XT KX)−WHHT + α1d×dW\n\u0011\nij\n(22)\nand\nHij ←Hij\nv\nu\nu\nu\nu\nt\n\u0010\nWT (XT KX)+ WH + βH\n\u0011\nij\n\u0010\nWT (XT KX)−WH + βH1d×d\n\u0011\nij\n.\n(23)\n(2) Optimizing with respect to η when W and H are fixed. Removing the irrelevant\n13\nterms, the optimization problem (21) becomes\narg min\nη\n−1\n2 Tr\n N\nX\ni=1\nηiK(i)\nc XWHHT WT XT\n!\n+ γ\n2 ∥η∥2\n2 = −1\n2\nN\nX\ni=1\nηifi + γ\n2\nN\nX\ni=1\nη2\ni\nsubject to\nN\nX\ni=1\nηi = 1,\nηi ≥0\n,\n(24)\nwhere\nfi = Tr\n\u0010\nK(i)\nc XWHHT WT XT \u0011\n.\n(25)\nThe optimization of (24) with respect to the kernel weights η could be solved as a standard\nquadratic programming (QP) problem. In Algorithm 2, we provide a complete algorithm for\nsolving the problem (21).\nAlgorithm 2 Multiple Kernel Alignment Unsupervised Feature Selection (MKAUFS)\n1: Input Data matrix X ∈Rn×d, A set of centered kernel matrices\nn\nK(i)\nc\noN\ni=1, the\nnumber of selected features k and three regularization parameters α, β and γ;\n2: Initialize W ∈Rd×k and H ∈Rk×d;\n3: Initialize the kernel weights as ηi = 1\nN , i = 1, 2, . . . , N;\n4: repeat\n5:\nCompute the combined kernel K = PN\ni=1 ηiK(i)\nc ;\n6:\nCompute\n\u0000XT KX\n\u0001+ and\n\u0000XT KX\n\u0001−using the rules (12) and (13);\n7:\nUpdate Wij by the rule (22);\n8:\nUpdate Hij by the rule (23);\n9:\nCompute fi for i = 1, 2, . . . , N by the rule (25);\n10:\nUpdate the kernel weights η by (24);\n11: until Convergence criterion has been satisfied;\n12: Output Sort all d features in descending order according to the value of\n\r\rwi\r\r\n2,\ni = 1, 2, . . . , d. The top k features of X are selected based on these top k highest\nl2-norm values of W to form the optimal feature subset, and the corresponding row\nindexes are regarded as the output of the MKAUFS algorithm.\n4.3\nComputational Complexity\nThe computational complexity of KAUFS and MKAUFS is illustrated below. We first analyze the\ncomputational complexity of the KAUFS method. The primary computation cost of the update\nrule (14) for W is the calculation of\n\u0000XT KcX\n\u0001\nWHHT . It’s worth noting that here we don’t\nneed to differentiate between\n\u0000XT KcX\n\u0001+ and\n\u0000XT KcX\n\u0001−, since the major computation cost\nin equations (12) and (13) is\n\u0000XT KcX\n\u0001\n. Assuming k, n ≪d, we can obtain\n\u0000XT KcX\n\u0001\nWHHT\nby first computing XT Kc and HHT , then XT KcX and WHHT , and finally left-multiplying\nXT KcX to WHHT .\nThis way, it takes 2dn2 + 4dk2 + 2nd2 + 2kd2 operations.\nSimilarly,\nthe cost of updating H is mainly allocated to calculating WT \u0000XT KcX\n\u0001\nWH, which takes\n2dn2 + 4kd2 + 2nd2 + 2dk2 operations. Consequently, the computational complexity for each\niteration of Algorithm 1 is almost equal to O\n\u0000d2\u0001\n.\n14\nNext, we analyze the computational complexity of the MKAUFS method. The major compu-\ntational cost of Algorithm 2 are the updating rules (22), (23), and (25). Assuming k, n, N ≪d,\nthe number of operations for updating rules (22) and (23) align with the conclusions drawn for\nAlgorithm 1. As for the equation (25), we determine the optimal sequence of matrix multipli-\ncation. Firstly, we compute XW, HHT , and WT XT , then K(i)\nc XW and HHT WT XT , and\nfinally K(i)\nc XWHHT WT XT . For N kernels, equation (25) requires N iterations, each taking\n2dk2 + 4kdn + 4kn2 + 2nk2 operations, which is fewer than updating rules (22) and (23). Thus,\nthe computational complexity for each iteration of Algorithm 2 is almost equal to O\n\u0000d2\u0001\n.\n5\nNumerical Experiments\nIn this section, we conduct experiments to demonstrate the effectiveness of our proposed al-\ngorithms in clustering applications using a variety of real-world datasets. Detailed information\nabout the datasets utilized in this paper, the experimental settings, and the performance compar-\nison of KAUFS and MKAUFS with other classic and state-of-the-art UFS methods are provided.\nSpecifically, the comparison framework proceeds through three key steps: Firstly, the UFS algo-\nrithm is applied to the input dataset to select a number of representative features. Subsequently,\nthe set of selected features and the desired number of classes are given as input to the k-means\nclustering algorithm to produce a vector containing predicted class labels for each sample. Fi-\nnally, the performance of all UFS methods is assessed by computing three evaluation metrics\nusing the predicted labels, true labels, and the selected features.\n5.1\nDatasets\nUtilizing datasets from diverse fields provides a robust platform for assessing the effectiveness\nof different UFS methods.\nIn this study, we used eight datasets including images, artificial\ndata, and gene expression data, each detailed in Table 1, to conduct computational experiments.\nSpecifically, to assess performance on large-scale applied datasets, we utilized a high-dimensional\nface dataset (orlraws). These datasets are available in the scikit-feature selection repository [39].\nTable 1: Summary of the benchmark datasets.\nDataset\n# of Instances\n# of Features\n# of Classes\nData types\nWarpAR\n130\n2400\n10\nFace image\nWarpPIE\n210\n2420\n10\nFace image\nYale64\n165\n4096\n15\nFace image\nOrlraws\n100\n10,304\n10\nFace image\nMadelon\n2600\n500\n2\nArtificial feature\nGLIOMA\n50\n4434\n4\nGene expression\nTOX 171\n171\n5748\n4\nGene expression\nProstate GE\n102\n5966\n2\nGene expression\n5.2\nComparison Methods\nTo evaluate the quality of our proposed methods, we compare our proposed KAUFS and MKAUFS\nmethods with using all features and 6 other UFS methods as follows:\n15\n• Baseline: All original features are selected.\n• LS [40]: Laplacian Score Feature Selection, which selects the features that can best preserve\nthe local manifold structure of data.\n• KMFFS [14]: Kernelized Matrix Factorization Feature Selection, which selects features\nthrough matrix factorization with an orthogonal constraint and leverages kernel functions\nto capture nonlinear relationships among features.\n• SCFS [12]: Subspace Clustering Unsupervised Feature Selection, which employs a self-\nexpressive model to learn the clustering similarities in an adaptive manner to select fea-\ntures.\n• DRFSMFMR [28]: Dual Regularized Unsupervised Feature Selection, which is based on\nmatrix factorization and employs two inner product-based regularizations along with the\nglobal correlation regularization to achieve minimum redundancy in the feature subset.\n• VCSDFS [16]: Variance–Covariance Subspace Distance Unsupervised Feature Selection,\nwhich considers variance and covariance information of the feature space to characterize a\ngroup of more representative features.\n• LS-CAE [41]: Laplacian Score-regularized Concrete Autoencoder (CAE) based Feature\nSelection, which employs autoencoder architecture to cope with correlated features and\nutilizes Laplacian score criterion to keep away from the selection of nuisance features.\n5.3\nEvaluation Metrics\nTo evaluate the clustering performance of unsupervised feature selection, we employ two most\nprevalent measures, they are [9]: Clustering Accuracy (ACC) and Normalized Mutual Infor-\nmation (NMI). ACC measures the extent to which each cluster contain data points from the\ncorresponding class. NMI reflects the consistency between clustering results and ground truth\nlabels. Additionally, we construct a metric for measuring the selected features’ redundancy rate\nbased on Distance Correlation (DC) [42], which allows the consideration of both linear and non-\nlinear relationships between two variables. We name this metric the Redundancy Rate (RED).\nHere, it should be emphasized that the higher the value of ACC or NMI, the better the clustering\nperformance is, which indicates that one can expect a feature selection method to be more suc-\ncessful in selecting more effective features. Additionally, the lower the value of RED, the more\nindependent the features are within the selected subset.\nThe measure ACC is defined as follows:\nACC =\nPn\ni=1 δ (pi, map (qi))\nn\n,\nwhere δ (a, b) = 1 if a = b and 0, otherwise. pi and qi are the labels provided by the dataset\nand the obtained clustering label, respectively. map(•) is the permutation mapping function\nthat matches the obtained clustering label to the equivalent label of the dataset using the\nKuhn–Munkres algorithm. The NMI measure is characterized as:\nNMI(P, Q) =\nI(P, Q)\np\nH(P)H(Q)\n,\nwhere H(P) and H(Q) are the entropies of P and Q, respectively, and I(P, Q) is the mutual\ninformation between P and Q. For clustering, P and Q are the clustering results and the true\nlabels, respectively.\n16\nAssume that F is the set of selected features, which contain m features. Then RED is defined\nas:\nRED (F) =\n2\nm(m −1)\nm−1\nX\ni=1\nm\nX\nj=i+1\nDC (fi, fj) ,\nwhere DC (fi, fj) represents the distance correlation between feature fi and fj, where fi, fj ∈F.\n5.4\nExperimental Setting\nThere are certain parameters in the proposed methods KAUFS and MKAUFS, as well as in the\nother comparison methods mentioned above, whose values need to be predefined. The numbers\nof the selected features are taken as {10t, t = 1, 2, 3, 4, 5, 6, 7, 8, 9, 10} for all datasets. In the\nLaplacian Score method, the parameter k in the built-in k-nearest neighbour algorithm is set to\n5 for all datasets. As offered in [14], the penalty parameter ρ for the KMFFS method is set to\n108. For the SCFS method, as stated in [12], the parameter γ is fixed as 106, and the value of the\nother regularization parameters are searched in {10t, t = −4, −2, 0, 2, 4}. For the DRFSMFMR\nmethod, we follow the settings in the article [28], where the trade-off parameters α, β, and γ are\ntuned in {10t, t = −3, −2, −1, 0, 1, 2, 3}. For the VCSDFS method, the value of the parameter\nρ is varied from {10t, t = −5, −4, −3, −2, −1, 0, 1, 2, 3, 4, 5}.\nThe settings related to the LS-\nCAE method are based on those given in [41]. Finally, for our proposed methods KAUFS and\nMKAUFS, all trade-off parameters α, β, and γ are tuned from {10t, t = −3, −2, −1, 0, 1, 2, 3}.\nIn addition, we apply four widely used types of kernel functions, including linear, polyno-\nmial, Gaussian, and Laplacian kernels, to the KMFFS, KAUFS, and MKAUFS methods. Their\nexpressions are as follows:\nLinear kernel:\nk (x, y) = xT y.\nPolynomial kernel:\nk (x, y) =\n\u0000xT y + c\n\u0001d .\nGaussian kernel:\nk (x, y) = exp\n\u0012\n−∥x −y∥2\n2\n2σ2\n\u0013\n.\nLaplacian kernel:\nk (x, y) = exp\n\u0012\n−∥x −y∥1\nσ\n\u0013\n.\nHere c, d and σ are the parameters of the kernels. For the parameter settings of the kernel\nfunctions, the parameters of the polynomial kernel are fixed as c = 1 and d = {2, 4, 6}, and\nthose of the Gaussian kernel and the Laplacian kernel are set as σ = {10t, t = −2, −1, 0, 1, 2}.\nFor single kernel methods, we run KMFFS and KAUFS on each kernel separately.\nAnd we\nreport both the best and the average results over all these kernels. For the MKAUFS method,\nwhich is a multiple kernel method, we implement it on a combination of the kernels mentioned\nabove. It’s worth noting that for the MKAUFS method, to avoid the influence of differences in\nmagnitudes between different kernel matrices during optimization process, we need to standardize\nthe centralized kernel. We use the following formula for all kernels:\n˜Ki,j =\nKi,j\np\nKi,iKj,j\n,\nwhere i and j respectively represent the i-th row and j-th column of the kernel matrix K.\nIn the experiments, we utilize the k-means algorithm to cluster the samples based on the\nselected features, evaluating the UFS methods’ performance through their clustering results.\nFor simplicity, we set the number of clusters as the number of classes. It’s also important to\n17\nnote that the k-means performance highly depends on the initial point selection. To mitigate\nthis issue, we repeat the process 30 times with random initialization and report the average\nvalue and the standard deviation value of the performance measures ACC and NMI. For the\nredundancy measure, for each UFS method, we report the average redundancy rate of all feature\nsubsets, which corresponds to the feature subset results where each achieves the best clustering\nperformance.\n5.5\nResult and Analysis\nWe first present the numerical experimental results of all algorithms on eight datasets, and finally,\nwe conduct parameter sensitivity tests on the KAUFS method and the MKAUFS method.\n5.5.1\nExperimental Results\nTo compare different UFS methods, we summarize their best clustering results in Tables 2 and\n3. Particularly, for the KMFFS method and the KAUFS method, “-b” denotes the best result\nand “-a” means the average of those 14 kernels. In Table 4, we report the average redundancy\nrate of these methods in all datasets. In Tables 2, 3, and 4, we bold the best and underlined the\nsecond-best performance results.\nFor the clustering performance, the ACC results given in Table 2 and NMI results given in\nTable 3, exhibit that our proposed methods provide the best results in most cases. Moreover,\namong the other state-of-the-art methods, DRFSMFMR method and KMFFS performs the best\nin most datasets.\nThe difference between DRFSMFMR and our methods demonstrates the\nadvantage of the kernel approach, while the contrast between KMFFS and our methods highlights\nthe advantage of kernel alignment as a subspace distance. However, for the ACC results, it’s\nworth noting that our proposed methods, along with others such as DRFSMFMR and LS-CAE,\nexhibit nearly equivalent efficacy to the Baseline on dataset Yale64. Particularly, in this dataset,\nthe SCFS method outperforms the other methods.\nIn addition, in the case of single kernel methods like KMFFS and KAUFS, significant differ-\nences between the best and average results are noticeable across datasets Yale64, Orlraws, and\nGLIOMA. This observation aligns with the general conclusion that the performance of single\nkernel methods is typically contingent upon the choice of kernel function. Such findings also\nmotivate the development of multiple kernel learning techniques. Furthermore, we can observe\nthat the MKAUFS method frequently achieves close or even superior performance compared to\nthe results obtained using the best single kernel of the KAUFS method. This observation reveals\nthe practical utility of multiple kernel learning methods, especially considering the avoidance of\nexhaustive searches across numerous candidate kernels.\nFor the average redundancy rate results in Table 4, we observe that the feature subsets\nselected by the three kernel-based algorithms, namely KMFFS, KAUFS, and MKAUFS meth-\nods, generally exhibit relatively low redundancy rates. This suggests the effectiveness of kernel\nmethods in capturing nonlinear relationships between features, thus enhancing the indepen-\ndence among the selected features. It is evident that high redundancy rates are observed for\nDRFSMFMR and VCSDFS, which solely consider linear relationships and cannot capture more\ncomplex relationships among features, resulting in higher redundancy rates. Ranking second are\nthe Laplacian-based LS and LS-CAE methods, whose performance also reflects the effectiveness\nof Laplacian-based feature selection algorithms in characterizing the manifold structure of the\ndata, thereby better capturing both linear and nonlinear relationships between features. How-\never, as these methods evaluate each feature individually, the interaction between features is\nignored, which could potentially compromise clustering performance.\n18\nTable 2: Clustering results (ACC% ± std%) of different feature selection algorithms on\ndifferent datasets. The best results are bolded and the second best results are underlined\n(the higher the better).\nDataset\nBaseline\nLS\nLS-CAE\nVCSDFS\nWarpAR\n25.89 ± 3.28\n27.53 ± 2.54\n38.28 ± 3.44\n41.27 ± 1.58\nWarpPIE\n26.19 ± 1.52\n35.44 ± 1.28\n42.09 ± 3.78\n40.88 ± 4.32\nYale64\n48.63 ± 3.55\n41.35 ± 2.64\n48.52 ± 2.34\n46.62 ± 2.59\nOrlraws\n75.77 ± 4.91\n76.10 ± 1.71\n75.36 ± 3.81\n69.86 ± 5.11\nMadelon\n50.32 ± 0.41\n54.67 ± 1.47\n55.22 ± 0.05\n58.63 ± 0.13\nGLIOMA\n56.53 ± 3.99\n52.37 ± 3.86\n52.95 ± 0.01\n62.33 ± 4.95\nTOX 171\n41.85 ± 2.31\n40.48 ± 2.16\n51.28 ± 2.94\n49.02 ± 3.51\nProstate GE\n58.62 ± 0.00\n60.68 ± 0.00\n69.10 ± 0.40\n62.74 ± 1.34\nDataset\nDRFSMFMR\nSCFS\nKMFFS-b\nKMFFS-a\nWarpAR\n44.28 ± 2.24\n34.23 ± 2.15\n40.40 ± 3.37\n36.93\nWarpPIE\n43.57 ± 3.71\n35.33 ± 1.75\n42.76 ± 3.42\n33.85\nYale64\n48.64 ± 4.34\n59.28 ± 2.55\n54.02 ± 1.85\n46.31\nOrlraws\n74.36 ± 3.42\n70.63 ± 3.29\n78.90 ± 3.88\n71.66\nMadelon\n62.65 ± 0.16\n59.81 ± 1.08\n61.18 ± 1.56\n59.27\nGLIOMA\n58.23 ± 4.76\n57.78 ± 0.81\n62.26 ± 8.02\n45.32\nTOX 171\n58.50 ± 2.64\n55.56 ± 0.06\n46.61 ± 4.99\n42.73\nProstate GE\n68.73 ± 4.51\n59.81 ± 0.48\n62.41 ± 2.33\n52.78\nDataset\nKAUFS-b\nKAUFS-a\nMKAUFS\nWarpAR\n46.22 ± 2.18\n40.38\n49.23 ± 2.66\nWarpPIE\n43.94 ± 1.29\n38.63\n45.57 ± 2.50\nYale64\n49.17 ± 3.48\n35.29\n49.82 ± 4.18\nOrlraws\n88.33 ± 6.69\n71.16\n89.37 ± 5.41\nMadelon\n66.98 ± 1.95\n64.46\n67.04 ± 1.13\nGLIOMA\n67.13 ± 4.81\n53.78\n68.24 ± 0.73\nTOX 171\n60.47 ± 3.15\n54.27\n60.88 ± 3.77\nProstate GE\n71.51 ± 4.45\n65.68\n73.24 ± 1.26\n19\nTable 3: Clustering results (NMI% ± std%) of different feature selection algorithms on\ndifferent datasets. The best results are bolded and the second best results are underlined\n(the higher the better).\nDataset\nBaseline\nLS\nLS-CAE\nVCSDFS\nWarpAR\n28.29 ± 2.44\n27.45 ± 2.52\n42.48 ± 2.54\n48.72 ± 6.95\nWarpPIE\n26.72 ± 2.07\n33.29 ± 1.60\n44.87 ± 2.02\n48.04 ± 1.98\nYale64\n56.42 ± 2.27\n51.93 ± 2.45\n54.77 ± 2.32\n53.30 ± 2.38\nOrlraws\n80.66 ± 3.97\n75.43 ± 1.47\n81.54 ± 2.51\n77.41 ± 3.12\nMadelon\n2.54 ± 0.00\n7.88 ± 0.45\n8.49 ± 0.05\n7.46 ± 0.00\nGLIOMA\n49.85 ± 2.68\n48.28 ± 2.38\n46.32 ± 5.18\n50.54 ± 3.55\nTOX 171\n14.23 ± 2.09\n19.91 ± 2.57\n26.84 ± 1.78\n37.21 ± 4.32\nProstate GE\n2.62 ± 0.00\n6.49 ± 0.00\n7.78 ± 0.45\n7.44 ± 0.67\nDataset\nDRFSMFMR\nSCFS\nKMFFS-b\nKMFFS-a\nWarpAR\n42.07 ± 2.30\n31.37 ± 1.61\n40.86 ± 2.08\n32.49\nWarpPIE\n47.32 ± 2.77\n41.61 ± 1.46\n44.74 ± 3.23\n35.21\nYale64\n55.04 ± 2.11\n64.28 ± 1.84\n62.24 ± 2.36\n55.04\nOrlraws\n78.11 ± 2.31\n79.83 ± 2.58\n84.87 ± 3.11\n78.45\nMadelon\n12.13 ± 1.10\n3.46 ± 0.00\n10.65 ± 1.69\n2.25\nGLIOMA\n53.98 ± 4.35\n51.61 ± 3.03\n53.06 ± 6.75\n25.62\nTOX 171\n33.58 ± 1.75\n34.62 ± 0.79\n35.64 ± 5.37\n24.32\nProstate GE\n18.13 ± 1.83\n7.46 ± 1.24\n8.90 ± 3.67\n3.79\nDataset\nKAUFS-b\nKAUFS-a\nMKAUFS\nWarpAR\n50.71 ± 4.37\n41.71\n54.04 ± 1.92\nWarpPIE\n45.81 ± 3.44\n39.18\n48.82 ± 1.84\nYale64\n50.79 ± 1.93\n41.58\n52.10 ± 2.56\nOrlraws\n86.67 ± 0.24\n74.33\n87.21 ± 2.32\nMadelon\n15.36 ± 0.78\n9.23\n16.01 ± 2.19\nGLIOMA\n59.11 ± 3.83\n44.68\n59.33 ± 3.07\nTOX 171\n29.47 ± 2.84\n24.22\n32.21 ± 4.04\nProstate GE\n15.46 ± 1.65\n10.51\n17.97 ± 1.92\n20\nTable 4: Average redundancy rates (average RED%) of different feature selection algo-\nrithms on different datasets. The best results are bolded and the second best results are\nunderlined (the lower the better).\nDataset\nLS\nLS-CAE\nVCSDFS\nDRFSMFMR\nWarpAR\n49.29\n45.39\n62.97\n52.91\nWarpPIE\n67.66\n47.58\n71.85\n70.19\nYale64\n34.64\n32.79\n42.25\n40.76\nOrlraws\n39.55\n35.06\n41.31\n46.60\nMadelon\n3.58\n3.67\n4.32\n4.12\nGLIOMA\n38.10\n38.21\n48.31\n44.47\nTOX 171\n18.20\n19.31\n21.95\n26.34\nProstate GE\n45.56\n48.2\n50.18\n47.06\nDataset\nSCFS\nKMFFS\nKAUFS\nMKAUFS\nWarpAR\n61.70\n39.63\n38.44\n42.89\nWarpPIE\n41.78\n43.89\n40.33\n44.21\nYale64\n28.63\n29.51\n23.51\n28.25\nOrlraws\n42.35\n32.18\n26.98\n27.59\nMadelon\n4.76\n3.27\n3.47\n3.32\nGLIOMA\n37.29\n42.73\n43.69\n39.82\nTOX 171\n27.87\n17.16\n16.23\n16.17\nProstate GE\n66.97\n38.67\n34.59\n31.71\n5.5.2\nParameter Analysis\nOur models KAUFS and MKAUFS have two main important trade-off parameters, α and β,\nwhich control inner-product regularizations to capture local correlation information among fea-\ntures.\nWe empirically assess their impact by varying α and β within the range {10t, t =\n−3, −2, −1, 0, 1, 2, 3, 4}, and setting γ to 1. We fix the number of selected features at 100 and\ncompute the clustering performance (ACC and NMI) of these two algorithms. Figures 1 and\n2 depict how the clustering results in terms of ACC and NMI evolve with these penalty pa-\nrameters on the WarpAR dataset and Yale64 datasets respectively. The figures reveal that the\nperformance of KAUFS and MKAUFS exhibits stability across a broad range of α and β values.\n21\nFigure 1: Clustering performance ACC and NMI of KAUFS and MKAUFS w.r.t α and\nβ on the WarpAR dataset.\n22\nFigure 2: Clustering performance ACC and NMI of KAUFS and MKAUFS w.r.t α and\nβ on the Yale64 dataset.\n6\nConclusions\nIn this paper, we propose an unsupervised feature selection algorithm based on subspace learn-\ning and matrix factorization.\nWe introduce a novel subspace distance, termed as the kernel\nalignment subspace distance, capable of capturing the nonlinear information embedded in the\ndataset features. Additionally, to further reduce the redundancy of the selected features subset,\ntwo inner product-based regularizations were employed to capture the dependence information\namong the selected features during the feature selection process. Furthermore, an efficient algo-\nrithm was developed to address the optimization problem of the proposed model, and we extend\nthe model to a multi-kernel framework, which effectively learns an appropriate consensus kernel\nfrom a pool of kernels. This approach reduces the complexity of kernel selection and therefore\nholds greater practical significance. Finally, our extensive experiments on real datasets clearly\ndemonstrated that our methods can effectively identify the most representative features with\nminimal redundancy. There are several interesting future directions. In particular, 1) improving\n23\nsubspace distance measurement methods could allow for the use of non-positive definite kernels,\n2) utilizing the manifold structure information of the dataset, such as constructing a Laplacian\ngraph and incorporating this information into the regularization framework, can also serve as a\npromising tool for enhancing the clustering performance and reducing redundancy of the feature\nsubset, and 3) developing fast and reliable optimization algorithms to search for the global opti-\nmum of such optimization problems, rather than using matrix-based gradient descent algorithms\nto find local minima.\n24\nReferences\n[1] Arshi Parvaiz, Muhammad Anwaar Khalid, Rukhsana Zafar, Huma Ameer, Muhammad\nAli, and Muhammad Moazam Fraz. Vision transformers in medical computer vision—a\ncontemplative retrospection. Engineering Applications of Artificial Intelligence, 122:106126,\n2023.\n[2] Arunabha M Roy, Jayabrata Bhaduri, Teerath Kumar, and Kislay Raj. Wildect-yolo: An\nefficient and robust computer vision-based accurate object localization model for automated\nendangered wildlife detection. Ecological Informatics, 75:101919, 2023.\n[3] Can Chen, Scott T Weiss, and Yang-Yu Liu. Graph convolutional network-based feature\nselection for high-dimensional and low-sample size data.\nBioinformatics, 39(4):btad135,\n2023.\n[4] Sanur Sharma and Anurag Jain. Hybrid ensemble learning with feature selection for senti-\nment classification in social media. In Research Anthology on Applying Social Networking\nStrategies to Classrooms and Libraries, pages 1183–1203. IGI Global, 2023.\n[5] Lance Parsons, Ehtesham Haque, and Huan Liu. Subspace clustering for high dimensional\ndata: a review. Acm sigkdd explorations newsletter, 6(1):90–105, 2004.\n[6] Jianqing Fan and Yingying Fan. High dimensional classification using features annealed\nindependence rules. Annals of statistics, 36(6):2605, 2008.\n[7] Isabelle Guyon and Andr´e Elisseeff.\nAn introduction to variable and feature selection.\nJournal of machine learning research, 3(Mar):1157–1182, 2003.\n[8] Isabelle Guyon, Steve Gunn, Masoud Nikravesh, and Lofti A Zadeh. Feature extraction:\nfoundations and applications, volume 207. Springer, 2008.\n[9] Sa´ul Solorio-Fern´andez, J Ariel Carrasco-Ochoa, and Jos´e Fco Mart´ınez-Trinidad. A review\nof unsupervised feature selection methods. Artificial Intelligence Review, 53(2):907–948,\n2020.\n[10] Nan Zhou, Yangyang Xu, Hong Cheng, Jun Fang, and Witold Pedrycz. Global and local\nstructure preserving sparse subspace learning: An iterative approach to unsupervised feature\nselection. Pattern Recognition, 53:87–101, 2016.\n[11] Zheng Wang, Feiping Nie, Lai Tian, Rong Wang, and Xuelong Li. Discriminative feature\nselection via a structured sparse subspace learning module. In IJCAI, pages 3009–3015,\n2020.\n[12] Mohsen Ghassemi Parsa, Hadi Zare, and Mehdi Ghatee. Unsupervised feature selection\nbased on adaptive similarity learning and subspace clustering. Engineering Applications of\nArtificial Intelligence, 95:103855, 2020.\n[13] Feiping Nie, Zheng Wang, Lai Tian, Rong Wang, and Xuelong Li. Subspace sparse discrim-\ninative feature selection. IEEE Transactions on Cybernetics, 52(6):4221–4233, 2020.\n[14] Shiping Wang, Witold Pedrycz, Qingxin Zhu, and William Zhu.\nSubspace learning for\nunsupervised feature selection via matrix factorization. Pattern Recognition, 48(1):10–19,\n2015.\n[15] Pengfei Zhu, Wangmeng Zuo, Lei Zhang, Qinghua Hu, and Simon CK Shiu. Unsupervised\nfeature selection by regularized self-representation.\nPattern Recognition, 48(2):438–446,\n2015.\n25\n[16] Saeed Karami, Farid Saberi-Movahed, Prayag Tiwari, Pekka Marttinen, and Sahar Vahdati.\nUnsupervised feature selection based on variance-covariance subspace distance. Neural Net-\nworks, 2023.\n[17] Shiping Wang, Witold Pedrycz, Qingxin Zhu, and William Zhu.\nUnsupervised feature\nselection via maximum projection and minimum redundancy. Knowledge-Based Systems,\n75:19–29, 2015.\n[18] Miao Qi, Ting Wang, Fucong Liu, Baoxue Zhang, Jianzhong Wang, and Yugen Yi. Unsuper-\nvised feature selection by regularized matrix factorization. Neurocomputing, 273:593–610,\n2018.\n[19] Nello Cristianini, John Shawe-Taylor, Andre Elisseeff, and Jaz Kandola. On kernel-target\nalignment. Advances in neural information processing systems, 14, 2001.\n[20] Corinna Cortes, Mehryar Mohri, and Afshin Rostamizadeh. Algorithms for learning kernels\nbased on centered alignment. The Journal of Machine Learning Research, 13(1):795–828,\n2012.\n[21] Xiaokai Wei, Bokai Cao, and Philip S Yu. Nonlinear joint unsupervised feature selection.\nIn Proceedings of the 2016 SIAM International Conference on Data Mining, pages 414–422.\nSIAM, 2016.\n[22] Xiaoying Xing, Hongfu Liu, Chen Chen, and Jundong Li.\nFairness-aware unsupervised\nfeature selection. In Proceedings of the 30th ACM International Conference on Information\n& Knowledge Management, pages 3548–3552, 2021.\n[23] Martin Palazzo, Pierre Beauseroy, and Patricio Yankilevich. Unsupervised feature selection\nfor tumor profiles using autoencoders and kernel methods. In 2020 IEEE Conference on\nComputational Intelligence in Bioinformatics and Computational Biology (CIBCB), pages\n1–8. IEEE, 2020.\n[24] Mehmet G¨onen and Ethem Alpaydın. Multiple kernel learning algorithms. The Journal of\nMachine Learning Research, 12:2211–2268, 2011.\n[25] Ronghua Shang, Yang Meng, Wenbing Wang, Fanhua Shang, and Licheng Jiao.\nLocal\ndiscriminative based sparse subspace learning for feature selection. Pattern Recognition,\n92:219–230, 2019.\n[26] Weiyi Li, Hongmei Chen, Tianrui Li, Jihong Wan, and Binbin Sang. Unsupervised fea-\nture selection via self-paced learning and low-redundant regularization. Knowledge-Based\nSystems, 240:108150, 2022.\n[27] Feiping Nie, Heng Huang, Xiao Cai, and Chris Ding. Efficient and robust feature selection\nvia joint l2, 1-norms minimization. Advances in neural information processing systems, 23,\n2010.\n[28] Farid Saberi-Movahed, Mehrdad Rostami, Kamal Berahmand, Saeed Karami, Prayag Ti-\nwari, Mourad Oussalah, and Shahab S Band. Dual regularized unsupervised feature selection\nbased on matrix factorization and minimum redundancy with application in gene selection.\nKnowledge-Based Systems, 256:109884, 2022.\n[29] Jiuqi Han, Zhengya Sun, and Hongwei Hao. Selecting feature subset with sparsity and low\nredundancy for unsupervised learning. Knowledge-Based Systems, 86:210–223, 2015.\n26\n[30] Chris HQ Ding, Tao Li, and Michael I Jordan. Convex and semi-nonnegative matrix fac-\ntorizations. IEEE transactions on pattern analysis and machine intelligence, 32(1):45–55,\n2008.\n[31] Daniel Lee and H Sebastian Seung. Algorithms for non-negative matrix factorization. Ad-\nvances in neural information processing systems, 13, 2000.\n[32] Chris Ding, Tao Li, Wei Peng, and Haesun Park.\nOrthogonal nonnegative matrix t-\nfactorizations for clustering. In Proceedings of the 12th ACM SIGKDD international con-\nference on Knowledge discovery and data mining, pages 126–135, 2006.\n[33] Paul Pavlidis, Jason Weston, Jinsong Cai, and William Noble Grundy. Gene functional\nclassification from heterogeneous data. In Proceedings of the fifth annual international con-\nference on Computational biology, pages 249–255, 2001.\n[34] J´erˆome Mariette and Nathalie Villa-Vialaneix. Unsupervised multiple kernel learning for\nheterogeneous data integration. Bioinformatics, 34(6):1009–1015, 2018.\n[35] S¨oren Sonnenburg, Gunnar R¨atsch, Christin Sch¨afer, and Bernhard Sch¨olkopf. Large scale\nmultiple kernel learning. The Journal of Machine Learning Research, 7:1531–1565, 2006.\n[36] Alain Rakotomamonjy, Francis Bach, St´ephane Canu, and Yves Grandvalet. More efficiency\nin multiple kernel learning. In Proceedings of the 24th international conference on Machine\nlearning, pages 775–782, 2007.\n[37] Danian Zheng, Jiaxin Wang, and Yannan Zhao. Non-flat function estimation with a multi-\nscale support vector regression. Neurocomputing, 70(1-3):420–429, 2006.\n[38] Zhao Kang, Chong Peng, and Qiang Cheng. Kernel-driven similarity learning. Neurocom-\nputing, 267:210–219, 2017.\n[39] Jundong Li, Kewei Cheng, Suhang Wang, Fred Morstatter, Robert P Trevino, Jiliang Tang,\nand Huan Liu. Feature selection: A data perspective. ACM computing surveys (CSUR),\n50(6):1–45, 2017.\n[40] Xiaofei He, Deng Cai, and Partha Niyogi. Laplacian score for feature selection. Advances\nin neural information processing systems, 18, 2005.\n[41] Uri Shaham, Ofir Lindenbaum, Jonathan Svirsky, and Yuval Kluger. Deep unsupervised\nfeature selection by discarding nuisance and correlated features. Neural Networks, 152:34–\n43, 2022.\n[42] G´abor J Sz´ekely, Maria L Rizzo, and Nail K Bakirov. Measuring and testing dependence\nby correlation of distances. 2007.\n27\n",
  "categories": [
    "cs.LG",
    "cs.NA",
    "math.NA",
    "65F10, 65F22, 90C26"
  ],
  "published": "2024-03-13",
  "updated": "2024-03-13"
}