{
  "id": "http://arxiv.org/abs/2403.15470v1",
  "title": "Vi-Mistral-X: Building a Vietnamese Language Model with Advanced Continual Pre-training",
  "authors": [
    "James Vo"
  ],
  "abstract": "The advancement of Large Language Models (LLMs) has significantly transformed\nthe field of natural language processing, although the focus on English-centric\nmodels has created a noticeable research gap for specific languages, including\nVietnamese. To address this issue, this paper presents vi-mistral-x, an\ninnovative Large Language Model designed expressly for the Vietnamese language.\nIt utilizes a unique method of continual pre-training, based on the Mistral\narchitecture, which incorporates grouped-query attention and sliding window\nattention techniques. This model, vi-Mistral-X, marks a significant step\nforward in improving the understanding and generation of the Vietnamese\nlanguage. It introduces an additional phase of continual pre-training,\nspecifically adapted for Vietnamese, enhancing the model's capability in\nunderstanding complex language nuances and generating accurate, context-aware\nVietnamese text. Through comprehensive testing on various benchmarks,\nvi-mistral-x has shown to outperform existing Vietnamese LLMs in several key\nareas, including text classification, question answering, and text generation.\nParticularly, in the Vietnamese Multitask Language Understanding (VMLU)\nbenchmark, vi-mistral-x sets a new standard, outperforming other available\nmodels significantly. This paper highlights the critical role of continual\npre-training in advancing language-specific LLMs and opens new avenues for the\ndevelopment of multilingual models. We aim for vi-mistral-x to not just be an\nimportant asset for processing the Vietnamese language but also to encourage\nmore advancements in creating large language models for languages that are less\nrepresented.",
  "text": "VI-MISTRAL-X\nBUILDING A VIETNAMESE LANGUAGE MODEL WITH ADVANCED CONTINUAL PRE-TRAINING\nJames Vo∗\nAI Algorithm Research Team\nAgileSoDA Inc.\nSeoul, South Korea\nanhdungitvn@agilesoda.ai\nMarch 26, 2024\nABSTRACT\nThe advancement of Large Language Models (LLMs) has significantly transformed the field of\nnatural language processing, although the focus on English-centric models has created a noticeable\nresearch gap for specific languages, including Vietnamese. To address this issue, this paper presents\nvi-mistral-x, an innovative Large Language Model designed expressly for the Vietnamese language.\nIt utilizes a unique method of continual pre-training, based on the Mistral architecture, which\nincorporates grouped-query attention and sliding window attention techniques. This model, vi-\nMistral-X, marks a significant step forward in improving the understanding and generation of\nthe Vietnamese language. It introduces an additional phase of continual pre-training, specifically\nadapted for Vietnamese, enhancing the model’s capability in understanding complex language\nnuances and generating accurate, context-aware Vietnamese text. Through comprehensive testing\non various benchmarks, vi-mistral-x has shown to outperform existing Vietnamese LLMs in several\nkey areas, including text classification, question answering, and text generation. Particularly, in the\nVietnamese Multitask Language Understanding (VMLU) benchmark, vi-mistral-x sets a new standard,\noutperforming other available models significantly. This paper highlights the critical role of continual\npre-training in advancing language-specific LLMs and opens new avenues for the development of\nmultilingual models. We aim for vi-mistral-x to not just be an important asset for processing the\nVietnamese language but also to encourage more advancements in creating large language models for\nlanguages that are less represented.\nKeywords Vietnamese · LLM · Pretraining\n1\nIntroduction\nThe field of natural language processing (NLP) has witnessed a paradigm shift with the advent of Large Language\nModels (LLMs), which have shown tremendous potential in understanding and generating human language. LLMs like\nChatGPT and GPT-4 have paved the way for innovations that edge closer to achieving Artificial General Intelligence\n(AGI). However, the progress in this domain has predominantly centered around English, leading to a substantial\ndisparity in the development and performance of LLMs for other languages. This disparity not only limits the global\napplicability of such models but also underscores a crucial gap in the research and development of language models\nthat cater to the diverse linguistic landscape of our world.\nIn particular, the Vietnamese language, with its unique syntactic and semantic complexities, has not been adequately\nrepresented in the current wave of LLM advancements. This oversight hinders the ability of Vietnamese NLP\napplications to achieve the same level of sophistication and effectiveness as their English counterparts, thereby creating\na significant bottleneck in the development of Vietnamese language technology.\nTo bridge this gap, our paper introduces vi-mistral-x, an LLM specifically designed to address the challenges associated\nwith processing and generating the Vietnamese language. Building on the foundation laid by the innovative Mistral\n∗http://agilesoda.ai\narXiv:2403.15470v1  [cs.CL]  20 Mar 2024\nvi-mistral-x\nTECHNICAL REPORT\narchitecture, vi-mistral-x incorporates advanced techniques such as grouped-query attention (GQA) and sliding window\nattention (SWA) Jiang et al. [2023]. These features are part of a unique approach to continual pre-training that is tailored\nto the Vietnamese language, enabling the model to capture its linguistic nuances more accurately.\nThe development of vi-mistral-x is inspired by recent efforts in the field to extend the capabilities of existing models\nto additional languages. This includes the adaptation of LLaMA for the Chinese language Cui et al. and the Korean\nlanguage L. Junbum [2023]. By employing a similar methodology of extending vocabulary and incorporating language-\nspecific pre-training and fine-tuning phases, we aim to achieve a leap in the quality of text understanding and generation\nin Vietnamese. This approach is underscored by the success of the Mistral 7B model, which has demonstrated the\neffectiveness of GQA and SWA in improving performance and efficiency across various NLP tasks.\nOur work on vi-mistral-x represents a critical step toward closing the research gap for the Vietnamese language within\nthe NLP community. By detailing our approach and sharing our findings, we hope to not only enhance the capabilities\nof language models for Vietnamese but also to encourage further research and development efforts focused on other\nunderrepresented languages. This endeavor aligns with our broader goal of promoting inclusivity and diversity in the\nadvancement of natural language processing technologies, ensuring that the benefits of these innovations are accessible\nto a wider audience around the globe.\n2\nProposed Method\nThis section details the methodology employed in developing vi-Mistral-X, focusing on the adaptation of the Mistral\narchitecture for the Vietnamese language. The process encompasses five main stages: corpus preparation, tokenizer\ntraining, model initialization, model training, and model alignment.\n2.1\nEffective Corpus Preparation\nThe stage involves refining a Vietnamese text corpus extracted from CulturaX, a comprehensive multilingual dataset\ndesigned to support the development of Large Language Models (LLMs) across 167 languages, including Viet-\nnamese Nguyen et al. [2023a]. The primary goal was to reduce the original corpus to a more manageable size while\nenhancing its quality, which is critical for the effective training of language models. We employed a multi-step\npreprocessing pipeline with the following components:\nRandom Selection\nAs an initial step, we used a random selection technique to significantly reduce the corpus’s size.\nThis method allowed us to manage computational resources better by focusing on a smaller, yet representative, subset\nof the original dataset.\nN-gram-based Filtering for Deduplication\nWe applied an n-gram-based filtering method to ensure the dataset’s\nuniqueness. This technique analyzes the frequency of contiguous sequences of n-gram in the text to identify and\nremove duplicate or nearly identical content. Such deduplication is crucial to reduce the risk of overfitting the model on\nrepetitive data.\nBERT-based Binary Classifier for Toxicity Filtering\nTo further enhance the corpus quality, we used a high-precision\nBERT-based binary classifier to filter out toxic content. Deploying this classifier helped exclude data that could propagate\nundesirable biases or harmful expressions within the trained model.\nPerplexity-based Filtering\nThe final preprocessing step was perplexity-based filtering. Perplexity, a measure of a\nprobability model’s predictive accuracy, was used to assess and filter documents based on their coherence and quality.\nThis criterion is vital for developing language models, as it ensures that only high-quality and coherent documents\ncontribute to the training process.\nThis comprehensive preprocessing pipeline was designed to enhance the quality of the Vietnamese corpus from CulturaX.\nTable 1 presents a detailed comparison between the original CulturaX corpus and the refined corpus used for training\nthe vi-mistral-x model. Although a formal evaluation employing quantitative measures to ascertain the processed\ndata’s specific impact on model training compared to the original data has not yet been conducted, there is reason to\nbelieve that selecting and refining data to improve the consistency and quality of each data sample can lead to enhanced\ncomputational efficiency. Specifically, reducing the size of the data by removing noisy and non-uniform samples can\ndecrease computing costs due to lower resource requirements and may also improve the training quality of the model by\nconcentrating on high-quality data, thereby optimizing the learning process.\n2\nvi-mistral-x\nTECHNICAL REPORT\nCulturaX/vi\nSelected corpus\nNo. of documents\n54,988,654\n7,331,840\nSize in GB (parquet)\n150.91\n20.656\nNo. of tokens\nNA\n8,323,137,536\nTable 1: Detailed comparison of the original CulturaX/vi and the refined corpus for vi-mistral-x model training\n2.2\nEffective Tokenizer Training\nThe second phase in the adaptation of the pretrained Mistral model for Vietnamese language processing involves the\ndevelopment of a tokenizer capable of efficiently handling Vietnamese text. Initially, we utilized Google SentencePiece2\nto train a new SentencePiece model (SPM). Subsequently, we performed rule-based token filtering on the trained SPM,\nwith a focus on Vietnamese character recognition. The enhanced SPM was then integrated with the original Mistral’s\nSPM model. This hybrid tokenizer maintains the ability to process English and other languages previously supported\nby Mistral-7B, while also effectively managing Vietnamese text. This capability is pivotal for facilitating bilingual or\nmultilingual continual training in the future.\nSPM Model Training\nThe new SPM model was developed by employing Google SentencePiece to train a new model\non our refined corpus, which was obtained in the initial stages. The corpus was significantly reduced to a manageable\nsize (20GB) without necessitating extra sampling, limiting maximum sentence length, or filtering character coverage.\nThe vocabulary size was determined by balancing the trade-off between input complexity and model complexity. A\nlarger vocabulary tends to decrease the number of tokens passed to the model, thereby reducing input complexity, but it\nincreases model complexity due to the expansion of the embedding and language model head dimensions. As illustrated\nin Figure 1, a vocabulary size of 8,096 is optimal for our dataset and the Mistral model, based on our observations.\nSPM Model Refining\nThis phase involved the removal of abnormal characters from the trained SPM model to achieve\na high-quality and coherent tokenizer. The refinement rules were established based on manual definitions and prioritized\ntokens with the highest frequency.\nModel Combination\nThe refined SPM was integrated with the original Mistral’s SPM model to create the final\ntokenizer. This integration process involved several rounds of tokenizer training and analysis, ensuring the new tokenizer\nmodel includes a comprehensive and relevant vocabulary for our project.\nThis meticulous approach to tokenizer training and refinement underscores the importance of adapting language\nprocessing tools to efficiently manage specific linguistic characteristics, thereby enhancing bilingual or multilingual\ntraining capabilities.\nmistral.spm\n32000\nvi.spm\n8096\nintersection\n1438\nunion\n38658\nFigure 1: Visualization of the Mistral SPM model and customized Vietnamese SPM\n2https://github.com/google/sentencepiece\n3\nvi-mistral-x\nTECHNICAL REPORT\nVocab Size\nRelative Input Complexity\nRelative Model Embedding Complexity\n1000\n0.841395049\n1.01640625\n2000\n0.630757254\n1.04403125\n3000\n0.584167181\n1.073125\n4000\n0.557951001\n1.1025\n5000\n0.539983642\n1.13171875\n6000\n0.526697283\n1.16078125\n7000\n0.516199365\n1.18978125\n8000\n0.50747301\n1.21909375\n9000\n0.500297369\n1.2479375\n10000\n0.494061494\n1.27684375\n11000\n0.488440053\n1.30575\n12000\n0.483667799\n1.33428125\n13000\n0.479315439\n1.36334375\n14000\n0.475368937\n1.39240625\n15000\n0.471816595\n1.42109375\n16000\n0.468521178\n1.4500625\n17000\n0.465535533\n1.4789375\n18000\n0.462752376\n1.50753125\n19000\n0.460176296\n1.5363125\n20000\n0.45770208\n1.5655625\n30000\n0.439826305\n1.85509375\n40000\n0.422321581\n2.14471875\n80000\n0.403156539\n3.31371875\n120000\n0.395356195\n4.49840625\nTable 2: Input Complexity and Model Embedding Complexity by Vocab Size\n2.3\nEffective Model Initialization\nFor model initialization, we adapted the Mistral architecture to accommodate the newly-generated Vietnamese token\nembeddings produced by the novel tokenizer. This adaptation necessitated the expansion of both the model’s embedding\nlayer and language model head to include the Vietnamese-specific tokens, whilst preserving the integrity of the original\nmodel’s architecture. Figure 2 illustrates the architectural comparison between the original Mistral framework and our\nmodified version tailored to accommodate Vietnamese-specific tokens.\nInitilization\nLet V = {1, . . . , n} be the model’s vocabulary, where n = 32000. Let w1:T be a sequence of words.\nLet pθ(wi|w1:i−1) be the LM parameterized by θ, defined by:\nP(wi|wi−n+1:i−1) =\nexp(hT\ni−1ewi)\nPm\nj=1 exp(hT\ni−1ej)\nwhere hi−1 = ϕθ(w1:i−1) ∈Rd is the representation of the prefix, and ei ∈Rd is the embedding for word i ∈V . The\nei are contained in θ.\nWhen a new word, input_ids ∈[32000, 38658], n + 1 /∈V is added to the vocab of the pretrained LM, the new word\nn + 1 /∈V ’s embedding needs to be initialized as en+1. Let pθ′(wi|w1:i−1) be the new LM, which has parameters\nθ′ = θ ∪{en+1}, defined by:\npθ′(wi|w1:i−1) =\nexp(hT\ni−1ewi)\nPm\nj=1 exp(hT\ni−1ej) + exp(hT\ni−1en+1)\npθ(wi|w1:i−1) = pθ(wi|w1:i−1) ×\n1\n1 + exp(hT\ni−1en+1)/ Pm\nj=1 exp(hT\ni−1ej)\n4\nvi-mistral-x\nTECHNICAL REPORT\nFigure 2: Relative Input Complexity and Relative Model Embedding Complexity by Vocab Size.\nEMBEDDING\nh\nv\nTRANSFORMER\nBLOCKS\nLM HEAD\nh\nv\nEMBEDDING\nh\nv\nTRANSFORMER\nBLOCKS\nLM HEAD\nh\nv\nEXPANDED\nEMBEDDING\nEXPANDED\nLM HEAD\nv'\nv\nMistral\nOur Mistral\nFigure 3: Model Architecture of the Mistral Model and Our Expanded Model\nThe updated probability of a particular word is the original probability of that word scaled by a multiplicative element\nthat is less than one, denoted as\n1\n1+exp(hT\ni−1en+1)/ Pm\nj=1 exp(hT\ni−1ej). This leads the model to incorrectly assign a\nprobability of 1 to newly added words and 0 to the original words.\nTherefore, the new embedding is adjusted as follows:\n5\nvi-mistral-x\nTECHNICAL REPORT\nexp(hT\ni en+1) = exp\n\nhT\ni\n1\nn\nn\nX\nj=1\nej\n\n\nFinally, the model’s embedding and language model (LM) head are resized to V ′ × H and H × V ′, respectively, where\nV ′ = 38659.\n2.4\nEffective Model training\nMemory and Computational Efficiency in Training\nThe core of vi-mistral-x’s development involved continual\npre-training on a Vietnamese corpus. In this stage, our research is driven by the need to leverage the most effective\nresources in computational linguistics and machine learning. We focus on addressing two main challenges in Large\nLanguage Models (LLMs): memory capacity limitations, which lead to Out Of Memory (OOM) errors, and the\nrequirement for significant computational power, causing long training times. Our work seeks to overcome these issues\nby optimizing the model architecture and training processes.\nIn our pursuit, we have concentrated on a curated selection of model architectures, including Llama2, Mistral, and\nGemma. These were chosen based on their potential for high efficiency and compatibility with our objectives.\nAdditionally, our strategy encompasses the integration of advanced parallelism techniques, such as Fully Sharded Data\nParallelism (FSDP), DeepSpeed ZeRO-3 (DSZERO3), Pipeline Parallelism (PP), and Tensor Parallelism (TP). These\nmethods are instrumental in distributing the computational load and memory usage across multiple devices, thereby\nalleviating the aforementioned constraints.\nOur optimizations have significantly increased training speed, making our library about twice as fast as similar open-\nsource options. Specifically, it’s 1.6 times faster than both Dao [2023] and the PyTorch version of Scaled Dot Product\nAttention (SDPA) (2024).\nOur findings highlight the possibility of greatly improving the efficiency of training transformer-based models, advancing\nartificial intelligence research.\nOptimization\nLet W /∈Rm×n be a weight matrix. Let\nGt = −∇W ϕt(Wt) ∈Rm×n\nbe the gradient matrix at step t. The updated weight matrix is computed by:\nWT = W0 + η\nT −1\nX\nt=0\nGt\nwhere η is the learning rate and ϕt is a stateful gradient regularizer, which is memory-intensive. For instance,\n[AdamW](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html) takes 4 × m × n memory for gradient,\nvariance, momentum, and parameters.\nTo enhance memory and computational efficiency during training, LoRA and its derivatives, which perform a low-rank\nprojection in the weight space as WT = W0 + BT AT , were not selected due to their inherent low-rank limitations. Our\ngoal is to achieve a full-rank update that is both memory- and computation-efficient. Therefore, ‘Tokenizer‘, ‘Model‘,\n‘Trainer‘, and ‘Optimizer‘ are imported from our XLLM library Vo [2023], which has the same interface as those in the\n[Transformers](https://github.com/huggingface/transformers.git) library. Techniques such as learning rate warm-up and\nlearning rate scheduling, which adjust the learning rate appropriately across layers, are also applied to optimize the\ntraining process.\nTraining\nThe model underwent training on a computational framework consisting of eight Nvidia H100 80GB SXM5\nGPUs. Due to the intentional interruption of the training process for purposes of advanced evaluation and optimization,\nan exact duration of training was not documented. However, a rough estimate suggests that, under conditions of\nuninterrupted training, the process would span approximately 104 hours. Financially, this duration translates to an\napproximate expenditure of 3902.08, giventheoperationalcostof37.52 per hour per node.\n2.5\nModel alignment\nFollowing the pre-training phase, vi-mistral-x underwent a series of fine-tuning processes aimed at aligning the model\nwith specific NLP tasks. This alignment involved training the model on task-specific Vietnamese datasets, such as text\n6\nvi-mistral-x\nTECHNICAL REPORT\nclassification, question answering, and text generation. Each task-focused fine-tuning phase allowed vi-mistral-x to\nadjust its parameters to optimize performance on that task, thereby ensuring its applicability and effectiveness across\na wide range of NLP applications. This step was crucial for benchmarking vi-mistral-x against existing Vietnamese\nLLMs and demonstrating its superior performance across several key areas.\nThrough these methodological steps, vi-mistral-x represents a significant advancement in the development of LLMs for\nthe Vietnamese language, offering enhanced understanding and generation capabilities that set a new benchmark for\nperformance in Vietnamese NLP tasks.\n3\nExperimental Results\n3.1\nPretrained Model\n3.1.1\nLoss and Accuracy\nReferences\n• anhdungitvn/vi-mistral-x\n• mistralai/Mistral-7B-v0.1 Jiang et al. [2023]\n• Viet-Mistral/Vistral-7B-Chat Nguyen et al. [2023b]\n• vinai/PhoGPT-7B5 Nguyen et al. [2024]\n• bkai-foundation-models/vietnamese-llama2-7b-120GB\n• meta-llama/Llama-2-7b-hf Touvron et al. [2023]\n• meta-llama/Llama-2-13b-hf Touvron et al. [2023]\n• google/gemma-7b Team et al. [2024]\nSetting\n• Task: CLM (next token prediction)\n• Test data: anhdungitvn/wiki_vi_splitted\n• Test data selection: random train_test_split\n• Test data size: 10000 documents\n• Metrics:\n– Tokens: smaller is better\n– Loss: smaller is better\n– Accuracy: larger is better\nModel\nType\nLength\nTokens\nLoss\nAccuracy*****\nanhdungitvn/vi-mistral-x*\nPretrained\n4096\n2068480\n2.1566\n0.5622\nmistralai/Mistral-7B-v0.1\nPretrained\n4096\n4517888\n1.3687\n0.6813\nViet-Mistral/Vistral-7B-Chat\nFinetuned**\n4096\n2224128\n1.7354\n0.6223\nvinai/PhoGPT-7B5\nPretrained\n2048***\n1982464\n16.5563****\n0.0029\nbkai.../vietnamese-llama2-7b-120GB\nPretrained\n4096\n2191360\n2.4808\n0.5207\nmeta-llama/Llama-2-7b\nPretrained\n4096\n4632576\n1.1287\n0.7295\nmeta-llama/Llama-2-13b\nPretrained\n4096\n4632576\n0.9543\n0.7700\ngoogle/gemma-7b\nPretrained\n4096\n2232320\n...\n...\nTable 3: Comparison of Pretrained Models\nExperimental Results\n* The model vi-mistral-x* is currently under development. The shown results were obtained by evaluating a\ncheckpoint at epoch 0.08.\n** The Viet-Mistral/Vistral-7B pretrained model is unpublished, so we evaluated the Viet-Mistral/Vistral-7B\nfinetuned model.\n7\nvi-mistral-x\nTECHNICAL REPORT\n*** The model vinai/PhoGPT-7B5 doesn’t support an input sequence length of 4096. A RuntimeError occurs\nin modeling_mpt.py on line 138: \"The size of tensor a (4096) must match the size of tensor b (2048) at\nnon-singleton dimension 3.\"\n**** The same evaluation method was applied to all models. The results indicate that the loss for this particular\nmodel is unusually high, suggesting that the evaluation method employed may not be appropriate for this\nmodel. Further investigation is required.\n***** Improved accuracy in a Causal Language Model (CLM) for next-token prediction does not guarantee enhanced\nperformance in other tasks or on different datasets. Loss and accuracy metrics merely indicate the model’s\ncurrent training state and can differ substantially among various models. Therefore, they cannot be directly\ncompared based solely on loss and accuracy.\n3.1.2\nVietnamese Multitask Language Understanding (VMLU)\nReferences\n• VMLU\n• anhdungitvn/vmlu_v1.5\nVMLU\nVMLU is a benchmark suite aimed at evaluating foundation models’ capabilities, focusing on the Vietnamese\nlanguage. It includes 10,880 multiple-choice questions across 58 subjects within STEM, Humanities, Social Sciences,\nand more, covering difficulty levels from basic to advanced.\nDataset: anhdungitvn/vmlu_v1.5\nThe dataset anhdungitvn/vmlu_v1.5 was originally created from vmlu_v1.5 by formatting it into the Hugging Face\ndatasets format for easier use.\n{\n    \"id\": \"28-0023\",\n    \"question\": \"Tỷ  giá thay đổi sẽ ảnh hưởng đến\",\n    \"choices\": [\n        \"A. Cán cân thương mại\",\n        \"B. Cán cân thanh toán\",\n        \"C. Sản lượng quốc gia\",\n        \"D. Các lựa chọn đều đúng\"\n    ],\n    \"answer\": \"\",\n    \"prompt\": \"Chỉ đưa ra chữ cái đứng trước câu trả lời đúng (A, B, C, \nD hoặc E) của câu hỏi trắc nghiệm sau: \\nTỷ  giá thay đổi sẽ ảnh \nhưởng đến\\n\\nA. Cán cân thương mại\\nB. Cán cân thanh toán\\nC. \nSản lượng quốc gia\\nD. Các lựa chọn đều đúng\\nĐáp án: \"\n}\nFigure 4: Example of VMLU\nExample\nExperimental Results\nThe model “vi-mistral-x*” is currently under development. The shown results were obtained\nby evaluating a checkpoint at epoch 0.08.\nThe comparison is shown in Table 4, and the detailed evaluation of “vi-mistral-x” is presented in Table 5.\n8\nvi-mistral-x\nTECHNICAL REPORT\n#\nModel\nCreator\nAccess\nEvalDate\nSTEM\nSS\nHum\nOthers\nAvg\n1\nGPT-4\nOpenAI\nAPI\n08/01/2024\n63.84\n71.78\n66.14\n60.37\n65.53\n2\ngemini\nGoogle\nAPI\n30/01/2024\n42.8\n60.31\n55.35\n51.30\n51.03\n3\nChatGPT\nOpenAI\nAPI\n08/01/2024\n43.24\n51.67\n46.96\n46.32\n46.33\n4\nViGPT-1.6B-v1\nVin BigData\nPrivate\n08/01/2024\n35.06\n48.72\n47.20\n42.54\n42.34\n5\ngemma-7b-it\nGoogle\nWeight\n22/02/2024\n39.95\n44.93\n43.39\n40.11\n41.9\n6\nQwen-7B\nAlibaba Cloud\nWeight\n08/01/2024\n30.64\n35.07\n34.15\n32.68\n32.81\n7\nvi-mistral-x*\nJames\nTBD\n15/03/2024\n24.88\n34.08\n35.11\n29.26\n30.32\n8\ngemma-2b-it\nGoogle\nWeight\n22/02/2024\n24.39\n29.59\n31.01\n26.81\n27.72\n9\nsealion7b\nAI Singapore\nWeight\n08/01/2024\n26.28\n28.57\n27.66\n27.34\n26.73\n10\nbloom-1b7\nBigScience\nWeight\n08/01/2024\n25.13\n25.09\n26.34\n25.19\n25.51\nTable 4: Comparision of Pretrained Models on VMLU\n3.2\nFinetuned Model\nThe following session is being updated.\nReferences\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas,\nFlorian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,\nPierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b,\n2023.\nYiming Cui, Ziqing Yang, and Xin Yao. Efficient and effective text encoding for chinese llama and alpaca.\nL. Junbum. llama-2-ko-7b (revision 4a9993e), 2023. URL https://huggingface.co/beomi/llama-2-ko-7b.\nThuat Nguyen, Chien Van Nguyen, Viet Dac Lai, Hieu Man, Nghia Trung Ngo, Franck Dernoncourt, Ryan A. Rossi,\nand Thien Huu Nguyen. Culturax: A cleaned, enormous, and multilingual dataset for large language models in 167\nlanguages, 2023a.\nTri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning, 2023.\nJames Vo. Enhancing memory and computational efficiency in training transformer-based models, 2023.\nChien Van Nguyen, Thuat Nguyen, Quan Nguyen, Huy Nguyen, Björn Plüster, Nam Pham, Huu Nguyen, Patrick\nSchramowski, and Thien Nguyen. Vistral-7b-chat - towards a state-of-the-art large language model for vietnamese.\n2023b.\nDat Quoc Nguyen, Linh The Nguyen, Chi Tran, Dung Ngoc Nguyen, Dinh Phung, and Hung Bui. Phogpt: Generative\npre-training for vietnamese, 2024.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen,\nGuillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj\nGoswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor\nKerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril,\nJenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor\nMolybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva,\nEric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang\nKuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien\nRodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat\nmodels, 2023.\nGemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre,\nMorgane Rivière, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, Léonard Hussenot, Aakanksha Chowdhery, Adam\nRoberts, Aditya Barua, Alex Botev, Alex Castro-Ros, Ambrose Slone, Amélie Héliou, Andrea Tacchetti, Anna\nBulanova, Antonia Paterson, Beth Tsai, Bobak Shahriari, Charline Le Lan, Christopher A. Choquette-Choo, Clément\nCrepy, Daniel Cer, Daphne Ippolito, David Reid, Elena Buchatskaya, Eric Ni, Eric Noland, Geng Yan, George\nTucker, George-Christian Muraru, Grigory Rozhdestvenskiy, Henryk Michalewski, Ian Tenney, Ivan Grishchenko,\nJacob Austin, James Keeling, Jane Labanowski, Jean-Baptiste Lespiau, Jeff Stanway, Jenny Brennan, Jeremy Chen,\nJohan Ferret, Justin Chiu, Justin Mao-Jones, Katherine Lee, Kathy Yu, Katie Millican, Lars Lowe Sjoesund, Lisa Lee,\nLucas Dixon, Machel Reid, Maciej Mikuła, Mateo Wirth, Michael Sharman, Nikolai Chinaev, Nithum Thain, Olivier\n9\nvi-mistral-x\nTECHNICAL REPORT\nBachem, Oscar Chang, Oscar Wahltinez, Paige Bailey, Paul Michel, Petko Yotov, Pier Giuseppe Sessa, Rahma\nChaabouni, Ramona Comanescu, Reena Jana, Rohan Anil, Ross McIlroy, Ruibo Liu, Ryan Mullins, Samuel L Smith,\nSebastian Borgeaud, Sertan Girgin, Sholto Douglas, Shree Pandya, Siamak Shakeri, Soham De, Ted Klimenko,\nTom Hennigan, Vlad Feinberg, Wojciech Stokowiec, Yu hui Chen, Zafarali Ahmed, Zhitao Gong, Tris Warkentin,\nLudovic Peran, Minh Giang, Clément Farabet, Oriol Vinyals, Jeff Dean, Koray Kavukcuoglu, Demis Hassabis,\nZoubin Ghahramani, Douglas Eck, Joelle Barral, Fernando Pereira, Eli Collins, Armand Joulin, Noah Fiedel, Evan\nSenter, Alek Andreev, and Kathleen Kenealy. Gemma: Open models based on gemini research and technology, 2024.\n10\nvi-mistral-x\nTECHNICAL REPORT\nTable 5: Detailed Evaluation of VI-Mistral-X* on VMLU\nCategory_Subcategory\nScore\ntotal\n30.32\nstem_applied_informatics\n39.44\nstem_computer_architecture\n31.11\nstem_computer_network\n34.64\nstem_discrete_mathematics\n23.64\nstem_electrical_engineering\n22.73\nstem_elementary_mathematics\n19.44\nstem_elementary_science\n55.00\nstem_high_school_biology\n15.00\nstem_high_school_chemistry\n22.78\nstem_high_school_mathematics\n16.22\nstem_high_school_physics\n23.33\nstem_introduction_to_chemistry\n14.53\nstem_introduction_to_physics\n23.12\nstem_introduction_to_programming\n29.05\nstem_metrology_engineer\n22.70\nstem_middle_school_biology\n31.18\nstem_middle_school_chemistry\n18.33\nstem_middle_school_mathematics\n17.59\nstem_middle_school_physics\n21.67\nstem_operating_system\n30.56\nstem_statistics_and_probability\n10.34\nstem_total\n24.88\nother_clinical_pharmacology\n26.11\nother_driving_license_certificate\n45.61\nother_environmental_engineering\n11.70\nother_internal_basic_medicine\n34.50\nother_preschool_pedagogy\n34.31\nother_tax_accountant\n20.69\nother_tax_civil_servant\n41.52\nother_total\n29.26\nother_accountant\n21.43\nother_civil_servant\n27.49\nhumanity_economic_law\n29.81\nhumanity_education_law\n33.13\nhumanity_elementary_history\n49.72\nhumanity_high_school_history\n31.11\nhumanity_high_school_literature\n25.56\nhumanity_history_of_world_civilization\n41.11\nhumanity_idealogical_and_moral_cultivation\n49.44\nhumanity_introduction_to_laws\n39.68\nhumanity_introduction_to_vietnam_culture\n28.33\nhumanity_logic\n18.97\nhumanity_middle_school_history\n37.78\nhumanity_middle_school_literature\n37.36\nhumanity_revolutionary_policy_of_the_vietnamese_commununist_part\n36.67\nhumanity_vietnamese_language_and_literature\n17.24\nhumanity_total\n35.11\nhumanity_administrative_law\n37.78\nhumanity_business_law\n39.11\nhumanity_civil_law\n41.11\nhumanity_criminal_law\n38.04\nsocial_science_middle_school_geography\n27.21\nsocial_science_principles_of_marxism_and_leninism\n36.67\nsocial_science_sociology\n39.89\nsocial_science_business_administration\n20.69\nsocial_science_high_school_civil_education\n43.89\nsocial_science_high_school_geography\n33.33\nsocial_science_ho_chi_minh_ideology\n41.34\nsocial_science_macroeconomics\n21.67\nsocial_science_microeconomics\n23.89\nsocial_science_middle_school_civil_education\n52.25\nsocial_science_total\n34.08\n11\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2024-03-20",
  "updated": "2024-03-20"
}