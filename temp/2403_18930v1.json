{
  "id": "http://arxiv.org/abs/2403.18930v1",
  "title": "Optimizing Wireless Networks with Deep Unfolding: Comparative Study on Two Deep Unfolding Mechanisms",
  "authors": [
    "Abuzar B. M. Adam",
    "Mohammed A. M. Elhassan",
    "Elhadj Moustapha Diallo"
  ],
  "abstract": "In this work, we conduct a comparative study on two deep unfolding mechanisms\nto efficiently perform power control in the next generation wireless networks.\nThe power control problem is formulated as energy efficiency over multiple\ninterference links. The problem is nonconvex. We employ fractional programming\ntransformation to design two solutions for the problem. The first solution is a\nnumerical solution while the second solution is a closed-form solution. Based\non the first solution, we design a semi-unfolding deep learning model where we\ncombine the domain knowledge of the wireless communications and the recent\nadvances in the data-driven deep learning. Moreover, on the highlights of the\nclosed-form solution, fully deep unfolded deep learning model is designed in\nwhich we fully leveraged the expressive closed-form power control solution and\ndeep learning advances. In the simulation results, we compare the performance\nof the proposed deep learning models and the iterative solutions in terms of\naccuracy and inference speed to show their suitability for the real-time\napplication in next generation networks.",
  "text": "1\nOptimizing Wireless Networks with Deep\nUnfolding: Comparative Study on Two Deep\nUnfolding Mechanisms\nAbuzar B. M. Adam ∗, Member, IEEE, Mohammed A. M. Elhassan, Elhadj Moustapha Diallo\nAbstract—In this work, we conduct a comparative study on\ntwo deep unfolding mechanisms to efficiently perform power\ncontrol in the next generation wireless networks. The power\ncontrol problem is formulated as energy efficiency over multi-\nple interference links. The problem is nonconvex. We employ\nfractional programming transformation to design two solutions\nfor the problem. The first solution is a numerical solution\nwhile the second solution is a closed-form solution. Based on\nthe first solution, we design a semi-unfolding deep learning\nmodel where we combine the domain knowledge of the wireless\ncommunications and the recent advances in the data-driven deep\nlearning. Moreover, on the highlights of the closed-form solution,\nfully deep unfolded deep learning model is designed in which\nwe fully leveraged the expressive closed-form power control\nsolution and deep learning advances. In the simulation results, we\ncompare the performance of the proposed deep learning models\nand the iterative solutions in terms of accuracy and inference\nspeed to show their suitability for the real-time application in\nnext generation networks\nIndex Terms—Deep unfolding, model-driven model, data-\ndriven model, power allocation, sixth generation (6G), wireless\nnetworks, fractional programming.\nI. Introduction\nA\nlthough the fifth generation (5G) will be deployed in\nmany countries and an intensive research work has been\ncontributed to enhance it, many researchers and developers\nhave pointed their attention into the six generation (6G) [1].\nVisions for 6G have been laid down and an initiative has been\nproposed in the international telecommunication union (ITU)\nto establish the standards for the 2030 and beyond wireless\ntechnologies [2].\nThe deployment of 6G is expected to meet technical features\nincluding high data rate that has the peak rate of 20 Gbps for\ndownlink and 10 Gbps for uplink. Quality of experience in\nwhich the user receives a minimum data rate at any given\nmoment regardless of their location within the network. This\ncan be measured in the cell edge to reflect the quality of the\nnetwork design. Additionally, latency is expected to improve\nin 6G compared to that of 5G taking into consideration the\ncomplication due the large number of entities in the network.\nFor energy efficiency (EE) and spectral efficiency, the power\nconsumption is expected to decrease and EE can improve\n∗Corresponding author: Abuzar B. M. Adam (abuzar@cqupt.edu.cn)\nA. B. M. Adam and E. M. Diallo are with School of Communications and\nInformation Engineering, Chongqing University of Posts and Telecommuni-\ncations, Chongqing, P. R. China, 400065.\nM. A. M. Elhassan is with School of Mathematics and Computer Science,\nZhejiang Normal University (e-mail: Mohammedac29@zjnu.edu.cn).\n100 times compared to 5G while the spectral efficiency is\nexpected to be three times higher [1], [4]. In this context,\nEE in next-generation networks, including 5G and beyond\n(6G and future technologies), is a critical area of focus\ndue to the exponentially increasing demand for data and the\nglobal emphasis on reducing energy consumption and carbon\nfootprint [5]–[8].\nTo meet the above requirements, intelligent communica-\ntions is envisioned as a key enabler for 6G networks [3].\nHence, deep learning (DL) is applied in to improve wire-\nless communication by solving different problems such as\nchannel estimation [9]–[14], optimal resource allocation [15]–\n[17]. Based on the mechanism of employing DL in solving\nwireless communications problems, the proposed methods can\nbe categorized into main three categories: data-driven meth-\nods, model-driven methods, joint model-data-driven models.\nIn data-driven methods, algorithms are designed to find the\npatterns and the connections which are hidden in the data.\nRecently, model-driven or deep unfolding has been intro-\nduced as a key to enhance the performance of communication\nsystem. This mechanism combines both domain knowledge\nwith learning ability of DL to overcome the problem of both\ntraditional methods and DL-based methods (i.e., data-driven\nmethods) [19]. Although, this mechanism has been mainly\ndeveloped signal recovery [20], [21], some studies used it for\nprecoding design [22], [23] and beamforming [24]–[27]. How-\never, very few studies have been dedicated for model-based\nresource allocation. For instance, the study in [28] investigated\nunfolding of WMMSE using graph neural network. The idea\nwas to perform energy-efficient power allocation. They have\nshown the advantages of the deep unfolding mechanism over\nthe data-driven methods in terms of time and computation cost.\nIn this work, we investigate the employment of deep un-\nfolding to design energy-efficient power control in multicell\nnetworks. We also consider modern neural networks architec-\ntures in addition to deep unfolding to design a hybrid model.\nTherefore, this study provides comparative aspects to conceive\nand incorporate the advantages of both mechanisms. The\nmotivations of this work can be summarized in the following\npoints:\n• In the literature, although there are several studies on deep\nunfolding in precoding and signal recovery, but there are\nfew studies dedicated for power control. We introduce this\nwork to contribute in improving the work in this field.\n• Although the study in [23] incorporated DNN to handle\npart of the problem, however there is a room to explore\narXiv:2403.18930v1  [cs.NI]  17 Feb 2024\n2\nthe advantages of both data-driven deep learning and\nmodel-driven to deep unfolding methods. Our goal is\nto incorporate the advantages of other DL models when\ndesigning deep unfolding based models.\n• The emphasis on specific patterns in multivariate system\nis one of domain knowledge nonnegligible trait in com-\nmunications systems. The impact of several parameters\nshould be incorporated during the design of DL models\nwhich can enhance the prediction [18]. In this work,\nwe want to show the advantage of multivariate patterns\nprediction in improving the model accuracy.\nOur contributions can be summarized as follows:\n• We investigate power control in multicell networks. The\nproblem is formulated as EE maximization. The formu-\nlated problem is sum-of-ratios problem (SoRP). Obtaining\nthe global optimal solution in the polynomial time for\nthis problem is difficult. Therefore, we propose multidi-\nmensional fractional programming-based power control\nframeworks. The proposed frameworks convert the prob-\nlem into a sequence of convex problems using auxiliary\nvariables. To perform this conversion, the auxiliary vari-\nables are used to separate the rate function from the total\npower consumption and the signal from the interference.\nBased on the above procedure, we design two solutions\nto handle the problem.\n• Despite that the proposed fractional programming so-\nlutions have the advantage of exploring most of the\npossible solutions in the solution space, However, they are\ntime consuming and require several iterations to converge\nwhich make them unsuitable for real-time application.\nHence, we resort to DL to handle the power control.\n• First, we propose deep unfolding model based on the\nclosed-from solution of power control i.e., we unfold\nthe iterative solution into layers where we determine the\nlearnable parameters from the unfolded solutions.\n• We design another an end-to-end model on the highlights\nof the numerical solution and the deep unfolding while\nincorporating multiple attention sub-neural networks to\ncompensate the loss. The attention blocks are used to\nperform multivariate prediction. Hence, we incorporate\nthe advantages of modern DL architectures and data-\ndriven mechanism.\n• An ablation study is conducted to investigate the optimal\ndesign architectures for both models and the impact of\neach part of the proposed models.\n• The simulation results have shown the efficiency of the\nproposed deep learning models in terms of accuracy and\nthe inference speed.\nThe remainder of this paper is organized as follows: In Section\nII, we discuss the system model and problem formulation. In\nSection III, we introduce the fractional programming based\nenergy-efficient power allocation where we investigate the\nnumerical solution and closed-form solution. Section IV, in-\nvestigates both model-driven designs energy efficient power\ncontrol. In Section V, we present our simulation results and\ndiscussion. In Section VI, we give the conclusions and the\nfuture works.\nII. System Model and Problem Formulation\nWe consider a multi-cell interference network includes M\nbase stations (BSs) serving K users. M and K represent the\nset of BS and users and denoted as M = {1, 2, ...., M} and\nK = {1, 2, ...., K}. We assume that each BS is equipped with\nN antenna while each user is equipped with single antenna.\nThe BS m transmits a signal to its associated users. Then, the\nreceived signal at the user k associated with the m is given as\nym,k =\np\nρm,kPmaxhH\nm,kxm +\nX\nn∈M,n,m\nX\nk′∈K\np\nρn,k′PmaxhH\nn,kxn + nm,k,\n(1)\nwhere ρm,k with 0 < ρm,k < 1 is the power allocation coefficient\nof the user k associated with BS m, and Pmax represents the\nmaximum transmit power of the BS m, respectively. xm is the\ntransmit signal at the BS m where E\nh\n∥xm∥2i\n= 1. hm,k ∈CN×1\nis the channel coefficient between the BS m the user k. The\nsecond term in (1) represents the intercell interference. nm,k is\nthe additive white Gaussian noise (AWGN) at the user k with\nvariance σ2. Hence, the signal-to-interference-plus-noise ratio\n(SINR) of the user k is expressed as,\nγm,k =\n\f\f\fhH\nm,k\n\f\f\f2ρm,kPmax\nP\nj,k\n\f\f\f\fhH\nm,k\n\f\f\f\f\n2\nρm, jPmax +\nP\nn∈M,n,m\nP\nk′∈K\n\f\f\f\fhH\nn,k\n\f\f\f\f\n2\nρn,k′Pmax + σ2\nm,k\n,\n(2)\nIn presence of intercell interference, the order of SINR is\nconsidered for power allocation and successive interference\ncancelation (SIC) in both BS and user[]. without loss of\ngenerality, we can assume that γm,1 > γm,2 > ... > γm,K. Using\nSINR information, power allocation coefficients are computed.\nAfter SIC is carried out, SINR of the user k associated with\nBS m can be expressed as,\nγm,k =\n\f\f\fhH\nm,k\n\f\f\f2ρm,kPmax\nk−1\nP\nj=1\n\f\f\f\fhH\nm,k\n\f\f\f\f\n2\nρm, jPmax +\nP\nn∈M,n,m\nP\nk′∈K\n\f\f\f\fhH\nn,k\n\f\f\f\f\n2\nρn,k′Pmax + σ2\nm,k\n,\n(3)\nAnd the achievable rate of the user k is given as\nRm,k = Blog2\n\u00001 + γm,k\n\u0001\n(4)\nThen, EE of the user k is given as follows\nηm,k =\nRm,k\nρm,kPmax + pk,c\n(5)\nwhere pk,c is the circuit power consumption.\nWe aim at maximize the weighted sum energy efficiency\n(WSEE) of the network. Unlike the global energy efficiency\n(GEE) where some links and users are favored, WSEE gives\nthe opportunity to do the balance by explicitly prioritizing\nsome links via different weight assignment [29]. The opti-\nmization problem can be formulated as,\nmaximize\n∀ρm,k\nM\nX\nm=1\nK\nX\nk=1\nωm,kηm,k\n(6)\nsubject to\nX\nm∈K\nρm,k ≤1,\n(6a)\n3\nwhere ωm,k is the weight. The problem is nonconvex and\nsum of ratio problem (SoRP). Therefore, it is difficult to\nobtain the global solution with limited complexity [29], [30].\nNevertheless, we can recast it as a tractable convex problem as\nin the Section III. Toward the goal of improving the network\nperformance, Section IV includes design of deep learning\nmodel to perform EE maximization in the real-time. This is\nachieved by projecting the traditional solution in Section III\nto a deep learning model.\nIII. Fractional Programming-based Energy-Efficient Power\nAllocation\nIn this section, we consider a low complexity algorithms to\nsuboptimally solve problem (6). The proposed solutions is a\nquadratic transform based fractional programming. For similar\nproblems as (6), some studies such as [29], [31] employed\nsuccessive convex approximation (SCA) to obtain the lower\nbound. But the result of the this approximation on (6) will be\nsum of pseudo-concave functions which is not guaranteed to\nbe pseudo-concave [18], [29]. To obtain a transformed function\nthat fulfills the desired properties, we resort to the fractional\nprogramming method in [30]. Let us consider the following\nproblem\nmaximize\nx\nF\nX\ni=1\nf\n gi (x)\nhi (x)\n!\nsubject to\nx ∈X,\n(7)\nAccording to this method, the transformed function should\nsatisfy the following technical conditions:\n• The transformed objective function ˜f (x, λ) can be decou-\npled as ˜f (x, λ) = f1 (λ) A (gi (x)) + B (hi (x)) f2 (λ) where\nλ is an auxiliary variable.\n• The function ˜f (x, λ) is concave over λ when x is fixed.\n• x∗maximizes f (x) if and only if x∗with some λ∗\nmaximize ˜f (x, λ).\n• For λ∗= arg max\ny\n˜f (x, λ) given some x, then ˜f (x, λ∗) =\nf (x) for this x.\nA. Numerical Solution for Power Allocation\nOn the highlights of the above conditions, we are going to\ntransform the objective function in (6). First applying quadratic\ntransform, we obtain the following equivalent problem\nmaximize\n∀ρm,k,y\nM\nX\nm=1\nK\nX\nk=1\nωm,k\n\u0012\n2y\u0000Rm,k\n\u0001 1\n2 −y2 \u0000ρm,kPmax + pk,c\n\u0001\u0013\n(8)\nsubject to\n(6a)\nym,k ∈R,\n(8a)\nwhere y is the set of variables \by1,1, ..., yM,K\n\t. The optimal\nvalue of y∗\nm,k is given when ρm,k is fixed as\ny∗\nm,k =\n\u0000Rm,k\n\u0001 1\n2\nρm,kPmax + pk,c\n(9)\nProblem (8) is equivalent to (6) and first technical condition\nis met. The term \u0000Rm,k\n\u0001 1\n2 is a non-decreasing and its concavity\ncan be established if Rm,k is linear or concave while ym,k can\nbe seen as a maximizer point. Obviously, when ρm,k is fixed,\nthe objective function is concave over y. However, due to the\ninterference, Rm,k is nonconcave. We employ multidimensional\nquadratic transform to linearize Rm,k. This is done by sep-\narating the signal from the interference. Thus, we have the\ntransformed function in (10) on top of next page.\nz represents the set \bz1,1, ...., zM,K\n\t. Where the optimal z∗\nm,k\nis given when other variables are fixed as follows\nz∗\nm,k =\n\u0012\f\f\fhH\nm,k\n\f\f\f2ρm,kPmax\n\u0013 1\n2\nk−1\nP\nj=1\n\f\f\f\fhH\nm,k\n\f\f\f\f\n2\nρm, jPmax +\nP\nn∈M,n,m\nP\nk′∈K\n\f\f\f\fhH\nn,k\n\f\f\f\f\n2\nρn,k′Pmax + σ2\nm,k\n(11)\nThe objective function is concave over zm,k when the variables\nρm,k and ym,k are fixed. Hence, the equivalent problem can be\nwritten as,\nmaximize\nρ,y,z\nM\nX\nm=1\nK\nX\nk=1\n˜f FP\nm,k\n(12)\nsubject to\n(6a), (8a)\nzm,k ∈R,\n(12a)\nwhere ˜f FP\nm,k is given as follows\n˜f FP\nm,k = ωm,k\n\u0012\n2ym,k\n\u0010 ˜Rm,k (ρ, z)\n\u0011 1\n2 −y2\nm,k\n\u0000ρm,kPmax + pk,c\n\u0001\u0013\n(13)\nThe problem in (12) is convex on ρm,k when zm,k and ym,k\nare fixed and it can be solved using CVX [32]. Algorithm 1\nillustrates the logical procedure to solve (12).\nAlgorithm 1 Proposed Iterative Numerical Solution for Prob-\nlem (??)\nInitialization: ρm,k, ϵ, t = 0.\n1: while | ˜f FP\nm,k (t) −˜f FP\nm,k (t −1) | ≥ϵ do\n2:\nUpdate zm,k according to (11).\n3:\nUpdate ym,k according to (9).\n4:\nUpdate ρm,k by solving (12) with fixed zm,k and ym,k.\n5:\nt = t + 1.\n6: end while\nB. Closed-form Solution for Power Allocation\nObtaining a closed-form solution for the power allocation\nis desirable to design a deep unfolding model. However, the\nsolution of power allocation problem (12) can not be expressed\nin a closed-form. To circumvent this issue, implement the\nfollowing steps.\nFirst, we apply Lagrange dual transform [33] on the data\nrate function, we get (14) on top of next page.\n4\n˜Rm,k (ρ, z) = Blog2\n1 + 2zm,k\n\u0012\f\f\fhH\nm,k\n\f\f\f2ρm,kPmax\n\u0013 1\n2 −z2\nm,k\n\nk−1\nX\nj=1\n\f\f\fhH\nm,k\n\f\f\f2ρm, jPmax +\nX\nn∈M,n,m\nX\nk′∈K\n\f\f\fhH\nn,k\n\f\f\f2ρn,k′Pmax + σ2\nm,k\n\n,\n(10)\nRm,k (ρ, γ) = Blog2\n\u00001 + γm,k\n\u0001 −Bγm,k +\nB\n\f\f\fhH\nm,k\n\f\f\f2ρm,kPmax\n\u00001 + γm,k\n\u0001\nk−1\nP\nj=1\n\f\f\f\fhH\nm,k\n\f\f\f\f\n2\nρm,jPmax +\nP\nn∈M,n,m\nP\nk′∈K\n\f\f\f\fhH\nn,k\n\f\f\f\f\n2\nρn,k′Pmax + σ2\nm,k\n,\n(14)\nρm,k = min\n\n\"\u0012\nzm,k\n\u00001 + γm,k\n\u0001 \f\f\fhH\nm,k\n\f\f\f2Pmax\n\u00132\n−y2\nm,k\n\u0010\nlog2\n\u00001 + γm,k\n\u0001 + γm,k + z2\nm,k\n\u0010\n1 −Im,k −σ2\nm,k\n\u0011\u0011#2\n4y4\nm,kz2\nm,k\n\u00001 + γm,k\n\u0001 \f\f\f\fhH\nm,k\n\f\f\f\f\n2\nPmax\n, 1\n\n,\n(19)\nNext,\napplying\nmultidimensional\nquadratic\ntransform,\nRm,k (ρ, γ) can be written as\nRm,k (ρ, γ, z) =Blog2\n\u00001 + γm,k\n\u0001 −Bγm,k −z2\nm,k\n+ 2zm,k\nr\nB\n\f\f\f\fhH\nm,k\n\f\f\f\f\n2\nρm,kPmax\n\u00001 + γm,k\n\u0001\n−z2\nm,k\n\nk−1\nP\nj=1\n\f\f\fhH\nm,k\n\f\f\f2ρm, jPmax\n+\nP\nn∈M,n,m\nP\nk′∈K\n\f\f\fhH\nn,k\n\f\f\f2ρn,k′Pmax + σ2\nm,k\n\n,\n(15)\nwhere z∗\nm,k is given as follows\nz∗\nm,k =\nr\nB\n\f\f\f\fhH\nm,k\n\f\f\f\f\n2\nρm,kPmax\n\u00001 + γm,k\n\u0001\nk−1\nP\nj=1\n\f\f\f\fhH\nm,k\n\f\f\f\f\n2\nρm, jPmax +\nP\nn∈M,n,m\nP\nk′∈K\n\f\f\f\fhH\nn,k\n\f\f\f\f\n2\nρn,k′Pmax + σ2\nm,k\n,\n(16)\nSubstituting (15) in (8), the final optimization problem is given\nas follows\nmaximize\nρ,γ,z,y\nM\nX\nm=1\nK\nX\nk=1\nf CF\nm,k\n(17)\nsubject to\n(6a),\nym,k, zm,k ∈R,\n(17a)\nwhere f CF\nm,k is given as follows\nf CF\nm,k =\n\u0010\n2ym,kRm,k (ρ, γ, z) −y2\nm,k\n\u0000ρm,kPmax −pk,c\n\u0001\u0011\n(18)\nThe problem (17) is convex in pm,k when other variables are\nfixed. The solution for pm,k can be expressed in a closed-form\nby taking the derivative for the objective function in (17) and\nset it to zero. Hence, the closed-form solution is given as in\n(19) on to of next page, where Im,k is given as\nIm,k =\nk−1\nX\nj=1\n\f\f\fhH\nm,k\n\f\f\f2ρm,jPmax +\nX\nn∈M,n,m\nX\nk′∈K\n\f\f\fhH\nn,k\n\f\f\f2ρn,k′Pmax,\n(20)\nAlgorithm 2 Proposed Iterative Solution for Problem (17)\nInitialization: ρm,k, ϵ, t = 0.\n1: while | f CF\nm,k (t) −f CF\nm,k (t −1) | ≥ϵ do\n2:\nUpdate zm,k according to (16).\n3:\nUpdate γm,k using (3).\n4:\nUpdate ym,k according to (9).\n5:\nUpdate ρm,k by solving (19) with fixed zm,k, γm,k, and\nym,k.\n6:\nt = t + 1.\n7: end while\nTheorem 1: Algorithm 1 and Algorithm 2 converge to\nstationary point of (6).\nProof: Please, refer to Appendix A.\nObtaining the solution for (12) requires solving the problem\nand updating the parameters for all users, this procedure\nrequires O\n\u0010\nK (3M + 1) log\n\u00101/ϵ\n\u0011\u0011\n. Algorithm 2 performs the\nupdate for one additional step. Hence, Algorithm 2 entails\nan asymptotic complexity O\n\u0010\nK (3M + 2) log\n\u00101/ϵ\n\u0011\u0011\nin the worst\ncase.\nIV. Model-based Design for Energy-Efficient Power\nControl\nIn this section, we discuss the structure of the proposed\nmodel-based design techniques for the energy-efficient power\ncontrol. Two models are designed; the first model is a semi-\nunfolding based model in which we unfold Algorithm 1 while\nin the second model we fully unroll Algorithm 2 to design the\nmodel.\nThe main idea of deep unfolding is the unrolling of the\niterations of principled inference algorithm to form a layered\nneural network [19]. Hence, in deep unfolding-based power\ncontrol, we employ the main domain knowledge in wireless\nresource allocation and deep leaning to build more efficient\nand low complexity models. Fig. 1 shows the steps of the\ntypical deep unfolding process.\nThe existence of the closed-form solution for the power\nallocation helps in designing a full unfolding-based model.\nHowever, this is difficult to apply in case of Algorithm 1 where\nthe final form of the problem is solved numerically.\n5\nFormulate optimization problem\nObtain an iterative solution\nUnroll iteration into layers\nSpecify trainable parameters\nDefine a loss function\nTrain, test and validate the \nunfolded model\nFig. 1. A typical deep unfolding procedure.\nA. Semi-Unfolding based Energy-Efficient Power Control\nFig.\n2\nshows\nthe\noverall\nstructure\nof\nthe\nproposed\nfractional programming based multi-attention-based semi-\nunfolding model (MASUM). When projecting the above def-\ninition and concepts on our case, we can conceptualize the\nmodel design as follows\n• Domain knowledge embedded design: Algorithm 1 re-\nquires initialization of power and channel gain as input.\nThe proposed algorithm has the advantage of separating\nthe signal and the interference and converting the original\nproblem into subproblems using the auxiliaries y and z.\nThe channel coefficients vector is considered as the main\ninput to MASUM in addition to the initial power. Since\ny and z are functions of p, sub-networks are designed to\nrepresent their corresponding equations (9) and (11) as in\nthe illustration of MASUM stages in Fig. 3.\n• Leveraging deep learning advances: Due to the difficul-\nties of fully representing the solution of (12) using the\ngeneric deep unfolding mechanism, we resort to the data-\ndriven advances to overcome this problem. As in Fig. 3,\nconvolutional layer is designed to create feature map for\nprediction. To increase the accuracy of the prediction, we\ndesign a permutation subnet [34] to augment the data\nand enrich the input with more possible variations. The\nconvolutional feature map is cloned and fed parallelly\ninto two pipelines. The first pipeline includes flattening\nlayer to reshape the features and a fully connected layer\nto convey power prediction.\nIn the second pipeline, multiple attention blocks take the\nconvolutional features and create feature maps where the\nimpact of each single parameter is coded and the patterns\nare learnt and spatially connected [18].\nGiven the convolutional feature map fc, we apply parallel\nthree 1 × 1 Conv layers on the fc. Thus, we obtain the\noutputs f 1\nc , f 2\nc , and f 3\nc . f 1\nc and f 2\nc are multiplied and\ninserted into a softmax as follows\nfs = softmax\n\u0010\nf 1\nc · f 2\nc\n\u0011\n(21)\nfs is the interaction between the different positions in\nthe data where each element in fs is a weighted sum of\nfeatures of other elements. Then, fs is used to calibrate the\nimpact of each element in the feature map fc as follows\nf cal\nc\n= fs · f 3\nc\n(22)\nf cal\nc\nis reshaped and 1 × 1 Conv is applied where ˆf cal\nc\nis\nobtained. The final output feature map of the attention\nblock is as\nfA = ˆf cal\nc\n+ fc\n(23)\nMultiple attention feature maps are fused using fusion\nnetwork. To perform feature fusion, multiple attention\nfeature maps are concatenated and fed into two 1 × 1\nConv layers to deliver the fusion map f fusion.\nThe fused features are fed into pooling and flattening\nlayer for reshaping. Then, a fully connected layer is used\nto predict the power pAtt. The second pipeline can be\nseen as compensation of the prediction loss in the first\npipeline.\nThe power predictions from the two pipelines are refined\nto convey the final power prediction ˆp. Where ˆp is given\nas\nˆp = σ \u0000pAtt + pmain\n\u0001 · \u0000pAtt + pmain\n\u0001\n(24)\nFig. 4 shows the detailed structure of the attention block,\nfusion block, and refining block.\nB. Full-Unfolding based Energy-Efficient Power Control\nThe closed-form solution in (19) is helpful in fully unfolding\nAlgorithm 2. Fig. 5 illustrates the block diagram of the full-\nunfolding based model(FUM). In Fig. 3, the second block\nrepresents the structural illustration of the single stage of\nFUM.\nThe structure in Fig. 5 can be viewed as DNN with number\nof layers equals to the iteration of Algorithm 2. The dynamics\nof each layer are described as in the second block in Fig. 3.\nThe structure can be viewed as a graph with shared variables\namong the computation nodes. To optimize the parameters,\nwe employ the well-known technique of backpropagation. To\nefficiently optimize the model in Fig. 5, employ the available\ndata and the underlaying distribution of the channel realization\nof in G.\nC. Technical Description and Inference Process\nIn this subsection, we give the technical description of the\nproposed two models\n1) MASUM Case:\n• Forward Propagation: Provided with the channel co-\nefficients the initial power, each block in the main\npipeline in Fig. 2 outputs a power vector. However,\ndue to the presence of the data-driven layers (namely,\nthe convolution, flatten, and fully connected layers), the\nmodel exhibits some performance loss. Hence, an effec-\ntive attention-based loss compensation mechanism in the\nsecond pipeline is adopted. The forward propagation F (·)\nof MASUM can be expressed as\nˆp = F ({G} ; p)\n(25)\n• Backpropagation: In this procedure, the gradients of the\nlearnable parameters are computed following the deriva-\ntive of the loss function L (·) [35].\n• Updating Parameters: Since each block in Fig. 2 receives\npower input from the previous block, the update of the\n6\nL1\nL2\nL3\nL14\nAttention\nAttention\nFusion\nPool\nFlatten\nFully Connected\nRefining\nH\n( )\n0\n\nmain\n\nAtt\n\nˆ\nFig. 2. Structure of the proposed model.\nEq. (11)\nEq. (9)\nnPr\nConvolution\nFlatten\nFully Connected\n(\n)\n1\np L −\n( )\np L\n,\nk c\np\nTo Attention\nEq. (16)\nEq. (3)\nEq. (9)\nEq. (19)\n,\nk c\np\n(\n)\n1\np L −\n(\n)\n1\nL −\n\n( )\nL\n\n( )\nL\n\nStages of MASUM\nStages of FUM\n2\n\na\n2\n\na\nH\nH\nFig. 3. Detailed structure of stages of the proposed two models.\nparameters is achieved based on (9) and (11) in addition\nto the gradient over L (·) with respect to the power.\n• Loss Function Characterization: The loss function can be\nexpressed as\nL ({G, p} ; ˆp)\n∆= ˜f (G, F ({G} ; p) ; ˆp)\n(26)\nwhere ˜f (·) given in (13).\nDuring the inference process, in each layer, the loss function is\noptimized in such a way that the power prediction is improved.\nHowever, performance loss is inevitable due to the data-driven.\nThis loss is compensated by employing the changes throughout\nthe main pipeline via the attention pipeline.\nSoftmax\nInput feature map\nAttention feature map\nσ \nAttention feature maps\n1 1Conv\n\nConcatenate\nAttention Block\nFusion Block\nRefining Block\nˆ\nAtt\n\nmain\n\nFig. 4. Detailed building blocks for attention, fusion, and refining blocks.\nL1\nL2\nL14\n( )\n0\n\nH\n( )\n0\n\n(\n)\n13\nL\n\n(\n)\n13\nL\n\nˆ\nFig. 5. Structure of the proposed FUM.\n2) FUM Case: The learnable parameters in this case are p\nand γ. Each block outputs the p and γ while the final block\nconveys only p. The forward propagation F (·) in this case\ncan be expressed as\n{ˆp, γ} = F (G; {p, γ})\n(27)\nIn the backpropagation, the gradients of the loss function with\nrespect to the learnable parameters are derived using the chain\nrule. The loss function L (·) can be expressed as\nL (G, {p, γ} ; ˆp)\n∆= f (G, F (G; {p, γ}) ; ˆp) ,\n(28)\n7\nTABLE I Simulation Parameters\nParameter\nValue\nNumber of users per BS\n2,4,8\nNumber of BSs\n4,7\nNumber of antennas N\n2\nFast-Fading\nZ ∼CN (0, 1)\nPath loss exponent\n3.5\nNumber of BS antennas\n8\nB\n250 kHz\nCarrier frequency\n2.4 GHz\nCircuit power\n20 dBm\nwhere f (·) is defined in (18). The updates of the learnable\nparameters at the sequence t can be expressed as\nγ(t+1) (L) = γ(t) (L) −δ\n∂L\n\u0010\nG, {p, γ} ; ˆp(t)\u0011\n∂γ\n(29)\np(t+1) (L) = p(t) (L) −δ\n∂L\n\u0010\nG, {p, γ} ; ˆp(t)\u0011\n∂p\n(30)\nwhere δ is the learning rate. From Fig. 3, the above update is\nalso equivalent to (3) and the first term in (19).\nDifferent from MASUM, during the inference, FUM op-\ntimizing the loss function straightforwardly while leveraging\nthe full domain knowledge of the problem and the statistical\ninformation.\nD. Training and Testing Process\nBoth of the models can be trained using the incremental\nlearning procedure. In the increment round τ ∈{0, ..., T}, we\noptimize the cost function for the layer l ∈{0, ..., L14} by\nlearning the learnable parameters of each model with mini-\nbatches. At the end of increment round τ, the layer l = l + 1\nis added to the network and a new round of training starts.\nThe training results of learnable parameters of the layer l are\nconsidered as initial values for the next round. At the final\nround, the entire model goes through a full training round.\nThis training mechanism is proven to be the most effective\ntraining procedure for deep unfolding-based model.\nFor the simulation, we generate 22000 samples where 8000\nsamples are used for training where we run each of Algorithm\n1 and Algorithm 2 multiple times to retain the best results. We\ngenerate the channel coefficients using the complex Gaussian\ndistribution CN (0, 1) and the number of antennas N = 8.\nThe batch size is set to 128 and 64 for MASUM and SUM\nrespectively. The learning rate is set to 0.001 for both models.\nFor MASUM, the kernel size is 3 in the main stage while the\nstride is set to 1. The simulations are performed with Python\n3.6, PyTorch 2.0, CUDA 10.1, cuDNN V8 on a desktop Intel\nCore i7-8700 CPU @ 3.20GHz, 8.00 GB RAM, and NVIDIA\nGeForce RTX 3050 8GB.\nV. SIMULATION RESULTS\nIn this section, we introduce the simulation results and\nperformance evaluation for the proposed models. Each cell\nradius is 500 m and the adopted channel model follow the in\nforce ITU-R P.1411-11. The users are randomly and uniformly\ndistributed in the cells. The subchannel frequency is 250 kHz\nand the carrier frequency is 2.4 GHz. The noise power spectral\ndensity is -174 dBm/Hz and the noise figure is 7dB. The\nsimulation parameters are deemed default as shown in TABLE\nI unless stated otherwise.\nFor comparison, we considered the deep learning model\nin [18] as a black-box model (pure data-driven model). The\nblack-box model is a multi-modal deep convolutional self-\nattention neural network. The black-box model is trained with\nmultivariate data generated from Algorithm 1.\nA. Accuracy and Convergence\nFrom Theorem 1, both Algorithm 1 and Algorithm 2\nconverge to a fixed point of the original problem (6). Fig.\n6 illustrates the convergence of both algorithms. The required\nnumber of iterations until convergence is 14 and 13 respec-\ntively for Algorithm 1 and Algorithm 2. Despite the slow\nconvergence, both algorithms have the advantage of exploring\nthe solution space.\nFrom the complexity discussion of both algorithms, the\nnumber of users has the most impact on the running time\nof both algorithms. For instance, running both algorithms on\nCPU for 4 users per BS, we obtain the final output in 25.44 ms\nand 25.56 ms respectively for Algorithm 1 and Algorithm 2. In\ncase of running both algorithms on GPU, we have the running\ntime 10.04 ms and 10.01 ms respectively for Algorithm 1 and\nAlgorithm 2.\nFig. 6. Convergence of Algorithm 1 and Algorithm 2.\nFig. 7 shows the training and validation accuracy of MA-\nSUM. MASUM has the advantage of partially exploiting the\ndomain knowledge of the problem and the available data.\nHowever, due to the presence of the data-driven parts in the\nmodel, the presence of the attention is advantageous as we\ncan see later in the ablation study (see Section V-E). MASUM\nachieves training accuracy 98.80% and validation accuracy of\n98.78 for the case of full exploitation of model number of\nlayers (e.g., 9 layers).\nDifferent from MASUM, FUM is fully exploiting both\ndomain knowledge of the problem and the given available\ndata. As in Fig. 8, FUM achieves accuracy 99.32% and\n99.30% respectively for training and validation. The proposed\nMASUM and FUM has a very quick inference since it does\nnot require solving the problem as in the case Algorithm 1\n8\nFig. 7. Accuracy of MASUM.\nand Algorithm 2. Hence, both MASUM and FUM exhibit\ngreat potential for real time applications. More details on the\ninference speed are revealed in Section V-E.\nFig. 8. Accuracy of FUM.\nB. Power Budget\nFig. 9 illustrates the energy efficiency performance for\ndifferent values of Pmax. We set the number of BSs to 7\nand the number of users per BS is set to 4. The network\nenergy efficiency is increasing with the increasing of Pmax\nand converges at 5.0342 × 108 bits/Joule when Pmax = −9\ndBW. This means any increase in Pmax will lead only to\nincreasing interference and the consumption and will not\nhelp in improving the rate. The performance of Algorithm\n1, Algorithm 2, and FUM is identical while MASUM has\nslightly lower performance. Black-box model achieves the\nworst performance among all.\nC. Problem Size\nFor smaller problem when we have 7 BSs and each BS is\nserving 2 users, the performance improves dramatically. We\nreduced the size of the model to 6 layers in the main pipeline\nwhile including the attention in the last 5 layers and trained\nit for shorter training sequence. In Fig. 10, we can notice that\nFig. 9. Network energy efficiency for different values of Pmax.\nthe accuracy of MASUM is high to 99.01 % and 98.94%\nrespectively for training and validation.\nFig. 10. Accuracy of MASUM for small problem (2 users per BS).\nIn case of FUM, we have 6 layers and we apply it on small\nproblem similar to that in last case where we have 7 BSs; each\nBS is serving 2 users. From Fig. 11 FUM achieves accuracy\n99.37% and 99.34% respectively for training and validation.\nFig. 11. Accuracy of FUM for 2 users per BS.\nCompared with the procedure in Fig. 9, we test the models\nover large problem as in Fig. 12. We set the number of BS to\n9\nTABLE II ABLATION STUDY ON THE IMPACT OF MODEL’S NUMBER OF STAGES/LAYERS\nModel\nNo. of stages\nPrediction accuracy (%)\nInference speed (ms)\nCPU\nGPU\nMASUM\n5\n98.55\n0.0692\n0.0571\n9\n98.80\n0.0823\n0.0615\n14\n98.80\n0.0936\n0.0652\nFUM\n5\n99.11\n0.0513\n0.0420\n9\n99.32\n0.0534\n0.0464\n14\n99.32\n0.0545\n0.0474\n7 while the number of users per BS is set to 8. Considering\nthe case of MASUM and FUM, we can observe that there is\na degradation in the performance of both models. However,\nunlike the case of MASUM, this degradation is smaller in\ncase of FUM. In case of MASUM, this degradation might be\nalleviated by increasing the number of neurons, the layers,\nand the attention blocks. Similarly, the increase of the number\nof neurons can decrease the impact of this degradation. It\nis worth mentioning that the adopted training procedure in\nthis study has great impact on solidifying the performance of\nboth models. MASUM achieves 2.4% performance less than\nAlgorithm 1 while FUM achieves about 1% performance less\nthan Algorithm 1. Although Black-box has the capability of\nlearn the statistical information, its performance deteriorates\nwith larger problems as in Fig. 12. Black-box achieves about\n6.8% performance less Algorithm 1.\nFig. 12. Network energy efficiency for different values of Pmax 7 BSs and 8 users per\nBS.\nD. Inference over Off-training Input\nTaking into consideration the changes in the channel condi-\ntions in the real-world, we test the schemes with off-training\ndata and the result is shown as in Fig. 13. We set the number\nof BSs to 7 and the number of users per BS is set to 4.\nAlthough there is a deterioration in the performance of FUM\nand MASUM compared with Algorithm 1 and Algorithm\n2, the proposed two deep learning models show robustness\nagainst the changes in the channel conditions. FUM employs\nthe full domain knowledge of the problem and learns the\nstatistical information from the data. Therefore, FUM performs\nbetter than MASUM. FUM achieves 99.20% performance of\nAlgorithm 1 while MASUM achieves 96.57% performance\nof Algorithm 1. Finally, Black-box performance significantly\ndegrades and only 89.55% of Algorithm 1 can be achieved.\nFig. 13. Performance of different models over off-training data.\nE. Ablation Study\nAccording to the deep unfolding concept, the number of the\nlayers of the resulting model is mostly expected to be equal\nto the number of the iterations of the traditional algorithm.\nHowever, the number of the required layers to achieve the\nmaximum possible accuracy usually does not depend on the\nnumber of iterations. Hence, we implement an ablation study\nto investigate the optimal structure to build each of the\nproposed MASUM and FUM models. TABLE II investigates\nthe optimal number of layers to obtain the maximum possible\naccuracy while maintaining the inference speed at reasonable\nlevel. In case of FUM, we can observe that five to six layers\nare enough to obtain excellent performance. For instance, for\nfive layers, the accuracy is 99.11% and the model delivers the\noutput in 0.0513 ms and 0.0420 ms respectively in CPU and\nGPU. However, the larger number of layers, take 9 layers, we\nobtain accuracy of 99.32% with inference speed 0.0534 ms\nand 0.0464 ms for CPU and GPU, respectively. Obviously,\nincreasing the number of layers beyond 6 layers will only lead\nto increase the inference speed with any improvement in the\ninference speed. It is also worth mentioning that the adopted\ntraining mechanism has great impact in tuning the model\nand optimizing the number of layers. Similar observations\ncan be drawn in the case MASUM. However, since MASUM\n10\nTABLE III IMPACT OF THE NUMBER OF ATTENTION BLOCKS\nNo. of attention blocks\nAccuracy (%)\nInference speed (ms)\nLocation of attention blocks\n0\n97.92\n0.0052\n-\n2\n98.68\n0.0588\n4, 5\n3\n98.55\n0.0602\n3,4,5\n5\n98.80\n0.0615\n5,6,7,8,9\n7\n98.80\n0.0711\n8,9,10,11,12,13,14\nis of a hybrid nature in which model-driven and data-driven\nmechanisms are fused, other factors such as structure of the\ndata-driven blocks and the position of these blocks have crucial\nimpact on the performance.\nTABLE III illustrates the impact of the attention block on\nthe performance of MASUM. First, it is obvious that the num-\nber of the usable attention block is dependent on the number of\nthe overall layers. Second, the locations of the attention blocks\nhave impact on the accuracy because it employ the changes in\nthe features and create spatial connection and better context for\nprediction. Therefore, in the case of have five-layer MASUM,\nwe deploy the attention blocks in the last two and three layers\n(depending on the number of the deployed attention blocks).\nFrom TABLE III, we can observe that the reasonable number\nof attention blocks is 5 if we considered the 7-layer structure.\nAlso it is obvious that the inference speed decreases with the\nincrease of number of attention blocks.\nVI. Conclusions and Future Works\nIn this work, we designed and compared two deep\nunfolding-based models for power control in multiple inter-\nference links. The problem was formulated as SoRP which is\nnonconvex. We considered two solution scenarios for the solu-\ntion. First, we employed multidimensional fractional program-\nming to obtain a numerical solution. Second, we combined\nmultidimensional fractional programming and Lagrange dual\ntransform to obtain a closed-form solution for the problem.\nOn the highlights of the two solutions, we designed two deep\nunfolding-based models. The first model (namely MASUM)\nis based on the numerical solution where two pipelines were\ndesigned to predict the power. The first pipeline emulates the\niterations of the iterative algorithm with the help of some\ndata-driven layers. Due to the loss in the main pipeline, a\nsecond pipeline consists of attention subnets is incorporated\nto compensate the loss. Due to the presence the closed-form\nsolution, the second model-driven neural network (namely\nFUM) can fully emulate the iterative solution with neither\nconvolution nor attention subnet. Both MASUM and FUM\nachieved high accuracy in terms of power allocation with\nspectacular inference speed compared to the iterative solutions.\nWe conclude that the presence of closed-form solution can\nfacilitate unfolding the iterative solution into model-driven\nneural network. Furthermore, in case of difficulty of fully\nunfolding the iterative solution, we resort to semi-unfolding\nwhere we can partially unfold the iterative solution then\nemploy the recent advances in data-driven models to enhance\nthe performance. In future, we consider designing improved\nmodel-driven neural networks for large size problems using\narchitectures such as inception-residual. To make the future\nmodel more suitable for dynamic environment and sparsity of\nwireless communication networks, we aim to incorporate the\nidea of liquid neural networks when building the model.\nAppendix A\nProof of Theorem 1\nLet us consider the case of Algorithm 1. Let f (p) =\nMP\nm=1\nKP\nk=1\nωm,kηm,k. We know from the definition of ηm,k it is non-\ndecreasing SoR. Hence, the problem (6) can be conveniently\nconsidered equivalent to the following:\nmaximize\nρ,y\nf (p, y)\nsubject to\n(6a), (8a),\nwhere f (p, y) =\nMP\nm=1\nKP\nk=1\nωm,k\n\u0010\n2ym,k\npRm,k −y2\nm,k\n\u0000pm,k + pk,c\n\u0001\u0011\n.\nFurthermore, we can rewrite the (6) as a function of η as\nbelow:\nmaximize\nρ,y\nf (η)\nsubject to\n(6a),\nηm,k =\nRm,k\nρm,kPmax + pk,c\n,\nHence,\naccording\nto\nthe\nquadratic\ntransform\ntheory\nin\n[30],\nwe\ncan\nreplace\nthe\nvariable\nη\nwith\nωm,k\n\u0010\n2ym,k\npRm,k −y2\nm,k\n\u0000ρm,kPmax + pk,c\n\u0001\u0011\nand\nsince\nf (η)\nis\nnondecreasing,\nmax\np\nMP\nm=1\nKP\nk=1\nmax\ny\nh\nωm,k\n\u0010\n2ym,k\npRm,k −y2\nm,k\n\u0000ρm,kPmax + pk,c\n\u0001\u0011i\ncan be rewritten as (31).\nNext, we recast the data rate term as concave as in (10).\nConsequently, we obtain problem (12). At each iteration, and\nwhen zm,k and ym,k are fixed, the solution represents a stationary\npoint in which Algorithm 1 converges. Similar reasoning can\nbe applied in case of Algorithm 2.\nReferences\n[1] W. Jiang, B. Han, M. A. Habibi, and H. D. Schotten, “The Road Towards\n6G: A Comprehensive Survey,” IEEE open j. Commun. Soc., vol. 2, pp.\n334-366, 2021.\n[2] R. Li, “Network 2030 A Blueprint of Technology, Applications and\nMarket Drivers Towards the Year 2030 and Beyond,” July 2020.\n[3] P. Yang, Y. Xiao, M. Xiao, and S. Li, “6G Wireless Communications:\nVision and Potential Techniques,” IEEE Network, vol. 33, no. 4, pp. 70-\n75, 2019.\n[4] M. A. Ouamri, G. Barb, D. Singh, A. B. M. Adam, M. S. A. Muthanna\nand X. Li, “Nonlinear Energy-Harvesting for D2D Networks Underlaying\nUAV With SWIPT Using MADQN,” IEEE Commun. Lett., vol. 27, no.\n7, pp. 1804-1808, July 2023.\n11\n[5] A. B. M. Adam, X. Wan and Z. Wang, “Energy Efficiency Maximization\nin Downlink Multi-Cell Multi-Carrier NOMA Networks With Hardware\nImpairments,” IEEE Access, vol. 8, pp. 210054-210065, 2020.\n[6] A. B. M. Adam, X. Wan, Z. Wang, “Energy Efficiency Maximization for\nMulti-Cell Multi-Carrier NOMA Networks,” Sensors, 2020, 20(22):6642.\n[7] A. B. M. Adam, X. Wan, Z. Wang, “Clustering and Auction-Based Power\nAllocation Algorithm for Energy Efficiency Maximization in Multi-Cell\nMulti-Carrier NOMA Networks,” App. Sci., 2019; 9(23):5034.\n[8] A. B. M. Adam, X. Wan, Z. Wang, “User scheduling and power\nallocation for downlink multi-cell multi-carrier NOMA systems,” Digital\nCommunications and Networks, vol. 9, no. 1, pp. 252-263, 2023.\n[9] B. Marinberg, A. Cohen, E. Ben-Dror, and H. H. Permuter, “A Study\non MIMO Channel Estimation by 2D and 3D Convolutional Neural\nNetworks,” in Proc. IEEE Int. Conf. Adv. Netw. Telecommun. Sys. (ANTS),\n2020, pp. 1-6.\n[10] H. Ye, G. Y. Li, and B. H. Juang, “Power of Deep Learning for Channel\nEstimation and Signal Detection in OFDM Systems,” IEEE Wireless\nCommun. Lett., vol. 7, no. 1, pp. 114-117, 2018.\n[11] H. Huang, J. Yang, H. Huang, Y. Song, and G. Gui, “Deep Learning\nfor Super-Resolution Channel Estimation and DOA Estimation Based\nMassive MIMO System,” IEEE Trans. Veh. Technol., vol. 67, no. 9, pp.\n8549-8560, 2018.\n[12] Z. Mao and S. Yan, “Deep learning based channel estimation in fog\nradio access networks,” China Communications, vol. 16, no. 11, pp. 16-\n28, 2019.\n[13] Q. Bai, J. Wang, Y. Zhang, and J. Song, “Deep Learning-Based Channel\nEstimation Algorithm Over Time Selective Fading Channels,” IEEE\nTrans. Cogn. Commun. Netw., vol. 6, no. 1, pp. 125-134, 2020.\n[14] L. Wan, K. Liu, and W. Zhang, “Deep Learning-Aided Off-Grid Channel\nEstimation for Millimeter Wave Cellular Systems,” IEEE Trans. Wireless\nCommun., pp. 1-1, 2021.\n[15] A. B. M. Adam and X. Wan, “Deep Learning based Efficient User Asso-\nciation and Subchannel Assignment in Multi-Cell Multi-Carrier NOMA\nNetworks,”\nin Proc. 7th Int. Conf. Inf. Sci. Control Eng. (ICISCE),\nChangsha, China, Dec. 2020, pp. 448-452.\n[16] M. Labana and W. Hamouda, “Unsupervised Deep Learning Approach\nfor Near Optimal Power Allocation in CRAN,” IEEE Trans. Veh. Technol.,\nvol. 70, no. 7, pp. 7059-7070, 2021.\n[17] A. B. M. Adam, Z. Wang, X. Wan, Y. Xu and B. Duo, “Energy-Efficient\nPower Allocation in Downlink Multi-Cell Multi-Carrier NOMA: Special\nDeep Neural Network Framework,” IEEE Tran. Cogn. Commun. Netw.,\nvol. 8, no. 4, pp. 1770-1783, Dec. 2022.\n[18] A. B. M. Adam, L. Lei, S. Chatzinotas, and N. U. R. Junejo, “Deep\nConvolutional Self-Attention Network for Energy-Efficient Power Control\nin NOMA Networks,” IEEE Trans. Veh. Technol., vol. 71, no. 5, pp. 5540-\n5545, May 2022.\n[19] A. Jagannath, J. Jagannath, and T. Melodia, “Redefining Wireless\nCommunication for 6G: Signal Processing Meets Deep Learning With\nDeep Unfolding,” IEEE Trans. Artificial Intell., vol. 2, no. 6, pp. 528-\n536, 2021.\n[20] D. Ito, S. Takabe, and T. Wadayama, “Trainable ISTA for Sparse Signal\nRecovery,”IEEE Trans. Signal Process., vol. 67, no. 12, pp. 3113-3125,\n2019.\n[21] S. Khobahi and M. Soltanalian, “Model-Based Deep Learning for One-\nBit Compressive Sensing,” IEEE Trans. Signal Process., vol. 68, pp. 5292-\n5307, 2020.\n[22] Q. Hu, Y. Cai, Q. Shi, K. Xu, G. Yu, and Z. Ding, “Iterative Algorithm\nInduced Deep-Unfolding Neural Networks: Precoding Design for Mul-\ntiuser MIMO Systems,” IEEE Trans. Wireless Commun., vol. 20, no. 2,\npp. 1394-1410, 2021.\n[23] G. Zhang, X. Fu, Q. Hu, Y. Cai, and G. Yu, “Hybrid Precoding\nDesign Based on Dual-Layer Deep-Unfolding Neural Network,” in Proc.\nIEEE 32nd Annual Int. Symp. Personal, Indoor Mobile Radio Commun.\n(PIMRC), 2021, pp. 678-683.\n[24] C. H. Lin, Y. T. Lee, W. H. Chung, S. C. Lin, and T. S. Lee, “Un-\nsupervised ResNet-Inspired Beamforming Design Using Deep Unfolding\nTechnique,” in Proc. IEEE Global Commun. Conf. (GLOBECOM), 2020,\npp. 1-7.\n[25] S. Takabe and T. Wadayama, “Deep Unfolded Multicast Beamforming,”\nin Proc. IEEE Global Commun. Conf. (GLOBECOM), 2020, pp. 1-6.\n[26] Y. Liu, Q. Hu, Y. Cai, G. Yu, and G. Y. Li, “Deep-Unfolding Beam-\nforming for Intelligent Reflecting Surface assisted Full-Duplex Systems,”\nIEEE Trans. Wireless Commun., pp. 1-1, 2021.\n[27] A. B. M. Adam et al., “Intelligent and Robust UAV-Aided Multiuser RIS\nCommunication Technique With Jittering UAV and Imperfect Hardware\nConstraints,” IEEE Trans. Veh. Technol., vol. 72, no. 8, pp. 10737-10753,\nAug. 2023.\n[28] A. Chowdhury, G. Verma, C. Rao, A. Swami, and S. Segarra, “Unfolding\nWMMSE Using Graph Neural Networks for Efficient Power Allocation,”\nIEEE Trans. Wireless Commun., vol. 20, no. 9, pp. 6004-6017, 2021.\n[29] A. Zappone and E. Jorswieck, Energy Efficiency in Wireless Networks\nvia Fractional Programming Theory, ser. Found. Trends Commun. Inf.\nTheory, Boston, MA, USA: Now, 2015, vol. 11, no. 3-4, pp. 113-381.\n[30] K. Shen and W. Yu, “Fractional Programming for Communication\nSystems-Part I: Power Control and Beamforming,” IEEE Trans. Signal\nProcess., vol. 66, no. 10, pp. 2616-2630, 2018.\n[31] A. Zappone, L. Sanguinetti, G. Bacci, E. Jorswieck, and M. Debbah,\n“Energy-Efficient Power Control: A Look at 5G Wireless Technologies,”\nIEEE Trans. Signal Process., vol. 64, no. 7, pp. 1668-1683, 2016.\n[32] M.\nGrant\nand\nS.\nBoyd,\n“CVX:\nMatlab\nSoftware\nfor\nDisci-\nplined Convex Programming, Version 3.0 Beta,” Available online:\nhttp://cvxr.com/cvx/beta/, 2017.\n[33] K. Shen and W. Yu, “Fractional Programming for Communication\nSystems-Part II: Uplink Scheduling via Matching,” IEEE Trans. Signal\nProcess., vol. 66, no. 10, pp. 2631-2644, 2018.\n[34] K. -L. Besser, B. Matthiesen, A. Zappone and E. A. Jorswieck,\n“Deep Learning Based Resource Allocation: How Much Training Data\nis Needed?,” 2020 In Proc. IEEE 21st Int. Workshop Signal Process.\nAdvances Wireless Commun. (SPAWC), Atlanta, GA, USA, 2020, pp. 1-\n5.\n[35] Y. Liu, Q. Hu, Y. Cai, G. Yu and G. Y. Li, “Deep-Unfolding Beam-\nforming for Intelligent Reflecting Surface assisted Full-Duplex Systems,”\nIEEE Trans. Wireless Commun., vol. 21, no. 7, pp. 4784-4800, July 2022.\n",
  "categories": [
    "cs.NI",
    "cs.LG"
  ],
  "published": "2024-02-17",
  "updated": "2024-02-17"
}