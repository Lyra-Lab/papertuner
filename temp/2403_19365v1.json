{
  "id": "http://arxiv.org/abs/2403.19365v1",
  "title": "EthioMT: Parallel Corpus for Low-resource Ethiopian Languages",
  "authors": [
    "Atnafu Lambebo Tonja",
    "Olga Kolesnikova",
    "Alexander Gelbukh",
    "Jugal Kalita"
  ],
  "abstract": "Recent research in natural language processing (NLP) has achieved impressive\nperformance in tasks such as machine translation (MT), news classification, and\nquestion-answering in high-resource languages. However, the performance of MT\nleaves much to be desired for low-resource languages. This is due to the\nsmaller size of available parallel corpora in these languages, if such corpora\nare available at all. NLP in Ethiopian languages suffers from the same issues\ndue to the unavailability of publicly accessible datasets for NLP tasks,\nincluding MT. To help the research community and foster research for Ethiopian\nlanguages, we introduce EthioMT -- a new parallel corpus for 15 languages. We\nalso create a new benchmark by collecting a dataset for better-researched\nlanguages in Ethiopia. We evaluate the newly collected corpus and the benchmark\ndataset for 23 Ethiopian languages using transformer and fine-tuning\napproaches.",
  "text": "EthioMT: Parallel Corpus for Low-resource Ethiopian Languages\nAtnafu Lambebo Tonja ♠,_,∗, Olga Kolesnikova ♠,\nAlexander Gelbukh ♠, Jugal Kalita ♣,\n♠Instituto Politécnico Nacional, Mexico, _ Lelapa AI,\n♣University of Colorado Colorado Springs, USA\nAbstract\nRecent research in natural language processing (NLP) has achieved impressive performance in tasks such as machine\ntranslation (MT), news classification, and question-answering in high-resource languages. However, the performance of MT\nleaves much to be desired for low-resource languages. This is due to the smaller size of available parallel corpora in\nthese languages, if such corpora are available at all. NLP in Ethiopian languages suffers from the same issues due to\nthe unavailability of publicly accessible datasets for NLP tasks, including MT. To help the research community and foster\nresearch for Ethiopian languages, we introduce EthioMT – a new parallel corpus for 15 languages. We also create a new\nbenchmark by collecting a dataset for better-researched languages in Ethiopia. We evaluate the newly collected corpus\nand the benchmark dataset for 23 Ethiopian languages using transformer and fine-tuning approaches.\nKeywords: Parallel corpus, EthioMT, Machine Translation, low resource language, Ethiopian languages\n1.\nIntroduction\nIn recent years, due to advances in deep learning\napproaches such as the development of transformers\n(Vaswani et al., 2017), machine translation (MT), a\ncore task in natural language processing (NLP), has\nshown dramatic improvements in terms of coverage\nand translation quality (Wang et al., 2021). It is well-\nknown that a critical requirement for advancing MT\nis the availability of parallel corpora. The availability\nof parallel corpora is also necessary to facilitate the\nincorporation of languages in MT applications like\nGoogle Translation, Bing, and DeepL (Van der Meer,\n2019). The majority of the languages in the world\ndo not have access to such translation tools since\nonly a few high-resource languages have received\nsignificant attention (Tonja et al., 2023b).\nMost models and methods developed for high-\nresource languages do not work well in low-resource\nsettings (Costa-jussà et al., 2022; Tonja et al., 2023b;\nKing, 2015). Low-resource languages have also suf-\nfered from language technology designs (Joshi et al.,\n2019; Tonja et al., 2022). Creating powerful novel\nmethods for language applications is challenging\nwhen resources are limited and only a small amount\nof even unlabeled data is available. The problem is\nexacerbated when no parallel dataset exists for spe-\ncific languages (Joshi et al., 2020; Ranathunga et al.,\n2023; Adebara and Abdul-Mageed, 2022).\nEthiopia is a country that stands out for its remark-\nable cultural and linguistic diversity, with over 85 spo-\nken languages (Woldemariam, 2007). Only a few lan-\n∗Work done during an internship at the University of\nColorado Colorado Springs.\nguages of Ethiopia have received attention in the area\nof NLP research and application development. Most\nlanguages have been left behind due to resource limi-\ntation (Costa-jussà et al., 2022; Tonja et al., 2023b). It\nis hard to find publicly available datasets for Ethiopian\nlanguages to pursue NLP research because many\nresearchers do not make their datasets publicly ac-\ncessible (Tonja et al., 2023b). The unavailability of\nbenchmark datasets and results for NLP tasks, in-\ncluding MT, makes research for newcomers and in-\nterested parties very difficult. This is obviously more\ndifficult for languages with limited data in different\ndigital forms.\nThis paper introduces EthioMT: a parallel corpus\nfor low-resource Ethiopian languages paired with En-\nglish, and a benchmark dataset and experimental\nresults for 23 Ethiopian languages. Our contributions\nare the following: (1) We create a new parallel cor-\npus for 15 Ethiopian languages paired with English.\n(2) We introduce the first benchmark dataset and\nresults for relatively better resourced Ethiopian\n(Amharic, Afaan Oromo, Tigrinya and Somali) lan-\nguages. (3) We evaluate MT performance with the\nnew corpus and present benchmark results. (4)\nWe open-source the parallel corpus to foster collab-\noration and facilitate research and development in\nlow-resource Ethiopian languages.\n2.\nRelated work\nEthiopian languages are categorized as low-\nresource due to the unavailability of resources\nfor NLP tasks, including MT (Tonja et al., 2023b).\nAlthough MT is a better-researched area for Ethiopian\nlanguages compared to other NLP applications\n(Tonja et al., 2023b), only a handful of languages\nhave received adequate attention from researchers.\narXiv:2403.19365v1  [cs.CL]  28 Mar 2024\nResearched\nLanguages\nCompared\nto\nother\nEthiopian languages, the following languages have\nreceived\nsignificant\nattention\nfrom\nresearchers.\nNevertheless, the collected corpora are not found in\none location. It is hard to find benchmark datasets in\nthese languages and datasets and associated results\nto reproduce and compare MT approaches.\nAmharic - Researchers have collected parallel\ndatasets and proposed different MT approaches for\nAmharic-English translation (Kenny, 2018; Teshome\nand Besacier, 2012; Hadgu et al., 2020; Ashengo\net al., 2021; Biadgligne and Smaïli, 2022; Belay et al.,\n2022; Gezmu et al., 2021b,a; Biadgligne and Smaïli,\n2021).\nAfaan Oromo - Similarly, there have been attempts to\ncreate Afaan Oromo-English MT datasets (Meshesha\nand Solomon, 2018; Solomon et al., 2017; Adugna\nand Eisele, 2010; Chala et al., 2021; Gemechu and\nKanagachidambaresan, 2021).\nTigrinya - For Tigrinya-English MT, researchers have\nattempted to create parallel datasets (Tedla and\nYamamoto, 2016, 2017; Berihu et al., 2020; Azath\nand Kiros, 2020; Kidane et al., 2021).\nMultilingual MT Some researchers have included\nEthiopian languages with other languages in multilin-\ngual MT systems. Lakew et al. (2020) collected and\ncreated benchmark results for five African languages,\nincluding those mentioned above from Ethiopia.\nCosta-jussà et al. (2022), Goyal et al. (2022) and\nFan et al. (2021) included Ethiopian languages in\ntheir multilingual MT models and benchmark test\nsets. Vegi et al. (2022) crawled a multilingual parallel\ndataset for African languages, including Amharic and\nAfaan Oromo from Ethiopia.\nOther languages There have been efforts to create\nand collect MT datasets for other Ethiopian lan-\nguages. For example, Tonja et al. (2021) presented\na parallel corpus for four low-resourced Ethiopian\nlanguages (Wolaita, Gamo, Gofa, and Dawuro).\n3.\nEthioMT\n3.1.\nDiscussion of Languages\nIn this section, we enumerate languages included\nin the EthioMT corpus. Languages include in the\nEthioMT corpus belong to Afro-Asiatic and Nilo-\nSaharan language families.\n3.1.1.\nAfro-Asiatic language family\nThe Afro-Asiatic language family comprises about\n250 languages spoken in North Africa, parts of sub-\nSaharan Africa, and the Middle East. Languages\nbelonging to this family are grouped into six sub-\ngroups: Berber, Chadic, Cushitic, Egyptian, Omotic,\nand Semitic (Epstein and Kole, 1998). EthioMT con-\ntains languages belonging to the Omotic, Cushitic,\nand Semitic sub-groups.\n1) Omotic Languages are a group of languages\nspoken in southwestern Ethiopia, in the Omo River\nregion. The Ge’ez script is used to write some of\nthe Omotic languages and the Latin script for others\n(Amha, 2017). Languages belonging to this group\nthat we included in EthioMT are given below.\nBasketo is spoken in the Basketo special woreda\nof the South Ethiopia Regional State. The Basketo\nlanguage is also called Basketto, Baskatta, Mesketo,\nMisketto, and Basketo-Dokka. The speakers call the\nlanguage \"Masketo\", while their neighbors call it \"Bas-\nketo\". The language has two dialects, Doko (Dokko)\nand Dollo (Dollo).\nDawuro is a language spoken by about 1.09 mil-\nlion people in the Dawro zone of the South West\nEthiopia Peoples’ Region. It is also known as Dauro,\nDawragna, Dawrogna, Ometay, Cullo, or Kullo. The\nlanguage has four dialects: Konta, Kucha, Longkhai,\nand Yawngkon.\nGamo is spoken by around 1.63 million people in\nthe Gamo Zone of the South Ethiopia Regional State.\nThe speakers call the language Gamotstso.\nGofa refers to the language spoken in the Gofa\nzone of the South Ethiopia Regional State with around\n392,000 speakers.\nKafa, also known as Kefa or Kafi noono is a North\nOmotic language spoken in Ethiopia. It is spoken\nby about 830,000 people in the Keffa Zone in the\nSouth West Ethiopia Peoples’ Region. The language\nis mainly spoken in and around the town of Bonga.\nMale is spoken in the Omo Region of Ethiopia. The\nMale people maintain their language vigorously de-\nspite exposure to outside pressures and languages.\nShakicho, also known as Mocha, Shakacho, or\nShekka, is spoken in the Sheka Zone of southwestern\nEthiopia. It is closely related to Kafa. Loan words\nfrom Majang and Amharic influence the language’s\nvocabulary.\nWolaytta is a North Omotic language spoken by\nthe Welayta people in the Wolayita Zone of Ethiopia.\nIt is estimated that 2 million people speak Wolaytta.\n2) Cushitic languages are spoken primarily in the\nHorn of Africa, including Djibouti, Eritrea, Ethiopia,\nSomalia, and Kenya (Comrie, 2002). The Cushitic lan-\nguages use the Latin and Ge’ez script. Languages be-\nlonging to this family that are included in the EthioMT\ngroup are discussed below.\nAfar is spoken by the Afar people in Ethiopia, Er-\nitrea, and Djibouti. It is also known as Afar Af, Afaraf,\nand Qafar af. About 1.5 million people speak Afar, the\nclosest relative to the Saho language.\nAfaan Oromo, also known as Oromo, is spoken by\nabout 37 million people in Ethiopia, Kenya, Somalia,\nand Egypt. It is the third-largest language in Africa\nand the largest language in the Cushitic group in\nterms of speakers. The Oromo people are the largest\nethnic group in Ethiopia and account for more than\n40 percent of the population.\nLanguage\nFamily\nExplored prev.\nNo. of\nSpeaker\nDomain\nSize\nAfar (aar)\nAfro-Asiatic / Cushitic\n×\n1.5M\nReligious\n11K\nAfaan Oromo (orm)\nAfro-Asiatic / Cushitic\n✓\n37M\nMisc\n2.9M\nAwngi (awn)\nAfro-Asiatic / Cushitic\n×\n490K\nReligious\n7K\nAmharic (amh)\nAfro-Asiatic / Ethio-Semitic\n✓\n57M\nMisc\n1.5M\nBasketo (bst)\nAfro-Asiatic/ Omotic\n×\n93K\nReligious\n7K\nDawuro (dwr)\nAfro-Asiatic/ Omotic\n✓\n1.5M\nReligious\n7K\nDashenech (dsh)\nAfro-Asiatic/ Cushitic\n×\n99K\nReligious\n7K\nGeez (gez)\nAfro-Asiatic / Ethio-Semitic\n×\nUNK\nReligious\n7K\nGamo (gmv)\nAfro-Asiatic / Omotic\n✓\n1.09M\nReligious\n7K\nGofa (gof)\nAfro-Asiatic / Omotic\n✓\n392K\nReligious\n7K\nGurage (sgw)\nAfro-Asiatic / Ethio-Semitic\n×\n5.8M\nReligious\n28K\nHadiya (hdy)\nAfro-Asiatic / Cushitic\n×\n1.3M\nReligious\n28K\nKafa (kbr)\nAfro-Asiatic / Omotic\n×\n830K\nReligious\n28K\nKorate (kxc)\nAfro-Asiatic / Cushitic\n×\n500K\nReligious\n7K\nMajang (mpe)\nNilo-Saharan / Eastern Sudanic\n×\n66K\nReligious\n9K\nMale (mdy)\nAfro-Asiatic / Omotic\n×\n105K\nReligious\n7K\nMurule (mur)\nNilo-Saharan / Eastern Sudanic\n×\n300K\nReligious\n9K\nNuer (nus)\nNilo-Saharan /Eastern Sudanic\n×\n900K\nReligious\n29K\nShakicho (moy)\nAfro-Asiatic / Omotic\n×\n80K\nReligious\n7K\nSidama (sid)\nAfro-Asiatic / Cushitic\n×\n4M\nReligious\n28K\nSomali (som)\nAfro-Asiatic / Cushitic\n✓\n22.3M\nMisc\n1.2M\nTigrinya (tir)\nAfro-Asiatic / Ethio-Semitic\n✓\n9M\nMisc\n140K\nWolaytta (wal)\nAfro-Asiatic / Omotic\n✓\n7M\nReligious\n29K\nTable 1: Languages and dataset details for EthioMT corpus. It shows languages, language families, the\nnumber of speakers, the domain, and the size of the collected dataset. In domain column Misc indicates\nmixed corpus collected from religious, news, and other sources. Bold and underlined size indicates a dataset\ncollected from different repositories and published works and merged into one dataset for the language to\ncreate a benchmark dataset\nAwngi is a Central Cushitic language spoken by\nabout 400,000 people in northwestern Ethiopia. It is\nalso known as Awiya, Awi, Agaw, Agau, Agew, Agow,\nAwawar, and Damot. Most speakers live in the Agew\nAwi Zone of the Amhara Region. Awngi is an Afro-\nAsiatic language spoken in parts of the Metekel Zone\nof the Benishangul-Gumuz Region.\nDashenech\nis\nalso\nknown\nas\nDasenech,\nDaasanech, or Daasanach. The Daasanach people\nspeak it in Ethiopia, South Sudan, and Kenya. The\nDaasanach people primarily live in the Lower Omo\nValley of southwestern Ethiopia, along the eastern\nshore of Lake Turkana in Kenya, and in some parts\nof South Sudan.\nHadiya is spoken by the Hadiya people of Ethiopia.\nThe language is also known as Hadiyyisa, Hadiyigna,\nAdiya, Adea, Adiye, Hadia, Hadiya, and Hadya. It is a\nHighland East Cushitic language. The Hadiya people\nare an ancient indigenous group in the southern part\nof Ethiopia. There are 1.4 million speakers of the\nHadiya language, with 1.25 million of them speaking\nit as their mother tongue.\nKorate is a Lowland East Cushitic language spo-\nken by the Konso people in southwest Ethiopia. It\nhas approximately 500,000 native speakers.\nThe\nlanguage has five dialects: Duuro, Fasha, Karatti,\nKholme, and Komso. The two main dialects are Fasha\nand Karatti. Konso is closely related to Dirasha (also\nknown as Gidole). It is used as a \"trade language\" or\nlingua franca beyond the area of the Konso people.\nThe Konso people are a Cushitic ethnic group who\nlive in large towns in south-central Ethiopia.\nSidama, or Sidaamu Afoo, is a Cushitic language\nspoken by the Sidama people in southern Ethiopia. It\nuses the Latin alphabet. Almost nine million peo-\nple speak Sidama.\nIt is the official language of\nthe Sidama National Regional State (SNRS) and is\nused as a medium of instruction in primary schools.\nSidama is a branch of the Highland East Cushitic\nfamily.\nSomali is the official language of Somalia, spoken\nby 6.5 million people. It is also spoken in Ethiopia,\nDjibouti, and Kenya. The total number of speakers\nworldwide is estimated at nearly 22 million. Its closest\nrelative is the Oromo language, spoken in parts of\nEthiopia and Kenya. Other related languages include\nAfar and Saho.\n3) Semitic languages belong to a subfamily of the\nAfro-Asiatic language family, including Hebrew, Ara-\nmaic, Arabic, and Ethiopic. Most scripts used to write\nSemitic languages are abjad. Abjad refers to an al-\nphabetic script that omits some or all vowels. Lan-\nguages belonging to this group that we study are\ngiven below.\nAmharic is spoken by the Amhara and other re-\ngions in Ethiopia.\nIt is the second most-spoken\nSemitic language in the world, after Arabic. Amharic\nis the official language of Ethiopia and has been since\nthe 14th century. It is also spoken in other countries,\nincluding Eritrea, Canada, the United States, and\nSweden. Amharic is written using graphemes called\nfidal, which means \"script\", \"alphabet\", \"letter\", or\n\"character\".\nGe’ez is an ancient Semitic language that orig-\ninated in Eritrea and northern Ethiopia.\nGe’ez is\nbelieved to be around 5,000 years old, making it\nolder than Hebrew and other Northern Semitic lan-\nguages. Orthodox and Catholic churches in Eritrea\nand Ethiopia still use it as a liturgical language. Ge’ez\nwent extinct as a natural language over 1,000 years\nago. It was written in two systems: an abjad and later\nan abugida.\nGurage is spoken by the Gurage people in central\nEthiopia. The Gurage languages are written using the\nGe’ez script, which is also used for other Ethiopian\nlanguages. The Gurage languages are not always\nmutually intelligible.\nTigrinya is spoken by about 9 million people, pri-\nmarily in Eritrea and Ethiopia. It is written in the Ge’ez\nscript, which is also used for Amharic, but the gram-\nmar and usage of Tigrinya differs significantly from\nAmharic.\n3.1.2.\nNilo-Saharan language family\nNilo-Saharan languages are a group of languages\nthat form one of the four language families on the\nAfrican continent (Dimmendaal et al., 2019). The fam-\nily covers major areas east and north of Lake Victoria\nin East Africa and extends westward to the Niger Val-\nley in Mali, West Africa (Comrie, 2002). Nilo-Saharan\nconstitutes ten distinct and separate language fami-\nlies, including Eastern Sudanic.\nEastern Sudanic languages are a group of ten fam-\nilies of languages that constitute a branch of the\nNilo-Saharan language family. Eastern Sudanic lan-\nguages are spoken from southern Egypt to northern\nTanzania. The languages used in our study by this\ngroup are given below.\nMajang is spoken by the Majangir people of\nEthiopia. It is a member of the Surmic language\ncluster, but it is the most isolated one in the group. It\nis classified as part of the Eastern Sudanic branch\nof the Nilo-Saharan language family. The Majang\npeople live in scattered settlements in southwestern\nEthiopia. They live around the urban areas of Tepi\nand Mett’i, southwest of Mizan Teferi and towards\nGambela.\nMurle is spoken by the Murle people in South Su-\ndan and Ethiopia. The language is also known as\nAjibba, Beir, Merule, Mourle, and Murule. The Murle\nlanguage is part of the Surmic language family and\nhas three dialects: Lotilla, Boma, and Olam. The\nMurle people number between 300,000 and 400,000.\nThey live in Pibor County in the southeastern Upper\nNile (Jonglei)\nNuer or Thok Naath is a West Nilotic language spo-\nken by the Nuer people of South Sudan and western\nEthiopia. The language is written in a Latin-based\nalphabet, similar to Dinka and Atuot. Over 900,000\npeople speak the Nuer language in diaspora commu-\nnities in East Africa, Australia, and the USA.\n4.\nDataset\n4.1.\nDataset Collection\nWe collected datasets for 16 languages from religious\ndomains from a website1.\nIn addition to that, for\nAmharic, Afaan Oromo, Somali, and Tigrinya, we col-\nlected publicly available datasets (Abate et al., 2019;\nLakew et al., 2020; Vegi et al., 2022) from different do-\nmains to create one benchmark dataset per language.\nFor Dawuro, Gamo, Gofa, and Wolaita languages, we\nused Tonja et al. (2021) dataset to create benchmark\nresults for fine-tuned models. A web crawler was\nused for each article to extract the Bible data from\nwebsites after identifying the structure of web doc-\numents. Python libraries such as requests, regular\nexpression (RE), and Beautiful Soup (BS) were uti-\nlized to analyze website structure and extract article\ncontent from a given URL.\n4.2.\nSentence Alignment\nAfter collecting the corpus for the languages, we\naligned each sentence of the Ethiopian languages\nto a sentence in English data to prepare the dataset\nfor the MT experiment. We followed the same pro-\ncedure as Tonja et al. (2023a) to perform sentence\nalignment.\n4.3.\nDataset Pre-processing\nAfter aligning the texts of the Ethiopian languages\nwith their equivalent translations in English, we pre-\nprocessed the corpus before splitting it for our experi-\nments. The pre-processing steps included removing\nthe numeric and special character symbols, etc. We\nalso removed parallel sentences that contain less\nthan five words. For the baseline experiments, we\nsplit the pre-processed corpus into training, devel-\nopment, and test sets in the ratio of 70:10:20, re-\nspectively. Table 1 shows detailed information on\nselected languages, language families, domain, and\ntheir dataset size.\n1https://www.bible.com/\n5.\nBaseline Models\nWe used the following two approaches to evaluate\nthe newly collected corpus’s usability and our new\nbenchmark dataset of four (amh, orm, som, and tir)\nEthiopian languages.\nThe baseline transformer is a type of neural net-\nwork architecture first introduced in the paper Atten-\ntion Is All You Need (Vaswani et al., 2017).\nThe\nkey innovation of the Transformer architecture is the\nattention mechanism, which allows the network to se-\nlectively focus on different parts of the input sequence\nwhen making predictions. This contrasts traditional re-\ncurrent neural networks (RNNs), which process input\nsequentially and are prone to the vanishing gradient\nproblem.\nIn the transformer architecture,\nmultiple self-\nattention layers and feed-forward neural networks\nprocess elements of the input sequence in parallel.\nEach layer can be considered a \"block\" that takes the\nprevious layer’s output as input and applies its trans-\nformations to it. The self-attention mechanism allows\nthe network to weigh the importance of each element\nin the input sequence when making predictions. In\ncontrast, the feed-forward networks help to capture\nnon-linear relationships among the components.\nTransformers\nare\nstate-of-the-art\napproaches\nwidely used in NLP tasks such as MT, text summa-\nrization, and sentiment analysis. Table 3 shows pa-\nrameters set up for the transformer model.\nParameters\nValues\nencoder_layer\n6\nencoder_attention_head\n4\ndecoder_layer\n6\nbatch_size\n512\nbatch_type\ntoken\ndecoder_attention_head\n8\nhidden_size\n256\nembed_dim\n256\ndropout\n0.2\nbeam_size\n5\noptimizer\nadam\ntokenizer_type\nsentencepiece\nmax_input_length\n150\nTable 2: Parameters used for transformer training\nFine tuning is the process of using a pre-trained\nMT model and adapting it to a specific translation task,\nsuch as translating between a particular language\npair or in a specific domain. The process of fine-\ntuning involves taking the pre-trained model, which\nhas already learned representations of words and\nphrases from a large corpus of text, and training it\non a smaller dataset of specific task examples. This\ninvolves updating the pre-trained model’s parameters\nto better capture the patterns and structures in the\ntarget translation task.\nFine-tuning can be helpful in MT because it allows\nthe pre-trained model to quickly adapt to a new task\nwithout having to train a new model from scratch. This\nis especially beneficial when working with limited data\nor when there is a need to quickly adapt to chang-\ning translation requirements. We used M2M100-48\na multilingual encoder-decoder (seq-to-seq) model\ntrained for many-to-many multilingual translation (Fan\net al., 2021). We used a model with 48M parameters\ndue to computing resource limitations. We used the\nfollowing parameters to fine-tune the m2m100 model.\nParameters\nValues\nencoder_layer\n12\nencoder_attention_head\n16\ndecoder_layer\n12\nbatch_size\n512\nbatch_type\ntoken\ndecoder_attention_head\n16\nhidden_size\n4096\nembed_dim\n1024\nattention_dropout\n0.1\nbeam_size\n5\nTable 3: Parameters used for m2m100-48 fine-tuning\n6.\nResults and Discussions\nWe evaluated the above approaches in bidirectional\ntranslation from Ethiopian languages to English and\nFrom English to Ethiopian languages. We used Sacre-\nbleu (Post, 2018) evaluation metrics to evaluate trans-\nlation models. Tables 4 and 5 show the translation\nresults in both directions.\n6.1.\nUsing English as a source language\nTable 4 shows the translation results from English to\nEthiopian languages. When comparing the results of\nthe two approaches, we observe poor performance\nwhen using a transformer rather than fine-tuning the\nm2m100 model. As we can see from the result, the\nperformance of the transformer model also varies in\nthe ranges of 0.01 – 17.8 spBLEU from language\nto language with different corpus sizes. This shows\nthat a bilingual translation model trained from scratch\nperforms poorly for low-resource language training\ncompared to other approaches like fine-tuning multilin-\ngual translation models. Fine-tuning the multilingual\nmodel shows better results than the model built from\nscratch for English to Ethiopian language translation.\nIn the fine-tuning approach, we can also observe a\nclear score difference between languages with larger\ncorpora (amh, orm, tir, som) and others (e.g awn, aar,\nbst, etc.). This shows that fine-tuning the multilingual\nmodel will work well for languages with the largest\n(e.g. orm, amh) corpus sizes than languages with\nFigure 1: Benchmark translation results for transformer and fine-tuned approaches in both (from and to\nEnglish/Ethiopian languages) direction\nModel\nen-xx\naar\nawn\namh\nbst\ndwr\ndsh\ngez\ngmv\ngof\nsgw\nhdy\nkbr\nkxc\nmdy\nmpe\nmur\nnus\nmoy\norm\nsid\nsom\ntir\nwal\nAvg.\nBleu Score\nTransformer\n1.28\n0.41\n16.79\n0.6\n2.57\n2.51\n0.01\n0.34\n1.82\n0.41\n1.69\n0.87\n3.36\n0.90\n3.65\n3.58\n7.73\n0.87\n17.8\n1.19\n13.06\n11.07\n3.84\n4.18\nm2m100-fine-tuned\n3.95\n3.93\n29.63\n3.61\n10.23\n9.45\n3.25\n2.03\n7.65\n3.04\n6.80\n6.58\n7.69\n4.15\n9.03\n9.10\n18.79\n6.58\n33.7\n6.10\n28.9\n46.63\n11.32\n11.83\nTable 4: Benchmark translation results from English to Ethiopian languages\nModel\nxx-en\naar\nawn\namh\nbst\ndwr\ndsh\ngez\ngmv\ngof\nsgw\nhdy\nkbr\nkxc\nmdy\nmpe\nmur\nnus\nmoy\norm\nsid\nsom\ntir\nwal\nAvg.\nBleu Score\nTransformer\n3.18\n3.14\n21.9\n3.39\n0.52\n3.07\n0.28\n2.68\n3.21\n3.18\n4.42\n3.26\n3.14\n3.21\n3.91\n3.92\n9.23\n2.63\n23.6\n4.77\n18.9\n17.2\n9.16\n6.60\nm2m100-fine-tuned\n15.61\n16.32\n65.34\n15.47\n18.92\n14.11\n16.57\n16.79\n18.79\n13.52\n19.04\n23.27\n15.90\n15.20\n13.26\n1.48\n24.40\n17.78\n63.9\n17.86\n25.71\n61.50\n24.62\n21.79\nTable 5: Benchmark translation results from Ethiopian languages to English\nsmall (e.g. awn, bst, etc.) corpus sizes. We can also\nsee from the results that both approaches work well\nfor languages with mixed-domain texts compared to\none domain (religion).\n6.2.\nUsing English as a target language\nTable 5 shows the translation result when using En-\nglish as a target language.\nSimilarly, as we can\nsee from the results, the transformer model performs\npoorly compared to the fine-tuned model when trans-\nlating from Ethiopian languages to English. Com-\npared to Table 4, translating to English shows im-\nprovements in the transformer model for similar lan-\nguages. We observe that the fine-tuned model shows\nbetter Bleu scores when translating to English than\nwhen translating to Ethiopian languages. The results\nshow that languages with large datasets have the\nhighest performance. This shows that both models\nshow improvements when translating from Ethiopian\nto English, while when translating from English to\nEthiopian languages, the model is struggling with\ntranslation.\n7.\nConclusion and Future Works\nThis paper presents EthioMT, a new MT corpus for\nlow-resource Ethiopian languages paired with En-\nglish, and discusses MT experiments with results.\nWe also present a new benchmark dataset for four\nEthiopian languages collected from public reposito-\nries. We obtained benchmark results with new train,\nvalidation, and test set splits and evaluated the new\ncorpus and new benchmark dataset using a trans-\nformer and fine-tuning multilingual translation models.\nFrom the two approaches, fine-tuning of the multilin-\ngual model outperformed the transformer approach\nin both translation directions.\nIn the future, we will work to increase the corpus\nsizes of the low-resource languages by extracting text\nfrom scanned documents and different sources. In\naddition, we will evaluate different MT approaches to\nlow-resource languages to improve performance.\n8.\nBibliographical References\nAndargachew Mekonnen Gezmu, Andreas Nürn-\nberger, and Tesfaye Bayu Bati. 2021a. Extended\nparallel corpus for amharic-english machine trans-\nlation. arXiv preprint arXiv:2104.03543.\nAndargachew Mekonnen Gezmu, Andreas Nürn-\nberger, and Tesfaye Bayu Bati. 2021b. Neural ma-\nchine translation for amharic-english translation. In\nICAART (1), pages 526–532.\nAngela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi\nMa, Ahmed El-Kishky, Siddharth Goyal, Mandeep\nBaines, Onur Celebi, Guillaume Wenzek, Vishrav\nChaudhary, et al. 2021. Beyond english-centric\nmultilingual machine translation. The Journal of\nMachine Learning Research, 22(1):4839–4886.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information process-\ning systems, 30.\nAsmelash Teka Hadgu, Adam Beaudoin, and Abel\nAregawi. 2020. Evaluating amharic machine trans-\nlation. arXiv preprint arXiv:2003.14386.\nAtnafu\nLambebo\nTonja,\nChristian\nMaldonado-\nSifuentes, David Alejandro Mendoza Castillo, Olga\nKolesnikova, Noé Castro-Sánchez, Grigori Sidorov,\nand Alexander Gelbukh. 2023a.\nParallel cor-\npus for indigenous language translation: Spanish-\nmazatec and spanish-mixtec.\narXiv preprint\narXiv:2305.17404.\nAtnafu Lambebo Tonja, Michael Melese Woldeyohan-\nnis, and Mesay Gemeda Yigezu. 2021. A parallel\ncorpora for bi-directional neural machine transla-\ntion for low resourced ethiopian languages. In 2021\nInternational Conference on Information and Com-\nmunication Technology for Development for Africa\n(ICT4DA), pages 71–76. IEEE.\nAtnafu Lambebo Tonja, Olga Kolesnikova, Muham-\nmad Arif, Alexander Gelbukh, and Grigori Sidorov.\n2022. Improving neural machine translation for low\nresource languages using mixed training: The case\nof ethiopian languages. In Mexican International\nConference on Artificial Intelligence, pages 30–40.\nSpringer.\nAtnafu Lambebo Tonja, Tadesse Destaw Belay, Is-\nrael Abebe Azime, Abinew Ali Ayele, Moges Ahmed\nMehamed, Olga Kolesnikova, and Seid Muhie Yi-\nmam. 2023b.\nNatural language processing in\nethiopian languages: Current state, challenges,\nand opportunities. arXiv preprint arXiv:2303.14406.\nAzeb Amha. 2017. The omotic language family. Cam-\nbridge University Press.\nBenjamin Philip King. 2015. Practical Natural Lan-\nguage Processing for Low-Resource Languages.\nPh.D. thesis.\nBernard Comrie. 2002. Languages of the world: who\nspeaks what.\nIn An encyclopedia of language,\npages 529–543. Routledge.\nDorothy Kenny. 2018. Machine translation. In The\nRoutledge handbook of translation and philosophy,\npages 428–445. Routledge.\nEbisa A Gemechu and GR Kanagachidambaresan.\n2021. Machine learning approach to english-afaan\noromo text-text translation: Using attention based\nneural machine translation. In 2021 4th Interna-\ntional Conference on Computing and Communica-\ntions Technologies (ICCCT), pages 80–85. IEEE.\nEdmund L Epstein and Robert Kole. 1998. The lan-\nguage of African literature. Africa World Press.\nGerrit J Dimmendaal, Colleen Ahland, Angelika\nJakobi, and Constance Kutsch Lojenga. 2019. Lin-\nguistic features and typologies in languages com-\nmonly referred to as ‘nilo-saharan’. Cambridge\nHandbook of African Languages, pages 326–381.\nHaifeng Wang, Hua Wu, Zhongjun He, Liang Huang,\nand Kenneth Ward Church. 2021. Progress in ma-\nchine translation. Engineering.\nHirut Woldemariam. 2007. The challenges of mother-\ntongue education in ethiopia: The case of north\nomo area. Language Matters, 38(2):210–235.\nIfe Adebara and Muhammad Abdul-Mageed. 2022.\nTowards afrocentric NLP for African languages:\nWhere we are and where we can go. In Proceed-\nings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 3814–3841, Dublin, Ireland. Associa-\ntion for Computational Linguistics.\nLidia Kidane, Sachin Kumar, and Yulia Tsvetkov. 2021.\nAn exploration of data augmentation techniques\nfor improving english to tigrinya translation. arXiv\npreprint arXiv:2103.16789.\nM Azath and Tsegay Kiros. 2020. Statistical machine\ntranslator for english to tigrigna translation. Int. J.\nSci. Technol. Res, 9(1):2095–2099.\nMarta R Costa-jussà, James Cross, Onur Çelebi,\nMaha Elbayad, Kenneth Heafield, Kevin Heffernan,\nElahe Kalbassi, Janice Lam, Daniel Licht, Jean\nMaillard, et al. 2022. No language left behind: Scal-\ning human-centered machine translation.\narXiv\npreprint arXiv:2207.04672.\nMatt Post. 2018. A call for clarity in reporting BLEU\nscores. In Proceedings of the Third Conference\non Machine Translation: Research Papers, pages\n186–191, Belgium, Brussels. Association for Com-\nputational Linguistics.\nMillion Meshesha and Yitayew Solomon. 2018.\nEnglish-afaan oromo statistical machine translation.\nInternational Journal of Computational Linguistic\n(IJCL), 9(1).\nMulu Gebreegziabher Teshome and Laurent Besacier.\n2012. Preliminary experiments on english-amharic\nstatistical machine translation.\nIn Spoken Lan-\nguage Technologies for Under-Resourced Lan-\nguages.\nNaman Goyal, Cynthia Gao, Vishrav Chaudhary,\nPeng-Jen Chen, Guillaume Wenzek, Da Ju, San-\njana Krishnan, Marc’Aurelio Ranzato, Francisco\nGuzmán, and Angela Fan. 2022. The flores-101\nevaluation benchmark for low-resource and mul-\ntilingual machine translation. Transactions of the\nAssociation for Computational Linguistics, 10:522–\n538.\nPavanpankaj Vegi, J Sivabhavani, Biswajit Paul, Abhi-\nnav Mishra, Prashant Banjare, KR Prasanna, and\nChitra Viswanathan. 2022. Webcrawl african: A\nmultilingual parallel corpora for african languages.\nIn Proceedings of the Seventh Conference on Ma-\nchine Translation (WMT), pages 1076–1089.\nPratik Joshi, Christain Barnes, Sebastin Santy, Sim-\nran Khanuja, Sanket Shah, Anirudh Srinivasan,\nSatwik Bhattamishra, Sunayana Sitaram, Monojit\nChoudhury, and Kalika Bali. 2019. Unsung chal-\nlenges of building and deploying language tech-\nnologies for low resource language communities.\narXiv preprint arXiv:1912.03457.\nPratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika\nBali, and Monojit Choudhury. 2020. The state and\nfate of linguistic diversity and inclusion in the nlp\nworld. arXiv preprint arXiv:2004.09095.\nSisay Adugna and Andreas Eisele. 2010.\nEn-\nglish—oromo machine translation: An experiment\nusing a statistical approach. In Proceedings of the\nSeventh International Conference on Language\nResources and Evaluation (LREC’10).\nSisay Chala, Bekele Debisa, Amante Diriba, Silas\nGetachew, Chala Getu, and Solomon Shiferaw.\n2021. Crowdsourcing parallel corpus for english-\noromo neural machine translation using com-\nmunity engagement platform.\narXiv preprint\narXiv:2102.07539.\nSolomon Teferra Abate, Michael Melese, Martha Yi-\nfiru Tachbelie, Million Meshesha, Solomon Atinafu,\nWondwossen Mulugeta, Yaregal Assabie, Hafte\nAbera, Biniyam Ephrem, Tewodros Gebreselassie,\net al. 2019. English-ethiopian languages statistical\nmachine translation. In Proceedings of the 2019\nWorkshop on Widening NLP, pages 27–30.\nSurafel M Lakew, Matteo Negri, and Marco Turchi.\n2020. Low resource neural machine translation:\nA benchmark for five african languages.\narXiv\npreprint arXiv:2003.14402.\nSurangika Ranathunga, En-Shiun Annie Lee, Mar-\njana Prifti Skenduli, Ravi Shekhar, Mehreen Alam,\nand Rishemjit Kaur. 2023. Neural machine trans-\nlation for low-resource languages: A survey. ACM\nComputing Surveys, 55(11):1–37.\nTadesse Destaw Belay, Atnafu Lambebo Tonja, Olga\nKolesnikova, Seid Muhie Yimam, Abinew Ali Ayele,\nSilesh Bogale Haile, Grigori Sidorov, and Alexan-\nder Gelbukh. 2022. The effect of normalization\nfor bi-directional amharic-english neural machine\ntranslation. In 2022 International Conference on\nInformation and Communication Technology for De-\nvelopment for Africa (ICT4DA), pages 84–89. IEEE.\nJaap Van der Meer. 2019. Translation technology–\npast, present and future. The Bloomsbury compan-\nion to language industry studies, pages 285–310.\nYeabsira Asefa Ashengo, Rosa Tsegaye Aga, and\nSurafel Lemma Abebe. 2021. Context based ma-\nchine translation with recurrent neural network for\nenglish–amharic translation. Machine Translation,\n35(1):19–36.\nYemane Tedla and Kazuhide Yamamoto. 2016. The\neffect of shallow segmentation on english-tigrinya\nstatistical machine translation. In 2016 Interna-\ntional Conference on Asian Language Processing\n(IALP), pages 79–82. IEEE.\nYemane Tedla and Kazuhide Yamamoto. 2017. Mor-\nphological segmentation for english-to-tigrinya sta-\ntistical machinetranslation. Int. J. Asian Lang. Pro-\ncess, 27(2):95–110.\nYitayew Solomon, Million Meshesha, and Wendewe-\nsen Endale. 2017.\nOptimal alignment for bi-\ndirectional afaan oromo-english statistical machine\ntranslation. vol, 3:73–77.\nYohanens Biadgligne and Kamel Smaïli. 2021. Par-\nallel corpora preparation for english-amharic ma-\nchine translation. In Advances in Computational\nIntelligence: 16th International Work-Conference\non Artificial Neural Networks, IWANN 2021, Virtual\nEvent, June 16–18, 2021, Proceedings, Part I 16,\npages 443–455. Springer.\nYohannes Biadgligne and Kamel Smaïli. 2022. Offline\ncorpus augmentation for english-amharic machine\ntranslation. In 2022 5th International Conference on\nInformation and Computer Technologies (ICICT),\npages 128–135. IEEE.\nZemicheal Berihu, Gebremariam Mesfin Assres, Mu-\nlugeta Atsbaha, and Tor-Morten Grønli. 2020.\nEnhancing bi-directional english-tigrigna machine\ntranslation using hybrid approach. In Norsk IKT-\nkonferanse for forskning og utdanning, 1.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2024-03-28",
  "updated": "2024-03-28"
}