{
  "id": "http://arxiv.org/abs/2403.19511v1",
  "title": "Improving Clinical NLP Performance through Language Model-Generated Synthetic Clinical Data",
  "authors": [
    "Shan Chen",
    "Jack Gallifant",
    "Marco Guevara",
    "Yanjun Gao",
    "Majid Afshar",
    "Timothy Miller",
    "Dmitriy Dligach",
    "Danielle S. Bitterman"
  ],
  "abstract": "Generative models have been showing potential for producing data in mass.\nThis study explores the enhancement of clinical natural language processing\nperformance by utilizing synthetic data generated from advanced language\nmodels. Promising results show feasible applications in such a high-stakes\ndomain.",
  "text": "Improving Clinical NLP Performance through Language Model-Generated Synthetic\nClinical Data\nShan Chen, MS1,2,3 Jack Gallifant, MBBS4,5 Marco Guevara, MS1,2 Yanjun Gao, PhD6\nMajid Afshar MD MSCR6 Timothy Miller, PhD3 Dmitriy Dligach, PhD7\nDanielle S. Bitterman, MD1,2,3\n1. Artificial Intelligence in Medicine (AIM) Program, Mass General Brigham, Harvard Medical School, Boston, MA,\nUSA\n2. Department of Radiation Oncology, Brigham and Women’s Hospital/Dana-Farber Cancer Institute, Boston, MA,\nUSA\n3. Computational Health Informatics Program, Boston Children’s Hospital, Harvard Medical School, Boston, MA, USA\n4. Institute for Medical Engineering and Science, Massachusetts Institute of Technology, Cambridge, MA, USA\n5. Department of Critical Care, Guy’s & St Thomas’ NHS Trust, London, United Kingdom,\n6. Department of Medicine, University of Wisconsin School of Medicine and Public Health, Madison, WI, USA\n7. Department of Computer Science, Loyola University Chicago, Chicago, IL, USA\nBrief communication 70 words: Generative models have been showing potential for producing\ndata in mass. This study explores the enhancement of clinical natural language processing\nperformance by utilizing synthetic data generated from advanced language models. Promising\nresults show feasible applications in such a high-stakes domain.\nCorresponding author:\nDr. Danielle S. Bitterman\nDepartment of Radiation Oncology\nDana-Farber Cancer Institute/Brigham and Women’s Hospital\n75 Francis Street, Boston, MA 02115\nEmail: dbitterman@bwh.harvard.edu\nPhone: (857) 215-1489\nFax: (617) 975-0985\nPrior presentations: AMIA 2024 Informatics Summit Oral Abstract\nThe\nauthors\nacknowledge\nfinancial\nsupport\nfrom\nthe\nWoods\nFoundation\n(DB)\nNIH\n(NIH-USA\nU54CA274516-01A1 (SC, DB), NIH-NIDA R01DA051464 (MA), NIH-USA R01LM012973 (TM,MA),\nNIH-USA R01MH126977 (TM), NIH-USA U54 TW012043-01 (JG), NIH-USA OT2OD032701 (JG).\nDisclosures:\nDSB: Associate Editor of Radiation Oncology, HemOnc.org (no financial compensation, unrelated to this\nwork); Funding from American Association for Cancer Research (unrelated to this work). MercurialAI\n(scientific advisory board).\nIntroduction:\nA common challenge for the development of clinical natural language processing (NLP)\nmethods is the availability of large annotated datasets for model training, fine-tuning, and\nevaluation. Traditional annotation processes are time-consuming, expensive, and often require\nexpert\nmedical\nknowledge,\ncreating\nsignificant\nresearch\nand\nbenchmark\ndevelopment\nconstraints.1 Furthermore, concerns around patient privacy and data governance further\ncomplicate the sharing of large clinical datasets and limit the development of generalizable\nmodels.2,3\nThere has been increasing interest in synthetic-based approaches to overcome these\nconstraints, with generative adversarial network-based methods having already shown promise\nin EHR-based tabular data.4 However, traditional text-based synthetic data methods like\nparaphrasing and word swapping have been constrained by their limited semantic and style\nvariety and challenges surrounding grammatical errors.5–8 The advances in large language\nmodels (LLMs), which excel at following natural language instructions to generate fluent text,\nnot only offer promise in solving clinical tasks 9–12, but may also be well-suited to synthetic text\ngeneration that is high enough quality to augment manually labeled datasets for NLP model\ndevelopment. 5–8 Such synthetic datasets could accelerate the development of high-performing\nclinical NLP models by minimizing clinical data and human effort requirements.\nThis study proposes a novel method for generating synthetic annotated clinical text datasets\nusing LLMs, and evaluates their impact on downstream task performance compared to those\ntrained on gold-standard, expert-annotated datasets. We introduce a novel approach of label\ncorrection, which is an active learning step that we apply to enhance synthetic dataset quality.\nWe demonstrate the efficacy of our synthetic data augmentation step on both NLP benchmarks\nand real-world long document clinical datasets.\nMethods:\nOur research evaluated synthetic data’s value for existing curated clinical benchmark tasks\n(Figure 1a) and for a real-world, long-document clinical task. For the curated tasks, we used the\nfollowing three clinical NLP tasks from DR.BENCH13, which was developed using the MIMIC III\ndataset14: medical natural language inference (MedNLI), Assessment and Plan relation labeling\n(A/P Reasoning), and problem list summarization (ProbSum). We employed two versions of the\nLlama-2 LLM for synthetic data generation: one with 7 billion parameters and another with 70\nbillion parameters15.\nFor synthetic data generation, a fixed subset of the training set served as exemplars for\nLLM-generated prompts to generate new data. The gold standard MedNLI dataset consists of\n11,232 annotated data points, of which 20% were used as exemplars. The A/P Reasoning\ndataset consists of 4,633 annotated data points, of which 100% were used as exemplars. The\nProbSumm dataset consists of 600 data points, of which 50% were used as exemplars.\nFor the label correction step, these exemplars were also used to fine-tune models for the\nDR.BENCH tasks, referred to as \"label corrector\" models, to improve the quality of synthetic\ndata labels. We generated an amount of synthetic data equal to the size of the original training\nset for each task. The amount of data used to generate synthetic data for each dataset was\nchosen by the amount of data needed to obtain a stable label corrector. 16\nFor the two classification tasks (MedNLI & A/P Reasoning), we fine-tuned the FLan-T5-3B17\nmodel with low-rank adaptors (LoRA)18. For the text generation task (ProbSumm), we fine-tuned\nthe Llama-2-7B model. We evaluated two approaches to using synthetic data: replacing the\ngold-standard\ntraining\ndataset\nentirely\nwith\nthe\nsynthetic dataset and augmenting the\ngold-standard training dataset with the synthetic dataset. Performance was assessed on the\nheld-out test sets for each task.\nFor the real-world, long-document clinical task (Figure 1b), we focused on a practical clinical\ntask involving detecting esophagitis and its severity in cancer patient notes.19 Adopting a similar\napproach, we fine-tuned PubMedBERT\n20 for three classification tasks related to this clinical\ncondition. A subset of 200 out of 1,243 gold-labeled notes from the original training set was\nutilized for synthetic data generation only. We used a HIPAA-compliant GPT-3.5 Turbo 0613\nmodel, accessed through the MGB-Azure OpenAI service, for summarization (to overcome long\ndocument challenges) and synthetic data generation via in-context learning, ensuring the\nstudy's adherence to privacy regulations.\nWe used T5/Llama models as classifiers for the DR.BENCH series of tasks and used\nPubMedBERT for our esophagitis grading tasks simply because the previous state-of-the-art for\nthese tasks deployed the same models for better direct comparisons. We used GPT-3.5 to\ngenerate synthetic data for the esophagitis grading tasks because Llama-2 failed to generate\nreliable summaries for this real-world task.\nThis study was approved by the Mass General Brigham IRB.\nYou\ncan\nfind\nall\nour\ndetailed\nprompts\nfor\nthe\nsynthetic\ndata\ngeneration\nstep\nat\nhttps://github.com/AIM-Harvard/fake2real. Due to real PHI limitations, we cannot share our\ngenerated synthetic data.\nResults:\nThe performance of clinical models was evaluated in three scenarios: (1) fine-tuning with only\ngold-standard data (i.e., expert-annotated real clinical text); (2) fine-tuning with only synthetic\ndata; (3) synthetic data generation with few-shot examples and no label corrections; and (4)\nfine-tuning with an augmented dataset consisting of gold plus synthetic data. Performance was\nbenchmarked using the Diagnostic Reasoning Benchmark (DR.BENCH13) dataset, which\nincludes tasks such as medical natural language inference (MedNLI), Assessment and Plan\nrelation labeling (A/P Reasoning), and problem list summarization (ProbSum); see Table 1.1.\nAmong all clinical benchmark tasks, we observed a large drop in performance when using\nsynthetic data alone without label correction. However, with label correction, incorporating\nsynthetic data via augmentation and replacement demonstrated competitive results across all\ntasks. The overall trends in performance differences between the approaches were similar\nacross the three tasks, regardless of the model sizes and types used to generate the synthetic\ndata. Notably, for the A/P Reasoning task, the augmentation approach with synthetic data\nexceeded the prior state-of-the-art performance achieved with gold-only data. In the case of the\nProbSumm and MedNLI tasks, synthetic-only datasets, when used with label correction,\napproached the performance of the gold-only dataset.\nThe generalisability of these results to real-world clinical tasks was also evaluated using a\ndisease classification task; more specifically, grading esophagitis severity in cancer patient\nnotes (Table 1.2). Using over 3600 annotated real patient notes, the macro F1 scores for the\nthree classification tasks were 0.92, 0.82, and 0.74, respectively; observing a small performance\ndrop using a gold-only summary compared to the gold-only model across the three tasks. In\ncontrast, the model fine-tuned on only synthetic data (labeled using the label corrected created\nwith 200 gold-labeled notes) outperformed the model fine-tuned on the same 200 gold-labeled\nnotes only, and reached comparable performance to the model fine-tuned with all 1243\ngold-labeled notes. Furthermore, the synthetic-only model outperformed in-context learning\napproaches using GPT-3.5. The highest performance was observed using the augmentation\nstrategy combining gold and synthetic data, which realized scores comparable to those of using\ndata augmentation with real silver-labeled clinic notes.\nFigure 1. a) Our proposed workflow for synthetic data generation with label correction. b)\nNote-level esophagitis classification included an additional summarization pre-processing step,\nwhere GPT-3.5-turbo was used to first summarize relevant information followed by synthetic\nsummary generation. Illustration style inspired by OpenAI Weak2strong project.\nDiscussion:\nWe show that LLMs can generate synthetic clinical datasets that approach or exceed\nstate-of-the-art performance on benchmarks and real-world tasks, particularly when used to\naugment expert-labeled examples. This approach could mitigate the challenges of generating\nlarge annotated clinical NLP datasets, which have applications across biomedical research and\nclinical care. Our novel label corrector approach yielded substantial performance improvements\ncompared to synthetic texts created using in-context learning alone.\nOne of the key implications of our work is the potential for reducing reliance on large volumes of\nreal clinical data, which is often difficult to obtain due to privacy concerns and the need for\nexpert annotation. By generating synthetic data that closely mimics real clinical text, our method\noffers a scalable solution to these challenges while reducing annotation requirements;\nexemplified by achieving comparable performance with less than one-sixth of the annotated\ndata in our real-world clinical task. Further, as AI-generated clinical text becomes more\nprevalent\nwith\nLLM-enabled\ndocumentation,\nsynthetic-based\napproaches could play an\nimportant role in fine-tuning models and generating clinical-based benchmarking methods to\nassure their robustness and safety in real-world settings.\nConclusion:\nOverall, our study underscores the significant potential of synthetic data generated by LLMs to\nenhance the performance of downstream clinical NLP tasks. By incorporating synthetic data\nalongside expert-annotated datasets, our methods could help address critical challenges such\nas data scarcity and the intensive demand for expert annotation.\nOur findings highlight the importance of continuous refinement in synthetic data generation and\nlabel correction techniques. This research demonstrates the promise of LLM-enabled data\naugmentation and model training in clinical NLP, and opens avenues for further investigation\ninto the refinement and integration of synthetic data. Future research directions include\nexploring\nsynthetic\nproxy\ndata-sharing\nacross\ninstitutions,\nmulti-institutional\nsynthetic\nbenchmarking, and assessing potential biases introduced by synthetic data.\nRole of the funding source: The study's funders had no role in the design, data collection,\nanalysis, interpretation, or report writing.\nReferences:\n1.\nChen, S. et al. Evaluation of ChatGPT Family of Models for Biomedical Reasoning and\nClassification. arXiv [cs.CL] (2023).\n2.\nWatson, H. et al. Delivering on NIH data sharing requirements: avoiding Open Data in\nAppearance Only. BMJ Health Care Inform 30, (2023).\n3.\nIyengar, A., Kundu, A. & Pallis, G. Healthcare Informatics and Privacy. IEEE Internet\nComput. 22, 29–31 (2018).\n4.\nYoon, J. et al. EHR-Safe: generating high-fidelity and privacy-preserving synthetic electronic\nhealth records. NPJ Digit Med 6, 141 (2023).\n5.\nPiedboeuf, F. & Langlais, P. Is ChatGPT the ultimate Data Augmentation Algorithm? in\nFindings of the Association for Computational Linguistics: EMNLP 2023 (eds. Bouamor, H.,\nPino, J. & Bali, K.) 15606–15615 (Association for Computational Linguistics, Singapore,\n2023).\n6.\nTang, R., Han, X., Jiang, X. & Hu, X. Does Synthetic Data Generation of LLMs Help Clinical\nText Mining? arXiv [cs.CL] (2023).\n7.\nLi, R., Wang, X. & Yu, H. Two directions for clinical data generation with large language\nmodels: Data-to-label and label-to-data. in Findings of the Association for Computational\nLinguistics: EMNLP 2023 (Association for Computational Linguistics, Stroudsburg, PA,\nUSA, 2023). doi:10.18653/v1/2023.findings-emnlp.474.\n8.\nVeselovsky, V. et al. Generating Faithful Synthetic Data with Large Language Models: A\nCase Study in Computational Social Science. arXiv [cs.CL] (2023).\n9.\nGuevara, M. et al. Large language models to identify social determinants of health in\nelectronic health records. NPJ Digit Med 7, 6 (2024).\n10. Lehman, E. et al. Do We Still Need Clinical Language Models? arXiv [cs.CL] (2023).\n11. Hegselmann, S. et al. TabLLM: Few-shot Classification of Tabular Data with Large\nLanguage Models. in Proceedings of The 26th International Conference on Artificial\nIntelligence and Statistics (eds. Ruiz, F., Dy, J. & van de Meent, J.-W.) vol. 206 5549–5581\n(PMLR, 25--27 Apr 2023).\n12. Agrawal, M., Hegselmann, S., Lang, H., Kim, Y. & Sontag, D. Large Language Models are\nFew-Shot Clinical Information Extractors. arXiv [cs.CL] (2022).\n13. Gao, Y. et al. DR.BENCH: Diagnostic Reasoning Benchmark for Clinical Natural Language\nProcessing. J. Biomed. Inform. 138, 104286 (2023).\n14. Johnson, A. E. W. et al. MIMIC-III, a freely accessible critical care database. Sci Data 3,\n160035 (2016).\n15. Touvron, H. et al. Llama 2: Open Foundation and Fine-Tuned Chat Models. arXiv [cs.CL]\n(2023).\n16. Ren, P. et al. A Survey of Deep Active Learning. ACM Comput. Surv. 54, 1–40 (2021).\n17. Longpre, S. et al. The Flan Collection: Designing Data and Methods for Effective Instruction\nTuning. arXiv [cs.AI] (2023).\n18. Hu, E. J. et al. LoRA: Low-Rank Adaptation of Large Language Models. arXiv [cs.CL]\n(2021).\n19. Chen, S. et al. Natural Language Processing to Automatically Extract the Presence and\nSeverity of Esophagitis in Notes of Patients Undergoing Radiotherapy. JCO Clin Cancer\nInform 7, e2300048 (2023).\n20. Gu, Y. et al. Domain-Specific Language Model Pretraining for Biomedical Natural Language\nProcessing. arXiv [cs.CL] (2020).\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2024-03-28",
  "updated": "2024-03-28"
}