{
  "id": "http://arxiv.org/abs/2403.19758v2",
  "title": "Quantum Natural Language Processing",
  "authors": [
    "Dominic Widdows",
    "Willie Aboumrad",
    "Dohun Kim",
    "Sayonee Ray",
    "Jonathan Mei"
  ],
  "abstract": "Language processing is at the heart of current developments in artificial\nintelligence, and quantum computers are becoming available at the same time.\nThis has led to great interest in quantum natural language processing, and\nseveral early proposals and experiments.\n  This paper surveys the state of this area, showing how NLP-related techniques\nhave been used in quantum language processing. We examine the art of word\nembeddings and sequential models, proposing some avenues for future\ninvestigation and discussing the tradeoffs present in these directions. We also\nhighlight some recent methods to compute attention in transformer models, and\nperform grammatical parsing. We also introduce a new quantum design for the\nbasic task of text encoding (representing a string of characters in memory),\nwhich has not been addressed in detail before.\n  Quantum theory has contributed toward quantifying uncertainty and explaining\n\"What is intelligence?\" In this context, we argue that \"hallucinations\" in\nmodern artificial intelligence systems are a misunderstanding of the way facts\nare conceptualized: language can express many plausible hypotheses, of which\nonly a few become actual.",
  "text": "Submitted manuscript awaiting manuscript No.\n(will be inserted by the editor)\nQuantum Natural Language Processing\nDominic Widdows · Willie Aboumrad · Dohun Kim · Sayonee Ray ·\nJonathan Mei\nPreprint of paper submitted to Springer KI Special Issue. Please check there for final edited version.\nAbstract Language processing is at the heart of cur-\nrent developments in artificial intelligence, and quantum\ncomputers are becoming available at the same time. This\nhas led to great interest in quantum natural language\nprocessing, and several early proposals and experiments.\nThis paper surveys the state of this area, showing\nhow NLP-related techniques have been used in quantum\nlanguage processing. We examine the art of word embed-\ndings and sequential models, proposing some avenues for\nfuture investigation and discussing the tradeoffs present\nin these directions. We also highlight some recent meth-\nods to compute attention in transformer models, and\nperform grammatical parsing. We also introduce a new\nquantum design for the basic task of text encoding (rep-\nresenting a string of characters in memory), which has\nnot been addressed in detail before.\nQuantum theory has contributed toward quantifying\nuncertainty and explaining “What is intelligence?” In\nthis context, we argue that “hallucinations” in modern\nartificial intelligence systems are a misunderstanding of\nthe way facts are conceptualized: language can express\nmany plausible hypotheses, of which only a few become\nactual.\nKeywords Quantum Language Processing · QNLP ·\nQuantum AI · Quantum String Encoding\n1 Introduction\nIn early 2024, quantum computing and AI are two of the\nmost rapidly-moving and talked-about areas of science\nIonQ, Inc. 4505 Campus Dr, College Park, Maryland, USA\nE-mail: [widdows|jonathan.mei|aboumrad|sayonee]@ionq.com\nPohang University of Science and Technology\nE-mail: dohunkim@postech.ac.kr\nand technology. The availability of dialog systems based\non large language models (LLMs) has raised the pro-\nfile of natural language processing (NLP) to a historic\nhigh, developing and expanding very quickly. This ex-\npansion has led to AI models being deployed as systems\nand introduced as components in new ways, leading to\nimprovements and efficiencies, but also mistakes and\nconcerns. Thus the demand for improvements in AI is\nat an all-time high, with a renewed focus on reliability\nand trust.\nQuantum theory offers new forms of mathematical\nmodeling, computation and communication. Mathemat-\nical models for language operations motivated explic-\nitly by quantum theory have been used in information\nretrieval [85, 80], logic and disambiguation [92], and\nlanguage composition [25, 26]. Similar models have been\ndeveloped in many social sciences and demonstrated\nsuccessful results over classical alternatives, long before\nany such models were implemented and run on quantum\ncomputers. More abstractly, entire classes of quantum\nmachine learning (ML) models have been theoretically\nshown to have more expressive power than comparable\nclassical models [27, 96], though this does not guarantee\nimprovements in results more generally [15]. Running\nbasic NLP algorithms on quantum has become possi-\nble only in the last few years, with early-stage results\nreported by [58, 94].\nThis paper is intended as an introduction to this\nlandscape, for those interested in language processing\nand quantum computing, but not necessarily specialists\nin either. Firstly, Section 2 gives a brief introduction to\nquantum gates and circuits. Section 3 continues with\nan idealized example of how quantum gates could be\nused to represent a text string of exponential length in\na register of qubits, including some caveats and pitfalls.\nThis gives a glimpse of some of the wonder, and some\narXiv:2403.19758v2  [quant-ph]  26 Apr 2024\n2\nDominic Widdows et al.\nof the challenges, of quantum computing. The main\nbody of the paper surveys ways in which other aspects\nof language processing have already been modeled on\nquantum computers, including embedding vectors, se-\nquences, attention, and grammatical structure (Sections\n4–7). This gives a snapshot of of where quantum NLP\nhas got to at this stage of the NISQ era. Finally, Section\n8 discusses the challenges of choosing and distinguishing\nbetween the hypothetical and the actual. This has taken\non fresh urgency in AI systems for fact-checking, to\navoid mistaking so-called hallucinations for assertions.\nWe note that language models are designed to produce\nboth hypothetical and actual statements, and that quan-\ntum mechanics is a better starting point than classical\nmechanics for modeling this.\n2 Quantum Computing Basics\nIn early 2024, quantum computers are real and in reg-\nular use, and quantum runtime is offered as-a-service\nby many companies, via the internet / cloud. This sec-\ntion introduces some of the basic building blocks of\nquantum computing, from the perspective of a devel-\noper designing quantum programs, particularly to run\non today’s noisy-intermediate scale quantum (NISQ)\nhardware. The development process involves specifying\na register of qubits, and saying what logic gates and\nmeasurements should be performed on these qubits.\nThe material here overlaps with the introduction\nof [91]. Some familiarity with quantum mechanics, es-\npecially Dirac notation, is assumed, so that |0⟩and\n|1⟩are the basis states for a single qubit whose state\nis represented in the complex vector space C2, a 2-\nqubit state is represented in the tensor product space\nC2 ⊗C2 ∼= C4 with basis states |00⟩, |01⟩, |10⟩and\n|11⟩, 3-qubit states are represented in C⊗3 ∼= C8 with\nbasis states |000⟩, |001⟩, . . . , |111⟩, and so on. For intro-\nductions to how linear algebra is written and used in\nquantum mechanics, see [67, Ch 2]. Quantum measure-\nment is probabilistic: if |ϕ⟩is an eigenvector of a given\nmeasurement operator, then a system in the state |ψ⟩\nis observed to be in the state |ϕ⟩with probability given\nby the square of their scalar product, ⟨ϕ|ψ⟩2 (the Born\nrule), and if this outcome is observed, the system is now\nin the state |ϕ⟩.\nIn mathematical terms, the key features that distin-\nguish quantum from classical computers are superposi-\ntion and entanglement. Superposition can be realized in\na single qubit: the state α |0⟩+ β |1⟩is a superposition\nof the states |0⟩and |1⟩, where α and β are complex\nnumbers, with |α2| + |β2| = 1. Each single-qubit logic\ngate is a linear operator that preserves the orthogonal-\nity of the basis states and this normalization condition,\nand the group of such operators is U(2), the group\nof complex 2 × 2 unitary matrices. Single-qubit gates\nthat feature prominently in this paper are shown in\nFigure 1. So single-qubit gates coherently manipulate\nthe superposition state of an individual qubit.\nEntanglement is a property that connects different\nqubits. Since the 1930’s, quantum entanglement has\ngone from a hotly-disputed scientific prediction, to a\nstatistical property demonstrated with large ensembles,\nto a connection created between pairs of individual par-\nticles, to a working component in quantum computers.\nAll modern quantum computers have some implementa-\ntion of an entangling gate, and only one kind is really\nneeded, because all possible 2-qubit entangled states\ncan be constructed mathematically by combining appro-\npriate single-qubit gates before and after the entangling\ngate. Furthermore, a single 2-qubit entangling gate and\na set of single-qubit gates forms a universal gateset\nfor quantum computing [67, §4.5]. Entanglement is the\ncrucial feature that distinguishes quantum computing\nalgorithmically, because predicting the probability dis-\ntributions that result from quantum operations with\nentanglement can become exponentially hard for classi-\ncal computers. In simpler terms, quantum computing\nis special because it offers special kinds of interference,\nnot because it offers special kinds of in-between-ness.\nA quantum circuit consists of a register of qubits,\nand a sequence of logic gates that act on these qubits.\nSome of the basic gates used in this paper are shown in\nFigures 1 and 2. The Pauli-X gate is commonly used to\nflip a qubit between the |0⟩and |1⟩gates, which is why\nit is also sometimes called the quantum NOT gate. X-\ngates applied to different qubits can be used to prepare\nan input state representing a binary-valued vector: the\nstate |010 · · · 001⟩is prepared by applying an X-gate to\neach of the qubits to be switched to the |1⟩state.\nThe Hadamard (H) gate is commonly used to put a\nqubit into a superposition state: for example, it maps\na qubit prepared in the state |0⟩to the superposition\n1\n√\n2(|0⟩+ |1⟩). Applying an H-gate to each qubit in an\narray is used to initialize a binary vector all of whose\ncoordinates have a 50-50 chance of being observed in\nthe |0⟩or |1⟩state.\nOther probabilities, anywhere in the range [0, 1], can\nbe arranged by using fractional rotations (which might\ninvolve sending just the same laser-pulse instructions,\nbut for different time periods). An example is given in\nthe RX(θ) gate. Several variational quantum algorithms\nwork by gradually optimizing such θ parameters.\nThe CNOT gate is a 2-qubit entangling gate, that\nacts upon the state α |00⟩+ β |01⟩+ γ |10⟩+ δ|11⟩. In\nthe standard basis, its behavior can be described as\nQuantum Natural Language Processing\n3\nPauli-X (NOT)\nX\n\u0014\n0 1\n1 0\n\u0015\nHadamard (H)\nH\n1\n√\n2\n\u0014\n1 1\n1 −1\n\u0015\nRX rotation\nexp(−i θ\n2X)\nRX(θ)\n\u0014\ncos θ\n2 i sin θ\n2\ni sin θ\n2 cos θ\n2\n\u0015\nFig. 1 Single-qubit gates used in this paper, and their corre-\nsponding matrices, which operate on the superposition state\nα |0⟩+ β |1⟩written as the column vector \u0002\nα β\u0003T .\nControlled Not\n(CNOT, CX)\n\n\n1 0 0 0\n0 1 0 0\n0 0 0 1\n0 0 1 0\n\n\nFig. 2 Two-qubit CNOT (controlled-X) and gate\nMulti-\nControlled\nNot\n=\nX\nX\nFig. 3 Three-qubit multi-controlled gate (Toffoli gate) with |1⟩\nand |0⟩control states.\n“performing a NOT operation on the target qubit if the\ncontrol qubit is in state |1⟩”.\nBy assembling several 1- and 2-qubit gates, more\nqubits can become entangled, and we can define multi-\ncontrolled gates. For example, the 3-qubit Toffoli gate\nin Figure 3 flips the lowest qubit if the top qubit is in\nthe |1⟩state and the middle qubit in the |0⟩state. It is\nrelatively easy mathematically to start arranging such\ngate recipes into higher-level operations: for example,\nthe 3-qubit gate acts as a logical AND, from which\nsimple binary arithmetic can be constructed.\nHowever, developers must be careful when using such\nconstructions, because gate complexity and errors can\neasily build up. In practice, it takes 5 CNOT gates and\n9 single-qubit gates to assemble the 3-qubit Toffoli gate,\nso if gate errors are above 1%, the error-rate in a circuit\nwith just 3 Toffoli gates would be over 50%. In 2024,\ngate error rates tend to be much better than this, but\nstill, typical circuits today do not reliably run more\nthan a few hundred gates. NISQ-era quantum circuit\ndevelopment tends to tradeoff between sophistication\n(more gates introduces more tunable parameters) and\nreliability (fewer gates gives fewer errors). It also leads\nto designs where classical components are relied upon for\nmany parts of an NLP pipeline, such as storing weights\nand comparing scores [94].\nIn quantum machine learning, variational circuits,\nor parametrized quantum circuits (PQCs), are a partic-\nularly clear example of such tradeoffs [77, Ch 5]. Vari-\national circuit designs use i. a quantum circuit with\nparameters {θi}, often implemented as variable gate\nangles, that can be optimized according to a given loss\nfunction; and ii. a classical optimizer, responsible for\nevaluating the measurement outputs of the quantum\ncircuit, and proposing updates to the parameters {θi}.\nWhen both classical and quantum components play such\na prominent role, the combination is sometimes called a\nhybrid system or hybrid workflow.\nThe transition from NISQ to fault-tolerant quantum\ncomputing will be gradual, and arguably is already un-\nderway: recent months have seen encouraging progress\nin quantum error correction and memory fidelity [17].\nThis raises long-term expectations that quantum com-\nputers will optimize crucial matrix operations, such as\nsolving systems of equations [42], and quantum singular\nvalue transformation [36]. However, it is important to\nremember that even fault-tolerant quantum computing\nwill come with serious caveats, and in particular, quan-\ntum components bring no advantage if their I/O costs\noutweigh their computational gains [1].\nThrough the rest of this paper, we highlight examples\nof some of these considerations and design differences\nas they appear.\n3 A Quantum String Encoding Example\nCharacter and string encoding is one of the most basic\ntasks in language processing, and this section gives a\nworked example of how this might be performed on a\nquantum computer using some of the standard quantum\ncircuit and gate patterns introduced in the previous\nsection. This makes a good case-study of some of the\npromise and challenges of quantum computing, and (as\nfar as we know) is the first such proposal for representing\ntext strings in a quantum computer, comparable to the\nuse of ASCII or Unicode specifications in mainstream\nclassical computing.\nTo encode a text of meaningful length, this encoding\nwould require many layers of 2-qubit gates, and this\ndesign would require error correction, rather than being\na NISQ-era proposal. Other methods of encoding the\nmeanings of texts in quantum NLP work have been\ndevised, such as vector embeddings for use in machine\nlearning classifiers, and several such techniques will be\nsurveyed in later sections. The example quantum circuit\ndesigns in this section are for the (much older) protocol\nof representing words as a sequence of characters cho-\nsen from a relatively small character set. Some of the\nquantum word-encoding models based on a sequences\n4\nDominic Widdows et al.\nX\nX\nX\nX\nEncodes a = 01\nEncodes b = 10\nEncodes c = 11\nFig. 4 Simple encodings for a three-letter alphabet in a two-\nqubit register, using the convention that the top qubit in the\nregister is the least-significant “units” bit in the binary encoding.\nand embeddings will be surveyed in later sections of this\npaper.\nThe established way to define a string in computer\nscience is to rely on an encoding standard such as ASCII\nwhich identifies letters with numbers (A=65, B=66, etc.),\nand then a string such as CAB can be represented as\nthe number sequence [67, 65, 66]. Here the quantum\ndeveloper faces an immediate and typical challenge:\narrays and lists are not standardized components, and\na strategy to read from the next location in memory\nneeds to be introduced as part of the design. This lack\ngeneralizes: there is much more established literature,\nsoftware, and hardware for quantum algorithms than\nfor quantum data structures.\nOne data structure we can use as a building block\nin quantum circuits is binary positional notation for\nintegers. For example, in a 4-qubit register, the state\n|1010⟩could represent the decimal number 10 (if read\nleft-to-right) or 5 (if read right-to-left). This convention\nwas used right at the beginning of quantum comput-\ning, in Feynman’s proposal of how to build a quantum\nadder circuit [30], and is part of Shor’s integer factoring\nalgorithm [79]. These numbers can be represented in\nquantum circuits using X-gate bit-flip operations on the\ncorresponding qubits, as in Figure 4.\nOur string encoding design works by entangling a\n“position” register that records where a character ap-\npears, along with an “alphabet” register that says which\ncharacter appears in that position. So instead of a se-\nquence [3, 1, 2], the string is represented as a tensor\nproduct 1P ⊗3C + 2P ⊗1C + 3P ⊗2C, where nP is\nthe state representing the P th position, and mC is the\nstate representing the Cth character. A similar pattern\nis used by [3] for representing images, and the encoding\nis called QPIXL. QPIXL uses one register to specify the\nlocation of a pixel in an image, entangled with another\nregister saying which channel (e.g., red, green, or blue)\nis being referred to. So, QPIXL also keeps “what it is”\nand “where it is” in separate registers and entangles\nthese. Emphasizing this similarity, we call our string\nencoding protocol QPOSTR, pronounced “Q-poster”,\nmeaning “Quantum Positional String”.\nThe implementation of this formula as a quantum\ncircuit component for encoding the string cab is outlined\npos0 :\n•\npos1 :\n•\nchr0 :\nGate for c = 11\nGate for a = 01\nchr1 :\nGate for b = 10\nFig. 5 Position and Character Encoding for the string cab.\npos0 :\nH\n•\npos1 :\nH\n•\nchr0 :\nGate for c = 11\nGate for a = 01\nchr1 :\nGate for b = 10\nFig. 6 QPOSTR Encoding for the string cab.\nin Figure 5. The top 2 qubits form the “position” register,\nand the values on the control qubits are represented by\nthe open and closed circles, ◦= 0, • = 1. Starting with\nthe top as the least significant bit, the control states are\n◦◦= 00 = 0, ◦• = 01 = 1, •◦= 10 = 2, etc. The bottom\n2 qubits form the “character” or “alphabet” register.\nEach letter in the alphabet is mapped to a number\ncorresponding to its position, so the gate-recipe for each\ncharacter is like one of the simple circuits shown in\nFigure 4. To encode the string cab, a character register\nof 2 qubits suffices. To encode 26 letters, a character\nregister of 5 qubits would be required (since 25 = 32),\nand for the ASCII character set, 7 qubits would be\nneeded.\nIf the circuit above is prepared in the conventional\n|0000⟩state, the first gate controlled on the ◦◦= 00\nstate is the only one active, and the circuit output will\nbe 00 (in the position register) and 11 (in the character\nregister), saying just “the zeroth character was a c.” To\nprepare a superposition of the characters in all of the\npositions, the position register is prepared in a uniform\ndistribution over all the available character positions, us-\ning a standard array of Hadamard (H) gates. This gives\nthe full circuit for encoding the string cab in Figure 6.\nCharacter positions beyond the length of the string are\nuntouched, or left with character “0” in that position.\nUsing the convention that character “0” represents a\nspace, this is equivalent to padding a string with trailing\nzeros to make its length a power of 2.\nWith n qubits, the position register can encode up to\n2n positions, and with m qubits, the alphabet register\ncan encode up to 2m characters. Thus, a QPOSTR\ncircuit with m + n qubits can represent a string of\nlength up to 2n, with up to 2m characters. By contrast,\na classical computer requires m × 2n classical bits to\nstore the same string.\nAs a thought experiment, we can use GPT-3 training\nmetadata to demonstrate this savings. The size of the\ntraining dataset is reported at ∼300B tokens [19], so a\nQuantum Natural Language Processing\n5\npos0 :\nH\n•\n•\n•\npos1 :\nH\n•\nchr2 :\nGate for c\nGate for a\n•\nchr3 :\nGate for b\n•\noutput0 :\noutput1 :\nclassical\nreadout\n: /2\n0\n\u000b\u0013\n1\n\u000b\u0013\nFig. 7 QPOSTR readout circuit which recovers the character\n“a” from position ◦• = 01 of the string cab.\ngenerous estimate of 12 characters-per-token allows for\n12 × 300 × 109 < 242 character-positions, for which the\npositional encoding fits in 42 qubits. There are currently\n149,813 Unicode characters, so even this alphabet fits in\n18 qubits, which means that the entire ∼45TB training\ndataset of GPT-3 could fit into a mere 18 + 42 = 60\nqubits!\nWe can recover information about which character is\nin which position by adding an output register with the\nsame number of qubits as the character register. The\ncircuit for this is shown in Figure 7. The multi-controlled\ngates that connect the QPOSTR representation to the\nreadout register are configured to detect the same po-\nsition in the string. Each extra qubit in the controls\nfor these gates is set to detect a particular bit in the\ncharacter register, and if this bit is set to a 1, then the\ncorresponding output bit is set to a 1.\nIf this process could be repeated many times, and if\nthe output qubits could be reset to |0⟩independently\nof the other qubits in the register, eventually the entire\ninput string or any parts of it could be read out to\nclassical memory. Superficially, this extension of the\nreadout circuit gives a circuit that demonstrates that the\nwhole original string can be recovered from the QPOSTR\nquantum encoding. In other words, it looks as if we can\nrecover an exponentially-long string (2n characters) from\nan encoding that uses n + ⌈log2(alphabet size)⌉qubits!\nHowever, this is deceptive: due to the H gates at the\nbeginning, we have no way of knowing which position\nwill be measured by each readout operation, and to\nguarantee statistically that each position is sampled, we\nwould need an exponential number of measurements on\ndifferent copies of the state. (In practice, many shots\nof the quantum circuit would need to be run.) This is\nin line with a general theorem in quantum computing,\nthe Holevo bound [67, 12.1.1], which limits the amount\nof classical information that can reliably be recovered\nfrom a quantum state.\nThus, QPOSTR gives an exponential space advan-\ntage over the classical alternative. However, it still takes\na length of time at least linear in the length of the\nstring to run the quantum circuit that loads the string\ninto quantum memory. Like many quantum encodings,\nQPOSTR only gives an exponential space saving, which\noffers no obvious practical advantage without a corre-\nsponding time saving. Thus, even if we were to encode\nthe GPT-3 training data, it is unclear at this moment\nwhat savings could come from doing so. That said, we\noptimistically speculate that future algorithms or data\nstructures may be able to better take advantage of this\nencoding.\nMore generally, the challenge of preparing a quan-\ntum memory that can be maintained and successively\nqueried is sometimes described as research in QROM\nand QRAM [37, 5]. Minimizing the number of gate op-\nerations is a key goal in such work, and the position\nencoding used in QPOSTR can be regarded as one of\nthe “simple (but suboptimal)” encoding methods de-\nscribed in Fig 3 of [5]. The task they are interested in\nis encoding electronic spectra, but they also consider\nencoding “words” as an example toy problem. The extra\nstep that QPOSTR takes is explicitly to map register\nvalues to alphabetic characters, which enables such a\nunitary positional encoding to represent a text as a\nsequence of alphabetic characters.\nRather than demonstrating a new quantum advan-\ntage, the QPOSTR example is intended to showcase\nsome of the excitement, but also some of the gotchas\nof quantum computing. It is astonishing that an expo-\nnentially long string can be encoded like this at all, but\nonce the engineering caveats around that statement are\nproperly understood, we see that the explicit informa-\ntion we can recover from this representation is much\nsmaller.\n4 Word Embeddings and Text Classification\nRepresenting words as vectors of coordinates is a tech-\nnique that goes back at least to the 1960s and early\ninformation retrieval systems [74]. The key theoretical\nmotivation behind such distributional semantics meth-\nods is that words that appear in similar contexts tend to\nhave similar meanings [95, 31]. Based on their distribu-\ntion in text, embedding techniques map words to vector\nspaces, where their similarity is typically measured by\nthe inner product of their corresponding vectors.\nSemantic properties of vectors in lower-dimensional\nprojections were analyzed in the 1990s [53], and by\nthe early 2000s, overlaps between the logic of word\nvectors in information retrieval and state vectors in\nquantum mechanics had been explicitly recognized [85,\n90]. In the past decade, embeddings for classical NLP\nhave jumped from having a resurgence in academia to\nbecoming massively mainstream in industry [64, 18, 63].\nNaturally, this suggests embeddings could be just as\n6\nDominic Widdows et al.\ncentral for QNLP, especially since the mathematics of\nvectors and tensors has become a common language for\nboth AI and quantum computing [93].\nThere are many ways to add a quantum flavor to\nembeddings. In information retrieval, [80] used density\nmatrices and quantum probability to include term-term\ndependencies in retrieval weighting. Their quantum prob-\nability model for bigrams prefigures the more general\nprobabilistic models developed by [16].\nWord2ket [69] was introduced as a quantum-inspired\nsolution for compressing embeddings. A tensor network\nis a decomposition of a high-dimensional tensor into an\napproximate product of lower-dimensional tensors or\nvectors. For example, if M ≈U ⊗V then M can be\nrepresented using approximately dim(U)+dim(V ) coor-\ndinates, rather than dim(U) × dim(V ). (More precisely,\nin matrix coordinates, M ′ ≈uvT for column vectors u\nand v corresponding to appropriately reshaped U and\nV , and M ′ is a reshaped version of M; see [44, 84] for\ndetails.) Word2ket uses tensor networks to create low-\ndimensional approximations for individual word vectors,\nand entire vocabularies. This mathematical initiative\ncontinues: for example, [82] report using tensor networks\nto compress the parameters of an LLM (LlaMA-2 7B\nmodel) to 30% of its original size while retaining over\n90% of the original accuracy.\nThese methods are quantum-inspired, in the sense of\ndrawing deliberately on quantum mathematical models,\nbut running on classical computers.\n4.1 Building Quantum Embeddings\nNext we discuss techniques relating to embeddings in-\ntended for use on actual quantum devices, treating sep-\narately the topics of building quantum embeddings and\nusing them. The circuits proposed in this section are\nintended for NISQ-era rather than fault-tolerant devices.\nOne of the most well-known recent techniques for\nbuilding word embeddings is word2vec [64]. Taking in-\nspiration from this line of work, we propose a quantum\ncomputing implementation of word2vec.\nWord2vec is a group of word embedding methods\nthat use shallow neural networks to capture the seman-\ntic properties of words. Word2vec includes two popular\nmethods: Continuous Bag-of-Words (CBOW) and Skip-\ngram. CBOW and Skip-gram have different ways of\nlearning the word embeddings. CBOW takes the con-\ntext words around the target word as input and tries to\npredict the target word. Skip-gram does the opposite:\nit takes the target word as input and tries to predict\nthe context words. The output of both methods is a\nprobability distribution over all of the words in the vo-\ncabulary, which is computed using the softmax function.\nSWAP TEST\nn\nn\n|0⟩\nH\nH\n|x⟩\n|y⟩\nFig. 8 Swap test circuit, where the probability of measuring\na |0⟩in the top qubit is | ⟨x|y⟩| reflecting the overlap between\nthe |x⟩and |y⟩states [8].\nThis can be computationally expensive and impractical\nwhen the size of vocabulary is large.\nSkip-gram with Negative Sampling (SGNS) [65] is a\nvariant of Skip-gram that reduces computational com-\nplexity by simplifying the objective function. Instead\nof predicting the probability over complete vocabulary,\nSGNS only tries to distinguish the true context words\nfrom a few randomly sampled negative words, which are\nassumed to be irrelevant to the target word. Thus, the\nclassification problem is simplified from multi-class to\nbinary. The negative sampling procedure also effectively\nbalances the training dataset.\nIn our implementation, we use quantum states as\nword vectors, and use quantum fidelity to apply cosine\nsimilarity. That is, encoding two words as |x⟩and |y⟩,\nwe measure their similarity as |⟨x|y⟩|2 via the swap test\n[8], as shown in Figure 8.\nOne of the challenges of quantum word embedding\nis how to efficiently load words as quantum states. We\nconsider two potential schemes for embedding words to\nquantum state: memory-efficient embedding and circuit-\nefficient embedding.\nIn memory-efficient embedding, the quantum state\nof every word in vocabulary of size N is represented\nby a single unitary operation U(θ) ∈SU(2n), where\nn = ⌈log2 N⌉and θ is a set of learnable parameters. The\nm-qubit quantum state for the k-th word |wk⟩∈C⊗m is\nobtained from U(θ) by applying it to the computational\nbasis state |k⟩and discarding ancillary n −m qubits.\nThis scheme allows us to store a large number of\nwords in small number of qubits, which is exponentially\nefficient in memory usage. However, the resulting quan-\ntum circuit that has sufficient expressiveness to imple-\nment U(θ) has exponential depth, making it impractical\nfor circuit-based quantum computation. Moreover, the\nstate preparation process involves post-selection, and is\nthus non-deterministic due to the probabilistic nature\nof measurement.\nIn contrast, the second scheme, circuit-efficient em-\nbedding, represents the k-th word by a quantum state\nof the form |wk⟩:= U(θk) |0⟩∈C⊗m, where U(θk) ∈\nSU(2m) is a unitary operation parameterized by θk,\nQuantum Natural Language Processing\n7\nFig. 9 Circuit-efficient embedding\nFig. 10 Memory-efficient embedding\nwhich is specific to each word. This allows us to prepare\nthe quantum state using a depth-efficient circuit in a\ndeterministic process, without using excessive ancillary\nqubits. While it requires more classical memory to store\nthe parameters, it is more flexible since one can add or\nremove words from the vocabulary during training.\nThe circuit-efficient and memory-efficient patterns\nare depicted in Figures 9 and 10. In structure, the circuit-\nefficient pattern is like the word-embedding in word2ket\n[69], and the memory-efficient pattern is like the whole\nvocabulary encoding. For word2ket, the motivation for\nexpressing a whole vocabulary in a more entangled ten-\nsor network is to reduce classical memory, whereas in\nthis design, it makes more use of the efficient quantum\nmemory.\nTo learn the parameters for these embedding schemes,\nwe adapt the classical word2vec methods, CBOW, Skip-\ngram and SGNS, to the quantum setting. For quantum\nCBOW and Skip-gram, we introduce a parameterized\nunitary operator V (ϕ) ∈SU(2n) that defines the prob-\nability distribution pϕ(k|w) := |⟨k| V (ϕ)(|w⟩⊗|0⟩)|2,\nwhere w is either a pooled (e.g. averaged) embedding\nof context words for CBOW, or the embedding for the\ntarget word for Skip-gram. This removes the need of\ncomputing the costly softmax function, by using the\nnatural output of the quantum circuit to predict the\nindex among 2n computational basis states. We note\nthat quantum CBOW, cannot be directly applied for\nquantum word embeddings, since the direct averaging of\nquantum states is not a natural operation for quantum\ncomputers. Instead, it may be possible to use superposi-\ntions of quantum states.\nQuantum CBOW and Skip-gram inherit the difficul-\nties of the multi-class problem from the classical version.\nBoth also suffer from the training problem that does not\naffect the classical versions: barren plateaus [61]. The\nbarren plateau describes the phenomenon where the loss\nfunction and its gradient exponentially concentrate as\nthe number of qubits increases. The unitary V (ϕ) leads\nto a barren plateau in the loss landscape, making the\ntraining process difficult to scale.\nHence, we propose quantum SGNS, which leverages\na simplified structure to mitigate the scaling issue. Quan-\ntum SGNS uses the embeddings for two words directly,\ninstead of needing to additionally train V (ϕ). Given\nthe target word |w⟩, quantum SGNS tries to maximize\nthe likelihood p(v|w) := |⟨v|w⟩|2 if v is a context word\nand minimize it if v is negative sample. By combin-\ning quantum SGNS with the circuit-efficient embedding\nscheme, we enable their practical use on current quan-\ntum devices. Future work includes exploring the effects\nof different similarity kernels on quantum word2vec,\nextending these circuits to implement word2ket, and\nunderstanding algebra in the embedding space.\n4.2 Using Quantum Embeddings\nOnce words are encoded as vectors, these vectors can be\nused in many machine learning systems, including sup-\nport vector machines, which can be used for supervised\nclassification. This sometimes involves calculating a ker-\nnel function, which computes similarities between input\nvectors, sometimes involving computations that would\nbe intractable if all the coordinates were constructed\nand compared explicitly [34, Ch 5]. This is regarded\nas a promising research direction for quantum machine\nlearning, because quantum kernel circuits that compare\n2n coordinates or amplitudes can be implemented using\njust n qubits [77]. However, as with the QPOSTR string\nencoding example in Section 3, if the number of gates\nstill scales with the number of coordinates, the use of\na logarithmic number of qubits is a saving of space but\nwith no corresponding time advantage.\nSeveral quantum vector encodings or “feature maps”\nfor word embedding vectors were compared by [2], and\nused for sentiment analysis experiments. The ZZ-feature\nmap was found to be the most successful, achieving a\nclassification accuracy of 62% on classification experi-\nments involving small test sets of roughly 10K words\neach. This result showed initial promise, and was the\nlargest quantum text experiment reported to-date, but\nalso indicates how small today’s quantum NLP experi-\nments, are compared with even modest-sized classical\nNLP systems.\nOne additional use case for embeddings is in factual\ngrounding and retrieval, which we discuss in more detail\nin Section 8.\n8\nDominic Widdows et al.\n5 Sequential Models for Text Generation\nQuantum generative modeling is still a largely unex-\nplored area of opportunity, with many unsolved chal-\nlenges. In some cases, the data is too large and is parti-\ntioned into segments that are correlated but treated as\nindependent for sake of computation [45]. In others, the\nterm is used to describe settings in which a quantum\ncircuit is used as a discrete source of randomness within\nan otherwise classical neural network that memorizes a\nselect few data samples [73].\nIn NLP specifically, [16] describes how to model\nthe joint distribution p(X0, X1) of a given a set of bi-\ngrams (x0, x1) and compute the marginal distributions\np(X0) and p(X1) using linear algebra operations native\nto quantum computing (here we use capital letters to\ndenote random variables and lower-case to denote par-\nticular values that the random variable can take). [94]\nfollows this theory to implement a Quantum Circuit\nBorn Machine (QCBM) [11] to learn the joint distribu-\ntion p(X0, X1). While the QCBM can efficiently sample\npairs from p(X0, X1) or marginals p(X0) and p(X1),\ngenerating text sequentially from this model requires\nsampling from the conditional p(X1|X0 = x0). This re-\nquires discarding samples for which X0 ̸= x0, which can\nbecome prohibitive when scaling to larger vocabularies,\nand especially for rare prefix words x0. In addition, the\nbigram model forgoes a hidden state like those found in\nRecurrent Neural Networks (RNNs) that can learn to\nrepresent longer dependencies. Thus, while this model\ncan easily sample bigrams directly, it is not optimized\nfor the task of sampling longer sequences.\nA class of Bayesian network models, called n-gram\nmodels, have been successful in multiple language pro-\ncessing tasks, including information retrieval, text gen-\neration, and part-of-speech tagging [50]. However, they\nsuffer from poor performance and generalizability issues\nin sparse data regimes and fail to capture nonlocal syn-\ntactic relations. Handling out-of-vocabulary words or\nresolving ambiguity also pose challenges as n-grams do\nnot have built-in semantic understanding [12]. A Hid-\nden Markov models (HMM) is a Markov model whose\noutput from any given state is probabilistic rather than\ndeterministic, which hides the internal state. A canoni-\ncal example is when the hidden states are part-of-speech\ntags, such N(oun) or V(erb), which generate explicit\nwords with given probabilities [60, §9.2].\nThe feed forward and recurrent neural networks are\nspecific instances of the HMM. However, HMMs also face\nchallenges in estimating accurate probabilities when the\ncontext size increases or when the correlations get too\nlong in languages. Beyond language processing, HMMs\nhave been widely used in other scientific fields as well.\nIn [29], the authors discuss probabilistic models, particu-\nlarly HMMs and their derivatives, and their applications\nin biological sequence analysis. Language models, par-\nticularly those developed for aligning and comparing\nsequences, can be designed to recognize patterns in bio-\nlogical sequences, infer evolutionary relationships, and\nidentify functional elements.\nRecently, quantum techniques are being explored\nalong this direction due to their potential in capturing\nlong-range correlations. [33] introduced a quantum en-\nhanced version of the HMM, named the basis-enhanced\nBayesian Circuit, which leverages quantum contextual-\nity and non-locality to boost the expressivity of classical\nHMMs. They developed a minimal quantum extension\nof the bigram HMM, by incorporating measurements in\na Bayesian circuit in multiple bases. They demonstrated\nimproved performance of the quantum enhanced model\nin certain sequential datasets, including one containing\nDNA sequences with non-local structures. This leads\nto many interesting questions about the potential and\nutility of similar quantum methods in natural language\nprocessing tasks. Particularly, if quantum properties like\ncontextuality and non-locality still give a provable ad-\nvantage in terms of model expressivity when processing\nand extracting semantic meaning from a long sequence\nof words or protein structures.\nBuilding toward longer sequences, [51] train a quan-\ntum classifier for predicting the topic of sentences and\ndescribe a classical scheme for using the classifier to\nperform conditional generation (that can also be used\non classical classifiers). Since it does not actually train\na standalone quantum generative model, this method\nsuffers from a similar problem of also needing to discard\nmany samples from a base model in order to generate\none sample from an induced model, though the corre-\nlated editing-based annealing scheme could be guided to\nbe more efficient than independently sampled shots from\na QCBM. [9], shown in Figure 11, proposes a framework\nfor a quantum RNN that can be used to build an ac-\ntual generative model to autoregressively produce text.\nThis autoregressive modeling allows for dealing with\ncorrelations across segments of larger data, addressing\nthe problem from [45]. However, the architecture, while\nexpressive, is far too expensive for current hardware on\nnon-trivial problem sizes. [57] perform sequence clas-\nsification on actual hardware as shown in Figure 12.\nUnfortunately, the architecture they propose is not pow-\nerful enough to perform autoregressive modeling.\nWe explore how to bridge some of the gap between\ngeneral but expensive sequence processing of [9] and\nthe currently-achievable but underpowered architecture\nof [57]. To achieve this, we use the paradigm of [35] to\ncombine the power of nonlinear Multi-Layer Perceptrons\nQuantum Natural Language Processing\n9\nQUANTUM RNN (BAUSCH 2020)\nn\nm\nI/O |0⟩\nE(x0)\nN(θm)\nN(θo)\n|0⟩\nE(x1)\nN(θm)\nN(θo)\nHidden |0⟩\nN(θi)\nN(θh)\nN(θi)\nFig. 11 Bausch (2023) circuit. The input is the sequence (x0, x1). The mixing and hidden blocks are prohibitive for current\nhardware. The output is the next token in the sequence as the outcome from a single shot.\nQUANTUM SEQUENCE CLASSIFIER (LONDON 2023)\nn−1\nInput\n|0⟩\nE(x0)\nE(x1)\n|0⟩\nFig. 12 London et al. (2023) circuit. The input is the sequence (x0, x1). Only the first qubit is measured. The output is the\nprobability that the sequence is in class 1.\nSIMPLE QUANTUM RNN (THIS PAPER)\nn\nm\nI/O |0⟩\nE(x0)\nE(x1)\nN(θo)\nHidden |0⟩\nN(θi)\nN(θi)\nFig. 13 Proposed circuit. The input is the sequence (x0, x1). The output can be the next token in the sequence as the outcome\nfrom a single shot.\n(MLPs) with the inherent randomness of quantum com-\nputing to directly sample from rich classes of probability\ndistributions. Compared to [9], our proposed architec-\nture, shown in Figure 13, drops the use of the reset\noperation and uses identity mixing, similar to [57].\nIn the figures, the details of the quantum neurons\nhave been abstracted away to highlight the main simi-\nlarities and differences between the methods. The text\nlabeling on the left denotes the purpose of the regis-\nter. E(x) denotes a block of gates for encoding inputs\nx, N(θ) denotes a nonlinear neuron parameterized by\nvariables θ as in [21], and the dark gray gate with the\n|0⟩-ket is a reset operation. The parameter subscript\ni denotes input, o denotes output, h denotes hidden,\nand m denotes mixing. The light gray vertical rounded\nrectangle denotes that the qubits in the register are\nbeing used as control for the corresponding neuron as\na sequence of single-control gates, not as true multi-\ncontrol gates. We depict the architectures from [9] and\n[57] with a sequence length of 2 with input sequence\n(x0, x1), and the Multi-Layer Perceptron (MLP) from\n[35] with a single hidden layer.\nIn simulation, the model is trained using backprop-\nagation from gradients computed from noiseless state\nvector simulation. The model produces the probability\ndistribution over the 11 words in the vocabulary cor-\nresponding to the word that the model predicts comes\nnext after the observed sequence. In actual implementa-\ntion, the model would be trained using backpropagation\nfrom estimated gradients e.g. via the parameter-shift\nrule [78]. The model would produce for each shot a\nsample corresponding to the index of a single predicted\nword.\nOur proposed architecture is evaluated in a small-\nscale noiseless simulation. We consider a dataset of 7\nsentences using a vocabulary of 11 unique words. We\ncompare our proposal against two baseline models: one\nrandom uniform prediction model and one inspired by\n[57]. We compare performance between models trained\non 5 sentences by evaluating on the remaining 2 sen-\ntences their perplexity [49], for which a lower score\nindicates better performance. A naive uniform random\nprediction on this dataset yields a perplexity of 11. Using\na 9-qubit [57] model with 297 parameters, we achieve\na perplexity of 8.15. Using a 9-qubit model that we\npropose with 172 parameters, we achieve a perplexity\nof 2.79.\n10\nDominic Widdows et al.\nTo our knowledge, this is the first fully quantum\nsequential text generation architecture that is designed\nwith the capabilities and limitations of current NISQ-\nera devices in mind. Our simulation results demonstrate\nthe viability of the approach for implementation on\nactual hardware while achieving a reasonable level of\nperplexity.\n6 Attention in Quantum NLP Models\nSo far we have discussed models for studying sentences\nas word / token sequences. Making such models scale\nto longer sequences has always been a challenge: with\nn-gram models, the value of n has always been small\n[60, Ch 6]; and RNN architectures including LSTMs,\nwhile accurate for short sequences, had trouble scaling\nto cover long-range dependencies [34, Ch 15].\n6.1 Attention in Classical LLMs\nAttention is designed to address this problem. The atten-\ntion methodology was used to enhance an RNN sequence\nmodel for machine translation by [6], which enabled the\nmodel to capture longer-range relationships as well. Al-\nthough it still relied on the encoder-decoder paradigm,\nthe bidirectional RNN architecture introduced in [6]\nfeatures a distinct context vector for each word in the\nsentence. Each context vector depends on a sequence of\nannotations which contains information about the entire\nsentence with a strong focus on the parts of the sentence\nsurrounding the context vector’s associated input word.\nThe annotations are weighted according to an alignment\nmodel, which scores how well an output token matches\ninputs around a given position.\n[86] developed this approach further, demonstrating\na system where transformer blocks incorporating atten-\ntion, layer norm, multi-layer perceptrons, and residual\nconnections, fully replace recursive units. This Trans-\nformer model — centered around scaled dot-product\nattention — made previous RNN-based encoder-decoder\narchitectures obsolete when it demonstrated improved\nperformance on various translation tasks.\nImportantly, [86] adapted the Transformer architec-\nture for use in text generation. Their model is auto-\nregressive, and at each step it consumes the previously\ngenerated symbols as additional input when produc-\ning new text. In addition, it is worthy to note that the\nTransformer was later adapted to the setting of computer\nvision, where it outperformed state-of-the-art convolu-\ntional neural networks in various image classification\nchallenges [28].\nIn general, “an attention function can be described\nas mapping a query and a set of key-value pairs to an\noutput, where the query, keys, values, and outputs are\nall vectors” representing embedded tokens; the output\nis a weighted sum of the values, with the weights mea-\nsuring the compatibility between corresponding query\nand key [86]. Self-attention refers to computing atten-\ntion coefficients intra-sequence, i.e., on the same input\nsequence. A key feature of self-attention layers is that\nthey provide a mechanism for different tokens in the\ninput sequence to interact, thereby allowing models to\ninfer contextual information about individual tokens\nby weighing the importance of pairwise interactions; in\nother words, how much attention a given input token\nshould pay to every other token in the sequence.\nIn particular, the “Scaled Dot-Product” attention\nlayer featured in [86] computes the dot-products of the\nquery with all the keys, normalizes according to the\ndimension of the query and key vectors, and then applies\nthe softmax function to obtain the weights of all the\npairs, which are the values. Rather then enumerate all\nthe indices for summation, it is typical to write the lists\nof vectors as matrices, whereby the definition takes the\ncommon form\nAttention(Q, K, V ) = softmax\n\u0012QKT\n√\nd\n\u0013\nV,\nwhere d is the embedding dimension, Q, K, and V are\nmatrices of size wd where w is the number of words /\ntokens in a sequence, and the softmax function xi →\nexi/ P\ni(exi) is applied to each row. This formulates dot\nproduct attention as a matrix multiplication (O(w2d)), a\nsoftmax step (O(w2)), and a final matrix multiplication\n(O(wd2)).\nA key advantage enjoyed by the Transformer over\nthe previous RNN architectures is that this multiplica-\ntion can be parallelized, which computes the pairwise\nrelationships between all the tokens in a sequence at\nonce. In addition, the computational cost does not de-\npend on the distance between tokens in the sequence,\nas in previous models. Together, these properties ac-\ncounted for a drastic reduction in training time over\nsequential RNN models. The main drawback is that the\ncomputational complexity still scales quadratically in\nthe number of tokens in a given sequence (roughly the\nnumber of words in a sentence). The problem of approxi-\nmating or providing an alternative to self-attention with\nsubquadratic complexity spawned its own burgeoning\nresearch field [97, 70].\nQuantum Natural Language Processing\n11\n6.2 Near-term quantum self-attention mechanisms\nIn hopes of improving this quadratic scaling, and since\nattention layers have become so successful as key compo-\nnents in state-of-the-art models for NLP tasks, various\nquantum approaches have been suggested. This section\nfocuses on near-term quantum circuit designs.\n[55] claim to offer the first quantum self-attention\nimplementation, QSANN. By mapping encoded feature\nvectors into a high-dimensional Hilbert space using a\nquantum circuit, QSANN aims to extract correlations\nthat are intractable classically. For an illustration see\nFigure 14. First they construct an encoder circuit to\nload classical feature vectors onto an n-qubit quantum\nstate; they use one classical feature vector for each to-\nken in the input sequence. The number of qubits n is\na hyper-parameter that should be adjusted as relevant\nto available hardware. Next they apply parametrized\nquantum circuits, with identical gate layouts but dif-\nferent parameter values in order to compute the query,\nkey, and value vectors for each classical feature vector.\nThe circuit layout is illustrated in Figure 15. At this\nstage the query, key, and value vectors are encoded as\nquantum states, so measurements must be made in order\nto extract useful information; the resulting query and\nkey are the expectation values of the Pauli-Z operators\napplied to the first qubit of the resulting states, and\nthe value is a vector of expectation values of various\nPauli operators. Attention scores are then computed on\na classical device as a weighted average of the output\nvalues. Interestingly, [55] introduce a Gaussian kernel\nto compute the weights on the values vector; they claim\nthe Gaussian kernel can more easily correlate quantum\nstates with little overlap, which is needed, e.g., if two\ntokens are closely related in a sentence but their quan-\ntum state embeddings happen to be distant in the qubit\nstate space. The proposal of [55] still requires quadratic\nclassical computation, and its main source of quantum\nadvantage relies on using efficiently processing vectors\nin high-dimensional Hilbert space to unearth hidden\nrelationships between embedded tokens.\nThe work of [99] builds on these ideas, seeking quan-\ntum advantage in the same vein. By introducing various\nsets of ancilla qubits, the authors obviate the need to\nperform intermediate measurements during the atten-\ntion computation. (This could be thought of as a more\nsophisticated example of the ancilla readout qubits pat-\ntern used in the QPOSTR design of Section 2.) In this\nmodality, query, key, and value quantum state-vectors\nare computed by applying parametrized ansatze and\nswapping onto ancilla registers sequentially, as shown\nin Figure 16. Compatibility between query and keys\nis computed by a Quantum Logical Similarity (QLS)\nFig. 14 The Quantum Self-Attention Neural Network (QSANN)\narchitecture proposed in [55]. The network features various con-\nsecutive self-attention layers. At the (l −1)st layer, the classical\nfeature vectors y(l−1)\nk\nare encoded into a high-dimensional qubit\nstate space (circuits boxed in purple). The process is repeated\nthree times. Then parametrized ansatze, with gate layout as in\nFigure 15, representing the query, key, and value transformations\nare applied (circuits boxed in red). The resulting states are mea-\nsured and various expectation values are computed to produce\nthe classical query, key, and value vectors. These are sent to\na classical device for processing, where weights are computed\nusing a Gaussian kernel and the results are averaged to obtain\nfinal attention coefficients.\nFig. 15 Parametrized ansatz implementing the query, key, and\nvalue transformations in [55]’s QSANN.\nmodule, which is implemented as a sequence of Toffoli\nand CNOT gates, as shown in Figure 17. This is a key\nstep: it computes the overlap between query on keys\ndirectly on the quantum device, thereby improving on\n[55].\nWhile these two proposals address quantum self-\nattention mechanisms in QNLP directly, [22] proposes\none for use in Vision Transformers for image classi-\nfication, seeking quantum advantage in reducing the\ncomputational cost of the scaled dot-product attention\ncalculation. Concretely, the authors introduce so-called\northogonal layers to compute compatibility scores be-\ntween query and keys on the quantum hardware; these\nlayers efficiently implement parametrized transforma-\ntions on encoded feature vectors, as described in Figure\n18. The main novelty here is that [22] use the unary\nencoding circuit to encode token feature vectors into the\nHamming weight-1 subspace of the qubit state space.\nThis encoding is advantageous because their orthogonal\nlayers preserve the subspace, and they can be used to\n12\nDominic Widdows et al.\nFig. 16 The Quantum Self-Attention Network (QSAN) in-\ntroduced in [99]. This architecture uses ancilla qubits to hold\nintermediate results and proposes computing the attention coef-\nficients entirely on the quantum processor, obviating the need\nfor intermediate measurements. In addition, it features a slicing\noperation to reduce the number of measurements required.\nFig. 17 The Quantum Logic Similarity (QLS) module proposed\nin [99], implemented as a sequence of Toffoli and CNOT gates.\ncompute dot-products between query and keys in loga-\nrithmic time, assuming quantum gates can be applied in\nparallel. [22] report preliminary results from simulation,\nand with a 6-qubit quantum processor.\n6.3 Attention on Fault-Tolerant Quantum Computers\nThe role of attention in systems like GPT [19] has\nspurred more ambitious proposals, including the recent\npreprints of [56] and [40], which describe large-scale\nversions of full transformer-inspired processes on fault-\ntolerant error-correcting quantum computers. [40] use\nquantum signal processing and singular value transfor-\nmation techniques to apply a polynomial approximate\nFig. 18 Computational complexity of the dot-product com-\npatibility between query and keys using circuits with parallel\ntwo-qubit gates as proposed by [22]’s quantum Vision Trans-\nformer.\nsoftmax, whereas [56], like [99], replace softmax with a\nlinear quantum alternative.\nA main challenge for such proposals remains quan-\ntum I/O, which these papers acknowledge, but do not\nsolve explicitly. To extract all the weights from a ma-\ntrix stored in a quantum state would typically require\nmany more shots than there are weights. A smaller al-\nternative is to output token indices directly. A natural\nNISQ-inspired suggestion is to adapt the token index\ngeneration approaches from Section 5 to transformer\nprototypes. While adding a token-generation head may\nbe useful for circumventing I/O issues, the complexity\nand depth of the remaining bodies of the circuits in\nthese proposals puts practical implementations beyond\nthe NISQ era.\nA quantum decoder, for optimized search through\na much larger space of long proposed token sequences,\nis proposed by [10], which casts the problem as proba-\nbilistically branching tree-search, which is mathemati-\ncally equivalent to probabilistic grammar parsing. Even\nthough the hardware capabilities for such quantum oper-\nations are still some years away, this connects optimiza-\ntions in sequence generation from today’s softmaxed\nprobabilities, with traditional syntactic approaches to\nlanguage modeling, which we discuss next.\n7 Syntactic Parsing and Logical Forms\nThe use of general AI techniques such as RNNs and\nattention has fueled much recent success with NLP,\npartly because it has enabled much cross-fertilization\nbetween language and other kinds of data such as im-\nages, audio, and graphs. There are also more traditional\nNLP techniques, based on grammatical structures found\nparticularly in human language.\nNatural languages (and artificial programming lan-\nguages) express many structured relationships that go\nbeyond proximity in a sequence model. For example, in\nthe sentence “Kim kicked the ball into the goal from\nhalf-way down the pitch, right past the goalkeeper, and\nscored”, the grammatical structure of English enables\nus to infer easily that the person who scored is Kim, in\nQuantum Natural Language Processing\n13\nspite of there being 16 words (including 4 other nouns)\nin between the noun Kim and the verb scored. Language\ngrammar and syntax studies how these relationships\nare expressed and structured in different languages, and\nthis field powerfully influenced much of theoretical and\ncomputational linguistics during the second-half of the\n20th century, particularly through the work of [23, 24].\nIn such a framework, syntax is the central generative sys-\ntem, on which other aspects of language like phonology\nand semantics depend [48, Ch 5].\nDuring the 21st century, the reliance of NLP tech-\nniques on grammatical rules has declined. This is partly\ndue to the success of statistical and machine-learned\nmodels that share methods with other data-intensive\nareas of AI, and perhaps because the increasing pre-\nponderance of informal text created on smartphones\nimmediately contradicts any assumption that the input\nto an NLP system should consist of distinct grammati-\ncal sentences. In computational terms, best-performing\nparsing algorithms have included the CYK-parser which\nis worst-case O(n3), while the more general attention\nmechanism discussed in the previous section has a base-\nline O(n2) performance, which robustly enables more\nresources to be targeted at important long-range rela-\ntionships. Hence in most large-scale NLP systems today,\na grammatical parser is not an explicit component: and\nwith the challenge of reducing computational costs be-\nlow quadratic, there would need to be a strong reason\nfor requiring the extra burden of a cubic computing step\nfor any large-scale model. When grammatical parsing is\ndiscussed, it is often as a historical challenge that LLMs\ncan solve quite effectively, not a contemporary challenge\nfor building language models in the first place [66].\nHowever, the use of grammatical parsers has been\nre-introduced in parts of quantum NLP, motivated espe-\ncially by a mathematical correspondence between the\ncompositional rules of tensors and categorial grammars\ndemonstrated by [26]. This framework is implemented in\nthe lambeq system of [52] which is particularly designed\nfor quantum computers [58]. In this system, a grammat-\nical parser prepares a parse-tree from a sentence, which\nis structurally mapped to a tensor network which is\ncompiled into a quantum circuit: so the quantum circuit\nencodes the grammatical dependencies of the sentence,\nassuming its input data consists of uniquely-parsed sen-\ntences.\nThe parsing problem itself is also an interesting chal-\nlenge for quantum computing, for combinatoric reasons.\nComputationally, the problem can be phrased as tak-\ning a list of words as input, and returning a parse-tree,\nwhich is a data structure saying how the elements of the\nsentence are grouped together, and what grammatical\nrole they play. The two possible tree-structures for a\nS\nNP\nD\nthe\nN\nmusic\nVP\nV\nplayed\nS\nNP\nNPlural\npeople\nVP\nV\nplayed\nNP\nN\nmusic\nLeft-branching tree\nRight-branching tree\nFig. 19 The simplest nontrivial tree-parsing challenge is to\ndistinguish between the two distinct branching options for a\ntree with 3 leaf nodes. S = Sentence, N = Nouns, V = Verb, D\n= Determiner, P = Phrase.\nFig. 20 The five possible binary-branching trees over a 4-item\nsequence, each of which can be constructed by deleting unwanted\nbranches and nodes from the connected lattice-graph in the\ncenter.\n3-word sentence are shown in Figure 19. For the simple\ncase of a 3-word sentence, there are only 2 possible trees,\none for the structure [[A B] C] and one for the structure\n[A [B C]]. For a 4-word sentence, the five possible trees\nare are shown in Figure 20. The number of possible parse\ntrees for a sentence of length n grows exponentially and\nis given by the Catalan number Cn =\n1\nn+1\n\u00002n\nn\n\u0001\n.\nIdeally, a parser will find an exact parse that accounts\nfor the role of each constituent in the sentence, repre-\nsented by a single tree-structure. Grammatical rules\nsuch as “A sentence in English can be made of a noun\nphrase subject followed by a verb phrase” can be repre-\nsented as recipes like S →(NP, VP), and a probabilistic\nphrase-structure grammar (PCFG) may include many\nsuch rules, along with probabilities or weights learned\nfrom training data. Various methods for parsing exist,\nrelying on dynamic programming and probabilistic tech-\nniques [50, Ch 17]. Multiple parses are often possible,\nbecause (for example) prepositional phrases may attach\nin various locations. The goal is to find the parse tree\n14\nDominic Widdows et al.\nthat maximizes the overall probability of the parse, com-\nputed from the combined probability of all the rules\nused in derivation.\nThe depiction in Figure 20 emphasizes the parsing\nproblem as a combinatoric challenge. The search for a\nbest parse looks like a kind of minimal spanning tree\nproblem, which might be formulated as an Ising model,\namenable to quantum optimization [59], if the weights\nor costs for visiting each node could be derived from the\nPCFG.\nHowever, a key challenge with the parsing problem is\nthat we start with labels only for the leaf nodes, and un-\ntil there is some hypothesis for the which internal nodes\nrepresent which syntactic chunks, there are no estimates\nfor the weights on internal links. (This is part of the mo-\ntivation for the use of dynamic programming in classical\nparsers.) Combinatorically, the problem is not exactly to\nfind a minimum spanning tree, because internal nodes\nthat do not correspond to distinct grammatical phrases\nor constituents do not need to be visited. (Moreover, the\ninitial graph is directed: this precludes solutions where\nthe leaf nodes are all visited by a zigzag path along the\nbottom, because upward steps are forbidden.)\nHence, quantum parsing does not appear to fit one\nof the known techniques of quantum combinatorial opti-\nmization, but the problems are tantalizingly similar, and\nwe hope this framing of the challenge helps the language\nparsing problem to capture the interest of quantum\nresearchers.\nOne potential benefit of quantum parsing is that a\nquantum system may represent more nuanced proposals\nthan just returning a single best parse, or even a clas-\nsical mixture (estimated probability distribution over\ndifferent discrete parse proposals). Rather than insisting\nthat all language inputs must pass a parsing test before\nbeing processed, a syntactic parser that exchanges quan-\ntum information with semantic components in parallel\nmay become a different kind of asset in more parallel\narchitectures.\nIn another interesting combination of quantum math-\nematics and natural language syntax, it has been shown\nthat tensor product networks can encode grammatical\nstructure more effectively than LSTMs for generating\nimage captions [46]. Tensor product networks have also\nbeen used to construct an attention mechanism from\nwhich grammatical structure can be recovered by un-\nbinding role-filler tensor compositions [47].\n8 Facts and Language Generation\nThroughout its history, quantum theory has motivated\nnew insights on probability and randomness, how the\npotential and actual are related, and this has led to pro-\nposed models for various aspects of human behavior and\nconsciousness [20, 4]. Rapid advancements in customer-\nfacing AI systems, particularly conversational dialog\nsystems supported by large language models (LLMs),\nhave raised many questions about how such systems\nshould be built and used, what should be expected of\nthem, and about whether they exhibit conscious behav-\nior. This section discusses some of the concerns and work\nin this area, including how quantum theory addresses\nthe difference between hypothetical and actual reality.\nOne of the key complaints about current LLMs is\ntheir propensity to hallucinate, or produce sentences\nwith factually false information that still correctly ad-\nhere to grammatical rules. While this behavior is in line\nwith what generative models are statistically designed\nto do, in practice it goes against the public expectation\nof the AI agent as an all-knowing oracle, especially when\nmanifested as an interactive, question-answering chat\ninterface. Thus, several lines of research have emerged\nto address the usability issues caused by hallucination.\nIn this section, we first review some of these research\ndirections, and then take a step back to see how quan-\ntum computing relates to the philosophical issues that\narise in understanding and using LLMs.\nTwo prominent realms of research are chain-of-\nthought (CoT) prompting and retrieval augmented gen-\neration (RAG). CoT, while often used in the context\nof complex reasoning tasks [88], is believed to produce\nmore self-consistent results and indeed can be improved\nby explicitly encouraging self-consistency [87]. However,\nin the setting of factuality, errors early in the chain may\nyield incorrect results even with consistent reasoning\nlater in the chain, and the explanation of reasoning\nprovided by the chain may not be correct [83].\nFactual grounding via RAG may avoid some issues\nstill present in CoT, often at the cost of either additional\ntraining or larger number of inferences. RAG can be\nlargely classed into two paradigms, a priori and post-\nhoc, though there is not reason that these techniques\ncould not be combined or even performed iteratively\nin a loop. [54, 41] explore a priori RAG using learned\nembeddings stored in a database and accessed with a\nlearned neural retriever. [13] scale up the database and\nshrink the language model, matching SotA performance\nand illustrating that the world knowledge of a LLM is\nmore separable from its linguistic ability than previously\ndemonstrated. Together, this line of work suggests sepa-\nrability and knowledge base scaling as one possible path\nforward for utilizing word embeddings to reduce halluci-\nnation. While these a priori methods show promise, they\nmodify the generation pipeline and require additional\ntraining.\nQuantum Natural Language Processing\n15\nPost-hoc text editing methods [81, 7, 75] are seeing\ninterest for use with LLMs in part due to the resources\nrequired to even fine-tune modern LLMs, let alone pre-\ntrain them from scratch. For example, [32] uses the\nabilities of pre-trained language models and existing in-\nformation retrieval systems to edit and verify generated\ntext, but requires running several rounds of inference\non the base LLM. This trades off a zero fixed cost of\nusing a pre-trained model for a higher variable cost of\ninference.\nIn such paradigms, one natural place where quantum\ncomputing has potential to enhance performance is in\naccessing and designing the knowledge base. Grover’s\nalgorithm [39] yields a quadratic speedup over classi-\ncal methods for searching a database, and hence forms\nthe backbone of quantum information retrieval systems\n[38]. Going one step further, [71] propose using Grover’s\nsearch to inform the design of the database itself, and\nspecifically target vector databases as the intended ap-\nplication. However, they do not directly compare the\nquadratic speedup with the complexities achievable by\nusing efficient classical data structures, and a potential\ninteresting direction in this area is the question of how\n(if possible) to design a practical hybrid knowledge base\nthat combines the best of both classical and quantum\nprocessing.\nAlthough techniques such as retrieval-based and\nknowledge-based generation are a new area in the present-\nday context of fixing LLMs, methods such as that of\n[13] hearken back to an older class of designs where the\nfacts are stored in a knowledge base, and the language\nmodel is effectively a source of templates, not of facts.\nThe spotlight on language generation in the past few\nyears has refocused work on such methods, and how\nbest to combine them with LLMs [98]. For example, in\nFigure 21, a knowledge base is used to find the variable\nthat satisfies the question “When was J.S. Bach born?”\n(the answer being “1685”), and then a language model is\nused to express this as the sentence “J.S. Bach was born\nin the year 1685.” (A standard early use of such designs\nwas in mail merging, where a template for a message\nis combined with a list of different names to generate\npersonalized messages.) Such a language model can just\nas easily generate the sentence “J.S. Bach was born in\n1985”, not because it’s hallucinating, but because it’s\nworking correctly with a different knowledge base.\nMore generally, probabilistic language models are\ndesigned to note that Wednesday and Thursday are\nsimilar, and so having seen the phrase “Let’s meet on\nWednesday”, the model should judge the phrase “Let’s\nmeet on Thursday” to be similarly plausible. Saying\nthat such a probabilistic model “hallucinates” when it\ngenerates an untrue sentence reflects a fundamental mis-\nFig. 21 A traditional division of responsibilities between a\nknowledge base and a language model that cooperate in gener-\nating the answer “J.S. Bach was born in 1685.”\nunderstanding of probabilistic models. Sampling from a\nprobabilistic model is like rolling dice: if we previously\nobserve a 3 and the dice-roll gives a 4, the dice aren’t\nhallucinating a 4 instead of the “true” value of 3. The\nproblem lies in the assumption that plausible probabilis-\ntic samples of language should correspond to facts at\nall.\nThe assumption that language expertise and factual\nreliability should go together is easy to make, especially\nsince a significant amount of actual knowledge is con-\nveyed both explicitly and implicitly through writing. In\nthe phrase “Dave beat John”, we might ask “Which\nDave and which John?” before assessing its truthfulness:\nbut sometimes words take on fixed unique meanings in\nparticular situations, so that if someone says “Caesar\nbeat Pompey”, we automatically assume they mean two\nparticular people from the 1st century BC, and if they\nsaid “Pompey beat Caesar”, that would be considered\nuntrue. However, speaking strictly in the sense of lan-\nguage modelling, a model that also generates “Pompey\nbeat Caesar” sometimes as well as “Caesar beat Pom-\npey”, is arguably better, because it generates a more\ncomprehensive variety of perfectly fluent and plausible\nsentences.\nThe practice of generating text from just a language\nmodel was popularized by successful machine transla-\ntion systems [6]. With machine translation, it makes\nsense that the system is not responsible for factual ac-\ncuracy, because this is the user’s responsibility. In con-\ncrete terms, a correct translation of “J.S. Bach was\nborn in 1985” from English to German might be “J.S.\nBach wurde 1985 geboren”, not “Input error: prompt\ncontains factual inaccuracy.” Gradually models such as\nGPT demonstrated that a whole range of prompts, not\njust translation targets, could elicit plausible and flu-\nent responses [19]. The Chomskian program claimed\ngrammatical fluency as the heart of language decades\nago: today, we are seeing that this fluency is another\naspect of human behavior that computers can mimic\neffectively; and the ability to assemble erudite text has\nbecome one of the most impressively-solved parts of AI,\nsometimes leading to problems elsewhere.\n16\nDominic Widdows et al.\nQuantum theory intersects with these topics even\nmore fundamentally, by explicitly distinguishing the\npossible from the actual. A quantum circuit has many\npossible outcomes that could be observed, but only\none outcome is observed when measured: and this fixes\nthe hypothetical situation so that the same outcome\nis observed next time. A multiplicity of possibilities\ncan become a single fixed event [43]. Formal similarities\nbetween this process and language ambiguity were noted\nby [89], and the quantum economic theory of [68] is\nbased on the use of quantum information to model\nbeliefs about values, and classical information to model\namounts of money agreed in fixed transactions.\nThe problem of distinguishing things that might\nhappen from things that do happen was behind some of\nthe controversies of early classical mechanics. Leibniz\ndiscussed the notion of possible worlds, and maintained\nthat there must be a rational necessity behind (God’s)\nchoosing this world [72]. Newton’s belief in absolute\nspace implied a fixed zero-point or origin, and Leibniz\nargued that this implied that God must have made an\narbitrary choice without a necessary reason, which was\nunacceptable [14]. Such considerations of necessity vs.\ncontingency and their relationship to past, present, and\nfuture in time, go back at least to the famous sea-fight\ndiscussion in Aristotle’s De Interpretatione [62].\nThe notion that there are different possible worlds\nwhere a macroscopic event did or did not happen, that\none of those worlds is chosen based on a small local de-\ncision, and this possible world thus becomes the actual\nworld, was thrust into the limelight by quantum me-\nchanics. The implication of superposition and large-scale\nrandomness was troubling to Einstein (“God does not\nplay dice!”) and Schr¨odinger, whose famous paradoxical\ncat was designed to illustrate the absurdity of quantum\nmechanics in large-scale reality, where “the working of\nan organism requires exact physical laws” [76]. By con-\ntrast, Bohr and Heisenberg supported the Copenhagen\nInterpretation, where the wave-function represents real\npossibilities, and “the transition from the “possible” to\nthe “actual” takes place during the act of observation”\n[43, Ch 3].\nSome of the challenges inherent in large stochastic\nproblems, like weather forecasting, are thus philosophi-\ncally related to key questions of how one possible future\nis selected and becomes the past. Quantum mechanics\ndoes not completely answer this question, but it does\nbetter than classical mechanics, where the assumption\nof a deterministic universe avoids the problem. Heisen-\nberg’s analysis of where different uncertainties come\nfrom, and how we should think about them, has useful\ninsights including “This probability function represents\na mixture of two things, partly a fact and partly our\nknowledge of a fact” [43, Ch 3]. This does not tell us\nhow to fix language models, but it is a good reminder\nthat our ways of stating and communicating facts are\nentirely human. Practically, it helps to understand prob-\nabilistic language models as generators of hypothetical\nutterances, rather than factual statements, and the gen-\nerative nature of language models is precisely what en-\nables them to go smoothly from data they encountered\nto data they might just as well have encountered. In a\nsense, a large language generator is a kind of hypothesis-\ngenerator with the gift of the gab. Language models do\nthis task very well, but this should never have convinced\nus that a model will generate truthful language with-\nout an independent source of knowledge. With these\nconsiderations, quantum theory has some insight on po-\ntential solutions to improve language modeling systems,\nand at least guards against mistakes that arise from\nover-deploying hypothesis-generation systems without\nsuitable observation processes.\n9 Conclusion\nWe have taken a whirlwind tour of the state of quantum\nNLP, seeing the potential and limitations of using quan-\ntum computers for understanding language. While we\nrecognize this overview, like any other, cannot be fully\ncomprehensive, we hope that it is nonetheless useful for\nboth the theoretician and practitioner alike.\nWe reviewed fundamentals of gate-based quantum\ncomputing, and from here moved into understanding\nhow these low-level structures and concepts can be used\nto efficiently encode basic units of language, i.e. text.\nFrom there, we built into progressively higher-level con-\ncepts, roughly following the hierarchy found in classical\nNLP.\nThrough this journey, we have seen how the current\nscale of applications for quantum NLP on actual hard-\nware has not yet matched that of classical computing\ntechniques. However, quantum methods being developed\nat the small scale show promise for use on intermedi-\nate scale problems as hardware continues progressing,\nand quantum models that have been shown to be more\nexpressive than their analogous classical counterparts\nhold potential at large scales.\nIn the meantime, methods from quantum theory\ncontinue to inform AI. During the 2010s, vectors and\ntensors became a common mathematical toolset perme-\nating AI, and the adaptation of tensor network methods\nfor scalability continues this theme. We have especially\nfocused on the topical problems that current classical\nLLMs face. Here, quantum theory has much philosophi-\ncal guidance to offer on the issues of assessing factuality\nand sequential inference.\nQuantum Natural Language Processing\n17\nReferences\n1. Aaronson S (2015) Read the fine print. Nature Physics\n11(4):291–293\n2. Alexander A, Widdows D (2022) Quantum text encoding\nfor classification tasks. In: 2022 IEEE/ACM 7th Symposium\non Edge Computing, pp 355–361\n3. Amankwah MG, Camps D, Bethel EW, Van Beeumen\nR, Perciano T (2022) Quantum pixel representations and\ncompression for n-dimensional images. Scientific reports\n12(1):7712\n4. Atmanspacher H (2020) Quantum Approaches to Conscious-\nness. In: Zalta EN (ed) The Stanford Encyclopedia of Philos-\nophy, Summer 2020 edn, Metaphysics Research Lab, Stan-\nford University\n5. Babbush R, Gidney C, Berry DW, Wiebe N, McClean J,\nPaler A, Fowler A, Neven H (2018) Encoding electronic spec-\ntra in quantum circuits with linear t complexity. Physical\nReview X 8(4):041015\n6. Bahdanau D, Cho K, Bengio Y (2014) Neural machine\ntranslation by jointly learning to align and translate. arXiv\npreprint arXiv:14090473\n7. Balachandran V, Hajishirzi H, Cohen WW, Tsvetkov Y\n(2022) Correcting Diverse Factual Errors in Abstractive\nSummarization via Post-Editing and Language Model Infill-\ning. DOI 10.48550/arXiv.2210.12378, URL http://arxiv.\norg/abs/2210.12378, arXiv:2210.12378 [cs]\n8. Barenco A, Berthiaume A, Deutsch D, Ekert A, Jozsa R,\nMacchiavello C (1997) Stabilisation of Quantum Compu-\ntations by Symmetrisation. SIAM Journal on Computing\n26(5):1541–1557, DOI 10.1137/S0097539796302452\n9. Bausch J (2020) Recurrent Quantum Neural Networks.\nIn: Advances in Neural Information Processing Systems,\nCurran Associates, Inc., vol 33, pp 1368–1379, URL\nhttps://proceedings.neurips.cc/paper/2020/hash/\n0ec96be397dd6d3cf2fecb4a2d627c1c-Abstract.html\n10. Bausch J, Subramanian S, Piddock S (2021) A quantum\nsearch decoder for natural language processing. Quantum\nMachine Intelligence 3(1):16\n11. Benedetti M, Garcia-Pintos D, Perdomo O, Leyton-Ortega\nV, Nam Y, Perdomo-Ortiz A (2019) A generative model-\ning approach for benchmarking and training shallow quan-\ntum circuits. npj Quantum Information 5(1):1–9, DOI 10.\n1038/s41534-019-0157-8, URL https://www.nature.com/\narticles/s41534-019-0157-8, number: 1 Publisher: Na-\nture Publishing Group\n12. Bengio Y, Ducharme R, Vincent P (2000) A neural\nprobabilistic language model. In: Leen T, Dietterich\nT,\nTresp\nV\n(eds)\nAdvances\nin\nNeural\nInformation\nProcessing Systems, MIT Press, vol 13, URL https:\n//proceedings.neurips.cc/paper_files/paper/2000/\nfile/728f206c2a01bf572b5940d7d9a8fa4c-Paper.pdf\n13. Borgeaud S, Mensch A, Hoffmann J, Cai T, Rutherford E,\nMillican K, Driessche GBVD, Lespiau JB, Damoc B, Clark\nA, Casas DDL, Guy A, Menick J, Ring R, Hennigan T,\nHuang S, Maggiore L, Jones C, Cassirer A, Brock A, Pa-\nganini M, Irving G, Vinyals O, Osindero S, Simonyan K, Rae\nJ, Elsen E, Sifre L (2022) Improving Language Models by Re-\ntrieving from Trillions of Tokens. In: Proceedings of the 39th\nInternational Conference on Machine Learning, PMLR, pp\n2206–2240, URL https://proceedings.mlr.press/v162/\nborgeaud22a.html, iSSN: 2640-3498\n14. Bouquiaux L (2008) Leibniz against the unreasonable new-\ntonian physics. In: Leibniz: What Kind of Rationalist?,\nSpringer, pp 99–110\n15. Bowles J, Ahmed S, Schuld M (2024) Better than classical?\nthe subtle art of benchmarking quantum machine learning\nmodels. arXiv preprint arXiv:240307059\n16. Bradley TD (2020) At the interface of algebra and statistics.\nPhD thesis, City University of New York\n17. Bravyi S, Cross AW, Gambetta JM, Maslov D, Rall P, Yoder\nTJ (2024) High-threshold and low-overhead fault-tolerant\nquantum memory. Nature 627(8005):778–782\n18. Bridgwater A (2023) The Rise Of Vector Databases. URL\nhttps://www.forbes.com/sites/adrianbridgwater/\n2023/05/19/the-rise-of-vector-databases/,\nsection:\nCloud\n19. Brown T, Mann B, Ryder N, Subbiah M, Kaplan JD, Dhari-\nwal P, Neelakantan A, Shyam P, Sastry G, Askell A, Agar-\nwal S, Herbert-Voss A, Krueger G, Henighan T, Child R,\nRamesh A, Ziegler D, Wu J, Winter C, Hesse C, Chen M,\nSigler E, Litwin M, Gray S, Chess B, Clark J, Berner C,\nMcCandlish S, Radford A, Sutskever I, Amodei D (2020)\nLanguage models are few-shot learners. Advances in Neural\nInformation Processing Systems 33:1877–1901, appendices\nincluded at https://arxiv.org/abs/2005.14165\n20. Busemeyer JR, Bruza PD (2012) Quantum models of cog-\nnition and decision. Cambridge University Press,\n21. Cao\nY,\nGuerreschi\nGG,\nAspuru-Guzik\nA\n(2017)\nQuantum Neuron: an elementary building block for\nmachine\nlearning\non\nquantum\ncomputers.\nDOI\n10.\n48550/arXiv.1711.11240,\nURL\nhttp://arxiv.org/abs/\n1711.11240, arXiv:1711.11240 [quant-ph]\n22. Cherrat EA, Kerenidis I, Mathur N, Landman J, Strahm M,\nLi YY (2022) Quantum vision transformers. arXiv preprint\narXiv:220908167\n23. Chomsky N (1957) Syntactic structures. Mouton de Gruyter,\nThe Hague\n24. Chomsky N (1965) Aspects of the Theory of Syntax. 11,\nMIT press,\n25. Clark S, Pulman S (2007) Combining symbolic and distri-\nbutional models of meaning. In: AAAI Spring Symposium:\nQuantum Interaction, pp 52–55\n26. Coecke B, Sadrzadeh M, Clark S (2010) Mathematical foun-\ndations for a compositional distributional model of meaning.\nCoRR, arXiv preprint arXiV:10034394\n27. Coyle B, Mills D, Danos V, Kashefi E (2020) The Born\nsupremacy: quantum advantage and training of an Ising\nBorn machine. npj Quantum Information 6(1):1–11, DOI\n10.1038/s41534-020-00288-9, URL https://www.nature.\ncom/articles/s41534-020-00288-9, number: 1 Publisher:\nNature Publishing Group\n28. Dosovitskiy A, Beyer L, Kolesnikov A, Weissenborn D, Zhai\nX, Unterthiner T, Dehghani M, Minderer M, Heigold G,\nGelly S, et al. (2020) An image is worth 16x16 words: Trans-\nformers for image recognition at scale. ICLR / arXiv preprint\narXiv:201011929\n29. Durbin R, Eddy SR, Krogh A, Mitchison G (1998) Biological\nSequence Analysis: Probabilistic Models of Proteins and\nNucleic Acids. Cambridge University Press\n30. Feynman RP (1985) Quantum mechanical computers. Op-\ntics news 11(2):11–20\n31. Firth J (1957) A synopsis of linguistic theory 1930-1955.\nStudies in Linguistic Analysis, Philological Society, Oxford\n32. Gao L, Dai Z, Pasupat P, Chen A, Chaganty AT, Fan Y,\nZhao V, Lao N, Lee H, Juan DC, Guu K (2023) RARR: Re-\nsearching and Revising What Language Models Say, Using\nLanguage Models. In: Rogers A, Boyd-Graber J, Okazaki N\n(eds) Proceedings of the 61st Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long Papers),\nAssociation for Computational Linguistics, Toronto, Canada,\n18\nDominic Widdows et al.\npp 16477–16508, DOI 10.18653/v1/2023.acl-long.910, URL\nhttps://aclanthology.org/2023.acl-long.910\n33. Gao X, Anschuetz ER, Wang ST, Cirac JI, Lukin MD\n(2022) Enhancing generative models via quantum correla-\ntions. Physical Review X 12(2):021037\n34. G´eron A (2019) Hands-On Machine Learning with Scikit-\nLearn, Keras, and TensorFlow: Concepts, Tools, and Tech-\nniques to Build Intelligent Systems. O’Reilly Media,\n35. Gili K, Sveistrys M, Ballance C (2023) Introducing nonlin-\near activations into quantum generative models. Physical\nReview A 107(1):012406\n36. Gily´en A, Su Y, Low GH, Wiebe N (2019) Quantum singu-\nlar value transformation and beyond: exponential improve-\nments for quantum matrix arithmetics. In: Proceedings of\nthe 51st Annual ACM SIGACT Symposium on Theory of\nComputing, pp 193–204\n37. Giovannetti V, Lloyd S, Maccone L (2008) Quantum random\naccess memory. Physical review letters 100(16):160501\n38. Giri PR, Korepin VE (2017) A review on quantum search\nalgorithms. Quantum Information Processing 16(12):315,\nDOI 10.1007/s11128-017-1768-7, URL https://doi.org/\n10.1007/s11128-017-1768-7\n39. Grover LK (1996) A fast quantum mechanical algorithm\nfor database search. In: Proceedings of the twenty-eighth\nannual ACM symposium on Theory of computing - STOC\n’96, ACM Press, Philadelphia, Pennsylvania, United States,\npp 212–219, DOI 10.1145/237814.237866, URL http://\nportal.acm.org/citation.cfm?doid=237814.237866\n40. Guo N, Yu Z, Agrawal A, Rebentrost P (2024) Quantum\nlinear algebra is all you need for transformer architectures.\narXiv preprint arXiv:240216714\n41. Guu K, Lee K, Tung Z, Pasupat P, Chang M (2020)\nRetrieval Augmented Language Model Pre-Training. In:\nProceedings of the 37th International Conference on Ma-\nchine Learning, PMLR, pp 3929–3938, URL https://\nproceedings.mlr.press/v119/guu20a.html, iSSN: 2640-\n3498\n42. Harrow AW, Hassidim A, Lloyd S (2009) Quantum algo-\nrithm for linear systems of equations. Physical review letters\n103(15):150502\n43. Heisenberg W (1958) Physics and philosophy: The revolu-\ntion in modern science. Vladimir Djambov\n44. Hitchcock FL (1927) The Expression of a Tensor or a\nPolyadic as a Sum of Products. Journal of Mathematics\nand Physics 6(1–4):164–189, DOI 10.1002/sapm192761164\n45. Huang HL, Du Y, Gong M, Zhao Y, Wu Y, Wang C,\nLi S, Liang F, Lin J, Xu Y, Yang R, Liu T, Hsieh\nMH, Deng H, Rong H, Peng CZ, Lu CY, Chen YA,\nTao D, Zhu X, Pan JW (2021) Experimental Quan-\ntum Generative Adversarial Networks for Image Gener-\nation. Physical Review Applied 16(2):024051, DOI 10.1103/\nPhysRevApplied.16.024051, URL http://arxiv.org/abs/\n2010.06201, arXiv:2010.06201 [quant-ph]\n46. Huang Q, Smolensky P, He X, Deng L, Wu D (2017) Tensor\nproduct generation networks for deep NLP modeling. arXiv\npreprint arXiv:1709.09118\n47. Huang Q, Deng L, Wu D, Liu C, He X (2019) Attentive\ntensor product learning. In: Proceedings of the AAAI Con-\nference on Artificial Intelligence, vol 33, pp 1344–1351\n48. Jackendoff R (2002) Foundations of Language. Oxford Uni-\nversiry Press,\n49. Jelinek F, Mercer RL, Bahl LR, Baker JK (1977) Perplex-\nity—a measure of the difficulty of speech recognition tasks.\nThe Journal of the Acoustical Society of America 62(S1):S63,\nDOI 10.1121/1.2016299, URL https://doi.org/10.1121/\n1.2016299\n50. Jurafsky D, Martin JH (2023) Speech and Language Pro-\ncessing (3rd Edition draft). Stanford, California, URL\nhttps://web.stanford.edu/˜jurafsky/slp3/\n51. Karamlou A, Pfaffhauser M, Wootton J (2022) Quantum\nNatural Language Generation on Near-Term Devices. DOI\n10.48550/arXiv.2211.00727, URL http://arxiv.org/abs/\n2211.00727, arXiv:2211.00727 [quant-ph]\n52. Kartsaklis D, Fan I, Yeung R, Pearson A, Lorenz R, Toumi\nA, de Felice G, Meichanetzidis K, Clark S, Coecke B (2021)\nlambeq: An Efficient High-Level Python Library for Quan-\ntum NLP. arXiv preprint arXiv:211004236\n53. Landauer T, Dumais S (1997) A solution to Plato’s prob-\nlem: The latent semantic analysis theory of acquisition.\nPsychological Review 104(2):211–240\n54. Lewis P, Perez E, Piktus A, Petroni F, Karpukhin V,\nGoyal N, K¨uttler H, Lewis M, Yih Wt, Rockt¨aschel\nT,\nRiedel\nS,\nKiela\nD\n(2020)\nRetrieval-Augmented\nGeneration\nfor\nKnowledge-Intensive\nNLP\nTasks.\nIn:\nAdvances in Neural Information Processing Systems,\nCurran Associates, Inc., vol 33, pp 9459–9474, URL\nhttps://proceedings.neurips.cc/paper/2020/hash/\n6b493230205f780e1bc26945df7481e5-Abstract.html\n55. Li G, Zhao X, Wang X (2022) Quantum self-attention\nneural networks for text classification. arXiv preprint\narXiv:220505625\n56. Liao Y, Ferrie C (2024) Gpt on a quantum computer. arXiv\npreprint arXiv:240309418\n57. London C, Brown D, Xu W, Vatansever S, Langmead\nCJ, Kartsaklis D, Clark S, Meichanetzidis K (2023) Pep-\ntide Binding Classification on Quantum Computers. DOI\n10.48550/arXiv.2311.15696, URL http://arxiv.org/abs/\n2311.15696, arXiv:2311.15696 [quant-ph]\n58. Lorenz R, Pearson A, Meichanetzidis K, Kartsaklis D, Co-\necke B (2023) Qnlp in practice: Running compositional\nmodels of meaning on a quantum computer. Journal of\nArtificial Intelligence Research 76:1305–1342\n59. Lucas A (2014) Ising formulations of many np problems.\nFrontiers in physics 2:5\n60. Manning CD, Sch¨utze H (1999) Foundations of Statistical\nNatural Language Processing. The MIT Press, Cambridge,\nMassachusetts\n61. McClean JR, Boixo S, Smelyanskiy VN, Babbush R, Neven\nH (2018) Barren plateaus in quantum neural network train-\ning landscapes. Nature communications 9(1):4812\n62. McKeon R (ed) (1941) The Basic Works of Aristotle. Ran-\ndom House\n63. Metinko\nC\n(2023)\nPinecone\nHits\n$750M\nValua-\ntion\nAs\nAI\nHeats\nUp\nVector\nDatabase\nMarket.\nURL\nhttps://news.crunchbase.com/ai-robotics/\nstartup-venture-funding-database-pinecone/\n64. Mikolov T, Chen K, Corrado G, Dean J (2013) Efficient\nestimation of word representations in vector space. DOI 10.\n48550/ARXIV.1301.3781, URL https://arxiv.org/abs/\n1301.3781\n65. Mikolov T, Sutskever I, Chen K, Corrado GS, Dean J (2013)\nDistributed representations of words and phrases and their\ncompositionality. Advances in neural information processing\nsystems 26\n66. Min B, Ross H, Sulem E, Veyseh APB, Nguyen TH, Sainz\nO, Agirre E, Heintz I, Roth D (2023) Recent advances in\nnatural language processing via large pre-trained language\nmodels: A survey. ACM Computing Surveys 56(2):1–40\n67. Nielsen MA, Chuang I (2002) Quantum computation and\nquantum information. American Association of Physics\nTeachers, Cambridge University Press Edition, 2016\nQuantum Natural Language Processing\n19\n68. Orrell D (2020) Quantum Economics and Finance: An Ap-\nplied Mathematics Introduction. Panda Ohana Publishing,\nNew York\n69. Panahi A, Saeedi S, Arodz T (2019) word2ket: Space-\nefficient word embeddings inspired by quantum entangle-\nment. arXiv preprint arXiv:191104975\n70. Poli M, Massaroli S, Nguyen E, Fu DY, Dao T, Bac-\ncus S, Bengio Y, Ermon S, Re C (2023) Hyena hierar-\nchy: Towards larger convolutional language models. In-\nternational Conference on Machine Learning URL https:\n//openreview.net/forum?id=1sxiBaGEtg\n71. Pronin CB, Ostroukh AV (2023) Synthesis of Quantum\nVector Databases Based on Grovers Algorithm. DOI\n10.48550/arXiv.2306.15295, URL http://arxiv.org/abs/\n2306.15295, arXiv:2306.15295 [quant-ph]\n72. Rescher N (1996) Leibniz on possible worlds. Studia Leibni-\ntiana pp 129–162\n73. Rudolph MS, Toussaint NB, Katabarwa A, Johri S, Per-\nopadre B, Perdomo-Ortiz A (2022) Generation of High-\nResolution Handwritten Digits with an Ion-Trap Quantum\nComputer. Physical Review X 12(3):031010, DOI 10.1103/\nPhysRevX.12.031010, URL https://link.aps.org/doi/\n10.1103/PhysRevX.12.031010, publisher: American Physi-\ncal Society\n74. Salton G, McGill M (1983) Introduction to modern infor-\nmation retrieval. McGraw-Hill, New York, NY\n75. Schick T, Dwivedi-Yu J, Jiang Z, Petroni F, Lewis P,\nIzacard G, You Q, Nalmpantis C, Grave E, Riedel S\n(2022) PEER: A Collaborative Language Model. DOI\n10.48550/arXiv.2208.11663, URL http://arxiv.org/abs/\n2208.11663, arXiv:2208.11663 [cs]\n76. Schr¨odinger E (1944) What is life? Cambridge University\nPress (with mind and matter and autobiographical sketches,\n1996)\n77. Schuld M, Petruccione F (2021) Machine Learning with\nQuantum Computers. Springer\n78. Schuld M, Bergholm V, Gogolin C, Izaac J, Killoran N\n(2019) Evaluating analytic gradients on quantum hard-\nware. Physical Review A 99(3):032331, DOI 10.1103/\nPhysRevA.99.032331, URL https://link.aps.org/doi/\n10.1103/PhysRevA.99.032331, publisher: American Physi-\ncal Society\n79. Shor PW (1994) Algorithms for quantum computation:\ndiscrete logarithms and factoring. In: Proceedings 35th\nannual symposium on foundations of computer science, Ieee,\npp 124–134\n80. Sordoni A, Nie JY, Bengio Y (2013) Modeling Term De-\npendencies with Quantum Language Models for IR. In:\nProceedings of the 36th International ACM SIGIR Confer-\nence on Research and Development in Information Retrieval,\nSIGIR ’13, pp 653–662\n81. Thorne J, Vlachos A (2021) Evidence-based Factual Error\nCorrection. In: Zong C, Xia F, Li W, Navigli R (eds) Pro-\nceedings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th International\nJoint Conference on Natural Language Processing (Volume\n1: Long Papers), Association for Computational Linguistics,\nOnline, pp 3298–3309, DOI 10.18653/v1/2021.acl-long.256,\nURL https://aclanthology.org/2021.acl-long.256\n82. Tomut A, Jahromi SS, Singh S, Ishtiaq F, Mu˜noz C, Bajaj\nPS, Elborady A, del Bimbo G, Alizadeh M, Montero D,\net al. (2024) CompactifAI: Extreme compression of large\nlanguage models using quantum-inspired tensor networks.\narXiv preprint arXiv:240114109\n83. Turpin M, Michael J, Perez E, Bowman SR (2023) Lan-\nguage Models Don’t Always Say What They Think: Un-\nfaithful Explanations in Chain-of-Thought Prompting. DOI\n10.48550/arXiv.2305.04388, URL http://arxiv.org/abs/\n2305.04388, arXiv:2305.04388 [cs]\n84. Van Loan CF (2000) The ubiquitous Kronecker prod-\nuct. Journal of Computational and Applied Mathematics\n123(1):85–100, DOI 10.1016/S0377-0427(00)00393-9\n85. Van Rijsbergen CJ (2004) The Geometry of Information\nRetrieval. Cambridge University Press,\n86. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L,\nGomez AN, Kaiser  L, Polosukhin I (2017) Attention is all\nyou need. In: Advances in neural information processing\nsystems, pp 5998–6008\n87. Wang X, Wei J, Schuurmans D, Le Q, Chi E, Narang S,\nChowdhery A, Zhou D (2023) Self-Consistency Improves\nChain of Thought Reasoning in Language Models. DOI\n10.48550/arXiv.2203.11171, URL http://arxiv.org/abs/\n2203.11171, arXiv:2203.11171 [cs]\n88. Wei J, Wang X, Schuurmans D, Bosma M, Ichter\nB, Xia F, Chi E, Le QV, Zhou D (2022) Chain-of-\nThought Prompting Elicits Reasoning in Large Language\nModels. Advances in Neural Information Processing Sys-\ntems\n35:24824–24837,\nURL\nhttps://proceedings.\nneurips.cc/paper_files/paper/2022/hash/\n9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.\nhtml\n89. Widdows D (2003) A mathematical model for context\nand word-meaning. In: Fourth International and Interdisci-\nplinary Conference on Modeling and Using Context, Stan-\nford, California\n90. Widdows D (2004) Geometry and meaning. CSLI Publica-\ntions, Stanford\n91. Widdows D, Bhattacharyya A (2024) Quantum financial\nmodeling on noisy intermediate-scale quantum hardware:\nRandom walks using approximate quantum counting. Quan-\ntum Economics and Finance\n92. Widdows D, Peters S (2003) Word vectors and quantum\nlogic. In: Proceedings of the Eighth Mathematics of Lan-\nguage Conference, Bloomington, Indiana\n93. Widdows D, Kitto K, Cohen T (2021) Quantum mathemat-\nics in artificial intelligence. Journal of Artificial Intelligence\nResearch 72:1307–1341\n94. Widdows D, Alexander A, Zhu D, Zimmerman C, Ma-\njumder A (2024) Near-term advances in quantum natural\nlanguage processing. Annals of Mathematics and Artificial\nIntelligence pp 1–24\n95. Wittgenstein L (1953) Philospphical Investigations. Black-\nwell, Blackwell, 3rd edition, 2001\n96. Yu\nZ,\nChen\nQ,\nJiao\nY,\nLi\nY,\nLu\nX,\nWang\nX,\nYang JZ (2023) Provable Advantage of Parameterized\nQuantum Circuit in Function Approximation. DOI\n10.48550/arXiv.2310.07528, URL http://arxiv.org/abs/\n2310.07528, arXiv:2310.07528 [quant-ph]\n97. Zaheer M, Guruganesh G, Dubey KA, Ainslie J, Alberti C,\nOntanon S, Pham P, Ravula A, Wang Q, Yang L, Ahmed\nA (2020) Big Bird: Transformers for Longer Sequences.\nIn: Advances in Neural Information Processing Systems,\nCurran Associates, Inc., vol 33, p 17283–17297, URL https:\n//papers.nips.cc/paper_files/paper/2020/hash/\nc8512d142a2d849725f31a9a7a361ab9-Abstract.html\n98. Zhang H, Song H, Li S, Zhou M, Song D (2023) A survey\nof controllable text generation using transformer-based pre-\ntrained language models. ACM Computing Surveys 56(3):1–\n37\n99. Zhao Rx, Shi J, Zhang S (2022) QSAN: A near-term\nachievable quantum self-attention network. arXiv preprint\narXiv:220707563\n",
  "categories": [
    "quant-ph",
    "cs.AI",
    "cs.CL"
  ],
  "published": "2024-03-28",
  "updated": "2024-04-26"
}