{
  "id": "http://arxiv.org/abs/2404.01794v1",
  "title": "Imitation Game: A Model-based and Imitation Learning Deep Reinforcement Learning Hybrid",
  "authors": [
    "Eric MSP Veith",
    "Torben Logemann",
    "Aleksandr Berezin",
    "Arlena Wellßow",
    "Stephan Balduin"
  ],
  "abstract": "Autonomous and learning systems based on Deep Reinforcement Learning have\nfirmly established themselves as a foundation for approaches to creating\nresilient and efficient Cyber-Physical Energy Systems. However, most current\napproaches suffer from two distinct problems: Modern model-free algorithms such\nas Soft Actor Critic need a high number of samples to learn a meaningful\npolicy, as well as a fallback to ward against concept drifts (e. g.,\ncatastrophic forgetting). In this paper, we present the work in progress\ntowards a hybrid agent architecture that combines model-based Deep\nReinforcement Learning with imitation learning to overcome both problems.",
  "text": "Imitation Game: A Model-based and Imitation\nLearning Deep Reinforcement Learning Hybrid\nEric MSP Veith1,2\nTorben Logemann1\nAleksandr Berezin2\nArlena Wellßow1,2\nStephan Balduin2\n1Carl von Ossietzky University Oldenburg\nResearch Group Adversarial Resilience Learning\nOldenburg, Germany\nEmail: firstname.lastname@uol.de\n2OFFIS – Institute for Information Technology\nR&D Division Energy\nOldenburg, Germany\nEmail: firstname.lastname@offis.de\nAbstract—Autonomous and learning systems based on Deep\nReinforcement Learning have firmly established themselves as a\nfoundation for approaches to creating resilient and efficient Cyber-\nPhysical Energy Systems. However, most current approaches\nsuffer from two distinct problems: Modern model-free algorithms\nsuch as Soft Actor Critic need a high number of samples to\nlearn a meaningful policy, as well as a fallback to ward against\nconcept drifts (e. g., catastrophic forgetting). In this paper, we\npresent the work in progress towards a hybrid agent architecture\nthat combines model-based Deep Reinforcement Learning with\nimitation learning to overcome both problems.\nI. INTRODUCTION\nEfficient operation of Critical National Infrastructure (CNI) is\na global, immediate need. The report of the Intergovernmental\nPanel on Climate Change (IPCC) of the United Nations (UN)\nindicates that greenhouse gas emissions must be halved by 2023\nin order to constrain global warming to 1.5 ◦C. Approaches\nbased on Machine Learning (ML), agent systems, and learning\nagents — i. e., such based on Deep Reinforcement Learning\n(DRL) — have firmly established themselves in providing\nnumerous aspects of efficiency increase as well as resilient\noperation. From the hallmark paper that introduced DRL\n[1] to the development of MuZero [2] and AlphaStar [3],\nlearning agents research has inspired many applications in the\nenergy domain, including real power management [4], reactive\npower management and voltage control [5], [6], black start [7],\nanomaly detection [8], or analysis of potential attack vectors\n[9], [10].\nA promising, and also the simplest form to employ DRL-\nbased agents is to make use of model-free algorithms, such as\nProximal Policy Gradient (PPO), Twin-Delayed DDPG (TD3),\nor Soft Actor Critic (SAC). Recent works show applications\nin, e. g., voltage control, real power management in order to\nincrease the share of Distributed Energy Resources (DERs), or\nfrequency regulation [11].\nHowever, model-free DRL approaches suffer from prominent\nproblems that hinder their wide-scale rollout in power grids: low\nsample efficiency and the potential for catastrophic forgetting.\nThe first problem describes the way agents are trained. In\nDRL, the agent learns from interactions with its environment,\nreceiving an indication of the success of its actions through\nthe reward signal that serves as the agent’s utility function.\nAlthough modern model-free DRL algorithms show that they\nnot only learn successful strategies but can also react very\nwell to situations they did not encounter during training, they\nrequire several thousand steps to learn such a policy. As DRL\nagents need interaction with their environment (a simulated\npower grid in our case), making the training of DRL agents\ncomputationally expensive. Thus, the efficiency of one sample,\ni. e., the quadruplet of state, action taken in the state, reward\nreceived, and follow-up state, (s, a, r, s′), is low.\nThe second problem, catastrophic forgetting, describes that\nagents can act suboptimal, erroneous even, or seem to “forget”\nestablished and well-working strategies when introduced to\na change in marginal distributions [12], [13]. The dominant\nliterature concentrates on robotics and disjoint task sets; most\napproaches cater to this specific notion of specific tasks.\nHowever, in the power grid domain, there is no disjoint task\nset; instead, changes occur in the environment, which can start\ncatastrophic forgetting in a subtle way.\nA particular way to address the first problem is the notion\nof model-based DRL. Here, the agent incorporates a model of\nthe world, which helps to not only gauge the effectiveness\nof an action but can also be queried internally to learn\nfrom [14]. Potential models for such an approach can also\nbe surrogate models [15]. However, there currently is no\narchitecture that also addresses the problem of catastrophic\nforgetting. Therefore, a clear research gap exists in constructing\na DRL agent approach that can make use of current advances\n(i. e., specifically, train in an efficient manner), but is still\nreliable for usage in power grids. Although it has been noted\nthat a combination of model-free DRL and a controller would\nprovide important benefits [16], a recent survey notes that\ncurrently, only very limited work exists in this regard [11];\ncurrent approaches require a tight integration of the existing\ncontroller logic with the DRL algorithm [17]. Aiding the agent’s\nlearning by also adding imitation learning is currently not done.\nHowever, current research seems to indicate that this is, in\nfact, necessary, and that agents able to cope with various\ndistributional shifts must have learned a causal model of the\ndata generator [18].\nIn this paper, we present such an agent approach. We describe\nthe work in progress towards a hybrid agent architecture that\narXiv:2404.01794v1  [cs.AI]  2 Apr 2024\nmakes use of a world model and can also efficiently learn from\nan existing policy, so-called imitation learning. Moreover, we\nuse the existing, potentially non-optimal control strategy as a\nsafety fallback to guarantee a certain behavior. We introduce\nthe notion of a discriminator that is able to gauge between\napplying the DRL policy and the fallback policy. We showcase\nour approach with a case study in a voltage control setting.\nThe rest of this paper is structured as follows: In Section II,\nwe give a survey of the relevant related work. We introduce our\nhybrid agent and discriminator architecture in Section III. We\nthen describe our case study and obtained results in Section IV.\nWe follow with a discussion of the results and implications\nin Section V. We conclude in Section VI and give an outlook\ntowards future results and development of this work in progress.\nII. RELATED WORK\nA. Deep Reinforcement Learning\nDRL is based on the Markov Decision Process (MDP), which\nis a quintuplet (S, A, T, R, γ): S denotes the set of states\n(e. g., voltage magnitudes: St = {V1(t), V2(t), . . . , Vn(t)});\nA is the set of actions (e. g., the reactive power genera-\ntion or consumption of a node the agent controls: At =\n{q1(t), q2(t), . . . , qn(t)}); T are the conditional transition\nprobabilities between any two states; R is the reward function\nof the agent R : S ×A →R; and γ, the discount factor, which\nis a hyperparameter designating how much future rewards will\nbe considered in calculating the absolute Gain of an episode.\nEssentially, an agent observes a state st at the time t,\nperforms an action at, and receives a reward rt. Transition to\nthe following state st+1 can be deterministic or probabilistic,\ndepending on T. The Markov property states that for each state\nst with an action at, only the previous state st−1 is relevant for\nthe evaluation of the transition. To capture a multi-level context\nand maintain the Markov property, st is usually enriched with\ninformation from previous states or the relevant context.\nThe goal of reinforcement learning is generally to learn a\npolicy in which at ∼πθ(·|st). The search for the optimal policy\nπ∗\nθ is the optimization problem on which all reinforcement\nlearning algorithms are based.\nMnih et al. [1] proposed DRL with their Deep Q Networks\n(DQNs). The reinforcement learning itself precedes this publi-\ncation [19]. However, Mnih et al. were able to introduce deep\nneural networks as estimators for Q-values that enable robust\ntraining. Their end-to-end learning approach is still one of the\nstandard benchmarks in DRL. The DQN approach has seen\nextensions until the Rainbow DQN [20] and newer work covers\nDQN approaches connected to behavior cloning [21]. Through,\nDQN only applies to environments with discrete actions.\nDeep Deterministic Policy Gradient (DDPG) [22] also builds\non the policy gradient methodology: It concurrently learns a\nQ-function and policy. It is an off-policy algorithm that uses\nthe Q-function estimator to train the policy. DDPG allows for\ncontinuous control; it can be seen as DQN for continuous\naction spaces. DDPG suffers from overestimating Q-values\nover time; TD3 has been introduced to fix this behavior [23].\nPPO [24] is an on-policy policy gradient algorithm that\ncan be used for discrete and continuous action spaces. It is\na development parallel to DDPG and TD3, not an immediate\nsuccessor. PPO is more robust towards hyperparameter settings\nthan DDPG and TD3 are. Still, as an on-policy algorithm, it\nrequires more interaction with the environment train, making\nit unsuitable for computationally expensive simulations.\nSAC, having been published close to concurrently with TD3,\ntargets the exploration-exploitation dilemma by being based\non entropy regularization [25]. It is an off-policy algorithm\noriginally focused on continuous action spaces but has been\nextended to support discrete action spaces as well. There also\nare approaches for distributed SAC [26].\nPPO, TD3, and SAC are the most commonly used model-free\nDRL algorithms today. Of these, off-policy learning algorithms\nare naturally suited for behavior cloning and imitation learning,\nas on-policy algorithms, by their nature, need data generated\nby the current policy to train.\nLearning from existing data without interaction with an\nenvironment is called offline reinforcement learning [27]. This\napproach can be employed to learn from existing experiences.\nThe core of reinforcement learning is the interaction with the\nenvironment. Only when the agent explores the environment,\ncreating trajectories and receiving rewards, can it optimize\nits policy. However, more realistic environments, like robotics\nor the simulation of large power grids, are computationally\nexpensive. Training from already existing data would be\nbeneficial. For example, an agent could learn from an existing\nsimulation run for optimal voltage control before being trained\nto tackle more complex scenarios.\nThe field of offline reinforcement learning can roughly\nbe subdivided into policy constraints, importance sampling,\nregularization, model-based offline reinforcement learning, one-\nstep learning, imitation learning, and trajectory optimization.\nLevine et al. [28]1 and Prudencio et al. [27] have published\nextensive tutorial and review papers, to which we refer the\ninterested reader instead of providing a poor replication of\ntheir work here. Instead, we will give only a concise overview\nconsidering the ones relevant for this work.\nOf the mentioned methods, imitation learning is especially\nof interest. It means the agent can observe the actions of\nanother agent (e.g., an existing controller) and learn to imitate\nit. Imitation learning aims to reduce the distance between the\npolicy out of the dataset D and the agent’s policy, such that the\noptimization goal is expressed by J(θ) = D(πβ(·|s), πθ(·|s)).\nThis so-called Behavior Cloning requires an expert behavior\npolicy, which can be hard to come by but is readily available\nin some power-grid-related use cases, such as voltage control,\nwhere a simple voltage controller could be queried as the\nexpert.\n1The tutorial by Levine et al. [28] is available only as a preprint. However,\nto our knowledge, it constitutes one of the best introductory seminal works so\nfar. Since it is a tutorial/survey and not original research, we cite it despite its\nnature as a preprint and present it alongside the peer-reviewed publication by\nPrudencio et al. [27], which cites the former, too.\nB. Model-free and Model-based Deep Reinforcement Learning\nfor Power Grids\nAs of today, a large corpus of works exists that consider\nthe application of DRL to power grid topics. As we have to\nconstrain ourselves to works relevant to this paper, we refer\nthe interested reader to current survey papers, such as the one\ncreated by Chen et al. [11] for model-free and Luo et al. [29]\nfor model-based learning, for a more complete overview.\nOne of the dominant topics for applying DRL in power\ngrids is voltage and reactive power control. As voltage control\ntasks dominate distribution systems, steady-state simulations\ncan be employed, making the connection to DRL frameworks\ncomparatively easy and helping maintain the Markov property.\nAlso, the design of the reward function is easy, allowing a\nconcise notation such as R = −P\ni(Vi −1).\nTo capture the underlying physical properties of the power\ngrid better, Lee et al. [30] utilize Graph Convolutional Neural\nNetworks (GCNNs). Other recent publications employ DDPG\nfor reactive power control, adding stability guarantees to the\nactor-network or training [31], [32]. Model-based approaches\ncan either incorporate offline learning from known (mis-) use\ncases [9] or use surrogate models [15]. Gao et al. [33] use SAC\nand model augmenting for safe volt-VAR control in distribution\ngrids.\nOther applications include solutions to the optimal power\nflow problem [34], [35], power dispatch [36], [37], or Electric\nVehicle (EV) charging [38]. Management of real power also\nplays a role in voltage control in distribution networks since\nconditions for reactive-reactive power decoupling are no longer\nmet due to comparable magnitudes of line resistance and\nreactance.\nIII. HYBRID AGENT ARCHITECTURE AND DISCRIMINATOR\nOur approach incorporates two policies, a world model, as\nwell as a discriminator that chooses which policy to follow. It is\npart of the Adversarial Resilience Learning agent architecture\n[39], [40].\nWhen an agent receives new sensor readings from the\nenvironment, usually, a policy proposes the setpoints for\nactuators, which are subsequently applied. In our case, we query\ntwo policies in parallel. The adaptive policy is based on SAC,\nthe deterministic rules policy incorporates a simple voltage\ncontroller. The voltage controller is based on the following\nformula:\nq(t + 1) = [q(t) −D(V (t) −1)]+ ,\n(1)\nwhere the notation [·]+ denotes a projection of invalid values\nto the range [qg, qg], i. e., to the feasible range of setpoints for\nq(t + 1) of each inverter. D is a diagonal matrix of step sizes.\nAs both policies return a proposal for setpoints to apply,\nthe discriminator has to choose the better approach. It does so\nby querying its world model. In the simplest case, the world\nmodel is an actual model of the power grid. However, we note\nthat the power grid model will not capture dynamics such as\nthe behavior of other nodes or constraints stemming from grid\ncodes (e. g., line loads or voltage gradients).\nThe quality of each decision proposal is quantified through\nthe agent’s internal utility function (reward). Our reward\nfunction consists of three elements,\n1) The world state as voltage levels of all buses: m(t)\n|V |;\n2) The buses observed by the agent, which is subset of\nthe world state to account for partial observability:\nΨ\n\u0010\nm(t)\n|V |\n\u0011\n⊆m(t)\n|V |;\n3) the number of nodes controlled by agents that are still\nin service (i. e., unaffected by grid code violations),\nP\nb\nr\nΨΩ\n\u0010\nm(t)\n|V |\n\u0011z\nb. Note that J·K are Iverson brackets.\nTo express the importance of the voltage band of 0.90 ≥\nV ≥1.10 pu, we map the voltage magnitude to a scalar by\nutilizing a specifically shaped function borrowed from the\nGaussian Probability Density Function (PDF):\nG(x, A, µ, C, σ) = A\n|x| ·\nX\nx\nexp\n \n−(x −µ)2\n2σ2\n−C\n!\n(2)\nGΩ(x) = G(x, µ = 1.0, σ = 0.032, C = 0.0, A = 1.0)\n(3)\nWeights can control each part’s influence. We set α = β =\nγ = 1\n3. We construct the agent’s performance function to be:\nPΩ\n\u0010\nm(t)\u0011\n= α · GΩ\n\u0010\nx = m(t)\n|V |\n\u0011\n+ β · GΩ\n\u0010\nx = ΨΩ\n\u0010\nm(t)\n|V |\n\u0011\u0011\n+ γ ·\n(X\nb\nr\nΨΩ\n\u0010\nm(t)\n|V |\n\u0011z\nb\n) (\f\f\fm(t)\n|V |\n\f\f\f\nX\nb\nd−1\n)−1\n.\n(4)\nHere, the term x denotes the mean of x.\nThe agent’s performance function is normalized, i. e., 0.0 ≤\nP(m(t)) ≤1.0 in general and specifically for PΩ(m(t)).\nThe discriminator does not use the performance function\nvalue of each policy’s proposal directly. Instead, the perfor-\nmance values are tracked and averaged over a time period t\nusing a simple linear time-invariant function:\npt1(y, u, t) =\n( u\nif t = 0\ny + u −y\nt\notherwise,\n(5)\nThis way, fluctuations, especially in the adaptive policy, will\nnot lead to a “flapping” behavior; instead, the discriminator\nwill prefer the existing controller for a longer period of time,\nswitching to the adaptive policy only when it provides beneficial\nsetpoints throughout a number of iterations. The dual-policy\nsetup also treats the existing controller strategy as a fall-back,\nshould the performance of the SAC policy drop (e. g., because\nof catastrophic forgetting).\nThe discriminator effectively causes the adaptive policy to\nreceive three samples per step: Two projected (the proposal\nchecked against the internal world model) and the actual reward\nreceived by the environment. As SAC is an off-policy algorithm,\npalaestrai.agent\nMuscle\nBrain\narl.agent\nMuscle\n-adaptive_inferer_performance : ﬂoat\n-rules_inferer_performance : ﬂoat\nDiscriminator\nBrain\n-_model : Grid\nWorldModel\nObjective\nharl.sac\nSACBrain\nSACMuscle\nmidas.tools.palaestrai\nARLOperator\nObjective\nReactivePower\nController\nadaptive_policy\nrules_policy\n<<use>>\n<<use>>\n<<use>>\n<<use>>\nFig. 1: Class diagram of the ARL agent, featuring the\nDiscriminator\nit can learn from all three. Hence, we effectively speed up the\nlearning procedure.\nFigure 1 depicts the architecture. Note that we also include\nthe DRL rollout worker (Muscle) and learner (Brain) classes\nto reference traditional DRL architectures.\nIV. CASE STUDY\nAs an approach to validate this work-in-progress state, we\nhave chosen the CIGR´E Medium Voltage benchmark grid. We\nhave connected inverter-based loads and generators to each\nbusbar. The simulation does not include additional actors, but\nit features constraints based on the German distribution system\ngrid code.\nThe agent can control each load and generator, and is able\nto observe the complete grid. Note that this actually poses a\nchallenge for DRL algorithms: Loads consume reactive power,\nand generators can inject it. However, an agent is able to use\nall actuators at once, even if this creates a conflicting situation\n(generating VArs and consuming them at the same time). Due\nto the possibility of grid code violations, the agent’s internal\nworld model usually overestimates the reward generated by an\naction (cf. Eq. (4)).\nWe conduct training at test runs over 5760 steps. We list\nall relevant parameters to the grid and hyperparameters for\nthe SAC agents in Table I. Results of the runs are depicted in\nTABLE I: Hyperparameters of SAC agents\nParameter\nValue\nHidden Layer Dimensions\n(16, 16)\nLearning Rate\n10−4\nWarmup Steps\n50\nTraining Frequency\nevery 5 steps\nγ\n0.9\nMax. load per node\n1.4 MW\nMax. reactive consumption p. n.\n0.46 MVar\nMax. real power generation p. n.\n0.8 MW\nMax. reactive power generation p. n.\n0.46 MVar\nFig. 2. Here, we show a side-by-side comparison of the same\nvoltage control task, learned and executed by a pure SAC-based\nagent, and by our hybrid (ARL agent) approach. We show the\nagent’s performance in terms of its utility function, as per\nEq. (4). For the ARL agent, we also note the policy estimates\nthe discriminator makes based on the world model.\nV. DISCUSSION\nBased on Fig. 2(a), one can discern the training phase of\nthe SAC agent and the testing phase. The fluctuations in the\nSAC agent’s reward curve are due to the violation of grid\ncode constraints, which happen because of the noise and\nentropy bonus SAC applies during training to explore the\nstate/action space. These violations are also visible in the\nvoltage magnitudes plot, as buses disabled due to grid code\nconstraints have 0 p u voltage magnitude.\nIn comparison, Fig. 2(b) shows the results of the hybrid\n(ARL) agent approach. There is no clear training/testing phase\ndistinction visible, which is because even during training, the\nfallback option provided by the controller policy safeguards\nthe agent against grid code violations. For this reason, we have\nalso plotted the internal reward estimate of the discriminator.\nThere is a gap due to the warm-up phase of the internal SAC\nlearner; afterward, the SAC agent learns quickly and overtakes\nthe controller policy.\nWe note that the hybrid agent approach does not cause\nany grid code violations. Moreover, the overestimation of\nthe grid code’s state does not yield any negative results. We\nassume that this is also due to the SAC algorithm’s explicit\nremedy of overestimated Q values, which was the Achilles’\nheel of DDPG, and which now helps with the interaction of\nthe world model. In addition, the pure SAC agent is not able to\nachieve the theoretical maximum of the utility function. As we\nnoted previously, the agent has to learn the different actuators\ncontradict each other, which the SAC implementation is not able\nto do during the approx. 5800 steps it has to do so. In contrast,\nthe imitation learning approach of our hybrid (Adversarial\nResilience Learning (ARL)) agent does so, most probably not\nonly due to a number of “correct” samples provided by the\ncontroller, but also because it has thrice as many samples in\nits replay buffer to train on, compared to the SAC agent.\n0\n0.2\n0.4\n0.6\n0.8\n0\n1000\n2000\n3000\n4000\n5000\n0\n0.5\n1\nSAC Agent Performance\nReward\nVoltage [p.u.]\n(a) SAC Agent\n0\n0.5\n1\n0\n1000\n2000\n3000\n4000\n5000\n0\n0.5\n1\nReward\nPolicy Reward Estimate\nAdaptive Policy\nController Policy\nHybrid Agent Performance\nReward\nVoltage [p.u.]\n(b) ARL Agent\nFig. 2: Utility function results (performance) and voltage magnitudes by the pure SAC agent and the Hybrid approach\nVI. CONCLUSIONS & FUTURE WORK\nIn this paper, we described a hybrid agent approach that\nincorporated both, model-based DRL as well as imitation\nlearning into a single hybrid agent architecture, which is part\nof the ARL agent approach. We have provided preliminary\nresults that indicate that our approach leads to faster training as\nwell as guarantees on the benign behavior of the agent, which\nis able to transparently alternate between a DRL policy and a\nknown and tried controller policy.\nIn the future, we will extend our approach by testing it in\nmore complex scenarios, adding other actors, time series for\nDER feed-in, more capable world models, as well as adversarial\nagents. As the ARL approach is a specific form of a DRL\nautocurriculum setup, we will evaluate the specific behavior of\nour approach in the face of even misactors. We will also employ\nmethods based on DRL [41] to estimate the effectiveness of\npolicies learned in this way. We expect that an autocurriculum\napproach is suitable especially to overcome adverse conditions,\nbut has its own drawbacks in its nature of being a (model-free)\nDRL approach. Here, we assume that our hybrid approach will\nbe able to bridge that gap and provide safety guarantees during\nnormal operation, while being able to answer unexpected events\nby the virtue of the DRL policy.\nACKNOWLEDGEMENT\nThis work was funded by the German Federal Ministry for\nEducation and Research (BMBF) under Grant No. 01IS22071.\nThe authors would explicitly like to thank Peter Palensky,\nJanos Sztipanovits, and Sebastian Lehnhoff in their help in\nestablishing the ARL research group.\nREFERENCES\n[1]\nV. Mnih et al., “Playing atari with deep reinforcement learning,”\narXiv preprint arXiv:1312.5602, 2013.\n[2]\nJ. Schrittwieser et al., “Mastering Atari, Go, Chess and Shogi\nby planning with a learned model,” pp. 1–21, 2019. arXiv:\n1911.08265. [Online]. Available: http://arxiv.org/abs/1911.\n08265.\n[3]\nO. Vinyals et al., “Grandmaster level in StarCraft II using\nmulti-agent reinforcement learning,” Nature, vol. 575, no. 7782,\npp. 350–354, Nov. 2019, Number: 7782 Publisher: Nature\nPublishing Group, ISSN: 1476-4687. DOI: 10.1038/s41586-\n019-1724-z. [Online]. Available: https://www.nature.com/\narticles/s41586-019-1724-z (visited on 01/29/2024).\n[4]\nE. M. Veith, Universal Smart Grid Agent for Distributed Power\nGeneration Management. Logos Verlag Berlin GmbH, 2017.\n[5]\nR. Diao et al., “Autonomous voltage control for grid oper-\nation using deep reinforcement learning,” IEEE Power and\nEnergy Society General Meeting, vol. 2019-Augus, 2019, ISSN:\n19449933. DOI: 10.1109/PESGM40551.2019.8973924. arXiv:\n1904.10597.\n[6]\nB. L. Thayer and T. J. Overbye, “Deep reinforcement learning\nfor electric transmission voltage control,” in 2020 IEEE Electric\nPower and Energy Conference (EPEC), IEEE, 2020, pp. 1–8.\n[7]\nZ. Wu, C. Li, and L. He, “A novel reinforcement learning\nmethod for the plan of generator start-up after blackout,”\nElectric Power Systems Research, vol. 228, p. 110 068, Mar. 1,\n2024, ISSN: 0378-7796. DOI: 10.1016/j.epsr.2023.110068.\n[Online]. Available: https://www.sciencedirect.com/science/\narticle/pii/S0378779623009550 (visited on 01/29/2024).\n[8]\nK. Arshad et al., “Deep reinforcement learning for anomaly\ndetection: A systematic review,” IEEE Access, vol. 10,\npp. 124 017–124 035, 2022. DOI: 10.1109/ACCESS.2022.\n3224023.\n[9]\nE. Veith, A. Wellßow, and M. Uslar, “Learning new attack\nvectors from misuse cases with deep reinforcement learning,”\nFrontiers in Energy Research, 2023.\n[10]\nT. Wolgast et al., “Analyse–learning to attack cyber-physical\nenergy systems with intelligent agents,” SoftwareX, Apr. 2023.\nDOI: 10.1016/j.softx.2023.101484. [Online]. Available: https:\n//ui.adsabs.harvard.edu/abs/2023SoftX..2301484W/abstract.\n[11]\nX. Chen, G. Qu, Y. Tang, S. Low, and N. Li, “Reinforcement\nlearning for selective key applications in power systems: Recent\nadvances and future challenges,” IEEE Transactions on Smart\nGrid, vol. 13, no. 4, pp. 2935–2958, 2022.\n[12]\nM. McCloskey and N. J. Cohen, “Catastrophic interference\nin connectionist networks: The sequential learning problem,”\nin Psychology of Learning and Motivation, G. H. Bower,\nEd., vol. 24, Academic Press, Jan. 1, 1989, pp. 109–165.\nDOI: 10.1016/S0079-7421(08)60536-8. [Online]. Available:\nhttps : / / www . sciencedirect . com / science / article / pii /\nS0079742108605368 (visited on 02/24/2024).\n[13]\nK. Khetarpal, M. Riemer, I. Rish, and D. Precup, Towards\ncontinual reinforcement learning: A review and perspectives,\nDec. 24, 2020. DOI: 10.48550/arXiv.2012.13490. arXiv: 2012.\n13490[cs]. [Online]. Available: http://arxiv.org/abs/2012.13490\n(visited on 10/14/2022).\n[14]\nT. Wang et al., Benchmarking model-based reinforcement\nlearning, Jul. 3, 2019. DOI: 10.48550/arXiv.1907.02057.\narXiv: 1907.02057[cs,stat]. [Online]. Available: http://arxiv.\norg/abs/1907.02057 (visited on 02/24/2024).\n[15]\nR. R. Hossain et al., “Efficient learning of power grid voltage\ncontrol strategies via model-based deep reinforcement learning,”\nMachine Learning, Nov. 6, 2023, ISSN: 1573-0565. DOI: 10.\n1007/s10994-023-06422-w. [Online]. Available: https://doi.\norg/10.1007/s10994-023-06422-w (visited on 02/16/2024).\n[16]\nQ. Wang, F. Li, Y. Tang, and Y. Xu, “Integrating model-\ndriven and data-driven methods for power system frequency\nstability assessment and control,” IEEE Transactions on Power\nSystems, vol. 34, no. 6, pp. 4557–4568, Nov. 2019, Conference\nName: IEEE Transactions on Power Systems, ISSN: 1558-0679.\nDOI: 10.1109/TPWRS.2019.2919522. [Online]. Available:\nhttps://ieeexplore.ieee.org/document/8723612 (visited on\n02/24/2024).\n[17]\nG. Qu, C. Yu, S. Low, and A. Wierman, Combining model-\nbased and model-free methods for nonlinear control: A\nprovably convergent policy gradient approach, Jun. 12, 2020.\nDOI: 10.48550/arXiv.2006.07476. arXiv: 2006.07476[cs,eess,\nmath]. [Online]. Available: http://arxiv.org/abs/2006.07476\n(visited on 02/24/2024).\n[18]\nJ. Richens and T. Everitt, Robust agents learn causal world\nmodels, 2024. arXiv: 2402.10877 [cs.AI].\n[19]\nR. S. Sutton and A. G. Barto, Reinforcement learning: An\nintroduction. MIT press, 2018.\n[20]\nM. Hessel et al., “Rainbow: Combining improvements in DQN,”\nThe Thirty-Second AAAI Conference on Artificial Intelligence\n(AAAI-18), pp. 3215–3222, 2018. [Online]. Available: https:\n//www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/\n17204/16680.\n[21]\nX. Li et al., “Supervised assisted deep reinforcement learning\nfor emergency voltage control of power systems,” Neurocom-\nputing, vol. 475, pp. 69–79, 2022.\n[22]\nT. P. Lillicrap et al., “Continuous control with deep reinforce-\nment learning,” 4th International Conference on Learning\nRepresentations, ICLR 2016 - Conference Track Proceedings,\n2016. arXiv: 1509.02971.\n[23]\nS. Fujimoto, H. Hoof, and D. Meger, “Addressing function\napproximation error in actor-critic methods,” in Proceedings of\nthe 35th International Conference on Machine Learning, ISSN:\n2640-3498, PMLR, Jul. 3, 2018, pp. 1587–1596. [Online].\nAvailable: https://proceedings.mlr.press/v80/fujimoto18a.html\n(visited on 01/04/2023).\n[24]\nJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O.\nKlimov, “Proximal policy optimization algorithms,” Jul. 19,\n2017. arXiv: 1707.06347. [Online]. Available: http://arxiv.org/\nabs/1707.06347.\n[25]\nT. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, “Soft\nactor-critic: Off-policy maximum entropy deep reinforcement\nlearning with a stochastic actor,” arXiv:1801.01290 [cs, stat],\nAug. 8, 2018. arXiv: 1801.01290. (visited on 12/22/2021).\n[26]\nJ. Xie and W. Sun, “Distributional deep reinforcement learning-\nbased emergency frequency control,” IEEE Transactions on\nPower Systems, vol. 37, no. 4, pp. 2720–2730, 2021.\n[27]\nR. F. Prudencio, M. R. O. A. Maximo, and E. L. Colombini, A\nsurvey on offline reinforcement learning: Taxonomy, review, and\nopen problems, Mar. 5, 2022. DOI: 10.48550/arXiv.2203.01387.\narXiv: 2203.01387[cs,stat]. (visited on 12/03/2022).\n[28]\nS. Levine, A. Kumar, G. Tucker, and J. Fu, Offline reinforce-\nment learning: Tutorial, review, and perspectives on open\nproblems, Nov. 1, 2020. DOI: 10.48550/arXiv.2005.01643.\narXiv: 2005.01643[cs,stat]. [Online]. Available: http://arxiv.\norg/abs/2005.01643 (visited on 01/02/2023).\n[29]\nF.-M. Luo et al., “A survey on model-based reinforcement\nlearning,” Science China Information Sciences, vol. 67, no. 2,\np. 121 101, 2024.\n[30]\nX. Y. Lee, S. Sarkar, and Y. Wang, A graph policy network\napproach for volt-var control in power distribution systems,\nJun. 20, 2022. DOI: 10 . 48550 / arXiv. 2109 . 12073. arXiv:\n2109.12073[cs,eess]. [Online]. Available: http://arxiv.org/abs/\n2109.12073 (visited on 02/24/2024).\n[31]\nD. Cao et al., “A multi-agent deep reinforcement learning\nbased voltage regulation using coordinated PV inverters,” IEEE\nTransactions on Power Systems, vol. 35, no. 5, pp. 4120–4123,\nSep. 2020, Conference Name: IEEE Transactions on Power\nSystems, ISSN: 1558-0679. DOI: 10 . 1109 / TPWRS . 2020 .\n3000652. [Online]. Available: https://ieeexplore.ieee.org/\ndocument/9113746 (visited on 02/24/2024).\n[32]\nY. Shi, G. Qu, S. Low, A. Anandkumar, and A. Wierman,\n“Stability constrained reinforcement learning for real-time\nvoltage control,” in 2022 American Control Conference (ACC),\nISSN: 2378-5861, Jun. 2022, pp. 2715–2721. DOI: 10.23919/\nACC53348 . 2022 . 9867476. [Online]. Available: https : / /\nieeexplore.ieee.org/document/9867476?denied= (visited on\n02/24/2024).\n[33]\nY. Gao and N. Yu, “Model-augmented safe reinforcement\nlearning for volt-var control in power distribution networks,”\nApplied Energy, vol. 313, p. 118 762, 2022.\n[34]\nJ. H. Woo, L. Wu, J.-B. Park, and J. H. Roh, “Real-time\noptimal power flow using twin delayed deep deterministic\npolicy gradient algorithm,” IEEE Access, vol. 8, pp. 213 611–\n213 618, 2020. DOI: 10.1109/ACCESS.2020.3041007.\n[35]\nY. Zhou et al., “A data-driven method for fast AC optimal\npower flow solutions via deep reinforcement learning,” Journal\nof Modern Power Systems and Clean Energy, vol. 8, no. 6,\npp. 1128–1139, 2020. DOI: 10.35833/MPCE.2020.000522.\n[36]\nL. Lin et al., “Deep reinforcement learning for economic\ndispatch of virtual power plant in internet of energy,” IEEE\nInternet of Things Journal, vol. 7, no. 7, pp. 6288–6301, 2020.\nDOI: 10.1109/JIOT.2020.2966232.\n[37]\nQ. Zhang, K. Dehghanpour, Z. Wang, and Q. Huang, “A\nlearning-based power management method for networked\nmicrogrids under incomplete information,” IEEE Transactions\non Smart Grid, vol. 11, no. 2, pp. 1193–1204, 2020. DOI:\n10.1109/TSG.2019.2933502.\n[38]\nF. L. D. Silva, C. E. H. Nishida, D. M. Roijers, and A. H. R.\nCosta, “Coordination of electric vehicle charging through\nmultiagent reinforcement learning,” IEEE Transactions on\nSmart Grid, vol. 11, no. 3, pp. 2347–2356, 2020. DOI: 10.\n1109/TSG.2019.2952331.\n[39]\nL. Fischer, J. M. Memmen, E. M. Veith, and M. Tr¨oschel,\n“Adversarial resilience learning—towards systemic vulnerability\nanalysis for large and complex systems,” in ENERGY 2019,\nThe Ninth International Conference on Smart Grids, Green\nCommunications and IT Energy-aware Technologies, Athens,\nGreece: IARIA XPS Press, 2019, pp. 24–32, ISBN: 978-1-\n61208-713-9.\n[40]\nE. M. Veith, “An architecture for reliable learning agents in\npower grids,” in ENERGY 2023, The Thirteenth International\nConference on Smart Grids, Green Communications and IT\nEnergy-aware Technologies, Barcelona, Spain: IARIA XPS\nPress, 2023, pp. 13–16.\n[41]\nT. Logemann and E. M. Veith, “NN2EQCDT: Equivalent\ntransformation of feed-forward neural networks as drl policies\ninto compressed decision trees,” IARIA, vol. 15, ThinkMind,\nJun. 2023, pp. 94–100, ISBN: 978-1-68558-046-9.\n",
  "categories": [
    "cs.AI"
  ],
  "published": "2024-04-02",
  "updated": "2024-04-02"
}