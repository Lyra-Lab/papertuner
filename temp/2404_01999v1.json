{
  "id": "http://arxiv.org/abs/2404.01999v1",
  "title": "Emergence of Chemotactic Strategies with Multi-Agent Reinforcement Learning",
  "authors": [
    "Samuel Tovey",
    "Christoph Lohrmann",
    "Christian Holm"
  ],
  "abstract": "Reinforcement learning (RL) is a flexible and efficient method for\nprogramming micro-robots in complex environments. Here we investigate whether\nreinforcement learning can provide insights into biological systems when\ntrained to perform chemotaxis. Namely, whether we can learn about how\nintelligent agents process given information in order to swim towards a target.\nWe run simulations covering a range of agent shapes, sizes, and swim speeds to\ndetermine if the physical constraints on biological swimmers, namely Brownian\nmotion, lead to regions where reinforcement learners' training fails. We find\nthat the RL agents can perform chemotaxis as soon as it is physically possible\nand, in some cases, even before the active swimming overpowers the stochastic\nenvironment. We study the efficiency of the emergent policy and identify\nconvergence in agent size and swim speeds. Finally, we study the strategy\nadopted by the reinforcement learning algorithm to explain how the agents\nperform their tasks. To this end, we identify three emerging dominant\nstrategies and several rare approaches taken. These strategies, whilst\nproducing almost identical trajectories in simulation, are distinct and give\ninsight into the possible mechanisms behind which biological agents explore\ntheir environment and respond to changing conditions.",
  "text": "EMERGENCE OF CHEMOTACTIC STRATEGIES WITH\nMULTI-AGENT REINFORCEMENT LEARNING\nSamuel Tovey†, Christoph Lohrmann†, Christian Holm\nInstitute for Computational Physics\nUniversity of Stuttgart\n70569, Stuttgart, Germany\n{stovey, clohrmann, holm}@icp.uni-stuttgart.de\nABSTRACT\nReinforcement learning (RL) is a flexible and efficient method for programming micro-robots in\ncomplex environments. Here we investigate whether reinforcement learning can provide insights into\nbiological systems when trained to perform chemotaxis. Namely, whether we can learn about how\nintelligent agents process given information in order to swim towards a target. We run simulations\ncovering a range of agent shapes, sizes, and swim speeds to determine if the physical constraints\non biological swimmers, namely Brownian motion, lead to regions where reinforcement learners’\ntraining fails. We find that the RL agents can perform chemotaxis as soon as it is physically possible\nand, in some cases, even before the active swimming overpowers the stochastic environment. We\nstudy the efficiency of the emergent policy and identify convergence in agent size and swim speeds.\nFinally, we study the strategy adopted by the reinforcement learning algorithm to explain how the\nagents perform their tasks. To this end, we identify three emerging dominant strategies and several\nrare approaches taken. These strategies, whilst producing almost identical trajectories in simulation,\nare distinct and give insight into the possible mechanisms behind which biological agents explore\ntheir environment and respond to changing conditions.\nKeywords Reinforcement learning, microrobotics, chemotaxis, active matter, biophysics\n1\nIntroduction\nMicroswimmers have the unique privilege of having evolved over millions of years to learn how to optimally navigate\nnoisy, Brownian motion-dominated environments in search of better living conditions. Most interactions humans are\nfamiliar with occur on length and time scales that are not subject to this noise. Therefore, we do naturally have an\nunderstanding of how these microswimmers can perform this navigation. However, understanding the emergence of this\nbehaviour is critical as scientists strive to construct the artificial counterparts of biological microswimmers. Previous\nreviews have discussed the emergence and function of biological microswimmers in great detail [Bastos-Arrieta et al.,\n2018, Elgeti et al., 2015], elucidating the mechanisms and strategies behind their movement. One recurring form of\nnavigation in microswimmers is the so-called run-and-tumble motion exhibited by Escherichia coli (E-coli) wherein\nthe bacteria will travel in a straight line for some extended period before spontaneously rotating into a random new\ndirection [Watari and Larson, 2010, Berg, 2004, Darnton et al., 2006]. One application of this navigation mechanism is\nbacterial chemotaxis [Hansen et al., 2007], the biased movement of a bacteria towards regions with higher concentrations\nof beneficial chemicals or lower concentrations of harmful chemicals [Wadhams and Armitage, 2004]. Bacteria achieve\nthis biased motion through changes in run duration depending on changes in the concentration of the chemo-attractant\nor -repellant. Learning chemotaxis has been the focal point of several research papers aimed at reproducing or better\nunderstanding biological microswimmers through the use of reinforcement learning (RL) [Tovey et al., 2023b, Mo and\nBian, 2022, Hartl et al., 2021, Mui˜nos-Landin et al., 2021]. In their 2021 study, Hartl et al. [2021] applied a genetic\nalgorithm to the problem of learning shape deformations for navigation in static and dynamic environments. They\n†These authors contributed equally\narXiv:2404.01999v1  [physics.bio-ph]  2 Apr 2024\nEmergence of Chemotactic Strategies with Multi-Agent Reinforcement Learning\nfound that the neural networks learned a movement closely resembling that of run-and-tumble motion. In another\n2021 study, Mui˜nos-Landin et al. [2021] applied Q-learning to learning navigation strategies in self-thermophoretic\nparticles from which they again see the emergence of run-and-tumble motion. They further investigated the effects\nof temperature on the learning process, identifying that models trained at higher temperatures took longer to learn their\nemergent strategy. Finally, our previous work [Tovey et al., 2023b] directly addressed the role of temperature in the\nemergent strategy of RL-driven microswimmers by studying chemotaxis learning by the actor-critic reinforcement\nlearning algorithm. It was found that, while the efficacy of the chemotaxis changed with different temperatures, the\nsame run-and-tumble motion arose from the majority of agents trained at different temperatures. While it is clear\nthat RL algorithms can and, in fact, seemingly often do learn run-and-tumble type motion for chemotaxis problems,\nwhat impact this has on our understanding of biological microswimmers and even optimal design of artificial swimmers\nis not clear. This study investigates natural limitations on emergent chemotaxis by training actor-critic RL models\nusing prolate, oblate, and spherical agents of different sizes and with different swim speeds in physically realistic fluid\nenvironments subject to translational and rotational Brownian motion. In this way, we hope to identify how optimal RL\nalgorithms are for the learning task and to identify, if any exist, optimal size/speed combinations of microswimmers in\nthese environments, which may guide our interpretation of biological microswimmers as well as advise the design of\nartificial ones. Furthermore, by investigating the deployment of the RL algorithms close to conditions where agents will\nbe dominated by rotational and translational Brownian motion, we can explore the emergence of different navigation\nstrategies that may be leveraged in the treatment of biological or artificial swimmers, essentially peering into the minds\nof bacteria as they navigate environments.\nThe manuscript is structured as follows. We will first discuss the theory behind the investigation, explaining the mecha-\nnism and physical limitations of chemotaxis in biological systems. We then introduce deep actor-critic reinforcement\nlearning and discuss its multi-agent realization. The simulation and training methods are discussed in detail before our\nresults are presented, and a brief outlook is presented.\n2\nTheory\n2.1\nBiological Chemotaxis\nAs mentioned briefly in the introduction, chemotaxis is the biased movement of bacteria towards favourable regions\nin their environment [Wadhams and Armitage, 2004]. Escherichia coli (e. coli) perform chemotaxis actively using\nrun-and-tumble motion. In their running phase, they bundle their flagella together and rotate them anti-clockwise,\nduring a tumble phase, one or more flagella change their rotation direction, breaking apart the bundle and causing\nrandom rotation of the bacteria before the bundle reforms and translational swimming is resumed [Turner et al., 2000].\nUtilizing a sensing mechanism, these bacteria can identify if they are moving towards or away from favourable regions\nof their environment, adjusting their tumble rates accordingly to maintain desired movement [Schnitzer et al., 1990].\nIn order to capture the essential features of bacterial motility, we consider here active particles that can perform four\ndistinct types of movement. Firstly, they can move forward along their intrinsic direction like bacteria in the run phase.\nSecondly, they can actively rotate clockwise or counterclockwise, like bacteria in the tumble phase. Our swimmers can\nchoose the direction of rotation while bacteria rotate towards a random new orientation. Thirdly, the swimmers can opt\nto do nothing, that is, only move passively by Brownian motion.\nIn our investigations, interactions of the particles with their surrounding fluid is modelled by the over-damped Langevin\nequations\n˙ri = γ−1\nt\n[F(t)ei(Θi) −∇V (ri, {rj})] +\nq\n2kBTγ−1\nt\nRt\ni(t),\n(1)\n˙Θ = γ−1\nr m(t) +\nq\n2kBTγ−1\nr Rr\ni (t).\n(2)\nwhere, ri is the (two-dimensional) position of particle i, Θi the angle describing the particle orientation, γ(t,r) the\ntranslational (rotational) friction coefficient, F and m a force and torque corresponding to the respective type of active\nmotion, e = (cos(Θ), sin(Θ))T the particle orientation, V an interaction potential between all particles in the system,\nkB the Boltzmann constant, T the temperature and R(t,r)\ni\na noise term with zero mean and correlations according to\nD\nR(t,r)\ni\n(t)R(t,r)\nj\n(t′)\nE\n= δijδ(t −t′), where ⟨·⟩denotes an ensemble average.\nTo quantify the relative importance of active and passive motion, we define translational and rotational P´eclet numbers\nPetrans,rot = τ diff\ntrans,rot\nτ act\ntrans,rot\n.\n(3)\n2\nEmergence of Chemotactic Strategies with Multi-Agent Reinforcement Learning\nHere,\nτ diff\nrot =\n1\n2Drot\n=\nγr\n2kBT ,\nτ act\nrot = 2π\nωact ,\n(4)\nare the timescale of decorrelation of the particle director through rotational diffusion and the timescale for one active\nrotation, respectively. For the translational degrees of freedom we have\nτ diff\ntrans =\na2\nDtrans\n= a2γt\nkBT ,\nτ act\ntrans =\na\nvact ,\n(5)\nas the timescale for diffusion of one particle radius and the timescale for swimming of one particle radius, respectively.\nIn regimes where Petrans,rot ≫1 the dynamics will be dominated by active motion and when Petrans,rot ≪1, it will\nresemble passive diffusion.\n2.2\nActor-Critic Reinforcement Learning\nReinforcement learning concerns itself with the interactions between an agent and its environment within which it\ngradually learns to achieve a desired task [Sutton and Barto, 2018]. This agent is typically provided with a set of actions\nit may perform and uses a policy, π(at|st) to decide at time t, based on its current state, st, what the best actions, at\nwill be such that it maximises a reward, r(st). Over the course of one or many simulations, this policy will be updated\nso that the agent becomes more efficient at accomplishing this task and maximising its reward,\nπ′ = arg max\nπ\n⟨r(st|π)⟩\n(6)\nDeep reinforcement learning accomplishes this task using deep neural networks as the policy, π [Arulkumaran et al.,\n2017]. During our investigations, the actor-critic approach to deep reinforcement learning has been adopted due to its\nflexibility and efficacy [Barto et al., 1983, Grondman et al., 2012]. In actor-critic reinforcement learning, the actor takes\non the role of the policy, πθ, parameterized by θ, taking as input the current state of the agent and returning oftentimes a\ndistribution over possible actions from which one is selected. The critic then takes on the role of a value function, V πθ\nω ,\nthe objective of which is to describe the expected return of an agent starting in state st and following policy π. During\ntraining, the actor is tasked with maximising its finite-horizon return of its policy\nJ(πθ) =\n*\" T\nX\nt=0\nlog πθ(at|st) · Aπθ(st, at)\n#+\nτ\n,\n(7)\nwhere Aπθ is the so-called advantage, computed by\nAπθ\nt\n= G(st, at) −V πθ\nω (st),\n(8)\nwhere G is an analytic expected returns function. In our studies, a simple decaying return function\nGt =\nT\nX\nt′=t\nϵt′−trt′,\n(9)\nwith decay factor ϵ is used. J(πθ) is maximised by way of gradient ascent on the actor parameters with updates taking\nthe form\nθ′ = θ + η · ∇θJ(πθ),\n(10)\nwith learning rate η. Recalling that the actor output is a distribution over actions, should the advantage be negative, i.e,\nthe critic believes a better trajectory could have been chosen, the log probability of these action will be discouraged. If\nthis number is positive, the actor has outperformed the expectation of the critic and the policy is reinforced. The critic\nnetwork is trained directly on the chosen expected returns function via the Huber loss\nLδ(ytrue, ypredicted) =\n\u001a 1\n2 · (ytrue −ypredicted)2\n, for |ytrue −ypredicted| ≤δ,\nδ · (|ytrue −ypredicted| −1\n2δ)\n, otherwise,\n(11)\nwith δ = 1 in all studies. Such an update procedure is referred to as simple or vanilla policy gradient [Sutton et al.,\n1999]. Whilst more sophisticated approaches exist, namely proximal policy optimization [Schulman et al., 2017], for\nthis particular study, the simpler approach sufficed.\n3\nEmergence of Chemotactic Strategies with Multi-Agent Reinforcement Learning\n2.3\nMulti-Agent Reinforcement Learning\nIn our simulations, we work with not one, but many agents simultaneously, moving from the general concept of\nreinforcement learning into multi-agent reinforcement learning (MARL) [Gronauer and Diepold, 2022]. In these cases,\neach agent shares a single actor and critic network and at the update time, also the experience that they have gathered.\nDuring the simulations, each agent asks the actor for an action to take individually and collects its own reward. At the\ntime of the update, J(πθ) becomes,\nJMARL = 1\nN\nN\nX\ni\nJi(πθ),\n(12)\nwhere i sums over the agents in the system and Ji is simply Equation (7) for a single agent. In this way, the experience\nof each agent is accumulated and updated together.\nThe field of MARL has a a vast set of definitions with respect to how individual agents interact and share knowledge in\norder to achieve the problem they are training on [Oliehoek et al., 2016]. In this work, a decentralized Markov decision\nprocess is used to describe how the agents in the system interact. The system is considered decentralized as each agent\nreceives only local information regarding its environment and a local reward for its own actions. During training, these\nrewards and local states are summed over and in doing so, the agents share the knowledge with one another.\n3\nMethods\n3.1\nSwarmRL\nAll reinforcement learning and simulation has been handled through the open-source software package, SwarmRL [Tovey\net al., 2023a]. SwarmRL is a Python library built to combine molecular dynamics engines with classical control and\nreinforcement learning algorithms. All machine learning uses the JAX [Bradbury et al., 2018] and Flax [Heek et al.,\n2023] libraries.\n3.2\nESPResSo Simulations\nIn this study, reinforcement learning is applied to training microswimmers in a physically realistic simulated environment.\nFor this environment, we employ the ESPResSo simulation engine [Weik et al., 2019]. Trajectories of the particles\nare simulated using the over-damped Langevin equations of motion for both position and orientation described in\nEquations (1) and (2). The friction coefficient of a spherical agent with radius r in a fluid with dynamic viscosity µ\nis calculated according to Stokes’ law as γt = 6πµr and γr = 8πµr3. Interactions between the spherical agents are\nmodelled with the two-body Weeks-Chandler-Anderson (WCA) potential [Weeks et al., 1971], which can be seen as an\nalmost-hard-sphere interaction\nV (rij) =\n\n\n\n4 · V0\n\u0014\u0010\nσ\nrij\n\u001112\n−\n\u0010\nσ\nrij\n\u00116\u0015\n+ V0,\nrij < 21/6σ\n0,\nelse.\n(13)\nHere, rij = ||ri −rj||2 is the Euclidean distance between the particles, and σ = 2a the colloid diameter. We choose\nthe interaction strength V0 = kBT. Details on the anisotropic particles’ friction coefficients and interaction potentials\ncan be found in the supplementary information.\n3.3\nReinforcement Learning Parameters\nIn our investigations, the actor-critic approach to reinforcement learning is utilised with a network architecture displayed\nin Figure 1. A two-layer network, each with 128 units, is deployed for both the actor and the critic, along with ReLU\nactivation functions. During the training, each network is trained for 10000 episodes, each of which consists of 20\napplications of the policy over 4000 simulation time steps. Each episode would be 2 s in real-time. Updates of the\nnetwork are handled by the Adam optimizer [Kingma and Ba, 2017] using a learning rate of 0.002. For each swim\nspeed and agent size, 20 reinforcement learning runs were performed to collect statistics.\n3.4\nAgent Definition\nThe study considered three agent shapes with a fixed set of actions. Agent shapes were designed to mimic some of\nthose found in biology: oblate, prolate, and spherical bacteria or microswimmers. Figure 2 displays renderings of these\n4\nEmergence of Chemotactic Strategies with Multi-Agent Reinforcement Learning\nFigure 1: Representation of actor-critic reinforcement learning architectures.\nFigure 2: Graphical Representation of the three agent shapes considered in this study, the sphere (center), prolate (right),\nand oblate (left). In each case, the volume of the agent is kept equal for a given radius value.\n5\nEmergence of Chemotactic Strategies with Multi-Agent Reinforcement Learning\nagents for radius 1µm constructed using the Vedo Python package [Musy et al., 2023]. As the agents are designed to\nmimic bacterial particles, we endow them with the ability to perform the four actions described in Section 2.1,\nA =\n\n\n\n\n\n\n\nTranslate:\nv = n · dµms−1, ω = 0.0s−1\nRotate CCW:\nv = 0µms−1, ω = 10.472s−1\nRotate CW:\nv = 0µms−1, ω = −10.472s−1\nDo Nothing:\nv = 0µms−1, ω = 0.0s−1,\n(14)\nwhere d is the colloid diameter, n is a scaling factor that we vary during the experiment, and ω is the angular velocity\nmeasured in radians per second. The rotation speed was chosen to be similar to that of Escherichia coli [Berg and\nBrown, 1972]. In line with [Murray and Jackson, 1992], we argue that agent volume is proportional to its swimming\nspeed. Therefore, the action is measured in body lengths, and all agents with the same radius will swim at the same\nspeed. The agents receive a state description designed to resemble a bacterium sensing changes in its surroundings,\ndefined mathematically by\noi(t) = f (||ri(t) −rs(t)||2) −f (||ri(t −∆t) −rs(t −∆t)||2) ,\n(15)\nwhere oi is the observable for the ith agent, f is the field chosen to represent the chemical being sensed, in our study,\n1\nris , ri(t) is the position of the ith agent at time t, ∆t is the amount of time since the last action was computed and ˆrs(t)\ndenotes the position of the source of the field at time t. To encourage chemotaxis, agents are rewarded using a similar\nfunction\nri(t) =\n\u001aoi\nif oi > 0\n0\nelse.\n(16)\nThis way, movement towards the source is encouraged, but movement away is not explicitly discouraged. We further\nrefrain from using an absolute measure of the field in this study as it would not resemble the natural sensing abilities of\nthe bacteria [Bren and Eisenbach, 2000]. The addition of such a reward might be used to encourage agents to form\ngroups reminiscent of biofilms or to replicate the act of digesting the source of the field, both of which are left for future\nstudies.\n3.5\nComputational Methods\nTraining and deployment of the reinforcement learning models was performed on the University of Stuttgart SimTech\ncompute cluster. Each simulation and training routine utilised six threads of an AMD EPYC 7702 CPU node, and\nall simulations were run in parallel. Due to the system sizes and machine learning being performed, no GPUs were\nrequired for these experiments. Training of each model required approximately twenty-four hours, and the deployment\nsimulations were approximately six hours. The simulations and models were analysed on the same cluster hardware.\n4\nResults\nDue to the similarity in the results and the amount of analysis, only the plots for the spherical agent analysis are shown\nin the main manuscript. All other plots are included in the SI, and any deviations between results are mentioned here.\n4.1\nProbability of Emergent Chemotaxis\nThis investigation aims to identify limits on emergent chemotaxis in RL agents in the hope that such limits cross over\ninto biology, allowing us to study natural biological processes using RL as a valid surrogate model. These limits\nsuggest formulating a phase diagram with forbidden regions where this behaviour is strictly prohibited. To this end,\nall simulations were collected where the final 50 % of the deployment trajectory was below 15 µm from the source of\nthe field. This distance was determined based on the visual observation that no model that had successfully learned\nchemotaxis was farther away from the source than this distance. The successful simulations were used to compute the\nprobability of learning chemotaxis by rationing them against the total number of simulations performed for a single\nspeed and agent size. Figure 3 shows the computed phase diagram. The figure plots the sampled data points; the alpha\nvalue corresponds to the probability, with the more transparent points being less probable. One can identify a forbidden\nregion in the size-speed space. Interestingly, it appears that smaller, faster colloids are more likely to learn an effective\nchemotaxis policy. Suppose we consider the difficulty the RL algorithm has in training a policy with the real-world\nproblem of evolving a suitable structure for life. In that case, these results suggest a trade-off between speed and size\nwhen learning how to perform chemotaxis. The most critical component of Figure 3 is the theoretical boundaries formed\nby considering the ratio between Brownian motion and the active motion of the particles described by Equations 4\nand 5. The green lines in Figure 3 correspond to the colloid radius and speed values for which this ratio is 1.0 for\n6\nEmergence of Chemotactic Strategies with Multi-Agent Reinforcement Learning\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nColloid Radius / m\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\nSwim Speed / body length s\n1\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nProbability of Emergent Chemotaxis\nFigure 3: Probability of successful chemotaxis emerging from RL studies. Raw data from the experiment. The colour\nof each point corresponds to the number of RL simulations that successfully learned how to perform chemotaxis. The\ngreen lines indicate the theoretical values at which translational (solid) and rotational (dashed) diffusion becomes\ndominant compared to the active motion of the agents.\ntranslational (solid) and rotational (dashed) diffusion. The translation ratio forms a boundary where the RL agents\ncan no longer learn successful chemotaxis. The rotational diffusion line appears less strict, particularly for the faster\nagents, where it appears that with enough translational activity, the agents can overcome having rotational diffusion\ndominate over active rotation. The alignment of our results with the theoretical values suggests that it does so as soon\nas it is physically possible for an RL agent to learn chemotaxis. Such a result encourages one to consider studying\nfurther features of the models to understand how these features might also arise in these agents’ biological or artificial\ncounterparts. Interestingly, the onset of successful chemotaxis can take place far below this theoretical limit but very\nrarely.\n4.2\nLearning Efficiency\nNext, we look at how the reward received from the reinforcement learning process changed depending on the size and\nspeed of the colloids. This measure will indicate how easy it was for the model to learn the policy required to perform\nchemotaxis. Figure 4 outlines the results of this study in a similar manner to Figure 3. In the figure, the point’s colour\ncorresponds to the total reward accumulated by the agents during all 10’000 training episodes. In order to compute the\ncolour values in Figure 4, we corrected the size difference between the colloids. In the original simulations, an explicit\ndistance to the source is used in the reward computation. However, this biases the results such that smaller colloids,\nno matter how successful they were, will achieve exponentially higher rewards as they can approach the source more\nclosely. Therefore, the rewards in Figure 4 were computed by converting the reward from distance to the number of\nbody lengths from the source. We can see that the reward diagrams roughly mirror the results shown in Figure 3 with\n7\nEmergence of Chemotactic Strategies with Multi-Agent Reinforcement Learning\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nColloid Radius / m\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\nSwim Speed / body lengths s\n1\n0\n1000\n2000\n3000\n4000\n5000\nTotal reward from training\nFigure 4: Probability of successful chemotaxis emerging from RL studies. Raw data from the experiment. The colour\nof each point corresponds to the maximum reward achieved by the agents during the 10’000 episodes.\nlarger discrepancies between the larger and smaller colloids. Namely, the rewards achieved by small and fast agents is\nnoticeably larger than those of the bigger agents. This effect is particularly evident in the prolate and oblate simulations\n(SI Figures 10 and 14). It is likely due to their hopping over the intended target and inability to sit on it as accurately as\nthe smaller agents.\n4.3\nPolicy Efficiency\nIt is clear from the previous section that microswimmers of different sizes and speeds differ in their probability of\nemergent chemotaxis. However, what the differences are, if any, between their adopted strategy still needs to be\ndetermined. To identify these differences, the deployment simulations were analysed to compute the final equilibrium\ndistance of the colloids around the source as well as after how many action updates they reached this distance. Figure 5\ndisplays the results of this investigation. Figure 5 (left) details the equilibrium distance of the agents as a function of\nradius for all studied swim speeds. On the right-hand side, we see a clear emergence of a linear trend as the limiting\nfactor getting closer to the source becomes the size of the colloid and its speed. The left non-linear side of the plot\nalso contains interesting features. Aside from the speed of one, the minimum distance from the source of the chemical\nfield is achieved at a similar colloid size for all of the different speeds, with faster agents able to achieve slightly better\nequilibrium distances with smaller bodies. In Figure 5 (right), we see the average time the colloids reach the equilibrium\ndistance. For the smaller agents, as is perhaps intuitive, the faster colloids can orient and move themselves to the\nsource faster than their slower counterparts. However, this relationship fades for larger colloids as we see that after\napproximately 1 µm radii, all colloid sizes and speeds converge at similar times except for the slowest at one body\nlength. The time to minimum converges slightly above 25 s. The results also suggest that after 0.5 µm radius, there is\nno conceivable benefit and, in fact, due to the larger equilibrium distance, perhaps even a detriment in being larger.\n8\nEmergence of Chemotactic Strategies with Multi-Agent Reinforcement Learning\n0.5\n1.0\n1.5\n2.0\n2.5\nColloid Radius / m\n2\n4\n6\n8\n10\n12\nEquilibrium Distance from Source / m\n0.5\n1.0\n1.5\n2.0\n2.5\nColloid Radius / m\n0\n2\n4\n6\n8\n10\nTime to minimum / minutes\nSwim Speed\n1.0\n2.0\n3.0\n4.0\n5.0\nFigure 5: (left) Mean distance from the source for each swim speed and colloid size. A clear minimum in each plot\nsuggests an optimal size dependent on swim speed. (right) Rate of convergence to the source for different swim speeds\nand sizes. Interestingly, the convergence rate of larger colloids is relatively similar, suggesting some redundancy in\nlarger body sizes and swim speeds.\nInterestingly, the most unstable equilibrium distances, identified by large variance in mean value and distance from\nthe source, occur close to or within the region displayed in Figure 3 where rotational diffusion overpowers the active\nrotation of the agents. This strong environmental effect could cause instability in these models as they must rely solely\non their active translation to achieve chemotaxis.\n4.4\nEmergent Policy\nAs a final investigation, we determine whether the emergent policy of the RL agent differs for changing physical\nproperties and shapes. Studying the particles’ trajectory alone is almost impossible as they do not show large deviations\nfrom one another. Therefore, to do so, the trained models are given test data over a domain, x ∈(−10.0, 10.0) and the\nprobability of selecting each action is computed from the network outputs. The network output will be four numbers\nfor each concentration value; therefore, before performing further analysis, these outputs are flattened into a single\nvector for each model, which we will refer to as the probability vectors of the network. In order to identify any structure\nin the data, we study the two-component t-distributed stochastic neighbourhood embedding (t-SNE) [van der Maaten\nand Hinton, 2008] of the probability vectors as implemented in the sklearn Python package [Pedregosa et al., 2011].\nFigure 6 outlines the results of the t-SNE for the policy data with a perplexity of 300 and principle-component-analysis\n(PCA) initialisation. Examining the t-SNE plots, we see the emergence of four groups, one of which is seemingly\ndivided into two smaller subgroups. Using this information, we perform k-means clustering [Lloyd, 1982] on the\nprobability vectors to split them into four clusters. The probability of the outcome of each policy is listed in Table 1\nalong with the explained variance from a PCA decomposition of the probability vectors. The probabilities are computed\nby examining the number of points clustered into each class by the k-means algorithm, which we assign to a policy\nby directly examining the action probabilities of the agents mapping into the class. The diagrams used to perform\nthis mapping are included in the SI, where we show the probabilities of each action being taken for all agent sizes,\nshapes, and speeds. We also include smaller sample policy diagrams to demonstrate the actions taken by the agents\nfor each strategy in SI Figure 7. However, as we only ask for four classes, this percentage will naturally ignore more\nconvoluted and rare policies that the t-SNE and K-Means cannot sufficiently distinguish from others. We also perform\nPCA decomposition on the probability vectors to identify how much each policy explains the data distribution. In this\napproach, we see that 5.9 % of the data belongs to components with a smaller than one % impact on the variance of\nthe PCA. When we examine the policies in this region, they are typically made up of weak combinations of the more\ndominant policies with very few exceptions. These policies may explain the splitting in the medium-sized group in\nFigure 6. The remainder of the section will discuss each emergent policy in detail, including some policies poorly\n9\nEmergence of Chemotactic Strategies with Multi-Agent Reinforcement Learning\n40\n30\n20\n10\n0\n10\n20\n30\n40\nt-SNE Dimension 1\n30\n20\n10\n0\n10\n20\n30\n40\n50\nt-SNE Dimension 2\nRun and Rotate, CW\nRun and Rotate, CCW\nGradient Gliding\nBrownian Piloting\n0.5\n1.0\n1.5\n2.0\n2.5\nColloid Size m\nFigure 6: t-SNE embedding of the policy vectors for all successful agents in the study. Four large groups are formed,\ncorresponding to the policies learned by the agents. The colour in these diagrams corresponds to the size of the studied\nagents.\nPolicy Name\nPercentage Learned (K-Means)\nExplained Variance (PCA)\nRun and Rotate\n83.49\n83.5\nGradient Gliding\n12.88\n7.1\nBrownian Piloting\n3.63\n3.5\nExotic Policies\n0.0\n5.9\nTable 1: Percentage and explained variance of agents which learned specific policy along with the explained variance of\nthe principle components for each policy identified.\ncaptured by the embedding methods.\nRun and Rotate\nIn the vast majority of cases (83 %), the agents learned a policy strikingly similar to the\nrun-and-tumble approach found in nature. In these cases, upon experiencing a negative input to the network,\nsignifying a movement away from the source of the gradient, the agents rotate either CW or CCW. Interestingly, once\nthe agents chose a direction to rotate, they did not use the other one. Upon positive input to the network, i.e., movement\ntowards the source of the gradient, the agents chose to translate with probability 1. This policy can be seen clearly for\nthe larger colloids in the top two rows of SI Figure 7. CW vs CCW selection was even throughout the simulations, with\nno preferred direction discovered.\nGradient Gliding\nFor large colloids, some took translate for most inputs, only rotating for minimal changes in the\ngradient, such as those occurring when far away from the source or moving equipotential around it. Even in these\ncases, the strategy is inconsistent and still has a high probability of translation. While this strategy might appear strange\ninitially, it helps to consider regions where the gradient will be so slight. As the agents are initialised in the simulation,\nthey will sit in a region with minimal gradient. Upon moving around this area, they will likely rotate until they are\ndragged into a region where the gradient increases enough for it to begin translating. This was common amongst the\nchosen policies, with somewhere between 7 % and 12 % opting for this approach. The discrepancy in percentage\narises due to the spurious policies discussed in a later section. We label this policy Gradient Gliding as the colloids\ngenerally follow a translation path with very small adjustments made under low gradient changes. An example plot of\nthis strategy can be seen in SI Figure 7, row three.\n10\nEmergence of Chemotactic Strategies with Multi-Agent Reinforcement Learning\nBrownian Piloting\nAn alternative policy, referred to here as Brownian Piloting, was seen particularly in the case\nof smaller agents where rotational Brownian motion overcomes the active rotation. The agents learned to do nothing\nwhen experiencing negative network inputs and to translate if they see a positive one. In this way, the agents do not\nfight against the Brownian forces when their active motion cannot overcome it. As this was only seen in the small\nagents, it is clear that such a policy could be more optimal. However, it does demonstrate that small, weak agents can\nstill successfully learn to navigate toward sources of nutrition. Overall, this policy was adopted in 3.6 % of cases. One\ncan see Brownian Piloting in the fourth row of SI Figure 7.\nExotic policies (EP)\nAs was previously mentioned, approximately 5.9 % of the emergent policies were not well\nmapped into single classes. However, we can identify several so-called exotic policies by manually looking at the action\nprobabilities.\n• EP 1: In these cases, the agents only translated when their input was negative. Otherwise, they chose to\ndo nothing. This was only observed in small agents where random fluctuations significantly impacted their\nrotation more than active swimming.\n• EP 2: In this approach, we saw the agents choose the Do Nothing action at almost all times, except when\nthe input to the network was a small positive gradient, at which point they would translate. This approach\noccurred for small colloids where all random forces outnumbered their active swimming. This indicates that\nmicroswimmers can survive in cases where their swimming is overpowered, effectively using their environment\nto perform successful chemotaxis. Whilst extremely inefficient, this swim strategy preserves energy and\nsuccessfully allows the agents to perform chemotaxis despite an almost impossible condition.\n• Combinations: We noticed combinations of other, more common approaches in many exotic policies. For\nexample, some smaller colloids in Brownian-dominated regimes performed active rotation in both CW and\nCCW directions for negative inputs and translate for positive. We identify this policy as more or less equivalent\nto Brownian Piloting as the active rotation will not yield more than simply sitting still. In other cases,\nthe onset of translation was delayed or accelerated, yielding slight variations of Run and Rotate. As these\npoints combined mixtures of the more dominant policies and yet did not occur often, the clustering algorithms\ncould not successfully separate them into distinct classes.\nThe results tell us that in the cases where active translation and rotation are possible and dominate over Brownian\neffects, the agents often learn to perform a run-and-rotate trajectory. In cases where Brownian effects dominate active\nmotion, the agents learn to adapt to this environment by performing only actions that move them into a new environment,\nby using the Brownian forces to their advantage. Interestingly, these policies are not well differentiated within the\ntrajectories alone; only by looking at the neural networks can we see how the agents make decisions. Such an insight\nmight guide us in understanding how real-world bacteria navigate their environments, and perhaps, how to disrupt, as in\nquorum quenching [Grandcl´ement et al., 2015], or support, as in quorum enhancement [Garc´ıa-Contreras et al., 2014],\nthis navigation.\n5\nConclusion\nIn this study, we have tested the role of size and swim speed on the emergent strategy of microscopic active agents\nlearning chemotaxis via multi-agent actor-critic reinforcement learning. Our simulations demonstrated that intelligent\nagents can learn chemotactic behaviour, even in environments where Brownian random forces begin to dominate\ntheir active motion. In such regimes, we found that the chemotaxis was not optimal in terms of their equilibrium\ndistance from the source of the chemical gradient or the speed at which they made it to the source. However, they\ncould consistently reach their target. Interestingly, we saw that as the P´eclet number grew above one and active motion\ndominated the Brownian forces and torques, the learned policies also converged quickly to a similar equilibrium\ndistance and time. After studying the policy efficiency, we looked into the strategies adopted by the agents to perform\nchemotaxis. We identified three dominant strategies which we named Run and Rotate, Gradient Gliding, and\nBrownian Piloting. The first policy occurred most often (83.5 %) but predominantly in cases where the colloids\nwere large enough to no longer be in regions where rotational or translation Brownian motion overcame their active\nswimming. This strategy involved translating as long as the input to the agent was positive, i.e., moving towards the\nsource and rotating if it was negative. The second most common strategy ( 7.1 % emergence), also occurring in larger\ncolloids, was to translate for most of the time whether the input was negative or positive, and only when the input\nto the network was small would they sometimes choose to rotate. This strategy meant that the colloids spent a long\ntime rotating when they were far away from the source but translating when they identified the direction of the source.\nThe final common strategy occurred when the rotational and sometimes translational Brownian motion dominated the\nactive swimming. In these cases, the agents would perform the Do Nothing action while the input to the network was\n11\nEmergence of Chemotactic Strategies with Multi-Agent Reinforcement Learning\nnegative, and only when it was positive would they begin translating. We identified further policies, including a kind\nof lazy swimming where the agents performed no actions except when they were far away from the source and the\ninput to the network was weakly positive, as well as some cases where agents learned to rotate in both CW and CCW\ndirections but often in a regime where this was not useful. Overall, we have identified that reinforcement learning\ncan replicate natural behaviour of organisms. However, it can also provide insight into biological swimmers’ possible\nstrategies and may provide a path forward for exploiting this knowledge. A further point of interest would be to identify\nnatural biological swimmers who have evolved such swimming patterns or can outperform the emergent strategies of\nthe RL agents.\n6\nData and Availability\nAll data can be made available upon reasonable request to the authors and, upon publication, will be made publicly\navailable through the DaRUS service.\n7\nAcknowledgements\nC.H and S.T acknowledge financial support from the German Funding Agency (Deutsche Forschungsgemeinschaft\nDFG) under Germany’s Excellence Strategy EXC 2075-390740016, and S. T was supported by an LGF stipend of the\nstate of Baden-W¨urttemberg. C.H and S.T acknowledge financial support from the German Funding Agency (Deutsche\nForschungsgemeinschaft DFG) under the Priority Program SPP 2363. C.H and C.L acknowledge funding by the\nDeutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Project Number 327154368-SFB 1313.\nThe authors would like to acknowledge funding from the Deutsche Forschungsgemeinschaft (DFG, German Research\nFoundation) Compute Cluster grant no. 492175459.\nReferences\nK. Arulkumaran, M. P. Deisenroth, M. Brundage, and A. A. Bharath. Deep reinforcement learning: A brief survey.\nIEEE Signal Processing Magazine, 34(6):26–38, 2017. doi: 10.1109/MSP.2017.2743240.\nA. G. Barto, R. S. Sutton, and C. W. Anderson. Neuronlike adaptive elements that can solve difficult learning control\nproblems. IEEE Transactions on Systems, Man, and Cybernetics, SMC-13(5):834–846, 1983. doi: 10.1109/TSMC.\n1983.6313077.\nJ. Bastos-Arrieta, A. Revilla-Guarinos, W. E. Uspal, and J. Simmchen. Bacterial biohybrid microswimmers. Frontiers\nin Robotics and AI, 5, 2018. ISSN 2296-9144. doi: 10.3389/frobt.2018.00097. URL https://www.frontiersin.\norg/articles/10.3389/frobt.2018.00097.\nH. Berg. coli in motion2004springer. New York, 2004.\nH. C. Berg and D. A. Brown. Chemotaxis in escherichia coli analysed by three-dimensional tracking. Nature, 239(5374):\n500–504, Oct 1972. ISSN 1476-4687. doi: 10.1038/239500a0. URL https://doi.org/10.1038/239500a0.\nJ. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula, A. Paszke, J. VanderPlas,\nS. Wanderman-Milne, and Q. Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL\nhttp://github.com/google/jax.\nA. Bren and M. Eisenbach. How signals are heard during bacterial chemotaxis: protein-protein interactions in sensory\nsignal propagation. J Bacteriol, 182(24):6865–6873, Dec. 2000.\nN. C. Darnton, L. Turner, S. Rojevsky, and H. C. Berg. On torque and tumbling in swimming escherichia coli. J\nBacteriol, 189(5):1756–1764, Dec. 2006.\nS. Datta and D. K. Srivastava. Stokes drag on axially symmetric bodies: a new approach. Proceedings - Mathematical\nSciences, 109(4):441–452, Nov 1999. ISSN 0973-7685. doi: 10.1007/BF02838005. URL https://doi.org/10.\n1007/BF02838005.\nJ. Elgeti, R. G. Winkler, and G. Gompper. Physics of microswimmers—single particle motion and collective behavior:\na review. Reports on Progress in Physics, 78(5):056601, apr 2015. doi: 10.1088/0034-4885/78/5/056601. URL\nhttps://dx.doi.org/10.1088/0034-4885/78/5/056601.\nR. Garc´ıa-Contreras, L. Nu˜nez-L´opez, R. Jasso-Ch´avez, B. W. Kwan, J. A. Belmont, A. Rangel-Vega, T. Maeda, and\nT. K. Wood. Quorum sensing enhancement of the stress response promotes resistance to quorum quenching and\nprevents social cheating. ISME J, 9(1):115–125, June 2014.\n12\nEmergence of Chemotactic Strategies with Multi-Agent Reinforcement Learning\nJ. G. Gay and B. J. Berne. Modification of the overlap potential to mimic a linear site–site potential. The Journal of\nChemical Physics, 74(6):3316–3319, 03 1981. ISSN 0021-9606. doi: 10.1063/1.441483. URL https://doi.org/\n10.1063/1.441483.\nC. Grandcl´ement, M. Tanni`eres, S. Mor´era, Y. Dessaux, and D. Faure. Quorum quenching: role in nature and applied\ndevelopments. FEMS Microbiology Reviews, 40(1):86–116, 10 2015. ISSN 0168-6445. doi: 10.1093/femsre/fuv038.\nURL https://doi.org/10.1093/femsre/fuv038.\nS. Gronauer and K. Diepold. Multi-agent deep reinforcement learning: a survey. Artificial Intelligence Review, 55(2):\n895–943, Feb 2022. ISSN 1573-7462. doi: 10.1007/s10462-021-09996-w. URL https://doi.org/10.1007/\ns10462-021-09996-w.\nI. Grondman, L. Busoniu, G. A. D. Lopes, and R. Babuska. A survey of actor-critic reinforcement learning: Standard\nand natural policy gradients. IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and\nReviews), 42(6):1291–1307, 2012. doi: 10.1109/TSMCC.2012.2218595.\nC. H. Hansen, R. G. Endres, and N. S. Wingreen. Chemotaxis in escherichia coli: a molecular model for robust precise\nadaptation. PLoS Comput Biol, 4(1):e1, Nov. 2007.\nB. Hartl, M. H¨ubl, G. Kahl, and A. Z¨ottl. Microswimmers learning chemotaxis with genetic algorithms. Proceedings\nof the National Academy of Sciences, 118(19):e2019683118, 2021. doi: 10.1073/pnas.2019683118. URL https:\n//www.pnas.org/doi/abs/10.1073/pnas.2019683118.\nJ. Heek, A. Levskaya, A. Oliver, M. Ritter, B. Rondepierre, A. Steiner, and M. van Zee. Flax: A neural network library\nand ecosystem for JAX, 2023. URL http://github.com/google/flax.\nD. P. Kingma and J. Ba. Adam: A method for stochastic optimization, 2017.\nS. H. Koenig. Brownian motion of an ellipsoid. a correction to perrin’s results. Biopolymers, 14(11):2421–2423,\n1975. doi: https://doi.org/10.1002/bip.1975.360141115. URL https://onlinelibrary.wiley.com/doi/abs/\n10.1002/bip.1975.360141115.\nS. Lloyd. Least squares quantization in pcm. IEEE Transactions on Information Theory, 28(2):129–137, 1982. doi:\n10.1109/TIT.1982.1056489.\nC. Mo and X. Bian.\nChemotaxis of sea urchin sperm cells through deep reinforcement learning, 2022.\nURL\nhttps://arxiv.org/abs/2209.07407.\nS. Mui˜nos-Landin, A. Fischer, V. Holubec, and F. Cichos. Reinforcement learning with artificial microswimmers.\nScience Robotics, 6(52):eabd9285, 2021. doi: 10.1126/scirobotics.abd9285. URL https://www.science.org/\ndoi/abs/10.1126/scirobotics.abd9285.\nA. G. Murray and G. A. Jackson. Viral dynamics: a model of the effects of size, shape, motion and abundance of\nsingle-celled planktonic organisms and other particles. Marine Ecology Progress Series, 89(2/3):103–116, 1992.\nISSN 01718630, 16161599. URL http://www.jstor.org/stable/24831780.\nM. Musy, G. Jacquenot, G. Dalmasso, J. Lee, R. de Bruin, J. Soltwedel, M. Tulldahl, Z.-Q. Zhou, RobinEnjalbert,\nA. Pollack, B. Hacha, F. Claudi, C. Badger, X. Lu, A. Sol, A. Yershov, B. Sullivan, B. Lerner, D. Hrisca, D. Volpatto,\nEvan, F. Matzkin, JohnsWor, mkerrinrapid, N. Schl¨omer, RichardScottOZ, and O. Schneider. marcomusy/vedo:\n2023.5.0, Nov. 2023. URL https://doi.org/10.5281/zenodo.4587871.\nF. A. Oliehoek, C. Amato, et al. A concise introduction to decentralized POMDPs, volume 1. Springer, 2016.\nF. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss,\nV. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn:\nMachine learning in Python. Journal of Machine Learning Research, 12:2825–2830, 2011.\nR. A. X. Persson. Note: Modification of the Gay-Berne potential for improved accuracy and speed. The Journal of\nChemical Physics, 136(22):226101, 06 2012. ISSN 0021-9606. doi: 10.1063/1.4729745. URL https://doi.org/\n10.1063/1.4729745.\nM. Schnitzer, S. Block, H. Berg, and E. Purcell. Biology of the chemotactic response (armitage, jp & lackie, jm eds)\n15–34, 1990.\nJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms, 2017.\nR. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. The MIT Press, second edition, 2018. URL\nhttp://incompleteideas.net/book/the-book-2nd.html.\nR. S. Sutton, D. McAllester, S. Singh, and Y. Mansour. Policy gradient methods for reinforcement learning with\nfunction approximation. In S. Solla, T. Leen, and K. M¨uller, editors, Advances in Neural Information Processing\nSystems, volume 12. MIT Press, 1999.\n13\nEmergence of Chemotactic Strategies with Multi-Agent Reinforcement Learning\nS. Tovey, C. Lohrmann, D. Zimmer, S. Koppenhoefer, and T. Merkt. SwarmRL, 2023a. URL https://github.com/\nSwarmRL/SwarmRL.\nS. Tovey, D. Zimmer, C. Lohrmann, T. Merkt, S. Koppenhoefer, V.-L. Heuthe, C. Bechinger, and C. Holm. Environ-\nmental effects on emergent strategy in micro-scale multi-agent reinforcement learning, 2023b.\nL. Turner, W. S. Ryu, and H. C. Berg. Real-time imaging of fluorescent flagellar filaments. Journal of bacteriology,\n182(10):2793–2801, 2000.\nL. van der Maaten and G. Hinton. Visualizing data using t-sne. Journal of Machine Learning Research, 9(86):\n2579–2605, 2008. URL http://jmlr.org/papers/v9/vandermaaten08a.html.\nG. H. Wadhams and J. P. Armitage. Making sense of it all: bacterial chemotaxis. Nature Reviews Molecular Cell\nBiology, 5(12):1024–1037, Dec 2004. ISSN 1471-0080. doi: 10.1038/nrm1524. URL https://doi.org/10.\n1038/nrm1524.\nN. Watari and R. G. Larson. The hydrodynamics of a run-and-tumble bacterium propelled by polymorphic helical\nflagella. Biophys J, 98(1):12–17, Jan. 2010.\nJ. D. Weeks, D. Chandler, and H. C. Andersen. Role of repulsive forces in determining the equilibrium structure of\nsimple liquids. The Journal of chemical physics, 54(12):5237–5247, 1971.\nF. Weik, R. Weeber, K. Szuttor, K. Breitsprecher, J. de Graaf, M. Kuron, J. Landsgesell, H. Menke, D. Sean, and\nC. Holm. Espresso 4.0 – an extensible software package for simulating soft matter systems. The European Physical\nJournal Special Topics, 227(14):1789–1816, Mar 2019. ISSN 1951-6401. doi: 10.1140/epjst/e2019-800186-9. URL\nhttps://doi.org/10.1140/epjst/e2019-800186-9.\n.1\nAnisotropic Friction Coefficients\nIn the case of the anisotropic agents, the translational friction coefficients are more challenging. Consider a spheroidal\nagent with axial semiaxis, rax and equatorial semiaxis, req. Let\na = max(rax, req),\ne =\ns\n1 −\n\u0012 req\nrax\n2\u0013\n,\nL = log\n\u00121 + e\n1 −e\n\u0013\n.\n(17)\nThe translational friction can now be defined following the approach taken by Datta and Srivastava [1999] where for a\nprolate particle\nγax\nt\n= 16πµae3 \u0002\u00001 + e2\u0001\nL −2e\n\u0003−1\n(18)\nis the axial friction coefficient and\nγeq\nt\n= 32πµae3 \u0002\n2e + (3e2 −1)L\n\u0003−1\n(19)\nthe equatorial, and for an oblate particle\nγax\nt\n= 8πµae3 h\ne(1 −e2)\n1\n2 −(1 −2e2) sin−1 e\ni−1\n(20)\nγeq\nt\n= 16πµae3 h\n(1 + 2e2) sin−1 e −e(1 −e2)\n1\n2\ni−1\n(21)\nare the axial and equatorial friction coefficients respectively. Rotational friction factors were compute using the Perrin\nfactors Koenig [1975]. To do so, we introduce further the aspect ratio\np = rax\nreq\n,\n(22)\nand\nξ =\np\n|p2 −1|\np\n.\n(23)\nWith these two equations, we can derive the Perrin S factors for prolate\nSprolate = 2tanh−1 ξ\nξ\n(24)\nand oblate\nSoblate = 2tan−1 ξ\nξ\n(25)\n14\nEmergence of Chemotactic Strategies with Multi-Agent Reinforcement Learning\nparticles respectively. Finally, we define the generic rotational friction coefficient for an equivalent sphere\nγsphere = 8πµraxr2\neq.\n(26)\nWith these definitions, the equatorial and axial friction coefficients for both prolate and oblate particles can be derived as\nprolate/oblateγeq\nr = 4\n3\n1\np2 −p2\n2 −Sprolate/oblate\n\u0014\n2 −\n\u0010\n1\np\n\u00112\u0015\n(27)\nand\nprolate/oblateγax\nr\n= 4\n3\np2 −1\n2p2 −Sprolate/oblate .\n(28)\nFor anisotropic particles, the translational friction coefficient in Equation (1) becomes a tensorial quantity with\nγt = diag(γeq\nt , γax\nt ) in the comoving frame of reference of each particle in which the second coordinate points along\nthe director e(t). Since particles are fixed to two dimensions, rotation happens only around an equatorial axis such\nthat for anisotropic particles γr = γeq\nr in eq. (2). The ESPResSo [Weik et al., 2019] simulation package is used to\nnumerically solve Equations (1) and (2) with a time-step δt = 0.005 s, the actions that determine F(t) and m(t) are\nupdated every time slice ∆t = 0.1 s. In all cases, unless otherwise specified, when referring to time in this investigation,\nwe refer to the time slice, i.e., the number of times an action is computed for each agent in the simulation.\n.2\nAnisotropic Interaction Potential\nFor the prolate and oblate agents, we use the modified Gay-Berne potential for anisotropic particles Persson [2012],\nGay and Berne [1981]\nV (ui, uj, rij) =\n\n\n\nϵ (ui, uj, rij)\n\u0014\u0010\nσ0\nrij−σ(ui,uj,rij)+σ0\n\u001112\n−\n\u0010\nσ0\nrij−σ(ui,uj,rij)+σ0\n\u00116\u0015\n,\nrij < 4σ max(l, l−1),\n0,\nelse,\n(29)\nwhere rij is the vector distance between the center of mass of each particle, ui is the direction vector of the ith particle,\nand ϵ (ui, uj, rij) and σ (ui, uj, rij) are additional functions depending on the orientations of the particles. We use the\naugmented forms of these function where\nϵ (ui, uj, rij) = ϵ0\n\u0014\n1 + d−1 −1\n2\n(||rij · u1||2 + ||rij · u2||2)\n\u0015\n(30)\nand\nσ (ui, uj, rij) = σ0\n\u0014\n1 + l −1\n2\n(||rij · u1||2 + ||rij · u2||2)\n\u0015\n,\n(31)\nwhere d is ratio between the side-by-side binding energy and the end-to-end binding energy and l is the aspect ration,\nwhich, in this work, was set to 3 for the prolate particles and 1\n3 for the oblates. We choose σ0 = σ and ϵ0 = V0.\n.3\nPolicy Examples\nHere we show example policy diagrams for each policy discussed in the manuscript.\n.4\nShape Studies\nAs discussed in the manuscript, we performed the chemotaxis study for spherical, prolate, and oblate particles. Here we\ndisplay and discuss the results not presented in the main manuscript.\n.4.1\nSphere\nWhile most of the sphere results are presented in the main manuscript, the raw policy plots are presented here. Note, bl\nin the figure stands for body lengths. The sphere policy diagrams outline the majority of the policies discussed in the\nmain text. On the x axis, the change in gradient is plotted and on the y, the colloid shape. The colour of the diagram\nrepresents the probability of an action being taken and each column corresponds to a single action. The rows are the\ndifferent swim speeds descending from one to five. The diagrams show the forbidden region in the chemotaxis below\napproximately 0.5µm. After this point, we see the emergence of non-zero probabilities as the networks have learned to\nperform chemotaxis.\n15\nEmergence of Chemotactic Strategies with Multi-Agent Reinforcement Learning\n.4.2\nOblate\nThe oblate particles demonstrated similar behaviour to the spherical colloids, not showing any unique policy deviations.\n.4.3\nProlate\nThe prolate simulations were similar to both the spherical and oblate studies with the exception of one very unique\npolicy. Mentioned in the main manuscript, we found an agent that appeared to perform no actions until the input to\nthe network was small and positive. Upon receiving such an input, the agent would translate. More interestingly, this\noccured for a particle of size around 0.3µm, far below the theoretical boundaries for random forces to begin dominating\nthe motion. This was also the smallest agent in all simulations capable of achieving chemotaxis.\n16\nEmergence of Chemotactic Strategies with Multi-Agent Reinforcement Learning\n10\n5\n0\n5\n10\n0.000\n0.001\n0.002\n0.003\n0.004\n0.005\n0.006\n0.007\nProbability\na) Run and Rotate CW\nCCW\n10\n5\n0\n5\n10\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTranslate\n10\n5\n0\n5\n10\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCW\n10\n5\n0\n5\n10\n0.000\n0.001\n0.002\n0.003\n0.004\n0.005\n0.006\nDo Nothing\n10\n5\n0\n5\n10\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nProbability\nb) Run and Rotate CCW\n10\n5\n0\n5\n10\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n10\n5\n0\n5\n10\n0.000\n0.001\n0.002\n0.003\n0.004\n10\n5\n0\n5\n10\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n10\n5\n0\n5\n10\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150\n0.175\nProbability\nc) Gradient Gliding\n10\n5\n0\n5\n10\n0.800\n0.825\n0.850\n0.875\n0.900\n0.925\n0.950\n0.975\n1.000\n10\n5\n0\n5\n10\n0.000\n0.005\n0.010\n0.015\n0.020\n10\n5\n0\n5\n10\n0.00000\n0.00025\n0.00050\n0.00075\n0.00100\n0.00125\n0.00150\n0.00175\n10\n5\n0\n5\n10\nConcentration Change\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\nProbability\nd) Brownian Piloting\n10\n5\n0\n5\n10\nConcentration Change\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n10\n5\n0\n5\n10\nConcentration Change\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n10\n5\n0\n5\n10\nConcentration Change\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure 7: Examples of each emergent policy found during the investigation. a) and b) are the run and rotate policy for\nCW and CCW directions. You can see that as the input to the network becomes negative, the agents decide to rotate and\nas they are moving towards the source, they translate. c) The Just run, rarely rotate policy where for most inputs, the\nagent will translate, but far away from the source, when the input is small, the agent may also decide to rotate. d) Do\nnothing when negative and run when positive.\n17\nEmergence of Chemotactic Strategies with Multi-Agent Reinforcement Learning\nFigure 8: Emergent policy of the spherical microswimmers for all speeds and sizes. We see the development of several\nstrategies depending on body size, notably, a run-and-tumble type strategy where colloids will either rotate or translate\ndepending on a decrease or increase in concentration, respectively. Interestingly, once a colloid has learned to rotate\neither CW or CCW, it does not change this direction at any point during the runs.\n18\nEmergence of Chemotactic Strategies with Multi-Agent Reinforcement Learning\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nColloid Radius / m\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\nSwim Speed / body length s\n1\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nProbability of Emergent Chemotaxis\nFigure 9: The probability of emergent chemotaxis for the oblate particles along with the theoretical boundaries from the\nP´eclet numbers.\n19\nEmergence of Chemotactic Strategies with Multi-Agent Reinforcement Learning\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nColloid Radius / m\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\nSwim Speed / body lengths s\n1\n0\n1000\n2000\n3000\n4000\n5000\n6000\n7000\nTotal reward from training\nFigure 10: Reward phase diagram computer from the oblate simulations. In this case, we see the best training emerges\nagain in the smaller but fast region of the diagram. While the width of the colloids is corrected for, more complex\ngeometric conditions may be impacting these results.\n0.5\n1.0\n1.5\n2.0\n2.5\nColloid Radius / m\n0\n2\n4\n6\n8\n10\n12\n14\nEquilibrium Distance from Source / m\n0.5\n1.0\n1.5\n2.0\n2.5\nColloid Radius / m\n2\n0\n2\n4\n6\n8\n10\nTime to minimum / minutes\nSwim Speed\n1.0\n2.0\n3.0\n4.0\n5.0\nFigure 11: Policy efficacy for the oblate particles appears almost identical to the spherical particles.\n20\nEmergence of Chemotactic Strategies with Multi-Agent Reinforcement Learning\nFigure 12: Emergent policy diagram for the oblate particles.\n21\nEmergence of Chemotactic Strategies with Multi-Agent Reinforcement Learning\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nColloid Radius / m\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\nSwim Speed / body length s\n1\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nProbability of Emergent Chemotaxis\nFigure 13: The probability of emergent chemotaxis for the prolate particles along with the theoretical boundaries from\nthe P´eclet numbers.\n22\nEmergence of Chemotactic Strategies with Multi-Agent Reinforcement Learning\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nColloid Radius / m\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\nSwim Speed / body lengths s\n1\n0\n2000\n4000\n6000\n8000\n10000\n12000\n14000\nTotal reward from training\nFigure 14: Reward phase diagram computer from the prolate simulations. In this case, we see the best training emerges\nagain in the smaller but fast region of the diagram. While the width of the colloids is corrected for, more complex\ngeometric conditions may be impacting these results.\n0.5\n1.0\n1.5\n2.0\n2.5\nColloid Radius / m\n0\n2\n4\n6\n8\n10\n12\n14\nEquilibrium Distance from Source / m\n0.5\n1.0\n1.5\n2.0\n2.5\nColloid Radius / m\n0\n2\n4\n6\n8\n10\nTime to minimum / minutes\nSwim Speed\n1.0\n2.0\n3.0\n4.0\n5.0\nFigure 15: Policy efficacy for the prolate particles appears almost identical to the spherical particles.\n23\nEmergence of Chemotactic Strategies with Multi-Agent Reinforcement Learning\nFigure 16: Emergent policy diagram for the prolate particles.\n24\n",
  "categories": [
    "physics.bio-ph",
    "cs.LG",
    "cs.MA"
  ],
  "published": "2024-04-02",
  "updated": "2024-04-02"
}