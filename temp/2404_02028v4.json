{
  "id": "http://arxiv.org/abs/2404.02028v4",
  "title": "QUSL: Quantum Unsupervised Image Similarity Learning with Enhanced Performance",
  "authors": [
    "Lian-Hui Yu",
    "Xiao-Yu Li",
    "Geng Chen",
    "Qin-Sheng Zhu",
    "Hui Li",
    "Guo-Wu Yang"
  ],
  "abstract": "Leveraging quantum properties to enhance complex learning tasks has been\nproven feasible, with excellent recent achievements in the field of\nunsupervised learning. However, current quantum schemes neglect adaptive\nadjustments for unsupervised task scenarios. This work proposes a novel quantum\nunsupervised similarity learning method, QUSL. Firstly, QUSL uses similarity\ntriplets for unsupervised learning, generating positive samples by perturbing\nanchor images, achieving a learning process independent of classical\nalgorithms. Subsequently, combining the feature interweaving of triplets, QUSL\nemploys metaheuristic algorithms to systematically explore high-performance\nmapping processes, obtaining quantum circuit architectures more suitable for\nunsupervised image similarity tasks. Ultimately, QUSL realizes feature learning\nwith lower quantum resource costs. Comprehensive numerical simulations and\nexperiments on quantum computers demonstrate that QUSL outperforms\nstate-of-the-art quantum methods. QUSL achieves over 50% reduction in critical\nquantum resource utilization. QUSL improves similarity detection correlation by\nup to 19.5% across multiple datasets, exhibiting robustness in NISQ\nenvironments. While using fewer quantum resources, QUSL shows potential for\nlarge-scale unsupervised tasks.",
  "text": "QUSL: Quantum Unsupervised Image Similarity\nLearning with Enhanced Performance\nLian-Hui Yua, Xiao-Yu Li†b, Geng Chenc, Qin-Sheng Zhua, Hui Lid,\nGuo-Wu Yangc,e\naSchool of Physics, University of Electronic Science and Technology of China, Cheng\nDu, 610054, Si Chuan, China\nbSchool of Information and Software Engineering, University of Electronic Science and\nTechnology of China, Cheng Du, 610054, Si Chuan, China\ncSchool of Computer Science and Engineering, University of Electronic Science and\nTechnology of China, Cheng Du, 610054, Si Chuan, China\ndJILA and Department of Physics, University of\nColorado, Boulder, 80309-0440, Colorado, USA\neAdvanced Cryptography and System Security Key Laboratory of Sichuan\nProvince, Cheng Du, 610103, Si Chuan, China\nAbstract\nLeveraging quantum properties to enhance complex learning tasks has been\nproven feasible, with excellent recent achievements in the field of unsuper-\nvised learning. However, current quantum schemes neglect adaptive adjust-\nments for unsupervised task scenarios. This work proposes a novel quan-\ntum unsupervised similarity learning method — QUSL. Firstly, QUSL uses\nsimilarity triplets for unsupervised learning, generating positive samples by\nperturbing anchor images, achieving a learning process independent of classi-\ncal algorithms. Subsequently, combining the feature interweaving of triplets,\nQUSL employs metaheuristic algorithms to systematically explore high- per-\nformance mapping processes, obtaining quantum circuit architectures more\nsuitable for unsupervised image similarity tasks. Ultimately, QUSL realizes\nfeature learning with lower quantum resource costs. Comprehensive numer-\nical simulations and experiments on quantum computers demonstrate that\nQUSL outperforms state-of-the-art quantum methods. QUSL achieves over\n50% reduction in critical quantum resource utilization. QUSL improves sim-\nilarity detection correlation by up to 19.5% across multiple datasets, exhibit-\ning robustness in NISQ environments. While using fewer quantum resources,\nQUSL shows potential for large-scale unsupervised tasks.\nPreprint submitted to Expert Systems with Applications\nAugust 23, 2024\narXiv:2404.02028v4  [quant-ph]  22 Aug 2024\nKeywords:\nQuantum machine learning, Image similarity detection,\nMetaheuristic algorithms, Unsupervised learning\n1. Introduction\nQuantum computing represents a paradigm shift in computational capa-\nbilities and has entered a stage known as Noisy Intermediate-Scale Quantum\n(NISQ) computing Knill (2010); Preskill (2018). Quantum resources possess\ninvaluable potential, and quantum computing demonstrates more apparent\nadvantages when dealing with complex tasks Arute et al. (2019); Huang et al.\n(2022, 2021); Li et al. (2024). Quantum Machine Learning (QML) Biamonte\net al. (2017) has emerged as a noteworthy research focus in the field of im-\nage processing, giving rise to representative models such as quantum neural\nnetworks (QNN) Schuld et al. (2014, 2015); Cong et al. (2019). QML boasts\nseveral advantages Schuld et al. (2014); Xu et al. (2021), one of which is its\nability to utilize a larger feature space, providing enhanced capabilities for\nmodeling complex patterns in image processing.\nOne particularly challenging task in the field of image processing is image\nsimilarity detection, which has far-reaching implications for complex appli-\ncations such as target tracking and object recognition in real-world scenarios\nWang et al. (2023); Cheng et al. (2017). It can be conceptualized as instances\nof image multiclassification problems characterized by a plethora of unknown\nclasses, each containing only one member. Classical approaches encounter\nprominent challenges. With increasing task scale, the complexity and com-\nputational demands of image similarity detection escalate rapidly. This lim-\nits the feasibility of employing simple and intuitive algorithms Zhang et al.\n(2011); Palubinskas (2017) in large-scale tasks, while the use of data dimen-\nsionality reduction techniques inevitably leads to information loss. This in-\ndirectly affects the generalization ability of unsupervised and self-supervised\nlearning schemes Bai et al. ((2019); Wang et al. ((2014), resulting in unac-\nceptable learning costs for large-scale, high-resolution image datasets.\nDue to the potential of quantum computing in image processing tasks,\nthe first wave of quantum similarity algorithms Dang et al. (2018); Liu et al.\n(2019) and quantum machine learning models Zhou and Sun (2015); Yan\net al. ((2012) specifically tailored to address challenges in image similarity\nanalysis have been developed. Series works have focused on establishing un-\nsupervised learning processes by referencing classical models. Jaderberg et\n2\nal. Jaderberg et al. (2022) adopted SimCLR’s unsupervised learning strat-\negy Chen et al. (2020), enhancing the original network using QNNs. Silver et\nal.’s research Silver et al. ((2023) introduced the first quantum unsupervised\nimage similarity model, employing tripletsMa et al. (2020) from contrastive\nlearning to encode image pairs and likewise utilizing QNNs for feature ex-\ntraction. Despite demonstrating promising performance, these works over-\nlooked the necessity of adapting quantum learning circuits to suit specific\ntask requirements. The critical challenge in employing quantum computing\nfor image similarity tasks lies in the design and training of high-performance\nquantum circuits that are tailored to a specific task scenarios.\nThe design of quantum circuits for learning scenarios presents a special-\nized optimization challenge Wang et al. (2022); Mohseni et al. (2022), aiming\nto minimize the usage of entanglement gates within the circuit structure’s\nshallowest possible depth while achieving superior expressive powerSim et al.\n(2019). Under such task scenarios, obtaining gradient information and en-\nsuring differentiability are challenging, at times entirely unknown for quan-\ntum circuits involving intricate quantum state evolution processes. Conse-\nquently, the introduction of metaheuristic algorithms with self-organization\nand self-learning characteristics has become an efficient approach for ob-\ntaining more advanced quantum circuits Zhang and Zhao (2023); Ding and\nSpector ((2022); Krylov and Lukac ((2019); Rasconi and Oddi ((2019).\nIn light of the aforementioned challenges and limitations, our research\nproposes a novel quantum unsupervised similarity learning framework —\nQUSL. The core motivation of QUSL is to fully harness the high expressive\npower of quantum circuits to more effectively process unsupervised image\nsimilarity tasks, breaking free from the reliance on classical image similar-\nity algorithms and the performance constraints imposed by parameterized\nquantum circuit templates.\nQUSL introduce an evolutionary algorithm-driven method to explore\nquantum circuit architectures customized for dataset features. This approach\nenables the automatic design of quantum circuits that are more efficient than\nfixed-template parameterized quantum circuits, effectively capturing the in-\ntricate patterns within image datasets and enhancing the performance of\nimage similarity detection. Subsequently, we propose a perturbation-based\nstrategy for constructing similarity triplets, allowing QUSL to independently\nlearn and detect image similarities without relying on classical algorithms,\nfully leveraging the advantages of quantum computing. QUSL provides a\nmore generalizable and scalable solution for image similarity detection.\n3\nThe highlights of our research can be summarized as follows:\n1. QUSL harnesses an evolutionary algorithm-driven method to explore\nquantum circuit architectures customized for dataset features, achiev-\ning more efficient quantum image feature extraction with reduced cir-\ncuit complexity and learning costs, demonstrating cross-scenario trans-\nferability. The code for QUSL is available.\n2. Leveraging quantum circuit properties, QUSL incorporates perturbed\nimages to independently construct quantum A, P, N-triplets for unsu-\npervised image similarity detection, offering an adjustable similarity\nthreshold without relying on classical algorithms.\n3. Numerical simulations and experiments on quantum computers across\nfive datasets demonstrate QUSL’s superiority over state-of-the-art quan-\ntum methods, reducing critical quantum resource utilization by over\n50% while enhancing similarity detection correlation up by 19.5%, show-\ncasing its performance advantage in near-realistic scenarios.\n2. Background\n2.1. Qubits and Quantum Circuits\nAt the core of quantum computing lies the quantum bit (qubit), which\nserves as the fundamental unit of information in quantum computation. Un-\nlike classical bits in binary states 0 and 1, A quantum bit |ψ⟩can simul-\ntaneously exist in multiple states due to the principle of superposition, as\ndepicted below\n|ψ⟩= α|0⟩+ β|1⟩\ns.t α, β ∈C & ||α||2 + ||β||2 = 1.\n(1)\nUpon measurement, a probability output is generated, where ||α||2 as |0⟩\nand with a probability of ||β||2 as |1⟩. The evolution of quantum bits can be\ndescribed using unitary transformations acting on single or multiple qubits,\nwhere the former allows for arbitrary superposition, and the latter entangles\nmultiple qubits into more complex quantum systems. These unique quantum\ncharacteristics grant access to an infinite-dimensional Hilbert space, which is\nthe fundamental source of the exceptional computational power of quantum\ncomputing.\nApplying a series of quantum gates to quantum bits yields a quantum\ncircuit, and the design of any quantum computing task involves crafting\n4\na specific functional quantum circuit.\nNISQ quantum computers exhibit\nheightened sensitivity to larger circuit depths, thus emphasizing lower depths,\nminimal quantum gate counts, and more efficient utilization of quantum en-\ntanglement resources in quantum circuit design Eisert (2021).\n2.2. Quantum Machine Learning and Parametrized Quantum Circuit\nQML is a method of performing machine learning tasks using quantum\ncomputers. In QML, parametrized quantum circuit (PQC) is a commonly\nused quantum circuit structure that can adapt to various types of scenes\nBenedetti et al. (2019).\nSimilar to selecting appropriate neural network architectures in classical\ndeep learning tasks, choosing the right PQC template is a challenging task.\nThese templates lack any unique handling of datasets or use cases, poten-\ntially leading to circuit redundancies and impacts on parallel quantum circuit\nexecution, which are particularly pronounced in multi-qubit tasks. Typically,\nthe parameterized quantum gates are connected with multiple CNOT gates\nin a ladder-like fashion to form a parameterized layer, and multiple layers of\nPQCs are cascaded to enhance the expressivity of the overall ansatz.\nAnother prominent feature of PQC is that the learning process is reflected\nin the continuous adjustment of parameters, implying that quantum circuits\nneed to be repeatedly executed with the same circuit depth a large number of\ntimesP´erez-Salinas et al. (2020). While some effective strategies can mitigate\nthis issue Cerezo et al. (2021), the accumulated total depth remains a key\nburden in large-scale learning tasks.\n2.3. Heuristic Quantum Circuit Design\nWhile templated PQCs offer versatility, they overlook the characteristics\nof datasets in task scenarios and exhibit redundancy in circuit resources.\nDesigning quantum circuits with higher performance is one of the core tasks\nin quantum computing.\nThe formulation of quantum circuit structures embodies an optimization\nproblem, governed by constraints such as the count of quantum physical\nqubits, adherence to the principles of quantum mechanics for logical consis-\ntency, evolution depth, and the composition and quantities of quantum gates.\nIn practical contexts, careful attention must also be paid to the characteris-\ntics of the quantum dataset under examination. From this perspective, the\nuniversal variational quantum circuit templates, which disregard the content\n5\nof the data, essentially undertake only an initial optimization endeavor by\nsimplifying and overlooking numerous constraints.\nEvolutionary algorithms have emerged as powerful tools for automatically\ngenerating optimal quantum circuits tailored to specific datasets. The study\nof evolutionary strategies has matured significantly Zhang and Zhao (2023);\nKrylov and Lukac ((2019); Arufe et al. (2022) finding widespread applications\nin diverse fields such as combinatorial optimization Arufe et al. (2023) and\nimage quantum processing Altares-L´opez et al. (2021).\nMore specifically, evolutionary algorithms need to select a set of candi-\ndate quantum gates and appropriately describe the topology of the quantum\ncircuit to adaptively design circuits. Both of these significantly influence the\nalgorithm’s performance Wu et al. ((2023). By strategically evolving, new\nindividuals with higher fitness are obtained until the optimal quantum circuit\nis found or a predetermined endpoint is reached.\n2.4. Unsupervised Quantum Approach for Image Similarity\nSilver et al.’s groundbreaking work Silver et al. ((2023) introduced the\nfirst quantum learning approach for effective unsupervised image similarity\ndetection on NISQ quantum computers. SliQ utilizes triplets to encode im-\nage pairs, followed by similarity learning using variational quantum circuits,\nand enhances model stability through a projection variance loss function,\nachieving promising performance on the landscape dataset. SliQ adopted an\nearly contrastive learning strategy, achieving a cost-effective transfer to the\nquantum domain.\nHowever, the process of constructing triplets in SliQ relies on predefining\nthe Euclidean distance to specify image similarity before training begins. On\none hand, performing a large number of classical image similarity calculations\nseverely limits the potential advantages of this quantum model. On the other\nhand, while SliQ employs a parametrized quantum circuit as the model ansatz\nand adopts a multi-layered hierarchical architecture to enhance its expressive\npower, it still fails to fully exploit the performance advantages of quantum\ncircuits.\n3. Quantum Unsupervised Similarity Learning\nMotivated by fully exploiting the advantages of quantum computing in\nunsupervised image similarity tasks, we propose the QUSL framework, which\nintegrates the strengths of quantum computing and unsupervised learning.\n6\nBuilding upon this motivation, QUSL achieves effective generation of quan-\ntum A, P, N-triplets without relying on classical algorithms and introduces\na heuristic learning process to systematically explore the solution space for\nrealizing high-performance quantum feature mappings.\n3.1. A, P, N-Triplet Construction and Quantum Embedding\nA, P, N-triplets are a key concept in unsupervised learning, forming the\nbasis for numerous mainstream self-supervised learning models Veit et al.\n((2017); He et al. ((2020).\nSpecifically, each triplet comprises an anchor\nimage, a positively labeled similar sample, and a negatively labeled dissimilar\nsample.\nDuring the learning process, these triplets are mapped to high-\ndimensional vectors in the embedding space, learning sample features by\ncapturing the distance between vectors.\nSliQ employs triplets as a modeling process for classical image quanti-\nzation, as illustrated in the lower part of Fig.1. In this process, SliQ uses\nEuclidean distance as an evaluation metric to select positive and negative\nsamples. This approach incurs significant additional classical costs in quan-\ntum tasks involving large-scale, high-resolution unlabeled image datasets,\nthus limiting the method’s advantages.\nQUSL uses the method of feature interweaving generated by perturbing\npositive images to improve mapping efficiency and eliminate the use of the\noracle of visual concept. After anchor image selection, data augmentation is\nperformed by applying slight perturbation to the anchor image as follow\nI[c, i, j] = I[c, i, j] + N(0, σ2)[c, i, j],\n(2)\nwhere I[c, i, j] denotes the anchor image with pixel index (i, j) and RGB\nchannel information c, I represents the positive sample image, and N(0, σ2)\nsignifies Gaussian noise with a mean of 0 and variance of σ2. Gaussian noise,\nas a widely representative form of noise, exhibits stable interference effects on\nimage features Hendrycks and Dietterich (2019), making it a universal source\nof perturbation. This method introduces similarity differences to the anchor\nimage through noise perturbation, using these as positive samples.\nThis\napproach draws inspiration from SimCLR’s data augmentation strategy Chen\net al. (2020), greatly reducing the cost of constructing triplets. In addition,\nQUSL adopts the strategy of utilizing feature interweaving to reduce the\nmapping process.\nIn the encoding phase, QUSL uses quantum amplitude embedding Reben-\ntrost et al. (2014) to encode input features into quantum bits as follows\n7\nA,P,N-triplets and quantum embedding in QUSL\n…\n…\n…\n…\n…\n…\n𝑅𝑖𝑛𝑓\n𝐺𝑖𝑛𝑓\n𝐵𝑖𝑛𝑓\nPixel channel\nvectors\n…\n…\nTraining sets for\nA,P and A,N\nAnchor\nPositive\nNegative\n𝑁(0, 𝜎2)\nConstructing A,P,N-triplets\nvia perturbation\nQuantum \nembedding\n|𝝍〉= ෍\n𝒊=𝟏\n⌈𝒍𝒐𝒈𝟐(𝟔𝑵𝟐)⌉\n𝒑𝒊−𝟏|𝒊−𝟏〉\nRandomly select a tuple\nLandscapes for training\nA,P,N-triplets and learning procress in SliQ\nRandomly select a triplet\nCalculate the Euclidean distance \n2\nAnchor\nPositive\nNegative\n𝑬𝒅𝟏\n𝑬𝒅𝟐\n𝑬𝒅=\n෍\n𝒊=𝟏\n𝒏\n(𝑯𝟏𝒊−𝑯𝟐𝒊)𝟐\nMapping \nresults\n𝑑𝐴𝑃\n𝑑𝐴‘𝑁\n𝑑𝐴𝐴‘\nQuamtum\nlearning model\n𝒍𝒐𝒔𝒔∶= 𝒇(𝒅𝑨𝑷, 𝒅𝑨‘𝑵, 𝒅𝑨𝑨‘)\n𝑅3\n𝑅3\n𝑅3\n𝑅3\n+\n+\n+\n𝑅3\n+\nFigure 1:\nThe preprocessing of A, P, N-triplets involves quantum embedding in QUSL.\nFor comparison, the triplet construction and learning process of SliQ are illustrated below\nthe figure. The specific meanings of the equations in the figure can be found in Eq.2, Eq.3,\nand Eq.7.\n|ψ⟩=\n⌈log26N2⌉\nX\ni=1\npi−1|i −1⟩,\n(3)\nwhere, |ψ⟩represents the quantum state obtained by amplitude embedding a\npair of training set. For an image of size N ∗N, pi represents the i-th element\nof the training set vector, which is a normalized vector of length 6N 2 after\nfeature interweaving of the triplet. Thus, amplitude embedding saves a large\nnumber of quantum bits and preserves the complete image features mapped\nto the Hilbert space without the need for compression methods such as PCA.\nFrom the perspective of quantum information, feature interweaving of images\nin the triplet also results in complex entanglement between different image\nfeatures, thereby augmenting the representation of image features Paine et al.\n(2023). The above process is shown in the Fig.1.\nIn summary, QUSL achieves efficient formal processing and quantumiza-\n8\ntion of classical image features.\n3.2. Dataset-Oriented Quantum Learning Circuit Construction\nHeuristic quantum circuit design methodologies, such as evolutionary\nalgorithms, can holistically address multiple constraints to determine the\nnearly optimal quantum state evolution process. The formulation of fitness\nfunctions for these algorithms should consider the attributes of enhanced\nA, P, N-triplets.\nUnlike conventional triplets Veit et al. ((2017), the quantum version’s\nmapping procedure cannot be simply represented as the projection of three\nvectors onto a two-dimensional plane. This complexity stems from the in-\ncorporation of training sets, which introduces entanglement among image\nfeatures, disturbing the direct correspondence between the mapped vectors\nand the features.\nTo reconcile this disparity, a corrective term must be included in the loss\nfunction for classical mapping to ensure projection coherence Silver et al.\n((2023). Based on the conceptualized quantum circuit model, the training\nsets of A, P, N-triplets are embedded into the Hilbert space and encapsu-\nlated by the probability distributions obtained from measuring the first four\nqubits of the quantum circuit across two mappings. By combining the prac-\ntical significance of fitness and the rectified mapping relationship, the fitness\nfunction, designed to maintain projection consistency, is formulated as fol-\nlows\nFobj :=\n1\nα(lQM) + β∆,\n(4)\nwhere, α and β represent harmonic hyperparameters employed to determine\nthe equilibrium of the correction term. The quantum mapping is intended to\nyield analogous outcomes to the classical mapping process. Therefore, lQM\nis defined as the discrepancy in vector distance as specified in the classical\nA, P, N-triplets method, formulated as\nlQM = (|Apx −Px| + |Apy −Py|) −(|Nx −Anx| + |Ny −Any|),\n(5)\nwhere, Apx, Apy, Px, Py denote the measure expectation of the first to fourth\nqubits of the training sets comprised of anchor images and positive images\nsubsequent to their traversal through the quantum circuit. Similarly, Nx, Ny,\nAnx, Any signify the measure expectation of the first to fourth qubits of the\ntraining sets formed by anchor images and negative images after undergoing\nthe quantum circuit.\n9\nThe correction term ∆is defined as the discrepancy in distance between\nthe vectors derived from the two mappings of anchor images within the two\ntraining sets, expressed as follows\n∆= |Apx −Anx| + |Apy −Any|.\n(6)\nThe fitness function takes into account the attributes of evolutionary algo-\nrithms and mitigates the adverse impacts resulting from the enhanced effi-\nciency of A, P, N-triplets.\nDesigning evolutionary algorithms for quantum circuits poses challenges\ndue to the unique characteristics of quantum circuits, such as performance\nsymmetry and constraints in heterogeneous quantum circuits.\nPerformance symmetry arises from the principle of qubit permutation\nsymmetry, where the numbering and arrangement of qubits do not affect\nthe system’s evolution operator, maintaining system equivalence. This can\nlead to heterogeneous quantum circuits achieving similar or identical perfor-\nmance, hindering population diversity in evolutionary algorithms Nam et al.\n(2018). To mitigate this issue and maintain comprehensive coverage of the\nquantum circuit structure solution space, tournament selection can be em-\nployed, effectively reducing the survival rate of similar redundant individuals\nand balancing population diversity and convergence.\nPerformance constraints, another fundamental issue in quantum circuit\ndesign, require a holistic consideration of various aspects of quantum circuit\nperformance evaluation.\nNon-dominated sorting Fang et al. (2008) cate-\ngorizes the population based on their support, with individuals within the\nsame category being non-dominated. This approach generates diverse trade-\noff solutions, focusing on the Pareto front. When applied to quantum circuit\ndesign, non-dominated sorting simplifies population complexity and prevents\nthe generation of high-depth, complex entangled quantum circuits. The syn-\nergistic application of tournament selection and non-dominated sorting en-\nsures expansive exploration and expedites convergence to local depth. The\ncomprehensive evolutionary strategy is illustrated in Fig. 2.\nIn conclusion, QUSL utilizes a comprehensive evolutionary strategy to\nexplore high-performance quantum circuits that conform to the constraints\nof image similarity tasks while ensuring projection consistency in fitness set-\ntings.\n10\n…\nTraining sets\nfor A,P\nH\n|0〉\nH\n|0〉\nH\n|0〉\nH\n|0〉\nH\n|0〉\nH\n|0〉\nH\n|0〉\nAmplitude Encoding\nRx\n0.41\nRz\n-2.64\nRy\n0.313\nRz\n-1.2\nRx\n3.74\nRx\n2.96\nRx\n2.96\n+\nRY\n4.64\nRZ\n4.78\nRX\n5.88\n+\n𝐴𝑝𝑥\n𝐴𝑝𝑦\n𝑃𝑥\n𝑃𝑦\nH\n|0〉\nH\n|0〉\nH\n|0〉\nH\n|0〉\nH\n|0〉\nH\n|0〉\nH\n|0〉\nAmplitude Encoding\n𝑁𝑥\n𝑁𝑦\n𝐴𝑛𝑥\n𝐴𝑛𝑦\n…\nTraining sets\nfor A,N\n𝑵+𝟏-th Population\n𝑵-th Population\nPopulation Individuals\nHigh-fitness individuals\nTournament\nselection\nH\nRX\nθ\nRY\nθ\nRz\nθ\n+\nCandidate gate-set\nMutation\nFitness Evaluation\nEvolutionary Strategy\nNon-dominated \nsorting\nAll individuals\nSequence Crossover\n𝐹𝑜𝑏𝑗: =\n1\n𝛼𝑙𝑄𝑀+ 𝛽Δ\nfitness\n𝑙𝑄𝑀=\n𝐴𝑝𝑥−𝑃𝑥+ 𝐴𝑝𝑦−𝑃𝑦\n \n−𝑁𝑥−𝐴𝑛𝑥+ 𝑁𝑦−𝐴𝑛𝑦\nΔ = 𝐴𝑝𝑥−𝐴𝑛𝑥+ 𝐴𝑝𝑦−𝐴𝑛𝑦\nEnhance diversity\nFigure 2:\nThe evolutionary strategy for quantum circuits design in QUSL. To compactly\nillustrate the strategy, the figure does not depict the details of the components in the\nevolutionary algorithm.\n3.3. Image Similarity Performance Evaluation for Quantum Model\nIn the domain of unsupervised image similarity assessment and similar\ntasks, correlation analysis is frequently employed as a primary evaluation\nmetric Mei et al. (2023). Correlation analysis evaluates model performance\nby assessing the consistency between the similarity scores generated by the\nmodel and the actual similarity between images, thereby addressing the lim-\nitations associated with directly comparing score disparities among different\nmodels. This classic comparative approach exhibits high versatility and can\nbe directly transferred to the quantum domain.\nIn QUSL, model performance is evaluated by computing the Spearman\ncorrelation coefficient between the similarity assessment scores outputted by\nthe quantum model and the Euclidean distance as follow\nEd =\nN\nX\ni=1\nq\n(R(2i) −R(1i))2 −(G(2i) −G(1i))2 −(B(2i) −B(1i))2,\n(7)\n11\nwhere (R(2i), G(2i), B(2i)) and (R(1i), G(1i), B(1i)) represent the RGB values of\nthe i-th pixel in the reference image and the image to be compared, respec-\ntively. In practical computations, this process is often simplified by utilizing\nmethods such as color frequency histograms to condense pixel information\nfor rapid calculations. In this case, the Euclidean distance can be expressed\nas Ed =\npPn\ni=1(H1i −H2i)2, Here, H1i and H2i represent the channel infor-\nmation in the i-th interval of the two histograms, with n denoting the total\nnumber of intervals in each histogram.\nThe computation of similarity assessment scores follows the following ap-\nproach: The improved A, P, N-triplets method, which employs feature entan-\nglement, is simplified to consider only the scenario involving anchor images\nand comparison images for similarity assessment. By exchanging the tuple\nlabels of the anchor image and the positive image, this approach enables the\nquantification of the similarity between two images. Therefore, the similarity\nassessment scores Ssim are defined as the difference between the vectors ob-\ntained from two mappings of the anchor image and the positive image after\nfeature interweaving as follow\nSsim := F(A(1), A(2), P (1), P (2)),\n(8)\nwhere A(i) and P (i) represent the vectors obtained from the anchor image\nand positive image, respectively, in the i-th mapping. F denotes the oper-\nator acting on the four sets of vector coordinates obtained during the two\nmapping processes, representing the differences in coordinates between the\ntwo mappings. For simplicity, cumulative coordinate differences can be used\nfor computation. When the coordinates from the two mappings are identical,\nSsim takes the minimum value of 0, indicating complete similarity between\nthe anchor image and the positive image. Conversely, when significant co-\nordinate changes occur, Ssim takes large positive values, with the numerical\nmagnitude positively correlated with the degree of difference between the an-\nchor image and the positive image. The range of Ssim and its correlation with\nimage similarity are analogous to the Euclidean distance Ed, thus making it\nsuitable for Spearman correlation computation. Let S ∗rg denote the sorted\ncollection of S ∗sim series outputted by the quantum model for a sequence\nof n pairs of similarity detection images, and let EDrg denote the sorted\ncollection of Euclidean distances Ed obtained for the corresponding image\n12\n𝑙𝑄𝑀=\n𝐴𝑝𝑥−𝑃𝑥+ 𝐴𝑝𝑦−𝑃𝑦\n \n−𝑁𝑥−𝐴𝑛𝑥+ 𝑁𝑦−𝐴𝑛𝑦\nΔ = 𝐴𝑝𝑥−𝐴𝑛𝑥+ 𝐴𝑝𝑦−𝐴𝑛𝑦\n𝑵-th Population\n𝑵+1-th Population\nPopulation Individuals\nForm training sets\nfor A, P, and A, N\nConstructing A,P,N-triplets\nvia perturbation\nPositive\nNegative\nAnchor\n𝑁(0, 𝜎2)\nRandomly select\na tuple\n…\n…\n…\n…\n…\n…\n𝑅𝑖𝑛𝑓\n𝐺𝑖𝑛𝑓\n𝐵𝑖𝑛𝑓\n…\n…\n𝐹𝑜𝑏𝑗: =\n1\n𝛼𝑙𝑄𝑀+ 𝛽Δ\n④Metaheuristic quantum circuit design\n①Construction of similarity triplets\nFitness Evaluation\nQuantum Circuit Structure \nEvolution\nEnhance diversity\n③Fitness calculation\n②Quantum feature extraction\n…\nTraining sets\nfor A,P\nH\n|0〉\nH\n|0〉\nH\n|0〉\nH\n|0〉\nH\n|0〉\nH\n|0〉\nH\n|0〉\nAmplitude Encoding\nRx\n0.41\nRz\n-2.64\nRy\n0.313\nRz\n-1.2\nRx\n3.74\nRx\n2.96\nRx\n2.96\n+\nRY\n4.64\nRZ\n4.78\nRX\n5.88\n+\n𝐴𝑝𝑥\n𝐴𝑝𝑦\n𝑃𝑥\n𝑃𝑦\nH\n|0〉\nH\n|0〉\nH\n|0〉\nH\n|0〉\nH\n|0〉\nH\n|0〉\nH\n|0〉\nAmplitude Encoding\n𝑁𝑥\n𝑁𝑦\n𝐴𝑛𝑥\n𝐴𝑛𝑦\n…\nTraining sets\nfor A,N\nFigure 3:\nThe complete process of QUSL. Some processes are summarized to ensure the\nclarity of the legend.\npairs sequence. Then, the Spearman correlation coefficient ρ is defined as\nρ = cov(Srg, EDrg)\nσ(Srg)σ(EDrg) =\nPn\ni=1(Ssim(i) −Srg)(Ed(i) −EDrg)\nqPn\ni=1(Ssim(i) −Srg)2\nqPn\ni=1(Ed(i) −EDrg)2\n, (9)\nwhere, Ssim(i) and Ed(i) denote the i-th elements of Srg and EDrg, respec-\ntively, while Srg and EDrg represent the mean values of elements in the two\nsets. The Spearman correlation coefficient, by definition, ranges from −1\nto 1, with 0 indicating no correlation, 1 denoting perfect positive correla-\ntion, and −1 representing perfect negative correlation. Models exhibiting\nhigher positive correlation values demonstrate relatively better performance.\nIn subsequent experiments and comparative analyses, this coefficient will be\nutilized to assess the performance of the quantum image similarity model.\nThe complete process of QUSL is shown in Fig. 3.\n13\n4. Experiments and Result\nThis section presents an experimental evaluation of the QUSL model,\ndemonstrating its effectiveness, robustness, and superiority compared to state-\nof-the-art quantum methods.\n4.1. Experiment Framework\nIn the domain of unsupervised image similarity tasks, we have selected\nfive datasets for training and testing purposes. Table 1 outlines the essential\ndetails of these datasets and the experimental configurations.\nTable 1: Datasets with basic setup used.\nDataset\nCategories Used\nPatch Size\nQubit Used\nlandscape\n7 object categoriesa\n80 × 80 × 3\n14\nCOCO\n80 object categories\n50 × 50 × 3\n14\nDISC21\n21 object categories\n50 × 50 × 3\n14\nCIFAR 10\n10 object categories\n32 × 32 × 3\n14\nImageNet\n1000 object categories\n50 × 50 × 3\n14\na All datasets do not utilize existing labels in the experiments.\nThe Flickr landscape Men et al. ((2022), comprising unlabeled colorful\nimages depicting diverse natural landscapes, along with the label-removed\nCOCO Lin et al. ((2014) and CIFAR 10 Krizhevsky et al. (2009), serve to\ninitially examine QUSL’s adaptability to unsupervised tasks. Experiments\non ImageNet Deng et al. (2009) further test QUSL’s performance. DISC21\nDouze et al. (2021), specifically designed for image similarity detection tasks,\nis instrumental in evaluating QUSL’s applicability in real-world scenarios,\nparticularly in social media contexts.\nThe experimental setup was configured using Python3, incorporating a\nhybrid combination of the Qiskit and MindSpore Developer (2021) frame-\nworks. Numerical simulations were executed on a workstation (CPU: Intel I9\n9900k, GPU: GTX3090). Guided by pre-experiment instructions, the evolu-\ntionary algorithm featured a population size of 20 and a maximum evolution\ngeneration of 20. The gate set constituting the quantum circuit comprised\nRX, RY , RZ, H, CNOT. Images within the training set underwent uniform\nprocessing to ensure consistent dimensions. Following the selection of anchor\nimages, random images from the training set were chosen to form A, P, N-\ntriplets. For all models, 1000 similarity tests were conducted, and model\nperformance was assessed using correlation detection.\n14\n0\n5\n10\n15\nPerturbation Level\n0.760\n0.786\n0.812\n0.838\n0.864\n0.890\nCorrelation\nMean: 0.8384\nMax: 0.8892\nAverage Correlation\nFigure 4:\nPerformance of the QUSL on the landscape dataset under varying levels of\nperturbation.\n4.2. Evaluation and Analysis\nTo determine the optimal perturbation level for the QUSL algorithm,\nwe conducted a series of preliminary experiments on the landscape dataset,\nevaluating the algorithm’s performance across different perturbation levels,\ncharacterized by the parameter σ.\nFive independent experimental runs were carried out at each perturbation\nlevel, with the results presented in Fig.4 and Table.2.\nTable 2: Performance of the QUSL under varying levels of perturbation.\nLevel\nCorrelation performancea\nMean value\n0\n0.830b\n0.762\n0.821\n0.803\n0.806\n0.804(±0.024)c\n5\n0.837\n0.829\n0.889\n0.814\n0.822\n0.838(±0.026)\n10\n0.823\n0.828\n0.820\n0.819\n0.823\n0.823(±0.024)\n15\n0.817\n0.791\n0.803\n0.853\n0.857\n0.824(±0.025)\na Five independent training and testing sessions were conducted at each level.\nb The bold numbers indicate the highest correlation achieved in each experimental group.\nc ”±” represents the standard deviation, reflecting the dispersion of sample data, as it\ndoes elsewhere in this paper.\nIn comparative experiments conducted under identical experimental con-\nditions, when the perturbation level was set to 5 (indicated by purple shading\nin the Fig. 4), the QUSL model achieved the best training performance of\n0.8892, exhibiting the highest average training effectiveness and acceptable\ndata dispersion. In contrast, the performance under other conditions showed\nlower training results and instability. Furthermore, the perturbation level\n15\nlandscape\nCOCO\nDISC21\nCIFAR_10\nImagnet\nDataset\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nCorrelation\np=0.045\np=0.000\np=0.002\np=0.051\np=0.001\nModel\nSliQ\nQUSL\nFigure 5: Performance evaluation of QUSL and SliQ across different datasets. In the violin\nplot, the blue and purple areas represent the distribution states of correlation obtained by\nSliQ and QUSL, respectively. Error bars represent 95% confidence intervals\nhas a significant impact on model performance, resulting in a performance\ndiscrepancy of up to 12% in terms of correlation.\nTherefore, by comprehensively considering relevant performance metrics,\nincluding maximum performance, average performance, and robustness, a\nperturbation level of 5 can be selected as the parameter setting for subsequent\nexperiments. Moreover, the perturbation level can serve as a controllable\nthreshold for image similarity during the learning process, or an adaptive\napproach can be employed to more precisely regulate the perturbation level.\nControlled experiment were designed to comprehensively examine the\nperformance efficacy and robustness of QUSL on five datasets, while con-\ncurrently benchmarking against SliQ. QUSL and SliQ underwent five train-\ning sessions and correlation tests of equal scale on each of the five datasets,\nrecording the best results obtained during the training process, which are\npresented in Fig. 5 and Table. 3.\nIn all five datasets, QUSL demonstrated significant enhancements in both\nperformance and stability. In comparison to SliQ, QUSL exhibited a remark-\nable improvement in correlation, with increases ranging from a minimum of\n8.7% to a maximum of 19.5%. Regarding stability, QUSL displayed a dis-\ntinct advantage over SliQ in terms of the standard deviation of performance\n16\nTable 3: Performance comparison between SliQ and QUSL based on five independent\nexperiments.\nDataset\nModel\n1st\n2nd\n3rd\n4th\n5th\nMean value\nImprova.\nlandscape\nSliQ\n0.835\n0.814\n0.758\n0.692\n0.657\n0.751 (±0.078)\n↑8.7%\nQUSL\n0.889\n0.837\n0.829\n0.822\n0.814\n0.838 (±0.031)\nCOCO\nSliQ\n0.757\n0.757\n0.722\n0.699\n0.668\n0.721 (±0.035)\n↑12.8%\nQUSL\n0.876\n0.875\n0.852\n0.822\n0.821\n0.849 (±0.027)\nDISC21\nSliQ\n0.758\n0.649\n0.593\n0.592\n0.528\n0.624 (±0.096)\n↑19.5%\nQUSL\n0.861\n0.847\n0.825\n0.787\n0.777\n0.819 (±0.037)\nCIFAR 10\nSliQ\n0.743\n0.742\n0.742\n0.686\n0.619\n0.706 (±0.052)\n↑11.9%\nQUSL\n0.890\n0.792\n0.769\n0.766\n0.731\n0.790 (±0.059)\nImagnet\nSliQ\n0.761\n0.706\n0.681\n0.647\n0.679\n0.695 (±0.042)\n↑17.0%\nQUSL\n0.848\n0.824\n0.818\n0.809\n0.765\n0.813 (±0.030)\na Improvement of QUSL(ours) over SliQ in terms of mean correlation performance.\nmeans, particularly evident in the DISC21 dataset, where SliQ’s performance\nexhibited substantial fluctuations, while QUSL’s stability remained largely\nunaffected by the application scenarios.\nQUSL’s performance improvements and high robustness can be primarily\nattributed to its ability to adapt to dataset characteristics through evolu-\ntionary algorithms. Conversely, the parameter adjustment of template-based\nvariational quantum circuits may encounter difficulties in capturing the in-\ntrinsic properties of the dataset, resulting in performance degradation under\nequivalent training conditions. This set of experiments directly demonstrates\nQUSL’s performance in scenarios close to real-world applications and show-\ncases its robustness across different types of image tasks.\nA series of experiments were designed to further investigate the detailed\ntraining process of QUSL, showcasing its accuracy and interpretability.\nFig.6 illustrates the fluctuation of fitness values for all individuals within\none evolutionary cycle. It is important to note that to ensure broad adapt-\nability of individuals to all image combinations within the dataset, QUSL\nemploys random sampling of the dataset during the training process, as-\nsembling multiple sets for fitness evaluation.\nThis approach leads to the\nstabilization of the best individual’s fitness within a certain range, rather\nthan strictly increasing monotonically and ultimately converging during the\nevolution process. It is evident that as the number of generations increases,\nthe fitness of the best individual improves, demonstrating the success of the\n17\n1-th\n2-th\n3-th\n4-th\n5-th\n6-th\n7-th\n8-th\n9-th\n10-th\n11-th\n12-th\n13-th\n14-th\n15-th\n16-th\n17-th\n18-th\n19-th\nEvolutionary Generation\n0.8\n0.9\n1.0\n1.1\n1.2\n1.3\nFitness Value\n1.089\n1.080\n1.077\n1.086\n1.085\n1.069\n1.251\n1.269\n1.298\n1.291\n1.264\n1.277\n1.273\n1.257\n1.272\n1.280\n1.287\n1.244\n1.276\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\nmax\nfitness\nFigure 6:\nFitness trajectory of individuals across generations during the evolutionary\nprocess in landscape. The figure annotates the fittest individuals in each generation of 20\nindividuals with corresponding values. The dashed line below the image represents the\nlowest fitness among individuals in the first generation. Given the tendency of evolution-\nary algorithms to converge towards optimization, individuals below the dashed line are\neliminated during the evolution process.\nevolutionary process.\nIt is worth noting that during the evolution process, there are still in-\ndividuals with significantly different quantum circuit structures but no ap-\nparent difference in fitness performance. A typical example of this scenario\nis demonstrated in Appendix\nB. This further demonstrates the threat of\nlow population diversity in metaheuristic quantum circuit design due to the\ninherent characteristics of quantum circuits.\nThe validity of using correlation as a model performance metric can be\nverified by the correspondence between the distribution of image similarities\nand their correlation. The results of this set of experiments are presented in\nFig.7, with the validation conducted on the landscape dataset.\nAs the number of generations increases, the correlation between QUSL\nnorm and Euclidean distance also gradually improves, reaching a maximum\ncorrelation of 0.889 in this evolution, which is a significant advantage com-\npared to SliQ. To more clearly demonstrate the correctness of the correlation\nmetric, Fig.8 and Fig.9 respectively illustrate the true distribution of images\nin the models with the best correlation on the COCO and DISC21 datasets.\n18\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nDistance(×104)\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nQUSL Norm(×10\n4)\nCorrelation = 0.527\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nDistance(×104)\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nQUSL Norm(×10\n2)\nCorrelation = 0.651\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nDistance(×104)\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nQUSL Norm(×10\n1)\nCorrelation = 0.749\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nDistance(×104)\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nQUSL Norm\nCorrelation = 0.889\nFigure 7:\nRelationship between euclidean distance and QUSL norm, the output of\nmethod, across different correlation levels. The purple line represents the regression anal-\nysis and corresponding confidence interval of the scatter plot, directly reflecting the cor-\nrelation between the similarity of QUSL and the Euclidean distance.\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\nDistance(×104)\n0.0\n0.5\n1.0\n1.5\n2.0\nQUSL Norm\nCorrelation = 0.876\nMin Norm=0.100\nNorm=1.095\nNorm=1.840\nFigure 8:\nResults of QUSL on the COCO. The model’s correlation is 0.876, achieved\nby the optimal individual during the evolution process. When QUSL norm reaches its\nminimum value of 0.1, QUSL identifies the most similar images in the current test set.\n19\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\nDistance(×104)\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\n2.25\nQUSL Norm\nCorrelation = 0.860\nMin Norm=0.341\nNorm=1.478\nNorm=2.040\nFigure 9:\nResults of QUSL on the DISC21. The model’s correlation is 0.860. When\nQUSL norm reaches its minimum value of 0.341, QUSL identifies the most similar images\nin the current test set.\nAs illustrated in the figures, the Euclidean distance and QUSL norm\nexhibit a positive correlation, and images with smaller QUSL norm values\ndemonstrate more significant visual similarity. This reflects the alignment\nbetween the image similarity determined by Euclidean distance and the image\nsimilarity assessed by QUSL.\nIt is crucial to highlight that the test set is randomly sampled from the\ndataset, and consequently, there may be an absence of image combinations\nthat are visually indistinguishable to human perception. Nevertheless, the\nfluctuations in the similarity values generated by QUSL can still offer valu-\nable insights into the variations in visual similarity among the images. The\naforementioned experiments collectively validate the appropriateness of em-\nploying correlation as a model performance evaluation metric and further\ncorroborate the model’s correctness.\nIn terms of core quantum resources, a set of numerical experiments and\nexperiments on quantum computers were designed to evaluate the compari-\nson between QUSL and SliQ in terms of quantum circuit depth and CNOT\ncount. Fig.10 and Table.4 illustrates a comparison of the quantum circuit\nperformance between the two approaches.\n20\nlandscape\nCOCO\nDISC21\nCIFAR_10\nImagnet\nDataset\n10\n15\n20\n25\n30\n35\n40\n45\n50\n55\n60\nCNOT Count\nCNOT Count of SliQ: 52\n10\n15\n20\n25\n30\n35\n40\n45\n50\n55\n60\n65\n70\nCircuit Depth\nCircuit Depth of SliQ: 64\nCNOT Count\nCircuit Depth\nCNOT Median\nDepth Median\nFigure 10:\nCNOT count and circuit depth utilized in the quantum circuit of QUSL. The\ngreen and blue dashed lines above the plots represent the circuit depth and CNOT count,\nrespectively, of the variational quantum circuit templates used by SliQ, which remain\nunchanged across different datasets.\nTable 4: CNOT count and circuit depth utilized in the quantum circuit of QUSL.\nDataset\nMetric\n1st\n2nd\n3rd\n4th\n5th\nMean valuea\nImprovb\nlandscape\nCNOT count\n35\n20c\n26\n24\n25\n26.0 (±4.9)\n↓50.0%\nCircuit depth\n36\n20\n28\n27\n26\n27.4 (±5.1)\n↓57.2%\nCOCO\nCNOT count\n30\n33\n36\n25\n25\n29.8 (±4.2)\n↓42.7%\nCircuit depth\n33\n40\n44\n27\n31\n35.0 (±6.2)\n↓45.3%\nDISC21\nCNOT count\n17\n24\n25\n36\n26\n25.6 (±6.3)\n↓50.8%\nCircuit depth\n26\n30\n25\n42\n32\n31.0 (±6.1)\n↓51.6%\nCIFAR 10\nCNOT count\n32\n19\n28\n18\n36\n26.6 (±7.1)\n↓48.8%\nCircuit depth\n36\n23\n34\n27\n29\n29.8 (±4.7)\n↓53.4%\nImagnet\nCNOT count\n41\n12\n23\n32\n26\n26.8 (±9.7)\n↓48.5%\nCircuit depth\n45\n16\n31\n34\n35\n32.2 (±9.3)\n↓49.7%\na The CNOT count and circuit depth are both integers, mean value merely reflects the\nrelative performance of the corresponding quantum circuits.\nb Improvement of QUSL(ours) over SliQ in terms of the decrease ratio of the average\nCNOT count and circuit depth compared to their respective values in SliQ.\nc Bold text indicates the best-performing parameters within a set of experiments.\n21\nTable 5: Comparison of the running times of optimal quantum circuits on real quantum\ncomputers.\nDataset\nlandscape\nCOCO\nDISC21\nRun time(s)a\nSliQ\nQUSL\nSliQ\nQUSL\nSliQ\nQUSL\n1\n1.64\n1.55\n1.60\n1.50\n1.67\n1.48\n2\n1.73\n1.59\n1.73\n1.61\n1.70\n1.49\n3\n1.76\n1.60\n1.76\n1.72\n1.70\n1.51\n4\n1.21\n1.64\n1.76\n1.72\n1.74\n1.65\n5\n1.90\n1.80\n1.80\n1.73\n1.74\n1.75\nMean value\n1.77c\n(±0.089)\n1.64\n(±0.087)\n1.77\n(±0.080)\n1.66\n(±0.089)\n1.71\n(±0.027)\n1.58\n(±0.109)\nOptimization\n-\n↓0.13\n-\n↓0.11\n-\n↓0.13\na Five independent running times sorted in ascending order.\nAcross all datasets used, the quantum circuits derived through QUSL\nexhibit reductions of around 50% in both CNOT counts and circuit depth.\nThis indicates that within acceptable fluctuations, the quantum circuits gen-\nerated by QUSL significantly outperform SliQ in key performance indicators,\ndirectly contributing to the model’s advantage in quantum resource utiliza-\ntion. The demonstration of high-performance quantum circuits is presented\nin Appendix A.\nValidation experiments on quantum computers were designed to examine\nthe potential advantages of QUSL in the NISQ era. The experiments were\nconducted on the ibmosaka quantum computer with 500 shots.\nTable 5\npresents a comparison of the running times between the high-performance\nquantum circuits used in QUSL and the template circuits employed in SliQ.\nThrough five independent circuit runs, it can be observed that QUSL\nconsistently reduces the running time of individual quantum circuits. The\ntime advantage at the single-circuit level is significantly amplified by the large\nnumber of circuit runs during the training process, which will be reflected\nin the model’s training efficiency and practical application efficiency. This\nadvantage of QUSL becomes more evident in large-scale tasks.\nThrough numerical simulations and a series of experiments on quantum\ncomputers, the QUSL algorithm demonstrates robust performance improve-\nments in image similarity tasks, achieving higher average correlations across\nmultiple datasets. In challenging datasets closer to real-world scenarios, such\nas DISC21, QUSL exhibits higher performance and stronger stability. More\nimportantly, compared to SliQ’s variational template, QUSL significantly\nreduces quantum resource consumption. Combined with validation on quan-\n22\ntum computers, QUSL demonstrates higher learning efficiency in NISQ en-\nvironments relative to SliQ.\nOverall, QUSL, as an unsupervised quantum image similarity model, sur-\npasses current state-of-the-art quantum image similarity detection methods\nin terms of accuracy, robustness, and quantum resource efficiency.\n5. Discussion and Futher Work\nAlong with our work, SliQ’s Silver et al. ((2023) performance demon-\nstrates that quantum computing can be applied to image processing tasks\nwith potential efficiency gains. Due to NISQ limitations, quantum machine\nlearning algorithms native to quantum computers are challenging to imple-\nment. Both QUSL and SliQ represent quantum transfers of classical unsu-\npervised image tasks, particularly contrastive learning tasks. In comparison,\nQUSL goes further than the latest methods in realizing a quantum imple-\nmentation of the classical triplet method. Furthermore, this work reveals\nthe importance of quantum circuit design in quantum unsupervised learning\ntasks. As an efficient method, heuristic quantum circuit architecture search\ncan design high-performance circuits that adapt to task requirements. This\ndirectly alleviates the dependence of complex unsupervised tasks on deep,\nparameter-rich PQCs, making it more suitable for the NISQ era. This work\nprovides a preliminary exploration of large-scale unsupervised contrastive\nlearning in the quantum domain.\nThese methods utilize the correlation between the computed model loss\nand classical similarity norm to evaluate model performance. This ingenious\nand transferable approach addresses the challenge of objectively assessing im-\nage similarity in unlabeled datasets, proving effective within the limited ap-\nplication scenarios addressed in our work. However, this evaluation method\nremains open to refinement. For instance, exploring more suitable correspon-\ndences between losses and classical image similarity detection methods within\ndifferent numerical ranges. This refinement process stands as a significant\nfuture endeavor within our research trajectory.\nIn the process of constructing A,P,N-triplets, noise is used for data aug-\nmentation. The efficacy of this strategy was validated through subsequent\nexperimental analysis. While noise is conventionally perceived as an imped-\niment in both classical and quantum contexts — noise cuts both ways — it\ncan also serve as a valuable asset for specific tasks Liu et al. (2023). Although\n23\nnoise mitigation techniques have been extensively explored in quantum com-\nputing, acknowledging noise as a potentially beneficial factor paves the way\nfor novel approaches and methodologies\n6. Conclusion\nThe QUSL method proposed in this work focuses on quantum unsuper-\nvised image similarity detection tasks.\nQUSL employs heuristic methods\nto adapt quantum circuits to dataset characteristics and utilizes noise-based\ndata augmentation for triplet construction. A series of numerical simulations\nand quantum computer experiments reveal that, compared to the current\nstate-of-the-art quantum methods, QUSL further improves the accuracy and\nrobustness of the task. QUSL achieves up to a 19.5% increase in correlation\nand reduces the use of critical quantum resources by over 50%. As a model\nframework, QUSL demonstrates transferability applicable to other task sce-\nnarios. This work contributes to further integration of classical unsupervised\nlearning techniques with quantum computing, showcasing the potential of\nquantum methods in complex image processing tasks.\nAuthors’ contributions\nLian-Hui Yu: Methodology, Software. Xiao-Yu Li: Conceptualization,\nProject administration. Geng Chen: Writing - Original Draft, Visualiza-\ntion. Qin-Sheng Zhu: Validation. Hui Li: Formal analysis. Guo-Wu\nYang: Supervision.\nAvailability of supporting data\nAll data generated or analysed during this study are available and in-\ncluded in this published article. Code for our work has been open-sourced.\nAcknowledgements\nThis work was supported by National Key R&D Program of China (Grant\nNo.2018FYA0306703), the Open Fund of Advanced Cryptography and Sys-\ntem Security Key Laboratory of Sichuan Province (Grant No. SKLACSS-\n202105), Chengdu Innovation and Technology Project (No.2021-YF05-02413-\nGX and 2021-YF09-00114-GX), Sichuan Province key research and develop-\nment project (No.2022YFG0315).\n24\nReferences\nAltares-L´opez, S., Ribeiro, A., Garc´ıa-Ripoll, J.J., 2021.\nAutomatic design of\nquantum feature maps. Quantum Science and Technology 6, 045015. doi:https:\n//doi.org/10.1088/2058-9565/ac1ab1.\nArufe, L., Gonz´alez, M.A., Oddi, A., Rasconi, R., Varela, R., 2022. Quantum\ncircuit compilation by genetic algorithm for quantum approximate optimization\nalgorithm applied to maxcut problem. Swarm and Evolutionary Computation\n69, 101030. doi:https://doi.org/10.1016/j.swevo.2022.101030.\nArufe, L., Rasconi, R., Oddi, A., Varela, R., Gonz´alez, M.A., 2023. New coding\nscheme to compile circuits for quantum approximate optimization algorithm by\ngenetic evolution. Applied Soft Computing 144, 110456. doi:https://doi.org/\n10.1016/j.asoc.2023.110456.\nArute, F., Arya, K., Babbush, R., Bacon, D., Bardin, J.C., Barends, R.,\nBiswas, R., Boixo, S., Brandao, F.G., Buell, D.A., et al., 2019.\nQuantum\nsupremacy using a programmable superconducting processor. Nature 574, 505–\n510. doi:https://doi.org/10.1038/s41586-019-1666-5.\nBai, Y., Ding, H., Bian, S., Chen, T., Sun, Y., Wang, W., (2019). Simgnn: A\nneural network approach to fast graph similarity computation, in: Proceedings\nof the twelfth ACM international conference on web search and data mining,\npp. 384–392. doi:https://doi.org/10.1145/3289600.3290967.\nBenedetti, M., Lloyd, E., Sack, S., Fiorentini, M., 2019.\nParameterized quan-\ntum circuits as machine learning models. Quantum Science and Technology 4,\n043001. doi:https://doi.org/10.1088/2058-9565/ab4eb5.\nBiamonte, J., Wittek, P., Pancotti, N., Rebentrost, P., Wiebe, N., Lloyd, S., 2017.\nQuantum machine learning. Nature 549, 195–202. doi:https://doi.org/10.\n1038/nature23474.\nCerezo, M., Arrasmith, A., Babbush, R., Benjamin, S.C., Endo, S., Fujii, K.,\nMcClean, J.R., Mitarai, K., Yuan, X., Cincio, L., et al., 2021.\nVariational\nquantum algorithms. Nature Reviews Physics 3, 625–644. doi:https://doi.\norg/10.1038/s42254-021-00348-9.\nChen, T., Kornblith, S., Norouzi, M., Hinton, G., 2020.\nA simple framework\nfor contrastive learning of visual representations, in: International conference\non machine learning, PMLR. pp. 1597–1607. URL: http://proceedings.mlr.\npress/v119/chen20j.html.\n25\nCheng, G., Han, J., Lu, X., 2017.\nRemote sensing image scene classification:\nBenchmark and state of the art.\nProceedings of the IEEE 105, 1865–1883.\ndoi:https://doi.org/10.1109/JPROC.2017.2675998.\nCong, I., Choi, S., Lukin, M.D., 2019.\nQuantum convolutional neural net-\nworks.\nNature Physics 15, 1273–1278.\ndoi:https://doi.org/10.1038/\ns41567-019-0648-8.\nDang, Y., Jiang, N., Hu, H., Ji, Z., Zhang, W., 2018. Image classification based on\nquantum k-nearest-neighbor algorithm. Quantum Information Processing 17,\n1–18. doi:https://doi.org/10.1007/s11128-018-2004-9.\nDeng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L., 2009. Imagenet: A\nlarge-scale hierarchical image database, in: 2009 IEEE conference on computer\nvision and pattern recognition, Ieee. pp. 248–255. doi:https://doi.org/10.\n1109/CVPR.2009.5206848.\nDeveloper, M., 2021. Mindquantum, version 0.9.11. URL: https://gitee.com/\nmindspore/mindquantum.\nDing, L., Spector, L., (2022).\nEvolutionary quantum architecture search for\nparametrized quantum circuits, in: Proceedings of the Genetic and Evolution-\nary Computation Conference Companion, pp. 2190–2195.\ndoi:https://doi.\norg/10.1145/3520304.3534012.\nDouze, M., Tolias, G., Pizzi, E., Papakipos, Z., Chanussot, L., Radenovic, F.,\nJenicek, T., Maximov, M., Leal-Taix´e, L., Elezi, I., et al., 2021.\nThe 2021\nimage similarity dataset and challenge. arXiv preprint arXiv:2106.09672 URL:\nhttps://arxiv.org/abs/2106.09672.\nEisert, J., 2021. Entangling power and quantum circuit complexity. Physical Re-\nview Letters 127, 020501. doi:https://doi.org/10.1103/PhysRevLett.127.\n020501.\nFang, H., Wang, Q., Tu, Y.C., Horstemeyer, M.F., 2008.\nAn efficient non-\ndominated sorting method for evolutionary algorithms. Evolutionary compu-\ntation 16, 355–384. doi:https://doi.org/10.1162/evco.2008.16.3.355.\nHe, K., Fan, H., Wu, Y., Xie, S., Girshick, R., (2020). Momentum contrast for\nunsupervised visual representation learning, in: Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition, pp. 9729–9738. URL:\nhttps://openaccess.thecvf.com/content_CVPR_2020/html/He_Momentum_\n26\nContrast_for_Unsupervised_Visual_Representation_Learning_CVPR_\n2020_paper.html.\nHendrycks, D., Dietterich, T., 2019. Benchmarking neural network robustness to\ncommon corruptions and perturbations. arXiv preprint arXiv:1903.12261 URL:\nhttps://arxiv.org/abs/1903.12261.\nHuang, H.Y., Broughton, M., Cotler, J., Chen, S., Li, J., Mohseni, M., Neven,\nH., Babbush, R., Kueng, R., Preskill, J., et al., 2022. Quantum advantage in\nlearning from experiments. Science 376, 1182–1186. doi:https://doi.org/10.\n1126/science.abn7293.\nHuang, H.Y., Broughton, M., Mohseni, M., Babbush, R., Boixo, S., Neven, H.,\nMcClean, J.R., 2021. Power of data in quantum machine learning. Nature com-\nmunications 12, 2631. doi:https://doi.org/10.1038/s41467-021-22539-9.\nJaderberg, B., Anderson, L.W., Xie, W., Albanie, S., Kiffner, M., Jaksch, D., 2022.\nQuantum self-supervised learning. Quantum Science and Technology 7, 035005.\ndoi:https://doi.org/10.1088/2058-9565/ac6825.\nKnill, E., 2010. Quantum computing. Nature 463, 441–443. doi:https://doi.\norg/10.1038/463441a.\nKrizhevsky, A., Hinton, G., et al., 2009. Learning multiple layers of features from\ntiny images doi:https://doi.org/10.1609/aaai.v34i07.6845.\nKrylov, G., Lukac, M., (2019). Quantum encoded quantum evolutionary algo-\nrithm for the design of quantum circuits, in: Proceedings of the 16th ACM\nInternational Conference on Computing Frontiers, pp. 220–225.\ndoi:https:\n//doi.org/10.1145/3310273.3322826.\nLi, X.Y., Zhu, Q.S., Hu, Y., Wu, H., Yang, G.W., Yu, L.H., Chen, G., 2024.\nA new quantum machine learning algorithm: split hidden quantum markov\nmodel inspired by quantum conditional master equation.\nQuantum 8, 1232.\ndoi:https://doi.org/10.22331/q-2024-01-24-1232.\nLin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll´ar, P.,\nZitnick, C.L., (2014). Microsoft coco: Common objects in context, in: Computer\nVision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September\n6-12, 2014, Proceedings, Part V 13, Springer. pp. 740–755. doi:https://doi.\norg/10.1007/978-3-319-10602-1_48.\n27\nLiu, D., Li, W., Duan, L., Tsang, I.W., Yang, G., 2023. Noisy label learning with\nprovable consistency for a wider family of losses. IEEE Transactions on Pat-\ntern Analysis and Machine Intelligence doi:https://doi.org/10.1109/TPAMI.\n2023.3296156.\nLiu, X., Zhou, R.G., El-Rafei, A., Li, F.X., Xu, R.Q., 2019. Similarity assessment\nof quantum images.\nQuantum Information Processing 18, 1–19.\ndoi:https:\n//doi.org/10.1007/s11128-019-2357-8.\nMa, Z., Dong, J., Long, Z., Zhang, Y., He, Y., Xue, H., Ji, S., 2020.\nFine-\ngrained fashion similarity learning by attribute-specific embedding network, in:\nProceedings of the AAAI Conference on artificial intelligence, pp. 11741–11748.\ndoi:https://doi.org/10.1609/aaai.v34i07.6845.\nMei, Y., Fan, Y., Zhang, Y., Yu, J., Zhou, Y., Liu, D., Fu, Y., Huang, T.S.,\nShi, H., 2023. Pyramid attention network for image restoration. International\nJournal of Computer Vision 131, 3207–3225. doi:https://doi.org/10.1007/\ns11263-023-01843-5.\nMen, Y., Yao, Y., Cui, M., Lian, Z., Xie, X., Hua, X.S., (2022). Unpaired cartoon\nimage synthesis via gated cycle mapping, in: Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition, pp. 3501–3510. URL:\nhttps://openaccess.thecvf.com/content/CVPR2022/html/Men_Unpaired_\nCartoon_Image_Synthesis_via_Gated_Cycle_Mapping_CVPR_2022_paper.\nhtml.\nMohseni, N., McMahon, P.L., Byrnes, T., 2022. Ising machines as hardware solvers\nof combinatorial optimization problems. Nature Reviews Physics 4, 363–379.\ndoi:https://doi.org/10.1038/s42254-022-00440-8.\nNam, Y., Ross, N.J., Su, Y., Childs, A.M., Maslov, D., 2018. Automated opti-\nmization of large quantum circuits with continuous parameters. npj Quantum\nInformation 4, 23. doi:https://doi.org/10.1038/s41534-018-0072-4.\nPaine, A.E., Elfving, V.E., Kyriienko, O., 2023.\nQuantum kernel methods for\nsolving regression problems and differential equations. Physical Review A 107,\n032428. doi:https://doi.org/10.1103/PhysRevA.107.032428.\nPalubinskas, G., 2017. Image similarity/distance measures: what is really behind\nmse and ssim?\nInternational Journal of Image and Data Fusion 8, 32–53.\ndoi:https://doi.org/10.1080/19479832.2016.1273259.\n28\nP´erez-Salinas, A., Cervera-Lierta, A., Gil-Fuster, E., Latorre, J.I., 2020.\nData\nre-uploading for a universal quantum classifier. Quantum 4, 226. doi:https:\n//doi.org/10.22331/q-2020-02-06-226.\nPreskill, J., 2018. Quantum computing in the nisq era and beyond. Quantum 2,\n79. doi:https://doi.org/10.22331/q-2018-08-06-79.\nRasconi, R., Oddi, A., (2019). An innovative genetic algorithm for the quantum\ncircuit compilation problem, in: Proceedings of the AAAI conference on artifi-\ncial intelligence, pp. 7707–7714. doi:https://doi.org/10.1609/aaai.v33i01.\n33017707.\nRebentrost, P., Mohseni, M., Lloyd, S., 2014. Quantum support vector machine\nfor big data classification.\nPhysical review letters 113, 130503.\ndoi:https:\n//doi.org/10.1103/PhysRevLett.113.130503.\nSchuld, M., Sinayskiy, I., Petruccione, F., 2014. The quest for a quantum neural\nnetwork. Quantum Information Processing 13, 2567–2586. doi:https://doi.\norg/10.1007/s11128-014-0809-8.\nSchuld, M., Sinayskiy, I., Petruccione, F., 2015.\nAn introduction to quantum\nmachine learning. Contemporary Physics 56, 172–185. doi:https://doi.org/\n10.1080/00107514.2014.964942.\nSilver, D., Patel, T., Ranjan, A., Gandhi, H., Cutler, W., Tiwari, D., (2023).\nSliq:\nquantum image similarity networks on noisy quantum computers, in:\nProceedings of the AAAI Conference on Artificial Intelligence, pp. 9846–9854.\ndoi:https://doi.org/10.1609/aaai.v37i8.26175.\nSim, S., Johnson, P.D., Aspuru-Guzik, A., 2019. Expressibility and entangling\ncapability of parameterized quantum circuits for hybrid quantum-classical algo-\nrithms. Advanced Quantum Technologies 2, 1900070. doi:https://doi.org/\n10.1002/qute.201900070.\nVeit, A., Belongie, S., Karaletsos, T., (2017). Conditional similarity networks, in:\nProceedings of the IEEE conference on computer vision and pattern recognition,\npp. 830–838.\nURL: https://openaccess.thecvf.com/content_cvpr_2017/\nhtml/Veit_Conditional_Similarity_Networks_CVPR_2017_paper.html.\nWang, H., Ding, Y., Gu, J., Lin, Y., Pan, D.Z., Chong, F.T., Han, S., 2022. Quan-\ntumnas: Noise-adaptive search for robust quantum circuits, in: 2022 IEEE In-\nternational Symposium on High-Performance Computer Architecture (HPCA),\nIEEE. pp. 692–708. doi:https://doi.org/10.1109/HPCA53966.2022.00057.\n29\nWang, J., Song, Y., Leung, T., Rosenberg, C., Wang, J., Philbin, J., Chen, B.,\nWu, Y., (2014). Learning fine-grained image similarity with deep ranking, in:\nProceedings of the IEEE conference on computer vision and pattern recognition,\npp. 1386–1393. URL: https://openaccess.thecvf.com/content_cvpr_2014/\nhtml/Wang_Learning_Fine-grained_Image_2014_CVPR_paper.html.\nWang, X., Du, Y., Yang, S., Zhang, J., Wang, M., Zhang, J., Yang, W., Huang,\nJ., Han, X., 2023. Retccl: Clustering-guided contrastive learning for whole-slide\nimage retrieval. Medical image analysis 83, 102645. doi:https://doi.org/10.\n1016/j.media.2022.102645.\nWu, W., Yan, G., Lu, X., Pan, K., Yan, J., (2023).\nQuantumdarts: differen-\ntiable quantum architecture search for variational quantum algorithms, in: In-\nternational Conference on Machine Learning, PMLR. pp. 37745–37764. URL:\nhttps://proceedings.mlr.press/v202/wu23v.html.\nXu, Y., Liu, X., Cao, X., Huang, C., Liu, E., Qian, S., Liu, X., Wu, Y., Dong,\nF., Qiu, C.W., et al., 2021. Artificial intelligence: A powerful paradigm for\nscientific research. The Innovation 2. doi:https://doi.org/10.1016/j.xinn.\n2021.100179.\nYan, F., Le, P.Q., Iliyasu, A.M., Sun, B., Garcia, J.A., Dong, F., Hirota, K.,\n(2012). Assessing the similarity of quantum images based on probability mea-\nsurements, in: 2012 IEEE Congress on Evolutionary Computation, IEEE. pp.\n1–6. doi:https://doi.org/10.1109/CEC.2012.6256418.\nZhang, A., Zhao, S., 2023.\nEvolutionary-based searching method for quantum\ncircuit architecture. Quantum Information Processing 22, 283. doi:https://\ndoi.org/10.1007/s11128-023-04033-x.\nZhang, L., Zhang, L., Mou, X., Zhang, D., 2011.\nFsim: A feature similarity\nindex for image quality assessment. IEEE transactions on Image Processing 20,\n2378–2386. doi:https://doi.org/10.1109/TIP.2011.2109730.\nZhou, R.G., Sun, Y.J., 2015. Quantum multidimensional color images similarity\ncomparison.\nQuantum Information Processing 14, 1605–1624.\ndoi:https://\ndoi.org/10.1007/s11128-014-0849-0.\n30\nAppendix A. The quantum circuits obtained by QUSL\nThe quantum circuits with the best expressive capabilities obtained by QUSL\non landscape and DISC21 are shown below. Summarizing the optimal circuits ob-\ntained across all datasets reveals some interesting patterns and structures. {RX, RY , RZ}\nand CNOT gates present complex interweaving, forming an intricate high-dimensional\nHilbert space. The CNOT-RX-RY -RZ sequence appears frequently in multiple\ncircuits, potentially serving as an effective module. Another prominent pattern\nis multiple qubits connected to a single qubit through CNOT gates, which may\neffectively transform this qubit into an attention center through entanglement,\nthereby achieving crucial feature extraction.\nFigure A.11:\nThe optimal-performing quantum circuit obtained by QUSL on the land-\nscape, with 26 CNOT operations and a circuit depth of 28.\nFigure A.12:\nThe optimal-performing quantum circuit obtained by QUSL on the DISC21,\nwith 17 CNOT operations and a circuit depth of 26.\n31\nAppendix B. Quantum circuits with similar fitness but signifi-\ncantly different structures\nDuring the training process, some circuits with similar expressive capabilities\nbut enormous structural differences emerged, as shown in the figure below. These\ncircuits are derived from the training process on the COCO dataset, exhibiting\nfitness differences on the order of 10−4, yet their quantum circuit depths and\nCNOT counts differ by almost a factor of two. This phenomenon poses challenges\nfor population diversity.\nFitness=0.98665\nFitness=0.98685\nFigure B.13:\nQuantum circuits with significant structural differences but similar fitness\nlevels.\n32\nAppendix C. Performance Evaluation of QUSL in classification\ntasks\nWe extend our evaluation to include multi-classification tasks as simplified\ninstances of image similarity tasks as explained in the introduction, offering insights\ninto QUSL’s performance.\nRigorous binary classification experiments on three\nwidely recognized multi-classification datasets — mnist, fashion-mnist, and AIDS\n— aim to validate the model’s usability and assess its performance under simplified\nscenarios. Table Appendix\nC compares the performance of SliQ and QUSL on\nthree multi-classification datasets.\nTable C.6: The comparison between QUSL and SliQ in terms of classification task accuracy\nDataset\nSliQ\nQUSL (ours)\nmnist\n97.16(±0.193)\n98.50(±0.235)\nfashion-mnist\n92.53(±0.409)\n92.76(±0.682)\nAIDS\n71.54(±0.257)\n71.32(±0.457)\nIn classification tasks, QUSL and SliQ exhibit similar accuracy, but with\nslightly reduced robustness in the case of QUSL. This may be attributed to the\nheuristic algorithm-based quantum circuit design facing more pronounced chal-\nlenges related to population diversity in simpler image classification tasks, thereby\ninducing instability in the model.\nAppendix D. Performance Evaluation of QUSL in NISQ Environ-\nments\nTo validate QUSL’s potential for practical application in the NISQ era, we con-\nducted simulated experiments using the quantum noise library provided by Pen-\nnylane. We added composite noise with a degree of 0.045 (including bit flip, phase\nflip, and depolarizing channel noise) to the experiments. Additionally, to improve\nefficiency, we compressed the images through downsampling. Five independent\ntrials were conducted in both clean and noisy environments with CIFAR 10:\nTable D.7: Performance of QUSL in noise-free and noisy environments\nEnvironment\n1st\n2nd\n3rd\n4th\n5th\nAverage (±std)\nNoise-free\n0.8094\n0.7941\n0.7824\n0.7778\n0.7755\n0.7878(±0.0143)\nNoisy\n0.8063\n0.7492\n0.7246\n0.7411\n0.7067\n0.7226(±0.0354)\nResults show that QUSL’s performance remained stable in noisy environments,\nwith only a slight decrease in robustness. We speculate that the noise primarily\n33\naffected the consistency in the triplet construction process, thereby influencing\nfeature extraction. Nevertheless, QUSL’s excellent performance in noisy environ-\nments demonstrates its potential for application on actual NISQ devices.\nAppendix E. Performance Comparison of QUSL with Classical\nMethods\nTo evaluate QUSL’s performance, we designed a set of comparative experi-\nments in CIFAR 10. The classical baseline used a medium-scale fully connected\nneural network with 4 hidden layers and approximately 1 million parameters, also\nemploying the triplet strategy.\nTable E.8: Performance comparison of Classical Network, SliQ, and QUSL\nMethod\n1st\n2nd\n3rd\n4th\n5th\nAverage (±std)\nClassical\n0.833\n0.771\n0.766\n0.745\n0.742\n0.771(±0.036)\nSliQ\n0.743\n0.742\n0.742\n0.686\n0.619\n0.706(±0.052)\nQUSL\n0.890\n0.792\n0.769\n0.766\n0.731\n0.790 (±0.059)\nThe experimental results show that the performance of the three networks is\nroughly on par, which is already an encouraging outcome. Notably, SliQ achieved\ncomparable performance to the classical network using only about 200 parameters,\ndemonstrating the potential of quantum methods in parameter efficiency. QUSL\noutperforms SliQ in effectiveness, and also shows better performance in quantum\ncircuit efficiency and quantum resource utilization, making it more adaptable to\ncurrent quantum hardware conditions.\n34\n",
  "categories": [
    "quant-ph"
  ],
  "published": "2024-04-02",
  "updated": "2024-08-22"
}