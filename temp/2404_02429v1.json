{
  "id": "http://arxiv.org/abs/2404.02429v1",
  "title": "AD4RL: Autonomous Driving Benchmarks for Offline Reinforcement Learning with Value-based Dataset",
  "authors": [
    "Dongsu Lee",
    "Chanin Eom",
    "Minhae Kwon"
  ],
  "abstract": "Offline reinforcement learning has emerged as a promising technology by\nenhancing its practicality through the use of pre-collected large datasets.\nDespite its practical benefits, most algorithm development research in offline\nreinforcement learning still relies on game tasks with synthetic datasets. To\naddress such limitations, this paper provides autonomous driving datasets and\nbenchmarks for offline reinforcement learning research. We provide 19 datasets,\nincluding real-world human driver's datasets, and seven popular offline\nreinforcement learning algorithms in three realistic driving scenarios. We also\nprovide a unified decision-making process model that can operate effectively\nacross different scenarios, serving as a reference framework in algorithm\ndesign. Our research lays the groundwork for further collaborations in the\ncommunity to explore practical aspects of existing reinforcement learning\nmethods. Dataset and codes can be found in https://sites.google.com/view/ad4rl.",
  "text": "AD4RL: Autonomous Driving Benchmarks for\nOffline Reinforcement Learning with Value-based Dataset\nDongsu Lee†, Chanin Eom†, and Minhae Kwon†,∗\nAbstract— Offline reinforcement learning has emerged as a\npromising technology by enhancing its practicality through the\nuse of pre-collected large datasets. Despite its practical benefits,\nmost algorithm development research in offline reinforcement\nlearning still relies on game tasks with synthetic datasets.\nTo address such limitations, this paper provides autonomous\ndriving datasets and benchmarks for offline reinforcement\nlearning research. We provide 19 datasets, including real-\nworld human driver’s datasets, and seven popular offline\nreinforcement learning algorithms in three realistic driving\nscenarios. We also provide a unified decision-making process\nmodel that can operate effectively across different scenarios,\nserving as a reference framework in algorithm design. Our\nresearch lays the groundwork for further collaborations in the\ncommunity to explore practical aspects of existing reinforce-\nment learning methods. Dataset and codes can be found in\nhttps://sites.google.com/view/ad4rl.\nI. INTRODUCTION\nConsiderable progress in intelligent machines has made\nremarkable strides, fueled by deep neural networks trained\non large datasets [1]–[3]. In contrast, an intelligent auto-\nmated system such as autonomous driving has seen limited\nimprovement. Deep reinforcement learning was a promising\nsolution for modern control systems [4], [5], but it faces\nchallenges due to its technical characteristics. The practical\nchallenges of online reinforcement learning are as follows [6],\n[7]. Firstly, trial-and-error based learning may lead to financial\nloss and social damage in mission-critical systems, e.g.,\ncar accidents. Secondly, simulator-based training has an\ninherent gap between the simulator and real-world dynamics,\nresulting in limited performance in real-world deployment.\nLastly, the active data collection during the training, i.e.,\nonline interaction between the agent and the environment, is\nexpensive and hampers the ability to exploit vast previously\ncollected datasets. Addressing these challenges is critical to\nrealizing the full potential of reinforcement learning.\nTo overcome the challenges, offline reinforcement learning,\nalso known as batch reinforcement learning [8], has recently\ngained attention as a promising approach for autonomous\nsystems. This paradigm utilizes large-scale pre-collected\nThis work was supported in part by the National Research Foundation\nof Korea (NRF) grant (RS-2023-00278812) and in part by the ITRC\n(Information Technology Research Center) support program (IITP-2022-2020-\n0-01602) funded by the Korea government (MSIT). Dongsu Lee is grateful\nfor financial support from Hyundai Motor Chung Mong-Koo Foundation.\n(Corresponding author: M. Kwon)\n†Authors are with the Department of Intelligent Semiconductors, Soongsil\nUniversity, Seoul 06978, Republic of Korea (e-mail:{movementwater,\neci0623}@soongsil.ac.kr, minhae@ssu.ac.kr)\n∗M. Kwon is with the School of Electronic Engineering, Soongsil\nUniversity, Seoul 06978, Republic of Korea\ndatasets to train policies for agents. Since online data\ncollection is no longer necessary for training, it allows us\nto avoid having agents perform immature and risky actions\nwith an unstable policy in the early training phase. Offline\nreinforcement learning offers a secure and efficient learning\nmethod by leveraging insights from successful data-driven\ndeep learning, providing the potential to shift the paradigm\nof mission-critical applications [9]–[11].\nDespite recent attention to offline reinforcement learn-\ning, research on autonomous driving still heavily relies\non online reinforcement learning [12]. Some recent efforts\nhave attempted to shift the research paradigm toward offline\nreinforcement learning, but they are still in the early stages [9],\n[13], [14]. For example, [13] provides synthetic datasets\nand benchmarks using the FLOW framework [15]. While\nit serves as a valuable dataset and benchmark in offline\nreinforcement learning studies, it focuses exclusively on\nacceleration maneuvers, neglecting lane-changing and present-\ning unrealistic, simplified driving scenarios. Additionally, it\nassumes that safety modules can prevent all accidents, which\nis an unrealistic assumption [16]. Another challenge with\nexisting benchmarks is the absence of real-world datasets.\nMost studies rely solely on synthetic datasets collected by\nonline reinforcement learning agents without incorporating\nany real-world human-driving datasets [13], [14], [17].\nContributions: Our primary contribution is the incorpora-\ntion of real-world human-driving datasets as well as synthetic\ndatasets in offline reinforcement learning for autonomous\ndriving tasks. We employ the US Highway 101 dataset [18]\ncollected by the next generation simulator (NGSIM) project\nof the Federal Highway Administration (FHWA) [19]. To\nprovide a useful dataset and benchmark, we pre-process the\nNGSIM dataset by labeling the reward, correcting errors,\nand normalizing values. We also propose a unified partially\nobservable Markov decision process (POMDP) that can\nbe applied across various driving scenarios. Finally, we\nbenchmark offline reinforcement learning algorithms within\nthe FLOW framework and extend its functionality.1\n1This study introduces a benchmark specifically tailored for autonomous\ndriving, aiming to ensure widespread accessibility and reproducibility. Con-\nsistent with previous literature, the study employs a simulated environment.\nThe decision to rely on a simulated environment is motivated by the inherent\nchallenges associated with evaluating the performance of autonomous driving\npolicies. Existing research suggests that off-policy evaluation approaches\nlack the necessary reliability [7], [13], [20]. Consequently, policy candidates\nare evaluated exclusively within the simulated environment as a practical\napproach to mitigate the risks inherent in policy evaluation. It is worth noting\nthat while policy evaluation takes place in the simulator, real-world datasets\nare incorporated for policy training.\narXiv:2404.02429v1  [cs.LG]  3 Apr 2024\nII. RELATED WORKS\nReinforcement Learning Based Autonomous Driving\nThe advancement of deep reinforcement learning has played a\npivotal role in propelling the progress of autonomous driving\nresearch. Researchers have diligently tackled a spectrum\nof control tasks, encompassing acceleration, lane-changing,\nand intersection negotiation [21], [22]. Previous studies have\npredominantly focused on specific roadway configurations,\nencompassing single-lane roads, multi-lane highways, on/off-\nramps, and scenarios involving lane reduction [15], [23]–\n[25]. Nonetheless, these decision-making frameworks remain\ntailored to distinct scenarios, lacking universality across\ndiverse driving contexts. The paramount objective of an\nautonomous driving decision-making model lies in its capacity\nto seamlessly and safely operate across a spectrum of\ndriving scenarios. Consequently, this paper introduces a\ncomprehensive decision-making process model designed to\naccommodate diverse driving scenarios.\nReinforcement Learning Using Pre-collected Data There\nis a growing interest in leveraging pre-collected dataset-\nbased training approaches for reinforcement learning, pri-\nmarily driven by its practical constraints. The new paradigm\nencompasses offline reinforcement learning [8], imitative\nlearning [26], and imitation learning [27], which leverage pre-\ncollected datasets to build an initial policy. However, these\nmethods suffer from two significant limitations. Firstly, in\nboth training and deployment phases, offline reinforcement\nlearning often suffers from extrapolation errors where the\npolicy samples out-of-distribution actions that are not included\nin the training dataset [7], [20]. This issue is commonly\nreferred to as the distribution shift problem between the\ntrajectory distribution of the trained policy and the dataset.\nRecent research attempts to mitigate this issue by introducing\nconservative or penalizing terms to align the policy’s actions\nmore closely with the training dataset, but such constraints\nmay impose performance limitations [28]–[31]. Secondly, the\nmajority of existing studies are confined to synthetic datasets\ngenerated by pre-trained policies within online reinforcement\nlearning settings rather than utilizing real-world datasets [13].\nIn response to these limitations and with the aim of providing\na comprehensive benchmark for offline reinforcement learning,\nthis paper assesses the performance of cutting-edge offline\nreinforcement learning algorithms using both human driver-\ngenerated datasets and synthetic datasets.\nValue-based Datasets for Autonomous Driving System In\nthe realm of autonomous driving research using reinforcement\nlearning, the predominant approach relies on image-based\ndata for observation [32]–[34]. This approach offers the\nadvantage of an end-to-end pipeline, where neural networks\nhandle the entire process from perception to decision-making.\nHowever, this end-to-end methodology poses challenges in\nterms of interpretability, making it challenging to pinpoint\nthe source of failures. In contrast, some studies opt for a\nvalue-based approach, employing sensor data as inputs for\npolicy training rather than relying on images [13], [15], [24],\n[35]. This approach allows researchers to focus more directly\non enhancing the decision-making capabilities of the policy\nwithout consideration of image processing ability. This study\ncontributes value-based datasets designed for training driving\npolicies across diverse driving scenarios.\nIII. PROBLEM FORMULATIONS FOR AUTONOMOUS\nDRIVING TASKS\nThis section introduces driving scenarios and datasets for\noffline reinforcement learning to train an autonomous driving\npolicy. We consider the human driver dataset to capture the\nfundamental essence of offline reinforcement learning and\nsynthetic datasets generated by online reinforcement learning\nagents. Subsequently, we look deeper into a unified POMDP\nthat can work across driving scenarios.\nA. Driving Scenarios\nThis subsection aims to extend the FLOW framework’s\ndriving scenario to a more realistic level by introducing three\ncomplex driving scenarios: 1) highway, 2) lane reduction,\nand 3) cut-in traffic. These scenarios are carefully designed\nto reflect real-world road environments more accurately.\nFig. 1: Highway sce-\nnario\nHighway Traffic (H): Fig. 1\nillustrates the highway traffic sce-\nnario. We aim to simulate a more\nrealistic driving environment by\nincorporating the patterns of the\nUS-101 highway dataset. This\nincludes capturing the diversi-\nfied characters of vehicles on\nthe road (e.g., desired velocity,\nsafety distance). The goal of the\nautonomous vehicle is to navigate\nthe complex environment by mak-\ning adaptive decisions, with the aim of selecting a reliable\nand optimal path.\nFig. 2: Lane reduction scenario\nLane\nReduction\n(L):\nFig.\n2\nrepresents\nthe\nlane\nreduction/expansion\nscenario. Lane reduction\nroad structures involve the\nconversion of a multi-lane road into a road with fewer lanes.\nReducing the number of lanes limits the vehicle capacity\nof the road, resulting in a bottleneck that concentrates all\nvehicles in a narrower area and causes traffic congestion.\nTherefore, drivers are forced to slow down and be more\ncautious. Additionally, the drivers negotiate with other\nvehicles to transit through the lane reduction area. The\nautonomous vehicle aims to find a low-density lane and\nmove while keeping a safe distance.\nFig. 3: Cut-in scenario\nCut-in Scenario (C): In\nFig. 3, the cut-in scenario\nis illustrated, which high-\nlights the autonomous vehi-\ncle’s capability to overtake\nother vehicles obstructing\nthe agent’s path. If the autonomous vehicle successfully\novertakes other slow vehicles and maintains its desired\nvelocity, it can maximize its rewards. To construct this\nscenario, we consider two setups: 1) adjusting the autonomous\nvehicle’s desired velocity to be higher than that of the non-\nautonomous vehicle, and 2) regularly placing non-autonomous\nvehicles on the road. We expect that the autonomous vehicle\nwill overtake other vehicles to maintain its desired velocity.\nTo configure realistic traffic in simulation, we have analyzed\nthe US Highway 101 dataset provided by the NGSIM\nproject [18], [19]. We introduce several attributes and proper-\nties of this dataset (Fig. 4). 1) Length of road and vehicle:\nFig. 4A represents the maximum longitudinal position as\napproximately 2195.4 feet, and Fig. 4B depicts the average\nvehicle length as approximately 14.6 feet. 2) Target velocities:\nWe have first confirmed quartile 3 of velocity distribution per\nvehicle. Fig. 4D shows the distribution comprising quartile\n3 velocities of all observed vehicles in the dataset. We have\nselected five values (min, Q1, Q2, Q3, and max) as\nthe target velocities. 3) The number of vehicles: Fig. 4C\nshows that approximately 117 vehicles have been driving on\naverage per time unit. Subsequently, we distribute the 117\nvehicles into five vehicle types, which mean vehicles with five\ndifferent target velocities (i.e., min, Q1, Q2, Q3, and max\nin Fig. 4D).\nB. Datasets\nThis subsection describes the actual and synthetic driving\ndatasets for applying offline reinforcement learning. The num-\nber of transitions included in each dataset is approximately\none million.\nHuman Driver Dataset. This research objective is to\nassess the feasibility and practicability of implementing offline\nreinforcement learning using an actual driving dataset. We\nutilize the Highway US-101 dataset, which is selected from\nthe FHWA traffic analysis tool of the NGSIM Project [18],\n[19]. We use the value-based dataset of NGSIM and pre-\nprocess the raw data fitted to the proposed POMDP (e.g.,\nerror correction, value normalization, and alignment with\nPOMDP in Section IV-A). Note that this dataset only applies\nto highway traffic scenarios due to the unique properties of\nthe roads where the data was gathered.\nSynthetic Driving Dataset. These datasets were generated\nby an online reinforcement learning agent using the deep\ndeterministic policy gradient (DDPG) [36] algorithm, which\nworks on continuous action and state spaces. We adopt diverse\ndatasets to comprehend how their quality influences the\nperformance of offline algorithms.\n• Human-like: It is a synthetic dataset generated by\nthe Intelligent Driver Model (IDM) controller [37]\ndesigned to reflect the human driving pattern. This\ncontrol-theoretic model focuses solely on acceleration\ncontrol; to incorporate a lane-changing maneuver, we\nalso employ the LC2013 model [38], a manually de-\nsigned lane-changing controller available within the\nSUMO simulator.\n• Final, Medium, and Random: These datasets are\ncollected by exploiting different policies, which could\nbe obtained at different points in the training phase of\nA. Longitudinal position\nB. Length of vehicles\nC. The number of vehicles per time\nD. Quartile 3 velocity distribution\n(feet)\nFig. 4: A: Distribution of all vehicles’ longitudinal position.\nB: Distribution of vehicle length over all driving vehicles in\ndataset. C: Distribution regarding the number of vehicles per\ntime frame. D: Distribution over quartile 3 velocity of all\nvehicles.\nonline reinforcement learning. When synthesizing the\n“Final” dataset, we consider the fully pre-trained policy\nthrough online reinforcement learning. Subsequently,\nthe “Medium” dataset is gathered using an intermediate\npolicy obtained by an early-stopping method. The\n“Random” dataset is based on the randomly initialized\nand unrolled policy in all scenarios.\n• Final-Medium, and Final-Random: These datasets are\na blend of two other datasets in equal proportions.\nThe “Final-Medium/Final-Random” dataset is literally\ncombined the “Final” and “Medium/Random” datasets.\nIV. REINFORCEMENT LEARNING FOR AUTONOMOUS\nDRIVING SYSTEM\nThe realistic reinforcement learning problem is generally\nformalized as a POMDP M = ⟨S, O, A, r, T , Ω, ρ0, γ⟩that\nincludes a state s ∈S, an observation o ∈O, an action\na ∈A, a reward r(s, a, s′) ∈R, a state transition probability\nT (s′|s, a), an observation probability Ω(o|s), an initial state\ndistribution ρ0, and temporal discount factor γ ∈[0, 1). The\nagent aims to maximize the expected discounted cumulative\nreward Es0∼ρ0,o∼Ω(·|s),a∼π(·|o),s′∼T (·|s,a)\nh P\nt γtr(s, a, s′)\ni\n.\nThe offline reinforcement learning samples the transitions\n(o, a, o′, r) from the fixed dataset D, thereafter minimizing\nan estimate of the Bellman error as follows.2\nL(θ) = E(o,a,o′,r)∼D\nh\nQθ(o, a) −r + γQθ′(o′, π(·|o′))\ni\nA. Unified POMDP Model\nThis subsection presents a unified POMDP structure, which\ncan address the three different scenarios as a decision-making\nprocess for autonomous driving. We employ an actor-critic\nalgorithm to train an autonomous vehicle as an agent on\ncontinuous observation and action spaces. The agent learns a\n2In this paper, the superscript ‘′’ implies the information on next timestep.\nFor instance, v′\nn represents the velocity of the vehicle en at the next timestep.\ndriving policy π that determines optimal behavior based on\nobservable information. To elaborate, the agent enhances its\npolicy by taking into account the reward signal that arises\nfrom the observation-action pair (o, a). The primary goal of\nthe agent is to maximize the accumulated reward.\nWe define road conditions as two sets: the number of vehi-\ncles and the number of lanes. The set E = {e1, e2, · · · , eN}\nrepresents the deployed vehicles on the road. It composes\nthe set of autonomous vehicles Eav and the set of non-\nautonomous vehicles Enon, i.e., E = Eav ∪Enon. Subse-\nquently, the set K = {1, 2, · · · , L} means the number of\nlanes configured on the road. Note that the number of lanes\nin specific segments can be less than the total number of\nlanes (e.g., lane reduction scenario).\nState: The state s ∈S contains information about all\nvehicles on the road, as follows:\ns = [v1, p1, k1, v2, p2, k2, · · · , vN, pN, kN]⊤,\nwhere vn, pn, and kn represent the velocity, position, and\nlane number of vehicle en, respectively. When the N vehicles\nexist on the road, the dimension of the state s ∈R3×N.\nObservation: The agent cannot access the complete\nstate information, thereby relying on partial information\nabout the state to make decisions. This constraint arises\nfrom the observability limitations of an autonomous vehicle.\nSpecifically, the agent can observe vehicles within a restricted\nperceivable space V surrounding them. The perceivable space\ncan be defined as the area that covers both the longitudinal\nspace Vlong ∈[V long\nmin , V long\nmax ] (front and behind areas) and\nthe lateral space Vlat ∈[V lat\nmin, V lat\nmax] (left and right sides).\nThe vehicles in this space are defined as perceivable vehicles\nei ∈E of the agent en. Let us define a set of perceivable\nvehicles as Esv. The set Esv can be formulated as {ei|ei ∈\nE −{en}, pi −pn ≤| Vlong\n2\n|, ki −kn ≤| Vlat\n2 |}.\nObservable\nvehicles\nare\ndefined\nas\nthe\nclosest\nleading and following vehicles per visible lane among\nperceivable\nvehicles.\nNamely,\nthe\nmaximum\nnumber\nof\nobservable\nvehicles\nis\n2(2Vlat\nmax + 1),\ncomprising\n(2Vlat\nmax + 1)\nleading\nand\n(2Vlat\nmax + 1)\nfollowing\nvehicles: the set of observable leading vehicles EL =\n{eLLVlat\nmax, · · · , eLL2, eLL1, eLS, eLR1, eLR2, · · · , eLRVlat\nmax},\nand the set of observable following vehicles EF\n=\n{eF LVlat\nmax, · · · , eF L2, eF L1, eF S, eF R1, eF R2, · · · , eF RVlat\nmax}\nHerein, the first subscripts L, F mean the leading and\nfollowing vehicle; the second subscripts L, S, R represent the\nleft, same, and right lanes. To elaborate, additional subscripts\nof L, R mean the lane gap based on the same lane S, i.e., L1\nand LVlat\nmax are the nearest and farthest left lane, respectively.\nThe observation o ∈O of the agent en is defined as\nfollows.\no = [vn, ∆v⊤, ∆p⊤, ρ⊤, ζ⊤]⊤\n(1)\nIn (1), ∆v denotes a vector of relative velocities between\nthe agent and observable vehicles, ∆p means a vector of\nrelative distances between the agent and observable vehicles,\nρ = [ρLVlat\nmax, · · · , ρS, · · · , ρRVlat\nmax]⊤represents a vector of\nlane traffic density, and ζ = [ζLVlat\nmax, · · · , ζS, · · · , ζRVlat\nmax]⊤\ndepicts a vector of existence of lanes beyond the longitudinal\nperceivable space.\nAction: The vector of action a ∈A of the agent comprises\nacceleration and lane-changing, i.e., a = [aacc, alc]⊤. The\nacceleration behavior aacc is decided in continuous action\nspace [Amin, Amax], range between maximum deceleration\nand acceleration. Next, the lane-changing maneuver alc is\ncontrolled in discrete action space {−1, 0, 1}. Herein, alc =\n−1 and alc = 1 indicate that the agent decides to move to the\nright and left lane, respectively; alc = 0 represents keeping\na lane.\nReward: The agent executes the action a in a given state\ns, thereby receiving a reward r. The reward is determined by\na reward function, which is expressed as a linear combination\nof five reward components, i.e.,\nr = R(s, a, s′) = η1R1 +η2R2 +η3R3 +η4R4 +η5R5 +C.\nThe non-negative coefficient ηn ≥0 refers to the weight\nassigned to the n-th reward component Rn, and C represents\na scaling constant.\nThe first reward component R1 is designed to keep the\nspeed near the desired speed v∗without overspeeding the\nspeed limit vlimit, which always satisfies vlimit −v∗> 0.\nR1 =\n( v′\nn\nv∗\nv′\nn ≤v∗\nvlimit−v′\nn\nvlimit−v∗\nv′\nn > v∗\nWhen 0 ≤v′\nn ≤vlimit, R1 becomes positive value, and if\nv′\nn = v∗, it is maximized (i.e., R1 = 1); when v′\nn > vlimit,\nR1 becomes negative value as penalty.\nThe second reward component R2 induces the agent to\nperform lane-changing that frees up driving space.\nR2 = |alc|(∆p′\nSL −∆pSL)\nThis component is activated when the agent performs lane-\nchanging (e.g., |alc| = 1). In other words, if |alc| = 0,\nthen R2 = 0. Subsequently, if driving space is secured after\nchanging lanes (i.e., ∆p′\nSL ≥∆pSL), and it means desirable\naction, resulting in R2 > 0. On the other hand, if the agent\nfails to secure the driving space after changing lanes (i.e.,\n∆p′\nSL < ∆pSL), thereby receiving a penalty R2 < 0.\nThe third and fourth reward components are related to the\nsafe distance between the leading and following vehicles,\nrespectively. These components prevent the violation of the\nsafe distance s∗.\nThe third reward component is defined as follows:\nR3 = min\n\"\n0, 1 −\n\u0012 s∗\nLS\n∆p′\nLS\n\u00132#\n,\n(2)\nwhere s∗\nLS means that the safe distance between the agent\nen and eLS and is defined as follows [37].\ns∗\nLS = s0 + max\n\"\n0, v′\nn\n \nt∗+\n∆v′\nLS\n2\np\n|Amin × Amax|\n!#\nHerein, s0 indicates the minimum safe distance between\nvehicles, and t∗means the minimum time headway, which is\nTABLE I: Average performance (± confidence interval with two standard deviations) on driving scenarios and datasets. The\nscores are based on 10 evaluations with five random seeds. Cyan and red highlight boxes depict the best score in each dataset\nand each scenario, respectively.\nTask Name\nBC\nImitative [26]\nBCQ [28]\nCQL [30]\nIQL [31]\nEDAC [39]\nPLAS [40]\nhighway-US101-NGSIM\n1202.07 ± 216.79\n1446.87 ± 242.17\n1501.99 ± 101.02\n1260.59 ± 134.1\n1261.58 ± 155.81\n1253.35 ± 16.46\n1468.11 ± 25.89\nhighway-final\n1565.64 ± 28.03\n821.77 ± 438.46\n1563.26 ± 33.76\n1473.31 ± 162.78\n1537.80 ± 28.13\n1219.17 ± 59.66\n1551.31 ± 24.56\nhighway-medium\n1393.42 ± 52.39\n419.57 ± 361.79\n1402.91 ± 30.69\n1377.20 ± 73.71\n1423.18 ± 27.39\n1244.65 ± 60.38\n1407.75 ± 30.69\nhighway-random\n622.15 ± 234.29\n-2.12 ± 13.05\n884.28 ± 93.03\n-10.42 ± 14.06\n433.69 ± 49.06\n1266.66 ± 65.83\n1543.02± 104.65\nhighway-final-medium\n1543.33 ± 49.49\n20.87 ± 291.16\n1450.23 ± 65.49\n1434.55 ± 144.34\n1615.46 ± 25.18\n1181.51 ± 53.19\n1431.30 ± 20.94\nhighway-final-random\n963.19 ± 218.79\n25.90 ± 83.13\n659.38 ± 76.95\n240.73 ± 30.09\n835.75 ± 335\n1261.58 ± 38.62\n1569.594 ± 65.28\nhighway-human-like\n980.80 ± 477.69\n123.02 ± 339.94\n797.98 ± 226.53\n766.19 ± 250.68\n1146.09 ± 270.23\n1245.43 ± 4.25\n771.11 ± 180.78\nlanereduction-final\n968.46 ± 36.73\n967.75 ± 67.39\n1266.84 ± 240.33\n984.36 ± 151.21\n1248.22 ± 235.20\n268.79 ± 18.89\n965.67 ± 30.83\nlanereduction-medium\n311.11 ± 82.90\n303.06 ± 33.04\n1260.16 ± 280.21\n344.17 ± 45.61\n1319.43 ± 190.68\n271.62 ± 18.74\n387.01 ± 31.95\nlanereduction-random\n106.99± 54.20\n139.36 ± 14.19\n31.05 ± 3.73\n3.18 ± 1.82\n312.86 ± 101.68\n269.61 ± 23.16\n1088.64 ± 45.48\nlanereduction-final-medium\n502.11 ± 44.96\n790.79 ± 120.77\n1162.62 ± 276.56\n554.56 ± 83.02\n1257.35± 175.99\n278.61 ± 21.85\n366.21 ± 35.64\nlanereduction-final-random\n157.98 ± 54.20\n791.42 ± 133.52\n9.82 ± 18.74\n66.40 ± 12.58\n157.27 ± 119.49\n312.99 ± 24.59\n604.6 ± 86.55\nlanereduction-human-like\n1160.63 ± 91.30\n960.40 ± 154.79\n1449.62 ± 176.54\n470.34 ± 71.13\n1443.84 ± 175.25\n166.88 ± 142.23\n1228.03 ± 8.66\ncutin-final\n1711.55 ± 159.27\n953.72 ± 74.21\n1155.46 ± 215.64\n1012.24 ± 180.62\n1737.22 ± 244.82\n946.1732 ± 19.3026\n1110.21 ± 5.94\ncutin-medium\n1634.74 ± 79.01\n971.67 ± 93.78\n1607.19 ± 80.29\n1201.13 ± 190.26\n1513.23 ± 88.41\n946.1717 ± 19.3038\n1596.30± 24.12\ncutin-Random\n972.58 ± 147.16\n1243.71 ± 257.91\n1101.06 ± 194.16\n638.37 ± 106.86\n756.27 ± 128.54\n946.1699 ± 19.3031\n1477.45 ± 333.78\ncutin-final-medium\n1084.62 ± 135.39\n866.66 ± 66.61\n1144.52 ± 196.31\n1085.40 ± 128.06\n1446.98 ± 143.01\n946.1731 ± 19.3008\n531.35± 41.69\ncutin-final-random\n599.56 ± 136.57\n1135.27 ± 283.30\n1425.52 ± 310.94\n1101.67 ± 188.81\n798.89 ± 119.34\n946.1719 ± 19.3014\n1655.24 ± 95.89\ncutin-human-like\n863.22 ± 166.86\n724.22 ± 113.87\n1391.40 ± 250.34\n1082.06 ± 178.75\n1105.40 ± 290.50\n946.1704 ± 19.3011\n1354.08 ± 19.02\nthe shortest time that a following vehicle can achieve without\nreducing velocity. This component induces that the agent\nmaintains the safe distance s∗\nLS from the leading vehicle in\nthe same lane. Specifically, if ∆p′\nLS < s∗\nLS, then R3 in (2)\nbecomes the negative value; otherwise, R3 is not activated\n(i.e., R3 = 0).\nThe fourth reward component is as follows:\nR4 = |alc| min\n\"\n0, 1 −\n\u0012 s∗\nF S\n∆p′\nF S\n\u00132#\n.\n(3)\nIn (3), s∗\nF S denotes that the safe distance between the agent\nen and eF S and is defined as follows [37].\ns∗\nF S = s0 + max\n\"\n0, v′\nF S\n \nt∗+\n∆v′\nF S\n2\np\n|Amin × Amax|\n!#\nThe same as (2), R4 ≤0 is always satisfied. In (3), R4\ncan be non-zero only if |alc| ̸= 0, which indicates that the\nagent changes the lanes. In contrast to R3, R4 switches\non only when the agent changes the lane, as maintaining a\nsafe distance while staying in the lane is contingent on the\nfollowing vehicle. Specifically, if ∆p′\nF S < s∗\nF S and |alc| ̸= 0,\nthen R4 < 0 is satisfied.\nFinally, the fifth reward component R5 is related to the\naccident (e.g., inter-vehicle crash, changing the nonexistent\nlanes) and is defined as follows.\nR5 =\n(\n0\naccident not happened\n−1\naccident happened\n(4)\nThe agent receives the penalty when the agent’s action\ncontributes to the accident (i.e., if the agent cannot continue\ndriving). Typically, the fifth balancing weight η5 is assigned\nthe highest value among coefficients η1, · · · , η5.\nRegardless of the driving scenario, the agent takes the\ndecision-making process using the proposed POMDP. Note\nthat the proposed POMDP is designed for safe and efficient\ndriving without considering the routing of the destination.\nV. BENCHMARKING BASELINE PERFORMANCES\nThis section provides the simulation results across all\ndriving scenarios and datasets. To provide the performance\nbaseline for autonomous driving scenarios, we performed\nextensive experiments and evaluated the performance of\nstate-of-the-art offline reinforcement learning algorithms,\nincluding Behavioral Cloning (BC), imitative learning (DDPG\n+ BC) [26], batch constrained Q (BCQ) [28], conservative Q\nlearning (CQL) [30], implicit Q learning (IQL) [31], ensemble-\ndiversified actor-critic (EDAC) [39], and policy in the latent\naction space (PLAS) [40].3 As discussed in section III-B,\nwe utilize the DDPG as the online reinforcement learning\nalgorithm.4\nA. Metrics\nWe use three evaluation metrics to verify the algorithm’s\nand dataset’s performance per driving scenario.\n1) Normalized Score: It measures the effectiveness of\noffline reinforcement learning algorithms when the expert\nonline agent’s performance scorefinal is set as a baseline. The\nnormalized score is calculated as\nnormalized score =\n(score −scorerandom)\n(scorefinal −scorerandom),\nwhere scorerandom represents the score of a random policy.\n3Note that all algorithms cannot aim to work on hybrid action space but\nwell work on discrete action space through simple quantization.\n4Why not use the TD3? We utilize the negative reward trick to prevent\nadverse effects on the approximation process when considering negative and\npositive reward signals. In such a setup, TD3 [41], a higher version of DDPG,\ncan lead to empirically overestimating the absolute Q-value. Therefore, we\nemploy the DDPG.\nC. Cut-in\nA. Highway\nB. Lane Reduction\nFig. 5: Normalized score for each algorithm per driving scenario. The average performance across synthetic datasets (except\nfor Human-like) is provided to mitigate the impact of the dataset and to highlight the impact of the algorithm.\nC. Cut-in\nA. Highway\nB. Lane Reduction\nFig. 6: The IQR range of performance for each dataset per driving scenario. To mitigate the impact of the algorithms and to\nhighlight the impact of the dataset, the IQR range includes the results of all algorithms. The diamonds indicate outliers, and\nthe boxes represent the interquartile range. Herein, the start, middle, and end of the box indicate the quartile 1, 2, and 3,\nrespectively.\n2) Inter-quartile Range (IQR): We use the IQR to\ndisplay the performance range for each dataset [42]. The\nIQR mitigates the impact of outliers, so it can be a more\nrobust and statistically efficient measure than the median or\nmean.\nB. Simulation Results\nThe results offer the following advantages: 1) facilitating\nautonomous driving research by exploring offline reinforce-\nment learning possibilities; 2) discussing the usability of the\nactual dataset in offline reinforcement learning. All simulation\nresults are presented in Table I. The presented score is the\naverage normalized score over five random seeds. Each policy\nis evaluated by averaging the performance over ten executions.\nPerformance over algorithms: Fig. 5 shows the average\nnormalized score per driving scenario and algorithm. This\nevaluation result presents the average performance across\ndatasets. We comprehend that online reinforcement learning\nperformance generally outperforms offline reinforcement\nlearning performance. It is intuitive because online reinforce-\nment learning can guarantee higher performance if there is\nenough exploration period.\nPerformance over dataset: Fig. 6 presents the IQR of\nthe evaluation score per driving scenario and dataset. This\nevaluation results include the performance of all algorithms.\nIn Fig. 6B-C, the results empirically imply two interesting\ninsights: 1) the performance of NGSIM and Human-like\ndatasets are comparable to synthetic datasets (close to Final\nand Medium for the most part), and 2) The performance ranks\nof the synthetic datasets is fair-minded (Final > Final-Medium\n> Medium > Final-Random > Random). On the other hand,\nIn Fig. 6C, the average performance of the dataset is far from\nexpected. The Final dataset contains samples with the highest\nperformance, but the overall performance is highest in the\nMedium.\nVI. CONCLUSION\nThis study presents an autonomous driving framework\nbased on offline reinforcement learning, accompanied by\nbenchmark performances and datasets that are readily ac-\ncessible and reproducible. The driving scenarios within the\nFLOW framework have been expanded to include three\nrealistic road structures: Cut-in, Lane Reduction, and Highway.\nA unified POMDP has been developed, applicable to all\ndriving scenarios. The contribution of this work extends\nbeyond providing synthetic datasets obtained from online\nreinforcement learning for each driving scenario, as it also\nincludes pre-processed real-world driving datasets, such as\nthe NGSIM dataset, aligned with the proposed POMDP. As\na result, interesting insights have been obtained through the\nanalysis of the results.\nIn summary, the primary aim of this study is to utilize\npre-collected datasets to facilitate research in the field of\nautonomous driving with offline reinforcement learning. We\nexpect to accelerate progress in this domain and open up new\navenues for further exploration.\nREFERENCES\n[1] I. Goodfellow, Y. Bengio, and A. Courville, Deep learning.\nMIT\npress, 2016.\n[2] S. Pouyanfar, S. Sadiq, Y. Yan, H. Tian, Y. Tao, M. P. Reyes, M. Shyu,\nS. Chen, and S. Iyengar, “A survey on deep learning: Algorithms,\ntechniques, and applications,” ACM Computing Surveys, vol. 51, no. 5,\npp. 1–36, 2018.\n[3] A. Kamilaris and F. X. Prenafeta-Bold´u, “Deep learning in agriculture:\nA survey,” Computers and Electronics in Agriculture, vol. 147, pp.\n70–90, 2018.\n[4] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.\nBellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski,\net al., “Human-level control through deep reinforcement learning,”\nNature, vol. 518, no. 7540, pp. 529–533, 2015.\n[5] R. Sutton and A. Barto, Reinforcement learning: An introduction. MIT\npress, 2018.\n[6] H. Niu, Y. Qiu, M. Li, G. Zhou, J. HU, X. Zhan, et al., “When\nto trust your simulator: Dynamics-aware hybrid offline-and-online\nreinforcement learning,” in Neural inf. process. syst., vol. 35, 2022, pp.\n36 599–36 612.\n[7] S. Levine, A. Kumar, G. Tucker, and J. Fu, “Offline reinforcement\nlearning: Tutorial, review, and perspectives on open problems,” arXiv\npreprint arXiv:2005.01643, 2020.\n[8] S. Lange, T. Gabel, and M. Riedmiller, “Batch reinforcement learning,”\nReinforcement learning, pp. 45–73, 2012.\n[9] X. Fang, Q. Zhang, Y. Gao, and D. Zhao, “Offline reinforcement\nlearning for autonomous driving with real world driving data,” in IEEE\nIntell. Transp. Syst. Conf., 2022, pp. 3417–3422.\n[10] X. Liang, T. Wang, L. Yang, and E. Xing, “CIRL: Controllable imitative\nreinforcement learning for vision-based self-driving,” in Eur. Conf. on\nComput. Vision, 2018, pp. 584–599.\n[11] A. Kumar, A. Singh, S. Tian, C. Finn, and S. Levine, “A workflow for\noffline model-free robotic reinforcement learning,” in Conf. on Robot\nLearn., 2022, pp. 417–428.\n[12] B. R. Kiran, I. Sobh, V. Talpaert, P. Mannion, A. A. Al Sallab, S. Yo-\ngamani, and P. P´erez, “Deep reinforcement learning for autonomous\ndriving: A survey,” IEEE Transactions on Intelligent Transportation\nSystems, vol. 23, no. 6, pp. 4909–4926, 2021.\n[13] J. Fu, A. Kumar, O. Nachum, G. Tucker, and S. Levine, “D4RL:\nDatasets for deep data-driven reinforcement learning,” arXiv preprint\narXiv:2004.07219, 2020.\n[14] T. Shi, D. Chen, K. Chen, and Z. Li, “Offline reinforcement learning for\nautonomous driving with safety and exploration enhancement,” arXiv\npreprint arXiv:2110.07067, 2021.\n[15] E. Vinitsky, A. Kreidieh, L. Le Flem, N. Kheterpal, K. Jang, C. Wu,\nF. Wu, R. Liaw, E. Liang, and A. M. Bayen, “Benchmarks for\nreinforcement learning in mixed-autonomy traffic,” in Conf. on Robot\nLearn., 2018, pp. 399–409.\n[16] A. R. Kreidieh, C. Wu, and A. M. Bayen, “Dissipating stop-and-go\nwaves in closed and open networks via deep reinforcement learning,”\nin IEEE Intell. Transp. Syst. Conf., 2018, pp. 1475–1480.\n[17] C. Gong, Z. Yang, Y. Bai, J. He, J. Shi, A. Sinha, B. Xu, X. Hou,\nG. Fan, and D. Lo, “Mind your data! Hiding backdoors in offline\nreinforcement learning datasets,” arXiv preprint arXiv:2210.04688,\n2022.\n[18] U. S. Department of Transportation Federal Highway Administra-\ntion, “Next generation simulation (NGSIM) program US-101 videos.\n[Dataset]. Provided by ITS DataHub through data.transportation.gov,”\n2016, Accessed 2022-06-05 from http://doi.org/10.21949/1504477.\n[19] U. D. of Transportation, “NGSIM—next generation simulation,” 2008.\n[20] R. Prudencio, M. Maximo, and E. Colombini, “A survey on offline\nreinforcement learning: Taxonomy, review, and open problems,” IEEE\nTransactions on Neural Networks and Learning Systems, 2023.\n[21] D. Troullinos, G. Chalkiadakis, I. Papamichail, and M. Papageorgiou,\n“Collaborative multiagent decision making for lane-free autonomous\ndriving,” in Int. Conf. on Auton. Agents and Multi-agent Syst., 2021,\npp. 1335–1343.\n[22] M. Stryszowski, S. Longo, E. Velenis, and G. Forostovsky, “A\nframework for self-enforced interaction between connected vehicles: In-\ntersection negotiation,” IEEE Transactions on Intelligent Transportation\nSystems, vol. 22, no. 11, pp. 6716–6725, 2020.\n[23] K. Jang, E. Vinitsky, B. Chalaki, B. Remer, L. Beaver, A. A.\nMalikopoulos, and A. Bayen, “Simulation to scaled city: zero-shot\npolicy transfer for traffic control via autonomous vehicles,” in Int. Conf.\non Cyber-Physical Syst., 2019, pp. 291–300.\n[24] C. Wu, A. Kreidieh, K. Parvate, E. Vinitsky, and A. Bayen, “FLOW:\nA modular learning framework for mixed autonomy traffic,” IEEE\nTransactions on Robotics, vol. 38, no. 2, pp. 1270–1286, 2021.\n[25] Q. Li, Z. Peng, L. Feng, Q. Zhang, Z. Xue, and B. Zhou, “Metadrive:\nComposing diverse driving scenarios for generalizable reinforcement\nlearning,” IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 2022.\n[26] S. Fujimoto and S. Gu, “A minimalist approach to offline reinforcement\nlearning,” in Neural inf. process. syst., vol. 34, 2021, pp. 20 132–20 145.\n[27] T. Osa, J. Pajarinen, G. Neumann, A. Bagnell, P. Abbeel, J. Peters,\net al., “An algorithmic perspective on imitation learning,” Foundations\nand Trends in Robotics, vol. 7, no. 1-2, pp. 1–179, 2018.\n[28] S. Fujimoto, D. Meger, and D. Precup, “Off-policy deep reinforcement\nlearning without exploration,” in Int. Conf. on Mach. Learn., 2019, pp.\n2052–2062.\n[29] A. Kumar, J. Fu, M. Soh, G. Tucker, and S. Levine, “Stabilizing off-\npolicy Q-learning via bootstrapping error reduction,” in Neural inf.\nprocess. syst., vol. 32, 2019.\n[30] A. Kumar, A. Zhou, G. Tucker, and S. Levine, “Conservative Q-learning\nfor offline reinforcement learning,” in Neural inf. process. syst., vol. 33,\n2020, pp. 1179–1191.\n[31] I. Kostrikov, A. Nair, and S. Levine, “Offline reinforcement learning\nwith implicit Q-learning,” in Int. Conf. on Learn. Representations,\n2022.\n[32] R. McAllister, B. Wulfe, J. Mercat, L. Ellis, S. Levine, and A. Gaidon,\n“Control-aware prediction objectives for autonomous driving,” in IEEE\nInt. Conf. on Robot. and Automat., 2022, pp. 01–08.\n[33] S. Pini, C. Perone, A. Ahuja, A. Ferreira, M. Niendorf, and\nS. Zagoruyko, “Safe real-world autonomous driving by learning to\npredict and plan with a mixture of experts,” in IEEE Int. Conf. on\nRobot. and Automat., 2023, pp. 10 069–10 075.\n[34] H. Chiu, J. Li, R. Ambrus¸, and J. Bohg, “Probabilistic 3D multi-modal,\nmulti-object tracking for autonomous driving,” in IEEE Int. Conf. on\nRobot. and Automat., 2021, pp. 14 227–14 233.\n[35] N. Kheterpal, E. Vinitsky, C. Wu, A. Kreidieh, K. Jang, K. Parvate,\nand A. Bayen, “FLOW: Open source reinforcement learning for traffic\ncontrol,” in Neural inf. process. syst., 2018.\n[36] T. Lillicrap, J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver,\nand D. Wierstra, “Continuous control with deep reinforcement learning,”\nin Int. Conf. on Learn. Representations, 2016.\n[37] M. Treiber, A. Hennecke, and D. Helbing, “Congested traffic states in\nempirical observations and microscopic simulations,” Physical Review\nE, vol. 62, no. 2, p. 1805, 2000.\n[38] J. Erdmann, “SUMO’s lane-changing model,” Modeling Mobility with\nOpen Data, pp. 105–123, 2015.\n[39] G. An, S. Moon, J.-H. Kim, and H. O. Song, “Uncertainty-based offline\nreinforcement learning with diversified Q-ensemble,” in Neural inf.\nprocess. syst., vol. 34, 2021, pp. 7436–7447.\n[40] W. Zhou, S. Bajracharya, and D. Held, “PLAS: Latent action space\nfor offline reinforcement learning,” in Conf. on Robot Learn., 2021,\npp. 1719–1735.\n[41] S. Fujimoto, H. Hoof, and D. Meger, “Addressing function approxi-\nmation error in actor-critic methods,” in Int. Conf. on Mach. Learn.,\n2018, pp. 1587–1596.\n[42] R. Agarwal, M. Schwarzer, P. S. Castro, A. Courville, and M. Bellemare,\n“Deep reinforcement learning at the edge of the statistical precipice,”\nin Neural inf. process. syst., vol. 34, 2021, pp. 29 304–29 320.\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2024-04-03",
  "updated": "2024-04-03"
}