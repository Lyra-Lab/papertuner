{
  "id": "http://arxiv.org/abs/2404.03997v1",
  "title": "Demonstration Guided Multi-Objective Reinforcement Learning",
  "authors": [
    "Junlin Lu",
    "Patrick Mannion",
    "Karl Mason"
  ],
  "abstract": "Multi-objective reinforcement learning (MORL) is increasingly relevant due to\nits resemblance to real-world scenarios requiring trade-offs between multiple\nobjectives. Catering to diverse user preferences, traditional reinforcement\nlearning faces amplified challenges in MORL. To address the difficulty of\ntraining policies from scratch in MORL, we introduce demonstration-guided\nmulti-objective reinforcement learning (DG-MORL). This novel approach utilizes\nprior demonstrations, aligns them with user preferences via corner weight\nsupport, and incorporates a self-evolving mechanism to refine suboptimal\ndemonstrations. Our empirical studies demonstrate DG-MORL's superiority over\nexisting MORL algorithms, establishing its robustness and efficacy,\nparticularly under challenging conditions. We also provide an upper bound of\nthe algorithm's sample complexity.",
  "text": "Demonstration-Guided Multi-Objective Reinforcement Learning\nJunlin Lu 1 Patrick Mannion 1 Karl Mason 1\nAbstract\nMulti-objective reinforcement learning (MORL)\nis increasingly relevant due to its resemblance\nto real-world scenarios requiring trade-offs be-\ntween multiple objectives. Catering to diverse\nuser preferences, traditional reinforcement learn-\ning faces amplified challenges in MORL. To\naddress the difficulty of training policies from\nscratch in MORL, we introduce demonstration-\nguided multi-objective reinforcement learning\n(DG-MORL). This novel approach utilizes prior\ndemonstrations, aligns them with user preferences\nvia corner weight support, and incorporates a self-\nevolving mechanism to refine suboptimal demon-\nstrations. Our empirical studies demonstrate DG-\nMORL’s superiority over existing MORL algo-\nrithms, establishing its robustness and efficacy,\nparticularly under challenging conditions. We\nalso provide an upper bound of the algorithm’s\nsample complexity.\n1. Introduction\nSingle-objective reinforcement learning (SORL) has exhib-\nited promise in various tasks, such as Atari Games (Mnih\net al., 2013), robotics (Schulman et al., 2017), drone racing\n(Kaufmann et al., 2023), and smart home energy manage-\nment (Lu et al., 2022a). However, most real-world chal-\nlenges are not confined to single objectives but often en-\ncompass multiple objectives. For example, there is always a\nneed to make a trade-off between safety and time efficiency\nin vehicle operation or to balance heating-energy usage\nand cost-saving for home heating. These examples under-\nscore the necessity to extend SORL into a multi-objective\nsetting, known as multi-objective reinforcement learning\n(MORL) (Van Moffaert & Now´e, 2014; Hayes et al., 2022).\nIn MORL, the target is to learn an optimal policy set that\nmaximizes the cumulative discounted reward vector. Users\ncan set preferences to scalarize this reward vector, aiding in\n1Department of Computer Science, University of Gal-\nway,\nGalway,\nIreland.\nCorrespondence to:\nJunlin Lu\n<J.Lu5@universityofgalway.ie>.\nthe comparison of different objectives.\nHowever, MORL introduces additional complexities and\nsignificantly magnifies the challenges already present in\nSORL. The challenges include: (i) Sparse reward (Oh et al.,\n2018; Ecoffet et al., 2019; 2021; Wang et al., 2023); (ii)\nHard beginning, where at the onset of training, policies\noften face substantial challenges in improving (Li, 2022;\nUchendu et al., 2023); (iii) Derailment, where the agent may\nsteer off the path returning to promising areas of state space\ndue to stochasticity (Ecoffet et al., 2019). We elaborate on\nthese challenges in Appendix A.1.\nEssentially, these issues—sparse feedback, lack of prior\nknowledge, and forgetfulness—represent different facets\nof the same underlying problem: low efficiency in explo-\nration. A potential solution is to leverage prior demon-\nstrations as guidance. This approach can be instrumen-\ntal in providing meaningful feedback, integrating expert\nknowledge, and guiding the agent back toward promising\nareas of the state space. Unfortunately, there are currently\nno established methods for using prior demonstrations as\nguidance in MORL. Existing techniques are exclusively\ndesigned for SORL (Ecoffet et al., 2019; 2021; Uchendu\net al., 2023). We attribute this gap to several key obstacles\nin applying demonstration as guidance within MORL: (i)\nDemonstration-preference misalignment: where the pref-\nerence rendering a demonstration optimal is unknown be-\nforehand; (ii) Demonstration deadlock: the paradox that the\nrequirement to have the number of prior demonstrations n is\ncontingent upon the outcome of the training process itself.\nThis stops directly extending the SORL demonstration uti-\nlization method to MORL; (iii) Sub-optimal demonstration:\nthere is a probability that sub-optimal demonstrations will\nlead to sub-optimal policy. These challenges are further\ndiscussed in Appendix A.2.\nIn this paper, we propose a novel MORL algorithm, i.e.\ndemonstration-guided multi-objective reinforcement learn-\ning (DG-MORL) that utilizes demonstrations as guidance\nto improve exploration efficiency while overcoming those\nobstacles. The demonstrations can take any form as long\nas they can be converted to action sequences. This makes\nit adaptable for utilizing data from either human experts, a\npre-trained policy, or even hand-crafted rule-based trajec-\ntories. We evaluate demonstrations initialized by different\n1\narXiv:2404.03997v1  [cs.LG]  5 Apr 2024\nDemonstration-Guided Multi-Objective Reinforcement Learning\nsources and experimentally prove they all work. To the best\nof our knowledge, DG-MORL is the first MORL algorithm\nusing demonstration as guidance. It surpasses the state-of-\nthe-art MORL algorithms in complex MORL tasks. The key\ncontributions of our work include:\n(i) We propose the DG-MORL algorithm which harnesses\ndemonstrations as guidance to enhance both the effective-\nness and efficiency of MORL training.\n(ii) We introduce a self-evolving mechanism to adapt and\nimprove the quality of guiding data by gradually transferring\nfrom prior generated demonstrations to the agent’s self-\ngenerated demonstrations. This mechanism safeguards the\nagent against sub-optimal demonstrations and helps it learn\nhigh-performing policies.\n(iii) Rather than requiring a large and costly demonstration\nset, our DG-MORL algorithm only needs a small number\nof demonstrations and achieves few-shot manner in train-\ning. When there is a significant absence of prior generated\ndemonstrations, it can still outperform the state-of-the-art\nMORL algorithm with a tolerable decline in performance.\n(iv) DG-MORL algorithm offers a universal framework that\ncan be utilized as an enhancement to any MORL algorithm,\nequipping it with the capability to utilize demonstrations as\na form of assistance.\nWe outline the related work in Section 2. In Section 3 we\npresent the preliminary of this paper. In Section 4, we for-\nmally introduce the DG-MORL algorithm. We illustrate the\nexperiment setting, demonstrate and discuss the experiment\nresults in Section 5. In Section 6, we conclude the paper.\n2. Related Work\n2.1. Multi-Objective Reinforcement Learning\nMORL methods are categorized as single-policy methods\nand multi-policy methods (Yang et al., 2019). They are dis-\ntinguished by the number of policies to solve the problem.\nIn single-policy methods, the MORL problem is reduced\nto a SORL problem by using a preference to scalarize the\nvectorized reward feedback. The preference is supposed to\nbe given, and therefore a standard SORL policy improve-\nment approach can be implemented (Mannor & Shimkin,\n2001; Tesauro et al., 2007; Van Moffaert et al., 2013). How-\never, when the preference is not known beforehand, these\nsingle-policy algorithms cannot work properly.\nThe multi-policy methods aim at learning a policy set to\ngenerate an approximation of the Pareto front. An intuitive\nand commonly used methodology is to train a single policy\nmultiple times upon the different preferences (Roijers et al.,\n2014; Mossalam et al., 2016), while Pirotta et al. (Pirotta\net al., 2015) introduced a policy-based algorithm to learn\na manifold-based optimal policy. These methods suffer\nfrom the curse of dimensionality when the preference space\ngrows large. This limits its scalability to complex tasks.\nTo overcome this limitation, methods taking preference as\ninput, while using a single neural network to approximate\na general optimal policy for various preferences were pro-\nposed (Abels et al., 2019; Yang et al., 2019; K¨allstr¨om &\nHeintz, 2019). Abels et al. and Yang et al. use vectorized\nvalue function updates. Specifically, in the work of Yang et\nal., the policy for a given preference can be improved using\nexperiences from other preferences. Distinguished from the\nother two methods, Kallstrom et al. use a scalarized value\nfunction rather than a vectorized value function approxi-\nmation. By directing the policy training with updating the\ngradient according to the alignment of value evaluation and\npreference, the general policy for MORL methodology is\nfurther improved (Basaklar et al., 2023; Alegre et al., 2023).\nBasaklar et al. used a cosine similarity between Q-values\nand preference weight to achieve an efficient update, while\nAlegre et al. used generalized policy improvement (GPI)\nto guide the policy to update along with the most potential\ndirection in preference space.\nThis work extends the single-network multi-policy MORL\nmethod by incorporating demonstrations to guide and en-\nhance the efficiency of the exploration and training.\n2.2. Prior demonstration Utilization Approaches\nIn literature, methods using prior policies for RL initializa-\ntion usually are a combination of imitation learning (IL)\nand RL. One of the direct ways to initialize an RL agent\nis to use behavior cloning to copy the behavior pattern of\nthe prior policy (Rajeswaran et al., 2017; Lu et al., 2022a).\nThis method, though can effectively help with the beginning\nstage of RL training, cannot work well with value-based RL\nframeworks (Uchendu et al., 2023). Furthermore, behavior\ncloning cannot work well with multi-objective settings if\nthe preference is not known beforehand.\nPrevious studies (Vecerik et al., 2017; Nair et al., 2018;\nKumar et al., 2020) have proposed methods for integrating\nprior data into replay memory to help with training. These\ntechniques have been advanced through the use of offline\ndatasets for pre-training, followed by policy fine-tuning\n(Nair et al., 2020; Lu et al., 2022b). Ball et al. proposed a\nsimple way to use offline data to help with online RL (Ball\net al., 2023) to reduce the size of the offline dataset. How-\never, these methods typically require an offline dataset. In\ncontrast, our approach operates under less stringent assump-\ntions, relying only on a few selected demonstrations.\nRegrettably, the aforementioned methods have not been\nadapted to or validated within MORL settings. This ab-\nsence of extension and empirical validation hinders direct\ncomparisons between our method and these approaches.\n2\nDemonstration-Guided Multi-Objective Reinforcement Learning\nAdditionally, the field of multi-objective imitation learning\nremains under-explored, presenting another limitation for\ncomparative analysis. Therefore, our algorithm is evaluated\nin comparison with other existing MORL algorithms.\n2.3. Effective Exploration Approaches\nAs we have introduced, the exploration efficiency may have\nmuch influence at the beginning stage of training. Sev-\neral approaches have been introduced to improve the explo-\nration efficiency, including reward shaping (Ng et al., 1999;\nMannion, 2017), curiosity-driven exploration (Barto et al.,\n2004), and model-based reinforcement learning (MBRL)\n(Polydoros & Nalpantidis, 2017; Alegre et al., 2023).\nHowever, these approaches still have notable shortcomings.\nFor example, the reliance on domain expertise for reward\nshaping (Devidze et al., 2022); The inherent vulnerability\ntrapped in local optima in curiosity-driven exploration (Ecof-\nfet et al., 2021); The computation overload and inaccuracy\nof environment model construction for MBRL.\nUsing prior demonstration has the following advantages to\nmitigate these drawbacks. It allows for the integration of\ndomain knowledge without an explicit reward-shaping func-\ntion. This can significantly alleviate the reliance on domain\nexpertise; With the guidance of demonstration, the agent\ncan navigate the state space more effectively and return to\npromising areas that it might have otherwise struggled to\nrevisit before the curiosity intrinsic signal is used up; Us-\ning prior demonstration does not require the construction\nof an environment model, as is the case in MBRL. This\nfrees the policy from computational demands and model\nuncertainties associated with model-based methods.\nUnfortunately, existing demonstration-utilization methods\noften need pre-trained Q functions (Nair et al., 2018;\nKostrikov et al., 2021; Lu et al., 2022c) which are not al-\nways available. Algorithms like Go-Explore (Ecoffet et al.,\n2021) and JSRL (Uchendu et al., 2023) do not require a\npre-trained Q function. However, these approaches face lim-\nitations in MORL, primarily due to their inability to address\nissues such as demonstration-preference misalignment and\ndemonstration deadlock.\n3. Preliminaries\nWe formalise a multi-objective Markov decision process\n(MOMDP) as M := (S, A, T , γ, µ, R) (Hayes et al.,\n2022), where S and A are state and action space; T\n:\nS × A × S −→[0, 1] is a probabilistic transition function;\nγ ∈[0, 1) is a discount factor; µ : S0 −→[0, 1] is the initial\nstate probability distribution; R : S × A × S −→Rd is a\nvectorized reward function, providing the reward signal for\nthe multiple (d ≥2) objectives.\nThe MORL state-action value function under policy π is\ndefined as:\nqπ(s, a) := Eπ[\nX\nt=0\nγtrt|s, a]\n(1)\nwhere qπ(s, a) is a d-dimensional vector denoting the ex-\npected vectorized return by following policy π, rt is the\nreward vector on t step.\nThe multi-objective value vector of policy π with the initial\nstate distribution µ is:\nvπ := Es0∼µ[qπ(s0, π(s0)]\n(2)\nvπ is the value vector of policy π, and the i-th element of\nvπ is the returned value of the i-th objective by following\npolicy π.\nThe Pareto front consists of a set of nondominated value\nvectors. The Pareto dominance relation (≻p) is:\nvπ ≻p vπ′ ⇐⇒(∀i : vπ\ni ≥vπ′\ni ) ∩(∃i : vπ\ni > vπ′\ni ) (3)\nWe say that vπ is nondominated when all element in vπ are\nnot worse than any vπ′ that at least one element of vπ is\ngreater than the other vπ′. The Pareto front is defined as:\nF := {vπ|∄π′ s.t. vπ′ ≻p vπ}\n(4)\nAn optimal policy should always be one of the set of all\npolicies whose resultant multi-objective value vector is on\nF. To select an optimal policy out of the set, some criteria\nneed to be used for further comparison. The criteria are usu-\nally given by the user and therefore termed as user-defined\npreference. To involve different user-defined preferences\nover the objectives, a utility function is used (Hayes et al.,\n2022) to map the value vector to a scalar. The utility func-\ntion is sometimes referred to as scalarization function in\nliterature. One of the most frequently used utility functions\nis the linear utility function (Hayes et al., 2022). A linear\nweight vector w denoting the user preference of importance\nover each objective is given to scalarize vπ.\nu(vπ, w) = vπ\nw = vπ · w\n(5)\nwhere w is on a simplex W : Pd\ni wi = 1, wi ≥0. W is\ntermed as weight space. With the linear utility function u\non W, we can define the convex coverage set (CCS) (Hayes\net al., 2022). The CCS is a finite convex subset of the Pareto\nfront. Any point on CCS denotes that there exists at least one\noptimal policy under some linear weight vector. Formally\na CCS is defined as: CCS := {vπ ∈F|∃w s.t. ∀vπ′ ∈\nF, vπ · w ≥vπ′ · w}. CCS has all the nondominated\nvalues when a linear utility function is used. The optimal\npolicy set is resultant values on the CCS.\n3\nDemonstration-Guided Multi-Objective Reinforcement Learning\nIn the cutting-edge MORL methods (Yang et al., 2019; Ale-\ngre et al., 2023), π is typically trained through learning a\nstate-action vector value function, i.e. the vectorized Q func-\ntion, Qπ(s, a, w). This vectorized Q function can adapt\nto any preference weight vector. However, though these\nmethods have achieved significant improvement upon the\nprevious MORL algorithms, they still suffer from poor sam-\nple efficiency as they search for the CCS by starting from\nthe original point of the solution space. See Figure 1(a),\na traditional MORL method starts from the red point, and\nsearches to expand to the orange CCS.\nFigure 1. (a) Training process of traditional MORL; (b) Training\nprocess of demonstration-guided MORL\nIdeally, by giving guiding demonstrations, the training pro-\ncess can be transformed into a scenario depicted in Figure\n1(b). Here, the policy is informed by knowledge derived\nfrom the demonstrations, allowing it to commence not from\nthe original point, but rather from the red points which rep-\nresent solutions provided by the demonstrations. This shift\nsignificantly reduces the distance between the “known” so-\nlution and the CCS, thereby decreasing sample complexity\nand enhancing the efficiency of the training process.\nHowever, a limitation is the provided demonstrations’ sub-\noptimality. This introduces the risk of the policy being\nconfined within the bounds of sub-optimal demonstrations.\nRefer to Figure 2 (a), where the green dot represents the pol-\nicy approximated by a Q network. The gap between the red\nstarting points indicating the demonstration, and the CCS\ncan be challenging to bridge if the policy solely satisfies\nabout meeting the performance of the prior given demon-\nstrations. To visually represent the challenge of reaching\nthe true CCS we have placed X marks on it.\nA potential solution is that the policy may chance upon\nbetter trajectories. When the policy turns to this better guid-\nance, it will exceed the initial demonstrations. Figure 2(b)\nillustrates this concept, where the dot alternating between\nred and green represents a demonstration identified by the\nQ function. Importantly, these new demonstrations are dis-\ncovered by the agent itself. The initial demonstrations serve\nmerely as a catalyst, helping the agent navigate the challeng-\ning early learning phase. This concept is formalized as the\nself-evolving mechanism.\nFigure 2. (a) Demonstration-guided MORL without self-evolving;\n(b) Demonstration-guided MORL with self-evolving\n4. Demonstration-Guided Multi-Objective\nReinforcement Learning\nIn this section, we present the formal model of the\nDG-MORL algorithm.\nWe begin by addressing the\ndemonstration-preference misalignment, followed by in-\ntroducing the self-evolving mechanism and then we explain\nhow demonstration deadlock and the sub-optimal demonstra-\ntion problem are addressed. Finally, we provide an in-depth\ndescription of the DG-MORL algorithm, accompanied by\ntheoretical analysis.\n4.1. Demonstrations Corner Weight Computation\nBy iterating action sequences from demonstrations, the set\nof resultant vector returns can be calculated as the vector-\nized return Vd. We denote the convex coverage set of the\ndemonstrations as CCSd.\nThe term corner weights denoted as Wcorner is first pro-\nposed by Roijers, D.M. (Roijers, 2016). It is a finite subset\nof the weight simplex W, over a set of value vectors V.\nDefinition 4.1. (Definition 19 of Roijers (Roijers, 2016))\nGiven a set of value vectors V, we can define a polyhedron\nP by:\nP = {x ∈Rd+1|V +x ≤0,\nX\ni\nwi = 1, wi ≥0, ∀i} (6)\nwhere V + represents a matrix in which the elements of\nthe set V serve as the row vectors augmented by -1’s as the\ncolumn vector.\nThe corner weights effectively partition the solution space.\nIn informal terms, they classify the policies. The choice\nof the optimal policy maximizing utility for a given weight\nvaries as we traverse across corner weights. By computing\nWcorner on CCSd, we can assign corresponding weights\nto the demonstrations.\nEmploying corner weights efficiently reduces the computa-\ntional burden by calculating on a finite set Wcorner rather\nthan on an infinite weight simplex W. Additionally, the sig-\nnificance of using corner weights in training is emphasized\nby the theorem:\n4\nDemonstration-Guided Multi-Objective Reinforcement Learning\nTheorem 4.2. (Theorem 7 of Roijers (Roijers, 2016)) There\nis a corner weight w that maximizes:\n∆(w) = uCCS\nw\n−max\nπ∈Π uπ\nw\n(7)\nwhere ∆(w) is the difference between the utility of optimal\npolicy given weight w and the utility obtained by a policy\nπ ∈Π, where Π denotes the current agent’s policy set. The-\norem 4.2 suggests that training along a corner weight allows\nfor faster learning of the optimal policy (set), as it offers the\npotential for achieving maximum improvement. Assuming\nthe prior demonstration is the known optimal solution, by re-\nplacing the CCS in equation 7 with CCSd, Equation 7 can\nbe rewritten as: ∆d(w) = uCCSd\nw\n−max\nπ∈Π uπ\nw. Equation 8\nfurther exploits the theorem to select the most proper corner\nweight wc from Wcorner to increase the training efficiency.\nwc = argmax\nw∈Wcorner\n∆(w)\n(8)\nWe have elucidated the calculation of the corner weights set\nand the selection of an appropriate candidate corner weight\nfor aligning the demonstration. With this approach, the issue\nof demonstration-preference misalignment is addressed.\n4.2. Self-Evolving Mechanism\nWe now propose the self-evolving mechanism. To remain\nconsistent with the terminology used in existing literature\n(Uchendu et al., 2023), the demonstrations are referred to\nas guide policies, while the policies developed are called\nexploration policies.\nWe use the notation Πg to represent the set of guide poli-\ncies.\nSince the exploration policy set is approximated\nby a single neural network approximator, we denote it as\nΠe = Pi\n|W|πwi\ne . It is conditioned on the input preference\nweight vector and serves as the policy to be improved via\nlearning. The agent interacts with the environment during\ntraining with a mix of guide policy and exploration policy,\nwe denote this mixed policy as π.\nThe self-evolving mechanism is designed to continuously\nupdate and augment the guide policy set Πg throughout the\nlearning process. When the agent utilizes the mixed pol-\nicy π, it may discover improved demonstrations over the\nexisting ones. This enables more effective learning from\nthese superior demonstrations rather than persisting with\nsub-optimal data, thereby mitigating the impact of initially\nsub-optimal demonstrations. Moreover, the agent might\nuncover demonstrations that were not initially provided.\nThese new demonstrations can introduce a fresh set of cor-\nner weights, enriching the guiding curriculum and address-\ning the demonstration deadlock. As learning progresses,\nless effective demonstrations are phased out, allowing the\ndemonstration set to evolve and improve in performance.\nWe refer to this ongoing process of enhancement as the\nself-evolving mechanism, symbolized by Πg −→Π′\ng. Given\nthat the initial demonstrations are typically sub-optimal, the\nguidance policy set at the end of training is likely to consist\npredominantly of self-generated data from the agent. The\nusage of the self-evolving mechanism is explained in Algo-\nrithm 1. We also did an ablation study to show the necessity\nof the self-evolving mechanism, see Appendix C.1.\n4.3. Multi-Stage Curriculum\nTo make full use of the demonstration, the mixed policy π\nis mostly controlled by the guide policy πg at the beginning\nof training. This can navigate the agent to a more promising\narea in the state space than randomly exploring. The control\nof the guide policy is diluted during training and taken over\nby the exploration policy. However, swapping the guide\npolicy with the exploration policy introduces the challenge\nof the policy shift. A multi-stage curriculum pattern, as\nsuggested by (Uchendu et al., 2023), is implemented. This\nstrategy involves gradually increasing the number of steps\ngoverned by the exploration policy, facilitating a smoother\ntransition and mitigating the policy shift issue.\nCompared with JSRL (Uchendu et al., 2023), our approach\nis further extended by using the self-evolving mechanism.\nBy eliminating outdated solutions, the MORL policy can\nachieve progressive improvement. Another departure is that\nit no longer involves swapping control between one single\nguide policy and the exploration policy. Instead, it entails\na controlling swap between the set of guide policies to one\nexploration policy. Given a preference weight vector w,\na guide policy upon which the exploration may gain the\nlargest improvement is selected in the next round of training\nas shown in Equation 9.\nπg = argmax\nπ∈Πg\n∆π(w) = argmax\nπ∈Πg\n(uπ\nw −uπe\nw )\n(9)\n4.4. Algorithm\nWe provide a detailed explanation of the DG-MORL in\nAlgorithm 1. To initiate the process, a set of prior demon-\nstrations is evaluated, and the return values are added to\nCCSd. Dominated values are subsequently discarded, and\nassume that these retained values represent “optimal” solu-\ntions. The corner weights are then computed based on these\nvalues.\nNext, a suitable guide policy is selected by using Equation 9.\nThe episode horizon is denoted as H. The guide policy con-\ntrol scale is denoted as h, i.e. for the first h steps, the guide\npolicy controls the agent, while for the next H −h steps, the\nexploration policy is performed. The exploration policy’s\nperformance is periodically evaluated. If it demonstrates bet-\nter performance, h is reduced to grant the more controllable\nsteps to the exploration policy, and the better demonstra-\n5\nDemonstration-Guided Multi-Objective Reinforcement Learning\ntion is popped into the demonstration repository to achieve\nself-evolving. The CCS is periodically updated to elimi-\nnate dominated solutions. A new set of corner weights is\ncomputed based on the updated CCS.\nIn certain instances, the guide policy may be so advanced\nthat it becomes challenging for the exploration policy to ex-\nceed its performance. This scenario can restrict the rollback\nsteps and therefore limit the exploration policy’s touching\nscope of states, consequently impeding the learning effi-\nciency. For example, the agent could find itself unable to\nprogress beyond the initial stages of learning a solution due\nto the high difficulty level of the guide policy. To address\nthis issue, we introduce the concept of a passing percent-\nage, denoted as β ∈[0, 1]. This strategy involves setting a\ncomparatively lower performance threshold initially, mak-\ning it more achievable for the agent to learn. Over time,\nthis threshold is incrementally raised, thereby progressively\nenhancing the agent’s performance.\nAlgorithm 1 DG-MORL Algorithm\n1: Input: prior demonstration Πg, curriculum pass thresh-\nold β\n2: Evaluate the Πg, add the corresponding multi-objective\nvalue vector to CCSd\n3: CCS ←−CCSd\n4: while max training step not reached do\n5:\nGet corner weight set Wcorner=Equation 6(CCS)\n6:\nGet candidate weight wc=Equation 7(Wcorner, Πg)\n7:\nSample the guide policy πg=Equation 9(wc, Πg)\n8:\nCalculate the utility threshold: uθ = vπg · wc\n9:\nThe guide policy πg controllable scale h = H\n10:\nwhile h ≥0 do\n11:\nSet mix policy π = π[0:h]\ng\n+ π[h+1:H]\ne\n12:\nRoll out π, gather the experience\n13:\nTrain explore policy πe\n14:\nEvaluate π: u = vπ · wc\n15:\nif u > uθ · β then\n16:\nAdd the better value vector vπ to CCS\n17:\nh ←−h −∆h, ∆h is the rollback span\n18:\nend if\n19:\nend while\n20:\nUpdate CCS and Πg by removing the dominated\nsolutions.\n21:\nUpdate the curriculum pass threshold β\n22: end while\n4.5. Theoretical Analysis\nWe provide a theoretical analysis of the algorithm sample\nefficiency’s lower bound and upper bound. When the explo-\nration policy is 0-initialized (all Q-values are zero initially),\na non-optimism-based method, e.g. ϵ-greedy, suffers from\nexponential sample complexity when the horizon expands.\nWe extend the combination lock (Koenig & Simmons, 1993;\nUchendu et al., 2023) under the multi-objective setting (2\nobjectives) and we have the following Theorem 4.3. For\nfurther explanation, please refer to Appendix D.1\nTheorem 4.3. ((Koenig & Simmons, 1993; Uchendu et al.,\n2023)) When using a 0-initialized exploration policy, there\nexists a MOMDP instance where the sample complexity is\nexponential to the horizon to see a partially optimal solu-\ntion and a multiplicative exponential complexity to see an\noptimal solution.\nThis provides the lower bound of the sample complexity\nfor an extreme case for a MORL agent to reach a satisfying\nperformance. We now give the general upper bound of the\nalgorithm in Theorem 4.4.\nTheorem 4.4. After T rounds of training, the difference\nexpected utility of the optimal policy π∗and current policy\nπ is shown as:\nEUπ∗−EUπ ≤C(T)f(T, Rd)\n(10)\nwhere C(T) is the upper bound of the guide policy perfor-\nmance that evolves over time, Rd is the d-dimensionality\nreward space, f(T, Rd) is the upper bound of the explo-\nration algorithm performance which is determined by the\ntotal training rounds and the reward range.\nThe upper bound is further discussed in Appendix D.2.\n5. Experiments\nIn this section, we introduce the benchmark environments.\nSubsequently, we present the baseline algorithms and the\ndetailed experiment setting. We then illustrate and discuss\nthe experiment results.\n5.1. Benchmark Environments\nWe evaluate our algorithm with commonly used challenging\nMORL tasks. The complexity of the tasks escalates from\nthe instance featuring discrete state and action spaces to\ncontinuous state space and discrete action space, and finally\nwith both continuous state and action spaces.\nDeep Sea Treasure. The deep sea treasure (DST) environ-\nment is commonly used in MORL setting (Vamplew et al.,\n2011; Abels et al., 2019; Yang et al., 2019; Alegre et al.,\n2023). The agent needs to trade off between the treasure\nvalue collected and the time penalty. It is featured by a\ndiscrete state space and action space. The state is the coor-\ndinate of the agent, i.e. [x,y]. The agent can move either\nleft, right, up, down. The treasure values are aligned with\nliterature (Yang et al., 2019; Alegre et al., 2023).\nMinecart. The Minecart environment (Abels et al., 2019;\nAlegre et al., 2023), is featured with continuous state space\n6\nDemonstration-Guided Multi-Objective Reinforcement Learning\nand discrete action space. The agent needs to balance among\nthree objectives, i.e. 1) to collect ore 1; 2) to collect ore 2;\n3) to save fuel. The minecart agent kicks off from the left\nupper corner, i.e. the base, and goes to any of the five mines\nconsisting of two different kinds of ores and does collection,\nthen it returns to the base to sell it and gets delayed rewards\naccording to the volume of the two kinds of ores.\nMO-Hopper. The MO-Hopper, shown in Figure 6(c), is an\nextension of the Hopper from OpenAI Gym (Borsa et al.,\n2018). It is used in literature to evaluate MORL algorithm\nperformance in robot control (Basaklar et al., 2023; Ale-\ngre et al., 2023). The agent controls the three torque of a\nsingle-legged robot to learn to hop. This environment in-\nvolves two objectives, i.e. maximizing the forward speed\nand maximizing the jumping height.\nIn each benchmark environment, we compare the algo-\nrithms with the metric of expected utility (EU): EU(Π) =\nEw∼W[maxπ∈Πvπ\nw]. This metric is a utility-based metric\n(Zintgraf et al., 2015; Hayes et al., 2022; Alegre et al., 2022;\n2023) used to evaluate the performance of the policy on the\nwhole preference simplex W1.\nFor a better comparison of exploration efficiency, we impose\nconstraints on episode lengths across benchmarks. This\nheightens the difficulty, compelling the agent to extract\ninformation from a shorter episode and ensuring a more\nrigorous assessment of candidate algorithms. See Appendix\nB.1 for further details of the benchmarks.\n5.2. Baseline Algorithms\nWe compare our DG-MORL algorithm with two state-of-\nthe-art MORL algorithms: generalized policy improvement\nlinear support (GPI-LS) and generalized policy improve-\nment prioritized Dyna (GPI-PD) (Alegre et al., 2023). The\nGPI-LS and GPI-PD are based on generalized policy im-\nprovement (GPI) (Puterman, 2014) while the GPI-PD algo-\nrithm is model-based and GPI-LS is model-free. As indi-\ncated in the literature (Alegre et al., 2023), the Envelope\nMOQ-learning algorithm (Yang et al., 2019), SFOLS (Ale-\ngre et al., 2022), and PGMORL (Wurman et al., 2022) are\nconsistently outperformed by the baseline algorithms em-\nployed in our study. Therefore, if our algorithm can surpass\nthe GPI-PD and GPI-LS baselines, it can be inferred that it\nalso exceeds the performance of these algorithms.\n5.3. Experiment Settings\nFor the sake of fair comparison, the implementation\nhttps://github.com/MORL12345/DG-MORL is adapted\nfrom the existing source (Alegre et al., 2023). Our imple-\n1To keep aligned with the literature, we calculate EU on 100\nequidistant weight vectors from W in each benchmark environ-\nment (Alegre et al., 2023).\nmentation discarded all components of the original model,\ne.g. GPI update and Dyna, etc. except the prioritized ex-\nperience replay. See details of the hyperparameters and\nexperiment settings in Appendix B.2. For baseline algo-\nrithms, we keep the setting as the original implementation.\nPlease refer to the original paper for further information\nabout those hyperparameters (Alegre et al., 2023).\n5.4. Result\nIn this section, we show the results of the EU evaluation.\nThe result of the ablation study without the self-evolving\nmechanism is in Appendix C.1. The sensitivity study of dif-\nferent initial demonstration quantities is in Appendix C.2.1\nand the sensitivity study of qualities of the guidance policy\nis in Appendix C.2.2. The impact of different rollback spans\nis shown in Appendix C.2.3. The impact of different pass-\ning percentages of the curriculum is presented in Appendix\nC.2.4. As the GPI-PD algorithm is a model-based version\nof GPI-LS, we note it as GPI-PD+GPI-LS in the Figures to\nkeep align with the literature (Alegre et al., 2023).\n5.4.1. EXPECTED UTILITY RESULTS\nWe randomly pick 5 seeds2 to run the DG-MORL algorithm\nand the baselines to evaluate the EU. The black dashed\nline in each result figure shows the EU performance of\nthe initial demonstrations. Initial demonstrations of DST,\nMinecart, and MO-Hopper are respectively from hard-coded\naction sequences, our manual play trajectories, and a pre-\nmatured policy of a TD3 agent. The EU is evaluated with\nthe agent solely using its exploration policy to interact with\nthe environment. The average results are depicted with solid\nlines, while the range of values is illustrated by shaded areas,\ncolor-matched to the corresponding algorithms.\nFigure 3. DST Expected Utility Result\n(i) EU result for DST\n2Random seeds: 2, 7, 15, 42, and 78\n7\nDemonstration-Guided Multi-Objective Reinforcement Learning\nFigure 4. Minecart Expected Utility Result\nFigure 5. MO-Hopper Expected Utility Result\nThe result of DST is shown in Figure 3. The EU is esti-\nmated per 4000 training steps. The result shows that the\nDG-MORL algorithm has learned a high-performing policy\nin the first round of evaluation and outperforms the initial\ndemonstration. In comparison, the GPI-PD baseline also\nattains a commendable level of performance in the early\nstages, but it is slightly inferior to DG-MORL and exhibits\nminor variations. The GPI-LS algorithm shows potential\nto reach a performance similar to GPI-PD initially, but it is\nmarkedly more vulnerable to variations, suggesting reduced\nrobustness. Furthermore, it fails to achieve the performance\nlevels of its counterparts. This finding aligns with expecta-\ntions, given the advantage GPI-PD has in the incorporation\nof an environment model. In comparison, our DG-MORL\noutperforms all baselines regarding learning efficiency, con-\nvergence capability, and overall robustness.\n(ii) EU result for Minecart\nFigure 4 shows the result on Minecart. The EU is calculated\nper 10000 training steps. The results suggest that under\nlimited exploration resources, the DG-MORL algorithm\nsurpasses the initial demonstration performances and shows\na more rapid learning curve, superior final performance, and\nincreased robustness than baseline algorithms. Although\nthe GPI-PD baseline eventually attains a satisfactory level\nof performance, it remains subpar relative to DG-MORL\nand suffers from instability due to varying random seed\ninfluences. Additionally, the GPI-PD does not match the\nlearning speed of our algorithm. The GPI-LS algorithm\nnot only faces challenges in developing an effective policy\nand exhibits a slower rate of learning, but also shows a\nvulnerability to randomness.\n(iii) EU result for MO-Hopper\nFigure 5 shows the result on MO-Hopper. The EU is esti-\nmated per 15000 steps. The results reveal that DG-MORL\nis adept at learning the most effective policy with the fastest\nrate of improvement and achieves competitive results as\nthe provided demonstration. Although it lags behind the\nGPI-PD baseline in the first evaluation, it quickly surpasses\nthis with a more pronounced rate of progress, ultimately\nachieving the highest performance. In contrast, the GPI-PD\nbaseline, despite showing notable improvement over time,\nadvances at a slower pace than DG-MORL and concludes\nwith a less effective policy. Lacking the support of an en-\nvironmental model and prior demonstrations, the GPI-LS\nbaseline gives the most inferior performance in the study.\nAcross all benchmark environments, the DG-MORL demon-\nstrates superiority in terms of performance, learning effi-\nciency, and robustness. An intriguing observation is that in\nenvironments characterized by greater continuity (both in\nstate and action spaces), the GPI-PD and GPI-LS exhibit\nreduced susceptibility to variations in random seeds. This\ncould be attributed to the fact that discrete environments are\neasier to be influenced by randomness. Conversely, this also\nunderscores the robustness of our method and its insensitiv-\nity to the continuity of the environment.\n6. Conclusion\nWe proposed the pioneering DG-MORL algorithm that uses\ndemonstrations to enhance training efficiency. We intro-\nduced the self-evolving mechanism to handle sub-optimal\ndemonstrations. Empirical evidence confirms DG-MORL’s\nsuperiority over state-of-the-art across various environments.\nAdditionally, our experiments highlight DG-MORL’s robust-\nness by excluding self-evolving and under other unfavoured\nconditions. We also provided theoretical insights into the\nlower bound and upper bound of DG-MORL’s sample effi-\nciency. Given that real-world scenarios frequently exhibit\nnon-linear preferences, a potential avenue for future research\nis addressing the limitation of DG-MORL’s applicability to\nlinear preference weights.\n8\nDemonstration-Guided Multi-Objective Reinforcement Learning\n7. Impact Statements\nThis paper endeavors to progress multi-objective reinforce-\nment learning by integrating demonstrations as guidance\nfor enhanced learning performance. Our research is still\nin preliminary stages and based on simulated experiments.\nThough it will inspire various societal benefits, there are no\nimmediate, specific social impacts necessitating emphasis\nat this point.\nReferences\nAbels, A., Roijers, D., Lenaerts, T., Now´e, A., and Steck-\nelmacher, D. Dynamic weights in multi-objective deep\nreinforcement learning. In International conference on\nmachine learning, pp. 11–20. PMLR, 2019.\nAlegre, L. N., Bazzan, A., and Da Silva, B. C. Optimistic\nlinear support and successor features as a basis for op-\ntimal policy transfer. In International Conference on\nMachine Learning, pp. 394–413. PMLR, 2022.\nAlegre, L. N., Bazzan, A. L., Roijers, D. M., Now´e, A., and\nda Silva, B. C. Sample-efficient multi-objective learning\nvia generalized policy improvement prioritization. arXiv\npreprint arXiv:2301.07784, 2023.\nBall, P. J., Smith, L., Kostrikov, I., and Levine, S. Efficient\nonline reinforcement learning with offline data. arXiv\npreprint arXiv:2302.02948, 2023.\nBarreto, A., Dabney, W., Munos, R., Hunt, J. J., Schaul, T.,\nvan Hasselt, H. P., and Silver, D. Successor features for\ntransfer in reinforcement learning. Advances in neural\ninformation processing systems, 30, 2017.\nBarreto, A., Hou, S., Borsa, D., Silver, D., and Precup, D.\nFast reinforcement learning with generalized policy up-\ndates. Proceedings of the National Academy of Sciences,\n117(48):30079–30087, 2020.\nBarto, A. G., Singh, S., Chentanez, N., et al. Intrinsically\nmotivated learning of hierarchical collections of skills.\nIn Proceedings of the 3rd International Conference on\nDevelopment and Learning, volume 112, pp. 19. Citeseer,\n2004.\nBasaklar, T., Gumussoy, S., and Ogras, U. Y. Pd-morl:\nPreference-driven multi-objective reinforcement learning\nalgorithm. 2023.\nBorsa, D., Barreto, A., Quan, J., Mankowitz, D., Munos,\nR., Van Hasselt, H., Silver, D., and Schaul, T.\nUni-\nversal successor features approximators. arXiv preprint\narXiv:1812.07626, 2018.\nDevidze, R., Kamalaruban, P., and Singla, A. Exploration-\nguided reward shaping for reinforcement learning under\nsparse rewards. Advances in Neural Information Process-\ning Systems, 35:5829–5842, 2022.\nDrugan, M. M. and Nowe, A. Designing multi-objective\nmulti-armed bandits algorithms: A study. In The 2013 in-\nternational joint conference on neural networks (IJCNN),\npp. 1–8. IEEE, 2013.\nEcoffet, A., Huizinga, J., Lehman, J., Stanley, K. O.,\nand Clune, J. Go-explore: a new approach for hard-\nexploration problems. arXiv preprint arXiv:1901.10995,\n2019.\nEcoffet, A., Huizinga, J., Lehman, J., Stanley, K. O., and\nClune, J. First return, then explore. Nature, 590(7847):\n580–586, 2021.\nHayes, C. F., R˘adulescu, R., Bargiacchi, E., K¨allstr¨om, J.,\nMacfarlane, M., Reymond, M., Verstraeten, T., Zintgraf,\nL. M., Dazeley, R., Heintz, F., et al. A practical guide\nto multi-objective reinforcement learning and planning.\nAutonomous Agents and Multi-Agent Systems, 36(1):26,\n2022.\nK¨allstr¨om, J. and Heintz, F. Tunable dynamics in agent-\nbased simulation using multi-objective reinforcement\nlearning. In Adaptive and Learning Agents Workshop\n(ALA-19) at AAMAS, Montreal, Canada, May 13-14,\n2019, pp. 1–7, 2019.\nKaufmann, E., Bauersfeld, L., Loquercio, A., M¨uller, M.,\nKoltun, V., and Scaramuzza, D. Champion-level drone\nracing using deep reinforcement learning. Nature, 620\n(7976):982–987, 2023.\nKoenig, S. and Simmons, R. G. Complexity analysis of\nreal-time reinforcement learning. In AAAI, volume 93,\npp. 99–105, 1993.\nKostrikov, I., Nair, A., and Levine, S. Offline reinforce-\nment learning with implicit q-learning. arXiv preprint\narXiv:2110.06169, 2021.\nKumar, A., Zhou, A., Tucker, G., and Levine, S. Con-\nservative q-learning for offline reinforcement learning.\nAdvances in Neural Information Processing Systems, 33:\n1179–1191, 2020.\nLi, Y. Reinforcement learning in practice: Opportunities\nand challenges. arXiv preprint arXiv:2202.11296, 2022.\nLu, J., Mannion, P., and Mason, K. A multi-objective multi-\nagent deep reinforcement learning approach to residential\nappliance scheduling. IET Smart Grid, 5(4):260–280,\n2022a.\nLu, J., Mannion, P., and Mason, K. Inferring preferences\nfrom demonstrations in multi-objective reinforcement\n9\nDemonstration-Guided Multi-Objective Reinforcement Learning\nlearning: A dynamic weight-based approach. Adaptive\nLearning Agent Workshop @ AAMAS 2023 London, 2023.\nLu, S., Wang, G., Hu, Y., and Zhang, L. Multi-objective gen-\neralized linear bandits. arXiv preprint arXiv:1905.12879,\n2019.\nLu, Y., Hausman, K., Chebotar, Y., Yan, M., Jang, E., Her-\nzog, A., Xiao, T., Irpan, A., Khansari, M., Kalashnikov,\nD., et al. Aw-opt: Learning robotic skills with imita-\ntion andreinforcement at scale. In Conference on Robot\nLearning, pp. 1078–1088. PMLR, 2022b.\nLu, Y., Yan, M., Zadeh, S. M. K., Herzog, A., Jang, E.,\nHausman, K., Chebotar, Y., Levine, S., and Irpan, A.\nLearning robotic skills with imitation and reinforcement\nat scale, December 29 2022c. US Patent App. 17/843,288.\nMannion, P. Knowledge-based multi-objective multi-agent\nreinforcement learning. PhD thesis, 2017.\nMannor, S. and Shimkin, N. The steering approach for\nmulti-criteria reinforcement learning. Advances in Neural\nInformation Processing Systems, 14, 2001.\nMnih, V., Kavukcuoglu, K., Silver, D., Graves, A.,\nAntonoglou, I., Wierstra, D., and Riedmiller, M. Playing\natari with deep reinforcement learning. arXiv preprint\narXiv:1312.5602, 2013.\nMossalam, H., Assael, Y. M., Roijers, D. M., and Whiteson,\nS. Multi-objective deep reinforcement learning. arXiv\npreprint arXiv:1610.02707, 2016.\nNair, A., McGrew, B., Andrychowicz, M., Zaremba, W.,\nand Abbeel, P. Overcoming exploration in reinforcement\nlearning with demonstrations. In 2018 IEEE international\nconference on robotics and automation (ICRA), pp. 6292–\n6299. IEEE, 2018.\nNair, A., Gupta, A., Dalal, M., and Levine, S. Awac: Accel-\nerating online reinforcement learning with offline datasets.\narXiv preprint arXiv:2006.09359, 2020.\nNg, A. Y., Harada, D., and Russell, S. Policy invariance\nunder reward transformations: Theory and application\nto reward shaping. In Icml, volume 99, pp. 278–287.\nCiteseer, 1999.\nOh, J., Guo, Y., Singh, S., and Lee, H. Self-imitation learn-\ning. In International Conference on Machine Learning,\npp. 3878–3887. PMLR, 2018.\nPirotta, M., Parisi, S., and Restelli, M. Multi-objective\nreinforcement learning with continuous pareto frontier\napproximation. In Proceedings of the AAAI conference\non artificial intelligence, volume 29, 2015.\nPolydoros, A. S. and Nalpantidis, L. Survey of model-\nbased reinforcement learning: Applications on robotics.\nJournal of Intelligent & Robotic Systems, 86(2):153–173,\n2017.\nPuterman, M. L.\nMarkov decision processes: discrete\nstochastic dynamic programming. John Wiley & Sons,\n2014.\nRajeswaran, A., Kumar, V., Gupta, A., Vezzani, G., Schul-\nman, J., Todorov, E., and Levine, S. Learning complex\ndexterous manipulation with deep reinforcement learning\nand demonstrations. arXiv preprint arXiv:1709.10087,\n2017.\nRoijers, D. M. Multi-objective decision-theoretic planning.\nAI Matters, 2(4):11–12, 2016.\nRoijers, D. M., Whiteson, S., and Oliehoek, F. A. Lin-\near support for multi-objective coordination graphs. In\nAAMAS’14: PROCEEDINGS OF THE 2014 INTERNA-\nTIONAL CONFERENCE ON AUTONOMOUS AGENTS\n& MULTIAGENT SYSTEMS, volume 2, pp. 1297–1304.\nIFAAMAS/ACM, 2014.\nSchulman, J., Wolski, F., Dhariwal, P., Radford, A., and\nKlimov, O. Proximal policy optimization algorithms.\narXiv preprint arXiv:1707.06347, 2017.\nTesauro, G., Das, R., Chan, H., Kephart, J., Levine, D.,\nRawson, F., and Lefurgy, C. Managing power consump-\ntion and performance of computing systems using re-\ninforcement learning. Advances in neural information\nprocessing systems, 20, 2007.\nUchendu, I., Xiao, T., Lu, Y., Zhu, B., Yan, M., Simon, J.,\nBennice, M., Fu, C., Ma, C., Jiao, J., et al. Jump-start\nreinforcement learning. In International Conference on\nMachine Learning, pp. 34556–34583. PMLR, 2023.\nVamplew, P., Dazeley, R., Berry, A., Issabekov, R., and\nDekker, E. Empirical evaluation methods for multiobjec-\ntive reinforcement learning algorithms. Machine learning,\n84:51–80, 2011.\nVan Moffaert, K. and Now´e, A. Multi-objective reinforce-\nment learning using sets of pareto dominating policies.\nThe Journal of Machine Learning Research, 15(1):3483–\n3512, 2014.\nVan Moffaert, K., Drugan, M. M., and Now´e, A. Scalar-\nized multi-objective reinforcement learning: Novel de-\nsign techniques. In 2013 IEEE symposium on adaptive\ndynamic programming and reinforcement learning (AD-\nPRL), pp. 191–199. IEEE, 2013.\nVecerik, M., Hester, T., Scholz, J., Wang, F., Pietquin, O.,\nPiot, B., Heess, N., Roth¨orl, T., Lampe, T., and Riedmiller,\n10\nDemonstration-Guided Multi-Objective Reinforcement Learning\nM. Leveraging demonstrations for deep reinforcement\nlearning on robotics problems with sparse rewards. arXiv\npreprint arXiv:1707.08817, 2017.\nWang, G., Wu, F., Zhang, X., Liu, J., et al. Learning diverse\npolicies with soft self-generated guidance. International\nJournal of Intelligent Systems, 2023, 2023.\nWurman, P. R., Barrett, S., Kawamoto, K., MacGlashan, J.,\nSubramanian, K., Walsh, T. J., Capobianco, R., Devlic,\nA., Eckert, F., Fuchs, F., et al. Outracing champion gran\nturismo drivers with deep reinforcement learning. Nature,\n602(7896):223–228, 2022.\nYang, R., Sun, X., and Narasimhan, K. A generalized al-\ngorithm for multi-objective reinforcement learning and\npolicy adaptation. Advances in neural information pro-\ncessing systems, 32, 2019.\nZintgraf, L. M., Kanters, T. V., Roijers, D. M., Oliehoek,\nF., and Beau, P. Quality assessment of morl algorithms:\nA utility-based approach. In Benelearn 2015: proceed-\nings of the 24th annual machine learning conference of\nBelgium and the Netherlands, 2015.\n11\nDemonstration-Guided Multi-Objective Reinforcement Learning\nA. Explanation of Challenges\nA.1. SORL Challenges in MORL Setting\nSparse reward: In scenarios where rewards are sparse, agents struggle to acquire sufficient information for policy improve-\nment. This challenge, widely recognized in reinforcement learning (RL) (Oh et al., 2018; Ecoffet et al., 2019; 2021; Wang\net al., 2023), can slow down the training process, heighten the risk of reward misalignment, and may result in suboptimal\npolicies or even training failure.\nHard beginning: At the onset of training, policies often face substantial challenges in improving (Li, 2022; Uchendu et al.,\n2023). This stems from the agent’s lack of prior knowledge about the task. Though exploration strategy like ϵ-greedy can be\nused to gradually mitigate this it is typically inefficient in the early stages of training.\nDerailment: The agent may steer off the path returning to promising areas of state space due to environmental stochasticity,\npolicy uncertainty, and the randomness inherent in exploration strategies (Ecoffet et al., 2019). This derailment can impede\nthe agent’s ability to revisit these advantageous areas, thereby restricting policy improvement.\nA.2. Challenges of Directly Extending SORL Demonstration-Utilization to MORL\nDemonstration-Preference Misalignment: In MORL, a demonstration may be optimal under certain preferences, but\ndetrimental under others. For an agent to effectively learn from a demonstration, it needs to know the preference that renders\nthe demonstration optimal, as a guide of training direction. In most cases, however, even the demonstrators themselves\ncannot identify the specific numeric preference under which the provided demonstration is the optimal (Lu et al., 2023).\nConsequently, when an agent is provided with a set of demonstrations without associated preference information, it struggles\nto correctly match a demonstration with the corresponding preference factor. This can lead the agent in a misguided direction,\nand potentially fail the training.\nDemonstration Deadlock: In SORL, the demonstration guidance method typically relies on at least one demonstration to\nassist in learning the optimal policy. However, in MORL, it becomes more complex due to the presence of several, let’s\nsay n, Pareto non-dominated policies, each optimal under different preferences. If we were to directly apply the existing\ndemonstration-based SORL approach to MORL, it would imply the need for at least n demonstrations to cover these various\npolicies. The challenge is that the exact number n remains unknown until all Pareto non-dominated policies are identified.\nThis creates a paradox: the requirement to have the number of demonstrations n is contingent upon the outcome of the\ntraining process itself. It can lead to a lack of necessary prior data and potentially impair the overall performance. We refer\nto this paradox as the demonstration deadlock.\nSub-optimal Demonstration: Though demonstrations that are slightly sub-optimal can still effectively guide the training\nprocess (Uchendu et al., 2023), it is uncertain whether sub-optimal demonstrations can similarly work in MORL. Sub-optimal\ndemonstrations might obstruct the training, potentially leading to the development of a sub-optimal policy, or in more severe\ncases, failure of the training.\nB. Experiment Setting\nB.1. Benchmark Environment Setting\nDeep Sea Treasure. The deep sea treasure (DST) environment is a commonly used benchmark for the MORL setting\n(Vamplew et al., 2011; Abels et al., 2019; Yang et al., 2019; Alegre et al., 2023), see Figure 6(a). The agent needs to make a\ntrade-off between the treasure value collected and the time penalty. It is featured by a discrete state space and action space.\nThe state is the coordinate of the agent, i.e. [x,y]. The agent can move either left, right, up, down in the environment. The\nreward vector is defined as r = [rtreasure, rstep], where rtreasure is the value of the treasure collected, and rstep = −1 is a\ntime penalty per step. The treasure values are adapted to keep aligned with recent literature (Yang et al., 2019; Alegre et al.,\n2023). The discounted factor γ = 0.99.\nMinecart. The Minecart environment (Abels et al., 2019; Alegre et al., 2023), see Figure 6(b), is featured with continuous\nstate space and discrete action space. The agent needs to balance among three objectives, i.e. 1) to collect ore 1; 2) to collect\nore 2; 3) to save fuel. The minecart agent kicks off from the left upper corner, i.e. the base, and goes to any of the five mines\nconsisting of two different kinds of ores and does collection, then it returns to the base to sell it and gets delayed rewards\naccording to the volume of the two kinds of ores.\n12\nDemonstration-Guided Multi-Objective Reinforcement Learning\nFigure 6. (a) Deep Sea Treasure; (b) Minecart; (c) MO-Hopper\nThis environment has a 7-dimension continuous state space, including the minecart’s coordinates, the speed, its angle cosine\nvalue and sine value, and the current collection of the two kinds of ores (normalized by the capacity of the minecart). The\nagent can select actions from the 6-dimension set, i.e. mine, turn left, turn right, accelerate, brake, idle. The reward signal is\na 3-dimensional vector consisting of the amount of the two kinds of ore sold and the amount of fuel it has used during the\nepisode. The reward for selling ores is sparse and delayed while the reward signal of fuel consumption is a constant penalty\nplus extra consumption when doing acceleration or mining.\nMO-Hopper. The MO-Hopper, shown in Figure 6(c), is an extension of the Hopper from OpenAI Gym (Borsa et al., 2018).\nIt is used in literature to evaluate MORL algorithm performance in robot control (Basaklar et al., 2023; Alegre et al., 2023).\nThe agent controls the three torque of a single-legged robot to learn to hop. This environment involves two objectives, i.e.\nmaximizing the forward speed and maximizing the jumping height. It has a continuous 11-dimensional state space denoting\nthe positional status of the hopper’s body components. This environment has a 3-dimensional continuous action space, i.e.\nthe different torques applied on the three joints. We use the same reward function and discounted factor γ = 0.99 (Alegre\net al., 2023).\nrvelocity = vx + C\n(11)\nrheight = 10(h −hinit) + C\n(12)\nwhere vx is the forward speed, h and hinit are the hoppers relative jumping height, C = 1 −P\ni a2\ni is the a feedback of the\nagent healthiness.\nTo more effectively compare the exploration efficiency of our algorithm with baseline methods, we impose constraints on\nepisode lengths across different environments. Specifically, we set the DST environment to terminate after 100 steps, the\nMinecart environment to conclude after 30 steps, and the MO-Hopper environment to end after 500 steps. This limitation\nintensifies the difficulty of the tasks, compelling the agent to extract sufficient information from a restricted number of\ninteractions. This constraint ensures a more rigorous assessment of each algorithm’s ability to efficiently navigate and learn\nwithin these more challenging conditions. The summary of benchmark settings is illustrated in Table 1.\nB.2. Implementation Detail\nWe use the same neural network architecture as the GPI-PD implementation in all benchmarks, i.e. 4 layers of 256 neurons\nin DST and Minecart, 2 layers with 256 neurons for both the critic and actor in MO-Hopper. We use Adam optimizer, the\nlearning rate is 3 · 10−4 and the batch size is 128 in all implementations. As for the exploration, we adopted the same\nannealing pattern ϵ-greedy strategy. In DST, the ϵ anneals from 1 to 0 during the first 50000 time steps. In Minecart, the ϵ\nis annealed from 1 to 0.05 in the first 50000 time steps. For the TD3 algorithm doing MO-Hopper, we take a zero-mean\nGaussian noise with a standard deviation of 0.02 to actions from the actor-network. All hyper-parameters are consistent with\nliterature (Alegre et al., 2023).\nThe initial demonstration data are from different sources to validate the fact that any form of demonstration can be used\n13\nDemonstration-Guided Multi-Objective Reinforcement Learning\nTable 1. Benchmark Environments\nEnvironment\nDST\nMinecart\nMO-Hopper\nState space S\ndiscrete\ncontinuous\ncontinuous\ndim(S)\n2\n7\n11\nAction space A\ndiscrete\ndiscrete\ncontinuous\ndim(A)\n4\n6\n3\ndim(R)1\n2\n3\n2\nEpisode max horizon\n100\n30\n500\nDiscounted factor2\n0.99\n0.98\n0.99\nPassing Percentage\n1\n1\n0.8−→0.98\nRollback Span3\n2\n2\n100\n1 The number of objectives / the dimension of reward vectors.\n2 Consistent with baselines (Alegre et al., 2023).\n3 The number of steps rolled back when the found trajectory\nsurpasses the given demonstration.\nas it can be converted to action sequences. The demonstration for DST is from hard-coded action sequences which are\nsub-optimal. Minecart uses the demonstrations manually played by the author, i.e. us, while the MO-Hopper, uses the\ndemonstration from a pre-matured policy from the TD3 algorithm.\nB.3. Baseline Algorithm\nWe compare our DG-MORL algorithm with two state-of-the-art MORL algorithms: generalized policy improvement linear\nsupport (GPI-LS) and generalized policy improvement prioritized Dyna (GPI-PD) (Alegre et al., 2023). The GPI-LS and\nGPI-PD are based on generalized policy improvement (GPI) (Puterman, 2014) while the GPI-PD algorithm is model-based\nand GPI-LS is model-free. As indicated in the literature (Alegre et al., 2023), the Envelope MOQ-learning algorithm\n(Yang et al., 2019), SFOLS (Alegre et al., 2022), and PGMORL (Wurman et al., 2022)are consistently outperformed by the\nbaseline algorithms employed in our study. Therefore, if our algorithm can surpass these baselines, it can be inferred that it\nalso exceeds the performance of these algorithms.\nThe GPI method combines a set of policies as an assembled policy where an overall improvement is implemented (Barreto\net al., 2017; 2020). It was introduced to the MORL domain by Algre et al. (Alegre et al., 2022). The GPI policy in MORL is\ndefined as πGP I(s|w) ∈arg max\na∈A\nmax\nπ∈Π qπ\nw(s, a). The GPI policy is at least as well as any policy in Π (policy set). The\nGPI agent can identify the most potential preference to follow at each moment rather than randomly pick a preference as in\nEnvelope MOQ-learning. This facilitates faster policy improvement in MORL. Furthermore, the model-based GPI-PD can\nfind out which piece of experience is the most relevant to the particular candidate preference to achieve even faster learning.\nC. Additional Experiments\nC.1. Ablation Study\nIn this section, we do an ablation study on the DG-MORL algorithm by excluding the self-evolving mechanism. The agent\nrelies solely on prior demonstrations, as opposed to progressively enhancing the guidance demonstration with self-generated\ndata. The ablation study uses the same random seeds set and the same demonstration set as in the EU evaluation part, see\nSection 5.4.1. The results of the GPI-PD algorithm serve as a reference point, depicted as a blue dashed line.\nFigure 7, Figure 8, and Figure 9 display the ablation experiment result of DG-MORL with and without the self-evolving\nmechanism. The absence of this mechanism makes DG-MORL fail to reach the performance levels of the fully equipped\nDG-MORL setup (except in DST), however, it can still overcome the bound of the prior demonstration. In the DST\nenvironment, due to the relative simplicity, the absence of the self-evolving mechanism does not impact the learning curve\nto reach the maximum value. However, the self-evolving mechanism guarantees a faster convergence.\nIn the Minecart environment, it’s observed that while the DG-MORL algorithm without the self-evolving mechanism\n14\nDemonstration-Guided Multi-Objective Reinforcement Learning\nFigure 7. Ablation Study - DST\nFigure 8. Ablation Study - Minecart\nFigure 9. Ablation Study - MO Hopper\nmaintains high performance for the most part, its performance eventually falls below that of the GPI-PD baseline. This is\nindicative of a key limitation: in the absence of self-evolving, the performance of DG-MORL tends to be constrained by the\nquality of the initial demonstrations.\nIn contrast, a notable pattern emerges in the MO-Hopper environment. Initially, DG-MORL without the self-evolving\nmechanism manages to outperform its fully equipped counterpart. This phenomenon could be attributed to the self-evolving\nmechanism’s impact on the early stages of training. When this mechanism is active, it replaces the initial demonstrations\nwith improved versions in the first few evaluation rounds. This replacement, while beneficial in the long term, makes\nthe exploration policy’s rollback process more challenging initially, as the policy now has to surpass these enhanced\ndemonstrations. Consequently, the rollback steps in the early stages are more difficult. However, over time, the exploration\npolicy in the self-evolving setup catches up and eventually exceeds the performance of the non-self-evolving version,\nhighlighting the long-term benefits of this mechanism.\nDespite these nuances, it is important to note that in both scenarios: with or without the self-evolving mechanism, DG-MORL\ndemonstrates superior performance compared to the GPI-PD baseline in terms of variations and training efficiency. The\nonly exception is in the Minecart environment, where DG-MORL without self-evolving falls short of the GPI-PD baseline\nat certain points. Nevertheless, for most of the training duration, it still performs significantly better than the baseline,\n15\nDemonstration-Guided Multi-Objective Reinforcement Learning\nunderlining the effectiveness of the DG-MORL algorithm in diverse settings.\nThe ablation study highlights the self-evolving mechanism as an integral component that significantly enhances the\nalgorithm’s performance. We provide a theory analysis in Corollary D.3 to show that the self-evolving mechanism can\nalways reduce the gap between the guide policy performance and an optimal policy. Furthermore, this study substantiates\nthe concept that utilizing demonstrations, even if they are slightly sub-optimal, can significantly benefit policy improvement,\nunderscoring the value of this approach in the learning process.\nC.2. Sensitivity Study\nIt is widely acknowledged in machine learning that both the quantity and quality of data play crucial roles in enhancing the\noutcomes. Alongside these, other variables such as the rollback span and passing percentage also play significant roles in\ndetermining training performance. To understand the extent and manner in which each of these factors impacts the learning\nprocess, we have carried out a series of sensitivity studies. We continue to utilize the same three benchmark environments\nand the EU metric. However, the focus shifts to a comparison among DG-MORL agents rather than among other baseline\nalgorithms.\nIn the first part, we explore the sensitivity of the DG-MORL algorithm to the quantity of initial demonstrations. The second\npart of this section examines how the quality of data impacts the algorithm’s performance. The third part of this section is\nabout the influence of different rollback spans. The last part showcases the impact of different passing percentages 3. For\nbetter comparison, the GPI-PD result is included for reference depicted by a blue dashed line.\nC.2.1. SENSITIVITY TO INITIAL DEMONSTRATION QUANTITY\nTable 2. Demonstration Quantity Sensitivity - DST (Expected Utility)\nNumber of Prior Demonstrations\nSteps\n2\n4\n6\n8\n10\n4k\n4.49+1.01\n−1.14\n4.98+.52\n−1.91\n5.24+.26\n−.87\n5.39+.11\n−.2\n5.49+.01\n−.01\n8k\n5.5+.01\n−.02\n5.5+.0\n−.0\n5.46+.04\n−.14\n5.47+.04\n−.14\n5.5+.01\n−.01\n12k\n5.51+.0\n−.0\n5.5+.0\n−.01\n5.5+.0\n−.0\n5.5+.0\n−.01\n5.5+.0\n−.0\n16k\n5.5+.0\n−.01\n5.51+.0\n−.0\n5.5+.0\n−.01\n5.47+.04\n−.14\n5.5+.0\n−.01\n20k\n5.5+.01\n−.02\n5.51+.0\n−.0\n5.5+.0\n−.01\n5.51+.0\n−.0\n5.5+.0\n−.0\n24k\n5.51+.0\n−.0\n5.5+.01\n−.01\n5.51+.0\n−.01\n5.51+.0\n−.01\n5.51+.0\n−.0\n28k\n5.51+.0\n−.0\n5.51+.0\n−.0\n5.51+.0\n−.0\n5.5+.01\n−.01\n5.51+.0\n−.0\n32k 5.51+.0\n−.01\n5.51+.0\n−.0\n5.5+.0\n−.0\n5.51+.0\n−.01\n5.51+.0\n−.0\n36k\n5.5+.01\n−.01\n5.51+.0\n−.0\n5.51+.0\n−.0\n5.51+.0\n−.0\n5.51+.0\n−.0\n40k\n5.51+.0\n−.0\n5.51+.0\n−.0\n5.51+.0\n−.0\n5.51+.0\n−.01\n5.51+.0\n−.0\nFigure 10, Figure 11 and Figure 12 shows DG-MORL algorithm’s sensitivity to the quantity of initial demonstrations in the\nthree benchmark environments separately. The figures presented depict the average results of these 5 runs for the sake of\nclarity. It should be noted that scenarios without any initial demonstrations are not taken into account, as the presence of\nsuch demonstrations is a fundamental prerequisite for DG-MORL. We ensure that the process commences with a minimum\nnumber of demonstrations equivalent to the number of objectives, e.g. for DST, as there are 2 objectives, the least amount\nof demonstration provided should be 2 as well. More detailed evaluation results is shown in Table 2 for DST, Table 3 for\n3The same set of random seeds 2, 7, 15, 42, and 78 were employed. This diversity in seed selection was intended to test the robustness\nand generalizability of our findings across different initial conditions. In the quantity evaluation, the initial demonstrations were randomly\nselected from a larger set of demonstrations. To ensure consistency of the demonstration selection in this experiment, they were conducted\n5 times using seed 42. This controlled approach allows for a clearer understanding of how variations in demonstration quantity affect the\nperformance of the learning algorithm.\n16\nDemonstration-Guided Multi-Objective Reinforcement Learning\nTable 3. Demonstration Quantity Sensitivity - Minecart (Expected Utility)\nNumber of Prior Demonstrations\nSteps\n3\n5\n7\n10k\n.01+.05\n−.06\n−.1+.06\n−.08\n−.05+.08\n−.1\n20k\n.12+.08\n−.1\n.13+.04\n−.08\n.15+.07\n−.07\n30k\n.17+.08\n−.04\n.2+.06\n−.08\n.18+.07\n−.09\n40k\n.19+.06\n−.06\n.2+.06\n−.05\n.25+.01\n−.02\n50k\n.22+.06\n−.09\n.23+.04\n−.04\n.27+.01\n−.01\n60k\n.23+.05\n−.1\n.24+.03\n−.04\n.27+.01\n−.01\n70k\n.23+.05\n−.11\n.24+.04\n−.06\n.26+.02\n−.04\n80k\n.24+.05\n−.11\n.24+.05\n−.04\n.27+.01\n−.0\n90k\n.23+.05\n−.12\n.22+.06\n−.11\n.27+.01\n−.02\n100k\n.24+.04\n−.13\n.24+.04\n−.08\n.27+.01\n−.02\n110k\n.25+.04\n−.11\n.25+.04\n−.06\n.27+.02\n−.01\n120k\n.24+.04\n−.13\n.25+.03\n−.05\n.27+.0\n−.01\n130k\n.24+.04\n−.12\n.25+.03\n−.05\n.27+.00\n−.01\n140k\n.24+.05\n−.14\n.25+.03\n−.04\n.28+.0\n−.01\n150k\n.24+.05\n−.14\n.25+.03\n−.05\n.27+.01\n−.02\nTable 4. Demonstration Quantity Sensitivity - MO Hopper (Expected Utility)\nNumber of Prior Demonstrations\nSteps\n2\n3\n5\n15k\n126.79+20.03\n−15.23\n135.1+15.11\n−21.31\n126.34+26.99\n−32.27\n30k\n136.86+34.41\n−21.63\n158.57+35.41\n−62.43\n168.48+23.95\n−21.18\n45k\n179.64+7.7\n−6.99\n175.63+35.48\n−39.68\n175.61+24.64\n−27.38\n60k\n169.61+40.03\n−33.78\n203.45+17.18\n−14.05\n205.08+4.63\n−3.91\n75k\n192.86+18.18\n−22.52\n191.21+26.01\n−33.81\n192.74+17.69\n−11.99\n90k\n196.48+12.28\n−8.96\n202.18+23.98\n−20.29\n210.34+6.48\n−5.67\n105k\n201.13+18.16\n−32.43\n212.57+12.83\n−10.7\n201.55+19.81\n−41.08\n120k\n201.42+9.26\n−9.35\n208.88+16.85\n−29.94\n204.54+14.49\n−24.94\n135k\n208.86+5.24\n−6.97\n218.75+11.87\n−19.36\n214.54+9.9\n−9.61\n150k\n206.5+9.81\n−8.5\n217.99+13.02\n−12.98\n214.07+6.28\n−10.74\n17\nDemonstration-Guided Multi-Objective Reinforcement Learning\nFigure 10. Demonstration Quantity Sensitivity - DST\nFigure 11. Demonstration Quantity Sensitivity - Minecart\nFigure 12. Demonstration Quantity Sensitivity - MO Hopper\nMinecart, and Table 4 for MO-Hopper.\nThough over time, there are more demonstrations added by the self-evolving mechanism, the outcomes of our research\nreveal a clear relationship between the quantity of initial demonstrations and the performance of the DG-MORL algorithm,\nparticularly evident in the DST environment. Specifically, there is a positive correlation observed: as the number of initial\ndemonstrations increases, there is a notable enhancement in training efficiency and a corresponding improvement in policy\nperformance.\nThis trend is consistent in the Minecart environment as well. Here too, the quantity of initial demonstrations plays a\nsignificant role in influencing the performance of the DG-MORL algorithm. The increased number of initial demonstrations\nprovides more comprehensive guidance and information, which in turn facilitates more effective learning and decision-\nmaking by the algorithm. This consistency across different environments underscores the importance of initial demonstration\nquantity as a key factor in the effectiveness of the DG-MORL algorithm.\nAn intriguing observation from the Minecart experiment was that an increase in the number of initial demonstrations actually\nled to lower performance at the beginning of training. This phenomenon was not evident in the DST experiment, suggesting\na unique interaction in the Minecart environment. We hypothesize that this initial decrease in performance is attributable\n18\nDemonstration-Guided Multi-Objective Reinforcement Learning\nto the complexity of the Minecart environment when paired with a large set of demonstrations. In such a scenario, the\nagent may struggle to simultaneously focus on multiple tasks or objectives presented by these demonstrations, leading to\na temporary dip in performance at the early stages of training. This is indicative of an initial overload or confusion state\nfor the agent, as it tries to assimilate and prioritize the extensive information provided by the numerous demonstrations.\nHowever, as training progresses and the agent becomes more familiar with the environment and its various tasks, it starts to\novercome this initial challenge. With time, the agent effectively integrates the insights from the demonstrations, leading to\nimproved performance. This dynamic suggests that while a wealth of demonstrations can initially seem overwhelming in\ncomplex environments, they eventually contribute to the agent’s deeper understanding and better performance, underscoring\nthe importance of considering both the quantity and nature of demonstrations in relation to the environment’s complexity.\nThe results from the MO-Hopper environment further support the notion that a greater number of initial demonstrations\ntends to yield better performance. Specifically, configurations with 3 and 5 initial demonstrations outperformed the scenario\nwith only 2 initial demonstrations. This suggests that reaching a certain threshold in the number of initial demonstrations\ncan enhance the agent’s performance.\nHowever, an interesting deviation was observed where the performance with 3 initial demonstrations was marginally better\nthan with 5. This outcome implies that, beyond a certain point, additional demonstrations might not always contribute\npositively to the learning process. Potential reasons for this include:\nConflicting Action Selection: More demonstrations could introduce complexity in terms of conflicting action choices in\nsimilar states under different preferences. This conflict might hinder the agent’s ability to learn a coherent and effective\npolicy.\nComplexity and Long Horizon: The intrinsic complexity of the MO-Hopper environment, along with its extended horizon,\nmight complicate the learning process. When the agent learns an effective policy that surpasses some of the guide policies,\nit might inadvertently “forget” how to surpass others, particularly those aligned with different preferences.\nThese observations indicate that while having a sufficient number of demonstrations is beneficial, there is a nuanced balance\nto be struck. Too many demonstrations, especially in complex environments, can introduce new challenges that potentially\noffset the benefits of additional information.\nRemarkably, the DG-MORL algorithm demonstrates its robust learning capabilities by outperforming the GPI-PD baseline,\neven when provided with a limited number of initial demonstrations. This aspect of DG-MORL underscores its efficacy\nin leveraging available resources to achieve superior learning outcomes, highlighting its potential for applications where\nabundant demonstration data may not be readily available.\nC.2.2. SENSITIVITY TO INITIAL DEMONSTRATION QUALITY\nThe quality of initial demonstrations emerges as another factor influencing the training process. While the self-evolving\nmechanism can to some degree mitigate this impact, it is still presumed that initial demonstrations of higher quality are\nassociated with more effective learning outcomes because they may lead the agent to more potential areas in the state space.\nFigures 13, 14, and 15 collectively illustrate the DG-MORL algorithm’s responsiveness to the quality of initial demonstrations\nacross three different environments. These figures compare the overall performance of the DG-MORL agent when initialized\nwith varying levels of demonstration quality. Additionally, they present the EU for each set of demonstrations, indicated by\ndashed lines in colors corresponding to each demonstration standard.\nIn the DST environment, for instance, the learning outcomes for all three categories of initial demonstrations (high,\nmedium, and low quality) eventually reach the environment’s upper-performance limit. However, noteworthy differences\nare evident in the initial phases of training. The DG-MORL agent initialized with the highest quality demonstrations\nachieves significantly better performance right from the first evaluation round. The performance curve of the agent with\nmedium-quality demonstrations shows a slightly quicker ascent compared to the one with lower-quality demonstrations.\nThis pattern underscores the impact of initial demonstration quality on both the speed and efficiency of learning, especially\nin the early stages of training.\nIn the Minecart and MO-Hopper environments, the performance outcomes for the three sets of initial demonstrations reveal\na notable observation: the quality of the demonstrations has a relatively minor influence on the final learning results. This\noutcome contrasts with the more pronounced effect seen in the DST environment. In Minecart and MO-Hopper, despite\nvariations in the quality of the initial demonstrations (high, medium, and low), the differences in the final learning outcomes\n19\nDemonstration-Guided Multi-Objective Reinforcement Learning\nFigure 13. Demonstration Quality Sensitivity - DST\nFigure 14. Demonstration Quality Sensitivity - Minecart\nFigure 15. Demonstration Quality Sensitivity - MO Hopper\nare less significant (but still higher). This suggests that, in these particular environments, the DG-MORL algorithm is\ncapable of achieving comparable levels of performance regardless of the initial quality of demonstrations.\nThis could be attributed to the inherent characteristics of the Minecart and MO-Hopper environments, where the algorithm\nmight have more flexibility or capability to compensate for the initial quality of demonstrations during the learning process.\nIt highlights the adaptive nature of the DG-MORL algorithm and suggests that its performance is not solely dependent on\nthe quality of initial demonstrations, especially in certain types of environments.\nSimilar to the observations made in the experiment evaluating the impact of demonstration quantity, the DG-MORL\nalgorithm consistently outperformed the GPI-PD method in all scenarios tested for demonstration quality. This consistent\nsuperiority across different conditions and environments empirically validates the efficacy and robustness of the DG-MORL\nalgorithm. It demonstrates that DG-MORL can effectively utilize the information provided in the demonstrations, whether\nof high or lower quality, to enhance its learning process and achieve superior outcomes. This finding is significant as it not\nonly attests to the performance capabilities of DG-MORL but also reinforces its potential applicability in a wide range of\nreal-world scenarios where the quality of available demonstrations may vary.\n20\nDemonstration-Guided Multi-Objective Reinforcement Learning\nC.2.3. IMPACT OF DIFFERENT ROLLBACK SPANS\nAnother aspect potentially impacting training outcomes is the rollback span, which dictates the granularity of the learning\nprocess. In simpler environments, such as DST, it is hypothesized that a finer granularity leads to enhanced training\nperformance. Conversely, in scenarios where actions are more low-level, as in the case of MO-Hopper, meaningful behavior\nmay emerge from a sequence of actions. Here, the agent might require a larger rollback span to allow for a more extensive\nexploration of meaningful action combinations for appropriate behavior.\nTo assess this hypothesis, we conducted evaluations of the DG-MORL’s performance under varying rollback spans. This\napproach aimed to understand better how changes in granularity affect learning efficiency and effectiveness in different\nenvironmental complexities.\nFigure 16. Rollback Span Sensitivity - DST\nFigure 17. Rollback Span Sensitivity - Minecart\nFigure 18. Rollback Span Sensitivity - MO Hopper\nThe figures presented depict the average results of these 5 runs for the sake of clarity. Figure 16 depicts the impact of\ndifferent rollback spans within the DST environment, revealing a direct correlation between finer granularity in the learning\nprocess and enhanced learning performance. Table 5 offers a detailed assessment of the effects of various rollback spans in\nDST.\nHowever, it’s noteworthy that the most granular level, represented by a rollback step of 1, exhibits some performance decline\n21\nDemonstration-Guided Multi-Objective Reinforcement Learning\nin the final two evaluation rounds. This phenomenon could be attributed to potential overfitting to the demonstrations\nwhen the granularity becomes “over-fine”. Such overfitting might hinder the agent’s ability to explore autonomously. This\nsituation suggests a need for a balanced approach that effectively integrates both the guide policy and the exploration policy,\nindicating that an optimal level of granularity exists, which maximizes learning while avoiding the pitfalls of excessive\nadherence to demonstrations.\nFigure 17 and Table 6 provide insights into how different rollback spans affect training in the Minecart environment. The\nresults roughly mirror those observed in the DST environment, particularly in how increased granularity impacts training\noutcomes.\nWhen a rollback span of 10 steps is used, the agent initially shows improved performance. However, this setup leads to a\nnoticeable decline in performance during the middle stages of training, accompanied by slightly higher variance compared\nto a finer granularity of rolling back for 2 steps. This suggests that while a larger rollback span can offer early benefits, it\nmay also introduce instability and inconsistency as training progresses.\nFurthermore, an even larger rollback span of 15 steps not only exhibits a similar mid-training performance drop but also\nlacks the initial training advantage seen with a span of 10 steps. This indicates that too large a rollback span can be\ndetrimental right from the early stages of training, hampering the agent’s ability to learn effectively. These findings highlight\nthe importance of carefully selecting the rollback span in the Minecart environment to balance the benefits of early learning\nacceleration against the risks of later performance decline and increased variability.\nThe MO-Hopper environment is characterized by the need for the agent to consider three different torques simultaneously to\nachieve a cohesive behavior. Consequently, the agent’s control over the hopper is at a very low level. Given this context,\nwe hypothesize that a smaller rollback span might impede the training process. Because, if the span is too short, the agent\nmay struggle to gain a meaningful understanding of the hopper’s behavior, as its perspective is too narrowly focused on\nimmediate, low-level torque adjustments. On the other hand, a larger rollback span could be more beneficial. It would allow\nthe agent to observe the hopper’s behavior over a broader timeframe, providing a more holistic understanding beyond mere\nlow-level torque control. This broader perspective could enable the agent to better comprehend and learn the synthesized\nbehavior necessary for effective control in the MO Hopper environment.\nWe selected rollback spans of 3, 100, and 300 to investigate their impact on training in the MO Hopper environment. Both\nFigure 18 and Table 7 provide valuable insights into this aspect. The empirical evidence gathered from these experiments\nsupports the hypothesis that in a task requiring low-level control, such as the MO Hopper, a small rollback span is detrimental\nto the learning process. Specifically, when the rollback span is short (3), the agent encounters significant difficulties in\nlearning an effective policy. This challenge is highlighted by the agent’s inability to surpass the performance indicated by\nthe initial demonstrations, represented by the black dashed line in the figure. This outcome suggests that a small rollback\nspan limits the agent’s ability to understand and integrate the broader behavioral patterns necessary for effective control in\nthe MO Hopper environment. It emphasizes the importance of selecting an appropriate rollback span that aligns with the\ngranularity of control and the nature of the task in reinforcement learning environments.\nC.2.4. IMPACT OF DIFFERENT PASSING PERCENTAGES\nAs introduced in Section 4.4, the concept of a passing percentage β ∈[0, 1], is introduced to alleviate the difficulty of\ncurriculum passing. This mechanism is crucial, especially when the demonstration poses a significant challenge to be\noutperformed, potentially preventing the agent from progressively gaining greater control from the guiding policy and\nconsequently hindering policy improvement. In this section, we explore how this parameter influences the learning process.\nGiven that DST and Minecart are comparatively simpler environments, we opted for a straightforward approach by setting\nthe passing threshold at 1. This means the objective for the agent in these environments is to fully surpass the performance\ndemonstrated in the initial demonstrations. Consequently, there was no need to vary the passing threshold in these simpler\nsettings but just in the MO-Hopper environment where a progressively increasing passing percentage is utilized.\nBy adjusting the threshold, we can observe the balance between the need for the agent to outperform the demonstrations and\nthe feasibility of doing so. The passing percentages, or thresholds, selected for this investigation are progressively scaled:\n0.8 →0.98, 0.9 →0.98, and a static threshold of 1.\nThe findings of our study on the effects of varying passing percentages in the MO-Hopper environment are depicted in\nFigure 19. This figure illustrates that employing an incrementing passing percentage (starting from a relatively lower value)\nis more effective in the MO-Hopper context compared to using a static threshold value 1.\n22\nDemonstration-Guided Multi-Objective Reinforcement Learning\nTable 5. Impact of Different Rollback Spans - DST (Expected Utility)\nRollback Span\nSteps\n1\n2\n3\n5\n4k\n5.48+.02\n−.03\n5.44+.06\n−.19\n5.49+.01\n−.02\n4.49+1.01\n−.3.33\n8k\n5.5+.01\n−.01\n5.5+.01\n−.02\n5.5+.01\n−.01\n5.5+.01\n−.01\n12k\n5.5+.01\n−.01\n5.51+.0\n−.0\n5.5+.0\n−.01\n5.51+.0\n−.0\n16k\n5.51+.0\n−.0\n5.51+.0\n−.0\n5.51+.0\n−.0\n5.5+.04\n−.01\n20k 5.51+.0\n−.0\n5.51+.0\n−.01\n5.5+.0\n−.01\n5.51+.0\n−.0\n24k 5.51+.0\n−.0\n5.51+.0\n−.0\n5.51+.0\n−.0\n5.51+.0\n−.01\n28k 5.51+.0\n−.01\n5.51+.0\n−.0\n5.51+.0\n−.0\n5.5+.01\n−.0\n32k 5.51+.0\n−.0\n5.51+.0\n−.0\n5.5+.0\n−.0\n5.51+.0\n−.01\n36k 5.5+.01\n−.01\n5.51+.0\n−.0\n5.51+.0\n−.01\n5.51+.0\n−.0\n40k 5.49+.02\n−.07\n5.51+.0\n−.0\n5.51+.0\n−.0\n5.51+.0\n−.01\nTable 6. Impact of Different Rollback Spans - Minecart (Expected Utility)\nRollback Span\nSteps\n2\n10\n15\n10k\n−.12+.12\n−.04\n−.06+.12\n−.09\n−.1+.08\n−.05\n20k\n.08+.15\n−.14\n.14+.03\n−.07\n.02+.09\n−.16\n30k\n.19+.06\n−.09\n.23+.02\n−.04\n.2+.06\n−.08\n40k\n.24+.02\n−.03\n.26+.01\n−.01\n.24+.04\n−.09\n50k\n.26+.01\n−.01\n.26+.01\n−.01\n.21+.07\n−.27\n60k\n.26+.01\n−.01\n.27+.01\n−.01\n.23+.04\n−.1\n70k\n.27+.01\n−.03\n.27+.01\n−.01\n.22+.05\n−.13\n80k\n.27+.0\n−.0\n.25+.04\n−.13\n.23+.05\n−.17\n90k\n.27+.01\n−.01\n.25+.04\n−.11\n.25+.03\n−.07\n100k\n.27+.01\n−.02\n.27+.01\n−.01\n.25+.02\n−.03\n110k\n.28+.01\n−.01\n.27+.01\n−.03\n.27+.0\n−.0\n120k\n.28+.0\n−.01\n.27+.02\n−.02\n.28+.01\n−.01\n130k\n.28+.0\n−.01\n.27+.02\n−.04\n.27+.02\n−.01\n140k\n.28+.0\n−.01\n.28+.0\n−.0\n.28+.01\n−.01\n150k\n.28+.0\n−.0\n.28+.01\n−.01\n.27+.01\n−.02\n23\nDemonstration-Guided Multi-Objective Reinforcement Learning\nTable 7. Impact of Different Rollback Spans - MO Hopper (Expected Utility)\nRollback Span\nSteps\n3\n100\n300\n15k\n82.46+35.46\n−16.61\n124.54+33.71\n−30.47\n143.11+18.08\n−39.13\n30k\n109.18+43.91\n−31.4\n158.15+12.12\n−10.77\n161.01+19.06\n−36.91\n45k\n110.99+41.21\n−22.7\n156.82+37.21\n−94.32\n190.42+19.21\n−20.35\n60k\n144.41+33.55\n−38.3\n188.46+20.63\n−29.09\n182.72+30.16\n−34.15\n75k\n125.19+44.65\n−39.65\n191.62+21.62\n−30.6\n198.64+10.83\n−11.07\n90k\n146.8+46.44\n−57.1\n204.39+13.72\n−19.22\n199.62+10.09\n−11.07\n105k\n128.73+61.02\n−94.84\n205.74+14.33\n−20.69\n203.18+9.59\n−10.78\n120k\n136.33+18.37\n−23.65\n217.12+6.16\n−4.63\n211.4+11.22\n−12.87\n135k\n139.29+26.12\n−59.2\n205.74+14.63\n−28.32\n208.93+8.91\n−12.7\n150k\n149.39+39.08\n−32.87\n210.56+19.88\n−13.76\n203.06+12.47\n−15.65\nFigure 19. Curriculum Passing Percentage-MO Hopper\n24\nDemonstration-Guided Multi-Objective Reinforcement Learning\nSpecifically, when a static threshold of 1 was used as in DST and Minecart, the DG-MORL agent exhibited poor performance\nin the MO-Hopper environment. Notably, the agent was unable to surpass or even match the initial demonstration’s\nperformance. This outcome can be attributed to the high difficulty of the initial demonstration, which set an overly\nchallenging bar for the agent. With such a high initial standard and no opportunity to progressively work towards surpassing\nit, the agent struggled to execute effective rollback steps and identify better demonstrations (self-evolving).\nThis study indicates the importance of the passing percentage as a factor in the application of DG-MORL, especially in\ncomplex environments like MO-Hopper. Adjusting the passing threshold to become incrementally more challenging allows\nthe agent to gradually improve its performance, aligning better the complexity of the tasks at hand.\nD. Theoretical Analysis\nD.1. Lower Bound Discussion\nWhen the exploration policy is 0-initialized (all Q-values are zero initially), a non-optimism-based method, e.g. ϵ-greedy,\nsuffers from exponential sample complexity when the horizon expands. We extend the combination lock (Koenig &\nSimmons, 1993; Uchendu et al., 2023) under the multi-objective setting (2 objectives), and visualize an example in Figure\n20. In the multi-objective combination lock example, the agent encounters three distinct scenarios where it can earn a\nFigure 20. Lower Bound Example: MO combination lock\nnon-zero reward vector, each scenario is determined by a specific preference weight vector. The three preference weight\nvectors are [1, 0], [0.5, 0.5], and [0, 1]. These vectors correspond to the agent’s varying preferences towards two objectives:\nWhen the agent demonstrates an extreme preference for objective 1, it may receive a reward vector of [1, 0]. Conversely, if\nthe agent shows an extreme preference for objective 2, it could be awarded a reward vector of [0, 1].\nIn the scenario where the agent’s preference is evenly split between the two objectives, as indicated by a preference vector\nof [0.5, 0.5], there’s a possibility for it to obtain a reward vector of [0.5, 0.5]. This outcome reflects a balanced interest in\nboth objectives. In this multi-objective combination lock scenario, the outcomes for the agent vary significantly based on its\npreference weights and corresponding actions. When the preference weights are set to [1, 0], the agent’s goal is to reach\nthe red state s∗\no1:h. To achieve this, it must consistently perform the action a∗\no1 from the beginning of the episode, which\nenables it to remain in the red state and subsequently receive a reward feedback of [1, 0]. Similarly, with preference weights\n25\nDemonstration-Guided Multi-Objective Reinforcement Learning\nof [0, 1], the agent needs to execute the action a∗\no2 right from the start to eventually reach a reward vector of [0, 1]. For a\nbalanced preference of [0.5, 0.5], the agent is expected to take a specific action, a∗\nbalance, at step H −1 to transition to the\nstate s∗\nbalance:h and receive a balanced reward of [0.5, 0.5]. This time point is referred to as the mutation point. If the agent\nderails from these prescribed trajectories, it only receives a reward of [0, 0]. Moreover, any improper action leads the agent\nto a blue state, from which it cannot return to the correct trajectory (derailment).\nAs the agent is 0-initialized, it needs to select an action from a uniform distribution. The preference given is also sampled\nfrom a uniform distribution. Then if the agent wants to know the optimal result for any of the three preferences, it needs at\nleast 3H−1 samples. If it thoroughly knows the optimal solution of one of the three preferences, it can achieve an expected\nutility of 0.3. There exist several (K) paths in this MOMDP instance delivering good rewards. To find all good policies, the\nagent needs at least to see K · 3H−1 samples. We formalize this in Theorem D.1.\nThe scenario where preferences precisely match the three specific weights of [1, 0], [0.5, 0.5], and [0, 1] is considerably\nrare, as preferences are typically drawn from an infinite weight simplex. In practical training situations, encountering a\nvariety of preferences can lead to conflicting scenarios for the agent. This conflict arises because different preferences may\nnecessitate different actions or strategies, potentially causing the agent’s policy to experience “forgetfulness” or an inability\nto consistently apply learned behaviors across varying preferences. Such a situation complicates the training process and\nincreases sample complexity.\nTheorem D.1. ((Koenig & Simmons, 1993; Uchendu et al., 2023)) When using a 0-initialized exploration policy, there\nexists a MOMDP instance where the sample complexity is exponential to the horizon to see a partially optimal solution and\na multiplicative exponential complexity to see an optimal solution.\nD.2. Upper Bound of DG-MORL\nSimilar to the work of Uchendu et al. (Uchendu et al., 2023), we start from an assumption about the guide-policy quality.\nAssumption D.2. ((Uchendu et al., 2023)) Assume there exists a feature mapping function ϕ : S −→Rd, that for any policy\nπ, Qπ(s, a) and π(s) depends on s only through ϕ(s). The guide policy πg4 is assumed to cover the states visited by the\noptimal policy corresponding to the preference weight vector w:\nsup\ns,h,w\ndπ∗\nh,w(ϕ(s))\ndπg\nh,w(ϕ(s)) ≤C\n(13)\nAs we have the self-evolving mechanism, the guide policy πg also improves over time. The constant C is also decreasing as\nthe guide policy is approaching the optimal policy during training. This assumption suggests that under a preference weight\nvector w, the guide policy can see the good states which are seen by the optimal policy.\nCorollary D.3. When using the self-evolving mechanism, the upper bound of guide policy πg is no longer time-homogeneous\nas πg improves over time. The Equation 13 can be rewritten as:\nsup\ns,h,w\ndπ∗\nh,w(ϕ(s))\ndπg\nh,w(ϕ(s)) ≤C(t) ≤C(t0)\n(14)\nwhere t is the training rounds, t0 is the initial training round.\nWe have provided the upper bound of the guide policy under a self-evolving mechanism. The next step is to give the upper\nbound of the exploration policy. When the whole horizon length is 1, the problem is reduced to a multi-objective contextual\nbandit (MOCB). We now establish a few key definitions to introduce the second assumption of the exploration step. One\ncritical concept is the Pareto suboptimality gap. This term refers to the measure of distance between the current policy’s\nperformance on a specific arm of the bandit and the true Pareto optimal solution for that arm.\nDefinition D.4. (Pareto suboptimality gap (Lu et al., 2019)) Let x be an arm in X. The Pareto suboptimality gap ∆x is\ndefined as the minimal scalar ϵ ≥0 so that x becomes Pareto optimal by adding ϵ to all entries of its expected reward.\nWith Definition D.4, we can define the regret in a multi-objective setting, i.e. Pareto regret (PR).\n4Given that the policy set Πg can be interpreted as a mixed policy tailored to specific scenarios, it retains its generality even when Πg\nis considered as a singular policy.\n26\nDemonstration-Guided Multi-Objective Reinforcement Learning\nDefinition D.5. (Pareto regret(Drugan & Nowe, 2013; Lu et al., 2019)) If x1, x2, ..., xn are the arms can be pulled by the\nlearner. The Pareto regret is: PR(N) = PN\nn=0 ∆xn\nWe can give the performance guarantee with the help of the aforementioned definitions. Note that, we build our analysis\non MOCB, which is suitable for single-step decision-making. As our algorithm works based on the Q-learning and TD3\nparadigm, which is more robust in our benchmarks, i.e. sequence decision-making, it is supposed to be less likely to violate\nthe assumption.\nTheorem D.6. (Performance guarantee for exploration algorithm in MOCB (Drugan & Nowe, 2013; Uchendu et al., 2023))\nIn MOCB where the reward vector is constrained by the reward space Rd (d is the dimensionality of the objectives), there\nexists some exploration algorithm performing a policy πt in each round t ∈[T], where the Pareto regret is bounded:\nPR ≤f(T, Rd)\n(15)\nWith Assumption D.2 and Theorem D.6, DG-MORL can guarantee that after T rounds:\nEUπ∗−EUπ ≤C(T)f(T, Rd)\n(16)\nThe upper bound depends on the exploration algorithm. When using the exploration algorithm as UCB (Drugan & Nowe,\n2013), this upper bound can be written as: EUπ∗−EUπ ≤eO(T) × C(T). When using the exploration algorithm as GLM\n(Lu et al., 2019), this upper bound can be written as EUπ∗−EUπ ≤eO(d\n√\nT) × C(T).\n27\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2024-04-05",
  "updated": "2024-04-05"
}