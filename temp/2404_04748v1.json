{
  "id": "http://arxiv.org/abs/2404.04748v1",
  "title": "Multilingual Brain Surgeon: Large Language Models Can be Compressed Leaving No Language Behind",
  "authors": [
    "Hongchuan Zeng",
    "Hongshen Xu",
    "Lu Chen",
    "Kai Yu"
  ],
  "abstract": "Large Language Models (LLMs) have ushered in a new era in Natural Language\nProcessing, but their massive size demands effective compression techniques for\npracticality. Although numerous model compression techniques have been\ninvestigated, they typically rely on a calibration set that overlooks the\nmultilingual context and results in significant accuracy degradation for\nlow-resource languages. This paper introduces Multilingual Brain Surgeon (MBS),\na novel calibration data sampling method for multilingual LLMs compression. MBS\novercomes the English-centric limitations of existing methods by sampling\ncalibration data from various languages proportionally to the language\ndistribution of the model training datasets. Our experiments, conducted on the\nBLOOM multilingual LLM, demonstrate that MBS improves the performance of\nexisting English-centric compression methods, especially for low-resource\nlanguages. We also uncover the dynamics of language interaction during\ncompression, revealing that the larger the proportion of a language in the\ntraining set and the more similar the language is to the calibration language,\nthe better performance the language retains after compression. In conclusion,\nMBS presents an innovative approach to compressing multilingual LLMs,\naddressing the performance disparities and improving the language inclusivity\nof existing compression techniques.",
  "text": "Multilingual Brain Surgeon: Large Language Models Can be Compressed\nLeaving No Language Behind\nHongchuan Zeng1, Hongshen Xu1, Lu Chen1,2†, Kai Yu1,2†\n1X-LANCE Lab, Department of Computer Science and Engineering\nMoE Key Lab of Artificial Intelligence, SJTU AI Institute\nShanghai Jiao Tong University, Shanghai, China\n2Suzhou Laboratory, Suzhou, China\n{charlie68, xuhongshen, chenlusz, kai.yu}@sjtu.edu.cn\nAbstract\nLarge Language Models (LLMs) have ushered\nin a new era in Natural Language Processing,\nbut their massive size demands effective com-\npression techniques for practicality. Although\nnumerous model compression techniques have\nbeen investigated, they typically rely on a cali-\nbration set that overlooks the multilingual con-\ntext and results in significant accuracy degrada-\ntion for low-resource languages. This paper in-\ntroduces Multilingual Brain Surgeon (MBS), a\nnovel calibration data sampling method for mul-\ntilingual LLMs compression. MBS overcomes\nthe English-centric limitations of existing meth-\nods by sampling calibration data from various\nlanguages proportionally to the language distri-\nbution of the model training datasets. Our ex-\nperiments, conducted on the BLOOM multilin-\ngual LLM, demonstrate that MBS improves the\nperformance of existing English-centric com-\npression methods, especially for low-resource\nlanguages. We also uncover the dynamics of\nlanguage interaction during compression, re-\nvealing that the larger the proportion of a lan-\nguage in the training set and the more similar\nthe language is to the calibration language, the\nbetter performance the language retains after\ncompression. In conclusion, MBS presents an\ninnovative approach to compressing multilin-\ngual LLMs, addressing the performance dis-\nparities and improving the language inclusiv-\nity of existing compression techniques. The\ncodes are available at: https://github.com/X-\nLANCE/MBS.\n1\nIntroduction\nLarge Language Models (LLMs) have revolution-\nized Natural Language Processing (NLP) with their\nremarkable performance. However, their colossal\nsize and computational demands necessitate effec-\ntive Model Compression (MC) techniques for prac-\ntical use. In the case of multilingual LLMs, the\n†Lu Chen and Kai Yu are the corresponding authors.\nvast size is crucial for retaining information from\nvarious languages and mitigating the curse of multi-\nlinguality (Conneau et al., 2020; Goyal et al., 2021).\nMoreover, wide language coverage and interfer-\nence among languages pose a harder challenge for\ncompressing multilingual LLMs.\nExisting approaches for MC have predominantly\nfocused on model quantization (Frantar et al., 2023;\nDettmers et al., 2022; Xiao et al., 2023; Yao et al.,\n2022), where model parameters are mapped to\nlower bit-level representations, and network prun-\ning, which reduces the size of neural networks\nby eliminating unnecessary connections. Inspired\nby the classic Optimal Brain Damage (OBD) and\nOptimal Brain Surgeon (OBS) pruning framework\n(Hassibi et al., 1993; Le Cun et al., 1989), various\napproaches, namely GPTQ (Frantar et al., 2023)\nfor model quantization, SparseGPT (Frantar and\nAlistarh, 2023) and Wanda (Sun et al., 2023) for\nnetwork pruning, have been proposed to compress\nLLMs. These compression methods utilize a cali-\nbration dataset to determine the priority of parame-\nters and thus are retraining-free, avoiding expensive\nfine-tuning cost especially for LLMs.\nHowever, neither of these methods has consid-\nered the multilingual scenario: all of them use a\nsingle-language (e.g., English) calibration dataset\nto determine the priority of parameters for multi-\nlingual models. A significant performance drop on\nmultilingual tasks is observed due to this English-\ncentric approach, especially in the case of low-\nresource languages.\nIn this paper, we propose Multilingual Brain\nSurgeon (MBS), which has successfully achieved\nsignificant sparsity levels when compressing mul-\ntilingual LLMs while simultaneously minimizing\nthe performance drop across different languages in\nthe models, leaving no language behind after com-\npression. Specifically, as shown in Figure 1, MBS\nsamples the calibration data of different languages\nproportionally to the language distribution of the\n1\narXiv:2404.04748v1  [cs.CL]  6 Apr 2024\nFigure 1: MBS samples calibration data from different languages proportionally to the language distribution of\ntraining datasets. This approach (right part) effectively addresses the multilingual compression problem compared\nto previous monolingual sampling methods (left part).\nmodel training dataset. This approach effectively\naddresses the multilingual compression problem\ncompared to previous monolingual sampling meth-\nods. Furthermore, we observed the dynamics of\nlanguage interaction during compression and drew\ntwo main conclusions: 1) The larger the propor-\ntion of a language in the model training dataset,\nthe more resistant it is to compression. 2) The\nmore similar the downstream language is to the\ncalibration language, the less performance drop it\nobtained after compression. We further propose a\nmeasure of similarity among languages to explain\nand predict the performance drop.\nThe experiments were conducted on BLOOM\n(BigScience Workshop, 2022), one of the most ef-\nfective open-source multilingual LLM models. We\nsample the calibration data from CC-100 (Wenzek\net al., 2020), a widely used dataset of web-crawled\ndata containing 100+ languages. The perplexity\nof languages is tested on XL-Sum (Hasan et al.,\n2021), a dataset that contains high-quality articles\nfrom BBC covering 45 languages. Experimental\nresults demonstrate that MBS enhances the per-\nformance of GPTQ, SparseGPT, and Wanda com-\npared to using only English calibration data. We\nwant to further highlight that MBS is applicable\nto all compression methods that involve the use\nof calibration data, especially those following the\nOBS/OBD framework (Hassibi et al., 1993; Le Cun\net al., 1989), which necessitates approximations of\nsecond-derivative information.\n2\nBackground\n2.1\nOptimal Brain Surgeon (OBS)\nOptimal Brain Surgeon (Hassibi et al., 1993) is\na classic network pruning algorithm. It assumes\nthat a network’s error converges to a local mini-\nmum and calculates the second-order derivatives\n(Hessian matrix H) of the error (E) with respect\nto each parameter (w) to determine which connec-\ntions can be safely pruned without significantly\naffecting performance. The increase in error (Lj)\nwhen a parameter (wj) is set to zero, and the opti-\nmal adjustment (δw) of the remaining weights to\ncompensate for the removal are given by:\nLj = 1\n2\nwj2\n[H−1]jj\n(1)\nδw = −\nwj\n[H−1]jj\nH−1\n:,j .\n(2)\n2.2\nError Measurement\nThe network’s error can be expressed in terms of\nthe l2-error between the outputs before and after\ncompression (Hubara et al., 2021). Given inputs X\n(the training dataset), the original weights W, the\nupdated weights ˆ\nW, and a sparsity mask M of the\nsame size as W, the error is defined as:\nE = ||WX −(M ⊙ˆ\nW)X||2\n2.\n(3)\nIn the case of quantization, the mask is a matrix\nfilled with ones. The second-order derivatives (H)\nof the error with respect to the parameters are there-\nfore represented as H = 2XXT, which forms the\nbasis of our approximation objective.\n2\n2.3\nSparseGPT, Wanda and GPTQ\nTo assess the importance of parameters, SparseGPT\nand Wanda employ different pruning metrics. Tak-\ning inspiration from OBS, SparseGPT defines its\nmetric as Si,j = [|W|2/diag((XTX+λI)−1)]i,j,\nwith λ being the Hessian dampening factor to pre-\nvent inverse computation collapse. On the other\nhand, Wanda uses Si,j = |Wi,j| · ||Xj||2 as its\npruning metric.\nRemarkably, these two metrics are essentially\nequivalent when λ is set to 0, and only the diagonal\nelements of the Hessian matrix XT X + λI are\nretained:\ndiag(((XTX + λI) ⊙I)−1) = (||Xj||2\n2)−1.\n(4)\nThis assumption aligns with the practice of Opti-\nmal Brain Damage (Le Cun et al., 1989), which\nretains only the diagonal elements of the second-\norder derivatives matrix. Consequently, we can\nconclude that:\nSSparseGPT = S2\nWanda\n(5)\nif we disregard the non-diagonal elements of H.\nThe primary distinctions between SparseGPT\nand Wanda are as follows:\n• SparseGPT retains the non-diagonal elements\nof the Hessian metrics, whereas Wanda takes\nthe opposite approach.\n• SparseGPT performs adjustments (δw) on\nnon-pruned parameters to compensate for re-\nmoval, while Wanda does not.\nEqually inspired by OBD, the quantization formu-\nlas provided by GPTQ are as follows:\nwj = argminwj\n(quant (wj) −wj)2\n[H−1]jj\n(6)\nδF = −wj −quant (wj)\n[H−1]jj\n·\n\u0000H−1\u0001\n:,j\n(7)\nHere, wj represents the greedy-optimal weight to\nquantize next, δF denotes the corresponding opti-\nmal update of weights, and quant(w) rounds the\nvalue of w to the nearest point on the quantiza-\ntion grid. It’s evident that these formulas follow a\nsimilar pattern to the OBD/OBS approach, and the\ninformation of the Hessian matrix H is crucial in\nall these methods.\n3\nIs Monolingual Calibrating Applicable\nto Multilingual MC?\nPrevious model compression methods only use En-\nglish corpus as the sole calibration data, neglecting\nother languages. This raises the question: how\ndoes monolingual calibration impact the perfor-\nmance of other languages during multilingual\nmodel compression? In this section, we aim to\nexplore this issue theoretically, focusing on two\nmain aspects: the proportion of languages in the\ntraining data, and the similarity between languages.\nFurther experimental analysis will be provided in\nSection 5.3.\nWe denote the total error of the model as E, and\nthe error on language m as Em. We know that\nmodel training convergence applies to the whole\ntraining dataset. Thus, E resides in a local mini-\nmum. However, for languages m and n, Em and\nEn may not necessarily be in their own local min-\nima. This also explains the presence of the multi-\nlingual curse (Conneau et al., 2020), where the per-\nformance of a multilingual model in all languages\nis lower than that of a monolingual model with the\nsame configuration. This occurs because the model\nis in a global local minimum, rather than individual\nlocal minima for each language. Due to their dif-\nfering distributions, the local minima for each lan-\nguage do not overlap. The reason why using larger\nlanguage models can alleviate this problem might\nbe that, with a huge amount of parameters, they can\nsimulate a distribution sophisticated enough where\ndifferent languages’ local minima are close.\n3.1\nProportion in training data\nDue to the fact that the size of English corpus is\nmuch larger than low-resource languages, we may\nsuppose a language pair m and n with a signifi-\ncantly different training corpus size (pn >> pm).\nIntuitively, we can assume that languages with\nlarger corpora in the training set tend to have their\nminimum error closer to the minimum of E be-\ncause they contribute more weight to the total error.\nThis characteristic makes them more robust against\ncompression. Conversely, languages with smaller\ncorpora inherently have their minimum error far-\nther from the minimum of E, and compression can\npotentially push them even further away.\nThis phenomenon is manifested in the follow-\ning way illustrated in Figure 2: when compressing\nmodels with only the calibration data of the well-\n3\nFigure 2: Languages with larger corpora have their min-\nimum error closer to the minimum of E. Monolingual\ncompression effectively \"pushed\" the model’s state to-\nwards the minimum error of that particular language.\nrepresented1 language n, it has a significant impact\non the performance of the underrepresented lan-\nguage m. However, compressing models with only\nthe calibration data of the underrepresented lan-\nguage m has a comparatively minor impact on the\nperformance of the well-represented language n.\n3.2\nSimilarity between languages\nIn the second scenario, we may suppose that the\ntwo languages are as well-represented as each other\n(pm ≈pn). According to Equation 1, the priority\nof compression is fully determined by H, so it is\nsufficient to compare Hm and Hn. We may sup-\npose the non-diagonal elements are trivial (Le Cun\net al., 1989) to calculate the inverse of H. The\nmetric is thus simplified to S = |W| · ||X||2, so\nwe can directly compare ||X||2, which is a vector\nof length q (number of parameters), and each of the\nelements is the sum of the square of the inputs at\nthe corresponding position.\nA classic method to compare the similarity of\ntwo vectors is cosine similarity. The choice of\ncosine similarity over Euclidean distance is moti-\nvated by the need to compare two vectors based\non the likelihood that their largest components re-\nmain consistent after undergoing the same element-\nwise multiplication with unknown vectors (model\nparameters). This can be modeled as the compar-\nison of two vectors after they have experienced\nthe same coordinate axis transformation, assessing\nwhether their largest components remain identical.\nClearly, when two vectors have a smaller angle\nbetween them, the likelihood that their largest com-\n1In the rest of the paper, we call a language \"well-\nrepresented\" when its proportion is relatively big in the model\ntraining set, and \"underrepresented\" when its proportion is\nrelatively small.\nponents remain the same after undergoing the same\ncoordinate axis transformation is relatively higher\n(demonstrated in Figure 3).\nFigure 3: The angle between language 2 and language 3\nis smaller than that between language 1 and language 2.\nAfter element-wise multiplication, language 2 and 3 are\nmore likely to prioritize the same parameter w1 because\ntheir angle before multiplication is smaller.\nHowever, it’s important to acknowledge that co-\nsine similarity does not fulfill the properties of a\ndistance metric, particularly the triangle inequal-\nity. Consequently, we cannot directly deduce the\nsimilarity between languages 1 and 3 from the sim-\nilarities between 1 and 2, and 2 and 3. However,\nthe property of a distance metric is less critical\nin the context of our work, since our goal is only\nto compare the similarity between the calibration\nlanguage and the non-calibration languages, rather\nthan among non-calibration languages.\nWe can compute the cosine similarity between\n||Xm||2 and ||Xn||2. When they are similar, using\nonly data of language m as calibration data will\nintroduce little performance drop in language n,\nand vice versa. That is to say, when two languages\nare very different, employing data from just one of\nthe two languages as calibration data will lead to\na significant performance decrease in the other.\n4\nMultilingual Brain Surgeon (MBS)\nTo mitigate interference among languages in mul-\ntilingual model compression, we introduce Multi-\nlingual Brain Surgeon (MBS), a method that pro-\nportionally samples calibration data from different\nlanguages based on their distribution in the model\ntraining dataset. We provide additional theoretical\ndetails as follows.\nIn the OBD/OBS framework, we treat the error\n(E) as a whole. This makes sense for monolin-\ngual models since they contain only one language.\nHowever, for multilingual models, the error can\nbe regarded as the sum of errors (En) associated\nwith different languages. For a model trained on\nmultiple languages, we can express the total error\n4\nas follows:\nE = E1 + E2 + E3 + . . . + En.\n(8)\nConsequently, the Hessian matrix can be rep-\nresented as the sum of Hessian matrices for each\nlanguage:\nH = H1 + H2 + H3 + . . . + Hn,\n(9)\nwhere Hn = XnTXn. Here, Xn represents the\ninputs (training data) for language n, with a shape\nof q × pn, where q is the total number of network\nparameters, and pn is the total number of training\nsamples for language n.\nLet’s denote a subset of training data as X[k]\nn .\nThen, we have:\nHn = XnTXn =\npn\nX\nk=1\nX[k]\nn\nT X[k]\nn ,\n(10)\nwhich leads to:\nH =\np1\nX\nk=1\nX[k]\n1\nT X[k]\n1\n+\np2\nX\nk=1\nX[k]\n2\nT X[k]\n2\n+ . . . +\npn\nX\nk=1\nX[k]\nn\nT X[k]\nn .\n(11)\nIt’s evident that each language’s contribution\nto H depends on its representation in the model’s\ntraining data. Therefore, when selecting calibration\ndata, it’s essential to choose samples from each lan-\nguage in proportion to its presence in the training\nset. Specifically, for language n, the percentage of\nits representation in the training set is pn/p, where\np is the total number of training samples. Thus,\nwe should allocate a proportionate amount of data\nfrom language n (i.e., pn/p percent) in the calibra-\ntion data used for compression.\n5\nExperiments\n5.1\nExperimental Setup\nModels. The experiments were conducted using\nthe BLOOM (BigScience Workshop, 2022) model\nfamily, which is recognized as one of the most\neffective open-source multilingual LLMs. Our pri-\nmary tests were performed on both the BLOOM-\n560m and BLOOM-7b1 models to provide insights\ninto the performance of smaller and larger models.\nFor the network pruning experiments, a pruning\nsparsity of 50% was applied. In the quantization\nexperiments, the models were quantized to 3 bits\nprecision with groupings of size 1024.\nDatasets & Language Selection. For calibra-\ntion data, we selected CC-100 (Wenzek et al.,\n2020), a dataset comprising web-crawled content\nin over 100 languages, similar to the setup used by\nprevious studies like Frantar and Alistarh (2023),\nSun et al. (2023), and Frantar et al. (2023) which\nused a monolingual English dataset called C4 (Raf-\nfel et al., 2019).\nTo evaluate multilingual perplexity, we em-\nployed XL-Sum (Hasan et al., 2021), a dataset\ncontaining high-quality articles from BBC cover-\ning 45 languages, as our benchmark. Addition-\nally, we assessed perplexity on the test sets of\nraw-WikiText2 (Merity et al., 2016), a widely used\nEnglish perplexity benchmark. Due to resource\nlimitations for certain languages in the BLOOM\nmodel, we conducted experiments on a subset of\n20 languages, which were those available in CC-\n100, XL-Sum, and BLOOM. These languages in-\nclude Arabic (ar), Bengali (bn), Chinese simplified\n(zh-Hans), Chinese traditional (zh-Hant), French\n(fr), Gujarati (gu), Hindi (hi), Igbo (ig), Indone-\nsian (id), Marathi (mr), Nepali (ne), Portuguese\n(pt), Spanish (es), Swahili (sw), Tamil (ta), Tel-\nugu (te), Urdu (ur), Vietnamese (vi), and Yoruba\n(yo).\nEvaluation. We evaluated the perplexity of the\ncompressed model separately for each language us-\ning XL-Sum. We also conducted zero-shot evalua-\ntions, employing the widely recognized EleutherAI-\neval-harness (Gao et al., 2021), with a focus on\nmultilingual tasks to assess the performance of less-\nrepresented languages. The zero-shot tasks that we\nhave chosen to evaluate the compressed model are\nspecified in Table 1.\nCalibration data & Baselines. Our calibration\ndata consisted of 256 segments, each containing\n2048 tokens, sampled from CC-100. We used our\nMBS sampling method and sampled 87, 47, 37,\n31, 14, 13, 7, 4, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\nsegments respectively for en, zh-Hans, fr, es, pt,\nar, vi, hi, id, bn, ta, te, ur, ne, mr, gu, zh-Hant,\nsw, yo and ig. Additionally, we conducted tests\nin the Equal MBS setting, in which an equal num-\nber of segments were sampled from each language.\nWe also implemented the monolingual compres-\nsion setting, using 256 segments from the same\nlanguage.\nLanguage Similarity Study. To study language\nsimilarity, we conducted monolingual pruning on\n5\nFigure 4: Perplexity for each language and their respective increases when compared to the dense BLOOM-7b1\nmodel after pruning (left) or quantization (right). From left to right, languages are ranked in order from the most\nwell-represented to the least represented.\nEnglish and Igbo, representing the best-represented\nand worst-represented languages in our dataset, re-\nspectively. We also performed similar experiments\non Urdu and Tamil, which respectively represent\nthe least and most similar languages to the others\n(further explanation is provided in the results sec-\ntion). To compare language similarity, we utilized\nthe representations after the embedding layer of the\nBLOOM model, as the compression algorithms do\nnot affect the embedding layer.\n5.2\nMain results\nWe conducted our MBS sampling technique to com-\npress both BLOOM-7b1 and BLOOM-560m mod-\nels, using GPTQ, SparseGPT, and Wanda. The\ntrends observed in the results for these two mod-\nels are similar. For the sake of better formatting,\nwe will present the results for the 7b1 model in\nthe main text and provide the results for the 560M\nmodel in the appendices.\nPerplexity. Figure 4 presents the evaluation of\nperplexity for each language after compression on\nthe BLOOM-7b1 model. The baselines consist\nof monolingual compression using English-only\ncalibration data.\n1. Across various compression methods, the\nMBS sampling technique consistently leads\nto minimal increases in perplexity. This holds\ntrue whether we utilize Wanda or SparseGPT\nfor pruning or GPTQ for quantization.\n2. For underrepresented languages (located on\nthe right side of the axis), MBS can notably\nreduce the increase in perplexity after com-\npression, thus preserving the model’s capac-\nity for lower-resourced languages, leaving no\nlanguages behind.\n3. Even for the most well-represented lan-\nguage, specifically English (on both datasets\n\"en\" and \"wikitext2\"), using MBS sampling\nintroduces a lower perplexity than its mono-\nlingual English-centric sampling counterpart.\nZero shot tasks. Table 1 provides an overview\nof the performance of zero-shot tasks after the com-\npression process. The results demonstrate that,\nin the majority of tasks, utilizing MBS sampling\nyields superior performance compared to other sam-\npling techniques. Furthermore, the performance\nafter compression closely approximates that of the\ndense model, highlighting the effectiveness of our\napproach.\nEqual MBS. Table 2 provides the results for\nEqual MBS, where an equal number of samples\nare taken from each language. While Equal MBS\nis not the optimal setting, it generally improves\nthe performance of the compressed model. This\ndemonstrates that even without access to the dis-\ntribution of languages in the training set, Equal\nMBS can still enhance compression results for the\nchosen languages, showcasing the versatility of our\nmethod.\n5.3\nMonolingual Compression Study\n5.3.1\nFactor 1: Proportion in training data\nTo investigate how the proportion of a language in\nthe training data affects compression results, we\nselected English (en) and Igbo (ig), which have\nthe largest and smallest proportions in the train-\ning data among the languages in our experiments,\nrespectively. The results are presented in Figure 5.\n6\nAccuracy of\n0-shot task\nDense\nWanda\nWanda\n+MBS\nSparseGPT\nSparseGPT\n+MBS\nGPTQ\nGPTQ\n+MBS\nxcopa↑\nid\n69.80%\n67.20%\n67.40%\n65.60%\n66.40%\n67.20%\n67.40%\nsw\n51.60%\n54.80%\n53.80%\n55.20%\n51.20%\n54.60%\n55.00%\nta\n59.20%\n61.20%\n57.80%\n60.60%\n58.60%\n58.40%\n57.80%\nvi\n70.80%\n69.80%\n67.20%\n66.80%\n66.40%\n67.00%\n68.20%\nzh\n65.20%\n62.00%\n63.60%\n62.20%\n63.80%\n61.00%\n62.60%\nAverage\n63.32%\n63.00%\n61.96%\n62.08%\n61.28%\n61.64%\n62.20%\nxstory_cloze↑\nar\n58.57%\n53.94%\n54.93%\n54.93%\n56.32%\n56.45%\n57.18%\nen\n70.75%\n68.23%\n67.70%\n69.23%\n68.96%\n68.70%\n68.96%\nes\n66.12%\n64.39%\n63.20%\n62.87%\n64.39%\n64.53%\n64.79%\nhi\n60.56%\n56.92%\n57.18%\n57.64%\n58.44%\n58.04%\n58.17%\nid\n64.46%\n59.96%\n60.29%\n59.23%\n61.81%\n60.89%\n62.54%\nsw\n53.94%\n50.89%\n51.69%\n50.69%\n52.02%\n52.28%\n52.95%\nte\n57.45%\n56.52%\n56.72%\n56.78%\n57.97%\n57.18%\n57.71%\nzh\n61.88%\n58.37%\n59.56%\n57.91%\n60.89%\n60.03%\n60.03%\nAverage\n61.71%\n58.65%\n58.91%\n58.66%\n60.10%\n59.76%\n60.29%\nxwinograd↑\nen\n82.15%\n79.40%\n78.88%\n80.09%\n79.74%\n79.35%\n79.57%\nfr\n71.08%\n71.08%\n67.47%\n72.29%\n73.49%\n65.06%\n67.47%\npt\n76.81%\n74.14%\n75.29%\n71.48%\n74.14%\n69.20%\n72.24%\nzh\n74.40%\n74.40%\n75.79%\n74.40%\n75.20%\n71.23%\n73.81%\nAverage\n76.11%\n74.76%\n74.36%\n74.57%\n75.64%\n71.21%\n73.27%\npawsx↑\nen\n61.30%\n53.60%\n54.75%\n57.50%\n58.25%\n56.75%\n58.60%\nes\n59.35%\n51.75%\n54.05%\n54.10%\n56.60%\n57.95%\n56.10%\nfr\n50.90%\n47.45%\n46.45%\n50.85%\n47.10%\n52.30%\n48.60%\nzh\n47.35%\n45.05%\n45.45%\n45.70%\n47.45%\n49.10%\n50.00%\nAverage\n54.73%\n49.46%\n50.18%\n52.04%\n52.35%\n54.03%\n53.33%\nxnli↑\nar\n33.83%\n33.67%\n33.91%\n34.89%\n34.51%\n33.67%\n34.75%\nen\n53.91%\n52.20%\n52.59%\n53.49%\n53.49%\n52.73%\n52.93%\nes\n48.70%\n48.14%\n47.47%\n45.13%\n46.81%\n46.63%\n47.54%\nfr\n49.68%\n43.57%\n48.38%\n46.29%\n49.00%\n48.58%\n48.62%\nhi\n46.51%\n42.63%\n44.51%\n40.60%\n45.97%\n44.19%\n46.63%\nsw\n37.92%\n38.36%\n37.80%\n37.35%\n36.29%\n36.63%\n37.33%\nur\n42.10%\n39.82%\n40.54%\n40.42%\n39.58%\n38.42%\n41.98%\nvi\n47.05%\n45.99%\n46.35%\n42.46%\n44.89%\n44.29%\n46.09%\nzh\n35.43%\n35.31%\n33.99%\n34.57%\n34.21%\n35.27%\n34.71%\nAverage\n43.90%\n42.19%\n42.84%\n41.69%\n42.75%\n42.27%\n41.35%\nAverage↑\n57.63%\n55.36%\n55.49%\n55.38%\n56.13%\n55.59%\n57.08%\nTable 1: 0-shot task performance of BLOOM-7b1 with different model compression methods.\n7\nFigure 5: Monolingual pruning results using Wanda with calibration data in English or Igbo. The size of each\nbubble corresponds to the magnitude of the increase in perplexity for the model in that particular language, while\nthe vertical axis represents the size of training data in log(bytes) from the language in the training set of BLOOM.\nThe languages with a smaller proportion in the training set experience a greater increase in perplexity.\nCompression\nMethods\nAverage\n0-shot Task\nAccuracy↑\nAverage\nppl↓\nWanda\n55.36%\n64.70\nWanda+\nEqual MBS\n55.20%\n24.97\nWanda+\nMBS\n55.49%\n26.28\nSparseGPT\n55.38%\n59.84\nSparseGPT+\nEqual MBS\n55.86%\n22.62\nSparseGPT+\nMBS\n56.13%\n23.82\nGPTQ\n55.59%\n31.17\nGPTQ+\nEqual MBS\n56.52%\n23.15\nGPTQ+\nMBS\n57.08%\n24.26\nTable 2: Performance of Equal MBS, where an equal\nnumber of segments are sampled from each language.\nIt is evident that if we use only English as\nour calibration data, it significantly impacts less\nwell-represented languages, causing substantial in-\ncreases in perplexity, particularly for Marathi (mr)\nand Gujarati (gu). However, for better-represented\nlanguages, English (en) has a relatively smaller\ninfluence, as observed with Chinese simplified\n(zh-Hans), French (fr), and Spanish (es). Con-\nversely, when we use only Igbo as our calibration\ndata, the increase in perplexity for the other lan-\nguages is relatively small. Clearly, languages with\na lower representation in the training set tend\nto experience a more substantial increase in per-\nplexity.\n5.3.2\nFactor 2: Similarity between languages\nWe calculated the cosine similarity of ||Xn||2\n2 for\ndifferent languages using BLOOM-7b1, and then\nconverted this similarity into degrees. This allowed\nus to create a distance map between languages.\nTo visualize the relative positions of different lan-\nguages, we employed Multidimensional scaling\n(Mead, 1992) and generated a 2-dimensional figure\n(Figure 6). The original distance map is included\nin the appendices.\nUpon observing this graph, we can identify some\ninteresting clusters. Typically, languages from dif-\nferent language families tend to form distinct clus-\nters. For instance, there is a cluster comprising\nIndo-European languages such as English, Spanish,\nFrench, and Portuguese, a cluster for Chinese sim-\nplified and Chinese traditional, both of which are\nChinese languages, and another cluster consisting\nof Niger-Congo languages like Yoruba, Igbo, and\nSwahili. This clustering may be attributed to the\nfollowing factors:\n• Shared Grammar Structure: Languages\nwithin the same language family often share\nsimilar grammar structures.\n• Shared Tokens: During the tokenization pro-\ncess, these languages frequently share tokens,\nincluding prefixes, suffixes, and other word-\nbuilding elements.\nTo investigate how language similarity impacts\ncompression outcomes, we chose to examine two\nextreme cases: Tamil (ta), which is the language\nthat is \"closest\" to all other languages (with an av-\nerage distance of 7.25), and Urdu (ur), which is the\nlanguage that is \"farthest\" from all other languages\n(with an average distance of 15.45). The results for\nWanda are displayed in Figure 7, while the results\nfor SparseGPT, which exhibit similar patterns to\n8\nFigure 6: Distance map of different languages associ-\nated with their corresponding language families. We can\nsee that languages with the same family cluster together\nfrom this map.\nthose of Wanda, are provided in the appendices.\nIt is evident that when we use Tamil (ta) as our\nsole calibration data, the increase in perplexity for\nother languages is relatively small, especially for\nlanguages that are \"closer\" to Tamil, such as Ben-\ngali (bn) and Hindi (hi). Conversely, when Urdu\n(ur) serves as our sole calibration data, the increase\nin perplexity for other languages is relatively signif-\nicant on average. The consistent pattern across all\nfour graphs reveals that languages more distant\nfrom the language being compressed tend to ex-\nhibit a more significant increase in perplexity.\nAn intriguing case study can be conducted on\nChinese simplified and Chinese traditional. De-\nspite their close proximity on the language map,\nthey significantly differ in corpus size. Chinese\nsimplified enjoys a much larger proportion, result-\ning in a more pronounced impact on Chinese tradi-\ntional after the compression process, while Chinese\nsimplified remains relatively unaffected. These ex-\nperiments demonstrate the validity and accuracy of\nour theory.\n6\nRelated Work\nLarge Language Model.\nLarge Language\nModels(Zhao et al., 2023) like GPT-4(OpenAI\net al., 2024), LLaMA(Touvron et al., 2023) and\nOPT(Zhang et al., 2022), which have revolu-\ntionized Natural Language Processing through\ntheir ability to understand and generate nu-\nanced text.\nAlongside, multilingual language\nmodels(Doddapaneni\net\nal.,\n2021)\nsuch\nas\nBLOOM(BigScience Workshop, 2022) and XLM-\nR(Conneau et al., 2020) are breaking language bar-\nriers by learning universal representations from\ntexts across numerous languages. These develop-\nments underscore a significant shift towards cre-\nating more versatile and inclusive NLP systems,\nwith research focusing on architectural innovations,\ntraining efficiencies, and cross-lingual capabilities\nto enhance global digital interaction.\nWe would like to emphasize that MBS can be ap-\nplied to any model compression method that utilizes\ncalibration data, particularly methods based on the\nOBS/OBD framework, where the approximation of\nsecond-derivative information is required. Thanks\nto a survey on model compression for large lan-\nguage models by (Zhu et al., 2023), we examined\nthe state-of-the-art model compression methods for\nlarge language models, and we found that our MBS\nis useful for almost all of them.\nPruning and quantization are two major model\ncompression methods for LLMs.\nPruning. Pruning reduces model size and com-\nplexity by eliminating unnecessary or redundant\ncomponents. It can be categorized into structured\npruning, where higher-granularity structures like\nrows or columns of weight matrices are removed,\nand unstructured pruning, which eliminates in-\ndividual weights, leading to irregular sparse struc-\ntures. In the domain of unstructured pruning,\nMBS can be applied to Wanda (Sun et al., 2023)\nand SparseGPT (Frantar and Alistarh, 2023) that\nwe presented in the background section, and also\nLoRAPrune (Zhang et al., 2023). In the structured\npruning domain, MBS can empower LLM-Pruner\n(Ma et al., 2023).\nQuantization. Quantization involves converting\nfloating-point numbers into lower bit-level repre-\nsentations, integers, or other discrete forms and can\nbe categorized into Quantization-Aware Training\nand Post-Training Quantization. MBS finds nu-\nmerous applications in quantization, particularly in\npost-training quantization. In post-training quan-\ntization, certain approaches focus on quantizing\nonly the weights of LLMs. Among these methods,\nMBS can be applied to AWQ (Lin et al., 2023),\nGPTQ (Frantar et al., 2023), OWQ (Lee et al.,\n2023), SpQR (Dettmers et al., 2023), SqueezeLLM\n(Kim et al., 2023), QuIP (Chee et al., 2023), and\nSignRound (Cheng et al., 2023). Some other meth-\nods try to quantize both weights and activations\nof LLMs. Among them, MBS can be applied to\n9\nFigure 7: Similarly to Figure 5, but focusing on Urdu or Tamil. The languages less similar to the calibration\nlanguage experience a greater increase in perplexity.\nSmoothQuant (Xiao et al., 2023), RPTQ (Yuan\net al., 2023), OliVe (Guo et al., 2023), ZeroQuant-\nV2 (Yao et al., 2023), Outlier Suppression+ (Wei\net al., 2023), FPTQ (Li et al., 2023), QuantEase\n(Behdin et al., 2023), and OmniQuant (Shao et al.,\n2023).\n7\nConclusions\nIn summary, the Multilingual Brain Surgeon (MBS)\nis a groundbreaking approach for improving mul-\ntilingual LLMs. It tackles the English-centric bias\nin existing techniques and enhances LLM perfor-\nmance after compressing. Our experiments on the\nBLOOM model highlight the effectiveness of MBS,\nbenefiting pruning and quantization methods like\nSparseGPT, Wanda, and GPTQ.\nWe also studied language interaction during com-\npression, finding that language proportion in the\ntraining dataset and language similarity are crucial\nfactors. Languages with larger proportion are less\naffected by compression, while similar languages\nperform better when only one language is used in\ncalibration data. Our proposed similarity measure\naccurately predicts performance drops in such sce-\nnarios.\nThis research not only enhances the practicality\nof multilingual LLMs compression methods but\nalso maintains language coverage, making multi-\nlingual NLP applications more inclusive and pow-\nerful.\nAcknowledgments\nThis work is funded by the China NSFC\nProjects\n(92370206,\nU23B2057,\n62106142\nand\n62120106006)\nand\nShanghai\nMunici-\npal Science and Technology Major Project\n(2021SHZDZX0102).\nReferences\nKayhan Behdin, Ayan Acharya, Aman Gupta, Sathiya\nKeerthi, and Rahul Mazumder. 2023. Quantease:\nOptimization-based quantization for language mod-\nels – an efficient and intuitive algorithm. Preprint,\narXiv:2309.01885.\nBigScience Workshop. 2022.\nBLOOM (revision\n4ab0472).\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot learn-\ners. Preprint, arXiv:2005.14165.\nJerry Chee, Yaohui Cai, Volodymyr Kuleshov, and\nChristopher De Sa. 2023. Quip: 2-bit quantization\nof large language models with guarantees. Preprint,\narXiv:2307.13304.\nWenhua Cheng, Weiwei Zhang, Haihao Shen, Yiyang\nCai, Xin He, and Kaokao Lv. 2023. Optimize weight\nrounding via signed gradient descent for the quanti-\nzation of llms. Preprint, arXiv:2309.05516.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020.\nUnsuper-\nvised cross-lingual representation learning at scale.\nPreprint, arXiv:1911.02116.\nTim Dettmers, Mike Lewis, Younes Belkada, and\nLuke Zettlemoyer. 2022. Llm.int8(): 8-bit matrix\n10\nmultiplication for transformers at scale. Preprint,\narXiv:2208.07339.\nTim Dettmers, Ruslan Svirschevski, Vage Egiazarian,\nDenis Kuznedelev, Elias Frantar, Saleh Ashkboos,\nAlexander Borzunov, Torsten Hoefler, and Dan Alis-\ntarh. 2023. Spqr: A sparse-quantized representation\nfor near-lossless llm weight compression. Preprint,\narXiv:2306.03078.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. Preprint, arXiv:1810.04805.\nSumanth Doddapaneni, Gowtham Ramesh, Mitesh M.\nKhapra, Anoop Kunchukuttan, and Pratyush Kumar.\n2021. A primer on pretrained multilingual language\nmodels. Preprint, arXiv:2107.00676.\nUtku Evci, Trevor Gale, Jacob Menick, Pablo Samuel\nCastro, and Erich Elsen. 2019. Rigging the lottery:\nMaking all tickets winners. CoRR, abs/1911.11134.\nElias Frantar and Dan Alistarh. 2023. Sparsegpt: Mas-\nsive language models can be accurately pruned in\none-shot. Preprint, arXiv:2301.00774.\nElias Frantar, Saleh Ashkboos, Torsten Hoefler, and\nDan Alistarh. 2023. Gptq: Accurate post-training\nquantization for generative pre-trained transformers.\nPreprint, arXiv:2210.17323.\nTrevor Gale, Erich Elsen, and Sara Hooker. 2019. The\nstate of sparsity in deep neural networks. Preprint,\narXiv:1902.09574.\nLeo Gao, Jonathan Tow, Stella Biderman, Sid Black,\nAnthony DiPofi, Charles Foster, Laurence Golding,\nJeffrey Hsu, Kyle McDonell, Niklas Muennighoff,\nJason Phang, Laria Reynolds, Eric Tang, Anish Thite,\nBen Wang, Kevin Wang, and Andy Zou. 2021. A\nframework for few-shot language model evaluation.\nNaman Goyal, Jingfei Du, Myle Ott, Giri Ananthara-\nman, and Alexis Conneau. 2021. Larger-scale trans-\nformers for multilingual masked language modeling.\narXiv preprint arXiv:2105.00572.\nCong Guo, Jiaming Tang, Weiming Hu, Jingwen Leng,\nChen Zhang, Fan Yang, Yunxin Liu, Minyi Guo, and\nYuhao Zhu. 2023. OliVe: Accelerating large lan-\nguage models via hardware-friendly outlier-victim\npair quantization. In Proceedings of the 50th Annual\nInternational Symposium on Computer Architecture.\nACM.\nSong Han, Jeff Pool, John Tran, and William Dally.\n2015. Learning both weights and connections for\nefficient neural network. In Advances in Neural In-\nformation Processing Systems, volume 28. Curran\nAssociates, Inc.\nTahmid Hasan, Abhik Bhattacharjee, Md. Saiful Is-\nlam, Kazi Mubasshir, Yuan-Fang Li, Yong-Bin Kang,\nM. Sohel Rahman, and Rifat Shahriyar. 2021. XL-\nsum: Large-scale multilingual abstractive summariza-\ntion for 44 languages. In Findings of the Association\nfor Computational Linguistics: ACL-IJCNLP 2021,\npages 4693–4703, Online. Association for Computa-\ntional Linguistics.\nB. Hassibi, D.G. Stork, and G.J. Wolff. 1993. Optimal\nbrain surgeon and general network pruning. In IEEE\nInternational Conference on Neural Networks, pages\n293–299 vol.1.\nItay Hubara, Brian Chmiel, Moshe Island, Ron Ban-\nner, Seffi Naor, and Daniel Soudry. 2021. Acceler-\nated sparse neural training: A provable and efficient\nmethod to find n:m transposable masks. Preprint,\narXiv:2102.08124.\nEugenia Iofinova, Alexandra Peste, Mark Kurtz, and\nDan Alistarh. 2021. How well do sparse imagenet\nmodels transfer? CoRR, abs/2111.13445.\nSehoon Kim, Coleman Hooper, Amir Gholami, Zhen\nDong, Xiuyu Li, Sheng Shen, Michael W. Mahoney,\nand Kurt Keutzer. 2023. Squeezellm: Dense-and-\nsparse quantization. Preprint, arXiv:2306.07629.\nMark Kurtz, Justin Kopinsky, Rati Gelashvili, Alexan-\nder Matveev, John Carr, Michael Goin, William Leis-\nerson, Sage Moore, Bill Nell, Nir Shavit, and Dan\nAlistarh. 2020. Inducing and exploiting activation\nsparsity for fast inference on deep neural networks.\nIn Proceedings of the 37th International Conference\non Machine Learning, volume 119 of Proceedings\nof Machine Learning Research, pages 5533–5543,\nVirtual. PMLR.\nGuillaume Lample and Alexis Conneau. 2019. Cross-\nlingual language model pretraining.\nPreprint,\narXiv:1901.07291.\nYann Le Cun, John S. Denker, and Sara A. Solla.\n1989. Optimal brain damage. In Proceedings of\nthe 2nd International Conference on Neural Infor-\nmation Processing Systems, NIPS’89, page 598–605,\nCambridge, MA, USA. MIT Press.\nChanghun Lee, Jungyu Jin, Taesu Kim, Hyungjun Kim,\nand Eunhyeok Park. 2023. Owq: Lessons learned\nfrom activation outliers for weight quantization in\nlarge language models. Preprint, arXiv:2306.02272.\nQingyuan Li, Yifan Zhang, Liang Li, Peng Yao,\nBo Zhang, Xiangxiang Chu, Yerui Sun, Li Du, and\nYuchen Xie. 2023. Fptq: Fine-grained post-training\nquantization for large language models. Preprint,\narXiv:2308.15987.\nJi Lin, Jiaming Tang, Haotian Tang, Shang Yang,\nXingyu Dang, Chuang Gan, and Song Han. 2023.\nAwq:\nActivation-aware weight quantization for\nllm compression and acceleration.\nPreprint,\narXiv:2306.00978.\n11\nLiyang Liu, Shilong Zhang, Zhanghui Kuang, Aojun\nZhou, Jing-Hao Xue, Xinjiang Wang, Yimin Chen,\nWenming Yang, Qingmin Liao, and Wayne Zhang.\n2021. Group fisher pruning for practical network\ncompression. CoRR, abs/2108.00708.\nXinyin Ma, Gongfan Fang, and Xinchao Wang. 2023.\nLlm-pruner: On the structural pruning of large lan-\nguage models. Preprint, arXiv:2305.11627.\nAl Mead. 1992. Review of the development of multidi-\nmensional scaling methods. Journal of the Royal Sta-\ntistical Society: Series D (The Statistician), 41(1):27–\n39.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2016. Pointer sentinel mixture mod-\nels. Preprint, arXiv:1609.07843.\nOpenAI, :, Josh Achiam, Steven Adler, Sandhini Agar-\nwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Ale-\nman, Diogo Almeida, Janko Altenschmidt, Sam Alt-\nman, Shyamal Anadkat, Red Avila, Igor Babuschkin,\nSuchir Balaji, Valerie Balcom, Paul Baltescu, Haim-\ning Bao, Mohammad Bavarian, Jeff Belgum, Ir-\nwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro,\nChristopher Berner, Lenny Bogdonoff, Oleg Boiko,\nMadelaine Boyd, Anna-Luisa Brakman, Greg Brock-\nman, Tim Brooks, Miles Brundage, Kevin Button,\nTrevor Cai, Rosie Campbell, Andrew Cann, Brittany\nCarey, Chelsea Carlson, Rory Carmichael, Brooke\nChan, Che Chang, Fotis Chantzis, Derek Chen, Sully\nChen, Ruby Chen, Jason Chen, Mark Chen, Ben\nChess, Chester Cho, Casey Chu, Hyung Won Chung,\nDave Cummings, Jeremiah Currier, Yunxing Dai,\nCory Decareaux, Thomas Degry, Noah Deutsch,\nDamien Deville, Arka Dhar, David Dohan, Steve\nDowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti,\nTyna Eloundou, David Farhi, Liam Fedus, Niko Felix,\nSimón Posada Fishman, Juston Forte, Isabella Ful-\nford, Leo Gao, Elie Georges, Christian Gibson, Vik\nGoel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-\nLopes, Jonathan Gordon, Morgan Grafstein, Scott\nGray, Ryan Greene, Joshua Gross, Shixiang Shane\nGu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris,\nYuchen He, Mike Heaton, Johannes Heidecke, Chris\nHesse, Alan Hickey, Wade Hickey, Peter Hoeschele,\nBrandon Houghton, Kenny Hsu, Shengli Hu, Xin\nHu, Joost Huizinga, Shantanu Jain, Shawn Jain,\nJoanne Jang, Angela Jiang, Roger Jiang, Haozhun\nJin, Denny Jin, Shino Jomoto, Billie Jonn, Hee-\nwoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Ka-\nmali, Ingmar Kanitscheider, Nitish Shirish Keskar,\nTabarak Khan, Logan Kilpatrick, Jong Wook Kim,\nChristina Kim, Yongjik Kim, Jan Hendrik Kirch-\nner, Jamie Kiros, Matt Knight, Daniel Kokotajlo,\nŁukasz Kondraciuk, Andrew Kondrich, Aris Kon-\nstantinidis, Kyle Kosic, Gretchen Krueger, Vishal\nKuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan\nLeike, Jade Leung, Daniel Levy, Chak Ming Li,\nRachel Lim, Molly Lin, Stephanie Lin, Mateusz\nLitwin, Theresa Lopez, Ryan Lowe, Patricia Lue,\nAnna Makanju, Kim Malfacini, Sam Manning, Todor\nMarkov, Yaniv Markovski, Bianca Martin, Katie\nMayer, Andrew Mayne, Bob McGrew, Scott Mayer\nMcKinney, Christine McLeavey, Paul McMillan,\nJake McNeil, David Medina, Aalok Mehta, Jacob\nMenick, Luke Metz, Andrey Mishchenko, Pamela\nMishkin, Vinnie Monaco, Evan Morikawa, Daniel\nMossing, Tong Mu, Mira Murati, Oleg Murk, David\nMély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak,\nArvind Neelakantan, Richard Ngo, Hyeonwoo Noh,\nLong Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex\nPaino, Joe Palermo, Ashley Pantuliano, Giambat-\ntista Parascandolo, Joel Parish, Emy Parparita, Alex\nPassos, Mikhail Pavlov, Andrew Peng, Adam Perel-\nman, Filipe de Avila Belbute Peres, Michael Petrov,\nHenrique Ponde de Oliveira Pinto, Michael, Poko-\nrny, Michelle Pokrass, Vitchyr H. Pong, Tolly Pow-\nell, Alethea Power, Boris Power, Elizabeth Proehl,\nRaul Puri, Alec Radford, Jack Rae, Aditya Ramesh,\nCameron Raymond, Francis Real, Kendra Rimbach,\nCarl Ross, Bob Rotsted, Henri Roussez, Nick Ry-\nder, Mario Saltarelli, Ted Sanders, Shibani Santurkar,\nGirish Sastry, Heather Schmidt, David Schnurr, John\nSchulman, Daniel Selsam, Kyla Sheppard, Toki\nSherbakov, Jessica Shieh, Sarah Shoker, Pranav\nShyam, Szymon Sidor, Eric Sigler, Maddie Simens,\nJordan Sitkin, Katarina Slama, Ian Sohl, Benjamin\nSokolowsky, Yang Song, Natalie Staudacher, Fe-\nlipe Petroski Such, Natalie Summers, Ilya Sutskever,\nJie Tang, Nikolas Tezak, Madeleine B. Thompson,\nPhil Tillet, Amin Tootoonchian, Elizabeth Tseng,\nPreston Tuggle, Nick Turley, Jerry Tworek, Juan Fe-\nlipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya,\nChelsea Voss, Carroll Wainwright, Justin Jay Wang,\nAlvin Wang, Ben Wang, Jonathan Ward, Jason Wei,\nCJ Weinmann, Akila Welihinda, Peter Welinder, Ji-\nayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner,\nClemens Winter, Samuel Wolrich, Hannah Wong,\nLauren Workman, Sherwin Wu, Jeff Wu, Michael\nWu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qim-\ning Yuan, Wojciech Zaremba, Rowan Zellers, Chong\nZhang, Marvin Zhang, Shengjia Zhao, Tianhao\nZheng, Juntang Zhuang, William Zhuk, and Bar-\nret Zoph. 2024. Gpt-4 technical report. Preprint,\narXiv:2303.08774.\nAlexandra Peste, Eugenia Iofinova, Adrian Vladu,\nand Dan Alistarh. 2021. Ac/dc: Alternating com-\npressed/decompressed training of deep neural net-\nworks. Preprint, arXiv:2106.12379.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2019. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. arXiv e-prints.\nWenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng\nXu, Lirui Zhao, Zhiqian Li, Kaipeng Zhang, Peng\nGao, Yu Qiao, and Ping Luo. 2023. Omniquant:\nOmnidirectionally calibrated quantization for large\nlanguage models. Preprint, arXiv:2308.13137.\nMingjie Sun, Zhuang Liu, Anna Bair, and J. Zico Kolter.\n2023. A simple and effective pruning approach for\nlarge language models. Preprint, arXiv:2306.11695.\n12\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models. Preprint,\narXiv:2302.13971.\nXiuying Wei, Yunchen Zhang, Yuhang Li, Xiangguo\nZhang, Ruihao Gong, Jinyang Guo, and Xiang-\nlong Liu. 2023.\nOutlier suppression+: Accurate\nquantization of large language models by equiva-\nlent and optimal shifting and scaling.\nPreprint,\narXiv:2304.09145.\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Con-\nneau, Vishrav Chaudhary, Francisco Guzmán, Ar-\nmand Joulin, and Edouard Grave. 2020.\nCCNet:\nExtracting high quality monolingual datasets from\nweb crawl data. In Proceedings of the 12th Lan-\nguage Resources and Evaluation Conference, pages\n4003–4012, Marseille, France. European Language\nResources Association.\nGuangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu,\nJulien Demouth, and Song Han. 2023. Smoothquant:\nAccurate and efficient post-training quantization for\nlarge language models. Preprint, arXiv:2211.10438.\nZhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang,\nXiaoxia Wu, Conglong Li, and Yuxiong He. 2022.\nZeroquant: Efficient and affordable post-training\nquantization for large-scale transformers. Preprint,\narXiv:2206.01861.\nZhewei Yao, Xiaoxia Wu, Cheng Li, Stephen Youn,\nand Yuxiong He. 2023. Zeroquant-v2: Exploring\npost-training quantization in llms from comprehen-\nsive study to low rank compensation.\nPreprint,\narXiv:2303.08302.\nZhihang Yuan, Lin Niu, Jiawei Liu, Wenyu Liu, Xing-\ngang Wang, Yuzhang Shang, Guangyu Sun, Qiang\nWu, Jiaxiang Wu, and Bingzhe Wu. 2023. Rptq:\nReorder-based post-training quantization for large\nlanguage models. Preprint, arXiv:2304.01089.\nMingyang Zhang, Hao Chen, Chunhua Shen, Zhen\nYang, Linlin Ou, Xinyi Yu, and Bohan Zhuang.\n2023. Loraprune: Pruning meets low-rank parameter-\nefficient fine-tuning. Preprint, arXiv:2305.18403.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-\nhaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel\nSimig, Punit Singh Koura, Anjali Sridhar, Tianlu\nWang, and Luke Zettlemoyer. 2022.\nOpt: Open\npre-trained transformer language models. Preprint,\narXiv:2205.01068.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Be-\nichen Zhang, Junjie Zhang, Zican Dong, Yifan Du,\nChen Yang, Yushuo Chen, Zhipeng Chen, Jinhao\nJiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang\nLiu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen.\n2023. A survey of large language models. Preprint,\narXiv:2303.18223.\nMichael Zhu and Suyog Gupta. 2017. To prune, or not\nto prune: exploring the efficacy of pruning for model\ncompression. Preprint, arXiv:1710.01878.\nXunyu Zhu, Jian Li, Yong Liu, Can Ma, and Weip-\ning Wang. 2023.\nA survey on model compres-\nsion for large language models.\narXiv preprint\narXiv:2308.07633.\n13\nA\nDetails of Calibration Data\nLanguage\nSize in Bytes\nin BLOOM\ntraining data\nMBS\nsampling\nEqual\nsampling\nen\n4.85E+11\n87\n13\nzh-Hans\n2.61E+11\n47\n13\nfr\n2.08E+11\n37\n13\nes\n1.75E+11\n31\n13\npt\n7.93E+10\n14\n13\nar\n7.49E+10\n13\n13\nvi\n4.37E+10\n7\n13\nhi\n2.46E+10\n4\n13\nid\n2.00E+10\n3\n13\nbn\n1.86E+10\n3\n13\nta\n7.99E+09\n1\n13\nte\n2.99E+09\n1\n13\nur\n2.78E+09\n1\n13\nne\n2.55E+09\n1\n13\nmr\n1.78E+09\n1\n13\ngu\n1.20E+09\n1\n13\nzh-Hant\n7.62E+08\n1\n12\nsw\n2.36E+08\n1\n12\nyo\n8.97E+07\n1\n12\nig\n1.41E+07\n1\n12\nTable 3: The number of segments taken from each lan-\nguage by each sampling method.\nThe number of segments taken from each lan-\nguage is detailed in Table 3. We rounded up the\nsegment counts for languages with fewer than one\nsegment to ensure their representation in the cal-\nibration data. In the equal sampling scenario, to\nmaintain comparability, some languages have one\nsegment less than others to achieve a total of 256\nsegments.\nB\nMBS results tables\nPerplexity. Table 6 showcases the perplexity eval-\nuation for each language after pruning on the\nBLOOM-560m model. The observed trends align\nclosely with those observed on the BLOOM-7b1\nmodel.\nZero shot tasks. Table 7 illustrates the zero-shot\ntask results for the pruned BLOOM-560m model.\nIt is noticeable that the average accuracy using the\nMBS sampling method continues to outperform\nthe baselines, although the results appear to exhibit\nmore variability. This variability can be attributed\nto the reduced capacity of smaller models to main-\ntain their multilingual capabilities.\nThe role of parameter compensation. We have\nobserved a notable distinction in the effects of\nSparseGPT and Wanda. In monolingual pruning,\nSparseGPT, which involves parameter updates and\nemploys a more precise pruning metric, appears\nto have a more detrimental impact on less well-\nrepresented languages. However, when we apply\nMBS, SparseGPT continues to outperform Wanda.\nThis phenomenon may be attributed to the fact that\nsmaller models are more sensitive to parameter\nupdates. A biased Hessian matrix can exacerbate\nthe model’s divergence from the correct direction\nthrough these updates. Conversely, a correctly ap-\nproximated Hessian matrix can effectively guide\nthe pruning in the correct direction.\nC\nMonolingual pruning results\nDistance map of BLOOM-560m model. The\ndistance map depicting the relationships between\nlanguages in the BLOOM-560m model is depicted\nin Figure 8. We can discern a similar clustering\npattern to that observed in the BLOOM-7b1 model.\nThe original distance matrices are provided in\nthe following tables(12, 13).\n14\nDataset\nDense\nWanda\n↑\nSparseGPT\n↑\nMBS +\nWanda\n↑\nMBS +\nSparseGPT\n↑\nen\n13.68\n15.67\n15%\n14.92\n9%\n15.55\n14%\n15.01\n10%\nzh-Hans\n23.70\n34.75\n47%\n35.56\n50%\n26.59\n12%\n25.87\n9%\nfr\n9.59\n13.16\n37%\n13.41\n40%\n10.68\n11%\n10.39\n8%\nes\n10.71\n13.82\n29%\n13.75\n28%\n11.91\n11%\n11.59\n8%\npt\n10.97\n14.58\n33%\n17.37\n58%\n12.25\n12%\n11.96\n9%\nar\n14.40\n29.19\n103%\n25.33\n76%\n16.45\n14%\n15.88\n10%\nvi\n10.16\n14.76\n45%\n15.00\n48%\n11.59\n14%\n11.24\n11%\nhi\n10.96\n18.26\n67%\n19.09\n74%\n12.52\n14%\n12.19\n11%\nid\n20.48\n29.37\n43%\n37.27\n82%\n23.76\n16%\n22.97\n12%\nbn\n17.27\n33.37\n93%\n40.50\n134%\n20.29\n17%\n19.51\n13%\nta\n16.55\n42.23\n155%\n44.10\n167%\n20.10\n21%\n19.34\n17%\nte\n18.10\n64.97\n259%\n69.20\n282%\n24.59\n36%\n22.05\n22%\nur\n13.26\n27.03\n104%\n30.56\n130%\n15.83\n19%\n15.10\n14%\nne\n27.22\n152.67\n461%\n148.00\n444%\n34.90\n28%\n32.91\n21%\nmr\n23.07\n176.78\n666%\n144.65\n527%\n32.25\n40%\n28.91\n25%\ngu\n21.52\n184.62\n758%\n118.48\n450%\n30.84\n43%\n26.97\n25%\nzh-Hant\n21.84\n113.34\n419%\n102.26\n368%\n24.96\n14%\n24.30\n11%\nsw\n34.35\n145.54\n324%\n135.84\n295%\n54.32\n58%\n44.23\n29%\nyo\n53.29\n128.12\n140%\n126.54\n137%\n79.62\n49%\n67.52\n27%\nig\n39.16\n90.41\n131%\n90.89\n132%\n59.00\n51%\n49.10\n25%\nwikitext2\n11.37\n16.15\n42%\n13.91\n22%\n13.82\n22%\n13.26\n17%\nAverage\n20.08\n64.70\n222%\n59.84\n198%\n26.28\n31%\n23.82\n19%\nTable 4: Perplexity for each language and their respective increases when compared to the dense BLOOM-7b1\nmodel after pruning. Evaluation performed on XL-Sum and WikiText2 datasets. From top to bottom, languages are\nranked in order from the most well-represented to the least represented.\nFigure 8: The graph illustrates the relative positions of different languages. Different dot shapes represent different\nlanguage families. The closer they are on the graph, the more similar they are to each other.\n15\nDataset\nDense\nGPTQ\nGPTQ+MBS\nen\n13.68\n15.3\n15.37\nzh-Hans\n23.7\n28.69\n26.28\nfr\n9.59\n10.94\n10.46\nes\n10.71\n12.49\n11.9\npt\n10.97\n13.05\n12.36\nar\n14.4\n17.35\n16.12\nvi\n10.16\n11.97\n11.15\nhi\n10.96\n13.72\n12.27\nid\n20.48\n25.36\n23.45\nbn\n17.27\n22.88\n19.83\nta\n16.55\n24.5\n19.53\nte\n18.1\n32.83\n22.67\nur\n13.26\n18.29\n15.4\nne\n27.22\n45.5\n33.97\nmr\n23.07\n43.32\n29.57\ngu\n21.52\n40.4\n27.83\nzh-Hant\n21.84\n26.85\n24.75\nsw\n34.35\n68.42\n46.19\nyo\n53.29\n98.46\n67.82\nig\n39.16\n71.56\n49.96\nwikitext2\n11.37\n12.6\n12.56\nAverage\n20.08\n31.17\n24.26\nTable 5: Perplexity for each language of BLOOM-7b1 model before and after quantization.\n16\nDataset\nDense\nWanda\nSparseGPT\nMBS +\nWanda\nMBS +\nSparseGPT\nen\n13.68\n34.30\n32.02\n35.08\n32.87\nzh-Hans\n23.70\n72.92\n101.79\n62.39\n59.28\nfr\n9.59\n22.89\n24.99\n21.66\n20.37\nes\n10.71\n25.41\n27.94\n23.98\n22.56\npt\n10.97\n27.47\n34.99\n25.82\n24.27\nar\n14.40\n80.09\n1.13E+14\n41.36\n47.57\nvi\n10.16\n43.26\n127.59\n30.75\n29.64\nhi\n10.96\n44.10\n1.20E+15\n29.17\n43.84\nid\n20.48\n109.13\n76038.83\n64.40\n79.85\nbn\n17.27\n102.78\n4.42E+23\n56.65\n121.11\nta\n16.55\n176.25\n1.71E+07\n64.40\n209.90\nte\n18.10\n355.98\n6.96E+05\n116.20\n355.54\nur\n13.26\n97.65\n3.62E+20\n44.60\n61.35\nne\n27.22\n555.21\n1.27E+17\n135.67\n200.16\nmr\n23.07\n503.51\n4.25E+12\n142.76\n619.28\ngu\n21.52\n327.11\n2.17E+13\n136.47\n197.21\nzh-Hant\n21.84\n137.00\n266.49\n61.30\n58.56\nsw\n34.35\n919.14\n6011.36\n357.63\n336.80\nyo\n53.29\n1.02E+03\n3557.88\n542.40\n450.00\nig\n39.16\n728.01\n1306.30\n370.35\n307.76\nwikitext2\n11.37\n30.58\n29.75\n31.09\n29.90\nAverage\n20.08\n257.99\n2.11E+22\n114.01\n157.51\nTable 6: Perplexity for each language and their respective increases when compared to the dense BLOOM-560m\nmodel after pruning. Evaluation performed on XL-Sum and WikiText2 datasets. From top to bottom, languages are\nranked in order from the most well-represented to the least represented.\n17\n0-shot task\nDense\nWanda\nSparseGPT\nWanda\nequal\nSparseGPT\nequal\nMBS+\nWanda\nMBS+\nSparseGPT\nxcopa\nid\n59.20%\n56.20%\n51.80%\n58.40%\n57.40%\n58.60%\n55.80%\nsw\n51.60%\n52.60%\n52.60%\n51.80%\n52.80%\n52.20%\n52.20%\nta\n55.80%\n57.20%\n54.00%\n56.00%\n56.00%\n54.80%\n56.20%\nvi\n61.00%\n55.40%\n51.60%\n57.60%\n56.20%\n56.00%\n55.60%\nzh\n58.60%\n52.80%\n53.00%\n53.40%\n53.60%\n53.80%\n55.00%\nAverage\n57.24%\n54.84%\n52.60%\n55.44%\n55.20%\n55.08%\n54.96%\nxstory_cloze\nar\n52.08%\n48.25%\n49.57%\n49.24%\n48.31%\n48.97%\n48.84%\nen\n61.22%\n57.78%\n59.23%\n56.92%\n57.97%\n57.51%\n59.70%\nes\n55.86%\n53.67%\n54.40%\n53.81%\n54.14%\n54.27%\n55.00%\nhi\n55.00%\n52.68%\n48.31%\n53.21%\n54.00%\n53.08%\n52.88%\nid\n55.53%\n53.14%\n48.31%\n53.14%\n52.42%\n53.34%\n52.22%\nsw\n49.83%\n49.11%\n48.78%\n49.24%\n49.24%\n49.44%\n48.51%\nte\n55.72%\n54.33%\n53.28%\n54.80%\n55.46%\n54.07%\n55.46%\nzh\n54.53%\n51.95%\n51.29%\n51.56%\n52.68%\n51.95%\n53.54%\nAverage\n54.97%\n52.61%\n51.65%\n52.74%\n53.03%\n52.83%\n53.27%\nxwinograd\nen\n65.89%\n61.98%\n62.58%\n62.28%\n63.44%\n62.02%\n62.92%\nfr\n60.24%\n56.63%\n51.81%\n56.63%\n56.63%\n59.04%\n57.83%\npt\n60.08%\n55.51%\n60.46%\n54.75%\n57.41%\n55.13%\n59.70%\nzh\n67.66%\n66.27%\n66.87%\n66.87%\n63.49%\n66.07%\n69.64%\nAverage\n63.47%\n60.10%\n60.43%\n60.13%\n60.24%\n60.57%\n62.52%\npawsx\nen\n52.00%\n49.90%\n48.60%\n49.15%\n49.85%\n47.60%\n50.85%\nes\n53.25%\n48.75%\n48.85%\n51.60%\n48.75%\n50.70%\n50.80%\nfr\n47.95%\n47.70%\n48.45%\n46.45%\n46.75%\n46.45%\n45.35%\nzh\n45.20%\n45.15%\n44.85%\n45.50%\n45.70%\n45.55%\n44.75%\nAverage\n49.60%\n47.88%\n47.69%\n48.18%\n47.76%\n47.58%\n47.94%\nxnli\nar\n33.35%\n33.47%\n33.55%\n33.59%\n33.57%\n33.41%\n33.67%\nen\n49.50%\n46.53%\n45.89%\n45.43%\n46.09%\n45.31%\n46.37%\nes\n45.23%\n45.71%\n41.72%\n43.11%\n42.87%\n44.45%\n42.95%\nfr\n45.29%\n45.51%\n42.20%\n45.07%\n45.27%\n45.71%\n43.75%\nhi\n40.84%\n38.64%\n33.35%\n37.90%\n36.45%\n38.78%\n34.77%\nsw\n33.17%\n33.65%\n33.51%\n33.45%\n33.21%\n33.63%\n33.29%\nur\n37.13%\n33.75%\n33.27%\n35.23%\n34.77%\n35.11%\n34.01%\nvi\n40.52%\n40.28%\n33.05%\n38.44%\n36.61%\n39.14%\n37.15%\nzh\n33.95%\n33.33%\n33.45%\n33.29%\n33.47%\n33.29%\n33.61%\nAverage\n39.89%\n38.99%\n36.67%\n38.39%\n38.04%\n38.76%\n37.73%\nTotal\nAverage\n51.24%\n49.26%\n47.95%\n49.26%\n49.15%\n49.31%\n49.41%\nTable 7: 0-shot tasks performance on each task of BLOOM-560m pruned model.\n18\nDataset\nDense\nen\n↑\nig\n↑\nta\n↑\nur\n↑\nen\n13.68\n15.67\n15%\n17.61\n29%\n17.20\n26%\n18.93\n38%\nzh-Hans\n23.70\n34.75\n47%\n33.97\n43%\n31.99\n35%\n31.70\n34%\nfr\n9.59\n13.16\n37%\n12.70\n32%\n13.94\n45%\n14.84\n55%\nes\n10.71\n13.82\n29%\n14.16\n32%\n15.83\n48%\n17.17\n60%\npt\n10.97\n14.58\n33%\n14.60\n33%\n15.78\n44%\n18.20\n66%\nar\n14.40\n29.19\n103%\n26.27\n82%\n22.82\n58%\n17.65\n23%\nvi\n10.16\n14.76\n45%\n12.74\n25%\n13.48\n33%\n14.62\n44%\nhi\n10.96\n18.26\n67%\n15.82\n44%\n13.18\n20%\n13.97\n28%\nid\n20.48\n29.37\n43%\n27.30\n33%\n27.48\n34%\n33.96\n66%\nbn\n17.27\n33.37\n93%\n26.71\n55%\n21.94\n27%\n30.11\n74%\nta\n16.55\n42.23\n155%\n27.14\n64%\n19.31\n17%\n24.44\n48%\nte\n18.10\n64.97\n259%\n39.77\n120%\n25.58\n41%\n37.68\n108%\nur\n13.26\n27.03\n104%\n25.07\n89%\n19.70\n49%\n15.38\n16%\nne\n27.22\n152.67\n461%\n64.74\n138%\n46.59\n71%\n46.60\n71%\nmr\n23.07\n176.78\n666%\n66.86\n190%\n45.71\n98%\n68.97\n199%\ngu\n21.52\n184.62\n758%\n48.30\n124%\n35.17\n63%\n37.96\n76%\nzh-Hant\n21.84\n113.34\n419%\n49.41\n126%\n56.20\n157%\n36.81\n69%\nsw\n34.35\n145.54\n324%\n58.74\n71%\n91.94\n168%\n125.09\n264%\nyo\n53.29\n128.12\n140%\n72.86\n37%\n127.05\n138%\n180.01\n238%\nig\n39.16\n90.41\n131%\n51.30\n31%\n91.28\n133%\n149.15\n281%\nwikitext2\n11.37\n16.15\n42%\n16.25\n43%\n14.14\n24%\n15.68\n38%\nAverage\n20.08\n64.70\n189.1%\n34.40\n68.7%\n36.49\n63.4%\n45.19\n90.2%\nTable 8: Monolingual pruning results using Wanda on BLOOM-7b1 with calibration data in en, ig, ta, and ur.\nPerplexity evaluated on XL-sum and wikitext2. Languages are ranked from the most well-represented to the least\nrepresented, from top to bottom.\n19\nDataset\nDense\nen\nig\nta\nur\nen\n13.68\n14.92\n9%\n16.28\n19%\n16.53\n21%\n16.80\n23%\nzh-Hans\n23.70\n35.56\n50%\n30.31\n28%\n31.45\n33%\n33.05\n39%\nfr\n9.59\n13.41\n40%\n11.42\n19%\n12.71\n32%\n12.75\n33%\nes\n10.71\n13.75\n28%\n12.82\n20%\n14.12\n32%\n14.58\n36%\npt\n10.97\n17.37\n58%\n13.49\n23%\n14.86\n35%\n15.38\n40%\nar\n14.40\n25.33\n76%\n18.54\n29%\n19.31\n34%\n16.87\n17%\nvi\n10.16\n15.00\n48%\n12.07\n19%\n13.26\n31%\n13.07\n29%\nhi\n10.96\n19.09\n74%\n14.44\n32%\n12.73\n16%\n12.46\n14%\nid\n20.48\n37.27\n82%\n25.56\n25%\n27.34\n33%\n26.84\n31%\nbn\n17.27\n40.50\n134%\n24.21\n40%\n21.07\n22%\n21.85\n26%\nta\n16.55\n44.10\n167%\n26.36\n59%\n17.84\n8%\n22.58\n36%\nte\n18.10\n69.20\n282%\n31.56\n74%\n24.12\n33%\n28.66\n58%\nur\n13.26\n30.56\n130%\n19.20\n45%\n17.32\n31%\n14.12\n7%\nne\n27.22\n148.00\n444%\n47.77\n76%\n42.62\n57%\n40.12\n47%\nmr\n23.07\n144.65\n527%\n43.74\n90%\n37.81\n64%\n49.41\n114%\ngu\n21.52\n118.48\n450%\n39.39\n83%\n34.87\n62%\n32.87\n53%\nzh-Hant\n21.84\n102.26\n368%\n30.51\n40%\n50.00\n129%\n43.85\n101%\nsw\n34.35\n135.84\n295%\n46.74\n36%\n72.81\n112%\n72.94\n112%\nyo\n53.29\n126.54\n137%\n61.08\n15%\n105.97\n99%\n113.09\n112%\nig\n39.16\n90.89\n132%\n41.37\n6%\n74.44\n90%\n86.14\n120%\nwikitext2\n11.37\n13.91\n22%\n13.56\n19%\n13.66\n20%\n13.95\n23%\nAverage\n20.08\n59.84\n169.3%\n27.64\n37.8%\n32.14\n47.3%\n33.40\n51.1%\nTable 9: Monolingual pruning results using SparseGPT on BLOOM-7b1 with calibration data in en, ig, ta, and ur.\nPerplexity evaluated on XL-sum and wikitext2. Languages are ranked from the most well-represented to the least\nrepresented, from top to bottom.\n20\nDataset\nDense\nen\nig\nte\nid\nen\n13.68\n34.30\n151%\n49.17\n259%\n53.12\n288%\n38.50\n181%\nzh-Hans\n23.70\n72.92\n208%\n87.98\n271%\n126.56\n434%\n79.06\n234%\nfr\n9.59\n22.89\n139%\n32.46\n238%\n41.75\n335%\n24.91\n160%\nes\n10.71\n25.41\n137%\n38.29\n258%\n47.38\n343%\n28.18\n163%\npt\n10.97\n27.47\n150%\n42.04\n283%\n57.68\n426%\n30.47\n178%\nar\n14.40\n80.09\n456%\n79.26\n451%\n66.66\n363%\n55.03\n282%\nvi\n10.16\n43.26\n326%\n39.63\n290%\n150.26\n1379%\n38.05\n275%\nhi\n10.96\n44.10\n303%\n50.54\n361%\n32.37\n195%\n36.25\n231%\nid\n20.48\n109.13\n433%\n108.26\n429%\n952.66\n4551%\n60.77\n197%\nbn\n17.27\n102.78\n495%\n141.25\n718%\n68.01\n294%\n83.81\n385%\nta\n16.55\n176.25\n965%\n147.99\n794%\n60.52\n266%\n163.29\n887%\nte\n18.10\n355.98\n1866%\n260.31\n1338%\n95.77\n429%\n445.67\n2362%\nur\n13.26\n97.65\n636%\n143.11\n979%\n49.14\n271%\n63.39\n378%\nne\n27.22\n555.21\n1940%\n320.61\n1078%\n143.19\n426%\n237.41\n772%\nmr\n23.07\n503.51\n2083%\n290.52\n1159%\n140.02\n507%\n220.62\n856%\ngu\n21.52\n327.11\n1420%\n215.13\n900%\n124.55\n479%\n286.66\n1232%\nzh-Hant\n21.84\n137.00\n527%\n156.15\n615%\n227.56\n942%\n167.85\n668%\nsw\n34.35\n919.14\n2576%\n419.52\n1121%\n897.07\n2511%\n465.66\n1256%\nyo\n53.29\n1024.94\n1823%\n529.35\n893%\n711.20\n1235%\n573.17\n976%\nig\n39.16\n728.01\n1759%\n285.28\n629%\n773.60\n1876%\n409.11\n945%\nwikitext2\n11.37\n30.58\n169%\n40.70\n258%\n46.07\n305%\n33.40\n194%\nAverage\n20.08\n257.99\n883.9%\n165.60\n634.4%\n231.67\n850.2%\n168.63\n610.0%\nTable 10: Monolingual pruning results of Wanda on BLOOM-560m.\nDataset\nDense\nen\nig\nte\nid\nen\n14\n32\n43\n48\n40\nzh-Hans\n24\n102\n128\n108\n110\nzh-Hant\n22\n266\n162\n227\n279\nfr\n10\n25\n28\n39\n28\nes\n11\n28\n34\n48\n32\npt\n11\n35\n37\n54\n35\nar\n14\n1.13E+14\n8.10E+07\n247\n3.52E+07\nvi\n10\n128\n70\n948\n78\nhi\n11\n1.20E+15\n3.93E+10\n128\n2.98E+09\nid\n20\n76039\n7049\n11397\n62\nbn\n17\n4.42E+23\n2.16E+10\n1788\n6.12E+15\nta\n17\n17114940\n361068\n85\n1.80E+08\nte\n18\n696319\n180476\n75\n2.40E+07\nur\n13\n3.62E+20\n55283\n807\n1.55E+07\nne\n27\n1.27E+17\n1.70E+09\n636\n6.43E+10\nmr\n23\n4.25E+12\n2.13E+08\n1250\n3.76E+11\ngu\n22\n2.17E+13\n3051\n252\n4.96E+06\nsw\n34\n6011\n467\n1363\n1502\nyo\n53\n3558\n342\n882\n1463\nig\n39\n1306\n205\n990\n753\nwikitext2\n11\n30\n36\n41\n34\nAverage\n20\n2.11E+22\n2.99E+09\n1020\n2.92E+14\nTable 11: Monolingual pruning results of SparseGPT on BLOOM-560m.\n21\nen\nzh-Hans\nzh-Hant\nfr\nes\npt\nar\nvi\nhi\nid\nbn\nta\nte\nur\nne\nmr\ngu\nsw\nyo\nig\nen\n0\n10\n11\n5\n6\n7\n9\n8\n10\n6\n9\n8\n12\n20\n14\n9\n12\n10\n10\n10\nzh-Hans\n10\n0\n1\n13\n15\n16\n8\n12\n3\n12\n6\n6\n5\n11\n8\n5\n6\n16\n17\n18\nzh-Hant\n11\n1\n0\n15\n17\n18\n8\n14\n4\n13\n6\n6\n5\n10\n7\n6\n6\n17\n19\n19\nfr\n5\n13\n15\n0\n2\n3\n11\n5\n12\n4\n10\n10\n14\n22\n16\n11\n13\n6\n6\n7\nes\n6\n15\n17\n2\n0\n1\n12\n7\n15\n6\n13\n12\n17\n25\n18\n13\n15\n7\n5\n6\npt\n7\n16\n18\n3\n1\n0\n12\n7\n15\n5\n13\n13\n17\n25\n18\n13\n16\n5\n4\n5\nar\n9\n8\n8\n11\n12\n12\n0\n8\n6\n8\n6\n5\n8\n14\n8\n4\n8\n11\n12\n12\nvi\n8\n12\n14\n5\n7\n7\n8\n0\n10\n2\n8\n8\n12\n19\n12\n8\n10\n4\n6\n6\nhi\n10\n3\n4\n12\n15\n15\n6\n10\n0\n10\n3\n3\n3\n10\n5\n3\n4\n14\n16\n16\nid\n6\n12\n13\n4\n6\n5\n8\n2\n10\n0\n8\n7\n12\n19\n13\n8\n10\n4\n6\n6\nbn\n9\n6\n6\n10\n13\n13\n6\n8\n3\n8\n0\n2\n4\n12\n6\n3\n3\n12\n14\n14\nta\n8\n6\n6\n10\n12\n13\n5\n8\n3\n7\n2\n0\n4\n12\n6\n2\n4\n11\n13\n13\nte\n12\n5\n5\n14\n17\n17\n8\n12\n3\n12\n4\n4\n0\n8\n4\n5\n2\n16\n18\n18\nur\n20\n11\n10\n22\n25\n25\n14\n19\n10\n19\n12\n12\n8\n0\n7\n12\n10\n23\n25\n25\nne\n14\n8\n7\n16\n18\n18\n8\n12\n5\n13\n6\n6\n4\n7\n0\n5\n5\n16\n18\n18\nmr\n9\n5\n6\n11\n13\n13\n4\n8\n3\n8\n3\n2\n5\n12\n5\n0\n4\n12\n13\n13\ngu\n12\n6\n6\n13\n15\n16\n8\n10\n4\n10\n3\n4\n2\n10\n5\n4\n0\n14\n16\n17\nsw\n10\n16\n17\n6\n7\n5\n11\n4\n14\n4\n12\n11\n16\n23\n16\n12\n14\n0\n3\n3\nyo\n10\n17\n19\n6\n5\n4\n12\n6\n16\n6\n14\n13\n18\n25\n18\n13\n16\n3\n0\n1\nig\n10\n18\n19\n7\n6\n5\n12\n6\n16\n6\n14\n13\n18\n25\n18\n13\n17\n3\n1\n0\nAverage\n9.3\n9.4\n10.1\n9.25\n10.6\n10.65\n8.5\n8.3\n8.1\n7.95\n7.6\n7.25\n9.2\n15.45\n10.2\n7.45\n8.75\n10.2\n11.1\n11.35\nTable 12: Original distance matrix generated from the BLOOM-7b1 model.\nen\nzh-Hans\nzh-Hant\nfr\nes\npt\nar\nvi\nhi\nid\nbn\nta\nte\nur\nne\nmr\ngu\nsw\nyo\nig\nen\n0\n8\n9\n2\n1\n4\n54\n14\n28\n12\n47\n48\n60\n52\n38\n31\n55\n15\n13\n13\nzh-Hans\n8\n0\n1\n7\n8\n5\n53\n7\n24\n6\n45\n46\n58\n49\n34\n28\n53\n7\n5\n5\nzh-Hant\n9\n1\n0\n8\n9\n6\n53\n6\n24\n5\n45\n46\n58\n49\n34\n28\n53\n6\n5\n5\nfr\n2\n7\n8\n0\n2\n4\n53\n12\n26\n10\n46\n47\n59\n51\n36\n30\n53\n13\n12\n12\nes\n1\n8\n9\n2\n0\n3\n54\n13\n27\n11\n46\n47\n59\n51\n37\n31\n54\n14\n12\n13\npt\n4\n5\n6\n4\n3\n0\n53\n11\n26\n9\n45\n46\n59\n50\n36\n29\n53\n11\n9\n9\nar\n54\n53\n53\n53\n54\n53\n0\n50\n29\n49\n12\n7\n9\n7\n19\n30\n5\n51\n52\n52\nvi\n14\n7\n6\n12\n13\n11\n50\n0\n22\n5\n42\n43\n55\n46\n31\n26\n50\n5\n6\n6\nhi\n28\n24\n24\n26\n27\n26\n29\n22\n0\n20\n23\n22\n34\n25\n11\n9\n29\n23\n23\n24\nid\n12\n6\n5\n10\n11\n9\n49\n5\n20\n0\n41\n42\n54\n45\n30\n23\n49\n6\n6\n5\nbn\n47\n45\n45\n46\n46\n45\n12\n42\n23\n41\n0\n10\n20\n9\n14\n26\n9\n43\n43\n44\nta\n48\n46\n46\n47\n47\n46\n7\n43\n22\n42\n10\n0\n13\n6\n13\n22\n9\n44\n45\n45\nte\n60\n58\n58\n59\n59\n59\n9\n55\n34\n54\n20\n13\n0\n13\n25\n32\n12\n56\n57\n57\nur\n52\n49\n49\n51\n51\n50\n7\n46\n25\n45\n9\n6\n13\n0\n15\n26\n6\n47\n48\n48\nne\n38\n34\n34\n36\n37\n36\n19\n31\n11\n30\n14\n13\n25\n15\n0\n13\n20\n32\n33\n33\nmr\n31\n28\n28\n30\n31\n29\n30\n26\n9\n23\n26\n22\n32\n26\n13\n0\n31\n26\n26\n26\ngu\n55\n53\n53\n53\n54\n53\n5\n50\n29\n49\n9\n9\n12\n6\n20\n31\n0\n51\n51\n52\nsw\n15\n7\n6\n13\n14\n11\n51\n5\n23\n6\n43\n44\n56\n47\n32\n26\n51\n0\n5\n4\nyo\n13\n5\n5\n12\n12\n9\n52\n6\n23\n6\n43\n45\n57\n48\n33\n26\n51\n5\n0\n3\nig\n13\n5\n5\n12\n13\n9\n52\n6\n24\n5\n44\n45\n57\n48\n33\n26\n52\n4\n3\n0\nAverage\n25.2\n22.45\n22.5\n24.15\n24.6\n23.4\n34.6\n22.5\n22.45\n21.4\n30.5\n30.05\n39.5\n32.15\n25.2\n24.65\n34.75\n22.95\n22.7\n22.8\nTable 13: Original distance matrix generated from the BLOOM-560m model.\n22\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2024-04-06",
  "updated": "2024-04-06"
}