{
  "id": "http://arxiv.org/abs/2404.06526v1",
  "title": "Onboard Processing of Hyperspectral Imagery: Deep Learning Advancements, Methodologies, Challenges, and Emerging Trends",
  "authors": [
    "Nafiseh Ghasemi",
    "Jon Alvarez Justo",
    "Marco Celesti",
    "Laurent Despoisse",
    "Jens Nieke"
  ],
  "abstract": "Recent advancements in deep learning techniques have spurred considerable\ninterest in their application to hyperspectral imagery processing. This paper\nprovides a comprehensive review of the latest developments in this field,\nfocusing on methodologies, challenges, and emerging trends. Deep learning\narchitectures such as Convolutional Neural Networks (CNNs), Autoencoders, Deep\nBelief Networks (DBNs), Generative Adversarial Networks (GANs), and Recurrent\nNeural Networks (RNNs) are examined for their suitability in processing\nhyperspectral data. Key challenges, including limited training data and\ncomputational constraints, are identified, along with strategies such as data\naugmentation and noise reduction using GANs. The paper discusses the efficacy\nof different network architectures, highlighting the advantages of lightweight\nCNN models and 1D CNNs for onboard processing. Moreover, the potential of\nhardware accelerators, particularly Field Programmable Gate Arrays (FPGAs), for\nenhancing processing efficiency is explored. The review concludes with insights\ninto ongoing research trends, including the integration of deep learning\ntechniques into Earth observation missions such as the CHIME mission, and\nemphasizes the need for further exploration and refinement of deep learning\nmethodologies to address the evolving demands of hyperspectral image\nprocessing.",
  "text": "Onboard Processing of Hyperspectral Imagery: Deep Learning Advancements,\nMethodologies, Challenges, and Emerging Trends\nNafiseh Ghasemi ∗a, Jon Alvarez Justo a,c, Marco Celesti ∗∗b, Laurent Despoisse a, Jens Nieke a\naEuropean Space Agency, European Space Research and Technology Centre (ESA-ESTEC), The Netherlands.\nbHE Space for ESA - European Space Agency, European Space Research and Technology Centre (ESA-ESTEC), The Netherlands.\ncDepartment of Engineering Cybernetics, Norwegian University of Science and Technology, , Norway.\n∗nafiseh.ghasemi@esa.com, ∗∗marco.celesti@ext.esa.int\nAbstract\nRecent advancements in deep learning techniques have spurred considerable interest in their application to hyper-\nspectral imagery processing. This paper provides a comprehensive review of the latest developments in this field,\nfocusing on methodologies, challenges, and emerging trends. Deep learning architectures such as Convolutional Neu-\nral Networks (CNNs), Autoencoders, Deep Belief Networks (DBNs), Generative Adversarial Networks (GANs), and\nRecurrent Neural Networks (RNNs) are examined for their suitability in processing hyperspectral data. Key chal-\nlenges, including limited training data and computational constraints, are identified, along with strategies such as data\naugmentation and noise reduction using GANs. The paper discusses the efficacy of different network architectures,\nhighlighting the advantages of lightweight CNN models and 1D CNNs for onboard processing. Moreover, the po-\ntential of hardware accelerators, particularly Field Programmable Gate Arrays (FPGAs), for enhancing processing\nefficiency is explored. The review concludes with insights into ongoing research trends, including the integration of\ndeep learning techniques into Earth observation missions such as the CHIME mission, and emphasizes the need for\nfurther exploration and refinement of deep learning methodologies to address the evolving demands of hyperspectral\nimage processing.\nKeywords: Hyperspectral, Deep Learning, Neural Networks, Image Processing, Classification, Segmentation,\nAccelerators, Hardware, CHIME Mission\n1. Introduction\nImaging spectroscopy within the visible to short-\nwave infrared (VSWIR) range of the electromagnetic\nspectrum is a powerful Earth observation tool that\nevolved significantly in the last forty years [1]. In the\ncontext of remote sensing, a broad scope of applica-\ntions and research fields can benefit from the unique ca-\npacity of imaging spectroscopy which accurately mea-\nsures the distinct spectral signatures characterizing the\nvariety of targets on the Earth’s surface. To this ex-\ntent, spectroscopy significantly enhances various mon-\nitoring applications in e.g., agriculture, chemical leak\ndetection [2], security [3] and oil spills. Furthermore,\nspectroscopic observations allow advancements in ge-\nological classification [4, 5], ocean color studies [6],\nwhile assisting the assessment of natural hazards before\nand after their occurrence [7]. Nowadays, hyperspec-\ntral (HS) sensors have the capability to capture hundreds\nof spectral bands spanning across a wide spectrum and\nwith high spectral resolution, hence resulting in highly-\ndimensional data cubes. Each pixel contained within\nthe data cubes consists of a spectral signature, similar\nto a unique fingerprint [8], revealing how light inter-\nacts with the imaged target of interest. This interaction\nis presented through variations in reflections and ab-\nsorption patterns across the different wavelengths. The\npatterns within each image pixel unveils valuable fea-\nture information of the Earth’s surface and its quan-\ntitative properties, which may be analysed with tech-\nniques such as image classification or quantitative re-\ntrieval of geophysical properties through e.g., regres-\nsion techniques.\nThe significance of imaging spec-\ntroscopy is emphasized by the numerous satellite mis-\nsions integrating, for instance, HS sensors in their pay-\nloads.\nA variety of HS missions are nowadays pro-\nducing a nearly uninterrupted stream of data. As an\nexample, HS technology is incorporated not only in\nairborne systems such as NASA/AVIRIS-NG [9], but\nPreprint available, venue TBD.\nApril 11, 2024\narXiv:2404.06526v1  [eess.IV]  9 Apr 2024\nalso in spaceborne missions such as DLR/EnMAP [10],\nASI/PRISMA [11], NASA/EMIT [12], ESA/CHIME\n[13, 14] and NASA/SBG [15] (both missions planned\nfor launch in 2028). In these missions, most of their\non-board HS sensors capture data with approximately\nspectral and spatial resolutions of, respectively, 10 nm\nand 30 m. When combined with repeated acquisitions\nin relatively large swaths (30 km to 150 km), the data\nproduced requires large storage and computational pro-\ncessing power.\nTherefore, considering the rising de-\nmands for this Earth observation data, current state-of-\nthe-art efforts focus on the development of automated\ndata analysis solutions to process, already in orbit, the\nvast amounts of data being generated. To accomplish\nthis automated feature extraction is of key relevance.\nAfter preparing and training the extraction models on\nthe ground segment, the models are uplinked to the\nsatellite for near real-time inference on e.g., commercial\noff-the-shelf hardware processing units embedded in the\nspaceborne payloads. The process has been schemati-\ncally presented in Figure 1. The success of the deploy-\nment of deep learning algorithms depends on several\naspects. For instance, the quality of the data used for\ntraining the models, the real acquired data by the satel-\nlite, and the limited on-board memory as well as power\nsupply constraints, among other factors.\nIt is a current trend to develop algorithms that can\nprocess data near real-time, and extract only the mis-\nsion required information to optimize storage and re-\nduce processing costs [16], while saving downlink re-\nsources.\nIn earlier studies, imaging spectroscopy analysis was\nperformed using machine learning (ML) techniques\nsuch as K-nearest neighbor [17], support vector ma-\nchines (SVMs) [18], and Gaussian unmixing models\n[19]. Moreover, sparse signal representation methods\nhave been used to classify noisy data with the help\nof a learned dictionary [20]. However, in many cases\ntraditional ML techniques that require manual feature\nextraction are not suitable alternatives, and hence the\nemerging deep learning (DL) models have found their\nplace within the HS community [21, 22], due to their\npromising potential to automate the learning of com-\nplex non-linear data patterns. DL techniques can de-\ntect features that are often not possible to be inferred by\nhumans analyses [23], while allowing solving diverse\ntasks for on-board data processing from data compres-\nsion [20, 24] to target detection [25] and feature extrac-\ntion [26]. For instance, DL techniques have made sig-\nnificant advancements in data compression as demon-\nstrated in [27], where for the first time the literature out-\nperforms traditional iterative reconstruction methods in\nthe field of Compressive Sensing (CS) for HS imagery\n[28]. Additionally, DL techniques are prevalent in other\ndata processing tasks such as pixel-wise classification,\nwhich is frequently referred as image classification in\nthe HS field. Conversely, in computer vision, the term\ncommonly used for pixel-level classification is image\nsegmentation. In the case of HS imagery, both terms\ncan be used interchangeably [25, 29]. Pixel-wise clas-\nsification for HS imagery results into non-overlapping\nsegments also called super-pixels [30] or homogeneous\nimage regions.\nThe objective of this paper is to evaluate various DL\ntechniques for on-board processing in terms of reliabil-\nity, ability to handle noisy data, and network architec-\nture. These factors, among others, play a crucial role\nin the implementation of DL for on-board applications.\nThis is of relevance for upcoming satellite missions such\nas CHIME and SBG. Additionally, the study assesses\nthe capability of the networks to be trained with limited\ntraining samples. The outcome of this analysis will in-\nform the decision on which network architectures and\nconfigurations may be optimal for on-board imaging\nspectroscopy segmentation.\n2. Deep Learning for Segmentation in Imaging\nSpectroscopy\nWe present first Convolutional Neural Networks\n(CNNs) with different data processing approaches from\nspectral or spatial to a combination of spectral and\nspatial.\nFurthermore, other significant architectures\nwe consider are Autoencoders, Deep Belief Networks\n(DBNs), Generative Adversarial Networks (GANs), and\nRecurrent Neural Networks (RNNs). These architec-\ntures are flexible and adaptable for on-board processing\nof imaging spectroscopy data. Finally, we discuss about\nthe architectures’ challenges as well as new trends to\ntackle them.\n2.1. Spectral and Spatial Dimensions in Imaging Spec-\ntroscopy Processing\nHS data can be processed based on any of its differ-\nent dimensions, either spectral and/or spatial. A dia-\ngram showing spectral and spatial dimensions on the\nhyperspectral data cube is shown in Figure 2. For in-\nstance, in earlier studies for DL methods, a prevalent\napproach involved pixel-wise processing to extract only\nthe spectral signature from each individual pixel and\nsubsequently comparing it to a known spectral signa-\nture of a target object, and hence prior knowledge about\nthe reference target was required (an example is given\n2\nFigure 1: On-board data processing diagram. The AI model will be trained on ground and network structure and hyper-parameters are transferred\nto the satellite.\nin [31]). However, it is a common practice to process\nmultiple neighboring spatial pixels, rather than just one\npixel, with the goal of incorporating additional spatial\ninformation to infer each pixel classification. In these\nscenarios involving the processing of both spectral and\nspatial domains, it is often more efficient to concentrate\non data sub-volumes, represented as 3D patches, rather\nthan handling entire image data cubes, as this can be\noften computationally more expensive. In previous re-\nsearch, findings from the HYPSO-1 mission [32] sug-\ngest that in certain scenarios, the segmentation of in-\ndividual spectral signatures without accounting for the\nneighboring spatial context can yield superior perfor-\nmance compared to the joint processing of both spectral\nand spatial domains. However, in any case, in imaging\nspectroscopy, it is common to reduce the correlated in-\nformation in the spectral domain and hence remove re-\ndundant spectral channels. To achieve this, dimension-\nality reduction methods such as PCA [33], Kernel-ICA\n[34, 35], and autoencoders [36] can be employed.\n2.2. Convolutional Neural Networks\nArtificial Neural Networks (ANNs) are inspired by\nbiological neural systems and consist of: an input layer,\none or more hidden layers and an output layer [37].\nWhen neural networks comprise more than two hid-\nden layers, we commonly refer to them as deep neural\nnetworks (DNNs) [38]. A subtype of DNNs is CNNs.\nThese convolutional networks have been introduced for\nthe purpose of extracting information from images in\ncomputer vision [39]. This type of network has found\nextensive applications in different image analyses tasks\n[40]. In CNNs, the input image is structured accord-\ning to the network’s architecture, typically arranged in\nthree dimensions: width (w), height (h), and depth (d).\nIn the context of HS imagery, the input depth (d) cor-\nresponds to the number of bands per image pixel. On\nthe other side, the input width (w) and height (h) cor-\nrespond to the spatial dimensions. Within each layer in\nthe network, neurons are connected to a selected subset\nof neurons from the previous layer. This arrangement\nserves to decrease the number of weights that must be\ntrained [40].\nCNNs have also been combined with ML methods\nsuch as SVMs to extract features and increase robust-\nness against overfitting, as demonstrated in [41].\nIn\nthis study, the approach involves assembling a spectral-\nspatial multi-feature cube by combining the data from\na target pixel and its nehigboring spectral information,\nall without the need for additional modifications to the\nnetwork’s architecture.\nThis combined approach has\nbeen employed for land cover classification.\nIn an-\nother study presented in [42], a 2-channel deep CNN\nwas employed for land cover classification by combin-\ning spectral-spatial features. Additionally, a hierarchi-\ncal framework was adopted for this task in [43]. Sim-\nilarly, in [44], a method is introduced to extract spa-\ntial and spectral features from imaging spectroscopy\n3\nFigure 2: A visualization of spatial and spectral dimensions on a) hyperspectral data cube and b) a schematic representation of hyperspectral pixel.\nand lidar data using a CNN. Furthermore, a pixel-wise\nclassification approach was applied in [45], using a 2-\nchannel CNN and multi-source feature extraction. To\ncontinue, the research in [46] introduces a framework\nfor the feature classification of imaging spectroscopy\ndata. Concretely, it employs a Fully Convolutional Net-\nwork (FCN) to predict spatial features by analyzing\nmultiscale local information. These spatial features are\nthen combined with spectral features using a weighted\nmethod. Subsequently, this approach utilizes SVMs for\nthe classification task.\n2.2.1. Spectral Dimensional CNN (1D-CNN)\nAlthough 2D CNNs are a common imaging analy-\nsis method employed in the field of computer vision,\n1D CNNs can also be used for pixel-level classification\nin imaging spectroscopy. These networks often apply\nthe 1D convolution over the spectral domain. However,\nthey are susceptible to noise, thus posing challenges to\nuse them in remote sensing [47]. A possible solution\nsuggested in the literature is to utilize averaged spec-\ntrum from a group of neighboring pixels, a method par-\nticularly suited for small-scale analyses such as crop\nsegmentation [48].\nAnother approach is to perform\nPCA before applying CNNs, offering another strategy\nto mitigate noise-related issues. A method used to re-\nduce dimensionality of HS imagery is Minimum Noise\nFraction (MNF) [49]. The MNF transform involves two\nsequential operations as presented next. First, it esti-\nmates the noise in the data using a correlation matrix\nand subsequently scales it based on variance. This op-\neration decorrelates and reduces noise within the data\nwithout considering inter-band noise.\nNext, the sec-\nond operation in the MNF transformation takes into ac-\ncount the original correlations and creates components\nthat incorporate weighted information for the variance\nacross all bands within the original dataset. MNF has\nfound application in tasks such as denoising and im-\nage classification [50–52]. Typically, the MNF algo-\nrithm requires more computational resources compared\nto other commonly employed HS processing techniques\nsuch as PCA or Linear Discriminant Analysis (LDA)\n[53]. Therefore, MNF may not be prioritized for on-\nboard image processing tasks. An alternative solution,\nas detailed in [54], involves the utilization of a multi-\nscale CNN applied to a pyramid of data encompassing\nspatial features at multiple scales. In case of limited\ntraining observations, a different strategy proposed in\n[55] consists of performing band selection before CNN\ninference.\n2.2.2. Spectral-Spatial Dimensional CNN (2D-CNN)\nIn many cases, utilizing both spectral and spatial fea-\ntures typically leads to better results in imaging spec-\ntroscopy processing.\nIn [56], a dual-stream channel\nCNN was employed.\nThis network extracts spectral\nfeatures using the approach in [47], while spatial fea-\ntures are derived from the method in [57]. These ex-\ntracted features are subsequently integrated using a soft-\nmax classifier.\nIn [58], a combination of the l2 norm and sparse con-\nstraints, incorporating a similar combination of spectral-\nspatial features, is employed. In other studies, AlexNet\n[59] has been employed for spatial-spectral analyses, in-\nvolving architectures like Densenet and VGG-16 [46,\n60]. Furthermore, other works [61] adopted a few-shot\n4\nlearning approach proposed in [62]. This approach has\nbeen used to learn a metric space, promoting the prox-\nimity of samples belonging to the same class. This was\ndone in order to address the challenge posed by a lim-\nited number of training smaples.\nAn alternative strategy to improve accuracy while\nhaving a shortage of training data has been proposed in\n[63]. In this approach, the redundant information within\nthe hidden layers is explored to identify connections\nand improve the training process.\nAdditional exam-\nples of combining spatial-spectral features to improve\nthe learning process can be found in various works [64–\n66]. These studies have used a variety of methods based\non super-pixel reconstruction of different features, aim-\ning to improve the accuracy of segmentation. In ad-\ndition to these methods, other works [67] have intro-\nduced a technique known as sensor-based feature learn-\ning consisting of the reconstruction of five layers of\nspectral-spatial features tailored to the specifications of\nthe sensor used, contributing to the improvement of the\nlearning process. An additional improvement to sensor-\nbased training was given in [68], which uses a novel\narchitecture that actively transforms input features into\nmeaningful response maps for classification tasks. All\nthe aforementioned studies have used complex multi-\nstep procedures, rendering them not suitable for near-\nreal-time processing in imaging spectroscopy. However,\nthese studies demonstrate that the adoption of multi-\nscale and multi-feature approaches has been substanti-\nated as a means of achieving improved performance, as\nemphasized in the mentioned studies.\nThere have been studies to compare both 1D vs 2D\nCNNs for on-board deployment. In one study 1D and\n2D have been test on HYPSO-1 cube sat launched\nby Norwegian University of Science and Technology\n(NTNU) [32]. They evaluate classification performance\nand model parameters. An example of the results is\nshown in Figure 3. The classification task involves iden-\ntifying oceanic, terrestrial, and cloud formations. The\nresults have shown advantage of using 1D-CNN for on-\nboard processing.\nThe 1D-CNN architectures under-\nwent additional testing in [69] using data from NASA’s\nEO-1 Hyperion and Ziyuan-1 satellites, confirming the\npromising results of these architectures.\nAnother example is the Phi-Sat-1 mission, an initia-\ntive under the European Space Agency (ESA) repre-\nsents a pioneering effort in deploying artificial intelli-\ngence (AI) directly onboard, leveraging a dedicated chip\nfor deep convolutional neural network (CNN) inference\n[70]. 1D and 2D-CNN have been tested on-board for\ncloud detection. It demonstrates the feasibility of on-\nboard CNN for cloud detection, achieving high accu-\nracy rates more than 90% by 1D CNN. An example of\nthe results of Phi-Sat-1 is shown in Figure 4.\nIt shows Following the success of the Phi-Sat-1 mis-\nsion. Phi-Sat 2 mission has been proposed aiming to\nintegrate cutting-edge AI technology into Earth Obser-\nvation missions [71]. Phi-Sat-2 will demonstrate the\ndeployment of AI applications onboard, enabling func-\ntionalities. This initiative represents a pivotal step in ad-\nvancing innovative techniques to meet user-driven sci-\nence and application needs.\n2.3. Autoencoders\nTo deal with the issue of limited training data for\nprocessing imaging spectroscopy, various approaches\nbased on autoencoders with different variations have\nbeen tested. For the first time [72], PCA was applied\nin the spectral dimension while auto-encoders were ap-\nplied at the same time to the other two spatial dimen-\nsions, resulting in improved feature extraction for clas-\nsification. Subsequent research in [73] and [74] utilized\nstacked autoencoders in combination with PCA to flat-\nten the spectral dimension, followed by the utilization\nof SVM and Multi-Layer Perceptron (MLP) to perform\nclassification. Furthermore, [75] focused on optimiz-\ning stacked autoencoders specifically for anomaly de-\ntection within imaging spectroscopy. A combination of\nauto-encoders and CNNs have also been tested in multi-\nscale approaches for feature extraction [76]. Another\nimportant advantage of using stacked autoencoders is\ntheir effectiveness in handling noisy input data. As il-\nlustrated in [77], a stacked autoencoder was utilized to\ngenerate feature maps from noisy input data, followed\nby the application of super-pixel segmentation and ma-\njority voting for further processing. In a different study,\n[78] used a pre-trained network consisting of stacked\nencoders combined with logistic regression to perform\nsupervised classification on noisy input data. Further-\nmore, a framework based on stacked autoencoders was\ninitially proposed in [79] to perform also unsupervised\nclassification on noisy input data. This framework was\nlater improved as an end-to-end classification pipeline\ntailored for imaging spectroscopy in [80].\n2.4. Deep Belief Networks, Generative Adversarial\nNetworks, and Recurrent Neural Networks\nDeep Belief Networks (DBNs) have the capability of\nreducing dimensions, and hence they are a suited al-\nternative for feature extraction tasks. In [81], a DBN\nwas employed combined with logistic regression to per-\nform feature extraction. Furthermore, a combination of\none- and two-layer DBN was combined with PCA for\n5\nFigure 3: 1D-CNN results on a sample dataset from HYPSO-1 cubesat captured over Spain [32].\nFigure 4: Captured image over Etna Volcano, Italy by Phi-Sat-1, a)original image, b)neural network detection results from on-board processor unit\n[70].\n6\nfeature extraction. To perform near real-time anomaly\ndetection, DBNs have been investigated and tested and\nhave shown to deliver promising results in the extrac-\ntion of local objects, as demonstrated in [82].\nAn-\nother approach proposed by [83] consisted of combin-\ning DBNs with wavelet transforms. Additionally, other\nworks [84, 85] utilized DBNs for unsupervised classi-\nfication. In the later study, an end-to-end classification\nframework based on DBNs and the spectral angle dis-\ntance metric was proposed.\nAs\nregards\nGenerative\nAdversarial\nNetworks\n(GANs), these networks employ a pair of competing\nneural networks, one as a generator and the other\nas a discriminator [86].\nThese networks have found\nusefulness to perform classification when dealing with\nlimited training data [87]. In similar scenarios, GANs\nhave been employed to perform the classification\nof imaging spectroscopy utilizing the discriminator\nnetwork for this purpose [88–90].\nFinally, Recurrent Neural Networks (RNNs) are\nmainly used for the analysis of time series data. How-\never, in specific applications, they can be adapted to\nprocess video series as sequences of data (with each\nspectral band representing a sequence) , enabling RNNs\nto identify temporal similarities within these frames\n[91, 92]. A novel approach was proposed in [93], where\na combination of RNNs was utilized to explore the spec-\ntral domain, while Long Short-Term Memory (LSTM)\nnetworks were used to explore spatial features. Further-\nmore, RNNs have been used to process mixed pixels\nin the spectral dimension, particularly in situations af-\nfected by noise [94].\n2.5. Unsupervised and Semi-Supervised Approaches\nDue to the common challenge of dealing with a lim-\nited pool of available training samples, semi-supervised\nand unsupervised approaches are getting more popular\nin the field of imaging spectroscopy.\nIllustrative ex-\namples can be found in studies such as [95] and [96],\nwhere semi-supervised techniques and layer-wise clas-\nsification have been adopted to process extensive sets\nof imaging spectroscopy data. An additional example\nof pixel-wise classification can be found in the research\nconducted in [97], which used an unsupervised method\nwith CNNs. Initially, the model was trained using im-\nprecise training samples, and subsequently, classifica-\ntion accuracy was improved by incorporating a small set\nof accurately labeled training samples. In [98], the chal-\nlenge of limited training samples was addressed by em-\nploying a convolution-deconvolution network for unsu-\npervised spectral-spatial feature learning. Particularly,\nthe convolution network was applied to reduce dimen-\nsionality, while the deconvolution stage was utilized to\nreconstruct input data. Another strategy to address the\nlack of training samples involves improving the train-\ning procedure as explained in [99]. In this work, un-\nlabeled data is used in combination with a few labeled\nsamples, and an RNN is used for the classification of\nimaging spectroscopy data. Finally, another method that\nwas tested involved the utilization of ResNet to learn\nspectral-spatial features from unlabeled data, showing\npromising results in [100].\n2.6. Challenges and Emerging Trends in Imaging Spec-\ntroscopy Processing\n2.6.1. The Training Data Gap in Imaging Spectroscopy\nAdvances have been made to improve the volume of\ntraining data for imaging spectroscopy. For instance,\na recent study in [101] introduced A labeled dataset,\ncomprising 200 HS images captured from the HYPSO-\n1 [102] covering sea, cloud and land to help training\nonboard segmentation algorithms. Another example is\nthe dataset captured by EMITS. This dataset includes\ndifferent images from various land covers all around\nthe world [103]. There are other datasets which have\nbeen extensively used in previous studies i.e. university\nof Pavia, indian pines, Huston and Botsowana can be\nmostly found in [104].\nHowever, despite these efforts to increase the training\ndata, there is still an important lack of training datasets,\ncoupled with a scarcity of ground-truth labels due to\nthe exceptionally time-consuming nature of the label-\ning process. Consequently, dealing with a shortage of\ntraining data still persists as a challenge also in the\nfield of imaging spectroscopy. Novel approaches have\nbeen explored to address this challenge, including the\nutilization of semi-supervised techniques [105], self-\nsupervised methods based on solving previous proxy\ntasks [106], and domain adoption techniques [107]. Ad-\nditionally, another approach known as active transfer\nlearning has been studied, using the most discriminative\nfeatures from unlabeled training observations [108].\n2.6.2. Management of Noisy Data\nSeveral approaches have gained attention to recon-\nstruct high-quality input data for classification. One im-\nportant work studies the application of super-resolution\ntechniques combined with transfer learning, with the\naim to reduce noise and improve the quality of input\ntraining samples [109]. Other studies have used CNNs\nwith sparse signal reconstruction [110], and have em-\nployed the Laplacian Pyramid Network (LPN) [111] for\n7\nenhancing input data with lower noise. Another alterna-\ntive explored in [112] uses structure tensors with a deep\nCNN to improve data quality and diminish the noise\nlevel.\n2.7. Improving Speed and Precision\nA new trend within the field of computer vision in-\nvolves the adoption of CapsuleNets (CapsNet) [113].\nCapsule Networks employ a series of nested neural lay-\ners, enabling them to improve model scalability while\nsimultaneously accelerating computational speed. Ex-\namples of CapsNets can be found in [114–116]. The uti-\nlization of spectra-spatial CapsNet has shown fast con-\nvergence capabilities, mitigating effectively overfitting\n[117].\n2.8. Hardware Accelerators\nTo enhance the efficiency of HS data processing, dif-\nferent hardware platforms have been explored, such as\ncomputing clusters [118], Graphics Processing Units\n(GPUs) and Field Programmable Gate Arrays (FPGAs)\n[119]. Recent advancements in FPGA technology have\npositioned them as a suitable option for performing on-\nboard image processing in both airborne and space-\nborne platforms [120]. An FPGA is a hardware pro-\ngrammable component consisting of an array of logic\nblocks, Random Access Memories (RAMs), hardcopy\nIPs, Input/Output (I/O) pads, routing channels, among\nother building blocks such as specialized Digital Sig-\nnal Processing (DSP) units [121]. The fabric logic can\nbe customized to execute different functions at different\ntimes and levels. A previous generation of similar tech-\nnology was named ASICs from Application-Specific In-\ntegrated Circuits [122]. However, FPGAs offer greater\nflexibility while achieving minimal power consumption\nand high performance compared to other platforms for\non-board processing of HS imagery [123]. Some stud-\nies have explored on-board HS processing using FP-\nGAs, including tasks such as data compression and im-\nage segmentation [124]. In preivous versions, FPGAs\nwere employed for tasks such as end-member extrac-\ntion [123], as exemplified in [125]. In particular, the\nXilinx Virtex-5 FPGA was utilized for automatic tar-\nget detection. Moreover, FPGAs were used to perform\nend-member extraction for multiple targets [126]. Fur-\nthermore, experiments involving spectral signature un-\nmixing have also been conducted on both FPGAs and\nGPUs, yielding competitive results in terms of accu-\nracy. In one particular study, an FPGA was utilized to\ndemonstrate its on-board processing capabilities to de-\ntect chemical plumes [127]. This study served as a pilot\nphase in the development of an Artificial Intelligce unit\nintended for use in an upcoming HS satellite mission\nto be launched by NASA’s Jet Propulsion Laboratory\n(JPL).\nHowever, a main drawback of design in FPGA plat-\nforms is the often difficulty of its configuration and pro-\ngramming.\nTo address this, Intel has developed the\nOpenCL package, while Xilinx has introduced Vitis to\nautomate the synthesis of low-level Register-Transfer\nLevel (RTL) code needed to program the FPGA fabric\nlogic [128]. Consequently, there have been limited stud-\nies exploring the implementation of DL on FPGAs. As a\nresult, future studies should involve the implementation\nof DNNs on FPGAs within the proposed hardware ar-\nchitecture for future Earth observation missions [129].\n3. Discussion and Summary\nWe have investigated the most recent developments\nin the application of deep learning to hyperspectral im-\nagery. Nearly all of the reviewed studies emphasized the\nscarcity of training data as a primary limiting factor, hin-\ndering the adoption of deep learning widely in the HS\nimage processing field. Another important obstacle was\nthe limiting computational infrastructure and hardware,\nparticularly in remote sensing. Our study revealed that\nwhile there is a substantial body of studies and work on\nusing deep learning for land cover classification, there\nremains a critical gap in research pertaining to target\nand anomaly detection, data fusion and spectral unmix-\ning, and deep learning on FPGA platforms.\nNetwork architectures such as UNet, ResNet, and\nVNet have proven their efficacy as promising start-\ning points.\nHowever, further refinement and specifi-\ncation based on application-specific contexts are still\nwarranted. As regards classification of imaging spec-\ntroscopy, deep learning has proven to be effective. Nev-\nertheless, the resource-intensive nature of deep learn-\ning, together with the satisfactory outcomes achieved\nalready through conventional classification techniques\nsuch as SVMs, has led to hesitance among many users\nwhen considering the adoption of deep learning.\nAddressing the challenges posed by limited training\ndatasets and noisy input data, employing using Genera-\ntive Adversarial Networks (GANs) emerges as a feasi-\nble solution to generate augmented datasets and reduce\nnoise within the training samples. Additionally, rein-\nforcement learning stands as a promising alternative that\nmerits further research.\nSeveral versions of neural networks have been re-\nviewed, where deep CNN and 3D-kernel-CNN net-\nworks have demonstrated highly promising results.\n8\nFigure 5: Effectiveness of various on-board analysis methods, with a scale from 0 to 10 representing their suitability in terms of different criteria.\nGiven the focus on optimizing network structures for\non-board processing, GhostNets may be a viable alter-\nnative, although these networks may not yield the most\noptimal accuracy. Furthermore, other studies have con-\nducted extensive testing on lightweight CNN models\nfor deployment aboard satellites, comparing 1D and 2D\nCNNs in which 1D CNNs consistently outperformed\n2D CNNs.\nWhen considering the challenges associated with on-\nboard processing in imaging spectroscopy, two key is-\nsues arise: noisy data and the scarcity of atmospheric\ncorrection at level zero data together with limited train-\ning datasets. However, despite the efforts to increase\nthe volume of training data, it still remains insufficient\nfor adequately training models. Consequently, it is cru-\ncial to focus on the assessment and testing of different\nnetwork structures, simulated data, and methods such as\nself-supervised learning to overcome the lack of training\ndata. To finalise, on-board processing of hyperspectral\nimagery is the new domain of study that will open many\nnew possibilities in the remote sensing domain and the\nautomation of hyperspectral satellite payloads. In the\nnew CHIME mission there is also ongoing research to\nperform target detection on board [130].\nTherefore,\nsince there is a trend towards on-board processing of\nimaging spectroscopy, encompassing both remote sens-\ning and non-remote sensing applications with the aid of\nhardware accelerators, a comprehensive summary of the\nprevailing methods, categorized by their suitability for\non-board implementation, is provided in Figure 5. Ac-\ncording to the figure, conventional methods are charac-\nterized by ease of implementation, but require a sub-\nstantial number of training samples and traditional pro-\ncessing and updating procedures. Therefore, within the\nhyperspectral community, there has been a shift towards\nemploying CNN-based methods.\n4. Conclusion\nIn this review, we have explored recent advance-\nments in the application of deep learning techniques to\nhyperspectral imagery processing. A recurring theme\nthroughout the examined studies has been the challenge\nposed by limited training data, which continues to hin-\nder widespread adoption of deep learning in hyperspec-\ntral image processing. Additionally, constraints in com-\nputational infrastructure, particularly in remote sensing\napplications, have emerged as a significant obstacle.\nWhile deep learning models such as UNet, ResNet,\nand VNet have demonstrated promising results, further\n9\ncustomization and refinement tailored to specific appli-\ncation contexts are warranted. Despite the effectiveness\nof deep learning in land cover classification, concerns\nabout its resource-intensive nature, coupled with the sat-\nisfactory performance of conventional techniques like\nSupport Vector Machines (SVMs), have led to cautious\nadoption among practitioners.\nAddressing challenges related to limited training\ndatasets and noisy input data, the utilization of Gener-\native Adversarial Networks (GANs) for data augmen-\ntation and noise reduction presents a viable solution.\nAdditionally, the exploration of reinforcement learning\nholds promise and merits further investigation.\nVarious neural network architectures, including deep\nCNNs and 3D-kernel CNNs, have exhibited promis-\ning outcomes.\nConsidering the focus on optimizing\nnetwork structures for onboard processing, lightweight\nCNN models and 1D CNNs have shown consistent ad-\nvantages over their counterparts. However, the issues\nof noisy data and the scarcity of atmospheric correction\nat level zero data, along with limited training datasets,\npersist as primary concerns in onboard processing for\nhyperspectral imagery.\nTo overcome these challenges, it is imperative to\ncontinue assessing and testing different network struc-\ntures, simulated data, and methodologies such as self-\nsupervised learning. The trend towards onboard pro-\ncessing of hyperspectral imagery, encompassing both\nremote sensing and non-remote sensing applications\nwith the support of hardware accelerators, indicates a\npromising frontier for future research and application.\nOngoing efforts in missions like CHIME underscore the\ngrowing interest and potential in onboard processing,\nparticularly in tasks such as target detection. In conclu-\nsion, this review underscores the evolving landscape of\nhyperspectral image processing, highlighting the transi-\ntion towards deep learning-based methods and the im-\nportance of addressing key challenges for enhanced op-\nerational efficiency and performance.\nAuthor Contributions\nNafiseh Ghasemi, is a researcher at European Space\nAgency (ESA) working on artificial intelligence for on-\nboard processing of hyperspectral data, who is the main\nauthor and has done most part of the research project.\nJon Alvarez Justo, is an expert visitor at ESA who was\nresponsible for rewriting, reviewing and adding content\nabout the new hyperspectral missions. Marco Celesti,\nis a mission scientist at ESA supporting activities for\ncopernicus missions and providing content about hyper-\nspectral applications. Laurent Despoisse is spacecraft\nmanager for hyperspectral missions at ESA supporting\ninformation on the status of near future CHIME mis-\nsion. Jens Nieke, as the project manager of the CHIME\ngroup, provided support for the research project and\ncontributed to the overall direction of the study.\nAcknowledgements\nThe authors express their gratitude to the European\nSpace Agency (ESA) for providing us support for our\nresearch.\nConflicts of Interest\nThe authors declare no conflicts of interest.\nReferences\n[1] M. Rast, T. H. Painter, Earth Observation Imaging Spec-\ntroscopy for Terrestrial Systems: An Overview of Its His-\ntory, Techniques, and Applications of Its Missions, Sur-\nveys in Geophysics 40 (3) (2019) 303–331. doi:10.1007/\ns10712-019-09517-z.\n[2] X. Zhou, J. Sun, Y. Tian, K. Yao, M. Xu, Detection of heavy\nmetal lead in lettuce leaves based on fluorescence hyperspec-\ntral technology combined with deep learning algorithm, Spec-\ntrochimica Acta Part A: Molecular and Biomolecular Spec-\ntroscopy 266 (2022) 120460.\n[3] P. W. T. Yuen, M. Richardson, An introduction to hyperspectral\nimaging and its application for security, surveillance and target\nacquisition, The Imaging Science Journal 58 (5) (2010) 241–\n253.\n[4] S. Peyghambari, Y. Zhang, Hyperspectral remote sensing in\nlithological mapping, mineral exploration, and environmental\ngeology: an updated review, Journal of Applied Remote Sens-\ning 15 (3) (2021) 031501–031501.\n[5] A. F. Gao, et al., Generalized unsupervised clustering of hy-\nperspectral images of geological targets in the near infrared,\nin: Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 2021, pp. 4294–4303.\n[6] M. E. Grøtte, R. Birkeland, E. Honor´e-Livermore, S. Bakken,\nJ. L. Garrett, E. F. Prentice, F. Sigernes, M. Orlandi´c, J. T.\nGravdahl, T. A. Johansen, Ocean color hyperspectral remote\nsensing with high resolution and low latency—The HYPSO-1\nCubeSat mission, IEEE Transactions on Geoscience and Re-\nmote Sensing 60 (2021) 1–19.\n[7] G. Lassalle, Monitoring natural and anthropogenic plant stres-\nsors by hyperspectral remote sensing: Recommendations and\nguidelines based on a meta-review, Science of the Total Envi-\nronment 788 (2021) 147758.\n[8] P. Shippert, Introduction to hyperspectral image analysis, On-\nline Journal of Space Communication 2 (3) (2003) 8.\n[9] G. Vane, R. O. Green, T. G. Chrien, H. T. Enmark, E. G.\nHansen, W. M. Porter, The airborne visible/infrared imag-\ning spectrometer (AVIRIS), Remote Sensing of Environment\n44 (2-3) (1993) 127–143.\n[10] L. Guanter, et al., The EnMAP spaceborne imaging spec-\ntroscopy mission for earth observation, Remote Sensing 7 (7)\n(2015) 8830–8857.\n10\n[11] E. Lopinto, L. Fasano, F. Longo, P. Sacco, Current Status of\nPRISMA Mission, in: IGARSS 2022-2022 IEEE International\nGeoscience and Remote Sensing Symposium, IEEE, 2022, pp.\n5389–5390.\n[12] R. O. Green, The NASA Earth Venture Instrument, Earth Sur-\nface Mineral Dust Source Investigation (EMIT), in: IGARSS\n2022-2022 IEEE International Geoscience and Remote Sens-\ning Symposium, 2022, pp. 5004–5006.\n[13] J. Nieke, L. Despoisse, A. Gabriele, H. Weber, H. Strese,\nN. Ghasemi, F. Gascon, K. Alonso, V. Boccia, B. Tsonevska,\net al., The Copernicus hyperspectral imaging mission for the\nenvironment (CHIME): an overview of its mission, system and\nplanning status, in: Sensors, Systems, and Next-Generation\nSatellites XXVII, Vol. 12729, SPIE, 2023, pp. 21–40.\n[14] M. Celesti, M. Rast, J. Adams, V. Boccia, F. Gascon, C. Isola,\nJ. Nieke, The Copernicus hyperspectral imaging mission for\nthe environment (CHIME): Status and planning, in: IGARSS\n2022-2022 IEEE International Geoscience and Remote Sens-\ning Symposium, IEEE, 2022, pp. 5011–5014.\n[15] A. Sen, Overview of the imaging spectrometer instrument for\nNASA’s Surface Biology and Geology Mission, in: Sensors,\nSystems, and Next-Generation Satellites XXVII, Vol. 12729,\nSPIE, 2023, p. 127290M.\n[16] M. J. Khan, et al., Modern trends in hyperspectral image anal-\nysis: A review, Ieee Access 6 (2018) 14118–14129.\n[17] L. Ma, et al., Local manifold learning-based k-nearest-\nneighbor for hyperspectral image classification, IEEE Transac-\ntions on Geoscience and Remote Sensing 48 (11) (2010) 4099–\n4109.\n[18] S. Van der Linden, A. Janz, B. Waske, M. Eiden, P. Hostert,\nClassifying segmented hyperspectral data from a heteroge-\nneous urban environment using support vector machines, Jour-\nnal of Applied Remote Sensing 1 (1) (2007) 013543.\n[19] Y. Altmann, et al., Nonlinear spectral unmixing of hyperspec-\ntral images using Gaussian processes, IEEE Transactions on\nSignal Processing 61 (10) (2013) 2442–2453.\n[20] W.\nSun,\net\nal.,\nA\ndissimilarity-weighted\nsparse\nself-\nrepresentation method for band selection in hyperspectral im-\nagery classification, IEEE Journal of Selected Topics in Ap-\nplied Earth Observations and Remote Sensing 9 (9) (2016)\n4374–4388.\n[21] L. Zhang, et al., Simultaneous spectral-spatial feature selection\nand extraction for hyperspectral images, IEEE Transactions on\nCybernetics 48 (1) (2016) 16–28.\n[22] Q. Gao, et al., Hyperspectral image classification using convo-\nlutional neural networks and multiple feature learning, Remote\nSensing 10 (2) (2018) 299.\n[23] N. He, et al., Feature extraction with multiscale covariance\nmaps for hyperspectral image classification, IEEE Transac-\ntions on Geoscience and Remote Sensing 57 (2) (2018) 755–\n769.\n[24] P. R. Lorenzo, et al., Hyperspectral band selection using\nattention-based convolutional neural networks, IEEE Access\n8 (2020) 42384–42403.\n[25] J. Nalepa, et al., Towards resource-frugal deep convolutional\nneural networks for hyperspectral image segmentation, Micro-\nprocessors and Microsystems 73 (2020) 102994.\n[26] D. Wang, et al., Adaptive spectral–spatial multiscale contextual\nfeature extraction for hyperspectral image classification, IEEE\nTransactions on Geoscience and Remote Sensing 59 (3) (2020)\n2461–2477.\n[27] D. Gedalin, et al., DeepCubeNet: reconstruction of spectrally\ncompressive sensed hyperspectral images with deep neural net-\nworks, Optics Express 27 (24) (2019) 35811–35822.\n[28] J. A. Justo, et al., A comparative study of compressive sensing\nalgorithms for hyperspectral imaging reconstruction, in: 2022\nIEEE 14th Image, Video, and Multidimensional Signal Pro-\ncessing Workshop (IVMSP), 2022, pp. 1–5.\n[29] Y. Tarabalka, et al., Segmentation and classification of hy-\nperspectral images using watershed transformation, Pattern\nRecognition 43 (7) (2010) 2367–2379.\n[30] L. Fang, et al., Spectral–spatial classification of hyperspectral\nimages with a superpixel-based discriminative sparse model,\nIEEE Transactions on Geoscience and Remote Sensing 53 (8)\n(2015) 4186–4201.\n[31] J.-M. Yang, et al., A nonparametric feature extraction and its\napplication to nearest neighbor classification for hyperspec-\ntral image data, IEEE Transactions on Geoscience and Remote\nSensing 48 (3) (2009) 1279–1293.\n[32] J. A. Justo, et al., Sea-Land-Cloud Segmentation in Satel-\nlite Hyperspectral Imagery by Deep Learning, arXiv preprint\narXiv:2310.16210 (2023).\n[33] D. Fernandez, et al., FPGA implementation of the principal\ncomponent analysis algorithm for dimensionality reduction of\nhyperspectral images, Journal of Real-Time Image Processing\n16 (5) (2019) 1395–1406.\n[34] M. Fong, Dimension reduction on hyperspectral images, Univ.\nCalifornia, Los Angeles, CA (2007).\n[35] C. Jayaprakash, et al., Dimensionality reduction of hyperspec-\ntral images for classification using randomized independent\ncomponent analysis, in: 2018 5th International Conference on\nSignal Processing and Integrated Networks (SPIN), 2018, pp.\n492–496.\n[36] J. Zabalza, et al., Novel segmented stacked autoencoder for ef-\nfective dimensionality reduction and feature extraction in hy-\nperspectral imaging, Neurocomputing 185 (2016) 1–10.\n[37] G. Cybenko, Approximation by superpositions of a sigmoidal\nfunction, Mathematics of control, signals and systems 2 (4)\n(1989) 303–314.\n[38] G. Montavon, et al., Methods for interpreting and understand-\ning deep neural networks, Digital signal processing 73 (2018)\n1–15.\n[39] S. Albawi, et al., Understanding of a convolutional neural net-\nwork, in: 2017 international conference on engineering and\ntechnology (ICET), 2017, pp. 1–6.\n[40] N. Tajbakhsh, et al., Convolutional neural networks for medical\nimage analysis: Full training or fine tuning?, IEEE transactions\non medical imaging 35 (5) (2016) 1299–1312.\n[41] J. Leng, et al., Cube-CNN-SVM: a novel hyperspectral image\nclassification method, in: 2016 IEEE 28th International Con-\nference on Tools with Artificial Intelligence (ICTAI), 2016, pp.\n1027–1034.\n[42] J. Yang, et al., Hyperspectral image classification using\ntwo-channel deep convolutional neural network, in:\n2016\nIEEE international geoscience and remote sensing symposium\n(IGARSS), 2016, pp. 5079–5082.\n[43] Y. Wei, et al., Spectral-spatial response for hyperspectral image\nclassification, Remote Sensing 9 (3) (2017) 203.\n[44] Y. Chen, et al., Deep fusion of remote sensing data for accurate\nclassification, IEEE Geoscience and Remote Sensing Letters\n14 (8) (2017) 1253–1257.\n[45] X. Xu, et al., Multisource remote sensing data classification\nbased on convolutional neural network, IEEE Transactions on\nGeoscience and Remote Sensing 56 (2) (2017) 937–949.\n[46] L. Jiao, et al., Deep fully convolutional network-based spatial\ndistribution prediction for hyperspectral image classification,\nIEEE Transactions on Geoscience and Remote Sensing 55 (10)\n(2017) 5585–5599.\n[47] W. Hu, et al., Deep convolutional neural networks for hyper-\nspectral image classification, Journal of Sensors 2015 (2015).\n11\n[48] H. Sun, et al., A supervised segmentation network for hyper-\nspectral image classification, IEEE Transactions on Image Pro-\ncessing 30 (2021) 2810–2825.\n[49] A. A. Green, et al., A transformation for ordering multispectral\ndata in terms of image quality with implications for noise re-\nmoval, IEEE Transactions on Geoscience and Remote Sensing\n26 (1) (1988) 65–74.\n[50] J. Wang, C.-I. Chang, Independent component analysis-based\ndimensionality reduction with applications in hyperspectral\nimage analysis, IEEE Transactions on Geoscience and Remote\nSensing 44 (6) (2006) 1586–1600.\n[51] A. A. Nielsen, Kernel maximum autocorrelation factor and\nminimum noise fraction transformations, IEEE Transactions\non Image Processing 20 (3) (2010) 612–624.\n[52] L. K. Sharma, R. K. Verma, AVIRIS-NG hyperspectral data\nanalysis for pre-and post-MNF transformation using per-pixel\nclassification algorithms, Geocarto International 37 (7) (2022)\n2083–2094.\n[53] T. V. Bandos, L. Bruzzone, G. Camps-Valls, Classification of\nhyperspectral images with regularized linear discriminant anal-\nysis, IEEE Transactions on Geoscience and Remote Sensing\n47 (3) (2009) 862–873.\n[54] M. He, B. Li, H. Chen, Multi-scale 3d deep convolutional neu-\nral network for hyperspectral image classification, in: 2017\nIEEE International Conference on Image Processing (ICIP),\nIEEE, 2017, pp. 3904–3908.\n[55] Y. Li, W. Xie, H. Li, Hyperspectral image reconstruction by\ndeep convolutional neural network for classification, Pattern\nRecognition 63 (2017) 371–383.\n[56] H. Zhang, Y. Li, Y. Zhang, Q. Shen, Spectral-spatial classifi-\ncation of hyperspectral imagery using a dual-channel convo-\nlutional neural network, Remote sensing letters 8 (5) (2017)\n438–447.\n[57] V. Slavkovikj, S. Verstockt, W. De Neve, S. Van Hoecke,\nR. Van de Walle, Hyperspectral image classification with con-\nvolutional neural networks, in: Proceedings of the 23rd ACM\ninternational conference on Multimedia, 2015, pp. 1159–1162.\n[58] Y. Chen, H. Jiang, C. Li, X. Jia, P. Ghamisi, Deep feature ex-\ntraction and classification of hyperspectral images based on\nconvolutional neural networks, IEEE Transactions on Geo-\nscience and Remote Sensing 54 (10) (2016) 6232–6251.\n[59] F. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J.\nDally, K. Keutzer, Squeezenet: Alexnet-level accuracy with\n50x fewer parameters and¡ 0.5 mb model size, arXiv preprint\narXiv:1602.07360 (2016).\n[60] L. Zhi, et al., A dense convolutional neural network for hyper-\nspectral image classification, Remote Sensing Letters 10 (1)\n(2019) 59–66.\n[61] B. Liu, et al., Deep few-shot learning for hyperspectral image\nclassification, IEEE Transactions on Geoscience and Remote\nSensing 57 (4) (2018) 2290–2304.\n[62] F.-F. Li, et al., One-shot learning of object categories, IEEE\nTransactions on Pattern Analysis and Machine Intelligence\n28 (4) (2006) 594–611.\n[63] M. E. Paoletti, et al., Deep&dense convolutional neural net-\nwork for hyperspectral image classification, Remote Sensing\n10 (9) (2018) 1454.\n[64] K. Makantasis, et al., Deep supervised learning for hyperspec-\ntral data classification through convolutional neural networks,\nin: 2015 IEEE International Geoscience and Remote Sensing\nSymposium (IGARSS), 2015, pp. 4959–4962.\n[65] J. Yue, et al., Spectral–spatial classification of hyperspectral\nimages using deep convolutional neural networks, Remote\nSensing Letters 6 (6) (2015) 468–477.\n[66] R. Achanta, et al., Slic superpixels compared to state-of-the-\nart superpixel methods, IEEE Transactions on Pattern Analysis\nand Machine Intelligence 34 (11) (2012) 2274–2282.\n[67] S. Mei, et al., Learning sensor-specific spatial-spectral features\nof hyperspectral images via convolutional neural networks,\nIEEE Transactions on Geoscience and Remote Sensing 55 (8)\n(2017) 4520–4533.\n[68] L. Fang, et al., Hyperspectral image classification with squeeze\nmultibias network, IEEE Transactions on Geoscience and Re-\nmote Sensing 57 (3) (2018) 1291–1301.\n[69] D. Kovac, J. Mucha, J. A. Justo, J. Mekyska, Z. Galaz,\nK. Novotny, R. Pitonak, J. Knezik, J. Herec, T. A. Jo-\nhansen, Deep learning for in-orbit cloud segmentation and\nclassification in hyperspectral satellite data, arXiv preprint\narXiv:2403.08695 (2024).\n[70] G. Giuffrida, L. Fanucci, G. Meoni, M. Batiˇc, L. Buckley,\nA. Dunne, C. Van Dijk, M. Esposito, J. Hefele, N. Vercruyssen,\nThe Φ-Sat-1 mission: The first on-board deep neural network\ndemonstrator for satellite earth observation, IEEE Transactions\non Geoscience and Remote Sensing 60 (2021) 1–14.\n[71] A. Marin, C. Coelho, F. Deconinck, I. Babkina, N. Longepe,\nM. Pastena, Phi-Sat-2: Onboard AI apps for earth observation,\nProc. Space Artif. Intell (2021).\n[72] Z. Lin, et al., Spectral-spatial classification of hyperspectral\nimage using autoencoders, in: 2013 9th International Confer-\nence on Information, Communications & Signal Processing,\n2013, pp. 1–5.\n[73] Y. Chen, et al., Deep learning-based classification of hyper-\nspectral data, IEEE Journal of Selected Topics in Applied Earth\nObservations and Remote Sensing 7 (6) (2014) 2094–2107.\n[74] C. Tao, et al., Unsupervised spectral–spatial feature learn-\ning with stacked sparse autoencoder for hyperspectral imagery\nclassification, IEEE Geoscience and Remote Sensing Letters\n12 (12) (2015) 2438–2442.\n[75] L. Zhang, B. Cheng, A stacked autoencoders-based adaptive\nsubspace model for hyperspectral anomaly detection, Infrared\nPhysics & Technology 96 (2019) 52–60.\n[76] J. Yue, et al., A deep learning framework for hyperspectral im-\nage classification using spatial pyramid pooling, Remote Sens-\ning Letters 7 (9) (2016) 875–884.\n[77] Y. Liu, et al., Hyperspectral classification via deep networks\nand superpixel segmentation, International Journal of Remote\nSensing 36 (13) (2015) 3459–3482.\n[78] C. Xing, et al., Stacked denoise autoencoder based feature ex-\ntraction and classification for hyperspectral images, Journal of\nSensors 2016 (2016).\n[79] L. Windrim, et al., A physics-based deep learning approach\nto shadow invariant representations of hyperspectral images,\nIEEE Transactions on Image Processing 27 (2) (2017) 665–\n677.\n[80] J. E. Ball, P. Wei, Deep learning hyperspectral image classifica-\ntion using multiple class-based denoising autoencoders, mixed\npixel training augmentation, and morphological operations, in:\nIGARSS 2018-2018 IEEE International Geoscience and Re-\nmote Sensing Symposium, 2018, pp. 6903–6906.\n[81] Y. Chen, et al., Spectral–spatial classification of hyperspec-\ntral data based on deep belief network, IEEE Journal of Se-\nlected Topics in Applied Earth Observations and Remote Sens-\ning 8 (6) (2015) 2381–2392.\n[82] N. Ma, et al., An unsupervised deep hyperspectral anomaly\ndetector, Sensors 18 (3) (2018) 693.\n[83] F. Huang, et al., Hyperspectral remote sensing image change\ndetection based on tensor and deep learning, Journal of Visual\nCommunication and Image Representation 58 (2019) 233–244.\n[84] M. Wang, et al., Nonlinear unmixing of hyperspectral data\nvia deep autoencoder networks, IEEE Geoscience and Remote\n12\nSensing Letters 16 (9) (2019) 1467–1471.\n[85] S. Ozkan, et al., Endnet: Sparse autoencoder network for end-\nmember extraction and hyperspectral unmixing, IEEE Transac-\ntions on Geoscience and Remote Sensing 57 (1) (2018) 482–\n496.\n[86] A. Creswell, et al., Generative adversarial networks:\nAn\noverview, IEEE signal processing magazine 35 (1) (2018) 53–\n65.\n[87] Z. He, et al., Generative adversarial networks-based semi-\nsupervised learning for hyperspectral image classification, Re-\nmote Sensing 9 (10) (2017) 1042.\n[88] M. Zhang, et al., Unsupervised feature extraction in hyperspec-\ntral images based on wasserstein generative adversarial net-\nwork, IEEE Transactions on Geoscience and Remote Sensing\n57 (5) (2018) 2669–2688.\n[89] Y. Zhan, et al., Semi-supervised classification of hyperspectral\ndata based on generative adversarial networks and neighbor-\nhood majority voting, in: IGARSS 2018-2018 IEEE Interna-\ntional Geoscience and Remote Sensing Symposium, 2018, pp.\n5756–5759.\n[90] L. Bashmal, et al., Siamese-gan: Learning invariant represen-\ntations for aerial vehicle image categorization, Remote Sensing\n10 (2) (2018) 351.\n[91] H. Wu, S. Prasad, Convolutional recurrent neural networks for\nhyperspectral data classification, Remote Sensing 9 (3) (2017)\n298.\n[92] L. Mou, et al., Deep recurrent neural networks for hyperspec-\ntral image classification, IEEE Transactions on Geoscience and\nRemote Sensing 55 (7) (2017) 3639–3655.\n[93] Q. Liu, et al., Bidirectional-convolutional lstm based spectral-\nspatial feature learning for hyperspectral image classification,\nRemote Sensing 9 (12) (2017) 1330.\n[94] C. Shi, C. Pun, Superpixel-based 3d deep neural networks\nfor hyperspectral image classification, Pattern Recognition 74\n(2018) 600–616.\n[95] F. Ratle, et al., Semisupervised neural networks for efficient\nhyperspectral image classification, IEEE Transactions on Geo-\nscience and Remote Sensing 48 (5) (2010) 2271–2282.\n[96] A. Romero, C. Gatta, G. Camps-Valls, Unsupervised deep fea-\nture extraction for remote sensing image classification, IEEE\nTransactions on Geoscience and Remote Sensing 54 (3) (2015)\n1349–1362.\n[97] E. Maggiori, et al., Convolutional neural networks for large-\nscale remote-sensing image classification, IEEE Transactions\non Geoscience and Remote Sensing 55 (2) (2016) 645–657.\n[98] L. Mou, P. Ghamisi, X. X. Zhu, Unsupervised spectral–spatial\nfeature learning via deep residual conv–deconv network for hy-\nperspectral image classification, IEEE Transactions on Geo-\nscience and Remote Sensing 56 (1) (2017) 391–406.\n[99] H. Wu, S. Prasad, Semi-supervised deep learning using pseudo\nlabels for hyperspectral image classification, IEEE Transac-\ntions on Image Processing 27 (3) (2017) 1259–1270.\n[100] Q. Feng, et al., Multisource hyperspectral and lidar data fusion\nfor urban land-use mapping based on a modified two-branch\nconvolutional neural network, ISPRS International Journal of\nGeo-Information 8 (1) (2019) 28.\n[101] J. A. Justo, et al., An open hyperspectral dataset with sea-land-\ncloud ground-truth from the hypso-1 satellite, arXiv preprint\narXiv:2308.13679 (2023).\n[102] S. Bakken, M. B. Henriksen, R. Birkeland, D. D. Langer, A. E.\nOudijk, S. Berg, Y. Pursley, J. L. Garrett, F. Gran-Jansen,\nE. Honor´e-Livermore, et al., Hypso-1 cubesat: first images and\nin-orbit characterization, Remote Sensing 15 (3) (2023) 755.\n[103] R. O. Green, N. Mahowald, D. R. Thompson, C. Ung,\nP. Brodrick, R. Pollock, M. Bennett, S. Lundeen, M. Joyce,\nW. Olson-Duvall, Performance and early results from the earth\nsurface mineral dust source investigation (emit) imaging spec-\ntroscopy mission, in: 2023 IEEE Aerospace Conference, 2023,\npp. 1–10.\n[104] Papers with Code,\nHyperspectral images datasets,\nRe-\ntrieved from https://paperswithcode.com/datasets?\nmod=hyperspectral-images.\n[105] B. Pan, Z. Shi, X. Xu, Mugnet: Deep learning for hyperspectral\nimage classification using limited samples, ISPRS Journal of\nPhotogrammetry and Remote Sensing 145 (2018) 108–119.\n[106] P. Ghamisi, Y. Chen, X. X. Zhu, A self-improving convolu-\ntion neural network for the classification of hyperspectral data,\nIEEE Geoscience and Remote Sensing Letters 13 (10) (2016)\n1537–1541.\n[107] Z. Wang, et al., Domain adaptation with discriminative distri-\nbution and manifold embedding for hyperspectral image clas-\nsification, IEEE Geoscience and Remote Sensing Letters 16 (7)\n(2019) 1155–1159.\n[108] P. Liu, H. Zhang, K. B. Eom, Active deep learning for classifi-\ncation of hyperspectral images, IEEE Journal of Selected Top-\nics in Applied Earth Observations and Remote Sensing 10 (2)\n(2016) 712–724.\n[109] Y. Yuan, X. Zheng, X. Lu, Hyperspectral image superresolu-\ntion by transfer learning, IEEE Journal of Selected Topics in\nApplied Earth Observations and Remote Sensing 10 (5) (2017)\n1963–1974.\n[110] J. Lin, et al., Dual-modality endoscopic probe for tissue sur-\nface shape reconstruction and hyperspectral imaging enabled\nby deep neural networks, Medical Image Analysis 48 (2018)\n162–176.\n[111] Z. He, L. Liu, Hyperspectral image super-resolution inspired\nby deep laplacian pyramid network, Remote Sensing 10 (12)\n(2018) 1939.\n[112] W. Xie, et al., High-quality spectral-spatial reconstruction us-\ning saliency detection and deep feature enhancement, Pattern\nRecognition 88 (2019) 139–152.\n[113] P. Roy, et al., Effects of degradations on deep neural network\narchitectures, arXiv preprint arXiv:1807.10108 (2018).\n[114] M. E. Paoletti, et al., Capsule networks for hyperspectral image\nclassification, IEEE Transactions on Geoscience and Remote\nSensing 57 (4) (2018) 2145–2160.\n[115] W.-Y. Wang, et al., Hyperspectral image classification based on\ncapsule network, in: IGARSS 2018-2018 IEEE International\nGeoscience and Remote Sensing Symposium, 2018, pp. 3571–\n3574.\n[116] K. Zhu, et al., Deep convolutional capsule network for hyper-\nspectral image spectral and spectral-spatial classification, Re-\nmote Sensing 11 (3) (2019) 223.\n[117] J. Yin, et al., Hyperspectral image classification using capsnet\nwith well-initialized shallow layers, IEEE Geoscience and Re-\nmote Sensing Letters 16 (7) (2019) 1095–1099.\n[118] Z. Wu, et al., Parallel and distributed dimensionality reduction\nof hyperspectral data on cloud computing architectures, IEEE\nJournal of Selected Topics in Applied Earth Observations and\nRemote Sensing 9 (6) (2016) 2270–2278.\n[119] C. Gonzalez, et al., Use of fpga or gpu-based architectures for\nremotely sensed hyperspectral image processing, Integration\n46 (2) (2013) 89–103.\n[120] A. Plaza, et al., Parallel hyperspectral image and signal pro-\ncessing [applications corner], IEEE Signal Processing Maga-\nzine 28 (3) (2011) 119–126.\n[121] I. Kuon, et al., Fpga architecture: Survey and challenges, Foun-\ndations and Trends® in Electronic Design Automation 2 (2)\n(2008) 135–253.\n[122] S. Mittal, et al., Fpga: An efficient and promising platform\n13\nfor real-time image processing applications, in: National Con-\nference On Research and Development In Hardware Systems\n(CSI-RDHS), 2008.\n[123] C. Gonz´alez, et al., Fpga implementation of the n-findr algo-\nrithm for remotely sensed hyperspectral image analysis, IEEE\nTransactions on Geoscience and Remote Sensing 50 (2) (2011)\n374–388.\n[124] S. Lopez, et al., The promise of reconfigurable computing for\nhyperspectral imaging onboard systems: A review and trends,\nProceedings of the IEEE 101 (3) (2013) 698–722.\n[125] S. Bernab´e, et al., Fpga design of an automatic target genera-\ntion process for hyperspectral image analysis, in: 2011 IEEE\n17th International Conference on Parallel and Distributed Sys-\ntems, 2011, pp. 1010–1015.\n[126] J. Lei, et al., A novel fpga-based architecture for fast auto-\nmatic target detection in hyperspectral images, Remote Sens-\ning 11 (2) (2019) 146.\n[127] J. Theiler, et al., Onboard cubesat data processing for hyper-\nspectral detection of chemical plumes, in: Algorithms and\nTechnologies for Multispectral, Hyperspectral, and Ultraspec-\ntral Imagery XXIV, Vol. 10644, 2018, pp. 31–42.\n[128] Z. Wang, et al., Briefly analysis about cnn accelerator based on\nfpga, Procedia Computer Science 202 (2022) 277–282.\n[129] A. A. Omar, M. M. Farag, R. A. Alhamad, Artifical intelli-\ngence: New paradigm in deep space exploration, in: 2021 14th\nInternational Conference on Developments in eSystems Engi-\nneering (DeSE), 2021, pp. 438–442.\n[130] N. Ghasemi, et al., Feasibility study to detect floating de-\nbris by hyperspectral mission using onboard ai, in: IGARSS\n2023-2023 IEEE International Geoscience and Remote Sens-\ning Symposium, 2023, pp. 5254–5256.\n14\n",
  "categories": [
    "eess.IV"
  ],
  "published": "2024-04-09",
  "updated": "2024-04-09"
}