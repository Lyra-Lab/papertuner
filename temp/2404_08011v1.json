{
  "id": "http://arxiv.org/abs/2404.08011v1",
  "title": "An inclusive review on deep learning techniques and their scope in handwriting recognition",
  "authors": [
    "Sukhdeep Singh",
    "Sudhir Rohilla",
    "Anuj Sharma"
  ],
  "abstract": "Deep learning expresses a category of machine learning algorithms that have\nthe capability to combine raw inputs into intermediate features layers. These\ndeep learning algorithms have demonstrated great results in different fields.\nDeep learning has particularly witnessed for a great achievement of human level\nperformance across a number of domains in computer vision and pattern\nrecognition. For the achievement of state-of-the-art performances in diverse\ndomains, the deep learning used different architectures and these architectures\nused activation functions to perform various computations between hidden and\noutput layers of any architecture. This paper presents a survey on the existing\nstudies of deep learning in handwriting recognition field. Even though the\nrecent progress indicates that the deep learning methods has provided valuable\nmeans for speeding up or proving accurate results in handwriting recognition,\nbut following from the extensive literature survey, the present study finds\nthat the deep learning has yet to revolutionize more and has to resolve many of\nthe most pressing challenges in this field, but promising advances have been\nmade on the prior state of the art. Additionally, an inadequate availability of\nlabelled data to train presents problems in this domain. Nevertheless, the\npresent handwriting recognition survey foresees deep learning enabling changes\nat both bench and bedside with the potential to transform several domains as\nimage processing, speech recognition, computer vision, machine translation,\nrobotics and control, medical imaging, medical information processing,\nbio-informatics, natural language processing, cyber security, and many others.",
  "text": "AN INCLUSIVE REVIEW ON DEEP LEARNING TECHNIQUES AND\nTHEIR SCOPE IN HANDWRITING RECOGNITION\nA PREPRINT\nSukhdeep Singh\nD.M. College (Affiliated to Panjab University, Chandigarh)\nMoga, Punjab, India\nsukha13@ymail.com\nSudhir Rohilla\nDepartment of Computer Science\nGopichand Arya Mahila College\nAbohar, Punjab, India\nrohilla2209@gmail.com\nAnuj Sharma\nDepartment of Computer Science and Applications\nPanjab University, Chandigarh, India\nanujs@pu.ac.in\nApril 15, 2024\nABSTRACT\nDeep learning expresses a category of machine learning algorithms that have the capability to combine\nraw inputs into intermediate features layers. These deep learning algorithms have demonstrated great\nresults in different fields. Deep learning has particularly witnessed for a great achievement of human\nlevel performance across a number of domains in computer vision and pattern recognition. For the\nachievement of state-of-the-art performances in diverse domains, the deep learning used different\narchitectures and these architectures used activation functions to perform various computations\nbetween hidden and output layers of any architecture. This paper presents a survey on the existing\nstudies of deep learning in handwriting recognition field. Even though the recent progress indicates\nthat the deep learning methods has provided valuable means for speeding up or proving accurate\nresults in handwriting recognition, but following from the extensive literature survey, the present\nstudy finds that the deep learning has yet to revolutionize more and has to resolve many of the most\npressing challenges in this field, but promising advances have been made on the prior state of the art.\nAdditionally, an inadequate availability of labelled data to train presents problems in this domain.\nNevertheless, the present handwriting recognition survey foresees deep learning enabling changes\nat both bench and bedside with the potential to transform several domains as image processing,\nspeech recognition, computer vision, machine translation, robotics and control, medical imaging,\nmedical information processing, bio-informatics, natural language processing, cyber security, and\nmany others.\nKeywords Deep learning · Classification · Handwriting Recognition · CNN · RNN · LSTM\n1\nIntroduction\nThe intelligent act with synthesis and analysis of computational agents represents Artificial Intelligence (AI). Here,\nan agent is who completes the signed goal with various learning techniques and training of data. The agent when\ncomputationally represented, it is called computational agent David L. Poole [2010], Elaine Rich [2010]. The artificial\nintelligence has made our life very exciting with state-of-the-art research in this area. However, the research in AI\nregularly demands new paradigms that could further help in error-free AI systems. The AI has many areas of research\nsuch as machine learning, data mining, intelligent tutoring, case-based reasoning, multi-agent planning, scheduling,\nuncertain reasoning, natural language understanding and translation, vision, virtual reality, games, robotics and other\narXiv:2404.08011v1  [cs.CV]  10 Apr 2024\narXiv Template\nA PREPRINT\ntopics Zawacki-Richter et al. [2019], Chen et al. [2020], Nilsson [2010], Goodrich and Schultz [2007], Buczak and\nGuven [2016], Bahrammirzaee [2010], Bengio et al. [2013], Brougham and Haar [2018], Corvalan [2018], Ghahramani\n[2015], Castelvecchi. The one of today’s popular research fields in AI is Machine Learning (ML). The machine\nlearning mainly includes intelligent system development using training of data. Therefore, the ML based system model\ndeveloped with train data further decides the nature of future data as test data. The common techniques of machine\nlearning are data understanding, regression, clustering, classification, dimension reduction, deep learning, big data,\nonline learning etc McDonald [1989], Musumeci et al. [2019], Bishop [2006], Chapelle et al. [2006], Collobert et al.\n[2011], Du et al., Freund and Schapire [1997], Grira et al. [2004], Guyon and Elisseeff [2006], LeCun et al. [2015a],\nPedregosa et al. [2011], Vapnik [1998]. Here, each ML technique offers uniqueness in terms of data handling, feature\ncomputation and respective output. Data understanding is data normalization and processing of data; regression is\npromising statistical area to understand continuous type of data; clustering allow class formation of data; classification\ndistinguish data for various classes; dimension reduction reduce feature size of data and retain useful information for\ndata; deep learning is promising data classification and understanding area; big data is dedicated to handle large amount\nof data using established scientific methods; online learning refers to handle data as it comes and not in conventional\nbatch mode. The recent research in ML suggests that deep learning is one of the promising techniques to achieve\nhigh accuracy results. Therefore, deep learning studied by many scientists in recent past and it has been observed that\nsuitable literature of deep learning always helps for readers working in this area. Especially, suitable review of deep\nlearning two popular methods as Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN) will\nbe meaningful from research and development point of view Abdel-Hamid et al. [2014], Cho et al. [2014], Zhao et al.\n[2017].\nIn machine learning, representational learning allow automatically appropriate way to represent data. Deep learning is\none such technique which includes representation learning feature. The progression of learned transformation helps\ndeep learning to achieve automatic representation of data. This has been realised in recent past that deep learning-based\napplications achieved promising results for large amount of data problem, which were not possible to automate data\nrepresentation before deep learning techniques. The popular deep learning model and its practical implementation\nbased quintessential example is deep network. These deep networks are inspired from common machine learning\ntechnique as multilayer perceptron which is a common neural network algorithm. The depth of deep network as a\ncomputer program is organised in such a way that each layer meant for a specific purpose and it could recall other\nlayers’ computer memory when working in parallel. This way, the network with more depth allows many instructions\nin a sequence. The two approaches of deep networks as CNN and RNN proved state-of-the-art results in recent past.\nFurther, LSTM and BLSTM are two particular forms of RNN. The two competitive techniques as CNN and RNN share\nmany commonalities in working, however, they do differ in suitability of various types of data. Interestingly, hybrid\ntechnique using CNN and RNN is another feasible technique with promising results.\nIn literature, we find many studies that include the working of CNN and RNN. However, overall working of CNN and\nRNN with their architecture, mathematical formulation and implementation from review point of view needs attention\nand is presented in this paper. This paper has been presented with updated literature information and focused on theory\nas well as the implementation of deep networks. This review is aiming to answer the question of the working of deep\nnetworks, CNN and RNN, and their performance in handwriting recognition in recent past with the availability of\nlatest system configurations. As a result, we highlight in summary the following contributions as: (i) deep networks\noutperform in handwriting recognition, (ii) the CNN and RNN results in state-of-the-art results for real life challenging\ndatasets, (iii) the architecture of deep networks offers enormous scope to researchers to enhance its architecture, and\nfurther offer many areas of research to improve deep networks. Moreover, this paper also presents general observations\nof deep networks and their applications in handwriting recognition based on suitable findings from literature work. In\nthis manner, it has been analysed that deep networks solely justify deep learning representative to real life handwriting\nrecognition applications. This analysis is based on the results discussed in this paper using benchmark datasets with\nvariants of deep learning approaches.\nThis paper maintains continuity as deep networks, CNN and RNN, architectures and DL results for handwriting\nrecognition in a sequence for better understanding from the reader’s point of view. The rest of the paper is organized as\nfollows. This article describes the deep networks’ fundamentals and evolution in section 2. The section 3 demonstrates\ndifferent deep learning architectures. The section 4 presents existing literature results using deep networks as CNN and\nRNN in handwriting recognition. The section 6 presents the general observations based on literature finding for CNN\nand RNN. The last section 7 concludes this article with findings and scope of future work.\n2\nDeep Forward Networks\nDeep learning quintessential are forward networks and forward networks are commonly called deep forward networks.\nThe forward networks are also known as multi-layer perceptron as it follows multiple layers architecture using perceptron\n2\narXiv Template\nA PREPRINT\nconcept. The CNN and RNN are the emergent variants of deep learning framework based on neural networks. Therefore,\nprior understanding of neural networks is important in this review to know how neural networks work and CNN or\nRNN based on neural networks. Before the introduction of deep neural networks, this section illustrates the evolution of\nneural networks with common benchmark algorithms of literature including most preferred backpropagation algorithm.\nIn early works, McCulloch and Pitts neuron was designed in 1943, it mainly included the combination of logic functions\nwith the concept of threshold McCulloch and Pitts [1943]. The Hebb network was designed in 1949, which included two\nactive neurons simultaneously with their strong inter-connections Hebb [1949]. One major contribution was perceptron\nmodel in 1958 by Rossenblatt, which included weights in connection path with their adjustment Rossenblatt [1958]. In\n1960, Adaline network was built with the ability to reduce difference between net input weights and out weights and\nresulted in minimizing the mean error rates Windrow and Hoff [1960]. One major development noticed for unsupervised\nlearning in 1982, Kohonen introduced Kohonen self-organizing maps where inputs were clustered together to form\noutput neurons Kohonen [1982]. This work was among initial findings to understand neural networks in supervised and\nunsupervised areas. One such study with fixed weights was done for Hopfield networks to act as associative memory\nnets Hopfield [1982]. In 1986, a complete neural network algorithm, with forward and backward ability to update\nweights and based on Multi-Layer Perceptron (MLP), was introduced as backpropagation algorithm Rumelhart et al.\n[1986]. This backpropagation algorithm was a complete multi-layer perceptron technique. This architecture included\ninput, hidden and output layers, forward moves to update weights and backward moves to improve weights with\npropagated error information at output unit in each iteration. After backpropagation, neural networks witnessed many\nimprovements subject to the nature of problem. Few networks were adaptive resonance theory, radial basis functions,\nneocognition until 1990 Carpenter and Grossberg [1988], Broomhead and Lowe [1988], Fukushima [2010]. The main\ndevelopment with state-of-the-art results were reported with MLP until the introduction of convolution networks in\n2000.\nThe MLP or deep forward network is a massively parallel distributed processor made up of simple processing units,\nwhich has a natural propensity for storing experiential knowledge and making it available for use Haykin [1998]. It\nis a directed graph consisting of nodes with interconnecting synaptic and activation links with main properties as:\neach neuron is represented by a set of linear synaptic links with bias, and a possibly nonlinear activation link; the\nsynaptic links of a neuron weight to their respective input signals; the weighted sum of the input signals defines the\ninduced local field of the current neuron; thus, the activation link squashes the induced local field of the neuron to\nproduce an output. The presence of one or more layers between input and output layers are called hidden layers\nand nodes of corresponding layers are hidden neurons. This enhances system learning capability and is referred as\nmulti-layer networks and results in MLP. The major characteristics of MLP include: the model of each neuron in\nnetwork includes nonlinear activation function; the network includes multiple hidden layers that enable network to\nlearn complex tasks by progressively extracting more meaningful features by minimizing errors at output layer; the\nnetwork exhibits high degree of connectivity determined by synapses of network and a change in network require\nchange in synaptic connections or their weights. The working of MLP has been presented in ??, where input layer\nincludes three nodes, two hidden layers with three nodes each and output layer with two nodes. In figure 1, forward\nmove is shown with connected lines and backward moves with dotted lines. The input layer includes initialized values\nthat are processed with weight vectors and each hidden layer node is updated in forward manner. The output layer\ncomputes final value and error is computed subject to target value against output value. Therefore, error values decide\nto move backward in order to minimize error values. This results in many iterations until the expected value of error is\nachieved. This way, it includes the computation of the function signal appearing at the output neuron, a continuous\nnonlinear function of input signal and synaptic weight associated with that neuron. Also, the computation of an estimate\nof gradient vector which is needed for backward pass of the network.\nThe MLP algorithm includes three major steps as forward computation, backward computation and weight updating.\nThe working of these steps has been presented with an instance of one hidden layer to another consecutive hidden\nlayer for neuron j to neuron k. The details of MLP including derivations have been studied extensively in literature.\nAccording to Haykin [1998], for an MLP, the first step as forward computation results in output as y(l)\nj\nfor neuron j in\nlayer l using:\nv(l)\nj (n) = Pm0\ni=0 w(l)\nji (n)y(l−1)\ni\n(n);\ny(l)\nj\n= ϕj(vj(n))\nThe forward computation is performed in forward direction until the output layer neuron value is not computed. In\nbackward computation, gradient value is computed for neuron j in hidden layer l as:\nδ(l)\nj (n) = e(L)\nj\n(n)ϕ\n′\nj(vL\nj (n)), for neuron j in output layer L;\nδ(l)\nj (n) = ϕ\n′\nj(vl\nj(n)) P\nk δ(l+1)\nk\n(n)w(l+1)\nkj\n(n)\n3\narXiv Template\nA PREPRINT\nFigure 1: Overview of multi layer perceptron technique\nFor one forward and backward computation, it completes one iteration and updates the weight for next iteration. The\nadjustment of the synaptic weights of network in layer l is computed as:\nw(l)\nji (n + 1) = w(l)\nji (n) + α[w(l)\nji (n −1)] + ηδ(l)\nj (n)y(l−1)\ni\n(n)\nIn above three steps of MLP, wji is the weight from i to j; v(l)\nj (n) is neuron j in layer l; y(l)\nj\nis output of neuron j in\nlayer l; δ(l)\nj (n) is gradient for neuron j in layer l; e(L)\nj\n(n) is error signal at layer L for neuron j; ϕ\n′\nj is differentiation of\nactivation function; η and α are the learning rate and momentum constant respectively.\n3\nDeep Learning Architecture\nThe pattern recognition field has started using DL architectures extensively and image recognition Krizhevsky et al.\n[2012] Szegedy et al. [2015a] has also attained good performance for recognizing faces Taigman et al. [2014], text\nrecognition Simard et al. [2003] Ciresan et al. [2011] Ciregan et al. [2012] Wang et al. [2012] Goodfellow et al. [2013]\nand estimation of human poses Tompson et al. [2014]. Deep learning, either uses deep architectures of learning or\nhierarchical learning approaches, is a class of machine learning developed mostly after 2006. The DL architectures\nhave basically made alteration of the traditional form of pattern recognition and have contributed a major development\nin various handwriting recognition tasks too. The traditional ML approaches perform better for lesser amounts of input\ndata. When the data size increases beyond a certain limit, the performance of traditional machine learning approaches\nbecomes steady, whereas deep learning performance increases with respect to the increment of data size. The key\nbreakthrough of DL is that these models can perform feature extraction and classification automatically. Deep learning\narchitectures with more than one hidden layer are referred to as multilayer perceptron. These architectures include\ndeep feed forward neural network, CNN, RNN, LSTM and the deep generative models as deep belief networks, deep\nBoltzmann machines, generative adversarial networks LeCun et al. [2015b] Goodfellow et al. [2016]. Deep learning\narchitectures include to learn patterns in data, mapping of input function to outputs and many more, and it can be\nattained with specialized architectures only. Among DL architectures, the CNN and RNN: LSTM and BLSTM are the\nmost commonly used vital architectures. LSTM and BLSTM are particularly designed for data in sequential form, have\nbeen used in the studies of pattern and handwriting recognition. One of the key problems with the RNN deep network is\nthat the hidden layers are influenced by the input layer and consequently the output layer goes on decaying as it cycles\nthrough the network’s recurrent connections, and this problem is known as vanishing gradient problem Bengio et al.\n[1994]. Such problem becomes a cause for an incomplete range of contextual information access by RNN, and the\ncontextual information cannot be retained for a longer period of time by an architecture of RNN.\n3.1\nConvolutional Neural Network\nAmong different models of DL, the CNN is largely used for the recognition of images. The CNN is a special form of\nmulti-layer NN. Like other networks, CNNs also make the use of back propagation algorithms for training, and the\ndistinction lies in their architectures Lecun et al. [1998] Simard et al. [2003]. The CNN is very well suited to represent\n4\narXiv Template\nA PREPRINT\nthe structure of an image, as there is a strong relationship of image pixels to their neighbouring pixels and have very\nsmall correlation with far away pixels. Furthermore, the CNN’s strategy of weight sharing ensures that the similar\nproperties as texture and brightness can be shared by different parts of an image. CNN can efficiently extract and\nabstract 2D features. The shape variations can be effectively absorbed by the CNN max-pooling layer. Further, the\ninvolvement of CNN with less parameters than a similar sized fully connected network has been made possible by\nsparse connection with tied weights. Most considerably, the gradient-based learning algorithm can train the CNN and\nthe CNN suffers less from the diminishing gradient problem. The recognition of text using CNN is more tricky and\ntough task than image recognition since the characters and words can have different appearances according to distinct\nwriters, writing styles and writing surfaces. Simply by employing DL techniques, the promising results can be attained\nfor pattern recognition. Nonetheless, with the intention of getting best results in the area of pattern recognition using\nDL, there are some other challenges that need to be dealt for DL as it is always required to choose the most appropriate\nDL framework for pattern recognition. As an illustration, for handwriting recognition, CNN was initially used in\ndigits recognition LeCun et al. [1998]. Then CNN and its variants are progressively used in various other handwriting\nrecognition applications. The CNN is the most successful model for image analysis. Since late seventies, the work\non CNNs has been done Fukushima and Miyake [1982] and these were already applied for image analysis in medical\nfield in 1995 Lo et al. [1995]. The CNNs were first successfully applied in real-world application in LeNet LeCun et al.\n[1998] for recognition of handwritten digits. Regardless of CNNs initial achievement, its use did not get momentum\nuntil several new techniques were developed to train deep networks efficiently, and advancements were made in core\ncomputing systems. The watershed was a contribution Krizhevsky et al. [2012] to the ImageNet challenge in 2012 and\nthe CNN AlexNet won that competition by a huge margin. In recent years, further development has been done using\nrelated but deeper architectures Russakovsky et al. [2015]. Deep convolution networks have become a technique of\nchoice in computer vision.\n3.1.1\nCNN Architecture\nThe architecture of CNN comprises two main components as extraction of features and classification. To extract features,\nCNN’s each layer obtains the output from its immediately preceding layer as an input and then it delivers the current\noutput as an input to the instant next layer, where classifier builds the expected outputs associated with the input data.\nThe figure 2 presents the basic architecture of CNN. Generally, the architecture of CNN has two fundamental layers as:\nconvolution layer and pooling layer LeCun et al. [1998]. Convolution layer’s each node carries out the convolution\noperation on the input nodes and does feature extraction from input. The max-pooling layer performs feature extraction\nusing average/maximum operation on input nodes. The output of n −1th layer is used as an input to nth layer, where\nthe inputs go through kernels set trailed by ReLU the nonlinear function. The advanced architectures of CNN make use\nof a stack of convolutional layers and max-pooling layers followed by completely connected and softmax layer at the\nend. Largely an efficient architecture of the CNN can be built using basic components as convolution layer, pooling\nlayer, softmax layer and fully connected layer.\n3.2\nRecurrent neural network\nThe RNN is a type of DL networks in which connections in nodes form a directed graph along a temporal sequence\nand it allows the demonstration of temporal dynamic behaviour. Feed forward NN is also used to derive an RNN\nand variable length sequences of inputs can be processed by using the internal state of an RNN. This feature of an\nRNN makes it applicable to various unsegmented tasks as speech recognition Sak et al. [2014] Li and Wu [2015] and\nconnected handwriting recognition Graves et al. [2009]. An RNN term refers to two major network classes having\nsimilar general structure, in which one RNN is a directed acyclic graph and a strictly feed forward neural network can\nunroll and replace it, and the second RNN is also a directed cyclic graph but it cannot be unrolled. Both of these RNNs\ncan have extra stored states, and the storage can be directly controlled by an RNN. The storage can also be replaced by\nother networks or graphs, if it includes time delays or has feedback loops. These controlled states are called gated states\nand memories, and these are also a part of LSTMs and gated recurrent units.\n3.2.1\nRNN Architecture\nThe RNNs are distinctive by the operations over a sequence of vectors over time are permitted by them. There are\ndifferent architectures of RNN with respective to the applications. The figure 3 represents various architectures of RNN.\nThese architectures can be categorized as: one to one, one to many, many to one and many to many.\nOne to one: It is a standard mode to classify without RNN and mostly used in image classification.\nOne to many: It takes an input and gives a sequence of outputs, and it has been successfully used in image captioning\nproblems where a set of words output is required for a single image input.\n5\narXiv Template\nA PREPRINT\nFigure 2: Basic architecture of CNN\nMany to one: It has a sequence of inputs and gives one output only. It is most commonly used in those problems where\ninputs are in the form of sentences or words set and output is a positive/negative expression.\nMany to many: It produces a sequence of outputs for a sequence of inputs and it is most commonly used in machine\ntranslation and video classification problems.\nIn machine translation problems, a sequence of words in one language is given as an input to a machine and translated\nto a sequence of words in other language. In video classification problems, video frames are taken as an input and each\nframe of the video is labelled as an output.\n3.2.2\nLong Short Term Memory\nAn LSTM Hochreiter and Schmidhuber [1997] Graves et al. [2009] is a particular form of RNN architecture that is\nspecially created to overcome the problem of vanishing gradient Bengio et al. [1994]. An LSTM hidden layer comprises\nthe recurrently connected memory blocks, in which each block has one or more recurrently connected memory cells,\nand three multiplicative gates (input, output, and forget gates) are used to activate and control it. These three gates make\nit feasible to store and access the contextual information over a long time period. More particularly, the activation of\ncell is not overwritten by new inputs until the input gate is closed. Similarly, as long the output gate remains open, the\ncell activation is accessible to the rest network and the forget gate controls the recurrent connection of the cell. Like\nCNN architecture, every LSTM layer can have multiple forward and backward layers; multiple feature maps at the\noutput layer; and max-pooling sub-sampling is used to stack multiple LSTM layers.\n3.2.3\nLSTM Architecture\nLSTM is an RNN architecture that takes into account the values over arbitrary intervals. LSTM is suitable for\nclassification, processing and prediction of time series given time lags of unknown duration. Back propagation through\ntime training algorithm is used to update weights in LSTM.\nIn last few years, there have been various advanced approaches developed for LSTM. The figure 4 shows the diagram\nof LSTM. The main scheme for LSTM is the cell state called gates. LSTM can add and remove information to the\ngates. An input gate, output gate and forget gate are defined as following:\nft = α ( Wf. [ ht−1, xt ]+ bf),\n6\narXiv Template\nA PREPRINT\nFigure 3: The different architectures of RNN (a) One to one (b) Many to one (c) One to many (d) Many to many (e)\nMany to many.\nFigure 4: Diagram for LSTM\n7\narXiv Template\nA PREPRINT\nFigure 5: Gated Recurrent Unit\nit = α ( Wi. [ ht−1, xt ]+ bi),\n˜Ct = tanh WC. [ hC−1, xt ]+ bC),\nCt = ft∗Ct−1 + it∗˜Ct,\nOt = α ( WO. [ ht−1, xt ]+ bO),\nht = Ot∗tanh (Ct).\nLSTM models are very well accepted for processing of temporal information. There is also little modified version\nof network with peephole connections Gers and Schmidhuber [2000]. Gated recurrent unit (GRU) comes from more\nvariation of LSTM Chung et al. [2014]. GRUs are very popular among those people who work with recurrent networks.\nThe major reason behind the recognition of GRU is its less computation cost and simple model as shown in figure\n5. GRU model is also faster as it needs fewer network parameters. But LSTM provides better results when we have\nenough data and computational power [174]. So, in term of computation cost, topology and complexity, GRUs are\nlighter versions of RNN approaches than standard LSTM. The GRU model combines the input and forget gates into a\nsingle update gate and unites the cell state and hidden state with other changes. The GRU can be expressed as following:\nzt = α ( Wz. [ ht−1, xt ]),\nrt = α ( Wr. [ ht−1, xt ]),\n˜ht = tanh (W. [ rt∗ht−1, xt ]),\nht = (1 −zt)∗ht−1 + zt∗˜ht.\n3.2.4\nBidirectional Long Short Term Memory\nIn most problems of pattern recognition, it is required to access the previous and future contexts at the same time.\nFor instance, in all problems of handwriting recognition, character recognition can be performed by recognizing the\ncharacters which appear both to the left and right of it. Further the bidirectional RNNs (BRNNs) Schuster and Paliwal\n[1997] can be employed to attain the context information in left and right directions along the input sequence. In two\ndifferent hidden layers of BRNNs, one layer is employed for processing of input sequence in forward direction and the\nsubsequent in backward direction. Since both the hidden layers of BRNN are connected to the same layer of output, so\nthe access of past and future context of every point in the sequence is given by it. BRNNs were effectively used to\npredict protein structure and speech processing Schuster and Paliwal [1997], and BRNNs did better than the standard\nRNNs in different tasks of sequence learning. BLSTM is a combination of LSTM and BRNN.\n8\narXiv Template\nA PREPRINT\nFigure 6: BLSTM architecture\n3.2.5\nBLSTM Architecture\nBidirectional LSTMs process the input sequences in both directions having two sub layers for consideration of the full\ninput context. The figure 6 presents the architecture of BLSTM. Two sub layers of BLSTM can compute both forward\n(−→h ) and backward (←−h ) hidden layers. Both −→h and ←−h are combined for the computation of the output sequence (y) as\nfollowing:\n−→h t = H(Wx−\n→\nh xt + W−\n→\nh −\n→\nh\n−→h t−1 + b−\n→\nh )\n←−h t = H(Wx←\n−\nh xt + W←\n−\nh ←\n−\nh\n←−h t+1 + b←\n−\nh )\nyt = (W−\n→\nh y\n−→h t + W←\n−\nh y\n←−h t + b←\n−\nh )\n4\nResults of CNN and RNN\n4.1\nResults of CNN in Handwriting Recognition\nWith rapid development of computation techniques, the GPU-accelerated computing techniques have been exploited\nto train CNNs more efficiently. Nowadays, CNNs have already been successfully applied to handwriting recognition,\nface detection, behaviour recognition, speech recognition, recommender systems, image classification, and NLP. In\ndeep learning, although CNN classification technique of neural systems is mostly used in image classification or image\ndata, but it has attained good results for handwriting recognition too. The table 1 presents the selected handwriting\nrecognition results using CNN.\nTable 1: Results of CNN in Handwriting Recognition\nScript\nReference\nMethodology\nDataset\nAccuracy\nKannada dig-\nits\nGu Gu [2021]\nCNN based architecture\nKannada-MNIST (test\nset)\n98.77%\nRoman digits\nGupta and Bag\nGupta and Bag\n[2021]\nCNN(2 layers)\nMNIST\n99.68%\n9\narXiv Template\nA PREPRINT\nDevanagari\ndigits\nGupta and Bag\nGupta and Bag\n[2021]\nCNN(2 layers)\nCMATERdb 3.2.1\n97.56%\nBangla digits\nGupta and Bag\nGupta and Bag\n[2021]\nCNN(2 layers)\nCMATERdb 3.1.1\n96.35$\nTelugu digits\nGupta and Bag\nGupta and Bag\n[2021]\nCNN(2 layers)\nCMATERdb 3.4.1\n98.82%\nArabic digits\nGupta and Bag\nGupta and Bag\n[2021]\nCNN(2 layers)\nCMATERdb 3.3.1\n96.53%\nOdia digits\nGupta and Bag\nGupta and Bag\n[2021]\nCNN(2 layers)\nISI-Odia\n97.76%\nDevanagari\ndigits\nGupta and Bag\nGupta and Bag\n[2021]\nCNN(2 layers)\nISI-Devanagari\n98.31%\nBangla digits\nGupta and Bag\nGupta and Bag\n[2021]\nCNN(2 layers)\nISI-Bangla\n96.70%\nGujarati digits\nGupta and Bag\nGupta and Bag\n[2021]\nCNN(2 layers)\nGujarati dataset\n99.22%\nPunjabi digits\nGupta and Bag\nGupta and Bag\n[2021]\nCNN(2 layers)\nPunjabi dataset\n99.43%\nRoman digits\nKusetogullari et\nal. Kusetogullari\net al. [2020]\nCNN\nARDIS\n98.60%\nKannada dig-\nits\nGati et al. GATI\net al. [2019]\nCNN based architecture with skip\nconnections\nKannada-MNIST(Dig-\nMNIST)\n85.02%\nGurmukhi\nstrokes\nSingh et al. Singh\net al. [2020]\nCNN, self controlled RDP based\nfeatures\nGurmukhi\n94.13%\nRoman digits\nSingh et al. Singh\net al. [2020]\nCNN, self controlled RDP based\nfeatures\nUNIPEN\n93.61%\nKannada dig-\nits\nPrabhu\nPrabhu\n[2019]\nEnd-to-end training using CNN\nbased architecture\nKannada-MNIST (test\nset)\n96.80%\nMalayalam\ncharacters\nManjusha et al.\nManjusha et al.\n[2019]\nCNN based on scattering transform-\nbased wavelet filters as feature ex-\ntractor and Linear SVM as classifier\nAmrita_MalCharDb\n91.05\nRoman digits\nChowdhury\net\nal.Chowdhury\net al. [2019]\nCNN based architecture\nMNIST\n99.25%\nBangla\nchar-\nacters\nChowdhury\net\nal.Chowdhury\net al. [2019]\nCNN based architecture\nBanglalekha-isolated\n91.81%\nBangla\nchar-\nacters\nChowdhury\net\nal.Chowdhury\net al. [2019]\nCNN based architecture\nEkush\n95.07%\nBangla\nchar-\nacters\nChowdhury\net\nal.Chowdhury\net al. [2019]\nCNN based architecture\nCMATERdb 3.1.2\n93.37%\nBangla numer-\nals\nGupta\net\nal.\nGupta\net\nal.\n[2019]\nMulti-objective optimisation to find\nthe informative regions of character\nimage + CNN features\nIsolated\nhandwritten\nBangla numerals\n96.54%\nBangla\nchar-\nacters\nGupta\net\nal.\nGupta\net\nal.\n[2019]\nMulti-objective optimisation to find\nthe informative regions of character\nimage + CNN features\nIsolated\nhandwritten\nBangla basic characters\n85.19%\n10\narXiv Template\nA PREPRINT\nEnglish\nnumerals\nGupta\net\nal.\nGupta\net\nal.\n[2019]\nMulti-objective optimisation to find\nthe informative regions of character\nimage + CNN features\nIsolated\nhandwritten\nEnglish numerals\n97.87%\nDevanagari\ncharacters\nGupta\net\nal.\nGupta\net\nal.\n[2019]\nMulti-objective optimisation to find\nthe informative regions of character\nimage + CNN features\nIsolated\nhandwritten\nDevanagari characterss\n87.23%\nRoman digits\nChakraborty et al.\nChakraborty et al.\n[2019]\nfeature map reduction in CNN\nMNIST\n99.19%\nRoman digits\nArora and Bhatia\nArora and Bhatia\n[2018]\nCNN, Keras\nMNIST\n95.63%,\n99.20%\nMalayalam\ncharacters\nManjusha et al.\nManjusha et al.\n[2018]\nCNN based on scattering transform-\nbased wavelet filters (ScatCNN)\nMalayalam_DB\n93.77%\nRoman digits\nManjusha et al.\nManjusha et al.\n[2018]\nScatCNN\nMNIST\n99.31%\nBangla numer-\nals\nManjusha et al.\nManjusha et al.\n[2018]\nScatCNN\nISI\n99.22%\nChinese char-\nacters\nManjusha et al.\nManjusha et al.\n[2018]\nScatCNN\nCASIA HWDB1.1\n92.09%\nEnglish words\nKang et al. Kang\net al. [2018]\nAttention based sequence to se-\nquence model\nIAM\n82.55%\nEnglish char-\nacters\nKang et al. Kang\net al. [2018]\nAttention based sequence to se-\nquence model\nIAM\n93.12%\nRoman digits\nSarkhel\net\nal.\nSarkhel\net\nal.\n[2017]\nA multi-column multi-scale CNN\narchitecture + SVM\nCMATERdb 3.4.1\n99.50%\nRoman digits\nSarkhel\net\nal.\nSarkhel\net\nal.\n[2017]\nA multi-column multi-scale CNN\narchitecture + SVM\nMNIST\n99.74%\nEnglish(mostly)\nwords\nPoznanski\nand\nWolf\nPoznanski\nand Wolf [2016]\nCNN-N-Gram\nIAM\n93.55%\nEnglish(mostly)\ncharacters\nPoznanski\nand\nWolf\nPoznanski\nand Wolf [2016]\nCNN-N-Gram\nIAM\n96.66%\nFrench words\nPoznanski\nand\nWolf\nPoznanski\nand Wolf [2016]\nCNN-N-Gram\nRIMES\n93.10%\nFrench charac-\nters\nPoznanski\nand\nWolf\nPoznanski\nand Wolf [2016]\nCNN-N-Gram\nRIMES\n98.10%\nHangual\nKim and Xie Kim\nand Xie [2015]\nDeep convolutional neural netwok\n(DCNN)\nSERI95a and PE92\n95.96%,\n92.92%\nRoman digits\nWan et al. Wan\net al. [2013]\nCNN based architecture with Drop-\nConnect layer\nMNIST\n99.79%\nRoman digits\nKrizhevsky et al.\nKrizhevsky et al.\n[2012]\nCNN, LeNet-5 system\nMNIST\n99.10%\nOne of the classical models of CNN was the LeNet-5 system. Its accuracy rate on MNIST data-set was above 99%.\nIt was extensively used for identification of handwritten checks on banks, but it could not recognize large images.\nWith the advancement of technology, Graphics Processing Unit (GPU) was developed, then in 2012, Krizhevsky et al.\nKrizhevsky et al. [2012] employed an efficient GPU supported program for solving ImageNet problem, which also\nmade CNN application popular. Actually, one of the problems of using CNN was that it took much time to train the\n11\narXiv Template\nA PREPRINT\nnetwork because of the many hidden nodes in the network. But the GPUs’ faster parallel computing, overcame this\nproblem too. A CNN based architecture with DropConnect layer was proposed by Wan et al. Wan et al. [2013] in\n2013, where they attained 99.79% recognition accuracy for MNIST digits. DropConnect generalizes Hinton et al.’s\nDropout Hinton et al. [2012] to the complete connectivity structure of a fully connected neural network (NN) layer.\nThey provided both empirical results and theoretical justification for showing that DropConnect helps to regularize\nlarge NN models. As deep convolutional neural netwok (DCNN) comprises many layers, so it can model much more\ncomplicated functions than shallow networks. Motivating from DCNN’s great results in various machine learning and\npattern recognition problems, in 2015, Kim and Xie Kim and Xie [2015] developed a new recognizer based on deep\nCNN to improve the Hangual handwrititng recognition performance. They built their own Hangul recognizers based\non DCNN and developed various novel techniques for performance and networks training speed improvement. They\nevaluated their proposed recognizers on image datasets of Hangul, named SERI95a and PE92, where recognition results\nas 95.96% on SERI95a and 92.92% on PE92 are attained. In 2016, Poznanski and Wolf Poznanski and Wolf [2016]\nproposed a CNN-N-Gram based system for handwriting recognition, and recognized handwritten English and French\nwords with 93.55% and 93.10% accuracy, respectively. Different persons’ variation in writing styles and single person’s\nvariation in handwriting from time to time, make recognition of the local invariant patterns of a handwritten digit\nand character difficult. For this purpose, in 2017, Sarkhel et al. Sarkhel et al. [2017] proposed a non-explicit feature\nbased approach, specifically it was a multi-column multi-scale CNN based architecture. Their proposed approach has\nbeen validated on different datasets of isolated handwritten digits and characters of Indic scripts, and best results are\nattained on MNIST dataset that is 99.74% without any data augmentation to the original dataset. Inspired from the deep\nlearning’s role in image classification, in 2018, Arora and Bhatia Arora and Bhatia [2018] used Keras for classification\nof handwritten images of MNIST dataset. In fact, they used feed forward NN and CNN to extract features and training\nthe model, it used Stochastic Gradient Descent for optimization. In their work, for classification of handwritten digits,\nit is observed that CNN attained greater accuracy in comparison to feed forward, and CNN obtained 95.63% and\n99.20% accuracy for 5 and 20 iterations, respectively. Malayalam handwritten character recognition is very challenging,\ndue to the isomorphic nature of character classes and a large number of character classes. To recognize handwritten\nMalayalam characters, in 2018, Manjusha et al. Manjusha et al. [2018] replaced the convolutional feature maps of first\nlayer in CNN architecture with scattering transform-based feature maps, and attained 93.77% as recognition accuracy.\nScattering transform can compute stable invariant description of input patterns where it applies a series of wavelet\ndecomposition, modulus and averaging operations. Their proposed hybrid CNN Manjusha et al. [2018] also achieved\nabove 99% recognition accuracy for MNIST digits and ISI Bangla numerals datasets. A convolve, attend and spell, an\nattention based sequence to sequence model to recognize handwritten words without the use of HTR system’s traditional\ncomponents, as connectionist temporal classification, language model nor lexicon was presented by Kang et al. Kang\net al. [2018] in 2018. It was an end-to-end system that contained an encoder, decoder and attention mechanism, and\nit outperformed most of the existing best results, and it attained 93.12% character recognition accuracy and 82.55%\nword recognition accuracy for IAM dataset on word-level. In 2019, Chowdhury et al. Chowdhury et al. [2019] used\nCNN to develop a handwritten character recognition model, and attained 99.25% accuracy for MNIST digits and\n91.81% accuracy for Banglalekha-isolated characters. In 2019, Gupta et al. Gupta et al. [2019] proposed an opposition\nbased multi-objective optimisation search algorithm to find the informative regions of character images, where they\nalso used CNN features to evaluate the proposed work on different Indic scripts’ isolated units of handwriting and\nobtained good results for isolated Bangla basic characters, Bangla numerals, English numerals, and isolated Devanagari\ncharacters. Considering the research efforts for Malayalam character handwriting recognition, Manjusha et al. Manjusha\net al. [2019] developed a handwritten character image database of Malayalam language script in 2019. In their work,\nrecognition experiments were conducted by using different techniques of feature extraction. Among the used feature\ndescriptors, scattering CNN attained the best recognition accuracy of 91.05%. In 2019, Prabhu Prabhu [2019] created a\nnew dataset for handwritten digits of Kannada language, which is called Kannada-MNIST dataset, and attained best\nresults as 96.80% using CNN based architecture. In 2019, Gati et al. GATI et al. [2019] described how great results and\nperformance can be attained on a very challenging Dig-MNIST dataset using a custom-built model based on the skip\nCNN architecture, where 85.02% recognition accuracy was attained using proposed model trained on Kannada-MNIST\nand tested on the Dig-MNIST dataset without any pre-processing. In 2019, Chakraborty et al. Chakraborty et al. [2019]\nproposed for reduction of the feature maps which are used in training the CNN for reduction of computation time and\nstorage space. Experimental results proved that the time requirement for training the CNN decreased with reduction in\nnumber of feature maps without affecting the accuracy much, and above 99% accuracy rate was attained for MNIST\ndigit dataset. In 2020, Kusetogullari et al. Kusetogullari et al. [2020] introduced different datasets of digits in ARDIS,\nand attained best recognition accuracy for digits using CNN that is 98.60%. A novel self-controlled RDP point based\nsmaller size feature vector approach to recognize online handwriting was proposed by Singh et al. Singh et al. [2020]\nin 2020, where they employed a CNN based network that trains in a few minutes on a single machine without GPUs\ndue to the use of Conv1Ds, and it attained 94.13% and 93.61% recognition rates for Gurmukhi and UNIPEN datasets,\nrespectively. Recently, in 2021, Gu Gu [2021] proposed a CNN based model to classify the Kannada-MNIST dataset\nand made analysis of the proposed model performance on training, testing and validation sets. The CNN model was\n12\narXiv Template\nA PREPRINT\ntrained on more than 51000 images and it was validated over 9000 images for 30 epochs, where the CNN model attained\na testing accuracy of 98.77%, and it outperformed other methods as SVM, logistic regression and a CNN baseline.\nThis study is the evidence for the capability of proposed CNN model, and it also demonstrates the benefit of using a\nCNN architecture over other classification methods when performing handwritten character recognition jobs. A script\nindependent CNN based system to recognize numerals was developed by Gupta and Bag Gupta and Bag [2021] in 2021,\nit is a system to recognize handwritten digits written in multi languages and it is independent of fusion where it has just\n10 classes corresponding to every numeric digit. This was the first study that addressed the problem of multilingual\nnumerals recognition, where experimental results attained the accuracy of 96.23% for eight Indic scripts collectively.\nThe attained results are promising and demonstrates the hypothesis that multilingual handwritten numeral recognition is\nvoid with CNN.\n4.2\nResults of RNN in Handwriting Recognition\nRNNs are very powerful machine learning models and have found use in a wide range of areas where sequential data is\ndealt. RNNs have been widely used in prediction problems, machine translation, face detection, speech Recognition,\nOCR based image recognition and handwriting recognition etc. RNNs have received great success when working\nwith sequential data, generally in the field of handwriting recognition. The table 2 presents the selected handwriting\nrecognition results using RNN.\nTable 2: Results of RNN in Handwriting Recognition\nScript\nReference\nMethodology\nDataset\nAccuracy\nEnglish words\nPham et al. Pham\net al. [2014]\nLSTM with dropout at the topmost\nhidden layer\nIAM\n60.52%\nEnglish words\nPham et al. Pham\net al. [2014]\nLSTM with dropout at multiple lay-\ners\nIAM\n68.56%\nFrench words\nPham et al. Pham\net al. [2014]\nLSTM with dropout at the topmost\nhidden layer\nRimes\n63.97%\nFrench words\nPham et al. Pham\net al. [2014]\nLSTM with dropout at multiple lay-\ners\nRimes\n72.99%\nEnglish char-\nacters\nPham et al. Pham\net al. [2014]\nLSTM with dropout at the topmost\nhidden layer\nIAM\n81.55%\nEnglish char-\nacters\nPham et al. Pham\net al. [2014]\nLSTM with dropout at multiple lay-\ners\nIAM\n86.08%\nFrench charac-\nters\nPham et al. Pham\net al. [2014]\nLSTM with dropout at the topmost\nhidden layer\nRimes\n87.83%\nFrench charac-\nters\nPham et al. Pham\net al. [2014]\nLSTM with dropout at multiple lay-\ners\nRimes\n91.38%\nEnglish words\nDoetsch\net\nal.\nDoetsch\net\nal.\n[2014]\nLSTM-RNN\nIAM\n87.80%\nFrench words\nDoetsch\net\nal.\nDoetsch\net\nal.\n[2014]\nLSTM-RNN\nIAM\n87.10%\nBangla\nchar-\nacters\nChollet\net\nal.\nChollet\net\nal.\n[2015]\nLSTM\nBanglalekha-isolated\n87.41%\nBangla\nchar-\nacters\nChollet\net\nal.\nChollet\net\nal.\n[2015]\nLSTM\nEkush\n93.06%\nArabic words\nChherawala et al.\nChherawala et al.\n[2016]\nWeighted Vote Combination of\nRNN\nFN/ENIT\n96%\nFrench words\nChherawala et al.\nChherawala et al.\n[2016]\nWeighted Vote Combination of\nRNN\nRIMES\n95.2%\nEnglish words\nShkarupa\net\nal.\nShkarupa\net\nal.\n[2016]\nCTC+BLSTM\nhandwritten\nmedieval\nLatin text\n78.10%\n13\narXiv Template\nA PREPRINT\nEnglish words\nShkarupa\net\nal.\nShkarupa\net\nal.\n[2016]\nSequence to sequence+LSTM\nhandwritten\nmedieval\nLatin text\n72.79%\nEnglish\nWords\nWigington et al.\nWigington et al.\n[2017]\nRNN+CTC\nIAM\n80.93%\nFrench Words\nWigington et al.\nWigington et al.\n[2017]\nRNN+CTC\nRimes\n88.71%\nEnglish char-\nacters\nWigington et al.\nWigington et al.\n[2017]\nRNN+CTC\nIAM\n93.93%\nFrench charac-\nters\nWigington et al.\nWigington et al.\n[2017]\nRNN+CTC\nRimes\n96.91%\nEnglish words\nDutta et al. Dutta\net al. [2018]\nHybrid CNN-RNN network\nIAM\n87.39%\nEnglish char-\nacters\nDutta et al. Dutta\net al. [2018]\nHybrid CNN-RNN network\nIAM\n95.12%\nFrench words\nDutta et al. Dutta\net al. [2018]\nHybrid CNN-RNN network\nRIMES\n92.96%\nFrench charac-\nters\nDutta et al. Dutta\net al. [2018]\nHybrid CNN-RNN network\nRIMES\n97.68%\nEnglish words\nDutta et al. Dutta\net al. [2018]\nHybrid CNN-RNN network\nGW\n87.02%\nEnglish char-\nacters\nDutta et al. Dutta\net al. [2018]\nHybrid CNN-RNN network\nGW\n95.71%\nEnglish words\nKrishnan\net\nal.\nKrishnan\net\nal.\n[2018]\nConvolutional recurrent neural net-\nwork (CRNN)\nIAM\n94.90%\nEnglish words\nSueiras\net\nal.\nSueiras\net\nal.\n[2018]\nSequence to sequence NN\nIAM\n87.30%\nFrench words\nSueiras\net\nal.\nSueiras\net\nal.\n[2018]\nSequence to sequence NN\nIAM\n93.40%\nEnglish char-\nacters\nKrishnan\net\nal.\nKrishnan\net\nal.\n[2018]\nCRNN\nIAM\n97.44%\nBengali words\nGhosh\net\nal.\nGhosh\net\nal.\n[2019]\nBLSTM\nBengali\ndataset\nof\n120000 words\n95.24% (lexi-\ncon 1K)\nBengali words\nGhosh\net\nal.\nGhosh\net\nal.\n[2019]\nBLSTM\nBengali\ndataset\nof\n120000 words\n90.78% (lexi-\ncon 5K)\nBengali words\nGhosh\net\nal.\nGhosh\net\nal.\n[2019]\nBLSTM\nBengali\ndataset\nof\n120000 words\n87.38% (lexi-\ncon 10K)\nDevanagari\nwords\nGhosh\net\nal.\nGhosh\net\nal.\n[2019]\nBLSTM\nBengali\ndataset\nof\n120000 words\n99.50% (lexi-\ncon 1K)\nDevanagari\nwords\nGhosh\net\nal.\nGhosh\net\nal.\n[2019]\nBLSTM\nBengali\ndataset\nof\n120000 words\n96.27% (lexi-\ncon 5K)\nDevanagari\nwords\nGhosh\net\nal.\nGhosh\net\nal.\n[2019]\nBLSTM\nBengali\ndataset\nof\n120000 words\n94.34% (lexi-\ncon 10K)\n14\narXiv Template\nA PREPRINT\nEnglish words\nGeetha\net\nal.\nGeetha\net\nal.\n[2021]\nCNN-RNN\nIAM\n95.20%\nEnglish char-\nacters\nGeetha\net\nal.\nGeetha\net\nal.\n[2021]\nCNN-RNN\nIAM\n97.48%\nFrench words\nGeetha\net\nal.\nGeetha\net\nal.\n[2021]\nCNN-RNN\nRIMES\n98.14%\nFrench charac-\nters\nGeetha\net\nal.\nGeetha\net\nal.\n[2021]\nCNN-RNN\nRIMES\n99.35%\nIn 2014, Pham et al. Pham et al. [2014] presented that the dropout can improve the performance of RNN greatly.\nThe word recognition networks having dropout at the topmost layer improved the character and word recognition by\n10% to 20%, and when dropout used with multiple LSTM layers, then it further improved the performance by 30%\nto 40%. They reported the best results on Rimes dataset as 91.38% and 72.99% for character and word recognition,\nrespectively. The simple RNN was modified by Koutnik et al. Koutnik et al. [2014] in 2014, and introduced a powerful\nClockwork RNN (CW-RNN), where hidden layers were divided into different modules and every layer processed the\ninputs at its own temporal granularity, which made computations over prescribed clock rate only. The CW-RNN reduced\nthe number of parameters of simple RNN, and also improved the speed and performance significantly. For online\nhandwriting recognition, CW-RNN outperformed the simple RNN and LSTM, and improved recognition accuracy by\n20% for English sentences. A modified topology for LSTM-RNN that controlled the shape of squashing functions\nin gating units was demonstrated by Doetsch et al. Doetsch et al. [2014] in 2014. An efficient framework of mini\nbatch training at sequence level in combination with sequence chunking approach was also proposed by them. They\nevaluated their framework on IAM and RIMES datasets by using GPU based implementation, and it was three times\nfaster in training RNN models which outperformed the state-of-the-art recognition results, where 87.80% and 87.10%\nrecognition accuracies were attained for handwritten words of IAM and RIMES datasets, respectively. In 2015, an\nimage classification system using LSTM with Keras was built and it was applied to handwritten Bangla character\ndatasets, where it achieved 87.41% and 93.06% recognition accuracy for two different datasets of Bangla characters\nChollet et al. [2015]. This architecture consisted of one LSTM layer having 128 units, had activation function as ’ReLU’\nand recurrent activation function had been set to ’hard sigmoid’. In 2016, Chherawala et al. Chherawala et al. [2016]\nproposed a novel method to extract the promising features of handwritten word images. They proposed a framework to\nevaluate feature set based on collaborative setting. In their work, they employed weighted vote combination of RNN\nclassifiers, where particular feature set was used to train every RNN. The major contribution of their study was the\nquantification of the feature sets’ importance through weight combination, and it also showed their complementarity\nand strength. They used RNN because of the state-of-the-art results, and provided the first feature set benchmark for\nRNN classifier. They evaluated different feature sets on different datasets of Arabic and Latin scripts, and attained best\naccuracies as 96% and 95.2% for IFN/ENIT and RIMES datasets, respectively. For historic handwritten Latin text\nrecognition, two important approaches based on RNN were proposed by Shkarupa et al. Shkarupa et al. [2016] in 2016.\nTheir first approach used connectionist temporal classification (CTC) output layer, and attained 78.10% word level\naccuracy. The other approach used sequence-to-sequence learning, and attained 72.79% word level accuracy. In their\nwork, when CTC approach was used with BLSTM, it outperformed the sequence-to-sequence based approach used\nwith LSTM. Their proposed system of handwriting recognition considered unsegmented word images as input and\nprovided decoded strings as output. In 2017, Wigington et al. Wigington et al. [2017] presented two data normalization\nand augmentation techniques, and these were used with CNN and LSTM. These techniques reduced the character\nerror rate and word error rate significantly, and significant results were reported for handwriting recognition tasks.\nThe novel normalization technique was applied to both word and line images. Their proposed approaches attained\nhigh accuracies for both characters and words over several existing studies, where IAM dataset character and word\nlevel recognition accuracy was reported as 96.97% and 94.39%, respectively. In 2018, Dutta et al. Dutta et al. [2018]\nproposed a modified CNN-RNN based hybrid architecture and mainly focussed for effective training with: (a) network’s\nefficient initialization with the use of synthetic data in pretraining, (b) slant correction with image normalization and\n(iii) domain specific transformation of data and distortion to learn important invariances. In their work, a detailed\nablation study for analysis of the contribution of individual module was performed and the results for unconstrained line\nand word recognition on IAM, RIMES and GW datasets were presented at par literature, where they attained lexicon\nfree word recognition accuracies as 87.39%, 92.96% and 87.02% on these three datasets, respectively. To represent\nhandwritten word images efficiently, an HWNet v2 architecture was presented by Krishnan et al. Krishnan et al. [2018]\nin 2018. The state-of-the-art attribute embedding was enabled by this work. An end-to-end embedding framework was\ndemonstrated by it, and it used textual representation and synthetic image for learning complementary information\n15\narXiv Template\nA PREPRINT\nto embed text and images. It also improved the word recognition performance using a convolutional recurrent neural\nnetwork (CRNN) architecture, by using the synthetic data and spatial transformer layer, and attained character and\nword level accuracies on IAM dataset as 97.44% and 94.90%, respectively. In 2018, a system based on sequence to\nsequence architecture with convolutional network was proposed by Sueiras et al. Sueiras et al. [2018] to recognize\noffline handwriting. This model had three major components, where first convolutional network extracted relevant\nfeatures of the characters present in the word. Then RNN captured the sequential relationships of extracted features.\nThirdly, the input word was predicted by decoding the sequence of characters with another RNN. Their proposed system\nwas tested on handwritten words of IAM and RIMES datasets, and attained the recognition accuracy as 87.3% and\n93.6%, respectively, where no language model was used and results were attained with closed dictionary. In 2019,\nGhosh et al. Ghosh et al. [2019] presented a new online handwritten word recognition system based on LSTM and\nBLSTM versions of RNN, and recognized Devanagari and Bengali words in lexicon dependent environment with above\n90% (for lexicon size 5K) recognition accuracy. Their proposed approach divided every handwritten word into upper,\nmiddle, and lower zones horizontally, and reduces the basic stroke order variations with in a word. Further, they also\nused various structural and directional features of different zones’ basic strokes of handwritten words. In 2021, Geetha\net al. Geetha et al. [2021] proposed a hybrid model to recognize handwritten text by utilizing deep learning that used\nsequence-to-sequence approach. It used various features of CNN and RNN-LSTM. It used CNN to extract features of\nhandwritten text images. The extracted features were then modelled with a sequence-to-sequence approach and fed in\nRNN-LSTM to encode the visual features and decoded the sequence of letters present in handwritten image. Their\nproposed model was tested with IAM and RIMES datasets, where above 95% accuracy was attained using CNN-RNN\nfor handwritten words of English and French.\n5\nGeneral Observations\nIn this section, an analysis of various techniques used for deep learning architectures, and deep learning use in\nhandwriting recognition and other related fields are presented.\n• Deep learning has been effectively used in various emerging fields to solve complex problems of real world\nwith different deep learning architectures. Deep learning architectures employ different activation functions\nfor the achievement of state-of-the-art performances, to perform various computations between the hidden\nand output layers of deep learning architectures. Further, the advancement in deep learning architectures’\nconfiguration brings new challenges, particularly for the selection of right activation functions to perform in\nvarious domains from the classification of objects Krizhevsky et al. [2012] Szegedy et al. [2015b] He et al.\n[2015] Md Noor et al. [2017], speech recognition Sainath et al. [2015] Graves et al. [2013], segmentation\nBadrinarayanan et al. [2017] Hu et al. [2018], machine translation Vinyals et al. [2014] Liu and Zhang [2018],\nscene description Karpathy and Fei-Fei [2015] Pinheiro and Collobert [2014], weather forecasting Grover\net al. [2015] Hossain et al. [2015], cancer detection Albarqouni et al. [2016] Wang et al. [2016] Cruz-Roa et al.\n[2013], self-driving cars Uçar et al. [2017] Chen et al. [2015] and other adaptive systems. With such challenges,\nthe comparison of present trends in the application of activation functions employed in deep learning, portrays\na gap of literature in this direction.\n• Deep learning performs most of the things that are familiar to machine learning approaches. Deep learning\ntechniques can be used both in supervised learning based applications that require prediction of one or more\noutcomes or labels related to each data point in place of regression approaches, as well as in unsupervised\nlearning based applications that need summary, explanation and identification of interesting patterns in a data\nset in the form of clustering.\n• Deep learning techniques surpass the existing state of the art in various studies of healthcare like patient and\ndisease categorization, basic biological study, genomics and treatment development.\n• Despite deep learning has dominated over competing machine learning approaches in many fields and made\nquantitative improvements in predictive performance, deep learning has yet to solve many problems. Deep\nlearning has not completely transformed the study of human disease. It has yet to realize its trans-formative\nstrength and to encourage a strategic inflection point.\n• Using deep learning in speech recognition, there have been great performance improvements with error rates\ndropped from more than 20% to less than 6% and exceeded human performance in the past years Xiong et al.\n[2017] Saon et al. [2017].\n• In medical imaging, diabetic macular oedema Gulshan et al. [2016], diabetic retinopathy Gulshan et al. [2016],\nskin lesion Esteva et al. [2017] and tuberculosis Lakhani and Sundaram [2017], deep learning based classifiers\nare greatly successful and can be compared to clinical performance. Some of these areas, we have surpassed\nthe lofty bar than others, generally, those that are more similar to the non-biomedical tasks that are now\n16\narXiv Template\nA PREPRINT\nmonopolized by deep learning. Deep learning can point experts to the most challenging cases that require\nmanual review, even if the risk of false negatives must be addressed.\n• Deep learning techniques also prioritize experiments and assist discovery. As an illustration, in chemical\nscreening for discovery of drugs, a deep learning system can successfully identify hundreds of target-specific,\nactive small molecules from an immense search space and it would have great practical value even though its\ntotal precision is modest.\n• The deep neural networks can be built resistant to the adversarial attacks. Further, there is also possibility to\ndesign reliable adversarial training methods. Thus, the findings from existing deep learning studies provide\nmotivation for having adversarial robust deep learning models within current reach.\n• Deep learning has witnessed for a great achievement of human level performance across a number of domains\nin biomedical science. But deep neural networks as other machine learning algorithms are also prone to errors\nthat are also made by humans most likely, such as miss-classification of adversarial examples Szegedy et al.\n[2013] Goodfellow et al. [2014], and it can be considered that the semantics of the objects presented cannot be\ncompletely understood by these algorithms. But the alliance between deep learning algorithms and human\nexperts addresses most of these challenges and can result in better performance than either individually Wang\net al. [2016].\n• We are confident about deep learning’s future in machine learning. It is certain that the deep learning will\nsurely revolutionize these fields, but given how rapidly these areas are evolving, we are optimistic that its full\npotential has not been explored yet. There are various challenges beyond improving the training and predictive\naccuracies. Ongoing research has begun to address most problems in these directions and proved that they are\nnot insuperable.\n• Deep learning provides a flexible way to model data in its natural form, as an illustration, molecular graphs\ninstead of pre-computed bit vectors for drug discovery and longer DNA sequences instead of k-mers for\nTF binding forecasting. This kind of flexible input feature interpretations have incited creative modelling\napproaches that are not feasible with rest of machine learning techniques. In forthcoming years, large\ncollections of input data can be summarized into interpretable models by deep learning algorithms, and it will\nencourage scientists to ask those questions which they do not know how to ask.\n• Although the deep learning has yet to attain more, existing studies show that it possesses the capability of faster\nand more reliable results. That power may well trigger a shift away from the currently employed decision\nsupport methods, such as support vector machines and k-nearest neighbour, towards deep learning.\n• Deep leaning training algorithms have a high computational complexity and it results to high run time\ncomplexity that translates into a long training time use. After choosing the architecture, there is always a\nneed to adjust the tuning parameters. The model is influenced by both the structure selection and parameter\nadjustment. So, there is need to have many test runs. Reducing the deep leaning models’ training phase is\nan active area of research. One of the challenges in deep learning is increasing the training process speed\nin a parallel distributed processing system Chen and Lin [2014]. As the network for individual processors\nbecomes the bottle neck Najafabadi et al. [2015] then GPUs are used to reduce the network latency Bergstra\net al. [2011].\n• Deep learning faces certain challenges as: using deep learning for big data analysis, dealing causality in\nlearning, scalability of approaches in deep learning, data generating ability when data does not exist for\nlearning the system, need of energy efficient techniques for special purpose devices, learning from different\ndomains or models together.\n• Although, present deep learning models works splendidly in various applications, but the solid theory of deep\nlearning still lacks. It is not mostly known that why and how it works essentially. It is required to make more\nendeavours to investigate the basic principles of deep learning. In the meantime, it is very worth on exploring\nhow to leverage natural visual perception mechanism for further improvement in the design of deep learning\nmodels.\n6\nConclusion\nThe recent decade observed an increasingly rapid progress in technology, mainly backed up by the advancements in the\narea of deep learning and artificial intelligence. The present paper presented various architectures of deep learning and\nsurveyed the current state-of-the-art on deep learning technologies used in handwriting recognition domain. After\nreviewing so many papers, the present study is able to distil the perfect deep learning methods and architectures for\ndifferent handwriting recognition tasks and general observations on other related application areas too. The CNN and\n17\narXiv Template\nA PREPRINT\nits derivatives are the out performers in most image analysis areas, and RNN and its derivatives are out performers in\ndealing with sequence data. Further, an outstanding conclusion can be drawn that the exact architecture of deep learning\nis an important determinant for finding a good solution in many problems. The present survey not only given a snapshot\nof the existing deep learning research status in handwriting recognition but also made an effort for identification of the\nfuture roadway for intended researchers. The findings indicate that there are remarkable opportunities in the deep\nlearning research and it shows that they will not disappear anytime soon. So, the present study encourages future\nresearchers that are interested in the area to start exploring as it currently seems to be wide open for new studies.\nReferences\nAlan K. Mackworth David L. Poole. Artificial Intelligence: Foundations of Computational Agents. Cambridge\nUniversity Press, 2010.\nShivashankar B. Nair Elaine Rich, Kevin Knight. Artificial Intelligence. Tata McGraw Hill, 2010.\nOlaf Zawacki-Richter, Victoria I. Marín, Melissa Bond, and Franziska Gouverneur. Systematic review of research on\nartificial intelligence applications in higher education – where are the educators? International Journal of Educational\nTechnology in Higher Education, 16(1):39, Oct 2019. ISSN 2365-9440. doi:10.1186/s41239-019-0171-0. URL\nhttps://doi.org/10.1186/s41239-019-0171-0.\nL. Chen, P. Chen, and Z. Lin. Artificial intelligence in education: A review. IEEE Access, 8:75264–75278, 2020.\ndoi:10.1109/ACCESS.2020.2988510.\nNils J. Nilsson. The Quest for Artificial Intelligence: A history of ideas and achievements. Cambridge University Press,\n2010.\nMichael A. Goodrich and Alan C. Schultz. Human-robot interaction: A survey. 1(3):203–275, January 2007. ISSN\n1551-3955. doi:10.1561/1100000005. URL https://doi.org/10.1561/1100000005.\nA. L. Buczak and E. Guven. A survey of data mining and machine learning methods for cyber security intrusion\ndetection. IEEE Communications Surveys Tutorials, 18(2):1153–1176, 2016. doi:10.1109/COMST.2015.2494502.\nArash Bahrammirzaee. A comparative survey of artificial intelligence applications in finance: Artificial neural networks,\nexpert system and hybrid intelligent systems. International Journal of Neural Computing and Application, Available\nonline 20 June 2010, 11 2010. doi:10.1007/s00521-010-0362-z.\nY. Bengio, A. Courville, and P. Vincent. Representation learning: A review and new perspectives. IEEE Transactions\non Pattern Analysis and Machine Intelligence, 35(8):1798–1828, 2013. doi:10.1109/TPAMI.2013.50.\nDavid Brougham and Jarrod Haar. Smart technology, artificial intelligence, robotics, and algorithms (stara): Em-\nployees’ perceptions of our future workplace. Journal of Management & Organization, 24(2):239–257, 2018.\ndoi:10.1017/jmo.2016.55.\nJuan Gustavo Corvalan. Artificial intelligence: Challenges and opportunities-prometea: The first artificial intel-\nligence of latin america at the service of the justice system. Revista de Investigações Constitucionais, 5:295\n– 316, 04 2018. ISSN 2359-5639. URL http://www.scielo.br/scielo.php?script=sci_arttext&pid=\nS2359-56392018000100295&nrm=iso.\nZoubin Ghahramani. Probabilistic machine learning and artificial intelligence. Nature, 521(7553):452–459, May 2015.\nISSN 1476-4687. doi:10.1038/nature14541. URL https://doi.org/10.1038/nature14541.\nD. Castelvecchi.\nCarlton McDonald. Machine learning: a survey of current techniques. Artificial Intelligence Review, 3(4):243–280,\nDec 1989. ISSN 1573-7462. doi:10.1007/BF00141197. URL https://doi.org/10.1007/BF00141197.\nF. Musumeci, C. Rottondi, A. Nag, I. Macaluso, D. Zibar, M. Ruffini, and M. Tornatore. An overview on application of\nmachine learning techniques in optical networks. IEEE Communications Surveys Tutorials, 21(2):1383–1408, 2019.\ndoi:10.1109/COMST.2018.2880039.\nC.M. Bishop. Pattern recognition and machine learning. Springer, 2006.\nO. Chapelle, B. Schölkopf, and A. Zien. Semi-supervised learning (1st ed.). The MIT Press, 2006.\nR. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa. Natural language processing (almost)\nfrom scratch. Journal of Machine Learning Research, 12:2493–2537, 2011.\nJ. Du, C.X. Ling, and Z.H. Zhou. When does cotraining work in real data? IEEE Transactions on Knowledge and Data\nEngineering.\n18\narXiv Template\nA PREPRINT\nY. Freund and R.E. Schapire. A decision-theoretic generalization of on-line learning and an application to boosting.\nJournal of Computer and System Sciences, 55(1):119–139, 1997.\nN. Grira, M. Crucianu, and N. Boujemaa. Unsupervised and semisupervised clustering: A brief survey. In 7th ACM\nSIGMM international workshop on multimedia information retrieval, 2004.\nI. Guyon and A. Elisseeff. An introduction to feature extraction. Feature extraction, page 1–25, 2006.\nY. LeCun, Y. Bengio, and G. Hinton. Deep learning. Nature, 521:436–444, 2015a.\nF. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss,\nV. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn:\nMachine learning in python. Journal of Machine Learning Research, 12:2825–2830, 2011.\nV. Vapnik. Statistical learning theory (Vol. 1). New York: Wiley, 1998.\nO. Abdel-Hamid, A. R. Mohamed, L. Deng H. Jiang, G. Penn, and D. Yu. Convolutional neural networks for speech\nrecognition. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 22(10):1533–1545, 2014.\nK. Cho, B. Merrienboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Yoshua Bengio. Learning phrase\nrepresentations using rnn encoder-decoder for statistical machine translation. In The Conference on Empirical\nMethods in Natural Language Processing, pages 1724–1734, 2014.\nY. Zhao, X. Jin, and X. Hu. Recurrent convolutional neural networks for speech processing. In IEEE International\nConference on Acoustics, Speech and Signal Processing, pages 5300–5304, 2017.\nWS. McCulloch and W. Pitts. A logical calculus of the ideas immanent in nervous activity. Bull Math Biophys, 5, 1943.\nD.O. Hebb. The organization of behavior. New York: Wiley & Sons, 1949.\nF. Rossenblatt. The perceptron: A probabilistic model for information storage and organization in the brain. Psycholog-\nical Review, 65(6):386–408, 1958.\nB. Windrow and M.E. Hoff. Adaptive switching circuits. IRE WESCON Convention Record, 4(96-104), 1960.\nT. Kohonen. Self-organized formation of topologically correct feature maps. Biol. Cybern, 43:59–69, 1982.\nJ.J. Hopfield. Neural networks and physical systems with emergent collective computational abilities. Proceedings of\nthe National Academy of Sciences of the USA, 79(8):2554–2558, 1982.\nD. Rumelhart, G. Hinton, and R. Williams. Learning representations by back-propagating errors. nature, 323:533–536,\n1986.\nGail A. Carpenter and Stephen Grossberg. The art of adaptive pattern recognition by a self-organizing neural network.\nComputer, 21(3):77–88, 1988.\nD.S. Broomhead and D. Lowe. Multivariable functional interpolation and adaptive networks. Complex Systems, 2:\n321–355, 1988.\nKunihiko Fukushima. Neocognitron trained with winner-kill-loser rule. Neural Netw., 23(7):926–938, 2010.\nSimon Haykin. Neural Networks: A Comprehensive Foundation. Prentice Hall PTR, 2nd edition, 1998.\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural\nnetworks. In Advances in neural information processing systems, pages 1097–1105, 2012.\nChristian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent\nVanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In The IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), June 2015a.\nYaniv Taigman, Ming Yang, Marc’Aurelio Ranzato, and Lior Wolf. Deepface: Closing the gap to human-level\nperformance in face verification. In Proceedings of the IEEE conference on computer vision and pattern recognition,\npages 1701–1708, 2014.\nPatrice Y Simard, David Steinkraus, John C Platt, et al. Best practices for convolutional neural networks applied to\nvisual document analysis. In ICDAR, volume 3, 2003.\nD. C. Ciresan, U. Meier, L. M. Gambardella, and J. Schmidhuber. Convolutional neural network committees for\nhandwritten character classification. In: Proceedings of the International Conference on Document Analysis and\nRecognition, pages 1135 – 1139, 2011.\nDan Ciregan, Ueli Meier, and Jürgen Schmidhuber. Multi-column deep neural networks for image classification. In\n2012 IEEE conference on computer vision and pattern recognition, pages 3642–3649. IEEE, 2012.\nTao Wang, David J Wu, Adam Coates, and Andrew Y Ng. End-to-end text recognition with convolutional neural\nnetworks. In Proceedings of the 21st International Conference on Pattern Recognition (ICPR2012), pages 3304–3308.\nIEEE, 2012.\n19\narXiv Template\nA PREPRINT\nIan J Goodfellow, Yaroslav Bulatov, Julian Ibarz, Sacha Arnoud, and Vinay Shet. Multi-digit number recognition from\nstreet view imagery using deep convolutional neural networks. arXiv preprint arXiv:1312.6082, 2013.\nJonathan J Tompson, Arjun Jain, Yann LeCun, and Christoph Bregler. Joint training of a convolutional network and a\ngraphical model for human pose estimation. In Advances in neural information processing systems, pages 1799–1807,\n2014.\nYann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436–444, 2015b.\nIan Goodfellow, Yoshua Bengio, and Aaron Courville.\nDeep Learning.\nMIT Press, 2016.\nhttp://www.\ndeeplearningbook.org.\nY. Bengio, P. Simard, and P. Frasconi. Learning long-term dependencies with gradient descent is difficult. IEEE\nTransactions on Neural Networks, 5(2):157–166, March 1994. ISSN 1045-9227. doi:10.1109/72.279181.\nY. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings\nof the IEEE, 86(11):2278–2324, Nov 1998. ISSN 1558-2256. doi:10.1109/5.726791.\nYann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document\nrecognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.\nKunihiko Fukushima and Sei Miyake. Neocognitron: A self-organizing neural network model for a mechanism of\nvisual pattern recognition. In Competition and cooperation in neural nets, pages 267–285. Springer, 1982.\nS-CB Lo, S-LA Lou, Jyh-Shyan Lin, Matthew T Freedman, Minze V Chien, and Seong Ki Mun. Artificial convolution\nneural network techniques and applications for lung nodule detection. IEEE transactions on medical imaging, 14(4):\n711–718, 1995.\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\nAditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of\ncomputer vision, 115(3):211–252, 2015.\nH. Sak, Andrew Senior, and F. Beaufays. Long short-term memory recurrent neural network architectures for large scale\nacoustic modeling. Proceedings of the Annual Conference of the International Speech Communication Association,\nINTERSPEECH, pages 338–342, 01 2014.\nX. Li and X. Wu. Constructing long short-term memory based deep recurrent neural networks for large vocabulary\nspeech recognition. In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),\npages 4520–4524, April 2015. doi:10.1109/ICASSP.2015.7178826.\nA. Graves, M. Liwicki, S. Fernández, R. Bertolami, H. Bunke, and J. Schmidhuber. A novel connectionist system for\nunconstrained handwriting recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 31(5):\n855–868, May 2009. ISSN 0162-8828. doi:10.1109/TPAMI.2008.137.\nSepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural Comput., 9(8):1735–1780, nov 1997.\nISSN 0899-7667. doi:10.1162/neco.1997.9.8.1735. URL http://dx.doi.org/10.1162/neco.1997.9.8.1735.\nFelix A Gers and Jürgen Schmidhuber. Recurrent nets that time and count. In Proceedings of the IEEE-INNS-\nENNS International Joint Conference on Neural Networks. IJCNN 2000. Neural Computing: New Challenges and\nPerspectives for the New Millennium, volume 3, pages 189–194. IEEE, 2000.\nJunyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent\nneural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.\nM. Schuster and K. K. Paliwal. Bidirectional recurrent neural networks. IEEE Transactions on Signal Processing, 45\n(11):2673–2681, Nov 1997. ISSN 1053-587X. doi:10.1109/78.650093.\nEmily Xiaoxuan Gu.\nConvolutional neural network based kannada-mnist classification.\nIn 2021 IEEE Inter-\nnational Conference on Consumer Electronics and Computer Engineering (ICCECE), pages 180–185, 2021.\ndoi:10.1109/ICCECE51280.2021.9342474.\nDeepika Gupta and Soumen Bag. Cnn-based multilingual handwritten numeral recognition: A fusion-free approach.\nExpert Systems with Applications, 165:113784, 2021.\nHuseyin Kusetogullari, Amir Yavariabdi, Abbas Cheddad, Håkan Grahn, and Johan Hall. Ardis: a swedish historical\nhandwritten digit dataset. Neural Computing and Applications, 32(21):16505–16518, 2020.\nELVIS S. GATI, BENJAMIN D. NIMO, and ELISHA K. ASIAMAH. Kannada-mnist classification using skip cnn. In\n2019 16th International Computer Conference on Wavelet Active Media Technology and Information Processing,\npages 245–248, 2019. doi:10.1109/ICCWAMTIP47768.2019.9067521.\nSukhdeep Singh, Vinod Kumar Chauhan, and Elisa H Barney Smith. A self controlled rdp approach for feature\nextraction in online handwriting recognition using deep learning. Applied Intelligence, 50(7):2093–2104, 2020.\n20\narXiv Template\nA PREPRINT\nVinay Uday Prabhu. Kannada-mnist: A new handwritten digits dataset for the kannada language, 2019.\nK. Manjusha, M. Anand Kumar, and K.P. Soman. On developing handwritten character image database for malayalam\nlanguage script. Engineering Science and Technology, an International Journal, 22(2):637–645, 2019. ISSN 2215-\n0986.\ndoi:https://doi.org/10.1016/j.jestch.2018.10.011.\nURL https://www.sciencedirect.com/science/\narticle/pii/S2215098618301447.\nRumman Rashid Chowdhury, Mohammad Shahadat Hossain, Raihan ul Islam, Karl Andersson, and Sazzad Hossain.\nBangla handwritten character recognition using convolutional neural network with data augmentation. In 2019 Joint\n8th international conference on informatics, electronics & vision (ICIEV) and 2019 3rd international conference on\nimaging, vision & pattern recognition (icIVPR), pages 318–323. IEEE, 2019.\nAnisha Gupta, Ritesh Sarkhel, Nibaran Das, and Mahantapas Kundu. Multiobjective optimization for recognition of\nisolated handwritten indic scripts. Pattern Recognition Letters, 128:318–325, 2019.\nSinjan Chakraborty, Sayantan Paul, Ram Sarkar, and Mita Nasipuri. Feature map reduction in cnn for handwritten digit\nrecognition. In Recent Developments in Machine Learning and Data Analytics, pages 143–148. Springer, 2019.\nShefali Arora and M. P. S Bhatia. Handwriting recognition using deep learning in keras. In 2018 International\nConference on Advances in Computing, Communication Control and Networking (ICACCCN), pages 142–145, 2018.\ndoi:10.1109/ICACCCN.2018.8748540.\nK Manjusha, M Anand Kumar, and KP Soman. Integrating scattering feature maps with convolutional neural networks\nfor malayalam handwritten character recognition. International Journal on Document Analysis and Recognition\n(IJDAR), 21(3):187–198, 2018.\nLei Kang, J Ignacio Toledo, Pau Riba, Mauricio Villegas, Alicia Fornés, and Marçal Rusinol. Convolve, attend and\nspell: An attention-based sequence-to-sequence model for handwritten word recognition. In German Conference on\nPattern Recognition, pages 459–472. Springer, 2018.\nRitesh Sarkhel, Nibaran Das, Aritra Das, Mahantapas Kundu, and Mita Nasipuri. A multi-scale deep quad tree\nbased feature extraction method for the recognition of isolated handwritten characters of popular indic scripts.\nPattern Recognition, 71:78–93, 2017. ISSN 0031-3203. doi:https://doi.org/10.1016/j.patcog.2017.05.022. URL\nhttps://www.sciencedirect.com/science/article/pii/S0031320317302200.\nArik Poznanski and Lior Wolf. Cnn-n-gram for handwriting word recognition. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), June 2016.\nIn-Jung Kim and Xiaohui Xie. Handwritten hangul recognition using deep convolutional neural networks. International\nJournal on Document Analysis and Recognition (IJDAR), 18:1–13, 2015. doi:10.1007/s10032-014-0229-4.\nLi Wan, Matthew Zeiler, Sixin Zhang, Yann Le Cun, and Rob Fergus. Regularization of neural networks using\ndropconnect. In International conference on machine learning, pages 1058–1066. PMLR, 2013.\nGeoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhutdinov. Improving neural\nnetworks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580, 2012.\nVu Pham, Théodore Bluche, Christopher Kermorvant, and Jérôme Louradour. Dropout improves recurrent neural\nnetworks for handwriting recognition. In 2014 14th international conference on frontiers in handwriting recognition,\npages 285–290. IEEE, 2014.\nPatrick Doetsch, Michal Kozielski, and Hermann Ney. Fast and robust training of recurrent neural networks for offline\nhandwriting recognition. In 2014 14th International Conference on Frontiers in Handwriting Recognition, pages\n279–284. IEEE, 2014.\nFrancois Chollet et al. Keras, 2015. URL https://github.com/fchollet/keras.\nYoussouf Chherawala, Partha Pratim Roy, and Mohamed Cheriet. Feature set evaluation for offline handwriting\nrecognition systems: Application to the recurrent neural network model. IEEE Transactions on Cybernetics, 46(12):\n2825–2836, Dec 2016. ISSN 2168-2275. doi:10.1109/TCYB.2015.2490165.\nYaroslav Shkarupa, Roberts Mencis, and Matthia Sabatelli. Offline handwriting recognition using lstm recurrent neural\nnetworks. In The 28th Benelux conference on artificial intelligence, 2016.\nCurtis Wigington, Seth Stewart, Brian Davis, Bill Barrett, Brian Price, and Scott Cohen. Data augmentation for\nrecognition of handwritten words and lines using a cnn-lstm network. In 2017 14th IAPR International Conference\non Document Analysis and Recognition (ICDAR), volume 01, pages 639–645, 2017. doi:10.1109/ICDAR.2017.110.\nKartik Dutta, Praveen Krishnan, Minesh Mathew, and CV Jawahar. Improving cnn-rnn hybrid networks for handwriting\nrecognition. In 2018 16th international conference on frontiers in handwriting recognition (ICFHR), pages 80–85.\nIEEE, 2018.\n21\narXiv Template\nA PREPRINT\nPraveen Krishnan, Kartik Dutta, and CV Jawahar. Word spotting and recognition using deep embedding. In 2018 13th\nIAPR International Workshop on Document Analysis Systems (DAS), pages 1–6. IEEE, 2018.\nJorge Sueiras, Victoria Ruiz, Angel Sanchez, and Jose F. Velez.\nOffline continuous handwriting recogni-\ntion using sequence to sequence neural networks.\nNeurocomputing, 289:119–128, 2018.\nISSN 0925-\n2312. doi:https://doi.org/10.1016/j.neucom.2018.02.008. URL https://www.sciencedirect.com/science/\narticle/pii/S0925231218301371.\nRajib Ghosh, Chirumavila Vamshi, and Prabhat Kumar. Rnn based online handwritten word recognition in de-\nvanagari and bengali scripts using horizontal zoning. Pattern Recognition, 92:203–218, 2019. ISSN 0031-3203.\ndoi:https://doi.org/10.1016/j.patcog.2019.03.030. URL https://www.sciencedirect.com/science/article/\npii/S0031320319301384.\nR Geetha, T Thilagam, and T Padmavathy. Effective offline handwritten text recognition model based on a sequence-to-\nsequence approach with cnn–rnn networks. Neural Computing and Applications, 33(17):10923–10934, 2021.\nJan Koutnik, Klaus Greff, Faustino Gomez, and Juergen Schmidhuber. A clockwork rnn. In International Conference\non Machine Learning, pages 1863–1871. PMLR, 2014.\nChristian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent\nVanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 1–9, 2015b.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level\nperformance on imagenet classification. In Proceedings of the IEEE international conference on computer vision,\npages 1026–1034, 2015.\nSiti Salwa Md Noor, Jinchang Ren, Stephen Marshall, and Kaleena Michael. Hyperspectral image enhancement and\nmixture deep-learning classification of corneal epithelium injuries. Sensors, 17(11):2644, 2017.\nTara N Sainath, Brian Kingsbury, George Saon, Hagen Soltau, Abdel-rahman Mohamed, George Dahl, and Bhuvana\nRamabhadran. Deep convolutional neural networks for large-scale speech tasks. Neural networks, 64:39–48, 2015.\nAlex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech recognition with deep recurrent neural networks.\nIn 2013 IEEE international conference on acoustics, speech and signal processing, pages 6645–6649. Ieee, 2013.\nVijay Badrinarayanan, Alex Kendall, and Roberto Cipolla. Segnet: A deep convolutional encoder-decoder architecture\nfor image segmentation. IEEE transactions on pattern analysis and machine intelligence, 39(12):2481–2495, 2017.\nRonghang Hu, Piotr Dollár, Kaiming He, Trevor Darrell, and Ross Girshick. Learning to segment every thing. In\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4233–4241, 2018.\nOriol Vinyals, Lukasz Kaiser, Terry Koo, Slav Petrov, Ilya Sutskever, and Geoffrey Hinton. Grammar as a foreign\nlanguage. arXiv preprint arXiv:1412.7449, 2014.\nYang Liu and Jiajun Zhang. Deep learning in machine translation. In Deep Learning in Natural Language Processing,\npages 147–183. Springer, 2018.\nAndrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating image descriptions. In Proceedings of\nthe IEEE conference on computer vision and pattern recognition, pages 3128–3137, 2015.\nPedro Pinheiro and Ronan Collobert. Recurrent convolutional neural networks for scene labeling. In International\nconference on machine learning, pages 82–90. PMLR, 2014.\nAditya Grover, Ashish Kapoor, and Eric Horvitz. A deep hybrid model for weather forecasting. In Proceedings of the\n21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 379–386, 2015.\nMoinul Hossain, Banafsheh Rekabdar, Sushil J Louis, and Sergiu Dascalu. Forecasting the weather of nevada: A deep\nlearning approach. In 2015 international joint conference on neural networks (IJCNN), pages 1–6. IEEE, 2015.\nShadi Albarqouni, Christoph Baur, Felix Achilles, Vasileios Belagiannis, Stefanie Demirci, and Nassir Navab. Aggnet:\ndeep learning from crowds for mitosis detection in breast cancer histology images. IEEE transactions on medical\nimaging, 35(5):1313–1321, 2016.\nDayong Wang, Aditya Khosla, Rishab Gargeya, Humayun Irshad, and Andrew H Beck. Deep learning for identifying\nmetastatic breast cancer. arXiv preprint arXiv:1606.05718, 2016.\nAngel Alfonso Cruz-Roa, John Edison Arevalo Ovalle, Anant Madabhushi, and Fabio Augusto González Osorio. A\ndeep learning architecture for image representation, visual interpretability and automated basal-cell carcinoma cancer\ndetection. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages\n403–410. Springer, 2013.\n22\narXiv Template\nA PREPRINT\nAy¸segül Uçar, Yakup Demir, and Cüneyt Güzeli¸s. Object recognition and detection with deep learning for autonomous\ndriving applications. Simulation, 93(9):759–769, 2017.\nChenyi Chen, Ari Seff, Alain Kornhauser, and Jianxiong Xiao. Deepdriving: Learning affordance for direct perception\nin autonomous driving. In Proceedings of the IEEE international conference on computer vision, pages 2722–2730,\n2015.\nW. Xiong, J. Droppo, X. Huang, F. Seide, M. Seltzer, A. Stolcke, D. Yu, and G. Zweig. Achieving human parity in\nconversational speech recognition, 2017.\nGeorge Saon, Gakuto Kurata, Tom Sercu, Kartik Audhkhasi, Samuel Thomas, Dimitrios Dimitriadis, Xiaodong Cui,\nBhuvana Ramabhadran, Michael Picheny, Lynn-Li Lim, Bergul Roomi, and Phil Hall. English conversational\ntelephone speech recognition by humans and machines, 2017.\nVarun Gulshan, Lily Peng, Marc Coram, Martin C Stumpe, Derek Wu, Arunachalam Narayanaswamy, Subhashini\nVenugopalan, Kasumi Widner, Tom Madams, Jorge Cuadros, et al. Development and validation of a deep learning\nalgorithm for detection of diabetic retinopathy in retinal fundus photographs. Jama, 316(22):2402–2410, 2016.\nAndre Esteva, Brett Kuprel, Roberto A Novoa, Justin Ko, Susan M Swetter, Helen M Blau, and Sebastian Thrun.\nDermatologist-level classification of skin cancer with deep neural networks. nature, 542(7639):115–118, 2017.\nParas Lakhani and Baskaran Sundaram. Deep learning at chest radiography: automated classification of pulmonary\ntuberculosis by using convolutional neural networks. Radiology, 284(2):574–582, 2017.\nChristian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus.\nIntriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.\nIan J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv\npreprint arXiv:1412.6572, 2014.\nXue-Wen Chen and Xiaotong Lin. Big data deep learning: challenges and perspectives. IEEE access, 2:514–525, 2014.\nMaryam M Najafabadi, Flavio Villanustre, Taghi M Khoshgoftaar, Naeem Seliya, Randall Wald, and Edin Muharemagic.\nDeep learning applications and challenges in big data analytics. Journal of big data, 2(1):1–21, 2015.\nJames Bergstra, Frédéric Bastien, Olivier Breuleux, Pascal Lamblin, Razvan Pascanu, Olivier Delalleau, Guillaume\nDesjardins, David Warde-Farley, Ian Goodfellow, Arnaud Bergeron, et al. Theano: Deep learning on gpus with\npython. In NIPS 2011, BigLearning Workshop, Granada, Spain, volume 3, pages 1–48. Citeseer, 2011.\n23\n",
  "categories": [
    "cs.CV",
    "cs.LG"
  ],
  "published": "2024-04-10",
  "updated": "2024-04-10"
}