{
  "id": "http://arxiv.org/abs/2404.08684v1",
  "title": "Is English the New Programming Language? How About Pseudo-code Engineering?",
  "authors": [
    "Gian Alexandre Michaelsen",
    "Renato P. dos Santos"
  ],
  "abstract": "Background: The integration of artificial intelligence (AI) into daily life,\nparticularly through chatbots utilizing natural language processing (NLP),\npresents both revolutionary potential and unique challenges. This intended to\ninvestigate how different input forms impact ChatGPT, a leading language model\nby OpenAI, performance in understanding and executing complex, multi-intention\ntasks. Design: Employing a case study methodology supplemented by discourse\nanalysis, the research analyzes ChatGPT's responses to inputs varying from\nnatural language to pseudo-code engineering. The study specifically examines\nthe model's proficiency across four categories: understanding of intentions,\ninterpretability, completeness, and creativity. Setting and Participants: As a\ntheoretical exploration of AI interaction, this study focuses on the analysis\nof structured and unstructured inputs processed by ChatGPT, without direct\nhuman participants. Data collection and analysis: The research utilizes\nsynthetic case scenarios, including the organization of a \"weekly meal plan\"\nand a \"shopping list,\" to assess ChatGPT's response to prompts in both natural\nlanguage and pseudo-code engineering. The analysis is grounded in the\nidentification of patterns, contradictions, and unique response elements across\ndifferent input formats. Results: Findings reveal that pseudo-code engineering\ninputs significantly enhance the clarity and determinism of ChatGPT's\nresponses, reducing ambiguity inherent in natural language. Enhanced natural\nlanguage, structured through prompt engineering techniques, similarly improves\nthe model's interpretability and creativity. Conclusions: The study underscores\nthe potential of pseudo-code engineering in refining human-AI interaction and\nachieving more deterministic, concise, and direct outcomes, advocating for its\nbroader application across disciplines requiring precise AI responses.",
  "text": " \nISSN: 2178-7727 \nDOI: 10.17648/acta.scientiae.8086 \n \n \n__________________ \nCorresponding author: Renato P. dos Santos. Email: renatopsantos@ulbra.edu.br  \n \n \nActa Sci. (Canoas), 26(1), 157-204, Jan./Feb. 2024 \n \nIs English the New Programming Language? How About \nPseudo-code Engineering? \n \nGian Alexandre Michaelsen\na \nRenato P. dos Santos\n a \n \na Universidade Luterana do Brasil (Ulbra), Programa de Pós-Graduação em Ensino de Ciências e \nMatemática (PPGECIM), Canoas, RS, Brasil \n \nReceived for publication 3 Apr. 2024. Accepted after review 3 Apr. 2024 \nDesignated editor: Claudia Lisete Oliveira Groenwald \n \nABSTRACT \nBackground: The integration of artificial intelligence (AI) into daily life, \nparticularly through chatbots utilizing natural language processing (NLP), presents \nboth revolutionary potential and unique challenges. This research is motivated by the \nintricacies of human-computer interaction within the context of conversational AI, \nfocusing on the role of structured inputs from pseudo-code engineering in enhancing \nchatbot comprehension and response accuracy. Objectives: This study investigates the \ncomparative effectiveness of natural language versus pseudo-code engineering \ngenerated inputs in eliciting precise and actionable responses from ChatGPT, a leading \nlanguage model by OpenAI. It aims to delineate how different input forms impact the \nmodel's performance in understanding and executing complex, multi-intention tasks. \nDesign: Employing a case study methodology supplemented by discourse analysis, the \nresearch analyzes ChatGPT's responses to inputs varying from natural language to \npseudo-code engineering. The study specifically examines the model's proficiency \nacross four categories: understanding of intentions, interpretability, completeness, and \ncreativity. Setting and Participants: As a theoretical exploration of AI interaction, this \nstudy focuses on the analysis of structured and unstructured inputs processed by \nChatGPT, without direct human participants. Data collection and analysis: The \nresearch utilizes synthetic case scenarios, including the organization of a \"weekly meal \nplan\" and a \"shopping list,\" to assess ChatGPT's response to prompts in both natural \nlanguage and pseudo-code engineering. The analysis is grounded in the identification \nof patterns, contradictions, and unique response elements across different input \nformats. Results: Findings reveal that pseudo-code engineering inputs significantly \nenhance the clarity and determinism of ChatGPT's responses, reducing ambiguity \ninherent in natural language. Enhanced natural language, structured through prompt \nengineering techniques, similarly improves the model's interpretability and creativity. \nConclusions: The study underscores the potential of pseudo-code engineering in \nrefining human-AI interaction, advocating for its broader application across disciplines \n \nActa Sci. (Canoas), 26(1), 157-204, Jan./Feb. 2024 \n158 \nrequiring precise AI responses. It highlights pseudo-code engineering's efficacy in \nachieving more deterministic, concise, and direct outcomes from AI systems like \nChatGPT, pointing towards future innovations in conversational AI technology. \nKeywords: Artificial Intelligence; ChatGPT; Natural Language Processing; \nPseudo-code Engineering; Human-Computer Interaction \n \nINTRODUCTION \nThe integration of artificial intelligence (AI) into our daily lives has not \nonly revolutionized our interaction with technology but also posed unique \nchallenges, particularly in the realm of natural language processing (NLP). As \nAI systems, like chatbots, become more sophisticated, they increasingly rely on \nNLP to interpret and respond to human language with a level of nuance \npreviously unattainable. As an example, we can see a input text for an AI, also \nknown as a prompt, developed from Mollick & Mollick in natural language that \naims to use the AI to generate examples for the explanation of complicated \nconcepts.  \n“I would like you to act as an example generator for students. \nWhen confronted with new and complex concepts, adding \nmany and varied examples helps students better understand \nthose concepts. I would like you to ask what concept I would \nlike examples of, and what level of students I am teaching. You \nwill provide me with four different and varied accurate \nexamples of the concept in action.”(2023, 5-6). \nIndependently the clarity of the written prompt, the inherent ambiguity \nand complexity of natural language can often hinder these systems' ability to \ndecipher user intent accurately (Carvalho, 2021; Jho, 2020). In the utilization \nof elaborated prompts with multiple functions, we notice that, in some cases \nand moments, the ChatGPT had opened window where the requested process \nwas processed thru a programming language. By noticing some nuances \nbetween the requisition and what ChatGPT was doing in the execution we delve \nourselves in approximate the natural language to programing languages that \ncould be executable to ChatGPT. This is where the concept of pseudo-code \nemerges as a potential bridge between the intuitive, flexible nature of human \nlanguage and the precise, logical structure of programming languages. Pseudo-\ncode, an informal high-level description of programming logic, offers a solution \nto the challenges posed by natural language's ambiguity. It provides a structured \nyet readable format that can significantly enhance the clarity of instructions for \nLarge Language Models (LLMs) like ChatGPT. By incorporating elements of \n159 \nActa Sci. (Canoas), 26(1), 157-204, Jan./Feb. 2024 \n \npseudo-code into user prompts, we can minimize misunderstandings and \nimprove the effectiveness of AI interactions. This approach aligns closely with \nthe latest advancements in Generative AI, which seeks not only to interpret data \nbut to create original content, underscoring the need for clear communication \nbetween humans and AI systems (Jovanovic & Campbell, 2022).  \nIn the spectrum of languages used with advanced models like ChatGPT, \npseudo-code can act as a new communication tool, bridging natural language \nand formal programming. Pseudo-code is an informal, high-level description \nmethod for problem-solving solutions, aiding programmers in algorithm \nconception before implementation in languages like C#, C++, or Java. This \napproach enhances logical reasoning and is accessible to non-experts, making \nit invaluable in language model interactions (Oda et al., 2015).  \nHere bellow we can see a briefly demonstration of the pseudo-code \nformat, as an example we can take a ChatGPT prompt showed before and \nrebuild it as a pseudo-code, with give him this following structure. \n1) ACT as an example generator for students to help them to \nbetter understand those concepts.   \n2) ASK what concept I would like examples of. \n3) ASK what level of students I am teaching. \n4) IF confronted with new and complex concepts, THEN add \nmany and varied examples  \n5) GENERATE four different and varied accurate examples of \nthe concept in action. \nAs we can see, the same requisition made the prompt developed by \n(Mollick & Mollick, 2023) can be pseudo-coded, as like any other requests that \nthe user can make to a chatbot. Based on this, our study utilizes pseudo-code to \nexplore the intersection of computational logic and human language flexibility, \nexamining how ChatGPT processes and responds to semi-algorithmic \ncommands. Incorporating chain of thought techniques (Ling et al., 2023; Wei \net al., 2022) and prompt engineering (Heston & Khun, 2023), and the some of \nthe logics of codes scription, pseudo-code could became a powerful tool for \neffective communication with language models, offering insights into human-\ncomputer interaction. \nOur study delves into the comparative effectiveness of traditional \nnatural language prompts versus those structured as pseudo-code and enhanced \n \nActa Sci. (Canoas), 26(1), 157-204, Jan./Feb. 2024 \n160 \npseudo-code in eliciting accurate responses from ChatGPT. Employing a case \nstudy methodology (Yin, 2009) coupled with discourse analysis (Jørgensen & \nPhillips, 2011), we aim to illuminate the nuances in how ChatGPT processes \nthese different forms of input. This investigation is motivated by the hypothesis \nthat pseudo-code, by reducing the ambiguity inherent in natural language, can \nlead to more precise and actionable responses from LLMs. \nThe article is methodically structured to guide the reader through our \nexploration of this hypothesis. Beginning with an overview of the operational \nprinciples of NLP, we transition to a focused examination of pseudo-code—its \ndefinition, significance, and application in improving LLM interactions \n(Calzadilla, 2018; Oda et al., 2015). Subsequent sections detail the \nmethodological framework adopted for this study, including the creation of \npseudo-codes and the analytical approach to evaluating ChatGPT's responses. \nBy juxtaposing the performance of traditional natural language prompts with \nthose framed as pseudo-code, we shed light on the potential of structured inputs \nto refine and elevate our engagement with AI technologies. \n \nTHEORETICAL FRAMEWORK \nNatural Language Understanding \nNatural Language Understanding (NLU), a cornerstone of Natural \nLanguage Processing (NLP), activates when a user engages with a chatbot. This \ninitial phase is pivotal, enabling machines to analyze and decipher language by \nidentifying concepts, emotions, entities, and keywords, all while zeroing in on \nthe user's intention. NLU's role is paramount in customer service applications, \nwhere grasping and addressing verbally or textually reported issues is necessary  \n(Khurana et al., 2022). \n \nExploring the Levels of Human Language in NLU \nPhonology: Understanding the sound organization in languages is \nessential. For instance, the phonological nuances in the pronunciation \nof \"read\" can alter its meaning based on the tense, thereby affecting the \nmessage's interpretation in an audio interpretation on a chatbot \ninteraction. A practical application is seen in virtual assistants like Siri \nor Alexa, which analyze vocal inputs to discern contextually relevant \nmeanings of homonyms based on the user's query. \n161 \nActa Sci. (Canoas), 26(1), 157-204, Jan./Feb. 2024 \n \nMorphology: Examines word structures and morphemes, the smallest \nmeaning units. Analyzing morphological variations helps interpret \ndifferent contextual uses, like understanding \"running\" as an action or \na continuous state in user queries. For example, chatbots in fitness apps \nutilize this understanding to differentiate between \"running as exercise\" \nversus \"running a program\" based on the user's activity and \npreferences. \nLexical: Delves into the meanings of words, crucial for resolving \nambiguities. For example, a chatbot discerning the word \"bat\" as sports \nequipment or an animal based on the conversation context. E-\ncommerce chatbots exemplify this by accurately suggesting products \n(a baseball bat) or information (about bats) depending on the \nsurrounding text in user inquiries. \nSyntactic: Entails grammatical sentence analysis, key to understanding \nword order and relationships, such as distinguishing between \"eating \napples\" and \"apples eating\" scenarios. Customer support chatbots use \nsyntactic analysis to correctly interpret requests, ensuring responses are \naccurate and relevant to the query's structure. \nSemantic: Aims to comprehend the actual meanings of texts or \nsentences, processing the logical structure to grasp the interactions \namong words or concepts, like differentiating \"I saw her duck\" based \non action or observation. Translation services in messaging platforms \nshowcase this level by providing contextually appropriate translations \nthat consider both literal and figurative language use. \nPragmatic: Focuses on implied meanings, considering knowledge \nbeyond the text. This layer allows NLU to infer that a user asking, \"Can \nyou open the window?\" is requesting an action, not querying the \nchatbot's capabilities. Smart home assistants demonstrate pragmatics \nby interpreting commands in the context of the home environment, \nenabling them to perform tasks like adjusting a thermostat based on \nconversational cues rather than explicit instructions. \n \nChallenges and Solutions in NLU \nOne of the principal challenges in Natural Language Understanding \n(NLU) is the presence of linguistic ambiguities that can lead to varied \n \nActa Sci. (Canoas), 26(1), 157-204, Jan./Feb. 2024 \n162 \ninterpretations of a sentence or phrase. These ambiguities are addressed at \ndifferent linguistic levels: \nSyntactic Ambiguity: A common issue where the structure of a \nsentence allows for multiple interpretations. Consider the phrase \n\"Visiting relatives can be annoying\"; it can be understood in two ways: \neither the act of visiting relatives is annoying, or the relatives who visit \nare annoying. NLU syntactic analysis tools strive to clarify such \nambiguities. For instance, an NLU system might use user interaction \nhistory or follow-up questions to discern if the user generally discusses \nfamily matters negatively or positively, guiding the system towards the \nintended interpretation. \nSemantic Ambiguity: Occurs when a word or phrase has multiple \nmeanings, leading to confusion about the intended message. The word \n\"crane\" can refer to a bird, a type of machinery for lifting, or an action \nto stretch the neck. Semantic clarification techniques in NLU determine \nthe correct meaning by analyzing the surrounding text or context. For \nexample, if a user is discussing construction, the NLU system is more \nlikely to interpret \"crane\" as machinery, showcasing the strategy of \npreservation by keeping the contextually relevant meaning. \nLexical Ambiguity: Deals with words that have multiple meanings, \nleading to confusion without context. The sentence \"I went to the bank\" \nis ambiguous without additional information. Lexical disambiguation \nmethods in NLU use context clues from the conversation, like recent \ntransactions or geographical location, to deduce whether \"bank\" refers \nto a financial institution or riverbank. This is an application of \nminimization, where the system reduces ambiguity by leveraging \nknown context. \nIn addressing these challenges, NLU systems employ strategies like \nminimization (to reduce the chances of ambiguity), preservation (to maintain \nthe intended meaning as closely as possible), and interactive disambiguation \n(engaging in a dialogue with the user to clarify ambiguous statements in real-\ntime), as outlined by Khurana et al. (2022). An example of interactive \ndisambiguation is when a chatbot, unsure of a user's intent due to ambiguous \ninput, asks clarifying questions to ensure accurate understanding and response, \nthus enhancing the chatbot's ability to comprehend and engage effectively. \nFurthermore, incorporating cognitive comprehension tasks such as \nsentiment analysis and emotion detection allows conversational agents to better \n163 \nActa Sci. (Canoas), 26(1), 157-204, Jan./Feb. 2024 \n \ngrasp the tone and underlying feelings in user communications. This might \nmanifest in a customer service chatbot that, detecting frustration in a user's \nmessage about \"waiting at the bank,\" understands the sentiment as negative and \nprioritizes a response designed to alleviate frustration, enriching the interaction \nexperience (Kusal et al., 2022). \n \nAction Orientation in NLU \nTo enrich the understanding of Action Orientation in Natural Language \nUnderstanding (NLU), it's crucial to delve deeper into the mechanism of \nconverting textual data into N-dimensional feature vectors or word embeddings \n(Wolfram, 2023). This transformative process is at the heart of NLU, enabling \nthe detailed semantic representation of language inputs. Consider the task of \nprocessing a user's query, such as \"Find me a quiet café nearby.\" NLU systems \ntackle this by breaking down the query into vectors that encapsulate the essence \nof \"quiet,\" \"café,\" and \"nearby,\" thereby allowing the model to understand and \ncategorize the user's intent with high precision. \nThese vectors serve as the foundation for intention classification \nmodels within NLU frameworks, crucial for the operation of sophisticated \ntechnologies like Large Language Models (LLMs)(Wolfram, 2023). Through \nvectorization, NLU facilitates an enhanced understanding of language nuances, \nsignificantly improving the accuracy and relevance of responses generated by \nchatbots in real-time interactions. This intricate process underscores the \nadvanced capabilities of NLU in interpreting and acting upon human language, \nproviding a seamless interface for human-chatbot communication (Kusal et al., \n2022). \nThis detailed exploration of action orientation in NLU showcases the \nessential role of word embeddings in enhancing the interpretative capabilities \nof AI systems, ensuring that user queries are met with precise and contextually \nrelevant responses. \n \nPseudo-code in the Context of ChatGPT and Large Language \nModels Interactions \nPseudo-code emerges as a pivotal tool in bridging the inherent \nchallenges of Natural Language Understanding (NLU) with the structured logic \nrequired for programming, as highlighted by Calzadilla (2018). It serves as a \nclarifying medium between the ambiguous flexibility of human language and \n \nActa Sci. (Canoas), 26(1), 157-204, Jan./Feb. 2024 \n164 \nthe deterministic precision of programming languages, exemplifying the \ninterdisciplinary approach central to this research. By converting a natural \nlanguage prompt by Mollick & Mollick (2023) into pseudo-code, this \nmethodology illuminates how structured action sequences can directly address \nNLU's obstacles, enhancing ChatGPT's response efficacy. The use of explicit \nprogramming constructs such as conditional statements (\"IF\", \"THEN\") and \ncommands (\"ASK\", \"GENERATE\"), emphasized in uppercase, signals precise \ninstructions to the LLM, thereby reducing the ambiguity that NLU systems \noften grapple with. \nThis application of pseudo-code not only refines the functionality of \nLLMs but also embodies the interdisciplinary nexus of linguistics, computer \nscience, and AI research. It showcases how integrating simple programming \nlogic into the instruction of AI models can mitigate some of the most persistent \nchallenges in NLU, including the disambiguation of user intent and the \ncontextual interpretation of queries. Consequently, pseudo-code becomes not \njust a tool for enhancing AI interactions but also a demonstration of how the \nconfluence of diverse scientific disciplines can lead to innovative solutions in \nAI technology (Calzadilla, 2018). This research underscores the value of \npseudo-code in transcending traditional boundaries between natural language \nand programming, offering a comprehensive approach to tackling the \ncomplexities of human-computer interaction. \n \nLarge Language Model (LLM)/ChatGPT \nFrom NLU to Vectorization \nThe journey of processing language in ChatGPT, a distinguished Large \nLanguage Model (LLM), begins with an essential phase known as Natural \nLanguage Understanding (NLU). In this initial stage, the model undertakes the \ntask of interpreting the intricacies of human language. The process progresses \ninto a critical phase termed vectorization or 'embedding', where the essence of \nlanguage transitions from a textual to a numerical form. This transformation is \npivotal, concentrating on encapsulating elements that are central to the user's \nintention. ChatGPT accomplishes this through the utilization of 'tokens', which \nare numerical representations of words or text fragments. These tokens are \nmeticulously selected based on their proficiency in capturing the core content \nand the underlying intent of the input. The selection process is comprehensive, \nincorporating an evaluation of the contextual relevance and semantic \n165 \nActa Sci. (Canoas), 26(1), 157-204, Jan./Feb. 2024 \n \nimportance of each word or phrase, ensuring that the most significant aspects \nof the language are retained (Wolfram, 2023). \nIn the subsequent phase, these tokens undergo a mapping process to \nbecome vectors, creating 'embeddings' that are imbued with rich semantic and \ncontextual properties. These embeddings are then integrated into the neural \nnetwork, essentially influencing the model's response mechanisms to user \ncommands. It's important to note that while the structural integrity of the \nnetwork remains intact during real-time processing, this mechanism enables \nChatGPT to offer dynamic interpretations and responses to a wide array of \nhuman language inputs. The selection and mapping of tokens to vectors are not \narbitrary; instead, they are governed by sophisticated algorithms designed to \nunderstand the nuanced relationships between words in different contexts. This \nensures that the embeddings reflect a deep understanding of language nuances, \nenabling the neural network to process requests with a high degree of accuracy \nand relevance (Wolfram, 2023). Once selected, these tokens are mapped to \nvectors, forming 'embeddings' rich in semantic and contextual attributes, and \nfed into the neural network. These embeddings dictate the neural network's \nresponse to user commands. While the network's structure remains unchanged \nin real-time processing, this approach allows ChatGPT to interpret and respond \ndynamically to diverse human language inputs (Wolfram, 2023). \n \nFrom Vectorization to Neural Network Training \nLarge Language Models (LLMs) are advanced systems that process \nenormous volumes of data to solve complex tasks and problems. They go \nbeyond being mere repositories of information; they are sophisticated structures \nfor understanding and generating language. Imagine a conceptual space where \ninformation is organized in distinct layers, like in a vast archive (Wolfram, \n2023). \nWhen input data (in the form of embeddings) arrive at an LLM, they \ngo through several layers of processing in the neural network. Each layer of the \nnetwork specializes in different aspects of language, such as semantics, \ngrammar, or context. The role of vectors here is crucial: they guide the model \nto relevant parts of the 'conceptual space' of stored knowledge. This space is an \nabstract representation of all the data that the model has been trained to \nunderstand (Wolfram, 2023). \nFor example, when creating ice cream recipes, the vectors direct the \nmodel to parts of the conceptual space related to ice creams. If the request \n \nActa Sci. (Canoas), 26(1), 157-204, Jan./Feb. 2024 \n166 \nspecifies 'lactose-free ice cream recipes', the system adjusts its vectors to focus \non the intersection between ice creams and lactose-free alternatives. This \ndynamic process of vector adjustment is what allows the model to respond \nflexibly and relevantly to user queries. \nIn this conceptual space, the vectors act as guides, highlighting relevant \ninformation and temporarily relegating less pertinent data to the background. \nThis dynamic serve to illustrate how a neural network reorganizes and \nprioritizes data in response to a specific query. Each user interaction prompts \nthe neural network to traverse this conceptual space, adapting to highlight \nrelevant information. This adaptive process is akin to 'network training' or \n'machine learning', where the model refines its responses based on user input, \nanalogous to human learning (Wolfram, 2023). \nSimilarly to human training, the machine may not initially act as \ndesired by the user. Internally, the layers of the network adjust through vectors \nto achieve a closer result to the desired one. During this training, the model \ngenerates different examples to refine its response, learning and shaping itself \naccording to the user's needs (Wolfram, 2023). \n \nNeural Networks: Managing Information \nChatGPT’s neural networks are characterized by attributes such as non-\nlinearity, unlimited capacity, adaptability, self-organization, and non-convexity. \nThese attributes are not just technical jargon; they are the backbone of the \nmodel's ability to handle complex tasks, adapt to varied inputs, and offer a \nvariety of solutions and responses. They are what make the network adept at \nlearning, storing associative memories, and finding optimal solutions. The \nprocess allows the model to dynamically adjust its outputs based on the nuances \nof each query, ensuring that responses are not only accurate but also \ncontextually aligned with the user's expectations. This adaptability is a \ntestament to ChatGPT's advanced understanding of language and its capacity \nto provide tailored responses across a spectrum of inquiries. This occurs thanks \nto the highly strategic selection process, which consider the semantic weight \nand contextual relevance of each word to be converted into tokens. This \nversatility is crucial, not just for tasks like image recognition and predictions, \nbut also for interpreting and responding to natural language and pseudo-code \ninput, each with its unique challenges and requirements (Kusal et al., 2022; \nWolfram, 2023; Wu & Feng, 2018). \n167 \nActa Sci. (Canoas), 26(1), 157-204, Jan./Feb. 2024 \n \nWhen faced with complex queries, for example, in responding to a \nmultifaceted question about climate change's impact on global agriculture, \nChatGPT would dissect the query into components such as 'climate change \neffects' and 'global agriculture'. It then applies its vectorized knowledge to each \nsegment, drawing on its vast training data to assemble a comprehensive \nresponse that addresses each aspect of the query. This process not only ensures \naccuracy but also enriches the response with a breadth of insight and depth that \nmight not be immediately apparent in the query itself. \nThe model's handling of complex queries and its adaptive response \nmechanism underscore its sophistication. By leveraging detailed embeddings \nand an intricate understanding of context, the LLM navigates the complexities \nof human language with agility. This nuanced approach enables the model to \noffer responses that are not just reactive but deeply informed, providing users \nwith insights that are both precise and contextually rich (Wolfram, 2023). \nThe evolution to deeper neural networks, a foundational aspect of Deep \nLearning, has significantly amplified the model's ability to process vast and \nintricate data sets. This enhanced processing capability is directly linked to the \nmodel's performance in interpreting both natural language and pseudo-code. \nWhile natural language demands an understanding of nuanced human \nexpression, pseudo-code requires a more structured and logical interpretation. \nThe neural network's multi-layered architecture equips ChatGPT to navigate \nthese diverse linguistic landscapes, extracting and responding to information in \na way that is representative, valuable, and, most importantly, aligned with the \nuser's intent (Wu & Feng, 2018). \n \nThe theorical difference to a LLM deal with a Natural Language and \na Pseudo-code prompt \nThe exploration of LLMs like ChatGPT highlights a fundamental \ndifference in how these models process natural language prompts compared to \npseudo-code, each imposing distinct demands. The NLP relies on the model's \ncapability to discern the intricacies of human communication, necessitating a \nflexible approach for accurately interpreting user intent (Wolfram, 2023). \nConversely, pseudo-code, with its structural similarity to programming \nlanguages, requires ChatGPT to engage in a more analytical manner, \ndemanding precise adherence to structured inputs to operationalize embedded \nlogical instructions accurately. The utilization of pseudo-code can significantly \nenhance ChatGPT's output determinism and response clarity, allowing for \n \nActa Sci. (Canoas), 26(1), 157-204, Jan./Feb. 2024 \n168 \noutputs that can more accurately reflect the user's procedural logic and \nintention, thereby minimizing ambiguity. \nThe process of vectorization, crucial in transforming textual inputs into \nnumerical vectors or embeddings, is instrumental across both domains but \nshines particularly in the domain of pseudo-code. This transformation can \nfacilitate the mapping of structured commands to specific actions or responses, \nstreamlining the interpretative process and enhancing the model's adaptability \n(Kusal et al., 2022; Wu & Feng, 2018). The neural network architecture \nunderlying ChatGPT further augments this adaptability, enabling the model to \nadeptly navigate the interpretative demands of natural language and the logical \nrigors of pseudo-code. The application of pseudo-code can showcase the \nmodel's capacity for high-level problem-solving and logical reasoning, \nattributes that are significantly bolstered by the structured nature of pseudo-\ncode inputs. \nIn the next section, we will explain the methodological framework \nchosen to conduct our case study and further demonstrate the evidence of the \nadvantages that resulted from the use of pseudo-codes. Doing a comparative \nanalysis, we provide empirical evidence supporting pseudo-code efficiency and \nclarity over natural language instructions. Expanding on the technical \nmechanisms utilized to write a pseudo-code, nominating this technique as \nPseudo-code Engineer.  \n \nMETHODOLOGY \nThe present article incorporates a case study methodology (Yin, 2001), \nwith the specific objective of investigating the peculiarities and unique \nchallenges presented by ChatGPT in the context of the interaction between \nnatural language and pseudo-code. ChatGPT, being an advanced language \nmodel developed by OpenAI, offers a unique case study scenario due to its \nlanguage processing capabilities and flexibility in dealing with different input \nformats. This case study will focus on uncovering how ChatGPT interprets and \nresponds to commands in natural language compared to pseudo-code. The \nchoice of ChatGPT as a case study is justified by its emerging relevance in \nvarious practical applications and the need to deeply understand its capabilities \nand limitations in a complex interactive scenario. \nThis case study methodology, as proposed by Yin (2001), allows for a \ndeep and contextual analysis of the case, which is the interaction with ChatGPT. \nThe choice of this approach is justified by the complexity and multifaceted \n169 \nActa Sci. (Canoas), 26(1), 157-204, Jan./Feb. 2024 \n \nnature of the phenomenon under study, the interaction between natural \nlanguage and programming language, and ChatGPT's ability to process and \nrespond to these two distinct forms of communication. The study will \nspecifically focus on the nuances of this interaction, exploring how ChatGPT \ninterprets and translates commands in natural language compared to pseudo-\ncode. This will include a detailed analysis of the processes of intention \ninterpretation, and the responses generated by the model in different scenarios \nand types of inputs. \nThe interpretation of the data will be carried out through the \nmethodology of discourse analysis (Jørgensen & Phillips, 2011). This \nmethodology offers a deep and reflective approach, allowing for a holistic \nunderstanding of the phenomenon studied. Discursive textual analysis is not \nlimited to the mere decomposition of texts; it involves an iterative and critical \nprocess of reading, interpretation, and reinterpretation of the data. \nSpecifically, this methodology will be employed to explore and \nunderstand the interactions between the user and ChatGPT. It will enable the \nidentification of patterns, contradictions, and nuances in the responses \ngenerated by the system in different input formats, whether they be natural \nlanguage or pseudo-code. The process of discursive textual analysis in this \nresearch will include data collection in the form of textual interactions with \nChatGPT, extracted into a text file for subsequent coding and categorization of \nthese data. Later, the categorized data will be subjected to a critical analysis, \nwhere the results brought by the LLM will be examined. This approach will \nallow for a deeper understanding of the internal mechanisms and characteristics \nof ChatGPT, as well as the practical implications of its use in different contexts. \nThe methodology adopted in this study, which encompasses both the \ncase study and the discourse analysis, has been carefully chosen to align \ndirectly with the specific research objectives. The case study, focused on \nChatGPT, provides a lens through which we can explore and deeply understand \nthe nuances of the interaction between natural language and pseudo-code. This \nmethod allows us to investigate in a detailed and contextualized manner how \nChatGPT processes and responds to different input formats, in line with our \nmain research questions (Yin, 2001). \nHow does ChatGPT handle multiple intentions inputted in the form of \nnatural language? We will analyze the interactions to understand how \nChatGPT identifies and responds to various intentions naturally \nexpressed by the user. \n \nActa Sci. (Canoas), 26(1), 157-204, Jan./Feb. 2024 \n170 \nHow does ChatGPT deal with multiple intentions when inputted in the \nform of pseudo-code? We will examine whether the clarity and \nstructure of the pseudo-code influence the accuracy and determinism \nof ChatGPT's responses compared to natural language inputs. \nWe assume as a proposition the conception that: \n• \nThe use of prompts with multiple intentions in natural language can \nlead to results that diverge in an indeterministic way from the user's \nintended requests. \n• \nThe use of pseudo-code may facilitate the identification of \nintentions recognized by the LLM. \n• \nThe possibility of increasing the determinism in the interactions \nthat follow the use of pseudo-code. \nThe analyses will be structured around three distinct units, each \nfocusing on a different method of interaction with ChatGPT. These units are: \nNatural Language (Unit A): This unit will consist of interactions \ncarried out using natural language. The focus will be on how ChatGPT \ninterprets and responds to inputs formulated in common human \nlanguage, identifying patterns and nuances in the model's responses. \nEnhanced Natural Language (Unit B): This unit delves into the \nrealm of interactions facilitated through an advanced variant of natural \nlanguage, meticulously crafted to augment the precision and lucidity of \nuser commands directed at ChatGPT. Unlike conventional natural \nlanguage inputs, this enhanced form incorporates sophisticated \nprompting techniques designed to refine and specify user intents, \nthereby facilitating a more accurate and nuanced response from the \nlanguage model. The enhancement of natural language encompasses \nstrategic modifications such as the inclusion of specific contextual \ncues, explicit instruction framing, and the integration of prompts that \nmimic a chain of thought process, aiming to guide the model through a \nlogical sequence of reasoning that mirrors human cognitive processes. \nExamples of such enhancements include structuring prompts to include \nstep-by-step questions or scenarios that lead ChatGPT to deduce the \nuser's intent more effectively, as well as embedding emotional or \nsituational context to elicit responses that are not only relevant but also \nempathetic and context-aware. These techniques, derived from recent \nadvancements in natural language processing and AI interaction studies \n171 \nActa Sci. (Canoas), 26(1), 157-204, Jan./Feb. 2024 \n \n(Heston & Khun, 2023; Li et al., 2023; Ling et al., 2023a, 2023b; Wei \net al., 2022), empower users to elicit more detailed, accurate, and \ncontextually rich responses from ChatGPT, illustrating the potential of \nenhanced prompts in bridging the gap between human queries and AI \ncomprehension. The subsection “Enhancement of Prompt in NL” \nprovides a comprehensive overview of these processes, detailing the \nmethods employed to elevate the standard of natural language \ncommunication with ChatGPT, thereby setting a precedent for \nimproved interaction quality and model responsiveness. \nPseudo-code (Unit C): This unit will concentrate on interactions \ncarried out through pseudo-codes, allowing the evaluation of how \nChatGPT responds to a more structured and directed form of input, and \nwhether this results in more accurate or relevant responses. The \nmethodology for enhancing pseudo-code entails the integration of \nspecific programming concepts, such as conditional statements, loops, \nand function calls, designed to test the language model's ability to \nfollow complex instructions and execute logical sequences.  \nIncorporating elements such as detailed comments to guide the model's \nreasoning, explicitly defining variables and their expected outcomes, and \nstructuring the pseudo-code to reflect the sequential logic found in software \ndevelopment practices, aims to provide ChatGPT with a clear, unambiguous set \nof instructions that require a higher level of cognitive processing. This approach \nnot only tests the model's capacity to parse and execute more complex \ninstructions but also its ability to engage with the underlying logic of \nprogramming tasks, offering insights into its computational thinking \ncapabilities. \nWe will further present the section which detail the actions they can be \ntaken to improve pseudo-codes. Thus techniques drawing from contemporary \nresearch in programming language theory and artificial intelligence interaction \n(Heston & Khun, 2023; Wei et al., 2022)(Yue et al., 2023), to offer a granular \nview of how structured, logical enhancements can significantly impact \nChatGPT's interpretative and problem-solving performance. \nTo conduct an effective comparison between the units of analysis, we \nwill adopt a systematic approach: \nParallelism in Interactions: We will ensure that the interactions in \nboth units are parallel in terms of intentionality and context. This means \n \nActa Sci. (Canoas), 26(1), 157-204, Jan./Feb. 2024 \n172 \nthat the same types of requests or commands will be presented to \nChatGPT, first in natural language and then in pseudo-code. \nComparative Analysis: We will use discourse analysis to examine \nChatGPT's responses in the units. This will include identifying \ndifferences and similarities in the responses, the accuracy of the \nmodel's interpretations, and the relevance of the given responses. \nInterpretation of Results: We will interpret the results to understand \nthe implications of the form of input on the effectiveness of ChatGPT's \nresponse. This will help us determine whether the use of pseudo-codes \noffers a significant advantage in terms of clarity and accuracy in \ninteraction with the model. \nData collection process \nThru the Chat GPT 4 we inserted the respective entry, in each format \nthat we describe above in the units A, B, C and D. In the “Unit A” we only we \nhad one single interaction since the prompt was already fulfilled with the \nparameters to perform solicited task. In the following units, as a demanding \ncharacteristic of the generalization process, we had to had two interactions with \nthe LLM. The first one only caring the text or the pseudo-code to describe and \norientate the LLM to perform the desired task and a second one to fulfill the \nLLM with the already selected parameters. To only analyze the coding \ndifferences described in each unit, we standardize the parameters interaction, \nutilizing the sabe format in the units B and C. It’s possible to see the parameters \ninteraction patterns in the nexus A, B and C. \nThrough this comparative method, grounded in discourse analysis \n(Jørgensen & Phillips, 2011), we establish clear objectives to evaluate \nChatGPT's responses in-depth. The specific objectives include: \n• \nExploring the accuracy of ChatGPT in understanding user \nintentions in both natural language and pseudo-code inputs, \nidentifying how the model interprets and responds to these two \ndistinct formats. \n• \nInvestigating the consistency of ChatGPT's responses concerning \nthe complexity and ambiguity of inputs, identifying how the model \ndeals with requests that have multiple intentions or are structurally \ncomplex. \n• \nExamining the differences in ChatGPT's responses when \nconfronted with inputs in natural language compared to pseudo-\n173 \nActa Sci. (Canoas), 26(1), 157-204, Jan./Feb. 2024 \n \ncodes, seeking to understand how the structure and form of the \ninput influence the response generation. \nDiscourse analysis (Jørgensen & Phillips, 2011) complements the \napproach of this case study (Yin, 2001), allowing us to uncover deeper layers \nof meaning in ChatGPT's responses. This conjunction of methodologies is \nessential for the critical and detailed interpretation of the collected data, \nenabling us to identify patterns and trends in the model's responses. Thus, the \nsynergy between the case study and discourse analysis equips us to approach \nour research questions with a robust and integrated methodology, ensuring that \neach aspect of the interaction with ChatGPT is explored in a comprehensive \nand reflective manner. \n \nAnalysis steps \nThe textual discourse analysis conducted in this study was meticulously \ndesigned to dissect the nuanced interactions between users and ChatGPT across \nvarious linguistic formats. This analytical process, deeply rooted in the \nprinciples of discourse analysis as outlined by (Jørgensen & Phillips, 2011), \nwas bifurcated into distinct phases: codification and categorization, each aimed \nat unraveling the complexities of language model interactions. \n \nCodification Process \nThe codification process commenced with a thorough examination of \nthe responses generated by ChatGPT under different linguistic inputs: natural \nlanguage, enhanced natural language and pseudo-code. Each response was \nmeticulously dissected to identify and extract key themes, patterns, and \nlinguistic nuances, aligning with the study's objectives to compare the model's \nproficiency across varied inputs. This phase was instrumental in breaking down \nthe textual data into manageable units for deeper analysis, allowing for the \nisolation of significant elements that denote the model's interpretative \ncapabilities and response accuracy. \n \nCategorization Framework \nAfter codification, the categorization phase involved the systematic \ngrouping of coded data into distinct categories, each representing a core aspect \nof ChatGPT's interaction with the input languages. Drawing from the \n \nActa Sci. (Canoas), 26(1), 157-204, Jan./Feb. 2024 \n174 \nfoundational methodologies of Jørgensen and Phillips (2011), four primary \ncategories were established: \n1. Understanding of Intentions: Assess the ability of ChatGPT to \nidentify and respond to multiple intentions contained in a single \nentry. This category will focus on analyzing whether the model can \ndiscern and address all aspects requested in a complex prompt. \n2. Interpretability: The focus here is on understanding whether \nChatGPT's responses are presented in a way that users can \ncomprehend the reasoning behind the provided solutions, \nparticularly in contexts that require step-by-step problem \nresolutions. The emphasis is on the clarity of communication and \nthe ease of understanding of the responses. \n3. Completeness: This category assesses the extent to which \nChatGPT can cover all the essential aspects of a meal plan. The \nfocus is on the model's ability to include each relevant element, \nensuring that the response is comprehensive and meets all the user's \nneeds and requirements. \n4. Creativity: This dimension assesses the degree of organizational \ncreativity (Lee & Choi, 2003) manifested in the meal suggestions \nproposed by each analytical unit, with a particular focus on the \ndiversity and originality of the options generated by the AI system. \nThrough this evaluation, it becomes possible to ascertain the \neffectiveness with which ChatGPT integrates and reconfigures \nnutritional and culinary knowledge to forge innovative ideas and \nsolutions.  \nThese categories were not merely descriptive but analytical, facilitating \na nuanced understanding of the language model's operational dynamics. Each \ncategory has their specific criterions where the units will be evaluated. Such \nevaluation will indicate if the present unit “Fully meet”, “Partially meet” or \n“Not meet” the respective criteria. The criterions are essential to evaluate how \neach unit performs in each category and could be helpful to identify their \npossible fragilities. \nRegarding the qualitative analyses of the collected materials, we \ndeveloped specific criterions to each one of the categories. Each unit of analysis \nwas analyzed though each criterion to verify if it attempts, partially attempt, or \nnot attempt at these specific criterions. To organize the criterions in their \ncategories we code them with a number, meeting the category he belongs and \n175 \nActa Sci. (Canoas), 26(1), 157-204, Jan./Feb. 2024 \n \na letter to distinguish either one. Down here we present the code, the name of \nthe criterion and respectively their explanation. \n1.A: Understanding User Goals: This includes the model's ability to \nrecognize the main goal of the request, such as creating a Paleo \nmeal plan to gain lean mass within a specific budget.  \n1.B: Budget Adaptation: Considers the model's ability to include the \nbudget factor in its responses, whether through listing prices or \nproviding tips for managing costs efficiently. \n2.A: Structure and Coherence of Responses: Evaluates exclusively \non the organizational and logical aspects of response presentation. \nHighlight the necessity for coherence, clear transitions, and logical \nprogression to ensure responses are intuitively structured and easy \nto follow. \n2.B: Information to Support Informed Decisions: Considers \nwhether the information given effectively supports the user in \nmaking informed and conscious choices. This entails an assessment \nof how well nutritional information, and recommendations are \nintegrated in a way that educates the user about the value of each \nchoice. \n3.A: Comprehensive Coverage of Needs: The analysis focuses on \nhow well each unit addresses all relevant needs within the category \nin question. In this case, it is checked that the meal plans cover all \nmain meals and snacks for each day of the week, ensuring that there \nare no important omissions that could affect the user's daily diet. \n3.B: Daily Meal and Snack Inclusion: Checks for the inclusion of all \nmeal types (breakfast, lunch, dinner, snacks) for each day, ensuring \na comprehensive daily diet plan. \n4.A Meals Development Creativity: Observes the diversity and \nquantity of proposed meal options, focusing on lean proteins, \nhealthy fats, and a selection of vegetables and fruits, in alignment \nwith the user's health and fitness goals. In these criteria we assume \nthat the unites that creates the biggest variety of meals will fully \nmeets the criteria, the units that developed in the middle range will \npartially meets and the unit that build up less meals, in comparison \nwith the others will be evaluate with the designation of not meeting \nthe criteria.  \n \nActa Sci. (Canoas), 26(1), 157-204, Jan./Feb. 2024 \n176 \n4.B Adaptability and Flexibility of Meal Plan: Evaluates suggestions \nfor substituting ingredients or meals to maintain variety and \ninterest, demonstrating flexibility in meal planning. Assesses the \nmodel's creativity in offering adaptable meal plans that can be \ncustomized according to individual preferences, seasonal \navailability of ingredients, or last-minute budget adjustments. \nTo the criterions that involved counting, like the criterion 4.A, the units \nwith the highest counts were denominated as “fully meeting the criteria” and \nrespectively, the units with the lower counts got the denomination of “not \nattempting”.  \n \nNumerical Analytic process \nThe assessment derived from each criterion will be quantified, \nassigning a numerical value to facilitate comprehensive conclusions for each \ncategory and within our overarching results. A value ranges from \"1\" to \"3\" will \nbe employed, corresponding to the unit's fulfillment status: \"1\" signifies the \nunit's failure to meet the criterion, \"2\" indicates partial achievement, and \"3\" \ndenotes full attainment. This methodical quantification will significantly aid in \nappraising the collective performance of each unit across all established \ncategories. \n \nCompositive analysis  \nThe literature on discourse Analysis (Jørgensen & Phillips, 2011) steers \nthe analyst towards a composite examination of outcomes to delineate \ndistinctions among the materials scrutinized. Initially, the findings of each \ncategory are deliberated upon, leveraging the designated criteria for each. \nSubsequently, a holistic analysis is conducted, entailing an integrative \ncomparison of each unit's cumulative performance across the entire evaluation, \nencompassing both criteria and categories. \n \nPrompts Development \nFor the development of this research, which aims to study how \nChatGPT handles complex interactions with multiple intentions, we will first \ndefine what we understand by this type of interaction. We identify complex \ninteractions with multiple intentions as those that, through a single prompt, \n177 \nActa Sci. (Canoas), 26(1), 157-204, Jan./Feb. 2024 \n \nencompass different aspects or functions. It is expected that these actions will \nbe executed in a single request through this prompt. In this way, we develop a \nsynthetic case that can better exemplify what we desire. \nAn organization of a \"weekly meal plan” and a “shopping list\" were the \nselected tasks to execute our analysis of ChatGPT interactions with multiple \nintentions, we consider the complexity and multifaceted nature of meal \nplanning as integral to understanding the depth and flexibility of ChatGPT. This \ntask stands out as a particularly poignant test case due to its inherent \nrequirement for understanding a diverse range of inputs, including dietary \npreferences, nutritional goal and the budget constraints. Such a scenario \ndemands the model not only to grasp the explicit requests made by the user but \nalso to infer underlying intentions and preferences that might not be directly \nstated. \nThe \"weekly meal plan and shopping list\" task encapsulates multiple \nlayers of intention within a single inquiry, challenging ChatGPT to demonstrate \nits capacity for nuanced understanding, contextual interpretation, and creative \nproblem-solving. This task requires the model to navigate through complex \ndietary requirements, optimize for health and budget considerations, and \ngenerate a coherent plan that aligns with the user’s goals. By employing this \ntask as a lens through which to examine ChatGPT's interactions, we aim to \ncapture a broad spectrum of the model's capabilities, from basic comprehension \nto advanced reasoning and innovation. This approach allows us to meticulously \ndissect the model's performance across our different types of inputs. \nIn this subsection, we will outline certain relevant steps for \nimprovement in the writing of prompts and pseudo-codes, the enhancements of \neach type of prompt and our expectations about how ChatGPT can deal with \neach one of them.  \n \nNatural Language prompt  \n \n \nCreate a meal plan for a paleolithic diet for each day of the week, \nconsidering a budget of 50 dollars per week, aiming for lean muscle \ngain, and including a detailed shopping list. Additionally, explain \nthe nutritional benefits of each chosen meal. \n \nActa Sci. (Canoas), 26(1), 157-204, Jan./Feb. 2024 \n178 \nIn this prompt, we can visualize a multi-intentional interaction, where \nit is expected that ChatGPT will be capable of: \n1. Planning the weekly meals, taking into account the indicated \ndietary habits and explaining the nutritional benefits of each meal. \n2. Creating a shopping list, taking into consideration budgetary \nrestrictions. \n \nEnhancement of Prompt in NL \nOnce the intentions and prompt are defined, we will enhance it, \nexploring one of the main characteristics of Generative Artificial Intelligence \n(IAGen), the generation of personalized responses (Jovanovic & Campbell, \n2022). Based on this principle, we make alterations so that, when inserting the \nprompt into ChatGPT, the user is questioned and provides information for the \nLLM to develop the objectives in a personalized and customized manner. \nAs we see in research that addresses the use of prompts in ChatGPT \nboth in the field of education (Mollick & Mollick, 2023), in medicine (Heston \n& Khun, 2023), and in the financial field (Yue et al., 2023), the focus is on \ndeveloping prompts that allow the user to insert specific information. Thus, \nexploring one of the specificities of chatbots, the interaction with the user \n(Heston & Khun, 2023). \nBased on these principles, we model the interactive prompt (Wang et \nal., 2023) so that, after the input, ChatGPT can request information from the \nuser and incorporate their data to ensure the completion of the requested \nprocess, similar to the work of Mollick & Mollick (2023). The incorporation of \nthe elements described above led us to restructure the prompt aiming for \ngeneralization; understood by us as the characteristic that makes the prompt \nusable with different variables and the train of thought, in this way the prompt \ntakes the form: \n \n \nCreate a meal plan for each weekday based on the user's \ninformation, inquire about the diet the user follows, ask about the \ngoal of the diet, inquire about the available budget, make a detailed \nshopping list, and explain the nutritional benefits of each chosen \nmeal. \n179 \nActa Sci. (Canoas), 26(1), 157-204, Jan./Feb. 2024 \n \nThe process of prompt development, even in natural language, can be \nenhanced by following a train of thought, as highlighted by Wei et al. (2022). \nThis approach, known as 'Chain of Thought', involves breaking down and \norganizing natural language into a series of sequential steps. By doing so, it \nguides the language model through a step-by-step reasoning process, mirroring \nhow humans solve complex problems. Essentially, this technique not only \nfacilitates the decomposition of multifaceted tasks into more manageable \ncomponents but also increases the transparency and interpretability of the \nmodel's reasoning process. Moreover, the 'Chain of Thought' proves \nparticularly effective in enhancing the ability of models to deal with problems \nrequiring more detailed and nuanced reasoning, as demonstrated in empirical \nstudies covering areas from arithmetic to symbolic reasoning. Applying such \nprinciples to our prompt, we realize that it takes the following form: \n \n \n \nWe notice here a logical organization that first takes into account the \nuser's diet and financial conditions before developing the meal plan. The \nrequest for the explanation of nutritional benefits was placed closer to the \"meal \nplan\" so that the nutritional information accompanies the plan and not the \nshopping list. Through the prompt, we reinforce the idea that the shopping list \nshould bring the necessary foods for preparing the meals, intending that the \nLLM adheres to such items. \nWe will also adopt one of the techniques brought by prompt \nengineering (Heston & Khun, 2023), in which the prompt requests that the \nchatbots assume a certain role. Fulfilling such a request provides a context to \nthe situation, which can guide the Generative AI to the desired pathway. \nRelating the technique and articulating the prompt, we have the following \nresult. \n \nTo develop a meal plan, inquire about the diet the user follows, ask \nabout the user's goal with the diet, inquire about the available \nbudget, create a meal plan for each weekday based on the user's \ninformation, and explain the nutritional benefits of each chosen \nmeal. Make a detailed shopping list with the necessary foods for \npreparing the meals and the user's budget. \n \nActa Sci. (Canoas), 26(1), 157-204, Jan./Feb. 2024 \n180 \n \n \nWith the prompt structured, contextualized, and organized, we will \nmove on to incorporating emotion into the text of the prompt (Li et al., 2023). \nUsing the emotional intelligence present in the LLM, it is possible to enhance \nits performance by adding certain sentences. Thus, within the context of the \nprompt, which deals with the user's diet, we will include the phrase \"The user's \nhealth depends on the results,\" leading the prompt to take the following form: \n \n \n \nWith these enhancements, we finalize the development of the prompt \nin natural language (LN). This prompt will be used to generate information for \nthe 'Natural Language' analysis unit. We will subsequently present the \nparameters to be inserted in the interactions with the chatbots, such as the type \nof diet and available budget. \n \nPseudocoding \nIn the context of this article, pseudocoding represents a crucial step in \nanalyzing the interaction with ChatGPT. Pseudo-code, as described by \nAct as a nutritionist who will develop a meal plan, inquire about \nthe diet the user follows, ask about the goal the user has with the \ndiet, inquire about the available budget, create a meal plan for each \nweekday based on the user's information, and explain the \nnutritional benefits of each chosen meal. Now act as a domestic \neconomy specialist and make a detailed shopping list with the \nnecessary foods for preparing the meals and the user's budget. \nAct as a nutritionist who will develop a meal plan, inquire about \nthe diet the user follows, ask about the goal the user has with the \ndiet, inquire about the available budget, create a meal plan for each \nweekday based on the user's information, and explain the \nnutritional benefits of each chosen meal. Now act as a domestic \neconomy specialist and make a detailed shopping list with the \nnecessary foods for preparing the meals and the user's budget. The \nuser's health and budget depend on the execution of these tasks. \n181 \nActa Sci. (Canoas), 26(1), 157-204, Jan./Feb. 2024 \n \nCalzadilla (2018), is a powerful tool for structuring complex solutions in a clear \nand comprehensible manner, even for those who do not have extensive \nprogramming experience. It uses language close to natural, with easily \nrecognizable terms and actions, and a symbolism inspired by algebra and \nmathematical logic, making the underlying logic to the instructions more \naccessible. \nIn this subsection, we will transform the prompt developed in the \nprevious chapter into pseudo-code similar to Python coding. This \ntransformation aims to demonstrate how the structure and clarity provided by \npseudo-code can influence the accuracy and efficiency of ChatGPT's responses. \nPseudocoding will serve as a means to organize and detail the steps and \nconditions required for ChatGPT to execute the tasks designated in the prompt. \nEven if there may not be a need for mastery of Python language, it is \nnecessary to be familiar with some of the programming structures of this \nsoftware. The first of these are the “Keywords”, which indicate a specific action \n(Rossum, 2023), and the structures of lines. It is interesting to differentiate \nfunctions from comments, as comments will only serve to guide the work of \nthe LLM, but not exactly defining its function. \nFirst, we will separate each of the functions present in the text into \nindividual lines. Then, we will number them so that the LLM can follow them \nstrictly. We will connect them with the commands “ACT”, “INQUIRE”, \n“CREATE”. Incorporating these concepts into the prompt, its structure in \npseudo-code takes the following format: \n \nActa Sci. (Canoas), 26(1), 157-204, Jan./Feb. 2024 \n182 \n \n \nThe prompt presented above will be the prompt entered into ChatGPT \nfor generating responses for the 'Pseudo-code' analysis unit. Along with the \nprompt, the usage parameters will be inserted, which are presented in the \nfollowing session. \n \nWhat we expect of this different type of inputs?  \nIn the exploration of ChatGPT's interactions with varied input forms, \nour anticipation pivots on understanding how different types of prompts shapes \nthe model's performance. Drawing from our discussions, this subsection delves \ninto the expected outcomes, potential advantages, or disadvantages tied to each \ninput type, alongside contemplating the technical feasibility and inherent \nchallenges of enhancing interactions with ChatGPT. \nNatural language inputs, characterized by their intuitive and user-\nfriendly nature, are presumed to facilitate broader accessibility, allowing users \nwith minimal technical background to interact with ChatGPT effectively \n(Wolfram, 2023). The hypothesis suggests that while natural language prompts \noffer ease of use, they might lead to ambiguities that challenge the model's \nprecision in understanding and executing user intentions. The inherent \nvagueness and the multiplicity of interpretations available in natural language \n1) ACT as a nutritionist who will develop a meal plan; \n2) INQUIRE about the diet the user follows; \n3) INQUIRE about the intended goal with the diet; \n4) INQUIRE about the available budget; \n5) Using the responses from 2, 3, and 4, CREATE a meal plan for \neach weekday that explains the nutritional benefits of each chosen \nmeal; \n6) ACT as a domestic economy specialist; \n7) Using the response from 4, CREATE a shopping list with the \nnecessary foods for preparing the meals. \n8) The user's health depends on this task. \n183 \nActa Sci. (Canoas), 26(1), 157-204, Jan./Feb. 2024 \n \ncould potentially hinder the model's performance in generating targeted \nresponses, highlighting a trade-off between accessibility and precision. \nPseudo-code input, with their inherent structure and logic, stand at the \nforefront of enhancing ChatGPT's precision in query comprehension and \nresponse generation. These inputs, by embodying the essence of programming \nlogic, are designed to significantly reduce ambiguity, steering ChatGPT toward \noutputs that are not only precise but also aligned with the functional \nexpectations of the query (Kusal et al., 2022; Wu & Feng, 2018). This precision, \nrooted in the structured nature of pseudo-code, facilitates a clearer \ncommunication channel between the user and ChatGPT, ensuring that the \nmodel's responses are directly relevant and applicable to the task at hand. \nFurthermore, the endeavor to refine ChatGPT's processing of natural \nlanguage inputs, aiming to mitigate ambiguities while nurturing the model's \ninventive output, complements the advancements made with pseudo-code. It \nentails a sophisticated recalibration of the model's interpretive algorithms, \nstriking a delicate balance that respects user intentions while fostering \nChatGPT's inherent capacity for generating creative and contextually rich \nresponses (Wei et al., 2022). \nIn essence, the exploration of the pseudo-code inputs can reveal a \ncompelling landscape of advantages that these structured inputs. It can not only \npromise to elevate ChatGPT's operational accuracy but also open avenues for a \nmore defined, logical, and goal-oriented interaction framework. While \nmaintaining a vigilant stance on the challenges and technical intricacies \ninvolved, the strategic emphasis on pseudo-code inputs heralds a promising \ndirection for enriching ChatGPT's interface with human language.  \n \nDefinition of Parameters for Data Generation \nFor this study, we will use the Chat GPT 4.0 turbo version, which \nincludes the DALL.E, web browsing, and analysis functions in its guidelines. \nTo ensure parallelism between the units of analysis, we defined standardized \nresponses to answer the questions that the respective chatbots will ask the user. \nThe choice of the Paleolithic diet, lean muscle gain as a fitness goal, and a \nbudget of 50 dollars per week were deliberate and strategic, designed to mirror \nrealistic scenarios that users might input into ChatGPT. This selection was \nguided by several key considerations: \n \nActa Sci. (Canoas), 26(1), 157-204, Jan./Feb. 2024 \n184 \nDiet: The Paleolithic diet was selected due to its popularity and \nspecificity. It represents a modern nutritional plan based on the \npresumed diet of early humans, focusing on whole foods, and \nexcluding processed foods, grains, and dairy (Frączek et al., 2021). \nThis diet's restrictive nature makes it a pertinent choice for testing \nChatGPT's capability to generate meal plans that adhere to specific \ndietary frameworks. It challenges the model's ability to understand and \napply nutritional principles within the constraints of a predefined \ndietary pattern, aligning with the study's goal to evaluate the \nadaptability and precision of ChatGPT in generating customized \nadvice. \nGoal: The objective of lean muscle gain complements the chosen diet \nby introducing a common fitness goal that requires precise nutritional \nstrategies (Altyar, 2020). This goal necessitates a balance of \nmacronutrients to support muscle development while adhering to the \ndietary restrictions of the Paleolithic framework. It tests ChatGPT's \ncapacity to tailor nutritional recommendations that not only fit dietary \nconstraints but also support specific fitness objectives, showcasing the \nmodel's potential in providing nuanced and goal-oriented guidance. \nBudget: Establishing a budget constraint introduces a layer of \ncomplexity to the task, simulating a common real-world consideration \nfor individuals following specific diet plans. It requires ChatGPT to \noptimize meal planning within financial limitations, reflecting the \nmodel's \nability \nto \nincorporate \neconomic \nfactors \ninto \nits \nrecommendations. This parameter is crucial for evaluating ChatGPT's \neffectiveness in generating practical and economically feasible meal \nplans, ensuring that the proposed solutions are not only nutritionally \nadequate but also accessible to users with budgetary considerations. \nBy setting these parameters, the study aims to explore ChatGPT's \nproficiency in navigating the intricacies of diet planning, nutritional \noptimization, and budget management. These specific choices align with the \nstudy's objectives to assess the depth, accuracy, and practicality of ChatGPT's \nresponses to complex, multi-dimensional queries, providing a comprehensive \nevaluation of its capabilities in delivering personalized and contextually \nrelevant advice. \nTo view the dialogs generated using the selected LLM for the study, see \nappendices C, D and E. \n185 \nActa Sci. (Canoas), 26(1), 157-204, Jan./Feb. 2024 \n \nDATA ANALYSIS \nIn this subsection, we will conduct a careful analysis of the interactions \nwith ChatGPT, applying the discourse analysis method (Jørgensen & Phillips, \n2011). Our goal is to understand ChatGPT's responses to different types of \ninputs, such as natural language and pseudo-codes. We will focus on identifying \npatterns, contradictions, and unique elements in the responses, assessing the \nefficiency and accuracy of ChatGPT in various contexts. This analysis seeks to \nreveal the limits and capabilities of the model, providing significant insights \ninto human-computer interaction in the field of conversational artificial \nintelligence. \nIn Figure 1, we present the results from the analysis of the units in each \nof the four selected categories. In the graphics it’s possible to visualize the unit \nattempted in every criterion of the categories. More details, such as elements \nthat led to the inference of each classification. \n \nFigure 1 \nCategory 1: Understanding of Intentions  \n \nNL\nE.NL\nPseudo\nUnderstanding Intentionality\nUnderstanding User\nGoals\nBudget Adaptation\nPartially meets the \ncriteria\nDoes not meet the \ncriteria\nFully Meets the \ncriteria\n \nActa Sci. (Canoas), 26(1), 157-204, Jan./Feb. 2024 \n186 \n \nIn the analysis of Understanding Intentionality across various units of \nChatGPT, the results demonstrate a consistent proficiency in identifying and \nadhering to user intentions. Unit A (Natural Language) and Unit B (Enhanced \nNatural Language) both accurately recognized and addressed the user's \nobjective of developing a Paleo diet-based meal plan focused on lean mass gain, \nwithin a specified budget. These units not only grasped the main goal but also \nincorporated detailed pricing information, directly aligning their responses with \nthe user's financial constraints. \nUnit C (Pseudo-code) effectively met the core request for a budget-\nfriendly meal plan. However, unlike Units A and B, which provided detailed \npricing, Unit C offered general cost-saving tips without specific price details. \nThis distinction suggests a difference in approach to integrating budget \nconsiderations into the meal planning process. \nThis examination indicates ChatGPT's capability to comprehend and \nexecute requests across different forms of input, from natural to structured \nlanguages, with a particular emphasis on understanding user goals and adapting \nresponses to budget constraints. The model consistently fulfills the primary \nrequirements of the tasks, demonstrating its adaptability and accuracy in \nresponding to varied user intents. \nIn examining interpretability across different units (Figure 2), there's a \nnotable differentiation in how each unit structures and presents its responses to \nfacilitate user comprehension. Unit A, employing natural language, partially \naligns with the criterion, integrating nutritional benefits close to meal \ndescriptions, which, while informative, may slightly disrupt the flow of the \nmeal plan presentation. Conversely, Units B and C, employing enhanced \nnatural language and pseudo-code respectively, fully adhere to the criteria, \nshowcasing a clear and logical arrangement of nutritional information \nalongside meal plans, thereby enhancing user understanding.  \n \n187 \nActa Sci. (Canoas), 26(1), 157-204, Jan./Feb. 2024 \n \nFigure 2 \nCategory 2: Interpretability \n \n \nIn the category of Completeness, which evaluates the thoroughness of \nChatGPT's meal plans, our analysis scrutinized how each unit tackled the \nintegration of daily dietary requirements (Figure 3). Comprehensive Coverage \nof Needs was the first criterion examined, assessing the all-encompassing \nnature of the meal plans provided by each unit. Natural Language (Unit A) \npartially met this criterion by covering all main meals and snacks, yet it lacked \nin offering a broad variety within each meal category. Conversely, Enhanced \nNatural Language (Unit B) and Pseudo-code (Unit C) fully satisfied this \ncriterion by presenting complete and balanced dietary plans, with Unit B \nstanding out for its diverse range of options. \n \nNL\nE.NL\nPseudo\nInterpretability\nStructure and Coherence of Responses\nInformation to Support Informed Decisions\nDoes not meet the \ncriteria\nPartially meets the \ncriteria\nFully Meets the \ncriteria\n \nActa Sci. (Canoas), 26(1), 157-204, Jan./Feb. 2024 \n188 \nFigure 3 \nCategory 3: Completeness:  \n \n \nFor the criterion of Daily Meal and Snack Inclusion, which looked at \nthe inclusion of diverse meal types throughout a day, Natural Language (Unit \nA) only partially met this, offering limited snack options. Enhanced Natural \nLanguage (Unit B) and Pseudo-code (Unit C) did not meet the criteria, as \nneither provided snack suggestions in their daily plans.  \nIn the Creativity category, the focus was on the language model's \ninnovation in generating meal plans (Figure 4). Meals Development Creativity \nwas our first lens, evaluating the diversity and quantity of meal options \nproposed. Natural Language (Unit A) did not meet this criterion due to its \nlimited variety, offering only four meals for the entire week. In contrast, \nEnhanced Natural Language (Unit B) and Pseudo-code (Unit C) excelled, each \ndeveloping fifteen diverse meals, thereby fully meeting the criterion.  \nFor Adaptability and Flexibility of Meal Plan, the assessment looked at \nthe model's ability to suggest ingredient substitutions or meal variations. \nNatural Language (Unit A) failed to meet this criterion, providing a rigid set of \nmeals with no evident flexibility. Enhanced Natural Language (Unit B) and \nNL\nE.NL\nPseudo\nCompleteness\nComprehensive Coverage of Needs:\nDaily Meal and Snack Inclusion\nDoes not meet the \ncriteria\nPartially meets the \ncriteria\nFully Meets the \ncriteria\n189 \nActa Sci. (Canoas), 26(1), 157-204, Jan./Feb. 2024 \n \nPseudo-code (Unit C) partially met the criteria by indicating the possibility of \nadjusting meals based on personal preference, though they presented fixed meal \nplans.  \n \nFigure 4 \nCategory 4: Creativity \n  \n \nDiscussion of Results \nThe definition of the four categories enabled a comprehensive \nunderstanding of the model's interaction with the form of language input by the \nuser. We will analyze the nuances brought by each category together at first \ngraphically, visible in the Figure 5 of integrated results.  \n \nNL\nE.NL\nPseudo\nCreativity\nMeals Development Creativity\nAdaptability and Flexibility of Meal Plan\nDoes not meet the \ncriteria\nPartially meets the \ncriteria\nFully Meets the \ncriteria\n \nActa Sci. (Canoas), 26(1), 157-204, Jan./Feb. 2024 \n190 \nFigure 5 \nOverall results \n  \n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\nNL\nE.NL\nPseudo\nUnits integraded results \nAdaptability and Flexibility of Meal Plan\nMeals Development Creativity\nDaily Meal and Snack Inclusion\nComprehensive Coverage of Needs:\nInformation to Support Informed Decisions\nStructure and Coherence of Responses\nBudget Adaptation\nUnderstanding User Goals\n191 \nActa Sci. (Canoas), 26(1), 157-204, Jan./Feb. 2024 \n \nTo enhance the clarity and academic rigor of the analysis, we leveraged \nnumerical evaluation methods, as detailed in a preceding chapter. The graphical \nrepresentation succinctly illustrates the comparative performance of the \ndifferent input methods. It reveals that while Natural Language displayed a \nrelatively lower performance, both Enhanced Natural Language and Pseudo-\ncode exhibited superior capabilities.  \nIn examining the \"Understanding Intentionality\" category, it is evident \nfrom the Figure 1 that all three units attained an equivalent performance level \nacross the defined criteria. This uniformity suggests that ChatGPT's ability to \ncomprehend and accurately fulfill user requests is not contingent on the input \nformat. Through this category and its specific criteria, we observe that \nChatGPT's response effectiveness is consistent across different user inputs. \nThis insight shifts the focus to evaluating how ChatGPT processes these \nrequests and responds to varied types of inputs. \nWhen contrasting natural language with its enhanced counterpart, \nwhich incorporates prompt engineering techniques (Heston & Khun, 2023) and \nthe chain of thoughts methodology (Ling et al., 2023; Wei et al., 2022), as well \nas emotional interaction with the chat (Li et al., 2023), notable advantages are \nobserved. In terms of Interpretability and Creativity, Enhanced Natural \nLanguage distinguishes itself by presenting a meal plan with a wider variety of \noptions and a more coherent informational flow. Our hypothesis suggested that \nwhile Natural Language is more accessible, the potential absence of guidance \nor the misplacement of requests within the structure of this input could \ncontribute to significant disparities in Creativity and Interpretability. Through \nthe application of prompt engineering (Heston & Khun, 2023), specific \ninstructions for the tasks were inferred, thereby providing more targeted \nguidance for the LLM's vectorization process, resulting in more inventive \nresponses. Furthermore, the adoption of the chain of thought approach (Ling et \nal., 2023a; Wei et al., 2022) enhanced the organization, as evidenced in \nCriterion 2.A, Structure and Coherence of Responses. The responses were \nmethodically segmented by the LLM, organizing the answers in alignment with \nthe sequence of instructions. \nIn engaging with ChatGPT through a code-resembling language, \nspecifically pseudo-code, referred to as Unit C, we incorporated chain of \nthought techniques (Ling et al., 2023a, 2023b; Wei et al., 2022) and prompt \nengineering (Heston & Khun, 2023) to formulate our pseudo-code. This \napproach yielded concise text that was expedient to construct. Despite its \nbrevity, this unit exhibited commendable performance in our evaluations, \n \nActa Sci. (Canoas), 26(1), 157-204, Jan./Feb. 2024 \n192 \nparalleling that of the Enhanced Natural Language. Regarding Interpretability, \nboth units excelled, fully meeting the criteria with their structured and coherent \nresponses. They adeptly presented detailed nutritional information in a \nlogically organized manner, thereby enhancing user comprehension, and \nfacilitating informed decision-making. This achievement reflects the model's \ncapacity to process and articulate information consistent with user expectations, \nreinforcing the significance of clarity and user understanding in human-\ncomputer interaction, as highlighted in the literature (Kusal et al., 2022; Wu & \nFeng, 2018). Concerning Completeness, both units were highly effective, \nencompassing all necessary elements of a meal plan. This observation \ncorroborates Lee & Choi's (2003) emphasis on the critical role of \ncomprehensive knowledge creation and dissemination within organizational \ncontexts, principles equally applicable to the model's generation of complete \nand thorough responses. In assessing creativity, both units demonstrated \nequivalent levels of performance across criteria, showcasing their innovative \ncapabilities in meal planning.  \nLeveraging the pseudo-code approach enabled us to achieve the \nrequisite simplicity. By employing an input method that can severally decrease \nthe potential for nuances in language and clearly and systematically delineates \nthe sequence of functions or requirements, we devised a more efficient means \nto elicit responses comparable to those obtained from the Enhanced Natural \nLanguage using our Pseudo-code. Regarding simplicity, Pseudo-code \ndemonstrated an easier and quicker method for composing very efficient \nprompts. \n \nCONCLUSION \nThroughout this study, our objective was to explore and elucidate the \ndistinctions among responses generated by ChatGPT when interfacing with \nvarious forms of inputs. The initial goals included examining ChatGPT's \ncapability to process multiple intentions and input formats, extending from \nnatural language to more structured forms, such as pseudo-code. The \nmethodology employed, integrating case study approaches (Yin, 2001) with \ndiscourse analysis (Jørgensen & Phillips, 2011), facilitated a thorough and \ncontextual investigation of these interactions. \nThe findings of this study elucidate how distinct forms of inputs \nuniquely influence the ChatGPT performance. Natural language, despite its \naccessibility and intuitiveness, often yielded responses that were limited in \n193 \nActa Sci. (Canoas), 26(1), 157-204, Jan./Feb. 2024 \n \ninterpretability and lacked creative innovation. Conversely, enhanced natural \nlanguage, augmented through sophisticated prompt engineering and chain of \nthought techniques, exhibited marked enhancements in both interpretability and \ncreativity, rendering it a superior tool for tasks necessitating nuanced and \ninventive responses. The adoption of pseudo-code marked a shift towards a \nmore structured mode of communication akin to programming languages, \nachieving parity in performance with Enhanced Natural Language.  \nThe selection of the optimal language for interacting with ChatGPT \nought to be contingent upon the user's specific context and requirements. It is \ncritical to recognize that employing natural language, particularly with multiple \nintentions, may result in unpredictable and varied responses. In contrast, \npseudo-code offers a structured approach that facilitates more explicit \nrecognition of intentions by the language model, leading to clearer and more \ndeterministic outcomes. This attribute is particularly advantageous in situations \nwhere precision and clarity in instructions are essential. Furthermore, the \nadoption of pseudo-code can enhance the determinism of interactions with \nChatGPT, proving invaluable in contexts demanding consistent and predictable \nresponses. Additionally, from a time efficiency perspective, pseudo-code can \nbe composed more swiftly due to its reduced character count, offering a \npractical advantage in rapid response generation. \nThis article has delineated the operational mechanics of ChatGPT, \ntracing the journey from user input to the language model's generated output. \nWe have meticulously explored various forms of language input and examined \nthe nuanced responses elicited by each. It is evident that every form of language \npossesses its unique characteristics and should be employed in alignment with \nthe user's specific intentions. \nIn this research, we have advanced the methodology of employing \npseudo-codes, observing their capacity to generate results that are more \ndeterministic, concise, and definitive. We introduced the concept of Pseudo-\ncode Engineering, where a natural language prompt undergoes transformation \ninto pseudo-code. The comprehensive analysis underscores that Pseudo-code \nemerge as optimal choices for formulating prompts for ChatGPT. \nIn conclusion, the exploration of Pseudo-code Engineering within our \nstudy illuminates a promising avenue for refining interactions with Large \nLanguage Models like ChatGPT, offering a spectrum of future applications \nspanning from educational tools to sophisticated software development. Our \nresults encourage subsequent research to assess the predictability and stability \nof AI responses when interfaced through pseudo-code, emphasizing the need \n \nActa Sci. (Canoas), 26(1), 157-204, Jan./Feb. 2024 \n194 \nfor a structured approach to enhance AI comprehension and output. The \nconcept of Pseudo-code Engineering not only aims to streamline AI \ncommunications but also seeks to unlock a new realm of possibilities in human-\nAI interaction, making it more precise, efficient, and accessible across varied \nsectors. Anticipating further experimentation and analysis, we remain \ncommitted to investigating the full potential of pseudo-codes in shaping the \nfuture of AI technologies, thereby ensuring their expanded utility and \neffectiveness. \nThis investigation illuminates ChatGPT's proficiencies and constraints \nin processing diverse linguistic inputs, providing indispensable insights for \nusers aiming to optimize their engagement with this sophisticated language \nmodel. The adoption of pseudo-codes holds significance across various \ndomains, from educational endeavors to software development, heralding new \navenues for research and practical implementation. \n \nACKNOWLEDGEMENTS \nThis study was financed in part by the Coordenação de \nAperfeiçoamento de Pessoal de Nível Superior - Brasil (CAPES) - Finance \nCode 001. \n \nREFERENCES \nAltyar, A. E. (2020). Lean Muscle Gain Diet-A Study from The Saudi Food \nMarket. Life Science Journal, 7(1), 15–20. \nhttps://doi.org/10.7537/marslsj170120.03  \nCalzadilla, Juan. C., Mavis. Matos, Lianne. (2018). A Raxintrização: \nRequisito Essencial para a Resolução de Problemas com o Uso de \numa Linguagem de Programação. Luz, 17(3), 30–43. \nCarvalho, A. C. P. de L. F. de. (2021). Inteligência Artificial: Riscos, \nbenefícios e uso responsável. Estudos Avançados, 35(101), 21–36. \nhttps://doi.org/10.1590/s0103-4014.2021.35101.003  \nHeston, T. F., & Khun, C. (2023). Prompt Engineering in Medical Education. \nInternational Medical Education, 2(3), 198–205. \nhttps://doi.org/10.3390/ime2030019  \n195 \nActa Sci. (Canoas), 26(1), 157-204, Jan./Feb. 2024 \n \nJho, H. (2020). Discussion for how to apply artificial intelligence to physics \neducation. New Physics: Sae Mulli, 70(11), 974–984. \nhttps://doi.org/10.3938/NPSM.70.974  \nJørgensen, M., & Phillips, L. (2011). Discourse analysis as theory and \nmethod. Sage. \nJovanovic, M., & Campbell, M. (2022). Generative Artificial Intelligence: \nTrends and Prospects. Computer, 55(10), 107–112. \nhttps://doi.org/10.1109/MC.2022.3192720  \nKhurana, D., Koli, A., Khatter, K., & Singh, S. (2022). Natural language \nprocessing: State of the art, current trends and challenges. Multimedia \nTools and Applications, 82(3713–3744). \nhttps://doi.org/10.1007/s11042-022-13428-4  \nKusal, S., Patil, S., Choudrie, J., Kotecha, K., Mishra, S., & Abraham, A. \n(2022). AI-based Conversational Agents: A Scoping Review from \nTechnologies to Future Directions. IEEE Access. \nhttps://doi.org/10.1109/ACCESS.2022.3201144  \nLee, H., & Choi, B. (2003). Knowledge Management Enablers, Processes, \nand Organizational Performance: An Integrative View and Empirical \nExamination. Journal of Management Information Systems, 20(1), \n179–228. \nLing, Z., Fang, Y., Li, X., Huang, Z., Lee, M., Memisevic, R., & Su, H. \n(2023). Deductive Verification of Chain-of-Thought Reasoning. \nhttp://arxiv.org/abs/2306.03872  \nMollick, E., & Mollick, L. (2023). Using AI to Implement Effective Teaching \nStrategies in Classrooms: Five Strategies, Including Prompts. \nhttps://doi.org/10.2139/ssrn.4391243  \nOda, Y., Fudaba, H., Neubig, G., Hata, H., Sakti, S., Toda, T., & Nakamura, S. \n(2015). Learning to Generate Pseudo-Code from Source Code Using \nStatistical Machine Translation. 2015 30th IEEE/ACM International \nConference on Automated Software Engineering (ASE), 574–584. \nhttps://doi.org/10.1109/ASE.2015.36  \nRossum, G. van. (2023). The Python Language Reference. Python Software \nFoundation. \nWang, J., Shi, E., Yu, S., Wu, Z., Ma, C., Dai, H., Yang, Q., Kang, Y., Wu, J., \nHu, H., Yue, C., Zhang, H., Liu, Y., Li, X., Ge, B., Zhu, D., Yuan, Y., \n \nActa Sci. (Canoas), 26(1), 157-204, Jan./Feb. 2024 \n196 \nShen, D., Liu, T., & Zhang, S. (2023). Prompt Engineering for \nHealthcare: Methodologies and Applications. \nhttp://arxiv.org/abs/2304.14670  \nWei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E., Le, \nQ., & Zhou, D. (2022). Chain-of-Thought Prompting Elicits \nReasoning in Large Language Models. \nhttp://arxiv.org/abs/2201.11903  \nWolfram, S. (2023). What Is ChatGPT Doing … and Why Does It Work? \nhttps://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-\nand-why-does-it-work/ . \nWu, Y., & Feng, J. (2018). Development and Application of Artificial Neural \nNetwork. Wireless Personal Communications, 102(2), 1645–1656. \nhttps://doi.org/10.1007/s11277-017-5224-x  \nYin, R. K. (20). Case study research: Design and methods (4. ed., [Nachdr.]). \nSage. \nYue, T., Au Chi Chu, D., Au Chi Chung, X., Scientist, D., Yuen, K. I., & \nBarrister Albert Luk, P. (2023). Democratizing financial knowledge \nwith ChatGPT by OpenAI: Unleashing the Power of Technology. In \nQMUL. https://ssrn.com/abstract=4346152  \n \n \n197 \nActa Sci. (Canoas), 26(1), 157-204, Jan./Feb. 2024 \n \nAPENDICES \nUnit A – Natural Language \nAvailable at https://chat.openai.com/share/d99607f7-b2af-4fe5-9208-\n5211b54c401d \n \n \n \nActa Sci. (Canoas), 26(1), 157-204, Jan./Feb. 2024 \n198 \n \n \nUnit B – Enhanced Natural Language \nAvailable at: https://chat.openai.com/share/f92051f4-0ffe-4f53-ba3a-\ne31f3805cd2d \n \n199 \nActa Sci. (Canoas), 26(1), 157-204, Jan./Feb. 2024 \n \n \n \n \nActa Sci. (Canoas), 26(1), 157-204, Jan./Feb. 2024 \n200 \n \n \n201 \nActa Sci. (Canoas), 26(1), 157-204, Jan./Feb. 2024 \n \n \n \nUnit C – Enhanced Natural Language \nAvailable \nat: \nhttps://chat.openai.com/share/1923921c-5042-4c5e-\n9015-061c7332e61c \n \n \nActa Sci. (Canoas), 26(1), 157-204, Jan./Feb. 2024 \n202 \n \n \n \n203 \nActa Sci. (Canoas), 26(1), 157-204, Jan./Feb. 2024 \n \n \n \n \nActa Sci. (Canoas), 26(1), 157-204, Jan./Feb. 2024 \n204 \n \n \n",
  "categories": [
    "cs.CL",
    "cs.AI",
    "cs.HC",
    "cs.CY",
    "J.4; K.3; I.2"
  ],
  "published": "2024-04-08",
  "updated": "2024-04-08"
}