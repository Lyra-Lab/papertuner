{
  "id": "http://arxiv.org/abs/2404.10887v2",
  "title": "Grounded Language Agent for Product Search via Intelligent Web Interactions",
  "authors": [
    "Moghis Fereidouni",
    "Adib Mosharrof",
    "A. B. Siddique"
  ],
  "abstract": "The development of agents powered by large language models (LLMs) to\naccomplish complex high-level user intents, has attracted significant attention\nrecently. However, employing LLMs with billions of parameters (e.g., GPT-4) may\nincur substantial costs on top of handcrafting extensive prompts. To address\nthis, we introduce a Grounded Language Agent for Intelligent Web Interactions,\nnamed GLAINTEL. GLAINTEL employs Flan-T5 as its backbone and is flexible in\ntraining in various settings: unsupervised learning, supervised learning, and\nunsupervised domain adaptation. Specifically, we tackle both the challenge of\nlearning without human demonstrations and the opportunity to leverage human\ndemonstrations effectively when those are available. Additionally, we explore\nunsupervised domain adaptation for cases where demonstrations are limited to a\nspecific domain. Experimental evaluations across diverse setups demonstrate the\neffectiveness of GLAINTEL in unsupervised settings, outperforming in-context\nlearning-based approaches that employ larger models with up to 540 billion\nparameters. Surprisingly, behavioral cloning-based methods that\nstraightforwardly use human demonstrations do not outperform unsupervised\nvariants of GLAINTEL. Additionally, we show that combining human demonstrations\nwith reinforcement learning-based training yields results comparable to methods\nutilizing GPT-4. The code is available at:\nhttps://github.com/MultifacetedNLP/WebAgents-Unsupervised.",
  "text": "Grounded Language Agent for Product Search via Intelligent Web\nInteractions\nMoghis Fereidouni, Adib Mosharrof, A.B. Siddique\nUniversity of Kentucky, Lexington, KY, USA\nmoghis.fereidouni@uky.edu, amo304@g.uky.edu, siddique@cs.uky.edu\nAbstract\nThe development of agents powered by large\nlanguage models (LLMs) to accomplish com-\nplex high-level user intents, has attracted signif-\nicant attention recently. However, employing\nLLMs with billions of parameters (e.g., GPT-4)\nmay incur substantial costs on top of handcraft-\ning extensive prompts. To address this, we\nintroduce a Grounded Language Agent for In-\ntelligent Web Interactions, named GLAINTEL.\nGLAINTEL employs Flan-T5 as its backbone\nand is flexible in training in various settings: un-\nsupervised learning, supervised learning, and\nunsupervised domain adaptation. Specifically,\nwe tackle both the challenge of learning with-\nout human demonstrations and the opportu-\nnity to leverage human demonstrations effec-\ntively when those are available. Additionally,\nwe explore unsupervised domain adaptation\nfor cases where demonstrations are limited to\na specific domain. Experimental evaluations\nacross diverse setups demonstrate the effective-\nness of GLAINTEL in unsupervised settings,\noutperforming in-context learning-based ap-\nproaches that employ larger models with up\nto 540 billion parameters. Surprisingly, behav-\nioral cloning-based methods that straightfor-\nwardly use human demonstrations do not out-\nperform unsupervised variants of GLAINTEL.\nAdditionally, we show that combining human\ndemonstrations with reinforcement learning-\nbased training yields results comparable to\nmethods utilizing GPT-4. The code is available\nat: https://github.com/MultifacetedNLP/Web-\nAgents-Unsupervised.\n1\nIntroduction\nLarge Language Models (LLMs) have demon-\nstrated their proficiency in diverse tasks such as text\nclassification, information extraction, and question\nanswering (Bommasani et al., 2021; Brown et al.,\n2020; Vaswani et al., 2017; Raffel et al., 2020; Rad-\nford et al., 2019). Similarly, reinforcement learn-\ning (RL) has evolved as a powerful paradigm for\ntraining intelligent agents to navigate complex en-\nvironments (Huang et al., 2022b; Ahn et al., 2022;\nLiang et al., 2023). Moreover, recent research high-\nlights the capabilities of agents powered by LLMs.\nFor example, agents utilizing GPT-4 can explore\nthe virtual world in Minecraft, acquire a diverse\nset of composable skills, and exhibit exceptional\nproficiency in playing the game (Wang et al., 2024).\nThe exceptional amount of world knowledge, often\nderived from vast text datasets, opens up possibil-\nities for developing LLM-assisted intelligent web\nnavigation agents capable of navigating and inter-\nacting with web pages akin to humans.\nDespite their remarkable capabilities, off-the-\nshelf pre-trained LLMs face challenges in ground-\ning and aligning themselves in interactive web envi-\nronments (Mahowald et al., 2023). This limitation\nhampers their functional competence without ad-\nditional customization. Additionally, employing\nLLMs with billion-scale parameters, such as GPT-\n4, may incur substantial costs on top of handcraft-\ning extensive prompts. On the other hand, train-\ning smaller LLMs (e.g., Flan-T5) as agents can be\nchallenging. For instance, consider a real-world\nproduct search scenario, where effective query for-\nmulation requires the agent to operate over a huge\naction space (i.e., language vocabulary), and navi-\ngating through diverse web pages poses additional\nchallenges that need strategic exploration due to\nthe presence of different actions on each page (i.e.,\ndynamic action space). This complexity prevents\nthe straightforward utilization of an action head on\ntop of LLM. Moreover, the challenge extends to\npreserving long-term memory capabilities, which\nare crucial for comparing items or backtracking\nduring the search process.\nIn this work, we introduce GLAINTEL, a\nGrounded Language Agent designed for In-\ntelligent Web Interactions. Given a user’s intent\nspecifying a product requirement, GLAINTEL for-\nmulates queries, navigates diverse web pages, and\narXiv:2404.10887v2  [cs.CL]  26 Jan 2025\nGoal: I am looking for a queen sized bed that is black, and price lower than 140.00 dollars.\nResult Page\nObservation:\n[User Instruction]:I am \nlooking for a queen sized …\n[Button] Back to Search\n[Text] Page 1 (Total \nresults: 50)\n[button] Next >\n[Link] B09NYM2SKT\n[Heading] ZTOZZ Isola \nPlatform Bed with 4 Storage \nDrawers – Queen …\n[Price] $379.0\n[Link] B09K46KXGR\n[Heading] Queen Size \nUpholstered Platform Bed …\n… …\nAgent\nDynamic Action Space:\nClick on [button] Back to Search\nClick on [button] Next >\nClick on [Link] B09NYM2SKT\nClick on [Link] B09K46KXGR\nClick on [Link] B09M714F8Z\nClick on [Link] B08ZXXKPSC\n…\nText Mode\nDistribution over \nActions\nClick …\nPre-trained Encoder\nUser Goal\nPrevious \nObservations\nCurrent \nObservation\nPre-trained Decoder\nAction: ___\nReward\nPPO\nFigure 1: Overview of GLAINTEL: Our agent employs the Flan-T5 architecture and incorporates a language\nmodeling head to adapt to dynamic action space, while the value head enables precise value estimation.\nexecutes various actions to identify, customize, and\npurchase the desired product. GLAINTEL uses\nthe open-source Flan-T5 language model (i.e., 780\nmillion parameters) as its backbone and can be\nflexibly trained in various scenarios: unsupervised,\nsupervised, and unsupervised domain adaptation\nsettings. Specifically, we address the following\nresearch questions.\n• RQ1: Effectiveness of Unsupervised Learning:\nCan LLM-based agents learn to address effec-\ntive query generation and exploration of com-\nplex web pages with no human demonstrations?\n• RQ2: Impact of Human Demonstrations: Can\nincorporating human demonstrations facilitate\nLLM-based agents to improve their overall per-\nformance? How to effectively leverage human\ndemonstrations for training robust agents?\n• RQ3: Unsupervised Domain Adaptation: Can\nLLM-based agents generalize to new, unseen\nproduct categories where no human demonstra-\ntions are available?\nWe employ a language modeling head to accom-\nmodate a dynamic action space and introduce an\nadditional value head for precise value estimates.\nFigure 1 provides an overview of GLAINTEL. The\nuser’s goal and observation are sequentially passed\nto the model at each step. First, we obtain the in-\nput representation for every potential action token\nand compute the normalized joint probability for\neach action conditioned on the user goal and obser-\nvation. Following the estimation of each action’s\nprobability, we apply a softmax function over these\nprobabilities and sample an action according to this\ndistribution. We fine-tune the agent using the Prox-\nimal Policy Optimization (PPO) algorithm (Dhari-\nwal et al., 2017).\nWe conduct extensive experimental evaluations\nacross diverse setups using the WebShop environ-\nment (Yao et al., 2022). WebShop is a simulated\nyet realistic e-commerce web platform featuring\n1.18 million real-world products and 12,087 crowd-\nsourced natural language intents. Based on our em-\npirical study, we demonstrate that training Flan-T5\n(e.g., 780 million parameters) in the unsupervised\nsetting (i.e., no human demonstrations) can outper-\nform in-context learning methods (Sridhar et al.,\n2023) that rely on models with up to 540 billion\nparameters. To quantify the impact of human super-\nvision, we utilized 1010 human demonstrations for\ntraining supervised learning models using behavior\ncloning (BC) (Pomerleau, 1988).\nOur findings indicate that incorporating human\ndemonstrations through straightforward BC does\nnot produce superior results when compared to\nthe unsupervised RL-based PPO algorithm. Fur-\nthermore, our investigations reveal that leveraging\nhuman demonstrations through BC and then fur-\nther training the agent with PPO in the unsuper-\nvised setting leads to the best results. Remarkably,\nthis approach achieves results comparable to the\nmethod (Ma et al., 2023) that utilizes GPT-4. In\nthe unsupervised domain adaptation (UDA) experi-\nment, we observe that incorporating human demon-\nstrations from a single category enables the agent to\ngeneralize to new product categories where no hu-\nman demonstrations are available. Additionally, we\nevaluate our trained model on a real website eBay\nwithout any additional fine-tuning, which shows\ncomparable results to methods that use the state-of-\nthe-art GPT-4 model.\n2\nProposed Agent: GLAINTEL\n2.1\nProblem Formulation\nGiven a user intent in natural language, the agent’s\ngoal is to buy the most appropriate product that\nfulfills the user’s intent. We formulate the task as a\ngoal-augmented Partially Observable Markov De-\ncision Process M = (S, A, T , R, G, O, γ), where\nS is a set of states s ∈S; A ⊂VN repre-\nsents action space sampled from LLM’s vocab-\nulary V of size N; G ⊂VN denotes the goal\nspace; T : S × A 7→S is the transition func-\ntion; R : S × A × G 7→R characterizes the goal-\nconditioned reward function; O is a set of obser-\nvations o ∈O (i.e., web page visible to agent);\nγ is the discount factor. We employ the language\nmodeling head (i.e., distribution over the vocabu-\nlary) to accommodate the dynamic action space,\nwhich also facilitates directly computing the log\nprobabilities of each action ai = (w0, · · · , w|ai|)\nsampled from a dynamic action space given the\nagent’s goal g ∈G and observation o.\nIt is important to note that each observation (i.e.,\nweb page) presents a dynamic set of actions to the\nagent, which prevents us from learning a probabil-\nity distribution over the action space as in classifi-\ncation tasks. For instance, a search page allows ac-\ntions such as typing an open-ended textual query or\npressing the ‘Search’ button. Conversely, a product\ndetail page offers actions such as ‘Back to Search’,\n‘< Prev’, ‘Description’, ‘Features’, ‘Reviews’, ‘Buy\nNow’, and the product-specific variable number of\noptions. Figure 1 shows the observation and action\nspace for the ‘search result page’.\n2.2\nOverview of GLAINTEL\nWe employ Flan-T5 1 as the core architecture, with\nthe integration of the language modeling head and\nvalue head on top of the model. Our proposed\nagent, GLAINTEL, is adaptable to training across\nvarious setups: (i) unsupervised learning: no hu-\nman demonstrations are available; (ii) unsupervised\ndomain adaptation: limited human demonstrations\nin a single domain are available; and (iii) super-\nvised learning: human demonstrations are acces-\nsible. In the following, we detail the specifics of\nthe training and inference phases. The inclusion\nor exclusion of these phases is contingent upon the\navailability of the human demonstration data.\n1Checkpoints:\nhttps://github.com/google-research/t5x/\nblob/main/docs/models.md#flan-t5-checkpoints\n2.3\nOptional Phase One: Supervised Training\nThe human demonstrations can serve as mappings\nfrom states to actions. Techniques such as imitation\nlearning or behavioral cloning (BC) (Pomerleau,\n1988) can be employed to fine-tune the policy π\nby minimizing the following loss over a dataset D\ncomprising human demonstrations:\nL(π) = E(s,a)∼D[−log π(a|s)].\nThe above formulation can be adapted to in-\ncorporate the interaction history with web pages\nπ(at|st, τ<t), where τ<t refers to the interaction\ntrajectory leading up to time t. Subsequently, this\nformulation readily extends to utilize LLMs to\nlearn an optimal policy where the encoder encodes\nthe history of observations (st, τ<t) and the de-\ncoder generates the next action at as:\nLLLM(π) = Eτ∼D[\nL\nX\nt=0\n−log π(at|τ<t, st)].\nBuilding upon the recent works in return-\nconditioned supervised learning (Brandfonbrener\net al., 2022; Paster et al., 2022; Yang et al., 2022),\nwe introduce an additional conditioning variable\ng ∈G (i.e., user goal). This variable captures over-\nall trajectory-level information, to steer the model\ntoward the goal. Moreover, in implementation, we\nuse observations o (i.e., visible web page) instead\nof the actual state s. Our final formulation is ex-\npressed as:\nLLLM(π) = Eτ∼D[\nL\nX\nt=0\n−log π(at|τ<t, ot, g)].\nThe training of this phase can be skipped or cho-\nsen based on the availability and feasibility of ac-\nquiring human demonstrations. In our approach\nto address RQ1 (Effectiveness of Unsupervised\nLearning), we skip this phase. We limit the hu-\nman demonstration data to a single category for\nRQ3 (Unsupervised Domain Adaptation). To in-\nvestigate RQ2 (Impact of Human Demonstrations),\nwe utilize all the available training data for the\nsupervised training phase.\n2.4\nPhase Two: Unsupervised Training\nThe unsupervised learning phase, which forms the\ncore of the proposed agent GLAINTEL, operates\nwithout any human demonstrations. This phase is\ndesigned to autonomously learn and adapt without\nrelying on expert-guided examples. The objective\nof the agent is to learn a policy π : O × G 7→P(A)\nthat optimizes the expected discounted cumula-\ntive rewards for a given goal g.\nIn this work,\nwe leverage PPO algorithm for training, which si-\nmultaneously learns a policy ˆπ and a value func-\ntion ˆV\n: O × G 7→R approximating to the\ntrue value V (s, g) = Ea∼ˆπ(O(s),g)\n\u0002\nR(s, a, g) +\nγV (T (s, a), g)\n\u0003\n. We can calculate the probability\nof each action ai ∈A using the likelihood com-\nputed by the model, expressed as: ˆπ(ai|o, g) =\nP(ai|g). That is, the likelihood of choosing each\naction is calculated based on the probability dis-\ntributions associated with the tokens that make up\nthe action. This approach ties the action probabil-\nities directly to the distributions of the individual\ntokens involved in constructing the action. Follow-\ning (Carta et al., 2023), we incorporate a multilayer\nperception (MLP) with a single output on top of\nthe last layer of the model to approximate the value\nV . Specifically, we employ the language model-\ning head to directly compute the log probabilities\nof each action ai = {w0, · · · , w|ai|} from the dy-\nnamic action space given the agent’s goal g ∈G\nand observation ot at time t as follows:\nP(ai) =\n1\n|ai|\n|ai|\nX\nk=0\nlog PLM-head(wk|g, ot, w<k).\nSubsequently, employing the softmax operation,\nwe calculate a probability distribution over the ac-\ntion space A as follows:\nP(ai|g) =\neP(ai)\nP\nak∈A eP(ak) .\nWhile the actions comprise multiple tokens,\nthe number of possible actions can vary substan-\ntially depending on the current observation (i.e.,\nweb page), which introduces additional complex-\nity. This phase is mandatory regardless of whether\ntraining is conducted in the optional first phase.\n2.5\nPhase Three: Inference\nIn the inference phase, various decoding tech-\nniques for action selection can be employed, such\nas greedy decoding and top-p. Given the well-\nestablished nature of these techniques, we omit de-\ntails and provide key insights only. Greedy decod-\ning, chosen for action selection, has a drawback as\nit tends to trap the agent in loops, ultimately result-\ning in suboptimal overall performance. Conversely,\nopting for top-p sampling can yield a higher suc-\ncess rate, as it provides a theoretical tradeoff be-\ntween sampling and greedy decoding. However,\nthe process of determining the optimal values for\np can be time-intensive. To address these issues,\nwe turn to the Epsilon-Greedy algorithm for action\nselection during inference. In particular, at a step t,\nthe greedy will choose the action with the highest\nprobability, while the epsilon will sample based on\nthe probability distribution across the action space.\nThis method achieves a higher success rate and an\nenhanced overall performance, all while avoiding\nthe issue of getting stuck in loops. It is worth noting\nthat a judiciously chosen, small value for epsilon\nhas been employed in our work, eliminating the\nneed for an exhaustive search.\n3\nExperimental Setup\n3.1\nWebShop Environment\nWebshop (Yao et al., 2022) is a simulated web-\nbased interactive environment with 1.18 million\nreal-world products and 12,087 crowd-sourced text\ninstructions. The goal of the agent is to buy a\nproduct with specific attributes and options given\nnatural language instruction. The environment con-\ntains 5 different categories, which exhibit signif-\nicant dissimilarities, particularly in terms of pos-\nsessing nearly exclusive attributes. For instance,\nas illustrated in Table 1, a substantial 95.9% of\nFashion’s attributes are unique to its category.\nHuman Demonstrations. The Webshop also con-\ntains a human demonstration dataset. The human\ndemonstration dataset encompasses a total of 1010\ndistinct trajectories, distributed across categories.\nThis dataset is created by asking humans to demon-\nstrate how they would query a product and then\ntake different steps in the Webshop environment to\nbuy a product with desired options and attributes.\nGLAINTEL has the flexibility to incorporate hu-\nman demonstrations through optional phase one\ntraining. We utilize human demonstration data\nto quantify the impact of human demonstrations\n(RQ2) and explore UDA (RQ3).\nAdditionally,\nGLAINTEL can be trained without any human\ndemonstrations (RQ1).\n3.2\nEvaluation Methodology\nReward. We assign a reward r ∈[0, 1] to the\nagent after it completes a purchase at the conclud-\ning step of an episode. Specifically, the reward is\ndetermined by how closely the purchased product\nmatches the specific attributes and options men-\ntioned in the user instructions as follows:\nr = rtype · |Uatt∩Yatt|+|Uopt∩Yopt|+1[yprice≤uprice]\n|Uatt|+|Uopt|+1\nCategory\n# Attributes\n% Unique\nAttributes\n# Human\nDemonstrations\nBeauty\n143\n85.3%\n224\nGarden\n133\n87.2%\n211\nGrocery\n117\n92.3%\n189\nElectronics\n141\n91.4%\n169\nFashion\n173\n95.9%\n217\nTable 1: Detail about Webshop Environment.\nThe reward incorporates three main components:\nUatt, Uopt, and uprice, representing a set of attributes,\na set of options, and the price set down in the\nuser’s instruction, respectively. Correspondingly,\nYatt, Yopt, and yprice denote the set of attributes,\nthe set of options, and the actual price of the\npurchased product by the agent. Additionally, rtype\nfunctions as a text-matching heuristic, assigning\na lower reward when the purchased product and\nthe targeted product in the user instruction have\nsimilar attributes and options while being different\ntypes of products. Interested readers are referred to\nWebShop (Yao et al., 2022) for details.\nEvaluation Metrics. Two evaluation metrics are\ncomputed using the rewards obtained from the\nepisodes: (i) the Score and (ii) the Success Rate.\nThe Score metric represents the average reward\nacross all test episodes multiplied by 100, while\nthe Success rate metric measures the percentage of\ntest episodes in which the full reward (1 out of 1)\nwas attained. Given that our inference step incor-\nporates sampling, the reported Score and Success\nRate metrics are averaged by running the model\nfour times. We provide additional implementation\ndetails in Appendix A.\n3.3\nCompeting Methods\nWebShop Baselines (Yao et al., 2022): We con-\nsider the following baselines from the WebShop pa-\nper: (i) rule-based (Rulews), (ii) behavioral cloning-\nbased supervised learning (BCws), (iii) two re-\ninforcement learning models—one with a trans-\nformer text encoder (PGws) and another with an\nRNN (RNNws), and (iv) a hybrid method (BC +\nPG). Human experts (Human) also set a benchmark\nfor human-level performance.\nDRRN (He et al., 2016): DRRN is a classic RL\nbaseline that uses separate neural networks to em-\nbed states and actions into embedding vectors. An\ninteraction function (e.g., inner product) then com-\nputes the Q-function value for the state-action pair.\nAct and ReAct (Yao et al., 2023): The ReAct\nmethod is an in-context learning approach using\nLLMs that combines reasoning and action execu-\ntion to tackle diverse tasks. In the WebShop en-\nvironment, ReAct adds reasoning at each step to\nguide the agent’s decisions on exploration, purchas-\ning, and option selection.\nWebGUM (Furuta et al., 2024): WebGUM is an\ninstruction-finetuned model, that is further trained\non human demonstrations for web navigation.\nASH Prompting (Sridhar et al., 2023): ASH con-\nsists of two main components: (i) Summarizer\ncondenses observations by retaining only relevant\ninformation, and (ii) Actor uses this condensed\nobservation to generate the next action.\nPIX2ACT (Shaw et al., 2024): PIX2ACT builds\nupon the Pix2Struct model (Lindenberger et al.,\n2021), utilizing an image transformer encoder\nalong with a text transformer decoder.\nLASER (Ma et al., 2023): LASER is a GPT-4-\nbased method that converts an interactive decision-\nmaking task into state space exploration by map-\nping all possible observations to a finite set of states,\nwith the agent navigating these states through pre-\ndefined actions specific to each state\nProspector (Kim et al., 2023): The Prospector uses\ntwo approaches: the AskAct method, which incor-\nporates self-asking steps in few-shot demonstra-\ntions to extract actions from LLMs, and the Trajec-\ntory Ranking (TR) method, where LLMs generate\ndiverse trajectories, and the most rewarding one is\nselected using a reward prediction model.\n4\nResults\n4.1\nQuantitative Analysis\nRQ1: Effectiveness of Unsupervised Learning.\nIn Table 2, we systematically evaluate the perfor-\nmance of various methods that do not use human\ndemonstrations for training.\nStarting with RL-\nbased models, our PPO-trained model with 1 mil-\nlion steps (PPO1M) emerges as the top performer,\nachieving a statistically significant score of 72.13\nand a success rate of 42.55. Notably, these results\nsurpass those obtained by alternative RL-based ap-\nproaches, namely PGws, DRRN, and RNNws, un-\nderscoring the superior efficacy of the PPO method-\nology. Among In-context learning methods, the\nAskAct stands out with the most impressive re-\nsults. However, even the best-performing AskAct,\n70 billion parameters, fails to outperform a smaller\nmodel fine-tuned in an unsupervised setting with\nPPO (PPO1M). Specifically, in terms of percent-\nage improvements, the PPO-trained model with\n1 million steps (PPO1M) outperforms the AskAct\nby 5.15% on the score metric and approximately\nApproach\nName\nModel\nParameters\nScore\nSuccess Rate\nZero Shot\nRandom\n-\n-\n33.74\n6.80\nRulews\n1\n-\n-\n45.60\n9.60\nZSL-Flan-T5\nFlan-T5-large\n780 Million\n41.10\n10.30\nIn-context Learning\nAct 2\nPaLM\n540 Billion\n62.30\n30.10\nASH 4\nCODE-DAVINCI-002\nN/A\n56.70\n30.20\nReAct 2\nPaLM\n540 Billion\n66.60\n40.00\nAskAct 3\nLlama-2\n70 Billion\n68.60\n42.20\nRL-based Method\nPGws\n1\nBART, BERT\n516 Million\n52.50\n11.20\nDRRN\nGRU\n1.2 Million\n46.87\n11.73\nRNNws\n1\nGRU\n5 Million\n55.20\n17.60\nPPO500K (Ours)\nFlan-T5-large\n780 Million\n68.19\n38.55\nPPO1M (Ours)\nFlan-T5-large\n780 Million\n72.13\n42.55\nHuman\nHuman 1\n-\n-\n82.10\n59.60\nResults are taken from published research: 1 from (Yao et al., 2022), 2 from (Yao et al., 2023), 3 from (Kim et al., 2023), and 4 from (Sridhar et al., 2023).\nTable 2: Results from methods in the WebShop environment that do not rely on human demonstration data.\nApproach\nName\nModel\nParameters\nScore\nSuccess Rate\nBehavioral Cloning\nPIX2ACT 3\nPix2Struct\n282 Million\n46.70\nNR\nBCws 1\nBART, BERT\n516 Million\n59.90\n29.10\nBCour\nFlan-T5-large\n780 Million\n66.56\n37.05\nWebGUM 2\nFlan-T5-XL\n3 Billion\n67.50\n45.00\nHybrid Methods\nBC + PG 1\nBART, BERT\n516 Million\n62.40\n28.70\nAskAct + TR (Prospector) 4\nLlama-2, FLAN-T5-XL\n70 + 3 Billion\n70.20\n43.60\nBC + PPO500K (GLAINTEL500K)\nFlan-T5-large\n780 Million\n74.60\n46.95\nBC + PPO1M (GLAINTEL1M)\nFlan-T5-large\n780 Million\n76.87\n49.60\nResults are taken from published research: 1 from (Yao et al., 2022), 2 from (Furuta et al., 2024), 3 from (Shaw et al., 2024), and 4 from (Kim et al., 2023).\nTable 3: Results from methods in the WebShop environment that use human demonstration data.\n0.83% on the success rate metric. This pattern\npersists when comparing ReAct (540 billion pa-\nrameters) with PPO1M model. This observation\nsuggests that fine-tuning of small models using RL\ncan yield superior performance compared to in-\ncontext learning methods. In addition to RL-based\nand in-context learning methods, Table 2 includes\nzero-shot learning methods, including zero-shot\nFlan-T5 (ZSL-Flan-T5) to quantify the role of un-\nsupervised training.\nRQ2: Impact of Human Demonstrations. Ta-\nble 3 presents the results of various methods in-\ncorporating human demonstration. In the behav-\nioral cloning approach, WebGum emerges as the\ntop performer, leveraging the Flan-T5-XL model\nwith 3 billion parameters. It achieves a score of\n67.5 and a success rate of 45.0. We also present\nthe results of our fine-tuned Flan-T5-large model\n(BCour) with 780 million parameters. Both mod-\nels outperform the PIX2ACT and BCws models,\nwhich utilize BART and BERT architectures. This\nnotable superiority underscores the effectiveness of\ninstruction-finetuned language models. Turning to\nhybrid methods, GLAINTEL500K, GLAINTEL1M,\nand BC + PG models initially undergo refinement\nthrough human demonstrations in a supervised set-\nting, followed by additional fine-tuning in an unsu-\npervised setting using RL. In contrast, Prospector\nemploys the AskAct method (in-context learning)\nand a reward prediction model, choosing the most\nrewarding trajectory through supervised learning.\nAmong these approaches, GLAINTEL1M achieves\nremarkable performance. It attains an exceptional\nScore of 76.87 and a Success Rate of 49.6. No-\ntably, our approach surpasses all other hybrid and\nbehavioral cloning methods in both metrics.\nEffective Utilization of Human Demonstrations:\nIn comparing two variants of the Flan-T5-large\nmodel, as presented in Table 3 and Table 2, we\nfocused on one fine-tuned in a supervised setting\nwith human demonstrations (referred to as BCour\nin Table 3) and another fine-tuned exclusively with\nPPO for 1 million steps in an unsupervised setting\n(referred to as PPO1M in Table 2). Surprisingly,\nthe unsupervised model (PPO1M) demonstrated an\n8.36% higher score and a 14.84% higher success\nrate compared to the supervised model, which is\nstatistically significant.\nThis outcome suggests\nthat relying only on human demonstrations does\nnot always lead to superior results. Moreover,\nwhen the supervised model is subjected to further\ntraining with PPO, it produces the best results.\nApproach\nName\nModel\nParameters\nScore\nSuccess Rate\nRL-based Method\nPPO1M\nFlan-T5-large\n780 Million\n72.12\n42.55\nHybrid Method\nBC + PPO1M (GLAINTEL1M)\nFlan-T5-large\n780 Million\n76.87\n49.6\nUnsupervised Domain Adaptation\nUDA1M\nFlan-T5-large\n780 Million\n74.69\n46.42\nState-Space Exploration\nLASER(Ma et al., 2023)\nGPT-4-0613\nN/A\n75.6\n50.0\nTable 4: Comparison of the Best Models.\nApproach −→\nSingle Domain Behavioral Cloning\nUnsupervised Domain Adaptation\nPPO Adaptation Configs −→\nNo PPO (SDBC)\nPPO for 500k steps (UDA500K)\nPPO for 1M steps (UDA1M)\nSingle-domain Supervision ↓\nScore\nSuccess Rate\nScore\nSuccess Rate\nScore\nSuccess Rate\nFine-tuned on Beauty\n64.23\n31.41\n73.99\n45.80\n74.49\n45.85\nFine-tuned on Garden\n64.79\n34.76\n73.97\n44.70\n75.27\n47.5\nFine-tuned on Grocery\n61.80\n27.50\n73.83\n45.75\n74.91\n47.60\nFine-tuned on Electronics\n62.03\n30.97\n73.46\n45.25\n74.41\n44.5\nFine-tuned on Fashion\n62.54\n31.60\n73.37\n44.45\n74.36\n46.65\nAverage −→\n63.07\n31.24\n73.72\n45.19\n74.68\n46.42\nTable 5: The results of unsupervised domain adaptation and single domain methods in the WebShop environment.\nComparison between the Best Models: We present\nthe results from the best models in Table 4. No-\ntably, GLAINTEL1M achieves a state-of-the-art\nscore (i.e., 76.87) surpassing all other models. Sur-\nprisingly, our model, based on Flan-T5-Large (780\nmillion parameters), has outperformed the LASER\nmethod, which relies on the latest GPT-4 model\nwith extensive handcrafted prompt, in terms of the\nScore metric. It also achieves comparable perfor-\nmance in terms of Success Rate (49.6 vs 50.0).\nThese findings strongly suggest that a model, when\nfurther fine-tuned with PPO after supervised train-\ning, can deliver superior results, even with a rela-\ntively smaller model size.\nRQ3: Unsupervised Domain Adaptation. The\nSingle Domain Behavioral Cloning (SDBC) ap-\nproach involves fine-tuning a Flan-T5-large model\nin a supervised setting using demonstrations spe-\ncific to a particular domain (e.g., Beauty). Sub-\nsequently, without any additional refinement for\nother domains, the model is directly tested using\nthe WebShop environment encompassing all do-\nmains. In contrast, UDA takes the Flan-T5-large\nmodel fine-tuned in a single domain and further\nrefines it across all domains using PPO in the un-\nsupervised setting. Table 5 presents two versions\nof UDA: UDA500K and UDA1M. Both UDA meth-\nods exhibit superior performance (i.e., statistically\nsignificant) in terms of Score and Success Rate met-\nrics when compared to the corresponding metrics\nof SDBC. This superiority is evident not only on a\ndomain-specific basis but also on the average per-\nformance across domains. In particular, concerning\nthe average performance across domains, UDA1M\nsurpasses SDBC by 18.4% in the Score and 48.6%\nin the Success Rate metrics. This emphasizes the\ncrucial role of unsupervised PPO refinement and\nits impact on enhancing overall performance.\nRole of Supervision in a Single Domain: To com-\npare the UDA results with RL-based ones, we can\nrefer to Table 5 and Table 2, where UDA500K\nmodel outperforms the PPO500K in terms of\nboth Score and Success Rate metrics.\nSimi-\nlarly, UDA1M surpassed PPO1M.\nSpecifically,\nthe UDA1M model achieves a 3.5% higher Score\nand a 9.09% higher Success Rate compared to the\nPPO1M model. Likewise, the UDA500K model at-\ntained an 8.1% higher Score and a 17.2% higher\nSuccess Rate compared to the PPO500K model.\nThese findings indicate that incorporating single-\ndomain human demonstration supervision signif-\nicantly enhances the model’s capacity for more\neffective fine-tuning during unsupervised training\nwith PPO. This approach outperforms models that\nlack any supervised training, which highlights the\nvalue of leveraging human demonstrations in the\nadaptation process.\nLearning Curves for PPO training.\nIn Fig-\nure 2, the learning curves of Score and Success\nRate metrics during PPO fine-tuning are illustrated\nfor various methodologies: the UDA, the hybrid\n(GLAINTEL) (BC + PPO), and the RL-based PPO.\nBoth the hybrid method and the unsupervised do-\nmain adaptation method demonstrate higher sample\nefficiency compared to the unsupervised method.\nThis aligns with expectations, considering that both\nthe hybrid method and the unsupervised domain\nadaptation method underwent some level of super-\nvised training before RL fine-tuning – a contrast to\nthe RL-based unsupervised method, which did not.\nApproach\nName\nModel\nParameters\nScore\nSuccess Rate\nHybrid Method\nBC + PG\nBART, BERT\n516 Million\n59.25\n24\nHybrid Method\nBC + PPO1M (GLAINTEL1M)\nFlan-T5-large\n780 Million\n78.35\n53\nState-Space Exploration\nLASER\nGPT-4-0613\nN/A\n83.55\n56\nTable 6: Results of Zero-shot simulation-to-real experiment on eBay.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSteps\n1e6\n10\n20\n30\n40\n50\n60\n70\nUnsupervised D.A.: Score\nHybrid (BC + PPO) (GLAINTEL): Score\nUnsupervised (PPO): Score\nUnsupervised D.A.: Success Rate\nHybrid (BC + PPO) (GLAINTEL): Success Rate\nUnsupervised (PPO): Success Rate\nFigure 2: Learning curves of different methodolo-\ngies: Unsupervised Domain Adaptation (UDA), Hybrid\n(BC + PPO) (GLAINTEL), and RL-based Unsupervised\n(PPO).\n4.2\nResults on Real Website: eBay\nWe also conduct limited evaluations on a real\nwebsite: eBay. For this experiment, we evalu-\nate the performance of three methods: (i) our\nbest model (GLAINTEL1M), (ii) the GPT-4-based\nmethod LASER, and (iii) the WebShop baseline\n(BC + PG). It is important to highlight that we\nused the models trained using the Webshop en-\nvironment and did not perform any fine-tuning\nusing the eBay website. Following (Yao et al.,\n2022), we randomly sampled 100 user instructions\nto evaluate the performance of these methods. As\npresented in Table 6, our method GLAINTEL1M\nsignificantly outperformed the WebShop baseline\n(BC + PG) by 32.23% in the Score metric and by\n120.83% in the Success Rate metric. Moreover,\nalthough LASER, utilizing GPT-4, has slightly\nhigher Score and Success Rate metrics compared\nto our model GLAINTEL1M, we are confident that\nGLAINTEL1M can achieve comparable or even su-\nperior results by enabling of unsupervised training\nusing PPO. Additionally, it is worth noting that our\napproach utilizes a 780 million parameter model,\nwhich is significantly smaller than GPT-4, not to\nmention the costs associated with GPT-4.\nWe\npresent an ablation study in Appendix B.\n5\nRelated Work\nFine-tuning LLMs with RL and Human Feed-\nback. Fine-tuning LLMs with human feedback\nand reinforcement learning has been studied ex-\ntensively.\n(Nakano et al., 2021) developed the\nWebGPT by fine-tuning the GPT-3 model using\nbehavior cloning and rejection sampling. More-\nover, InstructGPT (Ouyang et al., 2022) was de-\nveloped using the three-step approach: supervised\nfine-tuning, reward model training, and reinforce-\nment learning via PPO with the help of the trained\nreward model. Additionally, the authors in (Sti-\nennon et al., 2020) fine-tuned a model that may\nchoose a human-preferred summary, they used this\nmodel as a reward function to fine-tune a summa-\nrization policy using RL.\nFoundation Models for Decision Making. Foun-\ndation models possess robust decision-making ca-\npabilities, rendering them invaluable across various\ndownstream tasks. For instance, recent works (Ahn\net al., 2022; Huang et al., 2022a,b) showcase the\napplication of foundation models in the robotics do-\nmain. Moreover, works (Rawles et al., 2023; Wen\net al., 2023; Yan et al., 2023; Hong et al., 2023)\nutilize foundation models to intelligently navigate\nAndroid applications. Additionally, the foundation\nmodels have been utilized in gaming contexts (,\nFAIR; Lee et al., 2022; Reed et al., 2022; Fan et al.,\n2022; Wang et al., 2024; Carta et al., 2023).\nWeb Navigation. Many benchmarks and datasets\nexist for the training and assessment of web agents\n(Yao et al., 2022; Shi et al., 2017; Deng et al.,\n2024; Zhou et al., 2023; Liu et al., 2018). Re-\nsearchers have consequently proposed diverse web\nagents and tested their performance on these bench-\nmarks. The MiniWob++ benchmark is among these\nbenchmarks on which different methods have been\napplied. For example, (Humphreys et al., 2022)\nemployed a combination of reinforcement learn-\ning and behavioral cloning, (Furuta et al., 2024)\nutilized supervised training on an instruction-fine-\ntuned LLM, (Liu et al., 2018) introduced Workflow-\nguided exploration (WGE), and (Gur et al., 2019)\ntrained DQN agents (QWeb network and INET\nnetwork).\nAdditionally, the Mind2Web bench-\nmark introduced the MindAct model, synergiz-\ning the strength of small and large LLMs (Deng\net al., 2024).\nAdditionally, a visual language\nmodel named CogAgent was utilized for the bench-\nmark (Hong et al., 2023). (Zeng et al., 2023) pre-\nsented AgentTuning as another notable approach\nto tackle the Mind2Web benchmark.\nFurther-\nmore, considering the Webshop benchmark, var-\nious methodologies have been proposed that use\nin-context learning (Kim et al., 2023; Yao et al.,\n2023; Sridhar et al., 2023), supervised learning (Fu-\nruta et al., 2024; Shaw et al., 2024), and RL (Yao\net al., 2022). Nonetheless, no work has clearly out-\nlined the impact of human demonstrations and the\noptimal utilization of available demonstration data.\nFurthermore, UDA remains underexplored.\n6\nConclusion\nWe introduce GLAINTEL, a flexible agent designed\nfor training across diverse product search scenar-\nios, accommodating situations with limited or no\nhuman demonstrations for supervision. We also\ninvestigate the optimal utilization of demonstra-\ntion data, showing that straightforward supervised\nlearning approaches, like behavior cloning, do not\nyield superior results when using human demon-\nstration data. Through extensive experimental eval-\nuations in the WebShop environment, we highlight\nthe crucial role of the unsupervised training phase\nemploying the PPO algorithm. When combined\nwith supervised learning, this approach achieved\nresults comparable to methods utilizing GPT-4. Ad-\nditionally, we explore an underexplored scenario\nwhere demonstration data is confined to a single\ndomain, we employ UDA techniques to accommo-\ndate novel domains. We also present evaluations on\na real website, eBay, to showcase the applicability\nof GLAINTEL in the real world.\nAcknowledgments\nThis work is supported in part by the National Sci-\nence Foundation (NSF) under grant IIS-2401685.\n7\nLimitations\nIn our experiments, we only used the current and\nprevious observations as input to the model. Al-\nthough including additional observations (e.g., the\nlast four observations) can potentially improve per-\nformance, it is important to consider that the in-\ncrease in the number of observations also expands\nthe size of the context, leading to requirements\nfor higher GPU memory. Moreover, the current\narchitecture relies only on textual descriptions of\nthe environment, without embedding screenshots\nof web pages or product images. Improving the\nperformance of the agent can be achieved by inte-\ngrating these visual elements into the model.\nIt should be noted that other web environments,\nsuch as MiniWoB (Shi et al., 2017), have simple,\nplain backgrounds and minimal interaction within\na small area of 160 x 160 pixels. Because of these\nlimitations, we did not assess our method in this\nenvironment and considered a more realistic envi-\nronment, WebShop. However, we plan to evaluate\nthe performance of our approach in other web envi-\nronments in the future.\nReferences\nMichael Ahn, Anthony Brohan, Noah Brown, Yevgen\nChebotar, Omar Cortes, Byron David, Chelsea Finn,\nKeerthana Gopalakrishnan, Karol Hausman, Alexan-\nder Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz,\nBrian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui\nRuano, Kyle Jeffrey, Sally Jesmonth, Nikhil Jayant\nJoshi, Ryan C. Julian, Dmitry Kalashnikov, Yuheng\nKuang, Kuang-Huei Lee, Sergey Levine, Yao Lu,\nLinda Luu, Carolina Parada, Peter Pastor, Jor-\nnell Quiambao, Kanishka Rao, Jarek Rettinghouse,\nDiego M Reyes, Pierre Sermanet, Nicolas Siev-\ners, Clayton Tan, Alexander Toshev, Vincent Van-\nhoucke, F. Xia, Ted Xiao, Peng Xu, Sichun Xu, and\nMengyuan Yan. 2022. Do as i can, not as i say:\nGrounding language in robotic affordances. In Con-\nference on Robot Learning.\nRishi Bommasani, Drew A Hudson, Ehsan Adeli,\nRuss Altman, Simran Arora, Sydney von Arx,\nMichael S Bernstein, Jeannette Bohg, Antoine Bosse-\nlut, Emma Brunskill, et al. 2021. On the opportuni-\nties and risks of foundation models. arXiv preprint\narXiv:2108.07258.\nDavid Brandfonbrener, Alberto Bietti, Jacob Buckman,\nRomain Laroche, and Joan Bruna. 2022. When does\nreturn-conditioned supervised learning work for of-\nfline reinforcement learning? In Advances in Neural\nInformation Processing Systems.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nThomas Carta, Clément Romac, Thomas Wolf, Sylvain\nLamprier, Olivier Sigaud, and Pierre-Yves Oudeyer.\n2023. Grounding large language models in interac-\ntive environments with online reinforcement learning.\nIn International Conference on Machine Learning,\npages 3676–3713. PMLR.\nXiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam\nStevens, Boshi Wang, Huan Sun, and Yu Su. 2024.\nMind2web: Towards a generalist agent for the web.\nAdvances in Neural Information Processing Systems,\n36.\nPrafulla Dhariwal, Christopher Hesse, Oleg Klimov,\nAlex Nichol, Matthias Plappert, Alec Radford, John\nSchulman, Szymon Sidor, Yuhuai Wu, and Pe-\nter Zhokhov. 2017. Openai baselines. https://\ngithub.com/openai/baselines.\nMeta Fundamental AI Research Diplomacy Team\n(FAIR)†, Anton Bakhtin, Noam Brown, Emily Dinan,\nGabriele Farina, Colin Flaherty, Daniel Fried, An-\ndrew Goff, Jonathan Gray, Hengyuan Hu, Athul Paul\nJacob, Mojtaba Komeili, Karthik Konath, Minae\nKwon, Adam Lerer, Mike Lewis, Alexander H.\nMiller, Sasha Mitts, Adithya Renduchintala, Stephen\nRoller, Dirk Rowe, Weiyan Shi, Joe Spisak, Alexan-\nder Wei, David Wu, Hugh Zhang, and Markus Zi-\njlstra. 2022.\nHuman-level play in the game of\n<i>diplomacy</i> by combining language models\nwith strategic reasoning. Science, 378(6624):1067–\n1074.\nLinxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Man-\ndlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang,\nDe-An Huang, Yuke Zhu, and Anima Anandkumar.\n2022. Minedojo: Building open-ended embodied\nagents with internet-scale knowledge. In Thirty-sixth\nConference on Neural Information Processing Sys-\ntems Datasets and Benchmarks Track.\nHiroki Furuta, Kuang-Huei Lee, Ofir Nachum, Yutaka\nMatsuo, Aleksandra Faust, Shixiang Shane Gu, and\nIzzeddin Gur. 2024.\nMultimodal web navigation\nwith instruction-finetuned foundation models. In The\nTwelfth International Conference on Learning Repre-\nsentations.\nIzzeddin Gur, Ulrich Rueckert, Aleksandra Faust, and\nDilek Hakkani-Tur. 2019. Learning to navigate the\nweb. In International Conference on Learning Rep-\nresentations.\nJi He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Li-\nhong Li, Li Deng, and Mari Ostendorf. 2016. Deep\nreinforcement learning with a natural language ac-\ntion space. In Proceedings of the 54th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 1621–1630, Berlin,\nGermany. Association for Computational Linguistics.\nWenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng\nXu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang,\nYuxiao Dong, Ming Ding, and Jie Tang. 2023. Co-\ngagent: A visual language model for gui agents.\nPreprint, arXiv:2312.08914.\nWenlong Huang, Pieter Abbeel, Deepak Pathak, and\nIgor Mordatch. 2022a. Language models as zero-\nshot planners: Extracting actionable knowledge for\nembodied agents. In Proceedings of the 39th Inter-\nnational Conference on Machine Learning, volume\n162 of Proceedings of Machine Learning Research,\npages 9118–9147. PMLR.\nWenlong Huang, F. Xia, Ted Xiao, Harris Chan, Jacky\nLiang, Peter R. Florence, Andy Zeng, Jonathan\nTompson, Igor Mordatch, Yevgen Chebotar, Pierre\nSermanet, Noah Brown, Tomas Jackson, Linda Luu,\nSergey Levine, Karol Hausman, and Brian Ichter.\n2022b.\nInner monologue: Embodied reasoning\nthrough planning with language models. In Con-\nference on Robot Learning.\nPeter C. Humphreys, David Raposo, Tobias Pohlen, Gre-\ngory Thornton, Rachita Chhaparia, Alistair Muldal,\nJosh Abramson, Petko Georgiev, Alex Goldin, Adam\nSantoro, and Timothy P. Lillicrap. 2022. A data-\ndriven approach for learning to control computers. In\nInternational Conference on Machine Learning.\nByoungjip Kim,\nYoungsoo Jang,\nLajanugen Lo-\ngeswaran, Geon-Hyeong Kim, Yu Jin Kim, Honglak\nLee, and Moontae Lee. 2023. Prospector: Improving\nLLM agents with self-asking and trajectory ranking.\nIn NeurIPS 2023 Foundation Models for Decision\nMaking Workshop.\nKuang-Huei Lee, Ofir Nachum, Mengjiao (Sherry)\nYang, Lisa Lee, Daniel Freeman, Sergio Guadar-\nrama, Ian Fischer, Winnie Xu, Eric Jang, Henryk\nMichalewski, and Igor Mordatch. 2022. Multi-game\ndecision transformers. In Advances in Neural Infor-\nmation Processing Systems, volume 35, pages 27921–\n27936. Curran Associates, Inc.\nJacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol\nHausman, Brian Ichter, Pete Florence, and Andy\nZeng. 2023.\nCode as policies: Language model\nprograms for embodied control. In 2023 IEEE In-\nternational Conference on Robotics and Automation\n(ICRA), pages 9493–9500. IEEE.\nPhilipp Lindenberger, Paul-Edouard Sarlin, Viktor\nLarsson, and Marc Pollefeys. 2021. Pixel-perfect\nstructure-from-motion with featuremetric refinement.\nIn Proceedings of the IEEE/CVF International Con-\nference on Computer Vision (ICCV), pages 5987–\n5997.\nEvan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tian-\nlin Shi, and Percy Liang. 2018. Reinforcement learn-\ning on web interfaces using workflow-guided explo-\nration. In International Conference on Learning Rep-\nresentations (ICLR).\nKaixin Ma, Hongming Zhang, Hongwei Wang, Xiao-\nman Pan, and Dong Yu. 2023. LASER: LLM agent\nwith state-space exploration for web navigation. In\nNeurIPS 2023 Foundation Models for Decision Mak-\ning Workshop.\nKyle Mahowald, Anna A Ivanova, Idan A Blank, Nancy\nKanwisher, Joshua B Tenenbaum, and Evelina Fe-\ndorenko. 2023. Dissociating language and thought\nin large language models: a cognitive perspective.\narXiv preprint arXiv:2301.06627.\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,\nLong Ouyang, Christina Kim, Christopher Hesse,\nShantanu Jain, Vineet Kosaraju, William Saunders,\net al. 2021.\nWebgpt: Browser-assisted question-\nanswering with human feedback.\narXiv preprint\narXiv:2112.09332.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback.\nAdvances in Neural\nInformation Processing Systems, 35:27730–27744.\nKeiran Paster, Sheila McIlraith, and Jimmy Ba. 2022.\nYou can’t count on luck: Why decision transformers\nand rvs fail in stochastic environments. Advances\nin neural information processing systems, 35:38966–\n38979.\nDean A. Pomerleau. 1988. Alvinn: An autonomous\nland vehicle in a neural network. In Advances in\nNeural Information Processing Systems, volume 1.\nMorgan-Kaufmann.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the lim-\nits of transfer learning with a unified text-to-text\ntransformer. Journal of machine learning research,\n21(140):1–67.\nChristopher Rawles, Alice Li, Daniel Rodriguez, Ori-\nana Riva, and Timothy P Lillicrap. 2023.\nAn-\ndroidinthewild: A large-scale dataset for android de-\nvice control. In Thirty-seventh Conference on Neural\nInformation Processing Systems Datasets and Bench-\nmarks Track.\nScott Reed, Konrad Zolna, Emilio Parisotto, Ser-\ngio Gomez Colmenarejo,\nAlexander Novikov,\nGabriel Barth-Maron, Mai Gimenez, Yury Sulsky,\nJackie Kay, Jost Tobias Springenberg, Tom Eccles,\nJake Bruce, Ali Razavi, Ashley D. Edwards, Nico-\nlas Manfred Otto Heess, Yutian Chen, Raia Hadsell,\nOriol Vinyals, Mahyar Bordbar, and Nando de Freitas.\n2022. A generalist agent. Transactions on Machine\nLearning Research, 2022.\nPeter Shaw, Mandar Joshi, James Cohan, Jonathan Be-\nrant, Panupong Pasupat, Hexiang Hu, Urvashi Khan-\ndelwal, Kenton Lee, and Kristina N Toutanova. 2024.\nFrom pixels to ui actions: Learning to follow in-\nstructions via graphical user interfaces. Advances in\nNeural Information Processing Systems, 36.\nTianlin Shi, Andrej Karpathy, Linxi Fan, Jonathan Her-\nnandez, and Percy Liang. 2017. World of bits: An\nopen-domain platform for web-based agents. In In-\nternational Conference on Machine Learning, pages\n3135–3144. PMLR.\nAbishek Sridhar, Robert Lo, Frank F. Xu, Hao Zhu, and\nShuyan Zhou. 2023. Hierarchical prompting assists\nlarge language model on web navigation. In Con-\nference on Empirical Methods in Natural Language\nProcessing.\nNisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel\nZiegler, Ryan Lowe, Chelsea Voss, Alec Radford,\nDario Amodei, and Paul F Christiano. 2020. Learn-\ning to summarize with human feedback. Advances\nin Neural Information Processing Systems, 33:3008–\n3021.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing\nsystems, 30.\nGuanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Man-\ndlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and\nAnima Anandkumar. 2024. Voyager: An open-ended\nembodied agent with large language models. Trans-\nactions on Machine Learning Research.\nHao Wen, Yuanchun Li, Guohong Liu, Shanhui Zhao,\nTao Yu, Toby Jia-Jun Li, Shiqi Jiang, Yunhao Liu,\nYaqin Zhang, and Yunxin Liu. 2023. Empowering\nllm to use smartphone for intelligent task automation.\nArXiv, abs/2308.15272.\nAn Yan, Zhengyuan Yang, Wanrong Zhu, Kevin Lin,\nLinjie Li, Jianfeng Wang, Jianwei Yang, Yiwu Zhong,\nJulian McAuley, Jianfeng Gao, Zicheng Liu, and\nLijuan Wang. 2023. Gpt-4v in wonderland: Large\nmultimodal models for zero-shot smartphone gui nav-\nigation. ArXiv, abs/2311.07562.\nMengjiao Yang, Dale Schuurmans, Pieter Abbeel, and\nOfir Nachum. 2022. Dichotomy of control: Sepa-\nrating what you can control from what you cannot.\nInternational Conference on Learning Representa-\ntions.\nShunyu Yao, Howard Chen, John Yang, and Karthik\nNarasimhan. 2022. Webshop: Towards scalable real-\nworld web interaction with grounded language agents.\nIn Advances in Neural Information Processing Sys-\ntems, volume 35, pages 20744–20757. Curran Asso-\nciates, Inc.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik R Narasimhan, and Yuan Cao. 2023.\nReact: Synergizing reasoning and acting in language\nmodels. In The Eleventh International Conference\non Learning Representations.\nAohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao\nLiu, Yuxiao Dong, and Jie Tang. 2023. Agenttun-\ning: Enabling generalized agent abilities for llms.\nPreprint, arXiv:2310.12823.\nShuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou,\nRobert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan\nBisk, Daniel Fried, Uri Alon, et al. 2023. Webarena:\nA realistic web environment for building autonomous\nagents. arXiv preprint arXiv:2307.13854.\nHyperparameter\nValue\nNumber of Epochs\n10\nLearning Rate\n2 × 10−5\nWarmup Steps\n100\nWeight Decay\n0.01\nBatch Size\n32\nAdam Optimizer Epsilon\n10−8\nAdam Optimizer β1\n0.9\nAdam Optimizer β2\n0.999\nTable 7: Supervised Learning Hyperparameters.\nHyperparameter\nValue\n# of collected transitions\nbetween two updates\n640 (16 × 40)\nNumber of epochs per update\n1\nBatch Size\n8\nLearning Rate\n10−6\nAdam Optimizer Epsilon\n10−5\nAdam Optimizer β1\n0.9\nAdam Optimizer β2\n0.999\nDiscount Factor\n0.99\nLambda for Generalized\nAdvantage Estimate\n0.99\nEntropy Loss Coefficient\n0.01\nValue Loss Coefficient\n0.5\nMaximum Gradient Norm\n0.5\nClipping Epsilon\n0.2\nTable 8: Unsupervised Learning Hyperparameters.\nA\nImplementation Details\nOur implementation operates on a client-server\narchitecture, with the training scripts serving as\nthe client and communicating requests to LLM\nservers. Specifically, a master server manages these\nrequests, distributing them across multiple LLM\nservers. Once each LLM server completes its com-\nputations, the master server consolidates the results\nand sends them back to the training script. Further-\nmore, we use vertical model parallelism, enabling\nthe parallelization of individual LLMs across multi-\nple GPUs. In our experiments, we utilized a single\nLLM, Flan-T5-Large, with 780 million parameters.\nThis model was parallelized across 4 Nvidia V100\n32GB GPUs. We incorporated the last two obser-\nvations as the model input and an encoder context\nsize of 1024.\nTo train the agent using the human demonstra-\ntions, we used the Trainer library provided by Hug-\nConfigs −→\nSL (one cat) + PPO (500k)\nPPO (500k)\nModel ↓\nScore\nSuccess Rate\nScore\nSuccess Rate\nFlan-T5\n73.72\n45.19\n68.18\n38.55\nT5\n71.85\n43.10\n52.07\n25.35\nTable 9: Ablation Study (T5 vs Flan-T5)\n100000\n200000\n300000\n400000\n500000\nSteps\n20\n30\n40\n50\n60\n70\nT5: Score\nFlan T5: Score\nT5: Success Rate\nFlan T5: Success Rate\nFigure 3: Hybrid setting: BC + PPO: Flan-T5 is more\nsample efficient than T5 model.\ngingface 2. We employed the Adam optimizer, and\nfor the remaining hyperparameter values, refer to\nTable 7. In our unsupervised learning phase, we\nleverage the PPO algorithm, and the complete val-\nues of hyperparameters can be found in Table 8.\nB\nAblation Study\nFlan-T5 vs T5. We employed two models of iden-\ntical size, each with 780 million parameters: Flan-\nT5-Large and T5-Large. The results, as presented\nin Table 9, demonstrate that adopting the Flan-T5-\nLarge model instead of T5-Large leads to a sub-\nstantial improvement of 30.93% in the Score and a\nremarkable 52.07% increase in the Success Rate in\nthe unsupervised setting (PPO). Furthermore, in the\ndomain adaptation scenario, we observed a 2.60%\nScore enhancement and a 4.85% improvement in\nthe Success Rate. Moreover, Figure 3 demonstrates\nthat employing the Flan-T5 model over the T5\nmodel results in better sample efficiency. Specifi-\ncally, both Score and Success Rate metrics exhibit\nfaster growth during PPO fine-tuning in the Flan-\nT5 model compared to the T5 model. This outcome\nwas anticipated as the Flan-T5 model enjoys the\nadvantage of being fine-tuned on user instructions,\na benefit not shared by the T5 model.\n2Trainer:\nhttps://huggingface.co/docs/transformers\n/main_classes/trainer\n100000\n200000\n300000\n400000\n500000\nSteps\n20\n30\n40\n50\n60\n70\nOne Observation: Score\nTwo Observations: Score\nOne Observation: Success Rate\nTwo Observations: Success Rate\nFigure 4: The model is more sample efficient when we\nfeed it with the last two observations.\nConfigs −→\nSL (all cats)\nSL + PPO (500k)\nScore\nSuccess Rate\nScore\nSuccess Rate\n2 observations\n66.55\n37.05\n74.60\n46.95\n1 observation\n60.20\n27.20\n65.29\n33.60\nTable 10: Ablation Study (2 observations vs 1 observa-\ntion)\n2 Observations vs 1 Observation. As demon-\nstrated in Table 10, combining the present obser-\nvation state with the preceding observation state to\ncreate a historical context and subsequently provid-\ning the model with this new observation containing\nboth leads to a notable 10.54% boost in the Score\nand a remarkable 36.21% improvement in Success\nRate in the supervised setting. This substantial\nenhancement is equally observable in the context\nof the hybrid method (SL + PPO) where the super-\nvised training is coupled with unsupervised training\n(PPO), resulting in a significant 14.26% increase in\nthe Score and an impressive 39.73% improvement\nin Success Rate. Additionally, during the train-\ning, we noticed that employing a historical context\n(having the current and last observations) as input\nenhances the sample efficiency for the agent com-\npared to using just one observation (see Figure 4).\nSpecifically, Score and Success Rate metrics show\na swifter increase with fewer steps when leverag-\ning two observations (historical context) as input,\nwhile the progression is notably slower when uti-\nlizing only a single (or current) observation.\nComparison of Decoding Methods.\nIn Table\n11, we compare the performance of four differ-\nent decoding methods: (i) Epsioln-Greedy algo-\nrithm (with epsilon value of 0.2), (ii) Sampling with\ntop_p (with top_p = 0.8 and top_k = 0.0),(iii) Sam-\npling with no top_p and no top_k, and (iv) Argmax.\nThese results are determined by averaging the re-\nComparison\nScore\nSuccess Rate\nEpsilon-Greedy algorithm\n68.23\n39.29\nSampling with top_p\n66.25\n37.32\nSampling\n65.92\n36.41\nArgmax\n57.92\n35.59\nTable 11: Ablation Study (Decoding Methods)\nsults achieved from models trained with different\ntechniques and settings, including RL and UDA,\namong others. These results show that, on average,\nthe Epsilon-Greedy algorithm consistently attains\nthe best results during inference, with a Score of\n68.23 and a Success Rate of 39.29. Following\nclosely, the nucleus sampling (top_p) method has\nlower Scores and Success Rates of 66.25 and 37.32,\nrespectively. In the third position, traditional sam-\npling produces a score of 65.92 and a Success Rate\nof 36.41. The worst outcomes are associated with\nthe Argmax method, primarily since Argmax fre-\nquently causes the web agent to become stuck in\na loop. In simpler terms, the web agent ends up\nrepeatedly navigating back and forth between web\npages.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2024-04-16",
  "updated": "2025-01-26"
}