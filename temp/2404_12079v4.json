{
  "id": "http://arxiv.org/abs/2404.12079v4",
  "title": "Trajectory Planning for Autonomous Vehicle Using Iterative Reward Prediction in Reinforcement Learning",
  "authors": [
    "Hyunwoo Park"
  ],
  "abstract": "Traditional trajectory planning methods for autonomous vehicles have several\nlimitations. For example, heuristic and explicit simple rules limit\ngeneralizability and hinder complex motions. These limitations can be addressed\nusing reinforcement learning-based trajectory planning. However, reinforcement\nlearning suffers from unstable learning, and existing reinforcement\nlearning-based trajectory planning methods do not consider the uncertainties.\nThus, this paper, proposes a reinforcement learning-based trajectory planning\nmethod for autonomous vehicles. The proposed method involves an iterative\nreward prediction approach that iteratively predicts expectations of future\nstates. These predicted states are then used to forecast rewards and integrated\ninto the learning process to enhance stability. Additionally, a method is\nproposed that utilizes uncertainty propagation to make the reinforcement\nlearning agent aware of uncertainties. The proposed method was evaluated using\nthe CARLA simulator. Compared to the baseline methods, the proposed method\nreduced the collision rate by 60.17 %, and increased the average reward by\n30.82 times. A video of the proposed method is available at\nhttps://www.youtube.com/watch?v=PfDbaeLfcN4.",
  "text": "Trajectory Planning for Autonomous Vehicle Using\nIterative Reward Prediction in Reinforcement Learning\nHyunwoo Park1\nAbstract— Traditional trajectory planning methods for au-\ntonomous vehicles have several limitations. For example, heuris-\ntic and explicit simple rules limit generalizability and hinder\ncomplex motions. These limitations can be addressed using\nreinforcement learning-based trajectory planning. However,\nreinforcement learning suffers from unstable learning, and\nexisting reinforcement learning-based trajectory planning meth-\nods do not consider the uncertainties. Thus, this paper, proposes\na reinforcement learning-based trajectory planning method\nfor autonomous vehicles. The proposed method involves an\niterative reward prediction approach that iteratively predicts\nexpectations of future states. These predicted states are then\nused to forecast rewards and integrated into the learning\nprocess to enhance stability. Additionally, a method is proposed\nthat utilizes uncertainty propagation to make the reinforcement\nlearning agent aware of uncertainties. The proposed method\nwas evaluated using the CARLA simulator. Compared to the\nbaseline methods, the proposed method reduced the collision\nrate by 60.17%, and increased the average reward by 30.82\ntimes. A video of the proposed method is available at https:\n//www.youtube.com/watch?v=PfDbaeLfcN4.\nIndex terms—-Autonomous Vehicle, Reinforcement Learning,\nMotion Planning\nI. INTRODUCTION\nSince the 2007 DARPA Urban Challenge [1], autonomous\nvehicles(AVs) have been studied intensively. In addition,\nplanning methods for AVs have also been researched inten-\nsively and the findings have allowed AVs to drive success-\nfully within limited areas, using rule-based and optimization-\nbased algorithms, explicit heuristic rules, and parameters\nspecified for the given area. However, these traditional ap-\nproaches suffer from several limitations, including a lack\nof generality and a lack of complex motion. For example,\nthe heuristic rules and parameters specified for the given\narea may not be applied to the other areas, which impact\nthe scalability. In addition, many possible scenarios must be\nconsidered in real-world applications. If various scenarios\nare generalized with few scenarios(e.g., lane following, and\nlane changing), an overly simple policy could be obtained.\nNumerous studies have employed deep learning to address\nthese limitations [2]–[17].\nThe most popular approach is imitation learning(IL),\nwhich learns a driving policy directly from expert driving\ndata [2], [3]; however, IL also has several limitations, includ-\ning the cost of scalability, simple driving policies, and safety.\n*This work is supported by the Korea Agency for Infrastructure Technol-\nogy Advancement(KAIA) grant funded by the Ministry of Land, Infrastruc-\nture and Transport. (RS-2021-KA160853, Road traffic Infrastructure moni-\ntoring and emergency recovery support service technology development)\n1 ThorDrive, Seoul, 07268, Republic of Korea\nFig. 1: The AV(red car) plans a trajectory(red boxes) along its lane while\navoiding a parked car(blue car) on the right and a car that is changing\nlanes(another blue car) from the left lane. The AV’s planned trajectories\nare represented as red boxes, and their trajectories considering uncertainty,\nare represented as rounded light red boxes. The other vehicles’ predicted\ntrajectories are represented as blue boxes, and their trajectories, considering\nuncertainty, are represented as rounded light blue boxes. The goal of\neach AV’s trajectory is determined iteratively at each time step from the\npreviously predicted state of the AV and the states of the other vehicles.\nThe AV’s states and corresponding goals are represented by red, orange,\nyellow, green, blue, and purple circles(shown in chronological order). The\npredicted states of the other vehicles and the iteratively predicted states\nof the AV are employed for the prediction of reward within the planning\nhorizon, which stabilizes the RL learning process.\nFor example, to scale up the AV using the IL method, expert\ndata must be obtained for all scenarios and targeted areas,\nwhich is costly. In terms of driving policies, IL is typically\nused for handling simpler driving tasks, e.g., lane following.\nTo learn complex policies or policies in corner cases, it\nshould have a lot of data which can be costly and time-\nconsuming. In addition, an expert’s demonstrated policies\nfor complex scenarios or corner cases are more distributed\nthan the simple scenarios, which may yield large errors or\nlearning may be infeasible [4]. Relative to safety limitations,\ninsufficient amounts of data on dangerous cases or corner\ncases are available; thus, the IL agent could output dangerous\npolicies due to a lack of training data.\nAnother approach is the reinforcement learning(RL)\nmethod, which learns a policy via self-exploration and\nreinforcement without expert data. RL can also simulate\nand learn both complex policies and policies in corner\ncases. However, RL suffers from unstable learning when a\nneural network is used as a function approximation [18].\nError of function approximation results in unstable learning\nor even divergence [19], [20]. In addition, most previous\nRL-based trajectory planning studies did not consider the\nuncertainties of the object detection, trajectory prediction of\narXiv:2404.12079v4  [cs.RO]  12 May 2024\nother traffic participants, localization, and control modules\nthat are essential for AV to navigate. Note that not consid-\nering uncertainties could cause sudden decelerations or even\naccidents.\nThus, this study proposes an RL-based trajectory planning\nmethod for AVs that overcomes the identified limitations of\nRL and traditional planning methods. The proposed method\nemploys a reward prediction(RP) during the learning process\nwhich predicts expectations of future states. These predicted\nstates are then used to forecast rewards and integrated into\nthe learning process to enhance stability. In addition, an\niterative RP(IRP) method that uses RP iteratively to predicts\nrelevant states, actions, and corresponding rewards accurately\nis employed. As a result, the performance of the agent and\nthe learning stability are increased. Additionally, a method\nis proposed that utilizes uncertainty propagation [21], [22]\nto make the reinforcement learning agent aware of uncer-\ntainties. An overview of the proposed method is shown in\nFig.1.\nThe primary contributions of this study are summarized\nas follows:\n• The proposed method increases learning stability and\nthe performance of the RL agent.\n• The proposed method allows the RL agent to be aware\nof uncertainty.\n• A demonstration and comparison of the proposed\nmethod with the baseline methods in the CARLA sim-\nulator are presented.\nThe remainder of the paper is organized as follows:\nSection II reviews related works of RL-based trajectory\nplanning methods and uncertainty-aware planning methods.\nSection III defines problem formulation and RP, IRP, and\napplication of uncertainty propagation is proposed. Section\nIV shows how the proposed method and baseline methods\nare evaluated in the CARLA simulator. Section V analyzes\nthe evaluation results and shows how the key metrics are\nimproved. Section VI concludes the proposed method and\ndiscusses future works.\nII. RELATED WORKS\nA. RL-based Trajectory Planning\nPrevious RL-based trajectory planning methods for AVs\ncan be divided into two categories according to the action\nof an RL agent, i.e., 1) control command and 2) the Goal of\nthe trajectory.\n1) Control Command: Methods that action of an RL\nagent is a control command [5]–[10], [14] use a lateral\ncontrol(e.g., steer angle and steering rate) and a longitudinal\ncontrol(e.g., acceleration and jerk) as an output of an RL\nagent. However, such methods tend to fail to learn easily. The\nvariance of action affects the learning process; thus, to drive\nan AV successfully using a control command, very specific\npolicies are required to yield good rewards. For example, in\na highway scenario, a small turn of the steering wheel may\nyield catastrophic results. This specific policy requirement\nmakes it difficult for RL agents to explore and find good\nstates and actions, which leads to sample inefficiency during\ntraining and ineffective learning. Thus, unless the RL agent\nfind good policy early on by chance, learning will fail. In\naddition, an agent’s intentions are unknown; thus they lack\ninterpretability.\nKendall et al. [5] employed monocular images as an\nobservation and DDPG as the main algorithm to follow the\nlane in real-world scenarios. In addition, Chen et al. [6],\nemployed the bird-view semantic mask as an observation and\nevaluated their method in CARLA simulator. Their work was\ndeveloped further [7] by increasing interpretability using the\nprobabilistic graphical model. Saxena et al. [8], employed\na field-of-view as an observation and proximal policy op-\ntimization(PPO) as the main algorithm. Their primary task\nwas lane changing in dense traffic scenarios. In addition,\nWu et al. [9], employed the Dyna algorithm with PPO as the\nmain algorithm, and they imitated the world model using\nthe Gaussian process. Li et al. [14]. proposed a method\nusing an hierarchical RL(HRL). Their model-based high-\nlevel policy generates subgoals via optimization that utilizes\na low-level policy and an offline low-level policy outputs a\ncontrol command.\n2) Goal for Trajectory: Approaches that action of RL\nagent is goal/goals for trajectory [11]–[13], [15] are com-\nparably stable in learning. The action variance of these\napproaches has a relatively small effect on learning process\nof RL compared to approaches that action of an RL agent\nis control command. For example, in a highway scenario, a\nsmall change in the lateral deviation of a goal would result\nin a smaller amount of change in a result compared to the\nresult of the control command example.\nGao et al. [11] proposed method using HRL. Their high-\nlevel policy generates subgoals in the Frenet frame to guaran-\ntee the temporal and spatial reachability of the generated goal\nand the low-level policy outputs control commands. Their\nwork was developed further [12] by ensuring safety using the\nsafe-state enhancement method. In addition, Qiao et al. [13]\nemployed a hybrid HRL method, where a high-level policy\ngenerates optimal behavior decisions, and a low-level policy\ngenerates a trajectory point that the AV intends to trace. They\nalso employed a PID controller to trace the trajectory point.\nMa et al. [15] used the latent state inference method, and\nemploys PPO as a main algorithm.\nB. Uncertainty-aware Planning\nUncertainty-aware planning methods are used to plan a\ntrajectory for an AV by considering the uncertainty of the\nAV(i.e., localization and control) and the traffic partici-\npants(i.e., object detection, and trajectory prediction). Xu\net al. [21] employed a Kalman filter to estimate the uncer-\ntainty of the traffic participants, and the LQG framework\nto estimate the uncertainty of the AV. They also used the\nuncertainty estimation in planning by widening the size of\nthe AV and the traffic participants when checking for the\ncollision conditions. Fu et al. [22] and Qie et al. [23] also\nemployed a Kalman filter to estimate the uncertainty of\nthe traffic participant. Fu et al. used estimated uncertainty\nas a chance constraint when planning a velocity profile,\nand Qie et al. employed estimated uncertainty in a tube-\nbased MPC to plan a trajectory. In addition, Hubmann et\nal. [24] formulated the planning problem with uncertainties\nas a partially observable Markov decision process. They\nestimated the intent of a traffic participant and utilized it as an\nuncertainty. By using the adaptive belief tree and uncertainty,\nthey determined the optimal longitudinal motion of the AV.\nKhaitan et al. [25] estimated the uncertainty of the traffic\nparticipants by utilizing reachable set in short-term horizon\nand used it in the tube MPC to execute the trajectory safely\nin the presence of uncertainty.\nIII. METHOD\nTo solve trajectory planning problems using RL, RL\nalgorithms with continuous action space, e.g., the DDPG\n[26], TD3 [20], and PPO [27] algorithms, are more suitable\nthan algorithms that utilize a discrete action space. Since\ngetting smooth behaviors requires an increase in the size\nof the discrete action space which leads to discrete control\nmethods being intractable. In addition, for simplicity and\ninterpretability, algorithms with deterministic policies(rather\nthan stochastic policies) are selected. Furthermore, generat-\ning the goal for a trajectory is a better action choice than\ncontrol command because more specific policies are required\nfor control command methods to yield rewards successfully.\nThus, the proposed trajectory planning method generates a\ngoal using a deterministic continuous control RL algorithm.\nIt is assumed that information about the localization, route\npath, trajectory prediction of other traffic participants, and\nobject detection is provided. However, trajectory prediction\nof other traffic participants is only used during the learning\nprocess.\nA. Problem Formulation\nThe trajectory planning problem is formulated as a Markov\ndecision process, which is defined by the tuple (S, A, P, R).\nHere, s ∈S is the continuous state space, a ∈A is\nthe continuous action space, P is the probability of state\ntransition, and R is the reward received after each transition.\nThe return is defined as discounted sum of rewards Gt =\nP∞\nk=0 γkRt+k+1, where γ ∈(0, 1) is the discount factor\ndetermining the priority of short-term rewards. The purpose\nof RL is to learn an optimal policy that maximizes the\nexpected cumulative reward as follows:\nmax\nπ\nJ(π) = Es∼ρπ,a∼π[\n∞\nX\ni=0\nγir(s, a)],\n(1)\nwhere r is the reward function, ρπ is the state distribution\nunder the policy π. The future states of the AV and other\ntraffic participants are assumed to follow the planned trajec-\ntory and the predicted trajectory, respectively, with deviations\nfollowing a normal distribution, as assumed in [21].\nFig. 2: The AV(red car) is following the lane while the other traffic\nparticipant(blue car) attempts to change lanes. The predicted states of the\nAV and the other traffic participant are represented as red and blue boxes\nrespectively. The state of the AV at time t and its goal is represented as\nred circles. The predicted states of the other traffic participant are assumed\nto be given. The predicted states(red boxes) of the AV are predicted by the\nRL agent’s action π(st,0), i.e., the goal of the trajectory.\nB. Reward Prediction\nContinuous control RL algorithms employ the policy gra-\ndient method to learn policies directly. The policy gradient\nmethod maximizes the following objective function.\n∇θJ(πθ) = Es∼ρπ,a∼πθ[∇θlogπθ(a|s)Qπ(s, a)].\n(2)\nThis theorem is derived from the following objective\nfunction:\nJ(πθ) = Es∼ρπ,a∼πθ[r(s, a)].\n(3)\nThe objective function J(πθ) in (2) can be defined as the\naction value function Q, the advantage function A, and the\nTD error δ. The action value and advantage functions are\napproximated with neural networks in the deep RL method;\nhowever, function approximation with neural network always\ninvolves errors, which causes instability during the learning\nprocess and poor performance. To address this, the N-step\nSARSA concept is employed in RP. N-step SARSA improves\nlearning stability by utilizing the error reduction property of\nn-step returns. Nevertheless, the learning process of N-step\nSARSA remains susceptible to instability.\nTo overcome this, the RP method that utilizes the error\nreduction property of the n-step returns and enhances the\nlearning stability is proposed. To achieve this stability, RP\nenables the agent to explicitly learn the consequences of\nactions. This is accomplished by utilizing predictions of\ntraffic participants’ behavior and the planned trajectory of the\nAV, which is obtained using the agent’s output goal. Specifi-\ncally, future rewards are predicted by considering the planned\nfuture states of the AV and the predicted states of other\ntraffic participants. Thus, RP leverages the advantages of N-\nstep SARSA while explicitly learning action consequences.\nThese two traits of RP contribute to stabilizing the learning\nprocess. The proposed RP method is employed during the\naction value function update process as follows:\nse\nt,τ ∈T e\nt , so,k\nt,τ ∈T o,k\nt\n, so\nt,τ = {so,\nt,τ, so,1\nt,τ , · · · so,n\nt,τ }\nst,τ = f(se\nt,τ, so\nt,τ), Rt,τ+δ = g(st,τ, st,τ+δ),\nJ(π) = Qπ(st, at),\nL(θQ) = Est∼ρπ,at∼πθ[(Qπ(st, at) −yt)2],\nyt = Rt,δ + γ · Rt,2δ + · · ·\n+ γT/δ−1 · Rt,T + γT/δ · Qπ(st,T , π(st,T )),\n(4)\nwhere time t is when the AV’s trajectory is planned and the\ntrajectory prediction of other traffic participants is made, T\nis a planning/prediction horizon, δ is a sufficiently small time\nstep size, τ is the time within the planning horizon τ ∈[δ, T],\nse\nt,τ is the predicted future state of the AV at time t+τ from\ntrajectory T e\nt planned at time t using the goal π(st,0), so,k\nt,τ is\nthe predicted future state of the kth other traffic participant\nat time t + τ from prediction T o,k\nt\npredicted at time t, so\nt,τ\nis the predicted future states of n other traffic participants\nat time t + τ from prediction at time t. In addition, f is a\nfunction that combines the predicted future states of the AV\nand the other traffic participants at the same time to output\nthe state st,τ for the RL agent, g is a function that predicts\nthe reward Rt,τ+δ at time t + τ + δ during transition from\nst,τ to st,τ+δ, L(θQ) is the loss for the action value function\nQπ, yt is the expected return inferred by utilizing predicted\nrewards and the action value of the predicted state at the\nplanning horizon, and its corresponding action, and Rt,δ is\na reward received from the environment. Predicted future\nstates are used to predict rewards, which are then utilized\nduring the action value function update process. Fig.2 shows\nthe predicted states of the other vehicle and the AV with its\ngoal at time t.\nUnlike N-step SARSA, the expected return from the action\nvalue function learned by RP is not based on rewards\nresulting directly from state transitions, but rather on rewards\nassociated with expected states. Namely, RP increases learn-\ning stability by utilizing expected future states, yet it does\nnot directly account for the variation of these future states.\nHowever, in terms of predicting the states of other traffic\nparticipants and the AV, its variance is strongly related to\nsafety. Therefore, uncertainty propagation on future states of\nother traffic participants and the AV is utilized to consider\nthis variance. Additional details about the uncertainty prop-\nagation process are given in Section III-D.\nC. Iterative Reward Prediction\nIn general, AVs plan a trajectory at every time step,\nwhich is a common practice due to potential inaccuracies\nin predicting the behavior of other traffic participants and\nthe imprecise path tracking by the controller. Therefore, the\nproposed method also assumes trajectory planning at every\ntime step. Given this assumption, even if the controller tracks\nthe given trajectory perfectly, there is no guarantee that the\ntrajectory generated using the agent’s output will be the same\nas the previous one. Consequently, this inaccuracy of RP\nleads to inaccurate prediction of rewards, resulting in learn-\nFig. 3: Demonstration of inaccuracy problem of RP. Here, the agent has\nplanned a trajectory that takes AV close to the reference. The trajectories\nplanned at each time step beginning from each state-action pair are repre-\nsented by different colors. The return predicted using RP, QRP , is based on\nthe red trajectory planned at time t. The agent plans a new trajectory at each\nstep; therefore, the planned trajectory that RP utilized at time t(red) and the\nactual trajectory executed by the AV(blue) are different. Thus, QRP has an\nerror ϵ compared to the true return Qtrue, which is the return obtained by\nfollowing the blue trajectory.\ning instability and poor performance. Fig.3 demonstrates the\ninaccuracy problem of RP.\nTo address the inaccuracy problem of RP, the IRP method\nis proposed. This method predicts the rewards by iteratively\nplanning a new trajectory at the predicted state and predicting\nthe reward of that trajectory for one time step. Fig. 4 shows\nhow the proposed IRP method operates. Compared to the\nconventional RP, the proposed IRP predicts the rewards more\naccurately and reduces the error of the function approxi-\nmation of the action value function Qπ. The mathematical\nrepresentation of the IRP is given as follows:\nst+τ,0 = f(se\nt+τ,0, so\nt,τ)\nst+τ,δ = h(st+τ,0, π(st+τ,0)),\nRt+τ,δ = g(st+τ,0, st+τ,δ),\nJ(π) = Qπ(st, at),\nL(θQ) = Est∼ρπ,at∼π[(Qπ(st, at) −yt)2],\nyt = Rt,δ + γ · Rt+δ,δ + · · · + γT/δ−1 · Rt+T −δ,δ\n+ γT/δ · Qπ(st+T −δ,δ, π(st+T −δ,δ)),\n(5)\nwhere time t is when planning first started, st+τ,δ is\nthe predicted state at time t + τ + δ predicted from the\npreviously predicted state at time t + τ, and h is a function\nthat outputs a predicted state using the current state, and\naction. Here, the predicted state, such as st+τ,δ is used as\nst+τ+δ,0 for iteratively predicting the next state st+τ+δ,δ. In\nfunction h, the future states of the other traffic participants\nare assumed to follow the trajectories predicted at time t,\nwhich is assumed to be given. However, the future states of\nthe AV are updated iteratively at each time step.\nIn addition, considering the agent’s future action more\naccurately enables the agent to drive along a complicated\ntrajectory because the agent has a more comprehensive\nunderstanding of its future actions. As shown in Fig.4, the\nAV can plan a red trajectory by considering the agent’s future\nactions; thus, it is capable of driving a complicated trajectory.\nD. Uncertainty Propagation\nDuring RP and IRP, the expected future states are consid-\nered; however, the variance and uncertainty of the predicted\nFig. 4: Predicted states(blue boxes) of other traffic participants(blue cars)\nand predicted states(red boxes) of the AV(red car) and actions of the RL\nagent. The action of the RL agent is determined iteratively from the previous\nstate of the AV, its goal, and the states of the other vehicles. The goals and\nthe AV’s start states are represented as red, orange, yellow, green, blue,\nand purple circles in chronological order. The predicted states of the other\nvehicles and the iteratively predicted states of the AV are utilized to predict\nthe rewards during the learning process.\nfuture states are not considered. Note that not considering\nthe variance of the prediction leads to inaccurate function\napproximation and the RL agent being unaware of the\nuncertainty. In addition, ignoring the uncertainty can cause\nthe agent to perform sudden deceleration or become involved\nin an accident. For example, if a low-functioning controller\nis employed to track a trajectory, and the RL agent does not\ntake that into account, then the AV may encounter various\ndangerous situations and the predicted rewards may have\nsignificant error. To consider the variance of the prediction\nand the uncertainty of future states, uncertainty propagation\non the RP and IRP motivated by [21], [22] is utilized.\nThe uncertainty propagation process is built on the Kalman\nfilter; however, the measurement update process of the\nKalman filter is removed because observing future states is\nimpossible. The uncertainty propagation utilized on the RP\nis modeled as follows:\nˇse\nt,τ ∼N(se\nt,τ, Σe\nt,τ), ˇso\nt,τ ∼N(so\nt,τ, Σo\nt,τ)\nΣe\nt,τ+1 = FΣe\nt,τF T + Qe\nτ, Σo\nt,τ+1 = FΣo\nt,τF T + Qo\nτ,\n(6)\nwhere each state of the AV ˇse\nt,τ, and states of the other\ntraffic participants ˇso\nt,τ is modeled as Gaussian a random\nvariable with means se\nt,τ, and so\nt,τ, and covariance of Σe\nt,τ,\nand Σo\nt,τ, respectively, based on the assumption made on III-\nA, F is the state transition matrix and Qe\nτ, Qo\nτ represent the\nprocess noise of the AV and the other traffic participants,\nrespectively, where Qe\nτ is attributed to localization, control\nerror, and Qo\nτ is attributed to object detection, trajectory\nprediction error respectively. The introduced states ˇse\nt,τ and\nˇso\nt,τ are utilized in the collision checking process of the IRP\nmethod to account for uncertainty. Here, the ellipse defined\nby the covariance matrix can provide an upper bound of the\nprobability 1 −δ that the AV and other traffic participants\nexist. However, to check for a collision between the AV\nand other traffic participants, the rectangle shapes of the\nAV and other traffic participants must be considered. In\nthe proposed method, the Minkowski sum of the rectangle\nand the ellipse [21] is computed. The new shape from the\nMinkowski sum is then utilized for collision checking, which\nguarantees the probability of (1 −δ)2 whether collide or\nnot. The illustration of uncertainty propagation using the\nMinkowski sum is shown in Fig.1.\nE. Overall Algorithm\nBelow, a demonstration showcasing the combination of\nIRP and uncertainty propagation is provided. The proposed\nmethod, based on the existing deterministic policy gradi-\nent algorithm, is applied during the critic update process.\nThe pseudocode of the proposed method is given in Algo-\nrithm 1. Here, in lines 6-7, the uncertainty propagation and\nMinkowski sum are executed using the state of the AV and\nthe other traffic participants se\nt+τ,0, so\nt+τ,0. In line 9, the state\nst+τ,δ results from the utilization of the agent’s previous\nstate st+τ,0, and the agent’s action π(st+τ,0). In line 10,\nthe reward is predicted during the transition from st+τ,0 to\nst+τ,δ. In line 12, the state st+τ,δ is divided into se\nt+τ,δ, and\nso\nt+τ,δ, which are used as the states se\nt+τ+δ,0, and so\nt+τ+δ,0\nfor the next iteration.\nAlgorithm 1 Pseudo code of proposed method\n1: procedure CRITICUPDATE( )\n2:\nse ←se\nt,0, so ←so\nt,0\n3:\nr ←Rt,δ ▷Initialize predicted return with received\nreward from the simulator\n4:\nτ ←δ\n5:\nwhile τ < T do\n6:\nˇse, ˇso ←UncertaintyPropagation(se, so)\n7:\nse′, so′ ←MinkowskiSum(ˇse, ˇso)\n8:\ns ←f(se′, so′)\n▷Merge states\n9:\ns′ ←h(s, π(s))\n▷Prediction of st+τ,δ\n10:\nR ←g(s, s′)\n▷Predict the reward Rt+τ,δ\n11:\nr ←r + γτ/δR\n▷Update the predicted return\n12:\nse, so ←f −1(s′)\n▷Update the next state\n13:\nτ ←τ + δ\n14:\nend while\n15:\nˇse, ˇso ←UncertaintyPropagation(se, so)\n16:\nse′, so′ ←MinkowskiSum(ˇse, ˇso)\n17:\ns ←f(se′, so′)\n▷Merge states\n18:\nSet yt = r + γT/δQ(s, π(s|θπ)|θQ)\n19:\nUpdate critic by minimizing the loss:\n20:\nL(θQ) = (Q(st, at|θQ)) −yt)2\n21: end procedure\nIV. EXPERIMENTS\nThe proposed method and baseline methods were eval-\nuated using the CARLA simulator. The experimental con-\nfiguration, baseline methods, and implementation details are\ndescribed in the following sections.\nA. Experiment Configuration\nThe proposed and baseline methods were evaluated in\nfour distinct scenarios. Scenario 1 involved lane following\nwith static obstacles, scenario 2 involved lane following\nwith traffic participants, scenario 3 involved lane changing\nwith traffic participants, and scenario 4 entailed overtaking\nparked cars with traffic participants. All necessary inputs,\ne.g., route path, object detection, trajectory prediction, and\nlocalization were given. Success was determined if the AV\nreached the goal without a collision within specified time. In\nthis evaluation, the goal was 130m ahead of the AV’s initial\nposition. Here, a maximum lateral deviation of 1.5m from\nthe center of the target lane was permitted.\nIn each scenario, the AV was spawned on the road\nwith a random lateral deviation of [−1.5m, 1.5m] from\nthe center of the road, and a random heading angle de-\nviation of [−20deg, 20deg] with a random initial speed of\n[5km/h, 15km/h]. In scenario 1, a maximum of two static\nobstacles(i.e., vehicles) were spawned at random positions\nwith a lateral deviation of [−0.5m, 0.5m] from the center of\nthe road, and a heading angle deviation of [−20deg, 20deg].\nScenario 2 included a maximum of five randomly spawned\ntraffic participants. In scenario 3, the other traffic participants\nwere the same as in scenario 2; however, the AV’s goal\nwas to change lanes. In scenario 4, a maximum of two\nstatic obstacles(parked cars) and three traffic participants\nwere spawned, and the goal of the AV was to overtake the\nparked cars while avoiding collisions with the other traffic\nparticipants. Note that the traffic participants in each scenario\nwere designed to change lanes randomly.\nB. Baseline Methods\nThe following two baseline methods were considered in\nthis evaluation. In baseline 1, the output of an agent was a\ncontrol command, and it is identical to the method proposed\nin [5], except that in [5], monocular images are used as input.\nIn baseline 2, the output of the agent was the trajectory goal,\nthe same as that of the proposed method, for fair comparison.\nHowever, baseline 2 did not include RP, IRP, or uncertainty\npropagation. In addition, proposed method was evaluated\nindividually as follows: RP, IRP, and IRP with uncertainty\npropagation.\nC. Implementation Details\nAll five methods, i.e., baseline1, baseline2, RP, IRP, and\nIRP with uncertainty propagation were implemented using\nthe DDPG algorithm with the same state, and reward func-\ntion.\n1) State: The features of the state space s are composed\nof se and so. Here, se is the state of the AV and comprises\n(d, ˙d, ¨d, ˙σ, ¨σ, θ, vspeed_limit, l, w), where σ and d are the\nlongitudinal and lateral position on the Frenet frame, θ is\nthe heading angle difference with the center of the road,\nvspeed_limit is the speed limit of the road, and l and w\nare length and width of the AV in consideration of the\nMinkowski sum. so is composed of so,k, k ∈N, where N is a\nnatural number. so,k is the state of the kth traffic participant,\nwhich is composed of (σ, d, θ, l, w, vσ, vd), where vσ and vd\nare the longitudinal and lateral velocity on the Frenet frame.\n2) Action: The action space a is the goal of the trajec-\ntory which is composed of (Ttarget, dtarget, σtarget, ˙σtarget),\nwhere Ttarget is the time interval between the current state\nand the goal state, σtarget, and dtarget are the longitudinal\nand lateral target positions on the Frenet frame, and ˙σtarget\nis the target longitudinal speed. Here, the trajectory plan-\nning method [28] is employed to plan a trajectory toward\nthe goal. The lateral jerk-optimal trajectory is generated\ngiven the initial state of the AV [d, ˙d, ¨d], and the end state\n[dtarget, ˙dtarget\n= 0, ¨dtarget\n= 0] at Ttarget from the\naction. In addition, the longitudinal jerk-optimal trajectory\nis generated given the initial state of the AV [σ, ˙σ, ¨σ], and\nthe end state [σtarget, ˙σtarget, ¨σtarget = 0] at Ttarget from\nthe action. The final trajectory is obtained by combining the\nlateral and longitudinal trajectories. Note that the planned\ntrajectory is tracked using an MPC-based controller.\n3) Reward: The reward function is designed to encourage\nsafe, comfortable, and efficient driving as follows:\nR =λlat_acc · alat_acc + λlat_jerk · alat_jerk\n+λlong_acc · along_acc + λlong_jerk · along_jerk\n+λd · |d| + λv · |v −vdes| + rcollision,\n(7)\nwhere λlat_acc, and alat_acc represent the weight and penalty\nfor the lateral acceleration, λlat_jerk, and alat_jerk are\nthe weight and penalty for the lateral jerk, λlong_acc, and\nalong_acc are the weight and penalty for the longitudinal\nacceleration, λlong_jerk, and along_jerk are the weight and\npenalty for the longitudinal jerk, λd, and |d| are the weight\nand penalty for the lateral deviation from the target lane, λv,\n|v −vdes| are the weight and the penalty for being slower\nor faster than the desired speed respectively, and rcollision\nrepresents the reward and penalty for a collision event. Here,\nrcollision is negative when a collision occurs and positive\nwhen no collision occurs. Note that the above reward is also\nutilized during RP.\nV. RESULTS\nThe average reward per time step, collision rate, and\nsuccess rate across all training scenarios are depicted in Fig.\n5. Table I showcases the best scores achieved by each method\nacross all training scenarios, providing a comprehensive\noverview of performance comparisons.\nBaseline 2 consistently outperformed baseline 1 in every\nscenario and key metric while demonstrating stable learning.\nThe robustness of baseline 2 against action variance enabled\nit to explore and identify optimal states and actions, resulting\nin stable learning and improved performance. Furthermore,\nagents utilizing RP, IRP, and IRP with uncertainty propaga-\ntion exhibit stable learning compared to the baseline meth-\nods, as illustrated in Fig. 5. Across all scenarios, these agents\nconsistently outperformed the baseline methods, showing\nsequential performance improvements. The agent with RP\noutperformed the baseline methods, which is attributed to its\nutilization of the error reduction property of N-step SARSA\nand explicit learning of the consequences of actions. The\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\n(g)\n(h)\n(i)\n(j)\n(k)\n(l)\nFig. 5: Average reward per time step in an episode, collision rate, and success rate of all methods for all scenarios during training.\nagent with IRP surpassed the agent with RP due to accurate\nprediction of future actions, resulting in more precise reward\nprediction and improved overall performance. Additionally,\nthe agent with IRP and uncertainty propagation outperformed\nthe agent with IRP, benefiting from the consideration of\nuncertainty.\nIn scenario 3, although lane changes are quite challenging\nin typical road situations, the collision rate was relatively\nlow compared to the other scenarios. This was due to the\nconservative driving behavior of the other traffic participants\nused in the experiment, which helped reduce the collision\nrate. In scenario 4, the collision rate was much higher\nthan in scenarios 1-3 because it was considerably more\nchallenging to avoid parked cars while also avoiding the\nother traffic participants, compared to the other scenarios.\nA video of the agent with IRP and uncertainty propagation\nis available at https://www.youtube.com/watch?\nv=PfDbaeLfcN4.\nVI. CONCLUSIONS AND FUTURE WORK\nIn this study, methods including RP, IRP, and uncertainty\npropagation are proposed to reduce the function approx-\nimation error and improve the performance and learning\nstability of an AV’s RL-based planning agent. The proposed\nmethod was evaluated under several scenarios, and the results\ndemonstrated that the proposed method improves both learn-\ning stability and agent performance compared to baseline\nmethods. However, the result of the evaluation showed that\nthe proposed method still has poor performance in difficult\nand complex scenarios. Future work will involve increasing\nTABLE I: Best scores for each compared method for all scenarios during\ntraining.\nScenario\nAverage\nCollision\nSuccess\nreward\nrate\nrate\n1\nBaseline 1\n0.0589\n42.81%\n20.65%\nBaseline 2\n0.0942\n28.32%\n70.95%\nRP\n0.1854\n14.83%\n84.15%\nIRP\n0.2194\n13.47%\n84.81%\nIRP+UP\n0.2297\n5.682%\n93.86%\n2\nBaseline 1\n0.0383\n61.71%\n21.68%\nBaseline 2\n0.0912\n43.04%\n56.73%\nRP\n0.2200\n16.75%\n82.79%\nIRP\n0.2252\n8.385%\n91.50%\nIRP+UP\n0.2460\n5.782%\n94.11%\n3\nBaseline 1\n0.0073\n60.04%\n15.81%\nBaseline 2\n0.1424\n42.12%\n19.62%\nRP\n0.1980\n14.16%\n85.86%\nIRP\n0.2083\n5.913%\n93.85%\nIRP+UP\n0.2250\n3.590%\n96.08%\n4\nBaseline 1\n0.0694\n78.43%\n6.911%\nBaseline 2\n0.0909\n54.65%\n37.84%\nRP\n0.1870\n31.87%\n67.67%\nIRP\n0.2173\n21.55%\n78.40%\nIRP+UP\n0.2299\n18.26%\n81.91%\nsafety while having better performance.\nREFERENCES\n[1] Martin Buehler, Karl Iagnemma, and Sanjiv Singh. The DARPA urban\nchallenge: autonomous vehicles in city traffic, volume 56. springer,\n2009.\n[2] Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Xizhou\nZhu, Siqi Chai, Senyao Du, Tianwei Lin, Wenhai Wang, et al.\nPlanning-oriented autonomous driving.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition,\npages 17853–17862, 2023.\n[3] Stefano Pini, Christian S Perone, Aayush Ahuja, Ana Sofia Rufino\nFerreira, Moritz Niendorf, and Sergey Zagoruyko.\nSafe real-world\nautonomous driving by learning to predict and plan with a mixture\nof experts. In 2023 IEEE International Conference on Robotics and\nAutomation (ICRA), pages 10069–10075. IEEE, 2023.\n[4] Shai Shalev-Shwartz, Shaked Shammah, and Amnon Shashua. Safe,\nmulti-agent, reinforcement learning for autonomous driving.\narXiv\npreprint arXiv:1610.03295, 2016.\n[5] Alex Kendall, Jeffrey Hawke, David Janz, Przemyslaw Mazur, Daniele\nReda, John-Mark Allen, Vinh-Dieu Lam, Alex Bewley, and Amar\nShah. Learning to drive in a day. In 2019 International Conference\non Robotics and Automation (ICRA), pages 8248–8254. IEEE, 2019.\n[6] Jianyu Chen, Bodi Yuan, and Masayoshi Tomizuka. Model-free deep\nreinforcement learning for urban autonomous driving. In 2019 IEEE\nintelligent transportation systems conference (ITSC), pages 2765–\n2771. IEEE, 2019.\n[7] Jianyu Chen, Shengbo Eben Li, and Masayoshi Tomizuka.\nInter-\npretable end-to-end urban autonomous driving with latent deep rein-\nforcement learning. IEEE Transactions on Intelligent Transportation\nSystems, 23(6):5068–5078, 2021.\n[8] Dhruv Mauria Saxena, Sangjae Bae, Alireza Nakhaei, Kikuo Fujimura,\nand Maxim Likhachev.\nDriving in dense traffic with model-free\nreinforcement learning. In 2020 IEEE International Conference on\nRobotics and Automation (ICRA), pages 5385–5392. IEEE, 2020.\n[9] Guanlin Wu, Wenqi Fang, Ji Wang, Pin Ge, Jiang Cao, Yang Ping, and\nPeng Gou. Dyna-ppo reinforcement learning with gaussian process for\nthe continuous action decision-making in autonomous driving. Applied\nIntelligence, 53(13):16893–16907, 2023.\n[10] Bła˙zej Osi´nski, Adam Jakubowski, Paweł Zi˛ecina, Piotr Miło´s,\nChristopher Galias, Silviu Homoceanu, and Henryk Michalewski.\nSimulation-based reinforcement learning for real-world autonomous\ndriving.\nIn 2020 IEEE international conference on robotics and\nautomation (ICRA), pages 6411–6418. IEEE, 2020.\n[11] Lingping Gao, Ziqing Gu, Cong Qiu, Lanxin Lei, Shengbo Eben Li,\nSifa Zheng, Wei Jing, and Junbo Chen. Cola-hrl: Continuous-lattice\nhierarchical reinforcement learning for autonomous driving. In 2022\nIEEE/RSJ International Conference on Intelligent Robots and Systems\n(IROS), pages 13143–13150. IEEE, 2022.\n[12] Ziqing Gu, Lingping Gao, Haitong Ma, Shengbo Eben Li, Sifa Zheng,\nWei Jing, and Junbo Chen.\nSafe-state enhancement method for\nautonomous driving via direct hierarchical reinforcement learning.\nIEEE Transactions on Intelligent Transportation Systems, 2023.\n[13] Zhiqian Qiao, Jeff Schneider, and John M Dolan. Behavior planning\nat urban intersections through hierarchical reinforcement learning. In\n2021 IEEE International Conference on Robotics and Automation\n(ICRA), pages 2667–2673. IEEE, 2021.\n[14] Jinning Li, Chen Tang, Masayoshi Tomizuka, and Wei Zhan.\nHi-\nerarchical planning through goal-conditioned offline reinforcement\nlearning. IEEE Robotics and Automation Letters, 7(4):10216–10223,\n2022.\n[15] Xiaobai Ma, Jiachen Li, Mykel J Kochenderfer, David Isele, and\nKikuo Fujimura. Reinforcement learning for autonomous driving with\nlatent state inference and spatial-temporal relationships. In 2021 IEEE\nInternational Conference on Robotics and Automation (ICRA), pages\n6064–6071. IEEE, 2021.\n[16] Tung Phan-Minh, Forbes Howington, Ting-Sheng Chu, Momchil S\nTomov, Robert E Beaudoin, Sang Uk Lee, Nanxiang Li, Caglayan\nDicle, Samuel Findler, Francisco Suarez-Ruiz, et al. Driveirl: Drive\nin real life with inverse reinforcement learning.\nIn 2023 IEEE\nInternational Conference on Robotics and Automation (ICRA), pages\n1544–1550. IEEE, 2023.\n[17] Yantao Tian, Xuanhao Cao, Kai Huang, Cong Fei, Zhu Zheng, and\nXuewu Ji.\nLearning to drive like human beings: A method based\non deep reinforcement learning.\nIEEE Transactions on Intelligent\nTransportation Systems, 23(7):6357–6367, 2021.\n[18] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu,\nJoel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller,\nAndreas K Fidjeland, Georg Ostrovski, et al.\nHuman-level control\nthrough deep reinforcement learning.\nnature, 518(7540):529–533,\n2015.\n[19] Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforce-\nment learning with double q-learning. In Proceedings of the AAAI\nconference on artificial intelligence, volume 30, 2016.\n[20] Scott Fujimoto, Herke Hoof, and David Meger. Addressing function\napproximation error in actor-critic methods. In International confer-\nence on machine learning, pages 1587–1596. PMLR, 2018.\n[21] Wenda Xu, Jia Pan, Junqing Wei, and John M Dolan. Motion planning\nunder uncertainty for on-road autonomous driving.\nIn 2014 IEEE\nInternational Conference on Robotics and Automation (ICRA), pages\n2507–2512. IEEE, 2014.\n[22] Jiawei Fu, Xiaotong Zhang, Zhiqiang Jian, Shitao Chen, Jingmin Xin,\nand Nanning Zheng. Efficient safety-enhanced velocity planning for\nautonomous driving with chance constraints.\nIEEE Robotics and\nAutomation Letters, 2023.\n[23] Tianqi Qie, Weida Wang, Chao Yang, Ying Li, Yuhang Zhang, Wenjie\nLiu, and Changle Xiang.\nAn improved model predictive control-\nbased trajectory planning method for automated driving vehicles\nunder uncertainty environments.\nIEEE Transactions on Intelligent\nTransportation Systems, 24(4):3999–4015, 2022.\n[24] Constantin Hubmann, Jens Schulz, Marvin Becker, Daniel Althoff,\nand Christoph Stiller. Automated driving in uncertain environments:\nPlanning with interaction and uncertain maneuver prediction. IEEE\ntransactions on intelligent vehicles, 3(1):5–17, 2018.\n[25] Shivesh Khaitan, Qin Lin, and John M Dolan.\nSafe planning and\ncontrol under uncertainty for self-driving.\nIEEE Transactions on\nVehicular Technology, 70(10):9826–9837, 2021.\n[26] Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas\nHeess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra.\nContinuous control with deep reinforcement learning.\nIn ICLR\n(Poster), 2016.\n[27] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and\nOleg Klimov. Proximal policy optimization algorithms. arXiv preprint\narXiv:1707.06347, 2017.\n[28] Moritz Werling, Julius Ziegler, Sören Kammel, and Sebastian Thrun.\nOptimal trajectory generation for dynamic street scenarios in a frenet\nframe.\nIn 2010 IEEE international conference on robotics and\nautomation, pages 987–993. IEEE, 2010.\n",
  "categories": [
    "cs.RO"
  ],
  "published": "2024-04-18",
  "updated": "2024-05-12"
}