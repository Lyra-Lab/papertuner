{
  "id": "http://arxiv.org/abs/2404.16818v2",
  "title": "Boosting Unsupervised Semantic Segmentation with Principal Mask Proposals",
  "authors": [
    "Oliver Hahn",
    "Nikita Araslanov",
    "Simone Schaub-Meyer",
    "Stefan Roth"
  ],
  "abstract": "Unsupervised semantic segmentation aims to automatically partition images\ninto semantically meaningful regions by identifying global semantic categories\nwithin an image corpus without any form of annotation. Building upon recent\nadvances in self-supervised representation learning, we focus on how to\nleverage these large pre-trained models for the downstream task of unsupervised\nsegmentation. We present PriMaPs - Principal Mask Proposals - decomposing\nimages into semantically meaningful masks based on their feature\nrepresentation. This allows us to realize unsupervised semantic segmentation by\nfitting class prototypes to PriMaPs with a stochastic expectation-maximization\nalgorithm, PriMaPs-EM. Despite its conceptual simplicity, PriMaPs-EM leads to\ncompetitive results across various pre-trained backbone models, including DINO\nand DINOv2, and across different datasets, such as Cityscapes, COCO-Stuff, and\nPotsdam-3. Importantly, PriMaPs-EM is able to boost results when applied\northogonally to current state-of-the-art unsupervised semantic segmentation\npipelines. Code is available at https://github.com/visinf/primaps.",
  "text": "Published in Transactions on Machine Learning Research (09/2024)\nBoosting Unsupervised Semantic Segmentation\nwith Principal Mask Proposals\nOliver Hahn1\nNikita Araslanov2,3\nSimone Schaub-Meyer1,4\nStefan Roth1,4\n1 Department of Computer Science, TU Darmstadt\n2 Department of Computer Science, TU Munich\n3 Munich Center for Machine Learning (MCML)\n4hessian.AI\nReviewed on OpenReview: https: // openreview. net/ forum? id= UawaTQzfwy\nAbstract\nUnsupervised semantic segmentation aims to automatically partition images into semanti-\ncally meaningful regions by identifying global semantic categories within an image corpus\nwithout any form of annotation.\nBuilding upon recent advances in self-supervised rep-\nresentation learning, we focus on how to leverage these large pre-trained models for the\ndownstream task of unsupervised segmentation.\nWe present PriMaPs – Principal Mask\nProposals – decomposing images into semantically meaningful masks based on their feature\nrepresentation. This allows us to realize unsupervised semantic segmentation by fitting class\nprototypes to PriMaPs with a stochastic expectation-maximization algorithm, PriMaPs-EM.\nDespite its conceptual simplicity, PriMaPs-EM leads to competitive results across various\npre-trained backbone models, including DINO and DINOv2, and across different datasets,\nsuch as Cityscapes, COCO-Stuff, and Potsdam-3.\nImportantly, PriMaPs-EM is able to\nboost results when applied orthogonally to current state-of-the-art unsupervised semantic\nsegmentation pipelines. Code is available at https://github.com/visinf/primaps.\n1\nIntroduction\nSemantic image segmentation is a dense prediction task that classifies image pixels into categories from a\npre-defined semantic taxonomy. Owing to its fundamental nature, semantic segmentation has a broad range\nof applications, such as image editing, medical imaging, robotics, or autonomous driving (see Minaee et al.,\n2022, for an overview). Addressing this problem via supervised learning requires ground-truth labels for every\npixel (Long et al., 2015; Ronneberger et al., 2015; Chen et al., 2018b). Such manual annotation is extremely\ntime and resource intensive. For instance, a trained human annotator requires an average of 90 minutes to\nlabel up to 30 classes in a single 2 MP image (Cordts et al., 2016). While committing significant resources\nto large-scale annotation efforts achieves excellent results (Kirillov et al., 2023), there is natural interest in a\nmore economical approach. Alternative lines of research aim to solve the problem using cheaper – so-called\n“weaker” – variants of annotation. For example, image-level supervision describing the semantic categories\npresent in the image, or bounding-box annotations, can reach impressive levels of segmentation accuracy\n(Dai et al., 2015; Araslanov & Roth, 2020; Oh et al., 2021; Xu et al., 2022; Ru et al., 2023).\nAs an extreme problem scenario toward reducing the annotation effort, unsupervised semantic segmentation\naims to consistently discover and categorize image regions in a given data domain without any labels, know-\ning only how many classes to discover. Unsupervised semantic segmentation is highly ambiguous as class\nboundaries and the level of categorical granularity are task-dependent.1 However, we can leverage the fact\nthat typical image datasets have a homogeneous underlying taxonomy and exhibit invariant domain char-\nacteristics. Therefore, it is still feasible to decompose images in such datasets in a semantically meaningful\nand consistent manner without annotations.\n1While assigning actual semantic labels to regions without annotation is generally infeasible, the assumption is that the\ncategories of the discovered segments will strongly correlate with human notions of semantic meaning.\n1\narXiv:2404.16818v2  [cs.CV]  6 Oct 2024\nPublished in Transactions on Machine Learning Research (09/2024)\nImage\nMask 1\nMask 2\nMask 3\nPriMaPs (all)\nPseudo Label\n...\n...\n...\nFigure 1: PriMaPs pseudo-label example. Principal mask proposals (PriMaPs) are iteratively extracted\nfrom an image (dashed arrows). Each mask is assigned a semantic class, resulting in a pseudo label. The\nexamples are taken from the Cityscapes (top), COCO-Stuff (middle), and Potsdam-3 (bottom) datasets.\nDespite the challenges of unsupervised semantic segmentation, we have witnessed remarkable progress on\nthis task in the past years (Ji et al., 2019; Cho et al., 2021; Van Gansbeke et al., 2021; 2022; Ke et al., 2022;\nYin et al., 2022; Hamilton et al., 2022; Karlsson et al., 2022; Li et al., 2023; Seong et al., 2023; Seitzer et al.,\n2023). Deep representations obtained with self-supervised learning (SSL), such as DINO (Caron et al., 2021),\nhave played a critical role in this advance. However, it remains unclear whether previous work leverages the\nintrinsic properties of the original SSL representations, or merely uses them for “bootstrapping” and learns\na new representation on top. Exploiting the inherent properties of SSL features is preferable for two reasons.\nFirst, training SSL models incurs a substantial computational effort, justifiable only if the learned feature\nextractor is sufficiently versatile. In other words, one can amortize the high computational cost over many\ndownstream tasks, provided that task specialization is computationally negligible. Second, studying SSL\nrepresentations with lightweight tools, such as linear models, leads to a more interpretable empirical analysis\nthan with the use of more complex models, as evidenced by the widespread use of linear probing in SSL\nevaluation. Such interpretability advances research on SSL models toward improved cross-task generalization.\nEquipped with essential tools of linear modeling, i. e. Principal Component Analysis (PCA), we generate\nPrincipal Mask Proposals, or PriMaPs, directly from the SSL representation. Complementing previous\nfindings on object-centric images (Tumanyan et al., 2022; Amir et al., 2022), we show that principal com-\nponents of SSL features tend to identify visual patterns with high semantic correlation also in scene-centric\nimagery. Leveraging PriMaPs and minimalist post-processing, we construct semantic pseudo labels for each\nimage as illustrated in Fig. 1. Finally, instead of learning a new embedding on top of the SSL representation\n(Hamilton et al., 2022; Seong et al., 2023; Seitzer et al., 2023; Zadaianchuk et al., 2023), we employ a moving\naverage implementation of stochastic Expectation Maximization (EM) (Chen et al., 2018a) to assign a con-\nsistent category to each segment in the pseudo labels and directly optimize class prototypes in the feature\nspace. Our experiments show that this straightforward approach not only boosts the segmentation accu-\nracy of the DINO baseline, but also that of more advanced state-of-the-art approaches tailored for semantic\nsegmentation, such as STEGO (Hamilton et al., 2022) and HP (Seong et al., 2023).\nWe make the following contributions: (i) We derive lightweight mask proposals, leveraging intrinsic properties\nof the embedding space, e. g., the covariance matrix, provided by an off-the-shelf SSL approach. (ii) Based\non the mask proposals, we construct pseudo labels and employ moving average stochastic EM to assign a\nconsistent semantic class to each proposal. (iii) We demonstrate improved segmentation accuracy across a\nwide range of SSL embeddings and datasets.\n2\nPublished in Transactions on Machine Learning Research (09/2024)\n2\nRelated Work\nOur work builds upon recent advances in self-supervised representation learning, and takes inspiration from\nprevious unsupervised semantic and instance segmentation methods.\nThe goal of self-supervised representation learning (SSL) is to provide generic, task-agnostic feature\nextractors (He et al., 2020; Chen et al., 2020; Grill et al., 2020). A pivotal role in defining the behavior of\nself-supervised features on future downstream tasks is taken by the self-supervised objective, the so-called\npretext task. Examples of such tasks include predicting the context of a patch (Doersch et al., 2015) or its\nrotation (Gidaris et al., 2018), image inpainting (Pathak et al., 2016), and “solving” jigsaw puzzles (Noroozi\n& Favaro, 2016). Another family of self-supervised techniques is based on contrastive learning (Chen et al.,\n2020; Caron et al., 2020). More recently, Transformer networks (Dosovitskiy et al., 2020) revived some older\npretext tasks, such as context prediction (Caron et al., 2021; He et al., 2022), in a more data-scalable fashion.\nWhile the standard evaluation practice in SSL (e. g., linear probing, transfer learning) offers some glimpse\ninto the features’ properties, understanding the embedding space produced by SSL remains an active terrain\nfor research (Ericsson et al., 2021; Naseer et al., 2021). In particular, DINO features (Caron et al., 2021;\nOquab et al., 2024) are known to encode accurate object-specific information, such as object parts (Amir\net al., 2022; Tumanyan et al., 2022). However, it remains unclear to what extent DINO embeddings allow\nfor semantic representation of the more ubiquitous multi-object scenes. Here, following previous work (e. g.,\nHamilton et al., 2022; Seong et al., 2023), we provide further insights.\nEarly techniques for unsupervised semantic segmentation using deep networks (Cho et al., 2021;\nVan Gansbeke et al., 2021) approach the problem in the spirit of transfer learning and, under certain\nnomenclatures, may not be considered fully unsupervised. Specifically, starting with supervised ImageNet\npre-training (Russakovsky et al., 2015), a network obtains a fine-tuning signal from segmentation-oriented\ntraining objectives. Such supervised “bootstrapping” appears to be crucial in the ill-posed unsupervised for-\nmulation. Unsupervised training of a deep model for segmentation from scratch is possible, albeit sacrificing\naccuracy (Ji et al., 2019; Ke et al., 2022). However, training a new deep model for each downstream task\ncontradicts the spirit of SSL of amortizing the high SSL training costs over many computationally cheap\nspecializations of the learned features (Bommasani et al., 2021).\nRelying on self-supervised DINO pre-training, recent work (Hamilton et al., 2022; Li et al., 2023; Seong et al.,\n2023) has demonstrated the potential of such amortization with more lightweight fine-tuning for semantic\nsegmentation. Nevertheless, most of this work has treated the SSL representation as an inductive prior by\nlearning a new embedding space over the SSL features (e. g., Hamilton et al., 2022; Seong et al., 2023). In\ncontrast, following SSL principles, we use the SSL representation in a more direct and lightweight fashion –\nby extracting mask proposals using linear models (PCA) with minimal post-processing and learning a direct\nmapping from feature to prediction space.\nMask proposals have an established role in computer vision (Arbelaez et al., 2011; Uijlings et al., 2013), and\nremain highly relevant in deep learning (Hwang et al., 2019; Van Gansbeke et al., 2021; Yin et al., 2022).\nDifferent from previous work, we directly derive the mask proposals from SSL representations. Our approach\nis inspired by the recent use of classical algorithms, such as normalized cuts (Ncut Shi & Malik, 2000), in the\ncontext of self-supervised segmentation (Wang et al., 2023a;b). However previous approaches (Van Gansbeke\net al., 2021; 2022; Wang et al., 2023a;b) mainly proposed foreground object masks on object-centric data,\nutilized in a multi-step self-training. In contrast, we develop a straightforward method for extracting dense\npseudo labels for learning unsupervised semantic segmentation of scene-centric data and show consistent\nbenefits in improving the segmentation accuracy across a variety of baselines and state-of-the-art methods\n(Hamilton et al., 2022; Seong et al., 2023).\n3\nPriMaPs: Principal Mask Proposals\nIn this paper, we leverage recent advances in self-supervised representation learning (Caron et al., 2021;\nOquab et al., 2024) for the specific downstream task of unsupervised semantic segmentation. Our approach is\n3\nPublished in Transactions on Machine Learning Research (09/2024)\nbased on the observation that such pre-trained features already exhibit intrinsic spatial similarities, capturing\nsemantic correlations, thus providing guidance to fit global pseudo-class representations.\nA simple baseline.\nConsider a simple baseline that applies K-means clustering to DINO ViT features\n(Caron et al., 2021). Surprisingly, this already leads to reasonably good unsupervised semantic segmentation\nresults, e. g., around 15 % mean IoU to segment 27 classes on Cityscapes (Cordts et al., 2016), see Tab. 1.\nHowever, supervised linear probing between the same feature space and the ground-truth labels – the the-\noretical upper bound – leads to clearly superior results of almost 36 % mean IoU. Given this gap and the\nsimplicity of the approach, we conclude that there is valuable potential in directly obtaining semantic seg-\nmentations without enhancing the original feature representation, unlike in previous work (Hamilton et al.,\n2022; Seong et al., 2023).\nFrom K-means to PriMaPs-EM.\nWhen examining the K-means baseline as well as state-of-the-art\nmethods (Hamilton et al., 2022; Seong et al., 2023), see Fig. 4, it can be qualitatively observed that more\nlocal consistency within the respective predictions would already lead to less mis-classification. We take\ninspiration from (Drineas et al., 2004; Ding & He, 2004), who showed that the PCA subspace, spanned by\nprincipal components, is a relaxed solution to K-means clustering. We observe that principal components\nhave high semantic correlation for object- as well as scene-centric image features (cf. Fig. 1). We utilize this\nby iteratively partitioning images based on dominant feature patterns, identified by means of the cosine simi-\nlarity of the image features to the respective first principal component. We name the resulting class-agnostic\nimage decomposition PriMaPs – Principal Mask Proposals. PriMaPs stem directly from SSL representa-\ntions and guide the process of unsupervised semantic segmentation. Shown in Fig. 3, our optimization-based\napproach, PriMaPs-EM, operates over an SSL feature representation computed from a frozen deep neural\nnetwork backbone. The optimization realizes stochastic EM of a clustering objective guided by PriMaPs.\nSpecifically, PriMaPs-EM fits class prototypes to the proposals in a globally consistent manner by optimizing\nover two identically sized vector sets, with one of them being an exponential moving average (EMA) of the\nother. We show that PriMaPs-EM enables accurate unsupervised partitioning of images into semantically\nmeaningful regions while being comparatively lightweight and orthogonal to most previous approaches to\nunsupervised semantic segmentation.\n3.1\nDeriving PriMaPs\nWe start with a frozen pre-trained self-supervised backbone model F : R3×h×w →RC×H×W , which embeds\nan image I ∈R3×h×w into a dense feature representation f ∈RC×H×W as\nf = F(I) .\n(1)\nHere, C refers to the channel dimension of the dense features, and H = h/p, W = w/p with p corresponding\nto the output stride of the backbone. Based on this image representation, the next step is to decompose the\nimage into semantically meaningful masks to provide a local grouping prior for fitting global class prototypes.\nInitial principal mask proposal.\nTo identify the initial principal mask proposal in an image I, we\nanalyze the spatial statistical correlations of its features by means of PCA. Specifically, we consider the\nempirical feature covariance matrix\nΣ =\n1\nHW\nH\nX\ni=1\nW\nX\nj=1\n\u0000f:,i,j −¯f\n\u0001\u0000f:,i,j −¯f\n\u0001⊤,\n(2)\nwhere f:,i,j ∈RC are the features at position (i, j) and ¯f ∈RC is the mean feature. To identify the feature\ndirection that captures the largest variance in the feature distribution, we seek the first principal component\nof Σ by solving\nΣv = λv .\n(3)\nWe obtain the first principal component as the eigenvector v1 to the largest eigenvalue λ1, which can be\ncomputed efficiently with Singular Value Decomposition (SVD) using the flattened features f.\nTo identify a candidate region, our next goal is to compute a spatial feature similarity map to the dominant\nfeature direction.\nWe observe that doing so directly with the principal direction does not always lead\n4\nPublished in Transactions on Machine Learning Research (09/2024)\nImage I\nFeatures f Map M 1\nMask P 1\nFeatures f 2 Map M 2\nMask P 2\nFeatures f 3 Map M 3\nMask P 3\nPriMaPs (all)\n...\nFigure 2: PriMaPs process. Given the dense feature embeddings f of an image I, we compute the cosine-\nsimilarity map M 1 of all features f to their first principal component’s nearest neighbor feature. The first\nPriMaP P 1 is obtained by thresholding M 1. To obtain P 2, the features assigned to P 1 are masked out, and\nthe process is repeated with the remaining features f 2. We repeat the PriMaPs process until the majority\nof features have been assigned to masks. Finally, all masks P are upsampled and refined using a CRF.\nto sufficiently good localization, i. e., high similarities arise across multiple visual concepts in an image,\nelaborated in more detail in Appendix A.1.\nThis can be circumvented by first anchoring the dominant\nfeature vector in the feature map. To that end, we obtain the nearest neighbor feature ˜f ∈RC of the first\nprincipal component v1 by considering the cosine distance in the normalized feature space ˆf as\n˜f = ˆf:,m,n ,\nwhere\n(m, n) = arg max\ni,j\n\u0000v⊤\n1 ˆf\n\u0001\n.\n(4)\nGiven this, we compute the cosine-similarity map M 1 ∈RH×W of the dominant feature w. r. t. all features\nas\nM 1 = (Mi,j)i,j ,\nwhere\nMi,j =\n\u0000 ˜f\n\u0001⊤ˆf:,i,j .\n(5)\nNext, a threshold ψ ∈(0, 1) is applied to the similarity map in order to suppress noise and further localize\nthe initial mask. Accordingly, elements of a binary similarity map P 1 ∈{0, 1}H×W are set to 1 when the\nsimilarity is larger than a fraction ψ of the maximal similarity, and 0 otherwise, i. e.,\nP 1 =\nh\nM 1\ni,j > ψ · max\nm,n M 1\nm,n\ni\ni,j ,\n(6)\nwhere [·] denotes the Iverson bracket. This binary principal mask P 1 gives rise to the first principal mask\nproposal in image I.\nFurther principal mask proposals.\nSubsequent mask proposals result from iteratively repeating the\ndescribed procedure. To that end, it is necessary to suppress features that have already been assigned to a\npseudo label. Specifically in iteration z, given the mask proposals P s, s = 1, . . . , z −1, extracted in previous\niterations, we mask out the features that have already been considered as\nf z\n:,i,j = f:,i,j\n\u0014Xz−1\ns=1 P s\ni,j = 0\n\u0015\n.\n(7)\nApplying Eqs. (2) to (6) on top of the masked features f z yields the cosine-similarity map M z and the\nprincipal mask proposal P z, and so on. We repeat this procedure until the majority of features (e. g., 95%)\nhave been assigned to a mask. In a final step, the remaining features, in case there are any, are assigned to\nan “ignore” mask\nP 0\ni,j = 1 −\nZ−1\nX\nz=1\nP z\ni,j .\n(8)\nThis produces a tensor P ∈{0, 1}Z×H×W of Z spatial similarity masks, decomposing a single image into Z\nnon-overlapping regions.\nProposal post-processing.\nTo further improve the alignment of the masks with edges and color-correlated\nregions in the image, a fully connected Conditional Random Field (CRF) with Gaussian edge potentials\n(Krähenbühl & Koltun, 2011) is applied to the initial mask proposals P (after bilinear upsampling to the\nimage resolution) for 10 inference iterations. The process for obtaining PriMaPs is visualized in Fig. 2.\nPriMaPs pseudo-label generation.\nIn order to form a pseudo label for semantic segmentation out of\nthe Z class-agnostic mask proposals, each mask has to be assigned one out of K pseudo-class labels. This\n5\nPublished in Transactions on Machine Learning Research (09/2024)\nImage I\nAug. Image I′\nF\n_\nF\n_\nf\nθM\nf ′\nθR\n·\n·\ny\ny′\nPriMaPs\nPseudo Label\nGeneration\nP ∗\nLfocal\nEMA\n(a) PriMaPs-EM architecture\nFeatures f\nImage I\nlabel P ∗\nPred. y\nPseudo\n1st PC NN\nSimilarity Map\nFeat.\nMask.\nID Assign\nCRF\nP\n(b) PriMaPs pseudo-label generation\nFigure 3: (a) PriMaPs-EM architecture. An image I and its augmented version I′ are embedded by the\nfrozen self-supervised backbone F, resulting in the dense features f and f ′. The segmentation prediction y\nby the momentum class prototypes θM arises via the dot product with f. Likewise, y′ arises from the dot\nproduct of the running class prototypes θR with f ′. Pseudo labels P ∗are constructed using PriMaPs, I,\nand y. We use the pseudo labels to optimize θR, applying a focal loss. θR is gradually transferred to θM by\nmeans of an EMA. (b) PriMaPs pseudo-label generation. Masks P are proposed by iterative binary\npartitioning based on the cosine similarity of the features of any unassigned pixel to their first principal\ncomponent’s nearest neighbor feature. Gray indicates these iterative steps. Next, the masks P are aligned\nto the image I using a CRF. Finally, a per-mask pseudo-class ID is assigned using majority voting based on\nthe segmentation prediction y, resulting in the pseudo label P ∗.\nis accomplished using a segmentation prediction of our optimization process, called PriMaPs-EM, detailed\nin Sec. 3.2.\nGiven a segmentation prediction y, we assign the pseudo-class ID that is most frequently\npredicted within each proposal, yielding the final pseudo-label map P ∗∈{0, 1}K×h×w, a one-hot encoding\nof a pseudo-class ID. The entire PriMaPs pseudo-label generation process is illustrated in Figure 3b.\n3.2\nPriMaPs-EM\nShown in Fig. 3, PriMaPs-EM is an iterative optimization technique for fitting class prototypes for semantic\nsegmentation across a dataset utilizing pseudo labels P ∗based on PriMaPs (cf. Sec. 3.1). PriMaPs pro-\nvide guidance through local per-image consistency for fitting the global class prototypes across a dataset.\nPriMaPs-EM leverages a frozen pre-trained self-supervised backbone model F and optimizes two identically\nsized vector sets, the running class prototypes θR and their moving average, the momentum class prototypes\nθM. The class prototypes θR and θM are the K pseudo-class representations in the feature space, project-\ning the C-dimensional features linearly to K semantic pseudo classes, which equates to segmenting. More\nprecisely, we use the segmentation prediction of the momentum class prototypes θM to assign consistent se-\nmantic pseudo-class IDs to the class-agnostic PriMaPs. We optimize the running class prototypes θR using\nthe pseudo labels P ∗and gradually transfer θR to θM using an EMA.\nPriMaPs-EM performs the optimization in two stages, since in our case, a meaningful initialization of the\nclass prototypes is vital to provide a reasonable optimization signal. This can be traced back to the pseudo-\nlabel generation, which utilizes a segmentation prediction to assign globally consistent classes to the masks.\nInitializing the class prototypes randomly leads to a highly unstable and noisy signal.\nInitialization.\nWe initialize the class prototypes θM with the first K principal components using vanilla\nPCA computed over the feature embeddings of a large number of images across the respective dataset. Next,\na cosine-distance batch-wise K-means (MacQueen, 1967) loss\nLK-means(θM) = −\nX\ni,j\nmax\n\u0000θ⊤\nMf:,i,j\n\u0001\n(9)\nis minimized with respect to θM for a fixed number of epochs.\nThis minimizes the cumulative cosine\ndistances of the image features f:,i,j to their respective closest class prototype. θR is initialized with the\nsame prototypes.\n6\nPublished in Transactions on Machine Learning Research (09/2024)\nMoving average stochastic EM.\nVisualized in Fig. 3a, every optimization iteration starts with com-\nputing the segmentation prediction y by the dot product of the dense image features f of the image I with\nthe momentum class prototypes θM. Applying the arg max results in the dominant prototype for each fea-\nture location and, accordingly, in prototype-based binary masks y ∈{0, 1}K×h×w. Note that we bilinearly\nupsample the features to the image resolution before the dot product. Using y, f, and I, we get PriMaPs\npseudo labels P ∗as described in Sec. 3.1. We obtain the semantic prediction y′ by the dot product of the\nrunning class prototypes θR with the dense image features f ′ of the augmented image I′. θR is optimized by\napplying a batch-wise focal loss (Lin et al., 2020) with respect to the pseudo labels P ∗. The focal loss Lfocal\nis a weighted version of the cross-entropy loss, increasing the loss contribution of less confident classes, i. e.,\nLfocal(θR; y′) = −\nX\nk,i,j\n(1 −χk)2P ∗\nk,i,j log(y′\nk,i,j) ,\n(10)\nwhere y′\nk,i,j = softmax(θ⊤\nRf ′\n:,i,j) are the predictions and χk is the class-wise confidence value approximated\nby averaging y′\nk,:,: spatially. The running class prototypes θR are optimized with an augmented input image\nI′.\nWe employ photometric augmentations (Gaussian blur, grayscaling, and color jitter), introducing a\ncontrolled noise, thereby strengthening the robustness of our class representation. The momentum class\nprototypes θM are the EMA of the running class prototypes θR. This is utilized in order to stabilize the\noptimization, accounting for the noisy nature of the unsupervised signal used for optimization. We update\nθM every κ iterations with a decay γ as\nθt+κ\nM\n= γθt\nM + (1 −γ)θt+κ\nR\n,\n(11)\nwhere t is the iteration index of the previous update. This optimization approach resembles moving average\nstochastic EM. Hereby, the E-step amounts to finding pseudo labels using PriMaPs and the momentum\nclass prototypes. The M-step optimizes the running class prototypes with respect to their focal loss Lfocal.\nStochasticity arises from performing EM in mini-batches of images.\nInference.\nAt inference time, we obtain the segmentation prediction y via the momentum class proto-\ntypes θM, refined using a fully connected CRF with Gaussian edge potentials (Krähenbühl & Koltun, 2011)\nfollowing previous approaches (Van Gansbeke et al., 2021; Hamilton et al., 2022; Seong et al., 2023). This\nis the identical CRF as already used for refining the masks in the PriMaPs pseudo-label generation using\nthe same CRF parameters as previous work (Van Gansbeke et al., 2021; Hamilton et al., 2022; Seong et al.,\n2023).\n4\nExperiments\nTo assess the efficacy of our approach, we compare it to the current state-of-the-art in unsupervised semantic\nsegmentation. For a fair comparison, we closely follow the overall setup used by numerous previous works\n(Ji et al., 2019; Cho et al., 2021; Hamilton et al., 2022; Seong et al., 2023).\n4.1\nExperimental Setup\nDatasets.\nFollowing the practice of previous work, we conduct experiments on Cityscapes (Cordts et al.,\n2016), COCO-Stuff (Caesar et al., 2018), and Potsdam-3 (ISPRS). Cityscapes and COCO-Stuff are evaluated\nusing 27 classes, while Potsdam is evaluated on the 3-class variant. Adopting the established evaluation\nprotocol (Ji et al., 2019; Cho et al., 2021; Hamilton et al., 2022; Seong et al., 2023), we resize images to\n320 pixels along the smaller axis and crop the center 320 × 320 pixels. This is adjusted to 322 pixels for\nDINOv2. Different from previous work, we apply this simple scheme throughout this work, thus dispensing\nwith elaborate multi-crop approaches of previous methods (Hamilton et al., 2022; Yin et al., 2022; Seong\net al., 2023).\nSelf-supervised backbone.\nExperiments are conducted across a collection of pre-trained self-supervised\nfeature embeddings: DINO (Caron et al., 2021) based on ViT-Small and ViT-Base using 8 × 8 patches; and\nDINOv2 (Oquab et al., 2024) based on ViT-Small and ViT-Base using 14 × 14 patches. In the spirit of SSL\nprinciples, we keep the backbone parameters frozen throughout the experiments. We use the output from\nthe last network layer as our SSL feature embeddings. Since PriMaPs-EM is agnostic to the used embedding\n7\nPublished in Transactions on Machine Learning Research (09/2024)\nTable 1: Cityscapes – PriMaPs-EM (Ours) comparison to existing unsupervised semantic seg-\nmentation methods, using Accuracy and mean IoU (in %) for unsupervised and supervised probing.\nDouble citations refer to a method’s origin and the work conducting the experiment.\nMethod\nBackbone\nUnsupervised\nSupervised\nAcc\nmIoU\nAcc\nmIoU\nIIC (Ji et al., 2019; Cho et al., 2021)\n47.9\n6.4\n–\n–\nMDC (Caron et al., 2018; Cho et al., 2021)\n40.7\n7.1\n–\n–\nPiCIE (Cho et al., 2021)\n65.5\n12.3\n–\n–\nVICE (Karlsson et al., 2022)\nResNet18\n+FPN\n31.9\n12.8\n86.3\n31.6\nBaseline (Caron et al., 2021)\n61.4\n15.8\n91.0\n35.4\n+ TransFGU (Yin et al., 2022)\n77.9\n16.8\n–\n–\n+ HP (Seong et al., 2023)\n80.1\n18.4\n91.2\n30.6\n+ PriMaPs-EM\n81.2\n19.4\n91.0\n35.4\n+ HP (Seong et al., 2023) + PriMaPs-EM\nDINO\nViT-S/8\n76.6\n19.2\n91.2\n30.6\nBaseline (Caron et al., 2021)\n49.2\n15.5\n91.6\n35.9\n+ STEGO (Hamilton et al., 2022; Koenig et al., 2023)\n73.2\n21.0\n89.6\n28.0\n+ HP (Seong et al., 2023)\n79.5\n18.4\n90.9\n33.0\n+ PriMaPs-EM\n59.6\n17.6\n91.6\n35.9\n+ STEGO (Hamilton et al., 2022) + PriMaPs-EM\nDINO\nViT-B/8\n78.6\n21.6\n89.6\n28.0\nBaseline (Oquab et al., 2024)\n49.5\n15.3\n90.8\n41.9\n+ PriMaPs-EM\nDINOv2\nViT-S/14\n71.5\n19.0\n90.8\n41.9\nBaseline (Oquab et al., 2024)\n36.1\n14.9\n91.0\n44.8\n+ PriMaPs-EM\nDINOv2\nViT-B/14\n82.9\n21.3\n91.0\n44.8\nspace, we can also apply it on top of current state-of-the-art unsupervised segmentation pipelines. Here, we\nconsider STEGO (Hamilton et al., 2022) and HP (Seong et al., 2023), which also use DINO features but\nlearn a target domain-specific subspace.\nBaseline.\nFollowing (Hamilton et al., 2022; Seong et al., 2023), we train a single linear layer as a baseline\nwith the same structure as θR and θM by minimizing the cosine-distance batch-wise K-Means loss from\nEq. (9). Hereby, parameters, such as the number of epochs and the learning rate, are identical to those used\nwhen employing PriMaPs-EM.\nPriMaPs-EM.\nAs discussed in Sec. 3.2, the momentum class prototypes θM are initialized using the first\nK principal components; we use 2975 images for PCA, as this is the largest number of training images shared\nby all datasets. Next, θM is pre-trained by minimizing Eq. (9) using Adam (Kingma & Ba, 2015). We use\na learning rate of 0.005 for 2 epochs on all datasets and backbones. The weights are then copied to θR. For\nfitting the running class prototypes using EM, θR is optimized by minimizing the focal loss from Eq. (10)\nwith Adam (Kingma & Ba, 2015) using a learning rate of 0.005. The momentum class prototypes θM are\nupdated using an EMA according to Eq. (11) every γs = 10 steps with decay γψ = 0.98. We set the PriMaPs\nmask-proposal threshold to ψ = 0.4 and provide detailed ablation experiments in Appendix A.2. We use a\nbatch size of 32 for 50 epochs on Cityscapes and Potsdam-3, and use 5 epochs on COCO-Stuff due to its\nlarger size. Importantly, the same hyperparameters are used across all datasets and backbones. Moreover,\nnote that fitting class prototypes with PriMaPs-EM is quite practical, e. g., about 2 hours on Cityscapes.\nExperiments are conducted on a single NVIDIA A6000 GPU.\nSupervised upper bounds.\nTo assess the potential of the SSL features used, we report supervised upper\nbounds. Specifically, we train a linear layer using cross entropy and Adam with a learning rate of 0.005.\nSince PriMaPs-EM uses frozen SSL features, its supervised bound is the same as that of the underlying\nfeatures. This is not the case, however, for prior work (Hamilton et al., 2022; Seong et al., 2023), which\nproject the feature representation affecting the upper bound.\nEvaluation.\nFor inference, we use the prediction from the momentum class prototypes θM. CRF refinement\nuses 10 inference iterations and standard parameters a = 4, b = 3, θα = 67, θβ = 3, θγ = 1 from prior work\n8\nPublished in Transactions on Machine Learning Research (09/2024)\nTable 2: COCO-Stuff – PriMaPs-EM (Ours) comparison to existing unsupervised semantic\nsegmentation methods, using Accuracy and mean IoU (in %) for unsupervised and supervised probing.\nDouble citations refer to a method’s origin and the work conducting the experiment.\nMethod\nBackbone\nUnsupervised\nSupervised\nAcc\nmIoU\nAcc\nmIoU\nIIC (Ji et al., 2019; Cho et al., 2021)\n21.8\n6.7\n44.5\n8.4\nMDC (Caron et al., 2018; Cho et al., 2021)\n32.2\n9.8\n48.6\n13.3\nPiCIE (Cho et al., 2021)\n48.1\n13.8\n54.2\n13.9\nPiCIE+H (Cho et al., 2021)\n50.0\n14.4\n54.8\n14.8\nVICE (Karlsson et al., 2022)\nResNet18\n+FPN\n28.9\n11.4\n62.8\n25.5\nBaseline (Caron et al., 2021)\n34.2\n9.5\n72.0\n41.3\n+ TransFGU (Yin et al., 2022)\n52.7\n17.5\n–\n–\n+ STEGO (Hamilton et al., 2022)\n48.3\n24.5\n74.4\n38.3\n+ ACSeg (Li et al., 2023)\n–\n16.4\n–\n–\n+ HP (Seong et al., 2023)\n57.2\n24.6\n75.6\n42.7\n+ PriMaPs-EM\n46.5\n16.4\n72.0\n41.3\n+ HP (Seong et al., 2023) + PriMaPs-EM\nDINO\nViT-S/8\n57.8\n25.1\n75.6\n42.7\nBaseline (Caron et al., 2021)\n38.8\n15.7\n74.0\n44.6\n+ STEGO (Hamilton et al., 2022)\n56.9\n28.2\n76.1\n41.0\n+ PriMaPs-EM\n48.5\n21.9\n74.0\n44.6\n+ STEGO (Hamilton et al., 2022) + PriMaPs-EM\nDINO\nViT-B/8\n57.9\n29.7\n76.1\n41.0\nBaseline (Oquab et al., 2024)\n44.5\n22.9\n77.9\n52.8\n+ PriMaPs-EM\nDINOv2\nViT-S/14\n46.5\n23.8\n77.9\n52.8\nBaseline (Oquab et al., 2024)\n35.0\n17.9\n77.3\n53.7\n+ PriMaPs-EM\nDINOv2\nViT-B/14\n52.8\n23.6\n77.3\n53.7\n(Van Gansbeke et al., 2021; Hamilton et al., 2022; Seong et al., 2023). We evaluate common metrics in\nunsupervised semantic segmentation, specifically the mean Intersection over Union (mIoU) and Accuracy\n(Acc) over all classes after aligning the predicted class IDs with ground-truth labels by means of Hungarian\nmatching (Kuhn, 1955).\nSotA + PriMaPs-EM.\nTo explore our method’s potential, we additionally employ PriMaPs-EM on top\nof STEGO (Hamilton et al., 2022) and HP (Seong et al., 2023). For each backbone-dataset combination,\nwe apply it on top of the best previous method in terms of mIoU. To that end, the training signal for\nlearning the feature projection of (Hamilton et al., 2022; Seong et al., 2023) remains unchanged. We apply\nPriMaPs-EM fully orthogonally, using the DINO backbone features for pseudo-label generation and fit a\ndirect connection between the feature space of the state-of-the-art method and the prediction space.\n4.2\nResults\nWe compare PriMaPs-EM against prior work for unsupervised semantic segmentation (Ji et al., 2019; Cho\net al., 2021; Hamilton et al., 2022; Yin et al., 2022; Li et al., 2023; Seong et al., 2023). As in previous\nwork, we use DINO (Caron et al., 2021) as the main baseline.\nAdditionally, we also test PriMaPs-EM\non top of DINOv2 (Oquab et al., 2024), STEGO (Hamilton et al., 2022), and HP (Seong et al., 2023).\nOverall, we observe that the DINO baseline already achieves strong results (cf. Tabs. 1 to 3). DINOv2\nfeatures significantly raise the supervised upper bounds in terms of Acc and mIoU, the improvement in\nthe unsupervised case remains more modest. Nevertheless, PriMaPs-EM further boosts the unsupervised\nsegmentation performance.\nIn Tab. 1, we compare to previous work on the Cityscapes dataset.\nPriMaPs-EM leads to a consistent\nimprovement over all baselines in terms of unsupervised segmentation accuracy. For example, PriMaPs-\nEM boosts DINO ViT-S/8 by +3.6% and +19.8% in terms of mIoU and Acc, respectively, which leads to\nstate-of-the-art performance. Notably, we find PriMaPs-EM to be complementary to other state-of-the-art\nunsupervised segmentation methods like STEGO (Hamilton et al., 2022) and HP (Seong et al., 2023) on the\n9\nPublished in Transactions on Machine Learning Research (09/2024)\nTable 3: Potsdam-3 – PriMaPs-EM (Ours) comparison to existing unsupervised semantic seg-\nmentation methods, using Accuracy and mean IoU (in %) for unsupervised and supervised probing.\nDouble citations refer to a method’s origin and the work conducting the experiment.\nMethod\nBackbone\nUnsupervised\nSupervised\nAcc\nmIoU\nAcc\nmIoU\nRandomCNN (Cho et al., 2021)\n38.2\n–\n–\n–\nK-Means (Pedregosa et al., 2011; Cho et al., 2021)\n45.7\n–\n–\n–\nSIFT (Lowe, 2004; Cho et al., 2021)\n38.2\n–\n–\n–\nContextPrediction (Doersch et al., 2015; Cho et al., 2021)\n49.6\n–\n–\n–\nCC (Isola et al., 2015; Cho et al., 2021)\n63.9\n–\n–\n–\nDeepCluster (Caron et al., 2018; Cho et al., 2021)\n41.7\n–\n–\n–\nIIC (Ji et al., 2019; Cho et al., 2021)\nVGG\n11\n65.1\n–\n–\n–\nBaseline (Caron et al., 2021)\n56.6\n33.6\n82.0\n69.0\n+ STEGO (Hamilton et al., 2022; Koenig et al., 2023)\n77.0\n62.6\n85.9\n74.8\n+ PriMaPs-EM\n62.5\n38.9\n82.0\n69.0\n+ STEGO (Hamilton et al., 2022) + PriMaPs-EM\nDINO\nViT-S/8\n78.4\n64.2\n85.9\n74.8\nBaseline (Caron et al., 2021)\n66.1\n49.4\n84.3\n72.8\n+ HP (Seong et al., 2023)\n82.4\n69.1\n88.0\n78.4\n+ PriMaPs-EM\n80.5\n67.0\n84.3\n72.8\n+ HP (Seong et al., 2023)+ PriMaPs-EM\nDINO\nViT-B/8\n83.3\n71.0\n88.0\n78.4\nBaseline (Oquab et al., 2024)\n75.9\n61.0\n86.6\n76.2\n+ PriMaPs-EM\nDINOv2\nViT-S/14\n78.5\n64.3\n86.6\n76.2\nBaseline (Oquab et al., 2024)\n82.4\n69.9\n87.9\n78.3\n+ PriMaPs-EM\nDINOv2\nViT-B/14\n83.2\n71.1\n87.9\n78.3\ncorresponding backbone model. This suggests that these methods use their SSL representation only to a\nlimited extent and do not fully leverage the inherent properties of the underlying SSL embeddings. Similar\nobservations can be drawn for the experiments on COCO-Stuff in Tab. 2. PriMaPs-EM leads to a consistent\nimprovement across all four SSL baselines, as well as an improvement over STEGO and HP. For instance,\ncombining STEGO with PriMaPs-EM leads to +14.0% and +19.1% improvement over the baseline in terms\nof mIoU and Acc for DINO ViT-B/8. Experiments on the Potsdam-3 dataset follow the same pattern (cf.\nTab. 3). PriMaPs-EM leads to a consistent gain over the baseline, e. g. +17.6% and +14.4% in terms of\nmIoU and Acc, respectively, for DINO ViT-B/8. Moreover, it also boosts the accuracy of STEGO and HP. In\nsome cases, the gain of PriMaPs-EM is limited. For example, in Tab. 1 for DINO ViT-B/8 + PriMaPs-EM,\nthe class prototype for “sidewalk” is poor while the classes “road” and “vegetation” superimpose smaller\nobjects. For DINO ViT-S/8 + PriMaPs-EM in Tab. 3, the class prototype “road” is poor. This limits the\noverall performance of our method while still outperforming the respective baseline in both cases.\nOverall, PriMaPs-EM provides modest but consistent benefits over a wide range of baselines and datasets and\nreaches competitive segmentation performance w. r. t. the state-of-the-art using identical hyperparameters\nacross all backbones and datasets. Recalling the simplicity of the techniques behind PriMaPs, we believe\nthat this is a significant result. The complementary effect of PriMaPs-EM on other state-of-the-art methods\n(STEGO, HP) further suggests that they rely on DINO features for mere “bootstrapping” and learn feature\nrepresentations with orthogonal properties to those of DINO. We conclude that PriMaPs-EM constitutes a\nstraightforward, entirely orthogonal tool for boosting unsupervised semantic segmentation.\n4.3\nAblation Study\nTo untangle the factors behind PriMaPs-EM, we examine the individual components in a variety of ablation\nexperiments to access the contribution.\nPriMaPs pseudo-label ablations.\nIn Tab. 4a, we analyze the contribution of the individual sub-steps\nfor PriMaPs pseudo-label generation by increasing the complexity of label generation. We provide the DINO\n10\nPublished in Transactions on Machine Learning Research (09/2024)\nTable 4: Ablation study analyzing design choices and components in the PriMaPs pseudo-label generation\n(a) and PriMaPs-EM (b) for COCO-Stuff using DINO ViT-B/8.\n(a) PriMaPs pseudo-label ablation\nMethod\nAcc\nmIoU\nBaseline (Caron et al., 2021)\n38.8\n15.7\nSimilarity Masks\n46.3\n19.8\n+ NN\n44.9\n20.0\n+ P-CRF (≡PriMaPs-EM)\n48.4\n21.9\nPriMaPs-EM(non-iter. PC)\n47.9\n21.7\n(b) PriMaPs-EM ablation\nMethod\nAcc\nmIoU\nBaseline (Caron et al., 2021)\n38.8\n15.7\n+ PriMaPs pseudo label\n38.8\n18.0\n+ EMA\n45.0\n20.2\n+ Augment\n46.0\n20.4\n+ CRF (≡PriMaPs-EM)\n48.4\n21.9\nTable 5: Oracle quality assessment of PriMaPs pseudo labels for Cityscapes, COCO-Stuff, and\nPotsdam-3 by assigning oracle class IDs to the masks. “Pseudo” refers to evaluating only the pixels contained\nin the pseudo label, “All” to evaluating including the “ignore” assignments of the pseudo label.\nMethod\nCityscapes\nCOCO-Stuff\nPotsdam-3\nAcc\nmIoU\nAcc\nmIoU\nAcc\nmIoU\nPseudo\n92.4\n54.0\n93.4\n82.4\n95.2\n90.9\nAll\n73.2\n32.4\n74.1\n55.9\n67.4\n48.9\nDINO ViT-B/8 Baseline (Caron et al., 2021)\n49.2\n15.5\n38.8\n15.7\n66.1\n49.4\nbaseline, which corresponds to K-means feature clustering, for reference. In the most simplified case, we\ndirectly use the similarity mask, similar to Eq. (4). Next, we use the nearest neighbor (+NN in Tab. 4a) of\nthe principal component to get the masks as in Eq. (5), followed by the full approach with CRF refinement\n(+P-CRF). Except for the changes in the pseudo-label generation, the optimization remains as described in\nSec. 4.1. We observe that the similarity masks already provide a good staring point, yet we identify a gain\nfrom every single component step. This suggests that using the nearest neighbor improves the localization\nof the similarity mask. Similarly, CRF refinement improves the alignment between the masks and the image\ncontent. We also experiment with using the respective next principal component (non-iter. PC) instead of\niteratively extracting the first principal component from the masked features. This leads to slightly inferior\nresults. Naively using the K leading eigenvectors and simply assigning the masks based on the arg max of\ntheir cosine similarity to the features without any iterations would lead to significantly worse results with a\npixel Accuracy of 43.1 % and a mIoU of 19.9 %. Note that nearest neighbor anchoring and thresholding are\nused in both experiments. Additionally, we ablate the nearest neighbor anchoring, the threshold ψ, and the\nstop criterion in Appendix A.\nPriMaPs-EM architecture ablations.\nIn a similar vein, we analyze the contribution of the different\narchitectural components of PriMaPs-EM. Optimizing over a single set of class prototypes using the proposed\nPriMaPs pseudo labels already provides moderate improvement (+PriMaPs pseudo label in Tab. 4b), despite\nthe disadvantage of an unstable and noisy optimization. Adding the EMA (+EMA) leads to a more stable\noptimization and further improved segmentation. Augmenting the input (+Augment) results in a further\ngradual improvement.\nWe provide a more detailed breakdown of the augmentations in Appendix A.4.\nSimilarly, refining the prediction with a CRF improves the results further (+CRF).\nAssessing PriMaPs pseudo labels.\nTo estimate the quality of the pseudo labels, respectively the\nprincipal masks, we decouple those from the class ID assignment by providing the oracle ground-truth class\nfor each mask in Tab. 5. To that end, we evaluate all pixels included in our pseudo labels (“Pseudo”),\ncorresponding to the upper bound of our optimization signal. Furthermore, we evaluate “All” by assigning\nthe “ignore” pixels to a wrong class. The results indicate a high quality of the pseudo-label maps. We show\nqualitative examples of the PriMaPs mask proposals and pseudo labels in Appendix B.5.\nQualitative results.\nWe show qualitative results for Cityscapes, COCO-Stuff, and Potsdam-3 in Fig. 4.\nWe observe that PriMaPs-EM leads to less noisy results compared to the baseline, showcasing an improved\n11\nPublished in Transactions on Machine Learning Research (09/2024)\nCityscapes\nCOCO-Stuff\nPotsdam-3\nSTEGO +\nPriMaPs-EM\nSTEGO\nPriMaPs-EM\nBaseline\nGround Truth\nImage\nFigure 4: Qualitative results for the DINO ViT-B/8 baseline, PriMaPs-EM (Ours), STEGO (Hamilton\net al., 2022), and STEGO+PriMaPs-EM (Ours) for Cityscapes, COCO-Stuff, and Potsdam-3. Our method\nproduces locally more consistent segmentation results reducing overall misclassification compared to the\ncorresponding baseline.\nlocal consistency of the segmentation and reduced mis-classification. The comparison with STEGO as a base-\nline exhibits a similar trend. For further examples and comparisons with HP, please refer to Appendix B.2.\nLimitations.\nOne of the main challenges is to distinguish between classes that happen to share highly\nsimilar SSL feature representations. This is hardly avoidable if the feature representation is fixed, as was\nthe case here and in previous work (Hamilton et al., 2022; Seong et al., 2023). Another limitation across\nexisting unsupervised semantic segmentation approaches is the limited spatial resolution. This limitation\ncomes from the SSL training objectives (Caron et al., 2021; Oquab et al., 2024), which are image-level rather\nthan pixel-level. As a result, we can observe difficulties in segmenting very small, finely resolved structures.\n5\nConclusion\nWe present PriMaPs, a novel dense pseudo-label generation approach for unsupervised semantic segmen-\ntation. We derive lightweight mask proposals directly from off-the-shelf self-supervised learned features,\nleveraging the intrinsic properties of their embedding space. Our mask proposals can be used as pseudo la-\nbels to effectively fit global class prototypes using moving average stochastic EM with PriMaPs-EM. Despite\nthe simplicity, PriMaPs-EM leads to a consistent boost in unsupervised segmentation accuracy when applied\nto a variety of SSL features or orthogonally to current state-of-the-art unsupervised semantic segmentation\npipelines, as shown by our results across multiple datasets.\n12\nPublished in Transactions on Machine Learning Research (09/2024)\nAcknowledgments\nThis project is partially funded by the European Research Council (ERC) under the European Union’s\nHorizon 2020 research and innovation programme (grant agreement No. 866008) as well as the State of\nHesse (Germany) through the cluster projects “The Third Wave of Artificial Intelligence (3AI)” and “The\nAdaptive Mind (TAM)”.\nReferences\nShir Amir, Yossi Gandelsman, Shai Bagon, and Tali Dekel. Deep ViT features as dense visual descriptors.\nIn ECCVW, volume 13804, pp. 39–55, 2022.\nNikita Araslanov and Stefan Roth. Single-stage semantic segmentation from image labels. In CVPR, pp.\n4253–4262, 2020.\nPablo Arbelaez, Michael Maire, Charless Fowlkes, and Jitendra Malik. Contour detection and hierarchical\nimage segmentation. IEEE T. Pattern Anal. Mach. Intell., 33(5):898–916, 2011.\nRishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ B. Altman, and Simran Arora et al. On the oppor-\ntunities and risks of foundation models. arXiv:2108.07258 [cs.LG], 2021.\nHolger Caesar, Jasper Uijlings, and Vittorio Ferrari. COCO-Stuff: Thing and stuff classes in context. In\nCVPR, pp. 1209–1218, 2018.\nMathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsupervised\nlearning of visual features. In ECCV, volume 11218, pp. 132–149, 2018.\nMathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsu-\npervised learning of visual features by contrasting cluster assignments. In NeurIPS*2020, volume 33, pp.\n9912–9924, 2020.\nMathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand\nJoulin. Emerging properties in self-supervised vision transformers. In ICCV, pp. 9650–9660, 2021.\nJianfei Chen, Jun Zhu, Yee Whye Teh, and Tong Zhang. Stochastic expectation maximization with variance\nreduction. In NeurIPS*2018, volume 31, pp. 7967–7977, 2018a.\nLiang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L. Yuille. DeepLab:\nSemantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs.\nIEEE T. Pattern Anal. Mach. Intell., 40(4):834–848, 2018b.\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive\nlearning of visual representations. In ICML, pp. 1597–1607, 2020.\nJang Hyun Cho, Utkarsh Mall, Kavita Bala, and Bharath Hariharan. PiCIE: Unsupervised semantic seg-\nmentation using invariance and equivariance in clustering. In CVPR, pp. 16794–16804, 2021.\nMarius Cordts, Mohamed Omran, Sebastian Ramos, Timo Scharwächter, Markus Enzweiler, Rodrigo Be-\nnenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The Cityscapes dataset for semantic urban scene\nunderstanding. In CVPR, pp. 3213–3223, 2016.\nJifeng Dai, Kaiming He, and Jian Sun.\nBoxSup: Exploiting bounding boxes to supervise convolutional\nnetworks for semantic segmentation. In ICCV, pp. 1635–1643, 2015.\nChris Ding and Xiaofeng He. K-means clustering via principal component analysis. In ICML, 2004.\nCarl Doersch, Abhinav Kumar Gupta, and Alexei A. Efros. Unsupervised visual representation learning by\ncontext prediction. In ICCV, pp. 1422–1430, 2015.\n13\nPublished in Transactions on Machine Learning Research (09/2024)\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Un-\nterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil\nHoulsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2020.\nPetros Drineas, Alan M. Frieze, Ravi Kannan, Santosh S. Vempala, and V. Vinay. Clustering large graphs\nvia the singular value decomposition. Machine Learning, 56(1-3):9–33, 2004.\nLinus Ericsson, Henry Gouk, and Timothy M. Hospedales. How well do self-supervised models transfer? In\nCVPR, pp. 5414–5423, 2021.\nSpyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by predicting\nimage rotations. In ICLR, 2018.\nJean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena Buchatskaya,\nCarl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your\nown latent – A new approach to self-supervised learning. In NeurIPS*2020, volume 33, pp. 21271–21284,\n2020.\nMark Hamilton, Zhoutong Zhang, Bharath Hariharan, Noah Snavely, and William T. Freeman. Unsupervised\nsemantic segmentation by distilling feature correspondences. In ICLR, 2022.\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised\nvisual representation learning. In CVPR, pp. 9729–9738, 2020.\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked autoencoders\nare scalable vision learners. In CVPR, pp. 16000–16009, 2022.\nJyh-Jing Hwang, Stella X. Yu, Jianbo Shi, Maxwell D. Collins, Tien-Ju Yang, Xiao Zhang, and Liang-Chieh\nChen. SegSort: Segmentation by discriminative sorting of segments. In ICCV, pp. 7333–7343. IEEE, 2019.\nPhillip Isola, Daniel Zoran, Dilip Krishnan, and Edward H Adelson.\nLearning visual groups from co-\noccurrences in space and time. In arXiv:1511.06811 [cs.CV], 2015.\nISPRS.\nISPRS 2d semantic labeling contest.\nhttps://www.isprs.org/education/benchmarks/\nUrbanSemLab Accessed: 2024-04-19.\nXu Ji, Joao F. Henriques, and Andrea Vedaldi. Invariant information clustering for unsupervised image\nclassification and segmentation. In ICCV, pp. 9865–9874, 2019.\nRobin Karlsson, Tomoki Hayashi, Keisuke Fujii, Alexander Carballo, Kento Ohtani, and Kazuya Takeda.\nVICE: Improving dense representation learning by superpixelization and contrasting cluster assignment.\nIn BMVC, 2022.\nTsung-Wei Ke, Jyh-Jing Hwang, Yunhui Guo, Xudong Wang, and Stella X. Yu. Unsupervised hierarchical\nsemantic segmentation with multiview cosegmentation and clustering transformers. In CVPR, pp. 2561–\n2571, 2022.\nDiederik P. Kingma and Jimmy Lei Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\nAlexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao,\nSpencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollár, and Ross Girshick. Segment anything.\nIn ICCV, 2023.\nAlexander Koenig, Maximilian Schambach, and Johannes Otterbach.\nUncovering the inner workings of\nSTEGO for safe unsupervised semantic segmentation. In CVPRW, 2023.\nPhilipp Krähenbühl and Vladlen Koltun. Efficient inference in fully connected CRFs with Gaussian edge\npotentials. In NIPS*2011, volume 24, 2011.\nHarold W. Kuhn. The Hungarian method for the assignment problem. In NVL, volume 52, 1955.\n14\nPublished in Transactions on Machine Learning Research (09/2024)\nKehan Li, Zhennan Wang, Zesen Cheng, Runyi Yu, Yian Zhao, Guoli Song, Li Yuan, and Jie Chen. Dynamic\nclustering network for unsupervised semantic segmentation. In CVPR, pp. 7162–7172, 2023.\nTsung-Yi Lin, Priya Goyal, Ross B. Girshick, Kaiming He, and Piotr Dollár. Focal loss for dense object\ndetection. IEEE T. Pattern Anal. Mach. Intell., 42(2):318–327, 2020.\nJonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmenta-\ntion. In CVPR, pp. 3431–3440, 2015.\nDavid G. Lowe. Distinctive image features from scale-invariant keypoints. Int. J. Comput. Vision, 60(2):\n91–110, 2004.\nJames MacQueen. Some methods for classification and analysis of multivariate observations. In Berkeley\nSymp. on Math. Statist. and Prob., volume 1, pp. 281–297. Oakland, CA, USA, 1967.\nShervin Minaee, Yuri Boykov, Fatih Porikli, Antonio Plaza, Nasser Kehtarnavaz, and Demetri Terzopoulos.\nImage segmentation using deep learning: A survey. IEEE T. Pattern Anal. Mach. Intell., 44(7):3523–3542,\n2022.\nMuzammal Naseer, Kanchana Ranasinghe, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-\nHsuan Yang. Intriguing properties of vision transformers. In NeurIPS*2021, pp. 23296–23308, 2021.\nMehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw puzzles.\nIn ECCV, volume 9910, pp. 69–84, 2016.\nYoungmin Oh, Beomjun Kim, and Bumsub Ham. Background-aware pooling and noise-aware loss for weakly-\nsupervised semantic segmentation. In CVPR, pp. 6909–6918, 2021.\nMaxime Oquab, Timothée Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre\nFernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell Howes, Po-Yao Huang, Hu Xu,\nVasu Sharma, Shang-Wen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nicolas Ballas, Gabriel Syn-\nnaeve, Ishan Misra, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski.\nDINOv2: Learning robust visual features without supervision. In TMLR, 2024.\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,\nZeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Yang, Zach\nDeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai,\nand Soumith Chintala.\nPyTorch:\nAn imperative style, high-performance deep learning library.\nIn\nNeurIPS*2019, pp. 8024–8035, 2019.\nDeepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A Efros. Context encoders:\nFeature learning by inpainting. In CVPR, pp. 2536–2544, 2016.\nFabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel,\nMathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos,\nDavid Cournapeau, Matthieu Brucher, Matthieu Perrot, and Édouard Duchesnay. Scikit-learn: Machine\nlearning in python. In JMLR, volume 12, pp. 2825–2830, 2011.\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net: Convolutional networks for biomedical image\nsegmentation. In MICCAI, volume 9351, pp. 234–241, 2015.\nLixiang Ru, Heliang Zheng, Yibing Zhan, and Bo Du.\nToken contrast for weakly-supervised semantic\nsegmentation. In CVPR, pp. 3093–3102, 2023.\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej\nKarpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet large scale\nvisual recognition challenge. Int. J. Comput. Vision, 115(13):211–252, 2015.\n15\nPublished in Transactions on Machine Learning Research (09/2024)\nMaximilian Seitzer, Max Horn, Andrii Zadaianchuk, Dominik Zietlow, Tianjun Xiao, Carl-Johann Simon-\nGabriel, Tong He, Zheng Zhang, Bernhard Schölkopf, Thomas Brox, and Francesco Locatello. Bridging\nthe gap to real-world object-centric learning. In ICLR, 2023.\nHyun Seok Seong, WonJun Moon, SuBeen Lee, and Jae-Pil Heo. Leveraging hidden positives for unsupervised\nsemantic segmentation. In CVPR, pp. 19540–19549, 2023.\nJianbo Shi and Jitendra Malik. Normalized cuts and image segmentation. IEEE T. Pattern Anal. Mach.\nIntell., 22(8):888–905, 2000.\nNarek Tumanyan, Omer Bar-Tal, Shai Bagon, and Tali Dekel. Splicing ViT features for semantic appearance\ntransfer. In CVPR, pp. 10738–10747, 2022.\nJasper R. R. Uijlings, Koen E. A. van de Sande, Theo Gevers, and Arnold W. M. Smeulders. Selective search\nfor object recognition. Int. J. Comput. Vision, 104(2):154–171, 2013.\nWouter Van Gansbeke, Simon Vandenhende, Stamatios Georgoulis, and Luc Van Gool. Unsupervised se-\nmantic segmentation by contrasting object mask proposals. In ICCV, pp. 10052–10062, 2021.\nWouter Van Gansbeke, Simon Vandenhende, and Luc Van Gool. Discovering object masks with transformers\nfor unsupervised semantic segmentation. In arXiv:2206.06363 [cs.CV], 2022.\nXudong Wang, Rohit Girdhar, Stella X Yu, and Ishan Misra. Cut and learn for unsupervised object detection\nand instance segmentation. In CVPR, pp. 3124–3134, 2023a.\nYangtao Wang, Xi Shen, Yuan Yuan, Yuming Du, Maomao Li, Shell Xu Hu, James L. Crowley, and Do-\nminique Vaufreydaz. TokenCut: Segmenting objects in images and videos with self-supervised transformer\nand normalized cut. In IEEE T. Pattern Anal. Mach. Intell., volume 45, pp. 15790–15801, 2023b.\nLian Xu, Wanli Ouyang, Mohammed Bennamoun, Farid Boussaïd, and Dan Xu. Multi-class token trans-\nformer for weakly supervised semantic segmentation. In CVPR, pp. 4300–4309, 2022.\nZhaoyuan Yin, Pichao Wang, Fan Wang, Xianzhe Xu, Hanling Zhang, Hao Li, and Rong Jin. TransFGU:\nA top-down approach to fine-grained unsupervised semantic segmentation. In ECCV, volume 13689, pp.\n73–89, 2022.\nAndrii Zadaianchuk, Matthäus Kleindessner, Yi Zhu, Francesco Locatello, and Thomas Brox. Unsupervised\nsemantic segmentation with self-supervised object-centric representations. In ICLR, 2023.\n16\nPublished in Transactions on Machine Learning Research (09/2024)\nA\nFurther Analysis\nIn this appendix, we provide more detailed insights into PriMaPs-EM beyond the scope of the main paper.\nA.1\nNearest-Neighbor Anchoring\nAs described in Sec. 3 of the main paper, PriMaPs are anchoring the first principal component in each\niteration of the mask proposal generation. Corresponding to the quantitative findings (cf. Tab. 4a), here\nwe additionally analyze this qualitatively. Fig. 5 shows similarity maps of all features to the iteratively\ncomputed first principal component (Similarity 1st PC) as well as to the respective nearest-neighbor feature\n(Similarity 1st PC NN) for example images from Cityscapes (Cordts et al., 2016), COCO-Stuff (Caesar et al.,\n2018), and Potsdam-3 (ISPRS). We observe that the originally computed principal direction can have high\nsimilarities with multiple semantic concepts in an image. Hence, finding a suitable threshold that isolates a\nsingle main concept is difficult.\nHowever, using the nearest image feature to the principal component as an anchoring element helps to\ncircumvent high similarity values to multiple visual concepts. For instance, in the first example for Cityscapes,\nthe similarity map for the first principal component has high similarities to both “building” and “vegetation”.\nIn contrast, the similarity map for the nearest neighbor of the first principal component results in high\nsimilarity to the class “vegetation” only. Consequently, a suitable mask proposal can be obtained through\nthresholding. We observe this particularly for the Cityscapes dataset and some examples of COCO-Stuff.\nFurther, in cases where the similarity map is already localized to one visual concept, anchoring merely leads\nto a change in the similarity values without changing the shape of the thresholded proposal.\nA.2\nAblation of the Threshold\nIn the spirit of unsupervised learning, our method effectively has only a single additional hyperparameter –\nthe threshold ψ. Furthermore, this parameter can be set simply by examining the mask proposal as detailed\nnext. In addition, it should be noted that we keep this parameter unchanged for all backbone models and\ndatasets, which further emphasizes the generalizability of our method. As described in Sec. 3 of the main\npaper, the threshold ψ is used to remove noise in the similarity masks and to localize the optimization\nsignal. In Fig. 6, qualitative examples of PriMaPs mask proposals are visualized for DINO ViT-B/8 on\nCityscapes, COCO-Stuff, and Potsdam-3. We show mask proposals for ψ = 0.3, ψ = 0.4, and ψ = 0.5.\nVisually, it can be observed that meaningful masks are produced for all thresholds. Especially those for\nthreshold 0.3 and 0.4 align very well with the semantic content in the images. While ψ = 0.3 seems to\nprovide better mask proposals for COCO-Stuff and Potsdam-3, a problem arises with Cityscapes. Here, the\nmask proposals contain several semantic concepts and spatially small objects in a large mask (cf. the second\nCityscapes example, where the mask of the bush in the left half of the image covers both the traffic sign\nin the foreground, parts of the house in the background, as well as the sky). Since this would lead to a\npoor optimization signal and the masks of the other two datasets using ψ = 0.4 seem visually appealing, the\nthreshold is set to 0.4 in all other experiments.\nTo further shed light on these qualitative observations, we apply PriMaPs-EM for the scenario described\nabove and vary the threshold from ψ = 0.2 to ψ = 0.6 with a step size of 0.1.\nWe perform this with\nthe DINO ViT-B/8 backbone for Cityscapes, COCO-Stuff, and Potsdam-3 and show the mIoU in Fig. 7.\nThe quantitative results reflect our qualitative conclusion of the threshold well, and show the trade-off\nbetween better segmentation accuracy on COCO-Stuff and Potsdam-3 for a lower threshold and vice versa for\nCityscapes. In numbers, a threshold of ψ = 0.5 instead of ψ = 0.4 for Cityscapes would lead to an additional\ngain of 1.4 % mIoU, but also to slight losses on COCO-Stuff of 0.8 %. Additionally, this results in more mask\nproposal iterations. We conclude that the determined threshold ψ = 0.4 appears to be reasonable. Even if\nbetter results could be achieved for some backbone-dataset combinations with individually set thresholds,\nwe consider a fixed threshold that generalizes well across all scenarios to be sensible. We would like to\nemphasize that this experiment was conducted solely for this single backbone and serves only to validate\nthe qualitative judgement from above. Importantly, we did not determine the hyperparameters based on the\nevaluation sets.\n17\nPublished in Transactions on Machine Learning Research (09/2024)\nPotsdam-3\nCOCO-Stuff\nCityscapes\nImage\nSimilarity 1st PC\nSimilarity 1st PC NN\nGround truth\nFigure 5: Nearest neighbor anchoring of the principal direction in PriMaPs. Image, ground-truth\nlabel, and the first three similarity maps with respect to the principal direction (left) and their nearest\nneighbor (right) for all three datasets using DINO ViT-B/8. Anchoring localizes the signal for principal\ndirections with high similarities to multiple visual concepts.\nA.3\nAblation of the Stopping Criterion\nOur method iteratively decomposes images into mask proposals. Hence, a stopping criterion is essential\nto decide whether a sufficient number of masks has been generated for a particular image. This allows us\nto divide different images into a different number of masks. We use the percentage of features assigned to\nmasks. Specifically, the iterative PriMaPs process stops when 95% of image features are assigned to masks.\nWhile setting this value as high as possible to cover the entire image with masks might seem intuitive, not all\nfeatures can be assigned reasonably due to the inherent noise in the self-supervised feature representation.\nSetting the value too high would result in masks with very few features in the final iterations. We chose\na stopping criterion of 95%, which we confirmed quantitatively to yield good results across all datasets as\nshown in Fig. 8. The relatively low mIoU for the Potsdam-3 dataset with smaller stopping criteria results\nfrom a few initial mask proposals covering large areas, while the proposals in the final iterations represent\nfiner details.\nA.4\nAblation of the Image Augmentations\nIn addition to the Tab. 4b, we analyze the photometric augmentations used in PriMaPs-EM in greater detail.\nAs can be seen, the additional augmentation leads to slightly improved performance in terms of metrics. We\nused a combination of standard augmentations as well as the standard torchvision implementation and did\n18\nPublished in Transactions on Machine Learning Research (09/2024)\nCityscapes\nCOCO-Stuff\nPotsdam-3\nψ = 0.3\nPriMaPs\nψ = 0.4\nψ = 0.5\nImage\nFigure 6: Qualitative threshold ψ ablation with DINO ViT-B/8 using different ψ values for PriMaPs\nmask-proposal generation on all three datasets. The hyperparameter exhibits favorable stability properties.\n0.2\n0.3\n0.4\n0.5\n0.6\n20\n40\n60\nThreshold ψ\nmIoU (%)\nCityscapes\nCOCO-Stuff\nPotsdam-3\nFigure 7: Quantitative threshold ψ ablation with\nDINO ViT-B/8 using different ψ values for PriMaPs\nmask-proposal generation on all three datasets. The\nsingle hyperparameter of our method exhibits favor-\nable stability properties.\n85\n90\n95\n99\n20\n40\n60\nStop Criterion (%)\nmIoU (%)\nCityscapes\nCOCO-Stuff\nPotsdam-3\nFigure 8: Quantitative stopping-criterion abla-\ntion with DINO ViT-B/8 using different stopping\ncriteria for PriMaPs mask proposal generation on all\nthree datasets.\nnot tune any parameters. Tab. 6 provides a contribution breakdown of the used augmentation techniques\n(grayscaling, Gaussian blur, color jitter) on COCO-Stuff using DINO ViT-B/8. Each of the three photometric\naugmentations contributes to the method’s performance in terms of downstream metrics.\nA.5\nAblation on the Number of Pseudo Classes K\nFollowing previous works (Ji et al., 2019; Cho et al., 2021; Hamilton et al., 2022; Seong et al., 2023) in\nunsupervised semantic segmentation, generally the number of pseudo classes K is set to the number of\nannotated semantic classes in the dataset. This is done to evaluate the performance in terms of downstream\nmetrics on every semantic class after matching the predicted pseudo-class IDs with the ground-truth classes\nusing Hungarian matching. Using a K smaller than the number of ground-truth classes is hard to evaluate\nand compare as it results in ground-truth classes that need to be ignored by the metric. However, it is\npossible to have more pseudo classes than ground-truth classes by assigning multiple pseudo classes to a\nsingle ground-truth class. We conduct an experimented in this over-segmentation setup, i. e. predicting K\ncategories, where K is larger than the number of ground-truth classes. We realize the multi-to-one matching\n19\nPublished in Transactions on Machine Learning Research (09/2024)\nTable 6: Ablation study analyzing design choices and components of PriMaPs-EM for COCO-Stuff using\nDINO ViT-B/8 including a breakdown of the image augmentations.\nMethod\nAcc\nmIoU\nBaseline (Caron et al., 2021)\n38.8\n15.7\n+ PriMaPs pseudo label\n38.8\n18.0\n+ EMA\n45.0\n20.2\n+ Augment (grayscaling)\n45.3\n20.4\n+ Augment (grayscaling, Gaussian blur)\n45.8\n20.2\n+ Augment (grayscaling, Gaussian blur, color jitter)\n46.0\n20.4\n+ CRF (≡PriMaPs-EM)\n48.4\n21.9\nTable 7: Over clustering with PriMaPs-EM for COCO-Stuff using DINO ViT-B/8.\nMethod\nAcc\nmIoU\nPriMaPs-EM (K=27; 100%)\n48.5\n21.9\nPriMaPs-EM (K=40; 150%)\n53.1\n23.2\nby applying Hungarian matching first and subsequently matching the remaining pseudo-class IDs based on\ntheir highest correspondence to the ground-truth classes. In Tab. 7, we report the unsupervised semantic\nsegmentation results for COCO-Stuff with DINO ViT-B/8 once using K = 27, which matches the number\nof ground-truth classes, and for K = 40. Over clustering leads to better performance in terms of metrics,\nas multiple pseudo classes represent a single ground-truth class. Overall, estimating the number of pseudo\nclasses or semantic concepts in a dataset in an unsupervised manner represents an intriguing direction for\nfuture research.\nB\nFurther Experiments\nThis appendix provides insights beyond the experiments and ablations shown in Sec. 4.\nB.1\nClass-Level Quantitative Analysis\nTo gain a deeper understanding of PriMaPs-EM, we assess the segmentation accuracy in terms of IoU\nfor individual classes. Additionally, we present the confusion matrices among the semantic classes for the\nDINO ViT-B/8 Baseline, PriMaPs-EM, STEGO, and STEGO+PriMaPs-EM for the COCO-Stuff dataset\nin Fig. 9. Generally, we can observe that for both the DINO and STEGO baseline, for certain classes (e. g.,\n“Appliance”, “Indoor”, “Kitchen”) the discovered unsupervised class concept does not correlate with human-\ndefined semantic classes. This suggests that the respective backbone feature representation may already be\nhard to separate. Furthermore, some of the 27 intermediate COCO-Stuff classes merge visually distinct\nconcepts. For instance, the class “indoor” combines “hairbrush”, “toothbrush”, “hair dryer”, “teddy bear”,\n“scissors”, “vase”, “clock”, and “book”. We see this assumption partially confirmed by analyzing the class\nIoUs of linear probing of the DINO features. For the problematic classes, the linear probing IoUs are in the\nrange of approx. 16 %–30 %, whereas for the other classes the IoU is 50 % and higher. We conclude that if it\nis already difficult to linearly distinguish the classes based on the backbone features, our method can hardly\nimprove upon this.\nHowever, for classes meeting this requirement, our method clearly boosts class IoUs. In rare cases (e. g.,\nSTEGO comparison to STEGO+PriMaPs-EM for classes “Outdoor” and “Sports”), there is a decrease in one\nclass IoU with PriMaPs-EM while another class IoU increases. This can occur if the change in the prototype\nrepresentation results in a change of the Hungarian matching for evaluation, though this is rarely observed.\nIn terms of class confusion, the model’s predictions align with the ground-truth labels. Furthermore, the\nexisting confusions are reasonable. For instance, when using STEGO, confusions of the two “food” mid-\nlevel classes emerge. Overall, Fig. 9a indicates that our method either enhances or at least maintains the\n20\nPublished in Transactions on Machine Learning Research (09/2024)\nFigure 9: COCO-Stuff – Comparison of the segmentation performance for individual classes in\nterms of IoU (in %) (a) and class confusion for the DINO ViT-B/8 Baseline (b), PriMaPs-EM (c), STEGO\n(d), and STEGO + PriMaPs-EM (e). Overall, PriMaPs-EM preserves or boosts the individual class IoU\nacross most classes and moderately reduces class confusion.\n(a) Class IoUs (in %) for DINO ViT-B/8 Baseline, PriMaPs-EM (Ours), STEGO, and STEGO+PriMaPs-EM (Ours).\nElectronic\nAppliance\nFood\nFurniture\nIndoor\nKitchen\nAccessory\nAnimal\nOutdoor\nPerson\nSports\nVehicle\nCeiling\nFloor\nFood\nFurniture\nRawmaterial\nTextile\nWall\nWindow\nBuilding\nGround\nPlant\nSky\nSolid\nStructural\nWater\nBaseline\n0.0\n0.0\n0.0\n0.2\n0.0\n0.0\n0.0 15.3\n0.1 58.3\n0.5\n9.1 23.7\n3.1\n2.2\n8.3 12.2\n0.2 30.7 11.7 31.9 21.5 33.8 77.4 15.0 27.1 45.1\nOurs\n0.0\n0.2\n0.0\n0.8\n0.0\n0.0\n0.0 21.4\n4.6 68.7\n0.5 12.6 36.1 44.4\n0.0 12.1 14.0\n1.1 45.7 15.9 38.1 33.3 42.9 69.1 29.1 38.9 61.4\nSTEGO\n0.0\n0.3\n0.3 13.7\n0.1\n0.0\n0.7 74.1\n0.1 61.7 10.9 40.7 36.3 30.2 38.8 22.5 15.6 11.0 36.0\n2.7 51.8 44.1 50.2 82.9 19.8 37.8 58.8\nSTEGO+Ours\n0.2\n0.0\n0.0 14.1\n0.2\n0.0\n0.3 76.8 10.9 64.3\n0.9 53.8 48.2 31.9 39.4 25.4 16.0 19.7 36.1\n0.8 55.6 44.9 51.2 83.8 27.4 36.2 62.6\n(b) Confusion matrix for the Baseline\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n2\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n40\n48\n55\n44\n40\n23\n31\n63\n44\n4\n38\n48\n4\n2\n37\n20\n21\n15\n7\n7\n15\n2\n2\n0\n6\n6\n2\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n6\n22\n0\n6\n4\n0\n0\n0\n0\n0\n1\n0\n7\n1\n0\n45\n2\n0\n0\n0\n0\n0\n0\n2\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n2\n0\n0\n0\n2\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n0\n17\n12\n12\n11\n13\n13\n48\n18\n10\n13\n9\n21\n27\n1\n2\n14\n7\n14\n10\n3\n4\n7\n1\n2\n0\n3\n4\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n38\n0\n1\n0\n2\n0\n1\n0\n25\n0\n0\n1\n2\n0\n5\n4\n18\n9\n8\n7\n8\n9\n7\n7\n5\n4\n3\n8\n18\n6\n6\n7\n5\n4\n5\n3\n5\n2\n4\n6\n2\n8\n16\n6\n22\n4\n11\n5\n2\n7\n2\n2\n4\n20\n23\n16\n24\n13\n20\n12\n8\n8\n13\n8\n3\n12\n7\n11\n14\n2\n1\n0\n25\n4\n21\n0\n17\n1\n18\n4\n2\n0\n0\n2\n19\n9\n2\n1\n3\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n2\n8\n1\n27\n2\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n2\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n57\n0\n0\n10\n2\n4\n41\n4\n3\n0\n0\n0\n0\n1\n0\n13\n10\n0\n1\n1\n1\n1\n0\n2\n0\n0\n2\n3\n0\n0\n18\n13\n17\n10\n48\n9\n0\n0\n0\n1\n6\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n4\n0\n0\n2\n1\n0\n8\n6\n38\n0\n1\n0\n1\n4\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n18\n2\n1\n0\n2\n1\n0\n0\n23\n3\n0\n8\n1\n1\n1\n0\n3\n2\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n2\n0\n0\n0\n0\n4\n1\n0\n41\n0\n4\n5\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n3\n0\n0\n1\n86\n2\n0\n10\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n2\n1\n1\n6\n1\n40\n2\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\n0\n0\n1\n1\n0\n1\n2\n2\n0\n1\n1\n0\n44\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\n0\n2\n0\n0\n0\n14\n0\n0\n2\n0\n48\nPredicted labels\nGround-truth labels\nElectronic\nAppliance\nFood\nFurniture\nIndoor\nKitchen\nAccessory\nAnimal\nOutdoor\nPerson\nSports\nVehicle\nCeiling\nFloor\nFood\nFurniture\nRawmaterial\nTextile\nWall\nWindow\nBuilding\nGround\nPlant\nSky\nSolid\nStructural\nWater\nElectronic\nAppliance\nFood\nFurniture\nIndoor\nKitchen\nAccessory\nAnimal\nOutdoor\nPerson\nSports\nVehicle\nCeiling\nFloor\nFood\nFurniture\nRawmaterial\nTextile\nWall\nWindow\nBuilding\nGround\nPlant\nSky\nSolid\nStructural\nWater\n(c) Confusion matrix for PriMaPs-EM\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n2\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n0\n0\n0\n2\n0\n0\n0\n0\n0\n0\n0\n20\n1\n0\n6\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n5\n0\n0\n0\n0\n0\n0\n0\n9\n0\n0\n0\n1\n0\n0\n0\n0\n2\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n18\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n8\n0\n1\n0\n0\n38\n19\n76\n16\n51\n19\n35\n87\n35\n8\n37\n49\n3\n1\n64\n10\n17\n20\n3\n6\n13\n0\n2\n0\n4\n3\n1\n4\n6\n0\n2\n1\n2\n0\n0\n16\n0\n2\n4\n0\n1\n0\n6\n4\n0\n1\n3\n4\n1\n0\n0\n1\n5\n0\n0\n0\n0\n0\n0\n0\n9\n0\n0\n75\n2\n0\n0\n0\n0\n0\n0\n4\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n4\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n16\n0\n0\n2\n0\n14\n16\n37\n10\n48\n11\n53\n37\n1\n10\n2\n28\n27\n0\n1\n5\n7\n9\n7\n1\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n3\n0\n0\n0\n0\n0\n84\n0\n0\n3\n1\n1\n6\n1\n3\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n55\n0\n3\n0\n5\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n2\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n9\n31\n0\n10\n7\n0\n3\n11\n6\n18\n3\n8\n2\n1\n6\n1\n2\n2\n1\n6\n20\n20\n14\n17\n5\n4\n4\n2\n2\n0\n11\n3\n1\n18\n1\n0\n0\n26\n1\n2\n0\n14\n0\n7\n1\n0\n0\n0\n1\n20\n8\n2\n1\n2\n0\n0\n0\n0\n0\n0\n0\n1\n0\n1\n0\n9\n2\n0\n0\n5\n5\n4\n0\n0\n1\n1\n2\n1\n0\n0\n1\n0\n0\n0\n0\n0\n0\n5\n10\n0\n2\n1\n2\n1\n0\n0\n0\n0\n0\n5\n2\n0\n20\n7\n13\n64\n9\n13\n1\n0\n0\n8\n10\n1\n8\n8\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n17\n4\n15\n6\n49\n4\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n2\n1\n0\n0\n1\n2\n0\n2\n6\n42\n0\n0\n0\n1\n3\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n24\n0\n2\n0\n0\n0\n1\n0\n35\n0\n0\n0\n1\n0\n2\n0\n3\n2\n0\n0\n0\n1\n1\n0\n0\n0\n0\n0\n5\n0\n1\n0\n0\n4\n2\n0\n45\n1\n5\n8\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n3\n1\n0\n0\n76\n3\n0\n11\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n35\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n2\n0\n0\n0\n0\n0\n45\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n2\n0\n0\n1\n0\n65\nPredicted labels\nGround-truth labels\nElectronic\nAppliance\nFood\nFurniture\nIndoor\nKitchen\nAccessory\nAnimal\nOutdoor\nPerson\nSports\nVehicle\nCeiling\nFloor\nFood\nFurniture\nRawmaterial\nTextile\nWall\nWindow\nBuilding\nGround\nPlant\nSky\nSolid\nStructural\nWater\nElectronic\nAppliance\nFood\nFurniture\nIndoor\nKitchen\nAccessory\nAnimal\nOutdoor\nPerson\nSports\nVehicle\nCeiling\nFloor\nFood\nFurniture\nRawmaterial\nTextile\nWall\nWindow\nBuilding\nGround\nPlant\nSky\nSolid\nStructural\nWater\n(d) Confusion matrix for STEGO\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n4\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n18\n0\n0\n4\n0\n18\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n5\n0\n1\n0\n0\n0\n9\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n10\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n46\n63\n1\n53\n22\n62\n9\n0\n2\n0\n2\n0\n0\n4\n3\n19\n14\n8\n5\n2\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n2\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n8\n26\n0\n6\n2\n0\n2\n1\n0\n0\n0\n0\n3\n0\n0\n0\n0\n0\n0\n0\n0\n2\n2\n0\n1\n1\n1\n0\n0\n2\n0\n0\n0\n0\n1\n0\n0\n3\n0\n1\n88\n0\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n3\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n1\n0\n0\n12\n6\n0\n21\n4\n1\n0\n0\n0\n0\n1\n0\n10\n0\n0\n69\n3\n0\n0\n0\n0\n0\n0\n4\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n1\n4\n5\n3\n19\n3\n34\n8\n63\n16\n0\n0\n0\n1\n8\n6\n1\n0\n0\n2\n0\n0\n1\n2\n0\n2\n1\n0\n2\n3\n0\n7\n0\n18\n2\n5\n72\n0\n0\n0\n0\n9\n1\n1\n1\n7\n1\n0\n0\n0\n2\n0\n0\n0\n0\n0\n0\n0\n13\n0\n0\n0\n0\n0\n84\n0\n0\n2\n1\n1\n3\n4\n5\n0\n0\n1\n0\n0\n0\n2\n2\n1\n3\n2\n18\n0\n0\n0\n0\n0\n0\n0\n56\n8\n17\n7\n12\n2\n0\n0\n0\n0\n0\n1\n0\n0\n1\n0\n92\n3\n21\n10\n0\n0\n0\n0\n1\n0\n0\n0\n86\n1\n2\n1\n0\n0\n0\n0\n1\n0\n1\n0\n0\n7\n21\n0\n2\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n32\n4\n22\n11\n43\n1\n0\n0\n0\n0\n1\n0\n34\n0\n0\n0\n33\n0\n14\n0\n15\n0\n9\n0\n0\n0\n0\n1\n24\n9\n3\n1\n2\n0\n0\n0\n0\n0\n0\n0\n0\n0\n22\n0\n0\n12\n0\n0\n0\n0\n0\n0\n0\n0\n1\n2\n14\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n4\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n8\n0\n0\n9\n0\n2\n38\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n2\n0\n0\n0\n0\n0\n1\n2\n2\n2\n4\n2\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n2\n0\n1\n0\n2\n0\n0\n2\n2\n0\n0\n2\n10\n2\n9\n24\n68\n0\n1\n1\n1\n5\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n0\n29\n0\n0\n1\n0\n1\n1\n0\n47\n1\n0\n1\n1\n0\n0\n0\n1\n1\n0\n0\n1\n1\n1\n0\n0\n0\n0\n0\n1\n0\n1\n0\n0\n5\n1\n0\n53\n2\n8\n7\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n89\n3\n0\n10\n0\n0\n0\n0\n0\n0\n1\n0\n1\n0\n3\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n2\n1\n29\n1\n1\n0\n0\n0\n1\n0\n0\n0\n0\n12\n0\n0\n0\n0\n1\n0\n1\n2\n4\n5\n2\n2\n1\n1\n0\n1\n65\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n2\n0\n0\n1\n0\n62\nPredicted labels\nGround-truth labels\nElectronic\nAppliance\nFood\nFurniture\nIndoor\nKitchen\nAccessory\nAnimal\nOutdoor\nPerson\nSports\nVehicle\nCeiling\nFloor\nFood\nFurniture\nRawmaterial\nTextile\nWall\nWindow\nBuilding\nGround\nPlant\nSky\nSolid\nStructural\nWater\nElectronic\nAppliance\nFood\nFurniture\nIndoor\nKitchen\nAccessory\nAnimal\nOutdoor\nPerson\nSports\nVehicle\nCeiling\nFloor\nFood\nFurniture\nRawmaterial\nTextile\nWall\nWindow\nBuilding\nGround\nPlant\nSky\nSolid\nStructural\nWater\n(e) Confusion matrix for STEGO + PriMaPs-EM\n0\n0\n0\n0\n0\n0\n0\n0\n0\n10\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n8\n26\n0\n6\n3\n0\n46\n55\n2\n51\n21\n62\n12\n0\n1\n0\n1\n0\n0\n3\n3\n17\n15\n7\n3\n2\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n2\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n3\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n15\n6\n0\n8\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n2\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n3\n0\n0\n84\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n2\n2\n5\n2\n14\n2\n31\n3\n45\n14\n0\n0\n0\n0\n4\n3\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n13\n1\n0\n71\n4\n0\n0\n0\n0\n0\n0\n5\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n8\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n17\n0\n0\n2\n0\n15\n0\n0\n0\n0\n0\n0\n0\n0\n4\n0\n0\n61\n0\n0\n0\n0\n2\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n77\n0\n0\n1\n0\n0\n2\n1\n2\n0\n0\n0\n0\n0\n0\n1\n2\n1\n4\n2\n20\n0\n0\n0\n0\n0\n0\n0\n58\n5\n18\n7\n13\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n91\n3\n20\n7\n0\n0\n0\n0\n0\n0\n0\n0\n89\n0\n2\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n28\n32\n0\n4\n1\n2\n1\n0\n0\n0\n0\n0\n0\n2\n0\n37\n5\n14\n13\n46\n0\n0\n0\n0\n0\n0\n0\n19\n0\n1\n0\n35\n0\n30\n0\n16\n0\n14\n1\n1\n0\n0\n1\n23\n8\n2\n0\n2\n0\n0\n0\n0\n0\n0\n0\n0\n0\n23\n1\n0\n11\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n27\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n3\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n3\n0\n0\n8\n0\n1\n38\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n7\n0\n0\n0\n0\n1\n0\n3\n6\n1\n17\n3\n4\n1\n2\n0\n26\n4\n0\n1\n0\n0\n1\n4\n0\n3\n0\n9\n1\n0\n9\n13\n0\n0\n3\n16\n4\n5\n32\n77\n0\n1\n1\n1\n7\n0\n0\n1\n0\n2\n0\n0\n5\n0\n10\n2\n10\n5\n0\n30\n0\n1\n5\n1\n1\n2\n2\n50\n1\n0\n3\n3\n1\n0\n0\n0\n1\n0\n0\n1\n2\n1\n1\n0\n1\n0\n0\n0\n0\n1\n0\n0\n6\n2\n1\n56\n2\n8\n8\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n3\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n91\n2\n0\n11\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n30\n0\n0\n0\n0\n0\n2\n0\n0\n0\n0\n10\n1\n1\n1\n1\n0\n0\n1\n2\n7\n8\n1\n2\n0\n0\n0\n0\n66\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n2\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n0\n66\nPredicted labels\nGround-truth labels\nElectronic\nAppliance\nFood\nFurniture\nIndoor\nKitchen\nAccessory\nAnimal\nOutdoor\nPerson\nSports\nVehicle\nCeiling\nFloor\nFood\nFurniture\nRawmaterial\nTextile\nWall\nWindow\nBuilding\nGround\nPlant\nSky\nSolid\nStructural\nWater\nElectronic\nAppliance\nFood\nFurniture\nIndoor\nKitchen\nAccessory\nAnimal\nOutdoor\nPerson\nSports\nVehicle\nCeiling\nFloor\nFood\nFurniture\nRawmaterial\nTextile\nWall\nWindow\nBuilding\nGround\nPlant\nSky\nSolid\nStructural\nWater\n21\nPublished in Transactions on Machine Learning Research (09/2024)\nCityscapes\nCOCO-Stuff\nPotsdam-3\nHP +\nPriMaPs-EM\nHP\nPriMaPs-EM\nBaseline\nGround truth\nImage\nFigure 10: Qualitative results for the DINO ViT-S/8 Baseline, PriMaPs-EM (Ours), HP (Seong et al.,\n2023), and HP (Seong et al., 2023)+PriMaPs-EM (Ours) for all three datasets. Our method produces locally\nmore consistent segmentation results, reducing misclassification.\nsegmentation performance per class in terms of IoU regardless of the backbone model. Additionally, our\nmethod aids in reducing the confusion among classes.\nB.2\nQualitative Comparison to HP\nSimilar to the qualitative comparison in Fig. 4 in the main paper, we aim to compare PriMaPs-EM with HP\n(Seong et al., 2023). We present qualitative examples for the baseline, PriMaPs-EM, HP, and the combination\nof HP and PriMaPs-EM in Fig. 10. These qualitative examples align with the findings from the quantitative\nresults in the main paper (cf. Tabs. 1 to 3). It is evident that our method produces locally more consistent\nand less noisy results compared to both baselines. Despite the already impressive qualitative results of HP on\nCOCO-Stuff, our method excels in correcting misclassifications and achieving better segmentation of object\nboundaries. We observe one limitation, particularly with the DINO ViT-S/8 baseline, both independently\nand in conjunction with PriMaPs-EM for the Potsdam-3 dataset. In this case, the semantic concept of\n“street” is not recognized and is consequently rarely predicted.\nB.3\nComparison to HP using DINOv2\nCurrent state-of-the-art methods Hamilton et al. (2022); Seong et al. (2023) for unsupervised segmentation\ndo not provide experiments using DINOv2 features. To be able to compare with previous methods, we train\nHP using DINOv2 for the comparison in Tab. 8. We strictly follow the training schedule and hyperparameters\nused in the original implementation for all respective datasets. HP does not generalize well to the DINOv2\nfeatures and hardly keeps up with the strong baseline. PriMaPs-EM moderately but consistently improves\n22\nPublished in Transactions on Machine Learning Research (09/2024)\nTable 8: Comparing PriMaPs-EM (Ours) to HP using DINOv2 for the Cityscapes (ViT-B/14),\nCOCO-Stuff (ViT-B/14), and Potsdam-3 (ViT-S/14) datasets. We report Accuracy and mean IoU (in %)\nfor unsupervised probing.\nMethod\nCityscapes\nCOCO-Stuff Potsdam-3\nAcc mIoU Acc mIoU Acc mIoU\nDINOv2 Baseline (Oquab et al., 2024)\n49.5\n15.3\n44.5\n22.9\n82.4\n69.9\n+ HP (Seong et al., 2023)\n67.9\n15.9\n48.9\n19.8\n79.4\n65.7\n+ PriMaPs-EM\n71.6\n19.0\n46.4\n23.8\n83.1\n71.0\n+ HP (Seong et al., 2023) + PriMaPs-EM\n74.3\n16.6\n49.3\n20.2\n79.6\n66.0\nCityscapes\nCOCO-Stuff\nPotsdam-3\nDINOv2 +\nPriMaPs-EM\nDINOv2\nBaseline\nDINO +\nPriMaPs-EM\nDINO\nBaseline\nGround Truth\nImage\nFigure 11: Qualitative comparison of DINO and DINOv2 using ViT-B/8 and ViT-B/14 respectively.\nWe show the DINO Baseline, DINO+PriMaPs-EM (Ours), DINOv2 Baseline and DINOv2+PriMaPs-EM\n(Ours) for Cityscapes, COCO-Stuff, and Potsdam-3.\nupon both DINOv2 and HP across all datasets and metrics using the identical PriMaPs-EM hyperparameters\nwe use across all other experiments in this work.\nB.4\nQualitative Comparison of DINO and DINOv2\nOur experiments could not identify any clear differences between DINOv1 and DINOv2 and the different\ntransformer architectures in terms of the downstream task performance of unsupervised semantic segmenta-\ntion. Both methods provide excellent feature embeddings and similar quantitative results as baselines. When\napplying PriMaPs-EM, DINOv2 often provides slightly better quantitative results but coarser segmentation\n23\nPublished in Transactions on Machine Learning Research (09/2024)\nCityscapes\nCOCO-Stuff\nPotsdam-3\nGround truth\nPriMaPs\nOracle IDs\nPriMaPs\nColored\nImage\nFigure 12: Qualitative PriMaPs examples using DINO ViT-B/8 for Cityscapes, COCO-Stuff, and\nPotsdam-3.\nPriMaPs Colored – each mask proposal is visualized in a different color.\nPriMaPs Oracle\nclass IDs – each mask is colored in the corresponding ground-truth class color.\nmasks due to the larger patches (cf. Tabs. 1 to 3). We visualize both the DINO and DINOv2 baselines and\nwith PriMaPs-EM in Fig. 11.\nB.5\nQualitative PriMaPs Pseudo-Label Examples\nFollowing the quantitative assessment of the pseudo labels in Sec. 4.3, Fig. 12 visualizes qualitative examples\nof the PriMaPs mask proposals and pseudo labels. We visualize individual masks, each in a different color\n(PriMaPs Colored). We also display oracle pseudo labels, assigning each mask a color based on the ground-\ntruth label (PriMaPs Oracle class IDs). We observe that the mask proposals align well with the ground-truth\nlabels across all three datasets, generalizing across three distinct domains. PriMaPs effectively partitions\nimages into semantically meaningful masks.\nB.6\nFailure Cases\nFinally, we would like to discuss observed failure cases of PriMaPs-EM. Fig. 13 shows examples of failure\ncases occurring in the segmentation predictions as well as failure examples for PriMaPs pseudo labels. For\nPriMaPs-EM, we observe misclassifications, such as in Cityscapes where buses are often partially segmented\nas the class “car” or that cobblestone is misclassified as “sidewalk”. For COCO-Stuff, shadows and structures\ninfluence the segmentation predictions and confusions also occur (see “ground” and “floor” in example two).\nFor Potsdam-3, we observe that vehicles and road markings are sometimes erroneously attributed to the class\n“building” instead of “road”. For PriMaPs pseudo labels, we identify two main sources of error. First, small\nobjects in cluttered images are sometimes assigned to larger neighboring masks, a phenomenon that can be\nattributed to the limited backbone feature resolution. This is particularly noticeable for Cityscapes, where the\ncenter horizontal area is often detailed (cf. Fig. 12 “Pole”, “Traffic Light”, “Traffic Sign”). Second, PriMaPs\nsometimes oversegment images containing a large foreground object as seen in the COCO-Stuff example.\nSimilarly, different visual appearances of the same semantic class can lead to multiple masks (Potsdam-3).\nDespite these observations, the simple PriMaPs provide promising mask proposals that correspond well with\nthe ground-truth label.\n24\nPublished in Transactions on Machine Learning Research (09/2024)\nCityscapes\nCOCO-Stuff\nPotsdam-3\nCityscapes COCO-Stuff Potsdam-3\nPriMaPs-EM\nBaseline\nGround truth\nImage\nGround truth\nPriMaPs\nOracle IDs\nPriMaPs\nColored\nImage\nFigure 13: Failure cases for the PriMaPs-EM segmentation (left) as well as PriMaPs pseudo labels (right)\nusing DINO ViT-B/8 for Cityscapes, COCO-Stuff, and Potsdam-3.\nC\nImplementation\nSince all significant high-level implementation details and hyperparameters have been addressed in the main\npaper, this section addresses only few remaining details. Both the code and models are publicly available\nat https://github.com/visinf/primaps. Our work is implemented in PyTorch (Paszke et al., 2019). We\nbuild up on the code of Ji et al. (2019), Van Gansbeke et al. (2021) and Hamilton et al. (2022).\nC.1\nBackbone Models\nFor each backbone model, we use the corresponding original implementation. Specifically, for DINO (Caron\net al., 2021) and DINOv2 (Oquab et al., 2024), we utilize the PyTorch Hub implementation. In the case of\nSTEGO (Hamilton et al., 2022) and HP (Seong et al., 2023), we integrate the respective original implemen-\ntations into our framework.\nC.2\nComputational Requirements\nWe perform all experiments on a single NVIDIA A6000 GPU. The PriMaPs-EM optimization runtime varies\nbased on dataset size and the ViT architecture used as the backbone, with ViT-B/8 exhibiting the longest\nruntime and highest memory consumption. Specifically, using DINO ViT-B/8, PriMaPs-EM requires about\n2.5 hours for Cityscapes and Potsdam, and approximately 4 hours for COCO-Stuff, utilizing about 30GB\nof memory. For the lightest backbone, DINOv2 ViT-S/14, optimization times decrease to about 1.5 hours\nfor Cityscapes and Potsdam-3, and around 2.5 hours for COCO-Stuff, with a memory usage of about 20GB.\nDINO ViT-S/8 and DINOv2 ViT-B/14 lie in between.\nC.3\nDatasets\nWe close with some further details regarding the datasets used.\nCityscapes\n(Cordts et al., 2016) is an ego-centric street-scene dataset containing 5000 high-resolution\nimages with 2048 × 1024 pixels. It is split into 2975 train, 500 val, and 1525 test images. Following previous\nwork (Ji et al., 2019; Cho et al., 2021; Yin et al., 2022; Hamilton et al., 2022; Seong et al., 2023), evaluation\nis conducted on the 27 classes setup using the val split.\n25\nPublished in Transactions on Machine Learning Research (09/2024)\nCOCO-Stuff\n(Caesar et al., 2018) is a dataset of everyday life scenes containing 80 things and 91 stuff\nclasses. Following previous work (Ji et al., 2019; Cho et al., 2021; Hamilton et al., 2022; Yin et al., 2022;\nLi et al., 2023; Seong et al., 2023), we use a reduced variant by Ji et al. (2019) containing 49629 train and\n2175 test images. Hereby, all images consist of at least 75 % stuff pixels. The dataset is evaluated on the 27\nclasses setup.\nPotsdam-3\n(ISPRS) is a remote sensing dataset consisting of 8550 RGBIR satellite images with 200 × 200\npixels, which is split into 4545 train and 855 test images, as well as 3150 additional unlabeled images. In our\nexperiments, the 3-label variant of Potsdam is evaluated and the additional unlabeled images are not used.\n26\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2024-04-25",
  "updated": "2024-10-06"
}