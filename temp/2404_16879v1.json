{
  "id": "http://arxiv.org/abs/2404.16879v1",
  "title": "Learning Control Barrier Functions and their application in Reinforcement Learning: A Survey",
  "authors": [
    "Maeva Guerrier",
    "Hassan Fouad",
    "Giovanni Beltrame"
  ],
  "abstract": "Reinforcement learning is a powerful technique for developing new robot\nbehaviors. However, typical lack of safety guarantees constitutes a hurdle for\nits practical application on real robots. To address this issue, safe\nreinforcement learning aims to incorporate safety considerations, enabling\nfaster transfer to real robots and facilitating lifelong learning. One\npromising approach within safe reinforcement learning is the use of control\nbarrier functions. These functions provide a framework to ensure that the\nsystem remains in a safe state during the learning process. However,\nsynthesizing control barrier functions is not straightforward and often\nrequires ample domain knowledge. This challenge motivates the exploration of\ndata-driven methods for automatically defining control barrier functions, which\nis highly appealing. We conduct a comprehensive review of the existing\nliterature on safe reinforcement learning using control barrier functions.\nAdditionally, we investigate various techniques for automatically learning the\nControl Barrier Functions, aiming to enhance the safety and efficacy of\nReinforcement Learning in practical robot applications.",
  "text": "arXiv:2404.16879v1  [cs.LG]  22 Apr 2024\nLearning Control Barrier Functions and their\napplication in Reinforcement Learning: A\nSurvey\nMaeva Guerrier\nMISTLab\nPolytechnique Montr´eal\nMontr´eal, Canada\nmaeva.guerrier@polymtl.ca\nHassan Fouad\nMISTLab\nPolytechnique Montr´eal\nMontr´eal, Canada\nhassan.fouad@polymtl.ca\nGiovanni Beltrame\nMISTLab\nPolytechnique Montr´eal\nMontr´eal, Canada\ngiovanni.beltrame@polymtl.ca\nAbstract—Reinforcement learning is a powerful tech-\nnique for developing new robot behaviors. However, typ-\nical lack of safety guarantees constitutes a hurdle for\nits practical application on real robots. To address this\nissue, safe reinforcement learning aims to incorporate\nsafety considerations, enabling faster transfer to real robots\nand facilitating lifelong learning. One promising approach\nwithin safe reinforcement learning is the use of control\nbarrier functions. These functions provide a framework\nto ensure that the system remains in a safe state during\nthe learning process. However, synthesizing control barrier\nfunctions is not straightforward and often requires ample\ndomain knowledge. This challenge motivates the explo-\nration of data-driven methods for automatically deﬁning\ncontrol barrier functions, which is highly appealing. We\nconduct a comprehensive review of the existing literature on\nsafe reinforcement learning using control barrier functions.\nAdditionally, we investigate various techniques for automat-\nically learning the Control Barrier Functions, aiming to\nenhance the safety and efﬁcacy of Reinforcement Learning\nin practical robot applications.\nKeywords: Learning CBF, SRL, RL, Learning meth-\nods\nI. INTRODUCTION\nA. Background\nIn modern day robotics, one of the central quests\nthat has captured the attention and interest of numerous\nresearchers is to design autonomous robotic systems\nthat can operate in complex and highly dynamic en-\nvironments. However, there are several challenges that\nimpede the deployment of such robotic systems in real-\nlife scenarios such as high degree of uncertainty in\nenvironments where robots operate [23], parameter errors\nand imperfections of models describing robots [2], and\nthe increasingly complicated nature of some of these\nrobotic systems [7].\nAlthough there is a sizable body of literature on\nmodel based methods that can deal with some aspects\nof these challenges such as adaptive [1, 29] and robust\nTable I\nTABLE OF ACRONYMS\nAbbreviation\nMeaning\nML\nMachine Learning\nRL\nReinforcement Learning\nSRL\nSafe Reinforcement Learning\nCRL\nConstrained Reinforcement Learning\nCMDP\nConstrained Markov decision processes\nCBF\nControl Barrier Function\nRCBF\nRobust Control Barrier Function\nZBF\nZeroing Barrier Function\nSVM\nSupport Vector Machine\nPER\nPrioritized Experience Replay\nHCBF\nHand-Crafted CBF\nECBF\nExponential Control Barrier Function\nQP\nQuadratic Program\nHYCBF\nHybrid Control Barrier Function\nLTL\nLinear Temporal Logic\nLFD\nLearning from demonstration\nDOBs\nDisturbance Observers\nHJ\nHamitlon-Jacobi\nPACT\nPerception-Action Causal Transformer\nLiDAR\nLight Detection and Ranging\nCBVF\nControl Barrier Value Function\nGP\nGaussian Process\nDP\nDynamic Programming\nCLBF\nControl Lyapunov barrier Function\nGCBF\nGeneralized Control Barrier Function\nUTCBF\nUncertainty-Tolerant\nControl\nBarrier\nFunctions\nGCBF\nGraph Control Barrier Function\nGNN\nGraph neural networks\nPACT\nPerception-Action Causal Transformer\ncontrol [39] methods, these methods remain limited in\ntheir ability of generalizing to more complicated environ-\nments and robot dynamics due to the relative difﬁculty of\ndesigning accurate models representative of robots and\ntheir environments.\nConsequently, a lot of interest started shifting towards\nMachine Learning (ML) methods for designing control\npolicies for robots due to their ability to abstract robot\nmodels and to handle system uncertainties through proper\nuse of robot’s data, which deem them effective methods\nfor robot decision making [37, 59].\nOne speciﬁc ﬂavor of ML that has been gaining\nmuch traction in the recent years for the purpose of\ndesigning policies for robots in presence of uncertainty\nis Reinforcement Learning (RL) [69]. The main premise\nof RL is to have an agent that learns behaviors through\ntrial-and-error interactions with an uncertain environ-\nment [69], such that the agent is incentivized to pick\nactions that maximize a certain reward through its in-\nteraction with the environment. This quest for reward\nmaximization, i.e., exploitation, is coupled with a phase\nwhere the agent takes random actions to explore different\nparts of the solution space, i.e., exploration, with the aim\nof possibly ﬁnding other better policies on the long run,\ninstead of greedily maximizing the reward and possibly\ngetting stuck in local minima.\nThis interplay between exploration and exploitation\nimplies possible sacriﬁce of short term rewards, in favor\nof better long term rewards, and this is often called explo-\nration/exploitation dilemma [69, 20]. Since the training\nof a RL agent takes place for a system in an uncertain en-\nvironment, the policy coming out of the training process\nhas an inherent capacity to handle uncertainties, but due\nto the exploration/exploitation dilemma it is possible that\nan agent takes unsafe actions while exploring its solution\nspace, hoping for reward increase later.\nThis potential for taking risky actions can be a serious\nlimitation that limits the applicability of a learned policy\nin real-life [19], e.g. scenarios where safety is critical\nlike robots interacting with humans [72, 78]. Therefore,\nthis necessitates the need for endowing the RL process\nwith safety features to avoid safety violations during\ntraining and deployment. This can be generally chal-\nlenging because deciding appropriate safety criteria may\nnot be always straight forward, in addition to the fact\nthat extracting safety margins, i.e., how far the system\nis from being unsafe, in systems with high-dimensional\nobservations can introduce a lot of overhead.\nAnother motivation for the need to endow RL with\nsafety is the fact that policy training in RL often takes\nplace in simulated environments, which introduces an\nadditional challenge due to potential mismatch between\nsimulation and the real environment. This is often called\nsimulation to reality gap [27]. This gap makes it more\ntricky to deploy policies trained in simulators to real-\nlife environments, and mandates the presence of safety\nmeasures (during training and deployment) that are able\nto handle this gap.\nThis gives rise to the ﬁeld of Safe Reinforcement\nLearning (SRL) that aims to augment RL processes with\nsafety, typically through introduction of appropriate con-\nstraints that render RL policies safe during the training\nphase.\nB. Safety categorization in RL\nAccording to [9], there are three main types of con-\nstraints: 1) soft constraints that do not provide explicit\nsafety guarantees, but rather encourage system states to\nbehave in a safe manner 2) hard constraints which are\nactively enforced during the training phase, and ensure\nthat system states reside in favorable portions of the state\nspace 3) and probabilistic constraints that give a certain\nprobability limit on violating safety, and can be seen as\na middle ground between the previous two.\nOne common feature among soft constraint RL meth-\nods is that they do not explicitly alter an agent’s action\nduring exploration to prevent its state from being unsafe,\nbut rather they aim to produce a policy that eventu-\nally satisﬁes some constraints. Examples of methods to\nachieve this include manipulating rewards [71], approx-\nimate solutions of constrained MDP policy optimization\nwith upper bounds on constraint violations [3], ﬁnding\nmixed strategy distributions to constrain a reward vector\nso it eventually converges to a desired set [53] and using\nweighted sums over multiple rewards [51].\nOn the other hand, hard constraint RL methods aim to\nactively manipulate an agent’s action during exploration\nto guarantee preventing its state from becoming unsafe.\nOne popular way to achieve this manipulation is via\nintroducing a safety ”ﬁlter” or ”shields” that permit an\nagent’s action only if it is not leading to safety viola-\ntions. Some examples of such methods include using\nCBFs [26, 12] to ensure forward invariance of safe sets,\nand safety shields that can estimate reachable sets to\njudge agent’s potential of violating safety [64, 4, 21].\nProbabilistic guarantees can be introduced in several\nways, such as using Gaussian Processes (GP) to model\nthe system statistically and then use a ”safety ﬁlter”\nsetup to enforce constraints on that stochastic model with\nsome probability. One example is in [11] that uses CBF\nconstraints during the RL policy training, using a GP\nmodel. Backup policies are used in [42] with a GP model\ntrained from agent’s interaction with environment, so\nthey recover the state to a safe set around an equilibrium\npoint in case the expected state violates safety.\nSo far we can notice that one common theme in\nsafety methods in RL, through using safety ﬁlters and\nsafety shields that aim to provide hard or probabilistic\n2\nconstraints, is the notion that safety is predicated on\nthe ability to adjust RL policy to prevent the system\nfrom wandering to unsafe states, either deterministically\nor probabilistically. Our primary focus in this paper is\nsurveying SRL methods and applications where CBF is\nthe primary tool for ensuring safety (through providing\nguarantees on safe set invariance), both in training and\ndeployment.\nC. CBF methods in RL\nCBF methods have been gaining signiﬁcant amount\nof attention in the recent years as methods for certifying\nand guaranteeing safety in nonlinear systems [5]. This\nowes to their relative computational tractability, mission\nagnosticism and ability to provide guarantees on safety\nand forward invariance of safe sets, i.e. once a system\nstarts in a safe set, it stays in this safe set. These attractive\nqualities lend CBFs as adequate tools to be utilized for\nsafety in RL [76, 49, 11, 18].\nWith that in mind, CBF methods have some limita-\ntions that need some consideration to lend them more\nsuitable for SRL. One such limitation is the fact that\nbasic versions of CBFs are model-based, which need\nprior knowledge of the system model, both agent and\nenvironment, which affect their guarantees in scenarios\nwhere there is model uncertainty.[17, 22, 12] Another\nlimitation is the relative difﬁculty of crafting CBFs that\nencode desired safety for some applications, and the lack\nof a-priori knowledge of the biggest available safe set\nassociated with the desired safety speciﬁcation [5, 15].\nAlthough different methods exist for enabling CBFs\nto handle model uncertainty like Robust CBF [17, 14],\nand adaptive CBF [46, 70], the latter issue motivates the\nuse of learning methods to learn and formulate adequate\nCBFs based on system data [48, 60, 58, 62], with the\nintention of lending CBFs more suitable for integration\nin RL context.\nIn this survey we highlight various methods for en-\ndowing RL with safety guarantees, while giving special\nattention to CBF based methods for safety in RL. More-\nover, we shed some light on the body of literature related\nto learning CBFs from data, and the interplay between\nsuch methods with safety for RL during training and\ndeployment.\nD. CONTRIBUTIONS\nThe contributions of this survey are as follows:\n• We provide a comprehensive review of existing\nliterature on safe reinforcement learning using CBF.\n• We highlight the body of literature related to meth-\nods of learning CBFs from data\nE. ORGANIZATION\nThe organization of this paper is as follows. In sec-\ntion II we introduce necessary preliminaries related to\nFigure 1.\nIllustrative depiction of model-based, data-driven and the\ncombinaison of both approaches. Model-based approaches are robust,\nhowever, they are rigid and not ﬂexible to unseen scenarios. Whereas,\ndata-driven scenarios are adaptive to unforeseen cases. However, they\nare not bounded against unwanted cases. The combination of both\nmethods allow to both have the ﬂexibility of data-driven method while\nbeing able to enforce properties such as safety criteria.\nCBFs, RL and SRL, along with necessary deﬁnitions\nand concepts. In section III, we cover safe reinforcement\nlearning approaches in term of soft and hard constraints.\nThen we analyze different methods on how to construct\nCBF in section IV with a focus on learning from\nexpert demonstrations and CBF enhancement methods.\nSection V-C presents a discussion concerning limitations\nof the methods and ideas we cover in the review, and we\nend with concluding remarks in section VI.\nII. PRELIMINARIES\nA. Control Barrier Functions\nCBFs have become increasingly popular in the recent\nyears as a tool for guaranteeing safety of dynamical\nsystems [5]. In a CBF context, the deﬁnition of safety\nis associated with a continuously differentiable function\nh(x), where a system is considered to be safe if h(x) ≥\n0. Here x ∈D ⊂Rn is the state of the system, and\nD ⊂Rn is a subset of Rn where the states reside . This\nfunction h(x) can then be used to deﬁne the safe set C\nas [5]\nC = {x ∈D : h(x) ≥0}\nInt(C) = {x ∈D : h(x) > 0}\n∂C = {x ∈D : h(x) = 0}\n(1)\nwhere Int(C) is the interior of the safe set, and ∂C is its\nboundary. Therefore, maintaining safety implies the need\nfor making the safe set C forward invariant, i.e., given\nthat the system state start inside the safe set, they stay\ninside the safe set indeﬁnitely. Based on the deﬁnition of\nC in (1), this implies the need to maintain h(x) ≥0.\nMost of CBF methods are concerned with control\nafﬁne systems, for reasons to be clariﬁed shortly, that\nare deﬁned as\n˙x = f(x) + g(x)u\n(2)\n3\nwhere u ∈U ⊂Rm is the system input, U is the set\nof all admissible input values, and both f(x) and g(x)\nare two locally Lipschitz functions indicating system and\ninput dynamics respectively.\nOne way to achieve the safety requirement h(x) ≥0 is\nby manipulating the states such that ˙h(x) ≥0, however\nthis can be too restrictive, as it mandates the states to\nonly change to conservatively increase the safety margin,\nwhich affects mission execution. Instead, a more popular\nway to maintain h(x) ≥0 that is less restrictive is\nthrough achieving the following condition\nLfh + Lghu\n|\n{z\n}\n˙h(x)\n≥−α(h(x))\n(3)\nwhere Lfh = ∂h\n∂xf and Lgh = ∂h\n∂xg are the Lie deriva-\ntives of h(x) in the direction of f and g respectively.\nSeveral types of CBFs exist in the literature that pro-\nvide safety guarantees for dynamical systems in different\nscenarios like having sampled systems [66], uncertainty\nin system parameters [46], and uncertainty in system\ninput [10] to give a few examples. One of the popular\nversion of CBFs that showcase the basic concepts of safe\nset invariance using CBFs is the Zeroing Control Barrier\nFunctions (ZCBF), deﬁned as follows\nDeﬁnition II.1. For a subset W ⊂C ⊂D, a contin-\nuously differentiable function h(x) is a ZCBF if there\nexists an extended class K function α(h(x)) such that\nsup\nu∈U\n{Lfh(x) + Lgh(x)u} ≥−α(h(x)),\n∀x ∈D (4)\nHere α(h) belongs to extended class K functions if\nα(h) is strictly increasing and α(0) = 0. We can also\ndeﬁne a set of safe control inputs\nKcbf = {u ∈U : Lfh(x)+Lgh(x)u ≥−α(h(x))} (5)\nwhich means that if h(x) is a ZCBF, then any locally\nlipschitz function u ∈Kcbf will render the safe set C\nforward invariant, i.e., if x(0) ∈C ⇒x(t) ∈C,\n∀t >\n0.\nSafe control input u ∈Kcbf discussed earlier is an\ninput that can maintain x ∈C, but it is not necessarily\nthe input that a system needs to carry out a desired task.\nSuppose a control action k(x) ∈U is a nominal control\naction for a system to carry out a task, one popular way\nto endow it with safety is through using a Quadratic\nProgram (QP), where the CBF inequality (3) is a linear\nconstraint in the QP\nu∗=arg min\nu∈U\n1\n2 ∥u −k(x) ∥2\ns.t. Lfh(x) + Lgh(x)u ≥−αh(x)\n(6)\nNote the fact that the constraint in (6) is linear in\nu due to the fact that the system is control afﬁne in\ninput, which makes the optimization problem a QP.\nThis is advantageous owing to the existence of several\nnumerically efﬁcient methods for solving QP in real time.\nB. Reinforcement learning\nRL is a goal-oriented learning method that aims to\nmaximize rewards [69]. An RL agent learns directly from\ninteracting with the environment through observing the\nstate St, applying an action At, then receiving a reward\nRt+1 along with the new state St+1. This idea is depicted\nin Figure 2. Rewards can be positive, negative or neutral,\ni.e., zero. The agent then needs to adjust its action in an\nappropriate manner based on the received reward.\nIn order to generate reasonable actions that achieve a\ndesired task, an agent needs some knowledge about the\nrelationship between actions it takes and consequent re-\nwards it gets from the environment. Generally speaking,\nthis relationship could be available in two ways [54]:\n1) through a model that describes the environment and\nhelps the agent predict the relation between actions and\nrewards (model-based methods), 2) or the agent infers\nthis relationship through previous values of action-reward\npairs through repeated trials and interactions with the\nenvironment (model-free methods).\nModel-based RL methods typically use the ability to\npredict future states and rewards to ﬁnd adequate actions\neffectively. The presence of a model (readily available\nor learned from data) makes it applicable to ﬁnd proper\nactions through planning [47], or the ability to augment\nalready available data (resulting from interactions with\nthe environment) with data generated from the model\nand then using model-free techniques [40]. This ability\nto predict the environment leads to enhancing sample\nefﬁciency for model-based methods, but on the other\nhand it can cause model biasing (i.e., good control\nperformance with model, but poor results when used in\nreal systems or slightly different models) [54].\nModel-free RL methods, on the other hand, rely\nprimarily on interacting with the environment, through\napplying actions and receiving rewards to infer a re-\nlation between them, and use this knowledge directly\nto shape the actions needed to achieve a desired task.\nSuch approach can accommodate more general systems\nwithout any prior knowledge of its model, thus solving\nthe model biasing problem, but the tradeoff is that it\nneeds more data samples (i.e., more interactions with\nthe environment) to converge to a useful policy.\nIn the model-free RL paradigm, an agent generally has\ntwo choices while interacting with the environment:\n• either choose actions that help the agent better\nunderstand the underlying relationship between the\nenvironment and its action, i.e., prioritize explo-\n4\nration. This can be at the expense of sacriﬁcing\nreward maximization to a certain degree, hoping to\nﬁnd a path toward better rewards in the long run.\n• Or, choose actions that maximize the agent’s reward,\ngiven its current understanding of the system, i.e.,\nprioritize exploitation. This can be at the expense\nof getting to local maxima, meaning it can get\nstuck with locally optimal actions, while potentially\nneglecting better actions not known to the agent\nowing to its poor exploration.\nThis is\ncommonly called\nexploration-exploitation\ndilemma [20], which mandates some savvy ways of\nstriking a balance between how much an agent explores\ncompared to how much it exploits. This implies the\npossibility of taking sub-optimal or unsafe actions dur-\ning exploration, leading to undesirable behavior. When\ndeploying an agent into a robotic system safety concerns\narise due to this dilemma. In a non-stationary environ-\nment, there is a need to learn continuously and thus\nexplore. Hence the need of researching ways to guarantee\nsafety.\nFigure 2.\nIn reinforcement learning, an agent interacts with the\nenvironment. In a given state, the agent carries out an action and learns\nfrom the world by receiving a reward that is dependent on the previous\nstate and the proposed action. The agent goes from one state to another\nwith a state-transition probability that is unknown in real case scenarios.\nThe policy π(s) is a mapping of states of the environ-\nment to actions that the agent can take for a given state.\nTypically in RL the agent does not have full knowledge\nof the environment and must take decisions under uncer-\ntainty. One popular tool to model such environments is\nthe Markov decision process (MDP) described with the\ntuple M = (S, S0, A, P, R, γ), where S is a ﬁnite set of\nstates, S0 is an initial state, A is a ﬁnite set of actions,\nP : S × A 7→S is a probabilistic transition function,\nR : S × A 7→R is a reward function, and γ ∈[0, 1] is a\ndiscount factor.\nThe cumulative reward Gt is deﬁned as\nGt =\n∞\nX\nk=0\nγkRt+k\n(7)\nand here we note that if γ = 0, only immediate rewards\nare taken into account, while γ = 1 means equally\nvaluing all rewards, future and present.\nAn underlying key concept of RL is the Markov\nproperty. The Markov property states that the future is\nonly dependent of the present, in other words, only the\ncurrent state affects the next state:\nP(St+1|St, At, St−1, At−1, ...) = P(St+1|St, At) (8)\nThe majority of RL algorithms operate under this\nassumption. The presence of the Markov property within\nan environment grants the agent the capacity to predict\nboth the subsequent state and the anticipated reward,\nusing only the information from the current state. The\nagent’s goal is to ﬁnd an optimal policy π∗, which\nachieves the maximum cumulative reward Gt from all\nsates [69].\nπ∗= arg max\nπ\nE[G|π]\n(9)\nThe value of a state s under a policy π is given by\nthe state value function Vπ(s), which corresponds to the\nexpected return Gt we can get if the agent starts at state\ns and follows the policy π. The state-value function V\nis deﬁned as:\nVπ(s) =\nX\na∈A(s)\nπ(a|s)\nX\ns′∈S,\nr∈R\np(s′, r|s, a)(r + γVπ(s′))\n(10)\nThe optimal value function is written as:\nV∗(s) = max\na∈A(s′)\nX\ns′∈S,\nr∈R\np(s′, r|s, a)(r + γV∗(s′))\n(11)\nThe value of a state s given an action a is given by\nthe action value function, denoted as Qπ(s, a), which\nestimates the expected return Gt the agent can obtain\nstarting from state s, taking action a and following the\npolicy π.\nQπ(s, a) =\nX\ns′∈S,\nr∈R\np(s′, r|s, a)[r+γ\nX\na′∈A(s′)\nπ(a′|s′)Qπ(s′, a′)]\n(12)\nThe optimal action value function is denoted as\nQ∗(s, a) such that:\nQ∗(s, a) =\nX\ns′∈S,\nr∈R\np(s′, r|s, a)[r + γ max\na′∈A(s′)Q∗(s′, a′)]\n(13)\n5\nAs stated earlier, one of the core ideas behind RL\nis the exploration-exploitation trade-off, which implies\nthe possibility of exploring unfavorable actions that can\nmake the agent’s state unsafe. Typically in conventional\nRL methods, rewards do not explicitly incorporate safety,\nwhich limits the applicability of trained RL policies on\nphysical system. This gives rise to SRL that focuses on\nlearning optimal policies that satisfy safety requirements\nduring the learning and/or deployment phases.\nIII. SAFE REINFORCEMENT LEARNING\nA. SOFT CONSTRAINTS METHODS\nIn this section, we give a brief overview of the liter-\nature related to SRL methods that adopt soft constraint,\nas deﬁned in section I-B. We deliberately include such\nconcise account for literature in this domain for com-\npleteness, but more detailed treatment of the literature\ncan be found in [25] and [9].\nOne common characteristic in such methods is that\nthere are no explicit guarantees on safety during training,\nbut rather they encourage policies to achieve safety in\nthe long run. For example, [32] makes the RL policy\nsearching risk aware by augmenting the cost function\nwith a probabilistic risk model, and permits low impact\nsafety violations to learn this model so it prevents high\nimpact violations.\nSafety critics are presented in [67], where a pre-\ntraining phase aims to let a SAC agent explore unsafe\nstates so it can train a safety critic and a policy, followed\nby a ﬁne tuning phase where a new policy is trained while\nusing the safety critic.\nSeveral methods depend on using CMDPs as a way\nof introducing constraints [3, 71, 53] during policy con-\nstruction. In [3] the authors present a method in which the\ncost function for policy optimization and the constraints\nare replaced with surrogate function based on upper\nbounds of objectives’ divergence under policy change\n(both for main and constraint costs), which ensures\nmonotonic improvement of policy with respect to con-\nstraints. In [71] the CMDP based policy optimization is\ntackled using Lagrange relaxation to produce a penalized\nreward function that is then used to train a ”constrained”\nactor-critic, which converges to a policy that satisﬁes\nconstraints.\nIn previous works safety is encoded somehow in costs\nor rewards, but a different approach is taken in [53]\nwhere safety is deﬁned as eventual convergence of a\nmeasurement vector to a desired set. To this end, the\nauthors use a game theoretic framework to produce a\nmixed strategy probability distribution over a set of\npolicies, which eventually decreases the distance between\nlong term reward vector and the safe set.\nAnother approach to SRL is learning through demon-\nstration. In [52] a method is presented that uses expert\ndemonstrations to learn a safety critic that mimics the\nbehavior of a CBF and augments a PACT architecture,\nsuch that the safety critic optimizes a proposed action\nto achieve safety. Although this method facilitates the\nlabeling of demonstrations and has the ability to learn\ndifferent safety deﬁnitions implicitly, it does not give\nguarantees on safety, and it might be vulnerable to out\nof distribution data or getting stuck at local minima in\nthe action optimization phase.\nOne interesting approach is proposed in [27] which\naims to reduce the effect of reality gap while adding\nsafety through jointly training a backup policy, along\nwith a performance policy and a discriminator, all con-\nditioned on a latent variable to enhance generalization.\nThe discriminator chooses the appropriate policy to avoid\nsafety violations. The policies obtained in the simulation\nphase are then ﬁne tuned in a controlled environment\nsetup to enhance safety before deployment in real world.\nB. HARD and PROBABILISTIC CONSTRAINTS METH-\nODS\nUnlike methods adopting soft constraints, methods\nenforcing hard safety constraints aim to give guaran-\ntees that ensure system safety throughout training and\ndeployment, while probabilistic constraints qualify such\nguarantees to be within a certain desired probability\nrange [9]. We note that such methods depend largely on\nhaving some form of model that describes the system,\neither known [26] or learned [18, 12].\n1) Using safety shields: One idea that has garnered\nmuch attention by many researchers recently is the use\nof safety shields [4, 21, 64, 27, 78] Figure 3 shows a\nschematic of a basic safety shield architecture. Herein,\nwe use the notion of a safety ﬁlter and a safety shield\ninterchangeably. The core idea of as safety shield is that\nit monitors an action before being applied to the system,\nand it modiﬁes this action only in case it is expected to\nviolate safety.\nThe work in [4] introduces an unsafe action ﬁltration\nbased on the concept of a ﬁnite-sate reactive system or\nMealy machine [63] and Linear Temporal Logic. The\nmain idea is to construct a two player game between\nthe system and environment, and use it to estimate a\nwinning region, i.e., subset of state space where states\ncan always be recovered to safety by a backup controller.\nThe actions are then minimally modiﬁed and punishment\nsignals are used to enhance the RL policy training. The\nmain limitation in [4] is that it only considers discrete\nsets and spaces, whereas real-life scenarios are in a\ncontinuous space.\nThe same shield construct is extended to a multi-\nagent setting by [21], which presents a centralized shield\nthat monitors the states of all agents. This implies that\nthe problem grows in complexity with more robots. To\n6\nFigure 3. An illustrative example of how a ﬁlter function. The agent\ninteract with the environment by proposing an action at a given state\nand shift to the next state given an unknown state-transition probability.\nThe action is monitored by the ﬁlter while training and on deployment.\nThe ﬁlter maps unsafe actions to safe ones to maintain system safety.\nIf the ﬁlter has to change the action, the agent is aware of that fact and\nlearns form its mistake, otherwise, it directly receives the reward from\nthe environment.\nremedy this issue, the authors also introduce a set of\nfactorized shields, each monitoring a subset of the state\nspace such that each agent is covered by at least one\nfactorized shield. Agents switch shields near boundary\nstates between these shields to ensure coordination.\nThe method in [64] adopts a similar framework as [4],\nbut it decreases the complexity by learning a function\nas the shield directly, which predicts a binary outcome\n(safe/unsafe) based on observations. In order to reach the\ngoal, a planner system combined with the shield proposes\nintermediary goals that are inherently safe.\nAnother example of using shields is presented in [78],\nwhere a shield is combined with the SARSA algorithm\n[69]. Upon choosing an action that is deemed unsafe by\nthe shield, the action is removed from the action set and\nthe exploration strategy is used to pick another action. In\na similar fashion, the authors of [42] have embedded the\nshield concept within a policy, that overrides the current\naction upon constraint violation. The shield policy infers\nsafety by checking the next state degree of recoverability.\n2) USING CBF SAFETY FILTERS: Control Barrier\nFunctions have gained a lot of attention in the past decade\nand have become very popular tools for enforcing safety\nconstraints on dynamical systems [5]. Over the past few\nyears we notice a consistent increase in the domains\nof applications of such tool, but in this paper we are\ninterested in CBFs from an ML and RL perspective.\nMost common CBF formulations are model-based in\nnature, implying the need for a model to estimate CBF\ngradients and be able to derive safe actions. However, in\nan RL context it is common to not have a-priori known\nsystem models. Consequently, several CBF based SRL\nimplementations use uncertainty-aware CBFs. Typically,\nsuch methods use a nominal system model, and treat\ndifferences from the actual system as disturbances.\nSeveral works [11, 13], to cite a few, use CBFs to\nenforce safety during the training of RL agents. The au-\nthors of [11] proposes a method for augmenting a generic\nRL model-free method with safety during training using\nCBFs safety ﬁlters. Their method depend on a nominal\nsystem model with a GP to model the disturbance, and\nsuch framework allows for having probabilistic guaran-\ntees. Moreover, they propose a method to incentivize the\ntrained policy to produce safer actions through incorpo-\nrating the history of previously ﬁltered actions. In line\nwith the approach of using GP to learn disturbance, the\nauthors of [22] utilize the learned disturbance estimates\nto make subtle adjustments to the actions of their policy,\nensuring system safety with minimal alterations.\nDisturbance observers (DOBs) provide effective tools\nto deal with uncertainties [13]. In this work, the authors\npresent a high relative degree CBF formulation with\ndisturbance, and adopt a few assumptions about the\nboundedness of the disturbance and its derivative. This\nenables formulating a predictor for the upper bound of\ndisturbance and incorporate such function in the CBF\nconstraint that ﬁlters RL actions during training. The safe\naction and consequent reward are then added to a buffer\nthat is used in the RL training process to enhance the\nﬁnal policy.\nAddressing\nuncertainties,\nthis\npaper\n[76]\nintro-\nduces Uncertainty-Tolerant Control Barrier Functions\n(UTCBFs), a novel CBF category that accommodates\nmodel uncertainty while ensuring provable safety guar-\nantees. For systems featuring unknown dynamics, model\ndynamics and state predictions are derived using GP. Ini-\ntially, the UTCBF constraint is conservative during early\nexploration due to a large uniform error bound caused\nby model uncertainty. As more data is gathered, the error\nbound converges, gradually relaxing the restrictiveness of\nthe constraint. The policy is updated through constraint\noptimization.\nOne notable limitation identiﬁed by [49] is CBF\ninapplicability to dynamic systems with high relative\ndegrees. To address this issue, the authors introduce the\nGeneralized Control Barrier Function (GCBF), a mod-\niﬁcation designed speciﬁcally for handling constraints\nhigh degree systems. The GCBF restricts policy updates\nthrough a model-based constrained policy optimization\napproach, effectively mitigating the challenges posed by\nhigh relative-degree dynamics. The authors of [26] also\nacknowledge that traditional CBFs may not be suitable\nfor systems with high relative degree. In this context,\ncustomized CBFs, speciﬁcally exponential barrier func-\ntions (ECBFs), designed to handle high relative degree\nsystems, are employed. The RL agent’s proposed traction\ntorque actions undergo a safety ﬁltering process using\nECBFs before being applied to the system.\n7\nIV. CBF CONSTRUCTION\nFrom the discussion in section III-B2 we notice that\ngenerally speaking, methods utilizing CBFs to enforce\nsafety for RL policies need to accommodate uncertain-\nties [11, 12]. Moreover, in many scenarios constructing\nthe CBF for RL policy training is challenging [52, 75]. To\nthat end, it is of order to give a brief review of different\nmethods that aim to construct and adapt CBFs using data,\nwhich we ﬁnd applicable and relevant to CBF application\nin RL domains.\nA. CBF CONSTRUCTED WITH MACHINE LEARNING\nMETHODS\n1) RL based methods: Different ML can be used to\ndesign and infer CBFs from data. One such popular\ndirection is via using RL. For example, the method\nfrom [75] uses inverse RL, which originally infers a\nreward function from data. An expert provides demon-\nstration data for safe and unsafe actions, and a CBF\nis constructed as a neural network. Loss functions are\ndesigned to mimic the behavior of an actual CBF, and\nthe minimization of which leads to a CBF that can\ndiscriminate between safe and unsafe sets.\nIn a fashion similar to [75], the framework from [56]\nconstructs a CBF through proper use of adequate loss\nfunctions that enforce properties of a CBF. However,\nit differs from [75] in that instead of using explicit\nexpert demonstrations as input data, it collects data from\nsystem rollouts using the current policy from the recent\nepisode, then updates the policy and recollect data,i.e.,\nit adopts an on-policy training strategy. Moreover, it\napplies methods for rendering observations quantity and\npermutation invariant, and it adopts a setup inspired from\na CBF-QP framework [5] to reﬁne the policy.\nThe work in [18] aims to learn CLBFs, which are\nfunctions similar to CBFs, albeit with different properties\nthat ensure both safety and tracking simultaneously. The\nmethod is based on modifying the Bellman equation of\nthe state value function such that it respects properties of\na CLBF. The core idea is to learn the state value function\nunder CLBF constraints and ﬁnd the policy according to\nthat function. The learned policy is used as a critic in an\nactor-critic setting.\nIn [62] the authors use the RL value function to con-\nstruct a CBF and directly incorporates safety in rewards.\nThe rewards are only given at the end of an episode,\nhowever, this implies that the agent cannot learn from\nintermediate failures and successes.\nThe authors of [68] created a framework that con-\nstructs a CBF based on LiDAR data. The information\non the environment retrieved by the LiDAR outlines the\nset of safe and unsafe samples. The CBF is synthesize\nas a two hidden layer Gaussian Kernel neural network,\nthe ﬁrst being a kernel machine and the second a SVM.\nThe work in [77] presents a distributed framework\nfor safe multi-agent scenarios. They introduce the Graph\nControl Barrier Function framework denoted GCBF+,\nutilizing graph neural networks (GNNs) to parameterize\na candidate GCBF. To enable goal-reaching behavior,\na goal node and edges between each agent and their\nrespective goal are incorporated into the input features.\nTraining involves minimizing the sum of the CBF loss for\nboth the GCBF and the distributed controller. Alternative\nmethodologies [27, 21, 78] often grapple with a con-\nﬂicting reward-penalty framework, primarily because, in\nunsafe scenarios, the backup or nominal policy override\nthe optimal one, leading to suboptimal behavior. How-\never, in this case, their [77] loss formulation integrates\nboth safety and optimization within a uniﬁed policy,\neffectively sidestepping the trade-off dilemma between\ncollision avoidance and goal reaching.\nSafety critics could be used to construct CBF-like\nfunctions from expert demonstrations to endow RL poli-\ncies with safety on the long run [52]. Such methods have\nrelatively simple structure and allows for easy labeling of\ndemonstration data, however, they are vulnerable to get-\nting stuck in local minima during the policy optimization\nphase as discussed in section III-A.\n2) Learning CBF from demonstrations: Another set\nof popular techniques for CBF construction is through\nlearning from demonstrations. The work in [60] intro-\nduces a method for estimating a set of ZCBFs, each\nmodeled as a linear combination of states, from incoming\ntraining data. This framework adopts a method of incre-\nmentally clustering input points into subsets of unsafe\nstates, then ﬁts a number of ZCBFs for each of these\nclusters, and uses these ﬁtted functions to enforce safety\non the system.\nIn [58] the CBF is synthesized by learning from a\ndataset of safe and unsafe trajectories. Given the set of\nexpert trajectories, data points are extracted such that\nstate-action pairs present safe behaviors from demonstra-\ntions and with those points a safe set is learned.\nUnlike other methods that use both safe and unsafe\ndemonstrations [75, 52], the work in [44] presents a\nway for constructing a CBF for hybrid systems while\nusing only demonstrations of safe behaviors. Instead\nof having explicit unsafe demonstrations, the authors\ndeﬁne an unsafe set outside of safe demonstrations and\nuse constrained optimization to ﬁnd a CBF, modeled as\nDNN, that satisﬁes CBF properties, i.e. equations (1)\nand (4).\nThe framework from\n[45] extends the results from\n[44] by taking into account model and state estimation\nerrors. To that end, a Robust Output Control Barrier\nFunction (ROCBF) is constructed from expert demon-\nstrations through constrained optimization in a similar\nfashion to\n[44], with similar deﬁnitions of safe and\n8\nTable II\nOVERVIEW OF SRL METHODS FOCUSED ON CBFS. ONLINE: REAL-TIME STREAM OF DATA. OFFLINE: STATIC DATASET.\nReferences\nDeployed\nModel-base\nModel-free\nOnline\nOfﬂine\nGuarantees\n[78, 21, 12, 26, 72]\n✓\n✓\nHard constraints\n[27, 42]\n✓\n✓\n✓\nHard constraints\n[75]\n✓\n✓\nSoft constraints\n[18]\n✓\n✓\nSoft constraints\n[22]\n✓\n✓\n✓\nSoft constraints\n[76]\n✓\n✓\nProbabilistic constraints\n[11]\n✓\n✓\nProbabilistic constraints\n[49]\n✓\n✓\n✓\nProbabilistic constraints\nunsafe regions. The ROCBF can be then used to en-\nforce safety due to its ability to account for model and\nmeasurement uncertainties.\nGPs are used in [36] to model and construct CBFs\nfrom demonstration data. In this work, Gaussian CBFs\nare presented, which aims to produce a posterior with\na positive value for regions in state space with high\nconﬁdence. To that end, data samples from the safe\nsets directly shapes the CBF structure, which becomes\nnegative in areas where less data is collected (due to\nbeing unsafe). Such structure allows for great ﬂexibility\nin CBF formulation, as well as online construction and\nenforcement of safety with these CBFs.\nThe authors of [30] undertake the learning process\nof a GP and its associated conﬁdence while employing\nSatisﬁability Modulo Theories solvers to ﬁnd a valid\ncontrol barrier functions and the corresponding control\npolicies within the input space. This learned GP aid in\ncomprehending the unknown control afﬁne nonlinear dy-\nnamics and addressing uncertainties inherent in a learned\nmodel for the control barrier functions. The work in [34]\nalso addresses the limitations of CBF when confronted\nwith model uncertainties. The proposed method in [34]\ntackles this challenge by acquiring insights into the\nunmodeled dynamics through the utilization of GP. It\nintegrates the Gaussian CBF approach and formulates it\nwithin the context of an online QP optimization problem.\nFurthermore, the authors [35] tackle as well the chal-\nlenge of model uncertainties and their inﬂuence on\nCBF. These uncertainties are captured by calculating the\nposterior variance through GP, conditioned on previous\nmeasurement states. They integrate a Gaussian CBF,\nincorporating safety uncertainty by utilizing the posterior\nvariance derived from past measurement data. The sys-\ntem input is compared against the Gaussian-CBF framed\nas a QP problem, and adjustments are made if necessary.\nExpanding upon the same concern in a decentralized\nmulti-robot context, [28] integrates the assessment of\nindividual model uncertainty through GP models. This\nensures that each robot can ensure safety with a high\nlevel of conﬁdence. The formulation of safe controllers\ninvolves solving a QP problem, constrained by decen-\ntralized robust CBF conditions, guided by the GP un-\ncertainty estimates. Another strategy involves acquiring\nknowledge about the attraction region directly for non-\nlinear systems. For example, the methodology presented\nin [6] relies on non-parametric Bayesian frameworks like\nGP and Bayesian Optimization to assess and broaden the\nsafe set.\n3) Using CBF priors: One main feature of basic CBFs\nis the fact that they are myopic in nature, meaning\nthat they only care about enforcing safety regardless of\nmission execution. Additionally, the extent of the safe\nset to which the system states are conﬁned is implicitly\nimplied by the choice of CBF candidates, which might\nbe too conservative and not reﬂect a potentially bigger\ntrue safe set of states.\nTo that end, several methods exist in the literature\nthat aim to enhance the system performance and fea-\nsibility [24, 50], as well as ﬁnding CBF candidate with\nexpanded safe sets [41, 15]. The main idea behind these\nmethods is to start by some known CBF with some\ntunable factors, which are trained using different ML\ntechniques to achieve the aforementioned goals.\nThe authors of [74] suggest a learning-enabled method\nfor developing local CBFs to ensure the safety of diverse\nnonlinear hybrid dynamical systems. Their goal is to\ntackle the curse of dimensionality in high-dimensional\nsystems. In their approach, they contemplate the incor-\nporation of both safe and unsafe system conﬁgurations\nwhen updating the CBF. The CBF is directly adjusted\nbased on the dynamically computed new unsafe set using\ndynamic programming and is employed to guide hybrid\nsystems.\nThe work in [24] addresses the issue of potential\nreduction in system performance and lack of feasible so-\nlutions (i.e. potentially conﬂicting constraints, or system\ngetting stuck due to lack of safe actions that progress\n9\nthe mission) with a ﬁxed extended class K function in\nthe CBF constraint structure, i.e., a ﬁxed α(h) function in\n(3). The proposed framework represents the α function as\na GNN and trains an RL policy that regulates α to control\nhow conservative or aggressive the constraint behavior\nis. This policy aims to increase the chance of having\nfeasible solutions, while enhancing the overall system\nperformance and generalizability to new environments.\nSimilarly in [50], the authors aim to enhance applica-\nbility of CBF constraints to new environments, as well\nas system performance. However, they use Exponential\nCBFs to account for systems with high relative degree.\nThe framework laid out in [50] trains an agent that ma-\nnipulates the CBF constraint structure at higher relative\ndegree, in a manner similar to [24], to eventually enhance\nsystem performance, reduce control effort and enhance\nfeasibility.\nBesides the different methods of constructing CBFs we\ndiscussed previously, an additional approach is through\nthe reﬁnement of “less accurate” CBFs. For instance, [15]\nstarts by using a handcrafted CBF (HCBF), along with\nits safe set, as an initial guess for the true safe set. This\ncandidate is then augmented with an increment that aims\nto bring the CBF closer to the true, possibly bigger, safe\nset. The increment is modeled with a Deep Differential\nNetwork (DDN) and trained based on sampled data for\nsafe and unsafe states.\nThis framework was further extended in [16], which\nproposes a method for sampling data points based on\nPrioritized Experience Replay, leading to a better sample\nefﬁciency and needing less samples to train the CBF\nincrement.\nThe main theme in [15] and [16] is that we start by a\nconservative estimation of a safe set that is smaller than\nits true volume and the goal is to expand it. Conversely,\nin [41] the main theme is to start from a bigger set,\nwhich is not forward invariant and deﬁned by a set of\nconstraints, and shrink it to converge to the true safe\nset. To do so, an approximate CBF representation is\nobtained from the original constraints through scaling\nand offsetting the state variables, then safe and unsafe\ndata points are then sampled to help modifying the scale\nand offset of states in a manner that “trims” the bigger\nset and makes it converge to the true safe and forward\ninvariant set.\nIn [48], the authors leverage an approach that itera-\ntively learns barrier certiﬁcates with the help of a prior\nHCBF. However, the authors did not address the issue of\nhaving a limited amount of data,\nNone of the above works acknowledge the issue of\nupdating an invalid CBF. [73] addressed this concern\nwith a framework that allows the reﬁnement of invalid\nand overly conservative CBFs. The limit of their work is\nthe need of knowing the obstacles dynamics. The table III\ngives an overview of the CBF learning methods discussed\nabove.\nV. DISCUSSION\nA. MODEL-FREE VS MODEL-BASED\nFrom the discussion in section III we notice that\nin general the ability to provide hard or probabilistic\nsafety guarantees is contingent on having some form of\nsystem model, either known a-priori [26, 11, 78, 60], or\nlearned [76, 11, 22, 30, 35]. Moreover, the strength of\nguarantees depend directly on the ﬁdelity of the adopted\nmodel. Some methods ameliorate model imperfections,\nthat can potentially cause safety violations, through\nmaking assumptions about worst case disturbances [12],\nlearn such disturbances during training [22], or use\nsampling [42].\nWe also notice that in general, model-based RL meth-\nods can be more convenient to produce hard and prob-\nabilistic guarantees, especially with CBFs [49, 76, 22]\ndue to existence of system models that can help pre-\ndicting future states for a given action, in addition to\nestimating gradients of CBFs, which allows formulating\nsafe actions. However, the same observation holds true\nfor systems using model-free RL methods but with an\nadded model speciﬁcally for the purpose of ensuring\nsafety [12, 4, 26, 42].\nMethods that use actor-critics are highly dependable\non the learned critic, if the latter does not properly take\ninto account safety this will hinder the ﬁnal policy. One\nof the shortcomings of both approaches [62, 18] is the\nexploitation-exploration dilemma of RL, that still allow\nthe agent to brake safety criteria within a small margin.\nA response to that problem could be to only act greedily\nhowever this could lead to local optimum.\nModel-based RL methods have an advantage regarding\nsample efﬁciency in comparison to model-free RL meth-\nods, however, they have problems with model bias and\npoor generalisability [69]. Although model-free methods\ndo not suffer model bias, they are mostly associated with\nsoft constraints, provided they are not augmented with\nsystem models speciﬁcally for safety [32, 67, 3, 71].\nThis owes to the lack of predictive power from models,\nand limiting knowledge of environment on immediate\ninteractions.\nDue to the complete independence from models in\nmodel-free RL, some of these methods might permit\nsafety violations to learn safety on the long run [32, 67],\napproximate solutions of CMDPs [3] which might cause\nsafety violations, or encoding the constraints in the\nreward [71], aiming to converge to safe behaviors asymp-\ntotically.\nB. Effect of safety on policy performance\nAdding safety into account during training of RL\npolicies has several effects on the overall outcome of\n10\nTable III\nOVERVIEW OF CBF LEARNING METHODS. ONLINE: REAL-TIME STREAM OF DATA. OFFLINE: STATIC DATASET.\nReferences\nDeployed\nOnline\nOfﬂine\nLFD\nML\nCBF Prior\nComments\n[45, 52, 58]\n✓\n✓\nThese works rely on learning CBF through expert demon-\nstrations.\n[60]\n✓\n✓\n✓\nIncorporates a kinesthetic teaching strategy, the authors opted\nfor a ﬂexible addition or removal of learned constraints.\n[44]\n✓\n✓\nTackles the expansion of safe sets in the context of CBF\nunder the inﬂuence of model uncertainty.\n[24]\n✓\n✓\n✓\nExpands upon the application of CBF, their study further\nincorporates their utilization to guarantee safety in a decen-\ntralized manner within a multi-robot system.\n[62, 68, 50]\n✓\n✓\nThe authors express a concern regarding the acquisition of\na broadly applicable CBF through machine learning method-\nologies.\n[26]\n✓\n✓\nConsiders safety as a priority in the context of reinforcement\nlearning, moreover, the authors give due consideration to the\npreservation of energy.\n[35]\n✓\n✓\n✓\nThis study addresses the model’s uncertainty by leveraging\nreal-time measurements of system states, while incorporating\nthe uncertainty measure into a Gaussian process.\n[28, 34, 30, 6]\n✓\n✓\nUses Gaussian Processes to learn the underlying model\nuncertainty and incorporate it in the CBF formulation.\n[65]\n✓\n✓\nThe objective of this study is to achieve a ﬂawless training\nprocess with no safety violations by progressively enhancing\na minimal CBF through iterative development and reﬁnement.\n[48, 73, 15, 16]\n✓\n✓\nThe authors strive to acquire an improved CBF through data\nbased on a minimal CBF, all the while ensuring that safety\nconstraints remain unbreached.\n[18]\n✓\n✓\n✓\nEmploys a blend of control barrier and Lyapunov function\nstrategies to build upon a minimal valid CBF.\nthese policies. We are interested in giving some insights\ninto the effect of such safety consideration on three\nmain aspects, namely 1) sample efﬁciency and speed of\nconvergence, 2) policy overall safety 3) and ﬁnal task\nexecution quality. We are ﬁrst shed some light on the\neffect of adding different types of safety constraints (soft,\nhard, probabilistic) on the aforementioned metrics, and\ncontrast that with overall performance of RL methods\nthat do not consider safety. Moreover, we compare\nbetween the overall performance of safe RL methods\nenforcing different strengths of constraints in the light\nof the previously mentioned metrics.\nOne thing we can notice consistently is the increased\nsample efﬁciency and speed of policy convergence, as\nwell as policy safety, of different safe RL methods\napplying hard [22, 4], probabilistic [11, 49] and soft [52]\nconstraints in comparison to their non-safe counterparts.\nFor example, the results in [22] show an increased sample\nefﬁciency of a SAC augmented with robust CBF during\ntraining. The results in [4] show an improvement both\nin convergence speed and safety during training for the\nproposed shielded RL agent compared to the unshielded\none. Original TRPO [61] and DDPG [43] policies were\ncompared to their safety augmented counterparts in [11]\nand the results show a signiﬁcant improvement in both\nsample efﬁciency and safety. Similarly, [49] shows better\nadherence to safety and convergence speed of the modi-\nﬁed MBPO compared to an original MBPO [31].\nAlso introducing soft constraints to the RL learning\nprocess leads to an enhanced safety and sample efﬁciency\nas in [52] that shows an added advantage in achieving\nsafety compared to original version of PACT [8]. The\ntypical improvement in sample efﬁciency could be at-\ntributed to the fact that adding safety constraints, espe-\ncially hard and probabilistic, trims down the available\naction space, and consequently the state space, leading\nthe agent to explore less “non-useful” parts of the state\nspace, while being guided towards achieving the goal.\nIn addition to enhancing sample efﬁciency and safety\nperformance, adding safety during training also in many\noccasions has an advantage in terms of the overall reward\nduring training [11, 52, 4].\nC. Challenges and limitations\nHerein, we provide a brief discussion of challenges\nin the ﬁeld of safe RL in the light of the discussions\npresented so far, as well as potential opportunities for\nfuture research.\n1) Effect of sim2real gap on safety: Throughout this\nstudy we notice that most of the development of safe RL\n11\npolicies takes place mostly in simulated environments,\nmuch like normal RL. The inevitable mismatch between\nsimulation models and real environments can be a source\nof several issues regarding the applicability and validity\nof safe RL policies.\nSome popular solutions that we can ﬁnd in the liter-\nature to deal with this issue include domain randomiza-\ntion [57] and optimization of simulator parameters [33].\nHowever, there are few works, e.g. [26], that deal with\ncertiﬁcation of safe RL policies during deployment in\na fashion that takes into account this reality gap. The\ncertiﬁcation we ﬁnd in literature is mostly during training\nin the context of hard/probabilistic constraints. There\nare few attempts to tackle this issue (safety certiﬁcation\nduring deployment) in the context of soft constraints,\ne.g. [27], but not for more strict constraints.\nRegular safety ﬁltration, using robust and adaptive\nCBF methods for example, can be used as an external\nsafety layer applied to the output of the policy. However,\nsuch methods can be arbitrarily conservative in a manner\nthat does not match the conservativeness during training,\nwhich can affect the overall performance. We think that\nthis aspect of safety certiﬁcation of RL methods during\ndeployment could be a future direction of research.\n2) Generalizability and zero shot performance: It is\nworth mentioning that one important aspect that can asses\nthe overall quality of safe RL policies is their zero-\nshot performance, i.e. the validity of RL policies when\ndeployed in different simulated environment from the one\nused during training. We notice that few works consider\nthis aspect of assessment. Some of the few examples\nthat give some insight to zero shot [38, 55] performance\ninclude [22]\nVI. CONCLUSION\nReinforcement learning is a powerful tool that gained\nattention in the past couple of decades due to their\nsigniﬁcant versatility. Such potential will beneﬁt from\nmethods to endow RL policies with safety to enhance\nthe training process and bolster their applicability in real\nlife. In this short review we focus on safe RL methods\nthat attempt to provide policies with safety features, with\na speciﬁc interest in methods that use CBFs as a part of\nsafety ﬁlters during training.\nWe outline several types of safety constraints in lit-\nerature and show where CBF based methods fall, and\nwe give some examples of works using CBFs for the\npurpose of safety in RL. We also give an outline of the\nliterature related to learning CBFs from data, since this\nis a common aspect in the use of CBFs for safe RL.\nLastly, we discuss some insights from our observation\nof the literature.\nWe believe that the ﬁeld of safe RL, and more specif-\nically the use of CBFs for safety certiﬁcation of RL\nmethods, will keep getting more attention in the years\nto come, and researchers will tackle several issues that\ninclude linking certiﬁcation in training with deployment\nto reduce the effect of the sim2real gap, devise more sam-\nple efﬁcient methods for building CBFs from data, and\nimprove the generalization and zero shot performance of\nsafe RL methods.\nREFERENCES\n[1] Satoko Abiko and Gerd Hirzinger. Adaptive control\nfor a torque controlled free-ﬂoating space robot\nwith kinematic and dynamic model uncertainty. In\n2009 IEEE/RSJ International Conference on Intelli-\ngent Robots and Systems, pages 2359–2364. IEEE,\n2009.\n[2] Ian\nAbraham,\nAnkur\nHanda,\nNathan\nRatliff,\nKendall Lowrey, Todd D Murphey, and Dieter Fox.\nModel-based generalization under parameter uncer-\ntainty using path integral control. IEEE Robotics\nand Automation Letters, 5(2):2864–2871, 2020.\n[3] Joshua Achiam, David Held, Aviv Tamar, and Pieter\nAbbeel.\nConstrained policy optimization.\nIn In-\nternational conference on machine learning, pages\n22–31. PMLR, 2017.\n[4] Mohammed Alshiekh, Roderick Bloem, R¨udiger\nEhlers, Bettina K¨onighofer, Scott Niekum, and\nUfuk Topcu.\nSafe reinforcement learning via\nshielding.\nIn Proceedings of the Thirty-Second\nAAAI Conference on Artiﬁcial Intelligence and\nThirtieth Innovative Applications of Artiﬁcial Intel-\nligence Conference and Eighth AAAI Symposium\non Educational Advances in Artiﬁcial Intelligence,\nAAAI’18/IAAI’18/EAAI’18. AAAI Press, 2018.\nISBN 978-1-57735-800-8.\n[5] Aaron D. Ames, Samuel Coogan, Magnus Egerst-\nedt, Gennaro Notomista, Koushil Sreenath, and\nPaulo Tabuada. Control barrier functions: Theory\nand applications. CoRR, abs/1903.11199, 2019.\n[6] Felix Berkenkamp, Riccardo Moriconi, Angela P.\nSchoellig, and Andreas Krause. Safe learning of re-\ngions of attraction for uncertain, nonlinear systems\nwith gaussian processes.\nCoRR, abs/1603.04915,\n2016.\n[7] Sarthak Bhagat, Hritwick\nBanerjee, Zion\nTsz\nHo Tse, and Hongliang Ren. Deep reinforcement\nlearning for soft, ﬂexible robots: Brief review with\nimpending challenges. Robotics, 8(1):4, 2019.\n[8] Rogerio Bonatti, Sai Vemprala, Shuang Ma, Felipe\nFrujeri, Shuhang Chen, and Ashish Kapoor. Pact:\n12\nPerception-action causal transformer for autoregres-\nsive robotics pre-training. In 2023 IEEE/RSJ In-\nternational Conference on Intelligent Robots and\nSystems (IROS), pages 3621–3627. IEEE, 2023.\n[9] Lukas Brunke, Melissa Greeff, Adam W. Hall,\nZhaocong Yuan, Siqi Zhou, Jacopo Panerati, and\nAngela P. Schoellig.\nSafe learning in robotics:\nFrom learning-based control to safe reinforcement\nlearning, 2021.\n[10] Jyot Buch, Shih-Chi Liao, and Peter Seiler. Robust\ncontrol barrier functions with sector-bounded un-\ncertainties. IEEE Control Systems Letters, 6:1994–\n1999, 2021.\n[11] Richard Cheng, G´abor Orosz, Richard M. Murray,\nand Joel W. Burdick.\nEnd-to-end safe reinforce-\nment learning through barrier functions for safety-\ncritical continuous control tasks.\nIn Proceedings\nof the Thirty-Third AAAI Conference on Artiﬁ-\ncial Intelligence and Thirty-First Innovative Ap-\nplications of Artiﬁcial Intelligence Conference and\nNinth AAAI Symposium on Educational Advances in\nArtiﬁcial Intelligence, AAAI’19/IAAI’19/EAAI’19.\nAAAI Press, 2019. ISBN 978-1-57735-809-1. doi:\n10.1609/aaai.v33i01.33013387.\n[12] Yikun Cheng, Pan Zhao, and Naira Hovakimyan.\nSafe\nmodel-free\nreinforcement\nlearning\nusing\ndisturbance-observer-based control barrier func-\ntions, 2022.\n[13] Yikun Cheng, Pan Zhao, and Naira Hovakimyan.\nSafe and efﬁcient reinforcement learning using\ndisturbance-observer-based control barrier func-\ntions.\nIn Learning for Dynamics and Control\nConference, pages 104–115. PMLR, 2023.\n[14] Max H. Cohen, Calin Belta, and Roberto Tron.\nRobust control barrier functions for nonlinear con-\ntrol systems with uncertainty: A duality-based ap-\nproach. In 2022 IEEE 61st Conference on Decision\nand Control (CDC), pages 174–179, 2022.\ndoi:\n10.1109/CDC51059.2022.9992667.\n[15] Bolun Dai, Prashanth Krishnamurthy, and Farshad\nKhorrami. Learning a better control barrier func-\ntion, 2022.\n[16] Bolun Dai, Heming Huang, Prashanth Krishna-\nmurthy, and Farshad Khorrami. Data-efﬁcient con-\ntrol barrier function reﬁnement, 2023.\n[17] Ersin Das¸, Skylar X. Wei, and Joel W. Burdick.\nRobust control barrier functions with uncertainty\nestimation, 2023.\n[18] Desong\nDu,\nShaohang\nHan,\nNaiming\nQi,\nHaitham\nBou\nAmmar,\nJun\nWang,\nand\nWei\nPan. Reinforcement learning for safe robot control\nusing control lyapunov barrier functions, 2023.\n[19] Gabriel Dulac-Arnold, Daniel Mankowitz, and\nTodd Hester.\nChallenges of real-world reinforce-\nment learning, 2019.\n[20] Yonathan Efroni, Shie Mannor, and Matteo Pirotta.\nExploration-exploitation in constrained mdps, 2020.\n[21] Ingy Elsayed-Aly, Suda Bharadwaj, Christopher\nAmato, R¨udiger Ehlers, Ufuk Topcu, and Lu Feng.\nSafe multi-agent reinforcement learning via shield-\ning. In Frank Dignum, Alessio Lomuscio, Ulle En-\ndriss, and Ann Now´e, editors, AAMAS ’21: 20th In-\nternational Conference on Autonomous Agents and\nMultiagent Systems, Virtual Event, United King-\ndom, May 3-7, 2021, pages 483–491. ACM, 2021.\ndoi: 10.5555/3463952.3464013.\n[22] Yousef Emam, Gennaro Notomista, Paul Glotfelter,\nZsolt Kira, and Magnus Egerstedt. Safe reinforce-\nment learning using robust control barrier functions,\n2022.\n[23] Tingxiang Fan, Pinxin Long, Wenxi Liu, Jia Pan,\nRuigang Yang, and Dinesh Manocha.\nLearning\nresilient behaviors for navigation under uncertainty.\nIn 2020 IEEE International Conference on Robotics\nand Automation (ICRA), pages 5299–5305. IEEE,\n2020.\n[24] Zhan Gao, Guang Yang, and Amanda Prorok.\nOnline control barrier functions for decentralized\nmulti-agent navigation, 2023.\n[25] Javier Garcıa and Fernando Fern´andez.\nA com-\nprehensive survey on safe reinforcement learning.\nJournal of Machine Learning Research, 16(1):\n1437–1480, 2015.\n[26] Habtamu Hailemichael, Beshah Ayalew, Lindsey\nKerbel, Andrej Ivanco, and Keith Loiselle.\nSafe\nreinforcement learning for an energy-efﬁcient driver\nassistance system.\nIFAC-PapersOnLine, 55(37):\n615–620, 2022. ISSN 2405-8963. doi: https://doi.\norg/10.1016/j.ifacol.2022.11.250.\n2nd Modeling,\nEstimation and Control Conference MECC 2022.\n[27] Kai-Chieh Hsu, Allen Z. Ren, Duy Phuong Nguyen,\nAnirudha Majumdar, and Jaime F. Fisac.\nSim-\nto-lab-to-real: Safe reinforcement learning with\nshielding and generalization guarantees.\nCoRR,\nabs/2201.08355, 2022.\n13\n[28] Yifan Hu, Junjie Fu, and Guanghui Wen. Decentral-\nized robust collision-avoidance for cooperative mul-\ntirobot systems: A gaussian process-based control\nbarrier function approach. IEEE Transactions on\nControl of Network Systems, 10(2):706–717, 2023.\ndoi: 10.1109/TCNS.2022.3203928.\n[29] NVQ Hung, Hoang Duong Tuan, Tatsuo Narikiyo,\nand Pierre Apkarian.\nAdaptive control for non-\nlinearly parameterized uncertainties in robot ma-\nnipulators. IEEE transactions on control systems\ntechnology, 16(3):458–468, 2008.\n[30] Pushpak Jagtap, George J. Pappas, and Majid Za-\nmani. Control barrier functions for unknown non-\nlinear systems using gaussian processes. In 2020\n59th IEEE Conference on Decision and Control\n(CDC), pages 3699–3704, 2020.\ndoi: 10.1109/\nCDC42340.2020.9303847.\n[31] Michael Janner, Justin Fu, Marvin Zhang, and\nSergey Levine. When to trust your model: Model-\nbased policy optimization.\nAdvances in neural\ninformation processing systems, 32, 2019.\n[32] Gregory Kahn, Adam Villaﬂor, Vitchyr Pong, Pieter\nAbbeel, and Sergey Levine. Uncertainty-aware re-\ninforcement learning for collision avoidance. arXiv\npreprint arXiv:1702.01182, 2017.\n[33] Manuel Kaspar, Juan D Mu˜noz Osorio, and J¨urgen\nBock.\nSim2real transfer for reinforcement learn-\ning without dynamics randomization.\nIn 2020\nIEEE/RSJ International Conference on Intelligent\nRobots and Systems (IROS), pages 4383–4388.\nIEEE, 2020.\n[34] Mouhyemen Khan and Abhijit Chatterjee. Gaussian\ncontrol barrier functions: Safe learning and control.\nIn 2020 59th IEEE Conference on Decision and\nControl (CDC), pages 3316–3322, 2020. doi: 10.\n1109/CDC42340.2020.9303753.\n[35] Mouhyemen Khan, Tatsuya Ibuki, and Abhijit Chat-\nterjee. Safety uncertainty in control barrier func-\ntions using gaussian processes. In 2021 IEEE In-\nternational Conference on Robotics and Automation\n(ICRA), pages 6003–6009, 2021.\ndoi: 10.1109/\nICRA48506.2021.9561504.\n[36] Mouhyemen A Khan, Tatsuya Ibuki, and Abhijit\nChatterjee. Gaussian control barrier functions: Non-\nparametric paradigm to safety. IEEE Access, 10:\n99823–99836, 2022.\n[37] Khabat Khosravi, Himan Shahabi, Binh Thai Pham,\nJan Adamowski, Ataollah Shirzadi, Biswajeet Prad-\nhan, Jie Dou, Hai-Bang Ly, Gyula Gr´of, Huu Loc\nHo, Haoyuan Hong, Kamran Chapi, and Indra\nPrakash. A comparative assessment of ﬂood sus-\nceptibility modeling using multi-criteria decision-\nmaking analysis and machine learning methods.\nJournal of Hydrology, 573:311–323, 2019. ISSN\n0022-1694.\ndoi: https://doi.org/10.1016/j.jhydrol.\n2019.03.073.\n[38] Robert Kirk, Amy Zhang, Edward Grefenstette, and\nTim Rockt¨aschel.\nA survey of zero-shot gener-\nalisation in deep reinforcement learning. J. Artif.\nInt. Res., 76, may 2023.\nISSN 1076-9757. doi:\n10.1613/jair.1.14174.\n[39] Jaywant P Kolhe, Md Shaheed, TS Chandar, and\nSE Talole.\nRobust control of robot manipulators\nbased on uncertainty and disturbance estimation.\nInternational Journal of Robust and Nonlinear\nControl, 23(1):104–122, 2013.\n[40] Thanard Kurutach, Ignasi Clavera, Yan Duan,\nAviv Tamar, and Pieter Abbeel. Model-ensemble\ntrust-region policy optimization.\narXiv preprint\narXiv:1802.10592, 2018.\n[41] Jaemin Lee, Jeeseop Kim, and Aaron D. Ames.\nA data-driven method for safety-critical control:\nDesigning control barrier functions from state con-\nstraints, 2023.\n[42] Shuo Li and Osbert Bastani. Robust model predic-\ntive shielding for safe reinforcement learning with\nstochastic dynamics. In 2020 IEEE International\nConference on Robotics and Automation (ICRA),\npages 7166–7172, 2020. doi: 10.1109/ICRA40945.\n2020.9196867.\n[43] Timothy P Lillicrap, Jonathan J Hunt, Alexander\nPritzel, Nicolas Heess, Tom Erez, Yuval Tassa,\nDavid Silver, and Daan Wierstra.\nContinuous\ncontrol with deep reinforcement learning.\narXiv\npreprint arXiv:1509.02971, 2015.\n[44] Lars Lindemann, Haimin Hu, Alexander Robey,\nHanwen Zhang, Dimos V. Dimarogonas, Stephen\nTu, and Nikolai Matni.\nLearning hybrid control\nbarrier functions from data, 2020.\n[45] Lars Lindemann, Alexander Robey, Lejun Jiang,\nStephen Tu, and Nikolai Matni.\nLearning robust\noutput control barrier functions from safe expert\ndemonstrations, 2021.\n[46] Brett\nT\nLopez,\nJean-Jacques\nE\nSlotine,\nand\nJonathan P How. Robust adaptive control barrier\nfunctions: An adaptive and data-driven approach to\nsafety. IEEE Control Systems Letters, 5(3):1031–\n1036, 2020.\n14\n[47] Fan-Ming Luo, Tian Xu, Hang Lai, Xiong-Hui\nChen, Weinan Zhang, and Yang Yu. A survey on\nmodel-based reinforcement learning. arXiv preprint\narXiv:2206.09328, 2022.\n[48] Yuping Luo and Tengyu Ma.\nLearning barrier\ncertiﬁcates: Towards safe reinforcement learning\nwith zero training-time violations. In Marc’Aurelio\nRanzato, Alina Beygelzimer, Yann N. Dauphin,\nPercy Liang, and Jennifer Wortman Vaughan, ed-\nitors, Advances in Neural Information Processing\nSystems 34: Annual Conference on Neural Infor-\nmation Processing Systems 2021, NeurIPS 2021,\nDecember 6-14, 2021, virtual, pages 25621–25632,\n2021.\n[49] Haitong Ma, Jianyu Chen, Shengbo Eben Li, Ziyu\nLin, Yang Guan, Yangang Ren, and Sifa Zheng.\nModel-based constrained reinforcement learning\nusing generalized control barrier function.\nIn\nIEEE/RSJ International Conference on Intelligent\nRobots and Systems, IROS 2021, Prague, Czech\nRepublic, September 27 - Oct. 1, 2021, pages 4552–\n4559. IEEE, 2021. doi: 10.1109/IROS51168.2021.\n9636468.\n[50] Hengbo Ma, Bike Zhang, Masayoshi Tomizuka, and\nKoushil Sreenath.\nLearning differentiable safety-\ncritical control using control barrier functions for\ngeneralization to novel environments, 2022.\n[51] Shie Mannor and Nahum Shimkin.\nA geometric\napproach to multi-criterion reinforcement learning.\nThe Journal of Machine Learning Research, 5:325–\n360, 2004.\n[52] Yue Meng, Sai Vemprala, Rogerio Bonatti, Chuchu\nFan, and Ashish Kapoor. Conbat: Control barrier\ntransformer for safety-critical policy learning, 2023.\n[53] Sobhan Miryooseﬁ, Kiant´e Brantley, Hal Daum´e III\nau2, Miroslav Dudik, and Robert Schapire. Rein-\nforcement learning with convex constraints, 2019.\n[54] Hai Nguyen and Hung La. Review of deep rein-\nforcement learning for robot manipulation. In 2019\nThird IEEE International Conference on Robotic\nComputing (IRC), pages 590–595. IEEE, 2019.\n[55] Junhyuk Oh, Satinder Singh, Honglak Lee, and\nPushmeet Kohli. Zero-shot task generalization with\nmulti-task deep reinforcement learning. In Doina\nPrecup and Yee Whye Teh, editors, Proceedings\nof the 34th International Conference on Machine\nLearning, volume 70 of Proceedings of Machine\nLearning Research, pages 2661–2670. PMLR, 06–\n11 Aug 2017.\n[56] Zengyi Qin, Kaiqing Zhang, Yuxiao Chen, Jingkai\nChen, and Chuchu Fan. Learning safe multi-agent\ncontrol with decentralized neural barrier certiﬁ-\ncates, 2021.\n[57] Mahesh Ranaweera and Qusay Mahmoud.\nEval-\nuation of techniques for sim2real reinforcement\nlearning. In The International FLAIRS Conference\nProceedings, volume 36, 2023.\n[58] Alexander Robey, Haimin Hu, Lars Lindemann,\nHanwen Zhang, Dimos V. Dimarogonas, Stephen\nTu, and Nikolai Matni.\nLearning control barrier\nfunctions from expert demonstrations.\nIn 59th\nIEEE Conference on Decision and Control, CDC\n2020, Jeju Island, South Korea, December 14-\n18, 2020, pages 3717–3724. IEEE, 2020.\ndoi:\n10.1109/CDC42340.2020.9303785.\n[59] Abhaya Kumar Sahoo, Chittaranjan Pradhan, and\nHimansu Das. Performance Evaluation of Differ-\nent Machine Learning Methods and Deep-Learning\nBased Convolutional Neural Network for Health\nDecision Making, pages 201–212. Springer Inter-\nnational Publishing, Cham, 2020. ISBN 978-3-030-\n33820-6. doi: 10.1007/978-3-030-33820-6 8.\n[60] Matteo Saveriano and Dongheui Lee.\nLearning\nbarrier functions for constrained motion planning\nwith dynamical systems.\nIn 2019 IEEE/RSJ In-\nternational Conference on Intelligent Robots and\nSystems, IROS 2019, Macau, SAR, China, Novem-\nber 3-8, 2019, pages 112–119. IEEE, 2019. doi:\n10.1109/IROS40897.2019.8967981.\n[61] John Schulman, Sergey Levine, Pieter Abbeel,\nMichael Jordan, and Philipp Moritz. Trust region\npolicy optimization. In International conference on\nmachine learning, pages 1889–1897. PMLR, 2015.\n[62] Edvards Scukins and Petter Ogren.\nUsing rein-\nforcement learning to create control barrier func-\ntions for explicit risk mitigation in adversarial en-\nvironments.\npages 10734–10740, 05 2021.\ndoi:\n10.1109/ICRA48506.2021.9561853.\n[63] Muzammil Shahbaz and Roland Groz.\nInferring\nmealy machines. In Ana Cavalcanti and Dennis R.\nDams, editors, FM 2009: Formal Methods, pages\n207–222, Berlin, Heidelberg, 2009. Springer Berlin\nHeidelberg. ISBN 978-3-642-05089-3.\n[64] Hussein Sibai, Matthew Potok, and Sayan Mi-\ntra.\nSafe reinforcement learning for control sys-\ntems: A hybrid systems perspective and case study.\npages 1226–1235. University of Illinois at Urbana-\nChampaign, 2019.\n15\n[65] Thiago D. Sim˜ao, Nils Jansen, and Matthijs T. J.\nSpaan. Alwayssafe: Reinforcement learning with-\nout safety constraint violations during training. In\nFrank Dignum, Alessio Lomuscio, Ulle Endriss,\nand Ann Now´e, editors, AAMAS ’21: 20th Interna-\ntional Conference on Autonomous Agents and Mul-\ntiagent Systems, Virtual Event, United Kingdom,\nMay 3-7, 2021, pages 1226–1235. ACM, 2021. doi:\n10.5555/3463952.3464094.\n[66] Andrew Singletary, Yuxiao Chen, and Aaron D\nAmes. Control barrier functions for sampled-data\nsystems with input delays.\nIn 2020 59th IEEE\nConference on Decision and Control (CDC), pages\n804–809. IEEE, 2020.\n[67] Krishnan Srinivasan, Benjamin Eysenbach, Sehoon\nHa, Jie Tan, and Chelsea Finn.\nLearning to be\nsafe: Deep rl with a safety critic. arXiv preprint\narXiv:2010.14603, 2020.\n[68] Mohit\nSrinivasan,\nAmogh\nDabholkar,\nSamuel\nCoogan, and Patricio Vela. Synthesis of control bar-\nrier functions using a supervised machine learning\napproach, 2020.\n[69] Richard S. Sutton and Andrew G. Barto. Reinforce-\nment Learning: An Introduction. The MIT Press,\nsecond edition, 2018.\n[70] Andrew J Taylor and Aaron D Ames.\nAdaptive\nsafety with control barrier functions.\nIn 2020\nAmerican Control Conference (ACC), pages 1399–\n1405. IEEE, 2020.\n[71] Chen Tessler, Daniel J. Mankowitz, and Shie Man-\nnor. Reward constrained policy optimization, 2018.\n[72] Jakob Thumm and Matthias Althoff.\nProvably\nsafe deep reinforcement learning for robotic ma-\nnipulation in human environments.\nIn 2022 In-\nternational Conference on Robotics and Automa-\ntion, ICRA 2022, Philadelphia, PA, USA, May 23-\n27, 2022, pages 6344–6350. IEEE, 2022.\ndoi:\n10.1109/ICRA46639.2022.9811698.\n[73] Sander Tonkens and Sylvia Herbert.\nReﬁning\ncontrol barrier functions through hamilton-jacobi\nreachability, 2022.\n[74] Shuo Yang, Yu Chen, Xiang Yin, and Rahul Mang-\nharam. Learning local control barrier functions for\nsafety control of hybrid systems.\narXiv preprint\narXiv:2401.14907, 2024.\n[75] Yue Yang, Letian Chen, and Matthew C. Gombo-\nlay. Safe inverse reinforcement learning via control\nbarrier function. CoRR, abs/2212.02753, 2022. doi:\n10.48550/arXiv.2212.02753.\n[76] Hongchao Zhang, Zhouchi Li, and Andrew Clark.\nModel-based reinforcement learning with provable\nsafety guarantees via control barrier functions. In\nIEEE International Conference on Robotics and\nAutomation, ICRA 2021, Xi’an, China, May 30 -\nJune 5, 2021, pages 792–798. IEEE, 2021.\ndoi:\n10.1109/ICRA48506.2021.9561253.\n[77] Songyuan Zhang, Oswin So, Kunal Garg, and\nChuchu Fan. Gcbf+: A neural graph control barrier\nfunction framework for distributed safe multi-agent\ncontrol. arXiv preprint arXiv:2401.14554, 2024.\n[78] Zicong Zhao, Jing Xun, Xuguang Wen, and Jianqiu\nChen. Safe reinforcement learning for single train\ntrajectory optimization via shield SARSA.\nIEEE\nTrans. Intell. Transp. Syst., 24(1):412–428, 2023.\ndoi: 10.1109/TITS.2022.3218705.\n16\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.RO",
    "cs.SY",
    "eess.SY"
  ],
  "published": "2024-04-22",
  "updated": "2024-04-22"
}