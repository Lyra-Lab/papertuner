{
  "id": "http://arxiv.org/abs/2404.16992v1",
  "title": "A Catalog of Transformations to Remove Smells From Natural Language Tests",
  "authors": [
    "Manoel Aranda",
    "Naelson Oliveira",
    "Elvys Soares",
    "Márcio Ribeiro",
    "Davi Romão",
    "Ullyanne Patriota",
    "Rohit Gheyi",
    "Emerson Souza",
    "Ivan Machado"
  ],
  "abstract": "Test smells can pose difficulties during testing activities, such as poor\nmaintainability, non-deterministic behavior, and incomplete verification.\nExisting research has extensively addressed test smells in automated software\ntests but little attention has been given to smells in natural language tests.\nWhile some research has identified and catalogued such smells, there is a lack\nof systematic approaches for their removal. Consequently, there is also a lack\nof tools to automatically identify and remove natural language test smells.\nThis paper introduces a catalog of transformations designed to remove seven\nnatural language test smells and a companion tool implemented using Natural\nLanguage Processing (NLP) techniques. Our work aims to enhance the quality and\nreliability of natural language tests during software development. The research\nemploys a two-fold empirical strategy to evaluate its contributions. First, a\nsurvey involving 15 software testing professionals assesses the acceptance and\nusefulness of the catalog's transformations. Second, an empirical study\nevaluates our tool to remove natural language test smells by analyzing a sample\nof real-practice tests from the Ubuntu OS. The results indicate that software\ntesting professionals find the transformations valuable. Additionally, the\nautomated tool demonstrates a good level of precision, as evidenced by a\nF-Measure rate of 83.70%",
  "text": "A Catalog of Transformations to Remove Smells From\nNatural Language Tests\nManoel Aranda\nFederal University of Alagoas\nMaceió, Brazil\nmpat@ic.ufal.br\nNaelson Oliveira\nFederal University of Alagoas\nMaceió, Brazil\nnaelson@ic.ufal.br\nElvys Soares\nFederal Institute of Alagoas\nMaceió, Brazil\nelvys.soares@ifal.edu.br\nMárcio Ribeiro\nFederal University of Alagoas\nMaceió, Brazil\nmarcio@ic.ufal.br\nDavi Romão\nFederal University of Alagoas\nMaceió, Brazil\ndsr@ic.ufal.br\nUllyanne Patriota\nFederal University of Alagoas\nMaceió, Brazil\nufjp@ic.ufal.br\nRohit Gheyi\nFederal Univ. of Campina Grande\nCampina Grande, Brazil\nrohit@dsc.ufcg.edu.br\nEmerson Souza\nFederal University of Pernambuco\nRecife, Brazil\nepss@cin.ufpe.br\nIvan Machado\nFederal University of Bahia\nSalvador, Brazil\nivan.machado@ufba.br\nABSTRACT\nTest smells can pose difficulties during testing activities, such as\npoor maintainability, non-deterministic behavior, and incomplete\nverification. Existing research has extensively addressed test smells\nin automated software tests but little attention has been given to\nsmells in natural language tests. While some research has identified\nand catalogued such smells, there is a lack of systematic approaches\nfor their removal. Consequently, there is also a lack of tools to\nautomatically identify and remove natural language test smells.\nThis paper introduces a catalog of transformations designed to\nremove seven natural language test smells and a companion tool\nimplemented using Natural Language Processing (NLP) techniques.\nOur work aims to enhance the quality and reliability of natural\nlanguage tests during software development. The research employs\na two-fold empirical strategy to evaluate its contributions. First,\na survey involving 15 software testing professionals assesses the\nacceptance and usefulness of the catalog’s transformations. Second,\nan empirical study evaluates our tool to remove natural language\ntest smells by analyzing a sample of real-practice tests from the\nUbuntu OS. The results indicate that software testing professionals\nfind the transformations valuable. Additionally, the automated tool\ndemonstrates a good level of precision, as evidenced by a F-Measure\nrate of 83.70%.\nCCS CONCEPTS\n• Software and its engineering →Software testing and debug-\nging; Empirical software validation.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nEASE 2024, June 18–21, 2024, Salerno, Italy\n© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-1701-7/24/06...$15.00\nhttps://doi.org/10.1145/3661167.3661225\nKEYWORDS\nNatural Language Test, Test Smells, Software Testing\nACM Reference Format:\nManoel Aranda, Naelson Oliveira, Elvys Soares, Márcio Ribeiro, Davi Romão,\nUllyanne Patriota, Rohit Gheyi, Emerson Souza, and Ivan Machado. 2024. A\nCatalog of Transformations to Remove Smells From Natural Language Tests.\nIn 28th International Conference on Evaluation and Assessment in Software\nEngineering (EASE 2024), June 18–21, 2024, Salerno, Italy. ACM, New York,\nNY, USA, 10 pages. https://doi.org/10.1145/3661167.3661225\n1\nINTRODUCTION\nTest smells are indications of bad decisions when designing or im-\nplementing tests. Such decisions may introduce potential problems\nin testing activities [26]. Examples of these problems are poor main-\ntanability (e.g., duplication in test code [11]), non-deterministic\nscenarios [16], and missing verifications [22] due to code branches.\nIn this context, the literature is particularly vast when consider-\ning test smells in automated software tests. Previous studies have\nbeen introduced to provide catalogues of test smells [2, 9], to count\nand study their occurrences [4, 20, 21], and to provide tools capa-\nble of identifying and removing them automatically using code\ntransformations [14, 15, 20, 21, 26].\nUnfortunately, the same does not apply when considering test\nsmells in manual tests, i.e., natural language tests. These tests are\nwritten in natural language and also suffer from problems intro-\nduced by smells [10, 19]. Figure 1 illustrates part of a natural lan-\nguage test extracted from the Ubuntu Operating System (Ubuntu\nOS) [25]. The test contains the Ambiguous Test smell [10], which\nindicates an “under-specified test that leaves room for interpretation.”\nIn Step 2, the required action is to “Open any application.” How-\never, it is not specified which application should be opened. This\nambiguity can lead to different test results based on the application\nselected by the tester. Hence, identifying and removing this smell\nmight bring better quality standards for the tests.\nAs mentioned, differently from test smells in automated soft-\nware tests, test smells in natural language are still poorly explored.\narXiv:2404.16992v1  [cs.SE]  25 Apr 2024\nEASE 2024, June 18–21, 2024, Salerno, Italy\nAranda, et al.\nFigure 1: Natural Language Test Smell: Ambiguous Test.\nHauptmann et al. [10] presented seven natural language test smells\nand after ten years Soares et al. [19] complemented the list with\nsix additional smells. Both studies introduced rules to identify the\ntest smells. However, we still miss a catalog of transformations\nto remove natural language test smells. In addition, we miss tool\nsupport to make this whole process automatic, i.e., a tool capable\nof removing natural language test smells automatically.\nTo minimize these problems, in this paper we introduce a catalog\nof transformations to remove natural language test smells presented\nin recent research [19]. Our catalog contains seven transformations\ncapable of removing the following natural language test smells:\nUnverified Action, Misplaced Precondition, Misplaced Action, Mis-\nplaced Verification, Eager Action, Ambiguous Test, and Conditional\nTest. For each smell, our catalog contains details with respect to\nthe addressed test smell, the mechanics to apply the transformation\nin the test in order to remove such smell, and an example. The\ncatalog works considering a left-hand side test, i.e., the test that\ncontains the test smell; and a right-hand side test, i.e., the test after\nthe transformation without the smell. We also contribute to a tool\nbased on Natural Language Processing (NLP) techniques. The tool\nis capable of identifying the test smells and removing them auto-\nmatically. In case the test contains more than one smell, the tool\nworks sequentially, tackling one smell at a time.\nTo evaluate the catalog and the tool, we conduct a two-fold em-\npirical strategy. First, we conduct a survey to answer the research\nquestion RQ1:“How software testing professionals perceive and eval-\nuate the transformations of our catalog?” To answer it, we recruited\n15 software testing professionals from a large smartphone manufac-\nturer. The name of the company is omitted due to non-disclosure\nagreements. Answering RQ1 is important to better understand\nwhether the software testing professionals find our transforma-\ntions useful to improve the quality of the tests. Second, we conduct\nan empirical study to evaluate our tool. Here we focus on the re-\nsearch question RQ2:“How precise is our tool in the task of removing\nnatural language test smells?” To conduct this study and answer\nRQ2, we consider real-practice natural language tests from the\nUbuntu OS. The tests consist of not only software functionalities,\nbut also interactions with the hardware. After executing our tool\nagainst 973 natural language tests, we identified 8, 386 occurrences\nof the seven smells we focus on in this paper. In this scenario, to\nevaluate the precision of our tool in removing the smells, we would\nneed to manually analyze more than eight thousand transforma-\ntions, exceeding our capabilities. Thus, to make our manual analysis\nfeasible, we use the Cochran’s Sample Size Formula [3] and analyze\na sample of 264 randomly selected smell occurrences.\nThe results of our survey indicate an average acceptance of\n91.43% among the software testing professionals we recruited. Re-\ngarding the tool, our manual analysis showed that it achieved 83.70%\nrate of F-Measure.\nIn summary, this paper provides the following contributions:\n• A catalog of transformations to remove seven test smells\nfrom natural language tests (Section 3);\n• A survey with software testing professionals to validate the\ncatalog (Section 4);\n• The development of a tool that makes our transformations\nautomatic (Section 5);\n• An empirical study to check the precision of our tool using\nreal-practice natural language tests from the Ubuntu OS\n(Section 6).\nA replication package with all results is available at Figshare\n[23]. Our tool is available at our companion website [24].\n2\nNATURAL LANGUAGE TEST SMELLS\nPrevious works showed that test smells also exist in natural lan-\nguage tests [10, 19]. Besides the Ambiguous Test smell we discussed\nin Section 1, we now show three additional test smells in natural\nlanguage tests. We also discuss why they could be harmful during\ntesting activities. All examples of natural language tests in this pa-\nper are extracted from the Ubuntu OS manual test repository [25].\n2.1\nUnverified Action\nThis smell occurs when there is no verification for a given action.\nWhen compared to automated tests, this smell is similar to the As-\nsertionless smell, which is defined by the absence of assertions [1].\nFigure 2 presents the Unverified Action smell. Step 7 lacks a verifi-\ncation for the “Click one more time on the same message” action.\nFigure 2: Unverified Action - Example\nWithout proper verification, tests may not effectively validate\nwhether the application is behaving as expected. Regarding the ex-\nample, the tester may experience confusion regarding the outcomes\nof the clicking action. Questions such as “What should happen when\nthe message is clicked one more time?” and “Do additional elements\ndisappear? And if they do not, does the test fail?” may arise, leaving\nthe tester uncertain on how to proceed. Addressing these uncertain-\nties is crucial to maintain clarity and ensure effective test execution.\n2.2\nEager Action\nThis smell happens when a single step groups multiple actions.\nFigure 3 depicts an example of the Eager Action smell.\nThe Eager Action test smell can introduce problems such as non-\nisolated tests and difficulties in debugging. In our example, we have\nfour actions, represented by the verbs “select”, “enter”, “select”, and\nA Catalog of Transformations to Remove Smells From\nNatural Language Tests\nEASE 2024, June 18–21, 2024, Salerno, Italy\nFigure 3: Eager Action - Example\n“click.” Since we have one verification for four actions, in the event\nof a failure, it may be unclear which action caused it.\n2.3\nConditional Test\nThis smell is defined by a conditional clause appearing in a step\ndescription [10]. Figure 4 shows an example.\nFigure 4: Conditional Test - Example\nThe conditional introduces uncertainty and variability to the\ntest, as the tester may not know how to proceed or what to expect.\nNotice that the action in Step 2 contains a conditional clause “If\nyour printer doesn’t show up, add it to the list (click Add and follow\nthe wizard).” If the printer is already showing up, the tester might\nneed assistance deciding whether to skip Step 2 and proceed to Step\n3 or stop the test execution altogether because there is no mention\nof the printer being on the list. This situation can reduce the test’s\ndeterminism, maintainability, coverage, and reliability [21].\n3\nREMOVING NATURAL LANGUAGE TEST\nSMELLS\nThis section introduces a catalog of proposals to remove test smells\nin natural language tests. Concerning our terminology, according\nto Fowler et al. [8], refactorings preserve the observable behavior\nof a code. On the other hand, van Deursen et al. [26] define test\nrefactorings as changes in test code that do not add or remove test\ncases. Since our proposals do not strictly align with Fowler’s and\nvan Deursen’s definitions because they may alter the test behavior\nand even introduce new test cases, we call them transformations.\nOur catalog contains transformations in terms of a left-hand\nside and a right-hand side [21]. The left-hand side holds the prob-\nlematic test (i.e., the test with test smells). The right-hand side\npresents the test transformed, i.e., the test without test smells, after\nthe transformations. To apply a transformation, the catalog uses\nsemantic pattern matching. In other words, a natural language test\nthat matches the left-hand side of a transformation is converted to\nthe right-hand side.\n3.1\nNatural Language Test Template\nTo better explain our transformations, we first need to introduce\na template to represent natural language tests. Here, we define a\nnatural language test as 𝑇= (𝑃,𝑆1,𝑆2,𝑆3, . . . ,𝑆𝑖, . . . ,𝑆𝑛), where:\n• 𝑃is a boolean expression, representing the preconditions to\nexecute 𝑇;\n• 𝑆𝑖is a tuple representing the i-th step of the test. This tuple\nconsists of an ordered list of actions 𝐴𝑖and an ordered list\nof verifications 𝑉𝑖, defined as 𝑆𝑖= (𝐴𝑖,𝑉𝑖);\n• The elements of the 𝐴𝑖list are denoted by [𝑎𝑖1,𝑎𝑖2, . . . ,𝑎𝑖𝑛];\n• The elements of the 𝑉𝑖list are denoted by [𝑣𝑖1, 𝑣𝑖2, . . . , 𝑣𝑖𝑛].\nThus, given the above definitions, the following equations are\ninterchangeable:\n𝑆𝑖= (𝐴𝑖,𝑉𝑖)\n(1)\n𝑆𝑖= ([𝑎𝑖1,𝑎𝑖2, . . . ,𝑎𝑖𝑛], [𝑣𝑖1, 𝑣𝑖2, . . . , 𝑣𝑖𝑛])\n(2)\nConsidering the test illustrated in Figure 3, we have:\n• 𝑃= “This test will check that Firefox can print websites”;\n• 𝐴3 = [“Select ‘print to file’ as printer”, “enter ‘firefox.pdf’ as\nfilename”, “Select your home folder as location”, “Then click\non ‘Print’”];\n• 𝑉3 = [“A window opens, showing the progress of the print”].\nFigure 5 presents𝑇and all its elements in terms of a table. Notice\nthat in Step 𝑆𝑖we use the form presented in Equation 1. When con-\nsidering Steps 𝑆1 and 𝑆𝑛we use the form presented in Equation 2.\nFigure 5: Natural Language Test Template\nBy using this template and particularly the Step𝑆𝑖, we can discuss\nhow to avoid natural language test smells. For example: to avoid\nthe Ambiguous Test smell, the sentences in 𝐴𝑖and in 𝑉𝑖should not\nbe ambiguous; To avoid the Eager Action smell in 𝑆𝑖, the cardinality\nof the list 𝐴𝑖should be |𝐴𝑖| = 1, i.e., there should be only one action\ninstead of several ones in the i-th step; To avoid the Unverified\nAction smell in 𝑆𝑖, the cardinality of the list 𝑉𝑖should be |𝑉𝑖| ≥1,\ni.e., there is at least one verification in 𝑉𝑖.\nWhen presenting our transformations, we also consider the infix\n++ operator, which is used for list concatenation. It takes two lists\nas operands and combines them to form a new list. Equation 3\nshows a simple usage of the ++ operator.\n𝑆𝑖=[𝑎𝑖1,𝑎𝑖2, . . . ,𝑎𝑖𝑛] + + [𝑎𝑘]\n𝑆𝑖=[𝑎𝑖1,𝑎𝑖2, . . . ,𝑎𝑖𝑛,𝑎𝑘]\n(3)\nEASE 2024, June 18–21, 2024, Salerno, Italy\nAranda, et al.\n3.2\nA Catalog of Transformations\nWe now present our catalog having each transformation in terms\nof (i) the smell that the transformation addresses, (ii) the mechanics\nto apply the transformation (left-hand side and right-hand side),\n(iii) implications—in case the transformation removes a smell but\nadds another, or the transformation removes more than one smell\n—, and (iv) an example of the transformation in a natural language\ntest extracted from the Ubuntu OS.\nIn this paper, we propose transformations for seven natural lan-\nguage test smells, namely Unverified Action, Misplaced Precondition,\nMisplaced Action, Misplaced Verification, Eager Action, Ambiguous\nTest, and Conditional Test. We focus on these smells because they\nmay introduce difficulties in testing activities and have been studied\nby the literature [10, 19].\n3.2.1\nFill Verification.\nAddressed Smell. The Unverified Action. We discussed this smell\nin Section 2.1.\nFormalization. Figure 6a presents our transformation to remove\nthe Unverified Action smell. Notice that, at the left-hand side, the\nverifications list𝑉𝑖is empty. To remove the smell, we can implement\ntwo strategies. The first and simplest one is to just warn the tester\nthat 𝑉𝑖is empty and then add a sort of “FILL_VERIFICATION” flag\nin the Verifications field. The second strategy needs to deal\nwith more complex algorithms. As 𝐴𝑖and 𝑉𝑖should be somehow\nlinked by the same context, we can use NLP techniques to infer the\nverification from the action. Then, we add the inferred verification\nsentence in 𝑉𝑖.\nExample. Figure 6b shows an example of the Fill Verification trans-\nformation. In this example, we add the “Dash appears” verification\nbased on the text contained in the actions list 𝐴1.\n(a) Transformation\n(b) Example\nFigure 6: Fill Verification\n3.2.2\nExtract Precondition.\nAddressed Smell. The Misplaced Precondition, which appears when\na precondition is placed as the first action of the test. This situation\nmight bring difficulties in test correctness. For example, the tester\nmight report a test failure. However, what actually happened was\nthat a precondition to execute the test was not met.\nFormalization. Figure 7a shows a precondition 𝑝as the first ele-\nment of 𝐴1. We deal with the smell by removing 𝑝from the actions\nlist 𝐴1 and placing it in the boolean expression 𝑃using the con-\njunction operator, yielding 𝑃∧𝑝.\nExample. Figure 7b presents an example of applying our Extract\nPrecondition transformation. Notice that the precondition “Ensure\nthat Ristretto is loaded [...]” was removed from the actions list and\nadded to the Preconditions field.\n(a) Transformation\n(b) Example\nFigure 7: Extract Precondition\n3.2.3\nExtract Action.\nAddressed Smell. The Misplaced Action, which happens when\nthere is an action in the verifications list. Such confusion can result\nin inconsistent execution and potentially misguide the interpreta-\ntion of results.\nFormalization. Figure 8a illustrates the action element 𝑎in the\nverifications list of Step 𝑆𝑖. To remove the smell, all we need to do\nis to remove 𝑎from the verifications list 𝑉𝑖and add it to the actions\nlist 𝐴𝑖. To perform the adding task, we use the ++ operator.\nImplications. The Extract Action transformation might introduce\nthe Eager Action smell in case there already exists an action in 𝐴𝑖.\nIn this sense, to remove the Eager Action smell, we propose the\nSeparate Actions transformation. We introduce this transformation\nin Section 3.2.5.\nA Catalog of Transformations to Remove Smells From\nNatural Language Tests\nEASE 2024, June 18–21, 2024, Salerno, Italy\nExample. Figure 8b presents a test with the action “Open some\nwindows” located in the verifications list. After executing our trans-\nformation, such an action is placed in the actions list and removed\nfrom the verifications list.\n(a) Transformation\n(b) Example\nFigure 8: Extract Action\n3.2.4\nExtract Verification.\nAddressed Smell. The Misplaced Verification, which occurs when\nthere is a verification in the actions list.\nFormalization. Figure 9a illustrates a verification 𝑣in 𝐴𝑖list. To\naddress the smell, we remove 𝑣from the actions list 𝐴𝑖and add it\nto the verifications list 𝑉𝑖. In this transformation, we rely on the ++\noperator. Notice that we add 𝑣at the beginning of 𝑉𝑖. We opt for\nthis because 𝑣should be the first verification to be checked right\nafter executing action 𝑎𝑖𝑛.\nImplications. Applying this transformation might remove not\nonly the Misplaced Verification, but also the Unverified Action smell.\nThis happens in case |𝑉𝑖| = 0 and we move 𝑣from 𝐴𝑖to 𝑉𝑖, making\n|𝑉𝑖| = 1.\nExample. Figure 9b illustrates the verification “Verify that ‘Enable\nVolume Management’ is checked [...]” in the actions list. After our\ntransformation, we place such a verification in the verifications\nlist. With this transformation, notice that we remove the Misplaced\nVerification and the Unverified Action smells.\n3.2.5\nSeparate Actions.\nAddressed Smell. The Eager Action, discussed in Section 2.2.\nFormalization. Figure 10a illustrates that 𝐴𝑖contains 𝑛elements,\ncharacterizing the Eager Action smell: |𝐴𝑖| > 1. To remove the smell,\nwe define a new step for each element of the 𝐴𝑖list. Additionally,\nin our transformation, we associate the verifications list 𝑉𝑖with the\nstep created for the last action originally in 𝐴𝑖, i.e., the 𝑎𝑖𝑛action.\nWe opt for this because we consider that the tester will check the\nverifications list 𝑉𝑖only after executing the last action.\n(a) Transformation\n(b) Example\nFigure 9: Extract Verification\nImplications. The Separate Actions transformation may lead to the\nUnverified Action smell. After the transformation, we have empty\nverifications lists for the Steps from 𝑆𝑖to 𝑆𝑘+𝑛−1. To deal with the\nUnverified Action smell, we introduce the Fill Verification transfor-\nmation (Section 3.2.1).\nExample. Figure 10b shows Step 3 with two actions: “Add content\nto the popped up memo” and “Then click the green tick.” According\nto our transformation, they should be split in two different steps.\nMoreover, the verification “Did the window showed [...]” has been\nassociated with the last action, i.e., “Then click the green tick.”\n3.2.6\nExtract Ambiguity.\nAddressed Smell. The Ambiguous Test smell. Ambiguities and\nunder-specified activities may lead to problems during the test\nexecution, such as different outcomes.\nFormalization. Figure 11a shows the actions list 𝐴𝑖with 𝑛ele-\nments. Any of these elements might have ambiguous sentences,\nsuch as “quickly” or “accurately.” To remove the ambiguities, we\npropose the use of a 𝛾function that receives an action sentence\nas input and returns the same action sentence in case it is not am-\nbiguous; and a modified action sentence in case the original one is\nambiguous. We then apply 𝛾to all elements of 𝐴𝑖. Although our\ntransformation to remove the Ambiguous Test smell focused on the\nactions list, we can use the same rationale on the verifications list.\nExample. Figure 11b illustrates an action with the adverb of\nmanner “approximately.” Here the tester could ask: “Is 25 seconds\nenough to make the wireless network visible? If I wait 25 seconds and\nthe wireless network does not appear, did the test fail?”\nEASE 2024, June 18–21, 2024, Salerno, Italy\nAranda, et al.\n(a) Transformation\n(b) Example\nFigure 10: Separate Actions\n(a) Transformation\n(b) Example\nFigure 11: Extract Ambiguity\n3.2.7\nExtract Conditional.\nAddressed Smell. The Conditional Test smell, detailed in Sec-\ntion 2.3. To deal with this smell, we create two different tests [7].\nFormalization. Figure 12a illustrates our transformation. To do\nso, we rely on two functions. The first one, 𝛽, receives an action\nsentence 𝑎as input. In case 𝑎has no conditional, 𝛽simply skips.\nIn case 𝑎has a conditional 𝑐, 𝛽returns a map (𝑎′,𝑐), where 𝑎′ is\nthe action element without the conditional and 𝑐is the removed\nconditional. In Figure 12a, 𝛽identified a conditional 𝑐in action 𝑎𝑖𝑘\nand removed it, transforming 𝑎𝑖𝑘into 𝑎′\n𝑖𝑘. Because now we have\nthe conditional 𝑐, it is time to create two tests. We consider another\nfunction 𝜃that receives the original test 𝑇and the conditional 𝑐.\n𝜃yields two tests: 𝑇′ considering 𝑐as true; and 𝑇′′ considering 𝑐\nas false. Because the conditional 𝑐is necessary to execute 𝑇′, we\nplace it in the boolean expression 𝑃using the conjunction operator,\nyielding 𝑃∧𝑐. As to𝑇′′, we exclude the step that originally contained\nthe conditional and the following steps. Notice that we need to\nrepeat this process to all actions in all steps of the original test.\nExample. Figure 12b presents an example of the Conditional Test\nsmell removal. Once we identify the conditional “If you have a\nUSB drive,” we create a test considering it in the precondition and\nanother test without the step that originally had the conditional\nand the subsequent steps.\n3.2.8\nSummary. Illustrated in Table 1 is the list of our proposed\ntransformations and their addressed smells.\nTable 1: Transformations and addressed smells\nNo.\nTransformation\nAddressed Smell\n1\nExtract Conditional\nConditional Test\n2\nExtract Action\nMisplaced Action\n3\nSeparate Actions\nEager Action\n4\nExtract Verification\nMisplaced Verification\n5\nExtract Ambiguity\nAmbiguous Test\n6\nExtract Precondition\nMisplaced Precondition\n7\nFill Verification\nUnverified Action\n4\nCATALOG EVALUATION\nThis section presents our evaluation, conducted as an online survey\nwith software testing professionals. Here we aim to answer the\nresearch question RQ1: “How do software testing professionals\nperceive and evaluate the transformations of our catalog?”\n4.1\nPlanning\nThe goal of this study is to evaluate the effectiveness of our trans-\nformations. To do so, we assess the opinions of software testing\nprofessionals about the transformations through an online survey.\nBy analyzing “before” and “after” test transformation snippets—\noriginally taken from the Ubuntu OS manual tests and transformed\naccording to our proposals—along with the participants’ comments\non their answers, we would be able to validate whether the respon-\ndents were aware of any benefits.\nEach survey question had a test smell definition, problem, iden-\ntification steps, original and transformed samples, the query “Do\nyou agree that, in the example below, the identified problem was ad-\ndressed according to the definition?”, a Likert scale ranging from “I\nA Catalog of Transformations to Remove Smells From\nNatural Language Tests\nEASE 2024, June 18–21, 2024, Salerno, Italy\n(a) Transformation\n(b) Example\nFigure 12: Extract Conditional\nstrongly agree” to “I strongly disagree”, and an optional comments\nfield. Finally, we recruited participants from a large smartphone\nmanufacturer, which received invitations by email.\n4.2\nResults\nWe performed the survey in November 2023 – January 2024, achiev-\ning 15 responses. Concerning the demographics, 71% of the par-\nticipants defined their primary work area as the industry (over\nacademia) and their average experience with software testing was\n4,1 years. One participant works in Portugal and 14 in Brazil. Fig-\nure 13 details the obtained results concerning the participants’\nopinions about the transformation samples.\n4.3\nDiscussion\nRegarding the proposed transformations, the opinion of the experi-\nenced test professionals served as a validation that obtained a high\naverage acceptance rate of 91.43%. There were no disagreements to\nthe Extract Action transformation, that was accepted by 93.3% of\nthe respondents (one respondent was indifferent). Four transforma-\ntions reached 93.3% acceptance rate (14 respondents) and 6.7% of\nrejection rate (one respondent): Fill Verification, Separate Actions,\nExtract Verification, and Extract Conditional. Our transformations\nwith the lowest acceptance rate were the Extract Ambiguity and\nExtract Precondition, and still achieved an 86.7% approval. Among\nthe transformations positively commented, we highlight “Follow-\ning the logic that every action has a reaction, every test step must\nhave an expected result [...],” which reinforces the Fill Verification\ntransformation. Another comment received highlights that “In this\ncase, some steps could be included in a ‘test setup’ column, so that\nbefore the person/machine performs the steps, it ensures that it has the\nnecessary resources to carry out the test.” This comment reinforces\ntransforming the Misplaced Precondition smell.\nSome respondents disagreed with some transformations offered\nand exemplified in our research. For example, in one of the tests\npresented in our survey, we used the following actions: “Start Nau-\ntilus,” “Maximize it’s window,” and “Start Firefox.” Our transforma-\ntion considered three verifications for these three actions. However,\na respondent mentioned the following: “[...] its not necessary a ver-\nification in this case.” Another disagreement was in the Extract\nVerification, where a respondent commented: “There’s (verify xxxxx)\ntwo times. If you’ll join to one block, don’t repeat process.” This case is\nconnected to the provided example, where a test had two similar ver-\nifications written slightly differently but for the same action. This\noutcome highlights the need to identify semantic Test Clones [10]\nin future work.\nAnswer to RQ1. The online survey shows software testing pro-\nfessionals mostly agree with our proposals.\n4.4\nThreats to Validity\nAs a threat to external validity, collecting answers from only 15\nparticipants may bring bias in terms of generalization. We minimize\nthis threat by relying on the experience of the participants to pro-\nvide good responses. Also, we might experience a threat to internal\nvalidity when selecting the examples for the study. We minimize\nthis threat by using real-practice examples from the Ubuntu OS. A\nlast threat to the conclusion validity relates to all participants\nworking for the same company. We minimize this selection bias\nwith staff from different roles and hierarchical positions.\n5\nA TOOL TO REMOVE SMELLS FROM\nMANUAL TESTS\nWe now present details on developing an NLP-based tool called\nManual Test Alchemist. It extends prior work [19] on automatically\ndetecting natural language test smells and implements their removal\naccording to the transformations presented in Section 3.\nThe Manual Test Alchemist tool development is centered on\nspaCy [12]—a commercial open-source library for NLP—to imple-\nment our transformations for natural language test smells. The\nspaCy library features convolutional neural network models for\nEASE 2024, June 18–21, 2024, Salerno, Italy\nAranda, et al.\nFigure 13: Online Survey Results\npart-of-speech (POS) tagging [13], dependency parsing [17], mor-\nphology parsing, and named entity recognition [12]. Table 2 shows\nthe analysis of the action sentence in Figure 11b provided by spaCy.\nSuch analysis enables the implementation of our transformations.\nFor example, adverbs of manner can be identified through\nPOS=ADV, TAG=RB and Dependency=advmod properties, i.e., the ap-\nproximately found in Table 2. Such detection is used in the Extract\nAmbiguity transformation (Section 3.2.6).\nIn another example, imperative verbs identify action steps\nPOS=VERB, VerbForm=Inf and Dependency=ROOT properties, i.e.,\nopen in Table 2. This detection is used in the Separate Actions (Sec-\ntion 3.2.5), Extract Action (Section 3.2.3), and Extract Precondition\n(Section 3.2.2) transformations.\nTable 2: Example of spaCy’s text analysis in detail.\nText\nPOS\nTAG\nDependency\nMorphology\nAfter\nADP\nIN\nprep\napproximately\nADV\nRB\nadvmod\n30\nNUM\nCD\nnummod\nNumType=Card\nseconds\nNOUN\nNNS\npobj\nNumber=Plur\n,\nPUNCT\n,\npunct\nPunctType=Comm\nopen\nVERB\nVB\nROOT\nVerbForm=Inf\nthe\nDET\nDT\ndet\nDefinite=Def|PronType=Art\nnetwork\nNOUN\nNN\ncompound\nNumber=Sing\nmanager\nNOUN\nNN\ndobj\nNumber=Sing\n.\nPUNCT\n.\npunct\nPunctType=Peri\nIllustrated in Figure 14 is a simplified architecture of our tool,\nwhich functions by receiving an XML file of a test suite as its input.\nThis test suite may contain multiple tests and is parsed using a\ncustomizable parser implementation into a consistent test format.\nEvery implementation of the Transformator interface is responsi-\nble for identifying and adrressing a specific test smell.\nThe Extract Conditional transformation results in the creation\nof new tests. Therefore, a relation between the execution order\nof transformations and their impact was identified. According to\nthe implications shown in Section 3, the correction of some test\nsmells may result in new ones. For instance, adjusting actions be-\nfore Separate Actions is important for proper functionality, and Fill\nVerification should run last to address the Unverified Action smell\ngenerated by Separate Actions and Extract Action. Hence, to mini-\nmize this effect, we defined the execution sequence to be the one\npresented on Table 1.\nTo provide a more detailed account of the tool’s inner work-\nings, Algorithm 1 presents the pseudocode of the Separate Actions\nFigure 14: Simplified UML class diagram.\ntransformation. The algorithm highlights the key steps involved\nin identifying the presence of multiple action verbs within a single\ntest step, separating the sentences related to each of these actions,\nallocating the separated sentences to new steps, and notifying the\ntest professional that a verification must be written for the newly\nadded step.\nAlgorithm 1 The Separate Actions transformation (simplified)\nfor every test step do\nSearch for action verbs;\nif action verbs > 1 then\nfor every action verb do\nIdentify sentence connectors;\nSeparate sentences;\nfor every sentence do\nMove sentence to new step;\nFill missing verification advise;\nend for\nend for\nend if\nend for\n6\nTOOL EVALUATION\nIn this section, we present our tool evaluation. Here we aim to\nanswer the research question RQ2:“How precise is our tool in the\ntask of removing natural language test smells?”\n6.1\nPlanning\nThe goal of this evaluation is to validate the tool in terms of Pre-\ncision, Recall, and F-Measure metrics. To accomplish the goal, we\nfirst execute our tool against the 973 natural language tests from\nA Catalog of Transformations to Remove Smells From\nNatural Language Tests\nEASE 2024, June 18–21, 2024, Salerno, Italy\nUbuntu OS, yielding 8, 386 test smell occurrences. However, vali-\ndating more than eight thousand transformations manually would\nbe infeasible. Therefore, we perform a manual analysis in a sample\nof 264 randomly selected smell occurrences (90% confidence with\n5% margin of error, using the Cochran’s Sample Size Formula [3]).\nTable 3 distributes the total and sampled occurrences per test smell.\nTable 3: Total and sample occurrences per test smell\nTest Smell\nTotal\nSample\nUnverified Action\n1,967\n67\nMisplaced Precondition\n49\n5\nMisplaced Action\n345\n8\nMisplaced Verification\n426\n16\nEager Action\n2,663\n69\nAmbiguous Test\n2,656\n90\nConditional Test\n279\n9\nTOTAL\n8, 386\n264\nFor each transformation, three authors analyzed and validated\nwhether the tool performed the transformations steps correctly\nor not, assuming that the transformation itself has been correctly\nchosen [19]. In eight cases out of 264 occurrences (0.03%), they\ndisagreed. However, the researchers reached a consensus in all\ncases after discussions. They collected the results in terms of true\npositives (𝑇𝑃)—the smell was corrected—, false positives (𝐹𝑃)—the\nsmell was not corrected—, and false negatives (𝐹𝑁)—no correction\nwas attempted.\n6.2\nResults\nAs results, we achieved the following: a Precision of 86.75%, a Recall\nof 80.85%, and a F-Measure of 83.70%. Table 4 illustrates our results\nfor each transformation presented in Section 3.\nNotice that the Fill Verification transformation reached the high-\nest 𝑇𝑃percentage. On the other hand, the Separate Action trans-\nformation reached the lowest 𝑇𝑃percentage. Two extract transfor-\nmations reached 25% false positive rate: Extract Action and Extract\nVerification. Last but not least, the Separate Action transformation\nreached 36.23% false negative rate.\nTable 4: TP, FP, and FN per transformation\nTransformation\nTP %\nFP %\nFN %\nFill Verification\n89.55%\n2.99%\n7.46%\nExtract Precondition\n80.00%\n0.00%\n20.00%\nExtract Action\n62.50%\n25.00%\n12.50%\nExtract Verification\n62.50%\n25.00%\n12.50%\nSeparate Actions\n50.72%\n13.04%\n36.23%\nExtract Ambiguity\n76.67%\n12.22%\n11.11%\nExtract Conditional\n77.78%\n11.11%\n11.11%\n6.3\nDiscussion\nAfter having the results illustrated in Table 4, we performed an in-\ndepth analysis on some of our 264 sample tests to better understand\npotential problems of our tool.\nWhen there is poor test writing (e.g., wrong phrases, excessive\nuse of special characters, wrong formatting, etc.), the tool malfunc-\ntions, mostly adding new special characters and breaking phrases\ninto small ones. Also, in some tests, the special characters led the\ntool to apply the Extract Action and Extract Verification in a wrong\nway. For example, the tool detected the following action as a verifi-\ncation: “Re-Check release-setting [...],” leading the Extract Verification\ntransformation to wrongly move the action. Here, spaCy separated\n“Re-Check” in two words, and then the “Check” verb was considered\nas a verification.\nAs our tool depends on semantic pattern matching coupled with\nsyntax analysis, a transformation that relies on verb placement\nsuch as Separate Actions is heavily affected by spaCy rules that\ndo not comprehend some cases, such as in the following phrase:\n“Select a plugin and configure it by doing click on ‘Configure’”. In this\nexample, the tool considered “Configure” as an action. This way, it\ncreated a Step 𝑆𝑖containing only the “Configure” word.\nTo sum up, the results presented in Table 4 provide a comprehen-\nsive overview of the effectiveness of each transformation, guiding\nfurther refinements and optimizations in our tool.\nAnswer to RQ2. Our analysis shows promising tool results,\nachieving a F-Measure of 83.70%.\n6.4\nThreats to Validity\nAs a threat to external validity, we have validated the tool in 264\ntest smells occurrences, but all the sample is based only on one\nsystem (Ubuntu OS). This way, it may be difficult to generalize\nto other systems. To validate the tool, we performed a manual\nanalysis, which poses threats to internal validity. The subjectivity\nof the process and the unawareness regarding technical items of\nthe tests may led the authors who performed the manual analysis\nto disagreements and to commit errors. We minimize this threat\nby considering three persons to analyze the transformations. All\ndisagreements have reached a consensus.\n7\nRELATED WORK\nHauptmann et al. [10] firstly applied the idea of smells to natural\nlanguage tests. A set of seven test smells has been proposed and\nanalyzed in real-practice tests. Ten years later, Soares et al. [19]\ncomplemented the list of natural language test smells with six more.\nBoth works presented techniques to identify the smells. Whilst\nHauptmann et al. used keywords and metrics, Soares et al. used\nnewer NLP-based technologies. However, differently from our work,\nnone of them presented neither transformations nor tools to remove\nthe smells automatically.\nOur work is also related to the software requirements field, since\nsome test smells are similar to smells that appear in requirements.\nFor example, Femmer et al. [5] define a set of requirements smells\nfrom ISO 29148. As an example, they define the Ambiguous Adverbs\nand Adjectives requirement smell, which is quite related to the Am-\nbiguous Test smell. Also, Rajkovic and Enoiu [18] proposed a tool,\nnamed NALABS, to identify requirements smells, like Vagueness,\nOptionality, and Subjectivity (all of them also related to the Am-\nbiguous Test). Recently, Fischbach et al. [7] implemented a tool to\ngenerate a minimal set of test cases to a given set of requirements\ncontaining conditionals. The requirement smell explored is very\nEASE 2024, June 18–21, 2024, Salerno, Italy\nAranda, et al.\nrelated to the Conditional Test. These works provide tools to identify\nthe requirements smells, but not to remove them automatically as\nwe do in the context of natural language test smells.\nFischbach et al. [6] conducted a survey to understand how re-\nquirements engineers interpret conditionals in software require-\nments. The authors found that the conditionals led the engineers\nto interpret the requirements in an ambiguous way. In our work,\nwe considered an analogous smell, i.e., the Conditional Test smell.\nWe also conducted a survey and our results show that the majority\nof the respondents agrees with our transformation to remove the\nConditional Test smell.\n8\nIMPLICATIONS FOR PRACTICE\nAs implications for practitioners, our catalog and tool might provide\na standardized and systematic approach to address natural language\ntest smells, promoting consistency in test practices across projects\nand teams and ensuring that similar issues are addressed uniformly.\nAlso, they can help to improve test quality and maintenance. In\naddition, our tool avoids repetitive tasks related to identifying and\nfixing test smells, which provides time and cost savings and could\nlead practitioners to focus on more complex and valuable aspects\nof testing. Last but not least, our research could also bring im-\nplications for the training and development of software testing\nprofessionals. By providing a catalog of transformations along with\na tool to remove test smells, we introduce learning resources for\nunderstanding what these smells are and how to address them.\nFor researchers, our catalog represents a step forward towards\nmore research on NLP-based techniques to avoid and remove smells\nin natural language tests. For instance, researchers can build upon\nour catalog as a starting point to develop more transformations\nand improve our propositions. Our work might also be a prelimi-\nnary benchmark for evaluating and comparing different NLP-based\ntechniques and tools to remove natural language test smells.\n9\nCONCLUDING REMARKS\nIn this paper, we introduced a catalog of transformations to remove\nseven natural language test smells. We assessed the quality of our\ncatalog by recruiting 15 software testing professionals. The profes-\nsionals found the transformations valuable, which is supported by\nthe high average acceptance rate. We also introduced a tool that\nimplements the catalog. We executed the tool in 264 occurrences\nof test smells and achieved 83.70% of F-Measure rate.\nAs future work, we intend to (i) increase the set of transfor-\nmations to include other smells (e.g., Tacit Knowledge [19], Test\nClones [10], and Long Test Steps [10]); and (ii) use Large Language\nModels (LLMs) to improve the effectiveness of our tool.\nACKNOWLEDGMENTS\nThis work was partially funded by CNPq 312195/2021-4, 310313/2022-\n8, 403361/2023-0, 443393/2023-0, 404825/2023-0, 315840/2023-4, FA-\nPEAL 60030.0000000462/2020, 60030.0000000161/2022, and FAPESB\nPIE002/2022 grants.\nREFERENCES\n[1] Wajdi Aljedaani, Anthony Peruma, Ahmed Aljohani, Mazen Alotaibi, Mo-\nhamed Wiem Mkaouer, Ali Ouni, Christian D. Newman, Abdullatif Ghallab,\nand Stephanie Ludi. 2021. Test Smell Detection Tools: A Systematic Mapping\nStudy. In EASE 2021. 170–180.\n[2] Diogo Almeida, José Creissac Campos, João Saraiva, and João Carlos Silva. 2015.\nTowards a catalog of usability smells. In SAC 2015. 175–181.\n[3] James E. Bartlett II, Joe W. Kotrlik, and Chadwick C. Higgins. 2001. Organizational\nresearch: Determining appropriate sample size in survey research. Information\ntechnology, learning, and performance journal 19, 1 (2001), 43–50.\n[4] Gabriele Bavota, Abdallah Qusef, Rocco Oliveto, Andrea De Lucia, and Dave\nBinkley. 2015. Are test smells really harmful? an empirical study. Empirical\nSoftware Engineering 20 (2015), 1052–1094.\n[5] Henning Femmer, Daniel Méndez Fernández, Stefan Wagner, and Sebastian Eder.\n2017. Rapid quality assurance with Requirements Smells. Journal of Systems and\nSoftware 123 (2017), 190–213.\n[6] Jannik Fischbach, Julian Frattini, Daniel Mendez, Michael Unterkalmsteiner,\nHenning Femmer, and Andreas Vogelsang. 2021. How Do Practitioners Interpret\nConditionals in Requirements?. In PROFES 2021. 85–102.\n[7] Jannik Fischbach, Julian Frattini, Andreas Vogelsang, Daniel Mendez, Michael\nUnterkalmsteiner, Andreas Wehrle, Pablo Restrepo Henao, Parisa Yousefi, Tedi\nJuricic, Jeannette Radduenz, and Carsten Wiecher. 2023. Automatic creation of\nacceptance tests by extracting conditionals from requirements: NLP approach\nand case study. Journal of Systems and Software 197 (2023), 111549.\n[8] Martin Fowler and Kent Beck. 1997. Refactoring: Improving the design of existing\ncode.\n[9] Vahid Garousi and Barış Küçük. 2018. Smells in software test code: A survey of\nknowledge in industry and academia. Journal of systems and software 138 (2018),\n52–81.\n[10] Benedikt Hauptmann, Maximilian Junker, Sebastian Eder, Lars Heinemann,\nRudolf Vaas, and Peter Braun. 2013. Hunting for smells in natural language\ntests. In ICSE 2013. 1217–1220.\n[11] Benedikt Hauptmann, Maximilian Junker, Sebastian Eder, Elmar Juergens, and\nRudolf Vaas. 2012. Can clone detection support test comprehension?. In ICPC\n2012. 209–218.\n[12] Matthew Honnibal and Ines Montani. 2024. spaCy – Industrial-strength Natural\nLanguage Processing in Python. https://spacy.io/\n[13] Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Build-\ning a Large Annotated Corpus of English: The Penn Treebank. Computational\nLinguistics 19, 2 (1993), 313–330. https://aclanthology.org/J93-2004\n[14] Luana Martins, Heitor Costa, and Ivan Machado. 2023. On the diffusion of test\nsmells and their relationship with test code quality of Java projects. Journal of\nSoftware: Evolution and Process (2023).\n[15] Luana Martins, Heitor Costa, Márcio Ribeiro, Fabio Palomba, and Ivan Machado.\n2023. Automating Test-Specific Refactoring Mining: A Mixed-Method Investiga-\ntion. In SCAM 2023. 13–24.\n[16] Gerard Meszaros. 2006. XUnit Test Patterns: Refactoring Test Code. Prentice Hall\nPTR, USA.\n[17] Joakim Nivre, Marie-Catherine de Marneffe, Filip Ginter, Yoav Goldberg, Jan\nHajič, Christopher D. Manning, Ryan McDonald, Slav Petrov, Sampo Pyysalo,\nNatalia Silveira, Reut Tsarfaty, and Daniel Zeman. 2016. Universal Dependencies\nv1: A Multilingual Treebank Collection. In LREC 16. 1659–1666.\n[18] Kostadin Rajkovic and Eduard Paul Enoiu. 2022. NALABS: Detecting Bad Smells\nin Natural Language Requirements and Test Specifications. Technical Report.\nMälardalen Real-Time Research Centre, Mälardalen University. http://www.es.\nmdu.se/publications/6382-\n[19] Elvys Soares, Manoel Aranda, Naelson Oliveira, Márcio Ribeiro, Rohit Gheyi,\nEmerson Souza, Ivan Machado, André Santos, Baldoino Fonseca, and Rodrigo\nBonifácio. 2023. Manual Tests Do Smell! Cataloging and Identifying Natural\nLanguage Test Smells. In ESEM 2023. 1–11.\n[20] Elvys Soares, Márcio Ribeiro, Guilherme Amaral, Rohit Gheyi, Leo Fernandes,\nAlessandro Garcia, Baldoino Fonseca, and André Santos. 2020. Refactoring Test\nSmells: A Perspective from Open-Source Developers. In SAST 2020. 50–59.\n[21] Elvys Soares, Márcio Ribeiro, Rohit Gheyi, Guilherme Amaral, and André Santos.\n2023. Refactoring Test Smells With JUnit 5: Why Should Developers Keep Up-to-\nDate? IEEE Transactions on Software Engineering 49, 3 (2023), 1152–1170.\n[22] Davide Spadini, Martin Schvarcbacher, Ana-Maria Oprescu, Magiel Bruntink,\nand Alberto Bacchelli. 2020. Investigating Severity Thresholds for Test Smells. In\nMSR 2020. 311–321.\n[23] Manoel Terceiro, Naelson Oliveira, Elvys Soares, Márcio Ribeiro, Davi Romão,\nUllyanne Patriota, Rohit Gheyi, Emerson Souza, and Ivan Machado. 2024. A\nCatalog of Transformations to Remove Test Smells in Natural Language Tests -\nReplication Package. https://doi.org/10.6084/m9.figshare.24993906.v1\n[24] Manoel Terceiro, Naelson Oliveira, Elvys Soares, Márcio Ribeiro, Davi Romão,\nUllyanne Patriota, Rohit Gheyi, Emerson Souza, and Ivan Machado. 2024. Manual\nTest Alchemist. Retrieved April 19, 2024 from https://github.com/easy-software-\nufal/manual-test-alchemist\n[25] Ubuntu. 2024. Ubuntu Manual Tests in Launchpad. https://launchpad.net/ubuntu-\nmanual-tests\n[26] Arie Van Deursen, Leon Moonen, Alex Van Den Bergh, and Gerard Kok. 2001.\nRefactoring test code. In XP 2001. 92–95.\n",
  "categories": [
    "cs.SE",
    "D.2.5"
  ],
  "published": "2024-04-25",
  "updated": "2024-04-25"
}