{
  "id": "http://arxiv.org/abs/2404.19043v1",
  "title": "Improving Interpretability of Deep Active Learning for Flood Inundation Mapping Through Class Ambiguity Indices Using Multi-spectral Satellite Imagery",
  "authors": [
    "Hyunho Lee",
    "Wenwen Li"
  ],
  "abstract": "Flood inundation mapping is a critical task for responding to the increasing\nrisk of flooding linked to global warming. Significant advancements of deep\nlearning in recent years have triggered its extensive applications, including\nflood inundation mapping. To cope with the time-consuming and labor-intensive\ndata labeling process in supervised learning, deep active learning strategies\nare one of the feasible approaches. However, there remains limited exploration\ninto the interpretability of how deep active learning strategies operate, with\na specific focus on flood inundation mapping in the field of remote sensing. In\nthis study, we introduce a novel framework of Interpretable Deep Active\nLearning for Flood inundation Mapping (IDAL-FIM), specifically in terms of\nclass ambiguity of multi-spectral satellite images. In the experiments, we\nutilize Sen1Floods11 dataset, and adopt U-Net with MC-dropout. In addition, we\nemploy five acquisition functions, which are the random, K-means, BALD,\nentropy, and margin acquisition functions. Based on the experimental results,\nwe demonstrate that two proposed class ambiguity indices are effective\nvariables to interpret the deep active learning by establishing statistically\nsignificant correlation with the predictive uncertainty of the deep learning\nmodel at the tile level. Then, we illustrate the behaviors of deep active\nlearning through visualizing two-dimensional density plots and providing\ninterpretations regarding the operation of deep active learning, in flood\ninundation mapping.",
  "text": "Improving Interpretability of Deep Active Learning for\nFlood Inundation Mapping Through Class Ambiguity\nIndices Using Multi-spectral Satellite Imagery\nHyunho Leea, Wenwen Lia,∗\naSchool of Geographical Sciences and Urban Planning, Arizona State\nUniversity, Tempe, 85287-5302, AZ, USA\nAbstract\nFlood inundation mapping is a critical task for responding to the increasing\nrisk of flooding linked to global warming. Significant advancements of deep\nlearning in recent years have triggered its extensive applications, including\nflood inundation mapping.\nTo cope with the time-consuming and labor-\nintensive data labeling process in supervised learning, deep active learning\nstrategies are one of the feasible approaches. However, there remains limited\nexploration into the interpretability of how deep active learning strategies\noperate, with a specific focus on flood inundation mapping in the field of re-\nmote sensing. In this study, we introduce a novel framework of Interpretable\nDeep Active Learning for Flood inundation Mapping (IDAL-FIM), specifi-\ncally in terms of class ambiguity of multi-spectral satellite images. In the\nexperiments, we utilize Sen1Floods11 dataset, and adopt U-Net with MC-\ndropout. In addition, we employ five acquisition functions, which are the\nrandom, K-means, BALD, entropy, and margin acquisition functions. Based\non the experimental results, we demonstrate that two proposed class am-\nbiguity indices are effective variables to interpret the deep active learning\nby establishing statistically significant correlation with the predictive uncer-\ntainty of the deep learning model at the tile level. Then, we illustrate the\nbehaviors of deep active learning through visualizing two-dimensional den-\nsity plots and providing interpretations regarding the operation of deep active\nlearning, in flood inundation mapping.\n∗Corresponding author.\nEmail address: wenwen@asu.edu (Wenwen Li)\narXiv:2404.19043v1  [cs.CV]  29 Apr 2024\nKeywords:\nflood mapping, class uncertainty, remote sensing, Sentinel-2,\nexplainable artificial intelligence, XAI\n1. Introduction\nFlood inundation mapping, which determines the extent of the flooded\narea including depth, velocity and uncertainty (Bentivoglio et al., 2022; Mer-\nwade et al., 2008; Horritt, 2006), is increasingly important due to the intensifi-\ncation of extreme precipitation worldwide. This intensification is anticipated\ndue to global warming. Rising Earth’s average temperatures lead to higher\nwater vapor concentrations in the atmosphere, consequently contributing to\nmore extreme precipitation occurrences (Tabari, 2020). Significantly, the ex-\ntreme values, representing the 90th percentile value of precipitation duration\nfor each year globally, of long-duration flood events have exceeded 30 days\nin the recent decade, whereas they were less than 20 days in the 1980s and\n1990s (Najibi and Devineni, 2018). In addition, between 2000 and 2018, an\nestimated 255-290 million people were directly affected by floods in areas\nobserved by satellites (Tellman et al., 2021). Therefore, to respond to the\nrisks posed by floods, flood inundation mapping plays a fundamental role in\nnear real-time monitoring, damage assessment, post-flood evacuation, and\nprotection planning (Bentivoglio et al., 2022; Iqbal et al., 2021).\nIn the past decade, notable advancements have been made in deep learn-\ning, particularly with the introduction of Convolutional Neural Networks\n(CNNs; Jia et al., 2014). These advances have enabled automated and data-\ndriven analysis of large imagery, and they have also triggered extensive ap-\nplications of deep learning in environmental monitoring using remote sensing\nimagery (Li and Hsu, 2022; Li et al., 2024). Furthermore, this research trend,\ncoupled with advances in Earth observation data and high-performance com-\nputing, has led to the emergence of Geospatial Artificial Intelligence (GeoAI;\nLi, 2020), an interdisciplinary research area that applies and extends AI for\ngeospatial problem solving.\nFlood inundation mapping with remote sensing images, an important ap-\nplication of GeoAI, is primarily focused on identifying flooded areas from the\ngiven satellite images using deep learning models. Therefore, research in this\narea has predominantly centered on semantic segmentation which partitions\nan image into distinct regions corresponding to predefined classes. Previous\nresearch have shown that deep learning models, such as Fully Convolutional\n2\nNeural Network (FCN; Long et al., 2015), U-Net (Ronneberger et al., 2015),\nDeepLabV3+ (Chen et al., 2018), HRNet (Wang et al., 2020), outperformed\ntraditional methods including rule- and threshold-based approaches in flood\nmapping (Dong et al., 2021; Helleis et al., 2022). Additionally, new learning\nstrategies, such as dilated convolution (Yu and Koltun, 2015; Yu et al., 2017),\nwere integrated into deep learning-based segmentation models to further im-\nprove the models’ predictive performance (Nogueira et al., 2018; Wang et al.,\n2022). Recently, new geospatial foundation models such as Prithvi (Li et al.,\n2023) were applied to flood mapping to assess their generalizability.\nDespite these advances in AI model architecture for flood inundation\nmapping, the time-consuming and labor-intensive data annotation process\nremains a bottleneck in enabling supervised learning (Buscombe et al., 2022;\nBeluch et al., 2018; Takezoe et al., 2023). The data labeling process can be\ndivided into two stages: (1) selecting or sampling image tiles, and (2) labeling\nthe selected tiles in satellite images. Active learning, which aims to identify\na small yet highly informative set of data points for machine learning, is an\neffective approach to address the selection of data samples to reduce labeling\ncost (Settles, 2009; Cohn et al., 1996). In particular, active learning for deep\nlearning models is referred to as deep active learning (Takezoe et al., 2023).\nIn the field of remote sensing, research has applied deep active learning to\nsatellite image segmentation and change detection (Rˇuˇziˇcka et al., 2020; Li\net al., 2022). However, there remains limited exploration in interpreting how\ndeep active learning operates, especially within the context of flood inunda-\ntion mapping. This paper aims to bridge the knowledge gap by interpreting\ndeep active learning in flood inundation mapping, with a specific focus on\nclass ambiguity extracted from input satellite images.\nThe main contributions of this study are:\n(1) We introduce a novel framework of Interpretable Deep Active Learn-\ning for Flood inundation Mapping (IDAL-FIM) to enhance the inter-\npretability of deep active learning operations.\n(2) We demonstrate that the correlation between the two proposed class\nambiguity indices (boundary pixel ratio and Mahalanobis distance for\nflood-segmentation) and predictive uncertainty of the deep learning\nmodel are statistically significant at the tile level. This finding allows\nus to interpret the behavior of deep active learning using the proposed\nindices.\n3\n(3) We illustrate that the behaviors of deep active learning can be\nvisually interpreted through two-dimensional density plots, which show\nthe distribution patterns of selected data points to be labeled in the\nIDAL-FIM framework.\nTo achieve this research goal, the paper is structured as follows: Section\n2 provides a review of relevant literature; Section 3 describes the IDAL-FIM\nframework, acquisition functions, and proposes two class ambiguity indices;\nSection 4 explains the experimental setup; Section 5 presents the experi-\nmental results; Section 6 provides a discussion and interpretation about the\nbehavior of deep active learning in the context of flood mapping. Finally,\nin Section 7, we conclude the work, discuss limitations and propose future\nresearch directions.\n2. Literature review\n2.1. Multi-spectral Satellite Image Collection for Deep Learning-Based Flood\nInundation Mapping\nThere are two distinct approaches used to collect flood-observed training\ndata for deep learning in flood inundation mapping: (1) region-specific satel-\nlite image collection and (2) global satellite image collection. The collection\nof multi-spectral satellite images which were captured during flood events\nin a specific region is only feasible when a sufficient amount of data can be\nacquired. Therefore, this approach is applicable to study areas that experi-\nence recurrent flood damage over the years and cover a relatively extensive\ngeographical area. Examples of such study areas are the Yangtze River Basin\nand Lake Poyang in China, as well as the Atlantic coast of the southeastern\nUnited States, including nearby urban areas frequently affected by hurricanes\n(Peng et al., 2019; Mu˜noz et al., 2021; Wang et al., 2022; Zhang and Xia,\n2021).\nOn the other hand, flood events are infrequent hydrological phenomena;\ntherefore, securing a sufficient amount of training samples in a specific region\nis almost infeasible, except for a few regions stated above. Instead, collecting\ntraining data containing flood events from diverse global locations has be-\ncome a feasible solution. This has especially benefited from the availability\nof cloud platforms such as Google Earth Engine, NASA Earth Exchange,\nand Sentinel Hub which facilitate access to and processing of vast amounts\nof satellite imagery (Zhao et al., 2022). During satellite image collection,\n4\nresearchers often acquire images across diverse climates, atmospheric condi-\ntions, and land settings to ensure the generalizability of deep learning mod-\nels (Wieland et al., 2023; Shastry et al., 2023; Tellman et al., 2021; Bonafilia\net al., 2020; Wieland and Martinis, 2019). However, to the best of our knowl-\nedge, there have been very few studies (Popien et al., 2021) investigating the\nimpact of training data selection on the predictive performance of deep learn-\ning models in flood inundation mapping.\n2.2. Deep Active Learning\nActive Learning (AL) is designed to improve the performance of ma-\nchine learning models by utilizing fewer training data (Settles, 2009; Cohn\net al., 1996). The pool-based sampling scenario, employed in this study, is\none of the typical scenarios of active learning, which assumes a large pool\nof unlabeled data points along with a small initial labeled data set. In each\niteration, a model is trained using labeled data in a supervised learning man-\nner. An acquisition function prioritizes informative data points and guides\nthe selection of unlabeled data points from a pool. Unlabeled data selected\nthrough the acquisition function are labeled by human experts and then in-\ntegrated into the existing training data. This iterative process is repeated,\nwherein the model is trained from scratch using the newly incorporated la-\nbeled data, until a specific level of model performance is reached (Beluch\net al., 2018; Gal et al., 2017).\nDeep Active Learning (DAL) combines the advantages of active learn-\ning, which effectively reduces labeling costs by selecting informative data\npoints for model training, with a deep learning model, known for excep-\ntional high-dimensional data processing and automatic feature extraction\n(Ren et al., 2021).\nIn DAL, the acquisition functions are mainly catego-\nrized into uncertainty-based and density-based acquisition functions (Take-\nzoe et al., 2023; Beluch et al., 2018). Both categories of acquisition functions\nare relying on specific assumptions to select informative data points.\nThe uncertainty-based acquisition function evaluates the informativeness\nof unlabeled data under the assumption that data points with higher uncer-\ntainty provide more information for model training (Settles, 2009). In the\ncontext of flood mapping, high uncertainty data points include satellite im-\nages capturing complex boundary patterns in flooded areas, as well as areas\nexhibiting spectral reflectance similar to flooded areas, such as non-flooded\nvegetated areas or cloud shadows. Deep learning models encounter more dif-\nficulty in classifying pixels in these satellite images into the correct classes.\n5\nTherefore, by training on data points with high uncertainty, the model im-\nproves its ability to identify between classes in flood mapping, which can\neventually enhance the model’s performance (Takezoe et al., 2023).\nRegarding uncertainty estimation in deep learning, recent research has\npointed out that deep learning models often exhibit overconfidence in their\npredictions, especially when making misclassifications (Guo et al., 2017). To\nenhance the reliability of predictions, uncertainty estimation methods focus\non calibrating the predictions instead of relying solely on a single prediction\n(Wang et al., 2023). For this reason, prior studies on DAL (Rˇuˇziˇcka et al.,\n2020; Beluch et al., 2018; Gal et al., 2017) employed uncertainty estima-\ntion methods to measure more reliable predictions for relevant acquisition\nfunctions.\nParticularly, in previous studies on the segmentation of remote sensing\nimagery, three uncertainty estimation methods were utilized to obtain uncer-\ntainty from deep learning models at the pixel-level: (1) Monte-Carlo dropout\n(MC-dropout; Gal and Ghahramani, 2016), (2) deep ensembles (Lakshmi-\nnarayanan et al., 2017), and (3) fully-Bayesian CNN (LaBonte et al., 2019).\nThe MC-dropout method derives uncertainty estimates by regarding dropout\ntraining in deep neural networks as an approximation of Bayesian inference\nwithin deep Gaussian processes (Gal and Ghahramani, 2016). In practice,\napproximate Bayesian inference in deep learning models makes use of multi-\nple inferences with different dropout masks. In remote sensing studies, the\nMC-dropout method was employed to enhance prediction performance and\nprovide uncertainty estimation, in Land Use and Land Cover (LULC) tasks\n(Kampffmeyer et al., 2016; Dechesne et al., 2021). On the other hand, deep\nensembles utilize an ensemble of multiple deep learning models to estimate\nuncertainty, and the final output of deep ensembles is generally the averaged\nsoftmax vectors of each ensemble model (Beluch et al., 2018). In previous\ndeep learning-based roads segmentation, deep ensembles were shown to out-\nperform MC-dropout in pixel-level prediction, despite their significant com-\nputational cost (Haas and Rabus, 2021). More recently, another study com-\npared the reliability of uncertainty estimation methods between MC-dropout\nand fully-Bayesian CNN (LaBonte et al., 2019) in water segmentation (Hertel\net al., 2023). The fully-Bayesian CNNs learn the distribution of the weight\nspace instead of a single value. For implementation of the fully-Bayesian\nCNNs, the authors utilized the Bayesian Layers library (Tran et al., 2019) in\nTensorFlow Probability (Dillon et al., 2017). Their conclusion was that fully-\nBayesian CNNs were more reliable than MC-dropout in estimating pixel-level\n6\npredictive uncertainty (Hertel et al., 2023). However, the implementation of\nfull-Bayesian CNNs requires specific libraries and additional training time to\ndetermine weight distributions, compared to MC-dropout.\nIn contrast to uncertainty-based acquisition, density-based acquisition\nfunctions leverage the feature space of the input data (Takezoe et al., 2023).\nThis type of acquisition function is grounded in the assumption that data\npoints maximizing the diversity of data features are informative (Xie et al.,\n2020). However, while density-based acquisition functions have been primar-\nily studied for classification tasks in computer vision, no relevant research on\nthe segmentation of remote sensing images could be found.\nUncertainty measures in deep active learning are categorized into\npredictive uncertainty measures and model uncertainty measures. Predictive\nuncertainty is mainly estimated using a measure of entropy (Shannon, 1948)\nor margin (Scheffer et al., 2001), which is based on the class probability as-\nsigned to each pixel by the model. On the other hand, model uncertainty is\ncommonly quantified by measuring the variance in predictions resulting from\naveraging over multiple models trained on consistent training data (Laksh-\nminarayanan et al., 2017; Gal et al., 2017), such as Bayesian Active Learning\nby Disagreement (BALD; Houlsby et al., 2011).\n2.3. Deep Active Learning in the Field of Remote Sensing\nFlood inundation mapping mainly utilizes deep learning models for se-\nmantic segmentation to extract the distribution of water bodies along with\ndetailed boundaries. However, most research on DAL in the field of remote\nsensing had focused on pixel classification, which focuses on predicting pre-\ndefined classes by considering properties of a single pixel. Such work does\nnot consider partitioning image scenes into semantically meaningful areas,\nknown as the task of semantic segmentation. Even before deep active learn-\ning research, a substantial number of studies have investigated active learn-\ning for pixel classification, utilizing machine learning algorithms, including\nSupport Vector Machine (SVM), Random Forest (RF), and Artificial Neu-\nral Network (ANN), across both multi-spectral and hyper-spectral imagery\n(Thoreau et al., 2022; Ruiz et al., 2013; Stumpf et al., 2013; Pasolli et al.,\n2013; Crawford et al., 2013; Li et al., 2011; Tuia et al., 2011a,b, 2009; Rajan\net al., 2008; Mitra et al., 2004). Recently, there has been research on deep\nactive learning for pixel classification using remote sensing imagery (Patel\nand Patel, 2023; Di et al., 2023; Cao et al., 2020). In particular, various DAL\nstudies have been conducted for pixel classification based on hyper-spectral\n7\nsatellite images. Liu et al. (2016) introduced a DAL scheme that utilizes\nthe Deep Belief Network (DBN), and Haut et al. (2018) presented a DAL\nframework using CNNs with MC-dropout. In addition, Lei et al. (2021) pro-\nposed a DAL framework that includes an auxiliary light network, which is\nresponsible for the uncertainty prediction of unlabeled samples.\nUnlike studies focused on pixel classification, research on DAL for se-\nmantic segmentation using remote sensing images is limited, with only a few\nstudies in the field of remote sensing. Rˇuˇziˇcka et al. (2020) investigated deep\nactive learning, employing deep ensembles and the Monte Carlo Batch Nor-\nmalization (MCBN) method for change detection and map updating. The\nauthors demonstrated that their proposed DAL framework not only identi-\nfies highly informative samples but also automatically balances classes in the\ntraining data within the specific number of samples, even in the presence\nof an extreme class imbalance in the pool of unlabeled data. In addition,\nLi et al. (2022) proposed a DAL framework for building mapping to reduce\nthe effort of data labeling. Their framework integrates two deep learning\nmodels, U-Net and DeepLabV3+, along with uncertainty-based acquisition\nfunctions. Furthermore, the authors utilized landscape metrics to provide\na summary of the preliminary suggestions for data labeling. However, they\nonly used landscape metrics to describe the characteristics of the selected\ndata points in active learning, without quantifying the relationship between\nthese indices and the operation of active learning. Thus far, based on our\ncomprehensive review, there is a notable absence of studies interpreting the\nbehavior of deep active learning in the field of remote sensing.\n2.4. Uncertainty Propagation Theory and Uncertainty Descriptors in Remote\nSensing\nUncertainty propagation refers to quantifying how uncertainty in input or\nmodel parameter values affects the uncertainty of model predictions or com-\nputational procedure outputs (Wallach and G´enard, 1998; Lee and Chen,\n2009; Crosetto et al., 2001). Research on uncertainty propagation in remote\nsensing has been conducted based on the recognition of its importance in en-\nsuring the reliability and accuracy of high-level products essential for global\nchange research and environmental management decision-making (Crosetto\net al., 2001).\nIn the case of the multi-spectral satellite imagery, the pri-\nmary sources of uncertainty are sub-pixel mixing, spatial mis-registration,\nand sensor sampling bias (Bastin et al., 2002).\n8\nIn the previous study, Zhang and Zhang (2019) introduced two uncer-\ntainty descriptors, designed to quantitatively measure uncertainty when clas-\nsifying pixels in remote sensing, based on the uncertainty propagation theory.\nThe first descriptor, spatial distribution uncertainty, addresses the impact of\nadjacency effects within remote sensing images, while the second, semantic\nuncertainty, aims to quantify the considerable intra-class variations. The ra-\ntionale behind the two descriptors is that, due to the spatial and spectral\nresolution limitations of the sensors, ambiguity arises at the pixel level when\nclassifying objects. The main limitation of this study is that proposed uncer-\ntainty descriptors can only be calculated based on the labeled data or results\nof image segmentation prior to prediction.\n3. Method\n3.1. The Framework of Interpretable Deep Active Learning for Flood Inun-\ndation Mapping\nDeep active learning demonstrates strong predictive performance im-\nprovement, but its process is opaque and interpretability is limited due\nto the black-box nature of deep learning models (Goodchild and Li, 2021;\nHsu and Li, 2023).\nIn this study, we introduce the framework of Inter-\npretable Deep Active Learning for Flood Inundation Mapping (IDAL-FIM).\nThe main purpose of the IDAL-FIM framework is to provide interpreta-\ntion for the deep active learning operation in flood inundation mapping.\nFig. 1 illustrates the process of the IDAL-FIM framework. Our proposed\nframework assumes a pool-based sampling scenario and, accordingly, con-\nsists of five stages.\nIn the first stage, satellite images are collected glob-\nally to build an unlabeled data pool. The metadata regarding flood events,\nsuch as observation period and geographic coordinates, can be obtained from\nwebsites of agencies responsible for flood monitoring, including the United\nNations Satellite Centre (UNOSAT; https://unosat.org/products/, accessed\non 23 April 2024), the Copernicus Emergency Management Service (EMS;\nhttps://emergency.copernicus.eu/, accessed on 23 April 2024) and the Dart-\nmouth Flood Observatory (https://floodobservatory.colorado.edu/, accessed\non 23 April 2024) (Brakenridge, 2010). After that, a small number of satellite\nimages are selected from the unlabeled data pool and annotated to create the\ninitial training data. In addition, for validation and testing purposes, labeled\ndata is generated for satellite images that capture occurrences of flooding in\n9\nFigure 1: The process of the IDAL-FIM framework.\nthe target area. Then, the satellite images and corresponding labeled data\nare split into training, validation, and testing data.\nAfter the initial stage, the following stages are briefly outlined in this\nsection, with a more in-depth explanation presented in separate sections.\nIn the second stage, a deep learning model for flood inundation mapping\nis trained. In this study, as a deep learning model for segmentation tasks,\nU-Net with MC-dropout was employed for uncertainty estimation and per-\nformance evaluation. More details about the U-Net with MC-dropout are\nprovided in Section 3.3. In the third stage, the performance of the trained\ndeep learning model is evaluated using test data based on the performance\nmetric. Section 4.3 covers detailed configuration for the iteration of deep\nlearning model training and evaluation in the IDAL-FIM framework. In the\nfourth stage, an acquisition function selects most informative satellite images\nfrom an unlabeled data pool. Subsequently, human experts create labeled\ndata using the newly selected satellite images and then they are added to\nthe existing training data. Detailed explanations of the acquisition functions\nutilized in this study can be found in Section 3.2. Moreover, we demonstrate\nthe statistical significance of the rank correlation between class ambiguity\nindices and the scores obtained from uncertainty-based acquisition functions\nin Section 5.2. This statistical analysis aims to support the effectiveness and\n10\nvalidity of the class ambiguity indices in interpreting the behavior of active\nlearning. In the final stage, the characteristics of newly labeled multi-spectral\nsatellite images obtained in the prior stage are visualized based on the class\nambiguity indices. Two class ambiguity indices, which are Boundary Pixel\nRatio (BPR) and Mahalanobis Distance for Flood-segmentation (MDF), are\nfurther explained in Section 3.4.\n3.2. Acquisition Functions in the IDAL-FIM Framework\nIn the IDAL-FIM framework, the acquisition functions take on a pivotal\nrole in the selection of the informative data points, which is associated with\nreducing the number of labeled data points and improving predictive per-\nformance. In this study, we utilized two types of acquisition functions: (1)\nuncertainty-based and (2) density-based acquisition functions. Specifically,\nregarding uncertainty-based acquisition function, we leverage the predictive\nuncertainty and model uncertainty (Takezoe et al., 2023; Li et al., 2022;\nRˇuˇziˇcka et al., 2020; Beluch et al., 2018; Gal et al., 2017; Settles, 2009). For\npredictive uncertainty, entropy (Shannon, 1948) and margin (Scheffer et al.,\n2001) acquisition functions are employed. On the other hand, for model un-\ncertainty, Bayesian Active Learning by Disagreement (BALD; Houlsby et al.,\n2011) acquisition function is utilized. Furthermore, we implement a density-\nbased acquisition function using Principal Component Analysis (PCA) and\nK-Means algorithm and include the random acquisition function as a baseline\nmethod.\n3.2.1. Uncertainty-based acquisition function\nIn the IDAL-FIM framework, a deep learning model for segmentation\ntasks is trained employing the dropout technique on training data Dtrain.\nDuring the inference stage, T forward passes are performed. At each pass,\na new dropout mask is sampled, resulting in the model weight bωt at the\nt-th forward pass. In Eq.(1), the calibrated probability belonging to class c,\nlocated at (h, w), is computed as the average of T predicted probabilities\n(Gal et al., 2017; Wang et al., 2023). The notation yh,w represents the target\nclass at the pixel position (h, w). x denotes the input satellite tile image,\nand c denotes the predefined class.\np(yh,w = c|x, Dtrain) ≈1\nT\nT\nX\nt=1\np(yh,w = c|x, bωt)\n(1)\n11\nThe uncertainty-based acquisition function for segmentation tasks firstly\ncalculates pixel-wise uncertainty by applying each of the uncertainty mea-\nsures.\nThen the average uncertainty of all pixels is computed at the tile\nlevel. This output value becomes the score of the acquisition function for\nsegmentation tasks and is utilized to determine the priority of data points to\nbe labeled.\nEntropy. One of the most general uncertainty measures is entropy, which\nis an information-theoretic measure representing the amount of information\nneeded to encode a distribution. Therefore, entropy is commonly perceived\nas a metric of uncertainty in machine learning (Settles, 2009; Beluch et al.,\n2018). In the entropy acquisition function, pixel-level predictive uncertainty\nis quantified using entropy based on the calibrated class probability in Eq.(1).\nThen, the predictive uncertainty located at (h, w), uEntropy\nh,w\n, is calculated as\nin Eq.(2):\nuEntropy\nh,w\n= H[yh,w|x, Dtrain]\n= −\nC\nX\nc=1\np(yh,w = c|x, Dtrain) · log p(yh,w = c|x, Dtrain)\n≈−\nC\nX\nc=1\n( 1\nT\nT\nX\nt=1\np(yh,w = c|x, bωt)) · log( 1\nT\nT\nX\nt=1\np(yh,w = c|x, bωt)) (2)\nwhere H denotes entropy (Shannon, 1948). The score of the entropy ac-\nquisition function for segmentation sEntropy is the average of the pixel-level\npredictive uncertainties, and as the score becomes higher, the priority of\nselecting the data points also increases.\nsEntropy =\n1\nHW\nH\nX\nh=1\nW\nX\nw=1\nuEntropy\nh,w\n(3)\nMargin. The margin is defined as the difference between the probabilities of\nthe two most probable classes. A higher margin indicates that the model’s\nprediction is more certain, with lower predictive uncertainty, as the probabil-\nities of the two most probable classes are more distinct (Scheffer et al., 2001;\nBeluch et al., 2018). Notably, in the binary classification setting, the margin\n12\nfunction reduces to the entropy function, as both are equivalent to querying\nthe instance with a class posterior closest to 0.5 (Settles, 2009). Therefore,\nthe characteristics of the margin acquisition function and the entropy ac-\nquisition function become analogous in binary segmentation problems. The\npixel-level uncertainty measured by margin located at (h, w), uMargin\nh,w\n, is cal-\nculated as in Eq.(4) where c1 and c2 are the first and second most probable\nclass labels under the model, respectively.\nuMargin\nh,w\n= 1\nT\nT\nX\nt=1\np(yh,w = c1|x, bωt) −1\nT\nT\nX\nt=1\np(yh,w = c2|x, bωt)\n(4)\nThe score of the margin acquisition function for segmentation sMargin is cal-\nculated according to Eq.(5). As the score of the margin acquisition function\ndecreases, those data points are prioritized for selection.\nsMargin =\n1\nHW\nH\nX\nh=1\nW\nX\nw=1\nuMargin\nh,w\n(5)\nBALD (Bayesian Active Learning by Disagreement). BALD is defined as the\nmutual information between predictions and model posterior. This means\nthat the value of BALD-based function is maximized when the model gener-\nates uncertain predictions on average and also confidently produces disagree-\ning predictions simultaneously (Gal et al., 2017). Consequently, BALD was\nutilized as the measure of model uncertainty in the previous study (Jesson\net al., 2021) because it highlights the variability in class probabilities across\ndifferent stochastic forward passes (Gal et al., 2017). The pixel-level uncer-\ntainty using BALD located at (h, w), uBALD\nh,w\n, and the score of the BALD\nacquisition function for segmentation, sBALD, are calculated as in Eq.(6) and\n(7).\nuBALD\nh,w\n= H[yh,w|x, Dtrain] −Ep(ω|Dtrain)[H[yh,w|x, ω]]\n≈−\nC\nX\nc=1\n( 1\nT\nT\nX\nt=1\np(yh,w = c|x, bωt)) · log( 1\nT\nT\nX\nt=1\np(yh,w = c|x, bωt))\n−1\nT\nT\nX\nt=1\nC\nX\nc=1\n−p(yh,w = c|x, bωt) · log p(yh,w = c|x, bωt)\n(6)\n13\nsBALD =\n1\nHW\nH\nX\nh=1\nW\nX\nw=1\nuBALD\nh,w\n(7)\n3.2.2. Density-based acquisition function\nIn order to compare characteristics with uncertainty-based acquisition\nfunctions in the framework of IDAL-FIM, we implemented a simple density-\nbased acquisition function using Principal Component Analysis (PCA) and\nK-Means under the assumption that data points maximizing data feature\ndiversity are informative for training deep learning models (Xie et al., 2020;\nTakezoe et al., 2023). The density-based acquisition function used in this\nstudy has two steps. First, PCA was performed to reduce the dimensionality\nof the unlabeled multi-spectral satellite images. Here, the output of PCA is\nconsidered to be the features of the unlabeled data. Then, using the PCA\noutput as input, the K-means algorithm identified k new samples closest to\nthe centroid of each cluster.\nThe rationale behind selecting new samples\nclosest to the centroids formed by the K-means algorithm for each cluster is\nto maximize the diversity of features. This is because the objective of the K-\nmeans algorithm is to find clusters that are internally coherent but maximally\ndistinct from each other. We therefore name this acquisition function as K-\nmeans acquisition function.\nAlgorithm 1: Pseudocode of density-based acquisition function uti-\nlizing PCA and K-Means\nData: NewSamples, DataReducedDim, DataUnlabeledSat, NReducedDim,\nClusters\nResult: NewSamples\nNewSamples = [ ]\nDataReducedDim = Perform PCA on DataUnlabeledSat to reduce its\ndimension to NReducedDim\nClusters = Apply the K-means algorithm to cluster DataReducedDim\nfor cluster ∈Clusters do\nCalculate the centroid of the cluster\nFind the sample closest to the centroid within the cluster\nNewSamples ←−NewSamples ∪sample\nend\n14\n3.3. Deep Learning Model for Segmentation Task in the IDAL-FIM Frame-\nwork\nThe efficiency and effectiveness of the deep active learning framework are\nclosely linked to the choice of both a deep learning model and an uncer-\ntainty estimation method due to the iterative nature of active learning. We\nselected the U-Net as the deep learning model for segmentation within the\nIDAL-FIM framework, which was utilized in a recent uncertainty estimation\nstudy of water body mapping (Hertel et al., 2023). Furthermore, we con-\nsidered the following three factors to determine the uncertainty estimation\nmethod suitable for the IDAL-FIM framework. First, the computational cost\nof uncertainty estimation should be low since training and inference of the\ndeep learning model are repeatedly conducted within the IDAL-FIM frame-\nwork. MC-dropout achieves a lower computation cost by training a single\ndeep learning model and performing multiple inferences, compared to deep\nensembles and fully Bayesian CNNs. Second, as the uncertainty-based acqui-\nsition function for semantic segmentation computes the average uncertainty\nacross all pixels, the higher reliability of uncertainty estimation at a pixel-\nlevel holds relatively less importance in the IDAL-FIM framework. Lastly,\nthe ease of incorporating other deep learning models into the proposed frame-\nwork was also considered.\nFig. 2 illustrates the architecture of the U-Net with MC-dropout utilized\nin this study. This model consists of two main components: the encoder and\nthe decoder. Same as the existing U-Net model, the encoder utilizes a down-\nsampling process four times to extract features and reduce computational\ncost. This down-sampling process is composed of two convolutional layers\nthat increase the number of channels and a pooling layer that decreases the\nspatial resolution. These extracted features are then forwarded to the de-\ncoder, which has a symmetric structure to the encoder and employs a four\ntimes up-sampling process to reconstruct the spatial information of the in-\nput. U-Net with MC-dropout also integrates skip connections to capture\nprecise locations at each step of the decoder.\nThese skip connections in-\nclude concatenating the output of the decoder layers with the corresponding\nfeature maps from the encoder at the same level, thereby enhancing the pre-\ncision of pixel segmentation. Regarding MC-dropout, instead of using regular\ndropout, spatial dropout is applied at the end of the up-sampling process.\nSpatial dropout is a regularization technique in deep learning where specific\nproportions of two-dimensional feature maps are randomly set to zero on a\nper-channel basis during training to enhance model robustness and prevent\n15\nFigure 2: The architecture of U-Net with MC-dropout. The input image assumes uniform\nwidth and height (I).\nover-fitting (Tompson et al., 2015).\n3.4. Class Ambiguity Indices in the IDAL-FIM Framework\nWe present two class ambiguity indices to quantify the tile-level binary\ninter-class ambiguity of the input satellite image. For calculation of class am-\nbiguity indices, pixel-wise labeled data is required, similar to the uncertainty\ndescriptors in previous study (Zhang and Zhang, 2019). However, our pro-\nposed indices are utilized after the labeling stage, therefore, those limitations\nare not relevant to this study.\nThe two class ambiguity indices are designed to quantify the class am-\nbiguity between flood and non-flood class stemming from the spatial and\nspectral resolution constraints of the sensor in input satellite images. The\nproposed two class ambiguity indices are (1) Boundary Pixel Ratio (BPR)\nand (2) Mahalanobis Distance for Flood-segmentation (MDF). BPR is de-\nsigned to represent ambiguity between flooded and non-flooded classes due to\nspatial resolution constraints and is calculated as the proportion of boundary\n16\npixels within satellite images:\nBPR = BPS\nTPS\n(8)\nwhere BPS is the total number of boundary pixels, and TPS is the total\nnumber of pixels in a satellite image. Boundary pixel is defined as the class\nof the center pixel that differs from at least one of its surrounding eight\npixels.\nTherefore, increasing BPR implies more pixels with higher inter-\nclass ambiguity due to the spatial resolution limitations inherent in satellite\nimagery.\nOn the other hand, MDF is formulated to capture the semantic ambiguity\nbetween flooded and non-flooded classes at the tile level. This is calculated\nas the Mahalanobis distance of average pixel values between flood and non-\nflood class. Mahalanobis distance (Mahalanobis, 1936) is a measure of the\ndistance between points over a given distribution. Therefore, we assumed\nthat the pixel values of each class follow a multivariate normal distribu-\ntion. Additionally, decreasing MDF suggests increased semantic ambiguity\nbetween flooded and non-flooded areas at the tile level, attributed to the\nuncertainty in terms of spectral similarity.\nMDF =\np\n(pflood −pnon-flood)TΣ−1(pflood −pnon-flood)\n(9)\nwhere pflood and pnon-flood is a vector of average pixel values in each class,\nwhich are flood and non-flood, and Σ is positive-definite covariance matrix\nwith rows and columns matching the number of channels in the input satellite\nimages. In addition to the two class ambiguity indices, we employ the Flood\nPixel Ratio (FPR) as a class imbalance index to interpret the capability of\nmitigating class imbalance issues in the IDAL-FIM framework:\nFPR = FPS\nTPS\n(10)\nwhere FPS is the number of flood pixels.\n4. Experimental Setup\n4.1. Dataset and Data Preprocessing\nSen1Floods11, a georeferenced flood inundation mapping dataset (Bonafilia\net al., 2020), was utilized in this experiment. This dataset includes 446 pairs\n17\nFigure 3: Example images in Sen1Floods11. (left) Sentinel-2 false color composite image,\nand (right) corresponding labeled data with color codes: blue for flood, green for non-\nflood, and gray for no data.\nof image data, consisting of satellite imagery from Sentinel-1 and Sentinel-\n2, along with corresponding labeled data generated by experts for flood\ninundation mapping.\nSatellite imagery from Sentinel-1 and Sentinel-2 in\nSen1Floods11 captures 11 flood events occurring across various countries\nworldwide between 2016 and 2019. Each of the satellite images has a 10\nmeter resolution and dimensions of 512 × 512 pixels. The satellite imagery\nobserved by Sentinel-1 consists of SAR (Synthetic Aperture Radar) imagery,\nwhich is composed of VH and VV bands. The imagery captured by Sentinel-2\ncomprises multi-spectral imagery with 13 bands, including red, green, blue,\nNear InfraRed (NIR), and Short-Wave Infrared (SWIR). All bands of the\nmulti-spectral images are linearly interpolated to 10 meters to ensure uni-\nform spatial resolution.\nSen1Floods11 has been utilized in several studies focusing on deep learn-\ning models for flood inundation mapping (Konapala et al., 2021; Bai et al.,\n2021; Katiyar et al., 2021; Yadav et al., 2022). In particular, we drew upon\nthe findings of Konapala et al. (2021) for input bands selection and data\npreprocessing. The authors investigated the optimal input band combina-\ntion in U-Net for flood inundation mapping based on Sen1Floods11. The\nauthors showed that utilizing multi-spectral satellite imagery as input led\nto a higher F1-score compared to SAR imagery. This result suggests that\nmulti-spectral imagery provides more advantages over SAR imagery in au-\ntomating the process and diminishing the necessity for expert corrections in\nflood inundation mapping, especially in cases with minimal cloud cover. As\na result, we employ multi-spectral satellite imagery in Sen1Floods11 as input\n18\nfor this study.\nRegarding data preprocessing, Konapala et al. (2021) reported that HSV\n(Hue, Saturation, Value) conversion using the red, NIR, and SWIR2 bands is\neffective for flood inundation mapping through their experiment. Hence, HSV\ntransformed values based on red, NIR, and SWIR2 were employed as a data\npreprocessing for our experiments. In addition, the 512 × 512 multi-spectral\nsatellite images in Sen1Floods11 were divided into four non-overlapping 256\n× 256 pixel tiles for the efficient GPU memory utilization.\n4.2. Data Splitting\nAmong the 11 regions in Sen1Floods11, multi-spectral satellite images\nfrom 8 regions (Ghana, India, Pakistan, Paraguay, Somalia, Spain, Sri-Lanka,\nand USA) were used for the pool of unlabeled data, and the remaining 3 re-\ngions (Bolivia, Nigeria, and Vietnam) were designated for the target region.\nThis split was taken into consideration of their geographic locations, as de-\npicted in Fig. 4. The experiment is designed to carry out flood inundation\nmapping within the IDAL-FIM framework, targeting the regions of Bolivia,\nNigeria, and Vietnam, and using the same unlabeled data pool. In each tar-\nget region, the multi-spectral satellite images and corresponding labeled data\nare randomly split, with 50% allocated for validation and 50% for testing.\nInitial labeled data were randomly selected using a fixed seed number in a\nsingle experiment.\nTable 1: The number of multi-spectral satellite images in the unlabeled data pool\nRegion\nTotal\nGhana\nIndia\nPakistan\nParaguay\nSomalia\nSpain\nSri-Lanka\nUSA\nCount\n1,532\n212\n272\n112\n268\n104\n120\n168\n276\nTable 2: The number of multi-spectral satellite images in the target regions\nRegion\nTotal\nBolivia\nNigeria\nVietnam\nCount\n252\n60\n72\n120\n4.3. The Configurations for Iterative Training and Evaluation\nThis section explains the detailed configurations for iterative training and\nevaluation within the IDAL-FIM framework. Table 3 shows the components\n19\nFigure 4: The geographical regions that make up the unlabeled data pool and the target\nregions. The target regions were selected, one from each of the continents of South Amer-\nica, Africa, and Asia. The remaining 8 regions were utilized for the unlabeled data pool.\nand parameters of the IDAL-FIM framework for experiments. During ex-\nperiments, labeled data in Sen1Floods11 are considered to be created by\nhuman experts in the data labeling stage of the IDAL-FIM framework. For\nreproducibility, the settings related to randomness (e.g. weight initialization,\ndataset shuffling, nondeterministic algorithms, etc.) were configured to guar-\nantee consistent outputs using the same random seed. The number of initial\ntraining data, newly acquired samples per iteration, and number of iterations\nwere determined considering the size of the unlabeled data pool used in the\nexperiment.\nFurthermore, during each iteration, the U-Net with MC-dropout is trained\nfrom scratch using the hyperparameters specified in Table 4. Random flip\nwas applied for data augmentation. Early stopping is employed when the\nvalidation loss does not improve for the specified number of epochs.\nFor\ncarrying out the experiment, PyTorch 1.8.1 and the following hardware was\nused for all processing: Intel Xeon E5 1.9GHz (72 Cores), 64GB 2666MHz\nDDR4, 1.5 TB HD, Nvidia GeForce GTX 980 Ti 24GB.\nTo evaluate the performance in each iteration of the IDAL-FIM frame-\nwork, the F1-score is reported using a prediction probability threshold of 0.5\n20\nTable 3: Components and parameters of IDAL-FIM framework\nComponents / Parameters\nValue\nComponent\nAcquisition Functions\n(Section 3.2)\nBaseline (Random), Uncertainty-based\n(Entropy, Margin, BALD), Density-based (K-Means)\nDeep learning model &\nuncertainty estimation method\n(Section 3.3)\nU-Net / MC-dropout\nClass ambiguity and imbalance\nIndices (Section 3.4)\nBPR, MDF, FPR\nDataset\n(Section 4.1)\nSen1Floods11 (Multi-spectral satellite imagery and its\ncorresponding labeled data)\nParameter\nInitial training data\n100 (random selection)\nInitial unlabeled data pool\n1,532\nValidation and testing data\n50% of the data in the target region is allocated\nfor validation, and the other 50% is allocated for testing\nNewly acquired samples\nper iteration\n100\nNumber of iterations\n4\nNumber of total runs\n10\n(Wieland et al., 2023). Cross-validation is not taken into consideration due\nto the computational overhead involved in performing each iteration multiple\ntimes. The F1-score is calculated using True Positive (TP), False Positive\n(FP), False Negative (FN) in a confusion matrix. When calculating the con-\nfusion matrix for the flood class, the no-data class in the input multi-spectral\nsatellite image was considered as a non-flood class.\nPrecision =\nTP\nTP + FP\n(11)\nRecall =\nTP\nTP + FN\n(12)\nF1-score = 2 × Precision × Recall\nPrecision + Recall\n(13)\nIn addition, as the same experiment is repeated multiple times for each\nacquisition function, the mean F1-score (mF1-score) and the standard devi-\nation of F1-score (sdF1-score) is calculated as follows:\n21\nTable 4: Hyperparameters for training U-Net with MC-dropout\nHyperparameter\nValue\nLoss function\nBinary Cross Entropy\nOptimizer\nAdamW\nLearning rate\n5e-4\nWeight decay\n1e-2\nMaximum epoch\n300\nBatch size\n8\nThe number of inferences through MC-dropout\n10\nSpatial dropout rate\n0.5\nEarly stopping\n(monitoring variable / delta / patience)\nValidation loss / 5e-4 / 5\nmF1-score = 1\nN\nN\nX\ni=1\nF1-scorei\n(14)\nsdF1-score =\nv\nu\nu\nt 1\nN\nN\nX\ni=1\n(F1-scorei −mF1-score)2\n(15)\n5. Result\n5.1. Evaluation on Model Performance with Varying Acquisition Functions\nand Training Data Sizes\nFirst, we conducted experiments to evaluate the impact of active learning\nstrategies on model performance. The F1-score of the model was measured in\nthe evaluation stage during the iterative process of the IDAL-FIM framework.\nFor the convenience of notation regarding the model, ModelAF-N denotes the\nmodel trained on N data points selected by the acquisition function AF, and\nModelFull refers to the model trained on the entire 1,532 data points in the\nunlabeled data pool. We compared five acquisition functions for their effec-\ntiveness in terms of the mean F1-score across ten experiments. As depicted in\nFig. 5, we utilized one baseline and one upper bound mean F1-scores: mF1-\nscoreRandom-500, which is the mean F1-score of ModelRandom-500, displayed as\na horizontal black dashed line as a baseline, and mF1-scoreFull, which is the\nmean F1-score of ModelFull, represented as a horizontal blue dashed line as\na upper bound performance.\n22\nFigure 5: The comparison of the mean F1-score in different five acquisition functions:\n(left) Bolivia, (middle) Nigeria, (right) Vietnam. The horizontal blue dashed line (mF1-\nscoreFull) represents the mean F1-score from models trained on the entire 1,532 data points\nin the pool. The horizontal black dashed line (mF1-scoreRandom-500) displays the mean F1-\nscore from models trained on a selection of 500 data points using the random acquisition\nfunction.\nFig. 5 shows that the mean F1-score of the models, which are trained on\nthe data points acquired based on the predictive uncertainty such as margin\nand entropy, consistently achieved the most comparable mean F1-scores to\nthe mF1-scoreFull. This result shows that a model trained on a subset of\nthe entire dataset selected by the margin and entropy acquisition function\ncan achieve equivalent performance to a model trained on the entire dataset.\nAdditionally, the mean F1-score of models trained on the data points se-\nlected by the margin acquisition function outperformed that of the random\nacquisition function across the three regions. In Bolivia, both the margin\nand BALD acquisition function achieved a superior mean F1-score despite\nhaving 300 fewer training data points. Similarly, in Nigeria, the margin and\nentropy acquisition function outperformed the random acquisition function\ndespite having 300 fewer training data points. In Vietnam, the margin acqui-\nsition function surpassed despite having 200 fewer training data points. In\neach experiment, we used the same random seed to ensure the performance\nevaluation in an identical environment. Therefore, the mean F1-score of all\nfive acquisition functions has the same value when the number of training\ndata is 100 in each of the three regions.\nIn this experiment, we expected that increasing the amount of training\n23\nFigure 6: The comparison on the standard deviation of F1-score in different five acquisition\nfunctions: (left) Bolivia, (middle) Nigeria, (right) Vietnam.\ndata would lead to a gradual improvement in mean F1-scores.\nHowever,\nwe observed a degradation in mean F1-scores at specific points in Fig. 5\nwhen utilizing the random acquisition function. Degradation was particu-\nlarly observed with training data points of 200 in Bolivia and Nigeria, and\nwith training data points of 300 in Vietnam. To investigate the cause of\nthe decrease in the mean F1-score at a specific iteration, we examined the\nstandard deviation of the F1-score for each iteration. As shown in Fig. 6, we\nobserved a tendency for the mean F1-score to not consistently increase when\nthe standard deviation of the F1-score becomes relatively higher compared\nto other acquisition functions as each iteration progresses. Particularly, in\nthe case of the random acquisition function, the standard deviation of the\nF1-score was larger than the other four acquisition functions when the num-\nber of training data ranged from 200 to 400. On the contrary, in the case of\nthe margin acquisition function, which was the best-performing acquisition\nfunction within the IDAL-FIM framework, it tended to show a more rapid\ndecrease in the standard deviation of the F1-score as the number of training\ndata points increases, compared to other acquisition functions.\nFollowing the overall performance assessment of acquisition functions\nwithin the IDAL-FIM framework, in Fig. 7, we examined visual examples\nof prediction results from each target region. In this visualization, results\nfrom three models are compared: (1) ModelRandom-500, (2) ModelMargin-500,\nand (3) ModelFull. ModelMargin-500, is the model train on the data points se-\nlected by best-performed acquisition function in the IDAL-FIM framework,\n24\nFigure 7: The comparison on prediction results of examples in each region: (a) Bolivia,\n(b) Nigeria, (c) Vietnam. “S2-FCC” is the false color composite of input sentinel-2 image\nusing red, NIR and SWIR. “Label” has flood (white) and non-flood (black) pixel infor-\nmation. “Random” and “Margin” are the prediction results from a ModelRandom-500 and\nModelMargin-500, respectively. “Entire dataset” represents the prediction results from a\nModelFull.\nand ModelRandom-500 is a baseline model. ModelFull is a model representing\nupper bound performance. In Fig. 7, prediction results of ModelMargin-500,\nand ModelFull are consistently similar across three regions (a), (b), and\n(c) in terms of True Positive (TP), True Negative (TN), False Positive\n(FP) and False Negative (FN). On the other hand, the prediction result\nof ModelRandom-500 exhibited more false positive pixels in Bolivia (Fig. 7 (a))\nand Nigeria (Fig. 7 (b)) compared to ModelMargin-500 and ModelFull.\n5.2. Relationship Between Class Ambiguity Indices and the Score of Uncertainty-\nbased Acquisition Functions\nWe investigated the relationship between class ambiguity indices and the\nUncertainty-based Acquisition Function (UAF) score, which is directly asso-\n25\nFigure 8: The comparison on the Spearman’s rank correlation coefficient between class\nambiguity indices and the score of the uncertainty-based acquisition functions: (left) Bo-\nlivia, (middle) Nigeria, (right) Vietnam.\nciated with the average uncertainty of all pixels, by calculating the correlation\ncoefficient at each iteration within the IDAL-FIM framework. In each iter-\nation, at the stage of acquiring new labeled data, we calculate the two class\nambiguity indices, which are the BPR and the MDF, as well as scores of\nthe UAF, for all the data points in the unlabeled pool. Then, we obtain\nthe Spearman’s rank correlation coefficient between the BPR and the UAF\nscore, and between the MDF and the UAF score. The reason we chose the\nSpearman’s rank correlation coefficient is because the UAF scores determine\nthe ranking of selected data points.\nIn Fig. 8, we observed an evident positive rank correlation between the\nBPR, which reflects the ambiguity between classes at the tile level due to\nspatial resolution limitations in satellite imagery, and the score of the mar-\ngin and entropy acquisition function, both of which are acquisition functions\nbased on predictive uncertainty. In addition, across the three regions, the\nmedian correlation coefficients between the BPR and the margin acquisition\n26\nfunction score were consistently the highest. In addition, the MDF, indicat-\ning tile-level ambiguity between classes due to spectral resolution limitations,\nexhibits a statistically significant negative correlation with the UAF score.\nSpecifically, the scores from the margin and entropy acquisition function show\na notable negative rank correlation with MDF. On the other hand, BALD,\nwhich represents model uncertainty, shows weaker rank correlation compared\nto the margin and entropy acquisition function, which utilize predictive un-\ncertainty, in terms of the two class ambiguity indices.\nWhen interpreting our findings through the uncertainty propagation the-\nory, the experiment results suggest that class ambiguity arising from spatial\nand spectral resolution limitations of sensor in satellite imagery, as quanti-\nfied by the BPR and MDF, significantly impacts the scores of acquisition\nfunctions based on predictive uncertainty, such as the margin and entropy\nacquisition functions. Consequently, by synthesizing the experiment results\nin Section 5.1 and 5.2, we draw the conclusion that the BPR and MDF are\neffective indicators to represent the informativeness of data points under the\nassumption of uncertainty-based acquisition function.\n5.3. Visualization of Two-dimensional Density Plots Using the Class Ambi-\nguity and Class Imbalance Indices\n5.3.1. The Distribution of Multi-spectral Satellite Images in the Unlabeled\nData Pool\nExamining the distribution of data points within the unlabeled data pool\nis an important task for interpreting and understanding the behavior of the\nacquisition functions within the IDAL-FIM framework. Specifically, compar-\ning the distribution of the unlabeled data pool with that of newly acquired\ndata points selected by the acquisition functions helps clarify the behavior\nof the acquisition function. In this section, based on the established class\nambiguity indices in Section 5.2, we employed two-dimensional (2D) density\nplots with the MDF on the x-axis and the BPR on the y-axis, named the\nMDF-BPR density plot, to visualize the distribution of data points in the\nIDAL-FIM framework. Furthermore, in order to interpret the behavior of the\nacquisition functions from the perspective of mitigating the class imbalance\nproblem presented in the previous study (Rˇuˇziˇcka et al., 2020), we inves-\ntigated the correlation between the FPR and the BPR in terms of spatial\nstructure. Then, we visualized the distribution of data points in the unla-\nbeled data pool by utilizing the FPR-BPR density plot which is the FPR on\nthe x-axis and the BPR on the y-axis.\n27\nFigure 9: 2D-density plots in the unlabeled data pool based on the class ambiguity and\nimbalance indices: (left) MDF-BPR density plot represents the data distribution in terms\nof class ambiguity. This plot shows the diversity of data points from a class ambiguity\nperspective; (right) FPR-BPR density plot displays the relationship between FPR and\nBPR. In each plot, µx denotes the average of x-axis values, and µy means the average of\ny-axis values.\nRegarding the spatial structural relationship between the FPR and the\nBPR, we found a statistically significant correlation, indicating that BPR\nreaches its maximum value around an FPR of 0.5 in the given unlabeled\ndataset, using Pearson’s correlation coefficient. Table 5 displays Pearson’s\ncorrelation coefficient between the FPR and the BPR, categorized by the\nFPR threshold of 0.5 for the unlabeled data points. This result describes\nthat when FPR is less than 0.5, FPR and BPR have a positive correlation,\nand otherwise, FPR and BPR have a negative correlation.\nTable 5: Correlation coefficient between FPR and BPR in the unlabeled data pool\nCriteria\nn\nProportion (%)\nPearson’s correlation coefficient\nFPR <0.5\n1477\n96.4\n0.677\nFPR >= 0.5\n55\n3.6\n-0.584\nIn both the MDF-BPR and the FPR-BPR plots, the contour lines were\nestimated using the kernel density function, and they represent levels that\nrange from 0.05 to 0.95 at intervals of 0.05. These levels correspond to iso-\n28\nproportions of the density.\nFor instance, the contour line drawn for 0.05\nis the outermost line on the two-dimensional density plot.\nThis contour\nline represents the area where 5% of the probability mass lies outside the\ncontour lines. The same color of contour line represents an area at the same\nlevel. Fig. 9 displays the MDF-BPR and the FPR-BPR density plots of data\npoints in the unlabeled data pool. In the MDF-BPR density plot, the average\nvalues of the x-axis (MDF) and y-axis (BPR) are 3.87 and 0.03, respectively,\nand the data points are widely distributed along both axes.\nThis shape\nof distribution density means that the unlabeled data pool contains diverse\ndata points from the perspective of the class ambiguity indices. Lower MDF\nvalues and higher BPR values indicate higher class ambiguity, whereas higher\nMDF values and lower BPR values can be interpreted as data points with\nlower class ambiguity. In the FPR-BPR density plot, the average values of\nthe x-axis (FPR) and y-axis (BPR) are 0.08 and 0.03, respectively. This\nplot illustrates that the maximum value of BPR occurs when the FPR is 0.5,\naligned with the result in Table 5.\n5.3.2. The Distribution of Newly Acquired Multi-spectral Satellite Images\nwithin the IDAL-FIM framework\nIn the same visualization manner as in Section 5.3.1, we depicted the\ndistribution of the newly selected data points through acquisition functions\nin each iteration within the IDAL-FIM framework using MDF-BPR density\nplots and FPR-BPR density plots. Each figure consists of 20 2D-density plots\nrepresenting five different acquisition functions during four iterations. The\nvisualization results in Nigeria and Vietnam are similar to those in Bolivia;\ntherefore, we present the Bolivia results as the representative visualization\nin Figs. 10 and 11. One notable observation in Figs. 10 and 11 is that\nthe distribution of multi-spectral satellite images, selected by the acquisition\nfunction, is dependent on the distribution of the unlabeled data pool. In the\ncase of the random acquisition function, the averages of the x- and y-axes\nremain similar over four iterations, and the shapes of the contour lines in the\ndensity plot resemble those found in the unlabeled pool, as depicted in Fig. 9.\nThis observation apparently indicates that the multi-spectral satellite images\nselected by the random acquisition function are affected by the distribution\nof the unlabeled data pool.\nIn addition, the K-means acquisition function, which is the density-based\nacquisition function utilized in this study, also exhibited similar shape of\ncontour line patterns as the random acquisition function. Considering the\n29\nassumption of the density-based acquisition function that data points maxi-\nmizing the diversity of data features are informative, it can be inferred that\nthis function is sensitive to the distribution of the unlabeled data pool. In\naddition, the experiment results support that K-means acquisition function\nis influenced by the distribution of the unlabeled data pool. Therefore, in\nscenarios where the distribution of the unlabeled data pool is not uniform,\nthe effectiveness of the density-based acquisition function in selecting infor-\nmative data points may be compromised.\nOn the other hand, the margin and entropy acquisition function, which\nare acquisition functions based on the predictive uncertainty, showed distinct\nshapes of contour line patterns. In Fig. 10, the margin and entropy acquisi-\ntion function tend to select data points which have higher BPR values and\nlower MDF values compared to the random and K-means acquisition func-\ntion. In particular, these characteristics were most evident especially in the\nfirst iteration out of the four iterations. This observation indicates that the\nmargin and entropy acquisition function are capable of selecting data points\nwith higher levels of class ambiguity in the input satellite image while min-\nimizing dependence on the distribution of the unlabeled pool. Additionally,\nin Fig. 11, when the margin and entropy acquisition function select data\npoints with higher BPR values, this leads to the selection of data points with\nFPR values around 0.5. This trend is particularly noticeable during the first\niteration.\nHowever, as the number of iterations increases, the margin and entropy\nacquisition functions progressively become influenced by the data distribu-\ntion within the unlabeled data pool. This influence is illustrated by visualiz-\ning both the MDF-BPR and FPR-BPR density plots, which depict a pattern\nwhere the point representing the average values of each axis gradually moves\ntoward the average of the corresponding density plot for the unlabeled data\npool. Moreover, the shape of contour lines corresponding to the 95% prob-\nability mass was the largest in the first of the four iterations and gradually\ndecreased as the iterations progressed in both the MDF-BPR and the FPR-\nBPR density plots.\nLastly, the BALD acquisition function exhibited a different visual pattern\nthan the other four acquisition functions. As depicted in Fig. 10, the BALD\nacquisition function demonstrated superior capability in selecting data points\nwith high BPR compared to the random and K-means acquisition function,\nbut it was not as proficient in identifying data points with low MDF compared\nto the margin and entropy acquisition function. Since BALD is a measure\n30\nFigure 10: The MDF-BPR density plots in Bolivia. In each plot, µx denotes the average\nof x-axis values, and µy means the average of y-axis values.\n31\nFigure 11: The FPR-BPR density plots in Bolivia. In each plot, µx denotes the average\nof x-axis values, and µy means the average of y-axis values.\n32\nof model uncertainty, in situations where predictive uncertainty is high but\nconsistent predictions are made, the score of the BALD acquisition function\nbecomes low. Therefore, this property could cause the BALD acquisition\nfunction to struggle in identifying class ambiguity between flooded and non-\nflooded areas in multi-spectral satellite images at the tile level, especially\nwhen compared to the margin and entropy acquisition function.\n6. Discussion\nThrough experiments in Section 5.1, we have shown that the margin\nacquisition function consistently achieves the best performance and the en-\ntropy acquisition function is the second-best performer within the IDAL-FIM\nframework. Additionally, in Section 5.2, we demonstrated that the two class\nambiguity indices of input satellite images have statistically significant rank\ncorrelation with the score of the margin and entropy acquisition function.\nIn Sections 5.1 and 5.2, both the margin and entropy acquisition function,\nwhich are based on the predictive uncertainty, showed comparable perfor-\nmance and correlation with class ambiguity indices. This can be explained\nby the fact that both are equivalent to querying the instance with a class\nposterior closest to 0.5 in the binary classification setting (Settles, 2009).\nTherefore, when combining our findings with the uncertainty propagation\ntheory, the observed statistically significant correlations strongly support a\ncausal relationship between the class ambiguity of the input satellite image\nand the score of the predictive uncertainty-based acquisition functions, such\nas the margin and entropy acquisition function. Consequently, we conclude\nthat two class ambiguity indices, the BPR and MDF, are effective indicators\nto represent informative data points under the assumption of uncertainty-\nbased acquisition functions.\nWhen comparing the margin and entropy acquisition functions with other\nacquisition functions, Fig. 10 illustrates a noticeable shift in the shape of the\ncontour lines. This shift suggests that as the acquisition function becomes\nmore capable of identifying informative data points, the remaining informa-\ntive data points in the unlabeled pool are depleted more quickly. Conse-\nquently, it is crucial in practice to continually add more data points into the\nunlabeled pool to improve the diversity and representativeness of the train-\ning data. Our proposed class ambiguity indices can effectively monitor the\ncondition of the unlabeled data pool, thereby facilitating an active learning\nstrategy for flood mapping by assisting decision-making in its updates.\n33\nWhen considering the depletion of informative data points, the distribu-\ntion patterns of the data points selected by different acquisition functions\ntend to be more effectively highlighted in early iterations. For this reason,\nwe observed the most distinctive distribution of data points selected by ac-\nquisition functions in the first iteration of Fig. 10. In the first iteration of\nFig. 10, the average BPR of the data points selected by the margin acquisi-\ntion function is 0.1. This value is more than twice the average BPR value of\nthe random acquisition function, which is 0.03. In the case of the MDF, the\naverage value of the data points selected by the margin acquisition function\nis 2.34, on the other hand, the average value of data points selected through\nthe random acquisition function is 3.89.\nThe interpretations through MDF and BPR not only help in understand-\ning how acquisition functions behave in terms of class ambiguity but also\nprovide a summary of preliminary suggestions for data labeling similar to\nthe previous study (Li et al., 2022). Based on these findings, we suggest pri-\noritizing the labeling of flood-observed satellite images where there are more\nboundary pixels between flood and non-flood classes, and where there is a\nsmall difference in average pixel values between those two classes. In addi-\ntion, given that labeled data is necessary to select data points based on class\nambiguity indices, the advantage of deep active learning becomes more evi-\ndent, as informative data points can be chosen within statistical significance\nbased on predictive uncertainty, even without labels.\nBased on the spatial structural relationship between the BPR and the\nFPR, we demonstrate that the margin and entropy acquisition function have\nthe capability to alleviate class imbalance issues. In Section 5.3.1, we showed\nthat the BPR tends to reach a maximum when the FPR is 0.5. This means\nthat as data points with high BPR are selected by the acquisition function,\nthe FPR of selected data points tends to become concentrated around 0.5. In\nFig. 11, continuing with the concepts of the informative data points deple-\ntion, during the first iteration, the margin and entropy acquisition function\nexhibits a distinctive preference for selecting higher BPR data points. This\nselection pattern is associated with the ability to select data points near an\nFPR of 0.5. Therefore, the first iteration of the margin and entropy acquisi-\ntion function obviously illustrates its capability to select in mitigating class\nimbalance issues. This finding aligns with the conclusion in the previous\nstudy (Rˇuˇziˇcka et al., 2020), where the DAL framework was able to auto-\nmatically balance classes in the training data, even when dealing with an\nextreme class imbalance in the pool of unlabeled data. Rˇuˇziˇcka et al. (2020)\n34\nwere able to maintain class balance over the iterations by selecting up to\n950 pairs (1.1%) of training data, which is smaller than the 1,072 pairs of\n“changed” class out of a total of 83,144 pairs. On the other hand, in our\nstudy, experiments were conducted using 500 samples (32.6%) out of a total\nof 1,532 samples. As a result, since the proportion of selected data out of\nthe total dataset is higher compared to the previous study (Rˇuˇziˇcka et al.,\n2020), the depletion of informative data points is displayed more evidently\nin Fig. 11, as iterations progress.\n7. Conclusion\nIn this paper, we introduced a novel framework of Interpretable Deep\nActive Learning for Flood inundation Mapping (IDAL-FIM) by leveraging\nclass ambiguity indices based on the uncertainty propagation theory. In the\nexperiments, we utilized Sen1Floods11 dataset, and adopted U-Net with MC-\ndropout as deep learning model for flood inundation mapping. We employed\nfive acquisition functions, which are random, K-means, BALD, entropy, and\nmargin acquisition function, within the IDAL-FIM framework.\nBased on\nthe experiment results, we demonstrated the significance of two proposed\nclass ambiguity indices within the IDAL-FIM framework. This is achieved\nby establishing their statistically significant correlation with the predictive\nuncertainty of the deep learning model at the tile level. Then, we illustrated\nthat the behaviors of deep active learning are effectively interpreted using\ntwo class ambiguity indices within the IDAL-FIM framework, through visu-\nalizing two-dimensional density plots and providing explanations regarding\nthe operation of deep active learning.\nThe limitations of this study are as follows. In flood mapping, one of the\nnotable challenges is the distinction between flooded and non-flooded vege-\ntated areas, as they generally exhibit comparable spectral patterns. Based\non this study, satellite image tiles that observe both flooded and non-flooded\nvegetated areas are expected to be selected with high priority due to the\nhigh class ambiguity between those two classes. However, due to the absence\nof a distinct class for non-flooded vegetated areas in Sen1Floods11, we were\nnot able to thoroughly explore the behavior of acquisition functions in terms\nof flooded and non-flooded vegetated areas. Therefore, further research is\nneeded on the behavior of acquisition functions for such closely resembling\nclasses within the IDAL-FIM framework. In addition, the labeling cost was\nonly considered from the quantity point of view, and each individual la-\n35\nbeling difficulty was not taken into account.\nFurthermore, as one of the\ncharacteristics of remote sensing data is multi-modality, multi-modal data,\nsuch as multi-spectral images, SAR (Synthetic Aperture Radar) images and\nDEM (Digital Elevation Model), are valuable datasets for flood inundation\nmapping. However, in this study, only multi-spectral images in the binary\nsegmentation were considered for the interpretation of deep active learning.\nAs a research direction for future studies, it is important to focus on an active\nlearning framework that incorporates the multi-modality of remote sensing\ndata and methods for their interpretation.\nReferences\nBai, Y., Wu, W., Yang, Z., Yu, J., Zhao, B., Liu, X., Yang, H., Mas, E.,\nKoshimura, S., 2021. Enhancement of detecting permanent water and tem-\nporary water in flood disasters by fusing sentinel-1 and sentinel-2 imagery\nusing deep learning algorithms: Demonstration of sen1floods11 benchmark\ndatasets. Remote Sensing 13, 2220.\nBastin, L., Fisher, P.F., Wood, J., 2002. Visualizing uncertainty in multi-\nspectral remotely sensed imagery. Computers & Geosciences 28, 337–350.\nBeluch, W.H., Genewein, T., N¨urnberger, A., K¨ohler, J.M., 2018. The power\nof ensembles for active learning in image classification, in: Proceedings of\nthe IEEE Conference on Computer Vision and Pattern Recognition, pp.\n9368–9377.\nBentivoglio, R., Isufi, E., Jonkman, S.N., Taormina, R., 2022. Deep learning\nmethods for flood mapping: a review of existing applications and future\nresearch directions. Hydrology and Earth System Sciences 26, 4345–4378.\nBonafilia, D., Tellman, B., Anderson, T., Issenberg, E., 2020. Sen1floods11:\nA georeferenced dataset to train and test deep learning flood algorithms\nfor sentinel-1, in: Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition Workshops, pp. 210–211.\nBrakenridge, G.R., 2010. Global active archive of large flood events. Dart-\nmouth Flood Observatory, University of Colorado .\nBuscombe, D., Goldstein, E.B., Sherwood, C.R., Bodine, C., Brown, J.A.,\nFavela, J., Fitzpatrick, S., Kranenburg, C.J., Over, J., Ritchie, A.C., et al.,\n36\n2022. Human-in-the-loop segmentation of earth surface imagery. Earth and\nSpace Science 9, e2021EA002085.\nCao, X., Yao, J., Xu, Z., Meng, D., 2020. Hyperspectral image classification\nwith convolutional neural network and active learning. IEEE Transactions\non Geoscience and Remote Sensing 58, 4604–4616.\nChen, L.C., Zhu, Y., Papandreou, G., Schroff, F., Adam, H., 2018. Encoder-\ndecoder with atrous separable convolution for semantic image segmenta-\ntion, in: Proceedings of the European Conference on Computer Vision\n(ECCV), pp. 801–818.\nCohn, D.A., Ghahramani, Z., Jordan, M.I., 1996. Active learning with sta-\ntistical models. Journal of Artificial Intelligence Research 4, 129–145.\nCrawford, M.M., Tuia, D., Yang, H.L., 2013. Active learning: Any value\nfor classification of remotely sensed data? Proceedings of the IEEE 101,\n593–608.\nCrosetto, M., Ruiz, J.A.M., Crippa, B., 2001. Uncertainty propagation in\nmodels driven by remotely sensed data. Remote Sensing of Environment\n76, 373–385.\nDechesne, C., Lassalle, P., Lef`evre, S., 2021. Bayesian u-net: Estimating\nuncertainty in semantic segmentation of earth observation images. Remote\nSensing 13, 3836.\nDi, X., Xue, Z., Zhang, M., 2023. Active learning-driven siamese network for\nhyperspectral image classification. Remote Sensing 15, 752.\nDillon, J.V., Langmore, I., Tran, D., Brevdo, E., Vasudevan, S., Moore, D.,\nPatton, B., Alemi, A., Hoffman, M., Saurous, R.A., 2017.\nTensorflow\ndistributions. arXiv preprint arXiv:1711.10604 .\nDong, Z., Wang, G., Amankwah, S.O.Y., Wei, X., Hu, Y., Feng, A., 2021.\nMonitoring the summer flooding in the poyang lake area of china in 2020\nbased on sentinel-1 data and multiple convolutional neural networks. Inter-\nnational Journal of Applied Earth Observation and Geoinformation 102,\n102400.\n37\nGal, Y., Ghahramani, Z., 2016. Dropout as a bayesian approximation: Rep-\nresenting model uncertainty in deep learning, in: Iternational Conference\non Machine Learning, PMLR. pp. 1050–1059.\nGal, Y., Islam, R., Ghahramani, Z., 2017. Deep bayesian active learning with\nimage data, in: International Conference on Machine Learning, PMLR. pp.\n1183–1192.\nGoodchild, M.F., Li, W., 2021. Replication across space and time must be\nweak in the social and environmental sciences. Proceedings of the National\nAcademy of Sciences 118, e2015759118.\nGuo, C., Pleiss, G., Sun, Y., Weinberger, K.Q., 2017. On calibration of mod-\nern neural networks, in: International Conference on Machine Learning,\nPMLR. pp. 1321–1330.\nHaas, J., Rabus, B., 2021. Uncertainty estimation for deep learning-based\nsegmentation of roads in synthetic aperture radar imagery. Remote Sensing\n13, 1472.\nHaut, J.M., Paoletti, M.E., Plaza, J., Li, J., Plaza, A., 2018. Active learning\nwith convolutional neural networks for hyperspectral image classification\nusing a new bayesian approach. IEEE Transactions on Geoscience and\nRemote Sensing 56, 6440–6461.\nHelleis, M., Wieland, M., Krullikowski, C., Martinis, S., Plank, S., 2022.\nSentinel-1-based water and flood mapping: Benchmarking convolutional\nneural networks against an operational rule-based processing chain. IEEE\nJournal of Selected Topics in Applied Earth Observations and Remote\nSensing 15, 2023–2036.\nHertel, V., Chow, C., Wani, O., Wieland, M., Martinis, S., 2023. Probabilistic\nsar-based water segmentation with adapted bayesian convolutional neural\nnetwork. Remote Sensing of Environment 285, 113388.\nHorritt, M., 2006. A methodology for the validation of uncertain flood inun-\ndation models. Journal of Hydrology 326, 153–165.\nHoulsby, N., Husz´ar, F., Ghahramani, Z., Lengyel, M., 2011.\nBayesian\nactive learning for classification and preference learning. arXiv preprint\narXiv:1112.5745 .\n38\nHsu, C.Y., Li, W., 2023. Explainable geoai: can saliency maps help interpret\nartificial intelligence’s learning process? an empirical study on natural fea-\nture detection. International Journal of Geographical Information Science\n37, 963–987.\nIqbal, U., Perez, P., Li, W., Barthelemy, J., 2021. How computer vision can\nfacilitate flood management: A systematic review. International Journal\nof Disaster Risk Reduction 53, 102030.\nJesson, A., Tigas, P., van Amersfoort, J., Kirsch, A., Shalit, U., Gal, Y.,\n2021.\nCausal-bald: Deep bayesian active learning of outcomes to infer\ntreatment-effects from observational data. Advances in Neural Information\nProcessing Systems 34, 30465–30478.\nJia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R.,\nGuadarrama, S., Darrell, T., 2014. Caffe: Convolutional architecture for\nfast feature embedding, in: Proceedings of the 22nd ACM International\nConference on Multimedia, pp. 675–678.\nKampffmeyer, M., Salberg, A.B., Jenssen, R., 2016. Semantic segmentation\nof small objects and modeling of uncertainty in urban remote sensing im-\nages using deep convolutional neural networks, in: Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition Workshops, pp.\n1–9.\nKatiyar, V., Tamkuan, N., Nagai, M., 2021. Near-real-time flood mapping\nusing off-the-shelf models with sar imagery and deep learning. Remote\nSensing 13, 2334.\nKonapala, G., Kumar, S.V., Ahmad, S.K., 2021. Exploring sentinel-1 and\nsentinel-2 diversity for flood inundation mapping using deep learning. IS-\nPRS Journal of Photogrammetry and Remote Sensing 180, 163–173.\nLaBonte, T., Martinez, C., Roberts, S.A., 2019. We know where we don’t\nknow: 3d bayesian cnns for credible geometric uncertainty. arXiv preprint\narXiv:1910.10793 .\nLakshminarayanan, B., Pritzel, A., Blundell, C., 2017. Simple and scalable\npredictive uncertainty estimation using deep ensembles. Advances in Neu-\nral Information Processing Systems 30.\n39\nLee, S.H., Chen, W., 2009. A comparative study of uncertainty propagation\nmethods for black-box-type problems.\nStructural and multidisciplinary\noptimization 37, 239–253.\nLei, Z., Zeng, Y., Liu, P., Su, X., 2021. Active deep learning for hyperspec-\ntral image classification with uncertainty learning. IEEE Geoscience and\nRemote Sensing Letters 19, 1–5.\nLi, J., Bioucas-Dias, J.M., Plaza, A., 2011. Hyperspectral image segmenta-\ntion using a new bayesian approach with active learning. IEEE Transac-\ntions on Geoscience and Remote Sensing 49, 3947–3960.\nLi, W., 2020.\nGeoai: Where machine learning and big data converge in\ngiscience. Journal of Spatial Information Science , 71–77.\nLi, W., Hsu, C.Y., 2022. Geoai for large-scale image analysis and machine\nvision: Recent progress of artificial intelligence in geography. ISPRS In-\nternational Journal of Geo-Information 11, 385.\nLi, W., Hsu, C.Y., Wang, S., Yang, Y., Lee, H., Liljedahl, A., Witharana,\nC., Yang, Y., Rogers, B.M., Arundel, S.T., et al., 2024. Segment any-\nthing model can not segment anything: Assessing ai foundation model’s\ngeneralizability in permafrost mapping. Remote Sensing 16, 797.\nLi, W., Lee, H., Wang, S., Hsu, C.Y., Arundel, S.T., 2023.\nAssessment\nof a new geoai foundation model for flood inundation mapping, in: Pro-\nceedings of the 6th ACM SIGSPATIAL International Workshop on AI for\nGeographic Knowledge Discovery, pp. 102–109.\nLi, Z., Zhang, S., Dong, J., 2022. Suggestive data annotation for cnn-based\nbuilding footprint mapping based on deep active learning and landscape\nmetrics. Remote Sensing 14, 3147.\nLiu, P., Zhang, H., Eom, K.B., 2016. Active deep learning for classification of\nhyperspectral images. IEEE Journal of Selected Topics in Applied Earth\nObservations and Remote Sensing 10, 712–724.\nLong, J., Shelhamer, E., Darrell, T., 2015.\nFully convolutional networks\nfor semantic segmentation, in: Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, pp. 3431–3440.\n40\nMahalanobis, P., 1936. On the genaralised distance in science. India 2, 49–55.\nMerwade, V., Olivera, F., Arabi, M., Edleman, S., 2008.\nUncertainty in\nflood inundation mapping: Current issues and future directions. Journal\nof hydrologic engineering 13, 608–620.\nMitra, P., Shankar, B.U., Pal, S.K., 2004. Segmentation of multispectral re-\nmote sensing images using active support vector machines. Pattern Recog-\nnition Letters 25, 1067–1074.\nMu˜noz, D.F., Mu˜noz, P., Moftakhari, H., Moradkhani, H., 2021. From local\nto regional compound flood mapping with deep learning and data fusion\ntechniques. Science of the Total Environment 782, 146927.\nNajibi, N., Devineni, N., 2018. Recent trends in the frequency and duration\nof global floods. Earth System Dynamics 9, 757–783.\nNogueira, K., Fadel, S.G., Dourado, ´I.C., Werneck, R.d.O., Mu˜noz, J.A.,\nPenatti, O.A., Calumby, R.T., Li, L.T., dos Santos, J.A., Torres, R.d.S.,\n2018. Exploiting convnet diversity for flooding identification. IEEE Geo-\nscience and Remote Sensing Letters 15, 1446–1450.\nPasolli, E., Melgani, F., Tuia, D., Pacifici, F., Emery, W.J., 2013. Svm active\nlearning approach for image classification using spatial information. IEEE\nTransactions on Geoscience and Remote Sensing 52, 2217–2233.\nPatel, U., Patel, V., 2023.\nA comprehensive review: active learning for\nhyperspectral image classifications. Earth Science Informatics , 1–17.\nPeng, B., Meng, Z., Huang, Q., Wang, C., 2019. Patch similarity convolu-\ntional neural network for urban flood extent mapping using bi-temporal\nsatellite multispectral imagery. Remote Sensing 11, 2492.\nPopien, P., Sunkara, V., Leach, N., Tellman, B., 2021. Cost-effective global\nflood segmentation using convolutional neural networks, sentinel-1 and ac-\ntive learning, in: AGU Fall Meeting Abstracts, pp. H51A–04.\nRajan, S., Ghosh, J., Crawford, M.M., 2008. An active learning approach to\nhyperspectral data classification. IEEE Transactions on Geoscience and\nRemote Sensing 46, 1231–1242.\n41\nRen, P., Xiao, Y., Chang, X., Huang, P.Y., Li, Z., Gupta, B.B., Chen, X.,\nWang, X., 2021.\nA survey of deep active learning.\nACM Computing\nSurveys (CSUR) 54, 1–40.\nRonneberger, O., Fischer, P., Brox, T., 2015. U-net: Convolutional networks\nfor biomedical image segmentation, in: Medical Image Computing and\nComputer-Assisted Intervention–MICCAI 2015: 18th International Con-\nference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18,\nSpringer. pp. 234–241.\nRuiz, P., Mateos, J., Camps-Valls, G., Molina, R., Katsaggelos, A.K., 2013.\nBayesian active remote sensing image classification. IEEE Transactions on\nGeoscience and Remote Sensing 52, 2186–2196.\nRˇuˇziˇcka, V., D’Aronco, S., Wegner, J.D., Schindler, K., 2020.\nDeep ac-\ntive learning in remote sensing for data efficient change detection. arXiv\npreprint arXiv:2008.11201 .\nScheffer, T., Decomain, C., Wrobel, S., 2001. Active hidden markov mod-\nels for information extraction, in: International Symposium on Intelligent\nData Analysis, Springer. pp. 309–318.\nSettles, B., 2009. Active learning literature survey .\nShannon, C.E., 1948. A mathematical theory of communication. The Bell\nSystem Technical Journal 27, 379–423.\nShastry, A., Carter, E., Coltin, B., Sleeter, R., McMichael, S., Eggleston,\nJ., 2023. Mapping floods from remote sensing data and quantifying the\neffects of surface obstruction by clouds and vegetation. Remote Sensing of\nEnvironment 291, 113556.\nStumpf, A., Lachiche, N., Malet, J.P., Kerle, N., Puissant, A., 2013. Active\nlearning in the spatial domain for remote sensing image classification. IEEE\nTransactions on Geoscience and Remote Sensing 52, 2492–2507.\nTabari, H., 2020. Climate change impact on flood and extreme precipitation\nincreases with water availability. Scientific reports 10, 13768.\nTakezoe, R., Liu, X., Mao, S., Chen, M.T., Feng, Z., Zhang, S., Wang, X.,\net al., 2023. Deep active learning for computer vision: Past and future.\nAPSIPA Transactions on Signal and Information Processing 12.\n42\nTellman, B., Sullivan, J.A., Kuhn, C., Kettner, A.J., Doyle, C.S., Braken-\nridge, G.R., Erickson, T.A., Slayback, D.A., 2021. Satellite imaging reveals\nincreased proportion of population exposed to floods. Nature 596, 80–86.\nThoreau, R., Achard, V., Risser, L., Berthelot, B., Briottet, X., 2022. Ac-\ntive learning for hyperspectral image classification: A comparative review.\nIEEE Geoscience and Remote Sensing Magazine 10, 256–278.\nTompson, J., Goroshin, R., Jain, A., LeCun, Y., Bregler, C., 2015. Efficient\nobject localization using convolutional networks, in: Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition, pp. 648–\n656.\nTran, D., Dusenberry, M., Van Der Wilk, M., Hafner, D., 2019. Bayesian\nlayers: A module for neural network uncertainty.\nAdvances in Neural\nInformation Processing Systems 32.\nTuia, D., Pasolli, E., Emery, W.J., 2011a. Using active learning to adapt\nremote sensing image classifiers.\nRemote Sensing of Environment 115,\n2232–2242.\nTuia, D., Ratle, F., Pacifici, F., Kanevski, M.F., Emery, W.J., 2009. Active\nlearning methods for remote sensing image classification. IEEE Transac-\ntions on Geoscience and Remote Sensing 47, 2218–2232.\nTuia, D., Volpi, M., Copa, L., Kanevski, M., Munoz-Mari, J., 2011b.\nA\nsurvey of active learning algorithms for supervised remote sensing image\nclassification.\nIEEE Journal of Selected Topics in Signal Processing 5,\n606–617.\nWallach, D., G´enard, M., 1998. Effect of uncertainty in input and parameter\nvalues on model prediction error. Ecological Modelling 105, 337–345.\nWang, D., Gong, B., Wang, L., 2023. On calibrating semantic segmentation\nmodels: Analyses and an algorithm, in: Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pp. 23652–\n23662.\nWang, J., Sun, K., Cheng, T., Jiang, B., Deng, C., Zhao, Y., Liu, D., Mu,\nY., Tan, M., Wang, X., et al., 2020. Deep high-resolution representation\n43\nlearning for visual recognition. IEEE Transactions on Pattern Analysis\nand Machine Intelligence 43, 3349–3364.\nWang, J., Wang, S., Wang, F., Zhou, Y., Wang, Z., Ji, J., Xiong, Y., Zhao,\nQ., 2022. Fwenet: a deep convolutional neural network for flood water\nbody extraction based on sar images.\nInternational Journal of Digital\nEarth 15, 345–361.\nWieland, M., Martinis, S., 2019. A modular processing chain for automated\nflood monitoring from multi-spectral satellite data. Remote Sensing 11,\n2330.\nWieland, M., Martinis, S., Kiefl, R., Gstaiger, V., 2023. Semantic segmen-\ntation of water bodies in very high-resolution satellite and aerial images.\nRemote Sensing of Environment 287, 113452.\nXie, S., Feng, Z., Chen, Y., Sun, S., Ma, C., Song, M., 2020. Deal: Difficulty-\naware active learning for semantic segmentation, in: Proceedings of the\nAsian Conference on Computer Vision.\nYadav, R., Nascetti, A., Ban, Y., 2022. Attentive dual stream siamese u-\nnet for flood detection on multi-temporal sentinel-1 data, in: IGARSS\n2022-2022 IEEE International Geoscience and Remote Sensing Sympo-\nsium, IEEE. pp. 5222–5225.\nYu, F., Koltun, V., 2015. Multi-scale context aggregation by dilated convo-\nlutions. arXiv preprint arXiv:1511.07122 .\nYu, F., Koltun, V., Funkhouser, T., 2017. Dilated residual networks, in:\nProceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pp. 472–480.\nZhang, L., Xia, J., 2021. Flood detection using multiple chinese satellite\ndatasets during 2020 china summer floods. Remote Sensing 14, 51.\nZhang, Q., Zhang, P., 2019. An uncertainty descriptor for quantitative mea-\nsurement of the uncertainty of remote sensing images. Remote Sensing 11,\n1560.\nZhao, Q., Yu, L., Du, Z., Peng, D., Hao, P., Zhang, Y., Gong, P., 2022. An\noverview of the applications of earth observation satellite data: impacts\nand future trends. Remote Sensing 14, 1863.\n44\nList of Figures\n1\nThe process of the IDAL-FIM framework.\n. . . . . . . . . . .\n10\n2\nThe architecture of U-Net with MC-dropout. The input image\nassumes uniform width and height (I).\n. . . . . . . . . . . . .\n16\n3\nExample images in Sen1Floods11. (left) Sentinel-2 false color\ncomposite image, and (right) corresponding labeled data with\ncolor codes: blue for flood, green for non-flood, and gray for\nno data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\n4\nThe geographical regions that make up the unlabeled data\npool and the target regions. The target regions were selected,\none from each of the continents of South America, Africa, and\nAsia. The remaining 8 regions were utilized for the unlabeled\ndata pool. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\n5\nThe comparison of the mean F1-score in different five acqui-\nsition functions: (left) Bolivia, (middle) Nigeria, (right) Viet-\nnam. The horizontal blue dashed line (mF1-scoreFull) repre-\nsents the mean F1-score from models trained on the entire\n1,532 data points in the pool. The horizontal black dashed line\n(mF1-scoreRandom-500) displays the mean F1-score from mod-\nels trained on a selection of 500 data points using the random\nacquisition function.\n. . . . . . . . . . . . . . . . . . . . . . .\n23\n6\nThe comparison on the standard deviation of F1-score in dif-\nferent five acquisition functions: (left) Bolivia, (middle) Nige-\nria, (right) Vietnam.\n. . . . . . . . . . . . . . . . . . . . . . .\n24\n7\nThe comparison on prediction results of examples in each re-\ngion: (a) Bolivia, (b) Nigeria, (c) Vietnam. “S2-FCC” is the\nfalse color composite of input sentinel-2 image using red, NIR\nand SWIR. “Label” has flood (white) and non-flood (black)\npixel information. “Random” and “Margin” are the predic-\ntion results from a ModelRandom-500 and ModelMargin-500, respec-\ntively. “Entire dataset” represents the prediction results from\na ModelFull.\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n25\n8\nThe comparison on the Spearman’s rank correlation coeffi-\ncient between class ambiguity indices and the score of the\nuncertainty-based acquisition functions: (left) Bolivia, (mid-\ndle) Nigeria, (right) Vietnam.\n. . . . . . . . . . . . . . . . . .\n26\n45\n9\n2D-density plots in the unlabeled data pool based on the class\nambiguity and imbalance indices: (left) MDF-BPR density\nplot represents the data distribution in terms of class ambigu-\nity. This plot shows the diversity of data points from a class\nambiguity perspective; (right) FPR-BPR density plot displays\nthe relationship between FPR and BPR. In each plot, µx de-\nnotes the average of x-axis values, and µy means the average\nof y-axis values. . . . . . . . . . . . . . . . . . . . . . . . . . .\n28\n10\nThe MDF-BPR density plots in Bolivia. In each plot, µx de-\nnotes the average of x-axis values, and µy means the average\nof y-axis values. . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\n11\nThe FPR-BPR density plots in Bolivia. In each plot, µx de-\nnotes the average of x-axis values, and µy means the average\nof y-axis values. . . . . . . . . . . . . . . . . . . . . . . . . . .\n32\n46\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2024-04-29",
  "updated": "2024-04-29"
}