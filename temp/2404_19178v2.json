{
  "id": "http://arxiv.org/abs/2404.19178v2",
  "title": "Revenge of the Fallen? Recurrent Models Match Transformers at Predicting Human Language Comprehension Metrics",
  "authors": [
    "James A. Michaelov",
    "Catherine Arnett",
    "Benjamin K. Bergen"
  ],
  "abstract": "Transformers have generally supplanted recurrent neural networks as the\ndominant architecture for both natural language processing tasks and for\nmodelling the effect of predictability on online human language comprehension.\nHowever, two recently developed recurrent model architectures, RWKV and Mamba,\nappear to perform natural language tasks comparably to or better than\ntransformers of equivalent scale. In this paper, we show that contemporary\nrecurrent models are now also able to match - and in some cases, exceed - the\nperformance of comparably sized transformers at modeling online human language\ncomprehension. This suggests that transformer language models are not uniquely\nsuited to this task, and opens up new directions for debates about the extent\nto which architectural features of language models make them better or worse\nmodels of human language comprehension.",
  "text": "Published as a conference paper at COLM 2024\nRevenge of the Fallen? Recurrent Models Match Transformers\nat Predicting Human Language Comprehension Metrics\nJames A. Michaelova\nCatherine Arnettb\nBenjamin K. Bergena\naDepartment of Cognitive Science, bDepartment of Linguistics,\nUniversity of California San Diego\n{j1michae, ccarnett, bkbergen}@ucsd.edu\nAbstract\nTransformers have generally supplanted recurrent neural networks as the\ndominant architecture for both natural language processing tasks and for\nmodelling the effect of predictability on online human language compre-\nhension. However, two recently developed recurrent model architectures,\nRWKV and Mamba, appear to perform natural language tasks comparably\nto or better than transformers of equivalent scale. In this paper, we show\nthat contemporary recurrent models are now also able to match—and in\nsome cases, exceed—performance of comparably sized transformers at\nmodeling online human language comprehension. This suggests that trans-\nformer language models are not uniquely suited to this task, and opens up\nnew directions for debates about the extent to which architectural features\nof language models make them better or worse models of human language\ncomprehension.\n1\nIntroduction\nThe origins of recurrent neural networks lie in attempts to model human cognition, and\nspecifically the human language system (Jordan, 1986; Elman, 1990). Following improve-\nments such as long short-term memory (LSTM; Hochreiter & Schmidhuber, 1997; Gers\net al., 2000), recurrent neural networks were for a while the dominant architecture not\nonly for modeling human language comprehension (e.g. Frank et al., 2015), but for natu-\nral language systems in general (see, e.g. Goldberg, 2016). In recent years, they have in\nturn been superseded by transformer language models, which empirically tend to show\nbetter performance at both a range of natural language tasks (see, e.g., Radford et al., 2019;\nDai et al., 2019) and at predicting metrics of human language comprehension (e.g. Wilcox\net al., 2020; Merkx & Frank, 2021; Michaelov et al., 2022). Nonetheless, the question of how\nrecurrent and transformer language models compare as cognitive models of the human lan-\nguage system is still an open one. One the one hand, recurrent neural networks inherently\nmodel the process of maintaining a specific informational state and integrating this with\nnew information as it occurs incrementally. This principle is widely believed to underlie\nlanguage comprehension and other real-time processing (Merkx & Frank, 2021; Michaelov\net al., 2021). On the other hand, transformers have been argued to better model cue-based\nretrieval accounts of language comprehension (Ryu & Lewis, 2021; Merkx & Frank, 2021),\nand their direct access to previous words may allow them to better model human-like lexical\npriming effects (Michaelov et al., 2021). In addition, transformers’ superior performance\nat predicting metrics of human language comprehension in itself serves as evidence that,\nat the very least, the statistical patterns learned by transformer language models capture\nsomething also learned by humans.\nAs they have increased in scale (number of parameters, number of training tokens, or\nboth), transformers have been found to improve at natural language tasks (Brown et al.,\n2020; Kaplan et al., 2020; Rae et al., 2022; Hoffmann et al., 2022; Chowdhery et al., 2022;\nTouvron et al., 2023), as well as at predicting both behavioral (Wilcox et al., 2020; Merkx\n& Frank, 2021) and neural (Merkx & Frank, 2021; Michaelov et al., 2022) metrics of online\nlanguage comprehension. But in recent years, two wrinkles have emerged. The first is\n1\narXiv:2404.19178v2  [cs.CL]  26 Aug 2024\nPublished as a conference paper at COLM 2024\nevidence that larger models and those trained on more data may actually predict some\nbehavioral metrics of language comprehension (such as reading time) worse than smaller\nmodels do (Kuribayashi et al., 2021; Oh et al., 2022; Oh & Schuler, 2023a;b; Oh et al., 2024;\nShain et al., 2024). Second, two recently developed recurrent language model architectures\nappear to perform natural language tasks at least as well as transformers of equivalent size\nand training: RWKV (Peng et al., 2023) and Mamba (Gu & Dao, 2023). Transformers are\ntherefore no longer the definitively best-performing language model architecture, and it is\nno longer the case that we should expect further advances in transformers to necessarily\nlead to improved fit to metrics of human language comprehension. Thus the time is ripe to\nrevisit the question of which language model architecture best predicts human language\ncomprehension.\nTo this end, we compare the performance of the Pythia (Biderman et al., 2023), RWKV\n(Peng et al., 2023), and Mamba (Gu & Dao, 2023) suites of autoregressive language models\non 12 human language comprehension datasets (Federmeier et al., 2007; Hubbard et al.,\n2019; Michaelov et al., 2024; Szewczyk & Federmeier, 2022; Szewczyk et al., 2022; Wlotko\n& Federmeier, 2012; Boyce & Levy, 2023; Brothers & Kuperberg, 2021; Futrell et al., 2021;\nKennedy et al., 2003; Luke & Christianson, 2018; Smith & Levy, 2013) covering 5 different\nmetrics. Since all models were trained on the same dataset and have comparable numbers\nof parameters, we are able to measure the effect of architecture on the extent to which a\nlanguage model’s predictions correlate with metrics of human language comprehension.\n2\nModeling prediction in human language comprehension\nOver the years, a wide range of language models have been used to model data from\nexperiments on human language comprehension, including n-gram models (e.g., McDonald\n& Shillcock, 2003; Boston et al., 2008; Demberg & Keller, 2008; Mitchell et al., 2010; Smith &\nLevy, 2013; Brothers & Kuperberg, 2021), recurrent neural networks (RNNs; e.g., Frank\n& Bod, 2011; Fossum & Levy, 2012; Monsalve et al., 2012; Frank et al., 2015; Goodkind &\nBicknell, 2018; Aurnhammer & Frank, 2019), and most recently, transformers (e.g., Wilcox\net al., 2020; Hao et al., 2020; Merkx & Frank, 2021; Kuribayashi et al., 2021; Szewczyk &\nFedermeier, 2022; Michaelov et al., 2022; 2024; Boyce & Levy, 2023; Wilcox et al., 2023a;b; Oh\net al., 2022; 2024; Oh & Schuler, 2023a;b; Shain et al., 2024). Each approach can be evaluated\nin terms of how well it performs either as a computational-level model (in the vein of\nMarr, 1982) or as a cognitive model. Language models can serve as computational-level\nmodels since they calculate the probability of a word in a given context; and thus, their\npredictions can be compared with analogous measures of prediction in humans. Humans\nmay also be able to predict the probability of words based on the statistics of language,\ngiven evidence that they are sensitive to statistical properties of language such as word\nfrequency (Van Petten & Kutas, 1990; Van Petten, 1993; Dambacher et al., 2006; Rugg, 1990;\nFischer-Baum et al., 2014; Shain, 2024). This opens a door for thinking of language models\nas plausible cognitive models, something further supported by recent work arguing that\nlanguage models display linguistic competence (Piantadosi, 2023; Mahowald et al., 2024).\nExisting studies vary in where they fall along the computational-to-cognitive model contin-\nuum. At the computational end of the scale, a range of studies (Smith & Levy, 2013; Brothers\n& Kuperberg, 2021; Meister et al., 2021; Wilcox et al., 2023b; Hoover et al., 2023; Shain et al.,\n2024; Michaelov & Bergen, 2022; 2024) have focused on understanding the mathematical\nrelationship between language model probability and processing difficulty, which does not\nnecessarily require considering the specific model features. On the other end of the scale,\nseveral researchers have argued that contemporary transformers have a strong structural\nresemblance to the human language system (Schrimpf et al., 2021; Hosseini et al., 2024).\nMost studies fall somewhere between the two extremes. This is particularly evidenced in\nwork on the N400, a neural signal that is often considered to index the extent to which a\nword has been predicted based on its preceding context (DeLong et al., 2005; Van Petten &\nLuka, 2012; DeLong et al., 2014; Kuperberg et al., 2020). Frank & Willems (2017), for example,\nexplicitly choose to use a model with a modified n-gram architecture to investigate the role\nthat pure word-level surface-level statistics may have on language comprehension. More\n2\nPublished as a conference paper at COLM 2024\nrecently, Michaelov et al. (2024) used GPT-3 to investigate the extent to which prediction\nbased on language statistics alone could account for several known N400 effects, including\nthe fact that more semantically plausible words are processed more easily. We can also use\nthis approach to test the viability of specific hypotheses about the language comprehension\nsystem by comparing language models that instantiate different theories of language com-\nprehension. Frank et al. (2015), for example, investigate how well a traditional recurrent\nneural network predicts N400 amplitude compared to a model implementing a probabilistic\nphrase-structure grammar. They find that the former out-performs the latter, which they\nargue suggests that prediction during human language comprehension may rely more on\nstatistical properties of language than on explicit hierarchical grammatical structure.\nIn the present study, we focus on the difference between recurrent language models and\ntransformers. Many accounts theorize that human language comprehension involves\nconstruction of lossy and in some cases incorrect representations of the utterance and its\nmeaning, due to cognitive resource limitations (Ferreira, 2003; Christiansen & Chater, 2016;\nFutrell et al., 2020), and it has been argued that recurrent models are a good computational\nmodels of this (Merkx & Frank, 2021; Michaelov et al., 2021). This is perhaps most clear in\nthe case of the now-or-never bottleneck account of language comprehension, under which our\nlimited working memory means that ‘the brain must compress and recode linguistic input\nas rapidly as possible’ (Christiansen & Chater, 2016). In this case, the analogy with recurrent\nmodels is clear—a core component of such models is that they can take inputs of any length,\nbut are limited in that any context must be compressed into a representation with a fixed\nsize, which is updated with each new word.\nThis is not the case for transformers, however. While they have fixed (though ever-increasing,\nsee, e.g., Llama Team, 2024) context windows, they have perfect access to all the represen-\ntations of words within these context windows. Thus, their representations of a word’s\ncontext are not incrementally-compressed versions of the input like recurrent models; but\nrather, their representations of the context expand with each new word—within their context\nwindow, they have ‘unlimited working memory’ (Merkx & Frank, 2021), which puts them at\nodds with accounts such as the now-or-never bottleneck theory of language comprehension.\nOn the other hand, such seemingly lossless working memory may be more human-like than\nit may first appear. As Michaelov et al. (2021) note, humans do maintain specific past words\nin working memory, and indeed, there is evidence that reading a given word can lead to that\nword being easier to process for up to 45 minutes in some specific contexts (Besson et al.,\n1992; for discussion see Rommers & Federmeier, 2018). Furthermore, transformers have\nbeen argued to provide a good computational-level model of cue-based retrieval accounts\nof language comprehension (Ryu & Lewis, 2021; Merkx & Frank, 2021). Specifically, under\ncue-based retrieval accounts (McElree et al., 2003; Van Dyke & Lewis, 2003; for review see\nLewis et al., 2006; Parker et al., 2017), words are retrieved during language comprehension\nbased on the features of previous words in the context which are used as cues; and analo-\ngously, it has been argued, the features of the representations of the words that transformers\nattend to when they predict the next word can be considered to function as cues to which\nword should be predicted (Ryu & Lewis, 2021; Merkx & Frank, 2021).\nBeyond a priori cognitive plausibility, empirical studies with N400 data have almost univer-\nsally shown that transformers out-perform recurrent neural networks, and larger transform-\ners trained on more data (and with lower perplexities) generally perform best at predicting\nN400 amplitude (Merkx & Frank, 2021; Michaelov et al., 2022; Michaelov & Bergen, 2022).\nLanguage models have also been used to model reading time, which has also been hypothe-\nsized to reflect prediction in language comprehension. However, in this area, the results\nhave been less straightforward. Smaller models display the same pattern seen with the N400,\nwhere larger language models trained on more data and with lower perplexities perform\nbetter (Goodkind & Bicknell, 2018; Merkx & Frank, 2021; Wilcox et al., 2020; 2023a; Hao\net al., 2020). But past a certain number of parameters or training tokens, their performance\nappears to deteriorate (Kuribayashi et al., 2021; Oh et al., 2022; 2024; Oh & Schuler, 2023a;b;\nShain et al., 2024). On the question of whether recurrent neural networks or transformers\nbest predict reading time, the results have been mixed (Wilcox et al., 2020; Eisape et al.,\n2020; Kuribayashi et al., 2021), and have been found to differ depending on which metric of\nreading time is investigated (Merkx & Frank, 2021).\n3\nPublished as a conference paper at COLM 2024\nRWKV-4\nPythia\nMamba\nName\nParameters\nName\nParameters\nName\nParameters\n169M\n169,342,464\n160M\n162,322,944\n130M\n129,135,360\n430M\n430,397,440\n410M\n405,334,016\n370M\n371,516,416\n-\n-\n1B\n1,011,781,632\n790M\n793,204,224\n1.5B\n1,515,106,304\n1.4B\n1,414,647,808\n1.4B\n1,372,178,432\n3B\n2,984,627,200\n2.8B\n2,775,208,960\n2.8B\n2,768,345,600\nTable 1: All the models used in our analysis, displaying the model’s named size and the size\nas calculated using PyTorch. Models of comparable size are displayed next to each other.\nFurther details for each model are provided in the cited papers and their linked repositories.\nThe advent of new recurrent architectures that are increasingly feasible to train at a large scale\nand that can perform as well as or better than transformers—namely, RWKV and Mamba—\nis thus important in two ways. First, it allows us to test whether the patterns previously\nobserved in transformers—that larger and better models predict N400 amplitude better\nbut past a certain point predict reading time worse—also holds for other architectures with\ncomparable natural language processing performance. Second, and perhaps more crucially,\nit allows us to again evaluate whether, when matched on scale or performance, recurrent or\ntransformer architectures are better models of online human language comprehension.\n3\nMethod\n3.1\nLanguage Model Architectures\nThe aim of this study is to investigate how well metrics of online human language com-\nprehension can be predicted using three types of language model: the Pythia suite of\nautoregressive transformers (Biderman et al., 2023); and the recurrent RWKV (Peng et al.,\n2023) and Mamba models (Gu & Dao, 2023). All models are trained on the Pile, a 300B token\nEnglish-language dataset (Gao et al., 2020). For each architecture, we selected models of\ncomparable size (i.e., weight class) as shown in Table 1. We discuss each architecture below.\nPythia\nPythia (Biderman et al., 2023) is a set of autoregressive transformer models trained\nto be comparable across different model sizes, ranging from 70M to 12B parameters. The\narchitecture and hyperparameters are based on GPT-3 (Brown et al., 2020), with the addition\nof some changes based on recent advancements (Dao et al., 2022; Su et al., 2024; Wang &\nKomatsuzaki, 2021; Belrose et al., 2023).\nRWKV\nRWKV is a language model architecture described by its creators as a ‘Reinvent[ion\nof the] RNN for the Transformer Era’ (Peng et al., 2023). RWKV models combine the\nparallelizable training of transformers with unlimited context lengths, as well as several\nadditional features that make them RNN-like. First, their time-mixing block—which can\nmathematically formulated in a similar way to the recurrent states of an RNN (Peng et al.,\n2023)—allows the representations of past states to be combined with those of new words.\nIn addition, RWKV models explicitly have a decay parameter such that tokens earlier in\nthe context will be weighted less than later tokens during inference, thereby explicitly\nintroducing something analogous to working memory limitations (Merkx & Frank, 2021).\nMamba\nMamba is another recent recurrent model architecture (Gu & Dao, 2023). One\nof the key goals of the Mamba architecture is to allow models to optimally compress their\ncontexts, and especially very long contexts, into a state of fixed size such that they are still\nable to predict effectively. Like RWKV, Mamba computational complexity scales linearly\nwith sequence length while avoiding the quadratic complexity of transformers (Gu & Dao,\n2023). This is achieved by using a novel ‘selective scan’ mechanism that filters the input to\nselect the most important information. Thus, Mamba models intuitively function like the\n4\nPublished as a conference paper at COLM 2024\nDataset\nMetric\nStimuli\nN\nTrials\nFedermeier et al. (2007)\nN400\n564\n32\n7,856\nHubbard et al. (2019)\nN400\n192\n32\n5,705\nMichaelov et al. (2024)\nN400\n500\n50\n5,526\nSzewczyk & Federmeier (2022)\nN400\n600\n26\n4,822\nSzewczyk et al. (2022)\nN400\n672\n32\n4,939\nWlotko & Federmeier (2012)\nN400\n300\n16\n4,440\nBoyce & Levy (2023)\nMaze Response Time\n9,304\n63\n56,447\nBrothers & Kuperberg (2021)\nSPR Three-Word RT\n648\n216\n46,092\nFutrell et al. (2021)\nSPR Response Time\n9,303\n181\n1,566,641\nKennedy et al. (2003)\nGo-Past Duration\n38,186\n10\n195,507\nLuke & Christianson (2018)\nGo-Past Duration\n2,399\n84\n105,570\nSmith & Levy (2013)\nSPR Response Time\n6,297\n35\n119,120\nTable 2: A description of each of the datasets, including the metric, the number of stimuli,\nthe number of experimental participants (N), and the number of trials. See §3.2 for a brief\nexplanation of each metric, and Appendix A for further details of each dataset.\nmore recent recurrent neural network variants—crucially, they include a latent state that is\nupdated with each new input (like recurrent layers), and their selective scan method filters\ninput (much like gating mechanisms in gated recurrent units or long short-term memory).\n3.2\nDatasets\nIn this study, we use language models of each of the three architectures discussed in §3.1 to\nmodel 5 metrics of human language processing from 12 datasets, the details of which are\ngiven in Table 2. These datasets comprise 6 N400 datasets (Federmeier et al., 2007; Hubbard\net al., 2019; Michaelov et al., 2024; Szewczyk & Federmeier, 2022; Szewczyk et al., 2022;\nWlotko & Federmeier, 2012) and 6 reading time datasets. The latter comprise four types of\nreading time metric: the interval between when a word is first fixated by a reader and when\nthey first move onto the next word, as calculated using eye-tracking (Go-Past Duration or\nGPD; Kennedy et al., 2003; Luke & Christianson, 2018), the time taken to click to move onto\nthe next word in a self-paced reading task (Self-Paced Reading Response Time or SPR RT;\nFutrell et al., 2021; Smith & Levy, 2013), the total Response Time for a word and the two\nfollowing words (to account for spillover effects) in a self-paced reading task (Self-Paced\nReading Three-Word Response Time or 3W-RT; Brothers & Kuperberg, 2021), and the time\ntaken to respond to each word on the Maze task (Maze Response Time or Maze RT; Boyce\n& Levy, 2023). Further details of each metric and dataset are provided in Appendix A.\n3.3\nEvaluation Procedure\nWe used the language models discussed in §3.1 to calculate the surprisal of all critical\nwords in all datasets given their context. For the N400 and the Brothers & Kuperberg (2021)\ndatasets, this context was made up of the preceding words in the same sentence. In the\nremaining datasets (Luke & Christianson, 2018; Boyce & Levy, 2023), we included the whole\npreceding passage, comprising multiple sentences. For critical words made up of multiple\ntokens, surprisal was calculated as the sum of all the sequential tokens comprising them.\nWe ran regression analyses for each dataset using linear mixed-effects regression models,\npredicting each human language comprehension metric using the surprisal calculated\nusing each language model, as well as baseline covariates and random effects structures\nas described in Appendix A. For each regression, we calculate the Akaike Information\nCriterion (AIC; Akaike, 1973), a measure of how well a regression fits the data, with a lower\nAIC indicating a better fit. All language models were run in Python (Van Rossum & Drake,\n2009) using the transformers (Wolf et al., 2020) library with pytorch (Paszke et al., 2019),\npandas (McKinney, 2010), and numpy (Harris et al., 2020); and analyses were carried out in R\n(R Core Team, 2023) using Rstudio (Posit team, 2023) with the tidyverse (Wickham et al.,\n5\nPublished as a conference paper at COLM 2024\n2019), lme4 (Bates et al., 2015), and scales (Wickham et al., 2023) packages. Code and data\nare available at https://github.com/jmichaelov/recurrent-vs-transformer-modeling.\n4\nResults\n4.1\nN400 Datasets\nScale impacts model performance at a range of tasks, so we consider the differences between\nmodels while accounting for scale. Following previous work (e.g., Oh & Schuler, 2023a),\nwe consider differences between models when accounting for model size and for model\nperplexity. While the two are generally correlated, perplexity can help explain the effect\nof model size. Better models might align better with metrics of human language compre-\nhension given our own powerful predictive capabilities (Monsalve et al., 2012; Goodkind &\nBicknell, 2018; Michaelov et al., 2022), but by the same token, language models may learn to\npredict words too well to model human language comprehension (Oh et al., 2024).\nWe first consider the results arranged by model size (Figure 1A). Overall, we find that in\nmost cases, Mamba and RWKV performance is better than that of Pythia, and Mamba is\nalso better than RWKV. On the Federmeier et al. (2007) data, Mamba outperforms Pythia at\nall model sizes. On the Michaelov et al. (2024), Szewczyk & Federmeier (2022), and Wlotko\n& Federmeier (2012) datasets, Mamba is better at all but one scale. Lastly, on the Szewczyk\net al. (2022) and Hubbard et al. (2019) datasets, Mamba is better for all but two model\nsizes (and roughly equal at an additional one for the latter dataset). On the Federmeier\net al. (2007), Hubbard et al. (2019), and Szewczyk & Federmeier (2022) datasets, RWKV\noutperforms Pythia at all but one size. For the other studies, RWKV outperforms Pythia at\nall but 2 sizes. For all studies, the best model fit across all model sizes is a recurrent model;\neither Mamba or RWKV.\nOne additional pattern relates to scaling. In contrast to recent work on reading time (e.g.\nOh & Schuler, 2023b) but in line with previous work on the N400 (Merkx & Frank, 2021;\n(A) Model performance on N400 data by model size\nSzewczyk & Federmeier (2022)\nSzewczyk et al. (2022)\nWlotko & Federmeier (2012)\nFedermeier et al. (2007)\nHubbard et al. (2019)\nMichaelov et al. (2024)\n108\n108.5\n109\n109.5\n108\n108.5\n109\n109.5\n108\n108.5\n109\n109.5\n135510\n135520\n135530\n10300\n10305\n10310\n10315\n10320\n13660\n13670\n12035\n12040\n12045\n12050\n18430\n18450\n18470\n18490\n9820\n9825\n9830\nNumber of Model Parameters\nFit to N400 data (AIC)\nModel\nPythia\nMamba\nRWKV\n(B) Model performance on N400 data by perplexity\nSzewczyk & Federmeier (2022)\nSzewczyk et al. (2022)\nWlotko & Federmeier (2012)\nFedermeier et al. (2007)\nHubbard et al. (2019)\nMichaelov et al. (2024)\n15\n20\n25\n30\n15\n20\n25\n30\n15\n20\n25\n30\n135510\n135520\n135530\n10300\n10305\n10310\n10315\n10320\n13660\n13670\n12035\n12040\n12045\n12050\n18430\n18450\n18470\n18490\n9820\n9825\n9830\nPerplexity\nFit to N400 data (AIC)\nModel\nPythia\nMamba\nRWKV\nFigure 1: Language model performance at predicting N400 amplitude.\n6\nPublished as a conference paper at COLM 2024\nMichaelov et al., 2022; Michaelov & Bergen, 2022), we see that 4 of the 6 datasets (Federmeier\net al., 2007; Hubbard et al., 2019; Szewczyk & Federmeier, 2022; Wlotko & Federmeier, 2012)\nshow positive scaling effects—larger models tend to fit the data better.\nIn order to test how robust these patterns are, we run ordinary least-squares linear models\nfor each dataset, predicting the AIC of the linear mixed-effects regressions based on language\nmodel scale and model architecture (Pythia, Mamba, or RWKV). After correction for multiple\ncomparisons (Benjamini & Yekutieli, 2001), we see that model scale is a significant predictor\nof AIC, with surprisals calculated from larger models fitting the N400 data from 4 of the 6\ndatasets (Federmeier et al., 2007; Hubbard et al., 2019; Szewczyk & Federmeier, 2022; Wlotko\n& Federmeier, 2012) significantly better than smaller models. Given the low power of our\nanalysis (only 14 observations per dataset), it is also worth noting that before correction for\nmultiple comparisons, Mamba models produce surprisals that fit the N400 data significantly\nbetter than Pythia models on the Federmeier et al. (2007) and Wlotko & Federmeier (2012)\ndatasets. While these latter results are suggestive rather than conclusive, they are consistent\nwith the patterns observed in Figure 1A. The full results of our statistical analyses are\nprovided in Table 3.\nNext, we consider the results arranged by model perplexity (Figure 1B). Within each archi-\ntecture, there is no difference in pattern depending on whether we order language models\nby size or perplexity. However, we do see a difference across architectures. In the four\ndatasets that show positive scaling as a function of model size (larger models predict N400\namplitude better), when arranged by perplexity, Mamba models appear to perform worse\nrelative to the other model architectures than they do when arranged by model size, while\nRWKV models appear to perform better. Conversely, on the dataset where two recurrent\nmodels show negative scaling (Szewczyk et al., 2022), we see the opposite pattern—Mamba\nappears to perform better, and RWKV appears to perform worse.\nWhen we run ordinary least-squares linear models predicting AIC based on model perplexity\nand architecture, we see a similar effect to that seen for size. After correction for multiple\ncomparisons, better language models (i.e., those with a lower perplexity) produce surprisals\nthat better fit the N400 data on half of the datasets (Federmeier et al., 2007; Hubbard et al.,\n2019; Wlotko & Federmeier, 2012). The Szewczyk & Federmeier (2022) dataset also shows\nthis pattern before correction. Full results of our statistical analyses are provided in Table 5.\n4.2\nReading Time Datasets\nFor the behavioral reading data, we again first look at the data arranged by model size\n(Figure 2A). The clearest effects are seen on the eye-tracking datasets (Kennedy et al., 2003;\nLuke & Christianson, 2018), where Pythia outperforms (or in one case, performs equally as\nwell as the better of) Mamba and RWKV at all sizes. We also see that Pythia tends to perform\nbest overall on two of the other datasets (Boyce & Levy, 2023; Futrell et al., 2021), with\neither Mamba or RWKV performing better at one or two sizes. The clearest exception to this\npattern is the Brothers & Kuperberg (2021) dataset, where Mamba and RWKV outperform\nPythia at all but one size. We also see a different scaling pattern—unlike the the 5 datasets\n(Boyce & Levy, 2023; Futrell et al., 2018; Kennedy et al., 2003; Luke & Christianson, 2018;\nSmith & Levy, 2013) that generally show a negative scaling pattern(larger models perform\nproduce less well-fitting surprisals), the Brothers & Kuperberg (2021) shows the positive\nscaling effect seen with the N400 data. The Smith & Levy (2013) results are less clear, both\nin terms of differences between models and in terms of overall scaling patterns.\nAn ordinary least-squares linear model predicting AIC based on number of parameters\nand architecture also shows this difference. Even after correction for multiple comparisons,\nmodel size has a significant effect on 4 of the 6 datasets (Boyce & Levy, 2023; Futrell et al.,\n2021; Kennedy et al., 2003; Luke & Christianson, 2018; Smith & Levy, 2013; in addition\nto the Smith & Levy (2013) dataset before correction for multiple comparisons), with the\nsurprisal calculated from larger models showing a worse fit to the data. Intriguingly,\nin line with the aforementioned observations based on Figure 2A, before correction, the\nBrothers & Kuperberg (2021) dataset shows the opposite effect—the same positive scaling\nwe see on some of the N400 datasets. Returning to the differences between architectures,\n7\nPublished as a conference paper at COLM 2024\n(A) Model performance on reading time data by model size\nKennedy et al. (2003): GPD\nLuke & Christianson (2018): GPD\nSmith and Levy (2013): SPR RT\nBoyce & Levy (2023): Maze RT\nBrothers & Kuperberg (2021): 3W−RT\nFutrell et al. (2021): SPR RT\n108\n108.5\n109\n109.5\n108\n108.5\n109\n109.5\n108\n108.5\n109\n109.5\n1849200\n1849400\n1849600\n283080\n283090\n283100\n283110\n283120\n85880\n85890\n85900\n85910\n85920\n85930\n280950\n281000\n281050\n281100\n281150\n281200\n281250\n133400\n133600\n133800\n134000\n524200\n524400\n524600\nNumber of Parameters\nFit to Reading Time (AIC)\nModel\nPythia\nMamba\nRWKV\n(B) Model performance on reading time data by perplexity\nKennedy et al. (2003): GPD\nLuke & Christianson (2018): GPD\nSmith and Levy (2013): SPR RT\nBoyce & Levy (2023): Maze RT\nBrothers & Kuperberg (2021): 3W−RT\nFutrell et al. (2021): SPR RT\n15\n20\n25\n30\n15\n20\n25\n30\n15\n20\n25\n30\n1849200\n1849400\n1849600\n283080\n283090\n283100\n283110\n283120\n85880\n85890\n85900\n85910\n85920\n85930\n280950\n281000\n281050\n281100\n281150\n281200\n281250\n133400\n133600\n133800\n134000\n524200\n524400\n524600\nPerplexity\nFit to Reading Time (AIC)\nModel\nPythia\nMamba\nRWKV\nFigure 2: Language model performance at predicting 4 reading time metrics (see §3.2).\nafter correction, surprisals calculated using Mamba fit the Luke & Christianson (2018) and\nKennedy et al. (2003) data significantly worse than Pythia, with this also being true of RWKV\non both datasets before correction. Further details are provided in Table 4.\nFor the perplexity-ordered data (Figure 2B), the same pattern emerges as for the N400\ndata—for the dataset where positive scaling is found (Brothers & Kuperberg, 2021), Mamba\nmodels appear to perform relatively worse relative to Pythia than they do when the models\nare ordered by size and RWKV models perform relatively better, and for datasets where\nnegative scaling is found, Mamba models appear to perform relatively better and RWKV\nmodels relatively worse. Again, while we see N400-like positive scaling on the Brothers\n& Kuperberg (2021) dataset—with lower-perplexity models showing a better fit to the\ndata—we see the opposite pattern with the remaining 5.\nFurther confirmation of the different scaling patterns comes from ordinary least-squares\nlinear models predicting AIC based on perplexity and architecture. After correction for\nmultiple comparisons, models with a lower perplexity produce surprisals that are signifi-\ncantly better at predicting the Brothers & Kuperberg (2021) data, but significantly worse\nat predicting reading time in 4 datasets (Boyce & Levy, 2023; Futrell et al., 2021; Kennedy\net al., 2003; Luke & Christianson, 2018; and again, the Smith & Levy, 2013 dataset before\ncorrection). In this analysis, surprisal values calculated from the Mamba and RWKV models\nalso show a significantly worse fit to the Kennedy et al. (2003) data, with those from the\nRWKV models also showing this on the Luke & Christianson (2018) dataset before correction\nfor multiple comparisons. The details of all statistical analyses are provided in Table 6.\n5\nDiscussion\nTo the best of our knowledge, the present study is the first to compare the extent to which\ntransformers and contemporary recurrent language models can model online human lan-\nguage comprehension. Previous work has overwhelmingly found that transformers better\npredict the N400 than recurrent neural networks (Merkx & Frank, 2021; Michaelov et al.,\n8\nPublished as a conference paper at COLM 2024\n2021; 2022). We show, by contrast, that on 6 datasets, when comparing models of the same\nsize and trained on the same data, contemporary recurrent language model architectures\ngenerally out-perform transformers, with surprisal values calculated using Mamba models\ntending to provide the best fit to the N400 data. When accounting for model perplexity,\nthe comparison across architectures is less clear-cut; however, the contemporary recurrent\narchitectures at least match transformer performance.\nThe results are more mixed for reading time metrics. On the Kennedy et al. (2003) dataset, for\nexample, the Pythia models predict go-past duration best at any scale or perplexity; while\non the other hand, the recurrent models predict self-paced reading time on the Brothers &\nKuperberg (2021) dataset best except at the 1.4-1.5B scale. Such mixed results for behavioral\ndata is perhaps unsurprising given the conflicting results in previous work (Goodkind &\nBicknell, 2018; Merkx & Frank, 2021; Hao et al., 2020; Wilcox et al., 2020; 2023a; Kuribayashi\net al., 2021; Oh et al., 2022; 2024; Oh & Schuler, 2023a;b; Shain et al., 2024).\nWe also report several interesting scaling results. First, on the whole, scaling patterns are\nconsistent across architectures. For datasets where larger, lower perplexity models tend to\npredict the human metric better (Federmeier et al., 2007; Hubbard et al., 2019; Szewczyk\n& Federmeier, 2022; Wlotko & Federmeier, 2012; Brothers & Kuperberg, 2021), this tends\nto be true for all model architectures. The same is true for datasets where smaller models\nand those with a higher perplexity tend to predict the human metric better (Boyce & Levy,\n2023; Futrell et al., 2021; Kennedy et al., 2003; Luke & Christianson, 2018). The Szewczyk\net al. (2022) and Smith & Levy (2013) datasets offer possible exceptions, where different\nmodels appear to show different scaling effects. However, without more models of each\narchitecture, it is impossible to be certain.\nAnother surprising result is that contrary to the recent work that finds the same negative\nscaling pattern across all reading time datasets (including both self-paced reading and\neye-tracking metrics; Oh & Schuler, 2023b; Oh et al., 2024), here one dataset (Brothers &\nKuperberg, 2021) actually showed positive scaling. One possible explanation for this is that\nit is not log-transformed like the other reading time metrics. Another is that it includes\nthe reading times of the following two words. However, the fact that it is slightly better\npredicted by taking the surprisal of the first word rather than that of the combined three\nwords (see Shain et al., 2024, SI 1) suggests that the metric itself may not differ in this way\nas much as may be expected. A more likely explanation for the difference is that unlike\nthe other behavioral studies which involved the reading of naturalistic stimuli, the stimuli\nin the Brothers & Kuperberg (2021) were carefully constructed to have different degrees\nof predictability. All the N400 studies use such stimuli, and this may therefore explain\nwhy the Brothers & Kuperberg (2021) results more closely resemble the positively-scaling\nN400 results. In any case, the finding highlights the point made by Brothers & Kuperberg\n(2021) that the task and stimuli used in such studies should not be overlooked when making\nwider claims about the relationship between probability and processing difficulty. It further\nsuggests that the recent and ostensibly robust findings of negative scaling with behavioral\ndata (Oh et al., 2022; 2024; Oh & Schuler, 2023a;b) may be limited to a specific type of reading\ntime study, and that further analyses should be carried out.\nFinally, we note the finding that when comparing architectures by model perplexity rather\nthan model size, there was a consistent pattern in terms of which model best predicted\nthe data. Specifically, compared to when ordered by model size, when the dataset showed\npositive scaling, the performance of Mamba appeared worse relative to other architectures,\nand the performance of RWKV appeared better; and when the dataset showed negative\nscaling, the reverse was true. Given that at each size, Mamba has a lower perplexity than\nPythia and RWKV has a higher perplexity (Gu & Dao, 2023; Appendix B), this suggests that\na language model’s ability to predict the next word in a sequence does impact the extent to\nwhich it can model online human language comprehension above and beyond model size\nand architecture. Specifically, this result suggests that there are scaling effects across model\narchitectures related to model quality (i.e., performance at next-word prediction). Even\nwhen controlling for number of parameters and training data, on a dataset that exhibits\npositive scaling, models that are better at next-word prediction are better at the human\nmetric; and the converse is true for datasets that exhibit negative scaling.\n9\nPublished as a conference paper at COLM 2024\n5.1\nTheoretical implications\nUltimately, the results highlight a number of complicating facts. First, there is no single\nuniversal pattern accounting for the relationship between language model probability\nand all metrics of online human language comprehension. Second, general language\nmodeling performance has an effect on the extent to which language models can predict\nsuch metrics. And third, there are idiosyncratic differences between datasets, metrics, and\nmodel architectures.\nNonetheless, the present study opens up new lines of research. Crucially, in contrast to\nprevious work, the results show that transformers are not uniquely well-suited to modeling\nthe N400. They also align with previous research showing the same for some measures of\nreading time (Eisape et al., 2020; Kuribayashi et al., 2021; Merkx & Frank, 2021; Oh et al.,\n2022). Indeed, in our results, the differences in modeling performance between models of\ndifferent architectures at a given scale or perplexity tend to be dwarfed by the differences\nwithin architectures across these dimensions.\nIn the present study, the performance of transformers and recurrent models is comparable,\nand thus our results are not able to evaluate whether there are specific architectural fea-\ntures of transformers or the recurrent models that make them better able to model human\nlanguage comprehension. As discussed in §2,recurrent models provide a better model of\nthe role of the working memory bottleneck (Merkx & Frank, 2021; Michaelov et al., 2021),\nwhile transformers better simulate cue-based retrieval models of comprehension (Ryu &\nLewis, 2021; Merkx & Frank, 2021). It is not necessarily straightforward to disentangle the\ntwo. For example, Ryu & Lewis’s argument that transformers are good models of cue-based\nretrieval is based on the finding that they display interference effects on agreement (also\nknown as agreement attraction effects) and show patterns in attention that align with the\ntheory. But recurrent neural networks have been observed to display such effects (Arehalli\n& Linzen, 2020), which could be plausibly explained by lossy compression of the context.\nNonetheless, identifying cases where the behavior of recurrent models differs from that\nof transformers qualitatively—rather than quantitatively, as in the present study (with the\npossible exception of the Szewczyk et al., 2022 data)—and comparing these to the human\ndata is likely to be valuable across the computational-to-cognitive model continuum. At\nthe more purely computational end, it is important to know which type of model to use to\nbest capture the possible effects of statistics on language comprehension. Further towards\nthe direction of cognitive modeling, such experiments can help to evaluate which theories\nprovide more viable explanations of human language comprehension, for example, by\ncomparing the predictions of the now-or-never bottleneck and cue-based retrieval accounts.\nThe new generation of recurrent models is in its infancy. As these models continue to\nbe developed, optimized, and scaled up, the question of whether they or transformers\nprovide better models of human language comprehension (or at least, show a stronger\ndegree of correlation to specific metrics of online human language comprehension) is likely\nto become clearer. In the meantime, the results presented here suggest that recurrent models\nnot only match, but in some cases exceed, the performance of contemporary transformers\nat modeling human language comprehension, and may provide a valuable way to test\nhypotheses about the neurocognitive mechanisms underlying it.\n6\nConclusions\nWe compare how well transformers and two contemporary recurrent language model\narchitectures—RWKV and Mamba—can predict 5 different metrics of online human lan-\nguage comprehension. We find that overall, the recurrent models tend to match the perfor-\nmance of transformers at predicting both neural and behavioral human metrics, and that\nwhen specifically comparing across architectures by number of model paramaters, recurrent\nmodels in fact appear to be best at predicting N400 amplitude.\n10\nPublished as a conference paper at COLM 2024\nAcknowledgments\nWe would like to thank the San Diego Social Sciences Computing Facility Team for the use\nof the Social Sciences Research and Development Environment (SSRDE) cluster. Models\nwere evaluated using hardware provided by the NVIDIA Corporation as part of an NVIDIA\nAcademic Hardware Grant.\nReferences\nHirotogu Akaike. Information Theory and an Extension of the Maximum Likelihood\nPrinciple. In B. N. Petrov and F Cs´aki (eds.), Second International Symposium on Information\nTheory, Springer Series in Statistics, pp. 267–281, Budapest, Hungary, 1973. Akad´emiai\nKiad´o. doi: 10.1007/978-1-4612-1694-0 15.\nSuhas Arehalli and Tal Linzen. Neural Language Models Capture Some, But Not All Agree-\nment Attraction Effects. In Stephanie Denison, Michael Mack, Yang Xu, and Blair C.\nArmstrong (eds.), Proceedings of the 42th Annual Meeting of the Cognitive Science Soci-\nety. cognitivesciencesociety.org, 2020. URL https://www.cognitivesciencesociety.org/\ncogsci20/papers/0069/.\nChristoph Aurnhammer and Stefan L. Frank.\nEvaluating information-theoretic mea-\nsures of word prediction in naturalistic sentence reading. Neuropsychologia, 134:107198,\n2019.\nISSN 0028-3932.\ndoi: 10.1016/j.neuropsychologia.2019.107198.\nURL http:\n//www.sciencedirect.com/science/article/pii/S0028393219302404.\nDouglas Bates, Martin M¨achler, Ben Bolker, and Steve Walker. Fitting linear mixed-effects\nmodels using lme4. Journal of Statistical Software, 67(1):1–48, 2015. doi: 10.18637/jss.v067.\ni01.\nNora Belrose, Zach Furman, Logan Smith, Danny Halawi, Igor Ostrovsky, Lev McKinney,\nStella Biderman, and Jacob Steinhardt. Eliciting Latent Predictions from Transformers\nwith the Tuned Lens, 2023. URL http://arxiv.org/abs/2303.08112.\nYoav Benjamini and Yosef Hochberg. Controlling the False Discovery Rate: A Practical\nand Powerful Approach to Multiple Testing. Journal of the Royal Statistical Society. Series\nB (Methodological), 57(1):289–300, 1995. ISSN 0035-9246. URL https://www.jstor.org/\nstable/2346101.\nYoav Benjamini and Daniel Yekutieli. The Control of the False Discovery Rate in Multiple\nTesting under Dependency. The Annals of Statistics, 29(4):1165–1188, 2001. ISSN 0090-5364.\nURL https://www.jstor.org/stable/2674075.\nMireille Besson, Marta Kutas, and Cyma Van Petten. An Event-Related Potential (ERP)\nAnalysis of Semantic Congruity and Repetition Effects in Sentences. Journal of Cognitive\nNeuroscience, 4(2):132–149, 1992. ISSN 0898-929X. doi: 10.1162/jocn.1992.4.2.132. URL\nhttps://doi.org/10.1162/jocn.1992.4.2.132.\nStella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle\nO’Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, Usvsn Sai Prashanth,\nEdward Raff, Aviya Skowron, Lintang Sutawika, and Oskar Van Der Wal. Pythia: A Suite\nfor Analyzing Large Language Models Across Training and Scaling. In Proceedings of\nthe 40th International Conference on Machine Learning, pp. 2397–2430. PMLR, 2023. URL\nhttps://proceedings.mlr.press/v202/biderman23a.html.\nMarisa Ferrara Boston, John Hale, Reinhold Kliegl, Umesh Patil, and Shravan Vasishth.\nParsing costs as predictors of reading difficulty: An evaluation using the Potsdam\nSentence Corpus. Journal of Eye Movement Research, 2(1), 2008. ISSN 1995-8692. doi:\n10.16910/jemr.2.1.1. URL https://bop.unibe.ch/index.php/JEMR/article/view/2255.\nVeronica Boyce and Roger Levy. A-maze of Natural Stories: Comprehension and surprisal in\nthe Maze task. Glossa Psycholinguistics, 2(1), 2023. ISSN 2767-0279. doi: 10.5070/G6011190.\nURL https://escholarship.org/uc/item/6vh9d8zm.\n11\nPublished as a conference paper at COLM 2024\nTrevor Brothers and Gina R. Kuperberg. Word predictability effects are linear, not logarith-\nmic: Implications for probabilistic models of sentence comprehension. Journal of Memory\nand Language, 116:104174, 2021. ISSN 0749-596X. doi: 10.1016/j.jml.2020.104174. URL\nhttp://www.sciencedirect.com/science/article/pii/S0749596X20300887.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini\nAgarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya\nRamesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam\nMcCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language Models are\nFew-Shot Learners. In Advances in Neural Information Processing Systems, volume 33, pp.\n1877–1901. Curran Associates, Inc., 2020. URL https://papers.nips.cc/paper/2020/\nhash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,\nAdam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann,\nParker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker\nBarnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari,\nPengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Hen-\nryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov,\nRyan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanu-\nmalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon\nChild, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta,\nMark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas\nEck, Jeff Dean, Slav Petrov, and Noah Fiedel. PaLM: Scaling Language Modeling with\nPathways, 2022. URL http://arxiv.org/abs/2204.02311.\nMorten\nH.\nChristiansen\nand\nNick\nChater.\nThe\nNow-or-Never\nbottleneck:\nA\nfundamental\nconstraint\non\nlanguage.\nBehavioral\nand\nBrain\nSciences,\n39,\n2016.\nISSN 0140-525X, 1469-1825.\ndoi:\n10.1017/S0140525X1500031X.\nURL\nhttps://www.cambridge.org/core/journals/behavioral-and-brain-sciences/\narticle/nowornever-bottleneck-a-fundamental-constraint-on-language/\n938D54E80A2A90A1C5990F4915B5E8D8.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhut-\ndinov. Transformer-XL: Attentive Language Models beyond a Fixed-Length Context.\nIn Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,\npp. 2978–2988, Florence, Italy, 2019. Association for Computational Linguistics. doi:\n10.18653/v1/P19-1285. URL https://aclanthology.org/P19-1285.\nMichael Dambacher, Reinhold Kliegl, Markus Hofmann, and Arthur M. Jacobs. Frequency\nand predictability effects on event-related potentials during reading. Brain Research,\n1084(1):89–103, 2006. ISSN 0006-8993. doi: 10.1016/j.brainres.2006.02.010. URL https:\n//www.sciencedirect.com/science/article/pii/S0006899306003854.\nTri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R´e. FlashAttention: Fast and\nMemory-Efficient Exact Attention with IO-Awareness. Advances in Neural Information\nProcessing Systems, 35:16344–16359, 2022. URL https://proceedings.neurips.cc/paper\nfiles/paper/2022/hash/67d57c32e20fd0a7a302cb81d36e40d5-Abstract-Conference.\nhtml.\nKatherine A. DeLong, Thomas P. Urbach, and Marta Kutas. Probabilistic word pre-activation\nduring language comprehension inferred from electrical brain activity. Nature Neuroscience,\n8(8):1117–1121, 2005. ISSN 1546-1726. doi: 10.1038/nn1504. URL https://www.nature.\ncom/articles/nn1504.\nKatherine A. DeLong, Melissa Troyer, and Marta Kutas. Pre-Processing in Sentence Com-\nprehension: Sensitivity to Likely Upcoming Meaning and Structure. Language and Lin-\n12\nPublished as a conference paper at COLM 2024\nguistics Compass, 8(12):631–645, 2014. ISSN 1749-818X. doi: 10.1111/lnc3.12093. URL\nhttps://onlinelibrary.wiley.com/doi/abs/10.1111/lnc3.12093.\nVera Demberg and Frank Keller. Data from eye-tracking corpora as evidence for theories\nof syntactic processing complexity. Cognition, 109(2):193–210, 2008. ISSN 0010-0277. doi:\n10.1016/j.cognition.2008.07.008. URL http://www.sciencedirect.com/science/article/\npii/S0010027708001741.\nTiwalayo Eisape, Noga Zaslavsky, and Roger Levy. Cloze Distillation: Improving Neu-\nral Language Models with Human Next-Word Prediction. In Proceedings of the 24th\nConference on Computational Natural Language Learning, pp. 609–619, Online, 2020. As-\nsociation for Computational Linguistics.\ndoi: 10.18653/v1/2020.conll-1.49.\nURL\nhttps://aclanthology.org/2020.conll-1.49.\nJeffrey L. Elman. Finding Structure in Time. Cognitive Science, 14(2):179–211, 1990. ISSN\n1551-6709. doi: 10.1207/s15516709cog1402 1. URL https://onlinelibrary.wiley.com/\ndoi/abs/10.1207/s15516709cog1402 1.\nKara D. Federmeier, Edward W. Wlotko, Esmeralda De Ochoa-Dewald, and Marta Kutas.\nMultiple effects of sentential constraint on word processing. Brain Research, 1146:75–84,\n2007. ISSN 00068993. doi: 10.1016/j.brainres.2006.06.101. URL https://linkinghub.\nelsevier.com/retrieve/pii/S0006899306019986.\nFernanda Ferreira. The misinterpretation of noncanonical sentences. Cognitive Psychology,\n47(2):164–203, 2003. ISSN 0010-0285. doi: 10.1016/S0010-0285(03)00005-7. URL https:\n//www.sciencedirect.com/science/article/pii/S0010028503000057.\nSimon Fischer-Baum, Danielle S. Dickson, and Kara D. Federmeier. Frequency and regularity\neffects in reading are task dependent: Evidence from ERPs. Language, Cognition and\nNeuroscience, 29(10):1342–1355, 2014. ISSN 2327-3798. doi: 10.1080/23273798.2014.927067.\nURL https://doi.org/10.1080/23273798.2014.927067.\nVictoria Fossum and Roger Levy. Sequential vs. hierarchical syntactic models of human\nincremental sentence processing. In Proceedings of the 3rd Workshop on Cognitive Modeling\nand Computational Linguistics (CMCL 2012), pp. 61–69, 2012.\nW. Nelson Francis and Henry Kuˇcera. A Standard Corpus of Present-Day Edited Ameri-\ncan English, for use with Digital Computers, 1964. URL http://korpus.uib.no/icame/\nmanuals/BROWN/INDEX.HTM.\nStefan L. Frank and Rens Bod. Insensitivity of the Human Sentence-Processing System to\nHierarchical Structure. Psychological Science, 22(6):829–834, 2011. ISSN 0956-7976. doi:\n10.1177/0956797611409589. URL https://doi.org/10.1177/0956797611409589.\nStefan L. Frank and Roel M. Willems. Word predictability and semantic similarity show\ndistinct patterns of brain activity during language comprehension. Language, Cognition and\nNeuroscience, 32(9):1192–1203, 2017. ISSN 2327-3798. doi: 10.1080/23273798.2017.1323109.\nURL https://doi.org/10.1080/23273798.2017.1323109.\nStefan L. Frank, Leun J. Otten, Giulia Galli, and Gabriella Vigliocco. The ERP response to\nthe amount of information conveyed by words in sentences. Brain and Language, 140:1–11,\n2015. ISSN 0093-934X. doi: 10.1016/j.bandl.2014.10.006. URL http://www.sciencedirect.\ncom/science/article/pii/S0093934X14001515.\nRichard Futrell, Edward Gibson, Harry J. Tily, Idan Blank, Anastasia Vishnevetsky, Steven\nPiantadosi, and Evelina Fedorenko. The Natural Stories Corpus. In Nicoletta Calzolari,\nKhalid Choukri, Christopher Cieri, Thierry Declerck, Sara Goggi, Koiti Hasida, Hitoshi\nIsahara, Bente Maegaard, Joseph Mariani, H´el`ene Mazo, Asuncion Moreno, Jan Odijk,\nStelios Piperidis, and Takenobu Tokunaga (eds.), Proceedings of the Eleventh International\nConference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan, 2018.\nEuropean Language Resources Association (ELRA). URL https://aclanthology.org/\nL18-1012.\n13\nPublished as a conference paper at COLM 2024\nRichard Futrell, Edward Gibson, and Roger P. Levy.\nLossy-Context Surprisal: An\nInformation-Theoretic Model of Memory Effects in Sentence Processing.\nCognitive\nScience, 44(3):e12814, 2020.\nISSN 1551-6709.\ndoi: 10.1111/cogs.12814.\nURL https:\n//onlinelibrary.wiley.com/doi/abs/10.1111/cogs.12814.\nRichard Futrell, Edward Gibson, Harry J. Tily, Idan Blank, Anastasia Vishnevetsky, Steven T.\nPiantadosi, and Evelina Fedorenko. The Natural Stories corpus: A reading-time corpus of\nEnglish texts containing rare syntactic constructions. Language Resources and Evaluation,\n55(1):63–77, 2021. ISSN 1574-0218. doi: 10.1007/s10579-020-09503-7. URL https://doi.\norg/10.1007/s10579-020-09503-7.\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster,\nJason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor\nLeahy. The Pile: An 800GB Dataset of Diverse Text for Language Modeling. 2020. URL\nhttps://arxiv.org/abs/2101.00027v1.\nLeo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster,\nLaurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang,\nLaria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A\nframework for few-shot language model evaluation.\nZenodo, 2021.\nURL https:\n//doi.org/10.5281/zenodo.5371628.\nFelix A. Gers, J¨urgen Schmidhuber, and Fred Cummins. Learning to Forget: Continual\nPrediction with LSTM. Neural Computation, 12(10):2451–2471, 2000. ISSN 0899-7667. doi:\n10.1162/089976600300015015. URL https://doi.org/10.1162/089976600300015015.\nYoav Goldberg. A Primer on Neural Network Models for Natural Language Processing.\nJournal of Artificial Intelligence Research, 57:345–420, 2016. ISSN 1076-9757. doi: 10.1613/\njair.4992. URL https://www.jair.org/index.php/jair/article/view/11030.\nAdam Goodkind and Klinton Bicknell. Predictive power of word surprisal for reading\ntimes is a linear function of language model quality. In Proceedings of the 8th Workshop on\nCognitive Modeling and Computational Linguistics (CMCL 2018), pp. 10–18, Salt Lake City,\nUtah, 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-0102. URL\nhttps://aclanthology.org/W18-0102.\nAlbert Gu and Tri Dao. Mamba: Linear-Time Sequence Modeling with Selective State Spaces,\n2023. URL http://arxiv.org/abs/2312.00752.\nYiding Hao, Simon Mendelsohn, Rachel Sterneck, Randi Martinez, and Robert Frank.\nProbabilistic Predictions of People Perusing: Evaluating Metrics of Language Model\nPerformance for Psycholinguistic Modeling. In Proceedings of the Workshop on Cognitive\nModeling and Computational Linguistics, pp. 75–86, Online, 2020. Association for Computa-\ntional Linguistics. doi: 10.18653/v1/2020.cmcl-1.10. URL https://aclanthology.org/\n2020.cmcl-1.10.\nCharles R. Harris, K. Jarrod Millman, St´efan J. van der Walt, Ralf Gommers, Pauli Vir-\ntanen, David Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J.\nSmith, Robert Kern, Matti Picus, Stephan Hoyer, Marten H. van Kerkwijk, Matthew\nBrett, Allan Haldane, Jaime Fern´andez del R´ıo, Mark Wiebe, Pearu Peterson, Pierre\nG´erard-Marchant, Kevin Sheppard, Tyler Reddy, Warren Weckesser, Hameer Abbasi,\nChristoph Gohlke, and Travis E. Oliphant. Array programming with NumPy. Na-\nture, 585(7825):357–362, 2020. ISSN 1476-4687. doi: 10.1038/s41586-020-2649-2. URL\nhttps://www.nature.com/articles/s41586-020-2649-2.\nSepp Hochreiter and J¨urgen Schmidhuber. Long Short-Term Memory. Neural Computation,\n9(8):1735–1780, 1997. ISSN 0899-7667. doi: 10.1162/neco.1997.9.8.1735. URL https:\n//doi.org/10.1162/neco.1997.9.8.1735.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza\nRutherford, Diego de las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom\nHennigan, Eric Noland, Katherine Millican, George van den Driessche, Bogdan Damoc,\n14\nPublished as a conference paper at COLM 2024\nAurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Oriol Vinyals, Jack William\nRae, and Laurent Sifre. Training Compute-Optimal Large Language Models. In Advances\nin Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=\niBBcRUlOAPR.\nJacob Louis Hoover, Morgan Sonderegger, Steven T. Piantadosi, and Timothy J. O’Donnell.\nThe Plausibility of Sampling as an Algorithmic Theory of Sentence Processing. Open Mind,\n7:350–391, 2023. ISSN 2470-2986. doi: 10.1162/opmi a 00086. URL https://doi.org/10.\n1162/opmi a 00086.\nEghbal A. Hosseini, Martin Schrimpf, Yian Zhang, Samuel Bowman, Noga Zaslavsky, and\nEvelina Fedorenko. Artificial Neural Network Language Models Predict Human Brain\nResponses to Language Even After a Developmentally Realistic Amount of Training.\nNeurobiology of Language, pp. 1–21, 2024. ISSN 2641-4368. doi: 10.1162/nol a 00137. URL\nhttps://doi.org/10.1162/nol a 00137.\nRyan J. Hubbard, Joost Rommers, Cassandra L. Jacobs, and Kara D. Federmeier. Down-\nstream Behavioral and Electrophysiological Consequences of Word Prediction on Recog-\nnition Memory.\nFrontiers in Human Neuroscience, 13, 2019.\nISSN 1662-5161.\nURL\nhttps://www.frontiersin.org/articles/10.3389/fnhum.2019.00291.\nMichael I. Jordan. Serial Order: A Parallel Distributed Processing Approach. Technical\nReport 8604, Institute for Cognitive Science, University of California, San Diego, La Jolla,\nCalifornia, USA, 1986.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon\nChild, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling Laws for Neural\nLanguage Models, 2020. URL http://arxiv.org/abs/2001.08361.\nAlan Kennedy and Jo¨el Pynte. Parafoveal-on-foveal effects in normal reading. Vision\nResearch, 45(2):153–168, 2005. ISSN 0042-6989. doi: 10.1016/j.visres.2004.07.037. URL\nhttps://www.sciencedirect.com/science/article/pii/S0042698904003979.\nAlan Kennedy, Robin Hill, and Jo¨el Pynte. The Dundee Corpus. In The 12th European\nConference on Eye Movements, Dundee, UK, 2003.\nAlan Kennedy, Jo¨el Pynte, Wayne S. Murray, and Shirley-Anne Paul.\nFrequency and\npredictability effects in the Dundee Corpus: An eye movement analysis. Quarterly Journal\nof Experimental Psychology, 66(3):601–618, 2013. ISSN 1747-0218. doi: 10.1080/17470218.\n2012.676054. URL https://doi.org/10.1080/17470218.2012.676054.\nGina R. Kuperberg, Trevor Brothers, and Edward W. Wlotko.\nA Tale of Two Positiv-\nities and the N400: Distinct Neural Signatures Are Evoked by Confirmed and Vi-\nolated Predictions at Different Levels of Representation.\nJournal of Cognitive Neuro-\nscience, 32(1):12–35, 2020. ISSN 0898-929X, 1530-8898. doi: 10.1162/jocn a 01465. URL\nhttps://www.mitpressjournals.org/doi/abs/10.1162/jocn a 01465.\nTatsuki Kuribayashi, Yohei Oseki, Takumi Ito, Ryo Yoshida, Masayuki Asahara, and Kentaro\nInui. Lower Perplexity is Not Always Human-Like. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Linguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 1: Long Papers), pp. 5203–5217, Online, 2021.\nAssociation for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.405. URL\nhttps://aclanthology.org/2021.acl-long.405.\nMarta Kutas and Steven A. Hillyard. Reading senseless sentences: Brain potentials reflect\nsemantic incongruity. Science, 207(4427):203–205, 1980. ISSN 0036-8075, 1095-9203. doi: 10.\n1126/science.7350657. URL https://science.sciencemag.org/content/207/4427/203.\nMarta Kutas and Steven A. Hillyard. Brain potentials during reading reflect word expectancy\nand semantic association. Nature, 307(5947):161–163, 1984. ISSN 0028-0836, 1476-4687.\ndoi: 10.1038/307161a0. URL http://www.nature.com/articles/307161a0.\n15\nPublished as a conference paper at COLM 2024\nRichard L. Lewis, Shravan Vasishth, and Julie A. Van Dyke. Computational principles\nof working memory in sentence comprehension. Trends in Cognitive Sciences, 10(10):\n447–454, 2006. ISSN 1364-6613, 1879-307X. doi: 10.1016/j.tics.2006.08.007. URL https:\n//www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(06)00214-2.\nLlama Team. The Llama 3 Herd of Models. 2024. URL https://ai.meta.com/research/\npublications/the-llama-3-herd-of-models/.\nSteven G. Luke and Kiel Christianson. The Provo Corpus: A large eye-tracking corpus with\npredictability norms. Behavior Research Methods, 50(2):826–833, 2018. ISSN 1554-3528. doi:\n10.3758/s13428-017-0908-4. URL https://doi.org/10.3758/s13428-017-0908-4.\nKyle Mahowald, Anna A. Ivanova, Idan A. Blank, Nancy Kanwisher, Joshua B. Tenen-\nbaum, and Evelina Fedorenko. Dissociating language and thought in large language\nmodels.\nTrends in Cognitive Sciences, 0(0), 2024.\nISSN 1364-6613, 1879-307X.\ndoi:\n10.1016/j.tics.2024.01.011.\nURL https://www.cell.com/trends/cognitive-sciences/\nabstract/S1364-6613(24)00027-5.\nDavid Marr. Vision: A Computational Investigation into the Human Representation and Processing\nof Visual Information. W.H. Freeman, San Francisco, 1982. ISBN 978-0-7167-1284-8.\nScott A. McDonald and Richard C. Shillcock.\nEye Movements Reveal the On-Line\nComputation of Lexical Probabilities During Reading. Psychological Science, 14(6):648–\n652, 2003.\nISSN 0956-7976.\ndoi: 10.1046/j.0956-7976.2003.psci 1480.x.\nURL https:\n//doi.org/10.1046/j.0956-7976.2003.psci 1480.x.\nBrian McElree, Stephani Foraker, and Lisbeth Dyer. Memory structures that subserve sen-\ntence comprehension. Journal of Memory and Language, 48(1):67–91, 2003. ISSN 0749-596X.\ndoi: 10.1016/S0749-596X(02)00515-6. URL https://www.sciencedirect.com/science/\narticle/pii/S0749596X02005156.\nWes McKinney. Data Structures for Statistical Computing in Python. In St´efan van der\nWalt and Jarrod Millman (eds.), Proceedings of the 9th Python in Science Conference, pp. 56–\n61, Austin, Texas, 2010. doi: 10.25080/Majora-92bf1922-00a. URL https://conference.\nscipy.org/proceedings/sci/mckinney.htmlpy2010.\nClara Meister, Tiago Pimentel, Patrick Haller, Lena J¨ager, Ryan Cotterell, and Roger Levy.\nRevisiting the Uniform Information Density Hypothesis. In Proceedings of the 2021 Confer-\nence on Empirical Methods in Natural Language Processing, pp. 963–980, Online and Punta\nCana, Dominican Republic, 2021. Association for Computational Linguistics. doi: 10.\n18653/v1/2021.emnlp-main.74. URL https://aclanthology.org/2021.emnlp-main.74.\nStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer Sentinel\nMixture Models. In International Conference on Learning Representations, 2017. URL https:\n//openreview.net/forum?id=Byj72udxe.\nDanny Merkx and Stefan L. Frank. Human Sentence Processing: Recurrence or Attention?\nIn Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, pp. 12–22,\nOnline, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.cmcl-1.2.\nURL https://aclanthology.org/2021.cmcl-1.2.\nJames A. Michaelov and Benjamin K. Bergen. The more human-like the language model,\nthe more surprisal is the best predictor of N400 amplitude. In NeurIPS 2022 Workshop on\nInformation-Theoretic Principles in Cognitive Systems, 2022. URL https://openreview.net/\nforum?id=uCgYvb8GNQZ.\nJames A. Michaelov and Benjamin K. Bergen. On the Mathematical Relationship Between\nContextual Probability and N400 Amplitude. Open Mind, 8:859–897, 2024. ISSN 2470-2986.\ndoi: 10.1162/opmi a 00150. URL https://doi.org/10.1162/opmi a 00150.\nJames A. Michaelov, Megan D. Bardolph, Seana Coulson, and Benjamin K. Bergen. Different\nkinds of cognitive plausibility: Why are transformers better than RNNs at predicting\nN400 amplitude? In Proceedings of the 43rd Annual Meeting of the Cognitive Science Society,\npp. 300–306, University of Vienna, Vienna, Austria (Hybrid), 2021.\n16\nPublished as a conference paper at COLM 2024\nJames A. Michaelov, Seana Coulson, and Benjamin K. Bergen. So Cloze yet so Far: N400\nAmplitude is Better Predicted by Distributional Information than Human Predictability\nJudgements. IEEE Transactions on Cognitive and Developmental Systems, 2022. ISSN 2379-\n8939. doi: 10.1109/TCDS.2022.3176783.\nJames A. Michaelov, Megan D. Bardolph, Cyma K. Van Petten, Benjamin K. Bergen, and\nSeana Coulson. Strong Prediction: Language Model Surprisal Explains Multiple N400\nEffects. Neurobiology of Language, 5(1):107–135, 2024. ISSN 2641-4368. doi: 10.1162/\nnol a 00105. URL https://doi.org/10.1162/nol a 00105.\nJeff Mitchell, Mirella Lapata, Vera Demberg, and Frank Keller. Syntactic and semantic factors\nin processing difficulty: An integrated measure. In Proceedings of the 48th Annual Meeting\nof the Association for Computational Linguistics, pp. 196–206, 2010.\nIrene Fernandez Monsalve, Stefan L. Frank, and Gabriella Vigliocco. Lexical surprisal\nas a general predictor of reading time. In Proceedings of the 13th Conference of the Euro-\npean Chapter of the Association for Computational Linguistics, pp. 398–408. Association for\nComputational Linguistics, 2012.\nByung-Doh Oh and William Schuler.\nTransformer-Based Language Model Surprisal\nPredicts Human Reading Times Best with About Two Billion Training Tokens.\nIn\nHouda Bouamor, Juan Pino, and Kalika Bali (eds.), Findings of the Association for Com-\nputational Linguistics: EMNLP 2023, pp. 1915–1921, Singapore, 2023a. Association for\nComputational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.128. URL https:\n//aclanthology.org/2023.findings-emnlp.128.\nByung-Doh Oh and William Schuler. Why Does Surprisal From Larger Transformer-Based\nLanguage Models Provide a Poorer Fit to Human Reading Times? Transactions of the\nAssociation for Computational Linguistics, 11:336–350, 2023b. ISSN 2307-387X. doi: 10.1162/\ntacl a 00548. URL https://doi.org/10.1162/tacl a 00548.\nByung-Doh Oh, Christian Clark, and William Schuler. Comparison of Structural Parsers\nand Neural Language Models as Surprisal Estimators. Frontiers in Artificial Intelligence,\n5, 2022. ISSN 2624-8212. doi: 10.3389/frai.2022.777963. URL https://www.frontiersin.\norg/articles/10.3389/frai.2022.777963.\nByung-Doh Oh, Shisen Yue, and William Schuler. Frequency Explains the Inverse Corre-\nlation of Large Language Models’ Size, Training Data Amount, and Surprisal’s Fit to\nReading Times. In Yvette Graham and Matthew Purver (eds.), Proceedings of the 18th\nConference of the European Chapter of the Association for Computational Linguistics (Volume\n1: Long Papers), pp. 2644–2663, St. Julian’s, Malta, 2024. Association for Computational\nLinguistics. URL https://aclanthology.org/2024.eacl-long.162.\nDan Parker, Michael Shvartsman, and Julie A. Van Dyke. The cue-based retrieval theory of\nsentence comprehension: New findings and new challenges. In Linda Escobar, Vicenc¸ Tor-\nrens, and Teresa Parodi (eds.), Language Processing and Disorders, pp. 121–144. Cambridge\nScholars Publishing, Newcastle, 2017.\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,\nTrevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, An-\ndreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank\nChilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An Im-\nperative Style, High-Performance Deep Learning Library. In Advances in Neural Information\nProcessing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.\nneurips.cc/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html.\nBo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman,\nHuanqi Cao, Xin Cheng, Michael Chung, Leon Derczynski, Xingjian Du, Matteo Grella,\nKranthi Gv, Xuzheng He, Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong,\nBart\\lomiej Koptyra, Hayden Lau, Jiaju Lin, Krishna Sri Ipsit Mantri, Ferdinand Mom,\nAtsushi Saito, Guangyu Song, Xiangru Tang, Johan Wind, Stanis\\law Wo´zniak, Zhenyuan\nZhang, Qinghua Zhou, Jian Zhu, and Rui-Jie Zhu. RWKV: Reinventing RNNs for the\n17\nPublished as a conference paper at COLM 2024\nTransformer Era. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Findings of the\nAssociation for Computational Linguistics: EMNLP 2023, pp. 14048–14077, Singapore, 2023.\nAssociation for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.936.\nURL https://aclanthology.org/2023.findings-emnlp.936.\nSteven Piantadosi. Modern language models refute Chomsky’s approach to language, 2023.\nURL https://lingbuzz.net/lingbuzz/007180.\nPosit team. RStudio: Integrated Development Environment for R. Boston, MA, 2023. URL\nhttp://www.posit.co/.\nR Core Team. R: A Language and Environment for Statistical Computing. Vienna, Austria, 2023.\nURL https://www.R-project.org/.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.\nLanguage Models are Unsupervised Multitask Learners. pp. 24, 2019.\nJack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis\nSong, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford,\nTom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche,\nLisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl,\nSumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia\nCreswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya,\nDavid Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena\nMartens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya,\nDomenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria\nTsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby\nPohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d’Autume, Yujia Li, Tayfun\nTerzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia\nGuy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger,\nIason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer,\nOriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray\nKavukcuoglu, and Geoffrey Irving. Scaling Language Models: Methods, Analysis &\nInsights from Training Gopher, 2022. URL http://arxiv.org/abs/2112.11446.\nJoost Rommers and Kara D. Federmeier. Predictability’s aftermath: Downstream conse-\nquences of word predictability as revealed by repetition effects. Cortex, 101:16–30, 2018.\nISSN 0010-9452. doi: 10.1016/j.cortex.2017.12.018. URL http://www.sciencedirect.com/\nscience/article/pii/S0010945217304264.\nMichael D. Rugg. Event-related brain potentials dissociate repetition effects of high-and\nlow-frequency words. Memory & Cognition, 18(4):367–379, 1990. ISSN 1532-5946. doi:\n10.3758/BF03197126. URL https://doi.org/10.3758/BF03197126.\nSoo Hyun Ryu and Richard Lewis. Accounting for Agreement Phenomena in Sentence Com-\nprehension with Transformer Language Models: Effects of Similarity-based Interference\non Surprisal and Attention. In Proceedings of the Workshop on Cognitive Modeling and Compu-\ntational Linguistics, pp. 61–71, Online, 2021. Association for Computational Linguistics. doi:\n10.18653/v1/2021.cmcl-1.6. URL https://www.aclweb.org/anthology/2021.cmcl-1.6.\nMartin Schrimpf, Idan Asher Blank, Greta Tuckute, Carina Kauf, Eghbal A. Hosseini, Nancy\nKanwisher, Joshua B. Tenenbaum, and Evelina Fedorenko. The neural architecture of\nlanguage: Integrative modeling converges on predictive processing. Proceedings of the\nNational Academy of Sciences, 118(45):e2105646118, 2021. doi: 10.1073/pnas.2105646118.\nURL https://www.pnas.org/doi/10.1073/pnas.2105646118.\nCory Shain. Word Frequency and Predictability Dissociate in Naturalistic Reading. Open\nMind, 8:177–201, 2024. ISSN 2470-2986. doi: 10.1162/opmi a 00119. URL https://doi.\norg/10.1162/opmi a 00119.\nCory Shain, Clara Meister, Tiago Pimentel, Ryan Cotterell, and Roger Levy. Large-scale\nevidence for logarithmic effects of word predictability on reading time. Proceedings of the\n18\nPublished as a conference paper at COLM 2024\nNational Academy of Sciences, 121(10):e2307876121, 2024. doi: 10.1073/pnas.2307876121.\nURL https://www.pnas.org/doi/abs/10.1073/pnas.2307876121.\nNathaniel J. Smith and Roger Levy. The effect of word predictability on reading time is\nlogarithmic. Cognition, 128(3):302–319, 2013. ISSN 0010-0277. doi: 10.1016/j.cognition.2013.\n02.013. URL http://www.sciencedirect.com/science/article/pii/S0010027713000413.\nJianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. RoFormer:\nEnhanced transformer with Rotary Position Embedding. Neurocomputing, 568:127063, 2024.\nISSN 0925-2312. doi: 10.1016/j.neucom.2023.127063. URL https://www.sciencedirect.\ncom/science/article/pii/S0925231223011864.\nJakub M. Szewczyk and Kara D. Federmeier. Context-based facilitation of semantic access\nfollows both logarithmic and linear functions of stimulus probability. Journal of Memory\nand Language, 123:104311, 2022. ISSN 0749-596X. doi: 10.1016/j.jml.2021.104311. URL\nhttps://www.sciencedirect.com/science/article/pii/S0749596X21000942.\nJakub M. Szewczyk, Emily N. Mech, and Kara D. Federmeier. The power of “good”: Can\nadjectives rapidly decrease as well as increase the availability of the upcoming noun?\nJournal of Experimental Psychology: Learning, Memory, and Cognition, 48:856–875, 2022. ISSN\n1939-1285. doi: 10.1037/xlm0001091.\nWilson L. Taylor. “Cloze Procedure”: A New Tool for Measuring Readability. Journalism\nQuarterly, 30(4):415–433, 1953. ISSN 0022-5533. doi: 10.1177/107769905303000401. URL\nhttp://journals.sagepub.com/doi/10.1177/107769905303000401.\nWilson L. Taylor. “Cloze” readability scores as indices of individual differences in com-\nprehension and aptitude. Journal of Applied Psychology, 41(1):19–26, 1957. ISSN 1939-\n1854(Electronic),0021-9010(Print). doi: 10.1037/h0040591.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux,\nTimoth´ee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien\nRodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. LLaMA: Open and\nEfficient Foundation Language Models, 2023. URL http://arxiv.org/abs/2302.13971.\nJulie A Van Dyke and Richard L Lewis. Distinguishing effects of structure and decay on\nattachment and repair: A cue-based parsing account of recovery from misanalyzed\nambiguities.\nJournal of Memory and Language, 49(3):285–316, 2003.\nISSN 0749-596X.\ndoi: 10.1016/S0749-596X(03)00081-0. URL https://www.sciencedirect.com/science/\narticle/pii/S0749596X03000810.\nCyma Van Petten. A comparison of lexical and sentence-level context effects in event-related\npotentials. Language and Cognitive Processes, 8(4):485–531, 1993. ISSN 0169-0965. doi:\n10.1080/01690969308407586. URL https://doi.org/10.1080/01690969308407586.\nCyma Van Petten and Marta Kutas. Interactions between sentence context and word\nfrequency in event-related brainpotentials. Memory & Cognition, 18(4):380–393, 1990. ISSN\n1532-5946. doi: 10.3758/BF03197127. URL https://doi.org/10.3758/BF03197127.\nCyma Van Petten and Barbara J. Luka. Prediction during language comprehension: Benefits,\ncosts, and ERP components. International Journal of Psychophysiology, 83(2):176–190, 2012.\nISSN 0167-8760. doi: 10.1016/j.ijpsycho.2011.09.015. URL http://www.sciencedirect.\ncom/science/article/pii/S0167876011002819.\nGuido Van Rossum and Fred L. Drake. Python 3 Reference Manual. CreateSpace, Scotts Valley,\nCA, 2009. ISBN 1-4414-1269-7.\nBen Wang and Aran Komatsuzaki. GPT-J-6B: A 6 billion parameter autoregressive language\nmodel, 2021. URL https://github.com/kingoflolz/mesh-transformer-jax.\n19\nPublished as a conference paper at COLM 2024\nHadley Wickham, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino Mc-\nGowan, Romain Franc¸ois, Garrett Grolemund, Alex Hayes, Lionel Henry, Jim Hester,\nMax Kuhn, Thomas Lin Pedersen, Evan Miller, Stephan Milton Bache, Kirill M¨uller,\nJeroen Ooms, David Robinson, Dana Paige Seidel, Vitalie Spinu, Kohske Takahashi, Davis\nVaughan, Claus Wilke, Kara Woo, and Hiroaki Yutani. Welcome to the tidyverse. Journal\nof Open Source Software, 4(43):1686, 2019. doi: 10.21105/joss.01686.\nHadley Wickham, Thomas Lin Pedersen, and Dana Seidel.\nScales: Scale Functions for\nVisualization, 2023. URL https://CRAN.R-project.org/package=scales.\nEthan Wilcox, Clara Meister, Ryan Cotterell, and Tiago Pimentel. Language Model Qual-\nity Correlates with Psychometric Predictive Power in Multiple Languages. In Houda\nBouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Em-\npirical Methods in Natural Language Processing, pp. 7503–7511, Singapore, 2023a. Asso-\nciation for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.466. URL\nhttps://aclanthology.org/2023.emnlp-main.466.\nEthan G Wilcox, Jon Gauthier, Jennifer Hu, Peng Qian, and Roger P Levy. On the Predictive\nPower of Neural Language Models for Human Real-Time Comprehension Behavior. In\nProceedings of the 42nd Annual Meeting of the Cognitive Science Society (CogSci 2020), pp. 7,\n2020.\nEthan G. Wilcox, Tiago Pimentel, Clara Meister, Ryan Cotterell, and Roger P. Levy. Testing\nthe Predictions of Surprisal Theory in 11 Languages. Transactions of the Association for\nComputational Linguistics, 11:1451–1470, 2023b. ISSN 2307-387X. doi: 10.1162/tacl a 00612.\nURL https://doi.org/10.1162/tacl a 00612.\nEdward W. Wlotko and Kara D. Federmeier.\nFinding the right word: Hemispheric\nasymmetries in the use of sentence context information.\nNeuropsychologia, 45(13):\n3001–3014, 2007. ISSN 00283932. doi: 10.1016/j.neuropsychologia.2007.05.013. URL\nhttps://linkinghub.elsevier.com/retrieve/pii/S0028393207002126.\nEdward W. Wlotko and Kara D. Federmeier. So that’s what you meant! Event-related poten-\ntials reveal multiple aspects of context use during construction of message-level meaning.\nNeuroImage, 62(1):356–366, 2012. ISSN 1053-8119. doi: 10.1016/j.neuroimage.2012.04.054.\nURL http://www.sciencedirect.com/science/article/pii/S1053811912004508.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony\nMoi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer,\nPatrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao,\nSylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. Transformers:\nState-of-the-Art Natural Language Processing. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing: System Demonstrations, pp. 38–45, Online,\n2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-demos.6.\nURL https://www.aclweb.org/anthology/2020.emnlp-demos.6.\n20\nPublished as a conference paper at COLM 2024\nA\nData and Analysis Details\nA.1\nN400 Amplitude\nThe N400 is a negative-going component of the event-related brain potential that occurs\nroughly 300-500ms after the presentation of a stimulus, peaking at around 400ms (Kutas\n& Hillyard, 1980). A well-replicated finding is that the amplitude of the N400 response\nto a word is sensitive to the contextual probability of a word, either operationalized as\ncloze probability (Kutas & Hillyard, 1984)—the proportion of people to fill in a gap in a\nsentence with a given word (Taylor, 1953; 1957)—or when using the predictions of language\nmodels (Frank et al., 2015). Specifically, the amplitude of the N400 response elicited by a\nword is large by default, and decreases by the extent to which it is predictable based on the\npreceding context.\nIn this study, we compare how well the Pythia, RWKV, and Mamba models predict N400\namplitude based on the results of 6 experiments (Federmeier et al., 2007; Hubbard et al.,\n2019; Michaelov et al., 2024; Szewczyk & Federmeier, 2022; Szewczyk et al., 2022; Wlotko &\nFedermeier, 2012). The details of these datasets and how they were analyzed are outlined\nbelow.\nFedermeier et al. (2007)\nmeasured N400s to low- and high-cloze words in low- and\nhigh-constraint contexts. We use the data from this study as preprocessed by Szewczyk &\nFedermeier (2022). In this dataset, N400 amplitude is operationalized as the mean voltage\nat four centro-parietal electrodes (MiCe, MiPa, LMCe, RMCe) over the 300-500ms time\nwindow. N400 amplitudes are also not baseline-corrected; instead, the mean amplitude in\nthe -100-0ms time window is intended to be included as a covariate in analysis. This dataset\ncontains 7856 trials from 32 participants reading 564 stimuli.\nTo calculate model fit to the N400 data, we followed as closely as possible the approach used\nby Szewczyk & Federmeier (2022), which involved predicting N400 amplitude using a linear\nmixed-effects regression with surprisal, baseline amplitude, log-transformed frequency, the\nposition of the word in the sentence, orthographic neighborhood distance, and concreteness\nas fixed effects. We also used the same random effects structure, removing variables until a\nstructure that would not lead to singular fits for any regression was reached, which included\nrandom slopes of baseline for each subject and experimental item, random slopes of word\nposition for each subject, and random intercepts of subject and item. We then compared the\nfit of regressions using surprisal calculated from each language model.\nWith the exception of the Michaelov et al. (2024) dataset, our remaining N400 datasets are\nthe others provided online by Szewczyk & Federmeier (2022), and thus are preprocessed\nand analyzed in the same way.\nWlotko & Federmeier (2012)\nused stimuli from Federmeier et al. (2007) as well as (Wlotko\n& Federmeier, 2007), which were selected to cover a wide range of probabilities. This dataset\nwas made up of 4,440 trials (300 stimuli; 16 experimental participants).\nHubbard et al. (2019)\nused 192 stimuli from Federmeier et al. (2007). The dataset comprises\nof 5,705 trials (32 participants).\nSzewczyk et al. (2022)\nalso based their stimuli on those in Federmeier et al. (2007), with\nadjectives added before critical words, making them more or less predictable. The dataset is\ncomprised of 4,939 trials (672 stimuli; 32 participants).\nSzewczyk & Federmeier (2022)\nalso release an additional dataset with data from a\npreviously-unpublished study using stimuli based on Federmeier et al. (2007) and including\ndata from 4,822 trials (600 stimuli; 26 experimental participants). We refer to this as the\nSzewczyk & Federmeier (2022) dataset.\n21\nPublished as a conference paper at COLM 2024\nMichaelov et al. (2024)\nThe stimuli of the Michaelov et al. (2024) dataset differ from the\nother datasets in their design. Rather than having two versions of each sentence—the\nmost likely continuation and an unlikely one—each sentence has four possible endings:\nthe highest-cloze continuation, a low-cloze but plausible continuation that is semantically\nrelated to this highest-cloze continuation, an equally low-cloze but unrelated continuation,\nand an implausible continuation. The two low-cloze completions were matched for cloze\nprobability and plausibility. There were 125 sentence frames, for a total of 500 sentences.\nThere were fifty participants, and data from a total of 5,526 trials after cleaning.\nThe N400 was operationalized as the mean voltage in the 300-500ms time-window at each\nof the C3, Cz, C4, CP3, CPz, CP4, P3, Pz, and P4 centro-parietal electrodes. Unlike the\ndata released by Szewczyk & Federmeier (2022), the voltage at each electrode was treated\nas a separate data point and N400 amplitudes were baselined using the mean amplitude\nin the 100ms period before stimulus presentation. Thus this dataset comprises of 49,734\ndata points. We analyzed these data in the same way as in Michaelov et al. (2024), fitting a\nregression that predicted N400 amplitude using Surprisal, log-transformed word frequency,\northographic neighborhood distance as main effects, and included random intercepts of\nexperimental subject, sentence context, critical word, and electrode.\nA.2\nSelf-Paced Reading Response Time\nSelf-Paced Reading is an experimental paradigm in which participants read a text one word\nat a time, pressing a button or key to proceed to the next word. The reading time of a\nword is the time taken between button presses (i.e., between pressing the button to proceed\nto that word and pressing the button to proceed to the next word). Self-Paced Reading\nResponse Time is generally considered to reflect processing difficulty, with longer reading\ntimes indexing a more difficult word.\nFutrell et al. (2021)\nThe Natural Stories Corpus (Futrell et al., 2021) is made up of self-\npaced reading times from 181 experimental participants reading 10 texts, each comprising\nroughly 1000 words. These texts were constructed by taking publicly available texts and\nediting them to contain rare and hard-to-process syntactic constructions. Following Oh\net al. (2024), we excluded reading times for all words that appeared at the beginning or\nend of a sentence, all reading times shorter than 100ms or longer than 3000ms, and all data\nfrom participants who answered three or fewer comprehension questions correctly. The\nregression used to predict log-transformed reading time also followed that described by Oh\net al. (2024). In addition to language model surprisal surprisal, we included word length,\nlog-transformed word frequency, and the word’s position in the sentence as predictors, as\nwell as random slopes of each of these predictors for each subject, and random intercepts of\nsubject and sentence.\nSmith & Levy (2013)\nThis dataset is comprised of self-paced reading times from 35 experi-\nmental participants reading 292-902 word passages from the Brown Corpus of American\nEnglish (Francis & Kuˇcera, 1964), for a total of 2860-4999 words per subject. We used the\nsame exclusion criteria as with the Natural Stories data (with the exception of the compre-\nhension score, which was not available), and the linear mixed-effects regressions each had\nthe same structure as those used in analyzing the Natural Stories data.\nA.3\nSelf-Paced Reading Three-Word Response Time\nA well-known phenomenon in self-paced reading is that of spillover effects, where the\nextent to which a word is difficult to process also impacts the reading time of one or more\nfollowing words. One way to account for this is to use the reading time of a given word and\nthe following two words as a measure of the processing difficulty associated with the word.\nBrothers & Kuperberg (2021)\nIn this self-paced reading study, there were 216 sentence\nsets, each in a low-, medium-, or high-cloze condition, for a total of 648 stimulus sentences.\nParticipants were excluded by Brothers & Kuperberg (2021) if they had an average com-\nprehension check score of less than 75%. After exclusions, data from 216 of the total 240\n22\nPublished as a conference paper at COLM 2024\nparticipants were included in the analysis with a total of 46,092 data points. Data were\ncleaned and preprocessed by Brothers & Kuperberg (2021). We fit regressions following\nthe method in the original study, predicting reading time (un-transformed) using a linear\nmixed-effects model with a main effect of language model surprisal, random intercepts for\neach subject and item, and random slopes of surprisal for each subject and item.\nA.4\nMaze Task\nLike self-paced reading, in the Maze task, participants read a text one word at a time.\nHowever, in the Maze task, participants see pairs of words and can only proceed to the\nnext word in the text by choosing the correct next word on the screen. If the participant\nchooses the incorrect word, they receive feedback and are prompted to choose again. The\ntime it takes for participants to choose a word is recorded as the reaction time. We look\nat the reaction times from a previous study by Boyce & Levy (2023). Dataset and analysis\ndetails are provided below.\nBoyce & Levy (2023)\nIn this study, participants completed a Maze task using the stimuli\nfrom the Natural Stories corpus (Futrell et al., 2021), which comprises 10 texts based on\npublicly available texts, each approximately 1000 words long. In total, Natural Stories\ncontains 10,245 words. Boyce & Levy (2023) recruited 100 participants, but participants\nwere excluded if they did not self-report as native speakers of English. Following Boyce &\nLevy (2023) and Shain et al. (2024), we exclude data for all words with a reading time of\nless than 100ms or greater than 5000ms, incorrect words, words that were at the start or\nend of a sentence, and all data from participants that correctly answered fewer than 80% of\ncomprehension questions correctly. We construct linear mixed effects regressions predicting\nlog-transformed reaction time with surprisal, word length, log-transformed word frequency,\nand the word’s position in the sentence. We also included random slopes of surprisal, word\nlength, and word position for each subject, as well as a random intercept of sentence. Our\nanalysis is based on the preprocessed version of this dataset provided by Shain et al. (2024).\nA.5\nGo-Past Duration\nGo-past duration is an eye-tracking-based metric of reading time. In eye-tracking stud-\nies, participants generally read a text naturalistically. Unlike in the other experimental\nparadigms, participants can see the whole text at one time and are able to look at previously\nread words. The location of each participant’s gaze is recorded using an eye tracker, which\nalso records how long participants’ gaze is fixated on a given location. There are many\ndifferent possible eye-tracking metrics for a given word that can be calculated (see, e.g.,\nShain et al., 2024), but following recent work analyzing how well different language models\npredict eye-tracking data (Oh & Schuler, 2023b;a; Oh et al., 2024), we look at log-transformed\ngo-past duration, which is defined as the amount of time from when the word was first fix-\nated to when the participant first looked to the right of that word (in left-to-right languages\nlike English; see Luke & Christianson, 2018; Shain et al., 2024). We use data from the Provo\ncorpus (Luke & Christianson, 2018). The details of this dataset and how it was analyzed are\nprovided below.\nLuke & Christianson (2018)\nThe Provo Corpus (Luke & Christianson, 2018) is a dataset\nconsisting of eye-tracking data for 84 participants reading 55 passages (news articles, popular\nscience magazines, and fiction). Passages averaged 50 words long. In total, the texts\ncomprised 2,689 words. Participants’ go-past durations were recorded while they read each\ntext. As with the N400, we use linear mixed-effects regressions to calculate the fit of the\nsurprisals calculated by each model to the data. Following recent work (Oh et al., 2024), we\nexclude from our analysis all words that were not fixated, that followed saccades of longer\nthan 4 words, and that were at the start or end of sentences or files. Also following Oh et al.\n(2024), we constructed a regression to predict log-transformed go-past duration based on\nsurprisal as well as the following covariates: saccade length (in words), word length (in\ncharacters), word position in the sentence, log-transformed word frequency, and whether\nthe previous word was fixated. We also included random slopes of all predictors for each\n23\nPublished as a conference paper at COLM 2024\nsubject, as well as random intercepts for each subject and sentence. Our analysis is based on\nthe preprocessed version of this dataset provided by Shain et al. (2024) which was combined\nwith the full set of stimuli provided by Luke & Christianson (2018).\nKennedy et al. (2003)\nThe Dundee Corpus (Kennedy et al., 2003) is a dataset consisting\nof eye-tracking data from 10 participants reading 20 text files, each roughly 2,800 words\nin length, for a total of 56,212 words (Kennedy & Pynte, 2005; Kennedy et al., 2013, for\nadditional details, see). Our analysis approach was the same as for the Provo Corpus (Luke\n& Christianson, 2018): exclusion criteria for data were the same (except that, following Oh\net al., 2024, we also exclude words at the start and end of lines and of the screen, which\nare not annotated in the Provo Corpus), as was the structure of each linear mixed-effects\nregression. Our analysis is based on the preprocessed version of this dataset provided by\nShain et al. (2024).\n24\nPublished as a conference paper at COLM 2024\nB\nComparison of model scale and perplexity\nIn the original Mamba paper, Gu & Dao (2023) report the performance of Mamba models on\na number of benchmarks against comparable language models with different architectures.\nIn the 1.4-1.5B and 2.8-3B parameter range, Gu & Dao (2023) find that Mamba has a lower\nperplexity than Pythia on the Pile (Gao et al., 2020) validation set, and that RWKV has a\nhigher perplexity. This result suggests that at these scales, the Mamba models are best able\nto learn the statistics of language, followed by the Pythia transformers, which are in turn\nfollowed by the RWKV models.\nWe further replicate this finding for the other model sizes in Figure 3, finding that with the\nexception of the smallest models (130-170M parameters) where Pythia and RWKV have the\nsame perplexity, at every model size, Mamba has the lowest perplexity, followed by Pythia,\nfollowed by RWKV.\n15\n20\n25\n30\n108\n108.5\n109\n109.5\nNumber of Model Parameters\nPerplexity\nModel\nPythia\nMamba\nRWKV\nFigure 3: Comparison of the WikiText perplexity of each model of each architecture. Word-\nlevel perplexity of the WikiText-2 test set (Merity et al., 2017) was calculated using the\nLanguage Model Evaluation Harness (Gao et al., 2021).\n25\nPublished as a conference paper at COLM 2024\nC\nStatistical Analyses\nC.1\nFit to N400 data by model size\nDataset\nPredictor\nEstimate\nSE\nt (10)\np\np (uncor.)\nFedermeier et al. (2007)\nIntercept\n0.3139\n0.1737\n1.8068\n1.0000\n0.1009\nMamba\n-0.5605\n0.2459\n-2.2797\n0.6657\n0.0458\nRWKV\n-0.3979\n0.2605\n-1.5276\n1.0000\n0.1576\nScale\n-0.9164\n0.1078\n-8.4972\n0.0003\n<0.0001\nHubbard et al. (2019)\nIntercept\n0.3005\n0.2758\n1.0897\n1.0000\n0.3014\nMamba\n-0.7025\n0.3904\n-1.7997\n1.0000\n0.1021\nRWKV\n-0.1738\n0.4136\n-0.4202\n1.0000\n0.6832\nScale\n-0.7949\n0.1712\n-4.6428\n0.0267\n0.0009\nMichaelov et al. (2024)\nIntercept\n-0.0591\n0.4811\n-0.1229\n1.0000\n0.9046\nMamba\n-0.1731\n0.6809\n-0.2543\n1.0000\n0.8044\nRWKV\n0.4234\n0.7214\n0.5869\n1.0000\n0.5703\nScale\n-0.2270\n0.2987\n-0.7599\n1.0000\n0.4649\nSzewczyk & Federmeier (2022)\nIntercept\n0.4910\n0.2899\n1.6940\n1.0000\n0.1211\nMamba\n-0.8209\n0.4103\n-2.0008\n0.9786\n0.0733\nRWKV\n-0.6925\n0.4347\n-1.5932\n1.0000\n0.1422\nScale\n-0.7424\n0.1800\n-4.1257\n0.0484\n0.0021\nSzewczyk et al. (2022)\nIntercept\n0.3879\n0.4560\n0.8506\n1.0000\n0.4149\nMamba\n-0.8438\n0.6455\n-1.3073\n1.0000\n0.2204\nRWKV\n-0.3029\n0.6838\n-0.4429\n1.0000\n0.6673\nScale\n0.2281\n0.2831\n0.8056\n1.0000\n0.4392\nWlotko & Federmeier (2012)\nIntercept\n0.3373\n0.2122\n1.5894\n1.0000\n0.1431\nMamba\n-0.7280\n0.3004\n-2.4237\n0.5711\n0.0358\nRWKV\n-0.2705\n0.3182\n-0.8501\n1.0000\n0.4152\nScale\n-0.8666\n0.1317\n-6.5779\n0.0026\n<0.0001\nTable 3: Results of statistical analyses on the N400 datasets based on model size (opera-\ntionalized as number of parameters). Because all variables were z-scored before analysis,\nthe estimate does not directly reflect the difference but is helpful as an indication of effect\ndirection—a negative estimate indicates a lower AIC, and thus, a better fit to the data. The\nestimate for predictors Mamba and RWKV reflects their effect relative to the Pythia models.\nScale is operationalized as the logarithm of the number of parameters. We bold predictors\nthat are significant after correction for multiple comparisons (Benjamini & Hochberg, 1995).\nGiven the low power of our study (see §4), we also italicize variables that are significant\nbefore multiple comparisons.\n26\nPublished as a conference paper at COLM 2024\nC.2\nFit to reading time data by model size\nDataset\nPredictor\nEstimate\nSE\nt (10)\np\np (uncor.)\nBoyce & Levy (2023)\nIntercept\n-0.2065\n0.1726\n-1.1965\n1.0000\n0.2591\nMamba\n0.2558\n0.2443\n1.0471\n1.0000\n0.3197\nRWKV\n0.4031\n0.2588\n1.5573\n1.0000\n0.1505\nScale\n0.9279\n0.1072\n8.6588\n0.0003\n<0.0001\nBrothers & Kuperberg (2021)\nIntercept\n0.3774\n0.3298\n1.1445\n1.0000\n0.2791\nMamba\n-0.7448\n0.4668\n-1.5956\n1.0000\n0.1417\nRWKV\n-0.3900\n0.4945\n-0.7887\n1.0000\n0.4486\nScale\n-0.7049\n0.2047\n-3.4425\n0.1298\n0.0063\nFutrell et al. (2021)\nIntercept\n-0.2114\n0.1511\n-1.3997\n1.0000\n0.1918\nMamba\n0.4669\n0.2138\n2.1841\n0.7605\n0.0539\nRWKV\n0.1563\n0.2265\n0.6902\n1.0000\n0.5058\nScale\n0.9428\n0.0938\n10.0537\n0.0001\n<0.0001\nKennedy et al. (2003)\nIntercept\n-0.4226\n0.1032\n-4.0972\n0.0484\n0.0022\nMamba\n0.7710\n0.1460\n5.2809\n0.0110\n0.0004\nRWKV\n0.5155\n0.1547\n3.3326\n0.1441\n0.0076\nScale\n0.9320\n0.0640\n14.5533\n<0.0001\n<0.0001\nLuke & Christianson (2018)\nIntercept\n-0.4129\n0.1328\n-3.1089\n0.1999\n0.0111\nMamba\n0.7922\n0.1880\n4.2141\n0.0442\n0.0018\nRWKV\n0.4550\n0.1992\n2.2845\n0.6657\n0.0454\nScale\n0.9165\n0.0825\n11.1145\n<0.0001\n<0.0001\nSmith & Levy (2013)\nIntercept\n-0.3274\n0.3627\n-0.9025\n1.0000\n0.3880\nMamba\n0.3384\n0.5134\n0.6591\n1.0000\n0.5247\nRWKV\n0.7229\n0.5439\n1.3290\n1.0000\n0.2134\nScale\n0.6377\n0.2252\n2.8318\n0.2931\n0.0178\nTable 4: Results of statistical analyses on the reading time datasets based on model size\n(operationalized as number of parameters). Because all variables were z-scored before\nanalysis, the estimate does not directly reflect the difference but is helpful as an indication\nof effect direction—a negative estimate indicates a lower AIC, and thus, a better fit to the\ndata. The estimate for predictors Mamba and RWKV reflects their effect relative to the\nPythia models. Scale is operationalized as the logarithm of the number of parameters. We\nbold predictors that are significant after correction for multiple comparisons (Benjamini &\nHochberg, 1995). Given the low power of our study (see §4), we also italicize variables that\nare significant before multiple comparisons.\n27\nPublished as a conference paper at COLM 2024\nC.3\nFit to N400 data by model perplexity\nDataset\nPredictor\nEstimate\nSE\nt (10)\np\np (uncor.)\nFedermeier et al. (2007)\nIntercept\n0.2441\n0.1526\n1.5991\n1.0000\n0.1409\nMamba\n-0.1333\n0.2184\n-0.6103\n1.0000\n0.5553\nRWKV\n-0.6877\n0.2309\n-2.9784\n0.2359\n0.0138\nPerplexity\n-0.9651\n0.0983\n-9.8209\n0.0001\n<0.0001\nHubbard et al. (2019)\nIntercept\n0.2389\n0.2386\n1.0013\n1.0000\n0.3403\nMamba\n-0.3204\n0.3413\n-0.9386\n1.0000\n0.3701\nRWKV\n-0.4356\n0.3609\n-1.2070\n1.0000\n0.2552\nPerplexity\n-0.8710\n0.1536\n-5.6713\n0.0068\n0.0002\nMichaelov et al. (2024)\nIntercept\n-0.0739\n0.4882\n-0.1513\n1.0000\n0.8828\nMamba\n-0.0934\n0.6985\n-0.1336\n1.0000\n0.8963\nRWKV\n0.3752\n0.7386\n0.5080\n1.0000\n0.6225\nPerplexity\n-0.1624\n0.3143\n-0.5167\n1.0000\n0.6166\nSzewczyk & Federmeier (2022)\nIntercept\n0.4354\n0.2995\n1.4538\n1.0000\n0.1766\nMamba\n-0.4841\n0.4285\n-1.1298\n1.0000\n0.2849\nRWKV\n-0.9188\n0.4530\n-2.0281\n0.9611\n0.0700\nPerplexity\n-0.7544\n0.1928\n-3.9127\n0.0623\n0.0029\nSzewczyk et al. (2022)\nIntercept\n0.4018\n0.4657\n0.8629\n1.0000\n0.4084\nMamba\n-0.9151\n0.6663\n-1.3735\n1.0000\n0.1996\nRWKV\n-0.2625\n0.7045\n-0.3726\n1.0000\n0.7172\nPerplexity\n0.1371\n0.2998\n0.4574\n1.0000\n0.6572\nWlotko & Federmeier (2012)\nIntercept\n0.2723\n0.2288\n1.1904\n1.0000\n0.2614\nMamba\n-0.3345\n0.3273\n-1.0221\n1.0000\n0.3308\nRWKV\n-0.5350\n0.3461\n-1.5459\n1.0000\n0.1532\nPerplexity\n-0.8816\n0.1473\n-5.9859\n0.0051\n0.0001\nTable 5: Results of statistical analyses based on model perplexity. Because all variables were\nz-scored before analysis, the estimate does not directly reflect the difference but is helpful\nas an indication of effect direction—a negative estimate indicates a lower AIC, and thus,\na better fit to the data. The estimate for predictors Mamba and RWKV reflects their effect\nrelative to the Pythia models. Perplexity is operationalized as negative log-perplexity in\norder to preserve the relationship of the other variables (where negative indicates a better fit).\nWe bold predictors that are significant after correction for multiple comparisons (Benjamini\n& Hochberg, 1995). Given the low power of our study (see §4), we also italicize variables\nthat are significant before multiple comparisons.\n28\nPublished as a conference paper at COLM 2024\nC.4\nFit to reading time data by model perplexity\nDataset\nPredictor\nEstimate\nSE\nt (10)\np\np (uncor.)\nBoyce & Levy (2023)\nIntercept\n-0.1385\n0.2406\n-0.5754\n1.0000\n0.5777\nMamba\n-0.1504\n0.3443\n-0.4369\n1.0000\n0.6715\nRWKV\n0.6726\n0.3640\n1.8479\n1.0000\n0.0944\nPerplexity\n0.8996\n0.1549\n5.8072\n0.0060\n0.0002\nBrothers & Kuperberg (2021)\nIntercept\n0.3219\n0.2891\n1.1134\n1.0000\n0.2916\nMamba\n-0.3969\n0.4136\n-0.9596\n1.0000\n0.3599\nRWKV\n-0.6304\n0.4373\n-1.4416\n1.0000\n0.1800\nPerplexity\n-0.7990\n0.1861\n-4.2932\n0.0410\n0.0016\nFutrell et al. (2021)\nIntercept\n-0.1406\n0.1704\n-0.8250\n1.0000\n0.4286\nMamba\n0.0371\n0.2438\n0.1521\n1.0000\n0.8821\nRWKV\n0.4457\n0.2578\n1.7290\n1.0000\n0.1145\nPerplexity\n0.9643\n0.1097\n8.7906\n0.0003\n<0.0001\nKennedy et al. (2003)\nIntercept\n-0.3516\n0.0518\n-6.7941\n0.0021\n<0.0001\nMamba\n0.3359\n0.0740\n4.5363\n0.0297\n0.0011\nRWKV\n0.8108\n0.0783\n10.3565\n0.0001\n<0.0001\nPerplexity\n0.9833\n0.0333\n29.5132\n<0.0001\n<0.0001\nLuke & Christianson (2018)\nIntercept\n-0.3438\n0.143\n-2.4040\n0.5722\n0.0371\nMamba\n0.3721\n0.2046\n1.8182\n1.0000\n0.0991\nRWKV\n0.7383\n0.2164\n3.4126\n0.1310\n0.0066\nPerplexity\n0.9441\n0.0921\n10.2535\n0.0001\n<0.0001\nSmith & Levy (2013)\nIntercept\n-0.2781\n0.3479\n-0.7994\n1.0000\n0.4427\nMamba\n0.0336\n0.4978\n0.0676\n1.0000\n0.9475\nRWKV\n0.9313\n0.5263\n1.7696\n1.0000\n0.1072\nPerplexity\n0.6934\n0.2240\n3.0960\n0.1999\n0.0113\nTable 6: Results of statistical analyses based on model perplexity. Because all variables were\nz-scored before analysis, the estimate does not directly reflect the difference but is helpful\nas an indication of effect direction—a negative estimate indicates a lower AIC, and thus,\na better fit to the data. The estimate for predictors Mamba and RWKV reflects their effect\nrelative to the Pythia models. Perplexity is operationalized as negative log-perplexity in\norder to preserve the relationship of the other variables (where negative indicates a better fit).\nWe bold predictors that are significant after correction for multiple comparisons (Benjamini\n& Hochberg, 1995). Given the low power of our study (see §4), we also italicize variables\nthat are significant before multiple comparisons.\n29\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2024-04-30",
  "updated": "2024-08-26"
}