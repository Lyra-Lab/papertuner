{
  "id": "http://arxiv.org/abs/2404.19226v1",
  "title": "A Survey of Deep Learning Based Software Refactoring",
  "authors": [
    "Bridget Nyirongo",
    "Yanjie Jiang",
    "He Jiang",
    "Hui Liu"
  ],
  "abstract": "Refactoring is one of the most important activities in software engineering\nwhich is used to improve the quality of a software system. With the advancement\nof deep learning techniques, researchers are attempting to apply deep learning\ntechniques to software refactoring. Consequently, dozens of deep learning-based\nrefactoring approaches have been proposed. However, there is a lack of\ncomprehensive reviews on such works as well as a taxonomy for deep\nlearning-based refactoring. To this end, in this paper, we present a survey on\ndeep learning-based software refactoring. We classify related works into five\ncategories according to the major tasks they cover. Among these categories, we\nfurther present key aspects (i.e., code smell types, refactoring types,\ntraining strategies, and evaluation) to give insight into the details of the\ntechnologies that have supported refactoring through deep learning. The\nclassification indicates that there is an imbalance in the adoption of deep\nlearning techniques for the process of refactoring. Most of the deep learning\ntechniques have been used for the detection of code smells and the\nrecommendation of refactoring solutions as found in 56.25\\% and 33.33\\% of the\nliterature respectively. In contrast, only 6.25\\% and 4.17\\% were towards the\nend-to-end code transformation as refactoring and the mining of refactorings,\nrespectively. Notably, we found no literature representation for the quality\nassurance for refactoring. We also observe that most of the deep learning\ntechniques have been used to support refactoring processes occurring at the\nmethod level whereas classes and variables attracted minimal attention.\nFinally, we discuss the challenges and limitations associated with the\nemployment of deep learning-based refactorings and present some potential\nresearch opportunities for future work.",
  "text": " \nA Survey of Deep Learning Based Software Refactoring \n \nBRIDGET NYIRONGO, Beijing Institute of Technology, China \nYANJIE JIANG∗, Peking University, China \nHE JIANG, Dalian University of Technology, China \nHUI LIU, Beijing Institute of Technology, China \n \nRefactoring is one of the most important activities in software engineering which is used to improve the quality (especially the \nmaintainability) of a software system. The traditional approaches to software refactoring involve designing a series of heuristics for \nrefactoring detection, solution suggestions, and refactoring execution. However, these approaches usually employ manually designed \nheuristics, which are often tedious, time-consuming, and challenging. With the advancement of deep learning techniques, researchers \nare attempting to apply deep learning techniques to software refactoring. Consequently, dozens of deep learning-based refactoring \napproaches have been proposed. However, there is a lack of comprehensive reviews on such works as well as a taxonomy for deep \nlearning-based refactoring. To this end, in this paper, we present a survey on deep learning-based software refactoring. We classify \nrelated works into five categories according to the major tasks they cover, i.e., the detection of code smells, the recommendation of \nrefactoring solutions, the end-to-end code transformation as refactoring, quality assurance, and the mining of refactorings. Among \nthese categories, we further present key aspects (i.e., code smell types, refactoring types, training strategies, and evaluation) to give \ninsight into the details of the technologies that have supported refactoring through deep learning. The classification indicates that \nthere is an imbalance in the adoption of deep learning techniques for the process of refactoring. Most of the deep learning techniques \nhave been used for the detection of code smells and the recommendation of refactoring solutions as found in 56.25% and 33.33% of \nthe literature respectively. In contrast, only 6.25% and 4.17% were towards the end-to-end code transformation as refactoring and \nthe mining of refactorings, respectively. Notably, we found no literature representation for the quality assurance for refactoring. We \nalso observe that most of the deep learning techniques have been used to support refactoring processes occurring at the method \nlevel whereas classes and variables attracted minimal attention. Finally, we discuss the challenges and limitations associated with the \nemployment of deep learning-based refactorings and present some potential research opportunities for future work. \n \nCCS Concepts: • Software and its engineering → Software creation and management; • Software post-development issues; • \nSoftware quality assurance; \n \nAdditional Key Words and Phrases: Software refactoring, Deep learning, Code smells \n \n1 INTRODUCTION \nSoftware refactoring is used to improve software quality by changing the internal structure of a system software \nwithout altering its external behavior. It is a way of maintaining and improving the quality of software code that helps \nin the following tasks [26]. First, it helps in the discovery of bugs. When refactoring, more time and work are spent \non understanding what the program code does and incorporating any new understanding into the code. This process \n∗Corresponding author \n \nAuthors’ addresses: Bridget Nyirongo, larjean89@outlook.com, Beijing Institute of Technology, Beijing, China; Yanjie Jiang, yanjiejiang@pku.edu.cn, \nPeking University, Beijing, China; He Jiang, jianghe@dlut.edu.cn, Dalian University of Technology, Dalian, China; Hui Liu, liuhui08@bit.edu.cn, Beijing \nInstitute of Technology, Beijing, China. \n1 \n2 \nNyirongo and Jiang, et al. \n \n \nhelps to bring to light any assumptions that were previously made, making it less likely that bugs will go unnoticed. \nSecond, refactoring can help to improve the software design. With the development of software, various modifications \nmay be introduced due to misunderstanding of requirements or urgent task assignments. After such modifications, \nthe software will become harder to read and comprehend. Refactoring cleans up the program code as work is done to \nrearrange and remove parts that are not in order and are unnecessary. This helps to retain the program code’s structure \nthereby improving its design. Third, it helps in rapid software development. Refactoring helps in the faster development \nof software because it stops the design of the system from going bad. With frequent refactoring, not much time is \nspent on finding and fixing errors that might arise from poor code design. Fourth, it helps make software easier to \nunderstand. This is because a little more time spent on refactoring could make the code better communicate its purpose. \nThis code would say exactly what the developer writing it meant, as such any other developer using this code might \neasily understand it. \nResearchers have dedicated a great amount of time trying to find ways that could make the process of software refactoring \nless tedious and time-consuming. Different techniques, models, and concepts have been used. Semi-automatic and \nautomatic tools [22, 86, 89, 91, 92, 94, 100] have been developed to assist in the detection, recommendation, and safe \napplication of the refactorings. Most of these tools can easily be integrated as plugins in most of the modern IDEs like \nEclipse and Intellij IDE. The integration of these tools in the IDEs is usually alongside the already existing refactoring \nmenus within the IDEs thereby enriching the support rendered towards the process of refactoring. Even though this is \nthe case, it has been noted that most of the approaches used to come up with these tools rely on heuristics which are \nmanual in nature [51, 53]. Thus, researchers have adopted the use of machine learning techniques to minimize the use \nof manually designed heuristics [23, 25]. \nDeep learning is a sub-field of machine learning that focuses on creating large neural network models that are capable \nof making accurate data-driven decisions. Deep learning is mostly suitable for contexts where data is complex and \nwhere large datasets are available. The unique aspect of deep learning is the approach it takes to feature design which is \ncharacterized by the automatic learning of hierarchical representations from raw data thereby eliminating the need for \nmanual feature engineering. Deep learning models can learn useful features from low-level raw data and complex non- \nlinear mappings from inputs and outputs [29, 43]. This is unlike most of the statistical machine learning models where \nfeature design is a human-intensive task that can require deep domain expertise and consume a lot of time and resources. \nAt the core of deep learning are Artificial Neural Networks (ANN or NN) which are composed of interconnected \nnodes organized into layers [1, 12, 78]. Deep learning uses several types of Neural Networks each tailored for specific \ntasks. Some of the most common ones are as follows. Feedforward Neural Networks(FNN) also known as Multilayer \nPerceptrons(MLP). These are mostly used for general-purpose tasks including classification and regression. Convolutional \nNeural Networks (CNN). CNN utilizes convolutional layers to automatically learn hierarchical features from input images. \nThese were originally designed for image and grid-like data. Recurrent Neural Networks (RNN). RNN contain loops to \ncapture dependencies in sequential data. Two of its variants, Long Short Term Memory (LSTM) and Gated Recurrent \nUnit (GRU) address the vanishing gradient problem and improve the modeling of long-range dependencies. Graph \nNeural Networks (GNN). GNN learns to process and extract information from graph-structured inputs. These are used \nin tasks like node classification, link prediction, and recommendation systems. Generative Adversarial Networks (GAN). \nGAN consists of a generator network and a discriminator network that are trained simultaneously. These are used for \ngenerating new data samples, such as images, text, and music. Autoencoders. Autoencoders are neural networks that are \nused for unsupervised learning and dimensionality reduction. Autoencoders comprise an encoder network to reduce \nA Survey of Deep Learning Based Software Refactoring \n3 \n \n \nthe input data’s dimensionality and a decoder network to reconstruct the input data from the reduced representation. \nTransformers. Transformers use a self-attention mechanism to capture relationships between input elements. They are \npopularised by models like BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative \npretrained Transformer) for NLP tasks, but they have been applied to various other tasks. \nRecently, a considerable amount of research [32, 51, 53] has been proposed to investigate and explore the application \nand adoption of deep learning techniques to automate and support the process of software refactoring. Researchers \nhave employed various deep learning models in the different tasks involved in the process of software refactoring. \nTo present the state-of-the-art on the employment of deep learning in refactoring, researchers conducted literature \nreviews [2, 56, 63, 110] centering around the use of deep learning for refactoring activities, e.g., the detection of code \nsmells. Naik et al. [63] presented and analyzed 17 related works published from 2016 to 2022 to identify which deep \nlearning techniques had been used for code refactoring and how well they worked. Alabza et al. [2] conducted a \nsystematic review focusing on the deep learning approaches for bad smell detection on 67 studies published until \nOctober 2022. Malhotra et al. [56] conducted a systematic literature review examining deep learning’s capability to \nspot code smells on 35 primary studies published from 2013 to July 2023. Zhang et al. [110] conducted a survey on \ncode smell detection based on supervised learning models by analyzing 86 studies published from 2010 to April 2023. \nAlthough such reviews have significantly facilitated the understanding of deep learning-based refactoring, we still \nlack a comprehensive survey that considers the majority of related works, covering all aspects of deep learning-based \nsoftware refactoring (not just confined to code smell detection), and provides a taxonomy for deep learning-based \nrefactoring. \nTo this end, in this paper, we conduct a survey by collecting 48 primary studies published from January 2018 to October \n2023 and classify them based on the specific refactoring tasks being supported by the deep learning technique, i.e., \nthe detection of code smells, the recommendation of refactoring solutions, the end-to-end code transformation as \nrefactoring, quality assurance, and the mining of refactorings. Under each of these categories, we present key aspects (i.e., \ncode smell types, refactoring types, training strategies) related to the approaches to give insight into the technologies \nthat have supported software refactoring through deep learning. Based on such a presentation, we can provide a more \ncomprehensive perspective on deep learning-based refactoring. To the best of our knowledge, it is the first survey paper \nthat presents the hierarchical taxonomy for deep learning-based software refactoring. In our survey, we attempt to \ninvestigate the following research questions: \n• RQ1: Which tasks in software refactoring have been supported by deep learning techniques, and how often \nthey have been targeted by the surveyed papers? \n• RQ2: What are the common deep learning techniques used in software refactoring? \n• RQ3: How effective is the use of deep learning models in the process of software refactoring? \n• RQ4: What are the limitations and challenges associated with the use of deep learning techniques in software \nrefactoring? \nOur classification indicates that there is an imbalance in the refactoring tasks which have been supported by deep \nlearning techniques. Our survey indicates that most of it has been towards the detection of code smells and the \nrecommendation of refactoring solutions, unlike the end-to-end code transformation as refactoring, quality assurance, \n4 \nNyirongo and Jiang, et al. \n \n \nand mining of refactorings. Thus, there is a need for more future work to address the current imbalance, specifically in \nareas of deep learning supporting the application, quality assurance, and mining of refactorings. The rest of the paper \nis structured as follows: Section 2 introduces related work. Section 3 presents the methodology used for the survey. \nSection 4 presents and discusses studies on the detection of code smells. Section 5 presents and discusses studies on the \nrecommendation of refactoring solutions. Section 6 presents and discusses studies on end-to-end code transformation \nas refactoring. Section 7 presents and discusses studies on the mining of refactorings. Section 8 discusses some of the \nchallenges and opportunities associated with deep learning-based refactoring, and section 9 concludes the survey. \n \n2 RELATED WORK \nNaik et al. [63] conducted a systematic review of the current studies on deep learning-based code refactoring. The \nsurvey presented a high-level analysis of 17 primary works published from 2016 to 2022. The key insight of this survey \nwas to present the state-of-the-art in deep learning-based code refactoring. The review addressed research questions \nwhose main focus was on the commonly used deep learning techniques and the performance of the deep learning-based \nrefactoring approaches. This review indicated that CNN, RNN, and GNN are the commonly used deep learning models \nfor code refactoring with Multilayer Percepton (MLP) performing the best. They also noted that most of the existing \nstudies focus on Java code, method-level refactoring, and single-language refactoring with various evaluation methods. \nCompared to the survey by Naik et al. [63], our survey covers substantially more related works, increasing the number \nof surveyed papers from 17 to 48. Our well-designed search strategy retrieved many closely related papers missed by \ntheir survey. Another difference is that we present a comprehensive and hierarchical taxonomy of deep learning-based \nrefactoring, and classify all related works based on the taxonomy. \nAlabza et al. [2] conducted a review on deep learning-based approaches for bad smell detection, and their focus was to \nsummarise and synthesize the studies that used deep learning for bad smell detection. They collected and analyzed 67 \nstudies until October 2022. They analyzed deep learning models concerning the purpose of the model, the detected bad \nsmells, the employed training datasets, features, pre-processing techniques, and encoding techniques used for feature \ntransformation. Notably, this survey involved all kinds of code smells. This review indicated that code clones are the \nmost recurring smell. The review showed that supervised learning is the most adopted learning approach used for \ndeep learning-based code smell detection. Also, they observed that CNN, RNN, DNN, LSTM, Attention models, and \nAutoencoders are the most popularly used deep learning models. Notably, this review focused only on the use of deep \nlearning models for the detection of code smells which is one of the tasks used for the identification of refactoring \nopportunities. In contrast, our survey covers all aspects of refactoring. \nMalhotra et al. [56] conducted a literature review examining deep learning’s capability to spot code smells. They \npresented a total of 35 primary study works from the years 2013 to 2023. The consolidated studies highlighted four key \nconcepts, that is, the types of code smells addressed, the deep learning approach utilized in the experiment, evaluation \nstrategies employed in the studies, and the performance analysis of the model proposed. This review showed that the \nmost common code smells detected include feature envy, god class, long method, complex class, and large class. It \nalso indicated that the most common deep learning algorithms used are RNN and CNN, often combined with other \ntechniques for better results. Notably, this systematic analysis did not focus on the whole refactoring process (as what we \ndo). Instead, they only focused on the recommendation of refactoring solutions and the end-to-end code transformation \nas refactoring. \nA Survey of Deep Learning Based Software Refactoring \n5 \n \n \nZhang et al. [110] conducted a survey on code smell detection based on supervised learning models. They surveyed \n86 papers from January 2010 to April 2023. They formulated a total of 7 research questions which were empirically \nevaluated from different aspects such as dataset construction, data preprocessing, feature selection, and model training. \nBased on their analysis they concluded that most of the existing works suffer from issues such as sample imbalance, \ndifferent attention to types of code smell, and limited feature selection. They also made suggestions for future work, \none of which involves exploring the correlation between features and the perspective of code smells within the context \nof model interpretability. Notably, the core focus of this paper was on the types of deep learning models used for the \ndetection of code smells, and not the use of deep learning models in the process and support of software refactoring. \nOur paper differs from such reviews in that they focus on code smell detection only whereas we cover a much larger \nscope, i.e., the whole process of software refactoring. \n \n3 METHODOLOGY \nWe surveyed deep learning-based software refactoring by exploring and searching the following databases: https: \n//dl.acm.org, http://ieeexplore.org, https://springer.com, https://sciencedirect.com, https://onlinelibrary.wiley.com/,  \nand https://scholar.google.com. We used these databases since they contain a comprehensive coverage of academic \npublications, conferences, and journals in the field of software engineering. Utilizing these databases ensured a thorough \nand inclusive exploration of the existing literature, enabling a comprehensive understanding of the landscape of deep \nlearning-based refactoring techniques and advancements. We used a set of keywords based on the research questions \noutlined in Section 1 to retrieve studies from these databases. The main keywords were \"deep learning\", \"software \nrefactoring\", and \"code smells\". We also included synonyms of these keywords, such as \"refactoring\", \"code refactoring\", \nand \"bad smells\". These search terms were used with advanced search filters within the databases(e.g., specific year \nrange (2018 to 2023) and language specification (English)). For example, on ACM, we used the following search query: \n\"query\": ((\"deep learning\") AND (\"refactoring\")), \"filter\": publication date: 01/01/2018 TO 10/31/2023, owners.owner=HOSTED. \nWe obtained a total of 1,755 papers after the filtering, and selected the top 100 (if there are more than 100) from each \ndatabase based on relevance, resulting in 486 papers. The selection of studies was limited to the top 100 papers from \neach database based on relevance because our initial analysis suggests that items outside the top 100 are often outside \nthe scope of the survey. \nThe first author conducted a manual search to decide whether each paper should be included or not. This was done by \nreading through the title and abstract of each candidate paper. To ensure the correctness of the manual search, inclusion \ncriteria were defined, and continuous and open communication was maintained among the authors. The main criterion \nfor the classification was focused on papers that used deep learning for refactoring tasks, such as detecting code smells, \nrecommending refactorings, and the end-to-end code transformation as refactoring. The use of this criterion resulted \nin remaining with a total of 35 papers. To enhance the comprehensiveness of our literature search, we conducted a \nsnowballing process by scrutinizing the reference sections of the initial set of papers. This iterative process involved \nexploring citations within these papers to identify additional relevant studies. Google Scholar was utilized as the primary \ndatabase for this snowballing process, ensuring an expansive and thorough exploration. As a result of this snowballing \nprocedure, we identified and included 13 more papers that were deemed pertinent to our survey focus. Combining these \nnewly discovered papers with the initial set, a total of 48 papers were meticulously collected and considered as primary \nstudies for our survey. This approach allowed us to cast a wider net in the literature search, ensuring the inclusion \nof studies that may not have been initially captured, thereby enriching the depth and scope of our survey. Figure 1 \n6 \nNyirongo and Jiang, et al. \n \n \n10 \n \n8 \n \n6 \n \n4 \n \n2 \n \n0 \n2018 2019 2020 2021 2022 2023 \nYears \n \nFig. 1. Primary studies through the years \n \npresents the papers on deep learning-based refactoring from January 2018 to October 2023. The figure shows that there \nis a growing interest among researchers to adopt deep learning techniques for software refactoring. \nFrom the collected data we have developed a taxonomy classification hierarchy as presented in Figure 2. The classification \nhierarchy for the taxonomy is based on the software refactoring tasks that could be supported by deep learning \ntechniques. These tasks include the detection of code smells, recommendation of refactoring solutions, end-to-end code \ntransformation as refactoring, quality assurance, and the mining of refactorings. Figure 2 presents the total number \nof papers collected for each specific task (as presented on the lower right corners of the nodes). Our survey indicates \nthat there is an imbalance in the adoption of deep learning techniques for the process of refactoring. Most of the deep \nlearning techniques have been used to detect code smells and recommend refactoring solutions as found in 56.25% and \n33.33% of the literature respectively. In contrast, only 6.25% and 4.17% were towards the end-to-end code transformation \nas refactoring and the mining of refactorings, respectively. Notably, we found no literature representation for the \nquality assurance for refactoring. We have also observed that some researchers made contributions to more than one \nrefactoring task in the use of deep learning. For instance, 22.45% of the studies focused on applying deep learning \ntechniques on more than one refactoring task, i.e., the detection of code smells and recommendations for refactoring \nsolutions. For such papers, we discuss them in different sections regarding their contribution to the different tasks. As \nsuggested by the legend in the left bottom of Figure 2, the leftmost node presents the root of the taxonomy, and the \nnodes in the following layer represent different steps involved in software refactoring. Nodes in the rightmost two \nlayers represent different aspects (factors) that could be employed to further classify related works within the given \ncategory. For example, the \"code smell types\" node on the third layer suggests that we may classify related works on \n\"detection of code smells\" into sub-categories according to the code smells’ types they support. \nNumber of papers selected \nA Survey of Deep Learning Based Software Refactoring \n7 \n \nEvaluation \nResult Metrics \n \n \n \n \n \nDetection of \n    Code Smells:27  \n \nTraining Strategies \nCode Smell Types \nDatasets \nExplainable and \nFeedback Centric \nHybrid Approach \nGraph Based \n \n \n \nRecommendation \nof Refactoring \n  \nSolutions:16  \n  \n \nEnd to End Code \nTransformation \nas Refactoring:3 \nQuality As- \n  \nsurance:0 \n \nTraining Strategies \nRefactoring Types \n \n \n \n \n \n \nRoot Node of Taxonomy \nRefactoring Tasks \nAspects/Sub-aspects for classification \n \nFig. 2. Taxonomy of deep learning-based refactoring \n \n \n \n \n4 DETECTION OF CODE SMELLS \nRefactoring is a crucial task in software engineering. To conduct refactoring, there needs to be awareness of the issues \nin the code that might call for the process of refactoring. One of the most common issues that trigger the need for \nrefactoring is code smells. Code smells are certain structures in the code that indicate the possibility of refactoring [26]. \nDetecting code smells manually on a large codebase is a challenging task. Therefore, researchers have explored different \ntechniques, including automatic and semi-automatic ones, to aid in the detection of code smells. Despite various \ntechniques proposed by researchers, most of them rely on manually designed heuristics. The manual heuristics make \nthe process of detecting code smells for refactoring opportunities time-consuming. To solve this issue, researchers \nhave started exploring deep-learning techniques for the detection of code smells. Thus, researchers have proposed \nvarious deep-learning techniques for the detection of code smells. In this section, we discuss and present these deep \nlearning techniques based on the detection technologies, code smell types, training strategies, and evaluation techniques \nemployed. \nSequence Modelling \nResult Metrics \nDeep Learning \nBased \nRefactoring:48 \nDetection Technologies \nMining of \nRefactorings:2 \nRecommendation \nTechnologies \nDatasets \nEvaluation \n8 \nNyirongo and Jiang, et al. \n \n \n4.1 Detection Technologies \nResearchers [51, 53, 80, 109] have used various deep-learning approaches for the detection of code smells. The details of \nthese approaches are presented as follows: \n4.1.1 Sequence Modelling Based Approaches. This category pertains to technological approaches that focus on the \nsequence of code, such as source code tokens or characters. The approaches in this group utilize deep learning \ntechniques to capture contextual dependencies within code snippets. Also, these approaches detect code smells based on \nthe sequential nature of code. CNN, RNN, LSTM, Transformers, etc. are some of the deep learning techniques employed \nin this category. The deep learning techniques in this category can effectively learn patterns and relationships within \ncode sequences, thus aiding in the process of code smell detection. \nLiu et al. [53] proposed a deep learning-based approach to detecting feature envy, which is one of the most common \ncode smells. They used a Convolutional Neural Network (CNN) as their deep neural network-based classifier. The \nclassifier’s input was divided into two parts: textual input and numerical input. The textual input was a word sequence \nthat consisted of the method’s name, the name of its enclosing class, and the name of the potential target class. The \ninformation in the textual input had to pass through an embedding layer that converted the text description into \nnumerical vectors. The numerical vectors were then fed into the CNN. They had three CNN layers, each with filters=128, \nkernel size=1, and activation Tanh. The CNN classifier was trained automatically, without any human intervention. The \nlabeled samples for training were generated automatically based on open-source applications. To evaluate the approach, \nthey conducted a two-part verification process. First, they evaluated the approach on 7 well-known open-source \napplications (Junit, PMD, JExcelAPI, Areca, Freeplane, jEdit, and Weka) without automatically injecting feature envy \nsmells. This first evaluation gave an improvement on F-measure against state-of-the-art by 34.32%. The second part of \nthe evaluation was carried out on 3 open-source applications (XMD, JSmooth, and Neuroph) where no smells were \ninjected, and this outperformed the state-of-the-art. \nLiu et al. [21] proposed a method called feTruth to enhance the detection of feature envy in software using deep learning \nwith real-world examples. The feTruth technique used evolutionary histories of open-source projects stored in version \ncontrol systems such as GitHub to extract real examples of feature envy. The extracted real-world examples were then \nused to train a deep learning-based prediction model. During the testing phase, feTruth would examine the source code \nof a software project and generate a list of feature envy occurrences associated with methods in the project. feTruth \nincluded a heuristics-based filter and a learning-based filter. These two filters were used to exclude false positives \nreported by RefactoringMiner [93]. The heuristic-based filter would exclude false positives if the source class of the \npotential refactoring did not exist in the new version or if the target class of the potential refactoring did not exist in \nthe old version. The learning-based filter leveraged a decision tree-based classifier to distinguish false positives from \ntrue positives based on a sequence of features of the refactorings. By using these techniques, the researchers were able \nto generate high-quality and large-scale training data for feature envy detection. They used a CNN in the design for the \ndeep learning-based classifier. The CNN classifier leveraged new features not yet exploited by existing approaches. \nThe feTruth method was compared against Liu’s approach [51], JDeodorant [22], and JMove [89]. The subjects for \ntheir projects were divided into two parts. The first part consisted of 500 Java projects, which were used to discover \nreal-world examples of feature envy. These projects were collected from GitHub by selecting the top 500 most popular \nprojects with the largest number of stars. The second part consisted of 5 open-source Java projects, which were used \nto evaluate the proposed approach and the selected baseline. These 5 projects were chosen from Defects4J [42]. The \nA Survey of Deep Learning Based Software Refactoring \n9 \n \n \nevaluation results on real-world open-source projects suggested that the proposed approach substantially outperforms \nthe state-of-the-art in the detection of feature envy smells. The approach improves the precision and recall in feature \nenvy detection by 38.5%. \nDas et al. [18] proposed a deep learning approach to detect brain class and brain method code smells in software \napplications. They used Convolutional Neural Networks (CNN) to train a neural network-based classifier. The approach \nwas based on a large corpus of software applications, which generated a huge number of training samples. These \nsamples were labeled to indicate whether they were a code smell of kind brain class and brain method. The neural \nnetwork used in this approach had several layers - the first layer had a one-dimensional CNN layer with 256 filters and \na kernel size of 1. The activation function used was Tanh. The second layer was also a one-dimensional CNN layer with \n128 filters and Tanh as the activation function. They added a flattened layer as the third layer to connect the convolution \nlayer with dense layers. The fourth layer had a dense layer with 128 filters and ReLU as the activation function. The \nlast layer was a dense layer with only one filter and Sigmoid as the activation function. This layer acted as the output \nlayer. They used 30 open-source Java Projects as subject applications, acquired through sharing activities in GitHub \nrepositories. The dataset of the Java projects was split into mutually exclusive training and test sets. The experiment \ndemonstrated high-accuracy results for both code smells. \nLin et al. [48] proposed a new approach for detecting code smells. They used a full convolutional network that could \nidentify and use local correspondences by making use of semantic features. They defined a multidimensional array, \nh*w*d, to represent the convolutional network (where h and d are space dimensions and d is the channel). For their \nexperiment, they used an open-source database that was initially published by the author. They used this database to \ndetect various code smells such as long method, lazy class, speculative generality, refused bequest, duplicated code, \ncontrived complexity, shotgun surgery, and uncontrolled side effects. \nLiu et al. [53] proposed an approach for code smell detection using deep learning. However, this approach had some \nlimitations. To address these limitations, they presented a new approach in [51]. This approach was generic and \nevaluated on four code smells: feature envy, long method, god class, and misplaced class. They improved the deep \nneural network used in [53] by using bootstrap aggregating. They used a classifier that generated several bootstrap \nsamples simultaneously from a given training dataset. The classifier trained multiple binary classifiers that in turn \ndetermined the final classification by voting. The study highlighted that it is difficult to design and train a generic \nclassifier to detect all code smells since different features are needed for different smells. Therefore, they presented \ndifferent classifiers for different code smells. They used a Convolutional Neural Network (CNN) for the detection of \nfeature envy and misplaced class. A dense layer-based classifier was used for the detection of the long method. Dense \nlayers coupled with Long Short Term Memory(LSTM) were used for the detection of the god class. The classifier used \nfor feature envy was similar to the one in their earlier work [53]. The deep neural classifier used for the long method was \ncomposed of five dense layers besides the input and output layers. The resulting features which were extracted by the \nhidden layers were fed into the output layer. This process mapped the features into a single output to suggest if the \nfeature is associated with long method smell. The classifier for the god class was composed of two parts: a textual part \nand a numerical (code metric) input. The textual input was a word sequence formulated by concatenating the names of \nattributes and methods declared within the class under test. After converting the textual input into numerical vectors \nthrough embedding, the resulting vectors were handled by an LSTM layer. This was unlike with the code metrics which \nwere fed straight into a dense layer. This output was then merged with the output of the LSTM before being fed into \n10 \nNyirongo and Jiang, et al. \n \n \nanother dense layer. The output of the dense layer was the one that indicated whether a class should be decomposed \nor not. The classifier employed for the detection of the misplaced class was similar to that of feature envy because both \nthese smells are caused by misplaced software entities like methods, classes, etc. They evaluated the approach on 10 \nopen-source applications. The approach was compared against JDeodorant [22], DÉCOR [61], and TACO [68]. The \nresults indicated that the approach outperformed the state-of-the-art. They also evaluated the proposed approach on \nreal-world applications without any injection of smells. The two-step evaluation of the approach using generated \ndata and real-world data gave considerable differences in the performance of their proposed approach. This led to the \nconclusion that perhaps the evaluation of code smell detection approaches should rely more on manually validated \ntesting data that are often more reliable than generated data. \n \n4.1.2 Graph Based Approaches. This category refers to the use of deep learning techniques to analyze the structural \nproperties of code representation. Specifically, models like GNN and GCN are utilized to detect code smells by analyzing \nthe structural relationship between different elements in the code. By using these models, complex dependencies and \ninteractions can be captured, thereby improving the detection of code smells. \n \nYu et al. [105] proposed a Graph Neural Network (GNN) based approach to address the issue of inherent calling \nrelationships between methods that often result in low detection efficiency for feature envy detection. To achieve this \napproach, the authors collected code metrics and calling relationships. The collected features were then converted \ninto a graph where nodes represented the code metrics of a method and edges represented the calling relationships \nbetween methods. To address the imbalance of positive and negative samples, they introduced a graph augmenter \nto obtain an enhanced graph. They then fed the enhanced graph into a GNN model for training and prediction. The \nGNN classifier had four layers: the input layer, the GraphSAGE layer, the dropout layer, a fully connected layer, and \nan output layer. The input layer received the augmented graph after oversampling. The GraphSAGE updated the \nembedding of nodes based on the embedding space. The dropout layer prevented the classifier from overfitting during \nnode classification training. The fully connected layers converted the output of the dropout layer into a one-dimensional \nvector for final classification. The output layer received the vector and outputted the prediction of the classifier through \nthe activation function Sigmoid. For their experiment, the authors used five open-source projects (BinNavi, ActiveMQ, \nKafka, Alluxio, and Realm-java) collected from a dataset labeled by Sharma and Kessentini [82]. The dataset contained \n86,652 open-source projects mainly written in Java and C# on GitHub. To select the five projects, they considered \nprojects whose updates were: 1) within two years, 2) had more than 2,000 stars on GitHub, and 3) had more than 5,000 \nmethods and 500 classes. The approach for detecting the feature envy smell achieved an average F1-score of 78.90%, \nwhich is 37.98% higher than other comparison approaches. \n \nHanyu et al. [35] proposed a graph-based deep learning approach to detect long methods. Their approach extended the \nProgram Dependency Graph (PDG) into a Directed-Heterogeneous Graph. The Directed-Heterogeneous Graph was then \nused as the input graph. They employed the Graph Convolutional Network (GCN) to construct a graph neural network \nfor long method detection. The input in this approach consisted of two kinds of nodes (method node and statement \nnode) and four types of edges (include edge, control flow edge, control dependency edge, and data dependency edge). \nThe Graph Convolutional Network had two layers and one linear layer. To obtain enough data samples for the deep \nlearning classifier, they introduced a semi-automatic approach to generate a large number of data samples. To validate \ntheir approach, they compared it with existing methods using five groups of manually reviewed datasets. \nA Survey of Deep Learning Based Software Refactoring \n11 \n \n \n4.1.3 Hybrid Approaches. This category refers to the use of deep learning techniques in combination with other \nmethods to enhance the accuracy and effectiveness of code smell detection. For instance, deep learning techniques such \nas CNN, GNN, and attention mechanisms can be combined to leverage the strengths of each approach. Hybrid-based \napproaches aim to gain a more comprehensive understanding of the code, potentially leading to better performance in \ndetecting code smells. \nCombination of Structural and Semantic Features. Zhang et al. [109] proposed a new approach called DeleSmell to detect \ncode smells using a deep learning model and Latent Semantic Analysis (LSA). They argued that most of the existing \napproaches suffer from two things: 1) incomplete feature extraction and 2) an unbalanced distribution between positive \nand negative samples. To address these issues, they developed a refactoring tool to transform good source code into \nsmelly code and generate positive samples based on real-world project cases. They built a dataset with over 200,000 \nsamples from 24 real-world projects to improve dataset imbalance. DeleSmell collected both structural features through \niPlasma and semantic features via Latent Semantic Analysis and Word2Vec. DeleSmell’s model comprised a CNN \nbranch, a Gate Recurrent Unit (GRU)-attention branch, dense layers, and an SVM branch. The input was processed by \nthe GRU-attention branch and CNN branch in parallel. The CNN branch had a feature extraction component followed \nby a classification component. The feature extraction component included a set of hidden layers, including convolution, \nbatch normalization, and dropout layers. The output layer of the last dropout layer was connected to the input of a \ndensely connected network that comprised a stack of two dense layers. An attention mechanism was introduced in the \nGRU branch to learn the important features in the dataset while suppressing the interference of irrelevant information \non the classification results. For the SVM, the kernel method was used to map the nonlinear samples to high dimensional \nspace and identify the optimal hyperplane by maximizing the classification interval between the two samples. The \ninput of the last dense layer consisted of features concatenated by the GRU attention branch and the CNN branch \nand was connected to the SVM for the final classification. Grid search was used to tune the hyperparameters of the \nclassifiers. ReLU was used as the activation function of this approach. DeleSmell was used to detect brain class and \nbrain method code smells. \nIn their research, Ma et al. [55] explored the use of a pre-trained model called CodeT5 to detect feature envy, one \nof the most common code smells. They also investigated the performance of different pre-trained models on feature \nenvy detection by comparing CodeT5 with two other models, CodeBERT and CodeGPT. CodeT5 is an encoder-decoder \nmodel that considers the token type information in code. CodeGPT is a transformer-based language model that is \npre-trained on programming languages for code completion and text-to-code generation tasks. CodeBERT is a multilayer \ntransformer model that uses the same JavaTokenizer as CodeT5 to extract token sequences from source code. The \nresearchers used these models to extract semantic relationships between code snippets and compared their performance. \nThey evaluated their approach on ten open-source projects (Junit, PMD, JExtractAPI, Areca, Freeplane, JEdit, Weka, \nAdbextract, Aoi, and Grinder). Liu et al.’s [51] approach was used as their baseline for comparison. The results showed \nthat their approach improved the F-measure by 29.32% on feature envy detection compared to the state-of-the-art. \nIn their study, Hadj-Kacem and Bouassida proposed a hybrid approach for detecting code smells using deep autoencoder \nand Artificial Neural Network (ANN) [32]. Both unsupervised and supervised algorithms were used to identify the code \nsmells. The approach had two phases. In the first phase, a deep autoencoder was used for dimensionality reduction, \nwhich extracted the most relevant features. Once the feature space was reduced with a small reconstruction error, \nthe ANN classifier would then learn the newly generated data and output the final results. The second phase used a \n12 \nNyirongo and Jiang, et al. \n \n \nsupervised learning classification by using the ANN. The approach was applied to four code smells: god class, data \nclass, feature envy, and long method. The study adopted a set of four datasets that were extracted from 74 open-source \nsystems. The results showed high accuracy with precision and recall values. The best F-measure value was 98.93%, \nwhich was achieved with the god class code smell. Even at the method level, the F-measure surpassed 96%. These results \nvalidated the effectiveness of the approach. \nSharma et al. [80, 81] conducted a study on the feasibility of using deep learning models for detecting code smells \nwithout the need for extensive feature engineering. They investigated the possibility of applying transfer learning in \nthis context. The researchers trained smell detection models based on Convolutional Neural Networks (CNN), Recurrent \nNeural Networks (RNN), and autoencoder models. The CNN layer consisted of a feature extraction part followed by a \nclassification part. The feature extraction part was composed of an ensemble of layers, including convolution, batch \nnormalization, and max pooling layers. These layers formed the hidden layers of their architecture. The convolution \nlayer performed convolution operations based on the specified filter and kernel parameters. The convolution layer also \ncomputed the network’s weights to the next layer. The max pooling layer reduced the dimensionality of the feature \nspace. The batch normalization layer mitigated the effects of varied input distribution for each training mini-batch \nwhich optimized the training. The output of the max pooling layer was connected to the dropout layer, which performed \nanother regularization by ignoring some randomly selected nodes during training to prevent overfitting. The output \nof the last dropout layer was fed into a densely connected classifier network that had a stack of two dense layers. \nThese classifiers processed one-dimensional vectors, whereas the incoming output from the last hidden layer was a \nthree-dimensional tensor. For this reason, the flattened layer was used first to transform the data into the appropriate \nformat before feeding them into the first dense layer with 32 units and ReLU activation. This was followed by the \nsecond dense layer with one unit and Sigmoid activation. This second layer comprised the output layer and contained a \nsingle neuron to make predictions on whether a given instance belongs to the positive or negative class in terms of \nsmell investigation. The layer used the Sigmoid function to produce a probability within a range of 0 to 1. The RNN \nwas comprised of an embedding layer followed by a feature learning part (a hidden LSTM layer). It was succeeded by a \nregularization (a dropout layer) and classification (a dense layer) part. The embedding layer mapped discrete tokens into \ncompact vector representations. To avoid the noise produced by the padded zeros in the input arrays, they set the mask \nzero parameters by the Keras embedding layer implementation. Thus, the padding was ignored, and only meaningful \nparts of the input data were taken into account. The dropout and the recurrent dropout parameters of LSTM were set to \nlayer 0.1. The output from the embedding layer was fed into the LSTM layer, which, in turn, gave output to the dropout \nlayer. The training and evaluation samples for this approach were generated by downloading repositories containing \nC# and Java code from GitHub after filtering out low-quality repositories by RepoReapers. They downloaded 922 C# \nand 922 Java repositories in total. They applied this technique for detecting complex method, complex conditional, \nfeature envy, and multifaceted abstraction. Through this study, they discovered that although deep learning methods \ncould be used for code smell detection, the performance is smell-specific. That is, it is very difficult to have a simple and \ndirect solution for this. They also noted that they could not find a clear superior method between one-dimensional \nand two-dimensional CNNs. One-dimensional CNNs performed slightly better for the smells ’empty catch block and \nmultifaceted abstraction’, while two-dimensional CNNs performed better than their one-dimensional counterpart \nfor ’complex method and magic number’ [80]. \nIn their paper, Hadj-Kacem and Bouassida proposed a method for detecting code smells in software using a deep learning \nalgorithm [33]. They used an abstract syntax tree and a variational autoencoder to extract semantic information from \nA Survey of Deep Learning Based Software Refactoring \n13 \n \n \nthe source code. Firstly, they parsed the source code into the AST and transformed each tree into a vector representation \nthat was then fed into the variational autoencoder. The autoencoder generated a latent representation which was used \nto reconstruct the original input data. A logistic regression classifier was then applied to determine whether the code \nwas a code smell or not. The approach was evaluated on the Landfill dataset [67]. The results showed that the proposed \nmethod was effective in detecting code smells such as blob, feature envy, and long method. \nXu and Zhang [102] proposed a deep-learning approach to detect code smells based on Abstract Syntax Trees (ASTs). \nThe approach captures the structural and semantic features of code fragments from the ASTs, by utilizing sequences \nof statement trees. The sequences of statement trees were encoded using bi-directional GRU and maximum pooling. \nThen, semantic and structural features were extracted from the encoded sequence to obtain final vector representations \nof the code fragments. The approach was applied to four types of code smells: insufficient modularization, deficient \nencapsulation, feature envy, and empty catch block. The final detection results were obtained through fully connected \nlayers. The approach was applied to 500 high-quality Java projects from GitHub, outperforming state-of-the-art deep \nlearning models for both small-grained and larger-grained code smells. \nAttention Mechanism and Enhanced Neural Networks. Zhang and Dong proposed a new approach for detecting code \nsmells called MARS, which is based on a Metric-Attention-based Residual network [108]. This approach was used to \nidentify brain class and brain method code smells. MARS addresses the issue of gradient degradation by utilizing an \nimproved Residual Network (ResNet). The reason they chose ResNet is that it can reduce model parameters while \nspeeding up the training process. ResNet extracts deep feature information to enhance the accuracy of code smell \ndetection. The approach increases the weight value of important code metrics to label smelly samples by introducing a \nmetric attention mechanism. The attention mechanism used in this approach was inspired by SENet [37]. The approach \ncomprised a fully connected layer, used Tanh as the activation function to accelerate the convergence speed of the \nmodel, and used Sigmoid as the gated function. The improved ResNet had a convolution layer, a batch normalization \nlayer that accelerated the convergence speed of the network, used ReLU as the activation function, and had an addition \nas the sum operation. To train MARS, they extracted more than 270,000 samples from 20 real-world applications to \ngenerate a dataset called BrainCode, which is publicly available. They evaluated the effectiveness of the proposed \napproach by answering five research questions. The results showed that MARS achieved an average of 2.01% higher \naccuracy than the existing approaches. \nZhao et al. [111] developed a model to detect feature envy, which is based on dual attention and correlation feature \nmining. Firstly, they proposed a strategy for entity representation using multiple views. This strategy increased the \nmodel’s robustness and improved the correlation feature and the model’s suitability. Secondly, they added an attention \nmechanism to CNN’s channel and spatial dimensions. The addition of the attention mechanism enabled the accurate \ncapturing of the correlation features between entities and controlled the information flow. They compared their approach \nagainst Liu’s [53] method, JMove [89] and JDeodorant [22]. Five open-source projects were used as subject applications. \nThe experimental evaluation was divided into two parts: 1) large-scale data with feature envy automatically injected for \ntraining and classifier verification, and 2) small-scale data without feature envy injected for evaluating the approach’s \neffectiveness on real projects. The evaluation results for both feature envy-injected and non-injected projects showed \nthat the proposed approach outperformed the state-of-the-art. \nIn their paper, Wang et al. proposed a new model for detecting feature envy using a Bi-LSTM with self-attention \nmechanism [98]. They approached the problem as a deep learning task and used two input parts: a simpler distance \n14 \nNyirongo and Jiang, et al. \n \n \nmetric and text features extracted from the source code. Their approach consisted of three modules: text feature extract \nfor processing the text input, distance value enhancement for handling the distance metric input, and a feedforward \nneural network for classification. Notably, they introduced a basic attention function (AttentionBasic) that is widely \nused in language modeling, in addition to the three score functions related to attention mechanism (AttentionAdd, \nAttentionDot, and AttentionMinus) [98]. They evaluated their approach using a dataset generated from seven open- \nsource Java projects, which was released by Liu et al. [51]. The results showed that their approach outperformed \nJDeodorant [22], JMove [89], and the first deep learning method proposed by Liu et al. [51]. \n \nGuo et al. [31] proposed a method to detect feature envy code smell using a deep semantics-based approach that \ncombined method representation and a CNN model. The method representation technique was based on an attention \nmechanism and an LSTM network. The LSTM network represented the textual information of methods in source code. \nThis technique extracted semantic features from textual information and reflected the contextual relationships among \ncode parts. The attention mechanism helped in extracting specific features that were significant to code smell detection. \nThe semantic features supplemented the limited structural information in the code metrics and improved the accuracy \nof code smell detection. To achieve this approach, they first converted the textual information into vectors representing \nthe description of the method using the method representation. In the second part, the metric cluster was fed into a \nCNN-based model which had three convolutional layers and did not set the pooling layer. The CNN could fully extract \nthe features from the structural information in the code metrics to reflect the relations between adjacent metrics. After \nthis, they applied a flattened layer to turn the shape of the input into a one-dimensional layer. In the third part, they \nused a Multilayer Perceptron-based neural network. The outputs of the CNN model and method representation were \nconnected at the connection layer, which concatenated all inputs including the features from the textual input and \nthe metrics input. Behind the connection layer, they set two dense layers and one output layer to facilitate the final \nclassification which mapped the textual input and the metrics input into a single output. The output layer had only one \nneuron which represented the result of the identifier, i.e., smelly or non-smelly. Sigmoid was used as the activation \nfunction. The approach was evaluated using a dataset from 74 open-source projects. The results suggested that the \napproach achieved significantly better performance than the state-of-the-art approaches. \n \nIn a study by Liu et al. [54], an automated method was proposed to spot and refactor inconsistent method names. They \nused a graph vector and Convolutional Neural Network (CNN) to extract deep representations of the method names \nand bodies, respectively. The approach worked by computing two sets of similar names when given a method name. \nThe first set included those that could be identified by the trained model of method names. The second set included \nnames of methods whose bodies were positively identified as similar to the body of the input method. If the two sets \nintersected to some extent, the method name was identified to be consistent. If not, it was identified as inconsistent. \nThey then leveraged the second set of consistent names to suggest new names when the input method was flagged as \ninconsistent. The CNN used in this approach had two pairs of convolutional and subsampling layers. These layers were \nused to capture the local features of methods and decrease the dimensions of input data. The network layers from the \nsecond subsampling layer to the subsequent layers were fully connected. This meant that they could combine all local \nfeatures captured by convolutional and subsampling layers. For this approach, the output of dense layers was chosen \nto be the vector representation of method bodies, which synthesized all local features captured by the other layers. \nThe researchers collected both the training and test data from open-source projects from four different communities \n(Apache, Spring, Hibernate, and Google). They only considered 430 Java projects with at least 100 commits to ensure \nA Survey of Deep Learning Based Software Refactoring \n15 \n \n \nthat the projects had been well maintained. The experimental results showed that the approach achieved an F-measure \nof 67.9% on identifying inconsistent method names, improving about 15 percentage points over the state-of-the-art. \nLi and Zhang [47] proposed a hybrid model with a multi-level code representation to optimize code smell detection. \nThey first parsed the code into an Abstract Syntax Tree (AST) with control and data flow edges. Then, they applied a \nGraph Convolutional Network to get the prediction at the syntactic and semantic level. Next, they analyzed the code \ntoken at the token level using the bidirectional Long Short Term Memory network with an attention mechanism. They \napplied this approach to a total of 9 code smells (magic number, long identifier, long statement, missing default, complex \nmethod, long parameter list, complex conditional, long method, empty catch clause, and multi smells) and combined \nthem to come up with a multi-labeled dataset. \nZhang and Jia [107] proposed a new technique to detect feature envy using self-attention and Long Short Time Memory \n(LSTM). They were inspired by the transformer model in the formulation of this approach. The researchers added posi- \ntional encoding to preserve the meaning of sequences and compensate for the lack of positional information in the pure \nattention mechanism. They built on the existing deep learning model proposed by Liu et al. [51]. Also, the researchers \nconsidered attention mechanism, LSTM structure, and snapshot ensemble to improve the detection performance. In \ntheir approach, they utilized self-attention as the attention mechanism. This mechanism was implemented with the \npositional encoding layer right after the embedding layer. The self-attention score was calculated using the Softmax \nfunction and then multiplied with the original input to obtain the word embedding vector with attention score. They \nalso included an LSTM block after the attention layer to extract deeper semantic information. For the CNN model, they \ninitially tried adding a pooling layer but found it to be less effective than a dense layer. They changed the convolution \nblock’s structure from 128 dimensions to 64 dimensions and then to 32 dimensions. The researchers believed that this \nstructure could filter out the critical features while reducing the consumption of time and computing time. After the \nCNN, they inserted a dense layer with 1024 nodes right after concatenation. They also included the concept of integrated \nlearning by incorporating a snapshot ensemble in their approach, inspired by Huang et al. [39]. However, they proposed \na new periodic function instead of using the cosine function to adjust the learning rate. The results showed that their \nmodel achieved better performance on four evaluation metrics, with precision increasing by 0.048, recall increasing \nby 0.035, F-measure increasing by 0.043, and AUC increasing by 0.056. The introduction of the attention mechanism \nand LSTM illustrated the correlation between code smell detection and natural language processing. Compared to the \nmodel of feature envy detection in Liu et al. [51], this study optimized it from three aspects: modifying and expanding \nthe model structure, introducing the self-attention mechanism, and applying a snapshot ensemble. \nTraditional Machine Learning and Deep Learning. Menshawy et al. [60] proposed a mechanism to detect feature envy \ncode smell using machine and deep learning techniques. The study applied 6 deep learning techniques (CNN, Long \nShort Time Memory (LSTM), Bidirectional LSTM (BILSTM), Gated Recurrent Unit (GRU), Bidirectional Gated Recurrent \nUnit (BIGRU), and Autoencoder) and 11 machine learning techniques based on code structural features. The deep \nlearning models were implemented using TensorFlow and Keras frameworks [41]. The CNN model was inspired by an \nimage classification model. The CNN comprised of an input layer that passed the input features to the embedding layer. \nThe role of this layer was to map vocabularies in high-dimension space to vectors of fixed size. The embedding output \nwas fed to the convolution one-dimensional layer of a specific kernel and filter parameters. The new weights were \ncomputed to the next max pooling one-dimensional layer which reduced the dimensionality of the feature space. To \navoid overfitting, the weights were passed to a dropout layer to randomly disregard a specific percentage of nodes \n16 \nNyirongo and Jiang, et al. \n \n \nduring training. The weights were connected to a flattened layer and then a stack of three dense layers to predict \nif a given instance was smelly or belonged to the non-smelly data. The LSTM and GRU architectures were inspired \nby a typical NLP model. The input layer fed the next embedding layer with input textual features. The embedding \nlayer mapped the input tokens to vectors. The new vectors were passed to the LSTM layer in the LSTM model or \nthe GRU layer in the GRU model. Both networks (LSTM and GRU) had recurrent dropout and dropout values of 0.1. \nThe new weights were fed to the dropout layer to avoid overfitting and then fed to a flattened layer and a stack of \nthree dense layers to avoid underfitting. Similar to the CNN architecture, the input to the dense layers was one unit. \nThe Sigmoid function was applied to decide whether the output belongs to the positive class or the negative class. \nA bidirectional network was applied to the LSTM and GRU layers with the same layers and hyperparameters of the \nLSTM and GRU models respectively. For the machine learning approach, eleven individual algorithms of different \nclassifier families were applied. These included Decision Table, Instance-based learning with parameter K, J48, JRip, \nMultilayer Perception, Naive Bayes, Random Forest, Simple Logistics, Sequential minimal optimization, AdaBoost, and \nBagging. Both approaches (deep learning and machine learning) were applied to open-source Java projects from the \nQualitas Corpus dataset. In the machine learning approach, the Designate Java tool was used to detect the code smell \nand to extract the corresponding metrics in CSV files. The machine learning data processor module splits the extracted \ndata into positive and negative samples. The machine learning algorithms were then applied to the processed output \nCSV samples to train the models and to evaluate the classifier’s performance. In the deep learning approach, the data \nwas tokenized by the JavaTokenizer tool which exported tokenized text files. The deep learning data processing stage \nsplits the tokenized files into positive and negative samples according to the extracted detection information from \nDesignateJava. The tokenized input was then fed to the deep learning models to detect the code smells. The results \nshowed that deep learning techniques are promising and that they tend to achieve good results compared with the \nmachine learning approach. Based on their evaluation of the 6 deep learning techniques against machine learning \nmodels, the autoencoder models achieved superiority among all the deep learning techniques. In contrast, CNN achieved \nthe lowest F-score. Overall, the deep learning techniques showed high potential in predicting feature envy. However, \nthe deep learning techniques based on semantic features are not capable of detecting all code smell types. \nHamdy and Tazy [34] proposed an approach for detecting the occurrence of the god class smell in source code. Their \napproach utilized both the source code textual features and metrics to train three deep learning models (Long Short \nTerm Memory, Gated Recurrent Unit, and Convolutional Neural Network). They built a dataset for the god class \nsmell in source code acquired from the Qualitas Corpus repository. They extracted the textual features of the source \ncode using natural language processing techniques and integrated them with metric features. They then trained the \nthree deep learning models using different types of source code features, such as metrics, textual features, and hybrid \nmetrics-textual features. The Convolutional Neural Network (CNN) used in the deep learning approach comprised a \nstack of convolution stages, for feature selection, followed by a stack of dense layers for classification. The CNN applied \na set of filters on the input sequence and produced the feature map, which represented an input to the max pooling layer. \nThe output of the last max pooling layer was connected to a dropout layer, which performed regularization by ignoring \nsome random nodes during training to prevent overfitting. In the experiment, they set the dropout rate to be equal to \n0.5. The output of the last dropout layer was fed into a dense layer, which had a fully connected multi-perceptron neural \nnetwork that worked like a classifier. They used a stack of two dense layers: the first dense layer had 32 units and ReLU \nactivation, followed by a second dense layer with the number of outputs set to equal to one. The second dense layer \nmade predictions on whether the god class smell occurs or not in a given source code. This layer used the Sigmoid \nA Survey of Deep Learning Based Software Refactoring \n17 \n \n \nactivation function to produce a probability within the range of 0 and 1. The LSTM and GRU models comprised a stack \nof LSTM/GRU layers, a dropout layer, and a dense layer. The LSTM/GRU layer learned the representation of each class. \nThe regular dropout in the dropout layer was set to 0.5, while the recurrent dropout parameters of the LSTM/GRU were \nset to 0.1. For the machine learning approach, three traditional machine learning techniques were used: Naïve Bayes, \nRandom Forest, and Decision tree (C4.5). These traditional approaches were mainly used to compare the effectiveness \nof the proposed deep learning technique. The evaluation results showed that the deep learning technique performed \nbetter than the latter. \nDewangan et al. [19] proposed a machine learning-based approach to predict code smells in software and identify the \nmetrics that play a significant role in detecting them. They used four code smell datasets, i.e., god class, data class, \nfeature envy, and long method, generated from 74 open-source systems (Qualitas Corpus) obtained from Fontana \net al. [24]. Six different algorithms, including Naïve Bayes, KNN, Decision tree, logistic regression, Random Forest, \nand Multilayer Perceptron, were used in the statistical-heuristic machine learning models. The performance of each \ntechnique was evaluated individually for the four code smells. The Multilayer perceptron (MLP) algorithm was found \nto perform the best in terms of accuracy for detecting the data class code smell. \nBarbez et al. [10] proposed a machine learning-based ensemble method called Smart Aggregation of Anti-patterns \nDetectors (SMAD) to create an improved classifier compared to standalone approaches. To train and evaluate the model, \nthey created an oracle that contained the occurrences of god class and feature envy in eight open-source systems. \nHowever, neural networks often perform poorly on imbalanced datasets like their oracle, so they designed a training \nprocedure that maximizes the expected Matthews Correlation Coefficient (MCC). They then evaluated SMAD on the \noracle and compared its performance with other aggregated tools and competing ensemble methods. Although SMAD \nis intended for detecting antipatterns, it can serve as a benchmark for researchers looking to develop standalone tools \nfor identifying common code smells during the refactoring process. \n4.1.4 Explainable and Feedback Centric. This category refers to studies that have used explanation mechanisms or \nintegrated user feedback in their approach to detecting code smells using deep learning. Explanation mechanisms \ntypically aim to incorporate methods and techniques that facilitate the understanding and interpretability of the deep \nlearning model’s decision-making process during code smell detection. This is particularly important for developers, as \nit enables them to have faith in the process, especially when the detected code smells lead to critical code changes. On \nthe other hand, user feedback integration involves actively seeking and incorporating feedback from developers into \nthe deep learning-based code smell detection process. Continuous interaction with the developers ensures adaptability \nto evolving coding practices, preferences, and domain-specific knowledge. \nFor the use of explainable mechanisms, Yin et al. [104] have proposed an explainable approach to detecting feature envy \nbased on local and global features. To make the most of the code information, they designed different representation \nmodels for global and local code. They extracted different feature envy features and automatically combined those \nthat were beneficial for detection accuracy. They further designed a Code Semantic Dependency (CSD) to make the \ndetection result easy to explain. The global feature contained a global semantic feature and a global metrics feature. \nThey splice the extracted method name, enclosing class name, and target class name together to create the global \nsemantic features. The global semantic features were metric values calculated with plugins. LSTM was used for the \nglobal semantic features to extract context from the input statements. Then, it extracted the semantic relations from \ncontext features. LSTM focused on the cell state in the neural network and used three gates to determine how much \n18 \nNyirongo and Jiang, et al. \n \n \ncell state information was retained. The gate structure was used to select the appropriate call state information so \nthat LSTM could easily capture the context information between the legal name, the class name, and the target class \nname. For the global metrics, CNN was used to obtain complex mapping relations from simple metric information. They \nopted for CNN due to its ability to automatically extract features from the original features, which could reveal the \nrelationship between metrics information and code smell. CNN is also highly suitable for parallel training on GPU \nwhich could greatly reduce the training time. To achieve the CSD, a siamese network was formed based on a local \nfeature representation model. It had the same branch with different inputs. The code input would first pass through an \nembedding layer and then through an attention layer, meanwhile, the parameters were also being shared. At the final \nstage, they would calculate the code dependency of the feature obtained from each branch. To evaluate the approach, \nthey used manually constructed code smell projects (Junit, PMD, JExcelAPI, Areca, Freeplane, jEdit, and Weka) and 3 \nreal-world projects (Xmd, JSmooth, and Neuroph). They compared their approach against Liu et al.’s [53] approach, \nJDeodorant [22], and JMove [89]. The F-measure for the two experimental setups were 2.85% and 6.46%, respectively, \nwhich was higher compared to the state-of-the-art approaches. \nFor the integration of feedback, Nanadani et al. [64] conducted a study to investigate the effects of human feedback on the \nperformance of trained models in detecting code smells, which are subjective perceptions of developers. To create a robust \nand adaptable system, the study combined deep learning techniques, user feedback, and a containerized deployment \narchitecture for a locally-run web server. The deep learning techniques used in the study were an autoencoder with a \ndense multilayer perceptron, an autoencoder with a Long Short Term Memory, and a variational autoencoder with a \nthreshold strategy for classification. The first step in this approach was to train the autoencoder, which was used to \ncompress the input data into a lower dimensional representation called the latent representation. The autoencoder \nhad an encoder and a decoder, with the encoder starting with an input layer followed by a series of dense layers. To \nimprove the training stability and efficiency, batch normalization was added to standardize inputs for each mini-batch. \nThe decoder was then constructed in the reverse order of the layers. The variational autoencoder was used to serve as \na deep generative model that employed Bayesian inference to estimate the latent representation. The approach was \nused to detect complex method, long parameter lists, and multifaceted abstraction code smells. A plugin for IntelliJ \nIDEA was created, and a container-based web server was developed to offer services of the baseline deep learning \nmodel. The setup allowed developers to see code smells within the IDEA and provide feedback. Using this setup, the \nresearchers conducted a controlled experiment with 14 participants divided into experimental and control groups. In \nthe first round of the experiment, the code smells predicted by using the baseline deep learning model were shown, \nand feedback was collected from the participants. In the second round, the researchers fine-tuned and reevaluated the \nmodel’s performance before and after adjustment. The results showed that calibration improves the performance of the \nsmell detection model by an average of 15.49% in F1 score across the participants of the experimental group. \n \n4.2 Code Smell Types of Refactoring Opportunities \nCode smells are defined as certain structures in the code that call for the process of refactoring. Based on this definition \nFowler [26] proposed 22 types of code smells, for example, long method, duplicate class, feature envy, duplicate class, etc. \nCode smells have been analyzed and categorized based on the implementation, design, and architectural levels (based on \ntheir scope, granularity, and impact) [26–28, 81]. Essentially, code smells may occur at different levels of the codebase, \nthat is class, method, or variable levels. This occurrence at different levels affects how these smells may be detected \nwhen employing deep learning models. From our literature collection, we note and observe, as presented in Table 1, that \nA Survey of Deep Learning Based Software Refactoring \n19 \n \n \na total of 26 different types of code smells have been detected using deep learning techniques which could potentially \nlead to the identification of refactoring opportunities. The data collected shows that the smells detected were occurring \nat different levels of the code base from class up to variable level. Also, we observe that some researchers focused on the \ndetection of only one type of code smell [21, 53, 105] while others [18, 32, 48] focused on the detection of at least more \nthan one type of code smell. Overall, our data suggest that feature envy being one of the most common code smells is \nthe one whose detection researchers are employing the use of deep learning techniques more often. Feature envy code \nsmell appeared in at least 27.87% of the primary studies, seconded by long method 9.84%, god class 6.56%, complex \nmethod 6.56%, and the rest of the smells. These smells, i.e., feature envy and long method could potentially lead to the \nrecommendation and suggestion of refactorings which involve 1) moving features between objects, e.g., move method \nrefactoring. 2) composing methods to ensure that they are much easier to understand, e.g., extract method refactoring. \nThrough our literature search we have noted that other researchers like Xu and Zhang [102] combined several code \nsmells to create multi smells as a way of enhancing the efficiency and validation of their approach. \n \n \n4.3 Training Strategies \nDetecting code smells using deep learning models requires the adoption of effective training strategies. These strategies \ninvolve an organized approach to teaching the model to recognize patterns, make predictions, and perform tasks \nbased on the data. Training strategies are crucial in developing a robust and accurate model for detecting code smells. \nResearchers use various techniques to create effective training strategies that produce efficient models. Our analysis of \nthe literature focuses on how researchers preprocess and engineer features from various metrics to effectively represent \ncode smells. We also explore the embeddings and representations used to capture semantic relationships within the \ncode, which aids the model’s ability to learn code smells. Researchers use various data preprocessing, feature extraction, \nand data balancing techniques to enable the deep learning model to identify different code smells effectively. \n4.3.1 Data Preprocessing. Data preprocessing is an essential step to improve the quality of data that will be fed into deep \nlearning models. During the process of data preprocessing, several techniques are utilized, including data cleaning, data \nintegration, data transformation, and data regularization. Based on the nature of the datasets used, various techniques \nmay be applied to preprocess the candidate metrics such as code, text, graph data, etc. For instance, Sharma et al. [80, 81] \nperformed their preprocessing by analyzing the data using Designate and DesignateJava on their C# and Java code, \nrespectively. Then they split their code using Codesplit before applying tokenization. They performed duplicate removal \nafter tokenization to ensure that no duplicate data was fed into the deep learning model. In contrast, Himesh et al. [64] \nconducted their duplicate removal before tokenization using a hash function to compute a unique hash value for each \ncode instance and compared the hash values to identify any duplicates. Regularization is another widely adopted \ntechnique used during data preprocessing to prevent deep models from learning specific and irrelevant features of their \ndata. Barbez et al. [10] used regularization to add a special term to the loss function, which encourages the weights to \nbe small this was as defined by Witten [99]. Overall, data preprocessing is crucial in enhancing the quality and integrity \nof data, which in turn leads to better deep learning model performance [103]. \n4.3.2 \nFeature Extraction. Various features from data candidates are extracted for deep learning models to detect \nvarious code smells. The selection of features to be extracted plays a key role in the way a deep learning model makes \npredictions. The features that could be extracted for code smell detection include but are not limited to the following \n20 \nNyirongo and Jiang, et al. \n \n \nTable 1. List of code smells detected in the primary studies. \n \nCode Smells \nFrequencies \nReferences \nFeature envy \n17 \n[10, 19, 21, 31, 33, 51, 53, 55, 60, 80, 81, 98, 102, 104, 105, 107, 111] \nLong method \n6 \n[19, 33, 35, 47, 48, 51] \nGod class \n4 \n[10, 19, 32, 34] \nMisplaced class \n1 \n[51] \nBrain class and Brain method \n3 \n[18, 108, 109] \nComplex method \n4 \n[47, 64, 80, 81] \nComplex conditional \n2 \n[47, 81] \nMultifaceted abstraction \n3 \n[64, 80, 81] \nLazy class \n1 \n[48] \nSpeculative generality \n1 \n[48] \nRefused bequest \n1 \n[48] \nDuplicate code \n1 \n[48] \nShotgun surgery \n1 \n[48] \nContrived complexity \n1 \n[48] \nUncontrolled side effects \n1 \n[48] \nInsufficient modularization \n1 \n[102] \nDeficient encapsulation \n1 \n[102] \nEmpty catch block/clause \n3 \n[47, 80, 102] \nMagic number \n2 \n[47, 80] \nLong identifier \n1 \n[47] \nLong statement \n1 \n[47] \nMissing default \n1 \n[47] \nLong parameter list \n2 \n[47, 64] \nBlob \n1 \n[33] \nInconsistent method names \n1 \n[54] \nData class \n2 \n[19, 32] \n***Multi Smells \n1 \n[47] \n \nstructural features, semantic features, naming conventions and documentation, patterns, etc. Several of our primary \nstudies [18, 108, 109] were found to detect similar code smells such as feature envy, long method, god class, brain \nclass/method, etc. respectively. However, we note that different features were employed for the same code smell across \nthe literature. Liu et al. [51, 53] leveraged the use of semantic and structural features to detect feature envy code smell \nbut ignored the semantic information contained in input sequences. Thus, Zhang et al. [107] proceeded to include \nthe semantic information contained in the input sequences as a way of improving the efficiency of the deep learning \nmodel. Apart from just using structural and semantic features, we note that in the detection of feature envy code smells, \nresearchers used global metric features [104], calling relationships [105], and others even extracted ASTs from the code \nfragments and formed sequences of statement trees like the case of Xu and Zhang [102]. For the detection of the long \nmethod, we note that in as much as most of the researchers employed the use of source code metrics, there was a slight \ndifference in how these were processed for them to be extracted as features. For example, Lin et al. [48] converted \nA Survey of Deep Learning Based Software Refactoring \n21 \n \n \nTable 2. Representative feature sets. \n \nCode Smells \nFeatures \nReferences \n \nFeature envy \ncode metrics, textual features \n[10, 19, 21, 31, 33, 51, 53, 55, 60, 80, 81, 98, 102, 104, 105, 107, 111] \n \n \n \n \n \n \n \n \n \nthe metrics into XML while Hadj-Kacem and Bouassida [33] parsed the metrics into an AST. To detect god class code \nsmell, Hamdy et al. [34] used textual features from source code while Hadj-Kacem and Bouassida [32] and Dewangan \net al. [19] extracted their features from code metrics. Similarly, for brain class/method Zhang et al. [109] used both \nsemantic and structural features while Zhang and Dong [108]and Das et al. [18] used code metrics. Table 2 gives a \nrepresentative tabulation of the scenarios being highlighted. From Table 3 we note that it is only in the detection of \ndata class where researchers used the same feature set. \n \n \n4.3.3 \nFeature Embedding. Deep learning models require the features extracted to be converted into a suitable format \nfor the model to extract the relevant features. Tokenization and vectorization are common processes used to achieve this. \nTokenization breaks down text into smaller units (tokens), while vectorization converts these tokens into numerical \nvectors. Researchers also use embedding techniques to convert the candidate data into the required format. Embedding \nis a specific type of vectorization that represents words as dense vectors in a continuous space, capturing semantic \nrelationships between words. Table 3 lists some of the tools that researchers used to execute these processes. Our data \nindicates that most researchers used Word2Vec as an aid to conducting embedding for their data candidates, with 23.08% \nof the primary studies using it, followed by iplasma at 19.23%. We also discovered that at least 15% of the studies did not \nexplicitly highlight the tools used for the feature extraction processes. \n \n \n4.3.4 \nData Balancing. Data balancing is the process of ensuring that the different categories or labels in a dataset \nare represented equally in terms of their frequency. This is particularly important in classification problems where \nthe model tries to predict different classes. A balanced dataset prevents the model from being biased towards the \nglobal metrics,asts \nLong method \ncode metrics, source code (xml) \n[19, 33, 35, 47, 48, 51] \nGod class \ncode metrics, textual features \n[10, 19, 32, 34] \nBrain class/method \ncode metrics, textual features \n[18, 108, 109] \nComplex method \nsource code, code metrics \n[47, 64, 80, 81] \nComplex conditional \nsource code, code metrics \n[47, 81] \nMultifaceted abstraction \nsource code code metrics \n[64, 80, 81] \nEmpty catch block \nsource code, code metrics \n[47, 80, 102] \nMagic number \nsource code, code metrics \n[47, 80] \nLong parameter list \ncode metrics,source code \n[47, 64] \nData class \ncode metrics \n[19, 32] \n22 \nNyirongo and Jiang, et al. \n \n \nTable 3. Feature extraction tools. \n \nTools \nReferences \nWord2Vec \n[51, 53, 54, 102, 104, 109] \niplasma \n[18, 31, 32, 108, 109] \nJavalang \n[34] \nNLTK \n[34] \nTokenizer \n[80, 81] \nFluid tool \n[31, 32] \nJavatokenizer \n[60] \nGraphSAGE \n[105] \nWrapper \n[19] \nCodeT5 \n[55] \n \nmajority class and ensures that it performs well in all classes. Imbalanced datasets, on the other hand, can lead to poor \nperformance of the model, especially for the minority class. The detection of code smells also requires a balanced dataset \nwhere the number of positive samples (with code smell) and negative samples (without code smell) are maintained. \nDifferent techniques can be used to balance the dataset such as under-sampling or oversampling. Under-sampling \nremoves samples from the majority classes while oversampling generates new samples for the minority class. Synthetic \ndata generation and ensemble methods can also be used to reduce the disadvantages of having an imbalanced dataset. \nResearchers have used the Synthetic Minority Over Sampling Algorithm (SMOTE) to balance their data in the detection \nof code smells. SMOTE is an effective oversampling technique that has been widely used in practice. However, its \neffectiveness may vary depending on the specific characteristics of the data and the problem at hand. It is worth noting \nthat while 78.5% of the primary studies were clear about the techniques they used to balance their data, 21.5% did not \nmention any issues with data imbalance. \n4.3.5 Model Training. Various approaches exist for training deep learning models. According to our literature collection, \nmodels can undergo training using labeled data, unlabeled data, or a blend of both. The process of labeling data can \noccur automatically, semi-automatically, or manually. These training methodologies are commonly categorized as \nsupervised, unsupervised, or a combination of both, where a simultaneous application of both techniques is employed. \nThese techniques are presented in Figure 3. Our survey found that the majority of primary studies (65.38%) used \nsupervised learning techniques, while 23.08% used unsupervised techniques and 11.54% used a combination of both. \n \n4.4 Evaluation \n4.4.1 Datasets. Based on our survey findings, a majority of the primary studies utilized applications exclusively \ndeveloped in a single programming language, such as Java, for conducting experiments and evaluations. In contrast, \nsome studies opted for a diverse approach by employing a combination of applications developed in different languages, \nsuch as Java and C#, for their experiments and performance assessments. As depicted in Figure 4, the data reveals \nthat 87.5% of the primary studies relied on projects developed in a singular programming language, while only 12.5% \nA Survey of Deep Learning Based Software Refactoring \n23 \n \n65.38% \n11.54% \n23.08% \n \nSupervised \n \n \n \n \n \n \nCombination of both \n \n \nUnsupervised \n \nFig. 3. Model training strategies \n \nemployed a combination of programming languages in their application projects. Additionally, as depicted in Figure 5, \na significant majority, specifically 91.67%, of the studies employed projects that are publicly accessible, i.e., general \nopen-source projects available in various repositories, and real-world open-source projects. In contrast, 8.33% utilized \nprivate projects with restricted access. \n \n \nSingle language \n \nPublic \n \n \n \nMultiple languages \n \n \n \n \nPrivate \n \n \n \nFig. 4. Programming languages \nFig. 5. Dataset types \n \n4.4.2 Result Metrics. Researchers use metrics to evaluate the performance of a particular approach. Commonly used \nmetrics for this calculation include precision, recall, F-measure, and accuracy. Precision measures the accuracy of the \npositive predictions. Its formula is defined as (Precision = TP / (TP + FP)), where TP represents true positive and FP \nrepresents false positive. It is important in situations where false positives should be minimized, and there is a cost \nassociated with making incorrect positive predictions. Recall measures the model’s ability to capture all relevant cases. \nIts formula is defined as (Recall = TP / (TP + FN)), where FN represents a false negative. It is important when the \ngoal is to capture as many positive cases as possible, and there is a cost associated with missing positive predictions. \nThe F1 score or F-measure provides a balance between precision and recall. Its formula is defined as (F1 Score = 2 * \n(Precision * Recall) / (Precision + Recall)). It is useful when both false positives and false negatives need to be minimized. \nAccuracy measures the ratio of correctly predicted observations to the total observations. Its formula is defined as \n(Accuracy = (TP + TN) / (TP + TN + FP + FN)), where TN represents true negative. While accuracy is a common metric, \nit may not be suitable for imbalanced datasets as it can be misleading. Accuracy is widely used when the classes are \n87.5% \n12.5% \n91.67% \n8.33% \n24 \nNyirongo and Jiang, et al. \n \n \n25 \n \n20 \n \n15 \n \n10 \n \n5 \n \n0 \nPrecision   Recall   F1-score Accuracy   AUC \nMCC \nKappa \nPerformance assessment \n \nFig. 6. Metrics used for performance assessment \n \nbalanced. However, in imbalanced datasets, accuracy alone may not provide a complete picture of model performance. \nFrom our primary studies, we have noted that researchers use other metrics apart from the outlined (precision, recall, \nf-measure, and accuracy). For example, researchers [35, 51, 80, 107] have used Area Under Cover(AUC) to represent \nthe trade-off between true positive rate and false positive rate at various thresholds. For instance [10, 51], used the \nMatthews Correlation Coefficient (MCC). MCC is a correlation coefficient between the observed and predicted binary \nclassifications. It takes into account true and false positives and negatives and is particularly useful when dealing with \nimbalanced datasets. Its formula is defined as (MCC = (TP * TN - FP * FN) / sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN \n+ FN))). MCC is a balanced metric that works well for imbalanced datasets. Lin et al. used [48] Kappa (Cohen’s Kappa). \nKappa measures the agreement between observed and predicted classifications, correcting for the agreement that could \noccur by chance. Its formula is defined by ( Kappa = (Po - Pe) / (1 - Pe)), where Po is the observed agreement, and Pe \nis the expected agreement. Kappa is useful for classification problems where class distribution is imbalanced, and it \naccounts for the agreement that could occur by chance. From Figure 6 it becomes apparent that precision, recall, and \nF-measure are commonly used for performance assessment. Notably, 96.15% of our primary studies utilized precision, \nwhile 92.31% and 88.46% employed recall and F-measure, respectively. \n5 RECOMMENDATION OF REFACTORING SOLUTIONS \nThe recommendation of refactoring solutions involves identifying areas of code that could benefit from refactoring and \nsuggesting potential approaches or strategies for improving them. Recommendations are often made based on code \nreviews, automated analysis tools, or architectural discussions. The recommendation of refactoring solutions does not \ninvolve directly altering the code but rather advising on possible improvements. The recommendation of refactoring \nsolutions is more about identifying problems and proposing solutions than actually implementing them. In this section, \nwe will explore how deep learning techniques have been used by researchers to suggest refactoring solutions. We will \ndiscuss the recommendation technologies, the suggested refactorings, the training strategies, and the evaluation metrics \nused to assess the performance of the approach. \nNumber of papers which used the metric \nA Survey of Deep Learning Based Software Refactoring \n25 \n \n \n5.1 Recommendation technologies \nThrough the use of various deep-learning techniques, researchers have been able to suggest solutions for refactoring. \nFrom our literature search, we note that some researchers [3, 9, 44] have utilized deep learning models to predict \nspecific refactoring solutions. Conversely, others [51, 53, 55] have utilized deep learning models to initially detect the \npresence of code smells in a given codebase and subsequently recommend refactoring solutions based on the identified \ncode smells. Consequently, we have categorized these approaches into two distinct groups: refactoring-based and code \nsmell-based. We will discuss the technologies used in these studies in the following sections. \n5.1.1 Refactoring Based Technologies. Alenezi et al. [3] conducted a study on the effectiveness of deep learning \nalgorithms in building refactoring prediction models at the class level. They used a Gated Recurrent Unit (GRU) as \na key structural parameter. The GRU comprised a hidden layer, an epoch, and a normalization batch. The GRU had \nthree hidden layers and utilized the number of epochs between 10 to 2500, with a batch size of 2 to 25. The deep \nlearning algorithm proposed by Qasem et al. [76] was used after the three hidden layers. The study used 7 open-source \nJava-based projects to assess the effectiveness of the proposed algorithm, which had two main stages. In the first stage, \na set of necessary preprocessing procedures were performed on the datasets. During this initial stage, SMOTE was used \nto prevent any imbalance. In the second stage, the deep learning algorithm was applied to the dataset to predict the \nneed for refactoring at the class level by using the Gated Recurrent Unit algorithm. For the experiment, two sets of \ndatasets, balanced and unbalanced, were used to check if the balance of the dataset would affect the predictability of the \ndeep learning model. The results indicated that a balanced dataset enhances the prediction of class-level refactorings. \nAniche et al. [9] conducted a large-scale study to evaluate the effectiveness of various supervised machine learning \nalgorithms in predicting software refactoring. They aimed to demonstrate that machine learning methods can accurately \nmodel the refactoring recommendation problem. The study used a dataset containing over two million real-world \nrefactorings extracted from more than 11,000 real-world projects from Apache, F-Droid, and GitHub repositories. The \nstudy evaluated six machine learning algorithms, of which five were traditional machine learning approaches (logistic \nregression, naive Bayes, support vector machine, decision trees, and random forest), and one was a neural network used \nas the deep learning technique. The neural network used was a sequential network of three dense layers with 128, 64, \nand 1 units, respectively. To avoid overfitting, dropout layers were added between the sequential dense layers, keeping \nlearning in 80% of the units in dense layers. The number of epochs was set to 1000. All the algorithms employed for \nthis technique were then individually used to predict refactorings at different levels: class, method, and variable. The \nresulting models were able to predict 20 refactorings at the different levels with an accuracy higher than 90%. \nKumar et al. [44] developed a recommendation system that suggests which methods require refactoring. They used 25 \ndifferent source code metrics at the method level as input features in a machine learning framework. The system then \npredicts the need for refactoring. The authors conducted a series of experiments on a publicly available annotated dataset \nof five software systems to investigate the performance of the approach. The approach used ten different machine \nlearning classifiers, of which seven (AdaBoost, LegitBoost, Log, NB, BayesNet, RF, and RBFN) were traditional machine \nlearning techniques. Only three out of the ten were based on deep learning, namely MLP, ANN-GD, and ANN-LM. \nThe first phase of the approach focused on analyzing relevant features. The considered dataset was preprocessed to \nextract relevant features. At this stage, the Wilcoxon rank sum test was applied to handle uncertainty in the dataset, \nand also for extracting features. Subsequently, ULR analysis was applied to identify the final set of source code metrics \nconsidered for further implementation. The second phase involved the model-building process and the assessment of the \n26 \nNyirongo and Jiang, et al. \n \n \nperformance of the proposed model. During the model-building process, the dataset was normalized through min-max \nnormalization. The class imbalance issue of the dataset was addressed using SMOTE, RUSBoost, and Up-Sample. Then, \na 10-fold cross-validation was employed with the 10 machine learning techniques to implement the proposed approach. \nThe results obtained from each technique were compared with different performance measures to evaluate them. The \nauthors concluded that overall, from the experiments conducted, method-level refactoring prediction using source code \nmetrics and machine learning classifiers is possible. \n \nPanigrahi et al. [69] designed a recommendation system for predicting refactoring instances at the class level using \nmachine learning techniques, various data sampling approaches to address the data imbalance problem, and feature \nselection techniques. The approach centered around five main research questions: 1) how does data balancing improve \nthe prediction model’s capability? 2) how does feature selection improve the results of the refactoring prediction model? \n3) how do machine learning classifiers predict the refactoring model’s results across different performance parameters? \n4) how can refactoring instances be predicted in the case of cross-project? 5) how can refactoring instances be predicted \nin the case of intra-project? The authors highlighted the importance of using machine learning to refactor prediction \nmodels for large object-oriented software systems. They recommended using ensemble approaches and enhanced \nmachine learning classification algorithms to predict superior performance across different performance parameters. \nThe classifiers recommended for use as standalone or ensemble classifiers include SVM, LSSVM, Naïve Bayes, Random \nForest, ELM-based, K-nearest-neighbors, and deep learning models such as LSTM and CNN. For the approach in this \nstudy, the authors used LSTM and CNN classifiers. The first phase involved calculating software metrics using the \nsource meter tool, while the second phase involved data normalization. Relevant features were selected from all the \nfeatures extracted from the dataset using Principal Component Analysis (PCA), correlation tests, and Wilcoxon rank \ntests. The classifiers were then utilized to predict refactoring instances. To improve the refactoring model’s performance, \nthey implemented ensemble techniques, LSTM, and CNN. Performance was evaluated using different performance \nmetrics such as recall, accuracy, F-measure, and precision. For unbalanced data, the Area Under the Curve (AUC) was \ncomputed. \n \nSagar et al. [79] conducted research to determine whether code metrics can be used to predict refactoring activities in \nthe source code. They approached this by formulating refactoring operation type prediction as a multi-classification \nproblem and implementing both supervised learning and LSTM models. They used the metrics extracted from committed \ncode changes to extract the features that best represent each class and predict the method level refactoring being applied \n( move method, rename method, extract method, inline method, pull up method, and push down method) for any \nproject. For this approach, they developed two types of models - a metric-based model that included traditional machine \nlearning techniques (random forest, SVM, and logistic regression classifiers) and a text-based model with LSTM. The \ntext-based model had an input layer of word embedding metrics and an LSTM layer. The LSTM layer provided the \nfinal dense layer output. For the LSTM layer, they used 128 neurons for the dense layer, and five neurons since there \nwere five different refactoring classes. They used Softmax as an activation function in the dense layer and categorical \ncross-entropy as the loss function. The metric-based model was built with supervised machine learning models to \npredict the refactoring class. The random forest, SVM, and logistic regression were trained with 70% of the data. Initially, \nthe proposed metric model was implemented with only commit messages as input, but the authors realized that this \napproach was insufficient. Therefore, they combined commit messages with code metrics in the second experiment. \nThe model built with LSTM produced 54.3% accuracy. The model built with sixty-four different code metrics dealing \nA Survey of Deep Learning Based Software Refactoring \n27 \n \n \nwith cohesion and coupling characteristics of the code produced 75% accuracy when tested with 30% of data. The study \nshowed that commit messages with little vocabulary are not sufficient for training machine learning models. \nCui et al. [17] have proposed an approach for recommending move method refactoring called Rmove. The proposed \napproach involves automatically learning both structural and semantic representations from code snippets. To achieve \nthis, they first extracted method structural and semantic information from a dataset. Next, they created the structural \nand semantic representation and concatenated them. Finally, they trained a machine learning classifier to guide the \nmovement of the method to a suitable class. The authors used a total of nine classifiers, of which six (Decision Tree, Naïve \nBayes, SVM, Logistic Regression, Random Forest, and Extreme Gradient Boosting) were machine learning-based. Only \nthree were based on deep learning, namely CNN, LSTM, and GRU. The approach demonstrated significant improvement, \nwith an increase of 14%-36% in precision, 19%-45% in recall, and 27%-44% in F-measure compared to the state-of-the-art \ntechniques, such as PathMove [45], JDeodorant [22], and JMove [89]. \nNyamawe et al. [66] proposed a learning-based approach for recommending refactoring types based on the history \nof feature requests, code smells information, and the applied refactorings on the respective commits. The proposed \napproach learned from the training dataset associated with a set of applications. The approach could be used to \nsuggest refactoring types for feature requests associated with other applications or that are associated with the training \napplications. The proposed approach had six main steps. Firstly, the feature requests were extracted from the issue \ntracker JICA and their respective commits were retrieved from a repository on GitHub. Secondly, the previously applied \nrefactorings on the retrieved commits were recovered. Thirdly, RefDiff [84] and RMiner [95] were used to identify the \ncode smells associated with the source code in each of the retrieved commits. Fourthly, text processing was applied to \nthe contents of the file to prepare textual data into a numerical representation for training the classifiers. The fifth step \ninvolved the training of the classifiers which gave the prediction model for predicting and recommending refactorings \nfor new feature requests. Six classifiers were employed, out of which five (SVM, MNB, LR, RF, and DT) were machine \nlearning-based, while only one was deep learning-based (CNN). The study noted that CNN performed slightly lower \nthan the rest of the classifiers, partly because deep learning classifiers generally require significantly larger datasets to \nachieve competitive performance. Nonetheless, the overall evaluation of the approach based on two tasks (the need for \nrefactoring and recommending refactoring types) indicated that the approach attained an accuracy of up to 76.01% and \n83.19%, respectively. \nIn a similar vein to the work done in Nyamawe et al. [66], Nyamawe [65] proposed a machine learning approach \nthat made use of commit messages to improve software refactoring recommendations. This approach identified past \nrefactorings that were applied to commits used for implementing feature requests by analyzing the commit messages. \nThe approach employed six algorithms, including five based on machine learning (SVM, MNB, Random Forest, Logistic \nRegression, and Decision Tree) and one based on deep learning (CNN). To evaluate the approach, a dataset of commit \nmessages from 65 open-source projects was used. The results showed that leveraging commit messages improved \nrefactoring recommendation accuracy significantly compared to the state-of-the-art. \nA study conducted by Mastropaolo et al. [59] explored the potential of data-driven approaches to automate variable \nrenaming. They experimented with three techniques - a statistical language model and two deep learning-based \nmodels. Three datasets were used to train and evaluate the models. The first dataset was used to train the models, \ntune their parameters, and assess their performance. The second and third datasets were used to further evaluate the \nperformance of the techniques. The researchers found that under certain conditions, these techniques can provide \n28 \nNyirongo and Jiang, et al. \n \n \nvaluable recommendations and can be integrated into rename refactoring tools. The three representative techniques \nused were a statistical model, an n-gram cached language model proposed by Hellendoorn [36], T5 proposed by Raffel \net al. [77], and a transformer-based model presented by Liu et al. [50]. The study demonstrated that deep learning \nmodels, particularly those that generate predictions with high confidence, can be valuable support for variable rename \nrefactoring. \n \nAlomar et al. [4] developed a tool called AntiCopyPaster, which is a plugin for IntelliJ IDEA. The tool aims to provide \nrecommendations for extract method refactoring opportunities as soon as duplicate code is introduced in the opened \nfile in the IDE. The tool takes into account various semantic and syntactic code metrics as input and makes a binary \ndecision on whether the code fragment should be extracted or not. The goal of this approach is to increase the adoption \nof extract method refactoring while maintaining the workflow of the developer. To achieve this goal, Alomar et al. in [5] \ninvestigated the effectiveness of machine learning and deep learning algorithms. They defined the detection of extract \nmethod refactoring as a binary classification problem. Their proposed approach relied on mining prior applied extract \nmethod refactorings and extracting their features to train a deep learning classifier that detected them in the user’s code. \nThe approach was structured into four phases: data collection, refactoring detection, code metrics selection, and tool \ndesign and evaluation. The deep learning model used in the approach was CNN. The CNN comprised multiple layers \nof fully connected nodes, structured into convolutional, deconvolutional, and dense layers. A dropout stage was also \nincluded to prevent overfitting. The input to the CNN was a vector of 78 metric values which were batch-normalized \nto stabilize their distribution. The batch normalized inputs were then fed into a convolution that reduced the feature \nspace from 78 to 32. ReLU was used as the activation function for the convolutional layers. The convoluted data was \nthen fed into the deconvolutional layer, which was followed by a max pooling layer with a filter size of 2 that took the \nlargest number in the filter. The final layer was the dense layer, in which each node received input from all nodes of the \nprevious layer. The approach was implemented as a plugin in IntelliJ IDEA, which is a popular IDE for Java. The plugin \nconsists of four main components: Duplicate detector, Code analyzer, Method extractor, and Refactoring launcher. \nThe results showed that CNN recommended the appropriate extract method refactorings with an F-measure of 0.82. \nThese results solidify that machine learning models can recommend extract method refactorings while maintaining the \nworkflow of the developer. \n \nCui et al. [16] have proposed an automated approach called Representation-based Extract Method Refactoring Recom- \nmender System (REMS) to suggest appropriate extract method refactoring opportunities. The approach involves mining \nmulti-view representations from a code property graph. First, code property graphs were extracted from training and \ntesting samples. Then, multi-view representations such as tree-view and flow-view representations were generated from \nthe code property graph. Compact bilinear pooling was used to fuse the tree-view and the flow-view representations. \nFinally, machine learning classifiers were trained to guide the extraction of suitable lines of code as a new method. Six \nrelevant embedding techniques such as CodeBERT, GraphCodeBERT, CodeGPT, CodeT5, PLBART, and CoTexT were \nused to generate various representations of the abstract syntax tree, which were referred to as tree-view representations. \nThe researchers explored the impact of these representations on recommendation performance. The REMS operates in \nthree phases, including 1) feature extraction from code property graphs of training and testing samples, 2) model training \nbased on machine learning techniques, and 3) applicable analysis of behavior preservation and functional usefulness. \nSeven traditional machine learning models such as Decision Tree, K-nearest neighbor, Logistic Regression, Naïve Bayes, \nRandom Forest, Support Vector Machine, and Extreme Gradient Boosting, were used. CNN and LSTM were the only \nA Survey of Deep Learning Based Software Refactoring \n29 \n \n \ndeep-learning techniques employed.The results showed that the REMS approach outperformed five state-of-the-art \nrefactoring tools, including GEMS [101], JExtract [83], SEMI [14], JDeodorant [22], and Segmentation [90]. \nPantiuchina [71] has developed techniques to create a new generation of refactoring recommenders. These recom- \nmenders can predict code components that are likely to be affected by code smells in the near future and recommend \nmeaningful refactorings that emulate the ones that developers would perform. They refer to this approach as just-in-time \nrational refactoring, which has two main goals. First, predicting code quality decay aims to develop techniques that \nalert the developer when a code component is deviating from good design principles before design flaws are introduced. \nSecond, learning refactoring transformations investigates the possibility of applying deep learning models to learn code \nchanges performed by software developers. The researchers plan to investigate if neural machine translation models \ncan be used for the replication of refactoring operations performed by software developers. \nPinheiro et al. [74] investigated how trivial class-level refactorings could affect the prediction of non-trivial refactorings \nusing machine learning techniques. They selected 884 open-source projects and extracted the type of refactoring \nfrom classes involved in some operation and code metrics. The researchers grouped the refactorings into trivial and \nnon-trivial ones based on their level of change. Trivial refactorings included adding class annotations, changing access \nmodifiers, removing class annotations, etc. Non-trivial refactorings included extract class, move class, merge class, etc. \nAdditionally, they proposed contexts based on combinations of the refactoring types that made it possible to increase \nthe accuracy of supervised learning models. They used four traditional machine learning models, Decision Tree, Logistic \nRegression, Naïve Bayes, and Random Forest. They employed a Neural Network as the only deep-learning technique for \nthis approach. They followed a sequence of five steps: selection of software projects, refactoring, and feature mining, \nselection of contexts, training and testing of the machine learning-based models, and evaluation of the results. The \nselection of contexts in step number three had to do with creating several datasets with different combinations of \nrefactoring types. The datasets constructed by the combinations of C1, C2, and C3 were used to predict the refactorings. \nThe four machine learning models were used through the Scikit-learn library, while the Neural Network was used \nthrough Tensorflow Keras. After training, each generated model was validated by predicting the refactorings of the \nfeatures in the test set. The main findings of this approach were: 1) machine learning with tree-based models, such \nas Random Forest and Decision Tree, performed very well when trained with code metrics to detect refactorings, 2) \nseparating trivial and non-trivial refactorings into different classes resulted in a more efficient model, indicative to \nimprove the accuracy of machine learning-based automated solutions, and 3) using balancing techniques that increase \nor decrease samples randomly is not the best strategy to improve datasets composed of code metrics. \nPanigrahi et al. [70] developed a refactoring prediction model using an ensemble-based approach. They identified the \noptimal set of code metrics and their association with refactoring proneness by analyzing the structural artifacts of \nthe software program. This approach involved refactoring data preparation, feature extraction, multiphased feature \nextraction, sampling, and heterogeneous ensemble structure refactoring prediction. The proposed model extracted 125 \nsoftware metrics from object-oriented software systems using a robust multi-phased feature selection method, which \nincluded Wilcoxon significant text, Pearson correlation test, and Principal Component Analysis (PCA). The optimal \nfeatures characterizing inheritance, size, coupling, cohesion, and complexity were retained. A novel heterogeneous \nensemble classifier was developed using techniques such as ANN-Gradient Descent, ANN-Levenberg Marquardt, ANN- \nGDX, ANN-Radial Basis Function support vector machine, LSSVM-Linear, LSSVM-Polynomial, LSSVM-RBF, Decision \nTree algorithm, Logistic Regression algorithm, and Extreme Learning Machine (ELM) model as the base classifiers. The \n30 \nNyirongo and Jiang, et al. \n \n \nresults indicated that the Maximum Voting Ensemble (MVE) achieved better accuracy, recall, precision, and F-measure \nvalues (99.76, 99.93, 98.96, 98.44) compared to the Base Trained Ensemble (BTE). Additionally, it experienced fewer \nerrors (MAE = 0.0057, MORE = 0.0701, RMSE = 0.0068, and SEM = 0.0107) during the implementation to develop \nthe refactoring model. The experimental results recommended that MVE with up-sampling could be implemented to \nimprove the performance of the refactoring prediction model at the class level. \n5.1.2 Code Smell Based Technologies. Apart from proposing a deep learning-based approach to identify feature envy \nsmells, one of the most common code smells, Liu et al. [53], also used their approach described in Section 4 to recommend \nmove method refactorings. For methods that were predicted to be smelly (with feature envy), they suggested that such \nmethods should be moved via move method refactorings. If only one (noted as inputj) of the testing items generated \nfor a method m was predicted as positive, they suggested moving m to the target class tcj that was associated with \nthe positive testing item inputj. If more than one testing items were predicted as positive, they selected one (noted as \ninputi) with the greatest output and suggested moving method m to class tci that associated with inputi. Although their \nneural network described in Section 4 was a binary classifier the output of the neural network was a decimal varying \nfrom zero to one. The neural network interpreted the prediction as positive if and only if the output was greater than \n0.5. [15]. Their results indicated that the approach was accurate in recommending destinations for the smelly methods. \nThe approach achieved, on average, an accuracy of 74.94%. They also observed that the approach was more accurate \nthan the state-of-the-art tool in the recommendation of move method refactorings, i.e., JMove [89], JDeodorant [22]. \nThis study was extended in Liu et al. [51] where in addition to using deep learning to detect code smells they also \nexplored the recommendation of refactoring solutions for feature envy and misplaced class. The recommendation \nof refactoring solutions for feature envy was the same as in Liu et al. [53]. The CNN used for misplaced class was \nthe same as described in Section 4. To decide whether a given class should be moved from its enclosing package to \nanother package, they leveraged two categories of features code metrics and textual features. The used code metrics \nincluded coupling between objects and message-passing coupling. To evaluate the approach they compared their \nproposed approach to TACO [68] in recommendation of target packages for misplaced classes. The accuracy of the \nrecommendation was critical because if misplaced classes are moved to the wrong positions they remain misplaced. \nThe approach resulted in a greater number of accepted recommendations. In total, 488 of its recommendations were \naccepted whereas the number was reduced to 342 for TACO [68]. Secondly, TACO [68] was more accurate than the \nproposed approach in recommending target packages. It improved the average accuracy from 49.80 to 62.98 percent. \nHowever, on the same code smells where both the proposed approach and TACO [68] made recommendations, their \naccuracy in recommending target packages was close to each other 62.6% for TACO [68] and 60.05% for the proposed \napproach. Based on this analysis they concluded that the proposed approach outperformed the baseline in identifying \nmisplaced classes and it could be comparable to the baseline in recommending target packages. \nMa et al. [55] in their pursuit of using pre-trained model CodeT5 to extract the semantic relationship between code \nsnippets to detect feature envy code smell, also explored the effectiveness of their approach in recommending refactoring \nsolutions for the code smell. They wanted to find out if their approach could exceed the state-of-the-art approaches in \nrecommending destinations for the methods to be moved. In their approach, for methods that were predicted as smelly \n(with feature envy, inputec was predicted positive), they suggested where such a method should be moved via move \nmethod refactoring. Then, they fed all testing items inputptci=<code(m),code(ptci)> into the neural network. If only one \n(noted as inputptci) of the testing items generated for m is predicted as positive, then they suggested moving m to the \npotential target class (ptcj) that is associated with the positive testing item inputptcj. If more than one testing item is \nA Survey of Deep Learning Based Software Refactoring \n31 \n \n \npredicted as positive, they selected the one (noted as inputptci) with the greatest output and suggested moving method \nm to class ptci that is associated inputptci. The neural network interpreted the prediction as positive if and only if the \noutput was greater than 0.5. This approach was compared to Liu et al.’s approach [51]. The results indicated that their \napproach improved the accuracy in recommending target classes as it attained an accuracy rate of greater than 90% \nwhile Liu et al.’ was below 90%. \nThe Bi-LSTM with a self-attention mechanism that was proposed by Wang et al. [98] to detect feature envy code smell \nwas also used to recommend refactoring destinations for the methods to be moved. For a method m that had been \nmarked as positive, they predicted its refactoring destination as follows. If there is only one positive example, they \nregard the target class related to this example as the destination. If there was more than one, they chose the target \nclass related to the highest probability in the output set. The rationale behind this was that higher probability meant \nhigher confidence in the deep neural network obtained. This solution was presented as destination=Cmax where Cmax \nrelated to Pmax=Maxp1,p1,. ,pk. This functional mapping of input to output was presented and computed by the deep \nlearning model. This approach was compared to JMove [89], JDeodorant [22], and Liu et al.’s [53] approach. The results \nindicated that their approach was more accurate on destination recommendation than the state-of-the-art. \nYu et al. [105] did not only solve the problem of inherent calling relationships between methods which usually cause \nunimpressive detection efficiency by proposing a Graph Neural Network (GNN) based approach towards feature envy \ndetection but also utilized the strength of the calling relationship of one method to another to recommend refactoring \nsolutions for the feature envy code smell. For the methods that were predicted to have a smell, they provided refactoring \nsuggestions to move these methods to the classes that best fit their functional implementation. If a smelly method \naccessed only one external class, they recommended moving it to that external class. If the smelly method was interested \nin two or more external classes, they employed an algorithm to suggest the most suitable external class. For this \nalgorithm, they regarded the calling strength as the weight of the edge and obtained the calling strength graph. The \ncalling strength graph was constructed as G2=V, E, W where V represented the set of nodes, E represented the set of edges \nand W represented the weight matrix of edges. To validate their approach, they compared it against, JDeodorant [22], \nJMove [89], and Liu et al.’s [51] approach. The results indicated that their approach had higher accuracy than the three \nbenchmarks in refactoring recommendations. Specifically, compared with Liu et al.’s [51] work, JDeodorant [22], and \nJMove [89], it improved the accuracy by 10.10%, 5.13% and 11.00% respectively. \nLiu et al. [54] proposed an automated approach to detecting and improving inconsistent method names. In addition to \nidentifying inconsistent names, their approach provided a list of ranked suggestions for new names for a given method. \nThe ranked list of similar names was generated using four ranking strategies. The first strategy (R1) relied solely on \nthe similarities between method bodies, ranking the names of similar method bodies according to their similarity to \nthe given method body. The second strategy (R2) grouped identical names, ranked distinct names based on the size of \nthe associated groups, and broke ties based on the similarities between method bodies as per R1. The third strategy \n(R3) was similar to R2, but it ranked groups based on the average similarity, regardless of the group size. To avoid \nhighly-ranked but small groups, the fourth strategy (R4) re-ranked all groups produced in R3, downgrading all 1-size \ngroups to the lowest position. To evaluate the performance of their approach in suggesting new names for inconsistent \nnames, the suggested names were ranked using the aforementioned strategies. To ensure a comprehensive assessment, \nthree different scenarios were considered: inconsistency avoidance, first token accuracy, and full name accuracy. The \napproach achieved an accuracy of 34-50% in suggesting subtokens and 16-25% accuracy in suggesting full names. \n32 \nNyirongo and Jiang, et al. \n \n \nLiu et al. [21] conducted a study to enhance the deep learning-based feature envy detection approaches by providing \nreal-world examples. They used their approach to identify feature envy methods that should be moved from their \nenclosing classes to other classes that they envy. They achieved this by using a heuristic-based filtering method, as \noutlined in Section 4. Their approach, feTruth, utilized a trained classifier and a sequence of heuristic rules to predict \nwhether a given method in the testing project was associated with feature envy smell. If the method was associated \nwith feature envy smell, feTruth would suggest the class to which the method should be moved. The accuracy of feTruth \nwas compared against other approaches, namely JDeodorant [22], JMove [89], and Liu et al.’s [51] approach. The results \nshowed that feTruth was accurate in suggesting destination classes for feature envy methods with an accuracy of 93.1%. \nThis was higher than JDeodorant’s accuracy of 80% and comparable to Liu et al.’s [51] and JMove’s accuracy of 87.5% \nand 100%, respectively. \n5.2 Refactoring Types \nFrom the primary studies presented in this section, we note that various refactoring solutions were recommended as \na way of addressing issues related to a particular codebase. When recommending a refactoring solution, a particular \nrefactoring that could be applied is suggested to resolve the identified issue. Refactorings are techniques that are applied \nto the codebase to improve its quality without altering its external behavior. Refactorings are usually applied at different \nlevels of the codebase namely class, method, and variable. Refactorings applied at the class level are usually aimed \nat enhancing the overall structure, maintainability, and readability of the codebase by making changes at the class \nlevel. Examples of these include extract class, collapse hierarchy, rename class, etc. Method-level refactorings involve \nmodifying the internal structure of methods to enhance readability, maintainability, and performance without altering \nthe external behavior of the code. The goal of these is to create cleaner, more efficient, and easier-to-understand methods. \nExamples include extract method, rename method, move method, inline method, etc. Variable-level refactorings involve \nmaking changes to the variables within the codebase to improve the quality, readability, and maintainability without \naltering the external behavior of the program. Examples of these include rename variable, extract variable, inline \nvariable, encapsulate field, etc. \nTable 4 presents the level of the refactoring solutions that were recommended by the primary studies in our survey \nplus the representative refactorings suggested at that level. Analyzing the presented data, it becomes apparent that a \nmajority of the recommended refactoring solutions focus on the method level, with 58.06% of researchers proposing \nsolutions at this granularity. Class-level refactoring follows at 25.81%, and variable-level solutions are suggested in \n16.13% of the primary studies. Through this data, we also observed that under the method level refactoring, the most \ncommon refactoring types being suggested were the extract method and the move method. We noted that extract \nmethod refactoring was found in 62.50% of the studies that employed refactoring based recommendation approach \nwhile move method was found in 85.71% of the studies that employed code smell-based approach to recommendation \nrefactoring. \n \n \n5.3 Training Strategies \nAs previously mentioned, training strategies play a crucial role in developing a robust and accurate deep-learning \nmodel. In this section, we will discuss the technicalities, tools, and procedures that researchers have used to build deep \nlearning models that are effective in recommending refactoring solutions. Researchers have adopted various procedures \nA Survey of Deep Learning Based Software Refactoring \n33 \n \n \nTable 4. Representative refactoring types. \n \nLevel \nRefactorings \nReferences \nClass \nExtract class, Move class, Rename class \n[3, 9, 65, 66, 69–71, 74] \nMethod Extract method, Move method, Rename method [4, 6, 9, 16, 17, 21, 44, 51, 53–55, 65, 66, 71, 71, 79, 98, 105] \nVariable Extract variable,Rename variable, Move attribute \n[9, 59, 65, 66, 71] \n \nrelated to data preprocessing, feature extraction, and data balancing to ensure that they build a deep learning model \nthat is trained with high-quality data for accurate and efficient recommendations of refactoring solutions. \n5.3.1 Data Preprocessing. According to our survey, researchers have employed various processes such as tokenization, \nlemmatization, stop word removal, noise removal, and normalization to ensure that their data is well-preprocessed and \ncleaned. Tokenization breaks texts into words, phrases, symbols, or other meaningful elements called tokens. This is \nused to split text into constituent sets of words. Lemmatization replaces the suffix of a word or removes it to obtain \nthe basic word form. It is used for part of speech identification, sentence separation, and key phrase extraction. The \ngoal of lemmatization is to group different inflected forms of a word so that they can be analyzed as a single item. \nStop word removal involves filtering out common words that are considered to be of little value in understanding the \nmeaning of a text. Noise removal refers to the process of reducing or eliminating irrelevant or unwanted information, \noften referred to as \"noise,\" from a dataset. The goal of noise removal is to improve the quality of the data or signal for \nmore accurate analysis or interpretation. For instance, Sagar et al. [79] had to remove and clean HTML tags since their \ndata came from the web. Sagar et al. [79] also checked for special characters, numbers, and punctuation to remove any \nnoise. Normalization refers to the process of transforming data into a standard scale. The goal is to bring the values of \ndifferent variables or features into a comparable range, preventing one variable from dominating the analysis simply \nbecause of its larger scale. In this process, textual data may be converted to the standard required case, either lowercase, \nuppercase, camel case, etc. We also note through our literature search that other researchers, for example, Alenezi et \nal. [3], employed the process of basic data cleaning by first deleting all unnecessary features from their dataset, then \nfinalizing the process by deleting refactoring type features and replacing the summation of refactoring feature. \n5.3.2 \nFeature Extraction. Various types of features can be extracted to enable a deep learning model to recommend \nappropriate refactoring solutions. These features can include semantic, structural, code metrics, commit messages, or \ndocumentation. Most studies extract features from source code, which includes different metrics depending on the \nrefactoring solution they want to suggest. However, some researchers, such as Aniche et al. [9], used process and \nownership metrics instead of merely employing source code features. For the process metrics, Aniche et al. [9] collected \nfive different types of metrics: the number of commits, the sum of lines added and removed, the number of bug fixes, and \nthe number of previous refactoring operations. They calculated the number of bug fixes by using a heuristic whenever \nany of the keywords \"bug, error, mistake, fault, wrong, fail, and fix\" appeared in the commit message, and counted one \nor more bug fixes to that class. The number of previous refactoring operations was calculated based on the refactorings \nthey gathered from the refactoring mining tool. For the code ownership metrics, Aniche et al. [9] adopted the suite \nownership metrics as proposed by Bird et al. [11]. The number of authors was the total number of developers who had \ncontributed to the given software artifact. The minor authors were the number of contributors who had authored less \nthan 5% (in terms of commits) of an artifact. The major authors were the number of developers who contributed at \n34 \nNyirongo and Jiang, et al. \n \n \nleast 5% to an artifact. With this, ownership was calculated as the proportion of commits achieved by the most active \ndeveloper. \n5.3.3 \nFeature Embedding. The primary studies have employed various tools and techniques for feature embedding. \nEmbedding is a type of vectorization that represents words as dense vectors in a continuous space which captures \nsemantic relationships between them. For instance, Cui et al. [16, 17] used code and graph embedding techniques to \ngenerate corresponding structural and semantic representations. They also used them to create hybrid representations. \nFor code embedding, they used Code2vec [8] and Code2Seq [7]. Code2Vec is a neural network that automatically \ngenerates vectors from source code, while Code2Seq is a neural network that produces sequences from code snippets. \nFor graph embedding techniques, they explored the use of Deepwalk [72], Node2Vec [30], Walklets [73], GraRep [13], \nLine [88], ProNE [106], and SDNE [97]. DeepWalk and Node2Vec employ random walk to construct sample neighbor- \nhoods for nodes in a graph based on a Skip-gram Natural Language Processing (NLP) model. The goal of Skip-gram is to \nmaximize the likelihood of words appearing in a sliding window co-occurring. Walklets is another random walk-based \ngraph embedding technique that explicitly encodes multi-scale relationships between nodes to produce multi-scale \nrepresentations for them. GraRep is a matrix factorization-based graph embedding technique that constructs matrices \nfrom connections between nodes and factorizes them to produce the embedding result. Line calculates graph embedding \nresults by specifying two functions, one for the first-order node proximity and the other for the second-order node prox- \nimity. ProNE is a fast and scalable graph embedding technique that was recently introduced. It includes two steps, the \nfirst is to effectively initialize graph embedding results by phrasing the problem as sparse matrix factorization, motivated \nby the long-tailed distribution of most graphs and their sparsity. The second stage is to propagate the initial embedding \nresult using the higher-order Cheeger’s inequality [46], aiming at capturing the graph’s localized clustering information. \nSDNE employs deep autoencoders to generate embedding results. Other techniques also emerged from the researchers \nin the primary studies, for example, the employment of word embedding technologies, e.g., Word2Vec [51, 53, 79, 98], \nvector space models [65, 66], and CodeT5 [105] to achieve embedding. \n5.3.4 Data Balancing. Researchers often use various techniques to address the issue of data imbalance for recommending \nrefactoring solutions. These techniques include the Synthetic Oversampling Technique (SMOTE) and its variants \n(BLSMOTE, SVSMOTE, GraphSMOTE, etc.), UpSample, RUSBoost, Down sampling, and Random sampling. SMOTE \ntechnique is based on the oversampling approach in which synthetic examples are used for oversampling the minority \nclass rather than oversampling with replacement. UpSample is used to improve the number of samples of the minority \nclass by inserting zeros between the samples. RUSBoost is a hybrid approach of data sampling and boosting algorithm \nused to improve the performance of models trained on skewed data. To reduce the bias that may arise due to the use of \nimbalanced datasets, data balancing techniques are usually employed to create a balanced dataset. Most researchers \nemploy the use of sampling techniques to achieve data balance in their datasets. In our survey, 82.00% of primary \nstudies were found to employ sampling techniques in their variant forms. However, Pinheiro et al. [74] concluded that \nusing balancing techniques that increase or decrease samples randomly is not the best strategy for improving datasets \ncomposed of code metrics. \n \n5.4 Evaluation \n5.4.1 \nDatasets. Different types of datasets are utilized to suggest accurate refactoring solutions through deep learning \nmodels. These datasets are carefully chosen to ensure that the deep learning model receives the appropriate data for \nmaking the right recommendations. Based on our survey, we found that researchers typically clone or download the \nA Survey of Deep Learning Based Software Refactoring \n35 \n \n \nsubject projects from repositories and extract data relevant to their study refactorings. RefactoringMiner [93] was found \nto be the most popular tool for refactoring data mining tasks. RefactoringMiner was used by at least 91.30% of the \nprimary studies. As shown in Figure 7, 86.96% of the researchers used publicly available projects, while only 13.04% \nused private projects with restricted access. Interestingly, 100% of the studies in this category the applications that \nwere used for the experiments and evaluation were developed in the Java language. We also came across the work \nof Mastropaolo et al. [59], who created three datasets to train and evaluate their deep learning model. They built a \nlarge-scale dataset for training the model, tuning parameters, and performing an initial assessment of performance. \nAdditionally, they created reviewed and developers datasets to further evaluate the performance of their technique. \n \n \nPublic  \n \n86.96% \n \n \n \n13.04% \n \n \n \n \nPrivate \n \n \n \nFig. 7. Datasets \n \n5.4.2 \nResult Metrics. As previously discussed, result metrics are used to measure the effectiveness of a particular \napproach. The standard metrics for this calculation are precision, recall, F-measure, and accuracy. Our literature \nsearch findings on the metrics used by researchers in this category are shown in Figure 8. According to Figure 8, F- \nmeasure, recall, and precision are the most commonly used metrics for evaluating various approaches in recommending \nrefactoring solutions. F-measure was used in at least 95.65% of the primary works, while precision and recall were \nemployed in 95.65% and 86.95% of the studies, respectively. \n36 \nNyirongo and Jiang, et al. \n \n \n20 \n \n15 \n \n10 \n \n5 \n \n0 \nPrecision   Recall    F1-score Accuracy    AUC \nMCC \nPerformance assessment \n \nFig. 8. Metrics used for performance assessment. \n \n \n6 END-TO-END CODE TRANSFORMATION AS REFACTORING \nThe end-to-end code transformation as refactoring refers to the actual process of modifying an existing codebase \nto improve its structure, readability, maintainability, or performance without changing its external behavior. The \nend-to-end code transformation of refactorings differs from the other refactoring tasks (i.e., the detection of code \nsmells, the recommendation of refactoring solutions) in that it involves the application of the suggested refactoring \nto the codebase, reviewing the refactoring code to ensure it adheres to coding standards, validating that the external \nbehavior of the codebase remains unchanged, and updating any documentation to reflect the changes made during the \nrefactoring, such as comments, inline documentation, and external documentation if necessary. This process focuses on \nactively making changes to the codebase which can include the actual renaming of variables, the moving of methods, \nextracting methods, simplifying complex expressions, restructuring code, and so on. In this section, we will explore \nhow researchers have utilized deep learning models to conduct end-to-end code transformations as refactorings. Our \nspecific focus will be on the technologies utilized for conducting the refactoring code transformations. \nSzalontai et al. [87] have developed a method using deep learning to refactor source code, which was initially developed \nfor the general-purpose programming language and runtime environment, Erlang. This approach has two main \ncomponents: a localizer and a refactoring component. Together, they enable the localization and refactoring of non- \nidiomatic code patterns into their idiomatic counterparts. The method processes the source code as a sequence of \ntokens, making it capable of transforming even incomplete or non-compilable code. To do this, the source code is \ntransformed into a sequence of tokens, using the Erlang module tok to tokenize the source code. The module obtains \ntoken types such as atom, integer, variable, etc. These tokens are then provided as input into the neural network to \nlocalize non-idiomatic functions. The neural network consists of convolutional, recurrent, and feedforward components. \nThe tokens provided as input are embedded into a 64-dimensional vector space, and then a one-dimensional convolution \nis applied to each code chunk using 126=8 filters and a kernel size of 5. Two pooling operators are applied to the \nconvolutional outputs, average and minimax. These two operations yield two intermediate representations for each \nNumber of papers which used the metric \nA Survey of Deep Learning Based Software Refactoring \n37 \n \n \nchunk of the source code. The idiomatic alternative is generated using a recurrent sequence-to-sequence architecture \nwith an attention mechanism. The non-idiomatic tokenized code is first fed to the encoder, which produces a hidden \nrepresentation of the input sequence. This is achieved through the use of a Recurrent Neural Network consisting of four \nBiLSTM layers with 64 units each. The decoder uses a single LSTM layer with 256 units to generate an output sequence \nelement by element, producing the idiomatic alternative. Both the localizer and the refactoring models were evaluated \non a test set that was separated from the training data before the training process. The accuracy of the localizer was \nmeasured as the ratio of classified code chunks to the total number of chunks in the test set and was found to be \n99.09%. For the refactoring component, the ratio of error-free transformations against the total number of attempted \ntransformations was measured resulting in an accuracy of 99.46%. These results indicate that the presented models were \ntrained successfully and are capable of performing refactorings that are similar to the ones in the training datasets. \n \nTufano et al. [96] quantitatively investigated the ability of a Neural Machine Translation (NMT) model to learn \nhow to automatically apply code changes implemented by developers during pull requests. They harnessed NMT to \nautomatically translate a code component from its state before the implementation of the pull request (pre-PR) and \nafter the pull request has been merged (post-PR), thereby emulating the code changes that would be implemented by \ndevelopers in the pull request. This is the first work that used deep learning techniques to learn and create a taxonomy \nfrom a variety of code transformations taken from the developer’s pull requests. In this investigation, they first mined a \ndataset of complete and meaningful code changes performed by developers in merged pull requests, extracted from \nthree Gerrit repositories (Android, Google, and Ovirt). Then they trained the NMT models to translate pre-PR code into \npost-PR code, effectively learning code transformations as performed by developers. RNN Encoder-Decoder and Beam \nSearch Decoding were used as NMT models for this approach. The RNN Encoder-Decoder architecture was coupled with \nan attention mechanism which is commonly adopted in NMT tasks. The RNN encoder was used for encoding a sequence \nof tokens x into vector representation while the RNN Decoder was used for decoding the representation into another \nsequence of tokens y. The primary purpose of the employed beam search decoder was to improve the quality of the \ngenerated token sequences by exploring multiple possible paths instead of simply selecting the most likely next token \nat each step. The NMT model was able to predict and learn from some transformations. This was used to develop a \ntaxonomy of the transformations with three subcategories grouping the code transformation into bug fixing, refactoring, \nand other. The refactoring subtree included all code transformations that modified the internal structure of the system \nby improving one or more of its non-functional attributes without changing the system’s external behavior. Under this \nsubtree five subcategories were formulated, namely, inheritance (forbid method overriding by adding the final keyword \nto the method declaration, invoke overriding method instead of overridden by removing the super keyword to the \nmethod invocation and making a method abstract through the abstract keyword and deleting the method body), methods \ninteraction (add parameter refactoring (i.e., a value previously computed in the method body is now passed as parameter \nto it), and broadening the return type of a method by using the Java wildcard (?) symbol), readability (braces added to if \nstatements with the only goal of clearly delimiting their scope, the merging of two statements defining and initializing \na variable into a single statement, the addition/removal of the this qualifier, to match the project’s coding standards, \nreducing the verbosity of a generic declaration by using the Java diamond operator, refactoring anonymous classes \nimplementing one method to lambda expressions, to make the code more readable, simplifying Boolean expressions, \nand merging two catch blocks capturing different exceptions into one catch block capturing both exceptions using \nthe or operator), naming (renaming of methods, parameters, and variables), and encapsulation (modifying the access \nmodifiers, e.g., changing a public method to a private one). The results showed that NMT models are capable of learning \n38 \nNyirongo and Jiang, et al. \n \n \ncode changes and perfectly predict code transformations in up to 21% of the cases when only a single translation is \ngenerated and up to 32% when 10 possible guesses are generated. These results highlight the ability of the models to \nlearn from a heterogeneous set of pull requests belonging to different datasets, indicating the possibility of transfer \nlearning access projects and domains. \nTo facilitate the rename refactoring process and reduce the cognitive load of developers, Liu et al. [52] proposed a \ntwo-stage pre-trained framework called RefBERT. This framework is based on the BERT architecture and was designed \nto automatically suggest a meaningful variable name, which is considered a challenging task. The researchers focused on \nrefactoring variable names, which is more complex than refactoring other types of identifiers, such as method names and \ntype names. RefBERT uses 12 RoBERTa layers, which are a replicated version of the original BERT model with improved \nperformance. The approach is based on three observations. First, rename refactoring is similar to Masked Language \nModelling (MLM), a pretext task commonly used in pre-training BERT. MLM fills the masked part of a text according \nto its context. Similarly, rename refactoring aims to suggest a meaningful variable name according to the context. \nTherefore, MLM can be adopted for training an automatic rename refactoring model. Second, unlike the variable name \nprediction task, where only the context of the target variable is known, in rename refactoring, both the context of the \ntarget variable and the variable name before refactoring are known. Contrastive learning, which contrasts positive and \nnegative samples for improving representation learning, is an ideal learning paradigm for automatic rename refactoring. \nThe researchers expected the generated name to be close to the variable name after refactoring but far away from the \nvariable name before refactoring. Third, unlike natural language text where words should follow a strict order to ensure \ngrammatical correctness, subtokens in a variable name do not have such a restriction. Different orders of subtokens \nfor a variable name do not significantly affect our understanding of the variable. Thus, the standard cross-entropy \nloss that emphasizes the strict alignment between the prediction and the target is suboptimal for automatic rename \nrefactoring. RefBERT was trained to generate refactorings in two steps: Length Prediction (LP), where it predicts the \nnumber of tokens in the refactored variable name, and Token Generation (TG), where given the predicted number \nof tokens, RefBERT generates tokens in the refactored variable name. To train RefBERT, the researchers used the \nCodeSearchnet [40] and Java-Small [7] datasets in the pretraining stage. During the fine-tuning stage, they also used \nJavaRef and TL-Codesum [38] datasets. JavaRef was constructed by the researchers by applying data collection and \npreprocessing procedures on open-source datasets collected from GitHub. The experimental results demonstrated the \neffectiveness of RefBERT in automatic rename refactoring. \nFrom the literature presented, we note from Table 5, that Recurrent Neural Networks (RNN) through its variants (LSTM \nand GRU) have mostly been used for the end-to-end transformation as refactorings. The usage of RNN was found in at \nleast 75% of the studies. This was followed by Transformer technologies (e.g., BERT), which were utilized in 25% of the \nstudies. Notably, to enhance the performance of the proposed techniques the researchers adopted the inclusion of other \ntechniques in their approaches. Szantotai et al. [87] and Tufano et al. [96] used an attention mechanism to improve the \nmodel’s ability to focus on relevant parts of the input sequence when generating the output sequence. In contrast, Liu \net al. [52] used contrastive learning to contrast positive and negative samples for improving representation learning, \nand was used for automatic rename refactoring. The inclusion of contrastive learning helped the model to understand \nthe context of the target variable and the variable name before refactoring. \nFrom the presented studies, we note that researchers have employed various deep-learning techniques to perform \nthe code transformation for different types of refactorings. Szalontai et al. focused on using a deep learning model \nA Survey of Deep Learning Based Software Refactoring \n39 \n \n \nTable 5. Technologies used for the end-to-end refactorings transformation \n \nTechnologies \nRefactorings \nDeployment tools/plaforms \nReferences \nLSTM+GRU+attention mechanism \nNon idiomatic components \nErlang \n[87] \nNMT(RNN+Beam search)+attention mechanism \nNon functional attributes \nJava \n[96] \nBERT(12RoBERTA)+contrastive learning \nRename refactoring \nRefBERT-Java \n[52] \n \nto refactor nonidiomatic code patterns into idiomatic ones across various levels of code organization such as class, \nmethod, and variable. Typically, the choice of the appropriate level depends on the specific issues identified in the \ncodebase, with refactorings like extract class, extract method, and rename variable being associated with these patterns. \nSzalontai et al.’s approach primarily targeted the general-purpose programming language Erlang. Conversely, Liu et \nal.’s [52] and Tufano et al. [96] approaches specifically targeted the renaming refactoring for variables and refactoring \nof non-function attributes, respectively, using the Java language. Thus, from Table 5, we observe that researchers are \nemploying a specific approach to the conduction of end-to-end transformation of refactorings as found in 66.67% of \nthe studies i.e., variable renaming and non-function attribute refactoring. In contrast, 33.33% of the studies opted for a \ngeneral approach in the conduction of the refactoring transformation where the changes could be used at different \nlevels (i.e., class, method, variable). \n \n \n7 MINING OF REFACTORINGS \nMining of refactorings refers to the automatic process of identifying and extracting instances of refactorings from \nexisting codebases. To conduct the mining of refactorings for deep learning-based refactoring, traditional refactoring \nminers are used. The refactoring miners utilize various techniques such as static analysis, pattern recognition, and \nheuristic-based methods to identify and discover refactoring activities that were carried out within codebases. The \noutputs generated by these miners might consist of labeled examples that indicate where and how refactorings have \nbeen applied. These labeled examples serve as ground truth data, forming the foundation for training and evaluating \ndeep learning models. By leveraging the outputs of traditional refactoring miners, large datasets of labeled refactorings, \nenabling the development of accurate and robust deep-learning models capable of automating software refactoring \nprocesses can be created. This integration of mining approaches with advanced deep-learning methodologies accelerates \nthe advancement of intelligent tools aimed at enhancing code quality and maintainability. \nSeveral traditional refactoring miners have been proposed by researchers to aid in the mining of refactorings. Tsantalis \nproposed RefatoringMiner [93] which represents the implementation of software entities as abstract syntax trees (ASTs), \nand computes the similarity between two entities according to the name-based similarity and the statement-based \nsimilarity. With such similarities, RefactoringMiner maps entities between two successive versions and leverages a \nsequence of heuristics to discover software refactorings based on the mapping. RefactoringCrawler developed by Dig et \nal. [20], is an analysis tool that detects refactorings that happened between two versions of a component. The strength \nof the tool lies in the combination of a fast syntactic analysis to detect refactoring candidates, and a more expensive \nsemantic analysis to refine these candidates. Silva et al. [85] proposed RefDiff which utilizes static analysis and code \nsimilarity to detect various refactorings. It begins by tokenizing the source code of the project. Each code element (such \n40 \nNyirongo and Jiang, et al. \n \n \nas classes, methods, and fields) is transformed into a bag of tokens. Ref-Finder proposed by Prete et al. [75] encodes code \nelements (e.g., classes, methods, and fields) and their relationships using logic predicates to detect the refactorings. Liu \net al. [49] proposed ReMapper an automated iterative approach used to match software entities between two successive \nversions for the discovery of refactorings. ReMapper takes full advantage of the qualified names, the implementations, \nand the references of software entities. It leverages an iterative matching algorithm to handle the interdependence \nbetween entity matching and the computation of reference-based similarity. Researchers have utilized some of these \ntraditional refactoring miners to mine refactorings to train deep learning models in the process of refactoring as follows. \nAlthough deep learning technologies have not yet been employed to distinguish refactorings from other source code \nmodifications as RefactoringMiner or Ref-Finder do, deep learning technologies have been successfully employed to \nidentify refactoring-containing commits by analyzing their associated commit messages. For example, Marmolejos et \nal. [58] developed a framework that used text-mining, natural language preprocessing, and supervised machine learning \ntechniques to automatically identify and classify refactoring activities in commit messages. The framework focused on \ndetecting Self-Affirmed Refactorings (SAR), which are refactoring activities reported in commit messages. The approach \nused a binary classification method to overcome the limitations of the manual process proposed in previous studies. The \nframework had four main parts. The first part involved preparing the data and processing the content of the commit \nmessages to remove unnecessary and irrelevant information, as well as normalize the data. In the second part, the \ndata was converted into hash values, with each hash value representing one or more features in the commit messages. \nThe third part involved filtering the features to select only the most important ones from the dataset. Finally, in the \nfourth part, machine learning algorithms were trained and tested based on the selected features. The resulting two-class \nclassifier was able to operate over unlabelled texts. For this approach, the authors used four classifiers, including Bayes \nPoint Machine, Logistic Regression, Boosted Decision Tree, and Average Perceptron, as well as one deep learning-based \nclassifier, Neural Network. The dataset used in this approach contained 1,208,970 refactoring operations, extracted \nusing RefactoringMiner [93] from 3,795 open-source Java projects. From this dataset, the authors extracted commit \nmessages containing the required patterns to create their refactoring dataset. Since the employed machine learning \ntechniques could not directly identify text, the authors converted the collected data into hashes. They used the feature \nhashing technique, also known as a hashing trick, to derive features. In this technique, various words with varying \nlengths were mapped to different features based on the hash value. To determine the relevance of each attribute in \nthe dataset, Chi-Square (CHI) was used to give a score, while Fisher Score (FS) was used to select a subset of features \nand score the distance between them. The machine learning classifiers were trained using a stratified train-test split \nmethodology, where 70% of the rows of the transformed dataset from the selected features were used for training and \nthe remaining 30% were used to measure the error rate. The approach proved to be efficient, as the authors obtained \nsubstantial accuracy. \nAlomar et al. [6] aimed to investigate whether different words and phrases used in refactoring commit messages \nare unique to different types of refactorings. To achieve this, they employed machine learning techniques to predict \nrefactoring operation types based on the commit messages. The prediction of the refactoring operation was formulated \nas a multiclass classification problem, which relied on textual mining of commit messages to extract relevant features \nfor each class. The researchers collected a dataset of refactorings from 800 projects, where each instance presented \na commit message and a refactoring type. They identified six preferred method-level refactorings, including extract \nmethod, inline method, move method, pull-up method, push-down method, and rename method. To identify relevant \nfeatures, they used the n-gram technique proposed by Manning and Hinrich [57]. Nine supervised machine learning \nA Survey of Deep Learning Based Software Refactoring \n41 \n \n \nalgorithms were applied, and the results were compared against a keyword-based baseline approach used in Murphy \net al. [62]. The results revealed that the predictive accuracy for rename method, extract method, and move method \nranged from 63% to 93% in terms of F-measure. Nevertheless, the model encountered challenges in accurately discerning \nbetween Inline Method, Pull-up Method, and Push-down Method, with F-measure scores falling within the range of \n42% to 45%. Additionally, it’s noteworthy that the keyword-based approach exhibited significantly lower performance \ncompared to the machine learning models. \n \n8 CHALLENGES AND OPPORTUNITIES \nAs the use of deep learning models in the domain of software refactoring continues to grow, it becomes imperative to \nclosely look at the challenges and opportunities linked to their adoption. Despite showcasing promising capabilities in \naiding different tasks of the refactoring process, there are still some challenges associated with their application. This \nsection explores the challenges confronted by deep learning models in supporting the process of software refactoring, \nconcurrently shedding light on prospective opportunities for future work. \n \n8.1 Challenges \nWhile deep learning models have proven to be effective in supporting the process of software refactoring, their adoption \ninto this field is not without challenges. From our survey, we note the following challenges. \n• Limited generalization of deep learning techniques across diverse paradigms is a significant concern. Many \nstudies have developed approaches concentrating on specific code smells (e.g., feature envy, brain class, brain \nmethod) within a particular language, such as Java. This specialization restricts the applicability of these models \nto different code smells or programming languages. Given that code smells can manifest differently in diverse \ncontexts, a model trained on one set of smells may not exhibit robust performance on others. Moreover, code \nsmells often coexist and exhibit interactions. For instance, a lengthy method may signal a broader design issue, \nlike a god class. Approaches focused on individual smells in the detection of code smells might overlook these \nintricate interactions, resulting in incomplete or inaccurate outcomes. Notably, based on the compiled primary \nworks, a substantial majority (at least 87.50%) employed datasets developed in a singular programming language, \ni.e., Java, posing a challenge for the generalization of these approaches to other programming languages. \n• Challenges in creating generic classification and feature engineering. Developing a universal classifier for \ndiverse refactoring processes has proven challenging. This challenge is particularly evident in the detection of \ncode smells, where different types of code smells necessitate distinct features and characteristics for accurate \nidentification. Employing a one-size-fits-all approach may lead to diminished precision and recall, especially \nin studies utilizing sequential modeling-based deep-learning approaches to support software refactoring. \nAdditionally, extracting pertinent features from abstract syntax trees or sequences of statements presents \ndifficulties. The model’s effectiveness relies on the accurate capture of both semantic and structural features. \nEstablishing suitable mechanisms for feature extraction is pivotal for the success of deep learning-based \napproaches. \n• Concerns about data quality and representativeness are pivotal factors influencing the performance of deep \nlearning models. Certain approaches utilized automatically generated labeled data, potentially lacking accurate \n42 \nNyirongo and Jiang, et al. \n \n \nrepresentation of real-world scenarios. Incorporating real-world examples could enhance the effectiveness of \ndeep learning models. \n• Limited adoption. Developers may be resistant to adopting automated refactoring tools supported by deep \nlearning due to concerns about the reliability and trustworthiness of the generated code changes. Notably, most \nof the current studies on the end-to-end code transformation for refactorings by deep learning models have \nused prototype tools and non-industrial datasets which do not reflect the actual tools and data pools used by \nprogrammers. \n• Need for continuous learning. Software systems evolve as software systems undergo maintenance and updates. \nModels trained on a static dataset may become outdated and may not effectively support the process of software \nrefactoring in such without continuous learning mechanisms. \n8.2 Opportunities \nAmidst these challenges presented in Section 8.1, there exist some opportunities for advancing the integration of deep \nlearning models in software refactoring. Some of the opportunities, according to our survey, are as follows. \n• According to the taxonomy presented, deep learning models have been used for various tasks, including \ndetecting code smells, recommending refactoring solutions, conducting refactorings, and mining refactorings. \nOur survey of the literature shows that deep learning models are primarily employed for detecting code smells, \naccounting for at least 56.25% in this category. The recommendation of refactoring solutions accounts for 33.33%, \nend-to-end code transformation as refactoring for 6.25%, and mining of refactorings for 4.17%. However, we \ndid not find any significant study on the use of deep learning for software refactoring quality assurance in our \nliterature search. This revelation shows that there is an imbalance in how deep learning has been employed in \nsupporting refactoring tasks and thus, presents an opportunity for future work. It is highly valuable to fill this \ngap and ensure that all the tasks of software refactoring are fully supported by deep learning models. \n• According to the primary studies presented, various types of deep learning models have been used for software \nrefactoring. Most of the studies focused on utilizing hybrid models that combine deep learning models with \nother techniques to improve model performance. The data shows that at least 58.69% of the primary studies used \nhybrid approaches, while the remaining 41.31% used generic deep learning models. The latter encompassed \nsequential modeling, explainable and feedback-centric approaches that fit into developers’ workflow. Among \nthe deep learning models, CNN was the most commonly used model, found in at least 43.48% of the primary \nworks, followed by RNN at 34.78%, and its variants (such as GRU, LSTM, GRU, etc.). The other 21.74% of the \nstudies employed other deep learning models, such as GCN, GNN, ResNet, MLP, etc. However, only a few \nstudies, such as Sharma et al. [80, 81], explored the use of transfer learning techniques to enable deep learning \nmodels trained on one project to be effectively used for refactoring in different projects. There is a need to carry \nout more exploration of transfer learning approaches as they can result in better generalization and minimize \nthe need for extensive project-specific training data. Notably, Himesh et al. [64] and Yin et al. [104] were a \nfew of the researchers who explored the inclusion of feedback and developments of explainable deep learning \nmodels for refactoring. To improve the adoption of deep learning models in the software refactoring process by \nsoftware developers, it is necessary to do more explorations in the inclusion of feedback and explainability in \nthe deep learning models. \nA Survey of Deep Learning Based Software Refactoring \n43 \n \n \n• Our literature search has revealed that deep learning models have been predominantly used for method-level \nrefactorings. Among the primary studies we surveyed, 55.41% applied deep learning models for refactorings \nat the method level, followed by 30.45% for class level, 10.12% for variable level, and 4.02% for other types of \nrefactorings. The most frequent use of deep learning techniques was for detecting and applying move method \nand extract method refactorings, both of which occur at the method level of the codebase. It is worth noting \nthat most of the work on identifying refactoring opportunities by detecting code smells has been focused on \nmethod-level code smells, particularly the feature envy smell which often leads to the recommendation and \nsuggestion of move method refactoring. This opens up room for more future work in employing deep learning \nmodels for refactorings occurring at other levels than just the method level. \n \n• Based on the literature, it has been found that deep learning models are effective in supporting the process of \nsoftware refactoring. These models have outperformed the existing approaches or tools used in refactoring by \nachieving an average F-measure of 76%, which is a significant improvement of 30% on average compared to \nthe state-of-the-art approaches. However, there is still a need to develop dynamic and adaptive deep-learning \nmodels that can continuously learn and adjust to changes in coding standards, project goals, and evolving best \npractices. This could involve using reinforcement learning approaches or other adaptive learning strategies. \nAdditionally, according to our survey, most of the primary studies did not use industrial datasets, which makes it \ndifficult to generalize the findings of most of the techniques. To improve the effectiveness of deep learning-based \ntechniques for refactoring, it is necessary to incorporate more real-world industrial data. \n \n9 CONCLUSIONS \nIn this paper, we have presented a survey on deep learning-based software refactoring. Our focus was on the process of \nsoftware refactoring and how it can be supported by deep learning models. We have categorized the studies based on the \nspecific refactoring task that was supported by deep learning models. Our taxonomy has identified five main categories \nwhich include the detection of code smells, recommendation of refactoring solutions, end-to-end code transformation \nas refactoring, quality assurance for refactoring, and mining of refactorings. We have presented key aspects under \nthese categories which have provided insight into the research direction in the deployment of deep learning models for \nsoftware refactoring. \n \nREFERENCES \n[1] Charu C. Aggarwal. 2018. Neural Networks and Deep Learning: a textbook. Cham (Switzerland) Springer 2018. https://link.springer.com/book/10. \n1007/978-3-319-94463-0 \n[2] Amal Alazba, Hamoud Aljamaan, and Mohammad R. Alshayeb. 2023. Deep learning approaches for bad smell detection: a systematic literature \nreview. Empirical Software Engineering 28 (2023). https://api.semanticscholar.org/CorpusID:258591793 \n[3] Mamdouh Alenezi, Mohammed Akour, and Osama Al Qasem. 2020. Harnessing deep learning algorithms to predict software refactoring. \nTELKOMNIKA Telecommunication Computing Electronics and Control 18 (2020), 2977–2982. https://api.semanticscholar.org/CorpusID:225015544 \n[4] Eman Abdullah Alomar, Anton Ivanov, Zarina Kurbatova, Yaroslav Golubev, Mohamed Wiem Mkaouer, Ali Ouni, Timofey Bryksin, Le Nguyen, \nAmit Dilip Kini, and Aditya Thakur. 2021. AntiCopyPaster: Extracting Code Duplicates As Soon As They Are Introduced in the IDE. Proceedings of \nthe 37th IEEE/ACM International Conference on Automated Software Engineering (2021).  https://api.semanticscholar.org/CorpusID:245634394 \n[5] Eman Abdullah Alomar, Anton Ivanov, Zarina Kurbatova, Yaroslav Golubev, Mohamed Wiem Mkaouer, Ali Ouni, Timofey Bryksin, Le Nguyen, \nAmit Dilip Kini, and Aditya Thakur. 2023. Just-in-Time Code Duplicates Extraction. Inf. Softw. Technol. 158 (2023), 107169. https://api. \nsemanticscholar.org/CorpusID:256621589 \n[6] Eman Abdullah Alomar, Jiaqian Liu, Kenneth Addo, Mohamed Wiem Mkaouer, Christian D. Newman, Ali Ouni, and Zhe Yu. 2021. On the \ndocumentation of refactoring types. Automated Software Engineering 29 (2021). https://api.semanticscholar.org/CorpusID:244896267 \n44 \nNyirongo and Jiang, et al. \n \n \n[7] Uri Alon, Shaked Brody, Omer Levy, and Eran Yahav. 2018. code2seq: Generating Sequences from Structured Representations of Code. ArXiv \nabs/1808.01400 (2018). https://api.semanticscholar.org/CorpusID:51926976 \n[8] Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav. 2018. code2vec: learning distributed representations of code. Proceedings of the ACM on \nProgramming Languages 3 (2018), 1 – 29. https://api.semanticscholar.org/CorpusID:4710028 \n[9] Maurício Finavaro Aniche, Erick Galani Maziero, Rafael Serapilha Durelli, and Vinicius H. S. Durelli. 2020. The Effectiveness of Supervised \nMachine Learning Algorithms in Predicting Software Refactoring. IEEE Transactions on Software Engineering 48 (2020), 1432–1450. https: \n//api.semanticscholar.org/CorpusID:210157308 \n[10] Antoine Barbez, Foutse Khomh, and Yann-Gaël Guéhéneuc. 2019. A Machine-learning Based Ensemble Method For Anti-patterns Detection. ArXiv \nabs/1903.01899 (2019). https://api.semanticscholar.org/CorpusID:67877051 \n[11] Christian Bird, Nachiappan Nagappan, Brendan Murphy, Harald Gall, and Premkumar Devanbu. 2011. Don’t Touch My Code! Examining the \nEffects of Ownership on Software Quality. SIGSOFT/FSE 2011 - Proceedings of the 19th ACM SIGSOFT Symposium on Foundations of Software \nEngineering, 4–14. https://doi.org/10.1145/2025113.2025119 \n[12] Christopher M. Bishop. 1995. Neural networks for pattern recognition. https://api.semanticscholar.org/CorpusID:60563397 \n[13] Shaosheng Cao, Wei Lu, and Qiongkai Xu. 2015. GraRep: Learning Graph Representations with Global Structural Information. Proceedings of the \n24th ACM International on Conference on Information and Knowledge Management (2015). https://api.semanticscholar.org/CorpusID:17341970 \n[14] Sofia Charalampidou, Apostolos Ampatzoglou, Alexander Chatzigeorgiou, Antonios Gkortzis, and Paris Avgeriou. 2017. Identifying Extract \nMethod Refactoring Opportunities Based on Functional Relevance. IEEE Transactions on Software Engineering 43 (2017), 954–974. https: \n//api.semanticscholar.org/CorpusID:4642697 \n[15] François Chollet. 2018. Keras: The Python Deep Learning library. https://api.semanticscholar.org/CorpusID:215844202 \n[16] Di Cui, Qiangqiang Wang, Siqi Wang, Jianlei Chi, Jianan Li, Lu Wang, and Qingshan Li. 2023. REMS: Recommending Extract Method Refactoring \nOpportunities via Multi-view Representation of Code Property Graph. 2023 IEEE/ACM 31st International Conference on Program Comprehension \n(ICPC) (2023), 191–202. https://api.semanticscholar.org/CorpusID:259860601 \n[17] Di Cui, Siqi Wang, Yong Luo, Xingyu Li, Jie Dai, Lu Wang, and Qingshan Li. 2022. RMove: Recommending Move Method Refactoring Opportunities \nusing Structural and Semantic Representations of Code. 2022 IEEE International Conference on Software Maintenance and Evolution (ICSME) (2022), \n281–292. https://api.semanticscholar.org/CorpusID:254902806 \n[18] Ananta Kumar Das, Shikhar Yadav, and Subhasish Dhal. 2019. Detecting Code Smells using Deep Learning. In TENCON 2019 - 2019 IEEE Region 10 \nConference (TENCON). 2081–2086. https://doi.org/10.1109/TENCON.2019.8929628 \n[19] Seema Dewangan, Rajwant Singh Rao, Alok Mishra, and Manjari Gupta. 2021. A Novel Approach for Code Smell Detection: An Empirical Study. \nIEEE Access PP (2021), 1–1. https://api.semanticscholar.org/CorpusID:245065600 \n[20] Danny Dig, Can Comertoglu, Darko Marinov, and Ralph E. Johnson. 2006. Automated Detection of Refactorings in Evolving Components. In \nEuropean Conference on Object-Oriented Programming. https://api.semanticscholar.org/CorpusID:12303996 \n[21] Bo Liu et al. 22023. Deep Learning Based Feature Envy Detection Boosted by Real-World Examples. (22023). https://lyoubo.github.io/papers/ \nDeep_Learning_Based_Feature_Envy_Detection_Boosted_by_Real-World_Examples.pdf \n[22] Marios Fokaefs, Nikolaos Tsantalis, and Alexander Chatzigeorgiou. 2007. JDeodorant: Identification and Removal of Feature Envy Bad Smells. In \nInternational Conference on Smart Multimedia. https://api.semanticscholar.org/CorpusID:19001314 \n[23] Francesca Arcelli Fontana, Mika Mäntylä, Marco Zanoni, and Alessandro Marino. 2016. Comparing and experimenting machine learning techniques \nfor code smell detection. Empirical Software Engineering 21 (2016), 1143–1191. https://api.semanticscholar.org/CorpusID:16222152 \n[24] Francesca Arcelli Fontana, Mika Mäntylä, Marco Zanoni, and Alessandro Marino. 2016. Comparing and experimenting machine learning techniques \nfor code smell detection. Empirical Software Engineering 21 (2016), 1143–1191. https://api.semanticscholar.org/CorpusID:16222152 \n[25] Francesca Arcelli Fontana and Marco Zanoni. 2017. Code smell severity classification using machine learning techniques. Knowl. Based Syst. 128 \n(2017), 43–58. https://api.semanticscholar.org/CorpusID:39781104 \n[26] Martin Fowler. 2002. Refactoring: Improving the Design of Existing Code. In Extreme Programming and Agile Methods — XP/Agile Universe 2002, \nDon Wells and Laurie Williams (Eds.). Springer Berlin Heidelberg, Berlin, Heidelberg, 256–256. \n[27] Joshua Garcia, Daniel Popescu, George T. Edwards, and Nenad Medvidović. 2009. Identifying Architectural Bad Smells. 2009 13th European \nConference on Software Maintenance and Reengineering (2009), 255–258. https://api.semanticscholar.org/CorpusID:1847981 \n[28] Suryanarayana Girish, Samarthyam Ganesh, and Sharma Tushar. 2015. Refactoring for Software Design Smells: Managing Technical Debt. \n[29] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. 2016. Deep Learning. Cambridge (Massachusetts): MIT Press. https://doi.org/10.1007/s10710- \n017-9314-z \n[30] Aditya Grover and Jure Leskovec. 2016. node2vec: Scalable Feature Learning for Networks. Proceedings of the 22nd ACM SIGKDD International \nConference on Knowledge Discovery and Data Mining (2016). https://api.semanticscholar.org/CorpusID:207238980 \n[31] Xueliang Guo, Chongyang Shi, and He Jiang. 2019. Deep semantic-Based Feature Envy Identification. Proceedings of the 11th Asia-Pacific Symposium \non Internetware (2019). https://api.semanticscholar.org/CorpusID:207811924 \n[32] Mouna Hadj-Kacem and Nadia Bouassida. 2018. A Hybrid Approach To Detect Code Smells using Deep Learning. In International Conference on \nEvaluation of Novel Approaches to Software Engineering. https://api.semanticscholar.org/CorpusID:14006917 \n[33] Mouna Hadj-Kacem and Nadia Bouassida. 2019. Deep Representation Learning for Code Smells Detection using Variational Auto-Encoder. 2019 \nInternational Joint Conference on Neural Networks (IJCNN) (2019), 1–8. https://api.semanticscholar.org/CorpusID:203605428 \nA Survey of Deep Learning Based Software Refactoring \n45 \n \n \n[34] Abeer Hamdy and Mostafa Tazy. 2020. Deep Hybrid Features for Code Smells Detection. https://api.semanticscholar.org/CorpusID:221505409 \n[35] Zhang Hanyu and Tomoji Kishi. 2023. Long Method Detection Using Graph Convolutional Networks. Journal of Information Processing 31 (08 \n2023), 469–477. https://doi.org/10.2197/ipsjjip.31.469 \n[36] Vincent J. Hellendoorn and Premkumar T. Devanbu. 2017. Are deep neural networks the best choice for modeling source code? Proceedings of the \n2017 11th Joint Meeting on Foundations of Software Engineering (2017). https://api.semanticscholar.org/CorpusID:21164835 \n[37] Jie Hu, Li Shen, Samuel Albanie, Gang Sun, and Enhua Wu. 2017. Squeeze-and-Excitation Networks. 2018 IEEE/CVF Conference on Computer Vision \nand Pattern Recognition (2017), 7132–7141. https://api.semanticscholar.org/CorpusID:140309863 \n[38] Xing Hu, Ge Li, Xin Xia, D. Lo, and Zhi Jin. 2019. Deep code comment generation with hybrid lexical and syntactical information. Empirical \nSoftware Engineering 25 (2019), 2179 – 2217. https://api.semanticscholar.org/CorpusID:189927337 \n[39] Gao Huang, Yixuan Li, Geoff Pleiss, Zhuang Liu, John E. Hopcroft, and Kilian Q. Weinberger. 2017. Snapshot Ensembles: Train 1, get M for free. \nArXiv abs/1704.00109 (2017). https://api.semanticscholar.org/CorpusID:6820006 \n[40] Hamel Husain, Hongqiu Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt. 2019. CodeSearchNet Challenge: Evaluating the State of \nSemantic Code Search. ArXiv abs/1909.09436 (2019). https://api.semanticscholar.org/CorpusID:202712680 \n[41] Ferdin Joe John Joseph, Sarayut Nonsiri, and Annop Monsakul. 2021. Keras and TensorFlow: A Hands-On Experience. Advanced Deep Learning for \nEngineers and Scientists (2021). https://api.semanticscholar.org/CorpusID:237998052 \n[42] René Just, Darioush Jalali, and Michael D. Ernst. 2014. Defects4J: a database of existing faults to enable controlled testing studies for Java programs. \nIn International Symposium on Software Testing and Analysis. https://api.semanticscholar.org/CorpusID:12796895 \n[43] John D Keller. 2019. Deep Learning. Cambridge (Massachusetts): MIT Press. https://direct.mit.edu/books/book/4556/Deep-Learning \n[44] Lov Kumar, Shashank Mouli Satapathy, and Lalita Bhanu Murthy Neti. 2019. Method Level Refactoring Prediction on Five Open Source Java \nProjects using Machine Learning Techniques. Proceedings of the 12th Innovations on Software Engineering Conference (formerly known as India \nSoftware Engineering Conference) (2019). https://api.semanticscholar.org/CorpusID:60441115 \n[45] Zarina Kurbatova, Ivan Veselov, Yaroslav Golubev, and Timofey Bryksin. 2020. Recommendation of Move Method Refactoring Using Path- \nBased Representation of Code. Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops (2020). https: \n//api.semanticscholar.org/CorpusID:211133313 \n[46] James R. Lee, Shayan Oveis Gharan, and Luca Trevisan. 2011. Multi-way spectral partitioning and higher-order cheeger inequalities. In Symposium \non the Theory of Computing. https://api.semanticscholar.org/CorpusID:8212381 \n[47] Yichen Li and Xiaofang Zhang. 2022. Multi-Label Code Smell Detection with Hybrid Model based on Deep Learning. In International Conference on \nSoftware Engineering and Knowledge Engineering. https://api.semanticscholar.org/CorpusID:252098564 \n[48] Tao Lin, Xue Fu, Fu Chen, and Luqun Li. 2021. A Novel Approach for Code Smells Detection Based on Deep Leaning. Lecture Notes of the Institute \nfor Computer Sciences, Social Informatics and Telecommunications Engineering (2021). https://api.semanticscholar.org/CorpusID:238020089 \n[49] Bo Liu, Hui Liu, Nan Niu, Yuxia Zhang, Guangjie Li, and Yanjie Jiang. 2023. Automated Software Entity Matching Between Successive Versions. 2023 \n38th IEEE/ACM International Conference on Automated Software Engineering (ASE) (2023), 1615–1627. https://api.semanticscholar.org/CorpusID: \n265056437 \n[50] F. Liu, Ge Li, Yunfei Zhao, and Zhi Jin. 2020. Multi-task Learning based Pre-trained Language Model for Code Completion. 2020 35th IEEE/ACM \nInternational Conference on Automated Software Engineering (ASE) (2020), 473–485. https://api.semanticscholar.org/CorpusID:229703606 \n[51] Hui Liu, Jiahao Jin, Zhifeng Xu, Yanzhen Zou, Yifan Bu, and Lu Zhang. 2021. Deep Learning Based Code Smell Detection. IEEE Transactions on \nSoftware Engineering 47, 9 (2021), 1811–1837. https://doi.org/10.1109/TSE.2019.2936376 \n[52] Hao Liu, Yanlin Wang, Zhao Wei, Yongxue Xu, Juhong Wang, Hui Li, and Rongrong Ji. 2023. RefBERT: A Two-Stage Pre-trained Framework \nfor Automatic Rename Refactoring. Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis (2023). \nhttps://api.semanticscholar.org/CorpusID:258960338 \n[53] Hui Liu, Zhifeng Xu, and Yanzhen Zou. 2018. Deep Learning Based Feature Envy Detection. In 2018 33rd IEEE/ACM International Conference on \nAutomated Software Engineering (ASE). 385–396. https://doi.org/10.1145/3238147.3238166 \n[54] Kui Liu, Dongsun Kim, Tegawendé F. Bissyandé, Tae young Kim, Kisub Kim, Anil Koyuncu, Suntae Kim, and Yves Le Traon. 2019. Learning \nto Spot and Refactor Inconsistent Method Names. 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE) (2019), 1–12. \nhttps://api.semanticscholar.org/CorpusID:155144146 \n[55] Wenhao Ma, Yaoxiang Yu, Xiaoming Ruan, and Bo Cai. 2023. Pre-trained Model Based Feature Envy Detection. 2023 IEEE/ACM 20th International \nConference on Mining Software Repositories (MSR) (2023), 430–440. https://api.semanticscholar.org/CorpusID:259835399 \n[56] Ruchika Malhotra, Bhawna Jain, and Marouane Kessentini. 2023. Examining deep learning’s capability to spot code smells: a systematic literature \nreview. Cluster Computing 26 (2023), 3473 – 3501. https://api.semanticscholar.org/CorpusID:263654376 \n[57] Christopher D. Manning and Hinrich Schütze. 1999. Foundations of Statistical Natural Language Processing. MIT Press, Cambridge, MA, USA. \n[58] Licelot Marmolejos, Eman Abdullah Alomar, Mohamed Wiem Mkaouer, Christian D. Newman, and Ali Ouni. 2021. On the use of textual feature \nextraction techniques to support the automated detection of refactoring documentation. Innovations in Systems and Software Engineering 18 (2021), \n233 – 249. https://api.semanticscholar.org/CorpusID:233807785 \n[59] Antonio Mastropaolo, Emad Aghajani, Luca Pascarella, and Gabriele Bavota. 2022. Automated variable renaming: are we there yet? Empirical \nSoftware Engineering 28 (2022). https://api.semanticscholar.org/CorpusID:254564632 \n46 \nNyirongo and Jiang, et al. \n \n \n[60] Rana S. Menshawy, Ahmed H. Yousef, and Ashraf Salem. [n. d.]. Comparing the Effectiveness of Machine Learning and Deep Learning Techniques \nfor Feature Envy Detection in Software Systems. In 2023 Intelligent Methods, Systems, and Applications (IMSA). 470–475. https://doi.org/10.1109/ \nIMSA58542.2023.10217458 \n[61] Naouel Moha, Yann-Gael Gueheneuc, Laurence Duchien, and Anne-Francoise Le Meur. 2010. DECOR: A Method for the Specification and Detection \nof Code and Design Smells. IEEE Transactions on Software Engineering 36, 1 (2010), 20–36. https://doi.org/10.1109/TSE.2009.50 \n[62] Emerson R. Murphy-Hill, Chris Parnin, and Andrew P. Black. 2009. How we refactor, and how we know it. 2009 IEEE 31st International Conference \non Software Engineering (2009), 287–297. https://api.semanticscholar.org/CorpusID:5856772 \n[63] Purnima Naik, Salomi Nelaballi, Venkata Sai Pusuluri, and Dae-Kyoo Kim. 2023. Deep Learning-Based Code Refactoring: A Review of Current \nKnowledge. SSRN Electronic Journal (2023). https://api.semanticscholar.org/CorpusID:254267544 \n[64] Himesh Nanadani, Mootez Saad, and Tushar Sharma. 2023. Calibrating Deep Learning-based Code Smell Detection using Human Feedback. (2023). \nhttps://tusharma.in/preprints/SCAM23_HumanFeedbackOnSmells.pdf \n[65] Ally S. Nyamawe. 2022. Mining commit messages to enhance software refactorings recommendation: A machine learning approach. Machine \nLearning with Applications (2022). https://api.semanticscholar.org/CorpusID:248807768 \n[66] Ally S. Nyamawe, Hui Liu, Nan Niu, Qasim Umer, and Zhendong Niu. 2020. Feature requests-based recommendation of software refactorings. \nEmpirical Software Engineering 25 (2020), 4315–4347. https://api.semanticscholar.org/CorpusID:221521552 \n[67] Fabio Palomba, Dario Di Nucci, Michele Tufano, Gabriele Bavota, Rocco Oliveto, Denys Poshyvanyk, and Andrea De Lucia. 2015. Landfill: An \nOpen Dataset of Code Smells with Public Evaluation. 2015 IEEE/ACM 12th Working Conference on Mining Software Repositories (2015), 482–485. \nhttps://api.semanticscholar.org/CorpusID:16120092 \n[68] Fabio Palomba, Annibale Panichella, Andrea De Lucia, Rocco Oliveto, and Andy Zaidman. 2016. A textual-based technique for Smell Detection. \n2016 IEEE 24th International Conference on Program Comprehension (ICPC) (2016), 1–10. https://api.semanticscholar.org/CorpusID:36114894 \n[69] Rasmita Panigrahi, Sanjay Kumar Kuanar, and Lov Kumar. 2022. Machine Learning Implementation for Refactoring Prediction. In 2022 IEEE 4th PhD \nColloquium on Emerging Domain Innovation and Technology for Society (PhD EDITS). 1–2. https://doi.org/10.1109/PhDEDITS56681.2022.9955297 \n[70] Rasmita Panigrahi, Sanjay Kumar Kuanar, Sanjay Misra, and Lov Kumar. 2022. Class-Level Refactoring Prediction by Ensemble Learning with \nVarious Feature Selection Techniques. Applied Sciences (2022). https://api.semanticscholar.org/CorpusID:254363225 \n[71] Jevgenija Pantiuchina. 2019. Towards Just-In-Time Rational Refactoring. 2019 IEEE/ACM 41st International Conference on Software Engineering: \nCompanion Proceedings (ICSE-Companion) (2019), 180–181. https://api.semanticscholar.org/CorpusID:174799906 \n[72] Bryan Perozzi, Rami Al-Rfou, and Steven S. Skiena. 2014. DeepWalk: online learning of social representations. Proceedings of the 20th ACM SIGKDD \ninternational conference on Knowledge discovery and data mining (2014). https://api.semanticscholar.org/CorpusID:3051291 \n[73] Bryan Perozzi, Vivek Kulkarni, Haochen Chen, and Steven S. Skiena. 2016. Don’t Walk, Skip!: Online Learning of Multi-scale Network \nEmbeddings. Proceedings of the 2017 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 2017 (2016). \nhttps://api.semanticscholar.org/CorpusID:207699173 \n[74] Darwin Pinheiro, Carla Ilane Moreira Bezerra, and Anderson G. Uchôa. 2022. How do Trivial Refactorings Affect Classification Prediction Models? \nProceedings of the 16th Brazilian Symposium on Software Components, Architectures, and Reuse (2022). https://api.semanticscholar.org/CorpusID: \n252497875 \n[75] Kyle Prete, Napol Rachatasumrit, Nikita Sudan, and Miryung Kim. 2010. Template-based reconstruction of complex refactorings. 2010 IEEE \nInternational Conference on Software Maintenance (2010), 1–10. https://api.semanticscholar.org/CorpusID:2659467 \n[76] Osama Al Qasem, Mohammed Akour, and M. Alenezi. 2020. The Influence of Deep Learning Algorithms Factors in Software Fault Prediction. IEEE \nAccess 8 (2020), 63945–63960. https://api.semanticscholar.org/CorpusID:215816960 \n[77] Colin Raffel, Noam M. Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2019. \nExploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. J. Mach. Learn. Res. 21 (2019), 140:1–140:67. https: \n//api.semanticscholar.org/CorpusID:204838007 \n[78] Sanjiban Roy, Valentina Balas, Pijush Samui, and Sharma D. 2019. Handbook of Deep Learning Applications. \n[79] Priyadarshni Suresh Sagar, Eman Abdullah Alomar, Mohamed Wiem Mkaouer, Ali Ouni, and Christian D. Newman. 2021. Comparing Commit \nMessages and Source Code Metrics for the Prediction Refactoring Activities. Algorithms 14 (2021), 289. https://api.semanticscholar.org/CorpusID: \n244175361 \n[80] Tushar Sharma, Vasiliki Efstathiou, Panagiotis Louridas, and Diomidis D. Spinellis. 2019. On the Feasibility of Transfer-learning Code Smells using \nDeep Learning. ArXiv abs/1904.03031 (2019). https://api.semanticscholar.org/CorpusID:102351639 \n[81] Tushar Sharma, Vasiliki Efstathiou, Panos Louridas, and Diomidis D. Spinellis. 2021. Code smell detection by deep direct-learning and transfer- \nlearning. J. Syst. Softw. 176 (2021), 110936. https://api.semanticscholar.org/CorpusID:233329781 \n[82] Tushar Sharma and Marouane Kessentini. 2021. QScored: A Large Dataset of Code Smells and Quality Metrics. 2021 IEEE/ACM 18th International \nConference on Mining Software Repositories (MSR) (2021), 590–594. https://api.semanticscholar.org/CorpusID:232165224 \n[83] Danilo Silva, Ricardo Terra, and Marco Túlio Valente. 2015. JExtract: An Eclipse Plug-in for Recommending Automated Extract Method Refactorings. \nArXiv abs/1506.06086 (2015). https://api.semanticscholar.org/CorpusID:707971 \n[84] Danilo Silva and Marco Túlio Valente. 2017. RefDiff: Detecting Refactorings in Version Histories. 2017 IEEE/ACM 14th International Conference on \nMining Software Repositories (MSR) (2017), 269–279. https://api.semanticscholar.org/CorpusID:11506870 \nA Survey of Deep Learning Based Software Refactoring \n47 \n \n \n[85] Danilo Silva and Marco Tulio Valente. 2017. RefDiff: Detecting Refactorings in Version Histories. In 2017 IEEE/ACM 14th International Conference \non Mining Software Repositories (MSR). 269–279. https://doi.org/10.1109/MSR.2017.14 \n[86] Gustavo Soares. 2010. Making program refactoring safer. In 2010 ACM/IEEE 32nd International Conference on Software Engineering, Vol. 2. 521–522. \nhttps://doi.org/10.1145/1810295.1810461 \n[87] Balázs Szalontai, Péter Bereczky, and Dániel Horpácsi. 2023. Deep Learning-Based Refactoring with Formally Verified Training Data. Infocommu- \nnications journal (2023). https://api.semanticscholar.org/CorpusID:261570010 \n[88] Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. 2015. LINE: Large-scale Information Network Embedding. Proceedings \nof the 24th International Conference on World Wide Web (2015). https://api.semanticscholar.org/CorpusID:8399404 \n[89] Ricardo Terra, Marco Túlio Valente, Sergio Miranda, and Vitor Sales. 2018. JMove: A novel heuristic and tool to detect move method refactoring \nopportunities. J. Syst. Softw. 138 (2018), 19–36. https://api.semanticscholar.org/CorpusID:4412526 \n[90] Omkarendra Tiwari and Rushikesh K. Joshi. 2022. Identifying Extract Method Refactorings. 15th Innovations in Software Engineering Conference \n(2022). https://api.semanticscholar.org/CorpusID:246828642 \n[91] Nikolaos Tsantalis and Alexander Chatzigeorgiou. 2009. Identification of Extract Method Refactoring Opportunities. In 2009 13th European \nConference on Software Maintenance and Reengineering. 119–128. https://doi.org/10.1109/CSMR.2009.23 \n[92] Nikolaos Tsantalis and Alexander Chatzigeorgiou. 2009. Identification of Move Method Refactoring Opportunities. IEEE Transactions on Software \nEngineering 35, 3 (2009), 347–367. https://doi.org/10.1109/TSE.2009.1 \n[93] Nikolaos Tsantalis, Ameya Ketkar, and Danny Dig. 2022. RefactoringMiner 2.0. IEEE Transactions on Software Engineering 48, 3 (2022), 930–950. \nhttps://doi.org/10.1109/TSE.2020.3007722 \n[94] Nikolaos Tsantalis, Matin Mansouri, Laleh Eshkevari, Davood Mazinanian, and Danny Dig. 2018. Accurate and Efficient Refactoring Detection in \nCommit History. In 2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE). 483–494. https://doi.org/10.1145/3180155.3180206 \n[95] Nikolaos Tsantalis, Matin Mansouri, Laleh Mousavi Eshkevari, Davood Mazinanian, and Danny Dig. 2018. Accurate and Efficient Refactoring \nDetection in Commit History. 2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE) (2018), 483–494. https://api. \nsemanticscholar.org/CorpusID:49665673 \n[96] Michele Tufano, Jevgenija Pantiuchina, Cody Watson, Gabriele Bavota, and Denys Poshyvanyk. 2019. On Learning Meaningful Code Changes Via \nNeural Machine Translation. 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE) (2019), 25–36. https://api.semanticscholar. \norg/CorpusID:59316445 \n[97] Daixin Wang, Peng Cui, and Wenwu Zhu. 2016. Structural Deep Network Embedding. Proceedings of the 22nd ACM SIGKDD International \nConference on Knowledge Discovery and Data Mining (2016). https://api.semanticscholar.org/CorpusID:207238964 \n[98] Hongze Wang, Jing Liu, Jiexiang Kang, Wei Yin, Haiying Sun, and Hui Wang. 2020. Feature Envy Detection based on Bi-LSTM with Self-Attention \nMechanism. 2020 IEEE Intl Conf on Parallel & Distributed Processing with Applications, Big Data & Cloud Computing, Sustainable Computing & \nCommunications, Social Computing & Networking (ISPA/BDCloud/SocialCom/SustainCom) (2020), 448–457. https://api.semanticscholar.org/CorpusID: \n235340022 \n[99] Ian H. Witten, Eibe Frank, and Mark A. Hall. 2011. Data Mining: Practical Machine Learning Tools and Techniques (3rd ed.). Morgan Kaufmann \nPublishers Inc., San Francisco, CA, USA. \n[100] Sihan Xu, Aishwarya Sivaraman, Siau-Cheng Khoo, and Jing Xu. 2017. GEMS: An Extract Method Refactoring Recommender. In 2017 IEEE 28th \nInternational Symposium on Software Reliability Engineering (ISSRE). 24–34. https://doi.org/10.1109/ISSRE.2017.35 \n[101] Sihan Xu, Aishwarya Sivaraman, Siau-Cheng Khoo, and Jing Xu. 2017. GEMS: An Extract Method Refactoring Recommender. 2017 IEEE 28th \nInternational Symposium on Software Reliability Engineering (ISSRE) (2017), 24–34. https://api.semanticscholar.org/CorpusID:38648648 \n[102] Weiwei Xu and Xiaofang Zhang. 2021. Multi-Granularity Code Smell Detection using Deep Learning Method based on Abstract Syntax Tree. In \nInternational Conference on Software Engineering and Knowledge Engineering. https://api.semanticscholar.org/CorpusID:239678857 \n[103] Yanming Yang, Xin Xia, David Lo, and John Grundy. 2020. A Survey on Deep Learning for Software Engineering. \n[104] Xin Yin, Chongyang Shi, and Shuxin Zhao. 2021. Local and Global Feature Based Explainable Feature Envy Detection. 2021 IEEE 45th Annual \nComputers, Software, and Applications Conference (COMPSAC) (2021), 942–951. https://api.semanticscholar.org/CorpusID:237474266 \n[105] Dongjin Yu, Yihang Xu, Lehui Weng, Jie Chen, Xin Chen, and Quanxin Yang. 2022. Detecting and Refactoring Feature Envy Based on Graph \nNeural Network. 2022 IEEE 33rd International Symposium on Software Reliability Engineering (ISSRE) (2022), 458–469. https://api.semanticscholar. \norg/CorpusID:254930430 \n[106] Jie Zhang, Yuxiao Dong, Yan Wang, Jie Tang, and Ming Ding. 2019. ProNE: Fast and Scalable Network Representation Learning. In International \nJoint Conference on Artificial Intelligence. https://api.semanticscholar.org/CorpusID:189808933 \n[107] Minnan Zhang and Jingdong Jia. 2022. Feature Envy Detection with Deep Learning and Snapshot Ensemble. 2022 9th International Conference on \nDependable Systems and Their Applications (DSA) (2022), 215–223. https://api.semanticscholar.org/CorpusID:253124697 \n[108] Yang Zhang and Chunhao Dong. 2021. MARS: Detecting brain class/method code smell based on metric–attention mechanism and residual \nnetwork. Journal of Software: Evolution and Process (2021). https://api.semanticscholar.org/CorpusID:243792860 \n[109] Yang Zhang, Chuyan Ge, Shuai Hong, Ruili Tian, Chun-Ru Dong, and J. Liu. 2022. DeleSmell: Code smell detection based on deep learning and \nlatent semantic analysis. Knowl. Based Syst. 255 (2022), 109737. https://api.semanticscholar.org/CorpusID:251751777 \n[110] Yang Zhang, Chuyan Ge, Haiyang Liu, and Kun Zheng. 2024. Code smell detection based on supervised learning models: A survey. Neurocomputing \n565 (2024), 127014. https://doi.org/10.1016/j.neucom.2023.127014 \n48 \nNyirongo and Jiang, et al. \n \n \n[111] Shuxin Zhao, Chongyang Shi, Shaojun Ren, and Hufsa Mohsin. 2022. Correlation Feature Mining Model Based on Dual Attention for Feature Envy \nDetection. In International Conference on Software Engineering and Knowledge Engineering. https://api.semanticscholar.org/CorpusID:252100954 \n \n",
  "categories": [
    "cs.SE",
    "D.2.3"
  ],
  "published": "2024-04-30",
  "updated": "2024-04-30"
}