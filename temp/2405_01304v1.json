{
  "id": "http://arxiv.org/abs/2405.01304v1",
  "title": "Misclassification bounds for PAC-Bayesian sparse deep learning",
  "authors": [
    "The Tien Mai"
  ],
  "abstract": "Recently, there has been a significant focus on exploring the theoretical\naspects of deep learning, especially regarding its performance in\nclassification tasks. Bayesian deep learning has emerged as a unified\nprobabilistic framework, seeking to integrate deep learning with Bayesian\nmethodologies seamlessly. However, there exists a gap in the theoretical\nunderstanding of Bayesian approaches in deep learning for classification. This\nstudy presents an attempt to bridge that gap. By leveraging PAC-Bayes bounds\ntechniques, we present theoretical results on the prediction or\nmisclassification error of a probabilistic approach utilizing Spike-and-Slab\npriors for sparse deep learning in classification. We establish non-asymptotic\nresults for the prediction error. Additionally, we demonstrate that, by\nconsidering different architectures, our results can achieve minimax optimal\nrates in both low and high-dimensional settings, up to a logarithmic factor.\nMoreover, our additional logarithmic term yields slight improvements over\nprevious works. Additionally, we propose and analyze an automated model\nselection approach aimed at optimally choosing a network architecture with\nguaranteed optimality.",
  "text": "arXiv:2405.01304v1  [math.ST]  2 May 2024\nMisclassiﬁcation bounds for PAC-Bayesian sparse\ndeep learning\nThe Tien Mai\nDepartment of Mathematical Sciences, Norwegian University of Science\nand Technology, Trondheim, 7034, Norway .\nCorresponding author(s). E-mail(s): the.t.mai@ntnu.no;\nAbstract\nRecently, there has been a signiﬁcant focus on exploring the theoretical aspects\nof deep learning, especially regarding its performance in classiﬁcation tasks.\nBayesian deep learning has emerged as a uniﬁed probabilistic framework, seek-\ning to integrate deep learning with Bayesian methodologies seamlessly. However,\nthere exists a gap in the theoretical understanding of Bayesian approaches in\ndeep learning for classiﬁcation. This study presents an attempt to bridge that\ngap. By leveraging PAC-Bayes bounds techniques, we present theoretical results\non the prediction or misclassiﬁcation error of a probabilistic approach utilizing\nSpike-and-Slab priors for sparse deep learning in classiﬁcation. We establish non-\nasymptotic results for the prediction error. Additionally, we demonstrate that, by\nconsidering diﬀerent architectures, our results can achieve minimax optimal rates\nin both low and high-dimensional settings, up to a logarithmic factor. Moreover,\nour additional logarithmic term yields slight improvements over previous works.\nAdditionally, we propose and analyze an automated model selection approach\naimed at optimally choosing a network architecture with guaranteed optimality.\nKeywords: deep neural network, binary classiﬁcation, prediction bounds, PAC-Bayes\nbounds, fast rate, low-rank priors\n1 Introduction\nDeep learning has achieved remarkable success in various applications, including\ncomputer vision, natural language processing, and other areas related to pattern\nrecognition and classiﬁcation (Goodfellow et al., 2016; LeCun et al., 2015). As its\napplicability continues to expand, there is a growing interest in understanding the\n1\ntheoretical foundations that underlie its eﬀectiveness. To gain theoretical insights\ninto deep learning, numerous studies have been conducted from various perspectives,\nincluding approximation theory and statistical learning theory. The statistical learning\ncommunity has extensively investigated the generalization properties of Deep Neu-\nral Networks (DNNs), (Barron, 1994; Bartlett et al., 2017; Neyshabur et al., 2018;\nZhang et al., 2021; Schmidt-Hieber, 2020; Suzuki, 2018; Imaizumi and Fukumizu, 2019;\nSuzuki, 2019). Speciﬁcally, Schmidt-Hieber (2020) and Suzuki (2019) demonstrated\nthat estimators based on sparsely connected DNNs with ReLU activation functions\nand carefully selected architectures can achieve minimax estimation rates (up to log-\narithmic factors) under classical smoothness assumptions on the regression function.\nAdditionally, Imaizumi and Fukumizu (2019) and Hayakawa and Suzuki (2020) high-\nlighted the superiority of DNNs over linear operators in certain scenarios, where DNNs\nachieve the minimax rate of convergence while alternative methods fall short.\nOn the Bayesian front, theoretical research has been relatively limited. Studies\nsuch as Polson and Roˇckov´a (2018); Suzuki (2018); Lee and Lee (2022); Kong et al.\n(2023) have explored the properties of the posterior distribution, while Vladimirova\net al. (2019) investigated the regularization eﬀects of prior distributions at the unit\nlevel. Furthermore, Ch´erief-Abdellatif (2020); Bai et al. (2020) examined properties\nrelated to variational inference.\nIn the realm of classiﬁcation, theoretical investigations of deep learning classiﬁers\nhave been undertaken quite recently in various setups. Such as Kim et al. (2021);\nShen et al. (2022); Hu et al. (2022); Bos and Schmidt-Hieber (2022); Wang and\nShang (2022); Kohler et al. (2022); Meyer (2023); Kohler and Langer (2024). While a\nvariational Bayes classiﬁcation for dense deep neural networks has been studied Bhat-\ntacharya et al. (2024), however, the prediction error for classiﬁcation using Bayesian\nsparse deep learning remains unexplored. This study aims to address this gap by\npresenting an eﬀort to shed light on this aspect.\nWe investigate a probabilistic approach for a sparse deep neural network classiﬁer,\nwhere the network’s architecture is governed by a Gibbs posterior incorporating a risk\nconcept based on the hinge loss (Zhang, 2004). The hinge loss is commonly utilized for\nclassiﬁcation tasks in deep learning (Kim et al., 2021; Shen et al., 2022; Hu et al., 2022;\nBos and Schmidt-Hieber, 2022; Wang and Shang, 2022; Kohler et al., 2022; Kohler\nand Langer, 2024). Our analysis of prediction errors relies on the PAC-Bayes bound\ntechnique, originally introduced by McAllester (1998); Shawe-Taylor and Williamson\n(1997). This technique aims to provide numerical generalization certiﬁcates for vari-\nous probabilistic machine learning algorithms, and subsequently, Catoni (2004, 2007)\ndemonstrated its utility in deriving oracle inequalities and rates of convergence for\ndiﬀerent methods. This methodology shares signiﬁcant connections with the “informa-\ntion bounds” presented by Zhang (2006); Russo and Zou (2019). For a comprehensive\nexploration of this topic, we recommend consulting Guedj (2019); Alquier (2024).\nUtilizing Spike-and-Slab priors, our main results are presented in terms of non-\nasymptotic oracle inequalities. These inequalities demonstrate that the prediction error\nof our proposed method is as good as the best possible errors. Our results are applicable\nin a general setting. Speciﬁcally, we show that by considering diﬀerent architectures,\nour results can achieve minimax optimal rates for H¨older smooth functions in both\n2\nlow and high dimensions, up to a logarithmic factor. This alignment with works in\nthe frequentist literature, such as Kim et al. (2021) and Wang and Shang (2022),\nunderscores the robustness of our ﬁndings. Moreover, our inclusion of an additional\nlogarithmic term yields slight improvements over previous works. Furthermore, we\nalso propose and analyze an automated model selection approach aimed at optimally\nchoosing a network architecture with guaranteed optimality.\nThe rest of the paper is structured as follows: Section 2 introduces the problem,\ndescribes the deep neural network, and presents our proposed method. Our main\nresults are outlined in Section 3. All technical proofs are provided in Section 4.\nNotations\nLet us introduce the notations and the statistical framework we adopt in this paper.\nFor any vector x = (x1, ..., xd) ∈[−1, 1]d and any real-valued function f deﬁned on\n[−1, 1]d, d > 0, we denote: ∥x∥∞= max1≤i≤d |xi|, ∥f∥2 = (\nR\nf 2)1/2 and ∥f∥∞=\nsupy∈[−1,1]d |f(y)|. Put (a)+ := max(a, 0), ∀a ∈R. For any k ∈{0, 1, 2, ...}d, we deﬁne\n|k| = Pd\ni=1 ki and the mixed partial derivatives when all partial derivatives up to\norder |k| exist: Dkf(x) =\n∂|k|f\n∂k1 x1...∂kdxd (x). For β > 0, let ⌊β⌋denote the largest integer\nstrictly smaller than β and . Then f is said to be β-H¨older continuous (Tsybakov,\n2009) if all partial derivatives up to order ⌊β⌋exist and are bounded. The norm ∥f∥Cβ\nof the H¨older space Cβ = {f : ∥f∥Cβ < +∞} is deﬁned as:\n∥f∥Cβ := max|k|≤⌊β⌋∥Dkf∥∞+ max|k|=⌊β⌋supx,y∈[−1,1]d,x̸=y\n|Dkf(x)−Dkf(y)|\n∥x−y∥β−⌊β⌋\n∞\n.\n2 Problem and method\n2.1 Problem statement\nThis study examines a general binary classiﬁcation problem. Given a high-dimensional\nfeature vector x ∈Rd, the class label outcome Y is a binary variable derived from\n{−1, 1} with an unknown probability p(x). Speciﬁcally, Y is equal to 1 with a probabil-\nity of p(x), and −1 with a probability of 1−p(x) when conditioned on x. The accuracy\nof a classiﬁer η is determined by the prediction or misclassiﬁcation error, deﬁned as\nR(η) = P(Y ̸= η(x)).\nHowever, the probability function p(x) is unknown and the resulting classiﬁer ˆη(x)\nshould be designed from the data Dn: a random sample of n independent observations\n(x1, Y1), . . . , (xn, Yn). The design points xi may be considered as ﬁxed or random. The\ncorresponding (conditional) prediction error of ˆη is R(ˆη) = P(Y ̸= ˆη(x) |Dn). In this\nstudy, our objective is to construct a classiﬁer ˆη with a prediction error that closely\napproximates the ideal Bayes error R∗:= inf R(η), (Vapnik, 1998; Devroye et al.,\n1996).\n3\n2.2 Deep neural networks\nWe deﬁne a deep neural network as any function fθ : Rd →R, recursively deﬁned as:\n\n\n\n\n\nx(0) := x,\nx(ℓ) := ρ(Aℓx(ℓ−1) + bℓ)\nfor ℓ= 1, ..., L −1,\nfθ(x) := ALx(L−1) + bL\nwhere L ≥3 and ρ is an activation function acting componentwise.\nFor example, we can employ the ReLU activation function, ρ(u) = max(u, 0). Each\nAℓ∈RDℓ×Dℓ−1 serves as a weight matrix. Its entry at the (i, j)-th position, called\nedge weight, connects the j-th neuron from the (ℓ−1)-th layer to the i-th neuron of\nthe ℓ-th layer. And, each node vector bℓ∈RDℓacts as a shift vector. Its i-th entry\nsigniﬁes the weight linked with the i-th node of the ℓ-th layer.\nPut D0 = d the number of units in the input layer, DL = 1 the number of units in\nthe output layer and Dℓ= D the number of units in the hidden layers. The architecture\nof the network is characterized by its number of edges S (the total number of nonzero\nentries in matrices Aℓand vectors bℓ), its number of layers L ≥3 (excluding the input\nlayer), and its width D ≥1. We have S ≤T := PL\nℓ=1 Dℓ(Dℓ−1 + 1), the total number\nof coeﬃcients in a fully connected network.\nAt this point, we assume that S, L, and D are constants. We require d ≤D and\nT ≤LD(D + 1). It is assumed that the absolute values of all coeﬃcients are bounded\nabove by a constant CB ≥2. The parameter for a Deep Neural Network (DNN) is\ndenoted as θ = {(A1, b1), . . . , (AL, bL)}, and we deﬁne ΘS,L,D as the set of all possible\nparameters. Alternatively, we will also consider the stacked coeﬃcients parameter,\nθ = (θ1, . . . , θT ).\n2.3 EWA procedure using empirical hinge loss\nWe construct a classiﬁer ˆη(x) := sign(fθ(x)).\nWe adopt a PAC-Bayesian approach (Alquier, 2024) and consider an exponen-\ntially weighted aggregate (EWA) procedure. Let’s consider the following posterior\ndistribution:\nˆρλ(θ) ∝exp[−λrh\nn(θ)]π(θ)\n(1)\nwhere λ > 0 is a tuning parameter that will be discussed later and π(θ) is a prior\ndistribution, given in (2), that promotes (approximately) sparsity on the parameter\nvector θ. In this paper, we primarily focus on the hinge loss resulting in the following\nhinge empirical risk:\nrh\nn(θ) = 1\nn\nn\nX\ni=1\n(1 −Yi fθ(xi))+ .\nIt is noteworthy that several studies in deep learning for classiﬁcation have employed\nhinge loss as a surrogate loss to facilitate computation, as in (Kim et al., 2021; Wang\nand Shang, 2022).\n4\nThe EWA procedure has found application in various contexts in prior works\n(Dalalyan et al., 2018; Dalalyan and Tsybakov, 2012; Dalalyan, 2020; Dalalyan and\nTsybakov, 2008). The term ˆρλ is also referred to as the Gibbs posterior (Alquier et al.,\n2016; Catoni, 2007). The incorporation of ˆρλ is driven by the minimization prob-\nlem presented in Lemma 6, rather than strictly adhering to conventional Bayesian\nprinciples.\n2.3.1 Sparsity prior\nWe adopt a spike-and-slab prior π (Castillo et al., 2015; Ch´erief-Abdellatif, 2020)\nover the parameter space ΘS,L,D. The spike-and-slab prior is recognized as a relevant\nalternative to dropout in Bayesian deep learning, as discussed in Polson and Roˇckov´a\n(2018). More speciﬁcally, the prior is deﬁned hierarchically as follows. Initially, a binary\nvector of indicators γ = (γ1, . . . , γT ) is sampled from the set SS\nT of T -dimensional\nbinary vectors with precisely S non-zero entries. Subsequently, given γt for each t =\n1, . . . , T , we apply a spike-and-slab prior to θt, which returns 0 if γt = 0 and a random\nsample from a uniform distribution on [−CB, CB] otherwise:\n(\nγ ∼U(SS\nT ),\nθt|γt ∼γt U([−CB, CB]) + (1 −γt)δ{0}, t = 1, ..., T\n(2)\nwhere δ{0} is a point mass at 0 and U([−CB, CB]) is a uniform distribution on\n[−CB, CB]. We recall that the sparsity level S is ﬁxed here and that this assumption\nwill be relaxed in Section 3.4.\nRemark 1. We opt for uniform distributions for simplicity, as done in related studies\n(Polson and Roˇckov´a, 2018; Suzuki, 2018). However, Gaussian distributions can also\nbe employed when dealing with an unbounded parameter set ΘS,L,D, as indicated in\nCh´erief-Abdellatif (2020).\n3 Main results\n3.1 Assumptions\nThe result in this section applies to a wide range of activation functions, including\nthe popular ReLU activation and the identity map. In the following, we make the\nfollowing assumption regarding the activation function.\nAssumption 1. Assume that the activation function ρ is 1-Lispchitz continuous with\nrespect to the absolute value: |ρ(x)| ≤|x| for any x ∈R.\nPut R∗:= minθ P(Y ̸= sign(fθ(x))) ; θ∗:= arg minθ P(Y ̸= sign(fθ(x))) ; rn(θ) =\nn−1 Pn\ni=1 1{Yifθ(xi) < 0} , and r∗\nn := rn(θ∗).\nAssumption 2. We assume that there is a constant C′ > 0 such that rh\nn(θ∗) ≤\n(1 + C′)r∗\nn.\nAssumption 2 implies that for the underlying parameter θ∗, the function fθ∗(x) is\nbounded above by a constant.\n5\nAssumption 3 (Low-noise assumption). We assume that there is a constant C ≥1\nsuch that: E\nh\u00001Y fθ(x)≤0 −1Y fθ∗(x)≤0\n\u00012i\n≤C[R(θ) −R∗].\nAssumption 3 serves as the noise condition, capturing the diﬀerence between a clas-\nsiﬁer’s performance and random guessing. This assumption is also commonly referred\nto as the margin assumption, as discussed in Mammen and Tsybakov (1999), and\nfurther explored in Tsybakov (2004); Bartlett et al. (2006).\n3.2 General results\nHere, we present general results that do not require fθ to be β-H¨older and we consider\nany structure (S, L, D).\nTheorem 1. Given Assumption 1 and Assumption 2, for λ = √n, we ﬁnd that with\na probability of at least 1 −2ǫ, where ǫ ∈(0, 1), the following holds:\nZ\nRdˆρλ ≤(1 + 2C′)R∗+ c\nS log\n\u0010\nnT DL[(d+1)L+1]\nS(D−1)\n\u0011\n+ log(1/ǫ)\n√n\n,\nwhere c depends only ong CB, C′.\nIn addition to the results in Theorem 1 for the integrated error, we can derive a\nresult for, ˆθ ∼ˆρλ, a stochastic classiﬁer sampled from the Gibbs-posterior (1).\nTheorem 2. Under the same assumptions for Theorem 1, and the same deﬁnition\nfor λ, for any small ε ∈(0, 1). one has that E\nh\nPˆθ∼ˆρλ(ˆθ ∈B)\ni\n≥1 −ε, where\nB =\n(\nθ ∈ΘS,L,D : R ≤(1 + 2C′)R∗+ c\nS log\n\u0012\nT nDL[(d+1)L+1]\nS(D−1)\n\u0013\n+log(2/ǫ)\n√n\n)\n.\nThe oracle inequality presented in Theorem 1 links the integrated prediction risk of\nour method to the minimum attainable risk. By incorporating additional assumptions,\nthe bounds provided in Theorem 1 can be improved.\nTheorem 3. Suppose Assumption 1, Assumption 2 and Assumption 3 are satisﬁed.\nFor λ = 2n/(3C +2), we ﬁnd that with a probability of at least 1−2ǫ, where ǫ ∈(0, 1),\nthe following holds:\nZ\nRdˆρλ ≤(1 + 3C′)R∗+ c′ S log\n\u0010\nT nDL[(d+1)L+1]\nS(D−1)\n\u0011\n+ log (1/ǫ)\nn\n.\nwhere c′ is a universal constant depending only on C, C′, CB.\nIn contrast to Theorem 1, the bound in Theorem 3 exhibits a faster rate, scaling as\n1/n rather than 1/√n. These bounds enable a comparison between the out-of-sample\nerror of our method and the optimal error R∗.\nFor the noiseless case, i.e. Y = sign(fθ∗(x)) almost surely, then R∗= 0 and\nconsequently that E\nh\u00001Y fθ(x)≤0 −1Y fθ∗(x)≤0\n\u00012i\n= E\nh\n12\nY fθ(x)≤0\ni\n= E\n\u0002\n1Y fθ(x)≤0\n\u0003\n=\nR(θ) −R∗. Thus, Assumption 3 is satisﬁed with C = 1.\n6\nCorollary 4. In the noiseless case, for λ = 2n/5, with probability at least 1 −2ǫ, ǫ ∈\n(0, 1), Theorem 3 leads to\nZ\nRdˆρλ ≤c′ S log\n\u0010\nT nDL[(d+1)L+1]\nS(D−1)\n\u0011\n+ log (1/ǫ)\nn\n.\nwhere c′ is a universal constant depending only on C′, CB.\nIn analogy to Theorem 2, we can establish that a stochastic classiﬁer, ˆβ ∼ˆρλ,\ndrawn from our proposed pseudo-posterior in Equation (1) exhibits a fast rate.\nTheorem 5. Under the same assumptions for Theorem 3, and the same deﬁnition\nfor λ, along with any small ε ∈(0, 1), we ﬁnd that E\nh\nPˆθ∼ˆρλ(ˆθ ∈Ω)\ni\n≥1 −ε, where\nΩn =\n(\nθ ∈ΘS,L,D : R ≤(1 + 3C′)R∗+ c′\nS log\n\u0012\nT nDL[(d+1)L+1]\nS(D−1)\n\u0013\n+log(2/ǫ)\nn\n)\n.\nIn this section, we establish speciﬁc values for the tuning parameters λ in our pro-\nposed method within theoretical results for prediction errors. However, it is noted that\nthese values may not necessarily be optimal for practical applications. Cross-validation\ncan be employed in practical scenarios to ﬁne-tune the parameters appropriately. Nev-\nertheless, the theoretical values identiﬁed in our analysis oﬀer insights into the scale\nof the tuning parameters when utilized in real-world situations.\n3.3 Rates in speciﬁc settings\nLow-dimension setup\nNow, let’s explore the scenario of low-dimensional settings, where we assume that d is\nﬁxed and smaller than n. We will derive prediction error rates for the noiseless case,\nconsidering the following architecture.\nExample 1. Let us assume that fθ∗is β-H¨older smooth with 0 < β < d and that\nthe activation function is ReLU. We consider the architecture of Polson and Roˇckov´a\n(2018), for some positive constant CD independent of n, that: L = 8 + (⌊log2 n⌋+\n5)(1 + ⌈log2 d⌉), D = CD⌊n\nd\n2β+d / log n⌋, S ≤94d2(β + 1)2dD(L + ⌈log2 d⌉).\nProposition 6. For the architecture as in Example 1, then the rate in Corollary 4 is\nof order n\n−2β\n2β+d log n.\nRemark 2. The rate provided in Proposition 6, adjusted by a log n factor, is minimax\noptimal Tsybakov (2004). Additionally, it is noteworthy that our log n factor represents\na slight enhancement over the log3 n factor achieved by the authors in Kim et al.\n(2021) for obtaining the minimax optimal rate.\nHigh-dimensional setup\nNext, we delve into the realm of high-dimensional settings, where we assume that\nd > n. We will consider the following architecture.\nExample 2. Let us assume that fθ∗is β-H¨older smooth with 0 < β < d and that the\nactivation function is ReLU. We consider the architecture of Wang and Shang (2022):\nL ≍log n, D ≍d, S ≍n\nd\n2β+d log n.\n7\nProposition 7. For the architecture as in Example 2, then the rate in Corollary 4 is\nof order n\n−2β\n2β+d log n log d.\nRemark 3. The rate, n\n−2β\n2β+d log d, provided in Proposition 7, up to a log n factor,\nis minimax optimal in a high dimensional setting as in Wang and Shang (2022).\nAdditionally, it is noteworthy that our log n factor represents a slight enhancement\nover the log2 n log d factor achieved by the authors in Wang and Shang (2022) for\nobtaining the minimax optimal rate.\n3.4 Architecture design via model selection\nLet MS,L,D denote the model associated with the parameter set ΘS,L,D. We consider\na countable number of models, and introduce prior beliefs pS,L,D over the sparsity,\nthe depth and the width of the network, hierarchically.\nMore speciﬁcally, pS,L,D = pS|L,DpD|LpL where pL = 2−L, pD|L follows a uniform\ndistribution over {d, ..., max(eL, d)} given L, and pS|L,D is a uniform distribution\nover {1, ..., T } given L and D. Here, T represents the number of coeﬃcients in a\nfully connected network. This particular choice is sensible as it allows to consider\nany number of hidden layers and (at most) an exponentially large width with respect\nto the depth of the network. We continue to employ spike-and-slab priors in (2) on\nθS,L,D ∈ΘS,L,D given model MS,L,D, now denoted as π(S,L,D)(θ).\nThe Gibbs posterior is now as ˆρ(S,L,D)\nλ\n(θ) ∝exp[−λrh\nn(θ)]π(S,L,D)(θ). We formulate\nthe following optimization problem to select the architecture of the network.\n( ˆS, ˆL, ˆD) = arg min\nS,L,D\n\"Z\nrh\nndˆρ(S,L,D)\nλ\n+ KL(ˆρ(S,L,D)\nλ\n, π(S,L,D)) + log(1/pS,L,D)\nλ\n#\n(3)\nThe following theorem demonstrates that this strategy for selecting the architecture\nyields identical results to those in our main theorem, Theorem 3.\nTheorem 8. Suppose Assumption 1, Assumption 2 and Assumption 3 are satisﬁed.\nFor λ = 2n/(3C +2), we ﬁnd that with a probability of at least 1−2ǫ, where ǫ ∈(0, 1),\nthe following holds:\nZ\nRdˆρ( ˆS,ˆL, ˆ\nD)\nλ\n≤(1 + 3C′)R∗+ inf\nS,L,D\n\nc′ S log\n\u0010\nT nDL[(d+1)L+1]\nS(D−1)\n\u0011\n+ log(\n1\npS,L,D ) + log( 1\nǫ)\nn\n\n.\nwhere c′ is a universal constant depending only on C, C′, CB.\nTheorem\n8\nillustrates\nthat\nonce\nthe\ncomplexity\nterm\nlog(1/pS,L,D)/n,\nwhich\nreﬂects\nthe\nprior\nbeliefs,\nfalls\nbelow\nthe\neﬀective\nerror\nrS,L,D\nn\n:=\nS max (log(L), L log(D), log(nd)) /n, the selection method in (3) adaptively achieves\nthe best possible rate. This scenario leads to (near-)minimax rates for H¨older smooth\nfunctions and selects the optimal architecture. Notably, for the prior beliefs choice\nπL = 2−L, πD|L = 1/(max(eL, d) −d + 1), πS|L,D = 1/T , we obtain, up to some\n8\nabsolute constant K > 0, that:\nlog(\n1\npS,L,D )\nn\n≤K max(log(D), log L, L, log(d))\nn\nwhich is lower than rS,L,D\nn\n(up to a factor).\nIt is worth noting that various model selection approaches for choosing the archi-\ntecture of the network have been proposed in the study of deep learning, such as Kim\net al. (2021); Ch´erief-Abdellatif (2020).\n4 Proof\nProof for slow rate\nProof of Theorem 1.\nStep 1:\nPut Ui = 1Yifθ(xi)≤0 −1Yifθ∗(xi)≤0, then, −1 ≤Ui ≤1. One can utilize Hoeﬀding’s\nLemma 5 to derive that\nE exp {λ[R(θ) −R∗] −λ[rn(θ) −r∗\nn]} ≤exp\n\u0000λ2/2n\n\u0001\n.\nIntegrating with respect to π and then applying Fubini’s theorem, one gets, for any\nλ ∈(0, n), that\nE\nZ\nexp\n\u001a\nλ[R(θ) −R∗] −λ[rn(θ) −r∗\nn] −λ2\n2n\n\u001b\ndπ(θ) ≤1,\n(4)\nConsequently, using Lemma 6,\nE exp\n\u001a\nsup\nρ\nZ \u001a\nλ[R(θ) −R∗] −λ[rn(θ) −r∗\nn] −λ2\n2n\n\u001b\nρ(dθ) −KL(ρ, π)\n\u001b\n≤1.\nApplying Markov’s inequality, for ǫ ∈(0, 1), we obtain that:\nP\n\u001a\nsup\nρ\nZ \u0014\nλ[R(θ) −R∗] −λ[rn(θ) −r∗\nn] −λ2\n2n\n\u0015\nρ(dθ) −KL(ρ, π) + log ǫ > 0\n\u001b\n≤ǫ.\nSubsequently, considering the complement, we derive that with a probability of at\nleast 1 −ǫ, the following holds:\n∀ρ,\nλ\nZ\n[R(θ) −R∗]ρ(dθ) ≤λ\nZ\n[rn(θ) −r∗\nn]ρ(dθ) + KL(ρ, π) + λ2\n2n + log 1\nǫ .\nNow, note that as rh\nn ≥rn and as it stands for all ρ then the right hand side can\nbe minimized and, from Lemma 6, the minimizer over P(ΘS,L,D) is ˆρλ. Thus we get,\n9\nwhen λ > 0,\nZ\nRdˆρλ ≤R∗+\ninf\nρ∈P(ΘS,L,D)\n\u0014Z\nrh\nndρ + 1\nλKL(ρ, π)\n\u0015\n−r∗\nn + λ\n2n + 1\nλ log 1\nǫ .\n(5)\nStep 2:\nFirst, we have that,\nZ\nrh\nn(θ)ρ(dθ) = 1\nn\nZ\nn\nX\ni=1\n(1 −Yifθ(xi))+ ρ(dθ)\n≤1\nn\n\" n\nX\ni=1\n(1 −Yifθ∗(xi))+ +\nZ\nn\nX\ni=1\n|fθ(xi) −fθ∗(xi)| ρ(dθ)\n#\n≤rh\nn(θ∗) + 1\nn\nn\nX\ni=1\nZ\n|fθ(xi) −fθ∗(xi)| ρ(dθ).\n(6)\nAnd using Lemma 4,\nZ\n|fθ(xi) −fθ∗(xi)| ρ(dθ)\n≤\nZ\nrL(θ)ρ(dθ)\n≤\nZ\n(CBD)L−1 CBD(d + 1) −d\nCBD −1\nL\nX\nu=1\n˜Auρ(dθ) +\nZ\nL\nX\nu=1\n(CBD)L−u˜buρ(dθ)\n≤(CBD)L−1 CBD(d + 1) −d\nCBD −1\nL\nX\nu=1\nZ\n˜Auρ(dθ) +\nL\nX\nu=1\n(CBD)L−u\nZ\n˜buρ(dθ).\nHere, we take ρ(dθ) = q∗\nn(θ), as deﬁned in equation (11). We ﬁnd that\nZ\n˜Aℓq∗\nn(dθ) =\nZ\nsup\ni,j\n|Aℓ,i,j −A∗\nℓ,i,j|q∗\nn(dAℓ,i,j) ≤sn,\nand\nR ˜buq∗\nn(dθ) =\nR\nsupj |bu,j −b∗\nu,j|q∗\nn(dbu,j) ≤sn. Therefore,\nZ\n|fθ(xi) −fθ∗(xi)| q∗\nn(dθ) ≤(CBD)L−1 CBD(d + 1) −d\nCBD −1\nLsn + sn\nL\nX\nu=1\n(CBD)L−u\n≤(CBD)L−1 CBD(d + 1) −d\nCBD −1\nLsn + sn\nL−1\nX\nℓ=0\n(CBD)ℓ\n≤(CBD)L−1 CBD(d + 1) −d\nCBD −1\nLsn + sn\n(CBD)L −1\nCBD −1\n10\n≤sn\n\u0014\n(CBD)L CBD(d + 1) −d\n(CBD −1)CBDL + (CBD)L\nCBD −1\n\u0015\n≤sn\n(CBD)L\nCBD −1 [(d + 1)L + 1] .\n(7)\nFrom Assumption 2, we have rh\nn(θ∗) ≤(1 + C′)r∗\nn. Plug in (7) and (12), into (5),\nwe obtain\nZ\nRdˆρλ ≤R∗+ C′r∗\nn + sn\n(CBD)L\nCBD −1 [(d + 1)L + 1]\n+\nS log(T B) + S\n2 log\n\u0010\n1\ns2n\n\u0011\nλ\n+ λ\n2n + 1\nλ log\n\u00121\nǫ\n\u0013\n.\nThen, we use Lemma 1, with probability at least 1 −2ǫ, to obtain that\nZ\nRdˆρλ ≤(1 + 2C′)R∗+ C′ 1\nnς log 1\nǫ + sn\n(CBD)L\nCBD −1 [(d + 1)L + 1]\n+\nS log(T CB) + S log\n\u0010\n1\nsn\n\u0011\nλ\n+ λ\n2n + 1\nλ log\n\u00121\nǫ\n\u0013\n.\n(8)\nBy taking sn = S\nn\nh\n(CBD)L\nCBD−1 [(d + 1)L + 1]\ni−1\nand λ = √n, we can obtain that\nZ\nRdˆρλ ≤(1 + 2C′)R∗+ C′ 1\nnς log 1\nǫ + S\nn\n+\nS log\n\u0010\nT CB n\nS\n(CBD)L\nCBD−1 [(d + 1)L + 1]\n\u0011\n√n\n+ log(1/ǫ)\n√n\n.\nThe proof is completed.\nProof of Theorem 2. From (4), we have that\nE\n\"Z\nexp\n\u001a\nλ[R(θ) −R∗] −λ[rn(θ) −r∗\nn] −log\n\u0014dˆρλ\ndπ (θ)\n\u0015\n−λ2\n2n −log 1\nε\n\u001b\nˆρλ(dθ)\n#\n≤ε.\nWe employ the Chernoﬀ’s trick, which is based on exp(x) ≥1R+(x), leading to\nE\nh\nPθ∼ˆρλ(θ ∈B)\ni\n≥1 −ε, where\nB =\n\u001a\nθ : λ[R(θ) −R∗] −λ[rn(θ) −r∗\nn] ≤log\n\u0014dˆρλ\ndπ (θ)\n\u0015\n+ λ2\n2n + log 2\nε\n\u001b\n.\n11\nUtilizing the deﬁnition of ˆρλ and Lemma 6, and recognizing that rn ≤rh\nn, for θ ∈B,\nwe ﬁnd that:\nλ[R(θ) −R∗] ≤λ\n\u0010\nr(θ) −r∗\nn\n\u0011\n+ log\n\u0014dˆρλ\ndπ (θ)\n\u0015\n+ λ2\n2n + log 2\nε\n≤λ\n\u0010\nrh\nn(θ) −r∗\nn\n\u0011\n+ log\n\u0014dˆρλ\ndπ (θ)\n\u0015\n+ λ2\n2n + log 2\nε\n≤−log\nZ\nexp\n\u0002\n−λrh\nn(θ)\n\u0003\nπ(dθ) −λr∗\nn + λ2\n2n + log 2\nε\n= λ\n\u0010Z\nrh\nn(θ)ˆρλ(dθ) −r∗\nn\n\u0011\n+ K(ˆρλ, π) + λ2\n2n + log 2\nε\n= inf\nρ\n\u001a\nλ\n\u0010Z\nrh\nn(θ)ρ(dθ) −r∗\nn\n\u0011\n+ K(ρ, π) + λ2\n2n + log 2\nε\n\u001b\n.\nWe upper-bound the right-hand side exactly as Step 2 in the proof of Theorem 1. The\nresult of the theorem is followed.\nProof for fast rate\nProof of Theorem 3.\nStep 1:\nFix any θ and put Ui = 1Yifθ(xi)≤0 −1Yifθ∗(xi)≤0. Under Assumption 3, we have\nthat P\ni E[U 2\ni ] ≤nC[R(θ)−R∗]. Now, for any integer k ≥3, as the 0-1 loss is bounded,\nit results in\nX\ni\nE\n\u0002\n(Ui)k\n+\n\u0003\n≤\nX\ni\nE\n\u0002\n|Ui|k−2|Ui|2\u0003\n≤\nX\ni\nE\n\u0002\n|Ui|2\u0003\n.\nThus, we can apply Lemma 2 with v := nC[R(θ) −R∗], w := 1 and ζ := λ/n. We\nobtain, for any λ ∈(0, n),\nE exp{λ([R(θ) −R∗] −[rn(θ) −r∗\nn])} ≤exp\n\u001aCλ2[R(θ) −R∗]\n2n(1 −λ/n)\n\u001b\n,\nBy integrating with respect to π and subsequently applying Fubini’s theorem, we ﬁnd\nthat\nE\n\u0014Z\nexp\n\u001a\n(λ −\nCλ2\n2n(1 −λ/n))[R(θ) −R∗] −λ[rn(θ) −r∗\nn]\n\u001b\nπ(dθ)\n\u0015\n≤1.\n(9)\nConsequently, using Lemma 6,\nE\n\u0014\nesupρ\nR n\n(λ−\nCλ2\n2n(1−λ/n) )[R(θ)−R∗]−λ[rn(θ)−r∗\nn]\no\nρ(dM)−KL(ρ,π)\n\u0015\n≤1.\n12\nBy applying Markov’s inequality and subsequently considering the complement, we\nestablish that with a probability of at least 1 −ǫ, the following holds:\n∀ρ,\n\u0012\nλ −\nCλ2\n2n(1 −λ/n)\n\u0013 Z\n[R(θ) −R∗]ρ(dθ) ≤λ\nZ\n[rn(θ) −r∗\nn]ρ(dθ) + KL(ρ, π) + log 1\nǫ .\nNow, observe that rh\nn ≥rn,\nλ\n\u0014Z\nrndρ −r∗\nn\n\u0015\n+ KL(ρ, π) ≤λ\n\u0014Z\nrh\nndρ + 1\nλKL(ρ, π)\n\u0015\n−λr∗\nn.\nAs it stands for all ρ then the right hand side can be minimized and, from Lemma 6,\nthe minimizer over P(ΘS,L,D) is ˆρλ. Thus we get, when λ < 2n/(C + 2),\nZ\nRdˆρλ ≤R∗+\n1\n1 −\nCλ\n2n(1−λ/n)\n\u001a\ninf\nρ∈P(ΘS,L,D)\n\u0014Z\nrh\nndρ + KL(ρ, π) + log 1\nǫ\nλ\n\u0015\n−r∗\nn\n\u001b\n.\n(10)\nStep 2:\nFrom (6), we have that,\nZ\nrh\nn(θ)ρ(dθ) ≤rh\nn(θ∗) + 1\nn\nn\nX\ni=1\nZ\n|fθ(xi) −fθ∗(xi)| ρ(dθ).\nHere, we take ρ(θ) = q∗\nn(θ), as deﬁned in equation (11). Then from (7), and (12), we\ndeduce (10) into that, from Assumption 2, as rh\nn(θ∗) ≤(1 + C′)r∗\nn, we have that\nZ\nRdˆρλ ≤R∗+\n1\n1 −\nCλ\n2n(1−λ/n)\n\u001a\nC′r∗\nn + sn\n(CBD)L\nCBD −1 [(d + 1)L + 1]\n+\nS log(T CB) + S\n2 log\n\u0010\n1\ns2n\n\u0011\n+ log\n\u0000 1\nǫ\n\u0001\nλ\n\n\n.\nTaking λ = 2n/(3C + 2), we obtain:\nZ\nRdˆρλ ≤R∗+ 3\n2\n\u001a\nC′r∗\nn + sn\n(CBD)L\nCBD −1 [(d + 1)L + 1]\n+(3C + 2)\nS log(T CB) + S\n2 log\n\u0010\n1\ns2n\n\u0011\n+ log\n\u0000 1\nǫ\n\u0001\n2n\n\n\n.\nThen, we use Lemma 1, with probability at least 1 −2ǫ, to obtain that\n13\nZ\nRdˆρλ ≤(1 + 3C′)R∗+ 3\n2\n\u001a\nC′ 1\nnς log 1\nǫ + sn\n(CBD)L\nCBD −1 [(d + 1)L + 1]\n+(3C + 2)\nS log(T CB) + S log\n\u0010\n1\nsn\n\u0011\n+ log (1/ǫ)\n2n\n\n\n.\nBy setting sn = S\nn\nh\n(CBD)L\nCBD−1 [(d + 1)L + 1]\ni−1\n, one ﬁnds that\nZ\nRdˆρλ ≤(1 + 3C′)R∗+ 3\n2\n\u001a\nC′ 1\nnς log 1\nǫ + S\nn\n+(3C + 2)\nS log\n\u0010\nT CB n\nS\n(CBD)L\nCBD−1 [(d + 1)L + 1]\n\u0011\n+ log (1/ǫ)\n2n\n\n\n.\nThe proof is completed.\nProof of Theorem 5. From (9), we have that\nE\n\"Z\nexp\n\u001a\n(λ −\nCλ2\n2n(1 −λ/n))[R(θ) −R∗] −λ[rn(θ) −r∗\nn] −log\n\u0014dˆρλ\ndπ (θ)\n\u0015\n−log 1\nε\n\u001b\nˆρλ(dθ)\n#\n≤ε.\nUsing Chernoﬀ’s trick, i.e. exp(x) ≥1R+(x), this gives: E\nh\nPθ∼ˆρλ(θ ∈Ω)\ni\n≥1−ε where\nΩ=\n\u001a\nθ : (λ −\nCλ2\n2n(1 −λ/n))[R(θ) −R∗] −λ[rn(θ) −r∗\nn] ≤log\n\u0014dˆρλ\ndπ (θ)\n\u0015\n+ log 2\nε\n\u001b\n.\nUsing the deﬁnition of ˆρλ and noting that rn ≤rh\nn, for θ ∈Ωwe have\n\u0010\nλ −\nCλ2\n2n(1 −λ/n)\n\u0011h\nR(θ) −R∗i\n≤λ\n\u0010\nrn(θ) −r∗\nn\n\u0011\n+ log\n\u0014dˆρλ\ndπ (θ)\n\u0015\n+ log 2\nε\n≤λ\n\u0010\nrh\nn(θ) −r∗\nn\n\u0011\n+ log\n\u0014dˆρλ\ndπ (θ)\n\u0015\n+ log 2\nε\n≤−log\nZ\nexp\n\u0002\n−λrh\nn(θ)\n\u0003\nπ(dθ) −λr∗\nn + log 2\nε\n= λ\n\u0010Z\nrh\nn(θ)ˆρλ(dθ) −r∗\nn\n\u0011\n+ K(ˆρλ, π) + log 2\nε\n= inf\nρ\n\u001a\nλ\n\u0010Z\nrh\nn(θ)ρ(dθ) −r∗\nn\n\u0011\n+ K(ρ, π) + log 2\nε\n\u001b\n.\nWe upper-bound the right-hand side exactly as Step 2 in the proof of Theorem 3. The\nresult of the theorem is followed.\n14\nProof of Proposition 6. For certain constants K > 0, independent of n, we have that\nS log\n\u0010\nT nDL[(d+1)L+1]\nS(D−1)\n\u0011\nn\n≤K\nS log\n\u0010\nLD(D+1)nDL[(d+1)L+1]\nS(D−1)\n\u0011\nn\n≤K S max (log(L), L log(D), log n)\nn\n≤K n\nd\n2β+d\nn\nlog n = Kn\n−2β\n2β+d log n.\nProof of Proposition 7. For certain constants K > 0, independent of n, d, we have that\nS log\n\u0010\nT nDL[(d+1)L+1]\nS(D−1)\n\u0011\nn\n≤K\nS log\n\u0010\nLD(D+1)nDL[(d+1)L+1]\nS(D−1)\n\u0011\nn\n≤K S max (log(L), L log(D), log(nd))\nn\n≤K n\nd\n2β+d log n log d\nn\n= Kn\n−2β\n2β+d log n log d.\nProof of Section 3.4\nProof of Theorem 8. From (10), we have that\nZ\nRdˆρ(S,L,D)\nλ\n≤R∗+\n1\n1 −\nCλ\n2n(1−λ/n)\n(\ninf\nρ\n\"Z\nrh\nndρ + KL(ρ, π(S,L,D)) + log 1\nǫ\nλ\n#\n−r∗\nn\n)\n.\nFrom Lemma 6, it holds that ˆρ(S,L,D)\nλ\nis the minimizer of\nR\nrh\nndρ +\nKL(ρ,π(S,L,D))\nλ\n, thus\nZ\nRdˆρ(S,L,D)\nλ\n≤R∗+\n1\n1 −\nCλ\n2n(1−λ/n)\n×\n\n\n\nZ\nrh\nndˆρ(S,L,D)\nλ\n+\nKL(ˆρ(S,L,D)\nλ\n, π(S,L,D)) + log(\n1\npS,L,D ) + log 1\nǫ\nλ\n−r∗\nn\n\n\n.\nNow using the deﬁnition of ( ˆS, ˆL, ˆD) in (3), we arrive at\nZ\nRdˆρ( ˆS,ˆL, ˆ\nD)\nλ\n≤R∗+\n1\n1 −\nCλ\n2n(1−λ/n)\n×\n15\n\n\ninf\nS,L,D\n\n\nZ\nrh\nndˆρ(S,L,D)\nλ\n+\nKL(ˆρ(S,L,D)\nλ\n, π(S,L,D)) + log(\n1\npS,L,D ) + log 1\nǫ\nλ\n\n−r∗\nn\n\n\n,\nusing Lemma 6, we deduce that\nZ\nRdˆρ( ˆS,ˆL, ˆ\nD)\nλ\n≤R∗+\n1\n1 −\nCλ\n2n(1−λ/n)\n×\n(\ninf\nS,L,D inf\nρ\n\"Z\nrh\nndρ +\nKL(ρ, π(S,L,D)) + log(\n1\npS,L,D ) + log 1\nǫ\nλ\n#\n−r∗\nn\n)\n.\nTo obtain the result, proceed as in Step 2 in the proof of Theorem 3, page 13. This\ncompletes the proof.\nAuxiliary lemmas\nLemma 1. [Lemma 6 in Cottet and Alquier (2018)] For ǫ ∈(0, 1), with probability\nat least 1 −ǫ, we have, for every ς ∈(0, 1), that r∗\nn ≤(1 + ς)R∗+\n1\nnς log 1\nǫ, or we can\nhave that r∗\nn ≤2R∗+\n1\nnς log 1\nǫ.\nWe will use the following version of the Bernstein’s lemma, from (Massart, 2007,\npage 24).\nLemma 2. Let U1, . . . , Un be independent real valued random variables. Assuming\nthat there exist two constants v and w such that Pn\ni=1 E[U 2\ni ] ≤v and that for all\nintegers k ≥3, Pn\ni=1 E\n\u0002\n(Ui)k\n+\n\u0003\n≤vk!wk−2/2. Then, for any ζ ∈(0, 1/w),\nE exp\n\"\nζ\nn\nX\ni=1\n[Ui −EUi]\n#\n≤exp\n\u0012\nvζ2\n2(1 −wζ)\n\u0013\n.\nDeﬁnition 1. Put q∗\nn as\n(\nγ∗\nt = I(θ∗\nt ̸= 0),\nθt ∼γ∗\nt U([θ∗\nt −sn, θ∗\nt + sn]) + (1 −γ∗\nt )δ{0}, for each t = 1, ..., T.\n(11)\nLemma 3. We have, for q∗\nn in (11), that\nKL(q∗\nn∥π) ≤S log(T CB) + S\n2 log\n\u0012 1\ns2n\n\u0013\n.\n(12)\nProof. The proof of this Lemma can be found in the proof of Theorem 2 in Ch´erief-\nAbdellatif (2020), in particular in the third step of the proof.\nLemma 4. Deﬁne the loss of the output layer as rℓ(θ) := supx∈[−1,1]d |f L\nθ (x) −\nf L\nθ∗(x)| = supx∈[−1,1]d |fθ(x) −fθ∗(x)|. We have, under Assumption 1, that for any\nℓ= 1, ..., L:\n16\nrℓ(θ) ≤(CBD)ℓ−1 CBD(d+1)−d\nCBD−1\nPℓ\nu=1 ˜Au + Pℓ\nu=1(CBD)ℓ−u˜bu,\nwhere ˜Au = supi,j |Au,i,j −A∗\nu,i,j| and ˜bu = supj |bu,j −b∗\nu,j|.\nProof. The proof of this Lemma can be found in the proof of Theorem 2 in Ch´erief-\nAbdellatif (2020), in particular in the second step of the proof.\nWe remind here a version of Hoeﬀding’s inequality for bounded random variables.\nLemma 5. Let Ui, i = 1, . . . , n be n independent random variables with a ≤Ui ≤b\na.s., and E(Ui) = 0. Then, for any λ > 0, E exp\n\u0000 λ\nn\nPn\ni=1 Ui\n\u0001\n≤exp\n\u0010\nλ2(b−a)2\n8n\n\u0011\n.\nLemma 6 (Catoni (2007)Lemma 1.1.3). Let µ ∈P(Θ). For any measurable, bounded\nfunction h : Θ →R we have: log\nR\neh(θ)µ(dθ) = supρ∈P(Θ)\n\u0002R\nh(θ)ρ(dθ) −KL(ρ, µ)\n\u0003\n.\nMoreover, the supremum w.r.t ρ in the right-hand side is reached for the Gibbs\ndistribution, ˆρ(dθ) ∝exp(h(θ))π(dθ).\nAcknowledgements.\nThe author was supported by the Norwegian Research Coun-\ncil, grant number 309960, through the Centre for Geophysical Forecasting at NTNU.\nConﬂict of interest/Competing interests.\nThe author declares no potential\nconﬂict of interests.\nReferences\nAlquier, P.: User-friendly introduction to PAC-Bayes bounds. Foundations and\nTrends® in Machine Learning 17(2), 174–303 (2024)\nAlquier, P., Ridgway, J., Chopin, N.: On the properties of variational approximations\nof Gibbs posteriors. The Journal of Machine Learning Research 17(1), 8374–8414\n(2016)\nBarron, A.R.: Approximation and estimation bounds for artiﬁcial neural networks.\nMachine learning 14, 115–133 (1994)\nBartlett, P.L., Foster, D.J., Telgarsky, M.J.: Spectrally-normalized margin bounds for\nneural networks. Advances in neural information processing systems 30 (2017)\nBartlett, P.L., Jordan, M.I., McAuliﬀe, J.D.: Convexity, classiﬁcation, and risk bounds.\nJournal of the American Statistical Association 101(473), 138–156 (2006)\nBhattacharya, S., Liu, Z., Maiti, T.: Comprehensive study of variational bayes clas-\nsiﬁcation for dense deep neural networks. Statistics and Computing 34(1), 17\n(2024)\nBai, J., Song, Q., Cheng, G.: Eﬃcient variational inference for sparse deep learning\nwith theoretical guarantee. Advances in Neural Information Processing Systems 33,\n466–476 (2020)\nBos, T., Schmidt-Hieber, J.: Convergence rates of deep relu networks for multiclass\nclassiﬁcation. Electronic Journal of Statistics 16(1), 2724–2773 (2022)\n17\nCottet, V., Alquier, P.: 1-Bit matrix completion: PAC-Bayesian analysis of a varia-\ntional approximation. Machine Learning 107(3), 579–603 (2018)\nCh´erief-Abdellatif, B.-E.: Convergence rates of variational inference in sparse deep\nlearning. In: III, H.D., Singh, A. (eds.) Proceedings of the 37th International\nConference on Machine Learning, vol. 119, pp. 1831–1842 (2020). PMLR\nCatoni, O.: Statistical Learning Theory and Stochastic Optimization. Saint-Flour\nSummer School on Probability Theory 2001 (Jean Picard ed.), Lecture Notes in\nMathematics, vol. 1851, p. 272. Springer, Berlin (2004). https://doi.org/10.1007/\nb99352\nCatoni, O.: PAC-Bayesian Supervised Classiﬁcation: the Thermodynamics of Sta-\ntistical Learning. IMS Lecture Notes—Monograph Series, 56, p. 163. Institute of\nMathematical Statistics, Beachwood, OH (2007)\nCastillo, I., Schmidt-Hieber, J., Der Vaart, A.: Bayesian linear regression with sparse\npriors. Annals of Statistics 43(5), 1986–2018 (2015)\nDalalyan, A.S.: Exponential weights in multivariate regression and a low-rankness\nfavoring prior. Annales de l’Institut Henri Poincar´e, Probabilit´es et Statistiques\n56(2), 1465–1483 (2020)\nDevroye, L., Gy¨orﬁ, L., Lugosi, G.: A Probabilistic Theory of Pattern Recognition vol.\n31. Springer, ??? (1996)\nDalalyan, A.S., Grappin, E., Paris, Q.: On the exponentially weighted aggregate with\nthe laplace prior. The Annals of Statistics 46(5), 2452–2478 (2018)\nDalalyan, A., Tsybakov, A.B.: Aggregation by exponential weighting, sharp PAC-\nBayesian bounds and sparsity. Machine Learning 72(1-2), 39–61 (2008)\nDalalyan, A.S., Tsybakov, A.B.: Sparse regression learning by aggregation and\nlangevin monte-carlo. Journal of Computer and System Sciences 78(5), 1423–1443\n(2012)\nGoodfellow, I., Bengio, Y., Courville, A.: Deep Learning. MIT press, ??? (2016)\nGuedj, B.: A primer on PAC-Bayesian learning. In: SMF 2018: Congr`es de la Soci´et´e\nMath´ematique de France. S´emin. Congr., vol. 33, pp. 391–413. Soc. Math. France,\n??? (2019)\nHu, T., Liu, R., Shang, Z., Cheng, G.: Minimax optimal deep neural network classiﬁers\nunder smooth decision boundary. arXiv preprint arXiv:2207.01602 (2022)\nHayakawa, S., Suzuki, T.: On the minimax optimality and superiority of deep neu-\nral network learning over sparse parameter spaces. Neural Networks 123, 343–361\n(2020)\n18\nImaizumi, M., Fukumizu, K.: Deep neural networks learn non-smooth functions eﬀec-\ntively. In: The 22nd International Conference on Artiﬁcial Intelligence and Statistics,\npp. 869–878 (2019). PMLR\nKohler, M., Krzy˙zak, A., Walter, B.: On the rate of convergence of image classi-\nﬁers based on convolutional neural networks. Annals of the Institute of Statistical\nMathematics 74(6), 1085–1108 (2022)\nKohler, M., Langer, S.: Statistical theory for image classiﬁcation using deep convolu-\ntional neural networks with cross-entropy loss. Journal of Statistical Planning and\nInference ( to appear), 2011–13602 (2024)\nKim, Y., Ohn, I., Kim, D.: Fast convergence rates of deep neural networks for\nclassiﬁcation. Neural Networks 138, 179–197 (2021)\nKong, I., Yang, D., Lee, J., Ohn, I., Baek, G., Kim, Y.: Masked bayesian neu-\nral networks: Theoretical guarantee and its posterior inference. In: International\nConference on Machine Learning, pp. 17462–17491 (2023). PMLR\nLeCun, Y., Bengio, Y., Hinton, G.: Deep learning. nature 521(7553), 436–444 (2015)\nLee, K., Lee, J.: Asymptotic properties for bayesian neural network in besov space.\nAdvances in Neural Information Processing Systems 35, 5641–5653 (2022)\nMassart, P.: Concentration Inequalities and Model Selection. Lecture Notes in Math-\nematics, vol. 1896, p. 337. Springer, Saint-Flour (2007). Lectures from the 33rd\nSummer School on Probability Theory, July 6–23, 2003, Edited by Jean Picard\nMcAllester, D.A.: Some PAC-Bayesian theorems. In: Proceedings of the Eleventh\nAnnual Conference on Computational Learning Theory, pp. 230–234. ACM, New\nYork (1998)\nMeyer, J.T.: Optimal convergence rates of deep neural networks in a classiﬁcation\nsetting. Electronic Journal of Statistics 17(2), 3613–3659 (2023)\nMammen, E., Tsybakov, A.B.: Smooth discrimination analysis. The Annals of\nStatistics 27(6), 1808–1829 (1999)\nNeyshabur, B., Bhojanapalli, S., Srebro, N.: A pac-bayesian approach to spectrally-\nnormalized margin bounds for neural networks. In: International Conference on\nLearning Representations (2018)\nPolson, N.G., Roˇckov´a, V.: Posterior concentration for sparse deep learning. In:\nBengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., Garnett,\nR. (eds.) Advances in Neural Information Processing Systems, vol. 31. Curran\nAssociates, Inc., ??? (2018)\nRusso, D., Zou, J.: How much does your data exploration overﬁt? controlling bias\n19\nvia information usage. IEEE Transactions on Information Theory 66(1), 302–323\n(2019)\nSchmidt-Hieber, A.J.: Nonparametric regression using deep neural networks with relu\nactivation function. Annals of statistics 48(4), 1875–1897 (2020)\nShen, G., Jiao, Y., Lin, Y., Huang, J.: Approximation with cnns in sobolev space: with\napplications to classiﬁcation. Advances in Neural Information Processing Systems\n35, 2876–2888 (2022)\nShawe-Taylor, J., Williamson, R.: A PAC analysis of a Bayes estimator. In: Proceed-\nings of the Tenth Annual Conference on Computational Learning Theory, pp. 2–9.\nACM, New York (1997)\nSuzuki, T.: Fast generalization error bound of deep learning from a kernel perspective.\nIn: International Conference on Artiﬁcial Intelligence and Statistics, pp. 1397–1406\n(2018). PMLR\nSuzuki, T.: Adaptivity of deep relu network for learning in besov and mixed smooth\nbesov spaces: optimal rate and curse of dimensionality. In: The 7tth International\nConference on Learning Representations (2019)\nTsybakov, A.B.: Optimal aggregation of classiﬁers in statistical learning. The Annals\nof Statistics 32(1), 135–166 (2004)\nTsybakov, A.B.: Introduction to Nonparametric Estimation. Springer Series in Statis-\ntics, p. 214. Springer, ??? (2009). https://doi.org/10.1007/b13794 . Revised and\nextended from the 2004 French original, Translated by Vladimir Zaiats\nVapnik, V.N.: Statistical Learning Theory. Wiley, ??? (1998)\nVladimirova, M., Verbeek, J., Mesejo, P., Arbel, J.: Understanding priors in bayesian\nneural networks at the unit level. In: International Conference on Machine Learning,\npp. 6458–6467 (2019). PMLR\nWang, S., Shang, Z.: Minimax optimal high-dimensional classiﬁcation using deep\nneural networks. Stat 11(1), 482 (2022)\nZhang, C., Bengio, S., Hardt, M., Recht, B., Vinyals, O.: Understanding deep learning\n(still) requires rethinking generalization. Communications of the ACM 64(3), 107–\n115 (2021)\nZhang, T.: Statistical behavior and consistency of classiﬁcation methods based on\nconvex risk minimization. The Annals of Statistics 32(1), 56–85 (2004)\nZhang, T.: Information-theoretic upper and lower bounds for statistical estimation.\nIEEE Transactions on Information Theory 52(4), 1307–1321 (2006)\n20\n",
  "categories": [
    "math.ST",
    "stat.ML",
    "stat.TH"
  ],
  "published": "2024-05-02",
  "updated": "2024-05-02"
}