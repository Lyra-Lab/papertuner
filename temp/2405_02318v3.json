{
  "id": "http://arxiv.org/abs/2405.02318v3",
  "title": "Autoformalizing Natural Language to First-Order Logic: A Case Study in Logical Fallacy Detection",
  "authors": [
    "Abhinav Lalwani",
    "Tasha Kim",
    "Lovish Chopra",
    "Christopher Hahn",
    "Zhijing Jin",
    "Mrinmaya Sachan"
  ],
  "abstract": "Translating natural language into formal language such as First-Order Logic\n(FOL) is a foundational challenge in NLP with wide-ranging applications in\nautomated reasoning, misinformation tracking, and knowledge validation. In this\npaper, we introduce Natural Language to First-Order Logic (NL2FOL), a framework\nto autoformalize natural language to FOL step by step using Large Language\nModels (LLMs). Our approach addresses key challenges in this translation\nprocess, including the integration of implicit background knowledge. By\nleveraging structured representations generated by NL2FOL, we use\nSatisfiability Modulo Theory (SMT) solvers to reason about the logical validity\nof natural language statements. We present logical fallacy detection as a case\nstudy to evaluate the efficacy of NL2FOL. Being neurosymbolic, our approach\nalso provides interpretable insights into the reasoning process and\ndemonstrates robustness without requiring model fine-tuning or labeled training\ndata. Our framework achieves strong performance on multiple datasets. On the\nLOGIC dataset, NL2FOL achieves an F1-score of 78%, while generalizing\neffectively to the LOGICCLIMATE dataset with an F1-score of 80%.",
  "text": "Autoformalizing Natural Language to First-Order Logic:\nA Case Study in Logical Fallacy Detection\nAbhinav Lalwani1* Tasha Kim1* Lovish Chopra1* Christopher Hahn2 †\nZhijing Jin3,4,5,‡\nMrinmaya Sachan4,‡\n1Stanford University\n2X, the moonshot factory\n3Max Planck Institute for Intelligent Systems\n4ETH Zürich\n5University of Toronto\n{lalwani,tashakim,lovish}@stanford.edu\n{jinzhi,msachan}@ethz.ch\nAbstract\nTranslating natural language into formal lan-\nguage such as First-Order Logic (FOL) is\na foundational challenge in NLP with wide-\nranging applications in automated reasoning,\nmisinformation tracking, and knowledge val-\nidation. In this paper, we introduce Natural\nLanguage to First-Order Logic (NL2FOL), a\nframework to autoformalize natural language to\nFOL step-by-step using Large Language Mod-\nels (LLMs). Our approach addresses key chal-\nlenges in this translation process, including the\nintegration of implicit background knowledge.\nBy leveraging structured representations gen-\nerated by NL2FOL, we use Satisfiability Mod-\nulo Theory (SMT) solvers to reason about the\nlogical validity of natural language statements.\nWe present logical fallacy detection as a case\nstudy to evaluate the efficacy of NL2FOL. Be-\ning neurosymbolic, our approach also provides\ninterpretable insights into the reasoning pro-\ncess and demonstrates robustness without re-\nquiring model fine-tuning or labeled training\ndata. Our framework achieves strong perfor-\nmance on multiple datasets – on the LOGIC\ndataset, NL2FOL achieves an F1-score of 78%,\nwhile generalizing effectively to the LOGIC-\nCLIMATE dataset with an F1-score of 80%.1\n1\nIntroduction\nIn recent years, Large Language Models (LLMs)\nhave shown impressive advancements in under-\nstanding and generating natural language (Brown\net al., 2020). Despite this progress, their ability\nto tackle complex reasoning tasks remains limited\n(Bubeck et al., 2023; Wei et al., 2022). These chal-\nlenges are especially prevalent in multistep logical\ndeductions, abstract reasoning, and knowledge in-\ntegration in various domains (Dalvi et al., 2021;\nChen et al., 2024). Addressing these limitations\n*Equal contribution\n†Work done while at Stanford University\n‡Co-supervision\n1Code available at: github.com/lovishchopra/NL2FOL\nand improving the reasoning capabilities of LLMs\nhas become a critical focus in AI research (Halupt-\nzok et al., 2022; Gendron et al., 2024).\nIn contrast, formal reasoning tools such as Satis-\nfiability Modulo Theory (SMT) solvers excel in\nreasoning, providing rigorous, provable guarantees\nby leveraging symbolic representations and logi-\ncal calculus (Barrett et al., 2009; De Moura and\nBjørner, 2008). However, a key limitation of for-\nmal solvers is their reliance on structured logical\ninput, such as First Order Logic (FOL), which must\naccurately capture the semantics and context of\nnatural language statements (Beltagy et al., 2016).\nThis presents the challenge of translating unstruc-\ntured natural language into a structured form re-\nquired for formal reasoning while preserving essen-\ntial context and meaning.\nThis also brings a unique opportunity: if we can\nreliably translate natural language into structured\nlogical forms, we can harness the power of formal\nsolvers to reason systematically over natural lan-\nguage statements. However, achieving this transla-\ntion is nontrivial, as it involves accurately capturing\nnatural language semantics (Beltagy et al., 2016).\nMoreover, translating to a formal logical form may\ncause implicit and external context to be lost, which\nmust be reintroduced to ensure logical accuracy.\nTo address these challenges, we present NL2FOL,\na novel framework that bridges the gap be-\ntween natural language and formal reasoning sys-\ntems. NL2FOL employs a structured, step-by-step\npipeline to translate natural language inputs into\nfirst-order logic (FOL) representations, leveraging\nlarge language models (LLMs) at each step for en-\nhanced precision and adaptability. A distinguishing\nfeature of NL2FOL is its seamless integration of\nbackground knowledge into the generated logical\nforms, overcoming a major limitation of traditional\nformal logic frameworks - the inability to capture\narXiv:2405.02318v3  [cs.CL]  6 Mar 2025\nFallacy Name\nExample\nLogical Form\nFaulty Generalization\nSometimes flu vaccines don’t work; therefore\nvaccines are useless.\n(∃x ∈FluVaccines(DoesntWork(x)) ∧\n(FluVaccines ⊆Vaccines)) ⇒\n(∀y ∈Vaccines (DoesntWork(y)))\nFalse Causality\nEvery time I wash my car, it rains. Me washing\nmy car has a definite effect on the weather.\noccuredAfter(washingCar, rain)\n⇒\ncaused(washingCar, rain)\nAd Populum\nEveryone should like coffee: 95% of teachers do!\n(like(coffee, 95%Teachers))\n⇒\n(like(coffee, everyone))\nFalse Dilemma\nI don’t want to give up my car, so I don’t think I\ncan support fighting climate change.\n∀(a)(giveUpCar(a)\n∨\ndontSupportFightingClimateChange(a))\nTable 1: Sample logical fallacies from Jin et al. (2022) along with examples and their logical forms. For each type\nof fallacy, we show one possible logical form.\nimplicit information embedded in natural language.\nIn this paper, we demonstrate the effectiveness of\nNL2FOL through a case study on logical fallacy\ndetection, showcasing its ability to identify and\nexplain faulty reasoning in natural language argu-\nments. Detecting logical fallacies is particularly\nchallenging as they often rely on reasoning pat-\nterns that appear plausible yet are fundamentally\nflawed (Jin et al., 2022). To address this, NL2FOL\ntranslates logical fallacies from natural language\ninto FOL representations, enabling formal solvers\nto verify logical validity. These solvers generate\ncounterexamples and explanations, which are in-\nterpreted back into natural language to enhance\nhuman comprehensibility. By incorporating inter-\nmediate natural language outputs, our pipeline im-\nproves interpretability, transparency, and debugga-\nbility (Bai et al., 2020).\nWe show that our framework achieves strong perfor-\nmance on the logical fallacy detection benchmarks\nLOGIC and LOGICCLIMATE (Jin et al., 2022), with\nF1 scores of 78% and 80%, respectively - out-\nperforming existing models by 22% on the chal-\nlenge set, LOGICCLIMATE. These results high-\nlight NL2FOL as a generalizable and interpretable\ntool for reasoning tasks that demand the precision\nof formal reasoning systems. By analyzing the\nstrengths and weaknesses of LLMs at each step of\nthe NL2FOL pipeline, we further identify opportu-\nnities for improving logical reasoning capabilities.\nEven though LLMs prove to be effective in pars-\ning and generating logical representations for struc-\ntured inputs, they often struggle with ambiguities in\nnatural language and incorporating nuanced contex-\ntual knowledge. The ability to integrate symbolic\nsolvers with language models positions NL2FOL\nas a powerful neurosymbolic approach, bridging\nthe gap between formal reasoning and natural lan-\nguage understanding.\n2\nRelated Work\nLogical fallacy detection. Existing work on clas-\nsifying logical fallacies includes argument suffi-\nciency classification (Stab and Gurevych, 2017),\nad hominem fallacies from Reddit posts (Haber-\nnal et al., 2018b) and dialogues (Habernal et al.,\n2018a), rule parsers (Nakpih and Santini, 2020),\nstructure-aware Transformers (Jin et al., 2022),\nmultitask instruction based prompting (Alhindi\net al., 2022), and instance-based reasoning (Sourati\net al., 2022). To our knowledge, our work is the\nfirst on few-shot classification of logical fallacies\nin a step-by-step, explainable manner. By ensuring\nthat the reasoning process is transparent, we allow\nusers to understand and verify the system decision.\nNatural language to formal logic. While early\nwork on mapping text to formal logic relied heav-\nily on grammar-based approaches (Purdy, 1991;\nAngeli and Manning, 2014; MacCartney and Man-\nning, 2014), recent advances in deep learning and\nfoundation models have enabled new data-driven\ntechniques for translating natural language to lin-\near temporal logic (Cosler et al., 2023; Fuggitti and\nChakraborti, 2023; Liu et al., 2022) and first-order\nlogic (Singh et al., 2020; Yang et al., 2024; Hahn\net al., 2022). Neural models for parsing natural lan-\nguage to first-order logic (Singh et al., 2020; Yang\net al., 2024) and neuro-symbolic approach combin-\ning language models with first-order logic provers\n(Olausson et al., 2023) have since been explored.\nHowever, these approaches still face challenges in\naccurately capturing implicit information or trans-\nforming complex ambiguous sentences into logical\nform, mainly attributed to linguistic ambiguity.\nAly et al. (2023) integrated LLMs with logical in-\nference for fact verification, and while our method\nFigure 1: Overview of the proposed framework used for logical fallacy detection. Module A converts natural\nlanguage input to a first-order logic formula merged with contextual relationships, Module B compiles the negation\nof a given logical formula to an SMT file with well-defined sorts for variables and predicates, and Module C runs\nCVC on the SMT file and if the negation is satisfiable, interprets the counter-model in natural language.\nshares the fundamental idea of employing LLMs\nto construct proofs and analyze relationships be-\ntween textual spans, our task adds a layer of con-\ntextual reasoning by requiring the incorporation\nof background knowledge and maintaining interde-\npendency between proof steps, which is not present\nin approaches where each proof step is treated as\nan independent, isolated process.\nTheory solvers. Recent work by Hahn et al. (2022)\ndemonstrated the potential of integrating symbolic\nsolvers with large language models (LLMs), such\nas tool-augmented LLMs, to combine neural and\nsymbolic reasoning. While such approaches are\npromising, they often struggle to translate natural\nlanguage into symbolic representations and effec-\ntively capture background knowledge. Other recent\napproaches (Olausson et al., 2023; Pan et al., 2023)\nhave used theory solvers to logically reason with\nnatural language, which we build on with several\nkey advancements. First, we introduce a frame-\nwork that handles naturalistic, real-world data and\ntasks with ambiguous premises and conclusions.\nThen, we present a method to incorporate back-\nground knowledge into logical formulas. Finally,\nwe show that our approach introduces interpretabil-\nity by allowing human verification and modifica-\ntion throughout the intermediate reasoning steps.\n3\nMethodology\nAlthough powerful, LLMs struggle to detect logical\nfallacies in language, as it requires proper logical\nanalysis (Jin et al., 2022). On the other hand, SMT\nsolvers can reason over logical formulas with the-\noretical guarantees but require the input to be in a\nstructured, logical form. This approach combines\nthe strengths of both to classify logical fallacies.\nTask formulation. The task input is an argument in\nnatural language comprising one or more sentences,\nwhich is converted into formal logical form using\na chain of LLMs. Following this, an SMT solver\nprocesses the logical form and returns whether it is\nvalid. If invalid, the SMT solver provides a coun-\nterexample explaining why it is a logical fallacy,\nwhich is then interpreted with an LLM.\nFirst-order logic. In FOL, propositions are rep-\nresented using predicates that express properties\nor relations over objects in a domain. These predi-\ncates can be combined with constants, representing\nspecific objects and variables that represent unspec-\nified elements in the domain. An Interpretation\nassigns meaning to these symbols within a given\ncontext, while a Sort categorizes objects into differ-\nent types, facilitating precise reasoning about their\nproperties. Logical connectives of FOL, such as im-\nplication (⇒), universal quantifiers (∀), existential\nquantifiers (∃), and operators for conjunction/and\n(∧), disjunction/or (∨), and negation/not (¬), allow\nfor the construction of intricate statements.\nModule A: Natural language to first-order logic.\nOur approach for converting given natural lan-\nguage sentences into a logical form comprises mul-\ntiple steps involving few-shot prompting of LLMs:\n(i) decomposing a sentence into multiple smaller\nparts that can be represented in first-order logic,\n(ii) identifying relationships between different sub-\ncomponents to merge them and obtain a resultant\nlogical formula, and (iii) identifying real-world\nrelationships between these sub-components (back-\nground knowledge) and augmenting them to ob-\ntain a FOL formula by incorporating background\nknowledge in the statement. We demonstrate with\na Logical Fallacy (LF) and a Valid (V) example.\n1. LF Example: A Logical Fallacy Input\nI met a tall man who loved to eat cheese, now I\nbelieve all tall people like cheese.\n2. V Example: A Valid Input\nA boy is jumping on a skateboard in the middle of a\nred bridge. Thus the boy does a skateboarding trick.\nOur pipeline begins with a semantic decomposition\nmodule which decomposes natural language argu-\nments into respective claims and implications. Gen-\nerally, a sentence can be split into some claims and\nimplications based on those claims (see Prompt 2).\n1. LF Example: Claim and Implication Parser\nClaim: A tall man loved to eat cheese.\nImplication: All tall people like cheese.\n2. V Example: Claim and Implication Parser\nClaim: A boy is jumping on a skateboard in the\nmiddle of a red bridge.\nImplication: The boy does a skateboarding trick.\nThe claims and implications are split into further\nsub-components and used to build up the logical\nform of the sentence. The next step is to identify\nentities in the sentence. In our work, we treat noun\nphrases or surrogates for noun phrases as entities\n(see Prompt 3). Then, we find the relationship\nbetween the different entities using Zero-Shot clas-\nsification via Natural Language Inference (NLI).\nThese relationships (e.g., subset, equality, not re-\nlated) are generally helpful in deciding appropriate\nquantifiers in the logical form. For example, if the\nentities are man and people, then it can be inferred\nthat man is a subset of people and that the man\nwould be bound by an existential quantifier in the\nsentence x (see Prompt 4).\n1. LF Example: Entity Extractor\nReferring expressions:\n• man: x\n• cheese: c\n• people: y\n• x ⊆y\n2. V Example: Entity Extractor\nReferring expressions:\n• boy: b\n• skateboard: s\n• bridge\n• skateboardingTrick: y\nThe other set of sub-components are properties,\nwhich describe a trait of a referring expression or\nrelationship between multiple referring expressions.\nThese properties are predicates in first-order logic.\nWe use a single module to extract the properties and\nthe relation between properties and entities. (see\nPrompt 5). We also find the relationships between\nvarious properties (see Prompt 6). For instance,\nin the LF Example, it can be inferred that Like\nand Love are contextually similar. Similarly, in our\nvalid example, jumping over skateboard implies do-\ning a skateboard trick. These relationships provide\nan additional context that is not directly present in\nthe statement.\nTo identify these contextual relationships, we run\nNLI between each pair of properties, i.e., by setting\none property as the hypothesis and the other as the\npremise as the input to the NLI model. If we find\nthat any one property entails the other, we add the\nrelationship property1 ⇒property2 to our con-\ntext. Before running the NLI model between a pair\nof properties, we replace the variables in each prop-\nerty with the referring expressions that they repre-\nsent. This adds additional context that helps the\nNLI model identify relations. For instance, in the\nV Example, the NLI model is unable to find the re-\nlation between JumpsOn(x, s) and Does(x, y), but it\ncan identify the relationship between JumpsOn(boy,\nskateboard) and Does(boy, skateboardingTrick).\n1. LF Example: Property Extractor + Background\nKnowledge Retriever\nProperties: Tall, Love, Like\nProperty entity relations: Tall(x), Love(x, c)\nBackground knowledge:\n1. ∀x(Like(x, c) ⇒Love(x, c))\n2. ∀x(Love(x, c) ⇒Like(x, c))\n3. x ⊆y\n2.\nV Example: Property Extractor + Background\nKnowledge Retriever\nProperties: JumpsOn, inMiddleOf, Red, Does\nProperty entity relations: JumpsOn(b, s),\nRed(bridge), inMiddleOf(b, bridge), Does(b, y)\nBackground knowledge:\n1. ∀x(JumpsOn(b, s) ⇒Does(b, y))\nFinally, we combine all of this information using\nthe relationships between properties and entities to\nobtain the FOL form of the sentence with the help\nof an LLM (see Prompt 7). For a logical fallacy, the\nnegation of the formula is expected to be satisfiable.\nOn the contrary, for a valid statement, the negation\nof the formula should be unsatisfiable.\n1. LF Example: NL2FOL Output\nFirst-order logic: ((∀x(Like(x, c) ⇒Love(x, c)))∧\n(∀x(Love(x, c)\n⇒\nLike(x, c)))∧\n(∃x(Tall(x) ∧\nLove(x, c)))) ⇒(∀y(Tall(y) ⇒Like(y, c)))\n2. V Example: NL2FOL Output\nFirst-order\nlogic:\n(∀x(JumpsOn(x, s)\n⇒\nDoes(x, y)) ∧Red(bridge)∧inMiddleOf(b, bridge) ∧\nJumpsOn(b, s)) ⇒Does(b, y)\nModule B: First-order logic to SMT. The next\nstep involves automatically creating an SMT file\nfor the negation of the first-order logical formula\ngenerated. While one can easily write an SMT\nfile for a logical formula manually, generating one\nautomatically for an arbitrary formula has not been\ndone before. Thus, we develop a compiler that\nparses a given logical formula and converts it into\nan SMT file that can be given to CVC as input, as\ndescribed in Algorithm 1 (See Appendix).\nModule C: Interpreting SMT results. To ver-\nify the validity of the logical formulas, we utilize\nan SMT solver, CVC4 (Barrett et al., 2011). The\nsolver determines whether the formula is valid or\ninvalid, hence a logical fallacy. In the case of in-\nvalidity, the model provides a counterexample to\nthe original logical formula, which shows that the\ngiven claim or implication is a logical fallacy.\nExample (Module B Output):\nI met a tall man who loved to eat cheese, now I\nbelieve all tall people like cheese.\n↓\nFirst-order logic: ((∀x(Like(x, c) ⇒Love(x, c)))∧\n(∀x(Love(x, c)\n⇒\nLike(x, c)))∧\n(∃x(Tall(x) ∧\nLove(x, c)))) ⇒(∀y(Tall(y) ⇒Like(y, c)))\n↓\nSMT classification: Logical fallacy\nExplanation: Counterexample\n↓\n• John is tall (Tall(John) is True). John likes\ncheese (Likes(John, Cheese) is True).\n• Jane is tall (Tall(Jane) is True). No constraint\nJane likes cheese.\nTherefore, there exists a tall person (John) who likes\ncheese, but it does not follow that all tall people like\ncheese, since Jane serves as a counterexample.\nFigure 2: Example of logical fallacy detection using\nNL2FOL. The resulting classification is explained using\na counterexample generated by the SMT solver.\nThe result of the SMT solver is hard to interpret, as\nit uses technical terminology generally only well\nunderstood by those who are familiar with CVC4\nand SMT. To obtain an explanation in natural lan-\nguage, we prompt an LLM with the claim, impli-\ncation, referring expressions, properties, FOL for-\nmula, and the counterexample generated by CVC4.\nThe model then interprets the counterexample with\nnatural language, as depicted in Figure 2.\n4\nExperiments\nWe evaluate our approach on both logical falla-\ncies (positive class) and valid statements (negative\nclass). For logical fallacies, we use the LOGIC and\nLOGICCLIMATE (Jin et al., 2022) datasets, origi-\nnally designed for training models to identify and\nclassify different fallacies. These datasets contain\nexamples of logical fallacies, each labeled with\nmultiple categories from 13 different categories,\nincluding faulty generalization, circular claim, and\nad hominem. The LOGIC dataset contains 2,449\nexamples of common logical fallacies collected\nmostly from quiz websites. The LOGICCLIMATE\ndataset comprises 1,079 examples of logical falla-\ncies drawn from climate change news articles on\nthe Climate Feedback platform. It is intended to\ntest the model’s ability to generalize out-of-domain.\nTo test our approach with valid statements, we use\nthe Stanford Natural Language Inference (SNLI)\ncorpus (Bowman et al., 2015), which supports the\ndevelopment of natural language inference sys-\ntems. This dataset features over 570,000 human-\nannotated sentence pairs, where each pair consists\nof a premise and a hypothesis labeled as entailment,\ncontradiction, or neutral. We focus on the entail-\nment class in this study, extracting over 170,000\nsentence pairs where the premise entails the hypoth-\nesis. We construct valid sentences by combining\nthe premise and hypothesis into a single sentence.\nThe task is set up as a simple binary classification\ntask, where the input consists of sentences drawn\nfrom the LOGIC or LOGICCLIMATE datasets la-\nbeled as logical fallacies or from the SNLI dataset\nlabeled as valid sentences. Here, we treat logical\nfallacies as the positive class. To ensure a balanced\nevaluation, we select an equal number of fallacies\nand valid statements, allowing for a fair comparison\nacross both classes. Finally, our model is evaluated\non standard binary classification metrics such as\nprecision, recall, f1 score, and accuracy.\nModels. We compare our method to pretrained\nlanguage models, including Llama2-7B (Touvron\net al., 2023), GPT4o-mini (OpenAI, 2024), GPT4o\n(OpenAI et al., 2024a) and OpenAI o1-preview\n(OpenAI et al., 2024b) with few-shot in-context\nexamples (see Prompt 1). We also run NL2FOL\nwith each of the above models used for the LLM\nprompting stages. Llama2-7B was chosen for our\nexperiments as it had the best performance dur-\ning testing over an initial subset of the data, out-\nperforming Llama3.1-8B (Grattafiori et al., 2024),\nLlama3.2-11B (AI, 2024a), and Ministral-8B (AI,\n2024b). We evaluate BART (140M parameters)\n(Lewis et al., 2020) finetuned on MNLI (Williams\nLOGIC\nLOGICCLIMATE\nModel\nMethod\nAcc.\nP.\nR.\nF1\nAcc.\nP.\nR.\nF1\nLlama-7B\nEnd-to-end\n0.41\n0.45\n0.82\n0.58\n0.31\n0.38\n0.62\n0.47\nNL2FOL (Ours)\n0.63\n0.58\n0.92\n0.71\n0.66\n0.60\n0.94\n0.73\nGPT-4o-mini\nEnd-to-end\n0.91\n0.94\n0.88\n0.91\n0.64\n0.67\n0.55\n0.60\nNL2FOL (Ours)\n0.70\n0.64\n0.91\n0.75\n0.73\n0.66\n0.93\n0.77\nGPT-4o\nEnd-to-end\n0.96\n0.96\n0.96\n0.96\n0.70\n0.95\n0.42\n0.58\nNL2FOL (Ours)\n0.78\n0.76\n0.82\n0.78\n0.80\n0.80\n0.80\n0.80\nOpenAI o1-preview\nEnd-to-end\n0.93\n0.89\n0.98\n0.93\n0.73\n0.84\n0.56\n0.67\nNL2FOL (Ours)\n-\n-\n-\n-\n-\n-\n-\n-\nTable 2: Comparison of few-shot model performance metrics (abbreviations: Acc. = accuracy, P. = precision, R. =\nrecall, F1 = F1 score) on the LOGIC+SNLI and LOGICCLIMATE+SNLI datasets using End-to-end vs. NL2FOL\n(Ours). Results on NL2FOL with o1-preview are omitted as o1-preview failed to complete the pipeline in most\ncases, likely due to its poor instruction following capabilities.\net al., 2018) to analyze the relationships between\nproperties and referring expressions. We ran the\nexperiments on a V100 GPU, with one run costing\naround 2 GPU hours.\nPrompt tuning. For prompt tuning, 20 samples\nfrom the LOGIC dataset were selected and manu-\nally annotated with intermediate and final results.\nThey were then split into 10 train and 10 valida-\ntion examples. For each prompt, we start with a\nsimple description of the task. 4-6 examples were\nrandomly selected from the train set as in-context\nexamples, with the relevant intermediate outputs\ndepending on the stage. Results were tested on the\nvalidation examples, and the prompt was updated\nto address common mistakes. To ensure fairness, a\nfixed number of 5 improvement iterations was used\nfor each prompt, and the one showing best perfor-\nmance over the validation examples was chosen.\n5\nResults and Discussion\nAs shown in Table 2, our method achieves an\nF1 score of 78% when used with GPT-4o on the\nLOGIC dataset. When run end-to-end, the Llama-\n7B model reached an F1 score of only 58%, but\nwhen used with the NL2FOL pipeline, reached\na score of 71%. Although end-to-end classifica-\ntion has shown better performance in other models,\ncomparisons can be skewed because they may have\nbeen exposed to the LOGIC dataset and its labels\nduring training because this dataset was compiled\nfrom publicly accessible web sources. On average,\nNL2FOL demonstrated high recall, whereas end-\nto-end classification demonstrated high precision.\nOur challenge set LOGICCLIMATE+SNLI contains\nreal-world logical fallacies from climate change\nnews. Since this dataset was used to test gener-\nalization, the in-context examples we provide to\nall models are from the LOGIC dataset. NL2FOL\nyields results that are highly similar to the results\nfrom LOGIC, whereas end-to-end classification saw\na drop in performance. This demonstrates that\nour system is also robust and adapts well to real-\nworld texts, including texts with significant domain-\nspecific context. This makes it effective in detect-\ning and mitigating misinformation. Specifically,\non this dataset, we find that NL2FOL outperforms\ndirect translation with all LLMs that we tested.\n5.1\nQuantitative Analysis\nError analysis and interpretability. The pro-\nposed method is interpretable due to the use of\nnatural language inputs and outputs at each step\nof the pipeline. This structure allows for precise\nidentification of the specific module responsible\nfor a failure by examining intermediate results. To\nevaluate this aspect, we performed an in-depth er-\nror analysis by annotating the module responsible\nfor failure in 100 incorrect predictions made by the\nmodel. The results are summarized in Table 4.\nOur analysis reveals that the majority of errors oc-\ncur in the ‘Background Knowledge Retriever’, in-\nvolving missed or incorrectly added contextual in-\nformation in the logical form. Other errors typically\npertain to incorrect identification of claims, impli-\ncations, or properties. In contrast, inaccuracies in\nthe generation of logical forms are relatively infre-\nquent, suggesting that the model performs well in\nconstructing accurate logical representations when\nprovided with reliable information about the con-\nstituent entities and properties within a sentence.\nThis finding underscores the importance of improv-\ning the background knowledge retriever module to\nimprove overall model performance.\nType Sentence\nLogical Form\nPrediction\n1\nLF\nX has been around\nfor years now. Y is\nnew. Therefore, Y\nis better than X.\n(IsNew(a) ∧∼IsNew(b)) ⇒(IsBetterThan(a,b))\nLF: Correct prediction\n2\nLF\nEveryone is doing\nthe Low-Carb Diet.\n(∃b (∃a (IsDoing(b,a)))) ⇒(∃c (∃a (IsDo-\ning(c,a)))).\nV: Incorrect prediction - Wrong\ntranslation given when no claim\ngiven\n3\nV\nTwo dogs are fight-\ning in a field. Con-\nsequently, the two\ndogs are outside.\n(∃b (∃a (IsFighting(a, b) ∧IsInField(b) ∧IsIn-\nField(b)))) ⇒(∃a (IsOutside(a)))\nLF: Incorrect prediction - Missing\nsemantic ground truth claim: ∀a\n(IsInField(a) ⇒IsOutside(a))\n4\nV\nA baseball player\ngets ready to catch\na\nfly\nball\nnear\nthe outfield fence.\nTherefore, a person\nis playing baseball\noutdoors.\n(∃a (IsGettingReady(a) ∧(IsABaseballPlayer(a)\n∧\nIsCatchingFlyBall(a)\n∧\nIsNearOutfield-\nFence(a))) ∧(∀e ( IsABaseballPlayer(e) ⇒\nIsPlayingBaseball(e))) ∧(∀f ( IsPlayingBase-\nball(f) ⇒IsABaseballPlayer(f))) ∧(∀g (\nIsNearOutfieldFence(g)\n⇒\nIsOutdoors(g))))\n⇒(∃c (∃a (IsPlayingBaseball(a) ∧IsOut-\ndoors(c))))\nV: Correct Prediction - The method\nidentifies additional context by es-\ntablishing relationships such as Is-\nBaseballPlayer implying IsPlaying-\nBaseball, and IsNearOutfieldFence\nimplying IsOutdoors.\n5\nV\nA woman sits alone\non a park bench in\nthe sun. Hence, a\nwoman is in a park.\n(IsSittingOn(a, b) ∧isParkBench(b) ∧IsIn-\nSun(a)) ⇒(IsInPark(a)).\nLF: Incorrect prediction - Miss-\ning semantic ground truth claim:\n∀a∀b (IsSittingOn(a, b) ∧isPark-\nBench(b) ⇒IsInPark(a))\n6\nV\nA woman is stand-\ning at a podium.\nThus, a person is at\na podium.\n(∃a∃b (IsStandingAt(b, a)) ∧∀f∀e∀d (IsStandin-\ngAt(d,e) ⇒IsAt(f,e)) ⇒∃c∃a (IsAt(c, a))\nV: Correct prediction - The method\nidentifies additional context by\nestablishing the relationship Is-\nStandingAt implying IsAt.\nTable 3: Some example outputs of our model (abbreviations: LF = Logical Fallacy, V = Valid statement)\nSub-Module with Error\nError Proportion\nClaim and Implication Parser\n0.19\nIncorrect Label\n0.01\nProperty Extractor\n0.13\nBackground Knowledge Retriever\n0.54\nFOL Formulation Engine\n0.13\nTable 4: Categorization of model errors by type on\nNL2FOL (GPT-4o), based on a review by domain ex-\nperts in the logic of 100 randomly sampled examples\nImpact of adding background knowledge to\nNL2FOL. Based on the error analysis, missing or\nincorrect background knowledge was a significant\ncontributor to incorrect predictions of our method.\nTo quantitatively assess the impact of grounding\non model performance, we evaluated several ap-\nproaches for NLI in the Background Relation Ex-\ntractor. These included: (a) a pipeline without any\nbackground knowledge as a baseline, (b) a model\nwithout context where the LLM (GPT4o) only pro-\ncesses the input properties, (c) an LLM that incor-\nporates both the input sentence and properties and\n(d) a smaller model specifically fine-tuned for NLI\n(BART-MNLI). Results are presented in Table 5.\nWe see that precision and recall both improve sig-\nnificantly with better grounding techniques. The\nLLM model with sentence context achieves the\nhighest overall performance. This is likely due to\nthe sentence context providing information about\nclauses that are omitted due to the choice of rep-\nresentation in FOL. This indicates that integrating\nrobust grounding mechanisms is critical to enhanc-\ning the accuracy and reliability of the method.\nLOGIC+SNLI\nLOGICCLIMATE+SNLI\nMethod\nAcc.\nP.\nR.\nF1\nAcc.\nP.\nR.\nF1\n(a) No Grounding\n0.54 0.52 0.88 0.66 0.57 0.54 0.94\n0.69\n(b) LLM\n0.76 0.78 0.74 0.75 0.79 0.80 0.78\n0.79\n(c) LLM w/ context 0.78 0.76 0.82 0.78 0.80 0.80 0.80\n0.80\n(d) BART-MNLI\n0.71 0.71 0.70 0.70 0.77 0.81 0.71\n0.77\nTable 5: Comparison of different grounding methods on\nNL2FOL (GPT4o-mini) across the LOGIC+SNLI and\nLogicClimate+SNLI datasets\nImpact of using an SMT solver. To assess the\nimpact of using an SMT solver in our pipeline, we\ncompared its performance against an LLM as a\nbaseline for classifying the logical forms as valid\nor fallacies. The results, summarized in Table 6,\ndemonstrate a significant improvement in perfor-\nmance metrics with the integration of the SMT\nsolver. Results reveal the SMT-based approach sig-\nnificantly outperforms the LLM-based approach\nin all metrics across both the LOGIC and LOGIC-\nCLIMATE datasets. This underscores the advantage\nof formal reasoning systems like SMT solvers for\ntasks requiring precise logical inference and struc-\ntured reasoning compared to LLMs, which may\nlack systematic consistency in such contexts.\n5.2\nQualitative Analysis\n5.2.1\nSuccess Modes of NL2FOL\nS1: Captures implicit information not men-\ntioned in premises. Previous works that directly\ntranslate natural language to logical forms suffer\nfrom an inability to capture implicit information\nnot mentioned in the premises (Olausson et al.,\n2023). Our “Background Knowledge Retriever”\nstep allows us to capture this information in the\nfinal logical form. An illustration of this can be\nfound in Example 4 of Table 3.\nS2: Captures explicit information that is missed\nin the representation. Our pipeline is also able to\ncapture information that is explicitly mentioned in\nthe premises but missed due to the choice of repre-\nsentation in logical form. In Example 6, in Table 3,\nthe fact that the woman is both standing and is at\nthe podium is lost due to the choice representation\nIsStandingAt. However, the fact that the woman\nis at the podium is recovered in the final logical\nform due to the identified background knowledge\nIsStandingAt implies IsAt.\nS3: Comparison to direct translation. To evalu-\nate the efficacy of the multi-step LLM pipeline, we\ncompared it against a direct translation approach,\nwhere natural language inputs were converted into\nlogical forms with a single LLM call using a few-\nshot prompt. However, this task proved to be exces-\nsively complex for LLMs. Llama failed to generate\nany output, citing an inability to comprehend the\nprompt. Larger LLMs exhibited significant limi-\ntations, with over 95% of their outputs containing\nsyntax errors. These findings highlight the inad-\nequacy of direct translation for complex logical\nreasoning tasks and underscore the necessity of a\nstructured, multi-step approach to ensure the accu-\nracy and syntactic correctness of the logical form.\n5.2.2\nFailure Modes of NL2FOL\nF1: Misses some background knowledge. As can\nbe observed in Table 4, incorrect identification of\nbackground knowledge is the most common cause\nfor incorrect classifications. This is because any\ngaps in background knowledge can cause a valid\nstatement to be identified as a logical fallacy, and\nLOGIC\nLOGICCLIMATE\nClassifier Acc.\nP.\nR.\nF1 Acc.\nP.\nR.\nF1\nSMT\n0.78 0.76 0.82 0.78 0.80 0.80 0.80 0.80\nGPT-4o\n0.69 0.71 0.62 0.66 0.73 0.72 0.74 0.73\nTable 6: Comparison of classification methods used with\nNL2FOL (GPT4o) on LOGIC and LOGICCLIMATE\nan incorrectly added clause can cause a fallacy to\nbe identified as valid. One such case is present in\nexample 3 of the Table 3. In this case, the model\nis not able to identify the extra context statement\nbecause the NLI model does not identify a required\nground-truth relation. If this context were to be\nadded to the claim of the logical formula, then the\nstatement would have been predicted to be valid.\nF2: Limitations of NLI. Our current approach\nis limited to discerning relationships between two\nproperties at a time rather than handling multiple\nrelationships concurrently. For reference, consider\nExample 5 in Table 3. Here, the semantic claim\ninvolves the conjunction of two properties entail-\ning the third, while the ‘Background Knowledge\nRetriever’ only checks whether one property en-\ntails the other. Finding such complex extra context\nrequires more advanced techniques or additional\nhuman intervention. Including them could further\nimprove the precision of the model overall.\nF3: Imprecision of LLMs. Among the logical\nfallacies that our model incorrectly predicted to\nbe a valid statement, most of these predictions\nfailed due to the imprecision of the LLM, leading to\nfalse translations and incorrect results. Example 2\ndemonstrates a case where the input does not have\nany claim but instead jumps straight to an implica-\ntion. However, the model is not able to identify that\nthe example has no claim. As a result, we obtain\nan incorrect translation with our technique.\n6\nConclusion\nWe present an effective and automatic solution to\ndetect fallacies and tackle misinformation. We\ndeveloped a strategy to distinguish logical falla-\ncies from valid statements, involving a chaining\napproach to convert a sentence to first-order logic\nusing LLMs, followed by using SMT solvers to\nidentify whether the first-order logical statement is\nvalid or not. If not, we interpret the counter-model\ngenerated by the SMT solver in natural language.\nOur proposed technique shows promising results\nin identifying logical fallacies and valid statements,\nas well as good generalizability across domains.\nEthics Statement\nWhile the intended outcome of this research is to\nhelp fight misinformation and promote rational dis-\ncourse, there are several ethical challenges that we\nmust consider. First, dependence on AI to identify\nlogical fallacies could influence how individuals\nengage in debates and discussions. There is a risk\nthat people may over-rely on AI judgments, po-\ntentially stifling complex statements or dissenting\nopinions that are essential for a healthy democratic\nprocess. Moreover, the use of AI in moderating dis-\ncussions, especially in identifying logical fallacies,\nraises ethical questions about the automation of\ncontent moderation. While it can enhance the qual-\nity of public discourse by filtering out fallacious\nstatements, it also risks automating censorship and\nimpacting the dynamics of online communities. In\nthe wrong hands, logical fallacy detection tools\ncould be exploited to silence speech or suppress\nviewpoints under the pretext of promoting rational\ndiscourse. This potentially allows governments or\norganizations to stifle opposition or critique.\nTo address these issues, we advocate for the devel-\nopment of ethical guidelines for AI use that empha-\nsize transparency, accountability, and active user\nengagement. These measures are crucial in encour-\naging public literacy in AI and logical fallacies, ulti-\nmately empowering individuals to critically assess\nboth AI output and arguments they may encounter.\nLimitations\nScope of logical reasoning tasks. Correct iden-\ntification of background knowledge is crucial for\nour method. While we have shown its potential in\ndetecting logical fallacies for short and structured\npremises, it is important to note that this approach\nmay miss complex relational constructs (for exam-\nple, (a ∧b) ⇒(c ∨d))), in which richer logical\npatterns may often be required in real-world reason-\ning tasks such as those present in multi-paragraph\ncontexts or Question-Answering (QA) datasets.\nGeneralizability to other tasks and domains. We\nhave demonstrated promising results of our ap-\nproach to logical fallacy detection, but whether\nthe findings generalize to other logical tasks and\ndomains remains unexplored. The performance of\nour approach in other languages is untested and\nmay introduce unforeseen challenges.\nGoing beyond first-order logic. It is unknown\nwhether our approach would be sufficiently expres-\nsive for reasoning tasks requiring higher-order or\nnon-classical logic, as we limit our exploration\nto first-order logic. Conceptually, extending our\nmethod to the aforementioned domains is feasible\nbut would require modification to the SMT integra-\ntion and LLM-driven logic translation processes.\nThus, further testing may include translating to\nlogic beyond FOL, such as temporal and higher-\norder logic.\nComputational cost.\nUsing LLMs and SMT\nsolvers can incur high computational costs, such as\nhigh-performance GPUs for LLM inference, CPUs\noptimized for SMT solvers, and high API usage,\nparticularly for models like GPT-o1 and Llama-7B.\nReferences\nMeta AI. 2024a. Llama 3.2-11b model card. Accessed:\n2025-02-15.\nMistral AI. 2024b. Ministral-8b-instruct-2410 model\ncard. Accessed: 2025-02-15.\nTariq Alhindi, Tuhin Chakrabarty, Elena Musi, and\nSmaranda Muresan. 2022. Multitask instruction-based\nprompting for fallacy recognition. In Proceedings of\nthe 2022 Conference on Empirical Methods in Natural\nLanguage Processing, pages 8172–8187, Abu Dhabi,\nUnited Arab Emirates. Association for Computational\nLinguistics.\nRami Aly, Marek Strong, and Andreas Vlachos. 2023.\nQA-natver: Question answering for natural logic-based\nfact verification. In The 2023 Conference on Empirical\nMethods in Natural Language Processing.\nGabor Angeli and Christopher D Manning. 2014. Natu-\nralli: Natural logic inference for common sense reason-\ning. In Proceedings of the 2014 Conference on Empiri-\ncal Methods in Natural Language Processing (EMNLP),\npages 534–545.\nBing Bai, Jian Liang, Guanhua Zhang, Hao Li, Kun\nBai, and Fei Wang. 2020. Why attentions may not be\ninterpretable? Proceedings of the 27th ACM SIGKDD\nConference on Knowledge Discovery & Data Mining.\nClark Barrett, Christopher L. Conway, Morgan Deters,\nLiana Hadarean, Dejan Jovanovi´c, Tim King, Andrew\nReynolds, and Cesare Tinelli. 2011. Cvc4. In Computer\nAided Verification, pages 171–177, Berlin, Heidelberg.\nSpringer Berlin Heidelberg.\nClark Barrett, Aaron Stump, and Cesare Tinelli. 2009.\nSatisfiability modulo theories. Communications of the\nACM, 52(9):69–77.\nIz Beltagy, Stephen Roller, Pengxiang Cheng, Katrin\nErk, and Raymond J. Mooney. 2016.\nRepresenting\nmeaning with a combination of logical and distributional\nmodels. Computational Linguistics, 42(4):763–808.\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large annotated\ncorpus for learning natural language inference. In Pro-\nceedings of the 2015 Conference on Empirical Methods\nin Natural Language Processing, pages 632–642, Lis-\nbon, Portugal. Association for Computational Linguis-\ntics.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen\nKrueger, Tom Henighan, Rewon Child, Aditya Ramesh,\nDaniel Ziegler, Jeffrey Wu, Clemens Winter, Chris\nHesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott\nGray, Benjamin Chess, Jack Clark, Christopher Berner,\nSam McCandlish, Alec Radford, Ilya Sutskever, and\nDario Amodei. 2020. Language models are few-shot\nlearners. Advances in Neural Information Processing\nSystems, 33:1877–1901.\nSébastien Bubeck, Varun Chandrasekaran, Ronen Eldan,\nJohannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee,\nYin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori,\nHamid Palangi, Marco Tulio Ribeiro, and Yi Zhang.\n2023. Sparks of artificial general intelligence: Early\nexperiments with gpt-4.\nZixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji,\nand Quanquan Gu. 2024. Self-play fine-tuning converts\nweak language models to strong language models.\nMatthias Cosler, Christopher Hahn, Daniel Mendoza,\nFrederik Schmitt, and Caroline Trippel. 2023. nl2spec:\nInteractively translating unstructured natural language\nto temporal logics with large language models. In Com-\nputer Aided Verification. CAV 2023. Lecture Notes in\nComputer Science, volume 13965, Cham. Springer.\nBhavana Dalvi, Peter Jansen, Oyvind Tafjord, Zheng-\nnan Xie, Hannah Smith, Leighanna Pipatanangkura, and\nPeter Clark. 2021. Explaining answers with entailment\ntrees. In Proceedings of the 2021 Conference on Em-\npirical Methods in Natural Language Processing, pages\n7358–7370, Online and Punta Cana, Dominican Repub-\nlic. Association for Computational Linguistics.\nLeonardo De Moura and Nikolaj Bjørner. 2008. Z3: an\nefficient smt solver. In Proceedings of the Theory and\nPractice of Software, 14th International Conference on\nTools and Algorithms for the Construction and Analy-\nsis of Systems, TACAS’08/ETAPS’08, page 337–340,\nBerlin, Heidelberg. Springer-Verlag.\nFrancesco Fuggitti and Tathagata Chakraborti. 2023.\nNl2ltl - a python package for converting natural lan-\nguage (nl) instructions to linear temporal logic (ltl) for-\nmulas. In AAAI Conference on Artificial Intelligence.\nGaël Gendron, Qiming Bao, Michael Witbrock, and\nGillian Dobbie. 2024. Large language models are not\nstrong abstract reasoners.\nAaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri,\nAbhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle,\nAiesha Letman, Akhil Mathur, Alan Schelten, Alex\nVaughan, Amy Yang, Angela Fan, Anirudh Goyal, An-\nthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sra-\nvankumar, Artem Korenev, Arthur Hinsvark, Arun Rao,\nAston Zhang, Aurelien Rodriguez, Austen Gregerson,\nAva Spataru, Baptiste Roziere, Bethany Biron, Binh\nTang, Bobbie Chern, Charlotte Caucheteux, Chaya\nNayak, Chloe Bi, Chris Marra, Chris McConnell,\nChristian Keller, Christophe Touret, Chunyang Wu,\nCorinne Wong, Cristian Canton Ferrer, Cyrus Niko-\nlaidis, Damien Allonsius, Daniel Song, Danielle Pintz,\nDanny Livshits, Danny Wyatt, David Esiobu, Dhruv\nChoudhary, Dhruv Mahajan, Diego Garcia-Olano,\nDiego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab\nAlBadawy, Elina Lobanova, Emily Dinan, Eric Michael\nSmith, Filip Radenovic, Francisco Guzmán, Frank\nZhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis\nAnderson, Govind Thattai, Graeme Nail, Gregoire Mi-\nalon, Guan Pang, Guillem Cucurell, Hailey Nguyen,\nHannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov,\nImanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra,\nIvan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, Jan\nGeffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet\nShah, Jelmer van der Linde, Jennifer Billock, Jenny\nHong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu\nHuang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bit-\nton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua\nJohnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden\nAlwala, Karthik Prasad, Kartikeya Upasani, Kate Plaw-\niak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid\nEl-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu,\nKunal Bhalla, Kushal Lakhotia, Lauren Rantala-Yeary,\nLaurens van der Maaten, Lawrence Chen, Liang Tan,\nLiz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo,\nLukas Blecher, Lukas Landzaat, Luke de Oliveira,\nMadeline Muzzi, Mahesh Pasupuleti, Mannat Singh,\nManohar Paluri, Marcin Kardas, Maria Tsimpoukelli,\nMathew Oldham, Mathieu Rita, Maya Pavlova, Melanie\nKambadur, Mike Lewis, Min Si, Mitesh Kumar Singh,\nMona Hassan, Naman Goyal, Narjes Torabi, Nikolay\nBashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning\nZhang, Olivier Duchenne, Onur Çelebi, Patrick Al-\nrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter\nWeng, Prajjwal Bhargava, Pratik Dubal, Praveen Krish-\nnan, Punit Singh Koura, Puxin Xu, Qing He, Qingx-\niao Dong, Ragavan Srinivasan, Raj Ganapathy, Ra-\nmon Calderer, Ricardo Silveira Cabral, Robert Stojnic,\nRoberta Raileanu, Rohan Maheswari, Rohit Girdhar, Ro-\nhit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan\nSumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang,\nSaghar Hosseini, Sahana Chennabasappa, Sanjay Singh,\nSean Bell, Seohyun Sonia Kim, Sergey Edunov, Shao-\nliang Nie, Sharan Narang, Sharath Raparthy, Sheng\nShen, Shengye Wan, Shruti Bhosale, Shun Zhang,\nSimon Vandenhende, Soumya Batra, Spencer Whit-\nman, Sten Sootla, Stephane Collot, Suchin Gururan-\ngan, Sydney Borodinsky, Tamar Herman, Tara Fowler,\nTarek Sheasha, Thomas Georgiou, Thomas Scialom, To-\nbias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal\nKarn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ra-\nmanathan, Viktor Kerkez, Vincent Gonguet, Virginie\nDo, Vish Vogeti, Vítor Albiero, Vladan Petrovic, Wei-\nwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers,\nXavier Martinet, Xiaodong Wang, Xiaofang Wang, Xi-\naoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia,\nXuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yas-\nmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue\nLi, Yuning Mao, Zacharie Delpierre Coudert, Zheng\nYan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh,\nAayushi Srivastava, Abha Jain, Adam Kelsey, Adam\nShajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva\nGoldstand, Ajay Menon, Ajay Sharma, Alex Boesen-\nberg, Alexei Baevski, Allie Feinstein, Amanda Kallet,\nAmit Sangani, Amos Teo, Anam Yunus, Andrei Lupu,\nAndres Alvarado, Andrew Caples, Andrew Gu, Andrew\nHo, Andrew Poulton, Andrew Ryan, Ankit Ramchan-\ndani, Annie Dong, Annie Franco, Anuj Goyal, Apara-\njita Saraf, Arkabandhu Chowdhury, Ashley Gabriel,\nAshwin Bharambe, Assaf Eisenman, Azadeh Yazdan,\nBeau James, Ben Maurer, Benjamin Leonhardi, Bernie\nHuang, Beth Loyd, Beto De Paola, Bhargavi Paranjape,\nBing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram\nWasti, Brandon Spence, Brani Stojkovic, Brian Gamido,\nBritt Montalvo, Carl Parker, Carly Burton, Catalina\nMejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao\nZhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris\nTindal, Christoph Feichtenhofer, Cynthia Gao, Damon\nCivin, Dana Beaty, Daniel Kreymer, Daniel Li, David\nAdkins, David Xu, Davide Testuggine, Delia David,\nDevi Parikh, Diana Liskovich, Didem Foss, Dingkang\nWang, Duc Le, Dustin Holland, Edward Dowling, Eissa\nJamil, Elaine Montgomery, Eleonora Presani, Emily\nHahn, Emily Wood, Eric-Tuan Le, Erik Brinkman,\nEsteban Arcaute, Evan Dunbar, Evan Smothers, Fei\nSun, Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat\nOzgenel, Francesco Caggioni, Frank Kanayet, Frank\nSeide, Gabriela Medina Florez, Gabriella Schwarz,\nGada Badeer, Georgia Swee, Gil Halpern, Grant Her-\nman, Grigory Sizov, Guangyi, Zhang, Guna Lakshmi-\nnarayanan, Hakan Inan, Hamid Shojanazeri, Han Zou,\nHannah Wang, Hanwen Zha, Haroun Habeeb, Harrison\nRudolph, Helen Suk, Henry Aspegren, Hunter Gold-\nman, Hongyuan Zhan, Ibrahim Damlaj, Igor Molybog,\nIgor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai\nGat, Jake Weissman, James Geboski, James Kohli, Jan-\nice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Mar-\ncus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy\nReizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin,\nJingyi Yang, Joe Cummings, Jon Carvill, Jon Shep-\nard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg,\nJunjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Kar-\ntikay Khandelwal, Katayoun Zand, Kathy Matosich,\nKaushik Veeraraghavan, Kelly Michelena, Keqian Li,\nKiran Jagadeesh, Kun Huang, Kunal Chawla, Kyle\nHuang, Lailin Chen, Lakshya Garg, Lavender A, Le-\nandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo,\nLicheng Yu, Liron Moshkovich, Luca Wehrstedt, Ma-\ndian Khabsa, Manav Avalani, Manish Bhatt, Marty-\nnas Mankus, Matan Hasson, Matthew Lennie, Matthias\nReso, Maxim Groshev, Maxim Naumov, Maya Lathi,\nMeghan Keneally, Miao Liu, Michael L. Seltzer, Michal\nValko, Michelle Restrepo, Mihir Patel, Mik Vyatskov,\nMikayel Samvelyan, Mike Clark, Mike Macey, Mike\nWang, Miquel Jubert Hermoso, Mo Metanat, Moham-\nmad Rastegari, Munish Bansal, Nandhini Santhanam,\nNatascha Parks, Natasha White, Navyata Bawa, Nayan\nSinghal, Nick Egebo, Nicolas Usunier, Nikhil Mehta,\nNikolay Pavlovich Laptev, Ning Dong, Norman Cheng,\nOleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem\nKalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan\nBalaji, Pedro Rittner, Philip Bontrager, Pierre Roux,\nPiotr Dollar, Polina Zvyagina, Prashant Ratanchan-\ndani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel\nRodriguez, Rafi Ayub, Raghotham Murthy, Raghu\nNayani, Rahul Mitra, Rangaprabhu Parthasarathy, Ray-\nmond Li, Rebekkah Hogan, Robin Battey, Rocky Wang,\nRuss Howes, Ruty Rinott, Sachin Mehta, Sachin Siby,\nSai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara\nHunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan,\nSaurabh Mahajan, Saurabh Verma, Seiji Yamamoto,\nSharadh Ramaswamy, Shaun Lindsay, Shaun Lind-\nsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha,\nShishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang\nZhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe,\nSoumith Chintala, Stephanie Max, Stephen Chen, Steve\nKehoe, Steve Satterfield, Sudarshan Govindaprasad,\nSumit Gupta, Summer Deng, Sungmin Cho, Sunny Virk,\nSuraj Subramanian, Sy Choudhury, Sydney Goldman,\nTal Remez, Tamar Glaser, Tamara Best, Thilo Koehler,\nThomas Robinson, Tianhe Li, Tianjun Zhang, Tim\nMatthews, Timothy Chou, Tzook Shaked, Varun Von-\ntimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan,\nVinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad\nPoenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov,\nWei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz,\nWill Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan\nWang, Xilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun\nChen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying\nZhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yu Zhao,\nYuchen Hao, Yundi Qian, Yunlu Li, Yuzi He, Zach Rait,\nZachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu\nYang, Zhiwei Zhao, and Zhiyu Ma. 2024. The llama 3\nherd of models.\nIvan Habernal, Henning Wachsmuth, Iryna Gurevych,\nand Benno Kiesel. 2018a. \"dummy, grandpa, do you\nknow anything?\": Identifying and characterizing ad\nhominem fallacies in the wild. In Proceedings of the\n12th International AAAI Conference on Web and Social\nMedia (ICWSM), pages 206–215.\nIvan Habernal, Henning Wachsmuth, Iryna Gurevych,\nand Benno Stein. 2018b. Before name-calling: Dynam-\nics and triggers of ad hominem fallacies in web argu-\nmentation. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for Com-\nputational Linguistics: Human Language Technologies,\nVolume 1 (Long Papers), pages 386–396, New Orleans,\nLouisiana. Association for Computational Linguistics.\nChristopher Hahn, Frederik Schmitt, Julia J. Tillman,\nNiklas Metzger, Julian Siber, and Bernd Finkbeiner.\n2022. Formal specifications from natural language.\nPatrick M. Haluptzok, Matthew Bowers, and Adam Tau-\nman Kalai. 2022. Language modexrls can teach them-\nselves to program better. ArXiv, abs/2207.14502.\nZhijing Jin, Abhinav Lalwani, Tejas Vaidhya, Xiaoyu\nShen, Yiwen Ding, Zhiheng Lyu, Mrinmaya Sachan,\nRada Mihalcea, and Bernhard Schoelkopf. 2022. Logi-\ncal fallacy detection. In Findings of the Association for\nComputational Linguistics: EMNLP 2022, pages 7180–\n7198, Abu Dhabi, United Arab Emirates. Association\nfor Computational Linguistics.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020. BART:\nDenoising sequence-to-sequence pre-training for natu-\nral language generation, translation, and comprehension.\nIn Proceedings of the 58th Annual Meeting of the Associ-\nation for Computational Linguistics, pages 7871–7880,\nOnline. Association for Computational Linguistics.\nJason Xinyu Liu, Ziyi Yang, Benjamin Schornstein,\nSam Liang, Ifrah Idrees, Stefanie Tellex, and Ankit\nShah. 2022. Lang2LTL: Translating natural language\ncommands to temporal specification with large language\nmodels. In CoRL Workshop on Language and Robot\nLearning.\nB. MacCartney and C. D. Manning. 2014. Natural logic\nand natural language inference. In H. Bunt, J. Bos,\nand S. Pulman, editors, Computing Meaning: Volume 4,\npages 129–147. Springer Netherlands, Dordrecht.\nCallistus Ireneous Nakpih and Simone Santini. 2020.\nAutomated discovery of logical fallacies in legal argu-\nmentation. International Journal of Artificial Intelli-\ngence & Applications.\nTheo Olausson, Alex Gu, Ben Lipkin, Cedegao Zhang,\nArmando Solar-Lezama, Joshua Tenenbaum, and Roger\nLevy. 2023. LINC: A neurosymbolic approach for logi-\ncal reasoning by combining language models with first-\norder logic provers. In Proceedings of the 2023 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 5153–5176, Singapore. Association\nfor Computational Linguistics.\nOpenAI. 2024. Gpt-4o mini: Advancing cost-efficient\nintelligence. Accessed: 2025-02-15.\nOpenAI, Aaron Hurst, Adam Lerer, Adam P. Goucher,\nAdam Perelman, Aditya Ramesh, Aidan Clark, AJ Os-\ntrow, Akila Welihinda, Alan Hayes, Alec Radford, Alek-\nsander M ˛adry, Alex Baker-Whitcomb, Alex Beutel,\nAlex Borzunov, Alex Carney, Alex Chow, Alex Kirillov,\nAlex Nichol, Alex Paino, Alex Renzin, Alex Tachard\nPassos, Alexander Kirillov, Alexi Christakis, Alexis\nConneau, Ali Kamali, Allan Jabri, Allison Moyer, Alli-\nson Tam, Amadou Crookes, Amin Tootoochian, Amin\nTootoonchian, Ananya Kumar, Andrea Vallone, Andrej\nKarpathy, Andrew Braunstein, Andrew Cann, Andrew\nCodispoti, Andrew Galu, Andrew Kondrich, Andrew\nTulloch, Andrey Mishchenko, Angela Baek, Angela\nJiang, Antoine Pelisse, Antonia Woodford, Anuj Gos-\nalia, Arka Dhar, Ashley Pantuliano, Avi Nayak, Avi-\ntal Oliver, Barret Zoph, Behrooz Ghorbani, Ben Le-\nimberger, Ben Rossen, Ben Sokolowsky, Ben Wang,\nBenjamin Zweig, Beth Hoover, Blake Samic, Bob Mc-\nGrew, Bobby Spero, Bogo Giertler, Bowen Cheng,\nBrad Lightcap, Brandon Walkin, Brendan Quinn, Brian\nGuarraci, Brian Hsu, Bright Kellogg, Brydon Eastman,\nCamillo Lugaresi, Carroll Wainwright, Cary Bassin,\nCary Hudson, Casey Chu, Chad Nelson, Chak Li,\nChan Jun Shern, Channing Conger, Charlotte Barette,\nChelsea Voss, Chen Ding, Cheng Lu, Chong Zhang,\nChris Beaumont, Chris Hallacy, Chris Koch, Chris-\ntian Gibson, Christina Kim, Christine Choi, Chris-\ntine McLeavey, Christopher Hesse, Claudia Fischer,\nClemens Winter, Coley Czarnecki, Colin Jarvis, Colin\nWei, Constantin Koumouzelis, Dane Sherburn, Daniel\nKappler, Daniel Levin, Daniel Levy, David Carr, David\nFarhi, David Mely, David Robinson, David Sasaki,\nDenny Jin, Dev Valladares, Dimitris Tsipras, Doug Li,\nDuc Phong Nguyen, Duncan Findlay, Edede Oiwoh,\nEdmund Wong, Ehsan Asdar, Elizabeth Proehl, Eliz-\nabeth Yang, Eric Antonow, Eric Kramer, Eric Peter-\nson, Eric Sigler, Eric Wallace, Eugene Brevdo, Evan\nMays, Farzad Khorasani, Felipe Petroski Such, Fil-\nippo Raso, Francis Zhang, Fred von Lohmann, Fred-\ndie Sulit, Gabriel Goh, Gene Oden, Geoff Salmon,\nGiulio Starace, Greg Brockman, Hadi Salman, Haiming\nBao, Haitang Hu, Hannah Wong, Haoyu Wang, Heather\nSchmidt, Heather Whitney, Heewoo Jun, Hendrik Kirch-\nner, Henrique Ponde de Oliveira Pinto, Hongyu Ren,\nHuiwen Chang, Hyung Won Chung, Ian Kivlichan,\nIan O’Connell, Ian O’Connell, Ian Osband, Ian Silber,\nIan Sohl, Ibrahim Okuyucu, Ikai Lan, Ilya Kostrikov,\nIlya Sutskever, Ingmar Kanitscheider, Ishaan Gulrajani,\nJacob Coxon, Jacob Menick, Jakub Pachocki, James\nAung, James Betker, James Crooks, James Lennon,\nJamie Kiros, Jan Leike, Jane Park, Jason Kwon, Ja-\nson Phang, Jason Teplitz, Jason Wei, Jason Wolfe, Jay\nChen, Jeff Harris, Jenia Varavva, Jessica Gan Lee, Jes-\nsica Shieh, Ji Lin, Jiahui Yu, Jiayi Weng, Jie Tang,\nJieqi Yu, Joanne Jang, Joaquin Quinonero Candela,\nJoe Beutler, Joe Landers, Joel Parish, Johannes Hei-\ndecke, John Schulman, Jonathan Lachman, Jonathan\nMcKay, Jonathan Uesato, Jonathan Ward, Jong Wook\nKim, Joost Huizinga, Jordan Sitkin, Jos Kraaijeveld,\nJosh Gross, Josh Kaplan, Josh Snyder, Joshua Achiam,\nJoy Jiao, Joyce Lee, Juntang Zhuang, Justyn Harri-\nman, Kai Fricke, Kai Hayashi, Karan Singhal, Katy Shi,\nKavin Karthik, Kayla Wood, Kendra Rimbach, Kenny\nHsu, Kenny Nguyen, Keren Gu-Lemberg, Kevin But-\nton, Kevin Liu, Kiel Howe, Krithika Muthukumar, Kyle\nLuther, Lama Ahmad, Larry Kai, Lauren Itow, Lau-\nren Workman, Leher Pathak, Leo Chen, Li Jing, Lia\nGuy, Liam Fedus, Liang Zhou, Lien Mamitsuka, Lilian\nWeng, Lindsay McCallum, Lindsey Held, Long Ouyang,\nLouis Feuvrier, Lu Zhang, Lukas Kondraciuk, Lukasz\nKaiser, Luke Hewitt, Luke Metz, Lyric Doshi, Mada\nAflak, Maddie Simens, Madelaine Boyd, Madeleine\nThompson, Marat Dukhan, Mark Chen, Mark Gray,\nMark Hudnall, Marvin Zhang, Marwan Aljubeh, Ma-\nteusz Litwin, Matthew Zeng, Max Johnson, Maya\nShetty, Mayank Gupta, Meghan Shah, Mehmet Yatbaz,\nMeng Jia Yang, Mengchao Zhong, Mia Glaese, Mianna\nChen, Michael Janner, Michael Lampe, Michael Petrov,\nMichael Wu, Michele Wang, Michelle Fradin, Michelle\nPokrass, Miguel Castro, Miguel Oom Temudo de Cas-\ntro, Mikhail Pavlov, Miles Brundage, Miles Wang, Mi-\nnal Khan, Mira Murati, Mo Bavarian, Molly Lin, Mu-\nrat Yesildal, Nacho Soto, Natalia Gimelshein, Natalie\nCone, Natalie Staudacher, Natalie Summers, Natan La-\nFontaine, Neil Chowdhury, Nick Ryder, Nick Stathas,\nNick Turley, Nik Tezak, Niko Felix, Nithanth Kudige,\nNitish Keskar, Noah Deutsch, Noel Bundick, Nora\nPuckett, Ofir Nachum, Ola Okelola, Oleg Boiko, Oleg\nMurk, Oliver Jaffe, Olivia Watkins, Olivier Godement,\nOwen Campbell-Moore, Patrick Chao, Paul McMillan,\nPavel Belov, Peng Su, Peter Bak, Peter Bakkum, Pe-\nter Deng, Peter Dolan, Peter Hoeschele, Peter Welin-\nder, Phil Tillet, Philip Pronin, Philippe Tillet, Prafulla\nDhariwal, Qiming Yuan, Rachel Dias, Rachel Lim,\nRahul Arora, Rajan Troll, Randall Lin, Rapha Gontijo\nLopes, Raul Puri, Reah Miyara, Reimar Leike, Renaud\nGaubert, Reza Zamani, Ricky Wang, Rob Donnelly, Rob\nHonsby, Rocky Smith, Rohan Sahai, Rohit Ramchan-\ndani, Romain Huet, Rory Carmichael, Rowan Zellers,\nRoy Chen, Ruby Chen, Ruslan Nigmatullin, Ryan Cheu,\nSaachi Jain, Sam Altman, Sam Schoenholz, Sam Toizer,\nSamuel Miserendino, Sandhini Agarwal, Sara Culver,\nScott Ethersmith, Scott Gray, Sean Grove, Sean Metzger,\nShamez Hermani, Shantanu Jain, Shengjia Zhao, Sher-\nwin Wu, Shino Jomoto, Shirong Wu, Shuaiqi, Xia, Sonia\nPhene, Spencer Papay, Srinivas Narayanan, Steve Cof-\nfey, Steve Lee, Stewart Hall, Suchir Balaji, Tal Broda,\nTal Stramer, Tao Xu, Tarun Gogineni, Taya Chris-\ntianson, Ted Sanders, Tejal Patwardhan, Thomas Cun-\nninghman, Thomas Degry, Thomas Dimson, Thomas\nRaoux, Thomas Shadwell, Tianhao Zheng, Todd Un-\nderwood, Todor Markov, Toki Sherbakov, Tom Rubin,\nTom Stasi, Tomer Kaftan, Tristan Heywood, Troy Pe-\nterson, Tyce Walters, Tyna Eloundou, Valerie Qi, Veit\nMoeller, Vinnie Monaco, Vishal Kuo, Vlad Fomenko,\nWayne Chang, Weiyi Zheng, Wenda Zhou, Wesam Man-\nassra, Will Sheu, Wojciech Zaremba, Yash Patil, Yilei\nQian, Yongjik Kim, Youlong Cheng, Yu Zhang, Yuchen\nHe, Yuchen Zhang, Yujia Jin, Yunxing Dai, and Yury\nMalkov. 2024a. Gpt-4o system card.\nOpenAI, Aaron Jaech, Adam Kalai, Adam Lerer, Adam\nRichardson, Ahmed El-Kishky, Aiden Low, Alec Hel-\nyar, Aleksander Madry, Alex Beutel, Alex Carney, Alex\nIftimie, Alex Karpenko, Alex Tachard Passos, Alexan-\nder Neitz, Alexander Prokofiev, Alexander Wei, Alli-\nson Tam, Ally Bennett, Ananya Kumar, Andre Saraiva,\nAndrea Vallone, Andrew Duberstein, Andrew Kon-\ndrich, Andrey Mishchenko, Andy Applebaum, An-\ngela Jiang, Ashvin Nair, Barret Zoph, Behrooz Ghor-\nbani, Ben Rossen, Benjamin Sokolowsky, Boaz Barak,\nBob McGrew, Borys Minaiev, Botao Hao, Bowen\nBaker, Brandon Houghton, Brandon McKinzie, Brydon\nEastman, Camillo Lugaresi, Cary Bassin, Cary Hud-\nson, Chak Ming Li, Charles de Bourcy, Chelsea Voss,\nChen Shen, Chong Zhang, Chris Koch, Chris Orsinger,\nChristopher Hesse, Claudia Fischer, Clive Chan, Dan\nRoberts, Daniel Kappler, Daniel Levy, Daniel Sel-\nsam, David Dohan, David Farhi, David Mely, David\nRobinson, Dimitris Tsipras, Doug Li, Dragos Oprica,\nEben Freeman, Eddie Zhang, Edmund Wong, Elizabeth\nProehl, Enoch Cheung, Eric Mitchell, Eric Wallace, Erik\nRitter, Evan Mays, Fan Wang, Felipe Petroski Such,\nFilippo Raso, Florencia Leoni, Foivos Tsimpourlas,\nFrancis Song, Fred von Lohmann, Freddie Sulit, Ge-\noff Salmon, Giambattista Parascandolo, Gildas Chabot,\nGrace Zhao, Greg Brockman, Guillaume Leclerc, Hadi\nSalman, Haiming Bao, Hao Sheng, Hart Andrin, Hes-\nsam Bagherinezhad, Hongyu Ren, Hunter Lightman,\nHyung Won Chung, Ian Kivlichan, Ian O’Connell, Ian\nOsband, Ignasi Clavera Gilaberte, Ilge Akkaya, Ilya\nKostrikov, Ilya Sutskever, Irina Kofman, Jakub Pa-\nchocki, James Lennon, Jason Wei, Jean Harb, Jerry\nTwore, Jiacheng Feng, Jiahui Yu, Jiayi Weng, Jie Tang,\nJieqi Yu, Joaquin Quiñonero Candela, Joe Palermo,\nJoel Parish, Johannes Heidecke, John Hallman, John\nRizzo, Jonathan Gordon, Jonathan Uesato, Jonathan\nWard, Joost Huizinga, Julie Wang, Kai Chen, Kai Xiao,\nKaran Singhal, Karina Nguyen, Karl Cobbe, Katy Shi,\nKayla Wood, Kendra Rimbach, Keren Gu-Lemberg,\nKevin Liu, Kevin Lu, Kevin Stone, Kevin Yu, Lama\nAhmad, Lauren Yang, Leo Liu, Leon Maksin, Ley-\nton Ho, Liam Fedus, Lilian Weng, Linden Li, Lindsay\nMcCallum, Lindsey Held, Lorenz Kuhn, Lukas Kon-\ndraciuk, Lukasz Kaiser, Luke Metz, Madelaine Boyd,\nMaja Trebacz, Manas Joglekar, Mark Chen, Marko\nTintor, Mason Meyer, Matt Jones, Matt Kaufer, Max\nSchwarzer, Meghan Shah, Mehmet Yatbaz, Melody Y.\nGuan, Mengyuan Xu, Mengyuan Yan, Mia Glaese, Mi-\nanna Chen, Michael Lampe, Michael Malek, Michele\nWang, Michelle Fradin, Mike McClay, Mikhail Pavlov,\nMiles Wang, Mingxuan Wang, Mira Murati, Mo Bavar-\nian, Mostafa Rohaninejad, Nat McAleese, Neil Chowd-\nhury, Neil Chowdhury, Nick Ryder, Nikolas Tezak,\nNoam Brown, Ofir Nachum, Oleg Boiko, Oleg Murk,\nOlivia Watkins, Patrick Chao, Paul Ashbourne, Pavel Iz-\nmailov, Peter Zhokhov, Rachel Dias, Rahul Arora, Ran-\ndall Lin, Rapha Gontijo Lopes, Raz Gaon, Reah Miyara,\nReimar Leike, Renny Hwang, Rhythm Garg, Robin\nBrown, Roshan James, Rui Shu, Ryan Cheu, Ryan\nGreene, Saachi Jain, Sam Altman, Sam Toizer, Sam\nToyer, Samuel Miserendino, Sandhini Agarwal, Santi-\nago Hernandez, Sasha Baker, Scott McKinney, Scottie\nYan, Shengjia Zhao, Shengli Hu, Shibani Santurkar,\nShraman Ray Chaudhuri, Shuyuan Zhang, Siyuan Fu,\nSpencer Papay, Steph Lin, Suchir Balaji, Suvansh San-\njeev, Szymon Sidor, Tal Broda, Aidan Clark, Tao Wang,\nTaylor Gordon, Ted Sanders, Tejal Patwardhan, Thibault\nSottiaux, Thomas Degry, Thomas Dimson, Tianhao\nZheng, Timur Garipov, Tom Stasi, Trapit Bansal, Trevor\nCreech, Troy Peterson, Tyna Eloundou, Valerie Qi, Vi-\nneet Kosaraju, Vinnie Monaco, Vitchyr Pong, Vlad\nFomenko, Weiyi Zheng, Wenda Zhou, Wes McCabe,\nWojciech Zaremba, Yann Dubois, Yinghai Lu, Yining\nChen, Young Cha, Yu Bai, Yuchen He, Yuchen Zhang,\nYunyun Wang, Zheng Shao, and Zhuohan Li. 2024b.\nOpenai o1 system card.\nLiangming Pan, Alon Albalak, Xinyi Wang, and\nWilliam Wang. 2023. Logic-LM: Empowering large lan-\nguage models with symbolic solvers for faithful logical\nreasoning. In Findings of the Association for Compu-\ntational Linguistics: EMNLP 2023, pages 3806–3824,\nSingapore. Association for Computational Linguistics.\nWilliam C Purdy. 1991. A logic for natural language.\nNotre Dame Journal of Formal Logic, 32(3):409–425.\nHrituraj Singh, Milan Aggarwal, and Balaji Krish-\nnamurthy. 2020.\nExploring neural models for pars-\ning natural language into first-order logic.\nArXiv,\nabs/2002.06544.\nZhivar Sourati, Vishnu Priya Prasanna Venkatesh, Dar-\nshan Deshpande, Himanshu Rawlani, Filip Ilievski,\nHông-Ân Sandlin, and Alain Mermoud. 2022. Robust\nand explainable identification of logical fallacies in nat-\nural language arguments. Knowledge Based Systems,\n266:110418.\nChristian Stab and Iryna Gurevych. 2017. Recogniz-\ning insufficiently supported arguments in argumentative\nessays. In Proceedings of the 15th Conference of the\nEuropean Chapter of the Association for Computational\nLinguistics: Volume 1, Long Papers, pages 980–990,\nValencia, Spain. Association for Computational Lin-\nguistics.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay Bash-\nlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhos-\nale, Dan Bikel, Lukas Blecher, Cristian Canton Fer-\nrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude\nFernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cyn-\nthia Gao, Vedanuj Goswami, Naman Goyal, Anthony\nHartshorn, Saghar Hosseini, Rui Hou, Hakan Inan,\nMarcin Kardas, Viktor Kerkez, Madian Khabsa, Is-\nabel Kloumann, Artem Korenev, Punit Singh Koura,\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-\nana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-\ntinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-\nbog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein,\nRashi Rungta, Kalyan Saladi, Alan Schelten, Ruan\nSilva, Eric Michael Smith, Ranjan Subramanian, Xiao-\nqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams,\nJian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov,\nYuchen Zhang, Angela Fan, Melanie Kambadur, Sharan\nNarang, Aurelien Rodriguez, Robert Stojnic, Sergey\nEdunov, and Thomas Scialom. 2023. Llama 2: Open\nfoundation and fine-tuned chat models.\nJason Wei, Xuezhi Wang, Dale Schuurmans, et al. 2022.\nChain of thought prompting elicits reasoning in large\nlanguage models. Advances in Neural Information Pro-\ncessing Systems.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sentence\nunderstanding through inference. In Proceedings of\nthe 2018 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long Papers), pages\n1112–1122, New Orleans, Louisiana. Association for\nComputational Linguistics.\nYuan Yang, Siheng Xiong, Ali Payani, Ehsan Shareghi,\nand Faramarz Fekri. 2024. Harnessing the power of\nlarge language models for natural language to first-\norder logic translation.\nIn Proceedings of the 62nd\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 6942–6959,\nBangkok, Thailand. Association for Computational Lin-\nguistics.\nAppendix\nA\nAlgorithms\nAlgorithm 1: Compiling Logical Formula to\nSMT\nInput: Logical formula L in natural language or\nFirst-Order Logic (FOL)\nOutput: SMT file S formatted for formal solvers\n1 Step 1: Tokenize Formula\n2 T ←Tokenize(L) // Split L into tokens based\non operators, parentheses, and commas\n3 Step 2: Process Tokens\n4 P ←∅// Initialize processed tokens set\n5 foreach token t ∈T do\n6\nif t is a predicate then\n7\nIdentify arguments of t\n8\nRecursively ProcessTokens() for arguments\n9\nelse if t is an operator or variable then\n10\nAdd t to P\n11 Step 3: Convert Formula to Prefix Notation\n12 Fprefix ←InfixToPrefix(P) // Transform logical\nformula from infix to prefix notation\n13 Recursively apply InfixToPrefix() for predicate\narguments\n14 Step 4: Determine Sorts\n15 Ssorts ←UnifySort(Fprefix) // Assign sorts for\nvariables and predicates\n16 Step 5: Format Formula for SMT\n17 FSMT ←Parenthesize Fprefix according to SMT-LIB\nsyntax\n18 Step 6: Generate SMT File\n19 S ←GenerateSMT(Ssorts, FSMT)\n20 Include\n• (declare-sort) statements for sorts.\n• (declare-fun) statements for variables and\npredicates.\n• Negation of FSMT.\n• (check-sat) and (get-model) commands.\nreturn S // Return the SMT file for use in\nformal solvers\nB\nPrompt Examples\nNote: Additional in-context examples were re-\nmoved for brevity and denoted ‘[...]’ in the fol-\nlowing prompts.\nB.1\nEnd-to-end LLM Prompts\nPrompt 1. Classifying with in-context ex-\namples (Few-shot)\nLogical fallacies are common errors in\nreasoning that undermine the logic of an\nargument.\nA sentence is logically valid if and only if it\nis not possible for it to be false.\nAlgorithm 2: UnifySort for Predicate A(x, y)\nInput: Predicate A(x, y) with arguments and potential\ninstances\nOutput: Unified sort for predicate A or an error if sorts\nare incompatible\n1 Step 1: Declare the Current Sort Initialize the current\nsort of A: (NULL, NULL, Bool)\n2 Step 2: Process Each Instance of Predicate A foreach\ninstance of predicate A do\n3\nStep 2.1: Determine Instance Sorts foreach\nargument xi in the instance do\n4\nif xi is a formula then\n5\nSet sort(xi) = Bool\n6\nelse if xi is a variable then\n7\nSet sort(xi) = sort(variable) // May be\nNULL\n8\nStep 2.2: Unify Current Sort with Instance Sort\nforeach statement sort in current and instance\nsorts do\n9\nif sorts are not NULL and different then\n10\nRaise an error: Incompatible sorts\n11\nelse if current sort is NULL and instance sort is\nnot NULL then\n12\nUpdate current sort:\ncurrent_sort ←instance_sort\n13\nelse if instance sort is NULL and current sort is\nnot NULL then\n14\nUpdate variable sort to match current sort\nHere are some examples of classifying sen-\ntences as logical fallacies or valid sentences:\nExample 1:\nInput: \"I met a tall man who loved to eat\ncheese, now I believe all tall people like\ncheese\"\nAnswer: Logical Fallacy\n[...]\nNow,\nclassify the following sentence.\nAnswer with either \"Logical Fallacy\" or\n\"Valid\" at the start of your answer.\nInput:\nB.2\nIntermediate NL2FOL Prompts\nPrompt 2. Extracting claim and implication\nHere are some examples of extracting\nclaims and implications from an input\nparagraph. There can be multiple claims\nbut only one implication.\nInput: \"I met a tall man who loved to eat\ncheese, now I believe all tall people like\ncheese.\"\nOutput:\nClaim: \"A tall man loves cheese.\"\nImplication: \"All tall people like cheese.\"\n[...]\nDo not use any subordinating conjunctions\nin the implication. Replace pronouns with\nthe appropriate nouns so that there are\nno pronouns. Now extract the claim and\nimplication for the following input.\nInput:\nPrompt 3. Getting referring expressions\nYou are given a sentence.\nReferring\nexpressions are noun phrases, pronouns,\nand proper names that refer to some\nindividual objects that have some properties\nassociated with them.\nHere are some\nexamples of finding referring expressions\nin a sentence:\nInput: \"A tall man loved cheese\"\nReferring expressions: A tall man\n[...]\nNow, find the referring expressions for the\nfollowing input:\nPrompt 4. Getting entity relations\nPlease determine the relationship between\nthe two entities provided below. Choose the\nnumber corresponding to the statement that\nbest describes their relationship:\n1. \"[Entity A]\" is equal to \"[Entity B]\".\n2. \"[Entity A]\" is a subset of \"[Entity B]\".\n3. \"[Entity B]\" is a subset of \"[Entity A]\".\n4. \"[Entity A]\" is not related to \"[Entity B]\".\nInstructions:\n- Equality check: If the two entities are equal\n(case-insensitive after stripping whitespace),\nselect statement 1.\n- Subset determination: If they are not equal,\nassess whether one entity is a subset of the\nother based on general knowledge and logi-\ncal reasoning.\n- If \"[Entity A]\" is a subset of \"[Entity\nB]\", select statement 2.\n- If \"[Entity B]\" is a subset of \"[Entity\nA]\", select statement 3.\n- Unrelated entities: If none of the above\nstatements accurately describes the relation-\nship.\nHere are some examples:\nExample 1:\nEntity A: \"dogs\"\nEntity B: \"animals\"\nAnalysis: All dogs are animals, so \"dogs\"\nis a subset of \"animals\".\nAnswer: 2\n[...]\nEntities:\n- Entity A:\n- Entity B:\nYour Task:\n- Analyze the relationship between \"Entity\nA\" and \"Entity B\" based on the instructions.\n- Provide only the number (1, 2, 3, or 4)\nthat corresponds to the statement you have\nselected.\nPrompt 5. Getting properties (claim)\nGiven\na\nsentence,\nand\nthe\nreferring\nexpressions of that sentence.\nProperties\nare anything that describes a relationship\nbetween two referring expressions, or\nthey may describe a trait of a referring\nexpression. These properties are essentially\npredicates in first-order logic.\nHere are some examples of finding proper-\nties in a sentence:\nExample 1:\nInput sentence: A tall man loves cheese\nReferring expressions: tall man: a, cheese:\nb\nProperties: IsTall(x), LovesCheese(x)\n[...]\nNow extract the properties for the following\ninput:\nPrompt 6. Getting property relations\nYou are given two logical clauses. Your\ntask is to identify whether or not the\nfirst clause entails the second clause,\ntaking\ninto\naccount\nexternal\nknowl-\nedge or ’common sense’. Also, take into\naccount the context from the input sentence.\nHere are some examples:\nExample 1:\nInput sentence: A boy is jumping on skate-\nboard in the middle of a red bridge. Thus,\nthe boy does a skateboarding trick.\nClause 1: JumpsOn(boy,skateboard)\nClause 2: Does(boy, skateboarding_trick)\nAnswer: ENTAILMENT\n[...]\nNow given the following clauses. identify\nwhether the first clause entails the second\nclause.\nPrompt 7. Retrieving FOL expression\nGiven a sentence, the referring expressions\nof that sentence, and properties which are\nassociated with the referring expressions.\nUse the given properties to convert the\nsentence into a first-order logical form. Use\n-> to represent implies, & to represent and, |\nto represent or and to represent negations.\nExample 1:\nInput Sentence: A tall man loves cheese\nReferring Expressions: A tall man: x\nProperties: IsTall(x), LovesCheese(x)\nLogical Form: IsTall(x) & LovesCheese(x)\n[...]\nThe complete set of prompt examples is available\nin our public code repository at https://github.\ncom/lovishchopra/NL2FOL. We encourage read-\ners to visit the repository for details and latest up-\ndates.\n",
  "categories": [
    "cs.CL",
    "cs.AI",
    "cs.LG",
    "cs.LO"
  ],
  "published": "2024-04-18",
  "updated": "2025-03-06"
}