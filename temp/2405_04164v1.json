{
  "id": "http://arxiv.org/abs/2405.04164v1",
  "title": "Sign2GPT: Leveraging Large Language Models for Gloss-Free Sign Language Translation",
  "authors": [
    "Ryan Wong",
    "Necati Cihan Camgoz",
    "Richard Bowden"
  ],
  "abstract": "Automatic Sign Language Translation requires the integration of both computer\nvision and natural language processing to effectively bridge the communication\ngap between sign and spoken languages. However, the deficiency in large-scale\ntraining data to support sign language translation means we need to leverage\nresources from spoken language. We introduce, Sign2GPT, a novel framework for\nsign language translation that utilizes large-scale pretrained vision and\nlanguage models via lightweight adapters for gloss-free sign language\ntranslation. The lightweight adapters are crucial for sign language\ntranslation, due to the constraints imposed by limited dataset sizes and the\ncomputational requirements when training with long sign videos. We also propose\na novel pretraining strategy that directs our encoder to learn sign\nrepresentations from automatically extracted pseudo-glosses without requiring\ngloss order information or annotations. We evaluate our approach on two public\nbenchmark sign language translation datasets, namely RWTH-PHOENIX-Weather 2014T\nand CSL-Daily, and improve on state-of-the-art gloss-free translation\nperformance with a significant margin.",
  "text": "Published as a conference paper at ICLR 2024\nSIGN2GPT: LEVERAGING LARGE LANGUAGE MOD-\nELS FOR GLOSS-FREE SIGN LANGUAGE TRANSLA-\nTION\nRyan Wong1, Necati Cihan Camgoz2, Richard Bowden1\n1University of Surrey, 2Meta Reality Labs\n{r.wong, r.bowden}@surrey.ac.uk, neccam@meta.com\nABSTRACT\nAutomatic Sign Language Translation requires the integration of both computer\nvision and natural language processing to effectively bridge the communication\ngap between sign and spoken languages. However, the deficiency in large-scale\ntraining data to support sign language translation means we need to leverage re-\nsources from spoken language.\nWe introduce, Sign2GPT, a novel framework\nfor sign language translation that utilizes large-scale pretrained vision and lan-\nguage models via lightweight adapters for gloss-free sign language translation.\nThe lightweight adapters are crucial for sign language translation, due to the\nconstraints imposed by limited dataset sizes and the computational requirements\nwhen training with long sign videos. We also propose a novel pretraining strategy\nthat directs our encoder to learn sign representations from automatically extracted\npseudo-glosses without requiring gloss order information or annotations. We eval-\nuate our approach on two public benchmark sign language translation datasets,\nnamely RWTH-PHOENIX-Weather 2014T and CSL-Daily, and improve on state-\nof-the-art gloss-free translation performance with a significant margin.\n1\nINTRODUCTION\nSign languages are the primary form of communication for millions of Deaf individuals. Sign lan-\nguages make use of complex visual gestures as a form of communication (Braem & Sutton-Spence,\n2001). Automatic sign language translation is a challenging task for both natural language process-\ning and computer vision, as it requires understanding of both sign and spoken language semantics.\nMany prior studies have heavily relied on gloss annotations as a means to achieve better translation\nperformance (Camgoz et al., 2018; 2020b; Ye et al., 2023; Yao et al., 2023; Zhang et al., 2023a),\nwhere gloss annotations represent written descriptions of signs. These glosses are provided in sign\norder, aiding in the regularization of models to learn valuable sign features necessary for continu-\nous sign language recognition and translation. However, creating datasets with gloss annotations\nis resource-intensive and time-consuming. Hence, a recent trend is to shift towards gloss-free sign\nlanguage translation (Camgoz et al., 2020a; Yin et al., 2023; Zhou et al., 2023), which is the pri-\nmary focus of our paper. Gloss-free sign language translation poses significant challenges due to the\nunique grammatical structures and vocabulary of sign languages. Limited data availability, individ-\nual signing style variation and long sign videos add to the complexity of the task.\nIn this paper, we propose Sign2GPT for sign language translation to address the aforementioned\nchallenges. As depicted in Figure 1, Sign2GPT utilizes pretrained large vision (Oquab et al., 2023)\nand language models (Lin et al., 2021), benefiting from their extensive training on large-scale\ndatasets to improve sign language translation performance. We also present a novel pretraining\nstrategy before the translation task, which encompasses two key components. Firstly, we develop an\nalgorithm designed to automatically generate pseudo-glosses. Secondly, we introduce a prototype\ndriven method for pretraining the sign encoder using the pseudo-glosses. Notably, our approach\neliminates the necessity of manual gloss annotations as well as glosses to be in sign order.\nTo combat over-fitting and address memory constraints resulting from long video sequences dur-\ning training, we adopt a strategy of freezing the external pretrained models and utilize specialized\n1\narXiv:2405.04164v1  [cs.CV]  7 May 2024\nPublished as a conference paper at ICLR 2024\n...\n(XGLM)\nund nun die wettervorhersage ...\nPretraining Stage\nDownstream Translation Stage\nnun\nwettervorhersage\nfür\nmorgen\nsamstag\nzweiten\noktober\nfrozen weights\nKey:\ntrainable weights\n(Dino-V2 ViT)\n<bos> und nun die ...\nund nun die wettervorhersage für\nmorgen samstag den zweiten\noktober\npseudo-glosses\nFigure 1: Overview of Sign2GPT, which consists of a pretraining stage that makes use of pseudo-\nglosses and downstream translation that leverages a frozen GPT model.\nlow-rank adapters. Inspired by Hu et al. (2021), we employ low-rank adapters as a method for fa-\ncilitating the adaptation of frozen Generative Pretrained Transformers (GPT) (Radford et al., 2018;\nLin et al., 2021) as well as a Vision Transformer (ViT) (Dosovitskiy et al., 2020; Oquab et al., 2023)\nto the specialized domain of sign language. Our contributions can be summarized as follows: (1)\nWe introduce an end-to-end gloss-free sign language model, Sign2GPT, designed for sign language\ntranslation, leveraging a frozen GPT language model, (2) We propose a novel pseudo-gloss pretrain-\ning strategy, utilizing automatically extracted pseudo-glosses from sentences to pretrain the sign en-\ncoder. (3) Sign2GPT demonstrates significant performance improvements over previous gloss-free\nsign language translation approaches, offering a promising pathway for adapting frozen language\nand vision models to the domain of sign language translation.\n2\nRELATED WORK\nIsolated Sign Recognition (ISR), the task of identifying individual signs within a sign video, has\nbeen extensively studied in the literature. Many ISR approaches leverage deep learning, adapting\naction recognition models (Carreira & Zisserman, 2017; Tran et al., 2018; Yan et al., 2018; Jiang\net al., 2021; Albanie et al., 2020) on datasets such as WLASL (Li et al., 2020a), MSASL (Joze &\nKoller, 2018) and AUTSL (Sincan & Keles, 2020). New approaches have explored the integration\nof linguistic priors to enhance the accuracy of ISR models (Zuo et al., 2023; Wong et al., 2023). This\nis achieved through the utilization of pretrained word embeddings (Joulin et al., 2016) as features to\nregularize the models during training.\nWhile these methodologies have been successful for ISR, they address only a portion of the sign lan-\nguage translation challenge. Consequently, new datasets for Continuous Sign Language Recognition\nhave been developed, such as RWTH-PHOENIX-Weather 2014 (Koller et al., 2015) and CSL-Daily\n(Zhou et al., 2021). These datasets consist of sequences of coarticulated signs in a continuous video,\naccompanied by target labels known as glosses. These glosses are arranged in sign order and serve\nas an intermediary representation bridging the gap between sign videos and spoken language trans-\nlation. Many approaches use Connectionist Temporal Classification (CTC) loss (Graves et al., 2006)\nto localize and recognize glosses within sign videos (Camgoz et al., 2018; 2020b; Zhou et al., 2021;\nCui et al., 2017; Zheng et al., 2023) but this relies on the monotonicity of labels and content.\nAutomatic Sign Language Translation (SLT) is one of the main goals of computational sign language\nresearch, aimed at bridging the communication gap between sign languages and spoken languages.\nSLT datasets, such as RWTH-PHOENIX-Weather 2014T (Camgoz et al., 2018) and CSL-Daily\n(Zhou et al., 2021), contain video sequences of signers and corresponding spoken language sen-\ntences. These videos are often much longer than action recognition datasets, which typically use a\nfixed-length short sequence as input (Soomro et al., 2012; Carreira & Zisserman, 2017).\nSLT is generally divided into gloss-based and gloss-free translation. Gloss-based SLT involves\ntranslating sign language into spoken language with the aid of gloss annotations via the utilization\nof the aforementioned Continuous Sign Language Recognition methods. The gloss annotations are\nmanually annotated by expert annotators and depict sign type and order. Initial approaches used\n2\nPublished as a conference paper at ICLR 2024\nCNN and RNN-based models (Camgoz et al., 2018), but transformers have gained prominence in\nneural machine translation (Vaswani et al., 2017), leading to the development of Transformer-based\napproaches for SLT (Camgoz et al., 2020b;a; Chen et al., 2022b; Gan et al.; Zhang et al., 2023a).\nFurther improvements by progressive pretraining of large language models are also achieved by\nusing manually annotated sign-ordered gloss as supervision (Chen et al., 2022a).\nRecently, researchers have explored gloss-free sign language translation to avoid the dependencies\nof manually created gloss annotations. Approaches of finding glosses include using sign spotters\n(Sincan et al., 2023; Shi et al., 2022; Momeni et al., 2022), but these models are limited to the\ndomain of the sign datasets and the vocabulary it was trained on. We take an alternative approach by\nautomatically creating pseudo-glosses from the spoken language sentences. By using sentences and\nsign language priors, we automatically generate pseudo-glosses, overcoming limitations associated\nwith pretrained sign spotting models and manually annotated labels. While techniques for verifying\nthe existence of words exist (Zhao et al., 2021), they utilize separate models for each vocabulary item\nand two-stage translation system, incurring high training costs. In contrast, we propose a singular\nmodel that efficiently determines both the localization and existence of pseudo-glosses. This model\nserves as the foundation for an end-to-end solution in sign language translation.\nOther methods of gloss-free SLT make use of domain transfer through the integration of linguisti-\ncally pretrained models to enhance SLT performance by using pretrained encoder-decoder models\nlike mBart (Liu et al., 2020) and T5 (Raffel et al., 2020) models to improve translation quality (Zhou\net al., 2023; Uthus et al., 2023). We instead use a frozen Generative Pretrained Transformer (GPT)\n(Lin et al., 2021; Radford et al., 2018) as the decoder to enhance translation performance, rather\nthan fine-tuning or progressively pretraining encoder-decoder language models used in previous ap-\nproaches.\nRecently Low-Rank Adapters (Hu et al., 2021), which update only a small number of weights,\nhave been proposed to adapt frozen LLMs to spoken language tasks. They have also been applied\nto multi-modal learning for general visual understanding related tasks (Zhang et al., 2023c; Gao\net al., 2023; Zhang et al., 2023b). These approaches use features extracted from CLIP (Radford\net al., 2021) which are not suitable as a sign language representation due to being trained on image\ncaptions. Our proposed pretraining approach focuses on visual-linguistic sign language features and\nsubsequently adapting them to spoken language models, harnessing the capabilities of LLMs.\n3\nMETHOD\nOur approach aims to leverage the linguistic knowledge inherent in LLMs to enhance Sign Lan-\nguage Translation (SLT). We address two main challenges: handling memory-intensive sign lan-\nguage videos with numerous frames and creating sign representations that seamlessly integrate with\nLLMs. In the following subsections, we outline our model architecture (Figure 1) and training\nstrategy to tackle these challenges and enhance SLT using LLMs.\n3.1\nMODEL ARCHITECTURE\nSpatial Backbone.\nOur video-based sign language translation framework relies on the spa-\ntial model, designed to extract spatial features, Z∗, from input video frames denoted as X =\n{x0, x1, ..., xT ∗} with T ∗frames. These features of dimension C, represented as Z∗∈RT ∗×C,\nare subsequently used by the sign encoder. We employ the Dino-V2 Vision Transformer (Oquab\net al., 2023), specifically the ViT-S/14 variant. Dino-V2 is a self-supervised vision model known for\nits robust feature extraction capabilities across various visual tasks, making it a suitable choice for\nour framework. Fine-tuning Dino-V2 is essential for adapting it to the unique characteristics of sign\nlanguage translation datasets.\nThe spatial model often involves a significant number of parameters, posing memory and computa-\ntional challenges. To address this, we adopt LoRA (Low-Rank Adapters), a lightweight adaptation\ntechnique. LoRA has been shown to be effective in finetuning LLMs (Hu et al., 2021) and Diffusion\nmodels (Roich et al., 2022; Ruiz et al., 2022; Gal et al., 2022). LoRA is applied to the top encoder\nlayers, targeting fully connected layers in the MLP and Multi-Head Attention as shown in Figure 2\n(left). We utilize the class token’s output as a feature vector, undergoing linear transformation and\nbatch normalization to produce Z∗∈RT ∗×C, feeding it into our sign encoder.\n3\nPublished as a conference paper at ICLR 2024\nNorm\nAdapted\nMulti-Head\nAttention\nNorm\nAdapted\nMLP\nNorm\nAdapted\nMasked\nAttention\nNorm\nMLP\n \nZero-Gated\nCross Attention\nNorm\nshared\nshared\nFigure 2: Overview of adapting layers in the spatial model layers (left) and decoder layer (right).\nWe make use of adapters that introduce new low-rank weights to blocks shown by the dashed lines\nwhile keeping the original pretrained weights frozen.\nSign Encoder.\nWe utilize spatial features, Z∗, as input to our sign encoder, aiming to learn spatio-\ntemporal sign representations. Our translation model must handle sequences often comprising hun-\ndreds of frames. To meet this requirement, we designed a spatio-temporal transformer model, draw-\ning inspiration from prior sign language translation approaches (Camgoz et al., 2020b; Yin et al.,\n2023). This model incorporates two crucial modifications to enhance efficiency and effectiveness.\nFirstly, to address the challenge of processing many frames, we employ temporal downsampling\nafter specific layers within our encoder. This downsampling reduces the temporal dimension from\nT ∗to T ∗\n2 using strided averaging with a kernel size of three and a stride of two. This design was\nchosen to balance between computational efficiency and the preservation of temporal information.\nSecondly, we use local self-attention with a window size of seven, a technique proven to be highly\neffective in SLT tasks (Yin et al., 2023). This local attention mechanism is integrated with the\ntemporal downsampling, extending the model’s temporal receptive field deeper into the network.\nThis streamlined approach not only conserves memory but also minimizes redundancy, making it\nhighly compatible with the subsequent decoder model. The output sign representation is denoted\nas Z ∈RT ×C, with T representing the output temporal dimension which has been downsampled\nwhere T = T ∗\n2 .\nLanguage Decoder.\nIn the decoder, we adapt the XGLM model (Lin et al., 2021), a multilingual\nGPT Language Model known for its versatility in few-shot learning on text data. We chose the 1.7B\nparameter variant of XGLM to balance performance and memory utilization. We draw inspiration\nfrom successful language model adaptation techniques like zero-gated cross-attention and LoRA\nwhich have been used to adapt LLMs to different textual and multi-model tasks (Gao et al., 2023;\nZhang et al., 2023c;b; Alayrac et al., 2022).\nBefore passing the sign features to the decoder, we map our sign features to the decoder’s dimension\nwith a linear layer FCm. To adapt the XGLM decoder for sign language translation, we employ the\nzero-gated multi-head cross-attention to the decoder layer as shown in Figure 2 (right). This en-\nhancement shares weights from the pretrained masked multi-head attention and integrates a separate\nLoRA for masked multi-head attention (Adapted Masked Attention) and cross-attention (Zero-Gated\nCross Attention). The sign features are first passed through the frozen decoder’s layer normaliza-\ntion and then used as keys for cross-attention. To integrate sign features without overshadowing\nlinguistic features, we employ gated scaled dot-product attention into the cross-attention:\nGatedAttention(Q, K, V ) =\n\u0012\ng × softmax\n\u0012QKT\n√dk\n\u0013\u0013\nV\n(1)\nK and V represent the inputs from the key and value derived from the sign features, while Q orig-\ninates from the textual features. g is a learnable gate parameter for each attention head which is\nclamped between 0 and 1 and initialized to zero to preserve linguistic knowledge at the start of train-\ning. This modification allows XGLM to adapt seamlessly to sign language. The gate parameter and\n4\nPublished as a conference paper at ICLR 2024\nLoRA parameters drive this adaptability, gradually incorporating sign features while leveraging lin-\nguistic knowledge. As depicted in Figure 2 (right), the outputs from the Adapted Masked Attention\nand Zero-Gated Cross Attention are summed together.\n3.2\nTRAINING STRATEGY\nOur model architecture (Figure 1) allows direct video-to-text training for SLT. We prioritize high-\nquality sign features by concentrating most of the trainable parameters in the sign encoder. We\nfreeze the pretrained vision and language model and use the adapters for sign language domain\ntransfer. This approach ensures our model’s primary focus on capturing and utilizing sign language\nfeatures and then leverages the language model’s linguistic ability to adapt the features to spoken\nlanguage translation. The result is an architecture tailored to SLT, enabling effective translation from\nvideo input to textual output. In our framework, the spatial model and decoder are pretrained from\nlarge-scale datasets while the sign encoder is randomly initialized. We therefore develop a gloss-free\nstrategy to pretrain the sign encoder.\n3.3\nPRETRAINING STAGE\nPseudo-gloss generation.\nTo perform pseudo-gloss generation, we extract pseudo-gloss from\neach spoken language sentence using the spaCy natural language processing library (Honnibal et al.,\n2020). In the case of German (Phoenix14T), this involves lemmatization, while for Chinese (CSL-\nDaily), it encompasses word segmentation. The preprocessing step enables the extraction of the base\nforms of words.\nFollowing this step, we apply Parts-of-Speech (POS) tagging, retaining only words categorized as\n[”NOUN”, ”NUM”, ”ADV”, ”PRON”, ”PROPN”, ”ADJ”, ”VERB”]. This filtering process pri-\noritizes words that are most likely to convey meaningful information, ensuring that our extracted\npseudo-glosses are semantically relevant, retain most of the sentence context and have potential\ngloss correspondence. The lemmatized words (in German) or segmented words (in Chinese) that\nsatisfy this filter are considered as pseudo-glosses. It’s important to highlight that our pseudo-\nglosses are in spoken language order, unlike manually annotated glosses which are in sign order.\nConsequently, conventional approaches such as the CTC loss (Graves et al., 2006) are not suitable\nfor our pseudo-glosses.\nSign\nFeatures\nMapped\nFeatures\nsim\nscore\nFigure 3: Overview of pretraining process, which takes the sign features as input and predicts the\nexistence of pseudo-glosses.\nPseudo-gloss pretraining.\nOur goal is to enable the sign encoder to learn visual-linguistic rep-\nresentations. CLIP’s ability to unify visual and linguistic domains (Radford et al., 2021) motivates\nthe design of our sign encoder to develop sign features using pseudo-glosses for Sign Language\nTranslation (SLT), as illustrated in Figure 3.\nWe generate prototypes for each pseudo-gloss. The aim is to ensure that the sign encoder generates\nrepresentations that closely align with these prototypes when they are present within the sign video.\nThese prototypes are initialized with word embeddings obtained from fastText (Joulin et al., 2016),\neach of dimension D = 300. Additionally, we include an extra prototype initialized with zeros,\nwhich serves as the prototype for sign transitions or non-sign-related components. As a result, the\nprototype matrix takes the form of P ∈RD×U.\nThe sign features Z = {z0, z1, ..., zi, ..., zT } are learned to be aligned with the prototypes if the\nassociated pseudo-gloss exists within the sign video. We therefore project Z to Z′ ∈RT ×D through\na linear layer and compute the cosine similarity between the prototypes and each of the projected\n5\nPublished as a conference paper at ICLR 2024\nsign features such that:\nsi = sim(z′\ni, P) =\nz′\ni · P\n∥ˆzi∥∥P∥\n(2)\nwhich is the cosine similarity score for the ith feature index in T and S = {s0, s1, ..., sT } ∈RT ×U,\n−1 ≤si ≤1. High scores indicate similarity, and low scores indicate dissimilarity. We then\nintroduce temporal probability ( ˆST ) and prototype probability ( ˆSU) scores using temperature-scaled\nsoftmax operations across the time and prototype axis:\nˆST = softmaxT (S/τT )\n(3)\nˆSU = softmaxU(S/τU)\n(4)\nThese scores emphasize temporal and class-related aspects of similarity and ensure they fall within\nthe range of 0 to 1. Learnable scaling factors, τT and τU, modulate the extent of temporal and class\nsimilarity, allowing control of their influence on prototype creation. Element-wise multiplication of\nˆST and ˆSU yields E ∈RT ×U, forming the basis for prototype localization. To discern the presence\nor absence of prototypes, we aggregate E values over the temporal dimension, resulting in ˆE ∈RU\nsuch that:\nˆ\nEj =\nT\nX\ni=0\nEi,j =\nT\nX\ni=0\n( ˆST\ni,j × ˆSU\ni,j)\n(5)\nwhere 0 ≤Ei,j ≤1. High ˆEj value for the jth prototype indicates the presence of the prototype,\nwhile low values signify the absence. We employ binary cross-entropy loss to train the model,\noptimizing the presence or absence of prototypes. For our case each prototype is assigned as 1 if the\npseudo-gloss exists within the sign video and 0 if not.\nNote that during pretraining the learned sign representations are temporally invariant, therefore we\nalso add sinusoidal positional encoding before FCm for the translation task. The resulting pretraining\nallows the weights to be initialized with vision priors (spatial model), sign priors (sign encoder) and\nlinguistic priors (decoder). This enables us to then use these models for the downstream translation\ntask.\n4\nRESULTS\n4.1\nDATASETS AND EVALUATION PROTOCOL\nWe evaluate our approach using two distinct sign language translation datasets.\nRWTH-\nPHOENIX-WEATHER-2014T (Phoenix14T) (Camgoz et al., 2018) is a German Sign Language\ndataset for sign to spoken language translation tasks. It encompasses translations from German Sign\nLanguage to the German language, derived from German weather broadcast videos. CSL-Daily\n(Zhou et al., 2021) is a translation dataset focusing on Chinese Sign Language to Chinese. It com-\nprises of lab recorded videos that cover various daily interaction topics, including travel, family life,\nbank service and shopping.\nWe evaluate the translation performance using standard metrics commonly employed in sign lan-\nguage translation. These metrics include BLEU (Bilingual Evaluation Understudy) (Papineni et al.,\n2002) and ROUGE-L (Lin, 2004) scores. BLEU measures the similarity between machine-generated\nand reference translations based on n-gram overlap. ROUGE-L assesses translation quality by cal-\nculating the length of the longest common sub-sequence between generated and reference texts.\n4.2\nTRAINING SETTINGS\nThe model is trained end-to-end with a batch size of 8 on two A100 GPUs, subsampling every\nsecond frame. Due to memory constraints we apply spatial adapters to the top 3 layers of the spatial\nmodel. The sign encoder is a 4 layer transformer with hidden dimension of 512, 8 attention heads\nand intermediate size of 2048. The temporal downsampling is applied after the 2nd layer. Training\nis conducted in bfloat16 and Flash attention v2 (Dao, 2023) is used to optimise memory usage. We\nexplain further details of the libraries and models in Appendix A.1. We employ the Adam optimizer\n(Kingma & Ba, 2014) with a learning rate of 3 × 10−4 and weight decay of 0.001. Training spans\n6\nPublished as a conference paper at ICLR 2024\n100 epochs with gradient clipping of 1.0 and includes a one-cycle cosine learning rate scheduler\n(Loshchilov & Hutter, 2016) with warmup for the initial 5 epochs. Data augmentation techniques,\nsuch as color jitter, random resized cropping from 256×256 to 224×224 pixels, frame rotations and\nhorizontal flips are consistently applied across all frames within video sequences. During evaluation,\nwe use center cropping to 224 × 224 pixels.\nPretraining.\nWe initialize the prototype (τU) and time temperature (τT ) to 0.1. In Table 1(a),\nwe present the counts of extracted pseudo-glosses for each dataset using the approach described\nin Section 3.3. For the CSL-Daily dataset, the number of pseudo-glosses significantly exceeds the\nvocabulary size. This discrepancy arises because the vocabulary is based on Chinese characters,\nwhile pseudo-glosses represent word segmentation that convey meaning.\nDownstream Translation.\nWe utilize cross-entropy loss with label smoothing set to 0.1 during\ntraining. The LoRA rank and alpha values are both set to 4. During inference, we employ a beam\nsearch with a width of 4. In Table 1(b), we present detailed information regarding the number of\ntrainable parameters for each component of our network during the downstream training phase. No-\ntably, the sign encoder contains a significantly larger number of trainable parameters, emphasizing\nits role in learning sign features. For the CSL-Daily dataset, we implement additional tokenization\nprocessing to handle unknown tokens, as elaborated in Appendix A.2.\nTable 1: Training setting with (a) the number of pseudo-glosses during pretraining and (b) parameter\ncounts during downstream translation.\nDataset\n# Vocab\n# p-glosses\nPhoenix14T\n2, 887\n2, 533\nCSL-Daily\n2, 343\n7, 918\n(a) Vocabulary versus pseudo-glosses per dataset\nComponent\n# Params\n# Trainable\nSpatial\n22,328,448\n271,872\nSign Encoder\n12,613,632\n12,613,632\nDecoder\n1,736,710,528\n3,803,520\nTotal\n1,771,652,608\n16,689,024\n(b) Number of model parameters during translation\n4.3\nCOMPARISONS WITH STATE-OF-THE-ART METHODS\nResults on Phoenix14T.\nIn Table 2, we present a comparative analysis between our approach and\nstate-of-the-art methods for sign language translation on Phoenix14T. We observe an approximate\nimprovement of 1.1 BLEU4 on the test set when doing pseudo-gloss pretraining (PGP) followed\nby downstream translation (Sign2GPT(w/PGP)). Our gloss-free approach demonstrates substantial\nimprovements in BLEU-1,2,3 scores, surpassing the performance of previous gloss-free methods\nand reduces the performance gap between gloss-free and gloss-based SLT. This progress can be at-\ntributed to our approach’s capability to learn representations for individual pseudo-gloss, in contrast\nto prior gloss-free methods that primarily focused on sentence-based representations. Notably, even\nwithout pretraining (Sign2GPT), our approach remains competitive with previous state-of-the-art\n(SOTA) gloss-free results. We also find that downstream training with only the trainable decoder\nparameters and extract sign features from our frozen pretrained stage model, Sign(Z)2GPT(w/PGP),\nis able to achieve competitive results with under 4 million parameters showing the effectiveness of\nour learnt sign representations.\nResults on CSL-Daily.\nIn Table 3, we present the results of our SLT experiments on the CSL-\nDaily dataset. We observe a substantial performance increase in BLEU-4 compared to previous\nSOTA gloss-free results, with an approximate improvement of 4.4 BLEU-4 when utilizing our pre-\ntraining (Sign2GPT(w/PGP)). Sign2GPT(w/PGP) significantly outperforms training without PGP\n(Sign2GPT), with a notable 3.5 BLEU-4 improvement on the CSL-Daily dataset.\n4.4\nQUALITATIVE RESULTS\nWe visually demonstrate the effectiveness of our pseudo-gloss pretraining phase for sign localization\nin Figure 4 and Appendix A.4. To achieve this, we leverage the output values of E, with dimensions\n7\nPublished as a conference paper at ICLR 2024\nTable 2: Comparison of test set results on Phoenix14T. We present our gloss-free results for three\nexperimental settings: (1) Without pseudo-gloss pretraining (Sign2GPT), (2) with pseudo-gloss\npretraining (Sign2GPT(w/PGP)), and (3) extracted features Z from the frozen spatial and sign\nencoder model that has been trained with pseudo-gloss pretraining (Sign(Z)2GPT(w/PGP)).\nMethod\nTest Set\nBLEU1\nBLEU2\nBLEU3\nBLEU4\nROUGE\nGloss-based\nSL-Transformer (Camgoz et al., 2020b)\n46.61\n33.73\n26.19\n21.32\n−\nBN-TIN-Transf.+BT (Zhou et al., 2021)\n50.80\n37.75\n29.72\n24.32\n49.54\nMMTLB (Chen et al., 2022a)\n53.97\n41.75\n33.84\n28.39\n52.65\nSLTUNET (Zhang et al., 2023a)\n52.92\n41.76\n33.99\n28.47\n52.11\nTwoStream-SLT (Chen et al., 2022b)\n54.90\n42.43\n34.46\n28.95\n53.48\nGloss-free\nNSLT (Camgoz et al., 2018)\n29.86\n17.52\n11.96\n9.00\n30.70\nTSPNet (Li et al., 2020b)\n36.10\n23.12\n16.88\n13.41\n34.96\nCSGCR (Zhao et al., 2021)\n36.71\n25.40\n18.86\n15.18\n38.85\nGASLT (Yin et al., 2023)\n39.07\n26.74\n21.86\n15.74\n39.86\nGFSLT (Zhou et al., 2023)\n41.39\n31.00\n24.20\n19.66\n40.93\nGFSLT-VLP (Zhou et al., 2023)\n43.71\n33.18\n26.11\n21.44\n42.49\nSign2GPT\n45.43\n32.03\n24.23\n19.42\n45.23\nSign2GPT(w/PGP)\n49.54\n35.96\n28.83\n22.52\n48.90\nSign(Z)2GPT(w/PGP)\n47.06\n33.61\n25.85\n20.93\n47.11\nTable 3: Comparison of test set results on the CSL-Daily. We present gloss-free results for three\nexperimental settings: (1) Without pseudo-gloss pretraining (Sign2GPT), (2) with pseudo-gloss\npretraining (Sign2GPT(w/PGP)), and (3) extracted features Z from the frozen spatial and sign\nencoder model that has been trained with pseudo-gloss pretraining (Sign(Z)2GPT(w/PGP)).\nMethod\nTest Set\nBLEU1\nBLEU2\nBLEU3\nBLEU4\nROUGE\nGloss-based\nSL-Transformer (Camgoz et al., 2020b)\n37.38\n24.36\n16.55\n11.79\n36.74\nBN-TIN-Transf.+BT (Zhou et al., 2021)\n51.42\n37.26\n27.76\n21.34\n49.31\nMMTLB (Chen et al., 2022a)\n53.31\n40.41\n30.87\n23.92\n53.25\nSLTUNET (Zhang et al., 2023a)\n54.98\n41.44\n31.84\n25.01\n54.08\nTwoStream-SLT (Chen et al., 2022b)\n55.44\n42.59\n32.87\n25.79\n55.72\nGloss-free\nGASLT (Yin et al., 2023)\n19.90\n9.94\n5.98\n4.07\n20.35\nNSLT (Camgoz et al., 2018)\n34.16\n19.57\n11.84\n7.56\n34.54\nGFSLT (Zhou et al., 2023)\n37.69\n23.28\n14.93\n9.88\n35.16\nGFSLT-VLP (Zhou et al., 2023)\n39.37\n24.93\n16.26\n11.00\n36.44\nSign2GPT\n34.80\n24.00\n17.27\n12.96\n41.12\nSign2GPT(w/PGP)\n41.75\n28.73\n20.60\n15.40\n42.36\nSign(Z)2GPT(w/PGP)\n32.73\n20.52\n13.75\n9.73\n33.39\nT × U. The values in this matrix vary between 0 (indicating the absence of a sign) and 1 (indicating\nthe presence of a sign), reflecting the temporal occurrence of pseudo-glosses. Our observations sug-\ngest that our pretraining approach holds promise for sign spotting, even in scenarios where precise\nlocalization is not available. This aspect may warrant further exploration as a potential avenue for\nfuture research.\n8\nPublished as a conference paper at ICLR 2024\n他们\n想\n时候\n去\n买\n椅子\n...\n...\nnun\nwettervorhersage\nmorgen\nsamstag\nzweiter\noktober\n...\n他们想什么时候去买椅子？\n...\nnun\nwettervorhersage\nmorgen\nsamstag\noktober\n椅子\n他们\n时候\n去\n买\nund nun die wettervorhersage für morgen samstag den zweiten oktober\nFigure 4: Visualizations of the localization capabilities of our pretraining stage. We visualize only\nthe pseudo-glosses from the target sentence (y-axis) over time (x-axis), with whiter regions indicat-\ning a higher probability of the pseudo-gloss occurring during the time segment. We also display the\nlocalized gloss (under the video frames) based on a threshold of 0.2 on E.\n4.5\nABLATION STUDY\nWe conduct our ablation studies on the Phoenix14T dataset, evaluating the BLEU-4 score on the\ndevelopment set. In Table 4, we present the results of architectural modifications made to our net-\nwork. For our initial study, we conducted experiments without pretraining Sign2GPT. We observed\nsignificant performance improvements when incorporating spatial adapter. Conversely, the use of\nglobal attention led to a reduction in model performance compared to local attention. Furthermore,\nwe investigated the impact of downsampling on the model’s performance. While downsampling\nshowed only marginal improvements in terms of the BLEU-4 score, it notably reduced computa-\ntional complexity. This reduction is due to only half the number of features being passed through\nthe decoder during training.\nOur pretraining generates representations for pseudo-glosses, which inherently removes temporal\ninformation from the output sign representations. Subsequently, we investigated methods to reintro-\nduce temporal information into the model using various approaches as described in Section 3.3. We\nexplored learnable approaches for positional embeddings with both zero and random initialization.\nHowever, these approaches yielded minimal to no discernible benefits. In contrast, the utilization of\nsinusoidal positional embeddings demonstrated substantial performance improvements.\nTable 4: Ablation of results on the Phoenix14T dataset showing different architecture changes with\nno pseudo-gloss pretraining (Sign2GPT) and with pseudo-gloss pretraining (Sign2GPT(w/PGP)).\nArchitecture\nBLEU4\nSign2GPT\nSpatial Adapters + Local Attention + Downsampling\n19.55\n· No Spatial Adapters\n16.38\n· No Local Attention (+ Global Attention)\n18.56\n· No Downsampling on Sign Encoder\n19.30\nSign2GPT(w/PGP)\n· No positional\n21.68\n· Learnable positional (zero init)\n21.89\n· Learnable positional (random init)\n21.16\n· Sinusoidal positional\n23.20\n5\nCONCLUSIONS\nIn this paper, we have presented a novel approach to address the challenging problem of Sign Trans-\nlation in a gloss-free setting. Our method, Sign2GPT, demonstrates significant performance im-\nprovements over existing state-of-the-art techniques on the Phoenix14T and CSL-Daily datasets.\nWe introduce a novel pretraining strategy that learns from pseudo-glosses which are generated au-\ntomatically to learn word-level sign features, thereby allowing our sign encoder to be effectively\npretrained without the use of manually annotated glosses. Moreover, the proposed Sign2GPT ar-\nchitecture presents a promising direction for the exploration of fusing visual features to spoken\nlanguage models for sign language recognition and translation tasks.\n9\nPublished as a conference paper at ICLR 2024\nREPRODUCIBILITY STATEMENT\nTo facilitate reproducibility, we have provided details of the training settings in Section 4.2 with\nadditional details of the libraries we used for the pretrained models in Appendix A.1. We also give\nfurther details of the CSL tokenization and post processing to address issues with unknown tokens\nin Appendix A.2.\nACKNOWLEDGEMENTS\nThis work was supported by the EPSRC project ExTOL (EP/R03298X/1), SNSF project ’SMILE\nII’ (CRSII5 193686), European Union’s Horizon2020 programme (’EASIER’ grant agreement\n101016982) and the Innosuisse IICT Flagship (PFFS-21-47). This work reflects only the authors\nview and the Commission is not responsible for any use that may be made of the information it\ncontains. Neither Necati Cihan Camgoz nor Meta were involved in the model training, evaluation,\nor use of the datasets.\nREFERENCES\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel\nLenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language\nmodel for few-shot learning. Advances in Neural Information Processing Systems, 35:23716–\n23736, 2022.\nSamuel Albanie, G¨ul Varol, Liliane Momeni, Triantafyllos Afouras, Joon Son Chung, Neil Fox, and\nAndrew Zisserman. Bsl-1k: Scaling up co-articulated sign language recognition using mouthing\ncues. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28,\n2020, Proceedings, Part XI 16, pp. 35–53. Springer, 2020.\nP Boyes Braem and RL Sutton-Spence. The Hands Are The Head of The Mouth. The Mouth as\nArticulator in Sign Languages. Hamburg: Signum Press, 2001.\nNecati Cihan Camgoz, Simon Hadfield, Oscar Koller, Hermann Ney, and Richard Bowden. Neural\nsign language translation. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pp. 7784–7793, 2018.\nNecati Cihan Camgoz, Oscar Koller, Simon Hadfield, and Richard Bowden. Multi-channel trans-\nformers for multi-articulatory sign language translation. In Computer Vision–ECCV 2020 Work-\nshops: Glasgow, UK, August 23–28, 2020, Proceedings, Part IV 16, pp. 301–319. Springer,\n2020a.\nNecati Cihan Camgoz, Oscar Koller, Simon Hadfield, and Richard Bowden. Sign language trans-\nformers: Joint end-to-end sign language recognition and translation.\nIn Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition, pp. 10023–10033, 2020b.\nJoao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the kinetics\ndataset. In proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.\n6299–6308, 2017.\nYutong Chen, Fangyun Wei, Xiao Sun, Zhirong Wu, and Stephen Lin. A simple multi-modality\ntransfer learning baseline for sign language translation. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, pp. 5120–5130, 2022a.\nYutong Chen, Ronglai Zuo, Fangyun Wei, Yu Wu, Shujie Liu, and Brian Mak. Two-stream net-\nwork for sign language recognition and translation. Advances in Neural Information Processing\nSystems, 35:17043–17056, 2022b.\nRunpeng Cui, Hu Liu, and Changshui Zhang. Recurrent convolutional neural networks for continu-\nous sign language recognition by staged optimization. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pp. 7361–7369, 2017.\nTri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. 2023.\n10\nPublished as a conference paper at ICLR 2024\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An\nimage is worth 16x16 words: Transformers for image recognition at scale.\narXiv preprint\narXiv:2010.11929, 2020.\nRinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel\nCohen-Or. An image is worth one word: Personalizing text-to-image generation using textual\ninversion. arXiv preprint arXiv:2208.01618, 2022.\nShiwei Gan, Yafeng Yin, Zhiwei Jiang, Kang Xia, Lei Xie, and Sanglu Lu. Contrastive learning for\nsign language recognition and translation.\nPeng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu,\nConghui He, Xiangyu Yue, et al. Llama-adapter v2: Parameter-efficient visual instruction model.\narXiv preprint arXiv:2304.15010, 2023.\nAlex Graves, Santiago Fern´andez, Faustino Gomez, and J¨urgen Schmidhuber. Connectionist tem-\nporal classification: labelling unsegmented sequence data with recurrent neural networks. In\nProceedings of the 23rd international conference on Machine learning, pp. 369–376, 2006.\nJohn Hewitt. Initializing new word embeddings for pretrained language models. https:/nlp.\nstanford.edu/˜johnhew//vocab-expansion.html, 2021.\nMatthew Honnibal, Ines Montani, Sofie Van Landeghem, and Adriane Boyd. spaCy: Industrial-\nstrength Natural Language Processing in Python. 2020. doi: 10.5281/zenodo.1212303.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen.\nLora: Low-rank adaptation of large language models.\narXiv preprint\narXiv:2106.09685, 2021.\nSongyao Jiang, Bin Sun, Lichen Wang, Yue Bai, Kunpeng Li, and Yun Fu. Skeleton aware multi-\nmodal sign language recognition. In Proceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, pp. 3413–3423, 2021.\nArmand Joulin, Edouard Grave, Piotr Bojanowski, Matthijs Douze, H´erve J´egou, and Tomas\nMikolov. Fasttext.zip: Compressing text classification models. arXiv preprint arXiv:1612.03651,\n2016.\nHamid Reza Vaezi Joze and Oscar Koller. Ms-asl: A large-scale data set and benchmark for under-\nstanding american sign language. arXiv preprint arXiv:1812.01053, 2018.\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980, 2014.\nOscar Koller, Jens Forster, and Hermann Ney. Continuous sign language recognition: Towards large\nvocabulary statistical recognition systems handling multiple signers. Computer Vision and Image\nUnderstanding, 141:108–125, 2015.\nDongxu Li, Cristian Rodriguez, Xin Yu, and Hongdong Li. Word-level deep sign language recog-\nnition from video: A new large-scale dataset and methods comparison. In Proceedings of the\nIEEE/CVF winter conference on applications of computer vision, pp. 1459–1469, 2020a.\nDongxu Li, Chenchen Xu, Xin Yu, Kaihao Zhang, Benjamin Swift, Hanna Suominen, and Hong-\ndong Li. Tspnet: Hierarchical feature learning via temporal semantic pyramid for sign language\ntranslation. Advances in Neural Information Processing Systems, 33:12034–12045, 2020b.\nChin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization\nbranches out, pp. 74–81, 2004.\nXi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle\nOtt, Naman Goyal, Shruti Bhosale, Jingfei Du, et al. Few-shot learning with multilingual language\nmodels. arXiv preprint arXiv:2112.10668, 2021.\n11\nPublished as a conference paper at ICLR 2024\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis,\nand Luke Zettlemoyer. Multilingual denoising pre-training for neural machine translation. Trans-\nactions of the Association for Computational Linguistics, 8:726–742, 2020.\nIlya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv\npreprint arXiv:1608.03983, 2016.\nLiliane Momeni, Hannah Bull, KR Prajwal, Samuel Albanie, G¨ul Varol, and Andrew Zisserman.\nAutomatic dense annotation of large-vocabulary sign language videos. In European Conference\non Computer Vision, pp. 671–690. Springer, 2022.\nMaxime Oquab, Timoth´ee Darcet, Th´eo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov,\nPierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning\nrobust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic\nevaluation of machine translation. In Proceedings of the 40th annual meeting of the Association\nfor Computational Linguistics, pp. 311–318, 2002.\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language under-\nstanding by generative pre-training. 2018.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International conference on machine learning, pp.\n8748–8763. PMLR, 2021.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text\ntransformer. The Journal of Machine Learning Research, 21(1):5485–5551, 2020.\nDaniel Roich, Ron Mokady, Amit H Bermano, and Daniel Cohen-Or. Pivotal tuning for latent-based\nediting of real images. ACM Transactions on Graphics (TOG), 42(1):1–13, 2022.\nNataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman.\nDreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. arXiv\npreprint arXiv:2208.12242, 2022.\nBowen Shi, Diane Brentari, Greg Shakhnarovich, and Karen Livescu. Open-domain sign language\ntranslation learned from online video. arXiv preprint arXiv:2205.12870, 2022.\nOzge Mercanoglu Sincan and Hacer Yalim Keles. Autsl: A large scale multi-modal turkish sign\nlanguage dataset and baseline methods. IEEE Access, 8:181340–181355, 2020.\nOzge Mercanoglu Sincan, Necati Cihan Camgoz, and Richard Bowden. Is context all you need?\nscaling neural sign language translation to large domains of discourse. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision, pp. 1955–1965, 2023.\nKhurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions\nclasses from videos in the wild. arXiv preprint arXiv:1212.0402, 2012.\nDu Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann LeCun, and Manohar Paluri. A closer\nlook at spatiotemporal convolutions for action recognition. In Proceedings of the IEEE conference\non Computer Vision and Pattern Recognition, pp. 6450–6459, 2018.\nDavid Uthus, Garrett Tanzer, and Manfred Georg. Youtube-asl: A large-scale, open-domain ameri-\ncan sign language-english parallel corpus. arXiv preprint arXiv:2306.15162, 2023.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural informa-\ntion processing systems, 30, 2017.\nRyan Wong, Necati Cihan Camgoz, and Richard Bowden. Learnt contrastive concept embeddings\nfor sign recognition. In Proceedings of the IEEE/CVF International Conference on Computer\nVision, pp. 1945–1954, 2023.\n12\nPublished as a conference paper at ICLR 2024\nSijie Yan, Yuanjun Xiong, and Dahua Lin.\nSpatial temporal graph convolutional networks for\nskeleton-based action recognition. In Proceedings of the AAAI conference on artificial intelli-\ngence, volume 32, 2018.\nHuijie Yao, Wengang Zhou, Hao Feng, Hezhen Hu, Hao Zhou, and Houqiang Li. Sign language\ntranslation with iterative prototype. arXiv preprint arXiv:2308.12191, 2023.\nJinhui Ye, Wenxiang Jiao, Xing Wang, Zhaopeng Tu, and Hui Xiong. Cross-modality data augmen-\ntation for end-to-end sign language translation. arXiv preprint arXiv:2305.11096, 2023.\nAoxiong Yin, Tianyun Zhong, Li Tang, Weike Jin, Tao Jin, and Zhou Zhao. Gloss attention for\ngloss-free sign language translation. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pp. 2551–2562, 2023.\nBiao Zhang, Mathias M¨uller, and Rico Sennrich. Sltunet: A simple unified model for sign language\ntranslation. arXiv preprint arXiv:2305.01778, 2023a.\nHang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language\nmodel for video understanding. arXiv preprint arXiv:2306.02858, 2023b.\nRenrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng\nGao, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init atten-\ntion. arXiv preprint arXiv:2303.16199, 2023c.\nJian Zhao, Weizhen Qi, Wengang Zhou, Nan Duan, Ming Zhou, and Houqiang Li. Conditional\nsentence generation and cross-modal reranking for sign language translation. IEEE Transactions\non Multimedia, 24:2662–2672, 2021.\nJiangbin Zheng, Yile Wang, Cheng Tan, Siyuan Li, Ge Wang, Jun Xia, Yidong Chen, and Stan Z\nLi. Cvt-slr: Contrastive visual-textual transformation for sign language recognition with varia-\ntional alignment. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 23141–23150, 2023.\nBenjia Zhou, Zhigang Chen, Albert Clap´es, Jun Wan, Yanyan Liang, Sergio Escalera, Zhen Lei, and\nDu Zhang. Gloss-free sign language translation: Improving from visual-language pretraining.\narXiv preprint arXiv:2307.14768, 2023.\nHao Zhou, Wengang Zhou, Weizhen Qi, Junfu Pu, and Houqiang Li. Improving sign language\ntranslation with monolingual data by sign back-translation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pp. 1316–1325, 2021.\nRonglai Zuo, Fangyun Wei, and Brian Mak. Natural language-assisted sign language recognition.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.\n14890–14900, 2023.\nA\nAPPENDIX\nA.1\nLIBRARIES\nSpatial Backbone. The spatial backbone is ViT-S/14 distilled with the pretrained model down-\nloaded from github.com/facebookresearch/dinov2.\nDecoder. We use the HuggingFace library for the pretrained XGLM transformer model and tok-\nenization, specifically the pretrained weights from facebook/xglm-1.7B.\nPseudo-gloss generation.\nFor pseudo-gloss generation we use the SpaCy library.\nWe use\nthe German pipeline loaded from de core news sm for generating our pseudo-glosses for the\nPhoenix14T dataset. For the CSL-Daily dataset we use zh core web sm.\nFastText embeddings. We load the fastText embeddings from cc.de.300.bin for the German\nPhoenix14T dataset and cc.zh.300.bin for the Chinese CSL Dataset.\n13\nPublished as a conference paper at ICLR 2024\nA.2\nCSL TOKENIZATION AND POST PROCESSING\nBy using the pretrained tokenization from XGLM, we encountered unknown tokens during CSL-\ndaily training when Chinese characters were absent from the vocabulary. To address this, we per-\nformed vocabulary expansion, utilizing the average embedding technique by Hewitt (2021) for ini-\ntializing new tokens with slight noise. We maintained the consistency of our proposed architecture\nby keeping these new embeddings frozen throughout training, avoiding the introduction of addi-\ntional learnable parameters. In total, there are 252 new tokens added to the vocabulary. We also\napply post-processing of replacing punctuation tokens at inference time as the pretrained tokenizer\nconverts punctuation to the Unicode equivalent such that ’?’, ’!’, ’:’ and ’,’ are replaced by ’？’,\n’！’, ’：’ and ’，’ respectively.\nA.3\nQUALITATIVE TRANSLATION EXAMPLES\nIn Table 5 and 6 we present randomly selected qualitative translation results on Phoenix14T and\nCSL-Daily respectively. We also display the extracted pseudo-glosses for each of the target sen-\ntences which capture the core content of the sentences. Our findings demonstrate that our transla-\ntion model successfully generates sentences that effectively capture the semantic content of spoken\nlanguage sentences, albeit with variations in sentence structure.\nTable 5: Examples of translation results on the Phoenix14T dataset.\nHypothesis:\nim ¨ubrigen land scheint h¨aufig die sonne und es gibt nur wenig schauer . (in the\nrest of the country the sun often shines and there are only a few showers .)\nPseudo-glosses:\n¨ubrig, gebiet, sonne, nur, locker, wolke\nReference:\nin den ¨ubrigen gebieten viel sonne und nur ein paar lockere wolken . (lots of\nsun in the remaining areas and only a few loose clouds .)\nHypothesis:\nam tag zw¨olf grad an der ostsee und bis zu zwanzig grad im s¨uden . (twelve\ndegrees a day on the baltic sea and up to twenty degrees in the south .)\nPseudo-glosses:\ntag, zw¨olf, grad, ostsee, zwanzig, grad, niederbayer\nReference:\nam tag zw¨olf grad an der ostsee und bis zwanzig grad in niederbayern . (on\nthe day twelve degrees on the baltic sea and up to twenty degrees in lower\nbavaria . )\nHypothesis:\nich w¨unsche ihnen noch einen sch¨onen abend und machen sie es gut . (i wish\nyou a nice evening and do well .)\nPseudo-glosses:\nich, w¨unschen, ihnen, sch¨on, abend, machen, sie, es, gut\nReference:\nich w¨unsche ihnen einen sch¨onen abend und machen sie es gut . ( i wish you a\nnice evening and do well .)\nHypothesis:\nder wind weht schwach bis m¨aßig aus s¨ud bis s¨udost . ( the wind blows weakly\nto moderately from the south to southeast .)\nPseudo-glosses:\ndazu, wehen, schwach, wind, s¨udost, s¨ud\nReference:\ndazu weht ein schwacher bis m¨aßiger wind aus s¨udost bis s¨ud . ( in addition a\nweak to moderate wind blows from the southeast to the south .)\nA.4\nQUALITATIVE PSEUDO-GLOSS LOCALIZATION EXAMPLES\nWe visualize the localization capabilities of our pretraining on Phoenix14T in Figure 5 and CSL-\nDaily in Figure 6. We notice that our pretraining has automatic localization capabilities irrespective\nof the order of the pseudo-gloss when using the output E. This has the potential future avenues for\nautomatically creating sign-ordered glosses based on thresholds of E. We observe that while we\nprovide no information about the localization of the pseudo-glosses, our approach is able to identify\nthe potential glosses and the sign order of these identified glosses.\n14\nPublished as a conference paper at ICLR 2024\nTable 6: Examples of translation results on the CSL-Daily dataset.\nHypothesis:\n这个地方不离饭店，走几步就到饭店的门口。(This place is not far from\nthe hotel, just a few steps to the door of the hotel.)\nPseudo-glosses:\n可以/ 这里/ 不/ 远/ 有/ 饭馆/ 走/ 几/ 分钟/ 就/ 到\nReference:\n可以，离这里不远有一个饭馆，走几分钟就到了。(Okay, there is a\nrestaurant not far from here, it can be reached in a few minutes’ walk.)\nHypothesis:\n公司很远，他为什么不打车呢？(The company is far away, why doesn’t\nhe take a taxi?)\nPseudo-glosses:\n公司/离家/ 很/ 远/ 他/ 为什么/ 不/ 打车\nReference:\n公司离家很远，他为什么不打车？(The company is far from home, why\ndoesn’t he take a taxi?)\nHypothesis:\n我不去爬山，我有事情要去做。(I’m not going to climb mountains, I have\nthings to do.)\nPseudo-glosses:\n我/ 不/ 去/ 爬山/ 我/ 有事\nReference:\n我不去爬山，我有事。( I’m not going to climb the mountain, I have some-\nthing to do.)\nHypothesis:\n我喜欢下雪。(I like snow.)\nPseudo-glosses:\n我/ 喜欢/ 冬天/ 下雪/ 太/ 美\nReference:\n我喜欢冬天，下雪太美了。( I like winter, the snow is so beautiful.)\nB\nADDITIONAL ABLATION STUDIES\nB.1\nIMPACT OF SPATIAL BACKBONE\nIn Table 7, we present comparisons between the performance of a ResNet18 spatial backbone and\nthe proposed adapted ViT model within our Sign2GPT architecture. We find that the frozen ViT\nmodel trained with DinoV2 pretraining, incorporating adapters, exhibits slightly better performance\ncompared to the ResNet18 backbone. A noteworthy advantage of the adapted ViT model lies in the\napplication of LoRA instead of fine-tuning all parameters. Consequently, the proposed backbone\nhas under three hundred thousand trainable parameters, a significant reduction compared to the 11\nmillion trainable parameters of the ResNet18 model.\nTable 7: Ablation of different spatial backbones on CSL-Daily using our Sign2GPT architecture\nwithout Pseudo-Gloss Pretraining.\nSpatial Backbone\nTest Set\nBLEU4\nROUGE\nResNet18\n12.28\n38.31\nDinoV2 (ViT-S/14)\n12.96\n41.12\nB.2\nPRETRAINING PERFORMANCE\nIn Table 8, we demonstrate the performance of our pseudo-gloss pretraining by measuring the pre-\ncision, recall and F1-score for Phoenix14T and CSL-Daily using a threshold of 0.2.\nTable 8: Quantitative results of pseudo-gloss pretraining on the sign language datasets.\nDataset\nPrecision\nRecall\nF1-Score\nPhoenix14T\n0.52\n0.39\n0.44\nCSL-Daily\n0.38\n0.34\n0.36\n15\nPublished as a conference paper at ICLR 2024\nImpact of pseudo-gloss selection.\nIn Table 9, we illustrate the impact of utilizing POS on preci-\nsion, recall, and F1-score. Notably, when all words are used as tokens, recall significantly decreases\nfrom 0.39 to 0.28. This result validates our assertion that not all words have corresponding signs in\nsign language.\nTable 9: Ablation of pretraining results on Phoenix14T using all words as tokens vs the selected\npseudo-glosses with a threshold of 0.2\nTokens\nPrecision\nRecall\nF1-Score\nall words\n0.55\n0.28\n0.37\npseudo-glosses\n0.52\n0.39\n0.44\nImpact of Sign Encoder.\nIn Table 10, we demonstrate the importance of the sign encoder on\npretraining. The role of the sign encoder forms an important part of the model as it learns temporal\nfeatures.\nTable 10: Ablation of pretraining results on Phoenix14T with no sign encoder and the inclusion of a\nsign encoder.\nPrecision\nRecall\nF1-Score\nno sign encoder\n0.50\n0.25\n0.33\nsign encoder\n0.52\n0.39\n0.44\nB.3\nIMPACT OF MODEL SIZE.\nIn Table 11, we demonstrate the results of our approach with the smaller language model XGLM-\n564M. The table shows a marginal performance reduction on the Phoenix14T dataset compared to\nthe larger XGLM-1.7B while still outperforming GFSLT-VLP (Zhou et al., 2023) with a similar\nsize language model, mBART (Liu et al., 2020).\nTable 11: Ablation of XGLM backbones on Phoenix14T using our Sign2GPT architecture.\nBackbone\nTest Set\nBLEU4\nROUGE\nGFSLT-VLP (mBART) (Zhou et al., 2023)\n21.44\n42.49\nSign2GPT(w/ PGP) (XGLM-564M)\n22.29\n48.21\nSign2GPT(w/ PGP) (XGLM-1.7B)\n22.52\n48.90\n16\nPublished as a conference paper at ICLR 2024\ntag\nzwölf\ngrad\nostsee\nzwanzig\ngrad\nniederbayer\n...\n...\nnun\nwettervorhersage\nmorgen\ndonnerstag\ndritter\nmärz\n...\n...\nnun\nmorgen\nwettervorhersage\ndonnerstag\nmärz\nmild\nbleiben\nregenwolke\nosten\nfünfzehn\nsechzehn\ngrad\n...\n...\nosten\nmild\nfünfzehn\nsiebzehn\ngrad\nsüdosthälfte\ndeutschland\nbleiben\nmorgen\nleicht\nhochdruckeinfluss\nnoch\nrecht\nfreundlich\n...\n...\ndeutschland\nhochdruckeinfluss\nbleiben recht\nfreundlich\nin der südosthälfte deutschlands bleibt es morgen unter leichtem hochdruckeinfluss noch recht freundlich\nmilder bleibt es unter den regenwolken im osten mit fünfzehn sechzehn grad\nund nun die wettervorhersage für morgen donnerstag den dritten märz\nam tag zwölf grad an der ostsee und bis zwanzig grad in niederbayern\ntag\nzwölf\nostsee\ngrad\ngrad\nzwanzig\nFigure 5: Visualizations of the localization capabilities of our pretraining stage on the pseudo-\nglosses from the Phoenix14T dataset. We visualize only the pseudo-glosses from the target sentence\n(y-axis) over time (x-axis), with whiter regions indicating a higher probability of the pseudo-gloss\noccurring during the time segment. We also display the localized gloss (under the video frames)\nbased on a threshold of 0.2 on E.\n17\nPublished as a conference paper at ICLR 2024\n件\n红色\n衣服\n怎么样\n这是\n新\n...\n...\n微信\n是\n谁\n发送\n...\n...\n微信\n是\n谁\n发送\n你\n小张\n时候\n认识\n...\n...\n你\n小张\n时候\n认识\n要\n去\n超市\n买\n椅子\n你\n去\n...\n...\n超市\n买\n椅子\n你\n去\n我要去超市买椅子，你去吗？\n你和小张什么时候认识的？\n微信是谁发送的？\n这件红色的衣服怎么样？这是新的。\n件\n红色\n衣服\n怎么样\n新\nFigure 6: Visualizations of the localization capabilities of our pretraining stage on the pseudo-\nglosses from the CSL-Daily dataset. We visualize only the pseudo-glosses from the target sentence\n(y-axis) over time (x-axis), with whiter regions indicating a higher probability of the pseudo-gloss\noccurring during the time segment. We also display the localized gloss (under the video frames)\nbased on a threshold of 0.2 on E.\n18\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2024-05-07",
  "updated": "2024-05-07"
}