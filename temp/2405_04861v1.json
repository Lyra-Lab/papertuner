{
  "id": "http://arxiv.org/abs/2405.04861v1",
  "title": "Insights into Deep Learning Refactoring: Bridging the Gap Between Practices and Expectations",
  "authors": [
    "SiQi Wang",
    "Xing Hu",
    "Bei Wang",
    "WenXin Yao",
    "Xin Xia",
    "XingYu Wang"
  ],
  "abstract": "With the rapid development of deep learning, the implementation of intricate\nalgorithms and substantial data processing have become standard elements of\ndeep learning projects. As a result, the code has become progressively complex\nas the software evolves, which is difficult to maintain and understand.\nExisting studies have investigated the impact of refactoring on software\nquality within traditional software. However, the insight of code refactoring\nin the context of deep learning is still unclear. This study endeavors to fill\nthis knowledge gap by empirically examining the current state of code\nrefactoring in deep learning realm, and practitioners' views on refactoring. We\nfirst manually analyzed the commit history of five popular and well-maintained\ndeep learning projects (e.g., PyTorch). We mined 4,921 refactoring practices in\nhistorical commits and measured how different types and elements of refactoring\noperations are distributed and found that refactoring operation types'\ndistribution in deep learning projects is different from it in traditional Java\nsoftware. We then surveyed 159 practitioners about their views of code\nrefactoring in deep learning projects and their expectations of current\nrefactoring tools. The result of the survey showed that refactoring research\nand the development of related tools in the field of deep learning are crucial\nfor improving project maintainability and code quality, and that current\nrefactoring tools do not adequately meet the needs of practitioners. Lastly, we\nprovided our perspective on the future advancement of refactoring tools and\noffered suggestions for developers' development practices.",
  "text": "Insights into Deep Learning Refactoring: Bridging the Gap\nBetween Practices and Expectations\nSIQI WANG, Zhejiang University, China\nXING HU∗, Zhejiang University, China\nBEI WANG, Huawei, China\nWENXIN YAO, Zhejiang University, China\nXIN XIA, Huawei, China\nXINYU WANG, Zhejiang University, China\nWith the rapid development of deep learning, the implementation of intricate algorithms and substantial\ndata processing have become standard elements of deep learning projects. As a result, the code has become\nprogressively complex as the software evolves, which is difficult to maintain and understand. Existing studies\nhave investigated the impact of refactoring on software quality within traditional software. However, the\ninsight of code refactoring in the context of deep learning is still unclear. This study endeavors to fill this\nknowledge gap by empirically examining the current state of code refactoring in deep learning realm, and\npractitioners’ views on refactoring. We first manually analyzed the commit history of five popular and\nwell-maintained deep learning projects (e.g., PyTorch). We mined 4,921 refactoring practices in historical\ncommits and measured how different types and elements of refactoring operations are distributed and found\nthat refactoring operation types’ distribution in deep learning projects is different from it in traditional\nJava software. We then surveyed 159 practitioners about their views of code refactoring in deep learning\nprojects and their expectations of current refactoring tools. The result of the survey showed that refactoring\nresearch and the development of related tools in the field of deep learning are crucial for improving project\nmaintainability and code quality, and that current refactoring tools do not adequately meet the needs of\npractitioners. Lastly, we provided our perspective on the future advancement of refactoring tools and offered\nsuggestions for developers’ development practices.\nAdditional Key Words and Phrases: Refactoring, Deep Learning, Empirical Software Engineering\nACM Reference Format:\nSiqi Wang, Xing Hu, Bei Wang, Wenxin Yao, Xin Xia, and Xinyu Wang. 2024. Insights into Deep Learning\nRefactoring: Bridging the Gap Between Practices and Expectations. 1, 1 (May 2024), 24 pages. https://doi.org/\n10.1145/nnnnnnn.nnnnnnn\n∗Corresponding Author\nAuthors’ addresses: Siqi Wang, Zhejiang University, Hangzhou, Zhejiang, China, wangxinyu@zju.edu.cn; Xing Hu, Zhejiang\nUniversity, Hangzhou, Zhejiang, China, xinghu@zju.edu.cn; Bei Wang, Huawei, Hangzhou, Zhejiang, China, wangbei46@\nhuawei.com; Wenxin Yao, Zhejiang University, Hangzhou, Zhejiang, China, wenxinyao2002@gmail.com; Xin Xia, Huawei,\nHangzhou, Zhejiang, China, xin.xia@acm.org; Xinyu Wang, Zhejiang University, Hangzhou, Zhejiang, China, wangxinyu@\nzju.edu.cn.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee\nprovided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and\nthe full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.\nAbstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires\nprior specific permission and/or a fee. Request permissions from permissions@acm.org.\n© 2024 Association for Computing Machinery.\nXXXX-XXXX/2024/5-ART $15.00\nhttps://doi.org/10.1145/nnnnnnn.nnnnnnn\n, Vol. 1, No. 1, Article . Publication date: May 2024.\narXiv:2405.04861v1  [cs.SE]  8 May 2024\n2\nSiqi Wang, Xing Hu, Bei Wang, Wenxin Yao, Xin Xia, and Xinyu Wang\n1\nINTRODUCTION\nAs deep learning continues to evolve rapidly, deep learning projects continue to be rapidly updated\nto optimize model construction, improve computing, and increase algorithm performance [18, 43].\nHowever, if maintenance activities are not conducted properly, they can lead to a decrease in\nquality. The complexity of deep learning models and their high dependence on data, as well as\nconstantly updated algorithms and techniques, present unique challenges for their maintenance.\nThese unique challenges make the development and maintenance of deep learning projects distinct\nfrom traditional software [11].\nThere have been significant studies [7, 10, 16, 20, 25, 27] demonstrating the benefits of refactoring\nfor software maintenance, reuse, and code enhancement, focused on traditional software. However,\nfew studies have investigated code refactoring in deep learning projects, and there is also a lack\nof research into deep learning practitioners’ views on refactoring and related tools. Investigating\ncode refactoring in deep learning projects and uncovering the reasons behind such practices can\nhelp optimize the development process, improve team productivity, and enhance code quality.\nTherefore, we analyzed the state of code refactoring in deep learning repositories and investigated\nthe perceptions of deep learning practitioners on code refactoring.\nOur study aims to answer the following research questions:\nRQ1: How does code refactoring behave within deep learning projects?\nThis RQ studies code refactoring practices in deep learning projects and uncovers how it\ndiffers from code refactoring in traditional Java software. Understanding code refactoring\npractices in deep learning projects is crucial to enhance maintainability. We found that the\nmost common refactoring operation types in deep learning projects are Remove Dead Code\nand Rename. However, the distribution of usage for refactoring operations in Java differs from\nthat in deep learning projects. Additionally, method-level refactorings are frequently used in\ndeep learning projects, and variable-level refactorings are often introduced as they are easily\nmodifiable elements. These insights not only highlight the unique challenges and implications\nwithin the deep learning domain but also lay the groundwork for tailored software engineering\nthat is crucial to the rapidly evolving field of deep learning. We further surveyed developer\nperspectives on refactoring in deep learning at RQ2. Furthermore, the manual examination\nof commit messages unveiled indications of developers employing automation tools for\nrefactoring tasks. Based on this finding, we further surveyed the compatibility of existing\ntools with the unique needs of deep learning practitioners in RQ3.\nRQ2: What are the perspectives of deep learning practitioners regarding code refactoring?\nBuilding upon insights gained from RQ1, this RQ investigates into practitioners’ perspectives\non code refactoring, including their opinions on specific refactoring operations and elements\nwithin deep learning. Practitioners in deep learning projects prioritize Remove Dead Code\nand API Refactoring, while the importance of Pull Up and Push Down, which are widely\nstudied in traditional projects is not well recognized. Method and Class are highly regarded\nin deep learning refactoring. Examining practitioners’ perspectives on deep learning code\nrefactoring can validate and quantify the observations from RQ1, and provide a critical\nunderstanding of their preferences, challenges, and potential unmet needs. These insights\nhelp bridge the gap between the reasearch and practice in current AI development, thereby\naiding the development of targeted refactoring techniques for this domain.\nRQ3: How well do current refactoring tools meet practitioner needs?\nThis RQ investigates practitioners’ opinions on the effectiveness of existing refactoring tools.\nThe findings from RQ1 revealed evidence of refactoring tool utilization within commit mes-\nsages, which led us to assess the suitability of existing tools for deep learning requirements.\n, Vol. 1, No. 1, Article . Publication date: May 2024.\nInsights into Deep Learning Refactoring: Bridging the Gap Between Practices and Expectations\n3\nCommit Patch\nCommit Message\nManual Detection\nData Analysis\nRefactoring Operation\nCommit Intent\nStage1: Refactoring   Detection\nStage2: Online Survey\nRQ1: Code refactoring\nstatus in DL repositories:\nRQ2: Practitioners’ perspectives\non code refactoring:\nRQ3: Practitioners’ expectations\ntowards refactoring tools:\nOnline Survey\nQuestionnaire\nData Analysis\nFig. 1. Research methodology overview\nPractitioners suggested combining these tools with Large Language Models to achieve more\neffective and context-aware results. The suggestions for improvement from deep learning\npractitioners emphasized user-friendliness, project-specific customization, and comprehen-\nsive testing integration. These insights aim to inform the development of future tools tailored\nto the distinct requirements of deep learning practitioners in the deep learning domain.\nThe intention behind our investigation is to facilitate consideration by researchers regarding the\nrequirements of practitioners, thereby continuing the advancement of code refactoring for deep\nlearning projects. Furthermore, we aim to provide new insights that can promote the development\nof better refactoring tools for deep learning projects. This paper makes the following contributions:\n(1) We manually analyzed five deep learning projects’ commits and detected 53 types of 4,921\nrefactoring operations. We further analyzed the distribution of different refactoring operation\ntypes and elements in deep learning projects.\n(2) We surveyed 159 deep learning practitioners from 38 countries to shed light on practitioners’\nviews on refactoring and their expectations of refactoring tools. To the best of our knowledge,\nwe are the first to perform empirical study of refactoring practices in deep learning projects.\nPaper Organization: Section 2 describes the methodology of our research. Section 3 shows the\nresults of our study. We discuss the implications and threats of our results in Section 4. Section 5\ndiscusses related work. Section 6 draws conclusions and outlines avenues for future work.\n2\nRESEARCH METHODOLOGY\nIn this section, we present the design of our empirical study. Our main goal in this study is to\ncomprehensively understand code refactoring practices within deep learning projects, investigate\npractitioners’ perceptions of code refactoring, and assess the alignment of current refactoring tools\nwith the specific needs of deep learning practitioners. Our study aims to provide crucial insights for\nsoftware engineering practices in the realm of deep learning and drive advancements in pertinent\ntechnologies. The overview of the methodology in our study is shown in Figure 1 and consists\nof two stages. Stage 1: Manual mining of refactoring operations from repository history commit\nmessages manually. Stage 2: An online survey for confirming and extending the conclusions about\nthe current stage of refactoring in Open Source deep learning libraries.\n, Vol. 1, No. 1, Article . Publication date: May 2024.\n4\nSiqi Wang, Xing Hu, Bei Wang, Wenxin Yao, Xin Xia, and Xinyu Wang\nTable 1. Statistics of deep learning projects used in our study.\nProject\nTime Range\n#Commits\n#Star\nKeras\n2015/03-2023/09\n8,342\n60.2k\nScikit-learn\n2010/01-2023/09\n30,375\n57k\nPytorch\n2012/02-2023/09\n63,722\n74.3k\nTransformers\n2018/10-2023/09\n13,900\n118k\nTensorflow\n2015/11-2023/09\n50,863\n180k\nTotal\n167,202\n✓I fixing name position_embeddings to object_queries 1(#24652) \n* fixing name position_embeddings \nto object_queries \n* [fix] renaming variable \nand docstring \ndo object queries \n* [fix] comment position_embedding to object queries \n* [feat] \nchanges from make-fix-copies \nto keep consistency \n* Revert \n11 [feat] \nchanges from make-fix-copies \nto keep consistency\n11 \nThis reverts \ncommit 56e3e9e. \n* [tests] \nfix wrong expected score \n* [fix] wrong assignment causing wrong tensor shapes \nFig. 2. Commit 99c3d44906ec448c4559fecdc9a63eda364db4d4\n2.1\nStage 1: Refactoring Manual Detection\nSince most of the deep learning projects use Python as the main programming language [23, 33, 35],\nwe only analyzed refactoring commits that involve Python. To answer RQ1, we selected five open\nsource deep learning projects that are widely used and well-maintained: Keras [1], Scikit-Learn [3],\nPyTorch [2], TensorFlow [4], and Transformer [5]. These frameworks were chosen based on their\nhigh stars and forks in GitHub. Table 1 shows statistics of deep learning projects we used in RQ1.\nTo the best of our knowledge, there is no refactoring detection tool that can detect all common\nrefactoring operations in Python. Additionally, some commits might contain tangled changes,\nmaking it more difficult to isolate the changes related to refactorings. Therefore, we follow previous\nresearch to mine refactoring operations from the commit [6, 13, 24, 28, 38].\n2.1.1\nFilter. We first crawled all commits of these five deep learning projects using the GitHub\nAPI, totaling 167,202 commits up until September 2023. However, due to the enormous amount of\nmanual detection, we used a keyword-based filter to reduce the amount of work involved in manual\ndetection. Developers commonly use a range of textual patterns to document their refactoring\nactivities, including ‘extract’, ‘reorganize’, and ‘redesign’, in addition to ‘refactor’ [24]. In this part,\nfollowing Self-Affirmed Refactoring [6], we created a keyword-based commit message filter to\npre-filter potential refactoring commits. In particular, we only focused on refactoring commits\nin Python since the majority of the deep learning project is written in Python [33]. After the\napplication of the keyword-based filter, we filtered 28,803 commits for manual detection.\n, Vol. 1, No. 1, Article . Publication date: May 2024.\nInsights into Deep Learning Refactoring: Bridging the Gap Between Practices and Expectations\n5\ndef m1(self)\nRename\ndef N()\nstatement m\nstatement 1\nstatement 2\nstatement 3\ndef m()\nExtract\ndef m2(self)\nclass Foo:\n      def m(self)\nMove\nclass Bar:\n      def m(self)\ndef M()\nRename method m1 to m2\nMove method m from class Foo to class Bar\nInline method M into other methods\nInline\nExtract statements to a new method m\nFig. 3. Refactoring operations abstract example diagrams in method level\n2.1.2\nManual Detection. After filtering all refactoring commit messages detected by keywords,\nWe manually analyzed each commit message. This process entailed two authors analyzing the data\nof each code change and identifying the intent of commits. We examined each commit to identify\nits purpose because some commits contain refactoring-like code changes yet their actual intent\nis to debug or update. Commit 99c3d44906ec448c4559fecdc9a63eda364db4d4 in Figure 2 fixed the\nincorrect variable name, but may be mistaken for a Rename refactor operation. We employed a\ntwo-phase process:\n(1) We first divided all the filtered commits into two bins, each evaluator independently analyzed\nand labeled a piece of the commit as “1” if the commit was a refactoring commit, and “0” otherwise.\nThe evaluator further analyzed the code changes of the refactoring-related commit to determine\nwhat refactoring operation was performed by that commit, we followed Fowler’s description [16].\nThe evaluators ignored the tangled changes in the manual detection phase and only abstract\nrefactoring related changes. Since refactoring practices in real-world development may differ from\nthe description given in Fowler’s book, after each evaluator had marked 100 commits, they had a\ndiscussion to re-establish consistent refactoring classification standards. The evaluators repeated\nthis action in all five studied projects and corrected each of the previously marked commits according\nto the final standards. The final standards we used differed from Fowler’s in code elements. We\nfollowed a similar information modeling approach as the one used by REFACTORINGMINER [39],\nincluding the following elements:\n• Module: A module is defined as a Python file, containing classes and methods, and directly\naffiliated definitions and statements (i.e., those not defined within a method).\n• Class: A class may contain definitions, methods, and statements.\n• Method: A method contains a list of parameters (if any), and the statements it carries.\n• Statements: A statement is the smallest unit of execution in Python and is usually terminated\nby a line break or a semicolon.\n• Variable: A variable is a container for storing data and is the smallest nameable storage\nelement in python program.\nWe summarized 7 most common refactoring operation types according to Fowler’s book, Figure\n3 shows the abstract example diagrams of Rename, Move, Extract, and Inline.\n• Remove Dead Code: remove invalid or redundant code fragments that are no longer being\nexecuted by the program. This type of refactoring is accompanied by “clean” and “remove\nunused code” in the commit message.\n• Rename: change the name of elements for code clarity and readability. This type of refactor-\ning consists of Rename Module, Rename Class, Rename Method, Rename Variable.\n, Vol. 1, No. 1, Article . Publication date: May 2024.\n6\nSiqi Wang, Xing Hu, Bei Wang, Wenxin Yao, Xin Xia, and Xinyu Wang\n• Move: move code elements from one location to another. This type of refactoring consists of\nMove Module, Move Class, Move Method, Move Statement, and Move Variable.\n• Extract: extract independent and reusable elements from larger and more complex code\nelements. This type of refactoring consists of Extract Module, Extract Class, Extract Method,\nand Extract Variable.\n• Inline: inline a part of code directly into its place of use. This type of refactoring consists of\nInline Module,Inline Class, Inline Method, and Inline Variable.\n• API Refactoring: refactor the application programming interfaces in code. This type of\nrefactoring usually involves parameter changes, return value changes, etc.\n• Consolidate or Decompose Conditional Expression: refactor expressions to make them\nmore concise, readable, and logical by consolidating or decomposing them.\nThere are also other unlisted refactoring operation types that rarely appear in deep learning\nproject refactoring practices, and we identify them by following Fowler’s book [16], e.g. Pull Up\nand Push Down. After the independent evaluation, the two evaluators checked each other’s commit\ndata that was labeled as refactoring-related. The evaluators disagreed on 439 instances out of a\ntotal of 4,921 refactoring instances. These instances were carefully discussed to reach a consensus\nuntil there was no disagreement on any code change after the second stage of refactoring operation\nclassification. The inter-rater agreement was further quantified using Cohen’s Kappa coefficient,\nyielding a value of 0.81, indicating substantial agreement between the evaluators in the classification\nof refactoring operations.\nThe labeled dataset is available in our replication package1. Additionally, we also observed the\nusage of “Pylint”, “Flake”, “...Generated by Copilot..” in the commit message. This indicates that\ndevelopers use refactoring tools during the refactoring process, which motivated us to investigate\npractitioners’ perceptions of current refactoring tools in Stage 2.\n2.2\nStage 2: Online Survey\nTo further investigate deep learning code refactoring, we conducted an anonymous online survey\nwith deep learning participants. The survey aimed to validate and quantify the observations from\nStage 1, and to shed light on practitioners’ views on refactoring and their expectations of refactoring\ntools.\n2.2.1\nDesign. Combining the results of the first step and previous work [13, 14], we concluded\nnine refactoring operation types, five refactoring operation elements, and three types of refactoring\ntools to launch an online survey for deep learning practitioners. The online survey aims to provide\ninsights into Open Source code refactoring in deep learning projects and practitioners’ expectations\nof current refactoring tools. The refactoring operation types we employed consist of the seven most\ncommon refactoring operation types in deep learning projects from Stage 1 (Remove Dead Code,\nRename, Move, Extract, Inline, API Refactoring, Consolidate or Decompose Conditional Expression),\nand two refactoring operation types that are widely studied in traditional java refactoring (Pull\nUp and Push Down). The elements consist of Variable, Statement, Method, Class, and Module. The\nrefactoring tools consist of code smell detection tools and automatic refactoring tools.2\nThe survey included different types of questions, e.g., multiple-choice questions, short answer\nquestions, and rating questions (5-point Likert scale: Strongly Disagree to Strongly Agree). We\nincluded the category “I don’t know” to filter respondents who do not understand our brief\ndescriptions. The survey consists of three sections:\n1https://anonymous.4open.science/r/DLRF-E0C7\n2https://forms.gle/7mFJcHXbRFGGHY9r9\n, Vol. 1, No. 1, Article . Publication date: May 2024.\nInsights into Deep Learning Refactoring: Bridging the Gap Between Practices and Expectations\n7\nTable 2. Description of Code Refactoring Tools\nRefactoring Tool\nDescription\nCode Smell Detection\nTools\nCode smells are signs that your code is not as clean and maintainable\nas it could be. They can derive from the misuse of syntax and almost\nalways suggest code needs to be refactored or redesigned to improve\nthe overall quality of the program. Code smell detection tools can help\ndevelopers find where to refactor to improve the quality of their code.\nAutomatic Refactor-\ning Tools\nAutomatic refactoring tools identify problems in the code and eliminate\nthem through refactoring. These tools reduce the effort of developers\nas they have very little to do during the code refactoring process.\n• Demographics: The survey first asked for demographic information about the participants,\nincluding country/area of residence, current occupation, experience in years, and primary\nprogramming language.\n• Thoughts on Refactoring: This section focuses on providing insights into the current state\nof Open Source code refactoring in deep learning projects. We started by showing clear and\nconcise example diagrams of some of the classic refactoring operations in method level. We\nthen invited developers to rate the nine refactoring operations and five refactoring elements\nin terms of importance and frequency. This section highlights practitioners’ opinions towards\nrefactoring in the deep learning software development process.\n• Tool Utilization: The purpose of this section is to gather information about developers’\nusage of refactoring tools and investigate practitioners’ expectations of these tools. We first\nprovided respondents with a brief description of code refactoring tools shown in Table 2,\nconsisting of (1) code smell detection tool [19], and (2) automatic refactoring tools [17].\nThen we asked practitioners Have you used or are you familiar with such tools? and extended\nan invitation to respondents to share their observations regarding such tools. In addition,\npractitioners were invited to provide advice on improving refactoring tools through an\nopen-ended question.\nAt the end of the survey, we allowed respondents to provide free-text comments, suggestions,\nand opinions about code refactoring and our survey. A respondent may or may not provide any\nfinal comments.\nDuring the initial phase of our research, we conducted a preliminary survey with a small group\nof professionals who differed from our survey respondents. The purpose is to gather feedback on\ntwo key aspects: (1) the length of the questionnaire and (2) the clarity of the terms used. Based on\nthe feedback received, we made minor modifications to the survey and finalized the questionnaire.\nIt should be noted that the responses collected during the pilot survey were not included in the\nresults presented in our research paper.\n2.2.2\nParticipant Recruitment. We selected Github repositories with the top 100 popular open-\nsource deep learning projects (based on their number of stars) and mined these repositories to\nextract their contributors’ public email addresses. We finally mined 3,125 contributors’ email\naddresses and sent a link to our survey. We aimed to recruit open-source deep learning practitioners\nwho have software development experience in addition to professionals working in the industry.\nOut of these emails, four practitioners replied with blank responses; six practitioners replied that\nthey would not answer any survey. Finally, we received 159 valid responses. The 159 respondents\n, Vol. 1, No. 1, Article . Publication date: May 2024.\n8\nSiqi Wang, Xing Hu, Bei Wang, Wenxin Yao, Xin Xia, and Xinyu Wang\nTable 3. Participants Roles & Programming Experience\n0-1 y\n2-3 y\n4-5 y\n6-9 y\n>10 y\ntotal\nAlgorithm\n2\n25\n24\n14\n6\n71\nDevelopment\n5\n14\n10\n15\n6\n50\nArchitect\n0\n1\n2\n3\n5\n10\nProject Manager\n1\n1\n0\n1\n1\n4\nTesting\n0\n1\n0\n1\n1\n3\nOthers\n2\n7\n2\n2\n1\n14\ntotal\n11\n49\n38\n36\n20\n152\nresided in 38 countries across six continents. The top two countries where the respondents came\nfrom were India and the United States.\nAn overview of the surveyed participants and their experience is depicted in Table 3. Most\nparticipants are engaged in Algorithm and have 2-3 years of professional experience.\n2.2.3\nData Analysis. We analyzed the survey results based on the question types.\nTo understand trends in the Likert-scale questions, we reported the percentage of each option\nselected. We dropped “I don’t know” ratings and created bar charts (many of which are shown in\nthe remainder of this paper).\nTo obtain insights from responses to open-ended questions, we use open coding to analyze the\nsurvey results qualitatively by inspecting responses. The first author analyzed the interviews by\ntranscribing them and then performed open coding to generate codes of the questionnaire contents\nusing NVivo [32] qualitative analysis software. Then, the second author verified the initial codes\ncreated by the first author and provided suggestions for improvement.\n3\nRESULTS\nIn this section, we present the results of research questions that investigate code refactoring from\ncommits and practitioners.\n3.1\nRQ1: Refactoring Practices in Deep Learning Projects\nIn RQ1, we explored code refactoring practices in deep learning projects, including practitioners’\npractices on refactoring during development, and the distribution of different refactoring operation\ntypes and elements’ usage in their projects. We uncovered how refactoring operation types’ distri-\nbution in deep learning projects differs from code refactoring in traditional Java software [8]. In\naddition, we analyzed the distribution of different code elements in refactoring commits for deep\nlearning projects.\n3.1.1\nRefactoring Operations. We detected 53 refactoring operations of 4,921 practices by manual\ndetection, the complete distribution of all refactoring operations can be found in the replication\npackage. To enhance comprehension and organization of these refactorings, we categorized com-\nmon refactoring operations into seven operation types, each comprising various specific code\nrefactoring operations shown in Figure 4. Table 4 shows the result of our manual detection, includ-\ning the refactoring practice number, description, and example of each operation type. In previous\nresearch [8] of traditional Java software code refactoring, some refactoring operation types in\nthe given table were habitually overlooked, specifically, Remove Dead Code and Consolidate or\nDecompose Conditional Expression.\n, Vol. 1, No. 1, Article . Publication date: May 2024.\nInsights into Deep Learning Refactoring: Bridging the Gap Between Practices and Expectations\n9\nAccording to the table, Remove Dead Code is the most frequent refactoring operation type. This\nphenomenon may be attributed to the rapid iteration characteristic of deep learning, resulting in\ncode that is updated swiftly and not promptly cleaned up by developers as the commit message\ncontains “remove the old code...”. Besides, deep learning projects usually include experiments, such\nas testing new models, and algorithms or tuning hyper-parameters [11] as “remove test of model...”.\nThis leads to a large number of code snippets in the source, that ultimately become dead code when\nthe outputs of the experiment are determined and subsequently thrown away. Nevertheless, as\nrequirements can change over time or the originally intended functionality not be implemented, a\nsignificant amount of dead code is left behind. Remove Dead Code is an important step in maintaining\na clear code structure and improving the maintainability of projects [34].\nThe frequent occurrence of Consolidate or Decompose Conditional Expression operations in deep\nlearning projects can be attributed to the complexity of model structures, frequent adjustments and\noptimizations, and the impact of technical debt within the project. Complex models and long-term\ndevelopment can result in a build-up of technical debt [26], some of which could include confusing\nconditional logic. To enhance the maintainability and simplicity of deep learning projects, teams\nmight engage in frequent refactoring of conditional expressions.\nTo investigate the differences in code refactoring between deep learning projects and traditional\nJava projects [8], we chose seven refactoring operation types to compare the differences in usage\nwithin deep learning projects and traditional Java projects. Five refactoring operation types (Rename,\nMove, Extract, Inline, API Refactoring) are well studied in previous work and commonly used in\ndeep learning projects. The other two types (Pull Up and Push Down) are not common in deep\nlearning projects but are well studied in traditional Java projects.\nWe extracted the commits of the above seven refactoring operations and compared their distri-\nbution on deep learning projects with traditional Java projects. The results are shown in Figure\n5. Rename is significantly higher in deep learning projects than in Java projects, especially for\nRename Variable. The frequency of adjustments, renaming of model layers, and naming of variables\nin deep learning projects are the main reasons for this phenomenon. Move is also commonly used\nRemove Dead CodeA \nExtract Module \nExtract Class \nExtract Method \nExtract Variable \nlnline Module \nlnline Class \nlnline Method \nlnline Variable \nExtract \nlnline \nAPI Refactoring \nConsolidate or Decompose \nConditional ExpressionA \nRefacotring \nOperation \nRename \nMove \nOthers \nRename Module \nRename Class \nRename Method \nRename Variable \nMove Module \nMove Class \nMove Method \nMove Statement \nMove Variable \nPull Up* \nPush Down* \n•••••• \nFig. 4. Code refactoring operation types in deep learning projects. Δ: Operation commonly used in deep\nlearning projects while overlooked in previous work; ∗: Operation which is uncommon in deep learning\nprojects while being widely studied in previous work.\n, Vol. 1, No. 1, Article . Publication date: May 2024.\n10\nSiqi Wang, Xing Hu, Bei Wang, Wenxin Yao, Xin Xia, and Xinyu Wang\nin the refactoring of deep learning projects, although it is less frequent than those in Java projects.\nAs Java projects are usually developed by a large team and require a stricter code structure and\nspecification, Move is more frequently adopted to ensure code tidiness and consistency [16]. Deep\nlearning projects always prioritize rapid iteration and experimentation, featuring complex model\nstructures and diverse code structures. Refactoring practices in these teams may allow for greater\nfreedom, with Move being used less frequently in deep learning projects compared to Java. Extract,\nAPI Refactoring and Inline are more prevalent in deep learning projects than in traditional Java\nTable 4. Code Refactoring Operations in Deep Learning Projects\nRefactoring Operation\n#Number\nDescription\nRemove Dead Code\n1,730 (35.16%)\nRemove invalid or redundant code fragments that are\nno longer being executed by the program.\n1 \n- def _getitem_(self, \nval: int) \n-> Expr: \n2 \nreturn self.shape_env.duck_int(val} \n3 \n4 \n5 \ndef size_hint(self, \nexpr: Expr) -> int: \nif not isinstance(expr, \nExpr}: \nassert \nisinstance(expr, \nint) \n1 \n2 \n3 \ndef size_hint(self, \nexpr: Expr) -> int: \nif not isinstance(expr, \nExpr): \nassert \nisinstance(expr, \nint) \nRename\n1,170 (23.78%)\nChange elements’ name for code clarity and readability.\n1 \n- def rescale(self, \n2 \nimage: np.ndarray, \n3 \nscale: \nUnion[float, \nint] \n4 \n) -> np.ndarray: \n5 \nself._ensure_format_supported(image) \n6 \nreturn image* seal \n7 \ndef to_numpy_array(self, \nimage, \n8 \nrescale=None, \n9 \nchannel_first=True}: \n10 \n11 \n12 \n13 \n14 \n15 \nif rescale: \nimage= self.rescale{ \nimage.astype{np.float32), \n1 / 255.0) \n1 \n+ def rescale_image(self, \n2 \nimage: np.ndarray, \n3 \nscale: \nUnion[float, \nint] \n4 \n} -> np.ndarray: \n5 \nself._ensure_format_supported(image) \n6 \nreturn image* seal \n7 \ndef to_numpy_array(self, \nimage, \n8 \nrescale=None, \n9 \nchannel_first=True): \n10 \n11 \n12 \n+ \n13 \n14 \n15 \nif rescale: \nimage= self.rescale_image( \nimage.astype(np.float32), \n1 / 255.0) \nMove\n729 (14.81%)\nMove code elements from one location to another.\n1 \n- --torch/distributed/fsdp/fully_sharded_data_parallel.py\n2 \n3 \ndef _get_shard_functional( \n4 \n5 \n6 \n7 \n8 \n9 \n10 \n11 \n12 \n13 \n14 \n15 \ntensor: torch.Tensor, \nrank: int, \nworld_size: int, \n-> Tuple[torch.Tensor, int]: \nchunk, pad_num = FullyShardedDataParallel._get_chunk( \ntensor, rank, world_size, \nshard= chunk.clone() \nif pad_num > 0:\nshard= F.pad(shard, [0, pad_num]) \nreturn shard, pad_num \n1 \n+ --torch/distributed/fsdp/flat_param.py\n2 \n3 \ndef _get_shard_functional( \n4 \ntensor: torch.Tensor, \n5 \nrank: int, \n6 \nworld_size: int, \n7\n-> Tuple[torch.Tensor, int]: \n8 \n+\nchunk, pad_num = FlatParamHandle._get_unpadded_shard(\n9 \n10 \n11 \n12 \n13 \n14 \n15 \ntensor, rank, world_size, \nshard= chunk.clone() \nif pad_num > 0:\nshard= F.pad(shard, [0, pad_num]) \nreturn shard, pad_num \n... \nExtract\n499 (10.14%)\nExtract independent and reusable elements from larger\nand more complex code elements.\n1 \n2 \n3 \n4 \n5 \n6 \ndef make_buffer_allocation(self, \nbuffer}: \nname= buffer.get_name(} \ndevice= \nself.codegen_device(buffer.get_device(}} \ndtype = self.codegen_dtype{buffer.get_dtype{)) \nsize= \nself.codegen_shape_tuple{tuple{buffer.get_size(}}) \nstride= \nself.codegen_shape_tuple(tuple(buffer.get_stride()}} \n1 \ndef make_buffer_allocation(self, \nbuffer}: \n2 \n+ \nreturn self. make_a llocat ion ( \n3 \n+ \nbuffer. get_name (), \n4 \n+ \nbuffer. get_dev ice (}, \n5 \n+ \nbuffer. get_dtype {) , \n6 \n+ \nbuffer.get_size(), \n7 \n+ \n8 \n+ \nbuffer.get_stride(}, \nself.can_cache_buffer_in_thread \nlocal(buffer}} \n9 \n+ def make_a l location ( \n10 \n+ \n11 \n+ \n12 \n+ \n13 \n+ \n14 \n+ \n15 \n+ \nself, \nname, device, \ndtype, shape, stride, \ncan_cache_buffer_in_thread_local=False): \ndevice= \nself.codegen_device(device} \ndtype = self.codegen_dtype(dtype} \nsize= \nself.codegen_shape_tuple(shape} \nstride= \nself.codegen_shape_tuple(stride) \nContinue in next page.\n, Vol. 1, No. 1, Article . Publication date: May 2024.\nInsights into Deep Learning Refactoring: Bridging the Gap Between Practices and Expectations\n11\nRefactoring Operation\n#Number\nDescription\nAPI Refactoring\n287 (5.83%)\nRefactor the application programming interfaces.\n1 \ndef _get_initial_value(self, \n1 \ndef _get_initial_value(self, \n2 \nreplica_id=0, \ndevice=None, \n2 \n+ \nreplica_id, \ndevice, \n3 \nprimary_var=None, **kwa rgs): \n3 \n+ \nprima ry_va r, **kwa rgs): \n4 \nif replica_id \n== 0: \n4 \nif replica_id \n== 0: \n5 \nassert \ndevice is not None \n5 \nassert \ndevice is not None \nConsolidate or Decompose Conditional\nExpression\n228\nRefactor expressions to make them more concise, read-\nable, and logical by consolidating or decomposing them.\n1 \ndef reverse(x, \naxes): \n1 \ndef reverse(x, \naxes): \n2 \nif isinstance(axes, \nint): \n2 \n+ \nif isinstance(axes, \nlist): \n3 \naxes = [axes] \n3 \n+ \naxes = tuple(axes) \n4 \nfor a in axes: \n4 \n+ \nreturn \nnp.flip(x, \naxes) \n5 \nx = np.flip(x, \na) \n6 \nreturn \nx \nInline\n157 (3.19%)\nInline a part of code directly into its place of use.\n1 \n2 \n3 \n4 \n5 \n6 \n7 \n8 \n9 \n10 \n11 \n12 \n13 \ndef _iterate_columns(X, \ncolumns=None): \nfor i in columns: \nyield _get_column(X, i) \n- def _get_column(X, i): \nif issparse(X): \nelse: \nx = np.zeros(X.shape[0]) \ns_p, e_p = X.indptr[i], \nX.indptr[i \n+ 1] \nx[X.indices[s_p:e_p]] \n= X.data[s_p:e_p] \nX = X [: , i] \nreturn x \n1 \n2 \n3 \n4 \n5 \n6 \n7 \n8 \n9 \n10 \n11 \n12 \n+ \n+ \n+ \n+ \n+ \ndef _iterate_columns(X, \ncolumns=None): \nif issparse(X): \nfor i in columns: \nx = np.zeros(X.shape[0]) \ns_p, e_p = X.indptr[i], \nX.indptr[i \n+ 1] \nx[X.indices[s_p:e_p]] \n= X.data[s_p:e_p] \nyield x \nelse: \nfor i in columns: \nyield X [: , i] \nOthers\n121 (2.46%)\nOther refactoring operations rarely appear in deep\nlearning project refactoring practices, e.g. Pull Up, Push\nDown.\n1\n2\n3\n4\n5\n1: Inline Module\n2: Inline Variable\n3: Inline Method\nPull Up & Push Down\n4: Extract Class\n5: Move Variable\n(a) Deep learning projects\n1: Push Down Variable\n2: Push Down Method\n3: Push Down\n1\n2\n4\n5\n3\nInline\n6\n4: Pull Up Variable\n5: Extract Variable\n6: Extract Class\n(b) Traditional Java projects\nFig. 5. Distribution of refactoring operations for deep learning projects and traditional Java projects\n, Vol. 1, No. 1, Article . Publication date: May 2024.\n12\nSiqi Wang, Xing Hu, Bei Wang, Wenxin Yao, Xin Xia, and Xinyu Wang\nprojects. Model structures and algorithms in deep learning projects can often be complicated,\nrequiring more abstraction and optimization. Consequently, Extract is frequently employed to\nabstract complex methods or modules, thereby enhancing the modularity and reusability of code.\nDeep learning projects always need to refine and enhance their interfaces regularly to accommodate\nevolving needs, resulting in frequent usage of API Refactoring. Inline often occurs in pursuit of\nhigher performance, to reduce the overhead of calls, or to revert some inappropriate extraction\noperation [16]. Pull Up and Push Down operations are infrequent in both types of projects but\nare particularly uncommon in deep learning. Deep learning projects tend to concentrate on de-\nsigning model hierarchies and structures, with less emphasis on class inheritance and optimizing\nhierarchies.\nFinding 1: Remove Dead Code and Rename are two of the most frequent\noperation types in deep learning repositories. Rename, Move, Extract, Inline,\nAPI Refactoring, Pull Up and Push Down, which are widely studied in Java,\nshow a different distribution of usage in deep learning projects.\n3.1.2\nRefactoring elements. We counted the elements of all refactoring commits excluding Remove\nDead Code (as it is usually cluttered and only involves deletion), including Variable, Statement,\nMethod, Class, and Module. The elements’ frequency of occurrences on refactoring operations in\nthe deep learning repository is displayed in Figure 6. This distribution of refactoring operations’\nelements can offer insights into code optimization and project maintenance in deep learning\nprojects.\nInline Module\nMove Variable\nPull Up Method\nFig. 6. Distribution of refactoring elements for deep learning project\nMethod (37.31%), as the element of each functional unit of the deep learning model, accounts for\nthe largest proportion of refactoring operations. During our analysis of method-level commits, we\nfound that the most frequently utilized refactoring operations are Rename Method and Move Method.\nThese types of refactoring operations are relatively simple and contribute to both code readability\nand modularity. It is essential to consider these factors when undertaking code maintenance tasks.\nVariable (26.52%) has the second-highest percentage of refactoring operations. The most frequent\nof these is Rename Variable, which is typically linked to the “same variable name” in the commit\n, Vol. 1, No. 1, Article . Publication date: May 2024.\nInsights into Deep Learning Refactoring: Bridging the Gap Between Practices and Expectations\n13\nmessage to prevent “naming conflicts”. Class (12.80%) and Module (12.55%) share a similar percentage\nof refactoring operation elements, and are both related to the alignment of model components\nand structures. Rename and Move are the most prevalent actions in these two levels of operation.\nRename helps ensure that the code structure is clear and understandable, while Move enhances the\nmodularity of the code, making the entire model simpler to manage and maintain. In the context of\ndeep learning projects, these actions are critical for model comprehension and development. The\nproportion of Statement (10.82%) level operations is comparatively low. A notable component of this\npercentage is the Consolidate or Decompose Conditional Expression operation, probably because the\ncomplex conditional logic and computation in the deep learning model require frequent refactoring\nof conditional expressions to improve readability and maintainability.\nFinding 2: Method, being a more moderate element, is frequently employed in\nrefactoring operations, while Variable, being easier to modify, is also frequently\nused in refactoring. The frequency at which refactoring operations take place\nfor Class and Module are approximately equal. The proportion of statement-\nlevel operations is comparatively low.\n3.2\nRQ2: Practitioners’ Opinion on Refactoring\nIn RQ2, deep learning practitioners were surveyed and asked to evaluate the significance of\noperations or elements during the refactoring process. Of the 159 practitioners surveyed, 145\n(91.2%) “Strongly Agreed” or “Agreed” that “refactoring is an important part of software development\nin deep learning projects”, 11 chose “Neutral”, while the remaining three practitioners “Strongly\nDisagree” with the importance of refactoring.\n0%\n10%\n20%\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\nPercentage of Valid Responses\nRename\nMove\nExtract\nInline\nPull UP\nPush Down\nAPI Refactoring\nConsolidate or Decompose\nConditional Expression\nRemove Dead Code\nStrongly Disagree\nDisagree\nNeutral\nAgree\nStrong Agree\n(a) Importance rate of refactoring operations\n0%\n10%\n20%\n30%\n40%\n10%\n20%\n30%\n40%\n50%\n60%\n70%\nPercentage of Valid Responses\nRename\nMove\nExtract\nInline\nPull UP\nPush Down\nAPI Refactoring\nConsolidate or Decompose\nConditional Expression\nRemove Dead Code\nStrongly Disagree\nDisagree\nNeutral\nAgree\nStrong Agree\n(b) Frequency rate of refactoring operations\nFig. 7. Respondents’ rate of refactoring operation types’ importance and frequency\n3.2.1\nRefactoring Operations. We further investigated practitioners’ opinions on several common\nrefactoring operation types. Figure 7 illustrates respondents’ rating of refactoring operation types’\nimportance and frequency.\nIn general, it is apparent that Remove Dead Code is the most vital refactoring operation ac-\nknowledged by practitioners, trailed by API Refactoring, Extract, Inline, Consolidate or Decompose\nconditional Expression, Rename, Move, Pull Up and Push Down. Extract and Inline obtain similar\nscores of importance, whereas Rename showcase no noteworthy advantage over Move, and Pull\nUp and Push Down are comparable in significance among practitioners. It is worth noting that the\npractitioners’ scores for refactoring operations’ frequency are not consistent with our observation\n, Vol. 1, No. 1, Article . Publication date: May 2024.\n14\nSiqi Wang, Xing Hu, Bei Wang, Wenxin Yao, Xin Xia, and Xinyu Wang\nin RQ1. There are various possible explanations for this, the one that comes from our dataset used\nin RQ1 will be discussed in 4.2.\nWe can also get insights from the characterization of the rating results for each refactoring\noperation. As stated in RQ1, Remove Dead Code is the most frequent operation in refactoring commits.\nHowever, it cannot guarantee the same status in the minds of practitioners. This can be attributed\nto Remove Dead Code being a clean-up process when other code changes are made, rather than a\nspecific intention. As a result, practitioners may be unaware of this operation. API Refactoring may\nbe considered important due to the significance of code interfaces and organizational structure in\ndeep learning projects that typically entail extensive data processing and model creation. However,\nwe did not observe such a high frequency of API Refactoring usage in RQ1, which may be since the\nkeyword-based filters we used screened out some of the API Refactoring by mistake. Extract and\nInline refactoring operations that are important in other languages, are also of high importance in\nthe minds of deep learning practitioners, reflecting their concern for code reusability and structural\nclarity. while Move and Rename received relatively low scores, perhaps because they are also\nrelatively low in difficulty to perform, and so have received practitioner slights. Consolidate or\nDecompose Conditional Expression is more commonly used in the minds of practitioners than we\nobserved in RQ1, probably because our keyword-based filter incorrectly excluding these operations.\nIn contrast to the above refactoring operations, practitioners tend not to prioritize Pull Up and Push\nDown. Developers ranked these two actions lowest in terms of importance and frequency of usage,\nconsistent with our observations and discussions in RQ1. We also found an interesting phenomenon\nwhen analyzing the data, that more experienced practitioners (who have been working for a longer\nperiod of time) are less likely to agree with the importance of these two refactoring operations for\ncode refactoring in deep learning projects.\nFinding 3: The respondents from practitioners had formed beliefs that were\ninconsistent with our observation in RQ1. Remove Dead Code and API Refactor-\ning are more important to practitioners. The importance of Rename and Move\noperations which are currently more widely studied in traditional project, is\nnot well recognized by practitioners of deep learning project. Especially Pull\nup and Push down receive a very low recognition rate. This also suggests that\ncode refactoring in the deep learning project may differ significantly from\ntraditional code refactoring.\n3.2.2\nRefactoring elements. As for refactoring elements in the context of refactoring in deep learn-\ning projects, we also invite practitioners to answer whether these refactoring operation elements\nare important and frequently used in deep learning projects. Figure 8 illustrates respondents’ rating\nof refactoring elements’ importance and frequency.\nAccording to the results, it is evident that Method and Class rank first and second, respectively,\nwith Variable and Statement closely following in terms of practitioners’ rating for “Do you think\nrefactoring elements below are important to improve the quality of the code in your deep learning\nproject?” and “Do you think this refactoring element is frequently used in your deep learning project?”\nIn comparison, Module falls significantly behind in importance score, but its rating for importance\nis similar to that of Variable and Statement.\nWe further analyzed the rating results of each element. Method is considered to be the most\ncrucial and frequently refactoring element in both traditional software and deep learning projects.\nThis is consistent with our observation in RQ1. Class comes second in rating results which could\nindicate that, in the area of deep learning, optimizations at the method and class level are being\n, Vol. 1, No. 1, Article . Publication date: May 2024.\nInsights into Deep Learning Refactoring: Bridging the Gap Between Practices and Expectations\n15\nVariable \nStatement \nMethod \nClass \nModule \nI \nI \nI \nI \nI \nI \n---•-----------~ \n30% \n20% \n10% \n0% \n10% \n20% \n30% \n40% \n50% \n60% \n70% \n80% \nPercentage of Valid Responses \n-\nStrongly Disagree \n~~ Neutral \n-\nStrong Agree \nDisagree \nAgree \n(a) Importance rate of refactoring elements\nVariable \nStatement \nI \nMethod \nI \nClass \nI \nModule I \n20% \n10% \n0% \n10% \n20% \n30% \n40%, \n50% \n60% \n70% \n80% \nPercentage of Valid Responses \n-\nStrongly Disagree \n~~ Neutral \n-\nStrong Agree \nDisagree \n-\nAgree \n(b) Frequency rate of refactoring elements\nFig. 8. Respondents’ rate of refactoring elements’ importance and frequency\ngiven priority by practitioners. They not only directly affect the logic and structure of the model, but\nare also easier to refactor with the right level of element. Statement and Variable have comparable\nand relatively small ratings. This could imply that for practitioners, the impact of refactoring at the\nstatement and variable levels on code quality in deep learning projects is deemed minimal, or might\nbe regarded as less significant compared to the refactoring at method and class levels. However, the\nfrequency of variable-level refactoring usage does not align with our observations in RQ1, and a\npossible explanation for this could be that the minuscule of its modifications causes practitioners to\nneglect to carry out a Variable-level refactoring operation. Module has a relatively low rating, this\ncould be attributed to the fact that restructuring modules demands a comprehensive understanding\nof the entire project structure, which might pose a significant challenge for practitioners. Moreover,\nin deep learning projects, modules often emphasize model components, data processing flow,\nor training pipelines, requiring less frequent refactoring. We also found that more experienced\npractitioners are less likely to agree with the importance of Module-level code refactoring in deep\nlearning projects.\nFinding 4: The responses from practitioners indicated that they formed be-\nliefs consistent with our observations in RQ1 on smaller elements (Method,\nStatement, and Variable), whereas respondents held inconsistent beliefs on\nlarger elements (Class, Module). Method is considered to be the most crucial\nand frequently employed level of element in refactoring deep learning projects,\nwhile Class comes second in rating results. Variable, Statement, and Module\nhave a similar frequency of usage in practitioners’ minds, while Module has a\nrelatively lower rating for the importance of code refactoring element in deep\nlearning projects.\n3.3\nRQ3: Practitioners’ Perspectives on Refactoring Tools\nIn RQ3, we surveyed practitioners about their use of refactoring tools consist of code smell detection\ntools and automatic refactoring tools. We further invite them to provide their perceptions of\nrefactoring tools to uncover the shortcomings of existing tools.\n3.3.1\nCode Smell Detection Tools. Of the 156 questionnaires responding to this section, 85 respon-\ndents (54.5%) indicated that they had used or were familiar with code smell detection tools. 73\npeople gave valid opinions about the tool.\nThere are many practitioners (28) who find code smell detection tools useful but also contain\nmany drawbacks, the biggest one is “Too many false positives”, which is the common view of 17\npractitioners. Practitioners who do not want to use these tools in development stated “...don’t\n, Vol. 1, No. 1, Article . Publication date: May 2024.\n16\nSiqi Wang, Xing Hu, Bei Wang, Wenxin Yao, Xin Xia, and Xinyu Wang\nbelieve the system. It annoyingly makes easy mistakes...” and said “It never seemed worth the hassle”.\nThere are a number of concerns that have deterred practitioners from using code smell detection\ntools. One is the “who need them most can’t figure out how to set them up...”, and “sometimes you\nneed to suppress the feedback and if the ability to filter out this feedback is too granular it leads to a\nlot of filler in your code, if it’s too broad you may miss out on useful feedback.” Some practitioners\nfind “it difficult to distinguish more semantically ”, and “... are often too pedantic and lack contextual\ninformation about the project structure”.\nThere are also some participants who concerned that the current tools “...are built for only\ntraditional software eng”, a participant finds the code smell detection tools “...usually aren’t designed\nwith ML projects in mind, and sometimes raise inappropriate warnings. For example, an ML algorithm\nmay have many hyper-parameters, but a code smell detection tool will complain that a function\nshouldn’t have so many arguments.” There is also a practitioner who said “...Python is behind other\nlanguages I’ve used in terms of tooling for smell/refactoring”.\nIn addition, many practitioners offered some insights into their go-to code smell detection tools,\nincluding SonarQube, Ruff, Flake8, IDE plugins, ChatGPT, and Copilot. SonarQube, the automated\ncode review platform, has received a moderately positive response. As a practitioner said “Sonarqube.\nI hate working with it. Such a clunky piece of software. I am much more a fan of linters like pylint and\nflake8, albeit they are sometimes not as feature complete for coding styles”. Of the code analysis tools,\npractitioners seem to favour Ruff, with some practitioners arguing that “...ruff has been helpful,\npylint is too hard to configure and anything more complex is too annoying...”. Ruff is popular for its\nspeedy execution, as one practitioner attested “I use Pylint, Ruff, different pre-commit hooks to find\ncode that can be refactored, bugs in my code. They are pretty great but most are very slow. Ruff is the\nfastest tool I used. It’ll be very good if all the tools are as fast as ruff”. Tools that combined with Large\nLanguage Models, like Copilot and ChatGPT, have recently gained popularity, that “These have\nbeen my go-to choices for code-related tasks”.\nFinding 5: Developers acknowledge the usefulness of traditional code smell\ndetection tools but have some concerns: (1) they always appear false positive,\n(2) do not apply to deep learning projects, and (3) they are not user-friendly.\nAs an alternative, combining code smell detection tools with Large Language\nModels is considered the future trend.\n3.3.2\nAutomatic Refactoring Tools. Of the 155 questionnaires responding to this section, 71 re-\nspondents (%45.8) indicated that they had used or were familiar with code smell detection tools. 62\npeople gave valid opinions about the tool.\nThere are many practitioners (23) who find automatic refactoring tools useful which is “...abso-\nlutely necessary to keep the code clean and professional” but only for sample cases and also weak in\naccuracy. The automatic refactoring tool “...makes things faster but it usually doesn’t handle heavy\ncomplexity too well...” as a practitioner thought these tools “Very useful for variable renaming or\nmoving around methods, breaking out chunks into methods. Haven’t done anything more complex\nwith them, complex refactor I would rather do by hand (to trust that it’s done right, but also complex\nrefactoring is done infrequently enough that it’s not worth learning the tool)”. Furthermore, it has\nbeen argued that the tool does not consider the context and structure of the project as a practitioner\nsaid “Automatic refactoring tools can do small amount of refactoring for common patterns. However\nthey lack holistic analysis of the code and thus leaving rest to manual refactoring”.\nSimilar to the code smell detection tools, practitioners found that current automatic refactoring\ntools were not designed for deep learning. A practitioner said that “I have found them useful for\n, Vol. 1, No. 1, Article . Publication date: May 2024.\nInsights into Deep Learning Refactoring: Bridging the Gap Between Practices and Expectations\n17\nweb server development mainly, but not so much for deep learning development”. There is also a\npractitioner who said “I’ve mostly used it for strongly typed languages (Java, TypeScript), which I\ndidn’t face much challenges with since the tools were able to accurately detect usages. I would imagine\nit might be more challenging for loosely typed languages”. The practitioners also thought these tools\nare hard to use, a practitioner stated “The python Rope library is where I have the most experience\n(outside of IDE tooling). I’ve found the docs to be somewhat lacking in “how to” guidance. I’ve had some\nsuccess reviewing the unit tests and work out from that how to use specific functionality. Given the\ncomplexity of the domain having documentation that serves the purposes of being detailed and then\n(and perhaps seperate material) is able to concisely demonstrate how to achieve refactoring operations\n- is needed ”.\nAs a practitioner stated that code smell detection tools “...Can introduce bugs or syntax issues”, the\nmost important concern that has deterred practitioners from working with automatic refactoring\ntools is their tendency to introduce bugs. Because “Sometimes wrong refactoring, takes longer to fix”.\nThe most common tools considered by practitioners are IDE plugins and tools combined with\nthe Large Language Model, such as Copilot and ChatGPT, which are good at code generation. In\ncontrast to traditional tools that receive a rating of “not good, not bad”, ChatGPT and Copilot have\nhigh expectations. A practitioner said “... I have had moderately good success refactoring code with a\nLLM and I expect to do it more often”. There also had a voice that the traditional automatic refactoring\ntools “Great but limited - integrations with Copilot would be great”. However, a practitioner said “To\nreally trust an automatic refactoring tool, you need to have a lot of trust in your test suite”, and there\nis also a practitioner who said“...tried chatgpt and copilot. Their suggestion is the right direction to go.\nHowever, one needs to test all the edge cases thoroughly.”\nFinding 6: Developers recognized the usefulness of automatic refactoring\ntools. But they also have many concerns about traditional tools: (1) they only\nwork for easy cases (2) while weak in accuracy and trending to introduce bugs,\n(3) they lack context or project structure, and (4) are hard to not user-friendly.\nBesides, tools combined with Large Language Models have been well received,\nbut the code they refactored is also considered worthy of further testing.\n3.3.3\nPractitioners’ Advice. There are 48 practitioners who gave valid advice for refactoring tools\nenhancement.\nThe most common view is that these tools should take more information into account, including\ncontextual and constructive information. A practitioner who gave a proposal said “...let’s imagine a\ntool, that takes into account project desicions, rules, conventions. moreover, tool that scans git history\nand ‘understands’ such git changes that were specifically about refactorings. and let’s imagine that\nsuch tool ‘understands’ commands that are relevant to specific project in natural language format\n(‘please do such refactoring: make separation of <this> module’, etc.)”.\nAnother part of the practitioners advised that the current tools for deep learning should be\nenhanced with customization features. A practitioner stated “More configurable so that they can also\nbe applied to libraries and frameworks”. There is also a practitioner said these tools should “...learn\nthe user’s coding style and adapt to it instead of forcing a predefined standard”.\nThe practitioners have made their own suggestions regarding the current difficulties in using\nthe tool. A practitioner said the tools need “Documentation and tutorials. Additionally having a con-\nsistent definition of operations”. While another practitioner wants these tools “Reduce configuration\nvariability, extend the documentation”. There is also a part of practitioners who hope the automatic\ntools do not ‘force’ them and easy to undo. A practitioner stated “I hope those tools are less invasive.\n, Vol. 1, No. 1, Article . Publication date: May 2024.\n18\nSiqi Wang, Xing Hu, Bei Wang, Wenxin Yao, Xin Xia, and Xinyu Wang\nSometimes I feel they ‘force’ me to refactor my code although some parts do not need a refactoring job”,\nand another practitioner said “...it should always be easy to undo/have an easily navigatable history\nof recent changes...”. Furthermore, it has been suggested by numerous practitioners that refactoring\nmust be accompanied by a testing component, as a practitioner said “The best way to refactor is by\nmaintaining the invariant that test coverage is complete and the tests pass. test harness...”, which is a\ncrucial part of the refactoring process.\nFinding 7: Deep learning practitioners offered their advice on how to improve\ncurrent automatic refactoring tools, including (1) taking context and project\nstructure into consideration, (2) building complete documents and configura-\ntion for ease of use, (3) adding customized features to be used in deep learning\nprojects, (4) offering tracker of history change to rollback and (5) combining a\ncomplete test component.\n4\nDISCUSSION\n4.1\nImplications\nOur results highlight a number of points to be further discussed and several implications for the\nresearch community:\n4.1.1\nStrengthening research on code refactoring in deep learning projects. Our research demon-\nstrates that refactoring practices in deep learning projects differ greatly from those in traditional\nJava projects. Remove Dead Code, API Refactoring, and Consolidation or Decompose Expression\nare crucial refactoring operations in deep learning projects. Nevertheless, the current research\nconcentrates primarily on Java refactoring, and there is still an absence of research regarding the\naforementioned refactoring operations. Moreover, some developers have also made other requests\nfor refactoring like “Handling C++-Python FFI boundaries...” and “Rearchitecture”.\n4.1.2\nOptimizing the development process. According to our observations in RQ1, Remove Dead\nCode and Rename are the two most common refactoring operation types in deep learning projects,\nwhich could be a result of inadequate code review and naming conventions. Practitioners also\nmentioned “Since OSS usually has more contributors, it is more necessary to have clear guidance/con-\nfiguration for those tools, as it would lead to many conflicts otherwise”. Optimizing the development\nprocess for deep learning projects is essential, which includes establishing naming conventions,\nidentifying and cleaning up dead code. It is important to ensure that all team members share and\nfollow these standards to improve code readability and maintainability.\n4.1.3\nImproving the accuracy of refactoring tools. Among the concerns about either code smell\ndetection tools or automatic refactoring tools, the most frequently mentioned by developers is the\nissue of accuracy. For small-scale refactoring, many refactoring operations have been mentioned by\ndevelopers as challenges that current refactoring tools do not do a good job of dealing with, e.g.,“...it\ncan cause troubles sometimes if you rename a variable to a name that already is taken by another\nvariable...”. As for large-scale refactoring operations, current refactoring tools pose difficulties\nin implementing as they do not adequately consider context and project structure. As a part of\ndevelopers responded that refactoring tools “...should offer detailed custom configuration to guide the\ntool’s task...”, clear documentation and a guidebook should be provided to assist users in correctly\nutilizing the refactoring tool. Ensuring accuracy necessitates not only the functionality of the tool\nbut also its proper usage and comprehension by users.\n, Vol. 1, No. 1, Article . Publication date: May 2024.\nInsights into Deep Learning Refactoring: Bridging the Gap Between Practices and Expectations\n19\n4.1.4\nCombining refactoring with testing. Testing is a critical aspect of refactoring, which ensures\ncode refactoring does not break existing functionality and maintains the stability and reliability\nof the system. However, to the best of our knowledge, none of the prevalent refactoring tools\nare currently supported by a testing system that meets the developer’s requirements. This is also\none of the main factors that prevent practitioners from using automatic refactoring tools, that a\npractitioner said, “To really trust an automatic refactoring tool, you need to have a lot of trust in your\ntest suite”. In fact, when participants were asked to provide suggestions for enhancing the tool,\nsome suggested that “Just a better test coverage after the automatic refactoring”.\n4.1.5\nRefactoring in the Era of Large Language Model. According to the respondents of our survey,\ncurrent refactoring tools are “... not smart ...being inflexible”, and also lack the use of code context\nto support a customizable refactoring tool. Large Language Model can aid in comprehending\ncode, enhancing our understanding of the code context. By incorporating Large Language Model\nand code analysis techniques, automatic refactoring suggestions can be generated. This provides\ncontext-specific and personalized proposals based on the syntax rules, code context, and project\nstructure.\n4.2\nThreats to Validity\nOur research threats come from the two stages of our empirical study\n4.2.1\nRefactoring Detection. Due to the high number of project commits employed, conducting a\nfull and detailed analysis of this data was not feasible. Therefore, following the approach of Alomar\net al. [6], we utilized keyword-based filters to filter the commits initially. This approach may have\nlost some refactoring commits, leading to an incomplete analysis of our refactoring practices in the\ndeep learning repository.\nFurthermore, our selection comprises only five highly popular deep learning projects which\npossess extensive maintenance information and boast active open source communities. Neverthe-\nless, other deep learning projects may not have reached maturity in terms of development and\nmaintenance, and their actual implementation of code refactoring may vary from the projects we\nemployed. To reduce these threats, we conducted a survey of deep learning practitioners to explore\ntheir real refactoring practices.\n4.2.2\nOnline Survey. It is possible that some of our survey respondents may not have a clear under-\nstanding of code refactoring techniques or our questions, and thus their responses may introduce\nnoise to the data that we collect. To reduce this threat, we included the category “I don’t know”\nto filter respondents who do not understand our brief descriptions in multiple-choice questions\nand dropped “I don’t know” ratings in data analysis. When analyzing responses to short answer\nquestions, we eliminated responses such as “N/A”. We also drop responses by respondents who\ncomplete the survey in less than two minutes. Still, we cannot fully ascertain whether participant\nresponses are accurate reflections of their beliefs. This is a common and tolerable threat to validity\nin many previous studies about practitioners’ perceptions and expectations [24].\nAnother threat is that our participants are contributors from open source deep learning reposito-\nries, who may not be representations of commercial deep learning software engineers. It is still\nunclear whether our insights are still suitable for commercial deep learning projects. Investigating\nthe perspective of commercial deep learning developers is our ongoing work.\n5\nRELATEDWORK\nRefactoring is recognized as a fundamental practice for keeping software sustainable and healthy [16,\n27, 37, 39]. For this reason, extensive empirical research has recently been conducted to extend\n, Vol. 1, No. 1, Article . Publication date: May 2024.\n20\nSiqi Wang, Xing Hu, Bei Wang, Wenxin Yao, Xin Xia, and Xinyu Wang\nTable 5. Summary of Previous Work of Code Refactoring\nStudy\nContent\nMethodology\nSummary of Findings\nMurphy-Hill et al. [28]\nHow developers prac-\ntice refactoring activi-\nties\nAnalyzing\ncommit\nhistory\nDevelopers typically perform numerous\nrefactorings in a short period, 90% refactor-\nings are done manually.\nNegara et al. [29]\nManual\nand\nauto-\nmated refactoring\nAnalyzing refactoring\ninstances\nOver half of refactorings are done manually,\n30% of applied refactorings do not reach ver-\nsion control.\nAlOmar et al. [9]\nRefactoring activities\nand contributors in\nopen source projects\nAnalyzing 800 open\nsource projects using\nRefactoring Miner\nNo correlation between experience and refac-\ntoring motivation, top contributors perform\nmore variety of refactoring operations.\nChaves et al. [15]\nThe impact of refac-\ntoring\noperations\non internal quality\nattributes\nAnalyzing\n29,303\nrefactoring\nopera-\ntions\nOver 94% refactorings applied to code ele-\nments with at least one critical internal qual-\nity attribute, and improved the internal qual-\nity.\nVassallo et al. [41]\nWhich\nrefactoring\noperations are more\nprevalent, and main\nfactors\nleading\nto\nrefactoring\nAnalyzing\ncommit\nhistory of 200 open\nsource systems\nDevelopers adopt refactoring mainly to im-\nprove understandability, schedule refactoring\nafter system is stable, files likely refactored\nby their owners.\nWang et al. [42]\nIntrinsic and extrinsic\nfactors driving refac-\ntoring\nInterviewing 10 ex-\npert developers\nIdentified intrinsic and extrinsic factors driv-\ning refactoring, and also identified tool avail-\nability as a prominent factor.\nVakilian et al. [40]\nHow\nprogrammers\nuse refactoring tools\nCollecting and ana-\nlyzing Java program-\nming interaction data\nProgrammers prefer lightweight ways to in-\nvoke refactorings, and make small changes us-\ning tools even if tools are occasionally buggy.\nKim et al. [24]\nMicrosoft engineers’\nview on refactoring\nSurveying\n328\nMi-\ncrosoft engineers\nDevelopers place less importance on behav-\nior preservation, system-wide refactoring re-\nduced inter-module dependencies and post-\nrelease defects.\nJain et al. [22]\nRefactoring\ntrends\nand research opportu-\nnities\nSurveying 221 IT pro-\nfessionals\nRefactoring tools are under-used due to avail-\nability, usability, and trust issues, automated\nsystem is needed.\nOliveira et al. [30]\nRelevance of refactor-\ning customization and\ntool support\nAnalyzing 1,162 refac-\ntorings, surveying 40\ndevelopers\nDevelopers confirmed the relevance of cus-\ntomization patterns and agreed that im-\nprovements in IDE refactoring support were\nneeded.\nOliveria et al. [31]\nDevelopers’\nun-\nderstanding\nof\nrefactoring detection\ntool mechanics\nSurveying\n53\nJava\nproject developers\nTools do not detect many refactorings ex-\npected by developers, most developers do not\nfollow refactoring mechanisms used by refac-\ntoring detection tools.\nour knowledge of this practice. The two major lines of research related to our work are (1) studies\nbased on refactoring practices and (2) studies based on surveys and interviews. Table 5 shows an\noverview of empirical studies on code refactoring.\n5.1\nStudies based on refactoring practices\nThere has been much work that analyzed code changes or development documentation in reposi-\ntories to gain insights related to refactoring. Murphy-Hill et al. [28] investigated how developers\n, Vol. 1, No. 1, Article . Publication date: May 2024.\nInsights into Deep Learning Refactoring: Bridging the Gap Between Practices and Expectations\n21\npractice refactoring activities by analyzing historical commits. The researchers found that program-\nmers typically perform numerous refactorings within a brief timeframe, and 90% of refactorings\nare carried out manually. Negara et al. [29] presented the first comprehensive empirical study that\nconsiders both manual and automated refactoring, using a large corpus of refactoring instances\ndetected through an algorithm that infers refactorings from fine-grained code edits. Their central\nfindings reveal that over half of the refactorings are performed manually, and 30% of the applied\nrefactorings do not reach the version control system. AlOmar et al. [9] analyzed 800 open-source\nprojects by mining their refactoring activities using Refactoring Miner, and identified their cor-\nresponding contributors. They found there is no correlation between experience and motivation\nbehind refactoring, top contributed developers are found to perform a wider variety of refactoring\noperations. Chaves et al. [15] analyzed the version history of 23 open source projects with 29,303\nrefactoring operations and found that developers apply more than 94% of the refactoring operations\nto code elements with at least one critical internal quality attribute, and always improved the\ninternal quality attributes. Vassallo et al. [41] analyzed the change history of 200 open source\nsystems at the commit level to investigate which refactoring operations are more diffused when\nrefactoring operations are applied, and which are the main developer-oriented factors leading to\nrefactoring. They found developers adopt refactoring mainly to improve the understandability of\nsource code and mainly schedule refactoring after the system’s structure is stable. They also found\nthat source code files are most likely to be refactored by their owners rather than others.\nThere are also some studies investigating the performance of refactoring on issues such as reuse,\nsecurity, and maintenance [10, 12, 20, 21, 25, 27, 36]. Nevertheless, most of studies above have\nfocused on Java or JavaScript projects, leaving a gap in the insights of refactoring on deep learning\nprojects.\n5.2\nStudies based on surveys and interviews\nWang et al. [42] conducted a study involving 10 expert software developers. They identified both\nintrinsic (self-motivated) and external (influenced by peers or management) factors that drive\nrefactoring activity. The research also highlighted tool availability as a prominent factor that\nenables developers to translate their motivations into actions. Vakilian et al. [40] collected and\nanalyzed interaction data from Java programming and found that programmers prefer lightweight\nmethods of invoking refactorings, usually perform small changes using the refactoring tool. They\nalso found that programmers use predictable automated refactorings even if they have rare bugs\nor change the behavior of the program by interview. Kim et al. [24] surveyed 328 engineers from\nMicrosoft and found the findings reveal that developers place less importance on the requirement\nof preserving behavior within refactoring definitions. They further interviewed the Windows\nrefactoring team to gain insights into the methods employed in system-wide refactoring. They\nfound that binary modules refactored by the team showed a considerable reduction in inter-module\ndependencies and post-release defects. Jain et al. [22] surveyed 221 IT professionals to understand\nthe trends followed by developers and what are the research opportunities in the field of refactoring\nand developmental challenges. They found that refactoring tools are under-used as they have\navailability, usability and trust issues. An automated system is the need of the hour to ensure\nconsistency in change management, visualize the structure of code, inspect code, detect design\nissues, and carry out refactoring. Oliveira et al. [30] analyzed 1,162 refactorings composed of\nmore than 100k program modifications from 13 software projects and conducted a survey with 40\ndevelopers about the most frequent customization patterns they found. Developers confirmed the\nrelevance of customization patterns and agreed that improvements in IDE’s refactoring support\nwere needed. Oliveria et al. [31] surveyed 53 developers of popular Java projects on GitHub to\ngain a better understanding of the mechanics of refactoring detection tools. They found that\n, Vol. 1, No. 1, Article . Publication date: May 2024.\n22\nSiqi Wang, Xing Hu, Bei Wang, Wenxin Yao, Xin Xia, and Xinyu Wang\nrefactoring detection tools did not detect many refactoring operations expected by developers and\nmost developers did not follow the refactoring mechanisms used by refactoring detection tools.\nThe above work conducted interviews or surveys to discover developers’ perspectives about\nrefactoring and refactoring tools. However, they are still mostly limited to developers of Java\nprojects. The findings related to refactoring in deep learning projects, which is currently a fast-\ngrowing area and is very different from traditional Java projects in terms of development and\nmaintenance, still need to be investigated.\n6\nCONCLUSION AND FUTURE WORK\nCode refactoring is an important part of deep learning project development. In this work, we\nmanually analyzed five deep learning projects’ history commits to detect refactoring operation,\nand further surveyed 159 deep learning practitioners from 38 countries for practitioners’ views on\nrefactoring and their expectations of refactoring tools. The refactoring practices in deep learning\nprojects are different from those in Java projects, and some of the most common refactoring opera-\ntions are lack of research. Practitioners in deep learning recognized the importance of refactoring\nin the development of deep learning projects. They also offered comments and advice on current\nrefactoring tools.\nWe highlight the limitations of current research and suggest future directions for the improvement\nof code refactoring in deep learning projects. Moreover, we present practitioners’ expectations\nregarding refactoring tools and provide suggestions for their enhancements.\nTo further improve code refactoring practices in the field of deep learning, researchers should\ncollaborate with deep learning practitioners continuously. Future studies could put more effort into\nrefactoring in deep learning, and develop automation tools for deep learning projects to improve\nthe overall efficiency of code maintenance.\nREFERENCES\n[1] 2023. keras. https://github.com/keras-team/keras\n[2] 2023. Pytorch. https://github.com/pytorch/pytorch\n[3] 2023. Scikit-learn. https://github.com/scikit-learn/scikit-learn\n[4] 2023. Tensorflow. https://github.com/tensorflow/tensorflow\n[5] 2023. Transformers. https://github.com/huggingface/transformers\n[6] Eman AlOmar, Mohamed Wiem Mkaouer, and Ali Ouni. 2019. Can refactoring be self-affirmed? an exploratory study\non how developers document their refactoring activities in commit messages. In 2019 IEEE/ACM 3rd International\nWorkshop on Refactoring (IWoR). IEEE, 51–58.\n[7] Eman Abdullah AlOmar, Mohamed Wiem Mkaouer, Ali Ouni, and Marouane Kessentini. 2019. On the impact\nof refactoring on the relationship between quality attributes and design metrics. In 2019 ACM/IEEE International\nSymposium on Empirical Software Engineering and Measurement (ESEM). IEEE, 1–11.\n[8] Eman Abdullah AlOmar, Anthony Peruma, Mohamed Wiem Mkaouer, Christian Newman, Ali Ouni, and Marouane\nKessentini. 2021. How we refactor and how we document it? On the use of supervised machine learning algorithms to\nclassify refactoring documentation. Expert Systems with Applications 167 (2021), 114176.\n[9] Eman Abdullah AlOmar, Anthony Peruma, Mohamed Wiem Mkaouer, Christian D Newman, and Ali Ouni. 2021.\nBehind the scenes: On the relationship between developer experience and refactoring. Journal of Software: Evolution\nand Process (2021), e2395.\n[10] Eman Abdullah AlOmar, Tianjia Wang, Vaibhavi Raut, Mohamed Wiem Mkaouer, Christian Newman, and Ali Ouni.\n2022. Refactoring for reuse: an empirical study. Innovations in Systems and Software Engineering (2022), 1–31.\n[11] Saleema Amershi, Andrew Begel, Christian Bird, Robert DeLine, Harald Gall, Ece Kamar, Nachiappan Nagappan,\nBesmira Nushi, and Thomas Zimmermann. 2019. Software engineering for machine learning: A case study. In 2019\nIEEE/ACM 41st International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP). IEEE,\n291–300.\n[12] Peruma Anthony, Steven Simmons, Eman Abdullah AlOmar, Christian D Newman, Mkaouer Mohamed Wiem, and\nOuni Ali. 2022. How do i refactor this? An empirical study on refactoring trends and topics in Stack Overflow. Empirical\nSoftware Engineering 27, 1 (2022).\n, Vol. 1, No. 1, Article . Publication date: May 2024.\nInsights into Deep Learning Refactoring: Bridging the Gap Between Practices and Expectations\n23\n[13] Hassan Atwi, Bin Lin, Nikolaos Tsantalis, Yutaro Kashiwa, Yasutaka Kamei, Naoyasu Ubayashi, Gabriele Bavota, and\nMichele Lanza. 2021. PyRef: refactoring detection in Python projects. In 2021 IEEE 21st international working conference\non source code analysis and manipulation (SCAM). IEEE, 136–141.\n[14] Aline Brito, Andre Hora, and Marco Tulio Valente. 2021. Characterizing refactoring graphs in Java and JavaScript\nprojects. Empirical Software Engineering 26 (2021), 1–43.\n[15] Alexander Chávez, Isabella Ferreira, Eduardo Fernandes, Diego Cedrim, and Alessandro Garcia. 2017. How does\nrefactoring affect internal quality attributes? A multi-project study. In Proceedings of the XXXI Brazilian Symposium on\nSoftware Engineering. 74–83.\n[16] Martin Fowler. 2018. Refactoring. Addison-Wesley Professional.\n[17] Isaac Griffith, Scott Wahl, and Clemente Izurieta. 2011. TrueRefactor: An automated refactoring tool to improve legacy\nsystem and application comprehensibility. In Proceedings of the 24th International Conference on Computer Applications\nin Industry and Engineering (CAINE). 1.\n[18] Qianyu Guo, Sen Chen, Xiaofei Xie, Lei Ma, Qiang Hu, Hongtao Liu, Yang Liu, Jianjun Zhao, and Xiaohong Li. 2019. An\nempirical study towards characterizing deep learning development and deployment across different frameworks and\nplatforms. In 2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, 810–822.\n[19] Yuepu Guo, Carolyn Seaman, Nico Zazworka, and Forrest Shull. 2010. Domain-specific tailoring of code smells:\nan empirical study. In Proceedings of the 32nd ACM/IEEE International Conference on Software Engineering-Volume 2.\n167–170.\n[20] Mark Harman and Laurence Tratt. 2007. Pareto optimal search based refactoring at the design level. In Proceedings of\nthe 9th annual conference on Genetic and evolutionary computation. 1106–1113.\n[21] Emanuele Iannone, Zadia Codabux, Valentina Lenarduzzi, Andrea De Lucia, and Fabio Palomba. 2023. Rubbing salt in\nthe wound? A large-scale investigation into the effects of refactoring on security. Empirical Software Engineering 28, 4\n(2023), 89.\n[22] Shivani Jain and Anju Saha. 2019. An Empirical Study on Research and Developmental Opportunities in Refactoring\nPractices.. In SEKE. 313–418.\n[23] Nikhil Ketkar and Eder Santana. 2017. Deep learning with Python. Vol. 1. Springer.\n[24] Miryung Kim, Thomas Zimmermann, and Nachiappan Nagappan. 2014. An empirical study of refactoringchallenges\nand benefits at microsoft. IEEE Transactions on Software Engineering 40, 7 (2014), 633–649.\n[25] Yun Lin, Xin Peng, Yuanfang Cai, Danny Dig, Diwen Zheng, and Wenyun Zhao. 2016. Interactive and guided\narchitectural refactoring with search-based recommendation. In Proceedings of the 2016 24th ACM SIGSOFT International\nSymposium on Foundations of Software Engineering. 535–546.\n[26] Jiakun Liu, Qiao Huang, Xin Xia, Emad Shihab, David Lo, and Shanping Li. 2020. Is using deep learning frameworks\nfree? characterizing technical debt in deep learning frameworks. In Proceedings of the ACM/IEEE 42nd International\nConference on Software Engineering: Software Engineering in Society. 1–10.\n[27] Panita Meananeatra. 2012. Identifying refactoring sequences for improving software maintainability. In Proceedings of\nthe 27th IEEE/ACM International Conference on Automated Software Engineering. 406–409.\n[28] Emerson Murphy-Hill, Andrew P. Black, Danny Dig, and Chris Parnin. 2008. Gathering refactoring data: a comparison\nof four methods. In Proceedings of the 2nd Workshop on Refactoring Tools. https://doi.org/10.1145/1636642.1636649\n[29] Stas Negara, Nicholas Chen, Mohsen Vakilian, Ralph E Johnson, and Danny Dig. 2013. A comparative study of manual\nand automated refactorings. In ECOOP 2013–Object-Oriented Programming: 27th European Conference, Montpellier,\nFrance, July 1-5, 2013. Proceedings 27. Springer, 552–576.\n[30] Daniel Oliveira, Wesley KG Assunção, Alessandro Garcia, Ana Carla Bibiano, Márcie Ribeiro, Rohit Gheyi, and Baldoino\nFonseca. 2023. The untold story of code refactoring customizations in practice. In 2023 IEEE/ACM 45th International\nConference on Software Engineering (ICSE). IEEE, 108–120.\n[31] Jonhnanthan Oliveira, Rohit Gheyi, Leopoldo Teixeira, Márcio Ribeiro, Osmar Leandro, and Baldoino Fonseca. 2023.\nTowards a better understanding of the mechanics of refactoring detection tools. Information and Software Technology\n(2023), 107273.\n[32] QSR International. 2017. NVivo.\nComputer software. Available from https://www.qsrinternational.com/nvivo-\nqualitative-data-analysis-software/home.\n[33] Sebastian Raschka and Vahid Mirjalili. 2017. Python machine learning: Machine learning and deep learning with\npython. Scikit-Learn, and TensorFlow. Second edition ed 3 (2017).\n[34] Simone Romano, Christopher Vendome, Giuseppe Scanniello, and Denys Poshyvanyk. 2018. A multi-study investigation\ninto dead code. IEEE Transactions on Software Engineering 46, 1 (2018), 71–99.\n[35] Koki Saitoh. 2021. Deep learning from the basics: Python and deep learning: Theory and implementation. Packt Publishing\nLtd.\n[36] Giulia Sellitto, Emanuele Iannone, Zadia Codabux, Valentina Lenarduzzi, Andrea De Lucia, Fabio Palomba, and Filomena\nFerrucci. 2022. Toward understanding the impact of refactoring on program comprehension. In 2022 IEEE international\n, Vol. 1, No. 1, Article . Publication date: May 2024.\n24\nSiqi Wang, Xing Hu, Bei Wang, Wenxin Yao, Xin Xia, and Xinyu Wang\nconference on software analysis, evolution and reengineering (SANER). IEEE, 731–742.\n[37] Danilo Silva, Nikolaos Tsantalis, and Marco Tulio Valente. 2016. Why we refactor? confessions of github contributors.\nIn Proceedings of the 2016 24th acm sigsoft international symposium on foundations of software engineering. 858–870.\n[38] Danilo Silva and Marco Tulio Valente. 2017. RefDiff: Detecting refactorings in version histories. In 2017 IEEE/ACM\n14th International Conference on Mining Software Repositories (MSR). IEEE, 269–279.\n[39] Nikolaos Tsantalis, Ameya Ketkar, and Danny Dig. 2020. RefactoringMiner 2.0. IEEE Transactions on Software\nEngineering 48, 3 (2020), 930–950.\n[40] Mohsen Vakilian, Nicholas Chen, Stas Negara, Balaji Ambresh Rajkumar, Brian P Bailey, and Ralph E Johnson. 2012.\nUse, disuse, and misuse of automated refactorings. In 2012 34th international conference on software engineering (icse).\nIEEE, 233–243.\n[41] Carmine Vassallo, Giovanni Grano, Fabio Palomba, Harald C Gall, and Alberto Bacchelli. 2019. A large-scale empirical\nexploration on refactoring activities in open source software projects. Science of Computer Programming 180 (2019),\n1–15.\n[42] Yi Wang. 2009. What motivate software engineers to refactor source code? evidences from professional developers. In\n2009 ieee international conference on software maintenance. IEEE, 413–416.\n[43] Steven Euijong Whang and Jae-Gil Lee. 2020. Data collection and quality challenges for deep learning. Proceedings of\nthe VLDB Endowment 13, 12 (2020), 3429–3432.\n, Vol. 1, No. 1, Article . Publication date: May 2024.\n",
  "categories": [
    "cs.SE"
  ],
  "published": "2024-05-08",
  "updated": "2024-05-08"
}