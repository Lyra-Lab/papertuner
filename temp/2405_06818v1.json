{
  "id": "http://arxiv.org/abs/2405.06818v1",
  "title": "The Ghanaian NLP Landscape: A First Look",
  "authors": [
    "Sheriff Issaka",
    "Zhaoyi Zhang",
    "Mihir Heda",
    "Keyi Wang",
    "Yinka Ajibola",
    "Ryan DeMar",
    "Xuefeng Du"
  ],
  "abstract": "Despite comprising one-third of global languages, African languages are\ncritically underrepresented in Artificial Intelligence (AI), threatening\nlinguistic diversity and cultural heritage. Ghanaian languages, in particular,\nface an alarming decline, with documented extinction and several at risk. This\nstudy pioneers a comprehensive survey of Natural Language Processing (NLP)\nresearch focused on Ghanaian languages, identifying methodologies, datasets,\nand techniques employed. Additionally, we create a detailed roadmap outlining\nchallenges, best practices, and future directions, aiming to improve\naccessibility for researchers. This work serves as a foundational resource for\nGhanaian NLP research and underscores the critical need for integrating global\nlinguistic diversity into AI development.",
  "text": "arXiv:2405.06818v1  [cs.CL]  10 May 2024\nThe Ghanaian NLP Landscape: A First Look\nSheriff Issaka, Zhaoyi Zhang, Mihir Heda, Yinka Ajibola,\nRyan DeMar, Keyi Wang, Xuefeng Du\nUniversity of Wisconsin-Madison, United States\n{issaka, zzhang825, mheda, ovajibola,\nrdemar, kwang383, xdu66}@wisc.edu\nAbstract\nDespite comprising one-third of global lan-\nguages, African languages are critically\nunderrepresented in Artiﬁcial Intelligence\n(AI), threatening linguistic diversity and\ncultural heritage.\nGhanaian languages, in\nparticular, face an alarming decline, with\ndocumented extinction and several at risk.\nThis study pioneers a comprehensive sur-\nvey of Natural Language Processing (NLP)\nresearch focused on Ghanaian languages,\nidentifying methodologies, datasets, and\ntechniques employed. Additionally, we cre-\nate a detailed roadmap outlining challenges,\nbest practices, and future directions, aim-\ning to improve accessibility for researchers.\nThis work serves as a foundational resource\nfor Ghanaian NLP research and underscores\nthe critical need for integrating global lin-\nguistic diversity into AI development.\n1\nIntroduction\nUnderrepresented\nyet abundant,\nAfrican lan-\nguages make up a third of the world’s languages,\nwith over 2,000 distinct tongues.\nDespite their\ncultural signiﬁcance, symbolic meaning, diver-\nsity, and history, these languages remain con-\nspicuously underrepresented in Artiﬁcial Intelli-\ngence (AI). This paucity culminates macro socioe-\nconomic factors, historical events such as colo-\nnization and globalization, and local factors such\nas military conquest and misguided government\npolicies (Childs, 2020).\nConsequently, a stag-\ngering number of African languages – especially\nthose spoken by small speech communities – face\nendangerment, with up to 10 percent projected\nto go extinct within a century (UNESCO, 2010;\nChilds, 2020). The magnitude of this issue is best\nillustrated by Batibo’s comprehensive survey of\nlanguage situations in African countries (Batibo,\n2005), which identiﬁed 509 African languages as\nhighly endangered, extinct, or on the brink of ex-\ntinction. Alarmingly, this number may be an un-\nderestimation given the lack of complete socio-\nlinguistic information for most African languages\nand the potential for language shifts to accelerate\nthe extinction rate, putting even more languages at\nrisk of disappearing by the end of the 21st century.\nOf the 84 established languages in Ghana, one\nis extinct, and eight others are either in trouble\nor dying (Eberhard et al., 2024). This dire situa-\ntion underscores the urgent need for action to pre-\nserve these languages before they are lost forever.\nFortunately, the digital age offers a unique oppor-\ntunity to combat this decline. By harnessing ad-\nvanced Natural Language Processing (NLP) tech-\nniques, especially Machine Translation (MT), re-\nsearchers have contributed to preserving and dig-\nitizing some Ghanaian languages, ensuring their\nperpetuity and enhancing their representation in\nAI systems. However, the current state of research\nin this domain remains fragmented and limited in\nscope, with most Ghanaian languages still lacking\nany digitization or research efforts.\nWhile it is widely acknowledged within the\nNLP community that African languages, and by\nextension, Ghanaian languages, face a scarcity of\nresources and research attention, a comprehen-\nsive assessment of this imbalance is conspicuously\nabsent.\nPrevious surveys, such as (Magueresse\net al., 2020; Haddow et al., 2022), have offered\nonly high-level insights into the state of African\nlanguage research in NLP, lacking the granular-\nity needed for region-speciﬁc evaluations.\nThis\nlack of detailed analysis has led to inconsistencies\nin the reported state of the ﬁeld and challenges\nfor researchers in identifying the most pressing\nneeds and opportunities for impact.\nFor exam-\nple, the surveys fail to capture the speciﬁc chal-\nlenges faced by researchers working on Ghanaian\nlanguages, such as the lack of standardized orthog-\nraphy and the difﬁculty in collecting and annotat-\ning low-resource language (LRL) data.\nIn contrast, this study adopts a novel region-\nspeciﬁc\napproach\nto\nmeticulously\nexamine\nGhana’s linguistic landscape.\nThis research\nendeavors to achieve four primary objectives: (1)\nprovide the ﬁrst extensive survey of NLP research\nspeciﬁc to Ghanaian languages, thereby creating a\ncomprehensive overview of the current state of re-\nsearch in this domain; (2) scrutinize and compare\nmodels, metrics, datasets, and techniques utilized\nin prior studies, aiming to identify strengths and\nareas for improvement; (3) outline a roadmap of\nchallenges, best practices, and recommendations\nfor future research, aimed at fostering a robust and\neffective research framework; and (4) contribute\nto the sparse body of knowledge of LRLs, thereby\nenriching the broader academic discourse, pro-\nviding new insight, and paving the way for more\ninclusive linguistic studies.\n2\nRelated Work\nTo our knowledge, no previous work has con-\nducted surveys on Ghanaian languages concern-\ning MT or NLP. While some studies have dis-\ncussed Ghanaian languages (Azunre et al., 2021a),\nsurveyed other West African languages (Asubiaro\nand Igwe, 2021), or broadly addressed LRLs\n(Magueresse et al., 2020; Haddow et al., 2022;\nMabokela et al., 2023; Ranathunga et al., 2023),\nnone have provided the depth and speciﬁcity re-\nquired to understand the unique characteristics and\nchallenges of Ghanaian languages.\nOur work aims to ﬁll this critical literature gap\nby conducting the ﬁrst region-speciﬁc, large-scale\nNLP systematic review of Ghanaian languages.\n3\nGhanaian Languages\nGhana, a West African nation with a population\nexceeding 33 million, boasts a rich linguistic land-\nscape. While English, a legacy of British colonial\nrule, is the ofﬁcial language, the country is home\nto 73 living indigenous languages (Eberhard et al.,\n2024). Among these, Asante, Ewe, Fante, Boron,\nDagomba, Dangme, Dagarte, Kokomba, Akyem,\nand Ga stand out as the most widely spoken (Ta-\nble 1) (Ghana Statistical Service, 2023).\nAkan, consisting of distinct dialects, is the\nmost widely spoken language, with approximately\n44% of Ghana’s population being native speak-\ners (Osam, 2003). These dialects can be divided\ninto two main groups: Fante and Twi. The Fante\ngroup has several sub-dialects, including Gomua,\nEkumﬁ, Nkusukum, Iguae, Breman, and some-\ntimes Agona. On the other hand, the Twi group\nconsists of two main dialects: Akuapem and As-\nante. Both dialects are mutually intelligible, used\nfor instructional purposes, and spoken by predom-\ninantly southern communities in Ghana. Although\nAsante Twi enjoys wider usage, Akuapem Twi has\nhistorically been recognized as the written literary\nstandard (Schachter and Fromkin, 1968). The two\ndialects share signiﬁcant phonological similarities\nand structural variations, such as morphological\nredundancies and adherence to the same vowel\nharmony rules (Schachter and Fromkin, 1968).\nGovernment policies have played a pivotal role\nin shaping language use in Ghana. The current\nlanguage policy mandates using the child’s ﬁrst\nlanguage (L1) as the medium of instruction from\nkindergarten through Grade 3, with English taking\nover as the L1 thereafter (USAID, 2020). How-\never, these L1 indigenous languages are limited\nto 11 government-sponsored languages: Akuapem\nTwi, Asanti Twi, Fante, Nzema, Dagaare, Dag-\nbane, Ewe, Dangme, Ga, Gonja, and Kasem.\nStudies have revealed a relatively low implementa-\ntion of the mother tongue policy (Awedoba, 2009;\nAdika, 2012; Davis and Agbenyega, 2012), with\ncritics suggesting that the limited selection of lan-\nguages and the lack of textbooks and qualiﬁed\nteachers often force educators to resort to English\nas the medium of instruction (Akpanglo-Nartey\nand Akpanglo-Nartey, 2012; Owu-Ewie, 2017).\nThe unintended consequences of Ghana’s lan-\nguage policies have been far-reaching, with cer-\ntain indigenous languages, such as Akan, gaining\nprominence at the expense of others. This over-\nshadowing effect has posed a signiﬁcant threat to\nthe preservation and vitality of less widely spoken\nlanguages, rivaling the impact of English as the\nofﬁcial language. The media landscape in Ghana\nhas further exacerbated this issue, with Akan,\nparticularly the Twi dialect, dominating 90% of\nnon-English television programming (Akpanglo-\nNartey and Akpanglo-Nartey, 2012). This dispro-\nportionate representation has not only limited the\nexposure of other indigenous languages but has\nalso contributed to their marginalization and po-\ntential endangerment, highlighting the urgent need\nLanguage\n% of speakers\nAsante\n16%\nEwe\n14%\nFante\n11.6%\nBoron (Brong)\n4.9%\nDagomba\n4.4%\nDangme\n4.2%\nDagarte (Dagaba)\n3.9%\nKokomba\n3.5%\nAkyem\n3.2%\nGa\n3.1%\nOther\n31.2%\nTable 1: Percentage of speakers for top indigenous\nGhanaian languages.\nfor more inclusive and equitable language policies\nand practices in Ghana.\nThe combination of these language policies and\nthe dwindling number of ﬂuent elderly speakers in\nindigenous communities has compelled language\nenthusiasts to explore alternative ways to engage\nwith these languages.\nThe growing interest in\ntechnological solutions to preserve and revitalize\nendangered African languages stems from the de-\nsire to safeguard the vast knowledge and culture\nembedded within them (Galla, 2016).\n4\nNLP and MT in LRLs Revitalization\nNLP and MT have emerged as the primary tech-\nnological approaches for revitalizing African lan-\nguages. NLP, an interdisciplinary ﬁeld that com-\nbines Linguistics, Computer Science, and AI, en-\nables computers to understand and process hu-\nman language.\nMT, a subﬁeld of NLP, focuses\non translating text or speech from one language\nto another.\nThe development of MT systems\nhas evolved through four main paradigms: Rule-\nBased MT (RBMT), Statistical MT (SMT), Hy-\nbrid MT (HMT), and Neural MT (NMT).\nRBMT relies on grammatical rules and ex-\ntensive linguistic knowledge, resulting in poor\ntranslation quality and requiring signiﬁcant man-\nual post-editing (Wang et al., 2022a,b).\nSMT,\nwhich uses statistical models to generate transla-\ntions based on bilingual text analysis (Brown et al.,\n1990), faces challenges such as lack of context\nconsideration and reordering issues for distant lan-\nguage pairs (Wang et al., 2022b). HMT combines\nSMT and RBMT, leveraging translation memory\nto improve translation quality (Hunsicker et al.,\n2012; Costa-jussà and Fonollosa, 2015).\nNMT, the current state-of-the-art approach, em-\nploys Neural Networks to enhance translation ac-\ncuracy and speed, enabling training and scal-\ning to new languages (Maazouzi et al., 2017;\nSwathi and Jayashree, 2020; Mohamed et al.,\n2021; Ranathunga et al., 2023). Recent advance-\nments in Large Language Models (LLMs) have\nfurther revolutionized the ﬁeld, with models like\nGPT-4 (OpenAI et al., 2024) and PaLM (Chowdh-\nery et al., 2022) demonstrating great performance\nacross a wide range of NLP tasks, including MT.\n4.1\nPre-training and Fine-tuning\nThe development of powerful pre-trained lan-\nguage models, such as BERT (Devlin et al., 2019),\nGPT (Radford et al., 2018; OpenAI et al., 2024),\nand T5 (Raffel et al., 2020), has transformed the\napproach to NLP tasks. These models enable re-\nsearchers to ﬁne-tune pre-trained models for spe-\nciﬁc applications rather than building new models\nfrom scratch for each task. Pre-training has sig-\nniﬁcantly improved the accuracy of NLP models\nfor various tasks, including language translation,\nsentiment analysis, and text classiﬁcation (Dodda-\npaneni et al., 2021; Sun et al., 2022; Wang et al.,\n2022b). This approach has also facilitated the de-\nvelopment of NLP applications for LRLs, which\noften lack sufﬁcient data for training models from\nscratch. As the technology continues to advance,\npre-trained models are expected to become even\nmore powerful, enabling more accurate and nu-\nanced language analysis (Qiu et al., 2020; Han\net al., 2021; Chowdhery et al., 2022).\n4.2\nState of NLP in Ghanaian Languages\nLike many other African languages, all Ghanaian\nlanguages are considered LRLs. LRLs are charac-\nterized by limited research attention, minimal dig-\nitization, and a lack of privileged status in terms of\nteaching and resources (Magueresse et al., 2020;\nAfram et al., 2022).\nThe striking disparity be-\ntween the linguistic diversity of African languages\nand their inadequate representation in NLP can be\nattributed to factors such as the opaqueness of re-\nsources and the linguistic complexity of these lan-\nguages (Orife et al., 2020).\nDespite ongoing efforts to document and dig-\nitize Ghanaian languages, particularly Twi, most\navailable datasets are either religious texts or ex-\ntremely limited in size. Twi, like all indigenous\nGhanaian languages, is primarily used colloqui-\nally in unofﬁcial settings, resulting in a scarcity\nof digitized resources despite having millions of\nnative speakers. Consequently, making it a LRL.\nThe low-resource nature of all Ghanaian lan-\nguages has far-reaching negative consequences in\nsocial, cultural, and ﬁnancial domains. The lack\nof resources hinders the global Ghanaian dias-\npora from learning their languages and threatens\nthe electronic preservation of these languages and\ntheir associated cultures (Azunre et al., 2021b).\nAdditionally, service providers and health work-\ners face communication obstacles when reaching\nremote areas. The absence of advanced NLP tech-\nnologies, such as text summarization and classi-\nﬁcation, also poses threats to cybersecurity so-\nlutions that could be deployed to protect critical\nsocial and cultural infrastructure (Azunre et al.,\n2021b). Moreover, the scarcity of large, clean, and\nreliable datasets creates a negative feedback loop,\nhindering the advancement of NLP in the region\n(Siminyu et al., 2021).\n5\nSurvey Methodology\nThis study employs a systematic review methodol-\nogy to assess the state of NLP research for Ghana-\nian languages. The survey involves a rigorous pro-\ntocol for identifying, screening, and analyzing rel-\nevant papers from online databases.\n5.1\nSearch Strategy and Data Sources\nTo identify relevant publications, we searched four\nmajor online databases: Google Scholar, Scopus,\nInstitute of Electrical and Electronics Engineers\n(IEEE) Xplore, and Technology Collection. The\nsearch terms used were carefully selected to cap-\nture the core concepts of this study: NLP, MT,\nGhana, Twi, and Language.\nThese terms were\ncombined using boolean operators to ensure com-\nprehensive coverage of the research domain.\n5.2\nScreening and Eligibility Criteria\nIndividual researchers screened the initial search\nresults to determine their relevance to the study.\nPapers focused on NLP or MT applications for\nGhanaian languages were considered eligible. The\nscreening process involved examining each pa-\nper’s title, abstract, and keywords to assess its ﬁt\nwith the study’s objectives.\n5.3\nData Extraction and Synthesis\nWe extracted key information for papers that met\nthe eligibility criteria, such as the research objec-\ntives, methodologies employed, datasets used, and\nsigniﬁcant ﬁndings. The data was synthesized to\nidentify trends, challenges, and opportunities.\n6\nSurvey Results\nOur systematic search yielded 37,018 results\nacross four databases as of March 2024 (Table 2).\nThrough a rigorous screening process, we identi-\nﬁed 25 papers directly relevant to our study, with\nGoogle Scholar contributing the majority (18), fol-\nlowed by Scopus (3), IEEE Xplore (1), and Tech-\nnology Collection (2). To further reﬁne our analy-\nsis, we conducted a group review to eliminate du-\nplicates and unrelated works, resulting in a ﬁnal\nset of 12 papers focusing on NLP for Ghanaian\nlanguages (Table 3).\nIn the following sections, we present a summary\nof our ﬁndings, categorized by datasets, models,\nand performance. For each category, we discuss\nthe various approaches employed by the reviewed\npapers, providing a comprehensive overview of\nthe current state of NLP research in the context\nof Ghanaian languages.\n6.1\nData Collection Approaches\nThe reviewed papers employed diverse collec-\ntion approaches to curate datasets for their NLP\ntasks. We categorize these approaches into reli-\ngious texts, crowdsourcing, and web scraping.\n6.1.1\nReligious Texts\nFour papers (Adjeisah et al., 2020; Acheampong\nand Sackey, 2021; Yvette et al., 2021; Afram et al.,\n2022) utilized parallel corpora derived from the\nEnglish Bible and its corresponding Twi trans-\nlation.\nAcheampong et al.\n(Acheampong and\nSackey, 2021) extracted 20,000 raw Akan sen-\ntences from the Bible Society of Ghana’s 2012\nversion of YouVersion, which were pre-processed\nand aligned with their English counterparts. This\ndataset was directly employed by Agyei et al.\n(Agyei et al., 2021) for evaluating their MT model.\nAdjeisah et al.\n(Adjeisah et al., 2020) adopted\na more comprehensive approach, scraping multi-\nple Bible versions in various languages, including\nEnglish, French, Chinese, German, Spanish, Twi,\nGreek, and Hebrew, to construct a parallel corpus.\nDatabase\nSearch Terms\nTotal Results\nRelevant Results\nScopus\nMachine Translation AND African Languages\n59\n0\nScopus\nMachine Translation AND Twi\n2\n2\nScopus\nNLP AND Ghanaian Language\n2\n1\nGoogle Scholar\nMachine Translation AND Ghana Languages\n25,600\n9\nGoogle Scholar\nMachine Translation AND Twi\n11,300\n10\nIEEE Xplore\nMachine Translation AND Twi\n1\n1\nTechnology Collection\nMachine Translation AND Ghanaian Language\n54\n2\nTable 2: Results per search per database per search term.\nTwo papers (Azunre et al., 2021c; Hacheme,\n2021) leveraged the JW300 dataset, a parallel cor-\npus spanning 300 languages with approximately\n100,000 sentence pairs per language, derived from\nthe Jehovah’s Witness website. Despite its reli-\ngious origin, this dataset covers a range of do-\nmains (Agi´c and Vuli´c, 2019).\n6.1.2\nCrowdsourcing\nFive papers employed crowdsourcing techniques\n(Azunre et al., 2021b; Acheampong and Sackey,\n2021; Azunre et al., 2021a; Afram et al., 2022;\nGyasi and Schlippe, 2023) for their datasets.\nAzunre et al. (Azunre et al., 2021b,a) and Afram\net al. (Afram et al., 2022) utilized Google Forms\nto collect translations, while Acheampong et al.\n(Acheampong and Sackey, 2021) supplemented\ntheir Bible-derived dataset with 6,000 sentences\ngathered by volunteers from social media plat-\nforms, including WhatsApp, WeChat, and Signal.\n6.1.3\nWeb Scraping\nWeb scraping techniques were applied by three\npapers (Ogueji and Ahia, 2019; Yvette et al.,\n2021; Afram et al., 2022) to construct datasets\nfrom online sources such as news articles, liter-\nary texts, and government documents.\nThe re-\nsearchers employed various web scraping tools,\nincluding Beautiful Soup, SpiderLing, and the\nChrome extension Web Scraper.\n6.2\nModel Architectures and Fine-tuning\nAmong the six papers reviewed for model devel-\nopment, four exclusively employed Transformer-\nbased architectures or their variants, one utilized a\nLong Short-Term Memory (LSTM), and one uti-\nlized both (Table 3).\n6.2.1\nTokenization and Preprocessing\nFour of the six modeling papers (Hacheme, 2021;\nAcheampong and Sackey, 2021; Azunre et al.,\n2021b; Gyasi and Schlippe, 2023) employed Byte-\nPair Encoding (BPE) tokenization techniques,\nwhile the remaining two (Agyei et al., 2021; Ad-\njeisah et al., 2021) tokenized sentences into words.\n6.2.2\nModel Conﬁguration and Training\nAgyei et al. (Agyei et al., 2021) preprocessed in-\nput with an embedding layer and positional en-\ncoding, utilizing a 6-layer encoder with 2-head\nMulti-Head Attention (MHA) and a feed-forward\nlayer of 300 units. The modiﬁed Transformer en-\ncoder was trained on 26,000 sentences (85% train,\n10% validation, 5% test) for 100 epochs using the\nAdam optimizer with a learning rate of 0.0002 and\na batch size of 16.\nHacheme et al. (Hacheme, 2021) trained BPE\ntokenizers with vocabulary sizes of 4,000 and\n10,000 for Fon and Ewe, respectively, and 10,000\nand 6,000 for English and Gbe in the multilingual\nmodel. The Transformer encoder had 6 layers with\nan embedding size of 512, a feed-forward size\nof 2,048, and an 8-head MHA. The models were\ntrained on a subset of 100,000 sentence pairs for\n30 epochs using Adam optimizer with a learning\nrate of 0.0001 and batch sizes of 300 and 400 for\nbilingual and multilingual models, respectively.\nAcheampong et al. (Acheampong and Sackey,\n2021) trained a BPE tokenizer with a vocabulary\nsize of 10,000, using 300-dimensional source and\ntarget embeddings and a hidden layer size of 512.\nThe encoder-decoder was ﬁne-tuned using Adam\noptimizer with a learning rate of 0.0002 and regu-\nlarization techniques to reduce overﬁtting.\nAzunre et al. (Azunre et al., 2021b) and Gyasi et\nal. (Gyasi and Schlippe, 2023) loaded the OPUS-\nMT model using the Hugging Face Transform-\ners package with a BPE tokenizer.\nAzunre et\nal.\ntrained the model on 25,421 sentence pairs\nand evaluated it on 697 crowd-sourced pairs, ﬁne-\ntuning it using the Adam optimizer for 20 epochs.\nGyasi et al. trained the model on 8,566 sentence\npairs and validated it on 1,071 pairs, ﬁne-tuning it\nusing the Adam optimizer with a batch size of 8\nand a learning rate of 0.00002.\nAdjeisah et al. (Adjeisah et al., 2021) employed\na Neural Transformer with 4 encoder-decoder lay-\ners, a batch size of 64, a dropout rate of 0.5, and\na weight constraint of 0.5. The OpenNMT BiL-\nSTM model was trained with a batch size of 32,\nthe Adadelta optimizer, and regularization tech-\nniques to prevent overﬁtting.\n6.3\nModel Evaluation Metrics\nEvaluating the performance of MT models is cru-\ncial for assessing their effectiveness and guiding\nfuture research.\nWhile human evaluation is the\ngold standard, it is time-consuming and difﬁcult\nto quantify.\nTo address these challenges, vari-\nous automatic evaluation metrics have been devel-\noped. The reviewed papers primarily employed\nthree metrics: Bilingual Evaluation Understudy\n(BLEU) (Papineni et al., 2002), Translation Edit\nRate (TER) (Snover et al., 2006), and Character\nn-gram F-score (CHRF) (Popovi´c, 2015).\nBLEU is a widely used metric that calcu-\nlates the similarity between candidate and ref-\nerence translations by computing the precision\nof n-grams.\nIt is computationally inexpensive,\nlanguage-independent, and has shown a high cor-\nrelation with human evaluation.\nConversely, TER measures the minimum num-\nber of edits required to transform the candidate\ntranslation into the reference, normalized by the\naverage reference length. It involves ﬁnding a set\nof shifts to reduce the number of edits and then\ncomputing the minimum edits between the shifted\nhypothesis and the reference.\nCHRF calculates an F-1 score over character n-\ngrams, providing a more nuanced translation qual-\nity assessment.\nIt has demonstrated a superior\ncorrelation with human evaluations compared to\nBLEU and TER (Popovi´c, 2015).\nTable 3 summarizes the reviewed models’\nBLEU, CHRF, and TER scores. The results high-\nlight the variability in performance across differ-\nent language pairs and model architectures.\n7\nDiscussion and Roadmap\nThis section discusses the modeling and data col-\nlection approaches observed in the reviewed pa-\npers, comparing and contrasting their strategies\nand providing recommendations for future work.\n7.1\nModel Selection\nTransformer\nand\nRecurrent\nNeural\nNetwork\n(RNN) architectures dominate NLP for Ghanaian\nlanguages. The Transformer’s multi-head atten-\ntion mechanism enables parallel processing and\nstate-of-the-art performance across a wide range\nof applications (Vaswani et al., 2017). However,\ntraining Transformers on limited data can be chal-\nlenging, especially for LRLs that often lack ex-\ntensive labeled datasets.\nAlso, transfer learn-\ning from high-resource languages may not always\ncapture the linguistic nuances and cultural con-\ntext of LRLs (Salman et al., 2022; Choenni et al.,\n2023). Moreover, Transformers require signiﬁcant\ncomputational resources, which can be a limiting\nfactor for LRL researchers (Tay et al., 2022).\nRNNs, including LSTMs, excel at modeling se-\nquence data and retaining context over extended\nperiods. LSTMs address the exploding or vanish-\ning gradient problem (Greff et al., 2017). How-\never, sequential processing limits parallel process-\ning and increases memory usage, particularly for\nlong sequences (Le et al., 2015; Greff et al., 2017).\nGiven these constraints, the research commu-\nnity must develop specialized model architectures\nfor LRLs, starting with simpler yet robust pre-\ntrained models.\nUntil then, despite their chal-\nlenges, we advocate for adopting Transformers\ndue to their performance and ability to transfer\nknowledge across a broad range of NLP tasks.\n7.2\nModel Evaluation\nEnsuring a robust evaluation process for NLP\nmodels in LRLs requires a combination of auto-\nmatic metrics and human evaluation. Automatic\nmetrics, such as BLEU, TER, and CHRF, provide\ninsights into translation quality aspects like ﬂu-\nency, adequacy, and semantic similarity but should\nbe interpreted cautiously due to the scarcity of\nhigh-quality reference translations in LRLs. To\naddress this, we recommend complementing au-\ntomatic metrics with human evaluation by native\nspeakers and linguists, who can identify subtle er-\nrors, biases, and cultural nuances that automatic\nmetrics may overlook. To mitigate the challenges\nassociated with human evaluation, such as ﬁnan-\ncial and time investments and potential annota-\ntor biases, we propose the following strategies:\n1.\nAssemble a diverse team of human annota-\ntors to minimize dataset biases. 2. Complement\nPapers\nArchitecture\nLanguage Pair\nBLEU\nCHRF\nTER\n(Agyei et al., 2021)\nTransformer\nAkan-English\n0.1296\n-\n-\nTransformer\nEnglish-Akan\n0.1757\n-\n-\n(Hacheme, 2021)\nBidirectional-LSTM\nEnglish-Ewe\n0.357\n0.549\n0.511\nBidirectional-LSTM\nEnglish-Fon\n0.42\n0.541\n0.484\nBidirectional-LSTM\nEnglish-Gbe\n0.412\n0.566\n0.465\n(Acheampong and Sackey, 2021)\nTransformer\nAkan-English\n0.1129\n-\n-\nTransformer\nEnglish-Akan\n0.1666\n-\n-\n(Azunre et al., 2021b)\nTransformer\nEnglish-Twi\n0.720\n-\n-\n(Adjeisah et al., 2021)\nBidirectional-LSTM\nEnglish-Twi\n0.1963\n-\n0.5031\nBidirectional-LSTM\nTwi-English\n0.1948\n-\n0.5068\nTransformer\nEnglish-Twi\n0.1836\n-\n0.5451\nTransformer\nTwi-English\n0.1813\n-\n0.5476\n(Gyasi and Schlippe, 2023)\nTransformer\nTwi-French\n0.36\n-\n-\nTransformer\nFrench-Twi\n0.40\n-\n-\n(Ogueji and Ahia, 2019)\n-\n-\n-\n-\n-\n(Adjeisah et al., 2020)\n-\n-\n-\n-\n-\n(Azunre et al., 2021a)\n-\n-\n-\n-\n-\n(Azunre et al., 2021c)\n-\n-\n-\n-\n-\n(Yvette et al., 2021)\n-\n-\n-\n-\n-\n(Afram et al., 2022)\n-\n-\n-\n-\n-\nTable 3: Summary of the BLEU, CHRF, and TER scores of reviewed models.\nhuman evaluation with crowdsourcing methods to\ngather feedback from a larger and more diverse\ngroup of stakeholders. 3. Develop clear guidelines\nand quality control mechanisms for human eval-\nuation to ensure consistency and reduce individ-\nual biases. 4. Implement a two-stage evaluation\nprocess that ﬁrst uses automatic metrics to iden-\ntify low-quality translations and potential issues,\nfollowed by human annotators reﬁning the trans-\nlations and providing detailed feedback on qual-\nity and cultural appropriateness. 5. Continuously\nmonitor and update the evaluation process based\non feedback and advancements in the ﬁeld.\n7.3\nData Collection\nThe results section demonstrates several data col-\nlection approaches for NLP modeling for Ghana-\nian languages, each with merits and drawbacks.\n7.3.1\nReligious Texts\nReligious texts, particularly the Bible, are our re-\nsearch ﬁndings’ most prevalent data source. How-\never, their ancient origins may render some con-\ntent outdated or misaligned with contemporary\nperspectives.\nHistorical texts also carry inher-\nent biases, especially against marginalized groups\n(McCullagh, 2000; Dev et al., 2022).\nWhen using religious corpora in LRL research,\nwe recommend the following:\n1.\nFamiliarize oneself with the corpus’s his-\ntorical, cultural, and linguistic nuances to mini-\nmize misinterpretation or over-generalization. 2.\nAugment the religious corpus with contemporary\ntexts or other religious writings to ensure a more\nholistic and less biased dataset.\n3.\nImplement\nbias detection and mitigation techniques to avoid\npropagating historical prejudices. 4. Engage with\ntheologians, linguists, and community representa-\ntives for insights and validation of interpretations.\n5. Document and communicate the sources, pro-\ncesses, and potential limitations when using large\nvolumes of religious texts. 6. Regularly validate\nmodels against real-world scenarios and contem-\nporary language use and establish a mechanism for\ncontinuous feedback from users and experts.\n7.3.2\nCrowdsourcing and Web Scraping\nCrowdsourcing and web scraping require more ef-\nfort than religious corpus collection but offer the\npotential for a more diverse dataset and increased\nefﬁciency.\nHowever, assessing data quality and\nvetting the competence of participants or online\nresources can be challenging. Additionally, there\nare logistical and legal issues, such as copyright,\nterms of use, and ethical concerns when collecting\npersonal or sensitive information without consent.\nTo mitigate these challenges, we recommend\nthe following:\n1. Utilize robust preprocessing techniques, ex-\ntensive data validation, and comprehensive docu-\nmentation of data sources. 2. Uphold ethical con-\nsiderations, including obtaining appropriate con-\nsent and ensuring compliance with copyright and\nterms of use agreements. 3. Implement advanced\ndata-cleaning methodologies for handling missing\nor conﬂicting information. 4. Employ strategies\nfor bias detection and mitigation and conduct sen-\nsitivity analyses to assess the impact of external\nmotivations or biases among contributors.\n7.3.3\nOpen-Source Data\nOpen-source data is popular among Ghanaian\nNLP researchers due to its free availability, ac-\ncessibility, and the usual presence of parallel cor-\npora. However, LRLs are often underrepresented\nin open-source datasets, and data quality and con-\nsistency may vary.\nWhen working with open-source data for LRLs,\nwe recommend the following:\n1. Leverage freely available data to reduce costs\nand save time, especially if it is already in a par-\nallel corpus format. 2. Foster community collab-\noration to enhance resource development and en-\nsure ethical usage. 3. Customize and adapt open-\nsource data to suit the speciﬁc needs of the target\nlanguage. 4. Carefully assess data quality, harmo-\nnize inconsistent formats, and respect copyright\nand licensing terms.\n7.4\nFuture Directions\nThe ﬁeld of NLP for Ghanaian languages presents\nnumerous opportunities for future research and de-\nvelopment. Some promising directions include:\n1.\nDeveloping specialized pre-training tech-\nniques and architectures tailored to the unique\ncharacteristics of Ghanaian languages, such as\ntheir morphological complexity and tonal systems.\n2.\nExploring multilingual and cross-lingual ap-\nproaches to leverage resources from related lan-\nguages and facilitate transfer learning.\n3.\nIn-\nvesting in creating high-quality, diverse datasets\nthat cover a wide range of domains and gen-\nres, ensuring better representation of contempo-\nrary language use.\n4.\nDesigning and imple-\nmenting language-speciﬁc evaluation metrics and\nbenchmarks to accurately assess the performance\nof NLP models for Ghanaian languages. 5. Col-\nlaborating with local communities, linguists, and\ndomain experts to incorporate cultural knowledge\nand ensure NLP technologies’ ethical develop-\nment and deployment. 6. Investigating the poten-\ntial of low-resource techniques, such as few-shot\nlearning and unsupervised methods, to mitigate\nthe scarcity of labeled data. LLMs have shown\nsuccess in few-shot learning, suggesting their po-\ntential to power NLP applications for LRLs. 7.\nDeveloping user-friendly tools and platforms to\nfacilitate the adoption and use of NLP technolo-\ngies by non-expert users, such as educators, con-\ntent creators, and translators.\n8.\nInvestigating\nthe societal implications of deploying LLMs for\nGhanaian languages, including accessibility, eq-\nuity, and cultural appropriateness.\n8\nConclusion\nThis study presents the ﬁrst large-scale, region-\nspeciﬁc survey of NLP research dedicated to\nGhanaian languages. By analyzing 12 key publi-\ncations, we reveal the stark underrepresentation of\nGhanaian languages (and LRLs in general) within\nNLP research. Our ﬁndings highlight critical chal-\nlenges and opportunities, paving the way for a\nmore inclusive future in the ﬁeld. The concrete\nrecommendations outlined in this study, spanning\ndata collection to model evaluation, can signiﬁ-\ncantly boost future research efforts for LRLs. Im-\nplementing these recommendations can unlock the\nvast potential of LRLs, preserve cultural heritage,\nand empower speakers in the digital age.\nThis\ncomprehensive survey paves the way for more in-\nclusive AI and NLP research, fostering the devel-\nopment of robust, efﬁcient, and culturally sensitive\ntechnologies for all languages.\nReferences\nKingsley Nketia Acheampong and Nathaniel\nNii Oku Sackey. 2021.\nLanguage Revital-\nization:\nA Benchmark for Akan-to-English\nMachine Translation.\nIn Intelligent Systems\nand Applications, Advances in Intelligent Sys-\ntems and Computing, pages 231–244, Cham.\nSpringer International Publishing.\nGordon Senanu Kwame Adika. 2012. English in\nGhana: Growth, Tensions, and Trends. Inter-\nnational Journal of Language, Translation and\nIntercultural Communication, 1:151–166.\nMichael Adjeisah, Guohua Liu, Richard Nuetey\nNortey, Jinling Song, Khalid Odartey Lamptey,\nand Felix Nana Frimpong. 2020.\nTwi Cor-\npus: A Massively Twi-to-Handful Languages\nParallel Bible Corpus.\nIn 2020 IEEE Intl\nConf\non\nParallel\n&\nDistributed\nProcess-\ning with Applications,\nBig Data & Cloud\nComputing, Sustainable Computing & Com-\nmunications, Social Computing & Networking\n(ISPA/BDCloud/SocialCom/SustainCom),\npages 1043–1049.\nMichael\nAdjeisah,\nGuohua\nLiu,\nDou-\nglas\nOmwenga\nNyabuga,\nRichard\nNuetey\nNortey, and Jinling Song. 2021.\nPseudotext\nInjection and Advance Filtering of Low-Re-\nsource Corpus for Neural Machine Translation.\nComputational Intelligence and Neuroscience,\n2021:1–10.\nGabriel\nKwadwo Afram,\nBenjamin\nAsubam\nWeyori, and Felix Adebayo Adekoya. 2022.\nTWIENG: A Multi-Domain Twi-English Paral-\nlel Corpus for Machine Translation of Twi, a\nLow-Resource African Language.\nŽeljko Agi´c and Ivan Vuli´c. 2019.\nJW300: A\nWide-Coverage Parallel Corpus for Low-Re-\nsource Languages. In Proceedings of the 57th\nAnnual Meeting of the Association for Com-\nputational Linguistics, pages 3204–3210, Flo-\nrence, Italy. Association for Computational Lin-\nguistics.\nEmmanuel Agyei, Xiaoling Zhang, Sophyani Ba-\nnaamwini Yussif, and Bless Lord Y. Agbley.\n2021.\nAkan-English:\nTransformer for Low\nResource Translation.\nIn 2021 18th Interna-\ntional Computer Conference on Wavelet Active\nMedia Technology and Information Processing\n(ICCWAMTIP), pages 256–259.\nISSN: 2576-\n8964.\nJonas\nN.\nAkpanglo-Nartey\nand\nRebecca\nA.\nAkpanglo-Nartey. 2012.\nSome Endangered\nLanguages of Ghana. American Journal of Lin-\nguistics, 1(2):10–18.\nToluwase Victor Asubiaro and Ebelechukwu Glo-\nria Igwe. 2021. A State-of-the-Art Review of\nNigerian Languages Natural Language Process-\ning Research:.\nIn Alice S. Etim, editor, Ad-\nvances in IT Standards and Standardization Re-\nsearch, pages 147–167. IGI Global.\nA. Awedoba. 2009. Attitudes towards instruction\nin the local language – a case study of the per-\nspectives of the ‘small stakeholder’. Research\nReview of the Institute of African Studies.\nPaul Azunre, Salomey Osei, Salomey Addo,\nLawrence\nAsamoah\nAdu-Gyamﬁ,\nStephen\nMoore, Bernard Adabankah, Bernard Opoku,\nClara Asare-Nyarko, Samuel Nyarko, Cynthia\nAmoaba, Esther Dansoa Appiah, Felix Akw-\nerh, Richard Nii Lante Lawson, Joel Budu,\nEmmanuel Debrah, Nana Boateng, Wisdom\nOfori, Edwin Buabeng-Munkoh, Franklin Ad-\njei, Isaac Kojo Essel Ampomah, Joseph Otoo,\nReindorf Borkor,\nStandylove Birago Men-\nsah,\nLucien Mensah,\nMark Amoako Mar-\ncel,\nAnokye Acheampong Amponsah,\nand\nJames Ben Hayfron-Acquah. 2021a. NLP for\nGhanaian Languages. ArXiv:2103.15475 [cs].\nPaul Azunre, Salomey Osei, Salomey Addo,\nLawrence\nAsamoah\nAdu-Gyamﬁ,\nStephen\nMoore, Bernard Adabankah, Bernard Opoku,\nClara Asare-Nyarko, Samuel Nyarko, Cynthia\nAmoaba, Esther Dansoa Appiah, Felix Akw-\nerh, Richard Nii Lante Lawson, Joel Budu,\nEmmanuel Debrah, Nana Boateng, Wisdom\nOfori, Edwin Buabeng-Munkoh, Franklin Ad-\njei, Isaac Kojo Essel Ampomah, Joseph Otoo,\nReindorf Borkor,\nStandylove Birago Men-\nsah,\nLucien Mensah,\nMark Amoako Mar-\ncel,\nAnokye Acheampong Amponsah,\nand\nJames Ben Hayfron-Acquah. 2021b.\nEn-\nglish-Twi Parallel Corpus for Machine Transla-\ntion. ArXiv:2103.15625 [cs].\nPaul Azunre, Salomey Osei, Salomey Addo,\nLawrence\nAsamoah\nAdu-Gyamﬁ,\nStephen\nMoore, Bernard Adabankah, Bernard Opoku,\nClara Asare-Nyarko, Samuel Nyarko, Cyn-\nthia Amoaba, Esther Dansoa Appiah, Felix\nAkwerh,\nRichard Nii Lante Lawson,\nJoel\nBudu,\nEmmanuel Debrah,\nNana Boateng,\nWisdom\nOfori,\nEdwin\nBuabeng-Munkoh,\nFranklin\nAdjei,\nIsaac\nKojo\nEssel\nAm-\npomah,\nJoseph\nOtoo,\nReindorf\nBorkor,\nStandylove Birago Mensah, Lucien Mensah,\nMark Amoako Marcel, Anokye Acheampong\nAmponsah, and James Ben Hayfron-Acquah.\n2021c. Contextual Text Embeddings for Twi.\nArXiv:2103.15963 [cs].\nHerman Batibo. 2005.\nLanguage decline and\ndeath in Africa:\nCauses, consequences and\nchallenges. Multilingual Matters.\nPeter F. Brown, John Cocke, Stephen A. Della\nPietra, Vincent J. Della Pietra, Fredrick Jelinek,\nJohn D. Lafferty, Robert L. Mercer, and Paul S.\nRoossin. 1990.\nA statistical approach to ma-\nchine translation.\nComputational Linguistics,\n16(2):79–85.\nTucker Childs. 2020. Language Endangerment in\nAfrica.\nIn Oxford Research Encyclopedia of\nLinguistics. Oxford University Press.\nRochelle Choenni, Dan Garrette, and Ekaterina\nShutova. 2023.\nCross-Lingual Transfer with\nLanguage-Speciﬁc Subnetworks for Low-Re-\nsource Dependency Parsing.\nComputational\nLinguistics, 49(3):613–641.\nAakanksha Chowdhery, Sharan Narang, Jacob De-\nvlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung,\nCharles Sutton, Sebastian Gehrmann, Parker\nSchuh, Kensen Shi, Sasha Tsvyashchenko,\nJoshua Maynez, Abhishek Rao, Parker Barnes,\nYi Tay, Noam Shazeer, Vinodkumar Prab-\nhakaran, Emily Reif, Nan Du, Ben Hutchin-\nson,\nReiner Pope,\nJames Bradbury,\nJacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng\nYin, Toju Duke, Anselm Levskaya, Sanjay\nGhemawat, Sunipa Dev, Henryk Michalewski,\nXavier Garcia, Vedant Misra, Kevin Robinson,\nLiam Fedus, Denny Zhou, Daphne Ippolito,\nDavid Luan, Hyeontaek Lim, Barret Zoph,\nAlexander Spiridonov, Ryan Sepassi, David\nDohan, Shivani Agrawal, Mark Omernick, An-\ndrew M. Dai, Thanumalayan Sankaranarayana\nPillai, Marie Pellat, Aitor Lewkowycz, Er-\nica Moreira, Rewon Child, Oleksandr Polo-\nzov, Katherine Lee, Zongwei Zhou, Xuezhi\nWang,\nBrennan Saeta,\nMark Diaz,\nOrhan\nFirat,\nMichele Catasta,\nJason Wei,\nKathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav\nPetrov,\nand Noah Fiedel. 2022.\nPaLM:\nScaling Language Modeling with Pathways.\nArXiv:2204.02311 [cs].\nMarta R. Costa-jussà and José A. R. Fonollosa.\n2015.\nLatest trends in hybrid machine trans-\nlation and its applications. Computer Speech &\nLanguage, 32(1):3–10.\nErnest Davis and Joseph S. Agbenyega. 2012.\nLanguage policy and instructional practice di-\nchotomy:\nThe case of primary schools in\nGhana.\nInternational Journal of Educational\nResearch, 53:341–347.\nSunipa Dev, Emily Sheng, Jieyu Zhao, Aubrie\nAmstutz, Jiao Sun, Yu Hou, Mattie Sansev-\nerino, Jiin Kim, Akihiro Nishi, Nanyun Peng,\nand Kai-Wei Chang. 2022.\nOn Measures of\nBiases and Harms in NLP. ArXiv:2108.03362\n[cs].\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019.\nBERT: Pre-train-\ning of Deep Bidirectional Transformers for Lan-\nguage Understanding. ArXiv:1810.04805 [cs].\nSumanth\nDoddapaneni,\nGowtham\nRamesh,\nMitesh M. Khapra,\nAnoop Kunchukuttan,\nand Pratyush Kumar. 2021.\nA Primer on\nPretrained\nMultilingual\nLanguage\nModels.\nArXiv:2107.00676 [cs].\nDavid\nM.\nEberhard,\nGary\nF. Simons,\nand\nCharles D. Fennig. 2024.\nEthnologue: Lan-\nguages of the World.\nCandace Kaleimamoowahinekapu Galla. 2016.\nIndigenous Language Revitalization, Promo-\ntion,\nand Education:\nFunction of Digital\nTechnology.\nComputer Assisted Language\nLearning, 29(7):1137–1151.\nERIC Number:\nEJ1117501.\nGhana Statistical Service. 2023.\nGhana Fact\nSheet.\nKlaus Greff,\nRupesh Kumar Srivastava,\nJan\nKoutník,\nBas R. Steunebrink,\nand Jürgen\nSchmidhuber. 2017.\nLSTM: A Search Space\nOdyssey.\nIEEE Transactions on Neural Net-\nworks and Learning Systems, 28(10):2222–\n2232. ArXiv:1503.04069 [cs].\nFrederick Gyasi and Tim Schlippe. 2023.\nTwi\nMachine Translation. Big Data and Cognitive\nComputing, 7(2):114.\nGilles\nHacheme.\n2021.\nEnglish2Gbe:\nA\nmultilingual machine translation model for\n{Fon/Ewe}Gbe. ArXiv:2112.11482 [cs].\nBarry Haddow, Rachel Bawden, Antonio Vale-\nrio Miceli Barone, Jindˇrich Helcl, and Alexan-\ndra Birch. 2022. Survey of Low-Resource Ma-\nchine Translation. ArXiv:2109.00486 [cs].\nXu Han, Zhengyan Zhang, Ning Ding, Yuxian\nGu, Xiao Liu, Yuqi Huo, Jiezhong Qiu, Yuan\nYao, Ao Zhang, Liang Zhang, Wentao Han,\nMinlie Huang, Qin Jin, Yanyan Lan, Yang Liu,\nZhiyuan Liu, Zhiwu Lu, Xipeng Qiu, Ruihua\nSong, Jie Tang, Ji-Rong Wen, Jinhui Yuan,\nWayne Xin Zhao, and Jun Zhu. 2021.\nPre–\ntrained models: Past, present and future.\nAI\nOpen, 2:225–250.\nSabine Hunsicker, Chen Yu, and Christian Feder-\nmann. 2012. Machine learning for hybrid ma-\nchine translation. In Proceedings of the Seventh\nWorkshop on Statistical Machine Translation,\nWMT ’12, pages 312–316, USA. Association\nfor Computational Linguistics.\nQuoc V. Le, N. Jaitly, and Geoffrey E. Hinton.\n2015.\nA Simple Way to Initialize Recurrent\nNetworks of Rectiﬁed Linear Units. ArXiv.\nZakaria El Maazouzi, Badr Eddine El Moha-\njir, and Mohammed Al Achhab. 2017.\nA\nSYSTEMATIC READING IN STATISTICAL\nTRANSLATION: FROM THE STATISTICAL\nMACHINE TRANSLATION TO THE NEU-\nRAL TRANSLATION MODELS. Journal of\nInformation and Communication Technology,\n16(2):408–441.\nKoena Ronny Mabokela, Turgay Celik, and Mpho\nRaborife. 2023. Multilingual Sentiment Anal-\nysis for Under-Resourced Languages: A Sys-\ntematic Review of the Landscape. IEEE Access,\n11:15996–16020.\nAlexandre Magueresse, Vincent Carles, and Evan\nHeetderks. 2020. Low-resource Languages: A\nReview of Past Work and Future Challenges.\nArXiv:2006.07264 [cs].\nC. Behan McCullagh. 2000. Bias in Historical De-\nscription, Interpretation, and Explanation. His-\ntory and Theory, 39(1):39–66.\nShereen A. Mohamed, Ashraf A. Elsayed, Y. F.\nHassan, and Mohamed A. Abdou. 2021. Neu-\nral machine translation: past, present, and fu-\nture.\nNeural Computing and Applications,\n33(23):15919–15931.\nKelechi Ogueji and Orevaoghene Ahia. 2019.\nPidginUNMT: Unsupervised Neural Machine\nTranslation from West African Pidgin to En-\nglish. ArXiv:1912.03444 [cs].\nOpenAI, Josh Achiam, Steven Adler, Sandhini\nAgarwal, Lama Ahmad, Ilge Akkaya, Flo-\nrencia Leoni Aleman, Diogo Almeida, Janko\nAltenschmidt, Sam Altman, Shyamal Anad-\nkat, Red Avila, Igor Babuschkin, Suchir Bal-\naji, Valerie Balcom, Paul Baltescu, Haiming\nBao, Mohammad Bavarian, Jeff Belgum, Irwan\nBello, Jake Berdine, Gabriel Bernadett-Shapiro,\nChristopher Berner, Lenny Bogdonoff, Oleg\nBoiko, Madelaine Boyd, Anna-Luisa Brakman,\nGreg Brockman, Tim Brooks, Miles Brundage,\nKevin Button, Trevor Cai, Rosie Campbell, An-\ndrew Cann, Brittany Carey, Chelsea Carlson,\nRory Carmichael, Brooke Chan, Che Chang,\nFotis Chantzis, Derek Chen, Sully Chen, Ruby\nChen, Jason Chen, Mark Chen, Ben Chess,\nChester Cho, Casey Chu, Hyung Won Chung,\nDave Cummings, Jeremiah Currier, Yunxing\nDai, Cory Decareaux, Thomas Degry, Noah\nDeutsch, Damien Deville, Arka Dhar, David\nDohan, Steve Dowling, Sheila Dunning, Adrien\nEcoffet, Atty Eleti, Tyna Eloundou, David\nFarhi, Liam Fedus, Niko Felix, Simón Posada\nFishman, Juston Forte, Isabella Fulford, Leo\nGao, Elie Georges, Christian Gibson, Vik Goel,\nTarun Gogineni, Gabriel Goh, Rapha Gontijo-\nLopes, Jonathan Gordon, Morgan Grafstein,\nScott Gray, Ryan Greene, Joshua Gross, Shix-\niang Shane Gu, Yufei Guo, Chris Hallacy,\nJesse Han, Jeff Harris, Yuchen He, Mike\nHeaton, Johannes Heidecke, Chris Hesse, Alan\nHickey, Wade Hickey, Peter Hoeschele, Bran-\ndon Houghton, Kenny Hsu, Shengli Hu, Xin\nHu, Joost Huizinga, Shantanu Jain, Shawn\nJain, Joanne Jang, Angela Jiang, Roger Jiang,\nHaozhun Jin, Denny Jin, Shino Jomoto, Bil-\nlie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz\nKaiser, Ali Kamali, Ingmar Kanitscheider, Ni-\ntish Shirish Keskar, Tabarak Khan, Logan\nKilpatrick, Jong Wook Kim, Christina Kim,\nYongjik Kim, Jan Hendrik Kirchner, Jamie\nKiros, Matt Knight, Daniel Kokotajlo, Łukasz\nKondraciuk, Andrew Kondrich, Aris Konstan-\ntinidis, Kyle Kosic, Gretchen Krueger, Vishal\nKuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan\nLeike, Jade Leung, Daniel Levy, Chak Ming\nLi, Rachel Lim, Molly Lin, Stephanie Lin, Ma-\nteusz Litwin, Theresa Lopez, Ryan Lowe, Pa-\ntricia Lue, Anna Makanju, Kim Malfacini, Sam\nManning, Todor Markov, Yaniv Markovski,\nBianca Martin, Katie Mayer, Andrew Mayne,\nBob McGrew, Scott Mayer McKinney, Chris-\ntine McLeavey, Paul McMillan, Jake McNeil,\nDavid Medina, Aalok Mehta, Jacob Menick,\nLuke\nMetz,\nAndrey\nMishchenko,\nPamela\nMishkin, Vinnie Monaco, Evan Morikawa,\nDaniel Mossing, Tong Mu, Mira Murati, Oleg\nMurk, David Mély, Ashvin Nair, Reiichiro\nNakano, Rajeev Nayak, Arvind Neelakantan,\nRichard Ngo, Hyeonwoo Noh, Long Ouyang,\nCullen O’Keefe, Jakub Pachocki, Alex Paino,\nJoe Palermo,\nAshley Pantuliano,\nGiambat-\ntista Parascandolo, Joel Parish, Emy Parparita,\nAlex Passos, Mikhail Pavlov, Andrew Peng,\nAdam Perelman, Filipe de Avila Belbute Peres,\nMichael Petrov, Henrique Ponde de Oliveira\nPinto, Michael, Pokorny, Michelle Pokrass,\nVitchyr H. Pong, Tolly Powell, Alethea Power,\nBoris Power, Elizabeth Proehl, Raul Puri, Alec\nRadford, Jack Rae, Aditya Ramesh, Cameron\nRaymond, Francis Real, Kendra Rimbach, Carl\nRoss, Bob Rotsted, Henri Roussez, Nick Ry-\nder, Mario Saltarelli, Ted Sanders, Shibani San-\nturkar, Girish Sastry, Heather Schmidt, David\nSchnurr, John Schulman, Daniel Selsam, Kyla\nSheppard, Toki Sherbakov, Jessica Shieh, Sarah\nShoker, Pranav Shyam, Szymon Sidor, Eric\nSigler, Maddie Simens, Jordan Sitkin, Kata-\nrina Slama, Ian Sohl, Benjamin Sokolowsky,\nYang Song, Natalie Staudacher, Felipe Pet-\nroski Such, Natalie Summers, Ilya Sutskever,\nJie Tang, Nikolas Tezak, Madeleine B. Thomp-\nson, Phil Tillet, Amin Tootoonchian, Eliz-\nabeth Tseng, Preston Tuggle, Nick Turley,\nJerry Tworek, Juan Felipe Cerón Uribe, An-\ndrea Vallone, Arun Vijayvergiya, Chelsea Voss,\nCarroll Wainwright, Justin Jay Wang, Alvin\nWang, Ben Wang, Jonathan Ward, Jason Wei,\nC. J. Weinmann, Akila Welihinda, Peter Welin-\nder, Jiayi Weng, Lilian Weng, Matt Wiethoff,\nDave Willner, Clemens Winter, Samuel Wol-\nrich, Hannah Wong, Lauren Workman, Sher-\nwin Wu, Jeff Wu, Michael Wu, Kai Xiao,\nTao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan,\nWojciech Zaremba,\nRowan Zellers,\nChong\nZhang, Marvin Zhang, Shengjia Zhao, Tian-\nhao Zheng, Juntang Zhuang, William Zhuk, and\nBarret Zoph. 2024.\nGPT-4 Technical Report.\nArXiv:2303.08774 [cs].\nIroro Orife, Julia Kreutzer, Blessing Sibanda,\nDaniel Whitenack, Kathleen Siminyu, Laura\nMartinus, Jamiil Toure Ali, Jade Abbott, Vukosi\nMarivate,\nSalomon Kabongo,\nMusie Mer-\nessa, Espoir Murhabazi, Orevaoghene Ahia,\nElan van Biljon, Arshath Ramkilowan, Ade-\nwale Akinfaderin, Alp Öktem, Wole Akin,\nGhollah Kioko, Kevin Degila, Herman Kam-\nper,\nBonaventure\nDossou,\nChris\nEmezue,\nKelechi Ogueji, and Abdallah Bashir. 2020.\nMasakhane – Machine Translation For Africa.\nArXiv:2003.11529 [cs].\nKweku Osam. 2003. An introduction to the verbal\nand multi-verbal system of Akan.\nCharles Owu-Ewie. 2017. Language, Education\nand Linguistic Human Rights in Ghana. Legon\nJournal of the Humanities, 28(2):151–172.\nKishore Papineni, Salim Roukos, Todd Ward, and\nZhu Wei-Jing. 2002.\nBLEU: a Method for\nAutomatic Evaluation of Machine Translation.\n40th Annual Meeting of the Association for\nComputational Linguistics, pages 311–318.\nMaja Popovi´c. 2015. chrF: character n-gram F-s-\ncore for automatic MT evaluation. In Proceed-\nings of the Tenth Workshop on Statistical Ma-\nchine Translation, pages 392–395, Lisbon, Por-\ntugal. Association for Computational Linguis-\ntics.\nXiPeng Qiu, TianXiang Sun, YiGe Xu, YunFan\nShao, Ning Dai, and XuanJing Huang. 2020.\nPre-trained models for natural language pro-\ncessing: A survey. Science China Technolog-\nical Sciences, 63(10):1872–1897.\nAlec Radford, Karthik Narasimhan, Tim Sali-\nmans, and Ilya Sutskever. 2018.\nImproving\nLanguage Understanding by Generative Pre–\nTraining.\nColin Raffel, Noam Shazeer, Adam Roberts,\nKatherine\nLee,\nSharan\nNarang,\nMichael\nMatena, Yanqi Zhou, Wei Li, and Peter J. Liu.\n2020. Exploring the Limits of Transfer Learn-\ning with a Uniﬁed Text-to-Text Transformer.\nArXiv:1910.10683 [cs, stat].\nSurangika Ranathunga, En-Shiun Annie Lee, Mar-\njana Prifti Skenduli, Ravi Shekhar, Mehreen\nAlam, and Rishemjit Kaur. 2023. Neural Ma-\nchine Translation for Low-resource Languages:\nA Survey. ACM Computing Surveys, 55(11):1–\n37.\nHadi Salman, Saachi Jain, Andrew Ilyas, Logan\nEngstrom, Eric Wong, and Aleksander Madry.\n2022.\nWhen does Bias Transfer in Transfer\nLearning? ArXiv:2207.02842 [cs].\nPaul Schachter and Victoria Fromkin. 1968.\nA\nPhonology of Akan: Akuapem, Asante, Fante.\nTechnical report, Textbook Department, Stu-\ndent Store, University of California, Los An-\ngeles, California 90024 ($3.\nERIC Number:\nED022189.\nKathleen Siminyu, Godson Kalipe, Davor Or-\nlic, Jade Abbott, Vukosi Marivate, Sackey\nFreshia,\nPrateek\nSibal,\nBhanu\nNeupane,\nDavid I. Adelani, Amelia Taylor, Jamiil Toure\nALI, Kevin Degila, Momboladji Balogoun,\nThierno Ibrahima DIOP, Davis David, Chayma\nFourati, Hatem Haddad, and Malek Naski.\n2021.\nAI4D – African Language Program.\nArXiv:2104.02516 [cs].\nMatthew Snover, Bonnie Dorr, Rich Schwartz,\nLinnea Micciulla, and John Makhoul. 2006. A\nStudy of Translation Edit Rate with Targeted\nHuman Annotation.\nIn Proceedings of the\n7th Conference of the Association for Machine\nTranslation in the Americas: Technical Papers,\npages 223–231,\nCambridge,\nMassachusetts,\nUSA. Association for Machine Translation in\nthe Americas.\nKaili Sun, Xudong Luo, and Michael Y. Luo.\n2022. A Survey of Pretrained Language Mod-\nels.\nIn Knowledge Science, Engineering and\nManagement, Lecture Notes in Computer Sci-\nence, pages 442–456, Cham. Springer Interna-\ntional Publishing.\nS. Swathi and L. S. Jayashree. 2020.\nMachine\nTranslation Using Deep Learning: A Compar-\nison. In Proceedings of International Confer-\nence on Artiﬁcial Intelligence, Smart Grid and\nSmart City Applications, pages 389–395, Cham.\nSpringer International Publishing.\nYi Tay, Mostafa Dehghani, Jai Gupta, Dara Bahri,\nVamsi Aribandi, Zhen Qin, and Donald Metzler.\n2022. Are Pre-trained Convolutions Better than\nPre-trained Transformers?\nArXiv:2105.03322\n[cs].\nUNESCO. 2010. Atlas of the World’s Languages\nin Danger. Technical report, UNESCO.\nUSAID. 2020.\nGhana. Language of Instruction\nCountry Proﬁle: Ghana. Technical report, US-\nAID.\nAshish Vaswani, Noam Shazeer, Niki Parmar,\nJakob Uszkoreit, Llion Jones, Aidan N. Gomez,\nLukasz Kaiser, and Illia Polosukhin. 2017. At-\ntention Is All You Need.\nArXiv:1706.03762\n[cs].\nHaifeng Wang, Jiwei Li, Hua Wu, Eduard Hovy,\nand Yu Sun. 2022a.\nPre-Trained Language\nModels and Their Applications. Engineering.\nHaifeng Wang, Hua Wu, Zhongjun He, Liang\nHuang, and Kenneth Ward Church. 2022b.\nProgress in Machine Translation. Engineering,\n18:143–153.\nGbedevi Akouyo Yvette, Dr Kevin Zhang, and\nTchaye-Kondi Jude. 2021.\nGELR: A Bilin-\ngual Ewe-English Corpus Building and Evalu-\nation. International Journal of Engineering Re-\nsearch & Technology, 10(8). Publisher: IJERT-\nInternational Journal of Engineering Research\n& Technology.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2024-05-10",
  "updated": "2024-05-10"
}