{
  "id": "http://arxiv.org/abs/2405.07444v1",
  "title": "Motion Keyframe Interpolation for Any Human Skeleton via Temporally Consistent Point Cloud Sampling and Reconstruction",
  "authors": [
    "Clinton Mo",
    "Kun Hu",
    "Chengjiang Long",
    "Dong Yuan",
    "Zhiyong Wang"
  ],
  "abstract": "In the character animation field, modern supervised keyframe interpolation\nmodels have demonstrated exceptional performance in constructing natural human\nmotions from sparse pose definitions. As supervised models, large motion\ndatasets are necessary to facilitate the learning process; however, since\nmotion is represented with fixed hierarchical skeletons, such datasets are\nincompatible for skeletons outside the datasets' native configurations.\nConsequently, the expected availability of a motion dataset for desired\nskeletons severely hinders the feasibility of learned interpolation in\npractice. To combat this limitation, we propose Point Cloud-based Motion\nRepresentation Learning (PC-MRL), an unsupervised approach to enabling\ncross-compatibility between skeletons for motion interpolation learning. PC-MRL\nconsists of a skeleton obfuscation strategy using temporal point cloud\nsampling, and an unsupervised skeleton reconstruction method from point clouds.\nWe devise a temporal point-wise K-nearest neighbors loss for unsupervised\nlearning. Moreover, we propose First-frame Offset Quaternion (FOQ) and Rest\nPose Augmentation (RPA) strategies to overcome necessary limitations of our\nunsupervised point cloud-to-skeletal motion process. Comprehensive experiments\ndemonstrate the effectiveness of PC-MRL in motion interpolation for desired\nskeletons without supervision from native datasets.",
  "text": "Motion Keyframe Interpolation for Any Human\nSkeleton via Temporally Consistent Point Cloud\nSampling and Reconstruction\nClinton Mo1, Kun Hu1, Chengjiang Long2, Dong Yuan1, and Zhiyong Wang1\n1 The University of Sydney, Darlington NSW 2008, Australia\nclmo6615@uni.sydney.edu.au, {kun.hu, zhiyong.wang,\ndong.yuan}@sydney.edu.au\n2 Meta Reality Labs, Burlingame, CA, USA clong1@meta.com\nAbstract. In the character animation field, modern supervised keyframe\ninterpolation models have demonstrated exceptional performance in con-\nstructing natural human motions from sparse pose definitions. As su-\npervised models, large motion datasets are necessary to facilitate the\nlearning process; however, since motion is represented with fixed hierar-\nchical skeletons, such datasets are incompatible for skeletons outside the\ndatasets’ native configurations. Consequently, the expected availability\nof a motion dataset for desired skeletons severely hinders the feasibility\nof learned interpolation in practice. To combat this limitation, we pro-\npose Point Cloud-based Motion Representation Learning (PC-MRL), an\nunsupervised approach to enabling cross-compatibility between skeletons\nfor motion interpolation learning. PC-MRL consists of a skeleton obfus-\ncation strategy using temporal point cloud sampling, and an unsupervised\nskeleton reconstruction method from point clouds. We devise a temporal\npoint-wise K-nearest neighbors loss for unsupervised learning. Moreover,\nwe propose First-frame Offset Quaternion (FOQ) and Rest Pose Aug-\nmentation (RPA) strategies to overcome necessary limitations of our un-\nsupervised point cloud-to-skeletal motion process. Comprehensive exper-\niments demonstrate the effectiveness of PC-MRL in motion interpolation\nfor desired skeletons without supervision from native datasets.\nKeywords: 3D point clouds · Human body motion · Dataset creation\n1\nIntroduction\n3D character animation workflows largely rely on the concepts of keyframing\nwith interpolation, often referred to as the pose-to-pose principle [13]. By defin-\ning key poses at correct timings, algorithms can be employed to generate inter-\nmediate poses, and thereby eliminating the need for defining each frame individ-\nually [5, 40]. Although this keyframe-based workflow is less costly than frame-\nby-frame production, human motion is often complex, and natural depictions\ndemand a significant number of keyframes, especially for realistic motions that\narXiv:2405.07444v1  [cs.CV]  13 May 2024\n2\nC. Mo et al.\n(a) Linear interpolation\n(b) PC-MRL (Our method)\n(c) Ground truth\nFig. 1: Motion interpolation of walking keyframes conducted by our approach.\nadhere to physical properties and constraints. Although motion capture (Mo-\nCap) is frequently employed as an alternative for achieving realistic human mo-\ntions, MoCap often nonetheless necessitates subsequent keyframing to correct\nunwanted motion elements and address other imperfections.\nIn recent years, the interpolation process has been widely studied, particu-\nlarly with the advent of machine learning methods for sequential data. Compared\nto conventional methods such as linear interpolation (LERP), machine learning\nmethods have demonstrated the capability to derive visually natural motions\nfrom sparser keyframe sets [12, 18, 33, 35, 38], which we can observe in Fig. 1.\nData-driven interpolation approaches require large motion capture datasets to\nlearn the necessary motion features for effective synthesis of transitions between\nkeyframes. In the field of 3D motions, this aspect significantly restricts the fea-\nsibility of learned interpolation methods in animation practice, as motion/pose\ndata are tied to specific skeletal configurations and are not cross-compatible.\nThat is, the dataset’s native skeleton, i.e. source skeleton, is structurally differ-\nent and incompatible with the desired skeleton, i.e. target skeleton.\nTherefore, we propose a novel point cloud-based motion representation learn-\ning (PC-MRL) approach, in which skeletal hierarchies are obfuscated by sam-\npling into the point cloud medium, and reconstructed into skeletal motion data\nusing an unsupervised neural network. Unlike raw pose and motion data, the\npoint cloud format is a non-hierarchic representation, and effectively obscures\nany skeletal configuration as displayed in Fig. 2. PC-MRL samples a set of points\naround the rest position of skeletons using a combination of uniform and normal\ndistributions, and geometrically parents each point to an associated bone in or-\nder to effectively represent the associated skeletons’ motion data in a temporally\nconsistent manner. Subsequently, for the reconstruction of motion data into a\ngiven desired skeleton, we propose an unsupervised learning scheme, featuring\na K-nearest neighbors (KNN) loss between cross-skeleton point clouds to opti-\nmize a transformer neural network. The network processes the point cloud-based\nmotion representations and generates the corresponding target skeletal motion\nrepresentations. By obscuring both source and predicted target motion data into\nthe point cloud space, our strategy allows for a direct comparison, utilizing KNN\nto minimize the visual difference between the two point clouds.\nAbbreviated paper title\n3\n(a) Hierarchical skeleton\n(b) Skeletal motion\n(c) Point cloud representation\nFig. 2: Our method leverages point cloud representations (c) to obfuscate the hierar-\nchical dependencies of skeletal motion. Joint colours correspond to ordinal indices in\noriginal motion data representation. Point clouds are coloured by temporal index.\nThe unsupervised nature of our target skeleton reconstruction scheme cre-\nates two main disparities between the expected and predicted motion features.\nFirstly, the exact roll rotation axis of bones cannot be represented in the point\ncloud format. To this end, we introduce First-frame Offset Quaternion (FOQ)\nrepresentations to incorporate relative roll values, which are obtainable from\ntemporally consistent point clouds. This approach is agnostic to absolute roll\nrotations, providing a standardisation mechanism for the resulting motion data\nwithin the context of motion interpolation learning. Second, KNN-based objec-\ntives learn skeletal representations in a geometrically optimal manner, which\ndoes not always reflect the desired skeletal behaviour. This disparity is com-\nmonly observed with smaller skeletal features, such as shoulder and hip bones.\nAs such, we augment training motions using Rest Pose Augmentation (RPA)\nto increase the range of skeletal behaviours that our interpolation model learns.\nExtensive experiments demonstrate the effectiveness of our proposed method,\nachieving performance targets near the level of direct dataset supervision.\nIn summary, this paper presents the following key contributions:\n1. We propose a novel method for achieving unsupervised human motion re-\nconstruction from point cloud data, enabling skeleton-agnostic motion inter-\npolation learning for the first time.\n2. To train motion interpolation models, we formulate first-frame offset quater-\nnions to represent bone rotations with relative roll data, as well as a rest\nposition augmentation strategy to address skeletal configuration variability.\n3. We perform comprehensive experiments to demonstrate our method’s ef-\nfectiveness towards learning motion interpolation, despite the absence of\ndirectly compatible datasets in the training process.\n2\nRelated works\nWe explore existing research on motion interpolation approaches, and motion\ndata modelling methods in general. In addition, our proposed approach bears\nresemblance to unsupervised approaches for the motion re-targeting research\nproblem, and as such, we will also analyse existing methods in this field.\n4\nC. Mo et al.\n2.1\nData-Driven Motion Modelling Methods\nContemporary research in motion data modelling predominantly explore learned\ndata-driven methods, using neural networks to capture, model, and establish\ncorrelations between the motion features of various skeletal configurations [39,\n49,53]. Motion features alone have demonstrated considerable efficacy in cross-\nskeleton imitation using a variety of strategies, including task-driven objectives\n[19,43], diffusion-based generation [25,47,50], and latent feature consistency [1–\n3,7,44]. Latent consistency in particular aims to learn inter-skeleton correlations\nwithout the need for paired datasets, which reduces the barrier of entry for\npractical applications.\nLikewise, motion interpolation strategies have evolved through the use of\nlearning based approaches. The inherent numerical precision of motion data has\ntraditionally posed a challenge for the adoption of the learning approaches, due to\ntheir approximate nature [21,27]. This challenge is further amplified by the tem-\nporally sparse distribution of keyframes. Recently, a number of recurrent neural\nnetwork based methods [17,18,41,51] and transformer-based networks [12,33,35]\nhave demonstrated encouraging interpolation performance for the transition be-\ntween keyframes. Additionally, various other motion modelling methods are ca-\npable for keyframe based interpolation, albeit with limited intermediate frame\nrepresentation capabilities [19,25,43].\nA major constraint inherent in all data-driven motion modelling methods\nis their dependency on datasets featuring specific skeletal configurations. To\naddress this, skeleton-free training strategies have been explored for elemen-\ntary tasks involving 3D characters. These include vertex-based pose transfer\n[7,8,30,45], and point cloud-based human shape reconstruction [23,46]. Notably,\nin these methods, 3D mesh and point cloud coordinates serve as general 3D\ndata representations. Concurrently, other research has significantly minimized\nthe required volume of training data, bringing it down to single example se-\nquences. This reduction has been achieved through the use of patch matching\ntechniques [28] and imitation learning within physics-based simulations [29,37].\nReinforcement learning has facilitated the generation of goal-driven motion with-\nout pre-existing datasets [31, 36, 48]. However, these simulation-based methods\nhave typically been either exceedingly complex to implement in practice, or yield\nresults that are too inflexible and/or prone to errors for animation workflows.\n2.2\nMotion Re-Targeting without Supervision\nConventional approaches in motion re-targeting have been a cornerstone of 3D\nanimation for decades, primarily dedicated to converting motion capture data\ninto 3D skeletal motions [4, 15]. Typically, raw motion capture data is opti-\ncally recorded, yielding primarily spatial information. In contrast, motion data\nis largely rotational in nature [5], often represented in SO(3) space. Therefore, a\nprincipal objective of motion re-targeting solutions has been to effectively bridge\nthese two distinct types of representations. In [14], extensive manually adjustable\nAbbreviated paper title\n5\nconstraints were introduced for flexible re-targeting between skeletons of iden-\ntical topology (i.e., same hierarchy, different bone lengths and rest poses). Key\npractices in this approach, including end-effector matching, joint contacts, and\ninter-skeleton point correlations, are still relevant to current animation pipelines.\nPoint correlations using Inverse Kinematics (IK) have also been proposed as an\nintermediate representation to facilitate the adaptation between topologically\ndistinct skeletons [9,34], albeit with hierarchical pairing limitations.\nIn response, morphologically-independent motion control systems were pro-\nposed using IK joint chains and joint-wise constraints [20,26]. Such systems often\nexpect highly standardised motion skeleton features, and are generally impracti-\ncal for re-targeting motions produced for different control schemes. Additionally,\nthe reliance on Euler angles for extensive classical re-targeting constraints poses\na variety of issues regarding rotational freedom [11], as well as compatibility with\nanimation workflows [32], which predominantly use quaternion-based systems.\nDespite these advancements, the challenge of arbitrary skeleton re-targeting\nfrom motion datasets still remains largely unresolved, limiting any widespread\napplication of data-driven pipelines. Our paper aims to effectively address this\npervasive limitation.\n3\nMethodology\nAs illustrated in Fig. 3, our proposed method consists of two main components:\npoint cloud-to-motion reconstruction and motion keyframe interpolation.\n3.1\nQuaternion-based Motion Representations\nWe first declare a set of notations for representing skeletons. A skeleton S is\ndefined by a tree of bones B. A bone vn ∈B is defined by its bone parent\nvm ∈B, and a head position Hn ∈R3 which denotes the bone’s base offset from\nits parent head Hm. In addition, a tail position Tn ∈R3 helps to define the end\npoint of the bone’s line segment representation, starting from its head position,\nof vn. The root bone v0 has no parent, and thus no head position H0. The set\nof Hn and Tn for each bone vn ∈B make up the rest position of the skeleton.\nNext, we define the properties of motion data. At any given frame t, a skele-\nton’s pose position can be represented as global 3D positions pn,t ∈R3 and\nglobal quaternion rotations qn,t ∈R4 for each bone vn ∈B. For a given motion\nsequence M, only the root bone is given a 3D position in pose position, i.e. p0,t,\nas all other pose positions are derived using Forwards Kinematics (FK [11,24]):\n  \\l a bel { eqn:fk } \\mathbf {p}_{n,t} = \\mathbf {p}_{m,t} + QR(\\mathbf {H}_n, \\mathbf {q}_{m,t}), \n(1)\nwhere QR is the quaternion rotation function, and vm is the parent joint. In\nsummary, a motion sequence M of |M| frames is defined by:\n  \\labe l  {eqn : se q } M  =  \\ {\\mathbf {p}_{0, t} \\cup \\{\\mathbf {q}_{n, t} \\mid \\mathbf {v}_n \\in \\mathcal {B}\\} \\mid 0 \\leq t < |M|\\} \n(2)\n6\nC. Mo et al.\nFig. 3: Illustration of point cloud-based motion representation learning (PC-MRL)\npipeline for keyframe interpolation to any human skeleton. Our unsupervised point\ncloud-to-pose decoding approach (I) first adapts motion features from existing datasets\nto new skeletons. We then employ the adapted motion data to supervise motion inter-\npolation models (II), using FOQ and rest pose augmentations to address remaining\ndiscrepancies between the adapted and expected target skeleton motion patterns.\n3.2\nPoint Cloud Obfuscation and Skeleton Reconstruction\nOur approach is based on the hypothesis that 3D motion sequences can be\nvisually represented by a more universal format: 3D point cloud sequences. By\ngeometrically sampling the volume around a skeleton, point cloud data becomes\na non-hierarchical representation of its pose(s), obfuscating the original skeletal\nconfiguration. Due to this decoupling between the skeleton configuration and\nmotion representation, a successful motion data reconstruction from point cloud\ndata would be the first step to enabling learnable cross-skeleton motion features.\nFormally, for a given source skeleton SA with an associated motion sequence\nMA, and any humanoid skeletal configuration SB, the point cloud obfuscation\nand reconstruction module is to learn a function that can adequately perform\nFSA→SB(MA) = ˆ\nMB, where ˆ\nMB is a visually similar motion adaptation of MA\non skeleton SB, which is an estimation of MB.\nMotion Data Obfuscation with Point Clouds To sample the point cloud\nX, we first generate a set of points u ∈X along the line segments defined by\nthe global head and tail positions for each bone vn. In the bone-local space to\nvn, this would mean sampling u by a uniformly distributed factor α ∼U[0,1].\nSpecifically, α = 0 aligns with the head position of the bone, and α = 1 indicates\nthe tail position. By further sampling u via a normal distribution, our sampling\nstrategy can capture the surrounding volume around the skeleton. Formally, to\nsample u from its associated bone vn within a standard deviation of σ, we have:\n  \\ label {eq\nn:sa m ple}  \\begin  {split} \\mathbf {H}_x &\\sim \\mathcal {N}(\\alpha \\mathbf {T}_n, \\sigma ), \\\\ \\mathbf {p}_{u,t} &= \\mathbf {p}_{n,t} + QR(\\mathbf {H}_x, \\mathbf {q}_{n,t}) \\end {split}\n(3)\nNotably, this sampling process as in Eq. 3 is associated with the skeletal con-\nfiguration only, by which the sampled points are temporally consistent for fur-\nAbbreviated paper title\n7\nther processing with the motion sequence. Specifically, each point maintains a\nconstant position from its associated bone in the bone-local space. The lateral\ndistance of each point from their parent bone segment allows point clouds to\nreact to the bone’s rotational roll axis.\nIn a pure point cloud representation, it is possible to remove the relationships\nbetween bones. However, to clearly distinguish symmetrical body features in\ndifferent skeletons, we introduce and embed body group associations for the\npoints during our reconstruction process. Specifically, every bone is categorised\nby one of these body groups: [spine, left arm, right arm, left leg, right leg]. Each\nsampled point is assigned q group attribute in line with the associated bone.\nThe attribute is represented by a one-hot feature vector gu. To this end, we\ncharacterize u = {pu,t, gu} ∈X.\nSkeleton Reconstruction Given a sampled point cloud representation XA\nfrom SA and MA as input, we aim to train a motion adaptation function FSA→SB,\nwhich is tasked with producing a visually similar skeletal motion ˆ\nMB for SB,\nusing a temporal and set-based neural network. As shown in Fig. 3, to construct\nFSA→SB, we employ a Point Transformer [52] to learn embeddings for the un-\nordered point cloud sets frame-wisely, and a standard temporal transformer to\nprocess the sequence and decode XA into ˆ\nMB.\nTo optimize FSA→SB, our objective function maximises the similarity be-\ntween the sampled point clouds of both MA and ˆ\nMB. XA can be viewed as the\nground truth to XB, which enables an unsupervised strategy for FSA→SB. In de-\ntail, we first sample a point cloud XB from SB based on ˆ\nMB derived by FSA→SB.\nNext, a temporally consistent KNN-Loss objective is devised to minimise\nthe ℓ2 distance between any given point of XB from its K nearest neighbouring\npoints in XA. The K nearest neighbours are determined based on inter-point\ndistance throughout the entire motion sequence, instead of a frame-wise setting,\nas shown in Fig. 4. The points with differing body groups are excluded from the\nneighbour search. Mathematically, we have:\n  \\la bel  \n{e\nq n:knn} & \\delta (u\n_A , u _ B) =\n \\be\ngin {cases\n} \\\nsum _{t} ||\\ m\na\nthbf {p}_{u_A, t} \n- \\ma thbf {p}_{u_B, t}||_2, & \\text {if } \\mathbf {g}_{u_A} = \\mathbf {g}_{u_B},\\\\ \\inf , & \\text {otherwise}, \\end {cases}\\\\ &\\mathcal {L}_{\\text {KNN}}(\\mathcal {X}_A, \\mathcal {X}_B) = \\sum _{u_A\\in \\mathcal {X}_A, u_B\\in N_k(u_A, \\mathcal {X}_B)} \\delta (u_A, u_B),\n(5)\nwhere Nk(uA, XB) is the set containing points that are the k-nearest neighbors\nof uA in XB, based on δ(uA, uB) distance.\nIn addition to the KNN-Loss, we introduce an optional end-effector loss in\nthe skeleton space for the hand, foot, and head bones, which provides direct\npositional guidance for matched end-effector pairs EAB:\n  \\l a\nb\nel {eqn:end\nb\none}\n \n\\\nmathcal  {L}_\\text {end} &= \\sum _{(\\mathbf {v}_{A}, \\mathbf {v}_{B})\\in E_{AB}} \\frac {1}{|M_A|} \\sum _{t} ||\\mathbf {p}_{\\mathbf {v}_A, t} - \\mathbf {p}_{\\mathbf {v}_B, t}||_2.\n(6)\n8\nC. Mo et al.\nFig. 4: Visualisation of our temporally consistent KNN objective, processed indepen-\ndently per body group. For each point from XA (a), the total ℓ2 distance throughout\nthe sequence is measured between each point (b) from XB. The ℓ2 distances from the\nK-nearest points are summed to produce KNN-Loss (c).\n3.3\nFirst-Frame Offset Quaternions\nPoint clouds lack absolute roll axis information for each bone. As such, we in-\ntroduce a First-frame Offset Quaternion (FOQ) transform to incorporate the\navailable relative roll data for motion modelling. We formulate all quaternions\nof a motion sequence relative to their values from the initial frame. In detail,\nfor any quaternion sequence Q = {q0, q1, q2, ...}, we can simply obtain its FOQ\ntransform as {q0 × q−1\n0 , q1 × q−1\n0 , q2 × q−1\n0 , ...}, where q−1\n0\nis the quaternion\nconjugate of q0. An FOQ sequence can easily be converted back to a quaternion\nsequence as long as any qt ∈Q is known.\nCrucially, FOQ is a roll-invariant rotation representation, that is, FOQ\nremains identical regardless of a bone’s absolute roll position in the original\ndata. Trivially, to adjust the rotational roll of a bone (e.g., vn), we perform the\nquaternion multiplication for each frame (including the initial frame): qn,t ×\nr, where r is a roll quaternion. As a quaternion on the roll axis of vn, r is\nconstrained by r = α + β(xi + yj + zk), where x,y,z are the 3D XYZ values of\nTn, and α, β are scalars that control the roll magnitude. Note that r is constant\nthroughout the sequence. We can then deduce that for any constant r,\n  (\\m a th b f {q} _ {n, t } \\t i m e s \\ m ath\nbf \n{ r}) \\ti\nmes (\\mathbf {q}_{0, t} \\times \\mathbf {r})^{-1} &= \\mathbf {q}_{n, t} \\times \\mathbf {r} \\times \\mathbf {r}^{-1} \\times \\mathbf {q}_{0, t}^{-1}\\\\ &= \\mathbf {q}_{n, t} \\mathbf {q}_{0, t}^{-1}\nTherefore, FOQ is indeed roll-invariant.\n3.4\nMotion Interpolation Transformer and Rest Pose Augmentation\nTo leverage our point cloud-based representation learning for cross-skeleton in-\nterpolation, a transformer model - CITL [33] is adapted, allowing for an efficient\ninterpolation pipeline with existing datasets to alternative skeletons. We intro-\nduce two key modifications to CITL and its training strategy, focusing on en-\nhancing motion quality. First, quaternion predictions are substituted with FOQ\nAbbreviated paper title\n9\nAlgorithm 1 IK target alignment with maximum real quaternion component\n1: Input: pIK = 3D IK target unit vector\n2:\nT = resting tail position of associated bone\n3: Output: qmax = output quaternion, to be applied on original bone quaternion\n4: η ←\nT\n|T|\n\\triangleright 3D unit coordinates\n[ηx, ηy, ηz]\n5: ω ←\npIK\n|pIK|\n\\triangleright 3D unit coordinates [ωx, ωy, ωz]\n6: qmin ←0+(ηx+ωx)i+(ηy+ωy)j+(ηz+ωz)k\n2\n7: qmax ←qmin × (0 + ηxi + ηyj + ηzk)\npredictions. This is inspired by the RNN-based method TGcomplete [18], which\npredicts quaternion offsets from the preceding frame rather than direct quater-\nnion values, thereby achieving greater generalizability and superior quality in\ninterpolation.\nFor our model’s CITL-based architecture, we substitute the original sinu-\nsoidal positional encoding scheme with learned relative positions [42]. While the\noriginal implementation relies on the continual nature of sinusoidal functions,\nwe observe that relative positional embeddings with zero initialisation converges\nupon similarly continuous behaviour, and significantly lowers the complexity of\npositional attention. The lessened focus on positional relations allows the model\nto synthesise deeper behaviours within the pose space, improving overall gener-\nalisability. As a side effect, the model also accepts arbitrary length inputs beyond\nthe training data length.\nNotably, the intended rest pose of a skeletal configuration is often geometri-\ncally sub-optimal for point cloud matching, and can manifest in multiple forms.\nTo address this, we devise a rest position augmentation strategy, leveraging\na per-bone Inverse Kinematics (IK) system. RPA broadens the range of intended\nrest poses that the learned interpolation model can accommodate. In detail, dur-\ning each phase of the Forward Kinematics (FK) process, we augment the bone\ntails with a random additional offset. We re-align the augmented global tail po-\nsition as closely as possible to its original location using a quaternion rotation\nwith the maximum possible real component. Algorithm 1 describes the method\nby which this quaternion can be derived. Since the real component describes, in\ncosine, the amount of rotation around an axis, this minimizes numerical distur-\nbance from RPA while altering the visual representation to the desired extent.\n4\nExperimental Results\n4.1\nDatasets\nTo thoroughly evaluate the effectiveness of the proposed method, we conducted\nextensive experiments on the following widely used motion capture datasets:\n– LaFAN1 [18] contains long, high quality motions in controllable video game\ncharacter styles. The dataset was recorded on a large motion capture stage,\n10\nC. Mo et al.\nand manually cleaned to production standards. The native skeleton of LaFAN1\ncontains 6 spinal bones, and 4 bones for each limb.\n– Human3.6M [6,22] comprises 3.6 million motion frames. The motions cap-\ntured in Human3.6M tend to be less dynamic, as they were recorded on a\nrelatively compact 4x3m stage. Its native skeleton includes 5 spinal bones, 4\nbones for each limb, and an additional finger bone for each hand.\n– CMU Mocap [10] is a diverse motion dataset encompassing 113 overlapping\ncategories. It features a mix of static and dynamic motions, captured on a\nspacious 3x8m stage. The CMU skeleton is identical to that of Human3.6M,\nwith the exception of the pelvis, which is divided into two symmetrical hip\nbones and a lower back bone.\nFor PC-MRL, we designated the CMU MoCap skeleton as SB. Its native dataset\nis exclusively utilized for testing purposes; while LaFAN1 and Human3.6M are\nused simultaneously for training.\n4.2\nImplementation Details\nIn our experiments, point cloud obfuscation adopted a 256-point setting for\nthe sampling process with σ = 0.05 and each point was characterized by a\n64-dimensional vector. To maintain consistency, we ensure that all bone offsets\nand motion positions are defined in metre units. For skeletal reconstruction,\n4 Point Transformer [52] layers was utilized to construct the neural network.\nWithin each layer, the feature size was doubled, and the size of the point set\nwas downsampled by a factor of 4 through a farthest-first traversal approach.\nThis processing across layers resulted in a singular feature vector. In the final\nstep, two linear layers with ReLU activations were utilized to transform the\nresulted feature vector in a frame-wise manner, producing the output skeletal\nrepresentation. During training, we set K = 8 for the KNN loss to optimize\nthe network. Additionally, the use of unit quaternion values in practice relies on\nmodulus division for constraint, a process that can potentially lead to exploding\ngradients. To address this, we introduced an objective function Lq that measures\nthe ℓ2 norms’ difference between the raw quaternion output and the expected\nunit quaternion. All experiments were trained and evaluated on a single NVIDIA\nGeForce RTX 3090 GPU.\n4.3\nMotion Interpolation Comparison against Supervised Methods\nTo demonstrate the efficacy of our method, we produce and compare our method\nagainst the conventional LERP method, as well as three state-of-the-art interpo-\nlation models trained on the original CMU MoCap dataset,. Specifically, we mea-\nsure the performance of the RNN-based TGcomplete model [18], the BERT-based\nmotion interpolation adaptation [12], and a transformer-based encoder-decoder\napproach [33]. We additionally provide results on a variant of our method trained\nwithout our RPA strategy. The experimental configuration for each state-of-the-\nart model is identical to their original implementations. To measure interpolation\nAbbreviated paper title\n11\nMotion\nMethod\nL2P↓\nL2Q↓\nNPSS↓\ncategory\n5\n15\n30\n5\n15\n30\n5\n15\n30\nBasketball\nLERP\n0.0787\n0.4009\n0.8033\n0.1588\n0.5834\n0.9611\n0.1315\n0.6120\n1.8202\nTGcomplete [18]\n0.1050\n0.4419\n0.7104\n0.1859\n0.6439\n0.9303\n0.1600\n0.6579\n1.2136\nBERT [12]\n0.0889\n0.3737\n0.6865\n0.1585\n0.5560\n0.8992\n0.1689\n0.6177\n1.4587\nCITL [33]\n0.0625\n0.2973\n0.5708\n0.1410\n0.4932\n0.8011\n0.1047\n0.4098\n1.0959\nPC-MRL w/o RPA\n0.0767\n0.3534\n0.6715\n0.1505\n0.5431\n0.9049\n0.1364\n0.5596\n1.3471\nPC-MRL (Ours)\n0.0752\n0.3443\n0.6662\n0.1561\n0.5495\n0.9088\n0.1397\n0.5572\n1.3015\nGolf\nLERP\n0.0189\n0.1229\n0.3187\n0.0455\n0.1870\n0.3987\n0.0587\n0.3979\n1.0312\nTGcomplete [18]\n0.0473\n0.2616\n0.4546\n0.0693\n0.3191\n0.5297\n0.0954\n0.5062\n1.1534\nBERT [12]\n0.0276\n0.1159\n0.2967\n0.0612\n0.2082\n0.4258\n0.1099\n0.3906\n0.9255\nCITL [33]\n0.0201\n0.1043\n0.2589\n0.0476\n0.1766\n0.3552\n0.0636\n0.2581\n0.7799\nPC-MRL w/out RPA\n0.0274\n0.1304\n0.3622\n0.0555\n0.2065\n0.4923\n0.0911\n0.3759\n1.2083\nPC-MRL (Ours)\n0.0273\n0.1130\n0.2706\n0.0598\n0.2012\n0.3844\n0.1049\n0.3716\n0.8150\nSwimming\nLERP\n0.0731\n0.3680\n0.7374\n0.1326\n0.5327\n0.9476\n0.2147\n0.9222\n1.6972\nTGcomplete [18]\n0.1292\n0.5796\n0.9141\n0.1845\n0.7420\n1.1199\n0.2660\n0.9716\n1.7698\nBERT [12]\n0.1088\n0.3896\n0.7516\n0.1819\n0.5518\n0.9622\n0.2877\n0.9074\n1.6682\nCITL [33]\n0.0905\n0.3895\n0.7475\n0.1484\n0.5535\n0.9533\n0.2084\n0.8317\n1.6122\nPC-MRL w/o RPA\n0.0722\n0.3754\n0.7459\n0.1345\n0.5442\n0.9594\n0.1944\n0.8731\n1.6608\nPC-MRL (Ours)\n0.0734\n0.3595\n0.7139\n0.1355\n0.5331\n0.9248\n0.1956\n0.8591\n1.6370\nWalking &\nRunning\nLocomotion\nLERP\n0.0455\n0.2505\n0.5527\n0.1120\n0.3997\n0.6233\n0.0655\n0.2682\n0.9071\nTGcomplete [18]\n0.0540\n0.1786\n0.3103\n0.1147\n0.2866\n0.3866\n0.0895\n0.2616\n0.4265\nBERT [12]\n0.0552\n0.1998\n0.3544\n0.1111\n0.3101\n0.4762\n0.0901\n0.2760\n0.6458\nCITL [33]\n0.0328\n0.1034\n0.2427\n0.0942\n0.2102\n0.3384\n0.0526\n0.1568\n0.4080\nPC-MRL w/o RPA\n0.0397\n0.1657\n0.3417\n0.1058\n0.2879\n0.4722\n0.0640\n0.2288\n0.6021\nPC-MRL (Ours)\n0.0377\n0.1392\n0.2848\n0.1064\n0.2778\n0.4281\n0.0658\n0.2129\n0.5098\nTable 1: Performance comparisons for various motion categories, measured in L2P,\nL2Q, and NPSS. All methods except PC-MRL are trained on the original CMU MoCap\ndataset. Each motion contains 128 frames with keyframes placed every 5, 15, or 30\nframes. The top 2 results for each test are highlighted in purple and blue respectively.\naccuracy, we employ the standard ℓ2 positional distance (L2P), ℓ2 quaternion\ndifference (L2Q), and NPSS [16] for visual similarity evaluation [18].\nThe results listed in Table 1 clearly demonstrate our method’s ability to\napproach the accuracy levels exhibited by directly supervised state-of-the-art\nmethods. PC-MRL consistently outperforms both the LERP standard and RNN-\nbased TGcomplete model, particularly in longer keyframe interval scenarios where\nthe quality and depth of learned motion features is most critical and impactful.\nLikewise, the inclusion of RPA also tends to improve the long interval perfor-\nmance of PC-MRL, particularly when precise movements (i.e. golf) or deeper\nmotion features (i.e. locomotion) are expected. Given this, we can observe that\nRPA definitively improves the overall consistency of the PC-MRL method in\nscenarios that may be unseen or less effectively represented by our point cloud-\nto-motion system.\nNative supervision, i.e. with CITL, unsurprisingly produces the most accu-\nrate interpolation model, with a notable exception of swimming motions, where\nour PC-MRL supervision produces even stronger results. We believe the abun-\ndance of low crawl motions, a similar motion to swimming in LaFAN1, provides\nmore meaningful supervision over the original CMU dataset, which conversely\nhas few similar motions once swimming motions are filtered out. On the other\nend of the spectrum, i.e. for high data scenarios such as walking and running\nlocomotion from Fig. 5, all models including our PC-MRL approach observe the\nstrongest improvements for learned methods over the naive LERP method. Both\ncases strongly support our method’s capability to supervise highly intricate mo-\n12\nC. Mo et al.\nFig. 5: Walking keyframe interpolation examples produced by methods from Table 1.\nClose-up layered comparisons against the original motion are provided on the right.\ntion patterns, despite the incompleteness of the point cloud representation and\nreliance on relative rotations and geometric optimisation assumptions.\n4.4\nCross-Skeleton Motion Re-Targeting Experiments\nWe further conduct an experiment on motion re-targeting to directly bench-\nmark our method against the sole existing state-of-the-art method for unpaired\nmotion re-targeting with unrestricted skeletons, i.e. primal skeletons (PS) [1].\nFor direct motion re-targeting evaluation, we utilized the KNN-Loss and end-\neffector loss as metrics, owing to their effective measurement of visual similarity\nand non-sensitivity towards absolute roll axis correctness. Though absolute rolls\nare generally necessary for a complete motion re-targeting solution, this aspect\nis out of scope for our project as our main goal is motion interpolation.\nModel\nLKNN(XA, XB) ↓\nLend ↓\nσ = 0\nσ = 0.01\nσ = 0.05\nσ = 0.1\nσ = 0.2\nOriginal motion\n1.0099\n2.0234\n5.7399\n9.6636\n16.9223\n0\nPrimal skeleton [1]\n8.1109\n8.0353\n9.4206\n12.8291\n20.6332\n9.6990\nPC-MRL (Ours)\n3.8714\n4.1231\n6.7277\n10.3078\n17.5261\n5.6258\nTable 2: Average unpaired LaFAN1 to CMU motion re-targeting performance of the\nprimal skeleton method and our point cloud method. LKNN is measured using 1024-\npoint clouds at K = 8. Independently sampled point clouds from the original LaFAN1\nskeleton are provided as optimal LKNN reference. All values are scaled by ×100.\nAbbreviated paper title\n13\nFig. 6: Jumping motion re-targeting examples produced on the CMU skeleton by the\nprimal skeleton method (middle row) and our method (bottom row). Sampled point\ncloud representations of each motion sequence are provided on the right.\nFig. 7: Re-targeted falling motion from LaFAN1 (top) on the CMU skeleton (middle)\nand a short custom skeleton (bottom), performed by our point cloud-to-motion model.\nTable 2 indicates the superior performance of our method over the state-\nof-the-art PS method in terms of visual similarity. At low σ values, the higher\nrelative LKNN of the primal skeleton method underscores its geometric deviations\nfrom the original motion. In contrast, our method demonstrates significantly\nbetter visual adherence to the original geometry. As σ increases, our method\napproaches near-perfect alignment with the original motion. Figure 6 visually\nindicates this, showing our method follows the original motion closely, while\nthe latent consistency-based primal skeleton method struggles to generalise and\ndecode the skeleton’s internal structures without directly supervised objectives,\nsuch as the end-effector positions provided during training. In addition, Fig 7\ndemonstrates that, like existing re-targeting approaches, our method is able to\nproduce adequate results on disproportionate skeletons.\nSince our approach is largely focused on enabling non-native motion inter-\npolation supervision, we additionally compare our PC-MRL method against a\nCITL model supervised by PS. Due to the suboptimal re-targeting performance\nof PS, models supervised by PS exhibit expectedly poor accuracy, as shown in\nTable 3.\n14\nC. Mo et al.\nMethod\nL2P↓\nL2Q↓\nNPSS↓\n5\n15\n30\n5\n15\n30\n5\n15\n30\nCITL - PS\n0.2043\n0.6332\n1.1524\n0.2318\n0.7428\n1.1442\n0.1122\n0.4276\n1.4622\nCITL - Native\n0.0495\n0.2306\n0.4898\n0.0979\n0.3604\n0.6609\n0.1149\n0.4684\n1.1591\nPC-MRL\n0.0494\n0.2337\n0.4960\n0.1030\n0.3771\n0.6813\n0.1370\n0.5433\n1.1760\nTable 3: Average L2P, L2Q, and NPSS performance of CITL trained on native and\nPS-generated datasets, compared to our method.\n5\nLimitations\nDue to the inherent limitations of the point cloud representation, certain con-\nstraints on the applicability of our point cloud-to-motion method are inevitable.\nPrimarily, the reliance of our point cloud re-targeting method on the relative\nnature of First-frame Offset Quaternions (FOQ) for most motion learning tasks\nnecessitates the presence of at least one known pose. In the absence of a known\npose, relative quaternions are incapable of reconstructing absolute rotational val-\nues, which are essential for motion applications. Secondly, due to our method of\nsampling point clouds with a consistent deviation from each bone segment, the\nresultant representation inevitably obscures finer details. This includes elements\nlike fingers and facial controls, to an extent that is beyond rectification.\nOur proposed objective function relies entirely on geometric segments, ne-\ncessitating that all bones possess a non-zero length. This stipulation is deemed\na reasonable prerequisite for generic human skeletons, as each bone is typically\nresponsible for manipulating a discernible segment of the human body. Due to\ntechnical considerations, the skeletal configurations in each of our datasets in-\ncluded at least one bone of zero length, which we ignored in our experiments in\nfavour of global quaternion rotations of all bones with non-zero length.\nAs a result of these limitations, we refrained from claiming state-of-the-art\nperformance for general motion re-targeting tasks.\n6\nConclusion\nThis paper introduces a novel learning-based motion interpolation method de-\nsigned to enable cross-skeleton compatibility with existing motion datasets. Our\nproposed method PC-MRL employs a process of point cloud obfuscation and\nskeleton reconstruction. The point cloud space represents human motions in a\nnon-hierarchic and skeleton-agnostic manner. It enables a KNN-based objective\nto be optimised without dataset supervision, guiding a neural network to gener-\nate high-quality motion features with any target skeleton. We address the rota-\ntional information loss of our point cloud format by presenting an offset quater-\nnion strategy compatible with concurrent transformer-based models. Through\nextensive experiments, we have demonstrated the efficacy of PC-MRL in per-\nforming motion interpolation without relying on native motion data. Moreover,\nPC-MRL has achieved superior visual similarity metrics in the domain of motion\nre-targeting. We concluded our work by discussing the limitations of PC-MRL.\nAbbreviated paper title\n15\nReferences\n1. Aberman, K., Li, P., Lischinski, D., Sorkine-Hornung, O., Cohen-Or, D., Chen, B.:\nSkeleton-aware networks for deep motion retargeting 39(4), 62–1 (2020) 4, 12\n2. Aberman, K., Wu, R., Lischinski, D., Chen, B., Cohen-Or, D.: Learning character-\nagnostic motion for motion retargeting in 2d 38(4), 1–14 (jul 2019) 4\n3. Annabi, L., Ma, Z., et al.: Unsupervised motion retargeting for human-robot imita-\ntion. In: Companion of the 2024 ACM/IEEE International Conference on Human-\nRobot Interactio. ACM (2024) 4\n4. Bodenheimer, B., Rose, C., Rosenthal, S., Pella, J.: The process of motion capture:\nDealing with the data. In: Computer Animation and Simulation. pp. 3–18. Springer\n(1997) 4\n5. Burtnyk, N., Wein, M.: Computer-generated key-frame animation. Journal of the\nSMPTE 80(3), 149–153 (1971) 1, 4\n6. Catalin Ionescu, Fuxin Li, C.S.: Latent structured models for human pose estima-\ntion. In: International Conference on Computer Vision (2011) 10\n7. Chen, H., Tang, H., Timofte, R., Gool, L.V., Zhao, G.: Lart: Neural correspondence\nlearning with latent regularization transformer for 3d motion transfer. Advances\nin Neural Information Processing Systems 36 (2024) 4\n8. Chen, J., Li, C., Lee, G.H.: Weakly-supervised 3D pose transfer with keypoints.\nIn: ICCV. pp. 15156–15165 (2023) 4\n9. Choi, K.J., Ko, H.S.: Online motion retargetting. The Journal of Visualization and\nComputer Animation 11(5), 223–235 (2000) 5\n10. CMU: Carnegie-mellon university motion capture database (2003), http://mocap.\ncs.cmu.edu/ 10\n11. Dam, E.B., Koch, M., Lillholm, M.: Quaternions, interpolation and animation,\nvol. 2. Citeseer (1998) 5\n12. Duan, Y., Lin, Y., Zou, Z., Yuan, Y., Qian, Z., Zhang, B.: A unified framework for\nreal time motion completion. In: AAAI. vol. 36, pp. 4459–4467 (2022) 2, 4, 10, 11\n13. Frank, T., Johnston, O.: Disney Animation: The Illusion of Life. Abbeville Pub-\nlishing Group, New York (1981) 1\n14. Gleicher, M.: Retargetting motion to new characters. In: Computer Graphics and\nInteractive Techniques. pp. 33–42 (1998) 4\n15. Gleicher, M.: Animation from observation: Motion capture and motion editing\n33(4), 51–54 (1999) 4\n16. Gopalakrishnan, A., Mali, A., Kifer, D., Giles, L., Ororbia, A.G.: A neural temporal\nmodel for human motion prediction. In: CVPR. pp. 12116–12125 (2019) 11\n17. Harvey, F.G., Pal, C.: Recurrent transition networks for character locomotion. In:\nSIGGRAPH Asia 2018 Technical Briefs, pp. 1–4 (2018) 4\n18. Harvey, F.G., Yurick, M., Nowrouzezahrai, D., Pal, C.: Robust motion in-\nbetweening 39(4), 60–1 (2020) 2, 4, 9, 10, 11\n19. He, C., Saito, J., Zachary, J., Rushmeier, H., Zhou, Y.: Nemf: Neural motion fields\nfor kinematic animation 35, 4244–4256 (2022) 4\n20. Hecker, C., Raabe, B., Enslow, R.W., DeWeese, J., Maynard, J., van Prooijen, K.:\nReal-time motion retargeting to highly varied user-created morphologies 27(3),\n1–11 (2008) 5\n21. Hernandez, A., Gall, J., Moreno-Noguer, F.: Human motion prediction via spatio-\ntemporal inpainting. In: ICCV. pp. 7134–7143 (2019) 4\n22. Ionescu, C., Papava, D., Olaru, V., Sminchisescu, C.: Human3.6m: Large scale\ndatasets and predictive methods for 3D human sensing in natural environments.\n16\nC. Mo et al.\nIEEE Transactions on Pattern Analysis and Machine Intelligence 36(7), 1325–1339\n(jul 2014) 10\n23. Jiang, H., Cai, J., Zheng, J.: Skeleton-aware 3D human shape reconstruction from\npoint clouds. In: ICCV. pp. 5431–5441 (2019) 4\n24. Kantor, I.L., Solodovnikov, A.S., Shenitzer, A.: Hypercomplex numbers: an ele-\nmentary introduction to algebras, vol. 302. Springer (1989) 5\n25. Karunratanakul, K., Preechakul, K., Suwajanakorn, S., Tang, S.: Guided motion\ndiffusion for controllable human motion synthesis. In: ICCV. pp. 2151–2162 (2023)\n4\n26. Kulpa, R., Multon, F., Arnaldi, B.: Morphology-independent representation of mo-\ntions for interactive human-like animation. In: Eurographics (2005) 5\n27. Lehrmann, A.M., Gehler, P.V., Nowozin, S.: Efficient nonlinear markov models for\nhuman motion. In: CVPR. pp. 1314–1321 (2014) 4\n28. Li, P., Aberman, K., Zhang, Z., Hanocka, R., Sorkine-Hornung, O.: Ganimator:\nNeural motion synthesis from a single sequence 41(4), 1–12 (2022) 4\n29. Li, Z., Peng, X.B., Abbeel, P., Levine, S., Berseth, G., Sreenath, K.: Reinforce-\nment learning for versatile, dynamic, and robust bipedal locomotion control. arXiv\npreprint arXiv:2401.16889 (2024) 4\n30. Liao, Z., Yang, J., Saito, J., Pons-Moll, G., Zhou, Y.: Skeleton-free pose transfer\nfor stylized 3D characters. In: ECCV. pp. 640–656. Springer (2022) 4\n31. Makoviychuk, V., Wawrzyniak, L., Guo, Y., Lu, M., Storey, K., Macklin, M.,\nHoeller, D., Rudin, N., Allshire, A., Handa, A., et al.: Isaac gym: High performance\ngpu-based physics simulation for robot learning. arXiv preprint arXiv:2108.10470\n(2021) 4\n32. Merry, B., Marais, P., Gain, J.: Animation space: A truly linear framework for\ncharacter animation. ACM Transactions on Graphics (TOG) 25(4), 1400–1423\n(2006) 5\n33. Mo, C.A., Hu, K., Long, C., Wang, Z.: Continuous intermediate token learning with\nimplicit motion manifold for keyframe based motion interpolation. In: CVPR. pp.\n13894–13903 (2023) 2, 4, 8, 10, 11\n34. Monzani, J.S., Baerlocher, P., Boulic, R., Thalmann, D.: Using an intermediate\nskeleton and inverse kinematics for motion retargeting. In: Computer Graphics\nForum. vol. 19, pp. 11–19. Wiley Online Library (2000) 5\n35. Oreshkin, B.N., Valkanas, A., Harvey, F.G., Ménard, L.S., Bocquelet, F., Coates,\nM.J.: Motion inbetweening via deep δ-interpolator (2022) 2, 4\n36. Peng, X.B., Abbeel, P., Levine, S., van de Panne, M.: Deepmimic: Example-guided\ndeep reinforcement learning of physics-based character skills 37(4), 143:1–143:14\n(Jul 2018) 4\n37. Peng, X.B., Ma, Z., Abbeel, P., Levine, S., Kanazawa, A.: Amp: Adversarial motion\npriors for stylized physics-based character control 40(4) (Jul 2021) 4\n38. Qin, J., Zheng, Y., Zhou, K.: Motion in-betweening via two-stage transformers\n41(6), 1–16 (2022) 2\n39. Reda, D., Won, J., Ye, Y., van de Panne, M., Winkler, A.: Physics-based motion re-\ntargeting from sparse inputs. ACM Computer Graphics and Interactive Techniques\n6(3), 1–19 (2023) 4\n40. Reeves, W.T.: Inbetweening for computer animation utilizing moving point con-\nstraints 15(3), 263–269 (1981) 1\n41. Ren, T., Yu, J., Guo, S., Ma, Y., Ouyang, Y., Zeng, Z., Zhang, Y., Qin, Y.: Diverse\nmotion in-betweening from sparse keyframes with dual posture stitching. IEEE\nTransactions on Visualization and Computer Graphics (2024) 4\nAbbreviated paper title\n17\n42. Shaw, P., Uszkoreit, J., Vaswani, A.: Self-attention with relative position represen-\ntations. In: NAACL. Association for Computational Linguistics (2018) 9\n43. Tiwari, G., Antić, D., Lenssen, J.E., Sarafianos, N., Tung, T., Pons-Moll, G.: Pose-\nndf: Modeling human pose manifolds with neural distance fields. In: ECCV. pp.\n572–589. Springer (2022) 4\n44. Villegas, R., Yang, J., Ceylan, D., Lee, H.: Neural kinematic networks for unsu-\npervised motion retargetting. In: CVPR. pp. 8639–8648 (2018) 4\n45. Wang, J., Li, X., Liu, S., De Mello, S., Gallo, O., Wang, X., Kautz, J.: Zero-shot\npose transfer for unrigged stylized 3D characters. In: CVPR. pp. 8704–8714 (2023)\n4\n46. Wang, K., Xie, J., Zhang, G., Liu, L., Yang, J.: Sequential 3D human pose and\nshape estimation from point clouds. In: CVPR. pp. 7275–7284 (2020) 4\n47. Yuan, Y., Song, J., Iqbal, U., Vahdat, A., Kautz, J.: Physdiff: Physics-guided hu-\nman motion diffusion model. In: ICCV. pp. 16010–16021 (2023) 4\n48. Yuan, Y., Wei, S.E., Simon, T., Kitani, K., Saragih, J.: Simpoe: Simulated char-\nacter control for 3D human pose estimation. In: CVPR. pp. 7159–7169 (2021) 4\n49. Zhang, J., Weng, J., Kang, D., Zhao, F., Huang, S., Zhe, X., Bao, L., Shan, Y.,\nWang, J., Tu, Z.: Skinned motion retargeting with residual perception of motion\nsemantics & geometry. In: CVPR. pp. 13864–13872 (2023) 4\n50. Zhang, M., Li, H., Cai, Z., Ren, J., Yang, L., Liu, Z.: Finemogen: Fine-grained\nspatio-temporal motion generation and editing. Advances in Neural Information\nProcessing Systems 36 (2024) 4\n51. Zhang, X., van de Panne, M.: Data-driven autocompletion for keyframe animation.\nIn: ACM SIGGRAPH Conference on Motion, Interaction and Games. pp. 1–11\n(2018) 4\n52. Zhao, H., Jiang, L., Jia, J., Torr, P.H., Koltun, V.: Point transformer. In: ICCV.\npp. 16259–16268 (2021) 7, 10\n53. Zhu, W., Yang, Z., Di, Z., Wu, W., Wang, Y., Loy, C.C.: Mocanet: Motion retar-\ngeting in-the-wild via canonicalization networks. In: AAAI. vol. 36, pp. 3617–3625\n(2022) 4\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2024-05-13",
  "updated": "2024-05-13"
}