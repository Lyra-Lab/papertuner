{
  "id": "http://arxiv.org/abs/2405.09086v1",
  "title": "Chaos-based reinforcement learning with TD3",
  "authors": [
    "Toshitaka Matsuki",
    "Yusuke Sakemi",
    "Kazuyuki Aihara"
  ],
  "abstract": "Chaos-based reinforcement learning (CBRL) is a method in which the agent's\ninternal chaotic dynamics drives exploration. This approach offers a model for\nconsidering how the biological brain can create variability in its behavior and\nlearn in an exploratory manner. At the same time, it is a learning model that\nhas the ability to automatically switch between exploration and exploitation\nmodes and the potential to realize higher explorations that reflect what it has\nlearned so far. However, the learning algorithms in CBRL have not been\nwell-established in previous studies and have yet to incorporate recent\nadvances in reinforcement learning. This study introduced Twin Delayed Deep\nDeterministic Policy Gradients (TD3), which is one of the state-of-the-art deep\nreinforcement learning algorithms that can treat deterministic and continuous\naction spaces, to CBRL. The validation results provide several insights. First,\nTD3 works as a learning algorithm for CBRL in a simple goal-reaching task.\nSecond, CBRL agents with TD3 can autonomously suppress their exploratory\nbehavior as learning progresses and resume exploration when the environment\nchanges. Finally, examining the effect of the agent's chaoticity on learning\nshows that extremely strong chaos negatively impacts the flexible switching\nbetween exploration and exploitation.",
  "text": "Chaos-based reinforcement learning with TD3\nToshitaka Matsukia,∗, Yusuke Sakemib,c, Kazuyuki Aiharab,c\naNational Defense Academy of Japan, Kanagawa, Japan\nbResearch Center for Mathematical Engineering, Chiba Institute of Technology, Narashino, Japan\ncInternational Research Center for Neuro intelligence (WPI-IRCN), The University of Tokyo, Tokyo,\nJapan\nAbstract\nChaos-based reinforcement learning (CBRL) is a method in which the agent’s internal\nchaotic dynamics drives exploration. This approach offers a model for considering how\nthe biological brain can create variability in its behavior and learn in an exploratory\nmanner. At the same time, it is a learning model that has the ability to automatically\nswitch between exploration and exploitation modes and the potential to realize higher\nexplorations that reflect what it has learned so far. However, the learning algorithms\nin CBRL have not been well-established in previous studies and have yet to incorporate\nrecent advances in reinforcement learning. This study introduced Twin Delayed Deep\nDeterministic Policy Gradients (TD3), which is one of the state-of-the-art deep reinforce-\nment learning algorithms that can treat deterministic and continuous action spaces, to\nCBRL. The validation results provide several insights. First, TD3 works as a learning\nalgorithm for CBRL in a simple goal-reaching task. Second, CBRL agents with TD3\ncan autonomously suppress their exploratory behavior as learning progresses and resume\nexploration when the environment changes. Finally, examining the effect of the agent’s\nchaoticity on learning shows that extremely strong chaos negatively impacts the flexible\nswitching between exploration and exploitation.\nKeywords:\nchaos-based reinforcement learning, TD3, echo state network\n1. Introduction\nNeural networks (NNs) have been studied for many years, partially inspired by find-\nings in neuroscience research [1].\nIn recent years, the development of deep learning\ntechniques used to successfully train deep NNs has produced remarkable results in var-\nious machine learning fields. The recent progress in this field began with outstanding\nachievements in the study of image recognition with NNs [2, 3, 4]. Recurrent neural net-\nworks (RNNs) also performed well in time-series processing, such as speech recognition\nand natural language processing [5, 6, 7, 8]. Vaswani et al. proposed the transformer,\nwhich is an innovative model for natural language processing [9]. The transformer has\n∗Corresponding author\nEmail address: t_matsuki@nda.ac.jp (Toshitaka Matsuki)\nPreprint submitted to arxiv\nMay 16, 2024\narXiv:2405.09086v1  [cs.LG]  15 May 2024\nprovided a breakthrough in the capabilities of artificial intelligence (AI) as the underly-\ning technology for Large Language Models (LLMs) [10]. It has also demonstrated high\nperformance in other areas such as image processing and speech recognition [11, 12].\nAI capabilities based on deep learning techniques have developed quickly over the past\ndecade, leading to innovations in diverse fields.\nIn recent years, AI has demonstrated even high-quality creative abilities [13], and\nthe creativity of AI has been attracting much attention [14, 15]. The outputs of LLMs\ncan be made more conservative or more creative by modifying a probability distribution\nparameter called “temperature” [16].\nHistorically, J. McCarthy et al.\ndiscussed the\ndifference between creative thinking and unimaginative competent thinking in a proposal\nfor the Dartmouth Workshop in 1956 [17]. They raised the importance of randomness in\nAI and the need to pursue how randomness can be effectively injected into machines, just\nas the brain does. In reinforcement learning, a learning agent performs exploratory action\ndriven by random numbers to an environment and improves its policy based on feedback\nfrom the environment. Reinforcement learning is inspired by psychological findings and\nthe principle of reinforcement, which states that agents learn from their actions and their\nconsequences. Various studies on reinforcement learning have been conducted over long\nyears [18, 19, 20, 21, 22, 23]. In recent years, research on deep reinforcement learning,\nwhich incorporates deep learning techniques, has become popular, and this approach has\nmade it possible to learn even more difficult tasks [24, 25, 26, 27, 28]. Deep reinforcement\nlearning can also provide effective learning performance in tasks such as Go and video\ngames, where it is difficult to specify the desired behavior designed by human hands\n[29, 30, 31]. Randomness enables AI systems to generate creative outputs and perform\nexploratory learning.\nThis study focuses on the spontaneous and autonomous exploration of organisms.\nOrganisms can act spontaneously and autonomously in environments with diverse dy-\nnamics, and can adapt to the environment based on the experience gained through these\nbehavior [18]. Reinforcement learning agents stochastically explore the environment by\nintroducing random numbers to select actions that deviate from the optimal action de-\ntermined by the current policy. On the other hand, it remains a fascinating unsolved\nproblem to understand how the biological brain behaves in various ways and realizes ex-\nploratory learning. One hypothesis for the essential property of the source of exploration\nis to utilize fluctuations within the brain. Various studies have shown that fluctuations\ncaused by spontaneous background activity in the neural populations vary their responses\n[32]. Briggman et al. demonstrated that decision-making of escape behavior in leeches\ndepends on the activity of specific neurons that fluctuate under the influence of neuronal\npopulation dynamics [33]. Fox et al. measured activity in the human motor cortex with\nfMRI while performing a button-pressing task, and they found a correlation between\nfluctuations in the BOLD signals and behavioral variability [34]. These studies suggest\nthat fluctuations in neural activity influence decision-making and produce a variety of\nbehaviors.\nFreeman pointed out the possibility that the brain uses chaotic dynamics for ex-\nploratory learning [35]. Skarda and Freeman observed the EEG of the rabbit olfactory\nsystem and showed that there are many chaotic attractors in the dynamics of the olfactory\nbulb that are attracted when the rabbit is exposed to the known olfactory conditioned\nstimulus [36].\nThis study has also confirmed that when the rabbit is exposed to an\nunknown odorant, chaotic dynamics that itinerate between existing attractors emerge,\n2\nfollowed by the reorganization of existing attractors and the emergence of new attrac-\ntors corresponding to new stimuli. Freeman argues that the chaotic dynamics of the\nbrain continually generate new patterns of activity necessary to generate new structures\nand that this underlies the ability of trial and error problem solving [35]. Aihara et al.\ndiscovered the chaotic dynamics in the squid giant axon and constructed chaotic neural\nnetwork models based [37, 38, 39], and proposed Chaotic Simulated Annealing effective\nfor combinatorial optimization problems [40]. Hoerzer et al. showed that a reservoir\nnetwork, which fixes the recurrent and input weights, can acquire various functions using\nan exploratory learning algorithm based on random noise [41]. This study examined how\ngeneric neural circuits that resemble a cortical column can acquire and maintain compu-\ntational functions, including memory attractors, through biologically plausible learning\nrules rather than supervised learning. The result suggests that stochasticity (trial-to-\ntrial variability) of neuronal response plays a crucial role in the learning process. It has\nalso been shown that the system’s own chaotic dynamics can drive exploration in this\nlearning process [42, 43]. These studies have implications for understanding how the\nbrain achieves exploratory learning and utilizes chaotic dynamics in the process.\nShibata et al. have proposed chaos-based reinforcement learning (CBRL), which ex-\nploits the internal chaotic dynamics of the system for exploration [44]. This method uses\nan RNN as an agent system and its internal chaotic dynamics as a source of exploration\ncomponents. Shibata et al. hypothesize that the system can acquire transient dynamics\nwith higher functions by developing its dynamics purposive through exploratory learning.\nVarious studies have viewed brain activity as transitive or transient dynamics [45, 46, 47].\nCBRL provides insights from a reinforcement learning perspective on studies of the ex-\nploratory nature and the transient dynamics of the brain. CBRL agents also have the\nadvantage that their exploration can autonomously subside or resume depending on the\nsituation. It has been suggested that CBRL agents can acquire higher exploration strate-\ngies that reflect their learned behavior [48]. CBRL offers a new perspective on both un-\nderstanding the brain’s exploratory learning mechanisms and novel learning algorithms\nfor AI.\nThe algorithms used in CBRL require treating deterministic and continuous action to\neliminate stochastic exploration and human-designed action selection. Due to the neces-\nsity of a reinforcement learning algorithm that can handle deterministic and continuous\nactions without requiring random noise, Shibata et al. proposed causality trace learning\nand used it to train CBRL agents [44, 48, 49, 50]. However, this method has limited\nperformance, and the CBRL algorithm is not yet well established. This study introduces\nthe Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm, which is one\nof the state-of-the-art deep reinforcement learning algorithms designed for handling de-\nterministic and continuous actions. The ability of the agent is evaluated by learning a\nsimple goal task. We also examine how CBRL agents trained using TD3 respond to envi-\nronmental changes. Furthermore, we investigate how the behavior of the agents changes\ndepending on the levels of chaoticity in the model’s dynamics and how this affects their\nlearning performance and ability to adapt to environmental changes.\nThis paper is organized as follows. Section 2 summarizes chaos-based reinforcement\nlearning. Section 3 describes the experimental method. Section 4 presents the results of\nthe experiments. Section 5 summarizes the conclusions of this study and discusses future\nresearch directions.\n3\n(a) Regular reinforcement learning.\n(b) Chaos-based reinforcement learning.\nFig. 1: Chaos-Based Reinforcement Learning (CBRL). (a) Overview of Regular Reinforcement Learning:\nThe agent decides an action, and then stochastic choices or noise based on random numbers affect the\naction and drive exploration.\nThe action changes the agent’s state through the environment.\nThe\nagent improves its policy based on experience gained from the interactions. (b) Overview of CBRL: A\ndynamical system that exhibits chaotic behavior is used as the agent’s system. In CBRL, the agent\nbehaves exploratively due to fluctuations caused by internal dynamics rather than external random\nnumbers.\n2. Chaos-based reinforcement learning\n2.1. Exploration driven by internal chaotic dynamics\nIn the Chaos-based reinforcement learning method, the agent explores using internal\nchaotic dynamics.\nFigure 1 shows an overview of regular reinforcement learning and\nchaos-based reinforcement learning (CBRL). In general, exploration in reinforcement\nlearning is performed probabilistically based on external random numbers. The ϵ-greedy\nmethod selects a random action with a probability of ϵ. The softmax policy selects an\naction stochastically using random numbers from a Boltzmann distribution of the state\naction values. Some algorithms for continuous action spaces explore by adding random\nnoise to the action outputs for exploration purpose [51]. On the other hand, in CBRL,\nexploration is driven by chaotic dynamics within the agent system rather than relying\non external random numbers.\nA structure in which internal dynamics serves as the basis for exploration resources\nis plausible as a model of the biological brain and offers several learning benefits. As\nshown in Fig. 1(b), the source of exploration (i.e., chaotic dynamics) is a property of\nthe agent system itself in CBRL. The properties of the system dynamics can be changed\nby learning. Therefore, the agent has the potential to optimize its exploration strategy\nthrough training. Previous studies have shown that CBRL agents can autonomously\nswitch from an exploration mode to an exploitation mode as learning progresses [44, 52].\nIn the initial training episodes, when learning has not progressed sufficiently, the agent\nbehaves chaotically and exploratively in the environment. As learning progresses, the\nsystem’s dynamics becomes more ordered, and the agent’s exploratory behavior subsides\nautonomously. Additionally, the agent can autonomously resume exploration when the\nenvironment changes its rules and the previously learned behavior is no longer rewarding.\n4\n2.2. Expectation for CBRL\nShibata et al. hypothesized that the key to creating systems with higher exploratory\nbehavior and human-like intelligence is having a dynamical system with rich and spon-\ntaneous activity [44]. The dynamics of brain activity can be considered as a chaotic\nitinerancy [45] or as a process that transits through a series of saddle points [46]. The\nhypothesis expects the system to reconstruct its exploratory chaotic dynamics into tran-\nsient dynamics that is purposeful to maximize rewards.\nSince internal dynamics drives exploration, CBRL agents are also expected to be\nable to optimize exploration itself through learning.\nGoto et al.\ndemonstrated that\nCBRL agents can change motor noise-like exploration into more sophisticated exploration\nthat selects routes to avoid obstacles using an obstacle avoidance task [48, 49]. It is\nexpected that CBRL agents can acquire more advanced exploration capabilities that\neffectively utilize previously acquired behaviors by constructing the agent’s internal state\nand decision-making processes as transitive dynamics such as chaotic itinerancy.\n2.3. Issue of learning algorithm for CBRL\nTo ensure freedom and autonomy in learning, CBRL agents have used algorithms\nthat deal with deterministic and continuous action spaces rather than stochastic and\ndiscrete ones in which action selections are defined heteronomously. In previous studies,\nthe learning algorithm for CBRL agents has been an Actor-Critic, which can handle\ndeterministic and continuous action spaces [44]. However, a classic Actor-Critic method\nthat trains an actor using a correlation between the external exploration noise and the\nresulting change in value cannot be employed for CBRL, which does not use random\nnumber exploration.\nTherefore, causality trace learning, which is similar to Hebbian\nlearning, has been proposed and employed as the training method for the actor-network\n[44, 48, 49, 50].\nThis method generates the teacher signal for the actor-network by\nmultiplying the TD error by the input values stored in a tracing mechanism that changes\nthe input tracing rate according to changes in the neuron’s output. However, this method\nhas many problems, such as the inability to introduce the backpropagation method and\nthe difficulty of learning complex tasks. Therefore, the learning algorithm for CBRL has\nnot yet been sufficiently well-established.\n3. Method\n3.1. TD3\nThis study introduces Twin Delayed Deep Deterministic Policy Gradients (TD3)\n[53], a deep reinforcement learning algorithm that can handle deterministic policy and\ncontinuous action spaces, to CBRL. TD3 is an improved algorithm based on the model-\nfree deep reinforcement learning method called “Deep Deterministic Policy Gradients”\n(DDPG) [51], and is one of the state-of-the-art reinforcement learning algorithms[53].\nIn the following part of this subsection, we first describe the DDPG and then the\nimprovements introduced in TD3. In the DDPG, the agent model consists of an actor-\nnetwork µ(s|θµ) and a critic network Q(s, a|θQ). Here, θµ and θQ are the weight value\nparameters of each network. µ(s|θµ) determines the action output a based on the agent’s\nstate s and Q(s, a|θQ) estimates the state action value from s and a. Target networks\n5\nµ′(s|θµ′) and Q′(s, a|θQ′) are generated for each network with their weights copied and\nare used to generate the teacher signals to stabilize learning.\nThe agent acts in state st at each time t according to the following action output, to\nwhich the external exploration noise ϵa\nt is added as follows:\nat = µ(st|θµ\nt ) + ϵa\nt.\n(1)\nAs a result of the action, the agent receives the reward rt, and the state transitions to\ns′\nt(= st+1). The experience et = (st, at, rt, s′\nt) is stored in the replay buffer B.\nTraining is performed using N minibatch data of ei = (si, ai, ri, s′\ni) randomly sampled\nfrom B every step. Note that i indicates the index number of the samples in the N\nminibatch data. The teacher signal for the Critic network Q for the input data si and\nai is estimated as follows:\nT c\ni = ri + γQ′(s′\ni, µ′(s′\ni|θµ′)|θQ′),\n(2)\nwhere the discount rate 0 ≤γ ≤1 is a hyperparameter that determines the present value\nof future rewards. We then update θQ to minimize the loss function such that\nLQ = 1\nN\nN\nX\ni\n(T c\ni −Q(si, ai|θQ))2.\n(3)\nThe actor-network µ learns with deterministic policy gradient [54] estimated based on\nthe sampled data:\n∇θµJ(θµ) ≈1\nN\nN\nX\ni\n∇µ(si)Q(si, µ(si|θµ)|θQ)∇θµµ(si|θµ),\n(4)\nwhere J = E[Q(si, µ(si))]. Note that θQ is fixed and only θµ is updated for training\nbased on Equation (4). The target network weights θQ′ and θµ′ are updated as follows:\nθQ′\n←\nτθQ + (1 −τ)θQ′,\n(5)\nθµ′\n←\nτθµ + (1 −τ)θµ′,\n(6)\nwhere 0 < τ ≤1 is the constant parameter that determines the update speed of the\ntarget network.\nTD3 introduces three methods to the DDPG algorithm: Clipped Double Q-learning,\nDelayed Policy Updates, and Target Policy Smoothing. Clipped Double Q-learning is a\nmethod to suppress overestimaton of the value by preparing two Critic networks Q1 and\nQ2 and adopting the smaller output value when generating teacher signals. Target Policy\nSmoothing is a technique to suppress the overfitting of Q to inaccurate value estimations\nby adding noise limited between −C and C to the output of µ′ during the generation of\nthe teacher signal. With these techniques, the teacher signal for Q is given by\nT c\ni = ri + γ min\nj=1,2 Q′\nj(s′\ni, µ′(s′\ni|θµ′) + ϵt\ni|θQ′\nj),\nϵt\ni ∼clip(N(0, σ), −C, C).\n(7)\n6\nNote that the learning of µ by equation (4) always uses Q1. Therefore, the training is\nbased on the following gradient\n∇θµJ(θµ) ≈1\nN\nN\nX\ni\n∇µ(si)Q1(si, µ(si|θµ)|θQ1)∇θµµ(si|θµ).\n(8)\nDelayed Policy Updates stabilizes learning by limiting updates of µ, µ′ and Q′ to once\nevery d steps.\n3.2. Reservoir network\nCBRL requires the agent system to have chaotic dynamics. RNNs are dynamical\nsystems and appropriate models for CBRL agents. However, training RNNs with CBRL\npresents a challenge in balancing the formation of convergent dynamics beneficial for\ncompleting the task while maintaining the chaotic dynamics required for exploration.\nTo avoid this problem, we use a reservoir network (RN) that can be trained without\nmodifying the parameters that determine the dynamical properties of the recurrent layer.\nMaass and Jaeger have proposed RNs as a Liquid State Machine (LSM) and an Echo\nState Network (ESN), respectively [55, 56]. In this study, we use an ESN composed\nof rate model neurons.\nThe ESN consists of a recurrent layer called the “reservoir”\nand an output layer called the “readout.” The connection weights in the reservoir are\nrandomly initialized and fixed, and only the weights from the reservoir to the readout\nare trained. The reservoir receives the time-series input and generates an output that\nnonlinearly reflects the spatio-temporal context of the input series. The readout generates\nthe network output by performing a linear combination of the reservoir output and input.\nThe dynamical properties of the ESN can be modified with a single fixed parameter.\nTherefore, it is easy to tune the chaoticity of the system during learning with chaos-\nbased exploration [43]. Note that, in this study, the ESN is not used to process time\nseries data but rather to generate chaotic dynamics in the agent system.\nThe structure of the ESN is shown in Fig. 2. The reservoir has Nx neurons that are\nrecurrently connected with W rec ∈RNx×Nx with probability of p. The reservoir receives\nNi-dimensional inputs through the weight matrix W in ∈RNx×Ni. Then, the reservoir\noutput is computed by the following equation:\nxt = f(gW recxt−1 + W inut),\n(9)\nwhere g is a scaling parameter that scales the strength of the recurrent connections and\nut ∈RNi is the input vector. f(·) = tanh(·) is the activation function applied element-\nwise. Typically, W rec is a sparse and fixed weight matrix initialized with a spectral\nradius of 1 by the following procedure. A random Nx ×Nx matrix W is generated (from\na uniform distribution in this study), and then the elements of W are set to 0 with\nprobability of (1−p). The matrix is normalized by its spectral radius ρ. Thus, the W rec\nis initialized as follows\nW rec = 1\nρW .\n(10)\nThe arbitrary constant g can rescale the spectral radius of W rec. In general, g is usually\nset to g < 1 to fulfill the Echo State Property that requires the reservoir state to depend\non past input series, but the influence of these inputs fades over finite time. A small g\n7\nFig. 2: Echo state network (ESN). An ESN has a reservoir that is a special recurrent layer whose\nrecurrent and input weights are randomly and sparsely connected and fixed. Only the weights of the\noutput layer (indicated by the red arrows) are trained.\ntends to cause rapid decay of input context, while a large g tends to cause a slow decay\nof it. However, g > 1 sometimes achieves better performance, and it is argued that the\nbest value for learning is realized when the reservoir dynamics is around the edge of\nchaos [57, 58]. When g is further increased beyond 1, and the reservoir dynamics crosses\nthe edge of chaos, the reservoir state exhibits chaotic and spontaneous behavior. In this\nstudy, g is set larger than 1 to induce chaotic dynamics, which allows the CBRL agent\nto explore.\nThe network output zt ∈RNo is calculated as\nzt = tanh\n\u0000W out[xt; ut]\n\u0001\n,\n(11)\nwhere W out ∈RNo×(Nx+Ni) is the output weight matrix, No is the output dimension,\nand [·; ·] denotes the concatenation of two column vectors. W out is often fitted using\nalgorithms such as ridge regression in reservoir computing. This study uses a reservoir\nnetwork as an actor-network for CBRL agents.\nSince W out is trained by a slightly\nmodified version of the TD3 algorithm as described in the folloing subsection, W out is\nupdated by using gradient descent with the Adam optimizer [53].\n3.3. TD3-CBRL\nThe following modifications are made to the TD3 algorithm to adapt to the CBRL\napproach. We eliminated the random number exploration noise ϵa and ϵt. Instead of\nexploration by random numbers, we rely on exploration driven spontaneously by chaotic\ndynamics and use an ESN with a larger spectral radius, as shown in Fig. 3(a), for the µ\nnetwork. Here, g = 2.2 unless otherwise mentioned. We also add the reservoir output to\nthe experience stored in the replay buffer. That is, et = (ut, xt, at, rt, ut+1, xt+1) is stored\ninto the replay buffer B. The agent learns without the Back Propagation Through Time\nmethod using the stored u and x as state st. This method has been employed in several\nstudies and efficiently trains deep reinforcement learning agents using ESN [59, 60]. TD3\n8\n(a) TD3-CBRL\n(b) TD3 without external explo-\nration noise\n(c) Regular TD3\nFig. 3: Network Configurations. This study compares three network configurations of agents. (a) A\nTD3-CBRL agent, which uses a chaotic ESN in the actor-network µ. (b) An agent whose actor-network\nis an MLP with a hidden layer consisting of the same number of neurons as the reservoir in (a). In this\ncase, the agent learns without the external random noises ϵt\na, ϵt\ni as added in the regular TD3. (c) An\nagent that has the same structure as (b) but trains using external random noise as in the regular TD3.\nuses random numbers to randomly sample past experiences from the replay buffer. Since\nthis sampling has an exploration-like effect with the random numbers, the replay buffer\nand batch sizes are set to the same value of 64 to eliminate this effect. Therefore, the\ntraining is based on the memory of the past 64 steps of experience. The variant of CBRL\nthat introduces the above modified TD3 algorithm is called TD3-CBRL in this study. In\nthe experiment, we compare the three cases shown in Fig. 3 (a-c) to confirm that the\nESN dynamics contributes to the exploration during learning.\n3.4. Goal task\nThis study uses a goal task to estimate the agent’s learning ability. Figure 4 shows\nthe task outline. The task field is inside the range 0 to 20 on the x-y plane. There\nare 4 initial positions (x, y) = (2, 2), (2, 18), (18, 2), (18, 18) in the environment. At the\nbeginning of an episode, the agent is randomly placed at one of the initial positions, with\nGaussian noise N(0, 1) added to its position in the field. The agent obtains an input u\nbased on its position (xA, yA) and uses it to determine its action outputs −1 ≤ax ≤1\nand −1 ≤ay ≤1. As a result of the action, the agent’s position is updated according to\nthe following equation\nxA\nt+1\n=\nxA\nt + ax\nt ,\n(12)\nyA\nt+1\n=\nyA\nt + ay\nt ,\n(13)\n9\nFig. 4: Goal Task. The blue cylinder represents the agent, whose position is denoted by xA and yA.\nThe field is surrounded by walls, and xA and yA cannot be out of the range from 0 to 20. The agent\ncan move horizontally and vertically within the environment, with a maximum distance of 1 each. The\nyellow circle indicates the goal, and if the agent’s center is within this circle, the agent is rewarded and\nconsidered to have accomplished the task. The 4 red dots indicate the initial positions. At the beginning\nof each episode, the agent starts at one of these coordinates, with a slight noise added to the position.\nwhere the agent cannot leave the task field and its range of movement is limited to\n0 ≤xA ≤20 and 0 ≤xA ≤20. When the agent is at (xA, yA), the input u from the\nenvironment is given by\nu =\n\u0014xA\n20\n1 −xA\n20\nyA\n20\n1 −yA\n20\n1 −DG\n2L\n\u0015⊤\n,\n(14)\nwhere DG is the Euclidean distance between the center of the agent and the goal, and L\nis the length of the diagonal of the task field. This task is episodic, and an episode ends\nwhen the agent either enters the goal circle of radius 2 (i.e., DG < 2) or fails to reach\nthe goal within 200 steps. The agent receives a reward of r = 1 for reaching the goal,\nr = −0.01 for colliding with the wall surrounding the field, and r = 0 for any other step.\nWe also examine a goal change task to estimate the ability to resume exploration and\nre-learn when the environment changes. This task initially places the goal at the initial\nposition G1\np = (15, 10), but changes the position to G2\np = (5, 10) when the number of\nsteps reaches Nc. After the goal change, the agent can no longer receive rewards for the\nbehavior it has learned previously, and it needs to resume exploration and learn to reach\nthe new goal to receive rewards again.\n4. Experiment\n4.1. Conditions\nThe reservoir size was set to Nx = 256 and the recurrent connection probability was\nset to p = 0.1. The input weight matrix Win was sampled from a uniform distribution\nover [−0.5, 0.5]. The critic network is a fully connected Multilayer Perceptron (MLP)\nwith two hidden layers consisting of 32 ReLU nodes, and the output neuron is a linear\nnode. The initial weights were set using the default settings in PyTorch (version 1.13.1\nin this study.). Both the critic Q and the actor µ are trained using the Adam optimizer,\nwith a learning rate of 0.0003.\nWe paused training every Nv steps and verified the\n10\n(a) TD3-CBRL\n(b) TD3 without external exploration noises\n(c) Regular TD3\nFig. 5: Learning curves. The vertical axis shows the average number of steps required to reach the goal\nstarting from the 16 initial positions. The horizontal axis shows the training steps. The black line shows\nthe representative results with a specific random seed, and the blue line and shaded area show the mean\nand standard deviation of the steps from the results of experiments with 100 different random number\nseeds. (a) shows the learning results with TD3-CBRL. (b) shows the learning results with TD3 without\nexternal exploration noises. (c) shows the learning results with regular TD3.\nagent’s behavior starting from 8 initial positions (2, 2), (2, 10), (2, 18), (10, 2), (10, 18),\n(18, 2), (18, 10), (18, 18) and slightly different positions (shifted 0.002 in the x and y axes,\nrespectively). The replay buffer size and batch size were both set to 64. The discount\nrate γ was set to 0.95. The time constant τ of the target network was set to 0.05. We\nset ϵa = 0, ϵt = 0, and the Delayed Policy Updates step to d = 2.\n4.2. Learning result\nThe TD3-CBRL agent learned the goal task in 20000 steps. Figure 5(a) shows the\nlearning curve resulting from the test conducted every Nv = 2000 step. This figure shows\nthat the average number of steps required by the agent to reach the goal decreases as the\nlearning progresses. This result indicates that TD3-CBRL can successfully learn the goal\ntask. Figure 6(a) shows the trajectories of the agent’s movement in the environment for\neach test trial. This figure shows that in the initial stage of learning (0 step), the agent\nacts exploratively in the environment. On the other hand, in the trajectories after 4000\nsteps, the agent is moving toward the goal from each initial position, although it takes\ndetours in some cases. We also see that the exploratory behavior subsides as the learning\nprogresses. It is important to note that no external random noises for exploration were\nadded to the agent’s action output during the learning process. This result indicates that\nthe internal dynamics of the reservoir caused spontaneous exploratory behavior, and as\nlearning progressed, such variability in the output was autonomously suppressed.\n11\n(a) TD3-CBRL\n(b) TD3 without external exploration noises\n(c) Regular TD3\n(d) Regular TD3 (with external exploration noise in the test phase)\nFig. 6: Agent trajectories during the test. Each colored trajectory represents the agent’s behavior from\nthe 8 initial test positions. Each graph in the subfigures shows the test results at 0, 4000, 8000, 12000,\n16000, and 20000 training steps. The random seed is the same as the one used for the representative\nresults in Fig. 5. (a) shows the training results for TD3-CBRL. (b) shows the results of TD3 without\nexploration by random noises. (c) shows the results with regular TD3. (d) shows the behavior of the\nsame agent as in (c) when the exploration by random noises is not stopped during the test.\nAs seen in the purple trajectory where the agent starts from the initial position\n(10, 18) in Fig. 6(a), the agent wandered to the left around the goal before reaching\nit at the 20000 step, even though the agent had already learned to reach the goal in a\nstraight path by the 12000 step. This result seems to be caused by the chaoticity of\nthe agent’s dynamics, which causes its behavior to change sensitively depending on the\ninitial position and parameter changes. Figure 7 (a) shows the results of investigating the\nsensitivity of the agent’s behavior to variations of the initial positions in this case. This\nfigure shows the trajectories of agents starting from slightly different initial positions.\nAt the 0 step, the agent, starting from its original initial position, continues toward the\nwall and ends the episode without reaching the goal. On the other hand, although the\n12\n(a) TD3-CBRL\n(b) TD3 without external exploration noise\nFig. 7: Sensitivity of agent trajectories for initial position. The blue and orange lines show the agent’s\ntrajectory when the agent started from (10, 18) and from a slightly shifted initial position (10.002,\n18.002), respectively. Each graph in the subfigure shows the test results at 0, 4000, 8000, 12000, 16000,\nand 20000 training steps.\nagent starting from the slightly shifted position behaves like the agent starting from\nthe original initial position initially, it leaves the original trajectory after a while and\neventually reaches the goal. This result indicates that in the early phases of learning, a\nslight difference in the initial position can significantly change the agent’s behavior and\neven determine the task’s success or failure. At the 8000 step, the two agents eventually\nreach the goal, but their paths diverge from the middle of the episode. The agent, starting\nfrom the original initial position, explores both the left and right sides before reaching\nthe goal. On the other hand, the agent starting from a slightly different point did not\nexplore the right side of the goal; instead, it went to the goal. Thus, the sensitivity to\nthe initial condition of the TD3-CBRL agent allows it to exhibit both exploration driven\nby the chaoticity and exploitation of its previously learned behavior.\n4.3. Effects by presence or absence of exploration component\nTo confirm that the chaoticity of the model contributes to the agent’s exploration,\nwe evaluated the learning performance when the model does not have chaotic dynamics.\nSpecifically, instead of an actor-network with a chaotic reservoir, we used a Multilayer\nPerceptron (MLP) with one hidden layer consisting of 256 tanh neurons, as shown in Fig.\n3 (b). In this case, the learning rate was readjusted to 0.00003. Figure 5(b) shows the\nlearning curve for the MLP case without exploration random noise. This figure shows\nthat the number of steps to the goal did not decrease and that the agent failed to learn\nthe goal task. Figure 6(b) and Fig. 7 (b) show the agent trajectory during the test phase\nand the sensitivity test under this condition. These figures show that the agent lacked\nexploratory behavior and failed to learn. This result appears to be due to the fact that\nthe learning model did not have the dynamics to generate spontaneous activity, and thus,\nthe exploration did not occur without random noises.\n13\n(a) Regular case (g = 2.2).\n(b) Larger spectral radius case (g = 5).\nFig. 8: Learning curves of goal change task. The definitions of line colors are the same as in Fig. 5.\nTo clarify that the absence of exploration random noise is the direct cause of the\nlearning failure, we verified the case where the MLP is trained with random noise for\nexploration in the same way as in the regular TD3, as shown in Fig. 3 (c). Figure 5(c)\nshows the learning curve under this condition. This figure shows that the number of steps\nto reach the goal decreased and that the agent succeeded in learning the task. Figure\n6(c) shows the trajectories of the agent’s behavior in this validation. This figure shows\nthat the agent can learn the behavior of moving toward the goal due to exploration\nwith random numbers during the learning process.\nNote that during the test phase,\nadding random numbers to the action outputs is stopped. These results indicate that\nthe presence or absence of exploration by random numbers has a significant influence on\nthe success or failure of learning and that the TD3-CBRL agent successfully learns the\ngoal task through exploration driven by the chaotic dynamics of the reservoir.\nComparing (a) and (c) in Fig. 6, the regular TD3 agent can go to the goal in a\nstraighter path than the TD3-CBRL agent. However, this is due to an external interven-\ntion that removes the random noise during the test phase. Figure 6(d) shows the result\nwhen the agent in (c) acts with the exploration random noise during the test phase. If\nthe exploration random noise is not eliminated, the agent cannot reach the goal in a\nstraight path. On the other hand, the TD3-CBRL agent can autonomously reduce the\nvariability driven by its chaoticity as its learning progresses, although there are still cases\nwhere it takes detours depending on its initial position. This is a beneficial property and\na limitation of the CBRL agent.\n4.4. Goal change task.\nPrevious studies have shown that when the environment changes and the agent cannot\nbe rewarded with previously learned behaviors, CBRL agents can autonomously resume\ntheir exploration and learn to adapt to the new environment [44]. These results suggest\nthat CBRL agents have the flexibility to adapt to dynamic and uncertain environments.\n14\nHere, we observed the agent’s response to changing the goal position to test whether\nTD3-CBRL agents can re-learn when the task rule changes.\nIn the goal change task, the goal was initially placed at G1\np = (15, 10), and learning\nwas performed for 20000 steps. Then, at the Nc = 20001 step, the goal position was\nchanged to G2\np = (5, 10), and learning continued in the new environment for another\n30000 steps. The test is conducted every Nv = 2000 step. The learning curve under\nthese conditions is shown in Fig. 8(a). This figure shows that when the goal position\nis changed, the number of steps required to reach the goal, which had decreased during\nlearning in the initial environment, temporarily increases. However, as learning in the\nnew environment progresses, the number of steps decreases again. This result indicates\nthat the agent is adapting to the new environment.\nThe trajectories of the agent in the environment under these conditions are shown in\nFig. 9(a). This figure shows that at the 25000 steps, the agent autonomously resumes its\nexploration with internal chaotic dynamics due to the change in the goal position. After\nthat, the agent gradually adapts its behavior to move toward the new goal.\n4.5. Learning performance and chaoticity\nWe investigated the effect of the chaoticity of the system on learning performance.\nThe chaoticity of the reservoir network can be tuned by changing the spectral radius of\nthe reservoir’s recurrent weight matrix using the parameter g. We changed the value\nof g from 0 to 12 in 0.2 increments and conducted trials with 100 different random\nnumber seeds under each condition to obtain the learning success probability and the\naverage number of steps to reach the goal. Here, success is defined as an event when\nthe agent reaches the goal from all 16 initial positions during the final test. The results\nare shown in Fig. 10(a). This figure shows that learning performance begins to improve\nas g exceeds around 1, and learning becomes successful in most cases when g exceeds\napproximately 2. This result indicates that the chaoticity of the system is essential for\nsuccessful exploration and learning.\nThe results also indicate that the probability of\nsuccess remains high even when the value of g is further increased. In general, there is an\nappropriate range for the parameter g for time series processing in reservoir computing.\nIt can be considered that since this study does not target tasks that require memory and\nthe task is simple, the success probability of learning by TD3-CBRL remains stable even\nwhen g is larger than the typical critical value.\nWe examined how the flexibility of switching between exploration and exploitation\nwas affected by the value of g. The value of g was changed from 0 to 12 in 0.2 increments,\nand a goal change task was learned under 100 different random number seeds to ensure\nstatistical validity. The task settings are the same as in Section 4.4 except for g. The\nresults are shown in Fig. 10(b). This figure shows that performance on the goal change\ntask decreases with an extremely large g. This result suggests that when g is extremely\nlarge, the balance between exploration and exploitation is lost, and the flexibility to\ndeal with environmental changes is reduced. These results indicate that choosing an\nappropriate g value is still essential in CBRL using reservoir networks.\nWe experimented with long-term re-learning to investigate whether a larger g de-\ncreases the model’s re-learning ability or increases the number of steps required for re-\nlearning. Specifically, after changing the goal position at the Nc = 20001 step, the agent\nlearned for 180000 steps in the new environment. The results are shown in Fig. 10(c).\nThis figure shows that long-term learning mitigated the decrease in learning performance\n15\n(a) Regular case (g = 2.2)\n(b) Larger spectral radius case (g = 5)\nFig. 9: Agent trajectories during the test of the goal change task. Each graph in the subfigures shows\nthe test results conducted every 5000 training steps. The definitions of the line colors are the same as\nin Fig. 6.\nwhen g is large. This result indicates that a model with a larger g and stronger chaoticity\nrequires more re-learning steps, consequently making re-learning more difficult.\n4.6. Readout weights and chaoticity\nTo clarify the reason for the increase in steps required for re-learning when g is\nextremely large, we investigated how the acquired readout weights vary with the spectral\nradius g, which adjusts the chaoticity of the reservoir. The inputs for the readout are the\nreservoir output and the agent’s state in the environment. They are received through\n256 and 5 weights, respectively. Figure 11 shows the readout weights after training for\nspectral radius g of 2.2 and 5. The blue dots show the weights from the reservoir, and\nthe red dots show the bypass weights. Comparing these figures, we can see that the\nbypass weights are larger for the case of g = 5 than for the case of g = 2.2, and the\nweights from the reservoir are relatively smaller than the bypass weights.\nFigure 12\nshows how the average of the absolute values of the weights from the reservoir and the\n16\n(a) Goal task.\n(b) Goal change task.\n(c) Goal change task with long-term learning.\nFig. 10: Learning performance with varying spectral radius. The blue line shows the learning success\nprobability, indicated on the left vertical axis. The red line shows the average number of steps required\nto reach the goal, indicated on the right vertical axis. The horizontal axis shows the value of the spectral\nradius. (a) shows the learning results for the goal task. (b) shows the learning results of the goal change\ntask. (c) shows the results of learning over a long period of 18,000 steps after the goal change.\nbypass weights change when the spectral radius is varied. This figure shows that the\nbypass weights increase as the spectral radius increases. This result suggests that the\nagent ignores the reservoir outputs and focuses more on the state inputs directly given\nfrom the environment.\nWe have validated the goal change task with g = 5. Figure 8(b) shows the results of\nthis validation. Comparing this with the result under the setting of g = 2.2 shown in\nFig. 8(a), re-learning convergence was slower when g = 5. Figure 9(b) shows the agent’s\nbehavior in the environment at g = 5. Comparing Fig. 9(a) and (b), we can see that\nin the case of g = 5, the agent continues its exploratory behavior for more steps and\ntakes more steps to shift to an exploitation mode. This slow re-learning seems to be\nbecause more updates are required to modify the larger bypass weights to adapt them\nto the new environment when the spectral radius is large. To clarify this, we analyzed\nthe change in the bypass weights in the re-learning task, and Fig. 13 shows the results.\nComparing the two results, the bypass weight change in the case of g = 5 is significantly\nlarger than in the case of g = 2.2. In the case of g = 5, the bypass weights increased\nto attend to state inputs, ignoring the reservoir’s chaoticity, and thus, more steps are\nrequired. These results confirm that the inflated bypass weights inhibit flexible switching\nbetween exploration and exploitation. These results indicate that an excessive spectral\nradius and agent chaoticity impair the ability of TD3-CBRL agents to properly balance\nexploration and exploitation properly.\n17\n(a) g = 2.2\n(b) g = 5\nFig. 11: Readout weights. Blue dots indicate 256 weights from the reservoir to the readout. The red\ntriangles show the bypass weights through which the state of the environment is given directly to the\nreadout. (a) shows the result when g = 2.2. (b) shows the result when g = 5. The upper and lower\nfigures show the weights given to the action output decision unit in the x-axis and y-axis directions,\nrespectively.\n4.7. Exploration with random number layer\nThe above verification confirmed that learning proceeds to ignore the reservoir’s out-\nput if the reservoir is too chaotic. To verify this result further, we investigated learning\nwhen a random vector of independent and identically distributed random numbers re-\nplaces the reservoir. Specifically, we replaced the reservoir with a uniform random vector\nsampled from [−1, 1] and conducted learning under this condition. In this setting, the\nrandom vector serves as the exploration component but is worthless as information about\nthe agent’s state. Figure 14 shows the results of this experiment. This figure shows that\nthe agent succeeded in learning the task even when a random number vector replaces\nthe reservoir, although the number of steps required for learning tends to increase. The\nlearning results for the readout weights under this situation show that the bypass weights\nare significantly larger than the weights from the random layer. This result seems to be\nbecause the random layer’s output is worthless for accomplishing the task and suggests\nthat a similar phenomenon occurs when the spectral radius of the reservoir is too high.\nWe experimented with a goal change task to verify whether the exploration with a\nrandom layer is flexible enough to adapt to environmental changes. The task settings\nare the same as in Section 4.4. Figure 15 shows the experimental results under these\nconditions. This figure shows that the random layer model failed to learn the goal change\ntask. This result indicates that the model with the random layer cannot flexibly switch\nbetween exploration and exploitation.\n5. Conclusion\nThis study introduced TD3 as a learning algorithm for chaos-based reinforcement\nlearning (CBRL) and revealed several characteristics.\nIt was confirmed that the TD3-CBRL agent can learn the goal task. This suggests\nthat TD3 is a good candidate as a learning algorithm for CBRL, which had not been\nwell-established in previous studies. Although a regular TD3 agent succeeded in learning\nthe goal task, the agent failed to learn it without external exploration noise, despite the\nsimplicity of the task. On the other hand, the TD3-CBRL agent, whose actor model has\n18\n(a) x-axis direction unit\n(b) y-axis direction unit\nFig. 12: Spectral radius and readout weights. The vertical axis shows the absolute average of the learned\nweights across 100 different random seeds. The horizontal axis shows the spectral radius. The upper\ngraph shows the 256 weights from the reservoir to the readout, and the lower graph shows the 5 bypass\nweights. (a) and (b) show the weights of the readout units that determine the travel distance ax, ay in\nthe x and y axes, respectively.\nchaotic dynamics, can learn without exploration with random noises. This comparison\nindicates that the internal chaotic dynamics of the reservoir contributes to the agent’s\nexploratory behavior.\nWe also observed the agent’s adaptation to the environmental change of altering\nthe goal position during the learning process and found that the agent resumed chaotic\nexploration and successfully re-learned. Furthermore, our results indicate that there is\nan appropriate range for the strength of internal chaos for the TD3-CBRL agent to have\nthe flexibility to autonomously switch between exploration and exploitation modes. In\nparticular, it was found that when the spectral radius is too large, the weights from\nthe bypass become excessively large. Then, the inflated weights negatively affect the\nflexibility of switching between exploration and exploitation. These results indicate that\nthe appropriate choice of spectral radius is essential in designing reservoir networks in\nCBRL to achieve a proper balance between exploration and exploitation.\nWe tested the TD3-CBRL agent when the reservoir network, the source of the explo-\nration dynamics, was replaced by a random vector. The results showed that the agent\ncould learn the goal task, but the bypass weights became significantly large, and the\nagent failed to learn the goal change task.\nThese results indicate that the reservoir,\nwhich generates the dynamics in CBRL, does not act just as a mere noise source. While\nthe random vectors do not reflect any external input, the reservoir creates a variety\nof background activities in the system incorporating the input’s influence. This activity\n19\n(a) g = 2.2\n(b) g = 5\nFig. 13: Changes in the weights of the bypass. The vertical axis shows the weights, and the horizontal\naxis shows the number of training steps. (a) and (b) show the results at g = 2.2 and g = 5, respectively.\nThe upper and lower figures show the weights of the readout unit in the x-axis and y-axis directions,\nrespectively.\n(a) Learning curve.\n(b) Readout weights.\n(c) Agent trajectories.\nFig. 14: Result when a random vector is used instead of a reservoir. The definitions of line colors are\nthe same as in Fig. 5, 6, and 11.\n20\n(a) Learning curve.\n(b) Readout weights.\n(c) Agent trajectories.\nFig. 15: Result of the goal change task when a random vector is used instead of a reservoir.\nThe\ndefinitions of the line colors are the same as in Fig. 5, 6, and 11.\nseems to provide the variability necessary for exploration and to be a resource that drives\nnew exploratory behavior when encountering unknown experiences.\nVarious studies have shown that neural populations operate in a critical state [61, 62,\n63, 64]. Reservoir network performance has also been shown to be optimized at the edges\nof chaos [57, 58]. In addition, a study in which ESNs performed chaos-based exploration\nlearning also showed results suggesting that exploration and exploitation are balanced\nat the edges of chaos [43]. In this study, the spectral radius was not optimal around\ng = 1, where the reservoir dynamics is typically on the edge of chaos. One hypothesis for\nexplaining this result is that the spectral radius of the subsystem, the reservoir, needed\nto be larger to place the entire system’s dynamics, including the environment, at the\nedge of chaos. It is important to verify this by focusing on the entire system’s behavior,\nincluding the interaction between the CBRL agent and the environment.\nThis study introduced TD3 to CBRL and validated its ability with a simple task to\ninvestigate learning and re-learning in detail. In previous studies and this one, CBRL\nhas yet to learn difficult benchmark tasks used for deep reinforcement learning research,\nsuch as Atari game tasks and nonlinear control tasks using MUJOCO. In future works,\nimproving the model to learn more difficult tasks is necessary. A wider selection of tasks\nwill allow us to examine CBRL from newer perspectives.\nIntroducing the new reservoir structure proposed to extend its performance is useful\nto improve the learning performance of CBRL. Reservoirs do not perform well with\nhigh-dimensional input such as images. Several studies have proposed methods that use\nuntrained CNNs for feature extraction [65, 66]. Introducing these methods can enable\nCBRL agents to learn tasks with high-dimensional input, such as raw images. Structural\nimprovement that constructs reservoirs in multiple layers [67, 68, 69] and methods that\nreduce the model size by using multi-step reservoir outputs as input to readouts [70] have\n21\nbeen proposed. It is worth verifying the use of these new reservoir techniques to improve\nthe performance of CBRL agents.\nImprovements in learning algorithms are also worth considering. Sensitivity adjust-\nment learning (SAL), which adjusts the chaoticity of neurons based on ”sensitivity,”\nhas been proposed [71]. This method can modify the recurrent weights while keeping the\nchaoticity of the RNN. Using SAL to maintain chaoticity and learning with Backpropaga-\ntion Through Time may allow CBRL agents to learn more difficult tasks. Self-modulated\nreservoir computing (SM-RC) that extends the reservoir network’s ability by dynamically\nchanging the characteristics of the reservoir and attention to the input through a self-\nmodulated function has been proposed [72]. Since SM-RC can adjust its spectral radius,\nit is also expected that CBRL agents with SM-RC can learn to change their chaoticity\nand switch between exploration and exploitation states more dynamically.\nAcknowledgments\nThe author would like to thank Prof. Katsunari Shibata for useful discussions about\nthis research. This work was supported by Moonshot R&D Grant Number JPMJMS2021,\nAMED under Grant Number JP23dm0307009, Institute of AI and Beyond of UTokyo,\nthe International Research Center for Neurointelligence (WPI-IRCN) at The University\nof Tokyo Institutes for Advanced Study (UTIAS), JSPS KAKENHI Grant Numbers\nJP20H05921, JP22K17969, JP22KK0159.\nReferences\n[1] D. Hassabis, D. Kumaran, C. Summerfield, M. Botvinick, Neuroscience-inspired artificial intelli-\ngence, Neuron 95 (2) (2017) 245–258.\n[2] A. Krizhevsky, I. Sutskever, G. E. Hinton, Imagenet classification with deep convolutional neural\nnetworks, in: Advances in neural information processing systems, 2012, pp. 1097–1105.\n[3] K. Simonyan, A. Zisserman, Very deep convolutional networks for large-scale image recognition,\narXiv preprint arXiv:1409.1556.\n[4] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recognition, in: Proceedings of\nthe IEEE conference on computer vision and pattern recognition, 2016, pp. 770–778.\n[5] A. Graves, N. Jaitly, Towards end-to-end speech recognition with recurrent neural networks, in:\nInternational conference on machine learning, 2014, pp. 1764–1772.\n[6] D. Amodei, S. Ananthanarayanan, R. Anubhai, J. Bai, E. Battenberg, C. Case, J. Casper, B. Catan-\nzaro, Q. Cheng, G. Chen, et al., Deep speech 2: End-to-end speech recognition in english and\nmandarin, in: International conference on machine learning, 2016, pp. 173–182.\n[7] K. Cho, B. Van Merri¨enboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, Y. Bengio,\nLearning phrase representations using rnn encoder-decoder for statistical machine translation, arXiv\npreprint arXiv:1406.1078.\n[8] I. Sutskever, O. Vinyals, Q. V. Le, Sequence to sequence learning with neural networks, Advances\nin neural information processing systems 27.\n[9] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,  L. Kaiser, I. Polosukhin,\nAttention is all you need, Advances in neural information processing systems 30.\n[10] J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, Bert: Pre-training of deep bidirectional transformers\nfor language understanding, arXiv preprint arXiv:1810.04805.\n[11] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani,\nM. Minderer, G. Heigold, S. Gelly, et al., An image is worth 16x16 words: Transformers for image\nrecognition at scale, arXiv preprint arXiv:2010.11929.\n[12] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, I. Sutskever, Robust speech recognition\nvia large-scale weak supervision, in: International Conference on Machine Learning, PMLR, 2023,\npp. 28492–28518.\n22\n[13] T. Brooks, B. Peebles, C. Homes, W. DePue, Y. Guo, L. Jing, D. Schnurr, J. Taylor, T. Luhman,\nE. Luhman, C. W. Y. Ng, R. Wang, A. Ramesh, Video generation models as world simulators.\nURL https://openai.com/research/video-generation-models-as-world-simulators\n[14] G. Franceschelli, M. Musolesi, On the creativity of large language models, arXiv preprint\narXiv:2304.00008.\n[15] E. E. Guzik, C. Byrge, C. Gilde, The originality of machines: Ai takes the torrance test, Journal\nof Creativity 33 (3) (2023) 100065.\n[16] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang, J. Zhang, Z. Dong,\net al., A survey of large language models, arXiv preprint arXiv:2303.18223.\n[17] J. McCarthy, M. L. Minsky, N. Rochester, C. E. Shannon, A proposal for the dartmouth summer\nresearch project on artificial intelligence.\n[18] R. S. Sutton, A. G. Barto, Reinforcement learning: An introduction, MIT press, 2018.\n[19] C. W. Anderson, Strategy learning with multilayer connectionist representations, in: Proceedings\nof the Fourth International Workshop on Machine Learning, Elsevier, 1987, pp. 103–114.\n[20] C. W. Anderson, Learning to control an inverted pendulum using neural networks, IEEE Control\nSystems Magazine 9 (3) (1989) 31–37.\n[21] L.-J. Lin, Reinforcement learning for robots using neural networks, Carnegie Mellon University,\n1992.\n[22] G. Tesauro, Temporal difference learning and td-gammon, Communications of the ACM 38 (3)\n(1995) 58–68.\n[23] K. Shibata, T. Kawano, Learning of action generation from raw camera images in a real-world-like\nenvironment by simple coupling of reinforcement learning and a neural network, in: International\nConference on Neural Information Processing, Springer, 2008, pp. 755–762.\n[24] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, M. Riedmiller, Playing\natari with deep reinforcement learning, arXiv preprint arXiv:1312.5602.\n[25] H. Van Hasselt, A. Guez, D. Silver, Deep reinforcement learning with double q-learning, in: Pro-\nceedings of the AAAI conference on artificial intelligence, Vol. 30, 2016.\n[26] Z. Wang, T. Schaul, M. Hessel, H. Hasselt, M. Lanctot, N. Freitas, Dueling network architectures\nfor deep reinforcement learning, in: International conference on machine learning, PMLR, 2016, pp.\n1995–2003.\n[27] D. Horgan, J. Quan, D. Budden, G. Barth-Maron, M. Hessel, H. Van Hasselt, D. Silver, Distributed\nprioritized experience replay, arXiv preprint arXiv:1803.00933.\n[28] S. Kapturowski, G. Ostrovski, J. Quan, R. Munos, W. Dabney, Recurrent experience replay in\ndistributed reinforcement learning, in: International conference on learning representations, 2018.\n[29] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker,\nM. Lai, A. Bolton, et al., Mastering the game of go without human knowledge, nature 550 (7676)\n(2017) 354–359.\n[30] D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanctot, L. Sifre,\nD. Kumaran, T. Graepel, et al., A general reinforcement learning algorithm that masters chess,\nshogi, and go through self-play, Science 362 (6419) (2018) 1140–1144.\n[31] A. P. Badia, B. Piot, S. Kapturowski, P. Sprechmann, A. Vitvitskyi, Z. D. Guo, C. Blundell,\nAgent57: Outperforming the atari human benchmark, in: International conference on machine\nlearning, PMLR, 2020, pp. 507–517.\n[32] A. Fontanini, D. B. Katz, Behavioral states, network states, and sensory response variability, Journal\nof Neurophysiology 100 (3) (2008) 1160–1168.\n[33] K. L. Briggman, H. D. Abarbanel, W. Kristan Jr, Optical imaging of neuronal populations during\ndecision-making, Science 307 (5711) (2005) 896–901.\n[34] M. D. Fox, A. Z. Snyder, J. L. Vincent, M. E. Raichle, Intrinsic fluctuations within cortical systems\naccount for intertrial variability in human behavior, Neuron 56 (1) (2007) 171–184.\n[35] W. J. Freeman, The physiology of perception, Scientific American 264 (2) (1991) 78–87.\n[36] C. A. Skarda, W. J. Freeman, How brains make chaos in order to make sense of the world, Behavioral\nand brain sciences 10 (2) (1987) 161–173.\n[37] K. Aihara, T. Numajiri, G. Matsumoto, M. Kotani, Structures of attractors in periodically forced\nneural oscillators, Physics Letters A 116 (7) (1986) 313–317.\n[38] K. Aihara, T. Takabe, M. Toyoda, Chaotic neural networks, Physics letters A 144 (6-7) (1990)\n333–340.\n[39] M. Adachi, K. Aihara, Associative dynamics in a chaotic neural network, Neural Networks 10 (1)\n(1997) 83–98.\n[40] L. Chen, K. Aihara, Chaotic simulated annealing by a neural network model with transient chaos,\n23\nNeural networks 8 (6) (1995) 915–930.\n[41] M. Hoerzer, Gregor, R. Legenstein, W. Maass, Emergence of complex computational structures\nfrom chaotic neural networks through reward-modulated hebbian learning, Cerebral cortex 24 (3)\n(2012) 677–690.\n[42] T. Matsuki, K. Shibata, Reward-based learning of a memory-required task based on the internal dy-\nnamics of a chaotic neural network, in: International Conference on Neural Information Processing,\nSpringer, 2016, pp. 376–383.\n[43] T. Matsuki, K. Shibata, Adaptive balancing of exploration and exploitation around the edge of\nchaos in internal-chaos-based learning, Neural Networks 132 (2020) 19–29.\n[44] K. Shibata, Y. Sakashita, Reinforcement learning with internal-dynamics-based exploration using\na chaotic neural network, in: 2015 International Joint Conference on Neural Networks (IJCNN),\n2015, pp. 1–8.\n[45] K. Kaneko, I. Tsuda, Chaotic itinerancy, Chaos: An Interdisciplinary Journal of Nonlinear Science\n13 (3) (2003) 926–936.\n[46] M. I. Rabinovich, R. Huerta, P. Varona, V. S. Afraimovich, Transient cognitive dynamics, metasta-\nbility, and decision making, PLoS computational biology 4 (5) (2008) e1000072.\n[47] T. Kanamaru, T. K. Hensch, K. Aihara, Maximal memory capacity near the edge of chaos in\nbalanced cortical ei networks, Neural Computation 35 (8) (2023) 1430–1462.\n[48] Y. Goto, K. Shibata, Emergence of higher exploration in reinforcement learning using a chaotic\nneural network, in: Neural Information Processing, Springer International Publishing, Cham, 2016,\npp. 40–48.\n[49] Y. Goto, K. Shibata, Influence of the chaotic property on reinforcement learning using a chaotic\nneural network, in: Neural Information Processing, Springer International Publishing, Cham, 2017,\npp. 759–767.\n[50] K. Sato, Y. Goto, K. Shibata, Chaos-based reinforcement learning when introducing refractoriness\nin each neuron, in: Robot Intelligence Technology and Applications, Springer Singapore, Singapore,\n2019, pp. 76–84.\n[51] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, D. Wierstra, Continuous\ncontrol with deep reinforcement learning, arXiv preprint arXiv:1509.02971.\n[52] T. Matsuki, S. Inoue, K. Shibata, Q-learning with exploration driven by internal dynamics in chaotic\nneural network, in: 2020 International Joint Conference on Neural Networks (IJCNN), IEEE, 2020,\npp. 1–7.\n[53] S. Fujimoto, H. Hoof, D. Meger, Addressing function approximation error in actor-critic methods,\nin: International conference on machine learning, PMLR, 2018, pp. 1587–1596.\n[54] D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, M. Riedmiller, Deterministic policy gradient\nalgorithms, in: International conference on machine learning, Pmlr, 2014, pp. 387–395.\n[55] W. Maass, T. Natschl¨ager, H. Markram, Real-time computing without stable states: A new frame-\nwork for neural computation based on perturbations, Neural computation 14 (11) (2002) 2531–2560.\n[56] H. Jaeger, The “echo state” approach to analysing and training recurrent neural networks-with\nan erratum note, Bonn, Germany: German National Research Center for Information Technology\nGMD Technical Report 148 (34) (2001) 13.\n[57] N. Bertschinger, T. Natschl¨ager, Real-time computation at the edge of chaos in recurrent neural\nnetworks, Neural computation 16 (7) (2004) 1413–1436.\n[58] J. Boedecker, O. Obst, J. T. Lizier, N. M. Mayer, M. Asada, Information processing in echo state\nnetworks at the edge of chaos, Theory in Biosciences 131 (3) (2012) 205–213.\n[59] H.-H. Chang, L. Liu, Y. Yi, Deep echo state q-network (deqn) and its application in dynamic\nspectrum sharing for 5g and beyond, IEEE Transactions on Neural Networks and Learning Systems.\n[60] T. Matsuki, Deep q-network using reservoir computing with multi-layered readout, arXiv preprint\narXiv:2203.01465.\n[61] M. Beggs, John, D. Plenz, Neuronal avalanches in neocortical circuits, Journal of neuroscience\n23 (35) (2003) 11167–11177.\n[62] M. Beggs, John, The criticality hypothesis: how local cortical networks might optimize informa-\ntion processing, Philosophical Transactions of the Royal Society A: Mathematical, Physical and\nEngineering Sciences 366 (1864) (2007) 329–343.\n[63] L. Cocchi, L. Gollo, Leonardo, A. Zalesky, M. Breakspear, Criticality in the brain: A synthesis of\nneurobiology, models and cognition, Progress in neurobiology 158 (2017) 132–152.\n[64] J. Shi, K. Kirihara, M. Tada, M. Fujioka, K. Usui, D. Koshiyama, T. Araki, L. Chen, K. Kasai,\nK. Aihara, Criticality in the healthy brain, Frontiers in Network Physiology 1 (2022) 755685.\n[65] Z. Tong, G. Tanaka, Reservoir computing with untrained convolutional neural networks for image\n24\nrecognition, in: 2018 24th International Conference on Pattern Recognition (ICPR), IEEE, 2018,\npp. 1289–1294.\n[66] H. Chang, K. Futagami, Reinforcement learning with convolutional reservoir computing, Applied\nIntelligence 50 (8) (2020) 2400–2410.\n[67] C. Gallicchio, A. Micheli, L. Pedrelli, Deep reservoir computing: A critical experimental analysis,\nNeurocomputing 268 (2017) 87–99.\n[68] C. Gallicchio, A. Micheli, Echo state property of deep reservoir computing networks, Cognitive\nComputation 9 (3) (2017) 337–350.\n[69] Q. Ma, L. Shen, G. W. Cottrell, Deep-esn: A multiple projection-encoding hierarchical reservoir\ncomputing framework, arXiv preprint arXiv:1711.05255.\n[70] Y. Sakemi, K. Morino, T. Leleu, K. Aihara, Model-size reduction for reservoir computing by con-\ncatenating internal states through time, Scientific reports 10 (1) (2020) 1–13.\n[71] K. Shibata, T. Ejima, Y. Tokumaru, T. Matsuki, Sensitivity – local index to control chaoticity or\ngradient globally –, Neural Networks 143 (2021) 436–451.\n[72] Y. Sakemi, S. Nobukawa, T. Matsuki, T. Morie, K. Aihara, Learning reservoir dynamics with\ntemporal self-modulation, Communications Physics 7 (1) (2024) 29.\n25\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.NE"
  ],
  "published": "2024-05-15",
  "updated": "2024-05-15"
}