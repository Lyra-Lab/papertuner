{
  "id": "http://arxiv.org/abs/2405.10251v1",
  "title": "A Systematic Evaluation of Large Language Models for Natural Language Generation Tasks",
  "authors": [
    "Xuanfan Ni",
    "Piji Li"
  ],
  "abstract": "Recent efforts have evaluated large language models (LLMs) in areas such as\ncommonsense reasoning, mathematical reasoning, and code generation. However, to\nthe best of our knowledge, no work has specifically investigated the\nperformance of LLMs in natural language generation (NLG) tasks, a pivotal\ncriterion for determining model excellence. Thus, this paper conducts a\ncomprehensive evaluation of well-known and high-performing LLMs, namely\nChatGPT, ChatGLM, T5-based models, LLaMA-based models, and Pythia-based models,\nin the context of NLG tasks. We select English and Chinese datasets\nencompassing Dialogue Generation and Text Summarization. Moreover, we propose a\ncommon evaluation setting that incorporates input templates and post-processing\nstrategies. Our study reports both automatic results, accompanied by a detailed\nanalysis.",
  "text": "A Systematic Evaluation of Large Language Models for Natural\nLanguage Generation Tasks\nXuanfan Ni, Piji Li∗\nCollege of Computer Science and Technology,\nNanjing University of Aeronautics and Astronautics\nMIIT Key Laboratory of Pattern Analysis and Machine Intelligence\n{xuanfanni, pjli}@nuaa.edu.cn\nAbstract\nRecent efforts have evaluated large language models (LLMs) in areas such as com-\nmonsense reasoning, mathematical reasoning, and code generation. However, to the\nbest of our knowledge, no work has specifically investigated the performance of LLMs\nin natural language generation (NLG) tasks, a pivotal criterion for determining model\nexcellence. Thus, this paper conducts a comprehensive evaluation of well-known and\nhigh-performing LLMs, namely ChatGPT, ChatGLM, T5-based models, LLaMA-based\nmodels, and Pythia-based models, in the context of NLG tasks. We select English and\nChinese datasets encompassing Dialogue Generation and Text Summarization. More-\nover, we propose a common evaluation setting that incorporates input templates and\npost-processing strategies. Our study reports both automatic results, accompanied by a\ndetailed analysis.\n1\nIntroduction\nRecent studies have emphasized the importance of scaling large language models (LLMs),\nreferring to both the dimensions of the model size themselves and the amount of data used,\nresulting in enhanced capability of the models for tasks downstream (Chung et al., 2022).\nNumerous investigations have been conducted to explore the limits of performance by training\nincreasingly larger pre-trained language models, such as GPT-3 175B (Brown et al., 2020) and\nPaLM 540B (Chowdhery et al., 2022). Although scaling primarily involves increasing the model\nsize while maintaining similar architectures and pre-training tasks, these large-sized PLMs exhibit\ndistinct behaviors from their smaller counterparts and demonstrate surprising emergent abilities\nin solving complex tasks (Zhang et al., 2017; Frankle and Carbin, 2019; Zhang et al., 2021). An\nexample of this is the contrasting performance of GPT-3 and GPT-2 when it comes to solving\nfew-shot tasks. GPT-3 demonstrates effective problem-solving abilities by utilizing in-context\nlearning, whereas GPT-2 faces difficulties in this aspect. As a result, these large-scale language\nmodels (LLMs) has become a huge research topic in current NLP area. In existing literature,\nremarkable LLMs such as ChatGPT0, ChatGLM1, have been widely adopted as powerful AI\n∗Corresponding author.\n©2023 China National Conference on Computational Linguistics\nPublished under Creative Commons Attribution 4.0 International License\n0https://chat.openai.com/\n1https://chatglm.cn/\narXiv:2405.10251v1  [cs.CL]  16 May 2024\nComputational Linguistics\nassistants, benefiting from their exceptional generation capabilities.\nWe hypothesis that a language model’s performance in executing natural language generation\n(NLG) tasks is a crucial factor in determining its excellence (Dong et al., 2023). NLG tasks\ninvolve LLMs that are capable of accepting diverse types of input, such as texts and tables, and\ngenerating coherent and appropriate output text. We intuitively think that generate fluent, coherent,\nand consistent texts is the foundation of a language model, so as to large language models (Raffel\net al., 2020). When some research institutions release their large language models, they tend\nto evaluate these models first. Community workers are also interested in testing well-known\nlarge language models. However, most of these evaluations focus on checking LLMs’ ability of\ncommonsense reasoning (Davis and Marcus, 2015; Wei et al., 2022), mathematical reasoning\n(Saxton et al., 2019; Wei et al., 2022), code completion (Allamanis et al., 2018), etc., but ignore\nthe basic NLG tasks, such as dialogue generation (Chen et al., 2017), text summarization (Dong et\nal., 2023), and story generation (Al-Hussain and Azmi, 2022). Besides, Some researchers pointed\nout that the performance of a large model is determined not only by its size and architecture, but\nmore by the quality and quantity of training data. Based on this point of view, researchers open\nsource and propose that some smaller-scale models trained on more and higher-quality data sets\ncan achieve the same performance as models with more parameters than them. For example,\nLLaMA-13B (Touvron et al., 2023) outperforms GPT-3 on most benchmarks, despite being 10\ntimes smaller. This notable discovery makes us curious about the performance of models with\ndifferent architecture, data size, and mode size, trying to figure out which factor is more important.\nTherefore, we aim to address this gap by conducting a comparative analysis of LLM performance\non NLG tasks, considering different architectures and scales throughout the evaluation process.\nIn this paper, we present a systematic evaluation of existing LLMs for NLG tasks. The main\nobjective is to enhance our understanding of instruction and prompt design by conducting a\ncomparative analysis of these models. Initially, we provide an overview of classic NLG tasks,\nincluding their definitions and associated English and Chinese datasets. Subsequently, we devise\na model input template that includes instructions for each dataset. Following that, we introduce\nvarious LLMs, considering factors such as model size and architecture. Finally, we present the\nresults of both automatic and manual evaluation of LLMs on NLG datasets, and discuss the\nstrengths and weaknesses of their performance across different models.\n2\nNatural Language Generation\nIn this section, we will introduce the definition of NLG, and its sub-tasks with some corresponding\ndatasets that we will use to evaluate LLMs.\n2.1\nDefinition\nNatural Language Generation is the process of producing a natural language text in order to meet\nspecified communicative goals. The texts that are generated may range from a single phrase\ngiven in answer to a question, through multi-sentence remarks and questions within a dialog, to\nfull-page explanations. In our evaluation, we mainly focus on text-to-text styles. In general, the\nComputational Linguistics\ntask of NLG targets at finding an optimal sequence y<T+1 = (y1, y2, . . . , yT ) that satisfies:\ny<T+1 = arg max\ny<T +1∈Y\nlog Pθ (y<T+1 | x) = arg max\ny<T +1∈Y\nT\nX\nt=1\nlog Pθ (yt | y<t, x)\n(1)\nwhere T represents the number of tokens of the generated sequence, Y represents a set containing\nall possible sequences, and Pθ (yt | y<t, x) is the conditional probability of the next token yt\nbased on its previous tokens y<t = (y1, y2, . . . , yt−1) and the source sequence x with model\nparameters θ.\nNext, we will introduce some classic and widely-researched sub-tasks of NLG, with several\ncorresponding datasets.\n2.2\nDialogue Generation\nDialogue generation refers to the process of automatically generating coherent and contextually\nappropriate responses in a conversational setting (Chen et al., 2017; Ma et al., 2020; Dong et\nal., 2023). The ultimate goal of dialogue generation task is to create responses that are relevant,\ninformative, and engaging to the user.We utilize two English dialogue datasets characterized\nby clear emotional flow and topic constraints, as well as one English dataset that incorporates\nspeakers’ personalities. Furthermore, we employ a Chinese open-domain dialogue dataset for\nevaluation purposes.\n• DailyDialog (Li et al., 2017) is a comprehensive, human-authored, and relatively noise-free\nEnglish dataset that captures everyday communication styles and encompasses various topics\nrelated to our daily lives.\n• PersonaChat (Zhang et al., 2018) is a persona-grounded dialogue dataset which contains\n10k English multi-turn dialogues conditioned on personas, and each persona is described\nwith at least 5 profile sentences.\n• EmpatheticDialogue (Rashkin et al., 2019) is a large-scale multi-turn dialogue English\ndataset that contains 25k empathetic conversations between a speaker and a listener.\n• LCCC (Wang et al., 2020) is a large-scale cleaned Chinese conversation dataset.\n2.3\nText Summarization\nText summarization is the process of condensing a piece of text, such as an article, document,\nor news story, into a shorter version while preserving its key information and main ideas (El-\nKassas et al., 2021; Dong et al., 2023). Text summarization can be performed through two main\napproaches: Extractive Summarization and Abstractive Summarization. In our evaluation, we\nutilize multiple abstractive summarization datasets, specifically choosing two renowned datasets\nfor the English and Chinese languages.\n• CNN/DailyMail (Nallapati et al., 2016) is a large scale English summarization dataset\nwhich contains 93k and 220k articles collected from the CNN and Daily Mail websites,\nrespectively, where each article has its matching abstractive summary.\n• XSum (Narayan et al., 2018) is an extreme English summarization dataset containing BBC\narticles and corresponding single sentence summaries. In this dataset, 226,711 Wayback\narchived BBC articles are collected, which range from 2010 to 2017 and cover a wide variety\nof domains.\nComputational Linguistics\nBelow is an instruction that describes\na task. Write a response that appropr-\niately completes the request.\n### Instruction: {instruction}\n### Input: {text}\n### Response:\n以下是描述任务的说明。 编写准确的回复来\n完成这个任务。\n### 说明：{instruction}\n### 输入：{text}\n### 回复：\nFigure 1: Input templates for English (left) and Chinese (right) datasets. instruction and text\nwill be replaced with content corresponding different datasets.\n• THUCNews (Li and Sun, 2007) is a Chinese summarization dataset, which comes from\nfiltering the historical data of the Sina News RSS subscription channel from 2005 to 2011,\nincluding 740,000 news documents.\n• LCSTS (Liu, 2020) is a large corpus of Chinese short text summarization dataset constructed\nfrom the Chinese micro-blogging website Sina Weibo. This corpus consists of over 2 million\nreal Chinese short texts with short summaries given by the author of each text.\n3\nExperimental Settings\n3.1\nOverview for LLMs\nTypically, large language models (LLMs) refer to Transformer-based models containing tens or\nhundreds of billions of parameters and trained on extensive corpora of texts (Zhao et al., 2023).\nThese LLMs demonstrate significant capabilities in understanding natural language and solving\ncomplex tasks. Furthermore, they have showcased their ability to perform new tasks based on\ntextual instructions or with just a few examples (Chung et al., 2022). The emergence of these\nfew-shot properties is a result of scaling models to a sufficient size, leading to a line of research\nthat focuses on further scaling these models (Rae et al., 2021).\nPrevious LLMs, such as T5 (Raffel et al., 2020), GPT-3 (Brown et al., 2020), OPT (Zhang et\nal., 2022), and PaLM (Chowdhery et al., 2022), primarily emphasized scaling model size rather\nthan considering the quality and quantity of data. However, recent studies have demonstrated\nthat, given a fixed compute budget, the best performance is achieved by smaller models trained\non larger datasets (Hoffmann et al., 2022). Additionally, most of these models are not open-\nsource and can only be accessed through APIs for inference, which poses inconveniences for\nmodel evaluation and usage. In order to address this issue, numerous researchers have proposed\nexcellent open-source architectures and trained models, including GLM-130B (Zeng et al., 2022),\nChatGLM (Du et al., 2022), LLaMA (Touvron et al., 2023), and Pythia (Biderman et al., 2023).\nFurthermore, advancements in fine-tuning techniques have contributed to the success of deploying\nthese models with limited resources, such as Lora (Hu et al., 2022) and P-Tuning (Li and Liang,\n2021). Therefore, this paper aims to conduct systematic evaluations of these models and their\nfine-tuned versions, categorized into four groups: ChatGPT, ChatGLM, T5-based models,\nLLaMA-based models, and Pythia-based models.\nComputational Linguistics\nDataset\nInstruction\nText\nEmpathetic\nDialogues\nThis is an open-domain empathetic dialogue completion task.The\ninput is the Dialogue. You act as System in the dialogue. You need to\nfully understand the situation and combine the speaker’s emotion to\ncomplete the dialogue with natural content and a way closer to human\nspeech. There is no need for any additional notes or clarifications,\nyou just give the response in English.\nDialogue Context\nDailyDialog\nThis is an open-domain topic-aware dialogue completion task. The\ninput is the Dialogue. You act as System in the dialogue. You need\nto fully understand the topic and complete the dialogue with natural\ncontent and a way closer to human speech. There is no need for any\nadditional notes or clarifications, you just give the response in English\nDialogue Context\nPersonaChat\nThis is an open-domain personality-aware dialogue completion task.\nThe input is the Dialogue. You act as System in the dialogue. You\nneed to fully understand the personality and complete the dialogue\nwith natural content and a way closer to human speech. There is\nno need for any additional notes or clarifications, you just give the\nresponse in English.\nDialogue Context\nLCCC\n这是一个开放域的中文对话补全任务。输入是待完成的对话内\n容。你在对话中扮演系统。你需要完全理解说话者的话语，并\n用自然的内容和更接近于人类说话的方式完成对话，而不是用\n语言模型或者AI的身份。不需要任何额外的注释或者说明，你\n只需用中文给出回复。\nDialogue Context\nTable 1: Instruction and Text for each dataset.\n3.2\nChatGPT\nChatGPT2 is a large language model based on OpenAI’s GPT-3.5 architecture (Brown et al.,\n2020). It is designed specifically for generating conversations and answering user queries.\nChatGPT employs large-scale pretraining and fine-tuning methodologies, utilizing vast amounts\nof textual data to learn statistical patterns and semantic knowledge of language, and perform well\nin zero-shot and few-shot settings, and can understand the input instructions.\n3.3\nChatGLM\nChatGLM3 is a freely available dialogue language model that operates in both Chinese and\nEnglish languages. It follows the GLM architecture and boasts an impressive parameter count\nof 6.2 billion. ChatGLM-6B incorporates similar technology as ChatGPT, with a specific focus\non Chinese question answering and dialogue. The model undergoes extensive training on a\ndataset containing approximately 1 trillion tokens in Chinese and English. The training process\nincludes supervised fine-tuning, feedback bootstrap, and reinforcement learning with human\nfeedback. Despite having only 6.2 billion parameters, the model demonstrates the ability to\ngenerate responses that align with human preferences.\n2https://chat.openai.com/\n3https://chatglm.cn/\nComputational Linguistics\nModel\nScale\nArch\nPPL↓\nB-1\nB-2\nB-4\nMT\nR-L\nD-1\nD-2\nPPR↓\nEP-PG\n–\n–\n–\n16.74\n6.94\n2.39\n–\n–\n2.19\n8.25\n–\nMoEL\n23.1M\nDO\n33.58\n–\n–\n2.90\n–\n–\n1.06\n4.29\n–\nChatGPT\n175B\nDO\n10.52\n7.35\n2.40\n0.52\n9.26\n8.75\n4.71\n27.75\n0.00%\nChatGLM\n6B\nDO\n11.73\n6.05\n1.82\n0.27\n8.58\n7.71\n3.57\n22.82\n12.61%\nFlan-T5-XXL\n13B\nED\n19.97\n5.62\n2.40\n0.61\n5.38\n7.41\n5.66\n24.97\n0.00%\nFastChat-T5\n3B\nED\n9.25\n7.33\n2.35\n0.45\n8.50\n8.62\n3.55\n20.81\n0.12%\nOpen-LLaMA\n7B\nDO\n15.90\n8.50\n2.97\n0.63\n6.43\n8.74\n3.93\n17.91\n40.05%\nVicuna\n13B\nDO\n14.31\n6.18\n1.93\n0.35\n8.91\n7.81\n4.09\n25.84\n38.86%\nAlpaca-Lora\n7B\nDO\n16.10\n7.95\n2.52\n0.40\n7.34\n6.69\n7.59\n39.58\n0.24%\nChinese-Alpaca\n13B\nDO\n12.05\n6.51\n1.86\n0.35\n7.53\n6.64\n5.32\n29.14\n0.20%\nGPT4ALL\n13B\nDO\n11.14\n5.20\n1.47\n0.24\n8.75\n6.78\n3.94\n25.60\n1.81%\nDolly\n12B\nDO\n131.75\n8.29\n2.64\n0.46\n6.91\n7.96\n7.46\n42.69\n58.61%\nOasst-Pythia\n12B\nDO\n8.71\n5.48\n1.53\n0.26\n8.79\n6.92\n3.38\n21.18\n0.04%\nTable 2: Automatic evaluation results of LLMs on EmpatheticDialogues. Scale stands for the\nmodel size.ED and DO respectively stand for encoder-decoder and decoder-only. Arch is an\nabbreviation for Architecture. The bold numbers in the results represent the best scores, whereas\nthe underlined numbers indicate the second-best scores.\nModel\nScale\nArch\nPPL↓\nB-1\nB-2\nB-4\nMT\nR-L\nD-1\nD-2\nPPR↓\nPLATO\n–\nDO\n–\n39.70\n31.10\n–\n–\n–\n5.30\n29.10\n–\nDialogWAE\n–\nED\n–\n32.30\n–\n–\n–\n–\n31.30\n59.70\n–\nChatGPT\n175B\nDO\n11.41\n7.58\n2.71\n0.56\n10.13\n8.17\n10.98\n47.20\n0.00%\nChatGLM\n6B\nDO\n17.52\n10.54\n3.86\n0.93\n9.14\n11.91\n9.60\n42.69\n12.05%\nFlan-T5-XXL\n13B\nED\n16.31\n3.85\n1.61\n0.42\n6.64\n5.52\n14.54\n47.59\n0.00%\nFastChat-T5\n3B\nED\n10.27\n7.45\n2.59\n0.50\n9.15\n7.86\n9.58\n41.16\n0.50%\nOpen-LLaMA\n7B\nDO\n21.23\n6.72\n2.31\n0.46\n5.94\n5.59\n11.65\n38.72\n64.36%\nVicuna\n13B\nDO\n78.66\n6.13\n2.11\n0.42\n8.89\n6.96\n10.15\n45.18\n38.55%\nAlpaca-Lora\n7B\nDO\n28.63\n6.40\n2.16\n0.00\n6.04\n5.02\n17.49\n61.66\n3.41%\nChinese-Alpaca\n13B\nDO\n22.23\n6.52\n2.18\n0.38\n7.49\n5.93\n13.06\n51.02\n2.01%\nGPT4ALL\n13B\nDO\n14.72\n4.84\n1.24\n0.13\n7.72\n5.77\n10.24\n43.53\n25.50%\nDolly\n12B\nDO\n58.29\n6.09\n2.01\n0.40\n5.70\n4.25\n14.14\n52.33\n74.80%\nOasst-Pythia\n12B\nDO\n10.68\n5.40\n1.45\n0.19\n7.62\n6.09\n9.23\n38.91\n16.47%\nTable 3: Automatic evaluation results of LLMs on DailyDialog.\n3.4\nT5-Based models\nT5 (Raffel et al., 2020), which stands for Text-To-Text Transfer Transformer, is a transformer-\nbased language model developed by Google Research. Instead of training separate models for\ndifferent tasks, T5 is trained in a text-to-text pattern. This means that it is trained to perform a\nwide range of NLP tasks by transforming the input text into a standardized format that specifies\nthe task to be performed. In our evaluation, we select two new fine-tuned versions of T5, namely:\nComputational Linguistics\nModel\nScale\nArch\nPPL↓\nB-1\nB-2\nB-4\nMT\nR-L\nD-1\nD-2\nPPR↓\nPLATO\n–\nDO\n–\n40.60\n31.50\n–\n–\n–\n2.10\n12.10\n–\nCTRLStruct\n–\nED\n–\n31.60\n11.90\n–\n–\n16.10\n3.20\n11.40\n–\nChatGPT\n175B\nDO\n10.97\n6.36\n2.37\n0.52\n9.78\n8.42\n9.10\n40.65\n0.00%\nChatGLM\n6B\nDO\n13.89\n5.98\n2.07\n0.40\n8.85\n8.67\n6.85\n34.86\n12.05%\nFlan-T5-XXL\n13B\nED\n51.50\n6.51\n2.53\n0.43\n6.15\n7.46\n12.23\n39.82\n0.00%\nFastChat-T5\n3B\nED\n10.61\n5.53\n2.00\n0.43\n8.98\n7.94\n7.30\n33.66\n0.50%\nOpen-LLaMA\n7B\nDO\n15.69\n4.43\n1.16\n0.00\n5.86\n5.43\n7.83\n28.90\n64.36%\nVicuna\n13B\nDO\n12.53\n3.20\n1.01\n0.14\n7.30\n4.82\n5.88\n30.12\n38.55%\nAlpaca-Lora\n7B\nDO\n17.20\n4.19\n1.21\n0.24\n6.29\n4.40\n12.28\n50.33\n3.41%\nChinese-Alpaca\n13B\nDO\n14.95\n4.93\n1.66\n0.29\n7.70\n6.21\n10.18\n44.62\n2.01%\nGPT4ALL\n13B\nDO\n11.68\n2.74\n0.55\n0.07\n6.52\n4.39\n7.56\n35.23\n25.50%\nDolly\n12B\nDO\n29.76\n4.51\n1.39\n0.24\n5.02\n4.59\n10.55\n41.62\n74.80%\nOasst-Pythia\n12B\nDO\n9.57\n3.34\n0.69\n0.07\n6.58\n4.66\n6.48\n28.56\n16.47%\nTable 4: Automatic evaluation results of LLMs on PersonaChat.\nModels\nScale\nBLEU\nBLEU-1\nBLEU-4\nDist-1\nDist-2\nCDialGPT\n104M\n–\n–\n3.20\n0.83\n12.71\nGPT-Novel\n104M\n–\n–\n2.71\n0.80\n11.72\nChatGPT\n175B\n2.55\n5.45\n0.96\n4.83\n28.84\nChatGLM\n6B\n0.83\n1.51\n0.40\n2.08\n5.74\nVicuna\n13B\n3.84\n7.84\n1.58\n4.70\n26.72\nAlpaca\n7B\n4.79\n8.75\n2.33\n6.15\n25.26\nChinese-Alpaca\n13B\n2.88\n5.78\n0.35\n4.09\n25.84\nGPT4ALL\n13B\n3.78\n8.37\n1.33\n2.25\n7.83\nDolly\n12B\n5.30\n10.70\n2.21\n4.53\n20.12\nOasst-Pythia\n12B\n5.16\n11.36\n1.86\n2.04\n7.49\nTable 5: Automatic evaluation results of LLMs on LCCC.\nFlan-T5-XXL4 and FastChat-T55.\nFlan-T5-XXL\nFlan-T5 (Chung et al., 2022) is a fine-tuned version model class of T5 that has\nbeen trained on a variety of datasets phrased as instructions. It has shown impressive performance\non several benchmarks, demonstrating strong zero-shot, few-shot, and Chain-of-Thought (CoT)\n(Wei et al., 2022) abilities. Flan-T5-XXL is the largest released checkpoint of this model, boasting\na parameter volume of 13B. It inherits the extensive knowledge base of T5 while also being\ncapable of understanding natural language instructions and performing the corresponding tasks.\nFastChat-T5\nFastChat (Zheng et al., 2023a) is an open platform for training, serving, and\nevaluating large language model based chatbots. And FastChat-T5 is an open-source chatbot\n4https://huggingface.co/google/flan-t5-xxl\n5https://huggingface.co/lmsys/fastchat-t5-3b-v1.0\nComputational Linguistics\ntrained on this platform by fine-tuning Flan-T5-XL (3B parameters) on user-shared conversations\ncollected from ShareGPT.\n3.5\nLLaMA-Based Models\nLLaMA (Touvron et al., 2023) is a collection of foundation language models ranging from 7B\nto 65B parameters proposed by Meta AI. Unlike other famous LLMs, LLaMA is only trained\non publicly avaiable data, making it compatible with open-sourcing. Numerous remarkable and\nimpressive models have emerged as a result, built upon the LLaMA framework and trained using\ndiverse datasets. Among these models, we have chosen a few prominent ones for evaluation:\nOpen-LLaMA, Vicuna, Alpaca, and GPT4ALL.\nOpen-LLaMA\nOpen-LLaMA (Geng and Liu, 2023) is an open reproduction of LLaMA trained\non the RedPajama dataset (Computer, 2023). We leverage the 7B version6 of this model for\nevaluation.\nAlpaca\n(Taori et al., 2023) is fine-tuned based on a 7B LLaMA model using a dataset con-\nsisting of 52,000 instances of instruction-following data. This dataset is generated using the\ntechniques outlined in the Self-Instruct paper (Wang et al., 2022), which aims to address the\nlimited instruction-following capabilities of LLaMA models. To create the training data, the\nauthors initially generate the data using OpenAI’s GPT-3 and subsequently convert it into 52,000\ninstances of instruction-following conversational data using the Self-Instruct pipeline. This\ndataset is referred to as the Alpaca dataset. The Alpaca model is then fine-tuned to generate\nresponses in conversations similar to ChatGPT.\nIn our evaluation, we utilize Alpaca-Lora-7B7, a low-rank adapter for LLaMA-7b fit on the\nStanford Alpaca dataset, and Chinese-Alpaca-13b8, a Chinese model version of Alpaca.\nVicuna\n(Zheng et al., 2023b) is fine-tuned based on LLaMA models using user-shared conversa-\ntions collected from ShareGPT. It is an auto-regressive language model, based on the transformer\narchitecture. So it is basically fine-tuned with ChatGPT conversations. We utilize the 13B version\nof Vicuna, which is Vicuna-13B9.\nGPT4ALL\n(Anand et al., 2023) is a fine-tuned LLaMA 13B model and the GPT4All commu-\nnity10 has built the GPT4All Open Source datalake as a staging ground for contributing instruction\nand assistant tuning data for future GPT4All model trains.\n3.6\nPythia-Based Models\nPythia (Biderman et al., 2023) is a project by EleutherAI11 that combines interpret-ability\nanalysis and scaling laws to understand how knowledge develops and evolves during training in\nautoregressive Transformers. We utilize two versions of Pythia which are Oasst-Pythia and Dolly.\n6https://github.com/openlm-research/open llama\n7https://huggingface.co/chainyo/alpaca-lora-7b\n8https://huggingface.co/shibing624/chinese-alpaca-plus-13b-hf\n9https://huggingface.co/eachadea/vicuna-13b-1.1\n10https://home.nomic.ai/\n11https://github.com/EleutherAI/pythia\nComputational Linguistics\nModel\nScale\nArch\nPPL↓\nB-1\nB-2\nB-3\nB-4\nMT\nR-L\nPPR↓\nChatGPT\n175B\nDO\n10.86\n2.99\n0.58\n0.00\n0.00\n4.89\n5.02\n0.00%\nChatGLM\n6B\nDO\n18.56\n2.80\n0.87\n0.25\n0.00\n4.80\n4.91\n10.78%\nFlan-T5-XXL\n13B\nED\n15.96\n5.49\n1.21\n0.00\n0.00\n3.69\n5.16\n0.00%\nFastChat-T5\n3B\nED\n10.26\n2.62\n0.89\n0.46\n0.29\n4.80\n4.58\n0.03%\nOpen-LLaMA\n7B\nDO\n45.72\n0.02\n0.01\n0.00\n0.00\n0.35\n0.18\n73.67%\nVicuna\n13B\nDO\n10.94\n2.45\n0.81\n0.41\n0.23\n4.75\n4.40\n31.29%\nAlpaca-lora\n7B\nDO\n19.22\n3.41\n0.56\n0.00\n0.00\n4.19\n4.23\n0.13%\nChinese-Alpaca\n13B\nDO\n14.30\n4.40\n1.88\n1.13\n0.74\n3.55\n10.27\n0.15%\nGPT4ALL\n13B\nDO\n23.28\n3.03\n0.85\n0.49\n0.35\n5.14\n5.06\n3.37%\nDolly\n12B\nDO\n15.01\n3.35\n1.12\n0.62\n0.40\n5.40\n6.01\n46.67%\nOasst-Pythia\n12B\nDO\n18.83\n3.48\n1.15\n0.61\n0.41\n5.23\n6.31\n0.08%\nTable 6: Automatic evaluation results of LLMs on CNN/DailyMail.\nOasst-Pythia12\nis an open assistant model developed by the Open-Assistant project. It is based\non a Pythia 12B model that was fine-tuned on human demonstrations of assistant conversations\ncollected through the Open-Assistant human feedback web app.\nDolly13\nis a Language Model (LLM) with 12B parameters, designed to follow instructions\naccurately. It has been trained on approximately 15,000 instruction/response fine-tuning records\nknown as databricks-dolly-15k. These records were created by Databricks employees and cover\nvarious capability domains sourced from InstructGPT (Ouyang et al., 2022). These domains\ninclude brainstorming, classification, closed QA, generation, information extraction, open QA,\nand summarization.\n3.7\nDataset\nIn our evaluation, we aim to showcase the generation capabilities of LLMs in zero-shot scenarios.\nTherefore, we refrain from providing any additional information to the model for each of the\naforementioned datasets. Specifically:\n• For datasets of Text Summarization task, we input the text, document, or article to allow the\nmodel to extract key information and generate concise summaries.\n• For datasets of Dialogue Generation task, we input the dialogue history, enabling the model\nto generate appropriate responses for the final round of the conversation.\nWe defer the evaluation of LLMs on Chinese datasets and other NLG tasks such as story\ngeneration, along with results of manual and GPT-4 rating, to future research endeavors.\n3.8\nInput Template\nBecause LLMs that we evaluate possess the ability to comprehend instructions and perform\ncorresponding tasks, so in order to ensure fairness, we develop an input template that is applied\n12https://huggingface.co/OpenAssistant/pythia-12b-sft-v8-7k-steps\n13https://huggingface.co/databricks/dolly-v2-12b\nComputational Linguistics\nModel\nScale\nArch\nPPL↓\nB-1\nB-2\nB-3\nB-4\nMT\nR-L\nPPR↓\nChatGPT\n175B\nDO\n14.92\n7.55\n2.93\n1.27\n0.55\n11.47\n10.31\n0.00%\nChatGLM\n6B\nDO\n22.84\n5.45\n2.46\n1.19\n0.60\n10.76\n9.25\n8.79%\nFlan-T5-XXL\n13B\nED\n10.90\n12.48\n4.66\n2.19\n1.81\n17.60\n15.06\n0.00%\nFastChat-T5\n3B\nED\n14.08\n8.05\n3.78\n1.83\n0.78\n13.22\n11.01\n0.00%\nOpen-LLaMA\n7B\nDO\n31.13\n4.57\n1.31\n0.55\n0.00\n2.31\n2.70\n56.79%\nVicuna\n13B\nDO\n14.58\n7.13\n3.06\n1.41\n0.67\n12.61\n10.16\n30.11%\nAlpaca-lora\n7B\nDO\n23.49\n8.65\n2.95\n1.20\n0.49\n10.94\n9.54\n1.17%\nChinese-Alpaca\n13B\nDO\n19.21\n6.65\n3.31\n1.88\n1.19\n5.98\n8.34\n5.90%\nGPT4ALL\n13B\nDO\n18.79\n8.47\n3.46\n1.68\n0.95\n11.73\n9.81\n15.79%\nDolly\n12B\nDO\n20.89\n6.44\n2.64\n1.01\n0.00\n11.21\n9.95\n82.23%\nOasst-Pythia\n12B\nDO\n21.49\n6.27\n2.46\n0.99\n0.37\n9.98\n9.32\n28.42%\nTable 7: Automatic evaluation results of LLMs on XSum.\nModels\nLCSTS\nLOT\nRouge-1\nRouge-2\nRouge-L\nBLEU\nBLEU-1\nBLEU-4\nDist-1\nDist-2\nERNIE\n–\n–\n48.46\n–\n–\n–\n–\n–\nRNN-Context\n29.90\n17.40\n27.20\n–\n–\n–\n–\n–\nLongLM\n–\n–\n–\n–\n–\n5.97\n–\n–\nChatGPT\n17.20\n4.92\n12.05\n19.21\n33.34\n8.92\n7.56\n40.87\nChatGLM\n18.04\n5.88\n12.83\n15.20\n26.59\n6.99\n5.40\n34.00\nVicuna\n16.62\n4.49\n11.71\n19.48\n33.81\n9.39\n7.52\n37.59\nChinese-Vicuna\n–\n–\n–\n13.13\n26.55\n4.52\n5.66\n32.38\nAlpaca\n11.52\n3.51\n8.52\n0.63\n1.08\n0.33\n3.18\n8.02\nChinese-Alpaca\n11.98\n2.42\n9.05\n11.91\n23.68\n3.89\n4.85\n30.10\nGPT4ALL\n4.13\n0.93\n3.05\n0.94\n1.83\n0.40\n3.86\n10.02\nDolly\n10.83\n3.84\n7.41\n10.09\n17.43\n5.13\n14.42\n45.94\nOasst-Pythia\n12.95\n4.24\n9.19\n7.43\n11.61\n4.45\n9.11\n27.90\nTable 8: Automatic evaluation results of LLMs on LCSTS and LOT.\nto every dataset for each task, serving as the input for every large language model. This template\nconsists of two components: the instruction and the input. Figure 1 illustrates the templates\ndesigned for both the Chinese and English datasets, and Table 1 shows the content of instruction\nand text for each dataset.\n3.9\nHyperparameters\nAlthough each LLM may have its own optimal decoding strategy, for the sake of fairness, we\nhave standardized these hyperparameters across all LLMs. We employ the Top-k and Top-p\nsampling, with k = 40 and p = 0.75. Additionally, a temperature value of 0.2 and a repetition\nComputational Linguistics\nModels\nROCStories\nWritingPrompts\nB\nB-1\nB-4\nD-1\nD-2\nB\nB-1\nB-4\nD-1\nD-2\nMVP\n–\n–\n15.76\n3.02\n75.65\n–\n–\n–\n–\n–\nKEPM\n–\n32.60\n–\n–\n78.96\n–\n–\n–\n–\n–\nTextBox2.0\n–\n–\n–\n–\n–\n33.79\n–\n–\n–\n78.76\nChatGPT\n5.70\n13.60\n1.41\n21.98\n67.24\n4.57\n12.86\n0.37\n3.66\n27.92\nChatGLM\n0.91\n2.86\n0.04\n6.43\n36.85\n4.11\n11.13\n0.39\n2.79\n20.02\nFlan-T5\n9.11\n17.80\n3.73\n18.15\n54.32\n0.00\n0.00\n0.00\n14.47\n50.95\nFastChat-T5\n6.63\n14.80\n1.97\n15.23\n50.32\n0.26\n0.70\n0.03\n4.88\n26.26\nOpen-LLaMA\n3.60\n9.21\n0.58\n14.53\n46.94\n0.02\n0.04\n0.00\n4.72\n22.52\nLLaMA2-Chat\n4.44\n10.88\n0.94\n21.02\n65.48\n0.00\n0.00\n0.00\n10.82\n40.09\nVicuna\n7.07\n15.31\n2.23\n20.17\n64.82\n1.69\n4.56\n0.17\n4.48\n28.53\nChinese-Vicuna\n8.88\n18.02\n3.31\n20.02\n63.55\n0.37\n0.98\n0.05\n5.50\n29.10\nAlpaca\n4.69\n11.64\n0.99\n18.02\n62.61\n0.08\n0.22\n0.01\n6.16\n36.23\nChinese-Alpaca\n3.77\n9.30\n0.78\n17.89\n58.42\n0.65\n1.85\n0.05\n4.08\n26.41\nGPT4ALL\n4.61\n10.99\n1.17\n18.87\n61.51\n0.73\n2.03\n0.07\n5.18\n30.70\nDolly\n2.81\n7.04\n0.50\n11.31\n52.03\n0.24\n0.68\n0.02\n7.07\n42.52\nOasst-Pythia\n2.78\n7.16\n0.45\n10.65\n48.42\n0.53\n1.46\n0.05\n4.37\n27.26\nTable 9: Automatic evaluation results of LLMs on ROCStories and WritingPrompts.\npenalty factor of 1.15 are imposed. Furthermore, we specify a maximum token length of 512 and\na minimum token length of 10 for the generated content.\n3.10\nPost-Processing Strategy\nThrough case study, we observe that despite emphasizing the exclusion of any additional output in\nthe input, regrettably, most LLMs still generate redundant information in their output. Therefore,\nwe find it necessary to apply post-processing to the outputs of these models. To ensure fairness,\nwe adopt the same post-processing strategy for all LLMs. Specifically, we utilize the keywords\n“### response:” or “### 回复：” for segmentation. If the segmented content consists of a\nsingle line, we consider it as the final result. If the segmented content spans multiple lines, we\nuse “\\n” as segmentation keywords and select the first sentence with a length not less than 16 as\nthe final result.\n3.11\nBaselines\nThere have been numerous previous works on datasets we used, and these works have achieved\ngood results. Therefore, despite the fact that most of these works have proposed models much\nsmaller than LLMs and have predominantly utilized supervised fine-tuning methods, we still\ncompare them with LLMs to highlight some characteristics of LLMs. For each dataset, we select\nseveral recent works with better performance and report their results.\n• For EmpatheticDialogues, we utilize EP-PG (Li et al., 2022) that first generates event\ntransition plans and then obtains the final response, and MoEL (Lin et al., 2019) that are\nComputational Linguistics\nconsist of one emotion tracker and n emotion listeners.\n• For DailyDialog, we utilize PLATO (Bao et al., 2020), a pre-trained dialogue generation\nmodel, and DialogWAE (Gu et al., 2019), a conditional wasserstein autoencoder (WAE)\nspecially designed for dialogue modeling.\n• For PersonaChat, we utilize PLATO as mentioned above, and CTRLStruct (Yin et al.,\n2023) for dialogue structure learning to effectively explore topic-level dialogue clusters.\n3.12\nEvaluation Metrics\nAutomatic Metrics\nWe utilize several common automatic metrics for NLG tasks. PPL is used\nto assess the difficulty or confusion of a language model in predicting a sequence of words. BLEU\n(B-1, B-2, B-3, B-4) (Papineni et al., 2002) is used to assess the quality of machine-generated\ntranslations by comparing them to human reference translations. Meteor (MT) (Banerjee and\nLavie, 2005) considers the accuracy and recall based on the entire corpus, and get the final\nmeasure. Rouge-L (R-L) (Lin, 2004) calculates the overlap between the generated output and\nthe reference summaries or translations using various techniques such as N-gram matching.\nDISTINCT (D-1, D-2) (Li et al., 2016) quantifies how many distinct or different N-grams are\npresent in the generated text, providing an indication of the model’s ability to produce varied and\nnon-repetitive output.\nBesides these widely-used metrics, we also develop a new metric called PostProcess Rate\n(PPR), which means the proportion of samples that need to be post-processed to the total number\nof samples.\n4\nResults and Analysis\n4.1\nDialogue Generation\nThe automatic metrics results of LLMs on the three datasets are shown in Table 2, 3, 4 and 5.\nAlthough automatic metrics cannot fully reflect the performance of the models, we can still draw\nthe following conclusions from them.\nFirst, apart from ChatGPT that has the largest scale of 175B, the two T5-based models\nconsistently outperform others in terms of the PPR metric. This indicates that the generated\ncontent of Flan-T5-XXL and FastChat-T5 largely aligns with the instruction requirements stated\nin the input template: ”without any additional output.” Interestingly, both of these models follow\nan encoder-decoder architecture, while all other models follow a decoder-only architecture. This\nsuggests that encoder-decoder models demonstrate superior understanding of input instructions\nunder the same model scale. We speculate that having an encoder allows the model to comprehend\nthe input content effectively, thereby executing the corresponding task more successfully.\nSecond, Alpaca-Lora consistently ranks either first or second in the richness of output content.\nMoreover, the models using the same architecture as Alpaca-Lora also achieve higher scores in\nterms of D-1 and D-2. This indicates that LLAMA-based models are capable of producing more\ndiverse and less repetitive content.\nLast, ChatGPT, the model with the largest parameter scale, performs the best overall on all\nfour datasets, securing the first or second position most frequently. This suggests that increasing\nthe parameter size and training data volume of LLMs is consistently one of the most important\nmethods for improving model performance.\nComputational Linguistics\nDataset\nModels\nBLEU\nBLEU-1\nBLEU-4\nDist-1\nDist-2\nED\nPrevious SOTA\n–\n16.74\n2.39\n2.19\n8.25\nChatGLM-6B\n2.22\n6.08\n0.27\n3.57\n22.82\n+ LoRA\n5.98\n14.04\n1.46\n9.20\n47.32\n+ P-Tuning v2\n28.50\n45.66\n15.43\n2.59\n6.93\nLLaMA2-Chat-7B\n1.88\n5.25\n0.24\n4.18\n27.33\n+ LoRA\n4.98\n9.87\n1.84\n4.73\n20.46\nLLaMA2-7B + LoRA\n5.59\n10.51\n2.37\n4.32\n17.46\nLCCC\nPrevious SOTA\n–\n–\n3.20\n0.83\n12.71\nChatGLM-6B\n0.83\n1.51\n0.40\n2.08\n5.74\n+ LoRA\n11.73\n22.38\n5.49\n4.53\n8.87\n+ P-Tuning v2\n10.38\n19.92\n4.71\n0.84\n6.31\nROCStories\nPrevious SOTA\n–\n–\n15.76\n3.02\n75.65\nChatGLM-6B\n0.91\n2.86\n0.04\n6.43\n36.85\n+ LoRA\n37.07\n55.68\n23.03\n8.64\n18.86\n+ P-Tuning v2\n36.29\n55.91\n21.56\n5.63\n32.70\nLLaMA2-Chat-7B\n4.44\n10.88\n0.94\n21.02\n65.48\n+ LoRA\n9.61\n17.63\n4.34\n7.86\n35.89\nLLaMA2-7B + LoRA\n9.79\n17.60\n4.63\n8.07\n36.42\nLOT\nPrevious SOTA\n–\n–\n5.97\n–\n–\nChatGLM-6B\n15.20\n26.59\n6.99\n5.40\n34.00\n+ LoRA\n25.17\n38.98\n14.91\n13.33\n58.53\n+ P-Tuning v2\n17.35\n30.02\n9.26\n13.42\n57.47\nTable 10: Results of finetuning ChatGLM, and LLaMA2 by LoRA and P-Tuning V2 on Empa-\nthetic Dialogues, LCCC, ROCStories, and LOT.\n4.2\nText Summarization\nThe automatic metrics results of LLMs on the three datasets are shown in Table 6, 7 and 8. Our\nobservations from the two datasets can be summarized as follows:\nThe Flan-T5 and FastChat-T5 models employ an encoder-decoder architecture, exhibiting\nremarkable proficiency in instruction comprehension, as evident by their minimal requirement for\npost-processing. This finding is corroborated by the analysis of dialogue generation. Moreover,\nour investigation on the XSum dataset reveals that both models surpass other LLMs, consistently\nattaining top positions across various metrics such as BLEU and ROUGE scores. These impressive\nresults are likely attributed to the inherent strengths embedded within their model structures.\nComputational Linguistics\nModels\nROUGE-1\nROUGE-2\nROUGE-L\nChatGLM-6B\n18.04\n5.88\n12.83\n+ LoRA\n38.84\n22.26\n36.81\n+ P-Tuning v2\n39.20\n23.58\n36.95\nTable 11: Results of finetuning ChatGLM on LCSTS.\n4.3\nStory Generation\nThe automatic metrics results of LLMs on the three datasets are shown in Table 8 and 9. These\ntables provide further analysis of the performance of current LLMs on NLG tasks: when the\nrequired generated text is excessively long, the models struggle to follow instructions effectively.\nThis is evidenced by the WritingPrompts dataset from Table 9, where many models have BLEU\nscores that are close to or equal to zero.\n4.4\nFinetune LLMs\nTo illustrate the enhancement in the performance of LLMs using parameter-efficient fine-Tuning\nmethods, we employ LoRA (Hu et al., 2022) and P-Tuning V2 (Liu et al., 2021) to fine-tune\nChatGLM-6B, LLaMA2-7B, and LLaMA2-7B-Chat. The results on the Empathetic Dialogues,\nLCCC, ROCStories, LOT, and LCSTS are presented in Table 10 and 11. As shown in the tables,\nthe scores of various metrics significantly improve after fine-tuning the models compared to\nthe non-fine-tuned results. Furthermore, the results in N-grams Matching metrics (BLEU and\nRouge) far surpass the previous SOTA results. This demonstrates that LoRA and P-Tuning V2\ncan substantially enhance the fitting capability of LLMs to datasets without incurring excessive\ncomputational resources.\n5\nConclusion\nIn this paper, we conduct a comprehensive assessment of several existing large-scale language\nmodels (LLMs) in the context of natural language generation (NLG) tasks. Our evaluation\nencompasses English and Chinese datasets to examine the multilingual capabilities of these\nLLMs. The results and analyses from both automatic and manual evaluations of LLMs reveal\nnotable trends and phenomena.\nAcknowledgements\nThis research is supported by the National Natural Science Foundation of China (No.62106105),\nthe CCF-Baidu Open Fund (No.CCF-Baidu202307), the CCF-Zhipu AI Large Model Fund\n(No.CCF-Zhipu202315), the Scientific Research Starting Foundation of Nanjing University of\nAeronautics and Astronautics (No.YQR21022), and the High Performance Computing Platform\nof Nanjing University of Aeronautics and Astronautics.\nComputational Linguistics\nReferences\nArwa Al-Hussain and Aqil M. Azmi. 2022. Automatic story generation: A survey of approaches. ACM\nComput. Surv., 54(5):103:1–103:38.\nMiltiadis Allamanis, Earl T. Barr, Premkumar T. Devanbu, and Charles Sutton. 2018. A survey of machine\nlearning for big code and naturalness. ACM Comput. Surv., 51(4):81:1–81:37.\nYuvanesh Anand, Zach Nussbaum, Brandon Duderstadt, Benjamin Schmidt, and Andriy Mulyar. 2023.\nGpt4all: Training an assistant-style chatbot with large scale data distillation from gpt-3.5-turbo. https:\n//github.com/nomic-ai/gpt4all.\nSatanjeev Banerjee and Alon Lavie. 2005. METEOR: an automatic metric for MT evaluation with\nimproved correlation with human judgments. In Jade Goldstein, Alon Lavie, Chin-Yew Lin, and\nClare R. Voss, editors, Proceedings of the Workshop on Intrinsic and Extrinsic Evaluation Measures for\nMachine Translation and/or Summarization@ACL 2005, Ann Arbor, Michigan, USA, June 29, 2005,\npages 65–72. Association for Computational Linguistics.\nSiqi Bao, Huang He, Fan Wang, Hua Wu, and Haifeng Wang. 2020. PLATO: pre-trained dialogue\ngeneration model with discrete latent variable. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and\nJoel R. Tetreault, editors, Proceedings of the 58th Annual Meeting of the Association for Computational\nLinguistics, ACL 2020, Online, July 5-10, 2020, pages 85–96. Association for Computational Linguistics.\nStella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O’Brien, Eric Hallahan,\nMohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron,\nLintang Sutawika, and Oskar van der Wal. 2023. Pythia: A suite for analyzing large language models\nacross training and scaling. CoRR, abs/2304.01373.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot learners. CoRR, abs/2005.14165.\nHongshen Chen, Xiaorui Liu, Dawei Yin, and Jiliang Tang. 2017. A survey on dialogue systems: Recent\nadvances and new frontiers. SIGKDD Explor., 19(2):25–35.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana\nPillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei,\nKathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. Palm: Scaling\nlanguage modeling with pathways. CoRR, abs/2204.02311.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac\nSuzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Y.\nZhao, Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin,\nAdam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. 2022. Scaling instruction-finetuned language\nmodels. CoRR, abs/2210.11416.\nComputational Linguistics\nTogether Computer. 2023. Redpajama-data: An open source recipe to reproduce llama training dataset.\nErnest Davis and Gary Marcus. 2015. Commonsense reasoning and commonsense knowledge in artificial\nintelligence. Commun. ACM, 58(9):92–103.\nChenhe Dong, Yinghui Li, Haifan Gong, Miaoxin Chen, Junxin Li, Ying Shen, and Min Yang. 2023. A\nsurvey of natural language generation. ACM Comput. Surv., 55(8):173:1–173:38.\nZhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. GLM:\ngeneral language model pretraining with autoregressive blank infilling. In Smaranda Muresan, Preslav\nNakov, and Aline Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022,\npages 320–335. Association for Computational Linguistics.\nWafaa S. El-Kassas, Cherif R. Salama, Ahmed A. Rafea, and Hoda K. Mohamed. 2021. Automatic text\nsummarization: A comprehensive survey. Expert Syst. Appl., 165:113679.\nJonathan Frankle and Michael Carbin. 2019. The lottery ticket hypothesis: Finding sparse, trainable neural\nnetworks. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA,\nUSA, May 6-9, 2019. OpenReview.net.\nXinyang Geng and Hao Liu. 2023. Openllama: An open reproduction of llama, May.\nXiaodong Gu, Kyunghyun Cho, Jung-Woo Ha, and Sunghun Kim. 2019. Dialogwae: Multimodal response\ngeneration with conditional wasserstein auto-encoder. In 7th International Conference on Learning\nRepresentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford,\nDiego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland,\nKatie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen\nSimonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. 2022. Training compute-optimal\nlarge language models. CoRR, abs/2203.15556.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. 2022. Lora: Low-rank adaptation of large language models. In The Tenth International\nConference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net.\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Optimizing continuous prompts for generation. In\nChengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Proceedings of the 59th Annual\nMeeting of the Association for Computational Linguistics and the 11th International Joint Conference\non Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August\n1-6, 2021, pages 4582–4597. Association for Computational Linguistics.\nJingyang Li and Maosong Sun. 2007. Scalable term selection for text categorization. In Jason Eisner,\neditor, EMNLP-CoNLL 2007, Proceedings of the 2007 Joint Conference on Empirical Methods in\nNatural Language Processing and Computational Natural Language Learning, June 28-30, 2007,\nPrague, Czech Republic, pages 774–782. ACL.\nJiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. 2016. A diversity-promoting\nobjective function for neural conversation models. In Kevin Knight, Ani Nenkova, and Owen Rambow,\neditors, NAACL HLT 2016, The 2016 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, San Diego California, USA, June 12-17,\n2016, pages 110–119. The Association for Computational Linguistics.\nComputational Linguistics\nYanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang Cao, and Shuzi Niu. 2017. Dailydialog: A manually\nlabelled multi-turn dialogue dataset. In Greg Kondrak and Taro Watanabe, editors, Proceedings of\nthe Eighth International Joint Conference on Natural Language Processing, IJCNLP 2017, Taipei,\nTaiwan, November 27 - December 1, 2017 - Volume 1: Long Papers, pages 986–995. Asian Federation\nof Natural Language Processing.\nQintong Li, Piji Li, Wei Bi, Zhaochun Ren, Yuxuan Lai, and Lingpeng Kong. 2022. Event transition\nplanning for open-ended text generation. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio,\neditors, Findings of the Association for Computational Linguistics: ACL 2022, Dublin, Ireland, May\n22-27, 2022, pages 3412–3426. Association for Computational Linguistics.\nZhaojiang Lin, Andrea Madotto, Jamin Shin, Peng Xu, and Pascale Fung. 2019. Moel: Mixture of empa-\nthetic listeners. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, Proceedings of the\n2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint\nConference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November\n3-7, 2019, pages 121–132. Association for Computational Linguistics.\nChin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization\nbranches out, pages 74–81.\nXiao Liu, Kaixuan Ji, Yicheng Fu, Zhengxiao Du, Zhilin Yang, and Jie Tang. 2021. P-tuning v2: Prompt\ntuning can be comparable to fine-tuning universally across scales and tasks. CoRR, abs/2110.07602.\nCong Liu. 2020. Chinese newstitle generation project by gpt2.\nYukun Ma, Khanh Linh Nguyen, Frank Z. Xing, and Erik Cambria. 2020. A survey on empathetic\ndialogue systems. Inf. Fusion, 64:50–70.\nRamesh Nallapati, Bowen Zhou, C´ıcero Nogueira dos Santos, C¸ aglar G¨ulc¸ehre, and Bing Xiang. 2016.\nAbstractive text summarization using sequence-to-sequence rnns and beyond. In Yoav Goldberg\nand Stefan Riezler, editors, Proceedings of the 20th SIGNLL Conference on Computational Natural\nLanguage Learning, CoNLL 2016, Berlin, Germany, August 11-12, 2016, pages 280–290. ACL.\nShashi Narayan, Shay B. Cohen, and Mirella Lapata. 2018. Don’t give me the details, just the summary!\ntopic-aware convolutional neural networks for extreme summarization. In Ellen Riloff, David Chiang,\nJulia Hockenmaier, and Jun’ichi Tsujii, editors, Proceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, pages\n1797–1807. Association for Computational Linguistics.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton,\nLuke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan\nLowe. 2022. Training language models to follow instructions with human feedback. In NeurIPS.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic\nevaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for\nComputational Linguistics, July 6-12, 2002, Philadelphia, PA, USA, pages 311–318. ACL.\nJack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, H. Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob\nMenick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth\nRauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan\nUesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Sid-\ndhant M. Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela\nPaganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena\nGribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria\nComputational Linguistics\nTsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao\nGong, Daniel Toyama, Cyprien de Masson d’Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor\nBabuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew J.\nJohnson, Blake A. Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Edward Lockhart, Simon\nOsindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett,\nDemis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. 2021. Scaling language models: Methods,\nanalysis & insights from training gopher. CoRR, abs/2112.11446.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text\ntransformer. J. Mach. Learn. Res., 21:140:1–140:67.\nHannah Rashkin, Eric Michael Smith, Margaret Li, and Y-Lan Boureau. 2019. Towards empathetic\nopen-domain conversation models: A new benchmark and dataset. In Anna Korhonen, David R. Traum,\nand Llu´ıs M`arquez, editors, Proceedings of the 57th Conference of the Association for Computational\nLinguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pages\n5370–5381. Association for Computational Linguistics.\nDavid Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. 2019. Analysing mathematical\nreasoning abilities of neural models. In 7th International Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca: An instruction-following llama model. https:\n//github.com/tatsu-lab/stanford alpaca.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth´ee Lacroix,\nBaptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aur´elien Rodriguez, Armand Joulin,\nEdouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models.\nCoRR, abs/2302.13971.\nYida Wang, Pei Ke, Yinhe Zheng, Kaili Huang, Yong Jiang, Xiaoyan Zhu, and Minlie Huang. 2020. A\nlarge-scale chinese short-text conversation dataset. In Xiaodan Zhu, Min Zhang, Yu Hong, and Ruifang\nHe, editors, Natural Language Processing and Chinese Computing - 9th CCF International Conference,\nNLPCC 2020, Zhengzhou, China, October 14-18, 2020, Proceedings, Part I, volume 12430 of Lecture\nNotes in Computer Science, pages 91–103. Springer.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and\nHannaneh Hajishirzi. 2022. Self-instruct: Aligning language model with self generated instructions.\nCoRR, abs/2212.10560.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V.\nLe, and Denny Zhou. 2022. Chain-of-thought prompting elicits reasoning in large language models. In\nNeurIPS.\nCongchi Yin, Piji Li, and Zhaochun Ren. 2023. Ctrlstruct: Dialogue structure learning for open-domain\nresponse generation. In Ying Ding, Jie Tang, Juan F. Sequeda, Lora Aroyo, Carlos Castillo, and\nGeert-Jan Houben, editors, Proceedings of the ACM Web Conference 2023, WWW 2023, Austin, TX,\nUSA, 30 April 2023 - 4 May 2023, pages 1539–1550. ACM.\nAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,\nWendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Peng\nZhang, Yuxiao Dong, and Jie Tang. 2022. GLM-130B: an open bilingual pre-trained model. CoRR,\nabs/2210.02414.\nComputational Linguistics\nChiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. 2017. Understanding\ndeep learning requires rethinking generalization. In 5th International Conference on Learning Represen-\ntations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net.\nSaizheng Zhang, Emily Dinan, Jack Urbanek, Arthur Szlam, Douwe Kiela, and Jason Weston. 2018.\nPersonalizing dialogue agents: I have a dog, do you have pets too? In Iryna Gurevych and Yusuke Miyao,\neditors, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL\n2018, Melbourne, Australia, July 15-20, 2018, Volume 1: Long Papers, pages 2204–2213. Association\nfor Computational Linguistics.\nChiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. 2021. Understanding\ndeep learning (still) requires rethinking generalization. Commun. ACM, 64(3):107–115.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher\nDewan, Mona T. Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster,\nDaniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. OPT:\nopen pre-trained transformer language models. CoRR, abs/2205.01068.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen\nZhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang,\nRuiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023. A\nsurvey of large language models. CoRR, abs/2303.18223.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\nZhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023a. Judging\nllm-as-a-judge with mt-bench and chatbot arena. CoRR, abs/2306.05685.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\nZhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023b. Judging\nllm-as-a-judge with mt-bench and chatbot arena.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2024-05-16",
  "updated": "2024-05-16"
}