{
  "id": "http://arxiv.org/abs/2405.11512v1",
  "title": "Going into Orbit: Massively Parallelizing Episodic Reinforcement Learning",
  "authors": [
    "Jan Oberst",
    "Johann Bonneau"
  ],
  "abstract": "The possibilities of robot control have multiplied across various domains\nthrough the application of deep reinforcement learning. To overcome safety and\nsampling efficiency issues, deep reinforcement learning models can be trained\nin a simulation environment, allowing for faster iteration cycles. This can be\nenhanced further by parallelizing the training process using GPUs. NVIDIA's\nopen-source robot learning framework Orbit leverages this potential by wrapping\ntensor-based reinforcement learning libraries for high parallelism and building\nupon Isaac Sim for its simulations. We contribute a detailed description of the\nimplementation of a benchmark reinforcement learning task, namely box pushing,\nusing Orbit. Additionally, we benchmark the performance of our implementation\nin comparison to a CPU-based implementation and report the performance metrics.\nFinally, we tune the hyper parameters of our implementation and show that we\ncan generate significantly more samples in the same amount of time by using\nOrbit.",
  "text": "Going into Orbit: Massively Parallelizing Episodic\nReinforcement Learning\nJohann Bonneau\nKarlsruhe Institute of Technology\nKarlsruhe, Germany\nEmail: uivvm@student.kit.edu\nJan Oberst\nKarlsruhe Institute of Technology\nKarlsruhe, Germany\nEmail: uyvch@student.kit.edu\nAbstract—The possibilities of robot control have multiplied\nacross various domains through the application of deep reinforce-\nment learning. To overcome safety and sampling efficiency issues,\ndeep reinforcement learning models can be trained in a simula-\ntion environment, allowing for faster iteration cycles. This can\nbe enhanced further by parallelizing the training process using\nGPUs. NVIDIA’s open-source robot learning framework Orbit\nleverages this potential by wrapping tensor-based reinforcement\nlearning libraries for high parallelism and building upon Isaac\nSim for its simulations. We contribute a detailed description of\nthe implementation of a benchmark reinforcement learning task,\nnamely box pushing, using Orbit. Additionally, we benchmark\nthe performance of our implementation in comparison to a\nCPU-based implementation and report the performance metrics.\nFinally, we tune the hyper parameters of our implementation\nand show that we can generate significantly more samples in the\nsame amount of time by using Orbit.\nI. INTRODUCTION\nNowadays, robotics is a rapidly growing field. At the heart\nof it stands helping and assisting humans. Mainly, it aims at\ndoing jobs under conditions that are hazardous or impossible\nfor humans to operate under and replace humans in tasks that\nare tedious, monotonous, or disagreeable. Successful applica-\ntions range from industrial tasks in warehouse management\n[7] or automotive manufacturing [2] over medical tasks such\nas robot-assisted surgery [4] to social tasks in teaching [3] or\ngeriatric care [5]. While in some fields, robots are controlled\nby humans or perform simple, directly programmable tasks,\nmore sophisticated methods are required to enable robots to\nsolve complex tasks autonomously.\nTo accomplish this, Reinforcement Learning (RL), a sub-\nfield of machine learning, has proven to be highly successful\n[19]. In numerous applications within robotics, tasks can\nnaturally be modeled as RL problems [11]. The concept behind\nRL involves an agent interacting with an environment through\nactions and receiving feedback, referred to as reward [18].\nThe goal for the agent is to develop a policy based on\nwhich it decides which action to take in a specific scenario.\nConsequently, the agent learns to solve the task through trial-\nand-error by maximizing the cumulative reward, referred to\nas return. The basic approach is to provide feedback to the\nagent in a continuous fashion by updating its policy on each\nstep [18]. While some tasks with a long span such as process-\ncontrol go on continuously, others can be divided into closed\nsequences that end when a terminal state is reached [18]. These\nsequences of states and actions are called episodes and thus the\nlearning process is called episodic RL. Common examples of\nepisodic RL problems are the successful completion of games\nlike Go, StarCraft, Atari games, or Tetris.\nWhen applying RL methods to robotic applications, often\na large number of samples is required [19]. Instead of per-\nforming the robot actions in a real environment, it is common\npractice to perform the training in a simulation first and\ntransfer the result to a real setup [19]. This not only speeds\nup the training process but also alleviates safety concerns\nwith real robots [19]. There are many existing simulation\nenvironments, e.g. popular ones such as Gazebo1, MuJoCo2,\nPyBullet3, and Webots4, all having different advantages and\ncaveats [6]. In contrast, Isaac Sim - a robotics simulator\nby NVIDIA - has been recognized for its particularly good\nintegration with NVIDIA hardware. It is however still in\nits developmental stages. With Isaac Sim remaining closed-\nsource, contributing to the simulator and creating a unified\nframework for research poses challenges for users [8]. There-\nfore, NVIDIA recently released Orbit, a unified framework\nfor interactive robot learning [10]. Its goal is to allow the\ncommunity to collaboratively push forward in developing\nbenchmarks and robot learning systems. We contribute to\nthis goal by implementing a benchmark learning environment,\nnamely box pushing, laying the foundation for the evaluation\nof Orbit and the ecosystem surrounding it in the future. We\nadditionally provide a black-box wrapper for the environment\nfor use with movement primitives (MP). We successfully train\nagents in both scenarios and report the performance metrics.\nFor the comparison in the evaluation, we choose Fancy Gym5\n[12], a framework for robotic reinforcement learning using\nMuJoCo for its simulations. Therefore, we closely follow the\nimplementation of the box pushing environment in Fancy Gym\nto really highlight the difference in simulation capabilities of\nthe highly parallelized Isaac Sim compared to the CPU-based\nMuJoCo. For our evaluation, we take the training results of\nthe box pushing environment using Fancy Gym from [13].\n1Gazebo: gazebosim.org\n2MuJoCo: mujoco.org\n3PyBullet: pybullet.org\n4Webots: cyberbotics.com\n5Fancy Gym: github.com/ALRhub/fancy gym\narXiv:2405.11512v1  [cs.RO]  19 May 2024\nFig. 1.\nRelationship of the components for the experiments in this study.\nGreen: The simulation framework unifies the environments, learning libraries,\nand simulator to create a robot learning environment. Gray: Specific compo-\nnents used in this study.\nII. PROBLEM STATEMENT\nIn the following, we introduce Orbit and the ecosystem\nsurrounding it. An overview of the components used in the\nexperiments of this work are described subsequently. Finally,\na detailed description of the created benchmark environment\nis provided.\nA. Structure of Orbit\nOrbit aims at allowing developers to efficiently create robot\nlearning environments with fast and realistic physics in a\nstraightforward way. It is powered by NVIDIA Isaac Sim, an\nextensible robot simulator which aims at providing physically\naccurate environments. Isaac Sim runs on top of the NVIDIA\nOmniverse platform, building on the Open Univeral Scene\nDescription (OpenUSD) file format and NVIDIA’s raytracing\ntechnology NVIDIA RTX. Overall, Orbit offers fourteen robot\narticulations, three different physics-based sensors, twenty\nlearning environments, and wrappers to five different learning\nframeworks – namely RL Games, robomimic, RSL RL, Stable-\nBaselines3, and skrl [10]. Since both RL Games and RSL RL\nare optimized for GPUs, a training speed of 50,000-75,000\nframes per second (FPS) with 2048 environments is reached\nby the authors of [10].\nThe documentation of Orbit mentions an interesting feature\nof Isaac Sim: It allows developers to provide a seed and enable\ndeterministic execution to make simulation runs reproducible\n[9]. However, we could not confirm this reproducibility in our\nexperiments. As mentioned in [9], the root cause for this could\nbe the variance in GPU work scheduling which results in a\nvarying order of execution and finally, leading to diverging\nenvironments.\nThe different components used for the experiments in this\nstudy are depicted in Fig. 1. The box pushing environment is\ndefined, describing the physical surroundings and the actions\nthat an agent can take. The exact setup is described in Sec-\ntion II-B. Additionally, the machine learning library RSL RL is\nused to model and train the agent. Subsequently, Orbit serves\nFig. 2.\nBox pushing environment: The agent is supposed to push the box\ninto a designated goal pose by moving the table-mounted Franka robot arm.\nas a middleman between the different components involved\nin the learning process: The agent’s actions are sampled by\nRSL RL and the resulting state transitions are simulated in\nthe defined box pushing environment using Isaac Sim. The\ngenerated samples are in turn used by RSL RL to improve the\nagent’s policy. This process is repeated until the maximum\nnumber of training epochs is reached.\nB. Benchmark Environment Setup\nWe choose the box pushing task for our implementation\nbecause it is a good example for tasks that can be solved\nboth in a step-based manner and using movement primitives.\nBesides that, the box pushing domain is a commonly used\nbenchmark task for evaluating RL methods in robotics (e. g.\n[1], [14], [16]). For this task, the objective of the agent is\nto push the box from its initial position on the table to a\ntarget pose by actuating the robot arm’s joints [1]. This target\npose consists of a position on the table’s surface and an\norientation. The structure of the deduced environment can be\ndivided into two parts: the physical arrangement of objects\nand their properties in the simulator and the learning specific\ncomponents.\nPhysical arrangement. The Franka robot arm is mounted\nto a table and its end effector is complemented by a push rod\nas depicted in Fig. 2. The box consists of five quadratic plates\nwith an edge length of ten centimeters resulting in a cube\nwith an opening for the robot’s rod to push the box. The box\nis placed on the table surface and the robot arm starts with\nthe push rod inside the box. While the initial position and the\norientation of the box as well as the robot arm’s initial position\nare fixed, the box’s target pose is generated randomly.\nReinforcement learning components. We implement a\nreward function, termination criteria, and commands. The\nlength of an episode is limited by two seconds, after which\nthe environment is reset. As our goal was not to optimize\nfor an optimal learning environment but rather to create a\nbenchmark for comparability between different simulators,\nwe closely followed the implementation used by [13]. The\nimplementation details of the environment setup are described\nin the following section.\nFig. 3. Overview of the main file structure of the box pushing environment in Orbit (blue: folders, gray: files, Note: The MDP folder and the contained files\nare not represented as they are not relevant for the configuration of the environment).\nIII. IMPLEMENTATION\nThe implementation is divided into two parts. First, we\nimplement a step-based environment for the box pushing task.\nSecond, we implement a black-box wrapper for the step-\nbased box pushing environment in order to use movement\nprimitives. In this chapter, we explain how to perform these\nimplementations using the simulation framework Orbit.\nA. Step-Based Environment\nThe structure of the step-based environment is shown in\nFig. 3. In the assets folder, the files related to the assets\nof the scene and the configuration script for our robot can be\nfound. This script configures the initial state of the robot, the\nparameters of the PD-controller and the limits of the actuator\ngroups. In our environment, we use implicit actuators. The\nmain part of the configuration of the environment takes place\nin box_pushing_env_cfg.py. This script is extended\nwith additional scripts in the Config folder which configure\nwhether to perform the task in the joint space or in the task\nspace and finalize the configuration.\nThe configuration consists of implementing a configura-\ntion class for the components of the environment in the\nbox_pushing_env_cfg.py script. These configuration\nclasses are then instantiated in the main configuration class.\nA code snippet of the configuration script is shown in the\nappendix in Listing A. Configuring the components is simple\nas it consists of defining variables containing instances of\nconfiguration classes. The implementation of a component of\nthe environment is explained in Section III-A1 for the scene\ndesign.\nIn Orbit, the configuration of the step-based RL environ-\nment can be divided in two parts: configuration of the base\nenvironment which is then supplemented to support RL.\n1) Creating a Base Environment:\nScene definition. The first step to creating the base environ-\nment is to design the scene. Therefore, we recreate the exper-\niment setup in the simulator. We use already existing assets\nfor the table, the ground plane and the light. The OpenUSD\nfile for assets provided by Isaac Sim are stored in a Nucleus\ndatabase. For this scene, we use a customized OpenUSD file\nfor the 7-DoF Franka robotic arm with a cylinder placed in the\ngripper acting as a push rod. Finally, we recreated the box used\nby Fancy Gym. The box files in Fancy Gym are stored in the\nMJCF format but we encountered difficulties using the MJCF\nconverter of Isaac Sim. We therefore recreated the box with a\nCAD software and then imported the STEP file in Isaac Sim\nin order to save it to a OpenUSD file. The custom OpenUSD\nfiles we created are stored locally in the assets folder. In the\nappendix in Listing A, we show how the scene is configured\nin Orbit. We simply add variables containing the configuration\nclasses of the assets in the scene. The variables for the robot\nand the manipulated object are left empty to enable further\ncustomization without having to define multiple scenes.\nActions. After creating the scene, we define the actions of\nthe robot. In other terms, we define what type of controller is\nused to input actions to the robot. Orbit allows us to define\ndifferent controllers for different sets of joints. We use a joint\nposition controller for the arm and hand joints. No controller\nfor the gripper joints is assigned as they are not used in our\nexperiment.\nObservation. We then define the states of the environment\nthat can be observed by an agent. In Orbit, we can define\nmultiple observation groups that serve different purposes. We\nonly use the default group named ”policy” which is used to\nobserve the scene after an episode is finished. The observations\nrelevant to our environment are:\n• the joint positions,\n• the joint velocities,\n• the object pose,\n• the target pose and\n• the last action performed.\nRandomization. To finalize the base environment, we con-\nfigure the randomization. At the start, the push rod has to be\ninside the box. Randomizing the initial box position would\ntherefore require the computation of the joint angles with\ninverse kinematics at the start of every episode. We therefore\nfix the initial box position and only randomize the target pose.\nA random initial box position could however be subject to\nfuture work.\n2) Creating a RL Environment:\nTo allow for the environment to be used for RL tasks, it is\ncomplemented with a reward function, termination criteria,\nContext\nc\nPolicy\nπθ(w|c)\nDesired Trajectory\nτ d = Ψ(w) = (sd\n1, sd\n2, . . . , sd\nT )\nController\nf(st, sd\nt )\nEnvironment\nfor t in 1, 2, . . . , T\nat\nst+1\nFig. 4. Figure taken from [13]. Overview of the proposed Black-Box Reinforcement Learning (BBRL) framework. The normal distributed policy predicts,\ngiven the context c, the parameters of a movement primitive that translates to a desired trajectory τ d. A trajectory tracking controller f generates low-level\nactions at given the current state st and the desired state sd\nt from the trajectory τ d.\ncommands, and curriculums.\nReward. The reward function is implemented by defining\nreward terms. The terms use a function to compute a return\ngiven the observation. Given the reward terms R0, . . . , Rn and\ntheir weights w0, . . . , wn, the total reward R is:\nR =\nn\nX\ni=0\nwi · Ri\nWe implement the dense reward from [13]. The reward\nterms are:\n• Rgoal: the distance between the box position and the\ntarget position.\n• Rrot: the angular distance between the box orientation\nand the target orientation around the yaw-axis.\n• Ren: the energy cost.\n• Rlim: the sum of the values exceeding the joint position\nand velocity limits\n• Rrod: the distance between the tip of the push rod and\nthe box.\n• Rrod rot: the distance between the current orientation and\nthe target orientation of the rod. The goal of this term is\nto keep the push rod pointing down.\nThe total reward R is\nR = −Rrod−Rrod rot−5×10−4Ren−Rlim−2Rrot−3.5Rgoal.\nTermination Criteria. An episode can either be terminated\nby a timeout or be truncated by a condition that becomes true.\nThe timeout is set to 2 seconds. Additionally, we implement\nthe success condition from [13]. An episode ”is considered\nsuccessfully solved if the position distance ≤0.05 m and the\norientation error ≤0.5 rad”. This condition is however not\nused for truncation of an episode. Since the box’s velocity is\nnot regarded by the condition, the agent would launch the box\nto the target pose as quickly as possible. As our goal is for the\nagent to move the box to the target pose in a controlled way,\nthis behavior is undesired and no truncation is performed.\nCommands. The commands are used for goal condition\ntasks. They are resampled at the end of every episode and\ncan also be resampled after a specified time period. We\nimplemented a command that samples a new box target pose.\nAs in [13], the command uniformly samples the pose between\n[0.3; 0.6] for the X-axis, [−0.45; 0.45] for the Y-axis and\n[0; 2π] for the yaw-axis.\nCurriculum. Orbit allows to implement curriculum learning\nby implementing curriculum terms. The box pushing task does\nnot require curriculum learning.\nB. Black-Box Environment\nTasks involving physical interaction oftentimes can be di-\nvided into different action phases [17]. By solving separate\nsubtasks – so called movement primitives – individually, the\nsearch space is reduced drastically making the training process\nboth more efficient and more robust. Therefore, we wrap the\nexisting step-based environment to allow for episodic RL using\nmovement primitives.\n1) Black-Box Reinforcement Learning: The goal of BBRL\nis to learn a policy πθ(w|c) where c is the context and w is\na weight vector used to get the reference trajectory Ψ(w) for\nthe episode. To get the context, we apply a context mask to\nthe initial observation of the episode as follows:\nc = m ∗o0\nwith m ∈{0, 1}n being the mask and o0 being the ob-\nservation vector of dimension n. The weight vector is used\nby a trajectory generator Ψ to get the reference trajectory\nΨ(w) = [st]t=0:T . In our work, we use Probabilistic Move-\nment Primitives (ProMP) [15] to generate the trajectories.\nAs depicted in Fig. 5, the BBRL environment acts as a\nblack-box by wrapping around the step-based environment.\nDuring an episode, the weight vector w and the context c\nare taken as input, based on which the reference trajectory is\ngenerated and subsequently executed step-by-step to return the\nfinal reward. At each step the rewards are collected and then\nused to compute the final reward of the episode (e.g. the sum\nof the collected rewards).\n2) MP-based environment: With the step-based environ-\nment being implemented as explained in Section III-A, a\nwrapper around the step-based environment is implemented\nto provide a BBRL environment. The goal of the wrapper is\nto apply the context mask to the observation vector and to\noverride the step function. The step function executes a tra-\njectory using a given weight vector and returns the reward. The\ninitialization of the environment follows these steps. First, we\ninitialize the step-based environment as usual. Then, we gather\nthe configurations of the different elements of the BBRL\nenvironment e.g. the parameters of the trajectory generator or\nthe length of a trajectory. We then initialize these components\nto finally return the BBRL environment implementing the\nsame functions as the step-based environment. This allows\nus to use both environments interchangeably with the same\ntraining script by just changing the used environment and the\nalgorithm hyper parameters.\nFig. 5. Overview of the interaction of step-based and black-box reinforcement learning (BBRL) agents with their environment. Right: Step-based reinforcement\nlearning agent performing a step. Left: Black box reinforcement learning agent performing a step.\nStep Function. A step in the BBRL environment represents\nan entire episode in the step-based environment. The first\ntask is to get the reference trajectory. Therefore, we use the\ntrajectory generator that was initialized with its parameters.\nThe trajectory generator takes as input:\n• the start step ts (in our case ts = 0),\n• the current joint angles,\n• the current joint velocities,\n• the length of the trajectory T (in our case T = 100 steps),\nand returns a list of actions to be executed. The trajectory\nis then executed using the step-based environment. At each\nstep t of the trajectory, the current reward rt is computed and\ncollected. After the trajectory is executed, we compute the\nfinal reward R which is the sum of all collected rewards:\nR =\nT\nX\nt=ts\nrt.\nObservation Function. As explained in Section III-B1,\nthe observation is computed by taking the observation of the\nstep-based environment and applying the context mask. The\nreturned vector contains only the values filtered by the context\nmask.\nTo realize this, we used the wrapper in Fancy Gym that was\nalready implemented and modified it to execute operations on\na GPU with PyTorch. We extended this wrapper to use the\nconfigurations and context mask for our task. We also override\nthe functions that are responsible for returning the current joint\nangles and velocities in order to get these values from Isaac\nSim.\nIV. RESULTS & EVALUATION\nThe evaluations of both the step-based and the MP-based\nimplementation are performed in two steps. First, we run the\ntraining applying the hyper parameters used by [13] to our\nsetting for a benchmark and compare their performance in\nterms of success rate per number of environment interactions.\nSecond, we adapt the hyper parameters to better fit the\ncapabilities of Orbit and demonstrate the strength of its high\nparallelism and good integration with NVIDIA hardware.\nComponent\nSpecification\nProcessor\nAMD Ryzen 9 7900X\nGPU\nNVIDIA GeForce RTX 4080 16GB\nRAM\n64GB\nOS Version\nUbuntu 22.04.4 LTS\nPython Version\n3.10\nPyTorch\n2.1.0\nCUDA\n11.7\nTABLE I\nEXPERIMENT SETUP SPECIFICATIONS\nFor all experiments, the hardware and software versions\ndisplayed in Table I are used. Per experiment, ten trials are\nperformed.\nA. Benchmark: Orbit vs. Fancy Gym\nFor the first part of the evaluation, the comparison between\nFancy Gym and Orbit for the step-based and the BBRL\nenvironments is covered. In order to produce comparable\nresults, we set the number of environments to 160 and the\nnumber of steps per epoch to 100 in Orbit to match the 16000\nsamples performed at each learning iteration for the training\nof the Fancy Gym step-based environment. For the BBRL\nenvironments, the number of steps per epoch is set to 1. The\nfull set of hyper parameters can be found in the appendix.\nStep-based benchmark. Looking at the results of the\ntraining in Fig. 7, similar success rates for the step-based\nenvironments of Fancy Gym and Orbit can be observed.\nHowever, some differences are worth noting: First, the training\nof the step-based environment using Orbit achieves a higher\nsuccess rate at approximately 1.2 · 107 environment interac-\ntions, decreasing from that point onwards. In contrast, the\nsuccess rate of the Fancy Gym step-based environment does\nnot stop increasing to finally surpass the success rate of the\nOrbit step-based environment. A possible explanation for these\ndifferences could be that we use different learning libraries for\nthe training.\n0\n20\n40\n60\n80\n100\n0.2\n0.25\n0.3\n0.35\n0.4\n0.45\n0.5\nsteps\njoint angle\njoint 0\n0\n20\n40\n60\n80\n100\n−0.1\n0\n0.1\n0.2\n0.3\nsteps\njoint angle\njoint 1\n0\n20\n40\n60\n80\n100\n−0.1\n0\n0.1\n0.2\n0.3\nsteps\njoint angle\njoint 2\n0\n20\n40\n60\n80\n100\n−2.6\n−2.4\n−2.2\n−2\nsteps\njoint angle\njoint 3\n0\n20\n40\n60\n80\n100\n−0.2\n−0.15\n−0.1\n−5 · 10−2\n0\n5 · 10−2\nsteps\njoint angle\njoint 4\n0\n20\n40\n60\n80\n100\n2\n2.2\n2.4\n2.6\n2.8\n3\n3.2\nsteps\njoint angle\njoint 5\n0\n20\n40\n60\n80\n100\n1.2\n1.3\n1.4\n1.5\n1.6\n1.7\nsteps\njoint angle\njoint 6\nReference trajectory\nActual trajectory: Fancy Gym\nActual trajectory: Orbit\nFig. 6. The reference trajectory and the actual trajectories from Fancy Gym and Orbit for each joint of the Franka arm.\nWe can conclude that Orbit can achieve a similar success\nrate compared to Fancy Gym when training the box pushing\ntask using step-based RL.\nMP-based benchmark. The results of the training using the\nBBRL environments are also depicted in Fig. 7. A difference in\nthe success rate during the training can be observed. While the\nenvironment of Fancy Gym reaches a success rate of around\n0.36 at 4 · 107 environment interactions the environment of\nOrbit is only able to achieve a success rate of 0.1.\nThis difference can possibly be explained by the difference\nin the behavior of the robotic arm in MuJoCo and Isaac Sim.\nGiven the same weight vector w, the reference trajectories of\nOrbit and Fancy Gym are approximately the same:\nΨOrbit(w) ≈ΨF ancyGym(w).\nThe difference between both simulators lies in the actual\ntrajectories. The actual trajectory is the trajectory that is exe-\ncuted in the simulators based on the given reference trajectory.\nThe actual trajectory is composed of the joint angles of the\nrobot after executing a step with an action from the reference\ntrajectory. Fig. 6 shows the joint angle values of the reference\ntrajectory and the actual trajectories of Fancy Gym and Orbit.\nWe can note that the actual trajectories are not the same. That\nshows a difference in behavior of the robots in the simulator\nthat could explain the difference in the success rate of the\nblack-box environments of Fancy Gym and Orbit.\nB. Tuning Orbit\nTo demonstrate the capabilities of Orbit, we performed\nthe training of the box pushing environment using the hyper\nparameters of a similar task that is already implemented in\nOrbit. The hyper parameters are presented in Table II. A\ncomplete overview of the hyper parameters is provided in the\n0\n1\n2\n3\n4\n·107\n0\n0.2\n0.4\n0.6\n0.8\n1\nNumber Environment Interactions\nSuccess Rate\nBox Pushing - Dense reward\nFancy Gym step-based\nOrbit step-based\nFancy Gym BB\nOrbit BB\nFig. 7. The success rate of the training experiments for the 4 environments\nrelative to the number of environment interactions.\nappendix. The biggest strength of Orbit is that it is able to\nrun multiple environments on one simulation instance on one\nGPU. In this case, the training is performed by using 4096\nenvironments in parallel.\nThis parallelism capability is particularly notable when\nlooking at the training time and the number of interactions\nthat were performed in that time. On Fig. 8a, we can see\nthat the success rate of the BBRL environment using 4096\nenvironments reaches values close to 0.93 after 50 minutes\nof training time compared to 0.1 for 160 environments. Thus,\nwith the same training time, we can achieve a higher success\n(a) The success rate during the training for both sets of\nhyper parameters relative to the training time\n(b) The amount of environment interactions during the\ntraining for both sets of hyper parameters relative to the\ntraining time\nFig. 8. Comparing the success rate and the number of environment interactions for the two set of hyper parameters\nPrevious BBRL-HP\nTuned Orbit BBRL-HP\nnumber samples\n160\n4096\n# parallel env. in Orbit\n160\n4096\nhidden layers\n[256, 256]\n[256, 128, 64]\nhidden layers critic\n[256, 256]\n[256, 128, 64]\nhidden activation\ntanh\nelu\nTABLE II\nCOMPARISON OF HYPER PARAMETERS. FIRST COLUMN: HP PREVIOUSLY\nUSED IN SECTION IV-A FOR THE BENCHMARK WITH FANCY GYM.\nSECOND COLUMN: HP USED TO SHOW THE CAPABILITIES OF ORBIT.\nrate. This is because with 4096 parallel environments, Orbit\nis able to gather more samples in the same amount of time as\nshown in Fig. 8b.\nTo conclude, by increasing the number of environments\nused for the training, we can achieve a higher success rate\nfor BBRL. This showcases the capabilities of Orbit and Isaac\nSim when dealing with RL tasks.\nV. CONCLUSION & OUTLOOK\nThis study presented a detailed exploration of the applica-\ntion of reinforcement learning in the rapidly growing field of\nrobotics by developing and evaluating a benchmark learning\nenvironment within NVIDIA’s Orbit framework. Through the\nimplementation of a box pushing task using a Franka robot\narm in a simulated environment, this work demonstrated the\npotential of reinforcement learning in enabling robots to per-\nform complex tasks autonomously. By leveraging NVIDIA’s\nIsaac Sim for the simulation, and Orbit for the integration and\nexecution of reinforcement learning algorithms, we success-\nfully established a benchmark for evaluating the efficiency and\neffectiveness of reinforcement learning approaches in robotic\napplications.\nOur findings reveal that using Orbit for the application of re-\ninforcement learning to robotics holds significant promise for\nadvancing the capabilities of robots. Overall, the importance of\nsimulation in the training process is underscored for enhancing\nthe learning efficiency and accelerating iteration cycles.\nDespite the promising results, our experiments also high-\nlighted challenges related to reproducibility and the variance\nin behaviors between different simulators, suggesting areas for\nfurther refinement. The introduction of the Orbit framework\nrepresents a substantial step forward in the collaborative\ndevelopment of robot learning systems and also opens up new\navenues for research.\nFuture research should focus on overcoming these chal-\nlenges. On the one hand, the implemented environment could\nbe expanded to create a more sophisticated simulation en-\nvironment that can more accurately mirror the complexities\nof the real world. For example, a random initial pose of\nthe robot and the box could be modeled in the future. On\nthe other hand, expanding the scope of benchmark tasks and\nexploring other benchmark environments to further investigate\nthe capabilities of different simulators could be subject to\nfuture research. More sophisticated benchmarks and evaluation\nof Orbit compared to other simulation frameworks is highly\nencouraged to carve out the strengths and limitations of Orbit.\nAdditionally, other reinforcement learning techniques could\nbe explored such as Trust Region Project Layers. Ultimately,\nthe continued evolution of reinforcement learning in robotics\npromises to significantly expand the boundaries of what is\nachievable, paving the way for robots to become even more\nintegral and beneficial components of human society.\nACKNOWLEDGMENT\nThe authors would like to thank their supervisors, Tai Hoang\nand Ge Li, for their valuable guidance, support, and insightful\nfeedback throughout the course of this project. Their expertise\nwas inspiring and significantly contributed to the successful\ncompletion of this report.\nREFERENCES\n[1] Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel\nFong, Peter Welinder, Bob McGrew, Josh Tobin, OpenAI Pieter Abbeel,\nand Wojciech Zaremba. Hindsight experience replay. Advances in neural\ninformation processing systems, 30, 2017.\n[2] Michal Bartoˇs, Vladim´ır Bulej, Martin Bohuˇs´ık, J´an Stanˇcek, Vitalii\nIvanov, and Peter Macek.\nAn overview of robot applications in\nautomotive industry.\nTransportation Research Procedia, 55:837–844,\n2021.\n[3] Fabiane Barreto Vavassori Benitti. Exploring the educational potential\nof robotics in schools: A systematic review. Computers & Education,\n58(3):978–988, 2012.\n[4] Nima Enayati, Elena De Momi, and Giancarlo Ferrigno.\nHaptics\nin robot-assisted surgery: Challenges and benefits.\nIEEE reviews in\nbiomedical engineering, 9:49–65, 2016.\n[5] Anne-Katrin Haubold, Lisa Obst, and Franziska Bielefeldt. Introducing\nservice robotics in inpatient geriatric care—a qualitative systematic\nreview from a human resources perspective. Gruppe. Interaktion. Organ-\nisation. Zeitschrift F¨ur Angewandte Organisationspsychologie (GIO),\n51(3):259–271, 2020.\n[6] Marian K¨orber, Johann Lange, Stephan Rediske, Simon Steinmann, and\nRoland Gl¨uck. Comparing popular simulation environments in the scope\nof robotics and reinforcement learning. 2021.\n[7] Carman KM Lee. Development of an industrial internet of things (iiot)\nbased smart robotic warehouse management system.\nIn CONF-IRM\n2018 Proceedings. 43, 2018.\n[8] Mayank Mittal.\nOrbit - frequently asked questions, 2024.\nhttps:\n//isaac-orbit.github.io/orbit/source/refs/faq.html [Accessed: (March 16,\n2024)].\n[9] Mayank Mittal. Orbit - known issues, 2024. https://isaac-orbit.github.\nio/orbit/source/refs/issues.html#non-determinism-in-physics-simulation\n[Accessed: (March 07, 2024)].\n[10] Mayank Mittal, Calvin Yu, Qinxi Yu, Jingzhou Liu, Nikita Rudin,\nDavid Hoeller, Jia Lin Yuan, Ritvik Singh, Yunrong Guo, Hammad\nMazhar, Ajay Mandlekar, Buck Babich, Gavriel State, Marco Hutter, and\nAnimesh Garg. Orbit: A unified simulation framework for interactive\nrobot learning environments. IEEE Robotics and Automation Letters,\n8(6):3740–3747, 2023.\n[11] Eduardo F Morales, Rafael Murrieta-Cid, Israel Becerra, and Marco A\nEsquivel-Basaldua. A survey on deep learning and deep reinforcement\nlearning in robotics with a tutorial on deep reinforcement learning.\nIntelligent Service Robotics, 14(5):773–805, 2021.\n[12] Fabian Otto, Onur Celik, Dominik Roth, and Hongyi Zhou. Fancy gym.\n[13] Fabian Otto, Onur Celik, Hongyi Zhou, Hanna Ziesche, Vien Anh Ngo,\nand Gerhard Neumann.\nDeep black-box reinforcement learning with\nmovement primitives. In Conference on Robot Learning, pages 1244–\n1265. PMLR, 2023.\n[14] Fabian Otto, Hongyi Zhou, Onur Celik, Ge Li, Rudolf Lioutikov, and\nGerhard Neumann.\nMp3: Movement primitive-based (re-) planning\npolicy. arXiv e-prints, pages arXiv–2306, 2023.\n[15] Alexandros Paraschos, Christian Daniel, Jan R Peters, and Gerhard\nNeumann.\nProbabilistic movement primitives.\nAdvances in neural\ninformation processing systems, 26, 2013.\n[16] Matthias Plappert, Marcin Andrychowicz, Alex Ray, Bob McGrew,\nBowen Baker, Glenn Powell, Jonas Schneider, Josh Tobin, Maciek\nChociej, Peter Welinder, et al.\nMulti-goal reinforcement learning:\nChallenging robotics environments and request for research. arXiv e-\nprints, pages arXiv–1802, 2018.\n[17] Freek Stulp, Evangelos A Theodorou, and Stefan Schaal. Reinforcement\nlearning with sequences of motion primitives for robust manipulation.\nIEEE Transactions on robotics, 28(6):1360–1370, 2012.\n[18] Richard S Sutton and Andrew G Barto. Reinforcement learning: An\nintroduction. MIT press, 2018.\n[19] Wenshuai Zhao, Jorge Pe˜na Queralta, and Tomi Westerlund. Sim-to-real\ntransfer in deep reinforcement learning for robotics: a survey. In 2020\nIEEE symposium series on computational intelligence (SSCI), pages\n737–744. IEEE, 2020.\nAPPENDIX\nA. Environment configuration\n1 @configclass\n2 class ObjectTableSceneCfg(InteractiveSceneCfg):\n3\n# robots: will be populated\n4\nrobot: ArticulationCfg = MISSING\n5\n# end-effector sensor: will be populated\n6\nee_frame: FrameTransformerCfg = MISSING\n7\n# target object: will be populated\n8\nobject: RigidObjectCfg = MISSING\n9\n# Table\n10\ntable = AssetBaseCfg(...)\n11\n# plane\n12\nplane = AssetBaseCfg(...)\n13\n# lights\n14\nlight = AssetBaseCfg(...)\n15\n16 @configclass\n17 class CommandsCfg:\n18\n...\n19\n20 @configclass\n21 class ActionsCfg:\n22\n...\n23\n24 @configclass\n25 class ObservationsCfg:\n26\n...\n27\n28 @configclass\n29 class RandomizationCfg:\n30\n...\n31\n32 @configclass\n33 class RewardsCfg:\n34\n...\n35\n36 @configclass\n37 class TerminationsCfg:\n38\n...\n39\n40 @configclass\n41 class BoxPushingEnvCfg(RLTaskEnvCfg):\n42\n43\n# Scene settings\n44\nscene: ObjectTableSceneCfg = ObjectTableSceneCfg(...)\n45\n# Basic settings\n46\nobservations: ObservationsCfg = ObservationsCfg()\n47\nactions: ActionsCfg = ActionsCfg()\n48\ncommands: CommandsCfg = CommandsCfg()\n49\n# MDP settings\n50\nrewards: RewardsCfg = RewardsCfg()\n51\nterminations: TerminationsCfg = TerminationsCfg()\n52\nrandomization: RandomizationCfg = RandomizationCfg()\n53\n54\ndef __post_init__(self):\n55\n# Sim parameters\n56\n...\n57\n# PhysX parameters\n58\n...\nB. Hyper parameters\nTABLE III\nHYPERPARAMETERS FOR THE BOX PUSHING EXPERIMENTS.\nstep-based PPO Fancy Gym HP\nstep-based PPO Orbit HP\nBBRL PPO Fancy Gym HP\nBBRL PPO Orbit HP\nnumber samples\n16000\n98304\n160\n4096\nnumber parallel environments\n160\n4096\nn.a.\n4096\nGAE λ\n0.95\n0.95\nn.a.\n0.95\ndiscount factor\n0.99\n0.98\nn.a.\n0.98\nepochs\n10\n10\n100\n100\nlearning rate\n1e-4\n1e-4\n1e-4\n1e-4\nuse critic\nTrue\nTrue\nTrue\nTrue\nepochs critic\n10\n5\n100\n100\nlearning rate critic (and alpha)\n1e-4\n1e-4\n1e-4\n1e-4\nnumber minibatches\n40\n4\n1\n1\nnormalized observations\nTrue\nTrue\nFalse\nFalse\nnormalized rewards\nTrue\nn.a.\nFalse\nn.a.\nobservation clip\n10.0\nn.a.\nn.a.\nn.a.\nreward clip\n10.0\nn.a.\nn.a.\nn.a.\ncritic clip\n0.2\nn.a.\n0.2\nn.a.\nimportance ratio clip\n0.2\nn.a.\n0.2\nn.a.\nhidden layers\n[256, 256]\n[256, 128, 64]\n[128, 128]\n[256, 128, 64]\nhidden layers critic\n[256, 256]\n[256, 128, 64]\n[32, 32]\n[256, 128, 64]\nhidden activation\ntanh\nelu\ntanh\nelu\ninitial std\n1.0\n1.0\n1.0\n1.0\nnumber basis functions\nn.a.\nn.a\n5\n5\nnumber zero basis\nn.a.\nn.a.\n1\nn.a.\nweight scale\nn.a.\nn.a.\nn.a.\n0.3\n",
  "categories": [
    "cs.RO"
  ],
  "published": "2024-05-19",
  "updated": "2024-05-19"
}