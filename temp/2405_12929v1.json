{
  "id": "http://arxiv.org/abs/2405.12929v1",
  "title": "Code-mixed Sentiment and Hate-speech Prediction",
  "authors": [
    "Anjali Yadav",
    "Tanya Garg",
    "Matej Klemen",
    "Matej Ulcar",
    "Basant Agarwal",
    "Marko Robnik Sikonja"
  ],
  "abstract": "Code-mixed discourse combines multiple languages in a single text. It is\ncommonly used in informal discourse in countries with several official\nlanguages, but also in many other countries in combination with English or\nneighboring languages. As recently large language models have dominated most\nnatural language processing tasks, we investigated their performance in\ncode-mixed settings for relevant tasks. We first created four new bilingual\npre-trained masked language models for English-Hindi and English-Slovene\nlanguages, specifically aimed to support informal language. Then we performed\nan evaluation of monolingual, bilingual, few-lingual, and massively\nmultilingual models on several languages, using two tasks that frequently\ncontain code-mixed text, in particular, sentiment analysis and offensive\nlanguage detection in social media texts. The results show that the most\nsuccessful classifiers are fine-tuned bilingual models and multilingual models,\nspecialized for social media texts, followed by non-specialized massively\nmultilingual and monolingual models, while huge generative models are not\ncompetitive. For our affective problems, the models mostly perform slightly\nbetter on code-mixed data compared to non-code-mixed data.",
  "text": "1\nCode-mixed Sentiment and Hate-speech Prediction\nAnjali Yadav, Tanya Garg, Matej Klemen, Matej Ulˇcar,\nBasant Agarwal, Marko Robnik-ˇSikonja\nAbstract—Code-mixed discourse combines multiple languages\nin a single text. It is commonly used in informal discourse\nin countries with several official languages, but also in many\nother countries in combination with English or neighboring\nlanguages. As recently large language models have dominated\nmost natural language processing tasks, we investigated their\nperformance in code-mixed settings for relevant tasks. We first\ncreated four new bilingual pre-trained masked language models\nfor English-Hindi and English-Slovene languages, specifically\naimed to support informal language. Then we performed an\nevaluation of monolingual, bilingual, few-lingual, and massively\nmultilingual models on several languages, using two tasks that\nfrequently contain code-mixed text, in particular, sentiment\nanalysis and offensive language detection in social media texts.\nThe results show that the most successful classifiers are fine-tuned\nbilingual models and multilingual models, specialized for social\nmedia texts, followed by non-specialized massively multilingual\nand monolingual models, while huge generative models are\nnot competitive. For our affective problems, the models mostly\nperform slightly better on code-mixed data compared to non-\ncode-mixed data.\nI. INTRODUCTION\nCode-mixing, the practice of combining multiple languages\nor varieties in a single discourse, is common in today’s world.\nThis phenomenon is observed in bilingual and multilingual\ncommunities, and influenced by informal settings, cultural\nidentity, lack of vocabulary, media and pop culture, globaliza-\ntion, and the digital era [50]. In bilingual households, people\noften switch between languages based on context or personal\npreference. In informal settings, code-mixing is more common\nwhen speaking with friends or expressing ideas or emotions\nin a specific language [50]. It can also be a way to express\ncultural identity or adapt to linguistic diversity in multicultural\nenvironments. The digital era has introduced new avenues for\ncommunication, making code-mixing more prevalent [2].\nAnalyses of code-mixed data enable understanding of re-\nalistic interactions in real-world communication, which is es-\npecially important for understanding sentiment and emotions,\nexpressed by people in multilingual environments. Successful\nanalysis of code-mixed data also provides effective multilin-\ngual insights and enhances decision-making in various do-\nmains like marketing [27], customer support [46], and policy-\nmaking [47].\nRecently, pre-trained large language models (LLMs) have\ndominated the landscape of Natural Language Processing\n(NLP). LLMs appear in various sizes, from 100 million to sev-\neral hundred billion parameters, and cover different numbers\nof languages, from monolingual to massively multilingual,\nsupporting a few hundred languages. In the context of code-\nmixing, two types of LLMs are interesting: those covering\nall the languages that simultaneously appear in a code-mixed\ntext, and those pre-trained on informal texts, which are the\nmost frequently used in code-mixed scenarios. While several\nworks applied LLMs to tasks with code-mixed languages (see\nSection II for an overview), the existing analyses were limited\nto individual problems and pairs of code-mixed languages. We\nfill this gap and conduct an analysis, covering multiple types\nof LLMs on five languages (French, Hindi, Russian, Slovene,\nand Tamil). We tackle two affective computing tasks where\ncode-mixing is especially prominent: sentiment analysis and\nhate speech detection.\nAnalyses of sentiment and hate speech detection in a code-\nmixed setting are particularly relevant for affective comput-\ning due to increased cultural and linguistic diversity in a\nglobalized world. In this context, modern affective comput-\ning systems must handle and interpret inputs from diverse\nlinguistic backgrounds, including those where code-mixing is\na natural part of communication. Emotions can be expressed\ndifferently across languages, and the nuances might change\nsignificantly in code-mixed environments. Models trained on\nmonolingual data often fail to capture these subtleties. Adapt-\ning them to handle code-mixed text might improve their\nperformance, robustness, user experience, and inclusiveness,\nespecially in informal settings. Our study builds upon the\ngrowing interest in sentiment analysis of code-mixed text. A\nrecent study introduced robust transformer-based algorithms\nto enhance sentiment prediction in code-mixed text, specifi-\ncally focusing on English and Roman Urdu combinations.[23]\nThe study employed state-of-the-art transformer-based mod-\nels like Electra[13], code-mixed BERT (cm-BERT), and\nMultilingual Bidirectional and Auto-Regressive Transformers\n(mBART).[18]\nThe aim of our work is to investigate the abilities of different\nlarge language models, which nowadays represent the essential\ninfrastructure for language analysis, in the area of code-mixed\nsentiment and hate speech.\nOur main contributions are as follows:\n• We created four new masked bilingual large language\nmodels, focused on informal language.\n• We conducted an in-depth analysis of large language\nmodels on code-mixed language using two types of repre-\nsentative classification problems in five languages, where\nsuch problems are relevant and adequate large language\nmodels exist. The results show an advantage of bilingual\nmodels and models specialized for social media texts over\ngeneral massively multilingual and monolingual models.\n• We investigated the detection of code-mixed language\nand observed weaknesses in existing commonly used\nlanguage detectors.\nThe remainder of our paper is structured as follows. In\nrefsec:related, we review the related literature on code-mixing\narXiv:2405.12929v1  [cs.CL]  21 May 2024\n2\npatterns, language models, and code-mixed evaluation. In\nSection III, we describe the background and essential statistics\nof datasets we use in our code-mixed evaluation. In Section IV,\nwe present training of language models used in our study,\nfollowed by the description and analysis of the experimental\nresults. We conclude in Section VI with an overview of the\nfindings, a discussion on the limitations of the study, and\npossible future directions for code-mixed natural language\nprocessing.\nII. RELATED WORK\nIn this section, we overview the related literature on code-\nmixing. Initially, we present an overview of multilingual LLMs\nthat are suitable for modeling code-mixed language in Sec-\ntion II-A. We describe the existing resources used to evaluate\ncode-mixed NLP in Section II-B, and studies on code-mixed\nevaluation of LLMs in Section II-C. Finally, in Section II-D,\nwe present the existing works connecting affective computing\ntasks and code-mixing.\nA. Multilingual LLMs\nInitially, large language models (LLMs) were primarily\ntrained on well-resourced languages like English and Chinese\ndue to the availability of ample resources. However, there\nwas a limitation in understanding text containing multiple lan-\nguages simultaneously. Many widely spoken languages lacked\nsufficient resources for model training. Multilingual LLMs\nsuch as the multilingual BERT [16] and XLM-RoBERTa\n[14] were developed to address this issue. These models\nhave been trained on balanced datasets comprising about 100\nlanguages, aiming to provide better support for multilingual\ntext understanding and processing.\nSimilarly, several few-lingual LLMs have been developed,\neach trained on a limited number of languages. Examples\ninclude CroSloEngual BERT (supporting Croatian, Slovene,\nand English), FinEst BERT (trained on Finnish, Estonian,\nand English) [55], and MuRIL [28] (supporting 17 Indian\nlanguages)\nFor understanding code mixing, models need to comprehend\nhow different languages are interchangeably occurring within\na single sentence, typically in informal discourse and often\nused in an affective context. While multilingual models are\npre-trained on general language data (e.g., Wikipedia dumps),\ncode-mixing presents a specialized challenge. An alternative\nand direct way of improving the code-mixed performance is by\npre-training models directly on code-mixed texts. For example,\nNayak and Joshi release multiple transformer-based models\ntrained on a carefully curated Hindi-English corpus [41]. Such\nmodels are aimed at handling the specifics of code-mixing but\nare not available for many language pairs.\nIn this work, we extend the support for code-mixed lan-\nguage processing by introducing four new specialized LLMs:\ntwo for the previously unsupported Slovene-English language\npair, and two for Hindi-English. Additionally, we perform\na comprehensive code-mixed evaluation using a selection of\nLLMs, including few-lingual models previously untested in\nsuch scenarios.\nB. Evaluation in code-mixed settings\nCode-mixed LLMs are typically evaluated through extrinsic\nevaluation, i.e. by evaluating the system on a downstream\ntask that involves code-mixing. The tasks depend on the end\ngoal and are either generative or discriminative. Examples of\ngenerative tasks include code-switched text translation and\ncontrolled code-switched text generation [48], [39], while\nexamples of discriminative tasks include offensive language\nidentification, sentiment analysis, and natural language in-\nference [29], [11]. To provide a more general evaluation,\nauthors have also released benchmarks spanning multiple\ntasks [30]. Likely due to its informal setting, the code-mixed\ndatasets are commonly sourced from social networks (e.g.,\nTwitter) or other online platforms with comment sections.\nDespite the ubiquitous nature of code-mixing, the pool of\nlanguages used in code-mixing evaluations is relatively small\nand typically involves a language code-mixed with English, for\nexample, one of the Indo-Aryan or Dravidian languages [12];\nalternatively, a higher-resourced language such as Spanish [1]\ncan also be code mixed with English. However, code-mixing\ndoes not necessarily occur paired with English, and some\nresources have been collected for such settings: for example,\nthe Turkish-German dependency parsing dataset [10], or the\nmodern standard Arabic and Egyptian dialectal Arabic dataset\n[17].\nIn our work, we focus on the evaluation of LLMs on two\naffective tasks: offensive language identification and sentiment\nanalysis. In contrast to existing work, we consider a larger\npool of languages (five language pairs), including languages\nfor which code-mixed research is scarce.\nC. Comprehensive analyses of code-mixing\nMost works mentioned in Section II-B have focused on\nevaluating and optimizing LLMs for a specific code-mixed\ntask or a small pool of tasks in one language. Several works\nhave instead focused on a larger-scale evaluations, which we\nfocus on in our work as well.\nWinata et al. [57] study the effectiveness of multilingual\nLLMs on code-mixed named entity recognition and part of\nspeech tagging across three language pairs using three criteria:\ninference speed, performance, and number of parameters.\nZhang et al. [58] perform a similar analysis, comparing the\nperformance of few-hundred million parameters fine-tuned\nmassively multilingual LLMs to the performance of multi-\nbillion parameters models in zero-shot and few-shot settings.\nThey find that while large models are relatively successful,\nfine-tuned smaller models achieve the best results. Santy et\nal. [45] study the effect of using different types of synthetic\ndata to fine-tune multilingual LLMs for six tasks across one\nor two language pairs. They find that including any type of\nsythetic code-mixed data in the tuning process improves the\nresponsivity of attention heads to code-mixed data, indicating\nthe suboptimal support for code-mixing in multilingual mod-\nels. Tan and Joty [49] and Birshert and Artemova [7] study the\ncapability in an adversarial setting, and show that synthetically\nconstructed code-mixed examples cause a significant drop in\nthe accuracy of multilingual LLMs.\n3\nIn our work, we aim to continue the line of comprehensive\nanalyses on a pool of five languages, including multiple that\nare not commonly studied in the existing literature. We focus\nour analysis on two affective evaluation tasks and evaluate a\nlarge pool of monolingual, few-lingual, and massively multi-\nlingual LLMs.\nD. Affective Computing in Code-Mixed Language Modeling\nAffective computing, incorporating emotion and senti-\nment comprehension in human language, is an important\narea of NLP. aiming to decipher and understand emotional\nnuances[44]. Yet, when it comes to code-mixed content, the\nlandscape becomes more intricate.\nA primary obstacle to better understanding affective hints\narises from the fact that emotions can be deeply intertwined\nwith the choice of language for particular words or phrases.\nFor example, embedding a term of endearment in Hindi\nwithin an English sentence can completely shift the sentiment,\ntransforming a neutral statement into an affectionate one. For\nexample, consider the English neutral sentence ”I received a\nnice gift today” and the code-mixed sentence ”I received a\nnice gift today, meri pyaari maa”. In the original sentence,\n”nice” conveys a positive but neutral sentiment about the\ngift. However, in the code-mixed sentence, adding the Hindi\nphrase ”meri pyaari maa” completely transforms the tone.\n”meri pyaari maa” expresses affection and suggests the gift has\na deeper meaning because it came from the mother. Another\nexample with respect to hate speech is ”This politician is a\ncomplete bekaar” where bekaar is Hindi for ’useless’. The\nEnglish sentence criticizes the politician, but ”bekaar” adds a\nstronger layer of insult specific to Hindi. Hate speech detec-\ntion models trained only on English might miss the hateful\nconnotation because they wouldn’t understand the severity of\nthe Hindi word.\nSeveral works have ventured into understanding sentiment\nanalysis and emotion discernment in code-mixed contexts.\nBalahur et al. [4] introduce a model that leverages cultural\ncontext and societal norms to enhance emotion detection\nprecision. Additionally, Bedi et al.[6] probe the intricacies of\nspotting sarcasm in code-mixed exchanges, emphasizing the\nimportance of grasping the dynamics between languages.\nIII. CODE-MIXED AFFECTIVE DATASETS\nIn this section, we present ten affective code-mixed datasets\nused to test the newly introduced and existing LLMs. For\neach language, we select one sentiment analysis dataset and\none offensive language detection dataset. Their summary is\npresented in Table I. We start by describing the datasets\ngrouped by the primary language in Sections III-A - III-E.\nThen, we focus on the label distribution (Section III-F) and\nthe degree of code-mixed content present in the datasets\n(Section III-G).\nA. French Datasetss\n1) FrenchBookReviews: The French book reviews dataset1\ncontains 9 658 reviews by book readers made on the French\n1https://www.kaggle.com/datasets/abireltaief/books-reviews\nwebsites Babelio and Critiques Libres. The sentiment is de-\nrived from a five-star rating system used in the review process:\nreviews with a rating ≤2.5 are considered negative, reviews\nwith a rating ≥4.0 are considered positive, and the others are\nconsidered neutral. The dataset contains genuine code-mixing\nexamples in French reviews contributed by individuals.\n2) FrenchOLID: The French offensive language identifi-\ncation dataset [22] contains 5 786 tweets posted during the\nCOVID-19 pandemic. The authors consider a tweet offensive\nif the offense is directed towards somebody. Although the\ndataset is not specifically introduced as code-mixed, we use\nit as Twitter commonly contains informal language and code-\nmixing.\nB. Tamil datasets\nThe DravidianCodeMix dataset collection [11] contains\nYouTube comments in three Dravidian languages (Tamil,\nKannada, and Malayalam) annotated for sentiment analysis\nand offensive language identification. In our work, we decided\nto use only one Dravidian language, i.e. Tamil, as our LLMs\nare covering it. The Tamil dataset contains around 44, 000\nexamples. The dataset was annotated by between two and five\nstudent annotators:\n• In the sentiment analysis dataset, the data was annotated\nas positive, neutral, negative, or mixed feelings. We\ndecided to discard the examples annotated as “mixed feel-\nings” to maintain a unified three-label sentiment scheme\nacross all our tested languages.\n• In the offensive language identification dataset, the data\nwas annotated as not offensive, untargeted offensive, or\ntargeted offensive (three options). We consider any type\nof offensiveness as the positive class, and the rest as the\nnegative class.\nIf the example did not contain the Tamil language, it was\nlabeled as “not-Tamil”; in our experiments, we discard such\nexamples. We refer to the sentiment analysis dataset as Tamil-\nCMSenti, and the offensive language identification dataset as\nTamilCMOLID.\nC. Hindi datasets\n1) IIITH: The IIITH Hindi-English code-mixed sentiment\ndataset [25] contains 3 879 user comments sourced from\npopular Indian Facebook pages associated with influential\nfigures Salman Khan and Narendra Modi. The examples\nwere annotated by two annotators using a three-level polarity\nscale (positive, negative, or neutral); only the examples with\nmatching annotations were included in the final dataset.\n2) Hinglish Hate: The Hinglish Hate dataset [8] contains\nHindi-English code-mixed social media texts, consisting of\n4 578 tweets. The authors annotated the dataset with the\nlanguage at the word level and with the class the tweets belong\nto (Hate speech or Normal speech)\nD. Slovene datasets\n1) Sentiment15SL: The Sentiment15SL [40] is the Slovene\nsubset of the corpus of over 1.6 million tweets belonging\n4\nTABLE I: Summary of the used datasets. The % of code-mixing is manually estimated on 100 random samples, except for\nthe Hindi and Russian dataset (marked with *), where we used the CodeSwitch and langdetect libraries; see Section III-G for\ndetails.\nDataset\nDetected Languages\nTask\nGenre(s)\n#Inst.\nCode-mixing %\nFrenchBookReviews\nFrench, English\nsentiment analysis\nbook reviews\n9 658\n17.00\nFrenchOLID\nFrench, English\noffensive language\ntweets\n5 786\n11.00\nTamilCMSenti\nTamil, English\nsentiment analysis\nYouTube comments\n36 681\n46.00\nTamilCMOLID\nTamil, English\noffensive language\nYouTube comments\n41 760\n31.00\nIIITH\nHindi, English\nsentiment analysis\nFacebook comments\n3 879\n*63.88\nHinglish Hate\nHindi, English\noffensive language\ntweets\n4 578\n*86.57\nSentiment15SL\nSlovene, Croatian, English\nsentiment analysis\ntweets\n87 392\n36.00\nIMSyPPSL\nSlovene, Croatian, English\noffensive language\ntweets\n47 538\n29.00\nSentiment15RU\nRussian, Bulgarian\nsentiment analysis\ntweets\n86 948\n*9.15\nRussianOLID\nRussian, Bulgarian\noffensive language\nsocial media comments\n14 412\n*3.91\nto 15 European languages. The 102 392 Slovene tweets were\nposted between April 2013 and February 2015, and collected\nusing Twitter Search API by constraining the geolocation of\nthe tweet. They were annotated using a standard three-class\nannotation scheme (positive, neutral, or negative) by seven\nannotators.\n2) IMSyPPSL: IMSyPPSL [19] is a Slovene dataset contain-\ning tweets posted between December 2017 and August 2020.\nThe tweets were manually annotated twice for hate speech\ntype and target: we consider tweets containing any type of hate\nspeech as positive and others as negative. We preprocessed the\ndata, keeping only tweets where both its annotations agree, in\ntotal 47 538 examples.\nE. Russian datasets\n1) Sentiment15RU: The Sentiment15RU is the Russian sub-\nset of the Sentiment15 corpus of tweets. As the dataset was\ncollected as a whole, its collection is similar to the collection\nof the Slovene Sentiment15SL subset (see Section III-D1). In\ntotal, the dataset contains 87 384 examples.\n2) RussianOLID: The Russian language toxic comments\ndataset (RussianOLID) contains 14 412 comments from Rus-\nsian websites 2ch and Pikabu. The dataset was originally\npublished on Kaggle, with its annotation quality later being\nindependently validated by Smetanin [32]. The texts are an-\nnotated for toxicity using a binary annotation scheme (toxic\nor non-toxic).\nF. Label distribution in the code-mixed datasets\nTo provide insight into the used datasets, we quantify their\nlabel distribution in Table II, separately for sentiment and\noffensive language datasets. As the numbers show, all datasets\nare imbalanced to some degree. For offensive language iden-\ntification, all datasets contain a higher proportion of non-\noffensive than offensive examples. For sentiment analysis,\nFrenchBookReviews and TamilCMSenti lean heavily towards\npositive sentiment, in IIITH the positive sentiment is domi-\nnant to a lesser degree, while datasets Sentiment15-SL and\nSentiment15-RU show a relatively balanced mix of positive\nand negative labels with the largest class being neutral.\nTABLE II: Label distributions in the code-mixed datasets.\n(a) Sentiment analysis datasets.\nDataset name\npositive\nneutral\nnegative\nIIITH\n34.85%\n50.45%\n14.70%\nFrenchBookReviews\n69.06%\n22.04%\n8.90%\nTamilCMSenti\n67.11%\n18.67%\n14.20%\nSentiment15SL\n27.12%\n43.16%\n29.72%\nSentiment15RU\n27.92%\n40.08%\n32.00%\n(b) Offensive language identification datasets.\nDataset name\nnot offensive\noffensive\nFrenchOLID\n77.51%\n22.49%\nTamilCMOLID\n75.40%\n24.60%\nHinglish Hate\n63.73%\n36.27%\nIMSyPPSL\n66.58%\n33.42%\nRussianOLID\n66.51%\n33.49%\nG. Language proportions in code-mixed datasets\nAn important aspect of code-mixed datasets is the language\ndiversity they contain. We used two methods for detecting\ncode-mixing. The first one tackles code-mixing in French,\nRussian, Tamil, and Slovene tweets using the langdetect2\nlibrary. This method first removes non-alphabetic characters\nfrom the text and then uses a character n-gram-based na¨ıve\nBayes language detector. We obtain a list of detected languages\nfor each text and consider them code-mixed if more than one\nlanguage is detected above the threshold of 5%. We selected\nthis threshold by analyzing the accuracy of the langdetect\nlibrary on hand-picked samples, 50 actually code-mixed and\n50 not, from each of the aforementioned datasets. We observed\nthe peak accuracy for the 4-7% threshold range. Balancing\nprecision and recall, we chose 5% to reliably identify code-\nmixed content while minimizing false positives and false\nnegatives.\nDue to problems with code-switching identification in Hindi\ndatasets, and the availability of a better alternative, we tried\nthe CodeSwitch library3 for this language. The library is based\n2https://pypi.org/project/langdetect/\n3https://pypi.org/project/codeswitch/\n5\non multilingual BERT, which is known to effectively take the\ncontext of neighboring words into account. We used its mod-\nule, configured specifically for Hindi and English (hin-eng), to\nidentify languages within each tweet. The approach identified\nHindi, English, Nepali, and other languages. Upon manual\nexamination of a subset of labeled instances, we observed\nthat Hindi and Nepali were often identified interchangeably\ndue to their linguistic similarities and shared vocabulary. To\nenhance the accuracy of our language identification process,\nwe refined our function to categorize a tweet as code-mixed\nonly when it contained combinations of Hindi and English,\nNepali and English, or a trilingual mix of Hindi, Nepali, and\nEnglish. Tweets exhibiting a single language were labeled as\n’not code-mixed’.\nTABLE III: Manual assessment of code-mix detection tools\nprecision in Hindi and Slovene.\nDataset\nPrecision\nTool\nIIITH\n0.860\nCodeSwitch\nHinglish Hate\n0.900\nCodeSwitch\nSentiment15SL\n0.326\nlangdetect\nIMSyPPSL\n0.201\nlangdetect\nTable III shows the manual evaluation of the precision\nobtained by our code-mix detection tools. For Hindi and\nSlovene datasets, we manually checked 50 random samples\nflagged as code-mixed and 50 random samples flagged as not\ncode-mixed by our tools. While the CodeSwitch library, which\nis based on contextual LLM, shows a promising performance,\nthe langdetect library often attributes words to an arbitrary\nlanguage where they exist without considering their contextual\nusage. This limitation becomes evident when the same word\nappears in multiple languages within a sentence. For instance,\nconsider the word ”brat”, which refers to a male sibling in\nboth Croatian and Slovene. The langdetect tool, using the\nclass-conditional independence assumption of na¨ıve Bayes,\nstruggles to accurately determine the appropriate language for\n”brat” based on its surrounding context.\nAcknowledging these challenges and the need for better\ncode-switching detection accuracy, we manually annotated 100\nsamples from each dataset, excluding Hindi and Russian lan-\nguages. This exclusion was based on the CodeSwitch library’s\npromising precision rate in Hindi and the lack of collaborators\nto manually annotate Russian datasets. The results of this\nmanual annotation and code-mixing percentage are shown in\nthe rightmost column of Table I. The code-mixing percentages\nin Table I for the two Russian datasets are calculated on the\nwhole dataset using the langdetect library.\nIV. PRE-TRAINED LARGE LANGUAGE MODELS\nIn this section, we describe the pre-trained LLMs which\nwe use in our study. In Section IV-A, we first describe four\nnewly created LLMs trained on considerable amounts of non-\nstandard language aimed at better processing Slovene-English\nand Hindi-English code-mixed language. Then, we describe\nother (existing) LLMs used in our evaluation in Section IV-B.\nAll models are listed in Table IV, where the newly introduced\nmodels are marked with an asterisk (*).\nTABLE IV: Summary of used large language models. The\nmodels marked with * are newly introduced in this paper.\nModels marked with ▲are declared as monolingual but have\ncapabilities in multiple languages. M and B stand for millions\nand billions of parameters, respectively.\nModel\nLink\nLanguages\nParameters\nMassively multilingual models:\nmBERTc-base\nlink\n104\n178M\nXLM-R-base\nlink\n100\n278M\nTwHIN-BERT-base\nlink\n100\n279M\nFew-lingual models:\nSlEng-BERT*\nlink\nsl, en\n117M\nSloBERTa-SlEng*\nlink\nsl, en\n117M\nMuRIL-en-hi-codemixed*\nlink\nhi, en\n117M\nRoBERTa-en-hi-codemixed*\nlink\nhi, en\n117M\nMuRILc-base\nlink\nen + 16 in\n238M\nHingRoBERTa\nlink\nhi, en\n278M\nCroSloEngual BERT\nlink\nsl, hr, en\n124M\nMonolingual models:\nSloBERTa\nlink\nsl\n111M\nCamemBERT-base\nlink\nfr\n111M\nTamilBERT\nlink\nta\n238M\nRuBERTc-base\nlink\nru\n178M\nMonolingual generative models:\nGPT3 ▲\nlink\nen\n175B\nLlama2-7B ▲\nlink\nen\n7B\nA. New code-mixed language models\nWe trained four new large language models on considerable\namounts of non-standard language with the intention of using\nthem for affective computing tasks and code-mixed processing.\nAll the new models are masked LLMs utilizing transformer\narchitecture [56], with about 117 million parameters each.\nThe models are bilingual, two being Hindi-English and two\nSlovene-English. Two models were trained from scratch, and\ntwo were further trained from existing models on new data.\nTheir names and classification are shown in Table V.\nTABLE V: The names and properties of four newly trained\nbilingual masked language models.\nLanguages\nFrom scratch\nFrom existing\nHindi-English\nRoBERTa-en-hi-codemixed\nMuRIL-en-hi-codemixed\nSlovene-English\nSlEng-BERT\nSloBERTa-SlEng\nEach of the newly trained models has 12 transformer en-\ncoder layers, equal in architecture and roughly equal in number\nof parameters to the base-sized BERT [16] and RoBERTa [34]\nmodels. The models support a maximum sequence length of\n512 tokens. The pre-training task was masked language mod-\neling, with no other tasks (e.g., next-sentence prediction). The\nmodels are publicly available on the HuggingFace repository\nof the Centre for Language Resources and Technologies of the\nUniversity of Ljubljana4. Their short description is provided\nbelow.\n1) Slovene-English: The two new Slovene-English models\nSlEng-BERT and SloBERTa-SlEng are trained on Slovene\nand English conversational, non-standard, and slang language\ncorpora. Concretely, they are trained on English and Slovene\ntweets [20], Slovene part of the web crawl corpus MaCoCu\n4https://huggingface.co/cjvt\n6\n[5], the corpus of moderated web content Frenk [35], and a\nsmall subset of the English OSCAR corpus [43]. The size of\nthe English and Slovene corpora used is approximately equal.\nIn total, the training data contains about 2.7 billion words,\nwhich were tokenized into 4.1 billion subword tokens prior\nto training. Both models share the same vocabulary and input\nembeddings, containing 40 000 subword tokens.\nUsing the dataset, the models were trained using two dif-\nferent training regimes: SlEng-BERT was trained from scratch\nfor 40 epochs while SloBERTa-SlEng was initialized using the\nSloBERTa [53] Slovene monolingual masked LLM and further\npre-trained for two epochs.\n2) Hindi-English:\nThe two new Hindi-English models\nRoBERTa-en-hi-codemixed and MuRIL-en-hi-codemixed are\ntrained on English, Hindi, and code-mixed English-Hindi\ncorpora. The corpora used consist of primarily web-crawled\ndata, including code-mixed tweets, focusing on conversational\nlanguage and the COVID-19 pandemic. The training corpora\ncontain about 2.6 billion words, which were tokenized into 3.4\nbillion subword tokens prior to training. Both models share\nthe same vocabulary and input embeddings, containing 40 000\nsubword tokens.\nSimilarly as for the Slovene-English models, the mod-\nels were trained using two different training regimes. The\nRoBERTa-en-hi-codemixed model was trained from scratch\nfor 40 epochs while MuRIL-en-hi-codemixed was initialized\nusing pre-trained MuRIL multilingual masked LLM [28] and\nfurther pre-trained for two epochs.\nB. Existing language models\nIn addition to four newly introduced bilingual LLMs, we\nevaluate several existing massively multilingual, few-lingual,\nand monolingual LLMs which we describe next. Our newly\nintroduced models are variants of the encoder-only BERT\nand RoBERTa LLMs, and so are most of the other models,\nbut we also include two massive decoder-only LLMs due to\ntheir strong general performance. In addition to the general\ndomain models, we test multiple tweet domain-adapted LLMs\nas they are specialized for handling social media texts that\ncommonly include code-mixed language. Below, we split their\ndescriptions into three groups: massively multilingual mod-\nels, few-lingual models, monolingual models, and generative\nmonolingual models.\n1) Massively multilingual models: We consider three mas-\nsively multilingual LLMs trained on 100 or more languages.\nMultilingual\nBERT (mBERT) [16] is a multilingual\nmasked LLM based on the BERT architecture [16]. It was\ntrained on Wikipedia dumps in 104 languages with the largest\nWikipedia size. We use the cased base-size version of this\nmodel and refer to it as mBERTc-base.\nXLM-RoBERTa [14] is a multilingual masked LLM based\non the RoBERTa architecture [34], trained on Wikipedia\ndumps and web crawl data in 100 languages. We use the base-\nsize version and refer to the model as XLM-R-base.\nTwHIN-BERT [59] is a multilingual masked LLM based\non the BERT architecture [16]. It was trained on tweets in 100\nlanguages. In addition to masked language modeling, it was\ntrained using a contrastive social loss, the goal of which is to\nlearn if two tweets appeal to similar users. We use the base-\nsize version and refer to the model as TwHIN-BERT-base.\n2) Few-lingual models: As massively multilingual models\ncover a wide range of languages, their vocabulary and to-\nkenization are not adapted to any specific language, which\nmakes them inferior to LLMs incorporating fewer languages\nfor many tasks [54]. We consider three few-lingual LLMs\ncovering considerably fewer languages than massively mul-\ntilingual LLMs.\nMuRIL [28] is a multilingual masked LLM based on the\nBERT architecture [16]. It was trained on large Indian corpora\nconsisting of English and 16 Indian languages, including Hindi\nand Tamil which we consider in this work. We use the cased\nbase-size version of this model and refer to it as MuRILc-base.\nHingRoBERTa [41] is a bilingual masked LLM based\non RoBERTa architecture [34]. It was fine-tuned on a large\ncorpus of Hindi-English tweets. HingRoBERTa has demon-\nstrated competitive performance across various downstream\ntasks compared to other models trained on Hindi-English code-\nmixed datasets, as evidenced in research by Nayak et al. [42]\nCroSloEngual BERT [55] is a trilingual masked LLM\nbased on the BERT architecture [16]. It was trained on a\nmixture of news articles, and web-crawled data in Croatian,\nSlovene, and English.\n3) Monolingual models: While we hypothesize that mul-\ntilingual LLMs might be preferable for code-mixed affective\ntasks, we also test monolingual models. They might be com-\npetitive and familiar with languages other than their main\nlanguage, as a small number of other languages is likely to be\npresent in all monolingual training corpora due to their huge\nsize and likely presence in the news, textbooks, and social\nmedia. We consider four monolingual masked LLMs covering\nspecific languages.\nCamemBERT[37] is a French monolingual masked LLM\nbased on the BERT architecture [16]. It was trained using a\nwhole-word masking version of the masked language mod-\neling objective on web-crawled data. We use the base-size\nversion and refer to the model as CamemBERTbase.\nSloBERTa [53] is a Slovene monolingual masked LLM\nbased on the CamemBERT architecture [37]. It was trained on\na union of five Slovene corpora containing news articles, web-\ncrawled data, tweets, academic language, and parliamentary\ndata.\nTamilBERT [26] is a Tamil monolingual masked LLM\nbased on the BERT architecture [16]. It was trained on a Tamil\nmonolingual corpus.\nRuBERT [31] is a Russian monolingual masked LLM\nbased on the BERT architecture [16]. It was trained on the\nRussian Wikipedia dump and news articles. We use the cased\nbase-size version and refer to the model as RuBERTcbase.\n4) Monolingual generative models: While all other models\nin our evaluation are masked LLMs using only the encoder\nstack of the transformer architecture, for comparison, we\nalso include two popular and considerably larger generative\nLLMs that use only the decoder stack of the transformer\narchitecture. The GPT3 and Llama2 models we describe below\n7\nare primarily trained on English, although they have seen and\nare capable of processing other languages.\nGPT-3 [9] is a language model trained on a vast amount of\ntext and code. This vast training dataset allows it to perform\na variety of tasks, including generating different creative text\nformats, translating languages, and answering questions in an\ninformative way. The specific inner workings of this model\nremain undisclosed.\nLlama2 [51] is an English monolingual autoregressive\nLLM. The details about its training data are scarce: the authors\nstate that it was trained on mostly English data from publicly\navailable sources. We use the version with 7B parameters and\nrefer to the model as Llama2-7B.\nV. EVALUATION\nIn this section, we evaluate the described language models\non multiple code-mixed datasets. We first describe the exper-\nimental setup in Section V-A and continue with the analysis\nof results in Section V-B. In Section V-C we further analyze\nthe results, specifically focusing on the effect of code-mixed\nlanguage.\nA. Experimental setup\nIn our model evaluation process, we split the data into\ntraining, validation, and testing sets randomly in the proportion\n60%:20%:20%, using stratification across the class labels.\nHowever, for the GPT 3 and Lamma2 models, we implemented\na nuanced approach to manage costs and streamline perfor-\nmance. For GPT, we selected a random subset of 500 samples\nfrom the training dataset and 100 from the testing dataset.\nSimilarly, for Lamma2, we optimized output generation time\nby reducing the testing dataset to 300 samples. Subsequently,\nwe reported every model’s macro F1 score on the test set as\na comprehensive metric for overall effectiveness. To enhance\nthe quality of the input data, we conducted preprocessing\nsteps that involved the removal of special characters, such\nas ’@’, and trailing whitespaces from the text. To assure the\nreproducibility of our results, we share our code online5. We\nprovide descriptions of the finetuning process for different\nmodel groups next: in Section V-A1 for BERT-like models,\nin Section V-A2 for the GPT3 model, and in Section V-A3\nfor the Llama2 model.\n1) Fine-tuning BERT-like models: We evaluate the models\nin a discriminative (classification) setting, meaning we fine-\ntune the models to discriminate between the two (in offensive\nlanguage identification) or three (in sentiment analysis) unique\nclasses. To enable batch processing, we truncate and pad all\ninput sequences to a maximum length of 512 subword tokens.\nWe optimize the models using the AdamW [36] optimizer with\nthe learning rate 5·10−5 for up to five epochs, maximizing the\nbatch size based on the available GPU memory. On Slovene\ndatasets, we fine-tune the models for up to ten epochs as we\nnoticed some models did not converge after five epochs. Our\nfine-tuning settings are selected as reasonable defaults instead\n5https://github.com/matejklemen/sentiment-hate-speech-with-code-mixed-\nmodels\nof using a thorough hyperparameter optimization, and largely\nfollow existing practices for fine-tuning BERT-like models\n[38] [52].\n2) Fine-tuning GPT3 Model: The evaluation for genera-\ntive models necessarily differs from BERT-like classification\nmodels. Specifically, we fine-tune the GPT3 model to generate\ntextual classes. In this process, GPT-3 is trained to map input\ntext to output text using our prompt structure. We fine-tune\nthe model for 800 prompt completion pairs for 2 epochs\nusing the OpenAI API on randomly sampled training subsets.\nDuring the generation phase, we set the temperature to 0.1\nand the learning rate to 0.1 During fine-tuning, each example\nis transformed into a prompt by appending the text to the\nprefix ”Input:”. The model is optimized to produce text with\nthe prefix ”Sentiment is:” (or ”Label is:” in offensive language\nidentification), followed by the predicted class. The generated\noutput takes the form of ’Sentiment: class’ or ’Label: class’.\nAn example of a prompt and the output template are shown\nin Figure 1, for a code-mixed Hindi-English input.\nFig. 1: Example prompts and output templates for sentiment\nanaysis and offensive language detection tasks.\nAs shown, we fine-tune the GPT3 model to generate textual\nclasses. Initially, we fine-tuned the Curie variant of GPT3\nusing our prompt structure and hyperparameters as previously\ndescribed. However, due to Curie’s deprecation, we transi-\ntioned to the davinci-002 variant for completing our tests on\nthe Hindi datasets. For fine-tuning the davinci-002 model, we\nemployed a temperature setting of 0.1 and logit bias.\n3) Fine-tuning the Llama2 models:\nLlama2 generative\nmodel was fine-tuned with parameter-efficient fine-tuning\nmethod QLoRA[15], utilizing 16-bit precision quantization\nwith a learning rate of 2·10−4. The idea of parameter-efficient\nfine-tuning techniques [33] is to selectively fine-tune a limited\nset of additional model parameters, significantly reducing both\ncomputational and storage expenses associated with the fine-\ntuning process.\nThe same as the GPT3 model, we evaluate the Llama-2\nmodel in a generative setting, using prompts akin to those\nused in the GPT3 model (see Section V-A2).\nB. Results on the full datasets\nIn this section, we present the results of different models on\naffective code-mixed tasks, sentiment prediction, and offensive\nlanguage detection. Table VI presents the results split by\nlanguage, i.e. each subtable corresponds to a specific code-\nmixed datasets: Slovene (Table VIa), Tamil (Table VIb), Hindi\n(Table VIc), French (Table VId), and Russian (Table VIe).\n8\nTABLE VI: Macro F1 scores on sentiment prediction and\noffensive language detection tasks achieved with massively\nmultilingual, few-lingual, monolingual, and generative models\nacross five primary languages. The best F1 score for each\nlanguage is displayed in bold.\n(a) Results on Slovene datasets. The * marks the evaluation on a\nsample of 1000 examples.\nModel\nSentiment15SL\nIMSyPPSL\nmBERTc-base\n0.617\n0.785\nXLM-R-base\n0.645\n0.811\nTwHIN-BERT-base\n0.619\n0.808\nCroSloEngual BERT\n0.660\n0.834\nSlEng-BERT\n0.666\n0.850\nSloBERTa-SlEng\n0.659\n0.845\nSloBERTa\n0.650\n0.840\nGPT3*\n0.597\n0.825\nLlama2-7B\n0.596\n0.821\n(b) Results on Tamil datasets.\nModel\nTamilCMSenti\nTamilCMOLID.\nmBERTc-base\n0.749\n0.832\nXLM-R-base\n0.715\n0.778\nTwHIN-BERT-base\n0.795\n0.896\nMuRILc-base\n0.688\n0.843\nTamilBERT\n0.539\n0.772\nGPT3\n0.679\n0.781\nLlama2-7B\n0.620\n0.677\n(c) Results on Hindi datasets.\nModel\nIIITH\nHinglish Hate\nmBERTc-base\n0.749\n0.711\nXLM-R-base\n0.739\n0.677\nTwHIN-BERT-base\n0.830\n0.733\nMuRILc-base\n0.698\n0.748\nHingRoBERTa\n0.854\n0.833\nRoBERTa-en-hi-codemixed\n0.792\n0.729\nMuRIL-en-hi-codemixed\n0.718\n0.709\nGPT3\n0.660\n0.551\nLlama2-7B\n0.666\n0.595\n(d) Results on French datasets.\nModel\nFrenchBookReviews\nFrenchOLID\nmBERTc-base\n0.749\n0.915\nXLM-R-base\n0.644\n0.902\nTwHIN-BERT-base\n0.759\n0.936\nCamemBERT\n0.696\n0.914\nGPT3\n0.743\n0.663\nLlama2-7B\n0.571\n0.714\n(e) Results on Russian datasets.\nModel\nSentiment15RU\nRussianOLID\nmBERTc-base\n0.871\n0.915\nXLM-R-base\n0.846\n0.902\nTwHIN-BERT-base\n0.851\n0.969\nRuBERTc-base\n0.856\n0.971\nGPT3\n0.734\n0.665\nLlama2-7B\n0.677\n0.864\nOn Slovene datasets the best results are achieved by the\nnewly introduced SlEng-BERT model, achieving F1 score\n0.666 on Sentiment15SL and 0.850 on IMSyPPSL. In general,\nthe results achieved by the Slovene monolingual (SloBERTa)\nand few-lingual (SlEng-BERT, SloBERTa-SlEng, CroSloEn-\ngual BERT) models are better than those achieved by the\nmassively multilingual models and the significantly larger\ngenerative English models (GPT3 and Llama2). GPT3 and\nLlama2 achieve comparable scores on both datasets (F1 scores\n0.597 and 0.596 on Sentiment15SL, and 0.825 and 0.821\non IMSyPPSL), performing worse than massively multilingual\nmodels on Sentiment15SL and better than them on IMSyPPSL.\nOn Tamil datasets, TwHIN-BERT-base emerges as the top-\nperforming model, achieving the F1 score of 0.795 on the\nTamil sentiment dataset and 0.896 on the offensive language\ndataset. Following closely is mBERTc-base with an F1 score\nof 0.749 on the Tamil sentiment dataset and 0.832 on the\noffensive language dataset. XLM-R-base shows a competitive\nperformance to mBERTc-base in sentiment analysis with an\nF1 score of 0.715, but falls short in hate speech detection\ntasks with an F1 score of 0.778. However, it should be noted\nthat XLM-R-base has shown a trend of lower scores in Tamil\nin hate speech detection tasks in the research by Hossain et\nal.[24] where it shows lower results in Tamil when compared\nto other languages like English and Malayalam. The results\nindicate that specialized models trained for specific languages\nor tasks, such as TwHIN-BERT-base and mBERTc-base ,\ntend to outperform more generalized and larger models like\nGPT3 and Llama2-7B. GPT3, Llama2-7B, and TamilBERT\nshow competitive but slightly lower performance across both\ndatasets. MuRILc-base exhibit mixed performance across both\nthe datasets. Their error analysis suggests that XLM-R-base\nstruggles with distinguishing between HS (hate speech) and\nNHS (not hate speech) classes due to common code-mixed\nwords, potentially affecting its performance. Additionally, the\nhigh class imbalance and biasness towards the not offensive\nlabel in both the cases (table IIb) may cause misclassification\nof offensive as not offensive.\nOn Hindi sentiment and offensive language tasks, Hin-\ngRoBERTa emerges as the top model with F1 scores of\n0.854 and 0.833, showcasing the prowess of bilingual mod-\nels. TwHIN-BERT-base and RoBERTa-en-hi-codemixed fol-\nlow closely. The massively multilingual models mBERTc-base\n, MuRILc-base and XLM-R-base show comparable results.\nThe second newly introduced model MuRIL-en-hi-codemixed\nperforms worse than these models but better than generative\nmodels GPT3 and LlaMa2-7B.\nIn French sentiment detection, massively multilingual\nTwHIN-BERT-base leads the race in sentiment analysis\ntask with a F1 score of 0.759 followed by mBERTc-base,\nGPT3, and monolingual CamemBERT. The XLM-R-base and\nLlama2-7B are trailing with considerable gaps. In the offensive\nspeech detection task, several models have excelled in perfor-\nmance with TwHIN-BERT-base in the lead with a score of\nF1 score of 0.936, followed by mBERTc-base, CamemBERT\nand XLM-R-base. GPT3 performance is considerably lower\ncompared to other models.\nIn Russian tasks, the monolingual model RuBERTcbase leads\n9\nin offensive speech detection with F1 score of 0.971. Mul-\ntilingual models mBERTc-base, XLM-R-base, and TwHIN-\nBERT-base perform competitively in sentiment analysis with\nmBERTc-base leading with F1 score of 0.871. Massive gen-\nerative models, GPT3 and Llama2-7B, lag behind.\nTrying to draw more general conclusions, we observe two\nkey findings. The best results are achieved by either bilingual\nmodels (Slovene and Hindi) or a model specialized for social\nmedia content (TwHIN-BERT-base on Tamil and French); the\nRussian language, without a bilingual model and with very\nlow proportion of code-mixed text, is an exception here, but\nthe model specialized for social media ((TwHIN-BERT-base)\nis very competitive.\nThe advantage of bilingual models is two-fold. Firstly, their\ntargeted focus on a specific language pair allows them to\ncapture the intricacies of each language’s vocabulary and\ngrammar more effectively than massively multilingual models.\nThis is reflected in the superior performance of HingRoBERTa\nin the Hindi-English datasets and SlEng-BERT in the Slovene\ncode-mixed datasets compared to multilingual models, which\nmay struggle with the nuances of code-mixing present in such\ndata. Secondly, bilingual models typically have lower memory\nrequirements compared to their multilingual counterparts. This\ntranslates to greater practical efficiency, making them more\nsuitable for deployment in real-world applications, especially\nwhen dealing with resource constraints.\nFigure 2 presents a comparative analysis of various cat-\negories of models -— generative, monolingual, bilingual,\nfew-lingual, and massively multilingual -— in the context\nof the two affective computing tasks. For this comparison,\nwe considered the model with the best performance within\neach category. Figure 2a illustrates the results for sentiment\nanalysis. Generative models, while performing adequately,\nlag behind other models. Monolingual models perform better\nbut still lag behind other types of models. Bilingual models\ndemonstrate superior performance where they exist, closely\nfollowed by massively multilingual models. Interestingly, a\ndip in performance is observed for few-lingual models. Figure\n2b showcases the performance of the models in the offensive\nspeech detection task. The trends show that generative mod-\nels lag behind others, massively multilingual models either\nperform comparably to or fall below bilingual and fewlingual\nmodels, with bilingual models mostly maintaining a prominent\nposition.\nC. Results on the code-mixed subsets\nIn this section, we analyze the performance of models\nseparately for code-mixed and non-code-mixed subsets of\ntheir respective datasets. We select Slovene and Hindi as the\nrepresentative languages and curate the datasets to isolate\ncode-mixed examples. For Hindi, we use the CodeSwitch\nlibrary to separate code-mixed from non-code-mixed examples\nin the test sets. For Slovene, where language detection tools\nperform poorly on code-switched text, we manually selected\na subset of 1000 examples from the test set (identical to\nthe sample used for GPT3 evaluation in Section V-B) and\na) Sentiment analysis.\nb) Hate speech detection.\nFig. 2: Comparing the performance of the best model in each\nmodel category for the used tasks.\nmanually annotated it for code-mixing. Table VII provides a\ncomparison of model performance on code-mixed (CM) and\nnon-code-mixed (notCM) subsets.\nOn the code-mixed text, the overall best bilingual model\nSlEng-BERT, performs comparably or slightly worse to the\nbest-performing monolingual SloBERTa model. During our\nmanual inspection of the code-mixing patterns, we find that\ncode-mixing is very rarely the dominant cause for the target\nlabel, therefore, it is unsurprising that the code-mixed models\ndo not perform significantly better than the general-purpose\nones on the code-mixed subset. These findings might indicate\na lesser impact of code-mixing in affective tasks as commonly\nassumed. However, to confirm such findings. a future more\nfocused research of code-mixed text processing is necessary,\nusing better datasets with carefully curated texts (e.g., in the\n10\nTABLE VII: Separate results for code-mixed (CM) and not\ncode-mixed data (notCM).\n(a) Separate results on Slovene datasets.\nModel\nSentiment15SL\nIMSyPPSL\nCM\nnotCM\nCM\nnotCM\nN = 389\nN = 611\nN = 254\nN = 746\nmBERTc-base\n0.609\n0.623\n0.825\n0.763\nXLM-R-base\n0.631\n0.648\n0.828\n0.821\nTwHIN-BERT-base\n0.589\n0.633\n0.811\n0.822\nCroSloEngual BERT\n0.653\n0.671\n0.848\n0.853\nSlEng-BERT\n0.659\n0.690\n0.874\n0.865\nSloBERTa-SlEng\n0.649\n0.661\n0.862\n0.852\nSloBERTa\n0.666\n0.651\n0.876\n0.854\nGPT3*\n0.575\n0.606\n0.831\n0.816\nLlama2-7B\n0.595\n0.598\n0.803\n0.840\n(b) Separate results on Hindi datasets.\nModel\nIIITH\nHinglish Hate\nCM\nnotCM\nCM\nnotCM\nN = 473\nN = 303\nN = 780\nN = 135\nmBERTc-base\n0.776\n0.779\n0.725\n0.645\nXLM-R-base\n0.822\n0.797\n0.680\n0.588\nTwHIN-BERT-base\n0.828\n0.832\n0.740\n0.685\nMuRILc-base\n0.707\n0.650\n0.673\n0.564\nHingRoBERTa\n0.876\n0.867\n0.838\n0.831\nRoBERTa-en-hi-cm\n0.850\n0.787\n0.692\n0.564\nMuRIL-en-hi-cm\n0.723\n0.669\n0.704\n0.625\nGPT3\n0.466\n0.419\n0.548\n0.568\nLlama2-7B\n0.737\n0.688\n0.690\n0.720\nform of contrast sets [21]).\nResults on Hindi datasets show several intriguing patterns.\nThe bilingual HingRoBERTa consistently demonstrates the\nbest performance across all scenarios, showcasing its efficacy\nin capturing the nuances of both sentiment analysis and offen-\nsive speech detection in Hindi. The newly introduced mod-\nels, RoBERTa-en-hi-codemixed and MuRIL-en-hi-codemixed,\nperform better in the code-mixed data subsets compared to\nthe non-code-mixed subsets. This could be attributed to their\ntraining in Hinglish code-mixed corpora, giving them a certain\nadvantage in handling code-mixed text. In contrast, GPT3\ndisplays subpar results across both genres of tasks, indicating\npotential limitations in its adaptability to the complexities\nof Hindi text for sentiment and hate speech analysis. In-\nterestingly, when considering offensive language detection,\nmodels mostly perform better in code-mixed scenarios. This\nphenomenon may be attributed to the possibility that multiple\nlanguages share similar hate speech tendencies, as suggested\nby Arango et al.[3], who suggested the existence of common\npatterns in offensive speech across different languages.\nVI. CONCLUSION\nOur research analyzed the performance of several types\nof large language models on two affective computing tasks\nin a code-mixed setting. A notable finding is the dominance\nof bilingual and multilingual models, specialized for social\nmedia texts, over general massively multilingual, few-lingual,\nand monolingual models across diverse language pairs. For\ninstance, the bilingual HIndi-Engish HingRoBERTa model and\nthe newly introduced Slovene-English SlEng-BERT demon-\nstrated the best F1 scores in sentiment analysis and offensive\nspeech detection for Hindi and Slovene, respectively.\nA separate analysis of code-mixed and non-code-mixed\ndata subsets showed slightly better performance of almost\nall models on code-mixed data compared to non-code-mixed\ndata for both Slovene-English and Hindi-English code-mixing\nscenarios. While this might indicate that certain affective\nrole of code-mixing, more research is needed to confirm this\nhypothesis, especially as manual inspection showed relatively\nlittle impact of code-mixing in sentiment detection and offen-\nsive speech detection.\nOur findings provide a foundation for future explorations.\nWhile our focus has predominantly been on sentiment analy-\nsis and offensive speech detection, future work shall extend\nbeyond these realms to encompass a broader spectrum of\naffective tasks, including emotion and sarcasm detection. Di-\nversifying the language pairs and refining fine-tuning strategies\nfor low-resource settings are crucial steps forward. The transi-\ntion from research to practical applications involves testing\nsentiment analysis models and offensive speech detection\nmodels in real-world scenarios, further validating their utility.\nVII. ACKNOWLEDGEMENT\nThe work was partially supported by the Slovenian Research\nand Innovation Agency (ARIS) core research programme P6-\n0411, young researcher grant, as well as projects J7-3159, CRP\nV5-2297, L2-50070, BI-IN/22-24-015 and DST/ICD/lndo-\nSlovenia/2022/15(G). We sincerely thank Alice Baudhuin for\nher expertise in language analysis, particularly in manually\ndetecting code-mixing within our French datasets.\nREFERENCES\n[1] Gustavo Aguilar, Fahad AlGhamdi, Victor Soto, Mona Diab, Julia\nHirschberg, and Thamar Solorio. Named Entity Recognition on Code-\nSwitched Data: Overview of the CALCS 2018 Shared Task.\nIn\nProceedings of the Third Workshop on Computational Approaches to\nLinguistic Code-Switching, pages 138–147, July 2018.\n[2] Jannis Androutsopoulos. Code-switching in computer-mediated commu-\nnication. In Handbook of the Pragmatics of CMC. Mouton de Gruyter,\nDecember 2011.\n[3] Aym´e Arango, Jorge P´erez, and Barbara Poblete.\nCross-lingual hate\nspeech detection based on multilingual domain-specific word embed-\ndings. arXiv preprint arXiv:2104.14728, 2021.\n[4] Alexandra Balahur, Jes´us M Hermida, Andr´es Montoyo, and Rafael\nMunoz. EmotiNet: A Knowledge Base for Emotion Detection in Text\nBuilt on the Appraisal Theories. In Natural Language Processing and\nInformation Systems: 16th International Conference on Applications of\nNatural Language to Information Systems, NLDB 2011, Alicante, Spain,\nJune 28-30, 2011. Proceedings 16, pages 27–39. Springer, 2011.\n[5] Marta Ba˜n´on, Miquel Espl`a-Gomis, Mikel L. Forcada, Cristian Garc´ıa-\nRomero, Taja Kuzman, Nikola Ljubeˇsi´c, Rik van Noord, Leopoldo\nPla Sempere, Gema Ram´ırez-S´anchez, Peter Rupnik, V´ıt Suchomel,\nAntonio Toral, Tobias van der Werff, and Jaume Zaragoza. MaCoCu:\nMassive collection and curation of monolingual and bilingual data:\nfocus on under-resourced languages. In Proceedings of the 23rd Annual\nConference of the European Association for Machine Translation, pages\n303–304, 2022.\n[6] Manjot\nBedi,\nShivani\nKumar,\nMd\nShad\nAkhtar,\nand\nTanmoy\nChakraborty. Multi-modal Sarcasm Detection and Humor Classification\nin Code-mixed Conversations. IEEE Transactions on Affective Comput-\ning, 14(2):1363–1375, 2021.\n[7] Alexey Birshert and Ekaterina Artemova. Call Larisa Ivanovna: Code-\nSwitching Fools Multilingual NLU Models. In Recent Trends in Analysis\nof Images, Social Networks and Texts, pages 3–16, 2022.\n11\n[8] Aditya Bohra, Deepanshu Vijay, Vinay Singh, Syed Sarfaraz Akhtar,\nand Manish Shrivastava. A Dataset of Hindi-English Code-Mixed Social\nMedia Text for Hate Speech Detection. In Proceedings of the second\nworkshop on computational modeling of people’s opinions, personality,\nand emotions in social media, pages 36–41, 2018.\n[9] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D\nKaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish\nSastry, Amanda Askell, et al. Language Models are Few-Shot Learners.\nAdvances in neural information processing systems, 33:1877–1901,\n2020.\n[10]\n¨Ozlem C¸ etino˘glu and C¸ a˘grı C¸ ¨oltekin.\nTwo languages, one treebank:\nbuilding a Turkish–German code-switching treebank and its challenges.\nLanguage Resources and Evaluation, 57(2):545–579, Jun 2023.\n[11] Bharathi Raja Chakravarthi, Ruba Priyadharshini, Vigneshwaran Mu-\nralidaran, Navya Jose, Shardul Suryawanshi, Elizabeth Sherly, and\nJohn P. McCrae. DravidianCodeMix: Sentiment Analysis and Offensive\nLanguage Identification Dataset for Dravidian Languages in Code-Mixed\nText. Language Resources and Evaluation, 2022.\n[12] Khyathi Chandu, Ekaterina Loginova, Vishal Gupta, Josef van Genabith,\nG¨unter Neumann, Manoj Chinnakotla, Eric Nyberg, and Alan W. Black.\nCode-Mixed Question Answering Challenge: Crowd-sourcing Data and\nTechniques. In Proceedings of the Third Workshop on Computational\nApproaches to Linguistic Code-Switching, pages 29–38, Melbourne,\nAustralia, July 2018.\n[13] Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D\nManning.\nELECTRA: Pre-training Text Encoders as Discriminators\nRather Than Generators. arXiv preprint arXiv:2003.10555, 2020.\n[14] Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaud-\nhary, Guillaume Wenzek, Francisco Guzm´an, Edouard Grave, Myle Ott,\nLuke Zettlemoyer, and Veselin Stoyanov. Unsupervised Cross-lingual\nRepresentation Learning at Scale. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Linguistics, pages 8440–\n8451, 2020.\n[15] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer.\nQLoRA: Efficient Finetuning of Quantized LLMs.\narXiv preprint\narXiv:2305.14314, 2023.\n[16] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\nBERT: Pre-training of Deep Bidirectional Transformers for Language\nUnderstanding. In Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long and Short Papers),\npages 4171–4186, 2019.\n[17] Mona Diab, Mahmoud Ghoneim, Abdelati Hawwari, Fahad AlGhamdi,\nNada AlMarwani, and Mohamed Al-Badrashiny.\nCreating a Large\nMulti-Layered Representational Repository of Linguistic Code Switched\nArabic Data. In Proceedings of the Tenth International Conference on\nLanguage Resources and Evaluation (LREC’16), pages 4228–4235, May\n2016.\n[18] Praveen Dominic, Niranjan Purushothaman, Anish Skanda Anil Kumar,\nA Prabagaran, J Angelin Blessy, and A John. Multilingual Sentiment\nAnalysis using Deep-Learning Architectures. In 2023 5th International\nConference on Smart Systems and Inventive Technology (ICSSIT), pages\n1077–1083. IEEE, 2023.\n[19] Bojan Evkoski, Andraˇz Pelicon, Igor Mozetiˇc, Nikola Ljubeˇsi´c, and\nPetra Kralj Novak. Retweet communities reveal the main sources of\nhate speech. PLOS ONE, 17(3):1–22, 03 2022.\n[20] Darja Fiˇser, Nikola Ljubeˇsi´c, and Tomaˇz Erjavec. The Janes project: lan-\nguage resources and tools for Slovene user generated content. Language\nResources and Evaluation, 54(1):223–246, Mar 2020.\n[21] Matt Gardner, Yoav Artzi, Victoria Basmov, Jonathan Berant, Ben\nBogin, Sihao Chen, Pradeep Dasigi, Dheeru Dua, Yanai Elazar, Ananth\nGottumukkala, Nitish Gupta, Hannaneh Hajishirzi, Gabriel Ilharco,\nDaniel Khashabi, Kevin Lin, Jiangming Liu, Nelson F. Liu, Phoebe Mul-\ncaire, Qiang Ning, Sameer Singh, Noah A. Smith, Sanjay Subramanian,\nReut Tsarfaty, Eric Wallace, Ally Zhang, and Ben Zhou. Evaluating\nModels’ Local Decision Boundaries via Contrast Sets. In Findings of the\nAssociation for Computational Linguistics: EMNLP 2020, pages 1307–\n1323, 2020.\n[22] Yanzhu Guo, Virgile Rennard, Christos Xypolopoulos, and Michalis\nVazirgiannis. BERTweetFR : Domain Adaptation of Pre-Trained Lan-\nguage Models for French Tweets.\nIn Proceedings of the Seventh\nWorkshop on Noisy User-generated Text (W-NUT 2021), pages 445–450,\nNovember 2021.\n[23] Ehtesham Hashmi, Sule Yildirim Yayilgan, and Sarang Shaikh. Aug-\nmenting sentiment prediction capabilities for code-mixed tweets with\nmultilingual transformers.\nSocial Network Analysis and Mining,\n14(1):86, 2024.\n[24] Eftekhar Hossain, Omar Sharif, and Mohammed Moshiul Hoque. NLP-\nCUET@LT-EDI-EACL2021: Multilingual Code-Mixed Hope Speech\nDetection using Cross-lingual Representation Learner. arXiv preprint\narXiv:2103.00464, 2021.\n[25] Aditya Joshi, Ameya Prabhu, Manish Shrivastava, and Vasudeva Varma.\nTowards Sub-Word Level Compositions for Sentiment Analysis of\nHindi-English Code Mixed Text. In Proceedings of COLING 2016, the\n26th International Conference on Computational Linguistics: Technical\nPapers, 2016.\n[26] Raviraj Joshi. L3Cube-HindBERT and DevBERT: Pre-Trained BERT\nTransformer models for Devanagari based Hindi and Marathi Languages,\n2023.\n[27] Sujata S Kathpalia and KENNETH KENG WEE ONG. The use of code-\nmixing in Indian billboard advertising. World Englishes, 34(4):557–575,\n2015.\n[28] Simran Khanuja, Diksha Bansal, Sarvesh Mehtani, Savya Khosla,\nAtreyee Dey, Balaji Gopalan, Dilip Kumar Margam, Pooja Aggarwal,\nRajiv Teja Nagipogu, Shachi Dave, et al. MuRIL: Multilingual Rep-\nresentations for Indian Languages.\narXiv preprint arXiv:2103.10730,\n2021.\n[29] Simran Khanuja, Sandipan Dandapat, Sunayana Sitaram, and Monojit\nChoudhury. A New Dataset for Natural Language Inference from Code-\nmixed Conversations.\nIn Proceedings of the The 4th Workshop on\nComputational Approaches to Code Switching, pages 9–16, 2020.\n[30] Simran Khanuja, Sandipan Dandapat, Anirudh Srinivasan, Sunayana\nSitaram, and Monojit Choudhury. GLUECoS: An Evaluation Benchmark\nfor Code-Switched NLP. arXiv preprint arXiv:2004.12376, 2020.\n[31] Yuri Kuratov and Mikhail Arkhipov. Adaptation of Deep Bidirectional\nMultilingual Transformers for Russian Language. In Computational Lin-\nguistics and Intellectual Technologies: Proceedings of the International\nConference “Dialogue 2019”, 2019.\n[32] Yuri Kuratov and Mikhail Arkhipov.\nToxic comments detection in\nRussian. In Computational Linguistics and Intellectual Technologies:\nProceedings of the International Conference “Dialogue 2020”, 2020.\n[33] Brian Lester, Rami Al-Rfou, and Noah Constant. The Power of Scale for\nParameter-Efficient Prompt Tuning. arXiv preprint arXiv:2104.08691,\n2021.\n[34] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi\nChen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.\nRoBERTa: A Robustly Optimized BERT Pretraining Approach. arXiv\npreprint arXiv:1907.11692, 2019.\n[35] Nikola Ljubeˇsi´c, Tomaˇz Erjavec, and Darja Fiˇser. Datasets of Slovene\nand Croatian Moderated News Comments.\nIn Proceedings of the\n2nd Workshop on Abusive Language Online (ALW2), pages 124–131,\nOctober 2018.\n[36] Ilya Loshchilov and Frank Hutter. Decoupled Weight Decay Regular-\nization. In 7th International Conference on Learning Representations,\n2019.\n[37] Louis Martin, Benjamin Muller, Pedro Javier Ortiz Su´arez, Yoann\nDupont, Laurent Romary, ´Eric Villemonte de La Clergerie, Djam´e\nSeddah, and Benoˆıt Sagot.\nCamemBERT: a Tasty French Language\nModel. arXiv preprint arXiv:1911.03894, 2019.\n[38] Chris McCormick and Nick Ryan. BERT Fine-Tuning Tutorial with Py-\nTorch. https://mccormickml.com/2019/07/22/BERT-fine-tuning/, 2019.\n[39] Sneha Mondal, Ritika, Shreya Pathak, Preethi Jyothi, and Aravindan\nRaghuveer. CoCoa: An Encoder-Decoder Model for Controllable Code-\nswitched Generation.\nIn Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing, pages 2466–2479,\nDecember 2022.\n[40] Igor Mozetiˇc, Luis Torgo, Vitor Cerqueira, and Jasmina Smailovi´c. How\nto evaluate sentiment classifiers for Twitter time-ordered data? PLOS\nONE, 13(3):1–20, 03 2018.\n[41] Ravindra Nayak and Raviraj Joshi. L3Cube-HingCorpus and HingBERT:\nA Code Mixed Hindi-English Dataset and BERT Language Models.\nIn Proceedings of the WILDRE-6 Workshop within the 13th Language\nResources and Evaluation Conference, June 2022.\n[42] Ravindra Nayak and Raviraj Joshi. L3Cube-HingCorpus and HingBERT:\nA code mixed Hindi-English dataset and BERT language models. arXiv\npreprint arXiv:2204.08398, 2022.\n[43] Pedro Javier Ortiz Su´arez, Laurent Romary, and Benoˆıt Sagot.\nA\nMonolingual Approach to Contextualized Word Embeddings for Mid-\nResource Languages. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages 1703–1714, July\n2020.\n[44] Rosalind W Picard. Affective Computing. MIT press, 2000.\n[45] Sebastin Santy, Anirudh Srinivasan, and Monojit Choudhury. BERTo-\nlogiCoMix: How does Code-Mixing interact with Multilingual BERT?\n12\nIn Proceedings of the Second Workshop on Domain Adaptation for NLP,\npages 111–121, 2021.\n[46] Hope Jensen Schau, Stephanie Dellande, and Mary C. Gilly. The impact\nof code switching on service encounters. Journal of Retailing, 83(1):65–\n78, 2007. Service Excellence.\n[47] Dama Sravani, Lalitha Kameswari, and Radhika Mamidi.\nPolitical\nDiscourse Analysis: A Case Study of Code Mixing and Code Switching\nin Political Speeches.\nIn Proceedings of the Fifth Workshop on\nComputational Approaches to Linguistic Code-Switching, pages 1–5,\n2021.\n[48] Vivek Srivastava and Mayank Singh. PHINC: A Parallel Hinglish Social\nMedia Code-Mixed Corpus for Machine Translation. In Proceedings of\nthe Sixth Workshop on Noisy User-generated Text (W-NUT 2020), pages\n41–49, November 2020.\n[49] Samson Tan and Shafiq Joty. Code-Mixing on Sesame Street: Dawn\nof the Adversarial Polyglots. In Proceedings of the 2021 Conference\nof the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages 3596–3616, June\n2021.\n[50] S Thara and Prabaharan Poornachandran.\nCode-Mixing: A Brief\nSurvey. In 2018 International conference on advances in computing,\ncommunications and informatics (ICACCI), pages 2382–2388, 2018.\n[51] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Alma-\nhairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal\nBhargava, Shruti Bhosale, et al. LLaMa 2: Open Foundation and Fine-\nTuned Chat Models. arXiv preprint arXiv:2307.09288, 2023.\n[52] Chris K. Tran. BERT for Sentiment Analysis. https://chriskhanhtran.\ngithub.io/posts/bert-for-sentiment-analysis/, 2019.\n[53] Matej Ulˇcar and Marko Robnik-ˇSikonja. SloBERTa: Slovene monolin-\ngual large pretrained masked language model. In Proceedings of the\n24th International Multiconference – IS2021 (SiKDD), 2021.\n[54] Matej Ulˇcar and Marko Robnik-ˇSikonja. Training dataset and dictionary\nsizes matter in BERT models: the case of Baltic languages.\nIn\nInternational Conference on Analysis of Images, Social Networks and\nTexts, pages 162–172, 2021.\n[55] Matej Ulˇcar and Marko Robnik-ˇSikonja. FinEst BERT and CroSloEn-\ngual BERT: Less Is More in Multilingual Models. In Text, Speech, and\nDialogue: 23rd International Conference, TSD 2020, Proceedings, page\n104–111, 2020.\n[56] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\nJones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention\nis All You Need. In Proceedings of the 31st International Conference\non Neural Information Processing Systems, 2017.\n[57] Genta Indra Winata, Samuel Cahyawijaya, Zihan Liu, Zhaojiang Lin,\nAndrea Madotto, and Pascale Fung. Are Multilingual Models Effective\nin Code-Switching? In Proceedings of the Fifth Workshop on Compu-\ntational Approaches to Linguistic Code-Switching, pages 142–153, June\n2021.\n[58] Ruochen Zhang, Samuel Cahyawijaya, Jan Christian Blaise Cruz, Genta\nWinata, and Alham Aji.\nMultilingual Large Language Models Are\nNot (Yet) Code-Switchers.\nIn Proceedings of the 2023 Conference\non Empirical Methods in Natural Language Processing, pages 12567–\n12582, Singapore, December 2023.\n[59] Xinyang Zhang, Yury Malkov, Omar Florez, Serim Park, Brian\nMcWilliams, Jiawei Han, and Ahmed El-Kishky.\nTwHIN-BERT:\nA Socially-Enriched Pre-Trained Language Model for Multilingual\nTweet Representations at Twitter.\nIn Proceedings of the 29th ACM\nSIGKDD Conference on Knowledge Discovery and Data Mining, page\n5597–5607, 2023.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2024-05-21",
  "updated": "2024-05-21"
}