{
  "id": "http://arxiv.org/abs/2405.14374v1",
  "title": "State-Constrained Offline Reinforcement Learning",
  "authors": [
    "Charles A. Hepburn",
    "Yue Jin",
    "Giovanni Montana"
  ],
  "abstract": "Traditional offline reinforcement learning methods predominantly operate in a\nbatch-constrained setting. This confines the algorithms to a specific\nstate-action distribution present in the dataset, reducing the effects of\ndistributional shift but restricting the algorithm greatly. In this paper, we\nalleviate this limitation by introducing a novel framework named\n\\emph{state-constrained} offline reinforcement learning. By exclusively\nfocusing on the dataset's state distribution, our framework significantly\nenhances learning potential and reduces previous limitations. The proposed\nsetting not only broadens the learning horizon but also improves the ability to\ncombine different trajectories from the dataset effectively, a desirable\nproperty inherent in offline reinforcement learning. Our research is\nunderpinned by solid theoretical findings that pave the way for subsequent\nadvancements in this domain. Additionally, we introduce StaCQ, a deep learning\nalgorithm that is both performance-driven on the D4RL benchmark datasets and\nclosely aligned with our theoretical propositions. StaCQ establishes a strong\nbaseline for forthcoming explorations in state-constrained offline\nreinforcement learning.",
  "text": "State-Constrained Offline Reinforcement Learning\nCharles A. Hepburn1, Yue Jin2, Giovanni Montana2,3\n1Mathematics Institute, 2Warwick Manufacturing Group, 3Department of Statistics\nUniversity of Warwick\n{charlie.hepburn, yue.jin.3, g.montana}@warwick.ac.uk\nMay 24, 2024\nAbstract\nTraditional offline reinforcement learning methods predominantly operate in a batch-\nconstrained setting. This confines the algorithms to a specific state-action distribution present\nin the dataset, reducing the effects of distributional shift but restricting the algorithm greatly.\nIn this paper, we alleviate this limitation by introducing a novel framework named state-\nconstrained offline reinforcement learning. By exclusively focusing on the dataset’s state\ndistribution, our framework significantly enhances learning potential and reduces previous\nlimitations. The proposed setting not only broadens the learning horizon but also improves\nthe ability to combine different trajectories from the dataset effectively, a desirable property\ninherent in offline reinforcement learning. Our research is underpinned by solid theoretical\nfindings that pave the way for subsequent advancements in this domain. Additionally, we\nintroduce StaCQ, a deep learning algorithm that is both performance-driven on the D4RL\nbenchmark datasets and closely aligned with our theoretical propositions. StaCQ establishes a\nstrong baseline for forthcoming explorations in state-constrained offline reinforcement learning.\n1\nIntroduction\nOffline reinforcement learning (RL) focuses on deriving an optimal decision-making policy from a\nfixed dataset of previously collected experiences [36, 38]. It prohibits online exploration, meaning\nthat effective policies must be constructed solely from the evidence provided by an unknown,\npotentially sub-optimal behaviour policy active in the environment. This approach is especially\nsuited for real-world scenarios where executing sub-optimal actions can be dangerous, time-\nconsuming, or costly, yet there is an abundance of prior data. Relevant applications include\nrobotics [46, 34, 47], long-term healthcare treatment plans [51, 50, 44], and autonomous driving\n[43, 18, 16]. Despite its practical appeal, offline RL faces a significant challenge called distributional\nshift [33]. This phenomenon arises when attempting to estimate the values of actions not present in\nthe dataset, often manifesting as an overestimation. As a result, out-of-distribution (OOD) actions\nare perceived as more valuable than they truly are, causing the agent to select sub-optimal actions\nand leading to the accumulation of errors [22].\nCurrently available offline RL techniques mitigate the issue of distributional shift through\ntwo primary approaches. The first confines the policy to produce only actions likely to emerge\n1\narXiv:2405.14374v1  [stat.ML]  23 May 2024\nfrom the dataset or its distribution [22, 33, 55, 45, 31, 63, 21]. The second approach revolves\naround conservatively estimating the value of OOD actions [35, 58, 3]. Both strategies aim to\nanchor the learning algorithm to the state-action pairs within the dataset, an approach termed\nbatch-constrained [22]. By doing so, these methods seek to minimise the impact of distributional\nshift. However, adhering strictly to the state-action distribution can be restrictive. Thus, recent\ntechniques seek a balance between exploiting potentially valuable OOD actions and minimising\ndistributional shift repercussions.\nGiven the constraints of the state-action distribution, a pivotal question arises: can we confine\nour methods to the state distribution alone and still counteract distributional shift?\nConstraining exclusively to the dataset’s state distribution could significantly reduce the\nrequisite dataset size. Instead of needing all state-action pairs, the focus shifts to states alone.\nThis approach allows the agent to initiate out-of-distribution (OOD) actions, provided they lead to\na known state. Such flexibility can enhance the capability of offline RL algorithms in trajectory\nstitching—combining sub-optimal experiences into improved trajectories. Rather than connecting\nbased on actions from different trajectories, the state-constrained approach facilitates stitching by\nleveraging adjacent states. This method relies on understanding these proximate states, a concept\nwe label as reachability. This concept refers to the ability to reach a particular state from another\nstate within the dataset. In the state-constrained approach, understanding reachability is crucial\nfor effectively stitching together sub-optimal trajectories and improving the overall policy. In some\nenvironments, reachability is easily delineated; for example, in grid-based environments like mazes,\nwhere the agent can move directly between adjacent cells. In other environments, reachability must\nbe ascertained from the dataset, such as in complex robotic systems where state transitions depend\non intricate dynamics.\nBuilding upon the concept of reachability, in this paper, we develop a state-constrained offline\nRL approach and make several contributions to this new area. In the batch-constrained approach,\nthe learning algorithm is anchored to the state-action pairs within the dataset. In contrast, the\nstate-constrained approach focuses solely on the states, allowing for more flexibility in action\nselection while still mitigating distributional shift. We provide our state-constrained framework\nwith theoretical guarantees, built on a few foundational assumptions. We demonstrate that the\nstate-constrained framework will always construct a policy that produces higher or equal-value\nactions compared to the batch-constrained policy. Finally, we propose and empirically validate a\nnovel deep state-constrained offline RL algorithm, called StaCQ. StaCQ learns a state-constrained\nQ-function and updates the policy to remain close to the highest quality reachable states within the\ndataset. Our experiments show that StaCQ demonstrates competitive performance, outperforming\nseveral state-of-the-art methods on the D4RL benchmarking dataset [20] in many tasks. StaCQ\noutperforms all model-free baselines in the locomotion tasks and shows strong performance in the\nAntmaze tasks. These results provide a strong basis for many future state-constrained algorithms\nto be built upon. We believe that this work will serve as a foundation for the development of future\nstate-constrained algorithms, much like how BCQ [22] has done for batch-constrained techniques,\nand contribute to the advancement of offline RL.\n2\n2\nRelated work\nModel-free offline RL.\nBCQ [22] emerged as one of the pioneer offline deep RL methodologies.\nIt posits that possessing all (s, a)-pairs in the MDP allows for constructing an optimal QA-\nvalue by narrowing the update solely to (s, a)-pairs present in the dataset. Consequently, BCQ\nrestricts the policy to (s, a)-pairs in the dataset while allowing minimal perturbations to facilitate\nmarginal benefits. Post-BCQ, numerous algorithms have been proposed to devise alternative means\nto limit the policy to (s, a)-pairs in the dataset, be it in the support of the (s, a)-distribution\n[33, 55, 45, 31, 63, 8] or directly to the (s, a)-pairs [21]. In a distinct vein, instead of policy\nconstraints, certain methods have sought to circumvent distributional shifts by pessimistically\nevaluating the value function on out-of-distribution (s, a)-pairs [35, 58, 3]. A subset of offline RL\nmethodologies both pessimistically evaluate the value function and constrain the policy [14, 7].\nBoth clusters of methods adopt the batch-constrained approach which prioritises (s, a)-pairs in\nthe dataset over unobserved actions. Our state-constrained method eases the batch-constrained\nobjective, necessitating constraint only to dataset states rather than (s, a)-pairs.\nModel-based offline RL.\nModel-based methods learn the dynamics of the environment to aid\npolicy learning [48, 28]. There is usually two main approaches to model-based offline RL. The first\napproach learns a pessimistic model of the environment and performs rollouts using this augmented\nmodel, increasing the dataset size [59, 29, 42]. The other main approach aims to use the models\nto perform planning [4, 60, 27, 15]. This means that, during evaluation the models are used to\nlook in the future to decide the optimal path for the agent. In contrast, StaCQ uses the models to\nimplement our notion of state reachability. This means that the models are not used to imagine\nnew states, and there is no planning involved at evaluation.\nState reachability.\nThe state-constrained strategy heavily leans on state reachability. Earlier\nstudies have explored state reachability [25, 26], with a Gaussian distribution determining the\nstitching feasibility of s′ from s. Our approach, in contrast, deems a state as either reachable\nor not, steering clear of continuous probability distributions. The notion of state reachability\naligns with state similarity, a subject of extensive study in literature [61, 1, 37]. One technique to\ngauge state similarity is through bisimulation metrics grounded in the environment’s dynamics\n[19]. Bisimulations usually mandate full state enumeration [11, 5, 6], prompting the introduction\nof a scalable pseudometric approach [10]. Integrating a pseudometric into our state reachability\nconcept would be intriguing, but such exploration is deferred to future research. Our metric for\nstate reachability is intuitively graspable and stems directly from the foundational definition.\nTrajectory stitching.\nOffline RL’s hallmark is its stitching capability [32]. This entails amalga-\nmating previously observed trajectories to produce a novel trajectory that accomplishes the task.\nImitation learning strategies, such as behavioural cloning (BC) [40, 41], are ineffective at stitching,\ngiven their inability to discern between optimal and sub-optimal states. Analogous to BC, the\nDecision transformer (DT)[12] converts the offline RL challenge into a supervised learning task\nby employing a goal-conditioned policy. Despite DT’s deficient stitching capabilities, efforts have\nbeen undertaken to rectify this by incorporating offline RL characteristics [57, 56]. Integrating\nDT with the policy extraction method of StaCQ may offer potential insights into enhancing\n3\nstitching capabilities. However, our present study is squarely centred on a basic implementation of\nstate-constrained offline RL.\n3\nPreliminaries\nThe RL framework involves an agent interacting with an environment, which is typically represented\nas a Markov Decision Process (MDP). Such an MDP is defined as M = (S, A, P, R, γ), where S\nand A denote the state and action spaces, respectively; P = p(s′|s, a) represents the environment\ntransition dynamics; R = r(s, a, s′) is the reward function for transitioning; and 0 ≤γ < 1\nis the discount factor [49]. We focus on the deterministic MDP, where p(s′|s, a) = {0, 1} and\nr(s, a, s′) = r(s, s′). In RL, the agent’s objective is to identify an optimal policy, π(s), that\nmaximises the future discounted sum of rewards P\ni=t γi−tri(si, s′\ni).\nQA-values, Q(s, a) = Eπ[P\ni=0 γtrt(st, at)|s0 = s, a0 = a], are the expected future discounted\nrewards for executing action a in state s and there after following policy π. Q-learning [54], denoted\nas QA-learning in this paper, estimates the QA-values under the assumption that future actions\nare chosen optimally,\nQ(s, a) ←(1 −α)Q(s, a) + α\nh\nr(s, a, s′) + γ max\na′ Q(s′, a′)\ni\n.\n(1)\nThe optimal policy derived from QA-learning identifies the best action that maximises the QA-\nvalues, π∗(s) = arg maxa Q(s, a). In deterministic contexts, QA-learning parallels QS-learning [17],\nwhich estimates the expected rewards upon transitioning from state s to s′, followed by optimal\ndecisions:\nQ(s, s′) ←(1 −α)Q(s, s′) + α\nh\nr(s, s′) + γ max\ns′′ Q(s′, s′′)\ni\n.\n(2)\nQS-learning enables QS-values to be learned for transitioning, without evaluating actions. The\noptimal policy via QS-learning discerns the most advantageous subsequent state to maximise\nQS-values, π∗\ns(s) = arg maxs′ Q(s, s′). An action can then be retrieved from an inverse dynamics\nmodel as a = I(s, π∗\ns(s)), a strategy designed to address QA-learning’s challenges in redundant\naction spaces. Redundant action spaces refer to situations where multiple actions lead to the same\nnext state, making it challenging for QA-learning to distinguish between them.\nIn offline RL, the agent aims to discover an optimal policy; however, it must solely rely on a\nfixed dataset without further interactions with the environment [36, 38]. The dataset comprises\ntrajectories made up of transitions consisting of the current state, action, next state, and the reward\nfor transitioning, where actions are chosen based on an unknown behaviour policy, πβ. Batch-\nConstrained QA-learning (BCQL) [22] adapts QA-learning for the offline setting by restricting the\noptimisation to state-action pairs present in the dataset:\nQ(s, a) ←(1 −α)Q(s, a) + α\nh\nr(s, a, s′) + γ\nmax\na′s.t.(s′,a′)∈D Q(s′, a′)\ni\n.\n(3)\nThis approach is restrictive since convergence to the optimal value function is only ensured if\nevery conceivable (s, a)-pair from the MDP resides within the dataset. Such a limitation aims to\nsidestep extrapolation errors resulting from distributional shifts. Nonetheless, this framework places\na significant constraint on any learning algorithm. Subsequent sections relax this constraint: instead\n4\nof adhering strictly to state-action pairs (batch-constrained), the learning process is only bound by\nstates (state-constrained). In the following section, we introduce the state-constrained framework\nand provide a new algorithm called state-constrained QS-learning (SCQL). We show that, under\nminor assumptions, SCQL converges to the optimal QS-value and produces a less-restrictive policy\nthan BCQL.\n4\nState-constrained QS-learning\nIn this section, we provide a formal introduction to state-constrained QS-learning and establish its\nconvergence to the optimal QS-value under a set of minimal assumptions. Within this framework,\nthe learning updates are restricted exclusively to the states present in the dataset. A rigorous\ndefinition of state reachability is indispensable for this setting:\nDefinition 4.1. (State reachability) In a deterministic MDP M, a state s′ is considered\nreachable from state s if and only if there exists an action a such that p(s′|s, a) = 1. We denote\nthe set of states reachable from s as SRM(s), where s′ ∈SRM(s).\nDefinition 4.1 implies that a state is reachable if there exists an action that, when executed\nin the environment, leads to that state. This definition allows multiple reachable next states to\nbe evaluated rather than a single state-action pair. By evaluating multiple reachable next states,\nstate-constrained QS-learning allows for more flexibility in the learning updates compared to\nbatch-constrained approaches, which are limited to evaluating only existing state-action pairs in\nthe dataset.\n4.1\nTheory\nIn this section, we initially adapt the theorems presented in BCQL [22] to suit the state-constrained\ncontext (Theorems 4.4 - 4.6). All our subsequent theorems are based on QS-learning. However, it\nis important to note that these theorems still hold if QA-learning is used instead, with an inverse\nmodel defined to evaluate actions between states and reachable next states.\nOur theory operates under the following assumptions: (A1) the environment is deterministic;\n(A2) the rewards are bounded such that ∀(s, s′), |r(s, s′)| ≤c; (A3) the QS-values, Q(s, s′), are\ninitialised to finite values; and (A4) the discount factor is set such that 0 ≤γ < 1.\nFirst, we show that under these assumptions, QS-learning converges to the expert QS-value.\nTheorem 4.2. Under assumptions A1-4, and with the training rule given by\nQ(s, s′) ←r(s, s′) + γ\nmax\ns′′ s.t. s′′∈SRM(s′) Q(s′, s′′),\nassuming each (s, s′) pair is visited infinitely often, then as updates are performed, the agent’s\nhypothesis Qn(s, s′) after the nth update converges to the expert QS-value, Q∗(s, s′), as n →∞for\nall s, s′.\nProof. This follows from the convergence of QA-learning in a deterministic MDP [39]. For brevity\nand clarity, the full proof is shown in the Appendix.\n5\nThe convergence of QS-learning in a deterministic MDP, as shown in Theorem 4.2, is crucial\nfor establishing the convergence of SCQL. Since SCQL is based on QS-learning and operates\nin a deterministic MDP, Theorem 4.2 provides the foundation for proving the convergence and\noptimality of SCQL under certain assumptions. We now demonstrate that learning the value\nfunction from the dataset D is equivalent to determining the value function of an associated MDP,\ndenoted as MS. Intuitively, MS is the MDP where all transitions are possible between reachable\nstates found in D.\nDefinition 4.3. (State-constrained MDP) Let the state-constrained MDP MS = (S, A, PS, R, γ).\nHere, both S and A remain identical to those in the original MDP, M. The transition probability\nis given by: a = I(s, s′) ∈A\npS(s′|s, a) =\n\n\n\n1\nif (s, s′ ∈D and s′ ∈SRM(s)) or (s /∈D and s′ = sterminal)\n0\notherwise.\nThe reward function and discount factor remain the same as the original MDP.\nThe transition probability pS(s′|s, a) in the state-constrained MDP is defined such that tran-\nsitions are possible only between states that are reachable from each other according to the\nstate reachability definition. This ensures that the state-constrained MDP captures the essential\ndynamics of the original MDP while focusing on the states present in the dataset. The reward\nr(s, s′) is assigned as the original reward defined in M. Note that such states s and s′ may not\nexist as a pair (s, s′) in D. For the case where s is absent from the dataset, the rewards are set\nto the initialised values Q(s, s′). Importantly, the s and s′ in Definition 4.3 both exist in the\ndataset but not as a pair; this means that more transitions exist under this definition than in the\nbatch-constrained MDP defined in [22].\nTheorem 4.4. By sampling s from the dataset D, sampling s′ from SRM(s) and performing\nQS-learning on all reachable state-next state pairs, QS-learning converges to the optimal value\nfunction of the state-constrained MDP MS.\nProof. Given in Appendix A.\nTheorem 4.4 establishes that performing QS-learning on all reachable state-next state pairs from\nthe dataset converges to the optimal value function of the state-constrained MDP. This result is\ncrucial for understanding the convergence and optimality properties of SCQL, as it shows that QS-\nlearning effectively learns the optimal value function of the state-constrained MDP, which is closely\nrelated to the original MDP. We are now ready to define the state-constrained QS-learning (SCQL)\nupdate which is similar to the BCQL formulation except now the maximisation is constrained to\nthe states rather than state-action pairs in the dataset. This formulation allows the maximisation\nto be taken over more values composing more accurate Q-values while still staying close to the\ndataset:\nQ(s, s′) ←(1 −α)Q(s, s′) + α\nh\nr(s, s′) + γ\nmax\ns′′s.t.s′′∈D\ns′′∈SRM(s′)\nQ(s′, s′′)\ni\n.\n(4)\nSCQL, Eq. (4), converges under the identical conditions as traditional QS-learning, primarily\nbecause the state-constrained setting is non-limiting whenever every state in the MDP is observed.\n6\nTheorem 4.5. Under assumptions A1-4 and assuming every state s is encountered infinitely,\nSCQL converges to the optimal value function, denoted as Q∗.\nProof. This follows from Theorem 4.2, noting the state-constraint is non-restrictive with a dataset\nwhich contains all possible states.\nTheorem 4.5 is a reduction in the restriction compared to BCQL as now we only require every\nstate to be encountered infinitely rather than every (s, a) -pair.\nThe optimal policy for our state-constrained approach can be formulated as:\nπ∗\ns(s) = arg max\ns′ s.t.s′∈D\ns′∈SRM(s)\nQ∗(s, s′).\n(5)\nHere, the maximisation is taken over next states from the dataset and that are reachable from the\ncurrent state. We now demonstrate that Eq. (5) represents the optimal state-constrained policy.\nTheorem 4.6. Under assumptions A1-4 and assuming every s is visited infinitely, then SCQL\nconverges to Qπ\nS(s, s′), with the optimal state-constrained policy defined by Eq. (5) and s′ ∈SRM(s).\nProof. Given in Appendix A.\nTheorem 4.6 has important practical implications for the performance of SCQL in real-world\nscenarios with limited datasets. It suggests that SCQL can learn an optimal state-constrained\npolicy even when the dataset does not contain all possible state-action pairs, as long as every state\nis visited infinitely often. The state-constrained approach greatly reduces the limitations placed\non the learning algorithm compared to the batch-constrained method. In the batch-constrained\napproach, the necessity for every state-action pair to be present in the dataset demands a dataset\nof size at least |S| × |A|. On the other hand, the state-constrained method only mandates that\neach state be visited, which minimally requires a dataset size of |S|.\nWe will now show that BCQL is a special case of SCQL, and that policies produced by SCQL\nwill never be worse than policies produced by BCQL.\nTheorem 4.7. In a deterministic MDP, SCQL will produce a policy that is never worse than the\npolicy produced by BCQL.\nProof. Given in Appendix A.\nFrom Theorems 4.6 and 4.7, the state-constrained approach poses fewer restrictions than the\nbatch-constrained counterpart while also always producing a policy at least as good. SCQL can\nperform more Q-value updates and maximize over more states than BCQL because it considers\nall reachable state-next state pairs from the dataset, rather than being limited to the explicit\nstate-action pairs present in the dataset. This allows SCQL to exploit the structure of the state-\nconstrained MDP more effectively and learn a better policy, with less required data. In the next\nsection, this improvement is elaborated through an illustrative example which shows SCQL excelling\neven with a limited dataset.\n7\n4.2\nAn illustrative example: maze navigation\nTo elucidate the advantages of SCQL in comparison to BCQL, we explore a simple maze environment\nbacked by a dataset with few trajectories. The maze is visualised as a 10 by 10 grid. The available\nstates are shown as white squares, while the red squares represent the maze walls. Each state in\nthis environment is denoted by the (x, y) coordinates within the maze, and the agent’s available\nactions comprise of simple movements: left, right, up, and down. With each movement, the agent\nincurs a reward penalty of a small negative value, but upon successfully navigating to the gold star,\nit is rewarded with a large positive value. Consequently, the agent’s primary objective becomes\ndevising the most direct route to the star. In this deterministic environment, the outcomes of\ntransitioning to a state and executing an action are congruent, meaning Eq. (1) and (2) mirror each\nother. This allows for a direct comparison between BCQL and SCQL in terms of their performance\nand ability to leverage limited datasets.\n(a) Maze and Dataset\n(b) BCQL Final Policy\n(c) SCQL Final Policy\nFigure 1: Comparison of BCQL and SCQL methods on a simple maze environment. (a) The maze\nis a 10 by 10 grid where the co-ordinate values (x, y) represent the state. The high reward region\nis represented by a star and maze walls are represented by red grid squares. The dataset is made\nof 4 trajectories represented by the dotted lines where the open circle is the starting state and the\nclosed circle is the final state. (b) The final policy when applying BCQL to the dataset. (c) The\nfinal policy when applying SCQL to the dataset.\nFigure 1a provides a visual representation of the maze and accompanying dataset. Here,\ntrajectories start at the white circle and end at the black one. Despite its simplicity, the dataset\ncontains a scant number of states wherein multiple actions have been taken. Within this specific\nframework, the notion of state reachability is straightforward: any state that is just one block away\nfrom the current state is considered reachable. Formally the state reachability is, SRM((x, y)) =\n{(x + 1, y), (x −1, y), (x, y + 1), (x, y −1)}. In other words, a state is considered reachable from\nthe current state if it is one block away in any of the four cardinal directions (left, right, up, or\ndown). This simple definition of state reachability is sufficient for demonstrating the advantages of\nSCQL in this illustrative example.\nApplying BCQL with training as per Eq. (3) and policy extraction as per\nπ(s) =\nmax\na s.t. (s,a)∈D Q(s, a),\nwe obtain the policy depicted in Figure 1b, where arrows indicate policy direction. Conversely,\n8\nfor SCQL, training via Eq. (4) and policy training through Eq. (5) yields the policy in Figure\n1c. For clarity, if no optimal movement can be found in the state, due to no information in the\ndataset, it is left blank. SCQL is able to leverage the sparse dataset more efficiently, enabling\nthe agent to reach the gold star from any dataset state position. In contrast, BCQL succeeds in\nonly 11 of the 57 unique dataset states, this is equivalent to behavioural cloning’s performance.\nSCQL’s advantage stems from its ability to leverage state adjacency information to stitch together\noptimal trajectories, whereas BCQL relies solely on previously observed state-action pairs, limiting\nits performance in datasets with a small number of trajectories.\n5\nPractical implementation: StaCQ\nIn this section, we introduce StaCQ (State-Constrained deep QS-learning), our particular deep\nlearning implementation of SCQL. We present a policy constraint method that regularises the\npolicy towards the best reachable next state in the dataset. Our specific implementation learns\na QS-function that is aligned with the underlying theory. However, state-constrained methods\ncould be adapted from the theory in many ways, analogous to many techniques that modify the\nbatch-constraint for enhancing BCQ [22]. For example, alternative state-constrained methods\ncould incorporate different policy regularisation techniques, such as those based on divergence\nmeasures or constraint violation penalties, or they could explore different ways of estimating state\nreachability based on the available data and domain knowledge.\n5.1\nEstimating state reachability\nCentral to the state-constrained approach is the concept of state reachability, as per Definition 4.1.\nBecause the environments used in our benchmarks do not provide this information, we need to\nestimate reachability from the dataset. In our implementation, we consider ˆs′ reachable to s if we\ncan predict the action that reaches ˆs′. This entails the use of two key models: a forward dynamics\nmodel, fω1(s, a), and an inverse dynamics model, Iω2(s, s′). Both of these models are framed as\nneural networks and are trained via supervised learning using triplets of form (s, a, s′) ∼D. The\nloss function for the forward model is given by\nLω1 = E(s,a,s′)∼D[(fω1(s, a) −s′)2]\n(6)\nwhile the loss for the inverse model is:\nLω2 = E(s,a,s′)∼D[(Iω2(s, s′) −a)2].\n(7)\nConsistent with prior research [9, 13, 28, 4, 59, 58], we adopt an ensemble strategy. Each model\nwithin the ensemble possesses a distinct set of parameters. The ensemble’s final predictions for s′\nand a are computed by taking the average of their respective outputs. It should be noted that\nthese are very simple models and adding more complexity such as [62] will greatly improve model\nprediction and thus state reachability prediction.\n9\nUsing these models, we propose an estimate of state reachability, denoted by d\nSRM, as:\nˆs′ ∈d\nSRM(s)\niff\n||fω1(s, Iω2(s, ˆs′)) −ˆs′||∞≤ϵ.\nThis criterion suggests that ˆs′ is reachable from s only if an action can be predicted that transitions\nthe agent from s to ˆs′. It is important to note that ϵ is a small positive value to account for possible\nmodel inaccuracies. In Appendix B we detail a way of reducing the complexity of this calculation.\n5.2\nStaCQ\nStaCQ makes use of an actor-critic framework; the critic, i.e. the Q(s, s′) value of transitioning\nfrom s to s′, is trained by minimising the mean square error (MSE) between predicted and true\nQS-values. Following the theory, the QS-values are updated on all pairs of states and reachable\nnext states:\nLθ = E\ns∼D\nˆs′∈d\nSRM(s)\n\" \nr(s, ˆs′) + γQθ′(ˆs′, fω1(ˆs′, πϕ(ˆs′))) −Qθ(s, ˆs′)\n!2#\n.\n(8)\nIn this equation, the actor produces an action, πϕ(s′), which is input into the forward model\nfω1, before being evaluated in the QS-function. The specific reward for the reachable pair (s, ˆs′)\nis potentially unseen in the dataset, therefore if the reward function is unknown it must be\napproximated using a neural network. In a manner akin to the forward and inverse dynamics\nmodels, this can be achieved using supervised learning by reducing the mean square error (MSE)\nbetween the predicted and actual rewards:\nLω3 = E(s,r,s′)∼D[(rω3(s, s′) −r)2].\n(9)\nPast research indicates that this type of reward model is proficient in estimating rewards for\npreviously unseen transitions [26]. In Eq. (8), θ′ represents the parameters for a target Q-value\nwhich are incrementally updated towards θ: θ′ ←τθ + (1 −τ)θ′, with τ being the soft update\ncoefficient.\nTo train the actor, we seek to maximise the current QS-values while staying close to the best\nnext state in the dataset, therefore our loss to minimise is:\nLϕ = Es∼D\nh\nλQθ(s, fω1(s, πϕ(s))) + (fω1(s, πϕ(s))) −ˆs′)2i\n.\n(10)\nIn this equation we have a λ hyperparameter which provides our level of regularisation. The ˆs′\nis the best reachable next state, from s, according to the QS-value,\nˆs′ = arg\nmax\ns′∈d\nSRM(s)\nQθ(s, s′).\n10\nAlgorithm 1 StaCQ\n1: Input: Dataset D, T iterations, τ\n2: Initialise: ω1, ω2, ω3, θ, θ′, ϕ\n3: Pre-train fω1 & Iω2: Eqs.(6) & (7)\n4: Pre-train reachability criteria d\nSRM\n5: for t = 1, . . . , T do\n6:\nOptimise reward function: Eq. (9)\n7:\nOptimise QS-value: Eq. (8)\n8:\nOptimise policy: Eq. (10)\n9:\nUpdate target networks:\nθ′ ←τθ + (1 −τ)θ′.\n10: end for\nThis is therefore a similar policy extraction method to TD3+BC [21], where both methods\nuse a BC regulariser. However, StaCQ is able to utilise the state reachability metric enabling\nthe maximisation to be taken over more states, whereas TD3+BC only has a single state-action\npair. Due to the forward model being fixed, a small amount of Gaussian noise is added to the\npolicy action before being input into the model. This creates a more robust policy by ensuring the\npolicy does not exploit the forward model. The full algorithm is shown in Algorithm 1. Notably,\nall three constituent models are trained using supervised learning, making them straightforward\nsub-processes within this architecture.\n6\nExperimental results\nWe evaluate StaCQ against several model-free and model-based baselines, on the D4RL benchmark-\ning datasets from the OpenAI Mujoco tasks [52, 20]. The model-free baselines we compare to are\nBCQ [22], TD3+BC [21] and IQL [32]. BCQ is the most fundamental batch-constrained offline RL\nmethod which many future methods are based from and is thus the most direct comparison theory-\nwise. TD3+BC is the most similar method to StaCQ implementation wise as both techniques use\na BC-style regularisation process. IQL is a current state-of-the-art (SOTA) model-free method\nwhich also provides a useful comparison. The model-based baselines we compare to are MBTS\n[26], Diffuser [27] and RAMBO [42]. MBTS conducts a data augmentation strategy that uses a\ndifferent methodology of state reachability to StaCQ. Diffuser is a planning based method that\nuses a diffusion model, thus a more complex dynamics model than StaCQ. RAMBO is a SOTA\nmodel-based method that performs rollouts using a learned pessimistic dynamics model of the\nenvironment. Although StaCQ does not use models for planning nor rollouts, we include these\nmodel-based algorithms for comparison purposes. We have also devised a one-step [8] version of\nStaCQ, detailed in Appendix D, showing the flexibility of the general state-constrained framework.\nResults for both StaCQ and our one-step version of StaCQ are shown in Table 1. Note that for\nthe Antmaze tasks we do not need to train a reward model as Antmaze tasks are sparse rewards\nand so the reward for being in s′ will remain the same. Despite the model error introduced in\nthe state reachability metric and the policy by our simple models, StaCQ performs remarkably\nwell against the baselines. StaCQ outperforms all the model-free baselines in the locomotion tasks\n11\nTable 1: Average normalised scores on the D4RL datasets. StaCQ results have been obtained\nby taking an average over 5 seeds. The bolded scores are within 95% of the highest performing\nmethod.\nModel-free baselines\nModel-based baselines\nOur methods\nBC\nBCQ TD3+BCIQL\nMBTSDiffuserRAMBO\nStaCQ\nOneStep StaCQ\nHopper\nRand\n6.2\n7.6\n8.5\n-\n-\n-\n21.6\n9.1 ± 0.9\n7.5 ± 0.4\nMed-Rep\n22.5\n51.0\n60.9\n94.7\n50.2\n93.6\n96.6\n99.1 ± 0.7\n99.4 ± 0.6\nMed\n56.8\n60.9\n59.3\n66.3\n64.3\n74.3\n92.8\n101.8 ± 0.3 93.3 ± 3.2\nMed-Exp\n54.2\n85.9\n98.0\n91.5\n94.8\n103.3\n83.3\n111.2 ± 1.7 92.1 ± 9.2\nWalker2D\nRand\n1.4\n4.4\n1.6\n-\n-\n-\n11.5\n4.4 ± 6.4\n6.5 ± 0.9\nMed-Rep\n25.5\n60.7\n81.8\n73.9\n61.5\n70.6\n85.0\n87.2 ± 7.3\n88.1 ± 5.3\nMed\n39.4\n73.7\n83.7\n78.3\n78.8\n79.6\n86.9\n93.1 ± 1.2\n85.7 ± 10.4\nMed-Exp\n90.5\n94.5 110.1\n109.6\n108.8 106.9\n68.3\n114.8 ± 1.5 107.6 ± 8.5\nHalfcheetah\nRand\n2.1\n2.2\n11.0\n-\n-\n-\n40.0\n24.3 ± 1.3\n2.6 ± 1.6\nMed-Rep\n34.5 41.1\n44.6\n44.2\n39.8\n37.7\n68.9\n52.2 ± 0.5\n46.4 ± 0.4\nMed\n42.4\n46.6\n48.3\n47.4\n43.2\n42.8\n77.6\n57.6 ± 0.6\n50.0 ± 0.2\nMed-Exp\n66.6\n87.8\n90.7\n86.7\n86.9\n88.9\n93.7\n96.4 ± 2.9\n94.9 ± 1.0\nTotal\n(excl.\nrand)\n432.4 602.2 677.4\n692.6\n628.3 697.7\n753.1\n813.4\n757.5\nAntmaze\nUmaze\n53.4 70.0\n78.6\n87.5\n-\n-\n25.0\n88.6 ± 2.3\n75.6 ± 3.8\nU-diverse\n64.6 44.0\n71.4\n62.2\n-\n-\n0.0\n60.4 ± 26.4\n67.2 ± 13.8\nM-play\n0.0\n0.0\n3.0\n71.2\n-\n-\n16.4\n45.2 ± 26.0\n20.6 ± 12.5\nM-diverse\n0.8\n0.0\n10.6\n70.0\n-\n-\n23.2\n46.8 ± 26.2\n13.0 ± 4.2\nL-play\n0.0\n0.0\n0.0\n39.6\n-\n-\n0.0\n31.6 ± 5.0\n5.2 ± 5.2\nL-diverse\n0.0\n0.0\n0.2\n47.5\n-\n-\n2.4\n37.0 ± 21.5\n2.2 ± 1.6\nTotal\n(Antmaze)\n118.8 114.0 163.8\n378.0\n-\n-\n67.0\n309.6\n183.8\n(Hopper. Halfcheetah and Walker2d) on the whole range of datasets (random, medium-replay,\nmedium and medium-expert). StaCQ also outperforms the model-based baselines on most of\nthe locomotion tasks, except notably where RAMBO outperforms StaCQ in the Halfcheetah and\nrandom tasks. However on average across all locomotion tasks, StaCQ performs much better,\nwhere it is either the highest or second-highest performing method. Further, StaCQ outperforms\nRAMBO on the Antmaze tasks, which are notoriously difficult for methods that incorporate\ndynamics models [53, 42]; and compares well against the model-free baselines on these tasks.\nImportantly, StaCQ outperforms BCQ and TD3+BC which are the most comparable in theory and\nimplementation. StaCQ performs consistently well in the Antmaze tasks, despite using dynamics\nmodels. The performance improvement of IQL over StaCQ in the Antmaze tasks is likely due to\nthe advantage-weighted policy extraction method.\n12\n7\nDiscussion and conclusion\nIn this paper, we have introduced a method for state-constrained offline RL. The majority of\nprior offline RL approaches are batch-constrained, restricting the learning of the Q-function or the\npolicy to (s, a)-pairs or the (s, a)-distribution in the dataset. In contrast, our state-constrained\nmethodology confines learning updates solely to states within the dataset, offering significant\npotential reductions in dataset size for achieving optimality. By focusing on states rather than\nstate-action pairs, the state-constrained approach allows for more efficient learning updates and can\nachieve optimality with smaller datasets compared to batch-constrained methods. Central to the\nproposed state-constrained approach is the concept of state reachability, which we rigorously define\nand exemplify with a simple maze environment. The maze example underscores the enhanced\nstitching capability of state-constrained methods over batch-constrained ones, especially with\nlimited datasets that provide few trajectories. Further, we present theoretical backing affirming\nthe potential for our method’s convergence, predicated on dataset states rather than state-action\npairs. In addition, we have proposed StaCQ, an initial implementation of state-constrained deep\noffline RL. StaCQ leans on QS-learning [17] to estimate the value of transitioning from s to s′.\nThis is a particularly advantageous approach in a state-constrained context as it avoids action\ndependency. StaCQ determines state reachability through forward and inverse dynamics models,\nrefined via supervised learning on the dataset. While the performance of these models can influence\nStaCQ, the algorithm consistently demonstrates strong performance in complex environments,\noften surpassing baseline batch-constrained methods.\nWe envisage a number of potential improvements in future work. Despite the simplicity of\nthe models we used, our reachability measure resulted in state-of-the-art performance. Future\nenhancements could further improve this performance by adding more complexity to the dynamics\nmodels, reducing model error, and capturing the true environment dynamics more effectively.\nAlternatively, developing entirely new state reachability criteria independent of dynamics models\ncould significantly enhance the algorithm’s efficacy. Potential ideas include leveraging graph-based\napproaches or using reinforcement learning techniques to learn reachability directly from data.\nMoreover, the policy extraction process could be improved by incorporating elements such as a\nDecision Transformer [12] or diffusion policies [2]. These techniques have shown promising results\nin offline RL settings by leveraging the structure of the dataset and generating more diverse and\neffective policies. Integrating them into StaCQ could help the algorithm better exploit the available\ndata and improve its overall performance. Similar to batch-constrained methods, further advances\nmight arise from relaxed state constraints, albeit with tailored considerations for this setting.\nUtilising ensembles to assess the uncertainty of QA-values [3, 23, 7] and constraining actions to\nspecific regions has proven advantageous in the batch-constrained domain. Similar methodologies\ncould be extended to state-constrained approaches, such as employing ensembles of QS-functions\nto evaluate uncertainty.\nBeyond enhancing state reachability estimates, policy extraction mechanisms, and tweaking the\nstate-constraint, the prospect for advancement in state-constrained methodologies is vast. One\nintriguing avenue is to refine state reachability estimates to exclusively focus on states, excluding\naction predictions. These advancements could give rise to algorithms that exclusively rely on\nstate-only datasets. Such a paradigm is particularly relevant for real-world scenarios like video\n13\ndata where states are discernible (as images), but transitioning actions are unknown. Expanding\nthe notion of state reachability is another promising frontier. Our current definition, rooted in\none-step state reachability - ∃a such that p(s′|s, a) = 1 - can be extended to encompass multiple\nsteps. For instance, a two-step reachability could be expressed as s′′ ∈SRM(s) if ∃a1, a2 such that\np(s′|s, a1) = 1 and p(s′′|s′, a2) = 1. This principle can be further extrapolated, offering a foundation\nfor inventive model-based offline RL algorithms. Lastly, the state-constrained methodology has\nsignificant potential implications for multi-agent offline RL scenarios, where the complexity grows\nwith the size of the action space. By sidelining action dependence, our state-constrained approach\nseems well-suited for such challenges.\nTo conclude, in this research, we have ventured into the realm of state-constrained offline\nreinforcement learning, introducing a novel paradigm that emphasises the importance of states\nover state-action pairs in the dataset. While this paper lays the foundational groundwork, the\npath forward calls for additional investigation, refinement, and possible integration with other\nemerging techniques. We believe that such endeavours can push the boundaries of current offline\nreinforcement learning methodologies and offer unprecedented solutions to complex problems.\n14\nReferences\n[1] Rishabh Agarwal, Marlos C Machado, Pablo Samuel Castro, and Marc G Bellemare. Con-\ntrastive behavioral similarity embeddings for generalization in reinforcement learning. arXiv\npreprint arXiv:2101.05265, 2021.\n[2] Anurag Ajay, Yilun Du, Abhi Gupta, Joshua Tenenbaum, Tommi Jaakkola, and Pulkit\nAgrawal. Is conditional generative modeling all you need for decision-making? arXiv preprint\narXiv:2211.15657, 2022.\n[3] Gaon An, Seungyong Moon, Jang-Hyun Kim, and Hyun Oh Song. Uncertainty-based offline\nreinforcement learning with diversified q-ensemble. Advances in neural information processing\nsystems, 34:7436–7447, 2021.\n[4] Arthur Argenson and Gabriel Dulac-Arnold. Model-based offline planning. arXiv preprint\narXiv:2008.05556, 2020.\n[5] Giorgio Bacci, Giovanni Bacci, Kim G Larsen, and Radu Mardare. Computing behavioral\ndistances, compositionally. In Mathematical Foundations of Computer Science 2013: 38th In-\nternational Symposium, MFCS 2013, Klosterneuburg, Austria, August 26-30, 2013. Proceedings\n38, pages 74–85. Springer, 2013.\n[6] Giorgio Bacci, Giovanni Bacci, Kim G Larsen, and Radu Mardare. On-the-fly exact compu-\ntation of bisimilarity distances. In International conference on tools and algorithms for the\nconstruction and analysis of systems, pages 1–15. Springer, 2013.\n[7] Alex Beeson and Giovanni Montana.\nBalancing policy constraint and ensemble size in\nuncertainty-based offline reinforcement learning. Machine Learning, 113(1):443–488, 2024.\n[8] David Brandfonbrener, Will Whitney, Rajesh Ranganath, and Joan Bruna. Offline rl without\noff-policy evaluation. Advances in neural information processing systems, 34:4933–4946, 2021.\n[9] Jacob Buckman, Danijar Hafner, George Tucker, Eugene Brevdo, and Honglak Lee. Sample-\nefficient reinforcement learning with stochastic ensemble value expansion. Advances in neural\ninformation processing systems, 31, 2018.\n[10] Pablo Samuel Castro. Scalable methods for computing state similarity in deterministic markov\ndecision processes. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34,\npages 10069–10076, 2020.\n[11] Di Chen, Franck van Breugel, and James Worrell. On the complexity of computing proba-\nbilistic bisimilarity. In Foundations of Software Science and Computational Structures: 15th\nInternational Conference, FOSSACS 2012, Held as Part of the European Joint Conferences\non Theory and Practice of Software, ETAPS 2012, Tallinn, Estonia, March 24–April 1, 2012.\nProceedings 15, pages 437–451. Springer, 2012.\n[12] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter\nAbbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning\nvia sequence modeling. Advances in neural information processing systems, 34:15084–15097,\n2021.\n15\n[13] Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep reinforcement\nlearning in a handful of trials using probabilistic dynamics models.\nAdvances in neural\ninformation processing systems, 31, 2018.\n[14] Robert Dadashi, Shideh Rezaeifar, Nino Vieillard, Léonard Hussenot, Olivier Pietquin, and\nMatthieu Geist. Offline reinforcement learning with pseudometric learning. In International\nConference on Machine Learning, pages 2307–2318. PMLR, 2021.\n[15] Christopher Diehl, Timo Sievernich, Martin Krüger, Frank Hoffmann, and Torsten Bertram.\nUmbrella: Uncertainty-aware model-based offline reinforcement learning leveraging planning.\narXiv preprint arXiv:2111.11097, 2021.\n[16] Christopher Diehl, Timo Sebastian Sievernich, Martin Krüger, Frank Hoffmann, and Torsten\nBertram. Uncertainty-aware model-based offline reinforcement learning for automated driving.\nIEEE Robotics and Automation Letters, 8(2):1167–1174, 2023.\n[17] Ashley Edwards, Himanshu Sahni, Rosanne Liu, Jane Hung, Ankit Jain, Rui Wang, Adrien\nEcoffet, Thomas Miconi, Charles Isbell, and Jason Yosinski. Estimating q (s, s’) with deep\ndeterministic dynamics gradients. In International Conference on Machine Learning, pages\n2825–2835. PMLR, 2020.\n[18] Xing Fang, Qichao Zhang, Yinfeng Gao, and Dongbin Zhao. Offline reinforcement learning for\nautonomous driving with real world driving data. In 2022 IEEE 25th International Conference\non Intelligent Transportation Systems (ITSC), pages 3417–3422. IEEE, 2022.\n[19] Norman Ferns, Prakash Panangaden, and Doina Precup. Metrics for finite markov decision\nprocesses. arXiv preprint arXiv:1207.4114, 2012.\n[20] Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for\ndeep data-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020.\n[21] Scott Fujimoto and Shixiang Shane Gu. A minimalist approach to offline reinforcement\nlearning. Advances in neural information processing systems, 34:20132–20145, 2021.\n[22] Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning\nwithout exploration. In International conference on machine learning, pages 2052–2062. PMLR,\n2019.\n[23] Kamyar Ghasemipour, Shixiang Shane Gu, and Ofir Nachum. Why so pessimistic? estimating\nuncertainties for offline rl through ensembles, and why their independence matters. Advances\nin Neural Information Processing Systems, 35:18267–18281, 2022.\n[24] Antonin Guttman. R-trees: A dynamic index structure for spatial searching. In Proceedings of\nthe 1984 ACM SIGMOD international conference on Management of data, pages 47–57, 1984.\n[25] Charles A Hepburn and Giovanni Montana. Model-based trajectory stitching for improved\noffline reinforcement learning. arXiv preprint arXiv:2211.11603, 2022.\n[26] Charles A Hepburn and Giovanni Montana. Model-based trajectory stitching for improved\nbehavioural cloning and its applications. Machine Learning, 113(2):647–674, 2024.\n16\n[27] Michael Janner, Yilun Du, Joshua B Tenenbaum, and Sergey Levine. Planning with diffusion\nfor flexible behavior synthesis. arXiv preprint arXiv:2205.09991, 2022.\n[28] Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model:\nModel-based policy optimization. Advances in neural information processing systems, 32,\n2019.\n[29] Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel:\nModel-based offline reinforcement learning. Advances in neural information processing systems,\n33:21810–21823, 2020.\n[30] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv\npreprint arXiv:1412.6980, 2014.\n[31] Ilya Kostrikov, Rob Fergus, Jonathan Tompson, and Ofir Nachum. Offline reinforcement\nlearning with fisher divergence critic regularization. In International Conference on Machine\nLearning, pages 5774–5783. PMLR, 2021.\n[32] Ilya Kostrikov, Ashvin Nair, and Sergey Levine. Offline reinforcement learning with implicit\nq-learning. arXiv preprint arXiv:2110.06169, 2021.\n[33] Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing\noff-policy q-learning via bootstrapping error reduction. Advances in Neural Information\nProcessing Systems, 32, 2019.\n[34] Aviral Kumar, Anikait Singh, Stephen Tian, Chelsea Finn, and Sergey Levine. A workflow for\noffline model-free robotic reinforcement learning. arXiv preprint arXiv:2109.10813, 2021.\n[35] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for\noffline reinforcement learning. Advances in Neural Information Processing Systems, 33:1179–\n1191, 2020.\n[36] Sascha Lange, Thomas Gabel, and Martin Riedmiller. Batch reinforcement learning. In\nReinforcement learning: State-of-the-art, pages 45–73. Springer, 2012.\n[37] Charline Le Lan, Marc G Bellemare, and Pablo Samuel Castro. Metrics and continuity in\nreinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence,\nvolume 35, pages 8261–8269, 2021.\n[38] Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning:\nTutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.\n[39] Tom Mitchell. Machine Learning. McGraw Hill, 1997.\n[40] Dean A Pomerleau. Alvinn: An autonomous land vehicle in a neural network. Advances in\nneural information processing systems, 1, 1988.\n[41] Dean A Pomerleau. Efficient training of artificial neural networks for autonomous navigation.\nNeural computation, 3(1):88–97, 1991.\n17\n[42] Marc Rigter, Bruno Lacerda, and Nick Hawes. Rambo-rl: Robust adversarial model-based\noffline reinforcement learning. Advances in neural information processing systems, 35:16082–\n16097, 2022.\n[43] Tianyu Shi, Dong Chen, Kaian Chen, and Zhaojian Li. Offline reinforcement learning for\nautonomous driving with safety and exploration enhancement. arXiv preprint arXiv:2110.07067,\n2021.\n[44] Chamani Shiranthika, Kuo-Wei Chen, Chung-Yih Wang, Chan-Yun Yang, BH Sudantha, and\nWei-Fu Li. Supervised optimal chemotherapy regimen based on offline reinforcement learning.\nIEEE Journal of Biomedical and Health Informatics, 26(9):4763–4772, 2022.\n[45] Noah Y Siegel, Jost Tobias Springenberg, Felix Berkenkamp, Abbas Abdolmaleki, Michael\nNeunert, Thomas Lampe, Roland Hafner, Nicolas Heess, and Martin Riedmiller. Keep doing\nwhat worked: Behavioral modelling priors for offline reinforcement learning. arXiv preprint\narXiv:2002.08396, 2020.\n[46] Avi Singh, Albert Yu, Jonathan Yang, Jesse Zhang, Aviral Kumar, and Sergey Levine. Cog:\nConnecting new skills to past experience with offline reinforcement learning. arXiv preprint\narXiv:2010.14500, 2020.\n[47] Samarth Sinha, Ajay Mandlekar, and Animesh Garg. S4rl: Surprisingly simple self-supervision\nfor offline reinforcement learning in robotics. In Conference on Robot Learning, pages 907–917.\nPMLR, 2022.\n[48] Richard S Sutton. Dyna, an integrated architecture for learning, planning, and reacting. ACM\nSigart Bulletin, 2(4):160–163, 1991.\n[49] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press,\n2018.\n[50] Shengpu Tang, Maggie Makar, Michael Sjoding, Finale Doshi-Velez, and Jenna Wiens. Lever-\naging factored action spaces for efficient offline reinforcement learning in healthcare. Advances\nin Neural Information Processing Systems, 35:34272–34286, 2022.\n[51] Shengpu Tang and Jenna Wiens. Model selection for offline reinforcement learning: Practical\nconsiderations for healthcare settings. In Machine Learning for Healthcare Conference, pages\n2–35. PMLR, 2021.\n[52] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based\ncontrol. In 2012 IEEE/RSJ international conference on intelligent robots and systems, pages\n5026–5033. IEEE, 2012.\n[53] Jianhao Wang, Wenzhe Li, Haozhe Jiang, Guangxiang Zhu, Siyuan Li, and Chongjie Zhang.\nOffline reinforcement learning with reverse model-based imagination. Advances in Neural\nInformation Processing Systems, 34:29420–29432, 2021.\n[54] Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8:279–292, 1992.\n18\n[55] Yifan Wu, George Tucker, and Ofir Nachum. Behavior regularized offline reinforcement\nlearning. arXiv preprint arXiv:1911.11361, 2019.\n[56] Yueh-Hua Wu, Xiaolong Wang, and Masashi Hamaya. Elastic decision transformer. arXiv\npreprint arXiv:2307.02484, 2023.\n[57] Taku Yamagata, Ahmed Khalil, and Raul Santos-Rodriguez.\nQ-learning decision trans-\nformer: Leveraging dynamic programming for conditional sequence modelling in offline rl. In\nInternational Conference on Machine Learning, pages 38989–39007. PMLR, 2023.\n[58] Tianhe Yu, Aviral Kumar, Rafael Rafailov, Aravind Rajeswaran, Sergey Levine, and Chelsea\nFinn. Combo: Conservative offline model-based policy optimization. Advances in neural\ninformation processing systems, 34:28954–28967, 2021.\n[59] Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y Zou, Sergey Levine, Chelsea\nFinn, and Tengyu Ma. Mopo: Model-based offline policy optimization. Advances in Neural\nInformation Processing Systems, 33:14129–14142, 2020.\n[60] Xianyuan Zhan, Xiangyu Zhu, and Haoran Xu. Model-based offline planning with trajectory\npruning. arXiv preprint arXiv:2105.07351, 2021.\n[61] Amy Zhang, Rowan McAllister, Roberto Calandra, Yarin Gal, and Sergey Levine. Learning\ninvariant representations for reinforcement learning without reconstruction. arXiv preprint\narXiv:2006.10742, 2020.\n[62] Michael R Zhang, Tom Le Paine, Ofir Nachum, Cosmin Paduraru, George Tucker, Ziyu Wang,\nand Mohammad Norouzi. Autoregressive dynamics models for offline policy evaluation and\noptimization. arXiv preprint arXiv:2104.13877, 2021.\n[63] Wenxuan Zhou, Sujay Bajracharya, and David Held. Plas: Latent action space for offline\nreinforcement learning. In Conference on Robot Learning, pages 1719–1735. PMLR, 2021.\n19\nA\nMissing proofs\nProof of Theorem 4.2\nSince each (s, s′) pair is visited infinitely often, consider consecutive\nintervals during which each (s, s′) transition occurs at least once. We want to show the max error\nover all entries in the Q table is reduced by at least a factor of γ during each such interval.\nLet ∆n by the max error in Qn, ∆n := maxs,s′ |Qn(s, s′) −Q∗(s, s′)|. Then,\n|Qn+1(s, s′) −Q∗(s, s′)| = |(r(s, s′) + γ max\ns′′ Qn(s′, s′′) −(r(s, s′) + γ max\ns′′ Q∗(s′, s′′))|\n(11)\n= γ| max\ns′′ Qn(s′, s′′) −max\ns′′ Q∗(s′, s′′)|\n(12)\n= γ| max\ns′′ Qn(s′, s′′) −max\ns′′ Q∗(s′, s′′)|\n(13)\n≤γ max\ns′′ |Qn(s′, s′′) −Q∗(s′, s′′)|\n(14)\n≤γ max\nˆs,s′′ |Qn(ˆs, s′′) −Q∗(ˆs, s′′)|\n(15)\nWhich implies |Qn+1(s, s′) −Q∗(s, s′)| ≤γ∆n.\nWhere for brevity the maximisation maxs′′ is the shorthand for maxs′′ s.t. s′′∈SRM(s′). Also,\nEq. (11) is using the training rule; Eqs. (12) and (13) are simplifying and rearranging; (14) uses\nthe fact that | maxx f1(x) −maxx f2(x)| ≤maxx |f1(x) −f2(x)|; Eq. (15) we introduced a new\nvariable ˆs for which the maximisation is performed - this is permissible as allowing this additional\nvariable to differ will always be at least the maximum value.\nThus, the updated Qn+1(s, s′) for any s, s′ is at most γ times the maximum error in the Qn\ntable, ∆n. The largest error in the initial table, ∆0, is bounded because the values of Q0(s, s′)\nand Q∗(s, s′) are bounded ∀s, s′. Now, after the first interval during which each s, s′ is visited the\nlargest error will be at most γ∆0. After k such intervals, the error will be at most γk∆0. Since\neach state is visited infinitely often, the number of such intervals is infinite and ∆n →0 as n →∞.\n□\nProof of Theorem 4.4.\n1.Define deterministic state-constrained MDP MS. We define MS =\n(S, A, PS, R, γ) where S and A are the same as the original MDP. We have the transition probability\nwhere\npS(s′|s, a) =\n\n\n\n1\nif (s, s′ ∈D and s′ ∈SRM(s)) or (s /∈D and s′ = sterminal)\n0\notherwise.\nthis is a deterministic transition probability for s, s′ that are in the dataset and reachable. If a pair\nexists but is not in the dataset we set the rewards to be the initialised Q(s, s′) values, otherwise\nthey have the reward seen in the dataset.\n2. We have all the same assumptions under MS as we do under M, apart from infinite (s, s′)\nvisitation, so we need that and then it follows from Theorem 4.2.\nNote that sampling under the dataset D with uniform probability satisfies the infinite state-\nnext-state visitation assumptions of the MDP MS. For a reachable pair (s, s′) /∈D [this may be\ndue to s /∈D or s′ /∈D], Q(s, s′) will never be updated and will correspond to the initialised value.\nSo sampling from D is equivalent to sampling from the MDP MS and QS-learning converges to\n20\nthe optimal value under MS by following Theorem 4.2.\n□\nProof of Theorem 4.5.\nThis follows from Theorem 4.2, noting the state-constraint is non-\nrestrictive with a dataset which contains all possible states.\n□\nProof of Theorem 4.6.\nThis result follows from the fact that if every s is visited infinitely then\ndue to the definition of state reachability we can evaluate over all (s, s′)-pairs in the dataset. Then\nfollowing Theorem 4.4, which states QS-learning learns the optimal value for the MDP MS for\ns, s′ ∈D. However, the deterministic MS corresponds to the original M in all seen state and\nreachable next-state pairs. Noting that state-constrained policies operate only on s, s′ ∈D, where\nMS corresponds to the true MDP, it follows that π∗will be the optimal state-constrained policy\nfrom the optimality of QS-learning.\n□\nTheorem A.1. In a deterministic setting, QA-values are equivalent to QS-values\nProof. See Theorem 2.2.1 in [17]\nProof of Theorem 4.7.\nCase 1: In this situation, for all states in the dataset, we do not have\nany extra reachable states, other than the pairs i.e. ∀s ∈D, SRM(s) = {s′}, where (s, s′) ∈D. In\nthis case the state-constrained MDP, Definition 4.3, is equivalent to the batch-constrained MDP in\n[22]. The condition of the state-constrained MDP,\ns, s′ ∈D and s′ ∈SRM(s)\nbecomes (s, s′) ∈D as s′ ∈SRM(s) only exists where (s, s′) ∈D. From this, the probability\ntransition function for the state-constrained and batch-constrained MDPs are equivalent and thus\nso are the MDPs themselves. Finally, for this case we just need to show that the SCQL and BCQL\nupdates are the same. So under this case condition the SCQL update becomes\nQ(s, s′) ←(1 −α)Q(s, s′) + α\nh\nr(s, s′) + γ\nmax\ns′′ s.t. (s′,s′′)∈D Q(s′, s′′)\ni\n.\nFrom Theorem A.1, for a transition (s, a, s′) ∈D we have Q(s, a) = Q(s, s′) and thus for the\ntransition (s′, a′, s′′) ∈D we have Q(s′, a′) = Q(s′, s′′), therefore the SCQL update is equivalent to\nQ(s, a) ←(1 −α)Q(s, a) + α\nh\nr(s, a) + γ\nmax\na′ s.t. (s′,a′)∈D Q(s′, a′)\ni\n.\nTherefore, for case 1 BCQL and SCQL have the same Q-value update and therefore have the\nsame optimal policy.\nCase 2: In this case, for a single state in the dataset, we have one reachable next state that\nis unseen as a pair in the dataset, i.e. ∃˜s, ˆs′ ∈D s.t. ˆs′ ∈SRM(˜s) and (˜s, ˆs′) /∈D. In this case the\nstate-constrained MDP probability transition function becomes: for a = I(s, s′) ∈A\npS(s′|s, a) =\n\n\n\n1\nif ((s, s′) ∈D) or (s /∈D and s′ = sterminal) or (s = ˜s and s′ = ˆs′)\n0\notherwise.\n21\nThis is the transition function for the batch-constrained MDP but allowing for an extra transition\nfrom ˜s to ˆs′. Now comparing BCQL and SCQL Q-value updates, we have all equivalent values\nfor the transition (s, a, s′) except for the trajectory that contains the state ˜s. For the converged\nQS-value, Q∗, let ˜s−1 be the state in the trajectory previous to ˜s and ˜s′ be the next state after ˜s\nin the trajectory,\nQ∗(˜s−1, ˜s) ←(1 −α)Q∗(˜s−1, ˜s) + α\nh\nr(˜s−1, ˜s) + γ max{Q∗(˜s, ˜s′), Q∗(˜s, ˆs′)}\ni\nIf Q∗(˜s, ˜s′) ≥Q∗(˜s, ˆs′) then again BCQL is equivalent to SCQL. However, if Q∗(˜s, ˜s′) < Q∗(˜s, ˆs′)\nthen SCQL will have higher values than BCQL for all states previous to ˜s and therefore will\nproduce a higher quality policy by the policy improvement theorem.\nThen without loss of generality Case 2 can be extended to all cases where we have multiple\nreachable next states for multiple dataset states, that have higher value according to the expert\nQS-value.\n□\nB\nReducing the complexity of state reachability estimation\nDirectly computing d\nSRM for every state s would entail comparing each state in the dataset against\nall others, an approach that is computationally prohibitive for larger datasets. To address this\nchallenge, we calculate the range of potentially reachable states for each state dimension. This\ncalculation is based on a set of random actions, denoted as {ai\nrand}n\ni=1.\nFor a given state s, we first determine its range by calculating the minimum and maximum\nvalues using fω1. Specifically, the minimum range Rmin(s) is computed as mini fω1(s, ai\nrand), and\nthe maximum range Rmax(s) is maxi fω1(s, ai\nrand). Subsequently, we construct a smaller set of\nstates within the range (Rmin(s), Rmax(s)), using an R-tree [24], a data structure that efficiently\nidentifies states within a specified hyper-rectangle (our range). This approach, leveraging the\nR-tree’s efficient search capability, dramatically reduces the size of the dataset. Consequently,\nthe models are applied only to this refined set of states, leading to a significant reduction in\ncomputational complexity.\nC\nImplementation details\nFor our experiments we use ϵ = 0.1 for the state reachability criteria where the LHS is also scaled\nby the maximum of the difference in state range, i.e our state reachability estimate for state s is\ndetermined by\n\f\f\f\f\n\f\f\f\f\nfω1(s, Iω2(s, s′)) −s′\nRmax(s) −Rmin(s)\n\f\f\f\f\n\f\f\f\f\n∞\n< ϵ.\nThis means that the maximum state dimension model prediction error must be within 10% of\nthe true state range. Due to this we also normalise all states which gives more accurate model\nprediction and is done in the same way as TD3+BC [21], i.e let si be the ith feature of state s\nsi = si −µi\nσi + ϵs\n,\n22\nwhere µi and σi are the mean and standard deviation of the ith feature across all states and\nϵs = 10−3 is a small normalisation constant. Similar to TD3+BC, we do not normalise states for\nthe Antmaze tasks as this is harmful for the results.\nFor the MuJoCo locomotion tasks we evaluate our method over 5 seeds each with 10 evaluation\ntrajectories; whereas for the Antmaze tasks we also evaluate over 5 seeds but with 100 evaluation\ntrajectories. Just like TD3+BC [21], we scale our hyperparameter by the average Q-value across\nthe minibatch\nλ =\nα\n1\nN\nP\ni Q(si, s′\ni),\nwhere N is the size of the minibatch (in our case N = 256) and s′\ni is the next state where the\npolicy action leads from s. We aim to find a consistent α hyperparameter across the different\nenvironments. We find that the optimal consistent hyperparameters are α = {1, 5, 10} for Hopper,\nWalker2d and Halfcheetah respectively. However for Halfcheetah -medium expert we use α = 0.5\ndue to the significant improvement. For the Antmaze tasks, each maze size is a new environment\nand we use α = {2, 10, 19} for the umaze, medium and large environments respectively. Across\nall environments, we add a small amount of zero mean Gaussian noise to the policy action before\nbeing input into the forward model, we use a fixed variance of 0.1. It should be noted that making\nthis policy noise vary across the different environments could improve results greatly, however we\naim to keep this fixed so that we have a general method that can be easily optimised on other\nenvironments.\nThe actor, critic and reward model are represented as neural networks with two hidden layers of\nsize 256 and ReLU activation. They are trained using the ADAM optimiser [30] and have learning\nrates 3e−4, the actor also has a cosine scheduler. We use an ensemble of 4 critic networks and take\nthe minimum value across the networks. Also we use soft parameter updates for the target critic\nnetwork with parameter τ = 0.005, and we use a discount factor of γ = 0.99. For the locomotion\ntasks we use a shared target value to update the critic towards, whereas for the Antmaze tasks we\nuse independent target values for each critic value. Both the inverse and forward dynamics models\nare represented as neural networks with three hidden layers of size 256 and ReLU activation. They\nare trained using the ADAM optimiser with a learning rate of 4e −3 and a batch size of 256. We\nuse an ensemble of 7 forward models and 3 inverse models and then take a final prediction as an\naverage across the ensemble. Also our forward model predicts the state difference rather than the\nnext state directly, which improves model prediction performance.\nTo attain the results for BCQ, we re-implemented the algorithm from the paper [22] and\ntrained on the version 2 datasets. Following the same practises as StaCQ we evaluate BCQ over\n5 seeds each with 10 evaluations on the MuJoCo locomotion tasks and 5 seeds each with 100\nevaluations on the Antmaze tasks. All other results in Table 1 were obtained from the original\nauthors’ papers. Our experiments were performed with a single GeForce GTX 3090 GPU and an\nIntel Core i9-11900K CPU at 3.50GHz. Each run takes on average 2.5 hours, where models are\npre-trained and state reachability is given (the same models and reachability lists are given to each\nrun). We provide 5 runs for each dataset (18) which gives a total run time of 225 hours.\n23\nD\nOne-step method\nStaCQ, Algorithm 1, that we have introduced in this paper is an actor-critic method that learns the\nQS-value and policy together. Alternatively, the QS-value can be learned directly from the data, in\nan on-policy fashion, then a policy can be extracted directly from this on-policy QS-value. These\napproaches are known as one-step methods [8]. In this section we adapt StaCQ into a one-step\nmethod.\nD.1\nEstimating QS-values\nSo that the QS-values are learned on-policy while still taking advantage of the state-constrained\nframework we introduce a small modification to Eq. (4). Since the pair (s′, s′′) is unseen in the\ndataset, we use the following approximation:\nmax\ns′′s.t.(s′′,s′′′)∈D\ns′′∈SRM(s′)\n{r(s′, s′′) + γQ(s′′, s′′′)}\n(16)\nto replace max s′′s.t.s′′∈D\ns′′∈SRM(s′)\nQ(s′, s′′). This adjustment ensures that the QS-value is only evaluated\non explicit state-next-state pairs, thereby avoiding OOD (s, s′)-pairs. Although this approach\ndiverges slightly from the theoretical method, where QS-value updates are performed on every pair,\nit provides a practical solution.\nUsing Eq. (16), the approximation of state reachability and the reward model Eq. (9), the\non-policy state-constrained QS-values can be refined by reducing the MSE between the target and\nthe actual QS-values. The target is determined by identifying the maximum value across reachable\nstates:\nLθ = E(s,s′)∼D\n\" \nr(s, s′) + γ\nmax\ns′′s.t.(s′′,s′′′)∈D,\ns′′∈SRM(s′)\n{rω3(s′, s′′) + γQθ′(s′′, s′′′)} −Qθ(s, s′)\n!2#\n.\n(17)\nHere, θ′ represents the parameters for a target Q-value which are incrementally updated towards θ:\nθ′ ←τθ + (1 −τ)θ′, with τ being the soft update coefficient.\nD.2\nPolicy Extraction Step\nEq. (17) gives a one-step optimal state-constrained QS-value. However, this equation alone does\nnot produce an optimal action. To determine the optimal action, we need to add a policy extraction\nstep. We use the same policy extraction method as StaCQ, a state behaviour cloning regularised\npolicy update similar to TD3+BC [21]:\nLϕ = Es∼D\nh\nλQθ(s, fω1(s, πϕ(s))) + (fω1(s, πϕ(s))) −ˆs′)2i\n,\n(18)\nwhere\nˆs′ = arg\nmax\ns′s.t.(s′,s′′)∈D\ns′∈d\nSRM(s)\n{rω3(s, s′) + γQθ(s′, s′′)},\n24\nAlgorithm 2 StaCQ (One Step RL version)\n1: Input: Dataset D, T number of iterations, τ\n2: Initialise: parameters ω1, ω2, ω3, θ, θ′, ϕ\n3: Pre-train fω1 and Iω2 using Eqs.(6) and (7)\n4: Pre-train reachability criteria d\nSRM\n5: for t = 1, . . . , T do\n6:\nOptimise reward function according to Eq. (9)\n7:\nOptimise QS-value according to Eq. (17)\n8:\nUpdate target networks: θ′ ←τθ + (1 −τ)θ′.\n9: end for\n10: for t = 1, . . . , T do\n11:\nOptimise policy: Eq. (18)\n12: end for\nAgain, due to the forward model being fixed, a small amount of Gaussian noise is added to the\npolicy action before being input into the model. This creates a more robust policy by ensuring the\npolicy does not exploit the forward model. The complete procedure for the one step version of\nStaCQ is provided in Algorithm 2.\nD.3\nOne step method implementation details\nIn the original OneStepRL paper [8], they evaluate their method over 10 seeds where the λ\nhyperparameter has been tuned over the first 3 seeds and then evaluated with the λ fixed for the\nremaining 7. However as we want a consistent hyperparameter for each environment we choose\nα = {0.1, 1.0, 5.0} for Hopper, Walker2d and Halfcheetah respectively and for the Antmaze tasks\nwe choose α = {10, 40, 100} for the umaze, medium and large environments respectively. The one\nstep method also uses a single critic, we found that increasing the number of critics deteriorated\nresults. All other implementation details are the same as StaCQ, Appendix C.\n25\n",
  "categories": [
    "stat.ML",
    "cs.AI",
    "cs.LG"
  ],
  "published": "2024-05-23",
  "updated": "2024-05-23"
}