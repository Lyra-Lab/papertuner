{
  "id": "http://arxiv.org/abs/2405.15430v1",
  "title": "Counterexample-Guided Repair of Reinforcement Learning Systems Using Safety Critics",
  "authors": [
    "David Boetius",
    "Stefan Leue"
  ],
  "abstract": "Naively trained Deep Reinforcement Learning agents may fail to satisfy vital\nsafety constraints. To avoid costly retraining, we may desire to repair a\npreviously trained reinforcement learning agent to obviate unsafe behaviour. We\ndevise a counterexample-guided repair algorithm for repairing reinforcement\nlearning systems leveraging safety critics. The algorithm jointly repairs a\nreinforcement learning agent and a safety critic using gradient-based\nconstrained optimisation.",
  "text": "arXiv:2405.15430v1  [cs.LG]  24 May 2024\nCounterexample-Guided Repair of Reinforcement\nLearning Systems Using Safety Critics\nDavid Boetius[0000−0002−9071−1695] and Stefan Leue[0000−0002−4259−624X]\nUniversity of Konstanz, 78457 Konstanz, Germany\n{david.boetius,stefan.leue}@uni-konstanz.de\nAbstract. Naïvely trained Deep Reinforcement Learning agents may\nfail to satisfy vital safety constraints. To avoid costly retraining, we may\ndesire to repair a previously trained reinforcement learning agent to ob-\nviate unsafe behaviour. We devise a counterexample-guided repair al-\ngorithm for repairing reinforcement learning systems leveraging safety\ncritics. The algorithm jointly repairs a reinforcement learning agent and\na safety critic using gradient-based constrained optimisation.\nKeywords: Reinforcement Learning · Safety · Repair.\n1\nIntroduction\nDeep Reinforcement Learning is at the core of several recent breakthroughs in\nAI [13,23,26]. With the increasing abilities of reinforcement learning agents, it\nbecomes vital to eﬀectively constrain such agents to avoid harm, particularly\nin safety-critical applications. A particularly eﬀective class of constraints are\nformal guarantees on the non-occurrence of undesired behaviour (safety). Such\nguarantees are obtainable through formal veriﬁcation [22].\nCounterexample-guided repair is a successful iterative reﬁnement algorithm\nfor obtaining formally veriﬁed deep neural networks in supervised learning [6].\nThe repair algorithm alternates searching counterexamples and modifying the\nmodel under repair to remove counterexamples. A central component of the\nalgorithm is a function that quantiﬁes the safety of a neural network output. In\nreinforcement learning, the safety of an output depends on the interaction with\nthe environment. Therefore, quantifying the safety of an output is expensive,\nmaking it challenging to apply counterexample-guided repair to reinforcement\nlearning. To overcome this challenge, we propose to learn a safety critic [5,18,31]\nto quantify safety. Since safety critics are themselves imperfect machine learning\nmodels, we propose to repair the safety critic alongside the actual reinforcement\nlearning agent.\nIn the spirit of actor-critic reinforcement learning algorithms [33], a safety\ncritic learns to predict the safety of a state from gathered experience [5]. The\nidea of safety critics is analogous to the widely used [24,29] concept of value\ncritics that learn to predict the value of a state from experience. We can use\nrecorded unsafe trajectories, coupled with safe trajectories, as a starting point\n2\nD. Boetius, S. Leue\nfor learning a safety critic. Since our aim is to repair a reinforcement learning\nagent, we can assume that the agent was already found to behave unsafely and,\ntherefore, unsafe trajectories are available.\nWhen using a safety critic in counterexample-guided repair, it is vital that\nthe safety critic correctly recognises new counterexamples as unsafe. Otherwise,\ncounterexample-guided repair can not proceed to repair the reinforcement learn-\ning agent. To achieve this, we propose to iteratively repair the safety critic along-\nside the reinforcement learning agent, such that the safety critic correctly recog-\nnises newly found counterexamples.\nOur approach allows for great generality regarding the environments in which\nan agent operates. Similarly, our approach does not restrict the class of safety\nspeciﬁcations that can be repaired. It solely requires the ability to falsify or verify\nthe speciﬁcation given an agent and an environment. Even in the absence of a\nfalsiﬁer or veriﬁer, our approach can be used to repair a reinforcement learning\nagent whenever unsafe behaviour is detected.\nThe following section reviews the literature relevant to this paper. Follow-\ning this, we formally introduce safe reinforcement learning, safety critics, and\ncounterexample-guided repair. The subsequent section describes our approach\nof counterexample-guided repair using safety critics in more detail and rigour.\nWe conclude with an outlook on future work. This includes an experimental\nevaluation.\n2\nRelated Work\nReinforcement learning is the process by which an agent learns to solve a task by\nrepeatedly interacting with an environment. The agent leans by maximising the\nreturn it receives during the interaction. State-of-the-art algorithms for reinforce-\nment learning include Advantage Actor-Critic (A2C) [24], Asynchronous Ad-\nvantage Actor-Critic (A3C) [24], and Proximal Policy Optimisation (PPO) [29].\nThese algorithms are based on deep neural networks as reinforcement learn-\ning agents. Due to the prevalence of deep neural networks in state-of-the-art\nreinforcement learning methods, this paper is primarily concerned with deep\nreinforcement learning. However, our approach is not limited to this class of\nmodels.\nIn safe reinforcement learning, the agent must also respect additional safety\nconstraints. An overview of safe reinforcement learning is presented in [15].\nMore recent developments include shielding [1], safe reinforcement learning us-\ning abstract interpretation [20,32], and safe reinforcement learning via safety\ncritics [5,18,31,39]. In contrast to safe reinforcement learning, the repair of re-\ninforcement learning agents is concerned with making an existing reinforcement\nagent safe. In this paper, we apply safety critics to repairing reinforcement learn-\ning agents.\nVeriﬁcation of reinforcement learning systems is concerned with proving that\na reinforcement learning agent behaves safely. Recent approaches for reinforce-\nCounterexample-Guided Repair using Safety Critics\n3\nment learning veriﬁcation build upon reachability analysis [3,19,37] and model\nchecking [2,3,12]. A survey of the ﬁeld is provided by [22].\nIn the domain of supervised learning, machine learning models are veriﬁed\nusing Satisﬁability Modulo Theories (SMT) solving [11,17,21], Mixed Integer\nLinear Programming (MILP) [36], and Branch and Bound (BaB) [7,14,38,40].\nMany repair algorithms for supervised machine learning models are based on\ncounterexample-guided repair [6]. The approaches for removing counterexam-\nples range from augmenting the training set [28,34] and constrained optimisa-\ntion [4,16] to specialised neural network architectures [8,16]. Non-iterative repair\napproaches for neural networks include [30,35].\n3\nPreliminaries\nThis section introduces Safe Reinforcement Learning, Safety Critics, Veriﬁca-\ntion, Falsiﬁcation, and Repair. While our algorithm’s envisioned target is deep\nreinforcement learning agents (neural networks), our approach is not speciﬁc to\na particular model class.\n3.1\nSafe Reinforcement Learning\nFollowing [5], we adopt a perspective on safe reinforcement learning where un-\nsafe behaviour may occur during training but not when an agent is deployed.\nOur assumption is that we can train the agent to behave safely in a simulation\nwhere unsafe behaviour is inconsequential. A safe reinforcement learning task\nis formalised as a Constrained Markov Decision Process (CMDP). We consider\nCMDPs with deterministic transitions.\nDeﬁnition 1 (CMDP). A Constrained Markov Decision Process (CMDP)\nwith deterministic transitions is a tuple (S, A, P, R, S0, C), where S is the state\nspace, A is the set of actions, P : S × A →S is the transition function, R :\nS × A →R is the reward, C ⊂S is a set of safe states, and S0 ⊂C is a set of\ninitial states.\nFor a ﬁnite time-horizon T ∈N, τ = s0, a0, s1, a1, . . . , sT is a trajectory of\na CMDP if s0 ∈S0 and st = P(st−1, at−1), ∀t ∈{1, . . . , T }.\nDeﬁnition 2 (Return). Given a discount factor γ ∈[0, 1], the return G of a\ntrajectory τ = s0, a0, s1, a1, . . . , sT is\nG(τ) =\nT −1\nX\nt=0\nγtR(st, at).\nDeﬁnition 3 (Safe Trajectories). For a set of safe states C, a trajectory τ =\ns0, a0, s1, a1, . . . , sT is safe if s0, s1, . . . , sT ∈C. We write τ ⊨C if τ is safe.\n4\nD. Boetius, S. Leue\nAssuming a uniform distribution U(S0) over the initial states, our goal is to learn\na (deterministic) parametric policy πθ : S →A that maximises the expected\nreturn while maintaining safety\nmaximise\nθ\nEs0∼U(S0)[G(τ(s0, πθ))]\nsubject to\nτ(s0, πθ) ⊨C\n∀s0 ∈S0,\n(1)\nwhere τ(s0, πθ) = s0, a0, s1, a1, . . . , sT is a trajectory with at = πθ(st), ∀t ∈\n{0, . . . , T −1}.\nA parametric policy may be given, for example, by a neural network netθ :\nRn →A reading a numeric representation of a state xs ∈Rn, n ∈N, s ∈S\nand returning an action a ∈A. In this paper, we use the terms policy and agent\ninterchangeably.\n3.2\nSafety Critics\nSafety critics learn the safety value function V πθ\nC\n: S →R\nV πθ\nC (s) =\nmin\nt∈{0,...,T } c(st),\n(2)\nwhere s0 = s, st = P(st−1, πθ(st−1)), ∀t ∈{1, . . .T }, and c : S →R is a\nsatisfaction function [4] or robustness measure [9] for the safe set C.\nDeﬁnition 4 (Satisfaction Function). A function c : S →R is a satisfaction\nfunction of a set C ⊆S if ∀s ∈S : c(s) ≥0 ⇔s ∈C.\nThe concept of a safety critic is analogous to (value) critics in actor-critic rein-\nforcement learning [33]. Classical (value) critics learn the value of a state V πθ.\nThe value is the expected return when starting in a state. Safety critics can be\nlearned using the methods from [5,31,39].\n3.3\nVeriﬁcation, Falsiﬁcation, and Repair\nGiven a CMDP M = (S, A, P, R, S0, C) and a policy πθ, we are interested in the\nquestion whether the policy guarantees safety for all initial states. A counterex-\nample is an initial state for which following the policy leads to unsafe states.\nDeﬁnition 5 (Counterexample). Given a policy πθ, a counterexample is an\ninitial state s0 ∈S0, such that the trajectory τ = s0, a0, s1, a1, . . . , sT , T ∈N\nwith at = πθ(st−1), ∀t ∈{1, . . . , T −1} contains an unsafe state st /∈C for\nsome t ∈{1, . . ., T }.\nSince counterexamples lead to unsafe states, the safety value function V πθ\nC\nof a\ncounterexample is negative.\nWhen considering algorithms for searching counterexamples, we diﬀerentiate\nbetween falsiﬁers and veriﬁers. While falsiﬁers are sound counterexample-search\nalgorithms, veriﬁers are sound and complete.\nCounterexample-Guided Repair using Safety Critics\n5\nDeﬁnition 6 (Soundness and Completeness). A counterexample-search al-\ngorithm is sound if it only produces genuine counterexamples. Additionally, an\nalgorithm is complete if it terminates and produces a counterexample for every\nunsafe policy.\nProposition 1. A policy πθ is safe whenever a veriﬁer does not produce a coun-\nterexample for πθ.\nProof. Proposition 1 follows from contraposition on the completeness of veriﬁers.\nGiven an unsafe policy πθ, the task of repair is to modify the policy to be\nsafe while maintaining high returns. A successful repair algorithm for supervised\nlearning is counterexample-guided repair [4,6,34]. The following section intro-\nduces a counterexample-guided repair algorithm for reinforcement learning.\n4\nCounterexample-Guided Repair using Safety Critics\nExisting counterexample-guided repair algorithms repair supervised learning\nmodels by alternating counterexample search and counterexample removal. Al-\ngorithm 1 describes the algorithmic skeleton of counterexample-guided repair.\nThis skeleton is akin to all counterexample-guided repair algorithms.\nAlgorithm 1: Counterexample-Guided Repair\nInput: CMDP M = (S, A, P, R, S0, C), Policy πθ\n1 Sc ←ﬁnd counterexamples(M, πθ)\n2 do\n3\nθ ←remove counterexamples(Sc, πθ, M)\n4\nSc ←Sc ∪ﬁnd counterexamples(M, πθ)\n5 while ∃s0 ∈Sc : s0 is counterexample\nWhen using a veriﬁer to ﬁnd counterexamples, Algorithm 1 is guaranteed\nto produce a safe policy if it terminates [6]. However, Algorithm 1 is not gen-\nerally guaranteed to terminate [6]. Despite this, counterexample-guided repair\nhas proven successful in repairing deep neural networks [4] and other machine\nlearning models [34].\nAlgorithm 1 has two sub-procedures we need to instantiate for obtaining an\nexecutable algorithm: ﬁnding counterexamples and removing counterexamples.\nFor ﬁnding counterexamples, we can use tools for verifying reinforcement learn-\ning systems [2,3,12,19,37] (see [22] for a survey). In the remainder of this paper,\nwe address removing counterexamples using safety critics.\n6\nD. Boetius, S. Leue\n4.1\nRemoving Counterexamples\nSimilarly to the supervised setting [6], removing counterexamples corresponds\nto solving a constrained optimisation problem\nmaximise\nθ\nEs0∼U(S0)[G(τ(s0, πθ))]\nsubject to\nτ(s0, πθ) ⊨C\n∀s0 ∈Sc,\n(3)\nwhere τ(s0, πθ) is as in Equation (1) and Sc is a ﬁnite set of counterexamples.\nIn the supervised setting, we can remove counterexamples by directly solving\nthe analogue of Equation (3) using gradient-based optimisation algorithms [4].\nHowever, for repairing reinforcement learning policies, checking whether a set of\nparameters θ is feasible for Equation (3) is expensive, as it requires simulating\nthe CMDP. Additionally, the term τ(s0, πθ) ⊨C suﬀers from exploding gradi-\nents [27] due to the repeated application of πθ for obtaining the trajectory. These\nproperties of Equation (3) hinder the application of gradient-based optimisation\nalgorithms for removing counterexamples by solving Equation (3) directly.\nTo obtain an algorithm for removing counterexamples, we ﬁrst note that\nEquation (3) can equivalently be reformulated using the safety value function V πθ\nC\nfrom Equation (2). Concretely, we can replace the constraint τ(s0, πθ) ⊨C\nby V πθ\nC (s0) > 0. Now, when approximating V πθ\nC\nusing a safety critic eV πθ\nC , we\nobtain\nmaximise\nθ\nEs0∼U(S0)[G(τ(s0, πθ))]\nsubject to\neV πθ\nC (s0) ≥0\n∀s0 ∈Sc,\n(4)\nWhile Equation (4) is not equivalent to Equation (3) due to using an approxi-\nmation of V πθ\nC , Equation (4) can be solved using techniques such as stochastic\ngradient descent/ascent [10] or the ℓ1 penalty function method [4,25]. To ensure\nthat solving Equation (4) actually removes counterexamples, we repair the safety\ncritic eV πθ\nC\nalongside the policy.\n4.2\nRepairing Safety Critics\nTo allow us to remove counterexamples by solving Equation (4), the safety\ncritic eV πθ\nC\nneeds to correctly recognise the counterexamples in Sc as coun-\nterexamples. By recognising a counterexample s0 as a counterexample, we mean\nthat eV πθ\nC (s0) < 0. We can ensure that the safety critic recognises all counterex-\namples in Sc by solving\nminimise\nθ\nJ(eV πθ\nC )\nsubject to\neV πθ\nC (s0) < 0\n∀s0 ∈Sc with V πθ\nC (s0) < 0,\n(5)\nwhere J(eV πθ\nC ) is a loss function for training the safety critic [5,31,39]. Solving\nEquation (5) can itself be understood as removing counterexamples of the safety\ncritic. As Equation (4), we can solve Equation (5) using stochastic gradient\ndescent/ascent [10] or the ℓ1 penalty function method [4,25].\nCounterexample-Guided Repair using Safety Critics\n7\nAlgorithm 2: Counterexample Removal\nInput: CMDP M = (S, A, P, R, S0, C), Unsafe policy πθ, Safety Critic eV πθ\nC ,\nCounterexamples Sc\n1 while ∃s0 ∈Sc : s0 is counterexample do\n2\nupdate eV πθ\nC\nby solving Equation (5)\n3\nupdate πθ by solving Equation (4) using eV πθ\nC\n4.3\nCounterexample Removal Algorithm\nWe propose jointly modifying the safety critic and the unsafe reinforcement\nlearning agent πθ. Algorithm 2 summarises our approach. We ﬁrst update the\nsafety critic to recognise the available counterexamples. This corresponds to\nsolving Equation (5). Using the updated safety critic, we update πθ to remove\nthe counterexamples by solving Equation (4).\nSince the safety critic may fail to recognise a counterexample as a coun-\nterexample for the updated policy, we iterate the previous two steps until all\ncounterexamples are removed.\nIn principle, this procedure may fail to terminate if the safety-critic “forgets”\nto recognise the counterexamples of the initial policy when being updated in the\nsecond iteration of Algorithm 2. Since the policy is updated in the ﬁrst iteration\nof Algorithm 2, updating eV πθ\nC\nin the second iteration does not consider coun-\nterexamples for the initial policy. Therefore, the policy may revert to its initial\nparameters in the second iteration to evade the updated safety critic. This leads\nto an inﬁnite loop. However, this issue can be circumvented by including pre-\nvious unsafe trajectories in Equation (5), similarly to how counterexamples are\nretained in Algorithm 1 for later iterations to counter reintroducing counterex-\namples.\n5\nConclusion\nWe introduce a counterexample-guided repair algorithm for reinforcement learn-\ning systems. We leverage safety critics to circumvent costly simulations during\ncounterexample removal. Our approach applies to a wide range of speciﬁcations\nand can work with any veriﬁer and falsiﬁer. The central idea of our approach is\nto repair the policy and the safety critic jointly.\nFuture work includes evaluating our algorithm experimentally and comparing\nit with abstract-interpretation-based safe reinforcement learning [20,32]. Since\ncounterexample-guided repair avoids the abstraction error of abstract interpreta-\ntion, we expect that counterexample-guided repair can produce less conservative,\nsafe reinforcement learning agents. Additionally, our ideas are not inherently lim-\nited to reinforcement learning but can be applied whenever satisfaction functions\nare unavailable or costly to compute. Exploring such applications is another di-\nrection for future research.\n8\nD. Boetius, S. Leue\nDisclosure of Interests. The authors have no competing interests to declare that\nare relevant to the content of this article.\nReferences\n1. Alshiekh,\nM.,\nBloem,\nR.,\nEhlers,\nR.,\nKönighofer,\nB.,\nNiekum,\nS.,\nTopcu, U.: Safe\nreinforcement\nlearning via shielding.\nIn: McIlraith,\nS.A.,\nWeinberger,\nK.Q.\n(eds.)\nAAAI.\npp.\n2669–2678.\nAAAI\nPress\n(2018).\nhttps://doi.org/10.1609/AAAI.V32I1.11797\n2. Amir,\nG.,\nSchapira,\nM.,\nKatz,\nG.:\nTowards\nscalable\nveriﬁcation\nof\ndeep\nreinforcement\nlearning.\nIn:\nFMCAD.\npp.\n193–203.\nIEEE\n(2021).\nhttps://doi.org/10.34727/2021/ISBN.978-3-85448-046-4_28\n3. Bacci,\nE.,\nGiacobbe,\nM.,\nParker, D.: Verifying reinforcement\nlearning\nup\nto\ninﬁnity.\nIn:\nZhou,\nZ.\n(ed.)\nIJCAI.\npp.\n2154–2160.\nijcai.org\n(2021).\nhttps://doi.org/10.24963/IJCAI.2021/297\n4. Bauer-Marquart, F., Boetius, D., Leue, S., Schilling, C.: SpecRepair: Counter-\nExample Guided Safety Repair of Deep Neural Networks. In: Legunsen, O., Rosu,\nG. (eds.) SPIN. Lecture Notes in Computer Science, vol. 13255, pp. 79–96. Springer\n(2022). https://doi.org/10.1007/978-3-031-15077-7_5\n5. Bharadhwaj, H., Kumar, A., Rhinehart, N., Levine, S., Shkurti, F., Garg, A.:\nConservative safety critics for exploration. In: ICLR. OpenReview.net (2021),\nhttps://openreview.net/forum?id=iaO86DUuKi\n6. Boetius,\nD.,\nLeue,\nS.,\nSutter,\nT.:\nA\nrobust\noptimisation\nperspective\non\ncounterexample-guided repair of neural networks. In: Krause, A., Brunskill,\nE., Cho, K., Engelhardt, B., Sabato, S., Scarlett, J. (eds.) ICML. Proceed-\nings of Machine Learning Research, vol. 202, pp. 2712–2737. PMLR (2023),\nhttps://proceedings.mlr.press/v202/boetius23a.html\n7. Bunel, R., Lu, J., Turkaslan, I., Torr, P.H.S., Kohli, P., Kumar, M.P.: Branch and\nBound for Piecewise Linear Neural Network Veriﬁcation. J. Mach. Learn. Res. 21,\n42:1–42:39 (2020), http://jmlr.org/papers/v21/19-468.html\n8. Dong,\nG.,\nSun,\nJ.,\nWang,\nJ.,\nWang,\nX.,\nDai,\nT.:\nTowards\nRepair-\ning\nNeural\nNetworks\nCorrectly.\nIn:\nQRS.\npp.\n714–725.\nIEEE\n(2021).\nhttps://doi.org/10.1109/QRS54544.2021.00081\n9. Donzé,\nA.,\nMaler,\nO.:\nRobust\nsatisfaction\nof\ntemporal\nlogic\nover\nreal-\nvalued signals. In: Chatterjee, K., Henzinger, T.A. (eds.) FORMATS. Lec-\nture Notes in Computer Science, vol. 6246, pp. 92–106. Springer (2010).\nhttps://doi.org/10.1007/978-3-642-15297-9_9\n10. Eban, E., Schain, M., Mackey, A., Gordon, A., Rifkin, R., Elidan, G.: Scalable\nLearning of Non-Decomposable Objectives. In: Singh, A., Zhu, X.J. (eds.) AIS-\nTATS. Proceedings of Machine Learning Research, vol. 54, pp. 832–840. PMLR\n(2017), http://proceedings.mlr.press/v54/eban17a.html\n11. Ehlers,\nR.:\nFormal\nVeriﬁcation\nof\nPiece-Wise\nLinear\nFeed-Forward\nNeu-\nral\nNetworks.\nIn:\nD’Souza,\nD.,\nKumar,\nK.N.\n(eds.)\nATVA.\nLecture\nNotes\nin\nComputer\nScience,\nvol.\n10482,\npp.\n269–286.\nSpringer\n(2017).\nhttps://doi.org/10.1007/978-3-319-68167-2_19\n12. Eliyahu, T., Kazak, Y., Katz, G., Schapira, M.: Verifying learning-augmented sys-\ntems. In: Kuipers, F.A., Caesar, M.C. (eds.) SIGCOMM. pp. 305–318. ACM (2021).\nhttps://doi.org/10.1145/3452296.3472936\nCounterexample-Guided Repair using Safety Critics\n9\n13. Fawzi, A., Balog, M., Huang, A., Hubert, T., Romera-Paredes, B., Barekatain,\nM.,\nNovikov,\nA.,\nR.\nRuiz,\nF.J.,\nSchrittwieser,\nJ.,\nSwirszcz,\nG.,\nSil-\nver,\nD.,\nHassabis,\nD.,\nKohli,\nP.:\nDiscovering\nfaster\nmatrix\nmultiplica-\ntion algorithms with reinforcement learning. Nat. 610(7930), 47–53 (2022).\nhttps://doi.org/10.1038/s41586-022-05172-4\n14. Ferrari, C., Mueller, M.N., Jovanović, N., Vechev, M.: Complete Veriﬁcation via\nMulti-Neuron Relaxation Guided Branch-and-Bound. In: ICLR. OpenReview.net\n(2022), https://openreview.net/forum?id=l_amHf1oaK\n15. García,\nJ.,\nFernández,\nF.:\nA\ncomprehensive\nsurvey\non\nsafe\nrein-\nforcement\nlearning.\nJ.\nMach.\nLearn.\nRes.\n16,\n1437–1480\n(2015).\nhttps://doi.org/10.5555/2789272.2886795\n16. Guidotti, D., Leofante, F., Pulina, L., Tacchella, A.: Veriﬁcation and Repair\nof Neural Networks: A Progress Report on Convolutional Models. In: AI*IA.\nLecture Notes in Computer Science, vol. 11946, pp. 405–417. Springer (2019).\nhttps://doi.org/10.1007/978-3-030-35166-3_29\n17. Guidotti, D., Leofante, F., Tacchella, A., Castellini, C.: Improving reliability of\nmyocontrol using formal veriﬁcation. IEEE Trans. Neural Syst. Rehabilitation Eng.\n27(4), 564–571 (2019). https://doi.org/10.1109/TNSRE.2019.2893152\n18. Hans,\nA.,\nSchneegaß,\nD.,\nSchäfer,\nA.M.,\nUdluft,\nS.:\nSafe\nexplo-\nration\nfor\nreinforcement\nlearning.\nIn:\nESANN.\npp.\n143–148\n(2008),\nhttps://www.esann.org/sites/default/ﬁles/proceedings/legacy/es2008-36.pdf\n19. Ivanov, R.,\nWeimer,\nJ.,\nAlur,\nR.,\nPappas,\nG.J.,\nLee,\nI.:\nVerisig:\nverify-\ning\nsafety\nproperties\nof\nhybrid\nsystems\nwith\nneural\nnetwork\ncontrollers.\nIn:\nOzay,\nN.,\nPrabhakar,\nP.\n(eds.)\nHSCC.\npp.\n169–178.\nACM\n(2019).\nhttps://doi.org/10.1145/3302504.3311806\n20. Jin, P., Tian, J., Zhi, D., Wen, X., Zhang, M.: Trainify: A CEGAR-Driven Training\nand Veriﬁcation Framework for Safe Deep Reinforcement Learning. In: Shoham,\nS., Vizel, Y. (eds.) CAV(1). Lecture Notes in Computer Science, vol. 13371, pp.\n193–218. Springer (2022). https://doi.org/10.1007/978-3-031-13185-1_10\n21. Katz, G., Barrett, C.W., Dill, D.L., Julian, K.D., Kochenderfer, M.J.: Reluplex:\nAn Eﬃcient SMT Solver for Verifying Deep Neural Networks. In: Majumdar, R.,\nKuncak, V. (eds.) CAV (1). Lecture Notes in Computer Science, vol. 10426, pp.\n97–117. Springer (2017). https://doi.org/10.1007/978-3-319-63387-9_5\n22. Landers, M., Doryab, A.: Deep reinforcement learning veriﬁcation: A survey. ACM\nComput. Surv. 55(14s), 330:1–330:31 (2023). https://doi.org/10.1145/3596444\n23. Mankowitz, D.J., Michi, A., Zhernov, A., Gelmi, M., Selvi, M., Paduraru, C.,\nLeurent, E., Iqbal, S., Lespiau, J.B., Ahern, A., Köppe, T., Millikin, K., Gaﬀney,\nS., Elster, S., Broshear, J., Gamble, C., Milan, K., Tung, R., Hwang, M., Cemgil,\nT., Barekatain, M., Li, Y., Mandhane, A., Hubert, T., Schrittwieser, J., Hassabis,\nD., Kohli, P., Riedmiller, M., Vinyals, O., Silver, D.: Faster sorting algorithms\ndiscovered using deep reinforcement learning. Nat. 618(7964), 257–263 (2023).\nhttps://doi.org/10.1038/s41586-023-06004-9\n24. Mnih, V., Badia, A.P., Mirza, M., Graves, A., Lillicrap, T.P., Harley, T.,\nSilver, D., Kavukcuoglu, K.: Asynchronous methods for deep reinforcement\nlearning.\nIn:\nBalcan,\nM.,\nWeinberger,\nK.Q.\n(eds.)\nICML.\nJMLR\nWork-\nshop and Conference Proceedings, vol. 48, pp. 1928–1937. JMLR.org (2016),\nhttp://proceedings.mlr.press/v48/mniha16.html\n25. Nocedal, J., Wright, S.J.: Numerical Optimization. Springer, 2 edn. (2006).\nhttps://doi.org/10.1007/b98874\n26. OpenAI: Introducing ChatGPT (2022), https://openai.com/blog/chatgpt, ac-\ncessed 14th March 2024\n10\nD. Boetius, S. Leue\n27. Philipp, G., Song, D., Carbonell, J.G.: The exploding gradient problem demys-\ntiﬁed - deﬁnition, prevalence, impact, origin, tradeoﬀs, and solutions. CoRR\nabs/1712.05577 (2017), http://arxiv.org/abs/1712.05577\n28. Pulina, L., Tacchella, A.: An Abstraction-Reﬁnement Approach to Veriﬁcation of\nArtiﬁcial Neural Networks. In: CAV. Lecture Notes in Computer Science, vol. 6174,\npp. 243–257. Springer (2010). https://doi.org/10.1007/978-3-642-14295-6_24\n29. Schulman,\nJ.,\nWolski,\nF.,\nDhariwal,\nP.,\nRadford,\nA.,\nKlimov,\nO.:\nProx-\nimal\npolicy\noptimization\nalgorithms.\nCoRR\nabs/1707.06347\n(2017),\nhttp://arxiv.org/abs/1707.06347\n30. Sotoudeh, M., Thakur, A.V.: Provable repair of deep neural networks. In: PLDI.\npp. 588–603. ACM (2021). https://doi.org/10.1145/3453483.3454064\n31. Srinivasan,\nK.,\nEysenbach,\nB.,\nHa,\nS.,\nTan,\nJ.,\nFinn,\nC.:\nLearning\nto\nbe\nsafe:\nDeep\nRL\nwith\na\nsafety\ncritic.\nCoRR\nabs/2010.14603\n(2020),\nhttps://arxiv.org/abs/2010.14603\n32. Sun,\nX.,\nShoukry,\nY.:\nProvably\ncorrect\ntraining\nof\nneural\nnetwork\ncon-\ntrollers\nusing\nreachability\nanalysis.\nCoRR\nabs/2102.10806\n(2021),\nhttps://arxiv.org/abs/2102.10806\n33. Sutton, R.S., Barto, A.G.: Reinforcement Learning: An Introduction. Adap-\ntive\ncomputation\nand\nmachine\nlearning,\nMIT\nPress,\nSecond\nedn. (2018),\nhttps://mitpress.mit.edu/9780262039246/reinforcement-learning/\n34. Tan, C., Zhu, Y., Guo, C.: Building veriﬁed neural networks with speciﬁcations\nfor systems. In: Gunawi, H.S., Ma, X. (eds.) APSys. pp. 42–47. ACM (2021).\nhttps://doi.org/10.1145/3476886.3477508\n35. Tao,\nZ.,\nNawas,\nS.,\nMitchell,\nJ.,\nThakur,\nA.V.:\nArchitecture-preserving\nprovable\nrepair\nof\ndeep\nneural\nnetworks.\nCoRR\nabs/2304.03496\n(2023).\nhttps://doi.org/10.48550/arXiv.2304.03496\n36. Tjeng, V., Xiao, K.Y., Tedrake, R.: Evaluating Robustness of Neural Networks\nwith Mixed Integer Programming. In: ICLR (Poster). OpenReview.net (2019),\nhttps://openreview.net/forum?id=HyGIdiRqtm\n37. Tran, H., Yang, X., Lopez, D.M., Musau, P., Nguyen, L.V., Xiang, W., Bak,\nS., Johnson, T.T.: NNV: The Neural Network Veriﬁcation Tool for Deep\nNeural Networks and Learning-Enabled Cyber-Physical Systems. In: CAV (1).\nLecture Notes in Computer Science, vol. 12224, pp. 3–17. Springer (2020).\nhttps://doi.org/10.1007/978-3-030-53288-8_1\n38. Wang, S., Zhang, H., Xu, K., Lin, X., Jana, S., Hsieh, C., Kolter, J.Z.: Beta-\nCROWN: Eﬃcient Bound Propagation with Per-neuron Split Constraints for\nNeural Network Robustness Veriﬁcation. In: Ranzato, M., Beygelzimer, A.,\nDauphin, Y.N., Liang, P., Vaughan, J.W. (eds.) NeurIPS. pp. 29909–29921 (2021),\nhttps://proceedings.neurips.cc/paper/2021/hash/fac7fead96dafceaf80c1daﬀeae82a4-Abstract.html\n39. Yang, Q., Simão, T.D., Tindemans, S.H., Spaan, M.T.J.: Safety-constrained rein-\nforcement learning with a distributional safety critic. Mach. Learn. 112(3), 859–887\n(2023). https://doi.org/10.1007/S10994-022-06187-8\n40. Zhang, H., Wang, S., Xu, K., Li, L., Li, B., Jana, S., Hsieh, C., Kolter, J.Z.:\nGeneral Cutting Planes for Bound-Propagation-Based Neural Network Veriﬁca-\ntion. In: Oh, A.H., Agarwal, A., Belgrave, D., Cho, K. (eds.) NeurIPS (2022),\nhttps://openreview.net/forum?id=5haAJAcofjc\n",
  "categories": [
    "cs.LG",
    "cs.LO"
  ],
  "published": "2024-05-24",
  "updated": "2024-05-24"
}