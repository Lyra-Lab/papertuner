{
  "id": "http://arxiv.org/abs/2405.16030v1",
  "title": "Constrained Ensemble Exploration for Unsupervised Skill Discovery",
  "authors": [
    "Chenjia Bai",
    "Rushuai Yang",
    "Qiaosheng Zhang",
    "Kang Xu",
    "Yi Chen",
    "Ting Xiao",
    "Xuelong Li"
  ],
  "abstract": "Unsupervised Reinforcement Learning (RL) provides a promising paradigm for\nlearning useful behaviors via reward-free per-training. Existing methods for\nunsupervised RL mainly conduct empowerment-driven skill discovery or\nentropy-based exploration. However, empowerment often leads to static skills,\nand pure exploration only maximizes the state coverage rather than learning\nuseful behaviors. In this paper, we propose a novel unsupervised RL framework\nvia an ensemble of skills, where each skill performs partition exploration\nbased on the state prototypes. Thus, each skill can explore the clustered area\nlocally, and the ensemble skills maximize the overall state coverage. We adopt\nstate-distribution constraints for the skill occupancy and the desired cluster\nfor learning distinguishable skills. Theoretical analysis is provided for the\nstate entropy and the resulting skill distributions. Based on extensive\nexperiments on several challenging tasks, we find our method learns\nwell-explored ensemble skills and achieves superior performance in various\ndownstream tasks compared to previous methods.",
  "text": "Constrained Ensemble Exploration for Unsupervised Skill Discovery\nChenjia Bai 1 2 Rushuai Yang 3 Qiaosheng Zhang 1 Kang Xu 4 Yi Chen 3 Ting Xiao 5 Xuelong Li 1 6\nAbstract\nUnsupervised Reinforcement Learning (RL) pro-\nvides a promising paradigm for learning useful\nbehaviors via reward-free per-training. Existing\nmethods for unsupervised RL mainly conduct\nempowerment-driven skill discovery or entropy-\nbased exploration. However, empowerment often\nleads to static skills, and pure exploration only\nmaximizes the state coverage rather than learn-\ning useful behaviors. In this paper, we propose a\nnovel unsupervised RL framework via an ensem-\nble of skills, where each skill performs partition\nexploration based on the state prototypes. Thus,\neach skill can explore the clustered area locally,\nand the ensemble skills maximize the overall state\ncoverage. We adopt state-distribution constraints\nfor the skill occupancy and the desired cluster for\nlearning distinguishable skills. Theoretical analy-\nsis is provided for the state entropy and the result-\ning skill distributions. Based on extensive exper-\niments on several challenging tasks, we find our\nmethod learns well-explored ensemble skills and\nachieves superior performance in various down-\nstream tasks compared to previous methods.\n1. Introduction\nReinforcement Learning (RL) (Sutton & Barto, 2018) has\ndemonstrated strong abilities in decision-making for various\napplications, including game AI (Schrittwieser et al., 2020;\nYe et al., 2021), self-driving cars (Wu et al., 2022), robotic\nmanipulation (Hansen et al., 2022; He et al., 2024b;a), and\nlocomotion tasks (Miki et al., 2022; Shi et al., 2024). How-\never, most successes rely on well-defined reward functions\nbased on physical prior and domain knowledge (Haldar\n1Shanghai Artificial Intelligence Laboratory 2Shenzhen Re-\nsearch Institute of Northwestern Polytechnical University 3Hong\nKong University of Science and Technology 4Tencent 5East China\nUniversity of Science and Technology 6The Institute of Artificial\nIntelligence (TeleAI), China Telecom. Correspondence to: Ting\nXiao <xiaoting@ecust.edu.cn>.\nProceedings of the 41 st International Conference on Machine\nLearning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by\nthe author(s).\net al., 2022), which can be notoriously difficult to design\n(Kwon et al., 2023). In contrast to RL, other research fields\nlike language and vision have greatly benefited from unsu-\npervised learning (i.e., without annotations or labels), such\nas auto-regressive pre-training for Large Language Model\n(LLM) (Han et al., 2021; Touvron et al., 2023) and unsuper-\nvised representation learning for images (Chen et al., 2020a;\nClark & Jaini, 2023) that benefit various language and vi-\nsion tasks. Motivated by this, unsupervised RL aims to learn\nmeaningful behaviors without extrinsic rewards, where the\nlearned behaviors can be used to solve various downstream\ntasks via fast adaptation for generalizable RL.\nIn unsupervised RL research, previous methods often con-\nduct empowerment-driven skill discovery to learn distin-\nguishable skills (Gregor et al., 2016; Eysenbach et al., 2019).\nSpecifically, the agent learns skill-conditional policies by\nmaximizing an estimation of Mutual Information (MI) be-\ntween skills and trajectories, which leads to discriminating\nskill-conditional policies with different behaviors. However,\nsuch an MI objective often generates static skills with poor\nstate coverage (Strouse et al., 2022). Recent works partially\naddress this problem via Lipschitz constraints (Park et al.,\n2022; 2023) and random-walk guidance (Kim et al., 2023),\nwhile they still rely on the primary MI objective. Mean-\nwhile, estimating the MI needs variational estimators based\non sampling (Song & Ermon, 2020), which is challenging in\nhigh-dimensional and stochastic environments (Yang et al.,\n2023) and also leads to sub-optimal performance (Laskin\net al., 2021). Other methods perform pure exploration via cu-\nriosity (Burda et al., 2019) and state entropy (Liu & Abbeel,\n2021a;b; Yarats et al., 2021) in environments, while they\nonly focus on maximizing the state coverage rather than\nlearning meaningful behaviors for downstream tasks.\nIn this paper, we take an alternative perspective for unsu-\npervised RL and propose a novel skill discovery frame-\nwork, named Constrained Ensemble exploration for Skill\nDiscovery (CeSD). We adopt an ensemble of value func-\ntions to learn different skills, where each value function\nuses independent intrinsic rewards that encourage the agent\nto explore a partition of the state space based on the as-\nsigned prototype, without considering the states of other\nprototypes. The prototypes are learned by feature clustering\nof visited states and can act as representative anchors in the\nstate visitation space. Based on the ensemble value func-\n1\narXiv:2405.16030v1  [cs.LG]  25 May 2024\nConstrained Ensemble Exploration for Unsupervised Skill Discovery\ntion, we obtain the corresponding skills via policy gradient\nupdates. Since the skills perform entropy estimation based\non non-overlapping clusters, they can perform independent\nexploration to expand the boundary of the assigned cluster,\nleading to diverse behaviors. To overcome the potential\noverlap of the state coverage of updated skills, we adopt\nadditional constraints to the state distribution between skills\nand the assigned clusters, which enforce skills to visit non-\noverlapping states to generate more distinguishable skills.\nTheoretically, we show the state entropy of each skill is\nmonotonically increasing with the distribution constraints,\nand the ensemble skills maximize the global state coverage\nvia partition exploration in clusters. We conduct extensive\nexperiments on mazes and Unsupervised Reinforcement\nLearning Benchmark (URLB) (Laskin et al., 2021), show-\ning that CeSD learns well-explored and diverse skills.\nThe contribution can be summarized as follows. (i) Unlike\nprevious empowerment-based methods, CeSD takes an alter-\nnative perspective on skill discovery that bypasses MI esti-\nmation and also learns meaningful skills assisted by entropy-\nbased exploration. (ii) We propose ensemble skills that ex-\nplore the environment within individual clusters and apply\nadditional constraints to learn distinguishable skills. (iii) We\nprovide theoretical analysis for the state coverage of skills.\n(iv) We obtain state-of-the-art performance in various down-\nstream tasks from challenging DeepMind Control Suite\n(DMC) tasks of URLB. The open-sourced code is avail-\nable at https://github.com/Baichenjia/CeSD.\n2. Preliminaries\nWe consider a Markov Decision Process (MDP) with an ad-\nditional skill space, defined as (S, A, Z, P, r, γ, ρ0), where\nS is the state space, A is the action space, Z is a skill space,\nP(s′|s, a) is the transition function, γ is the discount factor,\nand ρ0 : S →[0, 1] is the initial state distribution. We\nuse a discrete skill space Z since learning infinite skills\nwith diverse behaviors can be difficult (Jiang et al., 2022).\nWhen interacting with the environment, the agent takes ac-\ntions a ∼π(·|s, z) by following the skill-conditional policy\nπ(a|s, z) with a one-hot skill vector z ∈Rn. We use zi\nto denote the vector with a 1 in the i-th coordinate and 0’s\nelsewhere. For example, z3 = (0, 0, 1, 0, 0) in R5. We use\nπi(a|s) and π(a|s, zi) interchangeable to denote the policy\ncondition on skill zi. Given clear contexts, we refer to the\n‘skill-conditional policy’ as ‘skill’ for simplification.\nIn the skill-learning stage, the policy is learned by maxi-\nmizing discounted cumulative reward denoted as P\nt γtrt,\nwhere rt is generated by some intrinsic reward function,\nsuch as empowerment or entropy-based methods. In the\npolicy-adaptation stage, we choose a specific skill vector z⋆\nand fine-tune the policy π(a|s, z⋆) with the extrinsic reward\nfor downstream tasks. In unsupervised RL, we allow the\nagent to perform sufficient interactions in the skill-learning\nstage to learn meaningful skills, while only allowing a small\nnumber of interactions in the fine-tuning stage to perform\npolicy adaptation. Overall, unsupervised RL aims to learn\nskills in the first stage for fast adaptation to various tasks in\nthe second stage.\nWe denote I(·; ·) by the mutual information between two\nrandom variables, and H(·) by either the Shannon entropy\nor differential entropy depending on the context. We use up-\npercase letters for random variables and lowercase letters for\ntheir realizations. The empowerment objective maximizes\nan MI-objective I(S; Z) estimation and the entropy-driven\nobjective maximizes H(S). In both objectives, s ∼dπ(s)\nis the normalized probability that a policy π encounters state\ns, defined as dπ(s) ≜(1 −γ) P∞\nt=0 γtP(st = s|π).\n3. Method\nThe proposed CeSD adopts ensemble Q-functions for skill\ndiscovery, where each skill performs partition exploration\nwith prototypes. We adopt constraints on state distribution\nfor regularizing skills. We give theoretical analyses to show\nthe advantage of our algorithm on state coverage.\n3.1. Ensemble Skill Discovery\nPrevious methods learn skill-conditional policy π(a|s, z)\nby maximizing the corresponding value function Qz(s, a).\nHowever, since different skills share the same network pa-\nrameters, optimizing one skill can affect learning other skills.\nAccording to our observations, learning a single value func-\ntion can have negative effects on learning diverse skills that\nhave significantly different behaviors.\nTo address this problem, we propose to use an ensem-\nble of value functions in CeSD. Specifically, we adopt\nan ensemble of Q-networks for different skills, defined\nas {Q1(s, a), . . . , Qn(s, a)}. The ensemble number is the\nsame as the number of skills. Each Q-network is learned by\nminimizing the temporal-difference (TD) error as\nmin\nϕi EDi\n\u0002\nQϕi(s, a)−\n\u0000ri(s, a)+γ max\na′ Qϕ′\ni(s′, a′)\n\u0001\u0003\n, (1)\nwhere ϕi is the parameter of i-th network, Qϕ′\ni is the corre-\nsponding target network, Di is a state buffer, and ri is the\nintrinsic reward and will be discussed later. Since different\nskills have independent parameters for the Q-function, the\ndifferent Q-functions can emerge diverse behaviors through\noptimization. In training the Q-networks in Eq. (1), we\nadopt efficient parallelization for ensemble networks to min-\nimize the run-time increase with the number of skills.\nFor learning the policy, we adopt a basic skill-conditional\nactor that maximizes the corresponding value function in\n2\nConstrained Ensemble Exploration for Unsupervised Skill Discovery\nthe ensemble, and the objective function is\nmaxψ Ea∼πψ(·|s,zi)\n\u0002\nQϕi(s, a)\n\u0003\n,\ni ∈[n]\n(2)\nwhere we denote ψ as the policy parameters. Since the value\nensemble has already learned the knowledge of different\nskills, we find that using a single network is sufficient to\nmodel the multi-skill policy.\nAlthough previous works have also adopted ensemble Q-\nnetworks (Lee et al., 2021; An et al., 2021) for online and\noffline RL, they are significantly different from our method.\nSpecifically, previous methods use the same objective for\nensemble Q-networks. Thus, the learned Q-values estimate\nthe approximate posterior of Q-function in online and offline\nRL (Bai et al., 2022; Li et al., 2022), essential for theoret-\nically grounded uncertainty estimation for optimism (Hao\net al., 2023; Bai et al., 2021b) or pessimism (Wang et al.,\n2024; Wen et al., 2024; Bai et al., 2024; Deng et al., 2023).\nIn contrast, we adopt ‘ensemble’ to represent a collection\nof Q-functions used for different skills. These skills are\nlearned in the state space via partition exploration and used\nfor downstream adaptation. The ensemble Q-networks in\nour method have different objectives that encourage indepen-\ndent exploration for separate areas with intrinsic rewards,\nwhich makes the ensembles represent value functions of\ndiverse skills that optimize the policy in different directions.\n3.2. Partition Exploration with Prototype\nWe learn state prototypes through self-supervised learning\nto divide the explored states into clusters. Then, each Q-\nfunction in the ensemble can perform independent explo-\nration based on the entropy estimation of states in the corre-\nsponding cluster. Specifically, we learn discrete state pro-\ntotypes through soft-assignment clustering, and the learned\nprototypes act as representative anchors in the state space.\nBased on the prototypes, each visited state can be assigned\nto a specific cluster, and each cluster corresponds to a spe-\ncific value function in exploration.\nThe training of prototypes is given as follows. For a specific\nstate st, we use a neural network to map the state to a\nvector ut = fθ(st) ∈Rm. We also define n continuous\nvectors {c1, . . . , cn} as prototypes, where ci ∈Rm. Then\nthe probability of st being assigned to the i-th prototype is\np(t)\ni\n= exp\n\u0000ˆu⊤\nt ci/τ\n\u0001\n/\nXn\nj=1 exp\n\u0000ˆu⊤\nt cj/τ\n\u0001\n,\n(3)\nwhere ˆut is the normalized vector as ut/∥ut∥2, and τ is\nthe temperature factor. Similar to Eq. (3), we use a fixed\ntarget network fθ−(·) with the same parameters as fθ(·) to\nobtain a normalized target vector u−\nt\n= fθ−(st). Then,\nthe target probability q(t)\ni\nis obtained by running an online\nclustering Sinkhorn-Knopp algorithm (Cuturi, 2013; Caron\net al., 2020) on the normalized target vector ˆu−\nt . Then, we\nuse the cross-entropy loss to update the prototypes as\nLproto = −\nX\nt\nX\ni q(t)\ni\nlog p(t)\ni .\n(4)\nIn the unsupervised stage, the prototypes {ci} will update\nwith more collected states. Nevertheless, we remark that\nsuch an update is gradual with gradient descent and will\nnot cause drastic changes in probability p(t)\ni , which makes\nthe cluster assignment stable for the collected states and\nbenefits the calculation of intrinsic rewards in exploration.\nBased on the learned prototypes, each state can be as-\nsigned to a specific cluster by following z(t) ∼p(t), where\np(t) = [p(t)\n1 , . . . , p(t)\nn ] represents a categorical distribution.\nIn practice, we adopt a small temperature value in Eq. (3)\nto obtain a near-deterministic cluster assignment. We de-\nnote the set of collected states by S, and then partition S\ninto n disjoint clusters as {S1, S2, . . . , Sn} according to\nthe categorical distribution. For convenience, we slightly\nabuse Si to include the whole transition {(s, a, s′)} for each\nstate. Based on the clusters, the skill policies can conduct\npartition exploration by maximizing the state entropy of\nthe corresponding cluster. Specifically, we adopt a simple\ncluster-skill correspondence mechanism by assigning the\ncluster Si to the value function Qi with the same skill in-\ndex. Since the state entropy of each cluster can be estimated\nseparately, we can calculate the intrinsic rewards for each\nvalue function independently to encourage partition explo-\nration without considering states from other clusters. For\nexample, the value function Qi will use {s, a, s′} ∈Si and\nthe corresponding intrinsic rewards rcesd\ni\ncalculated in Si\nfor TD-learning, which encourages policy π(a|s, zi) to ex-\nplore the state space based on Si without considering other\nclusters (i.e., Sj, j ̸= i), thus leading to diverse behaviors\nfor different skills.\nParticle Estimation\nTo calculate the entropy-based intrin-\nsic reward rcesd\ni\n, we adopt a popular particle-based entropy\nestimation algorithm in previous methods (Liu & Abbeel,\n2021b; Laskin et al., 2022), and the entropy is estimated\nby a sum of the log distance between each particle and its\nk-th nearest neighbor. Following this method, the particle\nentropy estimation for i-th cluster Si is calculated as,\nHk(Si) ∝\nX\nst∈Si ln\n\r\rut −NNk,fθ(ut)\n\r\r,\n(5)\nwhere the distance is calculated in the feature space of states.\nThen the intrinsic reward for (st, at, st+1) ∈Si is set to\nrcesd\ni\n(st, at) =\n\r\rut+1 −NNk,fθ(ut+1)\n\r\r.\n(6)\nFor each value function Qi in the ensemble, we perform clus-\ntering based on prototypes and obtain Si = {(st, at, st+1)}.\nWe follow Eq. (6) to calculate the intrinsic reward for each\nexample in Si, and obtain the reward-augmented cluster\n3\nConstrained Ensemble Exploration for Unsupervised Skill Discovery\n!!\n{#}\n. . .\n. . .\n\"\"\"# \"$\nSoftmax\nSinkhorn\nClustering\nCross\nEntropy\nPredicted Probability\n\"%\nInner product\n. . .\nCluster index\n1\n2\n3\nn\n. . .\n2\nsample\n(For each sample)\n. . .\nclusters \n{'!\n\"#$%}\nEntropy estimation\n{'&\n\"#$%}\n{''\"#$%}\nTD learning\n. . .\nFigure 1. The partition exploration process. We adopt Sinkhorn-Knopp algorithm to learn prototypes and perform clustering for states.\nThe intrinsic reward is calculated by entropy estimation within each cluster and then used for training a specific Q-network.\nset as ˆSi = {(st, at, st+1, rcesd\ni\n)}. Then, we minimize the\nTD-error of Qi by following Eq. (1) with experiences sam-\npled from ˆSi. We adopt the same training process for all\nclusters i ∈[n], which can be practically implemented via a\nmasking technique to determine whether a transition should\nbe used for training a specific Qi network. We illustrate the\nwhole process of partition exploration in Figure 1.\nEntropy Analysis\nWe give a simple analysis for entropy\nestimation based on clusters. The state entropy of partition\nexploration is calculated in each cluster Si, while in global\nexploration is calculated in S. Given fixed state sets, we\ndenote policies that obtain the maximum entropy in the clus-\nter Si and the overall state set S by π∗\ni and π∗, respectively.\nThen the following Theorem holds.\nTheorem 3.1. Let each cluster have the same number of\nsamples, for i ∈[n], the relationship between the maximum\nentropy of π∗in the state set S and π∗\ni in the cluster set Si is\nH\n\u0000dπ∗(s)\n\u0001\n= H\n\u0000dπ∗\ni (s)\n\u0001\n+ C(n),\n(7)\nwhere C(n) = log n depends on the number of clusters n.\nThe assumption holds since the Sinkhorn-Knopp algorithm\nconstrains assigning each cluster to the same number of\nsamples. We refer to Appendix A for a proof. Theorem 3.1\nshows the optimal policies {π∗\ni } with uniform visitation\nin cluster sets {Si} also obtains the maximum entropy in\nthe global state set S. Thus, performing partition explo-\nration in clusters also maximizes the global state coverage.\nMeanwhile, we can obtain diverse skills through partition\nexploration rather than a single exploratory policy in global\nexploration (Liu & Abbeel, 2021b; Laskin et al., 2022).\n3.3. Skill Distribution Constraint\nWe delve into the learning process of partition exploration\nand propose a state-distribution constraint for generating\ndistinguishable skills. In Figure 2, we show the information\ndiagrams of the learning process of CeSD. For a clear illus-\ntration, we only show three skills in each sub-figure, while\nwe may adopt more skills in practice for complex tasks.\nThe randomly initialized skills often have small entropy\n(i.e., H(dπi(s))), which signifies each skill only visits states\naround the start point, as shown in Figure 2(a). Considering\nin a tabular case, we use {Sinit\ni\n, Sinit\n2\n, Sinit\n3\n} to represent col-\nlected state sets for three skills and assume Sinit\ni\n∩Sinit\nj\n= ∅.\nSpecifically, we assume each skill has an independent ex-\nplored area initially since the corresponding value function\nin the ensemble has different initialized parameters.\nProblem Statement\nIn partition exploration, each skill\nuses the state entropy as intrinsic rewards defined in Eq. (6).\nThen, each skill π(·|s, zi) will learn to (i) assign uniform\noccupancy probability for the visited states, which increases\nthe entropy with a fixed state set. More importantly, (ii) each\nskill will learn to explore the environment to collect new\nstates that do not occur in Sinit\ni\n. As more states are added to\nSinit\ni\n, the corresponding state entropy H(dπi(s)) increases\nand the skill explores more unknown areas. As shown in\nFigure 2(b), the state coverage of each skill increased in\npartition exploration. We denote the new state sets after a\nround of partition exploration as {Spe\ni }, and the overall state\nset is defined as Spe = ∪i{Spe\ni }.\nNevertheless, since different skills perform independent\nexploration based on their previous state visitation, they\nmay explore the same intersection area after policy update\nand collect the same states in the updated sets {Spe\ni }, which\nmakes Spe\ni ∩Spe\nj\n̸= ∅, where i ̸= j. As shown in Figure 2(b),\nthe visitation area of each skill overlaps with other skills,\nwhich does not hurt exploration but reduces the distinguisha-\nbility of different skills. For example, in locomotion and\nmanipulation tasks, different skills generate similar behavior\nif their state distributions overlap significantly.\nState Distribution Constraint\nTo address this challenge,\nwe propose an explicit distribution constraint for the updated\nstate distribution to regularize skill learning. To achieve this,\nwe first perform clustering to divide the overlapping area\nand assign different parts to different skills. We denote the\ndifferent state set after clustering as {Sclu\ni }, then we have\nSclu\ni\n∩Sclu\nj\n= ∅(i ̸= j) since each state will be assigned to\na unique cluster. As shown in Figure 2(c), different colors\n4\nConstrained Ensemble Exploration for Unsupervised Skill Discovery\nd!! s\n𝑑!\" 𝑠\n𝑑!# 𝑠\n!𝑑!\" 𝑠\n!𝑑!# 𝑠\n!𝑑!$ 𝑠\n𝑑!$ s\nmin D 𝑑!$ ∥𝑑%!$\n𝑑!# 𝑠\n(a) Initialize\n(b) Exploration\n(c) Clustering\n(d) Distribution Constraint\n(e) Regularized Skills\n𝑑!\" s\n𝑑!! 𝑠\n𝑑!# 𝑠\nIteration\n𝑑!\" 𝑠\n𝑑!$ s\n𝑑!# 𝑠\n𝑑!\" 𝑠\nFigure 2. The learning process of CeSD. After initializing skills, we conduct entropy-based exploration for each skill and perform\nclustering to obtain non-overlapping clusters. Then the state distribution constraint is applied to enhance the diversity of skills. The\nregularized skills are used for partition exploration in the next round of iteration.\nrepresent the non-overlapping state sets after clustering,\nand Spe = ∪i{Sclu\ni } holds since the overall state set does\nnot change. In most cases, we have Sclu\ni\n⊆Spe\ni\nsince the\nclustering algorithm will keep the cluster-index of existing\nstates fixed and re-assign the newly added states to different\nclusters. Nevertheless, it does not always hold since the\nprototypes can be sub-optimal in training.\nBased on the above analyses and Figure 2(b), we denote\nthe state distribution lying on the state set Spe\ni\nas dπi. How-\never, a more desired state set is Sclu\ni\nin Figure 2(c), which\nhas non-overlapping states with other clusters and leads to\nmore diverse skills. Thus, we define a desired policy ˆπi\nbased on πi, where dˆπi(s) = 0 for s ∈Spe\ni\n−Sclu\ni\nthat\nrepresents the difference between two sets, and dˆπi(s) =\ndπi(s)/ P\ns∈Sclu\ni\ndπi(s) for other states by re-normalizing\ndπi in the cluster states. Ideally, our constraint for regulariz-\ning the skill behavior is defined as\nLreg(πθ(s, zi)) = 1\n2\nX\ns∈Spe\ni\n\f\fdˆπi(s) −dπi(s)\n\f\f.\n(8)\nHowever, it can be computationally expensive to minimize\nthe Total Variation (TV) distance Lreg via density estimation\nof states (Lee et al., 2020; Zhang et al., 2021). Alternatively,\nwe propose to approximately reduce such a gap by minimiz-\ning Es∼dπi[DTV(ˆπi(·|s)∥π(·|s))], which servers an upper\nbound of the density discrepancy (as shown in Lemma 3.2\nbelow). The main difference between ˆπi(·|s) and π(·|s) is\nthat, the desired policy ˆπi has zero visitation probability\nfor states s ∈Spe\ni\n−Sclu\ni , while the policy π(·|s) has some\nprobability to visit states in the intersection set of skills.\nThus, we propose a heuristic intrinsic reward to prevent the\ncurrent policy πi from visiting states in Spe\ni\n−Sclu\ni , as\nrreg\ni\n= 1/\n\u0000\f\fSpe\ni\n−Sclu\ni\n\f\f + λ\n\u0001\n.\n(9)\nThis reward is maximized when |Spe\ni −Sclu\ni | = 0, which sig-\nnifies πi only visits state in its assigned cluster set Sclu\ni\nthat\nhas no overlap to other clusters. As shown in Figure 2(d),\nmaximizing rreg\ni\nwill force skill πi to reduce the visitation\nprobability of states lied in clusters of other skills, which\nmakes policy πi closer to ˆπi that only visits states in Sclu\ni .\nWe illustrate the regularized skills in Figure 2(e).\nQualitative Analysis\nIn the next, we give a qualitative\nanalysis of the state entropy of skills in the learning process.\nWe start with a lemma to show that minimizing the policy\ndivergence can also reduce the constraint term Lreg(πi).\nLemma 3.2. The divergence between state distribution is\nbounded on the average divergence of policies ˆπi and πi, as\nDTV\n\u0000dˆπi∥dπi\u0001\n≤\nγ\n1 −γ Es∼dπi\n\u0002\nDTV(ˆπi(·|s)∥πi(·|s))\n\u0003\n,\n(10)\nwhere DTV(·∥·) is the total variation distance.\nWe refer to Appendix A for a proof.\nAccording to\nLemma 3.2, by minimizing the policy divergence (i.e.,\nDTV(ˆπi∥πi)) via the intrinsic reward, the difference\nbetween state distribution (i.e., DTV(dˆπi∥dπi)) can be\nbounded. Then we have the following theorem for the en-\ntropy of state distributions.\nTheorem 3.3. Assuming the distance between state dis-\ntribution is bounded by DTV\n\u0000dˆπi∥dπi\u0001\n≤δ, the entropy\ndifference between state distribution can be bounded by\n\f\fH(dˆπi) −H(dπi)\n\f\f ≤δ log\n\u0000|Spe\ni | −1\n\u0001\n+ h(δ).\n(11)\nwhere h(x) := −x log(x)−(1−x) log(1−x) is the binary\nentropy function.\nThe proof follows the coupling technique (Barbour, 2001)\nand Fano’s inequality (Fano, 2008), as given in Appendix A.\nCorollary 3.4. The state entropy of each πi is monotoni-\ncally increasing with partition exploration and constraints.\nProof. Assuming that DTV\n\u0000dˆπi||dπi\u0001\n≤\nδ, we have\n|H(dˆπi) −H(dπi)| ≤f(δ), where f(δ) = δ log(|Spe\ni | −\n1)+h(δ) is a constant determined by the state cluster and the\nstate distribution bound. Then we have H(dπi) ≥H(dˆπi)−\nf(δ). Corollary 4.4 holds since we maximize the state en-\ntropy (i.e., we set the intrinsic reward to rcesd\ni\n= H(dˆπi) in\n5\nConstrained Ensemble Exploration for Unsupervised Skill Discovery\npolicy learning. The state entropy of the skill policy (i.e.,\nH(dπi)) also increases since f(δ) is a positive constant\ngiven a fix δ and the state set.\nIn each iteration of CeSD, since we maximize the state\nentropy in each cluster (i.e., H(dˆπi)) via particle estima-\ntion, the state entropy of current policy (i.e., H(dπi)) is also\nforced to be increased according to Theorem 3.3. As a result,\nthe state entropy of each skill πi is monotonically increasing\nwith partition exploration and distribution constraints. Fur-\nther, according to Theorem 3.1, since the maximum entropy\nin each cluster (i.e., H(dˆπi)) has a constant gap with the\nmaximum entropy in the overall state set (i.e., H(dˆπ)), our\nmethod monotonically increases the global state coverage\nin exploration.\n4. Related Works\nUnsupervised Pretraining of RL Unsupervised Pretrain-\ning methods in RL aim at obtaining prior knowledge from\nunlabeled data or reward-free interactions to facilitate down-\nstream task learning (Xie et al., 2022). The methods pri-\nmarily fall into three categories: Unsupervised Skill Dis-\ncovery (USD) (Eysenbach et al., 2019; Sharma et al., 2020;\nPark et al., 2022; Ajay et al., 2020; Park et al., 2023; Laskin\net al., 2022), Data Coverage Maximization (Liu & Abbeel,\n2021b;a; Yarats et al., 2021; 2022; Lee et al., 2020), and\nRepresentation Learning (Mazoure et al., 2021; Yang &\nNachum, 2021; Yuan et al., 2022; Ghosh et al., 2023). Our\nwork falls into the first category and intersects the data\ncoverage maximization methods. To obtain skills with dis-\ncriminating behaviors, existing USD research mainly re-\nlies on the MI objectives (Gregor et al., 2016; Eysenbach\net al., 2019). However, the recent study (Strouse et al.,\n2022) has revealed the pessimistic exploration problem of\nthe MI paradigm. Thus, several works try to enhance the\nstate coverage via Euclidean distance constraint (Park et al.,\n2022), recurrent training (Jiang et al., 2022), discriminator\ndisagreement (Strouse et al., 2022), controllability-aware\nobjective (Park et al., 2023), and guidance skill (Kim et al.,\n2023). However, they still rely on MI objectives for skill\ndiscrimination (Strouse et al., 2022; Kim et al., 2023) and\nrequire variational estimators based on sampling (Song &\nErmon, 2020). In contrast, CeSD simultaneously enhances\nstate coverage and skill discrimination by performing parti-\ntion exploration and clustering-based skill constraint, with-\nout requiring inefficient recurrent training and state distance\nfunctions. Concerning the clustering method, a similar\ntechnique has also been utilized in prior RL pretraining\nworks (Yarats et al., 2021; Mazoure et al., 2021). Motivated\nby different purposes, we perform clustering to guarantee\ndistinct exploration regions of skills, instead of estimating\nstate visitation or learning generalizable representation.\nPolicy Regularization in RL Policy regularization has\nplayed diverse roles in RL algorithms, including constrain-\ning the policy to close to the behavior policy in offline\nRL (Nair et al., 2020; Wang et al., 2020; Fujimoto & Gu,\n2021), enhancing the exploration ability for online explo-\nration (Haarnoja et al., 2018; Flet-Berliac et al., 2020;\nChane-Sane et al., 2021), and reducing the policy distance\nto demonstrations (Ho & Ermon, 2016; Brown et al., 2019;\nXu et al., 2022; Ma et al., 2022). Formally, policy regular-\nization regularizes the current policy to some target policy\nwith a specific probabilistic form, or the target policy can be\nestimated via limited interactions (Rajeswaran et al., 2018)\nor offline datasets (Ma et al., 2022). In contrast, the target\nskill policy in our work does not have a specific form and is\ncharacterized by clustered states Sclu\ni\nsampled from the state\ndistribution dˆπi(s). Meanwhile, the target policy changes\nin each iteration with more sampled states, which makes\nour setting different from prior ones and particularly chal-\nlenging. Thus, we propose a simple but effective reward to\nencourage each skill to visit fewer states lying in clusters of\nother skills, which regularizes the policy learning.\nValue Ensemble To capture the epistemic uncertainty (Bai\net al., 2021a; Qiu et al., 2022) induced by the limited training\ndata in RL, the value ensemble has been proposed to ap-\nproximately estimate the posterior distribution of the value\nfunction in online (Fujimoto et al., 2018; Chen et al., 2020b;\nLee et al., 2021; Hao et al., 2023) and offline RL (Bai et al.,\n2022; Li et al., 2022; An et al., 2021). Recent works (Xu\net al., 2024; Wen et al., 2023) also adopt an ensemble for\ncross-domain policy adaptation and reuse (Xu et al., 2023).\nDifferent from these works, we utilize value ensemble to\nestimate the expected returns of different skills. To con-\nfront the non-stationary objective and mutual interference\nbetween skills, we adopt an independent value function for\neach skill.\n5. Experiments\nIn this section, we compare the performance of unsupervised\nRL methods in challenging URLB tasks (Laskin et al., 2021).\nWe also conduct experiments in a maze to illustrate the\nlearned skills in a continuous 2D space. We finally conduct\nvisualizations and ablation studies of our method.\n5.1. Skill Learning in 2D maze\nWe conduct experiments for skill discovery in a 2D maze\nenvironment from Campos et al. (2020). The observation\nof the agent is the current position S ∈R2, and the action\nA ∈R2 controls the velocity and direction. We consider\nseveral strong baselines for unsupervised training, including\nDIAYN (Eysenbach et al., 2019), DADS (Sharma et al.,\n2020), and CIC (Laskin et al., 2022). In these methods,\nDIAYN and DADS perform skill discovery by maximiz-\n6\nConstrained Ensemble Exploration for Unsupervised Skill Discovery\nCeSD (ours)\nDADS\nDIAYN\nCIC\nFigure 3. Visualization of skill discovery in Maze. Different colors represent the state trajectories with different skill vectors. We let the\nagent start moving from the black dot in the upper left corner and sample 20 trajectories for each skill for visualization.\ning the MI term of states and skills (i.e., I(S; Z)) via a\nreverse form (i.e., H(Z) −H(Z|S)) and forward form (i.e.,\nH(S) −H(S|Z)), respectively, to construct a variational\nestimation of the MI objective. CIC is a data-based method\nthat maximizes the state entropy estimation (i.e., H(S))\nfor pure exploration. For a fair comparison, we use a 10-\ndimensional one-hot vector for skills in all methods.\nIn Figure 3, we visualize the learned skills by sampling\ntrajectories from the skill-conditional policies of CeSD\nand other baselines. (i) Concerning the discriminability\nof skills, we find empowerment-based methods like DIAYN\nand DADS learn distinguishable skills, where each skill can\ngenerate trajectories that are different from those of other\nskills. In contrast, entropy-driven methods like CIC cannot\ngenerate discriminable skills due to the lack of mechanisms\nto distinguish different skills. (ii) Concerning state coverage,\nboth DIAYN and DADS have limited state coverage since\nthey rely on the MI objective without encouraging explo-\nration. The entropy-based CIC algorithm obtains the best\nstate coverage since the entropy maximization encourages\nexploration of the environment and also leads to a uniform\nstate visitation within the whole state space. (iii) The pro-\nposed CeSD performs the best in both skill discriminability\nand state coverage. CeSD takes the theoretical advantage\nof monotonic entropy increasing (as Theorem 3.3), and the\nensemble skills obtain well global coverage. Meanwhile,\nCeSD learns distinguishable skills with approximately non-\noverlapping coverage via state distribution constraints.\n5.2. URLB Benchmark Results\nWe evaluate CeSD in the URLB benchmark (Laskin et al.,\n2021). Walker domain contains biped locomotion tasks\nwith S ∈R24 and A ∈R6; Quadruped domain contains\nquadruped locomotion tasks with high-dimensional state\nand action space as S ∈R78 and A ∈R16, which have\nmuch larger space in exploration and is more challenging;\nand Jaco Arm domain contains a 6-DoF robotic arm and a\nthree-finger gripper with S ∈R55 and A ∈R9. In experi-\nments, each method performs unsupervised skill learning\nwith intrinsic rewards and adapts the skills to downstream\ntasks with extrinsic rewards. There are 3 downstream tasks\nfor each domain, including Stand, Walk, Run, and Flip tasks\nfor Walker domain, Stand, Walk, Run, and Jump tasks for\nQuadruped domain, and move the objective to Bottom Left,\nBottom Right, Top Left, and Top Right for Jaco Arm.\nWe compare CeSD to several strong baselines. Specifically,\nwe compare CeSD to (i) the skill discovery methods, in-\ncluding DIAYN (Eysenbach et al., 2019), SMM (Lee et al.,\n2020), and APS (Liu & Abbeel, 2021a); (ii) the entropy-\nbased exploration methods, including APT (Liu & Abbeel,\n2021b), ProtoRL (Yarats et al., 2021), and CIC (Laskin\net al., 2022); and (iii) curiosity-driven exploration methods,\nincluding ICM (Pathak et al., 2017), RND (Burda et al.,\n2019), and Disagreement (Pathak et al., 2019); (iv) the re-\ncently proposed BeCL (Yang et al., 2023) algorithm that\nperforms contrastive skill discovery. Most implementations\nof baselines follow URLB (Laskin et al., 2021) and the of-\nficial code of baselines. We refer to Appendix C for the\nhyper-parameters and implementation details.\nWe do not include DISCO-DANCE (Kim et al., 2023) as a\nbaseline since it does not open-source the code. Meanwhile,\nDyna-MPC (Rajeswar et al., 2023) Dyna-MPC is a model-\nbased finetuning method with extrinsic rewards, while our\nmethod focuses on unsupervised pertaining. Choreographer\n(Mazzaglia et al., 2023) is learned in an offline dataset col-\nlected by exploration algorithms, while CeSD and baselines\nare all learned from scratch via exploring the environment.\nThus, we do not use Dyna-MPC and Choreographer as base-\nlines. The recently proposed methods like LSD (Park et al.,\n2022), CSD (Park et al., 2023), and Metra (Park et al., 2024)\nare evaluated on different benchmarks other than URLB. We\ntried to re-implement these methods in URLB tasks based on\nthe official code, and the results are given in Appendix D.6.\nIn the unsupervised training stage, each method is trained\nfor 2M steps with its intrinsic reward. Then we randomly\nsample a skill as the policy condition and fine-tune the\npolicy for 100K steps in each downstream task for fast adap-\ntation. Rather than choosing the best skill in the fine-tuning\n7\nConstrained Ensemble Exploration for Unsupervised Skill Discovery\n0.4\n0.6\n0.8\nSMM\nDIAYN\nICM\nAPS\nDisagreement\nRND\nProtoRL\nAPT\nCIC\nBeCL\nCeSD\nMedian\n0.4\n0.6\n0.8\nIQM\n0.45\n0.60\n0.75\n0.90\nMean\n0.15\n0.30\n0.45\n0.60\nOptimality Gap\nExpert Normalized Score\nSMM\nDIAYN\nICM\nAPS\nDisagreement\nRND\nProtoRL\nAPT\nCIC\nBeCL\nCeSD\nFigure 4. Comparison of performance in 12 downstream tasks of URLB benchmark. We report the aggregate statistics of 10 seeds by\nfollowing Agarwal et al. (2021) after finetuning. CeSD achieves the new state-of-the-art results in the URLB benchmark.\nFigure 5. An illustration of the rolling skill learned in Quadruped.\nquadruped_stand\nquadruped_walk\nquadruped_jump\nquadruped_run\n0\n200\n400\n600\n800\nscore\nEnsemble-16\nEnsemble-8\nEnsemble-4\nFigure 6. An ablation study of the ensemble skills in Quadruped.\nstage, we comprehensively evaluate the generalizability of\nall skills for adaptation in downstream tasks. We run 10 ran-\ndom seeds for each baseline, which results in 11 algorithms\n× 10 seeds × 12 tasks = 1320 runs. We follow reliable\n(Agarwal et al., 2021) to evaluate the aggregated statistics,\nincluding mean, median, interquartile mean (IQM), and opti-\nmality gap (OG) with the 95% bootstrap confidence interval.\nThe expert score in calculating the metrics is adopted from\nLaskin et al. (2021), which is obtained by an expert DDPG\nagent. According to the results in Figure 4, our CeSD al-\ngorithm achieves the best results in the URLB benchmark.\nCompared to the entropy-based baselines, our method out-\nperforms CIC (with 75.18% IQM) and achieves 91.05%\nIQM. The results show that partition exploration and distri-\nbution constraints in CeSD benefit skill learning and lead to\nmore efficient generalization in downstream tasks compared\nto the global exploration performed in CIC. Compared to\nthe expert score, CeSD achieves the best OG result with\n15.47%, significantly outperforming the previous state-of-\nthe-art BeCL algorithm (with 25.44% OG). We also report\nthe evaluation scores of all methods in Appendix C.6.\n5.3. Visualization of Skills\nIn URLB, we visualize the behaviors of skills learned in\nthe unsupervised training stage. We find many interesting\nlocomotion and manipulation domain skills emerging in the\npretraining stage with our method. Specifically, CeSD can\nlearn various locomotion skills, including standing, walking,\nrolling, moving, somersault, and jumping in Walker and\nQuadruped. In Jaco, the agent learns various manipulation\nskills, including moving the arm to explore different areas\nand controlling the gripper to grasp objects in different loca-\ntions. An example of rolling skills in the Quadruped domain\nis shown in Figure 5. We provide more visualizations of\nskills in DMC domains in Appendix C.5.\nThrough partition exploration and distribution constraint,\nour method learns dynamic and non-trivial behavior during\nthe unsupervised training stage. In contrast, previous skill-\ndiscovery methods usually learn distinguishable posing or\nyoga-style skills but are often static and lack exploration\nability, which has been visualized in previous works (Laskin\net al., 2022; Yang et al., 2023). Since our method can learn\nmeaningful behaviors during the unsupervised stage, it ob-\ntains superior generalization performance in the fine-tuning\nstage in various downstream tasks, as shown in Figure 4.\n5.4. Ablation Study\nIn this section, we provide ablation studies on the ensemble\nnumber of skills and state distribution constraints in CeSD.\n8\nConstrained Ensemble Exploration for Unsupervised Skill Discovery\nα\n1.0\n0.1\n2.0\n10.0\n20.0\nFigure 7. An ablation study of different factors of the regularization reward for distribution constraint in the maze task.\nEnsemble Value Function\nWe conduct an ablation study\nof ensemble value functions in the Quadruped domain. We\nreduce the ensemble number of value functions while keep-\ning the skill number unchanged, which makes a single\nQ-network responsible for the learning of several skills.\nAs shown in Figure 6, as we reduce the ensemble num-\nber, the generalization performance of skills also decreases.\nSince the different skills are enforced to explore independent\nspace, reducing the number of skills will enlarge the explo-\nration space of each value function, reducing the uniqueness\nof each skill. As an extreme case, reducing the ensemble\nsize to 1 resembles CIC (Laskin et al., 2022) that only per-\nforms exploration without learning distinguishable skills.\nDistribution Constraints\nThe final intrinsic reward of\nCeSD is rcesd\ni\n(s, a) + α · rreg\ni\n(s, a), where rreg\ni\n(s, a) per-\nforms distribution constraint for different skills. We conduct\nan ablation study of distribution constraints using different\nvalues for α, as shown in Figure 7. (i) When α is very small\n(e.g., α = 0.1), the trajectories of different skills are mixed\nand CeSD is very similar to CIC. Nevertheless, since we\nadopt an ensemble network and partition exploration for\ndifferent skills, the state coverage of skills also has slight\ndifferences. (ii) As we increase α (e.g., α ∈[1.0, 2.0]),\nthe regularized reward will force each skill to reduce the\nvisitation probability of states lying in clusters of other\nskills, which makes the different skills more distinguishable.\n(iii) When the value of α becomes extremely large (e.g.,\nα ≥10.0), the distribution constraints will dominate the\nreward function, which may hinder the exploration of skills.\n6. Conclusion\nWe have introduced CeSD, a novel skill discovery method\nassisted by partition exploration and state distribution con-\nstraint. We perform self-supervised clustering for collected\nstates and constrain the exploration of each skill based on the\nassigned cluster, which leads to diverse skills with strong\nexploration abilities. Extensive experiments in the maze\nand URLB benchmark show that CeSD can explore com-\nplex environments and obtain state-of-the-art performance\nin adaptation to various downstream tasks. The main lim-\nitation of our method is that the ensemble value functions\ncannot be generalized to the continuous skill space. A future\ndirection is to adopt a randomized value function (Azizzade-\nnesheli et al., 2018) or hyper Q-network (Li et al., 2022) for\nimplicit ensembles with an infinite number of networks.\nAcknowledgements\nThis work was supported by the National Natural Science\nFoundation of China (No.62306242).\nImpact Statement\nThis paper presents work whose goal is to advance the field\nof Machine Learning. There are many potential societal\nconsequences of our work, none of which we feel must be\nspecifically highlighted here.\nReferences\nAgarwal, R., Schwarzer, M., Castro, P. S., Courville, A. C.,\nand Bellemare, M. Deep reinforcement learning at the\nedge of the statistical precipice. Advances in neural in-\nformation processing systems, 34:29304–29320, 2021.\nAjay, A., Kumar, A., Agrawal, P., Levine, S., and Nachum,\nO. Opal: Offline primitive discovery for accelerating of-\nfline reinforcement learning. In International Conference\non Learning Representations, 2020.\nAn, G., Moon, S., Kim, J.-H., and Song, H. O. Uncertainty-\nbased offline reinforcement learning with diversified q-\nensemble. In Advances in Neural Information Processing\nSystems, 2021.\nAzizzadenesheli, K., Brunskill, E., and Anandkumar, A.\nEfficient exploration through bayesian deep q-networks.\nIn 2018 Information Theory and Applications Workshop\n(ITA), pp. 1–9. IEEE, 2018.\nBai, C., Wang, L., Han, L., Garg, A., Hao, J., Liu, P., and\nWang, Z. Dynamic bottleneck for robust self-supervised\nexploration. Advances in Neural Information Processing\nSystems, 34:17007–17020, 2021a.\n9\nConstrained Ensemble Exploration for Unsupervised Skill Discovery\nBai, C., Wang, L., Han, L., Hao, J., Garg, A., Liu, P., and\nWang, Z. Principled exploration via optimistic bootstrap-\nping and backward induction. In International Confer-\nence on Machine Learning, pp. 577–587. PMLR, 2021b.\nBai, C., Wang, L., Yang, Z., Deng, Z.-H., Garg, A., Liu, P.,\nand Wang, Z. Pessimistic bootstrapping for uncertainty-\ndriven offline reinforcement learning. In International\nConference on Learning Representations, 2022.\nBai, C., Wang, L., Hao, J., Yang, Z., Zhao, B., Wang, Z.,\nand Li, X. Pessimistic value iteration for multi-task data\nsharing in offline reinforcement learning. Artificial Intel-\nligence, 326:104048, 2024.\nBarbour, A. D. Coupling, stationarity, and regeneration.\nJournal of the American Statistical Association, 96(454):\n780–780, 2001. doi: 10.1198/jasa.2001.s401.\nBelghazi, M. I., Baratin, A., Rajeshwar, S., Ozair, S., Ben-\ngio, Y., Courville, A., and Hjelm, D. Mutual information\nneural estimation. In International Conference on Ma-\nchine Learning, volume 80, pp. 531–540, 2018.\nBrown, D., Goo, W., Nagarajan, P., and Niekum, S. Extrap-\nolating beyond suboptimal demonstrations via inverse re-\ninforcement learning from observations. In International\nconference on machine learning, pp. 783–792. PMLR,\n2019.\nBurda, Y., Edwards, H., Storkey, A., and Klimov, O. Explo-\nration by random network distillation. In International\nConference on Learning Representations, 2019.\nCampos, V., Trott, A., Xiong, C., Socher, R., Gir´o-i Nieto,\nX., and Torres, J. Explore, discover and learn: Unsu-\npervised discovery of state-covering skills. In Interna-\ntional Conference on Machine Learning, pp. 1317–1327.\nPMLR, 2020.\nCaron, M., Misra, I., Mairal, J., Goyal, P., Bojanowski, P.,\nand Joulin, A. Unsupervised learning of visual features\nby contrasting cluster assignments. Advances in neural\ninformation processing systems, 33:9912–9924, 2020.\nCelik, O., Zhou, D., Li, G., Becker, P., and Neumann, G.\nSpecializing versatile skill libraries using local mixture\nof experts. In Conference on Robot Learning, pp. 1423–\n1433. PMLR, 2022.\nChane-Sane, E., Schmid, C., and Laptev, I.\nGoal-\nconditioned reinforcement learning with imagined sub-\ngoals. In International Conference on Machine Learning,\npp. 1430–1440. PMLR, 2021.\nChen, T., Kornblith, S., Norouzi, M., and Hinton, G. A\nsimple framework for contrastive learning of visual rep-\nresentations. In International Conference on Machine\nLearning, volume 119, pp. 1597–1607, 2020a.\nChen, X., Wang, C., Zhou, Z., and Ross, K. W. Randomized\nensembled double q-learning: Learning fast without a\nmodel. In International Conference on Learning Repre-\nsentations, 2020b.\nClark, K. and Jaini, P. Text-to-image diffusion models are\nzero-shot classifiers. In Neural Information Processing\nSystems, 2023.\nCover, T. M. Elements of information theory. John Wiley &\nSons, 2006.\nCsisz´ar, I. and K¨orner, J. Information theory: coding theo-\nrems for discrete memoryless systems. Cambridge Uni-\nversity Press, 2011.\nCuturi, M. Sinkhorn distances: Lightspeed computation\nof optimal transport. Advances in neural information\nprocessing systems, 26, 2013.\nDeng, Z., Fu, Z., Wang, L., Yang, Z., Bai, C., Zhou, T.,\nWang, Z., and Jiang, J. False correlation reduction for\noffline reinforcement learning. IEEE Transactions on\nPattern Analysis and Machine Intelligence, 2023.\nEysenbach, B., Gupta, A., Ibarz, J., and Levine, S. Diversity\nis all you need: Learning skills without a reward function.\nIn International Conference on Learning Representations,\n2019.\nFano, R. M. Fano inequality. Scholarpedia, 3(10):6648,\n2008. doi: 10.4249/scholarpedia.6648.\nFlet-Berliac, Y., Ferret, J., Pietquin, O., Preux, P., and Geist,\nM. Adversarially guided actor-critic. In International\nConference on Learning Representations, 2020.\nFujimoto, S. and Gu, S. S. A minimalist approach to offline\nreinforcement learning. Advances in neural information\nprocessing systems, 34:20132–20145, 2021.\nFujimoto, S., Hoof, H., and Meger, D. Addressing function\napproximation error in actor-critic methods. In Interna-\ntional conference on machine learning, pp. 1587–1596.\nPMLR, 2018.\nGhosh, D., Bhateja, C. A., and Levine, S. Reinforcement\nlearning from passive data via latent intentions. In Inter-\nnational Conference on Machine Learning, pp. 11321–\n11339. PMLR, 2023.\nGregor, K., Rezende, D. J., and Wierstra, D. Variational\nintrinsic control. arXiv preprint arXiv:1611.07507, 2016.\nHaarnoja, T., Zhou, A., Abbeel, P., and Levine, S. Soft\nactor-critic: Off-policy maximum entropy deep reinforce-\nment learning with a stochastic actor. In International\nconference on machine learning, pp. 1861–1870. PMLR,\n2018.\n10\nConstrained Ensemble Exploration for Unsupervised Skill Discovery\nHaldar, S., Mathur, V., Yarats, D., and Pinto, L. Watch and\nmatch: Supercharging imitation with regularized optimal\ntransport. In 6th Annual Conference on Robot Learning,\n2022.\nHan, X., Zhang, Z., Ding, N., Gu, Y., Liu, X., Huo, Y.,\nQiu, J., Yao, Y., Zhang, A., Zhang, L., et al. Pre-trained\nmodels: Past, present and future. AI Open, 2:225–250,\n2021.\nHansen, N. A., Su, H., and Wang, X. Temporal differ-\nence learning for model predictive control. In Interna-\ntional Conference on Machine Learning, pp. 8387–8406.\nPMLR, 2022.\nHao, J., Yang, T., Tang, H., Bai, C., Liu, J., Meng, Z.,\nLiu, P., and Wang, Z. Exploration in deep reinforcement\nlearning: From single-agent to multiagent domain. IEEE\nTransactions on Neural Networks and Learning Systems,\n2023.\nHe, H., Bai, C., Pan, L., Zhang, W., Zhao, B., and Li,\nX. Large-scale actionless video pre-training via discrete\ndiffusion for efficient policy learning. arXiv preprint\narXiv:2402.14407, 2024a.\nHe, H., Bai, C., Xu, K., Yang, Z., Zhang, W., Wang, D.,\nZhao, B., and Li, X. Diffusion model is an effective\nplanner and data synthesizer for multi-task reinforcement\nlearning. Advances in neural information processing\nsystems, 36, 2024b.\nHo, J. and Ermon, S. Generative adversarial imitation learn-\ning. Advances in neural information processing systems,\n29, 2016.\nJiang, Z., Gao, J., and Chen, J. Unsupervised skill discovery\nvia recurrent skill training. In Oh, A. H., Agarwal, A.,\nBelgrave, D., and Cho, K. (eds.), Advances in Neural\nInformation Processing Systems, 2022.\nKim, H., Lee, B., Park, S., Lee, H., Hwang, D., Min, K.,\nand Choo, J. Learning to discover skills with guidance.\nIn Advances in Neural Information Processing Systems,\n2023.\nKwon, M., Xie, S. M., Bullard, K., and Sadigh, D. Reward\ndesign with language models. In The Eleventh Interna-\ntional Conference on Learning Representations, 2023.\nLaskin, M., Yarats, D., Liu, H., Lee, K., Zhan, A., Lu, K.,\nCang, C., Pinto, L., and Abbeel, P. URLB: Unsuper-\nvised reinforcement learning benchmark. In Neural In-\nformation Processing Systems (Datasets and Benchmarks\nTrack), 2021.\nLaskin, M., Liu, H., Peng, X. B., Yarats, D., Rajeswaran,\nA., and Abbeel, P. Unsupervised reinforcement learning\nwith contrastive intrinsic control. In Advances in Neural\nInformation Processing Systems, 2022.\nLee, K., Laskin, M., Srinivas, A., and Abbeel, P. Sunrise: A\nsimple unified framework for ensemble learning in deep\nreinforcement learning. In International Conference on\nMachine Learning, pp. 6131–6141. PMLR, 2021.\nLee, L., Eysenbach, B., Parisotto, E., Xing, E., Levine,\nS., and Salakhutdinov, R. Efficient exploration via state\nmarginal matching. arXiv preprint arXiv:1906.05274,\n2019.\nLee, L., Eysenbach, B., Parisotto, E., Xing, E., Levine,\nS., and Salakhutdinov, R.\nEfficient exploration via\nstate marginal matching, 2020.\nURL https://\nopenreview.net/forum?id=Hkla1eHFvS.\nLi, Z., Li, Y., Zhang, Y., Zhang, T., and Luo, Z.-Q. Hy-\nperDQN: A randomized exploration method for deep\nreinforcement learning. In International Conference on\nLearning Representations, 2022.\nLiu, H. and Abbeel, P. Aps: Active pretraining with suc-\ncessor features. In International Conference on Machine\nLearning, pp. 6736–6747. PMLR, 2021a.\nLiu, H. and Abbeel, P. Behavior from the void: Unsuper-\nvised active pre-training. In Advances in Neural Informa-\ntion Processing Systems, volume 34, pp. 18459–18473,\n2021b.\nMa, Y., Shen, A., Jayaraman, D., and Bastani, O. Versatile\noffline imitation from observations and examples via reg-\nularized state-occupancy matching. In International Con-\nference on Machine Learning, pp. 14639–14663. PMLR,\n2022.\nMazoure, B., Ahmed, A. M., Hjelm, R. D., Kolobov, A.,\nand MacAlpine, P. Cross-trajectory representation learn-\ning for zero-shot generalization in rl. In International\nConference on Learning Representations, 2021.\nMazzaglia, P., Verbelen, T., Dhoedt, B., Lacoste, A., and\nRajeswar, S. Choreographer: Learning and adapting skills\nin imagination. In The Eleventh International Conference\non Learning Representations, 2023. URL https://\nopenreview.net/forum?id=PhkWyijGi5b.\nMiki, T., Lee, J., Hwangbo, J., Wellhausen, L., Koltun, V.,\nand Hutter, M. Learning robust perceptive locomotion\nfor quadrupedal robots in the wild. Science Robotics, 7\n(62):eabk2822, 2022.\nNair, A., Gupta, A., Dalal, M., and Levine, S. Awac: Accel-\nerating online reinforcement learning with offline datasets.\narXiv preprint arXiv:2006.09359, 2020.\n11\nConstrained Ensemble Exploration for Unsupervised Skill Discovery\nPark, S., Choi, J., Kim, J., Lee, H., and Kim, G. Lipschitz-\nconstrained unsupervised skill discovery. In International\nConference on Learning Representations, 2022.\nPark, S., Lee, K., Lee, Y., and Abbeel, P. Controllability-\naware unsupervised skill discovery. In International Con-\nference on Machine Learning, volume 202, pp. 27225–\n27245, 2023.\nPark, S., Rybkin, O., and Levine, S. METRA: Scalable\nunsupervised RL with metric-aware abstraction. In The\nTwelfth International Conference on Learning Represen-\ntations, 2024. URL https://openreview.net/\nforum?id=c5pwL0Soay.\nPathak, D., Agrawal, P., Efros, A. A., and Darrell, T.\nCuriosity-driven exploration by self-supervised predic-\ntion. In International Conference on Machine Learning,\npp. 2778–2787. PMLR, 2017.\nPathak, D., Gandhi, D., and Gupta, A. Self-supervised ex-\nploration via disagreement. In International Conference\non Machine Learning, pp. 5062–5071. PMLR, 2019.\nQiu, S., Wang, L., Bai, C., Yang, Z., and Wang, Z. Con-\ntrastive ucb: Provably efficient contrastive self-supervised\nlearning in online reinforcement learning. In Interna-\ntional Conference on Machine Learning, pp. 18168–\n18210. PMLR, 2022.\nRajeswar, S., Mazzaglia, P., Verbelen, T., Pich´e, A., Dhoedt,\nB., Courville, A., and Lacoste, A. Mastering the unsu-\npervised reinforcement learning benchmark from pixels.\nIn International Conference on Machine Learning, pp.\n28598–28617. PMLR, 2023.\nRajeswaran, A., Kumar, V., Gupta, A., Vezzani, G., Schul-\nman, J., Todorov, E., and Levine, S. Learning complex\ndexterous manipulation with deep reinforcement learning\nand demonstrations. Robotics: Science and Systems XIV,\n2018.\nSchrittwieser, J., Antonoglou, I., Hubert, T., Simonyan, K.,\nSifre, L., Schmitt, S., Guez, A., Lockhart, E., Hassabis,\nD., Graepel, T., et al. Mastering atari, go, chess and shogi\nby planning with a learned model. Nature, 588(7839):\n604–609, 2020.\nSharma, A., Gu, S., Levine, S., Kumar, V., and Hausman,\nK. Dynamics-aware unsupervised discovery of skills. In\nInternational Conference on Learning Representations,\n2020.\nShi, J., Bai, C., He, H., Han, L., Wang, D., Zhao, B., Zhao,\nM., Li, X., and Li, X. Robust quadrupedal locomotion via\nrisk-averse policy learning. In 2024 IEEE International\nConference on Robotics and Automation (ICRA). IEEE,\n2024.\nSong, J. and Ermon, S. Understanding the limitations of vari-\national mutual information estimators. In International\nConference on Learning Representations, 2020.\nStrouse, D., Baumli, K., Warde-Farley, D., Mnih, V., and\nHansen, S. S. Learning more skills through optimistic\nexploration. In International Conference on Learning\nRepresentations, 2022.\nSutton, R. S. and Barto, A. G. Reinforcement learning: An\nintroduction. MIT press, 2018.\nTassa, Y., Doron, Y., Muldal, A., Erez, T., Li, Y., Casas, D.\nd. L., Budden, D., Abdolmaleki, A., Merel, J., Lefrancq,\nA., et al.\nDeepmind control suite.\narXiv preprint\narXiv:1801.00690, 2018.\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi,\nA., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,\nBhosale, S., et al. Llama 2: Open foundation and fine-\ntuned chat models. arXiv preprint arXiv:2307.09288,\n2023.\nWang, C., Yu, X., Bai, C., Zhang, Q., and Wang, Z. Ensem-\nble successor representations for task generalization in\noffline-to-online reinforcement learning. arXiv preprint\narXiv:2405.07223, 2024.\nWang, Z., Novikov, A., Zolna, K., Merel, J. S., Springen-\nberg, J. T., Reed, S. E., Shahriari, B., Siegel, N., Gulcehre,\nC., Heess, N., et al. Critic regularized regression. Ad-\nvances in Neural Information Processing Systems, 33:\n7768–7778, 2020.\nWen, X., Yu, X., Yang, R., Bai, C., and Wang, Z. Towards\nrobust offline-to-online reinforcement learning via uncer-\ntainty and smoothness. arXiv preprint arXiv:2309.16973,\n2023.\nWen, X., Bai, C., Xu, K., Yu, X., Zhang, Y., Li, X., and\nWang, Z. Contrastive representation for data filtering in\ncross-domain offline reinforcement learning. In Interna-\ntional Conference on Machine Learning, 2024.\nWu, J., Huang, Z., and Lv, C. Uncertainty-aware model-\nbased reinforcement learning: Methodology and appli-\ncation in autonomous driving. IEEE Transactions on\nIntelligent Vehicles, 8(1):194–203, 2022.\nXie, Z., Lin, Z., Li, J., Li, S., and Ye, D. Pretraining in\ndeep reinforcement learning: A survey. arXiv preprint\narXiv:2211.03959, 2022.\nXu, H., Zhan, X., Yin, H., and Qin, H. Discriminator-\nweighted offline imitation learning from suboptimal\ndemonstrations. In International Conference on Machine\nLearning, pp. 24725–24742. PMLR, 2022.\n12\nConstrained Ensemble Exploration for Unsupervised Skill Discovery\nXu, K., Bai, C., Qiu, S., He, H., Zhao, B., Wang, Z., Li, W.,\nand Li, X. On the value of myopic behavior in policy\nreuse. arXiv preprint arXiv:2305.17623, 2023.\nXu, K., Bai, C., Ma, X., Wang, D., Zhao, B., Wang, Z., Li,\nX., and Li, W. Cross-domain policy adaptation via value-\nguided data filtering. Advances in Neural Information\nProcessing Systems, 36, 2024.\nYang, M. and Nachum, O. Representation matters: Of-\nfline pretraining for sequential decision making. In In-\nternational Conference on Machine Learning, pp. 11784–\n11794. PMLR, 2021.\nYang, R., Bai, C., Guo, H., Li, S., Zhao, B., Wang, Z., Liu,\nP., and Li, X. Behavior contrastive learning for unsuper-\nvised skill discovery. In Proceedings of the 40th Inter-\nnational Conference on Machine Learning, pp. 39183–\n39204, 2023.\nYarats, D., Fergus, R., Lazaric, A., and Pinto, L. Reinforce-\nment learning with prototypical representations. In Inter-\nnational Conference on Machine Learning, pp. 11920–\n11931. PMLR, 2021.\nYarats, D., Brandfonbrener, D., Liu, H., Laskin, M., Abbeel,\nP., Lazaric, A., and Pinto, L. Don’t change the algorithm,\nchange the data: Exploratory data for offline reinforce-\nment learning. arXiv preprint arXiv:2201.13425, 2022.\nYe, W., Liu, S., Kurutach, T., Abbeel, P., and Gao, Y. Mas-\ntering atari games with limited data. Advances in Neural\nInformation Processing Systems, 34:25476–25488, 2021.\nYuan, Z., Xue, Z., Yuan, B., Wang, X., Wu, Y., Gao, Y., and\nXu, H. Pre-trained image encoder for generalizable visual\nreinforcement learning. Advances in Neural Information\nProcessing Systems, 35:13022–13037, 2022.\nZhang, T., Rashidinejad, P., Jiao, J., Tian, Y., Gonzalez,\nJ. E., and Russell, S. Made: Exploration via maximizing\ndeviation from explored regions. In Advances in Neural\nInformation Processing Systems, volume 34, pp. 9663–\n9680, 2021.\nZhang, Z. Estimating mutual information via kolmogorov\ndistance. IEEE Transactions on Information Theory, 53\n(9):3280–3282, 2007.\n13\nConstrained Ensemble Exploration for Unsupervised Skill Discovery\nA. Theoretical Analysis\nA.1. Proof of Theorem 3.1\nTheorem (Restate of Theorem 3.1). Let each cluster have the same number of samples, for i ∈[n], the relationship between\nthe maximum entropy of π∗in the state set S and π∗\ni in the cluster set Si is\nH\n\u0000dπ∗(s)\n\u0001\n= H\n\u0000dπ∗\ni (s)\n\u0001\n+ C(n),\n(12)\nwhere C(n) = log n depends on the number of clusters n.\nProof. According to the assumption, each Si should have the same number of samples as |Si| = N\nn , where we denote the\ntotal samples as |S| = N. For a set of states, the entropy obtains its maximum when the policy uniformly visits each state,\nas dπ⋆\ni (s) =\n1\n|Si| = n\nN in partition exploration, and dπ⋆(s) =\n1\n|S| = 1\nN in global exploration.\nFor policy π⋆\ni in partition exploration, the corresponding entropy of state distribution is\nH(dπ⋆\ni (s)) = −\nX\ns∈Si\ndπ⋆\ni (s) log dπ⋆\ni (s) = −\nX\ns∈Si\nn\nN log n\nN = log N −log n.\n(13)\nSimilarly, for policy π⋆in global exploration, the corresponding entropy of state distribution is\nH(dπ⋆) = −\nX\ns∈S\ndπ⋆(s) log dπ⋆(s) = −\nX\ns∈S\n1\nN log 1\nN = log N.\n(14)\nThen we have the following relationship as\nH\n\u0000dπ∗(s)\n\u0001\n= H\n\u0000dπ∗\ni (s)\n\u0001\n+ C(n),\n(15)\nwhere C(n) = −log n that depends on the number of skills.\nThe primary purpose of Theorem 3.1 is to relate the entropy of the global optimal policy π∗and local optimal policies\n{π∗\ni }i∈[n], thus showing that maximizing the entropy of each local policy will effectively maximize the entropy of the global\npolicy. In many scenarios where uniform distributions over the state space S and subsets of state space Si are realizable,\nthe maximum entropy policies π∗and π∗\ni are exactly equal to these uniform distributions. Thus we have the relation\nH(dπ∗(s)) = H(dπ∗\ni (s)) + log n for any i ∈[n] (as stated in Theorem 3.1). We highlight this fact because it provides a\nsimple yet clear insight into why performing partition exploration in clusters also maximizes global state coverage.\nMeanwhile, there are scenarios where uniform distributions are not realizable. In this case, let d(s) be a distribution over S\nthat is composed by the local state distributions {dπ∗\ni (s)}i∈[n], such that\nd(s) = αi · dπ∗\ni (s) if and only if s ∈Si,\nwhere P\ni∈[n] αi = 1 and each αi represents the probability that a state belongs to Si. Then, we have\nH(d(s)) =\n n\nX\ni=1\nαi · H(dπ∗\ni (s))\n!\n+ H({α1, α2, . . . , αn}),\n(16)\nwhere H({α1, α2, . . . , αn}) is the entropy of the probability vector (α1, α2, . . . , αn). Thus, increasing/maximizing\neach local entropy H(dπ∗\ni (s)) will lead to an increase of the global entropy of the distribution d(s). Eqn. (16) is also\nconsistent with our current Theorem 3.1, where H(dπ∗\ni (s)) = log N −log n is the entropy of the uniform distribu-\ntion over Si, H(d(s)) = log N is the entropy of the uniform distribution over S, and αi = 1/n for all i ∈[n] (i.e.,\nH({α1, α2, . . . , αn}) = log n).\nAlthough we do not know the maximum entropy policy π∗, but it must satisfy\nH(dπ∗(s)) ≥\nmax\nαi∈[0,1], Pn\ni=1 αi=1\n n\nX\ni=1\nαi · H(dπ∗\ni (s))\n!\n+ H(α1, α2, . . . , αn).\n(17)\n14\nConstrained Ensemble Exploration for Unsupervised Skill Discovery\nProof of (16):\nH(d(s)) = −\nX\ns∈S\nd(s) log d(s)\n(18)\n= −\nn\nX\ni=1\nX\ns∈Si\nd(s) log d(s)\n(19)\n= −\nn\nX\ni=1\nX\ns∈Si\nαidπ∗\ni (s) · log(αidπ∗\ni (s))\n(20)\n= −\nn\nX\ni=1\nX\ns∈Si\nαidπ∗\ni (s) · [log(dπ∗\ni (s)) + log(αi)]\n(21)\n=\n n\nX\ni=1\n−αi\nX\ns∈Si\ndπ∗\ni (s) log dπ∗\ni (s)\n!\n+\n n\nX\ni=1\n−αi\nX\ns∈Si\ndπ∗\ni (s) log(αi)\n!\n(22)\n=\nn\nX\ni=1\nαiH(dπ∗\ni (s)) +\nn\nX\ni=1\n−αi log(αi)\n(23)\n=\nn\nX\ni=1\nαiH(dπ∗\ni (s)) + H({α1, α2, . . . , αn}).\n(24)\nThis inequality shows that maximizing the entropy of each local policy H(dπ∗\ni (s)) will maximize the lower bound on\nH(dπ∗(s)), which effectively maximizes global state coverage.\nA.2. Proof of Lemma 3.2\nLemma (Restate of Lemma 3.2). The divergence between state distribution is bounded on the average divergence of policies\nˆπi and πi, as\nDTV\n\u0000dˆπi∥dπi\u0001\n≤\nγ\n1 −γ Es∼dπi\n\u0002\nDTV(ˆπi(·|s)∥πi(·|s))\n\u0003\n,\n(25)\nwhere DTV(·∥·) is the total variation distance.\nProof. In our proof, we consider finite MDPs, although we can apply the divergence minimizing algorithm for large-scale\nMDPs. We recall the definition of discounted future state distribution as follows,\ndπ(s) = (1 −γ)\n∞\nX\nt=0\nγtPt\nπ(s).\n(26)\nWe take a vector form of dπ in a given state set S, then Pt\nπ ∈R|S| denotes a vector with components Pt\nπ(s) = P(st = s|π).\nFurther, we denote Pπ ∈R|S|×|S| as the transition matrix from s to s′ with components Pπ(s′|s) =\nR\nP(s′|s, a)π(a|s)da.\nThen we have\nPt\nπ = PπPt−1\nπ\n= (Pπ)2Pt−2\nπ\n= . . . = (Pπ)tµ,\n(27)\nwhere the µ ∈R|S| is the initial state distribution. Then we can derive the vector form of state distribution as\ndπ = (1 −γ)\n∞\nX\nt=0\n(γPπ)tµ = (1 −γ)(I −γPπ)−1µ.\n(28)\nThen we build the relationship between dˆπi and dπi, we have\ndˆπi −dπi = (1 −γ)\n\u0000(I −γPˆπi)−1 −(I −γPπi)−1\u0001\nµ\n(29)\nIn the following, we denote ˆG ≜(I −γPˆπi)−1 and G ≜(I −γPπi)−1, then we have\nˆG −G = ˆG(G−1 −ˆG−1)G = ˆG(I −γPπi −I −γPˆπi)G\n= γ ˆG(Pˆπi −Pπi)G.\n(30)\n15\nConstrained Ensemble Exploration for Unsupervised Skill Discovery\nPlugging (30) into (29), we obtain\ndˆπi −dπi = γ ˆG(Pˆπi −Pπi)(1 −γ)Gµ = γ ˆG(Pˆπi −Pπi)dπi.\n(31)\nwhere dπi = (1 −γ)Gµ. Then, we can bound the L1-norm of dˆπi −dπi as\n∥dˆπi −dπi∥1 = γ∥ˆG(Pˆπi −Pπi)dπi∥1\n≤γ∥ˆG∥1∥(Pˆπi −Pπi)dπi∥1.\n(32)\nIn (32), the first term ∥ˆG∥1 is bounded by\n\r\r ˆG\n\r\r\n1 =\n\r\r(I −γPˆπi)−1\r\r\n1 ≤\n∞\nX\nt=0\nγt ∥Pˆπi∥t\n1 =\n1\n1 −γ .\n(33)\nThe second term ∥(Pˆπi −Pπi)dπi∥1 of (32) is bounded by\n∥(Pˆπi −Pπi)dπi∥1 =\nX\ns′\n\f\f\f\f\f\nX\ns\n\u0000Pˆπi(s′|s) −Pπi(s′|s)\n\u0001\ndπi(s)\n\f\f\f\f\f ≤\nX\ns,s′\n\f\f\f\n\u0010\nPˆπi(s′|s) −Pπi(s′|s)\n\u0011\f\f\f dπi(s)\n=\nX\ns,s′\n\f\f\f\f\f\nX\na\nP(s′|s, a) (ˆπi(a|s) −πi(a|s))\n\f\f\f\f\f dπi(s) ≤\nX\ns,a,s′\nP(s′|s, a)\n\f\f\fˆπi(a|s) −πi(a|s)\n\f\f\fdπi(s)\n≤\nX\ns,a\n\f\f\fˆπi(a|s) −πi(a|s)\n\f\f\fdπi(s) = 2Es∼dπi [DT V (ˆπi∥πi)[s]] .\n(34)\nThen, we obtain\nDTV\n\u0000dˆπi∥dπi\u0001\n= 1\n2∥dˆπi −dπi∥1 ≤1\n2γ\n1\n1 −γ 2Es∼dπi [DT V (ˆπi∥πi)[s]]\n=\nγ\n1 −γ Es∼dπi [DT V (ˆπi∥πi)[s]] .\n(35)\nA.3. Proof of Theorem 3.3\nTheorem (Restate of Theorem 3.3). Assuming the distance between state distribution is bounded by DTV\n\u0000dˆπi∥dπi\u0001\n≤δ,\nthe entropy difference between state distribution can be bounded by\n\f\fH(dˆπi) −H(dπi)\n\f\f ≤δ log\n\u0000|Spe\ni | −1\n\u0001\n+ h(δ).\n(36)\nwhere h(x) := −x log(x) −(1 −x) log(1 −x) is the binary entropy function.\nProof. We first introduce two random variables X and Y on the state space ¯S = Spe\ni , such that X follows from the\ndistribution dˆπi, while Y follows from the distribution dπ\ni . Next, we use a coupling technique to construct a joint probability\ndistribution of X and Y , denoted by gXY ∈∆( ¯S × ¯S), such that:\n1. The marginal distributions of gXY , denoted by gX and gY , satisfy gX = dˆπi and gY = dπi respectively. More\nprecisely, we have\ngX(s) :=\nX\ns′∈S\ngXY (s, s′) = dˆπi(s),\n∀s ∈¯S,\n(37)\ngY (s) :=\nX\ns′∈S\ngXY (s′, s) = dπi(s),\n∀s ∈¯S.\n(38)\n2. For all s ∈¯S, gXY (s, s) = min{dˆπi(s), dπi(s)}.\n16\nConstrained Ensemble Exploration for Unsupervised Skill Discovery\nFor s, s′ ∈¯S such that s ̸= s′, we can choose the value of gXY (s, s′) arbitrarily, as long as the conditions in (37) and (38)\nare satisfied. Based on this joint probability distribution gXY , we can calculate the probability that X does not equal Y :\nPr(X ̸= Y ) = 1 −\nX\ns∈S\ngXY (s, s)\n= 1 −\nX\ns∈¯\nS\nmin{dˆπi(s), dπi(s)}\n= 1\n2\n\nX\ns∈¯\nS\ndˆπi(s) + dπi(s) −2 min{dˆπi(s), dπi(s)}\n\n\n= 1\n2\nX\ns∈¯\nS\n|dˆπi(s) −dπi(s)|\n= DTV(dˆπi∥dπi).\n(39)\nSince the random variable X follows from the distribution dˆπi, the entropy of X, denoted by H(X), is equivalent to the\nentropy H(dˆπi). Similarly, the entropy H(Y ) is equivalent to H(dπi). Using standard information-theoretic inequalities,\nwe have\n(\f\fH\n\u0000dˆπi\u0001\n−H\n\u0000dπi\u0001\f\f = H(X) −H(Y ) ≤H(X) −I(X; Y ) = H(X|Y ),\nif H(X) ≥H(Y );\n\f\fH\n\u0000dˆπi\u0001\n−H\n\u0000dπi\u0001\f\f = H(Y ) −H(X) ≤H(Y ) −I(X; Y ) = H(Y |X),\nif H(X) < H(Y );\n(40)\nwhere I(X; Y ) is the mutual information of X and Y (with respect to the joint probability distribution gXY ), and H(X|Y )\nand H(Y |X) denote the conditional entropy. Applying Fano’s inequality yields that\nH(X|Y ) ≤Pr(X ̸= Y ) · log(| ¯S| −1) + h(Pr(X ̸= Y )),\n(41)\nH(Y |X) ≤Pr(X ̸= Y ) · log(| ¯S| −1) + h(Pr(X ̸= Y )).\n(42)\nCombining Eqns. (39)-(42), we eventually obtain that\n|H(dˆπi) −H(dπi)| ≤DTV\n\u0000dˆπi∥dπi\u0001\n· log(| ¯S| −1) + h(DTV\n\u0000dˆπi∥dπi)\n\u0001\n,\n(43)\nwhich concludes our proof under the assumption that DTV(dˆπi∥dπi) ≤δ.\nTheorem 3.3 shows that if the distance between two probability distributions is bounded, then their entropy difference can\nalso be bounded. A similar proof of Theorem 3.3 was first appeared in Zhang (2007) and Csisz´ar & K¨orner (2011) (Ex 3.10).\nThe proof relies on a coupling technique (used to relate two random variables), standard information-theoretic inequalities\nin Cover (2006) (Sec 2.3), and Fano’s inequality in Cover (2006) (Sec 2.10).\n17\nConstrained Ensemble Exploration for Unsupervised Skill Discovery\nB. Additional Experiments in Maze\nB.1. Tree Map\nWe conducted an additional experiment on a Tree-like map. This map is more challenging since the agent needs to explore\nthe deepest branches to maximize the state coverage. According to Figure 8, empowerment-based methods can learn\ndistinguishable skills while having limited exploration ability in the tree map. In contrast, CIC obtains a well global state\ncoverage while the trajectories of different skills are indistinguishable. CeSD can also reach the deepest branches and\novercome the limitations of CIC. Specifically, CeSD generates distinguishable skills, where the different skills perform\nindependent exploration and have fewer overlapping visitation areas.\nCeSD (ours)\nDADS\nDIAYN\nCIC\nFigure 8. The visualization of skill discovery methods in a Tree-like maze. Different colors represent the state trajectories with different\nskill vectors. We let the agent start moving from the black dot in the upper corner and sample 20 trajectories for each skill for visualization.\nOur method can explore the deepest branches and also learn distinguishable skills.\nB.2. The Comparison of MI and Entropy Estimation\nWe compare the mutual information (MI) between states and skills (i.e., I(S; Z)) and the state entropy (i.e., H(dπ(s))) of\nthe final policies in different methods, where the dπ(s) is estimated by generating trajectories from all skills {πi}i∈[n].\nTo estimate the MI term, we generate several trajectories for each learned skill and perform MI estimation using the\nMINE (Belghazi et al., 2018) estimator. MINE adopts a score function T : S × Z →R represented by a neural network\nin estimation. The joint samples come from the joint distribution (s, z) ∼PS,Z, where the states are generated by the\ncorresponding skills. ¯s ∼dπ\ns and ¯z ∼PZ are sampled from the corresponding marginal distribution. Then the MINE\nestimation given as: supT ∈F EPS,Z[T(s, z)]−log(EPS⊗PZ[eT (s,z)]), where F is the function class. In addition, we perform\nentropy estimation by using the particle-based entropy estimator (Liu & Abbeel, 2021b), which is the same as in our method.\nAs shown in Figure 9, CIC obtains much lower MI than other skill discovery methods but obtains the largest state entropy.\nCeSD can balance state coverage and empowerment via partition exploration and distribution constraints, which leads to\ndiverse skills and also has better state coverage than previous skill discovery algorithms.\nFigure 9. The qualitative result for the mutual information estimation and the entropy estimation in maze.\n18\nConstrained Ensemble Exploration for Unsupervised Skill Discovery\nC. Additional Experiments in URLB\nC.1. Implementation Details and Hyperparameters\nWe introduce the implementation details of the proposed CeSD algorithm as follows. (i) Skills. In the pertaining stage, a\nskill vector zi is sampled from a n-dimensional discrete distribution following a uniform distribution every fixed time-steps.\nThe agent interacts with the environment based on π(a|s, zi), and the obtained transition (s, a, r, s′, zi) is stored in a replay\nbuffer. We use n = 16 skills in all tasks. (ii) Clustering. In training, we sample a batch of transitions {(s, a, r, s′, zi)}\nfrom the replay buffer and perform clustering for the states based on the prototypes. The encoder network fθ(s) of states\nis an MLP network with obs dim →1024 →1024 →1024 architecture and ReLU activations. The output of fθ(s)\nis the same dimensions as the prototype ci ∈Rm, where we use m = 16 in experiments. We perform a coarse search\nfor prototype updates per iteration for each domain from {4, 5, 6}. The temperature value in Eq. (3) is set to τ = 0.1.\nThe prototypes are trained using stochastic gradient optimization with Adam with a learning rate of 10−4. (iii) Entropy\nestimation. We perform particle estimation on the feature space and k is set to 16. The entropy estimation is performed\nin each cluster independently based on the prototypes. (iv) Ensemble value functions. Each Q-network is a MLP with\nobs dim + action dim →512 →1024 →1024 →1. In practice, we use the vectorized linear layers in critic for parallel\ninference of the ensemble Q-network. The ensemble size of the critic is the same as the number of skills. We denote the\nensemble Q-value for a batch with b samples as Q ∈Rn×b. Then we adopt a mask matrix M ∈Rn×b where each column is\na one-hot vector [0, . . . , 1, . . . , 0] that denotes the cluster index of this transition by following p(t). Then the masked value\nfunction is calculated by Q ⊙M for the TD-error calculation. (v) Policy learning. The policy network is an MLP with\nobs dim + skill dim →50 →1024 →1024 →action dim architecture, where we use the same actor architecture for\nCeSD and other baselines. (vi) Intrinsic reward. For practical implementation, the state set Spe and Sclu used in intrinsic\nreward is calculated in batches rather than all collected states. We use the batch size of 1024 in all methods.\nWe adopt DDPG as the basic algorithm in policy training for all baselines. Table 1 summarizes the hyperparameters of our\nmethod and the basic DDPG algorithm. We refer to our released code for the details.\nTable 1. Hyper-parameters for CeSD and the basic DDPG algorithm for all methods.\nBeCL hyper-parameter\nValue\nskill dim / ensemble size n\n16 discrete\nprototype dim m\n16\nprototype update iterations in clustering\n{4, 5, 6}\ntemperature κ for clustering\n0.1\nk-nearest-neighbor in particle estimation\n16\nskill sampling frequency (steps)\n50\nDDPG hyper-parameter\nValue\nreplay buffer capacity\n106\naction repeat\n1\nseed frames\n4000\nn-step returns\n3\nmini-batch size\n1024\nseed frames\n4000\ndiscount (γ)\n0.99\noptimizer\nAdam\nlearning rate\n10−4\nagent update frequency\n2\ncritic target EMA rate (τQ)\n0.01\nfeatures dim.\n1024\nhidden dim.\n1024\nexploration stddev clip\n0.3\nexploration stddev value\n0.2\nnumber of pre-training frames\n2 × 106\nnumber of fine-tuning frames\n1 × 105\n19\nConstrained Ensemble Exploration for Unsupervised Skill Discovery\nC.2. Algorithmic Description\nWe give algorithmic descriptions of the pretraining and finetuning stages in Algorithm 1 and Algorithm 2, respectively. We\nevaluate the adaptation efficiency of BeCL following the pretraining and finetuning procedures in URLB. Specifically, in the\npretraining stage, latent skill z is changed and sampled from a discrete distribution p(z) in every fixed step and the agent\ninteracts with the environments based on πθ(a|s, z). In the finetuning stage, a skill is randomly sampled and keep fixed in\nall steps. The actor and critic are updated by extrinsic reward after first 4000 steps.\nIn our experiments of the Walker domain, pretraining one seed of CeSD for 2M steps takes about 11 hours while fine-tuning\ndownstream tasks for 100k steps takes about 20 minutes with a single 4090 GPU.\nAlgorithm 1 Unsupervised Pretraining of CeSD\nInput: number of pretraining frames NP T , skill dimension n, batch size N, and skill sampling frequency Nupdate.\nInitialize the environment, random actor πψ(a|s, z), ensemble Q-network {Qϕi(s, a)} and target network {Qϕ′\ni(s, a)},\nstate encoder fθ, the prototype vectors {c1, . . . , cn}, and replay buffer D\nfor t = 1 to NP T do\nRandomly choose zi from category distribution p(z) every Nupdate steps.\nInteract with environment τzi ∼πψ(a|s, zi), p(s′|s, a) and store the transitions to buffer D.\nif t ≥t0 then\nSample a batch a transitions from D : {(si, ai, s′\ni, zi)}i∈[N].\nCalculate p(t) for each transitions based on prototypes via Eq. (3) and obtains the cluster index as {ˆzi}i∈[N].\nPerform particle estimation in each cluster and calculate the entropy-based intrinsic reward {rcesd\ni\n}i∈[N].\nCalculate the constraint reward {rreg\ni\n}i∈[N] based on the cluster index {ˆzi} (i.e., Sclu) and skill label {zi} (i.e., Spe).\nCalculate mask matrix M and update the ensemble Q-network with {(si, ai, s′\ni, zi)}i∈[N] and {rcesd\ni\n+α·rreg\ni\n}i∈[N].\nUpdate the policy network πψ(a|s, zi) by maximizing the corresponding critic function Qϕi(s, a).\nend if\nend for\nAlgorithm 2 Downstream Finetuning of CeSD\nInput: actor πψ(a|s, z) and critic Qϕ(s, a) with weights from the pretraining phase, randomly sampled a skill vector z⋆\nfrom p(z), and the number of finetuning frames NF T batch size N. Initialized environment and replay buffer D.\nfor t = 1 to NF T do\nChoose the action by at ∼πθ(a|st, z⋆).\nInteract with environment to obtain st+1, rt with extrinsic reward from downstream task.\nStore (st, at, st+1, rt, z⋆) into buffer D.\nif t ≥4, 000 then\nSample a batch {(a(i), s(i), s′(i), r(i), z(i))}N\ni=1 from the replay buffer D.\nUpdate actor πθ(a|s, z⋆) and critic Qψ(s, a, z⋆) using extrinsic reward r in Eq. (1) and Eq. (2).\nend if\nend for\nC.3. Description of Baselines\nA comparison of intrinsic rewards and representations used in unsupervised RL baselines is summarized in Table 2.\nAccording to the taxonomy in URLB (Laskin et al., 2021), (i) the knowledge-based baselines adopt curiosity measurements\nby training an encoder to predict the dynamics, and use the prediction-error of next-state (e.g., ICM (Pathak et al., 2017)),\nprediction variance (e.g., Disagreement (Pathak et al., 2019)), or the divergence between a random network prediction\n(e.g., RND (Burda et al., 2019)) as intrinsic rewards; (ii) the data-based or entropy-based methods estimate the state\nentropy via particle estimation and use the state entropy estimation as the intrinsic reward in exploration, including\nas APT (Liu & Abbeel, 2021b), ProtoRL (Yarats et al., 2021) and CIC (Laskin et al., 2022); (iii) the competence-\nbased or empowerment-based baselines aim to learn latent skill z by maximizing the MI between states and skills:\nI(S; Z) = H(S) −H(S|Z) = H(Z) −H(Z|S). The different methods adopt various variational forms in estimating the\nMI term, including the forward form in APS (Liu & Abbeel, 2021a) and DADS (Sharma et al., 2020), and the reverse form\nin DIAYN (Eysenbach et al., 2019). BeCL (Yang et al., 2023) is also a competence-based method and adopts a multi-view\nperspective and maximizes the MI term I(S(1); S(2)), where S(1) and S(2) are generated by the same skill.\n20\nConstrained Ensemble Exploration for Unsupervised Skill Discovery\nWe adopt the baselines of open source code implemented by URLB (https://github.com/rll-research/\nurl_benchmark), CIC (https://github.com/rll-research/cic), and BeCL (https://github.com/\nRooshy-yang/BeCL). CeSD can be considered as a data-based method, but also has the advantages of competence-based\nmethods in learning diverse skills. We adopt partition exploration with clusters to learn distinguishable skills without MI\nestimation. More descriptions of the baselines can be found in URLB (Laskin et al., 2021).\nTable 2. Summary of baseline methods.\nName\nAlgo. Type\nIntrinsic Reward\nExplicit max H(s)\nICM (Pathak et al., 2017)\nKnowledge\n∥f(st+1|st, at) −st+1∥2\nNo\nDisagreement (Pathak et al., 2019)\nKnowledge\nVar{fi(st+1|st, at)}\ni = 1, . . . , N\nNo\nRND (Burda et al., 2019)\nKnowledge\n∥f(st, at) −˜f(st, at)∥2\n2\nNo\nAPT (Liu & Abbeel, 2021b)\nData\nP\nj∈KNN log ∥f(st) −f(sj)∥\nf ∈random or ICM\nYes\nProtoRL (Yarats et al., 2021)\nData\nP\nj∈KNN log ∥f(st) −f(sj)∥\nf ∈prototypes\nYes\nCIC (Laskin et al., 2022)\nData 1\nP\nj∈KNN log ∥f(st, s′\nt) −f(sj, s′\nj)∥\nf ∈contrastive\nYes\nSMM (Lee et al., 2019)\nCompetence\nlog p∗(s) −log qz(s) −log p(z) + log d(z|s)\nYes\nDIAYN (Eysenbach et al., 2019)\nCompetence\nlog q(z|s) −log p(z)\nNo\nAPS (Liu & Abbeel, 2021a)\nCompetence\nrAPT\nt\n(s) + log q(s|z)\nYes\nBeCL (Yang et al., 2023)\nCompetence\nexp(f(s(1)\nt )⊤f(s(2)\nt )/κ)/ P\nsj∼S−S s(2)\nt\nexp(f(sj)⊤f(s(1)\nt )/κ\nNo\nC.4. URLB Environments\nAn illustration of URLB tasks is given in Figure 10. There are three domains (i.e., Walker, Quadruped, and Jaco), and each\ndomain has four different downstream tasks. The environment is based on DMC (Tassa et al., 2018). The episode lengths\nfor the Walker and Quadruped domains are set to 1000, and the episode length for the Joco domain is set to 250, which\nresults in the maximum episodic reward for the Walker and Quadruped domains being 1000, and for Jaco Arm being 250.\nReach Bottom Left\nReach Bottom Right\nReach Top Left\nReach Top Right\nJaco Arm\nRun\nJump\nStand\nWalk\nQuadruped\nWalker\nRun\nFlip\nStand\nWalk\nFigure 10. Illustration of domains and downstream tasks in URLB (Laskin et al., 2021). Each domain has four downstream tasks.\nC.5. Visualization of Skills\nAs shown in Figure 11, Figure 12, and Figure 13, we give more visualization results of DMC domains. CeSD can learn\nvarious locomotion skills, including standing, walking, rolling, moving, somersault, and jumping in Walker and Quadruped\ndomains; and also learns various manipulation skills by moving the arm to explore different areas, opening and closing the\ngripper in different locations in Jaco domain. The learned meaningful skills lead to superior generalization performance in\nthe fine-tuning stage of various downstream tasks.\n1The newest NeurIPS version of CIC https://openreview.net/forum?id=9HBbWAsZxFt has two designs of intrinsic\nreward including the NCE term and KNN reward, which represent competence-base and data-based designs respectively. Since CIC\nobtains the best performance in URLB with KNN reward only and NCE is used to update representation, we use KNN reward as its\nintrinsic reward and consider it as a data-based method in this paper.\n21\nConstrained Ensemble Exploration for Unsupervised Skill Discovery\nWalker Stand from Falling\nWalker Push the legs backwards to move forward\nWalker turn a somersault forward\nWalker turn a somersault backward\nFigure 11. Visualization of representative skills learned of CeSD in the Walker domain. The Walker agent learns some interesting skills\nlike standing and moving. The agent also learns highly difficult skills that turn a somersault forward and backward.\nQuadruped robot rolling its body to the right\nQuadruped robot jumping to the left\nQuadruped robot using two legs for side rolling\nQuadruped robot walking and jumping\nFigure 12. Visualization of representative skills learned of CeSD in the Quadruped domain. The Quadruped agent learns challenging\nskills like walking, rolling, and jumping that benefit downstream tasks. Also, a novel two-leg rolling skill is learned in pre-training.\nC.6. Numerical Results\nWe report the individual normalized return of different methods in state-based URLB after 2M steps of pretraining and 100k\nsteps of finetuning, as shown in Table 4. In the Quadruped and Jaco domains, BeCL obtains state-of-the-art performance in\ndownstream tasks. In the Walker domain, CeSD shows competitive performance against the leading baselines.\n22\nConstrained Ensemble Exploration for Unsupervised Skill Discovery\nJaco Arm reaching the right rear area and opening the gripper\nJaco Arm reaching the front area and opening the gripper\nJaco Arm grabbing objects by closing the gripper\nJaco Arm reaching the left area and grabbing objective\nFigure 13. The Jaco Arm agent learns various manipulation skills, including reaching different locations, which allows fast adaptation in\ndownstream tasks. The agent also learned to open and close grippers to manipulate objects.\nTable 3. Results of CeSD and other baselines on state-based URLB. All baselines are pre-trained for 2M steps with only intrinsic rewards\nin each domain and then finetuned to 100K steps in each downstream task by giving the extrinsic rewards. All baselines are run for 10\nseeds per task. The highest scores are highlighted.\nDomain\nTask\nDDPG\nICM\nDisagreement\nRND\nAPT\nProtoRL\nSMM\nDIAYN\nAPS\nCIC\nBeCL\nCeSD\nWalker\nFlip\n538±27\n390±10\n332±7\n506±29\n606±30\n549±21\n500±28\n361±10\n448±36\n641±26\n611±18\n541±17\nRun\n325±25\n267±23\n243±14\n403±16\n384±31\n370±22\n395±18\n184±23\n176±18\n450±19\n387±22\n337±19\nStand\n899±23\n836±34\n760±24\n901±19\n921±15\n896±20\n886±18\n789±48\n702±67\n959±2\n952±2\n960±3\nWalk\n748±47\n696±46\n606±51\n783±35\n784±52\n836±25\n792±42\n450±37\n547±38\n903±21\n883±34\n834±34\nQuadruped\nJump\n236±48\n205±47\n510±28\n626±23\n416±54\n573±40\n167±30\n498±45\n389±72\n565±44\n727±15\n755±14\nRun\n157±31\n125±32\n357±24\n439±7\n303±30\n324±26\n142±28\n347±47\n201±40\n445±36\n535±13\n586±25\nStand\n392±73\n260±45\n579±64\n839±25\n582±67\n625±76\n266±48\n718±81\n435±68\n700±55\n875±33\n919±11\nWalk\n229±57\n153±42\n386±51\n517±41\n582±67\n494±64\n154±36\n506±66\n385±76\n621±69\n743±68\n889±23\nJaco\nRe. bottom left\n72±22\n88±14\n117±9\n102±9\n143±12\n118±7\n45±7\n20±5\n84±5\n154±6\n148±13\n208±5\nRe. bottom right\n117±18\n99±8\n122±5\n110±7\n138±15\n138±8\n60±4\n17±5\n94±8\n149±4\n139±14\n186±13\nRe. top left\n116±22\n80±13\n121±14\n88±13\n137±20\n134±7\n39±5\n12±5\n74±10\n149±10\n125±10\n215±4\nRe. top right\n94±18\n106±14\n128±11\n99±5\n170±7\n140±9\n32±4\n21±3\n83±11\n163±9\n126±10\n195±9\nD. More Discussions\nD.1. Difference to Mixture-of-Expert (MoE) (Celik et al., 2022)\nThe fundamental difference is the problem setting. We focus on unsupervised skill discovery, aiming to learn distinguishable\nskills without extrinsic reward and task structure information, for efficiently solving downstream tasks via finetuning skills.\nIn contrast, the MoE work addresses learning skills in the context-conditioned tasks with extrinsic reward, where different\ntasks are represented by different contexts c. Correspondingly, the learned MoE model depends on the context for skill\ninference (i.e., π(θ|c) = P\no∈O π(o|c)π(θ|o, c), where o represents skills/components). Furthermore, the downstream task\nprovides explicit context for the algorithm, which makes the method less general. Thus, the MoE method may not be\ndeployed directly to the unsupervised skill discovery as far as we know; we can only receive the states from the environment\nand encourage diverse behaviors via some self-proposed objective (such as r = log(z|s) from DIAYN).\nSecond, the details of the method are quite different. As for maximizing state coverage, we propose portioned exploration\nto encourage local skill exploration, while the MoE algorithm uses policy entropy (i.e., H(π(θ|o, c))), which is common\n23\nConstrained Ensemble Exploration for Unsupervised Skill Discovery\nTable 4. Result comparison of MoE methods.\nTask\nCeSD+MoE (Finetune skill)\nCeSD+MoE (Freeze skill)\nCeSD\nwalker stand\n341 ± 16\n339 ± 48\n960 ± 3\nwalker run\n75 ± 4\n71 ± 8\n337 ± 19\nwalker walk\n157 ± 9\n159 ± 13\n834 ± 34\nwalker flip\n197 ± 8\n200 ± 13\n541 ± 17\nquadruped stand\n627 ± 203\n532 ± 101\n919 ± 11\nquadruped jump\n480 ± 147\n361 ± 151\n755 ± 14\nquadruped run\n327 ± 107\n297 ± 60\n586 ± 25\nquadruped walk\n295 ± 158\n255 ± 70\n889 ± 23\nin RL research. As for distinguishing between skills, we propose the clustering-based technique. In contrast, the MoE\nalgorithm does not introduce the technique to explicitly encourage skill/component diversity as we know. Given the\ncontext-conditioned task (e.g., the context c defines the target position in table tennis) and the context-conditioned extrinsic\nreward function r(s, a, c), the components/skills can naturally derive distinguishable behaviors under the guidance of the\ncontext. Imagine a simple case, we train multiple skill networks, and each skill is trained to maximize its own context-based\nreward function (i.e., π∗\ni = arg max Eπi[P∞\nt=0 r(s, a, ci)]), the trained skill will obtain distinguishable behaviors finally\n(e.g., different skill plays table tennis towards different target positions).\nD.2. Calculation of Eq. (9)\nThe size of |Spe\ni\n−Sclu\ni | is easy to calculate since the two state-sets are mostly overlapped. In clustering, for state s ∈Spe\ni\ncollected by the skill policy πi, (i) if s is collected in the previous rounds, we have the cluster label unchanged (i.e.,\n(s, a, s′) ∈Sclu\ni ) since the Sinkhorn-Knopp cluster algorithm will keep the cluster-index of existing states fixed; and (ii) if\n(s, a, s′) is the newly collected one in the current round, it may be assigned to cluster i or other clusters (e.g., j) according to\n{f(s)⊤cj}j∈[n]. Then we use r = 1/(|Spe\ni\n−Sclu\ni | + λ) as the rewards to force πi to reduce the visitation probability of\nstates lied in clusters of other skills. In implementation, we give each transition (s, a, s′) two skill labels (i.e., zpe and zclu).\nSpecifically, zpe signifies the transition (s, a, s′) is collected by which skill policy in exploration, and zclu is determined by\nthe clustering index of Sinkhorn-Knopp algorithm.\nD.3. Non-overlapping Property\nThe non-overlapping property of skills is not a hard constraint in our method but a soft one with a tolerance value. In Sec.\n3.3, we define a desired policy ˆπi based on the skill policy πi, where dˆπi(s) = 0 for overlapping states between clusters.\nThen our constraint for regularizing skill πi is defined as Lreg(πi) = DTV(dˆπi∥dπi). In practice, we adopt a heuristic\nintrinsic reward to prevent the policy πi from visiting states in Spe\ni −Sclu\ni , as rreg\ni\n= 1/(|Spe\ni −Sclu\ni | + λ), which we assume\nto make the TV-distance between state distributions bounded by DTV\n\u0000dˆπi|dπi\u0001\n≤δ, where δ is a tolerance value. In our\npaper, Theorem 3.3 and Corollary 3.4 hold with such a tolerance value. Some special cases exist in which each skill policy\nmust visit some bottleneck states. In these cases, the regularization reward rreg\ni\n= 1/(|Spe\ni\n−Sclu\ni | + λ) ≤1/(c + λ),\nwhere c is the number of bottleneck states. The reward rreg\ni\nwill become small if c is very large, which can be alleviated by\nremoving this constant or increasing the weight of rreg\ni\nin policy updating.\nD.4. Pixel-based URLB\nAccording to Pixel-URLB (Rajeswar et al., 2023), which evaluates the unsupervised RL algorithms on pixel-based URLB,\nthe performance in pixel-based URLB depends heavily on the basic RL algorithm. Specifically, according to Figure 1 of\nRajeswar et al. (2023), all unsupervised RL algorithms perform poorly when combined with a model-free method (e.g.,\nDrQv2), while they perform much better when using a model-based algorithm (e.g., Dreamer) as the backbone. APT obtains\nthe best performance in the challenging Quadruped domain compared to other methods. Following the official code of [1],\nwe re-implement CeSD with the Dreamer backbone. We compare CeSD-Dreamer and APT-Dreamer in the following table.\nThe result shows our method outperforms APT in the pixel-based domain on average.\n24\nConstrained Ensemble Exploration for Unsupervised Skill Discovery\nTable 5. Results comparison of Pixel-based URLB methods.\nPixel-based Task\nCeSD Dreamer\nAPT Dreamer\nquadruped jump\n756.5 ± 60\n584.6 ± 1\nquadruped run\n445.7 ± 23\n428.2 ± 21\nquadruped stand\n864.9 ± 1\n914.7 ± 7\nquadruped walk\n581.5 ± 129\n473.7 ± 27\nD.5. Discrete/Continuous Skill Space\nAlthough infinite skills (in continuous space) seem to be a better choice, infinite skills do not always lead to better\nperformance than discrete ones. As shown in Fig. 3, DADS have a continuous skill space while the resulting state coverage\nis limited. As for the DMC tasks, the baseline methods, including APS, DADS, and CIC, also have a continuous skill space.\nActually, learning infinite skills with diverse and meaningful behaviors is desirable, while it can be difficult for existing skill\ndiscovery methods. In our method, since we adopt partition exploration based on Sinkhorn-Knopp clustering, the cluster\nnumber is required to be finite to partition the state space, and each state should be assigned to a specific cluster.\nD.6. Additional Comparison to Re-Implemented Baselines\nThe recently proposed Metra (Park et al., 2024) uses Wasserstein dependency to measure (WDM) between states and skills,\ni.e., IW (S, Z), for skill discovery. Metra also contains experiments in URLB benchmark while it only reports the skill\npolicy’s coverage (see Fig. 5 of Park et al. (2024)), and the downstream tasks are specifically designed to reach a target\ngoal (see Appendix F.1 of Park et al. (2024)) rather than diverse task adaptation considered in our paper. As a result, we\nuse the official Metra code and carefully modify the goal adaptation process to evaluate the adaptation of various DMC\ntasks. We also add new baselines, including LSD (Park et al., 2022) and CSD (Park et al., 2023). Since LSD/CSD are\nevaluated on different benchmarks in their original papers, we have tried our best to re-implement LSD/CSD in URLB tasks\nbased on the official code. A comparison of the results is given in the following table. We find out that the method obtains\ncompetitive performance compared to CSD and Metra in the Walker domain and significantly outperforms other methods in\nthe Quadruped domain.\nTable 6. Results comparison to re-implemented baselines.\nTask\nLSD\nCSD\nMetra\nCeSD\nwalker flip\n223 ± 6\n602 ± 11\n589 ± 75\n541 ± 17\nwalker run\n130 ± 22\n457 ± 50\n361 ± 45\n337 ± 19\nwalker stand\n837 ± 3\n942 ± 8\n943 ± 13\n960 ± 3\nwalker walk\n323 ± 75\n802 ± 85\n850 ± 63\n834 ± 34\nquadruped jump\n247 ± 54\n520 ± 80\n224 ± 17\n775 ± 14\nquadruped run\n270 ± 55\n329 ± 62\n196 ± 34\n586 ± 25\nquadruped stand\n426 ± 131\n425 ± 120\n324 ± 173\n919 ± 11\nquadruped walk\n256 ± 83\n353 ± 142\n190 ± 44\n889 ± 23\n25\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2024-05-25",
  "updated": "2024-05-25"
}