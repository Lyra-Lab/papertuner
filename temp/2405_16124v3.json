{
  "id": "http://arxiv.org/abs/2405.16124v3",
  "title": "Unsupervised Meta-Learning via In-Context Learning",
  "authors": [
    "Anna Vettoruzzo",
    "Lorenzo Braccaioli",
    "Joaquin Vanschoren",
    "Marlena Nowaczyk"
  ],
  "abstract": "Unsupervised meta-learning aims to learn feature representations from\nunsupervised datasets that can transfer to downstream tasks with limited\nlabeled data. In this paper, we propose a novel approach to unsupervised\nmeta-learning that leverages the generalization abilities of in-context\nlearning observed in transformer architectures. Our method reframes\nmeta-learning as a sequence modeling problem, enabling the transformer encoder\nto learn task context from support images and utilize it to predict query\nimages. At the core of our approach lies the creation of diverse tasks\ngenerated using a combination of data augmentations and a mixing strategy that\nchallenges the model during training while fostering generalization to unseen\ntasks at test time. Experimental results on benchmark datasets showcase the\nsuperiority of our approach over existing unsupervised meta-learning baselines,\nestablishing it as the new state-of-the-art. Remarkably, our method achieves\ncompetitive results with supervised and self-supervised approaches,\nunderscoring its efficacy in leveraging generalization over memorization.",
  "text": "Published as a conference paper at ICLR 2025\nUNSUPERVISED META-LEARNING VIA IN-CONTEXT\nLEARNING\nAnna Vettoruzzoâˆ—\nHalmstad University, Sweden\nanna.vettoruzzo@hh.se\nLorenzo Braccaioliâˆ—\nUniversity of Trento, Italy\nlorenzo.braccaioli@unitn.it\nJoaquin Vanschoren\nEindhoven University of Technology, Netherlands\nj.vanschoren@tue.nl\nMarlena Nowaczyk\nHalmstad University, Sweden\nmarlena17nowaczyk@gmail.com\nABSTRACT\nUnsupervised meta-learning aims to learn feature representations from unsuper-\nvised datasets that can transfer to downstream tasks with limited labeled data. In\nthis paper, we propose a novel approach to unsupervised meta-learning that lever-\nages the generalization abilities of in-context learning observed in transformer ar-\nchitectures. Our method reframes meta-learning as a sequence modeling problem,\nenabling the transformer encoder to learn task context from support images and\nutilize it to predict query images. At the core of our approach lies the creation of\ndiverse tasks generated using a combination of data augmentations and a mixing\nstrategy that challenges the model during training while fostering generalization to\nunseen tasks at test time. Experimental results on benchmark datasets showcase\nthe superiority of our approach over existing unsupervised meta-learning base-\nlines, establishing it as the new state-of-the-art. Remarkably, our method achieves\ncompetitive results with supervised and self-supervised approaches, underscoring\nits efficacy in leveraging generalization over memorization.\n1\nINTRODUCTION\nMeta-learning, or learning-to-learn, enables models to accumulate knowledge from multiple tasks,\nallowing rapid adaptation and generalization to new tasks (Vettoruzzo et al., 2024; Vanschoren,\n2019). Traditional meta-learning approaches typically rely on labeled data to construct tasks during\nmeta-training. However, collecting large labeled datasets in real-world applications is challenging\nand often impractical. Unsupervised meta-learning (UML) methods address this issue by leveraging\nunlabeled data to learn transferable feature representations, enabling adaptation to new tasks with\nlimited labeled data (Vettoruzzo et al., 2024).\nVarious approaches have been proposed to address the UML problem (Hsu et al., 2018; Jang et al.,\n2022; Khodadadeh et al., 2019; Kong et al., 2021; Lee et al., 2022; 2020). However, UML still faces\nseveral challenges. Existing UML methods often rely on simple data augmentations to construct\nthe training tasks, while following the standard meta-learning task sampling pipeline for evaluation.\nThis results in a significant difference between training and testing tasks, limiting generalization and\noften requiring fine-tuning on the test domain. Furthermore, existing UML approaches typically\nassume that the training and test datasets belong to the same domain. In our framework, we loosen\nthis assumption resulting in a more challenging setting that necessitates a better model generalization\ncompared to usual meta-learning applications. We refer to this as the cross-domain scenario.\nIn this paper, we propose a novel approach to UML that addresses these challenges by leveraging\nin-context learning within a transformer architecture (Dong et al., 2022; Min et al., 2022). In-\ncontext learning allows the model to use the context provided by a sequence of input-output pairs\nto make predictions on new input data. Inspired by recent advancements in large language models\n(LLMs) (Wei et al., 2022; Brown et al., 2020; Liu et al., 2022), we formulate meta-learning as a\nâˆ—Equal contributions.\n1\narXiv:2405.16124v3  [cs.LG]  10 Feb 2025\nPublished as a conference paper at ICLR 2025\nğ‘¥ğ‘›,ğ‘˜\n(ğ‘ ğ‘) = ğ’œğ‘˜(ğ‘¥ğ‘›)\nğ‘¦ğ‘›,ğ‘˜\n(ğ‘ ğ‘) = ğ‘›\nSupport set\nQuery set\n. . .\nğ‘¥ğ‘—\n(ğ‘ğ‘Ÿ) = ğœ†ğ‘§ğ‘—+ 1 âˆ’ğœ†à·¤ğ‘¥ğ‘›,ğ‘—\nğ‘¦ğ‘—\n(ğ‘ğ‘Ÿ) = ğ‘›\nğ’Ÿğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›= {ğ‘¥ğ‘–}\nTransformer encoder ğ‘€ğœƒ\n. . .\nLinear\nà·ğ‘¦ğ‘—\n(ğ‘ğ‘Ÿ)\nLearned class encoder ğ‘”ğœ™\nFixed feature extractor ğ‘“ğœ“\n*\nmixup\naugment\nğ‘ \nsamples\nğ‘†ğ‘–,ğ‘—=\nğ‘“ğœ“ğ‘¥ğ‘—\nğ‘ğ‘Ÿ\n, ğ‘”ğœƒâˆ—\n,\nğ‘“ğœ“ğ‘¥ğ‘›,ğ‘˜\nğ‘ ğ‘\n, ğ‘”ğœ™ğ‘¦ğ‘›,ğ‘˜\nğ‘ ğ‘\nğ‘›,ğ‘˜\n \nFigure 1: Visualization of CAMeLU (with 3-way 5-shot tasks). The left side illustrates the task\ncreation mechanism, where N samples are drawn from an unlabeled dataset Dtrain. Each sample\nxn is augmented K times to obtain x(sp)\nn,k . A strategy inspired by mixup (Zhang et al., 2018) is utilized\nfor generating the query set by using an augmented version of xn, i.e., Ëœxn,j. The same pseudo-label\nn âˆˆ[1, N] is assigned to all data generated from the sample xn. On the right side, the so-created\ntask is fed into the transformer encoder for predicting the query input. Inspired by CAML (Fifty\net al., 2024), the transformer encoder processes demonstrations created by concatenating features\nfrom a fixed pre-trained feature extractor and a learned class encoder. The symbol âˆ—denotes the\nunknown query label that the transformer encoder aims to predict.\nsequence modeling problem, where a task is seen as a non-causal sequence of support images and\nan unknown query image. The support set is treated as the context utilized by the model to predict the\nclass of the query image. We call our approach CAMeLU, which stands for Context-Aware Meta-\nLearning in Unsupervised scenarios. Central to our approach is a novel task creation mechanism\nthat enables the generation of a large number of different tasks from an unlabeled dataset. Drawing\ninspiration from the natural decision process of learning by analogy (Winston, 1980), we construct\ntasks that closely resemble the structure of those encountered during inference. Specifically, we use a\ncombination of different data augmentation techniques based on basic image manipulations (Shorten\n& Khoshgoftaar, 2019) for generating the samples in the support set. Conversely, a strategy similar\nto mixup (Zhang et al., 2018) is employed to generate query images by combining a support element,\nafter applying a distinct augmentation function, and an image randomly sampled from the training\ndataset. This process ensures that the query contains sufficient information from the support image\nto be classified as the latter, while introducing diversity by blending them. Consequently, query\nimages appear distinct from their corresponding support images while still belonging to the same\nclass, better mimicking the tasks seen at test time and hence enhancing generalization. Following\ntask creation, support and query images are encoded using a fixed pre-trained feature extractor. The\nresulting latent representations are aggregated into a sequence and passed as input to a transformer\nencoder along with their label encodings. The transformer encoder learns to extract contextual\ninformation from support images and predict the query image in a single pass, eliminating the need\nfor the fine-tuning step during inference. An overview of our approach is visualized in Fig. 1.\nThroughout extensive experiments we demonstrate the effectiveness of the proposed approach to\ngeneralize to new tasks in real-time. Particularly, CAMeLU outperforms other UML baselines\nacross several datasets, establishing itself as the state-of-the-art in the field. It also achieves com-\nparable results to its supervised counterpart and to SSL approaches. While the latter requires fine-\ntuning on the test domain, CAMeLU obtains comparable performance with a single forward step,\nhighlighting its applicability to real-time applications. Furthermore, by recasting the meta-learning\nphase as in-context learning within a transformer architecture, we improve efficiency, ensuring the\nwhole training and inference phase can be executed with a consumer device with 8GB VRAM.\nThe main contributions of this paper are as follows:\nâ€¢ We introduce CAMeLU, a novel UML method that leverages in-context learning within a\ntransformer architecture, reframing meta-learning as a sequence modeling problem.\nâ€¢ We propose a novel task creation mechanism that generates diverse few-shot tasks from\nunlabeled datasets using a combination of data augmentations and a mixing strategy. This\n2\nPublished as a conference paper at ICLR 2025\nensures better alignment between training and testing tasks, thus improving generalization\nperformance.\nâ€¢ We demonstrate that CAMeLU outperforms existing UML baselines across five datasets,\nwithout the need for fine-tuning to the test domains.\nâ€¢ We investigate the ability of CAMeLU to generalize across various datasets, including\nthose significantly different from the training data.\n2\nRELATED WORK\nUnsupervised meta-learning.\nMeta-learning is a well-studied field in the machine learning com-\nmunity due to its ability to enable models to quickly adapt to tasks with limited labeled data. Pi-\noneering work in the field (Finn et al., 2017; Snell et al., 2017; Vinyals et al., 2016; Mishra et al.,\n2018; Sung et al., 2018) considers the scenarios where a large labeled dataset is available for meta-\ntraining, a challenging requirement in real-world applications. UML addresses this challenge by\nextracting meaningful information from unsupervised data that can be transferred to downstream\ntasks with limited labeled data. Different techniques have been explored in the literature to construct\ndiverse tasks. CACTUs (Hsu et al., 2018) applies clustering in the embedding space and assigns the\nsame pseudo-label to all images in the same cluster. Other methods focus on generating synthetic\nsamples, either using data augmentations, as in UMTRA (Khodadadeh et al., 2019), or leveraging\ninterpolation in the latent space of a generative model (Khodadadeh et al., 2020). Differently, Meta-\nGMVAE (Lee et al., 2020) and Meta-SVEBM (Kong et al., 2021) use variational autoencoders and\nmemory-based models for pseudo-label generation. Recent methodologies have also incorporated\nSSL techniques (Doersch et al., 2015) into UML methods. In particular, Set-SimCLR (Lee et al.,\n2022) builds on top of the SimCLR (Chen et al., 2020) approach and reframes meta-learning as a\nset-level problem, while PsCo (Jang et al., 2022), inspired by MoCo (He et al., 2020), utilizes a mo-\nmentum encoder and a queue of previous samples to improve pseudo-labeling and construct diverse\ntasks for UML applications. Similarly, BECLR (Poulakakis-Daktylidis & Jamali-Rad, 2024) intro-\nduces an approach for unsupervised few-shot learning by proposing a constrastive representation\nlearning framework, instead of meta-learning.\nData augmentation.\nSeveral UML approaches rely on data augmentation to construct the training\ntasks (Khodadadeh et al., 2019; Lee et al., 2022; Jang et al., 2022). However, traditional transforma-\ntions such as rotation, translation, cropping, resizing, and flipping (Shorten & Khoshgoftaar, 2019)\nmight generate images that are too similar to the original ones, ending up in tasks with low in-class\nvariability between the support and query images. This creates a problem when the model needs to\ngeneralize to test tasks, where the query data are different instances than the support ones, not only\naugmented versions of them. In this paper, we addressed this limitation by generating query images\nusing a strategy inspired by mixup (Zhang et al., 2018) to enhance model generalization. Similarly\nto mixup, which performs linear interpolation of the feature vectors at the pixel level, other strate-\ngies based on mixing images comprise CutMix (Yun et al., 2019), PatchMix (Liu et al., 2021), and\nManifold Mixup (Verma et al., 2019).\nIn-context learning.\nIn-context learning refers to the ability to perform a new task via inference\nalone by conditioning on a few input-output pairs and making predictions for new inputs (Dong\net al., 2022). Although typical of LLMs (Devlin et al., 2019; Radford et al., 2019; Touvron et al.,\n2023), this ability has also been explored in different fields, such as in-painting (Bar et al., 2022;\nZhang et al., 2024), image segmentation (Butoi et al., 2023), and notably meta-learning (Chan et al.,\n2022; Singh et al., 2024; Kirsch et al., 2022; Fifty et al., 2023; 2024; Min et al., 2022). Recent\nmethods, such as Chan et al. (2022) and Singh et al. (2024), examine the emergence of in-context\nlearning abilities from a data distribution perspective, extending these insights to images. GPICL\n(Kirsch et al., 2022) further demonstrates that transformers can be meta-trained as general-purpose\nin-context learners, while CAML (Fifty et al., 2024) adapts this concept to non-causal sequence\nmodeling problems. Building on these advancements, our work takes a different direction by tack-\nling the unsupervised meta-learning problem. Specifically, we introduce a novel task creation mech-\nanism that, together with an in-context learner, enables learning directly from an unlabeled dataset.\nThis approach differentiates our method from prior in-context learning techniques, aligning it with\nthe unique requirements of UML.\n3\nPublished as a conference paper at ICLR 2025\n3\nPROPOSED APPROACH\nOur proposed approach, Context-Aware Meta-Learning in Unsupervised scenarios (CAMeLU),\nleverages the in-context learning ability of transformers to address the challenges of UML. These\nchallenges include the need to construct meaningful tasks from unlabeled data and the requirement\nfor models to generalize effectively to new tasks during inference. CAMeLU consists of two phases\nthat are intertwined during the model training. Initially, tasks are automatically constructed from\nan unlabeled dataset utilizing a combination of two strategies. Subsequently, we reformulate the\nmeta-learning framework as a sequence modeling problem, aiming to harness the in-context learn-\ning capability of a transformer. This enables the model to extract context from the support sam-\nples and predict the unknown query samples without requiring any fine-tuning during the inference\nphase. The combination of these two phases is essential and guarantees good generalization per-\nformance without labeled information. Transformers excel at modeling dependencies and capturing\nrelationships between support and query samples, which is particularly beneficial in few-shot learn-\ning scenarios. The novel task creation mechanism complements this by constructing diverse and\nchallenging pseudo-tasks, effectively preparing the model for the complexities of target tasks. We\ndelve into the two phases in Sect. 3.1 and Sect. 3.2, respectively.\n3.1\nTASK CREATION\nCentral to our proposed approach is the task creation mechanism. In meta-learning, a task Ti corre-\nsponds to a data generating distribution Ti â‰œ{pi(x), pi(y|x)}, and consists of data from N distinct\nclasses. The data sampled from each task is divided into a support set, D(sp)\ni\n, containing K train-\ning examples per class, and a query set, D(qr)\ni\n. At meta-test time, only the support set D(sp)\nnew of a\ntask Tnew âˆ¼Dtest is labeled and used to fine-tune the model and make accurate predictions on the\nunlabeled query set. Contrary to supervised meta-learning, tasks in UML are only available at test\ntime, while a large unlabeled dataset Dtrain is available during training. The main goal is to extract\nprior knowledge from this unlabeled dataset that can be generalized to a target task, Tnew âˆ¼Dtest,\nduring inference. A critical aspect of UML approaches lies in the task creation mechanism to create\ntasks from Dtrain, which must ensure that the constructed training tasks reflect the structure of those\nencountered during testing, thereby facilitating effective generalization to novel tasks at test time.\nTo do so, we employ two distinct strategies for constructing the support and query sets of each task.\nFor the support set, we randomly sample N images from Dtrain under the assumption that they\nbelong to distinct categories, as shown in Fig. 1. This assumption is reasonable when N << C,\nwhere C denotes the total number of classes in Dtrain, which is satisfied using a large training\ndataset. If we assume that all samples are equally distributed among the classes, i.e., m samples per\nclass, the probability that two or more samples are in the same class is equal to\nP = 1 âˆ’(C Â· m) Â· ((C âˆ’1) Â· m) Â· Â· Â· ((C âˆ’N + 1) Â· m)\n(C Â· m) Â· (C Â· m âˆ’1) Â· Â· Â· (C Â· m âˆ’N + 1)\n= 1 âˆ’C! Â· mN Â· (C Â· m âˆ’N)!\n(C âˆ’N)! Â· (C Â· m)!\n.\nFor example, the probability for a 5-way classification on the ImageNet-964 dataset used in our\nexperiment is around 0.01, which is negligible. To emulate the K-shot scenario typical of meta-\nlearning tasks, we augment each of the N images K times, with an augmentation function Ak\nsampled from a predefined set of transformations A, and we assign the same pseudo-label n âˆˆ[1, N]\nto all data generated from the same sample xn. Specifically, for each image xn, K augmentation\nfunctions are applied to obtain x(sp)\nn,k = Ak(xn) with Ak âˆ¼A and k = 1, . . . , K. One requirement\nof Ak is that the function must preserve class membership, i.e., xn âˆˆc â†’Ak(xn) âˆˆc, for c âˆˆC.\nAlthough this property cannot be directly verified due to the lack of class information in the training\nset, it is reasonable to assume that it holds by selecting transformations that minimally alter the\nimage content.\nFor the query set, we employ a different approach. We demonstrate in Appendix A.5 that simply\napplying data augmentations sampled from A is not sufficient for creating a query set resembling\nthose in test tasks. At test time, the query set samples are different instances belonging to the same\nN classes encountered in the support set, not augmented versions of the support samples. However,\nsince Dtrain is unlabeled, we need a strategy to create new samples with the same implicit classes\nas those in the support set. For each query image x(qr)\nj\nthat we want to generate, we randomly select\n4\nPublished as a conference paper at ICLR 2025\nan image xn from the ones sampled for the support set generation and we apply an augmentation\nfunction Aj âˆ¼A, possibly different from the one used for the support generation. We then propose\na new strategy inspired by mixup, where we combined the augmented image Ëœxn,j = Aj(xn) and an\nimage zj sampled from Dtrain according to:\nx(qr)\nj\n= Î»zj + (1 âˆ’Î»)Ëœxn,j\n(1)\nwhere Î» âˆ¼Beta(Î±, Î²) with Î± = 1, Î² = 1 and Î» âˆˆ(0, 0.5), and x(qr)\nj\nis assigned the same label\nn as the support samples generated from xn. By merging a small proportion of a new image zj\ninto Ëœxn,j, we enhance diversity in the query set with respect to the images in the support set. This\nstrategy enforces the model to extract robust features and effectively generalize to scenarios where\nquery images differ from the support samples, as commonly encountered at test time.\nThis task-creation mechanism can be seen as a task augmentation strategy (Yao et al., 2021; Rajen-\ndran et al., 2020) that allows the generation of a large number (almost infinite) of diverse tasks. This\nis particularly useful for meta-learning and in-context learning applications where the model needs\nto acquire knowledge from a multitude of tasks to generalize to unseen tasks sampled from different\ndomains.\nDifferences with mixup.\nWhile the strategy used for generating the query images draws inspira-\ntion from the mixup strategy proposed in Zhang et al. (2018), there are some substantial differences.\nThe aim of mixup is to develop a new data augmentation strategy to expand the number of training\nexamples and diversify the data distribution used for training, thereby enhancing the robustness and\ngeneralization of neural networks. In CAMeLU, the primary objective of merging images is to en-\ncourage the model to learn even in scenarios where only a fraction of the class context is present\nin the image. In CAMeLU, Î» is sampled from a uniform distribution (obtained with a Beta distri-\nbution with Î± = 1, Î² = 1) in (0, 0.5), guaranteeing that the amount of information from zj that is\nembedded into x(qr)\nj\nis less than 50%, thus ensuring that the assigned label is consistent with the\nclass of the support images generated from xn. Indeed, we assign the same label n to x(qr)\nj\n, forcing\nthe network to learn to retrieve information in x(qr)\nj\nthat is related to the category of xn. Contrar-\nily, mixup creates new examples by interpolating both images and labels at the scope of limiting\nmemorization over the training distribution.\n3.2\nIN-CONTEXT LEARNING METHOD\nFollowing task creation, we rephrase the meta-learning framework as a non-causal sequence mod-\neling problem, where the order of the examples does not entail a causal relationship. Inspired by\nrecent developments in LLMs (Garg et al., 2022; Li et al., 2023; Devlin et al., 2019; Radford et al.,\n2019; Touvron et al., 2023), we treat each task as a prompt, where the support embeddings, together\nwith the learned projected labels, form the demonstration context, whereas the query represents the\nclassification problem that the network is required to solve. A model is said to in-context learn a task\nif it can approximate y(qr)\nj\nfor a new query input x(qr)\nj\nby conditioning on a sequence Si,j containing\nin-context (support) examples and one query input defined as follows:\nSi,j =\n\u0010\n(x(sp)\n1\n, y(sp)\n1\n), . . . , (x(sp)\nNK, y(sp)\nNK), x(qr)\nj\n\u0011\n,\nj = 1, . . . , Q,\n(2)\nwith Q the number of query samples to classify and NK the total number of context (support)\nsamples. Formally, MÎ¸ can in-context learn a task Ti if it can predict y(qr)\nj\nwith an average error\nE\nï£®\nï£°\nQ\nX\nj=1\nâ„“(MÎ¸(Si,j), y(qr)\nj\n)\nï£¹\nï£»< Ïµ,\n(3)\nwhere â„“is the loss function, Si,j is the sequence associated to x(qr)\nj\nin Ti, and y(qr)\nj\nâˆˆ[1, N].\nTo achieve this, we design a model comprising three components: (1) a feature extractor fÏˆ,\n(2) a class encoder gÏ•, and (3) a transformer encoder with a linear projection layer on top, i.e.,\nMÎ¸. The feature extractor aims to map support and query samples into a latent space where im-\nages with similar characteristics and semantic meaning are assigned similar representations. In\n5\nPublished as a conference paper at ICLR 2025\n(a) Feature extractor\n(b) Transformer encoder\nFigure 2: Visualization of clustered embeddings obtained with CAMeLU after the feature extractor\n(left) and the transformer encoder (right) on a 5-way 5-shot task sampled from the CUB dataset.\nCrosses indicate the centroids of each class, and the numbers denote the Euclidean distances between\nthe query (triangle) and each class centroid. The plots are obtained using t-SNE (Van der Maaten &\nHinton, 2008) with a perplexity equal to 9.\nAppendix A.4 we explore various feature extractors for this purpose, including those pre-trained\nvia a supervised approach or leveraging an SSL technique.\nThe resulting representations are\nthen concatenated with a class embedding.\nThe class embeddings for the support representa-\ntions are generated by encoding the corresponding classes using the class encoder gÏ•. However,\nas the classes of the queries are unknown, a randomly initialized learnable vector is appended\nto each query representation. The so-combined embeddings are then organized into sequences\nSi,j =\n\u0010\n(fÏˆ(x(sp)\n1\n), gÏ•(y(sp)\n1\n)), . . . , (fÏˆ(x(sp)\nNK), gÏ•(y(sp)\nNK)), fÏˆ(x(qr)\nj\n)\n\u0011\n, j = 1, . . . , Q, resembling\nthe one in Eq. 2. These sequences are fed into the transformer encoder, and only the transformer\noutput corresponding to the query sample is selected and passed through a projection layer to predict\nthe query label. This process iterates for all queries in the task, and the aggregated loss is employed\nfor model training. In particular, the training process can be formulated as an optimization process\nwhere the objective is as follows:\nmin\nÎ¸,Ï• ESi\nï£®\nï£°1\nQ\nQ\nX\nj=1\nâ„“(MÎ¸(Si,j), y(qr)\nj\n)\nï£¹\nï£»\n(4)\nwith Si = {Si,j}Q\nj=1 denoting the set of sequences associated to each task Ti generated from Dtrain\nand â„“is the cross-entropy loss function.\nDuring evaluation, when a new task is presented, the available examples in D(sp)\nnew are utilized as\ncontextual information to guide the classification of the query samples without requiring any fine-\ntuning or adaptation steps.\nAnalysis.\nTo gain a better understanding of how the in-context learner functions during inference,\nwe show the embedding space of an exemplary test task after the feature extractor and the trans-\nformer encoder in Fig. 2. The embedding space after the feature extractor appears sparse, with a\nlarge Euclidean distance between the query sample and the centroid of each class, which indicates\nlimited class separability and less informative representations. In contrast, the transformer encoder\nsignificantly improves the representation, producing more compact and well-separated clusters. No-\ntably, the query representation aligns closely with the support examples of the same class, show-\ncasing the effectiveness of the transformer in utilizing the task context to refine predictions. This\ndemonstrates the in-context learnerâ€™s ability to adapt representations dynamically based on the task\ncontext and also provides evidence supporting CAMeLUâ€™s superior performance, particularly in\ncross-domain and few-shot scenarios where generalization is more challenging. Experiments for the\nother datasets can be found in Appendix A.2.\n6\nPublished as a conference paper at ICLR 2025\n4\nEXPERIMENTS\nIn this section, we demonstrate the effectiveness of CAMeLU across different datasets, and we com-\npare the results with several baseline methods. In particular, we provide a quantitative comparison\nwith UML baselines in Sect. 4.3, and we highlight the ability of CAMeLU to leverage generalization\nover memorization in Sect. 4.4. We then present the results with a small-scale training dataset in\nSect. 4.5 and a comparison with SSL methods in Sect. 4.6.\n4.1\nDATASETS AND BASELINES\nFor the evaluation, we use two generic object recognition datasets, i.e., miniImageNet (Ravi &\nLarochelle, 2016) and CIFAR-fs (Bertinetto et al., 2019), and three fine-grained image classification\ndatasets, i.e., CUB (Wah et al., 2011), Aircraft (Maji et al., 2013), and Meta-iNat (Wertheimer\n& Hariharan, 2019). While miniImageNet and CIFAR-fs share some classes with ImageNet-1k,\nCUB, Aircraft, and Meta-iNat focus on more specialized domains, ensuring a rigorous cross-domain\nevaluation. Each dataset is split into training, validation, and test sets following the splits in Ravi\n& Larochelle (2016) and Bertinetto et al. (2019) for miniImageNet and CIFAR-fs, respectively,\nand in Triantafillou et al. (2019) and Poulakakis-Daktylidis & Jamali-Rad (2024) for the remaining\ndatasets. All labels are removed from the datasets during the training phase.\nWe compare CAMeLU with standard UML approaches such as CACTUs (Hsu et al., 2018), UM-\nTRA (Khodadadeh et al., 2019), Meta-GMVAE (Lee et al., 2020), and PsCo (Jang et al., 2022).\nThese methods are evaluated in-domain as recommended in the original papers, with training and\ntesting performed on the same dataset. While this setup is relatively simpler than the cross-domain\nevaluation employed for CAMeLU, applying these methods in a cross-domain scenario may not be\nfair, as they were not explicitly designed for such a challenging scenario. Only PsCo (Jang et al.,\n2022) is further evaluated in a cross-domain setting, as the authors demonstrate its adaptability to\nthis scenario through an additional adaptation phase to the test domain. We also compare CAMeLU\nwith BECLR (Poulakakis-Daktylidis & Jamali-Rad, 2024) in Sect. 4.5, a contrastive framework\nfor unsupervised few-shot learning, and CAML (Fifty et al., 2024), a supervised meta-learning ap-\nproach that assumes tasks are available both during the training and testing phases and leverages the\nin-context ability of transformer architectures to generalize to new tasks. Due to their similarities,\nwe refer to CAML as the supervised counterpart of CAMeLU. Furthermore, Sect. 4.6 provides a\ncomparative analysis with two fine-tuned state-of-the-art self-supervised trained networks, namely\nSimCLR (Chen et al., 2020) and SwAV (Caron et al., 2020).\n4.2\nTRAINING DETAILS\nWe report the results following the N-way K-shot classification task typical of meta-learning algo-\nrithms, where N = 5 and K = 1 or K = 5. All models are trained for 100 epochs with 500 episodes\nper epoch. Fine-tuning at test time (100 steps) is applied only if required. For CAMeLU, we do not\napply any fine-tuning step to demonstrate the strength of its training stage, which does not require\nadditional parameter updates during inference. Furthermore, we introduce ImageNet-964, a variant\nof ImageNet-1k (Deng et al., 2009) where classes from the validation and test splits of miniImageNet\nare removed to prevent data leakageâ€”a problem that is not taken into consideration by previous stud-\nies (Fifty et al., 2024; Jang et al., 2022). To provide a fair comparison, all cross-domain methods are\ntrained on ImageNet-964. For CAMeLU, we use a ResNet-50 (He et al., 2016) feature extractor pre-\ntrained on ImageNet-964 and a class encoder that maps one-hot label vectors to a 256-dimensional\nspace. In Appendix A.4, we also report the results with different feature extractors. The transformer\nencoder consists of 8 layers, each with an eight-head self-attention block, an MLP, and a single pro-\njection layer that maps the transformer output to the predicted category. The model is trained with\nthe Adam optimizer with a learning rate of 10âˆ’5 and a warmup cosine scheduler (Vaswani et al.,\n2017). To account for statistical variations, each algorithm is run three times in full, and the com-\nplete results reporting the standard deviations are presented in Appendix A.10. The experiments are\nexecuted using Python and the PyTorch library on an Nvidia GeForce RTX 3070 Ti Laptop GPU\nwith 8GB of VRAM, while ablation studies and competitors are executed on an Nvidia A100-SXM4\nGPU with 40GB of VRAM. More details about the training settings can be found in Appendix A.1,\nand the code is available at https://github.com/bracca95/CAMeLU.git.\n7\nPublished as a conference paper at ICLR 2025\nTable 1: Performance comparison on miniImageNet, CIFAR-fs, CUB, Aircraft, and Meta-iNat\ndatasets for 5-way 1-shot and 5-way 5-shot scenarios. Cross-domain approaches are trained using\nImageNet-964 and a ResNet-50 feature extractor. The symbol â€  indicates results that are affected\nby data leakage. The bold font highlights the best performing UML approach for each setting. Re-\nsults show the average across three complete runs of the algorithms. Complete results with standard\ndeviations are reported in Tab. 12 in Appendix A.10.\nminiImageNet\nCIFAR-fs\nCUB\nAircraft\nMeta-iNat\nMethod\n5w1s\n5w5s\n5w1s\n5w5s\n5w1s\n5w5s\n5w1s\n5w5s\n5w1s\n5w5s\nIn-Domain\nCACTUs-MAML\n43.30\n54.21\n42.00\n56.64\n31.19\n36.81\n24.06\n27.26\n20.13\n21.84\nCACTUs-ProtoNet\n48.85\n62.52\n50.90\n64.52\n33.93\n44.41\n26.27\n30.88\n27.30\n29.08\nUMTRA\n39.93\n50.73\n32.93\n46.13\n27.06\n36.6\n22.40\n31.73\n28.96\n37.12\nMeta-GMVAE\n55.38â€ \n65.10â€ \n52.02\n64.18\n33.59\n39.09\n24.83\n27.60\n34.22\n40.23\nPsCo\n47.29\n64.85\n42.21\n62.92\n33.09\n51.02\n26.19\n38.80\n36.97\n55.88\nCross-Domain\nPsCo\n67.89\n90.17\n53.34\n76.22\n43.35\n70.19\n29.87\n38.20\n46.21\n70.05\nCAMeLU\n76.51\n92.14\n61.79\n80.43\n65.52\n80.35\n33.17\n39.11\n57.27\n75.45\nCAML (supervised)\n81.75\n92.31\n59.44\n75.27\n54.63\n66.81\n28.92\n32.06\n50.86\n67.07\n4.3\nCOMPARATIVE RESULTS\nTable 1 provides an overview of the experimental results for both the 5-way 1-shot and the 5-way\n5-shot scenarios. The results demonstrate that CAMeLU outperforms the existing UML methods,\nregardless of the difference in the evaluation setting. As highlighted in Sect. 4.1, CACTUs, UM-\nTRA, and Meta-GMVAE are evaluated only in-domain, requiring knowledge about the test domain\nprior to training. This is not necessary for CAMeLU as it demonstrates high performance in the\nchallenging cross-domain scenario. Even compared to PsCo, the only UML method designed for\ncross-domain applications, CAMeLU exhibits a performance improvement across all datasets. Fur-\nthermore, PsCo requires a fine-tuning phase to adapt to the test domain, whereas CAMeLU achieves\ngood performance with a single forward pass, enhancing its applicability to real-time applications.\nIt is also worth noting that CAMeLU achieves comparable performance to its supervised counter-\npart, CAML, when evaluated on miniImageNet and it even outperforms CAML when evaluated on\nmore dissimilar domains, such as CUB, Aircraft, and Meta-iNat. This finding highlights the efficacy\nof the task construction strategy used in CAMeLU, which acts as a sort of task augmentation and\nenhances the generalization capability of the model.\n4.4\nMEMORIZATION TO GENERALIZATION PHASE SHIFT\n0\n20\n40\n60\n80\n100\nEpochs\n0.1\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nRelative validation accuracy\nMemorization\nLearning\nGeneralization\nminiImageNet\nCIFAR-fs\nCUB\nAircraft\nFigure 3: Analysis of learning behavior when\ntransferring knowledge from a different prior\ndataset.\nThe relative validation accuracy\nshows the difference between the current and\nfirst epoch accuracy on the validation set of\nminiImageNet, CIFAR-fs, CUB, and Aircraft.\nCAMeLU is trained with ImageNet-964.\nDuring the training of CAMeLU, we observed a\ndistinct trend in the validation accuracy, similar to\nthe findings in Kirsch et al. (2022). Fig. 3 illus-\ntrates this pattern, showing the validation accu-\nracy relative to its initial value, or, in other words,\nhow much the model learns from datasets differ-\nent from the one we are training on. Specifically,\nthe curves in Fig. 3 resemble a logistic curve,\nwhich can be divided into three phases that we\ndenote as memorization, learning, and general-\nization. In the memorization phase, the model\nmemorizes the tasks seen during training and ex-\ntends this knowledge to unseen tasks, resulting in\na slight improvement for datasets with high sim-\nilarity with ImageNet-964 (e.g., miniImageNet\nand CIFAR-fs). For the other datasets, instead,\ntransferring this knowledge can even result in a\nperformance decrease due to the intrinsic domain\ndistance of the dataset (see CUB and Aircraft,\n8\nPublished as a conference paper at ICLR 2025\nTable 2: Accuracy results obtained training PsCo, BECLR, and CAMeLU with a small-scale dataset,\nnamely miniImageNet, denoted as (mini) in the table. Results show both in-domain performance\n(on the test set of miniImageNet) and cross-domain performance on CIFAR-fs, CUB, Aircraft, and\nMeta-iNat. The average results across three complete runs of the algorithms are reported. Complete\nresults with standard deviations are presented in Tab. 13 in Appendix A.10.\nminiImageNet\nCIFAR-fs\nCUB\nAircraft\nMeta-iNat\n5w1s\n5w5s\n5w1s\n5w5s\n5w1s\n5w5s\n5w1s\n5w5s\n5w1s\n5w5s\nPsCo (mini)\n47.29\n64.85\n42.21\n62.92\n33.09\n51.02\n26.19\n38.80\n36.97\n55.88\nBECLR (mini)\n81.04\n87.88\n57.05\n72.82\n42.47\n58.03\n27.48\n38.46\n49.87\n65.05\nCAMeLU (mini)\n75.99\n90.38\n61.25\n78.79\n60.60\n74.77\n31.39\n36.52\n55.60\n72.12\nwhich are fine-grained datasets). As training progresses and the model observes more tasks, the\nlearning phase occurs. This phase is characterized by a transition to the learning-to-learn state\nwhere the model learns to identify the tasks and to extract the features that are more useful for\nsolving them. The duration of this phase varies, with datasets like miniImageNet and CIFAR-fs\nexhibiting rapid learning within approximately 10 epochs, while datasets such as CUB and Aircraft\nmay necessitate up to 40 epochs. This timespan depends on several factors, including the similarity\nbetween the training and evaluation datasets, the size of the test dataset, and the modelâ€™s learning\nability (Kirsch et al., 2022; Power et al., 2022). For instance, CUB, with its fine-grained nature\nand small test set size (around 1770 images), necessitates a longer learning phase compared to the\nminiImageNet dataset (which has a test set with 12 000 images). Subsequently, in the generalization\nphase, the model can generalize to tasks significantly different from those observed during training\nusing a single forward pass. Further analyses about the generalization capabilities of CAMeLU and\nthe number of epochs required for reaching the generalization phase are presented in Sect. 4.5 and\nAppendix A.8.\n4.5\nGENERALIZATION ON SMALL-SCALE DATASETS\nWhile most studies on training transformer architectures focus on large-scale training datasets, we\ninvestigate the generalization capabilities of CAMeLU using a small-scale training dataset. Specifi-\ncally, we train CAMeLU on miniImageNet and evaluate its performance both in-domain (i.e., on the\ntest set of miniImageNet) and cross-domain on CIFAR-fs, CUB, Aircraft, and Meta-iNat. CAMeLU\ndemonstrates effective generalization in this scenario, showing impressive performance in both in-\ndomain and cross-domain settings, as shown in Tab. 2, surpassing PsCo and BECLR by a large\nmargin.\nAdditionally, a comparison of CAMeLUâ€™s performance when trained on a small-scale dataset like\nminiImageNet (Tab. 2), on ImageNet-964 (Tab. 1), and on a large-scale dataset (Tab. 5 in Appendix\n0\n20\n40\n60\n80\n100\nEpochs\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nRelative validation accuracy\nCAML\nCAMeLU\nFigure 4:\nRelative validation accuracy of\nCAMeLU (orange) and CAML (blue) when\nevaluated in-domain on miniImagenet and com-\nputed as in Sect. 4.4. The curve obtained with\nCAMeLU reflects the three phases of mem-\norization, learning, and generalization even\nwhen using a small-scale dataset.\nA.3) show that our method is only slightly af-\nfected by the size of the training dataset. This\nrobustness enhances CAMeLUâ€™s applicability to\nscenarios where only a small unlabeled training\ndataset is available, which is common in real-\nworld applications.\nFinally, Fig. 4 shows the relative validation accu-\nracy of CAMeLU and CAML while trained and\nevaluated on miniImageNet. While the curve ob-\ntained with CAMeLU reflects the three phases of\nmemorization, learning, and generalization dis-\ncussed in Sect. 4.4, the relative validation ac-\ncuracy of CAML remains flat. This difference\nmay be attributed to the task creation mechanism\nof CAMeLU, which acts as a task augmentation\nstrategy, increasing the variability of tasks pre-\nsented to the model during training and thereby\nenhancing its generalization capabilities.\n9\nPublished as a conference paper at ICLR 2025\nTable 3: Comparison between CAMeLU and SSL approaches for the 5-way 1-shot and 5-way 5-\nshot scenario on miniImageNet, CIFAR-fs, CUB, Aircraft, and Meta-iNat. The symbol â€  indicates\nresults that are affected by data leakage. Results show the average across three complete runs of the\nalgorithms. Complete results with standard deviations are reported in Tab. 14 in Appendix A.10.\nminiImageNet\nCIFAR-fs\nCUB\nAircraft\nMeta-iNat\nMethod\n5w1s\n5w5s\n5w1s\n5w5s\n5w1s\n5w5s\n5w1s\n5w5s\n5w1s\n5w5s\nSimCLR\n83.32â€ \n94.86â€ \n64.52\n84.36\n47.35\n66.87\n29.36\n39.99\n52.44\n73.19\nSwAV\n74.83â€ \n94.96â€ \n66.97\n87.14\n47.84\n69.31\n30.33\n47.43\n53.57\n74.53\nCAMeLU\n76.51\n92.14\n61.79\n80.43\n65.52\n80.35\n33.17\n39.11\n57.27\n75.45\n4.6\nCOMPARISON WITH SSL METHODS\nIn this section, we compare CAMeLU with SimCLR (Chen et al., 2020) and SwAV (Caron et al.,\n2020). For training SSL methods in our experiments, we employed a backbone network with a\nResNet-50 architecture pre-trained on ImageNet-1k and obtained from PyTorch Lightning Bolts\n(Borovec et al., 2022). While this setup leads to data leakage when evaluated on miniImageNet,\ndue to overlap between the test and training classes, pre-training these SSL approaches from scratch\nusing a different training dataset was beyond our available computational resources. To facilitate\nmodel adaptation to the test domain, we fine-tuned a classification layer on top of the pre-trained\nbackbone using SGD with an initial learning rate of 0.01, momentum of 0.9, weight decay of 10âˆ’4,\nand 100 fine-tuning steps per task, following Jang et al. (2022). This setup differs from the evaluation\nsetting used in CAMeLU, where predictions are obtained with a single forward pass, leveraging\nthe in-context learning ability of transformer architectures. However, SSL approaches must adapt\nto the test domain before label predictions, resulting in a less challenging evaluation setting than\nCAMeLU. Results for SSL approaches are averaged over 500 test tasks and presented in Tab. 3.\nWhile SSL approaches outperform CAMeLU on miniImageNet and CIFAR-fs, their performance\ndecreases when evaluated on the other datasets. CUB, Aircraft, and Meta-iNat are fine-grained\ndatasets significantly different from ImageNet-1k, challenging the transferability of features learned\nby SSL methods to these datasets. Moreover, the high performance on miniImageNet and CIFAR-fs\nmay be attributed to the presence of data leakage with ImageNet-1k and the high similarity with\nCIFAR-fs, as discussed in Sect. 4.2. CAMeLU, in contrast, demonstrates effective generalization\nto tasks sampled from these datasets, once again highlighting its generalization ability over mere\nmemorization.\n5\nCONCLUSION\nIn this paper, we introduce CAMeLU, a novel approach for UML that leverages the in-context learn-\ning capabilities of transformer architectures to extract context from the support samples and make\neffective predictions on the query data. CAMeLU reframes meta-learning as a sequence model-\ning problem, where support images provide task context for predicting query images. At the core of\nCAMeLU is a novel task creation mechanism that generates diverse tasks from an unlabeled dataset,\npromoting effective generalization to unseen tasks. Our experimental results showcase the superior-\nity of CAMeLU over existing UML methods, highlighting the applicability of the proposed method\nto domains different from the training one. Notably, CAMeLU can generalize to new domains with a\nsingle forward pass (real-time predictions), and it even outperforms its supervised counterpart thanks\nto its task creation mechanism. Furthermore, the proposed model can be stored and trained with a\nsingle GPU with only 8GB of VRAM, underscoring its efficiency in learning-to-learn in-context,\nrather than using a meta-training phase typical of previous meta-learning approaches.\nFuture research directions may explore extensions of CAMeLU to more complex domains, as well\nas investigations into further improving the task creation mechanism for enhanced generalization. It\nwould be interesting to incorporate SSL techniques to obtain more robust feature representations and\nenhance generalization capabilities. Additionally, conducting further investigation into CAMeLUâ€™s\nability to encourage generalization over memorization would provide valuable insights into its learn-\ning dynamics and potential areas for improvement.\n10\nPublished as a conference paper at ICLR 2025\nETHICS STATEMENT AND REPRODUCIBILITY GUIDELINES\nIn this work, we used well-established, publicly available datasets to train and evaluate our archi-\ntecture. While these datasets and pre-trained models provide a valuable foundation for research, we\nacknowledge the potential for inherent biases that may not fully represent diverse real-world scenar-\nios. We have taken every precaution to ensure that our experiments are conducted responsibly, with\nno intention of causing harm or perpetuating any biases present in the data. Furthermore, we declare\nno conflicts of interest in the execution or reporting of this research. Our objective is to present the\nfindings in a transparent manner and contribute positively to the broader research community.\nTo ensure the reproducibility of our experiments, we have provided the code and detailed instructions\non how to run the experiments. The general configuration of our model is described in Sect. 4, with\nadditional technical details outlined in Sect. A.1 of the Appendix. Moreover, we have employed\nrandom seed initialization to ensure consistency across runs. The complete codebase, models, and\npre-trained weights are available on GitHub 1 to facilitate further research and replication.\nACKNOWLEDGMENTS\nThis work was supported by the â€œKnowledge Foundationâ€ (KK-stiftelsen).\n1https://github.com/bracca95/CAMeLU.git\n11\nPublished as a conference paper at ICLR 2025\nREFERENCES\nAmir Bar, Yossi Gandelsman, Trevor Darrell, Amir Globerson, and Alexei Efros. Visual prompting\nvia image inpainting. Advances in Neural Information Processing Systems, 35:25005â€“25017,\n2022.\nL Bertinetto, J Henriques, P Torr, and A Vedaldi. Meta-learning with differentiable closed-form\nsolvers. In International Conference on Learning Representations (ICLR), 2019. International\nConference on Learning Representations, 2019.\nJirka Borovec, William Falcon, Akihiro Nitta, Ananya Harsh Jha, otaj, Annika Brundyn, Donal\nByrne, Nathan Raw, Shion Matsumoto, Teddy Koker, Brian Ko, Aditya Oke, Sidhant Sundrani,\nBaruch, Christoph Clement, ClÂ´ement Poiret, Rohit Gupta, Haswanth Aekula, Adrian WÂ¨alchli,\nAtharva Phatak, Ido Kessler, Jason Wang, JongMok Lee, Shivam Mehta, Zhengyu Yang, and\nGarry Oâ€™Donnell. Pytorch lightning bolts. https://lightning-bolts.readthedocs.\nio/en/latest/, 2022. Online; accessed 25 Apr 2024.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877â€“1901, 2020.\nVictor Ion Butoi, Jose Javier Gonzalez Ortiz, Tianyu Ma, Mert R Sabuncu, John Guttag, and\nAdrian V Dalca.\nUniverseg: Universal medical image segmentation.\nIn Proceedings of the\nIEEE/CVF International Conference on Computer Vision, pp. 21438â€“21451, 2023.\nMathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin.\nUnsupervised learning of visual features by contrasting cluster assignments. Advances in neural\ninformation processing systems, 33:9912â€“9924, 2020.\nStephanie Chan, Adam Santoro, Andrew Lampinen, Jane Wang, Aaditya Singh, Pierre Richemond,\nJames McClelland, and Felix Hill. Data distributional properties drive emergent in-context learn-\ning in transformers. Advances in Neural Information Processing Systems, 35:18878â€“18891, 2022.\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for\ncontrastive learning of visual representations. In International conference on machine learning,\npp. 1597â€“1607. PMLR, 2020.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hi-\nerarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,\npp. 248â€“255. Ieee, 2009.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. In Proceedings of NAACL-HLT, pp. 4171â€“\n4186, 2019.\nCarl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual representation learning by\ncontext prediction. In Proceedings of the IEEE international conference on computer vision, pp.\n1422â€“1430, 2015.\nQingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu,\nand Zhifang Sui. A survey on in-context learning. arXiv preprint arXiv:2301.00234, 2022.\nChristopher Fifty, Jure Leskovec, and Sebastian Thrun. In-context learning for few-shot molecular\nproperty prediction. arXiv preprint arXiv:2310.08863, 2023.\nChristopher Fifty, Dennis Duan, Ronald Guenther Junkins, Ehsan Amid, Jure Leskovec, Christopher\nRe, and Sebastian Thrun. Context-aware meta-learning. In The Twelfth International Conference\non Learning Representations, 2024.\nChelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation\nof deep networks. In International conference on machine learning, pp. 1126â€“1135. PMLR, 2017.\nShivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What can transformers learn\nin-context? a case study of simple function classes. Advances in Neural Information Processing\nSystems, 35:30583â€“30598, 2022.\n12\nPublished as a conference paper at ICLR 2025\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing\nhuman-level performance on imagenet classification. In Proceedings of the IEEE international\nconference on computer vision, pp. 1026â€“1034, 2015.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.\n770â€“778, 2016.\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.\nMomentum contrast for\nunsupervised visual representation learning.\nIn Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pp. 9729â€“9738, 2020.\nKyle Hsu, Sergey Levine, and Chelsea Finn. Unsupervised learning via meta-learning. In Interna-\ntional Conference on Learning Representations, 2018.\nHuiwon Jang, Hankook Lee, and Jinwoo Shin. Unsupervised meta-learning via few-shot pseudo-\nsupervised contrastive learning. In The Eleventh International Conference on Learning Repre-\nsentations, 2022.\nSiavash Khodadadeh, Ladislau Boloni, and Mubarak Shah. Unsupervised meta-learning for few-\nshot image classification. Advances in neural information processing systems, 32, 2019.\nSiavash Khodadadeh, Sharare Zehtabian, Saeed Vahidian, Weijia Wang, Bill Lin, and Ladislau\nBoloni. Unsupervised meta-learning through latent-space interpolation in generative models. In\nInternational Conference on Learning Representations, 2020.\nLouis Kirsch, James Harrison, Jascha Sohl-Dickstein, and Luke Metz. General-purpose in-context\nlearning by meta-learning transformers. arXiv preprint arXiv:2212.04458, 2022.\nDeqian Kong, Bo Pang, and Ying Nian Wu. Unsupervised meta-learning via latent space energy-\nbased model of symbol vector coupling. In Fifth Workshop on Meta-Learning at the Conference\non Neural Information Processing Systems, 2021.\nDong Bok Lee, Dongchan Min, Seanie Lee, and Sung Ju Hwang. Meta-gmvae: Mixture of gaussian\nvae for unsupervised meta-learning. In International Conference on Learning Representations,\n2020.\nDong Bok Lee, Seanie Lee, Kenji Kawaguchi, Yunji Kim, Jihwan Bang, Jung-Woo Ha, and Sung Ju\nHwang.\nSelf-supervised set representation learning for unsupervised meta-learning.\nIn The\nEleventh International Conference on Learning Representations, 2022.\nYingcong Li, Muhammed Emrullah Ildiz, Dimitris Papailiopoulos, and Samet Oymak. Transformers\nas algorithms: Generalization and stability in in-context learning. In International Conference on\nMachine Learning, pp. 19565â€“19594. PMLR, 2023.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\nDollÂ´ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer\nVisionâ€“ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014,\nProceedings, Part V 13, pp. 740â€“755. Springer, 2014.\nChen Liu, Yanwei Fu, Chengming Xu, Siqian Yang, Jilin Li, Chengjie Wang, and Li Zhang. Learn-\ning a few-shot embedding model with contrastive learning. In Proceedings of the AAAI conference\non artificial intelligence, volume 35, pp. 8635â€“8643, 2021.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, William B Dolan, Lawrence Carin, and Weizhu Chen.\nWhat makes good in-context examples for gpt-3? In Proceedings of Deep Learning Inside Out\n(DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning\nArchitectures, pp. 100â€“114, 2022.\nSubhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-grained\nvisual classification of aircraft. arXiv preprint arXiv:1306.5151, 2013.\n13\nPublished as a conference paper at ICLR 2025\nSewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. Metaicl: Learning to learn in\ncontext. In Proceedings of the 2022 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies, pp. 2791â€“2809, 2022.\nNikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. A simple neural attentive meta-\nlearner. In International Conference on Learning Representations, 2018.\nStylianos Poulakakis-Daktylidis and Hadi Jamali-Rad. Beclr: Batch enhanced contrastive few-shot\nlearning. In The Twelfth International Conference on Learning Representations, 2024.\nAlethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra. Grokking: Gen-\neralization beyond overfitting on small algorithmic datasets. arXiv preprint arXiv:2201.02177,\n2022.\nAlec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nmodels are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International conference on machine learning, pp.\n8748â€“8763. PMLR, 2021.\nJanarthanan Rajendran, Alexander Irpan, and Eric Jang. Meta-learning requires meta-augmentation.\nAdvances in Neural Information Processing Systems, 33:5705â€“5715, 2020.\nSachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In International\nconference on learning representations, 2016.\nBrigit Schroeder and Yin Cui. Fgvcx fungi classification challenge 2018. https://github.\ncom/visipedia/fgvcx_fungi_comp, 2018. Online; accessed 25 Apr 2024.\nConnor Shorten and Taghi M Khoshgoftaar. A survey on image data augmentation for deep learning.\nJournal of big data, 6(1):1â€“48, 2019.\nAaditya Singh, Stephanie Chan, Ted Moskovitz, Erin Grant, Andrew Saxe, and Felix Hill. The\ntransient nature of emergent in-context learning in transformers. Advances in Neural Information\nProcessing Systems, 36, 2024.\nJake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. Ad-\nvances in neural information processing systems, 30, 2017.\nFlood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip HS Torr, and Timothy M Hospedales.\nLearning to compare: Relation network for few-shot learning. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition, pp. 1199â€“1208, 2018.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÂ´ee\nLacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and\nefficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\nEleni Triantafillou, Tyler Zhu, Vincent Dumoulin, Pascal Lamblin, Utku Evci, Kelvin Xu, Ross\nGoroshin, Carles Gelada, Kevin Swersky, Pierre-Antoine Manzagol, et al. Meta-dataset: A dataset\nof datasets for learning to learn from few examples. In International Conference on Learning\nRepresentations, 2019.\nLaurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine\nlearning research, 9(11), 2008.\nJoaquin Vanschoren. Meta-learning. Automated machine learning: methods, systems, challenges,\npp. 35â€“61, 2019.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nÅukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural informa-\ntion processing systems, 30, 2017.\n14\nPublished as a conference paper at ICLR 2025\nVikas Verma, Alex Lamb, Christopher Beckham, Amir Najafi, Ioannis Mitliagkas, David Lopez-\nPaz, and Yoshua Bengio. Manifold mixup: Better representations by interpolating hidden states.\nIn International conference on machine learning, pp. 6438â€“6447. PMLR, 2019.\nAnna Vettoruzzo, Mohamed-Rafik Bouguelia, Joaquin Vanschoren, Thorsteinn Rognvaldsson, and\nKC Santosh. Advances and challenges in meta-learning: A technical review. IEEE Transactions\non Pattern Analysis and Machine Intelligence, 2024.\nOriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one\nshot learning. Advances in neural information processing systems, 29, 2016.\nCatherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltech-ucsd\nbirds-200-2011 dataset. California Institute of Technology, 2011.\nZhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment:\nfrom error visibility to structural similarity. IEEE transactions on image processing, 13(4):600â€“\n612, 2004.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yo-\ngatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language\nmodels. Transactions on Machine Learning Research, 2022.\nDavis Wertheimer and Bharath Hariharan. Few-shot learning with localization in realistic settings.\nIn Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp.\n6558â€“6567, 2019.\nPatrick H Winston. Learning and reasoning by analogy. Communications of the ACM, 23(12):\n689â€“703, 1980.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,\nPierric Cistac, Tim Rault, RÂ´emi Louf, Morgan Funtowicz, et al. Huggingfaceâ€™s transformers:\nState-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019.\nHuaxiu Yao, Long-Kai Huang, Linjun Zhang, Ying Wei, Li Tian, James Zou, Junzhou Huang, et al.\nImproving generalization in meta-learning via task augmentation. In International conference on\nmachine learning, pp. 11887â€“11897. PMLR, 2021.\nSangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo.\nCutmix: Regularization strategy to train strong classifiers with localizable features. In Proceed-\nings of the IEEE/CVF international conference on computer vision, pp. 6023â€“6032, 2019.\nHongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical\nrisk minimization. In International Conference on Learning Representations, 2018.\nYuanhan Zhang, Kaiyang Zhou, and Ziwei Liu. What makes good examples for visual in-context\nlearning? Advances in Neural Information Processing Systems, 36, 2024.\n15\nPublished as a conference paper at ICLR 2025\nA\nAPPENDIX\nA.1\nEXPERIMENTAL DETAILS\nDatasets.\nFor training CAMeLU, we use ImageNet-964, which is a variant of the original\nImageNet-1k dataset (Deng et al., 2009) where classes belonging to the validation and test splits\nof miniImageNet (Ravi & Larochelle, 2016) are removed. This results in a total of 1 234 487 images\nfor training the model compared to the 1 281 167 in the original ImageNet-1k dataset. When a multi-\ndataset approach is utilized for training CAMeLU (see Appendix A.3), MSCOCO (Lin et al., 2014)\nand Fungi (Schroeder & Cui, 2018) are loaded into the program and used together with ImageNet-\n964 for creating the whole training dataset. MSCOCO is a dataset originally proposed for object\ndetection, where each image is assigned to G classes corresponding to the G objects present in it.\nTo use it for image classification, we replicate each image G times and we assign to each of them\none of the G classes. In this way, we obtain a dataset with 117 266 images for training, and we\nrely on the fact that the transformer is capable of applying self-attention to the object of the class\nin question. Fungi is a fine-grained dataset with a size of only 64 307, which is two orders of mag-\nnitude smaller than ImageNet-964 and MSCOCO. For evaluation and for in-domain training of the\nbaselines, we use miniImageNet, CIFAR-fs, CUB, Aircraft, and Meta-iNat. miniImageNet is split\ninto train/validation/test using the splits proposed in Ravi & Larochelle (2016), resulting in 38 400\nimages for training, 9600 for validation, and 12 000 for testing. The same number of images are also\npresent in CIFAR-fs, and the splits follow the work in Bertinetto et al. (2019). CUB and Aircraft,\ninstead, are two fine-grained datasets with a smaller size compared to the others. CUB (Wah et al.,\n2011) consists of 8239 in the training set, 1779 in the validation set, and 1770 in the test set, while\nAircraft has respectively 7000/1500/1500 images in the train/validation/test sets (Triantafillou et al.,\n2019). Finally, Meta-iNat (Wertheimer & Hariharan, 2019) consists of 243 986 images split into\n1135 classes, with 227 reserved for testing. All images are resized to 224 Ã— 224 and normalized\nwith zero mean and unit variance before input into the model. For the UML baselines, due to the\nsmaller model size utilized in the experiments, images are resized to 84 Ã— 84, as suggested in the\noriginal papers (Khodadadeh et al., 2019; Hsu et al., 2018; Lee et al., 2020; Jang et al., 2022).\nCAMeLU.\nThe architecture used for CAMeLU consists of a fixed pre-trained feature extractor,\na class encoder, and a transformer encoder. The feature extractor is pre-trained using ResNet-50\non ImageNet-964 following the same architecture and hyperparameters in He et al. (2016). The\nclass encoder is a single learnable layer with a dimensionality of 256 and initialized with Kaiming\ninitialization (He et al., 2015). Image embeddings are concatenated with class embeddings before\nbeing fed into the transformer encoder. This results in a vector with a total length of 2304, composed\nof 2048 features from the image embeddings and 256 from the label embeddings. When ablating the\nfeature extractor with CLIP (Radford et al., 2021) in Appendix A.4, a ViT-B/16 encoder architecture\nis utilized and downloaded from the Hugging Face website (Wolf et al., 2019). The pre-training is\nperformed using a large dataset with 400 million (image, text) pairs (Radford et al., 2021), and it is\nfixed during the training phase of CAMeLU. The output size of the image embedding is reduced to\n1024, with 768 features from the image embedding, which results in a reduced memory complexity\ncompared to ResNet-50. The transformer encoder comprises 8 encoder layers. Each layer consists of\n8 attention heads and an MLP with a reversed bottleneck of 3072 (with GeLU activation function). A\nprojection layer completes the model architecture to map the transformer output to a class prediction.\nThis architecture enables us to store the entire model in an Nvidia GeForce RTX 3070 Ti Laptop\nGPU with 8GB of VRAM, while a further reduction of memory can be achieved by utilizing CLIP\non ViT-B/16 as feature extractor, which requires only 4 GB of VRAM to store the entire model.\nFor the task creation mechanism, 3 augmentation functions are selected from a list comprising crop-\nping, rotation, horizontal flip, grayscale, color jittering, gaussian blur, and random affine transfor-\nmation. The exact parameters used in our experiments for each augmentation function are detailed\nin Tab. 4. For the query set, an additional pixel-level mixing strategy with Î» âˆ¼Beta(Î±, Î²) with\nÎ± = 1, Î² = 1 and Î» âˆˆ(0, 0.5) is utilized. More details about this selection choice can be found in\nAppendix A.6.\nThe training of CAMeLU is performed for 100 epochs, with 500 episodes each, using the Adam\noptimizer with an initial learning rate of 10âˆ’5 and a warmup cosine scheduler with 1500 warmup\nsteps and a final learning rate of 10âˆ’6. For the evaluation, instead, a single forward pass is performed\nand the accuracy between the output and the true label is calculated on the query set of each given\n16\nPublished as a conference paper at ICLR 2025\nTable 4: Complete list of transformations used for generating the support set of each task in\nCAMeLU. The names of the augmentations are taken from the torchvision library in Python.\nAugmentation\nParameters\nRandomResizedCrop\nimage size = 224, scale = (0.2, 0.8), ratio= (0.75, 1.33)\nRandomRotation\ndegrees = 60, probability = 1.0\nRandomHorizontalFlip\nprobability = 1.0\nGrayscale\noutput channels = 3\nColorJitter\nbrightness = 0.2, contrast = 0.2, saturation = 0.2, hue = 0.2\nGaussianBlur\nkernel size = 3, sigma = (0.1, 2.0)\nRandomAffine\ndegrees = 0, shear = [âˆ’45, 45, âˆ’45, 45]\ntask. Results are then averaged across 500 tasks, and the mean and standard deviation across three\ncomplete runs (consisting of training and evaluation) of the algorithm are used in our experiments.\nBaselines.\nWe compare our results with UML methods, an unsupervised few-shot learning\nmethod, a supervised meta-learning method, and two SSL methods. For the UML baselines, we\nconsider CACTUs-MAML (Hsu et al., 2018), CACTUs-ProtoNet (Hsu et al., 2018), UMTRA (Kho-\ndadadeh et al., 2019), Meta-GMVAE (Lee et al., 2020), and PsCo (Jang et al., 2022). All these meth-\nods are evaluated in-domain, i.e., using the same dataset for training and evaluation, to adhere to the\nsetting proposed in the original papers. Only PsCo is also extended to the cross-domain scenario\nthat we discuss in this paper. All methods are trained for 100 epochs, using the parameters reported\nin the original papers (Khodadadeh et al., 2019; Hsu et al., 2018; Lee et al., 2020; Jang et al., 2022),\nand evaluated with 100 adaptation steps on each task when required by the model. When evaluated\nin-domain, all approaches use a Conv5 architecture consisting of 5 convolutional layers with 64\nfilters and a kernel size of 3, followed by batch normalization, ReLU non-linearity, max pooling,\nand a classifier head. The only exception is Meta-GMVAE. For this method, the authors trained a\nConv5 feature extractor with SimCLR and input the learned features into a variational autoencoder\n(VAE) (Lee et al., 2020). Due to time limitations and the computational resources required to train\na model with SimCLR, in our experiments, we used a feature extractor consisting of a pre-trained\nversion of SimCLR on ResNet-50 (Borovec et al., 2022) using ImageNet-1k, followed by a pro-\njection layer fine-tuned for 100 steps to the training dataset, as done for the SSL baselines. This\napproach results in a better performance than the one reported in the original paper (Lee et al., 2020)\n(see Tab. 1), likely due to the improved ability of the feature extractor to extract meaningful fea-\ntures. For PsCo, when evaluated on a cross-domain setting, we utilized the ResNet-50 architecture\ntrained on ImageNet-964 to avoid data leakage, and we then applied the model to the test domain\nusing 100 adaptation steps to it. We also included BECLR (Poulakakis-Daktylidis & Jamali-Rad,\n2024) in our comparison utilizing the same hyperparameters and model architectures proposed in the\noriginal paper, given the importance of hyperparameter choice for final performance. Specifically,\nthe ResNet-50 feature extractor was trained only on miniImageNet and evaluated on cross-domain\nscenarios in Tab. 2.\nTo compare our results with CAML (Fifty et al., 2024), the same architecture and hyperparameter of\nour approach are applied to this method. This results in a lower performance for CAML compared\nto the original paper (Fifty et al., 2024), as the reduced size of the model and the different feature\nextractor (ResNet-50 instead of ViT-CLIP), but it guarantees fair comparisons and lets us train the\nmodel with the available computational resources.\nWe also provide a comparison with two SSL approaches - SimCLR (Chen et al., 2020) and SwAV\n(Caron et al., 2020). The details for training and evaluation are provided in Sect. 4.6.\nA.2\nIN-CONTEXT LEARNING ANALYSIS\nTo verify the contribution of the in-context learner in CAMeLU, we examine the embedding space\nlearned during inference, both after the feature extractor and the transformer encoder. Fig. 5 presents\na t-SNE visualization of a single test task, where clusters represent the embeddings of the support\nclasses. For simplicity, we illustrate a 5-way 5-shot task with one query sample for each dataset, and\nwe report the Euclidean distance between the query and the centroid of each class. As the dataset\n17\nPublished as a conference paper at ICLR 2025\n(a) Feature extractor (miniImageNet)\n(b) Transformer encoder (miniImageNet)\n(c) Feature extractor (CUB)\n(d) Transformer encoder (CUB)\n(e) Feature extractor (Aircraft)\n(f) Transformer encoder (Aircraft)\nFigure 5: Visualization of clustered embeddings obtained with CAMeLU after the fixed feature\nextractor (left) and the transformer encoder (right) across different datasets. The plots represent 5-\nway 5-shot tasks during inference. Crosses indicate the centroids for each class, triangles represent\nthe query sample embeddings, and the numbers denote the Euclidean distances between the query\nand each class centroid. The plots are obtained using t-SNE (Van der Maaten & Hinton, 2008) with\na perplexity equals to 9.\n18\nPublished as a conference paper at ICLR 2025\nTable 5: Comparison of CAMeLU and CAML when trained on the single ImageNet-964 dataset\n(IN-964) and on a multi-dataset (mds) consisting of ImageNet-964 + MSCOCO + Fungi. Results\nshow the mean and standard deviations for the 5-way 1-shot and the 5-way 5-shot settings across\nthree complete runs of the algorithms.\nminiImageNet\nCIFAR-fs\nCUB\nAircraft\nMeta-iNat\n5w1s\n5w5s\n5w1s\n5w5s\n5w1s\n5w5s\n5w1s\n5w5s\n5w1s\n5w5s\nCAMeLU (IN-964)\n76.51 Â± 0.79\n92.14 Â± 0.30\n61.79 Â± 0.59\n80.43 Â± 0.21\n65.52 Â± 0.37\n80.35 Â± 0.63\n33.17 Â± 0.94\n39.11 Â± 1.97\n57.27 Â± 0.39\n75.45 Â± 0.42\nCAMeLU (mds)\n76.56 Â± 0.36\n91.82 Â± 0.21\n62.41 Â± 0.70\n80.36 Â± 0.32\n65.35 Â± 0.70\n79.78 Â± 0.38\n32.54 Â± 0.41\n37.87 Â± 0.59\n57.36 Â± 0.33\n75.40 Â± 0.29\nCAML (IN-964)\n81.75 Â± 0.18\n92.31 Â± 0.11\n59.44 Â± 0.63\n75.27 Â± 0.77\n54.63 Â± 1.78\n66.81 Â± 3.12\n28.92 Â± 0.37\n32.06 Â± 0.43\n50.86 Â± 0.50\n67.07 Â± 0.39\nCAML (mds)\n81.90 Â± 0.54\n92.93 Â± 0.33\n63.08 Â± 0.43\n79.73 Â± 0.63\n56.85 Â± 1.92\n69.43 Â± 1.85\n28.36 Â± 2.26\n31.56 Â± 2.14\n54.72 Â± 0.63\n70.50 Â± 0.67\ncomplexity increases, we observe greater variation between the embeddings learned from the fixed\nfeature extractor and those refined by the transformer encoder. For instance, with the miniImageNet\ndataset, the feature extractor alone is able to recognize the class of the query, clustering the query\nsample with the support samples belonging to the same class. However, in more challenging datasets\nsuch as CUB and Aircraft, the embedding space after the feature extractor appears more sparse,\nreflected by the large Euclidean distance between the query sample and the centroid of each class. In\ncontrast, the transformer encoder significantly improves the representation, producing more compact\nand well-separated clusters, underscoring its crucial role in CAMeLU. For instance, in the Aircraft\ndataset, the query would be misclassified as class 5 (in grey) based on the feature extractor alone,\nbut it is correctly classified after passing through the transformer. This highlights the role of the\ntransformer encoder in updating the support and query representations based on the context of the\ntask, not only the image context, improving classification accuracy.\nA.3\nMULTI-DATASET TRAINING\nWe conduct additional experiments to evaluate the performance of CAMeLU when trained on a\nlarge-scale dataset. As our focus is on cross-domain classification through in-context learning, we\nhypothesize that training on a dataset spanning various concepts could enhance classification per-\nformance, as suggested in Min et al. (2022) and Fifty et al. (2024). To test this hypothesis, we com-\nbine three training datasets with varying levels of granularity: ImageNet-964 (Deng et al., 2009),\nMSCOCO (Lin et al., 2014), and Fungi (Schroeder & Cui, 2018). During each training episode, a\ndataset is uniformly sampled, and N data points are extracted. These samples are then utilized in our\nmethod as described in Sect. 3. Table 5 presents the results for CAMeLU and the supervised CAML\nmethod. Notably, training with a combination of multiple datasets (denoted as mds in Tab. 5) yields\nimproved performance for CAML (supervised) compared to training solely on ImageNet-964, likely\ndue to the increased variability in the sampled tasks. However, CAMeLU does not exhibit a similar\nperformance boost, as its task creation mechanism already introduces substantial variability, reduc-\ning the benefit of additional dataset diversity. Moreover, the large size of ImageNet-964 (around\n80% of the overall dataset), leads to its more frequent selection compared to the other datasets\nand limits the potential for performance gains from the other datasets. Consequently, to optimize\ncomputational resources and time, we conducted all the other experiments by training solely on\nImageNet-964.\nA.4\nABLATION STUDIES - FEATURE EXTRACTOR\nTo assess the impact of the pre-trained feature extractor, we evaluate CAMeLU with various extrac-\ntors pre-trained using different strategies. In particular, we compare the effectiveness of a ResNet-50\nencoder pre-trained in a supervised manner both on ImageNet-1k and on ImageNet-964, a ResNet-\n50 pre-trained on ImageNet-1k using two SSL strategies, i.e., SimCLR (Chen et al., 2020) and SwAV\n(Caron et al., 2020), and a ViT-B/16 architecture pre-trained with CLIP (Radford et al., 2021) on a\nlarge dataset with 400 million (image, text) pairs. The extractors were downloaded from the Hug-\nging Face (Wolf et al., 2019) and PyTorch Lightning Bolts (Borovec et al., 2022) websites, except\nfor the ResNet-50 architecture pre-trained on ImageNet-964. Results in Tab. 6a demonstrate that\nthe models pre-trained on ImageNet-1k exhibit significantly higher performance on miniImageNet,\ncompared to other datasets, primarily due to the data leakage issue described in Sect. 4.2. This issue\nis mitigated by pre-training the ResNet-50 architecture on ImageNet-964, which results in a drop\nin the performance on miniImageNet and CIFAR-fs due to the removal of data leakage, making it\n19\nPublished as a conference paper at ICLR 2025\nTable 6: Ablation study of feature extractors utilized in CAMeLU. The feature extractors include\nResNet-50 pre-trained on ImageNet-964 (ResNet50 (IN-964)), ResNet-50 pre-trained on ImageNet-\n1k (ResNet50 (IN-1k)), ResNet-50 pre-trained on ImageNet-1k with SimCLR and SwAV, as well\nas a ViT-B/16 architecture pre-trained with CLIP. All models are trained using CAMeLU on (a)\nImageNet-964 or (b) using a combination of ImageNet-964 + MSCOCO + Fungi (multi-dataset).\nThe symbol â€  indicates results that are affected by data leakage. Results show the mean and standard\ndeviations across three complete runs of the algorithms.\nminiImageNet\nCIFAR-fs\nCUB\nAircraft\nMeta-iNat\nExtractor\n5w1s\n5w5s\n5w1s\n5w5s\n5w1s\n5w5s\n5w1s\n5w5s\n5w1s\n5w5s\nResNet-50 (IN-964)\n76.51 Â± 0.79\n92.14 Â± 0.30\n61.79 Â± 0.59\n80.43 Â± 0.21\n65.52 Â± 0.37\n80.35 Â± 0.63\n33.17 Â± 0.94\n39.11 Â± 1.97\n57.27 Â± 0.39\n75.45 Â± 0.42\nResNet-50 (IN-1k)\n78.17 Â± 1.69â€ \n95.75 Â± 0.48â€ \n66.02 Â± 0.78\n84.40 Â± 0.64\n60.69 Â± 1.13\n79.08 Â± 0.75\n33.23 Â± 0.70\n40.05 Â± 0.85\n56.21 Â± 0.43\n74.35 Â± 0.21\nResNet-50 (IN-1k) - SimCLR\n56.10 Â± 1.16â€ \n79.45 Â± 2.37â€ \n46.14 Â± 1.24\n63.03 Â± 2.73\n36.85 Â± 2.69\n50.34 Â± 3.22\n24.30 Â± 1.06\n27.25 Â± 2.21\n42.61 Â± 0.41\n58.95 Â± 0.72\nResNet-50 (IN-1k) - SwAV\n60.16 Â± 0.70â€ \n84.32 Â± 0.34â€ \n56.81 Â± 0.84\n75.49 Â± 1.76\n44.39 Â± 0.75\n60.44 Â± 0.18\n27.82 Â± 0.62\n34.56 Â± 1.67\n47.31 Â± 0.41\n65.84 Â± 0.09\nViT-B/16 - CLIP\n76.44 Â± 0.51\n91.96 Â± 0.31\n69.74 Â± 0.95\n86.25 Â± 0.92\n61.05 Â± 1.91\n75.17 Â± 2.77\n37.82 Â± 2.14\n43.10 Â± 1.95\n61.22 Â± 0.67\n77.09 Â± 0.15\n(a) ImageNet-964 training\nminiImageNet\nCIFAR-fs\nCUB\nAircraft\nMeta-iNat\nExtractor\n5w1s\n5w5s\n5w1s\n5w5s\n5w1s\n5w5s\n5w1s\n5w5s\n5w1s\n5w5s\nResNet-50 (IN-964)\n76.56 Â± 0.36\n91.80 Â± 0.20\n62.28 Â± 0.69\n80.15 Â± 0.37\n65.06 Â± 0.82\n79.27 Â± 1.22\n31.89 Â± 1.43\n37.13 Â± 1.67\n57.36 Â± 0.33\n75.04 Â± 0.49\nResNet-50 (IN-1k)\n79.07 Â± 0.88â€ \n96.44 Â± 0.16â€ \n66.15 Â± 0.31\n84.90 Â± 0.42\n60.62 Â± 0.45\n79.26 Â± 0.20\n33.41 Â± 0.98\n41.23 Â± 1.14\n59.14 Â± 0.14\n74.31 Â± 0.51\nResNet-50 - SimCLR\n53.83 Â± 1.87\n78.10 Â± 1.94\n45.06 Â± 1.06\n61.90 Â± 0.33\n37.64 Â± 1.74\n51.40 Â± 1.70\n25.31 Â± 0.49\n28.87 Â± 0.99\n41.89 Â± 0.15\n58.87 Â± 0.36\nResNet-50 - SwAV\n58.82 Â± 0.34\n83.45 Â± 0.24\n57.33 Â± 0.57\n76.62 Â± 1.01\n44.79 Â± 0.24\n60.71 Â± 0.85\n27.30 Â± 0.77\n34.50 Â± 0.85\n47.18 Â± 0.35\n65.65 Â± 0.19\nViT-B/16 - CLIP\n77.92 Â± 1.89\n93.83 Â± 0.70\n78.04 Â± 0.91\n91.88 Â± 0.45\n74.08 Â± 1.81\n88.86 Â± 2.33\n49.21 Â± 2.46\n58.97 Â± 2.74\n67.95 Â± 1.25\n82.59 Â± 1.05\n(b) Multi-dataset training\ncomparable with the results on CLIP-ViT-B/16. For CLIP-ViT-B/16, however, thorough verifica-\ntion of potential data leakage was not possible due to the undisclosed nature of its training dataset.\nAs such, these results should be interpreted with caution. CLIP-ViT-B/16 stands out as the best-\nperforming method due to its dataset-agnostic nature and its ability to learn representations that\ngeneralize across a broad range of tasks. Furthermore, when training CAMeLU with a multi-dataset\napproach (Tab. 6b), as described in Appendix A.3, results for CLIP-ViT-B/16 improve further, high-\nlighting its applicability also to datasets significantly different from those used for training (Radford\net al., 2021). These findings demonstrate that CAMeLUâ€™s performance scales with the strength of\nthe feature extractor, indicating potential for further investigation as more robust feature extractors\nbecome available. However, in this work, we decided to use the ResNet-50 architecture to guarantee\na fair comparison with previous baselines.\nA.5\nEVALUATION OF THE TASK CREATION MECHANISM\n0\n20\n40\n60\n80\n100\nEpochs\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nValidation accuracy\nProposed startegy (mini)\nData augmentation (mini)\nProposed startegy (CUB)\nData augmentation (CUB)\nFigure 6: Validation accuracy on miniImageNet\n(mini) and CUB while training CAMeLU with\ntwo different task creation mechanisms.\nRed\nand purple curves are obtained with our proposed\nstrategy in Sect. 3.1, while orange and pink curves\nare obtained by applying only data augmentations\nbased on image manipulations to generate the sup-\nport and query samples. The training is performed\nusing ImageNet-964.\nTo assess the effectiveness of the task cre-\nation strategy employed in our proposed ap-\nproach, we conduct a comparative analysis of\nCAMeLUâ€™s performance under two different\ntask creation mechanisms.\nSpecifically, we\nevaluate CAMeLU when tasks are generated\nsolely using data augmentations for both the\nsupport and query sets, following a strategy\nsimilar to UMTRA (Khodadadeh et al., 2019),\nversus employing our proposed approach out-\nlined in Sect. 3.1. By applying our task cre-\nation strategy, we generate more complex tasks,\nmaking the generalization problem harder and\nthe in-context learner more robust (Chan et al.,\n2022; Singh et al., 2024). The results presented\nin Tab. 7a confirm our claim. Across all the\ndatasets, our proposed strategy enhances the\ngeneralization on cross-domain datasets such as\nCUB, Aircraft, and Meta-iNat.\nThis conclu-\nsion is further supported by Fig. 6, which shows\nthe validation accuracy on miniImageNet and\nCUB using the two mechanisms discussed be-\nfore, along with the CLIP-ViT feature extractor described in Appendix A.4. These results confirm\n20\nPublished as a conference paper at ICLR 2025\nTable 7: Ablation experiments of the proposed task creation mechanism by (a) generating tasks\nonly using data augmentations for the support and query set on ImageNet-964 and (b) applying\nk-means clustering on the ResNet-50 embeddings on miniImageNet. Results show the mean and\nstandard deviations across three complete runs of the algorithms in the 5-way 1-shot and 5-way 5-\nshot scenarios.\nminiImageNet\nCIFAR-fs\nCUB\nAircraft\nMeta-iNat\nMethod\n5w1s\n5w5s\n5w1s\n5w5s\n5w1s\n5w5s\n5w1s\n5w5s\n5w1s\n5w5s\nImageNet-964\nAugment\n78.20 Â± 0.38\n91.35 Â± 0.35\n64.30 Â± 0.31\n81.08 Â± 0.23\n62.19 Â± 1.29\n75.53 Â± 1.52\n31.90 Â± 1.69\n37.46 Â± 1.71\n56.46 Â± 0.53\n74.00 Â± 0.80\nProposed\n76.51 Â± 0.79\n92.14 Â± 0.30\n61.79 Â± 0.59\n80.43 Â± 0.21\n65.52 Â± 0.37\n80.35 Â± 0.63\n33.17 Â± 0.94\n39.11 Â± 1.97\n57.27 Â± 0.39\n75.45 Â± 0.42\n(a) Data augmentation vs. the proposed strategy\nminiImageNet\nCIFAR-fs\nCUB\nAircraft\nMeta-iNat\nMethod\n5w1s\n5w5s\n5w1s\n5w5s\n5w1s\n5w5s\n5w1s\n5w5s\n5w1s\n5w5s\nminiImageNet\nk-means\n75.86 Â± 1.27\n89.57 Â± 0.65\n46.14 Â± 1.96\n62.19 Â± 2.19\n33.76 Â± 3.58\n40.39 Â± 4.06\n24.48 Â± 3.34\n27.11 Â± 4.23\n36.32 Â± 1.68\n47.66 Â± 1.59\nProposed\n75.99 Â± 0.20\n90.38 Â± 0.21\n61.25 Â± 0.55\n78.79 Â± 0.21\n60.60 Â± 0.80\n74.77 Â± 1.70\n31.39 Â± 1.17\n36.52 Â± 0.88\n55.60 Â± 0.20\n72.12 Â± 0.35\n(b) k-means clustering vs. the proposed strategy\nTable 8: Evaluation of the proposed task creation strategy when applied to build pseudo-tasks on top\nof MAML. The results are compared with other MAML-based baselines for UML, such as UMTRA\nand CACTUs-MAML.\nminiImageNet\nCIFAR-fs\nCUB\nAircraft\nMeta-iNat\nMethod\n5w1s\n5w5s\n5w1s\n5w5s\n5w1s\n5w5s\n5w1s\n5w5s\n5w1s\n5w5s\nCACTUs-MAML\n43.30\n54.21\n42.00\n56.64\n31.19\n36.81\n24.06\n27.26\n20.13\n21.84\nUMTRA\n39.93\n50.73\n32.93\n46.13\n27.06\n36.60\n22.40\n31.73\n28.96\n37.12\nProposed + MAML\n34.04\n46.13\n37.02\n52.00\n31.22\n41.54\n26.12\n34.34\n30.94\n42.43\nthat our task creation strategy is more robust, particularly in cross-domain evaluations, even when\nstronger feature extractors are utilized.\nAdditionally, we experimented with k-means clustering as an alternative to the proposed task cre-\nation strategy. Inspired by CACTUs (Hsu et al., 2018), we applied clustering on the embeddings\ngenerated by the feature extractor to generate pseudo-labels. The results are presented in Tab. 7b\nwhen training on miniImageNet due to the high computational cost of k-means and indicate that our\nproposed mechanism generalizes better across domains. Furthermore, k-means clustering requires\nan insight into the number of classes in the training dataset, as choosing a high number of clus-\nters would lead to a lack of samples per class, whereas a low number may hinder generalization.\nIn contrast, CAMeLU does not rely on such assumptions, enhancing its robustness compared to\nclustering-based approaches.\nFinally, we show that the benefits of our task creation strategy extend beyond CAMeLU. In Tab. 8 we\napply our proposed mechanism to generate pseudo-tasks on top of MAML (Finn et al., 2017). This\nallows for a direct comparison with MAML-based approaches, such as UMTRA (Khodadadeh et al.,\n2019) and CACTUs-MAML (Hsu et al., 2018), by replacing their original task creation mechanisms.\nThe increased performance of our strategy applied to the baselines demonstrates its superiority over\nprevious task creation methods. It may be objected that CACTUs-MAML achieves higher perfor-\nmance on miniImageNet and CIFAR-fs. However, this is caused by the use of a feature extractor\npre-trained on ImageNet-1k, which introduces an unfair advantage by leaking information about the\ntest data into the training phase. This performance gap narrows when the test distribution deviates\nfrom the training data (e.g., CIFAR-fs) and disappears for datasets with low correlation to ImageNet-\n1k, supporting our hypothesis. Indeed, on datasets that share low similarity with ImageNet-1k, our\nmethod consistently outperforms both UMTRA and CACTUs-MAML. While these results high-\nlight the strength of our task creation strategy, the performance still remains significantly lower than\nCAMeLU, especially in cross-domain scenarios. This emphasizes the critical role of combining\nour robust task creation mechanism with the in-context learning capabilities of transformer-based\narchitectures to achieve superior performance.\n21\nPublished as a conference paper at ICLR 2025\n(a) Image Ëœxn,j (b)\nRandom\nimage zj\n(c) Query im-\nage x(qr)\nj\nwith\npixel level mix\n(d) Query im-\nage x(qr)\nj\nwith\npatch level mix\nFigure 7: Visualization of a query image Ëœxn,j generated by\nmixing (a) the support image and (b) a randomly sampled\nimage at (c) the pixel level or at (d) the patch level using\nÎ» = 0.49.\nTable\n9:\nmSSIM\nvalues\ncomputed as the average be-\ntween SSIM( Ëœxn,j, x(qr)\nj\n) and\nSSIM(zj, x(qr)\nj\n) when x(qr)\nj\nis\nobtained using a pixel level or a\npatch level mixing strategy with\nÎ» = 0.25 and Î» = 0.49.\nÎ» = 0.25\nÎ» = 0.49\nPixel level\n0.60\n0.61\nPatch level\n0.56\n0.57\nA.6\nQUERY SAMPLES GENERATION STRATEGY\nWe also conducted additional experiments to validate the choice of utilizing Eq. 1 for generating\nquery samples. In particular, we compare the results obtained as a linear combination of the aug-\nmented image Ëœxn,j with the randomly sampled image zj (pixel level), as in Eq. 1, and at the patch\nlevel, as in Yun et al. (2019). Specifically, for the latter, we randomly select a patch from zj with\nan area ratio proportional to Î», and we paste it into Ëœxn,j. Fig. 7 illustrates an example of these\ntwo techniques by mixing two images sampled from ImageNet-964. As shown in Fig. 7c, merging\nthe images at the pixel level results in a mixed image where some information from Ëœxn,j and zj is\nretained in every part of the image. Contrarily, in Fig. 7d, there is no information about Ëœxn,j in the\nlower left corner, forcing the network to attend only to the upper right part of the image to classify it\nwith the same class as Ëœxn,j. Therefore, we hypothesize that the pixel level strategy is more suitable\nfor our approach as the goal is to attend to the whole image to extract robust features that allow the\nmodel to classify the query image with the same class as the support one while ensuring diversity\nbetween the two. To validate this, we utilized the Structural Similarity Index (SSIM) (Wang et al.,\n2004). SSIM is used as a metric to measure the similarity between two given images based on\nthree image features: luminance, contrast, and structure. Formally, considering x and y two given\nimages, SSIM is calculated as follows:\nSSIM(x, y) =\n\u0014 2ÂµxÂµy + c1\nÂµ2x + Âµ2y + c1\n\u0015Î±\n+\n\u0014 2ÏƒxÏƒy + c2\nÏƒ2x + Ïƒ2y + c2\n\u0015Î²\n+\n\u0014 Ïƒxy + c3\nÏƒxÏƒy + c3\n\u0015Î³\n(5)\nwhere Âµ represents the mean of an image, Ïƒ denotes the standard deviation, c1, c2, c3 are constant\nvalues, and Î±, Î², Î³ denote the relative importance of each metrics. By assuming Î± = Î² = Î³ = 1\nand c2 = c2/2, we get\nSSIM(x, y) =\n(2ÂµxÂµy + c1)(2Ïƒxy + c2)\n(Âµ2x + Âµ2y + c1)(Ïƒ2x + Ïƒ2y + c2).\n(6)\nInstead of applying the above formula all over the image at once, (Wang et al., 2004) proposed\na local variant that consists of computing the SSIM index locally and averaging these values to\nobtain the global SSIM value. For our purpose, we computed this metric between each of the two\nimages used for the generation, i.e., Ëœxn,j and zj, and the resulting mixed image, i.e., x(qr)\nj\n. We then\naverage the results, obtaining an indicator, denoted as mSSIM, of how similar the query image is\nwith the images used for the generation, or in other words, how much local information is retained\nfrom Ëœxn,j and zj into x(qr)\nj\n. Results are shown in Tab. 9 for Î» = 0.25 and Î» = 0.49, confirming\nthe hypothesis that, even for high Î» values, mixing at the pixel level guarantees more information\nretained across the whole image compared to using a patch level strategy. This is also confirmed by\nthe results in Tab. 10, which shows a decrease in the performance when CAMeLU is trained with\nthe patch level strategy for query generation.\nWe also ablate the values of the Î± and Î² parameters used in the Beta distribution from which Î»\nis sampled. Tab. 10 presents the results for different values of Î± and Î² when Î» âˆ¼Beta(Î±, Î²) and\nÎ» âˆˆ(0, 0.5). The results indicate that the optimal choice for CAMeLU is to select Î± = 1, Î² = 1,\n22\nPublished as a conference paper at ICLR 2025\nTable 10: Accuracy results of CAMeLU when trained with different strategies for generating the\nquery samples. Pixel level mix refers to the scenario where query samples are generated with Eq. 1,\nwhile patch level mix refers to a strategy similar to the one proposed in Yun et al. (2019). Results are\nreported in the 5-way 5-shot scenario with Î» âˆ¼Beta(Î±, Î²) and different Î± and Î² values. Results\nshow the mean and standard deviations across three complete runs of the algorithms.\nminiImageNet\nCIFAR-fs\nCUB\nAircraft\nPixel level mix\nÎ± = 0.1, Î² = 0.1\n90.68 Â± 0.67\n75.88 Â± 2.08\n76.31 Â± 2.44\n35.31 Â± 2.57\nÎ± = 0.5, Î² = 0.5\n91.34 Â± 0.24\n77.70 Â± 0.73\n79.52 Â± 0.73\n37.56 Â± 1.92\nÎ± = 1, Î² = 1\n92.14 Â± 0.30\n80.43 Â± 0.21\n80.35 Â± 0.63\n39.11 Â± 1.97\nÎ± = 2, Î² = 5\n90.68 Â± 1.02\n77.00 Â± 1.40\n79.62 Â± 2.50\n38.02 Â± 1.85\nÎ± = 5, Î² = 5\n90.63 Â± 0.30\n78.12 Â± 0.15\n79.71 Â± 0.42\n37.57 Â± 0.11\nPatch level mix\nÎ± = 1, Î² = 1\n91.25 Â± 0.50\n77.80 Â± 0.45\n76.12 Â± 1.06\n33.60 Â± 1.27\nTable 11: Computational and time complexity of CAMeLU in comparison with PsCo. The compar-\nison is performed considering the time required for the task creation, the training time (expressed in\ntime per epoch), the inference time on a single task, and the GPU and CPU memory usage during\ntraining and inference.\nTime task construction\nTraining time\nInference time\n(ms)\n(ms/epoch)\n(ms/task)\nPsCO\n20772\n4613656\n605\nCAMeLU\n1376\n153000\n57\nGPU training\nCPU training\nGPU inference\nCPU inference\n(MiB)\n(MiB)\n(MiB)\n(MiB)\nPsCO\n43904\n20904\n1630\n2061\nCAMeLU\n6250\n2588\n3224\n1667\nwhich appears to be a uniform distribution. Additionally, Î± = 2, Î² = 5 also yields comparable\nresults, highlighting the importance of selecting a sufficiently small Î» to ensure the incorporation of\nsufficient information from Ëœxn,j into x(qr)\nj\nfacilitating the modelâ€™s ability to classify the latter with\nthe same class as x(sp)\nn,i .\nA.7\nCOMPUTATIONAL COMPLEXITY AND RESOURCES USAGE\nIn this Sect., we analyze the computational and time complexity of CAMeLU and we compare it\nwith PsCo Jang et al. (2022). Tab. 11 presents the time required for task generation, model training,\nand inference, along with GPU and CPU memory usage. The results demonstrate that CAMeLU is\nnot only faster than PsCo but also significantly more memory efficient. Notably, CAMeLU requires\nonly 57ms for task inference, making it particularly suitable for real-time applications.\nThe computational complexity of CAMeLU is primarily attributed to the transformer architecture,\nwhich is known for its computational demands due to the self-attention mechanism. The transformer\nmodel has a computational complexity of O(n2 Â· d) per layer (Vaswani et al., 2017), where n is the\nsequence length and d is the hidden dimension. In our context, n includes both the support samples\nand the query sample. Consequently, the total computational complexity for evaluating Q queries is\nO(Q Â· (NK + 1)2 Â· d), where N is the number of classes, K the number of shots, NK + 1 indicates\none query per input sequence, and Q is the total number of queries. This results in a quadratic\ncomplexity in the number of support samples which can be computationally demanding. However,\nwe have demonstrated in Tab. 1 that CAMeLU achieves good performance even with only K = 1\nsupport sample per class. Additionally, further experiments with only one query sample per episode\n23\nPublished as a conference paper at ICLR 2025\n(a) CAMeLU on miniImageNet\n(b) CAMeLU on CUB\n(c) CAML on miniImageNet\n(d) CAML on CUB\nFigure 8: Comparison of logistic function approximations and phase boundaries for learning and\ngeneralization phases in CAMeLU and CAML for miniImageNet and CUB datasets.\nand one support sample per class (i.e., N = 5, K = 1, Q = 1) yield results of 80.74 Â± 0.65 for\nminiImageNet, 63.07 Â± 1.14 for CIFAR-fs, 54.84 Â± 1.41 for CUB, 30.32 Â± 0.76 for Aircraft, and\n55.76 Â± 0.08 for Meta-iNat. These results are comparable to those reported in Tab. 1 for the 5-ways\n1-shot scenario using 25 queries, highlighting that a single query is sufficient for good performance,\nas also demonstrated in CAML.\nA.8\nQUANTITATIVE ANALYSIS OF LEARNING PHASES\nTo quantitatively assess the number of epochs required to enter the generalization phase, we propose\nto approximate the validation accuracy curves with the generalized logistic function\nf(x) = a +\nd âˆ’a\n1 + eâˆ’b(xâˆ’x0) = a +\nd âˆ’a\n1 + ceâˆ’bx ,\n(7)\nwhere parameters a, b, c, d are responsible for particular features of the logistic function. Parameters\na and d indicate the lower and, respectively, the upper asymptote. Parameter b is the logistic growth\nrate, and finally, parameter c = ebx0 is related to the inflection point x0 at which the maximum\ngrowth of the function occurs. To find the best fitting logistic curve we use a standard regression\nfunction. The logistic function is strictly increasing, thus the derivative, which is given by f â€²(x) =\nbc(dâˆ’a)eâˆ’bx\n(1+ceâˆ’bx)2 , is always positive. The derivative firstly increases (from values close to zero), and\nafter reaching its maximum value at the inflection point, it decreases. To determine the bounds for\nreaching the learning and generalization phases, we find the values for which the derivative is equal\nto a given fraction of the maximum possible growth rate. After testing several cases, the results\nshow that the choice of this threshold does not affect the relation between the phasesâ€™ boundaries.\nTherefore, we decided to conduct our analysis for 20% of maximum growth rate.\nFigures 8a and 8b show the results for CAMeLU on the miniImageNet and CUB datasets, respec-\ntively. For miniImageNet, we obtain the approximation function to be f(x) = 0.04+\n0.54\n1+9636eâˆ’0.43x .\nMoreover, the number of epochs where the learning phase begins is 15 and the number of epochs\nwhere the generalization phase begins is 29. On the other hand, for CUB dataset, the approximation\nfunction is f(x) = 0.01 +\n0.48\n1+25530eâˆ’0.47x , and the number of epochs where the following phases\nbegin is 16 and 28.\n24\nPublished as a conference paper at ICLR 2025\nFigures 8c and 8d show the results for CAML on the miniImageNet and the CUB datasets, re-\nspectively. Similarly, we obtain the approximation function for the miniImageNet to be f(x) =\n0.06 +\n0.56\n1+1326eâˆ’0.25x .\nand the number of epochs where the learning phase and the general-\nization phase begin is 18 and 42. Finally, for the CUB dataset, the approximation function is\nf(x) = âˆ’0.07 +\n0.50\n1+648eâˆ’0.13x , and the number of epochs where the following phases begin is\n29 and 74.\nRemarkably, CAML requires more training time to reach the generalization phase than CAMeLU.\nThis difference likely arises from CAMeLUâ€™s task creation mechanism, which generates tasks with\nhigh cross-task variance. This strategy acts as a form of task augmentation, facilitating quicker\ngeneralization to unseen tasks.\nA.9\nLIMITATIONS\nDespite the promising results demonstrated by our novel UML approach on several datasets, some\nlimitations remain. Its applicability and robustness in real-world scenarios with diverse and noisy\ndata remain to be thoroughly evaluated. In real-world applications, data can be incomplete, mis-\nlabeled, or drawn from significantly different distributions, leading to potential degradation in the\nmodelâ€™s performance in the presence of noisy or corrupted data. Additionally, the feature extractor\nused in CAMeLU is pre-trained in a supervised manner. While the pre-training dataset is indepen-\ndent of the data seen at inference, replacing it with an extractor pre-trained using an SSL strategy\ncould make the pipeline fully unsupervised, albeit at the price of performance degradation. Lastly,\nthe proposed approach is designed to handle a fixed number of classes (ways) per task during train-\ning and testing, requiring knowing the value of N in advance. Modifications to the task creation and\ntraining process would be necessary to extend our approach to handle an arbitrary number of ways.\nA.10\nCOMPLETE RESULTS WITH STANDARD DEVIATIONS\nTable 12: Performance comparison on miniImageNet, CIFAR-fs, CUB, Aircraft, and Meta-iNat\ndatasets for 5-way 1-shot and 5-way 5-shot scenarios. Cross-domain approaches are trained using\nImageNet-964 and a ResNet-50 feature extractor. The symbol â€  indicates results that are affected by\ndata leakage. The bold font highlights the best performing UML approach for each setting. Results\nshow the mean and standard deviations across three complete runs of the algorithms. This table\nrefers to Tab. 1 in Sect. 4.3.\nminiImageNet\nCIFAR-fs\nCUB\nAircraft\nMeta-iNat\nMethod\n5w1s\n5w5s\n5w1s\n5w5s\n5w1s\n5w5s\n5w1s\n5w5s\n5w1s\n5w5s\nIn-Domain\nCACTUs-MAML\n43.30 Â± 0.29\n54.21 Â± 1.00\n42.00 Â± 1.47\n56.64 Â± 0.49\n31.19 Â± 0.37\n36.81 Â± 0.68\n24.06 Â± 0.78\n27.26 Â± 0.04\n20.13 Â± 0.44\n21.840 Â± 0.14\nCACTUs-ProtoNet\n48.85 Â± 0.69\n62.52 Â± 0.71\n50.90 Â± 0.46\n64.52 Â± 0.94\n33.93 Â± 0.37\n44.41 Â± 1.31\n26.27 Â± 0.28\n30.88 Â± 0.51\n27.30 Â± 0.12\n29.08 Â± 0.13\nUMTRA\n39.93 Â± 1.15\n50.73 Â± 0.67\n32.93 Â± 1.68\n46.13 Â± 2.81\n27.06 Â± 1.41\n36.6 Â± 2.43\n22.40 Â± 3.42\n31.73 Â± 2.25\n28.96 Â± 0.32\n37.12 Â± 0.21\nMeta-GMVAE\n55.38 Â± 0.90â€ \n65.10 Â± 0.64â€ \n52.02 Â± 0.88\n64.18 Â± 0.62\n33.59 Â± 0.63\n39.09 Â± 0.57\n24.83 Â± 0.51\n27.60 Â± 0.52\n34.22 Â± 0.58\n40.23 Â± 0.54\nPsCo\n47.29 Â± 0.41\n64.85 Â± 0.38\n42.21 Â± 0.46\n62.92 Â± 0.44\n33.09 Â± 0.44\n51.02 Â± 0.42\n26.19 Â± 0.30\n38.80 Â± 0.38\n36.97 Â± 0.39\n55.88 Â± 0.41\nCross-Domain\nPsCo\n67.89 Â± 0.48\n90.17 Â± 0.23\n53.34 Â± 0.49\n76.22 Â± 0.40\n43.35 Â± 0.47\n70.19 Â± 0.46\n29.87 Â± 0.36\n38.20 Â± 0.39\n46.21 Â± 0.44\n70.05 Â± 0.45\nCAMeLU\n76.51 Â± 0.79\n92.14 Â± 0.30\n61.79 Â± 0.59\n80.43 Â± 0.21\n65.52 Â± 0.37\n80.35 Â± 0.63\n33.17 Â± 0.94\n39.11 Â± 1.97\n57.27 Â± 0.39\n75.45 Â± 0.42\nCAML (supervised)\n81.75 Â± 0.18\n92.31 Â± 0.11\n59.44 Â± 0.63\n75.27 Â± 0.77\n54.63 Â± 1.78\n66.81 Â± 3.12\n28.92 Â± 0.37\n32.06 Â± 0.43\n50.86 Â± 0.50\n67.07 Â± 0.39\nTable 13: Accuracy results obtained training PsCo, BECLR, and CAMeLU with a small-scale\ndataset, namely miniImageNet, denoted as (mini) in the table. Results show both in-domain per-\nformance (on the test set of miniImageNet) and cross-domain performance on CIFAR-fs, CUB, Air-\ncraft, and Meta-iNat. The mean and standard deviation across three complete runs of the algorithms.\nThis table refers to Tab. 2 in Sect. 4.5.\nminiImageNet\nCIFAR-fs\nCUB\nAircraft\nMeta-iNat\n5w1s\n5w5s\n5w1s\n5w5s\n5w1s\n5w5s\n5w1s\n5w5s\n5w1s\n5w5s\nPsCo (mini)\n47.29 Â± 0.41\n64.85 Â± 0.38\n42.21 Â± 0.46\n62.92 Â± 0.44\n33.09 Â± 0.44\n51.02 Â± 0.42\n26.19 Â± 0.30\n38.80 Â± 0.38\n36.97 Â± 0.39\n55.88 Â± 0.41\nBECLR (mini)\n81.04 Â± 1.24\n87.88 Â± 0.66\n57.05 Â± 1.58\n72.82 Â± 0.95\n42.47 Â± 1.30\n58.03 Â± 1.12\n27.48 Â± 0.83\n38.46 Â± 0.95\n49.87 Â± 1.35\n65.05 Â± 1.07\nCAMeLU (mini)\n75.99 Â± 0.20\n90.38 Â± 0.21\n61.25 Â± 0.55\n78.79 Â± 0.21\n60.60 Â± 0.80\n74.77 Â± 1.70\n31.39 Â± 1.17\n36.52 Â± 0.88\n55.60 Â± 0.20\n72.12 Â± 0.35\n25\nPublished as a conference paper at ICLR 2025\nTable 14: Comparison between CAMeLU and SSL approaches for the 5-way 1-shot and 5-way 5-\nshot scenario on miniImageNet, CIFAR-fs, CUB, Aircraft, and Meta-iNat. The symbol â€  indicates\nresults that are affected by data leakage. Results show the mean and standard deviations across three\ncomplete runs of the algorithms. This table refers to Tab. 3 in Sect. 4.6.\nminiImageNet\nCIFAR-fs\nCUB\nAircraft\nMeta-iNat\nMethod\n5w1s\n5w5s\n5w1s\n5w5s\n5w1s\n5w5s\n5w1s\n5w5s\n5w1s\n5w5s\nSimCLR\n83.32 Â± 0.23â€ \n94.86 Â± 0.61â€ \n64.52 Â± 0.69\n84.36 Â± 0.40\n47.35 Â± 0.53\n66.87 Â± 0.82\n29.36 Â± 0.90\n39.99 Â± 0.86\n52.44 Â± 0.47\n73.19 Â± 0.43\nSwAV\n74.83 Â± 0.71â€ \n94.96 Â± 0.91â€ \n66.97 Â± 0.15\n87.14 Â± 0.10\n47.84 Â± 0.31\n69.31 Â± 0.01\n30.33 Â± 0.31\n47.43 Â± 0.11\n53.57 Â± 0.82\n74.53 Â± 0.92\nCAMeLU\n76.51 Â± 0.79\n92.14 Â± 0.30\n61.79 Â± 0.59\n80.43 Â± 0.21\n65.52 Â± 0.37\n80.35 Â± 0.63\n33.17 Â± 0.94\n39.11 Â± 1.97\n57.27 Â± 0.39\n75.45 Â± 0.42\n26\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2024-05-25",
  "updated": "2025-02-10"
}