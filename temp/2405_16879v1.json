{
  "id": "http://arxiv.org/abs/2405.16879v1",
  "title": "Unsupervised Generative Feature Transformation via Graph Contrastive Pre-training and Multi-objective Fine-tuning",
  "authors": [
    "Wangyang Ying",
    "Dongjie Wang",
    "Xuanming Hu",
    "Yuanchun Zhou",
    "Charu C. Aggarwal",
    "Yanjie Fu"
  ],
  "abstract": "Feature transformation is to derive a new feature set from original features\nto augment the AI power of data. In many science domains such as material\nperformance screening, while feature transformation can model material formula\ninteractions and compositions and discover performance drivers, supervised\nlabels are collected from expensive and lengthy experiments. This issue\nmotivates an Unsupervised Feature Transformation Learning (UFTL) problem. Prior\nliterature, such as manual transformation, supervised feedback guided search,\nand PCA, either relies on domain knowledge or expensive supervised feedback, or\nsuffers from large search space, or overlooks non-linear feature-feature\ninteractions. UFTL imposes a major challenge on existing methods: how to design\na new unsupervised paradigm that captures complex feature interactions and\navoids large search space? To fill this gap, we connect graph, contrastive, and\ngenerative learning to develop a measurement-pretrain-finetune paradigm for\nUFTL. For unsupervised feature set utility measurement, we propose a feature\nvalue consistency preservation perspective and develop a mean discounted\ncumulative gain like unsupervised metric to evaluate feature set utility. For\nunsupervised feature set representation pretraining, we regard a feature set as\na feature-feature interaction graph, and develop an unsupervised graph\ncontrastive learning encoder to embed feature sets into vectors. For generative\ntransformation finetuning, we regard a feature set as a feature cross sequence\nand feature transformation as sequential generation. We develop a deep\ngenerative feature transformation model that coordinates the pretrained feature\nset encoder and the gradient information extracted from a feature set utility\nevaluator to optimize a transformed feature generator.",
  "text": "Unsupervised Generative Feature Transformation via Graph\nContrastive Pre-training and Multi-objective Fine-tuning\nWangyang Ying\nArizona State University\nTempe, Arizona, USA\nwangyang.ying@asu.edu\nDongjie Wang\nUniversity of Kansas\nLawrence, Kansas, USA\nwangdongjie@ku.edu\nXuanming Hu\nArizona State University\nTempe, Arizona, USA\nsolomonhxm@asu.edu\nYuanchun Zhou\nComputer Network Information\nCenter, Chinese Academy of Sciences\nBeijing, China\nzyc@cnic.cn\nCharu C. Aggarwal\nIBM T. J. Watson Research Center\nYorktown Heights, New York, USA\ncharu@us.ibm.com\nYanjie Fuâ€ \nArizona State University\nTempe, Arizona, USA\nyanjie.fu@asu.edu\nABSTRACT\nFeature transformation is to derive a new feature set from original\nfeatures to augment the AI power of data. In many science domains\nsuch as material performance screening, while feature transfor-\nmation can model material formula interactions and compositions\nand discover performance drivers, supervised labels are collected\nfrom expensive and lengthy experiments. This issue motivates an\nUnsupervised Feature Transformation Learning (UFTL) problem.\nPrior literature, such as manual transformation, supervised feed-\nback guided search, and PCA, either relies on domain knowledge or\nexpensive supervised feedback, or suffers from large search space,\nor overlooks non-linear feature-feature interactions. UFTL imposes\na major challenge on existing methods: how to design a new un-\nsupervised paradigm that captures complex feature interactions\nand avoids large search space? To fill this gap, we connect graph,\ncontrastive, and generative learning to develop a measurement-\npretrain-finetune paradigm for UFTL. For unsupervised feature set\nutility measurement, we propose a feature value consistency preser-\nvation perspective and develop a mean discounted cumulative gain\nlike unsupervised metric to evaluate feature set utility. For unsu-\npervised feature set representation pretraining, we regard a feature\nset as a feature-feature interaction graph, and develop an unsu-\npervised graph contrastive learning encoder to embed feature sets\ninto vectors. For generative transformation finetuning, we regard a\nfeature set as a feature cross sequence and feature transformation as\nsequential generation. We develop a deep generative feature trans-\nformation model that coordinates the pretrained feature set encoder\nand the gradient information extracted from a feature set utility\nevaluator to optimize a transformed feature generator. Finally, we\nconduct extensive experiments to demonstrate the effectiveness,\nefficiency, traceability, and explicitness of our framework. Our code\nand data are available at https://shorturl.at/pKQU5.\nKEYWORDS\nunsupervised feature transformation, representation learning\n1\nINTRODUCTION\nFeature transformation (FT) is to derive new features from original\nfeatures to reconstruct a transformed feature space (e.g. [ğ‘“1, ğ‘“2] â†’\nâ€  Corresponding Author.\nInput\nf1\nf2\nOriginal Feature Space\nGenerate\nTransformed Feature Space\nf1/f2\nf1 âˆ’f2\n(f1 + f2)/f1\nUnsupervised Measurement\nEmbedding Space\nGraph Contrastive Learning\nFeature-Feature \nSimilarity Graph\nFigure 1: Unsupervised Feature Transformation Learning.\n[ ğ‘“1\nğ‘“2 , ğ‘“1 âˆ’ğ‘“2, ğ‘“1+ğ‘“2\nğ‘“1 ]). As an essential task of data-centric AI, FT is\npractical and effective in industrial deployments, because it can\naugment the AI power of data (e.g., structural, predictive, inter-\naction, and expression levels) to achieve better predictive perfor-\nmance even with simple models. In many practices, FT is conducted\neither by human experts, or by machine-assisted search guided\nthrough downstream task feedback. However, in certain domains,\nsuch as material performance screening, although FT can model\nmaterial formula interactions and compositions to discover material\nperformance drivers, supervised labels are collected from lengthy\nand expensive experiments. This issue motivates a new learning\nproblem: Unsupervised Feature Transformation Learning (UFTL).\nSolving UFTL can reduce data costs, model complex feature-feature\ninteractions, learn from non-labeled data, and improve model gen-\neralization capability.\nExisting systems only partially solve UFTL: 1) manual trans-\nformations are effective and explicit, but not generalizable and\nincomplete, as they heavily rely on domain and empirical experi-\nences. 2) supervised transformations include exhaustive-expansion-\nreduction approaches [11, 12, 14, 16, 19] and iterative-feedback-\nimprovement approaches [15, 25, 28] that are enabled by reinforce-\nment, genetic algorithms, or evolutionary algorithms. Such methods\ncan search optimal feature sets in a vast discrete space, but they face\nexponentially growing combination possibilities, are computation-\nally costly, hard to converge, and unstable. Moreover, they rely on\nlabeled data, which limits their applicability. 3) unsupervised trans-\nformations, such as Principal Components Analysis (PCA) [21, 26],\ndonâ€™t rely on labeled data. But, PCA is based on a strong assumption\nof straight linear feature correlation, only uses two cross opera-\ntions (i.e., addition/subtraction) to generate new features, and only\nreduces (canâ€™t increase) feature space dimensionality.\nThe emergence of LLM (e.g., ChatGPT) shows that mechanism-\nunknown human language knowledge can be embedded into a\nlarge embedding space and model the world as generative AI to\narXiv:2405.16879v1  [cs.LG]  27 May 2024\nautoregress the next word. Following a similar spirit, we believe\nthat mechanism-unknown feature space knowledge can be em-\nbedded into large sequential foundational models for generating\na transformed feature set. For example, a transformed feature set\n( ğ‘“1\nğ‘“2 , ğ‘“1 âˆ’ğ‘“2, ğ‘“1+ğ‘“2\nğ‘“1 ) is seen as a feature cross sequence â€œğ‘“1/ğ‘“2, ğ‘“1 âˆ’\nğ‘“2, (ğ‘“1 + ğ‘“2)/ğ‘“1EOSâ€. This provides a great potential to transform\nthe traditional ways that we solve UFTL.\nOur contribution: a graph, contrastive, and generative\nlearning perspective. We connect graph, contrastive, and genera-\ntive learning to develop a measurement-pretrain-finetune paradigm,\nas shown in Figure 1. Specifically, 1) unsupervised feature set utility\nmeasurement: Under the unsupervised setting, downstream task\nfeedback and labeled data are not available for measuring feature\nset utility. We propose a feature value consistency preservation\nangle. The insight is that: if two data instances (i.e., rows) are sim-\nilar, both instances tend to be from the same class, therefore the\ntwo element values of a specific feature over the two instances\nshould be close to each other. We reformulate feature set utility as\ninformation gain of total feature value consistency from the lens of\ninstance pair similarity. 2) unsupervised graph contrastive pretrain-\ning: Aside from feature set utility, feature set representation needs\nto be learned in an unsupervised fashion. We see a transformed\nfeature set as a feature-feature similarity graph, and reformulate\nthe feature set2vec problem into an unsupervised graph contrastive\nlearning task. By seeing a feature set as a graph rather than just a\nfeature cross sequence, the graph contrastive pretraining can better\nmodel feature-feature interactions, learn a feature set level embed-\nding space, and accelerate later finetuning and training convergence.\n3) multi-objective finetuning: We regard a feature set as a feature\ncross sequence, and feature transformation as sequential feature\ncross generation. We find that: (i) deep sequential model can distill\nfeature knowledge in the continuous embedding space, in order to\ndrive autoregressive generation of transformation sequences; (ii)\nnavigating optimization and generation in the continuous embed-\nding space via gradient ascents can convert classic discrete search\nformulation into differentiable continuous optimization, and avoid\nlarge discrete search space.\nSummary of technical solution. Inspired by these insights,\nthis paper presents a principled and generic framework for the\nUFTL problem by the measurement-pretrain-finetune paradigm.\nSpecifically, given an original dataset, we can explore various fea-\nture combinations of original features to prepare different feature\nsets. In the measurement stage, we generalize the idea of mean dis-\ncounted cumulative gain to feature set utility measurement. We\nsee â€œinformation gainâ€ as the value consistency of a specific feature\nbetween two close data instances, and â€œcumulativeâ€ as the sum of\nthe consistency gains of all data instance pairs. â€œdiscountedâ€ means\nthat: when a feature is of high variance, its element values tend\nto be different. Under such a situation, if the element values of\nsuch a feature over any two close instances still stay consistent,\nthe cumulative gain of such a high-variance feature should be aug-\nmented. â€œmeanâ€ means that we average the cumulative discounted\ngain of all features as the final utility of a feature set. In the pre-\ntrain stage, we regard a feature set as a feature-feature similarity\ngraph and develop a contrastive GNN to learn feature set embed-\nding. In particular, to prepare contrastive pairs, we apply attribute\nmask and edge perturbation to feature-feature similarity graphs to\ngenerate augmented graphs. An augmented graph will pair with a\nsource augmented graph as a positive pair; an augmented graph\nwill pair with a non-sourced augmented graph as a negative pair.\nWe optimize the contrastive loss by enforcing that the embeddings\nof two augmented graphs from the same source graph should be\nsimilar, and different otherwise. In the fine-tune stage, we develop a\ndeep generative feature transformation model. This model includes\nan encoder, a decoder, and an evaluator. The set2vec encoder is\npretrained by and loaded from the pretrain stage to map a feature\nset to an embedding vector; the vec2seq decoder is to decode an em-\nbedding vector to generate a feature cross sequence that represents\na feature set; the evaluator that predicts the feature set utility given\nfeature set embedding can provide gradient-steered optimization\nto find the optimal transformed feature set embedding, by jointly\noptimizing the losses of decoder and evaluator.\n2\nPROBLEM STATEMENT\nReinforcement Learning (RL) based Data Collector. The suc-\ncess demonstrated by [28] highlights that feature learning knowl-\nedge can effectively be modeled using multi-agent systems. Thus,\nwe employ its framework to automatically collect high-quality\ntraining data using our defined unsupervised feature set utility\nmeasurement for transformation embedding space. More details\nabout the data collector are described in Appendix 8.1.\nImportant Definitions. 1) Feature Cross, which means we apply\nmathematical operations to original features to generate a new\nfeature (e.g., [ğ‘ ğ‘–ğ‘›(ğ‘“1 + ğ‘“2)/ğ‘“3 âˆ’ğ‘“4]). 2) Explored Feature Set. We com-\nbine multiple feature crosses to construct an explored feature set\n(e.g., [ğ‘ ğ‘–ğ‘›(ğ‘“1 + ğ‘“2)/ğ‘“3 âˆ’ğ‘“4, ğ‘“1 + ğ‘’ğ‘¥ğ‘(ğ‘“2)]). 3) Feature Cross Sequence.\nFigure 2(a) shows that we use a token sequence to represent the\nexplored feature set. <SOS>, <SEP>, and <EOS> indicate the start,\nseparate, and end tokens respectively. To use fewer tokens and\nreduce learning difficulty, this sequence can be represented by a\npostfix expression, as shown in Figure 2(b). In this paper, the term\n\"feature cross sequence\" refers to the representation of a trans-\nformed feature set using a postfix expression of token sequence.\n(a) Token Sequence\n(b) Feature Cross Sequence: Postï¬x Expression of Token Sequence\nSOS\n/\n+\nâˆ’\nsin\nexp\nf1 f2\nf3\nf4\nSEP\n+\nf1\nf2\nEOS\nSOS\n)\n/\n+\nâˆ’\nsin\nexp\nf1\nf2\n8\neFDEq1/kzb9xkuxBowUNRVU3V1BIrg2rvlFJaWV1bXiuljc2t7Z3y7l5Tx6li2GCxiFU7oBoFl9gw3AhsJwpFAhsBaObqd96RKV5LB/MOE/ogPJQ86osdJ92DvtlStu1Z2B/CVeTiqQo94rf3b7MUsjlIYJqnXHcxPjZ1QZzgROSt1UY0LZiA6wY6mkEWo/m506IUdW6ZMwVrakITP150RGI63HUWA7I2qGetGbiv\n95ndSEV37GZIalGy+KEwFMTGZ/k36XCEzYmwJZYrbWwkbUkWZsemUbAje4st/SfOk6l1Uz+/OKrXrPI4iHMAhHIMHl1CDW6hDAxgM4Ale4NURzrPz5rzPWwtOPrMPv+B8fAP2fY2a</latexit>f3\nf4\n(\nSEP\n(\n)\n+\nf1\nf2\nEOS\nFigure 2: Postfix Expression.\nThe UFTL Problem. Formally, given a data set ğ·= (ğ‘‹,ğ‘¦) and\nan operation set O, where ğ‘‹is an original feature set and ğ‘¦is the\ncorresponding label. Here, ğ‘¦is only used for testing purposes and\nis not involved in the training process. We first apply the RL-based\ndata collector on ğ‘‹to collect ğ‘training data samples, denoted by\nËœğ·= {Fğ‘–, Tğ‘–,ğ‘ ğ‘–}ğ‘\nğ‘–=1, where Fğ‘–, Tğ‘–, and ğ‘ ğ‘–indicate the explored feature\nset, feature cross sequence, and feature set utility score of the ğ‘–-\nth sample respectively. We aim to 1) embed explored feature sets\ninto a differentiable continuous space and 2) generate the optimal\ntransformed feature set. To accomplish Goal 1, we designed an\nencoder-evaluator-decoder framework that compresses the feature\nlearning knowledge of Ëœğ·into an embedding space E, optimized\nthrough pre-training and fine-tuning processes. To achieve Goal\n2, we adopt a gradient-steered search method to find the optimal\nembedding point ğ¸âˆ—and reconstruct the best feature space F âˆ—. It\ncan be formulated by\nF âˆ—= ğœ“(ğ¸âˆ—)[ğ‘‹] = arg max\nğ¸âˆˆE S(ğœ“(ğ¸)[ğ‘‹]),\n(1)\nwhere ğœ“is the well-trained decoder that can regenerate the feature\ncross sequence from any learned embedding ğ¸; S is the function\nfor calculating feature set utility; ğœ“(ğ¸)[ğ‘‹] means applying the\nreconstructed sequence on ğ‘‹to obtain the transformed feature set.\n3\nTHE MEASUREMENT-PRETRAIN-FINETUNE\nPARADIGM\n3.1\nFramework Overview\nFigure 3 show our uNsupervised gEnerative feAture Transformation\nframework (NEAT) includes three steps: 1) Data collection via un-\nsupervised feature set utility, 2) Graph contrastive pre-training, and\n3) Multi-objective fine-tuning. In Step 1, we design an unsupervised\nmeasurement to assess feature set utilities. Under the guidance of\nthis measurement, we employ the RL-based data collector to explore\nthe original feature space. The collector will automatically gather\nexplored feature sets, feature cross sequences, and feature set utility\nfor training data preparation. In Step 2, we pre-train shared GNNs\nto capture the knowledge of the training data via graph contrastive\nlearning and preserve it in an embedding space. In detail, we first\nextract feature-feature attributed graphs from the explored feature\nset. Then, sampling-based GNNs are employed to capture the char-\nacteristics of these graphs through contrastive learning. In Step 3,\nwe conduct multi-objective fine-tuning to update the created em-\nbedding space and identify the optimal transformed feature space.\nMore specifically, to update the embedding space, we load the pre-\ntrained GNNs as an encoder and create a decoder and an evaluator\nfor feature cross sequence reconstruction and utility score estima-\ntion respectively. Then, we search for enhanced embeddings within\nthe new embedding space and reconstruct feature cross sequences.\nFinally, these sequences are used to transform the original feature\nspace. The transformed feature set yielding the highest utility score\nis output as the optimal result.\n3.2\nUnsupervised Feature Set Utility\nMeasurement\nWhy measuring unsupervised feature set level utility matters.\nFeature set utility measurement reflects the quality of the feature\nspace, which is critical for guiding the search toward the optimal\nfeature set. Conventional strategies leverage supervised methods\n(e.g., decision trees, feature relevance) to measure individual-level\nfeature importance and aggregate them to set-level utility. However,\nsuch strategies are unsuitable for the UFTL setting where down-\nstream predictive task labels are unavailable or expensive. Note\nthat we can use unsupervised measures, such as feature redun-\ndancy, to evaluate individual-level feature utility, and then perform\naverage aggregation. But such a method only evaluates the neg-\native (e.g., information overlap) aspect of a feature, ignores the\nmeaningful interactions of feature-instance, instance-feature value,\nfeature value-feature vector, and fails to consider self-weighted or\nself-penalized aggregation when propagating information score\nfrom individual level to set level.\nMean Discounted Cumulative Gain (MDCG): a feature value\nconsistency preservation perspective. To overcome these limi-\ntations, we propose a feature value consistency preservation angle\nthat illustrates an important intuition: if two data instances (i.e.,\nrows) are similar, then both instances tend to be from the same\nclass, thereafter the two element values of a specific feature with\nrespect to the two similar instances should be similar. We see the\nalignment with such intuition as information gain and develop an\nMDCG based unsupervised measure.\nStep 1: Information Gain. We quantify the information gain of a\nfeature as the score of alignment with the intuition: a featureâ€™s ele-\nment values with respect to two similar instances should be similar.\nFormally, given a feature set F âˆˆRğ‘›Ã—ğ‘š, ğ‘›and ğ‘šare the number\nof instances and features respectively. Let ğ‘–and ğ‘—are indexes of\ninstances, the information gain of the ğ‘-th feature over the ğ‘–-th and\nğ‘—-th instances is given by: ğ‘”ğ‘\nğ‘–ğ‘—= (Fğ‘–ğ‘âˆ’Fğ‘—ğ‘)2ğ‘’âˆ’\n||Fğ‘–,âˆ—âˆ’Fğ‘—,âˆ—||2\nğ‘ğ‘œğ‘›ğ‘ ğ‘¡ğ‘ğ‘›ğ‘¡. In the\nexperiments, we set the value of constant as 2. Here, the lower the\nvalue of ğ‘”ğ‘\nğ‘–ğ‘—, the greater the information gain observed.\nStep 2: Cumulative Gain. The cumulative gain of the ğ‘-th feature\nover all the instance pairs is given by Ã\nğ‘–ğ‘—ğ‘†ğ‘–ğ‘—âˆ—ğ‘”ğ‘\nğ‘–ğ‘—, where ğ‘†ğ‘–ğ‘—is a\nbinary indicator: 1) when the ğ‘–(or ğ‘—)-th instance is among the K\nnearest neighbors of the ğ‘—(or ğ‘–)-th instance, ğ‘†ğ‘–ğ‘—= 1 indicates the\nğ‘–-th instance and the ğ‘—-th instance are similar; 2) otherwise, ğ‘†ğ‘–ğ‘—= 0\nindicates the two instances are not similar. K is a hyperparameter\ngiven by users.\nStep 3: Discounted Cumulative Gain. When a feature exhibits high\nvariance and its element values tend to diverge, if the element values\nof such feature over any two similar instances still stay similar, the\ncumulative gain of such high-variance feature should be augmented.\nTherefore, we propose the concept of discounted cumulative gain:\nÃ\nğ‘–ğ‘—ğ‘†ğ‘–ğ‘—âˆ—ğ‘”ğ‘\nğ‘–ğ‘—\nğ‘‰ğ‘ğ‘Ÿ(Fâˆ—,ğ‘) to describe feature importance.\nStep 4: Mean Discounted Cumulative Gain. Finally, we quantify the\nfeature set utility as the average score of the discounted cumulative\ngains of all the features: Ãğ‘š\nğ‘=1(1 âˆ’\nÃ\nğ‘–ğ‘—ğ‘†ğ‘–ğ‘—âˆ—ğ‘”ğ‘\nğ‘–ğ‘—\nğ‘‰ğ‘ğ‘Ÿ(Fâˆ—,ğ‘) )/ğ‘š.\nLeveraging the utility of the feature set, we employ the RL-based\ncollector [28] to gather explored feature sets F , feature cross se-\nquences T, and feature set utilities ğ‘ for training data preparation.\n3.3\nUnsupervised Feature-Feature Interaction\nGraph Contrastive Pre-Training\nWhy pre-training a feature set embedding space matters. Af-\nter collecting the training data, learning the embedding space of\nthe feature set becomes essential for two reasons: 1) feature set2vec\ncan learn a continuous embedding space, so we can convert classic\ndiscrete search into effective gradient ascent in continuous opti-\nmization. 2) feature set2vec can develop a database of feature set\nembedding vectors along with corresponding unsupervised utilities\nthat enable the learning of a feature set utility evaluator. This eval-\nuator can model the relationship between feature set embeddings\nand feature set utilities and provide gradient information to search\nfor optimal feature sets.\nDecoder\nEvaluator\nPredict Utility\n0.60\nMulti-Objective Fine-tuning\nConverge\nGradient-steered \nOptimization\nAutoregressive  Generation\nLevt\nLrec\nFeature Cross Sequence\nâ€¦\n+\nf1 f2\ne+\nProjection\nPredicted Result\nâ€¦\nf1\n/\nf3\nFeature Set Utility\n0.64\nSEP\nSEP\nEOS\nEOS\nSOS\nSOS\nAggragate\nRL Data Collector\nOriginal Features\nâ€¦\nOperation Set\n1\n_base64=\"2h9eXSnQrMS8EnUJ8\n6m8jDkQUpU=\">AB8nicbVBN\nS8NAEN3Ur1q/qh69LBZBsIZEpH\nosevFYwX5AGspmO2mXbrJhdyO\nW0J/hxYMiXv013vw3btsctPXBw\nO9GWbmBQlnSjvOt1VYWV1b3y\nhulra2d3b3yvsHLSVSaFJBRey\nExAFnMXQ1Exz6CQSBRwaAej2\n6nfgSpmIgf9DgBPyKDmIWMEm0\nk76x6XrVtuwpPSa9cWxnBrxM3\nJxUI5Gr/zV7QuaRhBryolSnu\nsk2s+I1IxymJS6qYKE0BEZgGdo\nTCJQfjY7eYJPjNLHoZCmYo1n6\nu+JjERKjaPAdEZED9WiNxX/87x\nUh9d+xuIk1RDT+aIw5VgLP0f\n95kEqvnYEIlM7diOiSUG1SKp\nkQ3MWXl0nrwnZrdu3+slK/yeM\noiN0jE6Ri65QHd2hBmoigR6R\nq/ozdLWi/VufcxbC1Y+c4j+wP\nr8AcSj6Y=</latexit>+, âˆ’, ..., exp\nMean Discounted \nCumulative Gain\nFeature Set Utility\n0.64\nData Collection via Unsupervised Feature Set Utility\ns\nFeedback\nEvaluate\nFeature Cross Sequence\nâ€¦\n+\nU\nY9eBoPgKeyKr2PQi8eI5gHJEmYns8mQ2dlplcISz7BiwdFvPpF3vwbJ8keNLGgoajqprsrSKQw6LrfTmFldW19o7hZ2tre2d0r7x80TZxqxhslrFuB9RwKRvoEDJ24nmNAokbwWj26nfeuLaiFg94j\njhfkQHSoSCUbTSQ9jzeuWKW3VnIMvEy0kFctR75a9uP2ZpxBUySY3peG6CfkY1Cib5pNRNDU8oG9EB71iqaMSNn81OnZATq/RJGtbCslM/T2R0ciYcRTYzoji0Cx6U/E/r5NieO1nQiUpcsXmi8JUEoz\nJ9G/SF5ozlGNLKNPC3krYkGrK0KZTsiF4iy8vk+Z1busXtyfV2o3eRxFOIJjOAUPrqAGd1CHBjAYwDO8wpsjnRfn3fmYtxacfOYQ/sD5/AHzdY2Y</latexit>f1 f2\nSOS\nSEP\nEOS\nQ\n3W8=\">AB8nicbVDLSgMxFL1TX7W+qi7dBIvgqsyIVJdFNy4r9AXToWTSTBuaSYkI5Shn+HGhSJu/Rp3/o2Zdhbae\niBwOdecu4JE860cd1vp7SxubW9U96t7O0fHB5Vj0+6WqaK0A6RXKp+iDXlTNCOYbTfqIojkNOe+H0Pvd7T1RpJkX\nbzBIaxHgsWMQINlbyBzE2E4J51p4PqzW37i6A1olXkBoUaA2rX4ORJGlMhSEca+17bmKCDCvDCKfzyiDVNMFkisfUt1\nTgmOogW0SeowurjFAklX3CoIX6eyPDsdazOLSTeUS96uXif56fmug2yJhIUkMFWX4UpRwZifL70YgpSgyfWYKJYjYrI\nhOsMDG2pYotwVs9eZ10r+peo954vK4174o6ynAG53AJHtxAEx6gBR0gIOEZXuHNMc6L8+58LEdLTrFzCn/gfP4AkL+\nRdQ=</latexit>T\nExplored Feature Set\nInstance\n\"\n>AB8nicbVDLSgMxFM3UV62vqks3wSK4KjMi1WVREJcV7AOmQ8mkmTY0kwzJHaEM/Qw3LhRx69e482/MtLPQ1gOBwzn3knNPmAh\nuwHW/ndLa+sbmVnm7srO7t39QPTzqGJVqytpUCaV7ITFMcMnawEGwXqIZiUPBuHkNve7T0wbruQjTBMWxGQkecQpASv5/ZjAmBKR\n3c0G1Zpbd+fAq8QrSA0VaA2qX/2homnMJFBjPE9N4EgIxo4FWxW6aeGJYROyIj5lkoSMxNk8gzfGaVIY6Utk8Cnqu/NzISGzON\nQzuZRzTLXi7+5/kpRNdBxmWSApN08VGUCgwK5/fjIdeMgphaQqjmNiumY6IJBdtSxZbgLZ+8SjoXda9Rbzxc1po3R1ldIJO0Tny0\nBVqonvUQm1EkULP6BW9OeC8O/Ox2K05BQ7x+gPnM8fe3mRZw=</latexit>F\nG\nFeature-Feature Graph\nAttribute\nShared GNN\nAugmentations\nProjection\nGraph Contrastive \nPre-training\nMinimize \nContrastive Loss\nProjection\nEdge Perturbation\nAttribute Masking\nPositive\nPairs\nNegative\nPairs\nâ€¦\nâ€¦\nAggragate\nAggragate\nFigure 3: An overview of NEAT. First, we explore the original feature set to collect training data under the guidance of our\nproposed unsupervised feature set utility. Second, we pre-train shared GNNs to capture the knowledge of the training data via\ngraph contrastive learning and preserve it in an embedding space. Finally, we conduct multi-objective fine-tuning to readjust\nthe created embedding space and identify the optimal transformed feature set.\nLeveraging graph contrastive learning for unsupervised fea-\nture set2vec. Feature set2vec, particularly under an unsupervised\nsetting, is challenging. We propose a graph perspective to regard a\nfeature set as a feature-feature similarity graph. This feature-feature\ngraph can describe the geometric topology of a feature space, and\nconvert set2vec into graph2vec, so graph contrastive learning can\nbe introduced to achieve unsupervised embedding pretraning. We\npre-train a GNN-based encoder to convert explored feature sets to\nvectors, thereby constructing a feature transformation embedding\nspace. During the pre-training, we use the explored feature sets\ncollected in the previous step to construct embedding space. To\nsimplify notations, we use F to represent any explored feature set.\nCreating Feature-Feature Similarity Graph. Given an explored fea-\nture set F , we create an adjacency matrix to represent the inter-\nactions among features, in which each element indicates the edge\nbetween two features. For each feature, we use the element values of\nthe corresponding feature within F as its attributes. Subsequently,\nwe calculate the cosine similarity between every feature pair based\non their attributes to determine the weight of the edge, constructing\na feature-feature similarity graph. Finally, we convert the similar-\nities adjacency matrix into a 0/1 matrix with the similarity 95th\npercentile as a cutoff.\nGraph Augmentation for Building Contrastive Pairs. We augment\neach graph with edge perturbation and attribute masking to obtain\naugmented graphs for building contrastive pairs. The augmented\ngraphs from the same graph form a positive pair, while the aug-\nmented graphs from different graphs form negative pairs. The de-\ntails of the augmentation strategies are: 1) Edge Perturbation. A\nrandom proportion of edges is either added or dropped to perturb\nthe connectivity of the graph. The assumption is that the graph has\nrobustness to the pattern variances of edge connection. 2) Attribute\nMasking. A random proportion of attributes is masked, prompting\nthe model to use contextual information to recover the obscured\ninformation. The assumption is that the missing attributes will not\nsignificantly affect the modelâ€™s predictions.\nGNN-based Encoder. The purpose of the GNN-based encoder is to en-\ncode the graphs to extract graph-level representation vectors. Since\nthe graphs have different structures (different adjacency matrices),\nthe encoder needs to adapt to various types of graph structures and\nefficiently handle the learning of node representations. Inspired\nby [9], we develop two GNNs with shared weights to encode aug-\nmented graphs of different types respectively. Here we take one\ngraph as an example to illustrate the encoding process. Given a\nnode ğ‘£from the graph, we sample its all neighbor nodes, denoted\nas {ğ‘¢1,ğ‘¢2, ...,ğ‘¢ğ‘}, where ğ‘is the number of neighbor nodes. The\nhğ‘£and hğ‘¢are the representation of node ğ‘£and node ğ‘¢respectively.\nThen we aggregate the representation of all neighbor nodes by\ncalculating their mean: hâˆ—ğ‘¢= Ãğ‘\n1 hğ‘¢/ğ‘. We update the representa-\ntion of node ğ‘£by hğ‘£= ğ‘…ğ‘’ğ¿ğ‘ˆ(ğ‘ŠÂ· ğ¶ğ‘‚ğ‘ğ¶ğ´ğ‘‡(hğ‘£, hâˆ—ğ‘¢)), where ğ‘…ğ‘’ğ¿ğ‘ˆ\nis the activation function and ğ‘Šis the learnable weight matrix.\nWe iterate this process with multi-step to update the node repre-\nsentation. Finally, we calculate the mean of all node presentations\nin the graph to extract graph-level representation vector for the\naugmented graph, denoted as: h = Ã hğ‘£/ğ‘š, where ğ‘šrepresents the\nnumber of nodes in the graph. Following the principles advocated\nin [3], we use a non-linear projection to project the graph-level rep-\nresentation vectors into a new latent space where the contrastive\nloss is calculated. In our framework, we use a two-layer Perceptron\nas a projection to map the h into a new embedding vector z.\nMinimize the Contrastive Loss. Given a Fğ‘–as theğ‘–-th explored feature\nset in training feature sets, we convert it into a graph and augment\nthe graph to obtain two augmented graphs of different types. We\nfeed the augmented graphs into the shared GNN respectively, to\nextract the embedding vectors, which are represented by zğ‘–,1 and\nzğ‘–,2 respectively. We calculate the distance between sample pairs\nusing cosine similarity ğ‘ ğ‘–ğ‘š(zğ‘–,1, zğ‘–,2) = zğ‘‡\nğ‘–,1zğ‘–,2/(âˆ¥zğ‘–,1âˆ¥âˆ¥zğ‘–,2âˆ¥), and\ndevelop a normalized temperature-scaled cross-entropy loss [23, 24]\nas contrastive loss to maximize the consistency between positive\npairs and increase the distance between negative pairs.\nLğ‘ğ‘™= âˆ’1\nğ‘\nğ‘\nâˆ‘ï¸\nğ‘›=1\nğ‘™ğ‘œğ‘”\nğ‘’ğ‘¥ğ‘(ğ‘ ğ‘–ğ‘š(zğ‘–,1, zğ‘–,2)/ğœ))\nÃğ‘\nğ‘–âˆ—=1,ğ‘–âˆ—â‰ ğ‘–ğ‘’ğ‘¥ğ‘(ğ‘ ğ‘–ğ‘š(zğ‘–,1, zğ‘–âˆ—,2)/ğœ)\n,\n(2)\nwhere ğœdenotes the temperature parameter. Finally, we minimize\nthe contrastive loss to optimize the continuous embedding space.\n3.4\nMulti-Objective Fine-tuning\nAfter pre-training, we conduct multi-objective fine-tuning to up-\ndate the embedding space. This process aims to assess the efficacy\nof the embedding space and fully reconstruct the feature cross\nsequence, thereby facilitating the generation of the optimal trans-\nformed feature set. We reload the pre-trained GNN as an encoder to\nobtain the embedding. Additionally, we create an evaluator and a\ndecoder for estimating feature set utility and reconstructing feature\ncross sequence respectively. Then, we identify the best embeddings\nwithin the updated embedding space and reconstruct feature cross\nsequences. Finally, these sequences are employed to transform the\noriginal feature space, and the transformed feature set that achieves\nthe highest utility score is presented as the optimal result.\nFine-tuning the Encoder-Decoder-Evaluator Structure. We\nuse the training data set {Fğ‘–, Tğ‘–,ğ‘ ğ‘–}ğ‘\nğ‘–=1 collected in section 3.2 to\nfine-tune the encoder-decoder-evaluator structure, where Fğ‘–, Tğ‘–and\nğ‘ ğ‘–indicate explored feature set, feature cross sequence, and feature\nset utility of the ğ‘–-th sample, respectively. To simplify notation, we\nuse the notation (F, T,ğ‘ ) to represent any set of training data.\nThe Encoder. We input an explored feature set F into the pre-trained\nencoder to map the feature set to an embedding vector z.\nThe Decoder. The decoder is to reconstruct T and built by LSTM [10].\nSubsequently, we add a softmax layer after the LSTM to estimate\nthe token distribution and infer the feature cross sequence based on\nthe embedding z. Specifically, assuming the token to be decoded is\nğ‘¡ğ‘—, the partially decoded sequence is [ğ‘¡1,ğ‘¡2, ...,ğ‘¡ğ‘—âˆ’1], and the length\nof T is ğ‘˜. Therefore, the probability of the ğ‘—-th token is:\nğ‘ƒğœ“(ğ‘¡ğ‘—|z, [ğ‘¡1,ğ‘¡2, ...,ğ‘¡ğ‘—âˆ’1]) =\nğ‘’ğ‘¥ğ‘(ğ‘œğ‘—)\nÃ\nğ‘˜ğ‘’ğ‘¥ğ‘(ğ‘œ) ,\n(3)\nwhereğ‘œğ‘—is the ğ‘—-th output of the softmax layer. We use the negative\nlog-likelihood of generating sequential tokens to minimize the\nreconstruction loss between the reconstructed tokens and the real\none, denoted by:\nLğ‘Ÿğ‘’ğ‘= âˆ’\nğ‘˜\nâˆ‘ï¸\nğ‘—=1\nğ‘™ğ‘œğ‘”ğ‘ƒğœ“(ğ‘¡ğ‘—|z, [ğ‘¡1,ğ‘¡2, ...,ğ‘¡ğ‘—âˆ’1])\n(4)\nThe Evaluator. We add a fully connected layer after the encoder for\nregression prediction, which is given by: Â¥ğ‘ = ğœƒ(ğ‘§), where ğœƒis the\nfunction notation of the evaluator. Specifically, we use the Mean\nSquared Error (MSE) to measure the evaluator loss:\nLğ‘’ğ‘£ğ‘¡= ğ‘€ğ‘†ğ¸(Â¥ğ‘ ,ğ‘ ),\n(5)\nwhere ğ‘ is the corresponding feature set utility. Finally, we trade\noff the two losses to form a joint training loss: L = ğ›¼Lğ‘’ğ‘£ğ‘¡+ ğ›½Lğ‘Ÿğ‘’ğ‘,\nwhere ğ›¼and ğ›½are the hyperparameters to balance the influence of\nthe two loss functions. We calculate the mean loss of the batch data\nto readjust the embedding space during the fine-tuning process.\nOptimization and Generation of the Optimal Transformed\nFeature Set. After updating the embedding space, we employ a\ngradient-ascent search method to find better embedding. In the\ncontinuous space, a good starting point can accelerate the search\nprocess and enhance the downstream performance. Such good\npoints are called initial seeds. Therefore, we first select the top\nK explored feature sets based on their corresponding feature set\nutilities as initial seeds to find better embeddings. More specifically,\nfor an initial seed, we obtain an embedding z through the well-\ntrained encoder. Next, we use a gradient-ascent method to move\nthe embedding with ğœ‚steps along the direction maximizing the\nfeature set utility. Formally, the moving calculation process can be\ndescribed as: z+ = z + ğœ‚ğœ•ğœƒ\nğœ•z , where ğ‘§+ is the better embedding. We\nfeed z+ into the well-trained decoder to generate a feature cross\nsequence in an autoregressive manner until finding <EOS>. We\nsplit the sequence into different segments according to <SEP>, and\nfollow the calculation rule of them to generate the transformed\nfeature set. Finally, the transformed feature set yielding the highest\nutility score is output as the optimal result. The pseudo-code of the\nalgorithm is released in Appendix 8.2.\n4\nEXPERIMENT\n4.1\nExperimental Setup\nDataset Descriptions. We utilize 23 publicly available datasets\nfrom UCI, LibSVM, Kaggle, and OpenML to conduct experiments.\nThese datasets comprise 14 classification tasks (C) and 9 regression\ntasks (R). Table 1 shows the statistics of these datasets.\nEvaluation Metrics. To alleviate the variance of the downstream\nML model for fair comparisons, we employ random forests (RF)\nto assess the quality of transformed feature set. For classification\ntasks, we use F1-score, Precision, and Recall as evaluation metrics.\nFor regression tasks, we use 1 - Relative Absolute Error (1-RAE), 1 -\nMean Average Error (1-MAE), and 1 - Mean Square Error (1-MSE)\nas evaluation metrics. In both cases, higher values of the evaluation\nmetrics indicate a higher quality of the transformed feature set.\nBaseline Algorithms and Model Variants of NEAT. We compare\nour method with 9 widely-used feature transformation methods:\n(A) Unsupervised Methods: 1) RDG randomly generates feature-\noperation-feature transformation records to create a new feature\nspace; 2) PCA [26] uses linear feature correlation to generate new\nfeatures; 3) LDA [2] is a matrix factorization-based method that\nobtains decomposed latent representation as the generated feature\nspace; (B) Supervised Methods: 3) ERG first applies a set of opera-\ntions to each feature to expand the feature space, and then selects\nkey features from it to form a new feature space; 4) AFAT [11] is an\nenhanced version of ERG, repeatedly generating new features and\nusing a multi-step feature selection method to select more informa-\ntive features for creating a new feature space; 5) NFS [4] embeds\nthe decision-making process of feature transformation into a pol-\nicy network and utilizes reinforcement learning (RL) to optimize\nthe entire feature transformation process; 6) TTG [15] represents\nthe feature transformation process as a graph and implements an\nRL-based discrete search method to find the optimal feature set;\n7) GRFG [28] uses three collaborative reinforcement intelligent\nagents for feature generation and employs feature grouping strate-\ngies to accelerate agent learning; 8) DIFER [36] embeds randomly\nTable 1: Overall Performance. In this table, the best and second-best results are highlighted by bold and underlined fonts\nrespectively. We evaluate classification (C) and regression (R) tasks in terms of F1-score and 1-RAE respectively. The higher the\nvalue is, the better the quality of the transformed feature set is.\nDataset\nSoruce\nC/R\nSamples\nFeatures\nRDG\nPCA\nLDA\nERG\nAFAT\nNFS\nTTG\nGRFG\nDIFER\nNEAT\nHiggs Boston\nUCI\nC\n50000\n28\n0.695\n0.538\n0.513\n0.702\n0.697\n0.691\n0.699\n0.707\n0.669\n0.702\nAmazon Employee\nKaggle\nC\n32769\n9\n0.932\n0.923\n0.916\n0.932\n0.930\n0.932\n0.933\n0.932\n0.929\n0.936\nPimaIndian\nUCI\nC\n768\n8\n0.751\n0.736\n0.729\n0.735\n0.732\n0.791\n0.745\n0.754\n0.760\n0.811\nSpectF\nUCI\nC\n267\n44\n0.760\n0.709\n0.665\n0.788\n0.760\n0.792\n0.760\n0.776\n0.766\n0.856\nSVMGuide3\nLibSVM\nC\n1243\n21\n0.806\n0.676\n0.635\n0.818\n0.794\n0.786\n0.798\n0.812\n0.773\n0.832\nGeman Credit\nUCI\nC\n1000\n24\n0.701\n0.679\n0.596\n0.662\n0.639\n0.649\n0.644\n0.667\n0.656\n0.723\nCredit Default\nUCI\nC\n30000\n24\n0.802\n0.768\n0.743\n0.803\n0.804\n0.801\n0.798\n0.806\n0.796\n0.809\nMessidor_features\nUCI\nC\n1151\n19\n0.627\n0.672\n0.463\n0.692\n0.657\n0.650\n0.655\n0.680\n0.660\n0.737\nWine Quality Red\nUCI\nC\n999\n12\n0.496\n0.468\n0.401\n0.485\n0.480\n0.451\n0.467\n0.454\n0.476\n0.522\nWine Quality White\nUCI\nC\n4898\n12\n0.524\n0.469\n0.438\n0.527\n0.516\n0.525\n0.521\n0.518\n0.507\n0.554\nSpambase\nUCI\nC\n4601\n57\n0.906\n0.815\n0.889\n0.917\n0.912\n0.925\n0.919\n0.922\n0.912\n0.932\nAp-omentum-ovary\nOpenml\nC\n275\n10936\n0.820\n0.718\n0.716\n0.849\n0.813\n0.830\n0.830\n0.830\n0.833\n0.865\nLymphography\nUCI\nC\n148\n18\n0.108\n0.170\n0.144\n0.202\n0.149\n0.166\n0.148\n0.136\n0.150\n0.298\nIonosphere\nUCI\nC\n351\n34\n0.942\n0.928\n0.743\n0.957\n0.942\n0.943\n0.932\n0.956\n0.905\n0.971\nHousing Boston\nUCI\nR\n506\n13\n0.434\n0.122\n0.020\n0.410\n0.401\n0.428\n0.396\n0.409\n0.381\n0.478\nAirfoil\nUCI\nR\n1503\n5\n0.519\n0.514\n0.207\n0.519\n0.507\n0.520\n0.500\n0.521\n0.528\n0.578\nOpenml_586\nOpenml\nR\n1000\n25\n0.568\n0.221\n0.110\n0.624\n0.553\n0.542\n0.544\n0.646\n0.482\n0.700\nOpenml_589\nOpenml\nR\n1000\n50\n0.509\n0.224\n0.011\n0.610\n0.508\n0.503\n0.504\n0.627\n0.463\n0.625\nOpenml_607\nOpenml\nR\n1000\n50\n0.521\n0.104\n0.107\n0.549\n0.509\n0.518\n0.522\n0.555\n0.476\n0.550\nOpenml_616\nOpenml\nR\n500\n50\n0.070\n0.057\n0.024\n0.193\n0.162\n0.147\n0.156\n0.372\n0.076\n0.536\nOpenml_618\nOpenml\nR\n1000\n50\n0.472\n0.078\n0.052\n0.561\n0.473\n0.467\n0.467\n0.577\n0.408\n0.616\nOpenml_620\nOpenml\nR\n1000\n25\n0.511\n0.088\n0.029\n0.546\n0.521\n0.519\n0.512\n0.530\n0.442\n0.584\nOpenml_637\nOpenml\nR\n500\n50\n0.136\n0.072\n0.043\n0.064\n0.161\n0.146\n0.144\n0.307\n0.072\n0.278\n0.80\n0.82\n0.84\n0.86\n0.88\n0.90\n0.92\nPrecision\nRecall\nF1-Score\nNEAT\nNEAT*\nMOTA\n(a) SpectF\n0.80\n0.82\n0.84\n0.86\nPrecision\nRecall\nF1-Score\nNEAT\nNEAT*\nMOTA\n(b) SVMGuide3\n0.68\n0.70\n0.72\n0.74\n0.76\n0.78\nPrecision\nRecall\nF1-Score\nNEAT\nNEAT*\nMOTA\n(c) German Credit\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1-MAE\n1-MSE\n1-RAE\nNEAT\nNEAT*\nMOTA\n(d) Openml_616\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1-MAE\n1-MSE\n1-RAE\nNEAT\nNEAT*\nMOTA\n(e) Openml_618\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1-MAE\n1-MSE\n1-RAE\nNEAT\nNEAT*\nMOTA\n(f) Openml_620\nFigure 4: The impact of graph contrastive pre-training.\n0.75\n0.80\n0.85\n0.90\nPrecision\nRecall\nF1-Score\nNEAT\nNEAT +\nNEAT -\nNEAT#\n(a) SpectF\n0.75\n0.80\n0.85\n0.90\nPrecision\nRecall\nF1-Score\nNEAT\nNEAT+\nNEAT-\nNEAT#\n(b) SVMGuide3\n0.65\n0.70\n0.75\nPrecision\nRecall\nF1-Score\nNEAT\nNEAT+\nNEAT-\nNEAT#\n(c) German Credit\n0.4\n0.6\n0.8\n1-MAE\n1-MSE\n1-RAE\nNEAT\nNEAT+\nNEAT-\nNEAT#\n(d) Openml_616\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1-MAE\n1-MSE\n1-RAE\nNEAT\nNEAT+\nNEAT-\nNEAT#\n(e) Openml_618\n0.5\n0.6\n0.7\n0.8\n0.9\n1-MAE\n1-MSE\n1-RAE\nNEAT\nNEAT+\nNEAT-\nNEAT#\n(f) Openml_620\nFigure 5: The impact of unsupervised feature set utility measurement, RL-based data collector, and initial seeds.\ngenerated feature cross sequences and then uses a greedy search to\nfind the separated transformed feature.\nAdditionally, to evaluate the necessity of each technical com-\nponent of NEAT, we implement four model variants: 1) NEATâˆ—\nremoves the pre-training stage and directly optimizes the encoder-\nevaluator-decoder structure for creating the feature transformation\nembedding space. 2) NEAT+ utilizes the training data collected\nby randomly generating explored feature sets to create the feature\ntransformation embedding space. 3) NEATâˆ’replaces the mean\ndiscounted cumulative gain with feature redundancy for creating\nthe feature transformation embedding space. 4) NEAT# replaces\nthe initial seeds with worst-ranked transformation sequencesâ€™ em-\nbeddings based on the utility value to identify better feature spaces.\nHyperparameters and Reproducibility. 1) Feature Transforma-\ntion Knowledge Acquisition: We use the reinforcement data collec-\ntor to collect explored feature set-feature utility score pairs. The\ncollector explores 512 episodes, and each episode includes 10 steps.\n2) Feature-Feature Similarity Graph Construction: The threshold\nfor creating an edge between two features is the 95th percentile of\nall similarity values. 3) Graph Contrastive Pre-training: We map\nthe attribute of each node to a 64-dimensional embedding, and use\na 2-layer GNN network and a 2-layer projection head to integrate\ntoken length\nMOTA\nNEAT\nexecute graphics memory (MiB)\n2,000\n4,000\n6,000\n8,000\n10,000\n12,000\n14,000\n16,000\ntoken length\n0\n200\n400\n600\n800\n1000\n1200\nGermanCredit\nOpenml_620\nOpenml_618\nSpectF\nOpenml_616\nSVMGuide3\n(a) GPU Memory Utilization\nNEAT -Spectf\nNEAT*-Spectf\nNEAT -Openml620\nNEAT*-Openml620\nTrain Loss\n0.05\n0.1\n0.2\n0.5\n1\nTrain Epoch\n0\n20\n40\n60\n80\n100\nTrain Epoch\n0\n20\n40\n60\n80\n100\n(b) Convergence Speed\nFigure 6: (a) GPU memory utilization comparison between\nNEAT and MOTA in different datasets. (b) Convergence\nspeed comparison between NEAT and NEATâˆ—\nsuch information. For graph augmentation, we randomly perturbed\n20% of the edges or masked 20% of the attributes to get diversified\ngraphs. During the optimization, the batch size is 1024, the pre-\ntraining epochs are 100, the learning rate is 0.001, and the value of ğœ\nis set to 0.5. 4) Multi-objective Fine-tuning: The decoder is a 1-layer\nLSTM network, which reconstructs a feature cross sequence. The\nevaluator is a 2-layer feed-forward network, in which the dimen-\nsion of each layer is 200. During optimization, ğ›¼and ğ›½are 10 and\n0.1 respectively, the batch size is 1024, the fine-tuning epochs are\n500, and the learning rate is 0.001.\nEnvironmental Settings. All experiments are conducted on the\nUbuntu 22.04.3 LTS operating system, Intel(R) Core(TM) i9-13900KF\nCPU@ 3GHz, and 1 way RTX 4090 and 32GB of RAM, with the\nframework of Python 3.11.4 and PyTorch 2.0.1.\n4.2\nExperimental Results\nOverall performance. In this experiment, we compare the perfor-\nmance of NEAT and baseline models for feature transformation in\nterms of F1-score and 1-RAE. Table 1 shows the comparison results.\nWe can find that in most cases, the NEAT performs the best, and\nin the other four cases, it ranks second-best. But NEAT performs\nthe best across all unsupervised approaches. The underlying dri-\nver for this observation is that NEAT can accurately capture the\ninternal principle of the features for identifying the optimal fea-\nture space through the integration of unsupervised measurement,\npre-training, and fine-tuning stages. Moreover, another interesting\nobservation is that NEAT exhibits a higher degree of robustness\nand effectiveness in its transformation performance when applied\nto classification tasks as compared to regression tasks. A potential\nreason for this observation is that NEAT effectively captures the\nintricate relationships within the feature space, generating non-\nlinear features to enhance the distinguishability of the feature space.\nSuch transformations have more influence on classification tasks\nthan regression tasks. In conclusion, this experiment shows the\neffectiveness of NEAT in feature transformation, underscoring the\ngreat potential of generative AI in this domain.\nThe impact of the graph contrastive learning based pretrain-\ning. One of the most important novelties of NEAT is to involve\ngraph contrastive learning to pre-train the transformation embed-\nding space. To analyze the influence of this component, we develop\ntwo model variants: 1) NEATâˆ—; 2) MOTA [29] is similar to NEAT,\nbut it lacks the pre-training stage and employs an LSTM encoder.\nTo ensure fair comparisons, we use our defined feature utility score\nParameter Size\nUnsupervised\nSupervised\nData Collection Time (S)\n1000\n2000\n5000\n10000\n20000\nModel Parameter Size (MB)\n0.18\n0.20\n0.22\n0.24\nGermanCredit\nOpenml_620\nOpenml_618\nSVMGuide3\nOpenml_616\nSpectF\n(a) Data collection time and model size\ntraining time\ninference time\nInference Time (S)\n0.5\n1.0\n1.5\nTrainingTime (S)\n2000\n2500\nGermanCredit\nOpenml_620\nOpenml_618\nSVMGuide3\nOpenml_616\nSpectF\n(b) Training and inference time\nFigure 7: (a) Data collection time comparison in terms of\nunsupervised and supervised collectors. Model parameter\nsize check. (b) Model training and inference time check.\nas feature space measurement in both MOTA and NEAT. Figure 4\nshows the comparison results on both classification and regression\ntasks. We can find that NEAT outperforms NEATâˆ—and MOTA in\nall cases. The underlying driver is that the pre-training stage ini-\ntializes an effective transformation embedding space by capturing\nintricate relationships within the feature space. This initialization\nintegrates with multi-objective fine-tuning to enhance the identifi-\ncation of the optimal feature space. Another interesting observation\nis that NEATâˆ—beats MOTA in most cases. A potential reason is\nthat the GNN-based encoder is adept at grasping feature-feature\ninteractions and correlations compared with LSTM to produce an\nimproved transformation embedding space for better reconstruc-\ntion. In summary, this experiment demonstrates the necessity of the\npre-training stage and the importance of the GNN-based encoder.\nThe impact of unsupervised feature set utility measurement,\nRL-based data collector, and initial seeds. In NEAT, to cre-\nate an effective transformation embedding space, we adopt three\nstrategies: 1) Using an RL-based data collector [28] to automatically\ncollect high-quality transformation records. 2) Developing an un-\nsupervised measurement to evaluate the transformation recordsâ€™\nutility. 3) Employing top-ranked transformation sequences, deter-\nmined by their feature utility score, as initial seeds to search for\nenhanced transformation sequences. We develop three model vari-\nants to study their impact: NEAT+, NEATâˆ’, and NEAT#. Figure 5\nshows the comparison results. We find that NEAT surpasses NEAT+,\nNEATâˆ’and and NEAT# across all datasets. There are three underly-\ning drivers: a) The RL-based collector can mimic human intelligence\nin feature learning to produce high-quality transformation records.\nThese records help create an informative and distinguishable em-\nbedding space to identify the optimal feature space. b) The mean\ndiscounted cumulative gain, which considers the feature value con-\nsistency preservation, can better evaluate the feature set utility\ncompared to feature redundancy. c) NEAT converts feature genera-\ntion as a continuous optimization task, in which a good initialization\ncan enhance the search for the optimal transformation sequence.\nAnalysis of GPU utilization and convergence speed for NEAT.\nTo further analyze the GPU utilization and convergence speed of\nNEAT, we compare it with both MOAT and NEATâˆ—. Figure 6 shows\nthe comparison results. From Figure 6(a), we find that NEAT signif-\nicantly saves the required GPU memory compared with MOTA. A\npotential reason is that MOTA employs LSTM as the encoder, with\nthe increase in feature space dimension, the required GPU memory\nwill increase correspondingly. Instead, NEAT organizes the original\nfeature space as a feature-feature attributed graph and utilizes GNNs\nto aggregate information into embedding. This learning approach\nis more memory-efficient. From Figure 6(b), we observe that NEAT\nconverges faster and achieves lower loss compared with NEATâˆ—.\nA possible reason is that the pre-training stage preserves complex\nfeature-feature relationships within the transformation embedding\nspace. This accelerates the learning process of the fine-tuning stage\nto achieve model convergence. In summary, this experiment shows\nthe efficient GPU memory usage and fast convergence of NEAT.\nRobustness check. To study the robustness of NEAT, we compare\nits transformation performance and other baselines across vari-\nous downstream ML models. We replace the Random Forest (RF)\nwith XGBoost (XGB), Support Vector Machine (SVM), K-Nearest\nNeighborhood (KNN), Decision Tree (DT), and LASSO respectively.\nTable 2 shows the F1-score comparison results for the German\nCredit dataset. We can find that NEAT consistently exhibits su-\nperior performance in most cases. A potential reason is that the\nunsupervised feature utility measurement of NEAT leads it to grasp\nmore general and intrinsic characteristics of the feature space. Such\ninformation is captured independently of the target labels, thereby\nenhancing the robustness and generalization capabilities of NEAT.\nIn summary, this experiment demonstrates the robustness of NEAT,\nwhich maintains consistent performance across various ML models.\nTime and space complexity analysis. To check the time and\nspace complexity of NEAT, we select 6 datasets to compare data\ncollection time, model parameter size, training time, and inference\ntime. Figure 7(a) shows the comparison results in terms of the time\ncost of the data collector and model parameter size. We find that\nthe data collector using our defined unsupervised feature utility\nmeasurement requires much less collection time compared with the\nsupervised collector using the target label. This observation shows\nthe computational efficiency of our unsupervised measurement. A\npossible reason is that the supervised collector has to spend much\ntime retraining the downstream ML model from scratch at each\niteration. Moreover, we notice that NEAT can maintain a very small\nparameter size across all datasets. A potential reason is that our\nsampling-based GNN encoder leads NEAT to focus on key informa-\ntion within the feature space instead of the global situation, signifi-\ncantly increasing the scalability of NEAT. Furthermore, Figure 7(b)\nshows the comparison results in terms of training and inference\ntime costs. We find that once NEAT converges, it can quickly infer\nand identify the optimal feature space. The exceptional inference\ntime of NEAT underscores its great practical potential. We further\ncompare the time cost of NEAT with various baselines in Appen-\ndix 8.3, and analyze the time cost of the different components within\nNEAT in Appendix 8.5.\nCase study. We employ our proposed unsupervised feature utility\nmeasurement (See Step 3 in Section 3.2) to calculate the feature\nimportance of the original and transformed feature space respec-\ntively using the Wine Quality Red dataset. This task aims to predict\nthe wine quality using the collected features. Figure 8 shows the\ncomparison results in terms of the top 10 important features. In\neach pie chart, the importance of a feature is indicated by the area of\nthe corresponding slice. The text labeling each slice of the pie chart\ndenotes the name of the associated feature. We observe that in the\ntransformed feature space, 60% of the top 10 features are generated\nby NEAT. This contributes to 14.5% improvements in wine quality\nTable 2: Robustness check. Test the transformed feature set\non various downstream ML models.\nRF\nXGB\nSVM\nKNN\nDT\nLASSO\nRDG\n0.701\n0.690\n0.694\n0.660\n0.652\n0.666\nPCA\n0.679\n0.641\n0.657\n0.641\n0.676\n0.659\nLDA\n0.596\n0.629\n0.589\n0.622\n0.604\n0.589\nERG\n0.662\n0.669\n0.599\n0.619\n0.630\n0.684\nAFAT\n0.639\n0.688\n0.639\n0.643\n0.691\n0.647\nNFS\n0.649\n0.690\n0.695\n0.635\n0.663\n0.672\nTTG\n0.644\n0.690\n0.695\n0.660\n0.663\n0.666\nGRFG\n0.667\n0.656\n0.685\n0.649\n0.687\n0.676\nDIFER\n0.656\n0.696\n0.617\n0.658\n0.683\n0.672\nNEAT\n0.723\n0.707\n0.703\n0.661\n0.692\n0.676\nF1-Score:0.456\n[alcohol]\n[ï¬xed acidity]\n[totle SO2]\n[density]\n[free SO2]\n[pH]\n[sulphates]\n[sugar]\n[citric acid]\n[volatile acidity]\n(a) Originial\nF1-Score:0.522\n1/[ï¬xed acidity]\n1/([alcohol]+\n[ï¬xed acidity])\n[alcohol]+\n[citric acid]\n[alcohol]+\n[ï¬xed acidity]\n[citric acid]+\n[sulphates]\n[alcohol]\n[citric acid]\n[ï¬xd acidity]\n[citric acid]+\n[volatile acidity]\n[sulphates]\n(b) Transformation\nFigure 8: Case study. Comparison of traceability on the\noriginal feature space and transformed feature space.\nprediction. Such an observation indicates that NEAT comprehends\nthe complex inherent relationships of the feature set, thereby gen-\nerating a better feature space. Furthermore, we notice that â€™alcoholâ€™\nand â€™fixed acidityâ€™ are often employed to generate new informative\nfeatures, such as â€™1/[fixed acidity]â€™ and â€™1/([alcohol]+[fixed acid-\nity])â€™. The observation aligns with known wine knowledge: alcohol\nenhances taste and body, while fixed acidity influences tartness and\nstability, collectively determining wine quality.\n5\nRELATED WORK\nFeature Engineering can derive a new feature set from original fea-\ntures to improve downstream predictive performance [5, 8, 17, 32].\nHumans can manually reconstruct a feature set with domain knowl-\nedge and empirical experiences. These methods are explicit, trace-\nable, and explainable, but are incomplete and time-consuming\ndue to their heavy reliance on domain expertise and empirical\ninsights. Supervised transformations include exhaustive-expansion-\nreduction [11, 12, 14, 16, 19], iterative-feedback-improvement [15,\n25, 28, 31, 33, 35], and AutoAI-based approaches [1, 4, 18, 22, 27, 30,\n34, 36]. Exhaustive-expansion-reduction methods use explicit [13]\nor greedy [6] techniques to construct feature cross, assessing their\nimpact on downstream ML tasks and then proceeding with feature\nselection to keep crucial features. However, these approaches heav-\nily rely on domain knowledge and labels, is hard to construct com-\nplex feature crosses, resulting in suboptimal performance. Iterative-\nfeedback-improvement methods leverage reinforcement learning,\ngenetic algorithms, or evolution algorithms to formulate feature\ntransformation as a discrete optimization task, searching for the\noptimal feature set iteratively. However, they face exponentially\ngrowing combination possibilities, are computationally costly, hard\nto converge, and unstable. AutoAI-based methods utilize AutoML [7,\n20] techniques to systematically explore various feature crosses in a\nstructured, non-exhaustive manner. However, they are constrained\nby their inability to generate high-order features and their sensitiv-\nity to performance instability. In contrast, unsupervised transforma-\ntions like Principal Components Analysis (PCA) [21] donâ€™t rely on\nlabels. However, PCA is based on a strong assumption of straight\nlinear feature correlation, only uses addition/subtraction to gener-\nate new features, and leads to only dimensionality reduction rather\nthan an increase. To fulfill these gaps, we develop a measurement-\npretrain-finetune paradigm, formulating Feature Transformation\nas an unsupervised continuous optimization task. This method con-\nstructs a continuous space with unsupervised graph contrastive\nlearning, facilitating the generation of the best feature set. By decou-\npling FT from downstream tasks and avoiding ineffective discrete\nsearches, our approach provides a more efficient solution.\n6\nCONCLUSION\nIn this paper, we integrate graph, contrastive, and generative learn-\ning to develop a label-free generative framework for unsupervised\nfeature transformation learning problems. There are three impor-\ntant contributions: 1) We develop an unsupervised measurement\nto evaluate feature set utilities, decoupling the framework from\ndownstream tasks, and avoiding the evaluation of feature sets from\nlengthy and expensive experiments. 2) We regard a feature set as\na feature-feature similarity graph and introduce an unsupervised\ntraining paradigm, embedding feature transformation knowledge\ninto a continuous space through graph contrastive pre-training. 3)\nWe see feature transformation as a sequential generation task and\ndevelop an encoder-decoder-evaluator structure, which facilitates\nthe identification of the optimal transformed feature set through\ngradient optimization. This structure enables the generation of op-\ntimal results, avoiding the need to explore exponentially growing\npossibilities of feature combinations in discrete space. Extensive\nexperiments validate the effectiveness, efficiency, traceability, and\nclarity of our framework. The superior experimental performance\nhighlights its potential as a valuable approach for unsupervised\ngenerative feature transformation in large quantities of domains,\nsuch as bioinformatics, health care, finance analysis, etc.\n7\nACKNOWLEDGEMENT\nThis research was partially supported by the National Science Foun-\ndation (NSF) via the grant number: GR45865.\nREFERENCES\n[1] Maroua Bahri, Flavia Salutari, Andrian Putina, and Mauro Sozio. 2022. AutoML:\nstate of the art with a focus on anomaly detection, challenges, and research\ndirections. International Journal of Data Science and Analytics 14, 2 (2022), 113â€“\n126.\n[2] David M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent dirichlet allocation.\nJournal of machine Learning research 3, Jan (2003), 993â€“1022.\n[3] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. A\nsimple framework for contrastive learning of visual representations. In Interna-\ntional conference on machine learning. PMLR, 1597â€“1607.\n[4] Xiangning Chen, Qingwei Lin, Chuan Luo, Xudong Li, Hongyu Zhang, Yong\nXu, Yingnong Dang, Kaixin Sui, Xu Zhang, Bo Qiao, et al. 2019. Neural feature\nsearch: A neural architecture for automated feature engineering. In 2019 IEEE\nInternational Conference on Data Mining (ICDM). IEEE, 71â€“80.\n[5] Yi-Wei Chen, Qingquan Song, and Xia Hu. 2021. Techniques for Automated\nMachine Learning. SIGKDD Explor. Newsl. 22, 2 (jan 2021), 35â€“50.\n[6] Ofer Dor and Yoram Reich. 2012. Strengthening learning algorithms by feature\ndiscovery. Information Sciences 189 (2012), 176â€“190.\n[7] Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. 2019. Neural architecture\nsearch: A survey. The Journal of Machine Learning Research 20, 1 (2019), 1997â€“\n2017.\n[8] Nanxu Gong, Wangyang Ying, Dongjie Wang, and Yanjie Fu. 2024. Neuro-\nSymbolic Embedding for Short and Effective Feature Selection via Autoregressive\nGeneration. arXiv preprint arXiv:2404.17157 (2024).\n[9] Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive representation\nlearning on large graphs. Advances in neural information processing systems 30\n(2017).\n[10] Sepp Hochreiter and JÃ¼rgen Schmidhuber. 1997. Long short-term memory. Neural\ncomputation 9, 8 (1997), 1735â€“1780.\n[11] Franziska Horn, Robert Pack, and Michael Rieger. 2020. The autofeat Python\nLibrary for Automated Feature Engineering and Selection. In Machine Learning\nand Knowledge Discovery in Databases, Peggy Cellier and Kurt Driessens (Eds.).\nSpringer International Publishing, Cham, 111â€“120.\n[12] James Max Kanter and Kalyan Veeramachaneni. 2015. Deep feature synthesis:\nTowards automating data science endeavors. In 2015 IEEE International Conference\non Data Science and Advanced Analytics (DSAA). 1â€“10. https://doi.org/10.1109/\nDSAA.2015.7344858\n[13] Gilad Katz, Eui Chul Richard Shin, and Dawn Song. 2016. Explorekit: Automatic\nfeature generation and selection. In 2016 IEEE 16th International Conference on\nData Mining (ICDM). IEEE, 979â€“984.\n[14] Udayan Khurana, Fatemeh Nargesian, Horst Samulowitz, Elias Khalil, and Deepak\nTuraga. 2016. Automating feature engineering. Transformation 10, 10 (2016), 10.\n[15] Udayan Khurana, Horst Samulowitz, and Deepak Turaga. 2018. Feature engi-\nneering for predictive modeling using reinforcement learning. In Proceedings of\nthe AAAI Conference on Artificial Intelligence, Vol. 32.\n[16] Udayan Khurana, Deepak Turaga, Horst Samulowitz, and Srinivasan Parthasrathy.\n2016. Cognito: Automated Feature Engineering for Supervised Learning. In 2016\nIEEE 16th International Conference on Data Mining Workshops (ICDMW). 1304â€“\n1307. https://doi.org/10.1109/ICDMW.2016.0190\n[17] A. Kusiak. 2001. Feature transformation methods in data mining. IEEE Trans-\nactions on Electronics Packaging Manufacturing 24, 3 (2001), 214â€“221.\nhttps:\n//doi.org/10.1109/6104.956807\n[18] Hoang Thanh Lam, Beat Buesser, Hong Min, Tran Ngoc Minh, Martin Wistuba,\nUdayan Khurana, Gregory Bramble, Theodoros Salonidis, Dakuo Wang, and\nHorst Samulowitz. 2021. Automated Data Science for Relational Data. In 2021\nIEEE 37th International Conference on Data Engineering (ICDE). 2689â€“2692. https:\n//doi.org/10.1109/ICDE51399.2021.00305\n[19] Hoang Thanh Lam, Johann-Michael Thiebaut, Mathieu Sinn, Bei Chen, Tiep Mai,\nand Oznur Alkan. 2017. One button machine for automating feature engineering\nin relational databases. arXiv:1706.00327 [cs.DB]\n[20] Yaliang Li, Zhen Wang, Yuexiang Xie, Bolin Ding, Kai Zeng, and Ce Zhang.\n2021. Automl: From methodology to application. In Proceedings of the 30th ACM\nInternational Conference on Information & Knowledge Management. 4853â€“4856.\n[21] Andrzej MaÄ‡kiewicz and Waldemar Ratajczak. 1993. Principal components\nanalysis (PCA). Computers & Geosciences 19, 3 (1993), 303â€“342.\n[22] Alhassan Mumuni and Fuseini Mumuni. 2024. Automated data processing and\nfeature engineering for deep learning and big data applications: a survey. Journal\nof Information and Intelligence (2024).\n[23] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning\nwith contrastive predictive coding. arXiv preprint arXiv:1807.03748 (2018).\n[24] Kihyuk Sohn. 2016. Improved deep metric learning with multi-class n-pair loss\nobjective. Advances in neural information processing systems 29 (2016).\n[25] Binh Tran, Bing Xue, and Mengjie Zhang. 2016. Genetic programming for feature\nconstruction and selection in classification on high-dimensional data. Memetic\nComputing 8 (2016), 3â€“15.\n[26] Md Palash Uddin, Md Al Mamun, and Md Ali Hossain. 2021. PCA-based feature\nreduction for hyperspectral remote sensing image classification. IETE Technical\nReview 38, 4 (2021), 377â€“396.\n[27] Dakuo Wang, Josh Andres, Justin D Weisz, Erick Oduor, and Casey Dugan. 2021.\nAutods: Towards human-centered automation of data science. In Proceedings of\nthe 2021 CHI conference on human factors in computing systems. 1â€“12.\n[28] Dongjie Wang, Yanjie Fu, Kunpeng Liu, Xiaolin Li, and Yan Solihin. 2022. Group-\nwise reinforcement feature generation for optimal and explainable representation\nspace reconstruction. In Proceedings of the 28th ACM SIGKDD Conference on\nKnowledge Discovery and Data Mining. 1826â€“1834.\n[29] Dongjie Wang, Meng Xiao, Min Wu, Yuanchun Zhou, Yanjie Fu, et al.\n2024. Reinforcement-enhanced autoregressive feature transformation: Gradient-\nsteered search in continuous space for postfix expressions. Advances in Neural\nInformation Processing Systems 36 (2024).\n[30] Marcel Wever, Alexander Tornede, Felix Mohr, and Eyke HÃ¼llermeier. 2021.\nAutoML for multi-label classification: Overview and empirical evaluation. IEEE\ntransactions on pattern analysis and machine intelligence 43, 9 (2021), 3037â€“3054.\n[31] Meng Xiao, Dongjie Wang, Min Wu, Kunpeng Liu, Hui Xiong, Yuanchun Zhou,\nand Yanjie Fu. 2022. Self-Optimizing Feature Transformation. arXiv:2209.08044\n[32] Wangyang Ying, Dongjie Wang, Haifeng Chen, and Yanjie Fu. 2024. Feature\nSelection as Deep Sequential Generative Learning. arXiv preprint arXiv:2403.03838\n(2024).\n[33] Wangyang Ying, Dongjie Wang, Kunpeng Liu, Leilei Sun, and Yanjie Fu. 2023.\nSelf-optimizing feature generation via categorical hashing representation and\nhierarchical reinforcement crossing. In 2023 IEEE International Conference on\nData Mining (ICDM). IEEE, 748â€“757.\n[34] Ziwei Zhang, Xin Wang, and Wenwu Zhu. 2021. Automated machine learning\non graphs: A survey. arXiv preprint arXiv:2103.00742 (2021).\n[35] Guanghui Zhu, Shen Jiang, Xu Guo, Chunfeng Yuan, and Yihua Huang. 2022. Evo-\nlutionary Automated Feature Engineering. In Pacific Rim International Conference\non Artificial Intelligence. Springer, 574â€“586.\n[36] Guanghui Zhu, Zhuoer Xu, Chunfeng Yuan, and Yihua Huang. 2022. DIFER:\ndifferentiable automated feature engineering. In International Conference on\nAutomated Machine Learning. PMLR, 17â€“1.\n8\nAPPENDIX\n8.1\nRL data collector\nOur perspective is to view reinforcement intelligence as a training\ndata collector in order to achieve volume (self-learning enabled\nautomation), diversity(exploration), and quality (exploitation). To\nimplement this idea, we design reinforcement agents to automat-\nically decide how to perform feature crosses and generate new\nfeature spaces. The reinforcement exploration experiences and cor-\nresponding accuracy will be used as training data. Specifically, the\napproach includes: 1) Multi-Agents: We design three agents to\nperform feature cross: a head feature agent, an operation agent, and\na tail feature agent. 2) Actions: In each reinforcement iteration,\nthe three agents collaborate to select a head feature, an operator\n(e.g., +,-,*,/), and a tail feature to generate a new feature. The newly\ngenerated feature is later added to the feature set for the next fea-\nture generation. 3) Environment: The environment is the feature\nspace, representing an updated feature set. When three feature\nagents take actions to cross two features to generate a new fea-\nture and add the new feature to the previous feature set, the state\nof feature space (environment) changes. The state represents the\nstatistical characteristics of the selected feature subspace. 4) Re-\nward Function: The reward is an unsupervised feature set utility\nmeasurement proposed in methodology. 5) Training and Opti-\nmization: Our reinforcement data collector includes many feature\ncross steps. Each step includes two stages: control and training. In\nthe control stage, each feature agent takes actions based on their\npolicy networks, which take the current state as input and output\nrecommended actions and the next state. The three feature agents\nwill change the size and contents of a new feature space. We regard\nthe new feature space as an environment. Meanwhile, the actions\ntaken by feature agents generate an overall reward. This reward\nwill then be assigned to each participating agent. In the training\nstage, the agents train their policies via experience replay indepen-\ndently. The agent uses its corresponding mini-batch samples to\ntrain its Deep Q-Network (DQN), in order to obtain the maximum\nlong-term reward based on the Bellman Equation.\n8.2\nPseudo-code of the algorithm\nThe entire procedure algorithm is described as Algorithm 1.\n8.3\nTime complexity check with baselines\nWe compared our method with baseline methods on 6 selected\ndatasets over classification and regression tasks to report their time\nAlgorithm 1: Entire Procedure\nInput\n:The original dataset ğ·= (ğ‘‹,ğ‘¦)\nOutput:The Optimal Transformed Feature set F âˆ—\n1 . Initialize the encoder ğœ™, decoder ğœ“and evaluator ğœƒ.\n2 Unsupervised Measurement (Section 3.2):\n3 Develop an unsupervised measurement (MDCG) to assess\nthe utility of the feature set.\n4 Using RL and MDCG to collect ğ‘training data\nËœğ·= {Fğ‘–, Tğ‘–,ğ‘ ğ‘–}ğ‘\nğ‘–=1\n5 Graph Contrastive Pre-training (Section 3.3):\n6 for ğ‘–ğ‘›ğ‘ğ‘Ÿğ‘’ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›âˆ’ğ‘’ğ‘ğ‘œğ‘â„do\n7\nConstruct feature-feature similarity graphs Fğ‘–â†’Gğ‘–.\n8\nGraph augmentation Gğ‘–â†’(Gğ‘–,1, Gğ‘–,2).\n9\nEncode: zğ‘–,1 = ğœ™(Gğ‘–,1), zğ‘–,2 = ğœ™(Gğ‘–,2)\n10\nContrastive learning:\nLğ‘ğ‘™= âˆ’1\nğ‘\nÃğ‘\nğ‘›=1 ğ‘™ğ‘œğ‘”\nğ‘’ğ‘¥ğ‘(ğ‘ ğ‘–ğ‘š(zğ‘–,1,zğ‘–,2)/ğœ))\nÃğ‘\nğ‘–âˆ—=1,ğ‘–âˆ—â‰ ğ‘–ğ‘’ğ‘¥ğ‘(ğ‘ ğ‘–ğ‘š(zğ‘–,1,zğ‘–âˆ—,2)/ğœ) ,\n11 end\n12 Multi-Objective Fine-tuning (Section 3.4):\n13 for ğ‘–ğ‘›ğ‘“ğ‘–ğ‘›ğ‘’ğ‘¡ğ‘¢ğ‘›ğ‘’âˆ’ğ‘’ğ‘ğ‘œğ‘â„do\n14\nEncode: z = ğœ™(G).\n15\nEstimate: m, ğœ.\n16\nDecode loss: Lğ‘Ÿğ‘’ğ‘= âˆ’ğ‘™ğ‘œğ‘”ğ‘ƒğœ“(T |z).\n17\nEvaluate loss: Lğ‘’ğ‘£ğ‘¡= ğ‘€ğ‘†ğ¸(ğ‘ ,ğœƒ(z)).\n18\nBackward: L = ğ›¼Lğ‘’ğ‘£ğ‘¡+ ğ›½Lğ‘Ÿğ‘’ğ‘\n19 end\n20 Select top-ğ‘˜transformed feature sets (graphs) (G)ğ‘˜from Ëœğ·.\n21 Encode: (z)ğ‘˜= ğœ™((G)ğ‘˜).\n22 Update (z)ğ‘˜with ğœ‚steps: (z+)ğ‘˜= (z)ğ‘˜+ ğœ‚âˆ—\nğœ•ğœ—\nğœ•(z)ğ‘˜.\n23 Decode: (T +)ğ‘˜= ğœ“((z+)ğ‘˜).\n24 Optimal Transformed Feature Set:\nF âˆ—= arg max S((T +)ğ‘˜[ğ‘‹]).\ncosts, as shown in Table 3. The unsupervised baselines cost less\ntime, but are less accurate. Compared with the supervised baselines,\nour method costs less time than DIFFER and MOTA and costs a\nbit more time than the other four supervised. However, 1). Feature\ntransformation is not a data preparation step and not timing critical\nfor most tasks; 2) The time cost increase (mins level) of training\nour method is acceptable. Compared with the time costs (days or\nmonths level) of human manual feature engineering, our method\nsaves time; 3) Although our method costs a bit more time, it achieves\nmuch better feature engineering performance; 4) The inference time\nis small and it generates transformed features quickly in 0.6 1.7 s; 5)\nBased on such unsupervised and encode-decode generative design,\nwe can use the strategy of pretraining a foundation model and then\nfinetuning to reduce training time costs.\n8.4\nThe impact of different RL\nWe conducted a thorough examination of the impact of training\ndata generated by policy gradient and value learning reinforcement\nlearning (RL) algorithms, as well as data generated randomly, as\nshown in Table 4. Our findings indicate that training data produced\nTable 3: Time cost comparisons with baselines\nDataset\nRDG(s)\nPCA(s)\nLDA(s)\nERG(s)\nAFAT(s)\nNFS(s)\nTTG(s)\nGRFG(s)\nDIFFER(min)\nMOTA(min)\nNEAT(min)\nspectf\n2.9\n0.3\n0.3\n4.9\n3.8\n8.2\n9.1\n282\n70.8\n156.0\n52.3\nsvmguide3\n7.0\n0.2\n0.8\n10.3\n8.8\n7.1\n8.3\n373\n68.1\n191.8\n59.7\ngerman\n3.6\n0.2\n0.7\n6.0\n3.5\n6.1\n7.9\n182\n85.6\n105.6\n78.1\nopenml_616\n27.6\n0.2\n0.2\n34.1\n2.6\n18.1\n24.7\n936\n75.6\n430.1\n62.1\nopenml_618\n67.3\n0.2\n0.2\n82.4\n3.0\n30.9\n38.7\n821\n90.3\n373.8\n81.7\nopenml_620\n33.4\n0.2\n0.2\n42.1\n2.1\n17.0\n22.6\n1131\n74.5\n500.9\n63.8\nTable 4: The impact of different RL.\nDataset\nvalue iteration RL Policy gradient RL randomly generation\nspectf\n0.856\n0.856\n0.823\nsvmguide3\n0.832\n0.830\n0.821\ngerman\n0.723\n0.726\n0.715\nopenml_616\n0.536\n0.540\n0.493\nopenml_618\n0.616\n0.612\n0.520\nopenml_620\n0.584\n0.585\n0.564\nTable 5: Time complexity check of the different components\nin NEAT.\nDataset\nRL(min)\nModel training(min)\nInference(s)\nspectf\n14.2\n38.1\n1.1\nsvmguide3\n23.6\n36.1\n1.1\ngerman\n48.5\n29.6\n0.6\nopenml_616\n22.5\n39.8\n1.3\nopenml_618\n50.2\n31.4\n0.8\nopenml_620\n34.3\n29.5\n0.7\nusing RL algorithms significantly outperforms randomly generated\ndata in terms of enhancing model performance. However, it is im-\nportant to note that despite the overall superiority of RL-generated\ndata, the variations in performance between different RL designs\nare relatively limited. This suggests that while the choice of using\nRL over random generation is crucial for improving results, the\nspecific type of RL algorithm employed may not drastically alter\nthe final outcomes.\n8.5\nTime complexity check of the different\ncomponents in NEAT\nWe reported the computational overheads of 1) RL data collection;\n2) model training; 3) inference, as shown in Table 5. We found that\nthe additional time cost incurred by NEAT occurs during the data\ncollection and model training phases. However, once the model\nconverges, NEATâ€™s inference time is significantly reduced. The\npotential driving factors are that the RL-based collector spends more\ntime gathering high-quality data, and the sequence formulation\nacross the entire feature space increases the learning time cost\nfor the sequence model. Nonetheless, during the inference phase,\nNEAT outputs the entire feature transformation at once, resulting\nin a very short inference time.\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2024-05-27",
  "updated": "2024-05-27"
}