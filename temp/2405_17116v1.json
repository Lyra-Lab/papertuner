{
  "id": "http://arxiv.org/abs/2405.17116v1",
  "title": "Mixtures of Unsupervised Lexicon Classification",
  "authors": [
    "Peratham Wiriyathammabhum"
  ],
  "abstract": "This paper presents a mixture version of the method-of-moment unsupervised\nlexicon classification by an incorporation of a Dirichlet process.",
  "text": "arXiv:2405.17116v1  [cs.CL]  27 May 2024\nMixtures of Unsupervised Lexicon Classiﬁcation\nPeratham Wiriyathammabhum1[0000−0001−5567−3104]\nperatham.bkk@gmail.com\nAbstract. This paper presents a mixture version of the method-of-\nmoment unsupervised lexicon classiﬁcation by an incorporation of a Dirich-\nlet process.\nKeywords: Method-of-moments · Unsupervised lexicon classiﬁcation ·\nMixture models · Dirichlet process.\n1\nIntroduction\n1.1\nUnsupervised Lexicon Classiﬁcation (Bayesian\nLexicon/BayesLex)\nLexicon classiﬁcation utilizes a classiﬁcation scheme where the classiﬁcation\nprobabilities are estimated from counting word occurrences, x, corresponding\nto each lexicon for each class, W0 and W1, classes Y ∈{0, 1}, as in Eq. 1. This\nlexicon classiﬁcation scheme is useful for ﬁltering documents and is less compu-\ntationally expensive than many other classiﬁcation methods.\nX\ni∈W0\nxi ≷\nX\nj∈W1\nxj,\n(1)\nwhere ≷denotes a decision rule operator.\nUnsupervised lexicon classiﬁcation [1] refers to an unsupervised probabilistic\nweighting scheme where each word in each lexicon will have a weight value\ninstead of counting as one. The weights are learned from a corpus and ﬁtted as a\nmultinomial Naïve Bayes model using a method-of-moment estimator on cross-\nlexical co-occurrence counts. As in many typical probabilistic models, there are\nmany also assumptions which are equal prior likelihood, lexicon completeness,\nequal predictiveness of words, and equal coverage.\nTo elaborate more, we provide some derivations starting from the original\npaper, let we have a prior probability PY for the class label Y , a likelihood\nfunction PX|Y , where X is a bag-of-word input vector. Then, we can have a\nfamiliar classiﬁcation rule from the conditional probability P(y|x) as,\nP(y|x) =\nP(x|y)P(y)\nP\ny′ P(x|y′)P(y′).\n(2)\nBy this, we can also assume that the costs of misclassiﬁcation are equal and we\ncan have the minimum Bayes risk classiﬁcation rule as,\nlog Pr(Y = 0) + log P(x|Y = 0) ≷log Pr(Y = 1) + log P(x|Y = 1).\n(3)\n2\nPeratham Wiriyathammabhum\nThen, we assume P(x|y) is multinomially distributed since we have a multinomial\nNaïve Bayes model such that we can have,\nlog P(x|y) = log Pmultinomial(x; θy, N)\n= K(x) +\nV\nX\ni=1\nxi log Pr(W = i|Y = y; θ)\n= K(x) +\nV\nX\ni=1\nxi log θy,i\n(4)\n,where θy is a class vector, W is a word or token, V is the vocab size, N is the\ntotal count PV\ni=1 xi, and xi is the raw count of word i ∈{1, 2, . . ., V } as in the\nstandard bag-of-word deﬁnition. K(x) is some function of x that is constant in\ny.\nThen, we have a completeness assumption that the lexicon is complete, that\nis, a word that is not in any lexicon has identical probabilities under both classes.\nFurthermore, we have another assumption that every word in the lexicon is\nequally predictive, such that,\nPr(W = i|Y = y)\nPr(W = i|Y = ¬y) = 1 + γ\n1 −γ ,\n(5)\nwhere ¬y is the label for the opposite class and γ is a predictive parameter of\nthe lexicon. Given both assumptions, we can deﬁne,\nθy,i =\n\n\n\n\n\n(1 + γ)µi\n, i ∈Wy\n(1 −γ)µi\n, i ∈W¬y\nµi\n, i /∈Wy ∪W¬y\n,\n(6)\nwhere µi is the base probability independent of labels.\nWe introduce another assumption of equal prior likelihood that Pr(Y = 0) =\nPr(Y = 1) so that we can cancel out the prior terms in Eq. 3. Then, we have\nour new classiﬁcation rule similar to Eq. 1 as,\nlog P(x|Y = 0) ≷log P(x|Y = 1),\nX\ni∈W0\nxi log 1 + γ\n1 −γ ≷\nX\ni∈W1\nxi log 1 + γ\n1 −γ ,\n(7)\nwhere γ ∈(0, 1). We omitted other proofs and derivations from the original\npaper since they are unrelated to our focus except that we can have an objective\nfunction to solve for γ by using the cross label counts and the method-of-moment\nestimator as for the cross label counts ci, the count of word i with all the opposite\nlexicon words,\nci =\nT\nX\nt=1\nX\nj∈W¬y\nxt\nixt\nj,\n(8)\nMixtures of Unsupervised Lexicon Classiﬁcation\n3\nwhere t ∈1, . . . , T for T documents in the collection and xt is its vector of word\ncounts. We got the objective function as,\nmin\nγ(0),γ(1)\n1\n2\nX\ni∈W0\n(ci −E[ci])2 + 1\n2\nX\nj∈W1\n(cj −E[cj])2\ns.t. µ(0) ˙γ(0) −µ(1) ˙γ(1) = 0\n∀i ∈(W0 ∪W1), γi ∈[0, 1).\n(9)\nAfter the γ parameters are ﬁtted. The original paper uses 2 formula variants\nfor prediction rules, one derived from multinomial distribution as a multinomial\nNaïve Bayes in Eq. 7 and another as a plug-and-play Dirichlet-compound multi-\nnomial [2], which can be directly substituted from the multinomial distribution\nto better model burstiness, since the model only models word presences instead\nof counts, and is more predictive [3]. We are going to modify these aforemen-\ntioned equations, especially the prediction rules which compute probabilities as\nin Eq. 4 and Eq. 7, not the γ solver.\nThe thing is whether the model is a multinomial Naïve Bayes (maximum\nlikelihood) or a Dirichlet-compound multinomial Naïve Bayes (multinomial with\na Dirichlet prior), the model does not handle group clustering [6] (or in other\nwords, mixed membership). The group clustering problem arises as there are\ngrouped data which can be subdivided into a set of groups and each group has\nclusters of data while there are also components which are shared across groups.\nSome natural examples of grouped data [7] are binary markers (single nucleotide\npolymorphisms or SNPs) in a localized region of the human genome or text\ndocuments. In this paper, our focus in on textual data so we will brieﬂy elaborate\nonly this example that, for a document, given a bag-of-words assumption that\nthe orders of words are exchangeable, it can consist of words from many topics\nshared across many documents in a collection. We continue this Bayesian lexicon\nline of work and incorporate Dirichlet processes to see whether topical modeling\nor mixtures can help in giving weights to lexicon better.\nx\ny\nN\nFig. 1: A representation of a Naïve Bayes model.\n4\nPeratham Wiriyathammabhum\n1.2\nMixture Models\nMixture models [8,9] are combinations of basic distributions which can give rises\nto model complex densities. A mixture model is usually in the form of a linear\ncombination,\nf(y) =\ng\nX\ni=1\nπifi(y)\ns.t.\ng\nX\ni=1\nπi = 1, πi ∈[0, 1],\n(10)\nwhere f denotes a mixture probability mass function, fi denotes a probability\nmass function of a base distribution which can also be called as a component of\nthe mixture, g is the number of ﬁnite mixtures (or the number of components),\nand πi is a mixing proportion or a mixing coeﬃcient which also satisﬁes the\nrequirements to be probabilities. Sometimes, we can view the mixing proportions\nas prior probabilities of picking the ith components.\nx\nπ\ny\nN\nx\nz\nπ\ny\nN\nFig. 2: A representation of a mixture model. The right plate diagram has a latent\nvariable z. In the left plate diagram, π is a variable instead of a constant.\n1.3\nDirichlet Processes\nA Dirichlet distribution is a multivariate generalization of a Beta distribution\nto more than 2 variables, in a similar spirit to a multinomial distribution to a\nBernoulli distribution. A K-dimensional/outcome Dirichlet distribution, Dir(µ|α),\nMixtures of Unsupervised Lexicon Classiﬁcation\n5\nis deﬁned as a probability density function,\nDir(µ|α) = C−1(α)\nK\nY\nk=1\nµαk−1\nC(α) =\nPK\nk=1 Γ(αk)\nΓ(PK\nk=1 αk)\n=\nPK\nk=1 Γ(αk)\nΓ(α0)\nΓ(α) =\nZ ∞\n0\nuα−1e−udu\ns.t.\nK\nX\nk=1\nµi = 1, µi ∈[0, 1],\n(11)\nwhere Γ(α) is the gamma function, C(α) is the beta function as a normalizer,\nall αk are non-negative parameters of the distribution, αo is the concentration\nparameter, and all µi ∈[0, 1] are non-negative input random base distributions.\nA symmetric Dirichlet distribution or prior has all αk = α\nK .\nDirichlet processes [4–7] are discrete-time stochastic processes whose realiza-\ntions are distributed as Dirichlet distributions. Some people call this as distribu-\ntions over distributions. Formally, let (Θ, B) be a measurable space, G0 is a prob-\nability measure on this space, and α0 is a positive real constant number. A Dirich-\nlet process DP(α0, G0) is the distribution of a random positive measure G over\n(Θ, B) such that for any ﬁnite measurable partition (A1, A2, . . . , Ag) of Θ, a ran-\ndom vector comprising of G, (G(A1), G(A2), . . . , G(Ag)), is distributed as a ﬁnite\nDirichlet distribution with parameters, (α0G0(A1), α0G0(A2), . . . , α0G0(Ag)).\nThat is,\n(G(A1), G(A2), . . . , G(Ag)) ∼Dir(α0G0(A1), α0G0(A2), . . . , α0G0(Ag)), (12)\nor, in short, G ∼DP(α0, G0). Dirichlet processes can also be deﬁned as a stick-\nbreaking construction, a Pólya-urn process, a Chinese restaurant process, or\ntaking an inﬁnite limit of ﬁnite mixture models. These alternative deﬁnitions\nare too verbose and will not be mentioned in this paper for brevity, except\nsome.\nDirichlet Process Mixture Models One of some interesting applications of\nDirichlet processes is Dirichlet process mixture models (DPMMs) by using a\nDirichlet process as a prior for the parameters of mixture models.\n1.4\nHierarchical Dirichlet Processes\n1.5\nContributions\nThis paper makes the contributions as follows:\n– We propose a new family of mixture models for unsupervised lexicon classi-\nﬁcation/Bayesian lexicon/BayesLex which can handle group clustering.\n– We provide a justiﬁcation that aggregating lexicon scores is equivalent to a\nmixture of multinomial Naïve Bayes models.\n– We\n6\nPeratham Wiriyathammabhum\nx\nθ\ny\nG\nG0\nα0\nN\nFig. 3: A representation of a Dirichlet process mixture model.\n2\nRelated Works\nThere are some works whose ideas have gone in the direction of incorporating\nDirichlet processes to the Naïve Bayes classiﬁers.\n3\nMixtures of Unsupervised Lexicon Classiﬁcation\n3.1\nMixture Models\nInstead of modeling as a multinomial Naïve Bayes for the likelihood P(x|y) in\nEq. 4, we instead use a ﬁnite mixture of multinomial likelihood as,\nlog P(x|y) = log\ng\nX\ni=1\nπiPmultinomiali(x; θy, N).\n(13)\nThis does not have a closed form solution since it is in the form of a sum inside a\nlog function. Besides, it does not look like any aforementioned decision rules like\nEq. 1 or 7 to justify its equivalent to lexicon classiﬁcation. Therefore, we need to\nrearrange the terms using some identities. Here, we can use (x + y) = x(1 + y\nx)\nMixtures of Unsupervised Lexicon Classiﬁcation\n7\nand a logarithm product rule so that log (x + y) = log (x) + log (1 + y\nx). Then,\nlog P(x|y) = log\ng\nX\ni=1\nπiPmultinomiali(x; θy, N)\n=\ng\nX\ni=1\nlog πiPmultinomiali(x; θy, N)\n+ [log (1 +\nPg\nj=2 πjPmultinomialj (x; θy, N)\nπ1Pmultinomial1(x; θy, N)\n)\n+ log (1 +\nPg\nj=3 πjPmultinomialj (x; θy, N)\nπ2Pmultinomial2(x; θy, N)\n)\n+ . . .\n+ log (1 +\nπgPmultinomialg(x; θy, N)\nπg−1Pmultinomialg−1(x; θy, N))]\n=\ng\nX\ni=1\nlog πiPmultinomiali(x; θy, N) + L(x, y).\n(14)\nApplying this to the decision rule in Eq. 3, we can have,\nlog Pr(Y = 0) + log P(x|Y = 0) ≷log Pr(Y = 1) + log P(x|Y = 1)\nlog P(x|Y = 0) ≷log P(x|Y = 1)\nX\ni∈W0\n[\ng\nX\nj=1\nxi log 1 + γj\n1 −γj\n+ L(x, y)] ≷\nX\ni∈W1\n[\ng\nX\nj=1\nxi log 1 + γj\n1 −γj\n+ L(x, y)].\n(15)\nClearly, the sums of log term obey the previous decision rule in Eq. 7. But, we\nstill need to show that the L(x, y) terms obey the decision rule too. Fortunately,\nby canceling out πjPmultinomialj in the fractions of L(x, y) terms, the remaining\nconsists of only combinations of the πs and γs, which we can further assume\nthat are equal for both Y and independent of x, so the L(x, y) terms cancel out.\nWe can have our new decision rule as a simple summation,\nX\ni∈W0\n[\ng\nX\nj=1\nxi log 1 + γj\n1 −γj\n] ≷\nX\ni∈W1\n[\ng\nX\nj=1\nxi log 1 + γj\n1 −γj\n].\n(16)\nThis is like aggregating scores from many lexicons and completes the proof for\nthe ﬁnite mixture case. This is also consistent with some published empirical\nresults, such as [11,12], whose results have shown that aggregating scores, even\nlinearly or with diﬀerent lexicon sets, improves the performance. Here, we provide\na justiﬁcation that aggregating many lexicons is equivalent to a ﬁnite mixture\nmultinomial Naïve Bayes model. We also noted that the component base mod-\nels must be diﬀerent or else it is just no diﬀerence than a base single model.\nFurthermore, as can be easily seen in our proof, we ﬁx the vocabulary of the\n8\nPeratham Wiriyathammabhum\nlexicon, W0 and W1. Extending to aggregating many diﬀerent lexicon sets might\nbe straightforward.\nMoreover, another way to resolve this [10] is to introduce a latent variable z\nwhich factorizes out the summation.\n3.2\nDirichlet Process Mixtures\nInstead of modeling as a multinomial Naïve Bayes for the likelihood P(x|y)\nin Eq. 4, we derive a new decision rule as we place a Dirichlet process prior\non the multinomial Naïve Bayes to create a mixture of models using similar\nassumptions.\nPDP (x; α) =\nZ\nπ\ndπP(x|π)P(π|α)\n(17)\nπ | αo ∼GEM(α0)\nzi | π ∼π\nφk | G0 ∼G0\nxi | zi, (φk)∞\nk=1 ∼F(φzi)\n(18)\nPDP (x|y) =\n(19)\nFrom Eq. 4, we instead put the nonparametric Dirichlet process prior as,\nlog P(x|y) = log PDP (x; α)\n= log PDP (x; α)\n(20)\n3.3\nHierarchical Dirichlet Process Mixtures\n4\nExperimental Results\n5\nConclusions and Discussions\nReferences\n1. Eisenstein, Jacob. \"Unsupervised learning for lexicon-based classiﬁcation.\" Proceed-\nings of the AAAI Conference on Artiﬁcial Intelligence. Vol. 31. No. 1. 2017.\n2. Madsen, Rasmus E., David Kauchak, and Charles Elkan. \"Modeling word burstiness\nusing the Dirichlet distribution.\" Proceedings of the 22nd international conference\non Machine learning. 2005.\n3. Pang, Bo, Lillian Lee, and Shivakumar Vaithyanathan. \"Thumbs up? Sentiment\nClassiﬁcation using Machine Learning Techniques.\" Proceedings of the 2002 Con-\nference on Empirical Methods in Natural Language Processing (EMNLP 2002).\n2002.\n4. Ferguson, Thomas S. \"A Bayesian analysis of some nonparametric problems.\" The\nannals of statistics (1973): 209-230.\n5. Teh, Yee Whye. \"Dirichlet Process.\" Encyclopedia of machine learning 1063 (2010):\n280-287.\n6. Teh, Yee, et al. \"Sharing clusters among related groups: Hierarchical Dirichlet pro-\ncesses.\" Advances in neural information processing systems 17 (2004).\nMixtures of Unsupervised Lexicon Classiﬁcation\n9\n7. Teh, Yee Whye, et al. \"Hierarchical Dirichlet Processes.\" Journal of the American\nStatistical Association 101 (2006): 1566-1581.\n8. McLachlan, Geoﬀrey J., and Kaye E. Basford. Mixture models: Inference and ap-\nplications to clustering. Vol. 38. New York: M. Dekker, 1988.\n9. McLachlan, Geoﬀrey J., Sharon X. Lee, and Suren I. Rathnayake. \"Finite mixture\nmodels.\" Annual review of statistics and its application 6 (2019): 355-378.\n10. Bishop, Christopher M. \"Pattern recognition and machine learning.\" Springer\ngoogle schola 2 (2006): 1122-1128.\n11. Wiriyathammabhum, Peratham. \"PromptShots at the FinNLP-2022 ERAI Task:\nPairwise Comparison and Unsupervised Ranking.\" Proceedings of the Fourth Work-\nshop on Financial Technology and Natural Language Processing (FinNLP). 2022.\n12. Czarnek, Gabriela, and David Stillwell. \"Two is better than one: Using a single\nemotion lexicon can lead to unreliable conclusions.\" Plos one 17.10 (2022): e0275910.\nϬ\nϱ\nϭϬ\nϭϱ\nϮϬ\nϮϱ\nϯϬ\nϯϱ\nϰϬ\nϰϱ\nϱϬ\nϬ\nϱ\nϭϬ\nϭϱ\nϮϬ\nϮϱ\nϯϬ\n\u0018ĂƚĂ\u0003\u0004\n\u0018ĂƚĂ\u0003\u0011\n",
  "categories": [
    "cs.CL",
    "cs.AI",
    "cs.CV",
    "cs.LG"
  ],
  "published": "2024-05-27",
  "updated": "2024-05-27"
}