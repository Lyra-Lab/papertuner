{
  "id": "http://arxiv.org/abs/2405.17287v2",
  "title": "Opinion-Guided Reinforcement Learning",
  "authors": [
    "Kyanna Dagenais",
    "Istvan David"
  ],
  "abstract": "Human guidance is often desired in reinforcement learning to improve the\nperformance of the learning agent. However, human insights are often mere\nopinions and educated guesses rather than well-formulated arguments. While\nopinions are subject to uncertainty, e.g., due to partial informedness or\nignorance about a problem, they also emerge earlier than hard evidence can be\nproduced. Thus, guiding reinforcement learning agents by way of opinions offers\nthe potential for more performant learning processes, but comes with the\nchallenge of modeling and managing opinions in a formal way. In this article,\nwe present a method to guide reinforcement learning agents through opinions. To\nthis end, we provide an end-to-end method to model and manage advisors'\nopinions. To assess the utility of the approach, we evaluate it with synthetic\n(oracle) and human advisors, at different levels of uncertainty, and under\nmultiple advice strategies. Our results indicate that opinions, even if\nuncertain, improve the performance of reinforcement learning agents, resulting\nin higher rewards, more efficient exploration, and a better reinforced policy.\nAlthough we demonstrate our approach through a two-dimensional topological\nrunning example, our approach is applicable to complex problems with higher\ndimensions as well.",
  "text": "Opinion-Guided Reinforcement Learning\nKyanna Dagenais1* and Istvan David1*\n1Department of Computing and Software, McMaster University, Hamilton, ON,\nCanada.\n*Corresponding author(s). E-mail(s): dagenaik@mcmaster.ca;\nistvan.david@mcmaster.ca;\nAbstract\nHuman guidance is often desired in reinforcement learning to improve the performance of the\nlearning agent. However, human insights are often mere opinions and educated guesses rather\nthan well-formulated arguments. While opinions are subject to uncertainty, e.g., due to partial\ninformedness or ignorance about a problem, they also emerge earlier than hard evidence can be\nproduced. Thus, guiding reinforcement learning agents by way of opinions offers the potential\nfor more performant learning processes, but comes with the challenge of modeling and managing\nopinions in a formal way. In this article, we present a method to guide reinforcement learning\nagents through opinions. To this end, we provide an end-to-end method to model and manage\nadvisors’ opinions. To assess the utility of the approach, we evaluate it with synthetic (oracle)\nand human advisors, at different levels of uncertainty, and under multiple advice strategies.\nOur results indicate that opinions, even if uncertain, improve the performance of reinforcement\nlearning agents, resulting in higher rewards, more efficient exploration, and a better reinforced\npolicy. Although we demonstrate our approach through a two-dimensional topological running\nexample, our approach is applicable to complex problems with higher dimensions as well.\nKeywords: artificial intelligence, belief, domain-specific languages, guided reinforcement learning,\nhuman guidance, machine learning, opinion, subjective logic, uncertainty\n1 Introduction\nReinforcement learning (RL) (Sutton and Barto, 2018) is a machine learning paradigm in which an\nautonomous agent explores its surroundings and learns optimal behavior through trial and error.\nBecause of the continuous learning process, RL is particularly well-suited to deal with open-ended\nproblems that feature unforeseen situations, tasks, and environments. Thanks to these benefits, the\npopularity of RL has been steadily increasing in domains where dealing with complexity is essential,\nsuch as fine-tuning the control of manufacturing processes at runtime (Cronrath et al, 2019), inferring\nsimulation components of digital twins (David and Syriani, 2024), and generating repair actions for\nconceptual domain models (Barriga et al, 2022).\nHowever, autonomous behavior does not obviate the need for human agency (Bradshaw et al,\n2013). Human input to autonomous systems often augments machine intelligence with higher-level\nstrategic directives that require creative problem-solving skills (Najar and Chetouani, 2021). RL is\nno exception to this rule either. The body of knowledge on guided RL methods is substantial and\nrapidly growing.\nGeneral guidance-based techniques allow humans to inform agents about future aspects of the\ntask, e.g., interesting regions to explore (Subramanian et al, 2016) or a trajectory (i.e., a sequence\nof actions) to follow (Thomaz and Breazeal, 2006). A severe limitation of such techniques is the\ninconsistent correctness of human advice. As Scherf et al (2022) report, the majority of interactive\nor guided RL approaches assume the human input to be useful and correct (Li et al, 2019). How-\never, this is not always true in real applications (Kessler Faulkner and Thomaz, 2021; Koert et al,\n1\narXiv:2405.17287v2  [cs.LG]  3 Aug 2024\n2020; Arakawa et al, 2018). Informed advisors, such as experts, often express advice in the form of\nopinions—cognitive constructs that are subject to epistemic uncertainty, i.e., uncertainty that is an\nartifact of a lack of knowledge, partial informedness, and ignorance. Opinions about the solutions\nor constraints of a problem emerge earlier than hard evidence can be produced. Thus, relying on\nopinions to provide advice to RL agents results in more agile and efficient RL methods. For this, epis-\ntemic uncertainty has to be approached in a formal way in order to avoid unsound advice that would\nadversely affect the agent. Unfortunately, despite the fact that the importance of dealing with uncer-\ntainty in machine learning has been well-recognized (H¨ullermeier and Waegeman, 2021), there is a\nlack of RL methods that treat epistemic uncertainty as a first-class citizen in guided RL. Specifically,\nmethods for managing epistemic uncertainty are missing (H¨ullermeier and Waegeman, 2021).\nTraditional probability cannot handle constructs such as ignorance or partial knowledge, and can\nlead to unsound formalization of the advice. For example, when the advisor lacks knowledge about a\nparticular situation, the only way to model the advice is to assign a 0.5 probability to it. A probability\nof 0.5 means that given a statement x captured in the advice (e.g., “there is high reward at location\n[2, 3]” in a grid world), x and not (x) are equally likely. This clearly does not represent ignorance and,\nthus, could mislead the RL agent. In general, forcing users to express their opinions by traditional\nprobability could lead to unreliable conclusions. The RL agent needs to know that the advice is\nbased on lacking knowledge in order to consider it with proper weight. It would be, thus, preferable\nto be able to say “I don’t know exactly” or “I’m not sure” within a piece of advice, and couple these\nmodifiers with a particular statement (which might exhibit traits of traditional probability).\nSubjective logic (Jøsang, 2016) is an extension of probabilistic logic (Adams, 1996), in which users\ncan express opinions by quantified parameters of belief and certainty. Opinions are formed from a\nbelief component and an uncertainty component. In statistics and economics, the uncertainty com-\nponent of subjective logic is often called second-order probability (Gardenfors and Sahlin, 2005), and\nis represented in terms of a probability density function over first-order probabilities. Unfortunately,\nthe complex framework of subjective logic limits its applicability. Most techniques that rely on it\nresort to simplifications, e.g., assuming that humans can express their opinions in terms of mathe-\nmatical abstractions (Burgue˜no et al, 2023); or emulating human opinion by objectively measured\nmetrics, taking away the core subjective element of the approach (Walkinshaw and Shepperd, 2020).\nGoal and research questions.\nOur goal is to evaluate the feasibility and utility of guiding RL agents\nby opinions—i.e., through advice that is subject to belief uncertainty—captured in constructs of\nsubjective logic. To meet our goal, we formulate the following research questions.\nRQ1. How can one use opinions to guide reinforcement learning agents?\nBy answering this research question, we aim to identify methods to formally externalize\nuncertain opinions and introduce them into the agent’s policy as guiding pieces of information.\nRQ2. How does opinion-based guidance affect the performance of reinforcement learning agents?\nBy answering this research question, we aim to identify relevant changes in key performance\nmetrics of opinion-guided RL agents.\nWe develop a method in which opinions are formulated through the sound mathematical foun-\ndations of subjective logic, and fused into the policy of RL agents. Subsequently, we evaluate the\nperformance of the advised agent through a series of experiments. Our results indicate that opinions,\neven if uncertain, improve the performance of RL agents, resulting in higher rewards, more efficient\nexploration, and a better reinforced policy. Since opinions emerge earlier and more easily than hard\nevidence can be produced (Dagenais and David, 2024), opinion-guided RL offers a more efficient and\neconomical alternative to traditional guided RL approaches.\nContributions.\nThe main contribution of this work is a modeling approach for guiding RL through\nopinions, i.e., advice that is subject to belief uncertainty. We investigate various flavors of RL algo-\nrithms and show where and how to augment them with advice expressed via subjective logic. Since\nthe human-in-the-loop can quickly become a bottleneck in human-machine collaboration due to its\nexpensive reward function (Christiano et al, 2017), we also recommend architectural patterns to\nimplement opinion-guided RL. Through detailed experiments, we show that the performance of RL\nagents improves with added human input and improves again with added uncertainty and disbelief\n2\ninformation. Our approach fosters more efficient collaboration between human and machine agents (Li\net al, 2021) while improving the performance of RL agents.\nStructure.\nThe rest of this paper is structured as follows. In Sec. 2, we present the running example\nwe use throughout the paper to illustrate key concepts, and to drive the evaluation of our approach.\nIn Sec. 3, we briefly review the background topics relevant to our work and review the related work.\nIn Sec. 4, we outline our approach to modeling human belief and certainty in RL. In Sec. 5, we\nevaluate our approach under various advice types, sources, and learning parameters. In Sec. 6, we\ndiscuss the results and their implications and outline a set of open challenges. In Sec. 7, we reflect on\nthe threats to the validity of this study and discuss the mitigation strategies we applied. In Sec. 8,\nwe draw the conclusions and outline future work.\n2 Running example\nTo illustrate the principles of our approach, we rely on the following running example of a self-driving\ncar through a frozen lake. The example is analogous to Open AI’s Gym’s Frozen Lake environment.1\nStart\nGoal\nFrozen\nHole\n0\n0\n1\n1\n2\n3\n2\n3\nFig. 1: The Frozen Lake running example\nFig. 1 visualizes the Frozen Lake; an ordinary grid world, on which an agent (a self-driving car)\nmoves one square at a time. The agent begins its exploration at the top leftmost tile (Start). Its aim\nis to reach the bottom rightmost tile (Goal). The agent traverses the Frozen Lake one tile at a time\nby choosing to move up, down, left, or right. There are three types of tiles the agent may encounter:\nseveral frozen tiles and holes, and one goal tile. Encountering these tiles has different consequences,\nas shown in Tab. 1.\nTable 1: Reward structure\nEvent\nConsequence\nReward\nEncountering a frozen tile\nNothing happens; the agent is allowed to continue\nthe traversal of the environment.\n0\nEncountering a hole\nThe task ends unsuccessfully.\n0\nEncountering the goal\nThe task is completed successfully.\n1\nThe goal of the agent is to learn the optimal path from the top leftmost tile to the bottom\nrightmost tile under the given reward structure. Through trial-and-error, the agent gradually learns\nwhich actions are beneficial in a specific situation. For example, in cell [2,2], the agent should\nnot move to the right, because this action would result in stepping into a hole and terminating the\nexploration unsuccessfully, with a reward of 0. Instead, in cell [2,2], the agent should move down,\nbecause cell [3,2] is a frozen tile, and it it allows the agent to move closer to the goal, which is in\ncell [3,3].\nEventually, the agent will develop a policy, which will allow it to find the goal faster and in a\nmore reliable fashion (i.e., not falling into holes).\n1https://gymnasium.farama.org/environments/toy text/frozen lake/\n3\n3 Background and related work\nHere, we provide a brief overview of the background of our work and review the related approaches.\n3.1 Reinforcement learning\nFig. 2: Reinforcement learning – conceptual overview (Sutton and Barto, 2018)\nThe RL setting (Fig. 2) as described by Sutton and Barto (2018), involves an agent that sequen-\ntially explores its environment, and uses feedback in the form of rewards to learn the optimal control\nof said environment. Typically, the RL process is formalized by finite horizon Markov decision pro-\ncesses (Puterman, 1990), defined as a four-tuple ⟨S, A, P, rt(s, a)⟩. Here, S denotes the set of states\nthe agent may observe from the environment. A is the set of actions the agent may take in the envi-\nronment. R ⊂R is the set of rewards, and rt(s, a) ∈R is the numeric reward received for choosing\naction a ∈A while in state s ∈S at time t. P = p(s′, r∣s, a) defines the dynamics of the environment,\nthat is, the probability of the environment transitioning to state s′ ∈S and producing reward r ∈R\ngiven the agent is in state s ∈S and performs action a ∈A. At each time step t, the agent observes a\nstate St ∈S, and carries out an action At ∈A. At the next time step, t+1, the environment produces\na new state St+1 as well as a reward Rt+1 ∈R based on the environment dynamics defined by P.\nThe agent uses a mapping from states to actions, called the policy π(a∣s), which gives the\nprobability of the agent taking action a ∈A given state s ∈S. The agent’s sequence of\nactions, as well as the states and rewards produced by the environment define the trajectory τ =\n(S0, A0, R1, ..., ST −1, AT −1, RT ) where T is the time of termination. By sampling many trajectories\nthrough acting in the environment, the goal of the agent is to act according to an optimal policy π∗,\nthat maximizes the sum of rewards, called the expected return, defined as Gt = Rt+1 +Rt+2 +...+RT .\n3.1.1 Various flavors of RL\nWhile all RL algorithms aim to maximize expected return, various methods achieve this goal dif-\nferently. Value-based methods aim to maximize either the state-value function vπ(s) = Eπ[Gt∣s],\nthe action-value function qπ(s, a) = Eπ[Gt∣s, a], or both. Value-based approaches are determinis-\ntic because they select actions greedily when maximizing the value function, which might lead to\nunder-exploration. In contrast, the goal of policy-based methods is to learn a parameterized policy\nthat maximizes the reward function in every state without using value functions. Policy-based meth-\nods explore the state space more thoroughly than value-based methods, but produce estimates with\nmore noise, potentially resulting in unstable learning processes. Finally, actor-critic methods provide\na trade-off between value-based and policy-based methods, in which the actor implements a policy-\nbased strategy, and the critic criticizes the actions made by the actor based on a value function.\nDifferent methods offer various benefits and perform with unique utility in specific learning problems.\nIn particular, policy gradient is a subset of the aforementioned policy-based methods. In policy\ngradient, the policy is parameterized through a parameter vector θ. For reasonably sized action/state\nspaces, the policy may be parameterized as numerical preferences for all state-action pairs h(s, a, θ) ∈\nR, and actions can then be chosen using a soft-max distribution. This is called discrete policy gradient,\nand often, a matrix-like representation of a look-up table is used. In contrast, the policy may be\nparameterized via a neural network such that θ is a vector of network connection weights or linear\nin-features, which defines deep policy gradient. In either case, the policy is defined as π(a∣s, θ), the\nprobability of taking action a in state s with parameter θ. The policy may be parameterized in\nany manner, subject to the condition that it is differentiable with respect to θ. This way, θ may be\nupdated to maximize some scalar performance metric J(θ).\n4\n3.2 Subjective logic\nTo formalize human guidance in RL, we rely on subjective logic. Subjective logic (Jøsang, 2016) is\nan extension of probabilistic logic (Adams, 1996), in which users can express opinions by quantified\nparameters of belief and certainty. Quantifying opinions is an improvement over traditional logic\nsystems as it promotes opinions to first-class citizens in a systematic fashion. Well-informed opinions\ncan be early indicators of emerging new knowledge and often, opinions are the only available insights\nin engineering. Subjective logic has seen growing applications, such as in model-driven software\nengineering, e.g., by Burgue˜no et al (2023), where it is used to model belief uncertainty (Barquero\net al, 2021) in domain models. As well, it has been used in knowledge graphs as per Navarrete and\nVallecillo (2021) to allow reasoning about graph databases enriched with uncertainty.\n3.2.1 Formal underpinnings\nGiven a boolean predicate x, a binomial opinion regarding the truth of x is given by ωx =\n(bx, dx, ux, ax), where bx represents the belief in the truthfulness of x; dx represents the disbelief in\nthe truthfulness of x; ux quantifies the degree of epistemic uncertainty or uncommitted belief con-\ncerning the truthfulness of x; and ax represents the base rate, i.e., the prior probability of x being\ntrue in the absence of (dis)belief. For each parameter, 0 ≤bx, dx, ux, ax ≤1. These parameters satisfy\nthe following conditions.\nbx + dx + ux = 1\n(1)\nPx = bx + axux.\n(2)\nEquation 2 expresses the projected probability of an opinion, effectively transforming opinions into\nthe probability domain, where P(x) means the probability that boolean predicate x holds. It is clear\nthat as uncertainty ux increases, projected probability Px is closer to base rate ax. In contrast, as\nuncertainty ux decreases, projected probability Px is closer to that of the belief parameter bx. Both\nEquations 1 and 2 are important invariants that we rely on throughout the paper, particularly in\nSections 4 and 5.\nA traditional probability p corresponds with ux = 0, and can thus be transformed into a binomial\nopinion as follows.\nωx = (p, 1 −p, 0, p)\n(3)\nInformed opinions work well at scale. That is, the more experts that express their opinions, the\nhigher the credibility of a collective opinion. Collective opinions can be inferred by semantically sound\nfusion operators (Jøsang et al, 2013). A fusion operator is a function f ∶Ω×Ω→Ωthat maps a pair\nof opinions onto a new, fused opinion. Various fusion semantics exist, and the right operator must\nbe chosen based on its fit for purpose. A detailed account of fusion operators is given in Sec. 4.3.3.\nExample Formulating opinion about the Frozen Lake\nLet x be a boolean predicate regarding frozen lake as follows; x: action a is beneficial when in\nstate s. Now, consider moving right while in cell [1, 0] of Fig. 1. This would cause the agent to\nmove to a hole tile, resulting in the task to ending unsuccessfully. In this case, a reasonable opinion\nwould be that the action of moving right is likely not beneficial while in cell [1, 0], corresponding to\nω = (0.0, 1.0, 0.0, 0.25), meaning there is absolute disbelief that moving right while in cell [1, 0] is\nbeneficial.\n3.2.2 Challenges in employing subjective logic\nThe high expressive power of subjective logic comes with the added challenge of calibrating its\nparameters, particularly uncertainty u and base rate a. There are two schools of thought to address\nthis problem. In permissive techniques, setting uncertainty and belief-disbelief are deferred to the\nuser (Burgue˜no et al, 2023; Jongeling and Vallecillo, 2023), and it is assumed that these parameters\nwill become available at some point. However, it is usually not explained when, how, and who has to\nset these parameters, leading to a limited applicability of subjective logic. In restrictive techniques,\nusers are not asked to parameterize the framework. Instead, parameters are controlled by external\nmeasures (Jøsang and Bondi, 2000; Margoni and Walkinshaw, 2023). Unfortunately, this takes away\nsubjective elements from subjective logic. Margoni and Walkinshaw (2023) and Walkinshaw and\n5\nShepperd (2020) use statistical evidence to set values, employing subjective logic to analyze empirical\nstudies and experimental results, respectively. In regards to base rate ax, authors often opt to set this\nvalue via statistical evidence (Jøsang and Bondi, 2000; Margoni and Walkinshaw, 2023). Navarrete\nand Vallecillo (2021) allow users to directly indicate the base rate, and Walkinshaw and Shepperd\n(2020) assume ax = 0.5.\nIn our approach, we alleviate to cognitive load on the advisor by deriving base rate a from the\nstructure of the problem; and calibrate uncertainty u by appropriate distance metrics between advisor\nand advised state (Sec. 4.2).\n3.3 Related work\nThe closest ours is the work fo Guo et al (2022), who combine feedback from several human trainers\nto create a reliable reward for the RL agent. Subjective logic is used to model the agent’s opinion\nor trust of the human trainer. Converting this opinion into a probability yields a measure of the\ntrainer’s trustworthiness, and after the agent utilizes human feedback, the agent’s opinion about the\ntrainers is updated. In contrast, our work uses subjective logic the other way around: to model the\nadvisor’s opinion about the environment, and uses that opinion to drive policy shaping and improve\nthe agent’s performance.\nOther related works focus either on guiding RL agents but without the use of subjective logic\n(Sec. 3.3.2) and employing subjective logic in RL but not for guidance (Sec. 3.3.1).\n3.3.1 Subjective logic in reinforcement learning (but not for guidance)\nThe combination of RL and subjective logic is present in a number of publications. Notable work\nincludes the one by Zennaro and Jøsang (2020), an initial inquiry into using subjective logic in multi-\narmed bandits, a simplified RL problem. In this work, the agent forms an opinion over the available\nactions, and if the agent receives a positive reward for an action, the opinion about said action can\nbe updated. Subjective logic to capture uncertainty in multi-armed bandit like problems is extended\nby Wang and Grace (2022). Similarly, Guo et al (2023) use a Deep Reinforcement Learning (DRL)\nmulti-agent in smart farm monitoring to collect data from sensors on cattle. Opinions about each\nanimal’s condition are computed via this sensor data. Zhou et al (2021) use RL to learn a low\ndimensional representation of safe regions of complex dynamical systems. Safety of states is estimated\nusing DSAF (Discretized Safety Assessment Function), which is described with subjective logic. Guo\net al (2023) use subjective logic to model and compute uncertainty in a DRL framework used to\nidentify the intents of tweets. Zhao et al (2019) use opinions to represent edges in a graph network,\nand for unknown edges, a set of best paths is determined via DRL. This work proposes 3 different\nDRL models, each with reward given based on different types of uncertainty that are computed via\nsubjective logic. Similarly, Zhao et al (2019) use subjective logic to represent users within an Online\nSocial Network. Different types of users are initialized with particular opinions, which can be updated\nvia subjective logic fusion operators to enhance DRL based Competitive Influence Maximization.\nWhile these works combine RL and subjective logic, our work differs by modeling both RL policy and\nuser advice in subjective logic, and using this model to guide the subjective logic agent via human\nopinion.\n3.3.2 Guidance in reinforcement learning (without Subjective Logic)\nWhile RL algorithms are self-sufficient-that is, they can run without external intervention-there is\na large body of work related to incorporating external human knowledge into these algorithms to\nimprove performance. There are a number of differing approaches for incorporating human advice\ninto RL without the use of subjective logic. Najar and Chetouani (2021) survey the field of RL with\nhuman advice, which they define as “teaching signals that can be communicated by the teacher to\nthe learning system without executing the task”. They define a taxonomy of how advice is provided,\nfirst differentiating general advice from contextual advice. General advice is characterized by general\nconstraints and construction, such as if-then rules (Maclin and Shavlik, 1996), or action plans (Vogel\nand Jurafsky, 2010). In contrast, contextual advice depends on the context in which it is given, and is\nsubdivided into feedback, and guidance. Feedback methods aim to be evaluative and corrective (Cruz\net al, 2015; Dai et al, 2023). In preference based RL (F¨urnkranz et al, 2012), trajectories are labeled\nwith human preferences. A set of preferences is a partial ordering of trajectories. In this manner,\none trajectory can be labeled as preferred over the other. Preferences allow for limited guidance of\n6\nRL agents, but fall short of representing uncertainty. Recent work (Zhang and Kashima, 2024) has\naimed to improve interpretability of agents trained with this framework.\nGuidance involves using human input to bias the exploration strategy, such as probabilistic, early,\nand importance advising as detailed in Cruz et al (2017). Contextual instructions are a subset of\nguidance, where advice is given about one action in a particular situation. This is present in the work\nof Grizou et al (2013), where a robot is given guidance about the next action to take via spoken\ninstruction.\nNajar and Chetouani (2021) also describe strategies to incorporate advice into RL algorithms\nas shaping methods. These methods are defined based on what point in the learning process they\nintegrated, and include reward shaping, value shaping, policy shaping, and decision biasing. Reward\nshaping is a particular form of advice that is given directly and translated into numerical rewards.\nThis makes rewards provided by the advisor analogous to the rewards provided by the environment.\nOften, these rewards are given in the form of feedback, e.g., in the work of Tenorio-Gonzalez et al\n(2010), where verbal feedback is used to provide additional reward to the agent. In value shaping,\nadvice is considered as a function of action preference. This advice can be used to update the\nagent’s value function (Najar et al, 2016). In policy shaping, advice is incorporated into the agents\npolicy to bias the exploration strategy. This has been used in several works (Griffith et al, 2013;\nKnox and Stone, 2009; Kessler Faulkner and Thomaz, 2021; Brawer et al, 2023; Cederborg et al,\n2015). In decision biasing, advice is used to directly influence the policy output. This advice is not\nincorporated into any structures of the RL algorithm, and the agent learns from the outcomes of\nfollowing the advice. This biasing may restrict the actions available to the agent when in a particular\nstate (Thomaz and Breazeal, 2006). Following this taxonomy, our work falls under the category of\nguidance, and specifically, policy shaping, as we aim to bias the agent’s exploration by infusing the\npolicy with human opinion by way of subjective logic. We are confident, however, with alteration,\nthat our method may be applicable to other advisement mechanisms.\nOther methods not explicitly mentioned in the survey include demonstration or imitation based\nlearning (Celemin et al, 2022; Pertsch et al, 2022; Wu et al, 2023). Such training protocols have\nlimitations, however, as the human must know how to properly demonstrate the task, and may\nbecome the bottleneck in learning leading to increased time to train the agent. Our work facilitates\nquicker human-machine interaction as the agent does not require continuous human interaction\nthrough the process in order to learn. As an alternative, Alshiekh et al (2018) propose the use of\nshielding, in which a shield is placed pre-exploration, listing safe actions for the agent to take if it\ntries to traverse an unsafe region. The shield may instead be placed during exploration if the agent\ndecides to take an unsafe action. Through the use of subjective logic, our work instead provides more\nsemantically rich information to the agent rather than a list of alternative actions.\n4 Approach to guiding reinforcement learning agents by\nopinions\nIn this section, we present our approach to opinion-guided reinforcement learning, with an emphasis\non modeling belief uncertainty for guiding purposes. We argue that subjective logic is an appropriate\nformalism to encode opinions, and therefore, opinions should be captured in terms of subjective logic.\nHowever, as explained in Sec. 3, subjective logic is not intuitive to human advisors, and therefore, we\nrecommend supporting the human advisor with a domain-specific language (DSL) (Schmidt, 2006)\nto express their opinions. Calibrating the uncertainty parameter u of subjective logic is particularly\nchallenging.\nThus, as shown in Fig. 3, in our approach, we construct opinions in two steps and use them as\nthe guiding advice in a subsequent step. First, ( x\nh\n1 ) the advisor (or multiple advisors) provide(s) their\nadvice through a suitable DSL, without the notion of uncertainty. Subsequently ( x\nh\n2 ), the advice is\ntranslated to an opinion by factoring in the level of uncertainty u and base rate a of the opinion.\nUncertainty is calibrated by a suitable external function, and the base rate is obtained from the struc-\nture of the reinforcement learning problem at hand. Opinions are formulated in terms of subjective\nlogic (certainty domain). To shape the agent’s policy with the opinion, the policy is translated to sub-\njective logic as well, and ( x\nh\n3 ) the advisor’s opinion is fused into it within the mathematically sound\nframework of subjective logic. Subjective logic provides an appropriate framework for our approach\nfor several reasons. First, probabilities can be expressed as opinions without loss of information. As\nwell, subjective logic defines several fusion operators that merge information. Finally, subjective logic\n7\ncaptures uncertainty inherent to human advice. Thus, raising probabilities to the domain of subjec-\ntive logic allows for the consolidation of the agent’s policy and the advisor’s knowledge, leading to a\nmore informed and realistic measure of the ideal policy, while incorporating uncertainty. Eventually\n( x\nh\n4 ), the fused opinion is translated back to the probability domain as the shaped policy.\nProblem domain\nProbability domain\nCertainty domain\nPolicy π\nAgent\nAdvice α\nAdvisor(s)\n1\n+\nOpinion ω\n3.3\n4\nTransformation\nInput\na, u\n+\n2.2\n2.1\n3.1\n3.2\nLegend\nLevel of abstraction\nd, b\nFig. 3: Overview of the approach\nWe now elaborate on each of these steps in detail, using the running example of Sec. 2 to illustrate\nconcepts. (Subsections 4.1–4.4 correspond to steps x\nh\n1 – x\nh\n4 in Fig. 3.)\n4.1 Providing advice\nIn the first step of the approach ( x\nh\n1 ), the advisor provides their advice about the benefits of the\nagent being in a specific state, as perceived by the advisor. Advice is a subjective construct of the\nadvisor and expresses beliefs about a statement, e.g., the reward that might be received for occupying\na particular state. Advice can be expressed at any time, given sufficient support for interaction. In\nthis work, we assume that advice is provided before the exploration begins; however, our approach\ncan be extended to interactive RL scenarios too (Celemin et al, 2022).\nIn our approach an advice α is given by a mapping α ∶s ∈S′ ↦v, where S′ ⊆S is the known\nsubset of the total problem space, s ∈S′ is a specific state within the known problem space, and\nv is a value that expresses the benefit of the particular state in a suitable language. Since it is not\nalways feasible to survey the whole problem space, we only assume that the advisor provides a set of\nadvice w.r.t. S′ ⊆S. For example, in Sec. 2, a partially informed advisor could formulate the advice\n“Cell (1,1) [a hole] is likely not beneficial”. Here, s refers to cell [1,1] in the grid world, and “not\nbeneficial” is the value of occupying cell [1,1]. This advice is then used to update the state-action\npairs that lead to the advised cell from neighboring cells, thereby shaping the policy.\nUnfortunately, the notion of value in the advice is not quantitative enough as of yet. Thus, advice\nneeds to be formulated in a suitable domain-specific language (DSL) (Schmidt, 2006) that either\nallows for expressing quantitative advice intuitively or quantifies the advice in the background. DSLs\nare languages tailored to a specific problem and offer a restricted set of syntactic elements to express\nstatements about the problem domain. This allows for working with domain concepts, e.g., expressing\nthe value of occupying a cell in the frozen lake, rather than having to express the reward function\nin the usual terms of RL. Working with domain concepts, in turn, narrows the gap between RL and\nhuman cognition, substantially improving the usability of the approach.\nExample The GridWorld DSL for the Frozen Lake problem\nFor the purposes of this article, we define the DSL in Listing 1 to provide advice about the grid world\nof the running example (Fig. 1). An example set of advice expressed in the DSL about the running\nexample (Fig. 1) is shown in Listing 2.\n8\n1 <AdviceList> ::= <Advice>+\n2 <Advice> ::= <AdviceLocation> ‘,‘ <AdviceValue>\n3 <AdviceLocation> ::= ‘[‘[0-9]+, [0-9]+‘]‘\n4 <AdviceValue> ::= ˆ-?[0-2]\nListing 1: GridWorld grammar in EBNF notation\n1 [1,1], -2\n2 [1,3], -2\n3 [0,3], -1\n4 [3,3], +2\nListing 2: Example set of opinions for the problem in Fig. 1\nThe value of the advice represents the advisor’s idea about how beneficial the given cell can be\nfor the agent. In our example DSL, the value is an integer that ranges between -2 and 2. In the\nexample in Listing 2, the respective list of advice corresponds to the following cells of the running\nexample (Fig. 1): two holes on the grid, which are not beneficial ([1,1], -2; and [1,3], -2);\nthe goal, which is beneficial ([3,3], +2); and the frozen tile that is not as hazardous as a hole but\nstill rather disadvantageous ([0,3], -1) due to the number of holes in its vicinity.\nIt is important to note that the above DSL is just an example, and alternative DSLs for different\nproblems could be developed. However, our approach is general enough to accommodate any DSL.\nNext, we show how advice is translated into the certainty domain as an opinion.\n4.2 Translating advice to opinion\nTo treat advice by formal means, we map them onto opinions of subjective logic. As shown in Fig. 3\n( x\nh\n2 ), this requires calibrating the level of uncertainty u at which the advice was communicated\nand setting base rate a (Sec. 4.2.1); and subsequently, compiling the advice at the specific level of\nuncertainty into an opinion by calculating the remaining parameters of subjective logic: disbelief d\nand belief b (Sec. 4.2.2).\n4.2.1 Calibrating the base rate (a) and the level of uncertainty (u)\nCalibrating base rate\nBase rate a is the prior probability in the absence of belief or disbelief. In the context of RL, the\nbase rate represents the default likelihood of the agent taking a specific action in a specific state.\nThus, given a set of actions A, the base rate is given as\na =\n1\n∣A∣.\n(4)\nIn the grid world of the running example, the base rate is 0.25 in each state because the agent\ncan choose from four actions to move to the next state. (If the agent is situated in an edge or corner\ncell and chooses an action that would lead out of the grid world, the agent stays in place.)\nA similar approach is suggested by Zennaro and Jøsang (2020, Sec 3.1).\nCalibrating uncertainty\nAs explained in Sec. 3, there are two classes of methods to calibrate uncertainty u: manual (deferred to\nthe advisor) and automated (calibrated by an appropriate metric). Since human advisors might find\nit challenging to quantify the uncertainty of their advice, in our approach, we recommend automated\nderivation of the uncertainty metric.\nUncertainty can be derived, for example, from an appropriate distance metric, which assumes\nthat distance discounts certainty, i.e., uncertainty increases with distance. Distance can be a pure\ntopological concept (e.g., a Manhattan distance in a grid world); or a more abstract concept, such as\na trace distance between design models (Dagenais and David, 2024; David et al, 2016) or a quantified\nnotion of subject matter expertise.\n9\nThere are two components to calibrating uncertainty by a distance measure: (i) choosing the spe-\ncific distance measure, and (ii) choosing the discount function that progressively discounts certainty\nas distance increases.\nIn general the discount function is given as γ ∶(umax, δ) ↦u ∈(0, 1), where δ is a distance\nmeasured by a distance measure ∆, and umax is the upper bound of uncertainty. From the identity\nof u = 1 −(b + d) (Sec. 3.2), it follows that umax = 1 −(b + d).\nIn the most simple case, a liner discount function can be chosen to calibrate uncertainty as follows.\nu =\nδ\nδmax × umax\n(5)\nHere,\nδ\nδmax is the distance relative to the maximum distance, that maps onto the (0,1) domain.\nIn some problems, uncertainty should not be scaled over the complete problem space, but to a\nsubset of it. For such cases, a threshold 0 < τ ∈∆< 1 is used to accelerate the discounting of\ncertainty and reach umax in τ. Generally, Equation 5 is adopted as follows.\nu = {\n1\nτ ×\nδ\nδmax × umax\nif δ ≤(τ × δmax);\numax\nif δ > (τ × δmax).\n(6)\n4.2.2 Compiling advice into opinion by computing disbelief (d) and belief (b)\nOnce uncertainty u has been calibrated, disbelief di and belief bi of opinion ωi, for advice αi can be\ncomputed. Given 0 ≤u ≤1, the remainder of opinion weights has to be distributed between bi and di\nin accordance with Equation 1. The calculation method of belief bi and disbelief di for the jth item\nin the n-point scale used for expressing advice values, in ascending order of confidence from least to\nmost confident, are given as follows.\nbi = j −1\nn −1 × (1 −u) ∣j ∈{1..n}\n(7)\ndi = (1 −u) −bi.\n(8)\nThe corresponding algorithm is shown in Algorithm 1. Here, j−1\nn−1 is the weight between b and d to\nsplit the remaining opinion weight of 1 −u.\nEventually, using Equations 4–8, the opinion is given as\nωi = ( j −1\nn −1 × (1 −u), n −j\nn −1 × (1 −u), u, 1\n∣A∣) .\n(Here, we also used that (8)←(7) reduces to n−j\nn−1 × (1 −u).)\nAlgorithm 1 Compiling advice into opinion\nRequire: Advice α\nRequire: a\n▷inferred base rate\nRequire: u\n▷automatically calibrated uncertainty\nRequire: n\n▷advice scale length\nOpinion ω\nω.a ←a\nω.u ←u\nω.b ←(order(α.value) −1)/(n −1) × (1 −ω.u)\nω.d ←1 −(ω.b + ω.u)\nreturn ω\nThree example mappings are shown in Tab. 2. For example, advice value +1 is the element number\n4 in the advice scale. At uncertainty level u = 0.2, the remaining opinion weight is 1 −0.2 = 0.8;\nwhich is distributed between b and d in a j−1\nn−1 = 3\n4 weight towards b. That is, b4 = 0.8 × 3\n4 = 0.6; and\nconsequently, b4 = 0.2.\n10\nTable 2: Example mappings of confidence levels onto the belief dimension of SL at\ndifferent degrees of uncertainty u\nAdvice\nvalue\nOrder j\nu=0.0\nu=0.2\nu=0.5\nu=0.833\nb\nd\nb\nd\nb\nd\nb\nd\n-2\n1\n0.00\n1.00\n0.0\n0.8\n0.000\n0.500\n0.000\n0.167\n-1\n2\n0.25\n0.75\n0.2\n0.6\n0.125\n0.375\n0.042\n0.125\n0\n3\n0.50\n0.50\n0.4\n0.4\n0.250\n0.250\n0.084\n0.084\n+1\n4\n0.75\n0.25\n0.6\n0.2\n0.375\n0.125\n0.125\n0.043\n+2\n5\n1.00\n0.00\n0.8\n0.0\n0.500\n0.000\n0.167\n0.000\nExample From advice to opinions in the Frozen Lake problem\nGiven an advisor situated in the bottom-left corner of the field (Fig. 4), we model the uncertainty\nof a piece of advice by the distance between the advisor and the cell the opinion pertains to. For\nsimplicity, we use a normalized Manhattan distance and consider the bottom-left cell at u(3,0) = 0.0\nand the top-right corner at u(0,3) = 1.0 uncertainty; and consider a linear relationship between\nuncertainty and distance. The Manhattan distance is a common distance measure in topological\nspaces, such as the running example illustrated in Sec. 2. The Manhattan distance D between points\nX = (x1, x2, ⋯, xn) and Y = (y1, y2, ⋯, yn) is defined as\nD(X, Y ) =\nn\n∑\ni=1\n∣xi −yi∣.\nHere, by the Manhattan distance and τ = 0: δmax = 6 (between the two corners), and corre-\nsponding to Equation 5, u = δ\n6 × 1.0. Indeed, each unit of distance from the advisor increases the\nuncertainty by 1/6 = 0.166. This progression constitutes the linear γ discount function of certainty.\n0.000\n0.166\n0.333\n0.500\n0.166\n0.333\n0.500\n0.666\n0.333\n0.500\n0.666\n0.833\n0.500\n0.666\n0.833\n1.000\nΔ~u\nGrowing\nuncertainty\nFig. 4: A visual intuition of the advisor’s limited knowledge, subject to epistemic uncertainty (left),\nand the corresponding uncertainty levels of the cells in the grid world. Uncertainty grows with\ndistance—in this specific example, with topological distance. Using the distance between the Advisor\nand the location the advice pertains to, the uncertainty of advice can be calibrated.\nTable 3: The four pieces of advice and their opinion equivalents of the running example\nCell\nAdvice\nSL\nu\nb\nd\na\n[1,1]\n-2\n0.500\n0.000\n0.500\n0.250\n[1,3]\n-2\n0.833\n0.000\n0.167\n0.250\n[0,3]\n-1\n1.000\n0.000\n0.000\n0.250\n[3,3]\n+2\n0.500\n0.500\n0.000\n0.250\n11\n4.3 Policy shaping by opinion fusion\nAfter having formed opinions from advice, the advisor’s input is ready to be used to shape the agent’s\npolicy. As shown in Fig. 3 ( x\nh\n3 ), this requires accessing the agent’s policy (Sec. 4.3.1), transforming\nit from the probability domain to the certainty domain (Sec. 4.3.2), and within the framework of\nsubjective logic, fusing it with the advisor’s opinion (Sec. 4.3.3).\n4.3.1 Policy\nThe product of the agent’s exploration process in RL is the policy π that maps states to actions.\nFormally, policy π is given by a mapping π = S × A →P, where S is the set of states accessible\nto the agent; A is the set of actions the agent can take, and P is a probability in the usual sense,\ni.e., P = [0, 1]. Policy π(a∣s) defines the conditional probability of choosing action a ∈A in state\ns ∈S. It follows that ∀s ∈S, a ∈A ∶0 ≤π(a∣s) ≤1 always holds. Furthermore, the default policy is\ninitialized as ∀s ∈S, ai, aj ∈A, i ≠j ∶π(ai∣s) = π(aj∣s). That is, before exploration, the probability\nof choosing any action in a given state is equal.\nExample Default policy in the Frozen Lake problem\nTab. 4 shows the default policy of the running example. As shown, in each state (here, each cell),\nthere are four actions available for the agent (left, down, right, up). Before training, the default policy\nis uniform. Each action is equally as likely to be taken in each state. Here, this probability is 0.25\nas there are 4 possible actions to take in each state. Through trial and error, the agent will modify\nthese probabilities.\nTable 4: Default policy of the Frozen Lake running example\nLeft\nDown\nRight\nUp\n[0,0]\n0.25\n0.25\n0.25\n0.25\n[0,1]\n0.25\n0.25\n0.25\n0.25\n...\n[3,2]\n0.25\n0.25\n0.25\n0.25\n[3,3]\n0.25\n0.25\n0.25\n0.25\n4.3.2 Transforming the policy into the certainty domain\nTo be able to shape the policy, we transform it from the probability domain to the certainty domain.\nThat is, the probabilities of the policy (e.g., in Tab. 4) are translated to opinions through the\nfollowing mapping:\nfP →Ω(π) ∶S × A →Ω∣P.\nThat is, mapping f(π) assigns an opinion ω ∈Ωof choosing action a ∈A in state s ∈S, given\nthe probability p ∈P of choosing action a ∈A in state s ∈S under policy π. As explained in Sec. 3.2,\nprobability p is translated to an opinion ω as follows:\nω ∶p ↦(p, 1 −p, 0, p).\n(9)\nThis definition also shows that base rate a (i.e., the last element of the ω tuple) is equal to the\ndefault probability of choosing an action, aligned with Equation 4.\nThe corresponding algorithm is shown in Algorithm 2. For clarity, we refer to a policy expressed\nin the probability domain as πp, and to a policy expressed in the certainty domain as πc.\nExample Default policy of the Frozen Lake problem in the certainty domain\nTab. 5 shows the default policy in the certainty domain transformed from the default policy in the\nprobability domain shown in Tab. 4.\nAs shown, the previous probabilities of p = 0.25 are now expressed as opinions, following\nEquation 9 as ω = (0.25, 1 −0.25, 0, 0.25) = (0.25, 0.75, 0, 0.25).\n12\nAlgorithm 2 Transforming a policy in the probability domain (πp) to a policy in the certainty\ndomain (πc)\nRequire: πp\n▷Policy in the probability domain\nπc\n▷Policy in the certainty domain\nfor Each state s ∈S do\nfor Each action a ∈A do\np ←πp[s, a]\nπc[s, a] ←(p, 1 −p, 0, p)\nend for\nend for\nreturn πc\nTable 5: Default policy of the Frozen Lake running example in the certainty domain\nLeft\nDown\nRight\nUp\n[0,0]\n(0.25, 0.75, 0.0, 0.25)\n(0.25, 0.75, 0.0, 0.25)\n(0.25, 0.75, 0.0, 0.25)\n(0.25, 0.75, 0.0, 0.25)\n[0,1]\n(0.25, 0.75, 0.0, 0.25)\n(0.25, 0.75, 0.0, 0.25)\n(0.25, 0.75, 0.0, 0.25)\n(0.25, 0.75, 0.0, 0.25)\n...\n[3,2]\n(0.25, 0.75, 0.0, 0.25)\n(0.25, 0.75, 0.0, 0.25)\n(0.25, 0.75, 0.0, 0.25)\n(0.25, 0.75, 0.0, 0.25)\n[3,3]\n(0.25, 0.75, 0.0, 0.25)\n(0.25, 0.75, 0.0, 0.25)\n(0.25, 0.75, 0.0, 0.25)\n(0.25, 0.75, 0.0, 0.25)\n4.3.3 Policy shaping by opinion fusion\nAfter the policy has been transformed to an opinion-based representation, the advisors’ opinions can\nbe combined with it. Combined (joined) opinions are obtained through fusing single opinions. By\nfusion, we mean a mapping Ω× Ω→Ωthat produces an opinion from other opinions.\nLocating the opinions to be fused\nIn our approach, the advisor provides advice about the value of occupying a specific state. This\ninformation affects the agent’s decision in states from which the advised state is reachable. To\nunambiguously formalize this step, we need some definitions.\nDefinition 1 (Neighboring states). States si, sj ∈S are said to be neighbors if there exists a state-\naction pair in the policy for which π(a∣si) ↦sj, i.e., choosing action a ∈A in state si will transition\nthe agent to state sj.\nThe neighborhood N of a state is the set of all its neighbors.\nDefinition 2 (Neighborhood (of a state)). N(si ∈S) = ⋃sj∈S ∃a ∈A ∶π(a∣si) ↦sj.\nIn the running example, the state space is topological; thus, trivially, neighboring states are the\nneighboring cells of a given cell.\nIntroducing advice α about state si to policy π is achieved by ∀a ∈N(si).A ∶ω(α)⊙ω(a). That\nis, the opinion formed from the advice (ω(α)) is fused with every opinion formed from actions (ω(a))\nthat lead from its neighboring states (N(si)) to si.\nChoosing a fusion operator\nThe seminal work of Jøsang (2016) defines an array of fusion operators and classifies them according\nto situational characteristics to facilitate the selection of the most appropriate operator. Here, we\nrely on the Belief Constraint Fusion (BCF) operator, which is an appropriate choice when agents\nand advisors have already made up their minds and will not seek compromise. Since the agent and\nadvisors formulate opinions about the problem independently from each other, it is fair to assume\nthat they will, indeed, not seek compromise.\nAs per Jøsang (2016), fused opinion ω⊙= (b⊙, d⊙, u⊙, a⊙) under Belief Constraint Fusion is\ncalculated from the overlapping beliefs (called harmony) and non-overlapping beliefs (conflict) of\nindividual opinions.\nHarmony = b1u2 + b2u1 + b1b2;\n(10)\nConflict = b1d2 + b2d1.\n(11)\n13\nFinally, the parameters of the fused opinion ω⊙are calculated as follows.\nb⊙=\nHarmony\n(1 −Conflict);\n(12)\nd⊙= 1 −(b⊙+ u⊙);\n(13)\nu⊙=\nu1u2\n(1 −Conflict);\n(14)\na⊙= a1(1 −u1) + a2(1 −u2)\n2 −u1 −u2\n(15)\nOf course, different fusion operators can be chosen as well. A detailed account of fusion operators\nis given by Jøsang (2016).\nThe case of multiple advisors\nThere is no limitation to the number of advisors in our approach. This is due to opinions forming a\nclosed structure under fusion in subjective logic, i.e., fusing two opinions results in another opinion. By\nthat and the commutative nature of fusion operators, arbitrary number of advisors can be involved.\nBy involving multiple advisors, the expectation is that the performance of guidance will increase.\nThis is due to the fact that advisors might have complementary knowledge, i.e., they might be able\nto provide high-certainty advice about different parts of the problem space.\nExample Shaped policy of the Frozen Lake problem in the certainty domain\nTab. 6 shows the impact of a single advice [1,1]→-2 on the default policy shown in Tab. 5. As\ndefined in Theorem 4.3.3, shaping the policy impacts the neighbors of the advised state.\nLocating opinions to be fused.\nFrom Definition 2, N[1, 1] = {[0, 1], [1, 0], [1, 2], [2, 1]}, with the\nrespective actions of {Down, Right, Left, Up} leading to [1,1]. The corresponding opinions will be\nfused with the opinion formed from the advice.\nFusing opinions.\nAs shown in Tab. 3, advice [1,1]→-2 translates to ω(α) = (0.00, 0.50, 0.50, 0.25).\nIn the default policy, each opinion is given by ω(a) = (0.25, 0.75, 0.00, 0.25). The fused opinion under\nBelief Constraint Fusion is calculated as follows.\nHarmony = b1u2 + b2u1 + b1b2 = 0.00 × 0.00 + 0.25 × 0.50 = 0.125.\nConflict = b1d2 + b2d1 = 0.00 × 0.75 + 0.25 × 0.50 = 0.125.\nb⊙=\nHarmony\n(1 −Conflict) =\n0.125\n1 −0.125 = 0.143.\nu⊙=\nu1u2\n(1 −Conflict) = 0.50 × 0.00\n1 −0.125 = 0.\nd⊙= 1 −(b⊙+ u⊙) = 1 −(0.143 + 0) = 0.857.\na⊙= a1(1 −u1) + a2(1 −u2)\n2 −u1 −u2\n= 0.25 × (1 −0.50) + 0.25 × (1 −0.00)\n2 −0.50 −0.00\n= 0.25.\nThus, the resulting fused opinion ω⊙= (0.143, 0.857, 0.000, 0.250), as highlighted in Tab. 6.\nTable 6: Advice [1,1]→-2 fused into the policy of the Frozen Lake running example\nLeft\nDown\nRight\nUp\n[0,0]\n(0.25, 0.75, 0.0, 0.25)\n(0.25, 0.75, 0.0, 0.25)\n(0.25, 0.75, 0.0, 0.25)\n(0.25, 0.75, 0.0, 0.25)\n[0,1]\n(0.25, 0.75, 0.0, 0.25)\n(0.143, 0.857, 0.0, 0.25)\n(0.25, 0.75, 0.0, 0.25)\n(0.25, 0.75, 0.0, 0.25)\n...\n[1,0]\n(0.25, 0.75, 0.0, 0.25)\n(0.25, 0.75, 0.0, 0.25)\n(0.143, 0.857, 0.0, 0.25)\n(0.25, 0.75, 0.0, 0.25)\n...\n[1,2] (0.143, 0.857, 0.0, 0.25)\n(0.25, 0.75, 0.0, 0.25)\n(0.25, 0.75, 0.0, 0.25)\n(0.25, 0.75, 0.0, 0.25)\n...\n[2,1]\n(0.25, 0.75, 0.0, 0.25)\n(0.25, 0.75, 0.0, 0.25)\n(0.25, 0.75, 0.0, 0.25)\n(0.143, 0.857, 0.0, 0.25)\n...\n[3,3]\n(0.25, 0.75, 0.0, 0.25)\n(0.25, 0.75, 0.0, 0.25)\n(0.25, 0.75, 0.0, 0.25)\n(0.25, 0.75, 0.0, 0.25)\n14\n4.4 Transforming opinion-infused policies from the certainty domain to\nthe probability domain\nAs the final step of the approach (Fig. 3 – x\nh\n4 ), the policy with the opinions fused into it, is transformed\nback from the certainty domain to the probability domain to be used by the agent. This is achieved\nin two steps: translating opinions to probabilities and, subsequently, normalizing probabilities in each\nstate.\nTransformation to the probability domain\nThe transformation to the probability domain is the inverse transformation of what has been\nexplained in Sec. 4.3.2. As explained in Sec. 3.2 and defined by Jøsang (2016), opinion ω is translated\nto a probability p as follows:\np ∶ω ↦b + au.\n(16)\nNormalization\nSince the actions available for the agent in a given state form a complete probability space, the\nfollowing invariant must always hold in a valid policy π.\n∀s ∈S ∶∑\na∈A\np(a∣s) = 1.\n(17)\nThus, we apply the usual normalization to scale the sum of probabilities to 1 as follows.\n∀s ∈S, a ∈A ∶p(a∣s) ∶= p(a∣s) ×\n1\n∑a∈A p(a∣s).\n(18)\nExample Shaped policy of the Frozen Lake problem in the probability domain\nCalculating probabilities.\nWe take the fusion of the advice and the policy in Tab. 6 and for\neach tuple, we apply Equation 16. Tab. 7 shows the result of this transformation and the new\npolicy that considers advice [1,1]→-2. Consider, e.g., the Down action in state [0,1]. Here,\nω = (0.143, 0.857, 0.0, 0.25)), and from Equation 16, p = b + au = 0.143 + 0.25 × 0.0 = 0.143 follows.\nAs seen by the highlighted values, the advice affects the actions of neighboring states that lead\nto state [1,1], i.e., [0,1] – Down, [1,0] – Right, [1,2] – Left, [2,1] – Up. Tab. 7a and\nTab. 7b show the policy before and after normalization, respectively.\nNormalization.\nThe resulting probabilities in row [0,1] violate invariant Equation 17 as\n∑a∈A p(a∣s = [0, 1]) = 0.893. Thus, we normalize by Equation 18 and multiply each probability by\n1\n∑a∈A p(a∣s=[0,1]), resulting in the probability vector (0.28, 0.16, 0.28, 0.28), as shown in the [0,1]\nrow of Tab. 7b.\nFull example.\nTo round out the running example, we show the full policy after considering every\npiece of advice in Listing 2. Tab. 8 shows the the new probabilities before (Tab. 8a) and after\n(Tab. 8b) normalization. As seen, some advice does not change the probabilities of the policy much,\ne.g., in the [0,3] – Down row of Tab. 8a (0.217, previously 0.250); while other pieces of advice have\nsubstantial impact, e.g., in [3,2] – Right (0348, previously 0.250). This is due to the uncertainty\nof the advice we modeled by the distance between the advisor and the advised state. In the running\nexample, the advisor was situated in the bottom-left corner, and therefore, as a consequence, their\nadvice about the distant [1,3] cell impacted the Down action of cell [0,3] less than the advice\nabout the closer [3,3] cell that impacted the Right action of cell [3,2].\n15\nTable 7: Shaped policy of the Frozen Lake running example in the probability domain after advice\n[1,1]→-2 affecting the actions of neighboring states [0,1], [1,0], [1,2], [2,1] that lead to\nstate [1,1]\n(a) Before normalization\nLeft\nDown\nRight\nUp\n[0,0]\n0.25\n0.25\n0.25\n0.25\n[0,1]\n0.25\n0.143\n0.25\n0.25\n...\n[1,0]\n0.25\n0.25\n0.143\n0.25\n...\n[1,2]\n0.143\n0.25\n0.25\n0.25\n...\n[2,1]\n0.25\n0.25\n0.25\n0.143\n...\n[3,3]\n0.25\n0.25\n0.25\n0.25\n(b) After normalization\nLeft\nDown\nRight\nUp\n[0,0]\n0.25\n0.25\n0.25\n0.25\n[0,1]\n0.28↑\n0.16↓\n0.28↑\n0.28↑\n...\n[1,0]\n0.28↑\n0.28↑\n0.16↓\n0.28↑\n...\n[1,2]\n0.16↓\n0.28↑\n0.28↑\n0.28↑\n...\n[2,1]\n0.28↑\n0.28↑\n0.28↑\n0.16↓\n...\n[3,3]\n0.25\n0.25\n0.25\n0.25\nTable 8: Shaped policy of the Frozen Lake running example\n(a) Before normalization\nLeft\nDown\nRight\nUp\n[0,0]\n0.25\n0.25\n0.25\n0.25\n[0,1]\n0.25\n0.143\n0.25\n0.25\n[0,2]\n0.25\n0.25\n0.250\n0.25\n[0,3]\n0.25\n0.217\n0.25\n0.25\n...\n[1,0]\n0.25\n0.25\n0.143\n0.25\n...\n[1,2]\n0.143\n0.25\n0.217\n0.25\n[1,3]\n0.25\n0.250\n0.25\n0.25\n...\n[2,1]\n0.25\n0.25\n0.25\n0.143\n...\n[2,3]\n0.25\n0.400\n0.25\n0.217\n...\n[3,2]\n0.25\n0.25\n0.400\n0.25\n[3,3]\n0.25\n0.25\n0.25\n0.25\n(b) After normalization\nLeft\nDown\nRight\nUp\n[0,0]\n0.25\n0.25\n0.25\n0.25\n[0,1]\n0.28↑\n0.16↓\n0.28↑\n0.28↑\n[0,2]\n0.25\n0.25\n0.25\n0.25\n[0,3] 0.259↑0.223↓0.259↑0.259↑\n...\n[1,0]\n0.28↑\n0.28↑\n0.16↓\n0.28↑\n...\n[1,2] 0.166↓\n0.29↑\n0.252↑\n0.29↑\n[1,3]\n0.25\n0.25\n0.25\n0.25\n...\n[2,1]\n0.28↑\n0.28↑\n0.28↑\n0.16↓\n...\n[2,3] 0.224↓0.358↑0.224↓0.194↓\n...\n[3,2] 0.217↓0.217↓0.348↑0.217↓\n[3,3]\n0.25\n0.25\n0.25\n0.25\nFig. 5: The result of policy shaping in the running example with the major changes highlighted.\nRed: decreased probability; green: increased probability.\n16\nVisualization of results.\nThe final policy in Tab. 8b is visualized in Fig. 5, with the major changes\nhighlighted. As seen, the negative advice about the hole in [1,1] resulted in a strong decrease in the\ncorresponding probabilities of actions from neighboring cells, while the negative advice about the hole\nin [1,3] resulted in slightly smaller decrease due to higher uncertainty of the advice. Conversely, the\npositive advice about the goal in [3,3] coupled with high certainty resulted in substantial positive\nbias that will guide the agent around the hole to the goal. Note that this policy is only the input to\nthe agent’s learning phase, and the preferences in each state have yet to be reinforced. For example,\nin state [0,2], there is no change based on the advice, but surely, the agent will learn not to move\ndown toward a hole but rather, to the right.\nReflection.\nThe agent’s policy is affected by the advice. Positive advice by opinions with belief\nbeing greater than base rate (b > a) causes the shaped policy to bias towards more belief to take\nactions that lead to the state that has been advised as a beneficial one. This bias manifests in an\nincreased probability of the agent taking corresponding actions that lead to the advised state from its\nneighboring states. Such cases can be observed around the goal in the running example. In contrast,\nnegative advice (b < a) shapes the policy in a way that it drives the agent way from advised states by\ndecreasing the probability of the agent taking corresponding actions that to the advised state from\nits neighboring states. Such cases can be observed around the holes in the running example. The\nlevel of uncertainty has a profound impact on the shaped policy, as demonstrated by the difference\nbetween the weight of negative advice about state [1,1] and [1,3].\nConclusion of RQ1\nIn Sec. 1, we posed our first research question: How can one use opinions to guide reinforcement\nlearning agents? In this section, we presented a method for guiding RL agents through opinions.\nRQ1\nOur method shows that through a combination of subjective logic, appropriate domain-specific\nlanguages, and a suitable distance function, opinions can be introduced into reinforcement learn-\ning as the means of guidance. The sound mathematical framework of subjective logic allows\nfor multiple sources of opinions to be uniformly fuse with the agent’s original policy, effectively\nachieving policy shaping through opinions. Decoupling the calibration of uncertainty allows for\nmore streamlined and intuitive advising by human experts.\n5 Evaluation\nIn this section, we empirically evaluate our approach and investigate the effects of opinion-based\nhuman advice on the performance of reinforcement learning agents.\n5.1 Study design\nWe conduct the study in a series of experiments on a sufficiently scaled-up version of the Frozen\nLake running example (Sec. 2). To find the most appropriate configuration of the running example\nthat is challenging enough but still feasible for the unadvised agent, we first manually experiment\nwith our framework (Sec. 5.1.1), and based on our experiences we finalize the scenario (Sec. 5.1.2)\nand the setup (Sec. 5.1.3).\n5.1.1 Manual experimentation\nBefore we run extensive, long-running experiments, we first manually investigate the effect of map\nconfiguration on the minimum number of episodes required for the vanilla RL agent to learn appro-\npriately. In general, the larger the map, the harder it is to explore it, and therefore, more episodes\nare required for the agent to start functioning appropriately. The placement of termination points\n(holes) is another factor that might increase the minimum number of required episodes.\nTo experiment with various map configurations in a reproducible way, we developed a map gen-\nerator that is able to generate maps of arbitrary size and hole density and disperse holes on the\nmap randomly, controlled by a user-defined seed. The map generator is available in the replication\npackage.\nEventually, we choose a 12×12 map with 20% hole density (which is the default density in OpenAI\nGymnasium) under seed 63. The final map is shown in Appendix A. On this map, we observe that\n17\nthe behavior of the unadvised RL agent is appropriate after 5 000 episodes. Thus, we will choose a\nsufficiently higher number of episodes in our experiments, as explained below.\n5.1.2 Scenario\nThe scenario is a scaled-up version of the running example. We use the frozen lake environment, as\ndescribed in detail in Sec. 2. The size of the lake in our experiments is 12×12 with 20% hole ratio\n(which is the default value in Gym’s Frozen Lake environment2). We also ensure that the goal is\nreachable from the start in the map.\nMap generation\nTo generate such a large map, we developed a map generator in Python that allows for generating\nmaps of arbitrary size and hole ratio. Holes are distributed randomly on the lake. For replicability, the\ngenerator can be parameterized with a seed. The generator outputs an Excel file for human inspection\nand the standard graphical rendering of the generated environment. The Excel file is then used as\ninput for each experiment execution. The map generator is available in the replication package.\n5.1.3 Experimental setup\nEach experiment is executed using the same setup as discussed below.\nParameters\nThe list of parameters is shown in Tab. 9.\nTable 9: Parameters of the experiments\nParameter\nValue\nFixed parameters\nRL method\nDiscrete policy gradient\nNumber of episodes\n10 000\nLearning rate\n0.9\nDiscount factor\n1.0\nAdvice strategy\nOnce at the beginning of the experiment\nFusion operator\nBCF\nGrid size\n12\nVariables\nAgent type\n{Random, Unadvised, Advised}\nSource of advice\n{Oracle, Single human,\nCooperating humans}\nAdvice quota – oracle\n{100% (“All”), 20% (“Holes&Goal”)}\nAdvice quota – single human\n{10%, 5%}\nAdvice quota – coop. human\n{10% each, 5% each}\nUncertainty (oracle and single human)\n{0.2k ∣k ∈0..4}\nUncertainty (cooperative humans)\nCalculated dynamically from a distance metric\nCooperative advice type\n{Sequential cooperation, parallel cooperation}\nFixed parameters.\nIn this work, we choose discrete policy gradient as the reinforcement learning\nmethod. Policy-based methods represent the policy explicitly, which allows for directly investigating\nthe effects of advice on the policy. To mitigate threats to validity, we use meaningful hyper parameters\nof the RL algorithm in order to allow the unadvised agent to perform as well as possible. Specifically,\nwe set the learning rate to 0.9, and the discount factor to 1.0.\nEach experiment runs for a number of episodes, defined by the number of episodes parameter.\nAn episode starts with a reset environment and the agent in the initial position. Within the same\nexperiment, the policy is updated and not reset. An episode lasts until the agent either finds the goal\nor steps into a hole. The higher the maximum number of episodes, the more opportunities agents\nhave to find the goal and reinforce a beneficial policy. After we observed in our manual experiments\nthat the behavior of the unadvised RL agents is appropriate after 5 000 episodes, we decided to run\nour experiments for a comfortable 10 000 episodes.\nThe advice strategy stipulates advice to be provided once and that it is done at the beginning\nof the experiment. Alternatively, more interactive advice strategies could be explored, but this is\noutside the scope of the current study.\n2https://gymnasium.farama.org/environments/toy text/frozen lake/#arguments\n18\nThe fusion operator is also fixed. We used the Belief Constraint Fusion (BCF) operator for\nsimplicity. BCF is an appropriate choice when agents have already made up their minds and will not\nseek compromise (Jøsang, 2016). Since the source of the advice formulates their opinions about the\nmap independently from the RL agent, it is fair to assume that compromise will not be sought.\nFinally, we fix the problem size to a 12 × 12, as explained in Sec. 5.1.1.\nVariables.\nWe execute experiments for each agent type to compare their performance. The random\nagent randomly samples from the available actions. Unadvised agents maintain a policy. Advised\nagents maintain a policy, and before their first training episode, advice is fused into their initial\npolicy. In each case, the initial policy is the one in which the probability of choosing an action is\nuniformly distributed.\nWe conduct experiments with different sources of advice. To form a ground truth, we first\nexperiment with an idealized oracle with full information about the problem space. The oracle pro-\nvides synthetic advice based on pre-defined rules as follows. If the cell is a goal, its value is +2. If\nthe cell if a hole, the value is -2. If the cell has no neighboring holes, its value is +1. If the cell has\none neighboring hole, its value is 0. If the cell has more than one neighboring hole, its value is -1. In\nthe single human mode, a human advisor provides advice, and we modulate the uncertainty of their\nadvice, similar to the oracle’s advice. In the cooperative mode, advice is provided by two human advi-\nsors who have partial and complementary information about the problem. Thus, this experimental\nmode is highly realistic. The human advisor labels hole and goal cells similarly to the oracle, but for\nother cells, may use values ±1, 0 differently than the oracle.\nWe use different advice quotas for different advisor types. We evaluate the oracle by two advice\nquotas: first, by providing the agent with advice about all the cells, i.e., 100% quota; second, by\nproviding the agent with advice about the holes and the goal on the map, which amounts to about\n20% of cells (see Sec. 5.1.2). We evaluate the single human advisor by two advice quotas: first, by\nproviding the agent with advice about 10% of the cells; second, by providing the agent with advice\nabout 5% of the cells. We evaluate the cooperating human advisors by two human advice quotas:\nfirst, by allowing each advisor 10% advice quota (thus, 20% in total); second, by allowing each\nadvisor 5% advice quota (thus, 10% in total).\nWe synthetically modulate the degree of uncertainty of the advice of the oracle and the single\nhuman by sweeping through the [0.0, 1.0) interval in 0.2 increments. Here, u = 0.0 reduces to classic\nprobability, and u = 1 represents a uniform distribution (Jøsang, 2016). This choice allows us to\ndirectly compare the oracle with the single human advisor at different levels of uncertainty.\nIn the cooperative mode, uncertainty is calculated from the distance between the advisor and\nthe location of the advice, in accordance with Sec. 4.2.1. We use the two dimensional Manhattan\ndistance, defined as follows.\nD((x1, y1), (x2, y2)) = ∣x1 −x2∣+ ∣y1 −y2∣.\nHere, (x1, y1) is the location of the advisor. We assume a linear certainty discount function with\nτ = 1, as explained in Equation 5. That is, uncertainty of an advisor in one of the corners of the grid\nreaches its maximum in the opposing corner. Thus, uncertainty at any point (x2, y2) is defined as\nfollows.\nu(x2, y2) = D((x1, y1), (x2, y2)) ⋅\n1\nD((x1, y1), (xn, yn)).\nFinally, we define two types of cooperative advice with partial information. In sequential\ncooperation, one human advises the agent in the first half of its mission, and subsequently, the other\nhuman advises the agent in the second half of its mission. To achieve this, we place the first human\nadvisor in the top-left corner of the grid (start), and the second human advisor in the bottom-\nright corner of the grid (goal). As the agent follows the main diagonal of the grid world, the first\nadvisor’s input will gradually lose its influence due to the distance explained above; and the second\nadvisor’s input will gradually gain more influence. In parallel collaboration, both humans provide\nadvice throughout the agent’s entire mission. To achieve this, we place one human advisor in the top-\nright corner of the grid, and the other human advisor in the bottom-left corner of the grid. By that,\nboth advisors are able to provide advice throughout the entire start-goal trajectory of the agent.\nHowever, due to the distance metric defined above, both advisors will focus on their half of the grid\nworld, i.e., the triangle under and above the diagonal, respectively. Clearly, this advice mode is the\n19\nmost realistic one of the three as in real settings, uncertainty pertains to individual pieces of advice,\nand cooperation among stakeholders and experts is the usual way to solve complex problems.\nPerformance metric and analysis methods\nFollowing community best practices, we use the cumulative reward as the metric of performance,\ni.e., the total reward RT = ∑T\ni=0 rt+1 collected by the agent throughout an episode. We observe the\namount of cumulative reward and its dynamics, i.e., how early and rapidly the cumulative reward\nincreases under different settings.\nExecution\nWe execute experiments using each combination of parameters defined above. We run each experiment\n30 times to achieve sufficient statistical power. The results of experiments are saved as csv files. These\ndata files are available in the replication package.\nWe run the experiments on standard office equipment with the following parameters: Apple Mac\nmini, equipped with a 3.6 GHz Quad-Core Intel Core i3 CPU, running Mac OS Sonoma 14.14.1 with\n8 GB 2667 MHz DDR4 of memory, using Python 3.11.5. This setup is satisfactory for our purposes\nas we are not interested in runtime performance.\n5.2 Results\nIn the following, we report the results of our experiments by reviewing the performance of guidance by\nthe oracle (Sec. 5.2.1), single human advisor (Sec. 5.2.2), and cooperative human advisors (Sec. 5.2.3).\nIn each case, we first investigate the performance in terms of the cumulative reward, and then we\nlook at the final policy.\nMain takeaways.\nTab. 10 and Fig. 6 report the cumulative rewards in different evaluation modes\nand highlights the key takeaways of our work.\nWe note that every advised agent performs better than the unadvised agent (607.267 mean\ncumulative reward), even at high levels of uncertainty.\nWe observe comparable results between the fully informed (100% quota) oracle and the 10%\nquota single human case at low levels of uncertainty. In fact, at u = 0.2, the human advice with 10%\nquota outperforms the oracle, even with 100% quota (8 607.500 vs 8 511.367).\nFinally, to compare cooperative performance with other experiments, we show the results of the\nfour realistic cooperative experiments aligned with the comparable oracle or single human exper-\niments with the same advice quota. For example, sequential cooperation with 2×10% quota is\ncomparable to the oracle with 20% quota. In this comparison, performances align best at u = 0.2,\nwhere cooperating humans with partial information perform nearly as well as the oracle with complete\ninformation about the problem space (7 551.567 vs 7 835.267). In general, sequential cooperation out-\nperforms parallel cooperation; and parallel cooperation at 5% outperforms the fully-informed single\nhuman advisor at 10%.\nTable 10: Cumulative rewards by experiment. Bold is best at the given level of certainty. Unadvised\n= 607.267. Random = 0.100. Cooperative modes aligned with the best matching uncertainty level of\nthe comparable oracle or single human mode.\nOracle\nSingle human\nCoop. – Sequential\nCoop. – Parallel\nu\n100% (All)\n20% (H&G)\n10%\n5%\n10%\n5%\n10%\n5%\n0.0\n9 386.467\n9 443.733\n8 907.733\n7 576.067\n0.2\n8 511.367\n7 835.267\n8 607.500\n4 656.633\n7 551.567\n0.4\n7 476.633\n4 218.000\n4 727.433\n1 718.300\n5 544.100\n4 924.967\n0.6\n2 751.833\n2 089.433\n2 737.600\n1 360.267\n2 867.633\n0.8\n1 454.400\n878.467\n1 907.933\n629.367\nIn the following, we elaborate on the experimental results in detail.\n20\nLinear scale\n0\n2000\n4000\n6000\n8000\n10000\nEpisode\n0\n2000\n4000\n6000\n8000\n10000\nCumulative Reward\nAdvice@u=0.0\nAdvice@u=0.2\nAdvice@u=0.4\nAdvice@u=0.6\nAdvice@u=0.8\nNo advice\nRandom\n(a) Oracle 100%\n0\n2000\n4000\n6000\n8000\n10000\nEpisode\n0\n2000\n4000\n6000\n8000\n10000\nCumulative Reward\nAdvice@u=0.0\nAdvice@u=0.2\nAdvice@u=0.4\nAdvice@u=0.6\nAdvice@u=0.8\nNo advice\nRandom\n(b) Oracle 20%\n0\n2000\n4000\n6000\n8000\n10000\nEpisode\n0\n2000\n4000\n6000\n8000\n10000\nCumulative Reward\nAdvice@u=0.0\nAdvice@u=0.2\nAdvice@u=0.4\nAdvice@u=0.6\nAdvice@u=0.8\nNo advice\nRandom\n(c) Human 10%\n0\n2000\n4000\n6000\n8000\n10000\nEpisode\n0\n2000\n4000\n6000\n8000\n10000\nCumulative Reward\nAdvice@u=0.0\nAdvice@u=0.2\nAdvice@u=0.4\nAdvice@u=0.6\nAdvice@u=0.8\nNo advice\nRandom\n(d) Human 5%\n0\n2000\n4000\n6000\n8000\n10000\nEpisode\n0\n2000\n4000\n6000\n8000\n10000\nCumulative Reward\nCoop - Sequential\nCoop - Parallel\nNo advice\nRandom\n(e) Coop 10%\n0\n2000\n4000\n6000\n8000\n10000\nEpisode\n0\n2000\n4000\n6000\n8000\n10000\nCumulative Reward\nCoop - Sequential\nCoop - Parallel\nNo advice\nRandom\n(f) Coop 5%\nLog scale\n0\n2000\n4000\n6000\n8000\n10000\nEpisode\n10\n1\n100\n101\n102\n103\n104\nCumulative Reward\nAdvice@u=0.0\nAdvice@u=0.2\nAdvice@u=0.4\nAdvice@u=0.6\nAdvice@u=0.8\nNo advice\nRandom\n(g) Oracle 100%\n0\n2000\n4000\n6000\n8000\n10000\nEpisode\n10\n1\n100\n101\n102\n103\n104\nCumulative Reward\nAdvice@u=0.0\nAdvice@u=0.2\nAdvice@u=0.4\nAdvice@u=0.6\nAdvice@u=0.8\nNo advice\nRandom\n(h) Oracle 20%\n0\n2000\n4000\n6000\n8000\n10000\nEpisode\n10\n1\n100\n101\n102\n103\n104\nCumulative Reward\nAdvice@u=0.0\nAdvice@u=0.2\nAdvice@u=0.4\nAdvice@u=0.6\nAdvice@u=0.8\nNo advice\nRandom\n(i) Human 10%\n0\n2000\n4000\n6000\n8000\n10000\nEpisode\n10\n1\n100\n101\n102\n103\n104\nCumulative Reward\nAdvice@u=0.0\nAdvice@u=0.2\nAdvice@u=0.4\nAdvice@u=0.6\nAdvice@u=0.8\nNo advice\nRandom\n(j) Human 5%\n0\n2000\n4000\n6000\n8000\n10000\nEpisode\n10\n1\n100\n101\n102\n103\n104\nCumulative Reward\nCoop - Sequential\nCoop - Parallel\nNo advice\nRandom\n(k) Coop: 10%\n0\n2000\n4000\n6000\n8000\n10000\nEpisode\n10\n1\n100\n101\n102\n103\n104\nCumulative Reward\nCoop - Sequential\nCoop - Parallel\nNo advice\nRandom\n(l) Coop 5%\nFig. 6: Performance comparison of every experiment in terms of cumulative reward\n5.2.1 Performance of guidance by an oracle\nAdvisor type.\nIn this experiment, we use an oracle, i.e., synthetic advice that assigns values to states\nbased on pre-defined rules (as explained in Sec. 5.1.3).\nAdvice quota.\nWe evaluate two advice quotas: 100% (the agent is provided with advice about all\nthe cells by an idealized oracle) and 20% (the agent is provided with advice about the terminating\nstates, i.e., the holes and goal).\nUncertainty calibration.\nWe execute each experiment at multiple characteristic levels of uncertainty,\nsweeping through the {0.0, 0.2, 0.4, 0.6, 0.8} range. The case of complete uncertainty u = 1.0 is not\ntested as these opinions carry no information that can be used by the agent.3\nCumulative reward\nFig. 7 shows the cumulative rewards of the agent with synthetic advice from the oracle under the\ntwo advice quotas.\nClear performance improvement in advised agents.\nAs the most important takeaways, we observe\nthat (i) advised agents clearly outperform the unadvised agent and that (ii) performance improves\nwith the level of certainty of the advice. This can be seen from the lines corresponding to lower uncer-\ntainty consistently being over lines corresponding to higher uncertainty. As expected, the random\nagent hardly accumulates any reward as it follows a random trajectory without learning.\nImproved performance manifests in two forms. First, the cumulative reward is higher and increases\nat a consistently higher pace in advised and certain agents, as indicated by the advised lines over\nothers in the linear and log scales, respectively. Second, cumulative reward increases earlier and\nreaches a gradual slope sooner in advised and certain agents than in the unadvised agent, indicated\nby the earlier rise of advised lines in the log scale charts.\nEven substantially uncertain advice makes a difference.\nDemonstrated in moderate (u = 0.6) and\nhigh (u = 0.8) uncertainty cases, advised agents still outperform the unadvised agent. The 100%\nadvice quota compensates for low certainty and renders the advised agent in the u = 0.8 case more\nperformant than the unadvised agent. This advantage is lost at 20% quota as shown in Fig. 7b by\nthe similarity of the 0.8 and “No advice” lines.\nLearning dynamics: more rapid learning.\nThe log scale charts further show the two main phases\nof the learning process. In the first phase, approximately between episodes 0–2 000, we see a steep\nincrease, indicating fast learning. The learning phase is substantially steeper as uncertainty decreases.\nIn the second phase, approximately after episode 2 000, the slopes become more gradual, indicating\na slowed-down collection of reward and a likely convergence to the maximum potential performance.\n3It can also be shown that in the fusion operator of choice in this work (BCF), u = 1.0 results in an idempotent\ntransformation, which follows trivially from Equations 10–15. The proof is left as an exercise for the reader.\n21\nLinear scale\n0\n2000\n4000\n6000\n8000\n10000\nEpisode\n0\n2000\n4000\n6000\n8000\n10000\nCumulative Reward\nAdvice@u=0.0\nAdvice@u=0.2\nAdvice@u=0.4\nAdvice@u=0.6\nAdvice@u=0.8\nNo advice\nRandom\n(a) Advice quota: 100% (All)\n0\n2000\n4000\n6000\n8000\n10000\nEpisode\n0\n2000\n4000\n6000\n8000\n10000\nCumulative Reward\nAdvice@u=0.0\nAdvice@u=0.2\nAdvice@u=0.4\nAdvice@u=0.6\nAdvice@u=0.8\nNo advice\nRandom\n(b) Advice quota: 20% (Holes&Goal)\nLog scale\n0\n2000\n4000\n6000\n8000\n10000\nEpisode\n10\n1\n100\n101\n102\n103\n104\nCumulative Reward\nAdvice@u=0.0\nAdvice@u=0.2\nAdvice@u=0.4\nAdvice@u=0.6\nAdvice@u=0.8\nNo advice\nRandom\n(c) Advice quota: 100% (All)\n0\n2000\n4000\n6000\n8000\n10000\nEpisode\n10\n1\n100\n101\n102\n103\n104\nCumulative Reward\nAdvice@u=0.0\nAdvice@u=0.2\nAdvice@u=0.4\nAdvice@u=0.6\nAdvice@u=0.8\nNo advice\nRandom\n(d) Advice quota: 20% (Holes&Goal)\nFig. 7: Cumulative reward with an oracle as the advisor\nPerformance differences remain consistent throughout the experiment. The effect of advice, thus, is\nshifting the exponentially growing initial phase earlier in the learning process.\nAdvice quotas: difference shows only in low-certainty cases.\nThere is not much difference between\nthe two advice quotas in high-certainty cases, as shown by the very similar lines of u = 0.0 and u = 0.2\nin Fig. 7a and Fig. 7b. In fact, as shown in Tab. 10, the oracle with 20% quota slightly outperforms\nthe oracle with 100% quota at u = 0.0. (+0.61%) The most noticeable performance deterioration is\nobserved at u = 0.4, when performance drops by 43.6% between the 100% quota and 20% quota.\n22\nEffects on the policy\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(a) No advice\nAdvice quota: 100% (ALL)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(b) u = 0.4\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(c) u = 0.0\nAdvice quota: 20% (HOLES&GOAL)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(d) u = 0.4\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(e) u = 0.0\nFig. 8: Heatmap visualization of the final learned policy with an oracle as the advisor\nFig. 8 visualizes the policy as a heatmap. The cells of each heatmap correspond to the cells of\nthe Frozen Lake problem; arrows represent the action with the highest probability; and shading\ncorresponds to the probability. Empty cells show unexplored states.\nBetter reinforced policy.\nOne of the two key improvements is indicated by the darker shades of cells\ncompared to the unadvised agent. Darker shades correspond to higher probabilities of the indicated\naction in a cell and an overall better reinforced beneficial policy. By providing advice, the agent\nnavigated through the state space more efficiently and had time (episodes) to explore crucial cells, e.g.,\nthe ones around holes and the goal. The agent with 20% advice quota reinforces some key decisions\nbetter than the agent with 100% advice quota, e.g., around the goal and around the starting point.\nMore thorough exploration.\nThe other key improvement is shown by the higher number of cells\nexplored by advised agents, as both advice quotas result in more cells being covered, compared to\nthe unadvised agent. The 20% advice quota allows to explore more states at higher levels of certainty\nthan the 100% advice quota (cf. Fig. 8e and Fig. 8c); however, this relation turns around in the\nlower-certainty case (cf. Fig. 8b and Fig. 8d).\nEffects clear even at relatively low certainty.\nAt u = 0.4, we still see demonstrated improvements\ncompared to the unadvised agent. That is, even fairly uncertain advice has a clear positive impact\non the policy.\n5.2.2 Performance of guidance by a single human advisor\nAdvisor type.\nIn this experiment, we use the advice of a single human advisor.\nAdvice quota.\nWe evaluate two advice quotas. In the first case, the human provides advice about\n10% of the cells at their discretion. In the second case, the human provides advice about 5% of the\ncells at their discretion.\n23\nUncertainty calibration.\nWe execute each experiment at multiple characteristic levels of uncertainty,\nsweeping through the {0.0, 0.2, 0.4, 0.6, 0.8} range. The case of complete uncertainty u = 1.0 is not\ntested as these opinions carry no information that can be used by the agent.\nCumulative reward\nFig. 9 shows the cumulative rewards of the agent with a human advisor under the two advice quotas.\nLinear scale\n0\n2000\n4000\n6000\n8000\n10000\nEpisode\n0\n2000\n4000\n6000\n8000\n10000\nCumulative Reward\nAdvice@u=0.0\nAdvice@u=0.2\nAdvice@u=0.4\nAdvice@u=0.6\nAdvice@u=0.8\nNo advice\nRandom\n(a) Advice quota: 10%\n0\n2000\n4000\n6000\n8000\n10000\nEpisode\n0\n2000\n4000\n6000\n8000\n10000\nCumulative Reward\nAdvice@u=0.0\nAdvice@u=0.2\nAdvice@u=0.4\nAdvice@u=0.6\nAdvice@u=0.8\nNo advice\nRandom\n(b) Advice quota: 5%\nLog scale\n0\n2000\n4000\n6000\n8000\n10000\nEpisode\n10\n1\n100\n101\n102\n103\n104\nCumulative Reward\nAdvice@u=0.0\nAdvice@u=0.2\nAdvice@u=0.4\nAdvice@u=0.6\nAdvice@u=0.8\nNo advice\nRandom\n(c) Advice quota: 10%\n0\n2000\n4000\n6000\n8000\n10000\nEpisode\n10\n1\n100\n101\n102\n103\n104\nCumulative Reward\nAdvice@u=0.0\nAdvice@u=0.2\nAdvice@u=0.4\nAdvice@u=0.6\nAdvice@u=0.8\nNo advice\nRandom\n(d) Advice quota: 5%\nFig. 9: Cumulative reward with a human advisor\nKey performance improvement trends are retained.\nWe see similar trends to those in Sec. 5.2.1.\nThat is, advised agents outperform the unadvised agent even with a severely reduced advice quota.\nThe human operating with 10% and 5% quotas is still able to improve the agent’s performance. With\n10% quota, the performance typically reaches that of the oracle. Moreover, at u = 0.2 and u = 0.8,\nthe human with 10% quota performs better than any oracle. At u = 0.4 and u = 0.6, the human\nadvisor with 10% advice quota performs better than the oracle with 20% advice quota. The only\ncase in which the human does not outperform at least one of the oracles is u = 0.0; but even in this\ncase, the human’s performance is only about 5% below of that of the oracles.\nThe role of certainty increases as the advice quota is reduced.\nThis trend is indicated by the distances\nbetween the lines both in the linear and log scales between 10% and 5%. At 10% advice quota,\nthe performance differences between different levels of certainty are similar; at 5% advice quota,\nperformance drops more prominently as certainty decreases.\n24\nAdvice quotas: differences more pronounced in low-certainty cases.\nThe difference between the two\nquotas is more obvious as certainty decreases. In the 5% case, even the fully certain advice u = 0.0\nperforms worse than the somewhat certain advice of u = 0.2 in the 10% case. Starting at u = 0.2,\nthe 5% quota deteriorates substantially; but still outperforming the unadvised agent.\nEffects on the policy\nFig. 10 visualizes the policy as a heatmap. The cells of each heatmap correspond to the cells of\nthe Frozen Lake problem; arrows represent the action with the highest probability; and shading\ncorresponds to the probability. Empty cells show unexplored states.\nKey improvements are retained.\nWe see similar trends to those in Sec. 5.2.1. The policy in advised\nagents is better reinforced, as indicated by the darker shades of blue, especially around holes and\nthe goal. We see more explored states by advised agents, though the difference between moderately\ncertain advice with low advice quota, and no advice is less substantial.\nCertainty becomes more of a factor.\nIn line with the observations about the cumulative reward in\nthe human-advised mode, certainty becomes an important factor in advice. The policy under 5%\nadvice quota and at u = 0.4 shows minor improvements over the unadvised mode, while fully certain\nadvice at u = 0.0 still shows substantial differences.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(a) No advice\nAdvice quota: 10%\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(b) u = 0.4\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(c) u = 0.0\nAdvice quota: 5%\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(d) u = 0.4\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(e) u = 0.0\nFig. 10: Heatmap visualization of the final learned policy with a human advisor\n5.2.3 Performance of guidance by cooperating human advisors with partial\ninformation\nAdvisor type.\nIn this experiment, we use the advice of two human advisors who have partial knowl-\nedge about the problem. Partial information is modeled by the advisors not giving advice about cells\nthat are beyond a given distance, as the increasing uncertainty would discount these pieces of advice,\nrendering their effect on the policy virtually nil.\n25\nAdvice quota.\nWe evaluate two advice quotas. In the first case, both human advisors provide advice\nabout 10% of the cells at their own discretion. In the second case, both human advisors provide\nadvice about 5% of the cells at their own discretion.\nUncertainty calibration.\nAs opposed to the previous experiments, where we evaluated guidance at\nmultiple levels of uncertainty, here, we calibrate uncertainty for each advice, based on the distance\nbetween the advisor and the location of the advice.\nCooperative advice type.\nWe evaluate two types of cooperation: sequential cooperation (one human\nin the top-left corner and on in the bottom-right corner), and parallel cooperation (one human in\nthe top-right corner and one in the bottom-left corner).\nCumulative reward\nFig. 11 shows the cumulative reward of the agent with two cooperating advisors under two different\nadvice quotas.\nLinear scale\n0\n2000\n4000\n6000\n8000\n10000\nEpisode\n0\n2000\n4000\n6000\n8000\n10000\nCumulative Reward\nCoop - Sequential\nCoop - Parallel\nNo advice\nRandom\n(a) Advice quota: 10%\n0\n2000\n4000\n6000\n8000\n10000\nEpisode\n0\n2000\n4000\n6000\n8000\n10000\nCumulative Reward\nCoop - Sequential\nCoop - Parallel\nNo advice\nRandom\n(b) Advice quota: 5%\nLog scale\n0\n2000\n4000\n6000\n8000\n10000\nEpisode\n10\n1\n100\n101\n102\n103\n104\nCumulative Reward\nCoop - Sequential\nCoop - Parallel\nNo advice\nRandom\n(c) Advice quota: 10%\n0\n2000\n4000\n6000\n8000\n10000\nEpisode\n10\n1\n100\n101\n102\n103\n104\nCumulative Reward\nCoop - Sequential\nCoop - Parallel\nNo advice\nRandom\n(d) Advice quota: 5%\nFig. 11: Cumulative reward with advice from two cooperating human advisors\nPerformance improvement over the unadvised agent is retained.\nSimilar to the previous experi-\nments, advised agents outperform the unadvised agent, despite the severely reduced advice quota,\nand the advisor’s partial information about the problem space.\nSequential cooperation outperforms parallel cooperation.\nThis performance difference is consistent\nboth with 10% and 5% advice quota. Parallel cooperation at 10% shows similar performance to\nsequential cooperation at 5%.\n26\nCooperation with partial information can perform comparably to fully informed humans.\nComparing\nthe cooperative results with single human experiments reveals that allowing 10% quota to each of the\ntwo humans in a sequential cooperation (Fig. 7a) performs similarly to a single human advisor with\n10% quota at almost complete certainty of u = 0.0–0.2 and outperforms the single human advisor\nwith 5% quota at very high levels of certainty at u = 0.2 (Fig. 9).\nLearning dynamics.\nLearning dynamics are affected in both cooperative cases, but the performance\ndeterioration is more pronounced in the 5% case. The learning curve is both delayed and less steep\nas compared to the previous experiments. Still, the pace of learning is significantly more rapid than\nthat of the unadvised agent.\nEffects on the policy\nFig. 12 visualizes the policy as a heatmap. The cells of each heatmap correspond to the cells of\nthe Frozen Lake problem; arrows represent the action with the highest probability; and shading\ncorresponds to the probability. Empty cells show unexplored states.\nCooperative advice outperforms single human advice.\nSpecifically, sequential cooperation results in\na better-reinforced policy and allows for more explored states in more uncertain situations. This is\ncan be inferred from the difference between Fig. 12 and Fig. 10: we encounter more darker cells\n(better reinforced local decisions) and less empty cells (less unexplored states). The effect on the\npolicy is weaker to that of the oracle.\nSequential cooperation is better in reinforcement and parallel cooperation is better in exploration\nperformance.\nIn pattern unique to cooperative guidance, we observe that heatmaps visualizing\nsequential cooperation show stronger reinforced local decisions (Fig. 12b over Fig. 12c, and Fig. 12d\nover Fig. 12e); while heatmaps visualizing parallel cooperation show better coverage of the problem\n(Fig. 12c over Fig. 12b, and Fig. 12e over Fig. 12d). In fact, parallel cooperative guidance at 10%\n(Fig. 12c) explores as many states as the oracle at 100% quota and u = 0.0 uncertainty (Fig. 8c),\ni.e., the fully informed and fully certain oracle.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(a) No advice\nAdvice quota: 10%\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(b) Sequential\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(c) Parallel\nAdvice quota: 5%\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(d) Sequential\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(e) Parallel\nFig. 12: Heatmap visualization of the final learned policy with two human advisors\n27\nConclusion of RQ2\nIn Sec. 1, we asked How does opinion-based guidance affect the performance of reinforcement learn-\ning agents? as our second research question. In this section, we designed and executed a study to\ninvestigate this research question.\nRQ2\nOur observations indicate that opinion-based guidance contributes to significant performance\nimprovement in reinforcement learning agents, even at moderate-to-high uncertainty. Human\nadvisors, even with substantially lower advice quotas, can perform at the level of an ora-\ncle, and might even outperform them. Uncertainty becomes more of a factor as advice quota\ndecreases. In realistic settings of cooperating humans with partial and complementary infor-\nmation about the problem, we find that guidance, even at low advice quotas, still results in\nsignificant improvements over unadvised agents.\n6 Discussion\nIn this section, we discuss the results and some of the important implications.\n6.1 The impact of advice – Discussion of the results\nEven uncertain advice is useful for guidance\nThe key takeaway of this work is that even uncertain opinions improve the performance of\nthe RL agent. The only case in which an opinion-guided RL agent did not outperform the unadvised\nagent is in the case with a single human advisor with 5% quota at uncertainty level u = 0.8 (Fig. 9b).\nTo provide an intuition of what uncertainty at the level of u = 0.8 means, e.g., in terms\nof the Frozen Lake problem, consider the two edge cases when an advisor is sure about a state\nbeing (i) disadvantageous and (ii) advantageous. Given that u = 0.8, the remaining opinion weight\nis 0.2, which has to be distributed over disbelief d and belief b. Case (i) translates to an opinion\nωi = (0.2, 0.0, 0.8, 0.25), and the second to ωii = (0.0, 0.2, 0.8, 0.25). Using the formulas from Sec. 3.2,\nthe two opinions translate to Pi = 0.2 + 0.25 × 0.8 = 0.6 and Pii = 0.0 + 0.25 × 0.8 = 0.4, respectively.\nThat is, the advisor can express they believe the probability of a particular state being advantageous\nshould be between 0.4–0.6 probability—good for a meager 20% wiggle room. In other words, the\nadvisor cannot express anything below 40% and above 60% which, evidently, is the descriptive power\nan unadvised agent has with 5% advice quota (Fig. 9b). (At higher advice quotas, the same 0.4–0.6\nadvice range, i.e., the same uncertainty of u = 0.8 is used more efficiently by the advisors, resulting\nin performance improvements over the unadvised agent.)\nConsidering that opinions emerge earlier than hard evidence can be produced (Dagenais and\nDavid, 2024), opinion-based guidance positions to be an important improvement over traditional\nguidance mechanisms. To leverage the benefits of opinion-based guidance, opinions should be elicited\nefficiently (Sec. 6.2).\nEffectiveness of guidance is impacted by certainty and exhaustiveness\nTwo key trends we observe are that performance deteriorates as uncertainty increases and\nthat performance deteriorates as advice quota decreases. We observe these trends in each\nof our experiments (see Tab. 10 and Fig. 6). The performance drop between advice quotas is more\npronounced at higher levels of uncertainty. Specifically, at u = 0.8, advice loses its guidance value at\n5% quota as the advised agent does not perform better than the unadvised one (single human case,\nFig. 9b). This combination of parameters seems to be the lower bound of utility of our approach.\nAnother interesting trend to observe is that at high levels of certainty (u = 0.0) the human\nadvisor with 5% advice quota reinforces as many key decisions as with 10% quota (cf. Fig. 10e and\nFig. 10c). Furthermore, two cooperating human advisors, each with 5% advice quota, reinforce the\nagent’s policy better than a single advisor with 10% quota (cf., e.g., Fig. 12d and Fig. 10c).\nThus, while higher certainty and higher advice quota (exhaustiveness) improve the effectiveness\nof guidance, the cost of a thorough advice strategy might not be always justified. We recommend\nadopters of our approach to seek trade-offs between certainty, exhaustiveness, and the costs of\nproviding advice.\n28\nHuman advisors are as effective as idealized oracle advisors\nComparing the performance of the oracles and human advisors (Tab. 10), we do not observe sub-\nstantial differences. The single human advisor with sufficient quota achieves a performance similar\nto that of the oracle. The performance is within 5% in three of five evaluated cases between oracles\nand human advisors (u = 0.0, 0.2, 0.6), with one of these cases showing an improvement of the human\nover the oracle (u = 0.2). In two of five evaluated cases, the difference is more pronounced, once in\nfavor of the oracle (u = 0.4) and once in favor of the human advisor (u = 0.8). Improvements due to\nhuman advice are particularly apparent in highly uncertain situations.\nWe attribute these observations to human creativity in providing advice. Thus, ours is another\none in the long line of works that highlight the role of human cognition in computer-automated\nmethods, such as machine learning Jarrahi (2018); Bansal et al (2019).\nSince the human advisor can quickly become the bottleneck in RL, adopters of our approach\nshould consider implementing efficient advice strategies, particularly asynchronous ones in which\nhuman and computer-automated agents can work independently.\nCooperative guidance with partial information performs well compared to advisors\nwith complete information\nOur experiments show that the performance of cooperative guidance with partial information about\nthe problem is only slightly below the performance of oracles and single human advisors with complete\ninformation about the problem. At u = 0.4, sequential cooperation with 2 × 5% quota outperforms\nthe comparable single human advisor with 10% quota (5 544.100 vs 4 727.433; +17.28%) and the\noracle with twice as much quota of 20% (5 544.100 vs 4 218.000; +31.44%). Related to this latter\ncomparison, at u = 0.2 sequential cooperation with 2 × 10% quota performs close to the oracle with\ncomparable quota (20%) – 7 551.567 vs 7 835.267 (-3.62%). These results demonstrate that partial\ninformation equates to moderate levels of uncertainty in fully-informed situations. The cooperative\nexperiments we conducted emulate a highly realistic setting. Thus, it is plausible to expect that the\nbenefits of opinion-based guidance can be retained in real applications.\nComparing cooperation modes, we observe that sequential cooperation gains more (Fig. 11a\nand Fig. 11b) cumulative reward and earlier (Fig. 11c and Fig. 11d) than parallel cooperation. In\nsequential cooperative guidance, uncertainty is lower in critical areas of map, i.e., around start, goal,\nand along path (Fig. 12b vs Fig. 12c; and Fig. 12d vs Fig. 12e). This means advice about critical areas\nhas more of an impact on the agent’s policy, which leads to higher cumulative reward. In particular,\nhaving an advisor located at the agent’s start state where uncertainty is low helps the agent navigate\nthe early parts of the map better. Thus, the agent is able to avoid holes early and take more steps\nin early episodes (more exploration), leading to learning the optimal policy earlier (as demonstrated\nin the log charts Fig. 11c and Fig. 11d).\n6.2 Open challenges and research opportunities\nOur approach opens up new challenges and research opportunities for prospective researchers.\nInteraction patterns for optimal guidance\nWe relied on one-time advice provided before exploration. However, more sophisticated advice dynam-\nics have been used in guided RL (Najar and Chetouani, 2021; Scherf et al, 2022). We recommend\nresearching more interactive patterns of advising. A known drawback of placing humans into\nthe learning loop is the high cost of human reward signals, which are caused by the human stake-\nholders quickly becoming performance bottlenecks (Knox and Stone, 2012). We see opportunities in\nopinion-based guidance in this aspect as opinions might be easier to formulate than hard evidence,\nalleviating the load on the human. Another interesting research opportunity our approach motivates\nis the investigation of push and pull mechanisms in advising RL agents, in which the human\ncan push advice toward the agent, and the agent can actively query the human for advice at any\ntime. Such mechanisms would allow for human advisors to provide advice under limited quotas when\nadvice matters the most.\nLanguages for expressing advice\nA key advantage of opinion-based guidance is the rapid pace at which opinions emerge. To lever-\nage this advantage to its fullest, intuitive languages are needed to express opinions. Domain-specific\nlanguages (DLSs) (Schmidt, 2006) have been widely used to capture expert opinions at high levels\n29\nof abstraction, narrowing the gap between the modeling language and human cognition. However,\nDSLs typically operate with reduced syntax to remain truly specific to the problem domain, and\ntherefore, automated methods for DSL engineering are in high demand (Bucchiarone et al,\n2020). This is true for guided RL as well. We envision language workbenches that allow for the\nautomated construction of DSLs through which advice can be provided by domain concepts. For\nexample, in the Frozen Lake example, one might want to use descriptive quantifiers, such as “haz-\nardous” or “beneficial” instead of relying on an integer scale. Unfortunately, there is little research\non domain-specific languages for RL, and existing languages mostly focus on programming RL\nprocedures (Molderez et al, 2019) rather than expressing opinions at high cognitive levels. We see\nplenty of room for research targeting domain-specific modeling languages for RL, inspired by the\nadvancements in the model-driven engineering community (David and G¨onczy, 2013; Kulagin et al,\n2022; Wu and Gray, 2005).\nOpinion-based guiding in value-based and deep reinforcement learning\nIn our approach, we opted for a policy-based RL algorithm because it allows for a better investigation\nof the impact of opinion on the explicitly represented policy. We are fairly confident that our method\ntranslates to value-based techniques as well, but validation is left for future work. A fairly more\ncomplicated challenge is introducing guidance into deep reinforcement learning, where the policy\nis encoded in a neural network, and the lack of explainability might hinder the development of a\nsound guiding framework. With the demonstrated utility of deep reinforcement learning in numerous\nproblems, such as the management of engineering systems (Andriotis and Papakonstantinou, 2019)\nand simulator inference (David and Syriani, 2022), its support by opinion-based guidance seems like\na natural next step.\n7 Threats to validity and limitations\nConstruct validity\nObservations of improved exploration performance may be artifacts of fortunately aligned problems\n(here: maps) or a few fortunate pieces of advice rather than artifacts of systematic effects. To mitigate\nthis threat to validity, first, we experimented with different map layouts and observed the same\nperformance trends. To facilitate consistent manual experimentation, we developed a map generator\nthat generates map layouts in a reproducible way. Second, we ran our scaled-up experiments with\na variety of settings, including both human and oracle advisors. A negligible threat to validity in\ncooperative experiments stems from the human advisors’ strategy of considering their own level of\ncertainty when giving advice. Human advisors prefer not to waste their limited advice quota and\nthus, will be likely biased towards giving advice with higher certainty. Thus, the positive results of\ncooperative guidance are artifacts of opinion-based guidance and human behavior. For the purposes\nof our research questions, human behavior does not threaten validity.\nInternal validity\nEvaluating the approach with human advisors inevitably poses threats to internal validity. Specif-\nically, maturation and testing might have influenced our results as the authors experimented with\nadvising strategies and advice values. For example, cells that are not holes or goals are labeled freely\nwith -1, 0, or 1, and a human might learn how to misuse these values to achieve a specific behavior\nby the agent. We mitigated these threats by defining clear advise score rules, which are also used by\nthe oracle advisor.\nExternal validity\nWe chose only one flavor of RL—a policy-based method—to demonstrate our approach, and we\ndid not provide evidence that our approach can be applied to other RL methods. Although we are\nreasonably confident that our methods translate to RL methods where the policy is not explicitly\nrepresented (e.g., value-based methods), future work will investigate this question in more detail.\nLimitations\nWe used a relatively simple guidance strategy, in which advice is provided once before the exploration.\nIt is reasonable to assume that more interactive guidance modes might shed light on additional\nbenefits and challenges. However, we are reasonably confident that the scope of this article was\n30\nsufficient to draw key takeaways that hold in more interactive reinforcement learning setups as well.\nNonetheless, we will experiment with interactive guidance modes in future work.\n8 Conclusion\nIn this article, we presented a method to guiding reinforcement learning agents by opinions—cognitive\nconstructs subject to uncertainty. We have devised a formal, mathematically sound method for using\nopinions for policy shaping, based on subjective logic, and demonstrated that opinions, even at\nmoderate-to-substantial levels of uncertainty help achieve better performance in reinforcement learn-\ning. Specifically, we observe improved cumulative rewards and faster convergence to the theoretical\nmaximum performance in opinion-guided reinforcement learning agents.\nOur method is particularly useful in situations where hard evidence is impractical or infeasible to\nproduce, e.g., in problems with a scarce reward structure, where human creativity plays an important\nrole. Such situations are, for example, problems with infeasible, inaccessible, hazardous, or costly\nstates, which, consequently, cannot be efficiently explored by the reinforcement learning agent. In\naddition, problems, where the expertise of multiple distinct domain experts is required, will benefit\nfrom our approach as well, thanks to the sound fusion semantics provided by subjective logic.\nAlthough demonstrated in a topological problem, our approach translates naturally to problems\nwhere the notion of distance is more abstract, e.g., in guided design-space exploration. Such directions\nare left for future work. Additional future work will focus on extending the method to different flavors\nof reinforcement learning, supporting more interactive advice modes, and evaluating our method in\nreal industry settings.\n31\nAppendix A\nStudy setup\nFig. A1: The map used in the experiments. (12x12 seed 63)\n32\nAppendix B\nInput artifacts\nB.1\nOracle\nB.1.1\nAdvice – 100% quota, oracle\n1 [11,11], +2\n2 [0,4], -2\n3 [0,5], -2\n4 [1,4], -2\n5 [1,5], -2\n6 [1,7], -2\n7 [1,8], -2\n8 [1,9], -2\n9 [1,11], -2\n10 [2,7], -2\n11 [3,0], -2\n12 [3,7], -2\n13 [4,4], -2\n14 [4,9], -2\n15 [5,5], -2\n16 [6,3], -2\n17 [6,4], -2\n18 [6,5], -2\n19 [6,7], -2\n20 [7,1], -2\n21 [7,3], -2\n22 [8,3], -2\n23 [9,1], -2\n24 [9,4], -2\n25 [9,6], -2\n26 [10,1], -2\n27 [10,4], -2\n28 [11,10], -2\n29 [0,0], +1\n30 [0,1], +1\n31 [0,2], +1\n32 [0,3], +0\n33 [0,6], +0\n34 [0,7], +0\n35 [0,8], +0\n36 [0,9], +0\n37 [0,10], +1\n38 [0,11], +0\n39 [1,0], +1\n40 [1,1], +1\n41 [1,2], +1\n42 [1,3], +0\n43 [1,6], -1\n44 [1,10], -1\n45 [2,0], +0\n46 [2,1], +1\n47 [2,2], +1\n48 [2,3], +1\n49 [2,4], +0\n50 [2,5], +0\n51 [2,6], +0\n52 [2,8], -1\n53 [2,9], +0\n54 [2,10], +1\n55 [2,11], +0\n56 [3,1], +0\n57 [3,2], +1\n58 [3,3], +1\n59 [3,4], +0\n60 [3,5], +1\n61 [3,6], +0\n62 [3,8], +0\n63 [3,9], +0\n64 [3,10], +1\n65 [3,11], +1\n66 [4,0], +0\n67 [4,1], +1\n68 [4,2], +1\n69 [4,3], +0\n70 [4,5], -1\n71 [4,6], +1\n72 [4,7], +0\n73 [4,8], +0\n74 [4,10], +0\n75 [4,11], +1\n76 [5,0], +1\n77 [5,1], +1\n78 [5,2], +1\n79 [5,3], +0\n80 [5,4], -1\n81 [5,6], +0\n82 [5,7], +0\n83 [5,8], +1\n84 [5,9], +0\n85 [5,10], +1\n86 [5,11], +1\n87 [6,0], +1\n88 [6,1], +0\n89 [6,2], +0\n90 [6,6], -1\n91 [6,8], +0\n92 [6,9], +1\n93 [6,10], +1\n94 [6,11], +1\n95 [7,0], +0\n96 [7,2], -1\n97 [7,4], -1\n98 [7,5], +0\n99 [7,6], +1\n100 [7,7], +0\n101 [7,8], +1\n102 [7,9], +1\n103 [7,10], +1\n104 [7,11], +1\n105 [8,0], +1\n106 [8,1], -1\n107 [8,2], +0\n108 [8,4], -1\n109 [8,5], +1\n110 [8,6], +0\n111 [8,7], +1\n112 [8,8], +1\n113 [8,9], +1\n114 [8,10], +1\n115 [8,11], +1\n116 [9,0], +0\n117 [9,2], +0\n118 [9,3], -1\n119 [9,5], -1\n120 [9,7], +0\n121 [9,8], +1\n122 [9,9], +1\n123 [9,10], +1\n124 [9,11], +1\n125 [10,0], +0\n126 [10,2], +0\n127 [10,3], +0\n128 [10,5], +0\n129 [10,6], +0\n130 [10,7], +1\n131 [10,8], +1\n132 [10,9], +1\n133 [10,10], +0\n134 [10,11], +1\n135 [11,0], +1\n136 [11,1], +0\n137 [11,2], +1\n138 [11,3], +1\n139 [11,4], +0\n140 [11,5], +1\n141 [11,6], +1\n142 [11,7], +1\n143 [11,8], +1\n144 [11,9], +0\nListing 3: Advice of the oracle\nwith\n100%\nquota,\nwith\nan\nopinion\nabout\nevery\ncell\nof\nFig. A1.\n33\nB.1.2\nAdvice – 20% quota, oracle\n1 [11,11], +2\n2 [0,4], -2\n3 [0,5], -2\n4 [1,4], -2\n5 [1,5], -2\n6 [1,7], -2\n7 [1,8], -2\n8 [1,9], -2\n9 [1,11], -2\n10 [2,7], -2\n11 [3,0], -2\n12 [3,7], -2\n13 [4,4], -2\n14 [4,9], -2\n15 [5,5], -2\n16 [6,3], -2\n17 [6,4], -2\n18 [6,5], -2\n19 [6,7], -2\n20 [7,1], -2\n21 [7,3], -2\n22 [8,3], -2\n23 [9,1], -2\n24 [9,4], -2\n25 [9,6], -2\n26 [10,1], -2\n27 [10,4], -2\n28 [11,10], -2\nListing 4: Advice of the oracle with 20% quota, with an opinion about the holes and the goal cell of\nFig. A1.\nB.2\nSingle human experiments\nB.2.1\nAdvice – 10% quota, single human advisor\n1 [1,1], +1\n2 [1,4], -2\n3 [2,2], +1\n4 [3,4], +1\n5 [3,5], +2\n6 [3,7], -2\n7 [4,4], -2\n8 [4,6], +2\n9 [4,9], -2\n10 [5,5], -2\n11 [5,7], +2\n12 [6,7], -2\n13 [11,10], -2\n14 [11,11], +2\nFig. B2: Advice of the single human advisor with 10% quota, and its visualization.\n34\nB.2.2\nAdvice – 5% quota, single human advisor\n1 [1,4], -2\n2 [3,7], -2\n3 [4,4], -2\n4 [5,5], -2\n5 [6,7], -2\n6 [11,10], -2\n7 [11,11], +2\nFig. B3: Advice of the single human advisor with 5% quota, and its visualization.\nB.3\nCooperative human experiments\nB.3.1\nSequential cooperation, 10% quota each\n1 [0,4], -2\n2 [0,5], -2\n3 [1,4], -2\n4 [1,5], -2\n5 [1,6], -1\n6 [2,4], +1\n7 [2,7], -2\n8 [3,0], -2\n9 [3,4], +1\n10 [3,5], +1\n11 [3,7], -2\n12 [4,4], -2\n13 [4,6], +1\n14 [5,5], -2\nFig. B4: Advice from the top-left corner in the sequential cooperative human experiments, with 10%\nquota, and its visualization. The counterpart of the cooperative advice is shown in Fig. B5.\n1 [1,10], -1\n2 [1,11], -2\n3 [4,9], -2\n4 [5,7], +1\n5 [5,8], +1\n6 [5,9], +1\n7 [6,5], -2\n8 [6,6], -1\n9 [6,7], -2\n10 [9,4], -2\n11 [9,6], -2\n12 [10,4], -2\n13 [11,10], -2\n14 [11,11], +2\nFig. B5: Advice from the bottom-right corner in the sequential cooperative human experiments, with\n10% quota, and its visualization. The counterpart of the cooperative advice is shown in Fig. B4.\n35\nB.3.2\nSequential cooperation, 5% quota each\n1 [1,4], -2\n2 [1,5], -2\n3 [1,6], -1\n4 [2,7], -2\n5 [3,7], -2\n6 [4,4], -2\n7 [5,5], -2\nFig. B6: Advice from the top-left corner in the sequential cooperative human experiments, with 5%\nquota, and its visualization. The counterpart of the cooperative advice is shown in Fig. B7.\n1 [4,9], -2\n2 [5,7], +1\n3 [6,5], -2\n4 [6,7], -2\n5 [9,4], -2\n6 [11,10], -2\n7 [11,11], +2\nFig. B7: Advice from the bottom-right corner in the sequential cooperative human experiments,\nwith 5% quota, and its visualization. The counterpart of the cooperative advice is shown in Fig. B6.\nB.3.3\nParallel cooperation, 10% quota each\n1 [1,4], -2\n2 [1,5], -2\n3 [1,6], -1\n4 [1,7], -2\n5 [1,8], -2\n6 [1,9], -2\n7 [1,10], -1\n8 [1,11], -2\n9 [2,7], -2\n10 [3,7], -2\n11 [4,9], -2\n12 [6,7], -2\n13 [10,11], +1\n14 [11,11], +2\nFig. B8: Advice from the top-right corner in the parallel cooperative human experiments, with 10%\nquota, and its visualization. The counterpart of the cooperative advice is shown in Fig. B9.\n36\n1 [4,4], -2\n2 [5,5], -2\n3 [6,3], -2\n4 [6,5], -2\n5 [7,1], -2\n6 [7,3], -2\n7 [8,3], -2\n8 [9,1], -2\n9 [9,4], -2\n10 [9,6], -2\n11 [10,1], -2\n12 [10,4], -2\n13 [11,10], -2\n14 [11,11], +2\nFig. B9: Advice from the bottom-left corner in the parallel cooperative human experiments, with\n10% quota, and its visualization. The counterpart of the cooperative advice is shown in Fig. B8.\nB.3.4\nParallel cooperation, 5% quota each\n1 [1,4], -2\n2 [2,7], -2\n3 [3,7], -2\n4 [4,9], -2\n5 [6,7], -2\n6 [10,11], +1\n7 [11,11], +2\nFig. B10: Advice from the top-right corner in the parallel cooperative human experiments, with 5%\nquota, and its visualization. The counterpart of the cooperative advice is shown in Fig. B11.\n1 [4,4], -2\n2 [5,5], -2\n3 [6,3], -2\n4 [6,5], -2\n5 [7,3], -2\n6 [9,4], -2\n7 [11,10], -2\nFig. B11: Advice from the bottom-left corner in the parallel cooperative human experiments, with\n5% quota, and its visualization. The counterpart of the cooperative advice is shown in Fig. B10.\n37\nReferences\nAdams EW (1996) A Primer of Probability Logic. Center for the Study of Language and Inf\nAlshiekh M, Bloem R, Ehlers R, et al (2018) Safe reinforcement learning via shielding. In: Proceedings\nof the AAAI conference on artificial intelligence\nAndriotis C, Papakonstantinou K (2019) Managing engineering systems with large state and action\nspaces through deep reinforcement learning. Reliability Engineering & System Safety 191:106483.\nhttps://doi.org/https://doi.org/10.1016/j.ress.2019.04.036\nArakawa R, Kobayashi S, Unno Y, et al (2018) DQN-TAMER: Human-in-the-loop reinforcement\nlearning with intractable feedback. 1810.11748\nBansal G, Nushi B, Kamar E, et al (2019) Beyond accuracy: The role of mental models in human-ai\nteam performance. In: Proceedings of the Seventh AAAI Conference on Human Computation and\nCrowdsourcing, HCOMP 2019, Stevenson, WA, USA, October 28-30, 2019. AAAI Press, pp 2–11,\nhttps://doi.org/10.1609/HCOMP.V7I1.5285, URL https://doi.org/10.1609/hcomp.v7i1.5285\nBarquero G, Troya J, Vallecillo A (2021) Improving query performance on dynamic graphs. Software\nand Systems Modeling 20(4):1011–1041. https://doi.org/10.1007/s10270-020-00832-3\nBarriga A, Heldal R, Rutle A, et al (2022) PARMOREL: a framework for customizable model repair.\nSoftware and Systems Modeling 21(5):1739–1762. https://doi.org/10.1007/s10270-022-01005-0\nBradshaw JM, Hoffman RR, Woods DD, et al (2013) The seven deadly myths of ”autonomous\nsystems”. IEEE Intelligent Systems 28(3):54–61. https://doi.org/10.1109/MIS.2013.70\nBrawer J, Ghose D, Candon K, et al (2023) Interactive policy shaping for human-robot collaboration\nwith transparent matrix overlays. In: Proceedings of the 2023 ACM/IEEE International Conference\non Human-Robot Interaction, pp 525–533\nBucchiarone A, Cabot J, Paige RF, et al (2020) Grand challenges in model-driven engineering: an\nanalysis of the state of the research. Software and Systems Modeling 19(1):5–13. https://doi.org/\n10.1007/s10270-019-00773-6\nBurgue˜no L, Mu˜noz P, Claris´o R, et al (2023) Dealing with belief uncertainty in domain models.\nACM Trans Softw Eng Methodol 32(2). https://doi.org/10.1145/3542947\nCederborg T, Grover I, Isbell Jr CL, et al (2015) Policy shaping with human teachers. In: IJCAI, pp\n3366–3372\nCelemin C, P´erez-Dattari R, Chisari E, et al (2022) Interactive imitation learning in robotics: A\nsurvey. Foundations and Trends in Robotics 10(1-2):1–197. https://doi.org/10.1561/2300000072\nChristiano P, Leike J, Brown TB, et al (2017) Deep reinforcement learning from human preferences.\narXiv preprint arXiv:170603741\nCronrath C, Aderiani AR, Lennartson B (2019) Enhancing digital twins through reinforcement\nlearning. In: Automation Science and Engineering, IEEE, pp 293–298\nCruz F, Twiefel J, Magg S, et al (2015) Interactive reinforcement learning through speech guidance\nin a domestic scenario. In: 2015 international joint conference on neural networks (IJCNN), IEEE,\npp 1–8\nCruz F, W¨uppen P, Magg S, et al (2017) Agent-advising approaches in an interactive reinforcement\nlearning scenario. In: 2017 Joint IEEE International Conference on Development and Learning\nand Epigenetic Robotics (ICDL-EpiRob), pp 209–214, https://doi.org/10.1109/DEVLRN.2017.\n8329809\nDagenais K, David I (2024) Driving requirements evolution by engineers’ opinions. In: ACM/IEEE\nInternational Conference on Model Driven Engineering Languages and Systems Companion,\n38\nMODELS-C. ACM\nDai J, Pan X, Sun R, et al (2023) Safe rlhf: Safe reinforcement learning from human feedback. arXiv\npreprint arXiv:231012773\nDavid I, G¨onczy L (2013) Ontology-Supported Design of Domain-Specific Languages: A Complex\nEvent Processing Case Study, IGI Global. https://doi.org/10.4018/978-1-4666-4494-6.ch006, URL\nhttp://www.igi-global.com/chapter/ontology-supported-design-domain-specific/78613/\nDavid I, Syriani E (2022) DEVS model construction as a reinforcement learning problem. In: 2022\nAnnual Modeling and Simulation Conference (ANNSIM), pp 30–41, https://doi.org/10.23919/\nANNSIM55834.2022.9859369\nDavid I, Syriani E (2024) Automated Inference of Simulators in Digital Twins. In: Handbook of\nDigital Twins. CRC Press, chap 8, p 122–148, https://doi.org/10.1201/9781003425724-11\nDavid I, Syriani E, Verbrugge C, et al (2016) Towards inconsistency tolerance by quantification\nof semantic inconsistencies. In: Proceedings of the 1st International Workshop on Collabora-\ntive Modelling in MDE (COMMitMDE 2016) co-located with ACM/IEEE 19th International\nConference on Model Driven Engineering Languages and Systems (MoDELS 2016), St. Malo,\nFrance, October 4, 2016, CEUR Workshop Proceedings, vol 1717. CEUR-WS.org, pp 35–44, URL\nhttps://ceur-ws.org/Vol-1717/paper8.pdf\nF¨urnkranz J, H¨ullermeier E, Cheng W, et al (2012) Preference-based reinforcement learning: a formal\nframework and a policy iteration algorithm. Machine learning 89:123–156\nGardenfors P, Sahlin NE (2005) Unreliable Probabilities, Risk Taking, and Decision Making,\nSpringer Netherlands, Dordrecht, pp 11–29. https://doi.org/10.1007/1-4020-3399-0 2, URL https:\n//doi.org/10.1007/1-4020-3399-0 2\nGriffith S, Subramanian K, Scholz J, et al (2013) Policy shaping: Integrating human feed-\nback with reinforcement learning. In: Advances in Neural Information Processing Systems,\nvol 26. Curran Associates, Inc., URL https://proceedings.neurips.cc/paper files/paper/2013/file/\ne034fb6b66aacc1d48f445ddfb08da98-Paper.pdf\nGrizou J, Lopes M, Oudeyer PY (2013) Robot learning simultaneously a task and how to interpret\nhuman instructions. In: 2013 IEEE Third Joint International Conference on Development and\nLearning and Epigenetic Robotics (ICDL), pp 1–8, https://doi.org/10.1109/DevLrn.2013.6652523\nGuo Z, Norman TJ, Gerding EH (2022) Mtirl: Multi-trainer interactive reinforcement learning sys-\ntem. In: International Conference on Principles and Practice of Multi-Agent Systems, Springer, pp\n227–242\nGuo Z, Zhang Q, An X, et al (2023) Uncertainty-aware reward-based deep reinforcement learning\nfor intent analysis of social media information. In: 1st AAAI Workshop on Uncertainty Reasoning\nand Quantification in Decision Making (UDM-AAAI’23)\nH¨ullermeier E, Waegeman W (2021) Aleatoric and epistemic uncertainty in machine learning: an\nintroduction to concepts and methods. Machine Learning 110(3):457–506. https://doi.org/10.1007/\ns10994-021-05946-3\nJarrahi MH (2018) Artificial intelligence and the future of work: Human-ai symbiosis in organizational\ndecision making. Business Horizons 61(4):577–586. https://doi.org/https://doi.org/10.1016/j.\nbushor.2018.03.007, URL https://www.sciencedirect.com/science/article/pii/S0007681318300387\nJongeling R, Vallecillo A (2023) Uncertainty-aware consistency checking in industrial settings. In:\n2023 ACM/IEEE 26th International Conference on Model Driven Engineering Languages and\nSystems (MODELS), pp 73–83, https://doi.org/10.1109/MODELS58315.2023.00026\nJøsang A (2016) Subjective Logic. Springer International Publishing, https://doi.org/10.1007/\n978-3-319-42337-1\n39\nJøsang A, Bondi VA (2000) Legal reasoning with subjective logic. AI and Law 8(4):289–315. https:\n//doi.org/10.1023/A:1011219731903\nJøsang A, Costa PC, Blasch E (2013) Determining model correctness for situations of belief fusion.\nIn: Proceedings of the 16th International Conference on Information Fusion, pp 1886–1893\nKessler Faulkner TA, Thomaz A (2021) Interactive reinforcement learning from imperfect teachers.\nIn: Companion of the 2021 ACM/IEEE International Conference on Human-Robot Interaction.\nACM, HRI ’21 Companion, p 577–579, https://doi.org/10.1145/3434074.3446361\nKnox WB, Stone P (2009) Interactively shaping agents via human reinforcement: the tamer frame-\nwork. In: Proceedings of the Fifth International Conference on Knowledge Capture. ACM, K-CAP\n’09, p 9–16, https://doi.org/10.1145/1597735.1597738, URL https://doi.org/10.1145/1597735.\n1597738\nKnox WB, Stone P (2012) Reinforcement learning from simultaneous human and mdp reward. In:\nProceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems\n- Volume 1. International Foundation for Autonomous Agents and Multiagent Systems, Richland,\nSC, AAMAS ’12, p 475–482\nKoert D, Kircher M, Salikutluk V, et al (2020) Multi-channel interactive reinforcement learning for\nsequential tasks. Frontiers in Robotics and AI 7. https://doi.org/10.3389/frobt.2020.00097\nKulagin G, Ermakov I, Lyadova L (2022) Ontology-based development of domain-specific languages\nvia customizing base language. In: 2022 IEEE 16th International Conference on Application\nof Information and Communication Technologies (AICT), pp 1–6, https://doi.org/10.1109/\nAICT55583.2022.10013619\nLi G, Gomez R, Nakamura K, et al (2019) Human-centered reinforcement learning: A survey. IEEE\nTransactions on Human-Machine Systems 49(4):337–349. https://doi.org/10.1109/THMS.2019.\n2912447\nLi Z, Shi L, Cristea AI, et al (2021) A survey of collaborative reinforcement learning: Interactive\nmethods and design patterns. In: Proceedings of the 2021 ACM Designing Interactive Systems\nConference. ACM, DIS ’21, p 1579–1590, https://doi.org/10.1145/3461778.3462135\nMaclin R, Shavlik JW (1996) Creating advice-taking reinforcement learners. Machine Learning\n22(1):251–281. https://doi.org/10.1023/A:1018020625251\nMargoni F, Walkinshaw N (2023) Subjective logic as a complementary tool to meta-analysis to\ntransparently address second-order uncertainty in research findings. Tech. rep.\nMolderez T, Oeyen B, De Roover C, et al (2019) Marlon: a domain-specific language for multi-\nagent reinforcement learning on networks. In: Proceedings of the 34th ACM/SIGAPP Symposium\non Applied Computing. Association for Computing Machinery, New York, NY, USA, SAC ’19, p\n1322–1329, https://doi.org/10.1145/3297280.3297413\nNajar A, Chetouani M (2021) Reinforcement learning with human advice: A survey. Frontiers in\nRobotics and AI 8. https://doi.org/10.3389/frobt.2021.584075\nNajar A, Sigaud O, Chetouani M (2016) Training a robot with evaluative feedback and unlabeled\nguidance signals. In: 2016 25th IEEE International Symposium on Robot and Human Interactive\nCommunication (RO-MAN), pp 261–266, https://doi.org/10.1109/ROMAN.2016.7745140\nNavarrete FJ, Vallecillo A (2021) Introducing subjective knowledge graphs. In: 2021 IEEE 25th\nInternational Enterprise Distributed Object Computing Conference (EDOC), IEEE, pp 61–70\nPertsch K, Lee Y, Wu Y, et al (2022) Demonstration-guided reinforcement learning with learned\nskills. In: Proceedings of the 5th Conference on Robot Learning, Proceedings of Machine Learning\nResearch, vol 164. PMLR, pp 729–739, URL https://proceedings.mlr.press/v164/pertsch22a.html\n40\nPuterman ML (1990) Markov decision processes. Handbooks in operations research and management\nscience 2:331–434\nScherf L, Turan C, Koert D (2022) Learning from unreliable human action advice in interactive\nreinforcement learning. In: 2022 IEEE-RAS 21st International Conference on Humanoid Robots\n(Humanoids), pp 895–902, https://doi.org/10.1109/Humanoids53995.2022.10000078\nSchmidt DC (2006) Model-driven engineering. Computer-IEEE Computer Society 39(2):25\nSubramanian K, Isbell CL, Thomaz AL (2016) Exploration from demonstration for interactive rein-\nforcement learning. In: Proceedings of the 2016 International Conference on Autonomous Agents\n& Multiagent Systems. International Foundation for Autonomous Agents and Multiagent Systems,\nRichland, SC, AAMAS ’16, p 447–456\nSutton RS, Barto AG (2018) Reinforcement learning: An introduction. MIT press\nTenorio-Gonzalez AC, Morales EF, Villase˜nor-Pineda L (2010) Dynamic reward shaping: Train-\ning a robot by voice. In: Advances in Artificial Intelligence – IBERAMIA 2010. Springer Berlin\nHeidelberg, Berlin, Heidelberg, pp 483–492\nThomaz AL, Breazeal C (2006) Reinforcement learning with human teachers: Evidence of feedback\nand guidance with implications for learning performance. In: Proceedings of the 21st National\nConference on Artificial Intelligence - Volume 1. AAAI Press, AAAI’06, p 1000–1005\nVogel A, Jurafsky D (2010) Learning to follow navigational directions. In: Proceedings of the 48th\nAnnual Meeting of the Association for Computational Linguistics. Association for Computational\nLinguistics, USA, ACL ’10, p 806–814\nWalkinshaw N, Shepperd M (2020) Reasoning about uncertainty in empirical results. In: Proceedings\nof the 24th International Conference on Evaluation and Assessment in Software Engineering. ACM,\nEASE ’20, p 140–149, https://doi.org/10.1145/3383219.3383234\nWang Q, Grace D (2022) Proactive edge caching in vehicular networks: An online bandit learning\napproach. IEEE Access 10:131246–131263\nWu H, Gray J (2005) Automated generation of testing tools for domain-specific languages. In:\nProceedings of the 20th IEEE/ACM International Conference on Automated Software Engi-\nneering. Association for Computing Machinery, New York, NY, USA, ASE ’05, p 436–439,\nhttps://doi.org/10.1145/1101908.1101993\nWu J, Zhou Y, Yang H, et al (2023) Human-guided reinforcement learning with sim-to-real trans-\nfer for autonomous navigation. IEEE Transactions on Pattern Analysis and Machine Intelligence\n45(12):14745–14759. https://doi.org/10.1109/TPAMI.2023.3314762\nZennaro FM, Jøsang A (2020) Using subjective logic to estimate uncertainty in multi-armed bandit\nproblems. 2008.07386\nZhang G, Kashima H (2024) Learning state importance for preference-based reinforcement learning.\nMachine Learning 113(4):1885–1901\nZhao X, Hu S, Cho JH, et al (2019) Uncertainty-based decision making using deep reinforcement\nlearning. In: 2019 22th International Conference on Information Fusion (FUSION), pp 1–8, https:\n//doi.org/10.23919/FUSION43075.2019.9011218\nZhou Z, Oguz OS, Leibold M, et al (2021) Learning a low-dimensional representation of a safe region\nfor safe reinforcement learning on dynamical systems. IEEE Transactions on Neural Networks and\nLearning Systems 34(5):2513–2527\n41\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2024-05-27",
  "updated": "2024-08-03"
}