{
  "id": "http://arxiv.org/abs/2405.17439v2",
  "title": "An Overview of Machine Learning-Enabled Optimization for Reconfigurable Intelligent Surfaces-Aided 6G Networks: From Reinforcement Learning to Large Language Models",
  "authors": [
    "Hao Zhou",
    "Chengming Hu",
    "Xue Liu"
  ],
  "abstract": "Reconfigurable intelligent surface (RIS) becomes a promising technique for 6G\nnetworks by reshaping signal propagation in smart radio environments. However,\nit also leads to significant complexity for network management due to the large\nnumber of elements and dedicated phase-shift optimization. In this work, we\nprovide an overview of machine learning (ML)-enabled optimization for RIS-aided\n6G networks. In particular, we focus on various reinforcement learning (RL)\ntechniques, e.g., deep Q-learning, multi-agent reinforcement learning, transfer\nreinforcement learning, hierarchical reinforcement learning, and offline\nreinforcement learning. Different from existing studies, this work further\ndiscusses how large language models (LLMs) can be combined with RL to handle\nnetwork optimization problems. It shows that LLM offers new opportunities to\nenhance the capabilities of RL algorithms in terms of generalization, reward\nfunction design, multi-modal information processing, etc. Finally, we identify\nthe future challenges and directions of ML-enabled optimization for RIS-aided\n6G networks.",
  "text": "This is an invited work towards IEEE Future Networks World Forum.\nAn Overview of Machine Learning-Enabled\nOptimization for Reconfigurable Intelligent\nSurfaces-Aided 6G Networks: From Reinforcement\nLearning to Large Language Models\nHao Zhou, Member, IEEE, Chengming Hu, Student Member, IEEE, Xue Liu, Fellow, IEEE.\nSchool of Computer Science, McGill University, Montreal, Quebec, Canada\n{hao.zhou4, chengming.hu}@mail.mcgill.ca, xueliu@cs.mcgill.ca\nAbstract—Reconfigurable intelligent surface (RIS) becomes\na promising technique for 6G networks by reshaping signal\npropagation in smart radio environments. However, it also leads\nto significant complexity for network management due to the\nlarge number of elements and dedicated phase-shift optimization.\nIn this work, we provide an overview of machine learning (ML)-\nenabled optimization for RIS-aided 6G networks. In particular,\nwe focus on various reinforcement learning (RL) techniques, e.g.,\ndeep Q-learning, multi-agent reinforcement learning, transfer\nreinforcement learning, hierarchical reinforcement learning, and\noffline reinforcement learning. Different from existing studies,\nthis work further discusses how large language models (LLMs)\ncan be combined with RL to handle network optimization\nproblems. It shows that LLM offers new opportunities to enhance\nthe capabilities of RL algorithms in terms of generalization,\nreward function design, multi-modal information processing, etc.\nFinally, we identify the future challenges and directions of ML-\nenabled optimization for RIS-aided 6G networks.\nIndex Terms—6G, Reconfigurable intelligent surfaces, Opti-\nmization, Machine learning, Large language models.\nI. INTRODUCTION\nReconfigurable intelligent surface (RIS) is a promising\ntechnology for 6G networks, providing attractive properties\nto reshape the signal propagation path [1]. Many studies have\ndemonstrated that RISs can significantly improve the network\nchannel capacity, signal coverage, transmission security, en-\nergy efficiency, etc [2]. In addition, RISs also have low energy\nconsumption and hardware cost, and therefore they can be\neasily deployed in various scenarios, such as building walls\nand ceilings, to improve the signal transmission environment.\nGiven the great potential, RISs have been combined with many\nother state-of-the-art techniques, such as mmWave and THz\ncommunications, non-terrestrial networks, unmanned aerial\nvehicles (UAVs), vehicle-to-everything (V2X) networks, non-\northogonal multiple access (NOMA), and so on. Despite the\nimproved performance, it is worth noting that integrating RISs\ninto existing wireless networks may considerably increase the\nnetwork optimization complexity [2]. In particular, RISs may\nconsist of hundreds of small elements, and each unit requires\ndedicated control of the phase shifts and on/off status, lead-\ning to large solution spaces with non-convex objectives and\nconstraints. Convex optimization is a widely considered ap-\nproach for RIS-related optimization problems, e.g., alternating\noptimization, majorization-minimization algorithm, successive\nconvex optimization, semidefinite relaxation, and others. How-\never, these optimization problems are usually highly non-\nconvex and non-linear, which must be reformulated to convex\nformats. Moreover, there may be integer control variables due\nto resource allocation and associate problems, and the mixed\ninteger non-linear programming (MINLP) problem is a well-\nknown challenge for convex optimization techniques.\nTo this end, machine learning (ML) offers promising oppor-\ntunities to overcome these complexities [3]. For instance, rein-\nforcement learning (RL) is the most widely used ML-enabled\noptimization algorithm, and many optimization problems can\nbe easily transformed into unified Markov decision processes\n(MDPs) [4]. In particular, the optimization objectives, such\nas achieved data rates and coverage, are defined as rewards,\ncontrol variables like RIS phase shifts become actions, and\nnetwork dynamics are transformed into environment states,\ne.g., channel state information and previous data rates. In\nthis way, RL algorithms can be applied to try different ac-\ntion combinations under various states, aiming to maximize\nthe long-term network performance. It indicates a unified\noptimization scheme regardless of the problem convexity or\ncontinuity, lowering the difficulty of optimizing network per-\nformance. Meanwhile, RL can also be combined with other\nML techniques, indicating diverse RL algorithms with unique\nadvantages. For instance, integrating transfer learning and RL\ncan produce transfer reinforcement learning (TRL), enabling\nknowledge sharing between previously completed tasks and\nincoming new tasks [5]. Such a knowledge reuse scheme can\nbe very useful for solving a series of tasks with similarities,\nwhich are very common in wireless networks.\nRecent years have witnessed the rapid progress of ML tech-\nniques, and specifically large language models (LLMs) have\nreceived considerable interest from academia and industry,\nleading to revolutionary changes in many fields [6]. Compared\nwith conventional ML techniques, LLMs have demonstrated\noutstanding comprehension and reasoning capabilities after the\npre-training on large datasets, e.g., instruction following, in-\n1\narXiv:2405.17439v2  [cs.NI]  17 Sep 2024\ncontext learning, and zero-shot learning, among others [7]. For\nexample, in-context learning means that LLMs can learn from\ncontextual inputs such as task descriptions and demonstrations,\nand then apply these hidden patterns to the downstream tasks\ndirectly. By contrast, in previous ML algorithms, learning\na new policy usually requires dedicated design, indicating\niterative and time-consuming model training or fine-tuning.\nFurthermore, LLMs can utilize contextual feedback from pre-\nvious tasks to improve the implementation of the next task,\nwhich is known as self-reflection. Recent studies reveal that\nLLMs can work as agents to improve the performance of target\ntasks by learning from the environment feedback iteratively\n[8]. Such optimization capabilities become more promising\nwhen combined with LLM’s instruction-following features.\nFor instance, operators can use natural language to instruct the\nnetwork management policy, and then the LLM-based agent\ncan improve the overall policy automatically.\nGiven the above advantages of LLM techniques, it is\ncrucial to investigate the applications of state-of-the-art ML\ntechniques to optimize RIS-aided 6G networks, paving the\nway to artificial general intelligence (AGI)-enabled 6G. In\nthis work, we provide a comprehensive overview of using\nML to optimize RIS-aided 6G networks, ranging from various\nreinforcement learning algorithms to LLM-aided optimization\ntechniques. Specifically, we present an in-depth discussion on\nstate-of-the-art LLM techniques for optimization problems,\nwhich is different than many existing studies on RIS-related\noptimization problems. The rest of this work is organized\nas the following. Section II introduces related studies, and\nSection III overviews the problem formulations and features\nof RIS-related optimization problems. Section IV presents\nvarious RL algorithms for RIS-related optimization tasks,\nand Section V discusses LLM-aided optimization techniques.\nFinally, Section VI identifies challenges and future directions\nand Section VII concludes this work.\nII. RELATED WORKS\nThere have been many studies that apply ML techniques\nto optimize RIS-aided wireless networks. For instance, Faisal\nand Choi summarized various ML algorithms for RISs in\n[3], including supervised learning, unsupervised learning, re-\ninforcement learning, federated learning, etc. Zhou et al.\npresented a comprehensive survey on model-based, heuris-\ntic, and ML algorithms for RIS-aided wireless networks in\n[2], and they further explored heuristic-aided ML for RIS\noptimization in [9]. Meanwhile, Liu et al. introduced RIS\nbeamforming, resource management and ML algorithms for\nRIS-aided networks in [1]. Puspitasari and Less provided\nan overview of RISs and RL technique implementations to\noptimize RIS design and operation [10]. The above studies\n[1]–[3], [9], [10] have investigated various ML algorithms for\nRIS-aided wireless networks. Especially, RL is involved in\nmost of the previous studies due to its critical importance\nin handling optimization problems, e.g., the RL framework\nproposed in [1], heuristic DQN in [9], and transfer RL in [2].\nFig. 1. Downlink transmission system for RIS-aided networks.\nHowever, the ML field is advancing rapidly, and LLM is\nconsidered the most state-of-the-art ML technique, leading to\nrevolutionary changes in many other fields such as education,\nfinance and healthcare. As a subfield of generative artificial\nintelligence (GenAI), researchers have started exploring LLM\napplications in wireless networks. For instance, Lin et al.\ninvestigated the LLM deployment at the network edge in [11],\nand overviewed several key techniques for efficient edge model\ntraining and inference. Xu et al. discussed some topics to\nintegrate LLMs into wireless networks, including end-edge-\ncloud collaboration, integrated sensing and communication,\nand digital twin techniques [12]. Similarly, Shen et al. also\nintroduced the LLM-enabled edge AI, indicating that LLMs\nin the central cloud can process the requests from network\nedge servers [13]. These useful studies have demonstrated the\ngreat potential of LLM-empowered wireless networks.\nThis work is different from existing studies [1]–[3], [9],\n[10] by introducing a comprehensive overview of various RL\nalgorithms and discussions on LLM-aided optimization tech-\nniques for RIS-aided 6G networks. In particular, we discuss\nhow LLMs can be combined with RL schemes to improve\nthe generalization capabilities, reward function design, multi-\ntask handling, etc. LLM-empowered RL techniques offer new\nopportunities for more efficient and accessible optimization\ntechnologies for 6G networks.\nIII. AN OVERVIEW OF RIS-RELATED OPTIMIZATION\nPROBLEMS\nThis section will first overview the RIS-related optimization\nproblems in terms of problem formulations, and then we\nwill analyze the problem convexity and complexity. Fig. 1\npresents a RIS-aided downlink transmission system with N\nRIS reflecting units, one base station (BS) with M antennas,\nand U single-antenna users. The RIS units provide an extra\ntransmission path, indicating that users can receive signals by\ntwo links, direct link BS-user and RIS-aided link BS-RIS-user.\nWith RISs, the signal-to-interference-plus-noise ratio (SINR)\nof user u in a multiple input single output (MISO) system is:\nΓu =\n|(hRIS\nu\nΘG + hD\nu )pu|2\nUP\nj=1,j̸=u\n|(hRIS\nu\nΘG + hD\nu )pj|2 + N 2\n0\n,\n(1)\n2\nwhere hD\nu ∈C1×M represents the channel gain of a direct\nlink from the BS to the user u, and the RIS-aided link is\nindicated by hRIS\nu\nΘG. In particular, hRIS\nu\n∈C1×N indicates\nthe channel gain from RIS elements to user u, G ∈CN×M\nrepresents the channel gain from BS antennas to RIS elements,\nand RIS units are defined by Θ = diag(θ1, θ2, ..., θn, ..., θN) ∈\nCN×N. Here each RIS unit has a specific phase-shift vector\nθn = ejϕn. Finally, pu and pj are the BS antenna transmit\npower for users, and N 2\n0 shows the transmission noise.\nEquation (1) reveals that the phase shifts of RIS ele-\nments will affect the SINR Γu of users. After that, vari-\nous problem formulations can be defined, e.g., maximizing\nsum rate/channel capacity, signal coverage, energy efficiency,\nnetwork fairness, and secrecy rate, or minimizing the power\nconsumption, which has been investigated in many existing\nstudies [2]. For example, sum-rate maximization problem is\nusually formulated as:\nmax\np,Θ\nU\nX\nu=1\nwu log(1 + Γu)\n(2)\ns.t.\nU\nX\nu=1\n||pu||2 ≤Pmax,\n(2a)\n|θn| = 1, n = 1, 2, ..., N,\n(2b)\nwhere wu is the pre-defined weight of user u, and Pmax is the\nBS antenna’s maximum transmission power. The objective in\nequation (2) is to maximize the sum-rate of all users under\nthe maximum transmission power constraint (2a) and RIS\noperation constraint (2b). The optimization problem defined\nin equation (2) is obviously highly non-convex and non-linear.\nSpecifically, it first involves multiple logarithm terms, and each\nitem includes a fractional term Γu. Meanwhile, the constraint\n(2b) is related to the 2-norm of value θn. In practice, the\nphase shift of RIS elements may be constrained by discrete\nvalues with θn ∈{0, 2π\n2ϱ , ..., (2ϱ −1) 2π\n2ϱ , 2π}, where ϱ is the\nresolution of RIS elements. This is a more practical setting\nby using discrete phase-shifts to replace the continuous RIS\nphase-shift control. But it will also lead to MINLP problems,\nwhich are very hard to solve efficiently by using conventional\nconvex optimization techniques. On the other hand, the control\nvariables p and Θ mean a large solution space, since the\nRISs may consist of hundreds of small units. Moreover, RIS\ntechnology is usually integrated into other techniques, such as\nNOMA, V2X, UAV, etc. These combinations will introduce\nother control variables, e.g., user decoding order in NOMA\nand UAV position and power control, which will further\nincrease the difficulties of finding optimal solutions.\nIn summary, the above examples and analyses demonstrate\nthat optimizing RIS-related problems can be extremely dif-\nficult, especially in dynamic wireless environments. To this\nend, ML techniques, especially RL, offer promising solutions\nto improve RIS-aided network performance, which will be\nintroduced in the following section.\nIV. REINFORCEMENT LEARNING-BASED OPTIMIZATION\nThis section will introduce various RL algorithms to\nsolve RIS-related optimization problems. It first presents\nRL fundamentals, especially MDP definitions, and then dis-\ncusses various RL techniques, including Q-learning, deep\nQ-learning (DQN) and variants, multi-agent reinforcement\nlearning (MARL), TRL, hierarchical reinforcement learning\n(HRL), and offline reinforcement learning (offline-RL).\nA. Reinforcement Learning Fundamentals\nDefining MDPs is the prerequisite for implementing RL\nalgorithms, which consist of a tuple < s, a, r, T\n> with\nstates s, actions a, rewards r, and transition probability T.\nIn particular, one agent can select one action a based on\nthe current state s, receive an instant reward r from the\nenvironment, and then move to the next state s′. The agent\naims to maximize the long-term accumulated reward by trying\ndifferent action combinations and balancing the exploration\nand exploitation policies. Proper MDP definition is crucial for\nusing RL, since the states, actions, and rewards are closely\nrelated to the objectives, control variables, and constraints in\nRIS-related optimization problems.\n• State definitions: The state should reflect the environ-\nment status, and then the agent can make decisions\naccordingly. For instance, it is obvious that RIS phase-\nshift control should consider the channel condition, and\nthen channel state information is frequently defined as\nthe state in many existing studies [2]. Other than that,\ncurrent RIS unit phases and positions, energy level, and\nprevious data rate can also contribute to state definitions.\n• Action definitions: The actions in an MDP are usually\nequivalent to the control variables in optimization prob-\nlems. For example, RIS phase shifts and BS transmission\npower in the equation (2) should be considered as ac-\ntions, since these decisions will affect the reward. The\naction space may extend when other control variables\nare involved, e.g., RIS location optimization and elements\non/off control.\n• Reward definitions: The reward definition is closely\nrelated to the optimization objective. Maximizing the\nlong-term reward indicates improving the corresponding\nobjective function iteratively. Therefore, the reward in\nRIS-related optimization problems usually depends on\nspecific objectives, e.g., sum-rate, energy efficiency, se-\ncrecy rate, signal coverage, etc.\nWith the above MDP definitions, a standard optimization\nproblem as shown in equation (2) can be transformed into\na unified scheme. Then various RL algorithms can be applied.\nB. From Q-learning to Deep Q-learning and Variants\nQ-learning is the most fundamental RL algorithm that has\nbeen extensively investigated, which employs a Q-table to\nrecord all the state-action values. However, such a tabular-\nbased approach cannot handle problems with large state-action\nspaces, indicating many iterations to converge the state-action\nvalues. To this end, DQN is proposed by using neural networks\n3\nFig. 2. Overview of various reinforcement learning techniques.\nto predict the state-action values. Two useful techniques in\nDQN are experience relay and double networks. In particular,\nas illustrated in Fig.2 (b), experience reply means collecting\nprevious experience in terms of MDP tuples < s, a, r, s′ >,\nwhich will be stored in the experience pool and then sampled\nfor neural network training. Meanwhile, double networks\ninvolve the main network to predict the current state-action\nvalues, and the target network to provide a stable target state-\naction value for main network training. Based on the DQN\nframework, many other variants have been proposed to further\nimprove the algorithm performance, including deep actor-\ncritic, deep deterministic policy gradient (DDPG), double\ndeep Q-learning (DDQN), twin delayed DDPG (TD3), etc.\nFor instance, DDPG can handle problems with continuous\naction spaces, which is particularly useful for RIS phase-shift\noptimization problems for continuous phase control.\nC. Multi-agent Reinforcement Learning (MARL)\nMulti-agent problems are very common in RIS-aided wire-\nless networks. For instance, RISs with different geographical\nlocations can be considered as different agents, and each agent\nhas its own objectives, e.g., maximizing the sum-rate of a\nmicrocell or minimizing the transmission power consumption\nof the serviced area. In particular, a multi-agent system in-\ndicates that each agent can make decisions autonomously to\nimprove its objective, which depends on the environment and\nalso the actions of other agents [4]. Therefore, the core of\na multi-agent system is to coordinate the policies of each\nlocal agent, aiming to optimize the overall performance of the\nwhole system. A straightforward coordination approach is to\ndefine a unified reward function to represent the overall system\nperformance, and then all local agents will select actions to\nprioritize the performance of the whole system, e.g., maximize\nthe data rate of the whole macrocell. Other than that, nash\nequilibrium is also a useful method to solve MARL problems,\nwhich means that no agent has anything to gain by changing\nonly one agent’s policies. In addition, other techniques such\nas correlated equilibrium and matching theory, can also be\ncombined with MARL to solve multi-agent problems in RIS-\naided wireless networks.\nD. Transfer Reinforcement Learning (TRL)\nTransfer learning provides an efficient approach to reuse the\nexperience on previous tasks, and then transfers the knowledge\nto current target tasks [14]. Specifically, as shown in Fig.2\n(d), the decision-making of conventional RL DRL can be\ndefined as DRL : s × K\na\n=⇒< r, s′ >. It means that the\nagent selects actions a under current state s by using its\nknowledge K, and then receives a reward r and moves to\nthe next state s′. By contrast, TRL decision-making DT RL\n4\nTABLE I\nSUMMARY OF RL TECHNIQUES FOR RIS-AIDED 6G NETWORKS.\nRL\ntechniques\nMain features\nand advantages\nPotential\ndifficulties\nRIS-aided 6G network\napplications\nDQN and\nvariants\nDQN provided the fundamental framework for many\ndeep reinforcement learning techniques, which is also\nthe most widely used RL techniques. There have\nbeen many variants based on the DQN framework,\ne.g., deep actor-critic, deep deterministic policy\ngradient (DDPG), double deep Q-learning, and twin\ndelayed DDPG. Each method has its unique\nadvantages such as handling continuous action spaces\nand overcoming over-estimation.\nA common problem of conventional\nDQN is the long training time, which\nmeans many iterations to produce a\nstable model. Meanwhile, another\nwell-known problem is the\nhyperparameter selection and\nfine-tuning, e.g., neural network\nlearning rate and number of hidden\nlayers.\nDDPG is particularly useful for\nsolving RIS phase-shift optimization\nproblems, since the action space is\ncontinuous for phase shift control.\nMeanwhile, DQN can also be used\nfor discrete RIS phase-shift\noptimization problems if\nθn ∈{0, 2π\n2ϱ , ..., (2ϱ −1) 2π\n2ϱ , 2π}.\nMulti-agent\nreinforcement\nlearning\nMARL focuses on problems with multiple elements\nand various objectives. In particular, each agent\nshould be able to make decisions autonomously,\naiming to improve its objective by interacting with\nthe environment and other agents. It provides a\nreasonable approach to modelling complicated\nrelationships between multiple agents.\nThe main difficulty lies in the\ncoordination mechanism design, since\nthe action of one agent will affect\nboth the environment and other\nagents. The corresponding techniques\ninclude unified reward function, nash\nequilibrium, correlated equilibrium,\netc.\nMARL can be applied to RIS-aided\nwireless networks when multiple\nnetwork elements are involved, e.g.,\nmultiple RISs, network slices, and\nunmanned aerial vehicles (UAVs).\nMARL can jointly consider the\nservice requirements of these\nelements for coordination.\nTransfer\nreinforcement\nlearning\nTransfer learning indicates an efficient approach to\nprocess a series of similar tasks. By reusing previous\nknowledge on former tasks, transfer learning enables\nshorter training iterations and faster convergence than\nconventional RL.\nThe main difficulty of transfer\nreinforcement learning lies in the\ndefinition of mapping functions. The\nreason is that knowledge may exist in\nvarious forms in the RL scheme, and\nthe mapping function must transform\nexisting knowledge to make it\naccessible for learner agents.\nTransfer learning aligns well with the\ninherent wireless communication\nfeatures to enable faster response\ntime. Based on existing experience in\nRIS phase-shift optimization, transfer\nlearning can be applied to more\ncomplicated scenarios in which RISs\nare combined with other techniques,\nsuch as RIS-UAVs and RIS-NOMA.\nHierarchical\nreinforcement\nlearning\nHRL introduces hierarchical architecture to\nconventional RL schemes, allowing for hierarchical\ndecision-making by multiple control layers. It usually\nincludes one meta-controller to produce high-level\npolicies, and then lower-level sub-controllers can\nmake specific decisions based on environment states\nand high-level policies.\nThe hierarchy architecture increases\nsystem flexibility, but it also leads to\nconcerns about the system stabilities.\nIn particular, the main difficulty is\nhow to guarantee the quality of the\nlong-term policies. This is crucial\nsince it will further affect the actions\nof the sub-controller.\nHRL allows for multiple decision\nvariables with different time scales,\nwhich is very common in RIS-aided\nwireless networks. For instance, the\npower consumption level is usually a\nlong-term performance metric, while\nlatency and sum-rate represent instant\nmetrics. HRL scheme may be used to\njointly optimize these indicators.\nOffline\nreinforcement\nlearning\nOffline RL offers a practical and realistic technique\nto train RL agents without interfering with the real\nenvironment. It uses offline datasets to train the RL\nagent, overcoming the cost and risks of training a\nnew agent from scratch in real-world scenarios.\nThe performance of offline RL\nalgorithm is closely related to the\ndataset quality. To be specific, the RL\nagent can learn good decision-making\nstrategies from datasets generated by\nhigh-quality policies. By contrast,\nrandomly generated datasets may lead\nto poor learning performance.\nOffline-RL is a promising technique\nfor RIS-aided 6G networks. It enables\noffline model training and learning\nwithout touching the real wireless\nnetwork environment, which is much\nmore practical than many existing\nonline algorithms.\nis: DT RL : s × M(Kexpert) × K\na\n=⇒< r, s′ >. Compared\nwith conventional RL, the main difference lies in M(Kexpert).\nKexpert indicates the existing knowledge that is achieved by\nprevious tasks of the expert agent. However, it is worth noting\nthat Kexpert may exist in various forms, e.g., neural net-\nwork weights, state-action values, or RIS phase-shift control\ndecisions. Therefore, a mapping function M is needed to\ntransform the expert knowledge into a more accessible format\nfor the learner agent. TRL can be a useful technique for RIS-\naided 6G networks to shorten the algorithm training, which is\nconsidered one of the bottlenecks of applying ML techniques\nto real-world applications. In addition, it also enables faster\nresponse time to network dynamics, and better adapts to\nchanging wireless environments.\nE. Hierarchical Reinforcement Learning (HRL)\nHRL introduces hierarchies into the standard RL agent.\nAs illustrated in Fig.2 (e), HRL includes a meta-controller\nand a sub-controller. Both controllers can receive rewards and\nstates from the environment, while the meta-controller can\nsend policy instructions to the sub-controller. To be specific,\nthe MDP of the meta-controller is < s, p, r, T >, and here\nwe use p to replace the action a in conventional MDP, which\nindicates the policies made by the meta-controller based on\ncurrent state s [15]. Then, the MDP of the sub-controller is\n< s, p, a, r, T >, which means that the action is selected based\non both current state s and policy p from the meta-controller:\nDRL : s × p × K\na\n=⇒< r, s′ >. HRL can be used to solve\nproblems with multiple decision variables that have different\ntimescales. For instance, Zhou et al. applied HRL to combine\n5\nBS sleep control with RIS phase-shift optimization, in which\nthe meta-controller decides the BS on/off status to save long-\nterm energy consumption, and the sub-controller optimizes the\nRIS phase shifts to improve the data rate [16].\nF. Offline Reinforcement Learning (Offline RL)\nConventional RL schemes improve policy performance\nthrough interactions with the environment to collect rewards\nand feedback [17]. However, such an online approach may be\nimpractical for many real-world scenarios, e.g., autonomous\ndriving or healthcare applications. In contrast, offline RL trains\nthe RL agent using offline datasets without direct interactions\nwith the real-world environment. This approach can be more\nrealistic and practical for real-world applications, due to the\nadvantages of learning from pre-collected data without the lim-\nitations of real-time constraints. For instance, training an RL\nagent directly in realistic RIS-aided 6G networks is impractical\ndue to the critical importance of telecommunication services.\nBuilding a specific simulator for the training can also be\ntime-consuming. To this end, offline RL provides a promising\nsolution by training the agent using existing datasets collected\nfrom the environment. However, it is worth noting that the\nperformance and effectiveness of offline RL depend on the\nquality of offline datasets used for training, as the agent cannot\naccess the realistic wireless environment. Hence, ensuring that\nthese datasets are comprehensive and representative of various\nscenarios is crucial for achieving robust performance of offline\nRL.\nV. LLM-ENABLED OPTIMIZATION TECHNIQUES\nAlthough RL algorithms can solve a range of RIS-related\noptimization problems, these techniques commonly encounter\nsome potential challenges, such as difficulties in generaliz-\ning to unseen environments. TRL and offline RL techniques\ncan improve generalization to a certain extent, but they still\nrequire significant model training to adapt to new environ-\nments. Moreover, designing reward functions in RL typically\nnecessitates specialized domain knowledge for target tasks,\nand conventional trial-and-error design methods can be time-\nconsuming. To address these limitations, LLMs have shown\nseveral promising features, which may be combined with RL\nalgorithms:\n• Instruction following: One core capability of LLMs\nis to follow natural language instructions, which im-\nproves the generalization ability across a wide range\nof language-related tasks [18]. The common format of\nnatural language instructions includes a task description,\nan optional input, a corresponding output, and several\nexamples as demonstrations. By constructing instructions\nin this manner, LLMs can be fine-tuned using supervised\nsequence-to-sequence training loss, which aids in enhanc-\ning language comprehension across multiple languages.\nHence, LLMs can follow a broader range of instructions,\nthereby enhancing their adaptability and generalization in\nmulti-lingual contexts.\n• Multi-modal learning: Multi-modal learning is a no-\ntable capability of LLMs, enabling the model to process\nrelated information from multiple modalities, including\ntext, audio, image, 3D maps, etc [19]. A multi-modal\nLLM can employ various encoders to extract features\nfrom multi-modal inputs into desired outputs, providing\na broader perspective and greater adaptability when pro-\ncessing multi-source information.\n• In-context learning: In-context learning utilizes format-\nted natural language prompts along with task descrip-\ntions to guide LLMs in recognizing and performing new\ntasks based on understanding contextual information [7].\nSpecifically, LLMs identify the task type by interpreting\nthe provided prompts and task descriptions, drawing on\npre-existing knowledge from their pre-training data [20].\nAfter task identification, LLMs employ the included\ndemonstrations to learn solution strategies, enabling effi-\ncient implementation of new tasks.\n• Step-by-step reasoning: LLMs exhibit strong perfor-\nmance in complex multi-step tasks through advanced\nprompting strategies, including chain-of-thought (CoT)\nprompting, tree-of-thought (ToT) prompting, graph-of-\nthought (GoT) prompting, etc. These strategies organize\nthe problem-solving process into sequential or hierarchi-\ncal steps, providing a more structured and comprehen-\nsive reasoning pathway. For instance, CoT prompting\naugments demonstrations into a sequence of reasoning\nsteps [21], which allows LLMs to solve complex prob-\nlems with greater transparency and logic.\nThe above capabilities indicate great potential for inte-\ngrating LLMs into RL schemes, including better general-\nization capabilities, automated reward design, multi-modality\ninformation processing, multi-task handling, and LLM-aided\nreasoning. In the following, we will introduce the potential\nbenefits in detail.\n• Improving generalization capabilities: LLMs have been\npre-trained by a large number of real-world datasets,\nwhich means that the system model can handle many re-\nalistic problems in a zero-shot manner. LLM’s real-world\nknowledge may complement the RL’s generalization ca-\npabilities to better adapt to unseen environments. For in-\nstance, LLM may provide prior knowledge for RL agents\nin a transfer learning manner, and then the algorithm\ncan quickly understand new environments, achieving a\njump-start with a very small number of iterations. Such\ngeneralization improvement is crucial for the real-world\napplications of RL algorithms, especially considering the\ncomplicated RIS-aided wireless environments.\n• Automated reward function design: In addition, with\nproper prompt designs, existing studies have shown that\nLLMs can be used to generate reward functions for\nRL algorithms, achieving comparable performance with\nmanually designed rewards [22]. For example, wireless\nnetwork performance may involve multiple indicators,\nsuch as latency, throughput, and packet drop rate, and\n6\nbalancing these metrics manually in a comprehensive\nreward design can be difficult. With proper instructions,\nLLM can automate the reward function design, balancing\nmultiple network indicators based on human preference\nand system feedback. Such a technique can significantly\nsave human efforts on reward function design, enabling\nautomated RL schemes for RIS-aided 6G.\n• Multi-modality information processing for RL: Multi-\nmodality refers to the capability of processing related\ninformation with different modalities, such as text, image,\naudio, video, and graphs. Combining multi-modal LLMs\nwith RL algorithms will improve RL agents’ comprehen-\nsion of complicated scenarios with diverse information\nsources. For instance, integrated sensing and commu-\nnication has become an important technique for future\n6G networks, and LLM-aided RL can make the most of\nsensing information, e.g., audio, images, and 3D maps, to\nbetter capture environment dynamics. Such comprehen-\nsive multi-modal input will improve the decision-making\nquality of RL agents, e.g., using environment images as\ninput for RIS-aided beamforming.\n• Multi-task handling: Conventional RL algorithms usu-\nally implement one single task, while LLMs can easily\nhandle multiple downstream tasks simultaneously using\nbillions of parameters. It means one single LLM can serve\nmultiple RL agents with different purposes, enabling\nefficient RL model training and fine-tuning on diverse\ntarget tasks. Multi-task handling can improve the RL\nsystem efficiency, processing a series of tasks efficiently\nwith the assistance of LLMs, e.g., RIS-aided resource\nallocation, RIS elements on/off control and elements\nphase-shift optimization, etc.\n• LLM-enabled reasoning for RL Despite the great per-\nformance in handling optimization problems, existing RL\ntechniques lack reasoning capabilities. In particular, it\nmeans that the RL agent cannot provide reasons behind\nthe selected decisions. For instance, after dedicated model\ntraining, the RL algorithm may decide to decrease the BS\ntransmission power to achieve higher energy efficiency\nin RIS-aided wireless networks. However, the agent is\nunable to provide the intellectual oversight behind this\ndecision, since decreasing the transmission power may\nalso lower the data rate. Integrating LLMs into the\nRL scheme may provide explanations for the optimal\npolicies selected by RL algorithms, and humans can better\nunderstand the insight of optimizing RIS-aided wireless\nnetworks.\nFinally, Fig.3 presents a scheme of using LLM as an agent to\nsolve optimization problems in RIS-aided wireless networks.\nThe LLM will first select an action, and receive new states and\nrewards from the RIS-aided wireless network environment.\nThen, the evaluator can provide instant feedback to LLM to\nevaluate the previous actions, e.g., ”The last selected action\nis bad because the data rate is too low, and you need to\nprovide another solution to achieve a higher data rate”.\nFig. 3. LLM as an agent for RIS-aided networks.\nOn the other hand, the evaluator should send some possible\nexperience to the experience pool, which will accumulate long-\nterm experience for the LLM agent reference, e.g., ”Users\nwith low-latency requirements should be prioritized in the\nservice queue”. Combining instant feedback with long-term\nexperience shows a similar design as an instant reward and\nexperience pool in DQN algorithms.\nVI. CHALLENGES AND FUTURE DIRECTIONS\nThis section will discuss the challenges and future di-\nrections of ML-enabled optimization for RIS-aided 6G net-\nworks, including practical RIS optimization techniques, wire-\nless domain-specific LLM training, and LLM’s demand for\ncomputational resources.\n• Practical RIS optimization techniques: A large number\nof techniques have been proposed to optimize RIS-aided\nwireless networks, e.g., convex optimization, heuristic\nalgorithms, and ML methods. However, most existing\ntechniques rely on ideal assumptions for the wireless\nenvironment, e.g., static and evenly distributed users,\nperfect channel state information acquisition, and in-\nstant information sharing and processing. In real-world\napplications, RIS control may face more difficult and\ncomplicated environments, e.g., limited computational\nresources, information-sharing delays between RISs and\nBSs, imperfect channel state information acquisition, etc.\nSolving these problems will further facilitate the real-\nworld applications of RIS technologies.\n• Wireless\ndomain-specific\nLLM\ntraining: Previous\nsections have discussed the great potential of inte-\ngrating LLM-aided RL into RIS-aided wireless net-\nworks. However, some wireless-specific problems may\nrequire professional knowledge and understanding of net-\nwork architecture, channel modelling, signal processing,\nand so on. Most existing LLM models are designed\n7\nfor general-domain purposes, lacking wireless domain-\nspecific knowledge. Therefore, a practical approach is to\nfine-tune a general-domain LLM such ChatGPT, LLaMA,\nand Google Gemini, to adapt to wireless service requests.\nSuch a wireless-LLM may bring revolutionary changes to\nthe operation of RIS-aided 6G networks.\n• LLMs are computationally demanding: Although pre-\nvious sections have discussed the advantages of LLM-\nenabled optimization techniques, there remain significant\nchallenges in deploying LLMs into RIS-aided 6G net-\nworks due to intensive computational resource require-\nments. For instance, LLM inference time can significantly\ncontribute to system latency, with response time ranging\nfrom 0.58 to 90 seconds [23], which can be a major obsta-\ncle to latency-critical applications. Furthermore, the com-\nplex architectures of LLMs generally require extensive\ncomputational requirements, leading to a misalignment\nwith the limited computational resources and storage\ncapacity of network devices. Hence, These challenges\npresent considerable obstacles to the scalability and fea-\nsibility of deploying LLMs in resource-constrained envi-\nronments, highlighting the need for further exploration to\ndeploy and optimize LLMs into real-world 6G networks.\nVII. CONCLUSION\nThis work provides a comprehensive overview of various\nRL techniques to solve RIS-related optimization problems,\nincluding Q-learning, DQN and variants, MARL, TRL, HRL,\nand offline RL. LLMs have demonstrated remarkable ca-\npabilities in many real-world applications, and this paper\nfurther explores the potential of LLM-enabled optimization\ntechniques by integrating LLMs with various RL techniques,\naiming to enhance the performance in RIS-aided 6G networks.\nFinally, the potential challenges of LLM-enabled optimization\ntechniques suggest future directions in several areas, including\ndeveloping advanced RIS optimization techniques, creating\nwireless domain-specific LLMs, and deploying computation-\nally efficient LLMs.\nREFERENCES\n[1] Y. Liu, X. Liu, X. Mu, T. Hou, J. Xu, M. D. Renzo, and N. Al-Dhahir,\n“Reconfigurable intelligent surfaces: Principles and opportunities,” IEEE\nCommun. Surveys Tuts., vol. 23, no. 3, pp. 1546–1577, 3rd quarter 2021.\n[2] H. Zhou, M. Erol-Kantarci, Y. Liu, and H. V. Poor, “A survey on model-\nbased, heuristic, and machine learning optimization approaches in RIS-\naided wireless networks,” IEEE Communications Surveys & Tutorials\n(Early Access), Dec. 2023.\n[3] K. Faisal and W. Choi, “Machine learning approaches for reconfigurable\nintelligent surfaces: A survey,” IEEE Access, vol. 10, pp. 27 343–27 367,\nJul. 2022.\n[4] H. Zhou, M. Elsayed, and M. Erol-Kantarci, “Ran resource slicing\nin 5G using multi-agent correlated Q-learning,” in 2021 IEEE 32nd\nAnnual International Symposium on Personal, Indoor and Mobile Radio\nCommunications (PIMRC), 2021, pp. 1179–1184.\n[5] H. Zhou, M. Erol-Kantarci, and V. Poor, “Knowledge transfer and reuse:\nA case study of AI-enabled resource management in RAN slicing,” IEEE\nWireless Commun., pp. 1–10, Dec. 2022.\n[6] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, et al., “A survey of large\nlanguage models,” arXiv:2303.18223, 2023.\n[7] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,\nA. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., “Language models\nare few-shot learners,” Advances in Neural Information Processing\nSystems, vol. 33, pp. 1877–1901, 2020.\n[8] N. Shinn, F. Cassano, A. Gopinath, K. Narasimhan, and S. Yao, “Re-\nflexion: Language agents with verbal reinforcement learning,” Advances\nin Neural Information Processing Systems, vol. 36, 2024.\n[9] H. Zhou, M. Erol-Kantarci, Y. Liu, and H. V. Poor, “Heuristic algorithms\nfor RIS-assisted wireless networks: Exploring heuristic-aided machine\nlearning,” IEEE Wireless Communications (Early Access), Apr. 2024.\n[10] A. A. Puspitasari and B. M. Lee, “A survey on reinforcement learning for\nreconfigurable intelligent surfaces in wireless communications,” Sensors,\nvol. 23, no. 5, p. 2554, 2023.\n[11] Z. Lin, G. Qu, Q. Chen, X. Chen, Z. Chen, and K. Huang, “Pushing large\nlanguage models to the 6G edge: Vision, challenges, and opportunities,”\narXiv:2309.16739, 2023.\n[12] M. Xu, N. Dusit, J. Kang, Z. Xiong, S. Mao, Z. Han, D. I. Kim, and\nK. B. Letaief, “When large language model agents meet 6G networks:\nPerception, grounding, and alignment,” arXiv:2401.07764, 2024.\n[13] Y. Shen, J. Shao, X. Zhang, Z. Lin, H. Pan, D. Li, J. Zhang, and K. B.\nLetaief, “Large language models empowered autonomous edge AI for\nconnected intelligence,” IEEE Communications Magazine, 2024.\n[14] H. Zhou, M. Erol-Kantarci, and H. V. Poor, “Learning from peers: Deep\ntransfer reinforcement learning for joint radio and cache resource alloca-\ntion in 5g ran slicing,” IEEE Transactions on Cognitive Communications\nand Networking, vol. 8, no. 4, pp. 1925–1941, 2022.\n[15] H. Zhou, M. Elsayed, M. Bavand, R. Gaigalas, S. Furr, and M. Erol-\nKantarci, “Cooperative hierarchical deep reinforcement learning based\njoint sleep, power, and RIS control for energy-efficient HetNet,” arXiv\npreprint arXiv:2304.13226, 2023.\n[16] H. Zhou, L. Kong, M. Elsayed, M. Bavand, R. Gaigalas, S. Furr, and\nM. Erol-Kantarci, “Hierarchical reinforcement learning for RIS-assisted\nenergy-efficient RAN,” in GLOBECOM 2022-2022 IEEE Global Com-\nmunications Conference, 2022, pp. 3326–3331.\n[17] S. Levine, A. Kumar, G. Tucker, and J. Fu, “Offline reinforcement\nlearning: Tutorial, review, and perspectives on open problems,” arXiv\npreprint arXiv:2005.01643, 2020.\n[18] J. Wei, M. Bosma, V. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M.\nDai, and Q. V. Le, “Finetuned language models are zero-shot learners,”\nin International Conference on Learning Representations, 2021.\n[19] Y. Zhang, K. Gong, K. Zhang, H. Li, Y. Qiao, W. Ouyang, and X. Yue,\n“Meta-transformer: A unified framework for multimodal learning,” arXiv\npreprint arXiv:2307.10802, 2023.\n[20] N. Wies, Y. Levine, and A. Shashua, “The learnability of in-context\nlearning,” Advances in Neural Information Processing Systems, vol. 36,\n2024.\n[21] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le,\nD. Zhou et al., “Chain-of-thought prompting elicits reasoning in large\nlanguage models,” Advances in neural information processing systems,\nvol. 35, pp. 24 824–24 837, 2022.\n[22] Y. J. Ma, W. Liang, G. Wang, D.-A. Huang, O. Bastani, D. Jayaraman,\nY. Zhu, L. Fan, and A. Anandkumar, “Eureka: Human-level reward\ndesign via coding large language models,” arXiv:2310.12931, 2023.\n[23] G. Charan, M. Alrabeiah, and A. Alkhateeb, “Vision-aided 6G wireless\ncommunications: Blockage prediction and proactive handoff,” IEEE\nTransactions on Vehicular Technology, vol. 70, no. 10, pp. 10 193–\n10 208, 2021.\n8\n",
  "categories": [
    "cs.NI",
    "cs.LG",
    "cs.SY",
    "eess.SY"
  ],
  "published": "2024-05-09",
  "updated": "2024-09-17"
}