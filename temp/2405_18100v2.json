{
  "id": "http://arxiv.org/abs/2405.18100v2",
  "title": "A Pontryagin Perspective on Reinforcement Learning",
  "authors": [
    "Onno Eberhard",
    "Claire Vernade",
    "Michael Muehlebach"
  ],
  "abstract": "Reinforcement learning has traditionally focused on learning state-dependent\npolicies to solve optimal control problems in a closed-loop fashion. In this\nwork, we introduce the paradigm of open-loop reinforcement learning where a\nfixed action sequence is learned instead. We present three new algorithms: one\nrobust model-based method and two sample-efficient model-free methods. Rather\nthan basing our algorithms on Bellman's equation from dynamic programming, our\nwork builds on Pontryagin's principle from the theory of open-loop optimal\ncontrol. We provide convergence guarantees and evaluate all methods empirically\non a pendulum swing-up task, as well as on two high-dimensional MuJoCo tasks,\nsignificantly outperforming existing baselines.",
  "text": "A Pontryagin Perspective on Reinforcement Learning\nOnno Eberhard12\nONNO.EBERHARD@TUE.MPG.DE\nClaire Vernade2\nCLAIRE.VERNADE@UNI-TUEBINGEN.DE\nMichael Muehlebach1\nMICHAEL.MUEHLEBACH@TUE.MPG.DE\n1Max Planck Institute for Intelligent Systems, Tübingen, Germany\n2University of Tübingen\nAbstract\nReinforcement learning has traditionally focused on learning state-dependent policies to solve opti-\nmal control problems in a closed-loop fashion. In this work, we introduce the paradigm of open-loop\nreinforcement learning where a fixed action sequence is learned instead. We present three new\nalgorithms: one robust model-based method and two sample-efficient model-free methods. Rather\nthan basing our algorithms on Bellman’s equation from dynamic programming, our work builds\non Pontryagin’s principle from the theory of open-loop optimal control. We provide convergence\nguarantees and evaluate all methods empirically on a pendulum swing-up task, as well as on two\nhigh-dimensional MuJoCo tasks, significantly outperforming existing baselines.\nKeywords: Reinforcement learning, Open-loop control\n1. Introduction\nπ( · | xt)\nClosed-loop control\nf(xt, ut)\nut\nxt+1\nxt\nxt\nu0:T−1\nOpen-loop control\nf(xt, ut)\nut\nxt+1\nxt\nFigure 1: Comparison of closed-loop (feedback) and open-\nloop (feedforward) control. In closed-loop reinforcement\nlearning (RL), the goal is to learn a policy (π). In open-loop\nRL, a fixed sequence of actions (u0:T−1) is learned instead,\nwith the action ut independent of the states x0:t.\nReinforcement learning (RL) refers to\n“the optimal control of incompletely-\nknown Markov decision processes”\n(Sutton and Barto, 2018, p. 2). It has\ntraditionally focused on applying dy-\nnamic programming algorithms, such\nas value iteration or policy iteration,\nto situations where the environment is\nunknown. These methods solve opti-\nmal control problems in a closed-loop\nfashion by learning feedback policies,\nwhich map states xt to actions ut.\nIn contrast, this work introduces the\nparadigm of open-loop reinforcement\nlearning (OLRL), in which fixed ac-\ntion sequences u0:T−1, over a horizon\nT, are learned instead. The closed-loop and open-loop control paradigms are illustrated in Fig. 1.\nAn open-loop controller receives no observations from its environment. This makes it impossible\nto react to unpredictable events, which is essential in many problems, particularly those with stochastic\nor unstable dynamics. For this reason, RL research has historically focused exclusively on closed-\nloop control. However, many environments are perfectly predictable. Consider the classic example of\nswinging up an inverted pendulum. If there are no disturbances, then this task can be solved flawlessly\nwithout feedback (as we demonstrate in Section 4.1). Where open-loop control is viable, it brings\n© 2024 O. Eberhard, C. Vernade & M. Muehlebach.\narXiv:2405.18100v2  [cs.LG]  28 Nov 2024\nEBERHARD VERNADE MUEHLEBACH\nconsiderable benefits. As there is no need for sensors, it is generally much cheaper than closed-loop\ncontrol. It can also operate at much higher frequencies, since there is no bandwidth bottleneck due\nto sensor delays or computational processing of measurements. Importantly, the open-loop optimal\ncontrol problem is much simpler, as it only involves optimizing an action sequence (finding one\naction per time step). In contrast, closed-loop optimal control involves optimizing a policy (finding\none action for each state of the system), which can be considerably more expensive. In this way,\nopen-loop control circumvents the curse of dimensionality without requiring function approximation.\nFor these reasons, open-loop control is widely used in practice (Diehl et al., 2006; van Zundert\nand Oomen, 2018; Sferrazza et al., 2020), and there exists a large body of literature on the theory\nof open-loop optimal control (Pontryagin et al., 1962). However, the setting of incompletely-known\ndynamics has received only little attention. In this work, we introduce a family of three new open-\nloop RL algorithms by adapting the existing theory to this setting. Whereas closed-loop RL is largely\nbased on approximating the Bellman equation, the central equation of dynamic programming, we\nbase our algorithms on approximations of Pontryagin’s principle, the central equation of open-loop\noptimal control. We first introduce a model-based method whose convergence we prove to be robust\nto modeling errors. This is a novel and non-standard result which depends on a careful analysis of the\nalgorithm. We then extend this procedure to settings with completely unknown dynamics and propose\ntwo fully online model-free methods. Finally, we empirically demonstrate the robustness and sample\nefficiency of our methods on an inverted pendulum swing-up task and on two complex MuJoCo tasks.\nRelated work. Our work is inspired by numerical optimal control theory (Betts, 2010; Geering,\n2007), which deals with the numerical solution of trajectory optimization problems. Whereas existing\nmethods assume that the dynamics are known, our algorithms only require an approximate model\n(model-based OLRL) or no model at all (model-free OLRL), and rely on a simulator to provide\nsamples. An in-depth review of related work can be found in Appendix A.\n2. Background\nWe consider a reinforcement learning setup with continuous state and action spaces X ⊂RD and\nU ⊂RK. Each episode lasts T steps, starts in the fixed initial state x0, and follows the deterministic\ndynamics f : X × U →X, such that xt+1 = f(xt, ut) for all times t ∈[T −1]0.1 After every\ntransition, a deterministic reward r(xt, ut) ∈R is received, and at the end of an episode, an additional\nterminal reward rT (xT ) ∈R is computed. The value of state xt at time t is the sum of future rewards\nvt(xt; ut:T−1) .=\nT−1\nX\nτ=t\nr(xτ, uτ) + rT (xT ) = r(xt, ut) + vt+1{f(xt, ut); ut+1:T−1},\nwhere we defined vT as the terminal reward function rT . Our goal is to find a sequence of actions\nu0:T−1 ∈UT maximizing the total sum of rewards J(u0:T−1) .= v0(x0; u0:T−1). We will tackle this\ntrajectory optimization problem using gradient ascent. Although our goal is to learn an open-loop\ncontroller (an action sequence), we assume that the state is fully observed during the training process.\nPontryagin’s principle. The gradient of the objective function J with respect to the action ut is\n∇utJ(u0:T−1) = ∇ur(xt, ut) + ∇uf(xt, ut) ∇xvt+1(xt+1; ut+1:T−1)\n|\n{z\n}\nλt+1∈RD\n,\n(1)\n1. For n ∈N, we write [n] .= {1, 2, . . . , n} and [n]0 .= {0, 1, . . . , n}. Unless explicitly mentioned, all time-dependent\nequations hold for all t ∈[T −1]0.\n2\nA PONTRYAGIN PERSPECTIVE ON REINFORCEMENT LEARNING\nwhere the terms of J related to the earlier time steps τ ∈[t −1]0 vanish, as they do not depend on\nut. We denote Jacobians as (∇yf)i,j .= ∂fj\n∂yi . The costates λ1:T are defined as the gradients of the\nvalue function along the given trajectory. They can be computed through a backward recursion:\nλT .= ∇vT (xT ) = ∇rT (xT )\n(2)\nλt .= ∇xvt(xt; ut:T−1) = ∇xr(xt, ut) + ∇xf(xt, ut)λt+1.\n(3)\nThe gradient (1) of the objective function can thus be obtained by means of one forward pass through\nthe dynamics f (a rollout), yielding the states x0:T , and one backward pass through (2) and (3),\nyielding the costates λ1:T . The stationarity condition arising from setting (1) to zero, where the\ncostates are computed from (2) and (3), is known as Pontryagin’s principle. (Pontryagin’s principle\nin fact goes much further than this, as it generalizes to infinite-dimensional and constrained settings.)\nWe re-derive (1) to (3) using the method of Lagrange multipliers in Appendix C.\n3. Method\nIf the dynamics are known, then the trajectory can be optimized by performing gradient ascent\nwith the gradients computed according to Pontryagin’s equations (1) to (3). In this work, we adapt\nthis idea to the domain of reinforcement learning, where the dynamics are unknown. In RL, we\nare able to interact with the environment, so the forward pass through the dynamics f is not an\nissue. However, the gradient computation according to Pontryagin’s principle requires the Jacobians\n∇xft .= ∇xf(xt, ut) and ∇uft .= ∇uf(xt, ut) of the unknown dynamics. In our methods, which\nfollow the structure of Algorithm 1, we therefore replace these Jacobians by estimates At ≃∇xft\nand Bt ≃∇uft. Before discussing concrete methods for open-loop RL, whose main concern is the\nconstruction of appropriate estimates At and Bt, we first show that replacing ∇xft and ∇uft in this\nway is a good idea. In particular, we show that, under certain assumptions on the accuracy of At and\nBt, Algorithm 1 converges to an unbiased local optimum of the true objective J. In the following\nsections we then discuss model-based and model-free open-loop RL methods.\n3.1. Convergence of Algorithm 1\nAlgorithm 1: Open-loop reinforcement learning\nInput: Optimization steps N ∈N, step size η > 0\n1 Initialize u0:T−1 (initial action sequence)\n2 for k = 1, 2, . . . , N do\n3\nx0:T ←rollout(u0:T−1)\n// Forw. pass\n// Backward pass\n4\n˜λT ←∇rT (xT )\n5\nfor t = T −1, T −2, . . . , 0 do\n// Jacobian estimation\n6\nAt, Bt ≃∇xf(xt, ut), ∇uf(xt, ut)\n// Pontryagin update\n7\n˜λt ←∇xr(xt, ut) + At˜λt+1\n8\ngt ←∇ur(xt, ut) + Bt˜λt+1\n9\nut ←ut + ηgt\n// Grad. ascent\nOur convergence result relies on the fol-\nlowing three assumptions.\nAssumption 1\nAll rewards are en-\ncoded in the terminal reward rT . In\nother words, r(x, u) = 0 for all x ∈X\nand u ∈U.\nThis assumption is without loss of gen-\nerality, since we can augment the state\nxt by a single real variable ρt that cap-\ntures the sum of the running rewards\n(i.e., ρ0 = 0 and ρt+1 = ρt + r(xt, ut)).\nAn equivalent setup that satisfies As-\nsumption 1 is then obtained by defining\na new terminal reward function r′\nT (xT , ρT ) .= rT (xT )+ρT and setting the running rewards r′ to zero.\n3\nEBERHARD VERNADE MUEHLEBACH\nAssumption 2 There exist constants γ, ζ > 0 with γ + ζ + γζ < 1 such that for any trajectory\n(u0:T−1, x0:T ) encountered by Algorithm 1, the following properties hold for all t ∈[T −1]0:\n(a) The error of At+s is bounded, for all s ∈[T −t], in the following way:\n∥At+s −∇xft+s∥≤γ\n3s ¯σ(∇uft)\n¯σ(∇uft)\n(s−1\nY\ni=1\n¯σ(∇xft+i)\n¯σ(∇xft+i)\n)\n¯σ(∇xft+s).\n(b) The error of Bt is bounded in the following way: ∥Bt −∇uft∥≤ζ¯σ(∇uft).\nHere, ¯σ(A) and ¯σ(A) denote the minimum and maximum singular value of A, and ∥A∥.= ¯σ(A).\nThis assumption restricts the errors of the estimates At and Bt that are used in place of the true\nJacobians ∇xft and ∇uft in Algorithm 1. Although the use of the true system for collecting rollouts\nprevents a buildup of error in the forward pass, any error in the approximate costate ˜λt can still be\namplified by the Jacobian estimates of earlier time steps, Aτ for τ ∈[t−1], during the backward pass.\nThus, to ensure convergence to a stationary point of the objective function J, the errors of these esti-\nmates need to be small. This is particularly important for t close to T, as these errors will be amplified\nover more time steps. Assumption 2 provides a quantitative characterization of this intuition.\nAssumption 3 There exists a constant L > 0 such that, for all action sequences uA\n0:T−1, uB\n0:T−1 ∈\nUT and all times t ∈[T −1]0, ∥∇utJ(uA\n0:T−1) −∇utJ(uB\n0:T−1)∥≤L∥uA\nt −uB\nt ∥.\nThis final assumption states that the objective function J is L-smooth with respect to the action ut at\neach time step t ∈[T −1]0, which is a standard assumption in nonconvex optimization. This implies\nthat the dynamics f are smooth as well. We are now ready to state the convergence result.\nTheorem 4\nSuppose Assumptions 1 to 3 hold with γ, ζ, and L. Let µ .= 1 −γ −ζ −γζ and\nν .= 1 + γ + ζ + γζ. If the step size η is chosen small enough such that α .= µ −1\n2ηLν2 is positive,\nthen the iterates (u(k)\n0:T−1)N−1\nk=0 of Algorithm 1 satisfy, for all N ∈N and t ∈[T −1]0,\n1\nN\nN−1\nX\nk=0\n∥∇utJ(u(k)\n0:T−1)∥2 ≤J⋆−J(u(0)\n0:T−1)\nαηN\n,\nwhere J⋆.= supu∈UT J(u) is the optimal value of the initial state.\nProof See Appendix E. The proof depends on a novel intricate analysis of the backpropagation\nprocedure in the case of an accurate forward pass and an inaccurate backward pass. This technique\nmay also be applicable to other (non-control) domains, as described in Appendix A.\n3.2. Model-based open-loop RL\nThe most direct way to approximate the Jacobians ∇xft and ∇uft is by using a (learned or manually\ndesigned) differentiable model ˜f : X ×U →X of the dynamics f and setting At = ∇x ˜f(xt, ut) and\nBt = ∇u ˜f(xt, ut) in Line 6 of Algorithm 1. Theorem 4 guarantees that this model-based open-loop\nRL method (see Algorithm B.1) is robust to a certain amount of modeling error. In contrast to this,\nconsider the more naive method of using the model to directly obtain a gradient by differentiating\nJ(u0:T−1) ≃r(x0, u0) + r{ ˜f(x0, u0)\n|\n{z\n}\n˜x1\n, u1} + · · · + rT { ˜f( ˜f(· · · ˜f( ˜f(x0, u0), u1) · · · ), uT−1)\n|\n{z\n}\n˜xT\n}\n4\nA PONTRYAGIN PERSPECTIVE ON REINFORCEMENT LEARNING\nwith respect to the actions u0:T−1 using the backpropagation algorithm. In Appendix D, we show that\nthis planning approach is exactly equivalent to an approximation of Algorithm 1 where, in addition\nto setting At = ∇x ˜f(xt, ut) and Bt = ∇u ˜f(xt, ut), the forward pass of Line 3 is replaced by the\nimagined forward pass ˜x0:T through the model ˜f. In Section 4, we empirically demonstrate that this\nplanning method, whose convergence is not guaranteed by Theorem 4, is much less robust to mod-\neling errors than the open-loop RL approach. Note that neither method is related to model-predictive\ncontrol (MPC), which relies on measurements to re-plan at every step. MPC is a closed-loop method\nthat solves a fundamentally different problem from the one we address in this work.\n3.3. Model-free on-trajectory open-loop RL\nAccess to a reasonably accurate model may not always be feasible, and as Algorithm 1 only requires\nthe Jacobians of the dynamics along the current trajectory, a global model is also not necessary. In\nthe following two sections, we propose two methods that directly estimate the Jacobians ∇xft and\n∇uft from rollouts in the environment. We call these methods model-free, as the estimated Jacobians\nare only valid along the current trajectory, and thus cannot be used for planning.\nOur goal is to estimate the Jacobians ∇xf(¯xt, ¯ut) and ∇uf(¯xt, ¯ut) that lie along the trajectory\ninduced by the action sequence ¯u0:T−1. These Jacobians measure how the next state (¯xt+1) changes\nif the current state or action (¯xt, ¯ut) are slightly perturbed. More formally, the dynamics f may be\nlinearized about the reference trajectory (¯u0:T−1, ¯x0:T ) as\nf(xt, ut) −f(¯xt, ¯ut)\n|\n{z\n}\n∆xt+1\n≃∇xf(¯xt, ¯ut)⊤(xt −¯xt)\n|\n{z\n}\n∆xt\n+∇uf(¯xt, ¯ut)⊤(ut −¯ut)\n|\n{z\n}\n∆ut\n,\nwhich is a valid approximation if the perturbations ∆x0:T and ∆u0:T−1 are small. By collecting a\ndataset of M ∈N rollouts with slightly perturbed actions, we can thus estimate the Jacobians by\nsolving the (analytically tractable) least-squares problem\narg min\n[A⊤\nt B⊤\nt ]∈RD×(D+K)\nM\nX\ni=1\n∥A⊤\nt ∆x(i)\nt\n+ B⊤\nt ∆u(i)\nt\n−∆x(i)\nt+1∥2.\n(4)\nThis technique is illustrated in Fig. 2a (dashed purple line). Using these estimates in Algorithm 1\nyields a model-free method we call on-trajectory, as the gradient estimate relies only on data\ngenerated based on the current trajectory (see Algorithm B.2 for details). We see a connection to\non-policy methods in closed-loop reinforcement learning, where the policy gradient estimate (or the\nQ-update) similarly depends only on data generated under the current policy. Like on-policy methods,\non-trajectory methods will benefit greatly from the possibility of parallel environments, which could\nreduce the effective complexity of the forward pass stage from M+1 rollouts to that of a single rollout.\nExploiting the Markovian structure. Consider a direct linearization of the objective function J\nabout the current trajectory. Writing the action sequence as a vector ¯u .= vec(¯u0:T−1) ∈RTK, this\nlinearization is given, for u ∈RTK close to ¯u, by\nJ(u) ≃J(¯u) + ∇J(¯u)⊤(u −¯u).\nWe can thus estimate the gradient of the objective function by solving the least squares problem\n∇J(¯u) ≃arg min\ng∈RT K\nM\nX\ni=1\n{J(ui) −J(¯u) −g⊤(ui −¯u)}2,\n5\nEBERHARD VERNADE MUEHLEBACH\n(¯xt, ¯ut)\n(xt, ut)i\n∆x(i)\nt+1\n∆x(i)\nt\n, ∆u(i)\nt\nft\n.= f(¯xt, ¯ut), ∆x .= x −¯xt, ∆u .= u −¯ut\n¯xt+1\nx(i)\nt+1\n(a)\nf(x, u)\nft + ∇xf ⊤\nt ∆x + ∇uf ⊤\nt ∆u\nft + A⊤\nt ∆x + B⊤\nt ∆u\n(least squares ﬁt)\nA⊤\nt x + B⊤\nt u + ct\n(least squares ﬁt)\n(xt, ut)k−3 (xt, ut)k−2 (xt, ut)k−1\n(xt, ut)k\nx(k−3)\nt+1\nx(k−2)\nt+1\nx(k−1)\nt+1\nx(k)\nt+1\n(b)\nReference transition\nM perturbed transitions\nf(x, u)\nTransitions of subsequent trajectories\nLinearizations of f at subsequent trajectories\nLeast squares ﬁt weighting all points equally\nFigure 2: (a) The Jacobians of f (slope of the green linearization) at the reference point (¯xt, ¯ut) can\nbe estimated from the transitions {(x(i)\nt , u(i)\nt , x(i)\nt+1)}M\ni=1 of M perturbed rollouts. (b) The Jacobians\nof subsequent trajectories (indexed by k) remain close. To estimate the Jacobian at iteration k, the\nmost recent iterate (k −1) is more relevant than older iterates.\nwhere {ui} are M ∈N slightly perturbed action sequences. Due to the dimensionality of ¯u, this\nmethod requires O(TK) rollouts to estimate the gradient. In contrast to this, our approach leverages\nthe Markovian structure of the problem, including the fact that we observe the states x0:T in each\nrollout. As the Jacobians are estimated jointly at all time steps, we can expect to get a useful gradient\nestimate from only O(D2 + DK) rollouts, which significantly reduces the sample complexity if T\nis large. This gain in efficiency is demonstrated empirically in Section 4.\n3.4. Model-free off-trajectory open-loop RL\nThe on-trajectory algorithm is sample-efficient in the sense that it leverages the problem structure,\nbut a key inefficiency remains: the rollout data sampled at each iteration is discarded after the action\nsequence is updated. In this section, we propose an off-trajectory method that implicitly uses the\ndata from previous trajectories to construct the Jacobian estimates. Our approach is based on the\nfollowing observation. If the dynamics f are smooth and the step size η is small, then the updated\ntrajectory (u(k)\n0:T−1, x(k)\n0:T ) will remain close to the previous iterate (u(k−1)\n0:T−1, x(k−1)\n0:T\n). Furthermore, the\nJacobians along the updated trajectory will be similar to the previous Jacobians, as illustrated in\nFig. 2b. Thus, we propose to estimate the Jacobians along the current trajectory from a single rollout\nonly by bootstrapping our estimates using the Jacobian estimates from the previous iteration.\nConsider again the problem of estimating the Jacobians from multiple perturbed rollouts, illus-\ntrated in Fig. 2a. Instead of relying on a reference trajectory and (4), we can estimate the Jacobians\nby fitting a linear regression model to the dataset of M perturbed transitions. Solving\narg min\n[A⊤\nt B⊤\nt\nct]∈RD×(D+K+1)\nM\nX\ni=1\n∥A⊤\nt x(i)\nt\n+ B⊤\nt u(i)\nt\n+ ct −x(i)\nt+1∥2\n(5)\nyields an approximate linearization f(xt, ut) ≃A⊤\nt xt + B⊤\nt ut + ct = Ftzt, with Ft .= [A⊤\nt B⊤\nt ct]\nand zt .= (xt, ut, 1) ∈RD+K+1. This approximation is also shown in Fig. 2a (dotted gray line).2\n2. If we replace (4) by (5) in Algorithm B.2, we get a slightly different on-trajectory method with similar performance.\n6\nA PONTRYAGIN PERSPECTIVE ON REINFORCEMENT LEARNING\nAt iteration k, given the estimate F (k−1)\nt\nand a new point z(k)\nt\n= (x(k)\nt , u(k)\nt , 1) with corresponding\ntarget x(k)\nt+1, computing the new estimate F (k)\nt\nis a problem of online linear regression. We solve this\nregression problem using an augmented version of the recursive least squares (RLS) algorithm (e.g.,\nLjung, 1999, Sec. 11.2). By introducing a prior precision matrix Q(0)\nt\n.= q0I for each time t, where\nq0 > 0, we compute the update at iteration k ∈N (see Algorithm B.3) as\nQ(k)\nt\n= αQ(k−1)\nt\n+ (1 −α)q0I + z(k)\nt\n{z(k)\nt\n}⊤\n(6)\nF (k)\nt\n= F (k−1)\nt\n+ {Q(k)\nt }−1z(k)\nt\n{x(k)\nt+1 −F (k−1)\nt\nz(k)\nt\n}⊤.\nForgetting and stability. The standard RLS update of the precision matrix corresponds to (6) with\nα = 1. In the limit as q0 →0, the RLS algorithm is equivalent to the batch processing of (5), which\ntreats all points equally. However, as illustrated in Fig. 2b, points from recent trajectories should\nbe given more weight, as transitions that happened many iterations ago will give little information\nabout the Jacobians along the current trajectory. We can incorporate a forgetting factor α ∈(0, 1)\ninto the precision update with the effect that past data points are exponentially downweighted:\nQ(k)\nt\n= αQ(k−1)\nt\n+ z(k)\nt\n{z(k)\nt\n}⊤\n⇝\nQ(k)\nt\n= αkq0I +\nk\nX\ni=1\nαk−iz(i)\nt {z(i)\nt }⊤.\n(7)\nThis forgetting factor introduces a new problem: instability. If subsequent trajectories lie close to\neach other, then the sum of outer products may become singular (e.g., if all z(i)\nt\nare identical, then\nthe sum has rank 1). As the prior q0I is downweighted, at some point inverting Q may become\nnumerically unstable. Our modification in (6) adds (1 −α)q0I in each update, which has the effect\nof removing the αk coefficient in front of q0I in (7). If the optimization procedure converges, then\neventually subsequent trajectories will indeed lie close together. Although (6) prevents issues with\ninstability, the quality of the Jacobian estimates will still degrade, as this estimation inherently\nrequires perturbations (see Section 3.3). In Algorithm B.3, we thus slightly perturb the actions used\nin each rollout to get more diverse data.\n4. Experiments\n4.1. Inverted pendulum swing-up\nWe empirically evaluate our algorithms on the inverted pendulum swing-up task shown in Fig. 3. As a\nperformance criterion we define Jmax .= maxk∈[N]0 J(u(k)\n0:T−1) as the return achieved by the best ac-\ntion sequence over a complete learning process of N optimization steps. The task is considered solved\nif Jmax exceeds a certain threshold. A detailed description of the task is given in Appendix F. We re-\npeat our experiments with 100 random seeds and show 95% bootstrap confidence intervals in all plots.\nRobustness: model-based open-loop RL. In Theorem 4, we proved that our model-based open-\nloop RL method (Algorithm B.1) can accommodate some model error and still converge to a local\nmaximum of the true objective. To test the robustness of our algorithm against model misspecification,\nwe use a pendulum system with inaccurate parameters as the model ˜f. Concretely, if mi is the ith\nparameter of the true system (cf. Appendix F), we sample the corresponding model parameter ˜mi\nfrom a log-normal distribution centered at mi, such that ˜mi = ξmi, with ln ξ ∼N(0, s2). The\nseverity of the model error is then controlled by the scale parameter s. In Fig. 4, we compare the\nperformance of our method with the planning procedure described in Section 3.2, in which the\nforward pass is performed through the model ˜f instead of the real system f. Whereas the planning\n7\nEBERHARD VERNADE MUEHLEBACH\nReturn: −0.019\nθ\nℓ\nF\nTip trajectory\n0\n100\nTime t\nFigure 3: The inverted pendulum swing-up\ntask. The goal is to control the force F such\nthat the tip of the pendulum swings up above\nthe base. The shown solution was found by\nthe on-trajectory method of Section 3.3.\n0\n10−3\n10−2\n10−1\n100\n101\nModel misspeciﬁcation s\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSolve rate\nModel-based OLRL\nNaive planning\nMLP model OLRL\nMLP model planning\nFigure 4: The model-based open-loop RL algorithm\ncan solve the pendulum problem reliably even with\na considerable model error.\nmethod only solves the pendulum reliably with the true system as the model (s = 0), the open-loop\nRL method can accommodate a considerable model misspecification.\nIn a second experiment, we represent the model ˜f by a small multi-layer perceptron (MLP). The\nmodel is learned from 1000 rollouts, with the action sequences sampled from a pink noise distribution,\nas suggested by Eberhard et al. (2023). Figure 4 compares the performance achieved with this model\nby our algorithm and by the planning method. As the MLP model represents a considerable misspec-\nification of the true dynamics, only the open-loop RL method manages to solve the pendulum task.\nStructure: on-trajectory open-loop RL. Our model-free on-trajectory method (Algorithm B.2)\nuses rollouts to directly estimate the Jacobians needed to update the action sequence. It is clear\nfrom (4) that more rollouts (i.e., larger M) will give more accurate Jacobian estimates, and therefore\nincrease the quality of the gradient approximation. In Fig. 5, we analyze the sample efficiency of the\nthis algorithm by comparing the performance achieved at different values of M, where the number\nN of optimization steps remains fixed. We compare our method to the finite-difference approach\ndescribed at the end of Section 3.3 and to the gradient-free cross-entropy method (CEM; Rubinstein,\n1999). Both these methods also update the action sequence on the basis of M perturbed rollouts in the\nenvironment. As in our method, the M action sequences are perturbed using Gaussian white noise\nwith noise scale σ. We describe both baselines in detail in Appendix H. The oracle performance\nshown in Fig. 5 corresponds to Algorithm 1 with the true gradient, i.e., At = ∇xft and Bt = ∇uft.\nFigure 5 shows that the performance of both the finite-difference method and CEM heavily de-\npends on the choice of the noise scale σ, whereas our method performs identically for all three values\nof σ. Even for tuned values of σ, the finite-difference method and CEM still need approximately twice\nas many rollouts per iteration as the open-loop RL method to reliably swing up the pendulum. At 10\nrollouts per iteration, our method matches the oracle’s performance, while both baselines are below\nthe solved threshold. This empirically confirms our theoretical claims at the end of Section 3.3, where\nwe argue that exploiting the Markovian structure of the problem leads to increased sample efficiency.\nEfficiency: off-trajectory open-loop RL. Finally, we turn to the method proposed in Section 3.4\n(Algorithm B.3), which promises increased sample efficiency by estimating the Jacobians in an\noff-trajectory fashion. The performance of this algorithm is shown in Fig. 6, where the learning\ncurves of all our methods as well as the two baselines and the oracle are plotted. For the on-trajectory\nmethods compared in Fig. 5, we chose for each the minimum number of rollouts M such that, under\nthe best choice for σ, the method would reliably solve the swing-up task. The hyperparameters for\n8\nA PONTRYAGIN PERSPECTIVE ON REINFORCEMENT LEARNING\n2\n5 10 20 50\nM + 1\n−1\n−0.1\nAvg. max. return Jmax\nOn-traj. OLRL\nOracle\nSolved\n2\n5 10 20 50\nM + 1\nNumber of rollouts per iteration\nFinite differences\n2\n5 10 20 50\nM\nCEM\nσ = 0.0001\nσ = 0.001\nσ = 0.01\nFigure 5: The on-trajectory open-loop RL method\nis more sample-efficient than the finite-difference\nand cross-entropy methods. It is also much less\nsensitive to the noise scale σ.\n0\n20000\n40000\nEpisodes\n−3\n−2\n−1\n0\nAverage return J\n104\n106\nEpisodes\nOn-traj. OLRL\nOff-traj. OLRL\nMLP model OLRL\nFinite differences\nCEM\nOracle\nFigure 6: Learning curves on the pendulum task.\nOn the right, we show a longer time period in log\nscale. The off-trajectory open-loop RL method\nconverges almost as fast as the oracle method.\nall methods are summarized in Appendix I. It can be seen that the off-trajectory method, which only\nrequires one rollout per iteration, converges much faster than the on-trajectory open-loop RL method.\n4.2. MuJoCo\n0\n10000\n20000\nEpisodes\n0\n100\n200\n300\nAverage return J\nAnt-v4\nOff-traj. OLRL\n0\n10000\n20000\nEpisodes\nHalfCheetah-v4\nSAC (closed-loop)\nSAC (open-loop)\nFigure 7: Learning curves of our off-trajectory open-loop RL\nmethod and soft actor-critic (SAC) for two MuJoCo tasks.\nAll experiments were repeated with 20 random seeds, and\nwe show 95%-bootstrap confidence intervals for the average\nreturn. The horizon is fixed to T = 100.\nWhile the inverted pendulum is illus-\ntrative for analyzing our algorithms\nempirically, it is a relatively simple\ntask with smooth, low-dimensional,\nand deterministic dynamics.\nIn\nthis section, we test our method\nin two considerably more challeng-\ning environments: the Ant-v4 and\nHalfCheetah-v4 tasks provided\nby the OpenAI Gym library (Brock-\nman et al., 2016; Towers et al., 2023),\nimplemented in MuJoCo (Todorov\net al., 2012). These environments are\nhigh-dimensional, they exhibit non-\nsmooth contact dynamics, and the initial state is randomly sampled at the beginning of each episode.\nWe tackle these two tasks with our model-free off-trajectory method (Algorithm B.3). The\nresults are shown in Fig. 7, where we compare to the closed-loop RL baseline soft actor-critic (SAC;\nHaarnoja et al., 2018a). It can be seen that the open-loop RL method performs comparably to SAC,\neven though SAC learns a closed-loop policy that is capable of adapting its behavior to the initial\ncondition.3 In the figure, we also analyze the open-loop performance achieved by SAC. Whereas the\nclosed-loop performance is the return obtained in a rollout where the actions are taken according to the\nmean of the Gaussian policy, the open-loop return is achieved by blindly executing exactly the same\nactions in a new episode. The discrepancy in performance is thus completely due to the stochasticity\nin the initial state. In Appendix G, we show that our method also works with a longer horizon T.\n3. In this comparison, our method is further disadvantaged by the piecewise constant “health” terms in the reward\nfunction of Ant-v4. Our method, exclusively relying on the gradient of the reward function, ignores these.\n9\nEBERHARD VERNADE MUEHLEBACH\nThe results demonstrate that the open-loop RL algorithm is robust to a certain level of stochasticity\nin the initial state of stable dynamical systems. Additionally, while our convergence analysis depends\non the assumption of smooth dynamics, these experiments empirically demonstrate that the algorithms\nare also able to tackle non-smooth contact dynamics. Finally, we see that the high dimensionality\nof the MuJoCo systems is handled without complications. While soft actor-critic is an elegant and\npowerful algorithm, the combination with deep function approximation can make efficient learning\nmore difficult. Our methods are considerably simpler and, because they are based on Pontryagin’s\nprinciple rather than dynamic programming, they evade the curse of dimensionality by design, and\nthus do not require any function approximation.\n5. Discussion\nThis paper makes an important first step towards understanding how principles from open-loop\noptimal control can be combined with ideas from reinforcement learning while preserving conver-\ngence guarantees. We propose three algorithms that address this open-loop RL problem, from robust\ntrajectory optimization with an approximate model to sample-efficient learning under fully unknown\ndynamics. This work focuses on reinforcement learning in continuous state and action spaces, a\nclass of problems known to be challenging (Recht, 2019). Although this setting allows us to leverage\ncontinuous optimization techniques, we expect that most ideas will transfer to the discrete setting,\nand we would be interested to see further research on this topic.\nIt is interesting to note that there are many apparent parallels between our open-loop RL algo-\nrithms and their closed-loop counterparts. The distinction between model-based and model-free\nmethods is similar to that in closed-loop RL. Likewise, the on-trajectory and off-trajectory methods\nwe present show a tradeoff between sample efficiency and stability that is reminiscent of the tradeoffs\nbetween on-policy and off-policy methods in closed-loop RL. The question of exploration, which is\ncentral to reinforcement learning, also arises in our case. We do not address this complex problem\nthoroughly here but instead rely on additive Gaussian noise to sample diverse trajectories.\nLimitations of open-loop RL. Another important open question is how open-loop methods fit into\nthe reinforcement learning landscape. An inherent limitation of these methods is that an open-loop\ncontroller can, by definition, not react to unexpected changes in the system’s state, be it due to random\ndisturbances or an adversary. An open-loop controller cannot balance an inverted pendulum in its\nunstable position4, track a reference trajectory in noisy conditions, or play Go, where reactions to\nthe opponent’s moves are constantly required. In these situations open-loop RL is not viable or only\neffective over a very short horizon T. However, if the disturbances are small, and the system is not\nsensitive to small changes in state or action (roughly speaking, if the system is stable and non-chaotic),\nthen a reaction is not necessary, and open-loop RL works even for long horizons T (as we highlight\nin our MuJoCo experiments, cf. Appendix G). Open-loop control can be viewed as a special case\nof closed-loop control, and therefore it is clear that closed-loop control is much more powerful. Our\nalgorithms provide a first solution to the open-loop RL problem and are not intended to replace\nany of the existing closed-loop RL algorithms. In control engineering, it is common to combine\nfeedback and feedforward techniques. In many situations, it can be shown that such a combination\nwill significantly outperform a solution based on feedback alone (e.g., Åström and Murray, 2021, Sec.\n12.4). We believe that ultimately a combination of open-loop and closed-loop techniques will also\nbe fruitful in reinforcement learning and think that this is an important direction for future research.\n4. Except with a clever trick called vibrational control (Meerkov, 1980).\n10\nA PONTRYAGIN PERSPECTIVE ON REINFORCEMENT LEARNING\nAcknowledgments\nWe thank the International Max Planck Research School for Intelligent Systems (IMPRS-IS) for their\nsupport. C. Vernade is funded by the German Research Foundation (DFG) under both the project\n468806714 of the Emmy Noether Programme and under Germany’s Excellence Strategy – EXC\nnumber 2064/1 – Project number 390727645. M. Muehlebach is funded by the German Research\nFoundation (DFG) under the project 456587626 of the Emmy Noether Programme.\nReferences\nKarl J. Åström and Tore Hägglund. PID Controllers: Theory, Design, and Tuning. International\nSociety for Measurement and Control, second edition, 1995. 16\nKarl J. Åström and Richard M. Murray. Feedback Systems: An Introduction for Scientists and\nEngineers. Princeton University Press, second edition, 2021. 10, 16\nJohn T. Betts. Practical Methods for Optimal Control and Estimation Using Nonlinear Programming.\nSIAM, 2010. 2, 16\nLucas Böttcher, Nino Antulov-Fantulin, and Thomas Asikis. AI Pontryagin or how artificial neural\nnetworks learn to control dynamical systems. Nature communications, 13(333), 2022. URL\nhttps://doi.org/10.1038/s41467-021-27590-0. 16\nLéon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine\nlearning. SIAM review, 60(2):223–311, 2018. URL https://doi.org/10.1137/16M108\n0173. 20\nGreg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and\nWojciech Zaremba. OpenAI Gym. arXiv:1606.01540, 2016. URL http://arxiv.org/ab\ns/1606.01540. 9\nThomas Carraro, Michael Geiger, Stefan Körkel, and Rolf Rannacher, editors. Multiple Shooting\nand Time Domain Decomposition Methods. Springer, 2015. 16\nYuqing Chen and David J. Braun. Hardware-in-the-loop iterative optimal feedback control without\nmodel-based future prediction. IEEE Transactions on Robotics, 35(6):1419–1434, 2019. URL\nhttps://doi.org/10.1109/TRO.2019.2929014. 16\nJonas Degrave, Federico Felici, Jonas Buchli, Michael Neunert, Brendan Tracey, Francesco\nCarpanese, Timo Ewalds, Roland Hafner, Abbas Abdolmaleki, Diego de Las Casas, et al. Magnetic\ncontrol of tokamak plasmas through deep reinforcement learning. Nature, 602(7897):414–419,\n2022. URL https://doi.org/10.1038/s41586-021-04301-9. 16\nMoritz Diehl, Hans Georg Bock, Holger Diedam, and Pierre-Brice Wieber. Fast direct multiple\nshooting algorithms for optimal robot control. In Fast Motions in Biomechanics and Robotics:\nOptimization and Feedback Control, pages 65–93. Springer, 2006. URL https://doi.org/\n10.1007/978-3-540-36119-0_4. 2\nOnno Eberhard, Jakob Hollenstein, Cristina Pinneri, and Georg Martius. Pink noise is all you\nneed: Colored noise exploration in deep reinforcement learning. In Proceedings of the Eleventh\n11\nEBERHARD VERNADE MUEHLEBACH\nInternational Conference on Learning Representations, 2023. URL https://openreview\n.net/forum?id=hQ9V5QN27eS. 8\nKaterina Fragkiadaki, Sergey Levine, Panna Felsen, and Jitendra Malik. Recurrent network models\nfor human dynamics. In Proceedings of the IEEE International Conference on Computer Vision,\npages 4346–4354, 2015. URL https://doi.org/10.1109/ICCV.2015.488. 16\nJavier Gamiz, Herminio Martínez, Antoni Grau, Yolanda Bolea, and Ramón Vilanova. Feed-forward\ncontrol for a drinking water treatment plant chlorination process. In 2020 25th IEEE International\nConference on Emerging Technologies and Factory Automation (ETFA), volume 1, pages 462–467,\n2020. URL https://doi.org/10.1109/ETFA46521.2020.9211884. 16\nHans P. Geering. Optimal Control with Engineering Applications. Springer, 2007. 2\nTuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy\nmaximum entropy deep reinforcement learning with a stochastic actor. In Proceedings of the 35th\nInternational Conference on Machine Learning, volume 80 of Proceedings of Machine Learning\nResearch, pages 1856–1865. PMLR, 2018a. URL http://proceedings.mlr.press/v8\n0/haarnoja18b.html. 9, 26\nTuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash\nKumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, and Sergey Levine. Soft actor-critic algorithms\nand applications. arXiv:1812.05905, 2018b. URL http://arxiv.org/abs/1812.05905.\n27\nDanijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and\nJames Davidson. Learning latent dynamics for planning from pixels. In Proceedings of the 36th\nInternational Conference on Machine Learning, volume 97 of Proceedings of Machine Learning\nResearch, pages 2555–2565. PMLR, 2019. URL https://proceedings.mlr.press/v9\n7/hafner19a.html. 25\nEric Hansen, Andrew Barto, and Shlomo Zilberstein. Reinforcement learning for mixed open-loop\nand closed-loop control. Advances in Neural Information Processing Systems, 9, 1996. URL\nhttps://proceedings.neurips.cc/paper/1996/hash/ab1a4d0dd4d48a2ba\n1077c4494791306-Abstract.html. 16\nGreg Horn, Sébastien Gros, and Moritz Diehl. Numerical trajectory optimization for airborne\nwind energy systems described by high fidelity aircraft models. In Uwe Ahrens, Moritz Diehl,\nand Roland Schmehl, editors, Airborne Wind Energy, pages 205–218. Springer, 2013. URL\nhttps://doi.org/10.1007/978-3-642-39965-7_11. 16\nNikolaus Howe, Simon Dufort-Labbé, Nitarshan Rajkumar, and Pierre-Luc Bacon. Myriad: a\nreal-world testbed to bridge trajectory optimization and deep learning. In Advances in Neural\nInformation Processing Systems, volume 35, pages 29801–29815, 2022. URL https://proc\needings.neurips.cc/paper_files/paper/2022/hash/c0b91f9a3587bf352\n87f41dba5d20233-Abstract-Datasets_and_Benchmarks.html. 16\nMuhammad Kamran Janjua, Haseeb Shah, Martha White, Erfan Miahi, Marlos C Machado, and\nAdam White. GVFs in the real world: making predictions online for water treatment. Machine\n12\nA PONTRYAGIN PERSPECTIVE ON REINFORCEMENT LEARNING\nLearning, 113(8):5151–5181, 2024. URL https://doi.org/10.1007/s10994-023-0\n6413-x. 16\nWanxin Jin, Zhaoran Wang, Zhuoran Yang, and Shaoshuai Mou. Pontryagin differentiable pro-\ngramming: An end-to-end learning and control framework. In Advances in Neural Information\nProcessing Systems, volume 33, pages 7979–7992, 2020. URL https://proceedings.ne\nurips.cc/paper/2020/hash/5a7b238ba0f6502e5d6be14424b20ded-Abstr\nact.html. 16\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings of\nthe Third International Conference on Learning Representations, 2014. URL http://arxiv.\norg/abs/1412.6980. 26\nJun Haeng Lee, Tobi Delbruck, and Michael Pfeiffer. Training deep spiking neural networks using\nbackpropagation. Frontiers in Neuroscience, 10, 2016. URL https://doi.org/10.3389/\nfnins.2016.00508. 16\nLennart Ljung. System Identification: Theory for the User. Prentice Hall, 1999. 7\nHao Ma, Dieter Büchler, Bernhard Schölkopf, and Michael Muehlebach. A learning-based iterative\ncontrol framework for controlling a robot arm with pneumatic artificial muscles. In Proceedings of\nRobotics: Science and Systems, 2022. URL https://www.roboticsproceedings.or\ng/rss18/p029.html. 16\nHao Ma, Dieter Büchler, Bernhard Schölkopf, and Michael Muehlebach. Reinforcement learning\nwith model-based feedforward inputs for robotic table tennis. Autonomous Robots, 47:1387–1403,\n2023. URL https://doi.org/10.1007/s10514-023-10140-6. 16\nMassimiliano Mattei, Raffaele Albanese, Giuseppe Ambrosino, and Alfredo Portone. Open loop\ncontrol strategies for plasma scenarios: Linear and nonlinear techniques for configuration transi-\ntions. In Proceedings of the 45th IEEE Conference on Decision and Control, pages 2220–2225,\n2006. URL https://doi.org/10.1109/CDC.2006.377412. 16\nSemyon M. Meerkov. Principle of vibrational control: Theory and applications. IEEE Transactions\non Automatic Control, 25(4):755–762, 1980. URL https://doi.org/10.1109/TAC.19\n80.1102426. 10\nKevin L. Moore. Iterative Learning Control for Deterministic Systems. Springer, 1993. URL\nhttps://doi.org/10.1007/978-1-4471-1912-8. 16\nMichael Niemeyer, Lars Mescheder, Michael Oechsle, and Andreas Geiger. Differentiable volumetric\nrendering: Learning implicit 3D representations without 3D supervision. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3504–3515, 2020.\nURL https://openaccess.thecvf.com/content_CVPR_2020/html/Niemeye\nr_Differentiable_Volumetric_Rendering_Learning_Implicit_3D_Repre\nsentations_Without_3D_Supervision_CVPR_2020_paper.html. 16\nCristina Pinneri, Shambhuraj Sawant, Sebastian Blaes, Jan Achterhold, Joerg Stueckler, Michal\nRolinek, and Georg Martius. Sample-efficient cross-entropy method for real-time planning. In\n13\nEBERHARD VERNADE MUEHLEBACH\nProceedings of the 2020 Conference on Robot Learning, volume 155 of Proceedings of Machine\nLearning Research, pages 1049–1065. PMLR, 2021. URL https://proceedings.mlr.\npress/v155/pinneri21a.html. 25\nLev S. Pontryagin, Vladimir G. Boltayanskii, Revaz V. Gamkrelidze, and Evgenii F. Mishchenko.\nMathematical Theory of Optimal Processes. Wiley, 1962. 2\nAntonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and Noah\nDormann. Stable-Baselines3: Reliable reinforcement learning implementations. Journal of\nMachine Learning Research, 22(268):1–8, 2021. URL http://jmlr.org/papers/v22/\n20-1364.html. 27\nAntonin Raffin, Olivier Sigaud, Jens Kober, Alin Albu-Schäffer, João Silvério, and Freek Stulp. A\nsimple open-loop baseline for reinforcement learning locomotion tasks. arXiv:2310.05808, 2023.\n16\nBenjamin Recht. A tour of reinforcement learning: The view from continuous control. Annual\nReview of Control, Robotics, and Autonomous Systems, 2:253–279, 2019. URL https://doi.\norg/10.1146/annurev-control-053018-023825. 10\nReuven Rubinstein. The cross-entropy method for combinatorial and continuous optimization.\nMethodology and Computing in Applied Probability, 1:127–190, 1999. URL https://doi.\norg/10.1023/A:1010091220143. 8, 25\nStefan Schaal and Christopher G. Atkeson. Learning control in robotics. IEEE Robotics & Automation\nMagazine, 17(2):20–29, 2010. URL https://doi.org/10.1109/MRA.2010.936957.\n16\nAndreas Schäfer, Peter Kühl, Moritz Diehl, Johannes Schlöder, and Hans Georg Bock. Fast reduced\nmultiple shooting methods for nonlinear model predictive control. Chemical Engineering and\nProcessing: Process Intensification, 46(11):1200–1214, 2007. URL https://doi.org/10\n.1016/j.cep.2006.06.024. 16\nCarmelo Sferrazza, Michael Muehlebach, and Raffaello D’Andrea. Learning-based parametrized\nmodel predictive control for trajectory tracking. Optimal Control Applications and Methods, 41\n(6):2225–2249, 2020. URL https://doi.org/10.1002/oca.2656. 2\nSigurd Skogestad and Ian Postlethwaite. Multivariable Feedback Control: Analysis and Design.\nWiley, second edition, 2005. 16\nRichard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. MIT press,\nsecond edition, 2018. 1\nEmanuel Todorov, Tom Erez, and Yuval Tassa. MuJoCo: A physics engine for model-based control.\nIn Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems,\npages 5026–5033, 2012. URL https://doi.org/10.1109/IROS.2012.6386109. 9\nMark Towers, Jordan K. Terry, Ariel Kwiatkowski, John U. Balis, Gianluca de Cola, Tristan Deleu,\nManuel Goulão, Andreas Kallinteris, Arjun KG, Markus Krimmel, et al. Gymnasium, 2023. URL\nhttps://github.com/Farama-Foundation/Gymnasium. 9\n14\nA PONTRYAGIN PERSPECTIVE ON REINFORCEMENT LEARNING\nJurgen van Zundert and Tom Oomen. On inversion-based approaches for feedforward and ILC.\nMechatronics, 50:282–291, 2018. URL https://doi.org/10.1016/j.mechatronics\n.2017.09.010. 2\nDiederik Verscheure, Bram Demeulenaere, Jan Swevers, Joris De Schutter, and Moritz Diehl.\nTime-optimal path tracking for robots: A convex optimization approach. IEEE Transactions on\nAutomatic Control, 54(10):2318–2327, 2009. URL https://doi.org/10.1109/TAC.20\n09.2028959. 16\nMarin Vlastelica, Anselm Paulus, Vít Musil, Georg Martius, and Michal Rolínek. Differentiation\nof blackbox combinatorial solvers. In Proceedings of the Eighth International Conference on\nLearning Representations, 2020. URL https://openreview.net/forum?id=BkevoJ\nSYPB. 16\nLogan G. Wright, Tatsuhiro Onodera, Martin M. Stein, Tianyu Wang, Darren T. Schachter, Zoey Hu,\nand Peter L. McMahon. Deep physical neural networks trained with backpropagation. Nature, 601\n(7894):549–555, 2022. URL https://doi.org/10.1038/s41586-021-04223-6. 16\n15\nEBERHARD VERNADE MUEHLEBACH\nAppendix A. Related work\nHistorically, control theory has dealt with both closed-loop and open-loop control, and there is a broad\nconsensus in the control community that both are important (Åström and Murray, 2021; Skogestad\nand Postlethwaite, 2005; Åström and Hägglund, 1995; Betts, 2010; Verscheure et al., 2009; Horn et al.,\n2013). A simple application of open-loop methods is the control of electric stoves with simmerstats,\nwhich regulate the temperature by periodically switching the power on and off. Open-loop control is\nalso applied to much more challenging problems, such as the regulation of drinking-water treatment\nplants (Gamiz et al., 2020) or plasmas in a tokamak (Mattei et al., 2006). These problems have also\nbeen of interest to the reinforcement learning community (Janjua et al., 2024; Degrave et al., 2022).\nThe numerical solution of trajectory optimization problems has also been studied in machine learn-\ning (Schaal and Atkeson, 2010; Howe et al., 2022). As in our approach, an important aspect of these\nmethods is to exploit the Markovian structure of the dynamics to reduce computation (Carraro et al.,\n2015; Schäfer et al., 2007). However, in contrast to the RL setting that we consider, existing methods\naddress situations where the dynamics are known. Another set of related methods is known as iter-\native learning control (Moore, 1993; Ma et al., 2022, 2023), which is a control-theoretic framework\nthat iteratively improves the execution of a task by optimizing over feedforward trajectories. However,\nthese methods are often formulated for trajectory tracking tasks, while we consider a more general\nclass of reinforcement learning problems. Chen and Braun (2019) explore an idea similar to that in\nour Algorithm B.1; their model-based control algorithm combines a rollout in a real system with an\ninaccurate model to construct an iterative LQR feedback controller. A combination of open-loop and\nclosed-loop methods in the context of reinforcement learning is explored by Hansen et al. (1996).\nRaffin et al. (2023) have recently proposed to use open-loop control as a baseline to compare\nto more complex deep reinforcement learning methods. They argue, as do we, that deep RL methods\nhave become very complex, and that open-loop solutions may be favored for certain tasks due to their\nsimplicity. Their approach is to combine a gradient-free optimization method (CMA-ES, very similar\nto our baseline CEM), with prior knowledge about the problem (instead of letting the ut be completely\nfree, they optimize the parameters of nonlinear oscillators). The authors express their surprise about\nthe good performance of the open-loop method compared to state-of-the-art deep RL algorithms on\ncertain MuJoCo tasks, similar to the ones we consider in Section 4.2 (cf. Raffin et al., 2023, p. 8).\nRecently, deep neural networks have been used to learn representations of complex dynamical\nsystems (Fragkiadaki et al., 2015) and Pontryagin’s principle was leveraged in the optimization of\ncontrol tasks based on such models (Jin et al., 2020; Böttcher et al., 2022). However, these methods\nonly consider the setting of closed-loop control. The combination of an exact forward pass with an\napproximate backward pass, which our methods are based on, has also been explored in different\nsettings in the deep learning literature, such as spiking (Lee et al., 2016) or physical (Wright et al.,\n2022) neural networks, or networks that include nondifferentiable procedures, for example used for\nrendering (Niemeyer et al., 2020) or combinatorial optimization (Vlastelica et al., 2020). The analysis\nof Appendix E that we developed for our convergence result (Theorem 4) could also be relevant for\nthese applications, as the fundamental structure (backpropagation with an accurate forward pass and\nan inaccurate backward pass) is identical.\nAppendix B. Algorithms\nIn this section, we provide detailed descriptions of the three open-loop RL algorithms presented in\nthe main text. The model-based algorithm of Section 3.2 is listed in Algorithm B.1, the model-free\n16\nA PONTRYAGIN PERSPECTIVE ON REINFORCEMENT LEARNING\non-trajectory method of Section 3.3 is listed in Algorithm B.2, and the off-trajectory method of\nSection 3.4 is listed in Algorithm B.3. The hyperparameters we use in these algorithms are discussed\nin Appendix I.\nAlgorithm B.1: Model-based open-loop RL\nInput: Differentiable model ˜f : X × U →X, optimization steps N ∈N, step size η > 0\n1 Initialize u0:T−1 (initial action sequence)\n2 for k = 1, 2, . . . , N do\n// Forward pass\n3\nx0:T ←rollout(u0:T−1)\n// Backward pass\n4\n˜λT ←∇rT (xT )\n5\nfor t = T −1, T −2, . . . , 0 do\n6\n˜λt ←∇xr(xt, ut) + ∇x ˜f(xt, ut)˜λt+1\n7\ngt ←∇ur(xt, ut) + ∇u ˜f(xt, ut)˜λt+1\n8\nut ←ut + ηgt\n// Gradient ascent\nAlgorithm B.2: Model-free on-trajectory open-loop RL\nInput: Number of rollouts M ∈N, noise scale σ > 0, optimization steps N ∈N,\nstep size η > 0\n1 Initialize ¯u0:T−1 (initial action sequence)\n2 for k = 1, 2, . . . , N do\n// Forward passes\n3\n¯x0:T ←rollout(¯u0:T−1)\n4\nfor i = 1, 2, . . . , M do\n5\nu(i)\n0:T−1 ∼N(¯u0:T−1, σI)\n6\nx(i)\n0:T ←rollout(u(i)\n0:T−1)\n7\n∆u(i)\n0:T−1 ←u(i)\n0:T−1 −¯u0:T−1\n8\n∆x(i)\n0:T ←x(i)\n0:T −¯x0:T\n// Backward pass\n9\n˜λT ←∇rT (¯xT )\n10\nfor t = T −1, T −2, . . . , 0 do\n// Jacobian estimation\n11\nAt, Bt ←arg minAt∈RD×D,Bt∈RK×D PM\ni=1∥A⊤\nt ∆x(i)\nt\n+ B⊤\nt ∆u(i)\nt\n−∆x(i)\nt+1∥2\n// Pontryagin update\n12\n˜λt ←∇xr(¯xt, ¯ut) + At˜λt+1\n13\ngt ←∇ur(¯xt, ¯ut) + Bt˜λt+1\n14\n¯ut ←¯ut + ηgt\n// Gradient ascent\n17\nEBERHARD VERNADE MUEHLEBACH\nAlgorithm B.3: Model-free off-trajectory open-loop RL\nInput: Forgetting factor α ∈[0, 1], noise scale σ > 0, initial precision q0 > 0,\noptimization steps N ∈N, step size η > 0\n1 Initialize ¯u0:T−1 (initial action sequence)\n2 Initialize Ft ∈RD×(D+K+1), ∀t ∈[T −1]0\n3 Qt ←q0I ∈R(D+K+1)×(D+K+1), ∀t ∈[T −1]0\n4 for k = 1, 2, . . . , N do\n// Forward pass\n5\nu0:T−1 ∼N(¯u0:T−1, σI)\n6\nx0:T ←rollout(u0:T−1)\n// Backward pass\n7\n˜λT ←∇rT (xT )\n8\nfor t = T −1, T −2, . . . , 0 do\n// Jacobian estimation\n9\nzt ←[x⊤\nt u⊤\nt 1]⊤\n10\nQt ←αQt + (1 −α)q0I + ztz⊤\nt\n11\nFt ←Ft + Q−1\nt zt(xt+1 −Ftzt)⊤\n12\n[A⊤\nt B⊤\nt ct] ←Ft\n// Pontryagin update\n13\n˜λt ←∇xr(xt, ut) + At˜λt+1\n14\ngt ←∇ur(xt, ut) + Bt˜λt+1\n15\n¯ut ←¯ut + ηgt\n// Gradient ascent\nAppendix C. Derivation of Pontryagin’s principle\nIn this section, we will derive Pontryagin’s principle, (1) to (3), using the method of Lagrange\nmultipliers. In the following we view the objective J as a function of states and actions, that is\nJ(x0:T , u0:T−1) .=\nT−1\nX\nt=0\nr(xt, ut) + rT (xT ).\nWe maximize J with respect to x0:T and u0:T−1 subject to the constraint that xt+1 = f(xt, ut) for\nall t = [T −1]0. The corresponding Lagrangian is\nL(x0:T , u0:T−1, λ1:T ) .=\nT−1\nX\nt=0\n{r(xt, ut) + λ⊤\nt+1(f(xt, ut) −xt+1)} + rT (xT ),\nwhere the constraints are included through the multipliers λ1:T . The costate equations are then\nobtained by setting the partial derivatives of the Lagrangian with respect to x0:T to zero:\n∇xtL = ∇xr(xt, ut) + ∇xf(xt, ut)λt+1 −λt .= 0\n=⇒\nλt = ∇xr(xt, ut) + ∇xf(xt, ut)λt+1\n18\nA PONTRYAGIN PERSPECTIVE ON REINFORCEMENT LEARNING\n∇xT L = ∇rT (xT ) −λT .= 0\n=⇒\nλT = ∇rT (xT ).\nSetting the partial derivatives of the Lagrangian with respect to λ1:T to zero yields the dynamics\nequations, and the partial derivatives of the Lagrangian with respect to u0:T−1 are\n∇utL = ∇ur(xt, ut) + ∇uf(xt, ut)λt+1,\nwhich is the same expression for the gradient of the objective as in (1).\nAppendix D. Pontryagin’s principle from backpropagation\nIn Section 3.2, we mention that an application of the backpropagation algorithm (i.e., a repeated\napplication of the chain rule) to the objective J leads naturally to Pontryagin’s principle. We have\nJ(u0:T−1) =\nT−1\nX\nt=0\nr(xt, ut) + rT (xT )\n= r(x0, u0) + r{f(x0, u0)\n|\n{z\n}\nx1\n, u1} + r{f(f(x0, u0), u1)\n|\n{z\n}\nx2\n, u2} + · · ·\n+ r{f(f(· · · f(f(x0, u0), u1) · · · ), uT−2)\n|\n{z\n}\nxT −1\n, uT−1}\n+ rT {f(f(· · · f(f(x0, u0), u1) · · · ), uT−1)\n|\n{z\n}\nxT\n}.\nThe chain rule states that for g : Rn →Rk, h : Rk →Rm and x ∈Rn,\n∇(h ◦g)(x) = ∇g(x) ∇h{g(x)},\nwhere ∇g : Rn →Rn×k, ∇h : Rk →Rk×m and ∇(h ◦g) : Rn →Rn×m. From this, we can\ncompute the gradient of the objective function with respect to the action ut at time t ∈[T −1]0 as\n∇utJ(u0:T−1) = ∇ur(xt, ut) + ∇uf(xt, ut)∇xr(xt+1, ut+1)\n+ ∇uf(xt, ut)∇xf(xt+1, ut+1)∇xr(xt+2, ut+2)\n+ · · ·\n+ ∇uf(xt, ut)∇xf(xt+1, ut+1) · · · ∇xf(xT−2, uT−2)∇xr(xT−1, uT−1)\n+ ∇uf(xt, ut)∇xf(xt+1, ut+1) · · · ∇xf(xT−1, uT−1)∇rT (xT )\n= ∇ur(xt, ut) + ∇uf(xt, ut)λt+1,\nwhere we have introduced the shorthand λt+1 for the blue part. This is the same expression for the\ngradient as in (1), and it can easily be seen that this definition of λt satisfies the costate equations (2)\nand (3).\nAppendix E. Proof of Theorem 4\nIn this section, we prove Theorem 4, our convergence result of Algorithm 1. The main part of\nthe proof is contained in the proof of Theorem 7, which provides a lower bound for the inner\n19\nEBERHARD VERNADE MUEHLEBACH\nproduct between the approximate and true gradients as well as an upper bound for the norm of the\napproximate gradients. Intuitively, this theorem turns Assumption 2, which is a statement about the\nerror of the approximate Jacobians, into a statement about the error of the approximate gradient.\nWe then show that Theorem 4 follows by making use of the L-smoothness (Assumption 3) of the\nobjective function. This latter part is a standard result in the analysis of stochastic gradient methods\n(e.g., Bottou et al., 2018).\nBefore coming to the main result, we introduce the following shorthand notation. Given a fixed\ntrajectory (u0:T−1, x0:T ), we define\n∇Jt .= ∇uftλt+1,\nεt .= At −∇xft,\nε′\nt\n.= Bt −∇uft\nand\nδt+1 .= ˜λt+1 −λt+1\nfor all times t ∈[T −1]0. By (1) and Assumption 1, the first quantity defines the true gradient and\nthe approximate gradient is given by gt = Bt˜λt+1. We also state two small lemmas, which we will\nuse routinely in the following proof.\nLemma 5 Let x, y ∈Rn for n ∈N and α ∈R such that ∥x∥≤α∥y∥. Then, |x⊤y| ≤α∥y∥2.\nProof ∥x∥≤α∥y∥=⇒∥x∥∥y∥≤α∥y∥2 =⇒|x⊤y| ≤α∥y∥2 (by Cauchy-Schwarz).\nLemma 6 Let A, B ∈Rm×n for some m, n ∈N and x, y ∈Rn such that ¯σ(A)∥x∥≤¯σ(B)∥y∥.\nThen, ∥Ax∥≤∥By∥.\nProof This is a simple corollary of the Courant-Fischer (min-max) theorem. The min-max theorem\nstates that, for a symmetric matrix C ∈Rn×n, the minimum and maximum eigenvalues ¯λ(C) and\n¯λ(C) are characterized in the following way:\n¯λ(C) = min\nz∈Rn\n∥z∥=1\nz⊤Cz\nand\n¯λ(C) = max\nz∈Rn\n∥z∥=1\nz⊤Cz.\nThis can be extended to a characterization of the singular values ¯σ(A) and ¯σ(B) by relating them to\nthe eigenvalues of A⊤A and B⊤B, respectively:\n¯σ(A) =\nq\n¯λ(A⊤A) = max\nz∈Rn\n∥z∥=1\n√\nz⊤A⊤Az = max\nz∈Rn\n∥z∥=1\n∥Az∥≥\n1\n∥x∥∥Ax∥,\n¯σ(B) =\nq\n¯λ(B⊤B) = min\nz∈Rn\n∥z∥=1\n√\nz⊤B⊤Bz = min\nz∈Rn\n∥z∥=1\n∥Bz∥≤\n1\n∥y∥∥By∥.\nCombining these inequalities, we get:\n∥Ax∥≤¯σ(A)∥x∥≤¯σ(B)∥y∥≤∥By∥.\nTheorem 7 Suppose Assumptions 1 and 2 hold with γ and ζ and define µ .= 1 −γ −ζ −γζ and\nν .= 1 + γ + ζ + γζ. Then,\ng⊤\nt ∇Jt ≥µ∥∇Jt∥2\nand\n∥gt∥≤ν∥∇Jt∥,\nfor all t ∈[T −1]0.\n20\nA PONTRYAGIN PERSPECTIVE ON REINFORCEMENT LEARNING\nProof Let t ∈[T −1]0 be fixed. Decomposing the left-hand side of the first inequality, we get\ng⊤\nt ∇Jt = ˜λ⊤\nt+1B⊤\nt ∇uftλt+1\n= (λt+1 + δt+1)⊤(∇uft + ε′\nt)⊤∇uftλt+1\n= ∥∇Jt∥2 + λ⊤\nt+1ε′⊤\nt ∇uftλt+1\n|\n{z\n}\na\n+ δ⊤\nt+1∇uf⊤\nt ∇uftλt+1\n|\n{z\n}\nb\n+ δ⊤\nt+1ε′⊤\nt ∇uftλt+1\n|\n{z\n}\nc\n≥∥∇Jt∥2 −|a| −|b| −|c|.\nWe will now show that\n|a| ≤ζ∥∇Jt∥2\nand\n|b| ≤γ∥∇Jt∥2\nand\n|c| ≤γζ∥∇Jt∥2,\nwhich, when taken together, will give us\ng⊤\nt ∇Jt ≥(1 −γ −ζ −γζ)∥∇Jt∥2 = µ∥∇Jt∥2.\nWe first derive the bound on |a|:\n¯σ(ε′\nt) ≤ζ¯σ(∇uft)\n(Assumption 2b)\n=⇒\n∥ε′\ntλt+1∥≤ζ∥∇uftλt+1∥\n(Theorem 6)\n(8)\n=⇒\n| λ⊤\nt+1ε′⊤\nt ∇uftλt+1\n|\n{z\n}\na\n| ≤ζ∥∇Jt∥2.\n(Theorem 5)\nThe expression for b involves δt+1, which is the error of the approximate costate ˜λt+1. This error\ncomes from the cumulative error build-up due to εt+1:T−1, the errors of the approximate Jacobians\nused in the backward pass. To bound |b| we therefore first need to bound this error build-up. To this\nend, we now show that for all s ∈[T −t],\n∥δt+s∥≤\nγ\n3s−1 κ−1(∇uft)\ns−1\nY\ni=1\nκ−1(∇xft+i)∥λt+s∥,\n(9)\nwhere we write the inverse condition number of a matrix A as κ−1(A) .= ¯σ(A)/¯σ(A). To prove this\nbound, we perform a backward induction on s. First, consider s = T −t. The right-hand side of (9)\nis clearly nonnegative. The left-hand side is\n∥δT ∥= ∥˜λT −λT ∥= 0,\nas ˜λT = λT . Thus, the inequality holds for s = T −t. We now complete the induction by showing\nthat it holds for any s ∈[T −t −1], assuming that it holds for s + 1. We start by decomposing δt+s:\nδt+s = ˜λt+s −λt+s\n= At+s˜λt+s+1 −∇xft+sλt+s+1\n= (∇xft+s + εt+s)(λt+s+1 + δt+s+1) −∇xft+sλt+s+1\n= εt+sλt+s+1 + ∇xft+sδt+s+1 + εt+sδt+s+1.\n21\nEBERHARD VERNADE MUEHLEBACH\nNow, we can bound ∥δt+s∥by bounding these individual contributions:\n∥δt+s∥≤∥εt+sλt+s+1\n|\n{z\n}\na′\n∥+ ∥∇xft+sδt+s+1\n|\n{z\n}\nb′\n∥+ ∥εt+sδt+s+1\n|\n{z\n}\nc′\n∥.\nWe start with ∥a′∥:\n¯σ(εt+s) ≤γ\n3s κ−1(∇uft)\ns−1\nY\ni=1\nκ−1(∇xft+i)¯σ(∇xft+s)\n(Assumption 2a)\n=⇒\n∥εt+sλt+s+1\n|\n{z\n}\na′\n∥≤γ\n3s κ−1(∇uft)\ns−1\nY\ni=1\nκ−1(∇xft+i)∥∇xft+sλt+s+1\n|\n{z\n}\nλt+s\n∥.\n(Theorem 6)\nNow, ∥b′∥:\n∥δt+s+1∥≤γ\n3s κ−1(∇uft)\ns\nY\ni=1\nκ−1(∇xft+i)∥λt+s+1∥\n(Induction hypothesis)\n⇐⇒\n¯σ(∇xft+s)∥δt+s+1∥≤γ\n3s κ−1(∇uft)\ns−1\nY\ni=1\nκ−1(∇xft+i)¯σ(∇xft+s)∥λt+s+1∥\n(Definition of κ−1)\n=⇒\n∥∇xft+sδt+s+1\n|\n{z\n}\nb′\n∥≤γ\n3s κ−1(∇uft)\ns−1\nY\ni=1\nκ−1(∇xft+i)∥∇xft+sλt+s+1\n|\n{z\n}\nλt+s\n∥. (Theorem 6)\nAnd finally, ∥c′∥:\n¯σ(εt+s) ≤¯σ(∇xft+s) ≤¯σ(∇xft+s)\n(10)\n=⇒\n¯σ(εt+s)∥δt+s+1∥≤γ\n3s κ−1(∇uft)\ns\nY\ni=1\nκ−1(∇xft+i)¯σ(∇xft+s)∥λt+s+1∥\n(Induction hypothesis)\n⇐⇒\n¯σ(εt+s)∥δt+s+1∥≤γ\n3s κ−1(∇uft)\ns−1\nY\ni=1\nκ−1(∇xft+i)¯σ(∇xft+s)∥λt+s+1∥\n(Definition of κ−1)\n=⇒\n∥εt+sδt+s+1\n|\n{z\n}\nc′\n∥≤γ\n3s κ−1(∇uft)\ns−1\nY\ni=1\nκ−1(∇xft+i)∥∇xft+sλt+s+1\n|\n{z\n}\nλt+s\n∥.\n(Theorem 6)\nHere, (10) follows from Assumption 2a by noting that that the constant before ¯σ(∇xft+s) on the\nright-hand side is not greater than 1. We can now put all three bounds together to give us (9):\n∥δt+s∥≤∥a′∥+ ∥b′∥+ ∥c′∥≤3 · γ\n3s κ−1(∇uft)\ns−1\nY\ni=1\nκ−1(∇xft+i)∥λt+s∥.\n22\nA PONTRYAGIN PERSPECTIVE ON REINFORCEMENT LEARNING\nEquipped with a bound on δt+s, we are ready to bound |b| and |c|. Starting with |b|, we have:\n∥δt+1∥≤γκ−1(∇uft)∥λt+1∥\n((9) for s = 1)\n⇐⇒\n¯σ(∇uft)∥δt+1∥≤γ¯σ(∇uft)∥λt+1∥\n(Definition of κ−1)\n=⇒\n∥∇uftδt+1∥≤γ∥∇uftλt+1∥\n(Theorem 6)\n(11)\n=⇒\n| δ⊤\nt+1∇uf⊤\nt ∇uftλt+1\n|\n{z\n}\nb\n| ≤γ∥∇Jt∥2.\n(Theorem 5)\nAnd finally, we can bound |c|:\n¯σ(ε′\nt) ≤ζ¯σ(∇uft) ≤ζ¯σ(∇uft)\n(Assumption 2b)\n=⇒\n¯σ(ε′\nt)κ−1(∇uft)∥λt+1∥≤ζ¯σ(∇uft)∥λt+1∥\n(Definition of κ−1)\n=⇒\n¯σ(ε′\nt)∥δt+1∥≤γζ¯σ(∇uft)∥λt+1∥\n((9) for s = 1)\n=⇒\n∥ε′\ntδt+1∥≤γζ∥∇uftλt+1∥\n(Theorem 6)\n(12)\n=⇒\n| δ⊤\nt+1ε′⊤\nt ∇uftλt+1\n|\n{z\n}\nc\n| ≤γζ∥∇Jt∥2.\n(Theorem 5)\nThis concludes the proof of the first inequality showing that\ng⊤\nt ∇Jt ≥µ∥∇Jt∥2.\nThe second inequality,\n∥gt∥≤ν∥∇Jt∥,\nfollows easily from the work we have already done. To show this, we start by decomposing gt:\ngt = Bt˜λt+1\n= (∇uft + ε′\nt)(λt+1 + δt+1)\n= ∇Jt + ∇uftδt+1 + ε′\ntλt+1 + ε′\ntδt+1.\nTo bound the norm of gt, we again make use of the triangle inequality:\n∥gt∥≤∥∇Jt∥+ ∥∇uftδt+1∥+ ∥ε′\ntλt+1∥+ ∥ε′\ntδt+1∥\n≤(1 + γ + ζ + γζ)∥∇Jt∥\n= ν∥∇Jt∥,\nwhere we have used (8), (11) and (12).\nProof of Theorem 4. Let N ∈N and t ∈[T −1]0 be fixed. In Algorithm B.1, the iterates are\ncomputed, for all k ∈[N −1]0, as\nu(k+1)\nt\n= u(k)\nt\n+ ηg(k)\nt\n,\nwhere g(k)\nt\nis the approximate gradient at iteration k. We denote the true gradient at iteration k by\n∇J(k)\nt\n. From the L-smoothness of the objective function (Assumption 3), it follows that\nJ(u(k+1)\n0:T−1) ≥J(u(k)\n0:T−1) + ∇utJ(u(k)\n0:T−1)⊤(u(k+1)\nt\n−u(k)\nt ) −L\n2 ∥u(k+1)\nt\n−u(k)\nt ∥2\n23\nEBERHARD VERNADE MUEHLEBACH\n= J(u(k)\n0:T−1) + ∇J(k)\nt\n⊤(ηg(k)\nt\n) −L\n2 ∥ηg(k)\nt\n∥2\n≥J(u(k)\n0:T−1) + ηµ∥∇J(k)\nt\n∥2 −η2Lν2\n2\n∥∇J(k)\nt\n∥2\n(Theorem 7)\n= J(u(k)\n0:T−1) + η\n\u0012\nµ −ηLν2\n2\n|\n{z\n}\nα\n\u0013\n∥∇J(k)\nt\n∥2.\nTheorem 4 demands that η > 0 is set small enough such that α > 0, which is possible because\n0 < µ < ν and L > 0. Thus, we get\nηα∥∇J(k)\nt\n∥2 ≤J(u(k+1)\n0:T−1) −J(u(k)\n0:T−1)\n=⇒\n1\nN\nN−1\nX\nk=0\n∥∇J(k)\nt\n∥2 ≤\n1\nαηN\nN−1\nX\nk=0\nn\nJ(u(k+1)\n0:T−1) −J(u(k)\n0:T−1)\no\n=\n1\nαηN\nn\nJ(u(N)\n0:T−1) −J(u(0)\n0:T−1)\no\n≤J⋆−J(u(0)\n0:T−1)\nαηN\n,\nwhere J⋆.= supu∈UT J(u) is the optimal value of the initial state.\nAppendix F. Inverted pendulum swing-up task\nWe give a brief description of the inverted pendulum system on which we evaluate our algorithms\nin Section 4.1. The setup is shown in Fig. 3. The state at time t is xt = (ℓ, ˙ℓ, θ, ˙θ)t ∈R4, where\nℓis the position of the cart on the bar and θ is the (signed) pendulum angle. The action ut is the\nhorizontal force F (in units of 50 N) applied to the cart at time t. Episodes are of length T = 100, the\nrunning reward r(x, u) = −0.001u2 penalizes large forces, and the terminal reward rT (x) = −∥x∥1\ndefines the goal state to be at rest in the upright position. The system has five parameters: the mass\nof the cart (m1 = 1 kg), the mass of the pendulum tip (m2 = 0.1 kg), the length of the pendulum\n(m3 = 0.5 m), the friction coefficient for linear motion (m4 = 0.01 N s m−1), and the friction\ncoefficient for rotational motion (m5 = 0.01 N m s rad−1). These are the model parameters that are\nrandomly sampled to test the robustness of our model-based algorithm in Section 4.1. We say that\nthe swing-up task is solved if Jmax > −0.03. This threshold was determined empirically. If the\nalgorithm or the model is randomized, then [Jmax > −0.03] is a Bernoulli random variable whose\nmean, which we call the solve rate, depends on the quality of the learning algorithm.\nAppendix G. Further experiments\nIn this section, we repeat the MuJoCo experiments of Section 4.2 with longer time-horizons T. The\nresults are shown in Fig. 8. Our algorithms are sensitive to the horizon T due to the backpropagation\nof the costates. At each propagation step, the approximation errors of the Jacobians amplify the\nerrors of the costates. For this reason, Theorem 4 demands (through Assumption 2) more accurate Ja-\ncobians at later time steps. Thus, for large T, our convergence result requires more accurate Jacobian\nestimates. However, in Fig. 8, we see that Algorithm B.3 is able to cope with longer horizons for the\ntwo MuJoCo environments. The reason for this discrepancy between our theoretical and empirical\n24\nA PONTRYAGIN PERSPECTIVE ON REINFORCEMENT LEARNING\n0\n5000\n10000\n15000\n20000\nEpisodes\n100\n200\n300\n400\n500\nAverage return J\nAnt-v4\n0\n5000\n10000\n15000\n20000\nEpisodes\n0\n100\n200\n300\n400\nHalfCheetah-v4\nT = 100\nT = 200\nT = 300\nFigure 8: Learning curves of our off-trajectory algorithm. All experiments were repeated with 20\nrandom seeds, and we show 95%-bootstrap confidence intervals for the average return.\nresult is that Theorem 4 does not consider the stability of the system under consideration. The two\nMuJoCo systems, Ant-v4 and HalfCheetah-v4, are stable along the trajectories encountered\nduring training, which prevents an exponential build-up of error in the costate propagation.\nAppendix H. Baselines\nWe compare our algorithms against two baselines: the finite-difference approach discussed at the end\nof Section 3.3 and the gradient-free cross-entropy method (CEM; Rubinstein, 1999). These methods\nare listed in Algorithms H.1 and H.2. In both algorithms, we perform M ∈N rollouts of perturbed\naction sequences {ui ∼N(¯u, σI)}M\ni=1. Here, ¯u is the current action sequence and σ > 0 is a noise\nscale parameter. In CEM, we then construct the elite set S of the L < M perturbed action sequences\nwith the highest returns, where L ∈N is a hyperparameter. Finally, the current action sequence ¯u is\nupdated to be the mean of the elite sequences, such that ¯u ←1\nL\nP\nu∈S u.\nWhile the gradient-free nature of this method can make it more efficient than the finite-difference\napproach, it still suffers from the same fundamental deficiency: it ignores the Markovian structure\nof the RL problem and treats the objective function J as a black box. CEM is commonly used\nin model-based closed-loop reinforcement learning for planning. In this setting, the rollouts are\nhallucinated using the approximate model. Instead of executing the complete open-loop trajectory,\nthe model-predictive control framework is typically employed. The planning procedure is repeated\nafter each step in the real environment with the executed action being the first item in the planned\naction sequence. Thus, this setting is very different from our open-loop RL objective. For this reason,\nwe slightly modify the CEM algorithm to better fit our requirements. In model-based RL, typically\nboth mean ¯u and standard variation σ are adapted in CEM (Hafner et al., 2019; Pinneri et al., 2021).\nIn our experiments, this approach led to very fast convergence (σ →0) to suboptimal trajectories.\nWe thus only fit the mean and keep the noise scale fixed, which we empirically observed to give\nmuch better results.\nAppendix I. Hyperparameters\nUnless stated otherwise, we used the hyperparameters listed in Table 1 in the inverted penulum\nexperiments of Section 4.1, and those listed in Table 2 in the MuJoCo experiments of Section 4.2\nand Appendix G. In each experiment, all actions in the initial action trajectory u(0)\n0:T−1 are sampled\nfrom a zero-mean Gaussian distribution with standard deviation 0.01. We use the Adam optimizer\n25\nEBERHARD VERNADE MUEHLEBACH\nAlgorithm H.1: Finite-difference method\nInput: Number of rollouts M ∈N, noise scale σ > 0, step size η > 0\n1 Initialize ¯u0:T−1 (initial action sequence)\n2 ¯u ←vec(¯u0:T−1) ∈RTK\n3 for k = 1, 2, . . . , N do\n// Forward passes\n4\n¯x0:T ←rollout(¯u0:T−1)\n5\n¯J ←PT−1\nt=0 r(¯xt, ¯ut) + rT (¯xT )\n6\nfor i = 1, 2, . . . , M do\n7\nu0:T−1 ∼N(¯u0:T−1, σI)\n8\nx0:T ←rollout(u0:T−1)\n9\nui ←vec(u0:T−1) ∈RTK\n10\nJi ←PT−1\nt=0 r(xt, ut) + rT (xT )\n// Gradient estimation\n11\ng ←arg ming∈RT K PM\ni=1{Ji −¯J −g⊤(ui −¯u)}2\n// Gradient ascent\n12\n¯u ←¯u + ηg\n13\n¯u0:T−1 ←reshape(¯u) ∈RT×K\nAlgorithm H.2: Cross-entropy method\nInput: Number of rollouts M ∈N, noise scale σ > 0, size of elite set L ∈N\n1 Initialize ¯u0:T−1 (initial action sequence)\n2 ¯u ←vec(¯u0:T−1) ∈RTK\n3 for k = 1, 2, . . . , N do\n// Forward passes\n4\nfor i = 1, 2, . . . , M do\n5\nu0:T−1 ∼N(¯u0:T−1, σI)\n6\nx0:T ←rollout(u0:T−1)\n7\nui ←vec(u0:T−1) ∈RTK\n8\nJi ←PT−1\nt=0 r(xt, ut) + rT (xT )\n// Elite set computation\n9\nS ←arg partitionL{(−Ji)M\ni=1}1:L\n// Action sequence update\n10\n¯u ←1\nL\nP\ni∈S ui\n11\n¯u0:T−1 ←reshape(¯u) ∈RT×K\n(Kingma and Ba, 2014) both for training the MLP model and for performing the gradient ascent steps\nin Algorithms B.1 to B.3 and H.1. We did not optimize the hyperparameters of soft actor-critic (SAC),\nbut kept the default values suggested by Haarnoja et al. (2018a), as these are already optimized for\nthe MuJoCo environments. The entropy coefficient of the SAC algorithm is tuned automatically\n26\nA PONTRYAGIN PERSPECTIVE ON REINFORCEMENT LEARNING\nTable 1: Pendulum experiments hyperparameters\nParameter\nValue\nNumber of optimization steps N\n50000\nStep size η\n0.001\nNoise scale σ\n0.001\nNumber of perturbed rollouts M\n10\nForgetting factor α\n0.8\nInitial precision q0\n0.001\nCross-entropy method: M5\n20\nFinite-difference method: M5\n20\nFinite-difference method: σ5\n0.0001\nMLP model: hidden layers\n[16, 16]\nMLP model: training rollouts\n1000\nMLP model training: epochs\n10\nMLP model training: batch size\n100\nMLP model training: step size\n0.002\nMLP model training: weight decay\n0.001\nTable 2: MuJoCo experiments hyperparameters\nParameter\nValue\nNumber of optimization steps N\n20000\nStep size η\n0.0001\nNoise scale σ\n0.03\nInitial precision q0\n0.0001\nForgetting factor α\nHalfCheetah-v4, T = 100\n0.9\nHalfCheetah-v4, T = 200\n0.8\nHalfCheetah-v4, T = 300\n0.8\nAnt-v4, T = 100\n0.95\nAnt-v4, T = 200\n0.9\nAnt-v4, T = 300\n0.85\naccording to the procedure described by Haarnoja et al. (2018b). In our experiments, we make use of\nthe Stable-Baselines3 (Raffin et al., 2021) implementation of SAC.\nFor our off-trajectory method, we found it worthwile to tune the forgetting factor α to the specific\ntask at hand. Large α means that data is retained for longer, which both makes the algorithm more\nsample efficient (i.e., faster convergece) and the Jacobian estimates more biased (i.e., convergence to\na worse solution). In Fig. 9, we show this trade-off in the learning curves for the MuJoCo tasks (with\nthe horizon T = 200). We found that the performance is much less senstitive to the choice of noise\nscale σ and initial precision q0 than to the choice of the forgetting factor α.\n0\n5000\n10000\n15000\n20000\nEpisodes\n200\n250\n300\n350\nAverage return J\nAnt-v4\n0\n5000\n10000\n15000\n20000\nEpisodes\n0\n100\n200\n300\nHalfCheetah-v4\n0.8\n0.85\n0.9\n0.95\n0.99\nForgetting factor α\nFigure 9: Analysis of the influence of the forgetting factor α on the performance of the off-trajectory\nmethod (Algorithm B.3) in the MuJoCo environments (T = 200). All experiments were repeated\nwith 20 random seeds, and we show 95%-bootstrap confidence intervals for the average return.\n5. This value was chosen on the basis of the experiment presented in Fig. 5.\n27\n",
  "categories": [
    "cs.LG",
    "math.OC"
  ],
  "published": "2024-05-28",
  "updated": "2024-11-28"
}