{
  "id": "http://arxiv.org/abs/2405.18281v1",
  "title": "MODL: Multilearner Online Deep Learning",
  "authors": [
    "Antonios Valkanas",
    "Boris N. Oreshkin",
    "Mark Coates"
  ],
  "abstract": "Online deep learning solves the problem of learning from streams of data,\nreconciling two opposing objectives: learn fast and learn deep. Existing work\nfocuses almost exclusively on exploring pure deep learning solutions, which are\nmuch better suited to handle the \"deep\" than the \"fast\" part of the online\nlearning equation. In our work, we propose a different paradigm, based on a\nhybrid multilearner approach. First, we develop a fast online logistic\nregression learner. This learner does not rely on backpropagation. Instead, it\nuses closed form recursive updates of model parameters, handling the fast\nlearning part of the online learning problem. We then analyze the existing\nonline deep learning theory and show that the widespread ODL approach,\ncurrently operating at complexity $O(L^2)$ in terms of the number of layers\n$L$, can be equivalently implemented in $O(L)$ complexity. This further leads\nus to the cascaded multilearner design, in which multiple shallow and deep\nlearners are co-trained to solve the online learning problem in a cooperative,\nsynergistic fashion. We show that this approach achieves state-of-the-art\nresults on common online learning datasets, while also being able to handle\nmissing features gracefully. Our code is publicly available at\nhttps://github.com/AntonValk/MODL.",
  "text": "MODL: Multilearner Online Deep Learning\nAntonios Valkanas1∗\nBoris N. Oreshkin2†\nMark Coates1\n1McGill University\n2Amazon Science\nantonios.valkanas@mail.mcgill.ca\nboris.oreshkin@gmail.com\nmark.coates@mcgill.ca\nAbstract\nOnline deep learning solves the problem of learning from streams of data, recon-\nciling two opposing objectives: learn fast and learn deep. Existing work focuses\nalmost exclusively on exploring pure deep learning solutions, which are much bet-\nter suited to handle the “deep” than the “fast” part of the online learning equation.\nIn our work, we propose a different paradigm, based on a hybrid multilearner ap-\nproach. First, we develop a fast online logistic regression learner. This learner does\nnot rely on backpropagation. Instead, it uses closed form recursive updates of model\nparameters, handling the fast learning part of the online learning problem. We\nthen analyze the existing online deep learning theory and show that the widespread\nODL approach, currently operating at complexity O(L2) in terms of the number of\nlayers L, can be equivalently implemented in O(L) complexity. This further leads\nus to the cascaded multilearner design, in which multiple shallow and deep learners\nare co-trained to solve the online learning problem in a cooperative, synergistic\nfashion. We show that this approach achieves state-of-the-art results on common\nonline learning datasets, while also being able to handle missing features gracefully.\nOur code is publicly available at https://github.com/AntonValk/MODL.\n1\nIntroduction\nOff-line machine learning algorithms are trained on bulk datasets, making multiple passes over them\nto gradually tune model parameters. In many cases it is desirable to be able to learn from data\nstreams, processing each instance sequentially, in a process known as Online Learning [Bottou, 1998].\nCompared to batch training, online learning allows scalable and memory efficient training regimes,\nwith potentially unbounded dataset sizes. Online learning techniques have been a research area for\ndecades, spanning both supervised [Zinkevich, 2003], semi-supervised [Belkin et al., 2006] and\nunsupervised learning [Guha et al., 2000] archetypes. Early techniques focused on shallow learners\nthat learn fast, but have limited expressive ability [Hoi et al., 2013]. In contrast, deep neural networks\nprovide unlimited expressive power, but their learning tends to be very slow. Sahoo et al. [2018]\naddress this by introducing the ODL (online deep learning) paradigm that jointly learns neural network\nparameters and the architecture “hedge” weights. ODL is rooted in Hedge backpropagation [Freund\nand Schapire, 1997] that oscillates between standard backpropagation and the hedge optimization\nstep that updates the scalar weights that link deep neural network early exit points with the final\nlayer. More recently, Agarwal et al. [2023] relied on ODL to address the issue of unreliable features\nin the data stream. Though these two approaches partially alleviate the issues with training neural\nnetworks online, they still suffer from computational complexity and stability issues due to reliance\non hedge backpropagation. Indeed, at a certain abstraction level, hedge backpropagation has a tint of\na joint architecture learning task, consisting of two optimization objectives (hedge weights and neural\nnetwork weights) that interfere with each other, making learning slower.\n∗Corresponding author.\n†This work is not related to author’s position at Amazon.\nPreprint. Under review.\narXiv:2405.18281v1  [cs.LG]  28 May 2024\nContributions. In our current work we address these inefficiencies by introducing MODL, a multi-\nlearner online deep learning paradigm. This paradigm is based on the following key inductive biases:\n(i) learners must learn in parallel, removing cross-learner dependencies as much as possible; (ii)\nbackpropagation is slow, therefore some learners can benefit from efficient statistical approximations,\nmaking them learn extremely fast, in a closed form recursive style; (iii) combining learner outputs\nusing a delta-regime makes their learning cooperative and synergistic. In the nutshell, our contribu-\ntions are threefold: (i) MODL achieves state-of-the-art convergence speed and accuracy on standard\nbenchmarks; (ii) we derive a fast recursive logistic regression algorithm that continuously learns on a\nstream of data; and (iii) we reduce training complexity of the existing ODL method from O(L2) to\nO(L) by analyzing its backpropagation and removing unnecessary complexities.\n2\nRelated Work\nOnline learning. Early online learning work with statistical methods dates back at least to Bottou\n[1998]. A notable result from Bottou and LeCun [2003] shows that online learning algorithms can\nbe at least as efficient learners as standard full batch methods. First-order gradient based algorithms\nhave long been used in online learning due to their simplicity [Zinkevich, 2003, Bartlett et al., 2007].\nSecond-order methods strive to increase convergence speed, but suffer from high computational\ncost [Hazan et al., 2007, Dredze et al., 2008]. Sahoo et al. [2018] propose a hedge-backpropagation\nstyle update to jointly learn the optimal neural network architecture and parameters to dynamically\nchange the model depth as more data become available. Learning from a stream can mean that feature\nreliability is questionable. Initial approaches that address online learning with haphazard features\nfocused on non-deep learning techniques [Beyazit et al., 2019, He et al., 2019]. More recently, a\nscalable approach for online deep learning with unreliable features was proposed by Agarwal et al.\n[2023]. Online logistic regression was studied by Agarwal et al. [2021], who proposed FOLKLORE,\nan iterative optimization scheme that still lacks the closed form updates that are needed to improve\nefficiency. de Vilmarest and Wintenberger [2021] studied online optimization using extended Kalman\nfilter framework with applications to online GLM models. In this work we derive the online logistic\nregression directly from the Bayesian approximation of posterior.\nApplications. There are three primary application areas of Online Learning. First, there are settings\nwhere storing training data can be undesirable. Privacy is a well established human right that is all\ntoo often violated due to data breaches. A good way to alleviate the danger of exposing sensitive\ndata is to use them for training but never store them at all [Yang et al., 2022]. This necessitates the\nability to train models from scratch in a single pass over the data [Min et al., 2022]. Second, in\nmassive data generating processes, it is physically impossible to store all the generated data, and\nthus learning has to be performed online as old data are deleted. One such example is the enormous\namount of data generated during physics experiments at CERN [Basaglia et al., 2023]. CERN only\nretains a small fraction of data in permanent tapes, while discarding TeraBytes of data after temporary\nstorage. Models can train from these massive data streams online as new data arrive and old data are\ndeleted3. Third, Online Learning is particularly useful for model adaptation when there is a mismatch\nbetween training datasets and deployment settings. Due to the difficulty in obtaining data in medical\nsettings, it is often necessary to train on a small initial dataset and continue training online as new\ndata arrive [Graas et al., 2023]. Even when large amounts of data are available in computer vision\nsettings [Li et al., 2020], online learning techniques are necessary to adjust the model in the presence\nof distribution shift. Online learning to address distribution shifts is also useful in analyzing power\nsystems [Hu et al., 2021] and addressing non-stationary noise [Liu et al., 2022]. Distribution drift can\nalso necessitate online learning to adapt to dynamic settings for tasks like the detection of bad actors\nin Internet-of-Things networks [Zhang et al., 2020, Shao et al., 2021, Abdel Wahab, 2022].\n3\nOnline Learning with Missing Features\n3.1\nProblem Satement\nWe address the Online Learning task with data streams that contain potentially missing fea-\ntures. In this problem setting the data generating process produces a sequence of triplets P =\n{(z1, y1, Ω1), . . . , (zT , yT , ΩT )} sampled sequentially in T time steps. P consists of input features\n3https://home.cern/science/computing/storage\n2\nAlgorithm 1 Online Deep Learning (ODL) using Hedge Backpropagation\nRequire: Learning rate Parameter: η, discount parameter β\nInitialize: F(x) with L hidden layers and L + 1 classifiers f (l); α(l) =\n1\nL+1, ∀l = 0, . . . , L\nfor t = 1,...,T do\nReceive instance xt and predict ˆyt = Ft(xt) = PL\nl=0 α(l)\nt f (l)\nt\nvia eq. (3)\nReveal true value yt and calculate L(l)\nt\n= L(f (l)\nt (xt), yt), ∀l, . . . , L;\nUpdate Θ(l)\nt+1, ∀l = 0, . . . , L via eq. (4) and W (l)\nt+1, ∀l = 1, . . . , L via eq. (5);\nUpdate α(l)\nt+1 = α(l)\nt βL(l)\nt , ∀l = 0, . . . , L and normalize α(l)\nt+1 to sum to 1.\nend for\nzt ∈Rd, ground truth yt ∈Y (where Y has a fixed dimension and may be discrete or continuous\ndepending on the task) and the set of available input indices Ωt = (ωj)t ∈{0, 1}d. Ωt is a binary\nmask encoding observed entries, i.e., it is a vector of missingness indicators such that (ωj)t = 1 or\n(ωj)t = 0 according to whether zt(j) is observed. During training we do not have access to P; we\ncan only observe an incomplete dataset D:\nxt = zt ⊙Ωt + NA ⊙(1d −Ωt),\n(1)\nD = [(x1, Ω1, y1), . . . , (xT , ΩT , yT )] .\n(2)\nSince the problem setting is online learning, the goal is to train a new model from scratch in a\nstreaming dataset setup such that at time t, we have access to only the input features xt and the mask\nΩt . The input to the model is a concatenated vector that consists of xt and Ωt and has length 2d.\nAfter a prediction byt is made, we obtain the output labels yt. We do not have access to any previous\ntraining examples from D so each datapoint is used for training only once. Online learning models\nare evaluated by the cumulative predictive error across all time steps ETOTAL = P\nt ϵ(yt, byt). In\ngeneral, ϵ(.) is a non-negative valued cost function.\n3.2\nBackground\nAlgorithm 1 outlines the current online deep learning state-of-the-art method ODL, which is based on\na joint bi-level optimization objective. The learning (or selection) of the architecture is accomplished\nby optimizing α values. Each alpha connects one of the hidden layers with the overall neural network\noutput. The other optimization level handles the optimization of model parameters Θ. The algorithm\noscillates between taking an optimization step in the network weights and a “hedge” [Freund and\nSchapire, 1997] optimization step that effectively updates the architecture by assigning weights to the\nskip connections (see last step of Algorithm 1).\nConsider a deep neural network with L hidden layers where each layer is connected to an early exit\npredictor f (i). The prediction function F for the deep neural network is given by:\nF(x) =\nL\nX\nl=0\nα(l)f (l) ,\nwhere f (l) = softmax(h(l)Θ(l)), ∀l = 0, . . . , L\n(3)\nh(l) = σ(W(l)h(l−1)), ∀l = 1, . . . , L\nand h(0) = x ∈Rdin.\nHere, Θ(l) is the parameter matrix of the early exit classifier f (l), and W(l−1) is the parameter matrix\nof the hidden layer that yields intermediate representation h(l). Learning the parameters Θ(l) for\nclassifiers f (l) can be done via online gradient descent (OGD), where the input to the lth classifier is\nh(l). This is essentially standard backpropagation with learning rate η:\nΘ(l)\nt+1 ←Θ(l)\nt\n−η∇Θ(l)\nt L(F(xt, yt)) = Θ(l)\nt\n−ηα(l)∇Θ(l)\nt L(f (l), yt) .\n(4)\nUpdating the feature representation parameters W(l) potentially requires backpropagating through all\nclassifiers f (l). Thus, using the adaptive loss function, L(F(x), y) = PL\nl=0 α(l) L(f (l)(x), y), and\napplying OGD, the update rule for W(l) is given by:\nW(l)\nt+1 ←W(l)\nt\n−η\nL\nX\nj=l\nα(j)∇W(l) L(f (j), yt) ,\n(5)\n3\nwhere ∇W(l) L(f (j), yt) is computed via backpropagation from error derivatives of f (j). The summa-\ntion in the gradient term starts at j = l because the shallower classifiers do not depend on W(l) for\nmaking predictions. This training technique is summarized in Algorithm 1.\nWhen eq. (5) is implemented directly, a single backpropagation pass for this architecture incurs\na O(L2) cost, which is evident from its implementation code4. We also demonstrate this in the\nproof of Proposition 1. This can grow prohibitively expensive for deep networks and has had major\nimplications in the efficiency of subsequent research methods and applications that rely on ODL. For\nexample, works as recent as Aux-Drop [Agarwal et al., 2023] build on this framework. In the next\nsection, we first show how an equivalent update to eq. (5) can be derived by “rewiring” the network\nto reduce time complexity from O(L2) to O(L) per backpropagation update. This constitutes an\nimprovement and further motivates us to introduce MODL.\n4\nMethodology\n4.1\nFast Online Deep Learning\nFast ODL. During training, rather than backpropagating through L individual early exit classifier\nlosses, we propose aggregating the outputs and backpropagating through a linear combination of them\nto obtain LTOT. Detailed architecture schematics and analysis of this are provided in Appendix I.1.\nThis change in the network topology has important ramifications for training via backpropagation.\nNotably, this alters eq. (5) as follows:\nW (l)\nt+1 ←W (l)\nt\n−η∇W (l) LTOT,\nwhere\n(6)\nLTOT =\nh\nα(1), . . . , α(L)i h\nL(f (1), yt), . . . , L(f (L), yt)\niT\n=\nL\nX\nj=1\nα(j) L(f (j), yt) .\n(7)\nProposition 1. The updates described by eq. (5) and eq. (6) are equivalent, but the proposed training\nscheme has lower complexity. For a dataset of size n and a network with L layers, eq. (5) has training\ntime complexity O(nL2), but eq. (6) only requires O(nL) computation.\nProof. The proof is in Appendix I.1.\nDespite the speed up provided by Proposition 1, there remain two main issues with eq. (6). The first is\nthat there is still an explicit assumption that all the learners update via backpropagation, which limits\nour algorithm choices. Furthermore, the update procedure in Equation (7) isolates learners through\nseparate loss terms, preventing their synergistic co-training. In the next section we address both of\nthese limitations by introducing a non-backpropagation based fast learner and a parallel co-training\nscheme, in which the fast learner serves as a baseline for the deeper and slower learner. The broader\nimpacts of our methodology are discussed in Supplementary A.\n4.2\nMultilearner Online Deep Learning\nOur proposed architecture is a result of three core elements in our design philosophy. First, different\ndatasets are best treated by different model architectures. A true foundational model for online\nlearning needs to adaptively select the architecture best suited for a task as it is training. This solves\nthe shallow/deep model dilemma, as during different stages of training, the model can be guided\nmore by shallow or deep models. Second, the base architecture should be simple and modular, so that\ndifferent types of networks/layers can be incorporated. This allows a smooth transition, as more data\nbecome available, from relying predominantly on simple, learning-efficient models to focusing on\nmore complex, highly expressive models. Third, the overall architecture needs to be efficient and\nminimize time complexity.\nThe main drawback of neural networks in the online setting is their sensitivity to hyperparameters.\nDeep neural networks require low learning rates to be stable during training, but this entails overfitting\nand accumulating large errors for millions of optimization steps. On the other hand, fast learners are\n4The most popular ODL implementation has 171 stars and 44 forks at the time of writing and has quadratic\ntraining complexity: https://github.com/alison-carrera/onn\n4\nStack 2\nMLP\nStack 1\nOnline Logistic\nGlobal model\noutput \nStack M\nSet Learner\n+\n+\nInput\nInput\nInput\nData Stream\nLabels\nFeatures\nFigure 1: Multilearner Online Deep Learning. The dataset is streamed sequentially. Fast learners\nquickly adapt to the data distribution, providing a strong baseline for the deeper models. By\nsynergizing models with different bias-variance trade-offs, the overall architecture can quickly adapt\nto the data and eventually learn deep representations.\ntypically shallow and train quicker, but lack the ability to learn complex representations. For example,\nit can be very difficult to know the optimal width or learning rate of the network before the dataset has\nbeen streamed. To address this challenge we propose the learning architecture depicted in Figure 1.\nHere the base fast learner based on online logistic regression quickly learns the linear approximation\nto the solution and provides the strongest signal guiding the global model output initially. As more\ndata arrive, the second stage learner, implemented as an MLP, kicks in, providing only the delta on\ntop of the online logistic regression baseline. Finally, the Set Learner, which has a deep structure\nand the ability to represent the semantics of variables, provides the modeling output that is the finest\nand therefore the hardest to learn. For example, learning semantic embeddings of variable IDs may\ntake a long time, but this endows this part of the architecture with the ability to handle missing data\nand encode complex input-output relationships. Of course, this can be generalized beyond the three\naforementioned learning levels. We choose to outline the structure used in our experiments, which\nalso happens to capture the most important methodological thinking behind our approach. It is also\nimportant to note that since the task of the more complex learners is only to provide the delta on top\nof faster learners that train faster, they are much less sensitive to hyper-parameter selection.\nFast Learner. Consider the standard task of fitting a logistic regression model. Denote the dataset by\nA = {(xi, yi)}n\ni=1, with x ∈Rm, yi ∈{0, 1}. For a simple generalized linear model p(y|x, θ) =\nσ(xθ), with weights θ ∈Rm, where σ(·) is the logistic function, we assign a normal prior p(θ) =\nN(θ; m0, P0), with mean m0 ∈Rm and symmetric positive definite covariance matrix P0 ∈Rm×m.\nProposition 2. Assuming an input feature distribution for x that is approximately normal, and\nlinearizing the non-linear relationship, y = σ(θx), a quadratic approximation to the pos-\nterior of model weights after observing the n-th datapoint is given by the recursive formula\np(θ|{(xi, yi)}n\ni=1) ≈N(θ; mn, Pn), where:\nmn = mn−1 + Pn−1x⊤\nn (1 −σ(xnmn−1)) σ (xnmn−1) [yn −σ (xnmn−1)]\nxnPn−1x⊤\nn + Pn−1 [(1 −σ(xnmn−1)) σ (xnmn−1)]2\n,\n(8)\nPn = Pn−1 −\nPn−1x⊤\nn xnP⊤\nn−1 [(1 −σ(xnmn−1)) σ (xnmn−1)]2\nxnPn−1x⊤\nn + Pn−1 [(1 −σ(xnmn−1)) σ (xnmn−1)]2 .\n(9)\nProof. The proof is in Appendix G.\nAs a consequence of this recursive formula, we derive the sequential Algorithm 2, based on the\nquadratic approximation to the log-likelihood. This algorithm can process the dataset in one pass,\nwhile never storing any data in memory. The approximation is demonstrated in Figure 2, showing\nthat it is accurate close to the maximizer of the exact log-likelihood. Note that S−1\nt\nin Algorithm 2 is\na scalar, so there is no need to invert any matrices in our approach. The only memory requirement is\nstorage of the parameters of the normal posterior distribution. The parameters update proportionally\nto the “innovation” (eq. (12)), which is data dependent, rather than set by the user. This fast learner is\na high bias and low variance model. In the next section, we propose a highly expressive module that\nlies at the opposite end of the bias-variance spectrum.\n5\n−6\n−4\n−2\n0\n2\n4\n6\n−6\n−4\n−2\n0\n2\n4\n6\n−12000\n−10000\n−8000\n−6000\n−4000\n−2000\n−11500 \n−10500  −10000 \n−9500 \n −9000 \n−8500  −8000 \n −7500  −7000 \n−6500 \n −6000 \n−5500 \n −5000 \n −4500 \n −4000 \n −3500 \n −3000 \n −2500 \n −2000 \n −1500 \n −1500 \n −1000 \n −500 \nExact Log−Likelihood\n−6\n−4\n−2\n0\n2\n4\n6\n−6\n−4\n−2\n0\n2\n4\n6\n−10000\n−8000\n−6000\n−4000\n−2000\n−9000 \n −7500 \n −6500 \n −6000 \n−6000  −5500 \n −5000 \n −5000 \n −4500 \n −4500 \n −4000 \n−4000 \n −3500 \n −3500 \n −3000 \n −3000 \n −2500 \n −2500 \n −2000 \n−2000 \n −1500 \n −1500 \n −1000 \n −500 \nQuadratic approximation\nFigure 2: Exact log-likelihood vs. our proposed quadratic approximation. We see that close to\nthe data-generating parameters (marked as x) both our approximation and the exact log-likelihood\nfunction agree. See Appendix G.3 for toy dataset experiment details.\nAlgorithm 2 Online Memoryless Bayesian Logistic Regression\nRequire: Dataset A = {(xi, yi)}n\ni=1\nInitialize: p(θ) = N(θ; m0, P0), m0 = 0, where P0 = I\nReturn p(θ|{(xi, yi)}n\ni=1) = N(θ; mn, Pn)\nfor t = 1,...,n do\nReceive instance: xt and predict: ˆyt = σ(xtmt−1)\nReveal true value: yt and update parameters:\nSt = xtPt−1x⊤\nt + P [(1 −σ(xtmt−1)) σ (xtmt−1)]2 ,\n(10)\nKt = Pt−1X⊤\nt (1 −σ(xtmt−1)) σ (xtmt−1) S−1\nt\n,\n(11)\nmt = mt−1 + Kt[yt −σ (xtmt−1)],\n(12)\nPt = Pt−1 −KtStK⊤\nt .\n(13)\nend for\nSlow learner with set inputs. Rather than masking missing features with a zero or using deter-\nministic dropout, as is done in [Agarwal et al., 2023], we treat the input as a set that excludes\nany missing features. Recall that the data generating process produces a sequence of triplets\nP = {(z1, y1, Ω1), . . . , (zT , yT , ΩT )}. Then, the set of input features Xt can be expressed as:\nXt = {zt,j : Ωt,j = 1} ,\nIt = {j : Ωt,j = 1} ,\nD = [(X1, I1, y1), . . . , (XT , IT , yT )] .\n(14)\nThe size of the input feature set Xt is time varying. To allow the model to determine which inputs are\navailable, it is necessary to pass a set of feature IDs in an index set It. Our proposed set learning\nmodule follows closely the ProtoRes architecture (please refer to Oreshkin et al. [2022] for details).\nIt takes the index set It and maps each of its active index positions to a continuous representation to\ncreate feature ID embeddings. It then concatenates each ID embedding to the corresponding feature\nvalue of Xt. These feature values and ID embedding pairs are aggregated and summed to produce\nfixed dimensional vector representations that we denote as x0. The main components of our proposed\nset learning module are blocks, each consisting of L fully connected (FC) layers. Residual skip\nconnections are included in the architecture so that blocks can be bypassed. An input set X is mapped\nto x0 = EMB(X). The overall structure of the module at block r ∈{1, . . . , R} (see Fig. 4) is:\nhr,1 = FCr,1(xr−1),\n. . . ,\nhr,L = FCr,L(hr,L−1),\n(15)\nxr = RELU(Wrxr−1 + hr−1,L) ,\nˆyr = ˆyr−1 + QLhr,L,\n(16)\nwhere Wr, QL are learnable matrices. We connect R blocks sequentially to obtain global output ˆyR.\n6\n0\n250\n500\n750\n1000\n0.30\n0.36\n0.42\ngerman\nMODL\nAux-ODL\n0\n310\n621\n932\n1243\n0.22\n0.24\n0.26\n0.27\nsvmguide3\nMODL\nAux-ODL\n0\n4755\n9510\n14265\n19020\n0.30\n0.36\n0.42\nmagic04\nMODL\nAux-ODL\n0\n8140\n16280\n24420\n32561\n0.20\n0.24\n0.28\na8a\nMODL\nAux-ODL\n0\n250k\n500k\n750k\n1M\n0.36\n0.40\n0.44\n0.48\nHIGGS\nMODL\nAux-ODL\n0\n250k\n500k\n750k\n1M\n0.21\n0.24\n0.27\nSUSY\nMODL\nAux-ODL\nFigure 3: Comparison of normalized online error for ODL (with Aux-Drop) vs. our proposed model\nMODL on standard benchmarks. Shaded regions indicate 95% C.I. over multiple trials.\n4.3\nCombining the learners\nThere are many options for combining the learners. We considered three candidate choices. First,\nan intuitive approach is to consider the relative performance of the models in the data seen so\nfar. For example, in a classification task, in order to merge the model predictions we may take a\nweighted combination by weighting each classifier proportionally to its accuracy rate. This approach\nconceptually mirrors the existing ODL approach, albeit using α values derived directly from past\nprediction performance, as opposed to learning them via hedge backpropagation as in Algorithm 1.\nAnother approach is to combine the classifiers by multiplying their class probabilities (or summing the\nlogarithm of the logits for better numerical stability). This approach can be theoretically interpreted as\na joint predictive distribution across multiple independent predictors’ posteriors. Finally, we explore\nthe simple approach of summing raw model outputs. Empirically this works surprisingly well and\nstrongly outperforms model ensembling, for example (see Tab. 5). Furthermore, our experiments\nimply that this is due to the fast and weak models learning to synergize as they are co-trained. The\nstrong learners receive strong baseline predictions that they build on top of and refine rather than\ntrying to make a prediction from scratch. Our architecture is summarized in Figure 1.\n5\nEmpirical Results\nOur experiments provide empirical support to the following: (i) our online learning model MODL\nconsistently outperforms state-of-the-art online deep learning methods; (ii) new proposed modules,\nincluding the online logistic regression as well as the set learning component, work synergistically\nwithin the MODL framework; (iii) combining the modules is best done via summation, compared to a\nfew alternatives, such as a mixture of experts, for example; (iv) the Fast ODL optimization framework\nis significantly more efficient than hedge backpropagation.\nDatasets. We use common online deep learning datasets and replicate the setup from the prior work\nby Agarwal et al. [2023]. Our results are based on 6 datasets of varying sizes. As stated in the review\nsection, particle physics colliders produce enormous amounts of data which can require learning from\na stream. For this reason we select two large particle physics datasets, HIGGS and SUSY [Baldi et al.,\n2014]. HIGGS captures the process of the detection of the HIGGS boson during a particle accelerator\nexperiment. SUSY is a dataset relating to the detection of SUper SYmmetric (SUSY) particles.\nSimilar to HIGGS, the goal is to distinguish between measurements caused by a background noise\nprocess and the signal produced by the elusive super symmetric particles that are notoriously difficult\nto detect. We also test our model on german [Chang and Lin, 2011], a financial dataset where the goal\nis to identify consumer credit risk. Svmguide3 [Dua et al., 2017] is a synthetic binary classification\ndataset. Another physics dataset, magic04 [Chang and Lin, 2011], detects the presence of gamma\nparticle radiation in Cherenkov telescope images. Finally, a8a [Dua et al., 2017] compiles census\n7\nTable 1: Comparison on standard datasets: cumulative error (mean ± st. deviation) over 20 runs.\nEach feature is present with probability pf (independently of other features and time).\nDataset\nOLVF\nAux-Drop(ODL)\nAux-Drop(OGD)\nMODL (ours)\npf\ngerman\n333.4±9.7\n306.6 ± 9.1\n327.0±45.8\n286 ± 5.3\n0.73\nsvmguide3\n346.4±11.6\n296.9±1.3\n296.6±0.6\n288 ± 1.0\n0.72\nmagic04\n6152.4±54.7\n5607.15 ± 235.1\n5477.45 ± 299.3\n5124 ± 153\n0.68\na8a\n8993.8±40.3\n6700.4±124.5\n7261.8±283.5\n5670 ± 278\n0.75\nTable 2: Comparison on HIGGS and SUSY for various feature probabilities pf. The metric is the\nmean (± standard deviation) cumulative error in thousands over 5 runs.\npf\nHIGGS\nAuxDrop (ODL)\nMODL (ours)\n.01\n440.2 ± 0.1\n439.6 ± 0.1\n.20\n438.4 ± 0.1\n435.6 ± 0.4\n.50\n427.4 ± 0.7\n422.7 ± 0.3\n.80\n411.8 ± 0.4\n399.6 ± 0.2\n.95\n399.4 ± 1.0\n377.1 ± 0.5\n.99\n392.0 ± 1.0\n366.4 ± 0.5\npf\nSUSY\nAuxDrop (ODL)\nMODL (ours)\n.01\n285.0 ± 0.1\n283.0 ± 0.1\n.20\n274.8 ± 0.9\n271.8 ± 0.1\n.50\n256.6 ± 1.0\n252.0 ± 0.1\n.80\n237.0 ± 0.7\n230.6 ± 0.1\n.95\n226.2 ± 0.4\n217.5 ± 0.1\n.99\n222.3 ± 0.2\n212.2 ± 0.2\ndata with the task of predicting individuals with high incomes based on demographic information.\nMore details about the datasets can be found in Appendix C.\nTraining Methodology. We closely follow the experimental setup of Aux-Drop [Agarwal et al.,\n2023]. For small and medium datasets we run 20 independent trials, whereas for the large datasets\nwe run 5. The random input masks for missing features and network initializations are from the same\nseed to ensure fairness. For all methods at each training step we process a single instance, so the batch\nsize is fixed to 1. Additionally, since in learning from data streams it is not feasible to segregate the\ndata into training, validation and test sets we report the results for basic hyperparameters and check\nhow sensitive each approach is to the hyperparameters chosen which can be found in Appendix B.4.\nFor missing feature experiments we randomly mask all features with some probability (except for\nthe first two features that are always available). Note that this is done to replicate the setting of prior\nwork for the purpose of comparing the algorithms; our algorithm does not require any features to be\nalways available. For a detailed description of the hyperparameters, see Supplementary D.\nKey Results. Figure 3 shows that during training MODL remains consistently ahead of the current\nstate-of-the-art model Aux-ODL with respect to the classification error rate. Besides converging\nfaster, our approach also achieves a lower overall error rate at the end of training. These findings\nare replicated across datasets that vary in size over 3 orders of magnitude and with different feature\nmissingness levels. The detailed results for the cumulative miss-classification metric are summarized\nin Tables 1 and 2. For the small and medium datasets shown in Table 1 we can see that MODL\nis the clear top performer, reducing error on average by 11%. We note that the improvements are\nmeasured over 20 trials and are all statistically significant at the 0.05 level using a paired Wilcoxon\ntest. For the large datasets in Table 6 we conduct a detailed feature missingness experiment for 13\ndifferent noise levels with 5 trials per level. The results show that MODL is consistently improving\nthe state-of-the-art for all noise levels. This is an important result as it verifies the strength of our\napproach for a wide spectrum of feature noise, ranging from near-zero up to extreme levels of noise.\nThe efficiency of the Fast ODL optimization scheme described in Section 4.1 is empirically verified\nto correspond to the expected theoretical time complexity in Table 5. For this experiment we employ\nthe standard ODL algorithm and vary the number of layers and the embedding size. We see that in all\ncases our proposed optimization called “Fast ODL” reduces the the training time dramatically, e.g.,\nfrom 63 hours to 9.\nAblation Studies. In this section we rigorously verify our model components. Specifically, we look\nat the impact of removing on or more modules of our architecture. Table 4 shows that removing any\nindividual component negatively affects the overall performance. From these results we ascertain that\nthe presence of diverse learners with respect to the bias-variance trade-off bolsters performance.\n8\nTable 3: Ablation study on merging constituent learner outputs HIGGS (pf=0.5), SUSY (pf=0.99).\nDataset\nSoft rel. error\nMult.\nEnsemble\nMix. of Experts\nSum (ours)\ngerman\n293.0±10.3\n294.9±7.1\n307.5±21.0\n316.2±9.3\n285.9±7.2\nsvmguide3\n296.3±1.5\n299.5±6.5\n296.6±1.5\n298.4±3.3\n287.5±4.0\nmagic04\n4720±154\n5146±75\n6506± 588\n6719±44\n5226 ± 98\na8a\n6495±709\n5691±40\n5865±40\n6190±168\n5673±36\nHIGGS\n442.8±0.3k\n422.7±0.6k\n428.8±0.1k\n431.9±0.9k\n422.7±0.3k\nSUSY\n220.2±0.8k\n212.3±0.1k\n218.7±0.2k\n217.0±0.2k\n212.2±0.2k\nTable 4: Ablation of proposed model components. We show that for each learner that we add the\noverall model performance improves.\npf\nHIGGS\nSUSY\nOL + MLP\nOL + Set\nMODL (ours)\nOL + MLP\nOL + Set\nMODL (ours)\n.01\n442.7±0.2\n450.2±0.4\n439.6 ± 0.1\n285.3±0.1\n332.6±0.3\n283.0 ± 0.1\n.20\n439.9±0.1\n450.3±0.2\n435.6 ± 0.4\n274.0±0.1\n352.4±0.3\n271.8 ± 0.1\n.50\n430.3±0.2\n450.1±0.1\n422.8 ± 0.3\n255.0±0.1\n343.3±0.6\n252.0 ± 0.1\n.80\n411.7±0.1\n449.6±0.2\n399.6 ± 0.2\n234.3±0.1\n326.3±0.4\n230.6 ± 0.1\n.95\n394.1±0.5\n448.5±0.1\n377.1 ± 0.5\n222.1±0.1\n320.9±0.4\n217.5 ± 0.1\n.99\n386.6±0.1\n447.1±0.1\n366.4 ± 0.6\n217.3±0.1\n316.3±0.6\n212.2 ± 0.2\nThe second ablation experiment explores different ways to merge learner predictions. MODL merges\npredictions by direct summation of the constituent learners. We consider alternative approaches that\nincude: (i) learnable gating functions in a Mixture of Experts (MoE) style setup where the final output\nis a weighted sum of each learner’s prediction; (ii) multiplying the logit probabilities (by summing\nthe logarithm of the predicted logits); and (iii) a greedy weighting scheme that assigns more weight\nto learners with higher running accuracy.\nA particularly challenging and sensitive hyperparameter to select in online learning is the learning\nrate. We run sensitivity experiments with respect to the learning rate in the large bennchmarks. Our\nresults in Table 9 show increased robustness to learning rate selection. This is expected because\nour framework incorporates learners that train without backpropagation, which safeguards against\nselecting learning rates that are far from optimal.\nTable 5: Comparison in hours between Aux-Drop (ODL) and Fast ODL on HIGGS for pf = 0.5 with\n20 layers and embedding dim. E. We massively speed up training while maintaining performance.\nExperiment\nODL (AuxDrop)\nFast ODL (AuxDrop)\nTime ODL vs. Fast ODL\nHIGGS (E = 25)\n431.4 ± 0.5\n431.0 ± 0.5\n57:04:12 vs. 9:23:13\nHIGGS (E = 50)\n429.2 ± 0.4\n429.4 ± 0.4\n54:04:10 vs. 8:43:11\nHIGGS (E = 100)\n427.6 ± 0.5\n427.8 ± 0.5\n63:19:05 vs. 8:58:51\n6\nConclusion\nLimitations. Our work is focused on supervised learning from streams of data. This is not trivially\napplicable to other areas such as online reinforcement learning. While our work shows that including\nmultiple learners with different bias-variance trade-offs is a sensible approach to online deep learning,\nit does not show how many learners, and which types, are necessary to achieve specific error bounds.\nAlthough MODL works in a general multi-class setting our experiments focused on binary tasks.\nThis limitation applies to all standard methods in our area [Sahoo et al., 2018, Agarwal et al., 2023].\nOverview. We demonstrate that the problem of online deep learning is best handled synergistically\nby multiple learners that operate at various points of the convergence speed/expressivity trade-off\ncurve. We derive a very fast learning and non-backpropagation based approach to support early stages\nof learning and to provide a strong baseline for other learners. Then, at the other end of the spectrum,\nwe propose a slowly learning, yet very expressive set learner that is invariant to feature ordering and\nrobust to missing features. When appropriately combined, these approaches significantly improve\nperformance while reducing training complexity.\n9\nReferences\n[1] Omar Abdel Wahab. Intrusion detection in the IoT under data and concept drifts: Online deep\nlearning approach. IEEE J. Internet of Things, 9(20):19706–19716, 2022.\n[2] Naman Agarwal, Satyen Kale, and Julian Zimmert. Efficient methods for online multiclass\nlogistic regression. In Proc. Int. Conf. Alg. Learning Theory (ALT), Online, Mar. 2021.\n[3] Rohit Agarwal, Deepak Gupta, Alexander Horsch, and Dilip K. Prasad. Aux-drop: Handling\nhaphazard inputs in online learning using auxiliary dropouts. Trans. Mach. Learn. Research\n(TMLR), 2023. ISSN 2835-8856.\n[4] Pierre Baldi, Peter Sadowski, and Daniel Whiteson. Searching for exotic particles in high-energy\nphysics with deep learning. Nature Communications, 5, 2014.\n[5] Peter L. Bartlett, Elad Hazan, and Alexander Rakhlin. Adaptive online gradient descent. In\nProc. Neural Info. Proces. Sys. (NIPS), page 65–72, Red Hook, NY, USA, Dec. 2007.\n[6] T. Basaglia, Matthew Bellis, Jakob Blomer, J. Boyd, C. Bozzi, Daniel Britzger, S. Campana,\nConcetta Cartaro, G. M. Chen, B. Couturier, G. David, C. Diaconu, A. Dobrin, Dirk Duellmann,\nM. Ebert, Peter Elmer, J. Fernandes, L. Fields, P. Fokianos, Gerardo Ganis, Achim Geiser,\nMihaela Gheata, J. B. Gonzalez Lopez, T. Hara, L. Heinrich, Michael D. Hildreth, Ken Herner,\nBo Jayatilaka, M. Kado, Oliver Keeble, Alexander Kohls, K. Naim, Clemens Lange, Kati\nLassila-Perini, Sergey Levonian, Mario Maggi, Zach Marshall, Pere Mato Vila, A. Mecionis,\nA. Morris, S. Piano, Maxim Potekhin, M. Schröder, Ulrich Schwickerath, Elizabeth Sexton-\nKennedy, Thomas Simko, T. Smith, D. South, A. Verbytskyi, Michel Vidal, A. Vivace, L Wang,\nGill Watt, Torre J. Wenaus, and DPHEP Collaboration. Data preservation in high energy physics.\nEuro. Phys. J. C, 83:1–41, 2023.\n[7] Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani. Manifold regularization: A geometric\nframework for learning from labeled and unlabeled examples. J. Mach. Learn. Res., 7:2399–\n2434, 2006.\n[8] Ege Beyazit, Jeevithan Alagurajah, and Xindong Wu. Online learning from data streams with\nvarying feature spaces. In Proc. Conf. Artificial Intell. (AAAI), pages 3232–3239, Feb. 2019.\nISBN 978-1-57735-809-1.\n[9] Dankmar Böhning. Multinomial logistic regression algorithm. Annals of the Institute of\nStatistical Mathematics, 44:197–200, 1992.\n[10] Léon Bottou. Online Learning and Stochastic Approximations, chapter 2, pages 9–42. Cam-\nbridge University Press, 1998.\n[11] Léon Bottou and Yann LeCun. Large scale online learning. In Proc. Adv. Neural Info. Proces.\nSys. (NIPS), pages 217–224, Vancouver, Canada, Dec. 2003.\n[12] Chih-Chung Chang and Chih-Jen Lin. Libsvm: A library for support vector machines. ACM\nTrans. Intell. Syst. Technol., 2(3), may 2011. ISSN 2157-6904.\n[13] Joseph de Vilmarest and Olivier Wintenberger. Stochastic online optimization using kalman\nrecursion. J. Machine Learning Research, 22(223):1–55, 2021.\n[14] Mark Dredze, Koby Crammer, and Fernando Pereira. Confidence-weighted linear classification.\nIn Proc. Int. Conf. Machine Learning (ICML), page 264–271, Helsinki, Finland, Jul. 2008.\n[15] Dheeru Dua, Casey Graff, et al. Uci machine learning repository, 2017.\n[16] Yoav Freund and Robert E Schapire. A decision-theoretic generalization of on-line learning and\nan application to boosting. Journal of Computer and System Sciences, 55(1):119–139, 1997.\nISSN 0022-0000.\n[17] Adriaan B.M. Graas, Sophia Bethany Coban, Kees Joost Batenburg, and Felix Lucka. Just-in-\ntime deep learning for real-time x-ray computed tomography. Scientific Reports, 13, 2023.\n10\n[18] Sudipto Guha, Nina Mishra, Rajeev Motwani, and Liadan O’Callaghan. Clustering data streams.\nIn Proc. Foundations Comp. Science, (FOCS), pages 359–366, Redondo Beach, California,\nUSA, Nov. 2000.\n[19] Elad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms for online convex\noptimization. Mach. Learn., 69(2-3):169–192, 2007.\n[20] Yi He, Baijun Wu, Di Wu, Ege Beyazit, Sheng Chen, and Xindong Wu. Online learning from\ncapricious data streams: A generative approach. In Proc. Int. Joint Conf. on Artificial Intell.\n(IJCAI), pages 2491–2497, Jul. 2019.\n[21] Steven C. H. Hoi, Rong Jin, Peilin Zhao, and Tianbao Yang. Online multiple kernel classification.\nMach. Learn., 90(2):289–316, 2013.\n[22] Xinyue Hu, Haoji Hu, Saurabh Verma, and Zhi-Li Zhang. Physics-guided deep neural networks\nfor power flow analysis. IEEE Trans. Power Systems, 36(3):2082–2092, 2021.\n[23] Shunkai Li, Xin Wang, Yingdian Cao, Fei Xue, Zike Yan, and Hongbin Zha. Self-supervised\ndeep visual odometry with online adaptation. In Proc. IEEE/CVF Conf. Comp. Vision Pattern\nRecognition (CVPR), pages 6339–6348, Online, Jun. 2020.\n[24] ZK Liu, LH Zhang, B Liu, ZY Zhang, GC Guo, DS Ding, and BS Shi. Deep learning enhanced\nRydberg multifrequency microwave recognition. Nature Commun., 13(1997):1–10, 2022.\n[25] Youngjae Min, Kwangjun Ahn, and Navid Azizan. One-pass learning via bridging orthogonal\ngradient descent and recursive least-squares. In Proc. IEEE Conf. Decision & Control (CDC),\npages 4720–4725, 2022.\n[26] Boris N. Oreshkin, Florent Bocquelet, Félix G. Harvey, Bay Raitt, and Dominic Laflamme.\nProtores: Proto-residual network for pose authoring via learned inverse kinematics. In Proc. Int.\nConf. Learning Representations (ICLR), Online, Apr. 2022.\n[27] Doyen Sahoo, Quang Pham, Jing Lu, and Steven C. H. Hoi. Online deep learning: learning\ndeep neural networks on the fly. In Proc. Int. Joint Conf. Artificial Intelligence (IJCAI), page\n2660–2666, Stockholm, Sweden, Jul. 2018.\n[28] Zhou Shao, Sha Yuan, and Yongli Wang. Adaptive online learning for iot botnet detection.\nInformation Sciences, 574:84–95, 2021. ISSN 0020-0255.\n[29] Luyu Yang, Mingfei Gao, Zeyuan Chen, Ran Xu, Abhinav Shrivastava, and Chetan Ramaiah.\nBurn after reading: Online adaptation for cross-domain streaming data. In Proc. Euro. Conf.\nComp. Vision (ECCV), page 404–422, Tel Aviv, Israel, Oct. 2022.\n[30] Huanhuan Zhang, Anfu Zhou, Jiamin Lu, Ruoxuan Ma, Yuhan Hu, Cong Li, Xinyu Zhang,\nHuadong Ma, and Xiaojiang Chen.\nOnrl: improving mobile video telephony via online\nreinforcement learning. In Proc. Int. Conf. Mobile Comp. Networking, pages 1–14, London,\nUnited Kingdom, Sep. 2020.\n[31] Martin A. Zinkevich. Online convex programming and generalized infinitesimal gradient ascent.\nIn Proc. Int. Conf. Machine Learning (ICML), Washington, DC, USA, Aug. 2003.\n11\nMODL: Multilearner Online Deep Learning\nSupplementary material\nTable of Contents\nA Broader Impact Statement\n13\nB Additional Experiments\n14\nB.1\nFeature Missingness Experiment\n. . . . . . . . . . . . . . . . . . . . . . . . .\n14\nB.2\nModel Ablation Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\nB.3\nTraining Time Additional Settings . . . . . . . . . . . . . . . . . . . . . . . . .\n15\nB.4\nLearning Rate Sensitivity Experiments\n. . . . . . . . . . . . . . . . . . . . . .\n15\nC Dataset Statistics\n16\nD Hyperparameters and Computational Resources\n16\nD.1\nHyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\nD.2\nComputational Resources . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\nE\nBackground\n17\nE.1\nDe Finetti Representation Theorem . . . . . . . . . . . . . . . . . . . . . . . .\n17\nE.2\nBayesian Linear Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\nE.3\nOnline Bayesian Linear Regression . . . . . . . . . . . . . . . . . . . . . . . .\n18\nF\nLogistic Regression\n18\nG Online Logistic Regression Derivation\n19\nG.1\nProof of Proposition 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\nG.2\nRelation to other Online Logistic Regression approaches . . . . . . . . . . . . .\n21\nG.3 Toy Experiment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\nH Architectural Details\n22\nI\nFast Online Deep Learning - Additional Details\n23\nI.1\nProof of Proposition 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23\nI.2\nComplexity analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n25\n12\nA\nBroader Impact Statement\nOur paper introduces a new Online Learning technique and improves existing training methodologies\nfor online learning of deep neural networks. We show that co-training multiple learners can lead to\nsignificantly faster convergence as well as improved overall model performance at inference time.\nA strongly positive outcome stemming from our contributions is the significant reduction of training\ntime from quadratic complexity in network parameters down to linear complexity. This has profound\nconsequences for training deep models online as it significantly decreases the necessary training\ncompute. Thus the energy spent for training deep online learners is massively reduced.\nFurthermore, the improved convergence speed and overall performance of our proposed technique\nMODL means that our model is more data efficient than existing ones leading to a moderately reduced\nneed for large dataset sizes. Efficiently learning in one pass means that data does not need to be\nstored which strongly protects the privacy rights of individuals and organizations. Training online,\ni.e., without storing data can help companies comply with data protection laws and can allow the\nconsumer to have greater confidence that their fundamental human right of privacy is protected.\nNevertheless, as with all methods in machine learning there are broader impact concerns with our\nwork. Our proposed methods are by no means immune to dataset bias; a well documented problem.\nRisk mitigation mechanisms include A.I. fairness research techniques that broadly apply to neural\nnetworks as well as model intrepretability. In our case is the potential to interpret model outputs is\nparticularly strong for the weaker learners such as logistic regression.\nWe are confident that our research effort offers more benefits in energy saving and data privacy as\nopposed to the risks posed by the usage of deep models that are prone to bias in the data. Furthmore,\nwe point out that the risks posed by a potential deployment of our model can be hedged against due\nto the interpretability of some of our constituent learners.\n13\nB\nAdditional Experiments\nB.1\nFeature Missingness Experiment\nIn this section we provide additional experiments in support of the summary results presented in the\nmain text. More concretely, we provide a full feature missningess study with 13 unique pf values for\nthe large benchmarks HIGGS and SUSY. As shown in Tab. 6, our model convincingly outperforms\nin all settings. We observe that the advantage of our method increases as more features become\navailable.\nTable 6: Comparison on HIGGS and SUSY for various feature probabilities pf. The metric is the\nmean (± standard deviation) cumulative error in thousands (5 runs).\npf\nHIGGS\nAuxDrop (ODL)\nMODL (ours)\n.01\n440.2 ± 0.1\n439.6 ± 0.1\n.05\n440.0 ± 0.1\n439.5 ± 0.2\n.10\n440.0 ± 0.2\n438.5 ± 0.1\n.20\n438.4 ± 0.1\n435.6 ± 0.4\n.30\n435.1 ± 0.2\n432.3 ± 0.3\n.40\n432.0 ± 0.3\n428.4 ± 0.3\n.50\n427.4 ± 0.7\n422.8 ± 0.4\n.60\n423.2 ± 0.5\n422.8 ± 0.2\n.70\n418.5 ± 0.7\n409.5 ± 0.4\n.80\n411.8 ± 0.4\n399.6 ± 0.3\n.90\n405.6 ± 0.7\n387.0 ± 0.3\n.95\n399.4 ± 1.0\n377.2 ± 0.5\n.99\n392.1 ± 1.0\n366.5 ± 0.6\npf\nSUSY\nAuxDrop (ODL)\nMODL (ours)\n.01\n285.0 ± 0.1\n283.0 ± 0.1\n.05\n283.3 ± 0.2\n281.0 ± 0.1\n.10\n280.6 ± 0.5\n278.0 ± 0.1\n.20\n274.9 ± 0.9\n271.9 ± 0.1\n.30\n269.0 ± 0.7\n265.5 ± 0.1\n.40\n262.8 ± 0.9\n258.9 ± 0.1\n.50\n256.7 ± 1.0\n252.0 ± 0.1\n.60\n250.0 ± 0.9\n244.7 ± 0.2\n.70\n243.9 ± 0.7\n238.0 ± 0.2\n.80\n237.0 ± 0.7\n230.6 ± 0.1\n.90\n230.0 ± 0.7\n222.3 ± 0.2\n.95\n226.2 ± 0.4\n217.5 ± 0.1\n.99\n222.2 ± 0.2\n212.2 ± 0.2\nB.2\nModel Ablation Study\nIn this section we empirically validate our proposed model MODL. As shown in Tab 7 the results in\nthe main paper persist in the additional feature missningness settings further validating our conclusion\nthat all of our model components are necessary.\nTable 7: Ablation of proposed model components. We show that for each learner that we add the\noverall model performance improves.\npf\nHIGGS\nSUSY\nOL + MLP\nOL + Set Learner\nMODL (ours)\nOL + MLP\nOL + Set Learner\nMODL (ours)\n.01\n442.8±0.2\n450.2±0.5\n439.6 ± 0.1\n285.3±0.1\n332.6±2.1\n283.0 ± 0.1\n.05\n442.7±0.1\n449.9±0.4\n439.5 ± 0.2\n283.3±0.1\n334.7±1.9\n281.1 ± 0.1\n.10\n441.9±0.1\n450.1±0.3\n438.5 ± 0.1\n280.0±0.1\n337.8±1.4\n278.0 ± 0.1\n.20\n439.9±0.1\n450.3±0.2\n435.6 ± 0.4\n274.0±0.1\n352.4±0.3\n271.9 ± 0.1\n.30\n437.4±0.1\n450.1±0.3\n432.3 ± 0.3\n267.9±0.1\n355.4±0.3\n265.5 ± 0.1\n.40\n434.9±332\n450.3±404\n428.4 ± 302\n261.7±0.1\n338.9±0.4\n258.9 ± 0.1\n.50\n430.3±193\n450.1±93\n422.8 ± 386\n255.0±0.1\n343.3±0.7\n252.0 ± 0.1\n.60\n425.4±178\n449.9±137\n422.8 ± 244\n248.3±0.1\n336.6±0.5\n244.7 ± 0.2\n.70\n420.5±484\n451.0±122\n409.5 ± 424\n241.3±0.1\n337.9±0.4\n238.0 ± 0.1\n.80\n411.8±106\n449.6±219\n399.6 ± 256\n234.3±0.1\n326.3±0.4\n230.6 ± 0.1\n.90\n401.4±517\n449.8±58\n387.0 ± 348\n226.5±0.2\n322.0±0.4\n222.3 ± 0.2\n.95\n394.1±565\n448.5±175\n377.2 ± 537\n222.1±0.1\n320.9±0.4\n217.5 ± 0.1\n.99\n386.7±133\n447.1±163\n366.4 ± 588\n217.4±0.1\n316.4±0.7\n212.2 ± 0.2\n14\nB.3\nTraining Time Additional Settings\nIn this section we explore the wall clock training time reduction offered by our proposed fast training\nscheme on large datasetrs. We note that the training time savings range from 30% to over 80%!\nTable 8: Time comparison between AuxDropODL and FastAuxDropODL on HIGGS and SUSY for\np = 0.5 (various layer and embedding dimensions).\nExperiment\nAux-ODL\nFast Aux-(FODL)\nTime ODL vs. FODL\nHIGGS (L = 5, E = 25)\n431.9 ± 0.5\n431.4 ± 0.5\n5:35:10 vs. 3:34:55\nHIGGS (L = 5, E = 50)\n429.6 ± 0.5\n429.6 ± 0.6\n5:41:05 vs. 3:38:18\nHIGGS (L = 5, E = 100)\n428.0 ± 0.3\n427.9 ± 0.3\n5:45:56 vs. 3:38:42\nHIGGS (L = 11, E = 25)\n429.8 ± 0.7\n429.7 ± 0.4\n22:44:06 vs. 6:21:58\nHIGGS (L = 11, E = 50)\n427.4 ± 0.5\n427.4 ± 0.7\n16:04:59 vs. 4:29:05\nHIGGS (L = 11, E = 100)\n426.9 ± 0.3\n426.6 ± 0.3\n20:05:39 vs. 6:32:20\nHIGGS (L = 20, E = 25)\n431.4 ± 0.5\n431.0 ± 0.5\n57:04:12 vs. 9:23:13\nHIGGS (L = 20, E = 50)\n429.2 ± 0.4\n429.4 ± 0.4\n54:04:10 vs. 8:43:11\nHIGGS (L = 20, E = 100)\n427.6 ± 0.5\n427.8 ± 0.4\n63:19:05 vs. 8:58:51\nSUSY (L = 5, E = 25)\n257.3 ± 1.0\n257.3 ± 1.2\n5:33:53 vs. 2:59:32\nSUSY (L = 5, E = 50)\n256.6 ± 0.9\n256.6 ± 0.9\n5:37:42 vs. 3:03:09\nSUSY (L = 5, E = 100)\n255.9 ± 0.4\n255.8 ± 0.5\n5:07:21 vs. 3:41:03\nSUSY (L = 11, E = 25)\n257.2 ± 0.9\n257.5 ± 1.1\n22:41:05 vs. 6:19:03\nSUSY (L = 11, E = 50)\n257.0 ± 1.2\n256.7 ± 1.0\n23:04:11 vs. 6:54:21\nSUSY (L = 11, E = 100)\n255.9 ± 0.8\n256.1 ± 0.8\n20:05:06 vs. 6:29:58\nSUSY (L = 20, E = 25)\n258.6 ± 1.1\n258.5 ± 1.3\n36:39:13 vs. 8:44:24\nSUSY (L = 20, E = 50)\n257.4 ± 0.9\n257.4 ± 0.9\n39:59:38 vs. 8:01:53\nSUSY (L = 20, E = 100)\n257.3 ± 1.0\n257.1 ± 1.0\n43:07:45 vs. 8:19:44\nB.4\nLearning Rate Sensitivity Experiments\nIn this section we demonstrate that our model has stable and better performance than the baselines\napproach for a broad range of learning rates. This is due to the incorporation of learners without\na backpropagation update which shields from but learning rate selections. Note that if we set the\nlearning rate too high the other learners may become unstable so that protection from bad learning\nrates has reasonable limits (i.e. cannot train with absurd learning rates such as 0.5 with or higher).\nThe conclusion from this section and Tab. 9 is that for any reasonable learning rate selection for deep\nmodels our model outperforms.\nTable 9: Error in HIGGS and SUSY for various learning rates with fixed probability of unreliable\nfeatures pf = 0.99. The metric is reported as the mean ± standard deviation of the number of errors\nin 5 runs (11 layer Aux networks).\nlr\nHIGGS\nSUSY\nAuxDrop (ODL)\nMODL (ours)\nAuxDrop (ODL)\nMODL (ours)\n0.00005\n470.9±0.8\n426.1± 1.2\n391.9 ± 67\n225.5 ± 0.7\n0.0001\n470.1±0.7\n415.4±0.8\n333.4±62\n222.2 ± 0.4\n0.0005\n442.8±3.5\n392.7±0.6\n278.6±21\n216.6 ± 0.2\n0.001\n402.1±6.4\n366.5±0.6\n225.9±1.9\n212.2 ± 0.2\n0.005\n439.4±11.4\n383.3±0.9\n253.6±11\n215.0 ± 0.3\n0.01\n389.9±1.8\n368.9±0.5\n222.7±0.9\n212.8 ± 0.1\n0.05\n392.1±0.8\n421.1±0.5\n222.4±0.2\n227.2 ± 0.1\n0.1\n418.7±5.9\n461.9±0.3\n227.6±0.5\n242.4 ± 0.4\n15\nC\nDataset Statistics\nOur dataset pre-processing follows exactly the steps from Agarwal et al. [3]. The data is publicly\navailable for download online5. We provide summary dataset statistics in Tab. 10. The use of diverse\ndataset sizes aims to illustrate the strength of our approach in settings with low data (one thousand),\nmoderate amount (tens of thousands) and large data (millions).\nTable 10: Dataset statistics for the data used in our experiments.\nDataset\nSize\nFeature Size\nTask\ngerman\n1000\n24\nClassification\nsvmguide3\n1243\n21\nClassification\nmagic04\n19020\n10\nClassification\na8a\n32561\n123\nClassification\nSUSY\n1000000\n8\nClassification\nHIGGS\n1000000\n21\nClassification\nD\nHyperparameters and Computational Resources\nD.1\nHyperparameters\nAux-ODL: We used the official repository of Aux-ODL6 with the tuned hyperparameters from the\noriginal paper by [3] without makiung any changes. Namely, for the small datasets in our experiments\nwe run 6-layer networks with learning rates 0.1 for german and svmguide3 and 0.01 for magic04\nand a8a. For the large datasets, we used 11 layer networks with learning rate equal to 0.05. For\nall experiments we used the recommended layer size and AuxLayer dimension and position in the\nnetwork. Additionally, the ODL discount rate was set to β = 0.99 and the smoothing rate was set to\ns = 0.2.\nMODL: We used the exact same architecture in all experiments. Specifically, the online logistic\nregression learner is implemented identically for all datasets and the MLP has 3 hidden layers with\n250 neurons. The set learner consists of 6 blocks with 3 layers per block. The width of each layer is\nset to 128 neurons. Our experiment code is publicly accessible7.\nD.2\nComputational Resources\nWe run our experiments on a HP DL-580 Gen 7 server equipped with Intel Xeon E7-4870 2.40GHz\n10-Core 30MB LGA1567 CPUs. In total we used 160 CPUs for our experiments. Note that using\nGPUs in the online learning setting is not effective as the batch size is 1. Therefore, given that our\ntask is purely sequential we opt for a CPU server that is actually faster than a GPU server given the\nnature of the online learning task.\n5Available here: https://github.com/Rohit102497/Aux-Drop/tree/main/Code/Datasets\n6Available here: https://github.com/Rohit102497/Aux-Drop\n7Available here: https://github.com/AntonValk/MODL\n16\nE\nBackground\nE.1\nDe Finetti Representation Theorem\nFor n ≥1 the de Finetti representation for the joint mass or density of exchangeable discrete random\nvariables Y1, . . . , Yn is given by\npY1,...,Yn(y1, . . . , yn) =\nZ\nΘ\nn\nY\ni=1\nfY (yi; θ)π0(dθ)\nwhere pY (y; θ) is a mass function in y, and θ is a parameter lying in a space Θ ∈Rp, for some\n(prior) distribution π0(dθ) defined on θ, where we may interpret Θ as the smallest set such that\nR\nΘ π0(dθ) = 1. Using basic properties of probability we may derive an expression for the posterior\npredictive distribution for Yn+1 (another element of the infinitely exchangeable sequence) conditional\non Y1 = y1, . . . , Yn = yn. This defines the posterior density, πn(dθ), as an updated version of\nπ0(dθ). We may re-write the postirior predictive using the definition of conditional probability as\npn(yn+1) = p0(y1, · · · , yn, yn+1)\np0(y1, · · · , yn)\n=\nZ\nfY (yn+1; θ)πn(θ) dθ\n(17)\nwhere\nπn(θ) =\n1\np0(y1, · · · , yn)\nn\nY\ni=1\nf(yi; θ)π0(θ),\n(18)\nis the posterior. Note that this is the same form as the De Finetti representation with an updated\nversion of π0.\nThis result is important as it shows that the posterior predictive is the same whether we consider\nthe initial prior and the likelihood of a full batch on n observations or if we use an iterative update\nrule to modify the prior at each time step. Thus, we may simply update the prior for the n + 1-th\nobservation to be the posterior after the n-th observation without the need to maintain the any of\nthe previous observations in memory. This allows us to derive recursive algorithms that update\nprior-posterior parameters without storing observations. Essentially, as long as we maintain a statistic\nthat is sufficient in the Bayesian sense, the posterior distribution depends on the data only through the\nobserved value of the statistic and the data may be discarded.\nE.2\nBayesian Linear Regression\nFor a dataset D = {(t1, y1), . . . , (yN, tN)}, consider a classic regression model of the form:\nyk = xkθ + εk,\n(19)\nwhere yk is a scalar outcome, θ ∈Rd×1 is the parameter of interest, xk ∈R1×d is the feature vector\nand εk ∼N(0, σ2) is normally distributed noise. To simplify the notation we may stack all feature\nvectors in the designer matrix X ∈RN×d and the corresponding outcomes in a vector yk ∈RN.\nThough the problem can be solved in a purely least squares fashion or equivalently via maximum-\nlikelihood approach we employ a standard Bayesian formulation. Assigning prior.\nπ0(θ) = N(θ | m0, P0)\n(20)\nfor some m0 ∈Rd, P0 ∈Rd×d and modeling p(yk | θ) as\np(yk | θ) = N(yk | xkθ, σ2),\n(21)\nwe may then apply Bayes rule to recover the posterior distribution over the parameter.\np(θ | y1:N) ∝p(θ)\nN\nY\nk=1\np(yk | θ) = N(θ | m0, P0)\nN\nY\nk=1\nN(yk | xkθ, σ2)\n(22)\n= N(θ | mN, PN),\n(23)\n17\nwhere the last equality follows by well known result Normal priors and Normal likelihoods yielding\nnormal posteriors. In fact, it can be shown that\nmN =\n\u0014\nP−1\n0\n+ 1\nσ2 X⊤X\n\u0015−1 \u0014 1\nσ2 X⊤y + P−1\n0 m0\n\u0015\n,\n(24)\nPN =\n\u0014\nP−1\n0\n+ 1\nσ2 X⊤X\n\u0015−1\n.\n(25)\nE.3\nOnline Bayesian Linear Regression\nA disadvantage of full batch regression is that it requires keeping a large matrix\n\u0002\nP−1\n0\n+\n1\nσ2 X⊤X\n\u0003−1\nin memory and needing to invert it. To address these concerns we can use an efficient and recursive\nversion of the algorithm. From the recursive de Finetti represention result we proved earlier we may\nalternatively calculate a posterior at step n-1 and treat it as a prior for step n.\np(θ | y1:n) ∝p(θ | y1:n−1)p(yn | θ) = N(θ | mn−1, Pn−1)N(yk | xkθ, σ2)\n(26)\n= N(θ | mn, Pn),\n(27)\nwhere\nmn =\n\u0014\nP−1\nn−1 + 1\nσ2 X⊤\nn Xn\n\u0015−1 \u0014 1\nσ2 X⊤\nn yn + P−1\nn−1mn−1\n\u0015\n,\n(28)\nPn =\n\u0014\nP−1\nn−1 + 1\nσ2 X⊤\nn Xn\n\u0015−1\n.\n(29)\nIn the above notation, Xn and yn indicate that we take the n-th rows of X, y.\nThe matrix inversion lemma8, also known as the Woodbury matrix identity.\nPn =\n\u0014\nP−1\nn−1 + 1\nσ2 X⊤\nn Xn\n\u0015−1\n= Pn−1 −Pn−1X⊤\nn\n\u0002\nXnPn−1X⊤\nn + σ2\u0003−1 XnPn−1.\n(30)\nNote that the inverse term in the right hand side is a scalar and easy to compute.\nThe recursive set of equations takes the form:\nSn = XnPn−1X⊤\nn + σ2,\n(31)\nKn = Pn−1X⊤\nn S−1\nn ,\n(32)\nmn = mn−1 + Kn[yn −Xnmn−1],\n(33)\nPn = Pn−1 −KnSnK⊤\nn .\n(34)\nF\nLogistic Regression\nConsider a Generalized Linear Model (GLM) with link function g(·) = logit−1(·). For binary\nclassification we define p(y | x) = EY |X[y | x] = g−1(xθ) = σ(xθ) = µ, σ(·) is the sigmoid\nfunction. The problem of finding the optimal parameter can be solved by maximizing the (log)\nlikelihood numerically as it is convex. More precisely the derivative of the log-likelihood and the\nHessian can be shown to be\n˙ℓn(θ) = X⊤(y −µ),\n(35)\n¨ℓn(θ) = −X⊤DX,\n(36)\nwhere µ = [σ(X1θ), . . . , σ(Xnθ)] is a vector containing the model prediction for each input, D is a\ndiagonal matrix with the diagonal entries [σ(X1θ)(1 −σ(X1θ)), . . . , σ(Xnθ)(1 −σ(Xnθ))]. Note\nthat since this is a convex problem so it has a unique solution that can be recovered via standard\nconvex optimization techniques.\n8https://en.wikipedia.org/wiki/Woodbury_matrix_identity\n18\nG\nOnline Logistic Regression Derivation\nG.1\nProof of Proposition 2\nBy standard random variable transformation theory we know that for invertible h(·) : Rn →Rm for\nz ∈Rn and y ∈Rm if\nz ∼N(z | m, P)\n(37)\ny = h(z) + εt,\n(38)\nwhere we model the output as receiving additive noise ε ∼N(εt | 0, Σt) independently from the\ninput. then p(y) = N(h−1(y) | m, P)|J (y)|−1, where |J (y)| is the Jacobian of h evaluated at y.\nA challenge in analyzing this model is that it is non-linear. A standard technique to approximate p(y)\nis to apply a first order Taylor approximation and linearize the function locally around the mean of\nthe normal N(z | m, P) with some random perturbation δz ∼N(δz | 0, P).\nh(z) = g(m + δz) ≈g(m) + J (m)δz = bh(z).\n(39)\nFirstly, note that E[h(z)] ≈E[h(m)] + J (m)E[δz] = E[h(m)]. Similarly, for the covariance matrix\nit follows that cov(h(x)) = E\n\u0002\n(h(z) −E[h(z)])(h(z) −E[h(z)])⊤\u0003\n≈J (m)PJ (m)⊤. To show\nthis in detail consider:\ncov(h(z)) = E\n\u0002\n(h(z) −E[h(z)])(h(z) −E[h(z)])⊤\u0003\n(40)\n≈E\n\u0002\n(h(z) −h(m))(h(z) −h(m))⊤\u0003\n(41)\n= E\n\u0002\n(h(m) + J (m)δz −h(m))(h(m) + J (m)δz −h(m))⊤\u0003\n(42)\n= E\n\u0002\n(J (m)δz)(J (m)δz)⊤\u0003\n(43)\n= J (m)E\n\u0002\n(δz)(δz)⊤\u0003\nJ (m)⊤\n(44)\n= J (m)PJ (m)⊤.\n(45)\nCombining the previous results we obtain an approximate joint distribution of z, h(z):\np\n\u0012\u0014\nz\ny\n\u0015\u0013\n= N\n\u0012\u0014\nm\nh(m)\n\u0015\n,\n\u0014\nP\n[PJ (m)]⊤\nPJ (m)\nΣt + J (m)PJ ⊤(m)\n\u0015\u0013\n(46)\nIn the case of input features that are multiplied to the parameter regression we have h(z)\ndef\n= h(xθ)\nwhere the input feature vector x is multiplied by the weights vector θ, i.e., z\ndef\n= xθ. This modifies the\njoint distribution we just derived:\nθ ∼N(θ | m, P)\n(47)\ny = h(xθ) + ε,\n(48)\np\n\u0012\u0014\nθ\ny\n\u0015\u0013\n= N\n\u0012\u0014\nm\nh(mθ)\n\u0015\n,\n\u0014\nP\n[PxJ (m)]⊤\nPxJ (m)\nΣt + J (m)xPx⊤J ⊤(m)\n\u0015\u0013\n(49)\nThen, from the joint p(θ, y) we may obtain the conditional p(θ | y) which has a well known closed\nform for jointly normal random variables,\np(θ | y = yk) = N(θ |(m′, P′),\nwhere\n(50)\nm′ = m + [PxJ (m)]⊤[Σk + J (m)xPx⊤J ⊤(m)]−1(yk −h(mθ))\n(51)\nP′ = P −[PxJ (m)]⊤[Σk + J (m)xPx⊤J ⊤(m)]−1[PxJ (m)]\n(52)\nThus we obtain a normal posterior over the weights. But observe that this is just another recursive\nnormal prior — normal likelihood model. Thus we may obtain recursive form updates similarly to\nthe ones we derived for the linear regression earlier.\np(θ | y1:n) ∝p(θ | y1:n−1)p(yn | θ) = N(θ | mn−1, Pn−1)N(yn | h(xnθ), σ2)\n(53)\n= N(θ | mn, Pn),\n(54)\n19\nwhere\nmn = mn−1 + [Pn−1x⊤\nn J (mn−1)]⊤[yn −h (xnmn−1)]\nΣn + J (mn−1)xnPn−1x⊤\nn J ⊤(mn−1) ,\n(55)\nPn = Pn−1 −J ⊤(mn−1)Pn−1x⊤\nn xnP⊤\nn−1J (mn−1)\nΣn + J (mn−1)xnPn−1x⊤\nn J ⊤(mn−1).\n(56)\nOnline\nBinary\nLogistic\nRegression\nConcretely,\nfor\na\nbinary\nlogistic\nregression\nh(·)\ndef\n= logit−1(xθ) = σ(xθ) we can calculate the Jacobian to obtain a closed form update.\np(θ | y1:n) = N(θ | mn, Pn),\n(57)\nmn = mn−1 + Pn−1 [(1 −σ(xnmn−1)) σ (xnmn−1)] x⊤\nn [yn −σ (xnmn−1)]\nΣn + Pn−1 [(1 −σ(xnmn−1)) σ (xnmn−1)]2\n,\n(58)\nPn = Pn−1 −Pn−1x⊤\nn xnP⊤\nn−1 [(1 −σ(xnmn−1)) σ (xnmn−1)]2\nΣn + Pn−1 [(1 −σ(xnmn−1)) σ (xnmn−1)]2\n.\n(59)\nThis yields the following update equations:\nSn = Σn + J (mn−1)Pn−1J ⊤(mn−1),\n(60)\nKn = Pn−1x⊤\nn J (mn−1)⊤S−1\nn ,\n(61)\nmn = mn−1 + Kn[yn −h (xnmn−1)],\n(62)\nPn = Pn−1 −KnSnK⊤\nn .\n(63)\nSn = Σn + P [(1 −σ(Xnmn−1)) σ (Xnmn−1)]2 ,\n(64)\nKn = Pn−1x⊤\nn (1 −σ(Xnmn−1)) σ (Xnmn−1) S−1\nn ,\n(65)\nmn = mn−1 + Kn[yn −σ (Xnmn−1)],\n(66)\nPn = Pn−1 −KnSnK⊤\nn .\n(67)\nTo recover the result in the main paper we may choose to model the process noise Σn\ndef\n= xnPn −1xn.\nThis provides a regularization effect, not allowing the update steps to become very large when xn has\na large magnitude.\nOnline Multinomial Logistic Regression\nIn this case the features have the same dimensions but\nthe observations are a one hot vector y ∈RK where K is the number of classes. Each of the K\nclasses has an associated parameter vector θ(k) ∈RD, k ∈1, . . . , K. Stacking these vectors yields a\nparameter matrix Θ ∈RD×K with element Θ(k)\nd\ndenoting the weight of input feature at index d for\nthe k-th class.\nThe function that maps the input vector to the prediction is the softmax function ˆpl\n=\nexp(Θ(l)T x)\n1+PK\nj=1 exp(Θ(j)T x) leading to log likelihood:\nL(Θ) =\n K\nX\nk=1\nykΘ(k)T x\n!\n−ln\n\n1 +\nK\nX\nj=1\nexp(Θ(j)T x)\n\n.\n(68)\nOne can show that the gradient of this function evaluates to the following Kronecker product [9]:\n∇ΘL(Θ) =\n\n\n(y −ˆp)1x\n(y −ˆp)2x\n...\n(y −ˆp)Kx\n\n= (y −ˆp) ⊗x,\n(69)\nwhich has the same dimensions as the weight matrix Θ. Thus, in the multinomial case we may still\napply the closed form updates derived previously by setting J (Θ) = ∇ΘL(Θ). This complicates the\nupdates as the parameter is now a matrix but the update complexity remains constant in the size of\nthe parameter and we never have to perform any matrix inversions.\n20\nG.2\nRelation to other Online Logistic Regression approaches\nThere are two main other approaches to online logistic regression that are relevant to our method.\nFirtly, FOLKLORE by Agarwal et al. [2] proposes an iterative optimization scheme were logistic\nregression parameters can be learned by iteratively solving an optimization problem. While this\napproach can work well, it lacks the closed form updates that are needed to improve efficiency. An\napproach with closed form updates is offered in the of work of de Vilmarest and Wintenberger [13].\nThese updates are quite similar to our and if we remove the regularization from our approach can be\nmade equivalent. However, de Vilmarest and Wintenberger [13] arrive at these updates using Kalman\nfilter theory to derive general results about exponential family models whereas our derivation uses\nmuch more basic tools from probability theory. We believe that our approach is intuitive and the\nderivation makes the linearization assumptions more obvious to the reader rather than automatic\napplication of Kalman filters.\nG.3\nToy Experiment\nIn this section we present a toy experiment that analyzes the online logistic regression. We consider\nthe following toy data generating process for a binary logistic regression model with two data\ngenerating parameters θ∗:\nθ∗= (1, 2)\n(70)\nX ∼N(X | −1.5, 1)\n(71)\nY =\n(\n0,\nwith probability\n1\n1+exp(x·θ)\n1,\nwith probability1 −\n1\n1+exp(x·θ)\n(72)\nWe generate 100 pairs by drawing independently 100 times from X and generating Y given each\nsample. Note that X ∈R2 is augmented with a constant value 1 to handle the intercept parameter.\nWe observe that the log-likelihood shown in Fig. 2 matches well with the quadratic log likelihood\napproximation around the data generating values.\n21\nH\nArchitectural Details\nResidual\nprojection\nFC layer 1\nBlock Input\nFC layer L\nSoftmax\nBlock 2\nGlobal Input\nBlock R\nBlock 1\nLinear\nLinear\nBlock \nOutput\nGlobal\nInput\nFigure 4: Visualization of our set learning decoder.\nSlow learner with set inputs. Rather than masking missing features with a zero or using deterministic\ndropout, as is done in [3], we treat the input as a set that excludes any missing features. Recall that\nthe data generating process produces a sequence of triplets P = {(z1, y1, Ω1), . . . , (zT , yT , ΩT )}.\nThen, the set of input features Xt can be expressed as:\nXt = {zt,j : Ωt,j = 1} ,\nIt = {j : Ωt,j = 1} ,\nD = [(X1, I1, y1), . . . , (XT , IT , yT )] .\n(73)\nThe size of the input feature set Xt is time varying. To allow the model to determine which inputs are\navailable, it is necessary to pass a set of feature IDs in an index set It. Our proposed set learning\nmodule follows closely the ProtoRes architecture (please refer to Oreshkin et al. [26] for details). It\ntakes the index set It and maps each of its active index positions to a continuous representation to\ncreate feature ID embeddings. It then concatenates each ID embedding to the corresponding feature\nvalue of Xt. These feature values and ID embedding pairs are aggregated and summed to produce\nfixed dimensional vector representations that we denote as x0. The main components of our proposed\nset learning module are blocks, each consisting of L fully connected (FC) layers. Residual skip\nconnections are included in the architecture so that blocks can be bypassed. An input set X is mapped\nto x0 = EMB(X). The overall structure of the module at block r ∈{1, . . . , R} is:\nhr,1 = FCr,1(xr−1),\n. . . ,\nhr,L = FCr,L(hr,L−1),\n(74)\nxr = RELU(Wrxr−1 + hr−1,L) ,\nˆyr = ˆyr−1 + QLhr,L,\n(75)\nwhere Wr, QL are learnable matrices. We connect R blocks sequentially to obtain global output ˆyR.\n22\nI\nFast Online Deep Learning - Additional Details\nI.1\nProof of Proposition 1\nIn this section we analyze a base case for a 3 layer network for a proof sketch. It is easy to generalize\nthis more formally, with a simple inductive argument.\nRecall the variable definitions:\nf (l) = softmax(h(l)Θ(l)), ∀l = 0, . . . , L\nh(l) = σ(W(l)h(l−1)), ∀l = 1, . . . , L\nh(0) = x\nAnd the update rule:\nW (l)\nt+1 ←W (l)\nt\n−η\nL\nX\nj=l\nα(j)∇W (l) L(f (j), yt)\n(76)\nThen for each layer we get the following updates (Fig. 5 shows the ODL computational graph):\nW(3)\nt+1 = W(3)\nt\n−η∇W(3)\nt α(3) L3 = W(3)\nt\n−ηα3\n∂L3\n∂f (3)\n∂f (3)\n∂h(3)\n∂h(3)\n∂W(3)\nt\n(77)\nW(2)\nt+1 = W(2)\nt\n−η\n\u0010\n∇W(2)\nt α(2) L2 +∇W(2)\nt α(3) L3\n\u0011\n= W(2)\nt\n−η\n \nα2\n∂L2\n∂f (2)\n∂f (2)\n∂h(2)\n∂h(2)\n∂W(2)\nt\n+ α3\n∂L3\n∂f (3)\n∂f (3)\n∂h(3)\n∂h(3)\n∂h(2)\n∂h(2)\n∂W(2)\nt\n!\n(78)\nW(1)\nt+1 = W(1)\nt\n−η\n\u0010\n∇W(1)\nt α(1) L1 +∇W(1)\nt α(2) L2 +∇W(1)\nt α(3) L3\n\u0011\n= W(1)\nt\n−η\n \nα1\n∂L1\n∂f (1)\n∂f (1)\n∂h(1)\n∂h(1)\n∂W(1)\nt\n+ α2\n∂L2\n∂f (2)\n∂f (2)\n∂h(2)\n∂h(2)\n∂h(1)\n∂h(1)\n∂W(1)\nt\n+ α3\n∂L3\n∂f (3)\n∂f (3)\n∂h(3)\n∂h(3)\n∂h(2)\n∂h(2)\n∂h(1)\n∂h(1)\n∂W(1)\nt\n!\n(79)\nNow, we consider our proposed architecture, shown in Fig. 6. In our case the weights can be updated\nusing standard backpropagation, i.e., W(l)\nt+1 ←W(l)\nt\n−η∇W(l)\nt L, where L = PL\ni=1 α(i) Li.\nLemma 1: The gradient\n∂Ll\n∂W(k) = 0 for all (k, l) ∈{1, 2, . . . , L}2 with k > l.\nProof: Suppose k > l and recall from the definition, f (l) = softmax(h(l)Θ(l)), with h(l) =\nσ(W(l)h(l−1)). Now following the recursive definition of h(l) we have:\nh(l) = σ\n\u0010\nW(l)h(l−1)\u0011\n= σ\n\u0010\nW(l)σ\n\u0010\nW(l−1)σ\n\u0010\n· · ·\n\u0010\nσ\n\u0010\nW(1)h(0)\u0011\u0011\n· · ·\n\u0011\u0011\u0011\n(80)\nf (l) = softmax(h(l)Θ(l)) = softmax\nh\nσ\n\u0010\nW(l)σ\n\u0010\nW(l−1)σ\n\u0010\n· · ·\n\u0010\nσ\n\u0010\nW(1)h(0)\u0011\u0011\n· · ·\n\u0011\u0011\u0011\nΘ(l)i\n(81)\nObserve that for k > l, W(k) does not appear in eq. (81) thus we immediately conclude\n∂f (l)\n∂W(k) = 0.\nRecall that the operator Ll is a function of f (l), i.e., Ll(f (l), y(t)). Given that y(t) is a fixed constant,\nby the chain rule,\n∂Ll\n∂W(k) = ∂Ll\n∂f (l)\n∂f (l)\n∂W(k) = 0.\nCorollary: 1 ∇W(k)\nt\nL = ∇W(k)\nt\nPL\ni=1 α(i) Li = ∇W(k)\nt\nPL\ni=k α(i) Li.\n23\nInput\nFigure 5: ODL hedge backpropagation. Red lines indicate individual backpropagation calculations.\nProof: All i < k the terms in the sum are equal to zero by direct application of the previous lemma:\n∇W(k)\nt\nL = ∇W(k)\nt\nL\nX\ni=1\nα(i) Li = ∇W(k)\nt\nk−1\nX\ni=1\nα(i) Li +∇W(k)\nt\nL\nX\ni=k\nα(i) Li\n= ∇W(k)\nt α(1) L1\n|\n{z\n}\n=0 by lemma 1\n+ · · · + ∇W(k)\nt α(k−1) Lk−1\n|\n{z\n}\n=0 by lemma 1\n+∇W(k)\nt\nL\nX\ni=k\nα(i) Li\n= ∇W(k)\nt\nL\nX\ni=k\nα(i) Li .\n(82)\n24\nNow, using Lemma 1 and by linearity of differentiation we we have:\nW(3)\nt+1 = W(3)\nt\n−η∇W(3)\nt\nL = W(3)\nt\n−η∇W(3)\nt\n3\nX\ni=1\nα(i) Li = W(3)\nt\n−η∇W(3)\nt α(3) L3\n= W(3)\nt\n−ηα3\n∂L3\n∂f (3)\n∂f (3)\n∂h(3)\n∂h(3)\n∂W(3)\nt\n(83)\nW(2)\nt+1 = W(2)\nt\n−η∇W(2)\nt\nL = W(2)\nt\n−η∇W(2)\nt\n3\nX\ni=1\nα(i) Li = W(2)\nt\n−η∇W(2)\nt\n\u0010\nα(2) L2 +α(3) L3\n\u0011\n= W(2)\nt\n−η\n \nα2\n∂L2\n∂f (2)\n∂f (2)\n∂h(2)\n∂h(2)\n∂W(2)\nt\n+ α3\n∂L3\n∂f (3)\n∂f (3)\n∂h(3)\n∂h(3)\n∂h(2)\n∂h(2)\n∂W(2)\nt\n!\n(84)\nW(1)\nt+1 = W(1)\nt\n−η∇W(1)\nt L = W(1)\nt\n−η∇W(1)\nt\n3\nX\ni=1\nα(i) Li = W(1)\nt\n−η∇W(1)\nt\n\u0010\nα(1) L1 +α(2) L2 +α(3) L3\n\u0011\n= W(1)\nt\n−η\n \nα1\n∂L1\n∂f (1)\n∂f (1)\n∂h(1)\n∂h(1)\n∂W(1)\nt\n+ α2\n∂L2\n∂f (2)\n∂f (2)\n∂h(2)\n∂h(2)\n∂h(1)\n∂h(1)\n∂W(1)\nt\n+ α3\n∂L3\n∂f (3)\n∂f (3)\n∂h(3)\n∂h(3)\n∂h(2)\n∂h(2)\n∂h(1)\n∂h(1)\n∂W(1)\nt\n!\n(85)\nI.2\nComplexity analysis\nIn this section we compare ODL and our proposed modified optimization setup which we call\nFast-ODL (FODL) in terms of training time complexity.\nODL training complexity: Consider the computations required to perform an update on hidden\nlayer parameters W(l). Based on the update rule eq. (76), to update weight matrix W(l) we need to\ncalculate L −l + 1 terms in the sum. Assuming that calculating each partial derivative in the previous\nexpressions takes constant time, and noting that the weight update needs to be done for each layer\nl ∈{0, 1, . . . , L} we obtain a complexity of:\nL\nX\nj=1\nL\nX\ni=j\nO(1) = L(L + 1)\n2\n= O(L2)\n(86)\nper training step. Given that we are in an online learning setting with batch size of 1, in a dataset\nof size n we are thus going to take n training steps (one step per training datum) for a total training\ncomplexity of:\nn\nX\n1\nL\nX\nj=1\nL\nX\ni=j\nO(1) = O(nL2)\n(87)\nThe quadratic complexity in the number of layers occurs because of redundant calculations. For\nexample, to update both W(l) and W(l−1) we need to compute ∂Ll\n∂f (l) . However, in the vanilla ODL\nsetup this result is not cached and needs to be recomputed two separate times.\nFODL training complexity: Looking at eq. (76), we note that the number of gradient calculations\nwould decrease significantly if the the gradient could be taken outside the sum:\nW (l)\nt+1 ←W (l)\nt\n−η∇W (l)\nL\nX\nj=l\nα(j) L(f (j), yt).\n(88)\nHowever, this expression cannot be directly computed using automatic differentiation as there\nis no node equal to PL\nj=l α(j) L(f (j), yt) is the computational graph in Fig 5. By introducing a\nconcatenation and weighted summation of the intermediate losses, as shown in Fig. 6, we can generate\nthis node and backpropagate from it. Then using PyTorch’s automatic differentiation framework,\ncomputed gradients are cached such that there is no redundant calculation, i.e., when updating W(1)\n25\nInput\nConcat\nInput\nFigure 6: Our proposed architectural modification that provides equivalent total gradients but a\nmore tractable computational graph. There is only one gradient calculation starting from Ltot. Top\nand bottom figure show the same architecture, bottom emphasizes that αi is a constant that is not\noptimized during backprop.\n26\nwe compute ∂Ll\n∂f (l) and store it. Then for all subsequent calculations for updating W(l) we access the\ncache at negligible computational cost. This yields a total training cost of:\nn\nX\n1\nL\nX\nj=1\nnO(1) = O(nL).\n(89)\nFor example, consider computing W(3)\nt+1 after having computed W(1)\nt+1. Recalling the equations for\ntheir updates shown earlier in this section, we know that:\nW(3)\nt+1 = W(3)\nt\n−ηα3\n∂L3\n∂f (3)\n∂f (3)\n∂h(3)\n∂h(3)\n∂W(3)\nt\nW(2)\nt+1 = W(2)\nt\n−η\n \nα2\n∂L2\n∂f (2)\n∂f (2)\n∂h(2)\n∂h(2)\n∂W(2)\nt\n+ α3\n∂L3\n∂f (3)\n∂f (3)\n∂h(3)\n∂h(3)\n∂h(2)\n∂h(2)\n∂W(2)\nt\n!\nW(1)\nt+1 = W(1)\nt\n−η\n \nα1\n∂L1\n∂f (1)\n∂f (1)\n∂h(1)\n∂h(1)\n∂W(1)\nt\n+ α2\n∂L2\n∂f (2)\n∂f (2)\n∂h(2)\n∂h(2)\n∂h(1)\n∂h(1)\n∂W(1)\nt\n+ α3\n∂L3\n∂f (3)\n∂f (3)\n∂h(3)\n∂h(3)\n∂h(2)\n∂h(2)\n∂h(1)\n∂h(1)\n∂W(1)\nt\n!\nClearly, caching the term in teal would reduce computation. Thus, in FODL computing the update\nfor the first layer W(1)\nt+1 and storing all the partial derivatives along the computational graph provides\nall the necessary information to calculate W(2)\nt+1, . . . , W(L)\nt+1 with computing only one new derivative\n∂h(l)\n∂W(l)\nt . Similarly this holds for computing W(2)\nt+1 after W(1)\nt+1, see orange terms.\nMore precisely, in the standard ODL formulation the redundancy of the teal term is very high as it is\ncomputed L times (the terms in orange L −1 times) but in our formulation every gradient is only\ncomputed once. This yields a total computational saving of:\nn\nX\n1\nL\nX\nj=1\nL\nX\ni=j+1\n= nL(L −1)\n2\nO(1).\n(90)\nBy subtracting the amount of computation saved by FastODL from the total complexity of the base\nODL method we obtain the complexity of our proposed approach and it is linear:\nn\nX\n1\nL\nX\nj=1\nL\nX\ni=j\nO(1) −\nn\nX\n1\nL\nX\nj=1\nL\nX\ni=j+1\nO(1) = n\n\u0012L(L + 1)\n2\n−L(L −1)\n2\n\u0013\nO(1) = nLO(1) = O(nL).\n(91)\n27\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2024-05-28",
  "updated": "2024-05-28"
}