{
  "id": "http://arxiv.org/abs/2405.20538v1",
  "title": "Q-learning as a monotone scheme",
  "authors": [
    "Lingyi Yang"
  ],
  "abstract": "Stability issues with reinforcement learning methods persist. To better\nunderstand some of these stability and convergence issues involving deep\nreinforcement learning methods, we examine a simple linear quadratic example.\nWe interpret the convergence criterion of exact Q-learning in the sense of a\nmonotone scheme and discuss consequences of function approximation on\nmonotonicity properties.",
  "text": "Published as a Tiny Paper at ICLR 2024\nQ-LEARNING AS A MONOTONE SCHEME\nLingyi Yang\nMathematical Institute\nUniversity of Oxford\nyangl@maths.ox.ac.uk\nABSTRACT\nStability issues with reinforcement learning methods persist. To better under-\nstand some of these stability and convergence issues involving deep reinforcement\nlearning methods, we examine a simple linear quadratic example. We interpret the\nconvergence criterion of exact Q-learning in the sense of a monotone scheme and\ndiscuss consequences of function approximation on monotonicity properties.\n1\nINTRODUCTION\nSutton & Barto (2018) coined the combination of bootstrapping, off-policy learning, and function\napproximations as “the deadly triad” due to stability issues. Let x denote the state, and u the action.\nA simple system for analysis is the linear quadratic (LQ) regulator where the dynamics is linear and\nthe objective function is quadratic\nxt+1 = Axt + But,\nmin\n\u001a X\nt\nx⊺\nt Qxt + u⊺\nt Put\n\u001b\n.\nRecht (2019) observed unstable behaviour on this system using a vanilla policy gradient method.\nAgarwal et al. (2021) showed policy gradients converge when the value function is increas-\ning/monotone pointwise. We know that in general, the monotonicity of a numerical method can\nhave a large impact on its convergence (Godunov & Bohachevsky, 1959; Crandall & Majda, 1980).\nWe say an explicit numerical method with updates un+1\ni\n= S({un\nj }j∈I), S : Rk →R is monotone\nif the operator S is monotone, i.e. if u ≥v ⇒Su ≥Sv. Linear schemes are monotone only if\nall coefficients of un\nj are non-negative (see the discussion on positive coefficient discretisation by\nForsyth & Labahn (2007)). Barles & Souganidis (1991) proved that a stable, consistent, and mono-\ntone scheme converges (as the mesh size tends to zero) to the viscosity solution (Crandall & Lions,\n1983; Crandall et al., 1992). We look at the impact of monotonicity in the context of continuous LQ\nproblems, and the implications for Q-learning if we view it as a discretised numerical method.\n2\n1D DETERMINISTIC LINEAR QUADRATIC PROBLEM\nConsider a continuous, infinite-horizon, discounted control problem with linear state dynamics\ndXx,u\nt\ndt\n= b(Xx,u\nt\n, ut),\nb(Xx,u\nt\n, ut) = αXx,u\nt\n+ ut,\nXx,u(0) = x,\nwhere {Xx,u}t is the process starting at x and following the policy u. Let the cost function be\nJ(us; Xs) =\nR ∞\n0\ne−βsf(Xx,u\ns\n, us)ds, where f(Xx,u\nt\n, ut) = (Xx,u\nt\n)2 + u2\nt, and define the value\nfunction V (x) = infu J(u; x). The Hamilton–Jacobi–Bellman (HJB) equation is\n−βV (x) + inf\nu\n\b\n∂xV (x) · b(x, u) + f(x, u)\n\t\n= 0.\n(1)\nTo solve the HJB (1) numerically, we can rearrange the formula to get a fixed point problem\nV n+1 = β + γ\nγ\nV n + 1\nγ min\nu\n\u001a\n∂xV n · (αx + u) + x2 + u2\n\u001b\n.\n(2)\nIf we choose a finite difference scheme for the derivative depending on the sign of αxi + u, then we\ncan obtain an upwind method that ensures monotonicity. This subtle variation of finite difference\nschemes for different parts of the domain has an enormous impact on the stability in estimating value\nfunction and policies through value/policy iteration. A brief introduction to the continuous control\nset-up and the stability analysis of (2) can be found in Appendix A.\n1\narXiv:2405.20538v1  [cs.LG]  30 May 2024\nPublished as a Tiny Paper at ICLR 2024\n2.1\nQ-LEARNING\nQ-learning arises as a fixed point iteration to the Bellman optimality equation in discrete time\nQn+1(xt, ut) =(1 −α)Qn(xt, ut) + α\n\u0014\nf(xt, ut) + γ min\nˆu Qn(xt+1, ˆu)\n\u0015\n,\nwhere f(x, u) denotes the instantaneous reward function (Appendix B). We see the coefficients are\nnon-negative for 0 ≤α ≤1; within this range, the update step is monotone. This aligns with the\nusual range for the step size α in Q-learning. We explore larger values outside this range analogous\nto over-relaxation in optimisation (Saad, 2003). In our experiments, Q-learning is seen to be stable\nagainst the theoretical limits for a deterministic LQ problem when 0 ≤α ≤1 (Figure 1). We\nconverge to the correct value function, and the differences in policy are due to discretisation error.\nSince monotonicity is a sufficient condition for convergence, having α outside of this range does not\nnecessitate the method breaking. In Figure 5 we see that we still converge to the theoretical values\nfor α = 1.3. Instability occurs when α is sufficiently large, α = 1.8 will do (Figure 2).\n−20\n−10\n0\n10\n20\nstate\n−600\n−500\n−400\n−300\n−200\n−100\n0\nvalue function\nValue function for step size = 0.8\n−20\n−10\n0\n10\n20\nstate\n−10\n−5\n0\n5\n10\npolicy\nPolicy for step size = 0.8\nFigure 1: Q-learning: learnt value function and policy (blue) against theoretical (orange) for α = 0.8\n−20\n−10\n0\n10\n20\nstate\n0.0\n0.5\n1.0\n1.5\n2.0\nvalue function\n×1040\nValue function for step size = 1.8\n−20\n−10\n0\n10\n20\nstate\n−20\n−10\n0\n10\npolicy\nPolicy for step size = 1.8\nFigure 2: Q-learning: learnt value function and policy (blue) against theoretical (orange) for α = 1.8\nEnsuring monotonicity with a function approximator is non-trivial. In the LQ case, note Q∗(xt, u) =\nf(xt, u)+V ∗(xt+1) and f and V can be expressed as a quadratic function in x and u (Appendix A).\nTherefore a linear function approximator for Q(x, u) with features of terms up to quadratic powers\nin x and u will be a suitable function class. To be precise, ˜Q(x, u, w) = X(x, u)⊺w, where X(x, u)\nare the features that we extract from our state-action pair, and w are the weights. Then (Appendix B)\n˜Qn+1(x, u, wn+1) = αnf(x, u)X⊺(x, u)X(x, u) + (1 −αnX⊺(x, u)X(x, u)) ˜Qn(x, u, wn)\n+ αnX⊺(x, u)X(x, u) max\n˜u\n˜Qn(x′, ˜u, wn).\nTo ensure monotonicity, the features X(x, u) need to be bounded and step sizes are sufficiently\nsmall such that αnX⊺(x, u)X(x, u) < 1, i.e. αn(x, u) < 1/(X⊺(x, u)X(x, u)). This condition is\ndependent on the state and action, so a potential issue is that we may not sufficiently explore the state\nspace (e.g. if for large values of x, the action u is also large then α needs to be very small). Even\na simple, linear function approximator can disrupt monotonicity causing instability so violations in\nthe nonlinear case (neural networks) may explain the stability issues we observe in practice.\n2\nPublished as a Tiny Paper at ICLR 2024\nACKNOWLEDGEMENTS\nThe author would like to thank Prof Samuel N. Cohen and Dr Jaroslav Fowkes for their support and\nfeedback. This work was supported by the EPSRC [EP/L015803/1].\nURM STATEMENT\nThe authors acknowledge that at least one key author of this work meets the URM criteria of ICLR\n2024 Tiny Papers Track.\nREFERENCES\nAlekh Agarwal, Sham M. Kakade, Jason D. Lee, and Gaurav Mahajan. On the Theory of Policy Gra-\ndient Methods: Optimality, Approximation, and Distribution Shift. Journal of Machine Learning\nResearch, 22(98):1–76, 2021.\nGuy Barles and Panagiotis E. Souganidis.\nConvergence of approximation schemes for fully\nnonlinear second order equations.\nAsymptotic Analysis, 4(3):271–283, 1991.\ndoi: 10.3233/\nASY-1991-4305. Publisher: IOS Press.\nMichael G. Crandall and Pierre-Louis Lions. Viscosity Solutions of Hamilton–Jacobi Equations.\nTransactions of the American Mathematical Society, 277(1):1–42, 1983. ISSN 00029947. Pub-\nlisher: American Mathematical Society.\nMichael G. Crandall and Andrew Majda. Monotone difference approximations for scalar conserva-\ntion laws. Mathematics of Computation, 34(149):1–21, 1980. ISSN 00255718, 10886842.\nMichael G. Crandall, Hitoshi Ishii, and Pierre-Louis Lions. User’s guide to viscosity solutions of\nsecond order partial differential equations. Bulletin of the American Mathematical Society, 27:\n1–67, 1992.\nPeter A. Forsyth and George Labahn. Numerical methods for controlled Hamilton–Jacobi–Bellman\nPDEs in finance. Journal of Computational Finance, 2007.\nSergei K. Godunov and I. Bohachevsky. Finite difference method for numerical computation of\ndiscontinuous solutions of the equations of fluid dynamics. Matematiˇceskij sbornik, 47(89)(3):\n271–306, 1959.\nNikolay V. Krylov. Controlled Diffusion Processes. Stochastic Modelling and Applied Probability.\nSpringer Berlin Heidelberg, 1980. ISBN 978-3-540-70914-5.\nBenjamin Recht. A Tour of Reinforcement Learning: The View from Continuous Control. Annual\nReview of Control, Robotics, and Autonomous Systems, 2(1):253–279, May 2019. ISSN 2573-\n5144. doi: 10.1146/annurev-control-053018-023825. Publisher: Annual Reviews.\nHerbert Robbins and Sutton Monro. A Stochastic Approximation Method. The Annals of Mathe-\nmatical Statistics, 22(3):400–407, 1951. ISSN 00034851. Publisher: Institute of Mathematical\nStatistics.\nYousef Saad. Iterative Methods for Sparse Linear Systems. Society for Industrial and Applied\nMathematics, 2nd edition, 2003. doi: 10.1137/1.9780898718003.\nRichard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2nd\nedition, 2018.\n3\nPublished as a Tiny Paper at ICLR 2024\nA\n1D CONTINUOUS LQ PROBLEM\nConsider the typical deterministic, infinite-horizon, discounted control problem with linear state\ndynamics\ndXx,u\nt\ndt\n= b(Xx,u\nt\n, ut),\nb(Xx,u\nt\n, ut) = αXx,u\nt\n+ ut,\nXx,u(0) = x,\n(3)\nwhere {Xx,u}t is the process starting at x and following the policy u thereafter. Let the cost function\nbe given by\nJ(us; Xs) =\nZ ∞\n0\ne−βsf(Xx,u\ns\n, us)ds,\n(4)\nwhere\nf(Xx,u\nt\n, ut) = (Xx,u\nt\n)2 + u2\nt.\nDefine the value function\nV (x) = inf\nu J(u; x).\n(5)\nLemma 1. The dynamic programming principle gives us\nV (x) = inf\nu\n\u001a Z h\n0\ne−βs\u0000(Xx,u\ns\n)2 + u2\ns\n\u0001\nds + e−βhV (Xx,u\nh\n)\n\u001b\n.\nProof.\nV (x) = inf\nu\nZ ∞\n0\ne−βsf(Xx,u\ns\n, us)ds\n= inf\nu\nZ h\n0\ne−βsf(Xx,u\ns\n, us)ds + inf\nu\nZ ∞\nh\ne−βsf(Xx,u\ns\n, us)ds\n= inf\nu\nZ h\n0\ne−βsf(Xx,u\ns\n, us)ds + inf\n˜u\nZ ∞\n0\ne−β(t+h)f(X\nXx,u\nh\n,˜u\nt\n, ˜ut)dt\n= inf\nu\nZ h\n0\ne−βsf(Xx,u\ns\n, us)ds + e−βh inf\n˜u\nZ ∞\n0\ne−βtf(X\nXx,u\nh\n,˜u\nt\n, ˜ut)dt\n= inf\nu\nZ h\n0\ne−βsf(Xx,u\ns\n, us)ds + e−βhV (Xx,u\nh\n).\n■\nSee (Krylov, 1980) for a rigorous formulation of this problem detailing the set of admissible controls\nand growth conditions assumed. From this form of the value function, we can derive the Hamilton–\nJacobi–Bellman (HJB) equation.\nLemma 2. The HJB equation of the system given by (3) and (4) is\n−βV (x) + inf\nu\n\b\n∂xV (x) · b(x, u) + f(x, u)\n\t\n= 0.\n(6)\nProof. Let us first consider a constant control ¯u. By the definition of the value function, we must\nhave\nV (x) ≤\nZ h\n0\ne−βsf(Xx,¯u\ns\n, ¯u)ds + e−βhV (Xx,¯u\nh\n).\nNow by the chain rule we have\nV (Xx,¯u\nh\n) = V (x) +\nZ h\n0\nb(Xx,¯u\nt\n, ¯u)∂xV (Xx,¯u\nt\n)dt,\ntherefore\nV (x) ≤\nZ h\n0\ne−βsf(Xx,¯u\ns\n, ¯u)ds + e−βh\n\u0012\nV (x) +\nZ h\n0\nb(Xx,¯u\nt\n, ¯u)∂xV (Xx,¯u\nt\n)dt\n\u0013\n.\n4\nPublished as a Tiny Paper at ICLR 2024\nUpon rearranging, we have\n1 −e−βh\nh\nV (x) ≤1\nh\nZ h\n0\ne−βsf(Xx,¯u\ns\n, ¯u)ds + e−βh\nh\nZ h\n0\nb(Xx,¯u\nt\n, ¯u)∂xV (Xx,¯u\nt\n)dt.\nWe consider the limit as h tends to 0. By L’Hˆopital’s rule\nlim\nh→0\n1 −e−βh\nh\n= lim\nh→0\nβe−βh\n1\n= β,\nand by applying the Mean Value Theorem, we obtain\nβV (x) ≤f(x, ¯u) + b(x, ¯u)∂xV (x).\nTherefore, for an arbitrary constant cost ¯u we have\n−βV (x) + f(x, ¯u) + b(x, ¯u)∂xV (x) ≥0,\nthus\n−βV (x) + inf\nu\n\u0002\nf(x, u) + b(x, u)∂xV (x)\n\u0003\n≥0.\nIf we apply the above analysis with the optimal control, we will find that equality holds\n−βV (x) +\n\u0002\nf(x, u∗) + b(x, u∗)∂xV (x)\n\u0003\n= 0,\nand therefore\n−βV (x) + inf\nu\n\u0002\nf(x, u) + b(x, u)∂xV (x)\n\u0003\n= 0\nas required.\n■\nWe can find the optimal feedback control of (6).\nLemma 3. The optimal control is given by\nu∗= −Γx.\nProof. Let us propose the following ansatz for (6)\nV (x) = Γx2 + 2κx + λ.\nWe then find the derivative with respect to x, dV/dx = 2Γx+2κ and substitute into the Hamiltonian\nto get\n∂xV (x) · b(x, u) + f(x, u) = Qx2 + Ru2 + (2Γx + 2κ)(Ax + Bu).\nBy taking the partial derivative w.r.t u and setting to zero for the stationary point, we have that\n2Ru + B(2Γx + 2K) = 0\nand the optimal control is given by\nu∗= −B(Γx + κ)\nR\n.\nEvaluate the Hamiltonian at the optimal control\n∂xV (x) · b(x, u) + f(x, u)|u∗= Qx2 + R\n\u0012\n−B(Γx + κ)\nR\n\u00132\n+ (2Γx + 2κ)\n\u0012\nAx −B2(Γx + κ)\nR\n\u0013\n= Qx2 + \u0018\u0018\u0018\u0018\u0018\u0018\nB2(Γx + κ)2\nR\n+ 2AΓx2 + 2Aκx −\u00012B2(Γx + κ)2\nR\n=\n\u0012\nQ + 2AΓ −B2Γ2\nR\n\u0013\n+\n\u0012\n2Aκ −2B2Γκ\nR\n\u0013\nx −B2κ2\nR\n.\nSubstitute this into the HJB\nβ(Γx2 + κx + λ) −\n \u0012\nQ + 2AΓ −B2Γ2\nR\n\u0013\n+\n\u0012\n2Aκ −2B2Γκ\nR\n\u0013\nx −B2κ2\nR\n!\n= 0\n5\nPublished as a Tiny Paper at ICLR 2024\nand set each coefficient to zero\n0 = B2Γ2\nR\n+ Γ(β −2A) −Q,\n(7)\n0 = 2κ(β + B2Γ\nR\n−A),\n(8)\n0 = βλ + B2κ2\nR\n.\n(9)\nFrom (8), we see that either κ = 0 or Γ = R(A−β)\nB2\n. The latter case would not generally satisfy (7).\nThus we must have κ = 0. Substituting this into (9), we also get that λ = 0. Thus the only non-zero\ncoefficient of V (x) is Γ, which satisfies the quadratic (7). We choose the root that ensures we have\na stable solution (typically positive definite).\nFor our problem (3) we have that A = α, B = R = Q = 1, hence Γ must satisfy\nΓ2 + (β −2α)Γ −1 = 0,\n(10)\nwhich has two roots Γ+ and Γ−. We choose the root that would result in a positive eigenvalue. Our\noptimal control is then u∗= −Γx.\n■\nIf we want to solve the HJB (6) numerically, we can rearrange the formula to get a fixed point\nmethod\nV n+1 = β + γ\nγ\nV n + 1\nγ min\nu\n\u001a\n∂xV n · (αx + u) + x2 + u2\n\u001b\n.\n(11)\nIf we na¨ıvely tried to solve this directly by approximating the derivative with finite differences, for\nexample, central difference, then we obtain the following numerical method\nV n+1\ni\n= β + γ\nγ\nV n\ni + 1\nγ min\nu\n\u001aV n\ni+1 −V n\ni−1\n2∆x\n· (αxi + u) + x2\ni + u2\n\u001b\n,\n(12)\nwhere Vi = V (xi) and xi = xi−1 + ∆x, and we can take forward and backward differencing at the\nboundary.\nThis method is not monotone in general as the coefficients of V n\ni+1 and V n\ni−1 are the opposite signs\nto each other.\nTo obtain a monotone method we look at an upwind scheme. Let us approximate the derivative w.r.t.\nx with forward differences or backward differences depending on the sign of αx + u\n∂xV ≈Vi+1 −Vi\n∆x\nif αx + u > 0\n∂xV ≈Vi −Vi−1\n∆x\nif αx + u < 0.\nFor αx + u > 0,\nV n+1\ni\n= β + γ\nγ\nV n\ni + 1\nγ min\nu\n\u001aV n\ni+1 −V n\ni\n∆x\n(αxi + u) + x2\ni + u2\n\u001b\n=\n\u0012β + γ\nγ\n−αxi + u∗\nγ∆x\n\u0013\nV n\ni + αxi + u∗\nγ∆x\nV n\ni+1 + x2\ni + (u∗)2\nγ\n,\nwhere u∗is the argmin of the Hamiltonian. Now the coefficient of V n\ni+1 is positive, but the coefficient\nof V n\ni will only be positive if we have\n∆x > αxi + u∗\nβ + γ\n.\n(13)\nHence taking a large value for γ enables us to use finer meshes.\n6\nPublished as a Tiny Paper at ICLR 2024\nSimilarly when αx + u < 0,\nV n+1\ni\n= β + γ\nγ\nV n\ni + 1\nγ min\nu\n\u001aV n\ni −V n\ni−1\n∆x\n(αxi + u) + x2\ni + u2\n\u001b\n=\n\u0012β + γ\nγ\n+ αxi + u∗\nγ∆x\n\u0013\nV n\ni −αxi + u∗\nγ∆x\nV n\ni−1 + x2\ni + (u∗)2\nγ\n.\nSince αx + u < 0, the coefficient of V n\ni−1 is positive, but the coefficient of V n\ni will only be positive\nif we have\n∆x > −αxi + u∗\nβ + γ\n.\n(14)\nNote that by updating our value function as\nV n+1 = β + γ\nγ\nV n + 1\nγ min\nu {∂xV n · (ax + u) + (x2 + u2)},\nthis is precisely the value iteration updates. We only do one cycle of policy evaluation before choos-\ning a new policy by taking u to be the argmin of the Hamiltonian (policy improvement). A monotone\nnumerical scheme for the LQ problem with value iteration updates is described in Algorithm 1.\nAlgorithm 1: Value Iteration for LQ\n1. Initialisation\nDiscretise the state and action spaces\nParameters: a small threshold θ > 0 determining accuracy of estimation (convergence\ncriterion), a maximum iteration count N\nInitialize V (x) as zeros\nInitialize ∆> θ.\n2. Iteration\nrepeat\nfor each x ∈X do\nˆV (x) ←V (x),\nu∗←argmin Hamiltonian at x\nV (x) ←β+γ\nγ\nˆV (x) + 1\nγ H(x, u∗, ˆV ),\nwhere H(x, u∗, ˆV ) is the Hamiltonian evaluated at x, with control u∗and using the\nsuitable differencing for a monotone scheme.\nend\n∆←max | ˆV −V |.\nuntil ∆< θ or N;\nFor a policy iteration-like update, we need to have a fixed policy that we evaluate the value func-\ntion on until convergence before we make a policy improvement step. The pseudocode is given in\nAlgorithm 2\nLet us be more precise on finding the control. To recap, for the value iteration, we are updating at\neach iteration with the rule\nV n+1 = −β −γ\nγ\nV n + 1\nγ min\nu\nn\n∂xV n · (αx + u) + (x2 + u2)\no\n.\nIn order to obtain a monotone scheme, we must apply forward differencing on ∂xV n if αx + u > 0\nor backward differencing otherwise. However, for value iteration, we are also minimising over all\nu, which means that there are a few cases we can fall into. Let us consider finding the correct action\nfor each discretised state xi.\nWe have the regions R1 = {u : αxi + u ≥0} and R1 = {u : αxi + u < 0}. Let\nHn(u) =\n\u001ahn\n1(u)\nif u ∈R1\nhn\n2(u)\nif u ∈R2\nwhere\nhn\n1 = V n\ni+1 −V n\ni\n∆x\n(αxi + u) + x2\ni + u2,\n7\nPublished as a Tiny Paper at ICLR 2024\nAlgorithm 2: Policy Iteration for LQ\n1. Initialisation\nDiscretise the state and action spaces\nParameters: two small thresholds θv > 0 and θu > 0 determining accuracy of estimation\n(convergence criterion), maximum iteration counts Nv, Nu\nInitialize u(x) ∈U arbitrarily for all x ∈X (let us say equal to 1).\nInitialize ∆> θ.\n2. Policy Evaluation\nrepeat\nfor each x ∈X do\nˆV (x) ←V (x),\nV (x) ←β+γ\nγ\nˆV (x) + 1\nγ H(x, u, ˆV ),\nwhere H(x, u, ˆV ) is the Hamiltonian evaluated at x, with the current control u and\nusing the suitable differencing for a monotone scheme.\nend\n∆←max | ˆV −V |.\nuntil ∆< θv;\n3. Policy Improvement\nfor each x ∈X do\nˆu(x) ←u(x)\nu(x) ←argmin of Hamiltonian at x\nend\nIf max |ˆu −u| < θu, then stop and return V ≈V ∗and u ≈u∗; else go back to Step 2.\nand\nhn\n2 = V n\ni −V n\ni−1\n∆x\n(αxi + u) + x2\ni + u2.\nOur problem is now to find\ny = min\nu Hn(u).\nThe smoothness of H(u) depends on the smoothness of the value function. In the case of the LQ\nproblem, the value function is smooth, therefore H(u) is smooth except on the boundary of R1 and\nR2, which in this case is a linear boundary, u = −αxi.\nWe can solve the above problem numerically, by finding\ny∗\n1 = min\nu h1(u),\nu ∈R1\nand\ny∗\n2 = min\nu h2(u),\nu ∈R2\nthen taking the minimum over these 2 values\ny = min{y1, y2}\nto get the desired control for value iteration.\nFor policy iteration, when we are doing policy evaluation, as the policy is fixed, we just need to\ncheck the sign of αx + u. However, we need to also ensure that monotonicity is maintained in\npolicy improvement. The policy improvement step can be described as\nu∗= arg min\nu\nH(u).\nIn this case we can again solve the two minimisation problem separately to find y∗\n1 and y∗\n2 and\nchoose u based on which of these have the lower value.\nIn Figures 3 and 4, we see the result of applying the downwind iteration and upwind iteration re-\nspectively. We see clearly the instability arising in the downwind case, and how important it is to\nchoose between forward and backward difference so that we ensure monotonicity.\n8\nPublished as a Tiny Paper at ICLR 2024\n−10\n−5\n0\n5\n10\nstate\n−4\n−2\n0\n2\n4\npolicy\noptimal policy\ndownwind learned policy\n−10\n−5\n0\n5\n10\nstate\n0\n10\n20\n30\nvalue function\noptimal value\ndownwind learned value\nFigure 3: An intermediate policy and value function for a downwind method (the policy and value\nfunction have not converged yet). Instability forms and becomes amplified with further iterations.\n−10\n−5\n0\n5\n10\nstate\n−4\n−2\n0\n2\n4\npolicy\noptimal policy\nupwind learned policy\n−10\n−5\n0\n5\n10\nstate\n0\n10\n20\n30\nvalue function\noptimal value\nupwind learned value\nFigure 4: An intermediate policy and value function for an upwind method. Note that whilst the\npolicy has not converged yet, there are no instabilities in this case.\nB\nQ-LEARNING (DISCRETE SETTING)\nLet the transition be xt+1 = b(xt, ut) and let f(x, u) denote the reward function. The state value\nfunction under policy π is defined as\nV π(x) =\n∞\nX\nt=0\nγtf(xt, π(xt)),\nx0 = x.\nThe Bellman optimality equations are\nV ∗(xt) = max\nut\nh\nf(xt, ut) + γV ∗(xt+1)|xt+1=b(xt,ut)\ni\n,\nQ∗(xt, ut) = f(xt, ut) + γV ∗(xt+1)|xt+1=b(xt,ut)\n= f(xt, ut) + γ min\nˆa Q(xt+1, ˆa)|xt+1=b(xt,ut).\nThus the first order condition is given by\n∂Q∗(st+1, ˆa)\n∂ˆa\n= 0\nor\n∂f\n∂ut\n+ γ ∂V ∗\n∂xt+1\n∂xt+1\n∂ut\n= 0.\nMonotonicity gives a sufficient condition for stability but it is not necessary. We see that Q-learning\nis also stable for α = 1.3 case, as seen in Figure 5.\nNote that, when we have a constant step size α > 1, not only can we no longer guarantee mono-\ntonicity, but the square summability condition of the step size α (Robbins & Monro, 1951) would\nalso be violated.\n9\nPublished as a Tiny Paper at ICLR 2024\n−20\n−10\n0\n10\n20\nstate\n−600\n−500\n−400\n−300\n−200\n−100\n0\nvalue function\nValue function for step size = 1.3\n−20\n−10\n0\n10\n20\nstate\n−10\n−5\n0\n5\n10\npolicy\nPolicy for step size = 1.3\nFigure 5: Learnt value function and policy (blue) against theoretical (orange) for α = 1.3\nFor ‘table-lookup’ methods, as long as all states are updated infinitely often and step sizes satisfy\nsquare summability conditions (Robbins & Monro, 1951), then we have convergence for policy\nevaluation. This is not guaranteed when we have a general function approximator. The problem with\nhaving exact table representation is that they are slow to converge, and the number of states/state-\naction pairs suffer from the curse of dimensionality. Using a function approximator, we postulate that\nas long as the function approximation preserves monotonicity, then Q-learning should still converge.\nWhen we are approximating Q(x, u) by ˜Q(x, u, w), we want to minimise the expected error\n1\n2E\n\u0002\n(Q(x, u) −˜Q(x, u, w))2\u0003\n.\nWe use a semi-gradient descent method and take update steps in the form of\nwn+1 = wn + αn\n\u0000Q(x, u) −˜Q(x, u, wn)\n\u0001\n∇wn ˜Q(x, u, wn).\nFor reinforcement learning/control problems, we do not have access to the true value function\nQ(x, u) and therefore use an approximation in its place. An approximate method derived from\nQ-learning uses\nQ(x, u) ≈f(x, u) + γ max\n¯u\n˜Q(x′, ¯u, w).\nSince\nQ∗(xt, u) = f(xt, u) + V ∗(xt+1)\nand we know in the LQ case, both f and V can be expressed as a quadratic function in x and u\n(Appendix A), a linear function approximator for Q(x, u) with features of terms up to quadratic\npowers in x and u will be a suitable function class for this approximation.\nThis linear approximator for Q(x, u) is represented as\n˜Q(x, u, w) = X(x, u)⊺w,\n(15)\nwhere X(x, u) are the (e.g. quadratic) features that we extract from our state-action pair, and w are\nthe weights. Then\n˜Qn+1(x, u, wn+1) = X⊺(x, u)\n\u0000wn + αn\n\u0000Q(x, u) −˜Qn(x, u, wn)\n\u0001\nX(x, u)\n\u0001\n≈˜Qn(x, u, wn)\n+ αnX⊺(x, u)\n\u0000f(x, u) + γ max\n˜u\n˜Qn(x′, ˜u, wn) −˜Qn(x, u, wn)\n\u0001\nX(x, u)\n\u0001\n= αnf(x, u)X⊺(x, u)X(x, u) + (1 −αnX⊺(x, u)X(x, u)) ˜Qn(x, u, wn)\n+ αnX⊺(x, u)X(x, u) max\n˜u\n˜Qn(x′, ˜u, wn).\n(16)\nMonotonicity implies that the action-value function ˜Qn+1(x, u, wn+1) must be non-decreasing with\nrespect to the action-value function at the other points. Hence to ensure monotonicity we need to\nensure that the features X(x, u) are bounded and we take sufficiently small step size such that\nαnX⊺(x, u)X(x, u) < 1, i.e. αn(x, u) < 1/(X⊺(x, u)X(x, u)), the step size is dependent on the\nstate and action. A potential problem is that we may not sufficiently explore the state space with a\nstep size that is dependent on the state-action space (e.g. if for large values of x, the action u is also\nlarge then α needs to be very small).\n10\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2024-05-30",
  "updated": "2024-05-30"
}