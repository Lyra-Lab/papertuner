{
  "id": "http://arxiv.org/abs/2406.03847v2",
  "title": "Lean Workbook: A large-scale Lean problem set formalized from natural language math problems",
  "authors": [
    "Huaiyuan Ying",
    "Zijian Wu",
    "Yihan Geng",
    "Jiayu Wang",
    "Dahua Lin",
    "Kai Chen"
  ],
  "abstract": "Large language models have demonstrated impressive capabilities across\nvarious natural language processing tasks, especially in solving mathematical\nproblems. However, large language models are not good at math theorem proving\nusing formal languages like Lean. A significant challenge in this area is the\nscarcity of training data available in these formal languages. To address this\nissue, we propose a novel pipeline that iteratively generates and filters\nsynthetic data to translate natural language mathematical problems into Lean 4\nstatements, and vice versa. Our results indicate that the synthetic data\npipeline can provide useful training data and improve the performance of LLMs\nin translating and understanding complex mathematical problems and proofs. Our\nfinal dataset contains about 57K formal-informal question pairs along with\nsearched proof from the math contest forum and 21 new IMO questions. We\nopen-source our code at https://github.com/InternLM/InternLM-Math and our data\nat https://huggingface.co/datasets/InternLM/Lean-Workbook.",
  "text": "Lean Workbook: A large-scale Lean problem set\nformalized from natural language math problems\nHuaiyuan Ying12∗, Zijian Wu13∗, Yihan Geng14∗, Jiayu Wang1, Dahua Lin1, Kai Chen1\n1Shanghai AI Laboratory, 2Tsinghua University, 3Shanghai Jiao Tong University, 4Peking University\ninternlm@pjlab.org.cn\nAbstract\nLarge language models have demonstrated impressive capabilities across various\nnatural language processing tasks, especially in solving mathematical problems.\nHowever, large language models are not good at math theorem proving using\nformal languages like Lean. A significant challenge in this area is the scarcity of\ntraining data available in these formal languages. To address this issue, we propose\na novel pipeline that iteratively generates and filters synthetic data to translate\nnatural language mathematical problems into Lean 4 statements, and vice versa.\nOur results indicate that the synthetic data pipeline can provide useful training\ndata and improve the performance of LLMs in translating and understanding\ncomplex mathematical problems and proofs. Our final dataset contains about 57K\nformal-informal question pairs along with searched proof from the math contest\nforum and 21 new IMO questions. We open-source our code at https://github.\ncom/InternLM/InternLM-Math and our data at https://huggingface.co/\ndatasets/InternLM/Lean-Workbook.\n1\nIntroduction\nProving theorems is one of the most fundamental goals in mathematics, which requires complex\nmath reasoning and a rich store of math knowledge. Recently, large language models (LLMs)\n[15, 25, 20, 3, 6, 24, 33] have made great progress in solving grade-school [5] and even high-school\nlevel math problems [8] through chain-of-thought reasoning [28]. LLMs can also interact with\nproof assistants including Lean [18], Coq [26], or Isabelle [21] to prove theorems. However, the\nperformance of theorem proving is not satisfying with LLMs [34].\nOne reason for this weakness is data sparsity. The mainstream approach for LLMs in learning\ntheorem proving is through expert iteration[1, 30, 14, 22, 31]. LLMs search the proof in the given\nmath problem and statement set like MiniF2F [34] and Mathlib [17] and learn from their success\ntrajectories. However, the amount of data in MiniF2F is limited because formalizing problems\nrequires significant labor from human experts. Though Mathlib is a very large dataset that contains\nthe formalization of different math subjects in Lean, it mainly proves fundamental math theorems\ninstead of contest-level problems. Therefore, an initial step toward a better automatic theorem-proving\nmodel is to create enough high-quality formalized statements.\nIn this work, we present Lean Workbook: an iterative autoformalization pipeline, together with\na large-scale Lean problem set. We train our autoformalization model based on active learning.\n∗Work done during internships at Shanghai AI Laboratory.\nSubmitted to the 38th Conference on Neural Information Processing Systems (NeurIPS 2024) Track on Datasets\nand Benchmarks. Do not distribute.\narXiv:2406.03847v2  [cs.CL]  7 Jun 2024\nFigure 1: The data contribution of our Lean Workbook pipeline. Three rounds of filtering will mostly\nensure the accuracy of output data. By applying the pipeline to the AOPS and the Compfiles data\nsources respectively, we derive 21 formalized IMO questions and about 57k synthetic training data\nfor autoformalizaion.\nAt each turn, we use our model to translate natural language problems into formal statements and\nback-translate to natural language problems collected from the math contest forum2. We use Lean\ncompiler and Natural Language Inference (NLI) to check if it is a valid formalization. We sample\ninvalid formalization and require human experts to modify them into a valid formalization and add\nthem to the training set. Through the supplement of human-labeled data pairs, the translation model\ngradually learned to translate between Lean 4 formal language and natural language questions of\ndifferent types of problems.\nWe autoformalized 57K math problems in the final round. Manual examination reports an accuracy\nof 93.5% of a random sample of the Lean Workbook. The same filtering process produces 21 new\nformal statements of the IMO questions which do not appear in Compfiles 3.\nIn conclusion, our contribution can be summarized as follows:\n• We propose an active learning pipeline for autoformalizing natural language questions.\n• We open-source our translation model and pipeline, which can be used for autoformalizing\ndiverse topics of math statements.\n• We open-source a dataset containing 57k formalized math problems (5k of them have formal\nsolutions) which can be used for autoformalization and auto theorem proving.\n• We formalize 21 new IMO questions that have not appeared in Compfiles.\n2\nPreliminaries\nFormal proof involves establishing claims that are expressed in precise mathematical terms in\nprogramming languages. Lean 4, which is the latest version of Lean Theorem Prover, aims to provide\nan open-source platform for correct and maintainable code for formal verification. The Lean language\ncan claim a theorem and prove it by tactics or pretend to complete the proof using \"sorry\". Lean 4\nwill return a \"No goals\" signal if the proof is completed.\nThe Mathlib is a user-maintained library for Lean 4. With the help of Mathlib, we can utilize other’s\npreviously formalized theorem or function to state our theorem and proof process. Therefore, in the\nfollowing paragraphs, we default talk about applying Mathlib as MiniF2F does in its environments.\n2https://artofproblemsolving.com/community\n3https://github.com/dwrensha/compfiles\n2\nOur work focuses on the translation of questions instead of proof. Therefore, we will always use\n\"sorry\" for the proof. A typical Lean 4 statement looks as follows. The ’theorem’ declares a type of\nthis proposition, followed by the theorem name. Then it specifies all the variables and their types,\nalong with several conditions separated by brackets. Finally, the conclusion starts after the colon, and\n\":= by sorry\" finishes the proof. Here is an example:\ntheorem ex_1 (n p : N) (hp: Nat.Prime p) (h1 : p | n) : { (x, y) : N × N | x + y =\nn ∧Nat.gcd x y = p }.Finite := by sorry\n3\nRelated works\n3.1\nAutoformalization\nAutoformalization [30] refers to translating natural language math statements or proofs into formal\nlanguages. Previous works have autoformalized different levels of mathematics including grade-\nschool level [9, 19], high-school contest level [30, 16], and undergraduate level mathematics [2, 11]\nutilizing in-context learning or fine-tuned LLMs. Our works focus on formalizing high-school\ncontest-level math problems with a much larger scale. A similar and concurrent work is DeepSeek-\nprover [31] which translates a large-scale Lean problem set from high-school problems. Compared\nto DeepSeek-prover, we apply active learning to reduce incorrect formalization and we manually\nevaluate our proposed dataset and find a high formalization accuracy.\n3.2\nAutomatic Theorem Proving\nUsing large language models to automatically prove math theorems do not have a unified approach.\nThe mainstream approach is to conduct a best-first search or tree search on proof states [7, 23, 14, 13,\n32, 29, 3, 33]. This approach can prevent to generate invalid tactics since they will be rejected by\nthe compiler immediately, but the model cannot predict tactics based on an overall perspective. In\ncontrast, another approach is to leverage LLMs to generate the whole proof based on itself[31] or\nhuman’s proof[12, 27].\n4\nData construction pipeline\nIn this section, we will detailedly describe the whole pipeline for iteratively translating and filtering\ncorrect samples as in Figure 4, and then demonstrate the final dataset construction procedure.\n4.1\nFirst-round pipeline\nWe first collect Lean 4 formal statements with their corresponding natural language questions from\nMiniF2F[34]4 and ProofNet[2]5. Since we do not test autoformalization on MiniF2F and ProofNet,\nwe use all samples from these two datasets.\nThe proof will be declared using \":= sorry\". All the sample pairs would be organized from two\ndirections into the training data to achieve a two-way translation between the formal language and\nnatural language. We also include multi-task Lean 4 instruction data including proving theorems,\npredicting the next tactics, and explaining the Lean proof using natural languages like [7, 11] during\ntraining.\nThe training data can be split into proof questions and questions with an exact gold answer. However,\nLean 4 only supports proof questions, so we rephrase all the solution questions by adding a proof\ngoal. Concretely, we append \"Show that it is {answer}.\" to the original natural questions, while the\nproof goal in formal statements is changed to prove the solved answer should be the gold one.\n4We use the version of https://github.com/rah4927/lean-dojo-mew. Under Apache Licence.\n5We use the version of https://github.com/rahul3613/ProofNet-lean4 Under MIT Licence.\n3\nMiniF2F & ProofNet\nIs problem A and B same?\nsame\nFirst-round Translator\nProblem: Is it \npossible to choose \n1983 numbers … ?\nHuman-labeled data\nImproved Translator\n…\nHuman-labeled data\nFinal Translator\nHuman-labeled data\nLean Workbook\nFigure 2: The main flowchart of our pipeline. Starting from the initial training data, we finetune\nour translation model which is then applied to a natural language problem set. The translated data\nis filtered by Lean 4 compiling, backtranslate and NLI test, and human diagnostic. We manually\nconclude patterns and accordingly add training data into the model fine-tuning in the next iteration.\nThe filtered samples are exported if the labelers consider them to reach enough accuracy.\nThe first-round data collection is fed to our translate model, which is initialized from InternLM-Math-\nPlus-20B [10] which has been pre-trained on Lean-related datasets. The model is fine-tuned for\nthree epochs with a learning rate of 4e −5, with two different but fixed prompts for each translation\ndirection. This translated data serves as a starting point for further iteration. Fine-tuning uses 32\nA100 GPUs and can be finished within several hours.\nAfter training a translation model, we want to improve our model on formalizing problems with\ndiverse math topics. We collect math problems from the math contest forum 6 as our active learning\ndataset. It contains problems from middle to high school math, with varying difficulties up to\nOlympiad levels. We utilize Qwen-1.5-14B-Chat [4] to extract the question, solution, and gold\nanswer from each post of the forum with the following prompt.\nYou are a data labeler. Here is a discussion between math students. It may contain several problems\nand several solutions. Please extract them in a JSON format. Each problem is an element and has\nkeys including problem (str, you should not miss any assumption like non-negativity of numbers, be\nformal), answer (return numbers as a string for calculation problems and return an empty string for\nproof problems), and tags (list of str). Tags should identify the category of this math problem. Possible\ntags contain: equation, inequality, number_theory, algebra, probability, combination, trigonometry,\nand etc.\nIt is observed that some kinds of problems are not suitable for formalizing. Meanwhile, the extraction\nprocess turns out to be unstable and gives badly-stated problems. Firstly, we only keep those with\none of the following tags: inequality, number theory, trigonometry, modular arithmetic, induction,\nfunctional equation, complex numbers, and polynomial. Secondly, we query the Qwen model whether\nthe problem is ill-defined with the following prompt.\nPlease check whether the following math problem is well-defined? Please follow the rules: 1.\nConsider each condition given in the problem, it is not well-defined one variable is used without\ndefinition anywhere in the question.\n2.The problem is not well-defined if it contains more than one goal or no clear goals to solve.\n3. Note that inequalities may omit the statement that x, y, z, a, b, c are real numbers, but they are\n6https://artofproblemsolving.com/community\n4\nwell-defined, do not judge them to be ill-defined.\n4. Please reply **well-defined** or **ill-defined** in the final sentence with bold format, be sure not\nto fail well-defined questions.\nWe filter out the ill-defined questions. The manual revision shows almost no ill-defined questions are\nleft, though a small part of well-defined ones are wrongly omitted. After such cleaning, we use our\ninitial translation model to translate all filtered problems into formal statements.\nThe well-defined subset contains 6652 different tags in total, with 223 tags containing over 100\nsamples. These tags cover a large range of questions from contest-level knowledge points to high-\nschool courses. More than three-thirds of the samples are labeled with algebra-relevant tags, while\ngeometry-related tags are rarely witnessed. It is also noticed that some tags are wrong, especially\nthe \"number theory\" tag is often allocated to inequality problems. Following these findings, we will\nremain keeping working with tags over 100 samples in later analysis and will pay special attention to\nwrong tags during manual diagnostic.\n4.2\nData Diagnostic and Iteration pipeline\nCompiling Correctness test To ensure the accuracy of the formal statements produced by our\ntranslation pipeline, each translated formal theorem undergoes a correctness check within a Lean 4\nenvironment. Initially, the theorem statements are verified independently, using a placeholder \"by\nsorry\" for the proofs, to filter out incorrect statements in advance. The complete theorem, including\nproofs, is then examined. The major bottleneck of this step is the compiling cost of Lean 4 projects.\nTo facilitate the process, we build up a Lean 4 read-eval-print loop (REPL), utilizing Lean 4’s runtime\nmeta-programming facility, which allows for the verification of Lean 4 statements in an interpreted\nmode. The correctness test program can be executed in a multi-process style and can be finished\nin one hour with a 32-core CPU. Our test environment is based on Lean v4.7.0 with Mathlib4 of\nthe same version (which can be cloned by specifying the tag v4.7.0). The Minif2f environment is\nequivalent to the header in https://huggingface.co/datasets/internlm/Lean-Workbook/\ndiscussions/1.\nData Filtering Firstly, the synthetic translation from all problems is processed by the compiling\ncorrectness test. However, it is usually witnessed that a correctly compiled translation actually does\nnot follow the original question. The second step of filtering is based on the back translation ability\nof our model. After the formal statement is translated back into natural questions, we can turn to\nusing a general domain LLM to leverage its Natural Language Inference ability. In our pipeline, we\nstill query the Qwen-1.5-14B-Chat to judge if the original question is the same as the back-translated\nversion. If we do not get a positive response, the sample is marked as needing human revision and\ncorrection. The prompt writes as:\nPlease check following two math problems is same or different? Please consider each statement in\ntwo problems, they are different if any statement is different. Please point out any differences you\nfound. Please reply **same** or **different** in the final sentence with bold format.\nDiagnostic and Human labeling Diagnostic for the data mainly focuses on two kinds of samples:\nthe ones that do not pass the compiling correctness test and the ones that pass the test but do not\nprove to be a correct translation with a positive NLI feedback. The other samples that pass the NLI\ntest are considered to be correct for now. In the first three rounds of our iterations, these two kinds of\nsamples both have relatively obvious patterns. Thus we conclude and modify them accordingly with\nthree human experts who are familiar with both Lean and contest-level math problems 7.\nThe manually modified samples are added to the training data, and a new translation model is fine-\ntuned for the next round of generating and filtering synthetic samples for human diagnostics. These\ntwo processes are the same as in the previous paragraph. Each iteration will add an average of about\n30 human-labeled samples into the training data, addressing the current model’s weakness.\n7They all won a prize in the National Mathematical Olympiad Contest.\n5\nAfter several rounds, it becomes difficult to conclude patterns. We change our diagnostic mode and\nrandomly sample math problems by tags. By manually checking the samples, we will add the correct\nor modified ones into the training data, and record the correct rate in the samples which pass the NLI\ntest. Each iteration will gain more samples passing the NLI test and an increase in the correct rate.\nWe stop our iteration after six rounds when the correct rate in sampled data almost reaches 95%, and\nwe add 341 problems into the training set during iterations.\n5\nResults\nThis section will introduce our evaluation metric, dataset statistics, and case studies together with our\nanalysis of the cases.\n5.1\nEvaluation setting\nUnlike auto theorem proving which depends totally on Lean 4 programming to check the accuracy,\nour evaluation for both the pipeline and the final translation datasets includes the three metrics: (1)\nCompile pass number (CPN): The number of all generated formal statements that can be correctly\ncomplied using Lean 4 under the environment of Mathlib. (2) NLI pass number (NPN): The number\nof generated formal statements that simultaneously can be compiled and the back translation is\nconsidered the same as the original questions by the model performing the NLI task. (3) Correct\ntranslation rate: The proportion of generated formal statements which is considered by human experts\nas a precise translation in those passing the NLI test. In real-world settings, it is too consuming to\nmanually review all the synthetic data, so the reported value is the rate on a sampled subset based on\nquestion types.\n5.2\nDataset Statistics and Evaluation Results\nThe original active learning dataset has 1088678 questions, among which 458692 questions are\nconsidered well-defined. The ill-defined questions come from an incomplete extraction from the\nwebsite, or a post containing attempts and parts of solutions for a specific problem. After filtering the\ntags, 327870 questions are selected to be formalized in our experiments.\nAfter six rounds of iteration, our model outputs 205079 questions that pass the compiling correctness\ntest, among which 57231 translations pass the NLI test. We randomly select five to ten samples for\neach common tag (tag with over 100 samples), and manually check whether they are truly correct.\nThe results are in Table 1. For the most common three tags, we sample 10 questions and all of\nthem achieve a sampled accuracy over 90%. The other tags each stand for a special kind of problem\nshowing up in mathematical contests and college examinations, among which almost all tags have at\nmost one wrong translation.\nWe use InternLM-Math-Plus to search proofs in the Lean Workbook by sampling multiple whole\nproofs and checking by our correctness checker. We sample 1024 proofs for each problem and we\nsolve 4898 of them (i.e. the Pass@1024 is 8.6%) which is significantly harder than MiniF2F. We will\nalso open-source these solutions to help improve automatic theorem proving.\nThough the overall accuracy has reached a high level, some kinds of mistakes still occasionally\nhappen, which is also indicated in the table as not all the tags have 100% accuracy. On the other\nhand, a number of patterns have been corrected during iterations. These patterns contain compiling\nerrors like conflict type and functions and continued inequalities, whose correction can significantly\nincrease the CPN. Our model demonstrates good learning ability in these samples. If the model does\nnot know how to translate a kind of problem, three manually written statements would help the model\nlearn how to translate it. However, when it comes to the errors of interpreting a contest problem, the\neffectiveness of iteratively adding human-labeled samples decreases. For example, when one integer\nis divided by another integer without specifying the type, the Lean language will return the floor of\nthe true quotient. So we add the type : R to them, but the model can only perform correctly about\nhalf the times. This may be attributed to the confusion from real number divisions but written in the\n6\nTable 1: Accuracy by tags in Lean Workbook. These are tags that show up more than 100 times in\nour final dataset. The first three most common tags are sampled 10 problems for each tag, while the\nothers are sampled 5 problems. It is worth noting that some tags are incorrect due to the mistake of\nthe tagging model, and we will choose another sample with this tag if we consider the current one\nunsuitable.\nTags\nNumber of samples\nSampled accuracy\ninequality\n46847\n10/10\nalgebra\n45218\n9/10\nnumber theory\n22474\n9/10\ntrigonometry\n4133\n4/5\nequation\n3255\n5/5\nproof\n3172\n5/5\ncalculus\n1061\n4/5\nsequence\n926\n4/5\ncombinatorics\n893\n4/5\nseries\n418\n5/5\nfunction\n351\n4/5\nmodular arithmetic\n339\n4/5\ninduction\n285\n5/5\nlogarithm\n269\n5/5\nlimit\n224\n3/5\nreal analysis\n170\n5/5\nWeighted Average\n-\n0.935\nsame form. Other instances include minimal/maximal problems where the model only states one-side\ninequality but omits the minimality (existence). Below we list the common patterns found in the\nmanual diagnostic process in table 2. We also find that some of the errors in the table can be partially\nfixed by post-processing.\n5.3\nEffectiveness and discussion\nFor an intuitive comparison of the effectiveness of our active learning pipeline, we derive the CPN\nand NPN for three models: The first-round model which is used for the initial filtering, the final-round\nmodel generating our dataset, and the final model further fine-tuned on our dataset Lean Workbook.\nThe results are shown in Table 3.\nThis table clearly shows the effectiveness both of our pipeline and our dataset. The human-labeled\ndata and filtered dataset achieve a gain of over 20000 more correct samples for both the compile\ntest and the NLI test, which promisingly indicates that this form of active learning can be further\niteratively utilized.\nThe model can further enhance its pass number by adding Lean Workbook data for translation, we\nwill also open-source this dataset (named Lean Workbook Plus). Although it shows a higher number\non NLI pass rate, the human evaluation finds that this dataset makes more mistakes on the number\ntheory problems, especially on the problem with prime numbers and maximal/minimal values.\n5.4\nFormalizing IMO problems\nThe accuracy table and case study table give us confidence in the performance of our model. As a\nhigh-level application, we try to translate new IMO problems using our model.\nWe aggregate the problems from Compfiles which have not been formalized. Each of the problems is\ntranslated 100 times under a temperature of 0.7 and we remove the wrong translations by compile test\nand NLI test. Finally, 23 problems with at least one correct translation passing the NLI are filtered out,\nand 21 problems are kept after manual evaluation, including 14 Algebra problems, 5 Number Theory\nproblems, and 2 Combinatorics problems. We also manually checked and made slight modifications\n7\nTable 2: Case study for false patterns. We list the common patterns concluded during the iterative\ndiagnostic process. This table gives one typical error for each pattern and also demonstrates one\nheuristic correction. Finally, the current performance column states how many portions the model\ncan translate correctly after the iteration in our manual check.\nPattern\nWrong example\nModified\nPerformance\nType confusion\na,b,c : R,\nsqrt (a ^ 2 + 8 * b * c)\nsqrt →Real.sqrt\nMostly Correct\nContinued\ninequalities\na >= b >= c > 0\na >= b ∧b >= c ∧c > 0\nMostly Correct\nMissing opera-\ntors\n2a+3b >= 0\n2*a+3*b >= 0\nMostly Correct\nInteger division\n(a*b*c)^(1/3)\n(a*b*c) ^((1:R)/3)\nHalf Correct\nTriangle condi-\ntion\na, b, c are side lengths of a triangle:\nnot translated\n(hx: a > 0∧b > 0∧c > 0)\n(hab : a + b > c)\n(hbc : b + c > a)\n(hca : a + c > b)\nMostly Correct\nAll solutions\n(x,y)=(1,5),(2,3)\n(x=1∧y=5) ∨(x=2∧y=3)\nMostly Correct\nSolution\nnum-\nber/sum\n(x,y)=(1,5),(2,3)\nA : Finset {x,y|...}\nA.card=2\nMostly Correct\nMin/Max\nThe maximal of a is 10: a <= 10\nIsGreatest {a | ...} 10\nHalf Correct\nExist\nInfinite\nnumber\nUnable to translate\n∀N: N,∃n > N, ....\nMostly Correct\nDigits\nn =abcde, a+b = ...\nFinset {n|\nsumOflist (Nat.digits 10 n)\nHalf Correct\nTable 3: We report the CPN (compile pass number) and NPN (NLI pass number) for each model\nduring iterations.\nTrain Dataset\nModel\nCPN\nNPN\nMiniF2F + ProofNet + MultiTask\nFirst-round Model\n136670\n37122\n+ Human-labeled\nFinal-round Model\n205079\n57231\n+ Lean Workbook\nFinal-round Model + Lean Workbook\n228928\n82893\nto the conclusion part if the correct answers to IMO problems are not extracted, and we ensure these\ntranslations are correct. These formal statements will be submitted to the Compfiles project. One\ncase below shows that our model has been able to skillfully use \"Finset\" functions to optimize formal\nstatements and avoid grammar mistakes. More cases are listed in Appendix B.\n/--\nIMO 1983 P5\nIs it possible to choose 1983 distinct positive integers, all less than or equal to\n10^5, no three of which are consecutive terms of an arithmetic progression?\n--/\ntheorem IMO1983_P5 :\n∃S : Finset N, S.card = 1983 ∧(∀x ∈S, x ≤10^5) ∧\n∀x ∈S, ∀y ∈S, ∀z ∈S, x < y ∧y < z →x + z ̸= 2 * y := by sorry\n6\nConclusion\nIn this paper, we introduce an automatic pipeline that can translate contest-level math problems into\nLean formal statements with high accuracy. Active learning proves its effectiveness in the data-sparse\n8\nscenario. We open-source Lean Workbook to help the machine learning community to improve the\nability of autoformalization and automatic theorem proving.\nLimitations\nWe find our proposed dataset has some similar problems which is hard to apply deduplication.\nFurthermore, our model is focused on contest-level problems during active learning which may not\nbe appropriate to formalize other level math problems.\nReferences\n[1] Thomas Anthony, Zheng Tian, and David Barber. Thinking fast and slow with deep learning\nand tree search. Advances in neural information processing systems, 30, 2017.\n[2] Zhangir Azerbayev, Bartosz Piotrowski, Hailey Schoelkopf, Edward W Ayers, Dragomir Radev,\nand Jeremy Avigad. Proofnet: Autoformalizing and formally proving undergraduate-level\nmathematics. arXiv preprint arXiv:2302.12433, 2023.\n[3] Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer,\nAlbert Q. Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An open language\nmodel for mathematics, 2023.\n[4] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin\nGe, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu,\nGao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren,\nChuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu,\nBenfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu,\nHongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang,\nChang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report. arXiv\npreprint arXiv:2309.16609, 2023.\n[5] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to\nsolve math word problems. arXiv preprint arXiv:2110.14168, 2021.\n[6] Google. Gemini: A family of highly capable multimodal models, 2023.\n[7] Jesse Michael Han, Jason Rute, Yuhuai Wu, Edward W Ayers, and Stanislas Polu. Proof artifact\nco-training for theorem proving with language models. arXiv preprint arXiv:2102.06203, 2021.\n[8] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn\nSong, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset.\narXiv preprint arXiv:2103.03874, 2021.\n[9] Yinya Huang, Xiaohan Lin, Zhengying Liu, Qingxing Cao, Huajian Xin, Haiming Wang,\nZhenguo Li, Linqi Song, and Xiaodan Liang. MUSTARD: Mastering uniform synthesis of\ntheorem and proof data. In The Twelfth International Conference on Learning Representations,\n2024.\n[10] InternLM. Internlm: A multilingual language model with progressively enhanced capabilities.\nhttps://github.com/InternLM/InternLM, 2023.\n[11] Albert Q Jiang, Wenda Li, and Mateja Jamnik. Multilingual mathematical autoformalization.\narXiv preprint arXiv:2311.03755, 2023.\n[12] Albert Q Jiang, Sean Welleck, Jin Peng Zhou, Wenda Li, Jiacheng Liu, Mateja Jamnik, Timothée\nLacroix, Yuhuai Wu, and Guillaume Lample. Draft, sketch, and prove: Guiding formal theorem\nprovers with informal proofs. arXiv preprint arXiv:2210.12283, 2022.\n9\n[13] Albert Qiaochu Jiang, Wenda Li, Szymon Tworkowski, Konrad Czechowski, Tomasz\nOdrzygó´zd´z, Piotr Miło´s, Yuhuai Wu, and Mateja Jamnik. Thor: Wielding hammers to integrate\nlanguage models and automated theorem provers. Advances in Neural Information Processing\nSystems, 35:8360–8373, 2022.\n[14] Guillaume Lample, Timothee Lacroix, Marie-Anne Lachaux, Aurelien Rodriguez, Amaury\nHayat, Thibaut Lavril, Gabriel Ebner, and Xavier Martinet. Hypertree proof search for neural\ntheorem proving. Advances in neural information processing systems, 35:26337–26349, 2022.\n[15] Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski,\nVinay V. Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu,\nBehnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems\nwith language models. In NeurIPS, 2022.\n[16] Chengwu Liu, Jianhao Shen, Huajian Xin, Zhengying Liu, Ye Yuan, Haiming Wang, Wei Ju,\nChuanyang Zheng, Yichun Yin, Lin Li, et al. Fimo: A challenge formal dataset for automated\ntheorem proving. arXiv preprint arXiv:2309.04295, 2023.\n[17] The mathlib Community. The lean mathematical library. In Proceedings of the 9th ACM\nSIGPLAN International Conference on Certified Programs and Proofs, POPL ’20. ACM,\nJanuary 2020.\n[18] Leonardo de Moura and Sebastian Ullrich. The lean 4 theorem prover and programming\nlanguage. In André Platzer and Geoff Sutcliffe, editors, Automated Deduction – CADE 28,\npages 625–635, Cham, 2021. Springer International Publishing.\n[19] Logan Murphy, Kaiyu Yang, Jialiang Sun, Zhaoyu Li, Anima Anandkumar, and Xujie Si.\nAutoformalizing Euclidean geometry. In International Conference on Machine Learning\n(ICML), 2024.\n[20] OpenAI. Gpt-4 technical report, 2023.\n[21] Lawrence C. Paulson. Isabelle: The next 700 theorem provers, 2000.\n[22] Stanislas Polu, Jesse Michael Han, Kunhao Zheng, Mantas Baksys, Igor Babuschkin, and Ilya\nSutskever. Formal mathematics statement curriculum learning, 2022.\n[23] Stanislas Polu, Jesse Michael Han, Kunhao Zheng, Mantas Baksys, Igor Babuschkin, and Ilya\nSutskever. Formal mathematics statement curriculum learning. arXiv preprint arXiv:2202.01344,\n2022.\n[24] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y. K. Li,\nY. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open\nlanguage models, 2024.\n[25] Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis\nSaravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language\nmodel for science. 2023.\n[26] The Coq Development Team. The Coq reference manual – release 8.18.0. https://coq.\ninria.fr/doc/V8.18.0/refman, 2023.\n[27] Haiming Wang, Huajian Xin, Chuanyang Zheng, Lin Li, Zhengying Liu, Qingxing Cao, Yinya\nHuang, Jing Xiong, Han Shi, Enze Xie, Jian Yin, Zhenguo Li, Heng Liao, and Xiaodan Liang.\nLego-prover: Neural theorem proving with growing libraries, 2023.\n[28] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le,\nDenny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models.\nAdvances in Neural Information Processing Systems, 35:24824–24837, 2022.\n10\n[29] Sean Welleck and Rahul Saha. Llmstep: Llm proofstep suggestions in lean. arXiv preprint\narXiv:2310.18457, 2023.\n[30] Yuhuai Wu, Albert Q. Jiang, Wenda Li, Markus N. Rabe, Charles Staats, Mateja Jamnik, and\nChristian Szegedy. Autoformalization with large language models, 2022.\n[31] Huajian Xin, Daya Guo, Zhihong Shao, Zhizhou Ren, Qihao Zhu, Bo Liu, Chong Ruan,\nWenda Li, and Xiaodan Liang. Deepseek-prover: Advancing theorem proving in llms through\nlarge-scale synthetic data. arXiv preprint arXiv:2405.14333, 2024.\n[32] Kaiyu Yang, Aidan Swope, Alex Gu, Rahul Chalamala, Peiyang Song, Shixing Yu, Saad Godil,\nRyan Prenger, and Anima Anandkumar. LeanDojo: Theorem proving with retrieval-augmented\nlanguage models. In Neural Information Processing Systems (NeurIPS), 2023.\n[33] Huaiyuan Ying, Shuo Zhang, Linyang Li, Zhejian Zhou, Yunfan Shao, Zhaoye Fei, Yichuan\nMa, Jiawei Hong, Kuikun Liu, Ziyi Wang, Yudong Wang, Zijian Wu, Shuaibin Li, Fengzhe\nZhou, Hongwei Liu, Songyang Zhang, Wenwei Zhang, Hang Yan, Xipeng Qiu, Jiayu Wang,\nKai Chen, and Dahua Lin. Internlm-math: Open math large language models toward verifiable\nreasoning, 2024.\n[34] Kunhao Zheng, Jesse Michael Han, and Stanislas Polu. Minif2f: a cross-system benchmark for\nformal olympiad-level mathematics. arXiv preprint arXiv:2109.00110, 2021.\n11\nA\nCase study\nWe would give some translated examples of Lean Workbook with respected to most common tags.\nInequality\nNatural Language problem: For a, b, c, d > 0, abcd = 1 prove that\n1\n1+(1+a)2 +\n1\n1+(1+b)2 +\n1\n1+(1+c)2 +\n1\n1+(1+d)2 ≤4\n5.\ntheorem lem1 (a b c d : R) (hab : 0 < a) (hbc : 0 < b) (hcd : 0 < c) (hda :\n0 < d) (habc : a * b * c * d = 1) :\n(1 / (1 + (1 + a) ^ 2) + 1 / (1 + (1 + b) ^ 2) + 1 / (1 + (1 + c) ^ 2) + 1 /\n(1 + (1 + d) ^ 2)) ≤(4:R) / 5 := by sorry\nAlgebra\nNatural Language problem: Prove that for m ≥5, the sum of the factorials of the first\nm natural numbers is not equal to the product of the factorials of the first m odd natural\nnumbers.\ntheorem sum_factorial_not_prod_factorial (m : N) (hm : 5 ≤m) : (Σ k in\nFinset.range m, k!) ̸= (Q k in Finset.Icc 1 m, (2 * k - 1)!) := by sorry\nNumber Theory & Combination\nNatural Language problem: For every p prime number show that p2 |\n\u00002p\np\n\u0001\n−2.\ntheorem p2_dvd_2pCp_2 (p : N) (hp : Nat.Prime p) : p^2 | (Nat.choose (2 * p)\np) - 2 := by sorry\nEquation\nNatural Language problem:\nSolve in R\n: sin(a) + 2sin(a + b + c)\n=\n0,\nsin(b) + 3sin(a + b + c) = 0, sin(c) + 4sin(a + b + c) = 0.\ntheorem sin_problem (a b c : R) : sin a + 2 * sin (a + b + c) = 0 ∧sin b +\n3 * sin (a + b + c) = 0 ∧sin c + 4 * sin (a + b + c) = 0 ↔∃(k n m :\nZ), a = π * k ∧b = π * n ∧c = π * m := by sorry\nTrigonometry\nNatural Language problem: If a, b, c be sidelengths of a triangle, then\na\nb+c +\nb\nc+a +\nc\na+b ≤\n1\n2 +\n3(a3+b3+c3)\n(a+b+c)(a2+b2+c2).\ntheorem imo_1964_p1_1 (a b c : R) (hx: a > 0 ∧b > 0 ∧c > 0) (hab : a + b\n> c) (hbc : b + c > a) (hca : a + c > b) : (a / (b + c) + b / (c + a) +\nc / (a + b) : R) ≤(1:R) / 2 + (3 * (a ^ 3 + b ^ 3 + c ^ 3)) / ((a + b +\nc) * (a ^ 2 + b ^ 2 + c ^ 2)) := by sorry\n12\nProof & Complex Number\nNatural Language problem:\nLet a , b\n∈\nR and A\n=\n{z ∈C |Re z = a} ,\nB = {z ∈C | Im z = b} .\nProve that if M ⊂C has the properties: 1) A ⊂M\nand B ⊂M ; 2) whatever z1, z2 ∈M ⇒z1 + z2 ∈M, then M = C.\ntheorem subset_of_1d_subsets {M : Set C} (a b : R)\n(hA : {z : C | z.re =\na} ⊆M)\n(hB : {z : C | z.im = b} ⊆M)\n(hM : ∀z1 z2 : C, z1 ∈M ∧\nz2 ∈M →z1 + z2 ∈M) : M = C := by sorry\nCalculus & Limit\nNatural Language problem: Prove that limn→∞1\nn\n\u0010Pn\ni=1\ni\n√\ni2+1\n\u0011\n= 1\ntheorem Injective_total : ∀ε : R, ε > 0 →∃N : N, ∀n : N, n ≥N →|(1\n/ n) * (Σ i in Finset.Icc 1 n, i / (Real.sqrt (i ^ 2 + 1))) - 1| < ε :=\nby sorry\nfunction\nNatural Language problem: Find all functions f : R →R satisfying: for all x, y ∈R :\nf(xf(x) + f(y)) = (f(x))2 + y\ntheorem FE (f : R →R):(∀x y, f (x * f x + f y) = (f x)^2 + y) ↔∀x, f\nx = x ∨∀x, f x = -x := by sorry\nsequence & induction\nNatural Language problem: Suppose that an is a sequence such that an+1 = a2\nn + nan −2\nwith a1 = 3 , Show that\n1\na1−2 +\n1\na2−2 + · · · +\n1\nan−2 < 2\ntheorem aops_1212 (n : N) (a : N →N) (ha : a 1 = 3) (hab : ∀n, a (n + 1)\n= (a n)^2 + n * a n - 2) : Σ k in Finset.Icc 1 n, (1 / (a k - 2)) < 2 :=\nby sorry\nModular Arithmetic\nNatural Language problem: Show that the cube of any integer is congruent to 0, 1, or -1\nmodulo 9.\ntheorem t_cubic_mod9 : ∀t : Z, t^3 ≡0 [ZMOD 9] ∨t^3 ≡1 [ZMOD 9] ∨t^3\n≡-1 [ZMOD 9] := by sorry\n13\nReal Analysis\nNatural Language problem: Let D be a compact subset of R and support that f : D →R is\ncontinuous. Prove f(D) is compact.\ntheorem continuous_compact_support (D : Set R) (f : R →R)\n(hD :\nIsCompact D) (hf : ContinuousOn f D) : IsCompact (Set.image f D) := by\nsorry\nB\nIMO example\nOur model also provides formalization for IMO-level problems. The translated questions focus on\nthree types: Algebra, Number Theory, and Combinatorics.\nAlgebra IMO 1975 P2\nNatural Language problem: Let a1 < a2 < a3 < · · · be positive integers. Prove that for\nevery i >= 1, there are infinitely many an that can be written in the form an = rai + saj,\nwith r, s positive integers and j > i.\ntheorem imo1975_p2 (a : N →Z) (apos : ∀i, 0 < a i) (ha : ∀i, a i < a (i\n+ 1)) (i : N) : ( ∀i n0:N , ∃n, n0 ≤n ∧∃r s : N, ∃j : N, a n =\nr * a i + s * a j ∧i < j ∧0 < r ∧0 < s ):= by sorry\nAlgebra IMO 1977 P4\nNatural Language problem: Define f(x) = 1 −a cos x −b sin x −A cos 2x −B sin 2x,\nwhere a, b, A, B are real constants. Suppose that f(x) ≥0 for all real x. Prove that\na2 + b2 ≤2 and A2 + B2 ≤1.\ntheorem imo1977_p4 (f : R →R) (a b A B : R)\n(h0 : ∀x, f x = 1 - a *\nReal.cos x - b * Real.sin x - A * Real.cos (2 * x) - B * Real.sin (2 *\nx))\n(h1 : ∀x, f x ≥0) : a ^ 2 + b ^ 2 ≤2 ∧A ^ 2 + B ^ 2 ≤1 := by\nsorry\nNumber Theory IMO 1978 P1\nNatural Language problem: m and n are positive integers with m < n. The last three decimal\ndigits of 1978m are the same as the last three decimal digits of 1978n. Find m and n such\nthat m + n has the least possible value.\ntheorem imo1978_p1 (m n : N) (hmn: m < n) (hmn2: m = 3 ∧n=103) : (1978^m)\n% 1000 = (1978^n) % 1000) ∧(∀m’ n’ : N, m’ < n’ ∧(1978^m’) % 1000 =\n(1978^n’) % 1000 →m + n ≤m’ + n’) := by sorry\n14\nNumber Theory IMO 1982 P4\nNatural Language problem: Prove that if n is a positive integer such that the equation\nx3 −3xy2 + y3 = n has a solution in integers x, y, then it has at least three such solutions.\nShow that the equation has no solutions in integers for n = 2891.\ntheorem imo1982_p4 (n : N) (hn : 0 < n) (hxy : ∃x y : Z, x^3 - 3 * x * y^2 +\ny^3 = n) : (n ̸= 2891) ∧∃x1 x2 x3 y1 y2 y3 : Z, (x1^3 - 3 * x1 * y1^2\n+ y1^3 = n ∧x2^3 - 3 * x2 * y2^2 + y2^3 = n ∧x3^3 - 3 * x3 * y3^2 +\ny3^3 = n ∧(x1 ̸= x2 ∨y1 ̸= y2) ∧(x1 ̸= x3 ∨y1 ̸= y3)\n∧(x2 ̸= x3 ∨\ny2 ̸= y3)) := by sorry\nCombinatorics IMO 1978 P6\nNatural Language problem: An international society has its members from six different\ncountries. The list of members has 1978 names, numbered 1, 2, . . . , 1978. Prove that there is\nat least one member whose number is the sum of the numbers of two (not necessarily distinct)\nmembers from his own country.\ntheorem imo1978_p6 (n : N) (hn : n = 1978) (C : Fin n →Fin 6) : ∃i : Fin\nn,\n∃j : Fin n,\n∃k : Fin n,\nC i = C j ∧C j = C k ∧i ̸= k ∧(i:N\n) + (k:N ) = (j:N ) + 1 := by sorry\nC\nDataset card\n1. Our dataset contains 57231 problems in the split of Lean Workbook and 82893 problems\nin the split of Lean Workbook Plus. We provide the natural language statement, answer,\nformal statement, and formal proof (if available) for each problem. These data can support\nautoformalization model training and searching for proofs.\n2. We open-source our code at https://github.com/InternLM/InternLM-Math and our\ndata at https://huggingface.co/datasets/InternLM/Lean-Workbook.\n3. Croissant\nmetadata\nURL:\nhttps://huggingface.co/api/datasets/internlm/\nLean-Workbook/croissant.\n4. The license of our dataset is Apache 2.0.\n5. We will host our dataset in Huggingface and our code in GitHub. We will maintain this\ndataset with further improvement.\n6. DOI of dataset: 10.57967/hf/2399\n15\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2024-06-06",
  "updated": "2024-06-07"
}