{
  "id": "http://arxiv.org/abs/2406.04215v1",
  "title": "mCSQA: Multilingual Commonsense Reasoning Dataset with Unified Creation Strategy by Language Models and Humans",
  "authors": [
    "Yusuke Sakai",
    "Hidetaka Kamigaito",
    "Taro Watanabe"
  ],
  "abstract": "It is very challenging to curate a dataset for language-specific knowledge\nand common sense in order to evaluate natural language understanding\ncapabilities of language models. Due to the limitation in the availability of\nannotators, most current multilingual datasets are created through translation,\nwhich cannot evaluate such language-specific aspects. Therefore, we propose\nMultilingual CommonsenseQA (mCSQA) based on the construction process of CSQA\nbut leveraging language models for a more efficient construction, e.g., by\nasking LM to generate questions/answers, refine answers and verify QAs followed\nby reduced human efforts for verification. Constructed dataset is a benchmark\nfor cross-lingual language-transfer capabilities of multilingual LMs, and\nexperimental results showed high language-transfer capabilities for questions\nthat LMs could easily solve, but lower transfer capabilities for questions\nrequiring deep knowledge or commonsense. This highlights the necessity of\nlanguage-specific datasets for evaluation and training. Finally, our method\ndemonstrated that multilingual LMs could create QA including language-specific\nknowledge, significantly reducing the dataset creation cost compared to manual\ncreation. The datasets are available at\nhttps://huggingface.co/datasets/yusuke1997/mCSQA.",
  "text": "mCSQA: Multilingual Commonsense Reasoning Dataset\nwith Unified Creation Strategy by Language Models and Humans\nYusuke Sakai, Hidetaka Kamigaito, Taro Watanabe\nNara Institute of Science and Technology\n{sakai.yusuke.sr9, kamigaito.h, taro}@is.naist.jp\nAbstract\nIt is very challenging to curate a dataset\nfor language-specific knowledge and common\nsense in order to evaluate natural language un-\nderstanding capabilities of language models.\nDue to the limitation in the availability of an-\nnotators, most current multilingual datasets are\ncreated through translation, which cannot eval-\nuate such language-specific aspects. Therefore,\nwe propose Multilingual CommonsenseQA\n(mCSQA) based on the construction process\nof CSQA but leveraging language models for a\nmore efficient construction, e.g., by asking LM\nto generate questions/answers, refine answers\nand verify QAs followed by reduced human\nefforts for verification. Constructed dataset\nis a benchmark for cross-lingual language-\ntransfer capabilities of multilingual LMs, and\nexperimental results showed high language-\ntransfer capabilities for questions that LMs\ncould easily solve, but lower transfer capabil-\nities for questions requiring deep knowledge\nor commonsense. This highlights the neces-\nsity of language-specific datasets for evalua-\ntion and training. Finally, our method demon-\nstrated that multilingual LMs could create QA\nincluding language-specific knowledge, sig-\nnificantly reducing the dataset creation cost\ncompared to manual creation. The datasets\nare available at https://huggingface.co/\ndatasets/yusuke1997/mCSQA.\n1\nIntroduction\nCan you choose the correct answer in Table 1?\nEach choice is semantically very close, making it\ndifficult for non-native speakers to distinguish them.\nHowever, native speakers who have language-\nspecific commonsense and knowledge can choose\nthe most plausible choice considering subtle nu-\nances. Despite the need to consider different back-\ngrounds for each language, the datasets to evaluate\nthe natural language understanding (NLU) capa-\nbilities of language models (LMs) are mostly for\n9HULI\\\u0003\n4XDOLWLHV\n*HQHUDWH\n4XHVWLRQV\n&64$\u0003\n-&64$\nP&64$\n([WUDFW\n6XE\u0010*UDSKV\n\u001d\u0003+XPDQ\n\u001d\u0003/0\nFigure 1: The comparison of the dataset creation process\nfor mCSQA and (J) CSQA includes two key changes for\nefficient and low-cost creation of multilingual datasets.\nFirst, the question generation process shifts from human\nannotators to an LM. Second, an LM assists humans for\nthe quality verification process.\nJapanese Question (Translated to English)\nQ: お年寄りは？\n(Who is the elderly person?)\n(a) わし\n(me)\n(b) わたし\n(me)\n(c) ぼく\n(me)\n(d) おれ\n(me)\n(e) うち\n(me)\nEnglish Question (Translated to Japanese)\nQ: How do we make a cake?\n(ケーキを作るにはどうする？)\n(a) roast\n(焼く)\n(burn)\n(b) broil\n(焼く)\n(burn)\n(c) grill\n(焼く)\n(burn)\n(d) toast\n(焼く)\n(burn)\n(e) bake\n(焼く)\n(burn)\nTable 1: Examples require language-specific knowledge.\nThey cannot be solved without such knowledge, as the\ntranslations consolidate the nuances into a single word.\na few major languages such as English, and thus,\nmany languages lack such datasets. When focusing\non the cross-lingual capability of LMs, datasets\ncreated from scratch in multiple languages are lim-\n1\narXiv:2406.04215v1  [cs.CL]  6 Jun 2024\nited, and currently, evaluations mostly use datasets\ncreated through translation. However, as can be\nseen from the example in Table 1, datasets cre-\nated through translation cannot accurately evalu-\nate language-specific commonsense or knowledge.\nTherefore, it is necessary to create datasets for each\nlanguage from scratch, but the manual creation of\nsuch datasets is limited by the availability of anno-\ntators and financial costs.\nTo tackle this problem, as shown in Figure 1,\nwe propose a method to efficiently create multi-\nlingual NLU datasets from multilingual resources\nby replacing some of the manual annotation pro-\ncesses with generative multilingual LMs. In this\nstudy, we focus on CommonsenseQA (CSQA) (Tal-\nmor et al., 2019), a dataset for evaluating com-\nmonsense reasoning capabilities within NLU eval-\nuations. CSQA is a major commonsense reason-\ning Question-Answering dataset manually created\nfrom the multilingual knowledge base Concept-\nNet (Speer et al., 2017). However, due to such\nlimitations, CSQA has been created from scratch\nonly in English and Japanese, JCommonsenseQA\n(JCSQA) (Kurihara et al., 2022). Therefore, we\ncreate a Multilingual CommonsenseQA (mCSQA)\nthat extends CSQA to eight languages1 using our\nproposed method.\nFurthermore, we evaluated the cross-lingual\nlanguage-transfer capabilities of multilingual LMs\nfocusing on language-specific common sense and\nknowledge using mCSQA. The results showed high\nlanguage-transfer capabilities for questions that\nLMs could easily solve, but lower transfer capa-\nbilities for questions requiring deep knowledge\nor commonsense. The total cost per question in\nmCSQA was reduced to one-hundredth of that for\nCSQA.\nTo summarize, our contributions are as follows:\n• We propose an efficient and low-cost method\nfor creating NLU datasets by generative mul-\ntilingual LMs.\n• We demonstrate the potential effectiveness of\nusing multilingual LMs for creating datasets\nfrom multilingual resources.\n• mCSQA makes it possible to analyze the\ncross-linguistic commonsense understanding\ncapabilities and transfer performance from\neach language beyond English.\n1English (en), Japanese (ja), Chinese (zh), German (de),\nPortuguese (pt), Dutch (nl), French (fr), Russian (ru)\n• The analysis revealed that, when focusing on\nlanguage transfer capabilities using mCSQA,\nwe identified cases where language-specific\nknowledge is required and cases where it is\nnot, thereby confirming the need for non-\ntranslated language-specific datasets.\n2\nBackground and Related Work\nCommonsense reasoning task\nThis task evalu-\nates how an LM can understand and infer object\nrecognition, visual information, and cultural or so-\ncietal common sense, which are not typically de-\nscribed in textual information. CSQA is a multiple-\nchoice question task that asks for the most plausi-\nble choice as an answer with some variants: JC-\nSQA is in Japanese, CommonsenseQA 2.0 (Tal-\nmor et al., 2021) is a more challenging dataset,\nECQA (Aggarwal et al., 2021) requires explaining\nthe process of deriving an answer, etc. There exist\nother types of commonsense tasks: COPA (Roem-\nmele et al., 2011) and BalancedCOPA (Kavumba\net al., 2019) ask about causal relationships between\neveryday events; SocialIQA (Sap et al., 2019b)\nasks about social common sense; PIQA (Bisk\net al., 2020) evaluates procedural knowledge; Hot-\npotQA (Yang et al., 2018) requires multi-hop infer-\nence; DROP (Dua et al., 2019) captures arithmetic\noperation capabilities; and tasks like understand-\ning language information (Liu et al., 2022b; Ko-\ncijan et al., 2023; Sakaguchi et al., 2021; Wang\net al., 2019), understanding causal relationships\nwithin documents (Mostafazadeh et al., 2020, 2016;\nZhang et al., 2018; Huang et al., 2019; Oster-\nmann et al., 2018; Smirnov, 2019), and Common-\nGen (Lin et al., 2020), which asks to generate com-\nmon sentences from given keywords. The above\ndatasets primarily focus on English, but there exist\ndatasets in Japanese (Omura et al., 2020; Takahashi\net al., 2019; Hayashibe, 2020), Chinese (Xu et al.,\n2021, 2020; Wang et al., 2022), Russian (Shav-\nrina et al., 2020; Taktasheva et al., 2022), and\nIndonesian (Koto et al., 2022). For multilingual\ndatasets, most are extended versions of existing\nones through translation, such as X-COPA (Ponti\net al., 2020) from COPA, X-CSQA (Lin et al.,\n2021) from CSQA, and X-CODAH (Lin et al.,\n2021) from CODAH (Chen et al., 2019). A few\ndatasets, such as TyDiQA (Clark et al., 2020), are\ncreated for each language from scratch.\nMultilingual datasets\nWhen focusing on the\nevaluation of multilingual performance of LMs,\n2\nMethods\nKnowledge Alignment Costs\nBy translation\n✗\n✓\n✓\nCompilation of similar tasks\n✓\n✗\n✓\nFrom multilingual resources\n✓\n✓\n✗\nOurs\n✓\n✓\n✓\nTable 2: Categorize the multilingual datasets creation\nmethods.\nthe evaluation datasets are almost exclusively cre-\nated through three methods, as shown in Table 2:\n(1) Translation from existing datasets in a major\nlanguage, e.g., English (Lin et al., 2021; Ponti\net al., 2020; Conneau et al., 2018; Artetxe et al.,\n2020; Yang et al., 2019); (2) Compilation of sim-\nilar tasks across multiple languages (Zhang et al.,\n2023c; Hu et al., 2023; Adelani et al., 2022; Roy\net al., 2020; Malmasi and Dras, 2015); (3) Cre-\nation from multilingual resources following the\nsame dataset creation process (Keung et al., 2020;\nHuang et al., 2020; Buchholz and Marsi, 2006;\nClark et al., 2020; Schwenk and Li, 2018; Kabra\net al., 2023). However, (1) translated datasets of-\nten do not account for language-specific culture,\nknowledge, common sense, or linguistic phenom-\nena, leading to a bias towards the background of the\nsource language (Hu et al., 2021; Lin et al., 2021;\nAcharya et al., 2020; Clark et al., 2020; Park et al.,\n2021; Kurihara et al., 2022). (2) Simply compiling\ndatasets curated for each individual language could\nallow the evaluation of language-specific knowl-\nedge and common sense. However, it is difficult\nto align tasks across languages since most tasks\ndiffer in their creation methods data sources or\nphilosophies. Thus, it just leads to evaluating the\ntransfer capability among comparable tasks, and\nnot evaluating the true transfer capabilities across\nlanguages. Therefore, (3) only the datasets created\nfrom multilingual resources can enable the evalu-\nation of language transfer capability, considering\nthe differences in language-specific knowledge and\ncommon sense. Nevertheless, the manual creation\nof such datasets is limited by the availability of\nannotators and financial costs.\nDataset creation with LMs\nThe superior per-\nformance of generative language models allows\nto create datasets automatically. SWAG (Zellers\net al., 2018) and HellaSwag (Zellers et al., 2019)\nhave created answer choice options through the out-\nput of LMs. Such efforts have also been extended\nto use LMs for data augmentation (Stali¯unait˙e\net al., 2021; Kumar et al., 2019, 2020; Lee et al.,\n2021). WANLI (Liu et al., 2022a), created from\nMNLI (Williams et al., 2018), employs GPT-\n3 (Brown et al., 2020) for adversarial data aug-\nmentation with manual checks to create chal-\nlenging datasets. Some studies propose methods\nto manually check quality of LM generation re-\nsults (Tekiro˘glu et al., 2020; Yuan et al., 2021;\nWiegreffe et al., 2022; Wang et al., 2021a; Li et al.,\n2023).\nAdditionally, there are attempts to cre-\nate datasets from scratch with emergent abilities\nof LMs, without using any examples (He et al.,\n2022; Wang et al., 2021b; Schick and Schütze,\n2021; Meng et al., 2022; Ye et al., 2022). How-\never, these studies have primarily focused on a sin-\ngle language, e.g., English. Recently, the outputs\nof language models themselves have been used\nto create datasets (Honovich et al., 2023; Shao\net al., 2023; Sun et al., 2023; Peng et al., 2023)\nfor instruction-tuning (Wei et al., 2022a).\nTar-\nGEN (Gupta et al., 2023) employs a single lan-\nguage model and splits the data generation process\ninto multiple steps, inputting the suitable prompt\nfor each step to ensure data diversity and reliability.\nPutri et al. (2024) focus on middle-resource (In-\ndonesian) and low-resource (Sundanese) languages,\nand investigate whether LLMs can create culturally\naware commonsense questions by comparing trans-\nlation datasets and those generated by LLMs from\nscratch.\n3\nDatasets Creation\nOur mCSQA construction involves three main\nsteps (see Figure 2): extraction of sub-graphs from\nConceptNet, creation of question and choice pairs\nwith LMs, and verification of their quality by both\nLMs and humans. We basically follow the creation\nprocesses of CSQA and JCSQA, but modified to\nallow for unified processing to support multiple\nlanguages.\n3.1\nExtract Sub-Graphs from ConceptNet\nConceptNet is a graph knowledge base defined as\na tuple, G = (C, R, T ), where C denotes a set of\nconcept entities, R denotes a set of relations and T\ndenotes a set of triples. Each triple is represented\nas (s, r, t) ∈T , where s and t ∈C are the source\nand target concept entities, respectively, and r ∈R\nis the relation, and carry commonsense knowledge\nsuch as “(student, CapableOf, forget to do home-\nwork)”.\n3\n\u000bD\f\u0003([WUDFWLRQ\u0003RI\u00036XE\u0010*UDSKV\n\u000bF\f\u00039HULI\\LQJ\u0003TXDOLWLHV\u0003E\\\u0003/0V\u0003DQG\u0003KXPDQV\n([WUDFW\u00036XE\u0010*UDSKV\u0003\nIRU\u0003(DFK\u0003/DQJXDJHV\n:KDW\u0003DQLPDO\u0003LV\u0003NQRZQ\u0003IRU\u0003LWV\u0003\nSOD\\IXO\u0003DQG\u0003DJLOH\u0003EHKDYLRXU\"\n*HQHUDWH\u00034XHVWLRQ\n:KLFK\u0003DQLPDO\u0003LV\u0003NQRZQ\u0003IRU\u0003\nLWV\u0003SOD\\IXO\u0003EHKDYLRU\u0003DQG\u0003\nDJLOH\u0003PRYHPHQWV\u0003\"\n5HILQH\u00034XHVWLRQ\n䞉PRQNH\\\n䞉OHPXU\n䞉MHOO\\ILVK\n$GG\u0003'LVWUDFWRUV\n&DQ\u0003/0V\u0003VROYH\u0003LW\"\n(DV\\\u00036HWV\n+DUG\u00036HWV\n<HV\n<HV\n&DQ\u0003KXPDQV\u0003VROYH\u0003LW\"\n1R\n'DWDVHWV\u0003\n\u000bP&64$\f\n>\u0003PRQNH\\\u000f\u0003OHPXU\u000f\u0003MHOO\\ILVK\u0003@\n4XHVWLRQ\u00036HWV\u0003\u000b46\f\n6RXUFH\u0003&RQFHSW\n7DUJHW\u0003&RQFHSW\n6DPH\u00035HODWLRQ\n)LOWHULQJ\nFDUWRRQ\n$W/RFDWLRQ\nPRQNH\\ OHPXU MHOO\\ILVK\n«\n\u000bE\f\u0003&UHDWLRQ\u0003RI\u00034XHVWLRQ\u00033DLUV\u0003ZLWK\u0003/0V\nၥ\u001d\n㉥䜔ⓑ䛺䛹䛾✀㢮䛜䛒䜚䚸ᩱ⌮䛸䛾┦ᛶ䜒\nⰋ䛔㣧䜏≀䛿ఱ䛷䛧䜗䛖䛛䠛\n㑅ᢥ⫥\u001d\n\u000bD\f\u0003䜴䜷䝑䜹\u0003\u000bE\f\u0003䝆䞁\u0003\u000bF\f\u0003䝽䜲䞁\u0003\n\u000bG\f\u0003䝡䞊䝹\u0003\u000bH\f\u0003᪥ᮏ㓇\nFDUWRRQ\n$W/RFDWLRQ\nPRQNH\\ OHPXU\nMHOO\\ILVK\n«\nၥ\u001d\n㉥䜔ⓑ䛺䛹䛾✀㢮䛜䛒䜚䚸ᩱ⌮䛸䛾┦ᛶ䜒\nⰋ䛔㣧䜏≀䛿ఱ䛷䛧䜗䛖䛛䠛\n㑅ᢥ⫥\u001d\n\u000bD\f\u0003䜴䜷䝑䜹\u0003\u000bE\f\u0003䝆䞁\u0003\u000bF\f\u0003䝽䜲䞁\u0003\n\u000bG\f\u0003䝡䞊䝹\u0003\u000bH\f\u0003᪥ᮏ㓇\n4\u001d\n:KLFK\u0003DQLPDO\u0003LV\u0003NQRZQ\u0003IRU\u0003LWV\u0003SOD\\IXO\u0003\nEHKDYLRU\u0003DQG\u0003DJLOH\u0003PRYHPHQWV\"\n&KRLFHV\u001d\n\u000bD\f\u0003RUDQJXWDQ\u0003\u000bE\f\u0003MHOO\\ILVK\u0003\u000bF\f\u0003OHPXU\u0003\n\u000bG\f\u0003JRULOOD\u0003\u000bH\f\u0003PRQNH\\\n䞉PRQNH\\\n䞉OHPXU\n䞉MHOO\\ILVK\n䞉RUDQJXWDQ\n䞉JRULOOD\nFigure 2: Creation process of mCSQA\nWe extract subgraphs from ConceptNet, as per\nFigure 2-(a), that have three distinct concept enti-\nties derived from queries of concept entities and\nrelations for each language. CSQA uses only for-\nward queries (s, r, ?), but, similar to JCSQA, we\nalso utilize backward queries (?, r, t). We name\nthis subgraph as Question Sets (QSs). After extrac-\ntion, we filter the QSs like CSQA and JCSQA, and\napplies unified filtering in mCSQA as follows:\n1. Similar to CSQA, we retain only QSs that\ncontain any types of the 22 relations2.\n2. We filter out QSs where any of the concept\nentities consist of more than four words or\nonly a single character3.\n3. We remove QSs where any pair of concept en-\ntities is connected by a ‘Synonym’ relation in\nConceptNet, or where entities are substrings\nof each other.\nAfter filtering with the above settings, we randomly\nselected 6,000 QSs for each language4.\n3.2\nCreate Questions with LMs\nWe employ the generative multilingual language\nmodel GPT-3.55 (Ouyang et al., 2022) to generate\nquestions automatically to eliminate the human\nlabor as done in CSQA and JCSQA.\n2Antonym, AtLocation, CapableOf, Causes, CausesDe-\nsire, DefinedAs, DerivedFrom, Desires, DistinctFrom, Ety-\nmologicallyDerivedFrom, HasA, HasFirstSubevent, HasLast-\nSubevent, HasPrerequisite, HasProperty, InstanceOf, MadeOf,\nMotivatedByGoal, NotDesires, PartOf, SymbolOf, UsedFor\n3Unsegmented languages, like Japanese, are segmented by\nmorphology in ConceptNet, so we can apply similar filtering.\n4For French and Russian, the number of QSs did not reach\n6,000, so we used all available QSs, totaling 4,125 and 3,901,\nrespectively.\n5We used gpt-3.5-turbo-1106.\nstep\ntemperature top_p seed\nCreating question sentences\n0.0\n0.0\n0\nRefining question sentences\n0.7\n0.5\n0\nAdding additional distractors\n1.2\n0.7\n0\nTable 3: The hyper-parameters for each step\nOur construction process comprises three steps\nof ‘question generation’, ‘question refinement’ and\n‘distractor augmentation’ as shown in Figure 2-(b).\nOur step differs from CSQA in the refinement step\nsince we need to improve the question generation\nfrom LM.\nWe designed prompts and tuned optimized hyper-\nparameters for each step for LMs. The details of\nthe prompts are described in Appendix D, and the\nhyper-parameters are shown in Table 3.\nCreating question sentences\nFor each QS, we\ngenerated question sentences by LMs where, for\neach of the three target concept entities, only one\nserves as the answer. The prompt for LMs was in-\nspired by the JCSQA filtering process for question\ncreation in which systematic filtering uses textual\ninformation. The key instructions are as follows:\n• Avoid including words of the target entities in\nthe question sentence.\n• Avoid using superficial information such as\ncharacter count.\n• End the sentence with a question mark (?).\n• Be an objective question sentence.\n• Consists of only one sentence.\nAfter generating questions with LMs, we re-\nmoved any question sentences that do not follow\n4\nen\nja\nzh\nde\npt\nnl\nfr\nru\nTotal\n14,722 15,695 17,254 16,542 16,679 15,992 10,770 10,215\nRefined\n3,654 12,007 6,534\n765\n585 7,927 3,109 6,734\npct. (%) 24.82 76.50 37.87\n4.63\n3.51 49.57 28.87 65.92\nTable 4: The percentage of sentences refined\nthese instructions or contain inappropriate expres-\nsions through pattern matching6.\nRefining question sentences\nLMs do not always\ngenerate appropriate outputs resulting in unnatu-\nral expressions or degeneration (Liu et al., 2022c;\nHonovich et al., 2023; Raunak et al., 2023; Lin\net al., 2020; Madaan et al., 2023). Hence, inspired\nby the idea of output refinement (Liu et al., 2022c;\nRaunak et al., 2023; Madaan et al., 2023), we refine\nunnatural generated question sentences into natural\nones using the LM again and remove inappropriate\nquestions as done in the previous step. Table 4\nshows the percentage of sentence refinement.\nAdding additional distractors\nWe added addi-\ntional incorrect choices to make the task more diffi-\ncult as done in CSQA and JCSQA, but we leverage\nLM, not crowd workers, to formulate distractors\nthat seemed plausible or related to the questions.\nHere, we asked LM to generate two plausible dis-\ntractors given the three choices of a question with-\nout question itself in order to separate the ques-\ntion generation and answering capabilities of LMs.\nThere is a risk of generating duplicated choices or\nadding correct choices since question sentence it-\nself is not fed in this process. Hence, we remove\nsuch questions through manual verification in Sec-\ntion 3.3.\n3.3\nQuestion Quality Verification by LMs and\nHumans\nIn CSQA and JCSQA, every question is manually\nverified to remove low-quality questions, such as\nthose with multiple correct answers or without cor-\nrect answers in the choices. However, due to the\nlarge number of questions, manually verifying ev-\nery question is not practical. Thus, we leverage\nsimple active learning methodologies for annota-\ntion (Liu et al., 2022a; Bartolo et al., 2022; Li et al.,\n2023; Kratzwald et al., 2020). As shown in Fig-\nure 2-(c), initially, the LM verifies whether the\nquestions can be answered or not, and only those\n6We detected inappropriate expressions using https://\nplatform.openai.com/docs/guides/moderation.\nTrain\nDev\nTest\nTotal\nEasy Hard\nTotal\nEasy Hard\nTotal\nEnglish\n10,910 1,071\n292 1,363 1,071\n292 1,363\nJapanese\n11,696 1,117\n344 1,461 1,117\n344 1,461\nChinese\n12,159\n972\n546 1,518\n972\n546 1,518\nGerman\n12,504 1,279\n283 1,562 1,279\n283 1,562\nPortuguese 12,659 1,234\n348 1,582 1,234\n348 1,582\nDutch\n12,215 1,255\n271 1,526 1,255\n271 1,526\nFrench\n8,047\n786\n219 1,005\n786\n219 1,005\nRussian\n6,623\n445\n382\n827\n445\n382\n827\nTable 5: The statistics of mCSQA\nquestions that the LM cannot answer are manually\nverified.\nVerification by LMs\nThe original questions can\nbe categorized into three types: those questions 1)\nwhich are correctly answerable by LMs, 2) which\nare wrongly answered by LMs, but humans can\nchoose the correct one, 3) which are not answer-\nable either by LMs or humans due to flaws in the\nquestion. Therefore, first, we identify the set of\nquestions LMs can answer, and then manually ver-\nify the questions that LMs could not answer cor-\nrectly to remove flawed questions.\nVerification by Humans\nWe hired two crowd\nworkers per language via Amazon Mechanical Turk\n(MTurk)7. The crowd workers were presented with\nthe question sentence, choices, and answer, and\nthey were asked to verify if the answer could be\nconcluded from the question and choices. We re-\ntained only those questions on which all crowd\nworkers agreed.\n3.4\nData Splitting and Statistics\nSimilar to CSQA and JCSQA, we randomly split\nthe data for each language into training, develop-\nment, and test sets with an 80/10/10 split. The\nmCSQA is evaluated by accuracy following the\nstandard practice in CSQA and JCSQA. Addition-\nally, in Section 3.3, questions that LMs could an-\nswer correctly are categorized as Easy, and those\nanswerable by human judgment are categorized as\nHard for development and test sets.\nTable 5 shows the number of questions per lan-\nguage and split, and Figure 3 shows the percentage\nfiltered at each step. The total cost per question is\n0.002 dollars for mCSQA compared to 0.33 dol-\nlars for CSQA, reducing the cost to less than one\n7There are workers for each language on MTurk (Pavlick\net al., 2014). We hired workers who have an approval rate\ngreater than 90% with at least 50 approved HITs.\n5\n1\u0019.22%\n1\u001c.11%\n30.3\u001b%\n15.\u001a3%\n1\u001c.35%\n15.0\u001a%\n1\u001a.\u001a5%\n32.\u0019\u001a%\n5\u001c.53%\n\u00192.10%\n54.03%\n\u001a1.0\u001c%\n\u0019\u001b.5\u0019%\n\u0019\u001c.\u001a4%\n\u00193.52%\n3\u001b.0\u0019%\n\u0019.04%\n5.\u001c\u001c%\n11.44%\n5.0\u001b%\n4.\u001a\u0019%\n4.03%\n5.\u001a\u001a%\n1\u0019.5\u001a%\n1\u001b.03%\n12.\u001a3%\n4.0\u0019%\n\u001b.05%\n\u001a.1\u001c%\n11.0\u001a%\n12.\u001b\u001b%\n10.\u001a\u0019%\n0.1\u001a%\n0.0\u001a%\n0.0\u001b%\n0.04%\n0.14%\n0.0\u001b%\n0.0\u001b%\n1.\u001c5%\nEQgliVh\nJaSaQeVe\nChiQeVe\nGeUmaQ PRUWXgXeVe\nDXWch\nFUeQch\nRXVViaQ\n0\n1k\n2k\n3k\n4k\n5k\n6k\n7k\n8k\n9k\n10k\n11k\n12k\n13k\n14k\n15k\n16k\n17k\n18k\nModeration\nViolate instruction\nFailed verification\nEasy\nHard\nFigure 3: The percentage of sentences processed at each\nstep. Easy and Hard were adopted for the dataset, while\nothers were removed during the generation process.\n\u0019\u0019.4%\n15.1%\n\u001c.\u001b\u001c%\n4.41%\n4.1\u0019%\nTRWDO\n$36.30455\nPHU QXHVWLRQV\n$0.00266\n5\u0019.2%\n20.3%\n12.2%\n5.\u001c%\n5.35%\nTRWDO\n$48.26667\nPHU QXHVWLRQV\n$0.00330\n\u001a0.\u001a%\n13.\u001b%\n\u001b.2\u001b%\n3.\u00194%\n3.5\u001a%\nTRWDO\n$64.00842\nPHU QXHVWLRQV\n$0.00419\n53.\u001b%\n20.\u001c%\n13.\u001a%\n\u0019.35%\n5.25%\nTRWDO\n$41.93523\nPHU QXHVWLRQV\n$0.00268\n\u00190.\u001c%\n1\u001a.5%\n11.\u0019%\n5.33%\n4.\u00191%\nTRWDO\n$42.73688\nPHU QXHVWLRQV\n$0.00270\n53%\n21%\n13.\u0019%\n\u0019.\u001b2%\n5.55%\nTRWDO\n$38.95217\nPHU QXHVWLRQV\n$0.00255\n\u00190.4%\n1\u001a.5%\n11.\u0019%\n5.\u001b%\n4.\u0019\u001b%\nTRWDO\n$29.00446\nPHU QXHVWLRQV\n$0.00288\n\u001a1.4%\n12.\u001c%\n\u001b.12%\n4.22%\n3.3\u0019%\nTRWDO\n$48.57447\nPHU QXHVWLRQV\n$0.00587\nVeULW\\ b\\ HXPaQ\nAdd DLVWUacWRUV\nGeQeUaWe QXeVWLRQV\nRefLQe QXeVWLRQV\nVeULf\\ b\\ LM\nEQgliVh\nJaSaQeVe\nChiQeVe\nGeUmaQ\nPRUWXgXeVe\nDXWch\nFUeQch\nRXVViaQ\nFigure 4: The cost details for each language and step.\nhundredth. Figure 4 shows the detailed costs.\nAppendix B discusses more detailed statistics,\nand Figure 11 shows examples of mCSQA.\n4\nEvaluation for mCSQA\nWe verify that the mCSQA dataset is meaningful\nfor evaluating the common sense reasoning capa-\nbility of LMs by using various multilingual LMs.\n4.1\nExperimental Setup\nSettings for LMs\nWe used mBERT (Devlin et al.,\n2019), XLM-100 (Conneau and Lample, 2019),\nXLM-R (Conneau et al., 2020), and mDeBERTa-\nv3 (He et al., 2023) as encoder-based multilingual\nLMs, Llama2-70B (Touvron et al., 2023), GPT-\n3.5 (Ouyang et al., 2022), and GPT-4 (OpenAI\net al., 2024) as decoder-based multilingual LMs for\nthe experiments. Decoder-based LMs inferred with\n0-shot and 3-shot settings. For detailed experimen-\ntal settings, please refer to the Appendix A.\nSettings for human baseline\nWe followed the\nCSQA setting and randomly selected 100 questions\neach from the validation and test data for every\nlanguage to measure the human baseline. We hired\nfive new crowd-workers per language on MTurk.\nThe answers were decided by a majority vote for\neach question.\n4.2\nEvaluation Results\nTable 6 shows the main results. Focusing on the\nperformance of zero-shot setting of GPT-3.5, which\nwas used for dataset creation, we find that its per-\nformance is equivalent to or worse than that of En-\ncoder models like XLM-RLARGE and mDeBERTa-\nv3 except for German and Russian. When compar-\ning the results of GPT-3.5 with GPT-4, the perfor-\nmance of GPT-3.5 is inferior for most languages\nto that of GPT-4. This indicates that the questions\nGPT-3.5 failed to answer correctly are those that\ncannot be answered by the knowledge of GPT-3.5,\nand it implies that the root cause is a lack of knowl-\nedge or reasoning capability of GPT-3.5. Further-\nmore, focusing on Decoder-based models, the re-\nsults are better in the 3-shot setting than in the\n0-shot in most cases. This trend was observed even\nwith the GPT-3.5 used for question creation.\nThe results show that the prompting technique\nis effective for mCSQA in exploiting the reason-\ning capabilities of decoder-based LMs. The trend\nis similar to other commonsense reasoning tasks\nlike CSQA (Qin et al., 2023; Chowdhery et al.,\n2023; Wei et al., 2022c; Brown et al., 2020; Dou\nand Peng, 2022), indicating that mCSQA can be\nequally effective as a dataset for commonsense rea-\nsoning tasks. Finally, when compared to the human\nbaseline, there is a significant gap in the results of\nall LMs. Thus, it can be said that even when using\nLMs for question creation, it is possible to create a\ndataset with sufficient quality and difficulty for the\nLMs themselves.\n5\nDiscussion\n5.1\nComparison of Easy vs. Hard\nWe compare the accuracy of Easy and Hard sets\nfor more fine-grained analysis. Figure 5 shows the\nresults in the test split. GPT-3.5 and GPT-4 could\nchoose the answer correctly in most cases for the\nEasy sets, but the accuracy is lower in the Hard sets\nwith a significant gap when compared with human\nresults; note that GPT-3.5 cannot answer there sets\nduring the dataset creation. The other LMs also\nshow a gap in evaluation accuracy with results for\nHard sets being lower than those for Easy ones.\nThese results, specifically the trend observed\n6\nEnglish\nJapanese\nChinese\nGerman\nPortuguese\nDutch\nFrench\nRussian\ndev\ntest\ndev\ntest\ndev\ntest\ndev\ntest\ndev\ntest\ndev\ntest\ndev\ntest\ndev\ntest\nHuman (Rand. 100 sent.) 87.0 93.0 89.0 95.0 91.0 87.0 96.0 96.0 93.0\n93.0\n98.0 97.0 96.0 92.0 87.0 94.0\nmBERT-cased\n60.6 61.3 66.0 63.5 65.9 63.5 58.6 57.9 65.2\n61.5\n54.8 57.8 46.3 47.3 32.2 31.3\nmBERT-uncased\n63.4 65.2 61.3 58.9 64.0 62.0 59.3 60.3 67.6\n63.9\n57.3 56.9 51.1 52.4 32.5 34.0\nXLM-100\n57.2 59.0 60.2 58.8 60.0 61.5 54.4 54.7 62.7\n59.5\n52.2 52.0 35.3 35.0 23.2 26.0\nXLM-RBASE\n68.0 69.1 68.5 66.2 69.8 68.3 63.9 62.8 69.5\n67.3\n62.0 64.0 47.6 45.5 36.9 37.0\nXLM-RLARGE\n77.2 77.5 75.7 72.6 75.0 74.1 76.2 75.4 79.0\n76.4\n73.0 74.7 62.0 62.3 48.9 49.5\nmDeBERTa-v3\n76.6 79.2 77.2 74.1 74.6 72.0 75.7 77.5 78.3\n78.2\n72.7 74.9 62.1 62.4 51.3 49.9\nLlama2-70B (0-shot)\n48.1 47.7 25.6 24.8 26.5 25.9 32.5 32.7 38.7\n37.6\n40.9 39.4 42.3 44.1 23.5 22.9\nLlama2-70B (3-shot)\n57.1 55.5 47.4 46.6 33.3 30.2 63.1 62.9 65.0\n63.7\n60.8 62.3 57.8 56.7 30.8 32.3\nGPT-3.5 (0-shot)\n76.7 77.0 76.3 76.7 64.0 63.6 81.3 81.4 77.9\n77.7\n82.1 81.5 78.6 77.1 53.3 53.0\nGPT-3.5 (3-shot)\n77.2 78.4 77.5 77.0 65.3 64.3 83.2 81.4 78.5\n78.0\n81.8 80.5 78.4 76.5 54.1 50.1\nGPT-4 (0-shot)\n80.9 80.9 78.4 77.2 66.0 65.6 81.0 81.0 78.6\n77.6\n83.4 81.5 78.8 77.0 49.9 47.8\nGPT-4 (3-shot)\n80.5 81.0 78.5 77.5 67.2 66.9 82.6 81.6 80.5\n78.8\n83.3 81.6 79.0 77.4 50.1 48.9\nTable 6: The results on mCSQA (acc. %)\nTotal\nEasy\nHard\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nTotal\nEasy\nHard\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nTotal\nEasy\nHard\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nTotal\nEasy\nHard\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nTotal\nEasy\nHard\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nTotal\nEasy\nHard\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nTotal\nEasy\nHard\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nTotal\nEasy\nHard\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nmBERT-cased\nmBERT-uncased\nXLM\nXLM-R_BASE\nXLM-R_LARGE\nmDeBERTa-v3\nLlama2-70B (0-shot)\nLlama2-70B (3-shot)\nGPT-3.5 (0-shot)\nGPT-3.5 (3-shot)\nGPT-4 (0-shot)\nGPT-4 (3-shot)\nHuman (100 sent.)\nChance Rate\nEnglish\nJapanese\nChinese\nGerman\nPortuguese\nDutch\nFrench\nRussian\nFigure 5: Comparison of the evaluation accuracy between Easy and Hard sets.\nwith GPT-3.5, show that even if LMs can create\nquestions, it does not necessarily mean that they\ncan answer them, and it entails that the question\ncreation and answering are totally different capa-\nbilities. Therefore, we conclude that LMs can sub-\nstitute for humans in parts of dataset creation pro-\ncesses from structured data and common sense rea-\nsoning task creations.\n5.2\nEvaluation of Multilingual LMs’\nCross-Lingual Transfer Capabilities\nThe cross-lingual transfer performance of multi-\nlingual LMs is often evaluated from English to\nother language directions due to linguistic resource\nreasons. The X-CSQA dataset (Lin et al., 2021),\nwhich consists solely of machine-translated ques-\ntions from CSQA’s development and test splits,\ncaptures only the one-way cross-lingual transfer\nperformance of LMs that were trained in English to\nevaluate their performance in other languages. In\ncontrast, mCSQA supports the evaluation of cross-\nlingual language transfer performance in any direc-\ntions among multilingual LMs that were trained in\neach of the eight languages.\nFigure 6 shows the results of the multilingual\nLM, XLM-RLARGE, which was fine-tuned in each\nof the eight languages separately and then evaluated\nacross all eight languages on mCSQA, using the\nsame settings as in Table 10. The results from Fig-\nure 6 show that, regardless of the language in which\nthey were trained, cross-lingual transfer abilities\nare observed in most cases for any languages given\nthe relative lower drop of performance when com-\npared with the monolingual performance. More-\nover, in the Easy sets, the drop is within 10% for\nmost language pairs, while in the Hard sets, it ex-\n7\nFigure 6:\nThe language transfer performance of\nXLM-RLARGE. The y-axis indicates the languages in\nwhich the model was fine-tuned, while the x-axis indi-\ncates the languages used for evaluation. It shows the\npercentage of performance achieved when compared\nwith the model trained and evaluated in the same lan-\nguage.\nceeds 20%. This indicates that questions that are\nrelatively easy to judge (Easy sets) facilitate the lan-\nguage transfer capability, but questions requiring\ndeep background knowledge (Hard sets) necessi-\ntate language-specific training and the development\nof LMs.\n5.3\nWhich is Better: Monolingual Fine-tuning\nor Multilingual Fine-tuning?\nSome studies (Tran and Bisazza, 2019; Dhamecha\net al., 2021; Trotta et al., 2021; Barbieri et al., 2022;\nPortelli et al., 2023) reported that multilingual fine-\ntuning could improve a part of NLU task perfor-\nmance more than monolingual tuning alone. On\nthe other hand, several studies (Tsai et al., 2019;\nKondratyuk, 2019; Rønningstad, 2023; Kondratyuk\nand Straka, 2019) reported that it did not always\nimprove performance in some tasks. We analyzed\nwhether multilingual fine-tuning is effective for\ncommonsense reasoning tasks through mCSQA.\nWe used the whole shuffled training split data in all\nlanguages and fine-tuned XLM-RLARGE with the\nsame setting as in Table 10. Table 7 compares the\naccuracy between monolingual fine-tuning, where\ntuning and evaluation are in the same language, and\nmultilingual fine-tuning, where tuning is performed\nfor all languages, evaluated for each language’s\naccuracy score. These results show that most lan-\nguages observed improvements, especially in all\ncases in Easy sets. However, in Hard sets, some\ncases observed a decline in performance compared\nto the monolingual setting. Therefore, while train-\ning in a multilingual setting generally promotes\nTest (%)\nen\nja\nzh\nde\npt\nnl\nfr\nru\nTotal\nMono. 77.5 72.6 74.1 75.4 76.4 74.7 62.3 49.5\nMulti.\n81.4 74.6 74.2 77.8 79.9 77.0 65.7 54.2\n∆\n3.9\n2.0\n0.1\n2.4\n3.5\n2.3\n2.4\n4.7\nUnseen 80.0 71.8 71.3 76.6 76.0 76.6 64.9 51.8\nEasy\nMono. 82.8 81.9 85.3 81.2 83.8 82.5 68.4 60.0\nMulti.\n86.5 83.4 85.7 84.4 86.6 84.9 73.2 70.1\n∆\n3.7\n2.5\n0.4\n3.2\n2.8\n2.4\n4.8 10.1\nUnseen 85.4 81.2 84.1 83.4 83.1 84.1 71.8 68.8\nHard\nMono. 58.6 42.4 54.6 49.1 50.6 37.6 39.7 35.1\nMulti.\n62.7 45.9 53.7 47.7 56.0 40.2 38.8 35.6\n∆\n4.1\n3.5 -0.9 -1.4\n5.4\n3.6 -0.9\n0.5\nUnseen 60.3 41.0 48.7 45.9 51.2 39.1 38.6 31.7\nTable 7: The performance comparison of XLM-RLARGE\non test data for each language when trained on mono-\nlingual training data versus multilingual data. ∆means\nthe differences in performance between the two settings.\nUnseen means the accuracy when trained on all training\ndata except for the evaluation language.\naccuracy improvement, multilingual training might\nlead to the loss of language-specific commonsense\ninformation for questions requiring more human\ncommonsense. This analysis complements the pre-\nvious reports (Dhamecha et al., 2021; Zhang et al.,\n2023a; Hu et al., 2021; Mueller et al., 2020) on the\nsuccesses and failures of multilingual training.\nFurthermore, Table 7 shows the evaluation re-\nsults of cross-lingual performance in the unseen\nsetting, where the model was not trained on the lan-\nguage for evaluation data. While some languages\noutperform the monolingual setting, overall results\nindicate that training with target language data con-\nsistently yields better outcomes. This suggests that\ntarget language data acts as the secret sauce for en-\nhancing NLU performance. Therefore, it suggests\nthat for language-specific deep knowledge and cul-\ntural understanding, language-transfer capability\nalone is insufficient, and training with datasets fo-\ncused on language-specific knowledge is necessary.\n5.4\nCase Study for Improvement through\nFew-Shot Learning\nAs shown in Figure 5, GPT-3.5 correctly answers\nmost questions in the Easy setting of mCSQA, but\nin the Hard setting, it fails to answer most questions\nin the 0-shot setting. This is because GPT-3.5 is\nused for quality filtering of mCSQA in Section 3.3,\nmaking it inherently unable to answer the questions\nin the Hard setting in the 0-shot setting. However,\n8\nQuestion\nAnswer 0-shot 3-shot\nWhich types of aquatic animals are commonly found in the open sea?\ne\na\ne\n(a) marine life, (b) earth, (c) waves, (d) coastline, (e) oceanic fish\nWhat is the purpose of using hand gestures while driving?\nb\nc\nb\n(a) determine what caused noise, (b) giving signal to, (c) checking for any potential dangers,\n(d) warning, (e) investigating the source of the noise\nTable 8: Examples of GPT-3.5 correctly answering in a 3-shot setting. In the top example, a 0-shot setting would\nchoose “marine life”, but considering the phrase “in the open sea” in the question, the answer should be narrowed\ndown to “oceanic fish”. On the other hand, in the bottom example, it chooses “checking for any potential dangers”,\nbut “hand gestures while driving” can include broader, non-dangerous signals such as thank-you gestures. Therefore,\nthe broader “giving signal to” is correct. In this way, the 3-shot setting tended to allow for appropriately granular\nanswers that matched the intent of the question.\nin the 3-shot setting, it shows improvement for\nsome questions. Table 8 shows examples of ques-\ntions correctly answered in the 3-shot setting. Both\nexamples in Table 8 are mainly due to the granu-\nlarity of the answers. The 3-shot setting promotes\nanswers at an appropriate granularity for questions\nthat are difficult to judge due to inclusive relation-\nships.\nIn the top example in Table 8, careful reading\nof the questions narrows down the answer choices.\nOn the other hand, in the bottom example, consider-\ning various common knowledge in daily life helps\nto choose the most appropriate answer. Similar\ncharacteristics were observed for other languages\nas well. For more details, qualitative analyses of\nthe mCSQA dataset are described in Appendix B.\n6\nConclusion and Future Directions\nWe proposed an efficient and low-cost method for\ncreating NLU datasets from structured data by\nutilizing generative LMs as an alternative to tra-\nditional human annotation, often crowdsourced.\nInspired by CSQA and JCSQA, we created the\nmultilingual commonsense reasoning task dataset,\nmCSQA, using GPT-3.5 from the structured mul-\ntilingual knowledge base ConceptNet. We demon-\nstrated that mCSQA is useful for evaluating the\ncommonsense reasoning capabilities of LMs. We\nalso analyzed the language-transfer capability be-\nyond English with mCSQA and examined the\nlanguage-specific learning from two aspects: ques-\ntion difficulty and language information. More-\nover, our study has shown that the use of multilin-\ngual LMs enables the construction of multilingual\ndatasets. Therefore, our method can significantly\nreduce human labor and financial costs.\nIn this study, we used a single multilingual LM,\nbut since each step is independent, it is possible to\nreplace the LM used in each step with another one.\nFurthermore, each step can be applied modularly\nto other methods, making it possible to use this\nmethod for creating multilingual datasets, such as\nthose expanded through translation and manual re-\nfinement (Yanaka and Mineshima, 2022; Seo et al.,\n2022). We aim to extend this method to other types\nof commonsense reasoning tasks and NLU tasks,\nto efficiently create multilingual data and conduct\na more comprehensive analysis of transfer capabil-\nities across a broader range of tasks and languages.\nWe focused on language-specific commonsense,\nbut languages are shared across various regions.\nFor example, English is spoken in the United States,\nthe United Kingdom, India, Australia, and many\nother regions each of which is geographically dis-\ntant and diverse in terms of climate, food, and\nculture. Therefore, it will be necessary to create\nmore detailed commonsense tasks that consider\ncultural differences rather than just language such\nas Kabra et al. (2023); Khanuja et al. (2024); Kim\net al. (2024); Cao et al. (2024); Fung et al. (2024);\nLee et al. (2024); Shwartz (2022); Hovy and Yang\n(2021); Yin et al. (2022); Shi et al. (2024). Our\ndataset construction method can be useful in creat-\ning various commonsense reasoning datasets that\noutgrow language limits.\n7\nLimitations\nData Resources\nThe number of multilingual re-\nsources is significantly smaller than that of mono-\nlingual resources. Additionally, quality is not con-\nsistent, and there are imbalances in data volume\nacross languages in these multilingual resources.\nIn this study, we used ConceptNET, a multilingual\nknowledge base, and encountered these issues as\nwell. For example, despite Spanish having a signif-\nicantly higher entity count, it obtained fewer QSs\n9\ndue to its inability to meet the required conditions\nbecause of ConceptNet’s sparsity issue, and thus it\nwas excluded from the language selection for mC-\nSQA. We believe these problems can be addressed\nthrough the automatic generation of knowledge\nbases (Zhang et al., 2020b,a; West et al., 2022; Ide\net al., 2023; Nguyen et al., 2023) and data augmen-\ntation techniques for knowledge bases (Malaviya\net al., 2020; Ju et al., 2022; Wu et al., 2023; Shen\net al., 2023), supported by their pre-trained knowl-\nedge (Sakai et al., 2023).\nDataset Quality\nIn this study, we used GPT-3.5\nand simple prompts for data creation. Therefore,\nthere is room for improvement in the selection of\nLMs and the refinement of prompts. In a pilot\nstudy, we tried using GPT-4 and recognized that\nit is more capable of creating datasets. However,\ndue to budgetary constraints, we have used GPT-\n3.5 in this study. Thus, it may become possible to\ncreate higher quality datasets at a lower cost when\nthe API prices decrease or by switching to other\nstrong LMs such as Gemini (Team et al., 2023),\nMixtral (Jiang et al., 2024), Llama (Touvron et al.,\n2023), phi (Abdin et al., 2024) or Qwen (Bai et al.,\n2023). Additionally, employing prompt strategies\nthat leverage the capabilities of LMs, such as Chain\nof Thought (CoT) (Wei et al., 2022c), Tree of\nThought (ToT) (Yao et al., 2023a) and ReAct (Yao\net al., 2023b), could potentially lead to the produc-\ntion of higher quality datasets.\nVerification of dataset quality by humans\nThe\nhuman baselines decreased the evaluation result un-\nder the Hard sets compared to Easy sets in Figure 5.\nTherefore, there exists a risk that the Hard sets in-\nclude flawed questions, even after manual quality\nverification. The JCSQA has pointed out that such\nlow-quality questions are included in CSQA, and\nwe have confirmed that they are similarly present\nin JCSQA. Thus, it is extremely difficult to com-\npletely eliminate such low-quality questions. Com-\nparing the percentage of data removed in quality\nverification, CSQA is 25% (3995/16242), and JC-\nSQA is 19% (2643/13906), whereas for mCSQA,\nit is 27% (604/2226) and 23% (599/2510) respec-\ntively, according to Figure 3 and referenced in their\nrespective papers. This indicates that the filtering\nratios are almost comparable when compared to\nthose, showing that this is not a problem unique\nto mCSQA. Therefore, reinforcing quality verifica-\ntion to filter out low-quality questions is a challenge\nin our future studies. However, as Figure 4 shows,\nsince more than half of the costs are already spent\non manual quality verification, simply hiring more\ncrowd workers would not be a better choice. Hence,\nexploring more efficient methods of quality verifi-\ncation as an alternative or to assist crowd workers\nin the future is necessary.\nHuman baseline\nThe experimental results in-\nclude human baselines using small sets of samples.\nHowever, Tedeschi et al. (2023) argue that human\nbaselines may lack reliability due to factors such\nas the payment issues for crowd workers and the\nimpact of random samples. Therefore, it should\nbe noted that the human baselines in this study are\nmerely reference values.\n8\nEthical Considerations\nLicense\nThe mCSQA dataset was created en-\ntirely from the outputs of GPT-3.5 and is therefore\nsubject to OpenAI’s license terms8. OpenAI as-\nsigns to us all rights, title, and interest in and to\nthe output. As a result, we are retaining the owner-\nship rights. There are no restrictions on distributing\nthe datasets, but using OpenAI’s model output to\ndevelop models that compete with OpenAI is pro-\nhibited. However, it’s possible that these terms\nmay change, and there may be a need to impose\ndistribution restrictions depending on the terms.\nModeration\nWe eliminated potentially harmful\nquestions such as violence, sexual content, and hate\nspeech by screening through OpenAI moderation\nAPIs9. However, in the commonsense reasoning\ndataset, it cannot be guaranteed that it does not\ninclude questions that contain societal biases as\ncollective knowledge. This issue has also been\npointed out in existing datasets such as CSQA, JC-\nSQA, and other commonsense reasoning datasets,\nand it is challenging to determine what is consid-\nered commonsense constitutes bias (Rajani et al.,\n2019; Sap et al., 2020; Bauer et al., 2023; An et al.,\n2023). If you encounter any harmful questions that\ncontain such biases, please report them.\nTranslation Tool\nWe used DeepL Pro10 to trans-\nlate the example sentence, especially Table 1, to\navoid arbitrary translation. The copyright of the\ntranslation sentences belongs to us11.\n8https://openai.com/policies/terms-of-use\n9https://platform.openai.com/docs/guides/\nmoderation\n10https://www.deepl.com/translator\n11https://www.deepl.com/pro-license\n10\nReferences\nMarah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan,\nJyoti Aneja, Ahmed Awadallah, Hany Awadalla,\nNguyen Bach, Amit Bahree, Arash Bakhtiari, Jian-\nmin Bao, Harkirat Behl, Alon Benhaim, Misha\nBilenko, Johan Bjorck, Sébastien Bubeck, Qin Cai,\nMartin Cai, Caio César Teodoro Mendes, Weizhu\nChen, Vishrav Chaudhary, Dong Chen, Dongdong\nChen, Yen-Chun Chen, Yi-Ling Chen, Parul Chopra,\nXiyang Dai, Allie Del Giorno, Gustavo de Rosa,\nMatthew Dixon, Ronen Eldan, Victor Fragoso, Dan\nIter, Mei Gao, Min Gao, Jianfeng Gao, Amit Garg,\nAbhishek Goswami, Suriya Gunasekar, Emman\nHaider, Junheng Hao, Russell J. Hewett, Jamie\nHuynh, Mojan Javaheripi, Xin Jin, Piero Kauff-\nmann, Nikos Karampatziakis, Dongwoo Kim, Ma-\nhoud Khademi, Lev Kurilenko, James R. Lee, Yin Tat\nLee, Yuanzhi Li, Yunsheng Li, Chen Liang, Lars Li-\nden, Ce Liu, Mengchen Liu, Weishung Liu, Eric Lin,\nZeqi Lin, Chong Luo, Piyush Madan, Matt Mazzola,\nArindam Mitra, Hardik Modi, Anh Nguyen, Brandon\nNorick, Barun Patra, Daniel Perez-Becker, Thomas\nPortet, Reid Pryzant, Heyang Qin, Marko Radmi-\nlac, Corby Rosset, Sambudha Roy, Olatunji Ruwase,\nOlli Saarikivi, Amin Saied, Adil Salim, Michael San-\ntacroce, Shital Shah, Ning Shang, Hiteshi Sharma,\nSwadheen Shukla, Xia Song, Masahiro Tanaka, An-\ndrea Tupini, Xin Wang, Lijuan Wang, Chunyu Wang,\nYu Wang, Rachel Ward, Guanhua Wang, Philipp\nWitte, Haiping Wu, Michael Wyatt, Bin Xiao, Can\nXu, Jiahang Xu, Weijian Xu, Sonali Yadav, Fan Yang,\nJianwei Yang, Ziyi Yang, Yifan Yang, Donghan Yu,\nLu Yuan, Chengruidong Zhang, Cyril Zhang, Jian-\nwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang,\nYunan Zhang, and Xiren Zhou. 2024. Phi-3 technical\nreport: A highly capable language model locally on\nyour phone.\nAnurag Acharya, Kartik Talamadupula, and Mark A\nFinlayson. 2020. Towards an atlas of cultural com-\nmonsense for machine reasoning.\nDavid Adelani, Graham Neubig, Sebastian Ruder,\nShruti Rijhwani, Michael Beukman, Chester Palen-\nMichel, Constantine Lignos, Jesujoba Alabi, Sham-\nsuddeen Muhammad,\nPeter Nabende,\nCheikh\nM. Bamba Dione, Andiswa Bukula, Rooweither\nMabuya, Bonaventure F. P. Dossou, Blessing Sibanda,\nHappy Buzaaba, Jonathan Mukiibi, Godson Kalipe,\nDerguene Mbaye, Amelia Taylor, Fatoumata Kabore,\nChris Chinenye Emezue, Anuoluwapo Aremu, Perez\nOgayo, Catherine Gitau, Edwin Munkoh-Buabeng,\nVictoire Memdjokam Koagne, Allahsera Auguste\nTapo, Tebogo Macucwa, Vukosi Marivate, Mbon-\ning Tchiaze Elvis, Tajuddeen Gwadabe, Tosin\nAdewumi, Orevaoghene Ahia, and Joyce Nakatumba-\nNabende. 2022. MasakhaNER 2.0: Africa-centric\ntransfer learning for named entity recognition. In Pro-\nceedings of the 2022 Conference on Empirical Meth-\nods in Natural Language Processing, pages 4488–\n4508, Abu Dhabi, United Arab Emirates. Association\nfor Computational Linguistics.\nShourya Aggarwal, Divyanshu Mandowara, Vishwa-\njeet Agrawal, Dinesh Khandelwal, Parag Singla, and\nDinesh Garg. 2021.\nExplanations for Common-\nsenseQA: New Dataset and Models. In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 3050–3065, Online.\nAssociation for Computational Linguistics.\nMichael Ahn, Anthony Brohan, Noah Brown, Yev-\ngen Chebotar, Omar Cortes, Byron David, Chelsea\nFinn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol\nHausman, Alex Herzog, Daniel Ho, Jasmine Hsu,\nJulian Ibarz, Brian Ichter, Alex Irpan, Eric Jang,\nRosario Jauregui Ruano, Kyle Jeffrey, Sally Jes-\nmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalash-\nnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey\nLevine, Yao Lu, Linda Luu, Carolina Parada, Pe-\nter Pastor, Jornell Quiambao, Kanishka Rao, Jarek\nRettinghouse, Diego Reyes, Pierre Sermanet, Nico-\nlas Sievers, Clayton Tan, Alexander Toshev, Vincent\nVanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu,\nMengyuan Yan, and Andy Zeng. 2022. Do as i can,\nnot as i say: Grounding language in robotic affor-\ndances.\nKabir Ahuja, Harshita Diddee, Rishav Hada, Milli-\ncent Ochieng, Krithika Ramesh, Prachi Jain, Ak-\nshay Nambi, Tanuja Ganu, Sameer Segal, Mohamed\nAhmed, Kalika Bali, and Sunayana Sitaram. 2023.\nMEGA: Multilingual evaluation of generative AI.\nIn Proceedings of the 2023 Conference on Empir-\nical Methods in Natural Language Processing, pages\n4232–4267, Singapore. Association for Computa-\ntional Linguistics.\nHaozhe An, Zongxia Li, Jieyu Zhao, and Rachel\nRudinger. 2023. SODAPOP: Open-ended discov-\nery of social biases in social commonsense reasoning\nmodels. In Proceedings of the 17th Conference of\nthe European Chapter of the Association for Compu-\ntational Linguistics, pages 1573–1596, Dubrovnik,\nCroatia. Association for Computational Linguistics.\nJordi Armengol-Estapé, Ona de Gibert Bonet, and Maite\nMelero. 2022. On the multilingual capabilities of\nvery large-scale English language models. In Pro-\nceedings of the Thirteenth Language Resources and\nEvaluation Conference, pages 3056–3068, Marseille,\nFrance. European Language Resources Association.\nArnav Arora, Lucie-aimée Kaffee, and Isabelle Augen-\nstein. 2023. Probing pre-trained language models for\ncross-cultural differences in values. In Proceedings\nof the First Workshop on Cross-Cultural Considera-\ntions in NLP (C3NLP), pages 114–130, Dubrovnik,\nCroatia. Association for Computational Linguistics.\nMikel Artetxe, Sebastian Ruder, and Dani Yogatama.\n2020. On the cross-lingual transferability of mono-\nlingual representations. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 4623–4637, Online. Association\nfor Computational Linguistics.\n11\nAbhijeet Awasthi, Nitish Gupta, Bidisha Samanta,\nShachi Dave, Sunita Sarawagi, and Partha Talukdar.\n2023. Bootstrapping multilingual semantic parsers\nusing large language models. In Proceedings of the\n17th Conference of the European Chapter of the As-\nsociation for Computational Linguistics, pages 2455–\n2467, Dubrovnik, Croatia. Association for Computa-\ntional Linguistics.\nJinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,\nXiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei\nHuang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin,\nRunji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu,\nKeming Lu, Jianxin Ma, Rui Men, Xingzhang Ren,\nXuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong\nTu, Peng Wang, Shijie Wang, Wei Wang, Sheng-\nguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang,\nJian Yang, Shusheng Yang, Yang Yao, Bowen Yu,\nHongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingx-\nuan Zhang, Yichang Zhang, Zhenru Zhang, Chang\nZhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang\nZhu. 2023. Qwen technical report.\nFrancesco Barbieri, Luis Espinosa Anke, and Jose\nCamacho-Collados. 2022.\nXLM-T: Multilingual\nlanguage models in Twitter for sentiment analysis\nand beyond. In Proceedings of the Thirteenth Lan-\nguage Resources and Evaluation Conference, pages\n258–266, Marseille, France. European Language Re-\nsources Association.\nMax Bartolo, Tristan Thrush, Sebastian Riedel, Pontus\nStenetorp, Robin Jia, and Douwe Kiela. 2022. Mod-\nels in the loop: Aiding crowdworkers with generative\nannotation assistants. In Proceedings of the 2022\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 3754–3767, Seattle,\nUnited States. Association for Computational Lin-\nguistics.\nLisa Bauer, Hanna Tischer, and Mohit Bansal. 2023. So-\ncial commonsense for explanation and cultural bias\ndiscovery. In Proceedings of the 17th Conference of\nthe European Chapter of the Association for Compu-\ntational Linguistics, pages 3745–3760, Dubrovnik,\nCroatia. Association for Computational Linguistics.\nYonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi,\net al. 2020. Piqa: Reasoning about physical com-\nmonsense in natural language. In Proceedings of the\nAAAI conference on artificial intelligence, volume 34,\npages 7432–7439.\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn Proceedings of the 2015 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n632–642, Lisbon, Portugal. Association for Compu-\ntational Linguistics.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In\nProceedings of the 34th International Conference on\nNeural Information Processing Systems, NIPS’20,\nRed Hook, NY, USA. Curran Associates Inc.\nSabine Buchholz and Erwin Marsi. 2006. CoNLL-X\nshared task on multilingual dependency parsing. In\nProceedings of the Tenth Conference on Computa-\ntional Natural Language Learning (CoNLL-X), pages\n149–164, New York City. Association for Computa-\ntional Linguistics.\nYong Cao, Yova Kementchedjhieva, Ruixiang Cui, An-\ntonia Karamolegkou, Li Zhou, Megan Dare, Lucia\nDonatelli, and Daniel Hershcovich. 2024. Cultural\nAdaptation of Recipes. Transactions of the Associa-\ntion for Computational Linguistics, 12:80–99.\nYong Cao, Li Zhou, Seolhwa Lee, Laura Cabello, Min\nChen, and Daniel Hershcovich. 2023.\nAssessing\ncross-cultural alignment between ChatGPT and hu-\nman societies: An empirical study. In Proceedings of\nthe First Workshop on Cross-Cultural Considerations\nin NLP (C3NLP), pages 53–67, Dubrovnik, Croatia.\nAssociation for Computational Linguistics.\nDu Chen, Yi Huang, Xiaopu Li, Yongqiang Li,\nYongqiang Liu, Haihui Pan, Leichao Xu, Dacheng\nZhang, Zhipeng Zhang, and Kun Han. 2024. Orion-\n14b: Open-source multilingual large language mod-\nels.\nMichael Chen, Mike D’Arcy, Alisa Liu, Jared Fer-\nnandez, and Doug Downey. 2019.\nCODAH: An\nadversarially-authored question answering dataset\nfor common sense. In Proceedings of the 3rd Work-\nshop on Evaluating Vector Space Representations for\nNLP, pages 63–69, Minneapolis, USA. Association\nfor Computational Linguistics.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\ndrew M. Dai, Thanumalayan Sankaranarayana Pil-\nlai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\nRewon Child, Oleksandr Polozov, Katherine Lee,\n12\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\nand Noah Fiedel. 2023. Palm: Scaling language mod-\neling with pathways. Journal of Machine Learning\nResearch, 24(240):1–113.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, Al-\nbert Webson, Shixiang Shane Gu, Zhuyun Dai,\nMirac Suzgun, Xinyun Chen, Aakanksha Chowdh-\nery, Alex Castro-Ros, Marie Pellat, Kevin Robinson,\nDasha Valter, Sharan Narang, Gaurav Mishra, Adams\nYu, Vincent Zhao, Yanping Huang, Andrew Dai,\nHongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Ja-\ncob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le,\nand Jason Wei. 2022. Scaling instruction-finetuned\nlanguage models.\nJonathan H. Clark, Eunsol Choi, Michael Collins, Dan\nGarrette, Tom Kwiatkowski, Vitaly Nikolaev, and\nJennimaria Palomaki. 2020. TyDi QA: A benchmark\nfor information-seeking question answering in typo-\nlogically diverse languages. Transactions of the As-\nsociation for Computational Linguistics, 8:454–470.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nAlexis Conneau and Guillaume Lample. 2019. Cross-\nlingual language model pretraining. In Advances in\nNeural Information Processing Systems, volume 32.\nCurran Associates, Inc.\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Adina\nWilliams, Samuel Bowman, Holger Schwenk, and\nVeselin Stoyanov. 2018. XNLI: Evaluating cross-\nlingual sentence representations. In Proceedings of\nthe 2018 Conference on Empirical Methods in Nat-\nural Language Processing, pages 2475–2485, Brus-\nsels, Belgium. Association for Computational Lin-\nguistics.\nSteven Coyne, Keisuke Sakaguchi, Diana Galvan-Sosa,\nMichael Zock, and Kentaro Inui. 2023. Analyzing\nthe performance of gpt-3.5 and gpt-4 in grammatical\nerror correction.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nTejas Dhamecha, Rudra Murthy, Samarth Bharad-\nwaj, Karthik Sankaranarayanan, and Pushpak Bhat-\ntacharyya. 2021. Role of Language Relatedness in\nMultilingual Fine-tuning of Language Models: A\nCase Study in Indo-Aryan Languages. In Proceed-\nings of the 2021 Conference on Empirical Methods\nin Natural Language Processing, pages 8584–8595,\nOnline and Punta Cana, Dominican Republic. Asso-\nciation for Computational Linguistics.\nZi-Yi Dou and Nanyun Peng. 2022. Zero-shot common-\nsense question answering with cloze translation and\nconsistency optimization. Proceedings of the AAAI\nConference on Artificial Intelligence, 36(10):10572–\n10580.\nDheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel\nStanovsky, Sameer Singh, and Matt Gardner. 2019.\nDROP: A reading comprehension benchmark requir-\ning discrete reasoning over paragraphs. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long and Short Papers), pages 2368–2378, Min-\nneapolis, Minnesota. Association for Computational\nLinguistics.\nDenis Emelin, Ronan Le Bras, Jena D. Hwang, Maxwell\nForbes, and Yejin Choi. 2021. Moral stories: Situ-\nated reasoning about norms, intents, actions, and\ntheir consequences. In Proceedings of the 2021 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 698–718, Online and Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nTao Fang, Shu Yang, Kaixin Lan, Derek F. Wong, Jin-\npeng Hu, Lidia S. Chao, and Yue Zhang. 2023. Is\nchatgpt a highly fluent grammatical error correction\nsystem? a comprehensive evaluation.\nAnjalie Field and Yulia Tsvetkov. 2020. Unsupervised\ndiscovery of implicit gender bias. In Proceedings of\nthe 2020 Conference on Empirical Methods in Natu-\nral Language Processing (EMNLP), pages 596–608,\nOnline. Association for Computational Linguistics.\nMaxwell Forbes, Jena D. Hwang, Vered Shwartz,\nMaarten Sap, and Yejin Choi. 2020. Social chem-\nistry 101: Learning to reason about social and moral\nnorms. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 653–670, Online. Association for\nComputational Linguistics.\nYi Fung, Ruining Zhao, Jae Doo, Chenkai Sun, and\nHeng Ji. 2024. Massively multi-cultural knowledge\nacquisition & lm benchmarking.\nHimanshu Gupta,\nKevin Scaria,\nUjjwala Anan-\ntheswaran,\nShreyas\nVerma,\nMihir\nParmar,\nSaurabh Arjun Sawant, Chitta Baral, and Swa-\nroop Mishra. 2023. Targen: Targeted data generation\nwith large language models.\n13\nSuchin Gururangan, Swabha Swayamdipta, Omer Levy,\nRoy Schwartz, Samuel Bowman, and Noah A. Smith.\n2018. Annotation artifacts in natural language infer-\nence data. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 2 (Short Papers), pages 107–112,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nShreya Havaldar,\nBhumika Singhal,\nSunny Rai,\nLangchen Liu, Sharath Chandra Guntuku, and Lyle\nUngar. 2023. Multilingual language models are not\nmulticultural: A case study in emotion. In Proceed-\nings of the 13th Workshop on Computational Ap-\nproaches to Subjectivity, Sentiment, & Social Media\nAnalysis, pages 202–214, Toronto, Canada. Associa-\ntion for Computational Linguistics.\nYuta Hayashibe. 2020. Japanese realistic textual en-\ntailment corpus. In Proceedings of the Twelfth Lan-\nguage Resources and Evaluation Conference, pages\n6827–6834, Marseille, France. European Language\nResources Association.\nPengcheng He, Jianfeng Gao, and Weizhu Chen. 2023.\nDeBERTav3: Improving deBERTa using ELECTRA-\nstyle pre-training with gradient-disentangled embed-\nding sharing. In The Eleventh International Confer-\nence on Learning Representations.\nXuanli He, Islam Nassar, Jamie Kiros, Gholamreza Haf-\nfari, and Mohammad Norouzi. 2022. Generate, an-\nnotate, and learn: NLP with synthetic text. Transac-\ntions of the Association for Computational Linguis-\ntics, 10:826–842.\nOr Honovich, Thomas Scialom, Omer Levy, and Timo\nSchick. 2023. Unnatural instructions: Tuning lan-\nguage models with (almost) no human labor.\nIn\nProceedings of the 61st Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 14409–14428, Toronto, Canada.\nAssociation for Computational Linguistics.\nDirk Hovy and Diyi Yang. 2021. The importance of\nmodeling social factors of language: Theory and\npractice. In Proceedings of the 2021 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 588–602, Online. Association\nfor Computational Linguistics.\nHai Hu, Ziyin Zhang, Weifang Huang, Jackie Yan-Ki\nLai, Aini Li, Yina Patterson, Jiahui Huang, Peng\nZhang, Chien-Jer Charles Lin, and Rui Wang. 2023.\nRevisiting acceptability judgements.\nHai Hu, He Zhou, Zuoyu Tian, Yiwen Zhang, Yina Pat-\nterson, Yanting Li, Yixin Nie, and Kyle Richardson.\n2021. Investigating transfer learning in multilingual\npre-trained language models through Chinese natural\nlanguage inference. In Findings of the Association\nfor Computational Linguistics: ACL-IJCNLP 2021,\npages 3770–3785, Online. Association for Computa-\ntional Linguistics.\nLifu Huang, Ronan Le Bras, Chandra Bhagavatula, and\nYejin Choi. 2019. Cosmos QA: Machine reading\ncomprehension with contextual commonsense rea-\nsoning. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n2391–2401, Hong Kong, China. Association for Com-\nputational Linguistics.\nTenghao Huang, Faeze Brahman, Vered Shwartz, and\nSnigdha Chaturvedi. 2021. Uncovering implicit gen-\nder bias in narratives through commonsense infer-\nence. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2021, pages 3866–3873,\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nXiaolei Huang, Linzi Xing, Franck Dernoncourt, and\nMichael J. Paul. 2020.\nMultilingual Twitter cor-\npus and baselines for evaluating demographic bias\nin hate speech recognition. In Proceedings of the\nTwelfth Language Resources and Evaluation Confer-\nence, pages 1440–1448, Marseille, France. European\nLanguage Resources Association.\nTatsuya Ide, Eiki Murata, Daisuke Kawahara, Takato\nYamazaki, Shengzhe Li, Kenta Shinzato, and Toshi-\nnori Sato. 2023. Phalm: Building a knowledge graph\nfrom scratch by prompting humans and a language\nmodel.\nAlbert Q. Jiang, Alexandre Sablayrolles, Antoine\nRoux, Arthur Mensch, Blanche Savary, Chris\nBamford, Devendra Singh Chaplot, Diego de las\nCasas, Emma Bou Hanna, Florian Bressand, Gi-\nanna Lengyel, Guillaume Bour, Guillaume Lam-\nple, Lélio Renard Lavaud, Lucile Saulnier, Marie-\nAnne Lachaux, Pierre Stock, Sandeep Subramanian,\nSophia Yang, Szymon Antoniak, Teven Le Scao,\nThéophile Gervet, Thibaut Lavril, Thomas Wang,\nTimothée Lacroix, and William El Sayed. 2024. Mix-\ntral of experts.\nYiqiao Jin, Mohit Chandra, Gaurav Verma, Yibo Hu,\nMunmun De Choudhury, and Srijan Kumar. 2023.\nBetter to ask in english: Cross-lingual evaluation of\nlarge language models for healthcare queries.\nJinhao Ju, Deqing Yang, and Jingping Liu. 2022. Com-\nmonsense knowledge base completion with relational\ngraph attention network and pre-trained language\nmodel. In Proceedings of the 31st ACM International\nConference on Information & Knowledge Manage-\nment, CIKM ’22, page 4104–4108, New York, NY,\nUSA. Association for Computing Machinery.\nAnubha Kabra, Emmy Liu, Simran Khanuja, Al-\nham Fikri Aji, Genta Winata, Samuel Cahyawijaya,\nAnuoluwapo Aremu, Perez Ogayo, and Graham Neu-\nbig. 2023. Multi-lingual and multi-cultural figurative\nlanguage understanding. In Findings of the Asso-\nciation for Computational Linguistics: ACL 2023,\npages 8269–8284, Toronto, Canada. Association for\nComputational Linguistics.\n14\nMasahiro Kaneko and Naoaki Okazaki. 2023. Reduc-\ning sequence length by predicting edit spans with\nlarge language models. In Proceedings of the 2023\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 10017–10029, Singapore.\nAssociation for Computational Linguistics.\nJungo Kasai, Yuhei Kasai, Keisuke Sakaguchi, Yutaro\nYamada, and Dragomir Radev. 2023.\nEvaluating\ngpt-4 and chatgpt on japanese medical licensing ex-\naminations.\nPride Kavumba, Naoya Inoue, Benjamin Heinzerling,\nKeshav Singh, Paul Reisert, and Kentaro Inui. 2019.\nWhen choosing plausible alternatives, clever hans can\nbe clever. In Proceedings of the First Workshop on\nCommonsense Inference in Natural Language Pro-\ncessing, pages 33–42, Hong Kong, China. Associa-\ntion for Computational Linguistics.\nPhillip Keung, Yichao Lu, György Szarvas, and Noah A.\nSmith. 2020. The multilingual Amazon reviews cor-\npus.\nIn Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 4563–4568, Online. Association for\nComputational Linguistics.\nTannon Kew, Florian Schottmann, and Rico Sennrich.\n2023. Turning english-centric llms into polyglots:\nHow much multilinguality is needed?\nSimran Khanuja,\nSathyanarayanan Ramamoorthy,\nYueqi Song, and Graham Neubig. 2024. An image\nspeaks a thousand words, but can everyone listen? on\nimage transcreation for cultural relevance.\nEunsu Kim, Juyoung Suk, Philhoon Oh, Haneul Yoo,\nJames Thorne, and Alice Oh. 2024. CLIcK: A bench-\nmark dataset of cultural and linguistic intelligence\nin Korean.\nIn Proceedings of the 2024 Joint In-\nternational Conference on Computational Linguis-\ntics, Language Resources and Evaluation (LREC-\nCOLING 2024), pages 3335–3346, Torino, Italia.\nELRA and ICCL.\nVid Kocijan, Ernest Davis, Thomas Lukasiewicz, Gary\nMarcus, and Leora Morgenstern. 2023. The defeat\nof the winograd schema challenge.\nDan Kondratyuk. 2019. Cross-lingual lemmatization\nand morphology tagging with two-stage multilin-\ngual BERT fine-tuning. In Proceedings of the 16th\nWorkshop on Computational Research in Phonetics,\nPhonology, and Morphology, pages 12–18, Florence,\nItaly. Association for Computational Linguistics.\nDan Kondratyuk and Milan Straka. 2019. 75 languages,\n1 model: Parsing Universal Dependencies univer-\nsally. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n2779–2795, Hong Kong, China. Association for Com-\nputational Linguistics.\nFajri Koto, Timothy Baldwin, and Jey Han Lau. 2022.\nCloze evaluation for deeper understanding of com-\nmonsense stories in Indonesian. In Proceedings of\nthe First Workshop on Commonsense Representation\nand Reasoning (CSRR 2022), pages 8–16, Dublin,\nIreland. Association for Computational Linguistics.\nBernhard Kratzwald, Stefan Feuerriegel, and Huan Sun.\n2020.\nLearning a Cost-Effective Annotation Pol-\nicy for Question Answering. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 3051–3062,\nOnline. Association for Computational Linguistics.\nVarun Kumar, Ashutosh Choudhary, and Eunah Cho.\n2020. Data augmentation using pre-trained trans-\nformer models. In Proceedings of the 2nd Workshop\non Life-long Learning for Spoken Language Systems,\npages 18–26, Suzhou, China. Association for Com-\nputational Linguistics.\nVarun Kumar, Hadrien Glaude, Cyprien de Lichy, and\nWlliam Campbell. 2019. A closer look at feature\nspace data augmentation for few-shot intent classi-\nfication. In Proceedings of the 2nd Workshop on\nDeep Learning Approaches for Low-Resource NLP\n(DeepLo 2019), pages 1–10, Hong Kong, China. As-\nsociation for Computational Linguistics.\nKentaro Kurihara, Daisuke Kawahara, and Tomohide\nShibata. 2022. JGLUE: Japanese general language\nunderstanding evaluation. In Proceedings of the Thir-\nteenth Language Resources and Evaluation Confer-\nence, pages 2957–2966, Marseille, France. European\nLanguage Resources Association.\nSang Kwon, Gagan Bhatia, El Moatez Billah Nagoudi,\nand Muhammad Abdul-Mageed. 2023. Beyond En-\nglish: Evaluating LLMs for Arabic grammatical er-\nror correction. In Proceedings of ArabicNLP 2023,\npages 101–119, Singapore (Hybrid). Association for\nComputational Linguistics.\nViet Lai, Nghia Ngo, Amir Pouran Ben Veyseh, Hieu\nMan, Franck Dernoncourt, Trung Bui, and Thien\nNguyen. 2023. ChatGPT beyond English: Towards\na comprehensive evaluation of large language mod-\nels in multilingual learning. In Findings of the As-\nsociation for Computational Linguistics: EMNLP\n2023, pages 13171–13189, Singapore. Association\nfor Computational Linguistics.\nKenton Lee, Kelvin Guu, Luheng He, Tim Dozat, and\nHyung Won Chung. 2021. Neural data augmentation\nvia example extrapolation.\nNayeon Lee, Yejin Bang, Holy Lovenia, Samuel\nCahyawijaya, Wenliang Dai, and Pascale Fung. 2023.\nSurvey of social bias in vision-language models.\nNayeon Lee, Chani Jung, Junho Myung, Jiho Jin, Jose\nCamacho-Collados, Juho Kim, and Alice Oh. 2024.\nExploring cross-cultural differences in english hate\nspeech annotations: From dataset construction to\nanalysis.\n15\nHeather Lent and Anders Søgaard. 2021. Common\nsense bias in semantic role labeling. In Proceedings\nof the Seventh Workshop on Noisy User-generated\nText (W-NUT 2021), pages 114–119, Online. Associ-\nation for Computational Linguistics.\nMinzhi Li, Taiwei Shi, Caleb Ziems, Min-Yen Kan,\nNancy Chen, Zhengyuan Liu, and Diyi Yang. 2023.\nCoAnnotating: Uncertainty-guided work allocation\nbetween human and large language models for data\nannotation. In Proceedings of the 2023 Conference\non Empirical Methods in Natural Language Process-\ning, pages 1487–1505, Singapore. Association for\nComputational Linguistics.\nBill Yuchen Lin, Seyeon Lee, Xiaoyang Qiao, and Xi-\nang Ren. 2021.\nCommon sense beyond English:\nEvaluating and improving multilingual language\nmodels for commonsense reasoning. In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 1274–1287, Online.\nAssociation for Computational Linguistics.\nBill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei\nZhou, Chandra Bhagavatula, Yejin Choi, and Xiang\nRen. 2020. CommonGen: A constrained text gen-\neration challenge for generative commonsense rea-\nsoning. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2020, pages 1823–1840,\nOnline. Association for Computational Linguistics.\nAlisa Liu, Swabha Swayamdipta, Noah A. Smith, and\nYejin Choi. 2022a. WANLI: Worker and AI collabo-\nration for natural language inference dataset creation.\nIn Findings of the Association for Computational\nLinguistics: EMNLP 2022, pages 6826–6847, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nEmmy Liu, Chenxuan Cui, Kenneth Zheng, and Graham\nNeubig. 2022b. Testing the ability of language mod-\nels to interpret figurative language. In Proceedings\nof the 2022 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 4437–4452,\nSeattle, United States. Association for Computational\nLinguistics.\nXin Liu, Dayiheng Liu, Baosong Yang, Haibo Zhang,\nJunwei Ding, Wenqing Yao, Weihua Luo, Haiying\nZhang, and Jinsong Su. 2022c. KGR4: Retrieval,\nretrospect, refine and rethink for commonsense gen-\neration. Proceedings of the AAAI Conference on\nArtificial Intelligence, 36(10):11029–11037.\nMengsay Loem, Masahiro Kaneko, Sho Takase, and\nNaoaki Okazaki. 2023. Exploring effectiveness of\nGPT-3 in grammatical error correction: A study\non performance and controllability in prompt-based\nmethods. In Proceedings of the 18th Workshop on\nInnovative Use of NLP for Building Educational\nApplications (BEA 2023), pages 205–219, Toronto,\nCanada. Association for Computational Linguistics.\nShayne Longpre, Le Hou, Tu Vu, Albert Webson,\nHyung Won Chung, Yi Tay, Denny Zhou, Quoc V. Le,\nBarret Zoph, Jason Wei, and Adam Roberts. 2023.\nThe flan collection: Designing data and methods for\neffective instruction tuning. In Proceedings of the\n40th International Conference on Machine Learning,\nICML’23. JMLR.org.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler\nHallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,\nNouha Dziri, Shrimai Prabhumoye, Yiming Yang,\nShashank Gupta, Bodhisattwa Prasad Majumder,\nKatherine Hermann, Sean Welleck, Amir Yazdan-\nbakhsh, and Peter Clark. 2023. Self-refine: Itera-\ntive refinement with self-feedback. In Thirty-seventh\nConference on Neural Information Processing Sys-\ntems.\nChaitanya Malaviya, Chandra Bhagavatula, Antoine\nBosselut, and Yejin Choi. 2020.\nCommonsense\nknowledge base completion with structural and se-\nmantic context. Proceedings of the AAAI Conference\non Artificial Intelligence, 34(03):2925–2933.\nShervin Malmasi and Mark Dras. 2015. Large-scale\nnative language identification with cross-corpus eval-\nuation. In Proceedings of the 2015 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 1403–1409, Denver, Colorado. Asso-\nciation for Computational Linguistics.\nYu Meng, Jiaxin Huang, Yu Zhang, and Jiawei Han.\n2022. Generating training data with language mod-\nels: Towards zero-shot language understanding. In\nAdvances in Neural Information Processing Systems.\nNasrin Mostafazadeh, Nathanael Chambers, Xiaodong\nHe, Devi Parikh, Dhruv Batra, Lucy Vanderwende,\nPushmeet Kohli, and James Allen. 2016. A corpus\nand cloze evaluation for deeper understanding of\ncommonsense stories. In Proceedings of the 2016\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 839–849, San Diego,\nCalifornia. Association for Computational Linguis-\ntics.\nNasrin Mostafazadeh, Aditya Kalyanpur, Lori Moon,\nDavid Buchanan, Lauren Berkowitz, Or Biran, and\nJennifer Chu-Carroll. 2020.\nGLUCOSE: Gener-\naLized and COntextualized story explanations. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 4569–4586, Online. Association for Computa-\ntional Linguistics.\nDavid Mueller, Nicholas Andrews, and Mark Dredze.\n2020. Sources of transfer in multilingual named en-\ntity recognition. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 8093–8104, Online. Association for\nComputational Linguistics.\n16\nTuan-Phong Nguyen, Simon Razniewski, Aparna Varde,\nand Gerhard Weikum. 2023. Extracting cultural com-\nmonsense knowledge at scale. In Proceedings of\nthe ACM Web Conference 2023, WWW ’23, page\n1907–1917, New York, NY, USA. Association for\nComputing Machinery.\nKazumasa Omura, Daisuke Kawahara, and Sadao Kuro-\nhashi. 2020. A method for building a commonsense\ninference dataset based on basic events. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP), pages\n2450–2460, Online. Association for Computational\nLinguistics.\nOpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal,\nLama Ahmad, Ilge Akkaya, Florencia Leoni Ale-\nman, Diogo Almeida, Janko Altenschmidt, Sam Alt-\nman, Shyamal Anadkat, Red Avila, Igor Babuschkin,\nSuchir Balaji, Valerie Balcom, Paul Baltescu, Haim-\ning Bao, Mohammad Bavarian, Jeff Belgum, Ir-\nwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro,\nChristopher Berner, Lenny Bogdonoff, Oleg Boiko,\nMadelaine Boyd, Anna-Luisa Brakman, Greg Brock-\nman, Tim Brooks, Miles Brundage, Kevin Button,\nTrevor Cai, Rosie Campbell, Andrew Cann, Brittany\nCarey, Chelsea Carlson, Rory Carmichael, Brooke\nChan, Che Chang, Fotis Chantzis, Derek Chen, Sully\nChen, Ruby Chen, Jason Chen, Mark Chen, Ben\nChess, Chester Cho, Casey Chu, Hyung Won Chung,\nDave Cummings, Jeremiah Currier, Yunxing Dai,\nCory Decareaux, Thomas Degry, Noah Deutsch,\nDamien Deville, Arka Dhar, David Dohan, Steve\nDowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti,\nTyna Eloundou, David Farhi, Liam Fedus, Niko Felix,\nSimón Posada Fishman, Juston Forte, Isabella Ful-\nford, Leo Gao, Elie Georges, Christian Gibson, Vik\nGoel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-\nLopes, Jonathan Gordon, Morgan Grafstein, Scott\nGray, Ryan Greene, Joshua Gross, Shixiang Shane\nGu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris,\nYuchen He, Mike Heaton, Johannes Heidecke, Chris\nHesse, Alan Hickey, Wade Hickey, Peter Hoeschele,\nBrandon Houghton, Kenny Hsu, Shengli Hu, Xin\nHu, Joost Huizinga, Shantanu Jain, Shawn Jain,\nJoanne Jang, Angela Jiang, Roger Jiang, Haozhun\nJin, Denny Jin, Shino Jomoto, Billie Jonn, Hee-\nwoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Ka-\nmali, Ingmar Kanitscheider, Nitish Shirish Keskar,\nTabarak Khan, Logan Kilpatrick, Jong Wook Kim,\nChristina Kim, Yongjik Kim, Jan Hendrik Kirch-\nner, Jamie Kiros, Matt Knight, Daniel Kokotajlo,\nŁukasz Kondraciuk, Andrew Kondrich, Aris Kon-\nstantinidis, Kyle Kosic, Gretchen Krueger, Vishal\nKuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan\nLeike, Jade Leung, Daniel Levy, Chak Ming Li,\nRachel Lim, Molly Lin, Stephanie Lin, Mateusz\nLitwin, Theresa Lopez, Ryan Lowe, Patricia Lue,\nAnna Makanju, Kim Malfacini, Sam Manning, Todor\nMarkov, Yaniv Markovski, Bianca Martin, Katie\nMayer, Andrew Mayne, Bob McGrew, Scott Mayer\nMcKinney, Christine McLeavey, Paul McMillan,\nJake McNeil, David Medina, Aalok Mehta, Jacob\nMenick, Luke Metz, Andrey Mishchenko, Pamela\nMishkin, Vinnie Monaco, Evan Morikawa, Daniel\nMossing, Tong Mu, Mira Murati, Oleg Murk, David\nMély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak,\nArvind Neelakantan, Richard Ngo, Hyeonwoo Noh,\nLong Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex\nPaino, Joe Palermo, Ashley Pantuliano, Giambat-\ntista Parascandolo, Joel Parish, Emy Parparita, Alex\nPassos, Mikhail Pavlov, Andrew Peng, Adam Perel-\nman, Filipe de Avila Belbute Peres, Michael Petrov,\nHenrique Ponde de Oliveira Pinto, Michael, Poko-\nrny, Michelle Pokrass, Vitchyr H. Pong, Tolly Pow-\nell, Alethea Power, Boris Power, Elizabeth Proehl,\nRaul Puri, Alec Radford, Jack Rae, Aditya Ramesh,\nCameron Raymond, Francis Real, Kendra Rimbach,\nCarl Ross, Bob Rotsted, Henri Roussez, Nick Ry-\nder, Mario Saltarelli, Ted Sanders, Shibani Santurkar,\nGirish Sastry, Heather Schmidt, David Schnurr, John\nSchulman, Daniel Selsam, Kyla Sheppard, Toki\nSherbakov, Jessica Shieh, Sarah Shoker, Pranav\nShyam, Szymon Sidor, Eric Sigler, Maddie Simens,\nJordan Sitkin, Katarina Slama, Ian Sohl, Benjamin\nSokolowsky, Yang Song, Natalie Staudacher, Fe-\nlipe Petroski Such, Natalie Summers, Ilya Sutskever,\nJie Tang, Nikolas Tezak, Madeleine B. Thompson,\nPhil Tillet, Amin Tootoonchian, Elizabeth Tseng,\nPreston Tuggle, Nick Turley, Jerry Tworek, Juan Fe-\nlipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya,\nChelsea Voss, Carroll Wainwright, Justin Jay Wang,\nAlvin Wang, Ben Wang, Jonathan Ward, Jason Wei,\nCJ Weinmann, Akila Welihinda, Peter Welinder, Ji-\nayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner,\nClemens Winter, Samuel Wolrich, Hannah Wong,\nLauren Workman, Sherwin Wu, Jeff Wu, Michael\nWu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qim-\ning Yuan, Wojciech Zaremba, Rowan Zellers, Chong\nZhang, Marvin Zhang, Shengjia Zhao, Tianhao\nZheng, Juntang Zhuang, William Zhuk, and Barret\nZoph. 2024. Gpt-4 technical report.\nSimon Ostermann, Ashutosh Modi, Michael Roth, Ste-\nfan Thater, and Manfred Pinkal. 2018. MCScript:\nA novel dataset for assessing machine comprehen-\nsion using script knowledge. In Proceedings of the\nEleventh International Conference on Language Re-\nsources and Evaluation (LREC 2018), Miyazaki,\nJapan. European Language Resources Association\n(ELRA).\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul F Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback. In Advances in Neural Information\nProcessing Systems, volume 35, pages 27730–27744.\nCurran Associates, Inc.\nSungjoon Park, Jihyung Moon, Sungdong Kim, Won Ik\nCho, Ji Yoon Han, Jangwon Park, Chisung Song, Jun-\nseong Kim, Youngsook Song, Taehwan Oh, Joohong\nLee, Juhyun Oh, Sungwon Lyu, Younghoon Jeong,\nInkwon Lee, Sangwoo Seo, Dongjun Lee, Hyunwoo\n17\nKim, Myeonghwa Lee, Seongbo Jang, Seungwon\nDo, Sunkyoung Kim, Kyungtae Lim, Jongwon Lee,\nKyumin Park, Jamin Shin, Seonghyun Kim, Lucy\nPark, Alice Oh, Jung-Woo Ha, and Kyunghyun Cho.\n2021. KLUE: Korean language understanding evalu-\nation. In Thirty-fifth Conference on Neural Informa-\ntion Processing Systems Datasets and Benchmarks\nTrack (Round 2).\nAlicia\nParrish,\nAngelica\nChen,\nNikita\nNangia,\nVishakh Padmakumar, Jason Phang, Jana Thompson,\nPhu Mon Htut, and Samuel Bowman. 2022. BBQ:\nA hand-built bias benchmark for question answering.\nIn Findings of the Association for Computational\nLinguistics: ACL 2022, pages 2086–2105, Dublin,\nIreland. Association for Computational Linguistics.\nEllie Pavlick, Matt Post, Ann Irvine, Dmitry Kachaev,\nand Chris Callison-Burch. 2014. The language de-\nmographics of Amazon Mechanical Turk. Transac-\ntions of the Association for Computational Linguis-\ntics, 2:79–92.\nBaolin Peng, Chunyuan Li, Pengcheng He, Michel Gal-\nley, and Jianfeng Gao. 2023. Instruction tuning with\ngpt-4.\nEdoardo Maria Ponti, Goran Glavaš, Olga Majewska,\nQianchu Liu, Ivan Vuli´c, and Anna Korhonen. 2020.\nXCOPA: A multilingual dataset for causal common-\nsense reasoning. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 2362–2376, Online. As-\nsociation for Computational Linguistics.\nBeatrice Portelli, Alessandro Tremamunno, Simone\nScaboro, Emmanuele Chersoni, and Giuseppe Serra.\n2023. Ailabud at the ntcir-17 mednlp-sc task: Mono-\nlingual vs multilingual fine-tuning for ade classifica-\ntion. NII Institutional Repository.\nRifki Afina Putri, Faiz Ghifari Haznitrama, Dea Adhista,\nand Alice Oh. 2024. Can llm generate culturally rele-\nvant commonsense qa data? case study in indonesian\nand sundanese.\nChengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao\nChen, Michihiro Yasunaga, and Diyi Yang. 2023. Is\nChatGPT a general-purpose natural language process-\ning task solver? In Proceedings of the 2023 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 1339–1384, Singapore. Associa-\ntion for Computational Linguistics.\nNazneen Fatema Rajani, Bryan McCann, Caiming\nXiong, and Richard Socher. 2019.\nExplain your-\nself! leveraging language models for commonsense\nreasoning. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 4932–4942, Florence, Italy. Association for\nComputational Linguistics.\nVikas Raunak, Amr Sharaf, Yiren Wang, Hany\nAwadalla, and Arul Menezes. 2023. Leveraging GPT-\n4 for automatic translation post-editing.\nIn Find-\nings of the Association for Computational Linguis-\ntics: EMNLP 2023, pages 12009–12024, Singapore.\nAssociation for Computational Linguistics.\nMelissa Roemmele, Cosmin Adrian Bejan, and An-\ndrew S Gordon. 2011.\nChoice of plausible alter-\nnatives: An evaluation of commonsense causal rea-\nsoning. In 2011 AAAI Spring Symposium Series.\nEgil Rønningstad. 2023. UIO at SemEval-2023 task 12:\nMultilingual fine-tuning for sentiment classification\nin low-resource languages. In Proceedings of the\n17th International Workshop on Semantic Evaluation\n(SemEval-2023), pages 1054–1060, Toronto, Canada.\nAssociation for Computational Linguistics.\nUma Roy, Noah Constant, Rami Al-Rfou, Aditya Barua,\nAaron Phillips, and Yinfei Yang. 2020. LAReQA:\nLanguage-agnostic answer retrieval from a multilin-\ngual pool. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 5919–5930, Online. Association for\nComputational Linguistics.\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavat-\nula, and Yejin Choi. 2021. Winogrande: an adver-\nsarial winograd schema challenge at scale. Commun.\nACM, 64(9):99–106.\nYusuke Sakai, Hidetaka Kamigaito, Katsuhiko Hayashi,\nand Taro Watanabe. 2023. Does pre-trained language\nmodel actually infer unseen links in knowledge graph\ncompletion? CoRR, abs/2311.09109.\nMaarten Sap, Saadia Gabriel, Lianhui Qin, Dan Juraf-\nsky, Noah A. Smith, and Yejin Choi. 2020. Social\nbias frames: Reasoning about social and power im-\nplications of language. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 5477–5490, Online. Association\nfor Computational Linguistics.\nMaarten Sap, Ronan Le Bras, Emily Allaway, Chan-\ndra Bhagavatula, Nicholas Lourie, Hannah Rashkin,\nBrendan Roof, Noah A. Smith, and Yejin Choi.\n2019a. Atomic: an atlas of machine commonsense\nfor if-then reasoning. In Proceedings of the Thirty-\nThird AAAI Conference on Artificial Intelligence and\nThirty-First Innovative Applications of Artificial In-\ntelligence Conference and Ninth AAAI Symposium\non Educational Advances in Artificial Intelligence,\nAAAI’19/IAAI’19/EAAI’19. AAAI Press.\nMaarten Sap, Hannah Rashkin, Derek Chen, Ronan\nLe Bras, and Yejin Choi. 2019b. Social IQa: Com-\nmonsense reasoning about social interactions. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 4463–\n4473, Hong Kong, China. Association for Computa-\ntional Linguistics.\nTimo Schick and Hinrich Schütze. 2021. Generating\ndatasets with pretrained language models. In Pro-\nceedings of the 2021 Conference on Empirical Meth-\nods in Natural Language Processing, pages 6943–\n18\n6951, Online and Punta Cana, Dominican Republic.\nAssociation for Computational Linguistics.\nTim Schott, Daniel Furman, and Shreshta Bhat. 2023.\nPolyglot or not? measuring multilingual encyclope-\ndic knowledge in foundation models. In Proceedings\nof the 2023 Conference on Empirical Methods in\nNatural Language Processing, pages 11238–11253,\nSingapore. Association for Computational Linguis-\ntics.\nHolger Schwenk and Xian Li. 2018. A corpus for mul-\ntilingual document classification in eight languages.\nIn Proceedings of the Eleventh International Confer-\nence on Language Resources and Evaluation (LREC\n2018), Miyazaki, Japan. European Language Re-\nsources Association (ELRA).\nJaehyung Seo, Seounghoon Lee, Chanjun Park, Yoonna\nJang, Hyeonseok Moon, Sugyeong Eo, Seonmin Koo,\nand Heuiseok Lim. 2022. A dog is passing over the\njet? a text-generation dataset for Korean common-\nsense reasoning and evaluation. In Findings of the\nAssociation for Computational Linguistics: NAACL\n2022, pages 2233–2249, Seattle, United States. Asso-\nciation for Computational Linguistics.\nZhihong Shao, Yeyun Gong, Yelong Shen, Min-\nlie Huang, Nan Duan, and Weizhu Chen. 2023.\nSynthetic prompting: generating chain-of-thought\ndemonstrations for large language models. In Pro-\nceedings of the 40th International Conference on\nMachine Learning, ICML’23. JMLR.org.\nTatiana Shavrina, Alena Fenogenova, Emelyanov An-\nton, Denis Shevelev, Ekaterina Artemova, Valentin\nMalykh, Vladislav Mikhailov, Maria Tikhonova, An-\ndrey Chertok, and Andrey Evlampiev. 2020. Rus-\nsianSuperGLUE: A Russian language understand-\ning evaluation benchmark. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 4717–4726,\nOnline. Association for Computational Linguistics.\nXiangqing Shen, Siwei Wu, and Rui Xia. 2023. Dense-\nATOMIC: Towards densely-connected ATOMIC\nwith high knowledge coverage and massive multi-hop\npaths. In Proceedings of the 61st Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 13292–13305, Toronto,\nCanada. Association for Computational Linguistics.\nFreda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang,\nSuraj Srivats, Soroush Vosoughi, Hyung Won Chung,\nYi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das,\nand Jason Wei. 2023. Language models are multi-\nlingual chain-of-thought reasoners. In The Eleventh\nInternational Conference on Learning Representa-\ntions.\nWeiyan Shi, Ryan Li, Yutong Zhang, Caleb Ziems,\nChunhua yu, Raya Horesh, Rogério Abreu de Paula,\nand Diyi Yang. 2024.\nCulturebank: An online\ncommunity-driven knowledge base towards cultur-\nally aware language technologies.\nVered Shwartz. 2022. Good night at 4 pm?! time ex-\npressions in different cultures. In Findings of the As-\nsociation for Computational Linguistics: ACL 2022,\npages 2842–2853, Dublin, Ireland. Association for\nComputational Linguistics.\nShivalika Singh, Freddie Vargus, Daniel Dsouza,\nBörje F. Karlsson, Abinaya Mahendiran, Wei-Yin\nKo, Herumb Shandilya, Jay Patel, Deividas Mat-\naciunas, Laura OMahony, Mike Zhang, Ramith\nHettiarachchi, Joseph Wilson, Marina Machado,\nLuisa Souza Moura, Dominik Krzemi´nski, Hakimeh\nFadaei, Irem Ergün, Ifeoma Okoh, Aisha Alaagib,\nOshan Mudannayake, Zaid Alyafeai, Vu Minh Chien,\nSebastian Ruder, Surya Guthikonda, Emad A. Al-\nghamdi, Sebastian Gehrmann, Niklas Muennighoff,\nMax Bartolo, Julia Kreutzer, Ahmet Üstün, Marzieh\nFadaee, and Sara Hooker. 2024. Aya dataset: An\nopen-access collection for multilingual instruction\ntuning.\nDenis Smirnov. 2019. Neural network-based models\nwith commonsense knowledge for machine read-\ning comprehension. In Proceedings of the Student\nResearch Workshop Associated with RANLP 2019,\npages 90–94, Varna, Bulgaria. INCOMA Ltd.\nAndrea Sottana, Bin Liang, Kai Zou, and Zheng Yuan.\n2023. Evaluation metrics in the era of GPT-4: Reli-\nably evaluating large language models on sequence\nto sequence tasks. In Proceedings of the 2023 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 8776–8788, Singapore. Associa-\ntion for Computational Linguistics.\nRobyn Speer, Joshua Chin, and Catherine Havasi. 2017.\nConceptNet 5.5: An open multilingual graph of gen-\neral knowledge. In Proceedings of the Thirty-First\nAAAI Conference on Artificial Intelligence, AAAI’17,\npage 4444–4451. AAAI Press.\nIeva Stali¯unait˙e, Philip John Gorinski, and Ignacio Ia-\ncobacci. 2021. Improving commonsense causal rea-\nsoning by adversarial training and data augmentation.\nProceedings of the AAAI Conference on Artificial\nIntelligence, 35(15):13834–13842.\nZhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin\nZhang, Zhenfang Chen, David Daniel Cox, Yiming\nYang, and Chuang Gan. 2023. Principle-driven self-\nalignment of language models from scratch with min-\nimal human supervision. In Thirty-seventh Confer-\nence on Neural Information Processing Systems.\nNorio Takahashi, Tomohide Shibata, Daisuke Kawahara,\nand Sadao Kurohashi. 2019. Machine comprehen-\nsion improves domain-specific Japanese predicate-\nargument structure analysis. In Proceedings of the\n2nd Workshop on Machine Reading for Question An-\nswering, pages 98–104, Hong Kong, China. Associa-\ntion for Computational Linguistics.\nEkaterina Taktasheva,\nAlena Fenogenova,\nDenis\nShevelev, Nadezhda Katricheva, Maria Tikhonova,\nAlbina Akhmetgareeva, Oleg Zinkevich, Anastasiia\n19\nBashmakova, Svetlana Iordanskaia, Valentina Kuren-\nshchikova, Alena Spiridonova, Ekaterina Artemova,\nTatiana Shavrina, and Vladislav Mikhailov. 2022.\nTAPE: Assessing few-shot Russian language under-\nstanding. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2022, pages 2472–2497,\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and\nJonathan Berant. 2019. CommonsenseQA: A ques-\ntion answering challenge targeting commonsense\nknowledge. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4149–4158, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nAlon Talmor, Ori Yoran, Ronan Le Bras, Chandra Bha-\ngavatula, Yoav Goldberg, Yejin Choi, and Jonathan\nBerant. 2021. CommonsenseQA 2.0: Exposing the\nlimits of AI through gamification. In Thirty-fifth Con-\nference on Neural Information Processing Systems\nDatasets and Benchmarks Track (Round 1).\nAlexandre Tamborrino, Nicola Pellicanò, Baptiste Pan-\nnier, Pascal Voitot, and Louise Naudin. 2020. Pre-\ntraining is (almost) all you need: An application to\ncommonsense reasoning. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 3878–3887, Online. Association\nfor Computational Linguistics.\nGemini Team, Rohan Anil, Sebastian Borgeaud,\nYonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu\nSoricut, Johan Schalkwyk, Andrew M. Dai, Anja\nHauth, Katie Millican, David Silver, Slav Petrov,\nMelvin Johnson, Ioannis Antonoglou, Julian Schrit-\ntwieser, Amelia Glaese, Jilin Chen, Emily Pitler,\nTimothy Lillicrap, Angeliki Lazaridou, Orhan Fi-\nrat, James Molloy, Michael Isard, Paul R. Barham,\nTom Hennigan, Benjamin Lee, Fabio Viola, Malcolm\nReynolds, Yuanzhong Xu, Ryan Doherty, Eli Collins,\nClemens Meyer, Eliza Rutherford, Erica Moreira,\nKareem Ayoub, Megha Goel, George Tucker, En-\nrique Piqueras, Maxim Krikun, Iain Barr, Nikolay\nSavinov, Ivo Danihelka, Becca Roelofs, Anaïs White,\nAnders Andreassen, Tamara von Glehn, Lakshman\nYagati, Mehran Kazemi, Lucas Gonzalez, Misha\nKhalman, Jakub Sygnowski, Alexandre Frechette,\nCharlotte Smith, Laura Culp, Lev Proleev, Yi Luan,\nXi Chen, James Lottes, Nathan Schucher, Federico\nLebron, Alban Rrustemi, Natalie Clay, Phil Crone,\nTomas Kocisky, Jeffrey Zhao, Bartek Perz, Dian Yu,\nHeidi Howard, Adam Bloniarz, Jack W. Rae, Han\nLu, Laurent Sifre, Marcello Maggioni, Fred Alcober,\nDan Garrette, Megan Barnes, Shantanu Thakoor, Ja-\ncob Austin, Gabriel Barth-Maron, William Wong,\nRishabh Joshi, Rahma Chaabouni, Deeni Fatiha,\nArun Ahuja, Ruibo Liu, Yunxuan Li, Sarah Cogan,\nJeremy Chen, Chao Jia, Chenjie Gu, Qiao Zhang,\nJordan Grimstad, Ale Jakse Hartman, Martin Chad-\nwick, Gaurav Singh Tomar, Xavier Garcia, Evan\nSenter, Emanuel Taropa, Thanumalayan Sankara-\nnarayana Pillai, Jacob Devlin, Michael Laskin, Diego\nde Las Casas, Dasha Valter, Connie Tao, Lorenzo\nBlanco, Adrià Puigdomènech Badia, David Reitter,\nMianna Chen, Jenny Brennan, Clara Rivera, Sergey\nBrin, Shariq Iqbal, Gabriela Surita, Jane Labanowski,\nAbhi Rao, Stephanie Winkler, Emilio Parisotto, Yim-\ning Gu, Kate Olszewska, Yujing Zhang, Ravi Ad-\ndanki, Antoine Miech, Annie Louis, Laurent El\nShafey, Denis Teplyashin, Geoff Brown, Elliot Catt,\nNithya Attaluri, Jan Balaguer, Jackie Xiang, Pi-\ndong Wang, Zoe Ashwood, Anton Briukhov, Al-\nbert Webson, Sanjay Ganapathy, Smit Sanghavi,\nAjay Kannan, Ming-Wei Chang, Axel Stjerngren,\nJosip Djolonga, Yuting Sun, Ankur Bapna, Matthew\nAitchison, Pedram Pejman, Henryk Michalewski,\nTianhe Yu, Cindy Wang, Juliette Love, Junwhan Ahn,\nDawn Bloxwich, Kehang Han, Peter Humphreys,\nThibault Sellam, James Bradbury, Varun Godbole,\nSina Samangooei, Bogdan Damoc, Alex Kaskasoli,\nSébastien M. R. Arnold, Vijay Vasudevan, Shubham\nAgrawal, Jason Riesa, Dmitry Lepikhin, Richard Tan-\nburn, Srivatsan Srinivasan, Hyeontaek Lim, Sarah\nHodkinson, Pranav Shyam, Johan Ferret, Steven\nHand, Ankush Garg, Tom Le Paine, Jian Li, Yu-\njia Li, Minh Giang, Alexander Neitz, Zaheer Abbas,\nSarah York, Machel Reid, Elizabeth Cole, Aakanksha\nChowdhery, Dipanjan Das, Dominika Rogozi´nska,\nVitaly Nikolaev, Pablo Sprechmann, Zachary Nado,\nLukas Zilka, Flavien Prost, Luheng He, Marianne\nMonteiro, Gaurav Mishra, Chris Welty, Josh Newlan,\nDawei Jia, Miltiadis Allamanis, Clara Huiyi Hu,\nRaoul de Liedekerke, Justin Gilmer, Carl Saroufim,\nShruti Rijhwani, Shaobo Hou, Disha Shrivastava,\nAnirudh Baddepudi, Alex Goldin, Adnan Ozturel,\nAlbin Cassirer, Yunhan Xu, Daniel Sohn, Deven-\ndra Sachan, Reinald Kim Amplayo, Craig Swan-\nson, Dessie Petrova, Shashi Narayan, Arthur Guez,\nSiddhartha Brahma, Jessica Landon, Miteyan Patel,\nRuizhe Zhao, Kevin Villela, Luyu Wang, Wenhao\nJia, Matthew Rahtz, Mai Giménez, Legg Yeung,\nHanzhao Lin, James Keeling, Petko Georgiev, Di-\nana Mincu, Boxi Wu, Salem Haykal, Rachel Sapu-\ntro, Kiran Vodrahalli, James Qin, Zeynep Cankara,\nAbhanshu Sharma, Nick Fernando, Will Hawkins,\nBehnam Neyshabur, Solomon Kim, Adrian Hut-\nter, Priyanka Agrawal, Alex Castro-Ros, George\nvan den Driessche, Tao Wang, Fan Yang, Shuo yiin\nChang, Paul Komarek, Ross McIlroy, Mario Luˇci´c,\nGuodong Zhang, Wael Farhan, Michael Sharman,\nPaul Natsev, Paul Michel, Yong Cheng, Yamini\nBansal, Siyuan Qiao, Kris Cao, Siamak Shakeri,\nChristina Butterfield, Justin Chung, Paul Kishan\nRubenstein, Shivani Agrawal, Arthur Mensch, Kedar\nSoparkar, Karel Lenc, Timothy Chung, Aedan Pope,\nLoren Maggiore, Jackie Kay, Priya Jhakra, Shibo\nWang, Joshua Maynez, Mary Phuong, Taylor Tobin,\nAndrea Tacchetti, Maja Trebacz, Kevin Robinson,\nYash Katariya, Sebastian Riedel, Paige Bailey, Ke-\nfan Xiao, Nimesh Ghelani, Lora Aroyo, Ambrose\nSlone, Neil Houlsby, Xuehan Xiong, Zhen Yang,\nElena Gribovskaya, Jonas Adler, Mateo Wirth, Lisa\nLee, Music Li, Thais Kagohara, Jay Pavagadhi, So-\nphie Bridgers, Anna Bortsova, Sanjay Ghemawat,\n20\nZafarali Ahmed, Tianqi Liu, Richard Powell, Vijay\nBolina, Mariko Iinuma, Polina Zablotskaia, James\nBesley, Da-Woon Chung, Timothy Dozat, Ramona\nComanescu, Xiance Si, Jeremy Greer, Guolong Su,\nMartin Polacek, Raphaël Lopez Kaufman, Simon\nTokumine, Hexiang Hu, Elena Buchatskaya, Yingjie\nMiao, Mohamed Elhawaty, Aditya Siddhant, Nenad\nTomasev, Jinwei Xing, Christina Greer, Helen Miller,\nShereen Ashraf, Aurko Roy, Zizhao Zhang, Ada Ma,\nAngelos Filos, Milos Besta, Rory Blevins, Ted Kli-\nmenko, Chih-Kuan Yeh, Soravit Changpinyo, Jiaqi\nMu, Oscar Chang, Mantas Pajarskas, Carrie Muir,\nVered Cohen, Charline Le Lan, Krishna Haridasan,\nAmit Marathe, Steven Hansen, Sholto Douglas, Ra-\njkumar Samuel, Mingqiu Wang, Sophia Austin,\nChang Lan, Jiepu Jiang, Justin Chiu, Jaime Alonso\nLorenzo, Lars Lowe Sjösund, Sébastien Cevey,\nZach Gleicher, Thi Avrahami, Anudhyan Boral,\nHansa Srinivasan, Vittorio Selo, Rhys May, Kon-\nstantinos Aisopos, Léonard Hussenot, Livio Baldini\nSoares, Kate Baumli, Michael B. Chang, Adrià Re-\ncasens, Ben Caine, Alexander Pritzel, Filip Pavetic,\nFabio Pardo, Anita Gergely, Justin Frye, Vinay\nRamasesh, Dan Horgan, Kartikeya Badola, Nora\nKassner, Subhrajit Roy, Ethan Dyer, Víctor Cam-\npos, Alex Tomala, Yunhao Tang, Dalia El Badawy,\nElspeth White, Basil Mustafa, Oran Lang, Ab-\nhishek Jindal, Sharad Vikram, Zhitao Gong, Sergi\nCaelles, Ross Hemsley, Gregory Thornton, Fangxi-\naoyu Feng, Wojciech Stokowiec, Ce Zheng, Phoebe\nThacker, Ça˘glar Ünlü, Zhishuai Zhang, Moham-\nmad Saleh, James Svensson, Max Bileschi, Piyush\nPatil, Ankesh Anand, Roman Ring, Katerina Tsihlas,\nArpi Vezer, Marco Selvi, Toby Shevlane, Mikel Ro-\ndriguez, Tom Kwiatkowski, Samira Daruki, Keran\nRong, Allan Dafoe, Nicholas FitzGerald, Keren\nGu-Lemberg, Mina Khan, Lisa Anne Hendricks,\nMarie Pellat, Vladimir Feinberg, James Cobon-\nKerr, Tara Sainath, Maribeth Rauh, Sayed Hadi\nHashemi, Richard Ives, Yana Hasson, YaGuang\nLi, Eric Noland, Yuan Cao, Nathan Byrd, Le Hou,\nQingze Wang, Thibault Sottiaux, Michela Paganini,\nJean-Baptiste Lespiau, Alexandre Moufarek, Samer\nHassan, Kaushik Shivakumar, Joost van Amers-\nfoort, Amol Mandhane, Pratik Joshi, Anirudh\nGoyal, Matthew Tung, Andrew Brock, Hannah Shea-\nhan, Vedant Misra, Cheng Li, Nemanja Raki´cevi´c,\nMostafa Dehghani, Fangyu Liu, Sid Mittal, Junhyuk\nOh, Seb Noury, Eren Sezener, Fantine Huot, Matthew\nLamm, Nicola De Cao, Charlie Chen, Gamaleldin\nElsayed, Ed Chi, Mahdis Mahdieh, Ian Tenney, Nan\nHua, Ivan Petrychenko, Patrick Kane, Dylan Scand-\ninaro, Rishub Jain, Jonathan Uesato, Romina Datta,\nAdam Sadovsky, Oskar Bunyan, Dominik Rabiej,\nShimu Wu, John Zhang, Gautam Vasudevan, Edouard\nLeurent, Mahmoud Alnahlawi, Ionut Georgescu, Nan\nWei, Ivy Zheng, Betty Chan, Pam G Rabinovitch,\nPiotr Stanczyk, Ye Zhang, David Steiner, Subhajit\nNaskar, Michael Azzam, Matthew Johnson, Adam\nPaszke, Chung-Cheng Chiu, Jaume Sanchez Elias,\nAfroz Mohiuddin, Faizan Muhammad, Jin Miao,\nAndrew Lee, Nino Vieillard, Sahitya Potluri, Jane\nPark, Elnaz Davoodi, Jiageng Zhang, Jeff Stanway,\nDrew Garmon, Abhijit Karmarkar, Zhe Dong, Jong\nLee, Aviral Kumar, Luowei Zhou, Jonathan Evens,\nWilliam Isaac, Zhe Chen, Johnson Jia, Anselm\nLevskaya, Zhenkai Zhu, Chris Gorgolewski, Peter\nGrabowski, Yu Mao, Alberto Magni, Kaisheng Yao,\nJavier Snaider, Norman Casagrande, Paul Sugan-\nthan, Evan Palmer, Geoffrey Irving, Edward Loper,\nManaal Faruqui, Isha Arkatkar, Nanxin Chen, Izhak\nShafran, Michael Fink, Alfonso Castaño, Irene Gian-\nnoumis, Wooyeol Kim, Mikołaj Rybi´nski, Ashwin\nSreevatsa, Jennifer Prendki, David Soergel, Adrian\nGoedeckemeyer, Willi Gierke, Mohsen Jafari, Meenu\nGaba, Jeremy Wiesner, Diana Gage Wright, Yawen\nWei, Harsha Vashisht, Yana Kulizhskaya, Jay Hoover,\nMaigo Le, Lu Li, Chimezie Iwuanyanwu, Lu Liu,\nKevin Ramirez, Andrey Khorlin, Albert Cui, Tian\nLIN, Marin Georgiev, Marcus Wu, Ricardo Aguilar,\nKeith Pallo, Abhishek Chakladar, Alena Repina, Xi-\nhui Wu, Tom van der Weide, Priya Ponnapalli, Car-\noline Kaplan, Jiri Simsa, Shuangfeng Li, Olivier\nDousse, Fan Yang, Jeff Piper, Nathan Ie, Minnie\nLui, Rama Pasumarthi, Nathan Lintz, Anitha Vi-\njayakumar, Lam Nguyen Thiet, Daniel Andor, Pedro\nValenzuela, Cosmin Paduraru, Daiyi Peng, Kather-\nine Lee, Shuyuan Zhang, Somer Greene, Duc Dung\nNguyen, Paula Kurylowicz, Sarmishta Velury, Se-\nbastian Krause, Cassidy Hardin, Lucas Dixon, Lili\nJanzer, Kiam Choo, Ziqiang Feng, Biao Zhang,\nAchintya Singhal, Tejasi Latkar, Mingyang Zhang,\nQuoc Le, Elena Allica Abellan, Dayou Du, Dan McK-\ninnon, Natasha Antropova, Tolga Bolukbasi, Orgad\nKeller, David Reid, Daniel Finchelstein, Maria Abi\nRaad, Remi Crocker, Peter Hawkins, Robert Dadashi,\nColin Gaffney, Sid Lall, Ken Franko, Egor Filonov,\nAnna Bulanova, Rémi Leblond, Vikas Yadav, Shirley\nChung, Harry Askham, Luis C. Cobo, Kelvin Xu,\nFelix Fischer, Jun Xu, Christina Sorokin, Chris Al-\nberti, Chu-Cheng Lin, Colin Evans, Hao Zhou, Alek\nDimitriev, Hannah Forbes, Dylan Banarse, Zora\nTung, Jeremiah Liu, Mark Omernick, Colton Bishop,\nChintu Kumar, Rachel Sterneck, Ryan Foley, Rohan\nJain, Swaroop Mishra, Jiawei Xia, Taylor Bos, Ge-\noffrey Cideron, Ehsan Amid, Francesco Piccinno,\nXingyu Wang, Praseem Banzal, Petru Gurita, Hila\nNoga, Premal Shah, Daniel J. Mankowitz, Alex\nPolozov, Nate Kushman, Victoria Krakovna, Sasha\nBrown, MohammadHossein Bateni, Dennis Duan,\nVlad Firoiu, Meghana Thotakuri, Tom Natan, An-\nhad Mohananey, Matthieu Geist, Sidharth Mudgal,\nSertan Girgin, Hui Li, Jiayu Ye, Ofir Roval, Reiko\nTojo, Michael Kwong, James Lee-Thorp, Christo-\npher Yew, Quan Yuan, Sumit Bagri, Danila Sinopal-\nnikov, Sabela Ramos, John Mellor, Abhishek Sharma,\nAliaksei Severyn, Jonathan Lai, Kathy Wu, Heng-\nTze Cheng, David Miller, Nicolas Sonnerat, Denis\nVnukov, Rory Greig, Jennifer Beattie, Emily Cave-\nness, Libin Bai, Julian Eisenschlos, Alex Korchem-\nniy, Tomy Tsai, Mimi Jasarevic, Weize Kong, Phuong\nDao, Zeyu Zheng, Frederick Liu, Fan Yang, Rui\nZhu, Mark Geller, Tian Huey Teh, Jason Sanmiya,\nEvgeny Gladchenko, Nejc Trdin, Andrei Sozanschi,\nDaniel Toyama, Evan Rosen, Sasan Tavakkol, Lint-\ning Xue, Chen Elkind, Oliver Woodman, John Car-\npenter, George Papamakarios, Rupert Kemp, Sushant\nKafle, Tanya Grunina, Rishika Sinha, Alice Tal-\n21\nbert, Abhimanyu Goyal, Diane Wu, Denese Owusu-\nAfriyie, Cosmo Du, Chloe Thornton, Jordi Pont-\nTuset, Pradyumna Narayana, Jing Li, Sabaer Fatehi,\nJohn Wieting, Omar Ajmeri, Benigno Uria, Tao Zhu,\nYeongil Ko, Laura Knight, Amélie Héliou, Ning\nNiu, Shane Gu, Chenxi Pang, Dustin Tran, Yeqing\nLi, Nir Levine, Ariel Stolovich, Norbert Kalb, Re-\nbeca Santamaria-Fernandez, Sonam Goenka, Wenny\nYustalim, Robin Strudel, Ali Elqursh, Balaji Laksh-\nminarayanan, Charlie Deck, Shyam Upadhyay, Hyo\nLee, Mike Dusenberry, Zonglin Li, Xuezhi Wang,\nKyle Levin, Raphael Hoffmann, Dan Holtmann-\nRice, Olivier Bachem, Summer Yue, Sho Arora,\nEric Malmi, Daniil Mirylenka, Qijun Tan, Christy\nKoh, Soheil Hassas Yeganeh, Siim Põder, Steven\nZheng, Francesco Pongetti, Mukarram Tariq, Yan-\nhua Sun, Lucian Ionita, Mojtaba Seyedhosseini,\nPouya Tafti, Ragha Kotikalapudi, Zhiyu Liu, An-\nmol Gulati, Jasmine Liu, Xinyu Ye, Bart Chrzaszcz,\nLily Wang, Nikhil Sethi, Tianrun Li, Ben Brown,\nShreya Singh, Wei Fan, Aaron Parisi, Joe Stanton,\nChenkai Kuang, Vinod Koverkathu, Christopher A.\nChoquette-Choo, Yunjie Li, TJ Lu, Abe Ittycheriah,\nPrakash Shroff, Pei Sun, Mani Varadarajan, Sanaz Ba-\nhargam, Rob Willoughby, David Gaddy, Ishita Das-\ngupta, Guillaume Desjardins, Marco Cornero, Brona\nRobenek, Bhavishya Mittal, Ben Albrecht, Ashish\nShenoy, Fedor Moiseev, Henrik Jacobsson, Alireza\nGhaffarkhah, Morgane Rivière, Alanna Walton, Clé-\nment Crepy, Alicia Parrish, Yuan Liu, Zongwei\nZhou, Clement Farabet, Carey Radebaugh, Praveen\nSrinivasan, Claudia van der Salm, Andreas Fidje-\nland, Salvatore Scellato, Eri Latorre-Chimoto, Hanna\nKlimczak-Pluci´nska, David Bridson, Dario de Ce-\nsare, Tom Hudson, Piermaria Mendolicchio, Lexi\nWalker, Alex Morris, Ivo Penchev, Matthew Mauger,\nAlexey Guseynov, Alison Reid, Seth Odoom, Lucia\nLoher, Victor Cotruta, Madhavi Yenugula, Dominik\nGrewe, Anastasia Petrushkina, Tom Duerig, Antonio\nSanchez, Steve Yadlowsky, Amy Shen, Amir Glober-\nson, Adam Kurzrok, Lynette Webb, Sahil Dua, Dong\nLi, Preethi Lahoti, Surya Bhupatiraju, Dan Hurt, Ha-\nroon Qureshi, Ananth Agarwal, Tomer Shani, Matan\nEyal, Anuj Khare, Shreyas Rammohan Belle, Lei\nWang, Chetan Tekur, Mihir Sanjay Kale, Jinliang\nWei, Ruoxin Sang, Brennan Saeta, Tyler Liechty,\nYi Sun, Yao Zhao, Stephan Lee, Pandu Nayak, Doug\nFritz, Manish Reddy Vuyyuru, John Aslanides, Nidhi\nVyas, Martin Wicke, Xiao Ma, Taylan Bilal, Ev-\ngenii Eltyshev, Daniel Balle, Nina Martin, Hardie\nCate, James Manyika, Keyvan Amiri, Yelin Kim,\nXi Xiong, Kai Kang, Florian Luisier, Nilesh Tripu-\nraneni, David Madras, Mandy Guo, Austin Waters,\nOliver Wang, Joshua Ainslie, Jason Baldridge, Han\nZhang, Garima Pruthi, Jakob Bauer, Feng Yang, Ri-\nham Mansour, Jason Gelman, Yang Xu, George\nPolovets, Ji Liu, Honglong Cai, Warren Chen, Xi-\nangHai Sheng, Emily Xue, Sherjil Ozair, Adams Yu,\nChristof Angermueller, Xiaowei Li, Weiren Wang, Ju-\nlia Wiesinger, Emmanouil Koukoumidis, Yuan Tian,\nAnand Iyer, Madhu Gurumurthy, Mark Goldenson,\nParashar Shah, MK Blake, Hongkun Yu, Anthony\nUrbanowicz, Jennimaria Palomaki, Chrisantha Fer-\nnando, Kevin Brooks, Ken Durden, Harsh Mehta,\nNikola Momchev, Elahe Rahimtoroghi, Maria Geor-\ngaki, Amit Raul, Sebastian Ruder, Morgan Red-\nshaw, Jinhyuk Lee, Komal Jalan, Dinghua Li, Ginger\nPerng, Blake Hechtman, Parker Schuh, Milad Nasr,\nMia Chen, Kieran Milan, Vladimir Mikulik, Trevor\nStrohman, Juliana Franco, Tim Green, Demis Has-\nsabis, Koray Kavukcuoglu, Jeffrey Dean, and Oriol\nVinyals. 2023. Gemini: A family of highly capable\nmultimodal models.\nSimone Tedeschi, Johan Bos, Thierry Declerck, Jan\nHajiˇc, Daniel Hershcovich, Eduard Hovy, Alexan-\nder Koller, Simon Krek, Steven Schockaert, Rico\nSennrich, Ekaterina Shutova, and Roberto Navigli.\n2023. What’s the meaning of superhuman perfor-\nmance in today’s NLU? In Proceedings of the 61st\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 12471–\n12491, Toronto, Canada. Association for Computa-\ntional Linguistics.\nSerra Sinem Tekiro˘glu, Yi-Ling Chung, and Marco\nGuerini. 2020. Generating counter narratives against\nonline hate speech: Data and strategies. In Proceed-\nings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pages 1177–1190, On-\nline. Association for Computational Linguistics.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, Dan Bikel, Lukas Blecher, Cristian Canton\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu,\nJude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\nCynthia Gao, Vedanuj Goswami, Naman Goyal, An-\nthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\nIsabel Kloumann, Artem Korenev, Punit Singh Koura,\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-\nana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-\ntinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-\nbog, Yixin Nie, Andrew Poulton, Jeremy Reizen-\nstein, Rashi Rungta, Kalyan Saladi, Alan Schelten,\nRuan Silva, Eric Michael Smith, Ranjan Subrama-\nnian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-\nlor, Adina Williams, Jian Xiang Kuan, Puxin Xu,\nZheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,\nMelanie Kambadur, Sharan Narang, Aurelien Ro-\ndriguez, Robert Stojnic, Sergey Edunov, and Thomas\nScialom. 2023. Llama 2: Open foundation and fine-\ntuned chat models.\nKe Tran and Arianna Bisazza. 2019. Zero-shot depen-\ndency parsing with pre-trained multilingual sentence\nrepresentations. In Proceedings of the 2nd Workshop\non Deep Learning Approaches for Low-Resource\nNLP (DeepLo 2019), pages 281–288, Hong Kong,\nChina. Association for Computational Linguistics.\nDaniela Trotta, Raffaele Guarasci, Elisa Leonardelli,\nand Sara Tonelli. 2021.\nMonolingual and cross-\nlingual acceptability judgments with the Italian CoLA\ncorpus. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2021, pages 2929–2940,\n22\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nHenry Tsai, Jason Riesa, Melvin Johnson, Naveen Ari-\nvazhagan, Xin Li, and Amelia Archer. 2019. Small\nand practical BERT models for sequence labeling. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 3632–\n3636, Hong Kong, China. Association for Computa-\ntional Linguistics.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Interna-\ntional Conference on Learning Representations.\nChenhao Wang, Jiachun Li, Yubo Chen, Kang Liu, and\nJun Zhao. 2022. CN-AutoMIC: Distilling Chinese\ncommonsense knowledge from pretrained language\nmodels. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing,\npages 9253–9265, Abu Dhabi, United Arab Emirates.\nAssociation for Computational Linguistics.\nShuohang Wang, Yang Liu, Yichong Xu, Chenguang\nZhu, and Michael Zeng. 2021a. Want to reduce la-\nbeling cost? GPT-3 can help. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2021, pages 4195–4205, Punta Cana, Dominican Re-\npublic. Association for Computational Linguistics.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa\nLiu, Noah A. Smith, Daniel Khashabi, and Hannaneh\nHajishirzi. 2023. Self-instruct: Aligning language\nmodels with self-generated instructions. In Proceed-\nings of the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 13484–13508, Toronto, Canada. Association\nfor Computational Linguistics.\nZirui Wang, Adams Wei Yu, Orhan Firat, and Yuan Cao.\n2021b. Towards zero-label language learning.\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu,\nAdams Wei Yu, Brian Lester, Nan Du, Andrew M.\nDai, and Quoc V Le. 2022a. Finetuned language\nmodels are zero-shot learners. In International Con-\nference on Learning Representations.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,\nBarret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, Ed H.\nChi, Tatsunori Hashimoto, Oriol Vinyals, Percy\nLiang, Jeff Dean, and William Fedus. 2022b. Emer-\ngent abilities of large language models. Transactions\non Machine Learning Research. Survey Certifica-\ntion.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le,\nand Denny Zhou. 2022c. Chain of thought prompt-\ning elicits reasoning in large language models. In\nAdvances in Neural Information Processing Systems.\nPeter West, Chandra Bhagavatula, Jack Hessel, Jena\nHwang, Liwei Jiang, Ronan Le Bras, Ximing Lu,\nSean Welleck, and Yejin Choi. 2022.\nSymbolic\nknowledge distillation: from general language mod-\nels to commonsense models. In Proceedings of the\n2022 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 4602–4625, Seat-\ntle, United States. Association for Computational\nLinguistics.\nChenxi Whitehouse, Monojit Choudhury, and Alham\nAji. 2023. LLM-powered data augmentation for en-\nhanced cross-lingual performance. In Proceedings of\nthe 2023 Conference on Empirical Methods in Natu-\nral Language Processing, pages 671–686, Singapore.\nAssociation for Computational Linguistics.\nSarah Wiegreffe, Jack Hessel, Swabha Swayamdipta,\nMark Riedl, and Yejin Choi. 2022.\nReframing\nhuman-AI collaboration for generating free-text ex-\nplanations. In Proceedings of the 2022 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 632–658, Seattle, United States.\nAssociation for Computational Linguistics.\nBrandon T Willard and Rémi Louf. 2023.\nEffi-\ncient guided generation for llms.\narXiv preprint\narXiv:2307.09702.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers), pages 1112–1122, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nSiwei Wu, Xiangqing Shen, and Rui Xia. 2023. Com-\nmonsense knowledge graph completion via con-\ntrastive pretraining and node clustering. In Findings\nof the Association for Computational Linguistics:\nACL 2023, pages 13977–13989, Toronto, Canada.\nAssociation for Computational Linguistics.\nCanwen Xu, Wangchunshu Zhou, Tao Ge, Ke Xu, Julian\nMcAuley, and Furu Wei. 2021. Blow the dog whistle:\nA Chinese dataset for cant understanding with com-\nmon sense and world knowledge. In Proceedings of\n23\nthe 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 2139–2145,\nOnline. Association for Computational Linguistics.\nLiang Xu, Hai Hu, Xuanwei Zhang, Lu Li, Chenjie Cao,\nYudong Li, Yechen Xu, Kai Sun, Dian Yu, Cong\nYu, Yin Tian, Qianqian Dong, Weitang Liu, Bo Shi,\nYiming Cui, Junyi Li, Jun Zeng, Rongzhao Wang,\nWeijian Xie, Yanting Li, Yina Patterson, Zuoyu Tian,\nYiwen Zhang, He Zhou, Shaoweihua Liu, Zhe Zhao,\nQipeng Zhao, Cong Yue, Xinrui Zhang, Zhengliang\nYang, Kyle Richardson, and Zhenzhong Lan. 2020.\nCLUE: A Chinese language understanding evalua-\ntion benchmark. In Proceedings of the 28th Inter-\nnational Conference on Computational Linguistics,\npages 4762–4772, Barcelona, Spain (Online). Inter-\nnational Committee on Computational Linguistics.\nHitomi Yanaka and Koji Mineshima. 2022. Compo-\nsitional evaluation on Japanese textual entailment\nand similarity. Transactions of the Association for\nComputational Linguistics, 10:1266–1284.\nYinfei Yang, Yuan Zhang, Chris Tar, and Jason\nBaldridge. 2019. PAWS-X: A cross-lingual adversar-\nial dataset for paraphrase identification. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 3687–3692, Hong\nKong, China. Association for Computational Linguis-\ntics.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,\nWilliam Cohen, Ruslan Salakhutdinov, and Christo-\npher D. Manning. 2018. HotpotQA: A dataset for\ndiverse, explainable multi-hop question answering.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2369–2380, Brussels, Belgium. Association for Com-\nputational Linguistics.\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,\nThomas L. Griffiths, Yuan Cao, and Karthik R\nNarasimhan. 2023a. Tree of thoughts: Deliberate\nproblem solving with large language models.\nIn\nThirty-seventh Conference on Neural Information\nProcessing Systems.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik R Narasimhan, and Yuan Cao.\n2023b. React: Synergizing reasoning and acting\nin language models. In The Eleventh International\nConference on Learning Representations.\nJiacheng Ye, Jiahui Gao, Qintong Li, Hang Xu, Jiangtao\nFeng, Zhiyong Wu, Tao Yu, and Lingpeng Kong.\n2022.\nZeroGen: Efficient zero-shot learning via\ndataset generation. In Proceedings of the 2022 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 11653–11669, Abu Dhabi, United\nArab Emirates. Association for Computational Lin-\nguistics.\nDa Yin, Hritik Bansal, Masoud Monajatipoor, Liu-\nnian Harold Li, and Kai-Wei Chang. 2022. GeoM-\nLAMA: Geo-diverse commonsense probing on multi-\nlingual pre-trained language models. In Proceedings\nof the 2022 Conference on Empirical Methods in Nat-\nural Language Processing, pages 2039–2055, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nAnn Yuan, Daphne Ippolito, Vitaly Nikolaev, Chris\nCallison-Burch,\nAndy Coenen,\nand Sebastian\nGehrmann. 2021. Synthbio: A case study in faster\ncuration of text datasets. In Thirty-fifth Conference\non Neural Information Processing Systems Datasets\nand Benchmarks Track (Round 2).\nRowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin\nChoi. 2018. SWAG: A large-scale adversarial dataset\nfor grounded commonsense inference. In Proceed-\nings of the 2018 Conference on Empirical Methods in\nNatural Language Processing, pages 93–104, Brus-\nsels, Belgium. Association for Computational Lin-\nguistics.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali\nFarhadi, and Yejin Choi. 2019. HellaSwag: Can a ma-\nchine really finish your sentence? In Proceedings of\nthe 57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 4791–4800, Florence,\nItaly. Association for Computational Linguistics.\nHao Zhang, Youlin Wu, Junyu Lu, Zewen Bai, Jiang-\nming Wu, Hongfei Lin, and Shaowu Zhang. 2023a.\nZBL2W at SemEval-2023 task 9: A multilingual\nfine-tuning model with data augmentation for tweet\nintimacy analysis. In Proceedings of the 17th Interna-\ntional Workshop on Semantic Evaluation (SemEval-\n2023), pages 770–775, Toronto, Canada. Association\nfor Computational Linguistics.\nHongming Zhang, Daniel Khashabi, Yangqiu Song, and\nDan Roth. 2020a. Transomcs: From linguistic graphs\nto commonsense knowledge. In Proceedings of the\nTwenty-Ninth International Joint Conference on Arti-\nficial Intelligence, IJCAI-20, pages 4004–4010. Inter-\nnational Joint Conferences on Artificial Intelligence\nOrganization. Main track.\nHongming Zhang, Xin Liu, Haojie Pan, Yangqiu Song,\nand Cane Wing-Ki Leung. 2020b. Aser: A large-\nscale eventuality knowledge graph.\nIn Proceed-\nings of The Web Conference 2020, WWW ’20, page\n201–211, New York, NY, USA. Association for Com-\nputing Machinery.\nSheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng\nGao, Kevin Duh, and Benjamin Van Durme. 2018.\nRecord: Bridging the gap between human and ma-\nchine commonsense reading comprehension.\nXiang Zhang, Senyu Li, Bradley Hauer, Ning Shi, and\nGrzegorz Kondrak. 2023b.\nDon’t trust ChatGPT\nwhen your question is not in English: A study of\nmultilingual abilities and types of LLMs. In Proceed-\nings of the 2023 Conference on Empirical Methods\n24\nin Natural Language Processing, pages 7915–7927,\nSingapore. Association for Computational Linguis-\ntics.\nZiyin Zhang, Yikang Liu, Weifang Huang, Junyu Mao,\nRui Wang, and Hai Hu. 2023c. Mela: Multilingual\nevaluation of linguistic acceptability.\nA\nDetails of the Experimental Settings\nWe used mBERT (Devlin et al., 2019), XLM-\n100 (Conneau and Lample, 2019), XLM-R (Con-\nneau et al., 2020), and mDeBERTa-v3 (He et al.,\n2023) as encoder-based multilingual LMs, Llama2-\n70B (Touvron et al., 2023), GPT-3.5 (Ouyang et al.,\n2022), and GPT-4 (OpenAI et al., 2024) as decoder-\nbased multilingual LMs for the experiments. Ta-\nble 9 shows the details of the LMs. Encoder-based\nLMs were fine-tuned following the settings in Ta-\nble 10. Decoder-based LMs inferred with 0-shot\nand 3-shot settings12 with a fixed seed value. For\nGPT-3.5 and GPT-4, top_p and temperature were\nset to 0 to achieve as deterministic outputs as possi-\nble. For Llama2-70B, output was generated greedy,\nand outlines (Willard and Louf, 2023) were used to\nfix the output format.\nB\nQualitative Analysis of mCSQA\nTable 11 shows examples of mCSQAs for each\nlanguage. The examples in Table 11 are accom-\npanied by English translations using DeepL13 to\navoid arbitrary translation.\nB.1\nCan Multilingual LMs Take into Account\nLanguage-specific Knowledge?\nCase study\nWhen we examine some cases in Ta-\nble 11, such as the examples from the Dutch Hard\nset and the Russian Hard set, we find that the En-\nglish translations contain duplications among the\nquestion choices. However, these duplications arise\nnot from differences in tense or conjugation, but\nfrom semantic differences unique to each language,\nwhich a native speaker, equipped with language-\nspecific knowledge and common sense, could eas-\nily distinguish. Furthermore, in the case of the Ger-\nman Easy sets, knowledge of Germany’s unique\neducation system is required, which might be chal-\nlenging for those unfamiliar with it. Yet, for Ger-\nman speakers, it is common knowledge that such\n12In the 3-shot setting, the examples were selected randomly\nfrom the training data and included both Easy and Hard sets.\n13We are using DeepL Pro (https://www.deepl.com/\ntranslator), therefore, the copyright of the translations be-\nlongs to us. (https://www.deepl.com/pro-license)\nType\nModel Name\nHuggingFace / OpenAI API\nEncoder\nmBERT-cased\nbert-base-multilingual-cased\nmBERT-uncased bert-base-multilingual-uncased\nXLM-100\nxlm-mlm-100-1280\nXLM-RBASE\nxlm-roberta-base\nXLM-RLARGE\nxlm-roberta-large\nmDeBERTa-v3\nmicrosoft/mdeberta-v3-base\nDecoder\nLlama2-70B\nmeta-llama/Llama-2-70b-chat-hf\nGPT-3.5\ngpt-3.5-turbo-1106\nGPT-4\ngpt-4-1106-preview\nTable 9: Details of the LMs for the experiments.\nHyper-parameter\nValue\nBatch Size\n64\nLearning Rate\n2e-5, 3e-5, 5e-5\nSeed\n42\nEarly Stopping\n3\nWarmup Ratio\n0.1\nMax Sequence Length\n128\nTable 10: The hyper-parameters used in the experiment,\nand others, were set to default settings. The implemen-\ntation used Transformers (Wolf et al., 2020).\neducation systems, such as the Abitur14 related to\nthe Gymnasium15, making it answerable for those\nknowledgeable in German. This demonstrates that\nmultilingual LMs are capable of generating ques-\ntions that include the kind of language-specific\nknowledge and common sense that a native speaker\nwould possess.\nThe effectiveness of the CSQA style QA\nWhen\nexamining the Japanese Hard set in Table 10, all\nthe choices translate into the names of seafood in\nEnglish, which does not match the context of a fe-\nmale singer mentioned in the question. Japanese\nnative speakers would normally recognize them as\nseafood names too, making it seem at first glance\nthat there is no correct answer. However, the cor-\nrect choice, ‘あゆ’ (ayu), when pronounced in\nJapanese, is read as ’ayu’. This pronunciation is\nwidely known across Japan as the nickname for the\nfamous singer ‘浜崎あゆみ’ (Ayumi Hamasaki)16,\nmaking it a plausible choice even though it’s not\nstrictly correct. It allows for a satisfactory selection\nby Japanese native speakers with language-specific\nknowledge, common sense, and cultural awareness,\nand is not answerable by English translation only.\nIn Japan, nicknames are often derived from abbre-\n14https://en.wikipedia.org/wiki/Abitur\n15https://en.wikipedia.org/wiki/Gymnasium_\n(school)\n16https://en.wikipedia.org/wiki/Ayumi_Hamasaki\n25\nviations of their names or can suggest the names of\nobjects. The distractor ‘Wakame’ is known as the\nname of a character from the long-running, famous\nanime ‘Sazae-san’17 but not as a singer, thus serv-\ning its purpose as a distractor in this question ef-\nfectively. Similarly, if there were a choice like ‘い\nくら’ (common meaning: red caviar; pronounced:\nikura), the plausibility of choice in this question\nmight have been divided. Recently, ‘ikura’18 has\nbecome a popular name, associated with a member\nof ‘Yoasobi’19, a popular artist group among young\npeople. Adding such a choice would confuse the\nchoice of the correct answer because both choices\nare plausible, so it would not serve effectively as\na distractor. This case shows that the choices can\ndefine the scope of common sense, thus making\nthe question effective in evaluating common sense\naccurately.\nB.2\nThe Relationship between Knowledge,\nCulture, Commonsense, and Social Bias\nB.2.1\nWhat is the Commonsense?\nAs can be seen from Table 11 and the discussions\nin section B.1, language-specific common sense is\nclosely related to knowledge and culture. The Con-\nceptNet used in this study does not limit the scope\nof common sense and deals with a wide range of\ncommon sense, enabling the inclusion of questions\nfrom various backgrounds into mCSQA, following\nthe same trend as CSQA and JCSQA.\nGenerally, commonsense not based on the spe-\ncific culture or knowledge of a language is likely\nto be a common understanding across all lan-\nguages, making such problems potentially answer-\nable through the language-transfer ability of mul-\ntilingual LMs. However, as shown in Table 1, the\ngranularity of actions, events, and behaviors differs\nby language, which can be considered to be influ-\nenced by the cultural background of the language\narea.\nThis study focuses on language-specific com-\nmon sense that cannot be addressed by translations\nof datasets from other languages, and the culture\nand knowledge included in them are shared among\nnative speakers. Therefore, answering questions\nthat require language-specific backgrounds neces-\nsitates a certain level of knowledge and culture\nspecific to each language. However, content that\n17https://en.wikipedia.org/wiki/Sazae-san\n18https://en.wikipedia.org/wiki/Lilas_Ikuta\n19https://en.wikipedia.org/wiki/Yoasobi\nis too specialized falls outside the scope of com-\nmon sense, and common sense and backgrounds\nvary among individuals. Therefore, we emphasize\nthe precision of coverage in the manual question\nquality verification steps and employ a majority\nvote baseline to avoid overly relying on specific\nknowledge or culture.\nIn this way, questions were created that have\nlanguage-specific common sense which is general\nfor native speakers but not too specialized. If there\nwas a need to create questions asking for knowl-\nedge specialized in specific fields, other knowl-\nedge bases such as ATOMIC (Sap et al., 2019a),\nand CCSK (Nguyen et al., 2023) could be used.\nHowever, this study focused on multilingual perfor-\nmance, deeming ConceptNet appropriate for mC-\nSQA.\nB.2.2\nIs Commonsense Social Bias?\nSince commonsense includes implicit cognition, it\nmay contain social and cultural biases, and some\nmethods for the removal of explicit and implicit\nsocial biases have been proposed (Sap et al., 2020;\nField and Tsvetkov, 2020; Huang et al., 2021; Lent\nand Søgaard, 2021; Emelin et al., 2021; Bauer et al.,\n2023).\nSocial Chemistry 101 (Forbes et al., 2020),\nBBQ (Parrish et al., 2022), and SODAPOP (An\net al., 2023) have been proposed for identifying\nbiases within models or for bias detection using\nLMs. However, it remains challenging to address\nsituations where biased thinking may only emerge\nwhen considering multiple-choice QA, where bias\ndoes not occur in isolation.\nThe definition of bias and common sense\nchanges over time and varies from society to so-\nciety, and what is considered common sense can\nshift to bias (Lee et al., 2023). Therefore, regular\nupdates to the commonsense reasoning datasets are\nnecessary. Our method for generating common-\nsense reasoning task datasets using LMs allows\nfor low-cost update operations, making it possible\nto adapt to the changing boundaries between com-\nmon sense and bias over time. However, this does\nnot fundamentally address the inclusion of bias\nin datasets. Moreover, such issues require a deep\nchain of semantic thinking for resolution, making\nfiltering based on textual information inappropriate.\nTherefore, it is necessary to develop methods to\nremove potential biases in commonsense reasoning\ntask datasets in future work.\n26\nC\nDiscoveries about the LMs Capabilities\nC.1\nCan LMs Create Questions including\nCommonsense?\nGeneration capability\nCommonGen (Lin et al.,\n2020) is one of the commonsense reasoning\ndatasets that evaluates whether it is possible to cre-\nate commonsense sentences from a given set of\nkeywords. According to the leaderboard of Com-\nmonGen20, the performance of GPT-3.5 used in\nour dataset creation demonstrates a capability for\ngenerating commonsense sentences comparable to\nthose written by humans. However, there is still\nroom for improvement in aspects such as word\norder. Therefore, we introduced refinement steps\nto encourage corrections in word order and other\nerrors. Since language models have high perfor-\nmance in Grammar Error Correction (GEC) (Loem\net al., 2023; Sottana et al., 2023; Fang et al., 2023;\nCoyne et al., 2023; Kaneko and Okazaki, 2023;\nKwon et al., 2023), combining sentence genera-\ntion from keywords with GEC capabilities in a\npipeline helps to compensate for the weaknesses\nof language models. We believe that the quality of\nmCSQA questions is at least not inferior to those\ncreated by crowd-workers. The capability of mul-\ntilingual LMs to create commonsense sentences\nfrom given keywords has also been demonstrated\nin the Korean CommonGen (Seo et al., 2022), indi-\ncating that it is possible to generate commonsense\nsentences multilingually.\nEnsuring the quality of questions\nIn this study,\nwe have created commonsense reasoning dataset\nquestions using keywords extracted from Concept-\nNet. Therefore, the language-specific knowledge\nand commonsense for each language are guaran-\nteed by ConceptNet. Moreover, the LM creates\nquestions following the given instructions through\nits emergent capabilities from each keyword. To\nenhance the language-specific performance of the\nmultilingual LM for each language, we have cre-\nated prompts for each language in this study. As\ncan be seen from the discussion in section B.1\nand Table 11, it has become possible to gener-\nate questions that possess language-specific knowl-\nedge. One of the reasons for the capability to create\nquestions with language-specific knowledge may\nbe attributed to the training data of the LM. For ex-\nample, Wikipedia, one of the common training data\nfor LMs, has each language which contains descrip-\n20https://github.com/allenai/CommonGen-Eval\ntions of knowledge unique to that language, so by\nposing questions in each language, it is thought that\nknowledge specific to each language is invoked, en-\nabling the generation of questions based on the\nknowledge of each language. However, this is a\nhypothesis, and further analysis will be necessary\nfor verification in future work. Moreover, we have\nadded distractors in addition to the keywords used\nfor generating the question, which means that even\nif a question can be generated, it may not neces-\nsarily be answerable. Furthermore, questions that\ncannot be answered have been removed, thus en-\nsuring the difficulty and answerability of the QA.\nC.2\nMultilingual Capabilities\nIs polyglot template effective?\nWe translated\nthe prompt to use question generation for each lan-\nguage and tuned it to convey the same meaning in\neach language in Section D aimed to emergence\nthe language-specific knowledge. However, it is\nknown that current generative LMs have mainly\ntrained on English, which is better performance\nfor queries made in English. However, several\nstudies (Ahn et al., 2022; Shi et al., 2023; Wei\net al., 2022b; Awasthi et al., 2023; Kasai et al.,\n2023; Jin et al., 2023) show enough performance\neven if multilingual queries. Note that the reported\nperformance focuses on the ability to answer spe-\ncific tasks on benchmarks and does not evaluate\nthe emergent multilingual ability, especially ques-\ntion generation. Nevertheless, Whitehouse et al.\n(2023) shows that the text generation capability be-\nyond English. As shown in Table 10, we were able\nto generate questions containing language-specific\nknowledge from the given keywords as intended\nby using prompts translated into each language.\nWe were able to generate questions that require\ndeep reasoning, including cultural backgrounds\nand language-specific pronunciation information\nas shown in Section B.1. Therefore, we conclude\nthat using prompts tailored for each language is\neffective.\nIs GPT-3.5 Multilingual LM?\nYes, some stud-\nies (Lai et al., 2023; Armengol-Estapé et al., 2022;\nZhang et al., 2023b) have indeed examined mul-\ntilingual performance, and the training data also\nincludes multilingually21. Therefore, the multilin-\ngual capabilities of GPT-3.5, GPT-4, and Llama\nused in our experiment have also been evalu-\n21https://github.com/openai/gpt-3/tree/master/\ndataset_statistics\n27\nated (Ahuja et al., 2023; Schott et al., 2023; Chen\net al., 2024), leading us to consider these as mul-\ntilingual LMs. However, they still rely predomi-\nnantly on information from Western norms (Cao\net al., 2023; Arora et al., 2023; Havaldar et al.,\n2023), making this issue an ongoing challenge to\nbe addressed in the future.\nExhortation to multilingual instruction-tuning\ndataset.\nInstruction-tuning (Wei et al., 2022a;\nLongpre et al., 2023; Chung et al., 2022; Wang\net al., 2023) can enhance the quality of LMs,\ne.g. ability to follow instructions and NLU per-\nformance. However, in Section 2, the current multi-\nlingual datasets include those created through trans-\nlation, which means that instruction-tuning using\nsuch data may not lead to the acquisition of data\nbias or language-specific knowledge. Given these\nconsiderations, the multilingual instruction-tuning\ndata (Kew et al., 2023; Singh et al., 2024) proposed\nrecently often utilize datasets created through trans-\nlation, leading to the occurrence of the aforemen-\ntioned issues to a considerable extent.\nConse-\nquently, the effectiveness of such instruction-tuning\nmay be diminished. For commonsense reasoning\ntasks in multilingual instruction-tuning datasets,\nthey sometimes use X-CSQA (Lin et al., 2021).\nHowever, since it cannot handle language-specific\nknowledge or commonsense effectively, it is prefer-\nable to use data created from scratch, like mCSQA.\nCurrently, due to data resource issues, reliance on\ntranslated data is inevitable, but we hope that in\nthe future, it will be replaced by language-specific\ndata.\nC.3\nHard Sets are Truly Hard?\nThe Hard sets consist of questions that the LM\nused for question creation could not answer, thus\nreflecting the characteristics of that LM. However,\ndespite the influence of specific LM’s character,\na performance decline in the Hard sets compared\nto the Easy sets was observed across all models.\nTherefore, while the strict division of sets depends\non the model, it has become clear that there is\na similar trend across LMs as a whole. For this\nreason, scoring is conducted without distinguishing\nbetween Easy and Hard, using a total score for\nthe entire set, which allows for the absorption of\ndifferences due to the models.\nC.4\nGeneration Bias and Annotation Artifacts\nIt has been pointed out that datasets created by LMs\ncontain generation bias (Omura et al., 2020; Zellers\net al., 2019; Tamborrino et al., 2020), and those cre-\nated by crowd-workers include specific patterns\n(Annotation Artifacts) (Gururangan et al., 2018;\nChen et al., 2019; Omura et al., 2020). Annotation\nartifacts, in particular, have been noted in natural\nlanguage inference tasks such as MNLI (Williams\net al., 2018) and SNLI (Bowman et al., 2015),\nwhere choices can be easily distinguished by super-\nficial words like “not”.\nHowever, Tamborrino et al. (2020) show that the\nimpact of Annotation Artifacts is not present in the\nCSQA task. Similarly, in this study, we have sep-\narated question generation ability and answering\nability during the question generation process and\nshuffled the options, so there are no clues included\nin the dataset. Moreover, we create Hard sets, even\nif such biased questions existed, the evaluation is\nconducted without these biases, allowing for an\nevaluation that removes these biases.\nD\nPrompts for Creating mCSQA\nThe prompts used for creating mCSQA are pre-\nsented as follows: English in Table 12, Japanese in\nTable 13, Chinese in Table 14, German in Table 15,\nPortuguese in Table 16, Dutch in Table 17, French\nin Table 18 and Russian in Table 19.\nIn each prompt template, the words within\nthe curly brackets are replaced with data-specific\nterms22 before input to the LM.\nFurthermore, as discussed in Section C.2, each\ntemplate was translated exactly to elicit language-\nspecific knowledge of each language. The trans-\nlations were carried out using both GPT-3.5 and\nDeepL to ensure there were no semantic differ-\nences, with manual fixing applied as needed. We\nuse the OpenAI API’s JSON mode23 has facilitated\nthe retrieval of generation results.\nOur findings as a tip, when inputting structured\ndata such as keywords, doing so in a format similar\nto a programming code like list type, allows us\nto obtain results that more following the prompt\ninstructions. This improvement can be attributed\nto the LM’s learning to enhance coding abilities,\nwhich is believed to have improved its recognition\ncapabilities.\n22https://peps.python.org/pep-0498/\n23https://platform.openai.com/docs/guides/\ntext-generation/json-mode\n28\nLang.\nQuestion\nChoices\nCorrect\nDistractors\nAdditional Distractors\nEN Easy If a cat is feeling irritated, what might it do?\nscratch if annoy\nlook out window\nfish with paw\nchase a toy\nnap in the sun\nHard Which animal is known for its playful behavior and\nagile movements?\nmonkey\njellyfish\nlemur\norangutan\ngorilla\nJA Easy 音を聞き分けるためには何をしますか？\n耳を澄ます\n学習する\n書き取る\n実践する\n経験する\n(What do you do to listen to the sounds?)\n(Listen carefully)\n(Learn)\n(Write)\n(Practice)\n(Experience)\nHard 日本の女性歌手で、自身の楽曲の作詞・作曲\nも手がける人気アーティストは誰ですか？\nあゆ\nどじょう\nわかめ\nうなぎ\nさけ\n(Which popular Japanese female singer also writes\nlyrics and composes her own songs?)\n(Sweetfish)\n(Loach)\n(Wakame)\n(Eel)\n(Salmon)\nZH Easy 你在考试前应该做什么？\n回家温习\n聊天\n作弊\n健身\n看电影\n(What should you do before your exam?)\n(Go home and\nstudy)\n(Chatting)\n(Cheat)\n(Work out)\n(Watch films)\nHard 在感情关系中，最令人痛苦的事情是什么？\n被甩\n花大钱\n心碎\n找到真爱\n实现梦想\n(What’s the most excruciating thing about being in\na relationship?)\n(Getting\ndumped)\n(Spending a\nlot of money)\n(Getting your\nheart broken)\n(Finding true love)\n(Realising your\ndreams)\nDE Easy Welche Art von weiterführender Schule bereitet\nSchüler auf das Abitur vor?\ngymnasium\ngesamtschule\nfachoberschule\nberufsschule\nrealschule\n(What type of secondary school prepares students\nfor the Abitur?)\n(grammar school) (comprehensive\nschool)\n(technical seco-\nndary school)\n(vocational school) (secondary school)\nHard Was ist die richtige Bezeichnung für das langsame\nAbwärtsbewegen auf einer schiefen Ebene?\nhinabgleiten\nhinabfliegen\ndahinab\nhinabtauchen\nhinabschweben\n(What is the correct term for moving slowly down-\nwards on an inclined plane?)\n(slide down)\n(fly down)\n(descend)\n(dive down)\n(float down)\nPT Easy Como demonstrar afeto a um animal de estimação?\nfazer carinho\nalegrar a vida\npentelhar\nabraçar\ndar um presente\n(How do you show affection to a pet?)\n(cuddle)\n(combing)\n(brighten up life)\n(cuddle)\n(give a gift)\nHard Qual a ação que um coelho pode fazer para se mover\nrapidamente?\npular\norientando\nsegurar\nesperar\ncorrer\n(What action can a rabbit do to move quickly?)\n(jump)\n(guiding)\n(hold)\n(wait)\n(run)\nNL Easy Kunt u mij vertellen wat gokken is?\nkansspel\ngelijkspel\nsteekspel\nvuurspel\nwedstrijd\n(Can you tell me what gambling is?)\n(game of chance)\n(draw)\n(joust)\n(match)\n(fire game)\nHard Kunt u uitleggen wat een veelvoorkomend begrip is\ndat verwijst naar iets wat algemeen geaccepteerd of\nverspreid is in een samenleving?\ngemeengoed\ngemeenschap\ngemeenplaats\ngezamenlijk\ngebruikelijk\n(Can you explain what is a common term that refers\nto something commonly accepted or widespread in\na society?)\n(common)\n(community)\n(commonplace)\n(common)\n(common)\nFR Easy Quelle unité de temps correspond à une période de\nvingt-quatre heures ?\njour\ndécade\nsiècle\nannée\nmois\n(What unit of time corresponds to a twenty-four\nhour period?)\n(day)\n(decade)\n(century)\n(year)\n(month)\nHard Quelle partie du corps utilise-t-on pour saisir des\nobjets de petite taille ?\ndoigt\nannulaire\nauriculaire\nmajeur\nindex\n(What part of the body is used to pick up small\nobjects?)\n(finger)\n(ring finger)\n(little finger)\n(middle finger)\n(index finger)\nRU Easy Какое время года обычно связывается с\nпраздниками Нового года и Рождества?\nзима\nвесна\nосень\nлетний сезон\nлето\n(What time of year is usually associated with the\nholidays of New Year’s Eve and Christmas?)\n(winter)\n(spring)\n(fall)\n(summer season)\n(summer)\nHard Какой звук издает довольный кот?\nурчание\nзаурчать\nпроурчать\nмурлыкать\nгромко урчать\n(What sound does a contented cat make?)\n(purr)\n(rumble)\n(purr)\n(purr)\n(purr)\nTable 11: The examples of mCSQA. The English translations are all machine-translated by DeepL. The translated\nresults sometimes are aggregated into one English word due to ignoring source language-specific subtle meaning\ndifferences caused by machine translation. This aggregation has also been observed in X-CSQA, which was created\nusing machine translation of CSQA. Hence, X-CSQA could not evaluate fine-grained, language-specific knowledge\nfor each language, but mCSQA can evaluate it because it is created from scratch for each language.\n29\nSteps\nPrompt (English)\nCreate\nquestion\nsentences\nPlease create a multiple-choice question with the following conditions:\n(a) The only correct answer is [\"{correct}\"].\n(b) The incorrect answers are [\"{distractor1}\", \"{distractor2}\"].\n(c) Do not use the words [\"{correct}\", \"{distractor1}\", \"{distractor2}\"] in the question.\n(d) Avoid using superficial information, such as character count.\n(e) The question ends with a question mark (?).\n(f) It should be an objective question that can be sufficiently answered with common sense knowledge alone.\n(g) The question must be a simple and short sentence consisting of only one sentence.\nQuestion:\nRefine\nquestion\nsentences\nIf the original sentence is semantically and grammatically correct, repeat it;\nif it is unnatural, please rewrite it into a correct and fluent sentence.\n{question}\nAdd\nadditional\ndistractors\nPlease only add two plausible and natural choices and save them in {’additional_choice’:[]}.\n[\"{choice1}\", \"{chioce2}\", \"{choice3}\"]\nVerify\nQualities\nPlease select only one alphabet as the answer from the Answer Choices\nand save it in the format: {’answer’: selected_answer}.\nQ: {question}\nAnswer Choices: (A) {choice_a} (B) {choice_b} (C) {choice_c} (D) {choice_d} (E) {choice_e}\nTable 12: The prompt templates used to create the mCSQA in the English version.\nSteps\nPrompt (Japanese)\nCreate\nquestion\nsentences\n以下の条件を満たす選択肢付きのクイズ問題を作成してください。\n(a) 正解は[\"{correct}\"]のみです。\n(b) 不正解は[\"{distractor1}\", \"{distractor2}\"]です。\n(c) 問題文に[\"{correct}\", \"{distractor1}\", \"{distractor2}\"]という単語を使わないでください。\n(d) 文字数などの表面的な情報の使用を避けてください。\n(e) 問題文は疑問符（？）で終わります。\n(f) 一般常識だけで十分に答えられる客観的な問題である必要があります。\n(g) 問題文は一文のみから成る単純で短い文でなければなりません。\n問題：\nRefine\nquestion\nsentences\n元の文が意味的・文法的に正しい場合は繰り返す、\n不自然な場合は正しい流暢な文へ書き換えてください。\n{question}\nAdd\nadditional\ndistractors\nもっともらしい自然な選択肢を2つだけ追加し、\nそれらを{’additional_choice’:[]}に保存してください。\n[\"{choice1}\", \"{chioce2}\", \"{choice3}\"]\nVerify\nQualities\nAnswer Choicesから解答となるアルファベットを１つだけ選び、\n次の形式で保存してください：{’answer’: selected_answer}。\nQ: {question}\nAnswer Choices: (A) {choice_a} (B) {choice_b} (C) {choice_c} (D) {choice_d} (E) {choice_e}\nTable 13: The prompt templates used to create the mCSQA in the Japanese version.\n30\nSteps\nPrompt (Chinese)\nCreate\nquestion\nsentences\n请根据以下条件创建一个多项选择题：\n(a) 唯一正确答案是[\"{correct}\"]。\n(b) 错误答案是[\"{distractor1}\", \"{distractor2}\"]。\n(c) 问题中不得使用[\"{correct}\", \"{distractor1}\", \"{distractor2}\"]这些词。\n(d) 避免使用表面信息，如字符数。\n(e) 问题以问号(?)结束。\n(f) 它应该是一个客观的问题，仅凭常识就能充分回答。\n(g) 问题必须是一个简单且短的句子，仅由一句话组成。\n问题：\nRefine\nquestion\nsentences\n如果原句在语义和语法上正确，请重复它；如果不自然，请将其改写为正确流畅的句子。\n{question}\nAdd\nadditional\ndistractors\n请只添加两个合理且自然的选择，并将它们保存在{’additional_choice’:[]} 中。\n[\"{choice1}\", \"{chioce2}\", \"{choice3}\"]\nVerify\nQualities\n请从Answer Choices中仅选择一个字母作为答案，并以以下格式保存：{’answer’: selected_answer}。\nQ: {question}\nAnswer Choices: (A) {choice_a} (B) {choice_b} (C) {choice_c} (D) {choice_d} (E) {choice_e}\nTable 14: The prompt templates used to create the mCSQA in the Chinese version.\nSteps\nPrompt (German)\nCreate\nquestion\nsentences\nBitte erstellen Sie eine Multiple-Choice-Frage mit folgenden Bedingungen:\n(a) Die einzig richtige Antwort ist [\"{correct}\"].\n(b) Die falschen Antworten sind [\"{distractor1}\", \"{distractor2}\"].\n(c) Verwenden Sie in der Frage nicht die Wörter [\"{correct}\", \"{distractor1}\", \"{distractor2}\"].\n(d) Vermeiden Sie oberflächliche Informationen, wie z.B. die Zeichenanzahl.\n(e) Die Frage endet mit einem Fragezeichen (?).\n(f) Es sollte eine objektive Frage sein, die allein mit Allgemeinwissen ausreichend beantwortet werden kann.\n(g) Die Frage muss ein einfacher und kurzer Satz bestehend aus nur einem Satz sein.\nFrage:\nRefine\nquestion\nsentences\nWenn der Originalsatz semantisch und grammatikalisch korrekt ist, wiederholen Sie ihn;\nwenn er unnatürlich ist, schreiben Sie ihn bitte in einen korrekten und flüssigen Satz um.\n{question}\nAdd\nadditional\ndistractors\nBitte fügen Sie nur zwei plausible und natürliche Optionen hinzu\nund speichern Sie diese in {’additional_choice’:[]}.\n[\"{choice1}\", \"{chioce2}\", \"{choice3}\"]\nVerify\nQualities\nBitte wählen Sie nur einen Buchstaben als Antwort aus den Answer Choices aus\nund speichern Sie ihn im Format: {’answer’: selected_answer}.\nQ: {question}\nAnswer Choices: (A) {choice_a} (B) {choice_b} (C) {choice_c} (D) {choice_d} (E) {choice_e}\nTable 15: The prompt templates used to create the mCSQA in the German version.\n31\nSteps\nPrompt (Portuguese)\nCreate\nquestion\nsentences\nPor favor, crie uma pergunta de múltipla escolha com as seguintes condições:\n(a) A única resposta correta é [\"{correct}\"].\n(b) As respostas incorretas são [\"{distractor1}\", \"{distractor2}\"].\n(c) Não use as palavras [\"{correct}\", \"{distractor1}\", \"{distractor2}\"] na pergunta.\n(d) Evite usar informações superficiais, como a contagem de caracteres.\n(e) A pergunta termina com um ponto de interrogação (?).\n(f) Deve ser uma pergunta objetiva que pode ser suficientemente\nrespondida apenas com conhecimento de senso comum.\n(g) A pergunta deve ser uma frase simples e curta, consistindo de apenas uma frase.\nPergunta:\nRefine\nquestion\nsentences\nSe a frase original estiver semanticamente e gramaticalmente correta, repita-a;\nse for pouco natural, por favor, reescreva-a em uma frase correta e fluente.\n{question}\nAdd\nadditional\ndistractors\nPor favor, adicione apenas duas escolhas plausíveis e naturais e salve-as em {’additional_choice’:[]}.\n[\"{choice1}\", \"{chioce2}\", \"{choice3}\"]\nVerify\nQualities\nPor favor, selecione apenas uma letra como resposta das Answer Choices\ne salve no formato: {’answer’: selected_answer}.\nQ: {question}\nAnswer Choices: (A) {choice_a} (B) {choice_b} (C) {choice_c} (D) {choice_d} (E) {choice_e}\nTable 16: The prompt templates used to create the mCSQA in the Portuguese version.\nSteps\nPrompt (Dutch)\nCreate\nquestion\nsentences\nMaak alstublieft een meerkeuzevraag met de volgende voorwaarden:\n(a) Het enige juiste antwoord is [\"{correct}\"].\n(b) De onjuiste antwoorden zijn [\"{distractor1}\", \"{distractor2}\"].\n(c) Gebruik de woorden [\"{correct}\", \"{distractor1}\", \"{distractor2}\"] niet in de vraag.\n(d) Vermijd het gebruik van oppervlakkige informatie, zoals het aantal tekens.\n(e) De vraag eindigt met een vraagteken (?).\n(f) Het moet een objectieve vraag zijn die alleen met algemene kennis voldoende beantwoord kan worden.\n(g) De vraag moet een eenvoudige en korte zin zijn die uit slechts één zin bestaat.\nVraag:\nRefine\nquestion\nsentences\nAls de originele zin semantisch en grammaticaal correct is, herhaal deze dan;\nals het onnatuurlijk is, herschrijf het dan naar een correcte en vloeiende zin.\n{question}\nAdd\nadditional\ndistractors\nVoeg alstublieft slechts twee aannemelijke en natuurlijke keuzes toe en sla ze op in {’additional_choice’:[]}.\n[\"{choice1}\", \"{chioce2}\", \"{choice3}\"]\nVerify\nQualities\nSelecteer alstublieft slechts één letter als antwoord uit de Answer Choices\nen sla het op in het formaat: {’answer’: selected_answer}.\nQ: {question}\nAnswer Choices: (A) {choice_a} (B) {choice_b} (C) {choice_c} (D) {choice_d} (E) {choice_e}\nTable 17: The prompt templates used to create the mCSQA in the Dutch version.\n32\nSteps\nPrompt (French)\nCreate\nquestion\nsentences\nVeuillez créer une question à choix multiples avec les conditions suivantes :\n(a) La seule bonne réponse est [\"{correct}\"].\n(b) Les réponses incorrectes sont [\"{distractor1}\", \"{distractor2}\"].\n(c) Ne pas utiliser les mots [\"{correct}\", \"{distractor1}\", \"{distractor2}\"] dans la question.\n(d) Évitez d’utiliser des informations superficielles, telles que le nombre de caractères.\n(e) La question se termine par un point d’interrogation (?).\n(f) Il doit s’agir d’une question objective qui peut être suffisamment répondue avec le seul sens commun.\n(g) La question doit être une phrase simple et courte composée d’une seule phrase.\nQuestion :\nRefine\nquestion\nsentences\nSi la phrase originale est correcte sémantiquement et grammaticalement, répétez-la ;\nsi elle est peu naturelle, veuillez la reformuler en une phrase correcte et fluide.\n{question}\nAdd\nadditional\ndistractors\nVeuillez ajouter seulement deux choix plausibles et naturels et les enregistrer dans {’additional_choice’:[]}.\n[\"{choice1}\", \"{chioce2}\", \"{choice3}\"]\nVerify\nQualities\nVeuillez sélectionner uniquement une lettre comme réponse parmi les Answer Choices\net enregistrez-la dans le format : {’answer’: selected_answer}.\nQ: {question}\nAnswer Choices: (A) {choice_a} (B) {choice_b} (C) {choice_c} (D) {choice_d} (E) {choice_e}\nTable 18: The prompt templates used to create the mCSQA in the French version.\nSteps\nPrompt (Russian)\nCreate\nquestion\nsentences\nПожалуйста, создайте вопрос с несколькими вариантами ответа с учетом следующих условий:\n(a) Единственный правильный ответ - [\"{correct}\"].\n(b) Неправильные ответы - [\"{distractor1}\", \"{distractor2}\"].\n(c) Не используйте слова [\"{correct}\", \"{distractor1}\", \"{distractor2}\"] в вопросе.\n(d) Избегайте использования поверхностной информации, такой как количество символов.\n(e) Вопрос заканчивается вопросительным знаком (?).\n(f) Это должен быть объективный вопрос,\nна который можно достаточно ответить только с помощью здравого смысла.\n(g) Вопрос должен быть простым и коротким, состоящим только из одного предложения.\nВопрос:\nRefine\nquestion\nsentences\nЕсли исходное предложение семантически и грамматически правильно, повторите его;\nесли оно звучит ненатурально, пожалуйста,\nперепишите его на корректный и свободно звучащий язык.\n{question}\nAdd\nadditional\ndistractors\nПожалуйста, добавьте только два правдоподобных и естественных выбора\nи сохраните их в {’additional_choice’:[]}.\n[\"{choice1}\", \"{chioce2}\", \"{choice3}\"]\nVerify\nQualities\nПожалуйста, выберите только одну букву алфавита в качестве ответа из Answer Choices и\nсохраните её в формате: {’answer’: selected_answer}.\nQ: {question}\nAnswer Choices: (A) {choice_a} (B) {choice_b} (C) {choice_c} (D) {choice_d} (E) {choice_e}\nTable 19: The prompt templates used to create the mCSQA in the Russian version.\n33\n",
  "categories": [
    "cs.CL",
    "cs.AI",
    "cs.LG"
  ],
  "published": "2024-06-06",
  "updated": "2024-06-06"
}