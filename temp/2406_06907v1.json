{
  "id": "http://arxiv.org/abs/2406.06907v1",
  "title": "SignMusketeers: An Efficient Multi-Stream Approach for Sign Language Translation at Scale",
  "authors": [
    "Shester Gueuwou",
    "Xiaodan Du",
    "Greg Shakhnarovich",
    "Karen Livescu"
  ],
  "abstract": "A persistent challenge in sign language video processing, including the task\nof sign language to written language translation, is how we learn\nrepresentations of sign language in an effective and efficient way that can\npreserve the important attributes of these languages, while remaining invariant\nto irrelevant visual differences. Informed by the nature and linguistics of\nsigned languages, our proposed method focuses on just the most relevant parts\nin a signing video: the face, hands and body posture of the signer. However,\ninstead of using pose estimation coordinates from off-the-shelf pose tracking\nmodels, which have inconsistent performance for hands and faces, we propose to\nlearn the complex handshapes and rich facial expressions of sign languages in a\nself-supervised fashion. Our approach is based on learning from individual\nframes (rather than video sequences) and is therefore much more efficient than\nprior work on sign language pre-training. Compared to a recent model that\nestablished a new state of the art in sign language translation on the How2Sign\ndataset, our approach yields similar translation performance, using less than\n3\\% of the compute.",
  "text": "SignMusketeers: An Efficient Multi-Stream Approach\nfor Sign Language Translation at Scale\nShester Gueuwou\nXiaodan Du\nGreg Shakhnarovich\nKaren Livescu\nToyota Technological Institute at Chicago\n{shesterg,xdu,greg,klivescu}@ttic.edu\nAbstract\nA persistent challenge in sign language video processing, including the task of sign\nlanguage to written language translation, is how we learn representations of sign\nlanguage in an effective and efficient way that can preserve the important attributes\nof these languages, while remaining invariant to irrelevant visual differences.\nInformed by the nature and linguistics of signed languages, our proposed method\nfocuses on just the most relevant parts in a signing video: the face, hands and body\nposture of the signer. However, instead of using pose estimation coordinates from\noff-the-shelf pose tracking models, which have inconsistent performance for hands\nand faces, we propose to learn the complex handshapes and rich facial expressions\nof sign languages in a self-supervised fashion. Our approach is based on learning\nfrom individual frames (rather than video sequences) and is therefore much more\nefficient than prior work on sign language pre-training. Compared to a recent\nmodel that established a new state of the art in sign language translation on the\nHow2Sign dataset, our approach yields similar translation performance, using less\nthan 3% of the compute.\n1\nIntroduction\nRecent work on sign language processing spans human-computer interaction [1, 2], computer\nvision [3, 4], and natural language processing (NLP) [5, 6] research. The nature of signed languages,\nwhich involve the use of manual features (handshape, orientation, location, and movement) and non-\nmanual features (facial expressions, head movements, and body posture), presents unique challenges\nfor machine learning models [7]. However, the question of how to effectively and efficiently represent\nsigned languages while preserving their inherent attributes remains a persistent challenge.\nOur focus is on the task of translation from sign language video to a written language (sign language\ntranslation, or SLT). This is one of the most practically important tasks, necessary to bridge (part of)\nthe communication gap between Deaf and hard of hearing (DHH) populations and hearing populations\n[8]. Recent work [9] proposed a self-supervised video pre-training approach to handle sign language\ntranslation, which achieved state-of-the-art performance on the How2Sign dataset of American Sign\nLanguage (ASL) to English translation [10]. The intuition behind this approach is to extend the\npre-training of a strong video model (in this case, Hiera [11]) with a large-scale unannotated set of\nsign language videos (in this case from YouTube-ASL [12]) and use this extended pre-trained model\nas a feature extractor for the supervised translation task. However, this pre-training is extremely\ncostly: Its longest pre-training run uses 64 A100 80GB GPUs for 14 days, making the approach\ninfeasible for many researchers and practitioners.\nThe approach in Rust et al. [9] implicitly treats sign language sequences like any other (long) videos.\nBut signed languages are, first and foremost, languages, and like any other language, they possess\nPreprint. Under review.\narXiv:2406.06907v1  [cs.CL]  11 Jun 2024\nPhase 1: Self-supervised DINOv2 Pre-training\nDINOv2-F\n2D Coord. \nProjector\nT5\n“This campus \nis home for all \nof us”\nProject \n& \nconcatenate\nPhase 2: Supervised Sign Language (video to English) Translation\nDINOv2-F\nDINOv2-H\nDINOv2-H\nOff-the-shelf \nface and hand \ndetectors\n       :  learned / fine-tuned\n       :  frozen\nOff-the-shelf \nupper body \npose estimator\nFigure 1: Overview of our approach to sign language translation. We parse every frame of the signing\nvideo with off-the-shelf face and hand detectors. (a) In phase 1 (left) we start from pre-trained\nDINOv2 visual feature extractors and continue training them with a DINO loss on cropped face\nboxes and hand boxes, producing two separate DINOv2s (DINOv2-F for the face and DINOv2-H for\nthe hands). This stage is purely self-supervised from random video frames; see also Fig 2 for more\ndetail. (b) In phase 2 (right), fixing the two pre-trained feature extractors, we add a (learned) feature\nextractor for coarse body pose estimated by an off-the-shelf method [16], concatenate and project the\nfeatures for each frame, and fine-tune a T5 model mapping the resulting sequence of frame features\nto English text. This stage is supervised by video clips paired with translations.\nlinguistic properties that may provide an inductive bias about the more important aspects of the\nvideo [13, 14]. In this work, we ask the question: Can we infuse basic linguistic properties of signed\nlanguages into self-supervised pre-training to develop a scalable compute-friendly approach?\nIn the context of signed language processing, the use of off-the-shelf human pose estimator models\n[15, 16] has been one of the most common ways of incorporating the linguistic constraints of sign\nlanguage into models. The intuition of using pose estimation is that it removes irrelevant features\nthat do not affect the meaning of signs, such as body shape and visual background, and therefore\nfocuses entirely on the linguistically relevant aspects of hand, face and body posture [17]. However,\nthis pose-based approach has several limitations that make it sub-optimal for capturing the details of\nsigned languages, particularly in the representation of hands [18] and faces [19]. First, the human pose\nestimator models used in existing methods [15, 16, 20] are typically trained on everyday handshapes,\nwhich are often less complex than the handshapes found in signed languages. Second, human pose\nestimators are unreliable in capturing crucial non-manual components which are essential for signed\nlanguages, such as eye gaze.1\nOur approach is inspired by the multi-stream/multi-channel property of signed languages (§2.2); that\nis, the fact that they consist of a combination of actions performed largely independently by multiple\nbody parts (channels). Specifically, our proposed method (§3, see overview in Fig 1) focuses on the\nmost relevant parts of a signing video—the face, hands, and body posture of the signer (§3.1)—to\nhandle sign language translation at scale (§2.1). Instead of relying on off-the-shelf human pose\nestimators, we propose to learn the complex handshapes and rich facial expressions directly from\nsigning videos using self-supervised learning (§3.2). By focusing on these some crucial aspects of\nsigned languages and learning their representations (§2.3) in a self-supervised manner, our method\ncan effectively capture the intricacies of handshapes and facial expressions for the supervised training\nstage (§3.3), without the need for extensive pre-training data or computational resources. This\napproach allows us to overcome the limitations of human pose estimators and preserve the crucial\nlinguistic information conveyed through handshapes, eye gaze, and facial expressions in signed\nlanguages. We name our approach SignMusketeers: Like the heroes of Dumas’ books [21], three\nimage channels (face and two hand boxes) join forces with a fourth companion (pose features) in the\nquest for glory (accurate sign language translation).\n1For instance, in British Sign Language, the main difference between the signs for \"God\" and \"Boss\" lies in\nthe eye gaze, which existing human pose estimators do not capture [14].\n2\nWe conduct experiments (§4) on How2Sign and find that our approach achieves competitive perfor-\nmance while using a smaller model (in terms of number of parameters), with 41x less pre-training\ndata and 160x fewer pre-training epochs, using roughly 3% of the compute resources of the state of\nthe art approach [9]. We analyse (§4.1) the difference in cost between our approach and the state of\nthe art, and provide ablation experiments showing the value of individual design decisions (§4.2).\n2\nRelated Work\n2.1\nSign Language Translation at Scale\nUntil very recently, the lack of large-scale datasets has been a major obstacle in advancing sign\nlanguage translation. Most research has been conducted on small datasets such as PHOENIX-2014T\n[22], which contains only 9 hours of content with a limited vocabulary size of 3,000. While fairly\nhigh BLEU scores (>20) have been reported on this dataset, translation of more realistic video is\nfar more challenging. Recent efforts to create larger datasets, such as BOBSL (∼1,500 hours) [23],\nOpenASL (∼300 hours) [24], JWSign (∼2,500 hours) [25], and the SRF corpus (∼400 hours) [26],\nhave revealed the difficulty of the task, with BLEU scores2 reaching only around 2-7.\nUthus et al. [12] demonstrated the potential of using the YouTube-ASL dataset for large-scale training\nof ASL translation models. By fine-tuning a T5 [28] model on YouTube-ASL and further fine-tuning\nit on the smaller benchmark dataset How2Sign [10], the authors achieved a BLEU score of 12.39.\nThe input to the T5 model consisted of selected human poses obtained from the off-the-shelf human\npose estimator MediaPipe [16].\nBuilding upon this paradigm, Rust et al. [9] further improved performance on How2Sign by training\na video encoder on YouTube-ASL initialized from a self-supervised video pre-trained model (Hiera-\nBase) with a masked autoencoding objective. However, this approach is computationally expensive,\nrequiring 64 A100 80GB GPUs for 14 days for a single training run, making it infeasible for\nmany researchers. The authors found that the good results indeed depend on these large compute\nrequirements, as substantially reducing the number of video frames ingested by the encoder from 128,\nor the number of pre-training iterations from 800 epochs, greatly reduced performance (by multiple\nBLEU points). We note that another key claim of this method is its privacy-awareness through face\nblurring, while our approach makes no attempt at privacy.3 However, we also note that face blurring\nis not always sufficient for preserving privacy in computer vision, especially when dealing with\nlarge-scale datasets [29].\nWe propose an alternative approach that focuses on learning sign language fine-grained handshapes\nand facial expressions using an image encoder with a smaller ViT backbone. Our encoder takes just a\nsingle frame at a time and requires much lower training time and compute resources.\n2.2\nMulti-Channel Sign Language Processing\nSigned languages are inherently multi-channel, employing a combination of manual features (hand-\nshapes, orientation, location, and movement) and non-manual features (facial expressions, head\nmovements, and body posture) to convey meaning [30, 31, 32]. Multi-channel sign language process-\ning aims to capture and integrate these diverse sources of information for various tasks, such as sign\nlanguage recognition, translation, and generation. The concept of tackling (American) sign language\nprocessing through a multi-channel approach was first introduced in the early 2000s [33], inspired by\nlinguistic evidence that American Sign Language can be modeled, at least partially, as a combination\nof independent channels [34]. Over the years, several other approaches have used multi-channel ideas\nfor sign language recognition [35, 36] and later translation [37, 38, 24] and production [39, 40] tasks,\nalthough the specific channels and how they are used varies. One common characteristic in these\napproaches is that the feature extractors for the different components were not learned specifically for\nsign languages. This is an example of the general issue in sign language research that the methods\nare not sufficiently adapted to the needs of these languages [8, 41].\n2Unless specified otherwise, BLEU means BLEU-4 scores, computed with sacrebleu [27] version :\nBLEU+c.mixed+#.1+s.exp+tok.13a+v.1.4.1.\n3The particular blurring technique of [9] is not reproducible, as it uses an internal software tool, and details\nof the blurring approach have not been published.\n3\n2.3\nFace, Hands, and Body Posture Representation Learning\nTo achieve our goal of learning semantically meaningful multi-channel features independently of ir-\nrelevant visual details, we draw inspiration from prior work related to facial expression representation\nlearning, hand pose (or shape) estimation, and body posture learning.\nFacial expression.\nOne approach for face analysis is da Silva et al. [42], which investigates the\nrecognition of affective and grammatical facial expressions in Brazilian Sign Language (Libras).\nThe authors utilize a combination of geometric features, such as facial landmarks, and appearance\nfeatures to represent facial expressions. Another approach is MARLIN [43], a masked autoencoder\nfor facial video representation learning, which is effective on various facial analysis tasks, including\nfacial expression recognition. Gao and Patras [44] propose a self-supervised learning approach for\nfacial representation learning with facial region awareness. This method leverages the structure of the\nhuman face by dividing it into regions, such as eyes, nose, and mouth. The authors utilize BYOL\n[45], a popular self-supervised learning framework based on instance discrimination. Our approach\nshares some ideas with the work of Gao and Patras [44]; however, instead of using BYOL, we employ\nDINOv2 [46], a state-of-the-art visual self-supervised learning framework that builds upon a similar\nprinciple of instance discrimination as BYOL.\nHand shape and orientation.\nDeepHand [47] is a convolutional neural network (CNN) approach\nfor hand shape classification in continuous sign language video streams. It addresses the challenge\nof weakly labeled data by proposing a training strategy that exploits the temporal coherence of\nhand shapes within a sign. Zimmermann et al. [48] propose a contrastive representation learning\napproach for hand shape estimation, which learns hand shape representations by contrasting positive\nand negative pairs of hand images in a self-supervised manner, using a novel loss function that\nencourages invariance to changes in viewpoint, articulation, and lighting conditions. FineHand [49]\nis a deep learning approach specifically designed for American Sign Language (ASL) recognition.\nOur approach draws inspiration from these studies and aims to learn hand shape representations in a\nself-supervised manner using DINOv2, which enables us to capture the fine-grained details of hand\nshapes and orientation without relying on explicit annotations.\nGlobal body posture.\nCompared to analysis of the fine-grained gestures of the face and hands,\ntechniques for general (global) human pose estimation [50, 51] are more mature and robust. In our\napproach, we therefore simply utilize an off-the-shelf human pose estimation model, MediaPipe [16],\nto represent the body posture of a signer.\n3\nMethod\nBelow we first describe the data preprocessing (frame parsing) procedure, then the self-supervised\npre-training of feature extractors, and finally the supervised learning of the ASL video to English\ntranslation system.\n3.1\nFrame Parsing\nOur frame parsing pipeline extracts and normalizes the relevant regions of interest (ROIs) from the\nsign language video frames, focusing on the face, left hand, right hand, and upper body pose. Each of\nthese four components is mapped to a feature vector (channel); concatenating and projecting the four\nchannels for each frame yields the frame vector, which is then fed to a sequence model for translation.\nWe use the MediaPipe Holistic framework [16] to extract face, hand, and pose landmarks from the\nvideo frames.\nFace cropping To extract the face ROI, we first determine the smallest square bounding box that can\nfit all the face landmarks while preserving the aspect ratio of the face. This initial bounding box is\nthen scaled up by a factor of 1.2 in each dimension to compensate for any parts of the face that might\nhave been missed.\nIn cases where face landmarks are not detected, we estimate the face region using the upper body\npose landmarks (indices 0 to 10). The bounding box is adjusted to ensure it fits within the frame\nboundaries.\n4\nema\nDINOv2-Small\nStudent\nDINOv2-Small\nTeacher\nSinkhorn \n- Knopp\nsoftmax\nps\npt\nCross-entropy loss\n      DINOv2-Hand pre-training\nLocal Views\nGlobal Views\n⁄⁄\nmasking\n⁄⁄ :  stop gradient\nFigure 2: Self-supervised pre-training of DINOv2 on hand crops (stage 1 of our approach), yielding\nthe hand-specific DINOv2 feature extractor. We pool the right and left hand boxes. We repeat this\nstep separately for face boxes, yielding the face-specific feature extractor.\nHand cropping For the hand ROIs, we follow a similar approach to the face ROI extraction when\nhand landmarks are available: We determine the smallest square bounding box that can fit all the\nhand landmarks while preserving the aspect ratio of the hand and scale it by a factor of 1.2.\nIn cases where hand landmarks are not detected, we estimate the hand regions using the few finger\npose landmarks (indices 17, 19, 21 for the left hand fingers and 18, 20, 22 for the right hand fingers).\nAlthough these are often inaccurate, they provide a good estimate of the hand location(s). We then\ncreate a square bounding box of the same size as the face bounding box with its center at the mean of\nthe relevant pose landmarks.\nTo handle the occasional cases where the MediaPipe hand landmark detector returns erroneous values\nwhen the hand is not in the frame, we adjust the bounding boxes to maintain temporal consistency\nacross the channels. Specifically, we use two strategies: shifting the bounding box inward to keep the\nhand within the frame or using the last previously detected hand ROI before it went out of the frame.\nThe extracted face and hand ROIs are then resized to a fixed 224x224 pixels using bicubic interpolation\nwhile preserving the aspect ratio. This resizing step ensures consistent input dimensions for the\nself-supervised learning models in the next stage.\nUpper body pose normalization We extract body poses from the relevant upper body landmarks,\nand normalize them to encourage invariance to position and scale differences. Specifically, we extract\nMediaPipe human poses for the nose (index 0), left shoulder (index 11), right shoulder (index 12), left\nelbow (index 13), right elbow (index 14), left wrist (index 15), and right wrist (index 16). We assume\nthat these seven landmarks are enough to capture the essential components of the upper body posture\nneeded to recognize movements and spatial positions of the hands and face with respect to each other,\nleaving the finer-grained hand pose and facial expression to the other channels (face and hand ROIs).\nNext, we define a normalized signing space based on the signer’s body proportions, similarly\nto Boháˇcek and Hrúz [52]. We define the head unit as the distance between the left and right\nshoulders divided by 2. The signing space width is set to 6 times the head unit, and the signing space\nheight is set to 7 times the head unit. The signing space bounding box is determined using the left\neye and nose landmarks as reference points.\nFinally, we normalize the seven pose coordinates by scaling the bounding box of the signing space\nto unit width/height with center at (0.5,0.5). The normalized coordinates are then flattened into a\n(14-dimensional) vector.\nTo handle cases where the pose landmarks are not detected in a frame, we employ a strategy similar\nto the one used for the hands: If the pose landmarks are not detected and there is a pose available\nfrom a previous frame, we use the previous pose for the current frame. If there is no previous pose\navailable, we create a placeholder array of negative values to indicate missing data [12].\n5\n3.2\nSelf-Supervised Sign Components pre-training\nOur method has two training phases. The first stage is self-supervised, and aims to produce encoders\nthat specialize in sign language facial expressions and hand gestures. Using the DINOv2-Small\narchitecture [46, 53], we pre-train the two encoders separately. We initialize ViT-small student and\nteacher backbones (due to computational constraints, we could not use a ViT-Base backbone as in\nRust et al. [9]) with the teacher weights from the original dinov2_vits14_reg checkpoint, while the\nlinear heads are randomly initialized.\nWe largely follow the training protocols of the DINOv2 paper with the recommended 4 registers\n[53]. The input face/hand images are randomly transformed into 2 global and 8 local views, using a\nscale of 0.5 to 1.0, and 0.25 to 0.5, respectively. Then, all views (both global and local) are passed\nto the student network. The student features are normalized with a Softmax to obtain the score\nvectors ps. The teacher network, on the other hand, only accepts global views as input. We use\nthe Sinkhorn-Knopp centering algorithm [54] on the features from the teacher network to obtain pt.\nWe compute cross-entropy loss between ps and pt and use it to update the student network. The\ngradient backpropagation is disabled for the teacher network. The weights of the teacher network\nare updated with an exponential moving average (ema) of the student network weights. In addition,\nDINOv2 also masks out random patches of the input views to the student and computes a patch-level\ncross-entropy loss (iBOT loss [55]) between the masked student tokens and the corresponding visible\nteacher tokens. Fig. 2 illustrates this setup.\nWe do this self-supervised pre-training using 1 million face crops and hand crops (obtained as\ndescribed in Section (§3.1)) independently. We use a base learning rate of 2 × 10−4 and batch size\nper GPU of 128 on 8 A6000Ada GPUs (i.e., an effective batch size of 1024). To account for our\nrelatively small dataset, we follow the recommendation of Roth et al. [56] and adjust the number of\niterations per pseudo-epoch and the number of pseudo-epochs, resulting in 5 effective epochs in total.\nIn the KoLeoLoss we change the hyperparameter ϵ from 10−8 to 10−4 to avoid infinite loss values.\nThe resulting teacher networks are used as our face encoder (DINOv2-F) and hand encoder (DINOv2-\nH) for the next stage, which is supervised training.\n3.3\nSupervised Sign Language Translation\nGiven a video of T frames with an associated written language translation, we first obtain channel\ncrops as described in (§3.1). Each crop is passed to the relevant frozen encoder, which is obtained\nas described in section (§3.2). This results in 3 T × 384 matrices for the face, left hand and right\nhand. These matrices are projected to T × 256 via stream-specific linear layers. The normalized body\nposture vectors are transformed to a higher dimensionality, from T × 14 to T × 128 (via a linear layer\ntrained from scratch). All four feature streams (face, left hand, right hand, and body posture) are then\ntemporally concatenated and projected to T × 768 (the input size of the T5 model) via another linear\nlayer, also trained from scratch.\nTo summarize: In stage 1, we independently pre-train two (face and hand specific) image-to-vector\nfeature extractors. In stage 2, we jointly train a human pose feature linear transformation layer and a\nsingle linear layer transformation for the concatenated four-stream features, and we fine-tune the T5\nmodel for translation. Both stages are shown in fig. 1.\n4\nExperiments\nAs in other works [12, 9], we use the YouTube-ASL and How2Sign [10] datasets. YouTube-ASL\ncontains roughly 600,000 clips, or roughly 700 hours, of ASL video with weakly aligned English text\ntranslations.4 How2Sign consists of 31,128 / 1,741 / 2,322 clips for the training / validation / test sets.\nFor the self-supervised training of DINOv2-Hand and DINOv2-Face, due to computational constraints\nwe limit ourselves to 1 million random face crops and 1 million random hand crops from YouTube-\nASL. We note that the state-of-the-art method proposed by Rust et al. [9] sees about 50 millions\nframes during pre-training.\n4Although Uthus et al. [12] report a total of 610,193 clips in YouTube-ASL, we were only able to retrieve\n601,995 clips, presumably because some clips have been deleted between the time of the dataset’s creation and\nour retrieval.\n6\nIn line with previous studies, we employ the following training schedules for the supervised (transla-\ntion) stage, using a stride of 2 for every video clip:\nH2S: Supervised training exclusively on the How2Sign dataset, without using the YouTube-ASL\ndataset.\nYT: Supervised training solely on YouTube-ASL, and evaluation on How2Sign in a zero-shot setting.\nYT→H2S: Supervised training on YouTube-ASL, followed by supervised fine-tuning on How2Sign.\nNote that Uthus et al. [12] and Rust et al. [9] include an additional training schedule, YT + H2S,\nwhich involves training on a mixture of YT and H2S.\nFor the supervised training stage, similarly to other works [12, 9], we initiliaze our T5 from a\nT5.1.1-Base pre-trained checkpoint. We use a batch size of 128 (16 per GPU running on 8 GPUs),\nwith other hyper-parameters identical to Rust et al. [9]. Additionally, when further fine-tuning on\nHow2Sign after training on YouTube-ASL, we perform an extra 5,000 steps of fine-tuning.\n4.1\nComparison to prior work\nTable 1 compares our models to prior results on the How2Sign ASL-English translation task, in-\ncluding two approaches that train on a combination of YouTube-ASL and How2Sign. First, we\nobserve that our method consistently outperforms the approach of Uthus et al. [12] across various\nmetrics. For example, we improve BLEU by 1.9 points when using the YT→H2S training schedule.\nThis improvement provides evidence for the benefits of using learned features over pose estimator\nfeatures/coordinates. When using the H2S-only supervised training schedule, our scores are 1.2\nBLEU scores above the ones of Uthus et al. [12] on this same training schedule (1.2 BLEU vs. 2.4\nBLEU).\nComparing with Rust et al. [9], in the most restrictive H2S-only schedule, our method is far behind\nSSVP-SLT (by 9 BLEU points). However, in the best-performing schedule (YT→H2S), the gap\nbetween the two approaches reduces significantly, and our performance is just 0.4 BLEU points\nbelow that of Rust et al. [9].5 We note that the same trend holds for the method of Uthus et al. [12]\nwhen compared to SSVP-SLT. On H2S alone, there is a substantial gap between the two methods\n(+10.5 BLEU points), but on YT→H2S, the gap reduces to +2.4 BLEU points.\nIntrinsically, human pose estimation, as used in the method of Uthus et al. [12], is multi-stream\nin nature since it returns vectors (coordinates) describing different body parts, which are later\nconcatenated. Thus, we suspect that multi-stream approaches, in general, do not perform well with\nsmall datasets in supervised training. We hypothesize that this might be because the features they\nreturn are entirely frame-level features and do not yet contain any information about how these\nframe-level features relate to each other. In contrast, SSVP-SLT pre-trained features already have\nsome information about how frames relate to each other.\nNevertheless, with a larger dataset during the supervised training stage, this lead of SSVP-SLT dimin-\nishes drastically compared to multi-stream approaches like Uthus et al. [12] and our SignMusketeers\nmethod. This suggests that as the amount of labeled training data increases, the advantage of pre-\ntrained features that capture some temporal relationships becomes less pronounced, and multi-stream\napproaches can bridge the performance gap.\nWe suspect that the 0.4 BLEU difference between our result and that of SSVP-SLT may be attributable\nnot only to the model architecture, but also to other significant factors that vary between the two\napproaches. One key difference is the amount of data used during pre-training. Our method uses a\nsample of only 1.2% of the YouTube-ASL frames, while SSVP-SLT uses 50% of the YouTube-ASL\nframes (every video at a stride of 2). Additionally, due to computational constraints, we pre-train\nour model for only 5 epochs, whereas SSVP-SLT pre-trains for 800 epochs. As another comparison,\nat epoch 100, SSVP-SLT achieves a BLEU score of 12.5 (see table 1 SSVP-SLT YT+H2S(50)\n100\nand fig. 3).\nIn addition, we note that SSVP-SLT is pre-trained on both YouTube-ASL and How2Sign—the\ntarget domain dataset—while our model is pre-trained only on YouTube-ASL. Fig. 3 shows the\n5It is important to note that the authors of Rust et al. [9] report an even higher BLEU score of 15.5 for a\nmethod that trains an additional CLIP [57] model on English text, on a union of YouTube-ASL and How2Sign.\nWe include this result in fig. 3 (top right). Such techniques might also improve our performance, but we are\nunable to do the experiment due to computation constraints.\n7\n0\n5000\n10000\n15000\n20000\n25000\n30000\n35000\nGPU-Hours\n0\n2\n4\n6\n8\n10\n12\n14\n16\nBLEU\nYT(5)\nH2S\nYT(5)\nYT\nYT(5)\nYT\nH2S\nYT(800)\nH2S\nYT(800)\nYT\nYT(800)\nYT+H2S\nYT(800)\nYT\nH2S\nYT+H2S(100)\nYT+H2S\nYT+H2S(200)\nYT+H2S\nYT+H2S(400)\nYT+H2S\nYT+H2S(800)\nYT+H2S\nYT+H2S(600\n200)\nYT+H2S\nType of models\nOurs\nSSVP-SLT\nSSVP-SLT-LSP\nNum. frames\n1.0\n42.0\n45.5\nFigure 3: Comparison of data and computation usage between SignMusketeers (Ours) and Rust et al.\n[9]. Horizontal axis: GPU-Hours for the entire training schedule i.e., self-supervised training and\nsupervised training. Vertical axis: BLEU score. Bubble size: number of frames (in millions) used\nduring the pre-training stage. Labels: the first line is the pre-training protocol and the second line is\nthe supervised training protocol. The number in parentheses is the number of pre-training epochs.\nYT: YouTube-ASL, H2S: How2Sign; X→Y means train on X then fine-tune on Y; X+Y means train\non the union X∪Y. Note: GPU-Hours for Rust et al. [9] is computed based on Section C.3 of Rust\net al. [9].\nperformance-resources tradeoff for multiple models, showing that our approach (top left, in blue)\nsurpasses SSVP-SLT by 1.7 BLEU points while training for 20 times fewer pre-training epochs and\nwithout utilizing the target domain dataset during pre-training.\nThese observations suggest that our method, despite using significantly less pre-training data and\nfewer pre-training epochs, can achieve competitive performance compared to the state-of-the-art.\nFurther investigation into the impact of pre-training data size and the number of pre-training epochs\non the final performance could provide valuable insights into the efficiency and scalability of sign\nlanguage translation models.\n4.2\nAblations\nWe conduct ablation studies to assess the effectiveness of pre-training DINOv2 on hand and face\ncrops, the benefits of the multi-stream approach, and the potential of incorporating raw frames as an\nadditional input stream.\nDo we benefit from pre-training face/hand specific DINOv2 feature extractors? As shown in\nTable 2, using the original DINOv2 features without further pre-training on hand and face crops\nachieves a BLEU score of 11.3. When we pre-train DINOv2 on hand and face crops and utilize the\nlearned features, the BLEU score improves to 14.3, a substantial increase of 3.0 points. This indicates\nthat while DINOv2 features are indeed robust, continued pre-training on domain-specific data (i.e.,\nhand and face crops) enhances the model’s ability to capture relevant information for sign language\ntranslation.\nIs the multi-stream approach beneficial compared to just using the original frames? To evaluate\nthe effectiveness of the multi-stream approach, we compare the performance of using only the\noriginal frames (uncropped) with that of the multi-stream model. As shown in Table 2, the model\ntrained on uncropped frames achieves a BLEU score of 4.9, while the multi-stream model (using\nthe same original DINOv2 checkpoint model) obtains a significantly higher BLEU score of 11.4.\nThis substantial improvement demonstrates the benefit of the multi-stream approach (given the same\nmodel as feature extractor) in capturing fine-grained details and relevant information from different\nbody parts for sign language translation.\n8\nTable 1: Quantitative results on How2Sign. GPU-Hrs = GPU hours used during the entire training\nstage. PT % of frames = Percentage of YouTube-ASL frames used in the self-supervised pre-training\nstage. ∗Adjusted by throughput ratio reported at https://lambdalabs.com/gpu-benchmarks .\nMETHOD\nBLEU-1\nBLEU-2\nBLEU-3\nBLEU\nGPU-Hrs\nPT % of Frames\nSupervised training Schedule: H2S\nLin et al. [58]\n14.9\n7.3\n3.9\n2.2\n—\n—\nTarrés et al. [59]\n34.0\n19.3\n12.2\n8.0\n—\n—\nUthus et al. [12]\n15.0\n5.1\n2.3\n1.2\n—\n—\nSSVP-SLT YT(50)\n800\n[9]\n38.1\n23.7\n16.3\n11.7\n18535∗\n50\nSignMusketeers YT(1.2)\n5\n(Ours)\n18.8\n8.1\n4.2\n2.4\n592\n1.2\nSupervised training Schedule: YT\nUthus et al. [12]\n20.9\n10.4\n6.1\n4.0\n—\n—\nSSVP-SLT YT(50)\n800\n[9]\n29.2\n16.6\n10.7\n7.1\n18729∗\n50\nSignMusketeers YT(1.2)\n5\n(Ours)\n26.3\n13.8\n8.2\n5.2\n864\n1.2\nSupervised training Schedule: YT + H2S\nUthus et al. [12]\n36.3\n23.0\n16.1\n11.9\n—\n—\nSSVP-SLT YT(50)\n800\n[9]\n41.6\n27.2\n19.3\n14.3\n18768∗\n50\nSSVP-SLT YT+H2S(50)\n100\n[9]\n—\n—\n—\n12.5\n2754∗\n50\nSSVP-SLT-LSP YT+H2S\n600→200 [9]\n43.2\n28.8\n20.8\n15.5\n32912∗\n50\nSupervised training Schedule: YT →H2S\nUthus et al. [12]\n37.8\n24.1\n16.9\n12.4\n—\n—\nSSVP-SLT YT(50)\n800\n[9]\n41.9\n27.7\n19.8\n14.7\n18768∗\n—\nSignMusketeers YT(1.2)\n5\n(Ours)\n41.5\n27.2\n19.3\n14.3\n880\n1.2\nTable 2: Ablation studies of our design choices. All models are pre-trained on YT in stage1 and\nfine-tuned with YT →H2S schedule for stage2 supervised training.\nBLEU-1\nBLEU-2\nBLEU-3\nBLEU\nUncrop + original DINOv2\n23.5\n12.4\n7.6\n4.9\nCrop + original DINOv2\n36.5\n23.1\n15.9\n11.3\nCrop + pre-trained DINOv2 + global frame\n36.7\n23.8\n16.7\n12.2\nCrop + pre-trained DINOv2 (Ours)\n41.5\n27.2\n19.3\n14.3\nDoes a 5th stream containing the raw frames help the model? We investigate the potential of\nincorporating an additional stream containing the raw frames alongside the upper body pose features\nand cropped hand and face features. As shown in Table 2, adding the global frame as an extra stream\nto the multi-stream model results in a BLEU score of 12.2, which is lower than the 14.3 BLEU score\nachieved by the model without the global frame. This suggests that the global frame may add noise\nrather than improve the model.\n5\nConclusion\nIn this paper, we propose SignMusketeers, a data- and compute-efficient method for sign language\ntranslation at scale. It uses a multi-stream encoding scheme that focuses on important parts of a\nsigning video (facial expressions, hands, and body posture) and requires only individual frames during\nself-supervised pre-training, in contrast to prior work on pre-training that requires long duration\nvideos. SignMusketeers achieves competitive performance with roughly 40× less data and 50× less\ncomputation than prior work. Our ablation studies provide evidence that learning separate feature\nencoders that specialize in different aspects of sign language (face, hand, and body) greatly boosts\ntranslation performance. Our results suggest that temporal information may not be necessary during\npre-training. We hope that our affordable approach can make sign language translation accessible to\na wider range of researchers and practitioners.\n9\nReferences\n[1] Zahoor Zafrulla, Helene Brashear, Pei Yin, Peter Presti, Thad Starner, and Harley Hamilton.\nAmerican sign language phrase verification in an educational game for deaf children. In 2010\n20th International Conference on Pattern Recognition, 2010.\n[2] Danielle Bragg, Naomi Caselli, John W Gallagher, Miriam Goldberg, Courtney J Oka, and\nWilliam Thies. Asl sea battle: gamifying sign language data collection. In Proceedings of the\n2021 CHI conference on human factors in computing systems, 2021.\n[3] Gul Varol, Liliane Momeni, Samuel Albanie, Triantafyllos Afouras, and Andrew Zisserman.\nRead and attend: Temporal localisation in sign language videos. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, 2021.\n[4] Marcelo Sandoval-Castaneda, Yanhong Li, Diane Brentari, Karen Livescu, and Gregory\nShakhnarovich. Self-supervised video transformers for isolated sign language recognition.\narXiv preprint arXiv:2309.02450, 2023.\n[5] Kayo Yin, Amit Moryossef, Julie Hochgesang, Yoav Goldberg, and Malihe Alikhani. Including\nsigned languages in natural language processing. In Proceedings of the 59th Annual Meeting of\nthe Association for Computational Linguistics and the 11th International Joint Conference on\nNatural Language Processing (Volume 1: Long Papers), 2021.\n[6] Mathias Müller, Sarah Ebling, Eleftherios Avramidis, Alessia Battisti, Michèle Berger, Richard\nBowden, Annelies Braffort, Necati Cihan Camgöz, Cristina España-bonet, Roman Grund-\nkiewicz, Zifan Jiang, Oscar Koller, Amit Moryossef, Regula Perrollaz, Sabine Reinhard, An-\nnette Rios, Dimitar Shterionov, Sandra Sidler-miserez, and Katja Tissi. Findings of the first\nWMT shared task on sign language translation (WMT-SLT22). In Proceedings of the Seventh\nConference on Machine Translation (WMT), 2022.\n[7] Danielle Bragg, Oscar Koller, Mary Bellard, Larwan Berke, Patrick Boudreault, Annelies\nBraffort, Naomi Caselli, Matt Huenerfauth, Hernisa Kacorri, Tessa Verhoef, et al. Sign language\nrecognition, generation, and translation: An interdisciplinary perspective. In Proceedings of the\n21st International ACM SIGACCESS Conference on Computers and Accessibility, 2019.\n[8] Neil Fox, Bencie Woll, and Kearsy Cormier. Best practices for sign language technology\nresearch. Universal Access in the Information Society, 2023.\n[9] Phillip Rust, Bowen Shi, Skyler Wang, Necati Cihan Camgöz, and Jean Maillard. Towards\nprivacy-aware sign language translation at scale. arXiv preprint arXiv:2402.09611, 2024.\n[10] Amanda Duarte, Shruti Palaskar, Lucas Ventura, Deepti Ghadiyaram, Kenneth DeHaan, Florian\nMetze, Jordi Torres, and Xavier Giro-i Nieto. How2sign: a large-scale multimodal dataset for\ncontinuous american sign language. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 2021.\n[11] Chaitanya Ryali, Yuan-Ting Hu, Daniel Bolya, Chen Wei, Haoqi Fan, Po-Yao Huang, Vaibhav\nAggarwal, Arkabandhu Chowdhury, Omid Poursaeed, Judy Hoffman, et al. Hiera: A hierarchical\nvision transformer without the bells-and-whistles. In International Conference on Machine\nLearning, 2023.\n[12] Dave Uthus, Garrett Tanzer, and Manfred Georg. Youtube-asl: A large-scale, open-domain\namerican sign language-english parallel corpus. Advances in Neural Information Processing\nSystems, 2023.\n[13] Diane Brentari. A prosodic model of sign language phonology. Mit Press, 1998.\n[14] Rachel Sutton-Spence and Bencie Woll. The Linguistics of British Sign Language: An Introduc-\ntion. Cambridge University Press, 1999.\n[15] Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh. Realtime multi-person 2d pose\nestimation using part affinity fields. In Proceedings of the IEEE conference on computer vision\nand pattern recognition, 2017.\n10\n[16] Camillo Lugaresi, Jiuqiang Tang, Hadon Nash, Chris McClanahan, Esha Uboweja, Michael\nHays, Fan Zhang, Chuo-Ling Chang, Ming Guang Yong, Juhyun Lee, et al. Mediapipe: A\nframework for building perception pipelines. arXiv preprint arXiv:1906.08172, 2019.\n[17] Mathieu De Coster, Ellen Rushe, Ruth Holmes, Anthony Ventresque, and Joni Dambre. Towards\nthe extraction of robust sign embeddings for low resource sign language recognition. arXiv\npreprint arXiv:2306.17558, 2023.\n[18] Amit Moryossef, Ioannis Tsochantaridis, Joe Dinn, Necati Cihan Camgoz, Richard Bowden, Tao\nJiang, Annette Rios, Mathias Muller, and Sarah Ebling. Evaluating the immediate applicability\nof pose estimation for sign language recognition. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, 2021.\n[19] Anna Kuznetsova and Vadim Kimmelman. Testing mediapipe holistic for linguistic analysis of\nnonmanual markers in sign languages. arXiv preprint arXiv:2403.10367, 2024.\n[20] MMPose Contributors. Openmmlab pose estimation toolbox and benchmark. https://\ngithub.com/open-mmlab/mmpose, 2020.\n[21] Alexandre Dumas. The Works of Alexandre Dumas: Three musketeers. Estes and Lauriat, 1894.\n[22] Necati Cihan Camgoz, Simon Hadfield, Oscar Koller, Hermann Ney, and Richard Bowden.\nNeural sign language translation. In Proceedings of the IEEE conference on computer vision\nand pattern recognition, 2018.\n[23] Samuel Albanie, Gül Varol, Liliane Momeni, Hannah Bull, Triantafyllos Afouras, Himel\nChowdhury, Neil Fox, Bencie Woll, Rob Cooper, Andrew McParland, et al. Bbc-oxford british\nsign language dataset. arXiv preprint arXiv:2111.03635, 2021.\n[24] Bowen Shi, Diane Brentari, Gregory Shakhnarovich, and Karen Livescu. Open-domain sign\nlanguage translation learned from online video. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing, 2022.\n[25] Shester Gueuwou, Sophie Siake, Colin Leong, and Mathias Müller. JWSign: A highly multilin-\ngual corpus of Bible translations for more diversity in sign language processing. In Findings of\nthe Association for Computational Linguistics: EMNLP 2023, 2023.\n[26] Mathias Müller, Malihe Alikhani, Eleftherios Avramidis, Richard Bowden, Annelies Braf-\nfort, Necati Cihan Camgöz, Sarah Ebling, Cristina España-Bonet, Anne Göhring, Roman\nGrundkiewicz, Mert Inan, Zifan Jiang, Oscar Koller, Amit Moryossef, Annette Rios, Dimitar\nShterionov, Sandra Sidler-Miserez, Katja Tissi, and Davy Van Landuyt. Findings of the second\nWMT shared task on sign language translation (WMT-SLT23). In Proceedings of the Eighth\nConference on Machine Translation, 2023.\n[27] Matt Post. A call for clarity in reporting BLEU scores. In Ondˇrej Bojar, Rajen Chatterjee, Chris-\ntian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck, Antonio Jimeno\nYepes, Philipp Koehn, Christof Monz, Matteo Negri, Aurélie Névéol, Mariana Neves, Matt Post,\nLucia Specia, Marco Turchi, and Karin Verspoor, editors, Proceedings of the Third Conference\non Machine Translation: Research Papers, 2018.\n[28] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified\ntext-to-text transformer. Journal of machine learning research, 2020.\n[29] Seong Joon Oh, Rodrigo Benenson, Mario Fritz, and Bernt Schiele. Faceless person recogni-\ntion: Privacy implications in social media. In Computer Vision–ECCV 2016: 14th European\nConference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part III 14, 2016.\n[30] Wendy Sandler and Diane Carolyn Lillo-Martin. Sign language and linguistic universals.\nCambridge University Press, 2006.\n[31] Roland Pfau, Josep Quer, et al. Nonmanuals: their grammatical and prosodic roles. Cambridge-\nCambridge University Press, 2010.\n11\n[32] Diane Brentari. Sign language phonology. Cambridge University Press, 2019.\n[33] Christian Vogler and Dimitris Metaxas. A framework for recognizing the simultaneous aspects\nof american sign language. Computer Vision and Image Understanding, 2001.\n[34] Scott K Liddell and Robert E Johnson. American sign language: The phonological base. Sign\nlanguage studies, 1989.\n[35] Eun-Jung Holden, Gareth Lee, and Robyn Owens. Australian sign language recognition.\nMachine Vision and Applications, 2005.\n[36] Junfu Pu, Wengang Zhou, and Houqiang Li. Sign language recognition with multi-modal\nfeatures. In Advances in Multimedia Information Processing-PCM 2016: 17th Pacific-Rim\nConference on Multimedia, Xi´ an, China, September 15-16, 2016, Proceedings, Part II, 2016.\n[37] Necati Cihan Camgoz, Oscar Koller, Simon Hadfield, and Richard Bowden. Multi-channel\ntransformers for multi-articulatory sign language translation. In Computer Vision–ECCV 2020\nWorkshops: Glasgow, UK, August 23–28, 2020, Proceedings, Part IV 16, 2020.\n[38] Hao Zhou, Wengang Zhou, Yun Zhou, and Houqiang Li. Spatial-temporal multi-cue network\nfor sign language recognition and translation. IEEE Transactions on Multimedia, 2021.\n[39] Ben Saunders, Necati Cihan Camgoz, and Richard Bowden. Adversarial training for multi-\nchannel sign language production. arXiv preprint arXiv:2008.12405, 2020.\n[40] Sandrine Tornay, Necati Cihan Camgoz, Richard Bowden, and Mathew Magimai Doss. A\nphonology-based approach for isolated sign production assessment in sign language. In Com-\npanion Publication of the 2020 International Conference on Multimodal Interaction, 2020.\n[41] Aashaka Desai, Maartje De Meulder, Julie A Hochgesang, Annemarie Kocab, and Alex X Lu.\nSystemic biases in sign language ai research: A deaf-led call to reevaluate research agendas.\narXiv preprint arXiv:2403.02563, 2024.\n[42] Emely Pujólli da Silva, Paula Dornhofer Paro Costa, Kate Mamhy Oliveira Kumada, José Mario\nDe Martino, and Gabriela Araújo Florentino. Recognition of affective and grammatical facial\nexpressions: a study for brazilian sign language. In Computer Vision–ECCV 2020 Workshops:\nGlasgow, UK, August 23–28, 2020, Proceedings, Part II 16, 2020.\n[43] Zhixi Cai, Shreya Ghosh, Kalin Stefanov, Abhinav Dhall, Jianfei Cai, Hamid Rezatofighi, Reza\nHaffari, and Munawar Hayat. Marlin: Masked autoencoder for facial video representation\nlearning.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 2023.\n[44] Zheng Gao and Ioannis Patras. Self-supervised facial representation learning with facial region\nawareness. arXiv preprint arXiv:2403.02138, 2024.\n[45] Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena\nBuchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar,\net al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in neural\ninformation processing systems, 2020.\n[46] Maxime Oquab, Timothée Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil\nKhalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell\nHowes, Po-Yao Huang, Hu Xu, Vasu Sharma, Shang-Wen Li, Wojciech Galuba, Mike Rabbat,\nMido Assran, Nicolas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou, Julien Mairal,\nPatrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features\nwithout supervision, 2023.\n[47] Oscar Koller, Hermann Ney, and Richard Bowden. Deep hand: How to train a cnn on 1 million\nhand images when your data is continuous and weakly labelled. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, 2016.\n[48] Christian Zimmermann, Max Argus, and Thomas Brox. Contrastive representation learning for\nhand shape estimation. In DAGM German Conference on Pattern Recognition, 2021.\n12\n[49] Panneer Selvam Santhalingam, Parth Pathak, Huzefa Rangwala, Jana Košecká, et al. Finehand:\nLearning hand shapes for american sign language recognition. In 2020 15th IEEE International\nConference on Automatic Face and Gesture Recognition (FG 2020), 2020.\n[50] Ryo Hachiuma, Fumiaki Sato, and Taiki Sekii. Unified keypoint-based action recognition\nframework via structured keypoint pooling. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, 2023.\n[51] Hong Yan, Yang Liu, Yushen Wei, Zhen Li, Guanbin Li, and Liang Lin. Skeletonmae: graph-\nbased masked autoencoder for skeleton sequence pre-training. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, 2023.\n[52] Matyáš Boháˇcek and Marek Hrúz. Sign pose-based transformer for word-level sign language\nrecognition. In Proceedings of the IEEE/CVF winter conference on applications of computer\nvision, 2022.\n[53] Timothée Darcet, Maxime Oquab, Julien Mairal, and Piotr Bojanowski. Vision transformers\nneed registers. In The Twelfth International Conference on Learning Representations, 2024.\n[54] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin.\nUnsupervised learning of visual features by contrasting cluster assignments. Advances in Neural\nInformation Processing Systems, 2020.\n[55] Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong.\nibot: Image bert pre-training with online tokenizer. International Conference on Learning\nRepresentations (ICLR), 2022.\n[56] Benedikt Roth, Valentin Koch, Sophia J. Wagner, Julia A. Schnabel, Carsten Marr, and Tingying\nPeng. Low-resource finetuning of foundation models beats state-of-the-art in histopathology,\n2024.\n[57] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International conference on machine learning,\n2021.\n[58] Kezhou Lin, Xiaohan Wang, Linchao Zhu, Ke Sun, Bang Zhang, and Yi Yang. Gloss-free end-\nto-end sign language translation. In Proceedings of the 61st Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers), 2023.\n[59] Laia Tarrés, Gerard I. Gállego, Amanda Duarte, Jordi Torres, and Xavier Giró i Nieto. Sign\nlanguage translation from instructional videos. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR) :Workshops, 2023.\n13\n",
  "categories": [
    "cs.CL",
    "cs.AI",
    "cs.CV",
    "cs.LG"
  ],
  "published": "2024-06-11",
  "updated": "2024-06-11"
}