{
  "id": "http://arxiv.org/abs/2406.07236v1",
  "title": "Let Go of Your Labels with Unsupervised Transfer",
  "authors": [
    "Artyom Gadetsky",
    "Yulun Jiang",
    "Maria Brbic"
  ],
  "abstract": "Foundation vision-language models have enabled remarkable zero-shot\ntransferability of the pre-trained representations to a wide range of\ndownstream tasks. However, to solve a new task, zero-shot transfer still\nnecessitates human guidance to define visual categories that appear in the\ndata. Here, we show that fully unsupervised transfer emerges when searching for\nthe labeling of a dataset that induces maximal margin classifiers in\nrepresentation spaces of different foundation models. We present TURTLE, a\nfully unsupervised method that effectively employs this guiding principle to\nuncover the underlying labeling of a downstream dataset without any supervision\nand task-specific representation learning. We evaluate TURTLE on a diverse\nbenchmark suite of 26 datasets and show that it achieves new state-of-the-art\nunsupervised performance. Furthermore, TURTLE, although being fully\nunsupervised, outperforms zero-shot transfer baselines on a wide range of\ndatasets. In particular, TURTLE matches the average performance of CLIP\nzero-shot on 26 datasets by employing the same representation space, spanning a\nwide range of architectures and model sizes. By guiding the search for the\nunderlying labeling using the representation spaces of two foundation models,\nTURTLE surpasses zero-shot transfer and unsupervised prompt tuning baselines,\ndemonstrating the surprising power and effectiveness of unsupervised transfer.",
  "text": "Let Go of Your Labels with Unsupervised Transfer\nArtyom Gadetsky * 1 Yulun Jiang * 1 Maria Brbi´c 1\nbrbiclab.epfl.ch/projects/turtle\nAbstract\nFoundation vision-language models have enabled\nremarkable zero-shot transferability of the pre-\ntrained representations to a wide range of down-\nstream tasks. However, to solve a new task, zero-\nshot transfer still necessitates human guidance to\ndefine visual categories that appear in the data.\nHere, we show that fully unsupervised transfer\nemerges when searching for the labeling of a\ndataset that induces maximal margin classifiers\nin representation spaces of different foundation\nmodels. We present TURTLE, a fully unsuper-\nvised method that effectively employs this guiding\nprinciple to uncover the underlying labeling of a\ndownstream dataset without any supervision and\ntask-specific representation learning. We evalu-\nate TURTLE on a diverse benchmark suite of 26\ndatasets and show that it achieves new state-of-\nthe-art unsupervised performance. Furthermore,\nTURTLE, although being fully unsupervised, out-\nperforms zero-shot transfer baselines on a wide\nrange of datasets. In particular, TURTLE matches\nthe average performance of CLIP zero-shot on\n26 datasets by employing the same representation\nspace, spanning a wide range of architectures and\nmodel sizes. By guiding the search for the un-\nderlying labeling using the representation spaces\nof two foundation models, TURTLE surpasses\nzero-shot transfer and unsupervised prompt tun-\ning baselines, demonstrating the surprising power\nand effectiveness of unsupervised transfer.\n1. Introduction\nTransfer learning is a fundamental machine learning\nparadigm that leverages large-scale pre-training of deep\n*Equal contribution 1EPFL, Lausanne, Switzerland. Correspon-\ndence to: Maria Brbi´c <mbrbic@epfl.ch>.\nProceedings of the 41 st International Conference on Machine\nLearning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by\nthe author(s).\nneural networks to improve model performance on down-\nstream tasks with limited resources (Pan & Yang, 2009).\nEarly transfer learning approaches relied on supervised fine-\ntuning of the entire model to solve a downstream task of\ninterest (Kolesnikov et al., 2020). Recent works (He et al.,\n2022; Li et al., 2022; Zhou et al., 2022; Oquab et al., 2023;\nDarcet et al., 2024) have shown that fine-tuning an entire\nmodel during transfer brings only marginal gains compared\nto training a linear classifier on top of the frozen pre-trained\nbackbone (i.e., linear probe). Although these approaches\neliminated the need for task-specific fine-tuning of represen-\ntations, they still require at least a few labeled examples per\nclass to achieve human-level performance on downstream\ntasks.\nRecently, foundation models (Bommasani et al., 2022) have\nemerged, approaching human-level intelligence on a vari-\nety of tasks in the zero-shot setting. In particular, Radford\net al. (2021) proposed CLIP, which trains representations\nby aligning images and their corresponding captions in the\njoint embedding space. After pre-training, a zero-shot clas-\nsifier is constructed by embedding the descriptions of visual\ncategories that appear in the data. Subsequent works have\nsuccessfully adopted this representation learning principle\nto enable zero-shot transfer in other domains, such as au-\ndio signal processing (Elizalde et al., 2023a;b), biomedicine\n(Lin et al., 2023; Robinson et al., 2023) and symbolic regres-\nsion (Meidani et al., 2024). Despite the remarkable success\nof foundation models, zero-shot transfer still requires human\ninstructions to solve a new task. But, can the representations\nof foundation models be utilized to solve a new task in a\nfully unsupervised manner?\nThe simplest approach for unsupervised transfer would be\nto apply off-the-shelf clustering methods (MacQueen, 1967)\non top of the pre-trained representations. However, this strat-\negy inevitably leads to a drastic decrease in performance\ncompared to (weakly) supervised and zero-shot transfer\n(Zhou et al., 2022; Oquab et al., 2023). Recently, Gadet-\nsky & Brbi´c (2023) introduced HUME, an unsupervised\nlearning framework for inferring the underlying human la-\nbeling of a given dataset from pre-trained representations.\nWhile HUME has achieved superior performance compared\nto unsupervised baselines, it still requires task-specific rep-\n1\narXiv:2406.07236v1  [cs.LG]  11 Jun 2024\nLet Go of Your Labels with Unsupervised Transfer\nFigure 1. Types of downstream transfer differ in the amount of available supervision. Given representation spaces of foundation\nmodels, (i) supervised transfer, represented as a linear probe, trains a linear classifier given labeled examples of a downstream dataset; (ii)\nzero-shot transfer assumes descriptions of the visual categories that appear in a downstream dataset are given, and employs them via\ntext encoder to solve the task; and (iii) unsupervised transfer assumes the least amount of available supervision, i.e., only the number of\ncategories is given, and aims to uncover the underlying human labeling of a dataset.\nresentation learning and does not close the gap between\nunsupervised and zero-shot transfer.\nHere, we present TURTLE, a method that enables unsuper-\nvised transfer from foundation models. The key idea behind\nour approach is to search for the labeling of a downstream\ndataset that maximizes the margins of linear classifiers in the\nspace of single or multiple foundation models to uncover the\nunderlying human labeling. Compared to zero-shot and su-\npervised transfer, unsupervised transfer with TURTLE does\nnot need the supervision in any form (Figure 1). Compared\nto deep clustering methods (Xie et al., 2016; Chang et al.,\n2017; Caron et al., 2018; Van Gansbeke et al., 2020; Niu\net al., 2022), TURTLE does not require task-specific repre-\nsentation learning that is expensive for modern foundation\nmodels.\nWe study the performance of TURTLE on the extensive\nevaluation suite spanning 26 datasets and 7 different foun-\ndation models. We compare TURTLE to various baselines\nthat differ in the amount of available supervision for the\ndownstream transfer. First, when compared to the recent\nstate-of-the-art unsupervised baselines, TURTLE outper-\nforms these baselines on all the considered datasets, setting\nthe new state-of-the-art unsupervised performance. Com-\npared to zero-shot transfer, TURTLE instantiated with two\nfoundation models surpasses CLIP zero-shot transfer across\nall studied model sizes, achieving exceptional absolute im-\nprovements up to 35% on the studied datasets. Given the\nsame single representation space, TURTLE closely matches\nthe performance of the CLIP zero-shot transfer on 7 out of 8\nstudied model architectures. In particular, the best TURTLE\nmodel, which utilizes the same model size and represen-\ntation space, outperforms CLIP zero-shot on 13 out of 26\ndatasets. Finally, when compared to supervised transfer\nrepresented by linear probe, TURTLE approaches its perfor-\nmance on 5 out of 26 studied datasets, suggesting that labels\nmay not be needed to infer the underlying human labeling\nwhen given sufficiently high-quality representations.\n2. Background\nIn this section, we introduce the problem setting of unsuper-\nvised transfer and provide an overview of key concepts that\nwe build upon.\nUnsupervised transfer. Let X ⊆Rd be an input space\nand D = {xn}N\nn=1, xn ∈X be a dataset consisting of N\nsamples and C classes, where C is known a priori. Let\nϕ(x) : X −→Rq denotes a mapping from an input space\nX to a q-dimensional representation space of a pre-trained\nfoundation model. The question we aim to answer is how\nto utilize representations from foundation models to solve\na new task in a fully unsupervised manner. Thus, by un-\nsupervised transfer we consider the task of inferring the\nunderlying human labeling1 of a dataset D without any su-\npervision given representations of foundation models.\nGeneralization-based\nlearning\nof\nhuman\nlabel-\nings.\nGadetsky & Brbi´c (2023) recently introduced a\n1We interchangeably use terms “task” and “labeling” in the\ncontext of this paper, since any labeling defines a task. Conse-\nquently, we refer to a task as human labeled if it corresponds to\nthe underlying human labeling of a given dataset D.\n2\nLet Go of Your Labels with Unsupervised Transfer\ngeneralization-based objective that evaluates the general-\nization ability of linear models on top of representations\nobtained from pre-trained models.\nThe objective is\nmotivated by a strong generalization ability of linear models\nin representation spaces of foundation models on many\nhuman labeled tasks. Equipped with this insight, the goal is\nto find such labeling that optimizes generalization ability of\na linear model over all possible labelings of a given dataset.\nThe quality of a labeling is measured by the ability of a\nlinear model to generalize on a task defined by the given\nlabeling.\nIn particular, let τ : X −→{1, . . . , C} denote a labeling\nfunction of a dataset. Let f(x) = wT ϕ(x) denote a lin-\near model in the representation space ϕ(x) of a founda-\ntion model. Given a train-test split (Dtr, Dte), one can\ntrain the model on a training split Dtr with labeling τ(Dtr)\nand classification loss function L to obtain ˆf. After train-\ning, the generalization ability of the model can be assessed\nby computing the error of ˆf on Dte. Consequently, the\ngeneralization-based objective is defined as follows:\nmin\nτ\nX\nx∈Dte\nL( ˆf(x), τ(x))\ns.t. ˆf = arg min\nf\nX\nx∈Dtr\nL(f(x), τ(x)),\n(1)\nwhere minimization is performed over the set of all possible\nlabelings of a dataset D. This leads to a difficult discrete\noptimization problem. To overcome this limitation, Gadet-\nsky & Brbi´c (2023) replace minimization w.r.t. a discrete\nlabeling τ with minimization w.r.t. continuous parameters\nθ of a task encoder τθ(x) : X −→∆C−1, where ∆C−1 de-\nnotes (C −1)-dimensional probability simplex. As a result,\ncareful design of τθ becomes crucial since it defines the\nsearch space explored by the generalization-based objective\n(1).\nHUME framework. The instantiation of this framework,\nproposed in HUME (Gadetsky & Brbi´c, 2023), models τθ\nusing a linear model in the representation space ψ(x) ob-\ntained via self-supervised pre-training on the target dataset\nD:\nτ HUME\nθ\n(x) = σ(θT ψ(x)),\n(2)\nwhere σ : R −→∆C−1 denotes an activation function. This\nmodeling choice corresponds to restricting the search space\nin (1) to a set of labelings which are linearly separable in\nthe representation space ψ(x). In addition, obtaining ψ(x)\nrequires task-specific representation learning, i.e., running\nself-supervised learning on the target dataset D. Since reli-\nable self-supervised pre-training necessitates a large amount\nof data (Wang & Isola, 2020), this prevents successful un-\nsupervised transfer on downstream tasks with limited re-\nsources.\nGiven the task encoder parametrization τ HUME\nθ\n, HUME op-\ntimizes the following objective to search for the underlying\nhuman labeling:\nLHUME(θ) =\nX\nx∈Dte\nLce(fapprox(x), τ HUME\nθ\n(x)),\n(3)\nwhere Lce is the cross-entropy loss function and fapprox is an\napproximate solution to ˆf obtained using iterative optimiza-\ntion algorithms. HUME resorts to iterative differentiation\n(Domke, 2012; Shaban et al., 2019) to solve the resulting\nbilevel optimization problem, leading to an expensive over-\nall training procedure.\n3. Analysis of Generalization-Based Objective\nTo understand inductive biases of the generalization-based\nobjective proposed in (1), we consider this objective in case\nof binary labelings τ(x) : X →{−1, +1} with exponen-\ntial loss function Lexp(f(x), τ(x)) = exp(−τ(x)f(x)). To\nsimplify the analysis, we assume that the task encoder τθ\nis a linear model in the same representation space ϕ(x),\ni.e., τθ(x) = σ(θT ϕ(x)), where σ : R −→[−1; 1] is an\nodd activation function such as tanh. This corresponds to\nrestricting the search space in (1) to a set of labelings which\nare linearly separable in the representation space ϕ(x). Ad-\nditionally, we do not distinguish between train and test splits,\ni.e., Dtr = Dte = D. We provide a detailed discussion of\nthe aforementioned assumptions in the remarks at the end\nof this section.\nTo obtain an approximate solution to ˆf, we use itera-\ntive optimization algorithms. Specifically, let wm+1 =\nΞ(wm, D) denote a one step of an optimization algorithm,\ni.e., Ξ(wm, D) = wm −η∇w\nP\nx∈D L(wT\nmϕ(x), τθ(x))\nfor the gradient descent with a step size η. Similarly, let\nwM = Ξ(M)(w0, D) denote M steps of an optimization\nalgorithm starting from w0. Eventually, the above specifica-\ntions result in the following bilevel optimization problem:\nLbinary\nM\n(θ) =\nX\nx∈D\nexp(−τθ(x)wT\nMϕ(x))\n(4)\ns.t. wM = Ξ(M)(w0, D),\n(5)\nwhere we refer to (4) and (5) as inner and outer objectives\nrespectively.\nThe key observation underlying our main result is that the\ninner optimization (5) corresponds to the unregularized lo-\ngistic regression on separable data, allowing us to employ\nthe seminal result by Soudry et al. (2018). This work shows\nthat gradient descent, when applied to the task of unregu-\nlarized logistic regression, outputs iterates that are biased\ntowards the direction of the max-margin hyperplane. Evi-\ndently, the task encoder τθ generates labelings of D, which,\nby definition, are linearly separable in the representation\n3\nLet Go of Your Labels with Unsupervised Transfer\nspace ϕ(x). Consequently, wM will follow the direction\nof max-margin hyperplane for a given labeling τθ. In turn,\nthe last point to observe is that substituting the iterates in\n(4), the outer objective is minimized when wM has a larger\nmargin τθ(x)wT\nMϕ(x) with respect to τθ. Equipped with\nthis intuition, we are now ready to state our main result:\nProposition 3.1. Given M ≫1, θ ̸= 0 and appropriate\nstep size η which ensures convergence, then\nLbinary\nM\n(θ) ≥g(θ)∥wSVM(θ)∥2\n2,\n(6)\nwhere g(θ) = (Mη exp(∥rM(θ)∥2))−1, the residual rM(θ)\nis bounded with limM→∞∥rM(θ)∥2 = 0, and wSVM(θ) is\nthe solution of the hard-margin SVM for a given θ:\nwSVM(θ) = min\nw\n∥w∥2\n2\ns.t.\nτθ(xn)wT ϕ(xn) ≥1\n∀xn ∈D.\n(7)\nWe defer the proof to Appendix A. This result shows that\nthe generalization-based objective upper bounds the norm of\nhard-margin SVM fitted to a labeling τθ. Consequently, min-\nimizing Lbinary\nM\nwill inevitably lead to minimizing the norm\n(i.e., maximizing the margin) with respect to a labeling. As\na result, the optimization procedure will yield labelings with\nlarge margin of the corresponding classifier. Overall, our\nresult unveils that the maximum margin principle (Vapnik,\n1995), widely employed by supervised learning algorithms,\nemerges as the inductive bias of the generalization-based\nobjective (1).\nRemark 3.2. (Search space restriction). The result above\nholds when labelings generated by τθ are linearly separable\nin the representation space ϕ(x). This assumption leads to\nthe analysis of the generalization-based objective (1) with\nthe restricted search space. Ji & Telgarsky (2019) showed\nthat in the case of non-separable labelings, gradient descent\nmirrors the separable case, following the max-margin di-\nrection of a maximal linearly separable subset of the data.\nTherefore, one could expect that the lower bound of the\ngeneralization-based objective (1) optimized over the com-\nplete search space inherits these properties, reflecting the\nseparable case.\nRemark\n3.3.\n(Train-test\nsplit\nassumption).\nThe\ngeneralization-based\nobjective\n(1)\nassumes\ndifferent\ntrain-test splits (Dtr, Dte) on the inner-outer levels respec-\ntively to obtain an unbiased estimate of the true risk of a\nmodel f. In our analysis, we simplify this assumption and\nemploy D on both levels. Our result shows that minimizing\nthe generalization-based objective in this case leads to\nmaximizing the margin of a linear model with respect to\na labeling τ on D. In turn, this will inevitably lead to\nlow error on a held out data given that margin size upper\nbounds generalization error (Bartlett & Shawe-Taylor,\n1999; Gronlund et al., 2020).\nRemark 3.4. (Asymptotic analysis) Proposition 3.1 is rather\ninformal since it substitutes the asymptotic behaviour of\nthe gradient descent iterates wM into the outer objective.\nAlthough a rigorous analysis of the residual is required to\nestablish exact bounds, these results serve to grasp the induc-\ntive bias incorporated in the generalization-based objective\ndesigned for the inference of human labelings.\nIn summary,\nthis result shows that optimizing the\ngeneralization-based objective (1) yields labelings that in-\nduce maximal margin classifiers in the representation space\nϕ(x). Our main result is greatly inspired by the seminal\nworks (Soudry et al., 2018; Ji & Telgarsky, 2019) that reveal\nthe implicit bias of gradient descent towards max-margin\nsolution. Likewise, we demonstrate that the generalization-\nbased objective (1) encourages labelings τ such that if one\nwere to subsequently train a max-margin classifier in the\nrepresentation space ϕ(x) to fit a labeling τ, the margin\nobtained would be maximal over all possible labelings.\n4. TURTLE Framework\nThese insights serve us as a guiding principle to develop\nTURTLE, a general framework for efficient fully unsuper-\nvised transfer given representations of foundation models.\nOptimization objective. Proposition 3.1 provides an im-\nportant insight on the inductive bias incorporated in the\ngeneralization-based objective (1). Indeed, one can search\nfor the underlying human labeling by maximizing the mar-\ngin of a linear model with respect to a labeling. Pushing\nthe limits of this principle, we propose to search for a la-\nbeling by maximizing margins of linear models in spaces\nof multiple foundation models at the same time. Given K\nfoundation models, let ϕk(x) be a representation space of\nk-th foundation model. Given labeling defined by a task\nencoder τθ, let wk\nM be k-th linear model trained to fit this\nlabeling in a representation space ϕk(x). Then, TURTLE’s\noptimization objective is as follows:\nLTURTLE\nM\n(θ) =\nK\nX\nk=1\nX\nx∈D\nLce(wk\nMϕk(x); τθ(x))\ns.t. wk\nM = Ξ(M)(wk\n0, D), ∀k,\n(8)\nwhere, ΞM(wk\n0, D) denotes an iterative optimization algo-\nrithm Ξ run for M steps starting from wk\n0. Intuitively, each\nof the K terms in the loss function encourages τθ to max-\nimize margin of k-th linear model in the corresponding\nrepresentation space ϕk. As opposed to the HUME’s objec-\ntive (3), which maximizes margin only in the single space\nψ(x), TURTLE provides more effective guidance to the\nsearch process.\nTask encoder parametrization. The parametrization of a\ntask encoder τθ defines the search space of labelings, thus\n4\nLet Go of Your Labels with Unsupervised Transfer\nit has a crucial importance on the optimization process. In\nTURTLE, we employ pre-trained representation spaces of\nfoundation models to define a task encoder τθ. These repre-\nsentations remain fixed during the overall training procedure,\nalleviating the need of task-specific representation learning.\nIn particular, given K representation spaces ϕk(x), we de-\nfine our task encoder τθ as follows:\nτ TURTLE\nθ\n(x) = 1\nK\nK\nX\nk=1\nτθk(x),\nwhere τθk(x) = σ(θT\nk ϕk(x)),\n(9)\nsuch that θ = {θ1, . . . , θK} denotes all trainable parameters\nand σ is a softmax activation function. After training, cluster\nassignments are computed as usual:\narg\nmax\nc=1,...,C\n\u0002\nτ TURTLE\nθ\n(x)\n\u0003\nc ,\n(10)\nwhere\n\u0002\nτ TURTLE\nθ\n(x)\n\u0003\nc denotes the probability of assigning a\nsample x to the c-th cluster.\nCompared to the HUME framework in (2) which searches\nfor the underlying human labeling only over all linearly\nseparable labelings in the self-supervised representation\nspace ψ(x), TURTLE’s parametrization greatly expands\nthe search space. Indeed, modeling τθ as a simple ensem-\nble induces the search space which is at least union of all\nlinearly separable labelings in each of the representation\nspaces of foundation models ϕ1, . . . , ϕK. One could fur-\nther suggest employing deeper architectures to model τθ,\nhowever such modeling choice may give rise to tasks that\ncapture spurious correlations in data and do not necessarily\nreflect human labelings (Atanov et al., 2022). Therefore,\nour design choice effectively increases the search space and\nalleviates the need of task-specific fine-tuning by employing\nstrong representations of foundation models.\nRegularization. The task encoder can synthesize degen-\nerate labelings, i.e., assign all samples to a single class\n(Gadetsky & Brbi´c, 2023). Although such labelings induce\nlinear classifiers with the largest possible margin in all rep-\nresentation spaces, they are irrelevant. To avoid such trivial\nsolutions, we separately regularize each term of the task\nencoder:\nR(θ) =\nK\nX\nk=1\nH(τ k\nθk),\n(11)\nwhere τ k\nθk = (|D|)−1 P\nx∈D τθk(x) ∈∆C−1 is an empiri-\ncal label distribution of k-th component τθk and H(·) is the\nentropy function of discrete distribution.\nFinal objective function. Putting (8) and (11) together,\nTURTLE finally optimizes the following objective function:\nmin\nθ\nLTURTLE\nM\n(θ) −γR(θ),\n(12)\nwhere we found γ = 10 is a good default choice for the\nentropy regularization strength γ. We show robustness to\nthis hyperparameter in Appendix G.\nEfficient optimization. The new optimization-based objec-\ntive (8) is a bilevel optimization problem with the convex\ninner part. Indeed, given τθ, computing wk\nM corresponds to\nthe logistic regression problem on D with labeling τθ(D)\nin the k-th representation space ϕk. Learning parameters θ\nusing gradient-based techniques involves computing a total\nderivative\nd\ndθLTURTLE\nM\n:\nd\ndθLTURTLE\nM\n= ∂\n∂θLTURTLE\nM\n+\nK\nX\nk=1\n(∂wk\nM\n∂θ )T\n∂\n∂wk\nM\nLTURTLE\nM\n,\n(13)\nwhere ∂wk\nM\n∂θ\nis the Jacobian, which is expensive to compute\nin practice (Domke, 2012; Shaban et al., 2019). The key\nobservation is that employing the same set of samples D on\nboth inner and outer levels allows us to discard the second\nterm of the total derivative. Indeed, after training wk\nM on\nD, one can approximate\nd\ndθLTURTLE\nM\n≈\n∂\n∂θLTURTLE\nM\nsince\nwk\nM is an approximate stationary point of the inner problem\nfor a given τθ, i.e.,\n∂\n∂wk\nM LTURTLE\nM\n≈0. Ablin et al. (2020)\nhave shown a strong performance of this estimator in prac-\ntice for bilevel optimization problems similar to ours. The\npseudocode of TURTLE is provided in Algorithm B1 with\nimplementation details in Appendix B.3.\n5. Experiments\n5.1. Experimental setup\nDatasets and evaluation metric. We study the perfor-\nmance of TURTLE on the extensive benchmark of 26 vision\ndatasets (Radford et al., 2021). The detailed description of\neach dataset is provided in Appendix B.1. We compare our\nframework with the baselines using accuracy metric and\nemploy Hungarian algorithm (Kuhn, 1955) to match the\nlabeling found by TURTLE (10) to the ground truth labeling\nof a corresponding dataset. By default, we train TURTLE\non the training split of a corresponding dataset and provide\nthe results on the test split. In Appendix H, we additionally\nshow that mimicking deployment regime, i.e., having only\ntest split available for training, does not lead to performance\ndecrease of TURTLE.\nFoundation models in TURTLE. We employ CLIP (Rad-\nford et al., 2021) representations which span different archi-\ntectures and model sizes, in particular, 5 different ResNets\n(R50, R101, R50x4, R50x16 and R50x64) and 3 different\nVision Transformers (ViT-B/32, ViT-B/16 and ViT-L/14).\nWe refer to the TURTLE as TURTLE 1-space if it utilizes\nonly a single space CLIP representation (K = 1 in (8)\nand (9)). We refer to the TURTLE as TURTLE 2-spaces\n5\nLet Go of Your Labels with Unsupervised Transfer\nif it utilizes two different foundation models. Namely, we\nuse DINOv2 ViT-g/14 (Oquab et al., 2023) as the second\nspace while the first space is always represented with one\nof the CLIP variants. Consequently, to specify the partic-\nular CLIP architecture when utilizing two representation\nspaces, e.g., ViT-L/14, we refer to TURTLE as TURTLE\n2-spaces ViT-L/14. We precompute all representations for\nthe entire benchmark and keep these representations fixed\nduring the overall training procedure. The detailed descrip-\ntion of the used models and other specifications to prepare\nrepresentations are provided in Appendix B.2.\nBaselines. We compare unsupervised transfer using TUR-\nTLE to baselines that differ in the amount of supervision\nthey use (Figure 1). First, we compare TURTLE to HUME\n(Gadetsky & Brbi´c, 2023), a method that has recently shown\nstate-of-the-art unsupervised learning performance and sur-\npassed traditional deep clustering approaches (Van Gans-\nbeke et al., 2020; Niu et al., 2022; Amrani et al., 2022;\nFeng et al., 2023). Next, to explore how far can we go\nwith unsupervised transfer, we compare TURTLE in a chal-\nlenging setting to zero-shot transfer, unsupervised prompt\ntuning and supervised baselines. All these baselines use\nsome form of supervision compared to TURTLE which is\nfully unsupervised. We start by comparing TURTLE to the\nCLIP zero-shot transfer (Radford et al., 2021) that employs\ndescriptions of ground truth classes as a form of supervision.\nFollowing (Radford et al., 2021), we perform prompt engi-\nneering and ensembling to construct a zero-shot classifier\nfor each dataset. As even stronger baselines, we compare\nTURTLE to the state-of-the-art unsupervised prompt tuning\nmethods UPL (Huang et al., 2022), POUF (Tanwisuth et al.,\n2023) and GDA (Wang et al., 2024). These approaches\nenhance class prototypes defined by the CLIP zero-shot\nclassifier via unsupervised adaptation on the downstream\ntask. Finally, we employ supervised linear probe on top of\nthe CLIP representations to serve as a supervised transfer\nbaseline. Differences between types of transfer are high-\nlighted in Table 1.\nTable 1. Differences between the considered types of down-\nstream transfer.\nAvailable Supervision\nTraining on D\nUnsupervised transfer (ours)\nNumber of classes\n✓\nZero-shot transfer\nClass descriptions\n✗\nUnsupervised prompt tuning\nClass descriptions\n✓\nSupervised transfer\nLabeled samples\n✓\nModel Selection.\nGadetsky & Brbi´c (2023) showed\nthat generalization-based objective (1) is strikingly well-\ncorrelated with human labelings, which we further confirm\nin Figure B1 on 26 datasets. Notably, this enables unsu-\npervised hyperparameter search in TURTLE. For super-\nvised linear probes, we perform standard cross-validation\nto search for the L2-regularization strength. We refer the\nreader to Appendix B.3 for the detailed description of our\nmodel selection procedures. Code is publicly available at\nhttps://github.com/mlbio-epfl/turtle.\n5.2. Results\nComparison to unsupervised baselines. We start by com-\nparing TURTLE to HUME. Originally, HUME utilized self-\nsupervised representation learning on the given dataset D to\nmodel the task encoder (2). To ensure the fair comparison,\nwe instead employ representation spaces of foundation mod-\nels for both modeling the task encoder (2) and for represen-\ntation space used to model fapprox in (3). Consequently, both\nTURTLE and HUME use the same representation spaces,\ni.e., CLIP ViT-L/14 and DINOv2. Furthermore, we improve\nthe optimization procedure of HUME to enable accelerated\nconvergence. In addition, we compare TURTLE to the K-\nMeans clustering (MacQueen, 1967) on top of concatenated\nembeddings from both representation spaces employed by\nTURTLE. The K-Means clustering serves as the simple un-\nsupervised transfer baseline since, like TURTLE, it does not\nrequire task-specific representation learning. We refer the\nreaders to Appendix C for the detailed description of the im-\nprovements made to HUME as well as the implementation\ndetails of the K-Means.\nAs shown in Figure 2, TURTLE substiantially outperforms\nHUME on all considered datasets, confirming that max-\nimizing margin in both spaces simultaneously to search\nfor the underlying human labeling (8) and expanding the\nsearch space of labelings (9) is the effective design choice.\nRemarkably, TURTLE leads to 23% and 11% absolute im-\nprovements (30% and 18% relative improvement) on the\nMNIST and Birdsnap datasets, respectively. Furthermore,\namong other datasets, TURTLE sets the new state-of-the-art\nunsupervised performance on the ImageNet dataset, achiev-\ning 72.9% accuracy and outperforming the previous state-\nof-the-art (Alkin et al., 2024) by 5.5%.\nMNIST\nCIFAR100\nFood101\nBirdsnap\nImageNet\n50\n60\n70\n80\n90\n100\nAccuracy (%)\n+23%\n+5%\n+6%\n+11%\n+4%\nTURTLE\nHUME\nK-Means\nFigure 2. TURTLE outperforms unsupervised baselines. Com-\nparison of TURTLE to unsupervised baselines with respect to\naccuracy. All methods use the CLIP ViT L/14 and DINOv2 repre-\nsentations. Bars represent the average performance with standard\ndeviations computed over three runs.\nIn addition, we validate optimization efficiency of TUR-\nTLE and compare training time between all the considered\nmethods in Figure 3. The results corroborate the use of first-\n6\nLet Go of Your Labels with Unsupervised Transfer\norder hypergradient approximation (13) in TURTLE. No-\ntably, TURTLE achieves 10× speedup compared to HUME,\nachieving the impressive training time on the ImageNet\ndataset that takes less than five minutes. Overall, our results\nshow that TURTLE effectively addresses the challenges of\nunsupervised transfer and outperforms unsupervised base-\nlines by a large margin.\nMNIST(10)\nCIFAR(100)\nFood(101)\nBirdsnap(500)\n10\n2\n10\n1\n100\n101\n102\nTime (min)\nImageNet(1000)\nTURTLE\nHUME\nK-Means\nFigure 3. TURTLE is an efficient method. Comparison of run-\nning time between TURTLE and unsupervised baselines. TURTLE\nemploys efficient first-order optimization procedure, achieving\nmore than 10× speedup compared to HUME. All methods use\nCLIP ViT L/14 and DINOv2 representations. Bars represent the\naverage performance over three runs. Standard deviations are neg-\nligible (Table C2) and omitted for clarity.\nComparison to zero-shot transfer. We compare TUR-\nTLE to the CLIP zero-shot transfer that uses descriptions of\nground truth classes as a form of supervision. Remarkably,\nwithout using any supervision, TURTLE 2-spaces outper-\nforms the zero-shot transfer of CLIP by a large margin\nacross 26 benchmark datasets for different ViT backbones\n(Figure 4).\nViT-B/32\nViT-B/16\nViT-L/14\n54\n56\n58\n60\n62\n64\n66\n68\nAverage Accuracy (%)\nTransfer performance on 26 datasets\nCLIP Zero-shot\nTURTLE 1-space\nTURTLE 2-spaces\nFigure 4. TURTLE enables unsupervised transfer given repre-\nsentation spaces of foundation models. Employing the same\nCLIP representation space, TURTLE closely matches the perfor-\nmance of the corresponding CLIP zero-shot classifier on average\nover 26 datasets. With the use of an additional representation\nspace, TURTLE outperforms zero-shot transfer, demonstrating\nexceptional abilities of unsupervised transfer learning.\nIn particular, TURTLE 2-spaces outperforms CLIP zero-\nshot by 9%, 7% and 4% absolute improvement (17%, 12%\nand 5% relative improvement) with ViT-B/32, ViT-B/16 and\nViT-L/14 backbones, respectively. Moreover, even TURTLE\n1-space matches the performance of CLIP zero-shot across\n20\n10\n0\n10\n20\n30\n-20.8\n-20.1\n-17.3\n-13.2\n-4.7\n-2.6\n-2.2\n-1.3\n-0.9\n-0.8\n-0.1\n+0.4\n+0.8\n+2.0\n+3.2\n+3.9\n+4.8\n+6.6\n+6.9\n+11.7\n+14.5\n+18.7\n+18.7\n+20.5\n+21.6\n+35.4\nCountry211\nFER2013\nSST2\nStanfordCars\nKinetics700\nImageNet\nGTSRB\nOxfordPets\nFood101\nHatefulMemes\nSUN397\nSTL10\nPatchCamelyon\nDTD\nCLEVRCounts\nCIFAR10\nFGVCAircraft\nCaltech101\nUCF101\nCIFAR100\nKITTI Distance\nRESISC45\nBirdsnap\nFlowers102\nMNIST\nEuroSAT\n Score (%)\nTURTLE Unsupervised Transfer vs. CLIP Zero-shot Transfer\nFigure 5. TURTLE outperforms the CLIP zero-shot classifier\non 15 out of 26 datasets. TURTLE is trained with CLIP ViT-L/14\nand DINOv2 representations. CLIP zero-shot utilizes the same\nCLIP ViT-L/14 architecture. Furthermore, we observe that even\nwith only a single CLIP representation space TURTLE outperforms\nCLIP on 13/26 datasets (Figure E1).\nall studied ViT models. It is important to note that both CLIP\nzero-shot and TURTLE 1-space are linear models in the\nsame representation space and differ only in the amount of\nsupervision which is available to produce the weights. When\ncomparing performance on individual datasets, TURTLE\noutperforms CLIP zero-shot transfer on 15 out of 26 datasets\nwith remarkable absolute gains of 35%, 21% and 20% on\nthe EuroSAT, MNIST and Flowers102 datasets, respectively\n(Figure 5). We provide individual scores for all TURTLE\nand CLIP zero-shot variants in Appendix D.\nComparison to unsupervised prompt tuning. Next, we\ncompare TURTLE to unsupervised prompt tuning baselines.\nWe follow previous works and use CLIP ResNet-50 repre-\nsentations for all methods. Although being fully unsuper-\nvised, TURTLE consistently outperforms all the considered\nbaselines by a large margin (Table 2). Specifically, TUR-\nTLE achieves 8% absolute improvement (12% relative im-\nprovement) in average accuracy over the best unsupervised\nprompt tuning baseline. On the Flowers102 and EuroSAT\ndatasets, our framework attains outstanding absolute gains\nof 27% and 41% (37% and 75% relative improvement), re-\nspectively. Overall, these results demonstrate the surprising\neffectiveness of the unsupervised transfer.\n7\nLet Go of Your Labels with Unsupervised Transfer\nTable 2. TURTLE 2-spaces outperforms unsupervised prompt tuning methods. ZS column indicates whether method utilizes zero-\nshot supervision to make predictions. All methods employ CLIP ResNet-50 representations. TURTLE additionally uses DINOv2\nrepresentations as the second representation space.\nMethod\nZS\nPets Flowers FGVC DTD EuroSAT Cars Food SUN Caltech UCF ImageNet Avg.\nPOUF\n✓\n88.0\n66.7\n16.7\n41.5\n42.1\n57.4\n74.7\n58.6\n86.9\n61.1\n55.2\n59.0\nUPL\n✓\n88.3\n68.9\n17.3\n46.6\n54.8\n62.1\n77.6\n64.0\n89.9\n67.2\n60.5\n63.4\nGDA\n✓\n89.9\n72.7\n18.7\n46.8\n49.9\n60.8\n78.3\n63.6\n87.5\n68.7\n61.2\n63.5\nTURTLE\n✗\n90.9\n99.7\n25.3\n57.0\n95.5\n32.6\n84.1\n65.7\n88.6\n77.7\n66.3\n71.2\nComparison to supervised transfer. Finally, we compare\nTURTLE 1-space ViT-L/14 to supervised linear probe in\nthe same representation space. This means that in this setup\nboth models are linear in the representation space of CLIP\nViT-L/14 and differ only in the amount of supervision uti-\nlized to produce the weights. Supervised linear probe is\ntrained using all available labels. Consequently, we can\nassume that it represents the maximal transfer learning per-\nformance that can be achieved by the unsupervised transfer.\nWe observe a high positive correlation of 0.87 (p-value\n< 10−8) between unsupervised transfer performance and\nits fully supervised counterpart (Figure 6).\nThis result\nindicates that with better supervised linear probe perfor-\nmance, TURTLE’s performance may also increase, which\nwe further investigate in the subsequent paragraph. Notably,\nTURTLE approaches the “optimal“ transfer performance on\nthe STL10, CIFAR10, Flowers102, Food101 and Hateful-\nMemes, demonstrating that labels may not be needed when\ngiven sufficiently high-quality representations, as measured\nby supervised linear probe. We perform similar analysis for\nTURTLE 2-spaces and observe stronger correlation, leading\nto reduced gap between TURTLE 2-spaces and supervised\nlinear probe (Figure E2).\nAblation of different representation spaces on ImageNet.\nResults from the previous paragraph speculate that incor-\nporating stronger representations may lead to the increased\nperformance of unsupervised transfer. To validate this, we\nrun TURTLE with pairs of different representation spaces\non the ImageNet-1000 dataset (Deng et al., 2009). Results\nin Figure 7 show a positive correlation of 0.74 (p-value\n< 10−8) between unsupervised transfer performance and\nthe quality of representations measured by supervised linear\nprobe. The obtained result confirms that employing stronger\nrepresentations for a given dataset leads to the improved per-\nformance of TURTLE. Consequently, TURTLE can further\nimprove performance by exploiting continual progress in\nthe development of foundation models. Furthermore, given\nhigh positive correlation between TURTLE’s accuracy and\nthe generalization-based objective (Figure B1), TURTLE\ncan be utilized as the proxy to measure the quality of given\nrepresentations in the absence of labels for the downstream\ntask.\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nSupervised Linear Probe Performance (%)\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nUnsupervsied TURTLE Performance (%)\nFood101\nCIFAR10\nCIFAR100\nBirdsnap\nSUN397\nStanfordCars\nFGVCAircraft\nDTD\nOxfordPets\nCaltech101\nFlowers102\nMNIST\nFER2013\nSTL10\nEuroSAT\nRESISC45\nGTSRB\nKITTI Distance\nCountry211\nPatchCamelyon\nUCF101\nKinetics700\nCLEVRCounts\nHatefulMemes\nSST2\nImageNet\n=0.87, p = 6.3 × 10\n9\nFigure 6. Unsupervised transfer performance of TURTLE is\ncorrelated with supervised linear probe performance. Dashed\nline y = x denotes the “optimal” unsupervised transfer. The\nperformance of TURTLE and supervised linear probe shows a\nstrong correlation (ρ = 0.87, p = 6.3×10−9 of two-sided Pearson\ncorrelation coefficient). On 5 datasets TURTLE approaches the\nperformance of the “optimal” unsupervised transfer (≤3 point\ndifference).\n6. Related Work\n(Weakly) supervised transfer. (Weakly) supervised trans-\nfer approaches require at least some amount of supervision\nto perform downstream transfer. For instance, BigTransfer\n(Kolesnikov et al., 2020) showed that supervised fine-tuning\nof the entire model after large-scale pre-training successfully\ntransfers knowledge in both fully supervised and few-shot\nregimes. Recent advances in self-supervised learning (He\net al., 2022; Li et al., 2022; Zhou et al., 2022; Oquab et al.,\n2023; Darcet et al., 2024) have demonstrated that a linear\nprobe suffices to achieve competitive performance compared\nto the fine-tuning the entire model. Despite the strength of\nthese approaches, they necessitate labeled examples to per-\nform downstream transfer.\nZero-shot transfer. Foundation models such as CLIP (Rad-\nford et al., 2021) have recently enabled zero-shot transfer,\nwhich relies only on a set of human instructions such as de-\n8\nLet Go of Your Labels with Unsupervised Transfer\n75\n80\n85\nAcc (%)\n76.2\n80.7 83.4 83.5 84.4 85.3 86.2\nSupervised linear probe\nMOCO\nEsViT\nCoCa\nOpenCLIP\nCLIP\nSWAG\nDINOv2\nMOCO\nEsViT\nCoCa\nOpenCLIP\nCLIP\nSWAG\nDINOv2\n52.2\n55.6\n56.5\n58.4\n58.9\n72.3\n68.5\n55.6\n66.8\n63.8\n69.9\n70.2\n74.3\n69.2\n56.5\n63.8\n65.8\n62.2\n64.1\n70.5\n68.7\n58.4\n69.9\n62.2\n66.1\n64.7\n71.7\n73.3\n58.9\n70.2\n64.1\n64.7\n63.5\n71.1\n72.9\n72.3\n74.3\n70.5\n71.7\n71.1\n72.2\n73.7\n68.5\n69.2\n68.7\n73.3\n72.9\n73.7\n70.3\nFigure 7. Top: Supervised linear probe on the ImageNet-1000\ndataset for 7 different representation spaces. Bottom: Heat map\nrepresents unsupervised performance of TURTLE on ImageNet-\n1000. Secondary diagonal cells correspond to TURTLE 1-space,\nwhile off-diagonal cells refer to TURTLE 2-spaces with the pair\nof corresponding representation spaces. The performance of TUR-\nTLE indicates a strong positive correlation with the performance of\nsupervised linear probe (ρ = 0.74, p = 1.4 × 10−9 of two-sided\nPearson correlation coefficient).\nscriptions of visual categories that appear in the data rather\nthan a set of labeled examples. Despite the success of zero-\nshot transfer in different domains (Elizalde et al., 2023a;b;\nLin et al., 2023; Robinson et al., 2023; Meidani et al., 2024),\ncollecting zero-shot annotations still requires expert domain\nknowledge which can be hard to get in many real-world ap-\nplications. In contrast to the zero-shot transfer approaches,\nTURTLE enables fully unsupervised transfer, effectively\nalleviating the need of any human guidance.\nDeep clustering. Deep clustering methods (Xie et al., 2016;\nChang et al., 2017; Caron et al., 2018; Van Gansbeke et al.,\n2020; Niu et al., 2022) aim to jointly perform deep repre-\nsentation learning and clustering on a target dataset. Recent\nstate-of-the-art approaches (Van Gansbeke et al., 2020; Niu\net al., 2022) rely on time-consuming three-stage procedures\nthat involve self-supervised representation learning, cluster-\ning and fine-tuning via self-labeling respectively. In contrast\nto the deep clustering approaches, TURTLE alleviates the\nneed for laborious task-specific representation learning by\nemploying representation spaces of pre-trained foundation\nmodels. Furthermore, compared to deep clustering methods\nthat heavily depend on image augmentations to induce se-\nmantically meaningful clusters, TURTLE builds upon the\nseminal maximum margin principle that is effortlessly ap-\nplicable beyond image data modality. Consequently, our\napproach offers an efficient and effective way to perform\nfully unsupervised transfer from foundation models.\nMaximum margin clustering. Our work has revealed that\noptimizing the generalization-based objective proposed in\nGadetsky & Brbi´c (2023) results in the search for a labeling\nthat maximizes the margin of a maximal margin classifier\nover all possible labelings of a dataset. The first attempt\nto employ maximum margin principle to perform cluster-\ning dates back to Maximum Margin Clustering (MMC)\n(Xu et al., 2004). Later works extended this framework\nto multi-class clustering (Xu & Schuurmans, 2005; Wang\net al., 2010), multi-view clustering (Zhao et al., 2009), or\nfocused on improving the scalability (Zhang et al., 2007;\nWang et al., 2010). Compared to TURTLE, which em-\nploys efficient first-order gradient optimization techniques,\nthe aforementioned approaches rely on the expensive dis-\ncrete optimization techniques. Furthermore, each of the\napproaches adopts maximum margin principle in its own\nway to enable multi-class or multi-space scenario, while\nTURTLE provides a unified framework for any number of\nclasses and representation spaces.\nImplicit bias of optimization algorithms. Understanding\nthe implicit bias of optimization algorithms plays a crucial\nrole in modern machine learning. The seminal work by\nSoudry et al. (2018) showed that the gradient descent, when\napplied to the task of unregularized logistic regression on\nseparable data, converges to the direction of the maximal\nmargin hyperplane without explicitly enforcing such margin\nmaximization. Later, Ji & Telgarsky (2019) extended the\nanalysis and demonstrated a similar behavior of gradient\ndescent in the case of non-separable data. In our work, we\nemploy the aforementioned findings to study the inductive\nbias of the generalization-based objective. Surprisingly, we\nreveal that it yields labelings that maximize the margin of\na maximal margin classifier with respect to labeling. As a\nresult, this insight allows us to develop TURTLE, a method\nthat enables fully unsupervised transfer given representa-\ntions of foundation models.\n7. Conclusion\nIn this work, we have shown that the representations of foun-\ndation models can be utilized to solve a new task in a fully\nunsupervised manner. The key insight behind our approach\nis to search for a labeling that induces maximal margin clas-\nsifiers in the representation spaces of foundation models. We\nutilize this insight to develop TURTLE, a general framework\nfor effective unsupervised transfer given representations of\ndifferent foundation models. Through extensive evaluation,\nwe found that TURTLE, being fully unsupervised, achieves\ncompetitive performance compared to zero-shot transfer by\nemploying only a single representation space. Furthermore,\nutilizing an additional representation space results in re-\nmarkable gains over zero-shot transfer. Given the flexibility\nof our framework, the results also suggest that TURTLE can\ndeliver even better unsupervised transfer performance by\ntaking advantage of new more powerful foundation models\nthat will emerge in the future.\n9\nLet Go of Your Labels with Unsupervised Transfer\nAcknowledgements\nWe thank Chanakya Ekbote, Shuo Wen and Tingyang Yu\nfor valuable suggestions that helped to improve the clar-\nity of the manuscript. We also thank Nikita Doikov for\nfruitful discussions regarding efficient bilevel optimization\ntechniques. We gratefully acknowledge the support of EPFL\nand ZEISS.\nImpact Statement\nAlthough the main goal of our work is to advance the field\nof Machine Learning, the proposed framework relies on\nrepresentation spaces of foundation models. These models\ninherit biases embedded in the data on which they were\ntrained on (Bommasani et al., 2022). Consequently, the\nextensive evaluation and alignment is recommended when\ndeploying TURTLE to critical use-cases such as medicine.\nReferences\nAblin, P., Peyr´e, G., and Moreau, T. Super-efficiency of\nAutomatic Differentiation for Functions Defined as a Min-\nimum. In International Conference on Machine Learning,\n2020.\nAlkin, B., Miklautz, L., Hochreiter, S., and Brandstetter,\nJ. MIM-Refiner: A Contrastive Learning Boost from\nIntermediate Pre-Trained Representations. arXiv preprint\narXiv:2402.10093, 2024.\nAmrani, E., Karlinsky, L., and Bronstein, A. Self-supervised\nClassification network. In European Conference on Com-\nputer Vision, 2022.\nAtanov, A., Filatov, A., Yeo, T., Sohmshetty, A., and Zamir,\nA. Task Discovery: Finding the Tasks that Neural Net-\nworks Generalize on. In Advances in Neural Information\nProcessing Systems, 2022.\nBartlett, P. and Shawe-Taylor, J. Generalization Perfor-\nmance of Support Vector Machines and Other Pattern\nClassifiers. In Advances in Kernel Methods: Support\nVector Learning, 1999.\nBerg, T., Liu, J., Woo Lee, S., Alexander, M. L., Jacobs,\nD. W., and Belhumeur, P. N. Birdsnap: Large-Scale Fine-\ngrained Visual Categorization of Birds. In Computer\nVision and Pattern Recognition, 2014.\nBolte, J., Pauwels, E., and Vaiter, S. One-Step Differen-\ntiation of Iterative Algorithms. In Advances in Neural\nInformation Processing Systems, 2023.\nBommasani, R., Hudson, D. A., Adeli, E., Altman, R.,\nArora, S., et al. On the Opportunities and Risks of Foun-\ndation Models. arXiv preprint arXiv:2108.07258, 2022.\nBossard, L., Guillaumin, M., and Van Gool, L.\nFood-\n101–Mining Discriminative Components with Random\nForests. In European Conference on Computer Vision,\n2014.\nCaron, M., Bojanowski, P., Joulin, A., and Douze, M. Deep\nclustering for unsupervised learning of visual features. In\nEuropean Conference on Computer Vision, 2018.\nCarreira, J., Noland, E., Hillier, C., and Zisserman, A. A\nShort Note on the Kinetics-700 Human Action Dataset.\narXiv preprint arXiv:1907.06987, 2019.\nChang, J., Wang, L., Meng, G., Xiang, S., and Pan, C. Deep\nAdaptive Image Clustering. In International Conference\non Computer Vision, 2017.\nChen, X., Xie, S., and He, K.\nAn Empirical Study of\nTraining Self-Supervised Vision Transformers. In Inter-\nnational Conference on Computer Vision, 2021.\nCheng, G., Han, J., and Lu, X. Remote Sensing Image\nScene Classification: Benchmark and State of the Art. In\nProceedings of the IEEE, 2017.\nCherti, M., Beaumont, R., Wightman, R., Wortsman, M.,\nIlharco, G., Gordon, C., Schuhmann, C., Schmidt, L.,\nand Jitsev, J. Reproducible Scaling Laws for Contrastive\nLanguage-Image Learning. In Computer Vision and Pat-\ntern Recognition, 2023.\nCimpoi, M., Maji, S., Kokkinos, I., Mohamed, S., and\nVedaldi, A. Describing Textures in the Wild. In Computer\nVision and Pattern Recognition, 2014.\nCoates, A., Ng, A., and Lee, H. An Analysis of Single-\nlayer Networks in Unsupervised Feature Learning. In\nInternational Conference on Artificial Intelligence and\nStatistics, 2011.\nDagr´eou, M., Ablin, P., Vaiter, S., and Moreau, T. A Frame-\nwork for Bilevel Optimization that Enables Stochastic\nand Global Variance Reduction Algorithms. In Advances\nin Neural Information Processing Systems, 2022.\nDarcet, T., Oquab, M., Mairal, J., and Bojanowski, P. Vision\nTransformers Need Registers. In International Confer-\nence on Learning Representations, 2024.\nDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei,\nL. Imagenet: A Large-scale Hierarchical Image Database.\nIn Computer Vision and Pattern Recognition, 2009.\nDomke, J. Generic Methods for Optimization-Based Model-\ning. In International Conference on Artificial Intelligence\nand Statistics, 2012.\n10\nLet Go of Your Labels with Unsupervised Transfer\nElizalde, B., Deshmukh, S., Al Ismail, M., and Wang, H.\nCLAP: Learning Audio Concepts from Natural Language\nSupervision. In International Conference on Acoustics,\nSpeech and Signal Processing, 2023a.\nElizalde, B., Deshmukh, S., and Wang, H. Natural Language\nSupervision for General-Purpose Audio Representations.\narXiv preprint arXiv:2309.05767, 2023b.\nFei-Fei, L., Fergus, R., and Perona, P. Learning Genera-\ntive Visual Models From Few Training Examples: An\nIncremental Bayesian Approach Tested on 101 Object\nCategories. In Conference on Computer Vision and Pat-\ntern Recognition Workshop, 2004.\nFeng, W., Tao, K., Rufeng, Z., and Huaping, L.\nSelf-\nsupervised Learning by Estimating Twin Class Distribu-\ntion. In IEEE Transactions on Image Processing, 2023.\nFinn, C., Abbeel, P., and Levine, S. Model-Agnostic Meta-\nLearning for Fast Adaptation of Deep Networks. In Inter-\nnational Conference on Machine Learning, 2017.\nGadetsky, A. and Brbi´c, M. The Pursuit of Human La-\nbeling: A New Perspective on Unsupervised Learning.\nIn Advances in Neural Information Processing Systems,\n2023.\nGeiger, A., Lenz, P., and Urtasun, R. Are We Ready for\nAutonomous Driving? The KITTI Vision Benchmark\nSuite. In Computer Vision and Pattern Recognition, 2012.\nGoodfellow, I. J., Erhan, D., Carrier, P. L., Courville, A.,\nMirza, M., Hamner, B., Cukierski, W., Tang, Y., Thaler,\nD., Lee, D.-H., et al. Challenges in Representation Learn-\ning: A Report on Three Machine Learning Contests. In\nNeural Network, 2015.\nGronlund, A., Kamma, L., and Larsen, K. G. Near-Tight\nMargin-Based Generalization Bounds for Support Vec-\ntor Machines. In International Conference on Machine\nLearning, 2020.\nHe, K., Chen, X., Xie, S., Li, Y., Doll´ar, P., and Girshick, R.\nMasked Autoencoders are Scalable Vision Learners. In\nComputer Vision and Pattern Recognition, 2022.\nHelber, P., Bischke, B., Dengel, A., and Borth, D. Eurosat:\nA Novel Dataset and Deep Learning Benchmark for Land\nUse and Land Cover Classification. In IEEE Journal\nof Selected Topics in Applied Earth Observations and\nRemote Sensing, 2019.\nHuang, T., Chu, J., and Wei, F.\nUnsupervised Prompt\nLearning for Vision-Language Models. arXiv preprint\narXiv:2204.03649, 2022.\nJi, K., Yang, J., and Liang, Y. Bilevel Optimization: Conver-\ngence Analysis and Enhanced Design. In International\nConference on Machine Learning, 2021.\nJi, Z. and Telgarsky, M. The Implicit Bias of Gradient De-\nscent on Nonseparable Data. In Conference on Learning\nTheory, 2019.\nJohnson, J., Hariharan, B., Van Der Maaten, L., Fei-Fei,\nL., Lawrence Zitnick, C., and Girshick, R. Clevr: A\nDiagnostic Dataset for Compositional Language and El-\nementary Visual Reasoning. In Computer Vision and\nPattern Recognition, 2017.\nKiela, D., Firooz, H., Mohan, A., Goswami, V., Singh, A.,\nRingshia, P., and Testuggine, D. The Hateful Memes\nChallenge: Detecting Hate Speech in Multimodal Memes.\nIn Advances in neural information processing systems,\n2020.\nKingma, D. P. and Ba, J. Adam: A Method for Stochastic\nOptimization. In International Conference on Learning\nRepresentations, 2015.\nKolesnikov, A., Beyer, L., Zhai, X., Puigcerver, J., Yung, J.,\nGelly, S., and Houlsby, N. Big Transfer (BiT): General\nVisual Representation Learning. In European Conference\non Computer Vision, 2020.\nKrause, J., Stark, M., Deng, J., and Fei-Fei, L. 3D Object\nRepresentations for Fine-grained Categorization. In In-\nternational Conference on Computer Vision Workshops,\n2013.\nKrizhevsky, A. and Hinton, G. Learning Multiple Layers of\nFeatures from Tiny Images. Technical Report, University\nof Toronto, 2009.\nKuhn, H. W. The Hungarian Method for the Assignment\nProblem. In Naval Research Logistics Quarterly, 1955.\nKwon, J., Kwon, D., Wright, S., and Nowak, R. D. A Fully\nFirst-Order Method for Stochastic Bilevel Optimization.\nIn International Conference on Machine Learning, 2023.\nLeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient-\nbased Learning Applied to Document Recognition. In\nProceedings of the IEEE, 1998.\nLi, C., Yang, J., Zhang, P., Gao, M., Xiao, B., Dai, X., Yuan,\nL., and Gao, J. Efficient Self-supervised Vision Trans-\nformers for Representation Learning. In International\nConference on Learning Representations, 2022.\nLin, W., Zhao, Z., Zhang, X., Wu, C., Zhang, Y., et al. PMC-\nCLIP: Contrastive Language-Image Pre-training using\nBiomedical Documents.\nIn International Conference\non Medical Image Computing and Computer Assisted\nIntervention, 2023.\n11\nLet Go of Your Labels with Unsupervised Transfer\nLiu, B., Ye, M., Wright, S., Stone, P., and Liu, Q. Bome!\nBilevel Optimization Made Easy: A Simple First-Order\nApproach. In Advances in Neural Information Processing\nSystems, 2022.\nLorraine, J., Vicol, P., and Duvenaud, D. Optimizing Mil-\nlions of Hyperparameters by Implicit Differentiation. In\nInternational Conference on Artificial Intelligence and\nStatistics, 2020.\nMacQueen, J. B. Some Methods for Classification and\nAnalysis of MultiVariate Observations. In Berkeley Sym-\nposium on Mathematical Statistics and Probability, 1967.\nMaji, S., Rahtu, E., Kannala, J., Blaschko, M., and Vedaldi,\nA. Fine-grained Visual Classification of Aircraft. arXiv\npreprint arXiv:1306.5151, 2013.\nMeidani, K., Shojaee, P., Reddy, C. K., and Farimani, A. B.\nSNIP: Bridging Mathematical Symbolic and Numeric\nRealms with Unified Pre-training. In International Con-\nference on Learning Representations, 2024.\nNilsback, M.-E. and Zisserman, A. Automated Flower Clas-\nsification over a Large Number of Classes. In Sixth Indian\nConference on Computer Vision, Graphics & Image Pro-\ncessing, 2008.\nNiu, C., Shan, H., and Wang, G. SPICE: Semantic pseudo-\nlabeling for image clustering. IEEE Transactions on\nImage Processing, 31:7264–7278, 2022.\nOquab, M., Darcet, T., Moutakanni, T., Vo, H., Szafraniec,\nM., Khalidov, V., Fernandez, P., Haziza, D., Massa, F.,\nEl-Nouby, A., et al. DINOv2: Learning Robust Visual\nFeatures Without Supervision. In Transactions on Ma-\nchine Learning Research, 2023.\nPan, S. J. and Yang, Q. A Survey on Transfer Learning. In\nIEEE Transactions on Knowledge and Data Engineering,\n2009.\nParkhi, O. M., Vedaldi, A., Zisserman, A., and Jawahar,\nC. Cats and Dogs. In Computer Vision and Pattern\nRecognition, 2012.\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,\nAgarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J.,\net al. Learning Transferable Visual Models from Natural\nLanguage Supervision. In International Conference on\nMachine Learning, 2021.\nRaschka, S., Patterson, J., and Nolet, C. Machine Learning\nin Python: Main Developments and Technology Trends\nin Data Science, Machine Learning, and Artificial Intelli-\ngence. In Information. MDPI, 2020.\nRobinson, L., Atkinson, T., Copoiu, L., Bordes, P., Pierrot,\nT., and Barrett, T. Contrasting Sequence with Structure:\nPre-training Graph Representations with PLMs. In Ad-\nvances in Neural Information Processing Systems AI for\nScience Workshop, 2023.\nSalimans, T. and Kingma, D. P. Weight Normalization:\nA Simple Reparameterization to Accelerate Training of\nDeep Neural Networks. In Advances in Neural Informa-\ntion Processing Systems, 2016.\nSanturkar, S., Tsipras, D., and Madry, A. Breeds: Bench-\nmarks for Subpopulation Shift. In International Confer-\nence on Learning Representations, 2021.\nSchuhmann, C., Beaumont, R., Vencu, R., Gordon, C. W.,\nWightman, R., Cherti, M., Coombes, T., Katta, A., Mullis,\nC., Wortsman, M., Schramowski, P., Kundurthy, S. R.,\nCrowson, K., Schmidt, L., Kaczmarczyk, R., and Jitsev,\nJ. LAION-5b: An Open Large-scale Dataset for Training\nNext Generation Image-text Models. In Neural Informa-\ntion Processing Systems Datasets and Benchmarks Track,\n2022.\nScieur, D., Gidel, G., Bertrand, Q., and Pedregosa, F. The\nCurse of Unrolling: Rate of Differentiating Through Opti-\nmization. In Advances in Neural Information Processing\nSystems, 2022.\nShaban, A., Cheng, C.-A., Hatch, N., and Boots, B. Trun-\ncated Back-propagation for Bilevel Optimization.\nIn\nInternational Conference on Artificial Intelligence and\nStatistics, 2019.\nSingh, M., Gustafson, L., Adcock, A., de Freitas Reis, V.,\nGedik, B., Kosaraju, R. P., Mahajan, D., Girshick, R.,\nDoll´ar, P., and Van Der Maaten, L. Revisiting Weakly\nSupervised Pre-training of Visual Perception Models. In\nComputer Vision and Pattern Recognition, 2022.\nSoomro, K., Zamir, A. R., and Shah, M. UCF101: A Dataset\nof 101 Human Actions Classes from Videos in the Wild.\narXiv preprint arXiv:1212.0402, 2012.\nSoudry, D., Hoffer, E., Nacson, M. S., Gunasekar, S., and\nSrebro, N. The Implicit Bias of Gradient Descent on Sep-\narable Data. In Journal of Machine Learning Research,\n2018.\nStallkamp, J., Schlipsing, M., Salmen, J., and Igel, C. Man\nvs. Computer: Benchmarking Machine Learning Algo-\nrithms for Traffic Sign Recognition. In Neural Networks,\n2012.\nTanwisuth, K., Zhang, S., Zheng, H., He, P., and Zhou, M.\nPOUF: Prompt-oriented Unsupervised Fine-tuning for\nLarge Pre-trained Models. In International Conference\non Learning Representations, 2023.\n12\nLet Go of Your Labels with Unsupervised Transfer\nVan Gansbeke, W., Vandenhende, S., Georgoulis, S., Proes-\nmans, M., and Van Gool, L. SCAN: Learning to Classify\nImages without Labels. In European Conference on Com-\nputer Vision, 2020.\nVapnik, V. The Nature of Statistical Learning Theory. In\nSpringer, 1995.\nVeeling, B. S., Linmans, J., Winkens, J., Cohen, T., and\nWelling, M.\nRotation Equivariant CNNs for Digital\nPathology. In Medical Image Computing and Computer\nAssisted Intervention, 2018.\nVicol, P., Lorraine, J. P., Pedregosa, F., Duvenaud, D., and\nGrosse, R. B. On Implicit Bias in Overparameterized\nBilevel Optimization. In International Conference on\nMachine Learning, 2022.\nWang, F., Zhao, B., and Zhang, C. Linear Time Maximum\nMargin Clustering. In IEEE Transactions on Neural Net-\nworks, 2010.\nWang, T. and Isola, P. Understanding Contrastive Represen-\ntation Learning through Alignment and Uniformity on the\nHypersphere. In International Conference on Machine\nLearning, 2020.\nWang, Z., Liang, J., Sheng, L., He, R., Wang, Z., and Tan, T.\nA Hard-to-Beat Baseline for Training-free CLIP-based\nAdaptation. In International Conference on Learning\nRepresentations, 2024.\nXiao, J., Ehinger, K. A., Hays, J., Torralba, A., and Oliva,\nA. Sun Database: Exploring a Large Collection of Scene\nCategories. In International Journal of Computer Vision,\n2016.\nXie, J., Girshick, R., and Farhadi, A. Unsupervised Deep\nEmbedding for Clustering Analysis. In International\nConference on Machine Learning, 2016.\nXu, L. and Schuurmans, D.\nUnsupervised and Semi-\nsupervised Multi-class Support Vector Machines. In AAAI\nConference on Artificial Intelligence, 2005.\nXu, L., Neufeld, J., Larson, B., and Schuurmans, D. Maxi-\nmum Margin Clustering. In Advances in Neural Informa-\ntion Processing Systems, 2004.\nYu, J., Wang, Z., Vasudevan, V., Yeung, L., Seyedhosseini,\nM., and Wu, Y. CoCa: Contrastive Captioners are Image-\ntext Foundation Models. In Transactions on Machine\nLearning Research, 2022.\nZhang, K., Tsang, I. W., and Kwok, J. T. Maximum Margin\nClustering Made Practical. In International Conference\non Machine Learning, 2007.\nZhao, B., Kwok, J., and Zhang, C. Multiple Kernel Cluster-\ning. In International Conference on Data Mining, 2009.\nZhou, J., Wei, C., Wang, H., Shen, W., Xie, C., Yuille, A.,\nand Kong, T. iBOT: Image BERT Pre-training with On-\nline Tokenizer. In International Conference on Learning\nRepresentations, 2022.\n13\nLet Go of Your Labels with Unsupervised Transfer\nA. Proof of Proposition 3.1\nHere, we first provide the simplified version of the main results from Soudry et al. (2018) for completeness and then\npresent the proof of Proposition 3.1. For clarity, we overload notation for xn and consider xn is already represented in a\nrepresentation space ϕ(x), i.e., xn = ϕ(xn). Given binary labeling function τ(x) ∈{−1, +1} of the dataset D = {xn}N\nn=1,\nlet L(w) be the exponential loss function:\nL(w) =\nN\nX\nn=1\nexp(−τ(xn)wT xn)\n(14)\nAssumption A.1. (Linear separability) The dataset D is linearly separable: ∃w∗∈Rd s.t. τ(xn)wT\n∗xn > 0 for all xn ∈D.\nWe consider minimizing (14) using gradient descent with a step size η:\nwm = wm−1 −η∇wL(wm−1)\n(15)\nLet wSVM denote the primal solution to the hard margin SVM problem:\nwSVM = min\nw\n∥w∥2\n2\ns.t.\nτ(xn)wT xn ≥1\n∀xn ∈D.\n(16)\nLet αSVM denote the dual solution to the hard margin SVM problem:\nαSVM = max\nα\nN\nX\nn=1\nαn −1\n2\nN\nX\ni=1\nN\nX\nj=1\nαiαjτ(xi)τ(xj)xT\ni xj\ns.t.\nαn ≥0\n∀n,\n(17)\nwhere primal and dual variables are related as wSVM = PN\nn=1(αSVM)nτ(xn)xn (Vapnik, 1995).\nAssumption A.2. (Non-degenerate dataset) Support vectors S = {xn ∈D|τ(xn)wT\nSVMxn = 1} span the data, i.e.,\nrank(DS) = rank(D), where DS is a matrix whose columns are xn ∈S. Furthermore, for each xn ∈S, the corresponding\ndual variables are strictly positive, i.e., (αSVM)n > 0, and the rest are zero.\nAfter above specifications, the simplified version of the seminal result by Soudry et al. (2018) is:\nProposition A.3. (Implicit Bias of Gradient Descent, Soudry et al. (2018)) For almost any non-degenerate (Assumption\nA.2) dataset which is linearly separable (Assumption A.1), any starting point w0 and step size η < 1/L(w0), the gradient\ndescent iterates (15) will behave as:\nwm = wSVM log m + ˜w + rm,\n(18)\nwhere wSVM is the max-margin vector (16), ˜w is a solution to:\n∀xn ∈S : η exp(−τ(x) ˜wT xn) = (αSVM)n,\n(19)\nand the residual rm is bounded with limm→∞∥rm∥2 = 0\nEquipped with this result, we analyze the generalization-based objective:\nLbinary\nM\n(θ) =\nX\nx∈D\nexp(−τθ(x)wT\nMϕ(x))\n(20)\ns.t. wM = Ξ(M)(w0, D),\n(21)\nwhere τθ(x) = σ(θT x) is the task encoder with an odd activation function such as tanh and wM = Ξ(M)(w0, D) denotes\nM steps of gradient descent with step size η and labeling defined by τθ. Without loss of generality, we can assume\n∥xn∥2 ≤1, ∀xn ∈D. Given the above specifications, we are now ready to state our main result:\n14\nLet Go of Your Labels with Unsupervised Transfer\nProposition A.4. (Lower bound for the generalization-based objective) Following the assumptions of Proposition A.3, given\nM ≫1 and θ ̸= 0, we have:\nLbinary\nM\n(θ) ≥g(θ)∥wSVM(θ)∥2\n2,\n(22)\nwhere g(θ) = (Mη exp(∥rM(θ)∥2))−1, the residual rM(θ) is bounded with limM→∞∥rM(θ)∥2 = 0, and wSVM(θ) is the\nsolution of the hard-margin SVM for a given θ:\nwSVM(θ) = min\nw\n∥w∥2\n2\ns.t.\nτθ(xn)wT xn ≥1\n∀xn ∈D.\n(23)\nProof. The key observation is that the task encoder τθ(x) generates linearly separable labelings, allowing us to apply\nProposition A.3 and substitute the explicit form of iterates wM into the outer objective (20). Indeed, for example, w∗= θ\nsatisfies Assumption A.1, i.e., τθ(xn)θT xn > 0 for all xn ∈D and θ ̸= 0. Thus, substituting the iterates wM into the outer\nobjective leads to:\nLbinary\nM\n(θ) =\nN\nX\nn=1\nexp(−τθ(xn) (wSVM(θ) log M + ˜w(θ) + rM(θ))T xn),\n(24)\nwhere we explicitly indicate that wSVM(θ), ˜w(θ) and rM(θ) depend on the parameters θ of the task encoder τθ. Let Ln(θ)\nbe n-th term of the sum and Sθ be the indices of support vectors, i.e., n ∈{1, . . . , N} s.t. τθ(xn)wT\nSVMxn = 1. Then, due\nto the non-negativity of exp(·), we have:\nLbinary\nM\n(θ) ≥\nX\nn∈Sθ\nLn(θ).\n(25)\nConsidering single term Ln(θ), n ∈Sθ and opening the brackets, we obtain:\nLn(θ) = exp(−log M · τθ(xn)wT\nSVM(θ)xn)\n|\n{z\n}\nL1\nexp(−τθ(xn) ˜w(θ)T xn)\n|\n{z\n}\nL2\nexp(−τθ(xn)rM(θ)T xn)\n|\n{z\n}\nL3\n.\n(26)\nInspecting (26) separately for each term Li, we obtain: (i) L1 =\n1\nM since n ∈Sθ; (ii) L2 = (αSVM(θ))n\nη\nby (19); and (iii)\nL3 ≥exp(−∥rM(θ)∥2) by Cauchy–Schwarz inequality given that ∥τθ(xn)xn∥2 ≤1. Combining this with (25), finally we\nobtain:\nLbinary\nM\n(θ) ≥(Mη exp(∥rM(θ)∥2))−1 X\nn∈Sθ\nαn(θ) = (Mη exp(∥rM(θ)∥2))−1∥wSVM(θ)∥2\n2,\n(27)\nwhere the last equality comes from the fact that:\n∥wSVM(θ)∥2\n2 = wSVM(θ)T wSVM(θ) = wSVM(θ)T X\nn∈Sθ\n(αSVM(θ))nτθ(xn)xn =\n=\nX\nn∈Sθ\n(αSVM(θ))nτθ(xn)wSVM(θ)T xn =\nX\nn∈Sθ\n(αSVM(θ))n · 1 =\nX\nn∈Sθ\n(αSVM(θ))n,\nconcluding the proof.\n15\nLet Go of Your Labels with Unsupervised Transfer\nB. Experimental Details\nB.1. Datasets\nWe evaluate our framework on 26 vision datasets studied in Radford et al. (2021). These datasets cover a wide range of\nvision tasks, including general object classification datasets CIFAR10 (Krizhevsky & Hinton, 2009), CIFAR100 (Krizhevsky\n& Hinton, 2009), STL10 (Coates et al., 2011), ImageNet (Deng et al., 2009), Caltech101 (Fei-Fei et al., 2004); fine-grained\nobject classification datasets Food101 (Bossard et al., 2014), Flowers (Nilsback & Zisserman, 2008), Birdsnap (Berg\net al., 2014), Stanford Cars (Krause et al., 2013), FGVC Aircraft (Maji et al., 2013), Oxford Pets (Parkhi et al., 2012);\nhandwritten digits classification dataset MNIST (LeCun et al., 1998); texture classification dataset DTD (Cimpoi et al.,\n2014); scene classification dataset SUN397 (Xiao et al., 2016); the facial emotion recognition dataset FER2013 (Goodfellow\net al., 2015); the satellite image classification datasets EuroSAT (Helber et al., 2019), RESISC45 (Cheng et al., 2017); the\nGerman Traffic Sign Recognition Benchmark (GTSRB) (Stallkamp et al., 2012); the KITTI Distance dataset (Geiger et al.,\n2012); the metastatic tissue classification dataset PatchCamelyon (PCam) (Veeling et al., 2018); action recognition datasets\nUCF101 (Soomro et al., 2012), Kinetics700 (Carreira et al., 2019); the CLEVR counting dataset (Johnson et al., 2017);\nthe Hateful Memes dataset (Kiela et al., 2020); the country classification dataset Country211 (Radford et al., 2021) and\nthe Rendered SST2 dataset (Radford et al., 2021). For CLEVR, we take 2000 random samples as training split and 500\nrandom samples as test split. For two video datasets UCF101 and Kinetics700, we take the middle frame of each video\nclip as the input of the pre-trained models. Details of each dataset are provided in Table B1. We use accuracy as the\nevaluation metric for all the datasets. Finally, it’s worth noting that TURTLE could also be applied to the tasks in vari-\nous modalities besides vision or even in cross-modalities scenarios, provided that the pre-trained representations are available.\nTable B1. Benchmark suite of 26 datasets. We use accuracy as the evaluation metric for all datasets.\nDataset\nNumber of Classes\nTrain size\nTest size\nFood101 (Bossard et al., 2014)\n101\n75,750\n25,250\nCIFAR10 (Krizhevsky & Hinton, 2009)\n10\n50,000\n10,000\nCIFAR100 (Krizhevsky & Hinton, 2009)\n100\n50,000\n10,000\nBirdsnap (Berg et al., 2014)\n500\n37,221\n2,500\nSUN397 (Xiao et al., 2016)\n397\n19,850\n19,850\nStanfordCars (Krause et al., 2013)\n196\n8,144\n8,041\nFGVC Aircraft (Maji et al., 2013)\n100\n6,667\n3,333\nDTD (Cimpoi et al., 2014)\n47\n3,760\n1,880\nOxfordPets (Parkhi et al., 2012)\n37\n3,680\n3,669\nCaltech101 (Fei-Fei et al., 2004)\n102\n3,060\n6,084\nFlowers (Nilsback & Zisserman, 2008)\n102\n2,040\n6,149\nMNIST (LeCun et al., 1998)\n10\n60,000\n10,000\nFER2013 (Goodfellow et al., 2015)\n7\n28,709\n3,589\nSTL10 (Coates et al., 2011)\n10\n5,000\n8,000\nEuroSAT (Helber et al., 2019)\n10\n10,000\n5,000\nRESISC45 (Cheng et al., 2017)\n45\n25,200\n6,300\nGTSRB (Stallkamp et al., 2012)\n43\n26,640\n12,630\nKITTI Distance (Geiger et al., 2012)\n4\n5,985\n1,496\nCountry211 (Radford et al., 2021)\n211\n42,200\n21,100\nPatchCamelyon (Veeling et al., 2018)\n2\n294,912\n32,768\nUCF101 (Soomro et al., 2012)\n101\n9,537\n3,783\nKinetics700 (Carreira et al., 2019)\n700\n536,485\n33,966\nCLEVR Counts (Johnson et al., 2017)\n8\n2,000\n500\nHatefulMemes (Kiela et al., 2020)\n2\n8,500\n500\nThe Rendered SST2 (Radford et al., 2021)\n2\n7,792\n1,821\nImageNet (Deng et al., 2009)\n1000\n1,281,167\n50,000\n16\nLet Go of Your Labels with Unsupervised Transfer\nB.2. Representations\nTURTLE is compatible with any pre-trained representations. This paper presents the comprehensive evaluation of TURTLE\non a wide range of representation spaces that vary on the pre-training datasets, model architectures and training objectives.\nSpecifically, we consider CLIP ResNets (RN50, RN101, RN50x4, RN50x16, RN50x64) and CLIP Vision Transformers\n(ViT-B/32, ViT-B/16, ViT-L/14) pre-trained on WebImageText-400M (Radford et al., 2021) for training TURTLE 1-space.\nThese models are pre-trained on the same dataset, scaling with number of the parameters. For TURTLE 2-spaces, we\nincorporate DINOv2 ViT-g/14 pre-trained on LVD-142M (Oquab et al., 2023) as the second representation space. Moreover,\nwe also include SWAG ViT-H/14 pre-trained on IG-3.6B (Singh et al., 2022), CoCa ViT-L/14 (Yu et al., 2022) pre-trained\non LAION-2B (Schuhmann et al., 2022) 2, OpenCLIP ViT-L/14 pre-trained on LAION-2B (Cherti et al., 2023), MOCOv3\nViT-B/16 pre-trained on ImageNet-1000 (Chen et al., 2021) and EsViT Swim-B pre-trained on ImageNet-1000 (Li et al.,\n2022) to study whether incorporating stronger representations on the given dataset may lead to the increased performance\nof unsupervised transfer with TURTLE. For all the models, we precompute the representations with standard image\npreprocessing pipelines and do not use any data augmentations during training of TURTLE. The details of pre-trained\nrepresentations are provided on Table B2.\nTable B2. Representation spaces used in TURTLE. “Weakly Supervised” indicates that the model is pre-trained with text supervision,\nsuch as caption or hashtag of the image.\nModel\nArchitecture\nParameters\nTrained on\nWeakly Supervised\nCLIP\n(Radford et al., 2021)\nRN50\n100M\nWebImageText-400M\n✓\nRN101\n120M\nRN50x4\n180M\nRN50x16\n290M\nRN50x64\n620M\nViT-B/32\n150M\nViT-B/16\n150M\nViT-L/14\n430M\nOpenCLIP (Cherti et al., 2023)\nViT-L/14\n430M\nLAION-2B\n✓\nSWAG (Singh et al., 2022)\nViT-H/14\n630M\nIG-3.6B\n✓\nCoCa (Yu et al., 2022)\nViT-L/14\n640M\nLAION-2B\n✓\nMOCOv3 (Chen et al., 2021)\nViT-B/16\n86M\nImageNet-1000\n✗\nEsViT (Li et al., 2022)\nSwin-B\n88M\nImageNet-1000\n✗\nDINOv2 (Oquab et al., 2023)\nViT-g/14\n1.1B\nLVD-142M\n✗\n2Since the original paper does not release the models, we use the reproduced version from the OpenCLIP project, which could be\nfound at https://github.com/mlfoundations/open clip.\n17\nLet Go of Your Labels with Unsupervised Transfer\nB.3. Implementation Details\nEfficient alternating optimization. TURTLE contains a bilevel objective that measures the loss of the task encoder using\nthe training loss of a linear classifier trained on the task produced by the task encoder. The hyper-gradient of the task\nencoder is ∇θL = ∂L\n∂θ + ( ∂w\n∂θ )T ∂L\n∂w|w=w∗, where the Jacobian ∂w\n∂θ is generally expensive to obtain. Existing works usually\nestimate the hyper-gradient via unrolling or approximation based on the implicit function theorem, e.g., see Finn et al.\n(2017); Lorraine et al. (2020); Ji et al. (2021); Kwon et al. (2023); Dagr´eou et al. (2022); Bolte et al. (2023); Liu et al. (2022).\nHowever, these methods might be inefficient and suboptimal in practice (Scieur et al., 2022). Fortunately, in the TURTLE\nframework, one could avoid the estimation of ( ∂w\n∂θ )T ∂L\n∂w given the fact that ∂L\n∂w|w=w∗≈0. Thus, the gradient of the task\nencoder is simplified to ∇θL = ∂L\n∂θ |w=w∗. This inspires us to train the task encoder via alternating optimization, which\nhas been shown efficient for the min-min optimization problems (Ablin et al., 2020). At each iteration, we first fix the task\nencoder and train the linear classifier for M steps to find its approximate optima. Note that one could choose to re-initialize\nthe linear classifier every time (cold-start), or just start from the values of last iteration (warm-start), which might introduce\ndifferent implicit bias as noted by Vicol et al. (2022). After that, we update the task encoder based on the loss of the linear\nclassifier. The training is efficient since no second-order gradient is needed in this process. The pseudo-code of TURTLE is\nprovided in Algorithm B1.\nAlgorithm B1 TURTLE for Unsupervised Transfer\n1: Input: Dataset D, number of classes C, number of iterations T, representation spaces ϕ1(·), ..., ϕK(·), task parameters\nθ = {θ1, ..., θK}, linear classifiers w1, ..., wK, learning rate η, optimization operator Ξ(·), number of adaptation steps\nM, entropy regularization weight γ\n// Ξ(·) can be any iterative operator, e.g., gradient descent\n2: Randomly initialize θ1, ..., θK and w1\n0, ..., wK\n0\n3: for t = 1 to T do\n4:\nSample mini-batch from dataset X ∼D\n5:\nGenerate task from task encoder τθ ←1\nK\nPK\nk=1 σ(θkϕk(X))\n6:\nUpdate linear classifiers for M steps ∀k ∈[K] : wk\nM ←Ξ(M)(wk\n0, X)\n7:\nUpdate task parameters ∀k ∈[K] : θk ←θk −η\n∂\n∂θk\n\u0002\nLce(wk\nMϕk(X), τθ) + γR(τ θ)\n\u0003\n// partial derivative\n∂\n∂θk\n8:\nif warm-start then update start points ∀k ∈[K], wk\n0 ←wk\nM\n// cold-start keeps the initial wk\n0\n9: end for\n10: Output: Task parameters θ = {θ1, ..., θK}\nTraining details. We precompute the feature representations for all datasets before the training. We use Weight Nor-\nmalization (Salimans & Kingma, 2016) to parameterize the task encoder since we found it helpful for the convergence.\nADAM (Kingma & Ba, 2015) optimizer is used for the training of both linear classifier and task encoder. We use 10000\nas the default batch-size. For datasets smaller than 10000, we train the model with full-batch at each iteration. Overall,\nwe found TURTLE is robust to the choice of the batch-size. We update the linear classifier for M = 10 steps at each\niteration and train the task encoder for T = 6000 iterations in total. If not specifically mentioned, we set the entropy\nregularization parameter γ = 10 for all experiments. We show robustness of TURTLE to this hyperparameter in Ap-\npendix G. For each dataset, we do a grid search over 5 different learning rates for both task encoder and linear classifier\nwith η ∈{0.01, 0.005, 0.001, 0.0005, 0.0001}, respectively. We combine each pair of learning rates with the choice of\nwarm-start or cold-start, and finally get set of 50 triplets to search over for each dataset. We use cross-validation to select\nhyper-parameters, as described below. Following Gadetsky & Brbi´c (2023); Van Gansbeke et al. (2020), we use Hungarian\nalgorithm (Kuhn, 1955) to match the labeling found by TURTLE and the ground truth labeling to compute the clustering\naccuracy. If not specified, we train our model on the training split and report the clustering accuracy on the test split. In\nSection H, we also consider the setting of both training and evaluating TURTLE on the test split, mimicking low data regime.\nCross-validation for task selection. For each dataset, we obtain 50 tasks after grid search, i.e., each corresponds to the\nhyperparameter triplet. We use 10-fold cross-validation to select the best task. The cross-validation regards the learned task\nas “pseudo-labels” and measures the generalization error of a linear classifier trained on these “pseudo-labels”. Specifically,\nwe randomly split the dataset into 10 folds. In each round, a linear classifier is trained on 9 folds and tested on the rest\nfold. The final cross-validation score is the average test accuracy over all rounds. Importantly, this process relies solely\non the learned tasks and does not need any information about the ground-truth labels. For TURTLE trained on multiple\nrepresentations, we do cross-validation on each representation space separately and average the final scores. The task with\nthe highest cross-validation score is selected as the final output of TURTLE. Figure B1 shows the performance of the learned\n18\nLet Go of Your Labels with Unsupervised Transfer\n92\n93\n94\n95\n82.5\n85.0\n87.5\n90.0\n92.5\nClustering Accuracy\n= 0.92, p=6.4×10\n22\nFood101\n98.4\n98.6\n98.8\n99.0\n94\n96\n98\n= 0.77, p=9.7×10\n11\nCIFAR10\n88\n89\n90\n91\n82\n84\n86\n88\n90\n= 0.88, p=7.3×10\n17\nCIFAR100\n40\n50\n60\n70\n80\n40\n45\n50\n55\n60\n65\n= 0.98, p=2.4×10\n23\nBirdsnap\n80\n82\n84\n86\n60\n62\n64\n66\n68\n= 0.91, p=3.7×10\n20\nSUN397\n60\n65\n70\n75\n80\n85\n40\n50\n60\n= 0.63, p=6.9×10\n5\nStanfordCars\n30\n40\n50\n60\n20\n25\n30\n35\nClustering Accuracy\n= 0.74, p=6.1×10\n8\nFGVCAircraft\n82\n84\n86\n88\n52.5\n55.0\n57.5\n60.0\n62.5\n= 0.60, p=3.7×10\n6\nDTD\n88\n90\n92\n94\n75\n80\n85\n90\n95\n= 0.47, p=5.5×10\n4\nOxfordPets\n92\n94\n96\n70\n75\n80\n85\n90\n= 0.69, p=5.1×10\n8\nCaltech101\n98.0\n98.4\n98.8\n99.2\n97\n98\n99\n= 0.10, p=5.7×10\n1\nFlowers102\n80\n85\n90\n95\n50\n60\n70\n80\n90\n100\n= 0.90, p=8.8×10\n19\nMNIST\n90\n91\n92\n93\n94\n30\n32\n34\n36\nClustering Accuracy\n= 0.49, p=3.0×10\n4\nFER2013\n98.0\n98.5\n99.0\n99.5\n75\n80\n85\n90\n95\n100\n= 0.96, p=6.9×10\n29\nSTL10\n92\n94\n96\n86\n88\n90\n92\n94\n96\n= 0.97, p=3.9×10\n32\nEuroSAT\n82.5\n85.0\n87.5\n90.0\n92.5\n65\n70\n75\n80\n85\n90\n= 0.94, p=8.4×10\n23\nRESISC45\n80.0 82.5 85.0 87.5 90.0\n30\n35\n40\n45\n50\n= 0.87, p=3.9×10\n15\nGTSRB\n98.0\n98.5\n99.0\n99.5\n39\n40\n41\n42\n43\n44\n= -0.48, p=4.1×10\n4\nKITTI Distance\n77\n78\n79\n80\n81\n82\n10.0\n10.5\n11.0\n11.5\n12.0\n12.5\nClustering Accuracy\n= 0.14, p=3.3×10\n1\nCountry211\n97.2\n97.6\n98.0\n98.4\n50\n52\n54\n56\n58\n60\n= 0.15, p=2.8×10\n1\nPatchCamelyon\n86\n88\n90\n92\n94\n96\nCross Validation Score\n70\n75\n80\n= 0.93, p=1.8×10\n21\nUCF101\n76\n78\n80\nCross Validation Score\n38\n40\n42\n= 0.90, p=8.3×10\n19\nKinetics700\n78\n80\n82\n84\n86\n88\nCross Validation Score\n21\n22\n23\n24\n25\n26\n= 0.34, p=3.0×10\n2\nCLEVRCounts\n94.0\n94.5\n95.0\n95.5\n96.0\nCross Validation Score\n54\n55\n56\n= 0.07, p=6.2×10\n1\nHatefulMemes\n94\n95\n96\n97\n98\nCross Validation Score\n51.0\n51.5\n52.0\nClustering Accuracy\n= 0.36, p=3.1×10\n2\nSST2\n80\n85\n90\nCross Validation Score\n60\n65\n70\n= 0.95, p=3.8×10\n25\nImageNet\nFigure B1. Task selection via cross-validation. We use TURTLE 2-spaces CLIP ViT-L/14 and DINOv2 to produce the tasks. We show\nthe cross-validation score and corresponding clustering accuracy of the tasks learned by TURTLE with 50 different hyperparameters\nfor each dataset. The cross-validation score is well correlated with the clustering accuracy (ρ = 0.61 of two-sided Pearson correlation\ncoefficient averaged over 26 datasets).\ntasks obtained by TURTLE 2-spaces CLIP ViT-L/14 and DINOv2 and their corresponding cross-validation scores over 26\ndatasets. As indicated by the plot, the cross-validation score is well correlated with the clustering accuracy with an average\nof ρ = 0.61 two-sided Pearson correlation coefficient over 26 datasets. Moreover, among 20 datasets, cross-validation\nsuccessfully identifies the best or near-best task (i.e., with less than 1.5 point difference of clustering accuracy). The result\nof cross-validation also empirically verifies the effectiveness of the generalization-based objective and suggests that the\nlabelings with low generalization error tend to be more aligned with human labeled tasks, confirming the original findings of\nGadetsky & Brbi´c (2023) on the wide suite of datasets.\nLinear probe. Supervised linear probe is a widely used method to evaluate the quality of representation learning (Radford\net al., 2021; Oquab et al., 2023). It trains a linear classifier on the train split on top of the representations extracted from the pre-\ntrained models and then evaluates the performance on the test split. We use the cuML.LogisticRegression (Raschka\net al., 2020) for linear probe evaluation in our paper 3.\nThe linear classifier is trained with L-BFGS optimizer\nfor maximum of 1000 iterations. The cuML library allows for GPU acceleration and, thus, it is much faster than\nsklearn.linear model.LogisticRegression counterpart, especially on large datasets such as ImageNet. To\ndetermine the strength of L2 norm regularization, we randomly take 20% of the training split for validation and search over\n96 values in the log-space ranging from 10−6 to 106. The selection process takes a few minutes on small datasets, and\naround 8 hours on ImageNet, with a single NVIDIA A100 GPU. After that, we train the model with the best regularization\nstrength on the entire training split and report the classification accuracy on the test split.\n3This library is available at https://github.com/rapidsai/cuml.\n19\nLet Go of Your Labels with Unsupervised Transfer\nC. Details on Unsupervised Baselines and Numerical Results\nK-Means clustering. We apply K-Means (MacQueen, 1967) clustering on top of pre-trained features as a simple baseline\nthat does not require task-specific representation learning. Similarly to the linear probe, we also use the implementation\nfrom CuML library for the GPU acceleration (i.e., CuML.KMeans). For each dataset and the corresponding representation,\nwe train K-Means with maximum 1000 iterations (max iter=1000) and 10 random initializations (n init=10) on the\ntrain split, and report the clustering accuracy on the test split. In the case when multiple representations are used, we first L2\nnormalize representation from each pre-trained model, and then apply K-Means clustering on top of the concatenation of all\nL2 normalized features.\nHUME. HUME (Gadetsky & Brbi´c, 2023) is the recent state-of-the-art unsupervised learning baseline that introduced\nthe instantiation of the generalization-based objective (3). Specifically, it learns task-specific representations on the target\ndataset to model the task encoder, and then measures the generalization error of a linear classifier in the representation space\nof a foundation model. We use the original source code 4 for the implementation of HUME, with modifications to improve\nthe speed and performance. In particular, we replace task-specific representations with a pre-trained foundation model\nsince we empirically found it yields better performance. Besides, we remove the variance reduction used in the original\nHUME and sample only the single mini-batch at every iteration (i.e., the same as TURTLE), since we found it significantly\nreduces the computational cost and does not influence the final performance. We update the linear classifier with M = 300\nsteps at each iteration, and train the task encoder with T = 6000 iterations in total. The default batch-size is set to 10000.\nMoreover, we follow the same hyperparameter selection procedure of TURTLE to select the inner/outer learning rates and\nwarm-start/cold-start for HUME.\nComparison of TURTLE to HUME and K-Means. For a fair comparison, we train TURTLE, HUME and K-Means using\nthe same representation spaces, i.e., CLIP ViT-L/14 and DINOv2 ViT-g/14. Given that HUME’s task encoder parametrization\nuses only the single space, we run HUME with the task encoder modeled using CLIP or DINOv2 (denoted as HUME\nCLIP and HUME DINOv2 respectively), and measure the generalization error using the rest representation space. For each\nmethod, we report the training time in minutes and the clustering accuracy averaged over 3 random seeds. For each random\nseed, we perform the hyperparameter selection for HUME and TURTLE as described in the corresponding subsection above.\nTable C1 and Table C2 show the obtained results on 5 datasets. Overall, the results indicate that TURTLE outperforms\nHUME and K-Means on all the considered datasets, highlighting the effectiveness of design choices made in TURTLE.\nFor instance, combining multiple representation spaces for modeling the task encoder in TURTLE brings substantial gains\ncompared to HUME. Namely, TURTLE achieves remarkable 28% and 23% absolute improvement (40% and 30% relative\nimprovement) over HUME DINOv2 and HUME CLIP respectively on the MNIST dataset. Furthermore, efficient first-order\noptimization techniques used in TURTLE allow for fast optimization, taking just 5 minutes even on large-scale datasets such\nas ImageNet.\nTable C1. Accuracy of TURTLE and unsupervised baselines. The results are averaged with standard deviations computed over 3 runs.\nMethod\nMNIST\nCIFAR100\nFood101\nBirdsnap\nImageNet\nK-Means\n68.9 ± 0.6\n75.1 ± 0.5\n78.0 ± 0.7\n54.0 ± 0.8\n64.8 ± 0.3\nHUME CLIP\n75.3 ± 5.0\n71.2 ± 2.2\n86.5 ± 1.3\n45.8 ± 1.8\n65.2 ± 0.9\nHUME DINOv2\n69.7 ± 5.9\n83.9 ± 1.2\n85.3 ± 1.7\n57.3 ± 0.7\n68.1 ± 0.2\nTURTLE\n98.0 ± 0.4\n89.1 ± 1.0\n92.8 ± 0.5\n67.8 ± 0.4\n72.4 ± 0.3\nTable C2. Training time (in minutes) of TURTLE and unsupervised baselines. The results are averaged with standard deviations\ncomputed over 3 runs. The standard deviation for K-Means and TURTLE is negligible.\nMethod\nMNIST\nCIFAR100\nFood101\nBirdsnap\nImageNet\nK-Means\n0.02 ± 0.0\n0.04 ± 0.0\n0.1 ± 0.0\n0.1 ± 0.0\n6.4 ± 0.0\nHUME CLIP\n15.2 ± 2.4\n44.1 ± 0.5\n45.5 ± 1.3\n156.8 ± 1.2\n285.4 ± 4.1\nHUME DINOv2\n11.1 ± 0.3\n31.7 ± 0.4\n31.0 ± 0.3\n96.7 ± 0.6\n185.6 ± 7.9\nTURTLE\n1.6 ± 0.0\n1.6 ± 0.0\n1.7 ± 0.0\n2.1 ± 0.0\n4.8 ± 0.0\n4Code could be found at https://github.com/mlbio-epfl/hume.\n20\nLet Go of Your Labels with Unsupervised Transfer\nD. Complete List of Individual Numerical Results\nTable D1. Complete list of numerical results. Results of supervised linear probe, CLIP zero-shot transfer, K-Means clustering and\nTURTLE unsupervised transfer on 26 datasets and 9 representation spaces (CLIP RN50, RN101, RN50x4, RN50x16, RN50x64, ViT-B/32,\nViT-B/16, ViT-L/14 and DINOv2 ViT-g/14). The results for K-Means 2-spaces and TURTLE 2-spaces are obtained using DINOv2 and\nthe corresponding CLIP model.\nFood101\nCIFAR10\nCIFAR100\nBirdsnap\nSUN397\nStanford Cars\nFGVC Aircraft\nDTD\nOxford Pets\nCaltech101\nFlowers102\nMNIST\nFER2013\nSTL10\nEuroSAT\nRESISC45\nGTSRB\nKITTI\nCountry211\nPCam\nUCF101\nKinetics700\nCLEVR\nHatefulMemes\nRendered SST2\nImageNet\nAverage\nLinear Probe\nDINOv2\n94.9 99.5 93.9 88.9 78.7 91.3 87.8 85.0 96.6 93.8 99.7 98.7 66.8 99.6 97.3 95.3 80.1 76.9 26.1 87.5 91.5 59.2 52.0 52.4 56.1 86.2 82.1\nRN50\n86.4 88.8 70.4 57.4 73.3 77.9 49.2 76.4 88.4 91.6 95.3 98.3 63.4 97.3 94.2 91.0 84.5 72.9 25.5 82.5 80.6 44.8 51.6 58.6 72.2 73.1 74.8\nRN101\n88.9 91.1 73.5 61.2 75.0 84.0 52.6 76.4 90.8 93.6 95.2 98.5 65.2 98.2 94.5 92.2 84.3 71.7 26.6 82.8 82.7 48.0 52.0 55.4 73.9 75.9 76.3\nRN50x4\n91.3 90.5 73.2 66.5 76.7 86.0 57.3 79.6 92.0 94.5 97.2 98.6 67.3 98.1 94.4 92.9 86.4 73.9 30.2 83.1 84.2 50.2 51.8 60.0 76.1 78.1 78.1\nRN50x16\n93.3 92.1 75.0 72.5 79.1 88.9 63.4 79.6 93.6 94.9 97.6 98.9 68.5 98.8 95.3 94.0 89.1 74.5 34.9 83.6 86.2 53.8 49.0 59.4 79.0 81.4 79.9\nRN50x64\n94.7 94.1 78.5 77.9 81.0 90.6 68.4 81.9 94.3 96.6 98.7 99.0 71.2 99.1 95.7 94.8 91.7 75.5 40.7 83.7 89.3 57.5 54.8 58.2 81.4 83.7 82.0\nViT-B/32\n88.6 95.1 80.2 59.9 76.1 80.9 50.4 76.5 89.4 94.2 96.1 98.9 66.3 98.6 95.2 92.5 86.7 71.4 27.3 83.1 83.6 48.6 50.8 57.0 70.4 75.7 76.7\nViT-B/16\n92.7 95.9 82.5 69.3 78.3 86.4 58.8 79.0 93.1 91.2 97.7 98.9 68.0 99.1 95.6 93.9 88.3 73.0 32.4 83.2 86.7 53.6 55.4 58.8 75.1 79.8 79.5\nViT-L/14\n95.2 98.1 87.4 76.6 81.5 90.7 68.4 81.9 95.2 96.3 99.1 99.0 71.1 99.8 97.0 95.5 93.2 72.7 41.8 85.3 90.6 60.4 57.4 62.0 81.6 84.4 83.1\nCLIP Zero-Shot\nRN50\n80.6 71.5 42.0 34.5 59.8 54.3 16.6 41.2 85.8 76.7 66.2 58.1 39.3 94.2 40.3 53.5 35.1 10.8 15.4 61.4 63.1 30.8 20.8 56.2 55.6 59.8 50.9\nRN101\n83.6 80.8 48.8 35.8 59.2 61.1 18.6 43.5 86.9 77.5 65.4 50.9 44.0 96.8 30.8 58.7 37.6 33.4 16.9 58.2 61.2 33.2 24.8 52.8 64.2 62.3 53.4\nRN50x4\n86.9 79.4 49.8 39.3 62.6 67.0 20.5 48.5 88.8 78.3 69.9 48.7 48.3 96.6 31.5 60.5 36.2 31.6 20.4 53.5 64.6 35.5 19.4 51.2 66.8 66.2 54.7\nRN50x16\n90.6 81.4 52.6 45.8 65.0 73.3 27.2 53.2 90.1 81.0 72.0 67.8 55.7 97.8 41.9 64.7 39.8 31.3 24.4 62.3 68.7 39.9 20.2 51.4 67.6 70.7 59.1\nRN50x64\n92.1 85.1 60.9 50.6 67.1 75.9 30.0 53.4 93.7 83.0 76.0 85.7 60.8 98.3 57.9 70.8 48.0 36.2 29.8 55.5 73.0 43.8 25.2 59.6 70.7 73.9 63.7\nViT-B/32\n83.9 89.9 65.1 37.1 62.9 59.7 19.4 43.8 87.3 81.6 66.5 48.6 48.0 97.1 44.2 61.0 32.7 29.6 17.2 60.7 64.1 35.1 23.2 52.0 58.5 63.4 55.1\nViT-B/16\n88.8 90.8 68.3 42.1 65.2 64.7 24.0 45.0 89.1 81.0 71.5 51.9 52.9 98.2 54.1 65.6 43.5 21.7 22.9 53.9 68.9 39.4 23.6 54.4 60.6 68.4 58.1\nViT-L/14\n93.1 95.6 78.2 49.4 68.0 77.8 31.6 55.3 93.6 83.3 79.1 76.2 56.3 99.4 61.2 70.9 50.6 24.9 31.9 51.2 75.4 47.7 20.8 55.0 68.9 75.5 64.3\nK-Means 1-space\nDINOv2\n72.3 66.7 70.3 47.1 57.4 25.4 20.3 50.1 78.0 78.5 98.7 46.8 33.7 40.8 64.7 66.6 23.8 50.1 8.9 59.8 68.7 34.4 19.8 50.4 52.2 62.3 51.8\nRN50\n47.2 54.9 25.9 29.8 42.3 31.8 20.6 41.2 39.9 67.3 69.2 51.2 25.5 89.8 54.4 55.2 27.7 47.3 8.8 64.9 52.1 17.4 27.4 58.0 53.9 33.3 43.7\nRN101\n54.1 69.3 34.9 34.0 46.9 40.0 22.9 43.9 49.5 70.9 70.5 48.9 26.7 96.9 51.8 61.6 26.4 48.6 9.4 64.2 59.7 21.2 27.6 57.2 55.0 39.4 47.4\nRN50x4\n62.6 60.5 30.6 38.2 49.1 42.7 25.4 47.3 53.5 75.4 76.5 45.5 27.6 83.8 52.8 61.4 25.3 48.1 10.4 61.0 59.7 21.8 26.4 57.6 55.8 41.9 47.7\nRN50x16\n70.2 66.4 34.7 42.8 49.0 47.3 30.0 49.2 59.3 72.2 78.5 59.2 25.6 94.7 59.4 69.5 35.4 49.1 11.0 64.3 65.0 24.0 23.4 56.6 55.1 45.8 51.4\nRN50x64\n75.9 67.9 36.8 46.2 51.7 49.8 33.9 49.5 56.0 78.5 82.5 67.0 26.4 94.7 60.4 73.2 35.8 49.0 11.2 63.3 67.2 25.4 23.6 57.8 55.0 50.9 53.4\nViT-B/32\n58.7 75.8 40.9 31.8 50.4 35.1 22.1 43.7 42.6 77.2 76.3 57.5 26.0 94.5 63.3 66.0 32.7 48.4 9.4 63.2 61.3 22.4 26.6 57.6 54.6 37.9 49.1\nViT-B/16\n72.2 78.2 46.9 38.5 52.4 42.5 27.9 47.3 49.7 78.8 81.6 54.9 27.7 94.9 73.2 72.8 34.6 50.1 10.8 63.2 64.1 25.3 24.8 59.2 55.2 43.1 52.7\nViT-L/14\n82.5 83.5 51.9 46.1 55.4 52.2 32.9 50.5 72.2 82.4 89.5 66.6 31.2 86.4 71.7 73.7 45.9 48.8 12.3 63.3 74.0 32.5 26.4 60.8 51.7 51.8 57.5\nK-Means 2-spaces\nRN50\n71.5 67.5 69.4 46.5 56.9 25.4 20.2 50.1 78.0 80.2 99.1 46.8 33.7 49.4 64.7 66.7 23.6 50.1 9.1 59.8 70.8 34.5 19.4 50.4 52.2 64.2 52.3\nRN101\n70.1 67.5 69.9 48.0 56.7 24.6 20.1 50.1 78.0 78.5 97.6 46.8 33.7 40.8 64.6 65.1 24.4 50.1 9.1 59.8 69.3 34.7 19.4 50.4 52.2 62.4 51.7\nRN50x4\n70.7 67.5 70.9 47.4 57.1 24.8 19.6 50.2 78.0 80.2 98.6 46.8 33.7 49.5 64.7 65.4 23.6 50.1 9.1 59.8 71.0 34.3 19.8 50.4 52.2 62.0 52.2\nRN50x16\n72.2 67.5 72.6 46.8 56.8 25.1 19.4 50.2 77.1 79.8 99.5 46.9 33.7 49.2 64.6 66.7 23.2 50.1 9.2 59.8 72.1 34.6 19.4 50.4 52.2 63.1 52.4\nRN50x64\n71.7 63.7 69.9 47.4 56.5 25.4 20.1 48.8 78.5 78.9 98.0 46.9 33.6 50.2 64.7 62.0 24.4 50.1 9.2 59.8 69.2 34.5 19.4 50.4 52.2 62.6 51.8\nViT-B/32\n69.7 76.4 69.3 48.4 57.5 24.8 19.9 50.1 80.1 76.0 98.5 46.9 33.6 55.0 64.8 65.3 23.4 50.0 9.1 59.8 67.8 34.0 20.4 50.4 52.3 62.5 52.5\nViT-B/16\n71.5 76.4 71.8 48.3 55.5 26.2 20.7 47.2 79.0 79.0 97.1 46.9 33.7 55.5 64.9 63.0 24.4 50.1 9.3 59.8 71.7 34.5 20.6 50.6 52.2 62.6 52.8\nViT-L/14\n78.8 85.9 75.1 53.0 60.6 35.6 21.9 49.0 79.3 87.4 95.6 69.0 33.1 84.8 84.7 73.9 31.3 50.5 9.9 61.7 73.3 37.7 23.0 50.6 52.1 64.9 58.6\nTURTLE 1-space\nDINOv2\n78.9 99.3 87.1 66.7 60.3 31.2 23.5 55.2 82.2 81.4 99.0 69.0 32.5 72.3 93.8 73.5 23.4 41.6 9.0 50.7 74.5 35.1 22.4 52.6 51.6 69.1 59.1\nRN50\n65.0 57.2 30.8 34.1 50.0 37.0 23.0 48.5 51.2 75.7 82.7 62.1 29.5 96.6 73.3 67.9 34.3 41.4 8.9 68.8 61.4 21.6 26.6 57.0 53.5 41.1 50.0\nRN101\n74.9 71.6 40.2 38.2 54.6 46.3 24.2 52.4 66.7 83.1 88.1 56.6 26.4 98.0 69.1 74.2 36.4 39.6 9.5 71.0 65.5 25.1 26.4 57.8 55.0 48.3 53.8\nRN50x4\n80.7 67.7 37.7 42.4 57.9 52.9 26.7 56.4 74.3 83.6 91.6 57.9 26.7 97.8 68.4 77.3 37.6 39.6 10.1 54.7 65.2 26.3 25.6 58.6 55.2 52.2 54.8\nRN50x16\n87.0 75.4 38.7 47.5 58.5 58.7 30.3 55.3 82.0 85.4 91.9 63.2 28.1 98.6 81.2 84.9 43.5 38.7 11.4 50.8 70.1 28.6 26.2 57.2 53.5 56.4 57.8\nRN50x64\n88.3 77.0 42.9 53.7 61.6 64.1 34.0 54.0 85.0 83.7 95.3 78.4 27.9 99.1 80.7 83.4 43.8 41.0 12.3 50.6 73.8 30.5 26.4 58.0 54.2 60.4 60.1\nViT-B/32\n72.4 85.9 45.8 35.2 58.1 42.3 24.0 49.1 62.1 81.5 85.2 80.9 30.3 98.3 69.8 82.2 39.6 40.7 9.8 50.5 67.4 25.4 24.6 57.6 55.2 46.8 54.6\nViT-B/16\n82.4 94.1 54.4 43.1 58.8 52.8 28.6 54.7 71.1 83.6 94.2 73.0 29.1 99.1 82.5 85.6 36.3 39.4 11.1 51.0 71.3 29.2 24.0 58.0 54.1 53.3 58.3\nViT-L/14\n93.1 97.6 68.7 54.2 63.4 66.6 35.9 57.6 90.5 84.3 98.1 66.3 31.0 99.8 93.8 87.6 50.3 39.1 13.7 50.3 78.3 36.8 27.4 59.0 54.6 62.5 63.9\nTURTLE 2-spaces\nRN50\n84.1 96.8 83.0 67.2 65.7 32.6 25.3 57.0 90.9 88.6 99.7 90.8 34.0 99.7 95.5 85.4 32.4 44.2 9.4 50.6 77.7 35.6 25.0 56.0 50.0 66.3 63.2\nRN101\n86.5 98.6 81.1 63.2 64.8 32.8 25.1 61.1 90.7 89.1 99.7 97.1 33.8 99.7 94.8 86.1 36.6 39.0 9.6 50.3 79.0 37.4 24.6 56.2 51.3 67.9 63.7\nRN50x4\n87.6 98.5 79.4 66.3 65.3 47.1 24.3 62.9 93.0 88.5 99.7 97.1 33.9 99.7 95.4 83.6 36.4 41.5 10.1 51.1 77.9 38.0 25.4 54.8 51.3 69.8 64.6\nRN50x16\n90.2 98.7 80.7 62.9 65.6 50.5 25.5 55.9 92.3 88.9 99.7 97.5 32.7 99.7 94.8 86.4 43.4 41.0 10.6 50.5 77.4 39.9 24.4 55.6 50.0 70.8 64.8\nRN50x64\n89.9 99.0 85.3 68.0 66.6 58.8 31.7 61.2 93.3 88.8 99.7 97.4 34.7 99.7 95.1 89.8 41.5 44.8 11.2 50.2 80.7 40.4 25.4 51.0 50.5 72.1 66.4\nViT-B/32\n86.4 99.1 84.6 64.0 65.2 46.0 27.7 56.0 92.8 89.0 99.7 97.2 34.2 99.7 95.6 88.0 38.3 42.0 10.0 52.8 75.6 37.6 23.2 52.6 51.2 68.6 64.5\nViT-B/16\n90.2 99.2 89.0 66.6 65.0 49.8 25.4 57.3 93.0 89.1 99.7 98.0 33.6 99.8 96.0 89.0 35.0 41.4 10.5 52.1 79.5 39.7 25.6 52.2 51.6 70.2 65.3\nViT-L/14\n92.2 99.5 89.9 68.1 67.9 64.6 36.5 57.3 92.3 89.8 99.6 97.8 36.2 99.7 96.6 89.6 48.4 39.4 11.1 52.0 82.3 43.0 24.0 54.2 51.6 72.9 67.6\n21\nLet Go of Your Labels with Unsupervised Transfer\nE. Additional Results on 26 Vision Datasets\nWe show all experimental results of supervised transfer with linear probe, CLIP zero-shot transfer, and TURTLE unsupervised\ntransfer on 26 vision datasets in Table D1. The linear probe performance of DINOv2 ViT-g/14 is also included for reference.\nAs indicated in the table, TURTLE achieves strong unsupervised transfer performance across various datasets and models.\nFor example, as illustrated in Figure E1, TURTLE 1-space CLIP ViT-L/14 surpasses the corresponding CLIP zero-shot\ntransfer on 13 out of 26 datasets. When trained with multiple representations (i.e., using CLIP models and DINOv2\nViT-g/14), TURTLE 2-spaces achieves superior performance on most datasets compared to TURTLE 1-space. Remarkably,\non datasets such as MNIST and CIFAR100, the absolute improvement is 31% and 21%, indicating the effectiveness of\nTURTLE in combining the knowledge of multiple foundation models. Furthermore, as shown in Figure 5, TURTLE trained\nwith CLIP ViT-L/14 and DINOv2 ViT-g/14 outperforms CLIP zero-shot on 15 out of 26 datasets.\nIn addition, we compare the performance of TURTLE to supervised linear probe using single representation space in\nFigure 6. It can be seen that there exists a strong positive correlation between the performance of unsupervised transfer\nand supervised linear probe. Figure E2 provides the analysis of TURTLE 2-spaces trained using CLIP ViT-L/14 and\nDINOv2 ViT-g/14, indicating that the performance of TURTLE 2-spaces is also strongly correlated with the average linear\nprobe performance. Overall, these results suggest that TURTLE could potentially benefit from the improved quality of\nrepresentations, as measured by supervised linear probe.\nFinally, it’s worth noting that TURTLE 2-spaces might underperform TURTLE 1-space on some datasets, as shown in\nTable D1. We hypothesize that the discrepancy might stem from the suboptimality of DINOv2 representations for the\ntasks heavily related to semantic comprehension, such as semantic analysis (Rendered SST, HatefulMemes), traffic sign\nrecognition (GTSRB), geolocation (Country211) and object counting (CLEVR). Since DINOv2 is pre-trained with self-\nsupervised objective (Oquab et al., 2023), the learned features might not be directly transferable to these semantic-intensive\ntasks. Such trend could also be observed by the linear probe performance, where CLIP ViT-L/14 outperforms DINOv2\nViT-g/14 by a large margin on the Rendered SST2, CLEVR, Country211, GTSRB and FER2013. Therefore, incorporating\nDINOv2 representations might not yield optimal results for these specific datasets.\n20\n10\n0\n10\n20\n30\n-25.4\n-18.2\n-14.2\n-13.0\n-11.2\n-10.9\n-9.8\n-9.5\n-4.6\n-3.2\n-0.9\n-0.3\n+0.0\n+0.4\n+1.0\n+2.0\n+2.2\n+2.9\n+4.0\n+4.2\n+4.7\n+6.6\n+14.2\n+16.7\n+19.1\n+32.6\nFER2013\nCountry211\nSST2\nImageNet\nStanfordCars\nKinetics700\nMNIST\nCIFAR100\nSUN397\nOxfordPets\nPatchCamelyon\nGTSRB\nFood101\nSTL10\nCaltech101\nCIFAR10\nDTD\nUCF101\nHatefulMemes\nFGVCAircraft\nBirdsnap\nCLEVRCounts\nKITTI Distance\nRESISC45\nFlowers102\nEuroSAT\n Score (%)\nTURTLE Unsupervised Transfer vs. CLIP Zero-shot Transfer\nFigure E1. Using the same representation space, TURTLE\noutperforms CLIP zero-shot classifier on 13 out of 26\ndatasets. TURTLE is trained with CLIP ViT-L/14 and does\nnot require any supervision. CLIP zero-shot utilizes the same\narchitecture, but requires the additional text encoder and de-\nscription of visual categories.\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nAverage Linear Probe Accuracy on CLIP and DINO (%)\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nTURTLE Accuracy (%)\nFood101\nCIFAR10\nCIFAR100\nBirdsnap\nSUN397\nStanfordCars\nFGVCAircraft\nDTD\nOxfordPets\nCaltech101\nFlowers102\nMNIST\nFER2013\nSTL10\nEuroSAT\nRESISC45\nGTSRB\nKITTI Distance\nCountry211\nPatchCamelyon\nUCF101\nKinetics700\nCLEVRCounts\nHatefulMemes\nSST2\nImageNet\n=0.88, p = 2.3 × 10\n9\nFigure E2. Unsupervised transfer learning performance is cor-\nrelated with supervised linear probe performance. The per-\nformance of TURTLE 2-spaces is strongly correlated with the\naverage performance of linear probe using CLIP ViT-L/14 and DI-\nNOv2 ViT-g/14 (ρ = 0.88, p = 2.3×10−9 for two-sided Pearson\ncorrelation coefficient).\n22\nLet Go of Your Labels with Unsupervised Transfer\nF. Results with Scaling Parameters.\nWe provide the complete results for TURTLE on each dataset for multiple architectures and sizes scaling in the number of\nparameters in Table D1. Specifically, we consider 8 CLIP models trained in Radford et al. (2021) with scaling model sizes\nof ResNets (RN50, RN101, RN50x4, RN50x16, RNx64) and Vision Transformers (ViT-B/32, ViT-B/16, ViT-L/14). For\neach CLIP representation, we train TURTLE 1-space with the aforemention representation spaces, and TURTLE 2-spaces\nwith DINOv2 ViT-g/14 used as the second representation space.\nFigure F1 and Figure F2 show the average performance of TURTLE trained with different CLIP ResNets and CLIP Vision\nTransformers, and compare it with the performance of the CLIP zero-shot transfer. As indicated by the plots, the performance\nof TURTLE 1-space and TURTLE 2-spaces smoothly improves as the model compute increases. Moreover, although being\nfully unsupervised, TURTLE 1-space achieves competitive performance (i.e., with less than 1 point difference) compared to\nthe zero-shot transfer for RN50, RN101, RN50x4, ViT-B/32, ViT-B/16 and ViT-L/14. Using RN50x16 and RN50x64, the\nperformance TURTLE 1-space becomes a little bit worse than zero-shot transfer. However, with the additional DINOv2\nrepresentations, TURTLE 2-spaces consistently outperforms zero-shot transfer for all the models by a large margin. For\nexample, TURTLE 2-spaces CLIP ViT-L/14 is on average 4% better than zero-shot transfer. For completeness, Figure F3\nand Figure F4 also provide the performance of TURTLE and CLIP zero-shot transfer on each individual dataset.\nOverall, the performance of TURTLE follows a similar scaling trend as the zero-shot transfer (Radford et al., 2021).\nFurthermore, TURTLE can effectively combine the knowledge of multiple foundation models to further improve the\nperformance of unsupervised transfer.\nRN50\nRN101\nRN50x4\nRN50x16\nRN50x64\n50.0\n52.5\n55.0\n57.5\n60.0\n62.5\n65.0\nAverage Accuracy (%)\nTransfer performance on 26 datasets\nCLIP Zero-shot\nTURTLE 1-space\nTURTLE 2-spaces\nFigure F1. Performance of TURTLE and CLIP zero-shot\naveraged on 26 vision datasets using ResNets. TURTLE\n2-spaces uses DINOv2 ViT-g/14 as the second representation\nspace.\nViT-B/32\nViT-B/16\nViT-L/14\n54\n56\n58\n60\n62\n64\n66\n68\nAverage Accuracy (%)\nTransfer performance on 26 datasets\nCLIP Zero-shot\nTURTLE 1-space\nTURTLE 2-spaces\nFigure F2. Performance of TURTLE and CLIP zero-shot\naveraged on 26 vision datasets using Vision Transformers.\nTURTLE 2-spaces uses DINOv2 ViT-g/14 as the second\nrepresentation space.\n23\nLet Go of Your Labels with Unsupervised Transfer\n65\n70\n75\n80\n85\n90\nAccuracy (%)\nFood101\n60\n70\n80\n90\n100\nCIFAR10\n30\n40\n50\n60\n70\n80\nCIFAR100\n40\n50\n60\nBirdsnap\n50\n55\n60\n65\nSUN397\n40\n50\n60\n70\nStanfordCars\n20\n25\n30\nAccuracy (%)\nFGVCAircraft\n45\n50\n55\n60\nDTD\n50\n60\n70\n80\n90\nOxfordPets\n77.5\n80.0\n82.5\n85.0\n87.5\nCaltech101\n70\n80\n90\n100\nFlowers102\n50\n60\n70\n80\n90\nMNIST\n30\n40\n50\n60\nAccuracy (%)\nFER2013\n94\n95\n96\n97\n98\n99\n100\nSTL10\n40\n60\n80\nEuroSAT\n60\n70\n80\n90\nRESISC45\n32.5\n35.0\n37.5\n40.0\n42.5\n45.0\n47.5\nGTSRB\n10\n20\n30\n40\nKITTI Distance\n10\n15\n20\n25\n30\nAccuracy (%)\nCountry211\n50\n55\n60\n65\n70\nPatchCamelyon\nRN50\nRN101\nRN50x4 RN50x16 RN50x64\n65\n70\n75\n80\nUCF101\nRN50\nRN101\nRN50x4 RN50x16 RN50x64\n25\n30\n35\n40\nKinetics700\nRN50\nRN101\nRN50x4 RN50x16 RN50x64\n20\n22\n24\n26\nCLEVRCounts\nRN50\nRN101\nRN50x4 RN50x16 RN50x64\n52\n54\n56\n58\n60\nHatefulMemes\nRN50\nRN101\nRN50x4 RN50x16 RN50x64\n50\n55\n60\n65\n70\nAccuracy (%)\nSST2\nRN50\nRN101\nRN50x4 RN50x16 RN50x64\n40\n50\n60\n70\nImageNet\nCLIP Zero-shot\nTURTLE 1-space\nTURTLE 2-spaces\nFigure F3. Per dataset performance of TURTLE and CLIP zero-shot using ResNets. TURTLE 2-spaces uses DINOv2 ViT-g/14 as the\nsecond representation space.\n75\n80\n85\n90\nAccuracy (%)\nFood101\n87.5\n90.0\n92.5\n95.0\n97.5\n100.0\nCIFAR10\n50\n60\n70\n80\n90\nCIFAR100\n40\n50\n60\nBirdsnap\n58\n60\n62\n64\n66\n68\nSUN397\n50\n60\n70\nStanfordCars\n20\n25\n30\n35\nAccuracy (%)\nFGVCAircraft\n45.0\n47.5\n50.0\n52.5\n55.0\n57.5\nDTD\n65\n70\n75\n80\n85\n90\n95\nOxfordPets\n82\n84\n86\n88\n90\nCaltech101\n70\n80\n90\n100\nFlowers102\n50\n60\n70\n80\n90\n100\nMNIST\n30\n35\n40\n45\n50\n55\nAccuracy (%)\nFER2013\n97.5\n98.0\n98.5\n99.0\n99.5\nSTL10\n50\n60\n70\n80\n90\nEuroSAT\n60\n65\n70\n75\n80\n85\n90\nRESISC45\n35\n40\n45\n50\nGTSRB\n25\n30\n35\n40\nKITTI Distance\n10\n15\n20\n25\n30\nAccuracy (%)\nCountry211\n50\n52\n54\n56\n58\n60\nPatchCamelyon\nViT-B/32\nViT-B/16\nViT-L/14\n65\n70\n75\n80\nUCF101\nViT-B/32\nViT-B/16\nViT-L/14\n25\n30\n35\n40\n45\nKinetics700\nViT-B/32\nViT-B/16\nViT-L/14\n22\n24\n26\nCLEVRCounts\nViT-B/32\nViT-B/16\nViT-L/14\n52\n54\n56\n58\nHatefulMemes\nViT-B/32\nViT-B/16\nViT-L/14\n55\n60\n65\nAccuracy (%)\nSST2\nViT-B/32\nViT-B/16\nViT-L/14\n50\n55\n60\n65\n70\n75\nImageNet\nCLIP Zero-shot\nTURTLE 1-space\nTURTLE 2-spaces\nFigure F4. Per dataset performance of TURTLE and CLIP zero-shot using Vision Transformers. TURTLE 2-spaces uses DINOv2\nViT-g/14 as the second representation space.\n24\nLet Go of Your Labels with Unsupervised Transfer\nG. Imbalanced Dataset and Entropy Regularization\nFollowing Xu et al. (2004); Van Gansbeke et al. (2020); Gadetsky & Brbi´c (2023), we use entropy regularization (11) to\nprevent the task encoder from producing trivial solutions, i.e., assigning all the samples to a single class. By default we\nset the regularization strength to γ = 10 for all the experiments. Note that the optimal solution of (11) is to produce a\nlabeling with the equal number of samples for each class. However, some of the datasets are not class balanced. In this\ncase, a strong entropy regularization might hurt the learning process. To understand the effect of the entropy regularization,\nwe show the average performance of TURTLE with γ ∈{0, 1, 3, 5, 10} separately on the imbalanced datasets (Birdsnap,\nFER2013, GTSRB, KITTI, HatefulMemes), and the rest 21 balanced datasets in Figure G1. The results indicate that the\nentropy regularization is generally helpful since η = 0 might lead to trivial solutions. Furthermore, for the balanced datasets,\nTURTLE is robust to the choice of the regularization hyperparameter. While for the imbalanced datasets, a properly chosen\nregularization parameter could further improve the performance.\n0\n1.0\n3.0\n5.0\n10.0\nRegularization Parameter \n30\n40\n50\n60\n70\nAccuracy (%)\nBalanced Datasets\n0\n1.0\n3.0\n5.0\n10.0\nRegularization Parameter \n30\n35\n40\n45\n50\nAccuracy (%)\nImbalanced Datasets\nFigure G1. Ablation of the entropy regularization. We show the average performance for class imbalanced datasets (Birdsnap, FER2013,\nGTSRB, KITTI, HatefulMemes) and class balanced datasets (the rest 21 datasets) for the different entropy regularization strength.\nH. TURTLE Trained and Evaluated on Test Split\nIn previous experiments, we train TURTLE on the training split Dtr and evaluate the clustering accuracy on the test split\nDte. In this section, to study the performance of TURTLE in low data regime, we consider the setting when training and\nevaluating TURTLE directly on the test split. Figure H1 compares the performance of TURTLE trained on Dtr and TURTLE\ntrained on Dte on the 26 datasets. Both settings are evaluated on the test split. As shown in the plot, TURTLE trained on\nDte achieves nearly identical performance as TURTLE trained on Dtr for 24 out of 26 datasets, except Caltech101 and\nFlowers102. We found the discrepancy might be attributed to the fact that the Caltech101 and Flowers102 have balanced\ntraining split but imbalanced test split. Overall, the results suggest that TURTLE does not require a large amount of data to\nperform successful unsupervised transfer.\n20\n40\n60\n80\n100\nFood101\nCIFAR10\nCIFAR100\nBirdsnap\nSUN397\nStanfordCars\nFGVCAircraft\nDTD\nOxfordPets\nCaltech101\nFlowers102\nMNIST\nFER2013\nSTL10\nEuroSAT\nRESISC45\nGTSRB\nKITTI Distance\nCountry211\nPatchCamelyon\nUCF101\nKinetics700\nCLEVRCounts\nHatefulMemes\nSST2\nImageNet\nTURTLE trained on\ntrain\ntest\nFigure H1. TURTLE trained on test split achieves similar performance as TURTLE trained on training split for 24 out 26 of\ndatasets. Results are both evaluated on the test split. The discrepancy of Caltech101 and Flowers102 is because that they are balanced on\ntraining split but imbalanced on test split.\n25\nLet Go of Your Labels with Unsupervised Transfer\nI. Additional Analysis on Fine-grained Classification\nIn previous sections, we have evaluated the performance of TURTLE on 26 datasets, including 6 fine-grained classification\ndatasets: Food101, Flowers102, Birdsnap, StanfordCars, FGVCAircraft and OxfordPets. According to the results from Ta-\nble D1 and Figure 5, TURTLE outperforms CLIP zero-shot transfer on 3 datasets (Flowers102, Birdsnap and FGVCAircraft)\nand performs comparably on 2 datasets (Food101 and OxfordPets). The results indicate that TURTLE remains effective for\nthe task of fine-grained classification.\nTo further study the dependence between number of classes and the performance of TURTLE, we perform the additional\nexperiments on 4 datasets from the BREEDS benchmark (Santurkar et al., 2021). These datasets are the subsets of ImageNet\nthat contain both coarse and fine-grained labels. We run TURTLE for each dataset to infer the coarse and fine grained labels\nseparately by specifying the ground truth number of classes for each case. The statistics for each dataset and TURTLE’s\nperformance are provided in Table I1.\nWe observe that TURTLE performs worse on the fine-grained classification compared to the coarse-grained classification\non LIVING-17. The result is expected since fine-grained classification is considered to be a more difficult task. However,\non ENTITY-13, ENTITY-30 and NONLOVING-26, the performance of the fine-grained classification is better than the\nperformance of the coarse-grained classification. We hypothesize that this might be due to the high intra-variance of coarse\nclasses, which was also reported in the previous works on unsupervised image classification (Van Gansbeke et al., 2020). In\nconclusion, the results suggest that the performance of TURTLE is not largely affected by the granularity of the dataset, but\nrather by the quality of the representations as indicated by Figure 6 and Figure 7.\nTable I1. BREEDS benchmark and performance of TURTLE. TURTLE column represents the performance of TURTLE on the given\ndataset with coarse or fine-grained taxonomy.\nDataset\n# Coarse classes\n# Fine classes\nTrain size\nTest size\nTURTLE\nCoarse\nFine\nENTITY-13\n13\n260\n334,712\n13,000\n73.1\n85.5\nENTITY-30\n30\n240\n307,828\n12,000\n79.5\n82.4\nLIVING-17\n17\n68\n88,400\n3,400\n96.0\n87.0\nNONLIVING-26\n26\n104\n132,765\n5,200\n76.9\n78.9\n26\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2024-06-11",
  "updated": "2024-06-11"
}