{
  "id": "http://arxiv.org/abs/2406.07259v1",
  "title": "Scientific Computing with Large Language Models",
  "authors": [
    "Christopher Culver",
    "Peter Hicks",
    "Mihailo Milenkovic",
    "Sanjif Shanmugavelu",
    "Tobias Becker"
  ],
  "abstract": "We provide an overview of the emergence of large language models for\nscientific computing applications. We highlight use cases that involve natural\nlanguage processing of scientific documents and specialized languages designed\nto describe physical systems. For the former, chatbot style applications appear\nin medicine, mathematics and physics and can be used iteratively with domain\nexperts for problem solving. We also review specialized languages within\nmolecular biology, the languages of molecules, proteins, and DNA where language\nmodels are being used to predict properties and even create novel physical\nsystems at much faster rates than traditional computing methods.",
  "text": "Scientific Computing with Large Language Models\nChristopher Culver, Peter Hicks, Mihailo\nMilenkovic, Sanjif Shanmugavelu, and Tobias Becker\nMaxeler Technologies, a Groq Company\n1.\nABSTRACT\nWe provide an overview of the emergence of large language models for scientific computing\napplications. We highlight use cases that involve natural language processing of scientific\ndocuments and specialized languages designed to describe physical systems. For the former,\nchatbot style applications appear in medicine, mathematics and physics and can be used\niteratively with domain experts for problem solving. We also review specialized languages\nwithin molecular biology, the languages of molecules, proteins, and DNA where language\nmodels are being used to predict properties and even create novel physical systems at much\nfaster rates than traditional computing methods.\n2.\nINTRODUCTION\nLanguage is a key component of human communication that has greatly enhanced the\nevolution of the species. Using language, humans are able to express ideas, exchange in-\nformation, and collectively make plans at scales unlike any other in the animal kingdom.\nSince the dawn of computing, humans have strived to create AI to automate human tasks.\nRecently, large language models (LLMs) demonstrated the ability to generate text that is\noften indistinguishable from human-written text. This enables them to aid a wide range\nof language-oriented tasks such as customer support, document summarization, language\ntranslation, coding assistance or content generation.\nAs with most AI applications, using LLMs requires two distinct phases: training and\ninference. Training is the process of feeding large amounts of text data from books, articles,\nand websites into the model in order to embed syntactical and semantic rules as well as\na wide range of knowledge into the model. This process has to occur only once but is\ncomputationally demanding, taking weeks to months on high performance computing (HPC)\narXiv:2406.07259v1  [cs.CL]  11 Jun 2024\n2\nsystems. Inference refers to using a trained model: A query is provided to the model and it\nwill predict an output sequence that will provide a suitable answer to the query. A single\ninference is less computationally demanding than training, but the challenge lies in that\npotentially thousands to millions of users interacting with a model will all require inferences\nto be computed in fractions of a second. Until recently, LLMs has been hampered by the\nextreme combinatorial complexity that arises from the large vocabulary of human language\nand its complex rules. This has made it impossible to generate longer, high quality text\nsequences. However, a number of recent innovations have led to a breakthrough in LLM\nperformance. Modern LLMs are based on a type of artificial neural network called the\ntransformer [1], one of the most important innovations in the field of AI in recent years.\nCentral to the transformer architecture is a technique called attention that allows the model\nto capture long-range dependencies while limiting the combinatorial complexity in longer\ntext sequences. At the same time, models have also increased from millions to billions of\nparameters, giving them enough capacity to not only understand the syntax, but also the\nsemantics of natural language. Finally, the introduction of new AI inference accelerators, such\nas the Groq Language Processing Unit (LPU TM) [2], has led to significant improvements in\nthroughput and latency during inference. This has made LLMs usable without encountering\ntedious delays during queries, paving the way for a new design space of real time applications.\nMany current LLM use cases focus on direct and simple language-related tasks such as\nquestion answering, customer support, chat bots, etc. There is potential for LLMs to be useful\nin a wider range of application areas including scientific computing. Here, we envision broadly\ntwo different approaches: firstly, LLMs could assist scientific processes by being used as a tool\non scientific text, e.g., summarization of research papers. Scientific language is noticeably\ndistinct from ordinary language due to using complex noun combinations and a specialized\nvocabulary. Within science, different disciplines employ language in different ways, it was\nshown that biology has imprecise yet densely packed language while physics typically has\nthe opposite [3]. The description of sciences through natural language is the mechanism\nthe enables humans to collectively solve problems and amass knowledge. Secondly, it is also\nconceivable to treat processes in physics, biology, or chemistry as specialized languages.\nMolecules, proteins, and DNA all can be seen as languages made up of a specific character\nset with syntactic rules and semantic meaning completely unlike any ordinary language. It\nis therefore possible to train special language models just for the purpose of understanding\n3\nDNA as a language. For general languages applied to scientific disciplines and the specialized\nlanguages encoding the behavior of physical processes its natural to apply LLMs to these\nlanguage tasks.\n3.\nSEQUENCE MODELLING ARCHITECTURES\nOriginally, sequence modelling tasks were done by RNNs [4], and LSTMs [5], until the\nintroduction of the Transformer model [1], which are now ubiquitous for sequence modelling\ntasks. In this section, we give a brief overview of the historical and current state-of-the-art\nmodels in this area.\nThe transformer architecture consists of stacked layers of feed-forward networks and at-\ntention blocks. The attention mechanism works as follows: Given the inputs Q, K, V ∈RN×d,\nit computes the outputs O according to O = softmax(QKT)V , ignoring a normalization fac-\ntor. We can interpret the attention mechanism as measuring the similarity between a set of\nd queries and keys (d is the length of the input sequence), and retrieving a weighted sum of\nthe values corresponding to those keys based on the similarity scores. There are three main\nvariants of the transformer architecture: encoder models, which process input sequences in\nparallel, decoder models, which generate sequences sequentially, and encoder-decoder mod-\nels, which take a sequence of inputs and generate a new sequence of outputs.\nOriginally, transformer encoder-only models were popularized by the BERT architec-\nture [6]. Encoders can be trained by masked language modelling - removing some words and\npredicting them based on the surrounding context during training. Other methods such as\ncontrastive objectives like CLIP [7] based models, or classification objectives with sentence\ntransformers [8] exist. Encoder style language models have been used for a wide variety\nof applications, such as clustering and classification tasks, aligning different modalities in\nmodels like LLaVA [9], Stable Diffusion [10], and retrieval augmented generation (RAG) [11].\nAnother popular framework is auto-regressive language modelling, which generates a\nsequence x by predicting a sequence of symbols based on the previous symbols in the sequence\none at a time, i.e. p(x) = Qn\ni=1 p(sn|s1, . . . , sn−1). The GPT series of models [12], which are\ndecoder only transformers, work exactly this way and can be trained on large amounts of\nunlabelled textual data.\nTransformer-based language models have exhibited predictable improvement in perplex-\n4\nity, ability to predict dataset information, both by increasing the size of the model as well\nas the amount of data used for training [13]. This has led to a drastic increase in the ca-\npabilities of recent language models, with models spanning trillions of parameters including\nGPT4 [14],Gemini [15], Llama [16, 17] and Mixtral [18] achieving state of the art performance\nin many language modelling tasks.\nOne of the challenges with pre-trained language models is that the information embedded\ninto the model if fixed and may be lacking specialised domain knowledge. One possible\nsolution is fine-tuning the model: it is a much more lightweight training step where a model\ncan be specialised for certain tasks, significantly improving the performance of the model [19].\nAnother important specialisation technique is retrieval augmented generation (RAG) where\ninformation from a local database is fed into the model, which enables the model to operate\non local information which is not part of the model itself [11].\nA current limitation of transformers is the quadratic time complexity of the attention\nmechanism, which poses practical limits on sequence lengths and limits a model’s ability\nto understand long distance context. This is driving research into more efficient operations\nto replace the attention mechanism, such as the Hyena operator [20], sparse matrix mul-\ntiplications such as MonarchMixer [21], and most recently state space models like S4 and\nMamba [22]. Newer architectures achieve very good performance on numerous tasks and\neven achieve lower perplexity for language modelling compared to transformer models with\nthe same number of parameters. There are still some tasks such as the copying task [23],\nleading to combinations of attention mechanism with more efficient primitives, such as SSM\nblocks [24] which were recently scaled up to sizes comparable to leading transformer-based\nmodels [25].\nNotably, the architectures mentioned above are not limited to only natural language\nrelated tasks, but can also model other discrete sequential data of different modalities such\nas audio [26, 27], images [28, 29], video [30], and the “language” of biology.\n4.\nMOLECULAR BIOLOGY\nMolecular biology is the study of living organisms through the interaction of molecules,\nthe building blocks of all materials. If we can understand how specific molecules dictate\nthe interactions between proteins, then drugs that target specific diseases or viruses can be\n5\ndesigned more readily as key ingredients will be identified. Unfortunately, this link between\nmolecules and their biological utility is so complex that serendipitous drug discovery is still\nrelatively common [31]. Part of this is due to the intractably complex chemical space of drug\nlike molecules (about 1033) that are estimated to be synthesizable [32]. Immense computing\nand automation efforts are required to explore only a tiny fraction of this domain. In the\nfollowing we highlight some applications of LLMs to the language of molecules, proteins,\nand DNA, and refer to Ref. [33] for a thorough survey.\n4.1.\nMolecules\nMolecules are a group of atoms held together by chemical bonds, attractive forces between\nthe constituent atoms. Before trying to physically synthesize a molecule, computations are\nperformed on a candidate molecule to predict its physical properties to ensure it meets\nthe target criteria. These computations are performed using molecular dynamics (MD) or\ndensity functional theory (DFT) simulations which require HPC resources. MD simulations\ncompute the positions of up to billions of atoms while DFT computations simulate only the\nelectrons of the system using a mean field approach. While there exist efforts to use neural\nnetworks to accelerate such algorithms, we focus here on the application of transformers to\nthe language of molecular structure which bypasses these expensive algorithms.\nThe atomic content and even physical structure of a molecule can be represented precisely\nby a 1-D string, i.e., the chemical formula for water is H2O. A string encoding in principle\nencompasses all of the properties of the molecule: size, shape, toxicity, 3-D structure, etc\nsince these are the only building blocks for molecules. Traditionally the properties have to\nbe explicitly computed through the atomic interactions using MD or DFT solvers. With\nthe recent advancements of transformers to understand not just syntactic but semantic\ninformation, its natural to wonder to try and employ transformers to learn the semantic\nrelationship between atomic sequences and physical properties. The chemical formula above\nis a bit naive and a more complete molecular description can be specified by, for example,\nSMILES [34] or SELFIES [35]. These molecular languages map both the atoms and chemical\nbonds to characters, for example in SMILES CO2 is represented as C(=O)=O where “=”\nrepresents a double bond.\nThe production of enough molecular data to train LLMs has only become available over\n6\nthe last two decades through the use of high-throughput screening (HTS). HTS with robotic\nassistance can currently screen over 100,000 compounds per day producing orders of mag-\nnitude more data than previously possible. There are quite large data sets for both training\nand bench-marking purposes such as PubChem [36] and MoleculeNet [37]. Much of this\ndata is unlabelled which works well with the self-supervised training language models usu-\nally undergo. These data sets are especially useful for validating a model’s ability to predict\nproperties given an atomic string representation.\nEncoder style models are primarily used for molecular property prediction and there are\nmany BERT [38] based models due to the large collections of unlabelled chemical data.\nSMILES-BERT [39] is one such example which is trained using masked language modelling\nwhere input molecular strings will be randomly masked. Applying masking to the pretraining\nphases enables the model to learn a very general embedding of the molecular space and\nthe role different atoms and bonds play. After this fine-tuning is applied for classification\ntasks, SMILES-BERT outperformed other state-of-the-art models in property prediction\nas of the time of its publication. One drawback to BERT style models is they focus too\nheavily on sequence information, causing them to struggle with comprehending molecular\nstructure[33]. Architectures which are also able to learn chemical structure information are\nemerging through specialised transformers, one based on relative position transformers is\nMolFormer [40]. This model was trained on one billion molecules and shown to capture the\nmolecular substructure and spatial interatomic distances. These kinds of advancements are\ncritical to enabling downstream inference tasks like determining a physiological effect from\nquantum mechanical properties of molecules.\nFor designing novel molecules with specific behavior it is common to use GPT like ar-\nchitectures where the transformer will output molecular strings. A pioneering work was\nMolGPT [41] which as far as the authors are aware was the first attempt at using GPT\narchitectures on molecular language. MolGPT is able to be trained on multiple properties\nand then used to generate novel valid and unique atomic configurations with the desired\nbehavior. There have since been several advancements based around this architecture, for\nexample cMolGPT [42] which can be used to design molecules that target specific proteins.\nBy inputting SMILES strings of a target molecule, the generated molecule should interact\nwith, every inference of cMolGPT produces a new molecular sequence, which can be checked\nagainst databases for uniqueness. The cMolGPT model had a 90% unique compound rate\n7\nwhen generating 10,000 valid molecules on three different targets. These generative models\noutput new molecular compounds but not their properties making it important that encoder\nonly models, like BERT style ones above, continue improving their predictive power.\n4.2.\nProteins\nProteins are made up of amino acids, organic molecules composed of specific compounds,\nand perform numerous functions inside living organisms. While there are hundreds of known\namino acids, only 20 are needed to encode the function of proteins and their biological pur-\npose. Proteins can serve as enzymes, send messages between cells, or provide solid structure\nin an otherwise fluid environment. Proteins are created at a molecular level by ribosome\nwhich reads the RNA of a cell and produces a 1-dimensional chain of amino acids. This\nhelps validate the usage of our language of proteins, which are characters representing the\namino acids as a 1-D sequence. Atomic interactions will cause this 1-dimensional chain to\nfold into a 3-D structure after it has been created. The 3-D structure of a protein is directly\nresponsible for its biological function and is in principle encoded in the textual representa-\ntion.\nThere are two traditional methods to protein folding, the task of computing its 3-D\nstructure from its textual encoding. One method is to perform a direct numerical simulation\nbased on the physics of molecular interactions. Another approach is to use an evolutionary\nalgorithm and do a simulation, which starts with a “bad” protein and evolves into a useful\none. There are both HPC and distributed computing solutions for these algorithms, one\nexample of the latter is Folding@Home, which anyone can contribute personal computing\nresources too, and has had tens of thousands of users.\nFor training LLMs on the language of proteins two resources are UniProt [43], which\nis a hub for protein functional information containing both manually and automatically\nannotated proteins, and Big Fantastic Database (BFD) which contains over 2 billion protein\nsequences. A general analysis of transformer architectures applied to datasets in UniProt\nand to BFD was performed by the ProtTrans project [44]. The project showed that certain\narchitectures were able to perform better than state-of-the-art evolutionary methods while\navoiding expensive database searches. Compared to traditional protein sequence algorithms\nthe LLMs were 5-30 times faster, dependent on model architecture, still the entire human\n8\nproteome (20,353 proteins with a median length of 415 amino acids) takes 40 minutes to\nprocess.\nThe first protein folding LLM to accurately predict atomic resolution structure was Al-\nphaFold [45]. They employed a new architecture using multiple sequence alignment, which\nhighlights homologous features that appear between sequences, which is common in protein\nstrings. To compute the structure of a protein with 2,500 residues took 18 hours, only after\nthis process can researchers learn about the protein’s functionality. A more recent model is\nESM-2 [46] which uses transformers with up to 15B parameters and avoids multi sequence\nalignments. Though there is no substantial improvement to protein structure accuracy there\nis an almost 60x inference speedup in comparison to other state-of-the-art models. The fast\ntime to 3-D structure prediction will furthermore guide the understanding of how specific\nproteins have impact at much larger scales [47].\n4.3.\nGenomics\nGenomics and transcriptomics are the study of DNA and RNA respectively. These subject\nareas both aim to deepen our knowledge of the biological macro-molecules they relate to,\nand then apply this knowledge to various downstream tasks. On a high level these tasks are:\nthe understanding of coding-DNA/RNA and its effects on proteins, and the understanding\nof non-coding-DNA/RNA and its effects on gene expression and regulation.\nGiven the vast scale of DNA and RNA sequences, with sequences of the order 3 billion\nnucleotides in the case of humans[48], the analysis side of genomics and transcriptomics\nis largely driven by computational algorithms. Classically, these have been deterministic,\nstatistical, and classical machine learning algorithms, however, as the availability of genomics\nand transcriptomics data [49] has rapidly grown there has been an uptick in the usage of\ndeep learning models in these areas [50]. We can consider DNA and RNA to be the languages\nof life, with their own patterns, grammar and semantic rules. Accordingly, it is no surprise\nthat LLMs, deep learning models typically intended for natural language processing (NLP),\nhave been proven highly effective in the areas of DNA and RNA sequencing analysis.\nOne of the earlier applications of LLMs to DNA was DNABERT [51], a genomics foun-\ndation model able to be finetuned on a variety of downstream tasks. Unlike with natural\nlanguage processing where we loosely tokenize on the word or sub-word level, there are many\n9\nvalid tokenization strategies for DNA and RNA. In the simplest cases tokenization could\nbe done at the single nucleotide level or codon level (3 nucleotide), but more complicated\nheuristics exist. The DNABERT authors choose to train multiple models with differing tok-\nenization techniques, splitting on varying length k-mers, overlapping sub sequences of a given\nlength. Training for a single model took 25 days with a cluster of 8xNVIDIA 2080Ti GPUs\nusing a masked token replacement strategy. Finetuned DNABERT models achieved, at the\ntime, state-of-the-art performance in promoter site, splice site, and transcription binding\nsite prediction which are, broadly speaking, related to gene expression and regulation.\nWhile DNABERT focuses on local aspects of DNA, other works, for example the award\nwinning [52] GenSLM [53], handle entire viral RNA genome sequences. The GenSLM authors\nadopt a codon (3 neuclotide bases) level tokenization strategy, with whole viral genomes of\nthe order 30,000 nucleotides, yielding sequence lengths of the order 10,000. GenSLM was\npretrained on a dataset of over 110 million prokaryotic gene sequences. This base model\nwas then further trained on whole SARS-CoV-2 genomes[53] and adapted for predictive and\ngenerative workloads for early warning of variants of concern (VOCs). In the first case, the\nmodel predicts whether a sampled viral genome is likely to be a VOC, i.e., one that is highly\naggressive or more harmful. The generative iteration of the model is used to create candidate\nSARS-CoV-2 genomes to serve as an early warning for potential VOCs [53]. As the size of\nthe sequences being handled by the model and the size of the training data scaled in this\nproject, so did the hardware requirements.\nDue to the computational challenges that come with the transformer architectures, we\nare seeing advancements driven by challenges faced in natural language processing trickle\ndown into DNA-LLM research. One such example of this is in the introduction of hyena\nlayers to genomics LLMs [54]. The Hyena layer was introduced to handle long context\nNLP problems, an issue parallel to the long sequence lengths found when dealing with\nwhole genomes. HyenaDNA [54] is trained on sequence lengths of 1 million nucleotides, a\nscale much greater than the earlier transformer-based DNA-language models. As well as\nthe increased scale, HyenaDNA has achieved state of the art performance on a number of\ngenomics benchmarks.\nThe focus of DNA/RNA LLMs thus far has primarily been proof of concept, that is to say,\nalthough DNA-LLMs are already achieving state of the art performance on genomics-based\nbenchmarks, they are yet to be adopted as common practice in a clinical setting [55]. This is\n10\nlargely due to barriers such as model explain-ability and the need for approval by bodies such\nas the FDA. In this early proof of concept phase the focus of research has been on the rapid\ntraining and development of new models, the necessity for large powerful hardware systems\nto support the training of these models is clear [38, 53]. However, as DNA-LLM systems move\ninto production the focus will need to shift from fast time-to-product (training) to inference.\nAccordingly, the hardware requirements for this will need to be addressed, looking toward\nrunning the DNA/RNA based LLMs on inference focused hardware systems.\n4.4.\nMedical Language Processing\nIn the medical field there are copious amounts of text written with specialized vocabulary\nfor healthcare workers like doctors and pharmacists. One of the important features of LLMs\nis the speed at which they can retrieve and summarize information, which is orders of\nmagnitude faster than a human would require just to gather the appropriate material. Since\n2018 there has been widespread use of BERT and other related LLMs in medical NLP tasks\nto assist with this problem. Applications of BERT-style transformer models in medicine\nhave included: medical Q&A bots [56], medical text mining/annotation [57], and filtering of\npublic health information [58].\nMore recently research of generative text-to-text LLMs in the medical domain has be-\ncome increasingly popular especially due to the relative ease of model fine-tuning. This has\nallowed for a high throughput of papers [59–61] that investigate the fine-tuning of genera-\ntive LLMs for medical work. Notably, from Google&DeepMind fine-tuning their PaLM [59]\nmodel achieved state-of-the-art performance on multiple biomedical Q&A benchmarks.\nOther works, for example Ref. [62], have investigated prompt tuning and engineering as\nmethods for enhancing the performance of generative LLMs in the medical domain. There\nhave also been efforts to train generative medical LLMs from scratch on medical papers, ab-\nstracts, and clinical guidelines, such as Meditron-70B [63]. Meditron-70B is an open-source\nmodel and was able to achieve accuracy within 10% of the closed-source Med-PaLM-2 [59]\non medical benchmarks, open-source pretrained models such as this are important as they\nopen up new research avenues, that is to say, anyone can access, fine-tune, and or prompt\nengineer these models for new medical applications.\nAnother exciting new area of research in medical LLMs is in combined vision language\n11\nmodels. [64] Vision transformers have already been proven useful in the medical domain,\nfor example, MedVInT [65] is a vision transformer that showed good performance on a\nrange of medical classification tasks such as chest x-ray analysis. As such, combined vision-\nlanguage models similar to the LLaVa [64] model could prove an exciting new area of re-\nsearch. Work [66] is already being done to fine-tune LLaVA models for Q&A on medical\nimages. In the paper [66] the authors use the LLM to dicuss a CT scan, the LLM is able\naccurately describe the location of a lesion in said scan. Due to the visual nature of a\nlarge amount of medical diagnosis vision and languagae models are a very appealing area of\nresearch.\nOne of the major difficulties in using LLMs in a medical context is the fact that they\nhallucinate [67]. In a setting like medicine, where the stakes are high, clinicians cannot\nrisk an LLM presenting them with incorrect information. Accordingly, methods such as\nretrieval augmented generation (RAG) [68], that are known to reduce hallucinations, will be\nof heightened importance in medical language processing tasks.\n5.\nMATH AND PHYSICS\nThe language of mathematics and physics can be described as ordinary language with\nmathematical symbols, but this understates the importance of the specialized vocabulary\nthat would not appear in ordinary text. Mathematical and physics terminology have precise\nmeaning which can be understood with a single sample, which is very different from the\nusual training of LLMs which learn through many examples. There are attempts to guide an\nLLM chatbot to aid researchers through prompt engineering, one such example is Ref. [69].\nHere a chatbot is given specialized prompts to guide the LLM towards following the correct\nanalytic steps for applying the Hartree-Fock method, a way to solve quantum many-body\nproblems. The LLM is able to successfully re-derive 13 of 15 Hartree-Fock Hamiltonians from\nresearch papers with only minor mistakes otherwise. These mistakes can be easily corrected\nby peer review from the chatbots human counterpart. Another example of using general\nLLMs is to guide numerical simulations [70]. In this work an LLM is prompted to generate\na 3-D mesh from a textual description, for example “Create a bar with a square section\ncentered on the end”. Physical simulations can then be run on the generated meshes, either\ntraditional physics solvers or machine learning powered solutions.\n12\nIn mathematics there is work to try and solve problems for which brute-force solutions are\nknown, but intractable. One such is FunSearch [71], an LLM that searches for interpretable\nfunction formalisms rather than direct solutions to the cap set problem, a combinatorial\nproblem. The LLM has attached to it a systematic evaluator to search the function space,\nwhich is designed to enable a feedback loop with domain experts. In the cap set problem,\nFunSearch discovered new constructions of large cap sets, it was applied to online bin pack-\ning where it found novel heuristics improving on widely used baselines. Efforts are also\nunderway to enable LLMs to directly solve mathematical proofs, for example AlphaGeom-\netry proves theorems for Euclidean plane geometry [72]. AlphaGeometry utilizes an LLM\nto guide a symbolic deduction engine through branching points encountered while writing\nproofs. AlpaGeometry generates human readable proofs, close to the performance of an\naverage International Mathematical Olympiad (IMO) gold medallist.\n6.\nCONCLUSION\nLLMs are being rapidly developed and deployed for software applications that involve\nnatural language processing. Beyond the world of chatbot style applications there are many\nscientific domains that employ human language or have their own rigorous scientific lan-\nguage. Physics, mathematics and medicine use natural language with specialized vocabular-\nies and different semantic meaning from ordinary language and are therefore suitable target\nfor traditional LLMs. In chemistry and biology, molecules, proteins, and DNA have their\nown scientific language that represents the underlying physical processes, and they can be\ntargeted with specialised language models.\nIn the domains of mathematics, physics and medicine, LLMs can already be employed\nto aid researchers by gathering the relevant information and proposing solutions to be re-\nviewed by the human domain experts. In medicine, this can greatly aid in the ability to sift\nthrough the massive amount of textual medical knowledge to focus on only the most rele-\nvant information. In mathematics and physics, LLMs can be used in an iterative process to\nsolve problems too complicated for brute force computation and automate derivations with\na well-defined set of steps. There is also work to train LLMs to perform mathematical proofs,\nwhich could enable rapid progression into the understanding of complex mathematical fields,\nmaybe one day generative LLMs will be able to spawn novel mathematical ideas.\n13\nBeyond natural language processing, scientists often encode physical processes in a novel\nlanguage. Molecules, proteins, and DNA have special languages which have different syn-\ntactic and semantic meaning from natural language. For molecular and protein research\nLLMs are able to predict properties from the textual representation as well as design novel\nstructures with target behavior and accomplish this with significant computational speedup.\nWhile research using LLMs in such applications is still evolving, it holds promise of enabling\nentirely new processes of material science, chemistry, and drug design. Finally, in order to\nserve these wide range of applications, computer systems and services need to evolve to\nprovide token capacity at massive scale, with high throughput, low latency, low energy, and\nlow cost.\n[1] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser,\nand\nI. Polosukhin, CoRR abs/1706.03762 (2017), 1706.03762.\n[2] D. Abts, G. Kimmell, A. Ling, J. Kim, M. Boyd, A. Bitar, S. Parmar, I. Ahmed, R. DiCecco,\nD. Han, J. Thompson, M. Bye, J. Hwang, J. Fowers, P. Lillian, A. Murthy, E. Mehtabuddin,\nC. Tekur, T. Sohmers, K. Kang, S. Maresh, and J. Ross, in Proceedings of the 49th Annual\nInternational Symposium on Computer Architecture, ISCA ’22 (Association for Computing\nMachinery, New York, NY, USA, 2022) p. 567–580.\n[3] T. Persson, ˚A. af Geijerstam, and C. Liberg, Nordic Studies in Science Education 12, 176\n(2016).\n[4] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, Biometrika 71, 599 (1986).\n[5] S. Hochreiter and J. Schmidhuber, Neural computation 9, 1735 (1997).\n[6] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, arXiv preprint arXiv:1810.04805 (2018).\n[7] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell,\nP. Mishkin, J. Clark, et al., in International conference on machine learning (PMLR, 2021)\npp. 8748–8763.\n[8] N. Reimers and I. Gurevych, arXiv preprint arXiv:1908.10084 (2019).\n[9] H. Liu, C. Li, Y. Li, and Y. J. Lee, arXiv preprint arXiv:2310.03744 (2023).\n[10] R. Rombach, A. Blattmann, D. Lorenz, P. Esser,\nand B. Ommer, in CVF Conference on\nComputer Vision and Pattern Recognition (CVPR) (2021) pp. 10674–10685.\n14\n[11] P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. K¨uttler, M. Lewis,\nW.-t. Yih, T. Rockt¨aschel, et al., Advances in Neural Information Processing Systems 33,\n9459 (2020).\n[12] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever, et al., (2018).\n[13] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford,\nJ. Wu, and D. Amodei, arXiv preprint arXiv:2001.08361 (2020).\n[14] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Al-\ntenschmidt, S. Altman, S. Anadkat, et al., arXiv preprint arXiv:2303.08774 (2023).\n[15] G. Team, R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M.\nDai, A. Hauth, et al., arXiv preprint arXiv:2312.11805 (2023).\n[16] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi`ere,\nN. Goyal, E. Hambro, F. Azhar, et al., arXiv preprint arXiv:2302.13971 (2023).\n[17] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra,\nP. Bhargava, S. Bhosale, et al., arXiv preprint arXiv:2307.09288 (2023).\n[18] A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bamford, D. S. Chaplot,\nD. d. l. Casas, E. B. Hanna, F. Bressand, et al., arXiv preprint arXiv:2401.04088 (2024).\n[19] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal,\nK. Slama, A. Ray, et al., Advances in neural information processing systems 35, 27730 (2022).\n[20] M. Poli, S. Massaroli, E. Nguyen, D. Y. Fu, T. Dao, S. Baccus, Y. Bengio, S. Ermon, and\nC. R´e, in International Conference on Machine Learning (PMLR, 2023) pp. 28043–28078.\n[21] D. Fu, S. Arora, J. Grogan, I. Johnson, E. S. Eyuboglu, A. Thomas, B. Spector, M. Poli,\nA. Rudra, and C. R´e, Advances in Neural Information Processing Systems 36 (2024).\n[22] A. Gu and T. Dao, arXiv preprint arXiv:2312.00752 (2023).\n[23] S. Jelassi, D. Brandfonbrener, S. M. Kakade, and E. Malach, arXiv preprint arXiv:2402.01032\n(2024).\n[24] S. De, S. L. Smith, A. Fernando, A. Botev, G. Cristian-Muraru, A. Gu, R. Haroun, L. Berrada,\nY. Chen, S. Srinivasan, et al., arXiv preprint arXiv:2402.19427 (2024).\n[25] O. Lieber, B. Lenz, H. Bata, G. Cohen, J. Osin, I. Dalmedigos, E. Safahi, S. Meirom, Y. Be-\nlinkov, S. Shalev-Shwartz, et al., arXiv preprint arXiv:2403.19887 (2024).\n[26] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever, in International\nConference on Machine Learning (PMLR, 2023) pp. 28492–28518.\n15\n[27] D. Lyth and S. King, arXiv preprint arXiv:2402.01912 (2024).\n[28] W. Peebles and S. Xie, in Proceedings of the IEEE/CVF International Conference on Com-\nputer Vision (2023) pp. 4195–4205.\n[29] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. De-\nhghani, M. Minderer, G. Heigold, S. Gelly, et al., arXiv preprint arXiv:2010.11929 (2020).\n[30] Y. Liu, K. Zhang, Y. Li, Z. Yan, C. Gao, R. Chen, Z. Yuan, Y. Huang, H. Sun, J. Gao, et al.,\narXiv preprint arXiv:2402.17177 (2024).\n[31] E. Hargrave-Thomas, B. Yu, and J. Reynisson, World J Clin Oncol 3, 1 (2012).\n[32] P. G. Polishchuk, T. I. Madzhidov,\nand A. Varnek, Journal of Computer-Aided Molecular\nDesign 27, 675 (2013).\n[33] Q. Zhang, K. Ding, T. Lyv, X. Wang, Q. Yin, Y. Zhang, J. Yu, Y. Wang, X. Li, Z. Xiang,\nZ. Xiang, Z. Wang, M. Qin, M. Zhang, J. Zhang, J. Cui, R. Xu, H. Chen, X. Fan, H. Xing,\nand H. Chen, ArXiv abs/2401.14656 (2024).\n[34] D. Weininger, Journal of Chemical Information and Computer Sciences 28, 31 (1988),\nhttps://doi.org/10.1021/ci00057a005.\n[35] M. Krenn, F. H¨ase, A. Nigam, P. Friederich, and A. Aspuru-Guzik, Machine Learning: Science\nand Technology 1, 045024 (2020).\n[36] S. Kim, J. Chen, T. Cheng, A. Gindulyte, J. He, S. He, Q. Li, B. A. Shoemaker, P. A. Thiessen,\nB. Yu, L. Zaslavsky, J. Zhang, and E. E. Bolton, Nucleic Acids Research 51, D1373 (2022),\nhttps://academic.oup.com/nar/article-pdf/51/D1/D1373/48441598/gkac956.pdf.\n[37] Z. Wu, B. Ramsundar, E. N. Feinberg, J. Gomes, C. Geniesse, A. S. Pappu, K. Leswing, and\nV. Pande, Chem Sci 9, 513 (2017).\n[38] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training of deep bidirectional\ntransformers for language understanding,” (2018).\n[39] S. Wang, Y. Guo, Y. Wang, H. Sun, and J. Huang, Proceedings of the 10th ACM International\nConference on Bioinformatics, Computational Biology and Health Informatics (2019).\n[40] J. Ross, B. Belgodere, V. Chenthamarakshan, I. Padhi, Y. Mroueh,\nand P. Das, Nature\nMachine Intelligence 4, 1256 (2022).\n[41] V. Bagal, R. Aggarwal, P. K. Vinod,\nand U. D. Priyakumar, J Chem Inf Model 62, 2064\n(2021).\n[42] Y.\nWang,\nH.\nZhao,\nS.\nSciabola,\nand\nW.\nWang,\nMolecules\n28\n(2023),\n16\n10.3390/molecules28114430.\n[43] T.\nU.\nConsortium,\nNucleic\nAcids\nResearch\n51,\nD523\n(2022),\nhttps://academic.oup.com/nar/article-pdf/51/D1/D523/48441158/gkac1052.pdf.\n[44] A. Elnaggar, M. Heinzinger, C. Dallago, G. Rehawi, Y. Wang, L. Jones, T. Gibbs, T. Feher,\nC. Angerer, M. Steinegger, D. Bhowmik, and B. Rost, IEEE Transactions on Pattern Analysis\nand Machine Intelligence 44, 7112 (2022).\n[45] Z. Yang, X. Zeng, Y. Zhao, and R. Chen, Signal Transduction and Targeted Therapy 8, 115\n(2023).\n[46] Z. Lin, H. Akin, R. Rao, B. Hie, Z. Zhu, W. Lu, N. Smetanin, R. Verkuil, O. Kabeli, Y. Shmueli,\nA. dos Santos Costa, M. Fazel-Zarandi, T. Sercu, S. Candido,\nand A. Rives, Science 379,\n1123 (2023), https://www.science.org/doi/pdf/10.1126/science.ade2574.\n[47] K. Tunyasuvunakool, J. Adler, Z. Wu, T. Green, M. Zielinski, A. ˇZ´ıdek, A. Bridgland,\nA. Cowie, C. Meyer, A. Laydon, S. Velankar, G. J. Kleywegt, A. Bateman, R. Evans,\nA. Pritzel, M. Figurnov, O. Ronneberger, R. Bates, S. A. A. Kohl, A. Potapenko, A. J.\nBallard, B. Romera-Paredes, S. Nikolov, R. Jain, E. Clancy, D. Reiman, S. Petersen, A. W.\nSenior, K. Kavukcuoglu, E. Birney, P. Kohli, J. Jumper, and D. Hassabis, Nature 596, 590\n(2021).\n[48] National Research Council (US) Committee on Mapping and Sequencing the Human Genome,\nMapping and Sequencing the Human Genome (National Academies Press (US), Washington,\nDC, 1988) Chap. Introduction.\n[49] L. Hood and L. Rowen, Genome Medicine 5, 79 (2013).\n[50] W. S. Alharbi and M. Rashid, Human Genomics 16 (2022), 10.1186/s40246-022-00396-x.\n[51] Y. Ji, Z. Zhou, H. Liu, and R. V. Davuluri, Bioinformatics 37, 2112–2120 (2021).\n[52] A. for Computing Machinery,.\n[53] M. Zvyagin, A. Brace, K. Hippe, Y. Deng, B. Zhang, C. O. Bohorquez, A. Clyde, B. Kale,\nD. Perez-Rivera, H. Ma, C. M. Mann, M. Irvin, J. G. Pauloski, L. Ward, V. Hayot, M. Emani,\nS. Foreman, Z. Xie, D. Lin, M. Shukla, W. Nie, J. Romero, C. Dallago, A. Vahdat, C. Xiao,\nT. Gibbs, I. Foster, J. J. Davis, M. E. Papka, T. Brettin, R. Stevens, A. Anandkumar, V. Vish-\nwanath, and A. Ramanathan, bioRxiv (2022), 10.1101/2022.10.10.511571.\n[54] E. Nguyen, M. Poli, M. Faizi, A. Thomas, M. Wornow, C. Birch-Sykes, S. Massaroli, A. Patel,\nC. Rabideau, Y. Bengio, et al., Advances in neural information processing systems 36 (2024).\n17\n[55] R. Dias and A. Torkamani, Genome Medicine 11 (2019), 10.1186/s13073-019-0689-8.\n[56] J. A. Alzubi, R. Jain, A. Singh, P. Parwekar, and M. Gupta, Arabian Journal for Science and\nEngineering 48, 11003–11013 (2021).\n[57] L. Luo, P.-T. Lai, C.-H. Wei, C. N. Arighi, and Z. Lu, Briefings in Bioinformatics 23 (2022),\n10.1093/bib/bbac282.\n[58] D. Q. Nguyen, T. Vu, A. Rahimi, M. H. Dao, L. T. Nguyen, and L. Doan, in Proceedings of the\nSixth Workshop on Noisy User-generated Text (W-NUT 2020) (Association for Computational\nLinguistics, 2020).\n[59] K. Singhal, S. Azizi, T. Tu, S. S. Mahdavi, J. Wei, H. W. Chung, N. Scales, A. Tanwani,\nH. Cole-Lewis, S. Pfohl, P. Payne, M. Seneviratne, P. Gamble, C. Kelly, A. Babiker, N. Sch¨arli,\nA. Chowdhery, P. Mansfield, D. Demner-Fushman, B. Ag¨uera y Arcas, D. Webster, G. S.\nCorrado, Y. Matias, K. Chou, J. Gottweis, N. Tomasev, Y. Liu, A. Rajkomar, J. Barral,\nC. Semturs, A. Karthikesalingam, and V. Natarajan, Nature 620, 172–180 (2023).\n[60] G. Wang, G. Yang, Z. Du, L. Fan, and X. Li, “Clinicalgpt: Large language models finetuned\nwith diverse medical data and comprehensive evaluation,” (2023).\n[61] Y. Li, Z. Li, K. Zhang, R. Dan, S. Jiang, and Y. Zhang, “Chatdoctor: A medical chat model\nfine-tuned on a large language model meta-ai (llama) using medical domain knowledge,”\n(2023).\n[62] H. Nori, Y. T. Lee, S. Zhang, D. Carignan, R. Edgar, N. Fusi, N. King, J. Larson, Y. Li,\nW. Liu, R. Luo, S. M. McKinney, R. O. Ness, H. Poon, T. Qin, N. Usuyama, C. White, and\nE. Horvitz, “Can generalist foundation models outcompete special-purpose tuning? case study\nin medicine,” (2023).\n[63] Z. Chen, A. H. Cano, A. Romanou, A. Bonnet, K. Matoba, F. Salvi, M. Pagliardini, S. Fan,\nA. K¨opf, A. Mohtashami, A. Sallinen, A. Sakhaeirad, V. Swamy, I. Krawczuk, D. Bayazit,\nA. Marmet, S. Montariol, M.-A. Hartley, M. Jaggi, and A. Bosselut, “Meditron-70b: Scaling\nmedical pretraining for large language models,” (2023).\n[64] C. Li, C. Wong, S. Zhang, N. Usuyama, H. Liu, J. Yang, T. Naumann, H. Poon, and J. Gao,\nAdvances in Neural Information Processing Systems 36 (2024).\n[65] X. Zhang, C. Wu, Z. Zhao, W. Lin, Y. Zhang, Y. Wang,\nand W. Xie, arXiv preprint\narXiv:2305.10415 (2023).\n[66] C. Li, C. Wong, S. Zhang, N. Usuyama, H. Liu, J. Yang, T. Naumann, H. Poon,\nand\n18\nJ. Gao, “Llava-med: Training a large language-and-vision assistant for biomedicine in one\nday,” (2023).\n[67] R. Azamfirei, S. R. Kudchadkar,\nand J. Fackler, Critical Care 27 (2023), 10.1186/s13054-\n023-04393-x.\n[68] P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. K¨uttler, M. Lewis,\nW.-t. Yih, T. Rockt¨aschel, S. Riedel,\nand D. Kiela, “Retrieval-augmented generation for\nknowledge-intensive nlp tasks,” (2020).\n[69] H. Pan, N. Mudur, W. Taranto, M. Tikhanovskaya, S. Venugopalan, Y. Bahri, M. P. Brenner,\nand E.-A. Kim, arXiv e-prints , arXiv:2403.03154 (2024).\n[70] A. Alexiadis and B. Ghiassi, Results in Engineering 21, 101721 (2024).\n[71] B. Romera-Paredes, M. Barekatain, A. Novikov, M. Balog, M. P. Kumar, E. Dupont, F. J. R.\nRuiz, J. S. Ellenberg, P. Wang, O. Fawzi, P. Kohli, and A. Fawzi, Nature 625, 468 (2024).\n[72] T. H. Trinh, Y. Wu, Q. V. Le, H. He, and T. Luong, Nature 625, 476 (2024).\n",
  "categories": [
    "cs.CL",
    "cs.AI",
    "cs.LG"
  ],
  "published": "2024-06-11",
  "updated": "2024-06-11"
}