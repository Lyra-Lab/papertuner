{
  "id": "http://arxiv.org/abs/2406.08686v1",
  "title": "Opportunities in deep learning methods development for computational biology",
  "authors": [
    "Alex Jihun Lee",
    "Reza Abbasi-Asl"
  ],
  "abstract": "Advances in molecular technologies underlie an enormous growth in the size of\ndata sets pertaining to biology and biomedicine. These advances parallel those\nin the deep learning subfield of machine learning. Components in the\ndifferentiable programming toolbox that makes deep learning possible are\nallowing computer scientists to address an increasingly large array of problems\nwith flexible and effective tools. However many of these tools have not fully\nproliferated into the computational biology and bioinformatics fields. In this\nperspective we survey some of these advances and highlight exemplary examples\nof their utilization in the biosciences, with the goal of increasing awareness\namong practitioners of emerging opportunities to blend expert knowledge with\nnewly emerging deep learning architectural tools.",
  "text": "Opportunities in deep learning methods\ndevelopment for computational biology\nAlex J. Lee1, Reza Abbasi-Asl1#\n1University of California, San Francisco\n#Corresponding author: Reza Abbasi-Asl (Email: Reza.AbbasiAsl@ucsf.edu)\nAbstract\nAdvances in molecular technologies underlie an enormous growth in the size of data sets\npertaining to biology and biomedicine. These advances parallel those in the deep learning\nsubfield of machine learning. Components in the differentiable programming toolbox that\nmakes deep learning possible are allowing computer scientists to address an increasingly large\narray of problems with flexible and effective tools. However many of these tools have not fully\nproliferated into the computational biology and bioinformatics fields. In this perspective we\nsurvey some of these advances and highlight exemplary examples of their utilization in the\nbiosciences, with the goal of increasing awareness among practitioners of emerging\nopportunities to blend expert knowledge with newly emerging deep learning architectural tools.\nIntroduction\nThe profusion of molecular and functional profiling techniques for biological systems is\nproducing an enormous amount of data. Correspondingly, there is a more urgent need to\nleverage new data analysis tools to turn this enormous amount of data into insights and\ntheories. Large scale and distributed projects such as the Human Cell Atlas1 and\nBICCN/BICAN initiative2 are generating enormous amounts of data across a variety of\nmodalities such as single-cell RNA sequencing (scRNA-seq), spatially resolved transcriptomics\n(ST), and electrophysiology, allowing researchers to study at multiple levels (molecular,\nanatomical, functional) the complexity of organ systems such as the brain.\nThese advances parallel a revolution in computational analysis methods encompassing both\nstatistics and machine learning (ML), and in particular the deep learning subfield of machine\nlearning (DL) has paralleled advances in experimental methods development. Increasing\nrecognition of the capacity of these methods to improve various aspects of the scientific\ndiscovery process as a whole3,4 hints at the tremendous opportunities for data science, which\nwe will loosely refer to as the combination of the areas of statistics, ML, and DL, to advance\nresearch in the biosciences.\nThe view that application of complex computational models to increasingly large and complex\ndatasets will lead to meaningful understanding of complex systems may be overly optimistic5.\nInsofar as the success of ML and DL tools has largely been supervised learning problems with\nvery large amounts of data, it is unsurprising that AlphaFold26, which was made possible by\nlarge amounts of public data7, remains one of the most salient example of DL applied to the\nbiosciences.\nHowever, what is unprecedented is the ease with which researchers can access a large and\npowerful toolbox of differentiable deep learning programming software such as PyTorch8 and\nJax[cite], as well as the increasing flexibility of this toolbox to address a wide variety of\nmodeling problems. Kaznatcheev and Kording9 describe the “evolutionary” development of\nincreasingly flexible and powerful components of the DL toolbox. In particular, they describe\nhow the tooling of DL, meaning the collection of optimization algorithms, objective functions,\nand [something else], can be interchanged amongst each other without failure. Said in another\nway, recent innovations in DL make it easier to apply DL to a variety of different tasks with\ngreater effectiveness.\nIn this perspective, we will discuss new opportunities to use the toolbox of DL to better\nincorporate inductive bias and prior knowledge into models, assuming a rudimentary\nunderstanding of ML and DL. The goal of this work is to highlight recent innovations in\nfundamental DL technologies and their potential to improve and assist biological discovery\nresearch.\nRecent expansion in the building blocks of deep learning\narchitectures and training schemes enables new directions in\ncomputational biology\nThe development of differentiable differentiation programming frameworks have helped\nestablish an array of general architectural building blocks and training schemes for DL models.\nThese components have allowed practitioners to augment the traditional pattern of composing\nmatrix operations with nonlinearities with more efficient and flexible subcomponents. Often\nthese building blocks can also incorporate useful inductive biases, such as in graph neural\nnetworks (GNNs), where entity dependencies (for example, between atoms in a molecule) can\nbe encoded via a combination of learned and fixed user-specified mechanisms. In the simple\nexample of molecular property prediction, this might take the form of a neural network where at\neach layer a weighted sum is taken of features from different atoms that have bonds between\nthem.\nThe concept of an inductive bias can be variably defined, but can be thought of as a practitioner\nchoice that increases the likelihood of a model to extract a specific pattern of correlations from a\ndataset. In the molecular property prediction example, a non-GNN model might take as inputs\nthe number of carbons, oxygens, and nitrogens in a given molecule and predict its solubility.\nThis model does not directly incorporate notions of different connectivity, and would be unable to\ndistinguish, for example, 1-proponal vs isopropyl alcohol, which have the same molecular\nformula of C3H8O. A model such as a GNN might be said to incorporate a useful inductive bias\nby allowing features of specific atoms to interact based on whether bonds are between them,\nmimicking the actual structure of the molecule.\nThe idea of extracting specific patterns of correlations that may be partially determined by the\npractitioner is a useful concept from which to describe a variety of recent innovations in DL\narchitectural components and training schemes. In the next sections we will discuss two\nparticularly important innovations, self- and semi-supervised training, and the attention\nmechanism, in terms of their ability to enable the incorporation of novel inductive biases in the\nmodeling process, and their potential to improve computational biology modeling efforts.\nIncorporating assumptions on the semantics of latent\nrepresentations using self-supervised learning\nData that are labeled at the scale which would be useful for supervised DL is relatively scarce.\nIn this context, it is useful to consider other regimes for training DL models, such as\nself-supervised learning (SSL). Although SSL is not new, in the presence of large (unlabeled)\ndatasets, it has significantly boosted many modern DL efforts10. In this section we will describe\nhow SSL is a general framework for constructing optimization problems that can yield useful\nlatent representations.\nOne of the easiest to understand methods in SSL is contrastive SSL, sometimes historically\nreferred to as deep metric learning. Triplet loss is perhaps the most intuitive example of\ncontrastive SSL: given a given reference data instance, a network is trained to compute a latent\nembedding which is optimized such that a positive example (for example a data point with the\nsame class label) is close in latent space and a negative example (different class label) is far.\nThis is in contrast to the traditional paradigm of supervising a network to predict a class label, or\nan unsupervised approach exemplified by methods such as autoencoders which are optimized\nfor compression and reconstruction. If applied to single cell sequencing data, a positive example\nmight be a cell with the same disease label as the reference cell and a negative example a cell\nwith a different disease label. Another example of SSL is masked image modeling, where a\nnetwork is trained to predict unseen image patches from observed ones.\nOne way to understand these methods is in seeing them as methods for grouping semantically\nsimilar data points together in the latent representation space of the model. Triplet loss can be\nseen as a way of preserving class label discreteness in latent space, and a masked image\nmodeling paradigm can be interpreted as a method for discovering high-level representations\nthat are invariant to patch loss11. Using the previous analogy of capturing correlations, SSL\ntechniques can be interpreted as preserving certain correlations depending on their relevance to\nthe specific objective function.\nCEBRA12, a recent work, showcases some possible advantages of a contrastive learning\nframework. Specifically, CEBRA can be used to discover latent variables for behavior (physical\nposition of an animal in space) and neural activity (calcium imaging) simultaneously by using\nmultiple variational autoencoders (VAEs), one for each modality. These VAEs are trained such\nthat latent features corresponding to time points that are close together are close together in\nlatent space, or optionally so that latent features corresponding to physical distance are\nlikewise close together when the physical positions are similar. These latent features are shown\nto be useful for decoding behavioral dynamics as well as multimodal data integration.\nAn important note is that the choice of which factor is used to supervise the training process\n(either position, time, or a behavioral categorical label for example) amounts to an assumption\nthat the latent variables from the network are conditionally independent given information on the\nsupervision factor. By offering practitioners a powerful means by which to structure the model\noptimization process, this framework can easily be extended to a variety of applications. For\nexample, CEBRA showcases how a user that is interested in animal-invariant features in the\ndata can train the network across animals and hypothesize that such a scheme forces the\nnetwork to identify features that are consistent amongst subjects.\nAnother illustrative example of SSL comes from the NCEM method13. This method uses a GNN\nthat predicts a cell’s observed gene expression vector (measured for example by MERFISH)\nfrom its cell type label and features from surrounding cells in the same neighborhood. In this\nway it is similar to the masked imaging modeling paradigm, but with unseen gene expression\ninstead of unseen image patches. NCEM is an excellent example of utilizing the machinery of\nSSL to answer high-level biological questions. Specifically, the problem of understanding how\nfar away cells need to be in order to be predictive of gene expression is formulated as a masked\nprediction problem. The authors explore the impact of varying neighborhood size (the radius at\nwhich a cell is considered within the neighborhood) and quantifying its effect on predictive\naccuracy. Using this framework to quantify cell-cell interaction, they also uncover a dependency\nof immune cell abundance to proximity to the tumor-immune boundary. The authors also\nparameterize a spatial sender-receiver interaction matrix that is optimized in the same process\nthat provides a mechanism to inspect cell-cell interaction. Thus in NCEM the formulation of a\ngeneral prediction problem on cellular neighborhoods is used as a way to inspect the relative\ncontributions of spatial distance on cell-cell interactions.\nThese methods will continue to be broadly useful for injecting human biases into optimization\nproblems and create generally useful representations. The spatial context of ST lends itself to a\nvariety of relatively simple priors that can be implemented via SSL. For example, NCEM\nimplicitly defines a prior that cellular communication ought to be on a relatively short distance\nscale. In an alternative construction, a model could be trained to create latent features for a\ncellular neighborhood in ST data and a contrastive loss could be used to optimize latent\nfeatures for neighborhoods nearby in space to be close in latent space. These techniques\nprovide biologists a useful toolkit for extracting meaningful representations from unlabeled data\nand operating on them. Furthermore, the techniques of SSL can be combined with more\nestablished tools in the ML and DL toolbox. Another way a model such as NCEM might be\naugmented might be to enforce sparsity regularization on the cell signal sender-receiver matrix,\nto extract only the largest magnitude interactions or to impart the prior that cellular signaling is\nenergetically expensive.\nIn the future, representations that are extracted using techniques in SSL may also be useful for\nenabling quantitative comparisons between cellular neighborhoods. A recent method called\nSCimilarity14 was trained on non-spatial scRNA-seq data using triplet loss (mentioned above),\nwith the explicit goal of optimizing a nonlinear latent variable projection such that Cell Ontology15\nrelations were preserved. The authors show that the learned similarity function can be used to\nthen query for cells similar to a generic input cell. A generally useful application of a similar idea\nmay be to train a network in a similar fashion (using SSL) for generic querying of cellular\nneighborhoods or spatial gene expression patterns.\nAttention as a general dependency modeling operation and its\npotential in multimodal data integration\nThe attention mechanism is a critical component of the deep learning toolbox and in particular\nunderlies the transformer16, a highly successful architecture for a variety of applications. Turner17\nand Phuong and Hutter18 give a substantive introduction to transformers and attention, but we\nwill give a very abridged description here.\nAttention is an operation on a sequence of “tokens”, which are matrix representations for\nelements of a sequence or set, for example one vector (token) per word in a sentence or cell in\na cellular neighborhood. Note that the token-wise representation is in itself a departure from\ncommon representations; for example previous methods might have encoded an entire\nsentence into a fixed-length vector, and instead here there would be a set of vectors\nrepresenting components of a sentence. Self-attention is the variant of attention which\nparameterizes a feature update for each token in parallel using its own features and features\nfrom other tokens. This occurs using a learned kernel or similarity function between elements of\nthe set, in what are commonly referred to as “key” and “query” projection operations (usually a\nsingle linear projection operation). At each layer, each token receives a feature update as a\nweighted average of its own and the other tokens' features. This is done by comparing its key\nprojection with the query projections of all of the tokens (including itself), which gives a\nweighting factor over the set of tokens. This weighting factor, referred to as an attention\ncoefficient, is then used to compute the weighted average of the features. The role of these\nlearned similarity functions then is to allow the network to compare tokens in a dynamic fashion\nacross the layers and use these similarities to compute a context-specific feature update for a\ngiven token given the others.\nAdding complexity to this scheme is that attention often refers to “multi-headed attention”,\nmeaning that instead of updating every feature for every token at once, each “head” updates a\nsubset of the features such that 2-headed attention would update half the features per head,\nand 3-headed attention would update one-third of the features per head and so on. Each head\nis parameterized by its own similarity function (in other words, each head is associated with a\ndifferent query and key projection) which importantly is based on all of the features,\nincorporating a notion of global-to-local feature importance. Another variant is cross-attention,\nwhere feature updates are computed in terms of another set or sequence, for example updating\ntokens representing a radiology report using tokens representing a radiology image. The\ntransformer architecture is unique in that its primary operation is composed attention operations\nfor modeling generic sequences or sets, initially in natural language processing. An important\nnote is that in computing pairwise-similarities between tokens, attention can be seen to be\nlearning a graph over a set of elements.\nA particular advantage of the self-attention mechanism is in its ability to learn an arbitrary\ndependency structure over data (in contrast to, for example, a message passing neural network,\nwhere the adjacency matrix is fixed). This capacity is generally useful for modeling\ndependencies amongst fixed items in, for example, natural language, or in amino acid\nsequences, but is also an important capability for multimodal data integration. We anticipate that\nthis capacity will be explored extensively over the next few years as researchers employ the\nflexibility of attention to model arbitrary dependencies between modalities, elements of\nbiological sequences (such as proteins or DNA sequences) or sets (such as cells in a spatial\nneighborhood).\nOne example of the relative ease of employing such a functionality, albeit in the clinical domain,\nis IRENE19, a transformer based model for multimodal learning on paired radiology images,\nclinical text, and structured metadata. The authors use a method called Perceiver[cite] to allow\nfor projection of large radiology images into a small set of tokens to reduce computational\nburden, but most importantly self-attention facilitates arbitrary feature interactions between\ntokens representing images, text, and metadata. With the appropriate datasets and training\nobjectives, these approaches are a powerful tool to facilitate multimodal modeling of biological\ndata at scale.\nThe attention mechanism itself is also a useful substrate for injection of human knowledge and\nexpertise. The clearest example of this is in AlphaFold2 (AF2). One of AF2’s hallmarks is a high\ndegree of communication between a multiple-sequence (MSA) alignment processing operation\nand a “pair representation” which could be thought to encode structural information using a\ndirected graph over residues.\nThe first way this is performed is via updates of the MSA features using information from the\npair representation. The attention coefficient for a given pair of residues (i, j) in the MSA is given\nby a sum of a term representing affinities (query-key dot products) computed using the MSA and\na term computed from the (i, j) features in the pair representation. One interpretation of this\ninteraction is that the evolutionary couplings in the MSA ought to be a function of the structural\nor biophysical interaction of a pair of residues.\nAnother way human knowledge is incorporated into the optimization process is in the pair\nrepresentation update. As mentioned, the pair representation can be thought to encode\nstructural information. Therefore, the attention mechanism in AF2 is restricted in a mechanism\nreferred to as triangle attention. Specifically, the idea that adjacent residues in a protein\nstructure ought to inform each other. This is implemented by only allowing pairs of residues that\nshare a residue (i.e. such that a sliding window of three residues should be allowed to attend to\neach other).\nIn the future, we believe that one impactful application of attention will be in examining\ncontributions of different modalities to predictive modeling. For example, if paired radiology\nclinical notes and images were used to predict a complex phenotype. A practitioner could then\ndevelop a classifier that learns latent features for radiology and text, using a sparse attention for\nupdate of image features using text features prior to prediction. By inspecting the attention\nweights, an empirical understanding could be developed regarding the importance of text\nfeatures, as some examples may not require a significant update using text features for\naccurate prediction.\nConclusions\nIn this perspective, we provide an overview of opportunities for increased usage of DL tools for\nbiology discovery. DL is already a critical tool in a variety of subfields of computational biology,\nparticularly in single cell biology and protein biophysics. As new and flexible tools are developed\nfor more fundamental computer science applications, we expect they will find fruitful application\nin biology.\nReferences\nFigure 1. Simple cartoons of methods described in this perspective. A.Scheme of contrastive\nlearning, wherein a neural network is used to encode different data points into some latent\ndimension. The network is optimized such that different data points (e.g. those that receive\ndifferent drug treatments) are far away in latent space and those that are similar (e.g. those that\nreceive the same drug treatment) are close together. B. A trivialized example of how a\ncontrastive learning scheme might impose a useful geometry on an imagined dataset of treated\ncells and cells with positive or negative controls. C. A cartoon of the self-attention mechanism\ninstantiated on a group of cells. The center cell, given in the red background, receives latent\nfeatures as a weighted average over the other cells. This pairwise similarity between elements\n(cells) is parameterized by a query and key projection of each cell’s features and amounts to\ncomputing a directed graph over the cells as shown in d. D. A prospective attention matrix,\nwhere in practice there would be several such attention matrices. In this hypothetical case, the\ncells’ features are not updated strongly by other cells, due to the strong diagonal. However,\ndifferent cells have stronger off-diagonal elements than others.\n(1)\nZhang, B.; He, P.; Lawrence, J. E. G.; Wang, S.; Tuck, E.; Williams, B. A.; Roberts, K.;\nKleshchevnikov, V.; Mamanova, L.; Bolt, L.; Polanski, K.; Li, T.; Elmentaite, R.; Fasouli, E.\nS.; Prete, M.; He, X.; Yayon, N.; Fu, Y.; Yang, H.; Liang, C.; Zhang, H.; Blain, R.; Chedotal,\nA.; FitzPatrick, D. R.; Firth, H.; Dean, A.; Bayraktar, O. A.; Marioni, J. C.; Barker, R. A.;\nStorer, M. A.; Wold, B. J.; Zhang, H.; Teichmann, S. A. A Human Embryonic Limb Cell\nAtlas Resolved in Space and Time. Nature 2023, 1–11.\nhttps://doi.org/10.1038/s41586-023-06806-x.\n(2)\nHawrylycz, M.; Martone, M. E.; Ascoli, G. A.; Bjaalie, J. G.; Dong, H.-W.; Ghosh, S. S.;\nGillis, J.; Hertzano, R.; Haynor, D. R.; Hof, P. R.; Kim, Y.; Lein, E.; Liu, Y.; Miller, J. A.;\nMitra, P. P.; Mukamel, E.; Ng, L.; Osumi-Sutherland, D.; Peng, H.; Ray, P. L.; Sanchez, R.;\nRegev, A.; Ropelewski, A.; Scheuermann, R. H.; Tan, S. Z. K.; Thompson, C. L.; Tickle, T.;\nTilgner, H.; Varghese, M.; Wester, B.; White, O.; Zeng, H.; Aevermann, B.; Allemang, D.;\nAment, S.; Athey, T. L.; Baker, C.; Baker, K. S.; Baker, P. M.; Bandrowski, A.; Banerjee, S.;\nBishwakarma, P.; Carr, A.; Chen, M.; Choudhury, R.; Cool, J.; Creasy, H.; D’Orazi, F.;\nDegatano, K.; Dichter, B.; Ding, S.-L.; Dolbeare, T.; Ecker, J. R.; Fang, R.; Fillion-Robin,\nJ.-C.; Fliss, T. P.; Gee, J.; Gillespie, T.; Gouwens, N.; Zhang, G.-Q.; Halchenko, Y. O.;\nHarris, N. L.; Herb, B. R.; Hintiryan, H.; Hood, G.; Horvath, S.; Huo, B.; Jarecka, D.; Jiang,\nS.; Khajouei, F.; Kiernan, E. A.; Kir, H.; Kruse, L.; Lee, C.; Lelieveldt, B.; Li, Y.; Liu, H.; Liu,\nL.; Markuhar, A.; Mathews, J.; Mathews, K. L.; Mezias, C.; Miller, M. I.; Mollenkopf, T.;\nMufti, S.; Mungall, C. J.; Orvis, J.; Puchades, M. A.; Qu, L.; Receveur, J. P.; Ren, B.;\nSjoquist, N.; Staats, B.; Tward, D.; Velthoven, C. T. J. van; Wang, Q.; Xie, F.; Xu, H.; Yao,\nZ.; Yun, Z.; Zhang, Y. R.; Zheng, W. J.; Zingg, B. A Guide to the BRAIN Initiative Cell\nCensus Network Data Ecosystem. PLOS Biol. 2023, 21 (6), e3002133.\nhttps://doi.org/10.1371/journal.pbio.3002133.\n(3)\nWang, H.; Fu, T.; Du, Y.; Gao, W.; Huang, K.; Liu, Z.; Chandak, P.; Liu, S.; Van Katwyk, P.;\nDeac, A.; Anandkumar, A.; Bergen, K.; Gomes, C. P.; Ho, S.; Kohli, P.; Lasenby, J.;\nLeskovec, J.; Liu, T.-Y.; Manrai, A.; Marks, D.; Ramsundar, B.; Song, L.; Sun, J.; Tang, J.;\nVeličković, P.; Welling, M.; Zhang, L.; Coley, C. W.; Bengio, Y.; Zitnik, M. Scientific\nDiscovery in the Age of Artificial Intelligence. Nature 2023, 620 (7972), 47–60.\nhttps://doi.org/10.1038/s41586-023-06221-2.\n(4)\nUhler, C. Building a Two-Way Street between Cell Biology and Machine Learning. Nat. Cell\nBiol. 2024, 26 (1), 13–14. https://doi.org/10.1038/s41556-023-01279-6.\n(5)\nJonas, E.; Kording, K. P. Could a Neuroscientist Understand a Microprocessor? PLOS\nComput. Biol. 2017, 13 (1), e1005268. https://doi.org/10.1371/journal.pcbi.1005268.\n(6)\nJumper, J.; Evans, R.; Pritzel, A.; Green, T.; Figurnov, M.; Ronneberger, O.;\nTunyasuvunakool, K.; Bates, R.; Žídek, A.; Potapenko, A.; Bridgland, A.; Meyer, C.; Kohl,\nS. A. A.; Ballard, A. J.; Cowie, A.; Romera-Paredes, B.; Nikolov, S.; Jain, R.; Adler, J.;\nBack, T.; Petersen, S.; Reiman, D.; Clancy, E.; Zielinski, M.; Steinegger, M.; Pacholska, M.;\nBerghammer, T.; Bodenstein, S.; Silver, D.; Vinyals, O.; Senior, A. W.; Kavukcuoglu, K.;\nKohli, P.; Hassabis, D. Highly Accurate Protein Structure Prediction with AlphaFold. Nature\n2021, 596 (7873), 583–589. https://doi.org/10.1038/s41586-021-03819-2.\n(7)\nBerman, H. M.; Westbrook, J.; Feng, Z.; Gilliland, G.; Bhat, T. N.; Weissig, H.; Shindyalov,\nI. N.; Bourne, P. E. The Protein Data Bank. Nucleic Acids Res. 2000, 28 (1), 235–242.\nhttps://doi.org/10.1093/nar/28.1.235.\n(8)\nBradbury, J.; Frostig, R.; Hawkins, P.; Johnson, M. J.; Leary, C.; Maclaurin, D.; Necula, G.;\nPaszke, A.; VanderPlas, J.; Wanderman-Milne, S.; Zhang, Q. JAX: Composable\nTransformations of Python+NumPy Programs, 2018. http://github.com/google/jax.\n(9)\nKaznatcheev, A.; Kording, K. P. Nothing Makes Sense in Deep Learning, except in the\nLight of Evolution. arXiv May 20, 2022. https://doi.org/10.48550/arXiv.2205.10320.\n(10) Balestriero, R.; Ibrahim, M.; Sobal, V.; Morcos, A.; Shekhar, S.; Goldstein, T.; Bordes, F.;\nBardes, A.; Mialon, G.; Tian, Y.; Schwarzschild, A.; Wilson, A. G.; Geiping, J.; Garrido, Q.;\nFernandez, P.; Bar, A.; Pirsiavash, H.; LeCun, Y.; Goldblum, M. A Cookbook of\nSelf-Supervised Learning. arXiv April 24, 2023. https://doi.org/10.48550/arXiv.2304.12210.\n(11) Kong, L.; Ma, M. Q.; Chen, G.; Xing, E. P.; Chi, Y.; Morency, L.-P.; Zhang, K.\nUnderstanding Masked Autoencoders via Hierarchical Latent Variable Models. arXiv June\n7, 2023. https://doi.org/10.48550/arXiv.2306.04898.\n(12) Schneider, S.; Lee, J. H.; Mathis, M. W. Learnable Latent Embeddings for Joint Behavioral\nand Neural Analysis. Nature 2023, 617 (7960), 360–368.\nhttps://doi.org/10.1038/s41586-023-06031-6.\n(13) Fischer, D. S.; Schaar, A. C.; Theis, F. J. Modeling Intercellular Communication in Tissues\nUsing Spatial Graphs of Cells. Nat. Biotechnol. 2023, 41 (3), 332–336.\nhttps://doi.org/10.1038/s41587-022-01467-z.\n(14) Heimberg, G.; Kuo, T.; DePianto, D.; Heigl, T.; Diamant, N.; Salem, O.; Scalia, G.;\nBiancalani, T.; Rock, J.; Turley, S.; Bravo, H. C.; Kaminker, J.; Heiden, J. A. V.; Regev, A.\nScalable Querying of Human Cell Atlases via a Foundational Model Reveals\nCommonalities across Fibrosis-Associated Macrophages. bioRxiv July 19, 2023, p\n2023.07.18.549537. https://doi.org/10.1101/2023.07.18.549537.\n(15) Diehl, A. D.; Meehan, T. F.; Bradford, Y. M.; Brush, M. H.; Dahdul, W. M.; Dougall, D. S.;\nHe, Y.; Osumi-Sutherland, D.; Ruttenberg, A.; Sarntivijai, S.; Van Slyke, C. E.; Vasilevsky,\nN. A.; Haendel, M. A.; Blake, J. A.; Mungall, C. J. The Cell Ontology 2016: Enhanced\nContent, Modularization, and Ontology Interoperability. J. Biomed. Semant. 2016, 7 (1), 44.\nhttps://doi.org/10.1186/s13326-016-0088-7.\n(16) Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, L.;\nPolosukhin, I. Attention Is All You Need. arXiv August 1, 2023.\nhttps://doi.org/10.48550/arXiv.1706.03762.\n(17) Turner, R. E. An Introduction to Transformers. arXiv February 8, 2024.\nhttps://doi.org/10.48550/arXiv.2304.10557.\n(18) Phuong, M.; Hutter, M. Formal Algorithms for Transformers. arXiv July 19, 2022.\nhttps://doi.org/10.48550/arXiv.2207.09238.\n(19) Zhou, H.-Y.; Yu, Y.; Wang, C.; Zhang, S.; Gao, Y.; Pan, J.; Shao, J.; Lu, G.; Zhang, K.; Li,\nW. A Transformer-Based Representation-Learning Model with Unified Processing of\nMultimodal Input for Clinical Diagnostics. Nat. Biomed. Eng. 2023, 7 (6), 743–755.\nhttps://doi.org/10.1038/s41551-023-01045-x.\n",
  "categories": [
    "q-bio.QM",
    "cs.LG"
  ],
  "published": "2024-06-12",
  "updated": "2024-06-12"
}