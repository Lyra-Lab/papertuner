{
  "id": "http://arxiv.org/abs/2406.08854v1",
  "title": "Current applications and potential future directions of reinforcement learning-based Digital Twins in agriculture",
  "authors": [
    "Georg Goldenits",
    "Kevin Mallinger",
    "Sebastian Raubitzek",
    "Thomas Neubauer"
  ],
  "abstract": "Digital Twins have gained attention in various industries for simulation,\nmonitoring, and decision-making, relying on ever-improving machine learning\nmodels. However, agricultural Digital Twin implementations are limited compared\nto other industries. Meanwhile, machine learning, particularly reinforcement\nlearning, has shown potential in agricultural applications like optimizing\ndecision-making, task automation, and resource management. A key aspect of\nDigital Twins is representing physical assets or systems in a virtual\nenvironment, which aligns well with reinforcement learning's need for\nenvironment representations to learn the best policy for a task. Reinforcement\nlearning in agriculture can thus enable various Digital Twin applications in\nagricultural domains. This review aims to categorize existing research\nemploying reinforcement learning in agricultural settings by application\ndomains like robotics, greenhouse management, irrigation systems, and crop\nmanagement, identifying potential future areas for reinforcement learning-based\nDigital Twins. It also categorizes the reinforcement learning techniques used,\nincluding tabular methods, Deep Q-Networks (DQN), Policy Gradient methods, and\nActor-Critic algorithms, to overview currently employed models. The review\nseeks to provide insights into the state-of-the-art in integrating Digital\nTwins and reinforcement learning in agriculture, identifying gaps and\nopportunities for future research, and exploring synergies to tackle\nagricultural challenges and optimize farming, paving the way for more efficient\nand sustainable farming methodologies.",
  "text": "Current applications and potential future directions of reinforcement learning-based\nDigital Twins in agriculture\nGeorg Goldenitsb,c,∗, Kevin Mallingerc, Sebastian Raubitzeka, Thomas Neubauera,c\naSBA Research gGmbH, Floragasse 7/5.OG, Vienna, 1040, Vienna, Austria\nbUniversity of Vienna, Universit¨atsring 1, Vienna, 1010, Vienna, Austria\ncTU Wien, Karlsplatz 13, Vienna, 1040, Vienna, Austria\nAbstract\nDigital Twins have recently gained attention in various industries for simulation, monitoring and decision-making purposes\nbecause most of them rely on ever-improving machine learning models in their architecture. However, agricultural Digital Twin\nimplementations are still limited compared to other industries. Meanwhile, machine learning in general, and reinforcement learning\nin particular, have demonstrated their potential in agricultural applications like optimising decision-making processes, task automa-\ntisation and resource management.\nA key aspect of Digital Twins is the representation of physical assets or systems in a virtual environment. This characteristic syn-\nergises well with the requirements for reinforcement learning, which relies on environment representations to accurately learn the\nbest policy for a given task. Therefore, the use of reinforcement learning in agriculture has the potential to open up a variety of\nreinforcement learning-based Digital Twin applications in agricultural domains.\nTo explore these domains, this review aims to categorise existing research works that employ reinforcement learning techniques in\nagricultural settings. On the one hand, categories are created regarding the application domain, such as robotics, greenhouse man-\nagement, irrigation systems, and crop management, identifying the potential future application areas for reinforcement learning-\nbased Digital Twins. On the other hand, the reinforcement learning techniques employed in these applications, including tabular\nmethods, Deep Q-Networks (DQN), Policy Gradient methods, and Actor-Critic algorithms, are categorised to gain an overview of\ncurrently employed models.\nThrough this analysis, the review seeks to provide insights into the current state-of-the-art in integrating Digital Twins and rein-\nforcement learning in agriculture. Additionally, it aims to identify gaps and opportunities for future research, including potential\nsynergies of reinforcement learning and Digital Twins to tackle agricultural challenges and optimise farming processes, paving the\nway for more efficient and sustainable farming methodologies.\nKeywords: Digital Twin, Reinforcement Learning, Sustainable Agriculture, Irrigation Management, Greenhouse Management,\nCrop Management, Automated Harvesting\n1. Introduction\nGrowing concerns about food security driven by population\ngrowth and increasing climate variability have raised the pres-\nsure for more productive and efficient farming [1][2][3][4]. A\nrecent research approach to optimising farming operations has\nbeen the introduction of Digital Twins in agricultural applica-\ntions. Digital Twins replicate a real entity in a virtual represen-\ntation and allow for simulating and optimising tasks and events\nsupported by machine learning models [5]. Simulation is espe-\ncially useful for scenarios where change would otherwise only\nbe observed over a long period of time, such as crop growth, or\nwhere the risk of taking incorrect actions is high, for example,\nin incorrect irrigation management that could lead to losses in\ncrop yield. This practice can also support sustainability in agri-\nculture and help maintain or increase crop yields. Even though\n∗Corresponding author\nEmail address: ggoldenits@sba-research.org (Georg Goldenits)\nthere already exist Digital Twins for automated harvesting using\nunmanned vehicles and irrigation management tasks, potential\nother areas of application remain unexplored [10][14].\nAs current Digital Twins rely on machine learning, a potential\nway to identify additional use cases for them is to look at tasks\nalready solved by machine learning, but so far, Digital Twins\nhave not been implemented. One machine learning technique\nthat lends itself well to simulation is reinforcement learning. It\ncan be used model-free and self-learn how to handle a situa-\ntion based on predefined parameter and environment settings.\nThese properties also allow reinforcement learning to adapt to\nunseen situations while trying to achieve the initially defined\ngoal [7]. Therefore, there are already existing reinforcement\nlearning implementations in agriculture, such as suggesting us-\ning resources like water and fertiliser more efficiently [64][65],\nincreasing crop yields by detecting pests and diseases using un-\nmanned aerial vehicles [51][21], planting crops in a suitable\norder [58] or reduce the energy consumption of greenhouses\n[74][75].\nPreprint submitted to Elsevier\nJune 14, 2024\narXiv:2406.08854v1  [cs.LG]  13 Jun 2024\nA significant factor for the applicability of results achieved by a\nreinforcement learning agent is how well the environment it in-\nteracts with has been modelled after the real environment [62].\nTherefore, research on Digital Twins and how to best replicate\nthe real world aligns with the needs of a well-trained reinforce-\nment learning agent and could pave the way for reinforcement\nlearning-based Digital Twins in agriculture.\nThe term rein-\nforcement learning-based Digital Twin is used in this review\nto define Digital Twins that rely on reinforcement learning as\ntheir machine learning model.\nTo identify promising reinforcement learning-based Digital\nTwin applications in agriculture, this review aims to categorise\nrecent applications of reinforcement learning in agriculture and\nseeks to provide a structured overview of them. For each cat-\negory, the strengths and weaknesses of reinforcement learning\ncompared to other possible solutions are discussed, and an as-\nsessment is made if and how it could be implemented in a Dig-\nital Twin. The obtained factual insights can inform future re-\nsearch directions and contribute to developing and implement-\ning advanced agricultural management systems. The goals are\nsummarised in the following research questions:\n• RQ1 What are the existing applications of reinforcement\nlearning in agriculture?\n• RQ2 Which application domains are suitable for reinforce-\nment learning-based Digital Twins?\nThe work will be structured as follows: Section 2 will introduce\nDigital Twins and reinforcement learning by defining them.\nSection 3 describes the methodology of this work. Section 4\nsummarises related literature reviews on Digital Twins in agri-\nculture and machine learning in agriculture. Section 5 attempts\nto categorise the application areas for reinforcement learning\nin agriculture. The potential for reinforcement learning-based\nDigital Twins is assessed based on the strengths and weak-\nnesses of reinforcement learning for applications in each cate-\ngory. Also, an outlook on potential future applications is given.\nSection 6 provides the conclusion of the manuscript.\n2. Definitions\nMichael Grieves first introduced Digital Twins in 2003[6] with\nthe goal of optimising a factory process.\nThe general idea\nof Digital Twins is to replicate a real-world object, entity or\nsystem, such as a train, a crop or an agricultural supply chain\nin a virtual environment.\nSensor data is commonly used in\nan Internet of Things (IoT) setting to create a digital image\nbecause it allows continuous measuring of a system or specific\nproperties. The data gets processed, and frequently, a machine\nlearning model is used to draw inferences from the collected\ndata and use it for predicting actions and conditions of the\nenvironment.\nOnce a Digital Twin is in place, simulations are commonly\nused to explore how an environment would behave in different\ncircumstances. This allows the user or an automated system to\nbe better prepared for various scenarios and react appropriately\nin the real world should a situation arise that was previously\nonly simulated.\nOf course, it may be difficult to replicate\nreal-world applications based on the task perfectly. Therefore,\nmodel outputs and simulation results might deviate from the\ncorresponding real-world situation. However, in a functioning\nDigital Twin system, real-world experiences get fed back into\nthe Digital Twin, leading to updates in models and simulations\n[5].\nAccording to Sutton & Barto[7],\nreinforcement learning\nis learning what to do. Within the machine learning world,\nreinforcement learning is a concept that does not fit in the\nclassical categories of supervised or unsupervised learning but\ninstead represents its own learning category.\nIn general, reinforcement learning tries to optimise a sequence\nof actions that may be previously unknown within a given\nenvironment by collecting rewards it obtains by interacting\nwith the environment.\nThe goal is to maximise the reward\nsignal and, in doing so, arrive at an optimal course of action.\nIn the classical approach to reinforcement learning, a table for\neach state-action pair is maintained, and the quality of each pair\nis incrementally updated using the Bellman Equation[8]. More\nrecent developments combine reinforcement learning with ar-\ntificial neural networks (ANN) to approximate the quality of a\nstate-action pair by minimising the error between the predicted\nvalue and the target value. Since the ANNs used to support\nreinforcement learning may vary in structure and complexity,\nthose models are summarised as deep reinforcement learning\n(DRL).\nCompared to tabular reinforcement learning, DRL can handle\nmuch larger state spaces as the function approximation is\ncomputationally more efficient, and no table of all possible\ncombinations of states and actions needs to be maintained.\nWhile DRL, in most cases, approximated the optimal solution\nwell, tabular reinforcement learning achieves the optimal\nsolution given enough time and, due to its maintained table,\nallows for easier explainability of its decision-making process.\n3. Methodology\nThis review aims to categorise current research in agricultural\nreinforcement learning by area of application and type of\nmodel used. For each manually defined category, strengths and\nweaknesses are highlighted. Only works published in 2020 or\nlater are considered to account for current research.\nThis review will summarise the areas of Digital Twins in\nagriculture and general machine learning in agriculture in the\nrelated work section to gain a broader picture of the current\nstate of research. The scope of this work can be summarised\nin Figure 1. The search queries can be found in Table 1. The\nqueries are used to search for published work online on Google\nScholar (https://scholar.google.com/).\nAs of the last access\ndate (Feb 7th 2024), searching for publications yielded more\nthan 17,000 results. Therefore, only the first 200 publications\nlisted are used as a sample for the categorisation as mentioned\nearlier, resulting in a corpus of 71 publications.\n2\nFigure 1: Research scope of this work\nTable 1\nSearch queries\n”Digital Twin” AND ”Agriculture”: Related Work\n”Machine Learning” AND ”Agriculture”: Related Work\n”Reinforcement Learning” AND ”Agriculture”: Main Part\nThe exclusion criteria for publications are:\n• Only papers where the full text is accessible and written in\nEnglish are considered.\n• Review papers, aside from those in the related work sec-\ntion, are excluded\n• As this work aims to capture the state-of-the-art, only pa-\npers published from 2020 onward are considered.\n• Publications that do not contribute novel ideas to rein-\nforcement learning but instead only cite it as related work\nin similar research areas are excluded\n• Only peer-reviewed publications are considered\n4. Related Work\nTo gain a broad overview of the current state-of-the-art for Dig-\nital Twins in agriculture, existing areas of application and cur-\nrent challenges identified in related review literature are sum-\nmarised. With a similar goal in mind, machine learning solu-\ntions for problems in agriculture are determined based on ex-\nisting review literature in that area.\nAside from gaining an\noverview, the summary provides a foundation to assess the\nstrengths and weaknesses of reinforcement learning implemen-\ntations compared to other machine learning solutions.\n4.1. Digital Twins in agriculture\nDigital Twins in agriculture are becoming a popular topic as\nthe potential of representing crops, automated robots or entire\nfarming systems virtually with the goal of optimising processes\nis realised. The increasing popularity is exemplified by seven\nreview papers that were published in 2022 and 2023 and can be\nfound in Table 2. These papers summarise recent advances and\npresent current challenges and opportunities.\nTable 2: Corpus of papers reviewed in Section 4.1\nPaper\nContent\nPurcell - 2023 [9]\nAgricultural applications\nNie - 2022 [10]\nAI in crop development processes\nAttaran - 2023 [11]\nIndustry applications\nNasirahmadi - 2022 [12]\nDigitization and Sensor Data\nPelardinos - 2023 [13]\nTechnical status-quo\nKhebbache - 2023 [14]\nIrrigation Management\nHolzinger - 2022 [15]\nHuman-Centered AI in Agriculture\nPurcell & Neubauer [9] conduct a review on Digital Twins in\nagriculture and conclude that current applications benefit from\nthe start-of-the-art technologies such as IoT, Machine Learning\nand Cyber-Physical Systems.\nHowever, current research is\nlimited and focused on proving feasibility and novel methods\nmust be adopted to apply the concept to all agricultural use\ncases.\nNie et al.[10] propose dividing the crop growing process into\npreproduction, mid-production, and postproduction stages,\nemphasising the use of artificial intelligence (AI) in each phase.\nDespite the current use of AI, the authors highlight the need\nfor further testing of methods, analysis of existing approaches\nin theory and practice, and addressing issues related to data\nacquisition, storage, safety, and cost in future developments.\nIn the context of Digital Twins, the authors note that most\napplications focus on a single entity, such as a plant or an\nanimal, and advocate for standardisations across different\napplications as well as larger-scale implementations of Digital\nTwins.\nAttaran & Celik [11] review a broader range of applications\nfor Digital Twins and discuss agricultural use cases as one\napplication area.\nThe publication attests that agricultural\nDigital Twins are in the early development stages. However,\napplications in efficiency and productivity optimisation, as\nwell as weather modelling, soil management, supply chain\nmanagement, and livestock monitoring, exist and are being\nfurther developed.\nNasirahmadi & Hensel [12] concentrate their review on soil\nand irrigation management, along with Digital Twin appli-\ncations for farming machinery and post-harvest processes.\nDespite limited research in this domain, the findings suggest\n3\npromising research avenues include optimising processes,\npredicting optimal management decisions, and monitoring and\nmaintaining machinery.\nPelardinos et al. [13] point out the low number of agricultural\napplications in Digital Twin research.\nMost Digital Twins\npredominantly rely on simulation, while many other initiatives\nremain conceptual, necessitating further research. The signif-\nicance of sensors in (IoT)-based Digital Twins is emphasised,\nand cloud-based services are identified to handle the increasing\nvolume of sensor data best. Visual model-based Digital Twins\nleverage game engines for 3D representations of real-world\nentities.\nThe authors stress the need for enhanced focus\non developing reference models and case studies in future\nresearch. They identify the current state of IoT technologies as\na constraint for accurate Digital Twin models, advocating for\nbroader applications integrating 3D visualisation, augmented\nreality (AR), virtual reality (VR), and geographic information\nsystems (GIS). In agriculture, the potential extends beyond\nplant representations, urging a holistic focus on entire farms.\nKhebbache et al.\n[14] argue that the amount of literature\non smart irrigation has increased in recent years due to the\nincreased use of IoT. While machine learning and deep learning\nare currently the primary methods for solving irrigation system\ntasks, Digital Twins play a minor role in representing irrigation\nsystems but could potentially be used for sensor monitoring,\npresenting live readings of sensor data and predicting sensor\nfailures. Future developments related to Digital Twins should\nextend beyond water management and encompass the entire\nsoil by mapping and modelling it comprehensively.\nHolzinger et al. [15] highlight the importance of explainable\nand robust AI in agriculture and forestry, as they are crucial to\nhuman life. AI applications are classified as autonomous, au-\ntomated, assisted, or augmenting and examples in agriculture\nand forestry are given. In general, trust in AI decisions can be\nincreased by incorporating human/expert knowledge. Various\nchallenges related to technical implementations, automated sys-\ntems, and robotics, as well as improving farmers’ access to AI,\nare discussed, with the goal of Digital Twins being to represent\nentire farm systems virtually and include expert knowledge.\n4.2. Machine Learning in agriculture\nMachine Learning has become a topic of interest in many re-\nsearch areas, and agriculture is no exception, as is exemplified\nby the four review papers in Table 3. With Machine Learn-\ning being a wide-ranging topic, the models used in agriculture\nstart at simple regression and classification tasks but also in-\nclude various neural network structures that are used, for exam-\nple, in task automation. The diverse agricultural use cases are\nsummarised in this section.\nAccording to Gautron et al.\n[16], reinforcement learning\nshows promise as a technique for decision support in crop\nmanagement tasks as it learns from real-world experiments.\nHowever, its applications have been limited due to varying\nuser goals for different tasks, limited data availability, and the\nTable 3: Corpus of papers reviewed in Section 4.2\nPaper\nContent\nGautron - 2022 [16]\nRL for crop management\nBenos - 2021 [17]\nManagement of agricultural systems\nSharma - 2020 [18]\nPrediction of agricultural systems\nAbioye - 2023 [19]\nML in irrigation Management\nhigh risks associated with taking wrong actions, particularly\nconcerning food security.\nTheoretical challenges in this\nfield include efficient learning, modelling decision problems,\ncreating explainable policies, and handling multiple objectives\nunder resource constraints.\nOne possible solution for these\nchallenges is the multi-armed bandit framework.\nBenos et al.\n[17] conducted a comprehensive review of\nmachine learning applications in crop, water, soil, and live-\nstock management, focusing on crop management.\nRemote\nsensing image data is commonly utilised, and ANNs and\nensemble learning are deemed the most efficient models.\nThe integration of machine learning with Information and\nCommunication Technology is seen as a solution to future\nagricultural challenges. Decision Support Systems tailored to\nspecific cultivation systems utilise collected data, promoting\nsustainable and productive farming.\nNevertheless, the up-\nfront costs for farmers must be acknowledged and mitigated\nwhen implementing these systems, particularly in developing\neconomies.\nSharma et al. [18] concluded that regression tasks are most\ncommon for predicting soil properties, weather, and crop\nyield, while deep learning is more frequently used in classi-\nfication tasks such as pest detection. Automating harvesting\nor fertilisation tasks by AI-empowered robots or drones can\nhelp complete work more efficiently. To successfully deploy\nsmart systems for every farmer, addressing challenges related\nto improving model performance, educating and motivating\nfarmers, and addressing connectivity issues in rural areas is\nessential.\nAbioye et al. [19] used various techniques, from simpler mod-\nels such as k-means to advanced methods such as RNNs, CNNs\nand reinforcement learning for autonomous irrigation. Chal-\nlenges included limited data set availability, limited access to\ncloud services, and the high cost of digitising farms and infras-\ntructure. Proposed future research emphasises reinforcement\nlearning’s adaptability and self-learning, federated learning for\nenhanced data security, deploying technologies in less devel-\noped countries, and exploring Digital Twins for smart irriga-\ntion. The role of fertigation in generating training data is recog-\nnised.\n4\n5. Current Applications and Future Directions\nAs is evident from the publications in the related work sec-\ntion, while theoretical interest in Digital Twins in agriculture\nis substantial, practical implementation remains limited. Cur-\nrent research focuses on feasibility rather than broad adoption\nacross diverse agricultural sectors.\nConversely, various ma-\nchine learning methodologies to solve problems in agriculture\nare used, showcasing a broad range of applications. Despite\nthis, Digital Twins are underutilised, indicating untapped po-\ntential. Thus, exploring existing reinforcement learning appli-\ncations is crucial to expanding the utility of Digital Twins in\nagriculture. Leveraging reinforcement learning’s adaptability\nand autonomous learning capabilities alongside other machine\nlearning techniques offers avenues for comprehensive agricul-\ntural automation. The simulation aspect inherent in both Dig-\nital Twins and reinforcement learning further emphasises their\ncompatibility and potential synergy in agricultural applications.\nTo get a more accessible overview of possible topics for re-\ninforcement learning-based Digital Twins, the potential appli-\ncation areas are categorised according to already existing re-\ninforcement learning applications in agriculture. Furthermore,\nthese applications’ specific reinforcement learning techniques\nare categorised separately to determine the most promising\ntechnical implementations.\nIn total, 71 publications of the 200 sampled were deemed ap-\npropriate according to the criteria mentioned in the methodol-\nogy section. The resulting corpus is categorised according to\nthe area of application, which is summarised in Figure 2. In\nFigure 3 potential reinforcement learning-based Digital Twin\napplications, which are described in the following sections, for\neach domain are summarised. A structured overview of the re-\nviewed literature can be found in Table .4 that is attached in\nthe Appendix section. In this table, the papers are ordered ac-\ncording to the category they belong to. Furthermore, the pa-\nper’s topic is summarised briefly, and the reinforcement learn-\ning technique, as well as whether an actor-critic method was\nused or not, is listed.\n5.1. Robotics\nThe first, and by far the most prevalent area of research in\nrecent years, is robotics. In the context of this review, robotics\nencompasses research on unmanned vehicles and automated\nmachines that replicate human actions like robotic arms.\nWithin this category, unmanned aerial vehicles (UAVs), usually\ndrones, are mentioned in most publications. Automated drones\nplay an increasingly important role in agriculture because\nthey offer a cheap, easy and fast way to monitor larger land\nareas and optimise the efficiency of farming operations.\nIn\nmany cases, drones are used for monitoring purposes like\npest, disease, fire detection [24], plant growth monitoring, fly\ntrap inspection [23], or localisation of autonomous vehicles\nin unknown terrain to ensure their connectivity to a central\ncontrolling point [26]. To accurately execute the monitoring\ntasks, drone usage in agriculture is often closely linked to\nimage processing [21][22].\nAside from monitoring, in one\ncase, a reinforcement learning agent learned to control the\nFigure 2: Publications per Category\nFigure 3: Reinforcement learning-based Digital Twin applications for each Cat-\negory\ndrone’s velocity and height and spray an appropriate amount\nof pesticide on plants [25]. The learning goals for these tasks\nare to collect and send enough data to cover the entire area of\ninterest, avoid obstacles and, in the spraying case, spray the\ncorrect parts of the plants.\nWhile drones have significantly improved the efficiency of\nmonitoring tasks in agriculture, they are not without lim-\nitations.\nFor instance, their battery life and data storage\nand submission capabilities are restricted. Therefore, agents\ncontrolling drones need to find paths efficiently, avoid obstacles\nand coordinate with other agents to ensure the longevity and\noptimal usage of the UAVs, especially in the case where more\nthan one drone gets used at the same time [27][28][29][30][31].\nManaging battery and securing data quality levels from the\ntechnical side includes task offloading, where the agent decides\nwhether a task should be performed by the drone or in a central\ndata collection point, and buffer overflow and channel fading\nreduction. These challenges can be mitigated by controlling\nthe speed of drones and managing their connection [32][33].\nIn an attempt to standardise reinforcement learning for UAV\noperations, [34] present the OmniDrones environment, in\nwhich different agents and implementations can be tested\n5\nvirtually.\nRegarding reinforcement learning methods, deep Q-learning\nor deep Q-networks(DQN) are the most common techniques,\nstill [24] [26] and [28] use classical tabular Q-learning\nmodels.\nIn some cases, the learning methods are extended\nto a multi-agent (MARL) setting or actor-critic setting like\nDeep Deterministic Policy Gradient (DDPG) to ensure better\nlearning results [26][27][29][33]. Even though Q-learning is\nat the core of most learning strategies, the approaches differ\nin their environment exploration strategies and overall goals,\ncritical components for efficiently learning optimal policies.\n[21] use the k-nearest neighbour (KNN) algorithm to cluster\nsimilar states and use the difference between the clustering\nresults and a convolutional neural network (CNN) based crop\nhealth prediction to determine the flight direction. [22] tackle\na task scheduling and a crop monitoring problem. In the task\nscheduling case, the agent must decide whether to solve a\ntask on edge, cloud, or FoG devices. Due to time constraints,\ncomputing an optimal solution is infeasible, and therefore, the\nagent relies on the ant colony optimisation heuristic (ACO) to\nschedule the tasks appropriately. A classical DQN performs\nthe task of prediction and monitoring in the cloud or fog. To\nhelp their DQN implementation learn optimal strategies faster\n[23] use rapidly exploring random trees (RRT) for quicker\nenvironment exploration.\nRobots hold the potential to revolutionise the agriculture\nindustry by taking over labour-intensive tasks such as har-\nvesting and fruit picking, which are currently costly and\ntime-consuming.\nCurrent research focuses on automating\nrobotic arms to perform these tasks. The ultimate goal for these\nfruit-picking robots is to identify the fruit accurately, chart the\noptimal path to it while avoiding obstacles, and harvest it with-\nout causing any damage to the fruit or the plant [35][36][37].\nThe agent controlling the arm should also be able to plan the\norder of picking the fruits, which can be achieved by exploring\nthe environment and selecting the next best target [38]. Case\nstudies have demonstrated the successful application of these\nrobots in apple detection and picking, cherry tomato picking,\nand automated harvesting in greenhouses [39][40][41]. Due to\ntheir linear layout, vineyards are the testing ground for moving\nrobots that, in the first step, learn how to reach the end of the\nline.\nIn the future, these robots will also learn to monitor,\nspray and harvest grapes [42]. Like harvesting, crop pruning\nis considered a labour-intensive task necessary to ensure crop\nhealth and maintain crop yields. One publication presented a\nreinforcement learning agent that learns to prune vine crops\n[43].\nFor harvesting tasks of larger fruits like bananas or fruits that\nare more challenging to reach, heavier machinery than a robot\narm is needed [44][45]. Current research for these unmanned\nground vehicles (UGVs) is at a similar stage to that of UAVs.\nPathfinding for harvesting machines or tractors on fields is\nnecessary to cover the entire area and efficiently fulfil a given\ntask.\nFurthermore, obstacles and avoiding them present the\nsame problems on the ground and in the air. Approaches to\npathfinding include presenting the UGV with a topographical\nmap of the land in advance, planning a path according to it,\nor using sensors and cameras to monitor the area close to the\nvehicle and decide, given on the sensory input, where to drive\n[46][47][48][50].\nAn additional challenge for ground-based\nvehicles is moving in difficult terrain, as the soil in fields is\nuneven or the fields are located in undulated terrain. Therefore,\nthe agent must learn how much power to use and which wheels\nto power so as not to damage the vehicle or the soil [49].\nThe technical reinforcement learning implementations for\nrobotic arms and UGVs are very similar to those used for\nUAVs, with DQN being used most frequently.\nCritic-based\nmethods are also as prevalent and range from soft actor-critic\n(SAC) methods [40][42][46][50] to custom implementations of\nstudent-teacher relations. In the last case, the teacher network\nacts as a target network with additional information not present\nin the student network and, at the same time, acts as a critic to\nthe student [37]. Interstingly, experience replay for faster and\nmore stable learning is more prevalent for UGVs and robotic\narms [38][40][45]. Regarding reward signals, [44] model their\nbanana harvesting problem in a sparse reward setting, meaning\nthat the agent rarely observers positive rewards.\nTherefore,\nthe authors implement an automatic goal generation, randomly\nsampling targets along the way to the overall goal to facili-\ntate efficient learning. To solve the pathfinding problem, [48]\ngradually increase the reward in circular areas around the target.\nAs is evident from the abundance of literature assigned\nto this category, reinforcement learning delivers promising\nresults regarding the automatisation of UAVs or UGVs. The\nclearly defined goals, such as monitoring a predefined area,\npicking a fruit or reaching a targeted area by driving there,\nhelp define action spaces and rewards, making it easy to\ndefine a reinforcement learning problem for these tasks. More\nchallenging is the accurate representation of the environment to\nachieve satisfying learning results and, therefore, the definition\nof the state space.\nThree-dimensional environments often\nrequire lots of computing power in their creation, and it takes\nlots of effort to represent the robot and the target accurately.\nEspecially compared to crop management tasks, predefined\nenvironments to quickly implement and test an agent are\nscarce, leaving lots of possible development paths in that\nregard.\nIn robotic automation, reinforcement learning seems to be\nthe most promising machine learning approach compared to\nother techniques, as it is self-learning and does not require a\npredefined dataset. For pathfinding problems where obstacle\navoidance is the primary goal, heuristic approaches such as ant\ncolony optimisation might be a competitor to reinforcement\nlearning techniques that deliver similar results where, in\naddition, the decision-making process can be explained.\nConcerning reinforcement learning-based Digital Twins, UGV\napplications are particularly promising domains because of the\nclearly defined real-world entity to model and goals. Therefore,\nmonitoring the vehicle’s condition and the surrounding area\ncan be connected to learning policies for automated task com-\npletion.\nFurthermore, the agent’s behaviour under changing\nconditions, for example, if different crops are planted or the\n6\nterrain changes, can be simulated and allows the assessment of\npotential scenarios before actually employing the vehicle in the\nreal world.\n5.2. Crop Management\nThe second largest category contains publications that present\ndevelopments in crop management practices. For this paper,\ncrop management encompasses research in crop yield predic-\ntion, nutrition management, crop growth estimation and crop\nplanning. Due to the comparatively large number of publica-\ntions, irrigation management and greenhouse applications will\nform separate categories.\nIn current research on reinforcement learning for crop man-\nagement, the overarching goal is to maintain, increase and\npredict crop yields.\nShort and long-term weather patterns,\nsoil conditions and plant conditions directly influence crop\nyields, so the main challenge is to find how and to what extent\neach factor influences crop yields. As no linear or non-linear\nrelationship could be modelled, [51] used reinforcement\nlearning to solve the crop yield prediction problem. Due to\nthe predictor variables’ time dependence, a recurrent neural\nnetwork (RNN) is used to predict crop yields and initialise the\nweights of the reinforcement learning agents DQN. The agent\nuses the RNN prediction as a target and tries to achieve the\ngoal by selecting parameter values for each variable to attain\nthe predicted value. Another approach to handle the non-linear\nrelationship is to use a reinforcement forest that was developed\nby [52]. In random forests, much focus lies on the splitting\ncriterion of a node. In a reinforcement forest, a reinforcement\nlearning model determines the variables’ importance in each\nnode and then splits based on the computed importance value.\nCrop growth and, therefore, crop yields are highly dependent\non soil nutrient levels, and it is of great interest to measure the\neffects of different nutrition levels on crop development and\ninfluence them accordingly. Reinforcement learning agents can\nlearn to manage nitrogen levels aiming at increasing crop yields\n[53] or rely on the classification output of a CNN to detect\nmalnourished rice and suggest fertilising it with nitrogen,\nphosphorus or potassium [54].\nOther publications in crop management focus on IoT-based\nsensor detection for optimising crop yields.\n[55] use tem-\nperature, humidity, fertiliser usage and rainfall as an input\nto a DQN-based agent to ensure optimal crop growth and\nyield, whereas [56] focus solely on beetroot by training a\nDDPG-based agent to model crop development based on light\nintensity, temperature, CO2 levels, humidity and soil nutrient\nlevels.\nA novel approach to reinforcement learning is taken by [57],\nwho try to predict crop evaporation based on minimal and\nmaximal temperature and sunshine for each day. The employed\nreinforcement learning model learns to pick the best baseline\nmodel for each prediction timestep among CNN with long\nshort-term\nmemory\n(CNN-LSTM),\nConvolutional-LSTM\n(Conv-LSTM), CNN with eXtreme gradient boosting (CNN-\nXGB) and CNN with support vector regression (CNN-SVR).\nAccording to the authors, this ensemble approach is necessary\nbecause each model alone cannot predict the evaporation as\nwell as the ensemble due to different baseline accuracies.\nCrop planning and crop rotations can positively impact crop\nyields, which leads [59] to develop a DQN-based agent that\nrecommends crops based on soil conditions.\nIn contrast to all other categories presented in this paper, crop\nmanagement sees much attention in reinforcement learning\nenvironment development. The environment is a crucial part of\nany reinforcement learning implementation, as the agent learns\nthe effects of actions by interacting with the environment and\nobserving states that the environment presents. Due to its sim-\nple usability, in general, reinforcement learning applications\nlike simple games, the Gymnasium environment (Gym) is used\nto train agents and evaluate their performance [20]. Efforts in\nagriculture have been undertaken to create environments that\nemulate the Gym structure but make it usable for agricultural\nsimulations. Available Gym-based environments are CropGym\n[61][62] that incorporates multiple process-based plant growth\nmodels and allows agents to study the effects of different\nnitrogen fertilisation schemes and CyclesGym [63] where\nagents can learn crop rotation policies based on soil nitrogen\nlevels and simulate plant growth among factors like soil nitro-\ngen, carbon levels water balance and external perturbations.\nAnother environment for a simple plant simulation model that\nalso has a Gym-based interface was implemented by [59].\nAn approach outside the Gym-based environment realm is\nrealised by [60], who developed an environment around the soil\nand water assessment tool (SWAT). Reinforcement learning\nagents trained within this environment learn to optimise crop\nyields while reducing water and fertiliser usage and, therefore,\nsaving resources but keeping track of external factors like\ntemperature, soil moisture levels and precipitation.\nMany crop management tasks involve predicting future\noutcomes, like crop yields, crop evaporation or crop growth.\nFrequently, many factors that cannot be modelled in a linear or\nnon-linear fashion influence these prediction targets. Through\nits exploration property, reinforcement learning learns how\nthe prediction target reacts if external factors are changed\nand can find suitable predictions. Another advantage of using\nreinforcement learning for crop management is that much effort\nhas already been made towards standardising environments,\nallowing faster development of new agents.\nUsually, for prediction tasks, there are historical data and data\non influencing factors available. Especially if the assumption\nof a linear relationship is reasonable, the available data enables\nusing simpler prediction models that are even more accessible\nto deploy than new agents in predefined environments.\nCrop modelling on a field or plant level are potential applica-\ntions for reinforcement learning-based Digital Twins. These\nDigital Twins can help optimise crop yields for entire fields by\ndetermining optimal crop orders, considering external weather\nfactors and managing water and fertiliser usage.\nFor closer\ninspection of how these factors influence a particular plant\nand how it would react to changes in these conditions, the\nsmaller-scale plant-level Digital Twins could be used.\n7\n5.3. Irrigation Management\nAn essential point of every agricultural operation is irrigation\nmanagement, as plant growth and crop yields are heavily\ninfluenced by the amount of water they get. Furthermore, water\nusage in agriculture is a topic that sees lots of interest in the\ncontext of climate change and more frequent occurrences of\nwater shortages worldwide. Because it is such an important\ntopic, there are many attempts at reinforcement learning con-\ntrolled irrigation management in agriculture. Compared to the\ntopics in the other sections, the goals for publications within\nthis category are similar: keep crops healthy by controlling\nsoil moisture levels while using as little water as possible.\nAside from the crop’s regular water usage that it needs for\ngrowth, weather patterns and evaporation are factors that\ninfluence soil moisture levels. [64][66]. Specific crops that\nwere used for testing irrigation schemes are tomatoes, rice and\nmaise [67][68][69][70]. Greenhouses lend themselves well to\nresearch as they present a controlled environment, which will\nbe discussed more closely in the next section. However, [71]\nspecifically focus on greenhouses’ irrigation management. [72]\ntrain multiple DQN-based agents for various plots of land and\nobserve the effects of water usage in the Colorado River Basin\naccording to the Colorado River Simulation System.\nSimilarly to crop management in general, environments to train\nreinforcement learning agents are also developed for irrigation\nmanagement. Both the Aquacrop-gym and the gym-DSSAT\nenvironments are Gym-based environments that can be used\nto develop irrigation policies [65][73].\nThe gym-DSSAT\nimplementation transfers the frequently used DSSAT model\ninto a Python environment.\nEvery reinforcement learning agent developed for use-cases\nin this category relies on DQN, except for the one used by\n[64], who use a tabular Q-learning approach and [66], who\nimplement Proximal Policy Optimization (PPO). [67][68] and\n[70] employ actor-critic methods with [67] using an LSTM\nnetwork and CNN for yield predictions in their tomato case\nstudy.\nEspecially in the context of environmental sustainability,\nwater management requires lots of precision in order not to\nwaste unnecessary resources. As the required water for optimal\ncrop growth can be monitored through soil moisture, crop\nevaporation and natural precipitation, reinforcement learning\nagents can quickly assess the situation and decide when to\nwater the plants dynamically. Using the available sensor data\nalso allows to virtually replicate the entire water management\nsystem, which can be integrated into the field-level reinforce-\nment learning-based Digital Twins discussed in the previous\nsection.\nThe required data to assess the current moisture levels can also\nbe used for other heuristic or non-heuristic decision support\nalgorithms that suggest when and how much water to use.\nGiven the importance of saving water amidst climate change,\nwasting as little water as possible should be the primary\ngoal in irrigation management. Quick adaptions to changing\nconditions are required and should be an optimal use case for\nreinforcement learning agents.\n5.4. Greenhouses\nCompared to regular farms, greenhouses allow control of\nenvironmental parameters like temperature, light, precipitation,\nand humidity.\nThe ability to control for external factors in\nplant growth makes greenhouses an interesting testing ground\nfor researchers and makes it possible to grow plants all year\nround. However, due to the constant environmental control,\ngreenhouses use lots of energy, making them less sustainable\ncompared to farming outside greenhouses.\nSince automated controlling is one of the strengths of re-\ninforcement learning, it is no surprise that agents are being\ndeveloped to control the environment efficiently in greenhouses\nwith the push towards more sustainable farming practices.\nDifferent from the applications in the other categories, there\nare more common models for greenhouse reinforcement\nlearning agents than DQN alone. In most cases, actor-critic\napproaches like DDPG [74] and SAC [75] are used. For [75]\nand [79], avoiding worst-case scenarios is essential, and they\nachieve this by masking specific actions that lead to critical\nconditions in advance. [76] use a MARL approach to simulate\nclimate control by splitting the action space into subactions,\nbuilding a reinforcement learning model for each subaction\nand maintaining correlations between the different actions.\nThese submodels are embedded into a hierarchical structure,\nwhere one overarching model controls the submodels.\nThe\nframework is called:\nstructured cooperative reinforcement\nlearning algorithm (SCORE). Another multi-agent reinforce-\nment learning model is implemented by [77], who aim to\nstabilise the power usage of their greenhouse to reduce stress\non the power network while also trying to reduce the total\npower consumption.\nThe agents are connected by a shared\nattention mechanism to facilitate faster learning and better\ncooperation between the agents.\nTo avoid online learning\nmethods, [78] rely on historical data and climate trajectories\nof their greenhouse DQN. In addition to the agent, a mixed\ninteger linear program is defined after each learning phase that\nhelps the agent avoid overfitting to the historical data.\nIn contrast, [80] use their greenhouse’s available climate and\nplant-specific sensor data to control the artificial lighting\nusing a DQN agent dynamically. Controlling artificial lighting\ninfluences not only energy consumption but also plant growth,\nwhich [81] use to increase the dry mass of Spirulina Sp. An\nLSTM network was used to predict the next day’s light inten-\nsity by relying on data from the past days, and based on the\nprediction, a tabular Q-learning model was trained to choose\namong four options how much light should be artificially added\nto maximise the dry mass of Spirulina Sp.\nMost greenhouse applications presented so far rely on sensor\ndata at some point in their automatisation process.\nThat\nmeans the quality of their models depends on the quality of\nthe collected data, which is why it is crucial to determine\noptimal sensor locations within a greenhouse.\n[82] try to\nfind an optimal solution to the sensor placement problem by\nimplementing a Bayesian reinforcement learning approach that\nrelies on Thompson sampling for exploration and exploitation.\nIn addition to climate control, crop yield predictions, for\nexample for strawberries, often grown in greenhouses due\n8\nto their permanent demand, can be made using the available\nclimate data.\nThe available data is fed into an informer\nthat predicts strawberry growth.\nThe predicted values are\nthen presented as a target to a Q-learning model that learns to\nregulate climate conditions by aiming to achieve the target [83].\nAs energy efficiency for greenhouses is one of the most\npressing goals for research in this category, reinforcement\nlearning shows its strength in simultaneously handling multiple\nvariables such as temperature, humidity, and light intensity\nto control the greenhouse climate accurately.\nWith respect\nto reinforcement learning-based Digital Twins, greenhouses\nbehave similarly to UGVs in that there is a clearly defined\nentity to replicate and the optimisation goals for reinforcement\nlearning can also be defined in straightforward way. Again the\nsimulation aspect of Digital Twins allows to gain knowledge\nhow certain actions and conditions would affect the greenhouse\nin advance and therefore help identify potential improvements\nfor their energy consumption.\nBesides energy reduction, the goal is maintaining an environ-\nment that secures stable crop yields or even increases them.\nHowever, depending on the available data, the problem might\nalso be solved using a linear program if constraints to variables\nand their relation to crop development are known.\nOther\ntechniques that can also handle non-numeric variables and\nare able to suggest actions are decision tree-based methods.\nBoth linear programs and decision tree methods are more\nstraightforward to implement, and the performed actions are\neasier to understand. However, reinforcement learning agents\nmight react faster to changing conditions and, therefore, handle\nthe climate more accurately.\n5.5. Other Applications\nThe publications in this category do not fit into any of the\nprevious groups.\nHowever, this does not imply that the\nworks’ contribution can be considered entirely independent\nof the previously discussed topics.\nMany applications rely\non collected sensor data, and [84], [85] and [86] illustrate\nhow reinforcement learning manages data transmission. The\npublications focus on the technical solution to the transmission\nproblem, which has potential applications in agriculture and\nbeyond.\nAccording to [84], sensors are small, light, and\ninexpensive but have a relatively short battery life. Therefore,\nthe DDPG algorithm is used to learn a policy that aims to\nsend information regularly to keep it up to date while trying\nto conserve energy simultaneously. The agent collects rewards\nonly if the information is accurate and the time between two\ntransmissions is long.\n[85] also aim to extend the battery life of sensors like UAVs,\nbut they focus less on the accuracy of data transfer than on\nsolving the tasks efficiently.\nTo achieve this, the authors\nhave implemented a Q-learning agent trained to maximise the\ndrone’s battery life while simultaneously finishing given tasks\nbefore a deadline. The reinforcement learning model for each\ndrone can determine whether it completes the task, passes it on\nto another drone, or sends it to a shared data collection point\nfor it to be finished there.\nEfficient data transmission is crucial for sustainable smart\nfarming. Smart farms typically use multiple sensors distributed\nover a large area and consume significant amounts of energy.\nTo optimise data transmission between sensors and a central\ndata collection point, [86] developed a tabular Q-learning\nmodel to identify the optimal transmission path.\nThe agent\naims to achieve complete and energy-efficient transmissions\nand transmit data without delay, even in areas without signal.\nThe authors claim that the approach can also be applied to\nmore extensive sensor networks.\n[87] focus on the quality of experience of video data, which\ncan be used for general surveillance purposes and specifically\nfor livestock monitoring in agriculture. An average advantage\nactor-critic (A3C) reinforcement learning agent is trained to\nhandle dynamic network conditions by adapting the bitrates to\nensure a high-quality video stream.\nWhile efficient data collection and transmission are essential\nin optimising farming operations, it is necessary to have the\nmeans to store and process the available data securely. The\nmost common practice in data analysis is to collect all available\ninformation in one dataset or database and access it when\nneeded, which may require lots of data transmissions and,\ntherefore, potential security risks. One way to mitigate this risk\nis to develop a federated learning network, where data analysis\nis done locally at the data collection point, and only model\nparameters are shared between different analysis entities.\nDeciding which user’s model parameters to incorporate into\nthe overall model is necessary to ensure model quality and\naccuracy. [88] propose to perform a spectral clustering on all\nusers in a federated learning network and have a DQN-based\nagent decide which clusters to use for the overarching model.\nReinforcement learning in agriculture is not only used to in-\ncrease crop yields, reduce labour or make tasks more efficient,\ninstead [89] show that both tabular Q-learning and DQN can\nbe used to optimise the agri-food supply chain by modelling it\nas a blockchain environment. The main goal for their agents is\nto increase product profits for the farmers.\nAnother DQN application in agriculture is frost forecasting.\nA Fuzzy-based DQN uses historical data to predict frosts to\nensure agricultural productivity and maintain stable crop prices\n[90].\nIn a very different application to frost prediction, DQNs’\nflexible use cases are highlighted as they can also be used for\ngenomic selection for plant breeding parent selection.\nThe\nagents’ goal is to allocate resources for each plant generation\nand, in doing so, decide which parents are best suitable for the\ndesired breeding goal [91].\nThe reinforcement learning applications in this category\nshow that using machine learning, data transmission accuracy,\nquality and security, as well as task offloading, can be im-\nproved. Reinforcement learning-based Digital Twins’ primary\nuse case might be to monitor and control network traffic\nand optimise it by more efficient task scheduling or sensor\nplacement.\nAs discussed for the previous categories, the reinforcement\nlearning agents might be replaced by similarly performing\n9\npathfinding heuristics or task scheduling techniques. However,\ntechnical improvements in longer-lasting batteries, more stable\nnetworks, and increased sensor transmission capabilities might\noutpace the quality of machine-learning implementations.\nTherefore, these novelties will alleviate some pressure for\noptimisation, as fewer sensors that last longer and transmit\nwith a higher degree of certainty might be introduced.\n5.6. Reinforcement Learning Techniques\nIn the previous sections, different reinforcement learning meth-\nods have been presented for varying use cases. Still, a com-\npact summary of the different models allows to assess the most\nprevalent techniques in current agricultural research. The rein-\nforcement learning methods are summarised in Figure 4. DQN\nFigure 4: Reinforcement Learning Techniques\nis by far the most common approach to solving reinforcement\nlearning tasks. As described in the previous sections, DQN is\nsuitable for many tasks, such as energy control in greenhouses,\ncontrolling a robotic arm or recommending crop planting orders\nand irrigation schemes. Frequently, though, DQNs are com-\nbined with other techniques to facilitate faster learning by effi-\nciently exploring the state space or collecting higher rewards,\nyielding better policies.\nIn time series-dependent situations,\nRNNs are commonly connected to DQN agents as they allow\nthe retention of a memory of past events. For image process-\ning tasks, CNNs have proven reliable in, for example, outlier\ndetection, where the agent decides how to best act given an out-\nlier. Actor-critic methods such as SAC are used to optimise\nthe learned policies. Figure 5 summarises in how many pub-\nlications actor-critic methods were used in total. Missing val-\nues signify that a learning environment was created that allows\nthe implementation of various reinforcement learning methods,\nboth actor-critic and non-actor-critic. In addition, for specific\ntasks, DQNs were combined with a Fuzzy learning approach,\nant colony optimisation, and a multi-integer linear program, or\nthey were embedded in a hierarchical structure like a tree or\nused multi-agent settings.\nFigure 5: Reinforcement Learning Techniques\nMulti-agent (MARL) settings are standard for cases where mul-\ntiple entities like a swarm of drones or different water users\nneed to be controlled simultaneously or if an agent handles the\nsubdivisions of state or action spaces.\nEven though deep reinforcement learning methods are the most\ncommon, there are cases where tabular methods like tabular Q-\nlearning are used, especially if the action space is small enough.\nTabular methods usually require more memory space as tables\nfor state-action pairs must be maintained. However, their easy\nimplementation and traceable decision-making processes still\nmake them an attractive alternative to more involved deep learn-\ning methods.\nEven though there are methods that allow for interpretations of\nlearned decisions, the explainability of reinforcement learning\nalgorithms for agricultural use cases is a topic that is widely\ndisregarded. However, explainable decisions are a crucial part\nof gaining trust in learned policies and, therefore, help decrease\nthe reluctance of farmers to use machine learning-powered de-\ncision support.\n6. Conclusion\nIn conclusion, this review highlights the early developmental\nstage of Digital Twins in agriculture but also the widespread\nutilisation of various machine learning techniques within the\nagricultural sector. Among these machine learning techniques,\nreinforcement learning is a frequently used approach, particu-\nlarly in the defined categories of robotics, crop management,\nirrigation management, and greenhouses. The most common\nmethod to solve reinforcement learning problems is to use\nDeep Q-Networks (DQN). It was also shown that reinforcement\nlearning is often combined with Recurrent Neural Networks\n(RNNs), Convolutional Neural Networks (CNNs), and other\noptimisation techniques. However, there is a notable absence\nof focus on explainable reinforcement learning techniques, in-\ndicating a significant area for future development, especially\n10\nconsidering the importance of trustworthiness in AI.\nMoreover, this review highlights already existing applications\nfor reinforcement learning-based Digital Twins, particularly in\nrobotics, greenhouses, and crop models. The scope of imple-\nmentation for these domains is manageable due to the clearly\ndefined and replicable nature of the corresponding real-world\nentities. Even though some of the Digital Twins are more ad-\nvanced, the lack of standardised learning environments and ap-\nplications only in small-scale use cases indicates that there is a\nneed to optimise these applications further to implement more\nefficient Digital Twins faster. Exploring the integration of rein-\nforcement learning-based Digital Twins in these areas presents\npromising opportunities to advance agricultural automation and\nenhance productivity.\nAcknowledgements\nThe authors acknowledge the funding of the project “digi-\ntal.twin.farm” funded by the Austrian Federal Ministry of Edu-\ncation, Science and Research.\n11\nAppendix\nTable .4: Corpus of the reviewed Publications\nAuthors\nTopic\nCategory\nRL Method\nActor-Critic\nZhang - 2020 [21]\nUAV Crop Monitoring\nRobotics\nDQN\nFALSE\nGanesh - 2023 [22]\nUAV Crop Monitoring\nRobotics\nDQN + ANT Colony\nFALSE\nBoubin - 2022 [27]\nUAV Swarm Manage-\nment\nRobotics\nMARL\nFALSE\nCastro - 2023 [23]\nUAV Path Planning and\nMonitoring\nRobotics\nDQN + RRT\nFALSE\nPamuklu - 2023 [24]\nUAV Crop Monitoring\nRobotics\nQ-learning\nFALSE\nYang - 2022 [46]\nUGV Path Planning\nRobotics\nSAC\nTRUE\nMartini - 2022 [42]\nUGV\nfor\nVineyard\nMonitoring\nRobotics\nDQN + SAC\nTRUE\nHao - 2022 [25]\nUAV\nSpraying\nAu-\ntomation\nRobotics\nDQN + AC\nTRUE\nPourroostaei Ardakani\n- 2021 [28]\nUAV Route Planning\nand Monitoring\nRobotics\nQ-learning\nFALSE\nTesti - 2020 [26]\nUAVs for UGV Moni-\ntoring and Connection\nRobotics\nMARL\nFALSE\nGuichao - 2021 [29]\nUAV Route Planning\nRobotics\nDQN + RNN + DDPG\nTRUE\nPetrenko - 2020 [41]\nAutomated Harvesting\nRobot in Greenhouse\nRobotics\nDQN + RNN\nFALSE\nFaryadi - 2021 [47]\nUGVs in map field To-\npography\nRobotics\nQ-learning\nFALSE\nYinchu - 2023 [45]\nUGVs\nfor\nKiwifruit\nPicking\nRobotics\nDQN\nFALSE\nZeng - 2022 [38]\nUGV Automated Envi-\nronment Detection\nRobotics\nDDQN\nFALSE\nYajun - 2024 [40]\nCherry Tomato Picking\nRobotics\nDQN + SAC\nTRUE\nTian - 2023 [35]\nArm Path Planning for\nFruit Picking\nRobotics\nEnvironment\nYandun - 2021 [43]\nVineyard Pruning\nRobotics\nDQN + CNN\nFALSE\nAndriyanov - 2023 [39]\nApple\nDetection\nand\nPicking\nRobotics\nQ-learning\nFALSE\nWiberg - 2022 [49]\nHeavy UGV Path Find-\ning in Difficult Terrain\nRobotics\nPPO + Bayesian AC\nTRUE\nJosef - 2020 [48]\nUGV for Close Range\nSensing and Path Find-\ning\nRobotics\nDQN\nFALSE\nNguyen - 2022 [32]\nUAV Task Offloading in\nArea Surveillance\nRobotics\nDQN\nFALSE\nNethala - 2023 [36]\nArm Path Planning for\nFruit Touching\nRobotics\nDDPG + PPO + Hierar-\nchy\nTRUE\n12\nTable .4: Corpus of the reviewed Publications\nAuthors\nTopic\nCategory\nRL Method\nActor-Critic\nYang - 2020 [37]\nVision-based\nArm\nLearning\nRobotics\nCross Modal DQN\nFALSE\nLin - 2022 [44]\nLarge UGV for Banana\nHarvesting\nRobotics\nTD3\nTRUE\nKurunatahn - 2021 [33]\nUAV\nCruise\nControl\nto reduce Buffer Over-\nflows\nand\nChannel\nFading\nRobotics\nDDPG\nTRUE\nAlon - 2020 [30]\nUAV Swarm Coordina-\ntion and Cooperation\nRobotics\nMulti-Agent PGO\nTRUE\nXu - 2024 [34]\nOmniDrones - Environ-\nment for RL Drone Im-\nplementations\nRobotics\nEnvironment\nRoghair - 2022 [31]\nUAV Obstacle detec-\ntion\nRobotics\nDQN\nFALSE\nDin - 2022 [50]\nLand\nArea\nCoverage\nControl\nRobotics\nDDQN\nFALSE\nElavarasan - 2020 [51]\nCrop Yield Prediction\nCrop Management\nDQN + RNN\nFALSE\nWu - 2022 [53]\nNitrogen Management\nCrop Management\nDQN + SAC\nTRUE\nYassine - 2022 [55]\nCrop Growth and Yield\nManagement\nCrop Management\nDQN + RNN\nFALSE\nMadondo - 2023 [60]\nYield\nMaximization\nand Resource Reduc-\ntion\nCrop Management\nEnvironment\nZheng - 2021 [56]\nSensor-based\nCrop\nYield Increase\nCrop Management\nDDPG + AC\nTRUE\nSharma - 2022 [57]\nCrop Evaporation Esti-\nmation\nCrop Management\nDQN\nFALSE\nElavarasan - 2021 [52]\nCrop Yield Prediction\nCrop Management\nReinforcement Forest\nFALSE\nOverweg - 2021 [61]\nCropGym\n-\nCrop\nGrowth Simulation\nCrop Management\nEnvironment\nWang - 2021 [54]\nMalnutrition Detection\nin Rice\nCrop Management\nQ-learning\nFALSE\nKallenberg - 2023 [62]\nCropGym\n-\nCrop\nGrowth Simulation\nCrop Management\nEnvironment\nTurchetta - 2022 [63]\nCyclesGym\n-\nCrop\nRotation Planning and\nPlant Growth\nCrop Management\nEnvironment\nAshcraft - 2021 [59]\nCrop Yield Optimisa-\ntion\nbased\non\nPlant\nGrowth model\nCrop Management\nPPO\nFALSE\nBouni - 2022 [58]\nCrop Planting Recom-\nmendations\nCrop Management\nDQN\nFALSE\nTropea - 2022 [64]\nIrrigation Management\nIrrigation Control\nQ-learning\nFALSE\n13\nTable .4: Corpus of the reviewed Publications\nAuthors\nTopic\nCategory\nRL Method\nActor-Critic\nZhou - 2020 [71]\nIrrigation Management\nin Greenhouse\nIrrigation Control\nDQN\nFALSE\nAlibabaei - 2022 [67]\nIrrigation Management\non Tomato Field\nIrrigation Control\nDQN + CNN\nFALSE\nAlibabaei - 2022 [68]\nIrrigation Management\nIrrigation Control\nA2C\nTRUE\nChen - 2021 [69]\nIrrigation Management\nfor Rice\nIrrigation Control\nDQN\nFALSE\nKelly - 2024 [65]\nIrrigation Management\nIrrigation Control\nEnvironment\nGautron - 2022 [73]\ngym-DSSAT\nfor\nPython\nIrrigation Control\nEnvironment\nHung - 2021 [72]\nWater User Coordina-\ntion\nIrrigation Control\nDQN\nFALSE\nDing - 2022 [66]\nIrrigation Management\nIrrigation Control\nPPO\nFALSE\nTao - 2022 [70]\nIrrigation and Fertiliza-\ntion Management\nIrrigation Control\nDQN + SAC\nTRUE\nWang - 2020 [74]\nClimate Control\nGreenhouse\nDDPG\nTRUE\nZhang - 2021 [75]\nClimate Control\nGreenhouse\nSAC\nTRUE\nUyeh - 2021 [82]\nSensor Placement\nGreenhouse\nBayesian RL\nFALSE\nLi - 2022 [76]\nSimulated Control\nGreenhouse\nMARL\nFALSE\nLu - 2023 [83]\nStrawberry Yield Pre-\ndiction\nGreenhouse\nQ-learning\nFALSE\nAjagekar - 2024 [77]\nPower Usage Optimiza-\ntion\nGreenhouse\nMARL + AC\nTRUE\nAjagekar - 2023 [78]\nEnergy\nEfficient\nCli-\nmate Control\nGreenhouse\nDQN + MILP\nFALSE\nChen - 2023 [79]\nEnergy Saving through\nLight Control\nGreenhouse\nPPO\nFALSE\nDecardi-Nelson - 2023\n[80]\nResource Optimization\nin\nPlant\nFactories\nthrough Light Control\nGreenhouse\nDQN\nFALSE\nDoan - 2021 [81]\nLight Controlled Spir-\nulina Sp. Farming\nGreenhouse\nQ-learning\nFALSE\nHerabad - 2022 [90]\nFrost Forecasting\nOther\nFuzzy DQN\nFALSE\nAhmadi - 2022 [88]\nFederated\nLearning\nuser Selection\nOther\nDDQN\nFALSE\nNaresh - 2022 [87]\nVideo Stream Quality\nImprovement\nOther\nFFE + A3C\nTRUE\nHribar - 2022 [84]\nSensor Battery Life Im-\nprovement\nOther\nDDPG\nTRUE\nNguyen - 2022 [85]\nUAV Task Offloading\nOther\nQ-learning\nFALSE\nMoeinizade - 2022 [91]\nGenomic Selection in\nPlant Breeding\nOther\nDQN\nFALSE\n14\nTable .4: Corpus of the reviewed Publications\nAuthors\nTopic\nCategory\nRL Method\nActor-Critic\nAli - 2023 [86]\nSensor Battery Life Im-\nprovement\nOther\nQ-learning\nFALSE\nChen - 2021 [89]\nAgri-Food\nSupply\nChain Optimization\nOther\nQ-learning + DQN\nFALSE\n15\nReferences\n[1] Malhi, G., Kaur, M. & Kaushik, P. Impact of Climate Change on\nAgriculture and Its Mitigation Strategies: A Review. Sustainability.\n13 (2021), https://www.mdpi.com/2071-1050/13/3/1318, Vis-\nited on 2024-01-10\n[2] World\nBank\nGroup.\nPopulation\nEstimates\nAnd\nProjections.\n(2024),\nhttps://databank.worldbank.org/source/\npopulation-estimates-and-projections,\nAccessed:\n2024-\n01-10\n[3] Burgos, D. & Ivanov, D. Food retail supply chain resilience and\nthe COVID-19 pandemic:\nA digital twin-based impact analy-\nsis and improvement directions. Transportation Research Part E:\nLogistics\nAnd\nTransportation\nReview.\n152\npp.\n102412\n(2021),\nhttps://www.sciencedirect.com/science/article/pii/\nS1366554521001794, Accessed: 2024-01-10\n[4] Lobell, D. & Gourdji, S. The Influence of Climate Change on\nGlobal Crop Productivity. Plant Physiology. 160, 1686-1697 (2012,10),\nhttps://doi.org/10.1104/pp.112.208298, Accessed: 2024-01-\n10\n[5] Brucherseifer, E., Winter, H., Mentges, A., M¨uhlh¨auser, M. & Hell-\nmann, M. Digital Twin conceptual framework for improving critical\ninfrastructure resilience. At - Automatisierungstechnik. 69, 1062-1080\n(2021), https://doi.org/10.1515/auto-2021-0104, Accessed:\n2024-01-10\n[6] Grieves, M. & Vickers, J. Digital Twin: Mitigating Unpredictable, Un-\ndesirable Emergent Behavior in Complex Systems. Transdisciplinary\nPerspectives On Complex Systems: New Findings And Approaches. pp.\n85-113 (2017), https://doi.org/10.1007/978-3-319-38756-7_\n4, Accessed: 2024-01-10\n[7] Sutton, R. & Barto, A. Reinforcement learning: An introduction. (MIT\npress,2018), Accessed: 2024-01-10\n[8] Richard\nBellman\nDynamic\nProgramming.\nScience.\n153,\n34-37\n(1966), https://www.science.org/doi/abs/10.1126/science.\n153.3731.34\n[9] Purcell, W. & Neubauer, T. Digital Twins in Agriculture: A State-of-\nthe-art review. Smart Agricultural Technology. 3 pp. 100094 (2023),\nhttps://www.sciencedirect.com/science/article/pii/\nS2772375522000594, Accessed on 2024-01-11\n[10] Nie, J., Wang, Y., Li, Y. & Chao, X. Artificial intelligence and digital\ntwins in sustainable agriculture and forestry: a survey. Turkish Journal\nOf Agriculture And Forestry. 46, 642-661 (2022), Accessed on 2024-01-\n11\n[11] Attaran, M. & Celik, B. Digital Twin: Benefits, use cases, challenges,\nand opportunities. Decision Analytics Journal. 6 pp. 100165 (2023),\nhttps://www.sciencedirect.com/science/article/pii/\nS277266222300005X, Accessed on 2024-01-11\n[12] Nasirahmadi, A. & Hensel, O. Toward the Next Generation of Digi-\ntalization in Agriculture Based on Digital Twin Paradigm. Sensors. 22\n(2022), https://www.mdpi.com/1424-8220/22/2/498, Accessed\non 2024-01-11\n[13] Peladarinos, N., Piromalis, D., Cheimaras, V., Tserepas, E., Munteanu,\nR. & Papageorgas, P. Enhancing Smart Agriculture by Implementing\nDigital Twins: A Comprehensive Review. Sensors. 23 (2023), https:\n//www.mdpi.com/1424-8220/23/16/7128, Accessed on 2024-01-\n11\n[14] Khebbache, R., Merizig, A., Rezeg, K. & Lloret, J. The recent techno-\nlogical trends of smart irrigation systems in smart farming: a review.\nInternational Journal Of Computing And Digital Systems. 14, 10317-\n10335 (2023)\n[15] Holzinger, A., Saranti, A., Angerschmid, A., Retzlaff, C., Gronauer, A.,\nPejakovic, V., Medel-Jimenez, F., Krexner, T., Gollob, C. & Stampfer,\nK. Digital Transformation in Smart Farm and Forest Operations Needs\nHuman-Centered AI: Challenges and Future Directions. Sensors. 22\n(2022), https://www.mdpi.com/1424-8220/22/8/3043, Accessed\non 2024-01-11\n[16] Gautron, R., Maillard, O., Preux, P., Corbeels, M. & Sabbadin,\nR. Reinforcement learning for crop management support:\nReview,\nprospects and challenges. Computers And Electronics In Agricul-\nture. 200 pp. 107182 (2022), https://www.sciencedirect.com/\nscience/article/pii/S0168169922004999, Accessed on 2024-\n01-11\n[17] Benos, L., Tagarakis, A., Dolias, G., Berruto, R., Kateris, D. & Bochtis,\nD. Machine Learning in Agriculture: A Comprehensive Updated Re-\nview. Sensors. 21 (2021), https://www.mdpi.com/1424-8220/21/\n11/3758, Accessed on 2024-01-11\n[18] Sharma, A., Jain, A., Gupta, P. & Chowdary, V. Machine Learning Ap-\nplications for Precision Agriculture: A Comprehensive Review. IEEE\nAccess. 9 pp. 4843-4873 (2021)\n[19] Abioye, E., Hensel, O., Esau, T., Elijah, O., Abidin, M., Ayobami, A.,\nYerima, O. & Nasirahmadi, A. Precision Irrigation Management Us-\ning Machine Learning and Digital Farming Solutions. AgriEngineering.\n4, 70-103 (2022), https://www.mdpi.com/2624-7402/4/1/6, Ac-\ncessed on 2024-01-11\n[20] Foundation, F. Gymnasium An API standard for reinforcement learn-\ning with a diverse collection of reference environments. (https://\ngymnasium.farama.org/,2023), retrieved on 09.01.2024\n[21] Zhang, Z., Boubin, J., Stewart, C. & Khanal, S. Whole-Field Rein-\nforcement Learning: A Fully Autonomous Aerial Scouting Method for\nPrecision Agriculture. Sensors. 20 (2020), https://www.mdpi.com/\n1424-8220/20/22/6585\n[22] Devarajan, G., Nagarajan, S., T.V., R., T., V., Ghosh, U. & Alnumay, W.\nDDNSAS: Deep reinforcement learning based deep Q-learning network\nfor smart agriculture system. Sustainable Computing: Informatics And\nSystems. 39 pp. 100890 (2023), https://www.sciencedirect.com/\nscience/article/pii/S2210537923000458\n[23] Castro, G., Berger, G., Cantieri, A., Teixeira, M., Lima, J., Pereira, A. &\nPinto, M. Adaptive Path Planning for Fusing Rapidly Exploring Random\nTrees and Deep Reinforcement Learning in an Agriculture Dynamic En-\nvironment UAVs. Agriculture. 13 (2023), https://www.mdpi.com/\n2077-0472/13/2/354\n[24] Pamuklu, T., Nguyen, A., Syed, A., Kennedy, W. & Erol-Kantarci,\nM. IoT-Aerial Base Station Task Offloading With Risk-Sensitive Rein-\nforcement Learning for Smart Agriculture. IEEE Transactions On Green\nCommunications And Networking. 7, 171-182 (2023)\n[25] Hao, Z., Li, X., Meng, C., Yang, W. & Li, M. Adaptive spraying decision\nsystem for plant protection unmanned aerial vehicle based on reinforce-\nment learning. International Journal Of Agricultural And Biological En-\ngineering. 15, 16-26 (2022)\n[26] Testi, E., Favarelli, E. & Giorgetti, A. Reinforcement Learning for Con-\nnected Autonomous Vehicle Localization via UAVs. 2020 IEEE Interna-\ntional Workshop On Metrology For Agriculture And Forestry (MetroA-\ngriFor). pp. 13-17 (2020)\n[27] Boubin, J., Burley, C., Han, P., Li, B., Porter, B. & Stewart, C. MAR-\nbLE: Multi-Agent Reinforcement Learning at the Edge for Digital Agri-\nculture. 2022 IEEE/ACM 7th Symposium On Edge Computing (SEC).\npp. 68-81 (2022)\n[28] Pourroostaei Ardakani,\nS. & Cheshmehzangi,\nA. Reinforcement\nLearning-Enabled UAV Itinerary Planning for Remote Sensing Appli-\ncations in Smart Farming. Telecom. 2, 255-270 (2021), https://www.\nmdpi.com/2673-4001/2/3/17\n[29] Lin, G., Zhu, L., Li, J., Zou, X. & Tang, Y. Collision-free path\nplanning for a guava-harvesting robot based on recurrent deep rein-\nforcement learning. Computers And Electronics In Agriculture. 188\npp. 106350 (2021), https://www.sciencedirect.com/science/\narticle/pii/S0168169921003677\n[30] Alon, Y. & Zhou, H. Multi-Agent Reinforcement Learning for Un-\nmanned Aerial Vehicle Coordination by Multi-Critic Policy Gradient\nOptimization. (2020)\n[31] Roghair, J., Niaraki, A., Ko, K. & Jannesari, A. A Vision Based Deep\nReinforcement Learning Algorithm for UAV Obstacle Avoidance. Intel-\nligent Systems And Applications. pp. 115-128 (2022)\n[32] Nguyen, A., Pamuklu, T., Syed, A., Kennedy, W. & Erol-Kantarci, M.\nDeep Reinforcement Learning for Task Offloading in UAV-Aided Smart\nFarm Networks. 2022 IEEE Future Networks World Forum (FNWF). pp.\n270-275 (2022)\n[33] Kurunathan, H., Li, K., Ni, W., Tovar, E. & Dressler, F. Deep Reinforce-\nment Learning for Persistent Cruise Control in UAV-aided Data Collec-\ntion. 2021 IEEE 46th Conference On Local Computer Networks (LCN).\npp. 347-350 (2021)\n[34] Xu, B., Gao, F., Yu, C., Zhang, R., Wu, Y. & Wang, Y. OmniDrones: An\nEfficient and Flexible Platform for Reinforcement Learning in Drone\nControl. IEEE Robotics And Automation Letters. pp. 1-7 (2024)\n16\n[35] Tian, X., Pan, B., Bai, L., Wang, G. & Mo, D. Fruit Picking Robot Arm\nTraining Solution Based on Reinforcement Learning in Digital Twin.\nJournal Of ICT Standardization. 11, 261-282 (2023)\n[36] Nethala, S. Autonomous Harvesting via Hierarchical Reinforcement\nLearning in Dynamic Environments. (Texas A&M University-Corpus\nChristi,2023)\n[37] Yang, K., Zhang, Z., Cheng, H., Wu, H. & Guo, Z. Domain central-\nization and cross-modal reinforcement learning for vision-based robotic\nmanipulation. International Journal Of Precision Agricultural Aviation.\n3 (2020)\n[38] Zeng, X., Zaenker, T. & Bennewitz, M. Deep Reinforcement Learning\nfor Next-Best-View Planning in Agricultural Applications. 2022 Inter-\nnational Conference On Robotics And Automation (ICRA). pp. 2323-\n2329 (2022)\n[39] Andriyanov, N. Development of Apple Detection System and Reinforce-\nment Learning for Apple Manipulator. Electronics. 12 (2023), https:\n//www.mdpi.com/2079-9292/12/3/727\n[40] Li, Y., Feng, Q., Zhang, Y., Peng, C., Ma, Y., Liu, C., Ru, M., Sun, J. &\nZhao, C. Peduncle collision-free grasping based on deep reinforcement\nlearning for tomato harvesting robot. Computers And Electronics In\nAgriculture. 216 pp. 108488 (2024), https://www.sciencedirect.\ncom/science/article/pii/S0168169923008761\n[41] Petrenko, V., Tebueva, F., Antonov, V. & Gurchinsky, M. A Robotic\nComplex Control Method Based on Deep Reinforcement Learning of\nRecurrent Neural Networks for Automatic Harvesting of Greenhouse\nCrops. Proceedings Of The 8th Scientific Conference On Information\nTechnologies For Intelligent Decision Making Support (ITIDS 2020). pp.\n340-346 (2020), https://doi.org/10.2991/aisr.k.201029.064\n[42] Martini, M., Cerrato, S., Salvetti, F., Angarano, S. & Chiaberge, M.\nPosition-Agnostic Autonomous Navigation in Vineyards with Deep Re-\ninforcement Learning. 2022 IEEE 18th International Conference On Au-\ntomation Science And Engineering (CASE). pp. 477-484 (2022)\n[43] Yandun, F., Parhar, T., Silwal, A., Clifford, D., Yuan, Z., Levine, G.,\nYaroshenko, S. & Kantor, G. Reaching Pruning Locations in a Vine Us-\ning a Deep Reinforcement Learning Policy. 2021 IEEE International\nConference On Robotics And Automation (ICRA). pp. 2400-2406 (2021)\n[44] Lin, G., Huang, P., Wang, M., Xu, Y., Zhang, R. & Zhu, L. An Inverse\nKinematics Solution for a Series-Parallel Hybrid Banana-Harvesting\nRobot Based on Deep Reinforcement Learning. Agronomy. 12 (2022),\nhttps://www.mdpi.com/2073-4395/12/9/2157\n[45] Wang, Y., He, Z., Cao, D., Ma, L., Li, K., Jia, L. & Cui, Y. Cov-\nerage path planning for kiwifruit picking robots based on deep re-\ninforcement learning. Computers And Electronics In Agriculture. 205\npp. 107593 (2023), https://www.sciencedirect.com/science/\narticle/pii/S0168169922009012\n[46] Yang, J., Ni, J., Li, Y., Wen, J. & Chen, D. The Intelligent Path Planning\nSystem of Agricultural Robot via Reinforcement Learning. Sensors. 22\n(2022), https://www.mdpi.com/1424-8220/22/12/4316\n[47] Faryadi, S. & Mohammadpour Velni, J. A reinforcement learning-based\napproach for modeling and coverage of an unknown field using a team of\nautonomous ground vehicles. International Journal Of Intelligent Sys-\ntems. 36, 1069-1084 (2021), https://onlinelibrary.wiley.com/\ndoi/abs/10.1002/int.22331\n[48] Josef, S. & Degani, A. Deep Reinforcement Learning for Safe Lo-\ncal Planning of a Ground Vehicle in Unknown Rough Terrain. IEEE\nRobotics And Automation Letters. 5, 6748-6755 (2020)\n[49] Wiberg, V., Wallin, E., Nordfjell, T. & Servin, M. Control of Rough\nTerrain Vehicles Using Deep Reinforcement Learning. IEEE Robotics\nAnd Automation Letters. 7, 390-397 (2022)\n[50] Din, A., Ismail, M., Shah, B., Babar, M., Ali, F. & Baig, S. A\ndeep reinforcement learning-based multi-agent area coverage control\nfor smart agriculture. Computers And Electrical Engineering. 101\npp. 108089 (2022), https://www.sciencedirect.com/science/\narticle/pii/S0045790622003445\n[51] Elavarasan, D. & Vincent, P. Crop Yield Prediction Using Deep Rein-\nforcement Learning Model for Sustainable Agrarian Applications. IEEE\nAccess. 8 pp. 86886-86901 (2020)\n[52] Elavarasan, D. & Vincent, P. A reinforced random forest model for en-\nhanced crop yield prediction by integrating agrarian parameters. Jour-\nnal Of Ambient Intelligence And Humanized Computing. 12 pp. 10009-\n10022 (2021), Accessed on 2024-01-10\n[53] Wu, J., Tao, R., Zhao, P., Martin, N. & Hovakimyan, N. Optimizing\nNitrogen Management With Deep Reinforcement Learning and Crop\nSimulations. Proceedings Of The IEEE/CVF Conference On Computer\nVision And Pattern Recognition (CVPR) Workshops. pp. 1712-1720\n(2022,6)\n[54] Wang, C., Ye, Y., Tian, Y. & Yu, Z. Classification of nutrient deficiency\nin rice based on CNN model with Reinforcement Learning augmenta-\ntion. 2021 International Symposium On Artificial Intelligence And Its\nApplication On Media (ISAIAM). pp. 107-111 (2021)\n[55] Yassine, H., Roufaida, K., Shkodyrev, V., Abdelhak, M., Zarour, L. &\nKhaled, R. Intelligent Farm Based on Deep Reinforcement Learning\nfor optimal control. 2022 International Symposium On INnovative In-\nformatics Of Biskra (ISNIB). pp. 1-6 (2022)\n[56] Zheng, Z., Yan, P., Chen, Y., Cai, J. & Zhu, F. Increasing Crop Yield Us-\ning Agriculture Sensing Data in Smart Plant Factory. Security, Privacy,\nAnd Anonymity In Computation, Communication, And Storage. pp. 345-\n356 (2021)\n[57] Sharma, G., Singh, A. & Jain, S. DeepEvap:\nDeep reinforcement\nlearning based ensemble approach for estimating reference evap-\notranspiration. Applied Soft Computing. 125 pp. 109113 (2022),\nhttps://www.sciencedirect.com/science/article/pii/\nS156849462200388X\n[58] Bouni, M., Hssina, B., Douzi, K. & Douzi, S. Towards an Ef-\nficient Recommender Systems in Smart Agriculture:\nA deep re-\ninforcement learning approach. Procedia Computer Science. 203\npp. 825-830 (2022), https://www.sciencedirect.com/science/\narticle/pii/S1877050922007293, 17th International Conference\non Future Networks and Communications / 19th International Con-\nference on Mobile Systems and Pervasive Computing / 12th Inter-\nnational Conference on Sustainable Energy Information Technology\n(FNC/MobiSPC/SEIT 2022), August 9-11, 2022, Niagara Falls, Ontario,\nCanada\n[59] Ashcraft, C. & Karra, K. Machine Learning aided Crop Yield Opti-\nmization. CoRR. abs/2111.00963 (2021), https://arxiv.org/abs/\n2111.00963\n[60] Madondo, M., Azmat, M., Dipietro, K., Horesh, R., Jacobs, M., Bawa,\nA., Srinivasan, R. & O’Donncha, F. A SWAT-based Reinforcement\nLearning Framework for Crop Management. (2023)\n[61] Overweg, H., Berghuijs, H. & Athanasiadis, I. CropGym:\na Re-\ninforcement Learning Environment for Crop Management. CoRR.\nabs/2104.04326 (2021), https://arxiv.org/abs/2104.04326\n[62] Kallenberg, M., Overweg, H., Bree, R. & Athanasiadis, I. Nitrogen man-\nagement with reinforcement learning and crop growth models. Environ-\nmental Data Science. 2 pp. e34 (2023)\n[63] Turchetta, M., Corinzia, L., Sussex, S., Burton, A., Herrera, J.,\nAthanasiadis, I., Buhmann, J. & Krause, A. Learning Long-Term\nCrop Management Strategies with CyclesGym. Advances In Neu-\nral Information Processing Systems. 35 pp. 11396-11409 (2022),\nhttps://proceedings.neurips.cc/paper_files/paper/2022/\nfile/4a22ceafe2dd6e0d32df1f7c0a69ab68-Paper-Datasets_\nand_Benchmarks.pdf\n[64] Tropea, M., Campoverde, L. & De Rango, F. Exploiting Ai in Iot Smart\nIrrigation Management System:\nReinforcement Learning vs Fuzzy\nLogic Models. Available At SSRN 4149708. (2022)\n[65] Kelly, T., Foster, T. & Schultz, D. Assessing the value of deep reinforce-\nment learning for irrigation scheduling. Smart Agricultural Technology.\n7 pp. 100403 (2024), https://www.sciencedirect.com/science/\narticle/pii/S277237552400008X\n[66] Ding, X. & Du, W. DRLIC: Deep Reinforcement Learning for Irrigation\nControl. 2022 21st ACM/IEEE International Conference On Information\nProcessing In Sensor Networks (IPSN). pp. 41-53 (2022)\n[67] Alibabaei, K., Gaspar, P., Assunc¸˜ao, E., Alirezazadeh, S. & Lima,\nT. Irrigation optimization with a deep reinforcement learning model:\nCase study on a site in Portugal. Agricultural Water Management. 263\npp. 107480 (2022), https://www.sciencedirect.com/science/\narticle/pii/S0378377422000270\n[68] Alibabaei, K., Gaspar, P., Assunc¸˜ao, E., Alirezazadeh, S., Lima, T.,\nSoares, V. & Caldeira, J. Comparison of On-Policy Deep Reinforce-\nment Learning A2C with Off-Policy DQN in Irrigation Optimization:\nA Case Study at a Site in Portugal. Computers. 11 (2022), https:\n//www.mdpi.com/2073-431X/11/7/104\n17\n[69] Chen, M., Cui, Y., Wang, X., Xie, H., Liu, F., Luo, T., Zheng, S. &\nLuo, Y. A reinforcement learning approach to irrigation decision-making\nfor rice using weather forecasts. Agricultural Water Management. 250\npp. 106838 (2021), https://www.sciencedirect.com/science/\narticle/pii/S0378377421001037\n[70] Tao, R. Optimizing crop management with reinforcement learning, imi-\ntation learning, and crop simulations. (University of Illinois at Urbana-\nChampaign,2022,11)\n[71] Zhou, N. Intelligent Control of Agricultural Irrigation Based on\nReinforcement\nLearning.\nJournal\nOf\nPhysics:\nConference\nSe-\nries.\n1601,\n052031\n(2020,8),\nhttps://dx.doi.org/10.1088/\n1742-6596/1601/5/052031\n[72] Hung,\nF.\n&\nYang,\nY.\nAssessing\nAdaptive\nIrrigation\nImpacts\non\nWater\nScarcity\nin\nNonstationary\nEnvironments—A\nMulti-\nAgent\nReinforcement\nLearning\nApproach.\nWater\nResources\nResearch.\n57,\ne2020WR029262\n(2021),\nhttps://agupubs.\nonlinelibrary.wiley.com/doi/abs/10.1029/2020WR029262,\ne2020WR0292622020WR029262\n[73] Gautron, R., Padr´on, E., Preux, P., Bigot, J., Maillard, O. & Emukpere,\nD. gym-DSSAT: a crop model turned into a Reinforcement Learning\nenvironment. (2022)\n[74] Wang, L., He, X. & Luo, D. Deep Reinforcement Learning for Green-\nhouse Climate Control. 2020 IEEE International Conference On Knowl-\nedge Graph (ICKG). pp. 474-480 (2020)\n[75] Zhang, W., Cao, X., Yao, Y., An, Z., Xiao, X. & Luo, D. Ro-\nbust Model-based Reinforcement Learning for Autonomous Greenhouse\nControl. Proceedings Of The 13th Asian Conference On Machine Learn-\ning. 157 pp. 1208-1223 (2021,11,17), https://proceedings.mlr.\npress/v157/zhang21e.html\n[76] Li, W., Wang, X., Jin, B., Luo, D. & Zha, H. Structured Cooperative\nReinforcement Learning With Time-Varying Composite Action Space.\nIEEE Transactions On Pattern Analysis And Machine Intelligence. 44,\n8618-8634 (2022)\n[77] Ajagekar,\nA.,\nDecardi-Nelson,\nB.\n&\nYou,\nF.\nEnergy\nman-\nagement\nfor\ndemand\nresponse\nin\nnetworked\ngreenhouses\nwith\nmulti-agent\ndeep\nreinforcement\nlearning.\nApplied\nEnergy.\n355\npp. 122349 (2024), https://www.sciencedirect.com/science/\narticle/pii/S0306261923017130\n[78] Ajagekar, A., Mattson, N. & You, F. Energy-efficient AI-based Con-\ntrol of Semi-closed Greenhouses Leveraging Robust Optimization\nin Deep Reinforcement Learning. Advances In Applied Energy. 9\npp. 100119 (2023), https://www.sciencedirect.com/science/\narticle/pii/S2666792422000373\n[79] Chen, L., Xu, L. & Wei, R. Energy-Saving Control Algorithm of\nVenlo Greenhouse Skylight and Wet Curtain Fan Based on Reinforce-\nment Learning with Soft Action Mask. Agriculture. 13 (2023), https:\n//www.mdpi.com/2077-0472/13/1/141\n[80] Decardi-Nelson, B. & You, F. Improving Resource Use Efficiency in\nPlant Factories Using Deep Reinforcement Learning for Sustainable\nFood Production. Chemical Engineering Transactions. 103 pp. 79-84\n(2023), https://www.cetjournal.it/index.php/cet/article/\nview/CET23103014\n[81] Doan, Y., Ho, M., Nguyen, H. & Han, H. Optimization of Spirulina sp.\ncultivation using reinforcement learning with state prediction based on\nLSTM neural network. Journal Of Applied Phycology. 33 pp. 2733-2744\n(2021), Accessed on 2024-01-10\n[82] Uyeh, D., Bassey, B., Mallipeddi, R., Asem-Hiablie, S., Amaizu, M.,\nWoo, S., Ha, Y. & Park, T. A Reinforcement Learning Approach for\nOptimal Placement of Sensors in Protected Cultivation Systems. IEEE\nAccess. 9 pp. 100781-100800 (2021)\n[83] Lu, Y., Gong, M., Li, J. & Ma, J. Optimizing Controlled Environmen-\ntal Agriculture for Strawberry Cultivation Using RL-Informer Model.\nAgronomy. 13 (2023), https://www.mdpi.com/2073-4395/13/8/\n2057\n[84] Hribar, J., Marinescu, A., Chiumento, A. & Dasilva, L. Energy-Aware\nDeep Reinforcement Learning Scheduling for Sensors Correlated in\nTime and Space. IEEE Internet Of Things Journal. 9, 6732-6744 (2022)\n[85] Nguyen, A., Pamuklu, T., Syed, A., Kennedy, W. & Erol-Kantarci, M.\nReinforcement Learning-Based Deadline and Battery-Aware Offloading\nin Smart Farm IoT-UAV Networks. ICC 2022 - IEEE International Con-\nference On Communications. pp. 189-194 (2022)\n[86] Ali, M., Alsaeedi, A., Shah, S., Yafooz, W. & Malik, A. Energy Ef-\nficient Data Dissemination for Large-Scale Smart Farming Using Rein-\nforcement Learning. Electronics. 12 (2023), https://www.mdpi.com/\n2079-9292/12/5/1248\n[87] Naresh, M., Das, V., Saxena, P. & Gupta, M. Deep reinforcement learn-\ning based QoE-aware actor-learner architectures for video streaming in\nIoT environments. Computing. 104 pp. 1527-1550 (2022), Accessed on\n2024-01-10\n[88] Ahmadi, M., Taghavirashidizadeh, A., Javaheri, D., Masoumian, A.,\nJafarzadeh Ghoushchi, S. & Pourasad, Y. DQRE-SCnet: A novel hy-\nbrid approach for selecting users in Federated Learning with Deep-\nQ-Reinforcement Learning based on Spectral Clustering. Journal Of\nKing Saud University - Computer And Information Sciences. 34,\n7445-7458 (2022), https://www.sciencedirect.com/science/\narticle/pii/S1319157821002226\n[89] Chen, H., Chen, Z., Lin, F. & Zhuang, P. Effective Management for\nBlockchain-Based Agri-Food Supply Chains Using Deep Reinforce-\nment Learning. IEEE Access. 9 pp. 36008-36018 (2021)\n[90] Herabad, M. & Afshar, N. Fuzzy-based Deep Reinforcement Learning\nfor Frost Forecasting in IoT Edge-enabled Agriculture. 2022 8th Iranian\nConference On Signal Processing And Intelligent Systems (ICSPIS). pp.\n1-5 (2022)\n[91] Moeinizade, S., Hu, G. & Wang, L. A reinforcement Learning approach\nto resource allocation in genomic selection. Intelligent Systems With\nApplications. 14 pp. 200076 (2022), https://www.sciencedirect.\ncom/science/article/pii/S2667305322000175\n18\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2024-06-13",
  "updated": "2024-06-13"
}