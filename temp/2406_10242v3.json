{
  "id": "http://arxiv.org/abs/2406.10242v3",
  "title": "Physics-Guided Actor-Critic Reinforcement Learning for Swimming in Turbulence",
  "authors": [
    "Christopher Koh",
    "Laurent Pagnier",
    "Michael Chertkov"
  ],
  "abstract": "Turbulent diffusion causes particles placed in proximity to separate. We\ninvestigate the required swimming efforts to maintain an active particle close\nto its passively advected counterpart. We explore optimally balancing these\nefforts by developing a novel physics-informed reinforcement learning strategy\nand comparing it with prescribed control and physics-agnostic reinforcement\nlearning strategies. Our scheme, coined the actor-physicist, is an adaptation\nof the actor-critic algorithm in which the neural network parameterized critic\nis replaced with an analytically derived physical heuristic function, the\nphysicist. We validate the proposed physics-informed reinforcement learning\napproach through extensive numerical experiments in both synthetic BK and more\nrealistic Arnold-Beltrami-Childress flow environments, demonstrating its\nsuperiority in controlling particle dynamics when compared to standard\nreinforcement learning methods.",
  "text": "Physics-Guided Actor-Critic Reinforcement Learning for Swimming in Turbulence\nChristopher Koh, Laurent Pagnier,∗and Michael (Misha) Chertkov†\nApplied Mathematics & Mathematics, University of Arizona, Tucson, AZ 85721.\n(Dated: November 12, 2024)\nTurbulent diffusion causes particles placed in proximity to separate. We investigate the required\nswimming efforts to maintain an active particle close to its passively advected counterpart. We ex-\nplore optimally balancing these efforts by developing a novel physics-informed reinforcement learn-\ning strategy and comparing it with prescribed control and physics-agnostic reinforcement learning\nstrategies. Our scheme, coined the actor-physicist, is an adaptation of the actor-critic algorithm\nin which the neural network parameterized critic is replaced with an analytically derived physical\nheuristic function, the physicist. We validate the proposed physics-informed reinforcement learning\napproach through extensive numerical experiments in both synthetic BK and more realistic Arnold-\nBeltrami-Childress flow environments, demonstrating its superiority in controlling particle dynamics\nwhen compared to standard reinforcement learning methods.\nI.\nINTRODUCTION\nThis article’s key technical advancement is the devel-\nopment of a Physics-Informed Reinforcement Learning\n(PIRL) approach where a physicist replaces the critic in\nthe standard Actor-Critic (AC) algorithm, see Fig. 1. We\ncoin it Actor-Physicist (AP) algorithm.\nThe physicist\ncomponent leverages physical insights on the statistics\nand control of swimming in chaotic flows, as elaborated\nin [1] and further developed in this manuscript.\nThis\napproach, derived from our understanding of Lagrangian\nseparation in turbulent flows, guides the control policies.\nFIG. 1. Actor-Physicist (AP) diagram. The main idea of the\npresent works is to substitute an expression obtained from the\nsystem’s dynamics in place of the standard neural network.\nWe consider a particle in a turbulent flow that swims\ntowards its passive target to maintain proximity.\nThe\nparticle will be controlled according to Reinforcement\nLearning (RL) [2] – a methodology for solving complex\ndecision-making problems. RL involves an agent learn-\ning through interaction with its environment, balancing\nexploration and exploitation. Exploration involves try-\ning new actions to gain information about the turbulent\n∗laurentpagnier@arizona.edu\n† chertkov@arizona.edu\nenvironment while exploitation uses accumulated knowl-\nedge to make optimal decisions. Further details on the\nRL framework are provided in Section II, which intro-\nduces this topic using physics-friendly terminology. This\nRL decision-making is linked to Stochastic Optimal Con-\ntrol (SOC), where the agent maximizes expected reward\nunder environmental uncertainty. Our reward function in\nthe SOC for particles consists of two competing terms:\none maintaining distance between the agent (i.e., the ac-\ntive particle) and its target (passive particle advected\nby the flow) and the other term penalizing the effort re-\nquired.\nIt is important to emphasize that we arrive at the RL\nsolution in a number of steps, which included analytical\nanalysis reported earlier in [1], and which we briefly re-\nview in Section III, and also extended in this manuscript.\nThe main handicap of the analytic analysis is that it\ndeals with an idealized setting where only a prescribed\n– and thus only conservative (suboptimal) control ap-\nplies.\nTherefore, RL becomes the universal and data-\ndriven tool which allows us to overcome the handicap of\nthe theory and help to develop a much more general, ro-\nbust, data-driven and assymptotically optimal approach\nto controlling the separation between active and passive\nparticles. Additional nuances of this manuscript RL ap-\nproach are revealed in the flowchart of relations between\ndifferent methods applied to the problem of the swim-\nming control in realistic flows illustrated in Fig. (2) which\nwe briefly outline next.\nSwim Control in Realistic Flows: Our primary objec-\ntive is to maintain proximity between passive and active\nparticles by maximizing a time-integrated and averaged\nreward function. We develop and evaluate several control\npolicies for the active particle, tailored to the available\ninformation about flow statistics (leading Lyapunov ex-\nponent):\n• Prescribed Control: In this case, used as a bench-\nmark for the RL schemes described in the following,\nthe control force exerted by the active particle is\nproportional to the separation with the coefficient\nof the proportionality prescribed/fixed.\narXiv:2406.10242v3  [eess.SY]  10 Nov 2024\n2\n• Actor-Critic Agents: This is a standard RL archi-\ntecture. Here, both the policy (actor) and a guiding\nprinciple based on the so-called value function (cri-\ntique) are built based on Neural Networks (NN).\nIn this work, we use two AC methods for compari-\nson with our AP method: Advantage Actor-Critic\n(A2C) [3] and Proximal Policy Optimization (PPO)\n[4].\n• Actor-Physicist Agent: This method modifies the\nAC framework, in this work the actor side of our\nAP agent is based on the A2C architecture [3].\nWhile the physicist side – replacing critic – uses an\nanalytically derived prescribed control estimate of\nthe value function instead of its NN-version. This\nanalytic estimation of the value function – based\non the Stochastic Optimal Control (SOC) formula-\ntion applied to an idelialized, so-called Batchelor-\nKraichnan (BK) modeling of the stochastic flow,\nbut applied to a realistic flow – is described in Sec-\ntion IV.\nFig. 2 summarizes the relations between these methods\nand the environment and the theory. On the left of the\nchart, we see two fundamental notions of the Reinforce-\nment Learning – the environment and the agent. The\nupper block illustrates that the theory provides a base\nfor the models and approaches discussed. In the lower\nblock of the chart, we list all the models we discuss and\nuse in the manuscript.\nStarting to the right, one has\nthe most interpretable control which rely on strong as-\nsumptions that may not be satisfied by the environment.\nMoving to the left, control becomes less interpretable.\nArrows describe relations between the models and how\nthey acquire information about the environment (physics\nof the stochastic flows).\nEnvironment\nActor-Critic\nRL\nSurrogate Model\n(Batchelor)\nPolicy NN\nPolicy NN &\nValue NN\nExperience\n(Interactions)\nAnalytic Value\nApproximate\nTheory\nPenalty\nModel\nPrescribed\nControl\n(White Box)\nNaive Actor-\nCritic\n(Black Box)\nActor-\nPhysicist\n(Grey Box)\nDomain Knowledge\n(Application Speciﬁc)\nMin. of Penalty\nApproximate\nFIG. 2. Flowchart explaining the relations between RL, envi-\nronment, theory, and the components of the different control\nschemes.\nRelevant Approaches by Others\nOptimally navigating an agent through a fluid is a chal-\nlenging and active research topic, with recent reinforce-\nment learning approaches for controlling particles and re-\nlated systems summarized in [5] and [6]. Single-agent RL\nmethods were developed for controlling particle (or solid\nbody) navigation specific for efficient gliding in natural\nstratified air-flow environments [7, 8] and for autonomous\nnavigation along an optimal point-to-point path in vari-\nous complex fluid flows [9–11]. RL with multiple, or at\nleast two, conflicting objectives was also extended to a\ncompetitive game played by two active particles in a tur-\nbulent environment, with one attempting to catch and\nthe other aiming to escape [12]. In a setting closely re-\nlated to the one discussed in this manuscript, the authors\nof [13] devised a multi-objective (pareto-seeking) RL ap-\nproach aimed at controlling separation and minimizing\nefforts between two active particles by switching in time\nbetween a number of prescribed strategies, with observa-\ntions of the separation and velocity difference available\nat all times. Notably, [9] utilized the Actor-Critic (AC)\nversion of RL, which is central to this manuscript, al-\nthough their critic is modeled via a neural network in a\nphysics-agnostic (of flow) manner. The RL study in [13]\nwas complemented by a parallel study by the same au-\nthors [14], which juxtaposed a stochastic optimal control\napproach to the problem of an active particle following a\npassive particle in a turbulent flow with a heuristic strat-\negy that relies on the particle’s local observation of the\nenvironment.\nMost of the RL papers mentioned above, with the ex-\nceptions of [12, 13, 15], utilize RL in a domain-agnostic,\nphysics-uninformed manner, relying heavily on feature\nengineering of the underlying neural networks to achieve\nsatisfactory performance. In contrast, in this manuscript\nwe utilize domain knowledge through value approxima-\ntion and do not require enhancements in the model-\ning of the agents themselves. Additionally, our physics-\ninformed RL approach helps address the difficulties tra-\nditional AC methods face in learning values across multi-\nple orders of magnitude [16] and serves as an alternative\nto jump-start or imitation learning [17] when learning\nfrom a warm start. This makes our principal physics-\nguided/informed approach relevant to many other navi-\ngation/control challenges beyond the context of RL for\nswimming where physical modeling of environment, even\nif approximate, is available.\nII.\nREINFORCEMENT LEARNING\nThe main part of this section provides a brief technical\nintroduction aimed at readers with a physics background\nbut without prior experience in Reinforcement Learning\n(RL) and its Actor-Critic (AC) framework (see, e.g., [2]).\nReaders already familiar with RL may wish to skip this\npart and proceed directly to Subsection II A, where we\nintroduce our custom and physics-informed modification\nof the AC consisted in replacement of the ”standard”\ncritic by a physicist-critic – a critic guided by a physical\nmodel of the environment.\nRL is a data-driven version of Stochastic Optimal Con-\ntrol (SOC) which in its continuous time formulation with\n3\nfinite time horizon (episode) can be stated as follows:\nmax\nπ\nZ T\n0\ndte−νt Ea∼π\nh\nr(s(t), a(t))\ni\n,\n(1)\nwhere the s(t) and a(t) are state observed and action\ntaken according to the policy π at the moment of time t,\nν is the discount factor that tells the agent whether to pri-\noritize short-term or long-term rewards, and r(s(t), a(t))\nis the reward encouraging or penalizing actions of the\nagents.\nIn RL, which should be considered as a data driven\nversion of SOC, an agent interacts with an environment,\nwhich evolves in discrete time steps. Let us now explain\nSOC (1) on its discrete time RL version. Both in SOC\nand RL the environment is uncertain, and the next state\ns′ is sampled based on the current state s and the agent’s\naction a: s′ ∼p(·|s, a), where the probability distribution\nis unknown (or at least not fully known). Then Eq. (1)\nturns to\nGπ(T) = ∆Es0→N\na0→N\nh\nN\nX\nn=0\nγnr(sn, an)\ni\n,\n(2)\nwhere π(an|sn) is the policy which defines the prob-\nability for the agent to select an action an = a(tn)\nconditioned to the current state sn = s(tn).\nHere,\ntn = n∆= nT/N, n = 0, · · · , N →∞, and 0 < γ ≤1\nis the discrete time version of the discount factor ν in\nEq. (1). In Eq. (2), sn→N and an→N are shorthand no-\ntations for (sk+1 ∼p(·|sk, ak)|k = n, · · · , N −1) and\n(ak ∼π(·|sk)|k = n, · · · , N −1), respectively.\nIt is also custom in the field to define the state-action\nand state value functions:\nQπ(sn, an) = ∆Esn→N\nan→N\nh\nN\nX\nk=n\nγk−nr(sk, ak)\n\f\f sn, an\ni\n, (3)\nVπ(sn) = Ean∼π(·|sn)\nh\nQπ(sn, an)\ni\n,\n(4)\nwhich describe the expected reward accumulated from\nthe current time tn = nT/N until the end of the episode,\nunder the given policy and conditioned on the current\nstate and action (sn and an in Eq. (3)) and the current\nstate (sn in Eq. (4)).\nA classic RL approach derives recursive equations for\nthe value functions under a given policy, followed by pol-\nicy optimization in a greedy, dynamic programming man-\nner. In this manuscript, we adopt a policy gradient-based\nmethod that focuses on maximizing the value function\nover the policy. In the age of AI, the policy function is\nparameterized via a Neural Network (NN) as πθ(·), where\nθ represents the parameters of the NN. The objective is\nto find the maximum of Vπθ(s) by evaluating its gradient\nwith respect to θ and iteratively updating θ to make the\ngradient zero in the limit:\nEa∼πθ(∼|s) [Qπ(s, a)∇θ log πθ(a|s)] = 0.\n(5)\nHowever, this method tends to have high variance, re-\nsulting in slow convergence and unreliable estimates. To\naddress this issue, it was suggested to replace the state-\naction value function in Eq. (5) with the so-called advan-\ntage function [2]:\nAπ(s, a) = Qπ(s, a) −b(s),\nwhere the baseline b(s) depends only on the state and\nnot on the action. This ensures that:\nEa∼πθ(∼|s) [Aπ(s, a)∇θ log πθ(a|s)] = 0,\n(6)\nassuming\nEq.\n(5)\nis\nvalid,\ndue\nto\nthe\nfact\nthat\nEa∼πθ(∼|s) [∇θ log πθ(a|s)] = 0 for any s. Although b(s)\ncan be any function of s, a common choice is b(s) =\nVπ(s).\nThis modification – from Q to A with b = V – results\nin the widely-used actor-critic methods, where in addi-\ntion to the policy function (the actor), the state-value\nfunction V (·) (the critic) is also approximated by a NN.\nThis actor-critic modification of the vanilla policy gra-\ndient (where only an actor is present, without a critic)\nraises the question: how should we interpret this base-\nline? Formally, it is a degree of freedom used to reduce\nthe variance in the value function’s gradient. Informally,\nthe baseline serves as a benchmark for comparison.\nThough the use of Vπ as a baseline is powerful, it has\nits limitations. The main drawback is that the training\nobjective changes significantly with each NN update, re-\nsulting in continued variations in the gradient estimates.\nWhile there are partial solutions, such as delaying up-\ndates to the baseline [18], we propose a physics-informed\nremedy.\nA.\nActor-Critic Reinforcement Learning with\nPhysics Informed Critic\nIn this manuscript, we elevate the concept of the base-\nline/critic/benchmark by using a physically derived esti-\nmate for the state-dependent value function, rather than\nrelying on a NN as in standard physics-agnostic actor-\ncritic RL approaches like [18]. In the following sections,\nwe demonstrate that under certain simplifying assump-\ntions about system dynamics and control, an explicit an-\nalytical expression for the baseline, as a function of the\nstate, can be derived for a pair of particles placed in a\nlarge-scale turbulent flow.\nTable I presents the mapping of general RL notations\nto the specific problem of controlling the relative sepa-\nration between two particles. In this context, the agent\nis represented by the active particle, whose state is the\nseparation distance from the passive particle. The action\ncorresponds to the swimming efforts of the active parti-\ncle, while the transition probability models the stochastic\nevolution influenced by relative velocity and Brownian\nforces.\n4\nGeneral RL\nPIRL for Particles\nAgent\nActive particle\nState, s\nSeparation between active and\npassive particles, s\nAction, a ∼π(·|s)\nSwimming efforts of the active swim-\nmer\nTransition proba-\nStochastic evolution according to rel-\nility p(s′ | s, a)\native turbulent velocity between swim-\nmers and Brownian force, as discussed\nin Section III A.\nBaseline, b(s)\nState value function, Vϕ(s), estimated\nfollowing assumptions of the BK the-\nory, as discussed in Section III D.\nTABLE I. Mapping of general RL notations to the specific\nproblem of controlling the relative separation of two particles.\nWe also present below a joint pseudo-algorithm for the\nActor-Critic (AC) and Actor Physicist (AP) where the\ndifference between the two is highlighted:\nRequire: Number of episodes N. Each episode is split\ninto T intervals. Learning rates α, β > 0. Discount\nfactor γ ∈[0, 1].\ninitialize NN for the policy function πθ (θ is the NN’s\nvector of parameters)\nAC: initialize NN for the value function Vw\nAP: select ϕ for the value function Vϕ\nInitialize s0\nfor episode = 1 to N do\nfor t = 1 to T do\nat ∼πθ(·|st)\nTake action at, observe st+1, rt\nAC: At ←rt + γVw(st+1) −Vw(st)\nAP: At ←rt + γVϕ(st+1) −Vϕ(st)\nθ ←θ + α\nt\nPt\ni=0 ∇θ log(πθ(at|st))At\nend for\nend for\nIII.\nPRELIMINARIES: PRESCRIBED AND\nOPTIMAL CONTROL OF SWIMMING IN\nTURBULENCE\nIn this introductory Section we follow [1] where\nstochastic flows, as well as a steady state stochastic opti-\nmal control, were discussed in an idealized setting. The\ngoal of this section is to set stages for further discussion\nwhich is original to this manuscript going beyond the pre-\nliminary theoretical estimations of [1] and using these to\ndevelop general RL approach for dealing with a realistic\nflow, discussed in the following.\nWe start this section in Section III A by setting up\nthe basic equations for the stochastic dynamics and the\nstatistics of separation between pairs of particles in a gen-\neral large scale chaotic flow conditioned on a prescribed\ncontrol. These expressions will depend on the finite-time\nstatistics of the largest Lyapunov exponent of the chaotic\nflow, which is also discussed here.\nThen in Section III B we will describe how the general\ntheory is extended to the case of the prescribed linear\ncontrol. It will allow us to get expression for the prob-\nability distribution of inter-swimming separation.\nWe\nobserve that even in the case of sufficiently strong pre-\nscribed control the probability distribution shows ex-\ntended algebraic tails. However, the prescribed control is\ntoo conservative, which leads us to the Stochastic Opti-\nmal Control (SOC) formulation in Section III D where we\nbriefly discuss how the SOC optimal value of the linear\ncontrol can be found in the stationary regime of the SK\nflow.\nA.\nTwo Particles in a Chaotic Flow\nWe consider a spatially smooth, large-scale chaotic ve-\nlocity field with a zero mean velocity. This type of flow\nhas been extensively studied in stochastic hydrodynam-\nics, see [19–24], and the relevant review [25]. Particles\nplaced in such flows are assumed to be very small rela-\ntive to the typical length scales of the flow and they thus\ndo not affect or alter the flow itself, making interactions\nbetween particles negligible. The particles separate expo-\nnentially fast. Our task is to navigate an active particle\nto control its separation from a passive target, assuming\nthey were released in almost the same position initially.\nLet sα(t) = (sα;i | i = 1, · · · , d) represent the positions of\nthe two particles, α = 1, 2, in d-dimensional flow (where\nd = 2 or d = 3). The separation vector s = s1 −s2\nevolves according to:\nds\ndt −v(t, s1, s2) = −a(t) + ξ(t)\nτ\n,\n(7)\nwhere τ is the friction coefficient (set to unity for simplic-\nity), a(t) = (ai(t)|i = 1, · · · , d) is the control term that\nrepresents the active particle’s action to fight against the\nflow, and ξ(t) is the difference in Brownian forces acting\non the active and passive particles, modeled as zero-mean\nwhite-Gaussian noise:\n∀i, j :\nE\n\u0002\nξi(t) ξj(t′)\n\u0003\n= κ δij δ(t −t′),\nwhere κ is the diffusion coefficient.\nThe relative velocity vector v(t, s1, s2) := v1(t, s1) −\nv2(t, s2) will be modeled differently throughout the\nmanuscript. Following standard stochastic hydrodynam-\nics assumptions [25] for large-scale flows, we approximate\nv(t, s1, s2) by the leading term in its Taylor expansion in\ns:\nv(t, s1, s2) ≈σ(t) s,\nwhere σ(t) is a possibly time-dependent velocity gradient\nmatrix. For the general case, we consider an auxiliary\nmultiplicative dynamics:\nd\ndt′ W (t′; t) = σ(t′) W (t′; t),\n(8)\n5\nwhere W (t′; t) ∈Rd×d, with W (t; t) = Id×d, repre-\nsenting the time-ordered exponential of σ(t), denoted as\nW (t′; t) = T\n\u0002\nexp\n\u0000 R t′\nt dt′′σ(t′′)\n\u0001\u0003\n.\nAccording to the Oseledets theorem [25–27], at suf-\nficiently large times, the matrix log(W ⊤(t′; t)W (t′; t))\n/ (t′ −t) stabilizes, with eigenvectors tending to d\nfixed orthonormal eigenvectors, fi, and the eigenvalues\nλi(t′; t) = log |W (t′; t)fi| / (t′ −t) stabilizing to their\nmean values. The finite-time statistics of λi is given by:\nP\n\u0000λ1(t′; t), · · · , λd(t′; t) | t′ −t\n\u0001\n(9)\n∝exp\nh\n−(t′ −t) S\n\u0000λ1(t′; t), · · · , λd(t′; t)\n\u0001i\n,\nwhere S(·), called the Cram´er function, is convex, and the\nLyapunov exponents are ordered: λ1 ≥λ2 ≥· · · ≥λd.\nOf particular interest are the finite-time statistics of the\nleading Lyapunov exponent:\nP(λ1(t′; t) | t′ −t) ∝exp\nh\n−(t′ −t) S1\n\u0000λ1(t′; t)\n\u0001i\n. (10)\nThis manuscript focuses primarily on the reduced de-\nscription of the flow associated with the statistics of the\nrelative separation of two particles, allowing us to limit\nour discussion to the finite-time statistics of λ1(t′; t).\nComputing S1(·) – the Cram´er function of λ1(t′; t) – an-\nalytically is possible only for a limited number of spe-\ncial, idealized flows, such as the stochastic Batchelor-\nKraichnan (BK) flow [22, 24, 28], in which case the\nCram´er function is positive, quadratic. Therefore, our\napproach to computing S1(λ1) in large-scale chaotic flows\n– such as discussed in Section V for our running exam-\nple of the ABC flow – will be empirical. We extract it\nfrom simulations by placing two particles close to each\nother initially, tracking their separation over time, and\nthen computing the Cram´er function by building statis-\ntics of the log-separation divided by time, accumulated\nover many trials, and fitting it to the expression on the\nright-hand side of Eq. (10).\nB.\nBatchelor Flow under Proportional Control\nConsider an active particle moving towards its pas-\nsive counterpart governed by a linear-feedback and time-\nindependent force.\nThis controlled motion can be ex-\npressed as\na →ϕ s.\nHere, ϕ is a constant (time-independent) parameter. As-\nsuming that ϕ is sufficiently large, we focus our analysis\non the steady state of the probability distribution func-\ntion representing distance between the two particles. We\ndiscuss this scenario under two contexts: first in the gen-\neral Batchelor case and then in the special short-time\ncorrelated case of the BK model.\nIn the general Batchelor case solution of Eq. (7) for\ns(t) observed at the time t conditioned to s(0) = s0,\nbecomes\ns(t) = e−ϕt W (t)\n\u0012\ns0+\nZ t\n0\ndt′eϕt′W −1(t′) ξ(t′)\n\u0013\n,\n(11)\nwhere the time-ordered exponential,\nW (t),\nsatisfies\nEq. (8). We will study the large time statistics of s(t) =\n|s(t)|, where the initial separation is forgotten, and thus\nthe first term within the brackets on the right hand side\nof Eq. (11) can be dropped. Moreover, one can see, fol-\nlowing the logic of [24] (see also references therein), that\nin the long-time regime, where the inter-particle separa-\ntion, s(t) is significantly larger than the so-called diffusive\nscale, sd .=\np\nκ / |λ1| and λ1(t) .= maxi(λi | i = 1, · · · , d)\nis the largest (finite-time) Lyapunov exponent of the\nBatchelor flow, the fluctuations of s(t) are mainly due\nto the Lyapunov exponents, distributed according to\nEq. (9). In other words, in this asymptotic we can ap-\nproximate the inter-particle distance by\ns(t) ≈exp\nh\u0000λ1(t; 0) −ϕ\n\u0001\nt\ni\nsd .\n(12)\nSubstituting λ1(t; 0), expressed via s(t) according to\nEq. (12), into Eq. (10), and expanding S1(·) in the Taylor\nSeries around ¯λ1, we arrive at the following asymptotic\nexpression for statistics of s(t) = |s(t)|:\nP(s | t) ∝1\ns exp\nh\n−tS1\n\u0000t−1 log\n\u0000s\n\u000e\nsd\n\u0001\n+ ϕ\n\u0001i\n→1\ns\n× exp\nh\n−t\n\u0010\nS1(¯λ1) +\n\u00101\nt log\n\u0000s\n\u000e\nsd\n\u0001\n+ ϕ −¯λ1\n\u00112\nS′′\n1 (¯λ1)\n\u0011i\n.\nTaking the limit t →∞and s ≫sd, leads to\nPst(s) ∝1\ns\n\u0010sd\ns\n\u00112 (ϕ−¯λ1) S′′\n1 (¯λ1)\n,\n(13)\nwhere sd, ¯λ1 and S′′\n1 are dependent of the system’s dy-\nnamics.\nNotice, that the stationary version, given in Eq. (13),\nsettles if ϕ > ¯λ1, and that it is fully consistent with\nEq. (15), derived for the short δ-correlated velocity gra-\ndient, then S′′\n1 (¯λ1) = 1/((d−1)D) and ¯λ1= d(d −1)D/2.\nC.\nBK Model under Proportional Control\nThe Batchelor-Kraichnan (BK) model is a special case\nof the Batchelor model where the velocity gradient is not\nonly large scale but is also assumed to be Gaussian and\ndelta-correlated in time (see [25] for the general discus-\nsion of the model’s historical roots and validity. The BK\nmodel in d-dimensions is described by the following pair-\ncorrelation function of the velocity gradient matrix, σ,\nentering Eq. (7):\nE\n\u0002\nσij(t)σkl(t′)\n\u0003\n= D (d + 1) δ(t −t′)\n×\n\u0012\nδjlδik −δijδkl + δjkδil\nd + 1\n\u0013\n, ∀i, j, k, l = 1, · · · , d ,\n6\nwhere δ(·) and δij are the δ-function and the Kronecker\nsymbol respectively.\nAssuming that ϕ is sufficiently large we derive from\nthe stochastic ODE (7) the following Fokker-Planck (FP)\nequation for the spherically symmetric probability den-\nsity of s: (see [29] for details):\nLP(s | ϕ) = 0,\n(14)\nL = s1−d d\ndssd\n\u0012\nϕ + 1\n2\n\u0010\nD(d −1)s + κ\ns\n\u0011 d\nds\n\u0013\n,\nwhere s = |s|; and κ stands for variance of the thermal\nnoise in Eq. (7). Solution of Eq. (14) is\nP(s | ϕ) = N −1\n\u0012\n1 + (d −1)Dr2\nκ\n\u0013−ϕ/(D(d−1))\n,\nwith\nN ≡\n\u0012\nπκ\n(d −1)D\n\u0013d/2 Γ\n\u0000ϕ/(D(d −1)) −d/2\n\u0001\ndΓ\n\u0000ϕ/(D(d −1))\n\u0001\n,\nwhere N is the normalization coefficient which guarantees\nthat,\nR ∞\n0\nΩsdsP(s | ϕ) = 1, Ωs = (πd/2/Γ(d/2 + 1))sd−1.\nThe solution is valid, i.e. the normalization integral is\nbounded, if ϕ > (d −1) dD/2.\nD.\nOptimal Stationary Control of BK flow\nThe control strategy discussed so far was not optimal,\nbut prescribed. In the case of the BK flow, and as dis-\ncussed in more details in [1], we can also find the optimal\nstationary control — that is one which solves Eq. (1) at\nT →∞in the case of no penalty for the special case of\nthe (negative) reward function\nr (s(t′), a(t′)) := −\n\r\ra(t′)\n\r\r2\n2 −β\n\r\rs(t′)\n\r\r2\n2,\n(15)\nmost suitable for the particle problem – where we balance\nof the quadratic cost of action with the quadratic cost\npenalizing quadratically for increase in the inter-particle\ndistance.\nConsidering the steady state (infinite horizon) and no\ndiscount and utilizing Eq. (15) while also assuming that\nϕ > ϕ(s) = ˜D/2, where ˜D = D(d + 2)(d −1), we arrive\nat\nϕ∗= arg min\nϕ\n\u0014\n(ϕ2 + β)\nZ ∞\n0\ns2 P(s|ϕ) ds\n\u0015\n= arg min\nϕ\nϕ2 + β\n2ϕ −˜D\n=\n˜D+\nq\n4β+ ˜D 2\n2\n.\n(16)\nIV.\nSTATE VALUE FUNCTION IN THE\nBATCHELO-KRAICHNAN FLOW\nIn this section, we use the BK theory to evaluate the\nstate value function. Significance of this result is in its\nfurther utilization in the next section as an estimate for\nthe physicist (replacing critic) bias in the PC RL algo-\nrithm.\nLet’s consider the continuous time version of Eq. (4);\nthe value function which we evaluate over a finite horizon\nfrom t to T, with the discount factor ν, reads\nVϕ\n\u0000t, s(t)\n\u0001\n= (β + ϕ2)\nZ T\nt\ndt′e−ν(t′−t)\n\u0012\ne−2ϕ(t′−t)\n× E\nh\r\rW (t′; t) s\n\r\r2\n2\ni\n+ κ\nZ t′\nt\ndt′′e−2ϕ(t′−t′′)\n× E\nh\nTr\n\u0002\nW ⊤(t′; t′′) W (t′; t′′)\n\u0003i\u0013\n,\n(17)\nwhere W satisfies Eq. (8); the expectations are over the\nstatistics of σ, which is not yet specified, and over the\nwhite-Gaussian thermal forces.\nHere, in Eq. (17), we\nevaluate the state value function conditioned to a value\nof the control coefficient ϕ fixed to a constant, and since\nϕ is the only parameter in PC, we will label V with it.\nIn\nthe\ncase\nof\nthe\nSK\nflow\nwe\nobserve\nthat\nE\n\u0002\nWki(t′; t)Wkj(t′; t)\n\u0003\n= δij × exp\n\u0000−(t′ −t) ˜D\n\u0001\n. This\nresults in the following closed-form expression for Eq.\n(17)\nVϕ\n\u0000t, s(t)\n\u0001\n= (β + ϕ2)\nZ T\nt\ndt′e−ν(t′−t)\n\u0012\ns2e−(2ϕ−˜\nD)(t′−t)\n+ κd\nZ t′\nt\ndt′′e−(2ϕ−˜\nD)(t′−t′′)\n\u0013\n=: B(t) s2 + C(t),\n(18)\nwith\nB(t) = (β + ϕ2)\n\u00001 −e−(T −t)(ν+2ϕ−˜\nD)\u0001\nν + 2ϕ −˜D\n,\nC(t) = dκ(β + ϕ2)\nν(2ϕ −˜D)\n\u00001 −e−ν(T −t)\u0001\n−\ndκ\n(2ϕ −˜D)\nB(t).\nIt is this expression which we will use as a baseline for\nthe ABC flow in Section VI, after estimating respective\nvalue of ˜D in Section V, hence providing physics-based\nguidance to the critic block of the method, as shown in\nFig. 1.\nSome remarks are in order. First, notice that in the\ninfinite horizon, T →∞, and then no discount, ν →∞,\nlimits the surviving, B, part of the final expression in\nEq. (18), is fully consistent with yet not optimized over\nϕ part of the steady expression in Eq. (16). Second, note\nthat even though we emphasize in Eq. (18) the depen-\ndence of A and B only on t, the formula actually pro-\nvides explicit dependence on all other parameters of the\nBK flow and of the optimization formulation.\nThird,\nthe expression in Eq. (18) for Vϕ applies only to cases\nwhere the statistics of separation stabilize over time, re-\nquiring that 2ϕ > ˜D.\nMoreover, we expect a critical\nslowdown effect – when approaching the boundary, i.e.,\nϕ = ˜D/2 + ε and ε →0, the time required to estab-\nlish the steady distribution of the separation diverges.\n7\nThis means that, given practical limitations on collect-\ning statistics, Eq. (18) does not apply in the regime of\nsmall positive ε, potentially leading to statistics of the\nseparation which does not stabilize at T →∞. In this\ncase we choose to be conservative and work with ϕ which\nare larger than ˜D/2 with a reserve.\nV.\nSWIMMING IN\nARNOLD–BELTRAMI–CHILDRESS FLOW\nTo\nvalidate\nour\nRL\napproaches\nwe\nuse\nthe\nArnold–Beltrami–Childress\n(ABC)\nflow\n[30]\nin\nthe\nchaotic regime, where trajectories diverge dramatically,\nlargely in an exponential fashion.\nHowever, in this\ncase, the assumptions of the BK model are not exactly\nsatisfied and the equation (18) do not provide an exact\nmatch and can only serve as a rough approximation.\nThis flow is a three-dimensional incompressible veloc-\nity field which is an exact solution of Euler’s equation.\nIts representation in Cartesian coordinates, augmented\nwith an weak additive noise is\nvα,x(sα) = A sin\n\u0000sα,z\n\u0001\n+ C cos\n\u0000sα,y\n\u0001\n+ ξα,x,\nvα,y(sα) = B sin\n\u0000sα,x\n\u0001\n+ A cos(sα,z) + ξα,y,\nvα,z(sα) = C sin\n\u0000sα,y\n\u0001\n+ B cos\n\u0000sα,x\n\u0001\n+ ξα,z,\nwith\nξα,x = √κ dwx\ndt ,\nξα,y = √κ dwy\ndt ,\nξα,z = √κ dwz\ndt ,\nwhere α = 1, 2 is the particle index (for the active par-\nticle and its passive counterpart, resepctively) and A, B\nand C are three flow parameters and wx, wy, wz are unit\nvariance Wiener processes. With appropriate selection of\nflow parameters the flows are chaotic so that if a pair of\npassive particles are placed in the flow their separation\ngrows exponentially with time in the s ≪2π regime, as\nillustrated in Fig. (3). The small additive noise strength\nκ is introduced to provide a change for divergence even\nif the active particle’s and its target’s initial positions\ncoincide.\nProbability distribution of sampled finite time leading\nLaypunov exponent, λ1(t) ≈log(W ⊤(t; 0)W (t; 0))/t, in\nthe ABC flow for A = 1., B = 0.7, C = 0.43 and\nevaluated at t equal to the duration of the episode is\nshown in Fig. (4). In this case relation between the ˜D\nparameter, measuring the eddy-diffusivity strength of the\nBK flow, and the average leading Lyapunov exponent is\n˜D = 2¯λ1(1 + 2/d). This relation was subsequently uti-\nlized in Eq. (18).\nVI.\nNUMERICAL EXPERIMENTS\nWe first show, in Section VI A, that an agent following\na proportional control do indeed reproduce what the the-\nory predicts. We then test the AP agent and demonstrate\nFIG. 3. Sample trajectories of an active particle (gray) and\nits passive target (red) in an ABC flow. (a) No control: tra-\njectories starting from the same point diverge chaotically. (b)\nThe case of PC control: the active particle closely follows the\ntrajectory of the target particle. While trajectories are not\nidentical, their divergences from the passive target is far less\nthan in the uncontrolled case.\nFIG. 4. Finite time statistics of the leading Lyapunov expo-\nnent λ1(t) ≈log(W ⊤(t; 0)W (t; 0))/t in the ABC flow, evalu-\nated at t equal to the episode’s duration.\nits superiority over the state-of-the-art AC agents in Sec-\ntion VI B, as well as compare it to a prescribed control\nwith a near-optimal swimming rate in Section VI C.\nWhen comparing the RL approaches – the AP agent\nversus the AC agents – we use the reward defined by\nEq. (15) with a weighting factor of β = 0.1. This choice\nwas tuned to the flow conditions, allowing both com-\nponents of the reward – maintaining proximity to the\ntarget (passive particle) and minimizing control efforts\n(energy expenditure) – to be clearly expressed and op-\ntimized simultaneously, resulting in a pronounced and\neffective trade-off.\nA.\nValidation of the Physicist (Baseline)\nFig. 5 compares the theoretical predictions from\nEq. (13) with simulation results. We observe good qual-\nitative agreement between theory and simulations for\nboth the BK and ABC flows. The agreement is slightly\nbetter in the BK case, where both the SOC formula-\ntion and the value function Vϕ are analytically tractable,\ncompared to the ABC flow, where the estimation of ¯λ1\n8\nis approximate.\nFIG. 5. Distribution of separation s(t) for prescribed control\nwith fixed values of ϕ: 0.6 (blue), 1.1 (red), and 1.6 (green)\nfor B-K flows (left column) and ABC flows (right column).\nThe lines represent the predictions of Eq. (13). For the ABC\nflow, ¯λ1 was obtained from simulations, as shown in Fig. 4.\nThe agreement shown in Fig. 5 naturally leads us to\nthe next step – a more detailed comparison of theory\nversus simulations for the value function, as presented in\nFig. 6. In that figure, we compare the empirical and the-\noretical values of the value function corresponding to the\ndiscounted return in Eq. (1), with the predicted baseline\nin Eq. (18) or, more precisely, with the (properly nor-\nmalized) expected return G, defined in Eq. (2). The pre-\ndictions for the ABC flow match surprisingly well, with\ndiscrepancies between simulation and theoretical values\nonly slightly larger than in the BK model.\nIn Fig. 6,\nwe present results for a single value of ϕ, chosen to be\nnear its asymptotically optimal value, ϕ∗; however, sim-\nilar agreements were observed for other values of ϕ (not\nshown), as anticipated from the agreement seen in Fig. 5.\nB.\nComparison with Standard Actor-Critic\nMethods\nNow that our physics baseline, described by Eq. (18),\nhas been validated, we proceed in this section to em-\nbed it into the critic, while keeping the actor tuned as\nbefore –using a NN. We refer to this combination of a\nphysics-based critic and an NN-based actor as the Actor-\nPhysicist (AP) agent/algorithm. Our focus is on com-\nparing the performance of the AP agent with that of\nstandard AC agents, specifically those using A2C [3] and\nPPO [4] schemes.\nFig. 7 (a) and (b) demonstrate that the AP agent con-\nsistently outperforms the AC agents across all our experi-\nments, with particularly notable results in the ABC flow\nscenarios. Specifically, the AC agents using both A2C\nFIG. 6. Panels (a)-(b) display the expected return, Vϕ(s(t)),\nobserved in simulations. The corresponding theoretical val-\nues, as predicted by Eq. (18), are shown in panels (c)-(d), and\nthe difference between simulation and theory is presented in\npanels (e)-(f). The left column – panels (a), (c), and (e) –\ncorresponds to the BK flow with ϕ = 0.574, while the right\ncolumn – panels (b), (d), and (f) – corresponds to the ABC\nflow with ϕ = 1.1. (See the main text for further details on\nthe choice of ϕ values in this validation study.)\nand PPO did not yield meaningful outcomes within 250\ntraining trials, as they failed to converge, whereas the\nAP agent achieved reliable convergence within the same\nnumber of trials. We attribute the poor performance of\nstandard AC agents to the fact that, while theoretically\noptimal in the asymptotic limit, practical implementa-\ntions are inherently non-asymptotic, especially in terms\nof the limited number of trials available for training. Ad-\nditionally, we hypothesize that the failure of A2C and\nPPO may be due to the non-standard, fat-tailed statis-\ntics of our problem, characterized by extended algebraic\ntails and significant variability in reward values across\ndifferent flow configurations, spanning several orders of\nmagnitude. Such conditions are known to present chal-\nlenges for standard AC methods [16].\nWe want to emphasis that, we do not conclude that\nAC agents cannot learn to swim in a turbulent flow at all,\nbut that, with limited observation and training resources,\nthey fail, whereas our AP agent is able to learn how to\nperform this task under the same conditions.\nC.\nComparison with Prescribed Control\nNow that we have shown that the AP agent outper-\nforms standard AC agents. We would like to check if our\nAP agent actually performs better than the prescribed\ncontrol it is built upon. To investigate this we compare\nthese two control schemes in different regimes.\nTable II shows a comparison between our AP agent\n9\nFIG. 7.\nPost-training average return Gπ, defined in Eq.(2),\nfor AP algorithm (red) and AC algorithm with the A2C\nscheme (blue) and with the PPO scheme (green). The av-\neraging is over 500 samples. The results shown correspond to\nthe optimal ϕ∗= 0.574 in the case of the BK flow (a) and\nestimate of the optimal ϕ∗= 1.1 in the case of the ABC flow\n(b).\nand PC for different values of ϕ. We observe that the\ntrained AP agents tend outperform the corresponding\nPC strategies.\nFor both flows, there is one exception\nwhich is when the value of the control is set (close) to\nthe asymptotically derived optimal ϕ∗.\n(a)\nEs0∼D\n\u0002\nV (s0)\n\u0003\nϕ\nPC ϕs(t)\nAP\n0.3\n-0.21477\n-0.18348\n0.574 -0.17589\n-0.18143\n0.9\n-0.21680\n-0.18953\n1.2\n-0.26607\n-0.17662\n(b)\nEs0∼D\n\u0002\nV (s0)\n\u0003\nϕ\nPC ϕs(t)\nAP\n0.6\n-0.40784\n-0.34884\n1.1\n-0.32381\n-0.36321\n1.6\n-0.40041\n-0.31272\n2.1\n-0.49141\n-0.34564\nTABLE II. This figure compares the Prescribed Control (PC)\ncase with the AP agent/algorithm at varying ϕ values for the\nBK flow (subfigure (a)) and the ABC flow (subfigure (b)).\nThe results presented are averages over 1,000 randomly drawn\ninitial states s0. The highlighted rows indicate the optimal\nϕ∗for (a) and the estimated optimal ϕ∗for (b) used in the\nsimulations.\nResults presented in Table II demonstrates that if the\noptimal value ϕ∗of the control can be determined ac-\ncurately, the prescribed control with the optimal value\nperforms better than our AP algorithm. However, the ex-\namination of the distributions of returns, shown in Fig. 8\n(a) and (b), suggests that the lower average return of the\nAP is due to large, but infrequent failures. Theses pan-\nels also show that the median performance of the AP is\nhigher than the median performance of the fixed ϕ agent.\nThis means that the trained AP is most likely to outper-\nform its baseline on a particular realization. The higher\nrate of extreme cases under the control of the AP incurs\nvery large penalties in particular realizations.\nAn interesting result is reported in Fig. 9 when we fo-\ncus on the performance of the trained agent over shorter\ntime horizons. In this case, the trained agent tends to\noutperform its baseline even with ϕ∗selected according\nto Eq. (16). This makes sense as the optimal strength ϕ∗\nwas optimized for an asymptotic stationary distribution\nof a particle under a proportional control. If the initial\nseparation is distributed such that it is often improba-\nble (as evaluated with the stationary, that is idealized,\ndistribution), PC with ϕ∗is not the best possible action\nand the AP agent is able to prevail in this context.\nFIG. 8. Distributions of the return, G := PN\nk=0 γkr(sk, ak),\nin an ABC flow with ϕ = 1.1 for the AP agent (a), prescribed\ncontrol (b) and the hybrid scheme (c). The mean and median\nare displayed as solid and dashed lines.\nFIG. 9.\nAverage return Gπ – defined in Eq. (2) and com-\nputed through averaging over 250 realizations – shown for\nthe trained AP agent (blue) and prescribed control (red) with\nasymptotically optimal ϕ∗(solid) and suboptimal ϕ (dashed).\nThe mention of typical versus atypical behavior in the\nchallenging case where the AP agent does not perform as\nwell as the fixed ϕ agent on average helps us draw use-\nful lessons. We learn that a major benefit of a physics-\ninformed baseline is the ability to monitor the trained\nagent’s performance and identify when the agent may be\nfalling into a regime it cannot handle. In such regimes,\n10\nintervention with a naive fixed ϕ strategy – that is, de-\nsigning a hybrid policy that mixes or explores in parallel\nthe RL and fixed ϕ strategies – can avoid catastrophic\nfailures and return the agent to a regime where it out-\nperforms the fixed ϕ policy. Even a simple hybrid so-\nlution that switches to the fixed ϕ strategy if the av-\nerage advantage of the previous n training steps is be-\nlow a threshold shows improvements.\nOur results, il-\nlustrated in Fig. 8, show improvements even with arbi-\ntrary choices for n and the threshold, suggesting that\nsignificant further enhancements of the approach can be\nachieved through more principled optimization.\nTo further elaborate on the nuances of typical versus\natypical flows, Fig. 10 shows sample trajectories of the\nevolution of the separation and the penalty accrued in\na typical trajectory versus an atypical trajectory. In the\ntypical case, the AP agent is much more efficient than the\nagent following the fixed ϕ policy. In the atypical case,\nthe AP agent is overly energy conservative and thus con-\nverges to the passive particle slowly, even showing diver-\ngence from the passive partner at the start. In this case,\nthe high penalty is accrued due to the cost of separation.\nThese results imply that the agent could greatly bene-\nfit from expanding the state space since it currently has\nno way to distinguish the typical and atypical episodes\nshown in Fig. 10.\nFIG. 10.\nSelected separation of trajectories for AP (trian-\ngle) and PC (circle): typical (a) and atypical (b), the former\nmeaning from the bulk of the distribution in Fig. 8 and the\nlatter meaning from its tail. Makers’ colors display the re-\nturn, G(t) := Pn\nk=0 γkr(sk, ak), with n = t/∆.\nVII.\nCONCLUSION\nWe study the challenging problem of controlling the\nseparation between a swimmer and its passive target\nin chaotic flows using reinforcement learning. Initially,\nwe observe that state-of-the-art deep NN-based RL algo-\nrithms of the actor-critic (AC) type fail at this task due\nto their inability to handle fat-tailed statistics, which ex-\nhibit extended power-law or algebraic tails. To address\nthis, we propose a physics-informed approach where the\ncritic utilizes analytical estimates, specifically the aver-\nage maximum Lyapunov exponent of the flow.\nWe demonstrate the effectiveness of this approach\non the BK flow and validate it on the ABC flow.\nThis method replaces NN approximations of value\nfunctions with flow-control-informed functions, reducing\nthe computational burden while incorporating domain-\nspecific knowledge and maintaining convergence guaran-\ntees. Even imperfect physics-informed functions can im-\nprove policy training, provide interpretable insights into\npolicy failures, and serve as an effective alarm mecha-\nnism.\nWhile this study focuses on controlling a pair of par-\nticles, the approach naturally extends to monitoring and\ncontrolling groups navigating turbulent flows, such as air\ndrones, bird flocks, or aquatic drone swarms. Some un-\nderlying multi-agent problems may be simplified to a k-\nnearest neighbors plus mean-field control framework, as\ndiscussed in [15]. Combining the approach developed in\nthis manuscript with the mean-field framework and other\nnotable methods, such as [31] and [32], presents signifi-\ncant opportunities for advancing the emerging field of\nmulti-agent reinforcement learning.\nA particularly in-\ntriguing direction is the control of swarms of Lagrangian\nparticles with contrasting objectives, such as maintaining\ngroup cohesion, avoiding close encounters, and minimiz-\ning energy expenditure for maneuvering.\nACKNOWLEDGMENTS\nWe acknowledge R. Ferrando for useful discussion.\nThis work was supported by a start up funding from\nthe University of Arizona and a subcontract from Los\nAlamos National Laboratory.\nCODE AVAILABILITY\nThe code that was used for this work and, in particular,\nthis environment is available at https://github.com/\nCfckoh/RL_swimmers/tree/main.\n[1] M. Chertkov, Universality and Control of Fat Tails\n(2023), arXiv:2303.09635 [cond-mat, physics:nlin, stat].\n[2] R. S. Sutton and A. G. Barto, Reinforcement learning: an\nintroduction, second edition ed., Adaptive computation\n11\nand machine learning series (The MIT Press, Cambridge,\nMassachusetts, 2018).\n[3] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap,\nT. Harley, D. Silver, and K. Kavukcuoglu, Asynchronous\nMethods for Deep Reinforcement Learning, in Proceed-\nings of The 33rd International Conference on Machine\nLearning, Proceedings of Machine Learning Research,\nVol. 48, edited by M. F. Balcan and K. Q. Weinberger\n(PMLR, New York, New York, USA, 2016) pp. 1928–\n1937.\n[4] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and\nO. Klimov, Proximal Policy Optimization Algorithms\n(2017), arXiv:1707.06347 [cs].\n[5] P.\nGarnier,\nJ.\nViquerat,\nJ.\nRabault,\nA.\nLarcher,\nA. Kuhnle, and E. Hachem, A review on deep reinforce-\nment learning for fluid mechanics, Computers & Fluids\n225, 104973 (2021).\n[6] J. Rabault, F. Ren, W. Zhang, H. Tang, and H. Xu, Deep\nreinforcement learning in fluid mechanics: A promising\nmethod for both active flow control and shape optimiza-\ntion, Journal of Hydrodynamics 32, 234 (2020).\n[7] G. Reddy, J. Wong-Ng, A. Celani, T. J. Sejnowski, and\nM. Vergassola, Glider soaring via reinforcement learning\nin the field, Nature 562, 236 (2018).\n[8] G. Novati, L. Mahadevan, and P. Koumoutsakos, Con-\ntrolled gliding and perching through deep-reinforcement-\nlearning, Phys. Rev. Fluids 4, 093902 (2019), publisher:\nAmerican Physical Society.\n[9] L. Biferale, F. Bonaccorso, M. Buzzicotti, P. Clark\nDi Leoni, and K. Gustavsson, Zermelo’s problem: Op-\ntimal point-to-point navigation in 2D turbulent flows us-\ning reinforcement learning, Chaos: An Interdisciplinary\nJournal of Nonlinear Science 29, 103138 (2019).\n[10] J. K. Alageshan, A. K. Verma, J. Bec, and R. Pandit, Ma-\nchine learning strategies for path-planning microswim-\nmers in turbulent flows, Physical Review E 101, 043110\n(2020), arXiv:1910.01728 [physics, stat].\n[11] P. Gunnarson, I. Mandralis, G. Novati, P. Koumoutsakos,\nand J. O. Dabiri, Learning efficient navigation in vortical\nflow fields, Nature Communications 12, 7143 (2021).\n[12] F. Borra, L. Biferale, M. Cencini, and A. Celani, Rein-\nforcement learning for pursuit and evasion of microswim-\nmers at low Reynolds number, Physical Review Fluids 7,\n023103 (2022).\n[13] C. Calascibetta,\nL. Biferale,\nF. Borra,\nA. Celani,\nand M. Cencini, Taming Lagrangian chaos with multi-\nobjective reinforcement learning, The European Physical\nJournal E 46, 9 (2023).\n[14] C. Calascibetta, L. Biferale, F. Borra, A. Celani, and\nM. Cencini, Optimal tracking strategies in a turbulent\nflow, Communications Physics 6, 256 (2023).\n[15] F. Borra, M. Cencini, and A. Celani, Optimal collision\navoidance in swarms of active Brownian particles, Jour-\nnal of Statistical Mechanics:\nTheory and Experiment\n2021, 083401 (2021).\n[16] H. van Hasselt, A. Guez, M. Hessel, V. Mnih, and D. Sil-\nver, Learning values across many orders of magnitude\n(2016), arXiv:1602.07714 [cs.LG].\n[17] I. Uchendu, T. Xiao, Y. Lu, B. Zhu, M. Yan, J. Si-\nmon, M. Bennice, C. Fu, C. Ma, J. Jiao, S. Levine, and\nK. Hausman, Jump-start reinforcement learning (2023),\narXiv:2204.02372 [cs.LG].\n[18] V. Mnih, Playing atari with deep reinforcement learning,\narXiv preprint arXiv:1312.5602 (2013).\n[19] G. K. Batchelor, Small-scale variation of convected quan-\ntities like temperature in turbulent fluid Part 1. General\ndiscussion and the case of small conductivity, Journal\nof Fluid Mechanics 5, 113 (1959), publisher: Cambridge\nUniversity Press.\n[20] R. H. Kraichnan, Small-Scale Structure of a Scalar Field\nConvected by Turbulence, Physics of Fluids 11, 945\n(1968).\n[21] B. I. Shraiman and E. D. Siggia, Lagrangian path inte-\ngrals and fluctuations in random flow, Physical Review\nE 49, 2912 (1994).\n[22] M. Chertkov, G. Falkovich, I. Kolokolov, and V. Lebedev,\nStatistics of a passive scalar advected by a large-scale\ntwo-dimensional velocity field: Analytic solution, Phys.\nRev. E 51, 5609 (1995), publisher: American Physical\nSociety.\n[23] D. Bernard, K. Gawedzki, and A. Kupiainen, Slow Modes\nin Passive Advection, Journal of Statistical Physics 90,\n519 (1998).\n[24] E. Balkovsky and A. Fouxon, Universal long-time prop-\nerties of Lagrangian statistics in the Batchelor regime\nand their application to the passive scalar problem, Phys.\nRev. E 60, 4164 (1999), publisher: American Physical\nSociety.\n[25] G. Falkovich, K. Gawedzki, and M. Vergassola, Particles\nand fields in fluid turbulence, Rev. Mod. Phys. 73, 913\n(2001), publisher: American Physical Society.\n[26] D. Ruelle, Ergodic theory of differentiable dynamical\nsystems, Publications math´ematiques de l’IH´ES 50, 27\n(1979).\n[27] I. Goldhirsch, P.-L. Sulem, and S. A. Orszag, Stability\nand Lyapunov stability of dynamical systems: A differen-\ntial approach and a numerical method, Physica D: Non-\nlinear Phenomena 27, 311 (1987).\n[28] M. Chertkov, A. Gamba, and I. Kolokolov, Exact field-\ntheoretical description of passive scalar convection in an\nN-dimensional long-range velocity field, Physics Letters\nA 192, 435 (1994).\n[29] M. Chertkov, On how a joint interaction of two inno-\ncent partners (smooth advection and linear damping)\nproduces a strong intermittency, Physics of Fluids 10,\n3017 (1998).\n[30] X.-H. Zhao, K.-H. Kwek, J.-B. Li, and K.-L. Huang,\nChaotic and Resonant Streamlines in the ABC Flow,\nSIAM Journal on Applied Mathematics 53, 71 (1993),\npublisher: Society for Industrial and Applied Mathemat-\nics.\n[31] G. Novati, H. L. De Laroussilhe, and P. Koumoutsakos,\nAutomating turbulence modelling by multi-agent rein-\nforcement learning, Nature Machine Intelligence 3, 87\n(2021).\n[32] H. J. Bae and P. Koumoutsakos, Scientific multi-agent\nreinforcement learning for wall-models of turbulent flows,\nNature Communications 13, 1443 (2022).\n",
  "categories": [
    "eess.SY",
    "cs.LG",
    "cs.SY",
    "nlin.CD",
    "physics.flu-dyn",
    "stat.ML"
  ],
  "published": "2024-06-05",
  "updated": "2024-11-10"
}