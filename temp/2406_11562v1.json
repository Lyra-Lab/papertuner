{
  "id": "http://arxiv.org/abs/2406.11562v1",
  "title": "An Imitative Reinforcement Learning Framework for Autonomous Dogfight",
  "authors": [
    "Siyuan Li",
    "Rongchang Zuo",
    "Peng Liu",
    "Yingnan Zhao"
  ],
  "abstract": "Unmanned Combat Aerial Vehicle (UCAV) dogfight, which refers to a fight\nbetween two or more UCAVs usually at close quarters, plays a decisive role on\nthe aerial battlefields. With the evolution of artificial intelligence,\ndogfight progressively transits towards intelligent and autonomous modes.\nHowever, the development of autonomous dogfight policy learning is hindered by\nchallenges such as weak exploration capabilities, low learning efficiency, and\nunrealistic simulated environments. To overcome these challenges, this paper\nproposes a novel imitative reinforcement learning framework, which efficiently\nleverages expert data while enabling autonomous exploration. The proposed\nframework not only enhances learning efficiency through expert imitation, but\nalso ensures adaptability to dynamic environments via autonomous exploration\nwith reinforcement learning. Therefore, the proposed framework can learn a\nsuccessful dogfight policy of 'pursuit-lock-launch' for UCAVs. To support\ndata-driven learning, we establish a dogfight environment based on the\nHarfang3D sandbox, where we conduct extensive experiments. The results indicate\nthat the proposed framework excels in multistage dogfight, significantly\noutperforms state-of-the-art reinforcement learning and imitation learning\nmethods. Thanks to the ability of imitating experts and autonomous exploration,\nour framework can quickly learn the critical knowledge in complex aerial combat\ntasks, achieving up to a 100% success rate and demonstrating excellent\nrobustness.",
  "text": "An Imitative Reinforcement Learning Framework for\nAutonomous Dogfight\nSIYUAN LI, Harbin Institute of Technology, China\nRONGCHANG ZUO, Harbin Institute of Technology, China\nPENG LIU, Harbin Institute of Technology, China\nYINGNAN ZHAOâˆ—, Harbin Engineering University, China\nUnmanned Combat Aerial Vehicle (UCAV) dogfight, which refers to a fight between two or more UCAVs\nusually at close quarters, plays a decisive role on the aerial battlefields. With the evolution of artificial intelli-\ngence, dogfight progressively transits towards intelligent and autonomous modes. However, the development\nof autonomous dogfight policy learning is hindered by challenges such as weak exploration capabilities,\nlow learning efficiency, and unrealistic simulated environments. To overcome these challenges, this paper\nproposes a novel imitative reinforcement learning framework, which efficiently leverages expert data while\nenabling autonomous exploration. The proposed framework not only enhances learning efficiency through\nexpert imitation, but also ensures adaptability to dynamic environments via autonomous exploration with\nreinforcement learning. Therefore, the proposed framework can learn a successful dogfight policy of â€˜pursuit-\nlock-launchâ€™ for UCAVs. To support data-driven learning, we establish a dogfight environment based on\nthe Harfang3D sandbox, where we conduct extensive experiments. The results indicate that the proposed\nframework excels in multistage dogfight, significantly outperforms state-of-the-art reinforcement learning\nand imitation learning methods. Thanks to the ability of imitating experts and autonomous exploration, our\nframework can quickly learn the critical knowledge in complex aerial combat tasks, achieving up to a 100%\nsuccess rate and demonstrating excellent robustness.\nAdditional Key Words and Phrases: Reinforcement Learning, Imitation Learning, Dogfight, Air Combat\nACM Reference Format:\nSiyuan Li, Rongchang Zuo, Peng Liu, and Yingnan Zhao. 2024. An Imitative Reinforcement Learning Framework\nfor Autonomous Dogfight. 1, 1 (June 2024), 20 pages. https://doi.org/XXXXXXX.XXXXXXX\n1\nINTRODUCTION\nIn recent years, emerging artificial intelligence technologies such as deep learning have made\nremarkable advancements in the military field. The increasing significance of artificial intelligence\napplications in military operations has propelled modern warfare toward intelligent development\n[2, 36]. Unmanned combat aerial vehicles (UCAVs), as key elements in aerial combat, have become\nessential in modern military and defense, demonstrating outstanding combat performance in tasks\nsuch as aerial strikes, reconnaissance, electronic warfare, and target tracking. The autonomous\ncontrol of UCAVs has gradually become a primary research focus [1, 6, 14], and dogfight is one of\nâˆ—Corresponding author\nAuthorsâ€™ addresses: Siyuan Li, siyuanli@hit.edu.cn, Harbin Institute of Technology, China, 150001; Rongchang Zuo,\n2021110788@stu.hit.edu.cn, Harbin Institute of Technology, China, 150001; Peng Liu, pengliu@hit.edu.cn, Harbin In-\nstitute of Technology, China, 150001; Yingnan Zhao, zhaoyingnan@hrbeu.edu.cn, Harbin Engineering University, China,\n150006.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee\nprovided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the\nfull citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored.\nAbstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires\nprior specific permission and/or a fee. Request permissions from permissions@acm.org.\nÂ© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM XXXX-XXXX/2024/6-ART\nhttps://doi.org/XXXXXXX.XXXXXXX\n, Vol. 1, No. 1, Article . Publication date: June 2024.\narXiv:2406.11562v1  [cs.LG]  17 Jun 2024\n2\nSiyuan Li, et al.\nthe most important tasks for UCAVs, which refers to a fight between two or more UCAVs usually\nat close quarters. This paper mainly focuses on autonomous control policy learning for dogfight\ntasks between two UCAVs.\nThere has been substantial research on autonomous control policy learning for UCAV dogfight\nengagements [28, 31, 34], and most existing works rely on supervised learning and offline imitation\nlearning techniques [18, 20, 32, 41]. With a delicately designed dataset, these methods can learn\na well-performed control policy for the UCAVs. However, the absence of online interactions can\nlead to the overfitting to these static datasets. In realistic combat environments, opponent policies\nare everchanging, and unforeseen obstacles may appear suddenly. The UCAV control policies\nlearned with a fixed dataset can hardly handle such dynamic environments. Furthermore,\nthe cost of constructing such a training dataset is expensive as well, and the performance of the\nlearned policy heavily depends on the quality of the dataset. Consequently, it is of great importance\nto develop more adaptive learning frameworks for UCAV control policy.\nRecently, more researchers have investigated reinforcement learning (RL) [33] techniques to\nachieve autonomous policy learning for UCAVs [5, 8, 9, 16, 19, 37]. Different from supervised\nlearning methods, RL enables the control agent for the UCAV to gradually learn policies through\nonline interactions with the environment. Considering the dynamic property of the dogfight\nenvironment, RL is suitable for such a learning problem [27], as it supports online exploration\nand therefore can better adapt to dynamic changes in the environments. However, dogfight policy\nlearning is quite a complex problem, which involves multiple control dimensions, e.g., the direction\nof flight, target lock, and missile launch. Due to the trial-and-error nature of RL agents, the sample\nefficiency of RL algorithms in such complex learning scenarios is low, leading to poor\nsuccess rates of dogfight engagements.\nBesides the learning frameworks, the simulated learning environment is substantially important\nas well. Since directly conducting dogfight policy learning in real environments is impossible, an\neffective and high-fidelity simulator is urgently required. However, the simulated environments\nin previous works suffer from the oversimplified action spaces and task setting. For\nexample, the action space in [8] is simplified into a discrete predefined maneuver library; [9]\nconfines the action space to a two-dimensional simulation environment; [5, 19, 37] merely use\npart of the parameters of the UCAV flight dynamics equations as the action space. As for the task\nsetting, [16] solely focuses on the flight control of UCAV without launching missiles; [39] tackles\nmissile evasion of UCAV without flight control; [7, 13, 15, 43] aim to achieve the objectives of route\nplanning; [9, 12] aim to accomplish the pursuit subtask; [19] completes the locking of the opponent\nbased on rule-based pursuing. Nevertheless, to successfully shoot down the opponent in a realistic\ndogfight task, UCAV needs to accomplish three subtasks at least. These include swiftly pursuing the\nopponent through the optimal path, locking onto the target within a proper range, and launching\nmissiles once the opponent is locked.\nTo address the aforementioned challenges in dogfight policy learning, we introduce a novel\nimitative reinforcement learning framework that integrates imitation learning with autonomous\nexploration by the RL agent. Imitating expert data improves learning efficiency, and autonomous\nexploration with the RL agent helps adapt to dynamic environments. In this work, the UCAV\ndogfight problem is formulated as a Markov Decision Process (MDP). To alleviate the limitations of\nprevious dogfight simulators, we construct a highly realistic UCAV simulation environment based\non the Harfang3D sandbox [25]. The constructed learning environment is endowed with an action\nspace design with high fidelity, controlling UCAV directional rudders, elevators, and ailerons, and\nthus more realistic than simplified dynamics equation parameter control in previous works. As a\nreplication of the real dogfight process, the simulated dogfight task consists of three stages, i.e.,\npursuit, lock, and launch, providing a more complex replication of real dogfight processes than\n, Vol. 1, No. 1, Article . Publication date: June 2024.\nAn Imitative Reinforcement Learning Framework for Autonomous Dogfight\n3\nprevious single-stage works. As an evaluation, we conduct experiments in the constructed dogfight\nenvironment. The experiment results demonstrate that the training environment constructed in\nthis paper has high fidelity, effectively simulating the UCAV dogfight process, with a physical\nmodel that matches the real-world scenarios better. The proposed learning framework performs\nexcellently in the UCAV multi-dimensional decision-making task, significantly outperforming\nstate-of-the-art deep RL and imitation learning algorithms, achieving higher learning efficiency,\nmore effective air combat policies, and higher success rates. The contributions of this work can be\nsummarized as follows.\nâ€¢ Problem Formulation: We construct a simulated dogfight environment with high fidelity,\nwhich contains multiple stages: pursuit, lock, and launch.\nâ€¢ Learning Algorithm: We propose a novel imitative reinforcement learning framework for\nUCAV dogfight that enhances learning efficiency and adapts to complex environments.\nâ€¢ Empirical Improvement: The proposed framework significantly surpasses the state-of-the-\nart RL and imitation learning methods in the dogfight problem.\nThis work is organized as follows. In Section 2, we introduce preliminary knowledge, and then in\nSection 3, we review the related works. After that, in Section 4, we formulate the dogfight problem\nas an MDP. In Section 5, we present the proposed framework for dogfight policy learning. Next,\nin Section 6, we conduct extensive experiments, and the results demonstrate that the proposed\nframework substantially outperforms the baselines. Finally, in Section 7, we conclude and point\nout the possible future directions.\n2\nPRELIMINARIES\nIn this section, we review the preliminary knowledge about Harfang3D Sandbox simulator, Markov\ndecision process, and the actor-critic framework.\n2.1\nHarfang3D Sandbox\nWe establish a simulation platform based on the Harfang3D sandbox [25], which is a customizable\nand flexible tool for combat aircraft dogfight. This platform facilitates the manipulation and access\nto flight dynamics models and environment states. The sandbox provides simulated aircrafts of\nfive different models, each equipped with unique missile configurations, and a limited missile\nsupply. Therefore, the aircraft must lock onto a target before launching a missile for avoiding\nmissing the target and consequently losing a missile, which increases the task difficulty. Within this\nsimulation environment, information about environment states can be obtained, including aircraft\nposition, attitude, heading angle, and health as well as target lock status, thrust, lock angle, and\nmissile loading. Specially, the sandbox also offers various unmanned aircraft control methods for\ncomponents including rudders, elevators, ailerons, missile launch, engine thrust, landing gear, and\nflaps. As illustrated in Figure 1, rudders control the aircraft heading, elevators control the aircraft\npitch, and ailerons control the aircraft roll. These three controls enable basic transformations of\nthe aircraftâ€™s attitude.\n2.2\nMarkov Decision Process\nThe Markov Decision Process (MDP) [11] is the theoretical foundation for RL, which provides a\nformal description of the learning environment of the agent. In this paper, we model the UCAV\ndogfight environment as an MDP. The MDP is represented as a tuple (ğ‘†,ğ´, ğ‘ƒ, ğ‘…,ğ›¾), where ğ‘†is the\nstate space of the environment, ğ´is the action space of the agent, ğ‘ƒis the state transition probability,\nwith ğ‘ƒ(ğ‘ ğ‘¡+1|ğ‘ ğ‘¡,ğ‘ğ‘¡) indicating the probability of transitioning from state ğ‘ ğ‘¡to state ğ‘ ğ‘¡+1 when taking\naction ğ‘ğ‘¡at time ğ‘¡, ğ‘…is the reward function, representing the immediate reward obtained by the\n, Vol. 1, No. 1, Article . Publication date: June 2024.\n4\nSiyuan Li, et al.\n(a) Rudders\n(c) Ailerons\n(b) Elevators\nFig. 1. Diagram of aircraft attitude control.\nintelligent agent after taking action ğ‘, and ğ›¾is the reward discount factor, with ğ›¾âˆˆ[0, 1) and\na ğ›¾value closer to 1 indicating that the intelligent agent focuses more on long-term rewards.\nReinforcement learning aims to learn policy function ğœ‹: ğ‘†â†’ğ´that maximizes discounted reward\nEğ‘ƒ,ğœ‹[Ãğ‘‡\nğ‘¡=1 ğ›¾ğ‘¡ğ‘…(ğ‘ ğ‘¡,ğ‘ğ‘¡)], where ğ‘‡is the total timestep in an episode.\n2.3\nActor-Critic Framework\nThe actorâ€“critic framework [17] is a policy optimization paradigm in RL that simultaneously\noptimizes both the value and policy functions, exhibiting good learning performance in sequential\ndecision problems with continuous action spaces. The actor represents the policy ğœ‹of the agent,\nwhich specifies the actions to take at each state. It aims to learn an optimal policy that maximizes\nthe expected cumulative reward. During training, the actor adjusts its policy based on feedback\nfrom the critic, gradually improving the policy performance. The critic estimates the value of each\naction to assess the quality of the policy and assists the actor in refining its policy. The actorâ€“critic\nframework is based on policy gradient methods, combining back-propagation with value function\nmethods to enhance the policy performance. Based on the actor-critic framework, the agent can\nlearn a policy and corresponding value function during interaction with the environment, enabling\nhigh-level decision-making and intelligent behavior.\n3\nRELATED WORK\nThe previous works deal with the UCAV dogfight problem either with the conventional methods or\nusing the advanced approaches with deep neural networks.\n3.1\nConventional Methods\nBefore deep learning methods are employed in the dogfight policy learning domain, conventional\nmethods are adopted to address tactical decision-making problems in UCAV operation, including\nexpert systems, game theory, and influence diagrams.\nExpert knowledge-based methods [3, 4, 38] involve creating an expert system that infers\ndecisions by directly applying predefined rules within the system. As the rules in the expert system\nare fixed, the system cannot provide effective control policies when facing situations not covered\nby the predefined rules. Fu et al. [9] proposed an expert system control method based on the rolling\ntime domain to improve the system response. In cases where traditional expert systems fail, their\nmethod allows for rapid autonomous decision-making to achieve attack conditions.\nGame-theory-based methods [26] allow modeling aerial combat scenarios as game problems\nby treating UCAV as players in the game. The actions of UCAV constitute the policy space, and\nreward functions are defined based on parameters such as the positions of UCAVs from the two sides.\nPark et al. [26] applied differential game theory to generate tactical action combinations for UCAVs.\n, Vol. 1, No. 1, Article . Publication date: June 2024.\nAn Imitative Reinforcement Learning Framework for Autonomous Dogfight\n5\nThrough a layered decision structure and calculation of a scoring matrix based on differential game\ntheory, optimal actions to deal with dynamic combat environments are determined.\nInfluence diagram-based methods [22] allow the abstraction of the decision network in UCAV\noperation. These methods involve scenario analysis based on the constructed influence diagram to\nselect the optimal course of action. Zhong et al. [22] used multistage influence diagrams to model\none-on-one aerial combat scenarios. They transformed the multistage influence diagram into a\ntwo-layer optimization problem as a solution to obtain the optimal attack trajectory.\nConventional methods typically rely on predefined rules or models. Because these rules and\nmodels are often manually designed, UCAVs lack mechanisms for autonomous learning and adap-\ntation. Consequently, these methods are generally inadequate for dealing with uncertainties on the\nbattlefield.\n3.2\nDeep Learning Methods\nWith the rapid development of deep learning, the control for UCAVs has become moreautonomous.\nDeep learning enables UCAVs to both execute fixed programs and exhibit learning capabilities.\nIn recent years, deep learning techniques such as supervised learning, imitation learning, and\nreinforcement learning have been widely used to address tactical decision-making problems in\nUCAV dogfights.\nSupervised learning is predominantly used in UCAV dogfight to predict the states after ex-\necuting actions, providing a basis for action selection. Zhang and Huang [41] proposed a deep\nlearning method integrating state evaluation and action selection for making flight action decisions.\nThe state evaluation function scores the next state after executing an action and selects the action\nthat generates the highest score. This approach enables autonomous flight control of UCAVs in\none-on-one aerial combat. Li et al. [20] trained a long short-term memory network with time-series\ndata, mapping historical state sequences onto flight control parameters. They used a stacked sparse\nautoencoder to reduce data redundancy and speed up decision-making.\nImitation learning [32] is a form of supervised learning method where the model at-tempts to\nlearn and replicate behaviors from examples or expert demonstrations. Imitation learning methods\ninclude 1) behavior cloning (BC), 2) inverse reinforcement learning, and 3) generative adversarial\nimitation learning.\n1. BC [32] involves training a model on expert demonstration data to resemble the expert\nbehaviors. SandstrÃ¶m et al. [32] adopted BC using control signals from human pilots to train a\nfeedforward neural network, enabling the model to mimic the behaviors of human pilots for aircraft\nflight.\n2. Inverse reinforcement learning [18] estimates the reward function from expert demonstrations\nand uses it to guide the training of intelligent agents in reinforcement learning. By extracting the\nhidden reward function from expert data, the system can understand task objectives from expert\ndemonstrations. Kong et al. [18] employ a reward shaping method to address the problem of sparse\nrewards in reinforcement learning and use maximum entropy inverse reinforcement learning to\nobtain a shaped reward function, thereby accelerating convergence in training.\n3. Generative adversarial imitation learning [37] uses adversarial generative networks to learn\nhidden policies from expert demonstrations, imitating expert actions. Wang and Wei [37] applied\nthis technique to address the problem of sparse rewards in reinforcement learning. They designed\ninternal rewards to stimulate intelligent agents to explore autonomously, thus expediting training.\nReinforcement learning introduces more autonomous and adaptive methods for dogfight\n[5, 8, 9, 19, 37]. The RL agent actively interacts with the environment and learns optimal policies\nthrough trials and errors. The RL agent adjusts behavior based on received reward signals, gradually\nimproving performance to adapt to the constantly changing environment. Fan et al. [8] propose an\n, Vol. 1, No. 1, Article . Publication date: June 2024.\n6\nSiyuan Li, et al.\nautonomous maneuver decision method for UCAV based on Asynchronous Advantage Actor-Critic\n(A3C) [24] algorithm. Relying on a library of seven predefined actions, they train UCAV to gain an\nadvantageous position in aerial combat, and the training efficiency is improved through a multi-\nthreaded asynchronous mechanism. Cao et al. [5] integrate RL with game theory, using Double\nDeep Q-Network (DDQN) [35] algorithm to train a three-degree-of-freedom UCAV to execute\nseven basic aerial combat maneuvers and maintain a superior position in air combat. Wang et al.\n[37] address the issue of multi-UCAV aerial combat by designing reward functions that incorporate\nprior knowledge. They utilize Multi-Agent Deep Deterministic Policy Gradient (MADDPG) [23]\nalgorithm to overcome the challenges associated with high-dimensional, continuous action spaces\nthat traditional reinforcement learning methods face. This approach enables multiple UCAVs to\nmaintain a superior position in aerial combat. Fu et al. [9] use Deep Deterministic Policy Gradient\n(DDPG) [21] algorithm to train a two-degree-of-freedom UCAV to complete tracking and evasion\ntasks in UCAV dogfight, introducing imitation learning to address the inefficiency of exploring\ndifferent scenarios during the training phase of reinforcement learning algorithms. Li et al. [19]\nbuild upon Soft Actor-Critic (SAC) [12] method, utilizing expert knowledge to prevent SAC from\ngetting stuck in local optima and accelerating the training process. This work trains a three-degree-\nof-freedom UCAV to perform pursuit and lock decisions in UCAV dogfight, but has not addressed\nthe weapon usage problem. Zhang et al. [42] propose to expand dogfight decision-making from a\nsingle dimension to multiple dimensions, including target pursuit, target lock, and weapon use.\nThis work employs the SAC algorithm for policy learning and establishes a set of meta-policies.\nHowever, this work is limited to a two-dimensional environment, and the weapon meta-policies\nare manually defined.\nAlthough deep learning methods, especially deep RL, have made significant progress in UCAV\ndogfight, current research still face with several challenges, such as unrealistic simulation environ-\nments, oversimplified task design, low algorithmic sample efficiency, and inadequate exploration\ncapabilities. To address these issues, we construct a realistic UCAV dogfight simulation environ-\nment based on the Harfang3D sandbox which utilizes a UCAV control mechanism aligned with\nreal operational procedures, and design a comprehensive dogfight task that incorporates multiple\ndecision dimensions. Furthermore, we propose a novel imitative reinforcement learning framework\nthat not only enhances learning efficiency through expert imitation but also improves the policyâ€™s\nadaptability to dynamic environments through autonomous exploration enabled by reinforcement\nlearning, thereby mastering complex multi-dimensional decision policies in UCAV dogfight tasks.\n4\nPROBLEM FORMULATION\nWe establish a realistic simulation environment for UCAV dogfight policy learning based on the\nHarfang3D sandbox [25] and model the â€˜pursuit-lock-launchâ€™ tasks in dogfight as an MDP, which\nis shown in Figure 2.\n4.1\nTask Description\nIn this subsection, we describe the UCAV dogfight task,which is divided into three consecutive\nstages: target pursuit, target lock, and missile launch. The initial positions of our aircraft and\nthe opponent aircraft are fixed, with a relative distance of 4060 meters. The opponent aircraft\nflies forward at a constant speed, while our aircraft is controlled intelligently through a neural\nnetwork. If our aircraft catches up with the opponent aircraft, locks onto it, launches a missile, and\nfinally shoots down the opponent aircraft, the mission is successful. To ensure that the agent can\nsuccessfully and accurately complete this task, it needs to master at least three policies of three\ndifferent stages. Successful target lock requires meeting lock conditions and maintaining them for\n, Vol. 1, No. 1, Article . Publication date: June 2024.\nAn Imitative Reinforcement Learning Framework for Autonomous Dogfight\n7\nEnvironment\nAgent\nState\nReward\nAction\nFig. 2. The formulated MDP environment.\n5 seconds, as given below.\n100 < | Â®ğ·| < 3000 and ğ‘< 15â—¦,\n(1)\nwhere | Â®ğ·| is the relative distance between our aircraft and the opponent aircraft, and ğ‘is the angle\nshown in Figure 3(a).\nğ‘‹\nğ‘Œ\nğ‘\nO\nğ‘‰ğ‘¢\nğ·\nq\nğ‘‰ğ‘’\nRoll angle\nYaw angle\nPitch angle\nğ‘‹\nğ‘\nğ‘Œ\n(a) Cartesian coordination diagram \n(b) Euler angle diagram\nFig. 3. The red aircraft denotes our aircraft, and the blue one denotes the opponent aircraft.\n4.2\nState Space\nAs shown in Figure 3, within the same Cartesian coordinate system, the coordinates of our aircraft\nare represented as (ğ‘¥ğ‘¢,ğ‘¦ğ‘¢,ğ‘§ğ‘¢), and those of the opponent aircraft are denoted as (ğ‘¥ğ‘’,ğ‘¦ğ‘’,ğ‘§ğ‘’). The\nrelative distance vector, denoted as Â®ğ·, is expressed as\nÂ®ğ·= (Î”ğ‘¥, Î”ğ‘¦, Î”ğ‘§) = (ğ‘¥ğ‘¢,ğ‘¦ğ‘¢,ğ‘§ğ‘¢) âˆ’(ğ‘¥ğ‘’,ğ‘¦ğ‘’,ğ‘§ğ‘’).\n(2)\n, Vol. 1, No. 1, Article . Publication date: June 2024.\n8\nSiyuan Li, et al.\nÂ®ğ‘‰ğ‘¢and Â®ğ‘‰ğ‘’are the velocity vectors of our aircraft and the opponent aircraft, respectively. The angle\nğ‘between the velocity vector of our aircraft and the relative distance vector is the locking angle.\nThe state space is comprised of 14 variables: the Cartesian coordinate differences between our\naircraft and the opponent aircraft (Î”ğ‘¥, Î”ğ‘¦, Î”ğ‘§), the Euler angle of our aircraft (in the simulation\nenvironment, the three-dimensional Euler angles correspond to the pitch angle, yaw angle, and\nroll angle, and the corresponding angles can be proportionally converted), heading angle of our\naircraft, heading angle of the opponent aircraft, pitch angle of the opponent aircraft, roll angle of\nthe opponent aircraft, locking angle, opponent aircraft health, locking status, and missile status.\n4.3\nAction Space\nThe action space consists of four types of actions: rudder, elevator, and aileron control as well as\nmissile launch control variable. The aircraft carried only one missile, thus having one opportunity\nto shoot down the opponent aircraft. The rudder, elevator, and aileron control are continuous\nvariables, and we normalize them to a range of (âˆ’1, 1). The missile launch control variable is\ndiscrete, with âˆ’1 representing no launch and 1 representing launch. This hybrid action space,\nconsisting of both continuous and discrete variables, presents increased challenges in dogfight\npolicy learning, but resembles the real-world scenarios more.\n4.4\nReward Function\nThe reward function is a critical component for applying reinforcement learning to real-world\nproblems. In the UCAV dogfight, the operated aircraft pursues the opponent aircraft, locks its aim,\nand launches a missile within an extremely short time. To accomplish this task, we propose the\nfollowing reward function with four terms:\nğ‘…= ğ‘…ğ‘‘ğ‘–ğ‘ ğ‘¡ğ‘ğ‘›ğ‘ğ‘’+ ğ‘…ğ‘™ğ‘œğ‘ğ‘˜+ ğ‘…ğ‘ğ‘™ğ‘¡ğ‘–ğ‘¡ğ‘¢ğ‘‘ğ‘’+ ğ‘…ğ‘™ğ‘ğ‘¢ğ‘›ğ‘â„.\n(3)\nğ‘…ğ‘‘ğ‘–ğ‘ ğ‘¡ğ‘ğ‘›ğ‘ğ‘’is a reward function designed to ensure that the operated aircraft can catch up with the\nopponent aircraft. This reward calculates the distance between the operated and opponent aircraft\nand uses a negative distance value as the reward function defined as\nğ‘…ğ‘‘ğ‘–ğ‘ ğ‘¡ğ‘ğ‘›ğ‘ğ‘’= âˆ’10âˆ’4| Â®ğ·|.\n(4)\nThe reward in Eq.(4) encourages the operated aircraft to swiftly catch the opponent aircraft for\nsubsequent target lock.\nğ‘…ğ‘™ğ‘œğ‘ğ‘˜is the reward that incentivizes the operated aircraft to lock onto the opponent aircraft. It\nuses the negative locking angle value for a smaller locking angle to provide a higher reward. The\nreward function is given by\nğ‘…ğ‘™ğ‘œğ‘ğ‘˜= âˆ’10ğ‘.\n(5)\nğ‘…ğ‘ğ‘™ğ‘¡ğ‘–ğ‘¡ğ‘¢ğ‘‘ğ‘’constrains the operated aircraft exploration. Penalties are imposed when the aircraft\nflies at excessively high or low altitudes as follows:\nğ‘…ğ‘ğ‘™ğ‘¡ğ‘–ğ‘¡ğ‘¢ğ‘‘ğ‘’=\n\u001a âˆ’4,\nif ğ‘§ğ‘¢> 7000 or ğ‘§ğ‘¢< 2000\n0,\nğ‘œğ‘¡â„ğ‘’ğ‘Ÿğ‘¤ğ‘–ğ‘ ğ‘’\n(6)\nğ‘…ğ‘™ğ‘ğ‘¢ğ‘›ğ‘â„is the reward that encourages a successful launch. This reward is given only when the\nlaunch succeeds as follows:\nğ‘…ğ‘™ğ‘ğ‘¢ğ‘›ğ‘â„=\nï£±ï£´ï£´ï£´ï£´ï£²\nï£´ï£´ï£´ï£´ï£³\nâˆ’1,\nif ğµğ‘“= 1 and ğµğ‘š= 0\n100,\nif ğµğ‘“ğµğ‘š= 1 and ğµğ‘™= 1\nâˆ’4,\nif ğµğ‘“ğµğ‘š= 1 and ğµğ‘™= 0\n0,\nğ‘œğ‘¡â„ğ‘’ğ‘Ÿğ‘¤ğ‘–ğ‘ ğ‘’\n(7)\n, Vol. 1, No. 1, Article . Publication date: June 2024.\nAn Imitative Reinforcement Learning Framework for Autonomous Dogfight\n9\nwhere ğµğ‘“, ğµğ‘š, and ğµğ‘™are Boolean values set to 1 (true) when the aircraft launches a missile, has a\nloaded missile, and locks onto the opponent, respectively. ğ‘…ğ‘™ğ‘ğ‘¢ğ‘›ğ‘â„promotes the aircraft to execute\nthe correct launch action, with the highest reward being achieved when the aircraft launches and\nhas a missile after locking onto the opponent and the lowest reward being achieved when the\naircraft launches a loaded missile without locking onto the opponent aircraft.\nAmong the four reward terms, ğ‘…ğ‘‘ğ‘–ğ‘ ğ‘¡ğ‘ğ‘›ğ‘ğ‘’, ğ‘…ğ‘™ğ‘œğ‘ğ‘˜, and ğ‘…ğ‘™ğ‘ğ‘¢ğ‘›ğ‘â„facilitate the smooth completion\nof pursuit, lock, and launch, respectively, while ğ‘…ğ‘ğ‘™ğ‘¡ğ‘–ğ‘¡ğ‘¢ğ‘‘ğ‘’constrains the exploration space of the\noperated aircraft to achieve effective exploration.\n5\nMETHODOLOGY\nTo accomplish the multistage and complex control tasks of pursuing an opponent aircraft, locking\nonto the opponent, launching a missile, and ultimately shooting down the opponent, we propose\nan imitative reinforcement learning method that integrates expert knowledge learning with the\nautonomous exploration of reinforcement learning. As illustrated in Figure 4, the proposed method\nemploys the actorâ€“critic network for learning while leveraging expert data to constrain learning,\nachieving efficient policy learning. The algorithm details are outlined below.\nğœğ‘’\nFire\nLock\nChase\nğ‘ğ‘’\nPolicy\nğ‘\nExpert learning\nExploration learning\nğ‘ ğ‘’\nPolicy\nCritic\nğ‘\nğ‘ \nEnvironment\nğœ†ğ¿ğµğ¶\n(1 âˆ’ğœ†) ğ¿ğ‘…ğ¿\nPolicy\nExpert policy\nLoss generation\nEvaluation\nğœ†\nsample\nOnline\nOffline\nPolicy Optimization\nPursuit\nLaunch\nFig. 4. The proposed learning framework for dogfight.\nThe actorâ€“critic framework serves as the foundation for dogfight policy learning. To mitigate\nthe overestimation of the critic, we adopt a dual Q-network architecture (ğ‘„ğœ™1 and ğ‘„ğœ™2). Inspired\nby the RL objective shown in Eq.(8), the actor network ğœƒğ‘ğ‘ğ‘¡ğ‘œğ‘Ÿis trained to maximize the smaller Q\n, Vol. 1, No. 1, Article . Publication date: June 2024.\n10\nSiyuan Li, et al.\nvalue between the two Q values.\nğ¿ğ‘…ğ¿= âˆ’ğ¸ğ‘ âˆ¼ğ·[ğ‘šğ‘–ğ‘›ğ‘–=1,2ğ‘„ğœ™ğ‘–(ğ‘ , ğœ‹(ğ‘ ;ğœƒğ‘ğ‘ğ‘¡ğ‘œğ‘Ÿ))],\n(8)\nwhere ğ¿ğ‘…ğ¿denotes the RL loss for the actor network, ğ·represents the experience pool, ğœ‹(ğ‘ ;ğœƒğ‘ğ‘ğ‘¡ğ‘œğ‘Ÿ)\ndenotes the policy network with parameter ğœƒğ‘ğ‘ğ‘¡ğ‘œğ‘Ÿ, and ğ‘„(ğ‘ ,ğ‘) represents the estimated Q value by\nthe critic network. Minimizing loss ğ¿ğ‘…ğ¿to update the parameters of the actor network can help\ndogfight policy learning.\nHowever, relying solely on the RL objective likely leads to low sample utilization, as the RL\nagent explores by trial and error in the environment, thus requiring substantial samples to discover\nsuccessful policies. In contrast, expert data contain rich prior knowledge. Hence, we introduce a\nBC term, ğ¿ğµğ¶, as shown in Eq.(9) for the agents to imitate expert policies and efficiently learn key\nknowledge. BC loss ğ¿ğµğ¶is defined as\nğ¿ğµğ¶= ğ¸(ğ‘ ğ‘’,ğ‘ğ‘’)âˆ¼Tğ‘’||ğ‘ğ‘’âˆ’ğœ‹(ğ‘ ğ‘’;ğœƒğ‘ğ‘ğ‘¡ğ‘œğ‘Ÿ)||2,\n(9)\nwhere T ğ‘’denotes the expert trajectory obtained through artificial intelligence (AI) mode in\nHarfang3D sandbox and (ğ‘ ğ‘’,ğ‘ğ‘’) denotes the expert stateâ€“action pairs. The proposed learning\nobjective, as expressed in Eq.(10), supports agents in autonomous exploration and improves their\nperformance for critical actions in tasks (e.g., launch) to approach expert-level capabilities.\nğœƒğ‘ğ‘ğ‘¡ğ‘œğ‘Ÿâ†ğ‘ğ‘Ÿğ‘”ğ‘šğ‘–ğ‘›ğœƒğ‘ğ‘ğ‘¡ğ‘œğ‘Ÿ[(1 âˆ’ğœ†)ğ¿ğ‘…ğ¿+ ğ›¼ğœ†ğ¿ğµğ¶],\n(10)\nwhere ğ›¼is a fixed weight to balance the magnitude ğ¿ğ‘…ğ¿and ğ¿ğµğ¶, and ğœ†is a weight to balance the\ntwo loss terms. In this work, we propose to use the loss function (1 âˆ’ğœ†)ğ¿ğ‘…ğ¿+ ğ›¼ğœ†ğ¿ğµğ¶to update\nthe actor network, so that we can take the advantage of both RL and imitation learning. However,\nbalancing these two learning objectives poses a significant challenge. To address this issue, we\ndevelop two mechanisms to set the balancing factor ğœ†: linear ğœ†and adaptive ğœ†.\nLinear ğœ†: The initial value for ğœ†is 1, and it gradually decreases with the training round ğ‘’as\nfollows:\nğœ†= 1 âˆ’ğ‘’Ã— ğ‘˜,\n(11)\nwhere ğ‘˜is a constant, and we set ğ‘˜as 0.001 in the experiments.\nAdaptive ğœ†: The weight ğœ†is automatically updated at each round by comparing the value\nfunctions of the current policy and expert policy as follows:\nğœ†=\n\u001a 1,\nif ğ‘„ğœ™1 (ğ‘ , ğœ‹ğ‘’(ğ‘ )) > ğ‘„ğœ™1(ğ‘ , ğœ‹(ğ‘ ;ğœƒğ‘ğ‘ğ‘¡ğ‘œğ‘Ÿ)) or ğ‘„ğœ™2 (ğ‘ , ğœ‹ğ‘’(ğ‘ )) > ğ‘„ğœ™2(ğ‘ , ğœ‹(ğ‘ ;ğœƒğ‘ğ‘ğ‘¡ğ‘œğ‘Ÿ))\n0,\nğ‘œğ‘¡â„ğ‘’ğ‘Ÿğ‘¤ğ‘–ğ‘ ğ‘’\n(12)\nwhere ğœ‹ğ‘’(ğ‘ ) is the expert policy network pre-trained with the BC method. In the actor-critic\nframework, the actor and critic form a training unit, with the critic evaluating the actorâ€™s actions to\nguide its updates. In the proposed learning framework, the critic evaluates both the learned policy\nğœ‹(ğ‘ ;ğœƒğ‘ğ‘ğ‘¡ğ‘œğ‘Ÿ) and the expert policy ğœ‹ğµğ¶as well. When the Q-value of the learned policy surpasses\nthat of the expert policy, it indicates that the learned policy has absorbed or even surpassed expert\nknowledge. At this point, reducing reliance on the expert policy and instead emphasizing the\nexploration and optimization of ğœ‹(ğ‘ ;ğœƒğ‘ğ‘ğ‘¡ğ‘œğ‘Ÿ) can encourage the agent to explore novel actions,\naiding in optimizing long-term performance and freeing the policy from being constrained by the\nstatic expert knowledge. In Eq.(12), the agent imitates the expert when the Q value of the expert\naction exceeds that of the learned policy, reflecting optimism toward expert data.\nThe proposed dogfight policy learning framework is outlined in Algorithm 1. First, the actor\nand critic networks are randomly initialized, and the target networks are synchronized. In lines\n7â€“8, samples are collected in the simulated environment and added to the replay buffer ğ·of the\n, Vol. 1, No. 1, Article . Publication date: June 2024.\nAn Imitative Reinforcement Learning Framework for Autonomous Dogfight\n11\nAlgorithm 1 Imitative Reinforcement Learning Framework for Dogfight\n1: Initialize the actor network with ğœƒğ‘ğ‘ğ‘¡ğ‘œğ‘Ÿand the critic networks with ğœ™1 and ğœ™2\n2: Initialize the target networks ğœ™â€²\n1 â†ğœ™1, ğœ™â€²\n2 â†ğœ™2, ğœƒâ€²\nğ‘ğ‘ğ‘¡ğ‘œğ‘Ÿâ†ğœƒğ‘ğ‘ğ‘¡ğ‘œğ‘Ÿ\n3: Initialize weight ğœ†, replay buffer ğ·, and expert trajectories T ğ‘’\n4: for ğ‘’= 1 to ğ‘€ğ‘ğ‘¥ğ¸ğ‘ğ‘–ğ‘ ğ‘œğ‘‘ğ‘’do\n5:\nInitialize the environment and get state ğ‘ \n6:\nfor ğ‘¡= 1 to ğ‘‡do\n7:\nğ‘â†ğœ‹(ğ‘ ;ğœƒğ‘ğ‘ğ‘¡ğ‘œğ‘Ÿ), and execute action ğ‘\n8:\nObtain reward ğ‘Ÿ, next state ğ‘ â€², and add this transition to ğ·\n9:\nUpdate the critic networks with Eq.(13)\n10:\nif ğ‘¡%2 == 0 then\n11:\nUpdate ğœ†in either linear or adaptive manner\n12:\nUpdate the actor network with Eq.(10)\n13:\nğœ™â€²\nğ‘–â†ğœğœ™ğ‘–+ (1 âˆ’ğœ)ğœ™â€²\nğ‘–|ğ‘–=1,2\n14:\nğœƒâ€²\nğ‘ğ‘ğ‘¡ğ‘œğ‘Ÿâ†ğœğœƒğ‘ğ‘ğ‘¡ğ‘œğ‘Ÿ+ (1 âˆ’ğœ)ğœƒâ€²\nğ‘ğ‘ğ‘¡ğ‘œğ‘Ÿ\n15:\nend if\n16:\nğ‘ â†ğ‘ â€²\n17:\nend for\n18: end for\nfirst-in-first-out store manner. In lines 9â€“12, samples are drawn randomly from the replay buffer ğ·,\nand the actor and critic networks are updated. The loss function for the critic network is as follows:\nğ¿ğ‘ğ‘Ÿğ‘–ğ‘¡ğ‘–ğ‘= ğ¸(ğ‘ ,ğ‘,ğ‘ â€²,ğ‘Ÿ)âˆ¼ğ·[(ğ‘Ÿ+ ğ‘šğ‘–ğ‘›ğ‘–=1,2ğ‘„ğœ™â€²\nğ‘–(ğ‘ â€², Ë†ğ‘) âˆ’ğ‘„ğœ™ğ‘–(ğ‘ ,ğ‘))2|ğ‘–=1,2],\n(13)\nwhere ğ‘„ğœ™â€²\nğ‘–represents the target Q networks, and Ë†ğ‘denotes the target action at the next state,\nË†ğ‘â†ğœ‹(ğ‘ â€²;ğœƒâ€²\nğ‘ğ‘ğ‘¡ğ‘œğ‘Ÿ). We employ the target networks to stabilize the learning process. As indicated in\nline 13-14 of Algorithm 1, the target networks are updated in a soft way with weight ğœ.\n6\nEXPERIMENTS\nWe evaluate the proposed learning framework in the constructed simulated combat environment to\nvalidate whether the proposed framework can learn successful dogfight policies. The experiments\nare designed to answer the following questions:\nâ€¢ How does the proposed learning framework perform compared with state-of-the-art RL and\nimitation learning approaches? (Section 6.1)\nâ€¢ Can the proposed framework learn effective dogfight policy? What is the learned policy like?\n(Section 6.2)\nâ€¢ What are the differences between the two modes (linear and adaptive ğœ†) in the proposed\nframework? (Section 6.2)\nâ€¢ How is the generalization ability of the proposed framework? (Section 6.3)\nâ€¢ How does the proposed learning framework perform using different kinds of expert data?\n(Section 6.4)\n6.1\nComparative Results\nWe compare the proposed learning framework with four state-of-the-art RL and imitation learning\nmethods:\nâ€¢ Behavior cloning (BC), which uses expert data to learn policies in a supervised manner.\n, Vol. 1, No. 1, Article . Publication date: June 2024.\n12\nSiyuan Li, et al.\nTable 1. Hyperparameters\nHyperparameter\nValue\nğ›¼\n1ğ‘’5\nğœ\n0.005\nğ‘‡\n6000\nMaxEpisode\n600\nLearning rates\n1ğ‘’âˆ’3\nSize of replay buffer ğ·\n1ğ‘’6\nBatch size\n128\nSizes of hidden layers\n512, 256\nTraining iterations of BC\n1.2ğ‘’7\nExpert buffer size of our method\n1.3ğ‘’4\nExpert buffer size of E-SAC\n1ğ‘’5\nâ€¢ Twin-delayed deep deterministic policy gradient (TD3) method [10], a RL method designed\nfor continuous action spaces, which improves the deep deterministic policy gradient (DDPG)\nalgorithm [21] with double Q value functions.\nâ€¢ Soft Actor-Critic (SAC) algorithm [12], a maximum entropy RL algorithm.\nâ€¢ Expert actor-based SAC approach (E-SAC) [19]: The E-SAC algorithm is an imitative RL\nalgorithm. By incorporating high-quality samples generated by experts into the training\nprocess of the SAC algorithm, the exploration efficiency and learning speed of the agent\nare significantly enhanced. Additionally, it can dynamically adjust the proportion of expert\nsamples to exploration samples, ensuring that the learned policy can reach the expert level.\nTable 1 lists the parameter settings used in the experiments. The actor and critic networks, besides\nthe input and output layers, consist of two hidden layers with the Leaky Relu activation function.\nExpert trajectories used by our method are collected in AI mode by 11 successful episodes, and\nit contains 13122 state-action pairs. In contrast, E-SAC method utilizes a larger dataset of expert\ntrajectories, which are collected by BC strategy, and it comprises a total of 100000 state-action\npairs. To enhance the quality of the data collected by BC strategy, we conduct a selection process,\nadopting only those trajectories that are successful and of high quality.\nThe experiment results are shown in Figure 5, which depict the success rates and the returns\nrespectively. Every 25 training episode, we evaluate the learned policies and calculate the average\nsuccess rate for 20 episodes. Each curve in Figure 5 represents the average values of four experiments\nfor the same method, with the shaded area indicating the 95% confidence interval.\nIn the highly realistic simulation environment constructed in this study, the agent has mastered\ntwo policies for shooting down opponent aircraft: (1) directly ramming the opponent aircraft and (2)\nhitting the opponent aircraft with a missile. Among them, using missiles to shoot down opponent\naircraft resulted in higher reward values, which is the correct decision-making we expect the agent\nto learn. Figure 5 shows the success rate of shooting down opponent aircraft using either of the\ntwo policies.\nAs shown in Figure 5, various methods employed can converge to high returns in the environment\nand task we constructed, demonstrating their applicability to reinforcement learning methods.\nAmong them, the adaptive weighting method proposed in this study achieved the highest convergent\nreturn, followed by the E-SAC method. Further comparison of the success rate curves between\nour method and the TD3 method reveals that the imitation of objectives can improve learning\n, Vol. 1, No. 1, Article . Publication date: June 2024.\nAn Imitative Reinforcement Learning Framework for Autonomous Dogfight\n13\nFig. 5. Comparative results of the proposed approach and the baselines in the dogfight task.\nTable 2. Success rates and returns for the best model during learning\nAlgorithm\nSuccess Rate\nHit success rate\nReturn\nOurs (adaptive)\n100.0% Â± 0.0%\n100.0% Â± 0.0%\nâˆ’680.8 Â± 6.7\nOurs (linear)\n100.0% Â± 0.0%\n100.0% Â± 0.0%\nâˆ’958.9 Â± 13.8\nTD3\n0.0% Â± 0.0%\n0.0% Â± 0.0%\nâˆ’4707.2 Â± 0.0\nE-SAC\n100.0% Â± 0.0%\n100.0% Â± 0.0%\nâˆ’1431.2 Â± 0.2\nSAC\n100.0% Â± 0.0%\n0.0% Â± 0.0%\nâˆ’2985.7 Â± 6.2\nBC\n62.8% Â± 1.0%\n62.8% Â± 1.0%\nâˆ’12228.3 Â± 880.2\nefficiency and success rates. This effect is significant when it is adopted in the form of adaptive\nweights. Moreover, compared to other methods, especially the E-SAC method, which also utilizes\nexpert knowledge, our method not only achieves a higher success rate but also exhibits superior\nstability. This superiority is primarily attributed to the imitated objectives, which directly optimize\nthe policy network and enable rapid improvement of the policy with small samples in a short\ntime, effectively restricting the agentâ€™s exploration space. This enables the agent to explore along\nthe correct path and gradually master key decision-making skills. On the other hand, the E-SAC\nmethod indirectly optimizes the policy and Q-value networks by incorporating expert knowledge\ninto the buffer. Although this approach also utilizes expert knowledge, it requires more expert input\nand results in lower sample efficiency. Our method minimizes the reliance on extensive expert\nknowledge while ensuring high stability alongside improving success rates by directly optimizing\nthe policy network. As the imitation learning method, BC, learns in an offline way, so it does not\nhave a learning curve, and therefore we report the convergent performance of BC in the following\ntable.\nTable 2 lists the success rates and returns of the best models among the four runs, with data\nformatted as â€œmean Â± standard deviationâ€, reflecting the average results and deviation for 50\nevaluation episodes. The table lists both the â€œsuccess rateâ€ achieved by shooting down opponent\naircraft by any means and the â€œhit success rateâ€ achieved by launching missiles to shoot down\nopponent aircraft. The code of the proposed approach, the learned models, and recorded videos of\nthe learned policies are available at https://github.com/zrc0622/HIRL4UCAV.\nAccording to Table 2, the adaptive weighting method proposed in this study demonstrates\nsuperior performance across three key indicators. Specifically, the TD3 method shows the lowest\nperformance in terms of the hit success rate, at 0%, among all methods. Conversely, the adaptive\nweighting method proposed in this study, along with the E-SAC and SAC methods, achieve a hit\n, Vol. 1, No. 1, Article . Publication date: June 2024.\n14\nSiyuan Li, et al.\nsuccess rate of 100%. However, although the SAC method achieves a success rate of 100%, its hit\nsuccess rate is 0%, indicating its failure to learn to correctly shoot down opponent aircraft with\nmissiles. Furthermore, the reward value of the SAC method is significantly lower than that of\nthe E-SAC method, which has mastered the correct method for shooting down enemies. Further\ncomparison between our method and the BC method reveals that, the BC method performs more\npoorly in terms of both success rate in task completion and reward value compared to our method\ndue to the lack of interaction with the environment and the inability to explore autonomously. It\nis noteworthy that although BC method has a higher success rate than TD3 method, the average\nreturns of BC method are significantly lower than those of TD3. This is also attributed to the\ninability to explore autonomously. When compounding errors occur [29, 30, 40], BC strategy tends\nto lose control and thus achieves very low returns, whereas TD3 methods can still closely track the\nopponent by pursuit strategy, thus preventing a rapid decline in returns.\n6.2\nAnalytical Results\nWe show weight ğœ†in the two modes (linear and adaptive) in Figure 6, and find that the value of\nğœ†is related to the learning performance. As shown in Figure 5(a), the linear weighting method\nproposed in this study demonstrates a faster learning rate in the initial stages of training compared\nto that of the adaptive weighting method. This is primarily attributed to the higher weight assigned\nto the imitation of objectives by the linear weighting method in the early stages, accelerating the\nlearning of expert knowledge. However, this approach increases the risk of overfitting and limits\nthe modelâ€™s generalization ability, resulting in instability and poor convergence performance in\ntraining processes.\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\nTimesteps\n1e6\n0.2\n0.4\n0.6\n0.8\n1.0\nOurs(linear)\nOurs(adaptive)\nFig. 6. The balancing factor between RL and imitation learning changes with timesteps.\nFigure 7 depicts the distance between our aircraft and the opponent in one episode, and the\ntrajectories are collected by the convergent models learned by the corresponding algorithms.\nSubfigures in Figure 7 annotate the moments of missile launch (red dots), phases of holding missiles\n(blue background), and stages of successfully locking onto targets (yellow background). If the first\nred dot on the curve falls within the intersection of the blue and yellow backgrounds, it indicates\nthat the agent has mastered an effective launch policy. Accordingly, it is observed that the method\nproposed, along with the E-SAC and BC methods, successfully executed missile launches and\naccurately shot down the opponent aircraft (i.e., mastering the three-dimensional policy of pursuit,\nlock, and launch). Further comparison of the speed of target lock and missile launch among these\nthree methods reveals that the proposed method achieves both with fewer steps, indicating higher\nattack efficiency. The distance curves of the TD3 and SAC methods exhibit a downward trend, and\n, Vol. 1, No. 1, Article . Publication date: June 2024.\nAn Imitative Reinforcement Learning Framework for Autonomous Dogfight\n15\nStep number\nStep number\nStep number\nStep number\nStep number\nStep number\nDistance with opponent\nDistance with opponent\n(b) Ours (linear)\n(c) TD3\n(e) SAC\n(a) Ours (adaptive)\n(d) E-SAC\n(f) BC\nHolding the missile\nLocking on the opponent\nLaunching the missile\nFig. 7. The distance between our aircraft and the opponent.\nthere are target locking zones; however, they fail to correctly launch missiles, demonstrating their\nmastery of only the two-dimensional policy of pursuit and lock. By comparing the data in Table 2,\nalthough the SAC method has mastered only a two-dimensional policy, it can successfully shoot\ndown the opponent aircraft through collision. In contrast, the TD3 approach fails to shoot down\nthe opponent aircraft. Therefore, the lock capability of SAC exceeds that of TD3.\nà¯¦à¯§à¯”à¯¥à¯§\nà¯˜à¯¡à¯—\n(b) Ours (linear)\n(c) TD3\n(e) SAC\n(a) Ours (adaptive)\n(d) E-SAC\n(f) BC\nFig. 8. The trajectories collected by the learned policy and the opponent.\n, Vol. 1, No. 1, Article . Publication date: June 2024.\n16\nSiyuan Li, et al.\nTable 3. Policy analysis\nAlgorithm\nPursuit\nLock\nLaunch\nOurs (adaptive)\nâœ“\nâœ“\nâœ“\nOurs (linear)\nâœ“\nâœ“\nâœ“\nTD3\nâœ“\nâœ“\nÃ—\nE-SAC\nâœ“\nâœ“\nâœ“\nSAC\nâœ“\nâœ“\nÃ—\nBC\nâœ“\nâœ“\nâœ“\nTable 4. Experiment results in the random initialization setting\nAlgorithm\nSuccess Rate\nHit success rate\nReturn\nOurs (adaptive)\n98.0% Â± 1.3%\n98.0% Â± 1.3%\nâˆ’1436.0 Â± 238.9\nOurs (linear)\n86.0% Â± 5.4%\n86.0% Â± 5.4%\nâˆ’5800.8 Â± 1420.3\nTD3\n0.0% Â± 0.0%\n0.0% Â± 0.0%\nâˆ’5720.9 Â± 715.8\nE-SAC\n90.0% Â± 2.8%\n90.0% Â± 2.8%\nâˆ’3722.2 Â± 395.5\nSAC\n44.0% Â± 3.3%\n0.0% Â± 0.0%\nâˆ’8318.1 Â± 822.8\nBC\n22.4% Â± 3.2%\n22.4% Â± 3.2%\nâˆ’20504.7 Â± 1156.3\nFigure 8 corresponds respectively to Figure 7, and Figure 8 shows the three-dimensional interac-\ntion scenarios between our aircraft and the opponent aircraft using three different methods. The\nred curves represent the movement trajectories of the opponent aircraft, while the blue curves\nrepresent the movement trajectories of our aircraft, with the colors gradually deepening over time.\nThe moments of launch are indicated by red dots on the blue trajectories. Figure 8(a) illustrates the\noperational effectiveness of the proposed learning framework, i.e., the controlled UCAV swiftly\ncatches up to the opponent aircraft, successfully locking onto it, launching a missile, and accurately\nhitting the target. By contrast, the UCAV with the TD3 policy (Figure 8(c)) can pursue the opponent\naircraft but keeps hovering around it, indicating insufficient lock and launch capabilities. In Figure\n7(e), the UCAV employing the SAC policy catches up to, locks onto, and rams the opponent aircraft\nby collision. Through the above analysis, we systematically summarize the policies mastered by\nvarious methods, which are presented in Table 3.\n6.3\nGeneralization Ability of the Learned Policy\nWe randomize the initial positions of the aircrafts to evaluate the generalization ability of the\nproposed learning framework. To investigate the generalization ability of the learned policy, we\nintroduce random offsets in the evaluation phase. The random perturbations range from 0 to 100\nmeters in the x, y, and z dimensions of the initial Cartesian coordinates of the aircraft to simulate\nuncertainty factors in real-world conditions. According to the experimental results in Table 4,\nour method has demonstrated outstanding performance, with the adaptive weighting method\nachieving a remarkable hit success rate of 98%. By contrast, although the linear weighting method\nalso performed well, its generalization performance was not as good as the E-SAC method.\nTo test the generalization ability and the launch efficiency, we further evaluate the models in the\nunlimited missile supply setting. Note that in the training phase, the aircraft only carry one missile.\nTable 5 shows the launch efficiency (calculated by the fraction between the hit number and the\nlaunch number) of the four methods that have mastered the missile launch policy in the unlimited\n, Vol. 1, No. 1, Article . Publication date: June 2024.\nAn Imitative Reinforcement Learning Framework for Autonomous Dogfight\n17\nTable 5. Launch efficiency in the unlimited missile setting\nAlgorithm\n#ğ»ğ‘–ğ‘¡ğ‘ \n#ğ¿ğ‘ğ‘¢ğ‘›ğ‘â„ğ‘’ğ‘ \nOurs (adaptive)\n100%\nOurs (linear)\n100%\nE-SAC\n11.4%\nBC\n92.3%\nmissile setting, and the methods that cannot effectively launch are omitted. The proposed learning\nframework has exhibited outstanding launch efficiency in this experiment, accurately launching\nmissiles regardless of the lock conditions, which demonstrates the effectiveness and reliability of\nthe learned launch policy.\n6.4\nAblation Study on Expert Data\nTo evaluate the impact of expert data quality on the learning performance of our framework,\nwe conduct ablation studies using two types of expert data: one collected in AI mode and the\nother collected by the BC policy. By using the expert data collected by the AI mode, the proposed\nframework can achieve an average return of âˆ’483.68, which is a little higher than that of employing\nthe BC data as the expert demonstration. The results are shown in Figure 9. The experiment results\ndemonstrate that the performance of using AI data and BC data only have a slight difference.\nThis phenomenon indicates that the proposed framework is robust to the quality of expert data.\nAlthough the lower quality of BC expert data results in instability during the training process and\nfluctuations in success rates, the highest success rates of both modes have reached 100%, which\nvalidates the effectiveness of the proposed framework.\nFig. 9. Ablation studies on expert data in the proposed framework.\n7\nCONCLUSION\nThis study focuses on the complex multi-dimensional decision-making problems in UCAV dogfight.\nLeveraging the Harfang3D sandbox, we construct a highly realistic simulation platform and propose\nan innovative high-imitative reinforcement learning framework for dogfight policy. This method\nintegrates expert experience with autonomous exploration mechanisms, achieving efficient and\nprecise policy learning. Experimental results demonstrate that the constructed simulation envi-\nronment serves as an effective platform for conducting reinforcement learning research in aerial\n, Vol. 1, No. 1, Article . Publication date: June 2024.\n18\nSiyuan Li, et al.\ncombat, enabling UCAVs to handle complex, multistage dogfight tasks, including target pursuit,\ntarget lock, and missile launch in simulated air combat missions. Policies learned with the proposed\nframework can achieve a hit success rate of up to 100.0%. Compared to RL and imitation learning\nalgorithms, the learned policies in this study demonstrate superior success rates and robustness\nwhile requiring less expert data.\nAlthough our framework effectively performs multistage tasks, it exhibits a certain dependence\non the quality of expert data, and its training outcomes are influenced by the reward design. In the\nfuture, we aim to explore a hierarchical policy to gather representative knowledge from limited\nexpert data, striving to reduce the reliance on expert data quality.\nACKNOWLEDGMENTS\nThis work is supported by the National Natural Science Foundation of China (Grant No.62306088),\nand Songjiang Lab (Grant No.SL20230309).\nREFERENCES\n[1] Herbert H Bell and Wayne L Waag. 2017. Evaluating the effectiveness of flight simulators for training combat skills: A\nreview. Simulation in Aviation Training (2017), 277â€“296.\n[2] Abder Rezak Benaskeur, Froduald Kabanza, and Eric Beaudry. 2010. CORALS: a real-time planner for anti-air defense\noperations. ACM Transactions on Intelligent Systems and Technology (TIST) 1, 2 (2010), 1â€“21.\n[3] George H Burgin and David M Eggleston. 1976. Design of an all-attitude flight control system to execute commanded\nbank angles and angles of attack. Technical Report. NASA.\n[4] George H Burgin, Lawrence J Fogel, and J Price Phelps. 1975. An adaptive maneuvering logic computer program for the\nsimulation of one-on-one air-to-air combat. volume 1: General description. Technical Report. NASA.\n[5] Yuan Cao, Ying-Xin Kou, Zhan-Wu Li, An Xu, et al. 2023. Autonomous maneuver decision of UCAV air combat based\non double deep Q network algorithm and stochastic game theory. International Journal of Aerospace Engineering 2023\n(2023).\n[6] Vinay Chamola, Pavan Kotesh, Aayush Agarwal, Navneet Gupta, Mohsen Guizani, et al. 2021. A comprehensive review\nof unmanned aerial vehicle attacks and neutralization techniques. Ad hoc networks 111 (2021), 102324.\n[7] Jinyoung Choi, Kyungsik Park, Minsu Kim, and Sangok Seok. 2019. Deep reinforcement learning of navigation in\na complex and crowded environment with a limited field of view. In 2019 International Conference on Robotics and\nAutomation (ICRA). IEEE, 5993â€“6000.\n[8] Zihao Fan, Yang Xu, Yuhang Kang, and Delin Luo. 2022. Air combat maneuver decision method based on A3C deep\nreinforcement learning. Machines 10, 11 (2022), 1033.\n[9] Xiaowei Fu, Jindong Zhu, Zhaoying Wei, Hui Wang, and Sili Li. 2022. A UAV pursuit-evasion strategy based on DDPG\nand imitation learning. International Journal of Aerospace Engineering 2022 (2022), 1â€“14.\n[10] Scott Fujimoto, Herke Hoof, and David Meger. 2018. Addressing function approximation error in actor-critic methods.\nIn International conference on machine learning. PMLR, 1587â€“1596.\n[11] FrÃ©dÃ©rick Garcia and Emmanuel Rachelson. 2013. Markov decision processes. Markov Decision Processes in Artificial\nIntelligence (2013), 1â€“38.\n[12] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. 2018. Soft actor-critic: Off-policy maximum entropy\ndeep reinforcement learning with a stochastic actor. In International conference on machine learning. PMLR, 1861â€“1870.\n[13] Shih-Hsi Hsu, Shao-Hung Chan, Ping-Tsang Wu, Kun Xiao, and Li-Chen Fu. 2018. Distributed deep reinforcement\nlearning based indoor visual navigation. In 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems\n(IROS). IEEE, 2532â€“2537.\n[14] Jinwen Hu, Luhe Wang, Tianmi Hu, Chubing Guo, and Yanxiong Wang. 2022. Autonomous maneuver decision making\nof dual-UAV cooperative air combat based on deep reinforcement learning. Electronics 11, 3 (2022), 467.\n[15] Gregory Kahn, Adam Villaflor, Bosen Ding, Pieter Abbeel, and Sergey Levine. 2018. Self-supervised deep reinforcement\nlearning with generalized computation graphs for robot navigation. In 2018 IEEE international conference on robotics\nand automation (ICRA). IEEE, 5129â€“5136.\n[16] H Kim, Michael Jordan, Shankar Sastry, and Andrew Ng. 2003. Autonomous helicopter flight via reinforcement\nlearning. Advances in neural information processing systems 16 (2003).\n[17] Vijay Konda and John Tsitsiklis. 1999. Actor-critic algorithms. Advances in neural information processing systems 12\n(1999).\n, Vol. 1, No. 1, Article . Publication date: June 2024.\nAn Imitative Reinforcement Learning Framework for Autonomous Dogfight\n19\n[18] Weiren Kong, Deyun Zhou, Zhen Yang, Yiyang Zhao, and Kai Zhang. 2020. UAV autonomous aerial combat maneuver\nstrategy generation with observation error based on state-adversarial deep deterministic policy gradient and inverse\nreinforcement learning. Electronics 9, 7 (2020), 1121.\n[19] Bo Li, Shuangxia Bai, Shiyang Liang, Rui Ma, Evgeny Neretin, and Jingyi Huang. 2023. Manoeuvre decision-making of\nunmanned aerial vehicles in air combat based on an expert actor-based soft actor critic algorithm. CAAI Transactions\non Intelligence Technology 8, 4 (2023), 1608â€“1619.\n[20] Bo Li, Shiyang Liang, Daqing Chen, and Xitong Li. 2022. A decision-making method for air combat maneuver based\non hybrid deep learning network. Chinese Journal of Electronics 31, 1 (2022), 107â€“115.\n[21] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan\nWierstra. 2015. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971 (2015).\n[22] Zhong Lin, Tong Mingâ€™an, Zhong Wei, and Zhang Shengyun. 2007. Sequential maneuvering decisions based on\nmulti-stage influence diagram in air combat. Journal of Systems Engineering and Electronics 18, 3 (2007), 551â€“555.\n[23] Ryan Lowe, Yi I Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. 2017. Multi-agent actor-critic\nfor mixed cooperative-competitive environments. Advances in neural information processing systems 30 (2017).\n[24] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver,\nand Koray Kavukcuoglu. 2016. Asynchronous methods for deep reinforcement learning. In International conference on\nmachine learning. PMLR, 1928â€“1937.\n[25] Muhammed Murat Ã–zbek, SÃ¼leyman YÄ±ldÄ±rÄ±m, Muhammet Aksoy, Eric Kernin, and Emre Koyuncu. 2022. Harfang3D\ndog-fight sandbox: a reinforcement learning research platform for the customized control tasks of fighter aircrafts.\narXiv preprint arXiv:2210.07282 (2022).\n[26] Hyunju Park, Byung-Yoon Lee, Min-Jea Tahk, and Dong-Wan Yoo. 2016. Differential game based air combat maneuver\ngeneration using scoring function matrix. International Journal of Aeronautical and Space Sciences 17, 2 (2016), 204â€“213.\n[27] Haiyin Piao, Shengqi Yang, Hechang Chen, Junnan Li, Jin Yu, Xuanqi Peng, Xin Yang, Zhen Yang, Zhixiao Sun,\nand Yi Chang. 2024. Discovering Expert-Level Air Combat Knowledge via Deep Excitatory-Inhibitory Factorized\nReinforcement Learning. ACM Transactions on Intelligent Systems and Technology (2024).\n[28] Nelson RamÃ­rez LÃ³pez and RafaÅ‚ Å»bikowski. 2018. Effectiveness of autonomous decision making for unmanned combat\naerial vehicles in dogfight engagements. Journal of Guidance, control, and Dynamics 41, 4 (2018), 1021â€“1024.\n[29] StÃ©phane Ross and Drew Bagnell. 2010. Efficient reductions for imitation learning. In Proceedings of the thirteenth\ninternational conference on artificial intelligence and statistics. JMLR Workshop and Conference Proceedings, 661â€“668.\n[30] StÃ©phane Ross, Geoffrey Gordon, and Drew Bagnell. 2011. A reduction of imitation learning and structured prediction\nto no-regret online learning. In Proceedings of the fourteenth international conference on artificial intelligence and\nstatistics. JMLR Workshop and Conference Proceedings, 627â€“635.\n[31] Wanying Ruan, Haibin Duan, and Yimin Deng. 2022. Autonomous maneuver decisions via transfer learning pigeon-\ninspired optimization for UCAVs in dogfight engagements. IEEE/CAA Journal of Automatica Sinica 9, 9 (2022),\n1639â€“1657.\n[32] Viktor SandstrÃ¶m, Linus Luotsinen, and Daniel Oskarsson. 2022. Fighter pilot behavior cloning. In 2022 International\nConference on Unmanned Aircraft Systems (ICUAS). IEEE, 686â€“695.\n[33] Richard S Sutton and Andrew G Barto. 2018. Reinforcement learning: An introduction. MIT press.\n[34] Robert B Trsek. 2007. The last manned fighter: Replacing manned fighters with ucavs. Air Command and Staff College,\nAir University (2007).\n[35] Hado Van Hasselt, Arthur Guez, and David Silver. 2016. Deep reinforcement learning with double q-learning. In\nProceedings of the AAAI conference on artificial intelligence, Vol. 30.\n[36] Magdalena C Vanâ€™t Wout, Shaun V Ball, and Rudolph Oosthuizen. 2019. A framework for implementing a data science\ncapability in a military intelligence system. International Command and Control Research & Technology Symposium.\n[37] Linxiang Wang and Hongtao Wei. 2022. Research on autonomous decision-making of UCAV based on deep reinforce-\nment learning. In 2022 3rd Information Communication Technologies Conference (ICTC). IEEE, 122â€“126.\n[38] Xuan Wang, WJ Wang, KP Song, and MW Wang. 2019. UAV air combat decision based on evolutionary expert system\ntree. Ordnance Industry Automation 38, 1 (2019), 42â€“47.\n[39] Fengguo Wu, wei Tao, hui Li, and Zhang Jianwei. 2023. Intelligent Avoidance Decision of Unmanned Aerial Vehicles\nBased on Deep Reinforcement Learning Algorithm. Systems Engineering and Electronic Technology 45, 6 (2023),\n1702â€“1711.\n[40] Tian Xu, Ziniu Li, and Yang Yu. 2020. Error bounds of imitating policies and environments. Advances in Neural\nInformation Processing Systems 33 (2020), 15737â€“15749.\n[41] Hongpeng Zhang and Changqiang Huang. 2020. Maneuver decision-making of deep learning for UCAV thorough\nazimuth angles. IEEE Access 8 (2020), 12976â€“12987.\n[42] Jiandong ZHANG, Dinghan WANG, Qiming YANG, Guoqing SHI, Yi LU, and ZHANG Yaozhong. 2023. Multi-\nDimensional Decision-Making for UAV Air Combat Based on Hierarchical Reinforcement Learning. Acta Armamentarii\n, Vol. 1, No. 1, Article . Publication date: June 2024.\n20\nSiyuan Li, et al.\n44, 6, Article 1547 (2023), 1547-1563 pages.\n[43] Yuke Zhu, Roozbeh Mottaghi, Eric Kolve, Joseph J Lim, Abhinav Gupta, Li Fei-Fei, and Ali Farhadi. 2017. Target-driven\nvisual navigation in indoor scenes using deep reinforcement learning. In 2017 IEEE international conference on robotics\nand automation (ICRA). IEEE, 3357â€“3364.\n, Vol. 1, No. 1, Article . Publication date: June 2024.\n",
  "categories": [
    "cs.LG",
    "cs.RO"
  ],
  "published": "2024-06-17",
  "updated": "2024-06-17"
}