{
  "id": "http://arxiv.org/abs/2406.13250v1",
  "title": "LangTopo: Aligning Language Descriptions of Graphs with Tokenized Topological Modeling",
  "authors": [
    "Zhong Guan",
    "Hongke Zhao",
    "Likang Wu",
    "Ming He",
    "Jianpin Fan"
  ],
  "abstract": "Recently, large language models (LLMs) have been widely researched in the\nfield of graph machine learning due to their outstanding abilities in language\ncomprehension and learning. However, the significant gap between natural\nlanguage tasks and topological structure modeling poses a nonnegligible\nchallenge. Specifically, since natural language descriptions are not sufficient\nfor LLMs to understand and process graph-structured data, fine-tuned LLMs\nperform even worse than some traditional GNN models on graph tasks, lacking\ninherent modeling capabilities for graph structures. Existing research overly\nemphasizes LLMs' understanding of semantic information captured by external\nmodels, while inadequately exploring graph topological structure modeling,\nthereby overlooking the genuine capabilities that LLMs lack. Consequently, in\nthis paper, we introduce a new framework, LangTopo, which aligns graph\nstructure modeling with natural language understanding at the token level.\nLangTopo quantifies the graph structure modeling capabilities of GNNs and LLMs\nby constructing a codebook for the graph modality and performs consistency\nmaximization. This process aligns the text description of LLM with the\ntopological modeling of GNN, allowing LLM to learn the ability of GNN to\ncapture graph structures, enabling LLM to handle graph-structured data\nindependently. We demonstrate the effectiveness of our proposed method on\nmultiple datasets.",
  "text": "LangTopo: Aligning Language Descriptions of Graphs\nwith Tokenized Topological Modeling\nZhong Guan\nTianjing University\nHongke Zhao\nTianjing University\nLikang Wu\nUniversity of Science and Technology of China\nMing He\nAI Lab at Lenovo Research\nJianpin Fan\nAI Lab at Lenovo Research\nAbstract\nRecently, large language models (LLMs) have been widely researched in the field\nof graph machine learning due to their outstanding abilities in language compre-\nhension and learning. However, the significant gap between natural language tasks\nand topological structure modeling poses a nonnegligible challenge. Specifically,\nsince natural language descriptions are not sufficient for LLMs to understand and\nprocess graph-structured data, fine-tuned LLMs perform even worse than some\ntraditional GNN models on graph tasks, lacking inherent modeling capabilities for\ngraph structures. Existing research overly emphasizes LLMs’ understanding of\nsemantic information captured by external models, while inadequately exploring\ngraph topological structure modeling, thereby overlooking the genuine capabilities\nthat LLMs lack. Consequently, in this paper, we introduce a new framework, Lang-\nTopo, which aligns graph structure modeling with natural language understanding\nat the token level. LangTopo quantifies the graph structure modeling capabilities of\nGNNs and LLMs by constructing a codebook for the graph modality and performs\nconsistency maximization. This process aligns the text description of LLM with the\ntopological modeling of GNN, allowing LLM to learn the ability of GNN to capture\ngraph structures, enabling LLM to handle graph-structured data independently. We\ndemonstrate the effectiveness of our proposed method on multiple datasets.\n1\nINTRODUCTION\nText-attributed graphs (TAGs) are ubiquitous in the real world Berge (2001), appearing in academic\ncitation networks Wang et al. (2020), web pages Mernyei and Cangea (2020), e-commerce plat-\nforms Shchur et al. (2018), and search services. These graphs are characterized by nodes that contain\nrich textual attributes, making them complex and challenging to analyze. Research on TAGs has be-\ncome a significant area within graph machine learning, primarily focusing on Graph Neural Networks\n(GNNs) based on message-passing mechanisms to exploit adjacency spaces effectively Defferrard\net al. (2016); Kipf and Welling (2016); Veliˇckovi´c et al. (2017).\nRecently, the advent of large language models (LLMs) like ChatGPT OpenAI (2023) and Llama Tou-\nvron et al. (2023) has sparked significant interest in their application to graph machine learning tasks\ndue to their impressive potential across various fields Singh et al. (2023); Driess et al. (2023). Some\nstudies have explored enhancing node embeddings Duan et al. (2023); He et al. (2023) and reinforcing\ngraph topological structures using LLMs Guo et al. (2024); Wang et al. (2024). However, unlike\nPreprint. Under review.\narXiv:2406.13250v1  [cs.AI]  19 Jun 2024\nAdapter/Proj\nDescription\n1-hop:            \n2-hop:\nLLM\nDescription\n1-hop:            \n2-hop:\nLLM\nDescription\n1-hop:            \n2-hop:\nLLM\nGNN\nTopo-modeling\n(a) Prompt \n(b) Access external\n(c)Our LangTopo\nIt can be decomposed \ninto four smaller \ngraphs, and based on \nthese four graphs, we \ncan .......\nFigure 1: Prompt: LLMs make predictions based solely on natural language descriptions. Access\nexternal: LLMs leverage external models (typically GNNs) to extract information for enhanced\npredictions. Ours LangTopo: Aligning the textual descriptive power of LLMs with the topological\nmodeling capabilities of GNNs in terms of model processing and operation.\nGNNs, which are based on message passing, LLMs grounded in natural language often struggle\nto describe and process node connections, leading to suboptimal performance on graph-related\ntasks Chen et al. (2024a); Huang et al. (2023).\nCombining the structural modeling capacity of GNNs with the text processing capability of LLMs\npresents a promising approach to addressing these challenges. A straightforward solution involves\nusing an external GNN to extract spatial structure embeddings, followed by training a projection\nlayer or adapter to inject these embeddings into the LLM, as illustrated in Figure 1(b). Recent\napproaches have enhanced LLM performance in graph tasks through such methods Chen et al.\n(2024b); Huang et al. (2024); Zhang et al. (2024). However, LLMs still lack the ability to handle\ngraph data independently and continue to rely on external models during inference. Furthermore,\naligning the embedding spaces is redundant in the context of text-attributed graphs because GNNs’\nembedding spaces already encapsulate natural language information. The fundamental issue is that\nLLMs lack the capability to model graph structures. Consequently, we posit that the crux of the\nproblem is modeling, not embedding.\nTo address this issue, we propose LangTopo, a novel learning paradigm that aligns language descrip-\ntions of graphs with tokenized topological modeling from a view of consistent modeling methods\nfor text and structure. Initially, we employ a graph neural network to introduce a modality-specific\ncodebook for graphs, capable of quantizing both textual and spatial information of nodes. By in-\ncorporating Gumbel-softmax Jang et al. (2016), we transform discrete quantization into continuous,\ndifferentiable quantization through relaxation techniques. Through this process, we efficiently extract\nthe modeling prowess of GNNs into two components: Relaxed Distribution, indicating the allocation\npatterns of subgraph structures relative to the codebook, and Quantized Embeddings, representing\nprecise subgraph structures with respect to the codebook. Subsequently, the LLM describes the graph\nstructure in natural language and obtains these two components based on the codebook embeddings.\nDriven by the goal of achieving similarity between the corresponding components produced by the\nLLM and GNN, LangTopo aligns the LLM’s natural language description of the graph structure with\nthe graph structure modeled by GNN, ultimately enhancing the LLM’s aptitude for modeling spatial\narchitectures.\nThe main contributions of this work are summarized below:\n• We propose LangTopo, a novel framework for learning graph structures using LLMs. Through\nthe adoption of Vector Quantized-Variational Autoencoder (VQ-VAE), we quantify the modeling\n2\ncapabilities of LLMs and GNNs for graph topological structures and enable LLMs to learn GNNs’\nability to model graph structures through supervised learning.\n• We achieve alignment between the natural language descriptive text in LLMs and the processing\nand operation of GNN models by constructing a codebook for the graph data modality and thereby\nensuring that the quantized embeddings and relaxation distributions are similar between LLMs and\nGNNs.\n• Unlike existing paradigms that usually introduce external modules to recognize graph structures,\nLangTopo endows the LLM itself with the ability to model graph structures, obviating the need\nfor external data or model integration during inference. LangTopo demonstrates excellent results\nacross multiple datasets.\n2\nRELARED WORK\nLLM for Graph. Recent studies have explored the application of Large Language Models (LLMs)\nin the field of graph structures Wu et al. (2024). GLEM Zhao et al. (2022) and others Yang et al.\n(2021); Li et al. (2021b) have investigated the joint training of LLMs and graph neural networks\n(GNNs). TAPE He et al. (2023) utilizes LLMs to predict the ranking classification of nodes and\nprovides detailed explanations to enhance the embedding quality of GNNs. Sun et al.Sun et al. (2023)\nleverage LLMs for pseudo-labeling generation to improve the topological structure information of\ngraphs. Furthermore, more research focuses on enhancing LLMs’ direct processing of text graphs.\nInstructGLM Ye et al. (2023) first uses LLM-based instruction tuning to describe the structure\nof graphs and node features, and solve graph tasks. GraphText Zhao et al. (2023) introduced\na generic graph reasoning framework for in-context learning and directed fine-tuning of graph\nstructures. Despite these advancements, LLMs struggle with structured data translated into natural\nlanguage, often yielding suboptimal results. To overcome this challenge, LLaGA Chen et al. (2024b)\nreformulates node link information as sequential data, applying instruction tuning that LLMs can\ncomprehend while maintaining structural node information. UniGraph He and Hooi (2024) adopts a\nmasked strategy for joint LLM and GNN training under co-training, achieving generalization across\nvarious graphs and datasets. GraphAdapter Huang et al. (2024) employs a GNN model as an adapter,\ncollaborates with LLMs for TAG tasks, and facilitates task-specific fine-tuning through external\naccess. Nevertheless, these methods do not empower LLMs with the ability to improve themselves,\nand LLMs still lack the knowledge and capabilities to handle graph data structures.\nGraph neural network.\nGraph Neural Networks (GNNs), standing at the forefront of graph\nmachine learning, have solidified their position as paramount instruments in graph-based learning\nscenarios. Their prowess lies in effectively modeling complex relational structures within graphs.\nPioneering work was conducted by Kipf et al Kipf and Welling (2016). with the development of\nGraph Convolutional Networks (GCNs), ingeniously aggregating features from one-hop neighbors\nfor each node. Following suit, the advent of GAT Veliˇckovi´c et al. (2017) and GraphSAGE Defferrard\net al. (2016) marked significant success in tackling graph learning challenges, with a common strategy\nof using a message passing mechanism. Based on this, research has further ventured into addressing\nissues of heterophily Luan et al. (2022); Zhu et al. (2021); Li et al. (2022); Ma et al. (2021); Wu et al.\n(2023) in graph learning tasks and the oversmoothing phenomenon in graph convolutions Li et al.\n(2021a); Liu et al. (2020); Chen et al. (2020); Giraldo et al. (2023).\n3\nPRELIMINARIES\nVector Quantised-Variational AutoEncoder (VQ-VAE). VQ-VAE Van Den Oord et al. (2017)\nis a vector quantizer that generates discrete codes by applying quantization to encoded continuous\nvariables and is capable of reconstructing the original data based on the latent variables corresponding\nto the discrete codes. It is trained jointly by updating the quantization codebook as well as the encoder\nand decoder. Formally, VQ-VAE first encodes the input x via encoder into ze(x). Then, based on the\nnearest neighbor lookup between ze(x) and the codebook ei, i = 1, 2, ..., K, the potential variable\nzq(x)is found (zq(x) = ei, where i = argminj||ze(x) −ej||). The potential variable zq(x) is then\npassed to the decoder, which attempts to reconstruct the input x. VQ-VAE, like the VAE, optimizes\nthe Evidence Lower Bound (ELBO) Kingma and Welling (2013).\nL(θ, ϕ; x) = −DKL(qϕ(z|x)||pθ(z)) + Eqϕ(z|x)[log pθ(x|z)]\n(1)\n3\n0\nEdge\nReconstruction\nGNN_node\nGNN_edge\n...\nQuantize the\nmodeling ability\nStage1 Quantifying modeling capability  \nQuantized\nEmbedding\nCodeBook\nRelated\nDistribution \nGumbel Softmax\nFeature\nReconstruction\nDecoder\nDecoder\n  Stage2   Aligning modeling capability\nDescription\n1-hop:            \n2-hop:\nLLM\nCodeBook\n?\n?\n?\nText\nsubgraph 1, subgraph \n7 , subgraph  7. \nsubgraph K\nFigure 2: The model architecture of our proposed LangTopo framework for graph structure learning.\nAt the same time, some modifications were made, and the loss function is as follows:\nL(x) = Lrecon + ||sg[ze(x)] −zq(x)||2\n2 + β · ||sg[zq(x)] −ze(x)||2\n2\n(2)\nwherein, Lreconis the reconstruction loss function, ||sg[ze(x)] −zq(x)||2\n2is the codebook loss, used\nonly for updating the codebook to make the selected zq close to the encoder’s output ze, where sg is\nthe stop-gradient operation. β · ||sg[zq(x)] −ze(x)||2\n2is the commitment loss, applicable only to the\nencoder weights, which encourages the encoder’s output to stay close to the selected code, preventing\nit from fluctuating too frequently between code vectors. Additionally, since the prior distribution is\nuniform , and each codeword in qϕ(z|x) is a discrete integer, the KL divergence is constant.\nText-Attributed Graph(TAG). We define a TAG as a graph where each node has corresponding\ntext. Define TAG as G = (V, A, X) with nodes V and adjacency matrix A ∈R|V |×|V |, and node\nattributes as the |V | × d feature matrix X. In this paper, our research objective is node classification\nin TAG. Given a few labeled nodes yL of L ⊆V , the goal is to predict the labels yU for the remaining\nunlabeled objects U = V \\ L.\n4\nMETHODOLOGY\nOur framework consists of two parts:\n(i) Quantization of Modeling Ability. By training the codebook based on the textual and spatial\ninformation of the TAG, we obtain quantized embeddings and relaxed distributions, alongside a\ncodebook that encapsulates diverse graph structural information. These quantized embeddings and\nrelaxed distributions represent the model’s capability to model graph structures.\n(ii) LLM’s graph structure learning. Train LLM adapted for graph tasks and evaluate its graph struc-\ntural modeling ability using established codebook. Further, improve the graph structural modeling\ncapability through alignment with GNN’s quantized embeddings and relaxed distributions.\n4.1\nQuantization of GNN Modeling Ability\nIn this section, we will discuss the training of a codebook that accounts for both node text and\nspatial structure using VQ-VAE. Through the graph modality-specific codebook, we obtain quantized\nembeddings and relaxed distributions that represent the GNN’s ability to model graph structure. As\nillustrated in Stage 1 of Figure 2\n4\n4.1.1 Quantization of Text and Spatial Structure.\nFirst, we extract node embeddings h(l)\nnode\nand h(l)\nedge by multi-layer aggregation through two GNN models. Then, according to the specific\nquantization selection method, we obtain a codeword znode, zedge from E = e1, e2, ..., eK. This can\nbe described as follows:\nh(l)\nnode = σ(AGGϕ(MSGϕ(h(l−1)\nNB(node), A), h(l)\nedge = σ(AGGθ(MSGθ(h(l−1)\nNB(edge), A)\n(3)\nznode = Lookup(E, hnode), zedge = Lookup(E, hedge)\n(4)\nHere, θ and ϕ respectively denote the GNN models focusing on node text information and spatial\nstructure, while \"Lookup\" represents the method for selecting discretized codewords from the\ncodebook. In the context of VQ-VAE, the embedding of the codebook is obtained through a nearest\nneighbor search. To enhance the representativeness of the quantized vectors, Section 4.1.2 introduces\nthe utilization of the Gumbel-softmax relaxation technique for acquiring codebook vectors.\nSubsequently, the codebook vectors znode and zedge are respectively passed through a simple linear\nprojection layer (pω : z−> ˆvi) for node feature reconstruction and adjacency matrix reconstruction.\n4.1.2 Gumbel softmax related disturbtion. Gumbel Softmax relaxation is a technique for handling\ndiscrete variables by allowing us to approximate the probability distribution of a discrete variable\nusing a continuous, differentiable function. The discretization step in VQ-VAE, where a single\ncodeword is selected, inherently neglects the structural information encapsulated by other codewords\nin the codebook, thereby limiting the comprehensive assessment of a GNN’s capacity to model\nintricate graphs. This shortfall motivates our choice of employing the Gumbel-Softmax relaxation\ntechnique. Formally, it can be expressed as:\ngi = −log (−log(u)) , u ∼Uniform(0, 1)\n(5)\npi =\nexp ((log(πi) + gi)/τ)\nPK\nj=1 exp ((log(πj) + gj)τ)\n,\ni = 1, 2, ..., K\n(6)\nz =\nk\nX\nj=1\npjzj, j = 1, 2, ..., k\n(7)\nWherein, gi denotes a sample from the Gumbel distribution, pi represents the probability of selecting\neach codeword, and z signifies the quantized embedding. Following the application of Gumbel-\nSoftmax, we obtain the quantized embedding Z and the relaxed distribution P, which effectively\nrepresent the model’s modeling of the graph modality. The temperature coefficient τ significantly\ngoverns the relaxation level of the distribution: higher values of τ promote a distribution closer to\nuniformity, whereas diminished values steer the distribution towards a sharper, nearly one-hot form.\nIn our experiments, we adopt an annealing strategy to gradually decrease the value of τ over time.\nWe demonstrate in Appendix G that after using Gumbel softmax, the new random variable z is the\nsame as the random variable π, that is, the probability of taking z is the same as the probability of\ntaking π.\n4.1.3 Optimization. Similar to VAE, we also optimize the ELBO(Eq 1), with the loss function as\nfollows:\nLStage1 = Eqϕ(z|x)[log pθ(x|z)] −DKL(qϕ(z|x)||pθ(z))\n= Ledge_recon + αnodeLnode_recon + βKLKL(qϕ(z|x)||pθ(z))\n(8)\nLnode_recon = 1\nN\nN\nX\ni=1\n(1 −\nvT\ni ˆvi\n∥vi∥· ∥ˆvi∥)γ\n|\n{z\n}\nnode reconstruction\n, γ > 1\n(9)\nLedge_recon = ∥A −σ(X · XT )∥2\n2\n|\n{z\n}\nedge reconstruction\n(10)\nWherein, Lnode_reconis the reconstruction loss for node features, and the reconstruction loss is\nmeasured using the cosine error. Furthermore, a scalable variant, the Scaled Cosine Error γ, is\nintroduced to further refine the cosine error metric. Ledge_reconis the reconstruction loss for edges.\nAdditionally, the term KL(qϕ(z|x)||pθ(z)) embodies the Kullback-Leibler divergence, which serves\n5\nto encourage the posterior distribution of codeword selections,qϕ(z|x), to closely align with the prior\ndistribution pθ(z), which is uniform.\n4.1.4 Other Methods for Selecting Vectors. A straightforward approach for selecting codebook\nvectors involves employing the argmax operation for nearest neighbor search. We conducted com-\nparative experiments in Section 5.2 and found that using Gumbel Softmax has better performance.\nAdditionally, we juxtaposed it with Gumbel-Argmax; however, both methods were found to inade-\nquately capture a broader spectrum of graph structural variations, thereby underlining the advantage\nof the Gumbel-Softmax approach in retaining richer structural representations.\n4.2\nLLM’s graph structure learning\nAfter Stage 1 , we acquire the quantized embeddings and relaxed distributions representing both node\ntext and spatial information. Next, we utilize the same approach to obtain the two components of the\nLLM and harmonize the GNN’s modeling capacity in an aligned manner. As Figure 2 Stage2\n4.2.1 Alignment of Graph Structure Modeling Ability. We input the nodes’ linkage and textual\ninformation into the LLM and fine-tune it for the specific main task. Concurrently, we derive the\nquantized embeddings and relaxed distributions of the LLM utilizing the trained codebook according\nto Equations 567. Subsequently, we perform alignment of the two components separately.\nFormally, for each node, we obtain the embeddings of the last layer of the LLM, hllm, and the\nembedding extracted by the GNN, hgnn. Then, through Gumbel Softmax relaxation processing, we\nobtain the weights for different codewords, pllm and pgnn, and the quantized embeddings, zllm and\nzgnn .\nzllm, pllm = Gumbel_softmax(hllm)\nzgnn, pgnn = Gumbel_softmax(hgnn)\n(11)\nThe overall training loss of the LLM is\nLStage2 = LCE + αmseLMSE(zllm, zgnn) + βklKL(pllm||pgnn)\n(12)\nWherein, LCE is the loss function for node classification of the LLM, LMSE and KL divergence are\nthe loss functions for measuring the LLM and GNN in terms of quantized embeddings and relaxed\ndistributions. αmse and βkl are the corresponding hyperparameters.\n4.2.2 LLM inference. During the inference stage, our model diverges from existing approaches by\ndispensing with the necessity of employing GNNs for extracting structural information or integrating\nsupplementary external data. It is capable of performing robust inference tasks using textual infor-\nmation and link information alone. We successfully develop a codebook tailored to the graph data\nmodality, thereby achieving alignment between the textual description of the graph and its topological\nstructure in the LLM’s processing and execution.\n4.3\nTheoretical analysis\nOur objective centers on learning the optimal quantized embedding value, z∗\nllm, at the LLM end.\nBoth zllm and zgnn (the quantized embedding at GNN end) are representations derived from graph\nstructural information, which is informed by prior knowledge y. Specifically, we aim to learn z∗\nllm\nby maximizing the conditional probability framework, refining these embeddings to encapsulate the\nstructural essence conveyed by the graph under the guidance of y.\ne∗= arg maxEp(zllm,zgnn)[p(y, zgnn|zllm)]\n(13)\nTheorem.\nMaximizing the posterior probability given prior information y, represented as\nEp(zllm, zgnn)[p(y, zgnn|zllm)], is equivalent to maximizing the mutual information I(zllm; zgnn)\nbetween the quantized embeddings zllm from the LLM side and zgnn from the GNN side.\nProof. Notably, the training process unfolds in stages, wherein the quantized embedding zgnn from\nthe GNN end is treated as static while optimizing the quantized embedding zllm at the LLM end.\n6\nType\nModel\nArxiv\nPubmed\nCora\nArxiv2023\nAverage\nShallow embedding\nGCN\n0.7182\n0.8031\n0.8778\n0.6760\n0.7687\nGraphSage\n0.7171\n0.8881\n0.8824\n0.6906\n0.7945\nGAT\n0.7366\n0.8328\n0.8595\n0.6784\n0.7768\nRevGAT\n0.7402\n0.8850\n0.8911\n0.6979\n0.8035\nDRGAT\n0.7416\n0.8962\n0.8977\n0.7003\n0.8089\nRaw text(title)\nUniGraph\n0.7291\n0.7433\n0.8184\n—\n–\nInstructGLM\n0.7297\n0.9105\n0.8977\n0.7651\n0.8258\nGraphMAE2\n0.7201\n0.6983\n0.8011\n0.7163\n0.7340\nLangTopo\n0.7365\n0.9287\n0.8998\n0.7738\n0.8347\nGNN transformer\nGraphformer\n0.6725\n0.7699\n0.8044\n0.6287\n0.7188\nNodeformer\n0.6960\n0.7958\n0.8848\n0.6744\n0.7627\nLLM enhance GNN\nGLEM\n0.7580\n0.9459\n0.8856\n0.7858\n0.8438\nGraphEdit\n0.7578\n0.9409\n0.9090\n0.7966\n0.8510\nEmbedding by LLM\nLLaGA\n0.7666\n0.9503\n0.8922\n0.8037\n0.8532\nENGINE\n0.7602\n0.9477\n0.9148\n0.7976\n0.8550\nLangTopo\n0.7681\n0.9667\n0.9158\n0.8126\n0.8658\nTable 1: Main Results. Experiments were conducted comparing four categories of baseline models.\nThe evaluation metric used was accuracy, with top-performing results emphasized in bold.\nThis sequential approach permits us to derive the following insights:\nEp(zllm,zgnn)[p(y, zgnn|zllm)] ∝Ep(zllm,zgnn) log\nZ\nZ\np(y, zgnn|zllm)\np(zgnn)\ndz\n= Ep(zllm,zgnn) log\nR\nZ py, zllm|zgnn)dz\np(zllm)\n= Ep(zllm,zgnn) log p(zllm|zgnn)\np(zllm)\n= I(zllm, zgnn).\n(14)\n5\nEXPERIMENTS\nDataset Settings. We evaluate the proposed model using several widely used public benchmark\ndatasets(Cora, Pubmed), and one OGB dataset(Arxiv, Arxiv-2023) Hu et al. (2020). And Arxiv-2023\nis employed to avoid data leakage issues. Details can be found in Appendix D.\nBaselines.\nTo evaluate the effectiveness of our proposed method, we compare it with several\nbaselines from four categories. (1) Traditional GNN models including GCN Kipf and Welling (2016),\nGAT Veliˇckovi´c et al. (2017), GraphSage Defferrard et al. (2016), (2) Graph Transformers models\nsuch as Graphormers Yang et al. (2021) and NodeFormer Wu et al. (2022), (3) LM+GNN methods\nincluding GLEM Zhao et al. (2022) and GraphEdit Guo et al. (2024), (4) LLM-based methods,\nwhich are further divided into two subcategories: raw text input (UniGraph He and Hooi (2024),\nInstructGLM Ye et al. (2023), GIANT-XRT+GraphMAE2 Hou et al. (2023)) and embedding input\n(LLaGA Chen et al. (2024b), GraphAdapter Huang et al. (2024), ENGINE Zhu et al. (2024)). Details\nof these methods are in Appendix A.\nImplementation Details. For fair comparison, we adopt GraphSAGE as the GNN model and\nshallow embedding. We employ the Llama-2-7B model as the base model. In the embedding input\ncategory of LLM-based methods, we use TAPE. Additionally, we compare different embeddings in\nthe Appendix B. In terms of codebook training, we adjust the hyperparameters such as codebook\ndimension and size based on different datasets, with more details provided in the Appendix E. The\ncomputing resource is NVIDIA RTX A800 80G.\n7\nFigure 3: The distribution of codebook embeddings with different strategies on the unit hypersphere.\n5.1\nMain Result\nWe evaluate LangTopo against several baseline models on three public datasets, and the results are\nsummarized in Table 1. It is evident that LangTopo demonstrates remarkable performance across\nall datasets. Whether employing raw text as input or utilizing preprocessed embeddings, LangTopo\nconsistently outperforms other models with LLM integration. Furthermore, we observe that using\nraw text directly yields inferior outcomes compared to using text embeddings. We attribute this\ndiscrepancy to the potential challenge LLMs face in handling lengthy texts, as raw texts tend to be\nextensive.\n5.2\nCodebook Analysis\nIn this subsection, we conduct a detailed analysis and comparison of the performance and effectiveness\nof the codebook.\nDifferent Methods for selecting Vectors. Our experiments employed Gumbel softmax relaxation\nfor the calculation of quantized embeddings. Additionally, we compared the selection of quantized\nembeddings using Argmin cosine, euclidean distance, and Gumbel argmax.\nTable 2 showcases the evaluation of codebooks constructed via differing strategies, revealing that\nGumbel-softmax relaxation notably excels in creating a codebook with a high utilization rate. This\ncharacteristic is particularly advantageous for accurately modeling intricate graph datasets. In\ncomparison, using argmax Euc yields the worst results, while the effects of cosine distance and\nGumbel argmax are similar. In Figure 3, we visualized the distribution of codebook embeddings with\ndifferent strategies on the unit hypersphere, further demonstrating the effectiveness of the Gumbel\nsoftmax relaxation. Its embedding distribution is more uniform and exhibits better generalization\nperformance.\nTable 2: code perplexity and usage rate.Code perplexity represents the model’s average usage of the\ncodebook, while the usage rate indicates the model’s utilization of the codebook. Generally, higher\nvalues are preferable.\nMethods\nGumbel softmax\nGumbel argmax\nargmax Cos\nargmax Euc\n#perplexity\n2263\n1230\n1168\n778\n#usage\n0.7324\n0.493\n0.501\n0.323\n5.3\nAblation Study\nLoss Ablation. To delve deeper into understanding the influence of different loss functions on\ncodebook construction and enhance the graph structural modeling capability of LLMs, we conducted\nan ablation study on the arxiv dataset, as shown in Figure 4 .\nAs evident from the table, each loss function in the LangTopo structure plays a crucial role, and we\nobserve a positive correlation between the improved codebook utilization by LLMs and the accuracy\nin LLM node classification, further validating the correctness of the design direction of the LangTopo\narchitecture.\n8\nFigure 4: Our investigation into diverse loss functions within the LangTopy architecture has substan-\ntiated the importance and efficacy of individual loss functions. The left figure examines the efficacy\nof node reconstruction and edge reconstruction loss functions, while the right figure delves into the\nimportance of relaxed distributions and quantized embeddings in the learning process of LLMs.\nEffect of different Hop. We conducted an ablation study to investigate the contributions of our\nvarying hop ranges to LangTopo’s performance. To this end, we examined the impact of different\nhop ranges – 0-hop, 1-hop, and 2-hop – on LangTopo’s efficacy across three datasets, as well as the\ncontribution of these different hop ranges to LLMs’ graph task-solving abilities without the LangTopo\nframework. The results, visible in Table 3, lead us to the following conclusions:\n(i) Compared to scenarios without LangTopo, the use of LangTopo yields superior performance in\nboth 1-hop and 2-hop settings. However, in the 0-hop configuration, LangTopo does not exhibit any\ndifference in effect, which aligns with expectations. Because LangTopo cannot model graph structure\neffectively without structural information, as it is inherently designed to leverage such details.\n(ii) LangTopo achieves its best performance in the 2-hop setting, whereas the without LangTopo only\nattains optimal results in the 1-hop scenario. These experiments further substantiate that LangTopo\nenhances the LLM’s capability to model graph structures.\nMethods\nhop\nARXIV\nPubmed\nCora\nw/o LangTopo\n0-hop\n73.88\n94.09\n85.64\n1-hop\n76.01\n94.60\n89.31\n2-hop\n75.78\n94.01\n89.12\nw LangTopo\n0-hop\n73.85\n94.17\n85.49\n1-hop\n76.64\n96.53\n91.45\n2-hop\n76.81\n96.67\n91.58\nTable 3: Effect comparison under different hop counts between with LangTopo and without LangTopo.\nEffect of different LLM. In our main experiment, Llama-2-7B serves as the foundational model. We\nextend our study beyond Llama-2-7B, considering alternative models such as Vicuna-7B Chiang et al.\n(2023), and OPT-2.7B Zhang et al. (2022). The results after substituting these models are presented\nin Table 9\nTable 4: Performance comparison using different LLM.\nLLM\nArxiv\nPubmed\nCora\nOPT-2.7B\n0.7677\n0.9629\n0.9184\nvicuna-7B\n0.7679\n0.9654\n0.9177\nLlama-2-7B\n0.7681\n0.9667\n0.9158\n9\nOthers. We assess the impact of different embeddings in Appendix B, examine the influence of\nselecting different GNNs on model performance in Appendix F, provide details on prompt design in\nAppendix H, and investigate the differing outcomes of rich versus minimal text in Appendix C.\n6\nCONCLUSION\nWe present an innovative graph structural learning framework, LangTopo, tailored for LLMs. This\nframework harnesses an approach to quantify the graph modeling capacities of GNNs and LLMs,\nthereby achieving a substantial enhancement in LLMs’ comprehension of graph topologies. Moreover,\nLangTopo transcends the limitations of prevailing paradigms that necessitate external models to\napprehend graph structures during the inference phase. Standing as a pioneering work, LangTopo is\npoised to furnish a robust benchmark for guiding future advancements within this domain.\nLimitation. A limitation in our experimental setup is the unexplored scenario of jointly training with\nmultiple datasets for graph modality codebooks. Presently, our evaluations are confined to individual\ndatasets. In future, we intend to investigate the generalizability and scalability of LangTopo.\n10\nReferences\nClaude Berge. 2001. The theory of graphs. Courier Corporation.\nDeli Chen, Yankai Lin, Wei Li, Peng Li, Jie Zhou, and Xu Sun. 2020. Measuring and relieving the\nover-smoothing problem for graph neural networks from the topological view. In Proceedings of\nthe AAAI conference on artificial intelligence, Vol. 34. 3438–3445.\nRunjin Chen, Tong Zhao, Ajay Jaiswal, Neil Shah, and Zhangyang Wang. 2024b. LLaGA: Large\nLanguage and Graph Assistant. arXiv preprint arXiv:2402.08170 (2024).\nZhikai Chen, Haitao Mao, Hang Li, Wei Jin, Hongzhi Wen, Xiaochi Wei, Shuaiqiang Wang, Dawei\nYin, Wenqi Fan, Hui Liu, et al. 2024a. Exploring the potential of large language models (llms) in\nlearning on graphs. ACM SIGKDD Explorations Newsletter 25, 2 (2024), 42–61.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,\nSiyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. 2023. Vicuna: An open-source chatbot\nimpressing gpt-4 with 90%* chatgpt quality, March 2023. URL https://lmsys. org/blog/2023-03-\n30-vicuna 3, 5 (2023).\nMichaël Defferrard, Xavier Bresson, and Pierre Vandergheynst. 2016. Convolutional neural networks\non graphs with fast localized spectral filtering. Advances in neural information processing systems\n29 (2016).\nDanny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan\nWahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. 2023. Palm-e: An embodied multimodal\nlanguage model. arXiv preprint arXiv:2303.03378 (2023).\nKeyu Duan, Qian Liu, Tat-Seng Chua, Shuicheng Yan, Wei Tsang Ooi, Qizhe Xie, and Junxian He.\n2023. Simteg: A frustratingly simple approach improves textual graph learning. arXiv preprint\narXiv:2308.02565 (2023).\nJhony H Giraldo, Konstantinos Skianis, Thierry Bouwmans, and Fragkiskos D Malliaros. 2023. On\nthe trade-off between over-smoothing and over-squashing in deep graph neural networks. In Pro-\nceedings of the 32nd ACM International Conference on Information and Knowledge Management.\n566–576.\nZirui Guo, Lianghao Xia, Yanhua Yu, Yuling Wang, Zixuan Yang, Wei Wei, Liang Pang, Tat-Seng\nChua, and Chao Huang. 2024. GraphEdit: Large Language Models for Graph Structure Learning.\narXiv preprint arXiv:2402.15183 (2024).\nXiaoxin He, Xavier Bresson, Thomas Laurent, Adam Perold, Yann LeCun, and Bryan Hooi. 2023.\nHarnessing explanations: Llm-to-lm interpreter for enhanced text-attributed graph representation\nlearning. In The Twelfth International Conference on Learning Representations.\nYufei He and Bryan Hooi. 2024. UniGraph: Learning a Cross-Domain Graph Foundation Model\nFrom Natural Language. arXiv preprint arXiv:2402.13630 (2024).\nZhenyu Hou, Yufei He, Yukuo Cen, Xiao Liu, Yuxiao Dong, Evgeny Kharlamov, and Jie Tang. 2023.\nGraphmae2: A decoding-enhanced masked self-supervised graph learner. In Proceedings of the\nACM Web Conference 2023. 737–746.\nWeihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta,\nand Jure Leskovec. 2020. Open graph benchmark: Datasets for machine learning on graphs.\nAdvances in neural information processing systems 33 (2020), 22118–22133.\nJin Huang, Xingjian Zhang, Qiaozhu Mei, and Jiaqi Ma. 2023. Can llms effectively leverage graph\nstructural information: when and why. arXiv preprint arXiv:2309.16595 (2023).\nXuanwen Huang, Kaiqiao Han, Yang Yang, Dezheng Bao, Quanjin Tao, Ziwei Chai, and Qi Zhu.\n2024. Can GNN be Good Adapter for LLMs? arXiv preprint arXiv:2402.12984 (2024).\nEric Jang, Shixiang Gu, and Ben Poole. 2016. Categorical Reparameterization with Gumbel-Softmax.\nIn International Conference on Learning Representations.\n11\nDiederik P Kingma and Max Welling. 2013. Auto-encoding variational bayes. arXiv preprint\narXiv:1312.6114 (2013).\nThomas N Kipf and Max Welling. 2016. Semi-Supervised Classification with Graph Convolutional\nNetworks. In International Conference on Learning Representations.\nChaozhuo Li, Bochen Pang, Yuming Liu, Hao Sun, Zheng Liu, Xing Xie, Tianqi Yang, Yanling Cui,\nLiangjie Zhang, and Qi Zhang. 2021b. Adsgnn: Behavior-graph augmented relevance modeling in\nsponsored search. In Proceedings of the 44th international ACM SIGIR conference on research\nand development in information retrieval. 223–232.\nGuohao Li, Matthias Müller, Bernard Ghanem, and Vladlen Koltun. 2021a. Training graph neural\nnetworks with 1000 layers. In International conference on machine learning. PMLR, 6437–6449.\nXiang Li, Renyu Zhu, Yao Cheng, Caihua Shan, Siqiang Luo, Dongsheng Li, and Weining Qian. 2022.\nFinding global homophily in graph neural networks when meeting heterophily. In International\nConference on Machine Learning. PMLR, 13242–13256.\nMeng Liu, Hongyang Gao, and Shuiwang Ji. 2020. Towards deeper graph neural networks. In\nProceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data\nmining. 338–348.\nSitao Luan, Chenqing Hua, Qincheng Lu, Jiaqi Zhu, Mingde Zhao, Shuyuan Zhang, Xiao-Wen\nChang, and Doina Precup. 2022. Revisiting heterophily for graph neural networks. Advances in\nneural information processing systems 35 (2022), 1362–1375.\nYao Ma, Xiaorui Liu, Neil Shah, and Jiliang Tang. 2021. Is homophily a necessity for graph neural\nnetworks? arXiv preprint arXiv:2106.06134 (2021).\nPéter Mernyei and C˘at˘alina Cangea. 2020. Wiki-cs: A wikipedia-based benchmark for graph neural\nnetworks. arXiv preprint arXiv:2007.02901 (2020).\nR OpenAI. 2023. Gpt-4 technical report. arxiv 2303.08774. View in Article 2, 5 (2023).\nOleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan Günnemann. 2018.\nPitfalls of Graph Neural Network Evaluation. arXiv e-prints (2018), arXiv–1811.\nIshika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter\nFox, Jesse Thomason, and Animesh Garg. 2023. Progprompt: Generating situated robot task plans\nusing large language models. In 2023 IEEE International Conference on Robotics and Automation\n(ICRA). IEEE, 11523–11530.\nShengyin Sun, Yuxiang Ren, Chen Ma, and Xuecang Zhang. 2023. Large language models as\ntopological structure enhancers for text-attributed graphs.\narXiv preprint arXiv:2311.14324\n(2023).\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open\nfoundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023).\nAaron Van Den Oord, Oriol Vinyals, et al. 2017. Neural discrete representation learning. Advances\nin neural information processing systems 30 (2017).\nPetar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua\nBengio. 2017. Graph attention networks. arXiv preprint arXiv:1710.10903 (2017).\nKuansan Wang, Zhihong Shen, Chiyuan Huang, Chieh-Han Wu, Yuxiao Dong, and Anshul Kanakia.\n2020. Microsoft academic graph: When experts are not enough. Quantitative Science Studies 1, 1\n(2020), 396–413.\nXinyuan Wang, Liang Wu, Liangjie Hong, Hao Liu, and Yanjie Fu. 2024. LLM-Enhanced User-Item\nInteractions: Leveraging Edge Information for Optimized Recommendations. arXiv preprint\narXiv:2402.09617 (2024).\n12\nLikang Wu, Zhaopeng Qiu, Zhi Zheng, Hengshu Zhu, and Enhong Chen. 2024. Exploring large\nlanguage model for graph data understanding in online job recommendations. In Proceedings of\nthe AAAI Conference on Artificial Intelligence, Vol. 38. 9178–9186.\nLikang Wu, Hongke Zhao, Zhi Li, Zhenya Huang, Qi Liu, and Enhong Chen. 2023. Learning\nthe explainable semantic relations via unified graph topic-disentangled neural networks. ACM\nTransactions on Knowledge Discovery from Data 17, 8 (2023), 1–23.\nQitian Wu, Wentao Zhao, Zenan Li, David P Wipf, and Junchi Yan. 2022. Nodeformer: A scalable\ngraph structure learning transformer for node classification. Advances in Neural Information\nProcessing Systems 35 (2022), 27387–27401.\nJunhan Yang, Zheng Liu, Shitao Xiao, Chaozhuo Li, Defu Lian, Sanjay Agrawal, Amit Singh,\nGuangzhong Sun, and Xing Xie. 2021. Graphformers: Gnn-nested transformers for representation\nlearning on textual graph. Advances in Neural Information Processing Systems 34 (2021), 28798–\n28810.\nRuosong Ye, Caiqi Zhang, Runhui Wang, Shuyuan Xu, and Yongfeng Zhang. 2023. Natural language\nis all a graph needs. arXiv preprint arXiv:2308.07134 (2023).\nMengmei Zhang, Mingwei Sun, Peng Wang, Shen Fan, Yanhu Mo, Xiaoxiao Xu, Hong Liu, Cheng\nYang, and Chuan Shi. 2024. GraphTranslator: Aligning Graph Model to Large Language Model\nfor Open-ended Tasks. arXiv preprint arXiv:2402.07197 (2024).\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher\nDewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022. Opt: Open pre-trained transformer\nlanguage models. arXiv preprint arXiv:2205.01068 (2022).\nJianan Zhao, Meng Qu, Chaozhuo Li, Hao Yan, Qian Liu, Rui Li, Xing Xie, and Jian Tang.\n2022. Learning on large-scale text-attributed graphs via variational inference. arXiv preprint\narXiv:2210.14709 (2022).\nJianan Zhao, Le Zhuo, Yikang Shen, Meng Qu, Kai Liu, Michael Bronstein, Zhaocheng Zhu, and Jian\nTang. 2023. Graphtext: Graph reasoning in text space. arXiv preprint arXiv:2310.01089 (2023).\nJiong Zhu, Ryan A Rossi, Anup Rao, Tung Mai, Nedim Lipka, Nesreen K Ahmed, and Danai Koutra.\n2021. Graph neural networks with heterophily. In Proceedings of the AAAI conference on artificial\nintelligence, Vol. 35. 11168–11176.\nYun Zhu, Yaoke Wang, Haizhou Shi, and Siliang Tang. 2024. Efficient Tuning and Inference for\nLarge Language Models on Textual Graphs. arXiv preprint arXiv:2401.15569 (2024).\n13\nA\nBaseline\nTraditional GNNs: This work employs five simple yet widely-used GNN models, namely GCN Kipf\nand Welling (2016), GraphSAGE Van Den Oord et al. (2017), GAT Veliˇckovi´c et al. (2017), DR-\nGAT Li et al. (2021a), and RevGAT Luan et al. (2022).\nGraphFormers Yang et al. (2021): These are graph transformers with nested GNN layers originally\ndesigned for link prediction tasks.\nNodeFormer Wu et al. (2022): It is an efficient large-graph transformer tool that devises a kernelized\nGumbel-Softmax operator.\nGLEM Sun et al. (2023): This is an effective framework, integrating large language models (LLMs)\nwith GNNs via a variational EM framework during training.\nGraphEdit Guo et al. (2024): It enhances the reliability of graph structure learning by effectively\ndenoising noisy inputs through the leveraging of LLMs.\nUniGraph He and Hooi (2024): It adopts a masking strategy to facilitate joint training of LLMs and\nGNNs under cascaded structures, enabling generalization to other graphs and datasets.\nInstructGLM Ye et al. (2023): It utilizes LLM-based instruction fine-tuning to describe the geometric\nstructure of graphs and node features, tackling graph tasks thereby.\nLLaGA Chen et al. (2024b): It reformats link information between nodes into sequential data, feeding\nLLM-compatible sequential inputs while preserving node structure, achieved through instruction\nfine-tuning.\nENGINE Zhu et al. (2024): By devising tunable side structures, it combines LLMs with GNNs and\nemploys various strategies to expedite model training and inference.\nB\nEffect of different Embedding\nTo evaluate the impact of different embeddings on model performance, we selected OGB embeddings\nand GIANT embeddings for comparison with various GNN models, and the results are as shown in\nthe table. It can be observed that LangTopo consistently outperforms GNN models across different\nembedding scenarios.\nTable 5: Performance comparison using different embeddings(arxiv).\nLM\nGCN\nSAGE\nGAT\nRevGAT\nDRGAT\nLangTopo\nOGB\n0.7174\n0.7119\n0.7366\n0.7402\n0.7416\n0.7451\nGIANT\n0.7329\n0.7435\n0.7416\n0.7590\n0.7611\n0.7624\nC\nrich and scarce text\nWe have delved into the capacity of LLMs to utilize textual information, categorizing it into two\nprimary types: 1) Rich Text, comprising both summaries and titles, and 2) Scarce Text, which is\nlimited to titles only. The outcomes, as depicted in the table 6, indicate that LLMs experience a\nnotable enhancement when furnished with rich text.\nTable 6: Performance comparison between rich text(abstract title) and scarce text(title). raw text\nLLM\nArxiv\nPubmed\nCora\nrich\n0.7598\n0.9395\n0.9085\ntitle\n0.7365\n0.9287\n0.8998\n14\nD\nDataset\nWe have compiled statistical measures for each dataset, as illustrated in the table 7 provided. Further-\nmore, we employed the arxiv2023 dataset, which compiles scholarly articles from the arXiv website\nfor the year 2023. This strategic choice was made to circumvent potential data leakage issues arising\nfrom the use of corpus included in the dataset during the pre-training phase of the LLMs.\nTable 7: Statistics of the evaluation datasets.\nDataset\nArxiv\nPubmed\nCora\narxiv23\n#Nodes\n169343\n19717\n2708\n46198\n#edges\n1166243\n44338\n5429\n78548\n#Classes\n40\n3\n7\n40\n#Split ratio(%)\n54/18/28\n60/20/20\n60/20/20\n60/20/20\nE\nHyperparameters\nTable E furnishes the hyperparameters for LangTopo across various datasets. For more detailed\nconfigurations, please refer to our source code.\nTable 8: Hyperparameters for the Arxiv, Pubmed, and Citeseer.\nHyperparameters\nArixv\nPubmed\nCiteseer\nGNN\nlayers\n3\n2\n2\nhidden dim\n512\n256\n256\nlearning rate\n0.001\n0.005\n0.005\ndropout\n0.5\n0.5\n0.5\nepoch\n1000\n1000\n1000\nearly stop\n50\n50\n50\ncodebook size\n4096\n2048\n2048\nαnode\n10\n10\n10\nβKL\n1e-2\n1e-2\n1e-2\nLLM\nlearning rate\n5e-5\n5e-5\n5e-5\nwarmup\n0.05\n0.05\n0.05\ngradient accumulation steps\n8\n8\n8\nbatch size\n4\n4\n4\nαmse\n0.5\n0.1\n0.1\nβkl\n1e-4\n1e-4\n1e-4\nF\nEffect of different GNN\nWe conducted an exploration of LangTopo by substituting it with different GNN models.\nTable 9: Performance comparison using different GNN.\nLLM\nArxiv\nPubmed\nCora\nGCN\n0.7640\n0.9566\n0.9012\nGraphSage\n0.7681\n0.9667\n0.9158\nGAT\n0.7704\n0.9633\n0.9123\n15\nG\nGumbel softmax\nWe will show that using gumbel softmax does not change the selection probability distribution.\nConsider a K-dimensional output vector where each component is denoted xk. The probability of\nobtaining each dimension via the softmax function is:\npik =\nexk\nPK\nk′=1 exk′\n(15)\nIf we add independent standard Gumbel noise (with scale parameter 1 and location parameter 0) to\neach xk, and select the dimension corresponding to the maximum value as the output, it is claimed\nthat the resulting probability remains πk. We proceed to prove this assertion. The probability density\nfunction (PDF) of a Gumbel distribution (with scale 1 and location µ) is:\nf(z; µ) = e−(z−µ)−e−(z−µ),\nand its cumulative distribution function (CDF) is:\nF(z; µ) = e−e−(z−µ).\nAssuming the k-th Gumbel distribution corresponds to xk, yielding zk = xk +Gk, where Gk follows\na Gumbel distribution with location parameter xk, we aim to show that the probability of obtaining\nzk is indeed πk, i.e.,\nP(zk ≥zk′, ∀k′ ̸= k|{xk′}K\nk′=1) = πk.\nThe conditional cumulative probability for zk is then:\nP(zk ≥zk′, ∀k′ ̸= k|zk, {xk′}K\nk′=1) =\nY\nk′̸=k\ne−e−(zk−xk′ ).\nIntegrating over zk to obtain the marginal probability yields:\nP(zk ≥zk′, ∀k′ ̸= k|{xk′}K\nk′=1)\n=\nZ\nP(zk ≥zk′, ∀k′ ̸= k|zk, {xk′}K\nk′=1) · f(zk; xk) dzk\n=\nZ\nY\nk′̸=k\ne−e−(zk−xk′ ) · e−(zk−xk)−e−(zk−xk) dzk\n=\nZ\ne−P\nk′̸=k e−(zk−xk′ )−(zk−xk)−e−(zk−xk) dzk\n=\nZ\ne−P\nk′̸=k e−(zk−xk′ )−(zk−xk)−e−(zk−xk) dzk\n=\nZ\ne−PK\nk′=1 e−(zk−xk′ )−zk+xk dzk\n=\nZ\ne−e\n−zk+ln(\nPK\nk′=1 exk′)−zk+xk dzk\n= e−ln(\nPK\nk′=1 exk′)+xk\nZ\ne−e\n−(zk−ln(\nPK\nk′=1 exk′))−(zk−ln(\nPK\nk′=1 exk′)) dzk\n=\nexk\nPK\nk′=1 exk′\nZ\ne−e\n−(zk−ln(\nPK\nk′=1 exk′))−(zk−ln(\nPK\nk′=1 exk′)) dzk\n=\nexk\nPK\nk′=1 exk′\nZ\n−e−(zk−ln(\nPK\nk′=1 exk′)) −e(zk−ln(\nPK\nk′=1 exk′)) dzk\n(16)\nInside the integral, we consider a Gumbel distribution characterized by µ = ln\n\u0010PK\nk′=1 exk′\u0011\n, from\nwhich it is known that the integral evaluates to 1. Consequently, this leads us to the conclusion that\nP(zk ≥zk′, ∀k′ ̸= k | {xk′}K\nk′=1) =\nexk\nPK\nk′=1 exk′ ,\nwhich is consistent with the output of the softmax function.\n16\nH\nPrompt in different dataset\nPrompt\nCora\n\"<User>: Given a node {text attribute}. the node connect {link information}. Cate-\ngorize the nodes into groups ’theory’,’reinforcement learning’, ’neural networks’,\n’probabilistic methods’, ’case based’, and ’rule learning’, according to the given\ninformation. the node {text attribute} is classified as <Assistant>: {label}\"\nPubmed\n\"<User>: Given a node {text attribute}. the node connect {link information}. Cate-\ngorize the nodes into groups ’Diabetes Mellitus, Experimental’,’Diabetes Mellitus,\nType 2’, and ’Diabetes Mellitus, Type 1’ according to the given information. the\nnode {text attribute} is classified as <Assistant>: {label}\"\nArxiv\n\"<User>: Given a node {text attribute}, the node connect {link information}. We\nneed to classify the node into 40 classes: ’Numerical Analysis’,’Multimedia’,’Logic\nin\nComputer\nScience’,’Computers\nand\nSociety’,’Cryptography\nand\nSecu-\nrity’,’Distributed,\nParallel,\nand Cluster Computing’,’Human-Computer In-\nteraction’,’Computational Engineering,\nFinance,\nand Science’,’Networking\nand\nInternet\nArchitecture’,’Computational\nComplexity’,’Artificial\nIntelli-\ngence’,’Multiagent\nSystems’,’General\nLiterature’,’Neural\nand\nEvolutionary\nComputing’,’Symbolic Computation’,’Hardware Architecture’,’Computer Vision\nand Pattern Recognition’,’Graphics’,’Emerging Technologies’,’Systems and\nControl’,’Computational Geometry’,’Other Computer Science’,’Programming\nLanguages’,’Software Engineering’,’Machine Learning’,’Sound’,’Social and Infor-\nmation Networks’,’Robotics’,’Information Theory’,’Performance’,’Computation and\nLanguage’,’Information Retrieval’,’Mathematical Software’,’Formal Languages and\nAutomata Theory’,’Data Structures and Algorithms’,’Operating Systems’,’Computer\nScience and Game Theory’,’Databases’,’Digital Libraries’,’Discrete Mathematics’.\nthe node {text attribute} is classified as <Assistant>: {label}\"\nArxiv2023\n\"<User>: Given a node {text attribute}, the node connect {link information}. We\nneed to classify the node into 40 classes: ’Numerical Analysis’,’Multimedia’,’Logic\nin\nComputer\nScience’,’Computers\nand\nSociety’,’Cryptography\nand\nSecu-\nrity’,’Distributed,\nParallel,\nand Cluster Computing’,’Human-Computer In-\nteraction’,’Computational Engineering,\nFinance,\nand Science’,’Networking\nand\nInternet\nArchitecture’,’Computational\nComplexity’,’Artificial\nIntelli-\ngence’,’Multiagent\nSystems’,’General\nLiterature’,’Neural\nand\nEvolutionary\nComputing’,’Symbolic Computation’,’Hardware Architecture’,’Computer Vision\nand Pattern Recognition’,’Graphics’,’Emerging Technologies’,’Systems and\nControl’,’Computational Geometry’,’Other Computer Science’,’Programming\nLanguages’,’Software Engineering’,’Machine Learning’,’Sound’,’Social and Infor-\nmation Networks’,’Robotics’,’Information Theory’,’Performance’,’Computation and\nLanguage’,’Information Retrieval’,’Mathematical Software’,’Formal Languages and\nAutomata Theory’,’Data Structures and Algorithms’,’Operating Systems’,’Computer\nScience and Game Theory’,’Databases’,’Digital Libraries’,’Discrete Mathematics’.\nthe node {text attribute} is classified as <Assistant>: {label}\"\nTable 10: A table with different prompt\n17\nNeurIPS Paper Checklist\n1. Claims\nQuestion: Do the main claims made in the abstract and introduction accurately reflect the\npaper’s contributions and scope?\nAnswer: [Yes]\nJustification: The abstract and introduction have accurately expressed the topic of the paper\nGuidelines:\n• The answer NA means that the abstract and introduction do not include the claims\nmade in the paper.\n• The abstract and/or introduction should clearly state the claims made, including the\ncontributions made in the paper and important assumptions and limitations. A No or\nNA answer to this question will not be perceived well by the reviewers.\n• The claims made should match theoretical and experimental results, and reflect how\nmuch the results can be expected to generalize to other settings.\n• It is fine to include aspirational goals as motivation as long as it is clear that these goals\nare not attained by the paper.\n2. Limitations\nQuestion: Does the paper discuss the limitations of the work performed by the authors?\nAnswer: [Yes]\nJustification: The relevant limitations of the paper have been discussed, and the relevant\nexploration has been insufficient.\nGuidelines:\n• The answer NA means that the paper has no limitation while the answer No means that\nthe paper has limitations, but those are not discussed in the paper.\n• The authors are encouraged to create a separate \"Limitations\" section in their paper.\n• The paper should point out any strong assumptions and how robust the results are to\nviolations of these assumptions (e.g., independence assumptions, noiseless settings,\nmodel well-specification, asymptotic approximations only holding locally). The authors\nshould reflect on how these assumptions might be violated in practice and what the\nimplications would be.\n• The authors should reflect on the scope of the claims made, e.g., if the approach was\nonly tested on a few datasets or with a few runs. In general, empirical results often\ndepend on implicit assumptions, which should be articulated.\n• The authors should reflect on the factors that influence the performance of the approach.\nFor example, a facial recognition algorithm may perform poorly when image resolution\nis low or images are taken in low lighting. Or a speech-to-text system might not be\nused reliably to provide closed captions for online lectures because it fails to handle\ntechnical jargon.\n• The authors should discuss the computational efficiency of the proposed algorithms\nand how they scale with dataset size.\n• If applicable, the authors should discuss possible limitations of their approach to\naddress problems of privacy and fairness.\n• While the authors might fear that complete honesty about limitations might be used by\nreviewers as grounds for rejection, a worse outcome might be that reviewers discover\nlimitations that aren’t acknowledged in the paper. The authors should use their best\njudgment and recognize that individual actions in favor of transparency play an impor-\ntant role in developing norms that preserve the integrity of the community. Reviewers\nwill be specifically instructed to not penalize honesty concerning limitations.\n3. Theory Assumptions and Proofs\nQuestion: For each theoretical result, does the paper provide the full set of assumptions and\na complete (and correct) proof?\nAnswer: [Yes]\n18\nJustification: Hypotheses and proofs are provided in the body and appendix\nGuidelines:\n• The answer NA means that the paper does not include theoretical results.\n• All the theorems, formulas, and proofs in the paper should be numbered and cross-\nreferenced.\n• All assumptions should be clearly stated or referenced in the statement of any theorems.\n• The proofs can either appear in the main paper or the supplemental material, but if\nthey appear in the supplemental material, the authors are encouraged to provide a short\nproof sketch to provide intuition.\n• Inversely, any informal proof provided in the core of the paper should be complemented\nby formal proofs provided in appendix or supplemental material.\n• Theorems and Lemmas that the proof relies upon should be properly referenced.\n4. Experimental Result Reproducibility\nQuestion: Does the paper fully disclose all the information needed to reproduce the main ex-\nperimental results of the paper to the extent that it affects the main claims and/or conclusions\nof the paper (regardless of whether the code and data are provided or not)?\nAnswer: [Yes]\nJustification: We provide the data and code\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• If the paper includes experiments, a No answer to this question will not be perceived\nwell by the reviewers: Making the paper reproducible is important, regardless of\nwhether the code and data are provided or not.\n• If the contribution is a dataset and/or model, the authors should describe the steps taken\nto make their results reproducible or verifiable.\n• Depending on the contribution, reproducibility can be accomplished in various ways.\nFor example, if the contribution is a novel architecture, describing the architecture fully\nmight suffice, or if the contribution is a specific model and empirical evaluation, it may\nbe necessary to either make it possible for others to replicate the model with the same\ndataset, or provide access to the model. In general. releasing code and data is often\none good way to accomplish this, but reproducibility can also be provided via detailed\ninstructions for how to replicate the results, access to a hosted model (e.g., in the case\nof a large language model), releasing of a model checkpoint, or other means that are\nappropriate to the research performed.\n• While NeurIPS does not require releasing code, the conference does require all submis-\nsions to provide some reasonable avenue for reproducibility, which may depend on the\nnature of the contribution. For example\n(a) If the contribution is primarily a new algorithm, the paper should make it clear how\nto reproduce that algorithm.\n(b) If the contribution is primarily a new model architecture, the paper should describe\nthe architecture clearly and fully.\n(c) If the contribution is a new model (e.g., a large language model), then there should\neither be a way to access this model for reproducing the results or a way to reproduce\nthe model (e.g., with an open-source dataset or instructions for how to construct\nthe dataset).\n(d) We recognize that reproducibility may be tricky in some cases, in which case\nauthors are welcome to describe the particular way they provide for reproducibility.\nIn the case of closed-source models, it may be that access to the model is limited in\nsome way (e.g., to registered users), but it should be possible for other researchers\nto have some path to reproducing or verifying the results.\n5. Open access to data and code\nQuestion: Does the paper provide open access to the data and code, with sufficient instruc-\ntions to faithfully reproduce the main experimental results, as described in supplemental\nmaterial?\n19\nAnswer: [Yes]\nJustification: We provide the corresponding data and code in zip file\nGuidelines:\n• The answer NA means that paper does not include experiments requiring code.\n• Please see the NeurIPS code and data submission guidelines (https://nips.cc/\npublic/guides/CodeSubmissionPolicy) for more details.\n• While we encourage the release of code and data, we understand that this might not be\npossible, so “No” is an acceptable answer. Papers cannot be rejected simply for not\nincluding code, unless this is central to the contribution (e.g., for a new open-source\nbenchmark).\n• The instructions should contain the exact command and environment needed to run to\nreproduce the results. See the NeurIPS code and data submission guidelines (https:\n//nips.cc/public/guides/CodeSubmissionPolicy) for more details.\n• The authors should provide instructions on data access and preparation, including how\nto access the raw data, preprocessed data, intermediate data, and generated data, etc.\n• The authors should provide scripts to reproduce all experimental results for the new\nproposed method and baselines. If only a subset of experiments are reproducible, they\nshould state which ones are omitted from the script and why.\n• At submission time, to preserve anonymity, the authors should release anonymized\nversions (if applicable).\n• Providing as much information as possible in supplemental material (appended to the\npaper) is recommended, but including URLs to data and code is permitted.\n6. Experimental Setting/Details\nQuestion: Does the paper specify all the training and test details (e.g., data splits, hyper-\nparameters, how they were chosen, type of optimizer, etc.) necessary to understand the\nresults?\nAnswer: [Yes]\nJustification: We provide the corresponding data and code in zip file\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• The experimental setting should be presented in the core of the paper to a level of detail\nthat is necessary to appreciate the results and make sense of them.\n• The full details can be provided either with the code, in appendix, or as supplemental\nmaterial.\n7. Experiment Statistical Significance\nQuestion: Does the paper report error bars suitably and correctly defined or other appropriate\ninformation about the statistical significance of the experiments?\nAnswer: [No]\nJustification: For LLM, the effect of repeated experiments is stable\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• The authors should answer \"Yes\" if the results are accompanied by error bars, confi-\ndence intervals, or statistical significance tests, at least for the experiments that support\nthe main claims of the paper.\n• The factors of variability that the error bars are capturing should be clearly stated (for\nexample, train/test split, initialization, random drawing of some parameter, or overall\nrun with given experimental conditions).\n• The method for calculating the error bars should be explained (closed form formula,\ncall to a library function, bootstrap, etc.)\n• The assumptions made should be given (e.g., Normally distributed errors).\n• It should be clear whether the error bar is the standard deviation or the standard error\nof the mean.\n20\n• It is OK to report 1-sigma error bars, but one should state it. The authors should\npreferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis\nof Normality of errors is not verified.\n• For asymmetric distributions, the authors should be careful not to show in tables or\nfigures symmetric error bars that would yield results that are out of range (e.g. negative\nerror rates).\n• If error bars are reported in tables or plots, The authors should explain in the text how\nthey were calculated and reference the corresponding figures or tables in the text.\n8. Experiments Compute Resources\nQuestion: For each experiment, does the paper provide sufficient information on the com-\nputer resources (type of compute workers, memory, time of execution) needed to reproduce\nthe experiments?\nAnswer: [Yes]\nJustification: We provided the appropriate information\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• The paper should indicate the type of compute workers CPU or GPU, internal cluster,\nor cloud provider, including relevant memory and storage.\n• The paper should provide the amount of compute required for each of the individual\nexperimental runs as well as estimate the total compute.\n• The paper should disclose whether the full research project required more compute\nthan the experiments reported in the paper (e.g., preliminary or failed experiments that\ndidn’t make it into the paper).\n9. Code Of Ethics\nQuestion: Does the research conducted in the paper conform, in every respect, with the\nNeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?\nAnswer: [Yes]\nJustification: Our experiment is ethical\nGuidelines:\n• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.\n• If the authors answer No, they should explain the special circumstances that require a\ndeviation from the Code of Ethics.\n• The authors should make sure to preserve anonymity (e.g., if there is a special consid-\neration due to laws or regulations in their jurisdiction).\n10. Broader Impacts\nQuestion: Does the paper discuss both potential positive societal impacts and negative\nsocietal impacts of the work performed?\nAnswer: [NA]\nJustification: Our experiment didn’t make a difference\nGuidelines:\n• The answer NA means that there is no societal impact of the work performed.\n• If the authors answer NA or No, they should explain why their work has no societal\nimpact or why the paper does not address societal impact.\n• Examples of negative societal impacts include potential malicious or unintended uses\n(e.g., disinformation, generating fake profiles, surveillance), fairness considerations\n(e.g., deployment of technologies that could make decisions that unfairly impact specific\ngroups), privacy considerations, and security considerations.\n• The conference expects that many papers will be foundational research and not tied\nto particular applications, let alone deployments. However, if there is a direct path to\nany negative applications, the authors should point it out. For example, it is legitimate\nto point out that an improvement in the quality of generative models could be used to\n21\ngenerate deepfakes for disinformation. On the other hand, it is not needed to point out\nthat a generic algorithm for optimizing neural networks could enable people to train\nmodels that generate Deepfakes faster.\n• The authors should consider possible harms that could arise when the technology is\nbeing used as intended and functioning correctly, harms that could arise when the\ntechnology is being used as intended but gives incorrect results, and harms following\nfrom (intentional or unintentional) misuse of the technology.\n• If there are negative societal impacts, the authors could also discuss possible mitigation\nstrategies (e.g., gated release of models, providing defenses in addition to attacks,\nmechanisms for monitoring misuse, mechanisms to monitor how a system learns from\nfeedback over time, improving the efficiency and accessibility of ML).\n11. Safeguards\nQuestion: Does the paper describe safeguards that have been put in place for responsible\nrelease of data or models that have a high risk for misuse (e.g., pretrained language models,\nimage generators, or scraped datasets)?\nAnswer: [NA]\nJustification: the paper poses no such risks.\nGuidelines:\n• The answer NA means that the paper poses no such risks.\n• Released models that have a high risk for misuse or dual-use should be released with\nnecessary safeguards to allow for controlled use of the model, for example by requiring\nthat users adhere to usage guidelines or restrictions to access the model or implementing\nsafety filters.\n• Datasets that have been scraped from the Internet could pose safety risks. The authors\nshould describe how they avoided releasing unsafe images.\n• We recognize that providing effective safeguards is challenging, and many papers do\nnot require this, but we encourage authors to take this into account and make a best\nfaith effort.\n12. Licenses for existing assets\nQuestion: Are the creators or original owners of assets (e.g., code, data, models), used in\nthe paper, properly credited and are the license and terms of use explicitly mentioned and\nproperly respected?\nAnswer: [Yes]\nJustification: We followed the relevant protocols\nGuidelines:\n• The answer NA means that the paper does not use existing assets.\n• The authors should cite the original paper that produced the code package or dataset.\n• The authors should state which version of the asset is used and, if possible, include a\nURL.\n• The name of the license (e.g., CC-BY 4.0) should be included for each asset.\n• For scraped data from a particular source (e.g., website), the copyright and terms of\nservice of that source should be provided.\n• If assets are released, the license, copyright information, and terms of use in the\npackage should be provided. For popular datasets, paperswithcode.com/datasets\nhas curated licenses for some datasets. Their licensing guide can help determine the\nlicense of a dataset.\n• For existing datasets that are re-packaged, both the original license and the license of\nthe derived asset (if it has changed) should be provided.\n• If this information is not available online, the authors are encouraged to reach out to\nthe asset’s creators.\n13. New Assets\nQuestion: Are new assets introduced in the paper well documented and is the documentation\nprovided alongside the assets?\n22\nAnswer: [Yes] ,\nJustification: We made the relevant Settings\nGuidelines:\n• The answer NA means that the paper does not release new assets.\n• Researchers should communicate the details of the dataset/code/model as part of their\nsubmissions via structured templates. This includes details about training, license,\nlimitations, etc.\n• The paper should discuss whether and how consent was obtained from people whose\nasset is used.\n• At submission time, remember to anonymize your assets (if applicable). You can either\ncreate an anonymized URL or include an anonymized zip file.\n14. Crowdsourcing and Research with Human Subjects\nQuestion: For crowdsourcing experiments and research with human subjects, does the paper\ninclude the full text of instructions given to participants and screenshots, if applicable, as\nwell as details about compensation (if any)?\nAnswer: [NA]\nJustification: the paper does not involve crowdsourcing nor research with human subjects.\nGuidelines:\n• The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects.\n• Including this information in the supplemental material is fine, but if the main contribu-\ntion of the paper involves human subjects, then as much detail as possible should be\nincluded in the main paper.\n• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,\nor other labor should be paid at least the minimum wage in the country of the data\ncollector.\n15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human\nSubjects\nQuestion: Does the paper describe potential risks incurred by study participants, whether\nsuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)\napprovals (or an equivalent approval/review based on the requirements of your country or\ninstitution) were obtained?\nAnswer: [NA]\nJustification: the paper does not involve crowdsourcing nor research with human subjects\nGuidelines:\n• The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects.\n• Depending on the country in which research is conducted, IRB approval (or equivalent)\nmay be required for any human subjects research. If you obtained IRB approval, you\nshould clearly state this in the paper.\n• We recognize that the procedures for this may vary significantly between institutions\nand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the\nguidelines for their institution.\n• For initial submissions, do not include any information that would break anonymity (if\napplicable), such as the institution conducting the review.\n23\n",
  "categories": [
    "cs.AI",
    "cs.CL",
    "cs.LG"
  ],
  "published": "2024-06-19",
  "updated": "2024-06-19"
}