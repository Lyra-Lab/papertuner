{
  "id": "http://arxiv.org/abs/2406.15096v1",
  "title": "Towards General Negotiation Strategies with End-to-End Reinforcement Learning",
  "authors": [
    "Bram M. Renting",
    "Thomas M. Moerland",
    "Holger H. Hoos",
    "Catholijn M. Jonker"
  ],
  "abstract": "The research field of automated negotiation has a long history of designing\nagents that can negotiate with other agents. Such negotiation strategies are\ntraditionally based on manual design and heuristics. More recently,\nreinforcement learning approaches have also been used to train agents to\nnegotiate. However, negotiation problems are diverse, causing observation and\naction dimensions to change, which cannot be handled by default linear policy\nnetworks. Previous work on this topic has circumvented this issue either by\nfixing the negotiation problem, causing policies to be non-transferable between\nnegotiation problems or by abstracting the observations and actions into\nfixed-size representations, causing loss of information and expressiveness due\nto feature design. We developed an end-to-end reinforcement learning method for\ndiverse negotiation problems by representing observations and actions as a\ngraph and applying graph neural networks in the policy. With empirical\nevaluations, we show that our method is effective and that we can learn to\nnegotiate with other agents on never-before-seen negotiation problems. Our\nresult opens up new opportunities for reinforcement learning in negotiation\nagents.",
  "text": "Published as a conference paper at RLC 2024\nTowards General Negotiation Strategies with\nEnd-to-End Reinforcement Learning\nBram M. Renting\nb.m.renting@liacs.leidenuniv.nl\nLeiden University\nDelft University of Technology\nThomas M. Moerland\nt.m.moerland@liacs.leidenuniv.nl\nLeiden University\nHolger H. Hoos\nhh@cs.rwth-aachen.de\nRWTH Aachen University\nLeiden University\nCatholijn M. Jonker\nc.m.jonker@tudelft.nl\nDelft University of Technology\nLeiden University\nAbstract\nThe research field of automated negotiation has a long history of designing agents\nthat can negotiate with other agents. Such negotiation strategies are traditionally\nbased on manual design and heuristics. More recently, reinforcement learning ap-\nproaches have also been used to train agents to negotiate. However, negotiation\nproblems are diverse, causing observation and action dimensions to change, which\ncannot be handled by default linear policy networks. Previous work on this topic\nhas circumvented this issue either by fixing the negotiation problem, causing policies\nto be non-transferable between negotiation problems or by abstracting the obser-\nvations and actions into fixed-size representations, causing loss of information and\nexpressiveness due to feature design. We developed an end-to-end reinforcement\nlearning method for diverse negotiation problems by representing observations and\nactions as a graph and applying graph neural networks in the policy. With empirical\nevaluations, we show that our method is effective and that we can learn to negotiate\nwith other agents on never-before-seen negotiation problems. Our result opens up\nnew opportunities for reinforcement learning in negotiation agents.\n1\nIntroduction\nIn multi-agent systems, agents sometimes must coordinate actions to improve payoff or even obtain\npayoff in the first place (e.g., surveying drone swarms or transporting goods using multiple robots).\nIn such scenarios, communication between agents can improve insight into other agents’ intentions\nand behaviour, leading to better coordination between agents and thus improving payoff. When\nagents have individual preferences besides a shared common goal, also known as mixed-motive or\ngeneral sum games, communication can become more complex, as this introduces an incentive to\ndeceive (Dafoe et al., 2020).\nA special case of communication in mixed-motive multi-agent systems is negotiation, which allows\nfor finding and agreeing on mutually beneficial coordinated actions before performing them. Nego-\ntiation plays a central role in many present and future real-world applications, such as traffic light\ncoordination, calendar scheduling, or balancing energy demand and production in local power grids,\nbut also in games, such as Diplomacy or Werewolves. Automated Negotiation is a long-standing\nresearch field that has focussed on designing agents that can negotiate (Smith, 1980; Rosenschein,\n1986; Sycara, 1988; Tawfik Jelassi & Foroughi, 1989; Klein & Lu, 1989; Robinson, 1990).\narXiv:2406.15096v1  [cs.MA]  21 Jun 2024\nPublished as a conference paper at RLC 2024\nTraditionally, many negotiating agents were manually designed algorithms based on heuristics, which\nis still a commonly seen approach in recent editions of the Automated Negotiation Agents Compe-\ntition (ANAC) (Aydoğan et al., 2023). However, manually designing such negotiation strategies is\ntime-consuming and results in highly specialised and fixed negotiation strategies that often do not\ngeneralise well over a broad set of negotiation settings. In later work, optimisation methods were\nused to optimise the parameters of negotiation strategies using evolutionary algorithms (Eymann,\n2001; Dworman et al., 1996; Lau et al., 2006), or algorithm configuration techniques (Renting et al.,\n2020). Such approaches allow negotiation strategies to be more easily adaptable to different negotia-\ntion problems but still require partial manual design to obtain a parameterized negotiation strategy,\nmaking them cumbersome and limiting their generalizability.\nWith the advent of Reinforcement Learning (RL) (Sutton & Barto, 2018), there have been attempts\nat using RL-based methods for creating negotiation agents (Bakker et al., 2019). There is, however,\nstill an open challenge. In automated negotiation, it is common for agents to deal with various\nnegotiation problems that would cause differently sized observation and action vectors for default\nlinear layer-based RL policies. Up until now, this issue has been dealt with by either abstracting\nthe observations and actions into a fixed-length vector (see, e.g., Bakker et al. (2019)) or by fixing\nthe negotiation problem, such that the observation and action space remain identical (see, e.g., Higa\net al. (2023)). The first approach causes information loss due to feature design, and the latter renders\nthe obtained policy non-transferable to other negotiation problems.\nWe set out on the idea that a more general RL-based negotiation strategy capable of dealing with\nvarious negotiation problems is achievable and that such a strategy can be learned using end-to-end\nreinforcement learning without using state abstractions. Developing such an RL negotiation strategy\nwould open up new avenues for RL in automated negotiation as policies are easily extendable. End-\nto-end methods are also able to learn complex relations between observations and actions, minimizing\nthe risk of information loss that is often imposed by (partially) manual design strategies.\nTo this extent, we designed a graph-based representation of a negotiation problem. We applied graph\nneural networks in the RL policy to deal with the changing dimensions of both the observation and\naction space. To the best of our knowledge, graph-based policy networks have not been used before\nto handle changing action spaces, except by Yang et al. (2024), who independently proposed a similar\napproach to another problem. We show that our method shows similar performance to a recent end-\nto-end RL-based method designed to deal only with a fixed negotiation problem. More importantly,\nwe show that our end-to-end method can successfully learn to negotiate with other agents and that\nthe obtained policy still performs on unseen, randomly generated negotiation problems.\n2\nRelated Work\nBakker et al. (2019) applied RL to decide what utility to demand in the next offer. They abstracted\nthe state to utility values of the last few offers and time towards the deadline. Translating utility\nto an offer, estimating opponent utility, and deciding when to accept were done without RL. Bagga\net al. (2022) also abstracted the state into a fixed representation with utility statistics of historical\noffers. They used an RL policy to decide whether to accept and a separate policy that outputs offers\nbased on a non-RL opponent utility estimation model.\nSengupta et al. (2021) encoded the state into a fixed length of past utility values. The action is the\nutility offer target, translated to an actual offer through an exhaustive search of the outcome space.\nThey trained a portfolio of policies and tried to select effective counterstrategies by classifying\nthe opponent type. Li et al. (2023) also build a portfolio of RL-based negotiation strategies by\nincrementally training best responses based on the Nash bargaining solution. During evaluation,\ntheir method searches for the best response in an effort to improve cooperativity. They only applied\ntheir method to fixed negotiation problems.\nAnother line of research on negotiation agents includes natural language. An environment for this\nwas developed by Lewis et al. (2017).\nKwon et al. (2021) used this environment and applied a\nPublished as a conference paper at RLC 2024\ncombination of RL, supervised learning, and expert annotations (based on a dataset) to iteratively\ntrain two agents through self-play. The negotiation problems considered are fixed, except for the\npreferences.\nTakahashi et al. (2022) and Higa et al. (2023) are closest to our work, as they also train an end-to-\nend RL method for negotiation games. Their approach does not use state abstractions and linearly\nmaps the negotiation problem and actions in a policy. The policy obtained can only be used for a\nfixed problem. They also trained and tested only against single opponents.\n3\nMethods\nWe formulate the negotiation game as a turn-based Partially Observable Stochastic Game (POSG),\na partially observable extension of a stochastic game (Shapley, 1953). We model the game as a tuple\nM = ⟨I, S, Oi, Ai, T , Ωi, Ri⟩, where I = {1, · · · , n} denotes the set of agents, S the set of states, Oi\nthe set of possible observations for agent i, and Ai the set of actions for agent i. For convenience, we\nwrite A = Ai, as we consider a turn-based game where only single agents take actions. Furthermore,\nT : S × A 7→p(S) denotes the transition function, Ωi : S × A 7→p(Oi) the observation function for\nagent i, and Ri : S × A 7→R the reward function for agent i.\nThe game starts in a particular state s.\nThen, at timestep t, an agent i selects an action at,i\nindependently of other agents. Based on this action, the state of the POSG changes according to\nst+1 ∼T (st+1|st, at). Subsequently, each agent receives its own observation ot,i ∼Ωi(ot,i|st, at) and\nassociated reward rt,i ∼Ri(rt,i|st, at).\nEach agent i selects actions according to its own policy πi : Oi×Oi×· · · →p(A). At timestep t, agent\ni samples an action at ∼πi(at|ot,i, ot−1,i, · · · ). Note that we can vary the length of the historical\nobservations by which we condition the policy for each agent. The more history we include, the\nmore we can overcome partial observability.\nOur goal is to find a policy πi for agent i that maximizes its cumulative expected return:\nπ⋆\ni ∈arg max\nπi\nEπ,T\n\" H\nX\nk=0\nRi(st+k, at+k)\n#\n,\n(1)\nwhere H denotes the horizon of the POSG (the number of rounds we select an action). Crucially,\nthe performance of a particular policy πi depends on the other agents’ policies.\n3.1\nNegotiation Game\nA negotiation game consists of a set of agents and a problem to negotiate over. This work only\nconsiders bilateral negotiation games with two agents. The negotiation problem, also known as a\nnegotiation domain, generally consists of a set of objectives (or issues) B = {1, · · · , m} with an\nassociated set of values Vb to choose from. Value sets can be continuous, integer, or discrete, but\nwe focus solely on discrete value sets in this work, which is the most general type, as continuous\nvalues can also be discretised. For each of the objectives b ∈B, both agents must agree upon a value\nvb ∈Vb. The total outcome space is the Cartesian product of all the value sets Ω= V1 × · · · × Vm\nwith a single outcome being ω = ⟨v1, · · · , vm⟩.\nBoth agents have preferences over the outcome space expressed through a utility function u : Ω7→\n[0, 1] that is private information. Here, 1 is their best possible outcome, and 0 is their worst. This\npaper only considers additive utility functions as shown in Equation 2. Here, weights are assigned\nto all values and objectives through weight functions w : B 7→[0, 1] and wb : Vb 7→[0, 1] such that\nP\nb∈B w(b) = 1, maxvb∈Vb wb(vb) = 1, and minvb∈Vb wb(vb) = 0.\nu(ω) =\nX\nb∈B\nw(b) · wb(vb)\n(2)\nPublished as a conference paper at RLC 2024\n3.1.1\nProtocol\nThe negotiation follows the commonly used Alternating Offers Protocol (Rubinstein, 1982), where\nagents take turns. During its turn, an agent can make a (counter) offer or accept the opponent’s\noffer. A deadline is imposed to prevent the agents from negotiating indefinitely. Failure to reach\nan agreement before the deadline results in 0 payoff. When an agreement is reached, both agents\nobtain the payoff defined by their utility function.\n3.2\nPPO\nWe will use reinforcement learning to optimize the policy πi of our own agent i in the negotiation\nproblem.\nFor simplicity, we will drop the subscript i and simply write π for the policy of our\nown agent. We also simplify by writing o instead of ⟨ot,i, ot−1,i, · · · ⟩. To optimize this policy, we\nuse Proximal Policy Optimisation (PPO) (Schulman et al., 2017) due to its empirical success and\nstability.\nAt each update iteration k, PPO optimises π relative to the last policy πk by maximising the PPO\nclip objective:\nπk+1 ∈arg max\nπ\nEo,a∼πk\n\u0014\nmin\n\u0012 π(a|o)\nπk(a|o)Aπk(o, a), clip\n\u0012 π(a|o)\nπk(a|o), 1 ± ϵ\n\u0013\nAπk(o, a)\n\u0013\u0015\n(3)\nwhere ϵ denotes a clip parameter, and Aπ(a, o) denotes the advantage function of taking action a\nin observation o (Sutton & Barto, 2018). The ratio gets clipped to ensure that the new policy does\nnot change too quickly from the policy at the previous step. Our PPO implementation is based on\nthe CleanRL repository (Huang et al., 2022).\n3.3\nGraph Neural Networks\nWe aim to learn to negotiate across randomly generated problems where the number of objectives\nand values differ. This forces us to design a policy/value network where the shape and number\nof parameters are independent of the number of objectives and values. Networks of linear layers,\noften the default in RL, do not fit this criterion, as they require fixed input dimensions. We chose\nto represent the input of the policy network as a graph and make use of Graph Neural Networks\n(GNN) to deal with the changing size of the input space, more specifically, Graph Attention Networks\n(GAT) (Veličković et al., 2018).\nThe input graph contains nodes that have node features. A layer of GNN encodes the features xu of\nnode u into a hidden representation hu based on the features of the set of neighbour nodes Nu and\non its own features. The specific case of GATs is defined in Equation 4. Here, neighbour features\nare encoded by a linear layer ψ and then weighted summed through a learned attention coefficient\na(xu, xv). The weighted sum is concatenated with xu and passed through another linear layer ϕ to\nobtain the embedding of the node hu.\nhu = ϕ\n \nxu,\nX\nv∈Nu\na(xu, xv) · ψ(xv)\n!\n(4)\n3.4\nImplementation\nAt each timestep, the agent receives observations that are the actions of the opponent in the ne-\ngotiation game. Based on these observations, the agent must select an action. The action space\ncombines multiple discrete actions: the accept action and an action per objective to select one of\nthe values in that objective as an offer. If the policy outputs a positive accept, then the offer action\nbecomes irrelevant as the negotiation will be ended.\nPublished as a conference paper at RLC 2024\nhead node\nobjective nodes\nvalue nodes\nGNNs\nvalue net\nobservation\naccept net\noffer net\naction logits\nFigure 1: Overview of our designed policy network based on GNNs. Observations are encoded in\na graph representation (left) and passed through GNNs. Action distribution logits and state-value\nare obtained by passing the learned representation of the head node and value nodes through linear\nlayers.\nA negotiation problem has objectives B and a set of values to decide on per objective Vb.\nWe\nrepresent the structure of objectives and values as a graph and encode the history of observations\n⟨ot,i, ot−1,i, · · · ⟩of a negotiation game in this structure to a single observation o (see the left side of\nFigure 1). Each objective and value is represented by a node, where value nodes are connected to\nthe objective node to which they belong. An additional head node is added that is connected to all\nobjective nodes. The node features of each node are:\n• 5 features for each value node: the weight wb(vb) of the value, a binary value to indicate the\nopponent’s last offer, a binary value to indicate the last offer of the agent itself, the fraction\nof times this value was offered by the opponent, and the fraction of times this value was\noffered by itself. Note that it might have been better to implement a recurrent network to\ncondition the policy on the full history of offers instead of summary statistics. However,\nthe added computational complexity would have rendered this work much more difficult.\nOur approach enables efficient learning, but future work should explore the use of the raw\nhistory of offers.\n• 2 features for each objective node: the number of values in the value set of this objective\n|Vb|, and the weight of this objective w(b).\n• 2 features for the head node: the number of objectives |B|, and the progress towards the\ndeadline scaled between 0 and 1.\nAs illustrated in Figure 1, we apply GAT layers to the observation graph to propagate information\nthrough the graph and embed the node features (Equation 4). The size of the representation is a\nhyperparameter. We then take the representation of the head node and pass it to a linear layer that\npredicts the state value V . The head representation is also passed through a linear layer to obtain\nthe two accept action logits. Finally, we take the representation of every value node and apply a\nsingle linear layer to obtain the offer action logits. These logits are concatenated per action and used\nto create the probability distribution over the action space. As we use the same linear layer for all\nvalue nodes, the number of output logits is independent of the number of parameters in the policy,\nthus satisfying our requirement. We also note that although the size of the outcome space suffers\nheavily from the curse of dimensionality when the number of objectives increases, our approach does\nnot. Our code implementation can be found on GitHub1.\n1https://github.com/brenting/RL-negotiation/tree/RLC-2024\nPublished as a conference paper at RLC 2024\nName\nType\nDescription\nBoulwareAgent\nTime-dependent\nUtility target decreases concave with time\nConcederAgent\nTime-dependent\nUtility target decreases convex with time\nLinearAgent\nTime-dependent\nUtility target decreases linearly with time\nRandomAgent\nRandom\nMakes random offers, accepts any utility > 0.6\nTable 1: Description of baseline negotiation agents used for benchmarking.\n0\n0.2M\n0.4M\n0.6M\n0.8M\n1M\n1.2M\n1.4M\n1.6M\n1.8M\n2M\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nHiga et al.\nOurs\nSteps\nEpisodic return\nFigure 2: Mean and 99% confidence interval of episodic return during training based on results from\n10 random seeds. The results of the policy designed by Higa et al. (2023) and our policy are plotted.\n4\nEmperical Evaluation\nTo train our agent, we need both negotiation problems and opponents to negotiate against. The ne-\ngotiation problems will be randomly generated with an outcome space size |Ω| between 200 and 1000.\nAs opponents, we use baseline agents and agents developed for the 2022 edition of the Automated Ne-\ngotiation Agents Competition (ANAC). The baseline agents are simple negotiation strategies often\nused within automated negotiation to evaluate and analyse new agents. We provide a description of\nthe opponents in Table 1. All agents were originally developed for the GENIUS negotiation software\nplatform (Lin et al., 2014).\nWe set a negotiation deadline of 40 rounds. An opponent is randomly selected during the rollout\nphase, and a negotiation problem is randomly generated. The policy is then used to negotiate until\nthe episode ends, either by finding an agreement or reaching the deadline. The episode is added to\nthe experience batch, which is repeated until the experience batch is full. We apply 4 layers of GATs\nwith a hidden representation size of 256. A complete overview of the hyperparameter settings can\nbe found in Appendix A.\n4.1\nFixed Negotiation Problem\nAs a first experiment, we compare our method to a recent end-to-end RL method by Higa et al.\n(2023) that can only be used on a fixed negotiation problem. Their method was originally only\ntrained and evaluated against single opponents.\nWe chose to train the agent against the set of\nbaseline players instead, as we consider that a more realistic scenario. The baseline agents show\nrelatively similar behaviour, making this an acceptable increase in difficulty.\nWe generated a single negotiation problem and trained a reproduction of their and our own method\nfor 2 000 000 timesteps on 10 different seeds. The training curve is illustrated in Figure 2, where we\nplot both the mean of the episodic return and the 99% confidence interval based on the results from\n10 training sessions. Every obtained policy is evaluated in 1000 negotiation games against every\nopponent on this fixed negotiation problem. We report the average obtained utility of the trained\npolicy and the opponent, including a confidence interval based on the 10 evaluation runs in Figure 3.\nPublished as a conference paper at RLC 2024\nBoulwareAgent\nConcederAgent\nLinearAgent\nRandomAgent\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n1.1\nHiga et al.\nOpponent\nUtility\nBoulwareAgent\nConcederAgent\nLinearAgent\nRandomAgent\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n1.1\nOurs\nOpponent\nUtility\nFigure 3: Evaluation results of the policy designed by Higa et al. (2023) and our GNN-based policy.\nResults are obtained by evaluating each trained policy for 1000 negotiation games against the set of\nbaseline agents. Mean and 99% confidence interval are plotted based on 10 training iterations.\n0\n0.2M\n0.4M\n0.6M\n0.8M\n1M\n1.2M\n1.4M\n1.6M\n1.8M\n2M\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nCompetition\nBaseline\nSteps\nEpisodic return\nFigure 4: Mean and 99% confidence interval of episodic return during training of our GNN policy\nbased on results from 10 different random seeds. The results from training against the baseline\nagents and training against the competition agents are plotted.\nWe can see in Figure 3 that our method performs similarly to the method proposed by Higa et al.\n(2023). This result is mostly a sanity check that our method can successfully learn to negotiate in\na relatively simple setup despite being more complex and broadly usable.\n4.2\nRandom Negotiation Problems\nWe now evaluate the performance of our end-to-end method on randomly generated negotiation\nproblems. Negotiation problems will continuously change during both training and evaluation.\n4.2.1\nBaseline Opponents\nWe first train and evaluate against the set of baseline agents as described in Table 1. We train our\nmethod for 2 000 000 steps on 10 random seeds. The learning curve is plotted in Figure 4. Results\nare again obtained by running 1000 negotiation sessions against the set of baseline opponents, but\nthis time, all negotiation problems are randomly generated and are never seen before. We note that\nthe observation and action space sizes are constantly changing. Results are plotted in Figure 5a.\nAs seen in Figure 5a, our method performs well against all baseline agents while negotiating on\nvarious structured negotiation problems it has never seen before. It is promising that an end-to-end\nlearned GNN-based policy appears to generalise over such different problems.\n4.2.2\nCompetition Opponents\nWe now repeat the experiment from Section 4.2.1, but increase the set of agents we negotiate\nagainst. More specifically, we add the agents of the 2022 edition of the Automated Negotiation\nAgents Competition (ANAC)2. The learning curve and results are plotted in Figure 4 and Figure 5b,\nrespectively.\n2https://web.tuat.ac.jp/~katfuji/ANAC2022/\nPublished as a conference paper at RLC 2024\nBoulwareAgent\nConcederAgent\nLinearAgent\nRandomAgent\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n1.1\nOurs\nOpponent\nUtility\n(a)\nAgent007\nAgent4410\nAgentFO2\nAgentFish\nChargingBoul\nDreamTeam109Agent\nLuckyAgent2022\nMiCROAgent\nRGAgent\nSmartAgent\nSuperAgent\nThirdAgent\nTjaronchery10Agent\nExploitAgent\nBoulwareAgent\nConcederAgent\nLinearAgent\nRandomAgent\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n1.1\nOurs\nOpponent\nUtility\n(b)\nFigure 5: Evaluation results of our GNN-based policy on randomly generated negotiation problem\nboth against the set of baseline opponents (left) and against the full set of opponents (right). Results\nare obtained by evaluating each trained policy for 1000 negotiation games against the set of agents.\nMean and 99% confidence interval are plotted based on 10 training iterations.\nThe results show much lower performance against all opponents, including those outperformed in\nSection 4.2.1. Our current method of encoding the observations and design of the policy likely leads\nto limited capabilities of learning opponent characteristics. Past work has shown that adapting to\nopponents is important to improve performance (Ilany & Gal, 2016; Sengupta et al., 2021; Renting\net al., 2022), which is currently impossible. However, this goes beyond the core contribution of this\nwork, which is about handling different-sized negotiation problems in end-to-end RL methods. We\ndiscuss potential solutions in Section 5.\n5\nConclusion\nWe developed an end-to-end RL method for training negotiation agents capable of handling differ-\nently structured negotiation problems. We showed that our method performs as well as a recent\nend-to-end method that is not transferrable beyond a single fixed negotiation problem. We see the\nlatter as a serious restriction since, in real-world applications, it would be extremely unlikely to\nencounter the exact same negotiation problem more than once.\nIn our work presented here, for the first time, we have demonstrated how the difficulty of dealing\nwith changing negotiation problems in end-to-end RL methods can be overcome. Specifically, we\nhave shown how an agent can learn to negotiate on diverse negotiation problems in such a way that\nperformance generalises to never-before-seen negotiation problems.\nOur method is conceptually\nsimple compared to previous work on reinforcement learning in negotiation agents.\nOur agent\nperforms well against strong baseline negotiation strategies, but leaves room for improvement when\nnegotiating against a broad set of highly competitive agents.\nOur approach is based on encoding the stream of observations received by our agent into a graph\nwhose node features are designed to capture historical statistics about the episode. This manual\nfeature design likely leads to information loss and goes against the end-to-end aim of our approach.\nFor example, the expressiveness of history is limited as the graph only encodes the last offer and\nfrequency of offers. This likely also causes limited adaptivity to a broad set of opponent strategies,\nwhich in turn causes the low performance observed in Section 4.2.2.\nWe note that, due to the competition setup of ANAC, competitive agents often play a game of\nchicken. Performing well against such strategies means that a policy must also learn this game\nof chicken. This can be challenging for RL, due to exploration problems, as it must learn a long\nsequence of relatively meaningless actions before having a chance to select a good action. We could\nattempt to improve upon this, but it might be more beneficial to prioritize mitigating this game of\nchicken behaviour, as it is inefficient and (arguably) undesirable.\nPublished as a conference paper at RLC 2024\nThe negotiation problems we generated have additive utility functions and a relatively small out-\ncome space, as is quite typical for benchmarks used in automated negotiation research. Real-world\nnegotiation problems, however, can have huge outcome spaces (de Jonge & Sierra, 2015). Our de-\nsigned policy can be applied to larger problems without increasing the trainable parameters, and\nthe effects on the performance of doing this should be investigated in future work.\nFurther promising avenues for future work include extending end-to-end policies with additional\ncomponents that, e.g., learn opponent representations based on the history of observations in the\ncurrent or previous encounter. Changing a negotiation strategy based on the opponent characteristics\nhas been shown previously to improve performance (Ilany & Gal, 2016; Sengupta et al., 2021; Renting\net al., 2022), but is likely difficult to learn through our current policy design. Furthermore, improving\nour method to handle continuous objectives would eliminate the necessity of discretizing them.\nOverall, we believe that in this work, we have taken a substantial step towards the effective use\nof end-to-end RL for the challenging and important problem of training negotiation agents whose\nperformance generalises to new negotiation problems and opens numerous exciting avenues for future\nresearch in this area.\nBroader Impact Statement\nIt is often envisioned that negotiating agents will represent humans or other entities in a future where\nAI is more integrated into society. Having access to more capable negotiation agents could increase\ninequalities in such societies, especially if the development of such agents is a highly skilled endeavour.\nRemoving the human aspect in negotiation might also lead to more self-centred behaviour. We should\nensure that we design for fairness and cooperative behaviour in such systems.\nAcknowledgments\nThis research was (partly) funded by the Hybrid Intelligence Center, a 10-year programme funded\nby the Dutch Ministry of Education, Culture and Science through the Netherlands Organisation for\nScientific Research, grant number 024.004.022 and by EU H2020 ICT48 project“Humane AI Net”\nunder contract # 952026. This research was also partially supported by TAILOR, a project funded\nby the EU Horizon 2020 research and innovation programme under GA No 952215.\nReferences\nReyhan Aydoğan, Tim Baarslag, Katsuhide Fujita, Holger H. Hoos, Catholijn M. Jonker, Yasser\nMohammad, and Bram M. Renting. The 13th International Automated Negotiating Agent Com-\npetition Challenges and Results.\nIn Rafik Hadfi, Reyhan Aydoğan, Takayuki Ito, and Ryuta\nArisaka (eds.), Recent Advances in Agent-Based Negotiation: Applications and Competition Chal-\nlenges, Studies in Computational Intelligence, pp. 87–101, Singapore, 2023. Springer Nature. ISBN\n978-981-9905-61-4. doi: 10.1007/978-981-99-0561-4_5.\nPallavi Bagga, Nicola Paoletti, and Kostas Stathis. Deep Learnable Strategy Templates for Multi-\nIssue Bilateral Negotiation. In Proceedings of the 21st International Conference on Autonomous\nAgents and Multiagent Systems, AAMAS ’22, pp. 1533–1535, Richland, SC, May 2022. Inter-\nnational Foundation for Autonomous Agents and Multiagent Systems. ISBN 978-1-4503-9213-6.\nURL https://ifaamas.org/Proceedings/aamas2022/pdfs/p1533.pdf.\nJasper Bakker, Aron Hammond, Daan Bloembergen, and Tim Baarslag. RLBOA: A Modular Re-\ninforcement Learning Framework for Autonomous Negotiating Agents. In Proceedings of the 18th\nInternational Conference on Autonomous Agents and MultiAgent Systems, AAMAS ’19, pp. 260–\n268, Richland, SC, May 2019. International Foundation for Autonomous Agents and Multiagent\nSystems. ISBN 978-1-4503-6309-9. URL https://www.ifaamas.org/Proceedings/aamas2019/\npdfs/p260.pdf.\nPublished as a conference paper at RLC 2024\nAllan Dafoe, Edward Hughes, Yoram Bachrach, Tantum Collins, Kevin R. McKee, Joel Z. Leibo,\nKate Larson, and Thore Graepel.\nOpen Problems in Cooperative AI, December 2020.\nURL\nhttp://arxiv.org/abs/2012.08630.\nDave de Jonge and Carles Sierra. NB^3: a multilateral negotiation algorithm for large, non-linear\nagreement spaces with limited time. Autonomous Agents and Multi-Agent Systems, 29(5):896–942,\nSeptember 2015. ISSN 1573-7454. doi: 10.1007/s10458-014-9271-3. URL https://doi.org/10.\n1007/s10458-014-9271-3.\nGarett Dworman, Steven O. Kimbrough, and James D. Laing. Bargaining by artificial agents in\ntwo coalition games: a study in genetic programming for electronic commerce. In Proceedings of\nthe 1st annual conference on genetic programming, pp. 54–62, Cambridge, MA, USA, July 1996.\nMIT Press. ISBN 978-0-262-61127-5. URL https://dl.acm.org/doi/abs/10.5555/1595536.\n1595544.\nTorsten Eymann. Co-Evolution of Bargaining Strategies in a Decentralized Multi-Agent System.\nIn symposium on negotiation methods for autonomous cooperative systems, pp. 126–134, January\n2001. ISBN 978-1-57735-137-5.\nRyota Higa, Katsuhide Fujita, Toki Takahashi, Takumu Shimizu, and Shinji Nakadai. Reward-\nbased negotiating agent strategies. In Proceedings of the Thirty-Seventh AAAI Conference on\nArtificial Intelligence and Thirty-Fifth Conference on Innovative Applications of Artificial Intelli-\ngence and Thirteenth Symposium on Educational Advances in Artificial Intelligence, volume 37 of\nAAAI’23/IAAI’23/EAAI’23, pp. 11569–11577. AAAI Press, February 2023. ISBN 978-1-57735-\n880-0. doi: 10.1609/aaai.v37i10.26367. URL https://doi.org/10.1609/aaai.v37i10.26367.\nShengyi Huang, Rousslan Fernand Julien Dossa, Chang Ye, Jeff Braga, Dipam Chakraborty, Ki-\nnal Mehta, and João G. M. Araújo. CleanRL: High-quality Single-file Implementations of Deep\nReinforcement Learning Algorithms. Journal of Machine Learning Research, 23(274):1–18, 2022.\nISSN 1533-7928. URL http://jmlr.org/papers/v23/21-1342.html.\nLitan Ilany and Ya’akov Gal. Algorithm selection in bilateral negotiation. Autonomous Agents and\nMulti-Agent Systems, 30(4):697–723, July 2016. ISSN 1573-7454. doi: 10.1007/s10458-015-9302-8.\nURL https://doi.org/10.1007/s10458-015-9302-8.\nMark Klein and Stephen C. Y. Lu. Conflict resolution in cooperative design. Artificial Intelligence in\nEngineering, 4(4):168–180, October 1989. ISSN 0954-1810. doi: 10.1016/0954-1810(89)90013-7.\nURL https://www.sciencedirect.com/science/article/pii/0954181089900137.\nMinae Kwon, Siddharth Karamcheti, Mariano-Florentino Cuellar, and Dorsa Sadigh. Targeted Data\nAcquisition for Evolving Negotiation Agents. In Proceedings of the 38th International Conference\non Machine Learning, pp. 5894–5904, Virtual, July 2021. PMLR. URL https://proceedings.\nmlr.press/v139/kwon21a.html.\nRaymond Y. K. Lau, Maolin Tang, On Wong, Stephen W. Milliner, and Yi-Ping Phoebe Chen. An\nevolutionary learning approach for adaptive negotiation agents: Research Articles. International\nJournal of Intelligent Systems, 21(1):41–72, January 2006. ISSN 0884-8173.\nMike Lewis, Denis Yarats, Yann Dauphin, Devi Parikh, and Dhruv Batra. Deal or No Deal? End-to-\nEnd Learning of Negotiation Dialogues. In Martha Palmer, Rebecca Hwa, and Sebastian Riedel\n(eds.), Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,\npp. 2443–2453, Copenhagen, Denmark, September 2017. Association for Computational Linguis-\ntics. doi: 10.18653/v1/D17-1259. URL https://aclanthology.org/D17-1259.\nZun Li, Marc Lanctot, Kevin R. McKee, Luke Marris, Ian Gemp, Daniel Hennes, Paul Muller, Kate\nLarson, Yoram Bachrach, and Michael P. Wellman. Combining Tree-Search, Generative Models,\nand Nash Bargaining Concepts in Game-Theoretic Reinforcement Learning, February 2023. URL\nhttp://arxiv.org/abs/2302.00797.\nPublished as a conference paper at RLC 2024\nRaz Lin, Sarit Kraus, Tim Baarslag, Dmytro Tykhonov, Koen Hindriks, and Catholijn M.\nJonker.\nGenius:\nAn Integrated Environment for Supporting the Design of Generic Auto-\nmated Negotiators. Computational Intelligence, 30(1):48–70, 2014. ISSN 1467-8640. doi: 10.\n1111/j.1467-8640.2012.00463.x.\nURL https://onlinelibrary.wiley.com/doi/abs/10.1111/\nj.1467-8640.2012.00463.x.\nBram M. Renting, Holger H. Hoos, and Catholijn M. Jonker.\nAutomated Configuration of\nNegotiation Strategies.\nIn Proceedings of the 19th International Conference on Autonomous\nAgents and MultiAgent Systems, AAMAS ’20, pp. 1116–1124, Auckland, May 2020. International\nFoundation for Autonomous Agents and Multiagent Systems.\nISBN 978-1-4503-7518-4.\nURL\nhttp://www.ifaamas.org/Proceedings/aamas2020/pdfs/p1116.pdf.\nBram M. Renting, Holger H. Hoos, and Catholijn M. Jonker. Automated Configuration and Usage\nof Strategy Portfolios for Mixed-Motive Bargaining. In Proceedings of the 21st International Con-\nference on Autonomous Agents and Multiagent Systems, AAMAS ’22, pp. 1101–1109, Richland,\nSC, May 2022. International Foundation for Autonomous Agents and Multiagent Systems. ISBN\n978-1-4503-9213-6. URL https://ifaamas.org/Proceedings/aamas2022/pdfs/p1101.pdf.\nW.N. Robinson. Negotiation behavior during requirement specification. In [1990] Proceedings. 12th\nInternational Conference on Software Engineering, pp. 268–276, March 1990. doi: 10.1109/ICSE.\n1990.63633.\nJeffrey Solomon Rosenschein.\nRational interaction: cooperation among intelligent agents.\nphd,\nStanford University, Stanford, CA, USA, 1986.\nAriel Rubinstein. Perfect Equilibrium in a Bargaining Model. Econometrica, 50(1):97–109, 1982.\nISSN 0012-9682. doi: 10.2307/1912531. URL https://www.jstor.org/stable/1912531.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.\nProximal\nPolicy Optimization Algorithms, August 2017.\nURL http://arxiv.org/abs/1707.06347.\narXiv:1707.06347 [cs].\nAyan Sengupta, Yasser Mohammad, and Shinji Nakadai. An Autonomous Negotiating Agent Frame-\nwork with Reinforcement Learning based Strategies and Adaptive Strategy Switching Mecha-\nnism. In Proceedings of the 20th International Conference on Autonomous Agents and Multi-\nAgent Systems, AAMAS ’21, pp. 1163–1172, Richland, SC, May 2021. International Founda-\ntion for Autonomous Agents and Multiagent Systems. ISBN 978-1-4503-8307-3. URL https:\n//www.ifaamas.org/Proceedings/aamas2021/pdfs/p1163.pdf.\nL. S. Shapley. Stochastic Games. Proceedings of the National Academy of Sciences, 39(10):1095–\n1100, October 1953. doi: 10.1073/pnas.39.10.1095. URL https://www.pnas.org/doi/abs/10.\n1073/pnas.39.10.1095. Publisher: Proceedings of the National Academy of Sciences.\nSmith. The Contract Net Protocol: High-Level Communication and Control in a Distributed Prob-\nlem Solver. IEEE Transactions on Computers, C-29(12):1104–1113, December 1980. ISSN 1557-\n9956. doi: 10.1109/TC.1980.1675516.\nRichard S. Sutton and Andrew G. Barto. Reinforcement Learning, second edition: An Introduction.\nMIT Press, USA, November 2018. ISBN 978-0-262-35270-3.\nKatia Sycara. Resolving goal conflicts via negotiation. In Proceedings of the Seventh AAAI National\nConference on Artificial Intelligence, AAAI’88, pp. 245–250, Saint Paul, Minnesota, August 1988.\nAAAI Press.\nToki Takahashi, Ryota Higa, Katsuhide Fujita, and Shinji Nakadai. VeNAS: Versatile Negotiating\nAgent Strategy via Deep Reinforcement Learning (Student Abstract). Proceedings of the AAAI\nConference on Artificial Intelligence, 36(11):13065–13066, June 2022. ISSN 2374-3468. doi: 10.\n1609/aaai.v36i11.21669. URL https://ojs.aaai.org/index.php/AAAI/article/view/21669.\nPublished as a conference paper at RLC 2024\nM. Tawfik Jelassi and Abbas Foroughi. Negotiation support systems: an overview of design issues\nand existing software.\nDecision Support Systems, 5(2):167–181, June 1989.\nISSN 0167-9236.\ndoi: 10.1016/0167-9236(89)90005-5. URL https://www.sciencedirect.com/science/article/\npii/0167923689900055.\nPetar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua\nBengio. Graph Attention Networks. February 2018. URL https://openreview.net/forum?id=\nrJXMpikCZ.\nTianpei Yang, Heng You, Jianye Hao, Yan Zheng, and Matthew E. Taylor. A Transfer Approach\nUsing Graph Neural Networks in Deep Reinforcement Learning. Proceedings of the AAAI Confer-\nence on Artificial Intelligence, 38(15):16352–16360, March 2024. ISSN 2374-3468. doi: 10.1609/\naaai.v38i15.29571. URL https://ojs.aaai.org/index.php/AAAI/article/view/29571.\nA\nPPO training hyperparameters\nParameter\nValue\ntotal timesteps\n2 · 106\nbatch size\n6000\nmini batch size\n300\npolicy update epochs\n30\nentropy coefficient\n0.001\ndiscount factor γ\n1\nvalue function coefficient\n1\nGAE λ\n0.95\n# GAT layers\n4\n# GAT attention heads\n4\nhidden representation size\n256\nAdam learning rate\n3 · 10−4\nLearning rate annealing\nTrue\nactivation functions\nReLU\nTable 2: Hyperparameter settings\n",
  "categories": [
    "cs.MA",
    "cs.LG",
    "I.2.11; I.2.6"
  ],
  "published": "2024-06-21",
  "updated": "2024-06-21"
}