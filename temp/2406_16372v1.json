{
  "id": "http://arxiv.org/abs/2406.16372v1",
  "title": "UniPSDA: Unsupervised Pseudo Semantic Data Augmentation for Zero-Shot Cross-Lingual Natural Language Understanding",
  "authors": [
    "Dongyang Li",
    "Taolin Zhang",
    "Jiali Deng",
    "Longtao Huang",
    "Chengyu Wang",
    "Xiaofeng He",
    "Hui Xue"
  ],
  "abstract": "Cross-lingual representation learning transfers knowledge from resource-rich\ndata to resource-scarce ones to improve the semantic understanding abilities of\ndifferent languages. However, previous works rely on shallow unsupervised data\ngenerated by token surface matching, regardless of the global context-aware\nsemantics of the surrounding text tokens. In this paper, we propose an\nUnsupervised Pseudo Semantic Data Augmentation (UniPSDA) mechanism for\ncross-lingual natural language understanding to enrich the training data\nwithout human interventions. Specifically, to retrieve the tokens with similar\nmeanings for the semantic data augmentation across different languages, we\npropose a sequential clustering process in 3 stages: within a single language,\nacross multiple languages of a language family, and across languages from\nmultiple language families. Meanwhile, considering the multi-lingual knowledge\ninfusion with context-aware semantics while alleviating computation burden, we\ndirectly replace the key constituents of the sentences with the above-learned\nmulti-lingual family knowledge, viewed as pseudo-semantic. The infusion process\nis further optimized via three de-biasing techniques without introducing any\nneural parameters. Extensive experiments demonstrate that our model\nconsistently improves the performance on general zero-shot cross-lingual\nnatural language understanding tasks, including sequence classification,\ninformation extraction, and question answering.",
  "text": "UniPSDA: Unsupervised Pseudo Semantic Data Augmentation for\nZero-Shot Cross-Lingual Natural Language Understanding\nDongyang Li1,2, Taolin Zhang2, Jiali Deng1, Longtao Huang2,\nChengyu Wang2, Xiaofeng He1, Hui Xue2\n1School of Computer Science and Technology, East China Normal University\n2Alibaba Group\n{dongyangli0612, jialideng1127}@gmail.com, hexf@cs.ecnu.edu.cn\n{zhangtaolin.ztl, kaiyang.hlt, chengyu.wcy, hui.xueh}@alibaba-inc.com\nAbstract\nCross-lingual representation learning transfers knowledge from resource-rich data to resource-scarce ones to improve\nthe semantic understanding abilities of different languages. However, previous works rely on shallow unsupervised\ndata generated by token surface matching, regardless of the global context-aware semantics of the surrounding text\ntokens. In this paper, we propose an Unsupervised Pseudo Semantic Data Augmentation (UniPSDA) mechanism for\ncross-lingual natural language understanding to enrich the training data without human interventions. Specifically,\nto retrieve the tokens with similar meanings for the semantic data augmentation across different languages, we\npropose a sequential clustering process in 3 stages: within a single language, across multiple languages of a\nlanguage family, and across languages from multiple language families. Meanwhile, considering the multi-lingual\nknowledge infusion with context-aware semantics while alleviating computation burden, we directly replace the key\nconstituents of the sentences with the above-learned multi-lingual family knowledge, viewed as pseudo-semantic.\nThe infusion process is further optimized via three de-biasing techniques without introducing any neural parameters.\nExtensive experiments demonstrate that our model consistently improves the performance on general zero-shot\ncross-lingual natural language understanding tasks, including sequence classification, information extraction, and\nquestion answering.\nKeywords: Cross-Lingual Representation, Data Augmentation, Zero-Shot Learning\n1.\nIntroduction\nCross-lingual representation learning facilitates\nresource-rich information to boost the performance\nof under-resourced languages in various down-\nstream natural language understanding (NLU)\ntasks, such as text classification (Huang, 2022;\nRathnayake et al., 2022; Li et al., 2021), sentiment\nanalysis (Szolomicka and Kocon, 2022; Sazzed,\n2020), information extraction (Huang et al., 2022a;\nAhmad et al., 2021; Wang et al., 2019; Fan et al.,\n2019), and question answering (Limkonchotiwat\net al., 2022; Perevalov et al., 2022). Although exist-\ning cross-lingual works (Li et al., 2023; Clou√¢tre\net al., 2022) share explicit language semantics\nacross different languages, they generally rely on\nsupervised parallel corpora and simple, shallow\nunsupervised mechanisms such as back transla-\ntion (Lam et al., 2022; Nishikawa et al., 2021) and\nrandom deletion (Sun et al., 2022).\nThe previous data augmentation (DA) ap-\nproaches in cross-lingual representation learning\ncan be roughly divided into two categories: super-\nvised parallel data augmenters and unsupervised\nWork done when Dongyang Li was doing an intern-\nship at Alibaba Group. Dongyang Li and Taolin Zhang\ncontributed equally to this work. Correspondence to\nChengyu Wang and Xiaofeng He.\nshallow data augmenters.\n1. Supervised Parallel Data Augmenter: These\nworks (Fernando et al., 2023; Lai et al., 2022;\nRiabi et al., 2021) utilize annotated parallel\ncorpora (e.g., bilingual dictionaries and trans-\nlation tools) to augment the training data by\naligning the same meanings across different\nlanguages for low-resource tasks. However,\nthe collection process for these parallel cor-\npora is time-consuming and relies on human\nannotation.\n2. Unsupervised Shallow Data Augmenter: Un-\nlike the supervised approaches mentioned\nabove, these methods employ unsupervised\neasy data augmentation (EDA) techniques\n(e.g., back translation, random deletion, and\nrandom replacement) to generate additional\ntraining samples for model training (Nishikawa\net al., 2021; Bari et al., 2021; Chen et al., 2021).\nThese methods focus solely on the surface se-\nmantics of the input samples to match cross-\nlingual data without considering the deeper\nlinguistic connections.\nAs shown in Figure 1, techniques like ‚Äúrandom dele-\ntion‚Äù and ‚Äúrandom replacement‚Äù may alter the sen-\ntence‚Äôs intended meaning. Hence, we aim to ex-\npand the multilingual training samples based on a\narXiv:2406.16372v1  [cs.CL]  24 Jun 2024\nApple Computer Inc introduced two PC-\ncompatible ‚Ä¶with the Windows software.\nApple Computer Inc introduced\ntwo PC-compatible computers that \nhave compatibility with the \nWindows software.\nE\nF\nParallel\nCorpora\nSupervised\nI prefer the morning flight.\nI prefer the early flight.\nJe pr√©f√®re le vol du matin.\n‚Ñé1\n‚Ñé4\nùíâùíã\nùíâùüë\nùíâùüê\nLanguage Family\nCluster\nApple Computer Inc eingef√ºhrt\ntwo PC-compatible ‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πåthat \nhave —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å with the \nWindows software.\nBilingual\nDictionary\n. . .\n‚Ä¶\n‚Ñéùëô\n‚Ñé1\n‚Ñé4\nùíâùíã\n‚Ä≤\nùíâùüë\n‚Ä≤\nùíâùüê\n‚Ä≤\n. . .\n‚Ä¶\n‚Ñéùëô\nBack Translation\nRandom Deletion\nÔÅè\nRandom Replacement\nYou prefer the evening flight.\nShallow Semantic\nDeep Semantic\nI prefer the morning flight.\nFrench\nEnglish\nEnglish\nEnglish\nEnglish\nEnglish\nEnglish\nEnglish\nUnsupervised\nPLMs\nFigure 1: Examples of previous data augmentation techniques, including supervised methods that rely on\nparallel data, and unsupervised methods which carry the risk of losing sentential semantic coherence.\ndeep semantic understanding that the model can\nautomatically derive, such as from the hidden lay-\ners of a pre-trained language model (PLM).\nTo overcome the issues mentioned above,\nwe propose an Unsupervised Pseudo Semantic\nData Augmentation (UniPSDA) mechanism, which\nmainly consists of two modules:\n‚Ä¢ Domino Unsupervised Cluster: To provide\nhigh-quality multilingual representations for\nperforming the subsequent deep unsupervised\ndata augmentation, we group languages into\na hierarchical structure organized by language\nfamilies1 to learn multilingual relations. We\nperform the clustering process via the domino\nchain process2 to collect semantically similar\nwords across different languages by compar-\ning the embeddings themselves, a method we\nname Domino Unsupervised Cluster. Specifi-\ncally, the domino cluster is a chain-rule process\ncomprised of three different sequential stages:\nthe single language stage, the language family\nstage, and the multi-language stage.\n‚Ä¢ Pseudo Semantic Data Augmentation: Con-\nsidering that previous data augmentation meth-\nods focus on the surface of naive training sam-\nples, we employ the learned multilingual inter-\nnal representations to address the semantic\ndeficiencies of the training samples. Specif-\nically, the domino clustering-enhanced ulti-\nmate multilingual representations directly re-\nplace the important positions‚Äô hidden states\nin training samples, as recognized by the\n<subject,verb,object> (SVO) structure. The\npotential incompatibility phenomena of insert-\ning clustering multilingual representations may\n1https://www.ethnologue.com/browse/families\n2https://en.wikipedia.org/wiki/Domino_effect\nresult in biased parameter learning. To fur-\nther alleviate the misalignment between the\nreplaced embeddings space and the context\noutput space of PLMs, we introduce three de-\nbiasing optimal transport affinity regularization\ntechniques to make the learning process faster\nand more stable.\n2.\nMethodology\n2.1.\nModel Notations\nThe architecture of UniPSDA is shown in Figure 2.\nThe goal of cross-lingual natural language under-\nstanding is to utilize a source language dataset\nDlang = (Xlang, Ylang) to train a model M. Then\nwe apply the trained model M to tasks in other\ntarget languages Dlang‚Ä≤ = (Xlang‚Ä≤, Ylang‚Ä≤), where X\ndenotes the input samples and Y is the label set.\nIn our work, each sentence of the training data\nis denoted as Si = (wi1, wi2, ¬∑ ¬∑ ¬∑ , wij, ¬∑ ¬∑ ¬∑ , wili),\nwhere wij denotes the j-th word in sentence Si\nand li is the maximum word count of the sentence.\nThe hidden state of word wij is hwij ‚ààR|u|√ód,\nwhere |u| is the maximum number of tokens con-\ntained in the word and d is the dimension of the\nhidden state. The hidden state of sentence Si is\nhsi ‚ààR|Ls|√ód, where |Ls| is the sentence‚Äôs maxi-\nmum token length. The specific notations for the\nthree clustering stages in the Domino Unsupervised\nCluster are as follows:\n‚Ä¢ In the single language stage, the words in the\nm-th single language GSinm are clustered into\n|GSinm| clusters. The t-th cluster is denoted as\nClusin\nmt.\n‚Ä¢ In the language family stage, the words in the\nn-th language family GFamn are clustered into\nùú∂ùüè\nùú∂ùüê\nùú∂ùüë\nùú∂ùüí\nùú∑ùüè\nùú∑ùüê\nSemantic Info.\nm-BERT\nUD\nGMM\nLang. Class\nExpert\nSingle Lang. \nCluster Process\nWord String\nPOS  Tag  Info.\n: Multi Lang. \nCluster\n: Single Lang.\nCluster\n: Lang.  Family\nCluster\nUD : Universal \nDependency\n: GMM Cluster\nMulti-Language \nVocab.\n: word embedding\nLang. Expert\nùú∂ùíä\nùú∑ùíã\nTraining sample: I prefer the morning flight through Denver.\nUniversal  Dependency\nI \n(subject)\nprefer\n(verb)\nflight \n(object)\nmorning through\nDenver\ncandidate\nembedding\nimportant words\nembedding\nm-BERT\nembedding\nI \nprefer\nthe \nmorning \nflight \nthrough \nDenver.\nreplace\nm-BERT\nùêøùëúùë†ùë†= ùúÜ1ùëôùëúùë†ùë†ùë°ùëéùë†ùëò+ ùúÜ2ùëôùëúùë†ùë†ùëúùë°\nreplace\nreplace\n‚ë†Domino Unsupervised Cluster\n‚ë°Pseudo Semantic Data Augmentation\nDomino Chain\nSingle Lang.\nMulti Lang.\nFigure 2: Model Overview of UniPSDA. (Best viewed in color)\n|GFamn| clusters. The g-th cluster is denoted\nas Clufam\nng .\n‚Ä¢ In the multi-language stage, all language fam-\nilies are collected into GMul. All the words in\nGMul are clustered into |GMul| clusters. The q-th\ncluster is denoted as Clumul\nq\n.\n2.2.\nText Encoder\nIn this paper, m-BERT (Devlin et al., 2019) is utilized\nas our encoder3 to obtain the hidden states, which\nare averaged from the embeddings of the first and\nlast layers. The final hidden state of the j-th word\nin sentence Si is formulated as:\nhwij = 1\n2 (Ffirst (wij) + Flast (wij))\n(1)\nwhere Ffirst and Flast denote the representations\nfrom the first and last layers, respectively. We aver-\nage these representations to obtain the sentence‚Äôs\nhidden state hsi.\n3Other multilingual pre-trained language models can\nalso be considered as the backbone.\n2.3.\nDomino Unsupervised Cluster\nTo enable the model to learn relevant word infor-\nmation corresponding to different languages, we\nperform three hierarchical, chain-rule-based clus-\ntering steps, sequentially applied to representations\nof varying language granularities.\n2.3.1.\nSingle Language Cluster\nIn the single language cluster stage, we aim to\ngroup similar words within a specific language. To\nrefine the clustering process, we clarify that ‚Äúsimi-\nlar words‚Äù refers not only to semantic similarity but\nalso to the concordance of part-of-speech (POS)\ntags. For instance, verbs with the meaning of ‚Äúhope‚Äù\nare grouped together, distinct from nouns with sim-\nilar meanings, thereby incorporating lexical POS\nknowledge into the clustering. Initially, we employ\nthe Universal Dependencies4-based PyTorch tool\nStanza5, to obtain the POS tag for each word. The\ntraining data contains 17 types of POS tags, and we\nrepresent each word with a 17-dimensional one-hot\n4https://universaldependencies.org/\n5Stanza is an off-the-shelf cross-lingual linguistic anal-\nysis package. URL: https://stanfordnlp.github.io/\nstanza/\nvector vPOS to signify the initial tag representations.\nThis one-hot vector is then mapped to a context-\naware space by a linear function (WPOSvPOS + bPOS).\nThe final embeddings hfinal\nwij are obtained by con-\ncatenating the original word representations with\nthe projected POS tags:\nhfinal\nwij = [hwij || (W\nPOS v\nPOS + b\nPOS )]\n(2)\nwhere ‚Äú||‚Äù denotes the concatenation operation.\nWords with similar final embeddings are clustered\ntogether using an expectation-maximization algo-\nrithm based on Gaussian Mixture Models (GMM).\nFor the m-th language, we obtain |GSinm| clusters\nat the end of the clustering process:\nClusin\nm1, Clusin\nm2, . . . , Clusin\nm|GSinm | = GMM(\nhw1, hw2, . . . , hw|GSinm |)\n(3)\nwhere |GSinm| is the total number of words in the\nm-th language.\n2.3.2.\nLanguage Family Cluster\nIn the Ethnologue6 linguistic categorical tree, each\nlanguage is considered a leaf node. Language\nfamilies serve as ancestor nodes within this tree\nstructure, and all descendant nodes of a particu-\nlar ancestor node are grouped into the same lan-\nguage family. We aggregate the results of single\nlanguage clusters within a specific language fam-\nily and calculate the expert weight Œ±mt for each\ncluster using a Gate mechanism (Li et al., 2018).\nThis weight is determined by the proportion of each\ncluster‚Äôs word count in relation to the total sam-\nple size. In essence, we incorporate the size in-\nformation of each cluster into the clustering pro-\ncess. The single language stage involves a total\nof Nsin = |Clusin\nm1| + |Clusin\nm2| + ¬∑ ¬∑ ¬∑ + |Clusin\nm|GSinm||\nelements, where |Clusin\nm1| denotes the number of\nelements in cluster Clusin\nm1. Thus, the expert weight\nfor the t-th cluster Œ±mt of language GSinm is de-\nfined as Œ±mt = |Clusin\nmt|\nNsin\n. We represent all expert\nweights across r languages of the n-th language\nfamily GFamn as the matrix Asin. The cluster-center\nembeddings of single language clusters, denoted\nas Censin\nmt, are used in the language family cluster\nstage. The expert-weighted cluster-center embed-\ndings hCensin\nmt ‚ààR|u|√ód are then input into the GMM\nclustering process. The matrix Hsin comprises all\ncluster-center embeddings across r languages of\nthe n-th language family GFamn. GMM clustering\ngroups semantically similar words from r languages\ninto a specific cluster Clufam\nng .\nClufam\nn1 , Clufam\nn2 , . . . , Clufam\nn|GFamn| = GMM(\nele(Asin ‚äôHsin))\n(4)\n6https://www.ethnologue.com/\nwhere the Asin and Hsin matrices are defined as\nfollows:\nAsin =\nÔ£Æ\nÔ£ØÔ£ØÔ£ØÔ£∞\nŒ±11\nŒ±12\n¬∑ ¬∑ ¬∑\nŒ±1|GSin1|\nŒ±21\nŒ±22\n¬∑ ¬∑ ¬∑\nŒ±2|GSin2|\n...\n...\n...\n...\nŒ±r1\nŒ±r2\n¬∑ ¬∑ ¬∑\nŒ±r|GSinr |\nÔ£π\nÔ£∫Ô£∫Ô£∫Ô£ª,\nHsin =\nÔ£Æ\nÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞\nhCensin\n11\nhCensin\n12\n¬∑ ¬∑ ¬∑\nhCensin\n1|GSin1 |\nhCensin\n21\nhCensin\n22\n¬∑ ¬∑ ¬∑\nhCensin\n2|GSin2 |\n...\n...\n...\n...\nhCensin\nr1\nhCensin\nr2\n¬∑ ¬∑ ¬∑\nhCensin\nr|GSinr |\nÔ£π\nÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª\n(5)\nwhere ele() denotes the operation of enumerating\nevery element of the matrix, and ‚äôrepresents the\nelement-wise product.\n2.3.3.\nMulti Languages Cluster\nFinally, we perform clustering on all language family\ncluster-center embeddings obtained from the lan-\nguage family cluster stage. For example, the first\ncluster-center of cluster Clufam\nn1 in the n-th language\nfamily is denoted as Cenfam\nn1 . Each cluster-center\nembedding is associated with a multi-language ex-\npert weight Œ≤ng, computed using the same Gate\nmechanism as in the language family cluster stage.\nWe represent all expert-weight elements across z\nlanguage families of the multi-language pool GMul\nas the matrix Bfam. These expert-weighted cluster-\ncenter embeddings hCenfam\nng ‚ààR|u|√ód are then used\nin the GMM clustering process. The matrix Hfam\ncontains all cluster-center embeddings across the z\nlanguage families of GMul. GMM clustering groups\nsemantically similar words from the z language fam-\nilies into specific clusters, denoted as Clumul\nq\n.\nClumul\n1 , Clumul\n2 , . . . , Clumul\n|GMul| = GMM(\nele(Bfam ‚äôHfam))\n(6)\nwhere the matrices Bfam and Hfam are defined as\nfollows:\nBfam =\nÔ£Æ\nÔ£ØÔ£ØÔ£ØÔ£∞\nŒ≤11\nŒ≤12\n¬∑ ¬∑ ¬∑\nŒ≤1|GSin1|\nŒ≤21\nŒ≤22\n¬∑ ¬∑ ¬∑\nŒ≤2|GSin2|\n...\n...\n...\n...\nŒ≤z1\nŒ≤z2\n¬∑ ¬∑ ¬∑\nŒ≤z|GSinz |\nÔ£π\nÔ£∫Ô£∫Ô£∫Ô£ª,\nHfam =\nÔ£Æ\nÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞\nhCenfam\n11\nhCenfam\n12\n¬∑ ¬∑ ¬∑\nhCenfam\n1|GFam1 |\nhCenfam\n21\nhCenfam\n22\n¬∑ ¬∑ ¬∑\nhCenfam\n2|GFam2 |\n...\n...\n...\n...\nhCenfam\nz1\nhCenfam\nz2\n¬∑ ¬∑ ¬∑\nhCenfam\nz|GFamz |\nÔ£π\nÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª\n(7)\nwhere ele() denotes the operation of enumerating\neach element of the matrix, and ‚äôrepresents the\nelement-wise product.\n2.4.\nPseudo Semantic Data\nAugmentation\nTo enrich the training data with diverse linguistic\ninformation, we augment the model with global mul-\ntilingual semantics obtained from the last domino\nunsupervised cluster module.\n2.4.1.\nPseudo Semantic Replacement\nWe propose two approaches for handling sentence\nsemantics. First, we pass sentences through m-\nBERT to generate embeddings for each sentence.\nSecond, we use Universal Dependencies to extract\nsyntactic parsing trees for the sentences. To guide\nthe model toward learning more accurate repre-\nsentations of crucial sentence words, we focus on\nkey elements identified through syntactic parsing.\nGiven that the subject, verb, and object (SVO) com-\nponents are essential for comprehension in many\ntasks (Dai et al., 2017; Zhang et al., 2017), we treat\nthe SVO as the crucial words of each sentence.\nSubsequently, we mark the position of each SVO\ncomponent within the sentence:\nhsi = [eti1, eti2, . . . , ewiS, . . . ,\newiV , . . . , ewiO, . . . , eti|Ls|]\n(8)\nwhere ewiS, ewiV , and ewiO denote the embeddings\nof the sentence‚Äôs subject, verb, and object compo-\nnents, respectively, and eti1 represents the embed-\nding of the first token in sentence Si. We replace\nthe original sentence‚Äôs SVO word embeddings with\nrandomly selected candidate embeddings from the\nsame cluster. These candidate word embeddings\nshare similar semantics with the SVO words but\ncome from different languages. Through the re-\nplacement of crucial words with cross-lingual knowl-\nedge, we can guide the model to learn more about\nthe critical linguistic elements of sentences and\nachieve better semantic representations. The up-\ndated sentence representation is expressed as:\nh\n‚Ä≤\nsi = [eti1, eti2, . . . , ecan‚Ä≤\niS, . . . ,\necan‚Ä≤\niV , . . . , ecan‚Ä≤\niO, . . . , eti|Ls|]\n(9)\nwhere ecan‚Ä≤\niS, ecan‚Ä≤\niV , and ecan‚Ä≤\niO denote the candi-\ndate embeddings for the subject, verb, and object\ncomponents, respectively. After the embedding\nreplacement, cross-lingual pseudo semantic infor-\nmation is introduced to the training data. We then\nfeed these enhanced representations into trans-\nformer models with base-level parameter sizes to\nrefine the embeddings.\n2.4.2.\nDe-biasing Optimal Transport Affinity\nRegularization\nSpatial misalignment exists between the original\nsentence and the enhanced sentence, as noted\nby (Huang et al., 2022b). To diminish the discrep-\nancy between the replaced sentence embedding\nh\n‚Ä≤\nsi and the original sentence embedding hsi, we in-\ntroduce an integrated regularization term based on\nthe optimal transport mechanism, named Optimal\nTransport Affinity Regularization.\n(1) Wasserstein Distance Abbreviation: To\nalign the space of original sentence representations\nwith that of cross-lingual knowledge-enhanced sen-\ntence representations (Wang and Henao, 2022;\nAlqahtani et al., 2021), we employ optimal trans-\nport (OT ) to facilitate the adjustment process. We\ncalculate a transport plan P that maps the original\nsentence to the augmented sentence with optimal\ncost C ‚ààR|Ls|√ó|Ls|, using the Euclidean distance\n(Danielsson, 1980) between the two sentence rep-\nresentations as a measure of cost:\nC(hsi, h\n‚Ä≤\nsi) =\n d\nX\nj=1\n\f\f\fhsij ‚àíh\n‚Ä≤\nsij\n\f\f\f\n2\n! 1\n2\n(10)\nWe aim to find the optimal transport plan P ‚àà\nR|Ls|√ó|Ls| that minimizes the cost C. This prob-\nlem is formulated as minimizing the p-Wasserstein\ndistance dp‚àíW ass. Due to the high computational\ncomplexity of calculating P, we approximate it using\nthe Sinkhorn algorithm (Altschuler et al., 2017):\nK = exp\n \n‚àíC(hsi, h\n‚Ä≤\nsi)\nŒµ\n!\n(11)\nP(hsi, h\n‚Ä≤\nsi) = diag(u)Kdiag(v)\n(12)\nWe compute u and v iteratively, starting with v(0) =\n1|Ls|, using the following update formulas:\nu(l+1) = a(hsi)\nKv(l) ,\nv(l+1) =\nb(h\n‚Ä≤\nsi)\nKT u(l+1)\n(13)\nwhere a and b are distribution mapping functions.\nThe OT loss can be defined as:\nlossOT = ‚ü®P(hsi, h\n‚Ä≤\nsi), C(hsi, h\n‚Ä≤\nsi)‚ü©\n(14)\nTo mitigate the OT learning biases between the\ntwo sentence representations, we introduce two\nauxiliary de-biasing terms to calibrate the loss.\n(2) De-biasing Eigenvectors Shrinkage: We\nutilize a linear orthogonal mapping parameter W ‚àà\nR|Ls|√ó|Ls| to approximate the replaced embeddings\nto the original ones, hsi ‚âàWh\n‚Ä≤\nsi. Singular value\ndecomposition (SVD) is directly applied to compute\nW (Xing et al., 2015):\nUŒ£VT = SVD(h\n‚Ä≤T\nsi hsi)\n(15)\nW = VUT\n(16)\nWe initialize the linear mapping function with weight\nW to simplify the learning process. However, eigen-\nvectors with small singular values can lead to poor\ntransformations if not suppressed (Chen et al.,\n2019). Thus, we penalize the smallest k singular\nvalues of Œ£, which is ordered by magnitude. The\neigenvectors shrinkage loss is defined as:\nlosseig = ‚àíŒ∑\nk\nX\nr=1\nœÉ2\nr\n(17)\nwhere Œ∑ is a hyper-parameter to control the degree\nof penalty, and œÉr is the r-th smallest singular value.\n(3) De-biasing Distance Shrinkage: To guide\nthe framework‚Äôs learning direction towards minimiz-\ning the discrepancy, we add a term based on the\ndistance between the two embeddings to the loss\nfunction. The distance shrinkage loss is defined\nas:\nlossdis = 1 ‚àísim(hsi, h\n‚Ä≤\nsi)\n(18)\nwhere sim() represents the similarity measure.\nFinally, the auxiliary OT affinity regularization is\ngiven by:\nlossReg = œÅ1lossOT + œÅ2losseig + œÅ3lossdis (19)\nwhere œÅi denotes the controlled weight of each\nregularization component, with the constraint that\nthe sum of œÅi equals 1.\n2.5.\nTraining Objective\nOur training objective combines the task-specific\nloss with the OT affinity regularization. The overall\nobjective function is formulated as:\nlosstotal = Œª1losstask + Œª2lossReg\n(20)\nwhere Œªi controls the relative contribution of each\ncomponent, and the sum of Œªi is constrained to be\n1.\n3.\nExperiments\n3.1.\nTasks and Datasets\nSequence Classification tasks include text classi-\nfication and sentiment analysis. We selected the fol-\nlowing datasets for these tasks: MLDoc (Schwenk\nand Li, 2018) for text classification, and the Multi-\nBooked Catalan and Basque (Barnes et al., 2018)7\nfor sentiment analysis. The evaluation metrics for\nthese tasks are accuracy and macro F1.\nFor Information Extraction, we focus on Rela-\ntion Extraction as a representative task. Here, the\ngoal is to predict the correct relation type present\nin the data. We use the ACE2005 dataset (Walker\net al., 2006), which spans three languages: En-\nglish, Chinese, and Arabic. The performance is\nmeasured using micro F1.\n7We refer to these datasets collectively by the term\n‚ÄúOpeNER‚Äù.\nQuestion Answering involves retrieving an-\nswers for specific questions from a given passage.\nWe conduct experiments on the cross-lingual ques-\ntion answering dataset BiPaR (Jing et al., 2019),\nwhich is commonly used for evaluating such sys-\ntems. The evaluation metrics for this task are Exact\nMatch (EM) and micro F1.\n3.2.\nExperiment Settings\nGiven computational resource constraints, we em-\nploy the base-level version of multilingual BERT\n(m-BERT) to obtain hidden states for words and\nsentences.\nThe encoder consists of 12 Trans-\nformer layers with 12 self-attention heads, and\nthe hidden state dimension is set to 768.\nDur-\ning training, we experiment with learning rates in\n{1e‚àí5, 2e‚àí5, 3e‚àí5, 1e‚àí6, 2e‚àí6, 3e‚àí6}. AdamW\nis chosen as the optimizer, with a learning rate of\n1e ‚àí3 and weight decay of 1e ‚àí5. For the Wasser-\nstein distance, we set p = 1, while the Sinkhorn\nalgorithm‚Äôs control parameter Œµ is 0.1. The last\nk = 300 singular values are used in the De-biasing\nEigenvectors Shrinkage section, with Œ∑ in the losseig\nformula being 0.001. The weight œÅ of lossReg is set\nto {0.4, 0.2, 0.4}, and the Œª of the total loss is set\nto {0.5, 0.5} independently. Statistical results are\nbased on 5 runs, and t-tests confirm that improve-\nments are statistically significant, with p < 0.05 for\nall results.8\n3.3.\nBaselines\nWe compare our approach against a variety of base-\nlines:\nMLDoc (Schwenk and Li, 2018) introduces a\ncross-lingual text classification dataset, with base-\nline results from basic neural network models.\nBLSE (Barnes and Klinger, 2018) presents a\nmodel for the sentiment analysis task, relying on\nsupervised parallel bilingual data.\nLASER (Artetxe and Schwenk, 2019) proposes\na system utilizing a BiLSTM to learn sentence rep-\nresentations across 93 languages, assessed on\nnatural language understanding (NLU) tasks.\nm-BERT (Devlin et al., 2019) offers a language\nmodel pre-trained on over 100 languages, generat-\ning representations for different languages.\nXLM-R (Conneau et al., 2020) is a transformer-\nbased masked language model known for its strong\ncross-lingual performance.\nmUSE (Yang et al., 2020b) is pre-trained in 16\nlanguages to project multilingual corpora into a sin-\ngle semantic space.\nCoSDA-ML (Qin et al., 2020) proposes a model\nusing shallow string surface data augmentation\n8The source code and data are available at https:\n//github.com/MatNLP/UniPSDA\nModel\nen\nde\nzh\nes\nfr\nit\nja\nru\nAverage\nMLDoc\n87.2\n71.7\n73.5\n65.3\n70.2\n65.1\n69.8\n56.9\n69.9(¬±0.7)\nLASER\n86.5\n86.0\n70.4\n71.3\n73.9\n65.6\n58.5\n63.4\n72.0(¬±0.5)\nm-BERT\n92.1\n74.3\n72.5\n67.0\n70.5\n61.8\n69.7\n61.5\n71.2(¬±0.3)\nXLM-R\n90.7\n78.5\n70.3\n66.4\n67.8\n63.9\n64\n64.0\n70.7(¬±0.6)\nZSIW\n91.3\n82.8\n79.6\n71.7\n78.1\n67.0\n68.5\n64.3\n75.4(¬±0.5)\nDAP\n94.1\n86.7\n81.7\n76.2\n84.3\n67.6\n73.9\n66.7\n78.9(¬±0.2)\nSOGOcos\n93.2\n87.0\n81.8\n76.2\n82.5\n68.7\n73.7\n63.9\n78.4(¬±0.1)\nX-STA\n93.8\n86.4\n81.7\n77.2\n84.3\n68.4\n73.4\n64.8\n78.8(¬±0.2)\nCoSDA-ML\n92.4\n79.1\n72.7\n69.9\n74.5\n64.3\n70.6\n66.9\n73.8(¬±0.6)\nUniPSDA\n94.5\n87.1\n82.3\n77.4\n84.4\n69.4\n74.0\n65.5\n79.3(¬±0.2)\nTable 1: General results of text classification in terms of accuracy (%) on the MLDoc dataset.\nto include various language strings in the training\ndata.\nCCCAR (Nguyen et al., 2021) designs a model\nfor information extraction tasks, leveraging datasets\nin three target languages.\nZSIW (Li et al., 2021) introduces a zero-shot\ninstance-weighting model for cross-lingual text clas-\nsification.\nHERBERTa (Seganti et al., 2021) uses an uncon-\nventional two-BERT-model pipeline for information\nextraction.\nX-METRA-ADA (M‚Äôhamdi et al., 2021) employs\nmeta-learning for cross-lingual transfer capability\nenhancement.\nSSDM (Wu et al., 2022) proposes a siamese se-\nmantic disentanglement model to separate syntax\nknowledge across languages.\nLaBSE (Feng et al., 2022) is a BERT-based sen-\ntence embedding model supporting 109 languages.\nDAP (Li et al., 2023) integrates sentence-level\nand token-level dual-alignment for cross-lingual pre-\ntraining.\nSOGOcos (Zhu et al., 2023) employs saliency-\nbased substitution and a novel token-level align-\nment strategy for cross-lingual spoken language\nunderstanding.\nX-STA (Cao et al., 2023) leverages an attentive\nteacher, gradient disentangled knowledge sharing,\nand multi-granularity semantic alignment for cross-\nlingual machine reading comprehension.\n3.4.\nGeneral Experimental Results\nSequence Classification:\nThe results for se-\nquence classification are presented in Table 1 for\ntext classification and Table 2 for sentiment anal-\nysis. We observe that: (1) Our approach outper-\nforms strong baselines and nearly reaches state-of-\nthe-art performance for each task. (2) The perfor-\nmance of the text classification task is significantly\nimproved by leveraging the Domino Cluster to se-\nlect appropriate candidates and injecting pseudo\nsemantic knowledge into critical components of\nModel\nen\neu\nca\nAverage\nBLSE\n86.1\n88.5\n73.9\n82.8(¬±0.4)\nm-BERT\n89.9\n87.9\n75.2\n84.3(¬±0.2)\nmUSE\n88.7\n90.0\n75.1\n84.6(¬±0.3)\nXLM-R\n87.9\n87.6\n71.7\n82.4(¬±0.2)\nDAP\n90.5\n91.7\n74.9\n85.7(¬±0.1)\nSOGOcos\n90.1\n91.2\n75.0\n85.4(¬±0.1)\nX-STA\n90.4\n90.1\n74.9\n85.1(¬±0.1)\nCoSDA-ML\n90.4\n91.6\n74.6\n85.3(¬±0.5)\nUniPSDA\n90.7\n91.9\n75.3\n86.0(¬±0.1)\nTable 2: General experimental results of sentiment\nanalysis in terms of macro F1 (%) and baselines\non the OpeNER dataset.\nthe sentences. We achieve an average accuracy\nof 79.3%, with a particularly notable improvement\nfor French, where accuracy increases by approx-\nimately 10% (from 74.5% to 84.4%) compared to\nthe method proposed by (Qin et al., 2020). (3) In\nthe sentiment analysis task, our UniPSDA model\nboosts the average macro F1 score to 86.0, as\nshown in Table 2. This represents the best reported\nresult for cross-lingual sentiment analysis on the\nOpeNER dataset.\nInformation Extraction: The results for infor-\nmation extraction are shown in Table 3. The find-\nings demonstrate that: (1) Our methodology is ef-\nfective for the relation extraction task, achieving\nmore accurate cross-lingual representations as ev-\nidenced by higher macro F1 scores compared to\nprior work. (2) The enhanced focus on acquiring\npertinent cross-lingual knowledge regarding crucial\nsentence components has led to a solid average\nF1 score of approximately 44.1 in our experiments,\nmarking an improvement of 2.7.\nQuestion Answering: Table 4 presents the per-\nformance of our question answering framework.\nThe results suggest that: (1) Despite the zero-shot\nexperimental setup, the pseudo data augmentation\nModel\nen\nzh\nar\nAverage\nm-BERT\n57.1\n44.8\n21.5\n41.1(¬±0.5)\nXLM-R\n54.9\n45.2\n21.7\n40.6(¬±0.4)\nLaBSE\n55.1\n45.8\n26.6\n42.5(¬±0.2)\nHERBERTa\n54.5\n45.7\n26.4\n42.2(¬±0.3)\nCoSDA-ML\n58.3\n45.5\n20.4\n41.4(¬±0.5)\nDAP\n59.0\n46.2\n26.2\n43.8(¬±0.2)\nSOGOcos\n58.7\n46.2\n26.5\n43.8(¬±0.3)\nX-STA\n57.9\n45.7\n26.2\n43.3(¬±0.1)\nCCCAR\n56.6\n43.9\n18.3\n39.6(¬±0.3)\nUniPSDA\n59.1\n46.3\n26.8\n44.1(¬±0.1)\nTable 3: General experimental results of informa-\ntion extraction in terms of micro F1 (%) and base-\nlines on the ACE2005 dataset.\nModel\nen\nzh\nAverage\nEM\nF1\nEM\nF1\nm-BERT\n31.9\n44.3\n23.5\n26.1\n31.5(¬±0.6)\nXLM-R\n32.3\n45.0\n23.3\n26.2\n31.7(¬±0.3)\nLaBSE\n33.7\n43.4\n24.2\n25.7\n31.8(¬±0.5)\nCoSDA-ML\n34.2\n44.5\n24.7\n25.9\n32.3(¬±0.2)\nX-METRA-ADA\n33.9\n44.9\n23.8\n26.8\n32.4(¬±0.1)\nSSDM\n34.1\n45.8\n24.1\n26.2\n32.6(¬±0.2)\nDAP\n34.3\n45.9\n24.6\n27.1\n33.0(¬±0.3)\nSOGOcos\n34.2\n45.5\n24.7\n27.1\n32.9(¬±0.2)\nX-STA\n33.8\n44.9\n24.2\n26.7\n32.4(¬±0.2)\nUniPSDA\n34.4\n45.7\n25.0\n27.3\n33.1(¬±0.2)\nTable 4: General experimental results of question\nanswering and baselines on the BiPaR dataset.\nmechanism employed by our framework demon-\nstrates a robust transfer capability. This translates\nto effective performance on the BiPaR dataset, with\nour work producing more accurate representations\nthan most of the baselines. (2) The scores obtained\nin the two languages evaluated affirm the efficacy\nof UniPSDA. However, our F1 scores for English\nare lower than those achieved by SSDM (Wu et al.,\n2022) and DAP (Li et al., 2023). This discrepancy\ncan be attributed to the fact that SSDM and DAP\nutilize specific parallel data for training, which was\nnot the case in our approach.\n4.\nDetailed Analysis\n4.1.\nAblation Study\nIn our ablation study, we independently remove key\ncomponents‚Äînamely, the Domino Unsupervised\nCluster module and the Pseudo Semantic Data\nAugmentation module‚Äîto evaluate their individual\ncontributions to the framework‚Äôs performance. The\nresults of the ablation experiments are presented\nin Table 5. We draw two main conclusions: (1)\nThe Domino Unsupervised Cluster module is cru-\nModel\nOpeNER\nMLDoc\nACE05\nBiPaR\nUniPSDA\n86.0\n79.3\n44.1\n45.7\n-Dom. Unsup.\n85.8\n74.9\n41.9\n45.1\n-Pse. Seman.\n85.1\n73.7\n41.1\n44.8\n-Aff. Regul.\n84.2\n71.1\n40.9\n44.0\nTable 5: Ablation study of our work on four datasets.\n‚Äú-‚Äù means returning to the original setting.\n0.2\n0.3\n0.4\n0.5\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nDomino Unsupervised Cluster\n0.2\n0.3\n0.4\n0.5\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nK-Means\nFigure 3: The words representations of five different\nsemantics after t-SNE dimensional reduction.\ncial for generating precise representations, while\nthe Pseudo Semantic Data Augmentation module\nsignificantly enhances the model‚Äôs performance by\nproviding additional cross-lingual information. (2)\nThe absence of the cluster module leads to a notice-\nable decline in performance across all downstream\ntasks. Specifically, in text classification, accuracy\nfalls by 4.4% (from 79.3% to 74.9%). This indicates\nthat clustering based on semantic embeddings is\nmore beneficial to the model than clustering based\non shallow string representations. The removal of\nthe Pseudo Semantic Data Augmentation module\nalso results in a marked decrease in performance\ndue to the lack of cross-lingual knowledge.\n4.2.\nInfluence of Domino Unsupervised\nCluster\nWe employ t-SNE (van der Maaten and Hinton,\n2008) to project the high-dimensional word repre-\nsentations into a two-dimensional space, facilitating\nthe visualization of the embeddings. The result-\ning plots compare word embeddings clustered by\nthe Domino Unsupervised Cluster and the naive\nK-Means algorithm. As depicted in Figure 3, the\ndomino unsupervised cluster gathers similar word\nembeddings more compactly, whereas the naive\nK-Means approach results in a more diffuse distri-\nbution of similar word embeddings.\n0\n2\n4\n6\n8\n10\nepoch\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nF1-Score\npseudo semantic repl.\nrandom word emb.\nrandom word str.\nFigure 4: Results comparison of different data aug-\nmentation skills in Pseudo Semantic Data Augmen-\ntation module.\n4.3.\nThe Influence of Pseudo Semantic\nData Augmentation\nTo examine the effect of Pseudo Semantic Data\nAugmentation on the information extraction task,\nwe experiment with three distinct replacement\nstrategies on the English test set. The comparative\nresults are illustrated in Figure 4.\nObservations indicate that replacements using\nrandom word strings or random word embeddings\nare less effective than those leveraging pseudo se-\nmantic methods. The pseudo semantic data aug-\nmentation approach demonstrates a superior abil-\nity to preserve the semantic integrity of sentences,\nleading to more meaningful augmentations and po-\ntentially better model performance.\n5.\nRelated Work\n5.1.\nCross-Lingual Pre-trained Models\nRecent cross-lingual pre-trained language models\n(PLMs) can be categorized into two groups:\n1. Monolingual Training Data Models: Multilin-\ngual BERT (m-BERT) (Devlin et al., 2019) and\nXLM-R (Conneau et al., 2020) utilize mono-\nlingual corpora for training with a masked lan-\nguage modeling task.\n2. Multilingual Training Data Models: Exten-\nsions of XLM-R by Jiang et al. (2022); H√§m-\nmerl et al. (2022); Chi et al. (2022); Barbi-\neri et al. (2022) demonstrate improvements\nwith high-quality static embedding alignments.\nTools facilitating bilingual language alignment\n(Tran et al., 2020; Chi et al., 2021; Yang et al.,\n2020a; Schuster et al., 2019) enable the learn-\ning of additional languages. These models\noften depend on parallel data and alignment\ntools to enrich the corpus diversity.\n5.2.\nCross-Lingual Data Augmentations\nCross-lingual data augmentation approaches are\ntypically divided into:\n1. Supervised Data Augmentation: Methods\nsuch as CoSDA-ML (Qin et al., 2020) and\nMulDA (Liu et al., 2021) utilize parallel cor-\npora to integrate knowledge from other lan-\nguages. Dong et al. (2021) employ parallel\nlanguage alignments for shared representa-\ntional spaces.\n2. Unsupervised Data Augmentation: Tech-\nniques like adversarial training and cross-\nlingual sample generation are employed by\nRiabi et al. (2021); Bari et al. (2021); Dong\net al. (2021); Guo et al. (2021) to improve\nmultilingual model performance. Nishikawa\net al. (2021) use back translation for enhancing\nword embeddings, while Cheng et al. (2022)\nreplace words based on a probabilistic distri-\nbution. Chen et al. (2021) focus on sentence\nselection from low-resource languages. These\nmodels tend to prioritize surface string varia-\ntions, often overlooking the rich, context-aware\nsemantics.\nWe address this limitation by incorporating global\ncross-lingual semantics into monolingual training\ndata, thereby enriching the diversity of language\nknowledge.\n6.\nConclusion\nIn this work, we introduce UniPSDA, an unsuper-\nvised data augmentation mechanism that leverages\nsemantic embeddings to enrich cross-lingual nat-\nural language understanding (NLU) tasks with di-\nverse linguistic information. The Domino Unsuper-\nvised Cluster module identifies semantically similar\ncross-lingual content, while the Pseudo Semantic\nData Augmentation module injects context-aware\nsemantics into the training corpus. Furthermore,\naffinity regularization serves to minimize the repre-\nsentational gap between original and augmented\nsentences. Through extensive experimentation,\nour methods demonstrate superior performance rel-\native to other strong baselines, underscoring their\neffectiveness in enhancing cross-lingual NLU.\nAcknowledgements\nWe would like to thank anonymous reviewers for\ntheir valuable comments. This work was supported\nin part by National Key R&D Program of China\n(No. 2022ZD0120302) and Alibaba Group through\nAlibaba Research Intern Program.\nBibliographical References\nWasi Uddin Ahmad, Nanyun Peng, and Kai-Wei\nChang. 2021. GATE: graph attention transformer\nencoder for cross-lingual relation and event ex-\ntraction. In AAAI, pages 12462‚Äì12470.\nSawsan Alqahtani, Garima Lalwani, Yi Zhang, Sal-\nvatore Romeo, and Saab Mansour. 2021. Using\noptimal transport as alignment objective for fine-\ntuning multilingual contextualized embeddings.\nIn Findings of EMNLP, pages 3904‚Äì3919.\nJason M. Altschuler, Jonathan Weed, and Philippe\nRigollet. 2017. Near-linear time approximation\nalgorithms for optimal transport via sinkhorn iter-\nation. In NeurIPS, pages 1964‚Äì1974.\nMikel Artetxe and Holger Schwenk. 2019. Mas-\nsively multilingual sentence embeddings for zero-\nshot cross-lingual transfer and beyond. Trans.\nAssoc. Comput. Linguistics, 7:597‚Äì610.\nFrancesco Barbieri, Luis Espinosa Anke, and Jos√©\nCamacho-Collados. 2022. XLM-T: multilingual\nlanguage models in twitter for sentiment analysis\nand beyond. In LREC, pages 258‚Äì266.\nM. Saiful Bari, Tasnim Mohiuddin, and Shafiq R.\nJoty. 2021. UXLA: A robust unsupervised data\naugmentation framework for zero-resource cross-\nlingual NLP. In ACL, pages 1978‚Äì1992.\nJeremy Barnes, Toni Badia, and Patrik Lambert.\n2018. Multibooked: A corpus of basque and\ncatalan hotel reviews annotated for aspect-level\nsentiment classification. In LREC.\nJeremy Barnes and Roman Klinger. 2018. Bilingual\nsentiment embeddings: Joint projection of sen-\ntiment across languages. In ACL, pages 2483‚Äì\n2493.\nTingfeng Cao, Chengyu Wang, Chuanqi Tan, Jun\nHuang, and Jinhui Zhu. 2023. Sharing, teaching\nand aligning: Knowledgeable transfer learning\nfor cross-lingual machine reading comprehen-\nsion. In Findings of EMNLP, pages 455‚Äì467.\nXinyang Chen, Sinan Wang, Bo Fu, Mingsheng\nLong, and Jianmin Wang. 2019. Catastrophic for-\ngetting meets negative transfer: Batch spectral\nshrinkage for safe transfer learning. In NeurIPS,\npages 1906‚Äì1916.\nYanda Chen, Chris Kedzie, Suraj Nair, Petra\nGalusc√°kov√°, Rui Zhang, Douglas W. Oard, and\nKathleen R. McKeown. 2021. Cross-language\nsentence selection via data augmentation and\nrationale training. In ACL, pages 3881‚Äì3895.\nQiao Cheng, Jin Huang, and Yitao Duan. 2022. Se-\nmantically consistent data augmentation for neu-\nral machine translation via conditional masked\nlanguage model. In COLING, pages 5148‚Äì5157.\nZewen Chi, Li Dong, Bo Zheng, Shaohan Huang,\nXian-Ling Mao, Heyan Huang, and Furu Wei.\n2021.\nImproving pretrained cross-lingual lan-\nguage models via self-labeled word alignment.\nIn ACL, pages 3418‚Äì3430.\nZewen Chi, Shaohan Huang, Li Dong, Shuming\nMa, Bo Zheng, Saksham Singhal, Payal Bajaj,\nXia Song, Xian-Ling Mao, Heyan Huang, and\nFuru Wei. 2022. XLM-E: cross-lingual language\nmodel pre-training via ELECTRA. In ACL, pages\n6170‚Äì6182.\nLouis Clou√¢tre, Prasanna Parthasarathi, Amal\nZouaq, and Sarath Chandar. 2022. Detecting\nlanguages unintelligible to multilingual models\nthrough local structure probes. In Findings of\nEMNLP, pages 5375‚Äì5396.\nAlexis Conneau, Kartikay Khandelwal, Naman\nGoyal, Vishrav Chaudhary, Guillaume Wenzek,\nFrancisco Guzm√°n, Edouard Grave, Myle Ott,\nLuke Zettlemoyer, and Veselin Stoyanov. 2020.\nUnsupervised cross-lingual representation learn-\ning at scale. In ACL, pages 8440‚Äì8451.\nBo Dai, Yuqi Zhang, and Dahua Lin. 2017. De-\ntecting visual relationships with deep relational\nnetworks. In CVPR, pages 3298‚Äì3308.\nPer-Erik Danielsson. 1980.\nEuclidean distance\nmapping. Computer Graphics and image pro-\ncessing, 14(3):227‚Äì248.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training\nof deep bidirectional transformers for language\nunderstanding. In NAACL, pages 4171‚Äì4186.\nXin Dong, Yaxin Zhu, Zuohui Fu, Dongkuan Xu,\nand Gerard de Melo. 2021. Data augmentation\nwith adversarial training for cross-lingual NLI. In\nACL, pages 5158‚Äì5167.\nYan Fan, Chengyu Wang, Boxing Chen, Zhongkai\nHu, and Xiaofeng He. 2019. SPMM: A soft piece-\nwise mapping model for bilingual lexicon induc-\ntion. In SDM, pages 244‚Äì252. SIAM.\nFangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen\nArivazhagan, and Wei Wang. 2022. Language-\nagnostic BERT sentence embedding. In ACL,\npages 878‚Äì891.\nAloka Fernando, Surangika Ranathunga, Dilan\nSachintha, Lakmali Piyarathna, and Charith Ra-\njitha. 2023. Exploiting bilingual lexicons to im-\nprove multilingual embedding-based document\nand sentence alignment for low-resource lan-\nguages. Knowl. Inf. Syst., 65(2):571‚Äì612.\nYingmei Guo, Linjun Shou, Jian Pei, Ming Gong,\nMingxing Xu, Zhiyong Wu, and Daxin Jiang. 2021.\nLearning from multiple noisy augmented data\nsets for better cross-lingual spoken language un-\nderstanding. In EMNLP, pages 3226‚Äì3237.\nKatharina H√§mmerl, Jindrich Libovick√Ω, and Alexan-\nder Fraser. 2022. Combining static and contex-\ntualised multilingual embeddings. In Findings of\nACL, pages 2316‚Äì2329.\nKuan-Hao Huang, I-Hung Hsu, Prem Natarajan,\nKai-Wei Chang, and Nanyun Peng. 2022a. Multi-\nlingual generative language models for zero-shot\ncross-lingual event argument extraction. In ACL,\npages 4633‚Äì4646.\nLang Huang, Shan You, Mingkai Zheng, Fei\nWang, Chen Qian, and Toshihiko Yamasaki.\n2022b. Learning where to learn in cross-view\nself-supervised learning. In CVPR, pages 14431‚Äì\n14440.\nXiaolei Huang. 2022. Easy adaptation to mitigate\ngender bias in multilingual text classification. In\nNAACL, pages 717‚Äì723.\nXiaoze Jiang, Yaobo Liang, Weizhu Chen, and Nan\nDuan. 2022. XLM-K: improving cross-lingual lan-\nguage model pre-training with multilingual knowl-\nedge. In AAAI, pages 10840‚Äì10848.\nYimin Jing, Deyi Xiong, and Yan Zhen. 2019. Bipar:\nA bilingual parallel dataset for multilingual and\ncross-lingual reading comprehension on novels.\nIn EMNLP, pages 2452‚Äì2462.\nHuiyuan Lai, Antonio Toral, and Malvina Nissim.\n2022. Multilingual pre-training with language and\ntask adaptation for multilingual text style transfer.\nIn ACL, pages 262‚Äì271.\nTsz Kin Lam, Shigehiko Schamoni, and Stefan Rie-\nzler. 2022. Sample, translate, recombine: Lever-\naging audio alignments for data augmentation in\nend-to-end speech translation. In ACL, pages\n245‚Äì254.\nChangliang Li, Liang Li, and Ji Qi. 2018. A self-\nattentive model with gate mechanism for spo-\nken language understanding. In EMNLP, pages\n3824‚Äì3833.\nIrene Li, Prithviraj Sen, Huaiyu Zhu, Yunyao Li,\nand Dragomir R. Radev. 2021. Improving cross-\nlingual text classification with zero-shot instance-\nweighting. In ACL, pages 1‚Äì7.\nZiheng Li, Shaohan Huang, Zihan Zhang, Zhi-Hong\nDeng, Qiang Lou, Haizhen Huang, Jian Jiao,\nFuru Wei, Weiwei Deng, and Qi Zhang. 2023.\nDual-alignment pre-training for cross-lingual sen-\ntence embedding. In ACL, pages 3466‚Äì3478.\nPeerat Limkonchotiwat, Wuttikorn Ponwitayarat,\nCan\nUdomcharoenchaikit,\nEkapol\nChuang-\nsuwanich, and Sarana Nutanong. 2022.\nCl-\nrelkt: Cross-lingual language knowledge transfer\nfor multilingual retrieval question answering. In\nNAACL, pages 2141‚Äì2155.\nLinlin Liu, Bosheng Ding, Lidong Bing, Shafiq R.\nJoty, Luo Si, and Chunyan Miao. 2021. Mulda:\nA multilingual data augmentation framework for\nlow-resource cross-lingual NER. In ACL/IJCNLP,\npages 5834‚Äì5846.\nMeryem M‚Äôhamdi, Doo Soon Kim, Franck Der-\nnoncourt, Trung Bui, Xiang Ren, and Jonathan\nMay. 2021. X-METRA-ADA: cross-lingual meta-\ntransfer learning adaptation to natural language\nunderstanding and question answering.\nIn\nNAACL, pages 3617‚Äì3632.\nMinh Van Nguyen, Tuan Ngo Nguyen, Bonan Min,\nand Thien Huu Nguyen. 2021. Crosslingual trans-\nfer learning for relation and event extraction via\nword category and class alignments. In EMNLP,\npages 5414‚Äì5426.\nSosuke Nishikawa, Ryokan Ri, and Yoshimasa Tsu-\nruoka. 2021. Data augmentation with unsuper-\nvised machine translation improves the structural\nsimilarity of cross-lingual word embeddings. In\nACL-IJCNLP, pages 163‚Äì173.\nAleksandr Perevalov, Andreas Both, Dennis Diefen-\nbach, and Axel-Cyrille Ngonga Ngomo. 2022.\nCan machine translation be a reasonable alter-\nnative for multilingual question answering sys-\ntems over knowledge graphs? In WWW, pages\n977‚Äì986.\nLibo Qin, Minheng Ni, Yue Zhang, and Wanxi-\nang Che. 2020. Cosda-ml: Multi-lingual code-\nswitching data augmentation for zero-shot cross-\nlingual NLP. In IJCAI, pages 3853‚Äì3860.\nHimashi Rathnayake, Janani Sumanapala, Ravee-\nsha Rukshani, and Surangika Ranathunga. 2022.\nAdapter-based fine-tuning of pre-trained multilin-\ngual language models for code-mixed and code-\nswitched text classification.\nKnowl. Inf. Syst.,\n64(7):1937‚Äì1966.\nArij Riabi, Thomas Scialom, Rachel Keraron,\nBeno√Æt Sagot, Djam√© Seddah, and Jacopo Sta-\niano. 2021.\nSynthetic data augmentation for\nzero-shot cross-lingual question answering. In\nEMNLP, pages 7016‚Äì7030.\nSalim Sazzed. 2020. Cross-lingual sentiment clas-\nsification in low-resource bengali language. In\nEMNLP, pages 50‚Äì60.\nTal Schuster, Ori Ram, Regina Barzilay, and Amir\nGloberson. 2019. Cross-lingual alignment of con-\ntextual word embeddings, with applications to\nzero-shot dependency parsing. In NAACL, pages\n1599‚Äì1613.\nHolger Schwenk and Xian Li. 2018. A corpus for\nmultilingual document classification in eight lan-\nguages. In LREC.\nAlessandro\nSeganti,\nKlaudia\nFirlag,\nHelena\nSkowronska, Michal Satlawa, and Piotr An-\ndruszkiewicz. 2021. Multilingual entity and re-\nlation extraction dataset and model. In EACL,\npages 1946‚Äì1955.\nXin Sun, Tao Ge, Shuming Ma, Jingjing Li, Furu\nWei, and Houfeng Wang. 2022. A unified strat-\negy for multilingual grammatical error correction\nwith pre-trained cross-lingual language model. In\nIJCAI, pages 4367‚Äì4374.\nJoanna Szolomicka and Jan Kocon. 2022. Multi-\naspectemo: Multilingual and language-agnostic\naspect-based sentiment analysis.\nIn ICDM,\npages 443‚Äì450.\nChau Tran, Yuqing Tang, Xian Li, and Jiatao Gu.\n2020. Cross-lingual retrieval for iterative self-\nsupervised training. In NIPS.\nLaurens van der Maaten and Geoffrey Hinton. 2008.\nVisualizing data using t-sne. pages 2579‚Äì2605.\nChristopher Walker, Stephanie Strassel, Julie\nMedero, and Kazuaki Maeda. 2006. Ace 2005\nmultilingual training corpus. Linguistic Data Con-\nsortium, Philadelphia, 57:45.\nChengyu Wang, Yan Fan, Xiaofeng He, and Aoying\nZhou. 2019. A family of fuzzy orthogonal pro-\njection models for monolingual and cross-lingual\nhypernymy prediction. In WWW, pages 1965‚Äì\n1976. ACM.\nRui Wang and Ricardo Henao. 2022. Wasserstein\ncross-lingual alignment for named entity recogni-\ntion. In ICASSP, pages 8342‚Äì8346.\nLinjuan Wu, Shaojuan Wu, Xiaowang Zhang, Deyi\nXiong, Shizhan Chen, Zhiqiang Zhuang, and\nZhiyong Feng. 2022.\nLearning disentangled\nsemantic representations for zero-shot cross-\nlingual transfer in multilingual machine reading\ncomprehension. In ACL, pages 991‚Äì1000.\nChao Xing, Dong Wang, Chao Liu, and Yiye Lin.\n2015. Normalized word embedding and orthog-\nonal transform for bilingual word translation. In\nNAACL, pages 1006‚Äì1011.\nJian Yang,\nShuming Ma,\nDongdong Zhang,\nShuangzhi Wu, Zhoujun Li, and Ming Zhou.\n2020a. Alternating language modeling for cross-\nlingual pre-training. In AAAI, pages 9386‚Äì9393.\nYinfei Yang, Daniel Cer, Amin Ahmad, Mandy Guo,\nJax Law, Noah Constant, Gustavo Hern√°ndez\n√Åbrego, Steve Yuan, Chris Tar, Yun-Hsuan Sung,\nBrian Strope, and Ray Kurzweil. 2020b. Multi-\nlingual universal sentence encoder for semantic\nretrieval. In ACL, pages 87‚Äì94.\nHanwang Zhang, Zawlin Kyaw, Jinyang Yu, and\nShih-Fu Chang. 2017. PPR-FCN: weakly super-\nvised visual relation detection via parallel pair-\nwise R-FCN. In ICCV, pages 4243‚Äì4251.\nZhihong Zhu, Xuxin Cheng, Zhiqi Huang, Dong-\nsheng Chen, and Yuexian Zou. 2023. Enhancing\ncode-switching for cross-lingual SLU: A unified\nview of semantic and grammatical coherence. In\nEMNLP, pages 7849‚Äì7856.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2024-06-24",
  "updated": "2024-06-24"
}