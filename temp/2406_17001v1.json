{
  "id": "http://arxiv.org/abs/2406.17001v1",
  "title": "Deep Learning for Prediction and Classifying the Dynamical behaviour of Piecewise Smooth Maps",
  "authors": [
    "Vismaya V S",
    "Bharath V Nair",
    "Sishu Shankar Muni"
  ],
  "abstract": "This paper explores the prediction of the dynamics of piecewise smooth maps\nusing various deep learning models. We have shown various novel ways of\npredicting the dynamics of piecewise smooth maps using deep learning models.\nMoreover, we have used machine learning models such as Decision Tree\nClassifier, Logistic Regression, K-Nearest Neighbor, Random Forest, and Support\nVector Machine for predicting the border collision bifurcation in the 1D normal\nform map and the 1D tent map. Further, we classified the regular and chaotic\nbehaviour of the 1D tent map and the 2D Lozi map using deep learning models\nlike Convolutional Neural Network (CNN), ResNet50, and ConvLSTM via cobweb\ndiagram and phase portraits. We also classified the chaotic and hyperchaotic\nbehaviour of the 3D piecewise smooth map using deep learning models such as the\nFeed Forward Neural Network (FNN), Long Short-Term Memory (LSTM), and Recurrent\nNeural Network (RNN). Finally, deep learning models such as Long Short-Term\nMemory (LSTM) and Recurrent Neural Network (RNN) are used for reconstructing\nthe two parametric charts of 2D border collision bifurcation normal form map.",
  "text": "Deep Learning for Prediction and Classifying the\nDynamical Behaviour of Piecewise Smooth Maps\nVismaya V S1, Bharath V Nair1, and Sishu Shankar Muni1\n1School of Digital Sciences,, Digital University Kerala,\nThiruvananthapuram, PIN 695317, Kerala, India\nJune 26, 2024\nAbstract\nThis paper explores the prediction of the dynamics of piecewise smooth maps us-\ning various deep learning models.\nWe have shown various novel ways of predicting\nthe dynamics of piecewise smooth maps using deep learning models.\nMoreover, we\nhave used machine learning models such as Decision Tree Classifier, Logistic Regres-\nsion, K-Nearest Neighbor, Random Forest, and Support Vector Machine for predicting\nthe border collision bifurcation in the 1D normal form map and the 1D tent map.\nFurther, we classified the regular and chaotic behaviour of the 1D tent map and the\n2D Lozi map using deep learning models like Convolutional Neural Network (CNN),\nResNet50, and ConvLSTM via cobweb diagram and phase portraits. We also classified\nthe chaotic and hyperchaotic behaviour of the 3D piecewise smooth map using deep\nlearning models such as the Feed Forward Neural Network (FNN), Long Short-Term\nMemory (LSTM), and Recurrent Neural Network (RNN). Finally, deep learning models\nsuch as Long Short-Term Memory (LSTM) and Recurrent Neural Network (RNN) are\nused for reconstructing the two parametric charts of 2D border collision bifurcation\nnormal form map.\n1\nIntroduction\nIn the study of complex systems, it is important to understand how the state of that system\nchanges with time to predict and explain the dynamical behaviour of the system. A dynamical\nsystem is a system that changes and [1] evolves over time. For a wide range of applications\nranging from predicting weather patterns, controlling complex machinery, and modelling\nbiological systems [2], understanding the behaviour of dynamical systems is important. In\naddition, the analysis of dynamical systems has applications in fields such as economics [3],\nfinance [4], ecology [5], and engineering [6], where the study of system behaviour can lead\nto the development of more efficient and effective strategies. For instance, in the field of\n1\narXiv:2406.17001v1  [cs.LG]  24 Jun 2024\necology, the study of population dynamics has been crucial in predicting and managing the\nimpact of human activities on wildlife populations. Dynamical systems theory [1] is a field\nthat provides the mathematical tools to analyse the evolution of such systems, which can\nexhibit either continuous or discrete behaviour.\nIn many domains, such as dynamical systems theory [7], physics [8], and engineering, pre-\ndicting and classifying various dynamical behaviour of piecewise smooth maps is important.\nPiecewise smooth (PWS) maps [9] are mathematical models showing continuous and discon-\ntinuous behaviours. At the boundaries between the regions, the map shows discontinuities\nand continuous behaviour within each region. The combination of these continuous regions\nand discontinuities can lead to complex dynamics, including phenomena like bifurcations [10],\nchaos [11], etc. There are different types of PWS systems. They are classified based on De-\ngree of Smoothness(DoS). When DoS equal to 1, such systems are called impacting systems\nor piecewise smooth hybrid systems [12], for example, oscillators in machines. When DoS\nequal 1, such systems are called Filippov systems [13], for example, relay feedback systems.\nWhen DoS equal to 2 or more, such systems are called piecewise smooth continuous sys-\ntems. Border collision bifurcation [14] is a distinct type of bifurcation occurring in piecewise\nsmooth maps when a fixed point, periodic point, chaotic attractor, or any attractor collides\nwith the boundary region. Border collision bifurcation [15] occurs when the parameter of the\nsystem is varied, resulting in a fixed point or periodic orbit colliding with the boundary, and\na discontinuous change will occur. Most of the bifurcations seen in the power converters are\nborder collision bifurcations [16]. Unlike other bifurcations like saddle-node (fold) bifurcation\n[17], transcritical bifurcation [18], pitchfork bifurcation [19], etc, border collision bifurcation\ncan lead to sudden changes. For instance, a stable fixed point can suddenly become unstable\nand bifurcate to a chaotic attractor [20]. This feature is not seen in smooth maps. Border\ncollision bifurcations are important to understand the evolution of piecewise smooth systems\n[21] [22], especially in the context of stability analysis and control systems. For example,\na study in the dynamics of automatic control systems with pulse-width modulation of the\nsecond kind (PWM-2) [23]. Predicting border collision bifurcations is important for control-\nling the dynamical behaviour of the systems that exhibit piecewise smooth dynamics [24].\nIn recent decades, studies have been conducted to understand the bifurcations in piecewise\nsmooth maps [25].\nThe piecewise smooth map can show a wide range of dynamical behaviours such as regular\n[26], chaotic [27] and hyperchaotic behaviours [28]. Understanding the behaviour like regular,\nchaotic [24] and hyperchaos are important for various applications like control systems [29],\ncryptography [30], etc. In engineering systems like robotic arms, mechanical systems with\nimpacts, and electrical circuits with switching elements are modelled using piecewise smooth\nmaps [9]. In control systems, piecewise smooth maps are used to model and analyze systems\nwith switching controllers such as sliding mode control [31]. Piecewise smooth models are also\nused in signal processing to handle signals with abrupt changes and discontinuity [32]. They\nare also used in biological systems like understanding heart rhythms [33], brain functions [34],\nin economics and social sciences [35] and in environmental models such as climate predictions,\netc.\nUsing machine learning to predict the border collision bifurcation is a novel approach\n[36]. Nowadays, Deep Learning techniques are becoming increasingly essential in various\n2\nfields, including climate forecasting [37], healthcare [38], virtual assistants [39], finance [40],\ne-commerce [41], agriculture [42], automotive [43] and transportation [44], security [45], etc.\nDeep learning has emerged as a powerful tool in the field of dynamical systems [46] [47]. Tra-\nditional methods in dynamical systems frequently depend on analytical methods that might\nbe difficult to handle complex, high-dimensional, or nonlinear processes. Deep learning tech-\nniques like Feed Forward Neural Network (FNN) [48], Long short-term Memory (LSTM) [49],\nRecurrent Neural Network (RNN) [50], and Convolutional Neural Network (CNN) [51] are\nsome of the techniques commonly used. Deep Learning techniques are useful in predicting\nand forecasting the future state of dynamcial systems like weather forecasting [52]. How-\never, analysing vast datasets, especially those involving time series data, presents notable\nchallenges.\nThe present paper explores predicting the dynamics of piecewise smooth maps like border\ncollision bifurcation using machine learning models such as Decision Tree Classifier, Logistic\nRegression, K-Nearest Neighbor, Random Forest and Support Vector Machine and classifica-\ntion of regular and chaotic behaviour using deep learning models like Convolutional Neural\nNetwork (CNN), ResNet50, and ConvLSTM via cobweb diagram and phase portraits. For\nthe classification of chaotic and hyperchaotic behaviour using deep learning techniques, var-\nious techniques are used, such as Feed Forward Neural Network (FNN), Recurrent Neural\nNetwork (RNN), Long short-term Memory (LSTM), Convolutional Neural Network (CNN),\nResNet50, etc.\nFinally, we used deep learning models to reconstruct the two parameter\ncharts. The study shows that it is important to understand the dynamics of the system for\napplication in various fields.\nThe novelty of the paper includes the following:\n• Regular, chaotic and hyperchaotic dynamical behaviour of piecewise smooth maps are\nclassified using deep learning models like FeedForward Neural Network (FNN), Long\nShort-Term Memory (LSTM) and Recurrent Neural Network (RNN).\n• Beyond deep learning models, the paper also includes machine learning models such as\nDecision Tree Classifier, Logistic Regression, K-Nearest Neighbor, Random Forest and\nSupport Vector Machine for the border collision bifurcation prediction tasks.\n• In the paper, machine learning models are used for predicting the higher periods of the\nperiodic orbits.\n• Deep learning models employ cobweb and phase portraits of 1D tent map and 2D Lozi\nmap to classify regular and chaotic dynamical behaviour.\n• Deep learning models are used to reconstruct the two-parameter charts in the paper.\nThe paper is organised as follows: Section 2 explores piecewise smooth maps like 1D\nborder collision normal form map and the 1D tent map and predicts the border collision\nbifurcations. Section 3 includes the prediction of border collision bifurcation using differ-\nent machine learning models, which include Decision Tree Classifier, Logistic Regression,\nK-Nearest Neighbor, Random Forest and Support Vector Machines. In section 4, the classifi-\ncation of the chaotic and regular behaviour of the 1D tent map and the 2D lozi map is done\n3\nusing deep learning models, namely Convolutional Neural Network (CNN), ResNet50, and\nConvLSTM. In Section 5, the classification of the chaotic and hyperchaotic behaviour of 3D\npiecewise smooth map using deep learning models like FeedForward Neural Networks (FNN),\nLong Short-Term Memory (LSTM) networks, and Recurrent Neural Networks (RNN), is dis-\ncussed. In section 6, two parameter charts are reconstructed using deep learning models.\nFinally, Section 7 includes the conclusion and future scopes.\n2\nBorder Collision Bifurcation in Piecewise Smooth\nMap\nHere, we considered the two simplest piecewise smooth maps, i.e., 1D normal form map and\n1D tent map.\n2.1\nNormal Form of One-Dimensional Piecewise Smooth Map\nThe normal form of 1D piecewise smooth map [53] is defined as follows:\nxn+1 =\n(\naxn + µ\nfor xn < 0\nbxn + µ + l\nfor xn > 0,\n(2.1)\nwhere xn represents the state of the system at time step n, xn+1 represents the state of the\nsystem at time step n + 1, µ represents the parameter affecting the system’s behavior, and l\nrepresents the length of discontinuity.\nL (left) and R (right) are the two sides of the state space in the normal form map. The\nfixed point L is located at\nx∗\nL =\nµ\n1 −a\n(2.2)\nand the fixed point R is located at\nx∗\nR = µ + l\n1 −b\n(2.3)\nWhen µ = 0, the fixed point x∗\nL collides with the border at x = 0. And when µ = l, the fixed\npoint x∗\nR collide with the border at x = 0. As a result, when the parameter µ is changed, we\nshould anticipate two border collision events at parameter µ = 0 and µ = l.\nFig:1 shows the one-parameter border collision bifurcation diagram of the normal form\nmap (2.1) at a = 0.5,b = 0.5, l = −0.1 and µ ∈(−0.1, 0.2) where x-axis represents the\nparameter µ and y-axis represents the state variable x. The figure shows that the behaviour\nof the normal form map changes as the parameter µ is varied from −0.1 to 0.2. The location\nof the fixed point varies as the parameter µ is changed between −0.1 and 0.2. At specific\nµ values, new dynamical behaviours occur as a result of the fixed points colliding with the\nborder, i.e., (x = 0). For example, when µ = −0.1, there is a single stable fixed point as\nin fig:1. As µ increased, the fixed point changes linearly with µ and becomes unstable at\nµ = 0. A sequence of dots on the diagram shows the emergence of a new periodic orbit\nat the border collision bifurcation point. The periodic orbit experiences several bifurcations\n4\nFigure 1: One-parameter border collision bifurcation diagram of normal form map (2.1) where\nx-axis presents the parameter µ and y-axis represents the state variable x, µ = (−0.1, 2).\nlike period-doubling bifurcation as µ increases, resulting in additional periodic orbits and\nchaotic behaviour. Based on the value of the parameter µ, the diagram shows the position\nand stability of various orbits.\nBecause of the presence of the periodic behaviour, we want to know the period of the\norbit x for a particular value of the parameter µ. It’s important to know if there is a periodic\nbehaviour. Detecting the periodic behaviour includes simulating the system’s behaviour and\nthen observing the trajectory. If the trajectory returns close to its starting point after a\ncertain number of iterations, then it shows a periodic behaviour. To determine the period,\nwe will count the number of iterations required to complete one cycle. In this way, we can\ndetermine the period of the orbit x for a particular parameter value µ.\nFigure 2 shows a one-parameter diagram of period vs µ. The x-axis represents the value\nof the parameter µ, and the y-axis represents the period. In the figure, the period of the orbit\nx for every value of µ is represented by blue. The sudden changes in the period at specific\nvalues of µ lead to a sequence of discrete points, which is why the line is not smooth. The\nfigure displays multiple areas with varying dynamical behaviours as the parameter µ changes.\nFor µ starting from −0.1, the period is constant, i.e., 1, but when the value of parameter\nµ increases, the period suddenly increases to 9, this happens because of the occurrence of\nthe border collision bifurcation. Then, the period varies till it approaches µ = 0.1, then it\nsuddenly decreases to µ = 1 because of the presence of several border collision bifurcations.\nNext, we want to know at which value of µ border collision bifurcation occurs.\nFor\nconvenience, we start with period = 1 or fixed point and then apply the condition, i.e.,\nfor which the value of µ, x converges to 0 or collides with the border.\nFinally, we got\ntwo values of µ having period 1, which shows border collision bifurcation; they are µ =\n1.3877787807814457e −17 and µ = 0.10000000000000003 as shown in the Fig:3 denoted\n5\n0.10\n0.05\n0.00\n0.05\n0.10\n0.15\n0.20\n1\n2\n3\n4\n5\n6\n7\n8\n9\nPeriod\nFigure 2: One-parameter Period vs µ diagram of normal form map (2.1) where x-axis repre-\nsents µ values and y-axis represents periods, µ varies from −0.10 to 0.20 and period varies\nfrom 1 to 9.\nby grey vertical lines. We also observe the occurrence of the border collision bifurcation of\nperiodic orbits of higher periods see fig:1. However, computational techniques are required to\npredict the border collision bifurcation. Here, we use machine learning techniques to predict\nthe border collision bifurcation of the period orbits having higher periods in Section 3.\nNext, we try to understand the border collision bifurcation in another prototypical piece-\nwise smooth map, i.e., the 1D tent map.\n2.2\n1D Tent Map\nThe 1D tent map is a simple but attractive illustration of a piecewise chaotic dynamical\nsystem [54]. The tent map is defined by the following equation:\nxn+1 =\n(\nµxn\nfor xn < 0.5\nµ(1 −xn)\nfor xn > 0.5,\n(2.4)\nwhere xn+1 represents the value of the state variable x at the next iteration n + 1, xn\nrepresents the current value of the state variable x at iteration n, and µ is the parameter\nthat determines the dynamical behaviour of the map.\nFigure 4 shows the one-parameter border collision bifurcation diagram µ vs x of the 1D\ntent map.\nThe values of the parameter µ are represented on the x-axis and range from\n−1.5 to 1.5. The values of x are represented on the y-axis. Starting from an initial value of\nx0 = 0.1, each point on the figure indicates the value of x after 10000 iterations of the tent\nmap. The tent map exhibits different dynamical behaviour when the µ value varies from\n6\nFigure 3: One-parameter border collision bifurcation diagram of normal form map (2.1) show-\ning the µ values at which border collision bifurcation occur i.e., µ = 1.3877787807814457e−17\nand µ = 0.10000000000000003 denoted by grey vertical lines.\nFigure 4: One-parameter border collision bifurcation diagram of the 1D tent map where\nx-axis represent the parameter µ and y-axis represent the state variable x, µ = (−1.5, 1.5).\n7\n−1.5 to 1.5. When µ changes from −1.5 to −1.0, the dynamical behaviour of the 1D tent\nmap changes from chaotic to periodic or fixed point and when µ varies from 1.0 to 1.5, the\ndynamical behaviour again becomes chaotic.\nBecause of the presence of periodic behaviour, we want to know the period of orbit x for\na particular value of µ. Detecting periodic behaviour is similar to the previous technique\nmentioned in section 2.1. In this way, we can determine the period of x for a particular value\nof µ.\n1.0\n0.5\n0.0\n0.5\n1.0\n2\n4\n6\n8\n10\n12\n14\n16\nPeriod\nFigure 5: One parameter period vs µ diagram of tent map where where x-axis represents the\nparameter µ values and y-axis represents the periods, µ varies from −1.0 to 1.0 and period\nvaries from 2 to 16.\nFigure 5 shows a one-parameter period vs µ diagram that shows the relationship between\nthe parameter µ and the period of the 1D tent map. The figure shows that as µ varies from\n−1.0 to 1.0, the period increases gradually for each step. There is a sudden increase in the\nperiod at certain values of parameter µ because of the presence of border collision bifurcation.\nFinally, the parameter value µ is predicted where the border collision bifurcation occurs,\ni.e., for which the value of the parameter µ, x converges to 0.5 or collides the border. And\nwe got two values of µ having period 1 which shows border collision bifurcation,they are\nµ = −0.9984984984984985 and µ = 0.9984984984984986 as shown in Fig:6 in grey vertical\nlines.\n3\nPrediction Using Machine Learning Models\nMachine Learning models are used to predict border collision bifurcation. The aim is to\nknow which model is best for prediction in both Normal Form and Tent Map. Five machine\nlearning models are used. They are:\n8\nFigure 6: One-parameter border collision bifurcation diagram of 1D tent map showing the\nparameter µ values at which border collision bifurcation occur i.e., µ = −0.9984984984984985\nand µ = 0.9984984984984986 denoted by grey vertical lines.\n1. Decision Tree Classifier [55]\n2. Logistic Regression [56]\n3. K-Nearest Neighbors [57]\n4. Random Forest [58]\n5. Support Vector Machine [59]\n3.1\nPrediction of the border collision bifurcation of 1D Normal\nForm Map\nThe data is generated in such a way that it simulates equation (2.1) across various values of\nthe parameter µ. It observes how the system’s state evolves for each parameter µ, identifying\nperiodic orbits and recording details such as µ value, final state, and period. To create the\ntraining and testing set, the original data are divided into features X and target y. The\ndata is split into 80% for training and 20% for testing. All the models are trained to predict\nthe border collision bifurcation from training data.\nAfter training, the accuracy of each\nmodel is determined by comparing the predictions of the model with the actual results of the\nexperimental data. The models and their corresponding accuracy are shown in table 1.\nThe figure:7 is an accuracy vs model plot that compares the accuracy of different ma-\nchine learning models. The x-axis represents the models, which are Decision Tree Classifier\n(DTC), Random Forest (RF) and K-Nearest Neighbors (KNN), and the y-axis represents the\n9\nDTC\nLR\nKNN\nRF\nSVM\nModels\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\nAccuracy\nFigure 7: Accuracy vs model plot. The five models: Decision Tree Classifier (DTC), Logistic\nRegression (LR), K-Nearest Neighbors (KNN), Random Forest (RF) and Support Vector\nMachine (SVM) and their corresponding accuracy in the prediction of border collision bifur-\ncation.\n10\nModels\nDTC\nLR\nKNN\nRF\nSVM\nAccuracy\n0.965\n0.685\n0.955\n0.975\n0.685\nTable 1: Models: Decision Tree Classifier (DTC), Logistic Regression (LR), K-Nearest Neigh-\nbor (KNN), Random Forest (RF), Support Vector Machine (SVM) and their corresponding\naccuracy.\naccuracy. From the figure, we can conclude that Random Forest has high accuracy and is\nthe best machine learning model for predicting the border collision bifurcation of 1D normal\nform map.\nDTC\nRF\nKNN\nML Models\n1\n2\n3\n4\n5\n6\n7\n9\nPeriods\n0.33\n0.33\n0.33\n0.33\n0.33\n0.34\n0.32\n0.32\n0.36\n0.36\n0.32\n0.32\n0.35\n0.35\n0.29\n0.14\n0.29\n0.57\n0.57\n0.29\n0.14\n0.00\n1.00\n0.00\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure 8: Heatmap of period vs predicted value (1D Normal Form) from the models(Decision\nTree Classifier (DTC), Random Forest (RF) and K-Nearest Neighbors (KNN).\nThe heatmap 8 shows the relative performance of the Decision Tree Classifier (DTC),\nRandom Forest (RF) and K-Nearest Neighbors (KNN) across periodic orbits of different\nperiods. The data is normalised in such a way that all the predicted values will come in one\nrange. Darker colour indicates larger values, and the figure illustrates each model’s relative\nperformance or importance over the period. The different colours represent the magnitude of\nthe normalized data values. In the heatmap, lower values are represented by yellow and higher\nvalues by green. The heatmap displays each machine learning model’s relative performance\nor importance (Decision Tree Classifier, Random Forest and K-Nearest Neighbors) across\nthe period because the data has been normalised. Variations in each model’s performance\nare indicated by colour changes. The light green colours in DTC and RF equivalent cells\ncompared to KNN show that they outperform KNN in periods 1, 2, and 3. Period 4 shows that\nDTC outperforms KNN, with RF performing similarly. DTC and RF perform comparably\nin period 5, which is marginally better than KNN. In periods 6 and 7, KNN and RF perform\nequally, whereas DTC performs poorly. When comparing KNN, RF and DTC respective\n11\ncells, the darker green colour indicates that RF performs better in period 9. Thus, we can\nconclude that the Random Forest (RF) is most reliable and consistent in predicting outcomes\nacross all periods.\n3.2\nPrediction of the border collision bifurcation of 1D Tent Map\nThe data is generated in such a way that equation (2.4) is simulated for the different values\nof the parameter µ and the remaining processes are similar to as mentioned in section 3.1.\nThe machine learning models and their corresponding accuracy are shown in the table 2\nModels\nDTC\nLR\nKNN\nRF\nSVM\nAccuracy\n0.992\n0.95\n0.985\n0.978\n0.971\nTable 2: Models: Decision Tree Classifier(DTC), Logistic Regression(LR), K-Nearest Neigh-\nbor(KNN), Random Forest(RF), Support Vector Machine(SVM) and their corresponding\naccuracy in predicting the border collision bifurcation.\nDTC\nLR\nKNN\nRF\nSVM\nModels\n0.95\n0.96\n0.97\n0.98\n0.99\nAccuracy\nFigure 9: Accuracy vs model plot. The five models: Decision Tree Classifier (DTC), Logistic\nRegression (LR), K-Nearest Neighbors (KNN), Random Forest (RF) and Support Vector\nMachine (SVM) and their corresponding accuracy in the prediction of border collision bifur-\ncation.\nThe figure:9 is an accuracy vs model plot that compares the accuracy of different machine\nlearning models in predicting border collision bifurcation. The x-axis represents the models,\nwhich are Decision Tree Classifier (DTC), Random Forest (RF) and K-Nearest Neighbors\n(KNN) and y-axis represents the accuracy. From the figure, we can conclude that the Decision\n12\nTree Classifier have high accuracy and is the best machine learning model for predicting\nborder collision bifurcation of 1D tent map.\nAlso, from the figure, we can see that apart from the Decision Tree Classifier, Random\nForest (RF) and K-Nearest Neighbors (KNN) show accuracy similar to the Decision Tree\nClassifier. So, we take these three models and plot a period vs predicted value heatmap.\nDTC\nRF\nKNN\nML Models\n1\n4\n8\n16\nPeriods\n0.33\n0.33\n0.33\n0.29\n0.43\n0.29\n0.50\n0.17\n0.33\n0.25\n0.38\n0.38\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50\nFigure 10: Heatmap of period vs predicted value (1D tent map) from the models(Decision\nTree Classifier (DTC), Random Forest (RF) and K-Nearest Neighbors (KNN).\nFig:10 shows the heatmap distribution of three different machine learning models (DTC,\nRF, and KNN) over five different periods (1, 2, 4, 8, and 16). The data has been normalised\nso that the sum of each row equals 1, and the colour hues in the heatmap show the relative\nproportion of each number in a row. All three models have high values, with DTC having\nthe greatest value, as seen in the first period.\nThis implies that while all three models\ndo well in training, DTC has a tiny advantage.\nIt is clear that as we move into higher\nperiods, each model’s value distribution varies. Concerning its performance, DTC shows\nlower values for higher periods. In contrast, the RF and KNN models have lower values for\nlower periods, but their performance improves with increased periods. The RF model, in\nparticular, performs well during training, as seen by its highest value throughout the most\nrecent period.\nTherefore, we can conclude that random forest is best for predicting the\nperiods of the periodic orbit.\nBorder collision bifurcation can lead to sudden changes like a fixed point after colliding\nwith the boundary, which can bifurcate to a chaotic attractor. So, there is a need to predict\nthe regular and chaotic behaviour of the piecewise smooth map, which will be discussed in\nthe next section.\n13\n4\nClassification of Chaotic and Regular Behaviour of\nPiecewise Smooth Map\nWe considered two piecewise smooth maps, 1D Tent Map and 2D Lozi map, for classifying\nthe regular and chaotic dynamical behaviour [60].\n4.1\n1D Tent Map\nThe 1D tent map shows both chaotic and regular dynamical behaviour. For classification of\nthis behaviour, the data is generated in such a way that the equation (2.4) is simulated for\ndifferent values of the parameter µ. The Lyapunov exponent is computed for 1000 randomly\nselected value of r in the range of −1.5 to 1.5. If the Lyapunov exponent is negative, then\nthe behaviour is regular; if the Lyapunov exponent is positive, then the behaviour is chaotic.\nCobweb diagrams are produced for the different values of r.\nThe cobweb diagram is an\neffective visualization tool for comprehending the behaviour of iterative maps. Each map\niteration points (x, f(x)) and (f(x), x) are plotted and connected with a line to generate the\nfinal result. The cobweb diagrams are generated separately based on the positive (chaotic)\nor negative (non-chaotic) Lyapunov exponent. The images and their corresponding labels (0\nfor regular and 1 for chaotic) are stored as labels in the dataset.\nFigure 11: Comparision of regular and chaotic behaviour of Tent Map using cobweb diagram\nwith the three models (CNN, ResNet50 and ConvLSTM). For (i) r = −0.53 (regular) and\n(ii) r = −1.37 (chaotic).\n4.2\n2D Lozi Map\nThe 2D Lozi map is a discrete dynamical system defined by the following equation:\nxn+1 = 1 −a|xn| + yn\nyn+1 = bxn\n(4.1)\n14\nHere, (xn, yn) represents the current state of the system, (xn+1, yn+1) denotes the next\nstate, and a and b are parameters that determine the behavior of the system.\nThe data is generated by simulating the system equation (4.1) for a ∈(−0.1, 1.7), while\nkeeping the parameter b = 0.5 constant. For each value of a, the system iterates the map\nfunction for 10000 iterations, starting with random initial values for x and y trajectories are\nshown. To measure the system’s sensitivity to initial conditions, the Lyapunov exponents\nare calculated for the longer duration of each iteration.\nThe chaotic (positive exponent) or regular (negative exponent) dynamical behaviour of\nthe 2D Lozi map is determined using the Lyapunov exponent. Additionally, the trajectory\nis plotted for each a to create a phase portrait, visually representing the system’s state\nevolution. The Lyapunov exponent categorises the phase portraits as chaotic or regular. A\ndataset of trajectories and Lyapunov exponents is produced by repeating this approach over\nthe entire range of a.\nFigure 12: Comparision of regular and chaotic behaviour of Lozi Map using phase portrait\nwith the three models (CNN, ResNet50 and ConvLSTM). For (i) r = 1.10 (regular) and (ii)\nr = 1.68 (chaotic).\n4.3\nDeep Learning Models\nThe dataset is splitted into training (70%) and testing (30%). Three deep learning architec-\ntures are used to classify chaotic and regular dynamical behaviour. They are Convolutional\nNeural Network (CNN), ResNet50 and ConvLSTM.\n• Convolutional Neural Network (CNN)\nThe first layer is a convolutional layer with 32 neurons, 3 × 3 kernel, and the ReLu\nactivation function is used. The second layer is a max pooling layer of 2 × 2, which\nreduces the spatial dimensions of the input neuron. The third layer is a flattening layer,\nwhich reshapes the 2D output into a 1D array. The fourth layer is a fully connected layer\nwith 256 neurons and with ReLU activation function. The fifth layer has a dropout rate\n15\nof 0.5, which prevents overfitting. The sixth layer is a fully connected layer with 512\nneurons and the ReLU activation function used. The final layer is a fully connected\nlayer with 2 neurons and a softmax activation function, which produces the output\npredictions for the model and converts the output values into possibilities that sum to\n1, allowing the model to predict multiple classes.\nInput Layer\nConvolutional + ReLU\nMax Pooling\nDense Layer\nOutput Layer\nFigure 13: CNN architecture showing how regular and chaotic behaviour are classified. Cob-\nweb diagram r = −1.37 is given as input and prediction (chaotic (1) or regular (0) ) are given\nas output.\n• ResNet50\nDefines a sequential model that includes multiple layers. The first layer is a pre-trained\nResNet50 model used as a feature extractor and set to be non-trainable. The second\nlayer is a global common pooling layer, which reduces the spatial dimensions of the\ninput. The third and fourth layers are fully connected layers with 256 neurons and a\nReLU activation function. The fifth layer has a dropout rate of 0.5, which prevents\noverfitting.\nThe final layer is a fully connected layer with 2 neuron and a sigmoid\nactivation function, producing the binary type classification output predictions.\n• ConvLSTM\nDefines a sequential model that includes multiple layers. The first layer is a TimeDis-\ntributed convolutional layer with 32 neurons and a kernel size of (3 × 3), which applies\nthe convolution operation to the step of the input collection. The second layer is a\nTimeDistributed max pooling layer with a pool length of (2 × 2), which reduces the\nspatial dimensions of the input at every time step. The third layer is a TimeDistributed\nflattening layer, which reshapes the 3D tensor into a 2D tensor. The fourth layer is an\nLSTM layer with 64 units, which learns patterns over time in the input sequence. The\nfinal layer is a fully connected layer with 2 neuron and a sigmoid activation function,\nwhich produces the output predictions for the binary classification task.\n16\nCNN\nResNet50\nConvLSTM\nLoss\nAccuracy\nLoss\nAccuracy\nLoss\nAccuracy\nTraining\n0.0001\n1.0\n0.1741\n1.0\n0.6323\n0.6739\nValidation\n0.000002\n1.0\n0.0977\n0.9955\n0.6739\n0.6797\nTest\n0.00000045\n1.0\n0.0866\n0.9960\n0.6215\n0.6901\nTable 3: Loss and Accuracy (%) values for different deep learning models, CNN, ResNet50\nand ConvLSTM of 1D Tent Map.\nCNN\nResNet50\nConvLSTM\nLoss\nAccuracy\nLoss\nAccuracy\nLoss\nAccuracy\nTraining\n0.0222\n1.0\n0.5607\n0.7216\n0.6475\n0.6853\nValidation\n0.0114\n1.0\n0.2903\n0.1\n0.6327\n0.7179\nTest\n0.0113\n1.0\n0.6367\n0.6851\n0.6609\n0.6363\nTable 4: Loss and Accuracy (%) values for different deep learning models, CNN, ResNet50\nand ConvLSTM of 2D Lozi map.\nThe Adam optimiser and the binary cross-entropy loss function are used for the construction\nof all models. A batch size of 32, the model is trained for 50 epochs. After training, the\naccuracy is evaluated.\nTable 3 and 4 shows the accuracy and loss of test, validation, and training datasets for the\nthree deep learning methods (CNN, ResNet50, and ConvLSTM). When we compare the test\ndata results, CNN appears to provide better results than the other two networks. In addition,\nwe calculated the accuracy in the test set of regular and chaotic samples, yielding the results\nshown in Table 5 and 6. The fact that both regular and chaotic sample percentages are 100%\nindicates that CNN has accurately learned the key characteristics of both kinds of dynamical\nbehaviour.\nWe discussed the regular and chaotic behaviour of 1D and 2D piecewise smooth maps.\nNow, we will discuss the hyperchaotic behaviour of the 3D piecewise smooth map.\nCNN\nResNet50\nConvLSTM\nAccuracy\nAccuracy\nAccuracy\nChaotic\n1.0\n0.987\n0\nRegular\n1.0\n1.0\n1.0\nTable 5: Accuracy of different deep learning models, CNN, ResNet50 and ConvLSTM for\nchaotic and regular behaviours of Tent Map.\n17\nCNN\nResNet50\nConvLSTM\nAccuracy\nAccuracy\nAccuracy\nChaotic\n1.0\n0\n0\nRegular\n1.0\n1.0\n1.0\nTable 6: Accuracy of different deep learning models, CNN, ResNet50 and ConvLSTM for\nchaotic and regular behaviours of Lozi Map.\nFigure 14: Epoch vs Loss in training and validation datasets of (i) Tent Map and (ii) Lozi\nMap for (a) CNN, (b) ResNet50, (c) ConvLSTM; There are two lines, blue representing the\ntraining loss and red representing the validation loss.\n18\nFigure 15: Epoch vs Accuracy in training and validation datasets of i Tent Map and ii Lozi\nMap for (a) CNN, (b) ResNet50, (c) ConvLSTM; There are two lines, blue representing the\ntraining accuracy and red representing the validation accuracy.\n5\nClassification of Chaotic and Hyperchaotic Behaviour\nof Piecewise Smooth Map\n5.1\n3D Piecewise Smooth Map\nThe three-dimensional piecewise smooth normal form map [28] is given by\nXn+1 =\n(\nAlXn + µC,\nif Xn ≤0\nArXn + µC,\nif Xn ≥0\n(5.1)\nwhere Xn = (xn, yn, zn)T ∈R3, C = (1, 0, 0)T ∈R3, Al and Ar are real valued 3 × 3\nmatrices Al =\n\n\nτl\n1\n0\n−σl\n0\n1\nδl\n0\n0\n\nand Ar =\n\n\nτr\n1\n0\n−σr\n0\n1\nδr\n0\n0\n\n\nFor a range of δr values (−1.05 to −0.85), the equation is simulated to produce data. For\neach δr, there are defined matrices Al and Ar that depend on parameters τl = −0.5, σl = 0.95,\nδl = 0.2, τr = 0.8, σr = −0.6 and µ = 0.1, which determine the dynamics of the system.\nAfter being initialised at x, it iterates through 10000 steps where the state is updated by the\nsystem’s map function, calculating Lyapunov exponents. It labels the system’s behaviour\naccording to the exponents. If all λ1, λ2 and λ3 are less than 0, then the label is 0 (regular),\nif any one of λ1, λ2 and λ3 is greater than 0, then the label is 1 (chaotic) and if any two of\nλ1, λ2 and λ3 are positive, then the label is 2 (hyperchaos) as shown in the figure 16.\nThe Deep Learning models used for the classification:\n19\nFigure 16: (a)Bifurcation diagram and (b)Lyapunov exponent spectrum, where green shows\nthe λ1 values, red shows the λ2 values, and blue shows the λ3 values and δr ∈(−1.05, −0.85).\n20\n• FeedForward Neural Network\nThe model’s architecture has dense layers containing the ReLU activation function\nand dropout layers to decrease overfitting. The model begins with an input layer of\n64 neurons and ReLU activation; each subsequent pair of dense layers maintains this\npattern, progressively reducing neuron count to 32, 16, and finally outputting 4 neurons\nwith a softmax activation function.\n• Long short-term memory (LSTM)\nThe model´s architecture is defined by a set of layers that consist of different numbers\nof neurons to capture sequential patterns efficiently. It begins with an LSTM layer with\nan input of 138 neurons. After the first layer, there are dropout layers after each LSTM\nlayer, which have been set at 20% to avoid overfitting. The following dense layers have\n64 and 32 neurons, respectively, using ReLU activation functions to allow a non-linear\nnature and to extract higher-level features from LSTM outputs. The last dense layer\nincludes 4 neurons with a softmax activation function.\n• Recurrent Neural Network\nThe model begins with several numbers of SimpleRNN layers, each having 64 neurons\nused for detecting temporal patterns. Dropout layers with a dropout rate of 20% come\nafter each RNN layer in order to avoid overfitting. When all RNN layers are passed,\na dense layer is added to this model. It contains 32 neurons and comes with a ReLU\nactivation function. The final layer has 4 neurons which use the softmax activation\nfunction.\nAll models are trained using an optimizer called the Adam optimiser and sparse categorical\ncross-entropy loss. Over 100 epochs with a batch size of 32 are used during the training.\nOnce training is complete, the accuracy of the model is evaluated.\nThe table 7 shows that the LSTM model performs better when compared to the Feed\nForward Neural Network (FNN) and the Recurrent Neural Network (RNN). It has the highest\ntraining accuracy with 92.00%, validation with 93.50% as well, as testing accuracy of 93.50%\nrespectively; besides, it experiences the lowest losses, which are coincidentally equal to 0.2067\nfor both training set error rates and 0.2048 for each other set. This can be interpreted as a\nsign that the LSTM model is better able to generalise beyond the seen data points while at\nthe same time producing a more precise hypothesis.\n6\nTwo Parameter Charts\nTwo parameter charts are used in the field of nonlinear dynamics and chaos theory to study\nthe stability and behaviour of the dynamical systems [61]. They are used to visualize how the\nsystem’s behaviour changes when the parameter varies. Here, we used a 2D border collision\nbifurcation normal form map for studying the behaviour using a two-parameter chart. The\nborder collision bifurcation normal form map is defined as follows:\n21\nFigure 17: Comparison of predicted behaviour (FNN in green, LSTM in orange and RNN in\nred) of 3D piecewise smooth map with Lyapunov spectrum.\nFNN\nLSTM\nRNN\nLoss\nAccuracy\nLoss\nAccuracy\nLoss\nAccuracy\nTraining\n0.2235\n0.9187\n0.2067\n0.9200\n0.2305\n0.9137\nValidation\n0.2406\n0.8899\n0.2048\n0.9350\n0.2327\n0.8899\nTest\n0.2406\n0.8899\n0.2048\n0.9350\n0.2327\n0.8899\nTable 7: Loss and Accuracy (%) values of training, validation, and test dataset of 3D Piece-\nwise Smooth Map for different deep learning models.\n22\nFigure 18: Comparison of predicted labels (regular (0), chaotic (1), hyperchaotic (2)) of three\nmodels (FNN, LSTM, RNN) with the help of phase portraits.\n23\nFigure 19: Epoch vs Loss in training and validation datasets of 3D Piecewise Smooth Map\nfor (i) FNN, (ii) LSTM, (iii) RNN; There are two lines, blue representing the training loss\nand red representing the validation loss.\n(i)                                                                                       (ii)                                                                                    (iii)\nFigure 20: Epoch vs Accuracy in training and validation datasets of 3D Piecewise Smooth\nMap for (i) FNN, (ii) LSTM, (iii) RNN; There are two lines, blue representing the training\nloss and red representing the validation loss.\n24\n[x, y] 7→\n\n\n\n\n\n\n\n\n\n\n\n\"\nτLx + y + µ\n−δLx\n#\n,\nx ≤0,\n\"\nτRx + y + µ\n−δRx\n#\n,\nx > 0\n(6.1)\nwhere δL = 2, δR = −0.2 and µ = −1. The data is generated by simulating the equation\n6.1, where τL ∈(−1, 3) and τR ∈(−0.2, 1) for 1000 points. The Lyapunov exponents are\ncalculated and labelled as regular(0) and chaotic(1). A biparametric plane is plotted using\nthese labels as shown in the figure 21. Figure: 21 shows the biparametric plane (τL, τR) of\n2D normal form map where τl ∈(−1, 3) and τR ∈(−0.2, 1). Figure: 21(a) is the original\nimage where coloured regions are periodic and white regions are chaotic. Then we converted\nfigure 21(a) to figure: 21(b) biparametric plane where regular regions are shown in blue and\nchaotic regions in red. For the training, τL ∈(0, 2) and τR ∈(0.6, 1) are taken. As generating\nthe two parametric charts computationally is time-consuming, we used Recurrent Neural\nNetwork (RNN) and Long Short-Term Memory (LSTM) to predict the labels. The data is\nsplitted into training 80% and testing(20%) datasets.\n• Recurrent Neural Network (RNN)\nThe model begins with a SimpleRNN layer with 256 neurons. After the first layer,\nthere are more SimpleRNN layers, gradually decreasing the number of neurons from\n256 to 2. The models then include flatten layers and dense layers with 256 neurons and\nthe ReLU activation function. Another dense layer with 64 neurons is added before the\nfinal output layer, which uses the softmax activation function.\n• Long Short-Term Memory (LSTM)\nThe model begins with the LSTM layer having 256 neurons. This layer is followed by\nseveral more LSTM layers, each gradually reducing the number of neurons from 256 to\n2. After the series of LSTM layers, the models include a flatten layer. The dense layers\nare added to perform the final classification, which includes two layers with 256 and\n128 neurons, respectively. Another dense layer with 64 neurons and the final output\nlayer uses the softmax activation function for the classification task.\nBoth models are compiled with the Adam optimiser and categorical cross-entropy loss and\ntrained for 100 epochs with a batch size of 64. After training, the accuracy of the models is\ncalculated.\nThe labels are predicted for the range of parameter τL ∈(−1, 3) and τR ∈(−0.2, 1) and\nplotted in such a way that regular region in blue and chaotic region in red as shown in the\nfigure: 22(b) using RNN and figure: 22(c) using LSTM. The accuracy of RNN and LSTM\nare 0.9647 and 0.9817, respectively, which shows that LSTM is more accurate than RNN.\n7\nConclusion\nIn this paper, we have used deep learning models for the prediction and classification of the\ndynamics of the piecewise smooth maps like 1D normal form map, 1D tent map, 2D Lozi\n25\nFigure 21: Biparametric plane (τL, τR) of 2D normal form map where τl ∈(−1, 3) and τR ∈\n(−0.2, 1) (a)Image showing periods in different colours (b)Blue (regular) and red (chaotic).\nFigure 22: Biparametric plane (τL, τR) of 2D normal form map where τl ∈(−1, 3) and\nτR ∈(−0.2, 1) (a)image plotted using actual values, image plotted using predicted values (b)\n(RNN model) (c) LSTM model\n26\nMap, 3D piecewise smooth map. The machine learning models used include Decision Tree\nClassifier, Logistic Regression, K-Nearest Neighbor, Random Forest and Support Vector Ma-\nchine for predicting border collision bifurcation of 1D normal form map and 1D tent map.\nMoreover, deep learning models such as Convolutional Neural Network (CNN), ResNet50 and\nConvLSTM are used for classifying regular and chaotic dynamical behaviour of 1D tent map\nand 2D lozi map. Feedforward Neural Network (FNN), Long Short-Term Memory(LSTM)\nand Recurrent Neural Network(RNN) are used for classifying chaotic and hyperchaotic dy-\nnamical behaviour in 3D piecewise smooth map. Finally, we reconstructed the two parametric\ncharts of 2D border collision bifurcation normal form map using deep learning models like\nRecurrent Neural Network (RNN) and Long Short-Term Memory (LSTM).\nWe have used the Lyapunov exponents, cobweb diagrams, and phase portraits to clas-\nsify regular, chaotic, and hyperchaotic dynamical behaviour during data generation. For\nthe prediction of border collision bifurcation of 1D normal form map, random forest shows\nmore accuracy, and in 1D tent map, the decision tree classifier shows more accuracy. The\nconvolutional neural network (CNN) is suitable for classifying regular and chaotic dynamical\nbehaviour. LSTM is the most accurate among other deep learning models for classifying\nchaotic and hyperchaotic behaviour. Also, LSTM is more precise than RNN in reconstruct-\ning the two parametric charts. Future scopes include applying deep learning to predict the\ncobweb symbolisses, as doing it manually is very hard. Deep learning models can also predict\nthe border collision bifurcation of higher periods.\nAcknowledgements\nWe acknowledge insightful discussions with Thomas M. Bury regarding deep learning and\nprediction of codimension-one bifurcations in smooth maps.\nConflict of interest\nThe authors declare that they have no conflict of interest.\nData Availability Statement\nThe data that support the findings of this study are available within the article.\nReferences\n[1] J. Moser. Dynamical Systems, Theory and Applications: Battelle Seattle 1974 Rencon-\ntres. Springer Berlin Heidelberg, 1975.\n[2] Daniel Lathrop. Nonlinear dynamics and chaos: With applications to physics, biology,\nchemistry, and engineering. Physics Today, 68(4):54–55, April 2015.\n27\n[3] David A. Hsieh. Chaos and nonlinear dynamics: Application to financial markets. The\nJournal of Finance, 46(5):1839–1877, December 1991.\n[4] Vinod Cheriyan and Anton J. Kleywegt. A dynamical systems model of price bubbles\nand cycles. Quantitative Finance, 16(2):309–336, February 2016.\n[5] Carling Bieg, Kevin S. McCann, and John M. Fryxell.\nThe dynamical implications\nof human behaviour on a social-ecological harvesting model.\nTheoretical Ecology,\n10(3):341–354, March 2017.\n[6] Lenci Stefano and Giuseppe Rega. Global Nonlinear Dynamics for Engineering Design\nand System Safety. Springer International Publishing, 2019.\n[7] M. di Bernardo, C.J. Budd, and A.R. Champneys. Normal form maps for grazing bi-\nfurcations in n-dimensional piecewise-smooth dynamical systems. Physica D: Nonlinear\nPhenomena, 160(3–4):222–254, December 2001.\n[8] S. Banerjee, P. Ranjan, and C. Grebogi.\nBifurcations in two-dimensional piecewise\nsmooth maps-theory and applications in switching circuits. IEEE Transactions on Cir-\ncuits and Systems I: Fundamental Theory and Applications, 47(5):633–643, May 2000.\n[9] Laurea Mario di Bernardo, R. Champneys, and Piotr Kowalczyk, Alan. Piecewise-smooth\nDynamical Systems. Springer London, 2008.\n[10] Laurea Mario di Bernardo, R. Champneys, and Piotr Kowalczyk, Alan.\nBifur-\ncations in piecewise-smooth feedback systems.\nInternational Journal of Control,\n75(16–17):1243–1259, January 2002.\n[11] Mahashweta Patra and Soumitro Banerjee. Robust chaos in 3-d piecewise linear maps.\nChaos: An Interdisciplinary Journal of Nonlinear Science, 28(12), December 2018.\n[12] Ilya Stolyarov, Ilya Orson Sandoval, Panagiotis Petsagkourakis, and Ehecatl Antonio\ndel Rio-Chanona. Piecewise Smooth Hybrid System Identification for Model Predictive\nControl, page 1645–1650. Elsevier, 2022.\n[13] Yu. A. Kuznetsov, S. Rinaldi, and A. Gragnani. One-parameter bifurcations in planar\nfilippov systems.\nInternational Journal of Bifurcation and Chaos, 13(08):2157–2188,\nAugust 2003.\n[14] Soumitro Banerjee and Celso Grebogi. Border collision bifurcations in two-dimensional\npiecewise smooth maps. Physical Review E, 59(4):4052–4061, April 1999.\n[15] Helena E. Nusse, Edward Ott, and James A. Yorke. Border-collision bifurcations: An\nexplanation for observed bifurcation phenomena. Physical Review E, 49(2):1073–1076,\nFebruary 1994.\n[16] Guohui Yuan, S. Banerjee, E. Ott, and J.A. Yorke. Border-collision bifurcations in the\nbuck converter. IEEE Transactions on Circuits and Systems I: Fundamental Theory and\nApplications, 45(7):707–716, July 1998.\n28\n[17] Masaki Inoue, Jun-ichi Imura, Takayuki Arai, Kenji Kashima, and Kazuyuki Aihara.\nSaddle-node bifurcation and its robustness analysis: A mechanism for inducing pluripo-\ntency in stem cell. In 53rd IEEE Conference on Decision and Control. IEEE, December\n2014.\n[18] Osmin Ferrer, Jos´e Guerra, and Alberto Reyes. Transcritical bifurcation in a multipara-\nmetric nonlinear system. AIMS Mathematics, 7(8):13803–13820, 2022.\n[19] Indika Rajapakse and Steve Smale. The pitchfork bifurcation. International Journal of\nBifurcation and Chaos, 27(09):1750132, August 2017.\n[20] A.A. Didov, M.Yu. Uleysky, and M.V. Budyansky. Stable and unstable periodic or-\nbits and their bifurcations in the nonlinear dynamical system with a fixed point vortex\nin a periodic flow. Communications in Nonlinear Science and Numerical Simulation,\n91:105426, December 2020.\n[21] Helena E. Nusse and James A. Yorke. Border-collision bifurcations including “period\ntwo to period three” for piecewise smooth systems. Physica D: Nonlinear Phenomena,\n57(1–2):39–57, June 1992.\n[22] Helana E. Nusse and James A. Yorke. Border-collision bifurcations for piecewise smooth\none-dimensional maps. International Journal of Bifurcation and Chaos, 05(01):189–207,\nFebruary 1995.\n[23] Z.T. Zhusubaliyev, V.S. Titov, E.Y. Emelyanova, and E.A. Soukhoterin. C-bifurcations\nin the dynamics of control system with pulse-width modulation. In 2000 2nd Interna-\ntional Conference. Control of Oscillations and Chaos. Proceedings (Cat. No.00TH8521),\nCOC-00. IEEE, 2000.\n[24] Zhanybai T. Zhusubaliyev, Evgeniy A. Soukhoterin, and Erik Mosekilde.\nBorder-\ncollision bifurcations and chaotic oscillations in a piecewise-smooth dynamical system.\nInternational Journal of Bifurcation and Chaos, 11(12):2977–3001, December 2001.\n[25] Bruno Robert and Carl Robert. Border collision bifurcations in a one-dimensional piece-\nwise smooth map for a pwm current-programmed h-bridge inverter. International Jour-\nnal of Control, 75(16–17):1356–1367, January 2002.\n[26] S.J Hogan, L Higham, and T.C.L Griffin. Dynamics of a piecewise linear map with a gap.\nProceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences,\n463(2077):49–65, July 2006.\n[27] Viktor Avrutin, Laura Gardini, Michael Schanz, and Iryna Sushko.\nBifurcations of\nchaotic attractors in one-dimensional piecewise smooth maps. International Journal of\nBifurcation and Chaos, 24(08):1440012, August 2014.\n[28] Mahashweta Patra and Soumitro Banerjee. Hyperchaos in 3-d piecewise smooth maps.\nChaos, Solitons and Fractals, 133:109681, April 2020.\n29\n[29] Sundarapandian Vaidyanathan and Taher Azar Ahmad. Chaos Modeling and Control\nSystems Design. Springer International Publishing, 2015.\n[30] N.K. Pareek, Vinod Patidar, and K.K. Sud.\nCryptography using multiple one-\ndimensional chaotic maps. Communications in Nonlinear Science and Numerical Sim-\nulation, 10(7):715–723, October 2005.\n[31] Sundarapandian Vaidyanathan and Lien Chang-Hua. Applications of Sliding Mode Con-\ntrol in Science and Engineering. Springer International Publishing, 2017.\n[32] Martin Storath and Andreas Weinmann. Smoothing splines for discontinuous signals.\nJournal of Computational and Graphical Statistics, 33(2):651–664, November 2023.\n[33] Michael Rosenblum and J¨urgen Kurths. A model of neural control of the heart rate.\nPhysica A: Statistical Mechanics and its Applications, 215(4):439–450, May 1995.\n[34] S. Coombes, Y. M. Lai, M. Sayli, and R. Thul. Networks of piecewise linear neural mass\nmodels. European Journal of Applied Mathematics, 29(5):869–890, February 2018.\n[35] Fabio Tramontana and Frank Westerhoff. One-Dimensional Discontinuous Piecewise-\nLinear Maps and the Dynamics of Financial Markets, page 205–227. Springer Berlin\nHeidelberg, May 2012.\n[36] Thomas M. Bury, Daniel Dylewsky, Chris T. Bauch, Madhur Anand, Leon Glass, Alvin\nShrier, and Gil Bub. Predicting discrete-time bifurcations with deep learning. Nature\nCommunications, 14(1), October 2023.\n[37] Afan Galih Salman, Bayu Kanigoro, and Yaya Heryadi. Weather forecasting using deep\nlearning techniques. In 2015 International Conference on Advanced Computer Science\nand Information Systems (ICACSIS). IEEE, October 2015.\n[38] Znaonui Liang, Gang Zhang, Jimmy Xiangji Huang, and Qmming Vivian Hu. Deep\nlearning for healthcare decision making with emrs. In 2014 IEEE International Confer-\nence on Bioinformatics and Biomedicine (BIBM). IEEE, November 2014.\n[39] Dipanshu Someshwar, Dharmik Bhanushali, Vismay Chaudhari, and Swati Nadkarni.\nImplementation of virtual assistant with sign language using deep learning and ten-\nsorflow. In 2020 Second International Conference on Inventive Research in Computing\nApplications (ICIRCA). IEEE, July 2020.\n[40] Omer Berat Sezer, Mehmet Ugur Gudelek, and Ahmet Murat Ozbayoglu. Financial\ntime series forecasting with deep learning: A systematic literature review: 2005–2019.\nApplied Soft Computing, 90:106181, May 2020.\n[41] Devashish Shankar, Sujay Narumanchi, H A Ananya, Pramod Kompalli, and Krishnendu\nChaudhury. Deep learning based large scale visual recommendation and search for e-\ncommerce, 2017.\n30\n[42] Thanh Tam Nguyen, Thanh Dat Hoang, Minh Tam Pham, Tuyet Trinh Vu, Thanh Hung\nNguyen, Quyet-Thang Huynh, and Jun Jo. Monitoring agriculture areas with satellite\nimages and deep learning. Applied Soft Computing, 95:106565, October 2020.\n[43] Kanwar Bharat Singh and Mustafa Ali Arat. Deep learning in the automotive industry:\nRecent advances and application examples, 2019.\n[44] J. Guerrero-Iba˜nez, J. Contreras-Castillo, and S. Zeadally. Deep learning support for\nintelligent transportation systems. Transactions on Emerging Telecommunications Tech-\nnologies, 32(3), November 2020.\n[45] Lerina Aversano, Mario Luca Bernardi, Marta Cimitile, and Riccardo Pecori. A sys-\ntematic review on deep learning approaches for iot security. Computer Science Review,\n40:100389, May 2021.\n[46] P. Rajendra and V. Brahmajirao. Modeling of dynamical systems through deep learning.\nBiophysical Reviews, 12(6):1311–1320, November 2020.\n[47] Yann LeCun,\nYoshua Bengio,\nand Geoffrey Hinton.\nDeep learning.\nNature,\n521(7553):436–444, May 2015.\n[48] G. Bebis and M. Georgiopoulos.\nFeed-forward neural networks.\nIEEE Potentials,\n13(4):27–31, October 1994.\n[49] Sepp Hochreiter and J¨urgen Schmidhuber. Long short-term memory. Neural Computa-\ntion, 9(8):1735–1780, November 1997.\n[50] Ke Lin Du and M. N. S. Swamy. Recurrent Neural Networks, page 337–353. Springer\nLondon, December 2013.\n[51] Nikhil Ketkar and Jojo Moolayil. Convolutional Neural Networks, page 197–242. Apress,\n2021.\n[52] Pradeep Hewage, Marcello Trovati, Ella Pereira, and Ardhendu Behera. Deep learning-\nbased effective fine-grained weather forecasting model. Pattern Analysis and Applica-\ntions, 24(1):343–366, June 2020.\n[53] Parag Jain and Soumitro Banerjee.\nBorder-collision bifurcations in one-dimensional\ndiscontinuous maps. International Journal of Bifurcation and Chaos, 13(11):3341–3351,\nNovember 2003.\n[54] T. Yoshida, H. Mori, and H. Shigematsu. Analytic study of chaos of the tent map:\nBand structures, power spectra, and critical behaviors. Journal of Statistical Physics,\n31(2):279–308, May 1983.\n[55] N.A. Priyanka and Dharmender Kumar.\nDecision tree classifier: a detailed survey.\nInternational Journal of Information and Decision Sciences, 12(3):246, 2020.\n31\n[56] Todd G. Nick and Kathleen M. Campbell. Logistic Regression, page 273–301. Humana\nPress, 2007.\n[57] Oliver Kramer. K-Nearest Neighbors, page 13–23. Springer Berlin Heidelberg, 2013.\n[58] Steven J. Rigatti. Random forest. Journal of Insurance Medicine, 47(1):31–39, January\n2017.\n[59] Shan Suthaharan. Support Vector Machine, page 207–235. Springer US, 2016.\n[60] Roberto Barrio, ´Alvaro Lozano, Ana Mayora-Cebollero, Carmen Mayora-Cebollero, An-\ntonio Miguel, Alfonso Ortega, Sergio Serrano, and Rub´en Vigara. Deep learning for\nchaos detection. Chaos: An Interdisciplinary Journal of Nonlinear Science, 33(7), July\n2023.\n[61] Mark McGuinness, Young Hong, Duncan Galletly, and Peter Larsen. Arnold tongues\nin human cardiorespiratory systems. Chaos: An Interdisciplinary Journal of Nonlinear\nScience, 14(1):1–6, March 2004.\n32\n",
  "categories": [
    "cs.LG",
    "nlin.CD"
  ],
  "published": "2024-06-24",
  "updated": "2024-06-24"
}