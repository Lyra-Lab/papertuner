{
  "id": "http://arxiv.org/abs/2406.17316v1",
  "title": "A review of unsupervised learning in astronomy",
  "authors": [
    "Sotiria Fotopoulou"
  ],
  "abstract": "This review summarizes popular unsupervised learning methods, and gives an\noverview of their past, current, and future uses in astronomy. Unsupervised\nlearning aims to organise the information content of a dataset, in such a way\nthat knowledge can be extracted. Traditionally this has been achieved through\ndimensionality reduction techniques that aid the ranking of a dataset, for\nexample through principal component analysis or by using auto-encoders, or\nsimpler visualisation of a high dimensional space, for example through the use\nof a self organising map. Other desirable properties of unsupervised learning\ninclude the identification of clusters, i.e. groups of similar objects, which\nhas traditionally been achieved by the k-means algorithm and more recently\nthrough density-based clustering such as HDBSCAN. More recently, complex\nframeworks have emerged, that chain together dimensionality reduction and\nclustering methods. However, no dataset is fully unknown. Thus, nowadays a lot\nof research has been directed towards self-supervised and semi-supervised\nmethods that stand to gain from both supervised and unsupervised learning.",
  "text": "A review of Unsupervised Learning in Astronomy\nSotiria Fotopouloua\naSchool of Physics, HH Wills Physics Laboratory, University of Bristol, Tyndall Avenue, Bristol, BS8 1TL, , United Kingdom\nAbstract\nThis review summarizes popular unsupervised learning methods, and gives an overview of their past, current, and future uses in\nastronomy. Unsupervised learning aims to organise the information content of a dataset, in such a way that knowledge can be\nextracted. Traditionally this has been achieved through dimensionality reduction techniques that aid the ranking of a dataset, for\nexample through principal component analysis or by using auto-encoders, or simpler visualisation of a high dimensional space, for\nexample through the use of a self organising map. Other desirable properties of unsupervised learning include the identification of\nclusters, i.e. groups of similar objects, which has traditionally been achieved by the k-means algorithm and more recently through\ndensity-based clustering such as HDBSCAN. More recently, complex frameworks have emerged, that chain together dimensionality\nreduction and clustering methods. However, no dataset is fully unknown. Thus, nowadays a lot of research has been directed towards\nself-supervised and semi-supervised methods that stand to gain from both supervised and unsupervised learning.\n1. Introduction\n1.1. What is learning?\nLearning from astrophysical data consists of extracting\nknowledge by constructing a mapping between the high-\ndimensional space of the observables to a lower dimensional\nspace through, either an inference tool, e.g.\nempirical pho-\ntometric redshift estimation, or an assumed model, e.g. esti-\nmating physical parameters from spectra such stellar mass or\nmetallicity. An additional goal, approached as a byproduct of\nthe learning process, is the identification of outliers. These are\nsources that do not conform to the inferred mapping as mea-\nsured by some appropriately defined distance or similarity mea-\nsure. Outlier or anomaly detection is becoming more and more\nprevalent in exploiting large astronomical datasets in pursuit of\ntransient sources (supernova, gravitational waves), rare sources,\ni.e, short-lived phases of otherwise normal sources, including\nplanet detection (Sarkar et al., 2022), and in pursuit of the un-\nknown unknowns (Lochner and Bassett, 2021).\nThe process of applying machine-learning (ML) methods on\nany kind of data typically encompasses the following steps 1)\ndata gathering and calibration verification, e.g.\nzero points\nfor photometric data, flux and wavelength calibration for spec-\ntroscopy 2) pre-processing, i.e. imputation of missing values,\nwhitening, normalization 3) optionally dimensional reduction\n4) hyperparameter tuning 5) performance validation. The first\nsection of Table 1 provides references to relevant reviews, that\nare not astronomy specific.\nLargely defined by the presence of ground truth during the\ntraining process, supervised and unsupervised learning come\nwith their distinct advantages and disadvantages. The field of\nsupervised learning has enjoyed significant advancements in the\npast decade, starting by implications of deep learning, and rang-\ning all the way to modern day transformer architecture. Super-\nvised learning is the mapping between an input space and, a\nknown, ground truth. It can be used to perform both classifica-\ntion and regression. However, it is impossible to extrapolate be-\nyond the properties of the training set. Contrary, unsupervised\nlearning encompasses methods that crystallise neighbourhood\nrelationships in a high-dimensional parameter space, and meth-\nods that go a step further into projecting these relationships into\nlower-dimensional spaces that preserve - to a certain extent -\nlocal or global distances. Hence, the distinct advantage is the\nopportunity to discover new attributes, or new categories of ob-\njects. However, even though continuous maps or ranking of the\ndata can be produced, the main goal of unsupervised learning is\ngrouping the data into clusters.\nAlternative approaches that fall in between the super-\nvised/unsupervised division have also been developed aiming\nto by pass the limitations imposed by limited labelled training\nsamples. These approaches include Self-supervised learning,\nSemi-supervised learning. Other methods aim to abstract in-\nformation from the learned features with aim to push machine-\nlearning towards artificial intelligence. These approaches can\nbe categorised under the general terms of Transfer learning,\nand Representation learning. The second half of Table 1 pro-\nvides reviews on each of these approaches without being do-\nmain specific. We shall return to these approaches in Section\n§6.\nBefore we delve into the subject of unsupervised learning\nin astronomy, we point out that a summary of the ML methods\ndiscussed in this work, corresponding acronyms, and references\nto original papers or reviews on the subject are given in Table\nA.3 and Table A.4.\n1.2. A brief historical perspective\nData-driven discovery is part of the astronomers’ DNA. Even\nthough the notion of ‘Big Data’ is a loosely defined, and ever\nevolving term (see Kitchin and McArdle, 2016, for a review),\nPreprint submitted to Astronomy & Computing\nJune 26, 2024\narXiv:2406.17316v1  [astro-ph.IM]  25 Jun 2024\nApproach\nReview\nPre-processing\nMaharana et al. (2022)\nDimensional Reduction\nJia et al. (2022)\nRepresentation learning\nBengio et al. (2013)\nAnomaly detection\nBansal and Pahuja (2022)\nSelf-supervised\nRani et al. (2023)\nContrastive learning\nHuertas-Company et al. (2023)\nSemi-supervised learning\nvan Engelen and Hoos (2020)\nWeakly-supervised learning\nZhou (2017)\nTransfer learning\nZhuang et al. (2020)\nDomain adaptation\nFarahani et al. (2021)\nTable 1: Reviews on the process of learning from data.\nthe significance and challenges associated to acquiring and pro-\ncessing large amounts of data has long standing recognition in\nastronomy. In the following, we give a very brief historical con-\ntext of advances in hardware, software, and data availability that\nhave influenced astronomy since the turn of the millennium.\n1.2.1. Early digital astronomy (pre-2000)\nAmong the first digitised data have been scans of photo-\ngraphic plates, that already amounted to a few terabytes of\ndata. These include digitised versions of Schmidt plates offered\nthrough the SuperCOSMOS Sky Survey1 (SSS, Hambly et al.,\n2001, 2004), access to plates obtained from German observa-\ntories through the Archives of Photographic PLates for Astro-\nnomical USE2 (APPLAUSE), while some of the plates obtained\nin the USA can be found through the project Digital Access to\na Sky Century Harvard3 (DASCH, Grindlay et al. (2012)) and\nthe Maria Mitchell Observatory4.\nPre-2000, major all-sky digital surveys where already in\nplace, including the Two Micron All Sky Survey (2MASS,\nSkrutskie et al., 2006), collecting a staggering 25TB of imaging\nbetween 1997-2001, and software packages such as IRAF had\nemerged for wider use in the community (Tody, 1986, 1993).\nHowever, the wider astronomical community had limited ac-\ncess to computing resources, and data were collected and trans-\nferred as physical copies on hard drives and compact discs. De-\nspite the challenges, we already have some applications of un-\nsupervised learning on spectra classification, stellar classifica-\ntion and so on (e.g. Storrie-Lombardi et al., 1994; Hernandez-\nPajares and Floris, 1994; Connolly et al., 1995; Lahav et al.,\n1996; Faundez-Abans et al., 1996; Naim et al., 1997; Bailer-\nJones et al., 1998; Galaz and de Lapparent, 1998; Tagliaferri\net al., 1999; Lee et al., 1999; Vilela Mendes, 1999).\n1.2.2. The multiwavelength era (2000-2010)\nIn the years 2000 – 2010, multiwavelength astronomy came\ninto focus with extragalactic surveys such as the Sloan Digi-\ntal Sky Survey (SDSS, Gunn et al., 1998; York et al., 2000),\n1http://ssa.roe.ac.uk//\n2https://www.plate-archive.org/cms/home/\n3https://dasch.cfa.harvard.edu/\n4https://www.mariamitchell.org/astronomical-plates-collection\nClassifying Objects by Medium-Band Observations in 17 filters\n(COMBO-17, Wolf et al., 2001b,a), the Cosmic Evolution Sur-\nvey (COSMOS, Scoville et al., 2007), to name only a few. At\nthe same time, affordable desktop computers, and the rise of\nWeb 2.0, enabled fast communication, data transfer and collab-\norations such as national Virtual Observatories and the Interna-\ntional Virtual Observatory Alliance5, established in 2002.\nPublications of that time start to take advantage the massive\ndatasets, and in addition to source classification they also fo-\ncus on data-mining and knowledge discovery in databases (e.g.\nAndreon et al., 2000; Xui et al., 2001; Odewahn et al., 2002;\nRajaniemi and M¨ah¨onen, 2002; Turmon et al., 2002; Hakkila\net al., 2003; Eyer and Blake, 2005; Wagstaff et al., 2005; Hoj-\nnacki et al., 2008; Rudick et al., 2009; Marzo et al., 2009; Sarro\net al., 2009; Barra et al., 2009). Ball and Brunner (2010) pro-\nvide a review of astronomical applications of that era.\n1.2.3. Computing goes mainstream (2010-2015)\nThe information technology revolution has shaped a new fu-\nture. Astronomy collaborations grow larger, with SDSS paving\nthe way. By this point in time, most - if not all - universities\nhave access to significant computing resources, laptop com-\nputers have become ubiquitous, while collaborative and open-\nsource software development has given rise to the first release\nof Astropy (Astropy Collaboration et al., 2013) and Scikit learn\n(Pedregosa et al., 2011). Ivezi´c et al. (2014) publish their book\non Statistics, Data Mining and Machine Learning in Astron-\nomy, with associated code examples, ready to run on astronom-\nical data.\nEase of data access and robust codes, lead to experimenta-\ntion of several methods. Many works focus in data cluster-\ning, classification, and outlier discovery (Andrae et al., 2010;\nS´anchez Almeida et al., 2010; Coppa et al., 2011; Var´on et al.,\n2011; Geach, 2012; Richards et al., 2012; D’Abrusco et al.,\n2012; Way and Klose, 2012; Shamir, 2012; Shamir et al., 2013;\nS´anchez Almeida and Allende Prieto, 2013; Graff et al., 2014;\nKrone-Martins and Moitinho, 2014a; Carrasco Kind and Brun-\nner, 2014; Damodaran and Nidamanuri, 2014).\n1.2.4. ML Revolution (2015-2020)\nThe next five years changed the machine-learning landscape\nforever. The public release of Tensorflow (Abadi et al., 2015),\nand PyTorch (2016) provided an easy-to-use Python interface to\nlower-level C++ code, able to run on GPUs. The collaborative\nnature of software development and hands-on training is fur-\nther cemented by the introduction of Project Jupyter (2015) and\nCloud computing (e.g. Google Cloud, Amazon Web Services).\nDeep learning applications have created a profound change\nin the world.\nData are getting ever larger; multi-terrabyte\narchives exist across wavelengths (VHS, GAIA, DES, etc), with\npetabyte and exascale astronomy in the works. Access to re-\nsources for testing is not an issue as hardware keeps getting\nbetter, and even more affordable. Inference at large scale starts\nto become challenging.\n5https://www.ivoa.net/\n2\nAstronomy stays on top the newest methods with applica-\ntions on morphology and image segmentation, and many papers\nshowcase the strengths and weaknesses of algorithms applied\non classification, dimensionality reduction and time-domain\nastronomy keeping in mind the challenges of upcoming ever\nlarger datasets (e.g., Fraix-Burnet et al., 2015; Kim et al., 2015;\nSchutter and Shamir, 2015; Huijse et al., 2015; K¨ugler et al.,\n2015; Koljonen, 2015; Tramacere et al., 2016; Armstrong et al.,\n2016; Sasdelli et al., 2016; Tammour et al., 2016; Lawlor et al.,\n2016; Rubin and Gal-Yam, 2016; Kuntzer et al., 2016; Zitlau\net al., 2016; Davies et al., 2016; Speagle and Eisenstein, 2017;\nWetzel, 2017; Baron and Poznanski, 2017; Dehghan Firooz-\nabadi et al., 2017; Meingast et al., 2017; Benavente et al., 2017;\nSelim and Abd El Aziz, 2017; Frontera-Pons et al., 2017; Mis-\nlis et al., 2018; Cabrera-Vives et al., 2018; Reis et al., 2018;\nGarcia-Dias et al., 2018; Hocking et al., 2018; Reis et al., 2021;\nGiles and Walkowicz, 2019; Garcia-Dias et al., 2019; Kounkel\nand Covey, 2019; Ralph et al., 2019).\n1.2.5. Meta algorithms (2020-)\nIn later years, the focus has shifted towards 1) developing\nmeta-algorithms and pipelines that either incorporate several\nalgorithms to achieve tailored data analysis 2) exploration of\nthe latest ML methods such as contrastive learning and gen-\nerative models. We witness an ever increasing interest in the\nlatent space: its robustness, its interpretation, and its limita-\ntions (Logan and Fotopoulou, 2020). Domain adaptation meth-\nods promise to transfer knowledge across samples, while likeli-\nhood free inference / simulation based inference are increasing\nin popularity, particularly due to their speed and flexibility (e.g.,\nvon Wietersheim-Kramsta et al., 2024; Chen et al., 2023). How-\never, as complexity increases, model interpretability is lost. At\nthe same time, physics-aware models try to capture some of the\nsought-after meaning in the data (e.g., Xu et al., 2023; Moschou\net al., 2023).\n1.3. Machine Learning in Astronomy\nAstronomy has, and will, remain enamoured with the pos-\nsibilities of ML applications as the field faces several of the\n‘Big Data’ definitions (volume, velocity, variety, veracity, etc;\nKitchin and McArdle, 2016). Along with the very extensive\nliterature, a number of very detailed reviews exist on vari-\nous applications of machine-learning and data-driven discovery\nbriefly summarised below.\n1.3.1. Previous ML Reviews\nBall and Brunner (2010) provide an overview of data-mining\nand knowledge extraction from databases, referring to any or-\nganised collection of data, including also FITS files. This re-\nview provides useful advice on pre-processing caveats relating\nto astronomy applications from a practitioner’s perspective, for\nexample source association and masking, and discuss individ-\nual ML methods.\nFraix-Burnet et al. (2015) reviewed ML methods used for\nclassification specifically for extragalactic astronomy. The au-\nthors discuss both supervised and unsupervised methods for\nclassification, describing commonly used methods and their ap-\nplications in extragalactic astronomy. Huijse et al. (2015) fo-\ncused on time-domain astronomy, and in particular on the chal-\nlenges introduced due to the scale of the anticipated LSST Sur-\nvey. They showcase that a combination of unsupervised, su-\npervised, as well as an active learning approach, that injects\ndomain knowledge when necessary, is mostly likely needed.\nBaron (2019) summarised in a practical and pedagogical\nmanner commonly used unsupervised and supervised methods,\nincluding performance metrics and instructive toy examples.\nEl Bouchefry and de Souza (2020) provide a brief historical\noverview of the nascent period of ML, cover definitions of vari-\nous types of learning, and provide application in astronomy and\ngeoscience. In Doorenbos et al. (2021) the authors discuss a de-\ntailed comparison of outlier detection methods, based on SDSS\ndata.\nFinally, Smith and Geach (2023) give an excellent pedagogi-\ncal review on the use of neural networks in astronomy, covering\ndevelopments from early years of using multi-layer perceptron\nto modern applications of auto-encoders and generative mod-\nels. More recently, Huertas-Company et al. (2023) provide a re-\nview of contrastive learning, a new approach of self-supervised\nlearning.\n1.3.2. About this review\nIn this review, we will focus on the use of unsupervised meth-\nods in astronomy in the past 30 years. A broad search of as-\ntronomy and other sciences using the NASA/ADS database us-\ning the terms ’unsupervised learning’, returned just over 1,000\nitems. We scanned through titles and abstracts for relevant lit-\nerature and reduced the number to about 500 articles and book\nchapters. Some of the unrelated publications regarded student\nsupervision and mentorship relationships, learning in terms of\neducation, etc.\nOf the 500 relevant papers, we tagged each entry by method\nused, and area of application. A number of the papers referred\nto Earth monitoring (including Moon and Mars) applications,\nvolcanic activity, and modelling turbulent fluids. In the follow-\ning, we will focus on the Astrophysical applications. Some of\nthe Sun and solar system works will be mentioned briefly, but\nnot discussed. We noticed a general trend of silos where com-\nmunities used a specific code, or approach to model the corre-\nsponding data. Hopefully this review will spark some inspira-\ntion for testing other methods. The discussion presented in the\nremaining of this review is of course not limited to the retrieved\n∼500 papers, which only served as a starting point.\nA few major areas of application emerged during the review\nof the literature. In addition to the works on anomaly detection\nbriefly mentioned earlier, the analysis of the GAIA survey is in\nthe forefront of ML applications, including the use of unsuper-\nvised methods for stellar cluster detection. In extragalactic as-\ntronomy, heavy use of unsupervised methods is found in galaxy\nmorphology in optical and radio images, source classification\n(e.g., star/galaxy/QSO), and spectroscopy.\nIn the remaining of this work, we will approach the subject\nneither in chronological order, nor in area of application or even\n3\nUnsupervised\nlearning in astronomy\nModern\napproaches\nWeak\nsupervision\nSemi-\nsupervision\nSelf-\nsupervision\nTransfer\nand\ndomain\nadap-\ntation\nClustering\nDensity-\nbased\nProbabilistic\nclustering\nTree-\nbased\nHierarchical\nclustering\nCentroid-\nbased\nDimensional\nReduction\nNetworks\nSOM\nAE\nNon-\nlinear\nmanifold\nlearning\nIsomap\nLLE\nt-SNE\nUMAP\nMatrix\nFactor-\nization\nSVD\nPCA\nkernel-\nPCA\nICA\nNMF\nFigure 1: Graphical overview of unsupervised learning in astronomy.\n4\nby method. Rather we take the more abstract approach of ex-\namining the process of learning from astrophysical data. This\nincludes first a discussion of traditional unsupervised approach\nincluding high-dimension space (§2) and features (§3), dimen-\nsional reduction (§4), data clustering (§5). Finally, we briefly\ndiscuss very recent applications of self- and semi-supervised\nlearning and domain adaptation (§6). We close with some rec-\nommendations on the way forward for future applications based\non the observations made in the literature and our own experi-\nence (§7).\nWe assume the reader is familiar with the nomenclature of\nmachine-learning applications. For the readers that might need\nin depth explanations of the terminology, we point them to the\nexcellent reviews mentioned previously in Section §1.3.1. Fi-\nnally, we use the term ‘model’ to refer to astrophysical models,\nas the spectral energy distribution of a galaxy and machine-\nlearning models, as well as the algorithm and tuned hyper-\nparameters created to describe a dataset, e.g. a trained self-\norganising map. The meaning of the term will be specified only\nwhen it is not clear from the context.\n2. Input Features and the Curse of Dimensionality\nAstronomical features fall in three main categories: 1) ob-\nserved 2) deduced based on a model or 3) deduced based on\ndata-driven property of the dataset at hand. The first category\nincludes for example fluxes and colours, spectra, and time se-\nries. The second category includes properties found usually in\na catalog produced with traditional methods, such a Sercic in-\ndex, Gini index, metallicities, mass, etc. Finally, the data-driven\nderivation might include PCA components, features learned by\na CNN, etc.\nAstronomical or other data live in a multi-dimensional space\nthat is impossible to visualise directly if the parameter space\nspans more than three-five dimensions, e.g. by inclusion of\ncolour and size gradients as extra dimensions. This parameter\nspace is typically sparse and creates computational challenges.\nEven if the data could be on a narrow hyperplane, uncertainties\nwill inevitably scatter the data above and below this hyperplane.\nThe uncertainties are introduced not only by the instrumental\nlimitations, but also by the intrinsic scatter of physical proper-\nties, which in addition might occupy a continuum and not dis-\ntinct classes that might be found in everyday physical objects.\nEmploying a clustering method directly on the high-\ndimensional space is rarely feasible or necessary, since we actu-\nally expect observed data to be correlated due to the underlying\nphysical emission mechanisms. Thus, unsupervised learning\nusually starts by reducing the observed or deduced parameter\nspace to a lower dimension space6 (see Section §4), usually\ninto three to four dimensions (e.g. Sasdelli et al., 2016; Lo-\ngan and Fotopoulou, 2020), in which clustering analysis is per-\nformed (see Section §5). The exact of number of dimensions\n6Supervised methods can also benefit from dimensional reduction but will\nnot be discussed here in detail.\nis found through experimentation, by monitoring model perfor-\nmance (Logan and Fotopoulou, 2020) or iteratively through e.g.\nthe average silhouette method as is the case for k-means.\n3. Pre-processing\nAs is the case for supervised learning, unsupervised algo-\nrithms work best when the data have been cleaned and nor-\nmalised before presented to the algorithm, e.g. by using the\nStandardScaler in Scikit.Learn. This is common practice\nin the ML community. In the following, we note a few domain-\nspecific issues relating to astronomical data.\n3.1. Data Imputation\nIn ML, substituting missing data values with the mean of\nthe distribution is standard recommendation. However, miss-\ning data in astronomy can arise due non-observed parts of the\nsky, due to technical artifacts (e.g. diffraction spikes), or due to\nnon-detection in particular wavelengths and to a certain sensi-\ntivity (depth). The first two cases are non-informative, and can\nbe usually made explicit in a catalog by the use of a placeholder\nvalue such as ‘-99’. The latter case of non-detections however,\ncarries information on a per-source basis.\nAstronomical catalogs do not always encode this missing in-\nformation, or they might include a homogeneous image depth\nacross the catalog, corresponding to the mean depth of the im-\nage. This approach is of course common practice due its sim-\nplicity. However, as data become larger, it is increasing im-\npossible to resort to new source detection for each source of\ninterest. Therefore, it would be beneficial for many analyses to\ninclude the measured flux at the location of the source across\nwavelengths.\n3.2. Normalization\nMany algorithms treat larger numerical values as more im-\nportant, hence it paramount that this effect is removed before\nmodeling the data. Standard ML practice includes the ‘whiten-\ning’ and ‘normalisation’ of the data, whereby the mean of the\ndistribution is centered to zero and the standard deviation is\nscaled to one.\nImportantly, this scaling has to be preserved and applied with\nthe same scaled factor to new data that might be presented to a\nmodel at a future instance.\n4. Dimensional reduction\nOften, astrophysical features are correlated, and thus we can\ncreate a mapping between the sparse high-dimensional space\nand a lower-dimensional space that capture the majority of the\ninformation included in the data. A review of popular methods\ncan be found Jia et al. (2022) and details on the methods can be\nfound in Bishop (2006).\nThe new representation of the data in the lower-dimension\nspace is called a latent space. Most methods will attempt to\ncreate latent space compression that is capturing the most in-\nformative aspect of the data, which at the same time are usually\n5\nnon-intuitive to interpret. However, since they aim to reveal the\ninformation content on the observed data, they are often used as\ninput in other downstream machine-learning methods, both in\nsupervised and unsupervised tasks. In the following, we discuss\npopular dimensional reduction methods in astronomy, grouped\nby the algorithmic approach, i.e. matrix factorization (§4.1),\nmanifold learning (§4.2), and networks (§4.3).\n4.1. Matrix factorization\n4.1.1. Singular Value Decomposition (SVD)\nSingular value decomposition (SVD) is the most general fac-\ntorization of an M × N matrix. Namely, any matrix A can be\nre-written as:\nA = U · Σ · VT\n(1)\nwhere U is an M × N column-orthogonal matrix, Σ is an N × N\ndiagonal matrix, and V is another N×N column-orthogonal ma-\ntrix. The diagonal elements of the Σ matrix are called the sin-\ngular values (see Press et al., 2007, for a detailed discussion).\nSVD is used frequently for signal decomposition, with appli-\ncations in astronomy ranging from decomposition of spectra\nof individual sources (e.g., Simon and Sturm, 1994; Piana and\nBrown, 1998; Hobbs et al., 2006; Amara and Quanz, 2012; Ro-\nmano and Cornish, 2017), to cosmology and large scale struc-\nture (e.g., Nicholson et al., 2010; Vanderplas and Connolly,\n2009; Bennett et al., 2013; Planck Collaboration et al., 2016a).\nSVD is closely linked to the derivation of principal component\nanalysis (PCA) discussed in the following section (§4.1.2), as\nit is computationally more attractive compared to solving for\neigenvalues using the determinant of a matrix.\n4.1.2. Principal Component Analysis (PCA)\nPrincipal component analysis (PCA, Hotelling, 1936) is the\ndata transformation that identifies the directions of maximum\nvariance in the data, i.e. the principal components are the eigen-\nvectors of the covariance matrix of the data. It is therefore, a\nlinear projection of N-dimensional data, into a d-dimensional\nspace with d < N. The majority of the information is usually\ncaptured in the first few components. The physical interpreta-\ntion of the principal components requires further domain spe-\ncific investigation.\nCalculating the eigenvectors of the covariance matrix could\nbe accomplished using the determinant of the matrix, but that\nwould require the matrix to be square. A very educational de-\nscription of the more computationally favourable application of\nSVD is given in Press et al. (2007); Ivezi´c et al. (2014). Briefly,\nthe first step before searching for the eigenvectors is to remove\nthe mean of the data, as would appear as the first principal com-\nponent. Let our M observations of N-features be described as\nan M × N matrix, A. The covariance matrix is:\nCA = AT · A\n(2)\nUsing the SVD factorization of eq. 1, we have:\nCA = AT · A\n= (U · Σ · VT)T · (U · Σ · VT)\n= V · Σ · UT · U · Σ · VT·\n= V · Σ2 · VT\n(3)\nTherefore, the SVD of the covariance matrix leads to a quick\nand stable determination of the eigenvalues of matrix A, par-\nticularly in the regime of very large number of features. In\npractice, popular PCA algorithms, including the sklearn im-\nplementation use some version of the SVD approach7.\nPCA has found an astonishing number of applications in as-\ntronomy. A NASA/ADS search at the end of 2023 returns 8.4\nthousand referred papers. Likely the first application of PCA\nin astronomy has been the classification of stellar spectra by\nDeeming (1964). Further PCA uses in galactic applications in-\nclude Storrie-Lombardi et al. (1994) where the authors showed\nthat the first five PCA components of stellar spectra can lead to\nfast and accurate stellar type classification when used as input\nin a neural network. Deb and Singh (2009) performed a PCA\ndecomposition of star lightcurves showing that PCA can pro-\nvide a fast alternative classification of variable stars, as a first\nstep before more detailed analysis. Krone-Martins and Moit-\ninho (2014b) introduced the framework ‘Unsupervised photo-\nmetric membership assignment in stellar clusters’ (UPMASK)\nto identify stellar cluster members, applied by many authors\non GAIA data. PCA is performed on the photometric data as\nthe first step of the analysis, to identify stars with similar com-\nposition. Hayes et al. (2020) used PCA to compress a model\nlibrary with the aim to extract informative priors to speed up\nforward modelling of exoplanet spectra. Matchev et al. (2022)\nused PCA to explore the physical properties of the exoplanet\nspectra benchmark dataset of M´arquez-Neila et al. (2018).\nExamples of extragalactic astronomy applications include\n(Connolly et al., 1995) who used PCA to produce a set of in-\nformative photometric filter combinations to infer photometric\nredshifts.\nSulentic et al. (2000) pieced together multiwave-\nlength data to inform the interpretation of Eigenvector 1, the\nfirst principal component derived from QSO spectra as shown\nearlier in Boroson and Green (1992), linked to the Edding-\nton ratio (Marziani et al., 2003). Wild et al. (2014) modeled\nBruzual and Charlot (2003) SEDs convolved with photometric\nfilter curves and used PCA components, dubbed ‘super-colours’\nto classify the SEDs of galaxies into star-forming and passive.\nLawlor et al. (2016) used PCA and a number of other methods\nto map SDSS spectra to lower dimensions and explored the cor-\nrelation of the projected space to physical properties of galax-\nies. More recently, Logan and Fotopoulou (2020) used PCA as\nmeans to reduce the input parameter space before performing\nstar/galaxy/QSO classification.\n7https://scikit-learn.org/stable/modules/generated/\nsklearn.decomposition.PCA.html\n6\n4.1.3. Kernel PCA\nKernel PCA (Sch¨olkopf et al., 1998) was designed to over-\ncome the limitations of linear projection of complex a data\nstructures.\nIn principle, mapping the data to higher dimen-\nsional parameter space would allow linear decomposition with\nPCA. However, the new space might need to be of too high\ndimensions, and the mapping function not easily known. The\nintractable computing difficulties are mitigated with appropri-\nate choice of kernels which substitute dot products in feature\nspace, with kernel functions. It is important to note, that, unlike\nPCA, kernel PCA does not allow the exact construction of the\ndata but only an approximation.\nThis method has been applied as pre-processing step for de-\ntecting Type 1 SNe photometric classification (Ishida and de\nSouza, 2013) and lensed QSO (Agnello et al., 2015) using a\ngaussian radial base function as kernel. Xiang et al. (2017)\nused kernel PCA with a gaussian radial basis function on LAM-\nOST stellar spectra to estimate physical parameters of stars.\nWang and Bon (2020); Amaya et al. (2020); Irfan and Bull\n(2021) have applied kernel PCA on solar image observations,\nPapaefthymiou et al. (2022) on mid-infrared spectra classifi-\ncation of Ultra Luminous Red Galaxies (ULRIGS) and QSO\n(gaussian kernel).\n4.1.4. Independent Component Analysis (ICA)\nIndependent Component Analysis (ICA - see Hyv¨arinen and\nOja, 2000, for a review) seeks to separate a signal into statisti-\ncally independent components, e.g. overlapping sound signals,\nunlike PCA which aims to recover a representation that cap-\ntures the most dominant components in the data. It is a linear\ntransformation, applicable only on non-Gaussian data. If all\ncomponents are gaussian, then the resulting mixture is also a\nsymmetric gaussian distribution that does not contain enough\ninformation to disentangle the signals. A disadvantage of this\nmethod is that the number of components must be defined man-\nually, which can be explored by inspecting the residuals. In\naddition, the importance of each component must be explored\non the basis of domain knowledge and existing understanding\nof the system.\nThis method has been used to perform blind source sepa-\nration, for example on mid-infrared Spitzer maps Meidt et al.\n(2014) and Cosmic Microwave Background maps (Planck Col-\nlaboration et al., 2014). Meidt et al. (2014) used ICA to isolate\nthe light of old stellar populations to derive accurate mass-to-\nlight ratio conversions on the Spitzer Survey of Stellar Struc-\nture in Galaxies (S4G, Sheth et al., 2010; Planck Collaboration\net al., 2016b). (Cardoso et al., 2008) extended the ICA approach\nto multi-spectral analysis of the Planck data (Planck Collabora-\ntion et al., 2014).\nICA has been also used to separated signals in spectra.\nIndicative applications include the spectra of X-ray binaries\n(Koljonen, 2015), exoplanets (Waldmann et al., 2013; Morello\net al., 2019), galaxies (Lu et al., 2006; Allen et al., 2013), AGN\n(Xu et al., 2007; Richardson et al., 2014), QSO (Temple et al.,\n2021). Chattopadhyay et al. (2019) used ICA as dimensional\nreduction method on 47 derived physical properties of galaxies\nas pre-processing before clustering, however they used PCA to\nfind the number of needed dimensions.\n4.1.5. Non-negative matrix factorization (NMF)\nNon-negative matrix factorization (NMF Lee and Seung,\n1999) was developed as a method that is able to identify parts of\nobjects, such part of a face. NMF mixing matrix is constrained\nto be strictly positive, contrary to PCA learns a holistic repre-\nsentation of the data and allows subtractions of the components\nto reconstruct an object.\nThe first application in astronomy was by Blanton and\nRoweis (2007) to derive k-corrections on galaxy spectra. They\nfound that a basis of five components can be used to recon-\nstruct the galaxy synthetic spectral library of Bruzual and Char-\nlot (2003). Soon after, Allen et al. (2011) used NMF to create a\nbasis of component that capture the information in QSO spec-\ntra. They created a new way to measure the ‘balcinity’ index of\na QSO by projecting broad absorption line QSO (BAL-QSO)\nspectra on the same basis. Koljonen (2015) found that NMF\nperformed better over PCA and ICA on decomposing the spec-\ntrum of X-ray spectrum of the X-ray binary GX 339-4. They\nfind that five components can provide a good description of the\ndata, and linked the components to physical properties of the\nsystem.\n4.2. Non-linear manifold learning\nManifold learning methods aim to capture neighbourhood re-\nlationships that exist in the input high dimensional space and\ncreate a mapping to a lower dimensional space that preserves\nthese relationships, allowing to visualise of even perform clus-\ntering on the data. In this section, we discuss four such methods\nthat are better suited for data exploration and visualisation. All\nmethods below claim to tackle ‘short-circuit’ effects, whereby\na manifold with many folds, like e.g. the swiss-roll dataset, can\nlead to jumps from one fold to the other. See Meil˘a and Zhang\n(2024) for a review on manifolds.\n4.2.1. Distance & Divergence measures\nThe most commonly distance measures used in manifold\nlearning are the Euclidean and geodesic distances. The well\nknown Euclidean distance gives the length of a line connecting\ntwo points:\nd =\nqX\n(x −y)2 or d = ||x −y||\n(4)\nWhile the geodesic is the shortest distance between two\npoints, which differs from the Euclidean distance on a non-flat\nspace.\nOther useful measures include the Mahalanobis distance that\ngives the distance between a point and distribution, the Wasser-\nstein distance that measures the ‘work’ needed to transform\non distribution to another, the Shannon divergence that mea-\nsures the similarity between two probability distributions, the\nKullback-Leibler (KL) divergence that measures the relative in-\nformation content between two distributions, and the Procrustes\nalignment that is a comparison measure of shapes.\n7\n(a)\n(b)\nFigure 2: Dimensional reduction with (a) Isomap, which finds the geodesic\nof neighbouring points, and b) Locally-linear Embedding, which find neigh-\nbourhoods where linear approximation holds. Both methods as showcased on\nthe ’Swiss roll’ dataset, where the curve of the manifold could cause a ’short-\ncircuit’ if a large neighbourhood is chosen. Fig. (a) adopted from Tenenbaum\net al. (2000), fig. (b) adopted from Roweis and Saul (2000).\n4.2.2. Isometric feature mapping (Isomap)\nIsomap (Tenenbaum et al., 2000) seeks to preserve global ge-\nometry of the input space, using geodesic distances. The algo-\nrithm operates in three steps 1) find neighbours within a certain\ndistance (for computational efficiency) 2) create a graph and\nfind minimum shortest path among all pairs 3) create a lower\ndimensional embedding for all points. The dimensionality of\nthe data can be found by assessing the reconstruction error as\na function of the number of projected dimensions. Generally\nspeaking, Isomap can embed very complex manifolds, but can\nsuffer from high computing time. A weakness of Isomap can be\nthat the embedded points are too close to each other, and often\norganised in elongated clusters, making it non-trivial to apply\nclustering methods the data in the new space.\nIsomap has found some applications in astronomy. Bu et al.\n(2014) used Isomap to classify SDSS DR9 (Ahn et al., 2012)\nstellar spectra. They show that PCA and Isomap lead to dif-\nferent stellar subclasses. Sasdelli et al. (2016) explored a wide\nvariety of unsupervised learning methods to automatically clas-\nsify Type Ia supernovae. They used Isomap to project to two di-\nmensions the 4d latent space learned by an autoencoder. More\nrecently, Matchev et al. (2022) explored dimensionality reduc-\ntion techniques as means to find an embedding that will isolate\nthe physical properties of exoplanet transmission spectra. Even\nthough Isomap components capture physical properties such as\nmolecule abundance and cloud opacity, they conclude that PCA\nis preferred as a more interpretable embedding.\n4.2.3. Locally Linear Embedding (LLE)\nLocally Linear Embedding8 (LLE, Roweis and Saul, 2000)\nwas designed to overcome the limitations of general linear pro-\n8Incidentally, Isomap and LLE appeared on the same journal side-by-side,\nwith almost simultaneous submission and acceptance dates.\njection methods (see Section §4.1), by creating local linear de-\ncomposition. The steps of the algorithm are largely similar to\nIsomap with the main difference on the second step. Namely,\nafter finding the nearest neighbours of each point, instead of\ncalculating the geodesic distances between the points as done\nin Isomap, LLE reconstructs the point in questions using a\nweighted sum of the neighbours. The fact that the algorithhm\noperates on small scales on the manifold, provides the flexibil-\nity to embed complex data. Contrary to PCA, LLE does not\nallow for the projection of new data once the manifold has been\nlearned, however it can a powerful tool for data exploration, in-\ncluding the identification of outliers. Vanderplas and Connolly\n(2009) presented the first application of LLE in astronomy, em-\nbedding the SDSS DR7 spectra (Abazajian et al., 2009) into two\ndimensions. They also presented a workaround for reusing the\nlearned weight matrices of the neighbours in order to project\nnew spectra. For the interested reader, Ghojogh and Sharma\n(2022) give a detailed overview of the LLE method and its many\nvariants.\nLLE applications in astronomy include embedding of stellar\nspectra (Vanderplas and Connolly, 2009; Daniel et al., 2011; Bu\net al., 2013) and lightcurves of eclipsing binary stars (Matijeviˇc\net al., 2012; Kirk et al., 2016; B´odi and Hajdu, 2021). Daniel\net al. (2011) showed that the more than two dimensions are nec-\nessary to embed SDSS spectra, and in particular that most of the\nstellar spectra fall on a sequence in a 3d space (see their Figure\n4). Bu et al. (2013) performed a comparison of LLE against\nPCA on M-type stellar spectra and found that LLE performs\nbetter, apart from the high noise regime.\n4.2.4. t-Distributed Stochastic Neighbor Embedding (tSNE)\nT-Distributed\nStochastic\nNeighbor\nEmbedding\n(tSNE,\nvan der Maaten and Hinton, 2008) is another data visualisation\nmethod, developed to operate on non-linear manifolds.\nIn\nparticular, t-SNE models the likelihood of the data in the high\ndimensional space using a Gaussian distribution mapped to a\nStudent-t distribution in the lower dimensional space. The long\ntails of the t-distribution solve the problem of over crowding\npresent in other methods. Due to the flexible representation of\nthe data, t-SNE can learn more than one manifolds if they exist\nin the data. However, severe weaknesses of t-SNE include, 1)\nover-segmentation of the data in the lower dimensional space,\ni.e. creating more clusters than necessary 2) inability to project\nnew data to an existing map 3) non-deterministic mapping 4)\ninability to map data to more than three dimensions, particu-\nlarly problematic if the intrinsic dimension of the data is higher\nthan that. Hence, the algorithm has been recommended since\nits inception to be used as data visualisation tool. However,\nparametric t-SNE (van der Maaten, 2009), aims to tackle some\nof these issues.\nEven though the weaknesses described above are clearly\nstated in the paper of van der Maaten and Hinton (2008), the\nastronomy community has made several attempts to use t-SNE\nas means for revealing clustering of data in the lower dimen-\nsional manifold. Often, the the projection is arbitrarily cho-\nsen to be two dimensional, which is not ideal for astronomical\ndata. In addition, all t-SNE projections mentioned below that\n8\ninclude a large number of objects suffer by increased complex-\nity of the projected map making it impossible to group objects\non the learned embedding. Kinson et al. (2021) used probabilis-\ntic random forest to identify Young Stellar Objects (YSOs). In\ntheir Figure 13, they show clearly that the minority class they\nare interested in, cannot be blindly identified in the t-SNE pro-\njection.\nˇCotar et al. (2019) used supervised and unsupervised meth-\nods to identify carbon-enhanced metal-poor (CEMP) candidate\nstars. The authors classified more than 600 thousand spectral\nfrom the GALctic Archaeology with HERMES pilot survey\n(GALAH, Duong et al., 2018). Their Figure 2, shows the clear\nlimitations of trying to group similar objects together. Apart\nfrom a prominent group of CEMP stars identified manually, and\nwith a supervised method, other CEMP stars are found spread\nover the t-SNE projection, and similarity to their neighbours is\nnot evident. Steinhardt et al. (2020) attempted to use t-SNE to\nidentify quiescent galaxies. However, the map produced cannot\nbe generalised. As the authors note, t-SNE needs to be recom-\nputed every time new data need to be mapped. They chose to\nselect a sample of galaxies with very narrow redshift ranges\n(0.9 < z < 1.1, and 1.9 < z < 2.1). Their test sample sample is\ndrawn from the same redshift distribution. However, if the map\nshould be produced every time, and there exists a training sam-\nple, the approach is no different to k-nearest neighbours, since\nt-SNE finds objects with similar colours using a Euclidean dis-\ntance. More recently, Youakim et al. (2023) used a known sam-\nple of ω Cen stars and a t-SNE projection of chemical abun-\ndances and stellar kinematics to identify members of the stellar\nstream. They defined a convex hull region on the t-SNE map\nbased on known ω Cen stars and performed 100-fold bootstrap\nto identify the stars that fall within the pre-defined region, to\ncombat the intrinsic stochasticity of t-SNE.\nWhen few data are examined with a variety of classes\npresent, t-SNE seems to perform well in grouping similar ob-\njects together. For example, George et al. (2017) created a 3d\nt-SNE map of the learned features of a convolutional neural net-\nwork applied on the GravitySpy benchmark data. The modest\nsize dataset ( 8,500 elements) shows very well separated clus-\nters. Contrary, t-SNE not only requires significant computing\nresources, but it also struggles to form distinct clusters as the\nnumber of points increases (roughly > 105). In such cases, a\nt-SNE projection can be used as data mining tool, for example\nto search for outliers (Giles and Walkowicz, 2019; Webb et al.,\n2020b) or visualise the learned features of other methods (Khan\net al., 2019; Chen et al., 2020a).\nFrom astronomical works available in the literature using t-\nSNE, we can conclude a few general trends: 1) 3d projections\nseem to separate the data better than 2d projections 2) t-SNE\ncannot identify well minority classes 3) 2d maps can be used\nin conjunction with domain expertise to explore the map for\nanomalies 4) the shape of the embedded map is usually too\ncomplex for automatic clusterers (see section §5). Often k-\nnearest neighbours can be used as an alternative if the aim is\nto search for similar objects based on a training set rather than\nsearching for neighbours in the t-SNE projection.\n4.2.5. Uniform Manifold Approximation and Projection for Di-\nmension Reduction (UMAP)\nUniform Manifold Approximation and Projection for Dimen-\nsion Reduction (UMAP, McInnes et al., 2018) is a more recent\ndevelopment in manifold learning. UMAP aims to better pre-\nserve the global structure of the data compared to t-SNE, with\nbetter scalability to larger datasets, and no restrictions on the\nnumber of dimensions. UMAP is based on the underlying as-\nsumptions that 1) the data are uniformly distributed on the man-\nifold and 2) the underlying manifold is locally connected, i.e.\nthere are no holes. As long as these conditions are satisfied lo-\ncally, UMAP can be used to create a directed graph to model\nthe k-nearest neighbours.\nDue to the stochastic approach on nearest neighbour search\nand gradient descent, UMAP, like t-SNE, does not create a de-\nterministic mapping. This is a significant drawback when cre-\nating an embedding with subsample of the data in order to per-\nform clustering in the lower dimensional space. Figure 7 of\nMcInnes et al. (2018) shows the alignment of various projected\nsubsamples. Even though UMAP is more stable than t-SNE, a\nnumber of clusters are misaligned, hence it is not recommended\nto use any of the previous methods for classification, but rather\nonly for data exploration and visualization.\nAstronomy works using UMAP span fields from molecules\n(Lee et al., 2021), to stars (Sanders and Matsunaga, 2023) and\ngalaxies (Vega-Ferrero et al., 2023), and across wavelengths.\nStorey-Fisher et al. (2021) searched for anomalies using a gen-\nerative adversarial network.\nThey used UMAP as a visual-\nization tool, and they show that their detected anomalies tend\nto cluster together. Clarke et al. (2020) used Random Forest\nto classify 111 million sources from SDSS. They showed that\nUMAP creates satisfactory clustering of spectroscopic data.\nHowever, we need to caution against assuming that the same\ntrend generalises to photometric data (C. Logan private commu-\nnication), as seemingly well separated classes gradually overlap\nand new, previously not seen clusters are created ad hoc. The\nsame phenomenon is also demonstrated in Fig. 21 of Clarke\net al. (2020).\nChen et al. (2022) used UMAP to project 13 input features of\nabout 600 fast radio bursts (FRB). UMAP creates nine, very dis-\ntinct, clusters. This is a similar phenomenon that has been ob-\nserved with t-SNE, namely the learned embedding can be infor-\nmative when examining smaller datasets, while a large dataset\n(> 105) will be mapped onto a continuous distribution, e.g.\nseen in the case of galaxies (Clarke et al., 2020; Storey-Fisher\net al., 2021; Slijepcevic et al., 2023). This effect is seen in the\nUMAP representation of repeating FRB 20201124A in Chen\net al. (2023). The authors expanded on their previous work,\nthis including 1,745 FRBs. They find that the UMAP projection\nstarts to become crowded. As has been noted with t-SNE, a 3d\nmap perhaps would have been more informative. For example,\nRicketts et al. (2023) used a 3d UMAP projection, among other\nmethods, to show the rich behaviour of lightcurve segments ob-\nserved in the X-ray binary GRS1915.\n9\nFigure 3: UMAP and t-SNE projections of 10% subsample (red) and full (blue) flow cytometry dataset. If is visually clear that projection of new data on clustered\nspaced based on either of this projections leads to gross misalignment, here defined using Procrustes-based alignment. Figure adopted from McInnes et al. (2018).\n4.3. Networks\nEven though technically networks fall under the non-linear\nmanifold learning category, they deserve a dedicated discussion\ndue to their different approach on modeling the manifold and\ntheir popularity.\n4.3.1. Self-organising maps (SOM)\nSelf-organising maps (SOM, Kohonen, 1982), or Kohonen\nnetworks, are an iterative dimensional reduction method based\non competitive learning. To begin with, the geometry of the fi-\nnal map is chosen, e.g. an N × M rectangle. Each cell on the\nmap corresponds to a neuron with a randomly or otherwise (e.g.\nPCA) initialized weight vector. Each neuron is assigned to the\nmost similar data point, hitherto known as the ‘best matching\nunit’ (BMU). At the next step, the weight vector of the BMU\nand its neighbour are adjusted according to the assigned in-\nput data. The assignment between data and neurons is achieve\nthrough a distance metric, usually Euclidean.\nTo this day, SOMs continue to be very popular in astron-\nomy and have found applications across all areas galactic and\nextragalactic astronomy, only a few can be highlighted here.\nEarly attempts to demonstrate the applicability of SOMs on as-\ntronomical data include Hernandez-Pajares and Floris (1994)\nwhich grouped the Hipparcos catalog of stars, along with\nsynthetic data, to identify stellar components in the Milky\nWay (disc, halo), and Maehoenen and Hakala (1995) which a\nSOM to perform point-source detection, providing as simulated\npoint-sources as input image stamps.\nAutomated classification of stellar spectra has remained\namong the most researched topics, as it has traditionally relied\nheavily on human visualisation. SOM applications on stellar\nand galaxy spectra include Xue et al. (2001); Teimoorinia et al.\n(2022), while lightcurve classification with SOM has been at-\ntempted by Armstrong et al. (2016); Sasdelli et al. (2016). An-\nother popular application of SOMs include morphological clas-\nsification of optical D´ıaz-Garc´ıa et al. (2019); Holwerda et al.\n(2022) and radio (Galvin et al., 2020; Mostert et al., 2021;\nGupta et al., 2022) images.\nPhotometric redshift estimation is the determination of\ngalaxy redshifts based on they observed colours (see Salvato\net al., 2019, for a review). Redshift estimation with SOMs re-\nlied on the fact that galaxies with similar colours are expected\nto be at similar redshifts. Hence, a SOM map can be use to infer\nphotometric redshifts of a galaxy populations with a restricted\nspectroscopic sample (Geach, 2012; Way and Klose, 2012; Car-\nrasco Kind and Brunner, 2014; Masters et al., 2015; Speagle\nand Eisenstein, 2017; S¨uveges et al., 2017; Wright et al., 2020;\nSt¨olzner et al., 2023). Even though labels are needed to assign\nredshifts, this application is more akin to label propagation as\nthe labels themselves are not part of the training. In a simi-\nlar fashion, recently SOMs have been used to estimate physical\nparameters of galaxies (Hemmati et al., 2019; Davidzon et al.,\n2022).\nFinally, Rajaniemi and M¨ah¨onen (2002) used a SOM to in-\nvestigate the three Gamma-Ray Burst (GRB) classes reported\nin Mukherjee et al. (1998) (discussed in detail in Section §5),\n10\nFigure 4: Autoencoder network, figure adopted from Kramer (1991).\nand find no significant evidence for the existence of the third\nclass (see also, Hakkila et al., 2003, for a discussion on sample\nincompleteness).\n4.3.2. Auto-encoders (AE)\nNeural-networks have been among the first ML methods to\nbe embraced in astronomy (e.g., Adorf and Meurs, 1988; Ode-\nwahn et al., 1992; Lahav et al., 1995; Bertin and Arnouts, 1996).\nAuto-encoders (AE, Kramer, 1991, 1992), as their unsuper-\nvised counterpart, are receiving heightened attention as they are\nable to benefit from convolutions, useful for detecting features\non images, while at the same time creating a reduced, infor-\nmative, latent space. They comprise two components; first, the\nencoder part is a series of layers of progressively lower num-\nber of neurons, while the decoder starts from the narrowest part\nof the encoder, progressively expanding to layers of more neu-\nrons. The decoder part can be a mirrored architecture of the\nencoder. The narrowest part of the network is the latent space,\nalso known as the bottleneck. AEs receive as input data which\nalso form the target. Therefore, using the same training strat-\negy of supervised neural networks, the weights and biases of\nthe model are trained through backpropagation with the goal\nto replicate the input data minimising the reconstruction error.\nThis means that the latent space carries enough information to\nbe interpreted as a compressed view of the input.\nRecently, AEs have received a lot of interest with further\ncomplex architectures being developed, including denoising\nAuto-encoders (DAE, Vincent et al., 2008) which learn to re-\nconstruct a corrupted version of the data, convolutional Auto-\nencoders (CAE, Masci et al., 2011) which use convolution lay-\ners in the encoder part of the AE, Variational Auto-encoders\n(VAE, Kingma and Welling, 2013) which substitute the bottle-\nneck with gaussian distributions of which the mean and stan-\ndard deviation are learned during the training. This means that\nthe decoder can be used a probabilistic generative model. On\nthe other hand, Vector Quantised-Variatonal AutoEncoder (VQ-\nVAEs, van den Oord et al., 2017) substitute the latent random\nvariables of VAEs with categorical variables, which have appli-\ncations relating to speech and language.\nThe flexibility of AEs has inspired many astronomical appli-\ncations. Some examples9 include morphological classification\n(Ma et al., 2019; Chang et al., 2021; Spindler et al., 2021; To-\nhill et al., 2023), galaxy lens detection (Cheng et al., 2020),\nphysical parameter estimation (Frontera-Pons et al., 2017),\nlightcurve classification (Tsang and Schultz, 2019), outlier de-\ntection (Liang et al., 2023; Han et al., 2022), among others.\nLahav et al. (1996) used 13 catalogued properties of the ESO-\nLV catalogue to identify galaxy classes. The input features in-\ncluded colours, parametric (de Vaucouleurs red and blue ex-\nponents) and non-parametric (light-ratios, asymmetry, surface\nbrightness, etc) quantities. They show that the ‘encoder’ net-\nwork captures more information than linear projection with\nPCA. In the radio domain AE example applications include Ma\net al. (2019) classified radio AGN based on their morphologies\nand Mesarcik et al. (2020) used AEs to reconstruct LOFAR data\nand assess the quality of the images.\nFurther examples on physical parameter estimation include\nFrontera-Pons et al. (2017) showed that the 2d latent space of\na denoising AE applied on galaxies with redshifts 0.1 < z < 1\nshows a clear distinction between the star-forming blue cloud\nand the passive red sequence of galaxies. Tsang and Schultz\n(2019) presented an architecture that extracts features, classi-\nfies, and performs anomaly detection on star lightcurves.\nAEs have also found application in the gravitational wave\nfield. In particular, Sakai et al. (2022) use a VAE to extract\nfeatures from the 2d time-frequency spectrograms provided by\nthe Gravity Spy dataset (Bahaadini et al., 2018) clustered later\nwith other methods to identify groups of similar signals. On\nthe other hand, Moreno et al. (2022) used AEs to learn noise\ndata and identify true signals as outliers. Finally, Shen et al.\n(2017) used a DAE as part of larger ML framework to denoise\ngravitational wave signals, while Yang et al. (2023) used the\nNoise2Noise approach (Lehtinen et al., 2018) to suppress in-\nstrumental noise on gravitational wave data.\n5. Clustering\nOnce the observations have been distilled into a space that\ncarries the majority of in the information present in the data,\nthe next step is the ranking of their similarity, leading to group-\ning of similar objects together. Jain et al. (1999) provide a re-\nview of core concepts and methods on ranking and clustering\ndata. Their Figure 7 shows the taxonomy of the various meth-\nods commonly used. Detailed discussions on data clustering\nmethods can be found in Aggarwal and Reddy (2013).\nIn the following, we discuss some commonly used algo-\nrithms grouped by cluster identification philosophy: centroid\nbased partitioning, hierarchical clustering, and density based\nclustering.\n9See Section §6.1 for further examples regarding knowledge extraction.\n11\n5.1. Dissimilarities\nWe first define the notion of dissimilarity. Much like the dis-\ntance measure was introduced for learning manifolds (see Sec-\ntion §4), a pairwise dissimilarity measure is needed to assess if\ntwo points should belong to the same cluster. A common choice\nis none other than the squared distance between two points xi,\nx j (Hastie et al., 2009):\nD(xi, xj) =\np\nX\nj=1\ndj(xij, xi′ j) =\np\nX\nj=1\n(xij −xi′ j)2\n(5)\nThe extension of the pairwise dissimilarity to a group of\npoints is introduced through the linkage, which can be com-\nplete, single, average, or centroid (e.g. see James et al., 2014).\nThey each correspond to a property of the pairwise dissimilari-\nties of two clusters; complete linkage is the maximum pairwise\ndissimilarity between two clusters, single linkage is the mini-\nmum dissimilarity, while average linkage is the average pair-\nwise dissimilarity, and finally centroid linkage is the dissimilar-\nity of the centroids.\n5.2. Centroid Based clustering\nCentroid or partition based algorithms (k-means and its vari-\nants) start by a predefined number of clusters to be found within\nthe data. The data are assigned to the k clusters through an it-\nerative process which progressively minimizes the total sum of\nthe distances between the data points and the center of their\nrespective cluster.\n5.2.1. k-means\nK-means (MacQueen et al., 1967; Lloyd, 1982), much like\nPCA, has been used in astronomy in numerous occasions. A\nheuristic partitioning of the data with this method is quick and\neasy to implement: we assign n data points into k clusters by\nminimizing the intra-cluster sum of squares with an iterative\nprocedure. First, pick k random locations that will act as the\nfirst guess of the cluster centers. Next, we calculate the Eu-\nclidean distance of each point from the cluster center, assign\neach point to its closest center. Recalculate the cluster means\nfrom the members. Repeat until the cluster center does not\nchange any more. This algorithm works best when the clus-\nters are well separated. The number of clusters can be deduced\nheuristically with the elbow method, i.e. by monitoring the sum\nof the distances as a function of number of clusters.\nDue to the simplicity of the method, k-means has been ap-\nplied to a variety of astrophysical applications, including stellar\nspectra (S´anchez Almeida and Allende Prieto, 2013; Garcia-\nDias et al., 2018, 2019), the debate on the number of GRB\nclasses (Hakkila et al., 2003), clustering of X-ray spectra (Ho-\njnacki et al., 2008), optical galaxy spectra (S´anchez Almeida\net al., 2010) and so on. In particular, S´anchez Almeida et al.\n(2010) looked for k clusters in 900k SDSS spectra and high-\nlight the importance of data normalisation. They note that if\nthe data are not scaled to a common flux level, in their case\nthe g −band, the classifier is driven by the flux of the source.\nThey find that k = 17 contains 99% of the galaxies. In follow\nup work, S´anchez Almeida and Allende Prieto (2013) applied\nk-means clustering to the SEGUE star sample, obtained also\nwith SDSS (Yanny et al., 2009), finding 16 classes in the data.\nGarcia-Dias et al. (2018) applied k-means with k = 50 on the\nAPOGEE SDSS stellar spectra, and later merged manually the\nclasses into nine groups, driven by measured physical proper-\nties (temperature, special gravity, and chemical abundance).\n5.2.2. k-medoids & c-means\nAmong the most rigid assumptions of k-means is the fact that\nthe centre of the cluster can be an arbitrary point, and the fact\nthat the membership assignment is strict. The k-medoids and\nfuzzy c-means algorithms are attempting to relax these condi-\ntions. These improvements have motivated applications in a\nwide variety of astronomical areas.\nThe k-medoids (Kaufman and Rousseeuw, 1990) algorithm\nwas an update to the k-means algorithm, that inspired many\nmore optimisations (e.g. Ng and Han, 2002; Park and Jun,\n2009).\nSimilarly to k-means, the expected number of clus-\nters has to be determined by the user. However, contrary to\nk-means, the center of the cluster is one of the data points, and\nthe distance of a data point to the center of the cluster can be\nany arbitrary similarity measure, not necessarily the Euclidean\ndistance. Applications include globular cluster membership al-\nlocation (Pasquato and Chung, 2019) clustering of the latent\nspace representations of galaxy morphologies to improve the\nHubble sequence in a data-driven way (Cheng et al., 2021) and\nclustering of eclipsing binary light curves (Modak et al., 2018).\nFuzzy c-means (Bezdek et al., 1984) is a soft classifier. Con-\ntrary to k-means and k-medoids where a source can belong to\nonly one cluster, c-means allows a fuzziness which controls the\ndegree up to which a source is permitted to belong to more than\none clusters. This classification can be seen as a probabilistic\nassignment, a desirable trait for astronomical data. Barra et al.\n(2008) and Benvenuto et al. (2018) used fuzzy c-means on solar\nimaging and solar flare classification respectively, while Jamal\net al. (2018) applied the same method to assess the reliability of\nspectroscopic redshifts.\n5.3. Hierarchical clustering\nHierarchical clustering methods do not use a predefined num-\nber of clusters. Instead, they use a dissimilarity measure and a\nstopping criterion to aggregate, or break up the data into clus-\nters.\nAgglomerative clustering (AL Johnson, 1967) is a ‘bottom-\nup’ clustering approach. Initially, each point is considered its\nown cluster.\nA dissimilarity measure is employed to merge\nclusters that are located close by, creating a dendrogram struc-\nture. There is an extensive list of linkage criteria that has been\nused in the literature. Divisive clustering is the inverse proce-\ndure compared to agglomerative clustering. It’s a ‘top-down’\napproach, where the entire dataset is considered one large clus-\nter, progressively split to sub-clusters based on a dissimilarity\ncriterion. A dendrogram is created, starting by the objects that\nhave the largest dissimilarity.\n12\nTo name a few examples, hierarchical clustering has been\napplied on galaxy morphologies (Hocking et al., 2018; Mar-\ntin et al., 2020; Dai et al., 2023), as part of a hybrid neural\nnetwork for point-source identification (Andreon et al., 2000),\nstellar type classification (Garcia-Dias et al., 2019), and fast ra-\ndio transients (Aggarwal et al., 2021). Hojnacki et al. (2008)\nused agglomerative clustering as part of their clustering frame-\nwork to identify the number of classes within the data, which\nwere later retrieved with k-means.\n5.4. Minimum Spanning Tree (MST) or Friends of Friends\n(FoF)\nThe Minimum Spanning Tree (MST, see Graham and Hell,\n1985, for a historical review) is better known as the Friends-\nof-friends algorithm in astronomy, used to measure the clus-\ntering of galaxies in 2d and 3d space (Press and Davis, 1982;\nEinasto et al., 1984), which is outside the scope of this review.\nA MST is an acyclic graph that provides the shortest path be-\ntween two points, hence it can be interpreted as the geodesic.\nExamples of MSTs applications relevant to source classifica-\ntion include refining the cluster membership fist found through\nk-means (Cantat-Gaudin et al., 2019), and similarity search of\nsupernova lightcurves (de Souza et al., 2023). MST is used as\npart of Isomap (see Section §), and (H)DBSCAN (see Section\n§5.6).\n5.5. Probabilistic Clustering\nProbabilistic clustering methods aim to model the observed\ndata distribution as a random variable drawn from an under-\nlying multivariate distribution (Aggarwal and Reddy, 2013, p.\n61-86). A very commonly used method is Gaussian Mixture\nModels (GMM, Dempster et al., 1977). As the name implies,\nGMMs aim to approximate the underlying distribution that gen-\nerated the observed sample as a mixture of K gaussians. Thus,\nthe clustering problem is transformed to an optimisation prob-\nlem under which we are trying to find the optimal number of\ngaussians, and their means and variances that will generate best\nthe observed data. A typical method to solve this optimisation\nproblem is the iterative Expectation Maximization (EM Demp-\nster et al., 1977; McLachlan, 2015) algorithm. The choice of\nthe optimal model parameters can be done with information\ncriteria, such as the Bayesian Information Criterion (BIC) or\nthe Akaike Information Criterion (AIC), which penalize mod-\nels with very high number of free parameters.\nMeingast et al. (2017) introduced Pnicer, a GMM to model\nthe line of sight extinction of the interstellar medium. GMMs\nhave been used frequently to model the latent space of VAEs,\nfor example to model X-ray Chandra data of Tycho’s super-\nnova (Iwasaki et al., 2019), Cheng et al. (2020) to model opti-\ncal images with strong gravitational lenses, and Karmakar et al.\n(2018) to detect stellar clusters in images. We will revisit this\ntype of application in Section 6.1.\nGMMs have also been used to model galaxies, for example\nin terms of kinematics (e.g., Ortega-Martinez et al., 2022; Du\net al., 2019, 2020) and as a population (Fraser et al., 2023). In\nthe latter, the authors modeled the IllustrisTNG-100 simulation\nas if was an observed population. They used GMM to extract\nthree clusters of galaxies based on their broad band photometry.\n5.6. Density Based Clustering\nDensity-based Spatial Clustering of Applications with Noise\n(DBSCAN Ester et al., 1996), and its hierarchical extension\n(HDBSCAN Campello et al., 2013, 2015), approach the pres-\nence of clusters within data from the perspective of minimum\nnumber of objects within a given radius. This definition can\nbe applicable to any number of dimensions, but it does suf-\nfer from the curse of dimensionality which naturally makes the\ndata sparse. However, since the input features can be often cor-\nrelated, a workaround is to apply these methods following a first\ndimensional reduction of the feature space to about five dimen-\nsions (e.g., Logan and Fotopoulou, 2020).\nEster et al. (1996) used the concept of ‘core points’ and ‘bor-\nder points’ to track the density of objects within a dataset. As\nthe minimum number of neighbours must be predefined, we\nmust take into account that border points will naturally have\nless neighbours. They define as ‘clusters’ collections of points\nthat are density-reachable within their respective clusters and\nas ‘noise’ points that do not belong to any cluster. By track-\ning the connectivity of the points, DBSCAN is able to identify\nnon-convex clusters, which is not the case for centroid algo-\nrithms. This is particularly of use in astronomy for spatial ap-\nplications such as non-parametric galaxy morphology detection\n(Tramacere et al., 2016) and stellar streams (e.g., Rudick et al.,\n2009; Kounkel and Covey, 2019).\nDBSCAN has been used in many areas of astronomy, rang-\ning from galaxies (Rudick et al., 2009; Tramacere et al., 2016),\nto planet detection (Mislis et al., 2018), young stellar objects\n(Prisinzano et al., 2022), pulsars (Pang et al., 2018), and GRBs\n(Abraham et al., 2021). A large amount of works have used\nDBSCAN to identify star clusters in physical space, with exten-\nsive application on GAIA data (e.g., Castro-Ginard et al., 2018;\nGarcia-Dias et al., 2019; Castro-Ginard et al., 2019; Noormo-\nhammadi et al., 2023; He et al., 2023; Alfonso and Garc´ıa-\nVarela, 2023).\nHDBSCAN, as the hierarchical extension of DBSCAN, has\nfound also a wide usage in recent applications. HDBSCAN\nuses minimum spanning trees which allow the discovery of\nclusters with varying density, contrary to DBSCAN that set the\ndensity as a constant across the entire dataset. The implemen-\ntation of McInnes et al. (2017) provides also the detection of\noutliers in the data, and prediction support.\nApplications of HDBSCAN on star clusters and stellar\nstreams dominate the astronomical literature (see Helmi, 2020,\nfor a review of GAIA results).\nKounkel and Covey (2019)\nused five dimensional input (galactic coordinates, parallax, and\nproper mottons) to search for stellar streams in the GAIA DR2\ndata, expanding previous work of Cantat-Gaudin et al. (2018) to\nmuch larger scales. However, their find that the resulting recov-\nered clusters depend on the algorithm configuration due to the\nfact that stellar clusters that are further away will naturally have\na smaller extent on the sky, and smaller parallaxes. Their final\ncatalog contains 1,901 individual clusters of about 288k stars,\na very small fraction of the GAIA DR2 catalog. Further works\n13\nsearching for stellar clusters in GAIA DR3 have continue to\nprovide a refined view of the Milky Way local neighbourhood\n(e.g. Moranta et al., 2022; Gagn´e et al., 2023)\nLogan and Fotopoulou (2020), searched for optimal config-\nuration to split the 100 million source KiDS dataset into stars,\ngalaxies, and quasars. As mentioned earlier, they found that\na dimensionality reduction to three to five dimensions was a\nnecessary pre-processing step to be able to apply HDBSCAN\nsuccessfully in photometric colour space. Webb et al. (2020a,\n2021) presented ASTRONOMALY, a framework for transient\ndiscovery and lightcurve classification.\nThe main engine of\ntheir system is HDBSCAN clustering combined with Isolation\nForest (IF, Liu et al., 2008). Aggarwal et al. (2021) tested a\nnumber of unsupervised methods for classification of single\npulse radio transients. They concluded that either DBSCAN, or\nHDBSCAN can be used for their application, with preference\non DBSCAN.\n6. Modern approaches to learning\nUp to now, we have discussed commonly used approaches to\nunsupervised learning, namely dimensional reduction and clus-\ntering. Modern takes on learning, increasingly lean on model\nensembles and frameworks that combine ML algorithms in so-\nphisticated ways. The sharp distinction between supervised and\nunsupervised learning is becoming more blurred by approaches\nincluding self-supervised, semi-supervised, as well as transfer\nlearning and domain adaptation. Relevant reviews on these top-\nics are listed in Table 1. In the following, we discuss some of\nthe approaches found in the astronomy literature, nonetheless\nthis is a very active field with interesting methods appearing\nregularly in the literature that could find applications to astron-\nomy soon (e.g., Berthelot et al., 2019; Yoon et al., 2020).\n6.1. Modelling the bottleneck\nDue to the compact representation of the data offered by AEs,\nseveral works have recently attempted to create a combination\nof architectures to model their bottleneck. These strategies take\nadvantage of the non-linear projection of data, to the expense\nof feature interpretability. For example, Karmakar et al. (2018)\nmodelled the bottleneck with Gaussian Mixture Models in or-\nder to identify stellar clusters in images. Villar et al. (2020)\nused a combination of Gaussian processes for feature extrac-\ntion, a recurrent-autoencoder for dimensionality reduction, and\nsupervised random forest (Breiman, 2001) as their final clas-\nsifier. Tsang and Schultz (2019) used the latent space mod-\nelling of Zong et al. (2018), to classify variable star lightcurves.\nThe framework comprises a recurring neural network AE and a\nGMM to model the latent space. The GMM is further used to\nprovide one-hot classification. Therefore, their dual-network\napproach classifies the data, and at the same time identifies\nanomalies. The authors find that their approach achieves ac-\ncuracies comparable to supervised methods, for known classes.\nRalph et al. (2019) (Fig. 5) used a series of algorithms to\nproject radio images into progressively lower dimensions, ulti-\nmately clustered with k-means. However, as the authors dis-\ncuss, significant assumptions are made when assigning hard\nboundaries of classes on a SOM, that should not be left with-\nout further investigation, especially keeping in mind that SOM\ncells do not form pure categories.\nRecently, Forest et al. (2019) proposed a method that com-\nbines an AE with a SOM at the bottleneck, training both at the\nsame time. Inspired by this idea, Mong et al. (2023) devel-\noped a framework to classify ‘real vs bogus’ transient sources.\nThey find that the DESOM framework requires long training\ntimes, therefore they decoupled the AE and the SOM, by using\na SOM projection which takes as input the latent space of the\nAE. However, the authors find that the current performance of\ntheir approach is not competitive against convolutional neural\nnetworks, trained on the same data. Even though the frame-\nwork can be used as extra flagging, significant work is needed,\nin particular in extracting features from the images, and mod-\nelling the bottleneck.\n6.2. Weakly-supervised\nWeakly supervised is learning from noisy or poorly labelled\ndata. In their review, Zhou (2017) highlight three situations\nthat lead to weak supervision. Incomplete supervision reflects\nthe fact that not all classes have assigned labels. This is a very\ncommon phenomenon in astronomy, as even the definition of\nboundaries between classes is non-trivial. Inexact supervision\ncorresponds to the fact that a label might represent a particular\npart of an images, e.g. morphology of the central galaxy, while\nneighbours are present in the same cutout. Finally, inaccurate\nsupervision refers to the fact that labels might be wrong. This\nis common occurrence in astronomy, whether the labels come\nfrom automated pipelines (see Pˆaris et al., 2018; Alexander\net al., 2023, for examples in SDSS and DESI spectroscopy) or\nhuman annotators (see Huertas-Company et al., 2015; Walms-\nley et al., 2020, for examples of vote distributions among ex-\nperts and volunteers alike).\nAll the above are present in astronomy data, with the added\ncomplication that the existing labels are more often than not a\nbiased sample, e.g. mostly high signal-to-noise ratio. In the\nfollowing, we discuss further only the methods related to unsu-\npervised learning, relating to incomplete supervision.\n6.2.1. Semi-supervised\nParticularly true for astronomy, labelled training sets are\nbound to be not only the best quality examples but also typi-\ncally well understood sources. Therefore, inferring on a larger\nsample leads to out-of-distribution problems. Semi-supervised\nlearning blurs the line between the traditional unsupervised vs\nsupervised split, by using both labelled and unlabelled data dur-\ning training.\nIn their review, van Engelen and Hoos (2020) summarise\nthe underlying assumptions that need to hold true for semi-\nsupervised learning to work (see also Chapelle et al., 2006,\nfor detailed discussion). These are the following assumptions:\n• Smoothness: any two input points close to each other\nshould lead to close by points in the target distribution.\n14\nFigure 5: Complex frameworks are emerging in modern ML applications. Here we show one example of modelling the autoencoder latent space with a self-\norganising map, subsequently clustered with k-means. Figure adopted from Ralph et al. (2019).\n• Cluster10: points belonging in the same cluster, should be-\nlong to the same class.\n• Manifold: the data can be embedded in a lower dimen-\nsional manifold.\nThere is a number of strategies utilised for semi-supervised\nlearning, largely split into inductive and transductive strategies.\nThe former aim to create a predictive model, i.e. a model that\ncan be applied to unseen data, such as a neural network. Con-\ntrary, the latter aims to answer the problem within the dataset\navailable to the algorithm, e.g. find a lower dimensional embed-\nding. The approach of combining known and unknown sam-\nples is standard practice in astronomy, therefore there is a num-\nber of applications of semi-supervised learning, including ap-\nplications from AGN to supernova classification (Lawlor et al.,\n2016; Villar et al., 2020; Slijepcevic et al., 2022, to name a few).\nA related approach that is outside the scope of this review\nis the case of active learning, which progressively incorporates\nthe most uncertain prediction, as chosen by the model, drawn\nfrom the entire dataset. The data point is then presented to an\noracle and it is added into the training set, and the loop starts\nagain (Settles, 2009; Stevens et al., 2021; Lochner and Bassett,\n2021; Walmsley et al., 2020).\n6.3. Self-supervised\nSelf supervised learning learns first a pretext task, aimed to\nextract semantic information from the data. The pretext task\n10An equivalent formulation is, the decision boundary should not cross a high\ndensity region.\ncomprises training a network to identify the original image\nbased on transformed versions of the data (i.e.\nrotated, oc-\nculted, colourised, etc).\nThe learned model is then used as\ninitial guess when the downstream task is needed, i.e. classi-\nfication or object detection. Two main approaches are used in\nthe literature for pretext training: 1) contrastive learning, and 2)\nnon-contrastive learning. In the first case, the algorithm is pre-\nsented with variations of the positive example, such as zoom\nin and zoom out views of the image, cutouts, rotated, or scaled\nversion of the original. At the same time some negative ex-\namples are also given to the algorithm. The pretext task, is to\nplace the learned representations of the positive examples as\nclose as possible in the learned latent space, while at the same\ntime keeping the negative examples further away. This con-\ntrastive learning forces the algorithm to generalise the learned\nfeatures of the positive class (for a recent review see Huertas-\nCompany et al., 2023). The second non-contrastive approach\nuses only positive examples as input. This strategy involves\n‘safety gates’ such stop-gradient operations to ensure that the\nmodel will not suffer mode collapse, i.e. learn an embedding\nthat is of lower dimension than required to describe the data.\nSome of the works described previous (§6.1), might fall under\nthe non-contrastive learning, as long as the input data have been\naugmented. Since the learned representations form an abstract\ndescription of the data, they can be used input in further down-\nstream tasks, which is a very desirable feature.\nIn the method A Simple Framework for Contrastive Learn-\ning of Visual Representations (simCLR, Chen et al., 2020b) the\nauthors highlight that the major reasons of the improved per-\n15\nFigure 6: Application of contrastive self-supervised learning. Input features are augmented (rotation, flip, etc), and the contrastive loss is trained to bring represen-\ntations of the same object closer in the latent space. Figure adopted from Hayat et al. (2021).\nformance of their approach are due to 1) multiple data augmen-\ntations 2) non-linear transformation between the representation\nand the contrastive loss 3) normalized embeddings and 4) deep\nand wide architecture, large batch sizes, and longer training\ntime. In a sense, the above observations offload the effort of\nlabelling to computing time11, which depending on the applica-\ntion might or might not be desirable.\nHayat et al. (2021) applied self-supervised contrastive learn-\ning on 1.2 million SDSS images (Fig. 6), by combining the ar-\nchitecture of Chen et al. (2020b), and He et al. (2020). They in-\ntroduced augmentations relevant to astrophysical data, includ-\ning galactic reddening and point-spread function smoothing.\nThey used UMAP to visualise the 2048d representation, which\nshows that relative ordering of the learned representation cor-\nresponding to the galaxy morphology and orientation. They\nfurther showed that the learned representations can be used to\nassess galaxy morphology and estimate photometric redshift.\nThey reported a training time of 12h, on eight NVIDIA V100\nGPUs, for 50 epochs.\nSince then, further works have explored the use of various\ncontrastive-learning architectures, starting from galaxy images.\nTo name a few, Stein et al. (2021) trained on a sample of 3.5\nmillion galaxy cutouts from the Legacy Survey DR9 12, using\nthe momentum architecture of He et al. (2020). Subsequently,\nthey applied the trained network on 42 million galaxy from the\nsame survey. The resulting representations were then used to\nidentify similar objects. The dataset can be queried based on\nsimilarity on a public webpage13. Sarmiento et al. (2021) used\n11The authors quote 1.5h of training, on 128 TPUs v3 cores for ResNet-50.\n12legacysurvey.org\n13https://github.com/georgestein/galaxy_search\nspatially resolved images of 9,507 MaNGA galaxies and re-\ncovered the two-three major galaxy populations (passive, star-\nforminng and intermediate) by performing k-means in the rep-\nresentation space. Other recent works that exploit contrastive\nlearning on large galaxy samples include Wei et al. (2022), and\nVega-Ferrero et al. (2023).\nOn the other hand, Bootstrap Your Own Latent space (BYOL,\nGrill et al., 2020) is an example of the non-contrastive self-\nsupervised approach. BYOL does not use negative examples\nduring the learning phase, which would be used, e.g. in the\ncase of simCLR, to avoid mode collapse. Instead the authors\nused two networks, the online and target neural networks which\nare built to interact and learn from each other. The online net-\nwork, predicts the target network’s representation of a different\naugmentation of the same image. The learning is refined by\niterating the procedure flipping between the online and target\nnetworks. This method has been shown to outperform simCLR\non everyday images, and it is slowly gaining popularity in as-\ntronomy (Guo et al., 2022; Slijepcevic et al., 2024).\n6.4. Transfer learning & Domain adaptation\nAnother approach to overcome the bottleneck of obtaining\nlarge labelled data and working with out-of-distribution data\nis transfer learning. Domain adaptation is a subset of trans-\nfer learning, whereby the objective is to match a source dis-\ntribution, S, that contains labelled data, to a target distribu-\ntion, T , that might be data not drawn from identical distribu-\ntion as the source. The goal of domain adaptation is to bridge\nthe difference between the source and target distributions, often\ncalled a shift. As the ultimate goal is to link the distribution\np(x, y) = p(y)p(x|y) in the source pS(x, y) and target domains\n16\npS(x)\npS(y)\npS(y|x)\nShift type\nvs\nvs\nvs\npT (x)\npT (y)\npT (y|x)\n,\n=\nPrior shift\n,\n=\nCovariate shift\n=\n,\nData drift\n,\n,\n,\nintractable\nTable 2: Possible reasons for domain adaption and necessary conditions for\ntraining.\npT (x, y), we can expect one of three discrepancies, prior shift,\ncovariate shift, data drift, based on the origin of the disagree-\nment. Table 2 summarises the shift types, and how they relate\nto the marginal distribution p(x), the prior, p(y), and the condi-\ntional distribution p(y|x). Zhuang et al. (2020) give a structured\ndescription of the various methods and strategies available in\nthe ML literature, split mainly into instance-based and feature-\nbased, while Farahani et al. (2021) give an overview of domain\nadaptation approaches in shallow and deep learning.\nTransformation equations are being used to, e.g. map instru-\nment photometric systems to AB magnitudes, or to map from\none standard photometric system to another (Bessell, 2005).\nSuch transformations are a precursor of domain adaptation,\nwhich is nascent field in its modern form in astronomy. Recent\napplications include unsupervised domain adaptation of stellar\nspectra (O’Briain et al., 2020). They used the UNIT framework\n(Liu et al., 2017) which creates a common latent space created\nwith a VAE, trained under adversarial conditions.\nO’Briain\net al. (2020) created two latent spaces, one for synthetic and\nreal data and one only for real data. They found that the shared\nlatent space was key in linking between data and synthetic spec-\ntra. Other recent applications include the prediction of physical\nparameters using simulation-based inference with a variety of\nmethods, (kernel PCA, Gilda et al., 2021), including supervised\napproaches that are outside the scope of this review.\n´Ciprijanovi´c et al. (2023) created a universal domain adapta-\ntion framework, DeepAstroUDA, using semi-supervised learn-\ning, with application on galaxy morphology. In their paper, the\ngive an excellent overview of domain adaptation methods rele-\nvant to astronomy. Their method combines the key ingredients\nof cross-entropy clustering to cluster source data, adaptive clus-\ntering to associate unlabelled data to labelled examples, and fi-\nnally entropy loss to separate emerging new categories from\ndata, including also outliers. They show that their method trans-\nfers knowledge successfully in the feature space, applicable in\nreal-world and astronomical data.\nWe are confident that domain adaptation will find soon many\nmore applications in astronomy, as we embrace simulation-\nbased or likelihood-free inference.\n7. Recommendations\nBelow are a few general recommendations based on past ex-\nperience, and many discussions with ML practitioners. See also\nBuchner et al., sub. for recommendations to newcomers in the\nfield of ML in astronomy.\n7.1. Appropriateness\nMachine-learning should be used only when necessary14.\nCertain ML approaches can be used as alternatives to traditional\nmethods, such as least squares minimisation, keeping in mind\nthat they will carry all the prior assumptions and biases of those\nmethods. ML methods can be used also as exploration tools,\nsuch as finding the number of classes in the data, or looking\nfor outliers. The pitfall in such exploration is that we always\nget back an answer. This means that we might look for distinct\nclasses when a continuous distribution is more appropriate, or\nuse a very biased sub-population to draw conclusions for the\nparent distribution. Manifold learning methods such as tSNE\nand UMAP are particularly prone to hallucinate new clusters.\n7.2. Benchmarks\nComputer science practitioners have invested many years to\ncreate benchmark datasets (MNIST, CIFAR, etc). Astronomy\napplications are far from such widely used benchmarks, proba-\nbly for good reasons. However, data challenges have emerged\nfor specific applications that pit codes against each other (e.g.,\nHildebrandt et al., 2010; Euclid Collaboration et al., 2020;\nSavi´c et al., 2023; Hartley et al., 2023). Such data challenges,\nwhile very useful, need to be interpreted with a pinch of salt\nwhen run on simulated datasets, due to domain shifts expected\nbetween data and simulations (see Section 6.4).\n7.3. Visualisation\nInput space: Topcat (Taylor, 2005) is an extremely powerful\ndata exploration tool. We recommend to always plot projections\nand 3d plots of the input parameter space. Astronomy data are\nfull problematic artifacts that creep into the final products, even\nfor the most sophisticated pipeline. Data correlation matrices,\nwhen the number of dimensions allows, are very useful to drop\nextraneous features.\nLatent space: When examining a latent space, it is impor-\ntant to not over interpret structures, critically important when\nexamining the latent representation of a labelled sample (e.g.\nspectroscopic sample of galaxies). As shown in Figure 3, a\nlatent space of a subsample might show different substructure\ncompared to the parent sample.\nOutput: Confusion matrices are very informative, especially\nwhen more than two classes are involved. It is good practice to\nwrite the number of objects per cell in addition to the colour-\nbar. We recommend that the colourbar is scaled fixed to the\nrange [0-1]. We motivate the field to adopt as standard prac-\ntice to assign uncertainties in the model performance. Random\nseeds should be kept fixed during development, and mentioned\nin publications. However, they can induce significant variation\nin the trained model when left free.\n14The author has heard the question: ‘What shall I do when I have only a few\ndata?’\n17\n7.4. Generalisation\nNew ideas must necessarily pass through a phase of experi-\nmentation and exploration before they are widely accepted. ML\nis no different. The enthusiasm of getting closer to ‘intelli-\ngent systems’ and ‘knowledge discovery from data’ is palpable.\nHowever, it is paramount that we distinguish between theory\nand practice. Many works have used simplified, simulated data\nto demonstrate zeroth order feasibility of a methods. The reality\nof astronomical data does not stop there. Missing data, either\nnot observed or not detected, noisy data, instrumental effects,\nand so on, need to be incorporated as part of the modelling, ei-\nther directly in the algorithm (e.g. bayesian neural networks) or\nthrough bootstraping.\nIt is worth noting, that some areas such as gravitational waves\nare in the exact opposite situation compared to galaxy evolu-\ntion. Namely, gravitational waves benefit from a strong theo-\nretical foundation and are currently in the process of collecting\nlarger observed samples. The approach and goal in this case is\ndifferent, where ML is used as part of the data reduction (e.g.\ndenoising) or signal detection as anomalies, and not for cluster-\ning.\n7.5. Barriers to discovery\nWe recommend to bravely explore higher dimensional spaces\nand prioritise continuous distributions over classification when-\never possible. It is tempting to refine the Hubble morphology\nsequence of galaxies using a data-driven approach (e.g. Cheng\net al., 2021, for an interesing discussion). Galaxy morpholo-\ngies, nevertheless, mostly likely occupy a continuous space and\ndiscovery tools based on similarity15 might be more informative\n(e.g., Walmsley et al., 2022).\nOften the model hyperparameter choices are driven by prior\nknowledge and intuitive expectations based on highly biased\nand incomplete examples drawn from past datasets. We recom-\nmend to trust the data over models when the goal is discovery\nof new phenomena. Allen et al. (2024) in their review, discuss\ncategories of discoveries, model intrepretability and validation\nin ML.\n8. Summary\nThis review summarised the usage of Unsupervised Learning\nin Astronomy applications. Following a learning workflow, we\nsummarised the most popular methods used in astronomy high-\nlighting only some of the many thousands published works. We\nobserved a few patterns: some works set out to use machine-\nlearning to solve a well defined and specific problem, other\nworks use a suite of algorithms on a common training dataset,\nothers use some machine learning as part of larger analysis\nframework. Recently, a number of works combine machine-\nlearning algorithms in very complex pipelines, some times with\n15https://mwalmsley-decals-similarity-similarity-papkyg.\nstreamlit.app/\nunclear advantages. The field is moving towards complex archi-\ntectures, which aim to link domains in order to extract knowl-\nedge by leveraging past efforts.\nHowever, no method is infallible. The choice and training\nof a machine-learning algorithm combined with implicit and\nexplicit prior assumptions, introduced for example in the con-\nstruction of the training set, harbour biases that might be diffi-\ncult to diagnose. It is important that the scientific question is\nwell defined before settling on any methods. Even though ex-\nperimenting with new ML algorithms is useful, and certainly\nfun, domain knowledge shall remain the ultimate tool in the as-\ntronomer’s toolbox.\nAcknowledgements\nThe author would like to acknowledge many interesting dis-\ncussions with colleagues that predate the writing of this review:\nJohannes Buchner, Kartheik Iyer, Kai Polsterer, Crispin Lo-\ngan, Grant Stevens, Myank Singhal.\nThe author is grateful\nfor her participation in the Kavli Summer Program in Astro-\nphysics 2019 hosted at University California Santa Cruz, and\nthe 2023 Kavli Institute for Theoretical Physics (KITP) pro-\ngramme ”Building a Physical Understanding of Galaxy Evo-\nlution with Data-driven Astronomy”. This research has made\nheavy use of NASA’s Astrophysics Data System.\n18\nAppendix A. Method overview\nTable A.3: Dimensional reduction methods discussed in this work\nMethod\nAcronym\nReference\nSingular Value Decomposition\nSVD\nPress et al. (2007); Ivezi´c\net al. (2014)\nPrincipal Component Analysis\nPCA\nHotelling (1936)\nKernel principal component analysis\nKernel PCA\nSch¨olkopf et al. (1998)\nIndependent Component Analysis\nICA\nHyv¨arinen and Oja (2000,\nreview)\nNon-Negative Matrix Factorization\nNNMF\nLee and Seung (1999)\nSelf-Organising Map\nSOM\nKohonen (1982)\nDeep Embedded Self-Organising Map\nDESOM\nForest et al. (2019)\nAuto-Encoder\nAE\nKramer (1991, 1992)\nVector quantized Variational Autoencoder\nVQ-VAE\nvan den Oord et al. (2017)\nGaussian Mixture Models\nGMM\nDempster et al. (1977)\nIsometric feature mapping\nIsomap\nTenenbaum et al. (2000)\nLocal Linear Embedding\nLLE\nRoweis and Saul (2000)\nt-distributed Stochastic Neighbor Embedding\ntSNE\nvan der Maaten and Hin-\nton (2008)\nUniform Manifold Approximation and Pro-\njection for Dimension Reduction\nUMAP\nMcInnes et al. (2018)\nSimple Framework for Contrastive Learning\nof Visual Representations\nsimCLR\nChen et al. (2020b)\nBootstrap Your Own Latent\nBYOL\nGrill et al. (2020)\nTable A.4: Clustering methods discussed in this work\nMethod\nAcronym\nReference\nk-means\nMacQueen et al. (1967);\nLloyd (1982)\nk-medoids\nNg and Han (2002); Park\nand Jun (2009)\nHierarchical Clustering\nHC\nHastie et al. (2009)\nAgglomerative clustering\nAL\nJohnson (1967)\nMinimum Spanning Tree16\nMST\nGraham and Hell (1985,\nhistorical review)\nDensity-based Spatial Clustering of Applica-\ntions with Noise\nDBSCAN\nEster et al. (1996)\nDensity-based Spatial Clustering of Applica-\ntions with Noise\nHDBSCAN\nCampello et al. (2013);\nMcInnes et al. (2017)\nUnsupervised Photometric Membership As-\nsignment in Stellar Clusters\nUPMASK\nKrone-Martins and Moit-\ninho (2014b)\nIsolation Forest\nIF\nLiu et al. (2008)\nFuzzy c-means\nFCM\nBezdek et al. (1984)\nUnsupervised fuzzy clustering\nGath and Geva (1989)\nBootstrap Your Own Latent\nBYOL\nGrill et al. (2020)\n16Known as Friends-of-Friends algorithm in astronomy.\n19\nReferences\nAbadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Cor-\nrado, G.S., Davis, A., Dean, J., Devin, M., Ghemawat, S., Goodfellow,\nI., Harp, A., Irving, G., Isard, M., Jia, Y., Jozefowicz, R., Kaiser, L.,\nKudlur, M., Levenberg, J., Man´e, D., Monga, R., Moore, S., Murray, D.,\nOlah, C., Schuster, M., Shlens, J., Steiner, B., Sutskever, I., Talwar, K.,\nTucker, P., Vanhoucke, V., Vasudevan, V., Vi´egas, F., Vinyals, O., War-\nden, P., Wattenberg, M., Wicke, M., Yu, Y., Zheng, X., 2015.\nTensor-\nFlow: Large-scale machine learning on heterogeneous systems.\nURL:\nhttps://www.tensorflow.org/. software available from tensorflow.org.\nAbazajian, K.N., Adelman-McCarthy, J.K., Ag¨ueros, M.A., Allam, S.S., Al-\nlende Prieto, C., An, D., Anderson, K.S.J., Anderson, S.F., Annis, J., Bah-\ncall, N.A., et al., 2009.\nThe Seventh Data Release of the Sloan Digital\nSky Survey. ApJS 182, 543–558. doi:10.1088/0067-0049/182/2/543,\narXiv:0812.0649.\nAbraham, S., Mukund, N., Vibhute, A., Sharma, V., Iyyani, S., Bhattacharya,\nD., Rao, A.R., Vadawale, S., Bhalerao, V., 2021. A machine learning ap-\nproach for GRB detection in AstroSat CZTI data. MNRAS 504, 3084–3091.\ndoi:10.1093/mnras/stab1082, arXiv:1906.09670.\nAdorf, H.M., Meurs, E.J.A., 1988. Large-scale structures in the universe, in:\nSeitter, W.C., Duerbeck, H.W., Tacke, M. (Eds.), Large-scale structures in\nthe universe. volume 310, p. 315. doi:10.1007/3-540-50135-5\\_86.\nAggarwal, C.C., Reddy, C.K., 2013. Data Clustering: Algorithms and Appli-\ncations. 1st ed., Chapman & Hall/CRC.\nAggarwal, K., Burke-Spolaor, S., Law, C.J., Bower, G.C., Butler, B.J., Demor-\nest, P.B., Lazio, T.J.W., Linford, J., Sydnor, J., Anna-Thomas, R., 2021. Ro-\nbust Assessment of Clustering Methods for Fast Radio Transient Candidates.\nApJ 914, 53. doi:10.3847/1538-4357/abf92b, arXiv:2104.07046.\nAgnello, A., Kelly, B.C., Treu, T., Marshall, P.J., 2015. Data mining for gravi-\ntationally lensed quasars. MNRAS 448, 1446–1462. doi:10.1093/mnras/\nstv037, arXiv:1410.4565.\nAhn, C.P., Alexandroff, R., Allende Prieto, C., Anderson, S.F., Anderton, T.,\nAndrews, B.H., Aubourg, ´E., Bailey, S., Balbinot, E., Barnes, R., Bautista,\nJ., Beers, T.C., Beifiori, A., Berlind, A.A., Bhardwaj, V., Bizyaev, D., Blake,\nC.H., Blanton, M.R., Blomqvist, M., Bochanski, J.J., Bolton, A.S., Borde,\nA., Bovy, J., Brandt, W.N., Brinkmann, J., Brown, P.J., Brownstein, J.R.,\nBundy, K., Busca, N.G., Carithers, W., Carnero, A.R., Carr, M.A., Casetti-\nDinescu, D.I., Chen, Y., Chiappini, C., Comparat, J., Connolly, N., Crepp,\nJ.R., Cristiani, S., Croft, R.A.C., Cuesta, A.J., da Costa, L.N., Davenport,\nJ.R.A., Dawson, K.S., de Putter, R., De Lee, N., Delubac, T., Dhital, S.,\nEalet, A., Ebelke, G.L., Edmondson, E.M., Eisenstein, D.J., Escoffier, S.,\nEsposito, M., Evans, M.L., Fan, X., Femen´ıa Castell´a, B., Fern´andez Al-\nvar, E., Ferreira, L.D., Filiz Ak, N., Finley, H., Fleming, S.W., Font-Ribera,\nA., Frinchaboy, P.M., Garc´ıa-Hern´andez, D.A., Garc´ıa P´erez, A.E., Ge, J.,\nG´enova-Santos, R., Gillespie, B.A., Girardi, L., Gonz´alez Hern´andez, J.I.,\nGrebel, E.K., Gunn, J.E., Guo, H., Haggard, D., Hamilton, J.C., Harris,\nD.W., Hawley, S.L., Hearty, F.R., Ho, S., Hogg, D.W., Holtzman, J.A.,\nHonscheid, K., Huehnerhoff, J., Ivans, I.I., Ivezi´c, ˇZ., Jacobson, H.R., Jiang,\nL., Johansson, J., Johnson, J.A., Kauffmann, G., Kirkby, D., Kirkpatrick,\nJ.A., Klaene, M.A., Knapp, G.R., Kneib, J.P., Le Goff, J.M., Leauthaud, A.,\nLee, K.G., Lee, Y.S., Long, D.C., Loomis, C.P., Lucatello, S., Lundgren,\nB., Lupton, R.H., Ma, B., Ma, Z., MacDonald, N., Mack, C.E., Mahade-\nvan, S., Maia, M.A.G., Majewski, S.R., Makler, M., Malanushenko, E.,\nMalanushenko, V., Manchado, A., Mandelbaum, R., Manera, M., Maras-\nton, C., Margala, D., Martell, S.L., McBride, C.K., McGreer, I.D., McMa-\nhon, R.G., M´enard, B., Meszaros, S., Miralda-Escud´e, J., Montero-Dorta,\nA.D., Montesano, F., Morrison, H.L., Muna, D., Munn, J.A., Murayama,\nH., Myers, A.D., Neto, A.F., Nguyen, D.C., Nichol, R.C., Nidever, D.L.,\nNoterdaeme, P., Nuza, S.E., Ogando, R.L.C., Olmstead, M.D., Oravetz, D.J.,\nOwen, R., Padmanabhan, N., Palanque-Delabrouille, N., Pan, K., Parejko,\nJ.K., Parihar, P., Pˆaris, I., Pattarakijwanich, P., Pepper, J., Percival, W.J.,\nP´erez-Fournon, I., P´erez-R`afols, I., Petitjean, P., Pforr, J., Pieri, M.M., Pin-\nsonneault, M.H., Porto de Mello, G.F., Prada, F., Price-Whelan, A.M., Rad-\ndick, M.J., Rebolo, R., Rich, J., Richards, G.T., Robin, A.C., Rocha-Pinto,\nH.J., Rockosi, C.M., Roe, N.A., Ross, A.J., Ross, N.P., Rossi, G., Rubi˜no-\nMartin, J.A., Samushia, L., Sanchez Almeida, J., S´anchez, A.G., Santiago,\nB., Sayres, C., Schlegel, D.J., Schlesinger, K.J., Schmidt, S.J., Schneider,\nD.P., Schultheis, M., Schwope, A.D., Sc´occola, C.G., Seljak, U., Sheldon,\nE., Shen, Y., Shu, Y., Simmerer, J., Simmons, A.E., Skibba, R.A., Skrutskie,\nM.F., Slosar, A., Sobreira, F., Sobeck, J.S., Stassun, K.G., Steele, O., Stein-\nmetz, M., Strauss, M.A., Streblyanska, A., Suzuki, N., Swanson, M.E.C.,\nTal, T., Thakar, A.R., Thomas, D., Thompson, B.A., Tinker, J.L., Tojeiro,\nR., Tremonti, C.A., Vargas Maga˜na, M., Verde, L., Viel, M., Vikas, S.K.,\nVogt, N.P., Wake, D.A., Wang, J., Weaver, B.A., Weinberg, D.H., Weiner,\nB.J., West, A.A., White, M., Wilson, J.C., Wisniewski, J.P., Wood-Vasey,\nW.M., Yanny, B., Y`eche, C., York, D.G., Zamora, O., Zasowski, G., Ze-\nhavi, I., Zhao, G.B., Zheng, Z., Zhu, G., Zinn, J.C., 2012.\nThe Ninth\nData Release of the Sloan Digital Sky Survey: First Spectroscopic Data\nfrom the SDSS-III Baryon Oscillation Spectroscopic Survey. ApJS 203, 21.\ndoi:10.1088/0067-0049/203/2/21, arXiv:1207.7137.\nAlexander, D.M., Davis, T.M., Chaussidon, E., Fawcett, V.A., X. Gonzalez-\nMorales, A., Lan, T.W., Y`eche, C., Ahlen, S., Aguilar, J.N., Armengaud,\nE., Bailey, S., Brooks, D., Cai, Z., Canning, R., Carr, A., Chabanier, S.,\nCousinou, M.C., Dawson, K., de la Macorra, A., Dey, A., Dey, B., Dhun-\ngana, G., Edge, A.C., Eftekharzadeh, S., Fanning, K., Farr, J., Font-Ribera,\nA., Garcia-Bellido, J., Garrison, L., Gazta˜naga, E., A Gontcho, S.G., Gor-\ndon, C., Medellin Gonzalez, S.G., Guy, J., Herrera-Alcantar, H.K., Jiang,\nL., Juneau, S., Karac¸aylı, N.G., Kehoe, R., Kisner, T., Kov´acs, A., Lan-\ndriau, M., Levi, M.E., Magneville, C., Martini, P., Meisner, A.M., Mezcua,\nM., Miquel, R., Camacho, P.M., Moustakas, J., Mu˜noz-Guti´errez, A., My-\ners, A.D., Nadathur, S., Napolitano, L., Nie, J.D., Palanque-Delabrouille,\nN., Pan, Z., Percival, W.J., P´erez-R`afols, I., Poppett, C., Prada, F., Ram´ırez-\nP´erez, C., Ravoux, C., Rosario, D.J., Schubnell, M., Tarl´e, G., Walther, M.,\nWeiner, B., Youles, S., Zhou, Z., Zou, H., Zou, S., 2023. The DESI Survey\nValidation: Results from Visual Inspection of the Quasar Survey Spectra.\nAJ 165, 124. doi:10.3847/1538-3881/acacfc, arXiv:2208.08517.\nAlfonso, J., Garc´ıa-Varela, A., 2023.\nA Gaia astrometric view of the open\nclusters Pleiades, Praesepe, and Blanco 1. A&A 677, A163. doi:10.1051/\n0004-6361/202346569, arXiv:2304.00164.\nAllen, G.I., Gan, L., Zheng, L., 2024.\nInterpretable Machine Learn-\ning for Discovery:\nStatistical Challenges and Opportunities.\nAnnual\nReview of Statistics and Its Application 11, annurev.\ndoi:10.1146/\nannurev-statistics-040120-030919, arXiv:2308.01475.\nAllen, J.T., Hewett, P.C., Maddox, N., Richards, G.T., Belokurov, V., 2011.\nA strong redshift dependence of the broad absorption line quasar fraction.\nMNRAS 410, 860–884.\ndoi:10.1111/j.1365-2966.2010.17489.x,\narXiv:1007.3991.\nAllen, J.T., Hewett, P.C., Richardson, C.T., Ferland, G.J., Baldwin, J.A., 2013.\nClassification and analysis of emission-line galaxies using mean field in-\ndependent component analysis. MNRAS 430, 3510–3536. doi:10.1093/\nmnras/stt151, arXiv:1301.5930.\nAmara, A., Quanz, S.P., 2012. PYNPOINT: an image processing package for\nfinding exoplanets. MNRAS 427, 948–955. doi:10.1111/j.1365-2966.\n2012.21918.x, arXiv:1207.6637.\nAmaya, J., Dupuis, R., Innocenti, M.E., Lapenta, G., 2020. Visualizing and\nInterpreting Unsupervised Solar Wind Classifications.\nFrontiers in As-\ntronomy and Space Sciences 7, 66. doi:10.3389/fspas.2020.553207,\narXiv:2004.13430.\nAndrae, R., Melchior, P., Bartelmann, M., 2010. Soft clustering analysis of\ngalaxy morphologies: a worked example with SDSS.\nA&A 522, A21.\ndoi:10.1051/0004-6361/201014169, arXiv:1002.0676.\nAndreon, S., Gargiulo, G., Longo, G., Tagliaferri, R., Capuano, N., 2000.\nWide field imaging - I. Applications of neural networks to object detec-\ntion and star/galaxy classification. MNRAS 319, 700–716. doi:10.1046/\nj.1365-8711.2000.03700.x, arXiv:astro-ph/0006115.\nArmstrong, D.J., Kirk, J., Lam, K.W.F., McCormac, J., Osborn, H.P., Spake,\nJ., Walker, S., Brown, D.J.A., Kristiansen, M.H., Pollacco, D., West, R.,\nWheatley, P.J., 2016. K2 variable catalogue - II. Machine learning classifi-\ncation of variable stars and eclipsing binaries in K2 fields 0-4. MNRAS 456,\n2260–2272. doi:10.1093/mnras/stv2836, arXiv:1512.01246.\nAstropy Collaboration, Robitaille, T.P., Tollerud, E.J., Greenfield, P., Droett-\nboom, M., Bray, E., Aldcroft, T., Davis, M., Ginsburg, A., Price-Whelan,\nA.M., Kerzendorf, W.E., Conley, A., Crighton, N., Barbary, K., Muna, D.,\nFerguson, H., Grollier, F., Parikh, M.M., Nair, P.H., Unther, H.M., Deil,\nC., Woillez, J., Conseil, S., Kramer, R., Turner, J.E.H., Singer, L., Fox,\nR., Weaver, B.A., Zabalza, V., Edwards, Z.I., Azalee Bostroem, K., Burke,\nD.J., Casey, A.R., Crawford, S.M., Dencheva, N., Ely, J., Jenness, T.,\nLabrie, K., Lim, P.L., Pierfederici, F., Pontzen, A., Ptak, A., Refsdal, B.,\nServillat, M., Streicher, O., 2013. Astropy: A community Python package\nfor astronomy. A&A 558, A33. doi:10.1051/0004-6361/201322068,\narXiv:1307.6212.\nBahaadini, S., Noroozi, V., Rohani, N., Coughlin, S., Zevin, M., Smith,\n20\nJ.,\nKalogera,\nV.,\nKatsaggelos,\nA.,\n2018.\nMachine learning for\ngravity spy:\nGlitch classification and dataset.\nInformation Sciences\n444, 172–186.\nURL: https://www.sciencedirect.com/science/\narticle/pii/S0020025518301634, doi:https://doi.org/10.1016/\nj.ins.2018.02.068.\nBailer-Jones, C.A.L., Irwin, M., von Hippel, T., 1998. Automated classification\nof stellar spectra - II. Two-dimensional classification with neural networks\nand principal components analysis. MNRAS 298, 361–377. doi:10.1046/\nj.1365-8711.1998.01596.x, arXiv:astro-ph/9803050.\nBall, N.M., Brunner, R.J., 2010.\nData Mining and Machine Learning in\nAstronomy.\nInternational Journal of Modern Physics D 19, 1049–1106.\ndoi:10.1142/S0218271810017160, arXiv:0906.2173.\nBansal, N., Pahuja, S., 2022. A generic review on anomaly detection, in: Tomar,\nA., Malik, H., Kumar, P., Iqbal, A. (Eds.), Proceedings of 3rd International\nConference on Machine Learning, Advances in Computing, Renewable En-\nergy and Communication, Springer Nature Singapore, Singapore. pp. 495–\n506.\nBaron, D., 2019.\nMachine Learning in Astronomy: a practical overview.\narXiv e-prints ,\narXiv:1904.07248doi:10.48550/arXiv.1904.07248,\narXiv:1904.07248.\nBaron, D., Poznanski, D., 2017. The weirdest SDSS galaxies: results from\nan outlier detection algorithm. MNRAS 465, 4530–4555. doi:10.1093/\nmnras/stw3021, arXiv:1611.07526.\nBarra, V., Delouille, V., Hochedez, J.F., 2008. Segmentation of extreme ultra-\nviolet solar images via multichannel fuzzy clustering. Advances in Space\nResearch 42, 917–925. doi:10.1016/j.asr.2007.10.021.\nBarra, V., Delouille, V., Kretzschmar, M., Hochedez, J.F., 2009. Fast and robust\nsegmentation of solar EUV images: algorithm and results for solar cycle 23.\nA&A 505, 361–371. doi:10.1051/0004-6361/200811416.\nBenavente, P., Protopapas, P., Pichara, K., 2017. Automatic Survey-invariant\nClassification of Variable Stars. ApJ 845, 147. doi:10.3847/1538-4357/\naa7f2d, arXiv:1801.09737.\nBengio, Y., Courville, A., Vincent, P., 2013.\nRepresentation learning: A\nreview and new perspectives.\nIEEE Trans. Pattern Anal. Mach. Intell.\n35, 1798–1828.\nURL: https://doi.org/10.1109/TPAMI.2013.50,\ndoi:10.1109/TPAMI.2013.50.\nBennett, C.L., Larson, D., Weiland, J.L., Jarosik, N., Hinshaw, G., Odegard, N.,\nSmith, K.M., Hill, R.S., Gold, B., Halpern, M., Komatsu, E., Nolta, M.R.,\nPage, L., Spergel, D.N., Wollack, E., Dunkley, J., Kogut, A., Limon, M.,\nMeyer, S.S., Tucker, G.S., Wright, E.L., 2013. Nine-year Wilkinson Mi-\ncrowave Anisotropy Probe (WMAP) Observations: Final Maps and Results.\nApJS 208, 20. doi:10.1088/0067-0049/208/2/20, arXiv:1212.5225.\nBenvenuto, F., Piana, M., Campi, C., Massone, A.M., 2018. A Hybrid Super-\nvised/Unsupervised Machine Learning Approach to Solar Flare Prediction.\nApJ 853, 90. doi:10.3847/1538-4357/aaa23c, arXiv:1706.07103.\nBerthelot,\nD.,\nCarlini,\nN.,\nCubuk,\nE.D.,\nKurakin,\nA.,\nSohn,\nK.,\nZhang,\nH.,\nRaffel,\nC.,\n2019.\nReMixMatch:\nSemi-Supervised\nLearning with Distribution Alignment and Augmentation Anchoring.\narXiv e-prints ,\narXiv:1911.09785doi:10.48550/arXiv.1911.09785,\narXiv:1911.09785.\nBertin, E., Arnouts, S., 1996.\nSExtractor: Software for source extraction.\nA&AS 117, 393–404. doi:10.1051/aas:1996164.\nBessell, M.S., 2005. Standard Photometric Systems. ARA&A 43, 293–336.\ndoi:10.1146/annurev.astro.41.082801.100251.\nBezdek, J.C., Ehrlich, R., Full, W., 1984. Fcm: The fuzzy c-means clustering\nalgorithm. Computers & Geosciences 10, 191–203. doi:https://doi.\norg/10.1016/0098-3004(84)90020-7.\nBishop, C.M., 2006. Pattern Recognition and Machine Learning (Information\nScience and Statistics). Springer-Verlag, Berlin, Heidelberg.\nBlanton, M.R., Roweis, S., 2007. K-Corrections and Filter Transformations\nin the Ultraviolet, Optical, and Near-Infrared. AJ 133, 734–754. doi:10.\n1086/510127, arXiv:astro-ph/0606170.\nB´odi, A., Hajdu, T., 2021. Classification of OGLE Eclipsing Binary Stars Based\non Their Morphology Type with Locally Linear Embedding. ApJS 255, 1.\ndoi:10.3847/1538-4365/ac082c, arXiv:2106.01039.\nBoroson, T.A., Green, R.F., 1992.\nThe Emission-Line Properties of Low-\nRedshift Quasi-stellar Objects. ApJS 80, 109. doi:10.1086/191661.\nBreiman, L., 2001.\nRandom forests.\nMachine Learning 45, 5–32.\nURL: https://doi.org/10.1023/A:1010933404324, doi:10.1023/\nA:1010933404324.\nBruzual, G., Charlot, S., 2003. Stellar population synthesis at the resolution\nof 2003. MNRAS 344, 1000–1028. doi:10.1046/j.1365-8711.2003.\n06897.x, arXiv:astro-ph/0309134.\nBu, Y., Chen, F., Pan, J., 2014. Stellar spectral subclasses classification based\non Isomap and SVM. NewA 28, 35–43. doi:10.1016/j.newast.2013.\n09.007.\nBu, Y., Pan, J., Jiang, B., Wei, P., 2013. Stellar Spectral Subclass Classification\nBased on Locally Linear Embedding. PASJ 65, 81. doi:10.1093/pasj/\n65.4.81.\nCabrera-Vives, G., Miller, C.J., Schneider, J., 2018. Systematic Labeling Bias\nin Galaxy Morphologies. AJ 156, 284. doi:10.3847/1538-3881/aae9f4,\narXiv:1811.03577.\nCampello, R.J.G.B., Moulavi, D., Sander, J., 2013. Density-based clustering\nbased on hierarchical density estimates, in: Pei, J., Tseng, V.S., Cao, L.,\nMotoda, H., Xu, G. (Eds.), Advances in Knowledge Discovery and Data\nMining, Springer Berlin Heidelberg, Berlin, Heidelberg. pp. 160–172.\nCampello, R.J.G.B., Moulavi, D., Zimek, A., Sander, J., 2015. Hierarchical\ndensity estimates for data clustering, visualization, and outlier detection.\nACM Trans. Knowl. Discov. Data 10. URL: https://doi.org/10.1145/\n2733381, doi:10.1145/2733381.\nCantat-Gaudin, T., Jordi, C., Vallenari, A., Bragaglia, A., Balaguer-N´u˜nez, L.,\nSoubiran, C., Bossini, D., Moitinho, A., Castro-Ginard, A., Krone-Martins,\nA., Casamiquela, L., Sordo, R., Carrera, R., 2018. A Gaia DR2 view of the\nopen cluster population in the Milky Way. A&A 618, A93. doi:10.1051/\n0004-6361/201833476, arXiv:1805.08726.\nCantat-Gaudin, T., Jordi, C., Wright, N.J., Armstrong, J.J., Vallenari, A.,\nBalaguer-N´u˜nez, L., Ramos, P., Bossini, D., Padoan, P., Pelkonen, V.M.,\nMapelli, M., Jeffries, R.D., 2019. Expanding associations in the Vela-Puppis\nregion. 3D structure and kinematics of the young population. A&A 626,\nA17. doi:10.1051/0004-6361/201834957, arXiv:1812.08114.\nCardoso, J.F., Le Jeune, M., Delabrouille, J., Betoule, M., Patanchon, G., 2008.\nComponent separation with flexible models—application to multichannel\nastrophysical observations. IEEE Journal of Selected Topics in Signal Pro-\ncessing 2, 735–746. doi:10.1109/JSTSP.2008.2005346.\nCarrasco Kind, M., Brunner, R.J., 2014. SOMz: photometric redshift PDFs\nwith self-organizing maps and random atlas.\nMNRAS 438, 3409–3421.\ndoi:10.1093/mnras/stt2456, arXiv:1312.5753.\nCastro-Ginard, A., Jordi, C., Luri, X., Cantat-Gaudin, T., Balaguer-N´u˜nez, L.,\n2019. Hunting for open clusters in Gaia DR2: the Galactic anticentre. A&A\n627, A35. doi:10.1051/0004-6361/201935531, arXiv:1905.06161.\nCastro-Ginard, A., Jordi, C., Luri, X., Julbe, F., Morvan, M., Balaguer-N´u˜nez,\nL., Cantat-Gaudin, T., 2018.\nA new method for unveiling open clusters\nin Gaia. New nearby open clusters confirmed by DR2. A&A 618, A59.\ndoi:10.1051/0004-6361/201833390, arXiv:1805.03045.\nChang, N., Xie, F.G., Liu, X., Ho, L.C., Dong, A.J., Han, Z.H., Wang, X., 2021.\nPossible evidence of a universal radio/X-ray correlation in a near-complete\nsample of hard X-ray selected seyfert galaxies. MNRAS 503, 1987–1998.\ndoi:10.1093/mnras/stab521, arXiv:2102.10578.\nChapelle, O., Scholkopf, B., Zien, A., 2006. Semi-Supervised Learning. The\nMIT Press.\nChattopadhyay, T., Fraix-Burnet, D., Mondal, S., 2019.\nUnsupervised\nClassification of Galaxies. I. Independent Component Analysis Feature\nSelection.\nPASP 131, 108010.\ndoi:10.1088/1538-3873/aaf7c6,\narXiv:1802.02856.\nChen, B., D’Onghia, E., Alves, J., Adamo, A., 2020a.\nDiscovery of\nnew stellar groups in the Orion complex. Towards a robust unsupervised\napproach.\nA&A 643, A114.\ndoi:10.1051/0004-6361/201935955,\narXiv:1905.11429.\nChen, B.H., Hashimoto, T., Goto, T., Kim, S.J., Santos, D.J.D., On, A.Y.L.,\nLu, T.Y., Hsiao, T.Y.Y., 2022. Uncloaking hidden repeating fast radio bursts\nwith unsupervised machine learning. MNRAS 509, 1227–1236. doi:10.\n1093/mnras/stab2994, arXiv:2110.09440.\nChen, B.H., Hashimoto, T., Goto, T., Raquel, B.J.R., Uno, Y., Kim, S.J., Hsiao,\nT.Y.Y., Ho, S.C.C., 2023. Classifying a frequently repeating fast radio burst,\nFRB 20201124A, with unsupervised machine learning. MNRAS 521, 5738–\n5745. doi:10.1093/mnras/stad930, arXiv:2303.17133.\nChen, H., Speagle, J., Rogers, K.K., 2023. Learning reionization history from\nquasars with simulation-based inference. arXiv:2311.16238.\nChen, T., Kornblith, S., Norouzi, M., Hinton, G., 2020b.\nA Sim-\nple Framework for Contrastive Learning of Visual Representations.\narXiv e-prints ,\narXiv:2002.05709doi:10.48550/arXiv.2002.05709,\narXiv:2002.05709.\n21\nCheng, T.Y., Huertas-Company, M., Conselice, C.J., Arag´on-Salamanca, A.,\nRobertson, B.E., Ramachandra, N., 2021. Beyond the hubble sequence - ex-\nploring galaxy morphology with unsupervised machine learning. MNRAS\n503, 4446–4465. doi:10.1093/mnras/stab734, arXiv:2009.11932.\nCheng, T.Y., Li, N., Conselice, C.J., Arag´on-Salamanca, A., Dye, S., Metcalf,\nR.B., 2020. Identifying strong lenses with unsupervised machine learning\nusing convolutional autoencoder. MNRAS 494, 3750–3765. doi:10.1093/\nmnras/staa1015, arXiv:1911.04320.\n´Ciprijanovi´c, A., Lewis, A., Pedro, K., Madireddy, S., Nord, B., Perdue,\nG.N., Wild, S.M., 2023. DeepAstroUDA: semi-supervised universal do-\nmain adaptation for cross-survey galaxy morphology classification and\nanomaly detection. Machine Learning: Science and Technology 4, 025013.\ndoi:10.1088/2632-2153/acca5f, arXiv:2302.02005.\nClarke, A.O., Scaife, A.M.M., Greenhalgh, R., Griguta, V., 2020. Identifying\ngalaxies, quasars, and stars with machine learning: A new catalogue of clas-\nsifications for 111 million SDSS sources without spectra. A&A 639, A84.\ndoi:10.1051/0004-6361/201936770, arXiv:1909.10963.\nConnolly, A.J., Csabai, I., Szalay, A.S., Koo, D.C., Kron, R.G., Munn,\nJ.A., 1995.\nSlicing Through Multicolor Space:\nGalaxy Redshifts\nfrom Broadband Photometry.\nAJ 110, 2655.\ndoi:10.1086/117720,\narXiv:astro-ph/9508100.\nCoppa, G., Mignoli, M., Zamorani, G., Bardelli, S., Lilly, S.J., Bolzonella,\nM., Scodeggio, M., Vergani, D., Nair, P., Pozzetti, L., Cimatti, A., Zucca,\nE., Carollo, C.M., Contini, T., Le F`evre, O., Renzini, A., Mainieri, V.,\nBongiorno, A., Caputi, K.I., Cucciati, O., de la Torre, S., de Ravel, L.,\nFranzetti, P., Garilli, B., Memeo, P., Iovino, A., Kampczyk, P., Kneib,\nJ.P., Knobel, C., Koekemoer, A.M., Kovaˇc, K., Lamareille, F., Le Borgne,\nJ.F., Le Brun, V., Maier, C., Pell`o, R., Peng, Y., Perez-Montero, E., Ric-\nciardelli, E., Scarlata, C., Silverman, J.D., Tanaka, M., Tasca, L., Tresse,\nL., Abbas, U., Bottini, D., Capak, P., Cappi, A., Cassata, P., Fumana,\nM., Guzzo, L., Leauthaud, A., Maccagni, D., Marinoni, C., Meneux, B.,\nOesch, P., Porciani, C., Scaramella, R., Scoville, N., 2011. The bimodal-\nity of the 10k zCOSMOS-bright galaxies up to z ˜1: a new statistical and\nportable classification based on optical galaxy properties. A&A 535, A10.\ndoi:10.1051/0004-6361/201016130, arXiv:1009.0723.\nD’Abrusco, R., Fabbiano, G., Djorgovski, G., Donalek, C., Laurino, O., Longo,\nG., 2012. CLaSPS: A New Methodology for Knowledge Extraction from\nComplex Astronomical Data Sets. ApJ 755, 92. doi:10.1088/0004-637X/\n755/2/92, arXiv:1206.2919.\nDai, Y., Xu, J., Song, J., Fang, G., Zhou, C., Ba, S., Gu, Y., Lin, Z.,\nKong, X., 2023. The Classification of Galaxy Morphology in the H Band\nof the COSMOS-DASH Field: A Combination-based Machine-learning\nClustering Model.\nApJS 268, 34.\ndoi:10.3847/1538-4365/ace69e,\narXiv:2307.02335.\nDamodaran, B.B., Nidamanuri, R.R., 2014. Assessment of the impact of di-\nmensionality reduction methods on information classes and classifiers for\nhyperspectral image classification by multiple classifier system. Advances\nin Space Research 53, 1720–1734. doi:10.1016/j.asr.2013.11.027.\nDaniel, S.F., Connolly, A., Schneider, J., Vanderplas, J., Xiong, L., 2011. Clas-\nsification of Stellar Spectra with Local Linear Embedding. AJ 142, 203.\ndoi:10.1088/0004-6256/142/6/203.\nDavidzon, I., Jegatheesan, K., Ilbert, O., de la Torre, S., Leslie, S.K., Laigle, C.,\nHemmati, S., Masters, D.C., Blanquez-Sese, D., Kauffmann, O.B., Magdis,\nG.E., Małek, K., McCracken, H.J., Mobasher, B., Moneti, A., Sanders, D.B.,\nShuntov, M., Toft, S., Weaver, J.R., 2022. COSMOS2020: Manifold learn-\ning to estimate physical parameters in large galaxy surveys. A&A 665, A34.\ndoi:10.1051/0004-6361/202243249, arXiv:2206.06373.\nDavies, G.R., Silva Aguirre, V., Bedding, T.R., Handberg, R., Lund, M.N.,\nChaplin, W.J., Huber, D., White, T.R., Benomar, O., Hekker, S., Basu,\nS., Campante, T.L., Christensen-Dalsgaard, J., Elsworth, Y., Karoff, C.,\nKjeldsen, H., Lundkvist, M.S., Metcalfe, T.S., Stello, D., 2016.\nOs-\ncillation frequencies for 35 Kepler solar-type planet-hosting stars using\nBayesian techniques and machine learning.\nMNRAS 456, 2183–2195.\ndoi:10.1093/mnras/stv2593, arXiv:1511.02105.\nde Souza, R.S., Thorp, S., Galbany, L., Ishida, E.E.O., Gonz´alez-Gait´an,\nS., Schmitz, M.A., Krone-Martins, A., Peters, C., COIN Collaboration,\n2023. A graph-based spectral classification of Type II supernovae. Astron-\nomy and Computing 44, 100715. doi:10.1016/j.ascom.2023.100715,\narXiv:2206.14335.\nDeb, S., Singh, H.P., 2009. Light curve analysis of variable stars using Fourier\ndecomposition and principal component analysis. A&A 507, 1729–1737.\ndoi:10.1051/0004-6361/200912851, arXiv:0903.3500.\nDeeming, T.J., 1964.\nStellar spectral classification, I.\nMNRAS 127, 493.\ndoi:10.1093/mnras/127.6.493.\nDehghan Firoozabadi, A., Diaz, A., Rojo, P., Soto, I., Mahu, R., Becerra Yoma,\nN., Sedaghati, E., 2017. Unsupervised Method for Correlated Noise Re-\nmoval for Multi-wavelength Exo-planet Transit Observations. PASP 129,\n074502. doi:10.1088/1538-3873/aa70df, arXiv:1706.08556.\nDempster, A.P., Laird, N.M., Rubin, D.B., 1977. Maximum likelihood from\nincomplete data via the em algorithm. Journal of the royal statistical society:\nseries B (methodological) 39, 1–22.\nD´ıaz-Garc´ıa, S., D´ıaz-Su´arez, S., Knapen, J.H., Salo, H., 2019. Inner and outer\nrings are not strongly coupled with stellar bars. A&A 625, A146. doi:10.\n1051/0004-6361/201935455, arXiv:1904.04222.\nDoorenbos,\nL.,\nCavuoti,\nS.,\nBrescia,\nM.,\nD’Isanto,\nA.,\nLongo,\nG.,\n2021.\nIntelligent astrophysics, in: Zelinka, I., Brescia, M., Baron, D.\n(Eds.), Intelligent Astrophysics. volume 39, pp. 197–223. doi:10.1007/\n978-3-030-65867-0\\_9.\nDu, M., Ho, L.C., Debattista, V.P., Pillepich, A., Nelson, D., Zhao, D., Hern-\nquist, L., 2020. Kinematic Decomposition of IllustrisTNG Disk Galaxies:\nMorphology and Relation with Morphological Structures. ApJ 895, 139.\ndoi:10.3847/1538-4357/ab8fa8, arXiv:2002.04182.\nDu, M., Ho, L.C., Zhao, D., Shi, J., Debattista, V.P., Hernquist, L., Nelson, D.,\n2019. Identifying Kinematic Structures in Simulated Galaxies Using Un-\nsupervised Machine Learning. ApJ 884, 129. doi:10.3847/1538-4357/\nab43cc, arXiv:1909.06063.\nDuong, L., Freeman, K.C., Asplund, M., Casagrande, L., Buder, S., Lind, K.,\nNess, M., Bland-Hawthorn, J., De Silva, G.M., D’Orazi, V., Kos, J., Lewis,\nG.F., Lin, J., Martell, S.L., Schlesinger, K., Sharma, S., Simpson, J.D.,\nZucker, D.B., Zwitter, T., Anguiano, B., Da Costa, G.S., Hyde, E., Horner,\nJ., Kafle, P.R., Nataf, D.M., Reid, W., Stello, D., Ting, Y.S., Wyse, R.F.G.,\n2018. The GALAH survey: properties of the Galactic disc(s) in the solar\nneighbourhood. MNRAS 476, 5216–5232. doi:10.1093/mnras/sty525,\narXiv:1801.01514.\nEinasto, J., Klypin, A.A., Saar, E., Shandarin, S.F., 1984. Structure of su-\nperclusters and supercluster formation - III. Quantitative study of the Local\nSupercluster. MNRAS 206, 529–558. doi:10.1093/mnras/206.3.529.\nEl Bouchefry, K., de Souza, R.S., 2020. Knowledge discovery in big data from\nastronomy and earth observation, in: ˇSkoda, P., Adam, F. (Eds.), Knowledge\nDiscovery in Big Data from Astronomy and Earth Observation, pp. 225–\n249. doi:10.1016/B978-0-12-819154-5.00023-0.\nvan Engelen, J.E., Hoos, H.H., 2020. A survey on semi-supervised learning.\nMachine Learning 109, 373–440.\nURL: https://doi.org/10.1007/\ns10994-019-05855-6, doi:10.1007/s10994-019-05855-6.\nEster, M., Kriegel, H.P., Sander, J., Xu, X., et al., 1996. A density-based algo-\nrithm for discovering clusters in large spatial databases with noise., in: kdd,\npp. 226–231.\nEuclid Collaboration, Desprez, G., Paltani, S., Coupon, J., Almosallam, I.,\nAlvarez-Ayllon, A., Amaro, V., Brescia, M., Brodwin, M., Cavuoti, S., De\nVicente-Albendea, J., Fotopoulou, S., Hatfield, P.W., Hartley, W.G., Ilbert,\nO., Jarvis, M.J., Longo, G., Rau, M.M., Saha, R., Speagle, J.S., Tramacere,\nA., Castellano, M., Dubath, F., Galametz, A., Kuemmel, M., Laigle, C.,\nMerlin, E., Mohr, J.J., Pilo, S., Salvato, M., Andreon, S., Auricchio, N., Bac-\ncigalupi, C., Balaguera-Antol´ınez, A., Baldi, M., Bardelli, S., Bender, R.,\nBiviano, A., Bodendorf, C., Bonino, D., Bozzo, E., Branchini, E., Brinch-\nmann, J., Burigana, C., Cabanac, R., Camera, S., Capobianco, V., Cappi,\nA., Carbone, C., Carretero, J., Carvalho, C.S., Casas, R., Casas, S., Cas-\ntander, F.J., Castignani, G., Cimatti, A., Cledassou, R., Colodro-Conde, C.,\nCongedo, G., Conselice, C.J., Conversi, L., Copin, Y., Corcione, L., Cour-\ntois, H.M., Cuby, J.G., Da Silva, A., de la Torre, S., Degaudenzi, H., Di\nFerdinando, D., Douspis, M., Duncan, C.A.J., Dupac, X., Ealet, A., Fab-\nbian, G., Fabricius, M., Farrens, S., Ferreira, P.G., Finelli, F., Fosalba, P.,\nFourmanoit, N., Frailis, M., Franceschi, E., Fumana, M., Galeotta, S., Gar-\nilli, B., Gillard, W., Gillis, B., Giocoli, C., Gozaliasl, G., Graci´a-Carpio,\nJ., Grupp, F., Guzzo, L., Hailey, M., Haugan, S.V.H., Holmes, W., Hor-\nmuth, F., Humphrey, A., Jahnke, K., Keihanen, E., Kermiche, S., Kilbinger,\nM., Kirkpatrick, C.C., Kitching, T.D., Kohley, R., Kubik, B., Kunz, M.,\nKurki-Suonio, H., Ligori, S., Lilje, P.B., Lloro, I., Maino, D., Maiorano,\nE., Marggraf, O., Markovic, K., Martinet, N., Marulli, F., Massey, R., Ma-\nturi, M., Mauri, N., Maurogordato, S., Medinaceli, E., Mei, S., Meneghetti,\nM., Metcalf, R.B., Meylan, G., Moresco, M., Moscardini, L., Munari, E.,\nNiemi, S., Padilla, C., Pasian, F., Patrizii, L., Pettorino, V., Pires, S., Po-\n22\nlenta, G., Poncet, M., Popa, L., Potter, D., Pozzetti, L., Raison, F., Renzi,\nA., Rhodes, J., Riccio, G., Rossetti, E., Saglia, R., Sapone, D., Schneider,\nP., Scottez, V., Secroun, A., Serrano, S., Sirignano, C., Sirri, G., Stanco,\nL., Stern, D., Sureau, F., Tallada Cresp´ı, P., Tavagnacco, D., Taylor, A.N.,\nTenti, M., Tereno, I., Toledo-Moreo, R., Torradeflot, F., Valenziano, L.,\nValiviita, J., Vassallo, T., Viel, M., Wang, Y., Welikala, N., Whittaker, L.,\nZacchei, A., Zamorani, G., Zoubian, J., Zucca, E., 2020. Euclid prepa-\nration. X. The Euclid photometric-redshift challenge.\nA&A 644, A31.\ndoi:10.1051/0004-6361/202039403, arXiv:2009.12112.\nEyer, L., Blake, C., 2005. Automated classification of variable stars for All-\nSky Automated Survey 1-2 data. MNRAS 358, 30–38. doi:10.1111/j.\n1365-2966.2005.08651.x, arXiv:astro-ph/0406333.\nFarahani, A., Voghoei, S., Rasheed, K., Arabnia, H.R., 2021. A brief review of\ndomain adaptation, in: Stahlbock, R., Weiss, G.M., Abou-Nasr, M., Yang,\nC.Y., Arabnia, H.R., Deligiannidis, L. (Eds.), Advances in Data Science\nand Information Engineering, Springer International Publishing, Cham. pp.\n877–894.\nFaundez-Abans, M., Ormeno, M.I., de Oliveira-Abans, M., 1996. Classifica-\ntion of planetary nebulae by cluster analysis and artificial neural networks.\nA&AS 116, 395–402.\nForest, F., Lebbah, M., Azzag, H., Lacaille, J., 2019. Deep architectures for\njoint clustering and visualization with self-organizing maps, in: Pacific-Asia\nConference on Knowledge Discovery and Data Mining.\nFraix-Burnet, D., Thuillard, M., Chattopadhyay, A.K., 2015. Multivariate Ap-\nproaches to Classification in Extragalactic Astronomy.\nFrontiers in As-\ntronomy and Space Sciences 2, 3.\ndoi:10.3389/fspas.2015.00003,\narXiv:1508.06756.\nFraser, T.S., Tojeiro, R., Chittenden, H.G., 2023. Applying unsupervised learn-\ning to resolve evolutionary histories and explore the galaxy-halo connection\nin IllustrisTNG. MNRAS 522, 5758–5774. doi:10.1093/mnras/stad015,\narXiv:2112.12516.\nFrontera-Pons, J., Sureau, F., Bobin, J., Le Floc’h, E., 2017. Unsupervised\nfeature-learning for galaxy SEDs with denoising autoencoders. A&A 603,\nA60. doi:10.1051/0004-6361/201630240, arXiv:1705.05620.\nGagn´e, J., Moranta, L., Faherty, J.K., Kiman, R., Couture, D., Larochelle, A.R.,\nPopinchalk, M., Morrone, D., 2023. The Oceanus Moving Group: A New\n500 Myr Old Host for the Nearest Brown Dwarf. ApJ 945, 119. doi:10.\n3847/1538-4357/acb8b7, arXiv:2208.00070.\nGalaz, G., de Lapparent, V., 1998. The ESO-Sculptor Survey: spectral classifi-\ncation of galaxies with Z ¡ 0.5. A&A 332, 459–478. doi:10.48550/arXiv.\nastro-ph/9711093, arXiv:astro-ph/9711093.\nGalvin, T.J., Huynh, M.T., Norris, R.P., Wang, X.R., Hopkins, E., Polsterer,\nK., Ralph, N.O., O’Brien, A.N., Heald, G.H., 2020.\nCataloguing the\nradio-sky with unsupervised machine learning: a new approach for the\nSKA era.\nMNRAS 497, 2730–2758.\ndoi:10.1093/mnras/staa1890,\narXiv:2006.14866.\nGarcia-Dias, R., Allende Prieto, C., S´anchez Almeida, J., Alonso Palicio, P.,\n2019. Machine learning in APOGEE. Identification of stellar populations\nthrough chemical abundances. A&A 629, A34. doi:10.1051/0004-6361/\n201935223, arXiv:1907.12796.\nGarcia-Dias, R., Allende Prieto, C., S´anchez Almeida, J., Ordov´as-Pascual, I.,\n2018. Machine learning in APOGEE. Unsupervised spectral classification\nwith K-means. A&A 612, A98. doi:10.1051/0004-6361/201732134,\narXiv:1801.07912.\nGath, I., Geva, A., 1989.\nUnsupervised optimal fuzzy clustering.\nIEEE\nTransactions on Pattern Analysis and Machine Intelligence 11, 773–780.\ndoi:10.1109/34.192473.\nGeach, J.E., 2012. Unsupervised self-organized mapping: a versatile empir-\nical tool for object selection, classification and redshift estimation in large\nsurveys. MNRAS 419, 2633–2645. doi:10.1111/j.1365-2966.2011.\n19913.x, arXiv:1110.0005.\nGeorge,\nD.,\nShen,\nH.,\nHuerta,\nE.A.,\n2017.\nGlitch Classification\nand\nClustering\nfor\nLIGO\nwith\nDeep\nTransfer\nLearning.\narXiv\ne-prints\n,\narXiv:1711.07468doi:10.48550/arXiv.1711.07468,\narXiv:1711.07468.\nGhojogh,\nB.,\nSharma,\nS.,\n2022.\nGravitational Dimensionality Re-\nduction Using Newtonian Gravity and Einstein’s General Relativity.\narXiv e-prints ,\narXiv:2211.01369doi:10.48550/arXiv.2211.01369,\narXiv:2211.01369.\nGilda, S., Lower, S., Narayanan, D., 2021.\nMIRKWOOD: Fast and Accu-\nrate SED Modeling Using Machine Learning. ApJ 916, 43. doi:10.3847/\n1538-4357/ac0058, arXiv:2101.04687.\nGiles, D., Walkowicz, L., 2019. Systematic serendipity: a test of unsupervised\nmachine learning as a method for anomaly detection. MNRAS 484, 834–\n849. doi:10.1093/mnras/sty3461, arXiv:1812.07156.\nGraff, P., Feroz, F., Hobson, M.P., Lasenby, A., 2014.\nSKYNET: an ef-\nficient and robust neural network training tool for machine learning in\nastronomy.\nMNRAS 441, 1741–1759.\ndoi:10.1093/mnras/stu642,\narXiv:1309.0790.\nGraham, R., Hell, P., 1985. On the history of the minimum spanning tree prob-\nlem. Annals of the History of Computing 7, 43–57. doi:10.1109/MAHC.\n1985.10011.\nGrill, J.B., Strub, F., Altch´e, F., Tallec, C., Richemond, P.H., Buchatskaya, E.,\nDoersch, C., Pires, B.A., Guo, Z.D., Azar, M.G., Piot, B., Kavukcuoglu,\nK., Munos, R., Valko, M., 2020. Bootstrap your own latent a new approach\nto self-supervised learning, in: Proceedings of the 34th International Con-\nference on Neural Information Processing Systems, Curran Associates Inc.,\nRed Hook, NY, USA.\nGrindlay, J., Tang, S., Los, E., Servillat, M., 2012.\nOpening the 100-Year\nWindow for Time-Domain Astronomy, in: Griffin, E., Hanisch, R., Seaman,\nR. (Eds.), New Horizons in Time Domain Astronomy, pp. 29–34. doi:10.\n1017/S1743921312000166, arXiv:1211.1051.\nGunn, J.E., Carr, M., Rockosi, C., Sekiguchi, M., Berry, K., Elms, B., de Haas,\nE., Ivezi´c, ˇZ.., Knapp, G., Lupton, R., Pauls, G., Simcoe, R., Hirsch, R.,\nSanford, D., Wang, S., York, D., Harris, F., Annis, J., Bartozek, L., Boroski,\nW., Bakken, J., Haldeman, M., Kent, S., Holm, S., Holmgren, D., Petravick,\nD., Prosapio, A., Rechenmacher, R., Doi, M., Fukugita, M., Shimasaku, K.,\nOkada, N., Hull, C., Siegmund, W., Mannery, E., Blouke, M., Heidtman,\nD., Schneider, D., Lucinio, R., Brinkman, J., 1998. The Sloan Digital Sky\nSurvey Photometric Camera. AJ 116, 3040–3081. doi:10.1086/300645,\narXiv:astro-ph/9809085.\nGuo, X., Liu, C., Qiu, B., Luo, A.l., Jiang, X., Shi, J., Li, X., Wang, L., 2022.\nUnsupervised clustering and analysis of WISE spiral galaxies. MNRAS 517,\n1837–1848. doi:10.1093/mnras/stac2620.\nGupta, N., Huynh, M., Norris, R.P., Wang, X.R., Hopkins, A.M., Andernach,\nH., Koribalski, B.S., Galvin, T.J., 2022. Discovery of peculiar radio mor-\nphologies with ASKAP using unsupervised machine learning. PASA 39,\ne051. doi:10.1017/pasa.2022.44, arXiv:2208.13997.\nHakkila, J., Giblin, T.W., Roiger, R.J., Haglin, D.J., Paciesas, W.S.,\nMeegan, C.A., 2003.\nHow Sample Completeness Affects Gamma-\nRay Burst Classification.\nApJ 582, 320–329.\ndoi:10.1086/344568,\narXiv:astro-ph/0209073.\nHambly, N., Read, M., Mann, R., Sutorius, E., Bond, I., MacGillivray, H.,\nWilliams, P., Lawrence, A., 2004. The SuperCOSMOS Science Archive, in:\nOchsenbein, F., Allen, M.G., Egret, D. (Eds.), Astronomical Data Analysis\nSoftware and Systems (ADASS) XIII, p. 137.\nHambly, N.C., MacGillivray, H.T., Read, M.A., Tritton, S.B., Thomson,\nE.B., Kelly, B.D., Morgan, D.H., Smith, R.E., Driver, S.P., Williamson,\nJ., Parker, Q.A., Hawkins, M.R.S., Williams, P.M., Lawrence, A., 2001.\nThe SuperCOSMOS Sky Survey - I. Introduction and description.\nMN-\nRAS 326, 1279–1294.\ndoi:10.1111/j.1365-2966.2001.04660.x,\narXiv:astro-ph/0108286.\nHan, Y., Zou, Z., Li, N., Chen, Y., 2022. Identifying Outliers in Astronom-\nical Images with Unsupervised Machine Learning.\nResearch in Astron-\nomy and Astrophysics 22, 085006.\ndoi:10.1088/1674-4527/ac7386,\narXiv:2205.09760.\nHartley, P., Bonaldi, A., Braun, R., Aditya, J.N.H.S., Aicardi, S., Alegre,\nL., Chakraborty, A., Chen, X., Choudhuri, S., Clarke, A.O., Coles, J.,\nCollinson, J.S., Cornu, D., Darriba, L., Veneri, M.D., Forbrich, J., Fraga,\nB., Galan, A., Garrido, J., Gubanov, F., Håkansson, H., Hardcastle, M.J.,\nHeneka, C., Herranz, D., Hess, K.M., Jagannath, M., Jaiswal, S., Jurek,\nR.J., Korber, D., Kitaeff, S., Kleiner, D., Lao, B., Lu, X., Mazumder, A.,\nMold´on, J., Mondal, R., Ni, S., ¨Onnheim, M., Parra, M., Patra, N., Peel, A.,\nSalom´e, P., S´anchez-Exp´osito, S., Sargent, M., Semelin, B., Serra, P., Shaw,\nA.K., Shen, A.X., Sj¨oberg, A., Smith, L., Soroka, A., Stolyarov, V., Tol-\nley, E., Toribio, M.C., van der Hulst, J.M., Sadr, A.V., Verdes-Montenegro,\nL., Westmeier, T., Yu, K., Yu, L., Zhang, L., Zhang, X., Zhang, Y., Al-\nberdi, A., Ashdown, M., Bom, C.R., Br¨uggen, M., Cannon, J., Chen, R.,\nCombes, F., Conway, J., Courbin, F., Ding, J., Fourestey, G., Freundlich, J.,\nGao, L., Gheller, C., Guo, Q., Gustavsson, E., Jirstrand, M., Jones, M.G.,\nJ´ozsa, G., Kamphuis, P., Kneib, J.P., Lindqvist, M., Liu, B., Liu, Y., Mao,\nY., Marchal, A., M´arquez, I., Meshcheryakov, A., Olberg, M., Oozeer, N.,\n23\nPandey-Pommier, M., Pei, W., Peng, B., Sabater, J., Sorgho, A., Starck,\nJ.L., Tasse, C., Wang, A., Wang, Y., Xi, H., Yang, X., Zhang, H., Zhang,\nJ., Zhao, M., Zuo, S., 2023.\nSKA Science Data Challenge 2: analysis\nand results. MNRAS 523, 1967–1993. doi:10.1093/mnras/stad1375,\narXiv:2303.07943.\nHastie, T., Tibshirani, R., Friedman, J., 2009. The elements of statistical learn-\ning: data mining, inference and prediction. 2 ed., Springer.\nHayat, M.A., Stein, G., Harrington, P., Luki´c, Z., Mustafa, M., 2021. Self-\nsupervised Representation Learning for Astronomical Images.\nApJ 911,\nL33. doi:10.3847/2041-8213/abf2c7, arXiv:2012.13083.\nHayes, J.J.C., Kerins, E., Awiphan, S., McDonald, I., Morgan, J.S., Chuanrak-\nsasat, P., Komonjinda, S., Sanguansak, N., Kittara, P., SPEARNet Collabo-\nration, 2020. Optimizing exoplanet atmosphere retrieval using unsupervised\nmachine-learning classification. MNRAS 494, 4492–4508. doi:10.1093/\nmnras/staa978, arXiv:1909.00718.\nHe, K., Fan, H., Wu, Y., Xie, S., Girshick, R., 2020. Momentum contrast for un-\nsupervised visual representation learning, in: Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR).\nHe, Z., Liu, X., Luo, Y., Wang, K., Jiang, Q., 2023. Unveiling Hidden Stellar\nAggregates in the Milky Way: 1656 New Star Clusters Found in Gaia EDR3.\nApJS 264, 8. doi:10.3847/1538-4365/ac9af8, arXiv:2209.08504.\nHelmi,\nA.,\n2020.\nStreams,\nSubstructures,\nand\nthe\nEarly\nHistory\nof\nthe\nMilky\nWay.\nARA&A\n58,\n205–256.\ndoi:10.1146/\nannurev-astro-032620-021917, arXiv:2002.04340.\nHemmati, S., Capak, P., Pourrahmani, M., Nayyeri, H., Stern, D., Mobasher,\nB., Darvish, B., Davidzon, I., Ilbert, O., Masters, D., Shahidi, A., 2019.\nBringing Manifold Learning and Dimensionality Reduction to SED Fitters.\nApJ 881, L14. doi:10.3847/2041-8213/ab3418, arXiv:1905.10379.\nHernandez-Pajares, M., Floris, J., 1994.\nClassification of the HIPPARCOS\nInput Catalogue Using the Kohonen Network. MNRAS 268, 444. doi:10.\n1093/mnras/268.2.444.\nHildebrandt, H., Arnouts, S., Capak, P., Moustakas, L.A., Wolf, C., Abdalla,\nF.B., Assef, R.J., Banerji, M., Ben´ıtez, N., Brammer, G.B., Budav´ari, T.,\nCarliles, S., Coe, D., Dahlen, T., Feldmann, R., Gerdes, D., Gillis, B., Ilbert,\nO., Kotulla, R., Lahav, O., Li, I.H., Miralles, J.M., Purger, N., Schmidt,\nS., Singal, J., 2010. PHAT: PHoto-z Accuracy Testing. A&A 523, A31.\ndoi:10.1051/0004-6361/201014885, arXiv:1008.0658.\nHobbs, G.B., Edwards, R.T., Manchester, R.N., 2006. TEMPO2, a new pulsar-\ntiming package - I. An overview. MNRAS 369, 655–672. doi:10.1111/j.\n1365-2966.2006.10302.x, arXiv:astro-ph/0603381.\nHocking, A., Geach, J.E., Sun, Y., Davey, N., 2018. An automatic taxonomy\nof galaxy morphology using unsupervised machine learning. MNRAS 473,\n1108–1129. doi:10.1093/mnras/stx2351, arXiv:1709.05834.\nHojnacki, S.M., Micela, G., Lalonde, S.M., Feigelson, E.D., Kastner, J.H.,\n2008. An unsupervised, ensemble clustering algorithm: A new approach\nfor classification of X-ray sources.\nStatistical Methodology 5, 350–360.\ndoi:10.1016/j.stamet.2008.02.008.\nHolwerda, B.W., Smith, D., Porter, L., Henry, C., Porter-Temple, R., Cook,\nK., Pimbblet, K.A., Hopkins, A.M., Bilicki, M., Turner, S., Acquaviva,\nV., Wang, L., Wright, A.H., Kelvin, L.S., Grootes, M.W., 2022. Galaxy\nand mass assembly (GAMA): Self-Organizing Map application on nearby\ngalaxies.\nMNRAS 513, 1972–1984.\ndoi:10.1093/mnras/stac889,\narXiv:2203.15611.\nHotelling, H., 1936. Relations between two sets of variates. Biometrika 28,\n321–377.\nHuertas-Company, M., Gravet, R., Cabrera-Vives, G., P´erez-Gonz´alez, P.G.,\nKartaltepe, J.S., Barro, G., Bernardi, M., Mei, S., Shankar, F., Dimauro, P.,\nBell, E.F., Kocevski, D., Koo, D.C., Faber, S.M., Mcintosh, D.H., 2015.\nA Catalog of Visual-like Morphologies in the 5 CANDELS Fields Us-\ning Deep Learning.\nApJS 221, 8.\ndoi:10.1088/0067-0049/221/1/8,\narXiv:1509.05429.\nHuertas-Company, M., Sarmiento, R., Knapen, J.H., 2023. A brief review of\ncontrastive learning applied to astrophysics. RAS Techniques and Instru-\nments 2, 441–452. doi:10.1093/rasti/rzad028, arXiv:2306.05528.\nHuijse, P., Estevez, P.A., Protopapas, P., Principe, J.C., Zegers, P., 2015. Com-\nputational Intelligence Challenges and Applications on Large-Scale Astro-\nnomical Time Series Databases. arXiv e-prints , arXiv:1509.07823doi:10.\n48550/arXiv.1509.07823, arXiv:1509.07823.\nHyv¨arinen, A., Oja, E., 2000. Independent component analysis: algorithms and\napplications. Neural Networks 13, 411–430. doi:https://doi.org/10.\n1016/S0893-6080(00)00026-5.\nIrfan, M.O., Bull, P., 2021. Cleaning foregrounds from single-dish 21 cm inten-\nsity maps with Kernel principal component analysis. MNRAS 508, 3551–\n3568. doi:10.1093/mnras/stab2855, arXiv:2107.02267.\nIshida, E.E.O., de Souza, R.S., 2013.\nKernel PCA for Type Ia supernovae\nphotometric classification. MNRAS 430, 509–532. doi:10.1093/mnras/\nsts650, arXiv:1201.6676.\nIvezi´c, ˇZ., Connolly, A.J., VanderPlas, J.T., Gray, A., 2014. Statistics, Data\nMining, and Machine Learning in Astronomy: A Practical Python Guide for\nthe Analysis of Survey Data. doi:10.1515/9781400848911.\nIwasaki, H., Ichinohe, Y., Uchiyama, Y., 2019. X-ray study of spatial structures\nin Tycho’s supernova remnant using unsupervised deep learning. MNRAS\n488, 4106–4116. doi:10.1093/mnras/stz1990, arXiv:1907.09210.\nJain, A.K., Murty, M.N., Flynn, P.J., 1999.\nData clustering:\nA review.\nACM Comput. Surv. 31, 264–323. URL: https://doi.org/10.1145/\n331499.331504, doi:10.1145/331499.331504.\nJamal, S., Le Brun, V., Le F`evre, O., Vibert, D., Schmitt, A., Surace, C.,\nCopin, Y., Garilli, B., Moresco, M., Pozzetti, L., 2018. Automated relia-\nbility assessment for spectroscopic redshift measurements. A&A 611, A53.\ndoi:10.1051/0004-6361/201731305, arXiv:1706.01103.\nJames, G., Witten, D., Hastie, T., Tibshirani, R., 2014. An Introduction to\nStatistical Learning: With Applications in R. Springer Publishing Company,\nIncorporated.\nJia, W., Sun, M., Lian, J., Hou, S., 2022. Feature dimensionality reduction:\na review. Complex & Intelligent Systems 8, 2663–2693. doi:10.1007/\ns40747-021-00637-x.\nJohnson, S.C., 1967. Hierarchical clustering schemes. Psychometrika 32, 241–\n254.\nKarmakar,\nA.,\nMishra,\nD.,\nTej,\nA.,\n2018.\nStellar Cluster Detec-\ntion\nusing\nGMM\nwith\nDeep\nVariational\nAutoencoder.\narXiv\ne-prints\n,\narXiv:1809.01434doi:10.48550/arXiv.1809.01434,\narXiv:1809.01434.\nKaufman, L., Rousseeuw, P., 1990. Partitioning Around Medoids (Program\nPAM). John Wiley and Sons, Ltd. chapter 2. pp. 68–125. doi:https://\ndoi.org/10.1002/9780470316801.ch2.\nKhan, A., Huerta, E.A., Wang, S., Gruendl, R., Jennings, E., Zheng, H., 2019.\nDeep learning at scale for the construction of galaxy catalogs in the Dark En-\nergy Survey. Physics Letters B 795, 248–258. doi:10.1016/j.physletb.\n2019.06.009, arXiv:1812.02183.\nKim, E.J., Brunner, R.J., Carrasco Kind, M., 2015. A hybrid ensemble learning\napproach to star-galaxy classification.\nMNRAS 453, 507–521.\ndoi:10.\n1093/mnras/stv1608, arXiv:1505.02200.\nKingma, D.P., Welling, M., 2013.\nAuto-Encoding Variational Bayes.\narXiv\ne-prints\n,\narXiv:1312.6114doi:10.48550/arXiv.1312.6114,\narXiv:1312.6114.\nKinson, D.A., Oliveira, J.M., van Loon, J.T., 2021.\nMassive young stel-\nlar objects in the Local Group irregular galaxy NGC 6822 identified us-\ning machine learning. MNRAS 507, 5106–5131. doi:10.1093/mnras/\nstab2386, arXiv:2108.07105.\nKirk, B., Conroy, K., Prˇsa, A., Abdul-Masih, M., Kochoska, A., Matijeviˇc,\nG., Hambleton, K., Barclay, T., Bloemen, S., Boyajian, T., Doyle, L.R.,\nFulton, B.J., Hoekstra, A.J., Jek, K., Kane, S.R., Kostov, V., Latham, D.,\nMazeh, T., Orosz, J.A., Pepper, J., Quarles, B., Ragozzine, D., Shporer,\nA., Southworth, J., Stassun, K., Thompson, S.E., Welsh, W.F., Agol, E.,\nDerekas, A., Devor, J., Fischer, D., Green, G., Gropp, J., Jacobs, T., John-\nston, C., LaCourse, D.M., Saetre, K., Schwengeler, H., Toczyski, J., Werner,\nG., Garrett, M., Gore, J., Martinez, A.O., Spitzer, I., Stevick, J., Thomadis,\nP.C., Vrijmoet, E.H., Yenawine, M., Batalha, N., Borucki, W., 2016. Kepler\nEclipsing Binary Stars. VII. The Catalog of Eclipsing Binaries Found in the\nEntire Kepler Data Set. AJ 151, 68. doi:10.3847/0004-6256/151/3/68,\narXiv:1512.08830.\nKitchin, R., McArdle, G., 2016.\nWhat makes big data, big data?\nexploring the ontological characteristics of 26 datasets.\nBig Data\n& Society 3, 2053951716631130.\ndoi:10.1177/2053951716631130,\narXiv:https://doi.org/10.1177/2053951716631130.\nKohonen, T., 1982. Self-organized formation of topologically correct feature\nmaps. Biological Cybernetics 43, 59–69.\nKoljonen, K.I.I., 2015. Unsupervised spectral decomposition of X-ray binaries\nwith application to GX 339-4. MNRAS 447, 2981–2991. doi:10.1093/\nmnras/stu2663, arXiv:1412.4966.\nKounkel, M., Covey, K., 2019. Untangling the Galaxy. I. Local Structure and\nStar Formation History of the Milky Way. AJ 158, 122. doi:10.3847/\n24\n1538-3881/ab339a, arXiv:1907.07709.\nKramer, M., 1992.\nAutoassociative neural networks.\nComputers &\nChemical Engineering 16, 313–328.\ndoi:https://doi.org/10.1016/\n0098-1354(92)80051-A. neutral network applications in chemical engi-\nneering.\nKramer, M.A., 1991. Nonlinear principal component analysis using autoas-\nsociative neural networks.\nAIChE Journal 37, 233–243.\ndoi:https:\n//doi.org/10.1002/aic.690370209.\nKrone-Martins, A., Moitinho, A., 2014a. UPMASK: unsupervised photometric\nmembership assignment in stellar clusters. A&A 561, A57. doi:10.1051/\n0004-6361/201321143, arXiv:1309.4471.\nKrone-Martins, A., Moitinho, A., 2014b. UPMASK: unsupervised photometric\nmembership assignment in stellar clusters. A&A 561, A57. doi:10.1051/\n0004-6361/201321143, arXiv:1309.4471.\nK¨ugler, S.D., Gianniotis, N., Polsterer, K.L., 2015. Featureless classification\nof light curves. MNRAS 451, 3385–3392. doi:10.1093/mnras/stv1181,\narXiv:1504.04455.\nKuntzer, T., Tewes, M., Courbin, F., 2016. Stellar classification from single-\nband imaging using machine learning.\nA&A 591, A54.\ndoi:10.1051/\n0004-6361/201628660, arXiv:1605.03201.\nLahav, O., Naim, A., Buta, R.J., Corwin, H.G., de Vaucouleurs, G., Dressler,\nA., Huchra, J.P., van den Bergh, S., Raychaudhury, S., Sodre, L., J., Storrie-\nLombardi, M.C., 1995. Galaxies, Human Eyes, and Artificial Neural Net-\nworks. Science 267, 859–862. doi:10.1126/science.267.5199.859,\narXiv:astro-ph/9412027.\nLahav, O., Naim, A., Sodr´e, L., J., Storrie-Lombardi, M.C., 1996. Neural com-\nputation as a tool for galaxy classification: methods and examples. MNRAS\n283, 207. doi:10.1093/mnras/283.1.207, arXiv:astro-ph/9508012.\nLawlor, D., Budav´ari, T., Mahoney, M.W., 2016. Mapping the Similarities of\nSpectra: Global and Locally-biased Approaches to SDSS Galaxies. ApJ\n833, 26. doi:10.3847/0004-637X/833/1/26, arXiv:1609.03932.\nLee, D.D., Seung, H.S., 1999. Learning the parts of objects by non-negative\nmatrix factorization. Nature 401, 788–791. doi:10.1038/44565.\nLee, J.S., Grunes, M.R., Ainsworth, T.L., Du, L.J., Schuler, D.L., Cloude, S.R.,\n1999. Unsupervised classification using polarimetric decomposition and the\ncomplex Wishart classifier. IEEE Transactions on Geoscience and Remote\nSensing 37, 2249–2258. doi:10.1109/36.789621.\nLee, K.L.K., Patterson, J., Burkhardt, A.M., Vankayalapati, V., McCarthy,\nM.C., McGuire, B.A., 2021.\nMachine Learning of Interstellar Chem-\nical Inventories.\nApJ 917, L6.\ndoi:10.3847/2041-8213/ac194b,\narXiv:2107.14610.\nLehtinen, J., Munkberg, J., Hasselgren, J., Laine, S., Karras, T., Aittala, M.,\nAila, T., 2018. Noise2Noise: Learning image restoration without clean data,\nin: Dy, J., Krause, A. (Eds.), Proceedings of the 35th International Confer-\nence on Machine Learning, PMLR. pp. 2965–2974.\nLiang, Y., Melchior, P., Lu, S., Goulding, A., Ward, C., 2023. Autoencoding\nGalaxy Spectra. II. Redshift Invariance and Outlier Detection. AJ 166, 75.\ndoi:10.3847/1538-3881/ace100, arXiv:2302.02496.\nLiu, F.T., Ting, K.M., Zhou, Z.H., 2008. Isolation forest, in: 2008 Eighth IEEE\nInternational Conference on Data Mining, pp. 413–422.\ndoi:10.1109/\nICDM.2008.17.\nLiu, M.Y., Breuel, T.M., Kautz, J., 2017.\nUnsupervised image-to-image\ntranslation networks, in: Neural Information Processing Systems.\nURL:\nhttps://api.semanticscholar.org/CorpusID:3783306.\nLloyd, S., 1982. Least squares quantization in pcm. IEEE transactions on\ninformation theory 28, 129–137.\nLochner, M., Bassett, B.A., 2021.\nASTRONOMALY: Personalised active\nanomaly detection in astronomical data.\nAstronomy and Computing 36,\n100481. doi:10.1016/j.ascom.2021.100481, arXiv:2010.11202.\nLogan, C.H.A., Fotopoulou, S., 2020. Unsupervised star, galaxy, QSO clas-\nsification. Application of HDBSCAN. A&A 633, A154. doi:10.1051/\n0004-6361/201936648, arXiv:1911.05107.\nLu, H., Zhou, H., Wang, J., Wang, T., Dong, X., Zhuang, Z., Li, C.,\n2006.\nEnsemble Learning for Independent Component Analysis of\nNormal Galaxy Spectra.\nAJ 131, 790–805.\ndoi:10.1086/498711,\narXiv:astro-ph/0510246.\nMa, Z., Xu, H., Zhu, J., Hu, D., Li, W., Shan, C., Zhu, Z., Gu, L., Li, J., Liu, C.,\nWu, X., 2019. A Machine Learning Based Morphological Classification of\n14,245 Radio AGNs Selected from the Best-Heckman Sample. ApJS 240,\n34. doi:10.3847/1538-4365/aaf9a2, arXiv:1812.07190.\nvan der Maaten, L., 2009. Learning a parametric embedding by preserving lo-\ncal structure, in: van Dyk, D., Welling, M. (Eds.), Proceedings of the Twelth\nInternational Conference on Artificial Intelligence and Statistics, PMLR,\nHilton Clearwater Beach Resort, Clearwater Beach, Florida USA. pp. 384–\n391. URL: https://proceedings.mlr.press/v5/maaten09a.html.\nvan der Maaten, L., Hinton, G.E., 2008. Visualizing data using t-sne. Journal\nof Machine Learning Research 9, 2579–2605.\nMacQueen, J., et al., 1967. Some methods for classification and analysis of\nmultivariate observations, in: Proceedings of the fifth Berkeley symposium\non mathematical statistics and probability, Oakland, CA, USA. pp. 281–297.\nMaehoenen, P.H., Hakala, P.J., 1995.\nAutomated Source Classification\nUsing a Kohonen Network.\nApJL 452, L77.\ndoi:10.1086/309697,\narXiv:astro-ph/9508019.\nMaharana, K., Mondal, S., Nemade, B., 2022. A review: Data pre-processing\nand data augmentation techniques.\nGlobal Transitions Proceedings 3,\n91–99.\ndoi:https://doi.org/10.1016/j.gltp.2022.04.020. inter-\nnational Conference on Intelligent Engineering Approach(ICIEA-2022).\nM´arquez-Neila, P., Fisher, C., Sznitman, R., Heng, K., 2018.\nSuper-\nvised machine learning for analysing spectra of exoplanetary atmospheres.\nNature Astronomy 2, 719–724.\ndoi:10.1038/s41550-018-0504-2,\narXiv:1806.03944.\nMartin, G., Kaviraj, S., Hocking, A., Read, S.C., Geach, J.E., 2020. Galaxy\nmorphological classification in deep-wide surveys via unsupervised ma-\nchine learning. MNRAS 491, 1408–1426. doi:10.1093/mnras/stz3006,\narXiv:1909.10537.\nMarziani, P., Zamanov, R.K., Sulentic, J.W., Calvani, M., 2003. Searching\nfor the physical drivers of eigenvector 1: influence of black hole mass and\nEddington ratio. MNRAS 345, 1133–1144. doi:10.1046/j.1365-2966.\n2003.07033.x, arXiv:astro-ph/0307367.\nMarzo, G.A., Roush, T.L., Hogan, R.C., 2009. Automated classification of\nvisible and infrared spectra using cluster analysis. Journal of Geophysical\nResearch (Planets) 114, E08001. doi:10.1029/2008JE003250.\nMasci, J., Meier, U., Cires¸an, D., Schmidhuber, J., 2011. Stacked convolu-\ntional auto-encoders for hierarchical feature extraction, in: Proceedings of\nthe 21th International Conference on Artificial Neural Networks - Volume\nPart I, Springer-Verlag, Berlin, Heidelberg. p. 52–59.\nMasters, D., Capak, P., Stern, D., Ilbert, O., Salvato, M., Schmidt, S., Longo,\nG., Rhodes, J., Paltani, S., Mobasher, B., Hoekstra, H., Hildebrandt, H.,\nCoupon, J., Steinhardt, C., Speagle, J., Faisst, A., Kalinich, A., Brod-\nwin, M., Brescia, M., Cavuoti, S., 2015.\nMapping the Galaxy Color-\nRedshift Relation: Optimal Photometric Redshift Calibration Strategies for\nCosmology Surveys. ApJ 813, 53. doi:10.1088/0004-637X/813/1/53,\narXiv:1509.03318.\nMatchev, K.T., Matcheva, K., Roman, A., 2022. Unsupervised Machine Learn-\ning for Exploratory Data Analysis of Exoplanet Transmission Spectra. PSJ\n3, 205. doi:10.3847/PSJ/ac880b, arXiv:2201.02696.\nMatijeviˇc, G., Prˇsa, A., Orosz, J.A., Welsh, W.F., Bloemen, S., Barclay, T.,\n2012. Kepler Eclipsing Binary Stars. III. Classification of Kepler Eclipsing\nBinary Light Curves with Locally Linear Embedding. AJ 143, 123. doi:10.\n1088/0004-6256/143/5/123, arXiv:1204.2113.\nMcInnes, L., Healy, J., Astels, S., 2017. hdbscan: Hierarchical density based\nclustering. Journal of Open Source Software 2, 205. doi:10.21105/joss.\n00205.\nMcInnes,\nL.,\nHealy,\nJ.,\nMelville,\nJ.,\n2018.\nUMAP:\nUniform\nManifold\nApproximation\nand\nProjection\nfor\nDimension\nReduction.\narXiv e-prints ,\narXiv:1802.03426doi:10.48550/arXiv.1802.03426,\narXiv:1802.03426.\nMcInnes, L., Healy, J., Saul, N., Großberger, L., 2018. Umap: Uniform mani-\nfold approximation and projection. Journal of Open Source Software 3, 861.\ndoi:10.21105/joss.00861.\nMcLachlan, G.J., 2015. Computation: Expectation-maximization algorithm,\nin: Wright, J.D. (Ed.), International Encyclopedia of the Social & Be-\nhavioral Sciences (Second Edition). second edition ed.. Elsevier, Oxford,\npp. 469–474. doi:https://doi.org/10.1016/B978-0-08-097086-8.\n42007-6.\nMeidt, S.E., Schinnerer, E., van de Ven, G., Zaritsky, D., Peletier, R., Knapen,\nJ.H., Sheth, K., Regan, M., Querejeta, M., Mu˜noz-Mateos, J.C., Kim, T.,\nHinz, J.L., Gil de Paz, A., Athanassoula, E., Bosma, A., Buta, R.J., Cis-\nternas, M., Ho, L.C., Holwerda, B., Skibba, R., Laurikainen, E., Salo,\nH., Gadotti, D.A., Laine, J., Erroz-Ferrer, S., Comer´on, S., Men´endez-\nDelmestre, K., Seibert, M., Mizusawa, T., 2014. Reconstructing the Stel-\nlar Mass Distributions of Galaxies Using S4G IRAC 3.6 and 4.5 µm Im-\n25\nages. II. The Conversion from Light to Mass. ApJ 788, 144. doi:10.1088/\n0004-637X/788/2/144, arXiv:1402.5210.\nMeil˘a, M., Zhang, H., 2024.\nManifold learning:\nWhat, how, and why.\nAnnual Review of Statistics and Its Application 11, null.\nURL:\nhttps://doi.org/10.1146/annurev-statistics-040522-115238,\ndoi:10.1146/annurev-statistics-040522-115238,\narXiv:https://doi.org/10.1146/annurev-statistics-040522-115238.\nMeingast, S., Lombardi, M., Alves, J., 2017. Estimating extinction using unsu-\npervised machine learning. A&A 601, A137. doi:10.1051/0004-6361/\n201630032, arXiv:1702.08456.\nMesarcik, M., Boonstra, A.J., Meijer, C., Jansen, W., Ranguelova, E., van\nNieuwpoort, R.V., 2020. Deep learning assisted data inspection for radio\nastronomy. MNRAS 496, 1517–1529. doi:10.1093/mnras/staa1412,\narXiv:2005.13373.\nMislis, D., Pyrzas, S., Alsubai, K.A., 2018. TSARDI: a Machine Learning\ndata rejection algorithm for transiting exoplanet light curves. MNRAS 481,\n1624–1630. doi:10.1093/mnras/sty2361, arXiv:1809.09722.\nModak, S., Chattopadhyay, T., Chattopadhyay, A.K., 2018.\nUnsupervised\nclassification of eclipsing binary light curves through k-medoids clus-\ntering.\narXiv e-prints , arXiv:1801.09406doi:10.48550/arXiv.1801.\n09406, arXiv:1801.09406.\nMong, Y.L., Ackley, K., Killestein, T.L., Galloway, D.K., Vassallo, C., Dyer,\nM., Cutter, R., Brown, M.J.I., Lyman, J., Ulaczyk, K., Steeghs, D., Dhillon,\nV., O’Brien, P., Ramsay, G., Noysena, K., Kotak, R., Breton, R., Nuttall, L.,\nPall´e, E., Pollacco, D., Thrane, E., Awiphan, S., Burhanudin, U., Chote, P.,\nChrimes, A., Daw, E., Duffy, C., Eyles-Ferris, R., Gompertz, B.P., Heikkil¨a,\nT., Irawati, P., Kennedy, M., Levan, A., Littlefair, S., Makrygianni, L.,\nMarsh, T., Mata S´anchez, D., Mattila, S., Maund, J.R., McCormac, J., Mkr-\ntichian, D., Mullaney, J., Rol, E., Sawangwit, U., Stanway, E., Starling, R.,\nStrøm, P., Tooke, S., Wiersema, K., 2023. Self-supervised clustering on\nimage-subtracted data with deep-embedded self-organizing map. MNRAS\n518, 752–762. doi:10.1093/mnras/stac3103, arXiv:2209.06375.\nMoranta, L., Gagn´e, J., Couture, D., Faherty, J.K., 2022.\nNew Coronae\nand Stellar Associations Revealed by a Clustering Analysis of the So-\nlar Neighborhood.\nApJ 939, 94.\ndoi:10.3847/1538-4357/ac8c25,\narXiv:2206.04567.\nMorello, G., Danielski, C., Dickens, D., Tremblin, P., Lagage, P.O., 2019. An\nIndependent Analysis of the Spitzer/IRAC Phase Curves of WASP43 b. AJ\n157, 205. doi:10.3847/1538-3881/ab14e2, arXiv:1908.06741.\nMoreno, E.A., Borzyszkowski, B., Pierini, M., Vlimant, J.R., Spiropulu, M.,\n2022. Source-agnostic gravitational-wave detection with recurrent autoen-\ncoders. Machine Learning: Science and Technology 3, 025001. doi:10.\n1088/2632-2153/ac5435, arXiv:2107.12698.\nMoschou, S.P., Hicks, E., Parekh, R.Y., Mathew, D., Majumdar, S., Vla-\nhakis, N., 2023.\nPhysics-informed neural networks for modeling as-\ntrophysical shocks.\nMachine Learning:\nScience and Technology 4,\n035032.\nURL: https://dx.doi.org/10.1088/2632-2153/acf116,\ndoi:10.1088/2632-2153/acf116.\nMostert, R.I.J., Duncan, K.J., R¨ottgering, H.J.A., Polsterer, K.L., Best, P.N.,\nBrienza, M., Br¨uggen, M., Hardcastle, M.J., Jurlin, N., Mingo, B., Morganti,\nR., Shimwell, T., Smith, D., Williams, W.L., 2021. Unveiling the rarest\nmorphologies of the LOFAR Two-metre Sky Survey radio source popula-\ntion with self-organised maps. A&A 645, A89. doi:10.1051/0004-6361/\n202038500, arXiv:2011.06001.\nMukherjee, S., Feigelson, E.D., Jogesh Babu, G., Murtagh, F., Fraley, C.,\nRaftery, A., 1998. Three Types of Gamma-Ray Bursts. ApJ 508, 314–327.\ndoi:10.1086/306386, arXiv:astro-ph/9802085.\nNaim, A., Ratnatunga, K.U., Griffiths, R.E., 1997. Galaxy Morphology without\nClassification: Self-organizing Maps. ApJS 111, 357–367. doi:10.1086/\n313022.\nNg, R., Han, J., 2002. Clarans: a method for clustering objects for spatial\ndata mining. IEEE Transactions on Knowledge and Data Engineering 14,\n1003–1016. doi:10.1109/TKDE.2002.1033770.\nNicholson, G., Contaldi, C.R., Paykari, P., 2010. Reconstruction of the primor-\ndial power spectrum by direct inversion. JCAP 2010, 016. doi:10.1088/\n1475-7516/2010/01/016, arXiv:0909.5092.\nNoormohammadi, M., Khakian Ghomi, M., Haghi, H., 2023. The membership\nof stars, density profile, and mass segregation in open clusters using a new\nmachine learning-based method. MNRAS 523, 3538–3554. doi:10.1093/\nmnras/stad1589, arXiv:2305.17728.\nO’Briain, T., Ting, Y.S., Fabbro, S., Yi, K.M., Venn, K., Bialek, S.,\n2020.\nInterpreting Stellar Spectra with Unsupervised Domain Adap-\ntation.\narXiv e-prints , arXiv:2007.03112doi:10.48550/arXiv.2007.\n03112, arXiv:2007.03112.\nOdewahn, S.C., Cohen, S.H., Windhorst, R.A., Philip, N.S., 2002. Automated\nGalaxy Morphology: A Fourier Approach. ApJ 568, 539–557. doi:10.\n1086/339036, arXiv:astro-ph/0110275.\nOdewahn, S.C., Stockwell, E.B., Pennington, R.L., Humphreys, R.M., Zumach,\nW.A., 1992. Automated Star/Galaxy Discrimination With Neural Networks.\nAJ 103, 318. doi:10.1086/116063.\nvan den Oord, A., Vinyals, O., Kavukcuoglu, K., 2017. Neural discrete repre-\nsentation learning, in: Proceedings of the 31st International Conference on\nNeural Information Processing Systems, Curran Associates Inc., Red Hook,\nNY, USA. p. 6309–6318.\nOrtega-Martinez, S., Obreja, A., Dominguez-Tenreiro, R., Pedrosa, S.E.,\nRosas-Guevara, Y., Tissera, P.B., 2022.\nMilky Way-like galaxies: stel-\nlar population properties of dynamically defined discs, bulges and stel-\nlar haloes.\nMNRAS 516, 197–215.\ndoi:10.1093/mnras/stac2033,\narXiv:2207.08776.\nPang, D., Goseva-Popstojanova, K., Devine, T., McLaughlin, M., 2018.\nA\nnovel single-pulse search approach to detection of dispersed radio pulses us-\ning clustering and supervised machine learning. MNRAS 480, 3302–3323.\ndoi:10.1093/mnras/sty1992, arXiv:1807.07164.\nPapaefthymiou, E.S., Michos, I., Pavlou, O., Papadopoulou Lesta, V., Efs-\ntathiou, A., 2022. Classification of local ultraluminous infrared galaxies\nand quasars with kernel principal component analysis. MNRAS 517, 4162–\n4174. doi:10.1093/mnras/stac2917, arXiv:2211.07758.\nPˆaris, I., Petitjean, P., Aubourg, ´E., Myers, A.D., Streblyanska, A., Lyke, B.W.,\nAnderson, S.F., Armengaud, ´E., Bautista, J., Blanton, M.R., Blomqvist, M.,\nBrinkmann, J., Brownstein, J.R., Brandt, W.N., Burtin, ´E., Dawson, K., de\nla Torre, S., Georgakakis, A., Gil-Mar´ın, H., Green, P.J., Hall, P.B., Kneib,\nJ.P., LaMassa, S.M., Le Goff, J.M., MacLeod, C., Mariappan, V., McGreer,\nI.D., Merloni, A., Noterdaeme, P., Palanque-Delabrouille, N., Percival, W.J.,\nRoss, A.J., Rossi, G., Schneider, D.P., Seo, H.J., Tojeiro, R., Weaver, B.A.,\nWeijmans, A.M., Y`eche, C., Zarrouk, P., Zhao, G.B., 2018. The Sloan Dig-\nital Sky Survey Quasar Catalog: Fourteenth data release. A&A 613, A51.\ndoi:10.1051/0004-6361/201732445, arXiv:1712.05029.\nPark, H.S., Jun, C.H., 2009. A simple and fast algorithm for k-medoids clus-\ntering.\nExpert Systems with Applications 36, 3336–3341.\ndoi:https:\n//doi.org/10.1016/j.eswa.2008.01.039.\nPasquato, M., Chung, C., 2019. Clustering clusters: unsupervised machine\nlearning on globular cluster structural parameters.\nMNRAS 490, 3392–\n3403. doi:10.1093/mnras/stz2766, arXiv:1901.05354.\nPedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O.,\nBlondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos,\nA., Cournapeau, D., Brucher, M., Perrot, M., Duchesnay, E., 2011. Scikit-\nlearn: Machine learning in Python. Journal of Machine Learning Research\n12, 2825–2830.\nPiana, M., Brown, J.C., 1998. Optimal inversion of hard X-ray bremsstrahlung\nspectra. I. SVD analysis.\nA&AS 132, 291–299.\ndoi:10.1051/aas:\n1998447.\nPlanck Collaboration, Ade, P.A.R., Aghanim, N., Alves, M.I.R., Armitage-\nCaplan, C., Arnaud, M., Ashdown, M., Atrio-Barandela, F., Aumont, J.,\nAussel, H., et al., 2014. Planck 2013 results. I. Overview of products and\nscientific results. A&A 571, A1. doi:10.1051/0004-6361/201321529,\narXiv:1303.5062.\nPlanck Collaboration, Aghanim, N., Arnaud, M., Ashdown, M., Aumont, J.,\nBaccigalupi, C., Banday, A.J., Barreiro, R.B., Bartlett, J.G., Bartolo, N.,\net al., 2016a. Planck 2015 results. XI. CMB power spectra, likelihoods,\nand robustness of parameters. A&A 594, A11. doi:10.1051/0004-6361/\n201526926, arXiv:1507.02704.\nPlanck Collaboration, Aghanim, N., Arnaud, M., Ashdown, M., Aumont, J.,\nBaccigalupi, C., Banday, A.J., Barreiro, R.B., Bartlett, J.G., Bartolo, N.,\net al., 2016b. Planck 2015 results. XI. CMB power spectra, likelihoods,\nand robustness of parameters. A&A 594, A11. doi:10.1051/0004-6361/\n201526926, arXiv:1507.02704.\nPress, W.H., Davis, M., 1982. How to identify and weigh virialized clusters of\ngalaxies in a complete redshift catalog. ApJ 259, 449–473. doi:10.1086/\n160183.\nPress, W.H., Teukolsky, S.A., Vetterling, W.T., Flannery, B.P., 2007. Numerical\nRecipes 3rd Edition: The Art of Scientific Computing. 3 ed., Cambridge\nUniversity Press, USA.\n26\nPrisinzano, L., Damiani, F., Sciortino, S., Flaccomio, E., Guarcello, M.G.,\nMicela, G., Tognelli, E., Jeffries, R.D., Alcal´a, J.M., 2022.\nLow-mass\nyoung stars in the Milky Way unveiled by DBSCAN and Gaia EDR3:\nMapping the star forming regions within 1.5 kpc.\nA&A 664, A175.\ndoi:10.1051/0004-6361/202243580, arXiv:2206.00249.\nRajaniemi, H.J., M¨ah¨onen, P., 2002.\nClassifying Gamma-Ray Bursts using\nSelf-organizing Maps. ApJ 566, 202–209. doi:10.1086/337959.\nRalph, N.O., Norris, R.P., Fang, G., Park, L.A.F., Galvin, T.J., Alger, M.J.,\nAndernach, H., Lintott, C., Rudnick, L., Shabala, S., Wong, O.I., 2019.\nRadio Galaxy Zoo:\nUnsupervised Clustering of Convolutionally Auto-\nencoded Radio-astronomical Images. PASP 131, 108011. doi:10.1088/\n1538-3873/ab213d, arXiv:1906.02864.\nRani, V., Nabi, S.T., Kumar, M., Mittal, A., Kumar, K., 2023.\nSelf-\nsupervised learning: A succinct review. Archives of Computational Meth-\nods in Engineering 30, 2761–2775. URL: https://doi.org/10.1007/\ns11831-023-09884-2, doi:10.1007/s11831-023-09884-2.\nReis, I., Poznanski, D., Baron, D., Zasowski, G., Shahaf, S., 2018. Detecting\noutliers and learning complex structures with large spectroscopic surveys -\na case study with APOGEE stars. MNRAS 476, 2117–2136. doi:10.1093/\nmnras/sty348, arXiv:1711.00022.\nReis, I., Rotman, M., Poznanski, D., Prochaska, J.X., Wolf, L., 2021. Effec-\ntively using unsupervised machine learning in next generation astronomical\nsurveys. Astronomy and Computing 34, 100437. doi:10.1016/j.ascom.\n2020.100437, arXiv:1911.06823.\nRichards, J.W., Homrighausen, D., Freeman, P.E., Schafer, C.M., Poznanski,\nD., 2012.\nSemi-supervised learning for photometric supernova classifi-\ncation.\nMNRAS 419, 1121–1135.\ndoi:10.1111/j.1365-2966.2011.\n19768.x, arXiv:1103.6034.\nRichardson, C.T., Allen, J.T., Baldwin, J.A., Hewett, P.C., Ferland, G.J., 2014.\nInterpreting the ionization sequence in AGN emission-line spectra. MNRAS\n437, 2376–2403. doi:10.1093/mnras/stt2056, arXiv:1310.6402.\nRicketts, B.J., Steiner, J.F., Garraffo, C., Remillard, R.A., Huppenkothen, D.,\n2023. Mapping the X-ray variability of GRS 1915 + 105 with machine\nlearning.\nMNRAS 523, 1946–1966.\ndoi:10.1093/mnras/stad1332,\narXiv:2301.10467.\nRomano, J.D., Cornish, N.J., 2017.\nDetection methods for stochas-\ntic gravitational-wave backgrounds:\na unified treatment.\nLiving\nReviews in Relativity 20,\n2.\ndoi:10.1007/s41114-017-0004-1,\narXiv:1608.06889.\nRoweis, S.T., Saul, L.K., 2000.\nNonlinear Dimensionality Reduction by\nLocally Linear Embedding.\nScience 290, 2323–2326.\ndoi:10.1126/\nscience.290.5500.2323.\nRubin, A., Gal-Yam, A., 2016.\nUnsupervised Clustering of Type II Super-\nnova Light Curves. ApJ 828, 111. doi:10.3847/0004-637X/828/2/111,\narXiv:1602.01446.\nRudick, C.S., Mihos, J.C., Frey, L.H., McBride, C.K., 2009. Tidal Streams of\nIntracluster Light. ApJ 699, 1518–1529. doi:10.1088/0004-637X/699/\n2/1518, arXiv:0906.1185.\nSakai, Y., Itoh, Y., Jung, P., Kokeyama, K., Kozakai, C., Nakahira, K.T., Os-\nhino, S., Shikano, Y., Takahashi, H., Uchiyama, T., Ueshima, G., Washimi,\nT., Yamamoto, T., Yokozawa, T., 2022. Unsupervised learning architecture\nfor classifying the transient noise of interferometric gravitational-wave de-\ntectors. Scientific Reports 12, 9935. doi:10.1038/s41598-022-13329-4,\narXiv:2111.10053.\nSalvato, M., Ilbert, O., Hoyle, B., 2019.\nThe many flavours of pho-\ntometric redshifts.\nNature Astronomy 3, 212–222.\ndoi:10.1038/\ns41550-018-0478-0, arXiv:1805.12574.\nS´anchez Almeida, J., Aguerri, J.A.L., Mu˜noz-Tu˜n´on, C., de Vicente, A.,\n2010. Automatic Unsupervised Classification of All Sloan Digital Sky Sur-\nvey Data Release 7 Galaxy Spectra. ApJ 714, 487–504. doi:10.1088/\n0004-637X/714/1/487, arXiv:1003.3186.\nS´anchez Almeida, J., Allende Prieto, C., 2013.\nAutomated Unsupervised\nClassification of the Sloan Digital Sky Survey Stellar Spectra using k-\nmeans Clustering.\nApJ 763, 50.\ndoi:10.1088/0004-637X/763/1/50,\narXiv:1211.5321.\nSanders, J.L., Matsunaga, N., 2023. Hunting for C-rich long-period variable\nstars in the Milky Way’s bar-bulge using unsupervised classification of Gaia\nBP/RP spectra. MNRAS 521, 2745–2764. doi:10.1093/mnras/stad574,\narXiv:2302.10022.\nSarkar, J., Bhatia, K., Saha, S., Safonova, M., Sarkar, S., 2022. Postulating\nexoplanetary habitability via a novel anomaly detection method. MNRAS\n510, 6022–6032. doi:10.1093/mnras/stab3556, arXiv:2109.02273.\nSarmiento, R., Huertas-Company, M., Knapen, J.H., S´anchez, S.F., Dom´ınguez\nS´anchez, H., Drory, N., Falc´on-Barroso, J., 2021. Capturing the Physics of\nMaNGA Galaxies with Self-supervised Machine Learning. ApJ 921, 177.\ndoi:10.3847/1538-4357/ac1dac, arXiv:2104.08292.\nSarro, L.M., Debosscher, J., Aerts, C., L´opez, M., 2009. Comparative clus-\ntering analysis of variable stars in the Hipparcos, OGLE Large Magellanic\nCloud, and CoRoT exoplanet databases. A&A 506, 535–568. doi:10.1051/\n0004-6361/200912009, arXiv:0906.0304.\nSasdelli, M., Ishida, E.E.O., Vilalta, R., Aguena, M., Busti, V.C., Camacho,\nH., Trindade, A.M.M., Gieseke, F., de Souza, R.S., Fantaye, Y.T., Mazzali,\nP.A., 2016. Exploring the spectroscopic diversity of Type Ia supernovae\nwith DRACULA: a machine learning approach. MNRAS 461, 2044–2059.\ndoi:10.1093/mnras/stw1228, arXiv:1512.06810.\nSavi´c, D.V., Jankov, I., Yu, W., Petrecca, V., Temple, M.J., Ni, Q., Shirley, R.,\nKovaˇcevi´c, A.B., Nikoli´c, M., Ili´c, D., Popovi´c, L. ˇC., Paolillo, M., Panda,\nS., ´Ciprijanovi´c, A., Richards, G.T., 2023. The LSST AGN Data Challenge:\nSelection Methods.\nApJ 953, 138.\ndoi:10.3847/1538-4357/ace31a,\narXiv:2307.04072.\nSchutter, A., Shamir, L., 2015. Galaxy morphology - An unsupervised machine\nlearning approach. Astronomy and Computing 12, 60–66. doi:10.1016/j.\nascom.2015.05.002, arXiv:1505.04876.\nSch¨olkopf, B., Smola, A., M¨uller, K.R., 1998. Nonlinear Component Analy-\nsis as a Kernel Eigenvalue Problem. Neural Computation 10, 1299–1319.\ndoi:10.1162/089976698300017467.\nScoville, N., Aussel, H., Brusa, M., Capak, P., Carollo, C.M., Elvis, M., Gi-\navalisco, M., Guzzo, L., Hasinger, G., Impey, C., Kneib, J.P., LeFevre, O.,\nLilly, S.J., Mobasher, B., Renzini, A., Rich, R.M., Sanders, D.B., Schin-\nnerer, E., Schminovich, D., Shopbell, P., Taniguchi, Y., Tyson, N.D., 2007.\nThe Cosmic Evolution Survey (COSMOS): Overview.\nApJS 172, 1–8.\ndoi:10.1086/516585, arXiv:astro-ph/0612305.\nSelim, I.M., Abd El Aziz, M., 2017.\nAutomated morphological classifica-\ntion of galaxies based on projection gradient nonnegative matrix factor-\nization algorithm. Experimental Astronomy 43, 131–144. doi:10.1007/\ns10686-017-9524-7.\nSettles, B., 2009. Active Learning Literature Survey. Computer Sciences Tech-\nnical Report 1648. University of Wisconsin–Madison.\nShamir, L., 2012. Automatic detection of peculiar galaxies in large datasets\nof galaxy images. Journal of Computational Science 3, 181–189. doi:10.\n1016/j.jocs.2012.03.004.\nShamir, L., Holincheck, A., Wallin, J., 2013. Automatic quantitative morpho-\nlogical analysis of interacting galaxies. Astronomy and Computing 2, 67–\n73. doi:10.1016/j.ascom.2013.09.002, arXiv:1309.4014.\nShen, H., George, D., Huerta, E.A., Zhao, Z., 2017.\nDenoising Gravi-\ntational Waves using Deep Learning with Recurrent Denoising Autoen-\ncoders.\narXiv e-prints , arXiv:1711.09919doi:10.48550/arXiv.1711.\n09919, arXiv:1711.09919.\nSheth, K., Regan, M., Hinz, J.L., Gil de Paz, A., Men´endez-Delmestre, K.,\nMu˜noz-Mateos, J.C., Seibert, M., Kim, T., Laurikainen, E., Salo, H.,\nGadotti, D.A., Laine, J., Mizusawa, T., Armus, L., Athanassoula, E., Bosma,\nA., Buta, R.J., Capak, P., Jarrett, T.H., Elmegreen, D.M., Elmegreen, B.G.,\nKnapen, J.H., Koda, J., Helou, G., Ho, L.C., Madore, B.F., Masters, K.L.,\nMobasher, B., Ogle, P., Peng, C.Y., Schinnerer, E., Surace, J.A., Zaritsky,\nD., Comer´on, S., de Swardt, B., Meidt, S.E., Kasliwal, M., Aravena, M.,\n2010. The Spitzer Survey of Stellar Structure in Galaxies (S4G). PASP 122,\n1397. doi:10.1086/657638, arXiv:1010.1592.\nSimon, K.P., Sturm, E., 1994. Disentangling of composite spectra. A&A 281,\n286–291.\nSkrutskie, M.F., Cutri, R.M., Stiening, R., Weinberg, M.D., Schneider, S., Car-\npenter, J.M., Beichman, C., Capps, R., Chester, T., Elias, J., Huchra, J.,\nLiebert, J., Lonsdale, C., Monet, D.G., Price, S., Seitzer, P., Jarrett, T., Kirk-\npatrick, J.D., Gizis, J.E., Howard, E., Evans, T., Fowler, J., Fullmer, L., Hurt,\nR., Light, R., Kopan, E.L., Marsh, K.A., McCallon, H.L., Tam, R., Van Dyk,\nS., Wheelock, S., 2006. The Two Micron All Sky Survey (2MASS). AJ 131,\n1163–1183. doi:10.1086/498708.\nSlijepcevic, I.V., Scaife, A.M.M., Walmsley, M., Bowles, M., Wong, O.I., Sha-\nbala, S.S., Tang, H., 2022. Radio Galaxy Zoo: using semi-supervised learn-\ning to leverage large unlabelled data sets for radio galaxy classification under\ndata set shift. MNRAS 514, 2599–2613. doi:10.1093/mnras/stac1135,\narXiv:2204.08816.\nSlijepcevic, I.V., Scaife, A.M.M., Walmsley, M., Bowles, M., Wong, O.I., Sha-\n27\nbala, S.S., White, S.V., 2023. Radio Galaxy Zoo: Towards building the first\nmulti-purpose foundation model for radio astronomy with self-supervised\nlearning. arXiv e-prints , arXiv:2305.16127doi:10.48550/arXiv.2305.\n16127, arXiv:2305.16127.\nSlijepcevic, I.V., Scaife, A.M.M., Walmsley, M., Bowles, M., Wong, O.I., Sha-\nbala, S.S., White, S.V., 2024. Radio galaxy zoo: towards building the first\nmultipurpose foundation model for radio astronomy with self-supervised\nlearning. RAS Techniques and Instruments 3, 19–32. doi:10.1093/rasti/\nrzad055, arXiv:2305.16127.\nSmith, M.J., Geach, J.E., 2023. Astronomia ex machina: a history, primer and\noutlook on neural networks in astronomy. Royal Society Open Science 10,\n221454. doi:10.1098/rsos.221454, arXiv:2211.03796.\nSpeagle, J.S., Eisenstein, D.J., 2017.\nDeriving photometric redshifts using\nfuzzy archetypes and self-organizing maps - I. Methodology. MNRAS 469,\n1186–1204. doi:10.1093/mnras/stw1485.\nSpindler, A., Geach, J.E., Smith, M.J., 2021.\nAstroVaDEr: astronomical\nvariational deep embedder for unsupervised morphological classification\nof galaxies and synthetic image generation.\nMNRAS 502, 985–1007.\ndoi:10.1093/mnras/staa3670, arXiv:2009.08470.\nStein, G., Harrington, P., Blaum, J., Medan, T., Lukic, Z., 2021.\nSelf-\nsupervised\nsimilarity\nsearch\nfor\nlarge\nscientific\ndatasets.\narXiv\ne-prints\n,\narXiv:2110.13151doi:10.48550/arXiv.2110.13151,\narXiv:2110.13151.\nSteinhardt, C.L., Weaver, J.R., Maxfield, J., Davidzon, I., Faisst, A.L., Masters,\nD., Schemel, M., Toft, S., 2020. A Method to Distinguish Quiescent and\nDusty Star-forming Galaxies with Machine Learning. ApJ 891, 136. doi:10.\n3847/1538-4357/ab76be, arXiv:2002.05729.\nStevens, G., Fotopoulou, S., Bremer, M., Ray, O., 2021.\nAstronomicAL:\nan interactive dashboard for visualisation, integration and classification of\ndata with Active Learning. The Journal of Open Source Software 6, 3635.\ndoi:10.21105/joss.03635, arXiv:2109.05207.\nSt¨olzner, B., Joachimi, B., Korn, A., LSST Dark Energy Science Collaboration,\n2023. Optimizing the shape of photometric redshift distributions with clus-\ntering cross-correlations. MNRAS 519, 2438–2450. doi:10.1093/mnras/\nstac3630, arXiv:2205.13622.\nStorey-Fisher, K., Huertas-Company, M., Ramachandra, N., Lanusse, F., Leau-\nthaud, A., Luo, Y., Huang, S., Prochaska, J.X., 2021.\nAnomaly de-\ntection in Hyper Suprime-Cam galaxy images with generative adversarial\nnetworks.\nMNRAS 508, 2946–2963.\ndoi:10.1093/mnras/stab2589,\narXiv:2105.02434.\nStorrie-Lombardi, M.C., Irwin, M.J., von Hippel, T., Storrie-Lombardi, L.J.,\n1994. Spectral classification with principal component analysis and arti-\nficial neural networks. Vistas in Astronomy 38, 331–340. doi:10.1016/\n0083-6656(94)90044-2.\nSulentic, J.W., Zwitter, T., Marziani, P., Dultzin-Hacyan, D., 2000. Eigenvector\n1: An Optimal Correlation Space for Active Galactic Nuclei. ApJL 536, L5–\nL9. doi:10.1086/312717, arXiv:astro-ph/0005177.\nS¨uveges, M., Barblan, F., Lecoeur-Ta¨ıbi, I., Prˇsa, A., Holl, B., Eyer, L., Ko-\nchoska, A., Mowlavi, N., Rimoldini, L., 2017. Gaia eclipsing binary and\nmultiple systems. Supervised classification and self-organizing maps. A&A\n603, A117. doi:10.1051/0004-6361/201629710, arXiv:1702.06296.\nTagliaferri, R., Ciaramella, A., Milano, L., Barone, F., Longo, G., 1999. Spec-\ntral analysis of stellar light curves by means of neural networks. A&AS 137,\n391–405. doi:10.1051/aas:1999254, arXiv:astro-ph/9906182.\nTammour, A., Gallagher, S.C., Daley, M., Richards, G.T., 2016. Insights into\nquasar UV spectra using unsupervised clustering analysis. MNRAS 459,\n1659–1681. doi:10.1093/mnras/stw586, arXiv:1603.03318.\nTaylor, M.B., 2005. Astronomical data analysis software and systems xiv, in:\nShopbell, P., Britton, M., Ebert, R. (Eds.), Astronomical Data Analysis Soft-\nware and Systems XIV, p. 29.\nTeimoorinia, H., Archinuk, F., Woo, J., Shishehchi, S., Bluck, A.F.L.,\n2022.\nMapping the Diversity of Galaxy Spectra with Deep Unsuper-\nvised Machine Learning. AJ 163, 71. doi:10.3847/1538-3881/ac4039,\narXiv:2112.03425.\nTemple, M.J., Hewett, P.C., Banerji, M., 2021. Modelling type 1 quasar colours\nin the era of Rubin and Euclid. MNRAS 508, 737–754. doi:10.1093/\nmnras/stab2586, arXiv:2109.04472.\nTenenbaum, J.B., de Silva, V., Langford, J.C., 2000.\nA Global Geometric\nFramework for Nonlinear Dimensionality Reduction. Science 290, 2319–\n2323. doi:10.1126/science.290.5500.2319.\nTody, D., 1986. The IRAF Data Reduction and Analysis System, in: Crawford,\nD.L. (Ed.), Instrumentation in astronomy VI, p. 733.\ndoi:10.1117/12.\n968154.\nTody, D., 1993. IRAF in the Nineties, in: Hanisch, R.J., Brissenden, R.J.V.,\nBarnes, J. (Eds.), Astronomical Data Analysis Software and Systems II, p.\n173.\nTohill, C.B., Bamford, S., Conselice, C., Ferreira, L., Harvey, T., Adams, N.,\nAustin, D., 2023. A Robust Study of High-Redshift Galaxies: Unsuper-\nvised Machine Learning for Characterising morphology with JWST up to z\n˜8. arXiv e-prints , arXiv:2306.17225doi:10.48550/arXiv.2306.17225,\narXiv:2306.17225.\nTramacere, A., Paraficz, D., Dubath, P., Kneib, J.P., Courbin, F., 2016. ASTEr-\nIsM: application of topometric clustering algorithms in automatic galaxy de-\ntection and classification. MNRAS 463, 2939–2957. doi:10.1093/mnras/\nstw2103, arXiv:1609.06728.\nTsang, B.T.H., Schultz, W.C., 2019. Deep Neural Network Classifier for Vari-\nable Stars with Novelty Detection Capability. ApJ 877, L14. doi:10.3847/\n2041-8213/ab212c, arXiv:1905.05767.\nTurmon, M., Pap, J.M., Mukhtar, S., 2002. Statistical Pattern Recognition for\nLabeling Solar Active Regions: Application to SOHO/MDI Imagery. ApJ\n568, 396–407. doi:10.1086/338681.\nVanderplas, J., Connolly, A., 2009.\nReducing the Dimensionality of Data:\nLocally Linear Embedding of Sloan Galaxy Spectra. AJ 138, 1365–1379.\ndoi:10.1088/0004-6256/138/5/1365, arXiv:0907.2238.\nVar´on, C., Alzate, C., Suykens, J.A.K., Debosscher, J., 2011. Kernel spectral\nclustering of time series in the CoRoT exoplanet database. A&A 531, A156.\ndoi:10.1051/0004-6361/201016419.\nˇCotar, K., Zwitter, T., Kos, J., Munari, U., Martell, S.L., Asplund, M., Bland-\nHawthorn, J., Buder, S., de Silva, G.M., Freeman, K.C., Sharma, S.,\nAnguiano, B., Carollo, D., Horner, J., Lewis, G.F., Nataf, D.M., Nord-\nlander, T., Stello, D., Ting, Y.S., Tinney, C., Traven, G., Wittenmyer,\nR.A., Galah Collaboration, 2019.\nThe GALAH survey: a catalogue of\ncarbon-enhanced stars and CEMP candidates. MNRAS 483, 3196–3212.\ndoi:10.1093/mnras/sty3155, arXiv:1807.07977.\nVega-Ferrero, J., Huertas-Company, M., Costantin, L., P´erez-Gonz´alez, P.G.,\nSarmiento, R., Kartaltepe, J.S., Pillepich, A., Bagley, M.B., Finkelstein,\nS.L., McGrath, E.J., Knapen, J.H., Arrabal Haro, P., Bell, E.F., Buitrago,\nF., Calabr`o, A., Dekel, A., Dickinson, M., Dom´ınguez S´anchez, H., El-\nbaz, D., Ferguson, H.C., Giavalisco, M., Holwerda, B.W., Kocesvski,\nD.D., Koekemoer, A.M., Pandya, V., Papovich, C., Pirzkal, N., Pri-\nmack, J., Yung, L.Y.A., 2023.\nOn the nature of disks at high redshift\nseen by JWST/CEERS with contrastive learning and cosmological simu-\nlations.\narXiv e-prints , arXiv:2302.07277doi:10.48550/arXiv.2302.\n07277, arXiv:2302.07277.\nVilela Mendes, R., 1999. Scientific applications of neural nets, in: Clark, J.W.,\nLindenau, T., Ristig, M.L. (Eds.), Scientific Applications of Neural Nets.\nvolume 522, p. 257. doi:10.48550/arXiv.cond-mat/9808043.\nVillar, V.A., Hosseinzadeh, G., Berger, E., Ntampaka, M., Jones, D.O., Chal-\nlis, P., Chornock, R., Drout, M.R., Foley, R.J., Kirshner, R.P., Lunnan, R.,\nMargutti, R., Milisavljevic, D., Sanders, N., Pan, Y.C., Rest, A., Scolnic,\nD.M., Magnier, E., Metcalfe, N., Wainscoat, R., Waters, C., 2020. Super-\nRAENN: A Semisupervised Supernova Photometric Classification Pipeline\nTrained on Pan-STARRS1 Medium-Deep Survey Supernovae. ApJ 905, 94.\ndoi:10.3847/1538-4357/abc6fd, arXiv:2008.04921.\nVincent, P., Larochelle, H., Bengio, Y., Manzagol, P.A., 2008.\nExtracting\nand composing robust features with denoising autoencoders, in: Proceed-\nings of the 25th International Conference on Machine Learning, Asso-\nciation for Computing Machinery, New York, NY, USA. p. 1096–1103.\nURL: https://doi.org/10.1145/1390156.1390294, doi:10.1145/\n1390156.1390294.\nWagstaff, K.L., Shu, H.P., Mazzoni, D., Castano, R., 2005. Semi-Supervised\nData Summarization: Using Spectral Libraries to Improve Hyperspectral\nClustering. Interplanetary Network Progress Report 42-163, 1–14.\nWaldmann, I.P., Tinetti, G., Deroo, P., Hollis, M.D.J., Yurchenko, S.N., Ten-\nnyson, J., 2013.\nBlind Extraction of an Exoplanetary Spectrum through\nIndependent Component Analysis. ApJ 766, 7. doi:10.1088/0004-637X/\n766/1/7, arXiv:1301.4041.\nWalmsley, M., Scaife, A.M.M., Lintott, C., Lochner, M., Etsebeth, V., G´eron,\nT., Dickinson, H., Fortson, L., Kruk, S., Masters, K.L., Mantha, K.B., Sim-\nmons, B.D., 2022. Practical galaxy morphology tools from deep supervised\nrepresentation learning. MNRAS 513, 1581–1599. doi:10.1093/mnras/\nstac525, arXiv:2110.12735.\n28\nWalmsley, M., Smith, L., Lintott, C., Gal, Y., Bamford, S., Dickinson, H.,\nFortson, L., Kruk, S., Masters, K., Scarlata, C., Simmons, B., Smethurst,\nR., Wright, D., 2020.\nGalaxy Zoo: probabilistic morphology through\nBayesian CNNs and active learning. MNRAS 491, 1554–1574. doi:10.\n1093/mnras/stz2816, arXiv:1905.07424.\nWang, J.M., Bon, E., 2020. Changing-look active galactic nuclei: close bina-\nries of supermassive black holes in action. A&A 643, L9. doi:10.1051/\n0004-6361/202039368, arXiv:2010.04417.\nWay, M.J., Klose, C.D., 2012.\nCan Self-Organizing Maps Accurately Pre-\ndict Photometric Redshifts?\nPASP 124, 274.\ndoi:10.1086/664796,\narXiv:1201.1098.\nWebb, K., Balogh, M.L., Leja, J., van der Burg, R.F.J., Rudnick, G., Muzzin,\nA., Boak, K., Cerulo, P., Gilbank, D., Lidman, C., Old, L.J., Pintos-Castro,\nI., McGee, S., Shipley, H., Biviano, A., Chan, J.C.C., Cooper, M., De Lu-\ncia, G., Demarco, R., Forrest, B., Jablonka, P., Kukstas, E., McCarthy, I.G.,\nMcNab, K., Nantais, J., Noble, A., Poggianti, B., Reeves, A.M.M., Vulcani,\nB., Wilson, G., Yee, H.K.C., Zaritsky, D., 2020a. The GOGREEN survey:\npost-infall environmental quenching fails to predict the observed age differ-\nence between quiescent field and cluster galaxies at z ¿ 1. MNRAS 498,\n5317–5342. doi:10.1093/mnras/staa2752, arXiv:2009.03953.\nWebb, S., Flynn, C., Cooke, J., Zhang, J., Mahabal, A., Abbott, T.M.C., Allen,\nR., Andreoni, I., Bird, S.A., Goode, S., Lochner, M., Pritchard, T., 2021.\nThe Deeper, Wider, Faster programme: exploring stellar flare activity with\ndeep, fast cadenced DECam imaging via machine learning. MNRAS 506,\n2089–2103. doi:10.1093/mnras/stab1798, arXiv:2106.13026.\nWebb, S., Lochner, M., Muthukrishna, D., Cooke, J., Flynn, C., Mahabal,\nA., Goode, S., Andreoni, I., Pritchard, T., Abbott, T.M.C., 2020b. Unsu-\npervised machine learning for transient discovery in deeper, wider, faster\nlight curves. MNRAS 498, 3077–3094. doi:10.1093/mnras/staa2395,\narXiv:2008.04666.\nWei, S., Li, Y., Lu, W., Li, N., Liang, B., Dai, W., Zhang, Z., 2022.\nUn-\nsupervised Galaxy Morphological Visual Representation with Deep Con-\ntrastive Learning. PASP 134, 114508. doi:10.1088/1538-3873/aca04e,\narXiv:2211.07168.\nWetzel, S.J., 2017. Unsupervised learning of phase transitions: From principal\ncomponent analysis to variational autoencoders. Phys. Rev. E 96, 022140.\ndoi:10.1103/PhysRevE.96.022140, arXiv:1703.02435.\nvon Wietersheim-Kramsta, M., Lin, K., Tessore, N., Joachimi, B., Loureiro,\nA., Reischke, R., Wright, A.H., 2024. Kids-sbi simulation-based inference\nanalysis of kids-1000 cosmic shear. arXiv:2404.15402.\nWild, V., Almaini, O., Cirasuolo, M., Dunlop, J., McLure, R., Bowler, R.,\nFerreira, J., Bradshaw, E., Chuter, R., Hartley, W., 2014. A new method for\nclassifying galaxy SEDs from multiwavelength photometry. MNRAS 440,\n1880–1898. doi:10.1093/mnras/stu212, arXiv:1401.7878.\nWolf, C., Dye, S., Kleinheinrich, M., Meisenheimer, K., Rix, H.W., Wisotzki,\nL., 2001a. Deep BVR photometry of the Chandra Deep Field South from\nthe COMBO-17 survey. A&A 377, 442–449. doi:10.1051/0004-6361:\n20011142, arXiv:astro-ph/0012474.\nWolf, C., Meisenheimer, K., R¨oser, H.J., 2001b.\nObject classification in\nastronomical multi-color surveys.\nA&A 365, 660–680.\ndoi:10.1051/\n0004-6361:20000474, arXiv:astro-ph/0010092.\nWright, A.H., Hildebrandt, H., van den Busch, J.L., Heymans, C., 2020. Pho-\ntometric redshift calibration with self-organising maps. A&A 637, A100.\ndoi:10.1051/0004-6361/201936782, arXiv:1909.09632.\nXiang, M.S., Liu, X.W., Shi, J.R., Yuan, H.B., Huang, Y., Luo, A.L., Zhang,\nH.W., Zhao, Y.H., Zhang, J.N., Ren, J.J., Chen, B.Q., Wang, C., Li, J., Huo,\nZ.Y., Zhang, W., Wang, J.L., Zhang, Y., Hou, Y.H., Wang, Y.F., 2017. Es-\ntimating stellar atmospheric parameters, absolute magnitudes and elemental\nabundances from the LAMOST spectra with Kernel-based principal compo-\nnent analysis. MNRAS 464, 3657–3678. doi:10.1093/mnras/stw2523,\narXiv:1610.00083.\nXu, D., Komossa, S., Zhou, H., Wang, T., Wei, J., 2007. The Narrow-Line\nRegion of Narrow-Line and Broad-Line Type 1 Active Galactic Nuclei. I.\nA Zone of Avoidance in Density. ApJ 670, 60–73. doi:10.1086/521697,\narXiv:0706.2574.\nXu, Q., Shi, Y., Bamber, J., Tuo, Y., Ludwig, R., Zhu, X.X., 2023.\nPhysics-aware Machine Learning Revolutionizes Scientific Paradigm\nfor\nMachine\nLearning\nand\nProcess-based\nHydrology.\narXiv\ne-\nprints\n,\narXiv:2310.05227doi:10.48550/arXiv.2310.05227,\narXiv:2310.05227.\nXue, J.q., Li, Q.b., Zhao, Y.h., 2001.\nAutomatic classification of stellar\nspectra using the SOFM method. ChA&A 25, 120–131. doi:10.1016/\nS0275-1062(01)00051-0.\nXui, J.q., Li, Q.b., Zhao, Y.h., 2001.\nAutomatic classification of stellar\nspectra using the SOFM method. ChA&A 25, 120–131. doi:10.1016/\nS0275-1062(01)00051-0.\nYang, Z., Zhang, H., Xu, P., Luo, Z., 2023.\nUnsupervised Noise Reduc-\ntions for Gravitational Reference Sensors or Accelerometers Based on\nthe Noise2Noise Method. Sensors 23, 6030. doi:10.3390/s23136030,\narXiv:2305.06735.\nYanny, B., Rockosi, C., Newberg, H.J., Knapp, G.R., Adelman-McCarthy, J.K.,\nAlcorn, B., Allam, S., Allende Prieto, C., An, D., Anderson, K.S.J., Ander-\nson, S., Bailer-Jones, C.A.L., Bastian, S., Beers, T.C., Bell, E., Belokurov,\nV., Bizyaev, D., Blythe, N., Bochanski, J.J., Boroski, W.N., Brinchmann,\nJ., Brinkmann, J., Brewington, H., Carey, L., Cudworth, K.M., Evans, M.,\nEvans, N.W., Gates, E., G¨ansicke, B.T., Gillespie, B., Gilmore, G., Nebot\nGomez-Moran, A., Grebel, E.K., Greenwell, J., Gunn, J.E., Jordan, C., Jor-\ndan, W., Harding, P., Harris, H., Hendry, J.S., Holder, D., Ivans, I.I., Iveziˇc,\nˇZ., Jester, S., Johnson, J.A., Kent, S.M., Kleinman, S., Kniazev, A., Krzesin-\nski, J., Kron, R., Kuropatkin, N., Lebedeva, S., Lee, Y.S., French Leger,\nR., L´epine, S., Levine, S., Lin, H., Long, D.C., Loomis, C., Lupton, R.,\nMalanushenko, O., Malanushenko, V., Margon, B., Martinez-Delgado, D.,\nMcGehee, P., Monet, D., Morrison, H.L., Munn, J.A., Neilsen, Eric H.,\nJ., Nitta, A., Norris, J.E., Oravetz, D., Owen, R., Padmanabhan, N., Pan,\nK., Peterson, R.S., Pier, J.R., Platson, J., Re Fiorentin, P., Richards, G.T.,\nRix, H.W., Schlegel, D.J., Schneider, D.P., Schreiber, M.R., Schwope, A.,\nSibley, V., Simmons, A., Snedden, S.A., Allyn Smith, J., Stark, L., Stauf-\nfer, F., Steinmetz, M., Stoughton, C., SubbaRao, M., Szalay, A., Szkody,\nP., Thakar, A.R., Sivarani, T., Tucker, D., Uomoto, A., Vanden Berk, D.,\nVidrih, S., Wadadekar, Y., Watters, S., Wilhelm, R., Wyse, R.F.G., Yarger,\nJ., Zucker, D., 2009. SEGUE: A Spectroscopic Survey of 240,000 Stars with\ng = 14-20. AJ 137, 4377–4399. doi:10.1088/0004-6256/137/5/4377,\narXiv:0902.1781.\nYoon, J., Zhang, Y., Jordon, J., van der Schaar, M., 2020.\nVime: Ex-\ntending the success of self- and semi-supervised learning to tabular do-\nmain, in: Neural Information Processing Systems. URL: https://api.\nsemanticscholar.org/CorpusID:227183683.\nYork, D.G., Adelman, J., Anderson, John E., J., Anderson, S.F., Annis, J., Bah-\ncall, N.A., Bakken, J.A., Barkhouser, R., Bastian, S., Berman, E., Boroski,\nW.N., Bracker, S., Briegel, C., Briggs, J.W., Brinkmann, J., Brunner, R.,\nBurles, S., Carey, L., Carr, M.A., Castander, F.J., Chen, B., Colestock, P.L.,\nConnolly, A.J., Crocker, J.H., Csabai, I., Czarapata, P.C., Davis, J.E., Doi,\nM., Dombeck, T., Eisenstein, D., Ellman, N., Elms, B.R., Evans, M.L., Fan,\nX., Federwitz, G.R., Fiscelli, L., Friedman, S., Frieman, J.A., Fukugita, M.,\nGillespie, B., Gunn, J.E., Gurbani, V.K., de Haas, E., Haldeman, M., Harris,\nF.H., Hayes, J., Heckman, T.M., Hennessy, G.S., Hindsley, R.B., Holm, S.,\nHolmgren, D.J., Huang, C.h., Hull, C., Husby, D., Ichikawa, S.I., Ichikawa,\nT., Ivezi´c, ˇZ., Kent, S., Kim, R.S.J., Kinney, E., Klaene, M., Kleinman,\nA.N., Kleinman, S., Knapp, G.R., Korienek, J., Kron, R.G., Kunszt, P.Z.,\nLamb, D.Q., Lee, B., Leger, R.F., Limmongkol, S., Lindenmeyer, C., Long,\nD.C., Loomis, C., Loveday, J., Lucinio, R., Lupton, R.H., MacKinnon, B.,\nMannery, E.J., Mantsch, P.M., Margon, B., McGehee, P., McKay, T.A.,\nMeiksin, A., Merelli, A., Monet, D.G., Munn, J.A., Narayanan, V.K., Nash,\nT., Neilsen, E., Neswold, R., Newberg, H.J., Nichol, R.C., Nicinski, T., Non-\nino, M., Okada, N., Okamura, S., Ostriker, J.P., Owen, R., Pauls, A.G., Peo-\nples, J., Peterson, R.L., Petravick, D., Pier, J.R., Pope, A., Pordes, R., Pros-\napio, A., Rechenmacher, R., Quinn, T.R., Richards, G.T., Richmond, M.W.,\nRivetta, C.H., Rockosi, C.M., Ruthmansdorfer, K., Sandford, D., Schlegel,\nD.J., Schneider, D.P., Sekiguchi, M., Sergey, G., Shimasaku, K., Siegmund,\nW.A., Smee, S., Smith, J.A., Snedden, S., Stone, R., Stoughton, C., Strauss,\nM.A., Stubbs, C., SubbaRao, M., Szalay, A.S., Szapudi, I., Szokoly, G.P.,\nThakar, A.R., Tremonti, C., Tucker, D.L., Uomoto, A., Vanden Berk, D.,\nVogeley, M.S., Waddell, P., Wang, S.i., Watanabe, M., Weinberg, D.H.,\nYanny, B., Yasuda, N., SDSS Collaboration, 2000. The Sloan Digital Sky\nSurvey: Technical Summary. AJ 120, 1579–1587. doi:10.1086/301513,\narXiv:astro-ph/0006396.\nYouakim, K., Lind, K., Kushniruk, I., 2023. Tidal debris from Omega Centauri\ndiscovered with unsupervised machine learning. MNRAS 524, 2630–2650.\ndoi:10.1093/mnras/stad1952, arXiv:2307.03035.\nZhou, Z.H., 2017. A brief introduction to weakly supervised learning. National\nScience Review 5, 44–53. doi:10.1093/nsr/nwx106.\nZhuang, F., Qi, Z., Duan, K., Xi, D., Zhu, Y., Zhu, H., Xiong, H., He, Q., 2020.\n29\nA comprehensive survey on transfer learning. Proceedings of the IEEE 109,\n43–76.\nZitlau, R., Hoyle, B., Paech, K., Weller, J., Rau, M.M., Seitz, S., 2016. Stacking\nfor machine learning redshifts applied to SDSS galaxies.\nMNRAS 460,\n3152–3162. doi:10.1093/mnras/stw1454, arXiv:1602.06294.\nZong, B., Song, Q., Min, M.R., Cheng, W., Lumezanu, C., Cho, D., Chen, H.,\n2018. Deep autoencoding gaussian mixture model for unsupervised anomaly\ndetection, in: International Conference on Learning Representations. URL:\nhttps://openreview.net/forum?id=BJJLHbb0-.\n30\n",
  "categories": [
    "astro-ph.IM",
    "cs.LG"
  ],
  "published": "2024-06-25",
  "updated": "2024-06-25"
}