{
  "id": "http://arxiv.org/abs/2406.17490v2",
  "title": "BricksRL: A Platform for Democratizing Robotics and Reinforcement Learning Research and Education with LEGO",
  "authors": [
    "Sebastian Dittert",
    "Vincent Moens",
    "Gianni De Fabritiis"
  ],
  "abstract": "We present BricksRL, a platform designed to democratize access to robotics\nfor reinforcement learning research and education. BricksRL facilitates the\ncreation, design, and training of custom LEGO robots in the real world by\ninterfacing them with the TorchRL library for reinforcement learning agents.\nThe integration of TorchRL with the LEGO hubs, via Bluetooth bidirectional\ncommunication, enables state-of-the-art reinforcement learning training on GPUs\nfor a wide variety of LEGO builds. This offers a flexible and cost-efficient\napproach for scaling and also provides a robust infrastructure for\nrobot-environment-algorithm communication. We present various experiments\nacross tasks and robot configurations, providing built plans and training\nresults. Furthermore, we demonstrate that inexpensive LEGO robots can be\ntrained end-to-end in the real world to achieve simple tasks, with training\ntimes typically under 120 minutes on a normal laptop. Moreover, we show how\nusers can extend the capabilities, exemplified by the successful integration of\nnon-LEGO sensors. By enhancing accessibility to both robotics and reinforcement\nlearning, BricksRL establishes a strong foundation for democratized robotic\nlearning in research and educational settings.",
  "text": "BricksRL: A Platform for Democratizing Robotics and\nReinforcement Learning Research and Education with\nLEGO\nSebastian Dittert\nUniversitat Pompeu Fabra\nsebastian.dittert@upf.edu\nVincent Moens\nPyTorch Team, Meta\nvincentmoens@gmail.com\nGianni De Fabritiis\nICREA, Universitat Pompeu Fabra\ng.defabritiis@gmail.com\nAbstract\nWe present BricksRL, a platform designed to democratize access to robotics for\nreinforcement learning research and education. BricksRL facilitates the creation,\ndesign, and training of custom LEGO robots in the real world by interfacing them\nwith the TorchRL library for reinforcement learning agents. The integration of\nTorchRL with the LEGO hubs, via Bluetooth bidirectional communication, enables\nstate-of-the-art reinforcement learning training on GPUs for a wide variety of\nLEGO builds. This offers a flexible and cost-efficient approach for scaling and also\nprovides a robust infrastructure for robot-environment-algorithm communication.\nWe present various experiments across tasks and robot configurations, providing\nbuilt plans and training results. Furthermore, we demonstrate that inexpensive\nLEGO robots can be trained end-to-end in the real world to achieve simple tasks,\nwith training times typically under 120 minutes on a normal laptop. Moreover,\nwe show how users can extend the capabilities, exemplified by the successful\nintegration of non-LEGO sensors. By enhancing accessibility to both robotics and\nreinforcement learning, BricksRL establishes a strong foundation for democratized\nrobotic learning in research and educational settings.\n1\nIntroduction\nAs the field of artificial intelligence continues to evolve, robotics emerges as a fascinating area for\ndeploying and evaluating machine learning algorithms in dynamic, real-life settings [14, 39, 46].\nThese applications allow embodied agents to interact within complex environments, similar to humans\nand animals, they must navigate a variety of challenging constraints during their learning process.\nReinforcement learning (RL), in particular, has emerged as a promising approach to learning complex\nbehavior with robots [20, 29]. Despite the rich potential for innovation, the learning process of\nalgorithms under real-world conditions is a challenge [1, 38, 46]. The complexity of setting up a\nrobotics lab, combined with the high cost of equipment and the steep learning curve in RL, often limits\nthe ability of researchers, educators, and hobbyists to contribute to and benefit from cutting-edge\ndevelopments. To address these challenges, we introduce BricksRL, a comprehensive open-source\nframework designed to democratize access to robotics and RL. BricksRL builds upon Pybricks [44],\na versatile Python package for controlling modular LEGO robotics hub, motors and sensors, actively\nmaintained and supported by a vibrant community, and TorchRL [6], a modern framework for training\n38th Conference on Neural Information Processing Systems (NeurIPS 2024).\narXiv:2406.17490v2  [cs.RO]  2 Dec 2024\nFigure 1: Communication overview of the agent, environment and robot.\nRL agents. This synergy provides an integrated solution that simplifies the creation, modularity,\ndesign, and training of custom robots in the real world.\nThe use of LEGO parts as the basic building blocks for constructing the robots for BricksRL has the\nadvantage of being cheap and widely available, which facilitates entry, but also makes it easier to\nreplace parts in the event of repairs and keeps costs down. In addition, the building blocks allow full\nreusability of all parts, which is not the case with other robots, and no special tools are required for\nconstruction or maintenance, which also keeps costs very low. By abstracting the complexities of\nrobot programming given a gym-like interface and RL algorithm implementation, BricksRL opens\nthe door for a broader audience to engage with robotics research and education, making it more\naccessible to researchers, educators, and hobbyists alike. The low cost, wide availability, and ease of\ndeployment also allow the introduction and use of BricksRL as a new artificial intelligence benchmark\nto test and evaluate RL algorithms in robotics for a variety of tasks and robots.\nThe contributions of our work with BricksRL are threefold. First, we provide a unified platform\nintegrating Pybricks and TorchRL within BricksRL, which facilitates the physical control of cost-\neffective robotic systems and the design of gym-like training environments for RL algorithms. This\nsetup enables scalable training across a diverse array of robots, tasks, and algorithms. Second, we\ndemonstrate how to extend the capabilities of BricksRL beyond the standard sensor set provided\nby LEGO. By integrating a camera sensor, we expand the platform’s capabilities, allowing for the\ncreation of more diverse robots and tasks and thereby broadening its potential applications. Third,\nwe present preliminary results that underscore the framework’s robustness and adaptability for real-\nworld robotics applications. Furthermore, we provide explicit examples of sim2real transfer and the\napplication of datasets with offline RL, demonstrating the practical utility of BricksRL in robotics and\nRL research. We make the source code available at: https://github.com/BricksRL/bricksrl.\nThe building plans, and evaluation videos of the experiments are publicly available at https:\n//bricksrl.github.io/ProjectPage/.\n2\nRelated Work\nHigh acquisition costs, ranging from 10,000$ to over 200,000$, pose significant barriers to robotics\nresearch. This pricing affects various robotics types, including robotic arms (e.g., Franka [31],\nKuka [23] and advanced robotic hands [29, 37, 41], similar to costly quadruped or humanoid robots\ndesigned for locomotion [5, 18, 42].\nThe popularization and increased consumer accessibility of 3D printing and DIY projects have\nheightened interest in low-cost robotics, thereby broadening access and facilitating the entry into\nrobotics [1–3, 7, 9–11, 22, 32], lowering the initial costs for simple quadrupeds starting at 300$\nrobotic arms and hands for 20,000$. However, there is a requisite need for access to a 3D printer, a\nworkshop, and equipment for the construction, maintenance, and repair of these robots. Additionally,\nprojects and companies have been established to cater to the niche of low-cost robotics with pre-built\nrobots for educational purposes that fall within a similar price range [4, 33–35]. Nevertheless, similar\nto off-the-shelf industrial robots, these and DIY robots are static, and it is not assured that printed parts\nor other components can be repurposed for different robots or adapted for new tasks. This limitation\noften confines experiments to a single robot and setting, which can be considered restrictive.\nLEGO parts provide standardized and robust components that facilitate simple reconstruction, modu-\nlarity of designs, and reproducibility. This modularity enables the construction and prototyping of\nvarious robots and the adaptation to different tasks, thereby simplifying the testing and benchmarking\nof RL algorithms across diverse robotic configurations. The initial cost for a starter kit to construct\n2\nrobots starts at approximately 400$ [40], and can be augmented with additional sets, specific elements,\nor sensors as required. [36] demonstrates the application of LEGO for constructing robots for under\n300$. However, using aluminum extrusions and 3D printed components, coupled with control via a\nRaspberry Pi rather than LEGO’s internal PrimeHub, diminishes the system’s flexibility.\nIn contrast, the robots in BricksRL use only LEGO elements for construction and control. The simple\nintegration of additional sensors is demonstrated but not necessary. Further, users interact with the\nrobots via a gym-like environment as is common in RL, simplifying the interaction. In comparison,\nthe industry standard for managing robots and sensors is the Robotics Operating System (ROS) [33].\nIt offers numerous tools, however, its steep learning curve can be a barrier for researchers, students,\nhobbyists, and beginners starting with RL and robotics.\nThe use of LEGO for education in robotics has a rich history through sets such as MINDSTORMS\n[26] or education sets [19]. These are used not only in official educational institutions [25, 26, 43]\nbut also in annual competitions around the world [17, 45] that attract a substantial number of children\nand students. To the best of our knowledge, these competitions do not currently incorporate machine\nlearning techniques such as RL. Being tailored to these groups, BricksRL could bridge that gap and\nprovide easy access to state-of-the-art algorithms.\n3\nBricksRL\nThe underlying architecture of BricksRL has three main components: the agent, the environment, and\nthe robot 1. TorchRL is utilized to develop the agent and the environment, while Pybricks is employed\nfor programming on the robot side. In the following sections, we will examine each component\nindividually and discuss the communication mechanisms between them.\n3.1\nAgents\nBricksRL utilizes TorchRL’s modularity to create RL agents. Modules such as replay buffers,\nobjective functions, networks, and exploration modules from TorchRL are used as building blocks\nto create agents that enable a uniform and clear structure. For our experiments and to showcase the\nintegration of RL algorithms within BricksRL, we have selected three state-of-the-art algorithms\nfor continuous control: TD3, SAC, and DroQ [12, 15, 16]. We primarily chose these off-policy\nalgorithms for their simplicity and their proven ability of sample-efficient training, which is essential\nas we mostly train our robots online. However, due to the flexible and modular structure of TorchRL,\nBricksRL can be easily adapted to include any algorithm developed within or compatible with the\nTorchRL framework, allowing us to emphasize the general applicability of our system rather than the\nspecific strategies. For example, methods commonly used in robotics, such as imitation learning [47]\nand the use of foundation models [28], are available with TorchRL and can be seamlessly integrated\ninto BricksRL.\n3.2\nEnvironment\nBricksRL incorporates TorchRL’s environment design principles among other components, to stan-\ndardize the structure and organization of its environments.\nPybricksHub Class.\nDeveloped by BricksRL, the PybricksHub class plays an important role in\nfacilitating communication with the LEGO Hub, which controls the robots. It achieves this through\nBluetooth Low Energy (BLE) technology, which enables efficient, low-power communication. This\nclass is designed to manage a two-way data exchange protocol, critical for meeting the real-time\nrequirements of interactive applications. Importantly, the PybricksHub class seamlessly bridges\nasynchronous communication with the traditionally synchronous structure of RL environments.\nEnvBase.\nIn BricksRL, environments are designed as classes that inherit from EnvBase provided\nby TorchRL, which is a foundational class for building RL environments. This structure gives access\nto the TorchRL ecosystem and simplifies the creation of environments. Users can create custom\nenvironments or extend existing environments for new tasks. All that needs to be done is to adapt the\nobservation and action specifications and, if necessary, define a new reward function and adapt the\nstep and reset function of the environment.\n3\nA key advantage of using TorchRL’s EnvBase in BricksRL is the ability to apply environment\ntransforms, a fundamental feature of TorchRL. These transforms enable simple manipulation of the\ndata exchanged between the environment and the agent. TorchRL provides a wide range of useful\ntransforms, such as frame stacking, observation normalization, and image transformations, which\nare particularly valuable for real-world robotics. Additionally, the integration of foundation models\nlike VIP [28] through transforms expands the experimentation capabilities within BricksRL. Detailed\ndescriptions of the environments we implemented for our experiments, along with a template for\ncreating custom environments, can be found in A.2 and A.2.8, respectively.\nAgent-Environment Communication.\nFor communication and data exchange between agent\nand environment, BricksRL makes use of TensorDict [6] as a data carrier. TensorDict enables a\nstandardized and adaptable transmission of information between agent and environment. TensorDict\ncan handle a wide range of data as observation by accurately describing the observation specs in the\nenvironment, without modifying the agent’s or environment’s essential structure. It enables users to\nshift between vector and picture observations or a combination of the two. This is a crucial building\ncomponent for being flexible to train different algorithms on robots performing a variety of tasks with\nand without sensors of the LEGO ecosystem.\n3.3\nLEGO Robots\nIn our experiments demonstrating the capabilities of BricksRL, we selected three distinct robot types\nto serve as an introductory platform for RL in robotics. These robots vary in complexity and their\ncapacity for environmental interaction, reflecting a progressive approach to robotic research. Addi-\ntionally, we incorporated various sensors and embraced a range of robot classifications, showcasing a\nbroad spectrum of applications and use cases.\n(a)\n(b)\n(c)\nFigure 2: Three robots that we used in the experiments: (a) 2Wheeler, (b) Walker, (c) RoboArm.\n2Wheeler.\nThe 2Wheeler robot 2a is built by us to represent an elementary robotic platform\ndesigned for introductory purposes, incorporating the LEGO’s Hub, a pair of direct current (DC)\nmotors equipped with rotational sensors, and an ultrasonic sensor for determining the proximity\nto objects. The independent control capability of the DC motors endows the robot with high\nmaneuverability.\nWalker.\nThe Walker 2b, a quadrupedal robot as built in a standard LEGO robotics kit, is equipped\nwith four motors, each integrated with rotational sensors and an additional ultrasonic sensor. In\ncomparison to the 2Wheeler robot 3.3, the Walker variant exhibits more degrees of freedom and\na higher level of complexity due to its increased motor count and the fact that it uses legs instead\nof wheels. In terms of structural design, this robot bears similarity to prevalent quadruped robots\ntypically employed in the domain of locomotion control [39].\nRoboArm.\nThe RoboArm 2c is built by us and is similar to the Walker 4 motors with rotation\nsensors equipped, however, it has a higher range of motion. Further, it is the only static robot and\nincludes another branch of robot types used for tasks like grasping and reaching or manipulating\nobjects [20, 21, 27].\nIn general, Pybricks allows wide access to different motors and sensors as well as IMU (Inertial\nMeasurement Unit) data in the LEGO’s hub, which permits a variety of possible modular robot archi-\ntectures and applications. For a detailed overview, please refer to the official Pybricks documentation\n4\n[30]. Furthermore, we highlight the large community that has already created various robots, which\ncan be rebuilt or used as inspiration. Any robot can be used with BricksRL as long as it is available\nin Pybricks’s interface. We welcome collaborations and encourage community members who want to\nexperiment with BricksRL to contact us for support.\nRobot-Environment Communication.\nBricksRL employs a bidirectional data flow between the\nrobot and the environment, facilitated by MicroPython in Pybricks. The agent’s actions are transmitted\nas byte streams via standard input (stdin), where they are parsed and applied to the robot’s motors. At\nthe same time, the robot sensor data is sent back to the environment through standard output (stdout)\nfor state evaluation and action selection. Each robot uses a dedicated client script to manage its\nmotors, sensors, and control flow for specific tasks. For exact details and an example of a typical\nclient script, see the provided template in A.3.\nCommunication Speed.\nIn robotics and motion control, the rate of communication is crucial,\nnecessitating high frequencies to ensure rapid responsiveness and precision in response to environ-\nmental changes or disturbances. Position or torque-based control systems in quadrupedal robots,\nfor instance, operate within a query frequency range of 20 to 200 Hz [8, 24]. This frequency range\nenables these robots to swiftly adjust to variations in terrain during locomotion.\nLikewise, the hub’s control loop is capable of exceeding frequencies of 1000 Hz, making it suitable\nfor managing complex robotic systems. Yet, when integrating with the BricksRL communication\nframework, a reduction in the system-wide frequency, including agent-environment and environment-\nrobot communications, to 11 Hz was observed. This decrease is primarily due to the overhead\nintroduced by utilizing stdin and stdout for communication. The process of reading from and writing\nto these streams, which requires system calls, is inherently slower compared to direct memory\noperations. Additionally, the necessity to serialize and deserialize data through ’ustruct.unpack’ and\n’ustruct.pack’ adds to this overhead, as it requires converting data between binary formats used in\ncommunication and the Python object representation, which is time-consuming.\nDespite the overhead, BricksRL’s communication speed, while on the lower spectrum, remains within\na reasonable range for robotic system applications. For instance,[13, 39] have demonstrated that\neffective motion control in quadrupedal robots can be achieved at much lower frequencies, such as\n20 or even 8 Hz, indicating that robust and dynamic locomotion can be maintained even at reduced\ncommunication frequencies. Moreover, we show in our experiments that communication frequency\nis task and robot depending, for specific tasks optimal behaviors can be learnt faster with lower\nfrequencies 7.\n3.4\nModularity and Reusability\nThe use of interlocking LEGO parts, and various sensors allows for endless possibilities in designing\nand building robots and robot systems. Additionally, precise construction plans and the reusability\nof components create additional opportunities. Unlike classic robot systems, users are not limited\nto a single design and functionality. Instead, robots can be customized to specific requirements or\ntasks, and can even be reassembled for new challenges or ideas once the initial task is completed\nsuccessfully. Building on this variable foundation with an infinite number of robotic applications,\nBricksRL enables easy interaction by abstracting the complexities of the underlying communication\nprocesses. To train RL algorithms, users can interact with the robots in gym-like environments, which\nprovide a natural and intuitive interface. This enables researchers and hobbyists to train any RL\nalgorithm, such as on-policy or off-policy, model-based or model-free.\nTo further illustrate modularity and scalability of BricksRL we extended the set of sensors and\nshow how easy it is to integrate sensors outside of the LEGO ecosystem. Namely, we integrate a\nUSB webcam camera into an environment (A.2.7) showcased in the experiments, demonstrating\nthat additional sensors can further augment the scope of applications to train robots with RL and\nBricksRL.\n4\nExperiments\nIn our experiments, we aim to address several critical questions regarding the feasibility and efficiency\nof training LEGO robots using RL algorithms in the real world with BricksRL. Thereby taking into\n5\naccount the practical challenges of training LEGO robots such as the lack of millimeter-precise robot\nconstructions, the presence of backlash, and noisy sensors.\nRobot\nEnvironments\n2Wheeler\nRunAway-v0\nSpinning-v0\nWalker\nWalker-v0\nWalkerSim-v0†\nRoboarm\nRoboArm-v0\nRoboArmSim-v0†\nRoboArm-mixed-v0*\nTable 1: Overview of BricksRL Robot and Environment Settings. Environments marked with an\nasterisk (*) utilize LEGO sensors and image inputs as observations for the agent. Environments\nindicated by a dagger (†) denote simulations of the real robot and do not use the real robot for training.\nTherefore we developed various task-specific environments to demonstrate the adaptability and ease\nof use of BricksRL, highlighting the scalability of training across different algorithms and robots\nwith diverse sensor arrays. Tasks ranging from driving and controlling the 2Wheeler to learning to\nwalk with the Walker and reaching tasks for the RoboArm demonstrate the applicability of BricksRL.\nTable 1 shows a complete overview of all environments used in our experiments.\nIn our experiments, we primarily focus on online learning, where the robot directly interacts with\nthe real world, encompassing the challenges inherent to this approach. However, we have also\ndeveloped simulation environments for certain tasks. Training in these simulations is significantly\nfaster compared to real-world training, as confirmed by our comparative experiments. Additionally,\nwe use these simulation environments to demonstrate the sim2real capabilities of LEGO robots with\nBricksRL.\nA complete overview and description of the environments implemented including action and observa-\ntion specifications as well as the definition of the reward function can be found in the appendix A.2.\nWe also provide an environment template 1 that demonstrates the straightforward process of creating\ncustom environments using BricksRL.\nIn all of our experiments, we initiated the training process with 10 episodes of random actions to\npopulate the replay buffer. The results are obtained by over 5 seeds for each algorithm and compared\nagainst a random policy. Evaluation scores of the trained policies are displayed in 2. We further\nprovide videos of trained policies for each robot and task A.1. Hyperparameter optimization was not\nconducted for any of the algorithms, and we adhered to default settings. Comprehensive information\non the hyperparameter is provided in the appendix 12. Although the option to utilize environment\ntransformations, such as frame stacking and action repetition, was available, we opted not to use these\nfeatures to maintain the simplicity of our setup. Further details are available in the appendix A.2.\nEnvironment\nAlgorithm\nRunAway-v0\nSpinning-v0\nWalker-v0\nWalkerSim-v0\nRoboArm-v0\nRoboArmSim-v0\nRoboArm-mixed-v0\nTD3\n7.64 ± 2.31\n7558.21 ± 28.61\n−62.94 ± 16.76\n−78.49 ± 9.75\n−20.29 ± 31.35\n−12.78 ± 21.09\n−60.24 ± 16.06\nSAC\n8.72 ± 2.82\n7407.20 ± 109.53\n−52.04 ± 8.79\n−55.03 ± 4.95\n−27.77 ± 37.13\n−3.45 ± 2.66\n−18.21 ± 6.98\nDroQ\n8.96 ± 0.87\n7456.85 ± 18.02\n−57.63 ± 10.44\n−56.62 ± 2.81\n−55.02 ± 62.45\n−14.04 ± 26.05\n−19.39 ± 11.07\nRandom\n−0.51 ± 1.84\n71.97 ± 501.79\n−191.99 ± 18.19\n−191.99 ± 18.19\n−149.26 ± 88.19\n−149.26 ± 88.19\n−57.23 ± 10.01\nTable 2: The table displays the mean and standard deviation of evaluation rewards for the trained TD3,\nSAC, DroQ algorithms, and a random policy, based on experiments conducted across 5 evaluation\nepisodes and 5 different seeds.\n4.1\n2Wheeler\nIn the RunAway-v0 task for the 2Wheeler robot, we trained RL algorithms over 40 episodes. Training\nsessions were completed in approximately 15 minutes per run for each agent. All algorithms\nsuccessfully mastered the task, as shown in Figure 3. Notably, despite the simplicity of the task,\nalgorithms adopted unique strategies. TD3 maximized its actions, achieving the highest distance\nfrom the wall but causing rapid acceleration, tilting, and noisy measurements, leading to occasional\n6\nabrupt episode termination. In contrast, the DroQ agent used smaller actions, resulting in more stable\nbut shorter distances and avoiding premature episode endings A.5.\nFor the Spinning-v0 task, we trained the agents over 15 episodes. The training was completed in\nabout 10 minutes, with all agents effectively learning to solve the task, as illustrated in Figure 3.\nTable 2 includes the evaluation scores for both tasks of the 2Wheeler.\nFigure 3: Training results for 2Wheeler robot for the RunAway-v0 and the Spinning-v0 environ-\nment.\n4.2\nWalker\nIn the Walker-v0 environment, we trained the Walker robot over 75 episodes. The entire training\nprocess took approximately 70 minutes. All agents successfully developed a forward-moving gait.\nRemarkably, the DroQ algorithm achieved this in significantly fewer episodes (5-10), requiring only\nabout 15 minutes of training, as illustrated in Figure 4. We trained the agents with a communication\nfrequency of 2 Hz, instead of the maximum frequency of 11 Hz, as we observed that training at\nthe lower frequency was faster and more stable. Results of a direct comparison can be found in the\nappendix 7.\nFigure 4 also presents results from training in the WalkerSim-v0 environment. To be comparable\nwith the WalkerSim-v0, we similarly trained the agents for 75 episodes but noted marked differences\nin training duration: TD3 and SAC completed within 1-3 minutes, whereas DroQ required about 20\nminutes due to a higher updating ratio. Simulation results revealed higher training performance with\nsmoother and more stable learning curves compared to real-world training 4.\nThe final evaluation scores for policies trained in both real-world and simulation environments, tested\non the actual robot, are summarized in Table 2. Although the simulation-trained policies slightly\nunderperformed, they demonstrated effective sim2real transfer, highlighting their efficiency with\nconsiderably less training time and reduced supervision.\nSuccess Rate (%)\nAlgorithm\nRoboArm-v0\nRoboArm-mixed-v0\nTD3\n88\n8\nSAC\n72\n68\nDroQ\n64\n68\nRandom\n32\n40\nTD3*\n88\n-\nSAC*\n100\n-\nDroQ*\n88\n-\nTable 3:\nComparison of success rates for different agents in the RoboArm-v0 and\nRoboArm-mixed-v0 environments. Success is defined as the agent reaching the goal or goal posi-\ntion within a specified threshold. Agents marked with an asterisk (*) were initially trained in the\nRoboArmSim-v0 environment. Each algorithm was evaluated for 5 epochs with 5 different seeds,\ntotaling 25 experiments per agent and task.\n7\nFigure 4: Training performance for Walker robot for the Walker-v0 and the WalkerSim-v0 envi-\nronment.\n4.3\nRoboArm\nIn the RoboArm-v0 task, agents were trained for 250 episodes. Training durations varied, with TD3\nand SAC completing in about 1 hour on the real robot, whereas DroQ required close to 2 hours. By\ncontrast, training in the RoboArmSim-v0 environment proved much quicker: TD3 and SAC finished\nwithin 1-2 minutes, and DroQ in approximately 25 minutes. The outcomes, depicted in Figure 5,\nconfirm that all agents successfully learned effective policies.\nTo enhance the interpretation of the training outcomes, we also plotted the final error—defined as\nthe deviation from the target angles at the last step of each episode—and the total number of steps\ntaken per episode. The data reveals a consistent decrease in both the final error and the number of\nsteps throughout the training period. This indicates not only improved accuracy but also increased\nefficiency, as episodes terminated sooner when goals were successfully met.\nThe evaluation results are detailed in Table 2. Additionally, we compiled success rates that illustrate\nhow often each agent reached the goal position within the predefined threshold. These success rates,\nderived from evaluation runs across 5 seeds with each seed running 5 episodes, are presented in Table\n3. Notably, the policies trained in the RoboArmSim-v0 environment achieved superior evaluation\nscores and also higher success rates upon testing. This demonstrates a successful sim2real transfer,\nachieving a significantly reduced training time.\nLastly, we present the training results for the RoboArm_mixed-v0 environment, where the algorithms\nunderwent training over 250 episodes. Training durations varied significantly due to the complexity\nadded by integrating additional image observations: SAC was completed in 40 minutes, TD3 in 60\nminutes, and DroQ took three hours. The inclusion of image data likely introduced considerable\nnoise in the training results, as illustrated in Figure 6, which displays the rewards achieved by the\nagents and the corresponding episode steps. Interestingly, while SAC and DroQ successfully learned\neffective policies, TD3 struggled to adapt, failing to develop a viable strategy. The success of SAC\nand DroQ is evident in the chart of episode steps, showing a decrease in steps over the training period,\nwhich indicates a more efficient achievement of the goal position.\nThe evaluation results, detailed in Table 2, confirm the performances. Notably, out of 25 evaluation\ntrials, both SAC and DroQ successfully reached the goal position 17 times, as recorded in Table\n3. This demonstrates the robustness of the SAC and DroQ algorithms in handling the complexities\nintroduced in the RoboArm_mixed-v0 environment.\n4.4\nOffline Training\nOffline RL uses pre-collected datasets to train algorithms efficiently, avoiding real-world interactions\nand complex simulations. To further highlight the capabilities of BricksRL for education and\nresearch in robotics and RL we collected offline datasets for the LEGO robots. With those datasets,\nBricksRL allows training of the LEGO robots via offline RL or imitation learning, both of which are\nstate-of-the-art methods for RL in robotics.\n8\nFigure 5: Training outcomes for the RoboArm robot in both the RoboArm-v0 and RoboArmSim-v0\nenvironments. The plot also includes the final error at the epoch’s last step and the total number of\nepisode steps.\nFigure 6: Training performance of the RoboArm robot in the RoboArm_mixed-v0 environment,\nshowing both the reward and the number of episode steps required to reach the target location.\nFor BricksRL, we curated datasets for three robot configurations: 2Wheeler, Walker, and RoboArm.\nThese datasets include both expert and random data for four tasks in our experiments (Walker-v0,\nRoboArm-v0, RunAway-v0, Spinning-v0). Details about the datasets and dataset generation can be\nfound in the appendix A.7. Using these datasets, we demonstrated that offline RL with BricksRL\nis feasible, successfully training both online and offline RL algorithms and applying them to a real\nrobot. The evaluation performance, shown in Table 4, highlights the superior performance of offline\nRL algorithms, particularly with expert data, while online algorithms struggle, suggesting overfitting\nor poor generalization. For further details on training parameters, please refer to the appendix A.8.\nWalker-v0\nRoboArm-v0\nRunAway-v0\nSpinning-v0\nAgent\nRandom\nExpert\nRandom\nExpert\nRandom\nExpert\nRandom\nExpert\nTD3\n−79.71\n−153.45\n−124.22\n−201.94\n19.86\n9.06\n6160.25\n6168.28\nSAC\n−66.91\n−255.41\n−54.67\n−218.76\n14.74\n10.80\n5416.11\n9349.52\nBC\n−202.46\n−85.65\n−117.72\n−7.34\n−0.27\n18.13\n35.55\n9150.02\nIQL\n−136.13\n−74.80\n−76.89\n−3.10\n14.07\n18.80\n4544.28\n9096.60\nCQL\n−72.75\n−77.93\n−46.91\n−17.41\n19.60\n19.74\n4509.31\n9099.03\nTable 4: Evaluation Results: Online (TD3, SAC) and Offline (BC, IQL, CQL) RL Algorithms. Scores\nrepresent the mean reward averaged over 5 episodes and 5 random seeds.\n9\n5\nConclusion\nIn this paper, we introduce BricksRL and detail its benefits for robotics, RL, and educational\napplications, emphasizing its cost-effectiveness, reusability, and accessibility. In addition, we\nshowcased its practical utility by deploying three distinct robots, performing various tasks with\na range of sensors, across more than 100 experiments. Our results underscore the viability of\nintegrating state-of-the-art RL methodologies through BricksRL within research and educational\ncontexts. By providing comprehensive building plans and facilitating access to BricksRL, we aim to\nestablish this investigation as a foundational proof of concept for utilizing LEGO-based robots to train\nRL algorithms. Moving forward, avenues for further research include creating more complex robots\nand tasks, exploring applications in multi-agent settings, and leveraging large datasets to enhance\nRL training through transformer-based imitation learning. Ultimately, BricksRL sets the stage for\na future where accessible, reusable robotic systems support and expand RL research, collaborative\nlearning, and interactive education.\nAcknowledgements\nWe thank A. De Fabritiis Campos, M. De Fabritiis Campos and P. Vallecillos Cusco for providing\ntheir LEGOs to this project.\nReferences\n[1]\nMichael Ahn, Henry Zhu, Kristian Hartikainen, Hugo Ponte, Abhishek Gupta, Sergey Levine,\nand Vikash Kumar. “ROBEL: Robotics Benchmarks for Learning with Low-Cost Robots”. en.\nIn: Proceedings of the Conference on Robot Learning. ISSN: 2640-3498. PMLR, May 2020,\npp. 1300–1313. URL: https://proceedings.mlr.press/v100/ahn20a.html (visited\non 04/09/2024).\n[2]\nJorge Aldaco, Travis Armstrong, Robert Baruch, Jeff Bingham, Sanky Chan, Debidatta\nDwibedi, Chelsea Finn, Pete Florence, Spencer Goodrich, Wayne Gramlich, Alexander Herzog,\nJonathan Hoech, Thinh Nguyen, Ian Storz, Baruch Tabanpour, Jonathan Tompson, Ayzaan\nWahid, Ted Wahrburg, Sichun Xu, Sergey Yaroshenko, and Tony Z Zhao. “ALOHA 2: An\nEnhanced Low-Cost Hardware for Bimanual Teleoperation”. en. In: ().\n[3]\nRafia Bindu, Sazid Alam, and Asif Neloy. “A Cost-Efficient Multipurpose Service Robot using\nRaspberry Pi and 6 DOF Robotic Arm”. In: Mar. 2019, pp. 16–22. DOI: 10.1145/3325693.\n3325701.\n[4]\nRinu Boney, Jussi Sainio, Mikko Kaivola, Arno Solin, and Juho Kannala. RealAnt: An Open-\nSource Low-Cost Quadruped for Education and Research in Real-World Reinforcement Learn-\ning. arXiv:2011.03085 [cs]. June 2022. DOI: 10.48550/arXiv.2011.03085. URL: http:\n//arxiv.org/abs/2011.03085 (visited on 04/09/2024).\n[5]\nBoston Dynamics’ Spot Robot Dog Now Available for $74,500 - IEEE Spectrum. en. URL:\nhttps://spectrum.ieee.org/boston-dynamics-spot-robot-dog-now-available\n(visited on 05/01/2024).\n[6]\nAlbert Bou, Matteo Bettini, Sebastian Dittert, Vikash Kumar, Shagun Sodhani, Xiaomeng\nYang, Gianni De Fabritiis, and Vincent Moens. TorchRL: A data-driven decision-making\nlibrary for PyTorch. arXiv:2306.00577 [cs]. Nov. 2023. DOI: 10.48550/arXiv.2306.00577.\nURL: http://arxiv.org/abs/2306.00577 (visited on 05/01/2024).\n[7]\nKen Caluwaerts, Atil Iscen, J. Chase Kew, Wenhao Yu, Tingnan Zhang, Daniel Freeman,\nKuang-Huei Lee, Lisa Lee, Stefano Saliceti, Vincent Zhuang, Nathan Batchelor, Steven Bohez,\nFederico Casarini, Jose Enrique Chen, Omar Cortes, Erwin Coumans, Adil Dostmohamed,\nGabriel Dulac-Arnold, Alejandro Escontrela, Erik Frey, Roland Hafner, Deepali Jain, Bauyrjan\nJyenis, Yuheng Kuang, Edward Lee, Linda Luu, Ofir Nachum, Ken Oslund, Jason Powell,\nDiego Reyes, Francesco Romano, Feresteh Sadeghi, Ron Sloat, Baruch Tabanpour, Daniel\nZheng, Michael Neunert, Raia Hadsell, Nicolas Heess, Francesco Nori, Jeff Seto, Carolina\nParada, Vikas Sindhwani, Vincent Vanhoucke, and Jie Tan. Barkour: Benchmarking Animal-\nlevel Agility with Quadruped Robots. en. arXiv:2305.14654 [cs]. May 2023. URL: http:\n//arxiv.org/abs/2305.14654 (visited on 05/13/2024).\n10\n[8]\nShuxiao Chen, Bike Zhang, Mark W. Mueller, Akshara Rai, and Koushil Sreenath. Learning\nTorque Control for Quadrupedal Locomotion. arXiv:2203.05194 [cs, eess]. Mar. 2023. URL:\nhttp://arxiv.org/abs/2203.05194 (visited on 03/06/2024).\n[9]\nMarc Peter Deisenroth and Carl Edward Rasmussen. “PILCO: A Model-Based and Data-\nEfficient Approach to Policy Search”. en. In: (), p. 8.\n[10]\nHongjie Fang, Hao-Shu Fang, Yiming Wang, Jieji Ren, Jingjing Chen, Ruo Zhang, Weiming\nWang, and Cewu Lu. Low-Cost Exoskeletons for Learning Whole-Arm Manipulation in the\nWild. arXiv:2309.14975 [cs]. Sept. 2023. URL: http://arxiv.org/abs/2309.14975\n(visited on 04/22/2024).\n[11]\nZipeng Fu, Tony Z Zhao, and Chelsea Finn. “Learning Bimanual Mobile Manipulation with\nLow-Cost Whole-Body Teleoperation”. en. In: ().\n[12]\nScott Fujimoto, Herke van Hoof, and David Meger. Addressing Function Approximation Error\nin Actor-Critic Methods. Number: arXiv:1802.09477 arXiv:1802.09477 [cs, stat]. Oct. 2018.\nURL: http://arxiv.org/abs/1802.09477 (visited on 06/20/2022).\n[13]\nSiddhant Gangapurwala, Luigi Campanaro, and Ioannis Havoutis. Learning Low-Frequency\nMotion Control for Robust and Dynamic Robot Locomotion. arXiv:2209.14887 [cs]. Feb.\n2023. DOI: 10.48550/arXiv.2209.14887. URL: http://arxiv.org/abs/2209.14887\n(visited on 03/06/2024).\n[14]\nTuomas Haarnoja, Ben Moran, Guy Lever, Sandy H. Huang, Dhruva Tirumala, Markus\nWulfmeier, Jan Humplik, Saran Tunyasuvunakool, Noah Y. Siegel, Roland Hafner, Michael\nBloesch, Kristian Hartikainen, Arunkumar Byravan, Leonard Hasenclever, Yuval Tassa,\nFereshteh Sadeghi, Nathan Batchelor, Federico Casarini, Stefano Saliceti, Charles Game,\nNeil Sreendra, Kushal Patel, Marlon Gwira, Andrea Huber, Nicole Hurley, Francesco Nori,\nRaia Hadsell, and Nicolas Heess. Learning Agile Soccer Skills for a Bipedal Robot with Deep\nReinforcement Learning. arXiv:2304.13653 [cs]. Apr. 2023. URL: http://arxiv.org/abs/\n2304.13653 (visited on 03/04/2024).\n[15]\nTuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan,\nVikash Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, and Sergey Levine. Soft Actor-\nCritic Algorithms and Applications. arXiv:1812.05905 [cs, stat]. Jan. 2019. URL: http :\n//arxiv.org/abs/1812.05905 (visited on 03/06/2024).\n[16]\nTakuya Hiraoka, Takahisa Imagawa, Taisei Hashimoto, Takashi Onishi, and Yoshimasa Tsu-\nruoka. Dropout Q-Functions for Doubly Efficient Reinforcement Learning. arXiv:2110.02034\n[cs]. Mar. 2022. URL: http://arxiv.org/abs/2110.02034 (visited on 03/16/2024).\n[17]\nHome Page | FIRST LEGO League. en. URL: https://www.firstlegoleague.org/\n(visited on 04/30/2024).\n[18]\nMarco Hutter, Christian Gehring, Dominic Jud, Andreas Lauber, C. Dario Bellicoso, Vassilios\nTsounis, Jemin Hwangbo, Karen Bodie, Peter Fankhauser, Michael Bloesch, Remo Diethelm,\nSamuel Bachmann, Amir Melzer, and Mark Hoepflinger. “ANYmal - a highly mobile and\ndynamic quadrupedal robot”. In: 2016 IEEE/RSJ International Conference on Intelligent\nRobots and Systems (IROS). ISSN: 2153-0866. Oct. 2016, pp. 38–44. DOI: 10.1109/IROS.\n2016.7758092. URL: https://ieeexplore.ieee.org/document/7758092 (visited on\n05/01/2024).\n[19]\nJuguetes de LEGO® Education | Oficial LEGO® Shop ES. es. URL: https://www.lego.\ncom/es-es/themes/lego-education (visited on 05/13/2024).\n[20]\nDmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog, Eric Jang,\nDeirdre Quillen, Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, and Sergey Levine.\nQT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation.\narXiv:1806.10293 [cs, stat]. Nov. 2018. DOI: 10.48550/arXiv.1806.10293. URL: http:\n//arxiv.org/abs/1806.10293 (visited on 03/04/2024).\n[21]\nDmitry Kalashnikov, Jacob Varley, Yevgen Chebotar, Benjamin Swanson, Rico Jonschkowski,\nChelsea Finn, Sergey Levine, and Karol Hausman. MT-Opt: Continuous Multi-Task Robotic\nReinforcement Learning at Scale. arXiv:2104.08212 [cs]. Apr. 2021. DOI: 10.48550/arXiv.\n2104.08212. URL: http://arxiv.org/abs/2104.08212 (visited on 03/04/2024).\n[22]\nNathan Kau. Stanford Pupper: A Low-Cost Agile Quadruped Robot for Benchmarking and\nEducation. en. arXiv:2110.00736 [cs]. Feb. 2022. URL: http://arxiv.org/abs/2110.\n00736 (visited on 04/30/2024).\n11\n[23]\nLBR iiwa. en-DE. URL: https://www.kuka.com/en-de/products/robot-systems/\nindustrial-robots/lbr-iiwa (visited on 05/01/2024).\n[24]\nJoonho Lee, Jemin Hwangbo, Lorenz Wellhausen, Vladlen Koltun, and Marco Hutter. “Learn-\ning Quadrupedal Locomotion over Challenging Terrain”. In: Science Robotics 5.47 (Oct. 2020).\narXiv:2010.11251 [cs, eess], eabc5986. ISSN: 2470-9476. DOI: 10.1126/scirobotics.\nabc5986. URL: http://arxiv.org/abs/2010.11251 (visited on 03/06/2024).\n[25]\nLego Robotics - Wagner College Lifelong Learning. en-US. URL: https://wagner.edu/\nlifelong-learning/robotics/ (visited on 05/13/2024).\n[26]\nLego Robotics Competition - Fort Hays State University (FHSU). en. URL: https://www.\nfhsu.edu/smei/lego-robotics/ (visited on 05/13/2024).\n[27]\nSergey Levine, Peter Pastor, Alex Krizhevsky, and Deirdre Quillen. Learning Hand-Eye\nCoordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection.\narXiv:1603.02199 [cs]. Aug. 2016. DOI: 10.48550/arXiv.1603.02199. URL: http:\n//arxiv.org/abs/1603.02199 (visited on 03/04/2024).\n[28]\nJason Yecheng Ma, Shagun Sodhani, Dinesh Jayaraman, Osbert Bastani, Vikash Kumar, and\nAmy Zhang. “VIP: TOWARDS UNIVERSAL VISUAL REWARD AND REPRESENTATION\nVIA VALUE-IMPLICIT PRE- TRAINING”. en. In: ().\n[29]\nOpenAI, Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob McGrew,\nArthur Petron, Alex Paino, Matthias Plappert, Glenn Powell, Raphael Ribas, Jonas Schneider,\nNikolas Tezak, Jerry Tworek, Peter Welinder, Lilian Weng, Qiming Yuan, Wojciech Zaremba,\nand Lei Zhang. Solving Rubik’s Cube with a Robot Hand. arXiv:1910.07113 [cs, stat]. Oct.\n2019. DOI: 10.48550/arXiv.1910.07113. URL: http://arxiv.org/abs/1910.07113\n(visited on 03/04/2024).\n[30]\nPybricks Documentation — pybricks v3.5.0 documentation. URL: https://docs.pybricks.\ncom/en/stable/ (visited on 04/30/2024).\n[31]\nResearch. en. URL: https://franka.de/research (visited on 05/01/2024).\n[32]\nPaulo Rezeck, Hector Azpurua, Mauricio FS Correa, and Luiz Chaimowicz. HeRo 2.0: A\nLow-Cost Robot for Swarm Robotics Research. arXiv:2202.12391 [cs]. Mar. 2022. DOI:\n10.48550/arXiv.2202.12391. URL: http://arxiv.org/abs/2202.12391 (visited on\n03/04/2024).\n[33]\nRobot Kits & STEM Toys for K-12 Schools and Home Education|Makeblock. en. URL: https:\n//www.makeblock.com/ (visited on 05/13/2024).\n[34]\nRobotik in Schulen - die Robotik für Bildung 4.0. de-DE. URL: https://variobotic.de/\nprodukt-kategorie/robotik-in-schulen/ (visited on 05/13/2024).\n[35]\nRobotis | The study of humanexperiences. en. URL: https://en.robotis.com (visited on\n05/13/2024).\n[36]\nLogan Saar, Haotong Liang, Alex Wang, Austin McDannald, Efrain Rodriguez, and A Gi-\nlad Kusne. “A Low-Cost Robot Science Kit for Education with Symbolic Regression for\nHypothesis Discovery and Validation”. en. In: ().\n[37]\nShadow Dexterous Hand Series - Research and Development Tool. en. Running Time: 66.\nSept. 2023. URL: https://www.shadowrobot.com/dexterous-hand-series/ (visited\non 05/01/2024).\n[38]\nLaura Smith, Yunhao Cao, and Sergey Levine. Grow Your Limits: Continuous Improvement\nwith Real-World RL for Robotic Locomotion. arXiv:2310.17634 [cs]. Oct. 2023. DOI: 10.\n48550/arXiv.2310.17634. URL: http://arxiv.org/abs/2310.17634 (visited on\n03/04/2024).\n[39]\nLaura Smith, Ilya Kostrikov, and Sergey Levine. A Walk in the Park: Learning to Walk in 20\nMinutes With Model-Free Reinforcement Learning. arXiv:2208.07860 [cs]. Aug. 2022. DOI:\n10.48550/arXiv.2208.07860. URL: http://arxiv.org/abs/2208.07860 (visited on\n03/04/2024).\n[40]\nSTEM & STEAM Solutions for the Classroom. en-us. URL: https://education.lego.\ncom/en-us/ (visited on 05/13/2024).\n[41]\nThe Dexterous Hands - INSPIRE. en-US. URL: https://en.inspire- robots.com/\nproduct-category/the-dexterous-hands (visited on 05/01/2024).\n[42]\nUNITREE Robotics® SHOP | Official Unitree Robot Dogs A1 Go1. en-US. URL: https:\n//unitreerobotics.net/ (visited on 05/01/2024).\n12\n[43]\nCarnegie Mellon University. LEGO Curriculum - Carnegie Mellon Robotics Academy -\nCarnegie Mellon University. en. URL: https : / / www . cmu . edu / roboticsacademy /\nroboticscurriculum/Lego%20Curriculum/index.html (visited on 05/13/2024).\n[44]\nLaurens Valk. Pybricks. en. URL: https://pybricks.com/ (visited on 04/30/2024).\n[45]\nWRO RoboMission - LEGO Roboter lösen Aufgaben auf WRO Parcours. URL: https://\nwww.worldrobotolympiad.de/world- robot- olympiad/robomission (visited on\n05/09/2024).\n[46]\nPhilipp Wu, Alejandro Escontrela, Danijar Hafner, Ken Goldberg, and Pieter Abbeel. Day-\nDreamer: World Models for Physical Robot Learning. en. arXiv:2206.14176 [cs]. June 2022.\nURL: http://arxiv.org/abs/2206.14176 (visited on 04/26/2024).\n[47]\nMaryam Zare, Parham M. Kebria, Abbas Khosravi, and Saeid Nahavandi. A Survey of Imitation\nLearning: Algorithms, Recent Developments, and Challenges. arXiv:2309.02473 [cs, stat]. Sept.\n2023. DOI: 10.48550/arXiv.2309.02473. URL: http://arxiv.org/abs/2309.02473\n(visited on 05/22/2024).\nA\nAppendix\nA.1\nRepository and Website\nBricksRL repository and our project website with evaluation videos and building instructions can be\nfound at the following locations:\n• GitHub: BricksRL\n• Project Website\nA.2\nEnvironments\nA.2.1\nRunAway-v0\nThe RunAway-v0 environment presents a straightforward task designed for the 2Wheeler robot. The\nobjective is to maximize the distance measured by the robot’s ultrasonic sensor. To accomplish this,\nthe agent controls the motor angles, deciding how far to move forward or backward. This environment\noperates within a continuous action space, and each episode spans a maximum of 20 steps. Episodes\nwill terminate early if the robot reaches the maximum distance of 2000 mm.\nAction and Observation Specifications.\nTorchRL’s BoundedTensorSpecs are employed to define\nthe action and observation specifications, which have 1 and 5 dimensions, respectively. Table 5\noutlines the specific ranges. Actions are initially defined in the range of [−1, 1] but are linearly\nmapped to the range of [−100, 100] before being applied to both the left and right wheel motors.\nType\nNum\nSpecification\nMin\nMax\nAction Spec\n0\nmotor\n-1\n1\nObservation Spec\n0\nleft motor angle\n0.0\n360.0\n1\nright motor angle\n0.0\n360.0\n2\npitch angle\n-90\n90\n3\nroll angle\n-90\n90\n4\ndistance\n0.0\n2000.0\nTable 5: Combined action and observation specifications for the RunAway-v0 environment.\nReward Function.\nThe reward function for the RunAway-v0 environment is defined as:\nRt =\n\n\n\n+1\nif distancet > distancet−1\n−1\nif distancet < distancet−1\n0\nelse\n(1)\n13\nA.2.2\nSpinning-v0\nThe Spinning-v0 environment is another setup designed for the 2Wheeler robot.\nUnlike\nRunAway-v0, this environment does not use the ultrasonic sensor to measure the robot’s distance to\nobjects in front of it. Instead, at each reset, a value is randomly selected from the discrete set 0, 1,\nwhich explicitly dictates the rotational direction in which the robot should spin. The robot’s IMU\n(Inertial Measurement Unit) enables the tracking of various parameters, including angular velocity.\nThis angular velocity is part of the agent’s observation in the Spinning-v0 environment, providing it\nwith information on its rotational direction to complete the task. The action space is also continuous,\nand each episode has a length of 50 steps.\nAction and Observation Specifications.\nThe BoundedTensorSpec for the observation in the\nSpinning-v0 environment comprises six floating-point values: the left and right motor angles, the\npitch and roll angles, the angular velocity ωz, and the rotational direction. Table 6 details these\nspecifications. The action specification is defined as two floating-point values representing the rotation\nangles applied to the left and right motors. Actions are initially defined in the range of [−1, 1] but will\nbe transformed to the range of [−100, 100] before being applied to the motors, as detailed in Table 6.\nType\nNum\nSpecification\nMin\nMax\nAction Spec\n0\nleft motor\n-1\n1\n1\nright motor\n-1\n1\nObservation Spec\n0\nleft motor angle\n0.0\n360.0\n1\nright motor angle\n0.0\n360.0\n2\npitch angle\n-90\n90\n3\nroll angle\n-90\n90\n4\nangular velocity ωz\n-100\n100\n5\ndirection\n0\n1\nTable 6: Combined action and observation specifications for the Spinning-v0 environment.\nReward Function.\nThe reward function for the Spinning-v0 environment is defined in 1. The\nangular velocity ωz is directly used as a reward signal and encourages adherence to a predefined\nrotational orientation.\nRt =\n\u001aωz,t\nif rotational direction = 0 (spinning left),\n−ωz,t\notherwise (spinning right).\n(2)\nA.2.3\nWalker-v0\nIn the Walker-v0 environment for the Walker robot, the objective is to master forward movement\nusing its four legs. To achieve this, the robot is provided with data on the current angles of each\nleg’s motors, along with IMU readings that include both pitch and roll angles, ensuring operational\nsafety. Additionally, an ultrasonic sensor is used to momentarily halt the agent’s actions when the\ndetected distance falls below a predefined threshold, preventing collisions with obstacles. Each\nepisode consists of 100 steps. To achieve reduced communication speed, a waiting time is added after\nthe actions are applied to the motors.\nAction and Observation Specifications.\nIn the Walker-v0 environment, the observation specifi-\ncation consists of seven floating-point values: four motor angles (one for each leg), the pitch and roll\nangles, and the distance measurements from the ultrasonic sensor. The action specification includes\nfour floating-point values corresponding to the four leg motors. These action values are initially\ndefined in the range of [−1, 1] but are linearly mapped to the range of [−100, 0] before being applied,\nas detailed in Table 7.\nReward Function.\nThe total reward is a sum of penalties for the actions and differences in angles,\nencouraging synchronized movement and appropriate angular differences between the legs of the\n14\nType\nNum\nSpecification\nMin\nMax\nAction Spec\n0\nleft front motor\n-1\n1\n1\nright front motor\n-1\n1\n2\nleft back motor\n-1\n1\n3\nright back motor\n-1\n1\nObservation Spec\n0\nleft front motor angle\n0.0\n360.0\n1\nright front motor angle\n0.0\n360.0\n2\nleft back motor angle\n0.0\n360.0\n3\nright back motor angle\n0.0\n360.0\n4\npitch angle\n-90\n90\n5\nroll angle\n-90\n90\n6\ndistance\n0.0\n2000.0\nTable 7: Combined action and observation specifications for the Walker-v0 environment.\nwalker. The reward components are defined as follows: - Raction,t: Penalty for the magnitude of\nactions taken. - Rlf-rb,t: Penalty for the angular difference between the left front (lf) and right back\n(rb) motor angles. - Rrf-lb,t: Penalty for the angular difference between the right front (rf) and left\nback (lb) motor angles. - Rlf-rf,t: Penalty for the deviation from 180 degrees between the left front\n(lf) and right front (rf) motor angles. - Rlb-rb,t: Penalty for the deviation from 180 degrees between\nthe left back (lb) and right back (rb) motor angles.\nRt = Raction,t + Rlf-rb,t + Rrf-lb,t + Rlf-rf,t + Rlb-rb,t\n(3)\nRaction,t = −\nP actionst\n40\n(4)\nRlf-rb,t = −angular_difference(θlf,t, θrb,t)\n180\n(5)\nRrf-lb,t = −angular_difference(θrf,t, θlb,t)\n180\n(6)\nRlf-rf,t = −180 −angular_difference(θlf,t, θrf,t)\n180\n(7)\nRlb-rb,t = −180 −angular_difference(θlb,t, θrb,t)\n180\n(8)\nwith the angular difference defined as:\nangular_difference(θ1,t, θ2,t) = |((θ2,t −θ1,t + 180)\nmod 360) −180|\n(9)\nA.2.4\nWalkerSim-v0\nAdditionally, we have developed a simulated version of the Walker-v0 environment, called\nWalkerSim-v0. This simulation mirrors the real-world setup without requiring communication\nwith the actual robot or using PyBricks. In the simulation, both IMU measurements and ultrasonic\nsensor inputs, which are used in the Walker-v0 environment, are set to zero. This simplification\nis made because modeling or simulating these sensors can be challenging due to their complexity\nand the nuances involved in accurately replicating their readings. However, since these sensors are\nprimarily used for safety in the real world, their absence is not a concern in the simulated environment.\nIn WalkerSim-v0, the next motor states are calculated by simulating the transition dynamics. This\ninvolves transforming the action output into the angle range and then applying the corresponding\nactions by adding to the current motor state. To model real-world inaccuracies, we add noise to the\nmotor states, sampled from a Gaussian distribution with a mean of 0 and a standard deviation of 0.1.\nSimilar to the real-world environment the WalkerSim-v0 episodes consist of 100 interactions.\n15\nAction and Observation Specifications.\nAction and observation specifications are the same as in\nthe Walker-v0 7.\nReward Function.\nThe reward function for the WalkerSim-v0 is the same as in the Walker-v0\nenvironment.\nA.2.5\nRoboArm-v0\nThe RoboArm-v0 is a pose-reaching task. At every reset, random goal angles for the four motors\nare sampled, defining a target pose. The objective is to adjust the robot’s articulation to reach the\nspecified goal pose within 100 steps. To achieve this the robot is provided with the current motor\nangles of all joints. The design of this environment permits the user to choose whether the robot\ntackles the task with dense or sparse rewards, effectively adjusting the difficulty level.\nAction and Observation Specifications.\nThe observation specification for the RoboArm-v0 en-\nvironment consists of eight floating-point values: four current motor angles and four goal motor\nangles, as detailed in Table 8. The action specification is defined by four floating-point values for\nthe four motors, initially in the range of [−1, 1]. Before applying the specific actions to each motor,\nthese values are transformed as follows: the rotation motor actions are linearly mapped to the range\n[−100, 100], the low motor actions to [−30, 30], the high motor actions to [−60, 60], and the grab\nmotor actions to [−25, 25].\nType\nNum\nSpecification\nMin\nMax\nAction Spec\n0\nrotation motor\n-1\n1\n1\nlow motor\n-1\n1\n2\nhigh motor\n-1\n1\n3\ngrab motor\n-1\n1\nObservation Spec\n0\nrotation motor angle\n0.0\n360.0\n1\nlow motor angle\n10\n70\n2\nhigh motor angle\n-150\n10\n3\ngrab motor angle\n-148\n-45\n4\ngoal rotation motor angle\n0\n360\n5\ngoal low motor angle\n10\n70\n6\ngoal high motor angle\n-150\n10\n7\ngoal grab motor angle\n-148\n-45\nTable 8: Combined action and observation specifications for the RoboArm-v0 environment.\nReward Function.\nThe reward function for the RoboArm-v0 environment can be chosen to be\neither dense or sparse. In the sparse case, the reward is 1 if the distance between the current motor\nangles and the goal motor angles is below a defined threshold; otherwise, it is 0. In our experiments,\nhowever, we used the dense reward function, which is calculated as follows:\nRt = −∥∆⃗θdeg,t∥1\n100\n(10)\nwhere ∆⃗θdeg,t is the vector of shortest angular distances at time step t between the goal motor angles\n⃗θgoal,t and the current motor angles ⃗θcurrent,t, defined as:\n∆⃗θdeg,t = degrees\n\u0012\narctan 2\n\u0010\nsin(radians(⃗θgoal,t) −radians(⃗θcurrent,t)),\ncos(radians(⃗θgoal,t) −radians(⃗θcurrent,t))\n\u0011 \u0013\n(11)\n16\nA.2.6\nRoboArmSim-v0\nRoboArmSim-v0 mirrors the real-world RoboArm-v0 environment but is entirely simulated, removing\nthe need for physical interaction with an actual robot or the use of PyBricks. In this virtual setup,\nthe RoboArm’s task remains a pose-reaching challenge, where it must align its articulation to match\nrandomly sampled goal angles for its four motors, setting a target pose at every reset. The robot in the\nsimulation is provided with the current motor angles of all joints and the goal angles to accomplish\nthis task within 100 steps. Crucially, the simulation is straightforward since it does not require\nmodeling or simulating complex sensor measurements for state transitions, but only the motor angles,\nsimplifying the simulation of state transitions significantly. Similar to the WalkerSim-v0 we add\nGaussian noise (N(0, 0.05) ) to the actions before the linear mapping and addition with the current\nmotor states. Like its real-world counterpart, this environment allows users to choose between dense\nand sparse reward structures, facilitating the adjustment of the task’s difficulty level.\nAction and Observation Specifications.\nAction and observation specifications are the same as in\nthe RoboArm-v0 8.\nReward Function.\nIn the RoboArmSim-v0 environment we use the same dense reward function as\ndefined for the RoboArm-v0 environment.\nA.2.7\nRoboArm-mixed-v0\nIn the RoboArm_mixed-v0 environment, an additional sensor input is available to the robot through\na webcam. The RoboArm holds a red ball in its hand and must move it to a target position. Each\nepisode has a maximum of 30 steps. The target position is randomly selected and displayed as a\ngreen circle in the image. The image serves as additional information for the algorithm and is used to\ndetermine if the conditions to solve the task are met.\nAction\nand\nObservation\nSpecifications.\nThe\nfull\nobservation\nspecifications\nfor\nthe\nRoboArm_mixed-v0 environment consist of three floating-point values representing the three motor\nangles (rotation motor, low motor, high motor) and image observation specifications with a shape of\n(64, 64), as detailed in Table 9.\nThe action specifications are also three floating-point values in the range of [−1, 1], which will be\ntransformed before being applied to the specific motor. The rotation motor angles are transformed to\nthe range of [−90, 90], the low motor angles to the range of [−30, 30], and the high motor angles to\nthe range of [−60, 60].\nType\nNum\nSpecification\nMin\nMax\nAction Spec\n0\nrotation motor\n-1\n1\n1\nlow motor\n-1\n1\n2\nhigh motor\n-1\n1\nObservation Spec\n0\nrotation motor angle\n0.0\n360.0\n1\nlow motor angle\n10\n70\n2\nhigh motor angle\n-150\n10\nImage Observation Spec\n0\nimage observation\n0\n255\nSize: (64, 64)\nTable 9: Combined action and observation specifications for the RoboArm_mixed-v0 environment.\nReward\nFunction.\nTo\ncalculate\nthe\nreward\nfor\nthe\nmixed\nobservation\nenvironment\nRoboArm_mixed-v0, we utilize the Python package OpenCV to detect the red ball and measure\nthe distance to the target location depicted in the image. First, we convert the image from BGR to\nHSV and define a color range to identify the contours. For each detected contour, we calculate the\ndistance to the center of the green target circle and take the mean distance as the reward:\n17\nRt = −\nP distancest\nnt · 100\n(12)\nwhere nt is the number of distances detected at the current time step. If no contours are detected, we\nuse the previous reward as the current reward.\nA.2.8\nTask Environment Template\nTo illustrate the simplicity of using the TorchBricksRL BaseEnv, which manages communication\nbetween the environment and the robot, we provide an example in Listing 1. This code can serve as a\ntemplate for creating new custom environments.\n1 import\ntorch\n2\n3 from\nenvironments.base.base_env\nimport\nBaseEnv\n4 from\ntensordict\nimport\nTensorDict , TensorDictBase\n5 from\ntorchrl.data.tensor_specs\nimport\nBoundedTensorSpec , CompositeSpec\n6\n7\n8 class\nTaskEnvironment (BaseEnv):\n9\n# Define\nyour\naction and state\ndimension , needs to be adapted\ndepending on your task and robot!\n10\naction_dim = 1\n# One action to control\nthe wheel\nmotors\ntogether\n11\nstate_dim = 5\n# 5 sensors\nreadings (left\nmotor angle , right\nmotor\nangle , pitch , roll , distance)\n12\n13\n# Define\nobservation\nspace\nranges.\n14\nmotor_angles = [0, 360]\n15\nroll_angles = [-90, 90]\n16\npitch_angles = [-90, 90]\n17\ndistance = [0, 2000]\n18\n19\nobservation_key = \" vec_observation \"\n20\n21\ndef\n__init__(\n22\nself ,\n23\n):\n24\nself._batch_size = torch.Size ([1])\n25\n26\n# Define\nAction\nSpec.\n27\nself.action_spec = BoundedTensorSpec (low=-1, high=1, shape =(1,\nself.action_dim))\n28\n# Define\nObservation\nSpec.\n29\nbounds = torch.tensor(\n30\n[\n31\nself.motor_angles ,\n32\nself.motor_angles ,\n33\nself.roll_angles ,\n34\nself.pitch_angles ,\n35\nself.distance ,\n36\n]\n37\n)\n38\nobservation_spec = BoundedTensorSpec (\n39\nlow=bounds [:, 0],\n40\nhigh=bounds [:, 1],\n41\n)\n42\nself. observation_spec = CompositeSpec ({ self. observation_key :\nobservation_spec }, shape =(1 ,))\n43\n44\nsuper ().__init__(\n45\naction_dim=self.action_dim , state_dim=self.state_dim ,\n46\n)\n47\n48\n18\n49\ndef _reset(self , tensordict: TensorDictBase , ** kwargs) ->\nTensorDictBase :\n50\n# Get\ninitial\nstate\nfrom hub.\n51\nobservation = self. read_from_hub ()\n52\n# Could\nalso add\nexternal\nsensors\nhere and return\nthem as well\n.\n53\n# img = self.camera.read ()\n54\nreturn\nTensorDict(\n55\n{\n56\nself. observation_key : norm_observation .float (),\n57\n# image_obs: img.float (),\n58\n},\n59\nbatch_size =[1],\n60\n)\n61\n62\ndef reward(self , state , action , next_state) -> Tuple[float , bool ]:\n63\n# Define\nyour\nreward\nfunction.\n64\n# ...\n65\nreward , done = 0, False\n66\nreturn reward , done\n67\n68\ndef _step(self , tensordict: TensorDictBase ) -> TensorDictBase :\n69\n# Send\naction to hub to receive\nnext\nstate.\n70\naction = tensordict.get(\"action\").cpu().numpy ().squeeze (0)\n71\nself.send_to_hub(action)\n72\n# Read next\nstate\nfrom hub.\n73\nnext_observation = self. read_from_hub ()\n74\n75\n# Compute\nthe reward.\n76\nstate = tensordict.get(self. original_vec_observation_key )\n77\nnext_state = next_tensordict .get(self.\noriginal_vec_observation_key )\n78\nreward , done = self.reward(\n79\nstate=state ,\n80\naction=action ,\n81\nnext_state=next_state ,\n82\n)\n83\n# Create\noutput\nTensorDict.\n84\nnext_tensordict = TensorDict(\n85\n{\n86\nself. observation_key : self. normalize_state (\nnext_observation ).float (),\n87\n\"reward\": torch.tensor ([ reward ]).float (),\n88\n\"done\": torch.tensor ([ done ]).bool (),\n89\n},\n90\nbatch_size =[1],\n91\ndevice=tensordict.device ,\n92\n)\n93\nreturn\nnext_tensordict\nListing 1: Task environment template\nA.3\nClient Script\nFor each task and robot, a custom client script is required to facilitate interaction between the robot\nand the environment. The client.py script defines the configuration of motors, sensors, and the\nworkflow for processing and exchanging data. This script must be uploaded to the Pybricks Hub and\nupdated whenever the robot’s configuration changes, such as when motors or sensors are added or\nremoved. Listing 2 provides a simple example of a client script tailored for the RunAway-v0 task. In\nthis example, a single float value representing the action is used to control the motors, while sensor\ndata is collected and transmitted back to the environment.\n1 import\nustruct\n2 from\nmicropython\nimport\nkbd_intr\n19\n3 from\npybricks.hubs\nimport\nInventorHub\n4 from\npybricks.parameters\nimport\nDirection , Port\n5 from\npybricks.pupdevices\nimport Motor , UltrasonicSensor\n6 from\npybricks.robotics\nimport\nDriveBase\n7 from\npybricks.tools\nimport\nwait\n8 from\nuselect\nimport\npoll\n9 from usys\nimport stdin , stdout\n10\n11\n12 # Initialize\nthe\nInventor\nHub.\n13 hub = InventorHub ()\n14\n15 # Initialize\nthe drive\nbase.\n16 left_motor = Motor(Port.E, Direction. COUNTERCLOCKWISE )\n17 right_motor = Motor(Port.A)\n18 drive_base = DriveBase(left_motor , right_motor)\n19 # Initialize\nthe\ndistance\nsensor.\n20 sensor = UltrasonicSensor (Port.C)\n21\n22 keyboard = poll ()\n23 keyboard.register(stdin)\n24\n25 while\nTrue:\n26\n27\n# Optional: Check\navailable\ninput.\n28\nwhile not\nkeyboard.poll (0):\n29\nwait (1)\n30\n31\n# Read\naction\nvalues for the motors.\n32\naction = ustruct.unpack(\"!f\", stdin.buffer.read (4))[0]\n33\n# Apply the action to the motors\n34\ndrive_base.straight(action , wait=True)\n35\n36\n# Read\nsensors to get\ncurrent\nstate of the robot.\n37\n(left_m_angle , right_m_angle) = (left_motor.angle (), right_motor.\nangle ())\n38\n(pitch , roll) = hub.imu.tilt ()\n39\ndist = sensor.distance ()\n40\n41\n# Send the\ncurrent\nstate\nback to the\nenvironment.\n42\nout_msg = ustruct.pack(\n43\n\"!fffff\", left_m_angle , right_m_angle , pitch , roll , dist\n44\n)\n45\nstdout.buffer.write(out_msg)\nListing 2: Client script example.\nA.4\nCommunication Frequency\nFigure 7 offers a direct performance comparison of the DroQ agent on the Walker-v0 task at\ncommunication frequencies of 11Hz and 2Hz. Interestingly, the agent operating at 2Hz shows quicker\nand more stable convergence. We suspect that the lower communication frequency functions similarly\nto ’frame skip’, a widely utilized technique in reinforcement learning. Frame skipping helps to\nreduce the number of actions an agent takes, thereby simplifying the decision-making processes. This\nmethod may explain the more efficient convergence observed with the 2Hz frequency.\nA.5\nRunAway-v0 Strategies\nFigure 8 illustrates the distinct strategies developed by the algorithms. Specifically, Figure 8b shows\nthe final distance measured, while Figure 8a displays the mean action taken over the entire episode.\n20\nFigure 7: Comparison of communication frequencies for the DroQ agent on the Walker-v0 task,\nillustrating the differences between the operational frequencies of 11Hz and 2Hz.\nFigure 8: Final distance and (mean) action taken over one episode for the RunAway-v0 task.\nA.6\nOnline Training Parameter\nTable 12 displays the hyperparameter used in all our experiments.\nA.7\nDataset Generation Process\nThe expert dataset for each robot configuration was generated by training a Soft Actor-Critic (SAC)\nagent to solve the respective task and record transitions over 100 episodes on the real robot. The\nrandom dataset was created by executing a random policy for 100 episodes. For example, the\ncollection process for the Walker robot took about an hour, yielding approximately 10,000 transitions.\nDetails such as mean reward, number of transitions, and collection episodes for each dataset can be\nfound in Table 11. The dataset is available on Hugging face.\nA.8\nOffline Training Parameter\nFor the online algorithms, we used the same parameters in our offline rl experiments as in the online\nexperiments.\nWe trained the models for various tasks and datasets with different update counts 13. The RunAway-v0\ntask was trained for 2,000 updates on both the expert and random datasets. For the Spinning-v0\ntask, we used 5,000 updates across both datasets. The Walker-v0 task required 10,000 updates for\nboth expert and random datasets. Finally, the RoboArm-v0 task was trained for 10,000 updates on\nthe random dataset and 5,000 updates on the expert dataset.\n21\nParameter\nDroQ\nSAC\nTD3\nLearning Rate (lr)\n3 × 10−4\n3 × 10−4\n3 × 10−4\nBatch Size\n256\n256\n256\nUTD Ratio\n20\n1\n1\nPrefill Episodes\n10\n10\n10\nNumber of Cells\n256\n256\n256\nGamma\n0.99\n0.99\n0.99\nSoft Update ϵ\n0.995\n0.995\n0.995\nAlpha Initial\n1\n1\n-\nFixed Alpha\nFalse\nFalse\n-\nNormalization\nLayerNorm\nNone\nNone\nDropout\n0.01\n0.0\n0.0\nBuffer Size\n1000000\n1000000\n1000000\nExploration Noise\n-\n-\n0.1\nTable 10: Hyperparameter for the agents DroQ, SAC, and TD3\nTask\nMean Reward\nExpert Transitions\nRandom Transitions\nEpisodes\nWalker-v0\n−69.12\n9, 244\n10, 000\n100\nRoboArm-v0\n−9.87\n1, 297\n10, 000\n100\nRunAway-v0\n18.04\n1, 987\n1, 612\n100\nSpinning-v0\n8981.19\n5, 000\n5, 000\n100\nTable 11: Dataset Statistics\nA.9\nNetwork Architecutre\nThroughout the experiments, all algorithms utilize the same architecture for the policy, Q-functions,\nand value functions (where applicable). Each network is structured as a three-layer multilayer\nperceptron (MLP), with specific TorchRL actor modules used for the policy, depending on the\nalgorithm. For more details, we refer readers to the code repository: GitHub.\nThe only variation in architecture occurs when incorporating pixel-based observations. In this case,\na convolutional neural network (CNN) is used to encode the image data. These encodings are then\nconcatenated with the sensor-based encodings, and the combined embeddings are passed through a\nshared MLP. Detailed implementation specifics can be found in the GitHub repository.\n22\nParameter\nBC\nIQL\nCQL\nLearning Rate (lr)\n3 × 10−4\n3 × 10−4\n3 × 10−4\nBatch Size\n256\n256\n256\nNumber of Cells\n256\n256\n256\nGamma\n-\n0.99\n0.99\nSoft Update ϵ\n-\n0.995\n0.995\nLoss Function\nL2\nL2\nL2\nTemperature\n-\n1.0\n1.0\nExpectile\n-\n0.5\n-\nMin Q Weight\n-\n-\n1.0\nMax Q Backup\n-\n-\nFalse\nDeterministic Backup\n-\n-\nFalse\nNum Random Actions\n-\n-\n10\nWith Lagrange\n-\n-\nTrue\nLagrange Threshold\n-\n-\n5.0\nNormalization\nLayerNorm\nNone\nNone\nDropout\n0.01\n0.0\n0.0\nBC Steps\n-\n-\n1,000\nTable 12: Hyperparameter for the agents BC, IQL, and CQL\nTask\nExpert\nRandom\nRunAway-v0\n2,000\n2,000\nSpinning-v0\n5,000\n5,000\nWalker-v0\n10,000\n10,000\nRoboArm-v0\n5,000\n10,000\nTable 13: Number of offline training updates for each task and dataset\n23\n",
  "categories": [
    "cs.RO",
    "cs.LG"
  ],
  "published": "2024-06-25",
  "updated": "2024-12-02"
}