{
  "id": "http://arxiv.org/abs/2407.00707v1",
  "title": "Deep learning quantum Monte Carlo for solids",
  "authors": [
    "Yubing Qian",
    "Xiang Li",
    "Zhe Li",
    "Weiluo Ren",
    "Ji Chen"
  ],
  "abstract": "Deep learning has deeply changed the paradigms of many research fields. At\nthe heart of chemical and physical sciences is the accurate ab initio\ncalculation of many-body wavefunction, which has become one of the most notable\nexamples to demonstrate the power of deep learning in science. In particular,\nthe introduction of deep learning into quantum Monte Carlo (QMC) has\nsignificantly advanced the frontier of ab initio calculation, offering a\nuniversal tool to solve the electronic structure of materials and molecules.\nDeep learning QMC architectures were initial designed and tested on small\nmolecules, focusing on comparisons with other state-of-the-art ab initio\nmethods. Methodological developments, including extensions to real solids and\nperiodic models, have been rapidly progressing and reported applications are\nfast expanding. This review covers the theoretical foundation of deep learning\nQMC for solids, the neural network wavefunction ansatz, and various of other\nmethodological developments. Applications on computing energy, electron\ndensity, electric polarization, force and stress of real solids are also\nreviewed. The methods have also been extended to other periodic systems and\nfinite temperature calculations. The review highlights the potentials and\nexisting challenges of deep learning QMC in materials chemistry and condensed\nmatter physics.",
  "text": "Article Title\nDeep learning quantum Monte Carlo for solids\nArticle Category\nAdvanced Review\nAuthors\nYubing Qian*\nORCID iD: 0000-0001-6980-163X\nAffiliation:\n1. School of Physics, Peking University, Beijing 100871, People’s Republic of China\n2. ByteDance Research, Fangheng Fashion Center, No. 27, North 3rd Ring West\nRoad, Haidian District, Beijing 100098, People’s Republic of China\nEmail: phyqyb@pku.edu.cn\nXiang Li*\nORCID iD: 0000-0001-8572-1875\nAffiliation:\n1. ByteDance Research, Fangheng Fashion Center, No. 27, North 3rd Ring West\nRoad, Haidian District, Beijing 100098, People’s Republic of China\nEmail: lixiang.62770689@bytedance.com\nZhe Li*\nORCID iD: 0000-0002-2493-9229\nAffiliation:\n1. ByteDance Research, Fangheng Fashion Center, No. 27, North 3rd Ring West\nRoad, Haidian District, Beijing 100098, People’s Republic of China\nEmail: lizhe.qc@bytedance.com\n1\narXiv:2407.00707v1  [physics.chem-ph]  30 Jun 2024\nWeiluo Ren*\nORCID iD: 0000-0002-4276-4856\nAffiliation:\n1. ByteDance Research, Fangheng Fashion Center, No. 27, North 3rd Ring West\nRoad, Haidian District, Beijing 100098, People’s Republic of China\nEmail: renweiluo@bytedance.com\nJi Chen*\nORCID iD: 0000-0003-1603-1963\nAffiliation:\n1. School of Physics, Peking University, Beijing 100871, People’s Republic of China\n2. Interdisciplinary Institute of Light-Element Quantum Materials, Frontiers Science\nCenter for Nano-Optoelectronics, Peking University, Beijing 100871, People’s\nRepublic of China\nEmail: ji.chen@pku.edu.cn\nConflicts of interest\nThe authors declare that they have no conflicts of interest.\nAbstract\nDeep learning has deeply changed the paradigms of many research fields. At the heart\nof chemical and physical sciences is the accurate ab initio calculation of many-body wave-\nfunction, which has become one of the most notable examples to demonstrate the power\nof deep learning in science. In particular, the introduction of deep learning into quantum\nMonte Carlo (QMC) has significantly advanced the frontier of ab initio calculation, offering\na universal tool to solve the electronic structure of materials and molecules. Deep learn-\ning QMC architectures were initial designed and tested on small molecules, focusing on\ncomparisons with other state-of-the-art ab initio methods. Methodological developments,\nincluding extensions to real solids and periodic models, have been rapidly progressing and\nreported applications are fast expanding. This review covers the theoretical foundation of\ndeep learning QMC for solids, the neural network wavefunction ansatz, and various of other\nmethodological developments. Applications on computing energy, electron density, electric\n2\npolarization, force and stress of real solids are also reviewed. The methods have also been\nextended to other periodic systems and finite temperature calculations. The review high-\nlights the potentials and existing challenges of deep learning QMC in materials chemistry\nand condensed matter physics.\nGraphical/Visual Abstract and Caption\nCaption: The introduction of deep learning into quantum Monte Carlo has significantly ad-\nvanced the frontier of ab initio calculation. The method has been extended from molecular\nsystems to real solids and periodic models, showing a great potential for accurate calcula-\ntions of chemical and physical properties, and exploration of intriguing phenomena.\n3\n1. Introduction\nSolving the Schr¨odinger equation for many-electron systems is an essential task of first-\nprinciple modeling, i.e. ab initio calculation, of materials and condensed matter systems.\nAlthough ab initio calculations have been widely employed now in chemistry and physics\nstudies, an exact ab initio treatment is impractical and accurate solutions are only available\nfor small enough molecules. The challenge mainly stems from the many-body electron\ncorrelation effects embedded in the Coulomb interactions between many electrons of such\nsystems. To ease the problem and go forward to obtain a reliable solution of real materials,\nthere are two different directions. One is to look for deterministic solutions based on neces-\nsary approximations to trade off accuracy for efficiency. Hartree–Fock (HF) approximation\nand post Hartree–Fock wavefunction theories such as coupled cluster and configuration\ninteraction can systematically improve the accuracy by explicit inclusion of higher level\nelectron excitation. The limitation is that the computational cost scales up so rapidly with\nthe number of electrons, which hinders their applications to materials and condensed mat-\nter systems. In a different direction, the density functional theory achieves a good balance\nbetween efficiency and accuracy, but the design and choice of the exchange-correlation\nfunctional cause questions about the reliability of DFT to strongly correlated systems. More\naccurate methods going beyond DFT via e.g. many-body perturbation are also being ac-\ntively developed, but most of them fall into similar situations that the computational cost\nscales up rapidly. Nowadays, there are many comprehensive reviews and textbooks of\nthese deterministic approaches,1–3 and in this review we focus on the latest development\nof another type of approach, namely quantum Monte Carlo (QMC), in which the main idea\nis to look for stochastic solutions to maintain accuracy with the cost of introducing statistical\nerrors.\nAn attractive feature of QMC is the possibility of obtaining direct and accurate treatment\nof many-body correlation effects with a favorable scaling. The accuracy of QMC can also\nbe systematically improved with an increased number of Monte Carlo samples. Significant\nprogress has made in the past few decades to expand the capability of QMC, e.g. the\ndevelopments in optimization methods,4–6 interatomic force evaluation,7–10 and phonon\ncalculation,11,12 to name a few. However, there are still some challenges for QMC simula-\ntion for materials and condensed matter systems. For example, the required computational\n4\nresource is large, and thus the simulation size is limited, which gives rise to the finite size\nproblem. Just as importantly, the accuracy in large systems is still to be improved.\nSince the seminal review of Foulkes et al.13 in 2001, VMC and DMC remains state-of-\nthe-art for accurate calculation of solid materials, with methodological contributions from a\ndiverse community.14–17 Among all the developments, the past few years have witnessed\nan explosion in the application of deep learning techniques in the development of wave-\nfunction ansatz for VMC. The neural network takes in Monte Carlo samples of electron\ncoordinates, and outputs the value of the wavefunction, requiring no external data. Initially,\nthese are applied in spin model systems,18,19 and molecule systems.20–25 Later, they are\ngenerated to real solids26,27 and other condensed matter systems such as the electron\ngas28–30 by encoding periodicity into the neural network. The employment of neural net-\nworks not only improves the accuracy, but also offers new opportunities to re-visit other\naspects and techniques used in traditional QMC for materials and condensed matter sys-\ntems.\nIn this advanced review, we would like to discuss the recent developments and\napplications of deep learning QMC for solids. We note that although the currently work-\ning architectures for solids are extended from existing architectures for molecules or lattice\nmodels, unique aspects in modeling solids, such as periodicity and finite-size errors, should\nbe clarified. In addition to the technical differences, the properties of interest go beyond\nsimply energies, and properties like susceptibility, stress tensors, and phonon spectra are\nalso of significant interest. Therefore, we aim to provide a complementary review article to\nthe seminal review articles of traditional QMC and the latest review on deep learning QMC,\nwhich mostly focuses on molecules and lattice models. The readers are recommended to\nread the previous reviews 13,15 for a comprehensive understanding of traditional QMC, and\nto read the article of Hermann et al.31 for details about different flavors of machine learning\narchitectures for many-body wavefunction ansatz. In section 2 we briefly introduce the key\naspects of QMC theory and detail the developments of deep learning QMC for solids. In\nsection 3 we discuss the applications of deep learning QMC for solids including real solid\nmaterials and other condensed matter systems such as electron gas.\n5\n2. Methodological developments\n2.1 Many-electron Schr¨odinger equation in solids\nThe starting point of QMC and other ab initio methods is the time-independent Schr¨odinger\nequation. Considering nucleus masses are significantly larger than electrons, it is advan-\ntageous to decouple their movements with Born–Oppenheimer approximation, treating the\ncoordinates of the nuclei RI as fixed parameters when solving the Schr¨odinger equation of\nthe electrons:\nˆHΨ(x1, . . . , xN) = EΨ(x1, . . . , xN),\n(1)\nˆH = −1\n2\nN\nX\ni=1\n∇2\ni −\nN\nX\ni=1\nNatom\nX\nI=1\nZI\n|ri −RI| + 1\n2\nN\nX\ni=1\nN\nX\nj=1\nj̸=i\n1\n|ri −rj| + 1\n2\nNatom\nX\nI=1\nNatom\nX\nJ=1\nJ̸=I\nZIZJ\n|RI −RJ|,\n(2)\nwhere ZI is the charge of the nucleus I, xi = (ri, si) is the spatial and spin coordi-\nnates of the electron i, and Ψ is the antisymmetric wavefunction under the permutation\nof (x1, . . . , xN).\nGiven the substantial number of electrons in real solids, exact calculations become in-\ntractable, and the supercell approximation is introduced to simplify the problem which en-\nforces the translational symmetry. A supercell is constructed by tiling multiple primitive cells,\nand the Coulomb interaction between particles in different supercells is approximated by\nthe interaction between particles and their periodic images in other supercells. In essence,\nsupercell Hamiltonian can be expressed in the following form\nˆHs = −1\n2\nN\nX\ni=1\n∇2\ni −\nN\nX\ni=1\nNatom\nX\nI=1\nX\nLs\nZI\n|ri −RI + Ls|\n+ 1\n2\nN\nX\ni=1\nN\nX\nj=1\n′\nX\nLs\n1\n|ri −rj + Ls| + 1\n2\nNatom\nX\nI=1\nNatom\nX\nJ=1\n′\nX\nLs\nZIZJ\n|RI −RJ + Ls|,\n(3)\nwhere {Lp}, {Ls} denotes the lattice vectors of primitive cell and supercell respectively.\nAnd the summation with a prime means that the Ls = 0 term is excluded when i = j or\nI = J. It should be pointed out that the summation of the Coulomb potentials is conditional\nconvergent, and should be treated specially, usually through the Ewald summation.13,32,33\n6\n2.2 Variational Monte Carlo method\nVariational Monte Carlo is a QMC method that employs the variational principle to solve the\nSchr¨odinger equation Eq. (1). The variational principle asserts that the expectation value\nof energy Ev for any ΨT is always greater or equal to the ground state energy E0:\nEv =\nR\nΨ∗\nT (X) ˆHΨT (X) dX\nR\nΨ∗\nT (X)ΨT (X) dX\n≥E0,\n(4)\nand the equal sign is taken if and only if ΨT is exactly the same as the ground state\nwavefunction Ψ0.\nGiven a trial wavefunction ΨT , the above integration can be calculated using Monte Carlo\nalgorithms. Specifically, a large number (M) of samples {Xi} obeying the probability dis-\ntribution\npT (X) =\nΨ∗\nT (X)ΨT (X)\nR\nΨ∗\nT (X)ΨT (X) dX\n(5)\ncan be obtained with the Markov-chain Monte Carlo and the Metropolis–Hastings algo-\nrithm.13,34 The energy Ev is represented by the average local energy EL = [ ˆHΨT (X)]/ΨT (X),\nEv = ⟨EL(X)⟩=\nZ\npT (X)EL(X) dX = lim\nM→∞\n1\nM\nM\nX\ni=0\nEL(Xi),\n(6)\nwhere ⟨·⟩means the expectation value under the probability distribution pT (X).\nAfterward, the trial wavefunction ΨT is optimized variationally. The Monte Carlo integration\nand optimization process are repeated until the energy Ev converges to a satisfactory level,\nand the final wavefunction ΨT is considered a good approximation of the ground state\nwavefunction Ψ0, within the limitation of the chosen ansatz.\n2.3 Wavefunction ansatz\nThe choice of an ansatz for the trial wavefunction is of great importance in VMC, as it\nultimately determines the achievable accuracy. In this section, we will discuss the symmetry\nrequirements, as well as various forms of ansatz, including traditional ones and more recent\nneural network–based ones.\n2.3.1 Symmetry requirements\nFirstly, a valid ansatz for a many-electron wavefunction must satisfy the fundamental re-\nquirement of antisymmetry under the exchange of any two electrons, reflecting the fermionic\n7\nnature of electrons:\nΨ(. . . , xi, . . . , xj . . . ) = −Ψ(. . . , xj, . . . , xi . . . ).\n(7)\nFurthermore, the supercell Hamiltonian ˆHs is invariant with a simultaneous translation of\nall electrons by a vector in {Lp} as well as a translation of any electron by a vector in {Ls},\nˆHs(r1 + Ls, . . . , rN) = ˆHs(r1, . . . , rN),\n(8a)\nˆHs(r1 + Lp, . . . , rN + Lp) = ˆHs(r1, . . . , rN),\n(8b)\nCorrespondingly, wavefunctions can be labeled by two quantum numbers kp, ks and trans-\nform as below\nΨ(r1 + Ls, . . . , rN) = eiks·LsΨ(r1, . . . , rN),\n(9a)\nΨ(r1 + Lp, . . . , rN + Lp) = eikp·LpΨ(r1, . . . , rN),\n(9b)\nwhere kp, ks are constrained in the Brillouin zone of primitive cell and supercell respectively.\nIt is also worth noting that ˆHs possesses spatial group symmetry of solids, and each quan-\ntum state will transform according to the specific representations.\nHowever, it is often\nchallenging to specify the exact representation of the ground state and associated trans-\nform law a priori. As a solution, ground state wavefunctions can be built without constraint\nunder spatial group transform, and are ensured to approach the ground truth via energy\nminimization.\n2.3.2 Traditional ansatz\nSlater–Jastrow ansatz is one of the most widely employed trial wavefunctions in VMC:13–17,35\nΨSJ\nks,kp(X) = eJ(X)\n\f\f\f\f\f\f\f\f\f\nψ1(x1)\n· · ·\nψN(x1)\n...\n...\n...\nψ1(xN)\n· · ·\nψN(xN)\n\f\f\f\f\f\f\f\f\f\n,\n(10)\nwhere we define X = (x1, . . . , xN), and ψj(xi) are spin orbitals. The determinant part is\nantisymmetric under electron exchange, while the Jastrow factor eJ is symmetric, and is\ndesigned to capture the correlations and satisfy the cusp condition.36 It is convenient to\n8\nremove the spin variables in Eq. (10) by rewriting it as13\nΨSJ\nks,kp(R) = eJ(R)\n\f\f\f\f\f\f\f\f\f\nϕ1(r1)\n· · ·\nϕN↑(r1)\n...\n...\n...\nϕ1(rN↑)\n· · ·\nϕN↑(rN↑)\n\f\f\f\f\f\f\f\f\f\n·\n\f\f\f\f\f\f\f\f\f\nϕN↑+1(rN↑+1)\n· · ·\nϕN(rN↑+1)\n...\n...\n...\nϕN↑+1(rN)\n· · ·\nϕN(rN)\n\f\f\f\f\f\f\f\f\f\n,\n(11)\nwhere R = (r1, . . . , rN), and the first N↑electrons are assumed to spin up. And ϕj(ri) are\nspatial parts of the spin orbitals.\nThe translational symmetry can be achieved by taking the spatial orbitals ϕj(ri) with a\nBloch form:\nϕj(ri) = eikj·riuj(ri),\n(12)\nwhere uk is invariant with a simultaneous translation of all electrons by a vector in {Lp} as\nwell as a translation of any electron by a vector in {Ls}, and kj lies on the grid of supercell\nreciprocal lattice vectors {Gs} offset by ks within the first Brillouin zone of the primitive cell.\nIf the determinant part is fixed, the nodes of the wavefunction, namely points where Ψ(X) =\n0, are unchanged during optimization, putting a limitation on the accuracy. A popular ap-\nproach is using the backflow transformation,13,14,37–39 replacing the electron coordinates in\nthe determinant with quasiparticle coordinates. Another approach is to replace the Slater\ndeterminant with more general ones, for example Pfaffian40,41 or antisymmetrized geminal\npower wavefunctions,42,43 empowering the expressiveness of the wavefunction ansatz.\n2.3.3 Neural network–based ansatz\nInspired by the backflow approach, the wavefunction can be constructed from the ground\nup with expressive neural networks, having a better treatment of electron correlation and\nobviating the need for Hartree–Fock or DFT starting points. The key idea is to rewrite\nEq. (12) with\nϕj(ri, {r/i}) = eikj·riuj(ri, {r/i}),\n(13)\nwhere contributions from all other electrons {r/i} are included in a permutation-equivariant\nway, keeping the wavefunction antisymmetric. The simplest permutation-equivariant oper-\nation for a neural network layer is to pass all individual inputs through the same function. To\nincorporate electron-electron correlation, it is also helpful to append the average outputs of\nthe previous layer to the inputs of the next. That is the essence of FermiNet.21 Regarding\nJastrow factors, however, they are not an essential part of the wavefunction because the\n9\n...\n...\n...\n...\n...\n...\n...\n...\n...\n......\n...\n...\n...\n...\n...\n+\n+\n=\nPeriodic\nDistance\nPeriodic Metric\nPeriodic\nFunction\nconcat\nMLP\nMLP\naverage\nHidden layers\n...\n...\nconcat\naverage\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n+\ni\n...\n...\n...\n...\n...\nComplex-valued orbitals\nMany-body\nwavefunction\nLegend\nMLP\nMultilayer perceptron\nReal/imaginary part of orbital\nPeriodic input feature\nCrystal momentum\nNucleus\nElectron\nLattice vector\nFigure 1: Illustration of the neural network architecture for solids according to DeepSolid.26\nPeriodicity is imposed on the electron-nucleus and electron-electron distance, forming the\ninput features. These features are then passed through multiple equivariant hidden layers\nto construct a set of complex-valued orbitals. A phase factor is also included. In the end,\nthe value of the wavefunction is calculated as the weighted summation of the determinants,\nwhere each element is a product of a phase factor and a complex-valued orbital.\nneural network-based orbitals are already powerful enough and can well approximate the\ncusp conditions.\nFig. 1 illustrates the architecture of DeepSolid, which is a good example of incorporating\nperiodicity into neural network ansatz. Neural networks can represent the periodic function\nuk(r) by generalizing input coordinate r and distance r = |r| to periodic ones ˜r and ˜r.\nAnd it is desirable for ˜r to be proportional to r when r →0, reflecting the isotropy of\nthe interactions and ensuring the representation of cusp conditions. With supercell lattice\nvectors {a1, a2, a3} and corresponding reciprocal lattice vectors {b1, b2, b3}, an arbitrary\nvector r can be decomposed as r = s1a1 +s2a2 +s3a3, where sα = r·bα/(2π). The square\nof the Euclidean distance is then\nr2 =\n \n3\nX\nα=1\naαsα\n!\n·\n\n\n3\nX\nβ=1\naβsβ\n\n=\n3\nX\nα=1\n3\nX\nβ=1\nsαsβaα · aβ.\n(14)\n10\nA typical periodic generalization is to replace sαsβ with Mαβ(sα, sβ):\n˜r2 =\n3\nX\nα=1\n3\nX\nβ=1\nMαβ(sα, sβ)aα · aβ,\n(15)\nwhere Mαβ is a smooth periodic function satisfying\nlim\nsα→0\nsβ→0\nMαβ(sα, sβ)\nsαsβ\n= constant.\n(16)\nVarious forms of Mαβ can be chosen, and here we focus on two of them, which we dub\n“nu”26 and “sin”29\nM nu\nαβ(sα, sβ) = f(sα)2δαβ + g(sα)g(sβ)(1 −δαβ),\n(17a)\nM tri\nαβ(sα, sβ) = [1 −cos(2πsα)][1 −cos(2πsβ)] + sin(2πsα) sin(2πsβ),\n(17b)\nwhere f(s) = |s|(1 −2|s|3) and g(s) = s(1 −3|s| + 2|s|2). As for the generalized vector ˜r,\nthere is no such restriction, and it can be chosen as aαg(sα),26 or a set of cos(2πsα) and\nsin(2πsα).29\nApart from the neural network structure used in FermiNet and DeepSolid, the permutation-\nequivariant neural network can have many other forms, evolving through an interplay be-\ntween physics-inspired designs and black-box-like ones.20–22,26,44–48 While most of these\nadvancements have been showcased on molecules and clusters, they can be transferred\nto solid systems. Early works utilized sophisticated architecture to outperform traditional\nmethods.20–22,44 However, recent transformer-based architectures have demonstrated bet-\nter performance with an even simpler design.47 A latest development in architecture is\nthe development of LapNet, combined with a new computational framework called for-\nward Laplacian, achieving state-of-the-art accuracy and improving the efficiency of VMC by\ndozens of times.48 Yet, despite these advancements, it remains an intriguing research chal-\nlenge to design neural networks so that the search space is broad while the optimization\nlandscape is friendly to the training process, especially for systems with strong electron-\nelectron correlations.\n2.4 Wavefunction optimization\nIn addition to the expressiveness of the ansatz, it’s also important to have the ability to be\nefficiently optimized with respect to loss function L(θ).\n11\nFor traditional ansatz, the stochastic reconfiguration (SR) approach4,5 can efficiently opti-\nmize the wavefunction. Consider a subspace of the Hilbert space spanned by {|ΨT (θ)⟩,\n|∂θ1ΨT (θ)⟩, |∂θ2ΨT (θ)⟩, . . . }, where |ΨT (θ)⟩is the trial wavefunction at the current step,\nand θi are parameters. Inspired by imaginary time evolution, the operator (1−τ ˆH) is applied\nto the state |ΨT (θ)⟩, where τ is a small imaginary time. The updated state |ΨT (θ + δθ)⟩\nis expanded to x0 |ΨT (θ)⟩+ P\nj xj |∂θjΨT (θ)⟩, where generally x0 ̸= 1 because the wave-\nfunction is unnormalized. The coefficients x0, x1, . . . are determined by projecting the state\n(1 −τ ˆH) |ΨT (θ)⟩back to the subspace:\nD\nΨT\n\f\f\f (1 −τ ˆH)\n\f\f\f ΨT\nE\n=\nX\nj\n\nΨT\n\f\f ∂θjΨT\n\u000b\nxj + ⟨ΨT | ΨT ⟩x0,\n(18a)\nD\n∂θiΨT\n\f\f\f (1 −τ ˆH)\n\f\f\f ΨT\nE\n=\nX\nj\n\n∂θiΨT\n\f\f ∂θjΨT\n\u000b\nxj + ⟨∂θiΨT | ΨT ⟩x0,\n(18b)\nThe solution of Eq. (18) specifies the parameters should be updated following\nδθ ∝−S−1f,\n(19)\nwhere\nSij =\n\u001c∂log Ψ∗\nT (X)\n∂θi\n∂log ΨT (X)\n∂θj\n\u001d\n−\n\u001c∂log Ψ∗\nT (X)\n∂θi\n\u001d \u001c∂log ΨT (X)\n∂θj\n\u001d\n,\n(20)\nfi =\n\u001c\nEL(X)∂log Ψ∗\nT (X)\n∂θi\n\u001d\n−Ev\n\u001c∂log Ψ∗\nT (X)\n∂θi\n\u001d\n.\n(21)\nPutting the same things in the language of deep learning, the parameters are optimized in\nthe Riemannian manifold instead of a Euclidean space, and the distance is\n∥dθ∥2 =\nX\ni,j\nGij(θ)dθidθj,\n(22)\nwhere Gij is the metric tensor of the space. Following the steepest descent direction on\nthe Riemannian manifold, we obtain the natural gradient descent scheme:49\nδθ ∝−G−1∇θL(θ).\n(23)\nIf the wavefunction is real-valued, we can choose G as the Fisher information metric F:50,51\nFij =\n\u001c∂log p(X)\n∂θi\n∂log p(X)\n∂θj\n\u001d\n,\n(24)\nwhere p(X) is the normalized probability defined in Eq. (5). Such a choice is equivalent\nto SR.18,21 In the case of a complex-valued wavefunctions, Fubini–Study metric can be\n12\nutilized, and we take G = S.51,52 Yet, calculating G−1 is impractical for neural networks\nas the number of parameters is huge. Instead, approximate G−1 is used with the help\nof Kronecker-factored approximate curvature (K-FAC).53 Moreover, modifications to K-FAC\nare needed for handling unnormalized distribution and Fubini–Study metric.21,26\nA few alternative optimization methods are available. One such approach is the conju-\ngate gradient–based method6 which can avoid constructing and storing the large S matrix,\nand has been successfully applied to neural network–based ansatz.45 Another notable\noptimization technique is the linear method, which resembles an approximate Newton\nmethod.16,54,55 These strategies together form a diverse toolkit for addressing the chal-\nlenges of variational wavefunction optimization.\n2.5 Diffusion Monte Carlo\nBesides VMC, Diffusion Monte Carlo is another QMC method that leverages imaginary\ntime projection to obtain accurate ground states. Given a linear combination of ground\nand excited states, the imaginary time evolution decays the coefficients of the excited state\nexponentially, leaving only the ground state. With the absence of the potential energy term,\nthe imaginary time evolution driven by the kinetic term resembles a diffusion process:\n∂\n∂τ Φ(X, τ) = 1\n2\nN\nX\ni=1\n∇2\ni Φ(X, τ).\n(25)\nTo get stable result with the presence of the potential term, a mixed distribution should be\nused instead of plain Ψ(X, τ):\nf(X, τ) = Φ(X, τ)ΨT (X),\n(26)\nwhere ΨT (X) is the guiding wavefunction which is often obtained from the VMC process,\nand further modifications to the diffusion simulation is needed accordingly.13,56,57\nWith traditional wavefunction ansatz, due to its limited expressiveness, the VMC process\nfails to give an accurate-enough wavefunction, and a subsequent DMC process is needed\nto project out the ground state. But the DMC process can still produce biased results\nbecause of the fixed-node approximation based on inaccurate guiding wavefunction from\nVMC. With deep learning VMC, high-quality guiding wavefunctions can be obtained for\nDMC simulation, and a higher accuracy in energy can be reached.58 Nevertheless, since\nthe wavefunction from deep learning VMC is already accurate-enough, and many physical\n13\nquantities, for example, the force8 and susceptibility,59 are harder to evaluate in DMC as\nopposed to VMC, the DMC process is not always necessary for deep learning VMC.\n2.6 Pseudopotential\nUp to this point, our discussions have focused on all-electron QMC simulations. How-\never, for systems involving heavy atoms, this can be inefficient. The computational cost\nscales rapidly with the number of electrons. The core electrons have a negligible impact on\nthe properties of the solid, but they contribute a large part of the energy, hence the most\ncomputational effort is spent on capturing the complicated behavior of the core electrons.\nPseudopotential, also known as effective core potential (ECP), is extremely useful for the\nsimulation of heavy atoms.60 It replaces the core electrons with an effective potential which\nmimics their influence on valence electrons. The reduction of the number of electrons en-\nables the simulation of larger systems. It also benefits the convergence of the neural net-\nwork by removing the overwhelming energy contributions of the core electron. Therefore,\ndifferent kinds of pseudopotentials have been developed for QMC during the past decades,\nincluding Burkatzki-Filippi-Dolg (BFD) ECP.61 and correlation consistent (cc) ECP60 The\npseudopotential technique has been implemented into neural network QMC in ref. 62 to\nstudy transitional metal atoms and oxides.\n2.7 Finite-size error\nThe supercell approximation used in calculations for solid systems results in the finite-size\nerror (FSE) concerning the thermodynamic limit. To reduce FSE on the energy, the most\nstraightforward approach is to perform calculations with different system sizes and extrap-\nolate the energy to the infinite-size limit.63 However, this approach is usually inapplicable\ndue to large computational costs, hence several more efficient methods to correct FSE\nhave been proposed.\nOne of the most popular techniques is twist averaging,64 which reduces FSE while retaining\nhigh computational efficiency. Its main point is to average results of different twists ks,\nwhich ensures a more thorough sampling of the supercell Brillouin zone and therefore faster\nconvergence towards the thermodynamic limit. Specifically, the twist averaged expectation\nvalue of the observable ˆO reads:\n⟨ˆO⟩twist =\nΩs\n(2π)3\nZ\ndks ⟨Ψks| ˆO|Ψks⟩≈\n1\nNtwist\nX\nks\n⟨Ψks| ˆO|Ψks⟩,\n(27)\n14\nwhere Ωs is the supercell volume, and the integral is practically approximated using a dis-\ncrete sum of the Monkhorst-Pack k-point grid.65 Compared with simply increasing supercell\nsize, twist average shows a much more efficient linear complexity to reduce FSE. To further\nreduce the cost of reaching the thermodynamic limit, there are many attempts to approx-\nimate the twist averaged integral in Eq. (27) with the result of a single special point k0,\nwhich reads\n⟨ˆO⟩twist ≈⟨Ψk0| ˆO|Ψk0⟩.\n(28)\nBaldereschi proposed some special points according to the spatial symmetry of solids and\nachieved better TDL convergence in insulators.66 Later, more special points were proposed\nby different groups67–69 from analyzing mean-field level results, which further broaden the\napplicable range of the single-point method.\nApart from the twist averaging method, the structure factor–correction scheme can be fur-\nther employed to reduce the FSE in Coulomb interactions.70 The main contribution of FSE\nin Coulomb energy origins from the difference between the Ewald summation and the orig-\ninal integral, and can be removed via\n∆VN = N 2π\nΩs\nlim\nk→0\nS(k)\nk2 ,\nS(k) = 1\nN [⟨ρ(k)ρ∗(k)⟩−⟨ρ(k)⟩⟨ρ∗(k)⟩] , ρ(k) =\nX\ni\nexp(ik · ri),\n(29)\nwhere N is the number of particles and S(k) is the structure factor. In practical calculations,\nS(k) is usually calculated with finite supercell, the k →0 limit is estimated via extrapolation.\n3. Applications\nHaving established the fundamental framework of deep learning QMC methods, we now\nturn our attention to a review of the simulation results obtained using these techniques in\nvarious systems.\n3.1 Real solids\n3.1.1 Energy\nThe cohesive energy and the equation of state are fundamental properties of solids. They\ncan be measured in experiments, and results with a wide range of accurate ab initio meth-\nods are also available, making them perfect benchmarks for new computational methods.\n15\nFigure 2: Energy results for different systems from DeepSolid26 (labeled as “Net”) and\nother methods.\n(a) Hydrogen chain dissociation curve.\n(b) Hydrogen chain energy of\ndifferent size N, extrapolated to TDL. The bond length is fixed at 1.8 Bohr. LR-DMC and\nVMC results71 use the cc-pVTZ basis and DFT orbitals with the local density approximation\n(LDA) functional. The AFQMC result71 is extrapolated to the complete basis set limit, and\nis considered as the current state-of-the-art. (c) Structure of graphene. The equilibrium\nlength is 1.421 ˚A. (d) Graphene cohesive energy per atom of Γ point and finite-size error\ncorrected result. The experimental data are taken from ref. 72. (e) Structure of rock-salt\nlithium hydride crystal. The depth of color represents the distance of the points. (f) Equation\nof state of lithium hydride crystal, fitted with the Birch–Murnaghan equation of state.73 The\nexperimental data are taken from ref. 74. Hartree–Fock based corrections are applied to\ncorrect the finite-size error. Adapted from ref. 26, CC BY 4.0.\n16\nEmploying deep learning QMC, Li et al.26 carried out calculations on various systems, in-\ncluding hydrogen chain, graphene, and lithium hydride crystal, as shown in Fig. 2. For the\nhydrogen chain, the equation of state from neural networks nearly coincided with the ac-\ncurate lattice-regularized diffusion Monte Carlo (LR-DMC)75 results, and significantly out-\nperformed traditional VMC. The cohesive energy after extrapolating to the thermodynamic\nlimit (TDL) was also comparable to the state-of-the-art results from LR-DMC and auxiliary\nfield quantum Monte Carlo (AFQMC).76 However, the finite-size error was significant with a\nlimited supercell size when comparing the energy results on graphene and lithium hydride\nwith the experimental data. The authors corrected the energies with the twist averaged\nboundary condition (TABC) in conjunction with the structure factor correction,70 and then\nthe cohesive energy and equation of state were in excellent agreement with the experimen-\ntal results.\nApart from the finite-size error, the convergence error also plays an important part in relative\nenergies. Fu et al.77 proposed an empirical scheme using the training data to extrapolate\nenergies to zero-variance.\nBy employing the scheme based on the original DeepSolid\ntraining data, the cohesive energy of the hydrogen chain and the graphene were greatly\nimproved.\n3.1.2 Electron density\nElectron density is also a crucial property in solid systems, characterizing features of dis-\ntinct solids. Li et al.26 calculated the electron density of the diamond-structured silicon and\nrock-salt sodium chloride, which are the typical covalent and ionic crystals, and success-\nfully revealed their characters. Besides, electron density also indicates whether the solid\nis a metal or insulator. As an example, electron densities of hydrogen chains with various\nbond lengths were calculated by Li et al.26 with deep learning QMC. The calculated elec-\ntron density is rather uniform in a compressed hydrogen chain, and becomes much more\nlocalized for a stretched chain, indicating the existence of the metal–insulator transition.\n3.1.3 Electric polarization\nDielectric response, as one of the fundamental properties of materials, is instrumental in\nmany electromagnetic phenomena. However, a proper microscopic theory of polarization\nwas not established until the 1990s, which is called the modern theory of polarization.78–80\nWithin this theoretical framework, the electric polarization is closely related to the Berry\n17\nphase. However, the currently most used methods such as DFT and Hartree–Fock neglects\nelectron correlations, leading to considerable errors in susceptibility results.81 Although\na self-consistent DMC workflow for susceptibility was proposed by Umari et al.59,82, its\ncomputational complexity hindered further application.\nIn contrast, within the deep learning VMC framework, an accurate calculation of dielectric\nproperties becomes feasible. Given a finite electric field E, the parameters of the wave-\nfunction are optimized against the electric enthalpy F:59,83\nF = Ev −ΩsE · P,\nP = −1\nΩs\nX\nα\naα\n2π Im ln\n*\nexp\n \nibα ·\nX\ni\nri\n!+\n,\n(30)\nwhere P is the polarization density calculated with the modern theory of polarization.\nLi et al.84 applied deep learning QMC to dielectric constant calculations, where neural\nnetworks played an important role in incorporating electron correlations and enhance the\nexpressiveness of the wavefunction ansatz.\nThe accuracy of deep learning QMC was\ndemonstrated on various systems by comparing with the most accurate methods and ex-\nperimental data. For example, the dielectric constant results for alkali metal hydrides are\nlisted in Table. 2, and results from DeepSolid are very close to the experimental results.\nTable 2: High-frequency dielectric constants ϵ∞of alkali metal hydrides. HF and DeepSolid\nresults are calculated in ref. 84. DFT results with LDA and Perdew–Burke–Ernzerhof (PBE)\nfunctionals come from ref. 85, and experimental data come from ref. 86–88.\nSystem\nLDA\nPBE\nHF\nDeepSolid\nExperiment\nLiH\n4.92\n4.28\n3.03\n3.391(2)\n3.61, 3.939\nNaH\n3.35\n3.12\n2.37\n2.556(1)\n2.161\nKH\n2.94\n2.69\n2.21\n1.967(1)\n2.111\nRbH\n3.07\n2.73\n2.25\n1.818(1)\nNot Available\nCsH\n3.45\n2.98\n2.41\n1.695(1)\n1.638\n3.1.4 Force and stress\nFor further application of deep learning QMC to solid systems, the interatomic force and\nstress tensor are essential physical quantities that are needed for structural optimization,\n18\nphonon calculation, molecular dynamics, etc. However, for interatomic force, a straight-\nforward VMC evaluation will lead to an infinite variance.8,89 Over the past decades, the\ninfinite variance problem has motivated many researchers developing better force estima-\ntors in traditional VMC.7,8,90,91 Qian et al.92,93 implemented various force estimators and\ntested their accuracy and efficiency for both solid and molecular systems. Compared with\ntraditional VMC, force calculations benefited from the quality of the neural network–based\nwavefunction, demonstrating the power of deep learning QMC by providing a better wave-\nfunction.\nTraditionally, the force estimators are often designed with DMC in mind. Consequently,\ndesigning a force estimator specially for deep learning VMC can potentially accelerate cal-\nculation and improve accuracy. Qian et al.93 proposed a new force estimator called fast-\nwarp estimator, which is a modification based on the space warp coordinate transformation\n(SWCT) estimator.7,9,10,94 Within the deep learning QMC framework, the fast-warp estima-\ntor produced smaller variance while being more computationally efficient compared with\nthe SWCT estimator, making it the preferred estimator for force calculation. The authors\nalso analyzed the performance of periodic input features for force calculations, showing\nunstable results of the SWCT estimator and highlighting the advantage of the fast-warp\nestimator.\nThe calculation of stress tensor in VMC does not suffer from the infinite variance problem,\nand the underlying quantum mechanics theory is well established.95,96 Qian et al.93 imple-\nmented the stress tensor estimator and tested it on the lithium hydride crystal. The diagonal\ncomponent of the stress tensor fits very well with the equation of state curve, validating the\nstress calculation.\n3.2 Other periodic systems\n3.2.1 Superfluid\nSuperconductivity and superfluidity are two famous microscopic quantum phenomena and\nare intricately interconnected like two sides of one coin. Both phenomena can be under-\nstood with regard to condensation of bosons or paired fermions. In these exotic phases,\ncouples of fermions are bound together by attractive interactions, which are absent in nor-\nmal solids. The weak pairing regime is known as the Bardeen-Cooper-Schrieffer (BCS)\nstate, associated with superconductivity, while the Bose-Einstein condensate (BEC) repre-\n19\nsents the state of tightly bound fermion pairs, characterizing superfluidity. The BCS-BEC\ncrossover, a transition followed by increasing interaction strength, illustrates the continuity\nwithin these two limits. The unitary Fermi gas (UFG) system modulated by an adjustable\ninteraction serves as an ideal platform to simulate BCS-BEC crossover both experimentally\nand numerically, which has drawn a lot of attention.\nRecently, the deep learning QMC was applied to the unitary Fermi gas (UFG) system97,98.\nThe Hamiltonian for this system incorporates a short-range, strongly attractive interaction\nbetween particles of opposite spins:\nˆH = −1\n2\nN\nX\ni\n∇2\ni +\nN/2\nX\nij\nU(r↑\ni −r↓\nj),\n(31)\nwhere U is the modified P¨oschl-Teller interaction U(r) = −2ν0µ2/ cosh2(µr).\nTo more\naccurately capture the electronic structure of the UFG system, wave function ansatzes\nincorporating pairing features, such as the antisymmetric geminal power singlet (AGPs)97\nand a more generalized Pfaffian form98, were employed:\nΨAGPs(Pfaffian) = Pf\nh\nϕ(r↑(↑,↓)\ni\n, r↓(↑,↓)\nj\n)\ni\n,\n(32)\nwhere AGPs specifically pairs electrons of opposite spins, whereas the Pfaffian accounts\nfor the pairing of any two electrons. The emergence of pairing was suggested by gap\nanalysis for these pairing-structured ansatzes. In contrast, the Slater-Jastrow type ansatz,\neven when augmented with backflow, did not adequately capture such correlations. Fur-\nthermore, the specially designed ansatz combined with deep learning achieved the lowest\nenergy and outperformed the state-of-the-art results from fixed node diffusion Monte Carlo.\n3.2.2 Electron gas\nHomogeneous electron gas (HEG) is one of the simplest model systems with electron–\nelectron correlation in condensed matter physics, yet has proven to exhibit a wide range\nof phenomena. Different groups, such as Wilson et al.28, Cassella et al.29, Li et al.26,\nand Pescia et al.,99 applied deep learning QMC to study the energy of HEG. Wilson et\nal.28 found that by adding a backflow process where the coordinated are generated from\nthe neural network, the energy can be significantly lowered, and is comparable or better\nthan the fixed-node iterative backflow DMC (IB-DMC) results. Pescia et al.99 built a neural\nnetwork–based backflow ansatz with orders of magnitudes fewer parameters and carried\n20\nFigure 3: N = 27 spin polarized HEG. (a) Plane wave (gas) and Gaussian orbital (crystal)\nsingle-determinant Slater–Jastrow backflow (SJB) ground-state energies per electron rela-\ntive to FermiNet results. (b) One-electron density results from FermiNet with rs = 10 (left)\nand rs = 70 (right). (c) Order parameter for the Wigner crystal state, averaged over crystal\naxes. The order parameter rises sharply to a finite value at rs = 2, corresponding to the\nemergence of a crystalline state. Reprinted from ref. 29, CC BY 4.0.\nout calculation with N = 128 electrons, enabling future work on finite-size extrapolations to\nthe thermodynamic limit.\nBesides the energy calculation, the HEG system is of particular interest for hosting Wigner\ncrystal phases. As the electron density decreases, HEG systems are expected to transition\nfrom a Fermi-liquid phase to a Wigner crystal phase, where electrons spontaneously orga-\nnize into crystals. Despite this phenomenon being predicted years ago, accurately locating\nthe critical density for this phase transition remains a significant challenge. In traditional\nQuantum Monte Carlo (QMC) methods, different ansatzes are typically constructed for the\nFermi-liquid and Wigner crystal phases based on a priori knowledge, and the ground state\nis determined by comparing the energies of these different ansatzes. However, with the\nflexibility of neural networks, a single form of neural network-based ansatz can capture\ndifferent phases, providing a unified description of the phase transition and yielding more\naccurate results.\nSpecifically, Cassella et al.29 extended FermiNet with periodic input features Eq. (17b), and\nallowed representation of symmetry broken Wigner crystal state by rewriting Eq. (13) with\nϕj(ri, {r/i}) =\nX\nm\nνjmeikm·riuj(ri, {r/i}),\n(33)\n21\nwhere kj are supercell reciprocal lattice vectors up to the Fermi wave vector of the non-\ninteracting electron gas, and νjm are learnable parameters. Thus, the symmetry Eq. (9b)\ncan be broken, while symmetry Eq. (9a) is still satisfied to keep the ansatz valid. The neu-\nral network was applied to 27-electron spin-polarized HEG with different densities, and the\nresults are shown in Fig. 3. The energy accuracy of the neural network was very close to\ntraditional DMC using single-determinant Slater–Jastrow with backflow. By calculating the\norder parameter, i.e. the Fourier component of the one-electron density, they found that\nthe neural network was capable of expressing both the Fermi liquid phase and the Wigner\ncrystal phase, with the phase transition occurring around rs = 2. Overall, neural network\nwavefunctions show great potential in discovering novel quantum phases and other emer-\ngent phenomena in condensed matter.\n3.3 Dense hydrogen at finite temperature\nBesides solving electronic properties under zero temperature, deep learning QMC can\nalso be used in conjunction with other methods to compute finite temperature properties.\nAlthough molecular dynamics is one of the most commonly used methods to calculate the\nproperties under finite temperatures, we have to first converge the electron wavefunction\nand get the Born–Oppenheimer potential energy surface before moving the nucleus, which\nis very expensive.\nXie et al.100 developed a neural network–based variational free energy approach for dense\nhydrogen, using a neural network to represent the wavefunction under a specific proton\nconfiguration, and another normalizing flow network to model the proton distribution. These\ntwo neural networks are optimized simultaneously by minimizing free energy,\nF =\nE\nS∼p(S) [kBT log p(S) + Ev(S)] ,\n(34)\nwhere Ev(S) stands for the energy at given proton coordinates S, and the overall expecta-\ntion is taken under the proton probability density p(S). With this framework, the free energy,\nenergy, entropy, and pressure of high-temperature dense hydrogen can be calculated with\nminimal physical constraints.\n22\nConclusion\nNeural networks can serve as a universal, powerful and flexible ansatz for QMC, offering\naccurate treatment of electron correlations. By obtaining high-quality many-body wave-\nfunction within the VMC framework, the neural network enables accurate QMC calculations\nof many observables that are otherwise difficult to compute in projection-based QMC ap-\nproaches. With the flexibility of the neural network, wavefunction ansatz can be constructed\nwithout a priori knowledge of the ground state, enabling future studies of many interesting\nphenomena, including a diversity of phase transitions, quantum Hall physics, topological\nmaterials, etc.\nGoing forward, it is necessary to further reduce the high computational cost that is currently\nlimiting the application of neural network QMC. Employing pseudopotentials is a natural op-\ntion, but the computational cost arising from the nonlocal integration is still intractable. One\npotential solution is to transform the nonlocal potentials into local pseudo-Hamiltonian by\nmodifying the kinetic energy operator.101,102 Although this may result in a loss of accuracy, it\nis still acceptable when compared to the efficiency gain. In addition, applying more sophis-\nticated and efficient optimization schemes, such as the Wasserstein QMC approach103,\noffers potential acceleration in convergence and reduction in computational cost for solid\nsystems. Another favorable strategy is to use the embedding scheme, which involves treat-\ning the strongly-correlated parts, such as the defects, with neural network QMC, while\nemploying more cost-effective methods such as HF or DFT for other parts.\nThe flexibility of neural networks also enables modeling wavefunctions of multiple atomic\nconfigurations, which has been achieved for molecular systems and the applicability to\nsolid systems is yet to be explored. Specifically, neural networks for different systems can\nbe simultaneously trained with a large portion of parameters shared, drastically reducing\nthe total computational time compared to separate optimizations.25 An even more promis-\ning approach involves using a meta neural network to extract the atomic information and\nreparametrize the neural network for electron wavefunction.45,104–107 This approach not\nonly saves computational time but also enables the development of a transferable wave-\nfunction model. After pretraining this model on smaller fragments, it can be effectively\napplied to more extensive systems. Moreover, it is feasible to pretrain a foundational wave-\nfunction model, which can yield highly accurate energy results with only minimal compu-\n23\ntational effort. Given their ability to handle diverse atomic configurations and the transfer-\nability to larger systems, neural networks hold great promise for performing more efficient\nfinite-size error corrections and ensuring a reasonable computational for tasks requiring\na substantial amount of atomic configurations, such as phonon calculation and force field\ntraining.\nFunding Information\nJC is supported by the National Key R&D Program of China under Grant No. 2021YFA1400500,\nthe Strategic Priority Research Program of the Chinese Academy of Sciences under Grant\nNo. XDB33000000, the National Natural Science Foundation of China under Grant No.\n12334003, and the Beijing Municipal Natural Science Foundation under Grant No. JQ22001.\nReferences\n[1] Jensen F. Introduction to Computational Chemistry. 3rd ed. Chichester, UK ; Hobo-\nken, NJ: Wiley; 2017.\n[2] Sholl DS, Steckel JA. Density Functional Theory: A Practical Introduction. 1st ed.\nWiley; 2009.\n[3] Leng X, Jin F, Wei M, Ma Y. GW Method and Bethe–Salpeter Equation for Calculating\nElectronic Excitations. WIREs Computational Molecular Science. 2016;6(5):532-50.\n[4] Sorella S. Green Function Monte Carlo with Stochastic Reconfiguration. Physical\nReview Letters. 1998 May;80(20):4558-61.\n[5] Sorella S.\nGeneralized Lanczos Algorithm for Variational Quantum Monte Carlo.\nPhysical Review B. 2001 Jun;64(2):024512.\n[6] Neuscamman E, Umrigar CJ, Chan GKL. Optimizing Large Parameter Sets in Vari-\national Quantum Monte Carlo. Physical Review B. 2012 Jan;85(4):045103.\n[7] Filippi C, Umrigar CJ. Correlated Sampling in Quantum Monte Carlo: A Route to\nForces. Physical Review B. 2000 Jun;61(24):R16291-4.\n[8] Assaraf R, Caffarel M. Zero-Variance Zero-Bias Principle for Observables in Quan-\ntum Monte Carlo: Application to Forces. The Journal of Chemical Physics. 2003\nNov;119(20):10536-52.\n24\n[9] Sorella S, Capriotti L. Algorithmic Differentiation and the Calculation of Forces by\nQuantum Monte Carlo. The Journal of Chemical Physics. 2010 Dec;133(23):234111.\n[10] Nakano K, Raghav A, Sorella S. Space-Warp Coordinate Transformation for Efficient\nIonic Force Calculations in Quantum Monte Carlo. The Journal of Chemical Physics.\n2022 Jan;156(3):034101.\n[11] Nakano K, Morresi T, Casula M, Maezono R, Sorella S. Atomic Forces by Quantum\nMonte Carlo: Application to Phonon Dispersion Calculations. Physical Review B.\n2021 Mar;103(12):L121110.\n[12] Ly K, Ceperley D. Phonons of Metallic Hydrogen with Quantum Monte Carlo. The\nJournal of Chemical Physics. 2022 Jan;156(4):044108.\n[13] Foulkes WMC, Mitas L, Needs RJ, Rajagopal G. Quantum Monte Carlo Simulations\nof Solids. Reviews of Modern Physics. 2001 Jan;73(1):33-83.\n[14] Kim J, Baczewski AD, Beaudet TD, Benali A, Bennett MC, Berrill MA, et al. QMC-\nPACK: An Open Source Ab Initio Quantum Monte Carlo Package for the Electronic\nStructure of Atoms, Molecules and Solids. Journal of Physics: Condensed Matter.\n2018 Apr;30(19):195901.\n[15] Needs RJ, Towler MD, Drummond ND, L´opez R´ıos P, Trail JR. Variational and Dif-\nfusion Quantum Monte Carlo Calculations with the CASINO Code. The Journal of\nChemical Physics. 2020 Apr;152(15):154106.\n[16] Nakano K, Attaccalite C, Barborini M, Capriotti L, Casula M, Coccia E, et al. TUR-\nBORVB : A Many-Body Toolkit for Ab Initio Electronic Simulations by Quantum Monte\nCarlo. The Journal of Chemical Physics. 2020 May;152(20):204121.\n[17] Wheeler WA, Pathak S, Kleiner KG, Yuan S, Rodrigues JNB, Lorsung C, et al.\nPyQMC: An All-Python Real-Space Quantum Monte Carlo Module in PySCF. The\nJournal of Chemical Physics. 2023 Mar;158(11):114801.\n[18] Nomura Y, Darmawan AS, Yamaji Y, Imada M. Restricted Boltzmann Machine Learn-\ning for Solving Strongly Correlated Quantum Systems.\nPhysical Review B. 2017\nNov;96(20):205152.\n[19] Carleo G, Nomura Y, Imada M. Constructing Exact Representations of Quantum\n25\nMany-Body Systems with Deep Neural Networks. Nature Communications. 2018\nDec;9(1):1-11.\n[20] Han J, Zhang L, E W.\nSolving Many-Electron Schr¨odinger Equation Using Deep\nNeural Networks. Journal of Computational Physics. 2019 Dec;399:108929.\n[21] Pfau D, Spencer JS, Matthews AGDG, Foulkes WMC. Ab Initio Solution of the Many-\nElectron Schr¨odinger Equation with Deep Neural Networks. Physical Review Re-\nsearch. 2020 Sep;2(3):033429.\n[22] Hermann J, Sch¨atzle Z, No´e F.\nDeep-Neural-Network Solution of the Electronic\nSchr¨odinger Equation. Nature Chemistry. 2020 Oct;12(10):891-7.\n[23] Entwistle MT, Sch¨atzle Z, Erdman PA, Hermann J, No´e F. Electronic Excited States\nin Deep Variational Monte Carlo. Nature Communications. 2023 Jan;14(1):274.\n[24] Choo K, Mezzacapo A, Carleo G.\nFermionic Neural-Network States for Ab-Initio\nElectronic Structure. Nature Communications. 2020 May;11(1):2368.\n[25] Scherbela M, Reisenhofer R, Gerard L, Marquetand P, Grohs P. Solving the Elec-\ntronic Schr¨odinger Equation for Multiple Nuclear Geometries with Weight-Sharing\nDeep Neural Networks. Nature Computational Science. 2022 May;2(5):331-41.\n[26] Li X, Li Z, Chen J. Ab Initio Calculation of Real Solids via Neural Network Ansatz.\nNature Communications. 2022 Dec;13(1):7895.\n[27] Yoshioka N, Mizukami W, Nori F. Solving Quasiparticle Band Spectra of Real Solids\nUsing Neural-Network Quantum States. Communications Physics. 2021 May;4(1):1-\n8.\n[28] Wilson M, Moroni S, Holzmann M, Gao N, Wudarski F, Vegge T, et al. Neural Network\nAnsatz for Periodic Wave Functions and the Homogeneous Electron Gas. Physical\nReview B. 2023 Jun;107(23):235139.\n[29] Cassella G, Sutterud H, Azadi S, Drummond ND, Pfau D, Spencer JS, et al. Discov-\nering Quantum Phase Transitions with Fermionic Neural Networks. Physical Review\nLetters. 2023 Jan;130(3):036401.\n[30] Pescia G, Han J, Lovato A, Lu J, Carleo G.\nNeural-Network Quantum States\n26\nfor Periodic Systems in Continuous Space.\nPhysical Review Research. 2022\nMay;4(2):023138.\n[31] Hermann J, Spencer J, Choo K, Mezzacapo A, Foulkes WMC, Pfau D, et al. Ab Initio\nQuantum Chemistry with Neural-Network Wavefunctions. Nature Reviews Chem-\nistry. 2023 Oct;7(10):692-709.\n[32] Ewald PP. Die Berechnung Optischer Und Elektrostatischer Gitterpotentiale. An-\nnalen der Physik. 1921;369(3):253-87.\n[33] Toukmaji AY, Board JA. Ewald Summation Techniques in Perspective: A Survey.\nComputer Physics Communications. 1996 Jun;95(2-3):73-92.\n[34] Thijssen JM.\nComputational Physics.\nCambridge: Cambridge University Press;\n1999.\n[35] Jastrow R. Many-Body Problem with Strong Forces. Phys Rev. 1955 Jun;98:1479-84.\nAvailable from: https://link.aps.org/doi/10.1103/PhysRev.98.1479.\n[36] Kato T. On the Eigenfunctions of Many-Particle Systems in Quantum Mechanics.\nCommunications on Pure and Applied Mathematics. 1957;10(2):151-77.\n[37] Kwon Y. Effects of Three-Body and Backflow Correlations in the Two-Dimensional\nElectron Gas. Physical Review B. 1993;48(16):12037-46.\n[38] Kwon Y.\nEffects of Backflow Correlation in the Three-Dimensional Electron Gas:\nQuantum Monte Carlo Study. Physical Review B. 1998;58(11):6800-6.\n[39] L´opez R´ıos P, Ma A, Drummond ND, Towler MD, Needs RJ. Inhomogeneous Back-\nflow Transformations in Quantum Monte Carlo Calculations. Physical Review E. 2006\nDec;74(6):066701.\n[40] Bajdich M, Mitas L, Drobn´y G, Wagner LK, Schmidt KE. Pfaffian Pairing Wave Func-\ntions in Electronic-Structure Quantum Monte Carlo Simulations. Physical Review\nLetters. 2006 Apr;96(13):130201.\n[41] Bajdich M, Mitas L, Wagner LK, Schmidt KE. Pfaffian Pairing and Backflow Wave-\nfunctions for Electronic Structure Quantum Monte Carlo Methods. Physical Review\nB. 2008 Mar;77(11):115112.\n27\n[42] Casula M, Sorella S.\nGeminal Wave Functions with Jastrow Correlation: A First\nApplication to Atoms. The Journal of Chemical Physics. 2003 Oct;119(13):6500-11.\n[43] Raghav A, Maezono R, Hongo K, Sorella S, Nakano K. Toward Chemical Accuracy\nUsing the Jastrow Correlated Antisymmetrized Geminal Power Ansatz. Journal of\nChemical Theory and Computation. 2023 Apr;19(8):2222-9.\n[44] Gerard L, Scherbela M, Marquetand P, Grohs P.\nGold-Standard Solutions to the\nSchr¨odinger Equation Using Deep Learning: How Much Physics Do We Need? In:\nKoyejo S, Mohamed S, Agarwal A, Belgrave D, Cho K, Oh A, editors. Advances in\nNeural Information Processing Systems. vol. 35. Curran Associates, Inc.; 2022. p.\n10282-94.\n[45] Gao N, G¨unnemann S. Ab-Initio Potential Energy Surfaces by Pairing GNNs with\nNeural Wave Functions. In: International Conference on Learning Representations.\narXiv; 2021. .\n[46] Lin J, Goldshlager G, Lin L. Explicitly antisymmetrized neural network layers for vari-\national Monte Carlo simulation. Journal of Computational Physics. 2023;474:111765.\n[47] von Glehn I, Spencer JS, Pfau D.\nA Self-Attention Ansatz for Ab-Initio Quantum\nChemistry. In: The Eleventh International Conference on Learning Representations,\nICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net; 2023. .\n[48] Li R, Ye H, Jiang D, Wen X, Wang C, Li Z, et al. A computational framework for\nneural network-based variational Monte Carlo with Forward Laplacian. Nature Ma-\nchine Intelligence. 2024 Feb;6(2):209-19.\nAvailable from: https://doi.org/10.\n1038/s42256-024-00794-x.\n[49] Amari Si. Natural Gradient Works Efficiently in Learning. Neural Computation. 1998\nFeb;10(2):251-76.\n[50] Ly A, Marsman M, Verhagen J, Grasman RPPP, Wagenmakers EJ. A Tutorial on\nFisher Information. Journal of Mathematical Psychology. 2017 Oct;80:40-55.\n[51] Facchi P, Kulkarni R, Man’ko VI, Marmo G, Sudarshan ECG, Ventriglia F. Classical\nand Quantum Fisher Information in the Geometrical Formulation of Quantum Me-\nchanics. Physics Letters A. 2010 Nov;374(48):4801-3.\n28\n[52] Aniello P, Marmo G, Volkert GF. Classical Tensors from Quantum States. Interna-\ntional Journal of Geometric Methods in Modern Physics. 2009 May;06(03):369-83.\n[53] Martens J, Grosse R. Optimizing Neural Networks with Kronecker-factored Approx-\nimate Curvature. In: Proceedings of the 32nd International Conference on Machine\nLearning. PMLR; 2015. p. 2408-17.\n[54] Nightingale MP, Melik-Alaverdian V.\nOptimization of Ground- and Excited-State\nWave Functions and van Der Waals Clusters.\nPhysical Review Letters. 2001\nJul;87(4):043401.\n[55] Toulouse J, Umrigar CJ. Optimization of Quantum Monte Carlo Wave Functions by\nEnergy Minimization. The Journal of Chemical Physics. 2007 Feb;126(8):084102.\n[56] Umrigar CJ, Nightingale MP, Runge KJ. A Diffusion Monte Carlo Algorithm with Very\nSmall Time-step Errors. The Journal of Chemical Physics. 1993 Aug;99(4):2865-90.\n[57] L¨uchow A. Quantum Monte Carlo Methods: Quantum Monte Carlo Methods. Wi-\nley Interdisciplinary Reviews: Computational Molecular Science. 2011 May;1(3):388-\n402.\n[58] Ren W, Fu W, Wu X, Chen J. Towards the Ground State of Molecules via Diffusion\nMonte Carlo on Neural Networks. Nature Communications. 2023 Apr;14(1):1860.\n[59] Umari P, Willamson AJ, Galli G, Marzari N. Dielectric Response of Periodic Sys-\ntems from Quantum Monte Carlo Calculations.\nPhysical Review Letters. 2005\nNov;95(20):207602.\n[60] Annaberdiyev A, Wang G, Melton CA, Bennett MC, Shulenburger L, Mitas L. A New\nGeneration of Effective Core Potentials from Correlated Calculations: 3d Transition\nMetal Series. The Journal of Chemical Physics. 2018 Oct;149(13):134108.\n[61] Burkatzki M, Filippi C, Dolg M.\nEnergy-Consistent Pseudopotentials for Quan-\ntum\nMonte\nCarlo\nCalculations.\nThe\nJournal\nof\nChemical\nPhysics.\n2007\nJun;126(23):234105.\n[62] Li X, Fan C, Ren W, Chen J. Fermionic Neural Network with Effective Core Potential.\nPhysical Review Research. 2022 Jan;4(1):013021.\n29\n[63] Drummond ND, Needs RJ, Sorouri A, Foulkes WMC. Finite-Size Errors in Continuum\nQuantum Monte Carlo Calculations. Physical Review B. 2008 Sep;78(12):125106.\n[64] Lin C, Zong FH, Ceperley DM. Twist-Averaged Boundary Conditions in Continuum\nQuantum Monte Carlo Algorithms. Physical Review E. 2001 Jun;64(1):016702.\n[65] Monkhorst HJ, Pack JD. Special Points for Brillouin-zone Integrations. Physical Re-\nview B. 1976 Jun;13(12):5188-92.\n[66] Baldereschi A. Mean-Value Point in the Brillouin Zone. Physical Review B. 1973\nJun;7(12):5212-5.\n[67] Rajagopal G, Needs RJ, Kenny S, Foulkes WMC, James A. Quantum Monte Carlo\nCalculations for Solids Using Special $k$ Points Methods. Physical Review Letters.\n1994 Oct;73(14):1959-62.\n[68] Dagrada M, Karakuzu S, Vildosola VL, Casula M, Sorella S.\nExact Special\nTwist Method for Quantum Monte Carlo Simulations.\nPhysical Review B. 2016\nDec;94(24):245108.\n[69] Mihm TN, Sch¨afer T, Ramadugu SK, Weiler L, Gr¨uneis A, Shepherd JJ. A shortcut\nto the thermodynamic limit for quantum many-body calculations of metals. Nature\nComputational Science. 2021;1(12):801-8.\n[70] Chiesa S, Ceperley DM, Martin RM, Holzmann M.\nFinite-Size Error in Many-\nBody Simulations with Long-Range Interactions.\nPhysical Review Letters. 2006\nAug;97(7):076404.\n[71] Motta M, Ceperley DM, Chan GKL, Gomez JA, Gull E, Guo S, et al. Towards the\nSolution of the Many-Electron Problem in Real Materials: Equation of State of the\nHydrogen Chain with State-of-the-Art Many-Body Methods. Physical Review X. 2017\nSep;7(3):031059.\n[72] Dappe YJ, Oszwaldowski R, Pou P, Ortega J, P´erez R, Flores F. Local-Orbital Occu-\npancy Formulation of Density Functional Theory: Application to Si, C, and Graphene.\nPhysical Review B. 2006 Jun;73(23):235124.\n[73] Birch F.\nFinite Elastic Strain of Cubic Crystals.\nPhys Rev. 1947 Jun;71:809-24.\nAvailable from: https://link.aps.org/doi/10.1103/PhysRev.71.809.\n30\n[74] Nolan SJ, Gillan MJ, Alf`e D, Allan NL, Manby FR. Calculation of Properties of Crys-\ntalline Lithium Hydride Using Correlated Wave Function Theory. Physical Review B.\n2009 Oct;80(16):165109.\n[75] Casula M, Filippi C, Sorella S. Diffusion Monte Carlo Method with Lattice Regular-\nization. Physical Review Letters. 2005 Sep;95(10):100201.\n[76] Motta M, Zhang S. Ab Initio Computations of Molecular Systems by the Auxiliary-\nField Quantum Monte Carlo Method.\nWIREs Computational Molecular Science.\n2018;8(5):e1364.\n[77] Fu W, Ren W, Chen J. Variance Extrapolation Method for Neural-Network Variational\nMonte Carlo. Machine Learning: Science and Technology. 2024 Jan;5(1):015016.\n[78] King-Smith RD, Vanderbilt D. Theory of Polarization of Crystalline Solids. Physical\nReview B. 1993 Jan;47(3):1651-4.\n[79] Resta R. Macroscopic Polarization in Crystalline Dielectrics: The Geometric Phase\nApproach. Reviews of Modern Physics. 1994;66(3):899-915.\n[80] Resta R, Vanderbilt D. Theory of Polarization: A Modern Approach. In: Physics\nof Ferroelectrics. vol. 105. Berlin, Heidelberg: Springer Berlin Heidelberg; 2007. p.\n31-68.\n[81] Kirtman B, Lacivita V, Dovesi R, Reis H.\nElectric Field Polarization in Con-\nventional Density Functional Theory: From Quasilinear to Two-Dimensional and\nThree-Dimensional Extended Systems.\nThe Journal of Chemical Physics. 2011\nOct;135(15):154101.\n[82] Umari P, Marzari N. Linear and Nonlinear Susceptibilities from Diffusion Quantum\nMonte Carlo: Application to Periodic Hydrogen Chains. The Journal of Chemical\nPhysics. 2009 Sep;131(9):094104.\n[83] Souza I, ´I˜niguez J, Vanderbilt D.\nFirst-Principles Approach to Insulators in Finite\nElectric Fields. Physical Review Letters. 2002 Aug;89(11):117602.\n[84] Li X, Qian Y, Chen J. Electric Polarization from a Many-Body Neural Network Ansatz.\nPhysical Review Letters. 2024 Apr;132(17):176401.\n31\n[85] Barrera GD, Colognesi D, Mitchell PCH, Ramirez-Cuesta AJ.\nLDA or GGA? A\nCombined Experimental Inelastic Neutron Scattering and Ab Initio Lattice Dynam-\nics Study of Alkali Metal Hydrides. Chemical Physics. 2005 Oct;317(2):119-29.\n[86] Staritzky Eugene, Walker DI. Crystallographic Data. 124. Lithium Hydride, LiH; 125.\nLithium Deuteride, LiD. Analytical Chemistry. 1956 Jun;28(6):1055-5.\n[87] Ghandehari K, Luo H, Ruoff AL, Trail SS, DiSalvo FJ. Band Gap and Index of Re-\nfraction of CsH to 251 GPa. Solid State Communications. 1995;95(6):385-8.\n[88] Batsanov SS, Ruchkin ED, Poroshina IA. Refractive Indices of Solids. SpringerBriefs\nin Applied Sciences and Technology. Singapore: Springer Singapore; 2016.\n[89] Hammond BL, Lester WA, Reynolds PJ. Monte Carlo Methods in Ab Initio Quantum\nChemistry. vol. 1 of World Scientific Lecture and Course Notes in Chemistry. WORLD\nSCIENTIFIC; 1994.\n[90] Chiesa S, Ceperley DM, Zhang S.\nAccurate, Efficient, and Simple Forces\nComputed with Quantum Monte Carlo Methods.\nPhysical Review Letters. 2005\nJan;94(3):036404.\n[91] Badinski A, Haynes PD, Trail JR, Needs RJ. Methods for Calculating Forces within\nQuantum Monte Carlo Simulations. Journal of Physics: Condensed Matter. 2010\nFeb;22(7):074202.\n[92] Qian Y, Fu W, Ren W, Chen J.\nInteratomic Force from Neural Network Based\nVariational Quantum Monte Carlo.\nThe Journal of Chemical Physics. 2022\nOct;157(16):164104.\n[93] Qian Y, Li X, Chen J. Force and Stress Calculation with Neural Network Wavefunction\nfor Solids. Faraday Discussions. 2024 Apr.\n[94] Umrigar CJ.\nTwo Aspects of Quantum Monte Carlo: Determination of Accurate\nWavefunctions and Determination of Potential Energy Surfaces of Molecules. In-\nternational Journal of Quantum Chemistry. 1989;36(S23):217-30.\n[95] Nielsen OH, Martin RM. Quantum-Mechanical Theory of Stress and Force. Physical\nReview B. 1985 Sep;32(6):3780-91.\n32\n[96] Martin RM.\nElectronic Structure: Basic Theory and Practical Methods.\n2nd ed.\nCambridge University Press; 2020.\n[97] Lou WT, Sutterud H, Cassella G, Foulkes WMC, Knolle J, Pfau D, et al.\nNeural\nWave Functions for Superfluids. Phys Rev X. 2024 May;14:021030. Available from:\nhttps://link.aps.org/doi/10.1103/PhysRevX.14.021030.\n[98] Kim J, Pescia G, Fore B, Nys J, Carleo G, Gandolfi S, et al. Neural-network quan-\ntum states for ultra-cold Fermi gases. Communications Physics. 2024 May;7(1):148.\nAvailable from: https://doi.org/10.1038/s42005-024-01613-w.\n[99] Pescia\nG,\nNys\nJ,\nKim\nJ,\nLovato\nA,\nCarleo\nG.\nMessage-Passing\nNeu-\nral Quantum States for the Homogeneous Electron Gas. American Physical\nSociety;\n2023.\nAvailable from:\nhttps://journals.aps.org/prb/accepted/\n74075Y28S541258187421f85c8b6f701fa59efc39.\n[100] Xie H, Li ZH, Wang H, Zhang L, Wang L. Deep Variational Free Energy Approach to\nDense Hydrogen. Physical Review Letters. 2023 Sep;131(12):126501.\n[101] Bachelet GB, Ceperley DM, Chiocchetti MGB. Novel Pseudo-Hamiltonian for Quan-\ntum Monte Carlo Simulations. Physical Review Letters. 1989 May;62(18):2088-91.\n[102] Foulkes WMC, Schluter M.\nPseudopotentials with Position-Dependent Electron\nMasses. Physical Review B. 1990 Dec;42(18):11505-29.\n[103] Neklyudov K, Nys J, Thiede L, Alvarez JFC, Liu Q, Welling M, et al. Wasserstein\nQuantum Monte Carlo: A Novel Approach for Solving the Quantum Many-Body\nSchr¨odinger Equation. In: Thirty-Seventh Conference on Neural Information Pro-\ncessing Systems; 2023. .\n[104] Gao N, G¨unnemann S. Sampling-Free Inference for Ab-Initio Potential Energy Sur-\nface Networks. In: The Eleventh International Conference on Learning Representa-\ntions; 2022. .\n[105] Gao N, G¨unnemann S. Generalizing Neural Wave Functions. In: Krause A, Brun-\nskill E, Cho K, Engelhardt B, Sabato S, Scarlett J, editors. Proceedings of the 40th\nInternational Conference on Machine Learning. vol. 202 of Proceedings of Machine\nLearning Research. PMLR; 2023-07-23/2023-07-29. p. 10708-26.\n33\n[106] Scherbela M, Gerard L, Grohs P. Variational Monte Carlo on a Budget – Fine-tuning\nPre-Trained Neural Wavefunctions. In: NeurIPS 2023; 2023. .\n[107] Scherbela M, Gerard L, Grohs P. Towards a Transferable Fermionic Neural Wave-\nfunction for Molecules. Nature Communications. 2024 Jan;15(1):120.\n34\n",
  "categories": [
    "physics.chem-ph",
    "cond-mat.str-el",
    "physics.comp-ph"
  ],
  "published": "2024-06-30",
  "updated": "2024-06-30"
}