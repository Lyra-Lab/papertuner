{
  "id": "http://arxiv.org/abs/2407.01264v2",
  "title": "SignCLIP: Connecting Text and Sign Language by Contrastive Learning",
  "authors": [
    "Zifan Jiang",
    "Gerard Sant",
    "Amit Moryossef",
    "Mathias Müller",
    "Rico Sennrich",
    "Sarah Ebling"
  ],
  "abstract": "We present SignCLIP, which re-purposes CLIP (Contrastive Language-Image\nPretraining) to project spoken language text and sign language videos, two\nclasses of natural languages of distinct modalities, into the same space.\nSignCLIP is an efficient method of learning useful visual representations for\nsign language processing from large-scale, multilingual video-text pairs,\nwithout directly optimizing for a specific task or sign language which is often\nof limited size.\n  We pretrain SignCLIP on Spreadthesign, a prominent sign language dictionary\nconsisting of ~500 thousand video clips in up to 44 sign languages, and\nevaluate it with various downstream datasets. SignCLIP discerns in-domain\nsigning with notable text-to-video/video-to-text retrieval accuracy. It also\nperforms competitively for out-of-domain downstream tasks such as isolated sign\nlanguage recognition upon essential few-shot prompting or fine-tuning.\n  We analyze the latent space formed by the spoken language text and sign\nlanguage poses, which provides additional linguistic insights. Our code and\nmodels are openly available.",
  "text": "SignCLIP: Connecting Text and Sign Language by Contrastive Learning\nZifan Jiang, Gerard Sant, Amit Moryossef,\nMathias Müller, Rico Sennrich, Sarah Ebling\nUniversity of Zurich\njiang@cl.uzh.ch\nAbstract\nWe present SignCLIP, which re-purposes CLIP\n(Contrastive Language-Image Pretraining) to\nproject spoken language text and sign language\nvideos, two classes of natural languages of dis-\ntinct modalities, into the same space. SignCLIP\nis an efficient method of learning useful visual\nrepresentations for sign language processing\nfrom large-scale, multilingual video-text pairs,\nwithout directly optimizing for a specific task\nor sign language which is often of limited size.\nWe pretrain SignCLIP on Spreadthesign, a\nprominent sign language dictionary consisting\nof ∼500 thousand video clips in up to 44 sign\nlanguages, and evaluate it with various down-\nstream datasets. SignCLIP discerns in-domain\nsigning with notable text-to-video/video-to-text\nretrieval accuracy. It also performs competi-\ntively for out-of-domain downstream tasks such\nas isolated sign language recognition upon es-\nsential few-shot prompting or fine-tuning.\nWe analyze the latent space formed by the spo-\nken language text and sign language poses,\nwhich provides additional linguistic insights.\nOur code and models are openly available1.\n1\nIntroduction\nSign(ed) languages are the primary communication\nmeans for ∼70 million deaf people worldwide2.\nThey use the visual-gestural modality to convey\nmeaning through manual articulations in combina-\ntion with non-manual elements like the face and\nbody (Sandler and Lillo-Martin, 2006). Sign lan-\nguage processing (SLP) (Bragg et al., 2019; Yin\net al., 2021) is a subfield of natural language pro-\ncessing (NLP) that is intertwined with computer\nvision (CV) and sign language linguistics.\nSLP spans the tasks of sign language recogni-\ntion (Adaloglou et al., 2021), translation (De Coster\n1https://github.com/J22Melody/fairseq/tree/\nmain/examples/MMPT\n2https://wfdeaf.org/our-work/\n…\nhouse\n<American SL>\nhouse\n<British SL>\nhouse\n<Japanese SL>\nI painted my \nhouse gray.\n<American SL>\nI am a \nhousewife.\n<lithuanian SL>\n…\nSignCLIP\nVideo\nEncoder\nSignCLIP\nText\nEncoder\nFigure 1: Illustration of SignCLIP, comprising a text\nencoder and a video encoder jointly trained on pairs\nof text and multilingual signing examples. Every sign\nis articulated in diverse languages and contexts with\nsubtle differences in hand shape, movement, place of\narticulation, etc. The screenshots of the videos are from\nSpreadthesign and the matrix part is taken from CLIP.\net al., 2023), and production (Rastgoo et al., 2021).\nTypical datasets, equipped with spoken language\ntext and sign language glosses in addition to videos,\nthat support SLP research are RWTH-PHOENIX-\nWeather 2014T, in German Sign Language (DGS),\nintroduced by Forster et al. (2014); Camgoz et al.\n(2018); and CSL-Daily, in Chinese Sign Language\n(CSL), introduced by Zhou et al. (2021). However,\nadvances on a specific task/dataset/language are\noften limited and non-transferable to more generic\nand challenging settings (Müller et al., 2022, 2023)\ndue to the small, domain-specific vocabulary (1,066\nand 2,000 signs, respectively) and data size (11 and\n23 hours, respectively). Recent sign language cor-\npora with a scale of more than a thousand signing\nhours have emerged for relatively high-resource\nsign languages, e.g., BOBSL (Albanie et al., 2021)\nfor British Sign Language (BSL) and YouTube-\nASL (Uthus et al., 2024) for American Sign Lan-\nguage (ASL), a subset of YouTube-SL-25 (Tanzer\nand Zhang, 2024). JWSign (Gueuwou et al., 2023)\nis also a notable corpus that consists of 2,530 hours\narXiv:2407.01264v2  [cs.CL]  6 Oct 2024\nof Bible translations in 98 sign languages.\nIn the meanwhile, outside the world of SLP, there\nis great progress in deep pretrained models of differ-\nent modalities, GPT (Achiam et al., 2023) for text,\nmasked autoencoders (He et al., 2022) for images,\nand wav2vec 2.0 (Baevski et al., 2020) for speech,\nto name a few. They are commonly pretrained on a\nhuge amount of data (e.g., over 15 trillion tokens\nfor Llama 3, Touvron et al. (2023)) with very little\nor weak supervision, but present striking multilin-\ngual and multi-task ability, for zero-shot prediction,\nfine-tuning, and representation learning.\nIdeally, the visual aspect of sign languages, i.e.,\nlexical similarity due to iconicity (Johnston and\nSchembri, 2007) (illustrated in Figure 1) makes\ntransferring between different sign languages easier\nthan the text of different spoken languages or even\nscripts. The latter faces unfair tokenization issues\n(Petrov et al., 2024) and out-of-vocabulary errors.\nAt the same time, unlike discrete text tokens (see\nthe comparison in Table 1), the dense continuous\nvideo signal is expensive to process computation-\nally and seems daunting for the above-mentioned\nself-supervised training approaches.\nSince SLP tasks and datasets usually involve\nboth the text and visual/signed modalities, we take\ninspiration from OpenAI’s CLIP model (Radford\net al., 2021) but use contrastive learning to connect\ntext with sign language videos instead of images.\nFor video understanding, follow-up work such as\nVideoCLIP (Xu et al., 2021) mainly deals with\ntasks including action recognition (Zhu et al., 2020)\nand VideoQA (Xu et al., 2016; Yu et al., 2018).\nHowever, both CLIP, VideoCLIP, and other existing\nmultimodal models understand visual content on\na coarse-grained level and generic domain and do\nnot address the intricacy of sign language. We\nshow the lack of sign language understanding in\ncontemporary AI models both intuitively (Figure 3\nin Appendix A) and empirically (Table 2).\nThis work uses sign-language-specific data to\ntrain a CLIP-like model for SLP. We first validate\nthe approach’s feasibility on fingerspelling, a sub-\nsystem of sign language, by a model named Finger-\nCLIP (§4), which correctly understands the finger-\nspelling of individual letters. We then curate the\nSpreadthesign3 dictionary as a large-scale pretrain-\ning dataset consisting of ∼500 hours of signing,\nas well as diverse public downstream task datasets\n3https://www.spreadthesign.com/.\nThe use of the\ndata is under a license granted by Spreadthesign.\nto run comprehensive pretraining, fine-tuning, and\nevaluation on full-fledged sign languages. By con-\ntrastive training on ∼500 thousand video-text pairs,\nwe obtain a multimodal and multilingual model\nnamed SignCLIP (§5), illustrated by Figure 1. Sign-\nCLIP excels at various SLP tasks and datasets and\npresents a compelling latent space for signed video\ncontent aligned with spoken language text (§6).\n2\nBackground: Sign Language\nRepresentation\nRepresentation is a key challenge for SLP. Unlike\nspoken languages, sign languages have no widely\nadopted written form. As sign languages are con-\nveyed through the visual-gestural modality, video\nrecording is the most straightforward way to cap-\nture them. The final goal of SignCLIP is to rep-\nresent a sign language video clip by an embed-\nding that aligns with a ubiquitous text encoder like\nSentence-BERT (Reimers and Gurevych, 2019).\nGenerally, end-to-end training on the raw videos\nis computationally costly, and various intermediate\nrepresentations alleviate this issue.\nVideoCLIP and Video Encoders\nOur work\nadapts VideoCLIP, which is pretrained by gen-\neral instructional videos from the HowTo100M\n(Miech et al., 2019) dataset. We aim at replacing\nHowTo100M with domain-specific sign language\nvideos, such as the dataset How2Sign (Duarte et al.,\n2021), albeit on a considerably smaller scale.\nVideos are very dense temporally (frame rate)\nand spatially (video resolution). A 3D-CNN-based\nvideo encoder is often used to extract informative\nfeatures with reduced dimensionalities for down-\nstream tasks. VideoCLIP uses an S3D (Zhang et al.,\n2018) model pretrained on HowTo100M that pro-\nduces one video token (i.e., a video embedding for\nthe temporal window) per second. For SLP, it is\npossible to use a video encoder pretrained specif-\nically on sign language videos. A prominent one\nis the I3D (Carreira and Zisserman, 2017) model\npretrained on the BSL sign language recognition\ntask (Varol et al., 2021) with the BSK-1K dataset\n(Albanie et al., 2020). A more recent approach to\nsimultaneously address temporal and spatial com-\nplexity is the Video Swin Transformer proposed by\nLiu et al. (2022), and Prajwal et al. (2022) trains\none such model for BSL fingerspelling recognition.\nPose Estimation\nA potentially more inter-\npretable and universal way of extracting sign\nlanguage-related features from videos is human\npose estimation (Zheng et al., 2023a), for example,\nusing MediaPipe Holistic (Lugaresi et al., 2019;\nGrishchenko and Bazarevsky, 2020). Each video\nframe is converted into the location (X, Y, Z) of\n543 full-body keypoints in a 3D space. However,\nthe immediate applicability of the pose estimation\nsystems for SLP is questionable (Moryossef et al.,\n2021), and known issues such as the lack of accu-\nrate depth information are presented (Holmes et al.,\n2024). Generally, there is a trade-off between user-\nfriendliness and accuracy for pose estimation tools\nand SLP researchers often prefer MediaPipe Holis-\ntic for the former (Selvaraj et al., 2022) over others\nlike OpenPose (Cao et al., 2019) and AlphaPose\n(Fang et al., 2022). An even more universal alter-\nnative approach to track keypoints named Tracking\nEverything Everywhere All at Once is proposed by\nWang et al. (2023). Sevilla et al. (2024) uses a\nsimilar CoTracker (Karaev et al., 2023) model to\nstudy sign language prosody. Such models produce\nsmoother signals than traditional pose estimation.\nDiscrete Representation\nNon-standard written\nforms of sign language, including SignWriting\n(Sutton, 1990), HamNoSys (Prillwitz and Zienert,\n1990), and glosses (Johnston, 2008), offer the pos-\nsibility to incorporate sign language content into a\ntext-based NLP pipeline (Jiang et al., 2023). How-\never, a good segmentation (Moryossef et al., 2023)\nand transcription model to process raw video input\nis first required, which is not well-researched and\nis not part of this paper.\nRecently, vector quantization (VQ) approaches\n(Van Den Oord et al., 2017) such as SignVQNet\n(Hwang et al., 2023) demonstrate the ability to con-\nvert the continuous signal of videos/poses to dis-\ncrete tokens similar to spoken language sub-words\n(Sennrich et al., 2016), which might be a promising\ndirection to pursue in future work.\nComparison\nIn this work, we only empirically\nexperiment with the video encoder and pose-based\nmethods since there are not yet mature and open so-\nlutions for the others at the time of writing. Given\na hypothetical 10-second, 30 FPS, 480p (640×480),\nRGB (3 channels) video of 12 consecutive signs,\nwe compare the dimensionalities of the most com-\nmon representations in Table 1. These approaches\ncompress the raw videos to a sequence of video\ntokens compatible with a Transformer (Vaswani\net al., 2017) or a pretrained language model for\nfurther training and processing (Gong et al., 2024).\nRepresentation\nTemporal\nSpatial\nInterpretable\nOriginal video\n10x30\n640×480×3\n–\nS3D (HowTo100M)\n10\n512\nno\nI3D (BSL-1K)\n10\n1024\nno\nMediaPipe Holistic\n10×30\n543×3\nyes\nVQ (e.g., SignVQNet)\n10\n1024*\nno\nSignWriting/HamNoSys/gloss\n12\n1024*\nyes\nTable 1: Temporal and spatial dimensions of different\nsign language representations for a 10-second, 30 FPS,\n480p (640×480), RGB (3 channels) video consisting of\n12 signs. In the parentheses are the datasets on which\nthe models are pretrained. For discrete representations,\nwe assume the embedding size to be 1024, marked with\nan asterisk (*), but this can be chosen arbitrarily.\n3\nModel Architecture\nWe follow the setups in VideoCLIP and reuse their\ncodebase4. The most essential model architecture\nwith minor modifications to adapt our experiments\nis described here. We take pairs of video and text\nsamples (v, t) as inputs, where for each video clip,\ncv is a sequence of continuous video frames and\nis processed by a video encoder fθve. This is then\nfollowed by a trainable MLP projection layer, fθvp,\nto project the embedding to the same dimension,\nd = 768, as the word embedding on the text side:\nxv = fθvp(stopgrad(fθve(cv))\n(1)\nThe frozen video encoder fθve is by default a\n3D-CNN network but can be replaced by any other\nvisual backbone or a black-box pose estimation\nsystem as summarized in Table 1. Likewise, text\ntoken vectors xt are acquired through embedding\nlookup from a frozen BERT model (Devlin et al.,\n2019). Then xv and xt are fed into two separate\ntrainable Transformers, fθv and fθt, followed by\naverage pooling over the sequence of the tokens to\nobtain the temporally aggregated embeddings:\nzv = Avg(fθv(xv)), zt = Avg(fθt(xt))\n(2)\nWe optionally add two linear multimodal projec-\ntion layers on top of zv and zt, which are missing\nin VideoCLIP but present in CLIP (see Figure 3\nof the CLIP paper). Finally, we employ the In-\nfoNCE loss (Oord et al., 2018) as the contrastive\nobjective to discern the relationship between the\nembedded N video-text pairs in each mini-batch\nand run contrastive training over the whole dataset.\n4https://github.com/facebookresearch/fairseq/\ntree/main/examples/MMPT\n4\nFingerCLIP\nAs a proof of concept, we first apply this contrastive\ntraining approach to Fingerspelling (Battison, 1978;\nWilcox, 1992; Brentari and Padden, 2001), a sub-\nsystem of sign languages heavily influenced by the\nsurrounding spoken languages. For concepts that\ndo not (yet) have associated signs (names of people,\nlocations, organizations, etc.), sign language users\nborrow a word of a spoken language by spelling it\nletter-by-letter with predefined signs for that lan-\nguage’s alphabet. Isolated fingerspelling recogni-\ntion5 can therefore be considered a toy task simi-\nlar to the MNIST (Deng, 2012) handwritten digits\nclassification task in CV. We name the model Fin-\ngerCLIP, a mini-version of SignCLIP (§5).\nDataset\nWe start with the RWTH German Finger-\nspelling Database (Dreuw et al., 2006), containing\n∼1400 videos of 35 DGS fingerspelling and num-\nber signs, five of which contain inherent motion.\nWe provide details and an illustration of the dataset\nin Appendix B. We split all examples randomly into\ntraining/validation/test sets at the ratio of 8:1:1.\nTraining Details\nWe adhere to most implementa-\ntion details outlined in VideoCLIP unless otherwise\nspecified. The two trainable Transformers, fθv and\nfθt, are initialized with the pretrained bert-base-\nuncased weights. We train 25 epochs within two\nhours on a Tesla V100-SXM2-32GB GPU, vali-\ndated by loss on the validation set. For contrastive\ntraining, we construct each batch as a collection of\n35 different signs with corresponding text prompts\n“Fingerspell the letter <letter_name> in DGS.”. By\noptimizing the InfoNCE loss, we move the embed-\nding of a sign closer to the paired text, and further\naway from the remaining 34 negative examples.\nWe test different combinations of video en-\ncoders: (a) S3D HowTo100M video features6; (b)\nI3D BSL-1K (M+D+A) video features7; (c) Medi-\naPipe Holistic pose estimation, and training strate-\ngies: (a) zero-shot VideoCLIP (no training); (b)\nfine-tuning VideoCLIP; (c) training from scratch.\nMediaPipe Holistic runs offline on a low-end\nCPU device. Pose estimation is normalized to a\n5Continuous fingerspelling recognition is more complex\nand is often solved by a CTC loss like speech recognition.\nFor more thorough research on fingerspelling, we recommend\nthe ChicagoFSWild+ dataset and Google’s ASL fingerspelling\nrecognition competition on Kaggle.\n6https://github.com/antoine77340/S3D_HowTo100M\n7https://www.robots.ox.ac.uk/~vgg/research/\nbslattend/\nconsistent scale by setting the mean width of each\nperson’s shoulders to 1, and the mid-point to (0, 0).\nThe leg values are removed since they are irrele-\nvant to signing. We further augment the data by\nrandomly rotating, shearing, and scaling the poses8.\nEvaluation\nWe view fingerspelling understand-\ning as a text-to-video/video-to-text retrieval task.\nThe candidates are ranked for both directions by\na dot-product-based similarity score to each tex-\nt/video query in the latent space. For the test text\nprompt of each sign, there is possibly more than\none correct video (e.g., the same letter signed by\ndifferent signers) in the test video pool, and they\nare all considered successful retrieval. We thus\nevaluate the text-to-video retrieval task by preci-\nsion@k, i.e., what percent of the top k candidates\nare correct answers. Each test video query has\nonly one correct text prompt out of the 35 possible\nprompts. We thus evaluate the video-text retrieval\ntask by recall@k, i.e., the chance to include the\nonly correct answer by taking the top k candidates.\nprecision@1 and recall@1 can be interpreted as\nthe retrieval accuracy, and in our video-to-text sce-\nnario where there is only one relevant text item,\nrecall@k also equals top-n accuracy. We keep us-\ning recall@k instead of accuracy in this paper for\ngenericity. We also add the metric median retrieval\nrank, i.e., the median value of the rank of the first\ncorrect answer in the candidate lists. We present\nthe experimental results in Table 2.\nDiscussion\nFingerCLIP distinguishes itself from\nthe supervised baseline method (Dreuw et al., 2006)\nin that it is not directly optimized for classification.\nInstead, a contrastive objective ties positive pairs\nof text and videos by learning meaningful embed-\ndings, which are then used for similarity-based\nretrieval. We find video-to-text retrieval reasonably\nmore challenging than text-to-video retrieval9 since\ntext-to-video retrieval is also often of less value, as\na trivial dictionary look-up does the job perfectly.\nE1, zero-shot VideoCLIP, presenting random\nguess results, shows that a common video under-\nstanding network pretrained on HowTo100M does\nnot necessarily address the nuance of sign language,\neven simply as fingerspelling. Therefore, dedicated\ntraining on sign language data is essential. Compar-\ning E1.1 and E1.2, neither is fine-tuning an existing\n8The Pose library implements related operations.\n9Note that the relatively low precision@5/10 values are\nuncomparable to the high recall@5/10 values, since the former\nmakes the task harder, while the latter simplifies the task.\nText-to-video\nVideo-to-text\nExperiment\nP@1↑\nP@5↑\nP@10↑\nMedianR↓\nR@1↑\nR@5↑\nR@10↑\nMedianR↓\nE0\nDreuw et al. (2006) (supervised HMM, appearance-based features)\n–\n–\n–\n–\n0.64\n–\n–\n–\nExplore training strategy\nE1\nVideoCLIP zero-shot (+ S3D HowTo100M video features)\n0.03\n0.02\n0.03\n22\n0.02\n0.14\n0.28\n18\nE1.1\nVideoCLIP fine-tuned (+ S3D HowTo100M video features)\n0.40\n0.36\n0.30\n2\n0.31\n0.75\n0.89\n2\nE1.2\nVideoCLIP trained from scratch (+ S3D HowTo100M video features)\n0.54\n0.35\n0.28\n1\n0.28\n0.69\n0.87\n3\nExplore video-based (I3D) features\nE2\nFingerCLIP trained from scratch (+ I3D BSL-1K video features)\n0.63\n0.47\n0.37\n1\n0.37\n0.78\n0.91\n2\nE2.1\nE2 + feature dimension average pooled from 1024 to 512\n0.74\n0.56\n0.44\n1\n0.47\n0.82\n0.94\n2\nExplore pose-based features\nE3\nFingerCLIP trained from scratch (MediaPipe Holistic pose features)\n0.89\n0.67\n0.42\n1\n0.68\n0.97\n1.00\n1\nE3.1\nE3 + dominant hand features only (26 times less keypoints)\n1.00\n0.72\n0.42\n1\n0.82\n0.99\n1.00\n1\nE3.2\nE3.1 + 2D augmentation on pose features (σ = 0.2)\n0.91\n0.74\n0.43\n1\n0.93\n1.00\n1.00\n1\nTable 2: FingerCLIP experimental results evaluated on the test set. P@k denotes precision@k, R@k denotes\nrecall@k, and MedianR denotes the median retrieval rank. The best score of each column is in bold. E0 is taken\nfrom Dreuw et al. (2006) as a baseline (R@1 derived from the best error rate 35.7%).\nVideoCLIP checkpoint helpful, so in the rest of the\npaper, models are trained from scratch.\nIn E2, I3D BSL-1K sign-language-specific fea-\ntures outperform HowTo100M S3D video features\n(E1.2), especially when downsampled to the same\ndimension as S3D (E2.1). MediaPipe Holistic pose\nestimation as a feature extractor (E3) works better\nthan 3D-CNN-based video encoders, presumably\nbecause it is more universal than an I3D model pre-\ntrained on a particular dataset and sign language.\nFor fingerspelling understanding, focusing on the\ndominant hand (E3.1) is beneficial10, which dras-\ntically reduces the number of keypoints from 543\nto 21. 2D data augmentation further improves the\noverall performance. Since MediaPipe Holistic as\nfeatures perform the best and are more interpretable\nand operable for potential data normalization and\naugmentation, we decide to use it as the frozen\nvideo encoder fθve for the rest of the paper11.\n5\nSignCLIP Pretraining\nTo fully realize the power of contrastive learning,\nwe train and evaluate SignCLIP on larger and more\ndiverse sign language datasets. For efficient experi-\nmenting, we start exploring datasets consisting of\nrelatively short-duration sign language video ex-\namples, e.g., for the task of isolated sign language\nrecognition (ISLR) instead of machine translation.\nIn Table 3, we summarize recent large-scale sign\nlanguage datasets in this context, focusing on ASL,\none of the highest-resourced languages in SLP.\n10Note that the DGS finger alphabet is one-handed.\n11An S3D/I3D model fine-tuned end-to-end may overcome\nsome limitations of pose estimation and yield superior per-\nformance for specific tasks. In this work, we trade pursuing\nstate-of-the-art numbers for the universality, interpretability,\nand cheap computation of pose estimation.\n5.1\nSpreadthesign Pretraining Dataset\nSpreadthesign is used as the pretraining dataset\nfor its large-scale and multilingual nature12. In\nthis work, we limit the text translations to English\nonly to avoid a cartesian product number of data\npoints, for the pretraining to be economical and\nsign-language-focused. After filtering English text,\nour dataset consists of 18,423 concepts in 41 sign\nlanguages, resulting in 456,913 video-text pairs\nwith a total duration of ∼500 hours. The data dis-\ntribution is presented in Appendix C in detail. We\nsplit all examples randomly into training, valida-\ntion, and test sets at the ratio of 98:1:1. A caveat of\nthis dataset is that there is normally only one sign-\ning example per text concept per sign language,\nwhich means the pretraining is prone to overfitting\nthe exact signs by particular signers. We still be-\nlieve that the diverse text-signing pairs will guide\nthe model to learn a useful visual representation.\nWe add the Spreadthesign data scale and that of\nCLIP and VideoCLIP to Table 3 for comparison.\nCLIP was trained on 400 million text-image pairs\ncollected from the Internet (500K queries and up\nto 20K pairs per query); VideoCLIP was pretrained\non 1.2 million videos from HowTo100M (each lasts\n∼6.5 minutes with ∼110 clip-text pairs). We addi-\ntionally add ImageNet (Deng et al., 2009), which\nhas a relatively closer scale to Spreadthesign.\n5.2\nTraining and Evaluation Details\nMost implementation details and evaluation proto-\ncols in FingerCLIP (§4) are reused for SignCLIP.\nThe text prompts now consist of the text content\n12https://www.spreadthesign.com/en.us/about/\nstatistics/. The data used in this work was crawled in\n2023 and might differ slightly from the official statistics.\nDataset\nLanguage\nType/Task\n#examples\n#signs/#classes\n#signers\nRWTH German Fingerspelling (Dreuw et al., 2006)\nDGS\nIsolated Fingerspelling\n1,400\n35\n20\nChicagoFSWild (Shi et al., 2018)\nASL\nContinuous fingerspelling\n7,304\n–\n160\nChicagoFSWild+ (Shi et al., 2019)\nASL\nContinuous fingerspelling\n55,232\n–\n260\nGoogle – American Sign Language Fingerspelling Recognition\nASL\nContinuous fingerspelling\n67,208\n–\n100\nWLASL (Li et al., 2020)\nASL\nISLR\n21,083\n2,000\n100\nASL Citizen (Desai et al., 2023)\nASL\nISLR\n83,399\n2,731\n52\nSem-Lex (Kezar et al., 2023)\nASL\nISLR\n91,148\n3,149\n41\nGoogle – Isolated Sign Language Recognition (i.e., asl-signs)\nASL\nISLR\n94,477\n250\n21\nGoogle – PopSign ASL v1.0 (Starner et al., 2024)\nASL\nISLR\n200,686\n250\n47\nHow2Sign (Duarte et al., 2021)\nASL\nContinuous signing\n35,000\n16,000\n11\nASL-LEX (Sehyr et al., 2021)\nASL\nDictionary (phonological)\n2,723\n2,723\n(unknown)\nSpreadthesign (SignCLIP, our filtered version)\nMultilingual\nDictionary\n456,913\n18,423*\n(unknown)\nImageNet (Deng et al., 2009)\n–\nImage classification\n1,431,167\n1,000\n–\nHowTo100M (Miech et al., 2019) (VideoCLIP)\n–\nVideo understanding\n136,000,000\n–\n–\nCLIP (Radford et al., 2021)\n–\nContrastive learning\n400,000,000\n–\n–\nTable 3: Summarization of datasets consisting of relatively short-duration video examples, compared with Spreadthe-\nsign and common CV datasets. SignCLIP has been tested with the datasets marked with a checkmark. asl-signs is a\nsubset of PopSign ASL v1.0. #signs/#classes for Spreadthesign is marked with an asterisk (*) since the signs of a\nconcept across different sign languages are barely classified as one sign.\nprepended with a spoken and a sign language tag,\ninspired by multilingual machine translation in\nJohnson et al. (2017). For example, the text prompt\nfor signing the phrase “Hello, can I help you?”\nin ASL is “<en> <ase> Hello, can I help you?”,\ntagged by the ISO 639-3 language code. Fitting\nmost examples, we limit the context length of the\nvideo Transformer fθv to 256 tokens, equivalent\nto a 10-second, 25 FPS video clip; and that of the\ntext Transformer fθt to be 64. Both Transformers\nare initialized with the pretrained bert-base-cased\nweights and trained on an NVIDIA A100-SXM4-\n80GB GPU to maximally afford a batch size of\n44813. We also measure the training efficiency by\nthe number of parameters and the training time.\nFor evaluation, we include the same video-to-\ntext metrics used in FingerCLIP and omit text-to-\nvideo for simplicity, as the latter correlates with and\nis more trivial than the former. We additionally test\nthe models with three recent ASL ISLR datasets in\na zero-shot way to evaluate out-of-domain general-\nization. We perform the following text preprocess-\ning to mitigate the effect of shifted text distribution\nwhen building text prompts from the raw gloss la-\nbels: (1) lowercasing the glosses; (2) removing\nthe gloss index for different variants of a sign14;\n(3) filtering their test sets by known text labels in\nSpreadthesign, which reduces the total number of\n13For reference, CLIP was trained with batch size 32,768\nand VideoCLIP was trained with batch size 512.\n14This operation might not be desired if the objective is sign\nclassification as different variants of a sign present different\nvisual forms. The main goal here is to test how well the sign\nembeddings align to text semantically regardless of the form.\nsigns/classes from the original test datasets.\nStarting from a baseline setup that resembles E3\nin FingerCLIP, we increase the video Transformer\nlayers from 6 to 12 and try linear multimodal pro-\njection layers after temporal pooling. For pose data,\nwe always simplify the face by using the contour\nkeypoints only, resulting in 203 keypoints. We fur-\nther experiment with the following modifications:\nPose Data Preprocessing\nBefore the regular nor-\nmalization, Dshoulders = 1, mid–point = (0, 0),\nas performed in all experiments, we further try\n(1) removing redundant keypoints and reposition-\ning the wrist to the hand model’s prediction (E6);\n(2) standardizing the keypoints by subtracting the\nmean pose values of all examples from Spreadthe-\nsign and dividing by the standard deviation (E6.1);\n(3) anonymizing by removing the appearance from\nthe first frame then adding the mean pose (E6.2).\nPose Data Augmentation\nAfter data normaliza-\ntion, we also employ pose-based data augmentation\ninspired by Boháˇcek and Hrúz (2022) at training\ntime to improve the models’ robustness, including\n(1) randomly flipping the poses horizontally; (2)\n2D spatial augmentation as done in FingerCLIP;\n(3) temporal augmentation of the signing speed by\nlinear interpolation between frames; (4) Gaussian\nnoise on keypoints.\n5.3\nExperimental Results and Discussion\nWe present the experimental results in Table 4. In\nthis scenario, an accurate retrieval is harder than\nFingerCLIP because there are 3,939 unique text\nprompts in the test set of 4,531 examples. The\nVideo-to-text (In-domain)\nVideo-to-text (out-of-domain)\nEfficiency\nExperiment\nR@1↑\nR@5↑\nR@10↑\nMedianR↓\nAS MedianR↓\nAC MedianR↓\nSL MedianR↓\n#Params↓\nTime↓\nE4\nBaseline\n0.33\n0.64\n0.77\n3/3939\n103/213\n253/1625\n455/1967\n175M\n29h\nInitial architectural changes\nE5\nE4 + six more video layers\n0.37\n0.68\n0.80\n2\n104\n192\n382\n217M\n28h\nE5.1\nE5 + multimodal projection layer\n0.38\n0.69\n0.80\n2\n104\n216\n418\n218M\n15h\nPose data preprocessing\nE6\nE5 + keypoint reduction & reposition\n0.37\n0.68\n0.80\n2\n105\n230\n665\n217M\n32h\nE6.1\nE6 + keypoint standardization\n0.40\n0.71\n0.83\n2\n99\n273\n551\n217M\n14h\nE6.2\nE6.1 + pose anonymization\n0.37\n0.68\n0.79\n2\n101\n251\n577\n217M\n40h\nPose data augmentation\nE7\nE5 + pose random flipping (p = 0.2)\n0.36\n0.67\n0.79\n2\n105\n200\n435\n217M\n29h\nE7.1\nE5 + spatial 2D augmentation (σ = 0.2)\n0.35\n0.65\n0.78\n3\n102\n219\n377\n217M\n39h\nE7.2\nE5 + temporal augmentation (σ = 0.2)\n0.39\n0.69\n0.80\n2\n104\n187\n372\n217M\n62h\nE7.3\nE5 + Gaussian noise (σ = 0.001)\n0.37\n0.68\n0.80\n2\n104\n198\n364\n217M\n29h\nTest-time-only normalization\nE8*\nE7.2 + flipping to right-handed\n0.39\n0.69\n0.80\n2\n103\n187\n359\n–\n–\nE8.1*\nE8 + pose anonymization (zero-shot-only)\n–\n–\n–\n–\n101\n214\n380\n–\n–\nTable 4: SignCLIP experimental results evaluated on the test set. R@k denotes recall@k, and MedianR denotes the\nmedian retrieval rank as well as the total number of unique signs/classes. AS = asl-signs, AC = ASL Citizen, SL =\nSem-Lex. Experiments marked with an asterisk (*) are test-time only. The best score of each column is in bold.\nin-domain results, E6.1 at the top (attained with in-\ncreased video layers and keypoint standardization),\nare impressive given the challenging nature. We at-\ntribute it to (1) the hypothesized multilingual trans-\nfer effect thanks to sign language iconicity; and\n(2) the broader supervision signal of contrastive\nlearning than fixed labels, coming from example\nphrases consisting of individual signs (Figure 1).\nOn the other hand, zero-shot performance on\nout-of-domain data is deficient. We posit that to\nreach noticeable performance on out-of-domain\ndata, few-shot learning or fine-tuning (§6.1) is es-\nsential given the current scale of pretraining. Nev-\nertheless, starting from E7, we experiment with\na few data augmentation techniques to increase\ndata variation given that in-domain standardization\nbenefits in-domain results but hurts out-of-domain\nresults. We then attempt test-time-only normaliza-\ntion to shift the test distribution of the poses closer\nto training. As a result, we manage to gradually\nfight against overfitting to the pretraining dataset\nand improve the overall zero-shot performance by\ntemporal augmentation (E7.2) and test-time pose\nflipping if the right hand is not present (E8). The\nformer provides robustness to signing speed change\nin videos, and the latter is helpful for unseen test\nexamples signed by left-handed signers.\n6\nDownstream Tasks and Analysis\nIn this section, we evaluate SignCLIP on several\ndownstream tasks and datasets, and we discuss the\nideas for further enhancement and evaluation in §9.\n6.1\nIsolated Sign Language Recognition\nWe provide a comprehensive evaluation for ASL\nISLR datasets in Table 5. For zero-shot predic-\ntion, we follow the optimal setups in E8/E8.1 but\nskip any text preprocessing on the raw glosses\nfor a fairer comparison. For few-shot learning,\nwe randomly sample 10 example videos for each\nclass from training data and use a nonparamet-\nric k-nearest neighbors (KNN) algorithm to in-\nfer test labels based on pose embedding similar-\nity (n_neighbors = #classes). We additionally\ntrain a supervised logistic regression classifier with\nall default settings offered by scikit-learn on top of\nthe embedded poses, also known as linear probe\n(Alain and Bengio, 2016). We also train Sign-\nCLIP models from scratch or fine-tune them from\nthe E7.2 checkpoint with the specific downstream\ndatasets for 25 epochs with batch size 256. Outliers\nlonger than 256 frames are removed.\nIn general, proper zero-shot prediction is hin-\ndered by distribution shift on both modalities: (1)\nfor asl-signs dataset, zero-shot prediction is flawed\nbecause we only have pose data normalized in\nan unknown way from Kaggle instead of the raw\nvideos; (2) PopSign ASL, the superset of asl-signs\nwith raw videos, is released later and the evaluation\non it (category game only) is therefore superior and\nmore meaningful; (3) ASL Citizen uses all upper\ncase glosses and Sem-Lex uses snake case glosses,\nboth unseen for the text encoder during training.\nFor asl-signs, we find pose anonymization eases\nthe domain shift issue in zero-shot (E8.1) and\nother settings. Besides, pose-based few-shot KNN\ngreatly improves the deficient zero-shot results,\nData\nModel\nR@1↑\nR@5↑\nR@10↑\nMedianR↓\nAS\nSignCLIP (E8.1) zero-shot\n0.01\n0.03\n0.05\n118/250\nSignCLIP (E8.1) 10-shot\n0.01\n0.03\n0.06\n109\nSignCLIP (E8.1) + linear\n0.12\n0.30\n0.41\n17\nSignCLIP train from scratch\n0.74\n0.91\n0.94\n1\nSignCLIP fine-tuned\n0.74\n0.91\n0.94\n1\nSignCLIP fine-tuned + linear\n0.78\n0.92\n0.94\n1\nSOTA Kaggle competition\n0.89*\n–\n–\n–\nPS\nSignCLIP (E8) zero-shot\n0.03\n0.10\n0.16\n62/250\nSignCLIP (E8) 10-shot\n0.04\n0.14\n0.22\n44\nSignCLIP (E8) + linear\n0.31\n0.58\n0.69\n4\nSignCLIP train from scratch\n0.83\n0.97\n0.99\n1\nSignCLIP fine-tuned\n0.84\n0.97\n0.98\n1\nSignCLIP fine-tuned + linear\n0.85\n0.97\n0.98\n1\nSOTA PopSign ASL\n0.84\n–\n–\n–\nAC\nSignCLIP (E8) zero-shot\n0.00\n0.00\n0.00\n1296/2731\nSignCLIP (E8) 10-shot\n0.05\n0.16\n0.23\n56\nSignCLIP (E8) + linear\n0.23\n0.47\n0.57\n7\nSignCLIP train from scratch\n0.39\n0.71\n0.80\n2\nSignCLIP fine-tuned\n0.46\n0.77\n0.84\n2\nSignCLIP fine-tuned + linear\n0.60\n0.84\n0.89\n1\nSOTA ASL Citizen\n0.63\n0.86\n0.91\n–\nSL\nSignCLIP (E8) zero-shot\n0.01\n0.02\n0.03\n853/3837\nSignCLIP (E8) 10-shot\n0.02\n0.06\n0.10\n235\nSignCLIP (E8) + linear\n0.15\n0.29\n0.36\n38\nSignCLIP train from scratch\n0.14\n0.30\n0.38\n32\nSignCLIP fine-tuned\n0.16\n0.34\n0.42\n22\nSignCLIP fine-tuned + linear\n0.30\n0.48\n0.55\n6\nSOTA Sem-Lex\n0.67*\n–\n–\n–\nSOTA + auxiliary phonology\n0.69*\n–\n–\n–\nTable 5: Comprehensive evaluations of SignCLIP on\nthe test set of ASL ISLR datasets. For ISLR, recall@k\nequals top-n accuracy. AS = asl-signs, PS = PopSign\n(the superset of AS), AC = ASL Citizen, SL = Sem-Lex.\nThe best score SignCLIP achieves on each dataset is\nin bold. SOTA numbers marked with an asterisk (*)\nare not directly comparable to ours. SOTA Kaggle is\ntrained with asl-signs but tested on a private test set;\nSOTA Sem-Lex is tested with a reduced test set of 2,731\nclasses that are aligned with ASL Citizen/ASL-LEX and\nis thus considered an easier objective.\nbypassing the influence of out-of-domain glosses.\nFinally, fine-tuning SignCLIP with a pretrained\ncheckpoint, compared to training from scratch on\nthe target dataset, yields more competitive or even\nbetter results than the state-of-the-art (SOTA) re-\nported in the previous literature.\n6.2\nSign Language Identification\nSign language identification (Gebre et al., 2013)\ncan be achieved by simply ranking text prompts\nof different sign languages without actual content,\ne.g., “<en> <ase>” for ASL. We evaluate the best\ncheckpoint E6.1 for in-domain test data and obtain\n0.99 recall@1, which solves the task perfectly with-\nout any direct supervision. However, the identifica-\ntion task is ill-defined in that the model can learn to\nidentify signers for particular sign languages. This\ntask is considered a simplified version of in-domain\nFigure 2: King – Man + Woman = Queen analogy re-\nvisited. 14 video examples of each sign are randomly\nsampled from the ASL Citizen dataset, embedded by a\nfine-tuned SignCLIP pose encoder, and then visualized\nby t-SNE (perplexity=15) with different shapes and col-\nors. Cluster centers are represented with a big symbol.\nvideo-to-text retrieval, as it eliminates the need to\nsimultaneously distinguish language and concept.\n6.3\nLatent Space Exploration\nWe make SignCLIP accessible via a RESTful API\nand perform analysis in a Colab notebook15.\nDistributional Hypothesis for Sign Language\nWe revisit the distributional hypothesis (Lenci and\nSahlgren, 2023) in a sign language context based\non the pose/sign embeddings instead of text/word\n(Mikolov et al., 2013). As illustrated by Figure 2,\nunlike a word with a discrete, unique text token,\neach sign has multiple realizations scattered in a\ncontinuous space. By aligning pose representation\nto text with contrastive learning, we have realized\ndistributional semantics in sign language, reflected\nby the cluster center of each sign, while maintain-\ning individual variance.\nWhat Is the Most Iconic Sign Crosslingually?\nIconicity is one of the key motivations for training\na multilingual SignCLIP (Figure 1), and now we\nask this linguistic question back to the model. We\nrank a sampled subset of 302 signs from Spreadthe-\nsign based on the variance of the pose embeddings\nacross 20+ different sign languages. As a result,\nthe sign for “scorpion” with a universal hook hand\nshape ranks at the top, and the motivational exam-\nple, i.e., the “house” sign, also ranks high (51/302).\nConversely, the signs for numbers rank low due\nto the diverse signing styles across languages. We\nappend the full rank in Appendix D.\n15https://colab.research.google.com/drive/\n1r8GtyZOJoy_tSu62tvi7Zi2ogxcqlcsz?usp=sharing\n7\nRelated Work\nContrastive learning has been increasingly used in\ncontemporary SLP work. We discuss some related\nwork in this session, which aims mainly at improv-\ning performance on dedicated tasks and datasets.\nTherefore, their goals are slightly different than\nSignCLIP, a universal pretrained sign language em-\nbedding model aligned to spoken language text.\nGan et al. (2023); Zheng et al. (2023b); Ye et al.\n(2024) focus on the small-scale RWTH-PHOENIX-\nWeather 2014T and CSL-Daily datasets. Gan et al.\n(2023) designs a visual contrastive loss and a se-\nmantic contrastive loss to tackle the CTC spike phe-\nnomenon in the Continuous Sign Language Recog-\nnition (CSLR) task and the exposure bias issue in\nthe Sign Language Translation (SLT) task. Zheng\net al. (2023b) proposes an explicit contrastive cross-\nmodal alignment between video frames and glosses\nin combination with an implicit cross-modal align-\nment by a variational autoencoder (VAE) for CSLR.\nYe et al. (2024) identifies a visual representation\ndensity issue in SLT and introduces a frame-wise\ncontrastive learning strategy to alleviate the issue\nand improve SLT.\nWong et al. (2023) has a similar sign embed-\nding idea as SignCLIP, namely Learnt Contrastive\nConcept, that aligns with word embeddings of the\nlinguistic labels for sign video and incorporates this\ninto ISLR pipelines for the WLASL and BOBSL\ndatasets. Raude et al. (2024), on the other hand,\ntackles CSLR on BOBSL by a multi-task setting,\nincluding two contrastive losses for sign-level and\nsentence-level retrieval, respectively. They show\nthat joint training for CSLR and sign language re-\ntrieval is mutually beneficial.\nLike our pretraining task framing introduced in\n§4, Cheng et al. (2023) explicitly formulates text-\nto-video/video-to-text retrieval as a cross-lingual\ncontrastive learning task. They address the data\nscarcity issue by combining a domain-agnostic and\na domain-aware video encoder and show its effec-\ntiveness on How2Sign and PHOENIX-2014T.\n8\nConclusion: Where Are We for SLP?\nThis work involves SLP, an interdisciplinary field\nthat suffers from low-resourceness compared to\nmainstream NLP and CV. To overcome the non-\ngeneralizability of a specific dataset/task/language,\nwe adapt (Video-)CLIP and propose SignCLIP.\nSignCLIP is trained on Spreadthesign, a multilin-\ngual, generic sign language dictionary consisting\nof ∼500 hours of signing in 41 sign languages, and\nis evaluated extensively for various purposes.\nSignCLIP demonstrates excellent in-domain per-\nformance but falls short of immediate zero-shot\nprediction on downstream ISLR tasks. This finding\nis consistent with previous CV studies (Li et al.,\n2017) before CLIP reached its scale. Similar mod-\nels trained on smaller datasets close to an Ima-\ngeNet scale performed much worse than supervised\nbaselines on common benchmarks. By comprehen-\nsively evaluating downstream ASL ISLR perfor-\nmance (Table 5), we also intend to shed some light\non data efficiency, as a fully supervised approach\nusually requires meticulous, task-specific data col-\nlection. This is more demanding for SLP, a niche\ndomain lacking human experts.\nAs a middle ground between full zero-shot pre-\ndiction and full supervision, few-shot learning or\nfine-tuning is essential to tackle domain shift and\nis more realistic given the present methodology\nand data scale. The cross-lingual transfer effect is\nprospective for sign language thanks to iconicity.\nWith no surprise, under a sensible data condition,\nthe universal pretraining paradigm that is trans-\nforming NLP and CV is also a promising research\ndirection for SLP.\n9\nLimitations\nThe main limitation of this work is the data scale,\nwhich interestingly is the primary breakthrough of\nthe original CLIP work. Apart from the inherent\ndata scarcity issue in SLP research, the concrete\nlimitations of SignCLIP come in a few aspects.\nFirst, to be not dataset/task/language-specific\nwhile possibly large-scale, we choose Spreadthe-\nsign as our pretraining dataset (§5.1), which is in-\ndeed highly multilingual and untied to any SLP\ntask. However, we cannot release the dataset pub-\nlicly, and future researchers who are interested\nin it have to first resolve or purchase the license\nfrom Spreadthesign. Fortunately, it is possible to\naugment or replace Spreadthesign with recently\nreleased public datasets of comparable or larger\nsize, which should increase data density and vari-\nation sign-wise, but with potentially decreased di-\nversity and balance among different sign languages.\nAlthough pretrained with a highly multilingual\ndataset, this paper focuses downstream task eval-\nuation on ASL, one of the highest-recourse sign\nlanguages. We leave further evaluation and explo-\nration of other sign languages to future work.\nSecondly, training large models on video data\nis very costly. The principle of this work is to fin-\nish every training process in less than three days\non a single Nvidia A100 GPU, and we managed\nthis goal (Table 4) by using pose-based models, a\nmoderate context length of 256 frames, and limit-\ning spoken language to English only. An ablation\nstudy on video length would be interesting to help\nunderstand the limitations of the current static em-\nbedding approach16. To scale up, multiple GPU\ntraining can be exploited, and architectural modi-\nfications that reduce the sequence length must be\nemployed, for which we refer to several techniques\ndiscussed in §2.\nAs a result of the above-mentioned optimiza-\ntions, we will be able to remove the limitation of\nthe relatively short context, and then train and eval-\nuate for longer-range tasks such as machine transla-\ntion and sign language production. This will make\nSignCLIP more versatile and generalizable, as well\nas support other types of deep pretrained networks\nfor SLP, such as large language models.\nAcknowledgments\nThis work is funded by the Swiss Innovation\nAgency (Innosuisse) flagship IICT (PFFS-21-47)\nand by the SIGMA project at the UZH Digital So-\nciety Initiative (DSI).\nWe thank the (meta-)reviewers for their valuable\nfeedback. We also thank Colin Leong, Gomèr Ot-\nterspeer, Jetske Adams, Oline Ranum, and Hamzah\nLuqman for their immediate interest and lively dis-\ncussion in our work.\nReferences\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama\nAhmad, Ilge Akkaya, Florencia Leoni Aleman,\nDiogo Almeida, Janko Altenschmidt, Sam Altman,\nShyamal Anadkat, et al. 2023. Gpt-4 technical report.\narXiv preprint arXiv:2303.08774.\nNikolas Adaloglou, Theocharis Chatzis, Ilias Papas-\ntratis, Andreas Stergioulas, Georgios Th Papadopou-\nlos, Vassia Zacharopoulou, George J Xydopoulos,\nKlimnis Atzakas, Dimitris Papazachariou, and Pet-\nros Daras. 2021. A comprehensive study on deep\nlearning-based methods for sign language recogni-\ntion. IEEE Transactions on Multimedia, 24:1750–\n1762.\n16This is also relevant to the polysemy phenomenon be-\ntween signs and words, i.e., one sign can mean more than one\nword and one word can also correspond to more than one sign.\nGuillaume Alain and Yoshua Bengio. 2016. Under-\nstanding intermediate layers using linear classifier\nprobes. arXiv preprint arXiv:1610.01644.\nSamuel Albanie, Gül Varol, Liliane Momeni, Triantafyl-\nlos Afouras, Joon Son Chung, Neil Fox, and Andrew\nZisserman. 2020. BSL-1K: Scaling up co-articulated\nsign language recognition using mouthing cues. In\nEuropean Conference on Computer Vision (ECCV).\nSamuel Albanie, Gül Varol, Liliane Momeni, Hannah\nBull, Triantafyllos Afouras, Himel Chowdhury, Neil\nFox, Bencie Woll, Rob Cooper, Andrew McParland,\nand Andrew Zisserman. 2021. BOBSL: BBC-Oxford\nBritish Sign Language Dataset.\nAlexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,\nand Michael Auli. 2020. wav2vec 2.0: A framework\nfor self-supervised learning of speech representations.\nAdvances in neural information processing systems,\n33:12449–12460.\nRobbin Battison. 1978. Lexical borrowing in American\nsign language. ERIC, Linstok Press, Inc., Silver\nSpring, Maryland 20901.\nMatyáš Boháˇcek and Marek Hrúz. 2022. Sign pose-\nbased transformer for word-level sign language recog-\nnition. In Proceedings of the IEEE/CVF winter con-\nference on applications of computer vision, pages\n182–191.\nDanielle Bragg, Oscar Koller, Mary Bellard, Larwan\nBerke, Patrick Boudreault, Annelies Braffort, Naomi\nCaselli, Matt Huenerfauth, Hernisa Kacorri, Tessa\nVerhoef, et al. 2019. Sign language recognition, gen-\neration, and translation: An interdisciplinary perspec-\ntive. In The 21st International ACM SIGACCESS\nConference on Computers and Accessibility, pages\n16–31.\nDiane Brentari and Carol Padden. 2001. A language\nwith multiple origins: Native and foreign vocabulary\nin american sign language. Foreign vocabulary in\nsign language: A cross-linguistic investigation of\nword formation, pages 87–119.\nNecati Cihan Camgoz, Simon Hadfield, Oscar Koller,\nHermann Ney, and Richard Bowden. 2018. Neural\nsign language translation. In IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR).\nZ. Cao, G. Hidalgo Martinez, T. Simon, S. Wei, and Y. A.\nSheikh. 2019. Openpose: Realtime multi-person\n2d pose estimation using part affinity fields. IEEE\nTransactions on Pattern Analysis and Machine Intel-\nligence.\nJoao Carreira and Andrew Zisserman. 2017. Quo vadis,\naction recognition? a new model and the kinetics\ndataset. In proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR),\npages 6299–6308.\nYiting Cheng, Fangyun Wei, Jianmin Bao, Dong Chen,\nand Wenqiang Zhang. 2023. Cico: Domain-aware\nsign language retrieval via cross-lingual contrastive\nlearning. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition,\npages 19016–19026.\nMathieu De Coster, Dimitar Shterionov, Mieke Van Her-\nreweghe, and Joni Dambre. 2023. Machine transla-\ntion from signed to spoken languages: State of the art\nand challenges. Universal Access in the Information\nSociety, pages 1–27.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. 2009. Imagenet: A large-scale hier-\narchical image database. In 2009 IEEE conference\non computer vision and pattern recognition, pages\n248–255. Ieee.\nLi Deng. 2012. The mnist database of handwritten digit\nimages for machine learning research [best of the\nweb]. IEEE signal processing magazine, 29(6):141–\n142.\nAashaka Desai, Lauren Berger, Fyodor O Minakov,\nVanessa Milan, Chinmay Singh, Kriston Pumphrey,\nRichard E Ladner, Hal Daumé III, Alex X Lu,\nNaomi Caselli, and Danielle Bragg. 2023. Asl cit-\nizen: A community-sourced dataset for advancing\nisolated sign language recognition. arXiv preprint\narXiv:2304.05934.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nPhilippe Dreuw, Thomas Deselaers, Daniel Keysers,\nand Hermann Ney. 2006. Modeling image variability\nin appearance-based gesture recognition. In ECCV\nworkshop on statistical methods in multi-image and\nvideo processing, pages 7–18.\nAmanda Duarte, Shruti Palaskar, Lucas Ventura, Deepti\nGhadiyaram, Kenneth DeHaan, Florian Metze, Jordi\nTorres, and Xavier Giro-i Nieto. 2021. How2Sign:\nA Large-scale Multimodal Dataset for Continuous\nAmerican Sign Language. In Conference on Com-\nputer Vision and Pattern Recognition (CVPR).\nHao-Shu Fang, Jiefeng Li, Hongyang Tang, Chao Xu,\nHaoyi Zhu, Yuliang Xiu, Yong-Lu Li, and Cewu Lu.\n2022. Alphapose: Whole-body regional multi-person\npose estimation and tracking in real-time.\nIEEE\nTransactions on Pattern Analysis and Machine In-\ntelligence.\nJens Forster, Christoph Schmidt, Oscar Koller, Martin\nBellgardt, and Hermann Ney. 2014. Extensions of\nthe sign language recognition and translation cor-\npus RWTH-PHOENIX-weather. In Proceedings of\nthe Ninth International Conference on Language\nResources and Evaluation (LREC’14), pages 1911–\n1916, Reykjavik, Iceland. European Language Re-\nsources Association (ELRA).\nShiwei Gan, Yafeng Yin, Zhiwei Jiang, Kang Xia, Lei\nXie, and Sanglu Lu. 2023. Contrastive learning for\nsign language recognition and translation. In IJCAI,\npages 763–772.\nBinyam Gebrekidan Gebre, Peter Wittenburg, and Tom\nHeskes. 2013. Automatic sign language identifica-\ntion.\nIn 2013 IEEE International Conference on\nImage Processing, pages 2626–2630. IEEE.\nJia Gong, Lin Geng Foo, Yixuan He, Hossein Rahmani,\nand Jun Liu. 2024. Llms are good sign language\ntranslators. arXiv preprint arXiv:2404.00925.\nIvan Grishchenko and Valentin Bazarevsky. 2020. Me-\ndiapipe holistic.\nShester Gueuwou, Sophie Siake, Colin Leong, and\nMathias Müller. 2023. JWSign: A highly multilin-\ngual corpus of Bible translations for more diversity\nin sign language processing. In Findings of the As-\nsociation for Computational Linguistics: EMNLP\n2023, pages 9907–9927, Singapore. Association for\nComputational Linguistics.\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Pi-\notr Dollár, and Ross Girshick. 2022. Masked autoen-\ncoders are scalable vision learners. In Proceedings\nof the IEEE/CVF conference on computer vision and\npattern recognition (CVPR), pages 16000–16009.\nRuth M Holmes, Ellen Rushe, and Anthony Ventresque.\n2024. The key points: Using feature importance\nto identify shortcomings in sign language recogni-\ntion models. In Proceedings of the 2024 Joint In-\nternational Conference on Computational Linguis-\ntics, Language Resources and Evaluation (LREC-\nCOLING 2024), pages 15970–15975.\nEui Jun Hwang, Huije Lee, and Jong C Park. 2023.\nAutoregressive sign language production: A gloss-\nfree approach with discrete representations. arXiv\npreprint arXiv:2309.12179.\nZifan Jiang, Amit Moryossef, Mathias Müller, and\nSarah Ebling. 2023. Machine translation between\nspoken languages and signed languages represented\nin SignWriting. In Findings of the Association for\nComputational Linguistics: EACL 2023, pages 1706–\n1724, Dubrovnik, Croatia. Association for Computa-\ntional Linguistics.\nMelvin Johnson, Mike Schuster, Quoc V. Le, Maxim\nKrikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat,\nFernanda Viégas, Martin Wattenberg, Greg Corrado,\nMacduff Hughes, and Jeffrey Dean. 2017. Google’s\nmultilingual neural machine translation system: En-\nabling zero-shot translation. Transactions of the As-\nsociation for Computational Linguistics, 5:339–351.\nTrevor Johnston and Adam Schembri. 2007. Australian\nSign Language (Auslan): An Introduction to Sign\nLanguage Linguistics. Cambridge University Press,\nCambridge, UK.\nTrevor Alexander Johnston. 2008.\nFrom archive to\ncorpus: transcription and annotation in the creation of\nsigned language corpora. In Pacific Asia Conference\non Language, Information and Computation.\nNikita Karaev, Ignacio Rocco, Benjamin Graham, Na-\ntalia Neverova, Andrea Vedaldi, and Christian Rup-\nprecht. 2023. Cotracker: It is better to track together.\narXiv:2307.07635.\nLee Kezar, Jesse Thomason, Naomi Caselli, Zed Sehyr,\nand Elana Pontecorvo. 2023. The sem-lex bench-\nmark: Modeling asl signs and their phonemes. In\nProceedings of the 25th International ACM SIGAC-\nCESS Conference on Computers and Accessibility,\npages 1–10.\nAlessandro Lenci and Magnus Sahlgren. 2023. Distri-\nbutional semantics. Cambridge University Press.\nAng Li, Allan Jabri, Armand Joulin, and Laurens Van\nDer Maaten. 2017. Learning visual n-grams from\nweb data. In Proceedings of the IEEE International\nConference on Computer Vision, pages 4183–4192.\nDongxu Li, Cristian Rodriguez, Xin Yu, and Hongdong\nLi. 2020. Word-level deep sign language recognition\nfrom video: A new large-scale dataset and methods\ncomparison.\nIn The IEEE Winter Conference on\nApplications of Computer Vision, pages 1459–1469.\nZe Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang,\nStephen Lin, and Han Hu. 2022. Video swin trans-\nformer. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition (CVPR),\npages 3202–3211.\nCamillo Lugaresi, Jiuqiang Tang, Hadon Nash, Chris\nMcClanahan, Esha Uboweja, Michael Hays, Fan\nZhang, Chuo-Ling Chang, Ming Guang Yong,\nJuhyun Lee, et al. 2019. Mediapipe: A framework\nfor building perception pipelines.\narXiv preprint\narXiv:1906.08172.\nAntoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac,\nMakarand Tapaswi, Ivan Laptev, and Josef Sivic.\n2019. HowTo100M: Learning a Text-Video Embed-\nding by Watching Hundred Million Narrated Video\nClips. In International Conference on Computer Vi-\nsion (ICCV).\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositionality.\nAdvances in neural information processing systems,\n26.\nAmit Moryossef, Zifan Jiang, Mathias Müller, Sarah\nEbling, and Yoav Goldberg. 2023. Linguistically\nmotivated sign language segmentation. In Findings\nof the Association for Computational Linguistics:\nEMNLP 2023, pages 12703–12724, Singapore. Asso-\nciation for Computational Linguistics.\nAmit Moryossef, Ioannis Tsochantaridis, Joe Dinn,\nNecati Cihan Camgoz, Richard Bowden, Tao Jiang,\nAnnette Rios, Mathias Muller, and Sarah Ebling.\n2021. Evaluating the immediate applicability of pose\nestimation for sign language recognition. In Proceed-\nings of the IEEE/CVF conference on computer vision\nand pattern recognition (CVPR), pages 3434–3440.\nMathias\nMüller,\nMalihe\nAlikhani,\nEleftherios\nAvramidis, Richard Bowden, Annelies Braffort,\nNecati Cihan Camgöz, Sarah Ebling, Cristina\nEspaña-Bonet,\nAnne Göhring,\nRoman Grund-\nkiewicz, Mert Inan, Zifan Jiang, Oscar Koller,\nAmit Moryossef, Annette Rios, Dimitar Shterionov,\nSandra Sidler-Miserez, Katja Tissi, and Davy\nVan Landuyt. 2023. Findings of the second WMT\nshared task on sign language translation (WMT-\nSLT23). In Proceedings of the Eighth Conference\non Machine Translation, pages 68–94, Singapore.\nAssociation for Computational Linguistics.\nMathias Müller, Sarah Ebling, Eleftherios Avramidis,\nAlessia Battisti, Michèle Berger, Richard Bowden,\nAnnelies Braffort, Necati Cihan Camgöz, Cristina\nEspaña-bonet, Roman Grundkiewicz, Zifan Jiang,\nOscar Koller, Amit Moryossef, Regula Perrollaz,\nSabine Reinhard, Annette Rios, Dimitar Shterionov,\nSandra Sidler-miserez, and Katja Tissi. 2022. Find-\nings of the first WMT shared task on sign language\ntranslation (WMT-SLT22). In Proceedings of the\nSeventh Conference on Machine Translation (WMT),\npages 744–772, Abu Dhabi, United Arab Emirates\n(Hybrid). Association for Computational Linguistics.\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018.\nRepresentation learning with contrastive predictive\ncoding. arXiv preprint arXiv:1807.03748.\nAleksandar Petrov, Emanuele La Malfa, Philip Torr,\nand Adel Bibi. 2024. Language model tokenizers\nintroduce unfairness between languages. Advances\nin Neural Information Processing Systems, 36.\nK R Prajwal, Hannah Bull, Liliane Momeni, Samuel\nAlbanie, Gül Varol, and Andrew Zisserman. 2022.\nWeakly-supervised fingerspelling recognition in\nbritish sign language videos. In British Machine\nVision Conference.\nSiegmund Prillwitz and Heiko Zienert. 1990. Hamburg\nnotation system for sign language: Development of\na sign writing with computer application. In Cur-\nrent trends in European Sign Language Research.\nProceedings of the 3rd European Congress on Sign\nLanguage Research, pages 355–379.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\net al. 2021. Learning transferable visual models from\nnatural language supervision. In International confer-\nence on machine learning, pages 8748–8763. PMLR.\nRazieh Rastgoo, Kourosh Kiani, Sergio Escalera, and\nMohammad Sabokrou. 2021. Sign language produc-\ntion: A review. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recogni-\ntion (CVPR), pages 3451–3461.\nCharles Raude, K R Prajwal, Liliane Momeni, Han-\nnah Bull, Samuel Albanie, Andrew Zisserman, and\nGül Varol. 2024. A tale of two languages: Large-\nvocabulary continuous sign language recognition\nfrom spoken language supervision. arXiv.\nNils Reimers and Iryna Gurevych. 2019. Sentence-bert:\nSentence embeddings using siamese bert-networks.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing. Associa-\ntion for Computational Linguistics.\nWendy Sandler and Diane Lillo-Martin. 2006. Sign\nlanguage and linguistic universals. Cambridge Uni-\nversity Press.\nZed Sevcikova Sehyr, Naomi Caselli, Ariel M Cohen-\nGoldberg, and Karen Emmorey. 2021. The asl-lex\n2.0 project: A database of lexical and phonological\nproperties for 2,723 signs in american sign language.\nThe Journal of Deaf Studies and Deaf Education,\n26(2):263–277.\nPrem Selvaraj, Gokul Nc, Pratyush Kumar, and Mitesh\nKhapra. 2022. OpenHands: Making sign language\nrecognition accessible with pose-based pretrained\nmodels across languages. In Proceedings of the 60th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 2114–\n2133, Dublin, Ireland. Association for Computational\nLinguistics.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 1715–1725,\nBerlin, Germany. Association for Computational Lin-\nguistics.\nAntonio F. G. Sevilla, José María Lahoz-Bengoechea,\nand Alberto Diaz. 2024. Automated extraction of\nprosodic structure from unannotated sign language\nvideo. In Proceedings of the 2024 Joint International\nConference on Computational Linguistics, Language\nResources and Evaluation (LREC-COLING 2024),\npages 1808–1816, Torino, Italia. ELRA and ICCL.\nBowen Shi, Aurora Martinez Del Rio, Jonathan\nKeane, Jonathan Michaux, Diane Brentari, Greg\nShakhnarovich, and Karen Livescu. 2018. Ameri-\ncan sign language fingerspelling recognition in the\nwild. In 2018 IEEE Spoken Language Technology\nWorkshop (SLT), pages 145–152. IEEE.\nBowen Shi, Aurora Martinez Del Rio, Jonathan Keane,\nDiane Brentari, Greg Shakhnarovich, and Karen\nLivescu. 2019. Fingerspelling recognition in the wild\nwith iterative visual attention. In Proceedings of the\nIEEE/CVF International Conference on Computer\nVision, pages 5400–5409.\nThad Starner, Sean Forbes, Matthew So, David Martin,\nRohit Sridhar, Gururaj Deshpande, Sam Sepah, Sahir\nShahryar, Khushi Bhardwaj, Tyler Kwok, et al. 2024.\nPopsign asl v1. 0: An isolated american sign lan-\nguage dataset collected via smartphones. Advances\nin Neural Information Processing Systems, 36.\nValerie Sutton. 1990. Lessons in sign writing. Sign-\nWriting.\nGarrett Tanzer and Biao Zhang. 2024. Youtube-sl-25: A\nlarge-scale, open-domain multilingual sign language\nparallel corpus. arXiv preprint arXiv:2407.11144.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nDave Uthus, Garrett Tanzer, and Manfred Georg. 2024.\nYoutube-asl: A large-scale, open-domain american\nsign language-english parallel corpus. Advances in\nNeural Information Processing Systems, 36.\nAaron Van Den Oord, Oriol Vinyals, et al. 2017. Neural\ndiscrete representation learning. Advances in neural\ninformation processing systems, 30.\nGül Varol, Liliane Momeni, Samuel Albanie, Triantafyl-\nlos Afouras, and Andrew Zisserman. 2021. Read\nand attend: Temporal localisation in sign language\nvideos. In proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR).\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing\nsystems, 30.\nQianqian Wang, Yen-Yu Chang, Ruojin Cai, Zhengqi Li,\nBharath Hariharan, Aleksander Holynski, and Noah\nSnavely. 2023. Tracking everything everywhere all\nat once. In International Conference on Computer\nVision.\nSherman Wilcox. 1992. The phonetics of fingerspelling,\nvolume 4. John Benjamins Publishing.\nRyan Wong, Necati Cihan Camgoz, and Richard Bow-\nden. 2023.\nLearnt contrastive concept embed-\ndings for sign recognition. In Proceedings of the\nIEEE/CVF International Conference on Computer\nVision, pages 1945–1954.\nHu Xu, Gargi Ghosh, Po-Yao Huang, Dmytro Okhonko,\nArmen Aghajanyan, Florian Metze, Luke Zettle-\nmoyer, and Christoph Feichtenhofer. 2021. Video-\nCLIP: Contrastive pre-training for zero-shot video-\ntext understanding. In Proceedings of the 2021 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 6787–6800, Online and Punta\nCana, Dominican Republic. Association for Com-\nputational Linguistics.\nJun Xu, Tao Mei, Ting Yao, and Yong Rui. 2016. Msr-\nvtt: A large video description dataset for bridging\nvideo and language. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition\n(CVPR), pages 5288–5296.\nJinhui Ye, Xing Wang, Wenxiang Jiao, Junwei Liang,\nand Hui Xiong. 2024. Improving gloss-free sign lan-\nguage translation by reducing representation density.\nKayo Yin, Amit Moryossef, Julie Hochgesang, Yoav\nGoldberg, and Malihe Alikhani. 2021. Including\nsigned languages in natural language processing. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 7347–\n7360, Online. Association for Computational Lin-\nguistics.\nYoungjae Yu, Jongseok Kim, and Gunhee Kim. 2018.\nA joint sequence fusion model for video question\nanswering and retrieval. In Proceedings of the Euro-\npean conference on computer vision (ECCV), pages\n471–487.\nDa Zhang, Xiyang Dai, Xin Wang, and Yuan-Fang\nWang. 2018. S3d: Single shot multi-span detector\nvia fully 3d convolutional network. In Proceedings\nof the British Machine Vision Conference (BMVC).\nCe Zheng, Wenhan Wu, Chen Chen, Taojiannan Yang,\nSijie Zhu, Ju Shen, Nasser Kehtarnavaz, and Mubarak\nShah. 2023a. Deep learning-based human pose esti-\nmation: A survey. ACM Comput. Surv.\nJiangbin Zheng, Yile Wang, Cheng Tan, Siyuan Li,\nGe Wang, Jun Xia, Yidong Chen, and Stan Z Li.\n2023b. Cvt-slr: Contrastive visual-textual transfor-\nmation for sign language recognition with variational\nalignment. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition\n(CVPR), pages 23141–23150.\nHao Zhou, Wengang Zhou, Weizhen Qi, Junfu Pu, and\nHouqiang Li. 2021. Improving sign language transla-\ntion with monolingual data by sign back-translation.\nIn Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR), pages\n1316–1325.\nYi Zhu, Xinyu Li, Chunhui Liu, Mohammadreza\nZolfaghari, Yuanjun Xiong, Chongruo Wu, Zhi\nZhang, Joseph Tighe, R Manmatha, and Mu Li. 2020.\nA comprehensive study of deep video action recogni-\ntion. arXiv preprint arXiv:2012.06567.\nA\nAn Intuitive Test on ChatGPT’s Sign Language Understanding Ability\nFigure 3: Screenshot of prompting ChatGPT 4o to sign “house” in ASL, which lacks sign language knowledge and\ntries to sketch a picture of a house on the open palm, tested in June 2024.\nB\nIntroduction of the RWTH German Fingerspelling Database\nRWTH German Fingerspelling Database contains ∼1400 videos of 20 different signers. Each video was\nrecorded by one webcam named cam1, recording the dominant hands only with a resolution of 320x240\nat 25 FPS; and one camcorder named cam2, recording the whole body with a resolution of 352x288 at 25\nFPS. We exclude all cam1 videos for pose-based models since we assume that the pose estimation system\nexpects whole-body input.\nFigure 4: Examples of the German finger-alphabet taken from the RWTH gesture database recorded with the\nwebcam showing the letters A-Z, Ä, Ö, Ü, SCH, and the numbers 1 to 5. Note that J, Z, Ä, Ö, and Ü are dynamic\ngestures. Figure taken from https://www-i6.informatik.rwth-aachen.de/aslr/fingerspelling.php.\nC\nExtended Spreadthesign Data Analysis\nFollowing the data statistics presented in §5.1, Figure 5 illustrates the distribution of the video examples\nin 41 sign languages we use from Spreadthesign.\nFigure 5: Sign language distribution of video examples in Spreadthesign, using the ISO 639-3 language codes.\nFigure 6 illustrates the distribution of pose/video length in Spreadthesign, depending on which we\ndecide the pretraining context length to be 256.\nFigure 6: Pose length distribution of video examples in Spreadthesign. The two red vertical lines denote the 1st and\n99th percentile of the number of frames.\nFigure 7 illustrates the distribution of the number of video examples for 18,423 cross-lingual concepts\nin Spreadthesign. Most concept has only one video example, or one video example per sign language, and\nvery few concepts have more than one video example for one specific sign language.\nFigure 7: Concept distribution of video examples in Spreadthesign.\nD\nFull Rank of the Signs for Iconicity Study\n1\n( ' s c o r pi o n ' ,\n180.44417)\n2\n( ' r e d u c t i o n ' ,\n181.40424)\n3\n( ' i l l u s t r a t i o n ' ,\n182.58504)\n4\n( ' envelope ' ,\n182.63124)\n5\n( ' observe ' ,\n183.66785)\n6\n( ' gold ' ,\n184.15184)\n7\n( ' t e l e p h o n e ' ,\n184.36679)\n8\n( ' email ' ,\n185.0535)\n9\n( ' c e n t e r ' ,\n185.35873)\n10\n( ' k n i f e ' ,\n185.70123)\n11\n( ' fake ' ,\n185.7079)\n12\n( ' fro zen ' ,\n186.34874)\n13\n( ' high ' ,\n186.40546)\n14\n( ' consume ' ,\n186.42805)\n15\n( ' c r o c o d i l e ' ,\n186.82196)\n16\n( ' t o o t h b r u s h ' ,\n186.83427)\n17\n( ' Youtube ' ,\n186.87538)\n18\n( ' earache ' ,\n186.9703)\n19\n( ' Mexico ' ,\n186.98878)\n20\n( ' ear ' ,\n187.11588)\n21\n( ' Remind ' ,\n187.29926)\n22\n( ' Notepad ' ,\n187.38866)\n23\n( ' Put ' ,\n187.41467)\n24\n( ' p o t a t o ' ,\n187.4263)\n25\n( ' c o n c e i t ' ,\n187.43402)\n26\n( ' thong ' ,\n187.492)\n27\n( ' sauce ' ,\n187.50092)\n28\n( ' obsessed ' ,\n187.57285)\n29\n( ' drum ' ,\n187.8526)\n30\n( ' Cuba ' ,\n188.00418)\n31\n( ' g e n e r a t i o n ' ,\n188.16415)\n32\n( ' g r i e f ' ,\n188.4431)\n33\n( ' g u i l l o t i n e ' ,\n188.59848)\n34\n( ' to ' ,\n188.62285)\n35\n( ' bind ' ,\n188.63608)\n36\n( ' umbrella ' ,\n188.81358)\n37\n( ' omit ' ,\n189.20493)\n38\n( ' Superman ' ,\n189.20782)\n39\n( ' advice ' ,\n189.48647)\n40\n( ' Refuse ' ,\n189.52502)\n41\n( ' speed ' ,\n189.6827)\n42\n( ' diamond ' ,\n189.76154)\n43\n( ' cute ' ,\n189.82486)\n44\n( ' headache ' ,\n190.24924)\n45\n( ' j e a l o u s y ' ,\n191.17343)\n46\n( ' f l a g ' ,\n191.18753)\n47\n( ' banana ' ,\n191.20346)\n48\n( ' Wait ! ' ,\n191.36217)\n49\n( ' yet ' ,\n191.75407)\n50\n( ' t h e f t ' ,\n191.75734)\n51\n( ' house ' ,\n191.79712)\n52\n( ' p e r c e n t a g e ' ,\n191.80008)\n53\n( ' eye ' ,\n191.84584)\n54\n( ' u n d e r s t a n d i n g ' ,\n191.95715)\n55\n( ' badly ' ,\n192.02959)\n56\n( ' skin ' ,\n192.04294)\n57\n( ' dvd ' ,\n192.05695)\n58\n( ' u n t i l ' ,\n192.08847)\n59\n( ' Denmark ' ,\n192.28465)\n60\n( ' flower ' ,\n192.2852)\n61\n( ' sew ' ,\n192.4188)\n62\n( ' a r r e s t ' ,\n192.55325)\n63\n( ' p r e v io u s ' ,\n192.55621)\n64\n( ' neighbor ' ,\n192.59973)\n65\n( ' spoon ' ,\n192.78313)\n66\n( ' b e l l ' ,\n192.82666)\n67\n( ' c l i c k ' ,\n192.86957)\n68\n( ' v a c c i n a t i o n ' ,\n192.90399)\n69\n( ' l e a d i n g ' ,\n193.04103)\n70\n( ' t o t a l ' ,\n193.0793)\n71\n( ' i n t e r n e t ' ,\n193.26022)\n72\n( ' sandwich ' ,\n193.27394)\n73\n( ' e r e c t ' ,\n193.31744)\n74\n( ' s i g n a t u r e ' ,\n193.40796)\n75\n( ' s i x ' ,\n193.43289)\n76\n( ' s t r a n g e ' ,\n193.51538)\n77\n( ' boy ' ,\n193.53194)\n78\n( ' Farewell ! ' ,\n193.86534)\n79\n( ' cafe ' ,\n193.86969)\n80\n( ' twice ' ,\n193.91452)\n81\n( ' four ' ,\n193.96509)\n82\n( ' s i l v e r ' ,\n193.97311)\n83\n( ' China ' ,\n194.03874)\n84\n( ' c e r t i f y ' ,\n194.06538)\n85\n( ' soup ' ,\n194.08856)\n86\n( ' rape ' ,\n194.09244)\n87\n( ' n e c e s s a r y ' ,\n194.1475)\n88\n( ' c u r i o u s ' ,\n194.27072)\n89\n( ' t a l l ' ,\n194.3718)\n90\n( ' b a t t l e ' ,\n194.45622)\n91\n( ' promise ' ,\n194.51105)\n92\n( ' d e a d l i n e ' ,\n194.61055)\n93\n( ' c e r t i f i c a t e ' ,\n194.736)\n94\n( ' genuine ' ,\n194.79256)\n95\n( ' blood ' ,\n194.82959)\n96\n( ' ban ' ,\n194.84705)\n97\n( ' uncle ' ,\n194.85689)\n98\n( ' q u a r a n t i n e ' ,\n194.90247)\n99\n( ' s a l a d ' ,\n195.1925)\n100\n( ' ant ' ,\n195.31888)\n101\n( ' t h i e f ' ,\n195.48566)\n102\n( ' f a l l ' ,\n195.60696)\n103\n( ' c h i l d l i k e ' ,\n195.6437)\n104\n( ' Japan ' ,\n195.68546)\n105\n( ' run ' ,\n195.70013)\n106\n( ' booking ' ,\n195.75223)\n107\n( ' homesick ' ,\n195.86086)\n108\n( ' advanced ' ,\n195.86685)\n109\n( ' Where? ' ,\n195.92314)\n110\n( ' b ridge ' ,\n195.98723)\n111\n( ' b eside ' ,\n196.01958)\n112\n( ' cup ' ,\n196.03467)\n113\n( ' s p a g h e t t i ' ,\n196.04333)\n114\n( ' d i z z i n e s s ' ,\n196.06311)\n115\n( ' mixer ' ,\n196.10735)\n116\n( ' Assessment ' ,\n196.14081)\n117\n( ' amnesia ' ,\n196.15091)\n118\n( ' g i g a n t i c ' ,\n196.21085)\n119\n( ' p r i e s t ' ,\n196.2539)\n120\n( ' s o r r y ' ,\n196.31093)\n121\n( ' investment ' ,\n196.35571)\n122\n( ' b e l i e v e ' ,\n196.41522)\n123\n( ' hang ' ,\n196.4303)\n124\n( ' t h r e e ' ,\n196.43767)\n125\n( ' h e a ring ' ,\n196.51305)\n126\n( ' p r i n c i p a l ' ,\n196.63132)\n127\n( ' p u n ct u a l ' ,\n196.70624)\n128\n( ' a d u l t ' ,\n196.91183)\n129\n( ' t h i n ' ,\n196.98502)\n130\n( ' word ' ,\n197.0534)\n131\n( ' arm ' ,\n197.11966)\n132\n( ' c e n s o r s h i p ' ,\n197.13493)\n133\n( ' s e v e r a l ' ,\n197.31216)\n134\n( ' bewilder ' ,\n197.43254)\n135\n( ' r e p l y ' ,\n197.46024)\n136\n( ' s e r i o u s ' ,\n197.57898)\n137\n( ' sewing ' ,\n197.62848)\n138\n( ' do ' ,\n197.69405)\n139\n( ' t o g e t h e r ' ,\n197.83093)\n140\n( ' hairgrowth ' ,\n197.85425)\n141\n( ' b u l l ' ,\n197.98575)\n142\n( ' honeymoon ' ,\n198.02545)\n143\n( ' b a l l ' ,\n198.05637)\n144\n( ' a n c i e n t ' ,\n198.14444)\n145\n( ' s e l f i s h ' ,\n198.15906)\n146\n( ' a r i s e ' ,\n198.17198)\n147\n( ' wedding ' ,\n198.21584)\n148\n( ' hour ' ,\n198.27957)\n149\n( ' granddaughter ' ,\n198.29315)\n150\n( ' c i r c l e ' ,\n198.32275)\n151\n( ' couch ' ,\n198.38385)\n152\n( ' s c i e n t i s t ' ,\n198.44269)\n153\n( ' i m p o r t a n t ' ,\n198.52599)\n154\n( ' h e l i c o p t e r ' ,\n198.61218)\n155\n( ' born ' ,\n198.69475)\n156\n( ' t r o u s e r s ' ,\n198.7119)\n157\n( ' a c c e p t a b l e ' ,\n198.82736)\n158\n( ' lamp ' ,\n198.86002)\n159\n( ' a p p e t i t e ' ,\n198.86995)\n160\n( ' a s s o c i a t i o n ' ,\n198.87001)\n161\n( ' leave ' ,\n198.95593)\n162\n( ' d y s l e x i a ' ,\n198.98013)\n163\n( ' twin ' ,\n199.01114)\n164\n( ' f o r c e ' ,\n199.04541)\n165\n( ' i n s i s t ' ,\n199.06236)\n166\n( ' vase ' ,\n199.08466)\n167\n( ' e a s t e r ' ,\n199.23846)\n168\n( ' p l a t e ' ,\n199.27231)\n169\n( ' b e s t ' ,\n199.32513)\n170\n( ' heal ' ,\n199.61261)\n171\n( ' p e t r o l ' ,\n199.672)\n172\n( ' c l e a n e r ' ,\n199.69077)\n173\n( ' pepper ' ,\n199.92517)\n174\n( ' economic ' ,\n199.97253)\n175\n( ' yoghurt ' ,\n200.06439)\n176\n( ' b r o t h e r ' ,\n200.0689)\n177\n( ' u n p l e a s a n t ' ,\n200.13562)\n178\n( ' grapes ' ,\n200.14981)\n179\n( ' buy ' ,\n200.29669)\n180\n( ' 2 ' ,\n200.31093)\n181\n( ' frog ' ,\n200.41377)\n182\n( ' committee ' ,\n200.56615)\n183\n( ' complain ' ,\n200.57298)\n184\n( ' 40 ' ,\n200.58508)\n185\n( ' f a u l t l e s s ' ,\n200.59167)\n186\n( ' l e t t e r ' ,\n200.63252)\n187\n( ' angel ' ,\n200.65413)\n188\n( ' c o r r u p t i o n ' ,\n200.66101)\n189\n( ' d i r e c t o r ' ,\n200.67601)\n190\n( ' e xport ' ,\n200.99376)\n191\n( ' acne ' ,\n201.08725)\n192\n( ' p a r t i c i p a t e ' ,\n201.14157)\n193\n( ' i n j u r y ' ,\n201.19518)\n194\n( ' o f f l i n e ' ,\n201.215)\n195\n( ' h u r t ' ,\n201.2529)\n196\n( ' shy ' ,\n201.31795)\n197\n( ' k i l o m e t e r ' ,\n201.32117)\n198\n( ' i n a u g u r a t i o n ' ,\n201.33997)\n199\n( ' t a l e ' ,\n201.4152)\n200\n( ' very ' ,\n201.45908)\n201\n( ' law ' ,\n201.47714)\n202\n( ' diploma ' ,\n201.56801)\n203\n( ' music ' ,\n201.57074)\n204\n( ' war ' ,\n201.63304)\n205\n( ' school ' ,\n201.65495)\n206\n( ' horse ' ,\n201.746)\n207\n( ' h e a r t b u r n ' ,\n201.86896)\n208\n( ' 21 ' ,\n201.90666)\n209\n( ' surname ' ,\n201.94667)\n210\n( ' a d d i c t e d ' ,\n201.98883)\n211\n( ' supper ' ,\n202.08438)\n212\n( ' fun ' ,\n202.09558)\n213\n( ' t e r r o r i s t ' ,\n202.22504)\n214\n( ' nanny ' ,\n202.32727)\n215\n( ' d e p a r t u r e ' ,\n202.34787)\n216\n( ' 600 ' ,\n202.38922)\n217\n( ' pet ' ,\n202.46806)\n218\n( ' thousand ' ,\n202.5422)\n219\n( ' i c e ' ,\n202.56726)\n220\n( ' menu ' ,\n202.57507)\n221\n( ' r e v i s e ' ,\n202.69331)\n222\n( ' h a i r e d ' ,\n202.70969)\n223\n( ' f e e l i n g ' ,\n202.82637)\n224\n( ' divorced ' ,\n202.85403)\n225\n( ' person ' ,\n203.0279)\n226\n( ' dawn ' ,\n203.36467)\n227\n( ' anxious ' ,\n203.46112)\n228\n( ' autism ' ,\n203.48038)\n229\n( ' d i s c u s s i o n ' ,\n203.56613)\n230\n( ' adoption ' ,\n203.58824)\n231\n( ' t r u t h ' ,\n203.6054)\n232\n( ' enemy ' ,\n203.6127)\n233\n( ' midnight ' ,\n203.63316)\n234\n( ' psychology ' ,\n203.70671)\n235\n( ' p o s s i b l e ' ,\n203.8542)\n236\n( ' pale ' ,\n203.85995)\n237\n( ' cucumber ' ,\n203.87328)\n238\n( ' f a v o u r i t e ' ,\n203.95981)\n239\n( ' r i c e ' ,\n204.09598)\n240\n( ' bedroom ' ,\n204.11554)\n241\n( ' sea ' ,\n204.18881)\n242\n( ' shock ' ,\n204.216)\n243\n( ' Admitted ' ,\n204.22227)\n244\n( ' a n x i e t y ' ,\n204.22739)\n245\n( ' ten ' ,\n204.28094)\n246\n( ' i n t e r n a t i o n a l ' ,\n204.30515)\n247\n( ' c u r l y ' ,\n204.32364)\n248\n( ' alarm ' ,\n204.42891)\n249\n( ' corn ' ,\n204.57687)\n250\n( ' upset ' ,\n204.593)\n251\n( ' morning ' ,\n204.65657)\n252\n( ' spinach ' ,\n204.6566)\n253\n( ' c e l e b r a t e ' ,\n204.74715)\n254\n( ' confused ' ,\n204.82571)\n255\n( ' 25 ' ,\n204.87206)\n256\n( ' achievement ' ,\n204.94815)\n257\n( ' c l i m a t e ' ,\n205.02107)\n258\n( ' communication ' ,\n205.26228)\n259\n( ' d e l e g a t e ' ,\n205.26901)\n260\n( ' d o o r b e l l ' ,\n205.41678)\n261\n( ' a n a e s t h e s i a ' ,\n205.66046)\n262\n( ' world ' ,\n205.86342)\n263\n( ' t e l e v i s i o n ' ,\n206.1195)\n264\n( ' i n f o r m a t i o n ' ,\n206.20271)\n265\n( ' s t y l e ' ,\n206.28078)\n266\n( ' chip ' ,\n206.31616)\n267\n( ' anonymous ' ,\n206.41803)\n268\n( ' f a s h i o n e d ' ,\n206.46509)\n269\n( ' development ' ,\n206.46605)\n270\n( ' s c a r f ' ,\n206.58081)\n271\n( ' uploading ' ,\n206.93182)\n272\n( ' 900 ' ,\n206.93893)\n273\n( ' 500 ' ,\n207.1253)\n274\n( ' camera ' ,\n207.15207)\n275\n( ' homeless ' ,\n207.25655)\n276\n( ' automatic ' ,\n207.29578)\n277\n( ' 1000000 ' ,\n207.40567)\n278\n( ' chef ' ,\n207.72531)\n279\n( ' 50 ' ,\n207.73314)\n280\n( ' i n f a n t ' ,\n207.88846)\n281\n( ' a c t r e s s ' ,\n207.97646)\n282\n( ' nurse ' ,\n208.75182)\n283\n( ' 800 ' ,\n208.8017)\n284\n( ' slow ' ,\n208.93741)\n285\n( ' c l i n i c ' ,\n208.93753)\n286\n( ' apartment ' ,\n209.07938)\n287\n( ' employed ' ,\n209.48071)\n288\n( ' e l e c t r i c i a n ' ,\n209.54414)\n289\n( ' p a i n t e r ' ,\n209.57893)\n290\n( ' d e s e r t ' ,\n209.70918)\n291\n( ' A u d i o l o g i s t ' ,\n209.97559)\n292\n( ' engine ' ,\n210.53745)\n293\n( ' bar ber ' ,\n210.76971)\n294\n( ' bathroom ' ,\n210.77377)\n295\n( ' d i a b e t i c ' ,\n211.66092)\n296\n( ' 7 ' ,\n211.76964)\n297\n( ' 27 ' ,\n211.83792)\n298\n( ' depressed ' ,\n212.88554)\n299\n( ' employee ' ,\n213.17842)\n300\n( ' 8 ' ,\n213.59476)\n301\n( ' farmer ' ,\n216.97792)\n302\n( ' 29 ' ,\n217.70363)\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2024-07-01",
  "updated": "2024-10-06"
}