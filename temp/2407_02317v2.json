{
  "id": "http://arxiv.org/abs/2407.02317v2",
  "title": "Soft Language Prompts for Language Transfer",
  "authors": [
    "Ivan Vykopal",
    "Simon Ostermann",
    "Marián Šimko"
  ],
  "abstract": "Cross-lingual knowledge transfer, especially between high- and low-resource\nlanguages, remains challenging in natural language processing (NLP). This study\noffers insights for improving cross-lingual NLP applications through the\ncombination of parameter-efficient fine-tuning methods. We systematically\nexplore strategies for enhancing cross-lingual transfer through the\nincorporation of language-specific and task-specific adapters and soft prompts.\nWe present a detailed investigation of various combinations of these methods,\nexploring their efficiency across 16 languages, focusing on 10 mid- and\nlow-resource languages. We further present to our knowledge the first use of\nsoft prompts for language transfer, a technique we call soft language prompts.\nOur findings demonstrate that in contrast to claims of previous work, a\ncombination of language and task adapters does not always work best; instead,\ncombining a soft language prompt with a task adapter outperforms most\nconfigurations in many cases.",
  "text": "Soft Language Prompts for Language Transfer\nIvan Vykopal1,2, Simon Ostermann3 and Marián Šimko2\n1 Faculty of Information Technology, Brno University of Technology, Brno, Czech Republic\n2 Kempelen Institute of Intelligent Technologies, Bratislava, Slovakia\n{name.surname}@kinit.sk\n3 German Research Center for Artificial Intelligence (DFKI), Saarbrücken, Germany\nsimon.ostermann@dfki.de\nAbstract\nCross-lingual knowledge transfer, especially\nbetween high- and low-resource languages, re-\nmains challenging in natural language process-\ning (NLP). This study offers insights for im-\nproving cross-lingual NLP applications through\nthe combination of parameter-efficient fine-\ntuning methods. We systematically explore\nstrategies for enhancing cross-lingual transfer\nthrough the incorporation of language-specific\nand task-specific adapters and soft prompts.\nWe present a detailed investigation of various\ncombinations of these methods, exploring their\nefficiency across 16 languages, focusing on\n10 mid- and low-resource languages. We fur-\nther present to our knowledge the first use of\nsoft prompts for language transfer, a technique\nwe call soft language prompts. Our findings\ndemonstrate that in contrast to claims of pre-\nvious work, a combination of language and\ntask adapters does not always work best; in-\nstead, combining a soft language prompt with\na task adapter outperforms most configurations\nin many cases.\n1\nIntroduction\nMany multilingual large language models (LLMs)\nhave been developed in recent years, demonstrat-\ning promising performance on various NLP tasks\nacross multiple languages (Xue et al., 2021; Work-\nshop et al., 2023). These models are pre-trained\non extensive corpora of unlabelled data in numer-\nous languages, allowing an adaptation to linguistic\ncharacteristics and nuances. In addition, LLMs\nare often further trained on downstream tasks in a\nselected subset of languages (Muennighoff et al.,\n2023). However, only few LLMs focus on low-\nresource languages (Tang et al., 2020; Xue et al.,\n2021; Üstün et al., 2024).\nAs the number of covered languages in the\nmodel increases, the issue of the curse of multi-\nlinguality arises. This problem occurs when the\nLLM’s capacity is limited, causing languages with\nen\nPhase I.\nTraining Lang. Representations\nde\nar\nbg\nur\nzh\nte\nsw\nsk\nsl\nru\nro\nml\nes\nel\nen\ncs\nPhase II.\nTraining Task Representations\nar\nde\nen\nes\nru\nzh\nPhase III.\nEvaluation\nQA\nNLI\nEntailment\nNeutral\nContradiction\nNER\nnot\ncheck-\nworthy\nCWCD\ncheck-\nworthy\nQA\nNLI\nEntailment\nNeutral\nContradiction\nNER\nnot\ncheck-\nworthy\nCWCD\ncheck-\nworthy\nbg\ncs\nel\nml\nro\nte\nur\nsl\nsk\nsw\nFigure 1: The full pipeline consists of training language\nand task representations along with evaluation on four\nselected tasks.\nless training data to perform poorly (Conneau et al.,\n2020). Various approaches have been employed\nto address this limitation, primarily involving ad-\nditional trainable parameters specific to individual\nlanguages (Pfeiffer et al., 2020, 2023).\nAn alternative to language-specific tuning is\ncross-lingual transfer, where researchers investi-\ngate the knowledge transfer between high and low-\nresource languages. In cross-lingual transfer meth-\nods, an LLM is trained on a downstream task in one\nlanguage, most often high-resource, and evaluated\nin other languages (Pikuliak et al., 2021). However,\ntraining only task-specific representations does not\nalways capture the nuances of languages on which\nthe LLM has not been trained or has been trained\nonly to a small extent. Therefore, incorporating\nlanguage-specific features can enhance knowledge\ntransfer across languages.\nPrevious work has primarily investigated lan-\nguage and task representations by training language\nand task-specific adapters (Pfeiffer et al., 2020;\nParovi´c et al., 2022) or by employing language\narithmetics (Klimaszewski et al., 2024). Nonethe-\nless, other approaches that involve adding addi-\ntional parameters to the model for language repre-\nsentation have not been thoroughly explored. This\nopens the opportunity to explore a combination\nof language and task representations using other\nmethods and their impact in cross-lingual settings.\nTo explore the utilization of language and task\narXiv:2407.02317v2  [cs.CL]  30 Oct 2024\nrepresentations, we evaluate various configurations\nby combining two parameter-efficient fine-tuning\n(PEFT) methods that incorporate additional param-\neters into the LLM, namely adapters and prompt-\ntuning. Adding these additional language- and\ntask-specific parameters increases the capacity of\nan mT0-BASE model and improves cross-lingual\nperformance.\nWe evaluate the performance of\neach configuration by training on six high-resource\nlanguages and evaluating its effectiveness on 10\nmid- and low-resource languages on four selected\ntasks1.Our main contributions are:\n• We propose soft language prompts as an al-\nternative method for cross-lingual transfer.\n• We comprehensively evaluate combinations\nof adapters and soft prompts in cross-lingual\ntransfer and find that language prompts pro-\nvide a viable alternative to language adapters,\nespecially for low-resource languages.\n• In addition, we provide an exhaustive evalu-\nation of both prompts and adapters for task\ntransfer. We find that the best combination of\nadapters and prompts for task and language\ntransfer depends highly on task and language,\nresp., and that no solution clearly outperforms\nthe others.\n2\nRelated Work\nAdapters and Soft Prompts.\nPEFT methods are\ndesigned to address the problem of the increasing\nnumber of trainable parameters in LLMs (He et al.,\n2022; Dettmers et al., 2023; Zhang et al., 2023;\nXu et al., 2023; Xie et al., 2024). These methods\nreduce the number of trained parameters and in-\ncorporate new parameters commonly used to train\nLLMs on other tasks. Adapters (Houlsby et al.,\n2019a) and Prompt-Tuning (Lester et al., 2021)\nrepresent two PEFT methods for adapting LLMs\nto different NLP domains. Adapters incorporate\nnew parameters into the transformer architecture\nby including down- and up-projection layers along\nwith residual connection, while prompt-tuning in-\ntroduced trainable soft-prompts prepended to input\nembeddings to condition the LLM’s generation.\nLimitations of Multilingual LLMs.\nOne ma-\njor limitation of LLMs is catastrophic forgetting,\nwhich occurs when training the LLM on a new task,\n1Code is available at: https://github.com/kinit-sk/\nadapter-prompt-evaluation\ncausing it to partially or entirely forget previously\nlearned knowledge for other tasks (McCloskey and\nCohen, 1989; Luo et al., 2024; Ren et al., 2024).\nThis forgetting extends beyond task-specific knowl-\nedge to language-specific knowledge if the model is\nfine-tuned on a subset of the original languages (Vu\net al., 2022a; Liu and Huang, 2023).\nAnother challenge with multilingual LLMs is\nassociated with the number of languages on which\nthese LLMs have been pre-trained (Conneau et al.,\n2020; Pfeiffer et al., 2022). Previous research has\nshown that as the number of languages covered\nby LLMs increases, their performance on various\nNLP tasks degrades (Hu et al., 2020; Ponti et al.,\n2020). Additionally, low-resource languages are\noften underrepresented during pre-training, result-\ning in poor performance in these languages (Wu\nand Dredze, 2020).\nCross-Lingual Transfer.\nGiven the many low-\nresource and underrepresented languages, cross-\nlingual transfer is crucial for training LLMs to\naddress NLP tasks in various languages (Piku-\nliak et al., 2021). A common approach involves\ntraining LLMs in one language and evaluating\nthem in another. Recent methods use additional\nparameters to create language-specific represen-\ntations, assisting LLMs in solving NLP tasks in\nlow-resource languages (Üstün et al., 2020; Ansell\net al., 2022). These include training task adapters\non top of language adapters (Pfeiffer et al., 2020;\nAnsell et al., 2021; Pfeiffer et al., 2023; Kunz and\nHolmström, 2024), training language adapters on\nsource and target languages (Parovi´c et al., 2022),\nand fusing multiple task (Lee et al., 2022) or lan-\nguage adapters (Rathore et al., 2023). Other ap-\nproaches leverage soft prompts (Huang et al., 2022;\nPhilippy et al., 2024) or grammar prompting (Wang\net al., 2024). While many works focus on spe-\ncific tasks, our study explored different combina-\ntions of adapters and soft prompts for cross-lingual\ntransfer on four tasks, minimizing the reliance on\nmachine translation, which is often unreliable for\nlow-resource languages.\n3\nMethodology\nWe propose a comprehensive study on combina-\ntions of language and task representations using\nadapters and soft prompts. We evaluate for the first\ntime the capabilities of soft language prompts in\na systematic manner and evaluate the performance\nof diverse combinations of prompts and adapters in\ncross-lingual settings. Our pipeline, consisting of\ntraining, evaluation, multiple languages and tasks\nthat constitute each step, is illustrated in Figure 1.\nIn the following sections, we first give details\non methods that we investigate for representing\nlanguage (Section 3.1) and task information (Sec-\ntion 3.2). We then explain the combinations of\nsoft prompts and adapters that we evaluate (Section\n3.3).\n3.1\nLanguage Representation\nLanguage Adapters.\nPrevious work has inves-\ntigated the effectiveness of training language-\nspecific transformation using the adapter architec-\nture (Houlsby et al., 2019b). Pfeiffer et al. (2020)\nproposed a MAD-X framework, which includes\ntraining language adapters using the masked lan-\nguage modeling objective on unlabelled data. In-\nspired by language adapters proposed by the au-\nthors, we build upon their architecture and the ap-\nproach used to train language adapters. Language\nadapters in our settings are incorporated into each\ntransformer layer of the LLM and trained using\nunlabelled data.\nSoft Language Prompts.\nSoft Prompt Tuning\noffers a promising, parameter-efficient method for\nadapting LLMs. While previous work has predom-\ninantly explored task-specific soft prompts aimed\nat enhancing task transferability, typically focus-\ning on a single language (Vu et al., 2022b; Asai\net al., 2022), we extend this approach by training\nlanguage-specific soft prompts to guide multilin-\ngual LLMs toward a target language. Given that\nmultilingual LLMs can generate responses in vari-\nous languages, we defined a soft language prompt\nas a set of token embeddings prepended to the in-\nput embedding. These embeddings are then fed\ninto the LLM to condition its output to the desired\nlanguage.\nExisting studies have highlighted the importance\nof soft prompt initialization in optimizing the per-\nformance of LLMs. Lester et al. (2021) outline\nthree possible strategies: (1) random initialization\nusing a Gaussian distribution; (2) initialization\nfrom the model’s vocabulary; and (3) initialization\nwith the embeddings of output classes for classifi-\ncation tasks. While each method has its strengths\nand limitations, none are directly applicable to our\nexperiments, which focus on multilingual LLMs.\nTo address this, we introduce a language-specific\ntext instruction for soft prompt initialization (see\nAppendix C). In this approach, the text instruction\nis first embedded, and if its length is shorter than\nthe required soft prompt size, the embedding is\nrepeated until the desired length is achieved.\nLanguage\nModeling\nObjective.\nTraining\nlanguage-specific representations requires unla-\nbelled data from the selected languages and careful\nselection of an appropriate training objective.\nGiven our use of an encoder-decoder architecture,\nwe adopt span corruption as the training objective,\nwhich has been shown to be effective in prior\nwork (Raffel et al., 2020; Xue et al., 2021). Unlike\nthe casual language modeling objective, where the\nLLM predicts the next token in a sequence, span\ncorruption randomly masks 15% of the tokens in\nthe input text using sentinel tokens. These tokens\nserve solely to mark the masked parts, which the\nLLM is tasked to reconstruct (Raffel et al., 2020).\nFinally, the LLM is trained to predict the original\ntokens for the masked portions, enabling it to learn\nlinguistic nuances and patterns that are crucial for\ntraining task-specific adapters and soft prompts.\n3.2\nTask Representation\nTask Adapters.\nSimilarly to language adapters,\nwe use task-specific adapters, represented by the\nsame architecture, which are incorporated into each\ntransformer layer of the LLM. However, when com-\nbining task representations with language represen-\ntations, the final architecture differs across con-\nfigurations and depends on the type of language\nrepresentation used during the training and infer-\nence. Detailed information on the architecture for\nall combinations is in Section 3.3.\nTask adapters are updated only during training\non the desired downstream task, while the rest of\nthe model, along with the language representation,\nis kept frozen. In the case of task-specific repre-\nsentations, LLMs learn knowledge that is charac-\nteristic of the specified tasks and that should be\nlanguage-independent.\nSoft Task Prompts.\nIn addition to task adapters,\nwe also use soft task prompts that employ the same\narchitecture and parameters used for soft language\nprompts. The difference when using a soft task\nprompt occurs in the configuration consisting of a\nsoft language prompt and a soft task prompt. With\nthis configuration, both soft prompts are combined\nusing a concatenation operation and further fed into\nthe model to condition the final generation.\n3.3\nEvaluated Combinations of Adapters and\nSoft Prompts\nSince our experiments are focused on evaluating\nlanguage and task representations and their combi-\nnation, we define six possible configurations: (1)\nonly task adapter; (2) only soft task prompt; (3)\nMAD-X (Pfeiffer et al., 2020), i.e. the combination\nof language and task adapter; (4) the combina-\ntion of language adapter and soft task prompt; (5)\nthe combination of soft language prompt and task\nadapter; and (6) the combination of soft language\nprompt and soft task prompt. The position of task\nrepresentations within the LLM highly depends on\nthe type of language representation used in exper-\niments. The architecture along with the form of\nthe input for all configurations are illustrated in\nFigure 2.\nSingle Task Adapters & Soft Task Prompts.\nThe configurations that employ only task adapters\nor task soft prompts aim at training task represen-\ntations, without incorporating language-specific\nrepresentation. Adapters and soft prompts were\ntrained independently on each selected dataset, and\nthe resulting task representations were evaluated\nacross all defined languages. During this process,\nonly the adapters and soft prompts are trained,\nwhile the rest of the LLM remained frozen.\nLanguage Adapters & Task Adapters.\nBeyond\ntraining task representations alone, we also trained\na task adapter on top of a pre-trained language\nadapter, reproducing the approach outlined in\nMAD-X (Pfeiffer et al., 2020). Our method uti-\nlizes the same architecture but with distinct train-\ning hyperparameters, fitted to the tasks at hand. In\nthis setup, the task adapter takes the output of the\nlanguage adapter as input and further processes it.\nDuring training, only the task adapter is trained,\nwhile both the language adapter and LLM remain\nfrozen.\nAdapters and Soft Prompts Combinations.\nIn\nour study, we introduce two combinations of lan-\nguage and task representation using adapters and\nsoft prompts. The first configuration involves soft\ntask prompts along with a language adapter. This\ncombination incorporates trained language-specific\nknowledge using a language adapter, and a soft task\nprompt trained on the desired downstream task.\nThe second combination includes training a task\nadapter with the trained soft language prompt.\nSoft language prompts condition LLMs to activate\nknowledge specific to the desired language, while\ntask adapters learn task-specific knowledge.\nSoft Language Prompts & Soft Task Prompts.\nThe last configuration includes soft language and\nsoft task prompts. Inspired by stacking language\nand task adapters on top of each other, we concate-\nnated embeddings of language and task prompts to\na final soft prompt, with the LLM and soft language\nprompt being frozen during training.\n4\nExperiments\n4.1\nModel Selection\nWe selected an encoder-decoder architecture, the\nmT0-BASE model, to conduct a cross-lingual eval-\nuation. mT0 is based on the pre-trained multilin-\ngual mT5 model, which has been further fine-tuned\non a collection of 46 languages across 16 NLP\ntasks (Muennighoff et al., 2023). The model se-\nlection played a crucial role in further experiments\nand we conducted several preliminary experiments\nwith the original mT5-BASE model. However, we\nobserved that in the case of using the pre-trained\nmodel, which has not been further fine-tuned on\ndownstream tasks, prompt-tuning is not sufficient\nto guide the LLM to produce meaningful outputs.\n4.2\nLanguages\nThe original mT5 model was pre-trained on over\n100 languages, while mT0 employed only 46 for\nfurther fine-tuning. From the list of languages sup-\nported by mT5, we selected 16 languages and cate-\ngorized them into two groups: high- and mid- along\nwith low-resource languages. On the one hand, we\nconsider Arabic, German, English, Spanish, Rus-\nsian and Chinese to be high-resource languages.\nOn the other hand, we consider Czech, Greek, Ro-\nmanian and Slovenian as mid-resource and Bul-\ngarian, Malayalam, Slovak, Swahili, Telugu and\nUrdu as low-resource languages. Our distinction\nbetween these two groups is based on the number\nof resources available for each language (in terms\nof unlabelled and labelled data).\nWe included languages from various families\n(e.g., Indo-European, Dravidian, Sino-Tibetan) and\nscript types in the low-resource category, such as\nLatin, Arabic, Cyrillic and other non-Latin. The\npurpose of including multiple scripts and language\nfamilies in our cross-lingual evaluation is to in-\nvestigate the ability of the mT0 model to transfer\nknowledge between more similar and more distant\nLanguage Adapter +\nTask Adapter\nLanguage Adapter +\n Soft Task Prompt\nSoft Language Prompt +\nTask Adapter\nSoft Language Prompt +\nSoft Task Prompt\nMulti-Head\nAttention\nAdd & Norm\nFeed\nForward\nAdd & Norm\nInput Text\nSoft\nLanguage\nPrompt\nSoft Task\nPrompt\nMulti-Head\nAttention\nAdd & Norm\nFeed\nForward\nAdd & Norm\nLanguage\nAdapter\nAdd & Norm\nTask \nAdapter\nInput Text\nMulti-Head\nAttention\nAdd & Norm\nFeed\nForward\nAdd & Norm\nAdd & Norm\nTask\nAdapter\nInput Text\nSoft\nLanguage Prompt\nMulti-Head\nAttention\nAdd & Norm\nFeed\nForward\nAdd & Norm\nAdd & Norm\nLanguage\nAdapter\nInput Text\nSoft \nTask Prompt\nFigure 2: The architecture for all combinations of language and task representations in our experiments. These\nconfigurations include: (1) Language and Task Adapters; (2) Language Adapter and Soft Task Prompt; (3) Soft\nLanguage Prompt and Task Adapter; and (4) Soft Language and Soft Task Prompts. Language representations are\nin red, while task representations are in green color.\nlanguages, with respect to both script and language\nfeatures.\nTo train language representations on unlabelled\ndata, we selected Wikipedia as a source that con-\ntains many articles in various languages, including\nlow-resource ones. All Wikipedia data were taken\nfrom the latest preprocessed dump from Hugging-\nFace2, November 2023.\n4.3\nTasks\nIn order to evaluate the capabilities of mT0-BASE\nfor cross-lingual transfer, we choose four distinct\ntasks involving various NLP areas to explore the\nmodel performance. These tasks differ in the type\nof the provided output and include question answer-\ning (QA), named-entity recognition (NER), natural\nlanguage inference (NLI), and check-worthy claim\ndetection (CWCD). They were selected based on\nthe availability of datasets for selected languages\nand to include various NLP tasks related to reading\ncomprehension, recognizing textual entailment, or\nfact-checking. Table 1 lists the datasets used in our\nexperiments. For Bulgarian, there is no question\nanswering dataset available.\n2https://huggingface.co/datasets/wikimedia/\nwikipedia\nDue to the absence of datasets for some lan-\nguages, we employed Google Translate to translate\ndata for several languages. This concerns, in par-\nticular, the dataset for the Slovak NLI task and the\ndataset for check-worthy claim detection. In the\ncase of the missing Slovak NLI dataset, we uti-\nlized the CS ANLI dataset and translated it from\nCzech to Slovak. For check-worthy claim detec-\ntion, we translated the English dataset into multiple\nlanguages to obtain results for comparison.3\n4.4\nExperimental Setup\nLanguage Representations.\nLanguage adapters\nand soft prompts were trained using a span corrup-\ntion objective with different learning rates for train-\ning language adapters and soft language prompts,\nwhich were identified based on experiments on En-\nglish data. Detailed parameters are listed in Table 2\nin Appendix D.\nTask Representations.\nIn training task represen-\ntations, we divided the training set into training\nand validation splits using 15% of the records for\n3To evaluate the accuracy of the translations, we manually\nverified a subset of samples, with a particular focus on transla-\ntions between Czech and Slovak, leveraging input from native\nspeakers. Our analysis found that the translations generated\nby Google Translate were correct for this language pair.\nDataset\nTask\nLanguages\nCitation\nSQUAD\nQA\nen\nRajpurkar et al. (2016)\nMLQA\nQA\nar, de, hi, zh, es, vi\nLewis et al. (2019)\nXQuAD\nQA\nel, ro\nArtetxe et al. (2020)\nSK-QUAD\nQA\nsk\nHládek et al. (2023)\nCZECH SQUAD\nQA\ncs\nMacková and Straka (2020)\nTeQuAD\nQA\nte\nVemula et al. (2022)\nKenSWQuAD\nQA\nsw\nWanjawa et al. (2023)\nUQA\nQA\nur\nArif et al. (2024)\nSlovene SQuAD\nQA\nsl\nBoroviˇc et al. (2022)\nIndicQA\nQA\nml\nDoddapaneni et al. (2023)\nWikiANN\nNER\nar, bg, cs, de, el,\nen, es, ml, ro, ru,\nsl, sk, sw, te, ur, zh\nRahimi et al. (2019)\nXNLI\nNLI\nar, bg, de, el, en,\nes, ru, sw, ur, zh\nConneau et al. (2018)\nIndicXNLI\nNLI\nml, te\nAggarwal et al. (2022)\nCS ANLI\nNLI\ncs, sk*\nCS-ANLI\nRoNLI\nNLI\nro\nPoesina et al. (2024)\nSl-NLI\nNLI\nsl\nKlemen et al. (2024)\nMultiClaim\nCWCD\nar, bg, cs, de*, el*, en,\nes, ml*, ro*, ru*, sl*,\nsk, sw*, te*, ur*, zh*\nPikuliak et al. (2023)\nHyben et al. (2023)\nTable 1: The list of datasets used in our experiments.\nLanguages marked with * represent language versions\nof datasets that are not original but were obtained by\ntranslating texts from Czech (CS ANLI) or English\n(MultiClaim).\nvalidation, which was done only for datasets that\ndo not include a test set and the original validation\nsplit was considered a test set. This is especially the\ncase for the question answering and check-worthy\nclaim detection tasks. Secondly, we preprocessed\neach dataset by transforming each record from the\nparticular dataset into the text-to-text format em-\nploying prompt templates listed in Appendix B.\nTask representations in all configurations were\ntrained using the same training parameters across\nall tasks, with differences only between learning\nrates and weight decay.4 In addition, the instruc-\ntion used for training soft prompts differs across\nlanguages and tasks. These variations are based on\nthe language in which the answer is to be generated\nand the task that the LLM is solving.\nThe best model was chosen based on the perfor-\nmance on the validation split with respect to the\nloss. For classification tasks, we set the maximum\nnumber of tokens to generate based on the pre-\ndicted classes. This minimizes the problem that the\nLLM continues to generate an answer and enables\nus to evaluate the LLM’s performance correctly.\nTable 2 in Appendix D shows the exact parameters\nfor training language and task representations.\n4We employed only one seed due to computational and\ntime limitations. However, we performed a check of the gener-\nalizability of the approach by training the task representation\non the German version of the WikiANN dataset for NER using\ntwo additional seeds and evaluated cross-lingual transfer from\nGerman to six languages. The results are in Appendix F.\nEvaluation.\nFor evaluation, we selected several\nstandard metrics employed for particular tasks.\nSpecifically, we use the F1-Score or Accuracy for\nclassification tasks and QA in the SQUAD format.\nBesides the F1-Score for QA, we also calculated\nExact Match, assessing how many of the answers\nexactly match the ground truth.5 For the evaluation,\nwe employed metrics implemented in the Hugging\nFace evaluate library6.\nWe evaluated the results on cross-lingual trans-\nfer from high-resource languages to mid- and low-\nresource ones, where task representations were\ntrained on datasets in high-resource languages. We\naim to assess the combination of language repre-\nsentations of low-resource languages with task rep-\nresentations trained on datasets from high-resource\nlanguages, i.e., high-resource language as source\nlanguage and low-resource as target ones.\nEx-\ntended results are shown in Appendix E.\nBaselines.\nTo evaluate the proposed methods,\nwe employed several baseline approaches and\nconfigurations. Baselines include task adapters,\nsoft task prompts (prompt-tuning approach), and\nMAD-X, the combination of a language and task\nadapter Pfeiffer et al. (2020). These baselines pro-\nvided a foundation for assessing the effectiveness\nof cross-lingual transfer in our experiments.\n5\nResults and Analyis\nOverall Results.\nOur study on cross-lingual\ntransfer performance between high-resource and\nmid- and low-resource languages is summarized\nin Table 3 in Appendix E, which reports averaged\nmetrics across four tasks for mid- and low-resource\nlanguages. Additionally, Figure 3 demonstrates\nthe comparison of all combinations across high-\nresource languages, where the presented scores\nrepresent the average calculated across all tasks\nand all mid- and low-resource languages.\nThe results demonstrate that the selection of\nsource languages plays an important role in the\noverall results, with distinct languages demonstrat-\ning different performance gains. Using English as\na source language yielded the highest performance\nfor most mid- and low-resource languages when\nemploying task representations alone. A possible\nexplanation might be that multilingual models of-\n5Exact Match tends to underestimate models’ performance\nfor low-resource languages, where LLMs are not often able to\nproduce the exact answer with the correct grammar.\n6https://huggingface.co/docs/evaluate\nEnglish\nGerman\nSpanish\nArabic\nRussian\nChinese\n40\n45\n50\n55\n60\n65\nTask adapter\nTask prompt\nLang. Adapter + Task Adapter (MAD-X)\nLang. Adapter + Task Prompt\nLang. Prompt + Task Adapter\nLang. Prompt + Task Prompt\nFigure 3: Average performance for the transfer from the\n6 high-resource languages to all low-resource languages,\naveraged over all low-resource languages. The graph\ncompares different configurations with varying perfor-\nmance for cross-lingual transfer from high-resource to\nlow-resource languages. In most cases, the combination\nof soft language prompts with task adapters (purple)\nproved best.\nten remain biased toward the source language, even\nafter adaptation, as demonstrated in Alabi et al.\n(2024). They show that language adaptation in\nmodels primarily occurs in the final layers, while\nearlier predictions are still influenced by the source\nlanguage. However, for Bulgarian and Slovak, the\ncombination of soft language prompts with task\nadapters proved to be more effective.\nIn contrast, when using Arabic, German, Span-\nish and Russian as source languages, configura-\ntions combining language and task representations\nyielded superior scores. Specifically, transferring\nknowledge from Spanish using a combination of\nsoft language prompts and task adapters resulted\nin the highest performance. Therefore, this con-\nfiguration using Spanish enhanced the model’s\nperformance, making Spanish the most effective\nhigh-resource language for cross-lingual transfer\nbetween languages across various scripts.\nQuestion Answering.\nOur experiments (see Ta-\nble 4) revealed that the configuration of a soft lan-\nguage prompt and task adapter achieved the highest\nperformance in many cases in the QA task when\ntransferring to mid- and low-resource languages,\nwith only small differences across languages. This\nconfiguration was particularly effective for Greek,\nRomanian and Slovak, while for Telugu and Urdu,\nthe task adapter without language representation\noutperformed other configurations. This suggests\nthat the complexity of the target language cannot be\nsufficiently modeled based on the small number of\nWikipedia articles in those languages. Furthermore,\ncs\nel\nml\nro\nsl\nsk\nsw\nte\nur\nTarget language\nar\nde\nen\nes\nru\nzh\nSource language\n-3.39 -0.55 -1.27 -1.19 -3.33 -2.79 -0.81 -4.43 -0.63\n0.47\n2.67\n-0.57\n2.85\n4.24\n1.03\n0.21\n-1.51\n0.04\n4.18\n4.17\n0.02\n4.99\n5.78\n4.45\n0.33\n1.55\n0.69\n1.09\n2.41\n-0.63\n2.05\n4.52\n1.56\n-0.02 -1.00 -0.96\n-9.28 -1.76 -0.46 -2.62 -6.89 -3.64 -0.61 -3.10 -1.15\n-9.13 -4.99 -0.84 -5.55 -7.65 -7.57 -1.13 -4.73 -3.78\n8\n6\n4\n2\n0\n2\n4\nDifference in F1 score\nFigure 4: Relative F1 improvement for the QA task in\ntransferring knowledge between languages using soft\nlanguage prompts and task adapters. The effectiveness\nof the selected configuration is compared with the re-\nsults obtained without using any language and task rep-\nresentations (i.e., mT0-BASE inference).\nEnglish excelled across the board, particularly with\nLatin and Greek scripts, showcasing its adaptability\nin cross-lingual transfer.\nIn addition to investigating the effects of indi-\nvidual configurations, we also evaluated the im-\nprovement of a soft language prompt combined\nwith a task adapter over the original mT0-BASE\nmodel without any language or task representa-\ntions (see Figure 4). Figure 4 contains relative F1-\nScore improvements and demonstrates that train-\ning task representations in English and evaluating\nin other languages provide the most evident im-\nprovement. We also observed that German, En-\nglish and Spanish improved performance for most\nlow-resource languages, with the exception of Tel-\nugu and Malayalam. In contrast, Arabic, Russian\nand Chinese, which have different scripts, exhib-\nited negative transfer across all cases, with Arabic\nand Chinese offering no improvement for any lan-\nguages. We conjecture that the cross-lingual trans-\nfer depends on the script used for the language,\nwhere we achieved the highest performance for\nlanguages in the Latin script.\nNamed Entity Recognition.\nIn the case of the\nNER task, Arabic, German, Spanish and Russian,\namong high-resource languages, performed best\nin cross-lingual transfer to mid- and low-resource\nlanguages, while English and Chinese performed\npoorly. However, based on the results in Table 5,\nthe best improvements were observed using a soft\nlanguage prompt with a task adapter, outperform-\ning the combination of language and task adapters\nfor languages, such as Arabic, Spanish and German.\nThis is especially the case for Telugu, where the dif-\nference between these configurations is more than\n37% in favor of the combination of soft language\nprompt and task adapter using Russian data.\nNatural Language Inference.\nThe cross-lingual\nevaluation of the NLI task from Table 6 demon-\nstrated the effectiveness of almost all proposed con-\nfigurations for knowledge transfer. In particular,\nwe mostly achieve superior results using the combi-\nnation of language adapters with soft task prompts\nin Czech, Slovenian and Slovak as target languages.\nWhile for Swahili, Telugu and Urdu, the best perfor-\nmance was achieved without employing language\nrepresentations. Furthermore, the high effect on the\nRomanian language observed in the cross-lingual\nevaluation is probably because Romanian has been\ninvolved during the training of mT5, but not as part\nof fine-tuning the mT0 model.\nAcross the six proposed configurations for trans-\nferring knowledge, we observed improvement for\nmost languages. However, for Czech, Slovenian\nand Slovak, several configurations resulted in lower\nperformance compared to inference-only baselines.\nNotably, for Slovenian, using Russian for the soft\ntask prompt was the only configuration that outper-\nformed the inference-only approach. Furthermore,\nthe combination of language and task adapters for\nSlovenian resulted in the poorest performance, with\nan average deterioration of 63%.\nCheck-Worthy Claim Detection.\nFor check-\nworthy claim detection, the configuration of soft\nlanguage prompts and task adapters performs com-\nparably to methods without language representa-\ntions (see Table 7). When considering both the best\nand second-best results, this combination proves\neffective across most language pairs, demonstrat-\ning the model’s enhanced capabilities for check-\nworthy claim detection. Notably, using Spanish for\nknowledge transfer within this setup resulted in the\nhighest performance gains.\n6\nDiscussion\nBased on our experiments, we summarize our ob-\nservations below.\nPrompt Tuning Performs Better with Fine-\nTuned Models.\nIn our preliminary model selec-\ntion experiments, we found that prompt tuning\ndoes not improve the performance of pre-trained\nLLMs (e.g., mT5) trained only on unlabelled data\nfor downstream tasks. However, prompt-tuning\ncan enhance the performance of already fine-tuned\nLLMs on any labelled data, even if the specific\ntasks were not part of the prior fine-tuning. This\nwas confirmed in our experiments with NER and\ncheck-worthy claim detection, where fine-tuned\nLLMs delivered superior results despite no previ-\nous task-specific training on these tasks.\nSoft Language Prompts with Task Adapters Per-\nform Best in Many Cases.\nOur approach of com-\nbining soft language prompts with task adapters\ndemonstrated better performance in many cases,\ncompared to the approach of combined language\nand task adapters, which has been shown to be very\neffective in previous work. Specifically, the combi-\nnation of soft language prompts and task adapters\nis most effective on the classification tasks, achiev-\ning superior results most often. For languages with\na different script (e.g., Spanish and Telugu), these\ndifferences were over 20%.\nLanguage Representations are Unable to Cap-\nture Linguistic Characteristics Using Small\nNumber of Unlabelled Data.\nLanguage repre-\nsentations have several limitations that led to con-\nfigurations without language representations per-\nforming consistently better on cross-lingual trans-\nfer to highly low-resource languages, such as Tel-\nugu, Urdu, and Malayalam. We postulate that the\nreason is the small number of Wikipedia articles on\nwhich the language representations were trained,\nrendering them unable to adequately capture suffi-\ncient linguistic characteristics.\n7\nConclusion\nOur study provides a comprehensive evaluation of\nvarious configurations of adapters and soft prompts\nfor cross-lingual transfer in mid- and low-resource\nlanguages. With the systematic evaluation of task\nadapters, soft task prompts, and combinations of\nlanguage and task representations, we identified\nconfigurations that positively affect LLM’s perfor-\nmance across different tasks and languages. Our\nfindings demonstrated that the combination of soft\nlanguage prompts and task adapters emerged as\nan effective alternative for transferring knowledge\nbetween languages.\nFurthermore, our findings\nprovide valuable insights for the utilization of a\ncombination of PEFT methods for cross-lingual\ntransfer, while highlighting the need to incorporate\nlanguage-specific knowledge.\nLimitations\nModel Selection.\nOur analysis of the effective-\nness of the language and task representations fo-\ncused on highly multilingual LLMs that include\na wider variety of low-resource languages. From\nthis perspective, there is not a vast number of open-\nsource multilingual LLMs with such extensive lan-\nguage coverage as the mT5 or BLOOM model,\nwhile having fewer than 1B parameters. We also\nconsidered the AYA model (Üstün et al., 2024), but\ndue to limited computational resources, it was not\nfeasible to conduct our experiments. Another as-\npect of the selection was the involvement of only\ngenerative models consisting of encoder-decoder\nor decoder-only architecture.\nOther Languages.\nIn selecting appropriate lan-\nguages, we were limited by the languages cov-\nered by the mT5 model. To select high-resource\nlanguages, we considered languages that are the\nmost extensive in terms of available resources and\nare in different scripts, e.g., not only Latin script.\nOn the other hand, when selecting mid- and low-\nresource languages, we also considered the avail-\nability of datasets in multiple languages from differ-\nent language families as well as the availability of\ndatasets in those languages (both human-annotated\nand machine-translated).\nOther Tasks.\nThe tasks in our experiments were\nselected based on the availability of datasets for\neach selected language and covered multiple areas\nof the NLP domain, i.e., reading comprehension,\nfact-checking, and recognizing textual entailment.\nWe mostly considered tasks involved in the instruc-\ntion fine-tuning of the mT0-model, but we also\nincluded tasks that were not originally used to train\nthe mT0-model, e.g., named-entity recognition and\ncheck-worthy claim detection.\nAcknowledgements\nThis research was partially supported by DisAI - Im-\nproving scientific excellence and creativity in com-\nbating disinformation with artificial intelligence\nand language technologies, a project funded by\nHorizon Europe under GA No.101079164, and\nby the MIMEDIS, a project funded by the Slo-\nvak Research and Development Agency under GA\nNo. APVV-21-0114. This work was supported\nby the Ministry of Education, Youth and Sports\nof the Czech Republic through the e-INFRA CZ\n(ID:90254).\nReferences\nCS ANLI.\nhttps://huggingface.co/datasets/\nctu-aic/anli_cs. Accessed: 2024-05-30.\nDivyanshu Aggarwal,\nVivek Gupta,\nand Anoop\nKunchukuttan. 2022. IndicXNLI: Evaluating multi-\nlingual inference for Indian languages. In Proceed-\nings of the 2022 Conference on Empirical Methods in\nNatural Language Processing, pages 10994–11006,\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\nJesujoba Alabi, Marius Mosbach, Matan Eyal, Dietrich\nKlakow, and Mor Geva. 2024. The hidden space\nof transformer language adapters. In Proceedings\nof the 62nd Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 6588–6607, Bangkok, Thailand. Association\nfor Computational Linguistics.\nAlan Ansell, Edoardo Ponti, Anna Korhonen, and Ivan\nVuli´c. 2022. Composable sparse fine-tuning for cross-\nlingual transfer. In Proceedings of the 60th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 1778–1796,\nDublin, Ireland. Association for Computational Lin-\nguistics.\nAlan Ansell, Edoardo Maria Ponti, Jonas Pfeiffer, Se-\nbastian Ruder, Goran Glavaš, Ivan Vuli´c, and Anna\nKorhonen. 2021. MAD-G: Multilingual adapter gen-\neration for efficient cross-lingual transfer. In Find-\nings of the Association for Computational Linguis-\ntics: EMNLP 2021, pages 4762–4781, Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nSamee Arif, Sualeha Farid, Awais Athar, and Agha Ali\nRaza. 2024. UQA: Corpus for Urdu question answer-\ning. In Proceedings of the 2024 Joint International\nConference on Computational Linguistics, Language\nResources and Evaluation (LREC-COLING 2024),\npages 17237–17244, Torino, Italia. ELRA and ICCL.\nMikel Artetxe, Sebastian Ruder, and Dani Yogatama.\n2020. On the cross-lingual transferability of mono-\nlingual representations. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 4623–4637, Online. Association\nfor Computational Linguistics.\nAkari Asai, Mohammadreza Salehi, Matthew Pe-\nters, and Hannaneh Hajishirzi. 2022. ATTEMPT:\nParameter-efficient multi-task tuning via attentional\nmixtures of soft prompts.\nIn Proceedings of the\n2022 Conference on Empirical Methods in Natu-\nral Language Processing, pages 6655–6672, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nMladen Boroviˇc, Kristjan Žagar, Marko Ferme, Sandi\nMajninger, Milan Ojsteršek, Uroš Šmajdek, Maj\nZirkelbach, Matjaž Zupaniˇc, Meta Jazbinšek, Slavko\nŽitnik, et al. 2022. Slovene translation of the squad2.\n0 dataset.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Ad-\nina Williams, Samuel R. Bowman, Holger Schwenk,\nand Veselin Stoyanov. 2018. Xnli: Evaluating cross-\nlingual sentence representations. In Proceedings of\nthe 2018 Conference on Empirical Methods in Natu-\nral Language Processing. Association for Computa-\ntional Linguistics.\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and\nLuke Zettlemoyer. 2023. Qlora: Efficient finetuning\nof quantized llms. In Advances in Neural Information\nProcessing Systems, volume 36, pages 10088–10115.\nCurran Associates, Inc.\nSumanth Doddapaneni, Rahul Aralikatte, Gowtham\nRamesh, Shreya Goyal, Mitesh M. Khapra, Anoop\nKunchukuttan, and Pratyush Kumar. 2023. Towards\nleaving no Indic language behind: Building monolin-\ngual corpora, benchmark and models for Indic lan-\nguages. In Proceedings of the 61st Annual Meeting\nof the Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 12402–12426, Toronto,\nCanada. Association for Computational Linguistics.\nJunxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-\nKirkpatrick, and Graham Neubig. 2022. Towards a\nunified view of parameter-efficient transfer learning.\nPreprint, arXiv:2110.04366.\nDaniel Hládek, Ján Staš, Jozef Juhár, and Tomáš Koc-\ntúr. 2023. Slovak dataset for multilingual question\nanswering. IEEE Access, 11:32869–32881.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin De Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly.\n2019a. Parameter-efficient transfer learning for nlp.\nIn International conference on machine learning,\npages 2790–2799. PMLR.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin De Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly.\n2019b. Parameter-efficient transfer learning for NLP.\nIn Proceedings of the 36th International Conference\non Machine Learning, volume 97 of Proceedings\nof Machine Learning Research, pages 2790–2799.\nPMLR.\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Gra-\nham Neubig, Orhan Firat, and Melvin Johnson.\n2020. XTREME: A massively multilingual multi-\ntask benchmark for evaluating cross-lingual gener-\nalisation. In Proceedings of the 37th International\nConference on Machine Learning, volume 119 of\nProceedings of Machine Learning Research, pages\n4411–4421. PMLR.\nLianzhe Huang, Shuming Ma, Dongdong Zhang, Furu\nWei, and Houfeng Wang. 2022.\nZero-shot cross-\nlingual transfer of prompt-based tuning with a unified\nmultilingual prompt. In Proceedings of the 2022 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 11488–11497, Abu Dhabi, United\nArab Emirates. Association for Computational Lin-\nguistics.\nMartin Hyben, Sebastian Kula, Ivan Srba, Robert Moro,\nand Jakub Simko. 2023.\nIs it indeed bigger bet-\nter? the comprehensive study of claim detection\nlms applied for disinformation tackling. Preprint,\narXiv:2311.06121.\nMatej Klemen, Aleš Žagar, Jaka ˇCibej, and Marko\nRobnik-Šikonja. 2024. SI-NLI: A Slovene natural\nlanguage inference dataset and its evaluation. In Pro-\nceedings of the 2024 Joint International Conference\non Computational Linguistics, Language Resources\nand Evaluation (LREC-COLING 2024), pages 14859–\n14870, Torino, Italia. ELRA and ICCL.\nMateusz Klimaszewski, Piotr Andruszkiewicz, and\nAlexandra Birch. 2024.\nNo train but gain: Lan-\nguage arithmetic for training-free language adapters\nenhancement. Preprint, arXiv:2404.15737.\nJenny Kunz and Oskar Holmström. 2024. The impact\nof language adapters in cross-lingual transfer for nlu.\nPreprint, arXiv:2402.00149.\nJaeseong Lee, Seung-won Hwang, and Taesup Kim.\n2022.\nFAD-X: Fusing adapters for cross-lingual\ntransfer to low-resource languages. In Proceedings of\nthe 2nd Conference of the Asia-Pacific Chapter of the\nAssociation for Computational Linguistics and the\n12th International Joint Conference on Natural Lan-\nguage Processing (Volume 2: Short Papers), pages\n57–64, Online only. Association for Computational\nLinguistics.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3045–3059, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nPatrick Lewis, Barlas Oguz, Ruty Rinott, Sebas-\ntian Riedel, and Holger Schwenk. 2019.\nMlqa:\nEvaluating cross-lingual extractive question answer-\ning.\narXiv preprint arXiv:1910.07475, arXiv:\n1910.07475.\nLei Liu and Jimmy Xiangji Huang. 2023. Prompt learn-\ning to mitigate catastrophic forgetting in cross-lingual\ntransfer for open-domain dialogue generation. In Pro-\nceedings of the 46th International ACM SIGIR Con-\nference on Research and Development in Information\nRetrieval, SIGIR ’23, page 2287–2292, New York,\nNY, USA. Association for Computing Machinery.\nYun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie Zhou,\nand Yue Zhang. 2024. An empirical study of catas-\ntrophic forgetting in large language models during\ncontinual fine-tuning. Preprint, arXiv:2308.08747.\nKateˇrina Macková and Milan Straka. 2020. Reading\ncomprehension in czech via machine translation and\ncross-lingual transfer. In Text, Speech, and Dialogue,\npages 171–179, Cham. Springer International Pub-\nlishing.\nMichael McCloskey and Neal J. Cohen. 1989. Catas-\ntrophic interference in connectionist networks: The\nsequential learning problem. volume 24 of Psychol-\nogy of Learning and Motivation, pages 109–165. Aca-\ndemic Press.\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika,\nAdam Roberts, Stella Biderman, Teven Le Scao,\nM Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hai-\nley Schoelkopf, Xiangru Tang, Dragomir Radev,\nAlham Fikri Aji, Khalid Almubarak, Samuel Al-\nbanie, Zaid Alyafeai, Albert Webson, Edward Raff,\nand Colin Raffel. 2023.\nCrosslingual general-\nization through multitask finetuning.\nPreprint,\narXiv:2211.01786.\nMarinela Parovi´c, Goran Glavaš, Ivan Vuli´c, and Anna\nKorhonen. 2022. BAD-X: Bilingual adapters im-\nprove zero-shot cross-lingual transfer. In Proceed-\nings of the 2022 Conference of the North Ameri-\ncan Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages\n1791–1799, Seattle, United States. Association for\nComputational Linguistics.\nJonas Pfeiffer, Naman Goyal, Xi Lin, Xian Li, James\nCross, Sebastian Riedel, and Mikel Artetxe. 2022.\nLifting the curse of multilinguality by pre-training\nmodular transformers. In Proceedings of the 2022\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 3479–3495, Seattle,\nUnited States. Association for Computational Lin-\nguistics.\nJonas Pfeiffer, Francesco Piccinno, Massimo Nicosia,\nXinyi Wang, Machel Reid, and Sebastian Ruder.\n2023.\nmmT5: Modular multilingual pre-training\nsolves source language hallucinations. In Findings\nof the Association for Computational Linguistics:\nEMNLP 2023, pages 1978–2008, Singapore. Associ-\nation for Computational Linguistics.\nJonas Pfeiffer, Ivan Vuli´c, Iryna Gurevych, and Se-\nbastian Ruder. 2020. MAD-X: An Adapter-Based\nFramework for Multi-Task Cross-Lingual Transfer.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 7654–7673, Online. Association for Computa-\ntional Linguistics.\nFred Philippy, Siwen Guo, Shohreh Haddadan, Cedric\nLothritz, Jacques Klein, and Tegawendé F. Bissyandé.\n2024. Soft prompt tuning for cross-lingual trans-\nfer: When less is more. In Proceedings of the 1st\nWorkshop on Modular and Open Multilingual NLP\n(MOOMIN 2024), pages 7–15, St Julians, Malta. As-\nsociation for Computational Linguistics.\nMatúš Pikuliak, Ivan Srba, Robert Moro, Timo Hro-\nmadka, Timotej Smolen, Martin Melisek, Ivan\nVykopal, Jakub Simko, Juraj Podrouzek, and Maria\nBielikova. 2023.\nMultilingual previously fact-\nchecked claim retrieval. Preprint, arXiv:2305.07991.\nMatúš Pikuliak, Marián Šimko, and Mária Bieliková.\n2021.\nCross-lingual learning for text process-\ning: A survey. Expert Systems with Applications,\n165:113765.\nEduard Poesina, Cornelia Caragea, and Radu Ionescu.\n2024. A novel cartography-based curriculum learn-\ning method applied on RoNLI: The first Romanian\nnatural language inference corpus. In Proceedings\nof the 62nd Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 236–253, Bangkok, Thailand. Association for\nComputational Linguistics.\nEdoardo Maria Ponti, Goran Glavaš, Olga Majewska,\nQianchu Liu, Ivan Vuli´c, and Anna Korhonen. 2020.\nXcopa: A multilingual dataset for causal common-\nsense reasoning. Preprint, arXiv:2005.00333.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21(1).\nAfshin Rahimi, Yuan Li, and Trevor Cohn. 2019. Mas-\nsively multilingual transfer for NER. In Proceedings\nof the 57th Annual Meeting of the Association for\nComputational Linguistics, pages 151–164, Florence,\nItaly. Association for Computational Linguistics.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392, Austin,\nTexas. Association for Computational Linguistics.\nVipul Rathore, Rajdeep Dhingra, Parag Singla, and\nMausam. 2023. ZGUL: Zero-shot generalization to\nunseen languages using multi-source ensembling of\nlanguage adapters. In Proceedings of the 2023 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 6969–6987, Singapore. Associa-\ntion for Computational Linguistics.\nWeijieying Ren, Xinlong Li, Lei Wang, Tianxiang\nZhao, and Wei Qin. 2024. Analyzing and reducing\ncatastrophic forgetting in parameter efficient tuning.\nPreprint, arXiv:2402.18865.\nYuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Na-\nman Goyal, Vishrav Chaudhary, Jiatao Gu, and An-\ngela Fan. 2020. Multilingual translation with extensi-\nble multilingual pretraining and finetuning. Preprint,\narXiv:2008.00401.\nAhmet Üstün, Arianna Bisazza, Gosse Bouma, and Gert-\njan van Noord. 2020. UDapter: Language adaptation\nfor truly Universal Dependency parsing. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP), pages\n2302–2315, Online. Association for Computational\nLinguistics.\nRakesh Vemula, Mani Nuthi, and Manish Srivastava.\n2022. TeQuAD:Telugu question answering dataset.\nIn Proceedings of the 19th International Conference\non Natural Language Processing (ICON), pages 300–\n307, New Delhi, India. Association for Computa-\ntional Linguistics.\nTu Vu, Aditya Barua, Brian Lester, Daniel Cer, Mo-\nhit Iyyer, and Noah Constant. 2022a. Overcoming\ncatastrophic forgetting in zero-shot cross-lingual gen-\neration. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing,\npages 9279–9300, Abu Dhabi, United Arab Emirates.\nAssociation for Computational Linguistics.\nTu Vu, Brian Lester, Noah Constant, Rami Al-Rfou’,\nand Daniel Cer. 2022b. SPoT: Better frozen model\nadaptation through soft prompt transfer. In Proceed-\nings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 5039–5059, Dublin, Ireland. Association\nfor Computational Linguistics.\nBailin Wang, Zi Wang, Xuezhi Wang, Yuan Cao, Rif A.\nSaurous, and Yoon Kim. 2024. Grammar prompting\nfor domain-specific language generation with large\nlanguage models. In Proceedings of the 37th Interna-\ntional Conference on Neural Information Processing\nSystems, NIPS ’23, Red Hook, NY, USA. Curran\nAssociates Inc.\nBarack W. Wanjawa, Lilian D. A. Wanzare, Florence\nIndede, Owen Mconyango, Lawrence Muchemi, and\nEdward Ombui. 2023. Kenswquad—a question an-\nswering dataset for swahili low-resource language.\nACM Trans. Asian Low-Resour. Lang. Inf. Process.,\n22(4).\nBigScience Workshop, :, Teven Le Scao, Angela Fan,\nChristopher Akiki, Ellie Pavlick, Suzana Ili´c, Daniel\nHesslow, Roman Castagné, Alexandra Sasha Luc-\ncioni, François Yvon, Matthias Gallé, Jonathan\nTow, Alexander M. Rush, Stella Biderman, Albert\nWebson, Pawan Sasanka Ammanamanchi, Thomas\nWang, Benoît Sagot, Niklas Muennighoff, Albert Vil-\nlanova del Moral, Olatunji Ruwase, Rachel Bawden,\nStas Bekman, Angelina McMillan-Major, Iz Belt-\nagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pe-\ndro Ortiz Suarez, Victor Sanh, Hugo Laurençon,\nYacine Jernite, Julien Launay, Margaret Mitchell,\nColin Raffel, Aaron Gokaslan, Adi Simhi, Aitor\nSoroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers,\nAriel Kreisberg Nitzav, Canwen Xu, Chenghao Mou,\nChris Emezue, Christopher Klamm, Colin Leong,\nDaniel van Strien, David Ifeoluwa Adelani, Dragomir\nRadev, Eduardo González Ponferrada, Efrat Lev-\nkovizh, Ethan Kim, Eyal Bar Natan, Francesco De\nToni, Gérard Dupont, Germán Kruszewski, Giada\nPistilli, Hady Elsahar, Hamza Benyamina, Hieu Tran,\nIan Yu, Idris Abdulmumin, Isaac Johnson, Itziar\nGonzalez-Dios, Javier de la Rosa, Jenny Chim, Jesse\nDodge, Jian Zhu, Jonathan Chang, Jörg Frohberg,\nJoseph Tobing, Joydeep Bhattacharjee, Khalid Al-\nmubarak, Kimbo Chen, Kyle Lo, Leandro Von Werra,\nLeon Weber, Long Phan, Loubna Ben allal, Lu-\ndovic Tanguy, Manan Dey, Manuel Romero Muñoz,\nMaraim Masoud, María Grandury, Mario Šaško,\nMax Huang, Maximin Coavoux, Mayank Singh,\nMike Tian-Jian Jiang, Minh Chien Vu, Moham-\nmad A. Jauhar, Mustafa Ghaleb, Nishant Subramani,\nNora Kassner, Nurulaqilla Khamis, Olivier Nguyen,\nOmar Espejel, Ona de Gibert, Paulo Villegas, Pe-\nter Henderson, Pierre Colombo, Priscilla Amuok,\nQuentin Lhoest, Rheza Harliman, Rishi Bommasani,\nRoberto Luis López, Rui Ribeiro, Salomey Osei,\nSampo Pyysalo, Sebastian Nagel, Shamik Bose,\nShamsuddeen Hassan Muhammad, Shanya Sharma,\nShayne Longpre, Somaieh Nikpoor, Stanislav Silber-\nberg, Suhas Pai, Sydney Zink, Tiago Timponi Tor-\nrent, Timo Schick, Tristan Thrush, Valentin Danchev,\nVassilina Nikoulina, Veronika Laippala, Violette\nLepercq, Vrinda Prabhu, Zaid Alyafeai, Zeerak Ta-\nlat, Arun Raja, Benjamin Heinzerling, Chenglei Si,\nDavut Emre Ta¸sar, Elizabeth Salesky, Sabrina J.\nMielke, Wilson Y. Lee, Abheesht Sharma, Andrea\nSantilli, Antoine Chaffin, Arnaud Stiegler, Debajy-\noti Datta, Eliza Szczechla, Gunjan Chhablani, Han\nWang, Harshit Pandey, Hendrik Strobelt, Jason Alan\nFries, Jos Rozen, Leo Gao, Lintang Sutawika, M Sai-\nful Bari, Maged S. Al-shaibani, Matteo Manica, Ni-\nhal Nayak, Ryan Teehan, Samuel Albanie, Sheng\nShen, Srulik Ben-David, Stephen H. Bach, Taewoon\nKim, Tali Bers, Thibault Fevry, Trishala Neeraj, Ur-\nmish Thakker, Vikas Raunak, Xiangru Tang, Zheng-\nXin Yong, Zhiqing Sun, Shaked Brody, Yallow Uri,\nHadar Tojarieh, Adam Roberts, Hyung Won Chung,\nJaesung Tae, Jason Phang, Ofir Press, Conglong Li,\nDeepak Narayanan, Hatim Bourfoune, Jared Casper,\nJeff Rasley, Max Ryabinin, Mayank Mishra, Minjia\nZhang, Mohammad Shoeybi, Myriam Peyrounette,\nNicolas Patry, Nouamane Tazi, Omar Sanseviero,\nPatrick von Platen, Pierre Cornette, Pierre François\nLavallée, Rémi Lacroix, Samyam Rajbhandari, San-\nchit Gandhi, Shaden Smith, Stéphane Requena, Suraj\nPatil, Tim Dettmers, Ahmed Baruwa, Amanpreet\nSingh, Anastasia Cheveleva, Anne-Laure Ligozat,\nArjun Subramonian, Aurélie Névéol, Charles Lover-\ning, Dan Garrette, Deepak Tunuguntla, Ehud Reiter,\nEkaterina Taktasheva, Ekaterina Voloshina, Eli Bog-\ndanov, Genta Indra Winata, Hailey Schoelkopf, Jan-\nChristoph Kalo, Jekaterina Novikova, Jessica Zosa\nForde, Jordan Clive, Jungo Kasai, Ken Kawamura,\nLiam Hazan, Marine Carpuat, Miruna Clinciu, Na-\njoung Kim, Newton Cheng, Oleg Serikov, Omer\nAntverg, Oskar van der Wal, Rui Zhang, Ruochen\nZhang, Sebastian Gehrmann, Shachar Mirkin, Shani\nPais, Tatiana Shavrina, Thomas Scialom, Tian Yun,\nTomasz Limisiewicz, Verena Rieser, Vitaly Protasov,\nVladislav Mikhailov, Yada Pruksachatkun, Yonatan\nBelinkov, Zachary Bamberger, Zdenˇek Kasner, Al-\nice Rueda, Amanda Pestana, Amir Feizpour, Ammar\nKhan, Amy Faranak, Ana Santos, Anthony Hevia,\nAntigona Unldreaj, Arash Aghagol, Arezoo Abdol-\nlahi, Aycha Tammour, Azadeh HajiHosseini, Bahareh\nBehroozi, Benjamin Ajibade, Bharat Saxena, Car-\nlos Muñoz Ferrandis, Daniel McDuff, Danish Con-\ntractor, David Lansky, Davis David, Douwe Kiela,\nDuong A. Nguyen, Edward Tan, Emi Baylor, Ez-\ninwanne Ozoani, Fatima Mirza, Frankline Onon-\niwu, Habib Rezanejad, Hessie Jones, Indrani Bhat-\ntacharya, Irene Solaiman, Irina Sedenko, Isar Ne-\njadgholi, Jesse Passmore, Josh Seltzer, Julio Bonis\nSanz, Livia Dutra, Mairon Samagaio, Maraim El-\nbadri, Margot Mieskes, Marissa Gerchick, Martha\nAkinlolu, Michael McKenna, Mike Qiu, Muhammed\nGhauri, Mykola Burynok, Nafis Abrar, Nazneen Ra-\njani, Nour Elkott, Nour Fahmy, Olanrewaju Samuel,\nRan An, Rasmus Kromann, Ryan Hao, Samira Al-\nizadeh, Sarmad Shubber, Silas Wang, Sourav Roy,\nSylvain Viguier, Thanh Le, Tobi Oyebade, Trieu Le,\nYoyo Yang, Zach Nguyen, Abhinav Ramesh Kashyap,\nAlfredo Palasciano, Alison Callahan, Anima Shukla,\nAntonio Miranda-Escalada, Ayush Singh, Benjamin\nBeilharz, Bo Wang, Caio Brito, Chenxi Zhou, Chirag\nJain, Chuxin Xu, Clémentine Fourrier, Daniel León\nPeriñán, Daniel Molano, Dian Yu, Enrique Manjava-\ncas, Fabio Barth, Florian Fuhrimann, Gabriel Altay,\nGiyaseddin Bayrak, Gully Burns, Helena U. Vrabec,\nImane Bello, Ishani Dash, Jihyun Kang, John Giorgi,\nJonas Golde, Jose David Posada, Karthik Ranga-\nsai Sivaraman, Lokesh Bulchandani, Lu Liu, Luisa\nShinzato, Madeleine Hahn de Bykhovetz, Maiko\nTakeuchi, Marc Pàmies, Maria A Castillo, Mari-\nanna Nezhurina, Mario Sänger, Matthias Samwald,\nMichael Cullan, Michael Weinberg, Michiel De\nWolf, Mina Mihaljcic, Minna Liu, Moritz Freidank,\nMyungsun Kang, Natasha Seelam, Nathan Dahlberg,\nNicholas Michio Broad, Nikolaus Muellner, Pascale\nFung, Patrick Haller, Ramya Chandrasekhar, Renata\nEisenberg, Robert Martin, Rodrigo Canalli, Rosaline\nSu, Ruisi Su, Samuel Cahyawijaya, Samuele Garda,\nShlok S Deshmukh, Shubhanshu Mishra, Sid Ki-\nblawi, Simon Ott, Sinee Sang-aroonsiri, Srishti Ku-\nmar, Stefan Schweter, Sushil Bharati, Tanmay Laud,\nThéo Gigant, Tomoya Kainuma, Wojciech Kusa, Ya-\nnis Labrak, Yash Shailesh Bajaj, Yash Venkatraman,\nYifan Xu, Yingxin Xu, Yu Xu, Zhe Tan, Zhongli\nXie, Zifan Ye, Mathilde Bras, Younes Belkada, and\nThomas Wolf. 2023.\nBloom: A 176b-parameter\nopen-access multilingual language model. Preprint,\narXiv:2211.05100.\nShijie Wu and Mark Dredze. 2020.\nAre all lan-\nguages created equal in multilingual bert? Preprint,\narXiv:2005.09093.\nZhihui Xie, Handong Zhao, Tong Yu, and Shuai Li.\n2024. Discovering low-rank subspaces for language-\nagnostic multilingual representations.\nPreprint,\narXiv:2401.05792.\nLingling Xu, Haoran Xie, Si-Zhao Joe Qin, Xiaohui\nTao, and Fu Lee Wang. 2023. Parameter-efficient\nfine-tuning methods for pretrained language mod-\nels: A critical review and assessment.\nPreprint,\narXiv:2312.12148.\nLinting Xue, Noah Constant, Adam Roberts, Mihir\nKale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua,\nand Colin Raffel. 2021. mt5: A massively multilin-\ngual pre-trained text-to-text transformer. Preprint,\narXiv:2010.11934.\nQingru Zhang, Minshuo Chen, Alexander Bukharin,\nNikos Karampatziakis, Pengcheng He, Yu Cheng,\nWeizhu Chen, and Tuo Zhao. 2023. Adalora: Adap-\ntive budget allocation for parameter-efficient fine-\ntuning. Preprint, arXiv:2303.10512.\nAhmet Üstün, Viraat Aryabumi, Zheng-Xin Yong, Wei-\nYin Ko, Daniel D’souza, Gbemileke Onilude, Neel\nBhandari, Shivalika Singh, Hui-Lee Ooi, Amr Kayid,\nFreddie Vargus, Phil Blunsom, Shayne Longpre,\nNiklas Muennighoff, Marzieh Fadaee, Julia Kreutzer,\nand Sara Hooker. 2024. Aya model: An instruction\nfinetuned open-access multilingual language model.\nPreprint, arXiv:2402.07827.\nA\nComputational Resources\nFor our experiments, we utilized a computational\ninfrastructure consisting of A10 and A40 NVIDIA\nGPUs, while our experiments ran in parallel on\nmultiple GPUs. In total, our experiments required\naround 3,200 GPU hours, ensuring model training\nand validation for cross-lingual transfer.\nB\nPrompts Used\nFor the purpose of the encoder-decoder model, the\nrecord from each dataset needs to be transformed\ninto a text-to-text format. To choose an appropri-\nate prompt format, we experimented with all the\nprompts used in the mT0 paper (Muennighoff et al.,\n2023) and with prompts used in the T5 paper (Raf-\nfel et al., 2020). Prompts, which achieved the best\nperformance during inference with the mT0-BASE\nmodel, were selected for transforming the records\ninto a text-to-text format. In the following para-\ngraphs, there are the prompts for the individual\ntasks that have been used to convert to text-to-text\nformat.\nB.1\nQuestion Answering\nTemplate: question:\n{question} context:\n{context}\nB.2\nNatural Language Inference\nTemplate: {premise} \\n\\n Question:\nDoes\nthis imply that \"{hypothesis}\"? Yes, no,\nor maybe?\nB.3\nNamed-Entity Recognition\nTemplate: tag: {text}\nB.4\nCheck-Worthy Claim Detection\nTemplate: checkworthiness claim: {claim}\nC\nSoft Prompt Initialization\nThis section includes templates for soft prompts\nused for the initialization for each language and\neach task. Templates are divided into language and\ntask templates.\nC.1\nLanguage Templates\nTo train language representation using a language\nmodeling objective, we employed a specific prompt\nthat varied only based on the language present in\nthe instruction, leaving the rest of the instruction\nthe same.\nThe template we used for initialization is as fol-\nlows: \"Generate the output in {Language}:\",\nwhere the Language is replaced by the desired lan-\nguage.\nC.2\nTask Templates\nThe following are initialization prompt templates\nfor each task, where the instruction depends not\nonly on the task but also on the language.\nQuestion Answering.\nFor the question answer-\ning task, we utilized \"Answer\nthe\nquestion\nin\n{Language}\nlanguage:\", while replacing\nLanguage with the desired language.\nNatural Language Inference.\nNatural language\ninference is the task of assessing whether a hypothe-\nsis logically follows from the premise. It is defined\nas a classification with three possible classes: en-\ntailment, contradiction or neutral. However, based\non the previous work and instruction tuning of the\nmT0 model, we replaced above mentioned classes\nwith Yes, No and Maybe, based on the used prompt\ntemplate.\nAccording to the employed classes, we defined\nan initialization prompt as follows: \"Select Yes,\nNo\nor\nMaybe\nbased\non\nthe\nimplication\nof\nthe\npremise\non\nthe\nhypothesis\nin\n{Language}:\", while Language is replaced by the\ndesired language.\nNamed-Entity Recognition.\nThe named-entity\nrecognition task aims to identify named entities\nwithin the input text. While there are many possible\ncategories, the WikiANN dataset focuses only on\ndetecting three categories: location (LOC), person\n(PER) and organization (ORG). Based on the de-\nfined classes, we created the initialization prompt\nas follows: \"Identify NER tags (ORG, PER,\nLOC) in the text in {Language}:\", where\nLanguage is substituted with the specific language.\nCheck-Worthy Claim Detection.\nThe latter task\nincludes check-worthy claim detection, which is\na binary classification of assessing whether the\ngiven claim is worthy of fact-checking or not. As\ntext labels, we used Not checkworthy and Check-\nworthy. This is the initialization prompt for the\ncheck-worthy claim detection task: Determine\nwhether a given claim in {Language} is\ncheckworthy:\", where Language is replaced by\nthe desired language.\nD\nHyperparameters\nTable 2 shows hyperparameters used for training\nlanguage and task representations using adapters\nand soft prompts.\nE\nCross-Lingual Evaluation\nTables 4 to 7 present the results for transferring\nknowledge from all high-resource languages to all\nmid- and low-resource languages. The first row\nin each table represents the scores obtained by in-\nference of the original mT0-BASE model without\nadditional training of language or task representa-\ntions.\nF\nEvaluation with Multiple Training\nSeeds\nIn Table 8, we report the evaluation results of all\nconfigurations that were trained on the German ver-\nsion of the WikiANN dataset using three different\nseeds. Along with the mean values, we also report\nthe standard deviation\nThe obtained results demonstrate that the best\nresults for knowledge transfer from German to\nother languages are obtained by using task adapters\nfor Bulgarian, Greek, Malayalam, Romanian and\nSwahili.\nIn contrast, the best combination for\nCzech, Slovenian, Slovak, Telugu and Urdu was a\nsoft language prompt with a task adapter. This\nobservation supports our previous findings that\nboth configurations achieved superior results on\nthe NER task when transferring knowledge from\nGerman.\nHyperparameters\nLanguage Modeling\nTask Modeling\nLanguage Adapter\nSoft Language Prompt\nTask Adapter\nSoft Task Prompt\nLearning rate\n5e-5\n5e-1\n5e-5\n5e-1\nWeight decay\n0\n1e-5\n0\n1e-5\nBatch size\n32\n32\n32\n32\nNo. Training steps\n100,000\n100,000\n50,000\n50,000\nOptimizer\nAdamW\nAdafactor\nAdamW\nAdafactor\nEvaluation steps\n500\n500\n1000\n1000\nMax input length\n256\n256\n256\n256\nToken size of soft prompt\nNaN\n50\nNaN\n50\nTable 2: Final parameters employed to train language and task representation using adapters and soft prompts.\nTask\nLanguage\nLanguage\nRepresentation\nTask\nRepresentation\nbg\ncs\nel\nml\nro\nsl\nsk\nsw\nte\nur\nar\nNone\nAdapter\n68.03\n48.98\n65.25\n46.74\n63.44\n48.74\n48.26\n50.86\n53.80\n48.15\nSoft Prompt\n64.67\n43.16\n62.66\n47.55\n64.43\n44.60\n40.33\n47.22\n51.44\n43.81\nAdapter\nAdapter\n64.82\n42.56\n64.59\n49.82\n61.73\n30.44\n39.92\n48.99\n40.36\n41.03\nSoft Prompt\n69.90\n39.84\n63.36\n48.49\n55.78\n55.53\n41.28\n49.52\n41.45\n43.75\nSoft Prompt\nAdapter\n71.23\n48.06\n67.22\n48.22\n65.66\n51.68\n48.26\n51.21\n53.54\n49.63\nSoft Prompt\n66.73\n41.59\n62.82\n47.66\n58.81\n44.69\n40.02\n44.10\n50.11\n44.72\nde\nNone\nAdapter\n67.76\n51.72\n71.12\n51.52\n71.55\n54.88\n51.27\n56.78\n57.50\n51.82\nSoft Prompt\n65.31\n47.61\n69.55\n51.77\n70.02\n50.97\n46.53\n53.16\n52.91\n46.66\nAdapter\nAdapter\n79.98\n53.81\n73.48\n50.39\n66.34\n49.69\n54.57\n54.29\n45.65\n45.88\nSoft Prompt\n72.61\n47.19\n68.12\n48.81\n60.14\n56.41\n46.63\n51.20\n44.41\n44.76\nSoft Prompt\nAdapter\n70.71\n51.91\n70.83\n50.86\n69.86\n57.76\n53.98\n55.90\n55.90\n52.34\nSoft Prompt\n68.98\n49.02\n69.83\n50.98\n69.18\n52.18\n48.36\n54.50\n54.50\n45.04\nen\nNone\nAdapter\n68.77\n49.64\n70.37\n46.34\n67.29\n52.54\n47.98\n49.70\n52.13\n48.29\nSoft Prompt\n64.81\n41.26\n68.02\n46.60\n70.58\n52.32\n40.19\n51.43\n52.43\n50.48\nAdapter\nAdapter\n60.90\n46.51\n65.03\n41.80\n72.02\n37.96\n43.30\n46.98\n38.47\n40.09\nSoft Prompt\n64.68\n38.29\n65.04\n42.93\n74.50\n54.80\n37.41\n48.36\n37.48\n52.54\nSoft Prompt\nAdapter\n68.95\n48.37\n69.39\n43.44\n64.20\n51.34\n50.88\n51.05\n49.16\n45.78\nSoft Prompt\n56.33\n47.75\n65.98\n43.22\n58.94\n52.64\n46.48\n48.96\n47.12\n38.62\nes\nNone\nAdapter\n68.65\n53.61\n70.06\n49.75\n73.83\n56.23\n51.82\n57.47\n57.12\n54.01\nSoft Prompt\n62.16\n50.28\n66.95\n47.26\n71.84\n50.34\n49.05\n54.09\n53.94\n49.71\nAdapter\nAdapter\n73.89\n50.03\n73.33\n51.15\n70.92\n45.79\n53.04\n54.24\n45.45\n44.63\nSoft Prompt\n72.63\n50.19\n64.62\n44.62\n72.70\n47.07\n51.71\n51.95\n39.95\n43.94\nSoft Prompt\nAdapter\n75.37\n55.23\n73.50\n51.39\n72.03\n58.88\n54.84\n57.41\n56.90\n54.00\nSoft Prompt\n69.07\n52.16\n64.43\n48.19\n72.98\n53.21\n50.93\n53.87\n50.89\n50.26\nru\nNone\nAdapter\n82.44\n45.29\n65.74\n46.79\n69.68\n49.24\n45.24\n54.40\n56.48\n52.27\nSoft Prompt\n79.26\n41.51\n60.83\n48.64\n68.09\n45.29\n40.70\n50.73\n55.13\n50.94\nAdapter\nAdapter\n80.74\n52.52\n73.86\n45.76\n73.75\n40.19\n51.99\n52.08\n39.55\n42.11\nSoft Prompt\n77.96\n34.33\n66.75\n44.48\n71.76\n49.84\n36.41\n50.61\n40.14\n45.31\nSoft Prompt\nAdapter\n83.96\n44.69\n70.25\n49.87\n72.15\n51.06\n47.34\n55.21\n56.89\n53.62\nSoft Prompt\n79.68\n44.50\n71.02\n50.63\n70.94\n52.60\n40.99\n52.37\n54.91\n52.04\nzh\nNone\nAdapter\n61.19\n43.74\n57.03\n40.50\n64.60\n44.42\n42.70\n50.23\n45.65\n44.33\nSoft Prompt\n58.20\n45.17\n59.24\n42.23\n66.36\n44.72\n43.38\n46.27\n50.10\n43.72\nAdapter\nAdapter\n62.73\n46.45\n61.14\n37.45\n66.19\n35.30\n42.70\n43.73\n35.98\n36.28\nSoft Prompt\n49.40\n43.50\n47.04\n42.18\n45.70\n56.09\n36.34\n42.75\n37.22\n39.00\nSoft Prompt\nAdapter\n65.97\n45.20\n60.49\n41.62\n65.64\n48.49\n45.52\n52.19\n50.20\n42.72\nSoft Prompt\n62.44\n48.74\n61.62\n43.09\n67.48\n49.20\n44.85\n48.53\n51.09\n44.69\nTable 3: Average scores for each configuration across all tasks for low-resource languages. The languages in rows\nrepresent the language in which the task representation was trained, and the languages in columns represent the\nlanguage representation that was used, if any (except for configurations with None in the language representation).\nFor each language pair, the best results are boldfaced and the second best are underlined.\nTask\nLanguage\nLanguage\nRepresentation\nTask\nRepresentation\ncs\nel\nml\nro\nsl\nsk\nsw\nte\nur\nNone\nNone\n31.34 (24.78)\n57.00 (47.56)\n1.37 (1.07)\n57.00 (47.56)\n31.50 (22.58)\n26.39 (9.78)\n3.24 (0.36)\n18.64 (12.10)\n13.37 (7.02)\nAdapter\n29.14 (21.80)\n55.90 (46.30)\n0.22 (18.94)\n55.90 (46.30)\n27.83 (19.74)\n23.15 (8.44)\n2.39 (0.36)\n15.70 (11.00)\n13.00 (10.38)\nNone\nSoft Prompt\n22.93 (18.02)\n56.08 (46.89)\n0.10 (0.82)\n56.08 (46.89)\n23.91 (17.33)\n19.89 (7.85)\n0.56 (0.18)\n11.53 (8.60)\n12.05 (8.29)\nAdapter\n25.60 (14.45)\n45.96 (35.71)\n1.12 (4.15)\n43.17 (31.85)\n23.55 (11.82)\n22.84 (6.03)\n1.77 (0.18)\n9.54 (4.90)\n9.04 (4.08)\nAdapter\nSoft Prompt\n24.70 (1.67)\n52.76 (42.94)\n0.95 (0.63)\n46.35 (35.38)\n27.65 (16.04)\n25.10 (1.28)\n2.57 (1.09)\n10.50 (5.10)\n10.43 (5.84)\nAdapter\n27.95 (20.78)\n56.45 (46.97)\n0.10 (21.27)\n55.81 (46.47)\n28.17 (19.88)\n23.60 (8.72)\n2.43 (0.45)\n14.21 (10.10)\n12.74 (10.07)\nar\nSoft Prompt\nSoft Prompt\n27.14 (20.55)\n55.51 (46.72)\n0.03 (0.31)\n56.11 (47.23)\n26.00 (18.62)\n20.96 (8.04)\n0.77 (0.27)\n11.04 (8.30)\n12.37 (8.63)\nAdapter\n35.37 (27.29)\n58.88 (49.08)\n0.99 (2.27)\n58.88 (49.08)\n36.15 (24.66)\n27.51 (10.16)\n3.84 (0.82)\n18.81 (12.80)\n14.02 (10.27)\nNone\nSoft Prompt\n28.56 (21.31)\n57.12 (47.65)\n0.57 (2.20)\n57.12 (47.65)\n30.34 (20.52)\n24.54 (8.92)\n1.93 (0.73)\n12.46 (9.70)\n10.54 (8.21)\nAdapter\n36.78 (27.82)\n57.46 (47.31)\n1.05 (1.38)\n57.69 (47.90)\n38.03 (25.07)\n31.43 (11.35)\n3.01 (0.54)\n11.32 (6.00)\n9.62 (4.72)\nAdapter\nSoft Prompt\n38.13 (31.20)\n54.70 (45.29)\n1.88 (0.94)\n55.11 (46.55)\n38.88 (27.69)\n30.70 (11.98)\n3.13 (1.00)\n16.05 (8.70)\n12.63 (8.01)\nAdapter\n31.81 (24.29)\n59.67 (48.91)\n0.80 (2.08)\n59.85 (49.08)\n35.74 (23.84)\n27.42 (10.16)\n3.45 (0.36)\n17.13 (11.20)\n13.41 (10.25)\nde\nSoft Prompt\nSoft Prompt\n32.68 (24.65)\n57.86 (48.15)\n0.58 (8.68)\n58.38 (49.08)\n30.67 (20.78)\n27.95 (10.30)\n2.25 (1.09)\n12.34 (9.50)\n9.35 (9.58)\nAdapter\n36.95 (28.57)\n60.24 (50.34)\n1.18 (0.94)\n60.24 (50.34)\n37.04 (26.07)\n30.11 (11.46)\n3.21 (0.36)\n19.65 (12.70)\n13.93 (9.37)\nNone\nSoft Prompt\n33.59 (25.55)\n60.35 (50.76)\n0.82 (0.94)\n60.35 (50.76)\n34.77 (24.36)\n27.76 (10.07)\n2.81 (0.45)\n19.38 (12.80)\n13.81 (9.25)\nAdapter\n33.75 (24.33)\n57.44 (48.24)\n1.25 (1.70)\n58.11 (49.33)\n31.65 (19.76)\n28.98 (10.58)\n3.25 (0.63)\n9.85 (5.00)\n9.06 (4.20)\nAdapter\nSoft Prompt\n33.94 (26.22)\n58.49 (49.58)\n0.97 (0.25)\n57.72 (48.74)\n35.89 (24.36)\n29.82 (11.54)\n3.06 (0.45)\n12.67 (6.60)\n9.67 (4.94)\nAdapter\n35.52 (27.27)\n61.17 (51.68)\n1.39 (2.71)\n61.99 (52.10)\n37.28 (25.96)\n30.84 (11.73)\n3.57 (0.27)\n20.19 (13.50)\n14.06 (10.54)\nen\nSoft Prompt\nSoft Prompt\n35.35 (27.02)\n59.80 (50.34)\n0.68 (2.20)\n60.02 (50.34)\n36.49 (26.02)\n29.89 (11.39)\n3.46 (0.54)\n11.71 (7.30)\n9.83 (5.68)\nAdapter\n33.72 (25.43)\n59.61 (50.34)\n1.29 (1.20)\n59.61 (50.34)\n34.97 (32.80)\n27.24 (9.71)\n3.42 (0.82)\n19.10 (12.50)\n13.03 (9.48)\nNone\nSoft Prompt\n25.98 (19.06)\n54.72 (45.29)\n0.12 (0.31)\n54.72 (45.29)\n27.38 (18.50)\n22.43 (7.62)\n1.10 (0.36)\n11.94 (9.00)\n9.13 (6.84)\nAdapter\n33.98 (23.88)\n55.44 (45.13)\n1.06 (1.26)\n56.58 (46.22)\n34.52 (21.06)\n28.66 (10.01)\n2.34 (0.45)\n10.76 (5.00)\n9.12 (4.81)\nAdapter\nSoft Prompt\n32.86 (24.45)\n53.57 (43.78)\n0.98 (0.57)\n52.82 (43.45)\n34.73 (23.35)\n27.82 (10.35)\n2.49 (0.54)\n10.89 (5.40)\n9.27 (4.17)\nAdapter\n32.43 (24.92)\n59.41 (49.75)\n0.74 (1.07)\n59.05 (49.24)\n36.02 (24.92)\n27.95 (10.27)\n3.22 (0.45)\n17.64 (11.40)\n12.41 (8.42)\nes\nSoft Prompt\nSoft Prompt\n27.21 (19.82)\n54.39 (44.45)\n0.27 (0.82)\n54.93 (45.21)\n28.25 (19.39)\n21.49 (7.49)\n1.11 (0.18)\n9.28 (7.20)\n8.36 (6.11)\nAdapter\n27.45 (15.86)\n55.56 (42.52)\n0.73 (5.73)\n55.56 (42.52)\n24.81 (14.05)\n23.69 (7.96)\n2.93 (0.45)\n16.80 (10.50)\n12.45 (10.12)\nNone\nSoft Prompt\n17.94 (8.27)\n51.61 (37.73)\n0.25 (0.13)\n51.61 (37.73)\n17.58 (8.13)\n16.98 (4.10)\n0.78 (0.27)\n12.40 (8.00)\n8.71 (5.49)\nAdapter\n31.24 (14.98)\n54.17 (40.25)\n1.27 (0.76)\n54.72 (40.50)\n32.90 (17.04)\n32.13 (9.71)\n2.72 (0.18)\n10.66 (5.30)\n8.98 (3.85)\nAdapter\nSoft Prompt\n32.53 (19.94)\n51.63 (38.91)\n0.89 (0.25)\n51.78 (37.31)\n33.57 (18.08)\n34.00 (13.34)\n1.96 (0.09)\n10.37 (4.80)\n8.78 (3.91)\nAdapter\n22.06 (12.33)\n55.24 (42.44)\n0.91 (1.32)\n54.38 (41.60)\n24.61 (14.26)\n22.75 (7.53)\n2.63 (0.27)\n15.54 (9.50)\n12.22 (7.80)\nru\nSoft Prompt\nSoft Prompt\n32.13 (18.00)\n53.90 (40.00)\n0.99 (0.25)\n53.37 (39.83)\n29.68 (14.91)\n31.42 (10.26)\n2.06 (0.18)\n16.86 (9.20)\n11.63 (6.04)\nAdapter\n22.06 (16.33)\n50.83 (40.08)\n0.65 (0.50)\n50.83 (40.08)\n21.96 (14.96)\n18.38 (6.74)\n1.42 (0.27)\n13.29 (9.20)\n9.66 (6.31)\nNone\nSoft Prompt\n26.25 (20.33)\n56.57 (47.39)\n0.65 (0.44)\n56.57 (47.39)\n25.64 (18.29)\n22.32 (8.53)\n0.91 (0.45)\n16.00 (11.80)\n12.49 (9.17)\nAdapter\n26.10 (16.29)\n43.57 (30.34)\n1.30 (0.57)\n41.48 (28.57)\n26.16 (14.21)\n22.50 (6.17)\n1.94 (0.36)\n11.77 (6.50)\n9.04 (5.01)\nAdapter\nSoft Prompt\n24.83 (14.06)\n47.06 (35.80)\n1.08 (0.06)\n39.35 (25.55)\n25.58 (12.45)\n24.19 (5.44)\n2.64 (0.82)\n10.34 (4.90)\n8.90 (4.24)\nAdapter\n22.21 (16.90)\n52.01 (41.01)\n0.53 (0.44)\n51.45 (40.25)\n23.85 (16.20)\n18.82 (6.95)\n2.11 (0.45)\n13.91 (9.10)\n9.59 (6.26)\nzh\nSoft Prompt\nSoft Prompt\n31.34 (24.20)\n56.51 (47.90)\n1.23 (0.82)\n56.39 (47.98)\n29.25 (21.17)\n25.06 (9.42)\n1.85 (0.45)\n17.52 (12.20)\n12.80 (9.48)\nTable 4: Results for the question answering task for cross-lingual transfer from high-resource to mid- and low-\nresource languages. The results are reported as F1-Score (Exact Match). For each source-target language pair,\nthe best-performing result is highlighted in bold, while the second-best scores are underlined. Additionally,\nlanguage pairs with improved performance compared to inference-only (without incorporating any language or task\nrepresentation) are marked in green, and those with decreased performance are marked in red.\nTask\nLanguage\nLanguage\nRepresentation\nTask\nRepresentation\nbg\ncs\nel\nml\nro\nsl\nsk\nsw\nte\nur\nNone\nNone\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\nar\nNone\nAdapter\n44.54\n60.54\n44.29\n31.32\n45.73\n55.63\n60.30\n49.92\n48.53\n32.10\nSoft Prompt\n38.11\n50.72\n39.73\n38.32\n36.44\n48.28\n48.69\n46.90\n45.53\n20.61\nAdapter\nAdapter\n63.91\n63.98\n63.89\n49.19\n53.44\n24.02\n62.98\n49.38\n18.05\n27.71\nSoft Prompt\n64.09\n40.09\n52.23\n48.21\n55.95\n44.67\n54.13\n51.37\n26.65\n25.77\nSoft Prompt\nAdapter\n53.66\n62.72\n52.68\n41.95\n54.82\n60.70\n64.62\n55.43\n51.43\n43.36\nSoft Prompt\n48.84\n40.30\n46.02\n42.93\n33.25\n37.20\n44.48\n30.41\n46.01\n25.18\nde\nNone\nAdapter\n30.63\n66.61\n53.35\n40.49\n62.39\n56.11\n67.96\n60.19\n48.56\n34.37\nSoft Prompt\n31.61\n61.04\n54.21\n44.55\n51.52\n54.04\n62.28\n57.26\n43.20\n24.21\nAdapter\nAdapter\n68.49\n64.70\n68.83\n46.45\n60.64\n49.32\n68.42\n60.92\n24.52\n28.41\nSoft Prompt\n62.16\n54.79\n60.52\n42.90\n54.74\n44.46\n51.17\n57.28\n27.96\n47.28\nSoft Prompt\nAdapter\n39.84\n67.12\n55.29\n40.43\n61.59\n63.28\n71.04\n56.53\n49.70\n45.00\nSoft Prompt\n43.30\n62.84\n61.20\n44.44\n52.13\n57.21\n70.80\n54.24\n48.67\n34.37\nen\nNone\nAdapter\n35.49\n41.53\n49.44\n24.71\n44.91\n55.13\n44.15\n36.52\n32.02\n25.96\nSoft Prompt\n29.04\n42.31\n46.31\n27.92\n53.48\n53.06\n45.40\n48.41\n33.41\n39.54\nAdapter\nAdapter\n21.42\n33.99\n39.86\n17.16\n42.90\n23.39\n23.18\n32.18\n13.91\n21.95\nSoft Prompt\n50.16\n40.62\n56.07\n37.99\n63.94\n56.46\n35.97\n45.38\n19.98\n61.68\nSoft Prompt\nAdapter\n44.12\n40.65\n47.99\n24.97\n52.36\n51.04\n51.62\n48.55\n31.23\n23.32\nSoft Prompt\n22.90\n44.33\n41.92\n25.22\n49.67\n55.77\n48.64\n48.77\n33.85\n9.87\nes\nNone\nAdapter\n35.41\n68.66\n49.50\n35.38\n66.54\n66.07\n70.46\n64.65\n46.24\n46.35\nSoft Prompt\n24.70\n63.95\n48.38\n31.45\n62.20\n60.00\n63.25\n62.21\n47.81\n45.11\nAdapter\nAdapter\n63.38\n58.05\n67.33\n42.61\n59.17\n44.54\n66.00\n56.36\n17.84\n30.86\nSoft Prompt\n69.95\n52.71\n64.27\n42.75\n59.96\n52.05\n60.82\n58.11\n31.27\n48.71\nSoft Prompt\nAdapter\n54.37\n67.45\n64.81\n40.37\n64.62\n70.83\n68.78\n66.53\n50.91\n53.78\nSoft Prompt\n44.72\n65.55\n39.99\n39.17\n63.38\n63.52\n66.04\n61.86\n40.59\n47.45\nru\nNone\nAdapter\n72.90\n46.64\n34.14\n25.54\n49.66\n45.84\n48.16\n50.02\n49.73\n37.42\nSoft Prompt\n69.54\n53.60\n25.83\n34.23\n51.11\n51.04\n51.10\n48.37\n49.36\n40.56\nAdapter\nAdapter\n71.70\n54.14\n72.40\n38.30\n59.91\n30.15\n50.78\n48.08\n18.41\n25.92\nSoft Prompt\n73.07\n53.33\n65.15\n37.04\n53.23\n58.78\n52.96\n50.20\n21.37\n27.01\nSoft Prompt\nAdapter\n77.27\n54.48\n54.56\n42.01\n60.01\n53.82\n56.33\n56.61\n55.41\n46.92\nSoft Prompt\n71.73\n59.97\n65.74\n44.44\n58.04\n60.23\n56.40\n54.53\n47.95\n46.62\nzh\nNone\nAdapter\n14.91\n43.07\n8.42\n1.21\n39.80\n41.54\n42.05\n40.50\n11.46\n11.33\nSoft Prompt\n12.35\n51.86\n17.37\n7.50\n45.41\n53.70\n49.48\n46.97\n28.33\n17.65\nAdapter\nAdapter\n29.40\n41.60\n39.63\n7.39\n37.02\n19.26\n28.99\n21.72\n5.56\n6.54\nSoft Prompt\n37.86\n46.02\n19.56\n43.69\n30.45\n34.32\n18.60\n55.69\n38.07\n33.44\nSoft Prompt\nAdapter\n28.41\n46.21\n24.62\n8.49\n50.18\n44.69\n47.55\n51.13\n28.76\n15.06\nSoft Prompt\n23.34\n55.43\n32.02\n12.41\n43.96\n53.55\n55.26\n48.51\n32.11\n18.78\nTable 5: Results for the named-entity recognition task using F1-Score. The best scores are boldfaced, and the\nsecond best are underlined.\nTask\nLanguage\nLanguage\nRepresentation\nTask\nRepresentation\nbg\ncs\nel\nml\nro\nsl\nsk\nsw\nte\nur\nNone\nNone\n43.35\n35.50\n40.88\n40.62\n4.98\n68.74\n36.42\n38.90\n39.58\n37.64\nAdapter\n74.77\n35.50\n74.05\n68.98\n66.06\n28.76\n37.08\n64.55\n66.67\n66.01\nNone\nSoft Prompt\n69.42\n35.92\n69.94\n65.47\n77.59\n25.45\n35.83\n59.94\n64.23\n60.68\nAdapter\n72.16\n35.67\n74.17\n65.59\n77.15\n2.91\n37.17\n62.18\n55.93\n47.17\nAdapter\nSoft Prompt\n56.39\n38.08\n62.69\n57.98\n42.24\n67.54\n37.17\n58.16\n46.19\n57.19\nAdapter\n74.21\n34.42\n72.46\n68.70\n63.62\n32.77\n36.17\n62.61\n66.05\n64.05\nar\nSoft Prompt\nSoft Prompt\n65.45\n37.50\n67.84\n62.53\n59.03\n30.96\n37.25\n61.02\n61.44\n59.60\nAdapter\n75.03\n35.17\n74.51\n69.92\n64.61\n30.16\n35.92\n65.83\n68.90\n65.73\nNone\nSoft Prompt\n69.84\n35.50\n70.44\n65.67\n79.10\n31.36\n37.00\n62.75\n64.45\n62.79\nAdapter\n74.41\n34.92\n72.34\n68.74\n49.85\n16.13\n35.50\n62.85\n66.81\n60.18\nAdapter\nSoft Prompt\n62.51\n37.17\n62.81\n55.29\n35.16\n46.19\n36.67\n56.67\n46.45\n53.13\nAdapter\n75.23\n33.75\n71.50\n70.06\n60.25\n35.17\n34.50\n65.81\n67.13\n64.25\nde\nSoft Prompt\nSoft Prompt\n69.12\n36.67\n69.06\n64.61\n70.75\n26.35\n37.67\n61.18\n64.37\n60.00\nAdapter\n75.03\n35.00\n74.33\n70.30\n69.48\n28.66\n34.58\n67.07\n69.38\n64.63\nNone\nSoft Prompt\n70.42\n33.83\n70.32\n64.53\n77.00\n29.96\n34.58\n63.31\n63.43\n60.22\nAdapter\n64.07\n35.33\n64.79\n56.51\n90.09\n1.00\n35.08\n61.30\n44.11\n39.78\nAdapter\nSoft Prompt\n55.19\n38.00\n51.30\n46.53\n81.35\n33.77\n38.75\n56.97\n33.33\n52.81\nAdapter\n75.11\n33.83\n72.50\n69.50\n54.05\n35.07\n35.58\n64.67\n67.88\n64.07\nen\nSoft Prompt\nSoft Prompt\n65.37\n35.58\n69.50\n64.51\n35.64\n34.07\n37.08\n60.94\n63.35\n59.62\nAdapter\n74.87\n35.08\n75.29\n70.40\n72.12\n28.06\n35.17\n66.17\n70.00\n66.09\nNone\nSoft Prompt\n68.98\n34.83\n70.12\n65.55\n79.83\n31.76\n35.75\n60.66\n64.11\n61.36\nAdapter\n73.95\n34.92\n74.61\n68.26\n71.78\n10.02\n35.83\n64.77\n59.40\n50.42\nAdapter\nSoft Prompt\n54.97\n38.75\n44.47\n41.48\n81.98\n8.42\n36.42\n55.11\n33.49\n46.13\nAdapter\n74.27\n33.92\n72.38\n69.68\n67.58\n32.67\n36.25\n64.35\n67.33\n62.95\nes\nSoft Prompt\nSoft Prompt\n66.15\n35.92\n67.54\n62.77\n78.52\n28.76\n36.33\n59.66\n63.19\n57.66\nAdapter\n75.55\n34.92\n74.81\n70.56\n75.24\n29.96\n34.67\n67.54\n69.54\n66.29\nNone\nSoft Prompt\n69.90\n36.25\n69.78\n64.85\n82.62\n28.16\n35.67\n61.60\n63.73\n62.36\nAdapter\n72.63\n35.75\n74.19\n61.14\n83.50\n4.91\n36.25\n65.77\n51.16\n46.19\nAdapter\nSoft Prompt\n63.45\n37.58\n53.43\n44.51\n87.01\n11.32\n38.08\n54.67\n37.60\n53.13\nAdapter\n76.03\n34.92\n73.75\n68.18\n75.68\n28.06\n35.83\n65.77\n68.24\n63.99\nru\nSoft Prompt\nSoft Prompt\n68.42\n37.25\n67.72\n63.53\n75.05\n23.65\n37.25\n57.72\n62.55\n60.08\nAdapter\n74.95\n35.75\n73.79\n69.16\n74.32\n28.66\n36.33\n68.00\n69.38\n66.11\nNone\nSoft Prompt\n70.80\n35.75\n70.44\n66.59\n78.52\n21.84\n35.17\n62.36\n65.75\n62.85\nAdapter\n61.72\n34.08\n64.27\n50.90\n89.01\n0.20\n35.17\n60.66\n35.71\n38.98\nAdapter\nSoft Prompt\n43.47\n33.33\n56.09\n56.29\n45.90\n98.20\n35.00\n47.07\n34.57\n47.76\nAdapter\n73.43\n35.42\n72.16\n68.06\n65.77\n31.06\n35.50\n64.37\n66.91\n63.67\nzh\nSoft Prompt\nSoft Prompt\n68.68\n34.67\n68.86\n64.69\n76.95\n23.55\n36.50\n60.42\n63.39\n61.78\nTable 6: For NLI, we report accuracy as a metric. The best results for each language pair are highlighted in bold and\nthe second best are underlined. Additionally, language pairs with improved performance compared to inference-only\nare marked in green, and those with decreased performance are marked in red.\nTask\nLanguage\nLanguage\nRepresentation\nTask\nRepresentation\nbg\ncs\nel\nml\nro\nsl\nsk\nsw\nte\nur\nNone\nNone\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\nar\nNone\nAdapter\n84.78\n70.74\n86.77\n86.45\n86.06\n82.74\n72.52\n86.59\n84.32\n81.50\nSoft Prompt\n86.48\n63.08\n84.90\n86.32\n87.61\n80.77\n56.91\n81.47\n84.47\n81.92\nAdapter\nAdapter\n58.38\n45.01\n74.35\n83.39\n73.17\n71.29\n36.69\n82.63\n77.91\n80.18\nSoft Prompt\n89.22\n56.49\n85.76\n86.82\n78.56\n82.24\n48.71\n85.96\n82.44\n81.62\nSoft Prompt\nAdapter\n85.83\n67.14\n87.29\n82.13\n88.41\n85.08\n68.66\n84.36\n82.47\n78.36\nSoft Prompt\n85.89\n61.41\n81.91\n85.13\n86.84\n84.60\n57.40\n84.20\n81.95\n81.71\nde\nNone\nAdapter\n97.63\n69.72\n97.74\n94.66\n98.11\n97.11\n73.70\n97.25\n93.73\n93.17\nSoft Prompt\n94.49\n65.32\n96.42\n96.28\n92.33\n88.16\n62.31\n90.69\n91.54\n89.10\nAdapter\nAdapter\n97.03\n78.84\n95.30\n85.30\n97.19\n95.28\n82.91\n90.39\n79.96\n85.29\nSoft Prompt\n93.15\n58.68\n94.46\n95.17\n95.57\n96.11\n67.99\n87.70\n93.06\n65.98\nSoft Prompt\nAdapter\n97.06\n74.94\n96.84\n92.17\n97.76\n96.85\n82.96\n95.40\n89.64\n86.71\nSoft Prompt\n94.51\n63.87\n91.18\n94.29\n95.44\n94.49\n57.84\n86.67\n93.02\n76.01\nen\nNone\nAdapter\n95.79\n85.10\n97.46\n89.15\n94.52\n89.31\n83.09\n91.98\n87.46\n88.65\nSoft Prompt\n94.98\n55.29\n95.09\n93.12\n91.50\n91.47\n53.03\n91.17\n93.50\n88.35\nAdapter\nAdapter\n97.22\n82.95\n98.04\n92.28\n96.98\n95.80\n85.98\n91.20\n85.99\n89.57\nSoft Prompt\n88.69\n40.60\n94.31\n86.24\n94.97\n93.09\n46.60\n88.02\n81.91\n86.00\nSoft Prompt\nAdapter\n87.61\n83.47\n95.89\n77.89\n88.38\n81.98\n85.49\n87.42\n77.34\n81.67\nSoft Prompt\n80.71\n75.72\n92.71\n82.49\n90.44\n84.21\n70.08\n82.65\n79.55\n75.16\nes\nNone\nAdapter\n95.66\n76.98\n95.85\n91.93\n97.06\n95.81\n74.40\n95.62\n93.15\n90.57\nSoft Prompt\n92.80\n76.38\n94.59\n91.93\n90.62\n82.23\n74.78\n92.40\n91.90\n83.25\nAdapter\nAdapter\n84.35\n73.15\n95.94\n92.67\n96.16\n94.07\n81.65\n93.48\n93.80\n88.13\nSoft Prompt\n92.98\n77.88\n96.15\n93.27\n96.03\n93.07\n81.02\n92.09\n83.19\n71.66\nSoft Prompt\nAdapter\n97.47\n87.14\n97.41\n94.78\n96.89\n96.01\n86.36\n95.55\n91.72\n86.85\nSoft Prompt\n96.35\n79.97\n95.79\n90.56\n95.08\n92.59\n78.85\n92.85\n89.11\n87.56\nru\nNone\nAdapter\n98.87\n72.15\n98.46\n90.34\n98.26\n96.35\n74.45\n97.10\n89.87\n92.93\nSoft Prompt\n98.33\n58.26\n96.09\n95.24\n87.02\n84.39\n59.06\n92.15\n95.03\n92.11\nAdapter\nAdapter\n97.90\n88.93\n94.70\n82.33\n96.89\n92.79\n88.80\n91.75\n77.95\n87.34\nSoft Prompt\n97.36\n13.87\n96.77\n95.50\n95.03\n95.70\n20.58\n95.59\n91.23\n92.34\nSoft Prompt\nAdapter\n98.59\n67.29\n97.46\n88.39\n98.53\n97.73\n74.44\n95.83\n88.39\n91.36\nSoft Prompt\n98.89\n48.65\n96.71\n93.57\n97.32\n96.85\n38.89\n95.17\n92.26\n89.83\nzh\nNone\nAdapter\n93.70\n74.08\n95.07\n90.97\n93.44\n85.53\n74.05\n90.99\n88.47\n90.22\nSoft Prompt\n91.44\n66.83\n92.58\n94.18\n84.92\n77.69\n66.55\n74.85\n90.33\n81.90\nAdapter\nAdapter\n97.08\n84.02\n97.09\n90.20\n97.27\n95.58\n84.14\n90.62\n90.89\n90.58\nSoft Prompt\n66.88\n69.81\n65.43\n67.65\n67.11\n66.28\n67.57\n65.58\n65.92\n65.92\nSoft Prompt\nAdapter\n96.07\n76.96\n93.18\n89.41\n95.15\n94.37\n80.19\n91.13\n91.22\n82.55\nSoft Prompt\n95.30\n73.52\n89.10\n94.01\n92.61\n90.44\n62.60\n83.33\n91.33\n85.38\nTable 7: Results for the check-worthy claim detection task for cross-lingual transfer. Results are reported using\nF1-Score, with the best scores in bold and the second best underlined.\nLanguage\nRepresentation\nTask\nRepresentation\nbg\ncs\nel\nml\nro\nsl\nsk\nsw\nte\nur\nNone\nAdapter\n38.46 ± 9.60\n66.00 ± 0.77\n55.88 ± 5.33\n40.99 ± 0.67\n60.97 ± 1.74\n56.29 ± 0.62\n67.55 ± 1.22\n58.60 ± 1.97\n47.69 ± 2.35\n29.15 ± 8.78\nSoft Prompt\n28.98 ± 3.24\n61.48 ± 0.69\n50.64 ± 6.63\n43.08 ± 1.81\n52.65 ± 1.58\n54.03 ± 0.98\n62.42 ± 0.75\n56.49 ± 2.86\n43.94 ± 1.64\n26.67 ± 3.05\nAdapter\nAdapter\n66.85 ± 2.03\n64.78 ± 0.15\n69.13 ± 1.48\n47.34 ± 1.86\n64.07 ± 4.97\n49.29 ± 2.42\n68.15 ± 1.19\n59.45 ± 2.28\n25.96 ± 2.86\n20.20 ± 11.17\nSoft Prompt\n63.93 ± 3.01\n50.76 ± 14.17\n62.05 ± 3.25\n43.61 ± 2.42\n56.11 ± 3.25\n50.30 ± 7.88\n53.63 ± 4.44\n55.27 ± 3.95\n26.83 ± 4.04\n46.63 ± 1.51\nSoft Prompt\nAdapter\n42.19 ± 2.93\n67.25 ± 0.23\n60.19 ± 6.02\n43.67 ± 4.32\n61.89 ± 1.73\n61.75 ± 1.90\n72.95 ± 2.35\n56.29 ± 0.65\n52.58 ± 5.43\n47.86 ± 3.77\nSoft Prompt\n50.66 ± 12.65\n64.00 ± 2.18\n63.38 ± 3.85\n46.37 ± 2.46\n55.90 ± 4.95\n57.77 ± 0.71\n69.87 ± 2.21\n54.26 ± 1.31\n48.35 ± 2.19\n37.46 ± 3.93\nTable 8: Results of cross-lingual transfer from German to six languages for the NER task. We report the mean of\nthree runs along with the standard deviation. The best results are bolded and the second best results are underlined.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2024-07-02",
  "updated": "2024-10-30"
}