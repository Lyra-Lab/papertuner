{
  "id": "http://arxiv.org/abs/2407.02425v1",
  "title": "Reinforcement Learning and Machine ethics:a systematic review",
  "authors": [
    "Ajay Vishwanath",
    "Louise A. Dennis",
    "Marija Slavkovik"
  ],
  "abstract": "Machine ethics is the field that studies how ethical behaviour can be\naccomplished by autonomous systems. While there exist some systematic reviews\naiming to consolidate the state of the art in machine ethics prior to 2020,\nthese tend to not include work that uses reinforcement learning agents as\nentities whose ethical behaviour is to be achieved. The reason for this is that\nonly in the last years we have witnessed an increase in machine ethics studies\nwithin reinforcement learning. We present here a systematic review of\nreinforcement learning for machine ethics and machine ethics within\nreinforcement learning. Additionally, we highlight trends in terms of ethics\nspecifications, components and frameworks of reinforcement learning, and\nenvironments used to result in ethical behaviour. Our systematic review aims to\nconsolidate the work in machine ethics and reinforcement learning thus\ncompleting the gap in the state of the art machine ethics landscape",
  "text": "Reinforcement Learning and Machine ethics:\na systematic review\nAjay Vishwanath a,*, Louise A. Dennisb and Marija Slavkovik c\naUniversity of Agder\nbUniversity of Manchester\ncUniversity of Bergen\nAbstract\nMachine ethics is the field that studies how ethical behaviour can be accomplished by autonomous systems. While there exist some\nsystematic reviews aiming to consolidate the state of the art in machine ethics prior to 2020, these tend to not include work that uses\nreinforcement learning agents as entities whose ethical behaviour is to be achieved. The reason for this is that only in the last years we\nhave witnessed an increase in machine ethics studies within reinforcement learning. We present here a systematic review of reinforcement\nlearning for machine ethics and machine ethics within reinforcement learning. Additionally, we highlight trends in terms of ethics spec-\nifications, components and frameworks of reinforcement learning, and environments used to result in ethical behaviour. Our systematic\nreview aims to consolidate the work in machine ethics and reinforcement learning thus completing the gap in the state of the art machine\nethics landscape.\n1\nIntroduction\nMachine ethics “is concerned with the behaviour of machines towards human users and other machines” [8]. This includes studying how to\nimplement machine reasoning in a way that includes the consideration of ethical factors [9, 75]. Since a 2006 AAAI Workshop on Machine\nEthics [8], there has been a rapid expansion in proposed systems and formalisms for performing machine ethics (e.g, [73, 28, 19, 17, 40, 30])\nas evident from surveys of the field [44, 71, 77].\nReviews of machine ethics, at present, cover the period up to 2020. Currently, we observe a notable trend in reinforcement learning,\nspecifically focusing on the ethical behavior of reinforcement learning agents.\nIn reinforcement learning (RL) [68] an artificial agent makes decisions in an environment and receives rewards and punishments as outcomes\nof those decisions. The goal, typically of the learning process, is to find a policy on how to make the decisions so that an optimal reward is\nattained. Reinforcement learning, and specifically deep reinforcement learning that combines neural networks and reinforcement learning, has\nbeen of particular recent interest since its successful application in training artificial agents to play human level computer games [42]. As\nreinforcement learning is about training an artificial agent how to make decisions, it is very intuitive as to why ethical decision-making would\nreceive attention in the reinforcement learning community.\nEarly papers on the reinforcement learning and machine ethics topic, such as [11], gained recognition, along with subsequent work by\nresearchers [13, 34, 43, 56, 45, 70, 74]. There is a gap in the understanding of the state of the art in machine learning if that understanding does\nnot include the most recent work on reinforcement learning. Our goal is to bridge this gap by conducting a literature review on reinforcement\nlearning in the context of machine ethics.\nFollowing a keywords search in the usual repositories of artificial intelligence research articles we found 605 potential papers which we have\nnarrowed down to 62 papers that were on topic and which were carefully analysed. We analysed the papers based on the RL paradigm they\ndeploy, how they modify which RL “component\" to achieve ethical behaviour, and which ethical theory they directly or indirectly implement\nas well as what kind of implementation examples the papers use. Beyond this we observe and discuss the trends in this body of work and draw\nrecommendation for future research in this field.\nThis article contributes to both the disciplines of reinforcement learning and machine ethics by establishing a clear state of the art of the\nintersection of these two fields and giving future work a good anchor to position and grow from existing experience. Furthermore, we highlight\nchallenges and possible pitfalls of reinforcement learning and machine ethics, thus ensuring a responsible and sustainable development of what\nat present appears to be a trendy and growing research field.\nBackground and related work\nIn this section, we discuss reinforcement learning and the various algorithms employed in the papers we review. This is followed by a discussion\non previous works in machine ethics and relevant gaps.\n∗Corresponding Author. Email: ajay.vishwanath@uia.no\narXiv:2407.02425v1  [cs.AI]  2 Jul 2024\nReinforcement Learning\nReinforcement learning (RL) is a machine learning paradigm in which an agent navigates an environment with states s ∈S, actions a ∈A,\nnext states s′ ∈S′ and rewards r ∈R. Such an agent learns through trial and error an optimum behavior which maximizes a numerical reward\nfunction [68]. This optimum behavior is denoted by π∗, or the optimum policy. Markov decision processes (MDP) are a mathematically\nidealized form of the RL problem. In an MDP, the probability of each possible value of the next state S′ and reward R depends on the\nimmediately preceding state and action, S and A, completely characterize the environment and that these states, actions and rewards are finite.\nA method to solve MDPs is by using value function approximation, where a numerical value is calculated for being in a state V (s) and\ncommitting an action in a given state Q(s, a). As the agent visits different states in the environment, a value table is updated based on the\nreward obtained. This method is often referred to as Q-learning, where a Q table is updated and based on these Q-values an optimal policy is\ndetermined. However, an issue with such an approach is that in an environment with a very high state space, such as the game of chess, it can\nbe very expensive to compute a Q-table.\nIn high state-space scenario, a neural network is often used to encode the state space and output the Q-value for being in that state. This\nparadigm of using a neural network in the Q-learning algorithm is referred to as Deep Q-learning. There are, however, limitations with using\nvalue function approximation, since there is an exploration-exploitation conundrum. These networks are said to be more exploratory in nature\nand often fail to converge to an optimum. To overcome this issue, actor-critic methods are used where two neural networks, the actor and the\ncritic are used to predict action probabilities and Q-values, respectively, instead of only calculating Q-values. Methods such as PPO, DDPG\netc are popular actor-critic methods.\nThere are other types of RL, such inverse RL, multi-objective RL and co-operative RL. In inverse RL (IRL) [1], the agent learns the\nreward functions in an environment, given the policy. This is useful in applications such as robotics, where the robot can \"watch\" a human\ndemonstrate a task (optimal policy) and then the robot can imitate the same, while learning a reward function in its environment. Next, multi-\nobjective RL (MORL), an agent is trained to optimize conflicting objective functions, demonstrated in works by Rodriguez et al. [59] and\nservral others discussed in this review. Finally, co-operative inverse reinforcement learning, is a variant of IRL in a multi-agent scenario.\nHowever, unlike classical IRL, where the human is the basis for learning, the other agents in the system often perform active teaching, learning\nand communicative actions [33].\nSystematic reviews in machine ethics\nWe here briefly summarise the findings of the three reviews in machine ethics, their findings and methodology.\nTolmeijer et al. [71] focused on the implementations of machine ethics specifically and found 49 papers from Web of Science, Scopus, ACM\nDigital Library, Wiley Online Library, ScienceDirect, AAAI Publications, Springer Link, and IEEE Xplore. These papers were then analysed\nbased on types of ethical theory they implemented, the nontechnical aspects when implementing those theories, and technological details.\nNallur [44] considers the landscape of machine ethics implementations focusing on the different problems, techniques, methodologies and\nevaluation methodologies that are used. He did not do a systematic review, but rather a more qualitative analysis of selected prominent works.\nYu et al [77] survey advances in techniques for incorporating ethics into AI. They consider publications in “leading AI research conferences\nincluding AAAI, AAMAS, ECAI and IJCAI, as well as articles from well-known journals”. They organise the papers into four categories:\nExploring Ethical Dilemmas, Individual Ethical Decision Frameworks, Collective Ethical Decision Frameworks; and 4. Ethics in Human-AI\nInteractions. There is of course significant overlap among the papers included in all three surveys.\nWe here follow to a large degree the approach of Tolmeijer et al. [71] in terms of where we search for papers. This is because Tolmeijer et\nal. [71] is the only systematic review in machine ethics. Understanding the state of the art in machine ethics is not without challenges. The\npapers published in this area are sent to a wide spectrum of venues. Machine ethics is also not a very large area of research and it is not very\nprecisely positioned in the context of the AI alignment and AI Ethics initiatives. Therefore, it has to be accepted that any systematic review of\nthe field is likely to not be exhaustive, even if we limit ourselves to a specific domain or methodology such as reinforcement learning.\n2\nSurvey methodology\nIn this section, we describe the strategy used to find papers related to machine ethics and reinforcement learning, following which we discuss\nthe process used to review the found articles.\nSearch strategy\nOur review on implementations in machine ethics focuses on RL, hence we mandated this terminology while querying the databases. Since\nRL is to be used towards machine ethics implementations, we used alternative search terms such as artificial morality [5], machine morality\nand robot ethics [41], computational ethics [10], roboethics [26], artificial moral agents [22], and value alignment [59]. These terms have been\nused interchangeably to communicate the embedding of ethical theories into machines/computers/artificial intelligence agents. We show the\nquery used below:\n\"reinforcement learning\" AND ( \"machine ethics\" OR \"artificial morality\" OR \"machine morality\" OR \"computational ethics\" OR\n\"roboethics\" OR \"robot ethics\" OR \"artificial moral agents\" OR \"value alignment\")\nThe query was used to search the following databases 1. In the right hand side column we give the number of ‘hits’ from the query. Further\nfiltering was performed on these ‘’hits.\nDatabase\nNumber of results\nACM Digital Library\n158\nIEEE Xplore\n51\nScienceDirect\n21\nScopus\n41\nSpringerLink\n146\nWeb of Science\n16\nWilley Online Library\n172\nTotal\n605\nTable 1.\nDatabases and respective number of results for the search query used.\nSelection process\nA total of 605 papers were further filtered based on relevance to RL and machine ethics (ME). We included only journal articles, conference\npapers, and book chapters which were published before 4th January 2024. The diagram on Figure 1 shows the elimination process we used to\narrive at the final list of manuscripts.\nSearch result\nNo\nYes\nMain focus\non RL?\nRemove from review\nYes\nNo\nModifies RL\nto perform\nethical decision\nmaking?\nRemove from review\nNo\nYes\nDuplicate?\nRemove from review\nInclude in review\nFigure 1.\nSelection process used to eliminate irrelevant articles but keep the relevant ones. First is to check whether the article focuses on RL, followed by\nchecking if the RL algorithm is used for moral decision making, and finally to eliminate duplicates that appeared across multiple databases.\nAfter applying this selection criteria, we ended up with 62 papers for review. Subsequently, we conducted a more focused review by filtering\nthe papers based on their relevance to both machine ethics and RL, resulting in a total of 48 papers. In the next section, we delve deeper into\nthe taxonomy creation and review process.\nThe analysis\nIn this section, we discuss the taxonomy and categorizations used to organize our survey. Since the focus is on RL, we closely examined\nits components such as the reward, policy, and value function. Additionally, we considered the specific RL paradigms used, allowing us to\nobserve trends within the research community. Furthermore, we explore ethical theories and implementation examples, which constitute the\ncore of machine ethics implementations. Finally, we categorize trends in machine ethics for RL based on contribution type, whether they are\nargumentative, empirical, mathematical, or a combination.\nComponent of RL Modified\nThere are several aspects that affect RL algorithms and their performance on tasks. For instance, the challenge of designing a good reward\nfunction to correctly incentivize agents can make or break the agent’s performance. Other aspects such as policy functions, value functions,\netc., also play a part. In terms of developing RL algorithms which make more ethical choices, several researchers modify either one or more\nof these components to achieve the desired result. We categorized the research articles based on which component of RL was modified and we\npresent our findings in Table 2.\nRL component\nReward function\n[2, 3, 48, 67, 76, 4, 72, 55, 62, 21, 24, 46, 69, 61, 31, 59, 32, 23, 20,\n66, 60, 56]\nPolicy function\n[54, 64, 46, 13, 61, 36, 56, 29]\nObjective function\n[74, 37]\nValue function\n[2, 47, 63]\nOthers\n[15, 35, 51, 6]\nTable 2.\nComponent of RL modified.\nWe can observe from these groupings that a majority of the articles modify either the reward function or the policy function. Reward\nfunctions act as means to instill ethical behavior, since the agent can obtain a higher reward or incur a lower penalty if it chooses the ethical\noption. On the other hand, inverse reinforcement learning algorithms are typically used to modify the policy function through demonstrations of\nan expert policy, based on which the agent learns the optimal reward function. Also, a few researchers demonstrated ethical behavior preference\nin RL agents by modifying either the value function or the neural network’s objective function (see Table 2). Other components include the\naction space [6], neural network [51] in case of deep RL algorithms, path planning [35] and regret [15].\nRL paradigm\nSimilar to observing the component of the RL algorithm modified, it was also interesting to categorize and analyse the RL paradigm adopted\nto bring about ethical behavior. By paradigm, we refer to the type of RL used based on which the relevant component is modified. We briefly\nintroduced some of the RL paradigms in Section 1, such as inverse RL, multi-objective RL, constrained RL, etc. Each of these paradigms has\nvariants and sub-variants, but these variations within paradigms largely function in a similar fashion. In Table 3, we specify the paradigms and\nthe corresponding research works.\nReinforcement learning framework\nMulti-objective RL\n[55, 61, 69, 58, 57, 72, 59, 24, 32, 49, 56, 46, 4]\nInverse RL and variants\n[54, 37, 29, 36, 4, 64, 48]\nPOMDP\n[65, 6]\nConstrained/safe RL\n[21, 13, 18]\nRegular RL\n[66, 3, 52, 50, 27, 12, 45, 25, 39, 76, 2, 37, 63, 15]\nOther RL algorithms\n[60, 51, 74, 67, 23]\nTable 3.\nRL frameworks used to result in ethical behavior.\nMost of the researchers adopted multi-objective RL (MORL) and inverse RL (IRL). The advantage of using MORL is that ethics specifica-\ntions can be codified as part of the reward function, in addition to the main objective. An MORL agent navigates its environment, maximizing\nits reward thereby fulfilling its main tasks along with ethical sub-tasks, or in many cases, completes its main task in an ethical manner. While in\nIRL, the agent learns an optimal reward function based on an ethical policy demonstrated by an expert. Variations of IRL include, co-operative\nIRL, and adversarial IRL, which use human experts as a co-operative and adversarial means, respectively, rather than an expert teacher, to\nembed ethical behaviour in an artificial agent.\nOther techniques such as constrained RL and safe RL, teach an agent to ethically navigate an environment by avoiding unethical states or\npolicies. Apart from these RL paradigms, several researchers also adopted regular RL, which refers to RL algorithms such as multi-arm bandits,\ndeep Q-learning, actor-critic models, etc., which most applications of RL use. Finally, other RL algorithms include dynamic self-organizing\nmaps [23], multi-agent RL [67], social RL [60], implicit RL [51] and affinity-based RL [74]. We also observe several uses of multi-agent RL,\nacross these frameworks, however, for simplicity’s sake, we did not single out this categorization.\nEthical Theory\nA crucial aspect to explore in this systematic literature review is the ethical theories used by the researchers in their respective works. We\nreviewed each article to try to understand its underlying ethical proclivities. Based on this examination, we found that a majority does not\nexplicitly state their ethics specifications and due to this, it was challenging to distinguish these manuscripts based on moral philosophy.\nHowever, to help researchers better understand the ethical underpinnings behind these machine ethics papers, we focus on the ethical theory\nthat influences the RL algorithm the most, rather than all the theories at play. For example, if researchers codified social norms and then\noptimized a utility function as is done in RL, then we consider such implementations as deontological. On the other hand, if a paper does\nnot explicitly mention deontological codification based on rules or norms, then it is considered consequentialist (which also includes variants\nof utilitarianism). We also found several other researchers using human values, where they optimized metrics based on human evaluation. In\nTable 4, we summarize the ethics distinctions.\nEthical theory\nConsequentialist\n[55, 61, 67, 47, 57, 32, 63, 72, 32, 49, 25, 39]\nDeontology\n[23, 31, 58, 45, 59, 37, 21, 6, 56, 38, 12, 18]\nVirtue ethics\n[65, 74, 27, 53]\nHuman expert values\n[76, 23, 54, 69, 46, 24, 29, 36, 4, 64, 49, 13, 62]\nOthers (unspecified)\n[2, 3, 60, 51, 50]\nTable 4.\nEthics specification used by researchers.\nFrom this it can be observed that a majority adopted a consequentialist, deontological and human expert approach. Deontological and\nconsequentialist ethics-based approaches being dominant is not surprising [65, 71], since they are arguably easier to codify and implement\n(compared to, for example, value ethics). We also see that there are several works under \"human expert values\", and that these \"values\" have\nbeen codified to help RL agents make ethical decisions in their environment. In an upcoming section, we deep-dive into who decides these\nvalues. Next, virtue ethics has fewer implementations, since the codification is challenging, however, it has been argued as a promising direction\nto pursue [74, 66]. Finally, the remaining manuscripts did not explicitly specify and instead had other goals such as social compliance [60],\nemulated imagination [50], etc.\nImplementation Example\nA wide variety of settings were used for experimental evaluation of systems. Very few of these settings were shared across teams of researchers\nrevealing a lack of comparative benchmarking sets. As a result, ethical frameworks were generally compared against baselines in which there\nwas no attempt to represent ethics, rather than alternative frameworks or theories. A broad categorisation of these examples can be seen in\nTable 5.\nThe only family of examples that appeared to have real traction in the literature were examples involving the equitable sharing of resources\nin a multi-agent setting – in which the metrics used were the ability of the community to support all its members. These scenarios ranged\nfrom abstract Grid Worlds representing a foraging games with various opportunities to share and/or steal resources from other agents, to more\nrealistic settings based on, for instance, water distribution among several communities [35].\nAnother common family of examples were abstract representations of forbidden behaviour – often as a Grid World with forbidden areas\n[4] has a variant on this that involved the agent learning to play Tetris taking into account human preferences about colour placement. In\nmany cases it was difficult to see these examples as specifically about ethical behaviour, as opposed to more general examples around safe\nreinforcement learning, learning preferences, or constrained learning.\nBeyond these two distinct families of examples, there was a varied range of examples which involved motion planning in an environment\nthat contained conflicting values or values conflicting with goals. For instance two papers [45, 46] required an agent to learn to play vegan\npac-man where it was deemed unethical to eat the ghosts. Another set of papers all by the same team [55, 58, 57, 59] involved a “public civility\ngame” in which agents need to reach a goal which is obstructed by rubbish and they must learn to divert to place the rubbish in a bin, rather\nthan throwing it at the other agents involved.\nTwo examples involved moderately sophisticated simulations of smart grids [23] and financial markets [21].\nThe remaining cases were ad-hoc examples some with clear ethical relevance – e.g., around patient consent to treatment [38]. However, in\nsome cases, particularly in systems where the focus was primarily on learning human norms, rather than a more specialised focus on values,\nwe saw examples that seemed more about learning preferences than anything with particular ethical force – for instance [29] focuses on robot\nchef learning to cook meals that its owner will like.\nImplementation example\nEquity Example\n[66, 65, 67, 32, 56, 35, 27]\nAbstract Example with Forbidden/Preferred States\n[37, 32, 51, 62, 12, 4]\nMotion Planning with conflicting goals and/or values\n[76, 54, 45, 46, 55, 58, 57, 59, 49, 48]\nSophisticated Simulations\n[23, 21]\nOther\n[31, 61, 2, 47, 15, 14, 38, 29, 25, 39]\nTable 5.\nCategorisation of examples.\nContribution type\nWe found several differences in claimed types of contributions in our literature review. Some were position papers, which were argumentative\nin nature, theorizing on how RL could be used in machine ethics research. Others were purely empirical contributions, either by referring to a\npreviously proposed theory on a new problem, or improving upon existing results. The remaining ones were a combination of argumentative,\nempirical and mathematical proofs. It was interesting to explore these contribution types mainly to understand the maturity of the field: RL in\nmachine ethics. The contributions are summarized in Table 6.\nContribution type\nArgumentative/Theoretical\n[54, 6, 69, 52, 32, 36, 50, 27, 64, 74, 62, 18, 23, 66, 65, 67, 15, 24,\n56, 51, 29, 48, 60, 72, 59]\nEmpirical\n[76, 61, 2, 47, 57, 37, 21, 49, 38, 35, 25, 39, 46, 4, 23, 66, 65, 67,\n15, 24, 56, 51, 29, 48, 45, 63, 13, 59]\nMathematical Proof\n[60, 72, 45, 63, 13, 59]\nTable 6.\nType of research.\nUsually, new fields begin with purely argumentative and position papers, discussing ways in which the research could move forward. Based\non this, new mathematical and empirical evaluations are performed, and newer positions are formed. We can observe from the Table 6 that\nempirical evaluations form a large part of the research while still having a sizable number of theoretical contributions. In a later section, we\ndiscuss the research field as a whole and the associated trends based on contribution type.\nDiscussion\nWe here discuss the trends we observed in the surveyed papers and draw out some recommendations for future research based on our observa-\ntions.\nRecent increase of contributions in RL\nBased on our survey, we found a steady increase in the use of RL in machine ethics. Figure 2 illustrates the trend, where we plot number of\ncontributions per year. It is clear from this bar chart that since 2020, the attention has seen a consistent rise, with the peak in 2023.\nNo moral theories\nWallach [75] categorises approaches to the problem of machine ethics as either “top-down” or “bottom up”. Broadly, top down approaches\nseek to operationalise some ethical theory from Philosophy and apply this to a decision faced by the machine. Bottom-up approaches seek to\nlearn ethical behaviour from data.\nFigure 2.\nRecent increase of contributions in RL based on the papers chosen for review.\nMuch of the work covered in the survey papers [44, 71, 77] and performed in the early years of the field has taken a top down approach with a\nfew systems taking a hybrid line – for instance the GenEth system [7] in which inputs from ethics experts were used to enable a system to learn\nexplicit rules for differentiating between choices in a medical ethics situation. In reinforcement learning, the trend is not to follow a specific\nmoral theory that will inform the agent’s policy. As we reflected in the previous section, there is no explicit intention to have reinforcement\nlearning agent trained into following a specific moral theory.\nParadigm trends\nA significant number of approaches we encountered have encoded the ethics in some fashion within the reward or objective function, within\nthese we observed three trends:\nEthical RL is Multi-Objective RL In many cases the ethics signal was implemented as a reward signal in addition to the other (non-ethical)\nrewards such as maximising profit or efficiency. This frames ethical RL as a form of multi-objective RL.\nEthics as Constraints In other cases the ethics were represented as constraints. Ethical behaviour did not confer some reward upon the agent,\nalthough in some cases unethical behaviour incurred a penalty. This frames ethical RL as a form of constrained RL. In some cases there was\nan additional requirement that behaviour remain ethical during training in which case ethical RL becomes a form of Safe RL.\nMulti-agent RL Another significant theme, highlighted by the number of implementation examples based around equitable distribution of\nresources, is framing ethical ethical behaviour in terms of maximising group benefit in a multi-agent context.\nWe would argue that it is a valuable contribution to the field to point out that much by way of ethical behaviour can be achieved by appropriate\napplication of techniques from Constrained Multi-Objective RL and Safe RL, but that going forward researchers should take this point as\nestablished and, if working in these areas, focus their attention on how ethics can be represented in terms of these multiple objectives and\nconstraints.\nHuman factors: whose ethics?\nBased on our analysis of ethics specifications and moral philosophy, we found that several researchers have included a human factor to\nimplement ethical behavior in RL agents. It is either the developer, the user, an expert or an adversary (Table 7), whose ethics an RL agent\nmust try and emulate. Clearly, there is no real agreement on which ethical theory we, as humanity fall back on. We use some version of\nutilitarianism, social contracts, virtue ethics, deontology and other ethical theories interchangeably.\nWhose ethics?\nDesigner/developer/researcher\n[76, 23, 54, 14]\nAlgorithm user\n[69, 24]\nCo-operative human\n[29, 64]\nHuman expert/demonstrator\n[36, 46, 48]\nHuman-machine team\n[39, 62, 4]\nOthers (unspecified)\n[31, 60, 53]\nTable 7.\nWho decides the ethics?\nThe problem of not clearly subscribing to a well understood moral theory is the human affinity for bias. Humans bias, is a hazard for design-\ning applications that affect thousands of people who have their own plethora of biases that may be counter to the algorithm developer’s biases.\nHence, it is crucial that algorithms consider a variety of viewpoints and evolve with changing times. While it is unrealistic to expect develop-\ners/product managers/researchers to know and understand so many viewpoints, it is still important that the makers strive towards at least an\nabstract understanding while developing ethical RL algorithms. Most of the algorithms in our survey have not explicitly specified who decides\nthe ethics or right from wrong. However, we urge the research community to explicitly identify and report their ethical biases/assumptions\nmade in their implementations, since future researchers who extend their work might be vulnerable to propagate the same.\nSome works (Table 7), where the human designer/developer/researcher decides ethical behavior, do indeed explicitly mention the human\nfactor, where the RL algorithms are trained based on whether certain results are allowed or not. For instance, some of this is demonstrated\nusing ethical movie recommendations, certain movies are excluded from the results for underage users [14]. While some use a judging RL\nagent trained with ethical values to train other RL agents in the environment [23]. An important challenge for such works is to codify human\nvalues and translate this to the algorithm. In cases where an expert human demonstrator is involved [36] in training the algorithm, it’s important\nthat they are aware of competing ethical perspectives rather than training the model on one specific moral behavior. In the case of using co-\noperative humans and a human teammate, these are useful ways to train AI, but it’s also vital to infer what exactly is learnt by the model in\nthe form of explanations and interpretations. Lastly, an interesting approach is to let the algorithm user specify the ethics [69]. This shifts the\nresponsibility from the developer to the user, who can then customize the algorithm to their ethical needs. However, this approach could also\naffect the usability of the algorithm because with so much customization, the interaction experience might become troublesome for the user.\nHence, striking the balance of usability and customization becomes important.\nSummary\nWe here present and analyze the results of a systematic review of articles that use reinforcement learning for machine ethics or consider\nthe problem of machine ethics in reinforcement learning. The increased volume of papers in machine ethics RL in recent years points to a\npromising area of research. The main challenges for the RL in machine ethics community going forward is to organise itself around a taxonomy\nthat will help authors position their efforts against existing work.\nThe secondary challenge is to avoid “anecdotal ethics”. RL in machine ethics most of the time uses bottom-up and sometimes hybrid\napproaches (in the sense of [75]) to achieve artificial moral agency, to avoid developing methods around the moral sensitivities and judgments\nof its developers. The secondary challenge is thus to establish a firmer connection with the fields of moral philosophy and moral psychology\n[16]. The systematic evaluation and bench-marking of the abilities of reinforcement learning ethical agents remains an open problem.\nReferences\n[1] P. Abbeel and A. Y. Ng. Apprenticeship learning via inverse reinforcement learning. In Proceedings of the Twenty-First International Conference on\nMachine Learning, ICML ’04, page 1, New York, NY, USA, 2004. Association for Computing Machinery. ISBN 1581138385. doi: 10.1145/1015330.\n1015430. URL https://doi.org/10.1145/1015330.1015430.\n[2] P. A. Alamdari, T. Q. Klassen, R. T. Icarte, and S. A. McIlraith. Be Considerate: Avoiding Negative Side Effects in Reinforcement Learning. In Proceedings\nof the 21st International Conference on Autonomous Agents and Multiagent Systems, AAMAS ’22, pages 18–26, Richland, SC, 2022. International\nFoundation for Autonomous Agents and Multiagent Systems. ISBN 978-1-4503-9213-6.\n[3] B. Alcaraz, O. Boissier, R. Chaput, and C. Leturc. Ajar: An argumentation-based judging agents framework for ethical reinforcement learning. In\nProceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems, AAMAS ’23, page 2427–2429, Richland, SC, 2023.\nInternational Foundation for Autonomous Agents and Multiagent Systems. ISBN 9781450394321.\n[4] S. Alkoby, A. Rath, and P. Stone. Teaching Social Behavior through Human Reinforcement for Ad Hoc Teamwork - The STAR Framework: Extended\nAbstract. In Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems, AAMAS ’19, pages 1773–1775, Richland,\nSC, 2019. International Foundation for Autonomous Agents and Multiagent Systems. ISBN 978-1-4503-6309-9. event-place: Montreal QC, Canada.\n[5] C. Allen, I. Smit, and W. Wallach. Artificial morality: Top-down, bottom-up, and hybrid approaches. Ethics Inf Technol, 7:149–155, 2005. doi: https:\n//doi.org/10.1007/s10676-006-0004-4.\n[6] P. Ammanabrolu, L. Jiang, M. Sap, H. Hajishirzi, and Y. Choi. Aligning to social norms and values in interactive narratives. In M. Carpuat, M.-C.\nde Marneffe, and I. V. Meza Ruiz, editors, Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages 5994–6017, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.\n18653/v1/2022.naacl-main.439. URL https://aclanthology.org/2022.naacl-main.439.\n[7] M. Anderson and S. Anderson. Geneth: A general ethical dilemma analyzer. Paladyn, Journal of Behavioral Robotics, 9:337–357, 11 2018. doi:\n10.1515/pjbr-2018-0024.\n[8] M. Anderson and S. L. Anderson. The status of machine ethics: A report from the aaai symposium. Minds Mach., 17(1):1–10, mar 2007. ISSN 0924-6495.\ndoi: 10.1007/s11023-007-9053-7. URL https://doi.org/10.1007/s11023-007-9053-7.\n[9] M. Anderson and S. L. Anderson, editors. Machine Ethics. Cambridge University Press, 2011.\n[10] M. Anderson, S. L. Anderson, and C. Armen. An approach to computing ethics. IEEE Intelligent Systems, 21(4):56–63, 2006.\n[11] S. Armstrong. Motivated value selection for artificial agents. In Workshops at the Twenty-Ninth AAAI Conference on Artificial Intelligence, 2015.\n[12] T. Arnold, D. Kasenberg, and M. Scheutz. Value alignment or misalignment - What will keep systems accountable? In AAAI Workshop - Technical Report,\nvolume WS-17-01 - WS-17-15, pages 81 – 88, 2017. URL https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045276127&partnerID=40&md5=\nfa6c749c157a02113fb2891967a225da. Type: Conference paper.\n[13] A. Balakrishnan, D. Bouneffouf, N. Mattei, and F. Rossi. Using contextual bandits with behavioral constraints for constrained online movie recommen-\ndation. In Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18, pages 5802–5804. International Joint\nConferences on Artificial Intelligence Organization, 7 2018. doi: 10.24963/ijcai.2018/843. URL https://doi.org/10.24963/ijcai.2018/843.\n[14] A. Balakrishnan, D. Bouneffouf, N. Mattei, and F. Rossi. Using multi-armed bandits to learn ethical priorities for online ai systems. IBM Journal of\nResearch and Development, 63(4/5):1:1–1:13, 2019. doi: 10.1147/JRD.2019.2945271.\n[15] A. Balakrishnan, D. Bouneffouf, N. Mattei, and F. Rossi. Incorporating Behavioral Constraints in Online AI Systems. In Proceedings of the Thirty-Third\nAAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on\nEducational Advances in Artificial Intelligence, AAAI’19/IAAI’19/EAAI’19. AAAI Press, 2019. ISBN 978-1-57735-809-1. doi: 10.1609/aaai.v33i01.\n33013. URL https://doi.org/10.1609/aaai.v33i01.33013. Place: Honolulu, Hawaii, USA.\n[16] P. Bello and B. F. Malle. Computational approaches to morality. In R. Sun, editor, Cambridge Handbook of Computational Cognitive Sciences, pages\n1037–1063. Cambridge University Press, 2023.\n[17] T. Bench-Capon and S. Modgil. Norms and value based reasoning: justifying compliance and violation. Artificial Intelligence and Law, 25(1):29–64, Mar\n2017. ISSN 1572-8382.\n[18] J. Bragg and I. Habli. What Is Acceptably Safe for Reinforcement Learning? In M. Hoshi and S. Seki, editors, Developments in Language Theory, volume\n11088, pages 418–430. Springer International Publishing, Cham, 2018. ISBN 978-3-319-98653-1 978-3-319-98654-8. doi: 10.1007/978-3-319-99229-7_\n35. URL http://link.springer.com/10.1007/978-3-319-99229-7_35. Series Title: Lecture Notes in Computer Science.\n[19] S. Bringsjord, K. Arkoudas, and P. Bello. Toward a general logicist methodology for engineering ethically correct robots. IEEE Intelligent Systems, 21(4):\n38–44, 2006. doi: 10.1109/MIS.2006.82.\n[20] P. Butlin. AI Alignment and Human Reward. In AIES ‘21: PROCEEDINGS OF THE 2021 AAAI/ACM CONFERENCE ON AI, ETHICS, AND SOCIETY,\npages 437–445, 1601 Broadway, 10th Floor, NEW YORK, NY, UNITED STATES, 2021. ASSOC COMPUTING MACHINERY. ISBN 978-1-4503-8473-\n5. doi: 10.1145/3461702.3462570. Backup Publisher: AAAI; Assoc Comp Machinery; ACM SIGAI Type: Proceedings Paper.\n[21] D. Byrd. Learning Not to Spoof. In Proceedings of the Third ACM International Conference on AI in Finance, ICAIF ’22, pages 139–147, New York, NY,\nUSA, 2022. Association for Computing Machinery. ISBN 978-1-4503-9376-8. doi: 10.1145/3533271.3561767. URL https://doi.org/10.1145/3533271.\n3561767. event-place: New York, NY, USA.\n[22] J.-A. Cervantes, S. López, L.-F. Rodríguez, S. Cervantes, F. Cervantes, and F. Ramos. Artificial Moral Agents: A Survey of the Current Status. Science\nand Engineering Ethics, 26(2):501–532, Apr. 2020. ISSN 1353-3452, 1471-5546. doi: 10.1007/s11948-019-00151-x. URL http://link.springer.com/10.\n1007/s11948-019-00151-x.\n[23] R. Chaput, J. Duval, O. Boissier, M. Guillermin, and S. Hassas. A Multi-Agent Approach to Combine Reasoning and Learning for an Ethical Behavior.\nIn AIES ‘21: PROCEEDINGS OF THE 2021 AAAI/ACM CONFERENCE ON AI, ETHICS, AND SOCIETY, pages 13–23, 1601 Broadway, 10th Floor,\nNEW YORK, NY, UNITED STATES, 2021. ASSOC COMPUTING MACHINERY. ISBN 978-1-4503-8473-5. doi: 10.1145/3461702.3462515. Backup\nPublisher: AAAI; Assoc Comp Machinery; ACM SIGAI Type: Proceedings Paper.\n[24] R. Chaput, L. Matignon, and M. Guillermin. Learning to identify and settle dilemmas through contextual user preferences. In 2023 IEEE 35th International\nConference on Tools with Artificial Intelligence (ICTAI), pages 474–479, Nov. 2023. doi: 10.1109/ICTAI59109.2023.00075.\n[25] J. Chen, Y. Zuo, D. Zhang, Z. Qu, and C. Wang. Promoting constructive interaction and moral behaviors using adaptive empathetic learning. In H. Yu,\nJ. Liu, L. Liu, Z. Ju, Y. Liu, and D. Zhou, editors, Intelligent Robotics and Applications, pages 3–14, Cham, 2019. Springer International Publishing. ISBN\n978-3-030-27526-6.\n[26] M. Coeckelbergh. Personal robots, appearance, and human good: a methodological reflection on roboethics. International Journal of Social Robotics, 1:\n217–221, 2009.\n[27] N. Crook and J. Corneli.\nThe Anatomy of moral agency: A theological and neuroscience inspired model of virtue ethics.\nCognitive Computation\nand Systems, 3(2):109–122, 2021. doi: https://doi.org/10.1049/ccs2.12024. URL https://onlinelibrary.wiley.com/doi/abs/10.1049/ccs2.12024. _eprint:\nhttps://onlinelibrary.wiley.com/doi/pdf/10.1049/ccs2.12024.\n[28] L. Dennis, M. Fisher, M. Slavkovik, and M. Webster.\nFormal verification of ethical choices in autonomous systems.\nRobotics and Autonomous\nSystems, 77:1–14, 2016.\nISSN 0921-8890.\ndoi: https://doi.org/10.1016/j.robot.2015.11.012.\nURL https://www.sciencedirect.com/science/article/pii/\nS0921889015003000.\n[29] J. F. Fisac, M. A. Gates, J. B. Hamrick, C. Liu, D. Hadfield-Menell, M. Palaniappan, D. Malik, S. S. Sastry, T. L. Griffiths, and A. D. Dragan. Pragmatic-\nPedagogic Value Alignment. In N. M. Amato, G. Hager, S. Thomas, and M. Torres-Torriti, editors, Robotics Research, volume 10, pages 49–57. Springer\nInternational Publishing, Cham, 2020. ISBN 978-3-030-28618-7 978-3-030-28619-4. doi: 10.1007/978-3-030-28619-4_7. URL http://link.springer.com/\n10.1007/978-3-030-28619-4_7. Series Title: Springer Proceedings in Advanced Robotics.\n[30] U. Grandi, E. Lorini, T. Parker, and R. Alami. Logic-based ethical planning. In A. Dovier, A. Montanari, and A. Orlandini, editors, AIxIA 2022 - Advances\nin Artificial Intelligence - XXIst International Conference of the Italian Association for Artificial Intelligence, AIxIA 2022, Udine, Italy, November 28 -\nDecember 2, 2022, Proceedings, volume 13796 of Lecture Notes in Computer Science, pages 198–211. Springer, 2022. doi: 10.1007/978-3-031-27181-6\\\n_14. URL https://doi.org/10.1007/978-3-031-27181-6_14.\n[31] T. Gu, H. Gao, L. Li, X. Bao, and Y. Li. An Approach for Training Moral Agents via Reinforcement Learning. Jisuanji Yanjiu yu Fazhan/Computer\nResearch and Development, 59(9):2039 – 2050, 2022. doi: 10.7544/issn1000-1239.20210474. URL https://www.scopus.com/inward/record.uri?eid=2-s2.\n0-85137043246&doi=10.7544%2fissn1000-1239.20210474&partnerID=40&md5=bc136c7f19e488a3e3f92fc96162f711. Type: Article.\n[32] J. Haas. Moral Gridworlds: A Theoretical Proposal for Modeling Artificial Moral Cognition. MINDS AND MACHINES, 30(2):219–246, June 2020.\nISSN 0924-6495. doi: 10.1007/s11023-020-09524-9. Place: VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS Publisher:\nSPRINGER Type: Article.\n[33] D. Hadfield-Menell, A. Dragan, P. Abbeel, and S. Russell. Cooperative inverse reinforcement learning. In Advances in Neural Information Processing\nSystems, pages 3916–3924, 2016.\n[34] D. Hendrycks, M. Mazeika, A. Zou, S. Patel, C. Zhu, J. Navarro, B. Li, D. Song, and J. Steinhardt. Moral scenarios for reinforcement learning agents. In\nICLR 2021 Workshop on Security and Safety in Machine Learning Systems, 2021.\n[35] A. Holgado-Sánchez, J. Arias, M. Moreno-Rebato, and S. Ossowski. On Admissible Behaviours for Goal-Oriented Decision-Making of Value-Aware\nAgents. In V. Malvone and A. Murano, editors, Multi-Agent Systems, volume 14282, pages 415–424. Springer Nature Switzerland, Cham, 2023. ISBN\n978-3-031-43263-7 978-3-031-43264-4. doi: 10.1007/978-3-031-43264-4_27. URL https://link.springer.com/10.1007/978-3-031-43264-4_27. Series\nTitle: Lecture Notes in Computer Science.\n[36] M. H. L. Kaas.\nRaising ethical machines: Bottom-up methods to implementing machine ethics.\n2021.\ndoi: 10.4018/978-1-7998-4894-3.\nch004.\nURL https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137310885&doi=10.4018%2f978-1-7998-4894-3.ch004&partnerID=40&md5=\n856b5207776676fcce3b40db0d108ec8. Publication Title: Machine Law, Ethics, and Morality in the Age of Artificial Intelligence Type: Book chapter.\n[37] D. Kasenberg and M. Scheutz. Inverse norm conflict resolution. In Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society, AIES\n’18, page 178–183, New York, NY, USA, 2018. Association for Computing Machinery. ISBN 9781450360128. doi: 10.1145/3278721.3278775. URL\nhttps://doi.org/10.1145/3278721.3278775.\n[38] D. Kasenberg, T. Arnold, and M. Scheutz. Norms, Rewards, and the Intentional Stance Comparing Machine Learning Approaches to Ethical Train-\ning. In PROCEEDINGS OF THE 2018 AAAI/ACM CONFERENCE ON AI, ETHICS, AND SOCIETY (AIES’18), pages 184–190, 1515 BROADWAY,\nNEW YORK, NY 10036-9998 USA, 2018. ASSOC COMPUTING MACHINERY. ISBN 978-1-4503-6012-8. doi: 10.1145/3278721.3278774. Backup\nPublisher: AAAI; Assoc Comp Machinery; ACM SIGAI; Berkeley Existential Risk Initiat; DeepMind Eth & Soc; Future Life Inst; IBM Res AI; PriceWa-\nterhouse Coopers; Tulane Univ Type: Proceedings Paper.\n[39] S. Krening. Q-learning as a model of utilitarianism in a human–machine team. Neural Computing and Applications, 35(23):16853–16864, Aug. 2023.\nISSN 1433-3058. doi: 10.1007/s00521-022-08063-x. URL https://doi.org/10.1007/s00521-022-08063-x.\n[40] B. Liao, P. Pardo, M. Slavkovik, and L. van der Torre. The jiminy advisor: Moral agreements among stakeholders based on norms and argumentation. J.\nArtif. Intell. Res., 77:737–792, 2023. doi: 10.1613/jair.1.14368. URL https://doi.org/10.1613/jair.1.14368.\n[41] B. F. Malle. Integrating robot ethics and machine morality: the study and design of moral competence in robots. Ethics and Information Technology, 18\n(4):243–256, Dec 2016. ISSN 1572-8439. doi: 10.1007/s10676-015-9367-8.\n[42] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, S. Petersen,\nC. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg, and D. Hassabis. Human-level control through deep reinforcement\nlearning. Nature, 518(7540):529–533, Feb. 2015. ISSN 1476-4687. doi: 10.1038/nature14236. URL https://doi.org/10.1038/nature14236.\n[43] M. S. A. Nahian, S. Frazier, B. Harrison, and M. O. Riedl.\nTraining value-aligned reinforcement learning agents using a normative prior.\nCoRR,\nabs/2104.09469, 2021. URL https://arxiv.org/abs/2104.09469.\n[44] V. Nallur. Landscape of machine implemented ethics. Science and Engineering Ethics, 2020. doi: 10.1007/s11948-020-00236-y. URL https://doi.org/10.\n1007/s11948-020-00236-y.\n[45] E. A. Neufeld, E. Bartocci, A. Ciabattoni, and G. Governatori. Enforcing ethical goals over reinforcement-learning policies. Ethics and Information\nTechnology, 24(4):43, Sept. 2022. ISSN 1572-8439. doi: 10.1007/s10676-022-09665-8. URL https://doi.org/10.1007/s10676-022-09665-8.\n[46] R. Noothigattu, D. Bouneffouf, N. Mattei, R. Chandra, P. Madan, K. R. Varshney, M. Campbell, M. Singh, and F. Rossi. Teaching ai agents ethical values\nusing reinforcement learning and policy orchestration. IBM J. Res. Dev., 63:2:1–2:9, 2019.\n[47] A. Pan, J. S. Chan, A. Zou, N. Li, S. Basart, T. Woodside, H. Zhang, S. Emmons, and D. Hendrycks. Do the Rewards Justify the Means? Measuring\nTrade-Offs between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark. In Proceedings of the 40th International Conference on Machine\nLearning, ICML’23. JMLR.org, 2023. Place: Honolulu, Hawaii, USA.\n[48] M. Peschl. Training for Implicit Norms in Deep Reinforcement Learning Agents through Adversarial Multi-Objective Reward Optimization. In Proceed-\nings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society, AIES ’21, pages 275–276, New York, NY, USA, 2021. Association for Computing\nMachinery. ISBN 978-1-4503-8473-5. doi: 10.1145/3461702.3462473. URL https://doi.org/10.1145/3461702.3462473. event-place: Virtual Event, USA.\n[49] M. Peschl, A. Zgonnikov, F. A. Oliehoek, and L. C. Siebert. Moral: Aligning ai with human norms through multi-objective reinforced active learning. In\nProceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems, AAMAS ’22, page 1038–1046, Richland, SC, 2022.\nInternational Foundation for Autonomous Agents and Multiagent Systems. ISBN 9781450392136.\n[50] R. Pinka. Synthetic Deliberation: Can Emulated Imagination Enhance Machine Ethics? Minds Mach., 31(1):121–136, Mar. 2021. ISSN 0924-6495. doi:\n10.1007/s11023-020-09531-w. URL https://doi.org/10.1007/s11023-020-09531-w. Place: USA Publisher: Kluwer Academic Publishers.\n[51] A. Plebe. Neurocomputational Model of Moral Behaviour. Biol. Cybern., 109(6):685–699, Dec. 2015. ISSN 0340-1200. doi: 10.1007/s00422-015-0669-z.\nURL https://doi.org/10.1007/s00422-015-0669-z. Place: Berlin, Heidelberg Publisher: Springer-Verlag.\n[52] B. A. Ribeiro and M. B. Da Silva. Machine Ethics and the Architecture of Virtue. In T. Guarda, F. Portela, and J. M. Diaz-Nafria, editors, Advanced\nResearch in Technologies, Information, Innovation and Sustainability, volume 1936, pages 384–401. Springer Nature Switzerland, Cham, 2024. ISBN\n978-3-031-48854-2 978-3-031-48855-9. doi: 10.1007/978-3-031-48855-9_29. URL https://link.springer.com/10.1007/978-3-031-48855-9_29. Series\nTitle: Communications in Computer and Information Science.\n[53] M. T. Ribeiro, S. Singh, and C. Guestrin. “why should i trust you?”: Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD\nInternational Conference on Knowledge Discovery and Data Mining, KDD ’16, page 1135–1144, New York, NY, USA, 2016. Association for Computing\nMachinery. ISBN 9781450342322. doi: 10.1145/2939672.2939778. URL https://doi.org/10.1145/2939672.2939778.\n[54] P. Robinson. Action Guidance and AI Alignment. In Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society, AIES ’23, pages\n387–395, New York, NY, USA, 2023. Association for Computing Machinery. ISBN 9798400702310. doi: 10.1145/3600211.3604714. URL https:\n//doi.org/10.1145/3600211.3604714. event-place: <conf-loc>, <city>Montréal</city>, <state>QC</state>, <country>Canada</country>, </conf-loc>.\n[55] M. Rodriguez-Soto, M. Lopez-Sanchez, and J. A. Rodriguez-Aguilar. A structural solution to sequential moral dilemmas. In Proceedings of the 19th\nInternational Conference on Autonomous Agents and MultiAgent Systems, AAMAS ’20, page 1152–1160, Richland, SC, 2020. International Foundation\nfor Autonomous Agents and Multiagent Systems. ISBN 9781450375184.\n[56] M. Rodriguez-Soto, M. López-Sánchez, and J. A. Rodríguez-Aguilar. Multi-objective reinforcement learning for designing ethical environments. In\nZ. Zhou, editor, Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI 2021, Virtual Event / Montreal, Canada,\n19-27 August 2021, pages 545–551. ijcai.org, 2021. doi: 10.24963/ijcai.2021/76. URL https://doi.org/10.24963/ijcai.2021/76.\n[57] M. Rodriguez-Soto, M. Lopez-Sanchez, and J. A. Rodriguez-Aguilar. Guaranteeing the learning of ethical behaviour through multi-objective reinforcement\nlearning. In Adaptive and Learning Agents Workshop (AAMAS 2021), 2021.\n[58] M. Rodriguez-Soto, J. A. Rodríguez-Aguilar, and M. López-Sánchez. Building multi-agent environments with theoretical guarantees on the learning of\nethical policies. 2022. URL https://api.semanticscholar.org/CorpusID:252285579.\n[59] M. Rodriguez-Soto, M. Serramia, M. Lopez-Sanchez, and J. A. Rodriguez-Aguilar. Instilling moral value alignment by means of multi-objective rein-\nforcement learning. Ethics and Information Technology, 24(1):9, 2022.\n[60] M. Rolf, N. Crook, and J. J. Steil. From social interaction to ethical AI: a developmental roadmap. In 2018 JOINT IEEE 8TH INTERNATIONAL\nCONFERENCE ON DEVELOPMENT AND LEARNING AND EPIGENETIC ROBOTICS (ICDL-EPIROB), Joint IEEE International Conference on De-\nvelopment and Learning and Epigenetic Robotics ICDL-EpiRob, pages 204–211, 345 E 47TH ST, NEW YORK, NY 10017 USA, 2018. IEEE. ISBN\n978-1-5386-6110-9. Backup Publisher: IEEE ISSN: 2161-9484 Type: Proceedings Paper.\n[61] E. Roselló-Marín, M. Lopez-Sanchez, I. Rodríguez, M. Rodríguez-Soto, and J. A. Rodríguez-Aguilar.\nAn Ethical Conversational Agent\nto Respectfully Conduct In-Game Surveys.\nFrontiers in Artificial Intelligence and Applications, 356:335 – 344, 2022.\ndoi: 10.\n3233/FAIA220356.\nURL https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141697734&doi=10.3233%2fFAIA220356&partnerID=40&md5=\n8604d129c94da70cb8af1396f0f58909. Type: Conference paper.\n[62] L. Sanneman and J. A. Shah. Validating Metrics for Reward Alignment in Human-Autonomy Teaming. Comput. Hum. Behav., 146(C), Sept. 2023. ISSN\n0747-5632. doi: 10.1016/j.chb.2023.107809. URL https://doi.org/10.1016/j.chb.2023.107809. Place: NLD Publisher: Elsevier Science Publishers B. V.\n[63] C. Shea-Blymyer and H. Abbas. Generating Deontic Obligations From Utility-Maximizing Systems. In Proceedings of the 2022 AAAI/ACM Conference\non AI, Ethics, and Society, AIES ’22, pages 653–663, New York, NY, USA, 2022. Association for Computing Machinery. ISBN 978-1-4503-9247-1. doi:\n10.1145/3514094.3534163. URL https://doi.org/10.1145/3514094.3534163. event-place: Oxford, United Kingdom.\n[64] A. Slater. The Golem and the Game of Automation. In 2021 IEEE Conference on Norbert Wiener in the 21st Century (21CW), pages 1–4, July 2021. doi:\n10.1109/21CW48944.2021.9532551. ISSN: 2643-4482.\n[65] J. Stenseke.\nArtificial virtuous agents: from theory to machine implementation.\nAI & SOCIETY, 2021.\ndoi: 10.1007/s00146-021-01325-7.\nURL\nhttps://doi.org/10.1007/s00146-021-01325-7.\n[66] J. Stenseke. Artificial virtuous agents in a multi-agent tragedy of the commons. AI & SOCIETY, Oct. 2022. ISSN 0951-5666, 1435-5655. doi: 10.1007/\ns00146-022-01569-x. URL https://link.springer.com/10.1007/s00146-022-01569-x.\n[67] F.-Y. Sun, Y.-Y. Chang, Y.-H. Wu, and S.-D. Lin. Designing Non-Greedy Reinforcement Learning Agents with Diminishing Reward Shaping. In Proceed-\nings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society, AIES ’18, pages 297–302, New York, NY, USA, 2018. Association for Computing\nMachinery. ISBN 978-1-4503-6012-8. doi: 10.1145/3278721.3278759. URL https://doi.org/10.1145/3278721.3278759. event-place: New Orleans, LA,\nUSA.\n[68] R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. A Bradford Book, Cambridge, MA, USA, 2018. ISBN 0262039249.\n[69] M. Tassella, R. Chaput, and M. Guillermin. Artificial Moral Advisors: enhancing human ethical decision-making. In 2023 IEEE International Symposium\non Ethics in Engineering, Science, and Technology (ETHICS), pages 1–5, May 2023. doi: 10.1109/ETHICS57328.2023.10155026.\n[70] E. Tennant, S. Hailes, and M. Musolesi. Modeling moral choices in social dilemmas with multi-agent reinforcement learning, 2023.\n[71] S. Tolmeijer, M. Kneer, C. Sarasua, M. Christen, and A. Bernstein. Implementations in machine ethics: A survey. ACM Comput. Surv., 53(6), dec 2021.\nISSN 0360-0300. doi: 10.1145/3419633. URL https://doi.org/10.1145/3419633.\n[72] P. Vamplew, R. Dazeley, C. Foale, S. Firmin, and J. Mummery. Human-Aligned Artificial Intelligence is a Multiobjective Problem. Ethics and Inf. Technol.,\n20(1):27–40, Mar. 2018. ISSN 1388-1957. doi: 10.1007/s10676-017-9440-6. URL https://doi.org/10.1007/s10676-017-9440-6. Place: USA Publisher:\nKluwer Academic Publishers.\n[73] D. Vanderelst and A. F. T. Winfield. An architecture for ethical robots inspired by the simulation theory of cognition. Cogn. Syst. Res., 48:56–66, 2018.\ndoi: 10.1016/j.cogsys.2017.04.002. URL https://doi.org/10.1016/j.cogsys.2017.04.002.\n[74] A. Vishwanath, E. D. Bøhn, O. Granmo, C. Maree, and C. W. Omlin. Towards artificial virtuous agents: games, dilemmas and machine learning. AI Ethics,\n3(3):663–672, 2023. doi: 10.1007/s43681-022-00251-8. URL https://doi.org/10.1007/s43681-022-00251-8.\n[75] W. Wallach and C. Allen. Moral Machines: Teaching Robots Right from Wrong. Oxford University Press, 2008.\n[76] Y.-H. Wu and S.-D. Lin. A low-cost ethics shaping approach for designing reinforcement learning agents. Proceedings of the AAAI Conference on Artificial\nIntelligence, 32(1), Apr. 2018. URL https://ojs.aaai.org/index.php/AAAI/article/view/11498.\n[77] H. Yu, Z. Shen, C. Miao, C. Leung, V. R. Lesser, and Q. Yang. Building ethics into artificial intelligence. In Proceedings of the 27th International Joint\nConference on Artificial Intelligence, IJCAI’18, page 5527–5533. AAAI Press, 2018. ISBN 9780999241127.\n",
  "categories": [
    "cs.AI"
  ],
  "published": "2024-07-02",
  "updated": "2024-07-02"
}