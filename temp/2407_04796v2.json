{
  "id": "http://arxiv.org/abs/2407.04796v2",
  "title": "Toucan: Many-to-Many Translation for 150 African Language Pairs",
  "authors": [
    "AbdelRahim Elmadany",
    "Ife Adebara",
    "Muhammad Abdul-Mageed"
  ],
  "abstract": "We address a notable gap in Natural Language Processing (NLP) by introducing\na collection of resources designed to improve Machine Translation (MT) for\nlow-resource languages, with a specific focus on African languages. First, we\nintroduce two language models (LMs), Cheetah-1.2B and Cheetah-3.7B, with 1.2\nbillion and 3.7 billion parameters respectively. Next, we finetune the\naforementioned models to create toucan, an Afrocentric machine translation\nmodel designed to support 156 African language pairs. To evaluate Toucan, we\ncarefully develop an extensive machine translation benchmark, dubbed\nAfroLingu-MT, tailored for evaluating machine translation. Toucan significantly\noutperforms other models, showcasing its remarkable performance on MT for\nAfrican languages. Finally, we train a new model, spBLEU-1K, to enhance\ntranslation evaluation metrics, covering 1K languages, including 614 African\nlanguages. This work aims to advance the field of NLP, fostering cross-cultural\nunderstanding and knowledge exchange, particularly in regions with limited\nlanguage resources such as Africa. The GitHub repository for the Toucan project\nis available at https://github.com/UBC-NLP/Toucan.",
  "text": "Toucan: Many-to-Many Translation for 150 African Language Pairs\nAbdelRahim Elmadanyξ,⋆Ife Adebaraξ,⋆Muhammad Abdul-Mageedξ,λ\nξThe University of British Columbia\nλ Invertible AI\n{a.elmadany,ife.adebara,muhammad.mageed}@ubc.ca\nAbstract\nWe address a notable gap in Natural Language\nProcessing (NLP) by introducing a collection\nof resources designed to improve Machine\nTranslation (MT) for low-resource languages,\nwith a specific focus on African languages.\nFirst, we introduce two language models\n(LMs), Cheetah-1.2B and Cheetah-3.7B,\nwith 1.2 billion and 3.7 billion parameters\nrespectively. Next, we finetune the aforemen-\ntioned models to create Toucan, an Afrocentric\nmachine translation model designed to support\n156 African language pairs. To evaluate Tou-\ncan, we carefully develop an extensive machine\ntranslation benchmark, dubbed AfroLingu-MT,\ntailored for evaluating machine translation.\nToucan significantly outperforms other models,\nshowcasing its remarkable performance on\nMT for African languages. Finally, we train a\nnew model, spBLEU1K, to enhance translation\nevaluation metrics, covering 1K languages,\nincluding 614 African languages. This work\naims to advance the field of NLP, fostering\ncross-cultural understanding and knowledge\nexchange, particularly in regions with limited\nlanguage resources such as Africa. The GitHub\nrepository for the Toucan project is available at\nhttps://github.com/UBC-NLP/Toucan.\n1\nIntroduction\nMachine Translation (MT) is an important tech-\nnology that bridges linguistic divides and enables\ncommunication across the globe. Although trans-\nfer learning methods (Zoph et al., 2016) employing\nmultilingual language models (mLM) (Xue et al.,\n2021a; Liu et al., 2020) have benefited the field\nspecially for languages with limited resources (Liu\net al., 2023), a significant gap remains for many\nAfrican languages. In particular, although a hand-\nful of mLMs finetuned for MT for African lan-\nguages (Adelani et al., 2022; Oladipo et al., 2023;\n⋆Authors contributed equally.\nFigure 1: Toucan is a powerful MT model, proficiently\ntrained on 156 language pairs. It covers a wide spectrum\nof 43 African languages as well as Arabic, English, and\nFrench\nJude Ogundepo et al., 2022), these pioneering\nworks only serve 31 out of the 2, 000+ languages of\nthe African continent. This lack of coverage means\nthe issues of language barriers, the risk of language\nextinction, and the under-representation of diverse\ncommunities in global conversations (Koehn and\nKnowles, 2017). Language barriers in particular\npose significant challenges, hindering the smooth\nexchange of ideas, information, and cultural nu-\nances across diverse linguistic landscapes. In con-\ntexts characterized by limited resources, where ac-\ncess to proficient human translators is constrained,\nMT has the potential to be a transformative remedy\nthat offer unparalleled advantages in dismantling\nlinguistic obstacles and promote heightened cross-\ncultural comprehension.\nIn this paper, we address this gap by presenting\na family of pretrained models that we also finetune\nfor machine translation, an extensive evaluation\nbenchmark, and an evaluation metric with wider\narXiv:2407.04796v2  [cs.CL]  12 Jul 2024\ncoverage of African languages. By introducing\nthese, we aim to contribute to the advancement\nof low-resources language technology especially\nAfrican languages, unlocking new possibilities for\ncross-cultural understanding and knowledge ex-\nchange. Our main contributions can be summarized\nas follow:\n1. AfroLingu-MT. We introduce AfroLingu-\nMT, a benchmark for African languages com-\nprising 156 language pairs. To the best of\nour knowledge, AfroLingu-MTis the largest\nAfrican MT benchmark to date. We design it\nto rigorously evaluate and advance the state\nof MT for a diverse host of African languages,\naddressing addressing a critical need in this\narea.\n2. Pretrained\nLLM.\nWe\npresent\na\nnew\nsequence-to-sequence large language model\n(LLM) that covers 517 African languages and\n10 foreign languages including - Arabic, En-\nglish, French, German, Greek, Italian, Por-\ntuguese, Russian, Spanish, and Turkish. The\nmodel is available in two sizes, 1.2B and\n3.7B parameters. We refer to these models\nas “Cheetah-1.2B” and “Cheetah-3.7B”.\n3. Toucan models. A versatile many-to-many\nfamily of MT models capable of translating\nbetween 46 different languages (43 African\nlanguages and the three non-Indigenous ma-\njor languages in Africa: Arabic, English, and\nFrench). Our models cover 156 translation\nlanguage pairs.\n4. Comprehensive evaluation. We offer a com-\nprehensive comparison between generative\nand sequence-to-sequence LMs by evaluating\na wide range of models on our AfroLingu-\nMT benchmark under both few-shot and full\nfinetuning scenarios. For our evaluation we\nuse both existing models and also introduce\nnew models as we outline next.\nspBLEU1K.\nWe introduce spBLEU1K, a\nsentencepiece model that covers 1, 003 lan-\nguages, including 614 African languages, de-\nsigned to improve translation evaluation qual-\nity. Our model aims to address the limita-\ntions found in traditional BLEU score evalu-\nations (Peters et al., 2018) and the FLORES\nspBLEU model (Goyal et al., 2022) by ex-\npanding coverage to a vast array of languages\nthat have historically been underrepresented\nin translation models and benchmarks.\nThe rest of the paper is organized as follows:\nIn Section 2, we discuss related work for MT\nbenchmarks, models, and tools. In Section 3 and\nSection 4, we describe AfroLingu-MT evaluation\nbenchmark and Toucan models respectively. We\nprovide details of the empirical evaluation in Sec-\ntion 5, including experimental setup, baselines, and\nevaluation metrics. We present the results and dis-\ncussion in Section 6. Finally, we conclude in Sec-\ntion 7, and outline a number of limitations, ethics\nand use cases for our work in Section 8 and Section\n9 respectively.\n2\nLiterature Review\nMT has seen remarkable advancements, partic-\nularly in supporting underrepresented languages\nsuch as those spoken in Africa (Team et al., 2022;\nJude Ogundepo et al., 2022; Adelani et al., 2022).\nThe development of MT tools for African lan-\nguages is vital for promoting linguistic diversity,\nfostering cross-cultural communication, and driv-\ning socio-economic progress. In this review, we\nbriefly cover LLMs supporting African languages\nand related topics such as datasets and evaluation\ntools. Also, we provide additional details in Sec-\ntion A in the Appendix.\n2.1\nDatasets and Benchmarks\nA major hindrance to developing MT models for\nAfrican languages is the scarcity of data (Adda\net al., 2016; Adebara and Abdul-Mageed, 2022).\nWhile web data collection typically provides ample\nfine quality datasets for high-resource languages,\nthe corpora for many African languages obtained\nthrough similar methods are often constrained in\nboth size and quality (Kreutzer et al., 2021; Al-\nabi et al., 2020). To address these issues, a num-\nber of benchmarks for both training and evalua-\ntion have been developed. Notable among them\nare FLORES-101 (Goyal et al., 2021), FLORES-\n200 (Goyal et al., 2022), Menyo-20 (Adelani et al.,\n2021), Lafand-MT (Adelani et al., 2022), and Salt\n(Akera et al., 2022).\n2.2\nModels and Tools\nModels such as “No Language Left Behind\"\n(NLLB) (Team et al., 2022) cater to translation\nneeds in 200 languages, including 23 African\nones.\nSimilarly, M2M-100 (Fan et al., 2020)\nType\nModel\nLang/Total\nALMA (Xu et al., 2023)\nEnglish only\nALMA-MT (Xu et al., 2023)\n0/6\nBloomz (Muennighoff et al., 2022)\n14/101\nBloomz-MT (Muennighoff et al., 2022)\nUnknown/46\nLlama-2 (Touvron et al., 2023b)\nEnglish only\nCLM\nAfri-mT5 (Adelani et al., 2022)\n17/17\nAfriTeVa (Oladipo et al., 2023)\n10/10\nAya (Üstün et al., 2024)\n15/101\nMistral (Jiang et al., 2023)\nEnglish only\nmT0 (Muennighoff et al., 2022)\n14/101\nmT5 (Xue et al., 2020)\n12/101\nSeq2Seq\nCheetah (Adebara et al., 2024)\n517/527\nTable 1: Models with African languages represented. Lang/Total: the number of African languages covered by the\nmodel/the number of total language covered.\nand AfriTeVa (Jude Ogundepo et al., 2022) sup-\nport 17 and 10 African languages, respectively,\nutilizing text-to-text architectures.\nAfriTeVa-\nV2 (Oladipo et al., 2023) has enhanced support\nfor 16 African languages with improved quality\npretraining data. AfroMT5 (Adelani et al., 2022)\nand AfriMBART (Adelani et al., 2022) each cover\n17 African languages. Additionally, Cheetah (Ade-\nbara et al., 2024) extends its support to 517 African\nlanguages and 10 widely spoken global languages.\nDecoder-only models, exemplified by the Genera-\ntive Pretrained Transformer (GPT) (OpenAI, 2023;\nBrown et al., 2020) and Llama (Touvron et al.,\n2023a,b), demonstrate outstanding performance\nacross various tasks such as text comprehension,\nlanguage translation, and content generation but\nusually fall short in terms of coverage of African\nlanguages as we will show in this work. Aya (Üstün\net al., 2024) is a massively multilingual genera-\ntive language model that supports instructions fol-\nlowing in 101 languages including 24 African lan-\nguages. Aya has been shown to outperform models\nlike mT0 (Muennighoff et al., 2023) and BLOOMZ\n(Muennighoff et al., 2023) on a wide variety of au-\ntomatic and human evaluations despite covering\ndouble the number of languages.\n2.3\nEvaluation Metrics\nProminent evaluation metrics such as BLEU (Pap-\nineni et al., 2002) and ChrF (Popovi´c, 2015a) fo-\ncus on assessing n-gram correspondence between\nmodel translations and human references, favor-\ning precision. Meanwhile, METEOR (Banerjee\nand Lavie, 2005) emphasizes both precision and\nrecall by considering synonyms, stemming, and\nword order. TER (Agarwal and Lavie, 2008) mea-\nsures edit distance between machine-generated and\nreference texts, while COMET (Rei et al., 2020;\nStewart et al., 2020) leverages contextual embed-\ndings for semantic similarity evaluation. These met-\nrics aid in benchmarking machine translation (MT)\nsystems, guiding improvements for higher quality\ntranslations. Goyal et al. (2022) introduced a new\nmetric, SentencePiece BLEU (spBLEU), which\nextends coverage to 101 languages, including 23\nAfrican languages. Similarly, AfriCOMET Wang\net al. (2023) is tailored for 17 African languages,\naddressing their unique challenges in MT evalua-\ntion. These developments signify ongoing efforts\nto create more robust and inclusive evaluation tools\nfor MT across diverse languages worldwide.\n3\nAfroLingu-MT Benchmark\nIn this section, we describe our data collection and\nconstruction procedures, along with the features\nof our comprehensive benchmark for evaluating\nAfrican MT systems, AfroLingu-MT. To create\nAfroLingu-MT , we execute several steps, encom-\npassing data curation, quality evaluation, pair se-\nlection, determination of translation directions, and\nspecification of the output format. We now explain\neach of these steps.\n3.1\nData Collection\nOur collection comprises data from a total of 43\ndatasets, encompassing 84 unique language pairs\nderived from 46 different languages. We also de-\nvelop a new manually translated dataset useful for\nevaluation in the government domain. In all, the\ndata cover 43 African languages from five language\nfamilies domiciled in 29 African countries. We\nalso include Arabic, English, and French, since\nthese are widely spoken in Africa. Table 2 in the\nAppendix provides detailed information about our\ncollected data, including the number of pairs and\ndata-points (i.e, examples) for each dataset. Ta-\nble C.1 (Appendix C) on the other hand has de-\ntails about each of the 46 languages in our dataset.\nThese tables serve as a valuable resource for un-\nderstanding the breadth and depth of our datasets,\nensuring transparency and facilitating further re-\nsearch in the field of MT for African languages.\nWe also translate into Yoruba a portion of the Arab-\nacquis data (Habash et al., 2017) and include it in\nthe benchmark. We refer to this data henceforth as\nLegal-genre.\nDatasets\n#Pairs\n#Examples\nAraOPUS-20 (Nagoudi et al., 2022)\n3\n2.01M\nBamanakan Lexicon (Bamba, 2016)\n2\n5, 978\nCorpora Ethiopia (Teferra Abate et al., 2018)\n7\n77, 739\nENGLISH-AKUAPEM TWI (azu, 2021)\n1\n25, 421\nEnglish-Luganda (muk, 2021)\n1\n15, 021\nFFR-Dataset\n1\n136, 098\nFlores-200 (Costa-jussà et al., 2022),\n237\n842\nFrench-Ewe-fongbe (deg, 2020)\n2\n70, 000\nGamayun(Öktem et al., 2021)\n7\n75, 000\nGlobal Voices (Tiedemann, 2012)\n9\n399, 178\nGnome (Tiedemann, 2012)\n14\n850, 068\nGourmet-MT\n5\n263, 882\nGov-ZA(Lastrucci et al., 2023; mar, 2023)\n54\n638, 737\nHorn-MT\n15\n21, 848\nIgbo-NLP\n1\n10, 008\nLafand-MT (Adelani et al., 2022)\n20\n5.2M\nLegal-genre (ours)\n1\n3, 580\nMasakhane Wazobia\n2\n66, 324\nMenyo-20k (Adelani et al., 2021)\n1\n20, 100\nMulti-paracrawl (Tiedemann, 2012)\n3\n46, 478\nNCHLT(Tiedemann, 2012)\n7\n746, 646\nOpen Subtitles (Tiedemann, 2012)\n1\n44, 703\nOpusinfopanki (Tiedemann, 2012)\n1\n47, 220\nOpusmemat (Tiedemann, 2012)\n1\n139, 260\nParacrawl\n2\n147, 396\nPidginUNMT_corpus\n1\n2, 101\nSalt (Akera et al., 2022)\n15\n25, 000\nTED (Reimers and Gurevych, 2020)\n6\n15, 401\nTico-19 (Tiedemann, 2012)\n18\n7, 740\nUmsuka eng - zul Corpus (Mabuya et al., 2021)\n1\n10, 701\nVuk’uzenzele (Lastrucci et al., 2023; mar, 2023)\n55\n66, 318\nWikimedia (Tiedemann, 2012)\n21\n132, 213\nXhosanavy (Tiedemann, 2012)\n1\n49, 981\nXNLI\n1\n1, 000\nYoruba Proverbs\n1\n5, 144\nTable 2: Detailed description of datasets in AfroLingu-\nMT benchmark.\n3.2\nData Quality\nTo ensure high quality, we follow a rigorous man-\nual review process for each dataset. This involves a\nthorough examination of the original paper associ-\nated with each dataset to gain a clear understanding\nof its data collection methodology. Following this\nreview, we classify the datasets into four quality\ntiers: Synthetic, Human evaluated, Gold, and\nUnknown. (1) Synthetic datasets consist of transla-\ntions generated solely by machine translation mod-\nels, without any human quality evaluation. We ex-\nclude these datasets from further consideration and\nthey are not part of our final collection. (2) Human\nevaluated quality translations are also generated by\nMT models, but typically undergo correction by\nhuman reviewers to improve their quality. (3) Gold\nquality translations are either directly translated or\nevaluated by human experts. Datasets in this cate-\ngory are also sourced from domains with a lower\nlikelihood of containing noisy data. (4) Unknown\nquality datasets either lack associated publications\nor detailed information about their data quality and\ncollection processes. Additionally, for certain lan-\nguages, we go beyond paper analysis and conduct\nspecific evaluations of translation quality. In partic-\nular, we manually assess translation pairs between\nEnglish and languages such as Yoruba, Hausa, and\nNigerian Pidgin.\n3.3\nPair Selection and Translation Directions\nWe exclude “Unknown” and “Syntheic” datasets,\nretaining only “Human Evaluated” and “Gols” qual-\nity data and standardizing the language codes to\nISO-693. Our objective is to facilitate develop-\nment of robust MT models capable of translating\nbetween a wide range of African languages as well\nas Arabic, English, and French.1 To achieve this,\nwe adopt a multifaceted approach essentially en-\nabling many-to-many translation. For instance, the\nuser may need to specify only the target language\nthus allowing for versatile translation possibilities\nincluding translation from any language into En-\nglish. This results in the selection of 156 distinct\nlanguage pairs.\n3.4\nData Selection\nAs depicted in Table 2 (Appendix), there is signif-\nicant variation in data distribution among the lan-\nguage pairs, with some having a substantial number\nof data points and others having much fewer. To\ncreate a balanced training dataset, especially since\nwe are targeting many-to-many translation, we aim\nto obtain data from each translation direction for\neach language pair. First, we maintain the original\ndataset splits where one exists from source; oth-\nerwise, we divide the language pairs into training,\ndevelopment, and testing datasets in an 80/10/10\nratio. Next we sample 5K/50/200 data points for\n1Arabic, English and French are widely spoken in Africa.\n{\"langcode\":\"nyn-ach\", \"instruction\":\"Translate the following text to Acholi language. Return only the\ntranslated sentence only. Do not repeat the instruction.\", \"input\":\"Bakakora omukoro gw’okuhendera\nemishomo yaabo, orwakashatu oruhwaire.\", \"output\":\"Gubed ki yub me kwero tyeko kwan i ceng adek\nma okato\"}\n{\"langcode\":\"ach-lug\",\"instruction\":\"Translate the following text to Luganda language. Return only the\ntranslated sentence only. Do not repeat the instruction.\", \"input\":\"Cal pa dako man onya i nyonyo pa\nmuno calo mac oro.\", \"output\":\"Ebifaananyi bye bibadde biyitangana ku mikuttu emigattabantu.\"}\n{\"langcode\":\"eng-teo\",\"instruction\":\"Translate the following text to Ateso language. Return only the\ntranslated sentence only. Do not repeat the instruction.\",\"input\":\"The Joint Anti-Terrorist Task Force\ncar was never recovered.\",\"output\":\"Mam aponi kodumunai emotoka loka Egurupu loka itunga luitijiete\nitunga lukodwaratau.\"}\nFigure 2: Examples from AfroLingu-MT benchmark train set.\ntraining, development, and testing, respectively,\nfor each language pair direction, such as English-\nto-Afrikaans (eng-afr) and Afrikaans-to-English\n(afr-eng). This approach enables us to facilitate\ntranslation for 46 unique languages. In addition,\nwhen dealing with abundant data, we ensure there\nis no overlap between source and target data points\nfor each pair in either of the directions. However,\nfor pairs with limited data, we swap the data points\nbetween the two directions to augment the dataset.\nAfroLingu-MT contains a total of 620, 573 parallel\ndata points, with 586, 261 allocated for training,\n7, 437 for development, and 26, 875 for the test\ndata split. As mentioned, it comprises translations\nfor 46 languages derived from 156 language pairs.\nWe show the data distribution for each language\npair in Table C.2 (Appendix C).\n3.5\nData Format\nFollowing the Alpaca style (Taori et al., 2023), we\norganize our dataset as follows:\nLang-code: This specifies the ISO-639-3 codes for\nboth the source and target languages.\nInstruction: This field provides a concise descrip-\ntion of the task.\nInput: This field contains the source text intended\nfor translation.\nOutput: This includes the text translated into the\ntarget language.\nWe save each data point on a separate line in\nJSONL format. Figure 2 shows examples of trans-\nlating from source language to target language\nin AfroLingu-MT.\n3.6\nAfroLingu-MT in Comparison\nTable 3 presents a comparison between AfroLingu-\nMT and existing benchmarks.\nThe table high-\nBenckmark\nLang/Total\nFLORES-101 (Goyal et al., 2021)\n23 / 101\nFLORES-200 (Goyal et al., 2022)\n23 / 200\nMenyo-20 (Adelani et al., 2021)\n1/2\nLafand-MT (Adelani et al., 2022)\n16/18\nSalt (Akera et al., 2022)\n5/5\nAfroLingu-MT (ours)\n43/46\nTable 3: AfroLingu-MT benchmark (ours) in compari-\nson with with other benchmarks with notable African\nlanguage coverage. Lang/Total column describes the\nnumber of African languages comparing with the cov-\nered languages in the language models.\nlights the total number of supported languages and\nlanguage pairs in each benchmark. As Table 3\nshows, compared to other benchmarks, AfroLingu-\nMT doubles the number of African languages cov-\nered and has an order of magnitude higher coverage\nin terms of language pairs/translation directions.\n4\nToucan Models\nIn this work, we develop a number of many-\nto-many Afrocentric machine translation models\ndubbed Toucan. For this purpose, we first pretrain a\nnumber of Afrocentric sequence-to-sequence mod-\nels that serve as the foundational backbone for our\nproposed machine translation Toucan models.\n4.1\nCheetah Backbone LMs\nTo effectively train a MT language model for\nAfrican languages, it is crucial to start with a pow-\nerful, Afrocentric pretrained language model. For\nthis purpose, we select Cheetah (Adebara et al.,\n2024), a recently introduced SoTA model with ex-\ntensive coverage encompassing 517 African lan-\nguages. One limitation of Cheetah, however, is that\nit is available only in a base architecture, featuring\n580M parameters. Given our objective to develop a\nlarge-scale language model for machine translation\ncapabale of serving 156 directions, this base model\ndoes not fully meet our requirements.\nTo address this limitation, we embark on training\nlarger and more expansive Afrocentric sequence-\nto-sequence models. We focus on two sizes: one\nmodel with 1.2B parameters and another with\n3.7B parameters.\nWe refer to the new mod-\nels “Cheetah-1.2B” and “Cheetah-3.7B”, respec-\ntively, to reflect their enhanced capabilities and pa-\nrameter scale. These models represent a significant\nadvancement in our efforts to improve machine\ntranslation for African languages, offering greater\ncapacities in handling the rich linguistic nuances of\nAfrican languages. Cheetah Pertaining. To train\nthe new Cheetah models, we utilize the same pre-\ntraining dataset employed in training the original\nCheetah-base model (Adebara et al., 2024). This\nstrategic choice ensures consistency in the founda-\ntional data across models, enabling the advanced\nCheetah-1.2B and Cheetah-3.7B versions to build\nupon the rich linguistic diversity captured in the\noriginal dataset. We refer to (Adebara et al., 2024)\nfor more information about the pretraining data of\nCheetah models. We employ a learning rate of 0.01,\na batch size of 1, 024 sequences, and a maximum\nsequence length of 1, 024. Each model undergoes\npretraining for 1 million steps. The training process\nis conducted on Google Cloud TPU with 128 cores\n(v3 −128) provided by the TensorFlow Research\nCloud (TFRC). We provide additional details on\npretraining in Section B in the Appendix.\n4.2\nToucan Finetuning\nWe finetune the vanilla Cheetah-base model as well\nas the newly proposed architectures, Cheetah-1.2B\nand Cheetah-3.7B, on our AfroLingu-MT. As ex-\nplained in Section 3, this dataset is the largest\nand most diverse African MT dataset. We refer\nto our new models fintuned for MT as Toucan-base,\nToucan-1.2B and Toucan-3.7B. We provide more\ninformation about model finetuning in Section 5.2.\n5\nEmpirical Evaluation\n5.1\nEvaluation Settings\nWe evaluate AfroLingu-MT in diverse scenarios,\nincluding both finetuning and zero-shot settings.\n(1) We conduct comparative analyses between\nmultilingual and Africa-centric pretrained lan-\nguage models by finetuning these models on\nour AfroLingu-MT dataset. Specifically, we uti-\nlize mT5 (Xue et al., 2020) and mT0 (Muennighoff\net al., 2022) as our representative multilingual pre-\ntrained models. In contrast, we compare these with\nAfrican-specific models such as Afri-mT5 (Ade-\nlani et al., 2022), AfriTeVa (Oladipo et al., 2023),\nand Cheetah (Adebara et al., 2024). (2) We also\nevaluate the performance of instruction-following\nLLMs on AfroLingu-MT in a zero-shot setting,\nemploying a prompt-based technique. We utilize\nLLMs that have been trained on general instruc-\ntions, such as LLaMA-2 (Touvron et al., 2023b),\nMistral (Jiang et al., 2023), and ALMA (Xu et al.,\n2023). Additionally, we assess LLMs that have\nbeen specifically trained on machine translation\ndata, including Bloomz-MT (Muennighoff et al.,\n2022) and mT0-XXL-MT (Muennighoff et al., 2022).\nTable 1 shows the comparison between these mod-\nels based on the African languages represented.\n5.2\nExperimental Setup\nWe finetune all models on AfroLingu-MT for 5\nepochs. For base and large architectures, we fine-\ntune the models using a learning rate of 5e-5, a\nbatch size of 8, and a maximum sequence length\nof 512 tokens. For XL model (3.7B parameters),\nwe use a learning rate 2e-5, a batch size of 2, and a\nmaximum sequence length of 256 tokens. During\nthe training process, we implement a linear learn-\ning rate scheduler, incorporating a warm-up phase\nthat accounts for 10% of the total training steps.\nIn all finetuning experiments, we rigorously se-\nlect the best-performing checkpoint for each model\nbased on performance in the respective develop-\nment set. We then report and analyze the perfor-\nmance of each model on the corresponding test\nset.\n5.3\nEvaluation Metrics\nIn this work, we present the performance outcomes\nof our proposed models as well as the baseline mod-\nels each evaluated independently on the AfroLingu-\nMT benchmark. This evaluation employs three\npertinent metrics specific to machine translation.\nThese metrics are: SentencePiece BLEU (i.e.,\nspBLEU) (Goyal et al., 2022), word-based Charac-\nter n-gram F-score (i.e., ChrF++) (Popovi´c, 2015b),\nand AfriCOMET (Wang et al., 2023). These metrics\nhave been selected for their effectiveness in assess-\ning the quality of machine translations from var-\nious perspectives, including lexical accuracy and\nfluency. We also introduce a wider coverage metric\nType Models\n#params\nDEV\nTEST\nspBLEU\nspBLEU1K\nChrF++ AfriCOMET\nspBLEU\nspBLEU1K\nChrF++ AfriCOMET\nZero-Shot Setting\nCLM\nALMA 7B MT⋆\n7B\n1.78\n2.15\n12.3\n25.10\n1.93\n2.24\n12.31\n24.69\nBloomz 7B MT⋆\n7B\n2.02\n2.63\n11.71\n23.89\n2.29\n2.85\n12.04\n23.67\nLlama-2 7B Chat\n7B\n1.87\n2.20\n13.42\n23.25\n1.79\n2.17\n13.29\n22.81\nMistral 7B Instruct v2\n7B\n2.00\n2.49\n14.79\n22.33\n1.89\n2.40\n14.95\n22.14\nALMA 13B MT⋆\n13B\n2.14\n2.52\n12.24\n27.16\n2.00\n2.33\n12.06\n26.89\nLlama-2 13B Chat\n13B\n1.07\n1.4\n9.07\n18.67\n0.94\n1.35\n9.09\n18.2\nS2S\nmT0 XXL MT⋆\n13B\n5.88\n7.48\n19.79\n33.82\n6.09\n7.67\n20.09\n33.71\nFinetuned Setting\nCLM Llama-2-7B-MT\n7B\n2.86\n3.39\n13.72\n24.67\n2.97\n3.54\n13.89\n24.17\nMistral-7B-MT\n7B\n2.04\n2.61\n12.57\n20.21\n1.95\n2.57\n12.66\n21.97\nS2S\nAfri-mT5 Base\n580M\n3.23\n3.43\n16.33\n29.69\n3.28\n3.47\n16.46\n30.01\nAfriTeVa Base\n229M\n0.00\n0.01\n8.12\n12.18\n0.00\n0.01\n8.04\n12.13\nAfriTeVa V2 Base\n428M\n3.61\n3.86\n16.94\n31.74\n3.96\n4.18\n17.45\n31.91\nmT0 Base\n580M\n10.96\n12.12\n28.55\n48.51\n11.66\n12.88\n29.48\n48.40\nmT5 Base\n580M\n11.54\n12.89\n28.89\n51.47\n11.93\n13.22\n29.46\n51.78\nToucan Base (ours)\n580M\n16.65\n17.6\n34.09\n60.42\n17.33\n18.2\n34.56\n60.21\nAfriTeVa Large\n745M\n3.35\n3.51\n17.05\n29.41\n3.31\n3.42\n17.14\n29.25\nAfriTeVa V2 Large\n1B\n6.06\n6.17\n22.86\n32.67\n6.24\n6.31\n23.23\n32.76\nmT0 Large\n1.2B\n12.03\n14.94\n30.93\n50.22\n12.10\n12.01\n30.2\n50.24\nmT5 Large\n1.2B\n13.28\n14.21\n30.21\n50.89\n13.33\n14.26\n30.34\n50.79\nToucan 1.2B (ours)\n1.2B\n18.30\n19.39\n35.89\n62.58\n18.78\n19.73\n36.41\n62.36\nmT0 XL\n3.7B\n14.56\n16.30\n35.54\n53.81\n14.21\n16.32\n34.34\n53.76\nmT5 XL\n3.7B\n15.56\n16.03\n35.16\n53.54\n15.45\n15.95\n35.16\n53.85\nToucan 3.7B (ours)\n3.7B\n22.11\n22.53\n38.91\n66.63\n22.67\n23.15\n39.53\n66.73\nTable 4: Performance on our AfroLingu-MT benchmark across both the zero-shot and full finetuning scenarios.\nFor all causal model, we use prompting to induce translations. For sequence-to-sequence models, we use target\nlanguage-based prefixes. We offer results both for development (i.e., DEV) and test (i.e., TEST) datasets. ⋆Notably,\nthese models are trained on multilingual MT datasets. Text highlighted in Bold Green indicates the highest scores\nacross all settings. Text highlighted in Bold Orange indicates a new evaluation metric (ours).\nModels\n#params\nDEV\nTEST\nspBLEU spBLEU1K ChrF++ AfriCOMET\nspBLEU spBLEU1K ChrF++ AfriCOMET\nNLLB-200-1.3B\n1.3B\n14.59\n14.74\n29.88\n53.06\n15.39\n15.40\n30.80\n53.16\nToucan 1.2B (ours)\n1.2B\n20.83\n21.93\n38.36\n62.69\n21.29\n22.37\n38.80\n62.15\nTable 5: A comparison between NLLB and Toucan on 59 language pairs both models support.\nmodeled after spBLEU, dubbed spBLEU1K, as we\nexplain next.\n5.3.1\nspBLEU1K\nEmploying the BLEU metric for evaluating trans-\nlations, particularly in the context of low-resource\nlanguages, is suboptimal due to its fundamental\nreliance on n-gram overlap. This reliance signif-\nicantly impacts the metric’s effectiveness, as it\nis heavily influenced by the specific tokenization\nmethod used. Notably, employing a more aggres-\nsive tokenization strategy can lead to artificially in-\nflated BLEU scores Goyal et al. (2022). To address\nthis, Goyal et al. (2022) proposed a novel metric,\nSentencePiece BLEU (i.e., spBLEU), designed to\nmeasure and analyze the performance of transla-\ntions across 101 languages. This approach involves\ntraining a new SentencePiece-based tokenizer us-\ning monolingual data for 101 languages, replacing\nthe default tokenizer typically used in SacreBLEU,\nknown as ‘mosestokenizer’ (Post, 2018). This inno-\nvation aims to standardize the tokenization process,\nthus providing more accurate and comparable trans-\nlation performance metrics across many languages.\nSignificantly, the spBLEU metric covers merely\n23 out of the 43 languages present in our\nAfroLingu-MT benchmark. To address this lim-\nitation, we adopt a methodology similar to that\nof Goyal et al. (2022). Namely, we develop a new\nSentencePiece tokenizer that utilizes 1000+ mono-\nlingual data sources.\nCategory\nspBLEU1K\nArabic ←→XX\n17.27\nXX −→Arabic\n16.77\nArabic −→XX\n17.78\nArabic −→(not supported)\nNA\nArabic −→XX (supported)\n17.78\nEnglish ←→XX\n25.16\nXX −→English\n21.48\nEnglish −→XX\n28.83\nEnglish −→XX (not supported)\n32.00\nEnglish −→XX (supported)\n25.22\nFrench ←→XX\n17.42\nXX −→French\n17.77\nFrench−→XX\n17.62\nFrench −→XX (not supported)\n12.54\nFrench −→XX (supported)\n23.07\nAfrican ←→African\n22.43\nFrench −→African (not supported)\n38.01\nFrench −→African (supported)\n18.69\nTotal supported languages\n26.57\nTotal unsupported languages\n22.36\nTable 6: The performance of our Toucan 3.7B model\nvaries based on language categories on TEST dataset.\nWe also compare the performance between the lan-\nguages in our benchmark that spBLEU supports -\n23 supported and not supported languages\nData. We collect monolingual data covering 1, 003\nlanguages, including 614 African languages, 53 In-\ndigenous American languages, and the remainder\nspanning the most resource-rich languages world-\nwide. We use a diverse data source, encompassing\nWikipedia, Wikibooks, the Bible, newspapers, and\ncommon web sources. Additionally, we utilize the\nMADLAD dataset (Kudugunta et al., 2023), which\ncovers 419 languages. A list with the 1, 003 we\ncover is available at Toucan.\nTraining a SentencePiece Model (SPM). One\nsignificant challenge is the uneven availability of\nmonolingual data across various languages. This\ndisparity is especially acute for low-resource lan-\nguages, which often suffer from a lack of compre-\nhensive coverage in subword units and may not pos-\nsess a sufficiently large corpus of sentences to en-\nsure a broad and diverse representation of content.\nTo address this issue and enhance the training of\nour new SPM, we adopt a temperature upsampling\ntechnique similar to the methodology described in\nConneau et al. (2019).\nIntegrating with SacreBleu. We integrate this\nnewly created SPM into SacreBLEU, resulting\nin the formulation of our more inclusive metric\nspBLEU1K. Our metric is thus designed to provide\na more comprehensive evaluation across a broader\nrange of languages, including those that are under-\nrepresented in existing metrics such as spBLEU.\n6\nResults and Discussion\nspBLEU1K Metric. The results indicate that our\nnew metric, spBLEU1K, enhances the translation\nscores by an average of 0.74 and 0.62 BLEU points\non the development and test datasets, respectively.\nWe also evaluate the performance of our best model,\nToucan-3.7B, based on language categories. Ta-\nble 6 presents a comparison between spBLEU and\nour new evaluation metric across different cate-\ngories of translation directions in our data. Results\ndemonstrate that the two metrics are almost iden-\ntical in the shared languages (the languages that\nboth metrics supported), however, the new metric\nimproves the translation results in languages not\nsupported by spBLEU.\nExploring different pretraining settings allows\nus to derive unique insights. Examples of insights\nthat can be gleaned from Table 4 include:\nSequence-to-sequence\nmodels\nwith\nwider\ncoverage perform better MT. Unsurprisingly, the\nfindings show that sequence-to-sequence models\npretrained with more languages enable better\nMT performance on our dataset. For example,\nToucan, which supports 517 African languages\nand ten of the most spoken languages worldwide,\noutperforms all other models.\nOn the other\nhand, AfriTeVa (which is pretrained on only ten\nlanguages) has the lowest performance. However,\nAfriTeVa-v2 outperforms Afri-MT5 even though it\nsupports fewer languages. We assume that the size\nof the LM may plays a role here.\nModels of the same size finetuned on more lan-\nguage pairs tend to perform better translations.\nAmong the models we evaluate in zero-shot, four\nare already finetuned on external MT parallel\ndata. These are ALMA-7B-MT, Bloomz-7B-MT,\nALMA-13B-MT,\nand\nMT0-XXL-MT\n(13B).\nWhile we do not see a clear pattern for the 7B\nmodel size, our results for the 13B do carry a\npattern: Results show that the MT0-XXL-MT\nmodel (13B), which is finetuned on pairs from 46\nlanguages gives significantly better translations\nthan ALMA-13B-MT, which is finetuned on pairs\nfrom six languages (i.e., 5.34 points in spBLEU1k\non TEST).\nLarger models perform better. Again, unsurpris-\ningly, we observe that larger models outperform\nLang pair\nAya 13B\nToucan 3.7B (ours)\nLang pair\nAya 13B\nToucan 3.7B (ours)\nspBLEU\nChrF++\nspBLEU\nChrF++\nspBLEU\nChrF++\nspBLEU\nChrF++\nafr→eng\n31.1\n55.2\n43.83\n64.63\neng→yor\n4.8\n19.6\n5.47\n23.81\namh→eng\n19.2\n44.6\n16.50\n40.10\neng→zul\n11.4\n39.7\n6.54\n35.42\neng→afr\n27.8\n51.8\n29.77\n54.61\nhau→eng\n19.3\n42.7\n24.66\n47.50\neng→amh\n11.9\n23.9\n3.14\n19.74\nibo→eng\n16.7\n40.3\n19.76\n42.28\neng→hau\n11.0\n38.4\n18.24\n43.13\nnso→eng\n17.3\n40.5\n26.88\n48.88\neng→ibo\n10.4\n32.3\n9.93\n31.27\nsna→eng\n16.6\n39.4\n18.72\n41.03\neng→nso\n6.1\n29.5\n2.53\n13.58\nsom→eng\n16.8\n40.3\n20.65\n43.98\neng→sna\n5.6\n33.2\n0.30\n4.80\nsot→eng\n20.7\n44.2\n26.72\n48.80\neng→som\n7.3\n35.0\n8.75\n36.28\nswa→eng\n23.0\n47.4\n32.47\n56.20\neng→sot\n16.3\n42.4\n14.63\n36.93\nxho→eng\n20.5\n43.7\n24.99\n47.42\neng→swa\n19.5\n46.7\n24.85\n51.16\nyor→eng\n11.1\n34.6\n14.21\n36.12\neng→xho\n8.5\n36.3\n6.08\n33.80\nzul→eng\n20.5\n44.4\n26.17\n48.69\nTable 7: Comparing Aya 13.B with Toucan on their intersection of pairs. Toucan outperforms Aya on 16 of 28 pairs.\nComparison is done with Flores devtest splits.\nsmaller ones. For example, models with 1.2B pa-\nrameters outperform base models by approximately\ntwo points spBLEU1k, whereas the larger models\nwith 3.7B outperform both the base and 1.2B pa-\nrameter models by approximately five and three\npoints spBLEU1k, respectively. Additionally, we\nnote that our model, Toucan, outperforms the sec-\nond best performing model (i.e., mT5) base, 1.2B,\nand 3.7B models by 4.98, 7.47, and 7.2 points\nspBLEU1k on TEST, respectively.\nAdditionally, we compare our model, Toucan-\n1.2B, to the Facebook’s NLLB model (Team et al.,\n2022; Costa-jussà et al., 2022). Again, we find\nToucan-1.2B outperforming NLLB-200-1.3B by\n6.96 points in spBLEU1K, as shown in Table 5.\nToucan outperforms Aya We compare the perfor-\nmance of Toucan with Aya (Üstün et al., 2024).\nWe use results for Aya as they appear in the paper,\nhence, we do not compute spBLEU1k results in this\nanalysis. Although Aya is a 13B parameter model,\nsignificantly larger than Toucan 3.7B, we report\nbetter performance in 16 of 28 pairs. In Table 7,\nwe show the performance of Toucan and Aya on\nFlores200.\n7\nConclusion\nIn this paper, we introduce a suite of resources\naimed at enhancing MT for low-resource African\nlanguages.\nWe present Cheetah-1.2B and\nCheetah-3.7B, with 1.2B and 3.7B parameters,\nrespectively. We further finetune these models into\nversatile many-to-many model capable of trans-\nlating between 46 different languages, including\n43 African languages and the three major non-\nIndigenous languages in Africa: Arabic, English,\nand French. We also introduce AfroLingu-MT as\nthe largest African MT benchmark to date. We pro-\nvide a comprehensive comparison between various\nLLMs by evaluating a wide range of models on\nour AfroLingu-MT benchmark. Finally we extend\nspBLEU - spBLEU1K - a sentencepiece-based eval-\nuation metric covering 1, 003 languages, including\n614 African languages. This aims to enhance trans-\nlation evaluation quality, particularly for languages\nhistorically underrepresented in translation models\nand benchmarks.\n8\nLimitations\nWe can identify a number of limitations that are\nrelevant to our work, as follows:\n• Even though we cover the largest number of\nAfrican languages in our MT models, com-\npared to previous research, there is still a long\nlist of African languages that lack MT support.\nIt will take the community more work to de-\nvelop new parallel datasets for these languages\nso that they can be supported. We believe that\nour new Cheetah models can be valuable in\nthis regards, since they have a coverage of 517\nlanguages and can be easily finetuned on new\nlanguages once parallel datasets are available.\n• Another limitation is that metrics such as\nAfriCOMET can only cover 17 African\nlanguages,\nalthough\nwe\ndeveloped\nthe\nspBLEU1K metric that allowed us to evalu-\nate on our dataset in a dependable way since it\nis based on wide coverage vocabulary. Again,\nnew parallel datasets can be helpful for ex-\ntending COMET to more African languages.\n9\nEthics Statement and Wider Impacts\nOur model, Toucan, is rooted in Afrocentric NLP\nprinciples, prioritizing the technological needs of\nAfrican communities.\nWe anticipate that Tou-\ncan will not only benefit speakers of the supported\nlanguages but also aid researchers in African lan-\nguages, including anthropologists and linguists. Be-\nlow, we highlight some potential use cases for Tou-\ncan and discuss its broader impacts:\nAddressing the technological disparity in approx-\nimately 90% of the world’s languages, which dis-\nproportionately affects native speakers, Toucan fo-\ncuses specifically on Africa. As the first massively\nmultilingual pre-trained language model (PLM) de-\nveloped for African languages and their variants, it\nencompasses knowledge of 517 African languages,\nmaking it the largest model for African NLP to\ndate.\nToucan facilitates enhanced access to critical\ninformation for the African community through\nIndigenous African languages. Particularly ben-\neficial for individuals with limited proficiency in\nother languages, this capability has the potential to\nfoster greater global connectivity.\nToucan supports language preservation efforts\nfor numerous African languages, many of which\nhave not been utilized in NLP tasks before. We\nbelieve it can encourage the continued use of these\nlanguages across various domains and spur the\ndevelopment of language technologies tailored to\ntheir specific needs.\nDespite their versatility, language models like\nToucan can be susceptible to misuse. Developed\nusing publicly available datasets that may contain\nbiases, we strive to conduct analyses and diagnos-\ntic case studies to assess our model’s performance.\nHowever, our investigations are not exhaustive and\ndo not guarantee the absence of bias in the data, es-\npecially considering limited access to native speak-\ners of most covered languages.\nIn summary, Toucan represents a significant step\nforward in Afrocentric NLP, addressing techno-\nlogical disparities, fostering language preservation,\nand promoting responsible deployment of language\ntechnologies in African contexts.\nReferences\n2020. Parallel text dataset for Neural Machine Transla-\ntion (French -> Fongbe, French -> Ewe). Zenodo.\n2021. ENGLISH-AKUAPEM TWI PARALLEL COR-\nPUS. Zenodo.\n2021. An English-Luganda parallel corpus. Zenodo.\n2023. The South African Gov-ZA multilingual corpus.\nZenodo.\nGilles Adda, Sebastian Stüker, Martine Adda-Decker,\nOdette Ambouroue, Laurent Besacier, David Bla-\nchon, Hélene Bonneau-Maynard, Pierre Godard, Fa-\ntima Hamlaoui, Dmitry Idiatov, et al. 2016. Break-\ning the unwritten language barrier: The bulb project.\nProcedia Computer Science, 81:8–14.\nIfe Adebara and Muhammad Abdul-Mageed. 2022. To-\nwards afrocentric NLP for African languages: Where\nwe are and where we can go. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n3814–3841, Dublin, Ireland. Association for Compu-\ntational Linguistics.\nIfe Adebara, AbdelRahim Elmadany, and Muhammad\nAbdul-Mageed. 2024. Cheetah: Natural language\ngeneration for 517 african languages.\nDavid Adelani, Jesujoba Alabi, Angela Fan, Julia\nKreutzer, Xiaoyu Shen, Machel Reid, Dana Ruiter,\nDietrich Klakow, Peter Nabende, Ernie Chang, Tajud-\ndeen Gwadabe, Freshia Sackey, Bonaventure F. P.\nDossou, Chris Emezue, Colin Leong, Michael Beuk-\nman, Shamsuddeen Muhammad, Guyo Jarso, Oreen\nYousuf, Andre Niyongabo Rubungo, Gilles Hacheme,\nEric Peter Wairagala, Muhammad Umair Nasir, Ben-\njamin Ajibade, Tunde Ajayi, Yvonne Gitau, Jade\nAbbott, Mohamed Ahmed, Millicent Ochieng, An-\nuoluwapo Aremu, Perez Ogayo, Jonathan Mukiibi,\nFatoumata Ouoba Kabore, Godson Kalipe, Derguene\nMbaye, Allahsera Auguste Tapo, Victoire Memd-\njokam Koagne, Edwin Munkoh-Buabeng, Valen-\ncia Wagner, Idris Abdulmumin, Ayodele Awokoya,\nHappy Buzaaba, Blessing Sibanda, Andiswa Bukula,\nand Sam Manthalu. 2022. A few thousand transla-\ntions go a long way! leveraging pre-trained mod-\nels for African news translation. In Proceedings of\nthe 2022 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 3053–3070,\nSeattle, United States. Association for Computational\nLinguistics.\nDavid Adelani, Dana Ruiter, Jesujoba Alabi, Damilola\nAdebonojo, Adesina Ayeni, Mofe Adeyemi, Ayo-\ndele Esther Awokoya, and Cristina España-Bonet.\n2021. The effect of domain and diacritics in Yoruba–\nEnglish neural machine translation. In Proceedings\nof the 18th Biennial Machine Translation Summit\n(Volume 1: Research Track), pages 61–75, Virtual.\nAssociation for Machine Translation in the Americas.\nAbhaya Agarwal and Alon Lavie. 2008. Meteor, M-\nBLEU and M-TER: Evaluation metrics for high-\ncorrelation with human rankings of machine transla-\ntion output. In Proceedings of the Third Workshop\non Statistical Machine Translation, pages 115–118,\nColumbus, Ohio. Association for Computational Lin-\nguistics.\nŽeljko Agi´c and Ivan Vuli´c. 2019. JW300: A wide-\ncoverage parallel corpus for low-resource languages.\nIn Proceedings of the 57th Annual Meeting of the As-\nsociation for Computational Linguistics, pages 3204–\n3210, Florence, Italy. Association for Computational\nLinguistics.\nBenjamin Akera, Jonathan Mukiibi, Lydia Sanyu Nag-\ngayi, Claire Babirye, Isaac Owomugisha, Solomon\nNsumba, Joyce Nakatumba-Nabende, Engineer Bain-\nomugisha, Ernest Mwebaze, and John Quinn. 2022.\nMachine translation for african languages: Commu-\nnity creation of datasets and models in uganda.\nJesujoba Alabi, Kwabena Amponsah-Kaakyire, David\nAdelani, and Cristina Espana-Bonet. 2020. Massive\nvs. curated embeddings for low-resourced languages:\nThe case of Yorùbá and Twi. In Proceedings of The\n12th Language Resources and Evaluation Confer-\nence, pages 2754–2762.\nMoussa Bamba. 2016. Bamanankan lexicon.\nSatanjeev Banerjee and Alon Lavie. 2005. METEOR:\nAn automatic metric for MT evaluation with im-\nproved correlation with human judgments. In Pro-\nceedings of the ACL Workshop on Intrinsic and Ex-\ntrinsic Evaluation Measures for Machine Transla-\ntion and/or Summarization, pages 65–72, Ann Arbor,\nMichigan. Association for Computational Linguis-\ntics.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In\nProceedings of the 34th International Conference on\nNeural Information Processing Systems, NIPS’20,\nRed Hook, NY, USA. Curran Associates Inc.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2019. Unsupervised\ncross-lingual representation learning at scale. arXiv\npreprint arXiv:1911.02116.\nMarta R Costa-jussà, James Cross, Onur Çelebi, Maha\nElbayad, Kenneth Heafield, Kevin Heffernan, Elahe\nKalbassi, Janice Lam, Daniel Licht, Jean Maillard,\net al. 2022.\nNo language left behind: Scaling\nhuman-centered machine translation. arXiv preprint\narXiv:2207.04672.\nAngela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi\nMa, Ahmed El-Kishky, Siddharth Goyal, Man-\ndeep Baines, Onur Celebi, Guillaume Wenzek,\nVishrav Chaudhary, Naman Goyal, Tom Birch, Vi-\ntaliy Liptchinsky, Sergey Edunov, Edouard Grave,\nMichael Auli, and Armand Joulin. 2020. Beyond\nenglish-centric multilingual machine translation.\nNaman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-\nJen Chen, Guillaume Wenzek, Da Ju, Sanjana Kr-\nishnan, Marc’Aurelio Ranzato, Francisco Guzmán,\nand Angela Fan. 2021. The flores-101 evaluation\nbenchmark for low-resource and multilingual ma-\nchine translation.\nNaman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-\nJen Chen, Guillaume Wenzek, Da Ju, Sanjana Kr-\nishnan, Marc’Aurelio Ranzato, Francisco Guzmán,\nand Angela Fan. 2022. The flores-101 evaluation\nbenchmark for low-resource and multilingual ma-\nchine translation. Transactions of the Association for\nComputational Linguistics, 10:522–538.\nNizar Habash, Nasser Zalmout, Dima Taji, Hieu Hoang,\nand Maverick Alzate. 2017. A parallel corpus for\nevaluating machine translation between Arabic and\nEuropean languages. In Proceedings of the 15th Con-\nference of the European Chapter of the Association\nfor Computational Linguistics: Volume 2, Short Pa-\npers, pages 235–241, Valencia, Spain. Association\nfor Computational Linguistics.\nAlbert Q Jiang, Alexandre Sablayrolles, Arthur Men-\nsch, Chris Bamford, Devendra Singh Chaplot, Diego\nde las Casas, Florian Bressand, Gianna Lengyel, Guil-\nlaume Lample, Lucile Saulnier, et al. 2023. Mistral\n7b. arXiv preprint arXiv:2310.06825.\nOdunayo Jude Ogundepo, Akintunde Oladipo, Mofe-\ntoluwa Adeyemi, Kelechi Ogueji, and Jimmy Lin.\n2022. AfriTeVA: Extending ?small data? pretrain-\ning approaches to sequence-to-sequence models. In\nProceedings of the Third Workshop on Deep Learn-\ning for Low-Resource Natural Language Processing,\npages 126–135, Hybrid. Association for Computa-\ntional Linguistics.\nPhilipp Koehn and Rebecca Knowles. 2017. Six chal-\nlenges for neural machine translation. In Proceedings\nof the First Workshop on Neural Machine Translation,\npages 28–39, Vancouver. Association for Computa-\ntional Linguistics.\nJulia Kreutzer, Isaac Caswell, Lisa Wang, Ahsan Wahab,\nDaan van Esch, Nasanbayar Ulzii-Orshikh, Allah-\nsera Tapo, Nishant Subramani, Artem Sokolov, Clay-\ntone Sikasote, Monang Setyawan, Supheakmungkol\nSarin, Sokhar Samb, Benoît Sagot, Clara Rivera, An-\nnette Rios, Isabel Papadimitriou, Salomey Osei, Pe-\ndro Ortiz Suárez, Iroro Orife, Kelechi Ogueji, An-\ndre Niyongabo Rubungo, Toan Q. Nguyen, Math-\nias Müller, André Müller, Shamsuddeen Hassan\nMuhammad, Nanda Muhammad, Ayanda Mnyak-\neni, Jamshidbek Mirzakhalov, Tapiwanashe Matan-\ngira, Colin Leong, Nze Lawson, Sneha Kudugunta,\nYacine Jernite, Mathias Jenny, Orhan Firat, Bonaven-\nture F. P. Dossou, Sakhile Dlamini, Nisansa de Silva,\nSakine Çabuk Ballı, Stella Biderman, Alessia Bat-\ntisti, Ahmed Baruwa, Ankur Bapna, Pallavi Baljekar,\nIsrael Abebe Azime, Ayodele Awokoya, Duygu Ata-\nman, Orevaoghene Ahia, Oghenefego Ahia, Sweta\nAgrawal, and Mofetoluwa Adeyemi. 2021. Quality\nat a glance: An audit of web-crawled multilingual\ndatasets. arXiv preprint arXiv:2103.12028.\nTaku Kudo and John Richardson. 2018. SentencePiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 66–71, Brussels, Belgium.\nAssociation for Computational Linguistics.\nSneha Kudugunta, Isaac Caswell, Biao Zhang, Xavier\nGarcia, Christopher A. Choquette-Choo, Katherine\nLee, Derrick Xin, Aditya Kusupati, Romi Stella,\nAnkur Bapna, and Orhan Firat. 2023. Madlad-400:\nA multilingual and document-level large audited\ndataset.\nRichard Lastrucci, Isheanesu Dzingirai, Jenalea Rajab,\nAndani Madodonga, Matimba Shingange, Daniel\nNjini, and Vukosi Marivate. 2023.\nPreparing\nthe vuk’uzenzele and ZA-gov-multilingual South\nAfrican multilingual corpora.\nIn Proceedings of\nthe Fourth workshop on Resources for African In-\ndigenous Languages (RAIL 2023), pages 18–25,\nDubrovnik, Croatia. Association for Computational\nLinguistics.\nShudong Liu, Xuebo Liu, Derek F. Wong, Zhaocong\nLi, Wenxiang Jiao, Lidia S. Chao, and Min Zhang.\n2023. kNN-TL: k-nearest-neighbor transfer learn-\ning for low-resource neural machine translation. In\nProceedings of the 61st Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 1878–1891, Toronto, Canada.\nAssociation for Computational Linguistics.\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey\nEdunov, Marjan Ghazvininejad, Mike Lewis, and\nLuke Zettlemoyer. 2020. Multilingual denoising pre-\ntraining for neural machine translation.\nRooweither Mabuya, Jade Abbott, and Vukosi Marivate.\n2021. Umsuka english - isizulu parallel corpus.\nArya D. McCarthy, Rachel Wicks, Dylan Lewis, Aaron\nMueller, Winston Wu, Oliver Adams, Garrett Nicolai,\nMatt Post, and David Yarowsky. 2020. The Johns\nHopkins University Bible corpus: 1600+ tongues\nfor typological exploration. In Proceedings of the\nTwelfth Language Resources and Evaluation Confer-\nence, pages 2884–2892, Marseille, France. European\nLanguage Resources Association.\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika,\nAdam Roberts, Stella Biderman, Teven Le Scao,\nM Saiful Bari, Sheng Shen, Zheng Xin Yong, Hai-\nley Schoelkopf, Xiangru Tang, Dragomir Radev,\nAlham Fikri Aji, Khalid Almubarak, Samuel Al-\nbanie, Zaid Alyafeai, Albert Webson, Edward Raff,\nand Colin Raffel. 2023.\nCrosslingual generaliza-\ntion through multitask finetuning. In Proceedings\nof the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 15991–16111, Toronto, Canada. Association\nfor Computational Linguistics.\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika,\nAdam Roberts, Stella Biderman, Teven Le Scao,\nM Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey\nSchoelkopf, et al. 2022. Crosslingual generaliza-\ntion through multitask finetuning. arXiv preprint\narXiv:2211.01786.\nEl Moatez Billah Nagoudi, AbdelRahim Elmadany, and\nMuhammad Abdul-Mageed. 2022. TURJUMAN: A\npublic toolkit for neural Arabic machine translation.\nIn Proceedinsg of the 5th Workshop on Open-Source\nArabic Corpora and Processing Tools with Shared\nTasks on Qur’an QA and Fine-Grained Hate Speech\nDetection, pages 1–11, Marseille, France. European\nLanguage Resources Association.\nAkintunde Oladipo,\nMofetoluwa Adeyemi,\nOre-\nvaoghene Ahia, Abraham Owodunni, Odunayo Ogun-\ndepo, David Adelani, and Jimmy Lin. 2023. Better\nquality pre-training data and t5 models for African\nlanguages. In Proceedings of the 2023 Conference on\nEmpirical Methods in Natural Language Processing,\npages 158–168, Singapore. Association for Compu-\ntational Linguistics.\nOpenAI. 2023. Gpt-4 technical report.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 311–318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long Papers), pages 2227–2237,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nMaja Popovi´c. 2015a. chrF: character n-gram F-score\nfor automatic MT evaluation. In Proceedings of the\nTenth Workshop on Statistical Machine Translation,\npages 392–395, Lisbon, Portugal. Association for\nComputational Linguistics.\nMaja Popovi´c. 2015b. chrf: character n-gram f-score for\nautomatic mt evaluation. In Proceedings of the Tenth\nWorkshop on Statistical Machine Translation, pages\n392–395. Association for Computational Linguistics.\nMatt Post. 2018. A call for clarity in reporting BLEU\nscores. In Proceedings of the Third Conference on\nMachine Translation: Research Papers, pages 186–\n191, Belgium, Brussels. Association for Computa-\ntional Linguistics.\nRicardo Rei, Craig Stewart, Ana C Farinha, and Alon\nLavie. 2020. COMET: A neural framework for MT\nevaluation. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP), pages 2685–2702, Online. Association\nfor Computational Linguistics.\nNils Reimers and Iryna Gurevych. 2020.\nMaking\nmonolingual sentence embeddings multilingual us-\ning knowledge distillation. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing. Association for Computational\nLinguistics.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 1715–1725,\nBerlin, Germany. Association for Computational Lin-\nguistics.\nCraig Stewart, Ricardo Rei, Catarina Farinha, and Alon\nLavie. 2020. COMET - deploying a new state-of-\nthe-art MT evaluation metric in production. In Pro-\nceedings of the 14th Conference of the Association\nfor Machine Translation in the Americas (Volume 2:\nUser Track), pages 78–109, Virtual. Association for\nMachine Translation in the Americas.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\nAn instruction-following llama model.\nhttps://\ngithub.com/tatsu-lab/stanford_alpaca.\nNLLB Team, Marta R. Costa-jussà, James Cross, Onur\nÇelebi, Maha Elbayad, Kenneth Heafield, Kevin Hef-\nfernan, Elahe Kalbassi, Janice Lam, Daniel Licht,\nJean Maillard, Anna Sun, Skyler Wang, Guillaume\nWenzek, Al Youngblood, Bapi Akula, Loic Bar-\nrault, Gabriel Mejia Gonzalez, Prangthip Hansanti,\nJohn Hoffman, Semarley Jarrett, Kaushik Ram\nSadagopan, Dirk Rowe, Shannon Spruit, Chau\nTran, Pierre Andrews, Necip Fazil Ayan, Shruti\nBhosale, Sergey Edunov, Angela Fan, Cynthia\nGao, Vedanuj Goswami, Francisco Guzmán, Philipp\nKoehn, Alexandre Mourachko, Christophe Ropers,\nSafiyyah Saleem, Holger Schwenk, and Jeff Wang.\n2022.\nNo language left behind: Scaling human-\ncentered machine translation.\nSolomon Teferra Abate, Michael Melese, Martha Yi-\nfiru Tachbelie, Million Meshesha, Solomon Ati-\nnafu, Wondwossen Mulugeta, Yaregal Assabie, Hafte\nAbera, Binyam Ephrem, Tewodros Abebe, Wondim-\nagegnhue Tsegaye, Amanuel Lemma, Tsegaye An-\ndargie, and Seifedin Shifaw. 2018. Parallel corpora\nfor bi-directional statistical machine translation for\nseven Ethiopian language pairs. In Proceedings of\nthe First Workshop on Linguistic Resources for Nat-\nural Language Processing, pages 83–90, Santa Fe,\nNew Mexico, USA. Association for Computational\nLinguistics.\nJörg Tiedemann. 2012. Parallel data, tools and inter-\nfaces in OPUS. In Proceedings of the Eighth In-\nternational Conference on Language Resources and\nEvaluation (LREC’12), pages 2214–2218, Istanbul,\nTurkey. European Language Resources Association\n(ELRA).\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023a. Llama: Open\nand efficient foundation language models.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023b.\nLlama 2: Open founda-\ntion and fine-tuned chat models.\narXiv preprint\narXiv:2307.09288.\nJiayi Wang, David Ifeoluwa Adelani, Sweta Agrawal,\nRicardo Rei, Eleftheria Briakou, Marine Carpuat,\nMarek Masiak, Xuanli He, Sofia Bourhim, An-\ndiswa Bukula, Muhidin Mohamed, Temitayo Ola-\ntoye, Hamam Mokayede, Christine Mwase, Wan-\ngui Kimotho, Foutse Yuehgoh, Anuoluwapo Aremu,\nJessica Ojo, Shamsuddeen Hassan Muhammad, Sa-\nlomey Osei, Abdul-Hakeem Omotayo, Chiamaka\nChukwuneke, Perez Ogayo, Oumaima Hourrane,\nSalma El Anigri, Lolwethu Ndolela, Thabiso Mang-\nwana, Shafie Abdi Mohamed, Ayinde Hassan,\nOluwabusayo Olufunke Awoyomi, Lama Alkhaled,\nSana Al-Azzawi, Naome A. Etori, Millicent Ochieng,\nClemencia Siro, Samuel Njoroge, Eric Muchiri, Wan-\ngari Kimotho, Lyse Naomi Wamba Momo, Daud\nAbolade, Simbiat Ajao, Tosin Adewumi, Iyanuoluwa\nShode, Ricky Macharm, Ruqayya Nasir Iro, Sa-\nheed S. Abdullahi, Stephen E. Moore, Bernard\nOpoku, Zainab Akinjobi, Abeeb Afolabi, Nnaemeka\nObiefuna, Onyekachi Raphael Ogbu, Sam Brian,\nVerrah Akinyi Otiende, Chinedu Emmanuel Mbonu,\nSakayo Toadoum Sari, and Pontus Stenetorp. 2023.\nAfrimte and africomet: Empowering comet to em-\nbrace under-resourced african languages.\nHaoran Xu, Young Jin Kim, Amr Sharaf, and Hany Has-\nsan Awadalla. 2023. A paradigm shift in machine\ntranslation: Boosting translation performance of\nlarge language models.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale,\nRami Al-Rfou, Aditya Siddhant, Aditya Barua, and\nColin Raffel. 2020. mt5: A massively multilingual\npre-trained text-to-text transformer. arXiv preprint\narXiv:2010.11934.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale,\nRami Al-Rfou, Aditya Siddhant, Aditya Barua, and\nColin Raffel. 2021a. mt5: A massively multilingual\npre-trained text-to-text transformer.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale,\nRami Al-Rfou, Aditya Siddhant, Aditya Barua, and\nColin Raffel. 2021b. mT5: A massively multilingual\npre-trained text-to-text transformer. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 483–498, On-\nline. Association for Computational Linguistics.\nBarret Zoph, Deniz Yuret, Jonathan May, and Kevin\nKnight. 2016. Transfer learning for low-resource neu-\nral machine translation. In Proceedings of the 2016\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 1568–1575, Austin, Texas.\nAssociation for Computational Linguistics.\nAlp Öktem, Eric DeLuca, Rodrigue Bashizi, Eric\nPaquin, and Grace Tang. 2021. Congolese swahili\nmachine translation for humanitarian response.\nAhmet Üstün, Viraat Aryabumi, Zheng-Xin Yong, Wei-\nYin Ko, Daniel D’souza, Gbemileke Onilude, Neel\nBhandari, Shivalika Singh, Hui-Lee Ooi, Amr Kayid,\nFreddie Vargus, Phil Blunsom, Shayne Longpre,\nNiklas Muennighoff, Marzieh Fadaee, Julia Kreutzer,\nand Sara Hooker. 2024. Aya model: An instruction\nfinetuned open-access multilingual language model.\narXiv preprint arXiv:2402.07827.\nAppendices\nA\nLiterature Review\nThe recent years have witnessed a substantial\nevolution in LLMs, marked by numerous break-\nthroughs across various applications such as MT\nand text summarization. Particularly in MT, sig-\nnificant advancements have occurred, with a no-\ntable emphasis on supporting underrepresented lan-\nguages, including those spoken in Africa (Team\net al., 2022; Jude Ogundepo et al., 2022; Adelani\net al., 2022). The development of MT tools tai-\nlored for African languages holds significant im-\nportance in promoting linguistic diversity, facili-\ntating cross-cultural communication, and driving\nsocio-economic progress within the region (Ade-\nbara et al., 2024).\nIn this section, we explore the current landscape\nof LLMs supporting African languages, covering\ntopics such as datasets and benchmarks, instruction\nfine-tuning, evaluation metrics, challenges, and\npractical applications. It examines key contribu-\ntions and advancements within this dynamic field,\nproviding insights into its ongoing development\nand potential impact.\nDatasets and Benchmarks One of the primary\nhindrances to developing MT models for African\nlanguages is the scarcity of data (Adda et al., 2016;\nAdebara and Abdul-Mageed, 2022). For many high\nresource languages, collecting data from the web\noften yields large and high quality datasets. How-\never, resulting corpora for many African languages\nusing similar methods are often limited in size and\nquality (Kreutzer et al., 2021; Alabi et al., 2020).\nFurthermore, texts from religious domains domi-\nnate the data landscape with most datasets com-\ning from bibles (McCarthy et al., 2020) and other\nreligious documents (Agi´c and Vuli´c, 2019). To\naddress these issues, a handful of benchmarks for\nboth training and evaluation have been developed.\nNotable among them are FLORES-101 (Goyal\net al., 2021), FLORES-200 (Goyal et al., 2022),\nMenyo-20 (Adelani et al., 2021), Lafand-MT (Ade-\nlani et al., 2022), and Salt (Akera et al., 2022).\nModels and Tools A few African languages have\nbenefited from the recent advancement of LM. We\nnow describe models and tools that support African\nlanguages. (1) No Language Left Behind (NLLB)\nis a suite of open-sources models capable of deliv-\nering evaluated, high-quality translations directly\nbetween 200 languages including 23 African lan-\nguages (Team et al., 2022). (2) M2M-100 supports\n100 languages including 17 African languages (Fan\net al., 2020). (3) AfriTeVa (Jude Ogundepo et al.,\n2022) supports ten African languages using a T5-\nstyle model. (4) AfriTeVa V2 (Oladipo et al., 2023)\nsupports 16 African languages using a better qual-\nity pre-training data. (5) AfroMT5 (Adelani et al.,\n2022) and AfriMBART (Adelani et al., 2022) each\nsupport 17 African languages. (6) T5 (Text-To-\nText Transfer Transformer) based models such as\nmT5 (Xue et al., 2021b), mT0 and (Muennighoff\net al., 2022). (7) Cheetah (Adebara et al., 2024),\nsupports 517 African languages and 10 widely spo-\nken languages in the world.\nDecoder-only models have also shown remark-\nable improvements across multiple tasks. Now, we\ndescribe some of these modes. (1) Generative Pre-\ntrained Transformer (GPT) (OpenAI, 2023; Brown\net al., 2020), a transformer architecture model, pre-\ntrained on expansive datasets, resulting in its pro-\nficiency in generating coherent, contextually rich\ntext. (2) Llama (Touvron et al., 2023a,b) represents\na state-of-the-art language model designed for nu-\nanced natural language understanding and genera-\ntion. LLama excels in tasks ranging from compre-\nhensive text comprehension to intricate language\ntranslation and sophisticated content creation. Its\nversatility positions it as a pivotal tool within the\nrealm of artificial intelligence, particularly in con-\ntexts requiring nuanced linguistic analysis. These\nmodels exhibit prowess in various facets of natural\nlanguage processing, treating each task as a text-\nto-text challenge. Its capabilities span translation,\nsummarization, and question answering, showcas-\ning versatility and effectiveness.\nEvaluation Metrics MT evaluation metrics are\ntools used to assess the quality of translations gener-\nated by MT models/systems. These metrics provide\nobjective, quantifiable measures of how accurately\nand fluently a machine-translated text matches a ref-\nerence translation, typically produced by humans.\nAmong the most widely used BLEU (Papineni\net al., 2002), ChrF (Popovi´c, 2015a) evaluate the\ncorrespondence of n-grams between the machine-\ngenerated text and the reference, favoring preci-\nsion. METEOR (Banerjee and Lavie, 2005), on the\nother hand, emphasizes both precision and recall,\nincorporating synonyms, stemming, and word or-\nder into its evaluation. TER (Agarwal and Lavie,\n2008) measures the number of edits required to\nchange a machine-translated text into a reference\ntext. COMET (Rei et al., 2020; Stewart et al., 2020)\nleverage contextual embedding to capture semantic\nsimilarities more effectively. These metrics facili-\ntate the benchmarking of MT systems, guiding re-\nsearchers and developers in improving translation\ntechnologies to achieve higher quality and more\nnatural translations.\nRecently, researchers have made strides in en-\nhancing the capabilities of evaluation metrics like\nBLEU and COMET to better accommodate low-\nresource languages. Goyal et al. (2022) introduced\na novel metric, SentencePiece BLEU (spBLEU),\nwhich extends coverage to 101 languages, includ-\ning 23 African languages. This development marks\na significant step forward in making language tech-\nnology more inclusive, especially for languages\nthat have traditionally been underrepresented in\nmachine translation research.\nSimilarly, Wang\net al. (2023) developed AfriCOMET, a COMET-\nbased evaluation metric, specifically designed to\nsupport 17 African languages. AfriCOMET rep-\nresents another leap towards recognizing and ad-\ndressing the unique challenges posed by African\nlanguages in machine translation. Both spBLEU\nand AfriCOMET exemplify the ongoing efforts\nwithin the research community to develop more\nrobust and inclusive tools for evaluating machine\ntranslation quality across a broader spectrum of the\nworld’s languages.\nB\nCheetah Models Pretraining Details\nVocabulary. We employ SentencePiece (Kudo\nand Richardson, 2018) to encode text into Word-\nPiece tokens (Sennrich et al., 2016), utilizing 250K\nWordPieces.\nAdditionally, our dataset encom-\npasses the top ten globally spoken languages: Ara-\nbic, English, French, German, Greek, Italian, Por-\ntuguese, Russian, Spanish, and Turkish, sourced\nfrom Wikipedia dumps. Each language comprises\n1 million sentences, solely included in the vocabu-\nlary.\nModels Architecture. We pretrain AfroLingu-\nMTusing the encoder-decoder architecture (Xue\net al., 2021b). Both the encoder and decoder com-\nponents are structured similarly to T5, featuring\n12 layers, each with 12 attention heads, and 768\nhidden units for the base model. Consequently,\nthe model comprises approximately ∼580 million\nparameters.\nObjective. We employ an unsupervised (denois-\ning) objective, where the model is trained on\nmasked (corrupted) versions of the original sen-\ntence to reconstruct the original sequence (Xue\net al., 2021b). This objective involves randomly\nsampling and dropping out 15% of tokens in the in-\nput sequence. Subsequently, all consecutive spans\nof dropped-out tokens are replaced by a single sen-\ntinel token.\nPretraining Procedure During the pretraining of\nAfroLingu-MT, we employ a learning rate of 0.01,\na batch size of 1, 024 sequences, and a maximum\nsequence length of 1, 024. Each model undergoes\npretraining for 1 million steps. The training process\nis conducted on Google Cloud TPU with 128 cores\n(v3 −128) provided by the TensorFlow Research\nCloud (TFRC).2\nC\nAfroLingu-MT Benchmark\n2https://sites.research.google/trc/about/\nISO\nName\nCountry(ies)\n# of Speakers\nFamily\nScript\naar\nAfar\nEthiopia\n2.36M\nAfro-Asiatic\nLatin\nach\nAcholi\nUganda, South Sudan\n1.58M\nNilo-Saharan\nLatin\nafr\nAfrikaans\nSouth Africa\n17.67M\nIndo-European\nLatin\naka\nAkan\nGhana\n9.88M\nNiger-Congo\nLatin\namh\nAmharic\nEthiopia\n57.56M\nAfro-Asiatic\nEthiopic\nara\nArabic\nNorth Africa\n372.56K\nAfro-Asiatic\nArabic\nbam\nBambara\nCôte d’Ivoire, Mali\n14.18M\nNiger-Congo\nLatin\nbas\nBassa\nCameroon\n300K\nNiger-Congo\nLatin\nbem\nBemba\nZambia, Democratic Republic of Congo\n4.11M\nNiger-Congo\nLatin\nbtg\nBhete\nCôte d’Ivoire\n329K\nNiger-Congo\nLatin\neng\nEnglish\nGlobal\n1.45B\nIndo-European\nLatin\newe\nEwe\nGhana, Togo\n5.5M\nNiger-Congo\nLatin\nfan\nFang\nEquatorial Guinea, Cameroon, Gabon\n1.06M\nNiger-Congo\nLatin\nfon\nFon\nBenin, Togo\n2.28M\nNiger-Congo\nLatin\nfra\nFrench\nGlobal\n310M\nIndo-European\nLatin\ngez\nGe’ez\nEthiopia\nReligious use\nAfro-Asiatic\nEthiopic\nhau\nHausa\nNigeria\n78.52M\nAfro-Asiatic\nLatin\nibo\nIbo\nNigeria\n30.89M\nNiger-Congo\nLatin\nkau\nKanuri\nNigeria, Niger\n9.47M\nNilo-Saharan\nLatin\nkbp\nKabiye\nTogo, Benin\n992K\nNiger-Congo\nLatin\nkin\nKinyawanda\nRwanda\n14.52M\nNiger-Congo\nLatin\nkon\nKongo\nDemocratic Republic of Congo, Angola, Congo\n7.01M\nNiger-Congo\nLatin\nlgg\nLugbara\nUganda, Democratic Republic of Congo\n1.94M\nNilo-Saharan\nLatin\nlin\nLingala\nDemocratic Republic of Congo, Congo\n40.27M\nNiger-Congo\nLatin\nlug\nLuganda\nUganda\n11.01M\nNiger-Congo\nLatin\nmlg\nMalagasy\nMadagascar\n25M\nAustronesian\nLatin\nnnb\nNande\nDemocratic Republic of Congo\n903K\nNiger-Congo\nLatin\nnya\nChichewa\nMalawi, Mozambique, Zambia\n13.38M\nNiger-Congo\nLatin\nnyn\nNyankore\nUganda, Rwanda\n3.43M\nNiger-Congo\nLatin\norm\nOromo\nEthiopia\n37.45M\nAfro-Asiatic\nLatin\npcm\nNigerian Pidgin\nNigeria\n116M\nCreole\nLatin\nsom\nSomali\nSomalia\n22.04M\nAfro-Asiatic\nLatin\nsot\nSesotho\nLesotho\n13.52M\nNiger-Congo\nLatin\nssw\nSiswati\nEswatini\n4.71M\nNiger-Congo\nLatin\nswa\nSwahili\nKenya, Tanzania\n73M\nNiger-Congo\nLatin\nswc\nSwahili Congo\nDemocratic Republic of Congo\n11.14M\nNiger-Congo\nLatin\nteo\nAteso\nUganda, Kenya\n2.77M\nNilo-Saharan\nLatin\ntir\nTigrinya\nEritrea, Ethiopia\n8.82M\nAfro-Asiatic\nEthiopic\ntsn\nTswana\nBotswana\n13.75M\nNiger-Congo\nLatin\ntso\nTsonga\nSouth Africa\n10M\nNiger-Congo\nLatin\ntwi\nTwi\nGhana\n9.88M\nNiger-Congo\nLatin\nwal\nWolaytta\nEthiopia\n2.49M\nAfro-Asiatic\nLatin\nwol\nWolof\nSenegal, Mauritania\n12.39M\nNiger-Congo\nLatin\nxho\nXhosa\nSouth Africa\n19.21M\nNiger-Congo\nLatin\nyor\nYoruba\nNigeria\n45.86M\nNiger-Congo\nLatin\nzul\nZulu\nSouth Africa\n27.8M\nNiger-Congo\nLatin\nTable C.1: Details of the Languages in our dataset.\nLang Pair\nTrain\nDev\nTest\nLang Pair\nTrain\nDev\nTest\nLang Pair\nTrain\nDev\nTest\naar-amh\n1166\n50\n145\neng-som\n5,000\n50\n200\nnyn-eng\n5000\n50\n200\naar-eng\n1146\n50\n143\neng-sot\n4408\n96\n248\nnyn-lgg\n5000\n50\n200\naar-orm\n1166\n50\n145\neng-ssw\n580\n36\n72\nnyn-lug\n5000\n50\n200\naar-som\n1166\n50\n146\neng-swa\n5000\n50\n200\nnyn-teo\n5000\n50\n200\naar-tir\n1165\n50\n145\neng-teo\n5000\n50\n200\norm-aar\n1166\n50\n145\nach-eng\n5000\n50\n200\neng-tir\n5000\n50\n200\norm-amh\n5000\n50\n200\nach-lgg\n5000\n50\n200\neng-tsn\n5000\n50\n200\norm-eng\n5000\n50\n200\nach-lug\n5000\n50\n200\neng-tso\n5000\n50\n200\norm-som\n1170\n50\n146\nach-nyn\n5000\n50\n200\neng-twi\n5000\n50\n200\norm-tir\n4980\n50\n200\nach-teo\n5000\n50\n200\neng-wal\n5000\n50\n200\norm-wal\n2338\n50\n146\nafr-eng\n5000\n50\n200\neng-wol\n5000\n50\n200\npcm-eng\n1681\n50\n105\naka-eng\n3145\n50\n196\neng-xho\n5000\n50\n200\nsom-aar\n1166\n50\n146\namh-aar\n1166\n50\n145\neng-yor\n5000\n50\n200\nsom-amh\n1156\n50\n145\namh-eng\n5000\n50\n200\neng-zul\n5000\n50\n200\nsom-eng\n5000\n50\n200\namh-gez\n4618\n50\n200\newe-eng\n128\n16\n16\nsom-fra\n4231\n50\n200\namh-mlg\n693\n43\n87\newe-fra\n5000\n50\n200\nsom-orm\n1170\n50\n146\namh-orm\n5000\n50\n200\nfan-btg\n222\n28\n27\nsom-swa\n1390\n50\n173\namh-som\n1156\n50\n145\nfon-fra\n5000\n50\n200\nsom-tir\n1153\n50\n144\namh-swa\n112\n14\n14\nfra-ara\n5000\n50\n200\nsot-eng\n4408\n96\n248\namh-tir\n5000\n50\n200\nfra-ewe\n5000\n50\n200\nssw-eng\n580\n37\n72\namh-wal\n3763\n50\n200\nfra-fon\n5000\n50\n200\nswa-amh\n112\n14\n14\nara-eng\n5000\n50\n200\nfra-lin\n4000\n50\n200\nswa-eng\n5000\n50\n200\nara-fra\n5000\n50\n200\nfra-nnb\n5000\n50\n200\nswa-fra\n5000\n50\n200\nara-yor\n1397\n50\n100\nfra-som\n4231\n50\n200\nswa-mlg\n5000\n50\n200\nbam-eng\n4461\n50\n200\nfra-swa\n5000\n50\n200\nswa-som\n1390\n50\n173\nbas-eng\n5000\n50\n200\nfra-swc\n5000\n50\n200\nswa-yor\n24\n3\n3\nbem-eng\n5000\n50\n200\ngez-amh\n4619\n50\n200\nswc-fra\n5000\n50\n200\nbtg-fan\n222\n28\n27\ngez-eng\n4724\n50\n200\nteo-ach\n5000\n50\n200\neng-aar\n1146\n50\n143\nhau-eng\n5000\n50\n200\nteo-eng\n5000\n50\n200\neng-ach\n5000\n50\n200\nibo-eng\n5000\n50\n200\nteo-lgg\n5000\n50\n200\neng-afr\n5000\n50\n200\nkau-eng\n4257\n50\n200\nteo-lug\n5000\n50\n200\neng-aka\n3145\n50\n197\nkbp-eng\n5000\n50\n200\nteo-nyn\n5000\n50\n200\neng-amh\n5000\n50\n200\nkin-eng\n5000\n50\n200\ntir-aar\n1165\n50\n145\neng-ara\n5000\n50\n200\nkon-eng\n4357\n50\n200\ntir-amh\n5000\n50\n200\neng-bam\n4462\n50\n200\nlgg-ach\n5000\n50\n200\ntir-eng\n5000\n50\n200\neng-bas\n5000\n50\n200\nlgg-lug\n5000\n50\n200\ntir-orm\n4981\n50\n200\neng-bem\n5000\n50\n200\nlgg-nyn\n5000\n50\n200\ntir-som\n1153\n50\n144\neng-ewe\n128\n16\n16\nlgg-teo\n5000\n50\n200\ntir-wal\n2024\n50\n126\neng-gez\n4724\n50\n200\nlin-eng\n246\n31\n31\ntsn-eng\n5000\n50\n200\neng-hau\n5000\n50\n200\nlin-fra\n4000\n50\n200\ntso-eng\n5000\n50\n200\neng-ibo\n5000\n50\n200\nlug-ach\n5000\n50\n200\ntwi-eng\n5000\n50\n200\neng-kau\n4257\n50\n200\nlug-eng\n5000\n50\n200\nwal-amh\n3763\n50\n200\neng-kbp\n5000\n50\n200\nlug-lgg\n5000\n50\n200\nwal-eng\n5000\n50\n200\neng-kin\n5000\n50\n200\nlug-nyn\n5000\n50\n200\nwal-orm\n2338\n50\n147\neng-kon\n4357\n50\n200\nlug-teo\n5000\n50\n200\nwal-tir\n2024\n50\n127\neng-lin\n246\n31\n31\nmlg-amh\n693\n43\n87\nwol-eng\n5000\n50\n200\neng-lug\n5000\n50\n200\nmlg-eng\n5000\n50\n200\nxho-eng\n5000\n50\n200\neng-mlg\n5000\n50\n200\nmlg-swa\n5000\n50\n200\nyor-ara\n1397\n50\n100\neng-nya\n1052\n50\n132\nmlg-yor\n10\n1\n1\nyor-eng\n5000\n50\n200\neng-nyn\n5000\n50\n200\nnnb-fra\n5000\n50\n200\nyor-mlg\n10\n1\n1\neng-orm\n5000\n50\n200\nnya-eng\n1052\n50\n132\nyor-swa\n24\n3\n3\neng-pcm\n1681\n50\n105\nnyn-ach\n5000\n50\n200\nzul-eng\n5000\n50\n200\nTable C.2: Statistics of each language pair in our dataset.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2024-07-05",
  "updated": "2024-07-12"
}