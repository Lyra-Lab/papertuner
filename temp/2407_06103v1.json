{
  "id": "http://arxiv.org/abs/2407.06103v1",
  "title": "QTRL: Toward Practical Quantum Reinforcement Learning via Quantum-Train",
  "authors": [
    "Chen-Yu Liu",
    "Chu-Hsuan Abraham Lin",
    "Chao-Han Huck Yang",
    "Kuan-Cheng Chen",
    "Min-Hsiu Hsieh"
  ],
  "abstract": "Quantum reinforcement learning utilizes quantum layers to process information\nwithin a machine learning model. However, both pure and hybrid quantum\nreinforcement learning face challenges such as data encoding and the use of\nquantum computers during the inference stage. We apply the Quantum-Train method\nto reinforcement learning tasks, called QTRL, training the classical policy\nnetwork model using a quantum machine learning model with polylogarithmic\nparameter reduction. This QTRL approach eliminates the data encoding issues of\nconventional quantum machine learning and reduces the training parameters of\nthe corresponding classical policy network. Most importantly, the training\nresult of the QTRL is a classical model, meaning the inference stage only\nrequires classical computer. This is extremely practical and cost-efficient for\nreinforcement learning tasks, where low-latency feedback from the policy model\nis essential.",
  "text": "QTRL: Toward Practical Quantum Reinforcement\nLearning via Quantum-Train\nChen-Yu Liu ∗†∥, Chu-Hsuan Abraham Lin‡∗∗,\nChao-Han Huck Yang ‡‡xi, Kuan-Cheng Chen§¶††, Min-Hsiu Hsieh ∗x\n∗Hon Hai (Foxconn) Research Institute, Taipei, Taiwan\n†Graduate Institute of Applied Physics, National Taiwan University, Taipei, Taiwan\n‡Department of Electrical and Electronic Engineering, Imperial College London, London, UK\n§Department of Materials, Imperial College London, London, UK\n¶Centre for Quantum Engineering, Science and Technology (QuEST), Imperial College London, London, UK\n‡‡NVIDIA, Taipei, Taiwan\nEmail: ∥chen-yu.liu@foxconn.com, ∗∗abraham.lin23@imperial.ac.uk,\n†† kuan-cheng.chen17@imperial.ac.uk,\nxi hucky@nvidia.com,\nx min-hsiu.hsieh@foxconn.com\nAbstract—Quantum reinforcement learning utilizes quantum\nlayers to process information within a machine learning model.\nHowever, both pure and hybrid quantum reinforcement learning\nface challenges such as data encoding and the use of quantum\ncomputers during the inference stage. We apply the Quantum-\nTrain method to reinforcement learning tasks, called QTRL,\ntraining the classical policy network model using a quantum\nmachine learning model with polylogarithmic parameter reduc-\ntion. This QTRL approach eliminates the data encoding issues of\nconventional quantum machine learning and reduces the training\nparameters of the corresponding classical policy network. Most\nimportantly, the training result of the QTRL is a classical model,\nmeaning the inference stage only requires classical computer.\nThis is extremely practical and cost-efficient for reinforcement\nlearning tasks, where low-latency feedback from the policy model\nis essential.\nIndex Terms—Quantum Machine Learning, Quantum Rein-\nforcement Learning, Quantum-Train\nI. INTRODUCTION\nQuantum machine learning (QML) is rapidly emerging as a\ntransformative field. By harnessing the unique computational\ncapabilities of quantum mechanics, such as quantum super-\nposition and entanglement, QML aims to revolutionize neural\nnetwork (NN) training and performance [1]–[4]. Integrating\nGrover’s search algorithm with QML, particularly for classifi-\ncation tasks, promises substantial performance enhancements.\nFrom an application perspective, QML exhibits immense po-\ntential in tackling complex datasets, with applications ranging\nfrom drug discovery and large-scale stellar classification to\nnatural language processing, recommendation systems, and\ngenerative learning models [5]–[19].\nSimilar to machine learning (ML) and QML, quantum\nreinforcement learning (QRL) [20]–[22] integrates quantum\nlayers and has the potential to solve complex decision-making\nproblems more effectively than classical approaches. However,\nQRL faces significant challenges, particularly in the areas of\ndata encoding and practical implementation. Encoding classi-\ncal data into quantum states efficiently is a non-trivial task,\noften requiring sophisticated techniques that can introduce\noverhead and complexity [23]–[25]. Furthermore, the practical\ndeployment of QRL systems is hindered by the need for quan-\ntum hardware during both training and inference stages, which\ncan be costly and less accessible than classical approaches.\nAddressing these challenges is crucial for realizing the full\npotential of QRL and making it a viable option for a wide\nrange of applications.\nThe Quantum-Train (QT) approach [26], [27], on the other\nhand, uses a quantum neural network (QNN) combined with\na mapping model to generate the parameters of a classical\nmachine learning model. By leveraging the fact that an n-qubit\nquantum state has a Hilbert space of size 2n, we can generate\nup to 2n distinct measurement probabilities corresponding to\nthe measured basis. With a polynomial number of layers with\nrespect to the number of qubits, it is possible to control 2n\nparameters using only O(poly(n)) rotational angles in the\nQNN, as we will describe in more detail in a later section.\nConsequently, the training result of QT using the quantum\napproach is a classical neural network model. When applying\nthis method to the policy model of a policy gradient reinforce-\nment learning task, this Quantum-Train reinforcement learning\n(QTRL) framework not only eliminates the data encoding issue\nof the QNN but also makes the trained result independent of\nthe usage of quantum hardware.\nII. QUANTUM-TRAIN AS POLICY NETWORK PARAMETER\nGENERATOR\nWe will start by introducing the policy gradient method, the\nreinforcement learning approach used in examining the QTRL\nframework. This will be followed by a description of the QT\napproach and an explanation of how we combine QT and RL.\nA. Policy Gradient Method\nThe policy gradient method is a well-established approach\nin RL that focuses on training agents to make decisions\nthrough interaction with an environment [20], [28]. An agent\naims to perform actions that maximize cumulative rewards\nover time. This process is typically modeled as a Markov\narXiv:2407.06103v1  [quant-ph]  8 Jul 2024\nDecision Process (MDP), which comprises a state space S, an\naction space A, a reward function, and transition probabilities.\nThe primary objective is to learn an optimal policy π∗(a|s),\nwhich prescribes the best action a ∈A to take in each state\ns ∈S, thereby maximizing the expected discounted return:\nπ∗= arg max\nπ\nEπ\n\" T\nX\nt=0\nγtRt\n#\n,\n(1)\nwhere Rt is the reward obtained at time step t, and γ ∈[0, 1]\nis a discount factor that determines the relative importance of\nimmediate versus future rewards.\nTo adjust the policy, π(a|s; θ) is typically parameterized by\na neural network model with parameters θ, where an input s\nproduces an output a. The goal is to optimize θ to maximize\nthe expected return. The fundamental principle involves using\ngradient to update θ in a manner that increases the probability\nof actions leading to higher rewards. This optimization process\nis crucial for improving the agent’s decision-making capabil-\nities. To achieve this, the policy loss function is then defined\nas the negative log probability of the taken actions, weighted\nby the normalized returns R′\nt:\nL(θ) = −\nT −1\nX\nt=0\nlog π(at|st; θ) · R′\nt,\n(2)\nwhere R′\nt = Rt−µR\nσR\n, µR is the mean of the returns and σR\nis the standard deviation. This loss function encourages the\npolicy to increase the probability of actions that lead to higher\nreturns. The gradient of the loss function with respect to the\npolicy parameters θ is used to update the policy network:\n∇θL = −\nT −1\nX\nt=0\n∇θ log π(at|st; θ) · R′\nt.\n(3)\nBy computing this gradient, the policy parameters θ can be\nadjusted to improve the policy.\nB. Quantum-Train\nSuppose the neural network model with parameters θ has k\ntrainable parameters: θ = (θ1, θ2, . . . , θk). In conventional ML\nand RL training, it is necessary to initialize k parameters and\ntune all k parameters during the training process. In contrast,\nthe QT framework [26], [27] utilizes a parameterized quantum\nstate |ψ(ϕ)⟩with n = ⌈log2 k⌉qubits and parameters ϕ,\nrepresented by a QNN, to generate 2n ≥k distinct measure-\nment probabilities |⟨i|ψ(ϕ)⟩|2 for i ∈{1, 2, . . . , 2n}. Using a\nmapping model Mβ, which is an additional classical neural\nnetwork with parameters β, the first k basis measurement\nprobabilities are mapped from values bounded between 0 and\n1 to −∞and ∞as follows:\nMβ(ib, |⟨i|ψ(ϕ)⟩|2) = θi,\ni = 1, 2, . . . , k\n(4)\nwhere ib is the bit-string representation of the basis |i⟩. Thus,\nθ is generated from the output of the QNN and the mapping\nmodel Mβ. Consequently, tuning ϕ and β effectively changes\nthe value of the loss function. With the β and ϕ dependency of\nθ from Eq. (4), θ = Mβ(ϕ), the gradient of the loss function\nwith respect to ϕ and β can be derived as follows:\n∇β,ϕL = −\nT −1\nX\nt=0\n∇β,ϕ log π(at|st; β, ϕ) · R′\nt.\n(5)\nThus, it is possible to train the policy by only tuning ϕ and β.\nAs for the number of parameters in ϕ and β, and their relation\nto k, suppose that the mapping in Eq. (4) could be constructed\nwith polynomial depth of the QNN. Since n = ⌈log2 k⌉, the\nrequired number of parameters for ϕ is O(polylog(k)). Given\nthat the input to Mβ is the bit-string representation of the basis\nwith length n and a scalar, the combined vector of length n+1,\nwith a polynomial depth neural network, could also require\nO(polylog(k)) parameters. Therefore, θ, with the number of\nparameters k, is generated by O(polylog(k)) parameters. The\ngraphical illustration of the QTRL framework is depicted in\nFig. 1(a).\nC. Natural Policy Gradient Convergence on QTRL\nQuantum Influence on Gradient Estimation. The parame-\nters θ of the policy network are generated by a QNN combined\nwith a mapping model. These quantum-related parameters,\ndenoted as (ϕ, β), influence the policy network parameters\nthrough the quantum state preparation and measurement pro-\ncesses. The gradient of the loss function, reflecting the quan-\ntum context, is given by:\n∇β,ϕL(πθ) =\n\u0012\n∂θ\n∂(β, ϕ)\n\u0013T\n· ∇θL(πθ)\n(6)\nwhere\n∂θ\n∂(β,ϕ) is the Jacobian matrix that represents the\nsensitivity of classical parameters θ to changes in the quantum\nparameters (ϕ, β).\nLearning Rate and Update Rule. The learning rate η\nplays a crucial role, especially given the complex dynamics\nintroduced by the quantum-classical interface. The update for\nthe quantum parameters can be modeled as:\nϕt+1, βt+1 = ϕt, βt + η∇ϕ,βL(πθt)\n(7)\nThis update rule ensures that the quantum parameters are\nadjusted to optimize the performance of the policy gradient.\nRegret Decomposition. Utilizing the performance differ-\nence lemma and the Lipschitz continuity of the value function,\nwe can express the improvement in policy performance from\none iteration to the next as:\nL(πt+1) −L(πt) ≥γ⟨∇θL(πθt), ∆θt⟩−L\n2 ∥∆θt∥2\n(8)\nHere, γ is a factor derived from policy improvement steps,\nand L is a Lipschitz constant for the value function, reflecting\nthe smoothness in policy updates.\nExpected Regret Bound. The expected regret after T steps\ncan be bounded using the aggregation of differences over all\ntimesteps. Considering the telescoping nature of these sums,\nthe expected regret is estimated as:\nRegretT ≤1\nγ (L(π∗) −L(π1)) + L\n2\nT\nX\nt=1\n∥∆θt∥2\n(9)\nThe expression in Eq. (9) shows the maximum expected regret\nin terms of the initial suboptimality and the sum of squared\nupdates, highlighting the influence of both the learning rate\nand the stability of the quantum parameter generation process.\nThis theoretical framework provides an approach to adapt-\ning regret analysis for NPG when the policy parameters are\ninfluenced by quantum-generated parameters. Further exper-\nimental and empirical studies are necessary to refine these\ntheoretical predictions, ensuring that quantum computational\nbenefits translate into measurable improvements in policy\nperformance.\nIn our setup, we utilize a specific type of quantum gate\nknown as the U3 gate for the QNN. This gate is crucial for\nour purposes because it allows us to adjust the quantum state in\nvery precise ways, characterized by its matrix representation:\nU3(µ, φ, λ) =\n\u0014\ncos(µ/2)\n−eiλ sin(µ/2)\neiφ sin(µ/2)\nei(φ+λ) cos(µ/2)\n\u0015\n(10)\nIn conjunction with the U3 gate, the controlled-U3 gate (or\nCU3) plays a crucial role to entangle qubits:\nCU3 = I ⊗|0⟩⟨0| + U3(µ, φ, λ) ⊗|1⟩⟨1|\n(11)\nThese parameterized gates, particularly the CU3 with its\ncircular layout, enable the specification of the number of\nparameters as a polynomial function of the qubit count.\nIII. RESULT AND DISCUSSION\nTo examine the applicability of the QTRL approach, we\ntested it in two well-known environments: CartPole-v1 [29]\nand MiniGrid-Empty-5x5-v0 [30]. The quantum circuit simula-\ntions were conducted using the TorchQuantum package [31],\nwhere the quantum states involved were simulated using state\nvector simulation.\nBeginning with the CartPole-v1 environment, as shown\nin Table I, the classical policy model used consists of 898\nparameters. This implies that the corresponding required qubits\nfor QTRL is ⌈log2 898⌉= 10. We trained the model for 2000\nepisodes, and as shown in Fig. 1(b), the QTRL with varying\ndepths (L = 1, 3, 5) are depicted in different shades of blue,\nwhile the classical method is shown in red. The deeper circuits,\nhaving better expressibility, result in the total reward curve\nbeing closer to the classical case. As emphasized in Table I, in\nthe L = 1 case, the 898 classical policy model parameters are\ngenerated by only 361 QNN and mapping model parameters,\nwhile in the L = 3 case, 531 parameters are used in the QNN\nand mapping model. In the L = 5 case, we found that the last\n10 episode average reward, 493.8, is better than the classical\ncase of 436.3, while using only 651 parameters during training.\nIn the MiniGrid-Empty-5x5-v0 environment, we observed\nsimilar behavior to the results in\nCartPole-v1, although\nMiniGrid-Empty-5x5-v0 is a slightly more complicated environ-\nment. We trained the model for 4000 episodes. As shown in\nTable II, the classical policy model utilizes 4835 parameters,\nmainly due to the larger size of the observation space in\nthis environment, and implying the qubit counts for QTRL\nis ⌈log2 4835⌉= 13. In Fig. 1(c), the classical method is\nshown in red, while QTRL with varying depths (L = 3, 7, 13)\nare shown in different shades of blue. As noted in Table II,\nwhile shallower circuits achieve lower average rewards, it is\nworth noting that in the L = 13 case, we achieved a last 10\nepisode average reward of 0.900 with only 2529 QNN and\nmapping model parameters, whereas the classical approach\nwith an average reward of 0.916 requires 4835 parameters.\nThe results from our experiments on the CartPole-v1 and\nMiniGrid-Empty-5x5-v0 environments demonstrate the potential\nof the QTRL approach to effectively train reinforcement learn-\ning policies using fewer parameters than classical methods.\nThis is particularly evident in the deeper quantum circuits,\nwhich show performance close to or even surpassing classical\nmethods while utilizing significantly fewer parameters.\nOne may observe in Fig. 1(a) that the QNN part only\nconsists of the U3 ansatz without data encoding layers, unlike\nconventional QML and QRL approaches [21]–[24], [32]. This\nis because our QNN only serves to generate parameters for the\nclassical policy model, meaning that both the observation state\ninput and the action output are handled in the classical part.\nWithout the data encoding issue, the circuit width and depth\nare independent of the input data size, making our approach\nextremely practical for dealing with larger observation state\ninputs. In conventional approaches, some data might be too\nlarge to construct a reasonable circuit either in depth or width\n(qubit counts).\nContinuing with the classical data input and output features\nof QTRL, this implies that once we use the training flow in\nFig. 1(a), the training result is the generated classical policy\nmodel πθ, which is identical to any other classical model\ntrained in a conventional manner. This means that:\n1) This approach is compatible with any potential transfer\nlearning or fine-tuning on the classical side. We can use\nthe quantum computer to initially train the model with\nfewer parameters and then use a classical computer for\nfull parameter fine-tuning.\n2) The model is now decoupled from quantum computing\nhardware. Thus, in the inference stage, this “quantum\ntrained” model can be run entirely on classical comput-\ning hardware.\nPoint (2) is particularly important in RL tasks because\nthese tasks usually demand low-latency feedback from the\npolicy model. For example, in Autonomous Driving tasks\n[33], using conventional QRL or hybrid quantum-classical RL\n(QCRL) approaches, where quantum jobs might need to be\nsubmitted to a cloud quantum computing platform and queued,\nis unlikely to meet the real-time response requirements of\nthe fast-changing road environment. In contrast, our QTRL\napproach produces a classical model that can be executed\non the vehicle’s edge device. Similar situations may arise in\ntasks interacting with real-time data in the real world. This\ndemonstrates the widespread potential of our QTRL approach\nin terms of practicality.\nFurthermore, considering the current and likely future stages\nof quantum computing development, the financial cost of\nusing quantum computing hardware is much higher than using\nFig. 1: (a) Illustration of QTRL framework. (b) Total reward as a function of episode number for the CartPole-v1 environment.\nThe classical method is shown in red, while QTRL with varying depths (L = 1, 3, 5) are shown in different shades of blue. (c)\nTotal reward as a function of episode number for the MiniGrid-Empty-5x5-v0 environment. The classical method is shown in\nred, while QTRL with varying depths (L = 3, 7, 13) are shown in different shades of blue. Insets show the visual representation\nof each environment.\nModel\nTopology\nLast 10 episode\naverage reward\nPara. size\nClassical Policy Model\n(4-128, 128-2)\n436.3\n898\nQTRL-1\nQNN: (U3 block)*1 + Mapping Model: (11-10, 10-10, 10-1)\n94.6\n361\nQTRL-3\nQNN: (U3 block)*3 + Mapping Model: (11-10, 10-10, 10-1)\n218.9\n531\nQTRL-5\nQNN: (U3 block)*5 + Mapping Model: (11-10, 10-10, 10-1)\n493.8\n651\nTABLE I: Comparative performance of Classical Policy and QT models with varying numbers of QNN blocks for CartPole-v1\nenvironment.\nModel\nTopology\nLast 10 episode\naverage reward\nPara. size\nClassical Policy Model\n(147-32, 32-3)\n0.916\n4835\nQTRL-3\nQNN: (U3 block)*3 + Mapping Model: (11-10, 10-10, 10-1)\n0.694\n1749\nQTRL-7\nQNN: (U3 block)*7 + Mapping Model: (11-10, 10-10, 10-1)\n0.827\n2061\nQTRL-13\nQNN: (U3 block)*13 + Mapping Model: (11-10, 10-10, 10-1)\n0.900\n2529\nTABLE II: Comparative performance of Classical Policy and QT models with varying numbers of QNN blocks for MiniGrid-\nEmpty-5x5-v0 environment.\nclassical CPUs and GPUs. The classical inference feature of\nQTRL is much more cost-efficient compared to QRL and\nQCRL approaches, which require quantum computer access\nevery time the model is used. This cost efficiency further\nenhances the practicality and appeal of the QTRL approach.\nIV. CONCLUSION\nIn this work, we introduced the QTRL approach, which\nleverages QNN to generate parameters for classical policy\nmodels. This method addresses significant challenges in QRL\nand QML, such as data encoding and dependency on quantum\nhardware during inference. Our experiments on the CartPole-\nv1 and MiniGrid-Empty-5x5-v0 environments demonstrate that\nQTRL can achieve comparable or superior performance to\nclassical methods while using significantly fewer parameters.\nThis efficiency makes QTRL particularly suitable for RL tasks\nrequiring low-latency feedback and practical for real-world\napplications like autonomous driving. Moreover, QTRL’s in-\ndependence from quantum hardware during inference and its\ncompatibility with classical transfer learning and fine-tuning\ntechniques enhance its practicality and cost-efficiency. This\nmakes QTRL a promising approach for various applications,\noffering both practical benefits and theoretical advancements\nin reinforcement learning.\nQTRL not only bridges the gap between quantum and\nclassical RL but also provides a practical solution that can\nbe readily applied with existing classical infrastructure. This\napproach paves the way for more accessible and scalable RL\nmodels, harnessing the strengths of both quantum and classical\ncomputing paradigms.\nREFERENCES\n[1] J. Biamonte, P. Wittek, N. Pancotti, P. Rebentrost, N. Wiebe, and\nS. Lloyd, “Quantum machine learning,” Nature, vol. 549, no. 7671, pp.\n195–202, 2017. [Online]. Available: https://doi.org/10.1038/nature23474\n[2] M. Schuld, I. Sinayskiy, and F. Petruccione, “An introduction to\nquantum machine learning,” Contemporary Physics, vol. 56, no. 2,\np. 172–185, Oct. 2014. [Online]. Available: http://dx.doi.org/10.1080/\n00107514.2014.964942\n[3] E. Farhi and H. Neven, “Classification with quantum neural networks\non near term processors,” arXiv preprint arXiv:1802.06002, 2018.\n[4] C. Ciliberto, M. Herbster, A. D. Ialongo, M. Pontil, A. Rocchetto,\nS. Severini, and L. Wossnig, “Quantum machine learning: a classical\nperspective,” Proceedings of the Royal Society A: Mathematical,\nPhysical and Engineering Sciences, vol. 474, no. 2209, p. 20170551,\nJan. 2018. [Online]. Available: http://dx.doi.org/10.1098/rspa.2017.0551\n[5] L. K. Grover, “A fast quantum mechanical algorithm for database\nsearch,” in Proceedings of the Twenty-Eighth Annual ACM Symposium\non Theory of Computing, ser. STOC ’96.\nNew York, NY, USA:\nAssociation for Computing Machinery, 1996, p. 212–219. [Online].\nAvailable: https://doi.org/10.1145/237814.237866\n[6] Y.\nDu,\nM.-H.\nHsieh,\nT.\nLiu,\nand\nD.\nTao,\n“A\ngrover-search\nbased quantum learning scheme for classification,” New Journal of\nPhysics, vol. 23, no. 2, p. 023020, feb 2021. [Online]. Available:\nhttps://dx.doi.org/10.1088/1367-2630/abdefa\n[7] Y. Cao, J. Romero, J. P. Olson, M. Degroote, P. D. Johnson,\nM. Kieferov´a, I. D. Kivlichan, T. Menke, B. Peropadre, N. P. D.\nSawaya, S. Sim, L. Veis, and A. Aspuru-Guzik, “Quantum chemistry\nin the age of quantum computing,” Chemical Reviews, vol. 119,\nno. 19, pp. 10 856–10 915, 10 2019. [Online]. Available: https:\n//doi.org/10.1021/acs.chemrev.8b00803\n[8] K.-C. Chen, X. Xu, H. Makhanov, H.-H. Chung, and C.-Y. Liu,\n“Quantum-enhanced support vector machine for large-scale stellar clas-\nsification with gpu acceleration,” arXiv preprint arXiv:2311.12328,\n2023.\n[9] K.-C. Chen, X. Li, X. Xu, Y.-Y. Wang, and C.-Y. Liu, “Quantum-hpc\nframework with multi-gpu-enabled hybrid quantum-classical workflow:\nApplications in quantum simulations,” arXiv preprint arXiv:2403.05828,\n2024.\n[10] C.-Y. Liu and H.-S. Goan, “Reinforcement learning quantum local\nsearch,” in 2023 IEEE International Conference on Quantum Computing\nand Engineering (QCE), vol. 2.\nIEEE, 2023, pp. 246–247.\n[11] C.-Y. Liu, “Practical quantum search by variational quantum eigen-\nsolver on noisy intermediate-scale quantum hardware,” arXiv preprint\narXiv:2304.03747, 2023.\n[12] K. Meichanetzidis, S. Gogioso, G. de Felice, N. Chiappori, A. Toumi,\nand B. Coecke, “Quantum natural language processing on near-term\nquantum computers,” Electronic Proceedings in Theoretical Computer\nScience,\nvol.\n340,\np.\n213–229,\nSep.\n2021.\n[Online].\nAvailable:\nhttp://dx.doi.org/10.4204/EPTCS.340.11\n[13] C.-Y. Liu, H.-Y. Wang, P.-Y. Liao, C.-J. Lai, and M.-H. Hsieh, “Imple-\nmentation of trained factorization machine recommendation system on\nquantum annealer,” arXiv preprint arXiv:2210.12953, 2022.\n[14] I. Kerenidis and A. Prakash, “Quantum recommendation systems,” arXiv\npreprint arXiv:1603.08675, 2016.\n[15] P.-L. Dallaire-Demers and N. Killoran, “Quantum generative adversarial\nnetworks,” Phys. Rev. A, vol. 98, p. 012324, Jul 2018. [Online].\nAvailable: https://link.aps.org/doi/10.1103/PhysRevA.98.012324\n[16] S. Lloyd and C. Weedbrook, “Quantum generative adversarial learning,”\nPhys. Rev. Lett., vol. 121, p. 040502, Jul 2018. [Online]. Available:\nhttps://link.aps.org/doi/10.1103/PhysRevLett.121.040502\n[17] J. Tian, X. Sun, Y. Du, S. Zhao, Q. Liu, K. Zhang, W. Yi, W. Huang,\nC. Wang, X. Wu et al., “Recent advances for quantum neural networks\nin generative learning,” IEEE Transactions on Pattern Analysis and\nMachine Intelligence, 2023.\n[18] X.-F. Yin, Y. Du, Y.-Y. Fei, R. Zhang, L.-Z. Liu, Y. Mao, T. Liu, M.-H.\nHsieh, L. Li, N.-L. Liu, D. Tao, Y.-A. Chen, and J.-W. Pan, “Efficient\nbipartite entanglement detection scheme with a quantum adversarial\nsolver,” Phys. Rev. Lett., vol. 128, p. 110501, Mar 2022. [Online].\nAvailable: https://link.aps.org/doi/10.1103/PhysRevLett.128.110501\n[19] H.-L. Huang, Y. Du, M. Gong, Y. Zhao, Y. Wu, C. Wang, S. Li,\nF. Liang, J. Lin, Y. Xu, R. Yang, T. Liu, M.-H. Hsieh, H. Deng,\nH. Rong, C.-Z. Peng, C.-Y. Lu, Y.-A. Chen, D. Tao, X. Zhu, and J.-W.\nPan, “Experimental quantum generative adversarial networks for image\ngeneration,” Phys. Rev. Applied, vol. 16, p. 024051, Aug 2021. [Online].\nAvailable: https://link.aps.org/doi/10.1103/PhysRevApplied.16.024051\n[20] Y. Li, “Deep reinforcement learning: An overview,” arXiv preprint\narXiv:1701.07274, 2017.\n[21] S. Y.-C. Chen, C.-H. H. Yang, J. Qi, P.-Y. Chen, X. Ma, and H.-S.\nGoan, “Variational quantum circuits for deep reinforcement learning,”\nIEEE Access, vol. 8, pp. 141 007–141 024, 2020.\n[22] S. Y.-C. Chen, C.-M. Huang, C.-W. Hsing, H.-S. Goan, and Y.-J.\nKao, “Variational quantum reinforcement learning via evolutionary\noptimization,” Machine Learning: Science and Technology, vol. 3,\nno. 1, p. 015025, Feb. 2022. [Online]. Available: http://dx.doi.org/10.\n1088/2632-2153/ac4559\n[23] A. P´erez-Salinas, A. Cervera-Lierta, E. Gil-Fuster, and J. I. Latorre,\n“Data re-uploading for a universal quantum classifier,” Quantum, vol. 4,\np. 226, 2020.\n[24] A. Mari, T. R. Bromley, J. Izaac, M. Schuld, and N. Killoran, “Transfer\nlearning in hybrid classical-quantum neural networks,” Quantum, vol. 4,\np. 340, 2020.\n[25] A. Tavakoli, A. Hameedi, B. Marques, and M. Bourennane, “Quantum\nrandom access codes using single d-level systems,” Physical review\nletters, vol. 114, no. 17, p. 170502, 2015.\n[26] C.-Y. Liu, E.-J. Kuo, C.-H. A. Lin, J. G. Young, Y.-J. Chang, M.-H.\nHsieh, and H.-S. Goan, “Quantum-train: Rethinking hybrid quantum-\nclassical machine learning in the model compression perspective,” arXiv\npreprint arXiv:2405.11304, 2024.\n[27] C.-Y. Liu, E.-J. Kuo, C.-H. A. Lin, S. Chen, J. G. Young, Y.-J. Chang,\nand M.-H. Hsieh, “Training classical neural networks by quantum\nmachine learning,” arXiv preprint arXiv:2402.16465, 2024.\n[28] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction.\nMIT press, 2018.\n[29] A. G. Barto, R. S. Sutton, and C. W. Anderson, “Neuronlike adaptive\nelements that can solve difficult learning control problems,” IEEE\nTransactions on Systems, Man, and Cybernetics, vol. SMC-13, no. 5,\npp. 834–846, 1983.\n[30] M. Chevalier-Boisvert, B. Dai, M. Towers, R. Perez-Vicente, L. Willems,\nS. Lahlou, S. Pal, P. S. Castro, and J. Terry, “Minigrid & miniworld:\nModular & customizable reinforcement learning environments for goal-\noriented tasks,” Advances in Neural Information Processing Systems,\nvol. 36, 2024.\n[31] H. Wang, Y. Ding, J. Gu, Z. Li, Y. Lin, D. Z. Pan, F. T. Chong, and\nS. Han, “Quantumnas: Noise-adaptive search for robust quantum cir-\ncuits,” in The 28th IEEE International Symposium on High-Performance\nComputer Architecture (HPCA-28), 2022.\n[32] O. Lockwood and M. Si, “Reinforcement learning with quantum vari-\national circuit,” in Proceedings of the AAAI conference on artificial\nintelligence and interactive digital entertainment, vol. 16, no. 1, 2020,\npp. 245–251.\n[33] B. R. Kiran, I. Sobh, V. Talpaert, P. Mannion, A. A. Al Sallab, S. Yo-\ngamani, and P. P´erez, “Deep reinforcement learning for autonomous\ndriving: A survey,” IEEE Transactions on Intelligent Transportation\nSystems, vol. 23, no. 6, pp. 4909–4926, 2021.\n",
  "categories": [
    "quant-ph"
  ],
  "published": "2024-07-08",
  "updated": "2024-07-08"
}