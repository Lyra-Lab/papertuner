{
  "id": "http://arxiv.org/abs/2407.08148v1",
  "title": "SCPNet: Unsupervised Cross-modal Homography Estimation via Intra-modal Self-supervised Learning",
  "authors": [
    "Runmin Zhang",
    "Jun Ma",
    "Si-Yuan Cao",
    "Lun Luo",
    "Beinan Yu",
    "Shu-Jie Chen",
    "Junwei Li",
    "Hui-Liang Shen"
  ],
  "abstract": "We propose a novel unsupervised cross-modal homography estimation framework\nbased on intra-modal Self-supervised learning, Correlation, and consistent\nfeature map Projection, namely SCPNet. The concept of intra-modal\nself-supervised learning is first presented to facilitate the unsupervised\ncross-modal homography estimation. The correlation-based homography estimation\nnetwork and the consistent feature map projection are combined to form the\nlearnable architecture of SCPNet, boosting the unsupervised learning framework.\nSCPNet is the first to achieve effective unsupervised homography estimation on\nthe satellite-map image pair cross-modal dataset, GoogleMap, under [-32,+32]\noffset on a 128x128 image, leading the supervised approach MHN by 14.0% of mean\naverage corner error (MACE). We further conduct extensive experiments on\nseveral cross-modal/spectral and manually-made inconsistent datasets, on which\nSCPNet achieves the state-of-the-art (SOTA) performance among unsupervised\napproaches, and owns 49.0%, 25.2%, 36.4%, and 10.7% lower MACEs than the\nsupervised approach MHN. Source code is available at\nhttps://github.com/RM-Zhang/SCPNet.",
  "text": "SCPNet: Unsupervised Cross-modal\nHomography Estimation via\nIntra-modal Self-supervised Learning\nRunmin Zhang1∗, Jun Ma2,1∗, Si-Yuan Cao2,1∗†, Lun Luo3,\nBeinan Yu1, Shu-Jie Chen4, Junwei Li1, and Hui-Liang Shen1\n1 College of Information Science and Electronic Engineering, Zhejiang University\n2 Ningbo Innovation Center, Zhejiang University\n3 HAOMO.AI Technology Co., Ltd.\n4 Zhejiang GongShang University\nAbstract. We propose a novel unsupervised cross-modal homography\nestimation framework based on intra-modal Self-supervised learning,\nCorrelation, and consistent feature map Projection, namely SCPNet.\nThe concept of intra-modal self-supervised learning is first presented\nto facilitate the unsupervised cross-modal homography estimation. The\ncorrelation-based homography estimation network and the consistent\nfeature map projection are combined to form the learnable architec-\nture of SCPNet, boosting the unsupervised learning framework. SCP-\nNet is the first to achieve effective unsupervised homography estima-\ntion on the satellite-map image pair cross-modal dataset, GoogleMap,\nunder [-32,+32] offset on a 128 × 128 image, leading the supervised\napproach MHN by 14.0% of mean average corner error (MACE). We\nfurther conduct extensive experiments on several cross-modal/spectral\nand manually-made inconsistent datasets, on which SCPNet achieves the\nstate-of-the-art (SOTA) performance among unsupervised approaches,\nand owns 49.0%, 25.2%, 36.4%, and 10.7% lower MACEs than the su-\npervised approach MHN. Source code is available at https://github.\ncom/RM-Zhang/SCPNet.\nKeywords: Homography estimation · Unsupervised learning · Multi-\nmodal and multi-spectral images\n1\nIntroduction\nHomography estimation aims to compute the global perspective transform among\nimages. Present supervised homography estimation approaches [6,8,12,28,37,45]\ncan usually handle the homography estimation task under large offsets and\nmodality gaps. However, in real applications, the homography deformation be-\ntween images is usually unknown, especially for the cross-modal images captured\nby different devices or at various times [11, 40]. Therefore, unsupervised cross-\nmodal homography estimation is vital for real-world tasks such as multi-spectral\n∗Equal Contributions. † Corresponding author. (cao_siyuan@zju.edu.cn)\narXiv:2407.08148v1  [cs.CV]  11 Jul 2024\n2\nR. Zhang et al.\nUDHN\nCA-UDHN\nbiHomE\n SCPNet (Ours)\nCL\nCL+P\nCL (percptual)\nCL+SL+C+P\nFig. 1: Unsupervised homography estimation results of UDHN [35], CA-UDHN [44],\nbiHomE [27], and our SCPNet on GoogleMap dataset under [-32,+32] offset. CL de-\nnotes the common cross-modal intensity-based learning, SL denotes the intra-modal\nself-supervised learning, C denotes correlation, and P denotes consistent feature map\nprojection. CL (perceptual) means the cross-modal intensity-based learning is con-\nducted by the perceptual loss. Green polygons denote the ground-truth homography\ndeformation from IB (map) to IA (satellite). Red polygons denote the estimated homog-\nraphy deformation using different algorithms on IA (satellite). Different from the pre-\nvious works that only adopt cross-modal intensity-based learning, SCPNet introduces\nintra-modal self-supervised learning as extra supervision and has a special architecture\nbased on correlation and consistent feature map projection, leading to successful un-\nsupervised cross-modal homography learning under large offsets and modality gaps.\nimage fusion [43, 46], multi-modal image restoration [13, 33], and GPS denied\nnavigation [19,45].\nFor the above reasons, unsupervised deep homography estimation has raised\ngrowing interest. Nguyen et al. [35] trained a deep homography estimation net-\nwork in an unsupervised manner by comparing the pixel intensity of warped\nsource image and target image. Wang et al. [38] constrained the intensity loss in\na cyclic manner that further improves the homography estimation accuracy. To\ncope with the illumination change, several works have been presented [41,44] to\nachieve unsupervised homography estimation by introducing the feature repre-\nsentation for both homography estimation and consistency supervision. Based on\nthe above two works, Hong et al. [23] adopted the GAN [20] model to improve the\nsupervision of feature similarity. However, most of the above approaches focus\nonly on cross-modal intensity-based learning, and can only separately address\neither large offsets or modality gaps [27].\nTo cope with the above problem, in this paper, we propose a novel unsuper-\nvised cross-modal homography estimation framework, namely SCPNet, which\nadopts intra-modal Self-supervised learning, Correlation, and consistent feature\nmap Projection. As illustrated in Fig. 1, different from the previous unsuper-\nvised works that only adopt cross-modal intensity-based learning, SCPNet intro-\nduces intra-modal self-supervised learning as extra supervision and has a special\narchitecture based on correlation and consistent feature map projection. It is\nobserved that SCPNet achieves successful unsupervised homography estimation\non the cross-modal data under such large offsets, while the others cannot.\nSCPNet\n3\nThe intra-modal self-supervised learning lays the foundation of our SCP-\nNet, which mines the two-branch self-supervised information via applying simu-\nlated homography within the two modalities. The network with shared weights\nis trained simultaneously by the two-branch self-supervised learning. Accord-\ning to the ablation on GoogleMap, simply using the intra-modal self-supervised\nlearning, our SCPNet can produce converged training, even without the cross-\nmodal intensity-based learning in [35,38,41,44]. On the contrary, only using the\ncross-modal intensity-based learning fails to converge on such a large modal-\nity gap and offset. The two learning strategies are combined to form the final\nsupervision of SCPNet. The correlation and consistent feature map projection\nhave been separately employed in many previous homography estimation frame-\nworks [6,23,37,41,44]. However, the strategy and effectiveness of combining them\nto form an effective unsupervised cross-modal homography estimation framework\nhaven’t ever been investigated. The above two parts form the powerful learn-\nable architecture of our SCPNet, which also boosts the unsupervised training\nframework.\nTo the best of our knowledge, SCPNet is the first method that achieves effec-\ntive unsupervised homography estimation on such a large offset (offset range of\n[-32,+32] on a 128 × 128 image) and modality gap (GoogleMap [45] of satellite-\nmap image pairs as in Fig. 1), outperforming the supervised approach MHN [28]\nby 14.0% of mean average corner error (MACE). We further evaluate our SCPNet\non Flash/no-flash [21] cross-modal dataset, Harvard [10] and RGB/NIR [5] cross-\nspectral datasets, and PDS-COCO [27] manually-made inconsistent dataset,\nwhich also achieves the state-of-the-art (SOTA) performance among unsuper-\nvised approaches. In summary, our contributions are as follows:\n– We propose SCPNet, a novel unsupervised cross-modal homography esti-\nmation framework, which combines three key components, including intra-\nmodal self-supervised learning, correlation, and consistent feature map pro-\njection. SCPNet ranks top in the unsupervised homography estimation on\ncross-modal/spectral and manually-made inconsistent data under large off-\nsets.\n– The concept of intra-modal self-supervised learning is devised to support the\nunsupervised learning framework, which mines the two-branch self-supervised\ninformation via applying simulated homography within the two modalities.\nBy simultaneously training the weight-shared network using the two-branch\nself-supervised learning, the homography estimation knowledge can be gen-\neralized from intra-modal to cross-modal.\n– We combine the correlation and consistent feature map projection to form\na powerful unsupervised learning network architecture of SCPNet. The cor-\nrelation constrains the network to learn a clearer knowledge that can be\ngeneralized from intra-modal to cross-modal. The projected consistent fea-\nture map can monitor both the cross-modal homography estimation and\ncross-modal consistent latent space projection, which will further improve\nthe estimation accuracy.\n4\nR. Zhang et al.\n2\nRelated Work\nTraditional Approaches. The most widely used traditional homography es-\ntimation approaches, namely feature-based approaches, typically involve three\nkey steps: feature extraction, feature matching, and homography estimation [37].\nCommonly used feature extraction approaches include SIFT [32], SURF [4],\nand ORB [34]. Popular homography estimation techniques include DLT [16],\nRANSAC [17], IRLS [22], and MAGSAC [3]. To further improve the robustness\nof cross-modal feature extraction, some approaches such as LGHD [1], RIFT [29],\nand DASC [26] have been presented. The above approaches achieve reliable ho-\nmography estimations under moderate intensity variance and deformation but\nmay produce unsatisfactory results dealing with cross-modal images under large\noffsets [6,7,28,37].\nSupervised Approaches. DeTone et al. [12] first introduced the end-to-end\nhomography estimation network DHN. To further improve the accuracy of ho-\nmography estimation, many approaches have been subsequently presented. For\nexample, MHN [28] used a multi-scale network concatenation and DLKFM [45]\nadopted deep Lucas-Kanade iteration. Furthermore, LocalTrans [37] trained a\nmulti-scale local transformer, IHN [6] employed deep learnable iteration, and\nRHWF [8] combined homography-guided image warping and focus transformer,\netc. However, obtaining the ground-truth is often difficult and costly, making it\nchallenging for supervised learning approaches to be widely applicable in prac-\ntice.\nUnsupervised Approaches. Nguyen et al. [35] trained the homography\nestimation network using pixel-level photometric loss in an unsupervised man-\nner. Based on this pioneering work, Wang et al. [38] added extra supervision\nby the invertibility constraints. Zhang et al. [44] presented CA-UDHN to de-\npict the similarity in feature space instead of pixel space. However, CA-UDHN\nhas poor robustness for images with large viewpoint changes [27]. Koguchiuk\net al. [27] then expanded CA-UDHN with perceptual loss [25], which improves\nthe robustness of unsupervised training of deep homography estimation under\nlarge intensity and viewpoint changes. Furthermore, Ye et al. [41] introduced\nfeature identity loss to enforce the image feature to be warp-equivalent, and pro-\nposed a homography flow representation. Besides the above approaches, some\nunsupervised techniques such as NeMAR [2], UMF-CMGR [14], and RFNet [39]\nuse modality transfer networks to migrate one modality to another, achieving\nunsupervised cross-modal/spectral motion estimation.\n3\nPilot Experiments and Finding\nWe first denote the image pair from modality A and B as IA and IB, with homog-\nraphy deformation between them. To train a homography estimation network in\na supervised manner, the objective of network training can be formulated as\n  \\ mat\nh\nop\n \n{\\arg \\min  }_\n{\n\\theta }\\mathcal {L}_\\mathrm {S}\\big (\\phi _\\theta (\\mathbf {I}_\\mathrm {A},\\mathbf {I}_\\mathrm {B}), \\mathbf {H}_\\mathrm {GT}\\big ), \\label {eq:supervised} \n(1)\nSCPNet\n5\n0\n20\n40\n60\n80\n100\n120\nTraining Iterations (k)\n0\n10\n20\n30\n40\n50\n60\n70\nMACE (pixels)\nCross-modal intensity-based learning\nIntra-modal self-supervised learning\nFig. 2: The cross-modal test MACEs of the network trained using intra-modal self-\nsupervised learning and cross-modal intensity-based learning during the training iter-\nations, respectively.\nwhere HGT denotes the ground-truth homography between the two images, ϕθ\nthe network, and θ the network parameters to be optimized. LS denotes the\nsupervised loss, which is usually L2 [28] or L1 [6,37] norm. However, in practical\napplications, ground-truth homography is generally difficult to obtain, especially\nfor the cross-modal images captured by different devices or at various times\n[11,40]. To cope with this difficulty, unsupervised homography estimation is then\ninvestigated, and the training for most of them [23,27,35,44] can be modeled as\n  \\ mat\nh\nop\n \n{\\a rg \\m in }_{ \\the\nt\na }\\mathcal {L}_\\mathrm {C}\\big (\\mathbf {I}_\\mathrm {A}, \\mathcal {W}(\\mathbf {I}_\\mathrm {B}, \\phi _\\theta (\\mathbf {I}_\\mathrm {A},\\mathbf {I}_\\mathrm {B}))\\big ), \\label {eq:cross} \n(2)\nwhere W denotes the warping operation using the predicted homography ϕθ(IA, IB),\nand LC denotes the cross-modal loss that monitoring the content similarity of\nthe warped IB and IA. The cross-modal intensity-based loss varies from the L1\npixel-wise photometric loss [35], the L1 similarity loss of the feature maps [41,44],\nand the perceptual loss [27]. Nevertheless, under large homography deformation\nand intensity variance, the above losses may fail, according to [27] and our ex-\nperiments. As the cross-modal image intensity similarity is generally highly non-\nconvex [7], making the solution space of the loss function hard to optimize, the\ntraining process is prone to fall into non-convergent as demonstrated in [27].\nInspired by multitask learning [9], which simultaneously tackles multiple\ntasks using a shared representation, we propose intra-modal self-supervised learn-\ning to achieve better supervision. Multitask learning implicitly learns task rela-\ntionships within a shared representation through gradient aggregation [9], and\nrecent studies have shown that various tasks benefit from it [15,18,24]. The moti-\nvation of our intra-modal self-supervised learning is to enhance the unsupervised\nlearning process by introducing highly related extra tasks that provide direct\nsupervision. While obtaining cross-modal ground-truth homography is challeng-\ning, intra-modal ground-truth homography can be easily generated by directly\napplying simulated deformations [12]. This allows the knowledge of homogra-\nphy transformation to be directly learned, rather than indirectly as in common\ncross-modal intensity-based learning. Additionally, the relationship between im-\n6\nR. Zhang et al.\nages from two modalities, such as mutual structures, is likely to be learned\nwithin the shared representation during two-branch intra-modal self-supervised\nlearning. To validate the aforementioned statement, we train a weight-shared\nnetwork to separately predict the homography within the two modalities under\ndirect supervision from simulation, which can be expressed as\n  \\ mat\nh\nop\n \n{\\arg \\m\nin }_{\\t\nh\ne ta\n \n} \\mat hc\nal {L}_\\\nm\nathrm {S}\\big (\\phi _\\theta (\\mathbf {I}_\\mathrm {A},\\mathbf {I}'_\\mathrm {A}), \\mathbf {H}_\\mathrm {GT,A}\\big ) + \\mathcal {L}_\\mathrm {S}\\big (\\phi _\\theta (\\mathbf {I}_\\mathrm {B},\\mathbf {I}'_\\mathrm {B}), \\mathbf {H}_\\mathrm {GT,B}\\big ), \\label {eq:selfsupervised} \n(3)\nwhere I′\nA denotes the homography warped IA with the simulated ground-truth\nhomography HGT,A, and modality B in the same manner. We conduct this pilot\nexperiment on the cross-modal dataset, GoogleMap [45], which is of large in-\ntensity and content difference, under [-32,+32] offset on a 128 × 128 image. The\ncross-modal test MACEs of the network trained using intra-modal self-supervised\nlearning and common cross-modal intensity-based learning during the training\niterations are illustrated in Fig. 2. Interestingly, we find that the network trained\nby intra-modal self-supervised learning has an evidently better cross-modal per-\nformance than the common cross-modal intensity-based learning trained one.\nTherefore, we can obtain the finding: The cross-modal homography es-\ntimation can be indirectly facilitated by training the weight-shared\nnetwork using the simulated transform within the two modalities.\n4\nSCPNet\nBased on the finding in Section 3, we hope to further design a network architec-\nture and complement it with an appropriate training strategy. For this purpose,\nwe propose the unsupervised cross-modal homography estimation framework\nthat adopts intra-modal Self-supervised learning, Correlation, and consistent\nfeature map Projection, namely SCPNet. Fig. 3(a) demonstrates the schematic\ndiagram of the training and inference framework of SCPNet. Fig. 3(b) and 3(c)\nshow the two learnable modules that form the architecture of SCPNet. Con-\nsidering that the learnable modules and the training strategy are coupled and\nmutually promote each other, in the following section, we will follow the idea\nof building a powerful unsupervised learning framework based on the finding of\nintra-modal self-supervised learning to demonstrate our SCPNet.\n4.1\nCorrelation-based Homography Estimation Network\nSimilar to the previous unsupervised network architectures [27,35,38,41,44], the\nfinding in Section 3 is obtained by concatenating the image pairs in the channel\ndimension and expecting the network to directly predict the homography. The\nknowledge of homography estimation is implicitly learned without any constraint\nor hint, and hence the potential of our intra-modal self-supervised learning might\nnot be fully explored.\nWith the above consideration, we alter to construct the homography estima-\ntion network with correlation. The architecture of the correlation-based homog-\nraphy estimation network is illustrated in Fig. 3(b). The feature extractor with\nSCPNet\n7\nConsistent \nFeature Map \nProjector\nHomography \nEstimation \nNetwork\nWeight-Sharing Symbol\nFeature Map/Image Warping\nHomography \nEstimation \nNetwork\nHomography \nEstimation \nNetwork\nConsistent \nFeature Map \nProjector\nConsistent \nFeature Map \nProjector\nConsistent \nFeature Map \nProjector\n(a) Training/Inference Framework of SCPNet \n(c) Consistent Feature \nMap Projector\n3×3 Conv.\nRes. Block\nRes. Block\n1×1 Conv.\nFeature \nExtractor\nFeature \nExtractor\nCorrelation\nHomography\nEstimator\n(b) Homography Estimation \nNetwork\nSelf-supervised Homography Generator\nOnly Training Stage\nBoth Training and Inference Stage\nInner Product\nB\nI\nA\nI\n'\nA\nI\n'\nB\nI\nGT,B\nH\nGT,A\nH\n'\nA\nP\nA\nP\nB\nP\n'\nB\nP\nB\nH\nGT,B\nH\nGT,A\nH\nAB\nH\nA\nH\nB,W\nP\nC\nA\nP\nB\nP\nA\nF\nB\nF\nI\nP\nAB\nH\nS\nS\nC\nFig. 3: Schematic diagram of unsupervised cross-modal homography estimation frame-\nwork using intra-modal Self-supervised learning, Correlation, and consistent feature\nmap Projection, namely SCPNet. (a) Overall structure and training/inference strategy\nof SCPNet. (b) Detailed illustration of the correlation-based homography estimation\nnetwork. (c) Detailed structure of the consistent feature map projector.\nshared weights produces the features of the two modalities, namely FA and FB.\nThe correlation is realized by computing the inner product of FA and FB around\na local area, which can be expressed as\n  \\b eg i n {aligned} \\ma t hbf \n{C}( \\ mathbf {x},\\mathbf {r}) = \\mathrm {ReLU}(\\mathbf {F}_\\mathrm {A}(\\mathbf {x})^{\\mathsf {T}} \\mathbf {F}_\\mathrm {B}(\\mathbf {x} + \\mathbf {r})),\\ \\ \\ \\ \\|\\mathbf {r}\\|_{\\infty } \\leq R \\label {eq:correlation_volume} \\end {aligned} \n(4)\nwhere R controls the radius of each local area. The correlation is then sent into\nthe homography estimator to conduct the homography prediction. The structural\ndetails of the homography estimation network can be found in the supplemen-\ntary material. By adopting correlation, the homography estimation network is\nclarified into the weight-shared feature extractor, correlation computation, and\nthe homography estimator. Under the intra-modal self-supervised learning, each\nof the above parts is constrained to learn a clearer knowledge that can be gener-\nalized to cross-modal: 1) the feature extractor is constrained to produce feature\nrepresentations that are effective for correlation, which is indirectly enforced to\nshare the intra-modal self-supervised knowledge to cross-modal; 2) the similar-\nity of features is explicitly encoded by correlation, which is unified among each\nmodality; 3) the knowledge of homography decoding is strictly defined by the\ncorrelation input, which is also unified. The intra-modal self-supervised learning\n8\nR. Zhang et al.\nof the network can then be formulated by\n  \\ mat\nh\nop\n \n{\\arg \\m\nin }_{\\x\ni\n } \n\\\nmathca l \n{L} _\\mat\nh\nrm {S}\\big (\\psi _\\xi (\\mathbf {I}_\\mathrm {A},\\mathbf {I}'_\\mathrm {A}), \\mathbf {H}_\\mathrm {GT,A}\\big ) + \\mathcal {L}_\\mathrm {S}\\big (\\psi _\\xi (\\mathbf {I}_\\mathrm {B},\\mathbf {I}'_\\mathrm {B}), \\mathbf {H}_\\mathrm {GT,B}\\big ), \\label {eq:selfsupervised_corr} \n(5)\nwhere ψξ denotes the correlation-based homography estimation network, with\nthe parameters to be optimized by ξ.\n4.2\nConsistent Feature Map Projector\nAfter introducing intra-modal self-supervised learning, we then consider bringing\nin valid cross-modal supervision to further improve the estimation accuracy.\nAs discussed in Section 3, directly applying intensity-based supervision in Eq.\n2 on cross-modal images with severe content differences is infeasible. To cope\nwith the problem, we introduce consistent feature map projection to assist the\nintensity-based cross-modal supervision, which projects the input images from an\nintensity-variant space to an intensity-invariant latent space. The architecture of\nthe consistent feature map projector is illustrated in Fig. 3(c). The input image\nis processed with a convolutional block of kernel size 3 × 3 first. The produced\nfeature map is then processed by two residual blocks. Finally, the feature map\nwith a higher number of channels is projected into the consistent feature map\nof 1 channel by a 1 × 1 convolutional block.\nBoosted by the consistent feature map projector, the cross-modal intensity-\nbased training can be expressed as\n  \\ mat\nhop\n {\n\\\narg \\mi n }_{\\xi , \\zeta }\\ mathcal \n{\nL}_\\mathrm {C}\\big (\\delta _\\zeta (\\mathbf {I}_\\mathrm {A}), \\mathcal {W}(\\delta _\\zeta (\\mathbf {I}_\\mathrm {B}), \\psi _\\xi (\\delta _\\zeta (\\mathbf {I}_\\mathrm {A}),\\delta _\\zeta (\\mathbf {I}_\\mathrm {B})))\\big ), \\label {eq:cross_proj} \n(6)\nwhere δζ denotes the consistent feature map projector, with ζ denoting its learn-\nable parameters. We note that, with the consistent feature map projector, the\ncross-modal intensity-based learning not only supervises the cross-modal ho-\nmography estimation but also makes the projected feature maps as similar as\npossible, which will further boost the estimation accuracy.\n4.3\nTraining/Inference Framework\nNow that we have separately introduced the intra-modal self-supervised learning,\nthe correlation-based homography estimation network, and the consistent feature\nmap projector with cross-modal intensity-based learning. The complete frame-\nwork of SCPNet can be determined by combining the above learning strategies\nand modules. The training framework of SCPNet contains two self-supervised\nlearning branches and one cross-modal learning branch, which is the most sig-\nnificant difference compared to the previous approaches. The three branches\napply simultaneously supervision on the weight-shared learnable modules as\ndemonstrated in Fig. 3(a). For better illustration, the projected consistent fea-\nture maps are denoted by PA = δζ(IA), PB = δζ(IB), and the warped PB\nby PB,W = W(δζ(IB), ψξ(δζ(IA), δζ(IB))). The predicted cross-modal homogra-\nphy is denoted by HAB = ψξ(δζ(IA), δζ(IB)), and intra-modal ones by HA =\nSCPNet\n9\nψξ(δζ(IA), δζ(I′\nA)) and HB = ψξ(δζ(IB), δζ(I′\nB)). As mentioned in Section 3, for\nthe two self-supervised branches, the input IA and IB are separately deformed\nand trained under the direct supervision of the simulated homography HGT,A\nand HGT,B. Meanwhile, cross-modal intensity-based learning is conducted by\napplying supervision on the projected consistent feature map PA and warped\none PB,W. The correlation-based homography estimation network and consis-\ntent feature map projector are both absorbed to form the network. Finally, the\nentire unsupervised cross-modal learning framework can be formulated as\n  \\ beg\nin \n{a\nl\nigned} \\mathop { \\arg \\min }_{\\xi ,\n\\\nze ta\n \n} \\ \\ & \\m athca\nl {L }_\\ma\nt\nhr m \n{\nC} \\big (\\ delta\n _\\z eta (\n\\\nmathbf {I}_\\mathrm {A}), \\mathcal {W} ( \\delta _\\zeta (\\mathbf {I}_\\mathrm {B}), \\psi _\\xi ( \\delta _\\zeta (\\mathbf {I}_\\mathrm {A}), \\delta _\\zeta (\\mathbf {I}_\\mathrm {B})))\\big )\\\\ +&\\lambda \\mathop {\\mathcal {L}_\\mathrm {S}}_{}\\big (\\psi _\\xi (\\delta _\\zeta (\\mathbf {I}_\\mathrm {A}),\\delta _\\zeta (\\mathbf {I}'_\\mathrm {A})), \\mathbf {H}_\\mathrm {GT,A}\\big )_{\\ }\\\\ + &\\lambda \\mathop {\\mathcal {L}_\\mathrm {S}}_{}\\big (\\psi _\\xi (\\delta _\\zeta (\\mathbf {I}_\\mathrm {B}),\\delta _\\zeta (\\mathbf {I}'_\\mathrm {B})), \\mathbf {H}_\\mathrm {GT,B}\\big ). \\label {eq:full} \\end {aligned} \n(7)\nWe note that once combined, the correlation can also facilitate the projected con-\nsistent feature map to have clear contents with the promotion of the cross-modal\nintensity learning, which will be discussed in Section 5.2. As for the inference\nphase, only the cross-modal prediction branch of SCPNet functions.\n4.4\nLoss Function and Implementation Details\nFor the intra-modal self-supervised loss, we parameterize the homography matrix\nby the offsets of four corner points to stabilize the training [6,8,12]. We use the\nL1 norm on the differences between the predicted offsets O ∈R2×2×2 and the\nground-truth OGT ∈R2×2×2, which can be formulated as:\n  \\ ma t hcal {L}_\\mathrm {S} = \\|\\mathbf {O} - \\mathbf {O}_\\mathrm {GT}\\|_1. \n(8)\nThe homography parameterization using offsets of four corner points can be\nfound in the supplementary material.\nWe set the cross-modal intensity-based loss as follows:\n  \\ mat h cal {L\n}_\\ m athr\nm {C} = \\frac {\\|\\mathbf {P}_\\mathrm {A} - \\mathbf {P}_\\mathrm {B,W}\\|_1}{\\|\\mathbf {P}_\\mathrm {A} - \\mathbf {P}_\\mathrm {B}\\|_1}, \\label {eq:lc} \n(9)\nwhere the numerator minimize the differences between the consistent feature\nmap PA and the warped one PB,W, while the denominator maximizing the dif-\nferences between PA and PB, which can prevent invalid feature map projection.\nWe set λ = 0.1 in Eq. 7 during training. We use the AdamW [31] optimizer,\nwith the maximum learning rate of 4×10−4 for the network training. The batch\nsize is set to 8, with a total of 120000 training iterations.\n5\nExperiments\n5.1\nDatasets and Experimental Settings\nDatasets. We evaluate our SCPNet on cross-modal datasets including GoogleMap\n[45] and Flash/no-flash [21], cross-spectral datasets including Harvard [10] and\n10\nR. Zhang et al.\nRGB/NIR [5], together with the manually-made inconsistent dataset PDS-COCO\n[27]. The GoogleMap dataset contains satellite images and the corresponding\nmap images, which can be used for navigation and geolocation. We use the same\ntraining and test data splitting as in [45]. The Flash/no-flash dataset contains\n120 pairs of images that are with and without flash. We randomly select 60 image\npairs for training and 60 for testing. For multi-spectral data, the Harvard dataset\ncontains 77 real-world image scenes, with each scene containing 31 band images.\nWe take the 16th band image of each scene as the reference image and form a\ncross-spectral image pair with the image of each remaining band respectively.\nThe training and test data are divided by different scenes of 1170 and 1140 image\npairs. For the RGB/NIR dataset, we use 103 pairs of images for training and 153\npairs for testing. PDS-COCO artificially simulates random combined changes in\nbrightness, contrast, saturation, and hue noise to the MS-COCO dataset [30].\nWe use the same training and test splitting as the MS-COCO dataset.\nExperimental Settings. The homography deformation is generated in the\nsame way as [6, 8, 12, 45], which randomly perturbs the four corner points of\na 128 × 128 image. Unless otherwise stated, the perturbation range is set to\n[-32, +32]. We adopt the mean average corner error (MACE) for homography\naccuracy evaluation. Lower MACE indicates higher accuracy.\nComparison Approaches. We evaluate SCPNet on cross-modal and cross-\nspectral datasets with handcrafted approaches including SIFT [32], ORB [36],\nDASC [26], RIFT [29], unsupervised approaches including UDHN [35], CA-\nUDHN [44], biHomE [27], BasesHomo [41], UMF-CMGR [14], and supervised\napproaches including DHN [12], MHN [28], LocalTrans [37], IHN [6], RHWF [8].\nFor SIFT, ORB, DASC, and UMF-CMGR, we choose RANSAC [17] as their ho-\nmography estimation and outlier rejection algorithm. In addition, UMF-CMGR\nis an image fusion approach based on registration, and we use the registration\nnetwork part for comparison. We also tried to compare with the unsupervised\napproaches MU-Net [42] and NeMAR [2], but according to our experiments,\nneither of them performs successful homography estimation. To make a more\ncomprehensive comparison, we also evaluate our SCPNet on PDS-COCO [27].\n5.2\nAblation\nAblation Study on GoogleMap Dataset. We conduct extensive ablation\nstudies on the architecture and supervision of our SCPNet by evaluating the\nmean average corner error (MACE), as shown in Table 1. Using only cross-modal\nintensity-based learning for supervision leads to non-convergence or unsatisfac-\ntory results (Settings 1–4). In contrast, our intra-modal self-supervised learning\nachieves superior performance (Settings 5–9). Moreover, the results of SCPNet\nshow gradual improvement as additional ablation components are incorporated.\nThe Effectiveness of Correlation on Consistent Feature Map Pro-\njection. We further show the consistent feature maps produced by concatenation\nand correlation architecture in Fig. 4. It is observed that the correlation visibly\nfacilitates the consistent feature map generation by the direct constraint of the\nSCPNet\n11\nTable 1: Ablation study of SCPNet. NC denotes the training is not converged. Self\ndenotes intra-modal self-supervised learning, projection denotes consistent feature map\nprojection, and cross denotes cross-modal intensity-based learning.\nSetting\nSelf\nCorrelation\nProjection\nCross\nMACE↓\n1\n%\n%\n%\n!\nNC\n2\n%\n!\n%\n!\nNC\n3\n%\n%\n!\n!\n24.64\n4\n%\n!\n!\n!\n24.80\n5\n!\n%\n%\n%\n13.06\n6\n!\n!\n%\n%\n9.68\n7\n!\n%\n!\n%\n10.01\n8\n!\n!\n!\n%\n7.70\n9\n!\n!\n!\n!\n4.35\nconcatenation\ncorrelation\nA\nI\nB\nI\nFig. 4: Comparison of the consistent feature maps produced by concatenation and\ncorrelation.\ninput feature maps using the inner product. Therefore, cross-modal intensity-\nbased learning can then be boosted by high-quality feature maps.\n5.3\nEvaluation on Cross-modal/spectral Datasets\nWe divide testing image pairs into three levels by the degree of ground-truth\noffsets, and define the 0 ∼30% as ‘Easy’, the 30 ∼60% as ‘Moderate’, and the\n60 ∼100% as ‘Hard’. Table 2 shows the quantitative comparison of cross-modal\ndatasets. On GoogleMap, homography estimation faces greater challenges due to\nthe large modality differences between image pairs. The handcrafted and other\nunsupervised approaches produce unsatisfactory results even under the Easy\nlevel. On the contrary, our SCPNet can produce stable and accurate homog-\nraphy estimation, owning 37.2% and 14.0% lower MACEs than the supervised\napproaches DHN and MHN. On Flash/no-flash, SCPNet also provides the best\nperformance compared to all handcrafted and unsupervised approaches, and is\nsuperior to the supervised DHN and MHN. However, our SCPNet fails to sup-\npress the supervised approaches LocalTrans, IHN and RHWF. It is due to the\nsupervision difference and their architectures that combine iterative and multi-\nscale refinement.\n12\nR. Zhang et al.\nTable 2: Quantitative results of our SCPNet and other approaches on cross-modal\ndatasets. NC denotes the training is not converged. Bold indicates the best result\namong unsupervised methods.\nDataset\nGoogleMap\nFlash/no-flash\nOffset\nEasy\nModerate\nHard\nMean\nEasy\nModerate\nHard\nMean\nHandcrafted\nSIFT [32]\n19.17\n23.87\n29.04\n24.53\n14.61\n18.69\n23.85\n19.53\nORB [36]\n19.11\n23.9\n29.02\n24.52\n16.91\n22.44\n27.01\n22.63\nDASC [26]\n14.29\n20.73\n28.12\n21.76\n11.64\n19.50\n28.11\n20.59\nRIFT [29]\n10.43\n15.46\n21.93\n16.55\n11.22\n13.95\n21.66\n16.21\nUnsupervised\nUDHN [35]\n18.63\n21.55\n26.89\n22.84\n16.27\n21.27\n24.85\n21.20\nCA-UDHN [44]\n19.31\n23.92\n29.10\n24.61\n16.01\n21.54\n25.14\n21.32\nbiHomE [27]\nNC\nNC\nNC\nNC\n8.24\n12.56\n14.04\n11.86\nBasesHomo [41]\n19.43\n23.97\n28.66\n24.49\n19.45\n24.73\n29.66\n25.12\nUMF-CMGR [14]\n19.22\n24.01\n29.02\n24.60\n17.99\n22.43\n28.40\n23.49\nSCPNet (Ours)\n3.60\n4.44\n4.85\n4.35\n1.80\n2.33\n3.59\n2.67\nSupervised\nDHN [12]\n7.06\n6.82\n7.00\n6.93\n5.28\n6.13\n7.51\n6.42\nMHN [28]\n4.75\n5.00\n5.34\n5.06\n3.18\n6.55\n5.81\n5.24\nLocalTrans [37]\n0.91\n1.43\n6.30\n3.22\n0.49\n0.67\n4.05\n1.96\nIHN [6]\n0.70\n0.96\n1.06\n0.92\n0.76\n0.65\n0.94\n0.80\nRHWF [8]\n0.62\n0.68\n0.93\n0.76\n0.79\n0.68\n0.53\n0.65\nTable 3: Quantitative results of our SCPNet and other approaches on cross-spectral\ndatasets. NC denotes the training is not converged. Bold indicates the best result\namong unsupervised methods.\nDataset\nHarvard\nRGB/NIR\nOffset\nEasy\nModerate\nHard\nMean\nEasy\nModerate\nHard\nMean\nHandcrafted\nSIFT [32]\n17.27\n21.49\n26.70\n22.30\n15.54\n23.90\n28.81\n24.40\nORB [36]\n18.61\n23.06\n28.29\n23.82\n17.75\n22.84\n27.01\n23.00\nDASC [26]\n11.85\n18.29\n25.03\n19.05\n13.50\n17.91\n25.73\n19.78\nRIFT [29]\n10.41\n15.69\n21.98\n16.62\n11.22\n13.80\n23.34\n16.84\nUnsupervised\nUDHN [35]\n18.03\n22.20\n26.55\n22.69\n18.54\n23.27\n27.16\n23.43\nCA-UDHN [44]\n18.77\n23.64\n28.55\n24.14\n18.31\n23.88\n28.66\n24.12\nbiHomE [27]\nNC\nNC\nNC\nNC\n18.61\n23.05\n28.18\n23.77\nBasesHomo [41]\n19.77\n24.20\n28.46\n24.57\n19.23\n23.44\n28.89\n24.41\nUMF-CMGR [14]\n16.61\n21.08\n26.25\n21.81\n17.04\n22.16\n26.53\n22.38\nSCPNet (Ours)\n2.34\n3.70\n5.48\n4.00\n1.65\n4.69\n7.13\n4.78\nSupervised\nDHN [12]\n5.30\n6.34\n8.09\n6.72\n9.55\n10.08\n14.87\n11.88\nMHN [28]\n4.37\n5.09\n6.27\n5.35\n6.88\n7.10\n8.26\n7.51\nLocalTrans [37]\n0.27\n0.43\n4.58\n2.04\n0.53\n0.77\n5.15\n2.47\nIHN [6]\n1.40\n1.72\n2.03\n1.75\n1.25\n2.14\n2.21\n1.90\nRHWF [8]\n1.37\n1.76\n1.85\n1.68\n0.68\n1.44\n1.08\n1.07\nSCPNet\n13\nSIFT\nORB\nDASC\nRIFT\nUDHN\nbiHomE\nUMF-CMGR\nDHN\nSCPNet (Ours)\nMHN\nFig. 5: Qualitative homography estimation results on GoogleMap, Flash/no-flash, Har-\nvard, and RGB/NIR datasets respectively. Green polygons denote the ground-truth\nhomography deformation from IB (source, the deformed image) to IA (target). Red\npolygons denote the estimated homography deformation using different algorithms on\nthe target images.\nTable 3 lists the results of the cross-spectral datasets. It is observed that our\nSCPNet consistently surpasses other handcrafted and unsupervised approaches\non Harvard and RGB/NIR datasets. On Harvard, SCPNet outperforms DHN\nand MHN by 40.5%, 25.2% respectively. We note that on Harvard dataset, the\nimages are under intensity and gradient variation caused by the alternation of 31\nspectral bands, but the training strategy of our SCPNet still works robustly. On\nRGB/NIR dataset, SCPNet also outperforms a part of supervised, unsupervised,\nand handcrafted methods as in other datasets.\nFig. 5 visualizes the homography estimation results of SCPNet and other\ncomparison approaches on GoogleMap, Flash/no-flash, Harvard, and RGB/NIR\ndatasets. It can be seen that our SCPNet can produce accurate homography\npredictions on a variety of data, while the others fail due to the large modal-\nity/spectral variance and homography deformation.\n5.4\nEvaluation on PDS-COCO\nWe further conduct an evaluation on PDS-COCO, with results illustrated in\nTable 4. Following [27], δ represents the content distortion of brightness, contrast,\nsaturation, and hue, the absolute value of which is bigger when there is a larger\ndistortion. As the intensity and gradient variation of PDS-COCO dataset is\ninferior to the cross-modal/spectral ones, some unsupervised methods such as\nbiHomE and UDHN produces much more accurate homography estimation than\non the previous datasets. However, they are still inferior to our SCPNet. SCPNet\nleads biHomE by 58.2% under the maximum content distortion and 59.5% under\nthe minimum one, and also outperforms the supervised methods including DHN\nand MHN.\n14\nR. Zhang et al.\nTable 4: Quantitative results of our SCPNet and other approaches on PDS-COCO. δ\nrepresents the content distortion of brightness, contrast, saturation, and hue. NC de-\nnotes the training is not converged. Bold indicates the best result among unsupervised\nmethods.\nDistortion\nUnsupervised\nSupervised\nUDHN CA-UDHN biHomE SCPNet (Ours) DHN MHN LocalTrans IHN RHWF\nδ = ±8\n3.24\nNC\n2.20\n0.89\n2.09\n1.07\n0.68\n0.19\n0.07\nδ = ±16\n5.51\nNC\n2.37\n0.88\n2.24\n1.10\n0.70\n0.19\n0.07\nδ = ±32\nNC\nNC\n2.61\n1.09\n2.50\n1.22\n0.79\n0.21\n0.09\n5.5\nComputational Burden\nThe computational burden of two-branch intra-modal self-supervised learning\nin the training process mainly involves the extra synthetic data generation, the\nnetwork forward propagation, and the computation and backward propagation\nof the self-supervised loss. Table. 5 lists the training time and memory usage on\nan NVIDIA GeForce RTX 4090. Besides, we note that the inference time and\nmemory usage will not increase.\nTable 5: The computational burden of training process. Self denotes intra-modal self-\nsupervised learning.\nSetting\nTime (Hours)\nMemory usage (MBs)\nw/o Self\n3.18\n4622\nw/ Self\n6.98\n9144\n6\nConclusions\nWe have proposed a novel unsupervised cross-modal homography estimation\nframework, named SCPNet. The concept of intra-modal self-supervised learn-\ning is introduced for the first time, wherein two-branch self-supervised informa-\ntion is fully exploited by applying simulated homography within two modalities,\nproviding strong support for unsupervised cross-modal training. Building upon\nthis, by combining correlation and consistent feature map projection, SCPNet\nachieves successful unsupervised homography estimation on multiple challeng-\ning datasets. Extensive experiments demonstrate the effectiveness of SCPNet in\nhandling large offsets and modality gaps.\nLimitations. The homography estimation network of SCPNet is designed\nfor better facilitating the unsupervised training framework. The strategies such\nas multi-scale, iteration, and replacing CNN using transformer that can further\nimprove the homography estimation accuracy at the network design level can be\nfurther investigated in our future work.\nSCPNet\n15\nAcknowledgments\nThis work was supported in part by the National Key Research and Development\nProgram of China under Grant No. 2023YFB3209800, in part by “Pioneer” and\n“Leading Goose” R & D Program of Zhejiang under grant 2023C03136, in part\nby Zhejiang Provincial Natural Science Foundation of China under Grant No.\nLD24F020003, in part by the National Natural Science Foundation of China\nunder Grant No. 62301484.\nReferences\n1. Aguilera, C.A., Sappa, A.D., Toledo, R.: LGHD: A feature descriptor for matching\nacross non-linear intensity variations. In: Proceedings of the IEEE International\nConference on Image Processing. pp. 178–181. IEEE (2015)\n2. Arar, M., Ginger, Y., Danon, D., Bermano, A.H., Cohen-Or, D.: Unsupervised\nmulti-modal image registration via geometry preserving image-to-image transla-\ntion. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition. pp. 13410–13419 (2020)\n3. Barath, D., Matas, J., Noskova, J.: MAGSAC: marginalizing sample consensus.\nIn: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition. pp. 10197–10205 (2019)\n4. Bay, H., Tuytelaars, T., Van Gool, L.: SURF: Speeded up robust features. In: Pro-\nceedings of the European Conference on Computer Vision. pp. 404–417. Springer\n(2006)\n5. Brown, M., Süsstrunk, S.: Multi-spectral SIFT for scene category recognition. In:\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog-\nnition. pp. 177–184 (2011)\n6. Cao, S.Y., Hu, J., Sheng, Z., Shen, H.L.: Iterative deep homography estimation.\nIn: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition. pp. 1879–1888 (2022)\n7. Cao, S.Y., Shen, H.L., Chen, S.J., Li, C.: Boosting structure consistency for mul-\ntispectral and multimodal image registration. IEEE Transactions on Image Pro-\ncessing 29, 5147–5162 (2020)\n8. Cao, S.Y., Zhang, R., Luo, L., Yu, B., Sheng, Z., Li, J., Shen, H.L.: Recurrent\nhomography estimation using homography-guided image warping and focus trans-\nformer. In: Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition. pp. 9833–9842 (2023)\n9. Caruana, R.: Multitask learning. Machine learning 28, 41–75 (1997)\n10. Chakrabarti, A., Zickler, T.: Statistics of real-world hyperspectral images. In: Pro-\nceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recogni-\ntion. pp. 193–200 (2011)\n11. Chen, S.J., Shen, H.L., Li, C., Xin, J.H.: Normalized total gradient: A new mea-\nsure for multispectral image registration. IEEE Transactions on Image Processing\n27(3), 1297–1310 (2017)\n12. DeTone, D., Malisiewicz, T., Rabinovich, A.: Deep image homography estimation.\narXiv preprint arXiv:1606.03798 (2016)\n13. Dharejo, F.A., Zawish, M., Deeba, F., Zhou, Y., Dev, K., Khowaja, S.A., Qureshi,\nN.M.F.: Multimodal-boost: Multimodal medical image super-resolution using\nmulti-attention network with wavelet transform. IEEE/ACM Transactions on\nComputational Biology and Bioinformatics (2022)\n16\nR. Zhang et al.\n14. Di, W., Jinyuan, L., Xin, F., Liu, R.: Unsupervised misaligned infrared and visible\nimage fusion via cross-modality image generation and registration. In: International\nJoint Conference on Artificial Intelligence (2022)\n15. Doersch, C., Zisserman, A.: Multi-task self-supervised visual learning. In: Pro-\nceedings of the IEEE international conference on computer vision. pp. 2051–2060\n(2017)\n16. Dubrofsky, E.: Homography estimation. Diplomová práce. Vancouver: Univerzita\nBritské Kolumbie 5 (2009)\n17. Fischler, M.A., Bolles, R.C.: Random sample consensus: a paradigm for model\nfitting with applications to image analysis and automated cartography. Communi-\ncations of the ACM 24(6), 381–395 (1981)\n18. Girdhar, R., Singh, M., Ravi, N., Van Der Maaten, L., Joulin, A., Misra, I.: Omni-\nvore: A single model for many visual modalities. In: Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition. pp. 16102–16112 (2022)\n19. Goforth, H., Lucey, S.: GPS-denied UAV localization using pre-existing satellite\nimagery. In: 2019 International Conference on Robotics and Automation. pp. 2974–\n2980. IEEE (2019)\n20. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S.,\nCourville, A., Bengio, Y.: Generative adversarial nets. Advances in Neural Infor-\nmation Processing Systems 27 (2014)\n21. He, S., Lau, R.W.: Saliency detection with flash and no-flash image pairs. In: Pro-\nceedings of the European Conference on Computer Vision. pp. 110–124. Springer\n(2014)\n22. Holland, P.W., Welsch, R.E.: Robust regression using iteratively reweighted least-\nsquares. Communications in Statistics-theory and Methods 6(9), 813–827 (1977)\n23. Hong, M., Lu, Y., Ye, N., Lin, C., Zhao, Q., Liu, S.: Unsupervised homography\nestimation with coplanarity-aware GAN. In: Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition. pp. 17663–17672 (2022)\n24. Hu, R., Singh, A.: Unit: Multimodal multitask learning with a unified transformer.\nIn: Proceedings of the IEEE/CVF international conference on computer vision.\npp. 1439–1449 (2021)\n25. Johnson, J., Alahi, A., Fei-Fei, L.: Perceptual losses for real-time style transfer and\nsuper-resolution. In: Proceedings of the European Conference on Computer Vision.\npp. 694–711. Springer (2016)\n26. Kim, S., Min, D., Ham, B., Ryu, S., Do, M.N., Sohn, K.: DASC: Dense adap-\ntive self-correlation descriptor for multi-modal and multi-spectral correspondence.\nIn: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition. pp. 2103–2112 (2015)\n27. Koguciuk, D., Arani, E., Zonooz, B.: Perceptual loss for robust unsupervised ho-\nmography estimation. In: Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition. pp. 4274–4283 (2021)\n28. Le, H., Liu, F., Zhang, S., Agarwala, A.: Deep homography estimation for dynamic\nscenes. In: Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition. pp. 7652–7661 (2020)\n29. Li, J., Hu, Q., Ai, M.: RIFT: Multi-modal image matching based on radiation-\nvariation insensitive feature transform. IEEE Transactions on Image Processing\n29, 3296–3310 (2019)\n30. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P.,\nZitnick, C.L.: Microsoft COCO: Common objects in context. In: Proceedings of\nthe European Conference on Computer Vision. pp. 740–755. Springer (2014)\nSCPNet\n17\n31. Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. arXiv preprint\narXiv:1711.05101 (2017)\n32. Lowe, D.G.: Distinctive image features from scale-invariant keypoints. Interna-\ntional Journal of Computer Vision 60(2), 91–110 (2004)\n33. Marivani, I., Tsiligianni, E., Cornelis, B., Deligiannis, N.: Designing cnns for mul-\ntimodal image restoration and fusion via unfolding the method of multipliers.\nIEEE Transactions on Circuits and Systems for Video Technology 32(9), 5830–\n5845 (2022)\n34. Mur-Artal, R., Montiel, J.M.M., Tardos, J.D.: ORB-SLAM: A versatile and accu-\nrate monocular SLAM system. IEEE Transactions on Robotics 31(5), 1147–1163\n(2015)\n35. Nguyen, T., Chen, S.W., Shivakumar, S.S., Taylor, C.J., Kumar, V.: Unsuper-\nvised deep homography: A fast and robust homography estimation model. IEEE\nRobotics and Automation Letters 3(3), 2346–2353 (2018)\n36. Rublee, E., Rabaud, V., Konolige, K., Bradski, G.: ORB: An efficient alternative\nto sift or surf. In: Proceedings of the IEEE/CVF International Conference on\nComputer Vision. pp. 2564–2571 (2011)\n37. Shao, R., Wu, G., Zhou, Y., Fu, Y., Fang, L., Liu, Y.: LocalTrans: A multiscale local\ntransformer network for cross-resolution homography estimation. In: Proceedings\nof the IEEE/CVF International Conference on Computer Vision. pp. 14890–14899\n(2021)\n38. Wang, C., Wang, X., Bai, X., Liu, Y., Zhou, J.: Self-supervised deep homography\nestimation with invertibility constraints. Pattern Recognition Letters 128, 355–360\n(2019)\n39. Xu, H., Ma, J., Yuan, J., Le, Z., Liu, W.: RFNet: Unsupervised network for mutu-\nally reinforcing multi-modal image registration and fusion. In: Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 19679–\n19688 (2022)\n40. Yasuma, F., Mitsunaga, T., Iso, D., Nayar, S.K.: Generalized assorted pixel camera:\npostcapture control of resolution, dynamic range, and spectrum. IEEE Transac-\ntions on Image Processing 19(9), 2241–2253 (2010)\n41. Ye, N., Wang, C., Fan, H., Liu, S.: Motion basis learning for unsupervised deep ho-\nmography estimation with subspace projection. In: Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision. pp. 13117–13125 (2021)\n42. Ye, Y., Tang, T., Zhu, B., Yang, C., Li, B., Hao, S.: A multiscale framework with\nunsupervised learning for remote sensing image registration. IEEE Transactions\non Geoscience and Remote Sensing 60, 1–15 (2022)\n43. Ying, J., Shen, H.L., Cao, S.Y.: Unaligned hyperspectral image fusion via regis-\ntration and interpolation modeling. IEEE Transactions on Geoscience and Remote\nSensing (2021)\n44. Zhang, J., Wang, C., Liu, S., Jia, L., Ye, N., Wang, J., Zhou, J., Sun, J.: Content-\naware unsupervised deep homography estimation. In: Proceedings of the European\nConference on Computer Vision. pp. 653–669. Springer (2020)\n45. Zhao, Y., Huang, X., Zhang, Z.: Deep Lucas-Kanade homography for multimodal\nimage alignment. In: Proceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition. pp. 15950–15959 (2021)\n46. Zhou, Y., Rangarajan, A., Gader, P.D.: An integrated approach to registration and\nfusion of hyperspectral and multispectral images. IEEE Transactions on Geoscience\nand Remote Sensing 58(5), 3020–3033 (2019)\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2024-07-11",
  "updated": "2024-07-11"
}