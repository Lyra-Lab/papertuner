{
  "id": "http://arxiv.org/abs/2407.08464v2",
  "title": "TLDR: Unsupervised Goal-Conditioned RL via Temporal Distance-Aware Representations",
  "authors": [
    "Junik Bae",
    "Kwanyoung Park",
    "Youngwoon Lee"
  ],
  "abstract": "Unsupervised goal-conditioned reinforcement learning (GCRL) is a promising\nparadigm for developing diverse robotic skills without external supervision.\nHowever, existing unsupervised GCRL methods often struggle to cover a wide\nrange of states in complex environments due to their limited exploration and\nsparse or noisy rewards for GCRL. To overcome these challenges, we propose a\nnovel unsupervised GCRL method that leverages TemporaL Distance-aware\nRepresentations (TLDR). Based on temporal distance, TLDR selects faraway goals\nto initiate exploration and computes intrinsic exploration rewards and\ngoal-reaching rewards. Specifically, our exploration policy seeks states with\nlarge temporal distances (i.e. covering a large state space), while the\ngoal-conditioned policy learns to minimize the temporal distance to the goal\n(i.e. reaching the goal). Our results in six simulated locomotion environments\ndemonstrate that TLDR significantly outperforms prior unsupervised GCRL methods\nin achieving a wide range of states.",
  "text": "TLDR: Unsupervised Goal-Conditioned RL via\nTemporal Distance-Aware Representations\nJunik Bae\nKwanyoung Park\nYoungwoon Lee\nYonsei University\nhttps://heatz123.github.io/tldr\nAbstract: Unsupervised goal-conditioned reinforcement learning (GCRL) is a\npromising paradigm for developing diverse robotic skills without external supervi-\nsion. However, existing unsupervised GCRL methods often struggle to cover a wide\nrange of states in complex environments due to their limited exploration and sparse\nor noisy rewards for GCRL. To overcome these challenges, we propose a novel\nunsupervised GCRL method that leverages TemporaL Distance-aware Representa-\ntions (TLDR). Based on temporal distance, TLDR selects faraway goals to initiate\nexploration and computes intrinsic exploration rewards and goal-reaching rewards.\nSpecifically, our exploration policy seeks states with large temporal distances (i.e.\ncovering a large state space), while the goal-conditioned policy learns to minimize\nthe temporal distance to the goal (i.e. reaching the goal). Our results in six simu-\nlated locomotion environments demonstrate that TLDR significantly outperforms\nprior unsupervised GCRL methods in achieving a wide range of states.\nKeywords: Unsupervised Goal-Conditioned Reinforcement Learning, Temporal\nDistance-Aware Representations\n1\nIntroduction\nHuman babies can autonomously learn goal-reaching skills, starting from controlling their own\nbodies and gradually improving their capabilities to achieve more challenging goals, involving\nlonger-horizon behaviors. Similarly, for intelligent agents like robots, the ability to reach a large set\nof states‚Äìincluding both the environment states and agent states‚Äìis crucial. This capability not only\nserves as a foundational skill set by itself but also enables achieving more complex tasks.\n(a) TLDR (ours)\n(b) METRA\n(c) PEG\nFigure 1: Trajectories (red) of an ant robot in a\ncomplex maze trained by TLDR, METRA [1],\nand PEG [2]. While prior methods yield limited\nexploration, TLDR explores the entire maze.\nCan robots autonomously learn such long-horizon\ngoal-reaching skills like humans? This is partic-\nularly compelling as learning goal-reaching be-\nhaviors in robots is task-agnostic and does not\nrequire any external supervision, offering a scal-\nable approach for unsupervised pre-training of\nrobots [3, 4, 5, 6, 7, 8, 9]. However, prior unsu-\npervised goal-conditioned reinforcement learning\n(GCRL) [10, 2] and unsupervised skill discovery [1]\nmethods exhibit limited coverage of reachable states\nin complex environments, as shown in Figure 1.\nThe major challenges in unsupervised GCRL are\ntwofold: (1) exploring diverse states that the agent\ncan learn to achieve, and (2) effectively learning a goal-reaching policy. Prior unsupervised GCRL\nmethods focus on exploring novel states [11] or states with high uncertainty in next state prediction [10,\n2]. However, discovering unseen states or state transitions may not lead to meaningful states.\n8th Conference on Robot Learning (CoRL 2024), Munich, Germany.\narXiv:2407.08464v2  [cs.LG]  9 Dec 2024\nAdditionally, training a goal-reaching policy to maximize sparse [8] or heuristic [10, 12] goal-reaching\nrewards is often insufficient for long-horizon goal-reaching behaviors in complex environments.\nIn this paper, we propose a novel unsupervised GCRL method that leverages TemporaL Distance-\naware Representations (TLDR) to improve both goal-directed exploration and goal-conditioned\npolicy learning. TLDR uses temporal distance (i.e. the minimum number of environment steps\nbetween two states) induced by temporal distance-aware representations [1, 13, 14] for (1) selecting\nfaraway goals to initiate exploration, (2) learning an exploration policy that maximizes temporal\ndistance, and (3) learning a goal-conditioned policy that minimizes temporal distance to a goal.\nTLDR demonstrates superior state coverage compared to prior unsupervised GCRL and skill dis-\ncovery methods in complex AntMaze environments, as shown in Figure 1. Our ablation studies\nconfirm that our temporal distance-aware approach enhances both goal-directed exploration and\ngoal-conditioned policy learning. Furthermore, our method outperforms prior work across diverse\nlocomotion environments, underscoring its general applicability.\n2\nRelated Work\nUnsupervised goal-conditioned reinforcement learning (GCRL) aims to learn a goal-conditioned\npolicy that can reach diverse goal states without external supervision [15, 16, 10, 2]. The major chal-\nlenges of unsupervised GCRL can be summarized in two aspects: (1) optimizing a goal-conditioned\npolicy and (2) collecting trajectories with novel goals that effectively enlarge its state coverage.\nTo improve the efficiency of goal-conditioned policy learning, hindsight experience reply (HER) [8]\nand model-based policy optimization [10, 12] have been widely used. However, learning complex,\nlong-horizon goal-reaching behaviors remains difficult due to sparse (e.g. whether it reaches the\ngoal [8]) or heuristic rewards (e.g. cosine similarity between the state and goal [10, 12]).\nInstead, temporal distance, defined as the number of environment steps between states estimated from\ndata, can provide more dense and grounded rewards [17, 10, 18, 19]. LEXA [10] and PEG [2] use\nthe expected temporal distances regarding the current policy as goal-reaching rewards [19]. However,\nthis does not reflect the ‚Äúshortest temporal distance‚Äù between states, often leading to sub-optimal\ngoal-reaching behaviors. In this paper, we propose to use the estimated shortest temporal distance as\nreward signals for GCRL, inspired by QRL [14] and HILP [13]. We apply the learned representations\nto compute goal-reaching rewards rather than directly learning the value function in QRL or using it\nfor skill-learning rewards in HILP.\nExploration in unsupervised GCRL relies heavily on selecting exploratory goals that lead an agent\nto novel states and expand state coverage. Exploratory goals can be simply sampled from a replay\nbuffer as in LEXA [10], or can be selected from less visited states [20], states with low-density in\nstate distributions [21, 11], and states with high uncertainty in dynamics [2]. Instead of sampling\nuncertain or less visited states as goals, we select states temporally distant from the visited state\ndistribution as goals, encouraging the discovery of temporally farther away states.\nIn addition to exploratory goal selection, an explicit exploration policy [22, 20] can further encourage\nexploration by maximizing intrinsic rewards, such as uncertainty in dynamics used in LEXA and\nPEG. For better exploration, our approach opts for maximizing temporal distance from the visited\nstates, continuously seeking novel and faraway states.\nUnsupervised skill discovery [23, 24, 25, 26, 27, 28, 29, 1] is another approach to learning diverse\nbehaviors without supervision, yet often lacks robust exploration capabilities [29], requiring manual\nfeature engineering or limiting to low-dimensional state spaces. METRA [1] addresses these limi-\ntations by computing skill-learning rewards with temporal distance-aware representations. While\nachieving remarkable exploration and zero-shot goal-reaching capabilities, METRA exhibits limited\ncoverage in complex environments, as depicted in Figure 1. We find that METRA tends to focus on\nreaching the known farthest states rather than exploring less visited states, whereas our exploration\nstrategy explicitly encourages reaching unseen farther states.\n2\nEncode temporal distances \ninto representations\n(a) Learn representations\nùúô(ùíÆ)\n6\n4\n5\n6\n4\n5\nùíÆ\n: goal\n: visited state\n: starting state\n: new state\nSelect temporally farthest \ngoal from visited states\n(b) Select goal\nLearn to move temporally \ncloser to the goal\n(c) Run GC policy\nGo-phase\nLearn to move temporally \nfarther from visited states \n(d) Run exploration policy\nExplore-phase\nFigure 2: Overview of TLDR algorithm. TLDR leverages temporal distance-aware representations\nfor unsupervised GCRL. (a) We start by learning a state encoder œï(s) that maps states to temporal\ndistance-aware representations. With the temporal distance-aware representations, TLDR (b) selects\nthe temporally farthest state from the visited states as an exploratory goal, (c) reaches the chosen\ngoal using a goal-conditioned policy, which learns to minimize temporal distance to the goal, and\n(d) collects exploratory trajectories using an exploration policy that visits states with large temporal\ndistance from the visited states.\n3\nApproach\nIn this paper, we introduce TemporaL Distance-aware Representations (TLDR), an unsupervised\ngoal-conditioned reinforcement learning (GCRL) method, integrating temporal distance-aware\nrepresentations (Section 3.2) into every facet of the Go-Explore strategy [20] (Section 3.3), as\nillustrated in Figure 2. TLDR first chooses a goal from experience (Section 3.4), reaches the selected\ngoal via the goal-conditioned policy, and executes the exploration policy to gather diverse experiences.\nBoth the exploration policy (Section 3.5) and goal-conditioned policy (Section 3.6) are then trained\non the collected data and rewards computed using the temporal distance-aware representations. We\ndescribe the full algorithm in Algorithm 1 and implementation details in Appendix A.\n3.1\nProblem Formulation\nWe formulate the unsupervised GCRL problem with a goal-conditioned Markov decision process,\ndefined as the tuple M = (S, A, p, G). S and A denote the state and action spaces, respectively.\np : S √ó A ‚Üí‚àÜ(S) denotes the transition dynamics, where ‚àÜ(X) denotes the set of probability\ndistributions over X. The goal of the agent is to learn an optimal goal-conditioned policy œÄG :\nS √ó G ‚ÜíA, where œÄG(a | s, g) outputs an action a ‚ààA that can navigate to the goal g ‚ààGe state\ns within minimum steps. In this paper, we set G = S, allowing any state as a potential goal for the\nagent.\n3.2\nLearning Temporal Distance-Aware Representations\nTemporal distance, defined as the minimum number of environment steps between states, can provide\nmore dense and grounded rewards for goal-conditioned policy learning as well as exploration. For\nGCRL, instead of relying on sparse and binary goal-reaching rewards, the change in temporal distance\nbefore and after taking an action can be an informative learning signal. Moreover, exploration in\nunsupervised GCRL can be incentivized by discovering temporally faraway states.\nTherefore, in this paper, we propose to use temporal distance for unsupervised GCRL. We first\nestimate the temporal distance by learning temporal distance-aware representations, inspired by Park\net al. [13], Wang et al. [14]. The learned representation œï : S ‚ÜíZ encodes the temporal distance\n3\nAlgorithm 1 TLDR: unsupervised goal-conditioned reinforcement learning algorithm\n1: Initialize goal-conditioned policy œÄG\nŒ∏ , exploration policy œÄE\nŒ∏ , temporal distance-aware represen-\ntation œï, and replay buffer D\n2: while not converged do\n3:\ns0 ‚àºp(s0)\n4:\nSample a minibatch B ‚àºD\n5:\ng ‚Üêarg maxs‚ààB(rTLDR(s))\n‚ñ∑Select state with the highest TLDR reward (Eq. (2))\n6:\nfor t = 0, . . . , T ‚àí1 do\n7:\nif t < TG then\n8:\nat ‚àºœÄG\nŒ∏ (¬∑ | st, g)\n‚ñ∑Follow goal-conditioned policy œÄG\nŒ∏ for TG steps\n9:\nelse\n10:\nat ‚àºœÄE\nŒ∏ (¬∑ | st)\n‚ñ∑Explore using exploration policy œÄE\nŒ∏\n11:\nst+1 ‚àºp(¬∑ | st, at)\n12:\nD ‚ÜêD ‚à™{st, at, st+1}\n13:\nTrain representations œï to minimize Lœï in Eq. (1)\n14:\nTrain exploration policy œÄE\nŒ∏ to maximize Eq. (3)\n15:\nTrain goal-conditioned policy œÄG\nŒ∏ using HER with dense reward in Eq. (4)\nbetween two states into the latent space Z, where ‚à•œï(s1) ‚àíœï(s2)‚à•represents the temporal distance\nbetween s1 and s2. This representation is then used across the entire unsupervised GCRL algorithm:\nexploratory goal selection, intrinsic reward for exploration, and reward for a goal-conditioned policy.\nTo train temporal distance-aware representations, we adopt QRL‚Äôs constrained optimization [14]:\nmax\nœï\nEs‚àºps,g‚àºpg [f(‚à•œï(s) ‚àíœï(g)‚à•)]\ns.t.\nE(s,a,s‚Ä≤)‚àºptransition [‚à•œï(s) ‚àíœï(s‚Ä≤)‚à•] ‚â§1,\n(1)\nwhere f is an affine-transformed softplus function that assigns lower weights to larger distances\n‚à•œï(s) ‚àíœï(g)‚à•. We optimize this constrained objective using dual gradient descent with a Lagrange\nmultiplier Œª, and we randomly sample s and g from a minibatch during training.\n3.3\nUnsupervised GCRL with Temporal Distance-Aware Representations\nWith temporal distance-aware representations, we can integrate the concept of temporal distance\ninto unsupervised GCRL. Our approach is built upon the Go-Explore procedure [20], a widely-\nused unsupervised GCRL algorithm comprising two phases: (1) the ‚ÄúGo-phase,‚Äù where the goal-\nconditioned policy œÄG(a | s, g) navigates toward a goal g, and (2) the ‚ÄúExplore-phase,‚Äù where the\nexploration policy œÄE(a | s) gathers new state trajectories to refine the goal-conditioned policy.\nWhile Go-Explore relies on task-specific information for goal selection and executes random actions\nfor exploration, our method uses task-agnostic temporal distance metrics induced by temporal\ndistance-aware representations. The subsequent sections detail how our method leverages the\ntemporal distance-aware representations for selecting goals in the Go-phase (Section 3.4), enhancing\nthe exploration policy (Section 3.5), and facilitating the GCRL policy training (Section 3.6).\n3.4\nExploratory Goal Selection\nFor unsupervised GCRL, selecting low-density (less visited) states as exploratory goals can enhance\ngoal-directed exploration [15, 16]. However, the concept of the ‚Äúdensity‚Äù of a state does not\nnecessarily indicate how rare or hard it is to reach the state. For example, while a robotic arm might\nactively seek out unseen (low-density) joint positions, interacting with objects could offer more\nsignificant learning opportunities [29]. Thus, we propose selecting goals that are temporally distant\nfrom states that are already visited (i.e. in the replay buffer) to explore not only diverse but also\nhard-to-reach states.\nTo sample a faraway goal at the start of each episode, we employ the non-parametric particle-based\nentropy estimator [27] on top of our temporal distance-aware representations. Among states in a\n4\nAnt\nHalfCheetah\nHumanoid-Run\nQuadruped-Escape\nAntMaze-Large\nAntMaze-Ultra\nFigure 3: We evaluate our method on 6 state-based robotic locomotion environments.\nminibatch, we choose N goals with h entropy and collect N corresponding trajectories using the\ngoal-reaching policy. The entropy can be estimated as follows, which we refer to as TLDR reward:\nrTLDR(s) = log\nÔ£´\nÔ£≠1 + 1\nk\nX\nz(j)‚ààNk(œï(s))\n‚à•œï(s) ‚àíz(j)‚à•\nÔ£∂\nÔ£∏,\n(2)\nwhere Nk(¬∑) denotes the k-nearest neighbors around œï(s) within a minibatch.\n3.5\nLearning Exploration Policy\nAfter the goal-conditioned policy navigates towards the chosen goal g for TG steps, the exploration\npolicy œÄE\nŒ∏ is executed to discover states even more distant from the visited states. This objective of\nthe exploration policy can be simply defined as:\nrE(s, s‚Ä≤) = rTLDR(s‚Ä≤) ‚àírTLDR(s).\n(3)\nSimilar to LEXA [10], we alternate between goal-reaching episodes and exploration episodes. For\ngoal-reaching episodes, we execute the goal-conditioned policy until the end of the episodes. For\nexploration episodes, we sample the timestep TG ‚àºUnif(0, T ‚àí1) at the beginning of each episode\nand execute the exploration policy if the current timestep t ‚â•TG.\n3.6\nLearning Goal-Conditioned Policy\nThe goal-conditioned policy aims to minimize the distance to the goal. However, defining ‚Äúdistance‚Äù\nto the goal often requires domain knowledge. Instead, we propose leveraging a task-agnostic metric,\ntemporal distance, as the learning signal for the goal-conditioned policy:\nrG(s, s‚Ä≤, g) = ‚à•œï(s) ‚àíœï(g)‚à•‚àí‚à•œï(s‚Ä≤) ‚àíœï(g)‚à•.\n(4)\nIf our representations accurately capture temporal distances between states, optimizing this reward in\na greedy manner becomes sufficient for learning an optimal goal-reaching policy.\n4\nExperiments\nIn this paper, we propose TLDR, a novel unsupervised GCRL method that utilizes temporal distance-\naware representations for both exploration and optimizing a goal-conditioned policy. Through our\nexperiments, we aim to answer the following three questions: (1) Does TLDR explore better compared\nto other exploration methods? (2) Is our goal-conditioned policy better than prior unsupervised GCRL\nmethods? (3) How crucial is TLDR for goal-conditioned policy learning and exploration?\n4.1\nExperimental Setup\nTasks.\nAs illustrated in Figure 3, we evaluate TLDR in 6 state-based environments: Ant and\nHalfCheetah from OpenAI Gym [30], Humanoid-Run and Quadruped-Escape from DeepMind\nControl Suite (DMC) [31], AntMaze-Large from D4RL [32], and AntMaze-Ultra [33]. For\nHumanoid-Run and Quadruped-Escape, we include the 3D coordinates of the agents in their obser-\nvations. In addition, we also evaluate on two pixel-based environments: Quadruped (Pixel) from\nMETRA [1] and Kitchen (Pixel) from D4RL [32], with the 64 √ó 64 √ó 3 image observation.\n5\nTLDR (Ours)\nMETRA\nPEG\nDisagreement\nAPT\nRND\n0\n3\n6\n9\n12\nTime (hrs)\n0\n10000\n20000\n30000\nState Coverage\n(a) Ant\n0\n3\n6\n9\n12\nTime (hrs)\n0\n100\n200\n300\nState Coverage\n(b) HalfCheetah\n0\n3\n6\n9\n12\nTime (hrs)\n0\n2000\n4000\n6000\nState Coverage\n(c) Humanoid-Run\n0\n3\n6\n9\n12\nTime (hrs)\n0\n150\n300\n450\nState Coverage\n(d) Quadruped-Escape\n0\n3\n6\n9\n12\nTime (hrs)\n0\n300\n600\n900\nState Coverage\n(e) AntMaze-Large\n0\n3\n6\n9\n12\nTime (hrs)\n0\n500\n1000\n1500\nState Coverage\n(f) AntMaze-Ultra\nFigure 4: State coverage in state-based environments. We measure the state coverage of unsuper-\nvised exploration methods. Our method consistently shows superior state coverage compared to other\nmethods, except in HalfCheetah compared against METRA.\nComparisons.\nWe compare our method with 6 prior unsupervised GCRL, skill discovery, and\nexploration methods. For state-based environments, we compare with METRA, PEG, APT, RND,\nand Disagreement. For pixel-based environments, we compare with METRA and LEXA.\n‚Ä¢ METRA [1]: leverages temporal distance-aware representations for skill discovery.\n‚Ä¢ PEG [2]: plans to obtain goals with maximum exploration rewards.\n‚Ä¢ LEXA [10]: uses world model to train an Achiever and Explorer policy.\n‚Ä¢ APT [27]: maximizes the entropy reward estimated from the k-nearest neighbors in a minibatch.\n‚Ä¢ RND [34]: uses the distillation loss of a network to a random target network as rewards.\n‚Ä¢ Disagreement [35]: utilizes the disagreement among an ensemble of world models as rewards.\n4.2\nQuantitative Results\nIn Figure 4, we compare the state coverage during training (i.e. the number of 1 √ó 1 sized (x, y)-\nbins occupied by any of the training trajectories). TLDR outperforms all prior works, except in\nHalfCheetah compared to METRA. METRA learns low-dimensional skills and focuses on extending\nthe temporal distance along a few directions specified by the skills, providing a strong inductive bias\nfor simple locomotion tasks like HalfCheetah. On the other hand, TLDR achieves much larger state\ncoverage in complex environments than METRA, including AntMaze-Large, AntMaze-Ultra, and\nTLDR (Ours)\nMETRA\nPEG\n0\n3\n6\n9\n12\nTime (hrs)\n0\n20\n40\n60\nGoal Distance (‚Üì)\n(a) Ant\n0\n3\n6\n9\n12\nTime (hrs)\n0\n20\n40\n60\nGoal Distance (‚Üì)\n(b) HalfCheetah\n0\n3\n6\n9\n12\nTime (hrs)\n0\n15\n30\n45\nGoal Distance (‚Üì)\n(c) Humanoid-Run\n0\n3\n6\n9\n12\nTime (hrs)\n0\n2\n4\n6\n# Reached Goals (‚Üë)\n(d) AntMaze-Large\n0\n3\n6\n9\n12\nTime (hrs)\n0\n7\n14\n21\n# Reached Goals (‚Üë)\n(e) AntMaze-Ultra\nFigure 5: Goal-reaching metrics of a goal-conditioned policy. For (a) Ant, (b) HalfCheetah, and\n(c) Humanoid-Run, we report the average distance between goals and the last states of trajectories\n(lower is better). TLDR achieves a comparable average goal distance to METRA. For AntMaze\nenvironments, we report the number of pre-defined goals reached by a goal-reaching policy (7 for\n(d) AntMaze-Large and 21 for (e) AntMaze-Ultra), and TLDR significantly outperforms prior works.\n6\nTLDR (Ours)\nMETRA\nLEXA\n0\n4\n8\n12\n16\nTime (hrs)\n0\n300\n600\n900\n1200\nState Coverage (‚Üë)\n0\n4\n8\n12\n16\nTime (hrs)\n0\n5\n10\n15\nGoal Distance (‚Üì)\n(a) Quadruped (Pixel)\n0\n3\n6\n9\n12\nTime (hrs)\n0\n2\n4\n6\nState Coverage (‚Üë)\n0\n3\n6\n9\n12\nTime (hrs)\n0\n2\n4\n6\n# Achieved Tasks (‚Üë)\n(b) Kitchen (Pixel)\nFigure 6: Results in pixel-based environments. We compare TLDR with prior works in pixel-based\nQuadruped and Kitchen environments. In Quadruped (Pixel), TLDR demonstrates a slow learning\nspeed compared to METRA and LEXA. For Kitchen (Pixel), TLDR could interact with all six objects\nduring training but shows low success rates for evaluation.\nQuadruped-Escape, where all other methods struggle and only explore limited regions. This shows\nthe strength of our method in the exploration of complex environments.\nWe then compare the goal-reaching performance of TLDR with PEG and METRA in Figure 5 by\nmeasuring the average distance between goals and the last states of trajectories. The results in\nFigures 5a to 5c show that TLDR can navigate towards the given goals closer than or at least on par\nwith METRA. Figures 5d and 5e show that TLDR is the only method that can navigate towards a\nvarious set of goals in both mazes, demonstrating its superior exploration and goal-conditioned policy\nlearning with temporal distance.\nIn Appendix B, we further show the comparisons in environment steps, not in hours. PEG shows\nbetter sample efficiency in relatively low-dimensional or easy-exploration tasks, such as Ant and\nHalfCheetah. However, the state coverages of PEG quickly converge to narrower regions, especially\nin AntMazes, than those of TLDR. METRA generally shows worse sample efficiency than TLDR.\nFigure 6 shows the results in pixel-based environments. In Quadruped (Pixel), TLDR explores diverse\nregions but learns slower than LEXA and METRA. For Kitchen (Pixel), TLDR interacts with all\nsix objects during training, but struggles at learning the goal-conditioned policy. Further analysis in\nAppendix F suggests that the performance bottleneck is related to goal-conditioned policy learning\nwith pixel observations. We leave more detailed analyses for future works.\n4.3\nQualitative Results\n(a) TLDR (ours)\n(b) METRA\n(c) PEG\nFigure 7: TLDR can cover more goals compared\nto METRA and PEG in AntMaze-Ultra.\nFigure 7 visualizes the learned goal-reaching\nbehaviors on the AntMaze-Ultra environment.\nTLDR can successfully reach both near and far-\naway goals in diverse regions. On the other\nhand, METRA and PEG fail to navigate to di-\nverse goals. METRA could reach some goals\ndistant from the initial position, whereas PEG\nfails to reach temporally faraway goals. This\nclearly shows the benefit of using temporal dis-\ntance in unsupervised GCRL. More qualitative\nresults can be found in Appendix D.\n4.4\nAblation Studies\nTo investigate the importance of temporal distance-aware representations in our algorithm, we conduct\nablation studies on GCRL reward designs and exploration strategies.\nGCRL reward design.\nWe compare with three different goal-conditioned policy learning methods:\n(1) QRL [14], which learns a quasimetric value function and latent dynamics model, (2) sparse\nHER [8], which uses the sparse goal-reaching reward ‚àí1(s Ã∏= g), and (3) DDL [19], which uses\n7\nTLDR\nQRL\nHER\nDDL\n0\n3\n6\n9\n12\nTime (hrs)\n0\n10000\n20000\n30000\nState Coverage\nAnt\n0\n3\n6\n9\n12\nTime (hrs)\n0\n300\n600\n900\nState Coverage\nAntMaze-Large\n(a) TLDR with different GCRL rewards\nTLDR\nAPT\nRND\nDisagreement\n0\n3\n6\n9\n12\nTime (hrs)\n0\n10000\n20000\n30000\nState Coverage\nAnt\n0\n3\n6\n9\n12\nTime (hrs)\n0\n300\n600\n900\nState Coverage\nAntMaze-Large\n(b) TLDR with different exploration methods\nFigure 8: We evaluate our method with different design choices for (a) GCRL rewards and (b) ex-\nploration methods on Ant and AntMaze-Large. TLDR shows better state coverages than its ablated\nversions in both ablation studies, indicating the importance of using temporal distance-aware repre-\nsentations for both exploration and GCRL.\nexpected temporal distances as rewards. Figure 8a and Figure 21 show the superior performance\nof our temporal distance-based GCRL reward over HER and DDL, suggesting the importance of\nusing optimal temporal distance as a dense reward signal. Furthermore, although QRL learns a value\nfunction that preserves optimal temporal distances, it struggles to learn an effective goal-reaching\npolicy. Unlike QRL, which directly uses the learned value function along with an additional latent\ndynamics model, TLDR leverages temporal distance-aware representations to compute dense rewards\nfor the goal-conditioned policy and shows better performance. In Appendix G, we show that this\ntrend also holds with a fixed dataset, which ignores the effect of exploration and only compares\ngoal-reaching reward designs for goal-reaching performances.\nExploration strategy.\nFor goal selection and exploration rewards, we replace TLDR reward in\nEquation (2), with other exploration bonuses: APT (with ICM [36] representations), RND, and\nDisagreement. Note that goal-conditioned policies are still trained with the same temporal distance-\nbased rewards as TLDR, thereby comparing only exploration strategies. As shown in Figure 8b, using\nTLDR reward for goal selection and exploration rewards achieves significantly higher performance\nthan other exploration bonuses. This result implies that our temporal distance-based rewards are\neffective for unsupervised exploration.\n5\nConclusion\nIn this paper, we introduce TLDR, an unsupervised GCRL algorithm that incorporates temporal\ndistance-aware representations. TLDR leverages temporal distance for exploration and learning the\ngoal-reaching policy. By pursuing states with larger temporal distances, TLDR can continuously\nexplore challenging regions, achieving better state coverage. The experimental results demonstrate\nthat TLDR can cover significantly larger state spaces across diverse environments than existing\nunsupervised RL algorithms.\nLimitations.\nWhile TLDR achieves remarkable state coverages, it still has several limitations:\n‚Ä¢ TLDR shows a slow learning speed compared to METRA in pixel-based environments. Our\nanalysis in Appendix F demonstrates the need for further research in representation learning and\nGCRL for pixel observations.\n‚Ä¢ Our temporal distance-aware representations do not capture asymmetric temporal distance\nbetween states, which can make policy learning challenging for highly asymmetric environments.\n‚Ä¢ Applying unsupervised RL to real robots has many challenges, including safety. While not\ntested on real robots, our preliminary results in Appendix E indicate that combining TLDR with\nsafety-aware techniques [37, 38] is a promising future direction for real robotic systems.\n‚Ä¢ TLDR achieves high efficiency in terms of wall clock time, but not in terms of sample efficiency,\nas shown in Appendix B. We believe that increasing the update-to-data ratio or using model-\nbased RL could enhance the sample efficiency of our method.\n8\nAcknowledgments\nThis work was supported in part by the Institute of Information & Communications Technology\nPlanning & Evaluation (IITP) grant (RS-2020-II201361, Artificial Intelligence Graduate School\nProgram (Yonsei University)), the National Research Foundation of Korea (NRF) grant (RS-2024-\n00333634), and the Electronics and Telecommunications Research Institute (ETRI) grant (24ZR1100)\nfunded by the Korean Government (MSIT).\nReferences\n[1] S. Park, O. Rybkin, and S. Levine. Metra: Scalable unsupervised rl with metric-aware abstrac-\ntion. In International Conference on Learning Representations, 2024.\n[2] E. S. Hu, R. Chang, O. Rybkin, and D. Jayaraman.\nPlanning goals for exploration.\nIn\nInternational Conference on Learning Representations, 2022.\n[3] L. P. Kaelbling. Learning to achieve goals. In International Joint Conference on Artificial\nIntelligence, volume 2, pages 1094‚Äì8, 1993.\n[4] K. Deguchi and I. Takahashi. Image-based simultaneous control of robot and target object\nmotions by direct-image-interpretation method. In IEEE/RSJ International Conference on\nIntelligent Robots and Systems, pages 375‚Äì380, 1999.\n[5] T. Schaul, D. Horgan, K. Gregor, and D. Silver. Universal value function approximators. In\nInternational Conference on Machine Learning, pages 1312‚Äì1320, 2015.\n[6] M. Watter, J. Springenberg, J. Boedecker, and M. Riedmiller. Embed to control: A locally\nlinear latent dynamics model for control from raw images. In Advances in Neural Information\nProcessing Systems, pages 2746‚Äì2754, 2015.\n[7] C. Finn, X. Y. Tan, Y. Duan, T. Darrell, S. Levine, and P. Abbeel. Deep spatial autoencoders for\nvisuomotor learning. In IEEE International Conference on Robotics and Automation, pages\n512‚Äì519. IEEE, 2016.\n[8] M. Andrychowicz, F. Wolski, A. Ray, J. Schneider, R. Fong, P. Welinder, B. McGrew, J. Tobin,\nO. Pieter Abbeel, and W. Zaremba. Hindsight experience replay. In Advances in Neural\nInformation Processing Systems, volume 30, 2017.\n[9] Y. Zhu, R. Mottaghi, E. Kolve, J. J. Lim, A. Gupta, L. Fei-Fei, and A. Farhadi. Target-driven\nvisual navigation in indoor scenes using deep reinforcement learning. In IEEE International\nConference on Robotics and Automation, pages 3357‚Äì3364, 2017.\n[10] R. Mendonca, O. Rybkin, K. Daniilidis, D. Hafner, and D. Pathak. Discovering and achieving\ngoals via world models. In Neural Information Processing Systems, 2021.\n[11] S. Pitis, H. Chan, S. Zhao, B. Stadie, and J. Ba. Maximum entropy gain exploration for long\nhorizon multi-goal reinforcement learning. In International Conference on Machine Learning,\npages 7750‚Äì7761. PMLR, 2020.\n[12] D. Hafner, K.-H. Lee, I. Fischer, and P. Abbeel. Deep hierarchical planning from pixels. In\nNeural Information Processing Systems, volume 35, pages 26091‚Äì26104, 2022.\n[13] S. Park, T. Kreiman, and S. Levine. Foundation policies with hilbert representations. In\nInternational Conference on Machine Learning, 2024.\n[14] T. Wang, A. Torralba, P. Isola, and A. Zhang. Optimal goal-reaching reinforcement learning via\nquasimetric learning. In International Conference on Machine Learning, pages 36411‚Äì36430.\nPMLR, 2023.\n9\n[15] V. H. Pong, M. Dalal, S. Lin, A. Nair, S. Bahl, and S. Levine. Skew-Fit: State-covering\nself-supervised reinforcement learning. In International Conference on Machine Learning,\n2020.\n[16] S. Pitis, H. Chan, S. Zhao, B. C. Stadie, and J. Ba. Maximum entropy gain exploration for long\nhorizon multi-goal reinforcement learning. In International Conference on Machine Learning,\n2020.\n[17] Y. Lee, S.-H. Sun, S. Somasundaram, E. S. Hu, and J. J. Lim. Composing complex skills by\nlearning transition policies. In International Conference on Learning Representations, 2019.\nURL https://openreview.net/forum?id=rygrBhC5tQ.\n[18] Y. Lee, A. Szot, S.-H. Sun, and J. J. Lim. Generalizable imitation learning from observation via\ninferring goal proximity. In Neural Information Processing Systems, 2021.\n[19] K. Hartikainen, X. Geng, T. Haarnoja, and S. Levine. Dynamical distance learning for semi-\nsupervised and unsupervised skill discovery. In International Conference on Learning Repre-\nsentations, 2020.\n[20] A. Ecoffet, J. Huizinga, J. Lehman, K. O. Stanley, and J. Clune. First return, then explore.\nNature, 590(7847):580‚Äì586, 2021.\n[21] V. H. Pong, M. Dalal, S. Lin, A. Nair, S. Bahl, and S. Levine. Skew-fit: State-covering\nself-supervised reinforcement learning. In International Conference on Machine Learning,\n2020.\n[22] R. Sekar, O. Rybkin, K. Daniilidis, P. Abbeel, D. Hafner, and D. Pathak. Planning to explore\nvia self-supervised world models. In International Conference on Machine Learning, 2020.\n[23] K. Gregor, D. J. Rezende, and D. Wierstra. Variational intrinsic control. ArXiv, abs/1611.07507,\n2016.\n[24] J. Achiam, H. Edwards, D. Amodei, and P. Abbeel. Variational option discovery algorithms.\nArXiv, abs/1807.10299, 2018.\n[25] B. Eysenbach, A. Gupta, J. Ibarz, and S. Levine. Diversity is all you need: Learning skills\nwithout a reward function. In International Conference on Learning Representations (ICLR),\n2019.\n[26] A. Sharma, S. Gu, S. Levine, V. Kumar, and K. Hausman. Dynamics-aware unsupervised\ndiscovery of skills. In International Conference on Learning Representations (ICLR), 2020.\n[27] H. Liu and P. Abbeel. Behavior from the void: Unsupervised active pre-training. In Neural\nInformation Processing Systems, 2021.\n[28] S. Park, J. Choi, J. Kim, H. Lee, and G. Kim. Lipschitz-constrained unsupervised skill discovery.\nIn International Conference on Learning Representations, 2022.\n[29] S. Park, K. Lee, Y. Lee, and P. Abbeel. Controllability-aware unsupervised skill discovery. In\nInternational Conference on Machine Learning, pages 27225‚Äì27245. PMLR, 2023.\n[30] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba.\nOpenAI Gym. ArXiv, abs/1606.01540, 2016.\n[31] Y. Tassa, Y. Doron, A. Muldal, T. Erez, Y. Li, D. de Las Casas, D. Budden, A. Abdolmaleki,\nJ. Merel, A. Lefrancq, T. P. Lillicrap, and M. A. Riedmiller. Deepmind control suite. arXiv\npreprint arXiv:1801.00690, 2018.\n[32] J. Fu, A. Kumar, O. Nachum, G. Tucker, and S. Levine. D4rl: Datasets for deep data-driven\nreinforcement learning. arXiv preprint arXiv:2004.07219, 2020.\n10\n[33] Z. Jiang, T. Zhang, M. Janner, Y. Li, T. Rockt¬®aschel, E. Grefenstette, and Y. Tian. Efficient plan-\nning in a compact latent action space. In International Conference on Learning Representations,\n2023.\n[34] Y. Burda, H. Edwards, A. J. Storkey, and O. Klimov. Exploration by random network distillation.\nIn International Conference on Learning Representations, 2019.\n[35] D. Pathak, D. Gandhi, and A. K. Gupta. Self-supervised exploration via disagreement. In\nInternational Conference on Machine Learning, 2019.\n[36] D. Pathak, P. Agrawal, A. A. Efros, and T. Darrell. Curiosity-driven exploration by self-\nsupervised prediction. In International conference on machine learning, pages 2778‚Äì2787.\nPMLR, 2017.\n[37] K. Srinivasan, B. Eysenbach, S. Ha, J. Tan, and C. Finn. Learning to be safe: Deep rl with a\nsafety critic. arXiv preprint arXiv:2010.14603, 2020.\n[38] S. Kim, J. Kwon, T. Lee, Y. Park, and J. Perez. Safety-aware unsupervised skill discovery. In\nIEEE International Conference on Robotics and Automation, pages 894‚Äì900. IEEE, 2023.\n[39] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine. Soft actor-critic: Off-policy maximum entropy\ndeep reinforcement learning with a stochastic actor. In International Conference on Machine\nLearning, 2018.\n[40] M. Laskin, D. Yarats, H. Liu, K. Lee, A. Zhan, K. Lu, C. Cang, L. Pinto, and P. Abbeel. Urlb:\nUnsupervised reinforcement learning benchmark. In Neural Information Processing Systems\n(NeurIPS) Datasets and Benchmarks Track, 2021.\n[41] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In International\nConference on Learning Representations, 2015.\n[42] J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization. arXiv preprint arXiv:1607.06450,\n2016.\n[43] A. Gupta, V. Kumar, C. Lynch, S. Levine, and K. Hausman. Relay policy learning: Solving\nlong-horizon tasks via imitation and reinforcement learning. In Conference on Robot Learning,\n2019.\n[44] K. Zakka, Y. Tassa, and MuJoCo Menagerie Contributors. MuJoCo Menagerie: A collection of\nhigh-quality simulation models for MuJoCo, Sept. 2022.\n[45] J.-B. Grill, F. Strub, F. Altch¬¥e, C. Tallec, P. Richemond, E. Buchatskaya, C. Doersch,\nB. Avila Pires, Z. Guo, M. Gheshlaghi Azar, et al. Bootstrap your own latent-a new ap-\nproach to self-supervised learning. Neural Information Processing Systems, 33:21271‚Äì21284,\n2020.\n[46] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton. A simple framework for contrastive learning\nof visual representations. In International Conference on Machine Learning, pages 1597‚Äì1607.\nPMLR, 2020.\n[47] D. Hafner, T. P. Lillicrap, J. Ba, and M. Norouzi. Dream to control: Learning behaviors by\nlatent imagination. In International Conference on Learning Representations, 2020.\n11\nA\nTraining Details\nA.1\nComputing Resources and Experiments\nAll experiments are done on a single RTX 4090 GPU and 4 CPU cores. Each state-based experiment\ntakes 12 hours for all methods, following METRA [1], which trains each method for 10-12 hours.\nWe report the number of environment steps used for the methods in our experiments in Table 1. We\nuse 5 random seeds for all experiments and report the mean and standard deviation of the results.\nTable 1: The number of environment steps for experiments.\nEnvironment\nTLDR\nMETRA\nPEG\nLEXA\nAPT\nRND\nDisagreement\nAnt\n56.5M\n83.2M\n0.7M\n-\n2.4M\n4.1M\n4.8M\nHalfCheetah\n51.4M\n103.5M\n0.7M\n-\n2.5M\n4.2M\n5.0M\nAntMaze-Large\n42.6M\n62.5M\n0.7M\n-\n2.4M\n6.4M\n5.0M\nAntMaze-Ultra\n31.2M\n44.5M\n0.6M\n-\n2.4M\n4.5M\n3.4M\nQuadruped-Escape\n28.0M\n34.8M\n0.6M\n-\n2.2M\n4.5M\n4.4M\nHumanoid-Run\n40.8M\n59.9M\n0.6M\n-\n3.5M\n4.7M\n4.7M\nQuadruped (Pixel)\n3.9M\n4.1M\n-\n2.1M\n-\n-\n-\nKitchen (Pixel)\n1.1M\n1.7M\n-\n1.0M\n-\n-\n-\nA.2\nImplementation Details\nOur method, TLDR, is implemented on top of the official implementation of METRA. Similar to\nMETRA, we use SAC [39] for learning the goal-reaching policy and exploration policy. We train our\ntemporal distance-aware representation œï(s) by maximizing the following objective:\nEs‚àºps,g‚àºpg [f(‚à•œï(s) ‚àíœï(g)‚à•) + Œª ¬∑ min (œµ, 1 ‚àí‚à•œï(s) ‚àíœï(s‚Ä≤)‚à•)] ,\n(5)\nwhere f is an affine-transformed softplus function:\nf(x) = ‚àísoftplus(500 ‚àíx, Œ≤ = 0.01),\n(6)\nwhich prevents the distances ‚à•œï(s) ‚àíœï(g)‚à•from diverging, following QRL [14].\nFor training the exploration policy, we normalize the TLDR reward used in Equation (3) to keep the\nrewards on a consistent scale. We simply divide the TLDR reward by a running estimate of its mean\nvalue, following APT [27].\nFor METRA, PEG, and LEXA, we use their official implementation. For random exploration\napproaches (APT, RND, Disagreement), we use the implementation from URLB [40].\nA.3\nHyperparameters\nThe hyperparameters used in our experiments are summarized in Table 2.\nFor METRA, we use 2-D continuous skills for Ant, 16-D discrete skills for HalfCheetah, 24-D\ndiscrete skills for Kitchen (Pixel), and 4-D continuous skills for other environments. We use the batch\nsize of 1024 for state-based environments and 256 for pixel-based environments. We set the number\nof gradient steps per epoch for each experiment to be the same as ours. We use the default values for\nthe remaining hyperparameters. To perform goal-reaching tasks with METRA, we set the skill z as\nœï(g)‚àíœï(s)\n‚à•œï(g)‚àíœï(s)‚à•for continuous skills or arg maxdim (œï(g) ‚àíœï(s)) for discrete skills.\nIn PEG, we use the same hyperparameters used in their ntMaze experiments. Since PEG uses the\nnormalized goal space, we measure the range of the observations and normalize the goal states\naccording to the minimum and maximum range.\nIn LEXA, we follow their hyperparameters and opt for the temporal distance reward for training the\nAchiever policy.\n12\nFor APT (with ICM encoder), RND, and Disagreement, we use the same hyperparameters as in\nURLB [40].\nFor the ablation with QRL, we use the learning rate of 0.0003 for the critic. We use an (input\ndim)-1024-1024-128 network for the encoder, 256-1024-2048 for the projector, IQE-maxmean head\nof 64 components of size 32, and 128-1024-1024-128 for the latent dynamics model. The transition\nloss is weighted by 1. For HER, we use the discount factor Œ≥ = 0.99.\nTable 2: List of hyperparameters.\nHyperparameter\nValue\nLearning rate\n0.0001\nLearning rate for œï\n0.0005\nBatch size\n1024 (State), 256 (Pixel)\nReplay buffer size\n106 (State), 3 √ó 105 (Quadruped (Pixel)), 105 (Kitchen)\nFrame stack (Pixel)\n3\nOptimizer\nAdam [41]\nRelaxation constant œµ in Eq. (5)\n10‚àí3\ndim œï(s)\n8 (Kitchen), 4 (Others)\nk in Eq. (2)\n12\nInitial Œª\n3 √ó 103\nSAC entropy coefficient\n0.01 (Kitchen), target entropy as (‚àídim A)/2 (others)\nDiscount factor Œ≥\n0.97 (Goal-reaching policy), 0.99 (Exploration policy)\nNormalization\nLayerNorm [42] for the critics, None for œï and actors\nEncoder for image observations\nCNN\nMLP dimensions\n1024\nMLP depths\n2\nGoal relabelling\n0.8 (sampled from future observations), 0.2 (no relabelling)\n# of gradient steps per epoch\n50 (Ant, HalfCheetah, Humanoid-Run, Quadruped-Escape),\n75 (AntMaze-Large), 100 (Kitchen (Pixel)), 150 (AntMaze-Ultra),\n200 (Quadruped (Pixel))\n# of episode rollouts per epoch\n8\nœÑ for updating the target network\n0.995\nA.4\nEnvironment Details\nAnt.\nWe use the MuJoCo Ant environment in OpenAI gym [30]. The observation space is 29-D\nand the action space is 8-D. Following METRA, we normalize the observations for Ant with a fixed\nmean and standard deviation of observations computed from randomly generated trajectories. The\nepisode length is 200.\nHalfCheetah\nWe use the MuJoCo HalfCheetah environment in OpenAI gym [30]. The observation\nspace is 18-D and the action space is 6-D. Following METRA, we normalize the observations for\nHalfCheetah with a fixed mean and standard deviation of observations from randomly generated\ntrajectories. The episode length is 200.\nHumanoid-Run.\nWe use the Humanoid-Run task from DeepMind Control Suite [31]. The global\nx, y, z coordinates of the agent are added to the observation. Humanoid has 55-D observation space\nwith 21-D action space. The episode length is 200.\nQuadruped-Escape.\nQuadruped-Escape is included in DeepMind Control Suite [31].\nThe\nquadruped robot is initialized in a basin surrounded by complex terrains. Due to the complex\nterrains, moving further away from the initial position is challenging. Similar to the AntMaze\nenvironments, we fix the terrain shape. Also, we add the global x, y, z coordinates of the agent to the\nobservation. Quadruped-Escape has 104-D observation space with 12-D action space. The episode\nlength is 200.\n13\nAntMaze-Large.\nWe use antmaze-large-play-v2 in D4RL [32]. The observation and action\nspaces are the same as the Ant environment. The episode length is 300. To make exploration more\nchallenging, we fix the initial location of the agent to be the bottom right corner of the maze, as\nshown in Figure 3 (AntMaze-Large).\nAntMaze-Ultra.\nWe use antmaze-ultra-play-v0 proposed by Jiang et al. [33]. The observation\nand action spaces are the same as the Ant environment. The episode length is 600. Similar to AntMaze-\nLarge, we fix the initial location of the agent to be the bottom right corner of the maze, as shown in\nFigure 3 (AntMaze-Ultra).\nQuadruped (Pixel).\nWe use the pixel-based version of the Quadruped environment [31] used in\nMETRA [1]. Specifically, we use the image size of 64 √ó 64 √ó 3 with episode length of 200.\nKitchen (Pixel).\nWe use the pixel-based version of the Kitchen environment [43] used in ME-\nTRA [1] and LEXA [10]. Specifically, we use the image size of 64 √ó 64 √ó 3 with the episode length\nof 50. The action space has 9 dimensions.\nA.5\nEvaluation Protocol\nFor Ant, Humanoid, and Quadruped (Pixel), we sample goals with (x, y)-coordinates from [‚àí50, 50]2,\n[‚àí40, 40]2, and [‚àí15, 15]2, respectively. For the rest of the goal state (e.g. joint poses), we use the\ninitial robot configuration following Park et al. [1].\nFor HalfCheetah, we sample goals with x-coordinates from [‚àí100, 100].\nFor AntMaze-Large and AntMaze-Ultra, we use the pre-defined goals as shown in Figure 7. A goal\nis deemed to be reached when an ant gets closer than 0.5 to the goal.\nFor Kitchen (Pixel), we use the same 6 single-task goal images used in LEXA [10], which consist of\ninteractions with Kettle, Microwave, Light switch, Hinge cabinet, Slide cabinet, and Bottom burner.\nWe report the total number of achieved tasks during evaluation.\nFor all environments, we use a full state as a goal. Specifically, for state-based observations, we\nuse the observation upon reset as the base observation and switch the x, y coordinates (or x for\nHalfCheetah) to the corresponding dimensions. For Quadruped (Pixel), we render the image of the\nstate where the agent is at the goal position and use it as the goal.\nFor each environment, state coverage is calculated by the number of 1 √ó 1-sized (x, y)-bins ((x)-bins\nfor HalfCheetah) occupied by any of the training trajectories. For Kitchen (Pixel), the state coverage\nis calculated as the number of tasks achieved at least once during the last 100000 environment steps.\nB\nSample Efficiency Comparison\nWe compare the sample efficiency of TLDR, METRA and PEG under the same setting as in Sec-\ntion 4.2. Since PEG requires longer training time for the same amount of environment steps compared\nto TLDR, PEG is trained for more than 72 hours, while TLDR and METRA are each trained for 12\nhours.\nFigures 9 and 10 illustrate the state coverage and goal-reaching metrics with respect to the number\nof environment steps used for training. TLDR exhibits superior state coverages and goal-reaching\nperformance in hard-exploration tasks like AntMazes. In contrast, PEG tends to be more sample-\nefficient in environments with relatively low-dimensional state and action spaces, such as Ant and\nHalfCheetah, but it quickly converges to narrower regions in other environments that require hard-\nexploration (AntMazes) or have higher-dimensional state and action spaces (Quadruped-Escape,\nHumanoid-Run). METRA shows worse sample efficiency overall compared to TLDR.\nPEG‚Äôs exploration via latent disagreement may over-prioritize less critical dimensions of the state\nspaces (e.g., joint angles) and may not scale well to high-dimensional observation spaces. Additionally,\n14\nits reliance on expected temporal distances can be less effective for training a goal-reaching policy\nthan TLDR‚Äôs optimal temporal distances, as shown in Section 4.4. Moreover, METRA‚Äôs skill learning\nobjective can incentivize revisiting known distant states rather than exploring new ones, leading to\nsuboptimal convergence.\nEfficient exploration in high-dimensional spaces remains a major challenge in learning complex\nreal-world tasks. Unlike other methods that quickly converge to suboptimal solutions in these settings,\nTLDR effectively handles this challenge and continues to improve steadily. We believe that increasing\nthe update-to-data ratio or incorporating model-based reinforcement learning approaches could\nfurther enhance TLDR‚Äôs sample efficiency.\nTLDR (Ours)\nMETRA\nPEG\n0\n1\n2\n3\n4\nSteps (M)\n0\n3000\n6000\n9000\n0\n25\n50\n75\n100\nSteps (M)\n0\n10000\n20000\n30000\nState Coverage\n(a) Ant\n0\n1\n2\n3\n4\nSteps (M)\n0\n150\n300\n450\n0\n25\n50\n75\n100\nSteps (M)\n0\n150\n300\n450\nState Coverage\n(b) HalfCheetah\n0\n15\n30\n45\n60\nSteps (M)\n0\n2000\n4000\n6000\nState Coverage\n0\n1\n2\n3\n4\nSteps (M)\n0\n200\n400\n600\n(c) Humanoid-Run\n0\n10\n20\n30\n40\nSteps (M)\n0\n150\n300\n450\nState Coverage\n0\n1\n2\n3\n4\nSteps (M)\n0\n120\n240\n360\n(d) Quadruped-Escape\n0\n1\n2\n3\n4\nSteps (M)\n0\n150\n300\n450\n0\n20\n40\n60\n80\nSteps (M)\n0\n300\n600\n900\nState Coverage\n(e) AntMaze-Large\n0\n15\n30\n45\n60\nSteps (M)\n0\n500\n1000\n1500\nState Coverage\n0\n1\n2\n3\n4\nSteps (M)\n0\n400\n800\n1200\n(f) AntMaze-Ultra\nFigure 9: State coverage in state-based environments (sample efficiency). We plot the state\ncoverage in terms of the environment steps used for training. PEG is trained for >72 hours for\ncomparison. PEG, as a model-based GCRL algorithm, is more sample efficient for relatively low-\ndimensional tasks like Ant or HalfCheetah but struggles to learn in more challenging environments\nsuch as AntMaze. METRA is generally less sample efficient compared to TLDR.\n15\nTLDR (Ours)\nMETRA\nPEG\n0\n1\n2\n3\n4\nSteps (M)\n0\n20\n40\n60\n0\n25\n50\n75\n100\nSteps (M)\n0\n20\n40\n60\nGoal Distance (#)\n(a) Ant\n0\n1\n2\n3\n4\nSteps (M)\n0\n20\n40\n60\n0\n25\n50\n75\n100\nSteps (M)\n0\n20\n40\n60\nGoal Distance (#)\n(b) HalfCheetah\n0\n1\n2\n3\n4\nSteps (M)\n0\n15\n30\n45\n0\n15\n30\n45\n60\nSteps (M)\n0\n15\n30\n45\nGoal Distance (#)\n(c) Humanoid-Run\n0\n1\n2\n3\n4\nSteps (M)\n0\n2\n4\n6\n0\n20\n40\n60\n80\nSteps (M)\n0\n2\n4\n6\n# Reached Goals (\")\n(d) AntMaze-Large\n0\n1\n2\n3\n4\nSteps (M)\n0\n7\n14\n21\n0\n15\n30\n45\n60\nSteps (M)\n0\n7\n14\n21\n# Reached Goals (\")\n(e) AntMaze-Ultra\nFigure 10: Goal-reaching metrics of a goal-conditioned policy (sample efficiency). Similar to the\nresults in Figure 9, while PEG can be trained efficiently in relatively low-dimensional tasks, TLDR\nhas better sample complexity in more challenging tasks.\n16\nC\nMore Ablation Studies\nWe conduct the ablation studies on the number of nearest neighbors k (Figure 11) and dim œï(s)\n(Figure 12) used in Equation (2). Figure 11 shows that in Ant environment, k = 12 provides the best\nresults, with exploration slightly degrading at k = 5 or 20; in the AntMaze-Large environment, the\nperformance is rarely affected by the changes in k. Regarding dim œï(s), the performance is nearly\nthe same across different settings. Our main experimental results in Section 4.2 use k = 12 and\ndim œï(s) = 4, which demonstrates robust performance across diverse environments.\nk = 5\nk = 12\nk = 20\n0\n3\n6\n9\n12\nTime (hrs)\n0\n10000\n20000\n30000\nState Coverage\n(a) Ant\n0\n3\n6\n9\n12\nTime (hrs)\n0\n300\n600\n900\nState Coverage\n(b) AntMaze-Large\nFigure 11: State coverage on state-based environments with different k. We measure the state\ncoverage of our method with k ‚àà{5, 12, 20} used for calculating the TLDR reward in Equation (2).\nFor Ant, k = 12 works the best. For AntMaze-Large, k does not affect the final state coverage.\ndim œÜ(s) = 2\ndim œÜ(s) = 4\ndim œÜ(s) = 8\ndim œÜ(s) = 16\n0\n3\n6\n9\n12\nTime (hrs)\n0\n10000\n20000\n30000\nState Coverage\n(a) Ant\n0\n3\n6\n9\n12\nTime (hrs)\n0\n300\n600\n900\nState Coverage\n(b) AntMaze-Large\nFigure 12: State coverage on state-based environments with different dim œï(s). We measure the\nstate coverage of our method with dim œï(s) ‚àà{2, 4, 8, 16}, where dim œï(s) is the dimension of the\ntemporal distance-aware representations. The results show that dim œï(s) does not have a critical\nimpact on the performance in these environments.\n17\nD\nMore Qualitative Results\nWe include more qualitative results in Figures 13 to 16. For the qualitative results in Quadruped-\nEscape (Figure 14), we evenly select 48 states satisfying x2 + y2 = 102, where x, y represents the\nagent position. The z coordinate is selected as the minimum possible height that the agent does\nnot collide with the terrain. For all environments, TLDR achieves the best goal-reaching behaviors\ncompared to the other unsupervised GCRL methods, covering the goals in more diverse regions.\n(a) TLDR\n(b) METRA\n(c) PEG\nFigure 13: Goal-reaching ability in Humanoid-Run. We evaluate each method with the goals\nsampled according to Appendix A.5. TLDR moves further towards the goal in diverse directions\ncompared to other methods.\n(a) TLDR\n(b) METRA\n(c) PEG\nFigure 14: Goal-reaching ability in Quadruped-Escape. We evaluate each method with the goals\nthat are evenly selected at the same distance from the origin. TLDR can not only cover more regions\nbut also have a better goal-reaching capability than other methods.\n18\n(a) TLDR\n(b) METRA\n(c) PEG\nFigure 15: Goal-reaching ability in AntMaze-Large. TLDR can reach most of the goals in\nAntMaze-Large, while other GCRL methods struggle to reach distant goals.\n(a) TLDR\n(b) METRA\n(c) PEG\nFigure 16: Goal-reaching ability in AntMaze-Ultra. Similar to Figure 15, TLDR can cover the\nmost number of goals in AntMaze-Ultra, outperforming other methods.\n19\nE\nUnitree A1 Simulation Results\nFigure 17: Unitree A1 Simulation\nEnvironment. To demonstrate the\npotential applicability for real-world\nrobots, we choose the environment\nthat simulates a Unitree A1 robot\nwith 12 DoFs.\nWhile most unsupervised goal-conditioned RL and skill dis-\ncovery research currently focuses on simulated environments,\nunsupervised RL holds great potential for learning emergent\nand efficient skills for real-world robots.\nTo investigate whether TLDR can explore in environments\nwith real-world robotic counterparts, we train TLDR on the\nUnitree A1 robot in simulation [44] (Figure 17), consider-\ning sim-to-real transfer approaches. TLDR and METRA are\ntrained for 6 hours, with the same hyperparameter settings\nwe used in Quadruped-Escape and the episode length of 200.\nFor goal-conditioned evaluation, we sample goals with (x, y)-\ncoordinates from [‚àí15, 15]2.\nAs shown in Figure 18a and Figure 18b, TLDR achieves sub-\nstantially better state coverage and goal-reaching performance\ncompared to METRA, suggesting the potential of TLDR for au-\ntonomous exploration and effective goal-reaching gaits learn-\ning in real robotics systems.\nHowever, the learned behaviors with TLDR might be unsafe to transfer to reality since it does not\nimpose any constraint on the learned behaviors beyond the goal-reaching objective. To address this,\nwe test incorporating a safety reward for learning the exploration and goal-conditioned policies. The\nsafety reward is defined as rsafe = [0, 0, 1] ¬∑ vtorso, where vtorso is the orientation of robot torso, which\nequals to [0, 0, 1] when the robot is upright.\nThe results in Figure 18 show that TLDR with this safety reward can match the performance of TLDR\nwithout regularization in terms of state coverage and goal-reaching metrics while also maximizing the\nsafety reward. These findings indicate that TLDR is compatible with additional reward signals, and\napplying advanced safety-aware techinques [37, 38] could facilitate the learning of safer behaviors.\nVideos on learned behaviors can be found at https://heatz123.github.io/tldr.\nTLDR\nTLDR w/ safety reward\nMETRA\n0.0\n1.5\n3.0\n4.5\n6.0\nTime (hrs)\n0\n500\n1000\n1500\nState Coverage (‚Üë)\n(a) State Coverage\n0.0\n1.5\n3.0\n4.5\n6.0\nTime (hrs)\n0\n5\n10\n15\nGoal Distance (‚Üì)\n(b) Goal Distance\n0.0\n1.5\n3.0\n4.5\n6.0\nTime (hrs)\n‚àí1.0\n‚àí0.5\n0.0\n0.5\n1.0\nSafety Reward\n(c) Safety Reward\nFigure 18: Learning curves of Unitree A1 Simulation Results. TLDR achieves better state\ncoverage (a) and goal-reaching performance (b) compared to METRA. Since the learned behavior can\nbe unsafe, we consider the setting that the safety reward (c) is given by rsafe = [0, 0, 1]¬∑vtorso, where\nvtorso is the orientation of robot torso which equals to [0, 0, 1] with upright direction. Even with\nadding the reward, TLDR can (a) still explore the state space and (b) learn effective goal-reaching\nbehaviors (c) while maximizing the safety reward.\n20\nF\nAnalysis on Pixel-based Environments\nWhile achieving remarkable exploration and goal-reaching performance in state-based environments,\nexploration slows down in pixel-based environments, as observed in Figure 6. To identify the perfor-\nmance bottleneck for learning in pixel-based settings, we compare the performance when replacing\npixel observations with state observations for the inputs of goal-conditioned policy, exploration policy,\nand TLDR encoder, respectively.\nFigure 19 shows that the performance of TLDR becomes comparable to METRA when state observa-\ntions are used instead of pixel observations for the goal-reaching policy, while other modifications do\nnot improve upon the original TLDR. This suggests that the main bottleneck for exploration is likely\nto be the representations for the goal-conditioned policy. Based on this result, a promising future\ndirection for improving TLDR in pixel-based environments could be the integration of advanced\nrepresentation learning techniques [45, 46, 47] into our learning pipeline.\nTLDR\nTLDR w/ state GC policy\nTLDR w/ state exploration policy\nTLDR w/ state TLDR encoder\nMETRA\n0\n4\n8\n12\n16\nTime (hrs)\n0\n300\n600\n900\n1200\nState Coverage (‚Üë)\n(a) State Coverage\n0\n4\n8\n12\n16\nTime (hrs)\n0\n5\n10\n15\nGoal Distance (‚Üì)\n(b) Goal Distance\nFigure 19: Result of component-wise analysis in Quadruped (Pixel). To identify the bottleneck of\nexploration with pixel observations, we swap pixel observations to state observations for the input\nof goal-conditioned policy, exploration policy, and TLDR encoder, respectively. Exploration of\nTLDR significantly improves when we input state observations to the goal-conditioned policy, which\nsuggests the main bottleneck for exploration is the representations for the goal-conditioned policy.\nAdditionally, while METRA quickly learns to achieve skills in Kitchen (Pixel) environment compared\nto TLDR, we observe that its performance degrades with continuous skills, as shown in Figure 20.\nThis implies the difficulty of learning policies with continuous goals in TLDR‚Äôs goal-conditioned\npolicy learning, possibly because learning to reach arbitrary states is more challenging than mastering\na specific set of discrete behaviors. Investigating ways to restrict the size of the goal space in TLDR\ncould be an interesting direction for future research.\nTLDR (Ours)\nMETRA: Discrete 24 skills\nMETRA: Continuous 4-D skills\nMETRA: Continuous 8-D skills\n0\n3\n6\n9\n12\nTime (hrs)\n0\n2\n4\n6\n# Achieved Tasks (‚Üë)\nFigure 20: Performance of METRA in Kitchen (Pixel) with different skill settings. When METRA\nuses continuous skill vectors in Kitchen (Pixel), METRA‚Äôs performance substantially degrades with\ncontinuous skills in Kitchen (Pixel) environment, which is similar to our setting of learning to reach\nany goals.\n21\nG\nAnalysis on Goal-reaching Reward Design\nWe compare the goal-reaching performance of TLDR with different goal-conditioned policy learning\nmethods using the same experimental setup as in Figure 8a. As presented in Figure 21, other\ngoal-reaching policy learning methods cannot reach the same level of performance as TLDR. This\nhighlights the importance of our GCRL reward design on goal-reaching performance, consistent with\nthe results in Figure 8a, where TLDR achieves the best state coverage while other methods struggle.\nTLDR\nQRL\nHER\nDDL\n0\n3\n6\n9\n12\nTime (hrs)\n0\n20\n40\n60\nGoal Distance (‚Üì)\n(a) Ant\n0\n3\n6\n9\n12\nTime (hrs)\n0\n7\n# Reached Goals (‚Üë)\n(b) AntMaze-Large\nFigure 21: Goal-reaching metrics with GCRL reward design ablations. TLDR shows better\ngoal-reaching performance compared to other choices of goal-conditioned policy learning methods,\nshowing the effectiveness of our design of the GCRL reward.\nTo further isolate the impact of the exploration strategy and focus solely on goal-reaching policy\nlearning, we evaluate the goal-reaching performance in an offline learning setting. In this setup,\npolicies are trained on a fixed dataset of 1M samples collected from rollouts of a trained TLDR policy.\nAlthough goal-reaching performances are degraded in this setting due to off-policy training, our\nchoice of the goal-reaching reward still demonstrates superior results compared to other methods, as\nshown in Figure 22. This suggests that our design of the goal-reaching reward‚Äîminimizing the L2\ndistance to the goal in the temporal distance-aware representation space‚Äîprovides effective signals\nfor goal-reaching.\nTLDR\nQRL\nHER\nDDL\n0.0\n2.5\n5.0\n7.5\nGradient Steps (100K)\n0.0\n0.5\n1.0\n1.5\n# Reached Goals (‚Üë)\nFigure 22: Goal-conditioned policy learning ablation with fixed dataset. With 1 million samples\nof rollouts from a trained TLDR policy, we learn the goal-conditioned policy without adding the data\nto the replay buffer, differing only in the goal-conditioned policy learning methods. TLDR shows the\nbest performance in this setting where the impact of exploration is isolated.\n22\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2024-07-11",
  "updated": "2024-12-09"
}