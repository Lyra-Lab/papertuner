{
  "id": "http://arxiv.org/abs/2407.10351v1",
  "title": "Comparing Complex Concepts with Transformers: Matching Patent Claims Against Natural Language Text",
  "authors": [
    "Matthias Blume",
    "Ghobad Heidari",
    "Christoph Hewel"
  ],
  "abstract": "A key capability in managing patent applications or a patent portfolio is\ncomparing claims to other text, e.g. a patent specification. Because the\nlanguage of claims is different from language used elsewhere in the patent\napplication or in non-patent text, this has been challenging for computer based\nnatural language processing. We test two new LLM-based approaches and find that\nboth provide substantially better performance than previously published values.\nThe ability to match dense information from one domain against much more\ndistributed information expressed in a different vocabulary may also be useful\nbeyond the intellectual property space.",
  "text": "Comparing Complex Concepts with Transformers \nMatching Patent Claims Against Natural Language Text \n \nMatthias Blume \nIP Aptly, Inc.  \nSan Diego, CA, USA \nmatthias.blume@ipaptly.com\n \nGhobad Heidari \nIP Aptly, Inc. \nSan Diego, CA, USA \nghobad.heidari@ipaptly.com\n \nChristoph Hewel \nIP Aptly, Inc. \nMunich, Germany \nchristoph.hewel@ipaptly.com \n \nABSTRACT \nA key capability in managing patent applications or a patent \nportfolio is comparing claims to other text, e.g. a patent \nspecification. Because the language of claims is different from \nlanguage used elsewhere in the patent application or in non-patent \ntext, this has been challenging for computer based natural \nlanguage processing. We test two new LLM-based approaches \nand find that both provide substantially better performance than \npreviously published values. The ability to match dense \ninformation from one domain against much more distributed \ninformation expressed in a different vocabulary may also be \nuseful beyond the intellectual property space. \nCCS CONCEPTS \n• Computing methodologies→Language resources; Supervised \nlearning • Social and professional topics→Patents • Information \nsystems→Retrieval tasks and goals \nKEYWORDS \nCross-vernacular information retrieval, patent claim search, vector \nspace representation of complex concepts \nACM Reference format: \nMatthias Blume, Ghobad Heidari, and Christoph Hewel. 2024. Comparing \nComplex Concepts with Transformers: Matching Patent Claims Against \nNatural Language Text. In Proceedings of the 5th Workshop on Patent Text \nMining and Semantic Technologies (PatentSemTech’24). \n1 INTRODUCTION \nA patent application consists of claims and other text. The claims \nvery densely represent the key aspects of the invention. They are \nwritten to be as general as possible and utilize a distinct \nvocabulary and grammar: each claim is limited to at most one \nsentence, and that sentence typically does not have a subject-verb-\nobject structure. The remainder of the patent application is similar \nto technical text from its domain, which again differs from other \ntypes of text such as marketing documents. \nA patent examiner must search for prior art documents in order \nto determine whether the claimed invention is novel and may be \nallowable, or whether all aspects of the claim have previously \nbeen disclosed. A patent owner may want to search a database of \ndocuments or all text on the web in order to find products that \npotentially infringe on the inventions, as specified in the claims. \nAn entity defending itself against infringement may attempt to \ninvalidate a patent by finding novelty-destroying prior art to that \npatent. In all cases, the key task is to search through a set of \ndocuments and determine whether those documents cover all \naspects of each claim of the subject patent application or granted \npatent. Thus, a claim of a subject patent (application) may be \nconsidered a query to an information retrieval system whose \nobjective is to retrieve a document or set of documents that \ncontain all aspects of that claim. \nRisch et al. [2021] identified EPO Search Reports as a \npotential source of ground truth data for training and evaluating \nmodels specific to matching patent claims against prior patent \napplications. The European Patent Office trained and evaluated \nSentence Transformer models on this data. Our approach is \nsimilar but different in several important ways. \nSection 2 of this paper describes the US and EPO patent \napplication data, the EPO Search Report data, and our parsing of \nthese datasets. Section 3 describes our algorithms for fine-tuning \nlarge language models (LLMs) for the purpose of comparing \nclaims against non-claim text (e.g. from patent specifications) and \nscoring document similarity with respect to a patent claim. \nSection 4 describes our results and compares them to previous \npublished results. Section 5 describes a proof-of-concept system \nfor using a claim as a query for real-time semantic search of a \nlarge corpus of documents. Section 6 provides conclusions. \n2 DATA \nThe basis of our dataset is the EPO’s EP full-text data for text \nanalytics1 and USPTO’s Patent Application Full Text Data (No \nImages)2 bulk download data sets. The EPO data includes full-text \nand metadata of all patent applications and patent documents \npublished by the EPO from 1978 through July, 2022. The USPTO \ndata used for this study includes full-text and metadata of all \npatent applications published by the USPTO from March 15, 2001 \nthrough July, 2022. \nThe EPO data includes search reports from 2012 onwards that \ncan be used to create a labeled dataset for supervised training and \n \n1 https://www.epo.org/en/searching-for-patents/data/bulk-data-sets/text-analytics \n2 https://bulkdata.uspto.gov/ \nCopyright © 2024 for this paper by its authors. Use permitted under Creative \nCommons License Attribution 4.0 International (CC BY 4.0). \nPatentSemTech’24, July, 2024, Washington, D.C. USA \nM. Blume, G. Heidari, and C. Hewel \n \n \n \nevaluation. In these reports, patent examiners cite prior art \ndocuments that are relevant for judging the novelty of each \napplication claim. We utilize two categories of citations provided \nby the examiners: an “X” document negates the novelty of the \nclaimed invention, and an “A” document is a relevant prior art that \nhowever does not negate the novelty or inventive step.  \nEach citation references the relevant passages of the prior art \ndocument, e.g. “abstract; figure 1; paragraph [0002] \n- paragraph [0023]; claims 1-13”. We parse and \nstandardize this field. We keep only references to abstract, claims, \nand paragraphs and discard references to figures, page and line \nnumber, and some other rarer formats. Linking the passage \nreferences to the full text of EPO and USPTO patent applications \nyields a dataset with ground truth of not only which document, but \nwhich passages are prior art to a specific claim. \nOur Search Reports dataset includes 467,558 claim 1 “A” and \n“X” citations. It seems almost ideal for training a classifier that \ndistinguishes between text passages that cover all aspects or do \nnot cover all aspects of a particular claim. However, the data has \nseveral limitations and peculiar characteristics. 1) The examiner \nmay initially identify a document as an “X” citation but, after \nrebuttal by the inventors, allow that it is not novelty-destroying \nafter all. Thus, “X” citations are not really “ground truth”. 2) “A” \ncitations are more likely than “X” citations to reference a small \namount of text (fewer paragraphs of the prior art document), e.g., \n30% vs 19% reference text with a total length of fewer than 3000 \ncharacters. 3) “A” citations are more likely than “X” citations to \nreference EPO applications: 27% vs 24%. Points #2 and #3 make \nit possible to build a model that distinguishes between “X” and \n“A” citations but would be useless in practice. \nBoth EPO and USPTO (since 2001) delimit different claim \nelements via <claim-text> XML tags, e.g.: \n<claim id=\"CLM-00001\"> \n<claim-text>. A hip protecting device for inflating a poc\nket over a hip joint of a wearer of the device upon a fal\nl comprising:  \n<claim-text>a belt; </claim-text> \n<claim-text>a substantially gas impermeable first pocket \nfixedly suspended … from said belt; </claim-text> \n… \n</claim-text> \n</claim> \nThis tag is used to split claims into elements (Section 3.2.2).  \n3 METHODS \n3.1 Model Training \nWe split the search reports into 80% training and 20% \ntest/evaluation sets by subject patent application ID. That is, each \nquery EPO patent application occurs only in the training set or \nonly in the test set and not both. \nFor each “X” and “A” citation in the search reports, we \nconcatenate the text of all cited paragraphs, claims, abstracts, and \nfigure descriptions. We split each cited text into chunks of \nmaximum \nlength \nMaxSeqLength, \nrespecting \nparagraph \nboundaries. That is, the beginning of a chunk is always aligned \nwith the beginning of a paragraph. The context window size of \nour base model defines MaxSeqLength = 512 tokens. For each \nclaim 1, we choose pairs of “X” and “A” chunks, using each \nchunk at most once. For example, if there are five “X” chunks and \nthree “A” chunks, we create three records, each with one “X” and \none “A” chunk. This yields 171,323 training records where the \n“X” chunk is more relevant to the claim than the “A” chunk. We \ncreate a second set of records where each of the “A” chunks is \nused as the positive example and a random “X” chunk is used as \nthe negative example. I.e., the negative example in this second set \nis an “X” citation for a different claim 1. This prevents the model \nfrom learning that some chunks are inherently positive or negative \ndue to overall differences between “X” and “A” citations chosen \nby the examiners. \nWe use contrastive learning to tune a model such that the \nsimilarity between a relevant chunk and the query claim 1 is \ngreater than the similarity between the less relevant chunk and the \nquery claim 1. Specifically, we use the Sentence Transformers \ntechnique [5] to fine tune the distilroberta-base 3 model. (This \n“small LLM” will not yield the highest possible performance. \nRather, we chose it for rapid training and evaluation of the \ntechniques described in the next section.) Below, we refer to this \nfine-tuned model as “CCX”, short for claim-chunk transformer. \n3.2 Similarity Measurement \nGiven the models described in the previous section, we can \ncompare text from a patent claim against arbitrary text from a \ndifferent patent application. \nThe PatentMatch [3] and SearchFormer [6] papers describe \nhow to distinguish between a paragraph from an “X” document \nand a paragraph from a different document. But operating at the \nparagraph level entails several deficiencies. First, a single \nparagraph does not generally include all aspects of a patent claim \nand should not be considered an “X” paragraph. Rather, the \ncomplete set of paragraphs identified in the Search Report \nconstitute the “X” citation. Second, the examiners’ passage \nidentification is less accurate than the X/A classification. For \nexample, an examiner may cite “paragraphs 1-20” for \nconvenience even if some of the paragraphs in the range did not \nconvey key features of the prior art. Finally, the examiner is \ninitially interested in identifying documents that contain the prior \nart, so the scores for multiple paragraphs should be combined to \nrank documents. (Once a candidate document has been identified, \nit is desirable to identify those paragraphs of the candidate \ndocument which actually anticipate the claim elements.) \nHere, we describe two ways to make multiple comparisons \nbetween text from a patent claim and text from a different \ndocument and then aggregate the results from the multiple \ncomparisons into a single score that can be used to rank \ndocuments by their similarity with a patent claim. \n3.2.1 Maximum Chunk-Claim Similarity. We break each \nsection of a patent application into chunks of maximum length \nMaxSeqLength at paragraph boundaries. Thus, a chunk will not \n \n3 https://huggingface.co/distilbert/distilroberta-base \nComparing Complex Concepts with Transformers \nPatentSemTech’24, July, 2024, Washington, D.C. USA \n \n \nspan multiple sections (\"Abstract\", \"CrossRef\", \"Background\", \n\"Summary\", \"BriefFig\", \"Description\", \"Claims\", \"Admin\") but \ntypically contains multiple paragraphs. We compute the cosine \nsimilarity between the query claim 1 (if the full claim is longer \nthan MaxSeqLength, we use only the last elements of the claim) \nand each target document chunk. The score of the document with \nrespect to the claim is the maximum similarity of any chunk from \nthe document. \n3.2.2 Weighted Sum of Paragraph-Element Similarity. We \nsplit the query claim 1 into multiple claim elements using the \n<claim-text> XML tag as shown above. We compute the \nsimilarity between each query claim element and each target \ndocument paragraph. We compute the score of the document with \nrespect to the claim element and then the score of the document \nwith respect to the claim via functions of the cosine similarity \nbetween the claim element and paragraph as well as element and \nparagraph salience characteristics. \n3.3 GPT 4o \nWe explored whether GPT 4o can distinguish between “X” and \n“A” citations w.r.t. a query claim. We tested the OpenAI API \n(which relies exclusively on OpenAI’s data) and chatgpt.com with \nfile uploads4, uploading the full text of the X and A reference \ndocuments. The GPT prompts were structured as: \nThe file US20080295019A1.txt contains the text of patent \napplication US20080295019. The file US20050060664A1.txt \ncontains the text of patent application US20050060664. \nEach of the following lines is an element of a patent \nclaim. Which patent application better covers all of the \nelements, US20080295019 or US20050060664? Choose one or \nthe other, do not say \"neither\". Output only \n\"US20080295019\" or \"US20050060664\". \n[query claim element 1] \n[query claim element 2] \n… \nThe first two sentences above were not used with the API. \n4 RESULTS \nSearchFormer [6] defined a “hard” task of distinguishing between \n“X” and “A” cited documents with respect to claim 1 of a patent \napplication and an “easy” task of distinguishing between “X”-\ncited and random documents. Table 1 presents the results of \nseveral models and techniques on these two tasks. \nMethod \nX/A \nX/Random \nPatentMatch 2021 \n54% \nSearchFormer 2023 \n53.85% \n98.04% \nIP Rally 2021 \n58% \nMax Chunk-Claim GP BERT \n53.89% \nMax Chunk-Claim CCX \n63.05% \n99.61% \nWeighted Paragraph-Element CCX \n60.46% \nGPT 4o internal data only \n52.75% \n \nGPT 4o upload full text \n59.17% \n \nTable 1: Accuracy by negative example type for several models. \nRandom selection would yield 50% accuracy (binary choice). \n \n4 https://help.openai.com/en/articles/8555545-file-uploads-faq \nThe first three rows of Table 1 show PatentMatch “balanced” \n[3], SearchFormer [6], and IP Rally [2] published performance \nnumbers. Each of these used a different evaluation set, so the \nnumbers should not be compared exactly. A likely explanation for \nPatentMatch’s and SearchFormer’s low values is that they \nevaluated the ability to distinguish between “X” and “A” \nparagraphs rather than “X” and “A” documents. \nThe next three rows compare our two different models and two \ndifferent aggregation techniques, as described below. Evaluation \nis on a hold-out set of 20,012 records that was not used for CCX \ntraining. Each record contains the query claim 1, one “X” citation, \nand one “A” citation. Note that for training the model, as \ndescribed in Section 3.1, we used one record per matched pair of \nX, A chunks from the search reports, whereas for evaluation, we \nhave one record per matched pair of X, A documents. \nGP BERT uses the output of the first token (the [CLS] token) \nof Google’s BERT for Patents model5 without further fine-tuning. \nThis model performed poorly in this task, which concurs with the \ncentral finding of the Sentence Transformers paper [5]: large \nlanguage models trained to generate text do not inherently know \nwhich type of similarity is relevant for a particular task. E.g., a \nbase LLM could reasonably find any pair of phrases “Method for \nperforming XYZ comprising the following steps” similar because \n7 words match exactly. Search reports provide excellent data for \nsupervised training to fine-tune models to focus on the \ndistinctions relevant in the patent domain. \nCCX is the model described in Section 3.1. Max Chunk-Claim \nindicates that the maximum chunk-claim similarity technique was \nused to compute the similarity between a document and the query \nclaim 1. Weighted Paragraph-Element indicates that the weighted \nsum of paragraph-element similarity technique was used to \ncompute the similarity between a document and the query claim 1. \nBoth aggregation techniques yield >60% accuracy on the “hard” \ntask: substantially better than previously published values. The \nmaximum chunk-claim similarity technique yields 99.61% \naccuracy on the “easy” task: substantially better than \nSearchFormer’s published value. We did not evaluate the \nweighted sum of paragraph-element similarity aggregation \ntechnique on the “easy” task. \nZero shot performance of GPT 4o via the API is poor due to \nthe limited patent text data accessible to the model. Even when \nuploading the full text of the X and A documents, GPT 4o’s \nperformance is only comparable to our much smaller CCX model \ntuned for claim-chunk similarity. Qin 2024 [4] note that \ngenerative LLMs are sensitive to the order of the items in pairwise \ncomparison. Without file uploads, GPT 4o responded with the \ndocument ID that appeared first in the prompt 65.5% of the time. \nGPT 4o’s explanations of which patent application better covers \nthe query claim are well-phrased and compelling even when the \nconclusion disagrees with the EPO search reports. \nRisch et al. [2021] stated: “The complex linguistic patterns, the \nlegal jargon, and the patent-domain-specific language make it \nsheer impossible for laymen to manually solve this task.” \n \n5 https://huggingface.co/anferico/bert-for-patents  \nPatentSemTech’24, July, 2024, Washington, D.C. USA \nM. Blume, G. Heidari, and C. Hewel \n \n \n \nVowinckel and Hähnke [2023] “found that hard negatives (A-\ncitations) alone are too challenging”. Kallio [2021] stated “It \ndoesn’t tell much about the search results directly, as the A \ncitations are good and important results too.” We concur that it is \na difficult task and that the A citations are also relevant. Our \nresults demonstrate that it is possible to achieve far better than \nrandom performance, and we anticipate further improvements by \nusing a better base model fine tuned with more data. \nA major question was whether an LLM can effectively \nrepresent a set of concepts as complex as a patent claim in its \nhidden layer, and whether comparison of these vector embeddings \ncould be effective for comparing similar concepts. The Max \nChunk-Claim approach relies only on this embedding, and the \nperformance indicates that comparing embedding vectors can in \nfact identify similarity of concepts as complex as a patent claim. \nThe Weighted Paragraph-Element approach breaks down the \ncomplex concept into several smaller snippets and compares at a \nparagraph level rather than a larger chunk of text. Initial results do \nnot demonstrate a substantial improvement over letting the LLM \ndo all the work. A few possible reasons for this are: \n1) The weighting scheme is arbitrary (an optimal weighting \nscheme could be learned from the data). \n2) CCX was tuned to compare the similarity of claims and \nchunks, not elements and paragraphs. A model trained for the \nlatter scenario should perform better. \n5 REAL-TIME SEARCH \nAbove, we described several ways to compare a claim against one \ndocument using semantic vector embeddings of a LLM. Using \napproximate nearest neighbors search, it is practical to compare a \nclaim against a corpus of documents, for example all published \npatent applications, in real time. We implemented such a system \nand describe it at a high level below as a proof of concept. Figure \n1 shows the user interface. \nWe pre-compute one vector per chunk of each document in the \ncorpus using the algorithm described in Section 3.2.1 and store \nthese in a vector database. Vowinckel and Hähnke [2023] state: \n“There are around 70 million simple patent families with at least \none document that contains English text. If the average of 126 \nparagraphs per document holds true, this corresponds to more than \n8.8 billion passages that need to be vectorized.” Since we compute \na vector per chunk rather than per paragraph, we need only about \n20 vectors per patent application rather than 126, or 1.4 billion \nvectors in total. Using a model with a larger context window \nwould reduce the number of vectors per patent. \nOur current vector database represents 3.5 million patent \napplications and comprises approximately 70 million vectors. \nComputing the embedding vector for the query and retrieving the \nranked list of the nearest 5000 chunks in the vector database takes \na small fraction of a second. We further support on-the-fly \ncalculation of an embedding vector for each paragraph in the top \nN retrieved documents and re-ranking based on weighted \nparagraph-element similarity. Single-threaded on an RTX 4090 \nGPU, this operation takes less than a minute. Thus, real-time \nLLM claim search of a corpus of all patent applications and re-\nranking the top results is practical.  \n \nFigure 1. IP Aptly real-time patent search user interface. \n6 CONCLUSIONS \nThe key features of an invention are densely stated in a patent \nclaim with a peculiar vocabulary and grammar. We demonstrate \nthat it is practical to fine-tune an LLM to compare a claim against \na much larger chunk of natural language text. Each chunk should \nbe much larger than a paragraph, as describing the concepts from \na single claim typically requires multiple paragraphs of plain text. \nThe “small LLM” fine-tuned for this task performs as well as \nGPT 4o. To the best of our knowledge, the values published here \nare the current state of the art on the task of distinguishing \nbetween “X” and “A” citations w.r.t. a query claim. \nREFERENCES \n[1] Daniel S. Hain, Roman Jurowetzki, Tobias Buchmann, and Patrick Wolf. 2022. \nA text-embedding-based approach to measuring patent-to-patent technological \nsimilarity. Technological Forecasting and Social Change 177, Article 121559 \n(April 2022), 45 pages.  https://doi.org/10.1016/j.techfore.2022.121559. \n[2] Juho Kallio. 2021. Patent search metrics: We can do better than recall. \n(November \n2021). \nRetrieved \nApril \n29, \n2024 \nfrom \nhttps://www.iprally.com/news/patent-search-metrics-we-can-do-better-than-\nrecall  \n[3] Julian Risch, Nicolas Alder, Christoph Hewel, and Ralf Krestel. 2021. \nPatentMatch: A dataset for matching patent claims & prior art. In: Proceedings \nof the 2nd Workshop on Patent Text Mining and Semantic Technologies \n(PatentSemTech’21), July 15, 2021, online. ACM Inc., New York, NY, 5 pages. \nhttps://doi.org/10.48550/arXiv.2012.13919 \n[4] Zhen Qin et al. 2024. Large Language Models are Effective Text Rankers with \nPairwise Ranking Prompting. In Findings of the Association for Computational \nLinguistics: \nNAACL \n2024, \nMexico \nCity, \nMexico. \nAssociation \nfor \nComputational Linguistics. https://aclanthology.org/2024.findings-naacl.97.pdf  \n[5] N. Reimers and I. Gurevych. 2019. Sentence-BERT: Sentence embeddings \nusing siamese BERT-networks. In Proceedings of the 2019 Conference on \nEmpirical Methods in Natural Language Processing and the 9th International \nJoint Conference on Natural Language Processing (CEMNLP-IJCNLP’19), \nNovember 3–7, 2019, Hong Kong, China. Association for Computational \nLinguistics, Stroudsburg, PA, 3982-3992. https://doi.org/10.18653/v1/D19-\n1410  \n[6] Konrad Vowinckel and Volker D. Hähnke. 2023. SEARCHFORMER: \nSemantic patent embeddings by siamese transformers for prior art search. \nWorld Patent Information 73, Article 102192 (June 2023), 16 pages. \nhttps://doi.org/10.1016/j.wpi.2023.102192  \n",
  "categories": [
    "cs.CL",
    "I.2.7"
  ],
  "published": "2024-07-14",
  "updated": "2024-07-14"
}