{
  "id": "http://arxiv.org/abs/2407.12389v1",
  "title": "Morphosyntactic Analysis for CHILDES",
  "authors": [
    "Houjun Liu",
    "Brian MacWhinney"
  ],
  "abstract": "Language development researchers are interested in comparing the process of\nlanguage learning across languages. Unfortunately, it has been difficult to\nconstruct a consistent quantitative framework for such comparisons. However,\nrecent advances in AI (Artificial Intelligence) and ML (Machine Learning) are\nproviding new methods for ASR (automatic speech recognition) and NLP (natural\nlanguage processing) that can be brought to bear on this problem. Using the\nBatchalign2 program (Liu et al., 2023), we have been transcribing and linking\ndata for the CHILDES database and have applied the UD (Universal Dependencies)\nframework to provide a consistent and comparable morphosyntactic analysis for\n27 languages. These new resources open possibilities for deeper crosslinguistic\nstudy of language learning.",
  "text": " \nLanguage Development Research \n \n \n \n \n \n \nVolume 3, Issue 1, 31 December 2023 \n \n1 \nMorphosyntactic Analysis for CHILDES \n \nHoujun Liu \nStanford University \n \nBrian MacWhinney \nCarngie Mellon University \n \nAbstract: Language development researchers are interested in comparing the pro-\ncess of language learning across languages.  Unfortunately, it has been difficult to \nconstruct a consistent quantitative framework for such comparisons. However, re-\ncent advances in AI (Artificial Intelligence) and ML (Machine Learning) are providing \nnew methods for ASR (automatic speech recognition) and NLP (natural language pro-\ncessing) that can be brought to bear on this problem. Using the Batchalign2 program \n(Liu et al., 2023), we have been transcribing and linking data for the CHILDES data-\nbase and have applied the UD (Universal Dependencies) framework to provide a con-\nsistent and comparable morphosyntactic analysis for 27 languages. These new re-\nsources open possibilities for deeper crosslinguistic study of language learning. \n \nKeywords: morphology; grammatical relations; ASR; NLP  \n \nCorresponding author: Brian MacWhinney, Department of Psychology, Carnegie \nMellon University, Pittsburgh, PA. Email: macw@cmu.edu  \n \nORCID ID: https://orcid.org/0000-0002-4988-1342  \n \nCitation: Liu, H. & MacWhinney, B. (2024). Morphosyntactic analysis for CHILDES. \nLanguage Development Research, 2(1), XX–XX. https://doi.org/10.34xxx/xxxx-xxxx \n \n \n \n \nLanguage Development Research \n \n \n \n \n \n \nVolume 3, Issue 1, 31 December 2023 \n \n2 \nIntroduction \n \nChild language research involves three, partially separate, formats for data collection. \nThe first focuses on the development of a single child or pair of children, often across \nseveral years. Work in this tradition includes classic diary studies from German (Stern \n& Stern, 1907), French (Bloch, 1921; Guillaume, 1927), Polish (Smoczynska, 2017; \nSzuman, 1959), Hungarian (Kenyeres, 1926; Ponori, 1871), Mandarin (Chao, 1951), \nBulgarian (Gvozdev, 1949), Serbian (Pavlovitch, 1920) and other languages. It also in-\ncludes diary and transcript studies of particular aspects of development such as pho-\nnology (Smith, 1973), grammatical morphology (Brown, 1973), lexicon (Tomasello, \n1992), or all of the above (Leopold, 1939, 1947, 1949a, 1949b). This case-study work has \nhelped us understand the  diverse ways in which children acquire and use language \nto express their needs (Karniol, 2010). \n  \nA second data collection format measures and evaluates learning across groups of \nchildren within a single language. This type of analysis is particularly important for \nclinicians who need to diagnose, assess, and remediate language learning disorders. \nData collection in this format includes standardized tests (Bishop, 1982; Goldman & \nFristoe, 2000), language sample analysis (Garbarino et al., 2020), and language profil-\ning (Bernstein Ratner & MacWhinney, 2023; Crystal et al., 1989; Scarborough, 1990).    \n \nA third data collection format examines development across languages. This work \nconsiders the ways in which variations in language structure and social input pose \nchallenges or opportunities to the learner. For various reasons, this work has had a \nconcentration of  data from WEIRD (Western, educated,  industrialized, rich, and \ndemocratic) participants (Henrich et al., 2010) along with an emphasis on monolin-\ngual acquisition.  To broaden our crosslinguistic coverage, Slobin and colleagues \n(Slobin, 1985) have provided descriptions of linguistic and social development in a \nseries of languages, including some from non-WEIRD communities. However, with-\nout quantitative tools to compare across these many languages, it has been difficult \nto generalize about patterns of language learning methods, structures, and chal-\nlenges. The introduction of the MacArthur-Bates Communicative Development In-\nventory (Dale & Fenson, 1996) provided quantitative methods to bridge the WEIRD \ngap for the earliest stages of lexical development.  That tool has now been validated \nfor several Western languages (Frank et al., 2021), but extensions to less well-re-\nsourced languages and multilingualism (Tamis-LeMonda et al., 2024) will take addi-\ntional time and effort. \n  \nThe CHILDES data-sharing system (MacWhinney, 2000) offers yet another approach \nto extending child language research beyond WEIRD participants.  CHILDES includes \nlanguage samples from 49 languages, along with 41 corpora from children learning \ntwo or more languages, all contributed by researchers who are speakers of these lan-\nguages. Although many of these families are WEIRD, there are also many from \n \nLanguage Development Research \n \n \n \n \n \n \nVolume 3, Issue 1, 31 December 2023 \n \n3 \nsocieties that are not Western, and not fully industrialized, rich, or democratic. Alt-\nhough nearly 40% of the data is from English, there are many large corpora from lan-\nguages such as Mandarin, Spanish, German, French, and Japanese as well as a smaller \nnumber of large corpora from another 15 languages.  \n \nCreating child language corpora requires major commitments of researcher effort for \nrecording, transcription, and analysis. However, recent advances in AI (artificial in-\ntelligence) and ML (machine learning) have led to marked improvements in ASR (au-\ntomatic speech recognition)(Radford et al., 2022) and NLP (natural language pro-\ncessing)(Nivre et al., 2016) methods that can markedly facilitate this work.  The use of \nASR can greatly speed transcription (Liu et al., 2023), although recognition of child \nvocalizations before age 3 is still poor. When recording is done well, ASR can recog-\nnize adult input accurately enough to allow a transcript to be finalized after a much \nbriefer period of hand correction.  A further advantage is that ASR it creates a tran-\nscript that is linked to the audio on both the utterance and single word level, thereby \nfacilitating analyses of phonology, fluency, and total time talking. Moreover, the out-\nput can be structured directly in the CHAT (Codes for Human Analysis of Talk) for-\nmat, thereby allowing analysis through the utilities built into the CLAN (Child Lan-\nguage Analysis) program (MacWhinney & Fromm, 2022).  ASR methods can also be \nused to automatically link an unlinked transcript to the corresponding media (audio \nor video) on the utterance and word level. This process is particularly useful for tran-\nscripts in the CHILDES database that have media, but which have not yet been linked \nto that media. \n \nAfter a transcript has been created in correct CHAT format, we can then use NLP \nmethods to automatically construct a complete morphosyntactic analysis. In the next \nsections, we will describe how these ASR and NLP methods are being applied to im-\nprove the use of CHILDES data across all three data analysis formats with a special \nemphasis on facilitating crosslinguistic comparisons.  \n \nAutomatic Speech Recognition \n \nOnce a language sample has been recorded, the next task is to create a transcript. \nDepending on the nature of the interaction, manual transcription of one hour of in-\nteraction can take from 10 to 16 hours (Bernstein Ratner & MacWhinney, 2020). To \nspeed up this process, researchers can apply ASR methods using the Batchalign2 sys-\ntem (Liu et al., 2023) which outputs a transcript in the CHAT format required for in-\nclusion in the CHILDES database. Batchalign2 offers access to two ASR systems: the \nRev.AI ASR cloud service (Del Rio et al., 2022) or a local ASR model based on OpenAI \nWhisper (Radford et al., 2023).  If IRB (Institutional Review Board) regulations do not \nallow transmission of data to a cloud service, users may prefer to use Whisper, alt-\nhough Rev.AI explicitly allows the user to determine that the data will not be stored \non their cloud server.  For English, Rev.AI output is a bit more accurate than Whisper \n \nLanguage Development Research \n \n \n \n \n \n \nVolume 3, Issue 1, 31 December 2023 \n \n4 \ndue to the its use of a large amount of two-party conversations as training data (Del \nRio et al., 2022). In addition, processing through Rev.AI is much faster than running \nwith Whisper when local hardware is limited, but both options are good choices. \n   \nAnother factor that favors use of Whisper is that the training data for the NLP models \nused in downstream analysis use native orthographies of each language (De Marneffe \net al., 2021; Qi et al., 2020). Latinized transcripts must be converted back into the \nstandard orthography for the language before downstream analysis. Because of this \nlimitation, the significantly wider language and orthographic profile of the Whisper \nmodel (in particular, WhisperV3 available at https://huggingface.co/openai/whisper-\nlarge-v3) is advantageous for non-English languages; therefore, the majority of the \nrecognition needed to cover all the languages described here (and in particular ones \nwith non-latinized native orthography) is performed with the Whisper option. \n \nUtterance Segmentation \nTagging for morphological categories and grammatical dependency structure re-\nquires accurate delineation of sentences or utterances. Segmentation of naturalistic \nspoken language data requires attention to features not found in written text (Fraser \net al., 2015), such as incompletion, repetition, retracing, and other features. Sections \n9.1 and 9.2 of the CHAT manual (https://talkbank.org/manuals/CHAT.pdf) provide a \nset of standards for utterance segmentation. For example, one important feature is \nthat clauses joined only with coordinating conjunctions (and, or, but) are treated as \nseparate utterances.  \n \nBecause currently available tokenizers are all based on written language and because \nspoken language segmentation follows quite different rules and patterns, we have \ncreated novel tokenizers based on spoken language training data.  To create the to-\nkenizer for spoken English data, we turned to the TalkBank database, which contains \nmany Gold Standard utterances segmented according to the rules mentioned above.  \nThe tokenizer (Liu et al., 2023) is trained via a token-classification task, which assigns \neach input text token as being the start (label 1), middle (label 0), a phrase which \nshould be separated by a comma (label 5), or end of each utterance (label 2,3,4); in \nparticular, there are three utterance-ending labels, each corresponding to the utter-\nance being declarative, interrogative, or exclamatory respectively. The tokenizer uses \na BERT-class model (Devlin et al., 2018) to generate semantic embeddings for lan-\nguage modeling, and a deep neural network (DNN) to perform token-level annota-\ntions. \n \nCurrently, Batchalign2 provides tokenizers for English and Mandarin. The English \nmodel was trained on the MICASE (The Michigan Corpus of Academic Spoken Eng-\nlish) (Römer, 2019) corpus in CABank (https://ca.talkbank.org/access/MICASE.html), \nwhich includes transcribed data from 300 participants in a wide variety of interactions \nbetween students and faculty at the University of Michigan. The Mandarin model was \n \nLanguage Development Research \n \n \n \n \n \n \nVolume 3, Issue 1, 31 December 2023 \n \n5 \ntrained on three corpora available on the TalkBank CHILDES database—Zhou Assess-\nment (Li & Zhou, 2011), Chang Personal Narrative (Chang & McCabe, 2013), and Li \nShared Reading. The ability to train new segmentation models based on segmented \nCHAT transcripts has been released along with the Batchalign2 software. In addition, \nwork \ncurrently \nin \nprogress \nby \nthe \nHuggingFace \ndiarization \nteam \n(https://github.com/huggingface/diarizers) using the pyannote framework (Bredin, \n2023) with TalkBank data should be able to provide tokenizers for a wider variety of \nlanguages. \n \nText-Media Alignment \nApart from the processing of new recordings, ASR can be useful for linking previously \nhand-transcribed transcripts to media for timing-aware analysis. The Batchalign2 \n“align” command supports this process by running a two-pass alignment of tran-\nscripts to media. The first pass of this process involves performing rough, utterance \ntime diarizations using ASR as a silver annotation reference. The second pass involves \nlatent feature extraction of each form’s timestamp within the utterance by using la-\ntent attention activations from the Whisper ASR model described above. \n \nUtterance Timing Recovery \nWe begin by assuming that the transcript to be linked has correctly segmented utter-\nance text, but that it does not yet have any utterance time values. If the transcript has \nimprecise time values, we can use the CLAN CHSTRING command with the +cbul-\nlets.cut switch to remove them.  We must then identify the relative time within the \nmedia in which an utterance occurred. This task is difficult to perform with classic \nalignment schemes, which face difficulty generating correct alignments among \nlonger timestamps without some form of hierarchical or recursive scheme (Moreno \net al., 1998), due to the exponential growth in number of possible alignments as se-\nquence length increases.  \n \nTo address this limitation, we take an optimistic, silver-labeling approach by using an \nASR-generated transcript (which can process the audio linearly by splitting it into seg-\nments) to obtain a silver transcript which we call the “backplate.” Because this ASR \ntranscript has been generated directly from the audio, each of its tokens are linked \nagainst a relative timestamp within the audio file. By then aligning the transcript \nagainst the backplate, we can induce the timestamp in which each utterance in the \ngold standard transcript exists by reading the corresponding times on the backplate.  \n \nTo perform the actual transcript-to-transcript alignment described above, we apply \ndynamic programming (Bellman, 1966) to create an alignment solution which mini-\nmizes the form-level Levenshtein edit distance (Jurafsky & Martin, 2009) between the \ngold transcript and the backplate. We can then calculate the level timings via direct \ncomputation using the first and last timestamps of aligned forms within an utterance \nlabelled by the gold transcript, plus some time on each end to account for errors \n \nLanguage Development Research \n \n \n \n \n \n \nVolume 3, Issue 1, 31 December 2023 \n \n6 \nwhich will be tightened in the second step of the overall alignment procedure. \n \nAlthough this procedure could theoretically also recover the timing of each individual \ntoken (simply by aligning the backplate transcript against gold at a token level) this \ninitial alignment is only practically feasible for utterance timing recovery. Instead, \nwe assume that the overall time alignment for an utterance (as denoted by the timing \nbetween its first aligned token and the last aligned token) should be roughly accurate. \nBecause we are doing utterance level alignment, any errors in the backplate (such as \nmissing a filled pause, a very common error in ASR) which are within the bounds of \nan utterance are essentially irrelevant to this procedure. Even if a particular utterance \nis not properly transcribed in the backplate, we can infer its temporal alignment by \nknowing the values for the previous and following utterances. In comparison, appli-\ncation of this procedure on the token level would result in missing time values for all \nforms which do not have precise alignments between the gold and backplate tran-\nscripts—reducing the quality of the resulting data.  \n \nWord-level Forced Alignment \nNext, to obtain word-level or token-level alignment, we use latent attention analysis \nthrough the Whisper ASR model. Recall that Whisper is an encoder-decoder architec-\nture model (Radford et al., 2023), whereby the encoder creates a latent embedding per \nsample (usually 16,000Hz) of the input audio sequence which is then used as input to \nthe cross-attention (Niu et al., 2021) computation against the output text sequence. \n \nThe key motivation of our analysis follows closely to previous work in cross-attention \nactivation analyses (Hou et al., 2019). We take advantage of the heuristic that the most \nhighly activated (high value) encoder-decoder cross-attention pairs are likely the most di-\nrectly relevant pairings. In the context of speech analysis, this means that the most \nhighly activated encoder time slice to decoder token activation is likely the best tem-\nporal alignment for the token. \n \nTo take advantage of this fact, we run a single forward pass on the Whisper model per \ntime-segmented utterance, supplying the utterance time slice (derived in the previous \nstep of the overall alignment procedure) as the encoder input and the gold utterance \ntext as the decoder input. Then, we extract the last cross-attention activation matrix \nfrom the model activations during this forward pass. \n \nFrom this, we apply a series of normalization procedures — mean centering and me-\ndian filter smoothing (Brownrigg, 1984) — to obtain a smoothed cross-attention ma-\ntrix. Taking highest values indices of this matrix along each axis reveals two se-\nquences — one for time along each slice and another for transcript-token along each \nslice. Finally, alignment between these two sequences — which are already sorted in \ntemporal order with alignments between them given by the matrix — will provide a \nresolved time-per-token value given by Dynamic Time Warping (DTW). \n \nLanguage Development Research \n \n \n \n \n \n \nVolume 3, Issue 1, 31 December 2023 \n \n7 \n \nThis procedure is relatively quick to compute. Although DTW has O(nm) time com-\nplexity, the sequences are reasonably short, and they do not require perfect ASR per-\nformance because the gold transcript is provided directly to the Whisper decoder. \nThrough this scheme, we obtain a precise time alignment for each input form which \ncan be used in downstream analysis. \n \nUniversal Dependencies \nNext, we will explain how Batchalign2 operates to produce morphosyntactic analyses. \nThis work relies on the application of Universal Dependency (UD) models trained \nthrough the Stanza Python NLP package (Qi et al., 2020). This system, which can be \nused with over 70 languages (https://universaldependencies.org), is based on a con-\nsistent language-general set of codes for POS (parts of speech), GFs (grammatical fea-\ntures), and GRs (grammatical relations). Stanza models for each UD language can be \ndownloaded for use by the Batchalign2 Python program which is freely available for \ndownload from https://github.com/talkbank.  Before reviewing the details of the ap-\nplication of UD tagging to CHILDES data, we need to consider the previous state-of-\nthe-art for tagging CHILDES transcripts. \n \nBeginning in 1995, Brian MacWhinney, Roland Hausser, and Mitzi Morris created a \nsystem for word-level morphological coding called MOR (MacWhinney, 2008). This \nsystem relied on a series of hand-crafted declarative rules governing possible word \nanalyses and a program called POST, created by Christophe Parisse (Parisse & Le Nor-\nmand, 2000) for disambiguating alternative readings in context. The resultant anal-\nyses were entered on a %mor line in which each word on the main speech line is given \nits own morphological analysis. The manual for MOR is available at https://talk-\nbank.org/manuals/MOR.pdf. Across the years, Leonid Spektor extended the the MOR \nprogram and Brian MacWhinney refined the lexicon and rules to achieve a high level \nof accuracy and coverage.  However, extending MOR to other languages represented \na major challenge.  Versions of MOR were created for French, Hebrew, Italian, Japa-\nnese, and Mandarin. However, these additional versions of MOR were created by a \nsingle person and learning how to build a new MOR grammar is difficult. Given this, \nextensions to the remaining 44 languages in CHILDES are outside the current scope \nof the project.   \n \nThe creation of automatic programs for syntactic analysis across these 49 languages \nfaced similar hurdles. Sagae and colleagues (Sagae et al., 2010) created a program \ncalled MEGRASP (maximum entropy grammatical relations syntactic processor) that \nuses the SVM (Support Vector Machine) method to tag CHILDES English and Spanish \ncorpora for grammatical relation dependency structure. In principle, MEGRASP \ncould be extended to cover additional languages.  However, settling on consistent la-\nbels for the grammatical relations in each language and applying those labels to a \nlarge corpus of training utterances represented yet another major task that would \n \nLanguage Development Research \n \n \n \n \n \n \nVolume 3, Issue 1, 31 December 2023 \n \n8 \nhave to be done one-by-one for all the languages in CHILDES. \n \nGiven the scope of the work needed to build MOR and MEGRASP analyzers for 49 lan-\nguages and for languages that will be added to CHILDES in the future, we looked for \nalternative methods for building morphosyntactic analyses across languages. Fortu-\nnately, the UD Project provides almost exactly what was needed. Relying on the latest \nAI/NLP technology, the UD community has been working to create taggers for 70 lan-\nguages, including a majority that are outside of Indo-European.  UD uses six open \nclass POS (part-of-speech) tags (ADJ, ADV, INTJ, NOUN, PROPN, and VERB) and eight \nclosed class POS tags (ADP, AUX, CCONJ, DET, NUM, PART, PRON, and SCONJ. It \nclusters GFs into seven lexical feature sets (PronType, NumType, Poss, Reflex, For-\neign, Abbr, and Typo), nine nominal inflectional feature sets (Gender, Animacy, \nNounClass, Number, Case, Definite, Deixis, DeixisRef, and Degree) and ten verbal in-\nflectional feature sets (VerbForm, Mood, Tense, Aspect, Voice, Evident, Polarity, Per-\nson, Polite, and Clusivity). Within each set, a further set of GF values is described. For \nexample, Gender has the values Masc, Fem, Neut, and Com.  Apart from this system-\natic listing of POS and GFs, UD provides a uniform nomenclature for grammatical \nrelations (GRs) with six core arguments (nsubj, obj, iobj, csubj, ccomp, and xcomp), \nten non-core dependents (obl, vocative, expl, dislocated, advcl, advmod, discourse, \naux, cop, and mark), and ten coordination relations (conj, cc, fixed, flat, list, para-\ntaxis, compound, orphan, goeswith, and reparandum). The UD web pages provide \ncomplete descriptions of all these POS, GFs, and GRs and the documentation for each \nlanguage shows how they map onto the language.  \n \nPreparing for UD Analysis \nTo align with the various format requirements of UD, Stanza, and Batchalign2, we \nfirst require transcripts need to be in full compliance with the CHAT format as vali-\ndated through the Chatter program which is available for download from https://talk-\nbank.org/software/chatter.html.  Because the CHILDES database had been validated \nusing earlier versions of Chatter that failed to enforce some of these requirements, \nwe had to sharpen the specifications in Chatter and reapply the new version to the \nentire CHILDES database.  That process involved a series of format fixes, such as sys-\ntematization of spacing, use of new fluency codes, and elimination of use of the plus \nsign for marking compounds. To permit alignment of text to audio, we also needed to \neliminate use of repetition codes such as [x 3] for three repetitions of a word or phrase \nand make overlap and retracing marking more consistent.  \n \nOnce the data are in the required format, we can run the “morphotag” command in \nBatchalign2. Internally, this process creates data in the CONLL-U format which is \nthen reformatted to the CHAT format to be written out in the %mor and %gra lines. \nThe POS and GF information is formatted into the %mor line and the GR information \nis outputted to the %gra line. \n \n \nLanguage Development Research \n \n \n \n \n \n \nVolume 3, Issue 1, 31 December 2023 \n \n9 \nMatching the requirements of the UD grammars with the tokenization and transcripts \nin the CHILDES files faces problems that vary from language to language. One chal-\nlenge found in nearly all the corpora is the use of eye-dialect to transcribe spoken \nforms. For example, in English some corpora may have used an apostrophe to repre-\nsent conversion of final /ŋ/ to final /n/ as in singin' which then had to be converted to \nsingin(g). Or German hab'n would be converted to hab(e)n for consistent recognition \nby the UD grammar. A form such as tactor could be converted to t(r)actor, whereas \npractor would be practor [: tractor].  For languages such as French, Italian, and Spanish \nthat had already gone through analysis by MOR, these conversions were already done, \nbut for other languages they had to be done from scratch. \n \nFor the Romance languages - French, Italian, Catalan, Portuguese, and Spanish - there \nwere often issues relating to clitics and portanteau forms.  For example, the French \ncorpora often inserted a space between preclitics and stems, as in j' ai rather than j'ai \nwith the latter being the form expected in standard French orthography. Such diver-\ngences were easy enough to fix using global replacements. More complicated cases \ninvolved conversions such as qu'est-ce-que into qu'est que. In each case, the goal of the \nconversions was to produce output that would match standard orthography, because \nthis is how UD is trained and what it expects. \n \nAnother issue facing UD analysis involved how best to handle multi-word expressions \n(MWE) which the NLP literature refers to as multi-word tokens (MWT).  For example, \nthe French word for today is aujourd'hui, but without entering this form specifically \nas an MWT, Stanza’s models would separate the front part as the prepositional phrase \nau jour (on the day) and then was unable to tag the remaining segment d'hui. To ad-\ndress this problem, we introduced a modification in the Stanza pipeline that allowed \nfor a specified set of MWTs which is checked against during its analysis for each lan-\nguage before downstream analysis such as lemmas, POS, dependencies, and features \nwhich would block this form of over-analysis. \n \nIt was also necessary to make sure that the word-level transcription for each language \nmatched the standard orthography used for that language, because each UD grammar \nwas trained on data in the standard orthography.  This meant that CHILDES corpora \nthat had been transcribed in a Latin or Roman orthography needed to be converted \nback to the standard orthography for that language.  For some languages, this conver-\nsion was simple, but for others it represented a greater problem.   \n \nCurrent State of UD Tagging \nHere we summarize the status of the conversion and tagging process for the 27 lan-\nguages in CHILDES that have available UD grammars. The 10 languages that have UD \ngrammars, but which have not yet been processed with UD are identified with aster-\nisks.  The other 27 have been either fully or partially tagged. These UD taggings rep-\nresent first drafts that have not yet been checked by native speakers and which will \n \nLanguage Development Research \n \n \n \n \n \n \nVolume 3, Issue 1, 31 December 2023 \n \n10 \nsurely require further fine-tuning and use of the MWT method described above. At \nthis point, no further conversion work will be needed for these 27 languages, and they \ncan all go smoothly through future automatic analysis when new versions of UD have \nbeen fine-tuned for each language. \n1. Afrikaans: Given its limited morphology and the limited use of eye-dialect in tran-\nscription, application of UD to Afrikaans went smoothly. \n2. *Arabic: The two current Arabic corpora use a romanization which will have to be \nconverted to Arabic script and analyzed through methods that rely on right-to-left \northography. \n3. *Basque: There are no obvious barriers to application of UD to Basque, but guid-\nance from native speakers would make the result more reliable. \n4. *Bulgarian: The Bulgarian romanization must be converted back to Cyrillic.  Un-\nfortunately, there are conflicting standards for romanization and many digraphs \nare ambiguous, so this conversion will require further analysis. \n5. Cantonese: Because the Cantonese corpora were transcribed in Hanzi, no script \nconversion was necessary.  In addition, UD for Chinese languages handles word-\nlevel tokenization directly, so there are no issues about any need to add or remove \nspaces between words.  \n6. Catalan: Processing of Catalan was straightforward. \n7. Croatian: Processing of Croatian was straightforward. \n8. Czech: Processing of Czech was straightforward. However, the contributors of the \nCzech corpus had already created a carefully done %mor analysis which they pre-\nferred to keep in place without the UD tags. \n9. Danish: Processing of Danish was straightforward. \n10. Dutch: Processing of Dutch was straightforward. \n11. English: To maintain backward compatibility of the English corpora, we keep the \ncurrent %mor and %gra lines and add in the new UD lines as %umor and %ugra.  \nThe %mor line provides greater morphological detail than the %umor line, partic-\nularly for compounds (which are not analyzed by UD).  However, the %ugra line \nis more accurate than the %gra line.  Making the UD lines available is important \nfor facilitating cross-linguistic analyses with the other languages, all of which have \nUD tagging. \n12. Estonian: Processing of Estonian was straightforward. \n13. French:  The French database is quite extensive.  However, after much detailed \nrepair, processing went smoothly. \n14. German: The German corpora required extensive revision of eye-dialect forms.  \nOnce that was done, processing went smoothly.  UD did a much better job than the \nprevious MOR in its assignment of case/number/gender roles to modifiers and \nnouns, as well as in creating an accurate %gra line. \n15. *Greek: Processing of Greek will depend on creation of a method for converting \nfrom the romanization back to the Modern Greek alphabet. \n16. *Hebrew: Hebrew has already been processed by a MOR grammar. However, UD \nprocessing of Hebrew will require conversion from romanization to Hebrew script \n \nLanguage Development Research \n \n \n \n \n \n \nVolume 3, Issue 1, 31 December 2023 \n \n11 \nand we have not yet located a method for doing this.   \n17. *Hungarian: The current Hungarian transcripts make extensive use of eye-dialect \nand phonological forms.  Once these are modified, processing should be straight-\nforward. \n18. Icelandic: Processing of Icelandic required extensive modification of eye-dialect \nforms that will need to be re-checked.  Otherwise, analysis was straightforward. \n19. *Indonesian: The huge size of the Indonesian corpus and the extensive use of eye-\ndialect will require a fair amount of work for this corpus. \n20. Irish:  Processing of Irish was straightforward. \n21. Italian: Processing of Italian was straightforward. Because Italian had earlier been \nanalyzed by MOR, there were few word level problems, except for dealing with \nseparation of clitics by spaces. \n22. Japanese:  Processing of Japanese has represented a unique challenge because of \nthe use of three orthographies (Kanzi, hiragana, katakana) and difficulties with \nword segmentation.  Two of the Japanese corpora have been tagged, but others \nwill need further orthographic work. \n23. Korean:  Korean involved no script transformation and processing went quite \nsmoothly.  \n24. Mandarin:  Because Mandarin had already been processed through MOR, there \nwere few irregularities in the transcripts.  Also, Mandarin involved no script trans-\nformation and processing went quite smoothly.  \n25. Norwegian:  Processing of Norwegian was straightforward. \n26. Polish: Processing of Polish was straightforward. \n27. Portuguese:  After some repair for clitics, MWEs, and format, processing of Portu-\nguese was straightforward. \n28. *Romanian:  Processing of Romanian is currently in progress. \n29. *Russian: Like Bulgarian, Russian will need conversion of romanization to Cyril-\nlic.  However, the extensive use of eye-dialect and phonological forms in the Rus-\nsian corpora will make this difficult. \n30. Serbian:  Serbian UD allows for Roman orthography.  As a result, processing of \nSerbian was straightforward. \n31. Slovenian: Processing of Slovenian was straightforward. \n32. Spanish: Most of the Spanish corpora had earlier been analyzed by MOR.  For \nthose corpora, processing was straightforward.  However, there are several Span-\nish corpora that will need further work for eye-dialect, phonological forms, and \nother divergences. \n33. Swedish:  Processing of the Andren corpus was straightforward.  However, work \nwith the Lund corpus will require treatment of eye-dialect and phonological \nforms. \n34. *Tamil: Processing of the Tamil transcripts will require conversion of the roman-\nization to Abugida orthography. \n35. *Thai: Like many other Asian languages, Thai orthography does not include spac-\ning, which makes tokenization difficult. Current Thai transcripts all use \n \nLanguage Development Research \n \n \n \n \n \n \nVolume 3, Issue 1, 31 December 2023 \n \n12 \nromanization and there is no clear path for conversion to Sukhothai script. \n36. Turkish: Processing of Turkish was straightforward.  However, because UD mor-\nphology is non-analytic, the %mor line fails to capture the agglutinative nature of \nTurkish word formation. A similar problem arises with Hungarian and Estonian.  \n37. Welsh: Processing of Welsh was straightforward, even though there are many \nforms that involve apostrophes for omissions.  Apparently, these forms are al-\nready accepted in standard Welsh in the training set for UD. \n \nMorphosyntactic Analysis \nHere we describe in further detail the application of the neural analysis models pro-\nvided by the Stanza (Qi et al., 2020) system, along with the modifications we make for \ncharacteristics of spoken language, child language, and language-specific forms. \n \nWord Tokenization \nThe first step of analysis involves tokenizing each utterance in the CHAT transcript \ninto tokens. Because the CHAT format (https://talkbank.org/manuals/CHAT.pdf) en-\ncodes tokenization by using whitespace delineated token groups to identify words, \ntokenization is frequently given natively in the transcript. \n \nHowever, for some languages token representations have little to do with word-level \nrepresentations. In Japanese child language, for instance, two of the language’s three \nwriting systems—hiragana and katakana—are moraic-based units frequently em-\nployed to transcribe a child during L1 development (Ota, 2015) while the third—kanji, \noften used for actual word representations needed for morphosyntactic analysis, \nhave little to do with phonology. Moreover, Japanese is not written with spaces. Be-\ncause of this, whitespace-delineated token representations are not a reliable source \nof information for word representations. \n \nFor languages which have this limitation—and in particular, for our analysis of Japa-\nnese—we employ the more complex token segmentation scheme given in Stanza \nwhich involves formulating word-level tokenization as a token labeling task—ignoring \nany transcribed tokenizations and labeling each input character as belonging to the \nstart, middle, or end of a token—before further processing each resulting \"token \ngroup\" via the downstream, semantic aware modules such as the Stanza lemmatizer.  \nFor instance, consider the Japanese phrase karuto dantai “cult group” : \nカルト団体 \nThe DNN tagger would first treat all constituent forms as separate and assign to each \none a beginning and inside tags representing word boundaries. This creates the se-\nquence: \nB I I B I \nFinally, separating the forms following the B tags, we obtain: \n[カルト] [団体] \nas the final word tokenizations, which we place back into the CHAT file as space-\n \nLanguage Development Research \n \n \n \n \n \n \nVolume 3, Issue 1, 31 December 2023 \n \n13 \ndelimited tokens as follows: \nカルト 団体 \n \nIn this way, we recover a canonical tokenization for those particular languages based \non the annotation style chosen by the working group of the target language in UD \nannotation; for Japanese, for instance, this may include some resulting orthographic \nKanji formed by joining tokens from other syllabaries following the short-unit word \n(SUW) style (Den et al., 2008). We then use this canonical tokenization to \"retokenize\" \nthe original CHAT transcript with this new tokenization. Once this initial re-tokeniza-\ntion is obtained, we can then proceed to the remaining analysis by the pipeline de-\nscribe here. \n \nMulti-Word Token and Form Correction \nUD (De Marneffe et al., 2021) distinguishes between tokens—continuous character \nspans without delineation in between—and syntactic words used in analysis. This dis-\ntinction is particularly relevant with respect to the treatment of multi-word tokens \n(MWTs)—a single continuous text span which contains multiple syntactic words, each \nwith individual features and dependencies which need to be analyzed independently. \nAugmenting Stanza’s neural-only analysis, we use a lexicon and orthography driven \napproach to identify and expand three types of such MWTs. \n \nTwo types of such MWTs are usually automatically recognized by Stanza through the \nsame tokenization procedure described in the section above: clitics and contractions. \nClitics are independent syntactical forms attached to other words, such as in Spanish \ndespertarme (despertar + me)—with the latter being a separate syntactic word which \nmodifies the previous word which needs to be analyzed independently (i.e. modifying \nthat I am who woke the object up); contractions are combinations of multiple words \ninto one token, such as in English I’m (I + am). \n \nIf clitics and contractions are not automatically expanded by Stanza, we use a rules-\nbased analysis of orthography to detect some of these common forms and manually \nexpand them. This functionality is currently supported for detection of subject con-\ntractions in French and Italian (i.e. t’aime to te + aime), prepositional contractions (i.e. \njusqu’ici to jusque + ici), and be-contractions in English (i.e. you’re to you + are). \n \nThe third type of MWT not typically expanded by Stanza, but which our pipeline uses \na lexicon to detect and expand, are single-unit, multi-word forms which are usually \njoined by an underscore in the CHAT transcription format (MacWhinney 2014) be-\ncause they are a single semantic form and multiple syntactic words. For instance, the \nform pirates_des_Caraïbes (Pirates of the Caribbean) is one such form, broken into pi-\nrates des Caraïbes. \n \nWe implement this correction functionality as a custom step in the Stanza analysis \n \nLanguage Development Research \n \n \n \n \n \n \nVolume 3, Issue 1, 31 December 2023 \n \n14 \npipeline; this step takes the \"draft\" tokenizations from Stanza as input and returns the \ncorrect tokenization and word expansions to downstream analysis functions in \nStanza—ensuring that POS, GFs, and GRs will be analyzed on the corrected word. \n \nAdditionally, the neural tokenizer in Stanza would occasionally mark forms as MWTs \nwhen they are simply single-token single-word forms with a punctuation within (i.e. \nthe French word aujourd’hui); in those cases, we perform the opposite correction \nforcing Stanza to treat the resulting token a single word instead of an MWT. These \ncases are identified and corrected using a lexicon as well. \n \nIn final output into the CHAT transcription format, we follow the convention set forth \nby the CLAN MOR/MEGRASP system (MacWhinney et al., 2012) and join the morphol-\nogy analyses of multi-word tokens together with a tilde (~), maintaining token-level \nalignment between the transcript and analysis yet being able to encode multiple \nwords within a token. \n \nMorphology and Dependency Analysis \nAfter tokenization and MWT correction, we make no further adjustments to the \nStanza morphology, dependency, and feature analysis of each language and simply \nrun the remaining Stanza analysis pipeline with the corrected tokens. Because most \nStanza models are trained via the Universal Dependencies dataset, some datasets, \nsuch as UD Dutch Alpino (Bouma et al., 2001), will be rich in annotated feature infor-\nmation whereas some others, such as UD Japanese GSD (Nivre et al., 2020), will have \nlittle to no GFs annotated. For Japanese, this is true in part because many of the GRs \nare expressed in separate morphology. Our UD analysis, therefore, carries the design \nchoices of analysis made within these gold datasets. Once this information on POS, \nGFs, and GRs has been annotated by the Stanza system, we proceed to perform mor-\nphology-dependent extraction and correction of the resulting features as a final pro-\ncessing step. \n \nMorphosyntactic Transcription and Feature Correction \nAfter analysis by Stanza, we output the extracted GFs using an annotation format very \nsimilar to the one used in the MOR/MEGRASP system (described further in \nhttps://talkbank.org/manuals/mor.pdf) for the %mor and %gra lines in CHAT. Our \noverarching goal is to report the maximal set of GFs which 1) can be reported for each \nlanguage and 2) provide additional information beyond the \"default\" case. \n \nIn accord with these principles, the GFs for aspect, mood, tense, polarity, clusivity, \ncase, type, degree, conjugation (form), and politeness are reported exactly as in the \nUD annotation specifications. Gender is reported for all tagged genders except \"com-\nmon neutral\" (ComNeut); and number is reported for all except singular. For person-\nhood, fourth and zeroth person are both reported as \"fourth person\". As in MOR, GFs \nare joined after the lemma by using a dash \"-\" and contractions and clitics are marked \n \nLanguage Development Research \n \n \n \n \n \n \nVolume 3, Issue 1, 31 December 2023 \n \n15 \nwith ~, as in the earlier MOR standard. \n \nDependency Structure \nIn addition to creating a %mor line with its analysis of POS and GFs, Batchalign2 also \nproduces a %gra line that encodes the GRs for each utterance. The creation of this GR \nanalysis is the primary goal of the Universal Dependencies project. The encoding in-\nvolves a directed acyclic graph in which words are connected through unidirectional \narcs from the dependent word to its head. Each arc is labeled with a grammatical re-\nlation tag taken from the list summarized earlier. Using the GraphViz web service \n(https://github.com/xflr6/graphviz), one can double-click on a %gra line to produce a \ndisplay such as the screenshot in Figure 1 which comes from a parental utterance in \nthe Brown/Eve/020000b.cha file on line 44. \n \n \n \n \nFigure 1. Dependency analysis by UD for an example utterance. \n \nThis graph derives from processing of this utterance: \n*MOT: but you don't have a brown one . \n%mor: cconj|but pron|you-Prs-Nom-S2 aux|do-Fin-Ind-Pres-S2~part|not  \n \nverb|have-Inf-S det|a-Ind-Art adj|brown-Pos-S1 noun|one . \n%gra: 1|5|CC 2|5|NSUBJ 3|5|AUX 4|5|ADVMOD 5|8|ROOT 6|8|DET 7|8|AMOD 8|5|OBJ \n \n9|5|PUNCT \nIn the %gra line, each word has two numbers and a GR. The first number is its serial \nposition in the utterance and the second is the position of the word to which it is linked \nthrough a GR.  After the two numbers comes the label on the GR. In Figure 1, for ex-\nample, we see that the word one links to the verb have through the OBJ relation, that \nthe word brown links to one through the MOD relation, and so on. This form of display \nis essentially the same as what was produced by MEGRASP (Figure 2), although the \nlabels on the arcs are changed and in UD the word not is linked to the auxiliary do \nrather than directly to the verb. \n \nLanguage Development Research \n \n \n \n \n \n \nVolume 3, Issue 1, 31 December 2023 \n \n16 \n \n \n \nProcessing based on UD Analysis \n \nHaving tagged corpora in 27 of the languages in CHILDES for POS, GFs, and GRs, we \nare able to apply many of the TalkBank analytic tools that were earlier available only \nfor English. This opportunity can go a long way toward reducing the WEIRD emphasis \nin child language studies. Most of these tools and frameworks will work directly, but \nsome require further configuration. We can now use them to compute indices and \nprofiles for the three data formats discussed earlier: longitudinal case studies, cross-\nsectional group studies, and crosslinguistic comparisons. In other words, having this \nmorphosyntactic information available for all 27 languages benefits not only cross-\nlinguistic comparison, but also the language-internal examination of development for \nindividual children and clinically-important comparison groups within each lan-\nguage. The tools that are available now or which will soon be available include: \n1. Basic analysis commands: Researchers could make use of the 26 basic analysis \ncommands in CLAN on all languages prior to running of Batchalign2.  How-\never, because most of the languages previously had no %mor or %gra line, \nanalyses were limited to the main speech tier. Now these same programs can \nrun on these additional lines, making many additional types of analyses possi-\nble. \n2. KIDEVAL: This command combines 57 CLAN analyses into a single package.  \nIt includes tracking of the most common GFs in each language, repetitions, \nvocabulary diversity, error types, MLU (mean length of utterance), and other \nindicators. In a single command, KIDEVAL can be run on a single transcript or \na whole folder of transcripts. It gives both the results for each child on each \nmeasure as well as a z-score for the extent to which the child matches a larger \ncomparison group for that measure.  The comparison group can be selected \nfor age group in 6-month intervals, participant type (TD, DLD, ASD, etc.) and \nrecording type (narrative, free play, elicited). For this comparison to be mean-\ningful, KIDEVAL needs a comparison sample of at least 25 cases.  This is cur-\nrently possible for Dutch, English, French, Japanese, Mandarin, and Spanish. \nConstruction of comparison corpora for other languages that have sufficient \ncomparison data is in progress.  \n \nFigure 2. Dependency analysis by MEGRASP for an example utterance. \n \nLanguage Development Research \n \n \n \n \n \n \nVolume 3, Issue 1, 31 December 2023 \n \n17 \n3. DSS:  DSS (Developmental Sentence Score) (Lee, 1974) is a profiling method \nthat focuses on early learning of grammatical morphology and basic syntax in \nEnglish. Given the new availability of a consistent set of POS, GF and GR tags, \nit will now be much easier to configure versions of DSS for additional lan-\nguages. \n4. IPSyn: IPSyn (the Index of Productive Syntax) (Scarborough, 1990) is similar to \nDSS. However, it includes measures of more advanced syntactic structures. \nBuilding on recent analyses (MacWhinney et al., 2020; Yang et al., 2021) we can \ncreate streamlined, automatic versions of IPSyn for multiple languages. \n5. Vocabulary diversity: CLAN provides four measures of vocabulary diversity: \nTTR (type token ratio), NDW (number of different words), MATTR (moving av-\nerage type token ratio) (Covington & McFall, 2010), and vocD (Malvern & Rich-\nards, 1997). Analysis through MATTR and vocD requires use of lemmas on the \n%mor line which is now possible across the 27 languages to which UD has been \napplied. \n6. GF analysis: Although a basic level of GF analysis is built into KIDEVAL, there \nare many types of crosslinguistic analysis that will be best conducted using pro-\ngrams like FREQ on the %mor line across languages.  For example, we can now \nlook consistently at learning of tense marking across all these languages and \nobserve how that feature is acquired in comparison with other features. \n7. GR analysis: It is now possible to use GraphViz to visualize the syntactic struc-\nture for all 27 languages. In addition, Section 7.9.14 of the CLAN manual de-\nscribes how to use FREQ with the UD %gra line to study the emergence of more \ncomplex relations, such as xcomp (a clausal complement without its own sub-\nject) or expl:pass (a reflexive marker of a middle or passive clause), as well as \ncombinations of GRs. \n8. Cross-tier analysis: We are currently building a new program called FLUPOS \nfor tracking features across multiple coding tiers, including the main line, \n%mor, %gra, and the %pho line for phonology. One particularly important ap-\nplication of FLUPOS will be to determine the degree to which disfluencies are \nproportionally higher with certain lexical, morphological, phonological, and \nsyntactic configurations. \n \nThe combination of these new %mor and %gra tiers for these 27 languages, along with \ncurrent analytic methods and ones we plan to build will provide us with a strong quan-\ntitative foundation for crosslinguistic analysis of language development. We will be \nable to track the impact of language structure and input on the development of lexi-\ncon, morphology, and syntax in a set of languages that goes well beyond the limits of \ndata from only WEIRD participants. \n \n \nAuthorship and Contributorship Statement \n \n \nLanguage Development Research \n \n \n \n \n \n \nVolume 3, Issue 1, 31 December 2023 \n \n18 \nHoujun Liu is the author of the Batchalign2 program and Brian MacWhinney applied \nthe program to analyze corpora in 27 languages. Both authors shared in the concep-\ntion of the work, writing of this report, and approval of the final version. Both ensure \nthat questions related to the accuracy or integrity of any part of the work will be ap-\npropriately investigated and resolved.  \n \nData, code and materials availability statement  \n \nBatchalign2 is available from https://github.com/talkbank/batchalign2. The utter-\nance segmentation models trained using the manner described in (Liu et al. 2023), as \nwell as fine-tuned, language specific Whisper models, are additionally available for \nUS English and Mandarin; these models are available at https://huggingface.co/talk-\nbank.  The tagged corpora for the 27 languages discussed here are available from \nhttps://childes.talkbank.org.  All the corpora in CHILDES are open-access and have \nassociated DOIs. \n \nLicense \n \nLanguage Development Research (ISSN 2771-7976) is published by TalkBank and the Car-\nnegie Mellon University Library Publishing Service. Copyright © 2023 The Author(s). \nThis work is distributed under the terms of the Creative Commons Attribution-Non-\ncommercial 4.0 International license (https://creativecommons.org/licenses/by-\nnc/4.0/), which permits any use, reproduction and distribution of the work for non-\ncommercial purposes without further permission provided the original work is at-\ntributed as specified under the terms available via the above link to the Creative Com-\nmons website. \n \nReferences  \n \nBellman, R. (1966). Dynamic programming. Science, 153(3731), 34-37. PMC  \nBernstein Ratner, N., & MacWhinney, B. (2020). TalkBank resources for \npsycholinguistic analysis and clinical practice. In A. Pareja-Lora, M. Blume, & B. Lust \n(Eds.), Development of Linguistic Linked Open Data Resources for Collaborative Data-\nIntensive \nResearch \nin \nthe \nLanguage \nSciences \n(pp. \n131-150). \nMIT \nPress. \nhttps://psyling.talkbank.org/years/2018/RatnerMacW.pdf  \nBernstein Ratner, N., & MacWhinney, B. (2023). Assessment and therapy goal \nplanning using free computerized language analysis software. Perspectives of the ASHA \nSpecial Interest Groups, 8(1), 19-31. https://doi.org/10.1044/2022_PERSP-22-00156 PMC  \nBishop, D. (1982). The test of reception of grammar. Medical Research Council.  \nBloch, O. (1921). Les premiers stades du langage de l’enfant. Journal de Psychologie, 18, \n693-712. PMC  \nBouma, G., Van Noord, G., & Malouf, R. (2001). Alpino: Wide-coverage computational \nanalysis of Dutch. In Computational linguistics in the Netherlands 2000 (pp. 45-59). Brill.  \n \nLanguage Development Research \n \n \n \n \n \n \nVolume 3, Issue 1, 31 December 2023 \n \n19 \nBredin, H. (2023). pyannote. audio 2.1 speaker diarization pipeline: principle, \nbenchmark, and recipe. 24th INTERSPEECH Conference (INTERSPEECH 2023),  \nBrown, R. (1973). A first language: The early stages. Harvard University Press. \nhttps://doi.org/10.4159/harvard.9780674732469  \nBrownrigg, D. R. (1984). The weighted median filter. Communications of the ACM, \n27(8), 807-818. PMC  \nChang, C.-j., & McCabe, A. (2013). Evaluation in Mandarin Chinese children’s \npersonal narratives. Studies in Narrative (SiN). PMC  \nChao, Y. R. (1951). The Cantian ideolect: An analysis of the Chinese spoken by a \ntwenty-eight-months-old child. University of California Publications in Semitic \nPhilolology, 1, 27-44. PMC  \nCovington, M. A., & McFall, J. D. (2010). Cutting the Gordian knot: The moving-\naverage type–token ratio (MATTR). Journal of Quantitative Linguistics, 17(2), 94-100. \nhttps://doi.org/10.1080/02687038.2012.693584 PMC PMC4569132  \nCrystal, D., Fletcher, P., & Garman, M. (1989). The grammatical analysis of language \ndisability. Second Edition. Cole and Whurr.  \nDale, P. S., & Fenson, L. (1996). Lexical development norms for young children. \nBehavior Research Methods, Instruments, and Computers, 28. PMC  \nDe Marneffe, M.-C., Manning, C. D., Nivre, J., & Zeman, D. (2021). Universal \ndependencies. Computational Linguistics, 47(2), 255-308. PMC  \nDel Rio, M., Ha, P., McNamara, Q., Miller, C., & Chandra, S. (2022). Earnings-22: A \npractical benchmark for accents in the wild. arXiv preprint arXiv:2203.15591. PMC  \nDen, Y., Nakamura, J., Ogiso, T., & Ogura, H. (2008). A Proper Approach to Japanese \nMorphological Analysis: Dictionary, Model, and Evaluation. LREC,  \nDevlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2018). Bert: Pre-training of deep \nbidirectional \ntransformers \nfor \nlanguage \nunderstanding. \narXiv \npreprint \narXiv:1810.04805. PMC  \nFrank, M. C., Braginsky, M., Yurovsky, D., & Marchman, V. A. (2021). Variability and \nconsistency in early language learning: The Wordbank project. MIT Press.  \nFraser, K. C., Ben-David, N., Hirst, G., Graham, N., & Rochon, E. (2015). Sentence \nsegmentation of aphasic speech. Proceedings of the 2015 conference of the North \nAmerican chapter of the Association for Computational Linguistics: Human language \ntechnologies,  \nGarbarino, J., Bernstein Ratner, N., & MacWhinney, B. (2020). Use of computerized \nlanguage analysis to assess child language. Language, Speech, and Hearing Services in \nSchools, 51(2), 504-506. https://doi.org/10.1044/2020_LSHSS-19-00118 PMC 7225019  \nGoldman, R., & Fristoe, M. (2000). The Goldman-Fristoe Test of Articulation - 2. Pearson \nAssessments.  \nGuillaume, P. (1927). Les débuts de la phrase dans le langage de l'enfant. Journal de \nPsychologie, 24, 1-25. PMC  \nGvozdev, A. N. (1949). Formirovaniye u rebenka grammaticheskogo stroya. Akademija \nPedagogika Nauk RSFSR.  \nHenrich, J., Heine, S. J., & Norenzayan, A. (2010). The weirdest people in the world? \n \nLanguage Development Research \n \n \n \n \n \n \nVolume 3, Issue 1, 31 December 2023 \n \n20 \nBehavioral and Brain Sciences, 33(2-3), 61-83. PMC  \nHou, R., Chang, H., Ma, B., Shan, S., & Chen, X. (2019). Cross attention network for \nfew-shot classification. Advances in Neural Information Processing Systems, 32. PMC  \nJurafsky, D., & Martin, J. H. (2009). Speech and Language Processing: An Introduction \nto Natural Language Processing, Computational Linguistics, and Speech Recognition. \nIn. New York: Prentice-Hall. \nKarniol, R. (2010). Social development as preference management: How infants, children, \nand parents get what they want from one another. Cambridge University Press.  \nKenyeres, E. (1926). A gyermek elsö szavai es a szófajók föllépése. Kisdednevelés.  \nLee, L. (1974). Developmental Sentence Analysis. Northwestern University Press.  \nLeopold, W. (1939). Speech development of a bilingual child: a linguist's record: Vol. 1. \nVocabulary growth in the first two years (Vol. 1). Northwestern University Press.  \nLeopold, W. (1947). Speech development of a bilingual child: a linguist's record: Vol. 2. \nSound-learning in the first two years. Northwestern University Press.  \nLeopold, W. (1949a). Speech development of a bilingual child: a linguist's record: Vol. 3. \nGrammar and general problems in the first two years. Northwestern University Press.  \nLeopold, W. (1949b). Speech development of a bilingual child: a linguist's record: Vol. 4. \nDiary from age 2. Northwestern University Press.  \nLi, L., & Zhou, J. (2011). Preschool children’s development reading comprehension of \npicture storybook: from a perspective of multimodal meaning making. East China Normal \nUniversity]. Shanghai.  \nLiu, H., MacWhinney, B., Fromm, D., & Lanzi, A. (2023). Automation of language \nsample analysis. Journal of Speech, Language, and Hearing Research, 66, 2421-2433. \nhttps://doi.org/10.1044/2023_JSLHR-22-00642 PMC  \nMacWhinney, B. (2000). The CHILDES project: Tools for analyzing talk. 3rd edition. \nLawrence Erlbaum Associates. https://www.amazon.com  \nMacWhinney, B. (2008). Enriching CHILDES for morphosyntactic analysis. In H. \nBehrens (Ed.), Trends in corpus research: Finding structure in data (pp. 165-198). John \nBenjamins. https://psyling.talkbank.org/years/2008/morphosyntax.pdf  \nMacWhinney, B., & Fromm, D. (2022). Language sample analysis with TalkBank: An \nupdate \nand \nreview. \nFrontiers \nin \nCommunication, \n7, \n865498. \nhttps://doi.org/10.3389/fcomm.2022.865498 PMC  \nMacWhinney, B., Roberts, J., Altenberg, E., & Hunter, M. (2020). Improving \nautomatic IPSyn coding. Language, Speech, and Hearing Services in Schools, 51(4), 1187-\n1189. https://doi.org/10.1044/2020_LSHSS-20-00090 PMC 7842849  \nMacWhinney, B., Spektor, L., Chen, F., & Rose, Y. (2012). Best practices in the \nTalkBank framework. 8th International Conference on Language Resources and \nEvaluation (LREC), Istanbul. https://psyling.talkbank.org/years/2012/LREC-best.pdf  \nMalvern, D., & Richards, B. (1997). A new measure of lexical diversity. In A. Ryan & \nA. Wray (Eds.), Evolving models of language (pp. 58-71). Multilingual Matters.  \nMoreno, P. J., Joerg, C. F., Van Thong, J.-M., & Glickman, O. (1998). A recursive \nalgorithm for the forced alignment of very long audio segments. ICSLP,  \nNiu, Z., Zhong, G., & Yu, H. (2021). A review on the attention mechanism of deep \n \nLanguage Development Research \n \n \n \n \n \n \nVolume 3, Issue 1, 31 December 2023 \n \n21 \nlearning. Neurocomputing, 452, 48-62. PMC  \nNivre, J., De Marneffe, M.-C., Ginter, F., Goldberg, Y., Hajic, J., Manning, C. D., \nMcDonald, R., Petrov, S., Pyysalo, S., & Silveira, N. (2016). Universal dependencies v1: \nA multilingual treebank collection. Proceedings of the Tenth International \nConference on Language Resources and Evaluation (LREC'16),  \nNivre, J., De Marneffe, M.-C., Ginter, F., Hajič, J., Manning, C. D., Pyysalo, S., \nSchuster, S., Tyers, F., & Zeman, D. (2020). Universal Dependencies v2: An \nevergrowing multilingual treebank collection. arXiv preprint arXiv:2004.10643. PMC  \nOta, M. (2015). L1 phonology: phonological development. The handbook of Japanese \nlanguage and linguistics: Phonetics and phonology, 681-717. PMC  \nParisse, C., & Le Normand, M.-T. (2000). Automatic disambiguation of the \nmorphosyntax in spoken language corpora. Behavior Research Methods, Instruments, \nand Computers, 32(3), 468-481. https://doi.org/10.3758/bf03200818 PMC  \nPavlovitch, M. (1920). Le langage enfantin: Acquisition du serbe et du francais par un \nenfant serbe. Champion.  \nPonori, T. E. (1871). A gyermeknyelvról. Természettudományi Közlöny, 3, 117-125. PMC  \nQi, P., Zhang, Y., Zhang, Y., Bolton, J., & Manning, C. D. (2020). Stanza: A Python \nnatural language processing toolkit for many human languages. Proceedings of the \n58th Annual Meeting of the Association for Computational Linguistics: System \nDemonstrations,  \nRadford, A., Kim, J. W., Xu, T., Brockman, G., McLeavey, C., & Sutskever, I. (2022). \nRobust speech recognition via large-scale weak supervision.  \nRadford, A., Kim, J. W., Xu, T., Brockman, G., McLeavey, C., & Sutskever, I. (2023). \nRobust speech recognition via large-scale weak supervision. International \nConference on Machine Learning,  \nRömer, U. (2019). MICASE: Michigan corpus of academic spoken English. \nhttps://doi.org/doi:10.21415/QT9V-2J96 \nSagae, K., Davis, E., Lavie, A., MacWhinney, B., & Wintner, S. (2010). \nMorphosyntactic annotation of CHILDES transcripts. Journal of Child Language, 37(3), \n705-729. https://doi.org/10.1017/S0305000909990407 PMC 4048841  \nScarborough, H. (1990). Index of Productive Syntax. Applied Psycholinguistics, 11(1), 1-\n22. https://doi.org/10.1017/S0142716400008262 PMC  \nSlobin, D. (1985). The crosslinguistic study of language acquisition. Volume 1: The data. \nLawrence Erlbaum Associates.  \nSmith, N. V. (1973). The acquisition of phonology: A case study. Cambridge University \nPress.  \nSmoczynska, M. (2017). The acquisition of Polish. In The crosslinguistic study of \nlanguage acquisition (pp. 595-686). Psychology Press.  \nStern, C., & Stern, W. (1907). Die Kindersprache. Barth.  \nSzuman, S. (1959). O wypowiedzianej oraz domyślnej treści wypowiedzi dziecka z \npierwszychlat jego życia. Przegląd Psychologiczny. PMC  \nTamis-LeMonda, C. S., Kachergis, G., Masek, L. R., Gonzalez, S. L., Soska, K. C., \nHerzberg, O., Xu, M., Adolph, K. E., Gilmore, R. O., & Bornstein, M. H. (2024). \n \nLanguage Development Research \n \n \n \n \n \n \nVolume 3, Issue 1, 31 December 2023 \n \n22 \nComparing apples to manzanas and oranges to naranjas: A new measure of English-\nSpanish \nvocabulary \nfor \ndual \nlanguage \nlearners. \nInfancy. \nhttps://doi.org/10.1111/infa.12571 PMC  \nTomasello, M. (1992). First verbs: A case study of early grammatical development. \nCambridge University Press.  \nYang, J. S., MacWhinney, B., & Ratner, N. B. (2021). The Index of Productive Syntax: \nPsychometric Properties and Suggested Modifications. American Journal of Speech-\nLanguage Pathology, 1-18. PMC 9135028  \n \n",
  "categories": [
    "cs.CL"
  ],
  "published": "2024-07-17",
  "updated": "2024-07-17"
}