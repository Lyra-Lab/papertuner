{
  "id": "http://arxiv.org/abs/2407.17628v1",
  "title": "PEEKABOO: Hiding parts of an image for unsupervised object localization",
  "authors": [
    "Hasib Zunair",
    "A. Ben Hamza"
  ],
  "abstract": "Localizing objects in an unsupervised manner poses significant challenges due\nto the absence of key visual information such as the appearance, type and\nnumber of objects, as well as the lack of labeled object classes typically\navailable in supervised settings. While recent approaches to unsupervised\nobject localization have demonstrated significant progress by leveraging\nself-supervised visual representations, they often require computationally\nintensive training processes, resulting in high resource demands in terms of\ncomputation, learnable parameters, and data. They also lack explicit modeling\nof visual context, potentially limiting their accuracy in object localization.\nTo tackle these challenges, we propose a single-stage learning framework,\ndubbed PEEKABOO, for unsupervised object localization by learning context-based\nrepresentations at both the pixel- and shape-level of the localized objects\nthrough image masking. The key idea is to selectively hide parts of an image\nand leverage the remaining image information to infer the location of objects\nwithout explicit supervision. The experimental results, both quantitative and\nqualitative, across various benchmark datasets, demonstrate the simplicity,\neffectiveness and competitive performance of our approach compared to\nstate-of-the-art methods in both single object discovery and unsupervised\nsalient object detection tasks. Code and pre-trained models are available at:\nhttps://github.com/hasibzunair/peekaboo",
  "text": "ZUNAIR, HAMZA: PEEKABOO: UNSUPERVISED OBJECT LOCALIZATION\n1\nPEEKABOO: Hiding Parts of an Image for\nUnsupervised Object Localization\nHasib Zunair\nhasibzunair@gmail.com\nA. Ben Hamza\nhamza@ciise.concordia.ca\nConcordia University\nMontreal, QC, Canada\nAbstract\nLocalizing objects in an unsupervised manner poses significant challenges due to\nthe absence of key visual information such as the appearance, type and number of ob-\njects, as well as the lack of labeled object classes typically available in supervised set-\ntings. While recent approaches to unsupervised object localization have demonstrated\nsignificant progress by leveraging self-supervised visual representations, they often re-\nquire computationally intensive training processes, resulting in high resource demands\nin terms of computation, learnable parameters, and data. They also lack explicit model-\ning of visual context, potentially limiting their accuracy in object localization. To tackle\nthese challenges, we propose a single-stage learning framework, dubbed PEEKABOO,\nfor unsupervised object localization by learning context-based representations at both the\npixel- and shape-level of the localized objects through image masking. The key idea is to\nselectively hide parts of an image and leverage the remaining image information to infer\nthe location of objects without explicit supervision. The experimental results, both quan-\ntitative and qualitative, across various benchmark datasets, demonstrate the simplicity,\neffectiveness and competitive performance of our approach compared to state-of-the-art\nmethods in both single object discovery and unsupervised salient object detection tasks.\nCode and pre-trained models are available at: https://github.com/hasibzunair/peekaboo\n1\nIntroduction\nLocalizing objects in images or videos is a fundamental task in many real-world computer\nvision systems, including autonomous driving and robotics. The goal is to detect/segment the\nmost salient objects in an image. High-performing methods for object localization typically\nrely on large cohorts of human-annotated datasets for supervised learning [19]. However, this\nlearning paradigm faces two main significant limitations. First, acquiring annotated datasets\nis notoriously time-consuming and costly, as well as prone to errors due in part to annotators’\nfatigue. Second, the finite and predefined nature of object classes limits the scope of what\nsupervised models can localize, rendering them ineffective for segmenting novel objects.\nIn recent years, several approaches have been proposed to tackle the daunting task of\nunsupervised object localization, which is inherently challenging. This difficulty arises from\nthe need to identify salient objects without prior knowledge of their appearance or reliance\non human-annotated datasets. Some approaches for unsupervised object discovery leverage\n© 2024. The copyright of this document resides with its authors.\nIt may be distributed unchanged freely in print or electronic forms.\narXiv:2407.17628v1  [cs.CV]  24 Jul 2024\n2\nZUNAIR, HAMZA: PEEKABOO: UNSUPERVISED OBJECT LOCALIZATION\nregion proposal techniques and representer point selection [32, 34, 48], while others rely\non adversarial training strategies [23, 37]. Moreover, some methods refine mask propos-\nals [39, 44], albeit at the expense of requiring millions of learnable parameters, extensive\nunlabeled image datasets, and multi-stage training procedures. Drawing upon the localiza-\ntion properties of self-supervised visual representations [7], training-free graph-based meth-\nods [24, 30, 40] have demonstrated robust performance across various tasks and benchmarks.\nHowever, these approaches often struggle in practical real-world scenarios due largely to\nvariations in illumination, reflections on water and shiny surfaces [31], overlapping ob-\njects [30], occlusions of small objects, complex backgrounds, assumptions that the largest\nobject is salient, and segmenting multiple objects as one [22, 24, 40]. Furthermore, they\nmay erroneously count multiple instances of the same object as one, leading to over- or\nunder-segmentation, as well as segment non-salient regions and produce noisy and discon-\ntinuous predictions. In addition, training-based methods impose significant computational\ndemands [29], rely on extensive unlabeled image datasets [27], employ combinations of\nmultiple networks, ensemble methods, and require test-time training [1, 29]. Overcoming\nthese challenges is essential for deploying reliable vision systems in real-world scenarios,\nsuch as autonomous driving, where accuracy and efficiency are paramount. Therefore, there\nis a pressing need to develop unsupervised object localization methods that are both accurate\nand computationally efficient.\nTo address the aforementioned challenges, we introduce PEEKABOO1, a novel single-\nstage learning framework for unsupervised object localization by adopting a context-based\nrepresentation learning strategy to enhance foreground-background discrimination, without\nresorting to pixel-level reconstruction. Given an image and its heavily masked counterpart,\nour model first predicts pixel-level semantic labels for both the unmasked and masked in-\nputs. This process exploits contextual information by utilizing nearby non-masked pixels to\ninform predictions for masked pixels, followed by learning shape-level context by ensuring\nconsistency between the object mask predictions for the masked and unmasked inputs. The\nmain contributions of this work can be summarized as follows: (1) We propose a learning\nparadigm that aims to explicitly model context through hiding parts of an image for unsuper-\nvised object localization; (2) We show through experimental results and ablation studies on\nsix benchmark datasets that PEEKABOO yields competitive performance against state-of-\nthe-art methods in both single object discovery and unsupervised saliency detection tasks.\n2\nRelated work\nUnsupervised Object Localization.\nGenerative representation learning methods like the\none proposed by Melas-Kyriazi et al. [23] employ a two-stage adversarial training approach,\nutilizing BigBiGAN [11] to create a synthetic dataset, followed by segmenter training. An-\nother line of work involves generating mask proposals that are refined through training.\nFreeSOLO [39] proposes Free Mask to generate coarse object masks refined via self-training,\nbut requires several millions of learnable parameters and large-scale unlabeled image datasets.\nMore recent efforts leverage the strong localization properties of DINO [7], a self-supervised\npre-training method, with or without additional training. For instance, LOST [30] is training-\nfree and constructs a weighted graph of patches, segmenting foreground objects based on\ntheir similarity, but struggles in scenarios involving object overlap or when objects cover\n1The name PEEKABOO is inspired by the children’s game of hiding one’s face and then revealing it suddenly.\nZUNAIR, HAMZA: PEEKABOO: UNSUPERVISED OBJECT LOCALIZATION\n3\nmost of the image. TokenCut [40] and Deep Spectral Methods (DSMs) [24] are graph-based\nmethods, but their performance is hindered by the computational overhead of computing\nthe eigenvectors of the Laplacian matrix. They often assume a single salient object occu-\npies the foreground and may count multiple instances of the same object as one. Moreover,\nthey struggle with small and occluded objects, leading to over- or under-segmentation. Self-\nMask [29] is training-based method, which employs an ensemble of three self-supervised\nvision encoders (SwAV [6], DINO [7], MoCov2 [10]) and spectral clustering to generate\nmore than twenty salient masks for each image, making the training considerably more ex-\npensive. DINOSAUR [27] learns representations from hundreds of thousands of unlabeled\nimages by separating the features of an image and reconstructing them into individual objects\nor parts. WSCUOD [22] proposes a semantic-guided method that extracts object-centric rep-\nresentations to generate the final object regions from the image. DeepCut [1] utilizes features\nextracted by DINO [7] and trains a graph neural network to generate segmentation masks,\nincorporating test-time training on each image. In contrast to these approaches, our method\navoids heavy and complex training procedures such as large-scale adversarial training, multi-\nstage training, learning millions of parameters, and test-time training.\nMasked Image Modeling for Vision.\nWhile early approaches for masked image model-\ning focused on reconstructing missing parts from masked images [20, 26], recent methods\nsuch as Masked Autoencoders [16] and DINO [7] have aimed to predict visual tokens in a\nself-supervised manner [2]. Also, masking has recently been utilized as supervisory signals\nfor models to learn context-based representations in fully supervised learning settings, fa-\ncilitating accurate perception of 2D [49] and 3D [21] visual data. However, the application\nof masked modeling tailored for unsupervised learning, specifically for discovering novel\nobjects from unlabeled data, remains relatively unexplored. Moreover, our approach distin-\nguishes itself from existing methods by making pixel-level predictions instead of predicting\nmissing parts of the input itself (i.e., pixel or visual token).\n3\nMethod\nWe begin by formulating the task at hand and subsequently introduce the main components of\nthe proposed masked unsupervised object localization framework, as illustrated in Figure 1.\nProblem Statement.\nWe address two main tasks in unsupervised object localization from\nimages: single object discovery and unsupervised saliency detection. The former involves\nenclosing the main object or one of the main objects of interest within a bounding box, while\nthe latter aims to generate a binary mask where foreground objects are assigned a value of 1\nand the background is assigned 0, thereby highlighting the object(s) of interest in the image.\nBoth tasks are class-agnostic, i.e., they are not restricted to a predefined set of object classes.\n3.1\nMasked Unsupervised Object Localization\nSelf-Training Unsupervised Segmenter.\nWe design an unsupervised segmenter model\nfθ by leveraging the frozen Self-distillation with No Labels (DINO) [7] as an encoder for\nfeature extraction with a lightweight single 1 × 1 convolutional layer decoder having only\n770 learnable parameters. The model is trained end-to-end in a self-supervised fashion, and\nlearns to predict a refined version of its own predictions from the input image I. The pre-\ndicted mask Mp is refined by applying image binarization, followed by bilateral solver [4],\nan edge-aware smoothing technique that enhances mask quality. The refined mask, denoted\n4\nZUNAIR, HAMZA: PEEKABOO: UNSUPERVISED OBJECT LOCALIZATION\nUnsupervised Segmentor\nMasked Feature\n     Predictor\nDINO\nshared weights\nconv 1x1\nconv 1x1\nDINO\nPredictor Consistency \n             Loss\nFigure 1: Overview of PEEKABOO framework for unsupervised object localization.\nThe proposed learning paradigm consists of an Unsupervised Segmentor, a Masked Feature\nPredictor and a Predictor Consistency Loss. Here, fθ is a frozen Self-distillation with No La-\nbels (DINO) encoder [7] paired with a lightweight trainable 1×1 convolutional layer decoder\nhaving only 770 learnable parameters. The two branches are identical and share weights. Af-\nter training, fθ is utilized to generate class-agnostic predicted segmentation masks.\nas ζ(Mp), serves as pseudo ground-truth for self-training, with ζ(·) representing the image\nbinarization and bilateral solver operation. We train fθ by minimizing the segmenter loss\nLseg(Mp,ζ(Mp)), which is the binary cross-entropy between the predicted mask and its re-\nfined version. Application-specific loss functions can also be used in lieu of cross-entropy to\nfurther improve performance depending on the use-case.\nMasked Input.\nIncorporating image masking into context-based representation learning is\nintuitive as the masked image retains some contextual information, requiring the model to\ncapture background knowledge about the image. To facilitate this, we leverage the Irregular\nMask (IMs) dataset [20], which is a collection of masks with random streaks and holes\nof arbitrary shapes commonly used for image inpainting [20, 33]. From this dataset, we\ncreate a subset of images containing only large masks by filtering the masks based on the\npercentage p of zero pixels they contain. Masks with p greater than 50% are included,\nwhile others are discarded. In our experiments, we observe that these large masks generally\nenhance localization performance compared to smaller ones. During PEEKABOO training,\nwe randomly sample a large mask, perform binary thresholding to set pixel values to either 0\nor 1, and denote this mask as Mscribble. We generate a masked image Imasked = I⊙Mscribble,\nwhere I is the original input image and ⊙denotes element-wise multiplication. Essentially,\nthe masked image retains a layout akin to the original input, albeit with substantial portions\nof the pixels removed.\nMasked Feature Predictor (MFP).\nWe introduce MFP to explicitly learn context-based\nrepresentations. MFP is tasked with generating pixel-level semantic labels, distinguishing\nbetween foreground and background, for the masked portions of the image. This prompts\nthe model to learn contextual information by utilizing the available pixel-level details of\nneighboring non-masked pixels to predict the label for each masked pixel. Given the masked\nimage Imasked, we train fθ to produce the predicted mask Mpm. It is important to note that\nZUNAIR, HAMZA: PEEKABOO: UNSUPERVISED OBJECT LOCALIZATION\n5\nfθ is a Siamese network-like architecture [5], where the branches are identical and share\nweights, and is trained by minimizing the binary cross-entropy loss Lmfp(Mpm,ζ(Mpm)).\nPredictor Consistency Loss (PCL).\nThe objective of PCL is to ensure consistency be-\ntween predictions for the input image and its masked version. By aligning these predic-\ntions, PCL learns shape-level representations that complement the pixel-level representations\nlearned by MFP. This alignment is achieved by ensuring that the predictions of the masked\ninput match those of the unmasked input. Since both predictions are generated by the same\nmodel fθ, this fosters invariance to images with partial inputs, ultimately leading to im-\nproved localization performance. More specifically, we maximize the similarity between the\npredictions from the unsupervised segmenter and masked feature predictor by minimizing\nthe L2-loss Lpcl = ∥ζ(Mp)−ζ(Mpm)∥2.\nOverall Loss Function. Using the unsupervised segmenter, masked feature predictor and\npredictor consistency loss, we define the overall loss function as follows:\nLtotal = αLseg(Mp,ζ(Mp))+Lmfp(Mpm,ζ(Mpm))+Lpcl(ζ(Mp),ζ(Mpm)),\n(1)\nwhere the scalar α is a nonnegative trade-off hyperparameter, which controls the contribution\nof the segmenter loss and is set to 3/2. During training, Ltotal is minimized between predic-\ntions and pseudo ground-truth labels using stochastic gradient descent for several epochs to\nlearn the parameters of fθ. Similar to [29, 31], we use only 10,553 images from the DUTS-\nTR dataset [38] for training PEEKABOO. For inference, given a test image I, the trained\nnetwork fθ is employed in unsupervised object localization to obtain segmentation masks.\n4\nExperiments\nIn this section, we evaluate the performance of PEEKABOO in comparison with state-of-the-\nart methods. Details on the implementation, architecture and training, as well as method cost\ndiscussion and additional experimental results are included in the supplementary material.\n4.1\nExperimental Setup\nDatasets.\nFor single object discovery, we conduct experiments on PASCAL VOC07 [14]\nand VOC12 [13], which consist images of single large object, and also on COCO20K [19],\nwhich includes two and often dozens of objects. For the unsupervised saliency detection task,\nwe evaluate the performance of our approach on three popular saliency detection benchmark\ndatasets: DUT-OMRON [43], DUTS-TE [38] and ECSSD [28]. These datasets encompass a\ndiverse array of objects set against various backgrounds.\nBaselines.\nWe evaluate the performance of our method against several state-of-the-art\ntraining-free methods, including LOST [30], TokenCut [40] and Deep Spectral Methods [24].\nWe also compare against training-based generative methods like BigBiGAN [37] and Melas-\nKyriazi et al. [23], along with Transformer-based methods such as FOUND [31], Self-\nMask [29], FreeSOLO [39], and DINOSAUR [27]. Furthermore, we compare against more\nrecent methods such as WSCUOD [22] and DeepCut [1]. Notably, these recent approaches\ntypically entail multiple stages of training, learning several millions of parameters, test-time\ntraining, combining multiple learnable networks, model ensembling, and require training on\nhundreds of thousands of images.\n6\nZUNAIR, HAMZA: PEEKABOO: UNSUPERVISED OBJECT LOCALIZATION\nEvaluation Metrics.\nFor single object discovery, we report results using the Correct Local-\nization (CorLoc) metric [30, 31], which measures the percentage of correct boxes, where pre-\ndicted boxes have an intersection-over-union (IoU) greater than 0.5 with one of the ground-\ntruth boxes. For unsupervised saliency detection, we report results in terms of IoU, pixel\nwise accuracy (Acc) and maximal Fβ score (max Fβ).\n4.2\nResults and Analysis\nSingle Object Discovery Results.\nWe evaluate the performance of PEEKABOO against\nseveral state-of-the-art methods on three single object discovery datasets, and the results are\nsummarized in Table 1. We use PEEKABOO with a ViT-S/8 [12] architecture pre-trained\nwith DINO [7]. As reported in Table 1, PEEKABOO demonstrates superior performance\ncompared to all training-free methods, including LOST [30], TokenCut [40], and DSM [24].\nMoreover, PEEKABOO maintains computational efficiency by avoiding the computation of\neigenvectors, thereby ensuring faster inference [24, 40]. This demonstrates the efficiency of\nPEEKABOO for real-time object discovery. PEEKABOO also performs better than training-\nbased methods (SelfMask [29], FreeSOLO [39], FOUND [31], WSCUOD [22] and Deep-\nCut [1]), while being notably simpler to train. In addition, it outperforms DINOSAUR [27]\non VOC07 and VOC12, while DINOSAUR [27] performs better on COCO20K. However,\nit is worth noting that DINOSAUR [27] incurs an extensive training cost, as it employs a\nsignificantly larger ViT-based architecture trained on over three hundred thousand images\nsourced from both synthetic and real-world datasets. Evidently, PEEKABOO outperforms\nDINOSAUR [27] on VOC12, achieving a relative improvement of 7.8% in terms of Cor-\nLoc. It also performs better than DeepCut [1], which employs multiple learnable networks\n(ViTs and GNNs) and involves test-time training. PEEKABOO stands out for its accuracy\nand efficiency, boasting approximately 42,857 times fewer learnable parameters compared to\nSelfMask [29], which relies on an ensemble of three self-supervised vision encoders. Sim-\nilarly, it is significantly more parameter-efficient than FreeSOLO [39], which employs over\ntwo hundred thousand images and 66 million learnable parameters, roughly 24 and 85,714\ntimes higher than our method, respectively. These results underscore the accuracy and effi-\nciency of our method in single object discovery.\nUnsupervised Saliency Detection Results.\nIn Table 2, we present the performance com-\nparison across three popular saliency detection benchmarks. PEEKABOO demonstrates\nsuperiority over training-based techniques such as BigBiGAN [37] and Melas-Kyriazi et\nal.[23], which rely on large-scale adversarial training. Notably, PEEKABOO consistently\noutperforms training-free methods, including LOST[30], TokenCut [40], and DSM [24]\nacross all evaluation metrics. Interestingly, PEEKABOO achieves these results with com-\nputational efficiency, even when compared to approaches incorporating bilateral solver post-\nprocessing [4]. Moreover, it demonstrates superior performance compared to SelfMask [29],\nexcept for IoU on DUT-OMRON, despite the substantial training cost associated with Self-\nMask. This cost stems from its ensemble approach, utilizing three self-supervised vision\nencoders (SwAV [6], DINO [7], MoCov2 [10]), and generating twenty masks for each im-\nage during training. Furthermore, PEEKABOO outperforms DeepCut [1], which employs\nmultiple learnable networks (ViTs and GNNs) and requires test-time training. It also con-\nsistently outperforms WSCUOD [22] across all datasets and evaluation metrics, even when\nWSCUOD employs bilateral solver post-processing. Notably, WSCUOD utilizes weak su-\npervisory signals for training, whereas PEEKABOO operates directly on unlabeled images\nZUNAIR, HAMZA: PEEKABOO: UNSUPERVISED OBJECT LOCALIZATION\n7\nTable 1: Performance comparison of our method and state-of-the-art methods in single\nobject discovery using CorLoc as evaluation metric. The best results are in bold, while\nthe second-best are underlined. Higher values indicate better results. “+CAD” denotes an\nadditional second-stage class-agnostic detector trained with unsupervised pseudo-box labels.\nMethod\nLearning VOC07\nVOC12\nCOCO20K\nZhang et al. [46]\n46.2\n50.5\n34.8\nDDT+ [41]\n50.2\n53.1\n38.2\nrOSD [35]\n54.5\n55.3\n48.5\nLOD [36]\n53.6\n55.1\n48.5\nDINO [7]\n45.8\n46.2\n42.1\nLOST [30] (ViT-S/16)\n61.9\n64.0\n50.7\nLOST + CAD [30]\n65.7\n70.4\n57.5\nDSM [24] (ViT-S/16)\n62.7\n66.4\n52.2\nTokenCut [40] (ViT-S/16)\n68.8\n72.1\n58.8\nTokenCut + CAD [40]\n71.4\n75.3\n62.6\nSelfMask [29]\n✓\n72.3\n75.3\n62.7\nFOUND† [31]\n✓\n71.7\n75.6\n61.1\nFreeSOLO [39]\n✓\n56.1\n56.7\n52.8\nDeepCut [1]\n✓\n69.8\n72.2\n61.6\nWSCUOD [22]\n✓\n70.6\n72.1\n63.5\nDINOSAUR [27]\n✓\n-\n70.4\n67.2\nPEEKABOO (ViT-S/8) (Ours)\n✓\n72.7\n75.9\n64.0\nand has significantly fewer learnable parameters, approximately 2597 times less. In addition,\neven with bilateral solver post-processing, PEEKABOO maintains its superior performance,\ndemonstrating its ability to produce high-quality object masks without heavy reliance on\npost-processing techniques for enhanced results.\nQualitative Results.\nIn Figure 2, we present visual examples showcasing the predictions\ngenerated by our PEEKABOO model and the FOUND [31] baseline. The first two rows,\nfeaturing samples from the ECSSD dataset, illustrate that the baseline tends to over-segment\nsalient objects and struggles with cases involving reflections on water and shiny surfaces,\nresulting in nearly duplicated segmentation. Moving to the middle two rows, which depict\nsamples from the DUT-OMRON dataset, we observe the baseline’s challenges with complex\nbackgrounds, dark scenes, and small objects, along with its tendency to segment non-salient\nregions and produce noisy predictions. Finally, the last two rows, representing instances\nfrom the DUTS-TE dataset, highlight the baseline’s difficulty in localizing small objects and\nits tendency to generate noisy and discontinuous predictions. By comparison, PEEKABOO\nexcels in localizing salient objects under various conditions, including when they are small,\nreflecting on water or shiny surfaces, or in complex or low-illumination backgrounds. More-\nover, PEEKABOO avoids common pitfalls such as over-segmentation of salient objects,\nsegmentation of non-salient regions, and generation of noisy predictions. This is because\nMFP operates on partial inputs through masking, thereby generating context-based pixel-\nlevel representations. PCL, on the other hand, complements MFP by learning shape-level\nrepresentations through aligning predictions from both unmasked and masked inputs.\n8\nZUNAIR, HAMZA: PEEKABOO: UNSUPERVISED OBJECT LOCALIZATION\nTable 2: Performance comparison of our method and state-of-the-art methods in un-\nsupervised saliency detection. “+BS” and “+CRF” indicate the utilization of the post-\nprocessing techniques [4] and [17], respectively. The symbol † denotes results reproduced\nusing the publicly available codes.\nDUT-OMRON\nDUTS-TE\nECSSD\nMethod\nLearning Acc\nIoU\nmax Fβ\nAcc\nIoU\nmax Fβ\nAcc\nIoU\nmax Fβ\nHS [42]\n84.3 43.3\n56.1\n82.6\n36.9\n50.4\n84.7\n50.8\n67.3\nwCtr [47]\n83.8 41.6\n54.1\n83.5\n39.2\n52.2\n86.2\n51.7\n68.4\nWSC [18]\n86.5 38.7\n52.3\n86.2\n38.4\n52.8\n85.2\n49.8\n68.3\nDeepUSPS [25]\n77.9 30.5\n41.4\n77.3\n30.5\n42.5\n79.5\n44.0\n58.4\nBigBiGAN [37]\n85.6 45.3\n54.9\n87.8\n49.8\n60.8\n89.9\n67.2\n78.2\nE-BigBiGAN[37]\n86.0 46.4\n56.3\n88.2\n51.1\n62.4\n90.6\n68.4\n79.7\nMelas-Kyriazi et al. [23]\n88.3 50.9 -\n89.3\n52.8\n-\n91.5\n71.3\n-\nLOST [30]\n79.7 41.0\n47.3\n87.1\n51.8\n61.1\n89.5\n65.4\n75.8\nDSM [24]\n80.8 42.8\n55.3\n84.1\n47.1\n62.1\n86.4\n64.5\n78.5\nTokenCut [40]\n88.0 53.3\n60.0\n90.3\n57.6\n67.2\n91.8\n71.2\n80.3\nSelfMask [29]\n✓\n90.1\n58.2\n-\n92.3\n62.6\n-\n94.4\n78.1\n-\nFOUND† [31]\n✓\n90.7\n57.1\n79.9\n93.5\n63.7\n85.2\n94.9\n80.6\n95.1\nDeepCut [1]\n✓\n-\n-\n-\n-\n59.5\n-\n-\n74.6\n-\nWSCUOD [22]\n✓\n89.7 53.6\n64.4\n91.7\n59.9\n73.1\n92.2\n72.7\n85.4\nPEEKABOO (Ours)\n✓\n91.5\n57.5\n80.4\n93.9\n64.3\n86.0\n94.6\n79.8\n95.3\nLOST + BS [30]\n✓\n81.8 48.9\n57.8\n88.7\n57.2\n69.7\n91.6\n72.3\n83.7\nDSM + CRF [24]\n✓\n87.1 56.7\n64.4\n83.8\n51.4\n56.7\n89.1\n73.3\n80.5\nWSCUOD + BS [22]\n✓\n90.9 58.5\n68.3\n92.5\n63.0\n76.4\n92.8\n74.2\n89.6\nTokenCut + BS [40]\n✓\n89.7 61.8\n69.7\n91.4\n62.4\n75.5\n93.4\n77.2\n87.4\nSelfMask + BS [29]\n✓\n91.9\n65.5\n-\n93.3\n66.0\n-\n95.5\n81.8\n-\nFOUND + BS† [31]\n✓\n91.7 60.9 69.1\n94.0\n66.1\n75.0\n95.2\n81.7\n93.0\nPEEKABOO + BS (Ours)\n✓\n92.4\n61.2\n71.4\n94.4\n66.3\n77.4\n94.9\n80.6\n93.7\n4.3\nAblation Study\nWe analyze how each of the key components of the proposed framework affects the overall\nperformance. We also examine the impact of masking on model performance.\nEffectiveness of Masked Feature Predictor (MFP).\nFigure 3 (left) illustrates the benefit\nof using MFP, which helps capture local semantics by generating pixel-level predictions\nfor masked portions of the image. This enables the model to learn context by leveraging\ninformation from nearby non-masked pixels to make a prediction for the masked pixel. The\neffectiveness of MFP is evident in the improved localization performance across all datasets.\nParticularly noteworthy is the significant performance improvement observed on COCO20K,\nwhich contains a diverse range of images, including those with multiple objects, compared\nto the VOC07 and VOC12 datasets, which primarily consist of single large objects.\nEffectiveness of Predictor Consistency Loss (PCL).\nAs depicted in Figure 3 (left), PCL\ncontributes to performance improvement across all datasets. By aligning the predictions\nof the input and its masked counterpart, we can learn representations that are invariant to\nmasking. This enables the model to capture the shapes of objects to be segmented, resulting\nin more accurate predictions. Notably, the most substantial improvement is observed on\nCOCO20K, underscoring the potential of PEEKABOO in scenarios featuring diverse images\nZUNAIR, HAMZA: PEEKABOO: UNSUPERVISED OBJECT LOCALIZATION\n9\nImage\nFOUND\nOurs\nGround Truth\nImage\nFOUND\nOurs\nGround Truth\nImage\nFOUND\nOurs\nGround Truth\nFigure 2: Visual comparison of PEEKABOO and state-of-the-art FOUND [31] on EC-\nSSD, DUT-OMRON and DUTS-TE datasets. Across all datasets, PEEKABOO excels\nin localizing salient objects, particularly when they are small, reflective, or situated against\ncomplex or dimly illuminated backgrounds. Zoom in to observe the results more closely.\nVOC07\nVOC12\nCOCO20K\n60.0\n62.5\n65.0\n67.5\n70.0\n72.5\n75.0\n77.5\n80.0\nCorLoc (%)\n71.6\n75.2\n61.8\n72.2\n75.5\n62.3\n72.7\n75.9\n64.0\nEffectiveness of MFP and PCL ( )\nBaseline\nw/ MFP\nw/ MFP + PCL\nVOC07\nVOC12\nCOCO20K\n60.0\n62.5\n65.0\n67.5\n70.0\n72.5\n75.0\n77.5\n80.0\n71.7\n75.6\n62.0\n72.7\n75.9\n64.0\nImpact of masking ( )\nlow masking\nhigh masking\nFigure 3: Ablation study of different modules of PEEKABOO (left) and impact of masking\n(right) using VOC07, VOC12 and COCO20K datasets. MFP and PCL consistently help\nimprove performance. PEEKABOO with high masked pixels yields better performance.\nwith multiple objects.\nImpact of Masking.\nIn Figure 3 (right), we illustrate the impact of masking on model\nperformance during the training of PEEKABOO. Our training strategy involves utilizing\nsubsets of high and low masks, where Mscribble in the subset either comprises large or small\nportions of zero pixels. We observe that localization performance generally improves when\na large portion of the image is masked. This observation aligns with intuition since low\nmasking preserves most of the information, leading the model to learn redundant features.\n5\nDiscussion and Limitations\nRelation to Self-Supervised Learning via Dual Networks.\nSelf-supervised learning\n(SSL) serves as a pre-training strategy aimed at mitigating the reliance on labeled data for\nsupervised learning. Typically, SSL involves a pretext task that exploits the inherent seman-\ntics and structure of unlabeled data to learn the relationship between input and pseudo-label\n(i.e., image representations), easily generated from the input itself. Numerous SSL meth-\n10\nZUNAIR, HAMZA: PEEKABOO: UNSUPERVISED OBJECT LOCALIZATION\nods [3, 6, 8, 9, 15, 45] rely on Siamese networks [5] to encourage the similarity between two\noutputs, showcasing the effectiveness of self-supervised pre-training in various target tasks,\nincluding image classification, segmentation, and detection [3].\nWhile this similarity concept resembles the predictor consistency loss introduced in\nPEEKABOO, there is a key distinction in the type of output: SSL methods yield latent rep-\nresentations (i.e., feature vectors), whereas PEEKABOO outputs prediction labels (i.e., seg-\nmentation maps). Moreover, PEEKABOO differs from SSL methods in a number of aspects.\nFirst, SSL methods typically involve a pre-training stage on large amounts of unlabeled data\nfollowed by fine-tuning for the target task. In contrast, PEEKABOO directly applies to the\ntarget task of unsupervised object localization. Second, while SSL methods operate on two\nrandomly augmented samples of the image through rotation, cropping, translation, or blur-\nring [3, 8, 45], PEEKABOO works with the original image and its masked version. Third,\nunlike methods such as BYOL [15], SwAV [6] and SimSiam [9], which employ two indepen-\ndent networks operating on the two samples, PEEKABOO utilizes a Siamese network with\nshared weights, where both networks are identical. Fourth, PEEKABOO employs a com-\npletely different objective function compared to existing methods: SimCLR [8] uses a con-\ntrastive loss requiring both positive and negative pairs, Barlow Twins [45] and VICReg [3]\nuse a cross-correlation matrix based loss, while BYOL [15], SwAV [6] and SimSiam [9]\nuse a cosine similarity loss. In summary, PEEKABOO aims to unify the concepts of self-\nsupervised pretraining [3, 8, 9, 45] on unlabeled data and fine-tuning on a target task into\na single framework that models contextual relationship among pixels and learns semantics\nand structure through image masking. This approach potentially avoids the discrepancies\ninvolved in separate pre-training and fine-tuning stages.\nFigure 4: Visualization of failure cases of\nPEEKABOO on DUT-OMRON for object\nlocalization. No refinement step is applied.\nZoom in to observe the results more closely.\nLimitations.\nFigure 4 illustrates some\ninstances where PEEKABOO encounters\nchallenges. One notable limitation is ob-\nserved in indoor scenes, where PEEK-\nABOO struggles to accurately segment\nsalient objects. Moreover, in cases where\nsalient objects, such as trees, are easily dis-\ntinguishable visually, PEEKABOO may ex-\nhibit difficulties in segmentation, suggest-\ning potential over-generalization issues.\n6\nConclusion\nWe introduced PEEKABOO, a new single-stage learning paradigm that models contextual\nrelationship among pixels through image masking for unsupervised object localization. The\nproposed framework learns context-based representations at the pixel-level by making pre-\ndictions on masked images and at the shape-level by matching the predictions of the masked\ninput to the unmasked one. Through extensive experiments, our quantitative and qualita-\ntive results demonstrated that our approach outperforms state-of-the-art methods, excelling\nin localizing salient objects, especially when they are small, reflect on water or shiny sur-\nfaces, or when the background is complex or poorly illuminated. Notably, our method avoids\nover-segmenting salient objects, segmenting non-salient regions, and producing noisy pre-\ndictions. For future work, we aim to explore learning context-based representations for direct\nlocalization of individual concepts in open-vocabulary semantic segmentation.\nZUNAIR, HAMZA: PEEKABOO: UNSUPERVISED OBJECT LOCALIZATION\n11\nAcknowledgments.\nThis work was supported in part by the Discovery Grants Program\nof the Natural Sciences and Engineering Research Council of Canada under grant RGPIN-\n2024-04291.\nReferences\n[1] Amit Aflalo, Shai Bagon, Tamar Kashti, and Yonina Eldar. DeepCut: Unsupervised\nsegmentation using graph neural networks clustering.\nIn Proc. IEEE International\nConference on Computer Vision, pages 32–41, 2023.\n[2] Hangbo Bao, Li Dong, and Furu Wei. BEiT: BERT pre-training of image transformers.\nIn International Conference on Learning Representations, 2022.\n[3] Adrien Bardes, Jean Ponce, and Yann LeCun.\nVICReg:\nVariance-invariance-\ncovariance regularization for self-supervised learning. In International Conference on\nLearning Representations, 2021.\n[4] Jonathan T Barron and Ben Poole. The fast bilateral solver. In Proc. European Confer-\nence on Computer Vision, pages 617–632, 2016.\n[5] Jane Bromley, Isabelle Guyon, Yann LeCun, Eduard Säckinger, and Roopak Shah.\nSignature verification using a \"siamese\" time delay neural network. In Advances in\nNeural Information Processing Systems, 1993.\n[6] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Ar-\nmand Joulin. Unsupervised learning of visual features by contrasting cluster assign-\nments. In Advances in Neural Information Processing Systems, 2020.\n[7] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bo-\njanowski, and Armand Joulin. Emerging properties in self-supervised vision transform-\ners. In Proc. IEEE International conference on Computer Vision, pages 9650–9660,\n2021.\n[8] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple\nframework for contrastive learning of visual representations. In International Confer-\nence on Machine Learning, pages 1597–1607. PMLR, 2020.\n[9] Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In\nProc. IEEE Conference on Computer Vision and Pattern Recognition, pages 15750–\n15758, 2021.\n[10] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with\nmomentum contrastive learning. arXiv preprint arXiv:2003.04297, 2020.\n[11] Jeff Donahue and Karen Simonyan. Large scale adversarial representation learning. In\nAdvances in Neural Information Processing Systems, 2019.\n[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiao-\nhua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold,\nSylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words:\nTransformers for image recognition at scale. In International Conference on Learning\nRepresentations, 2021.\n12\nZUNAIR, HAMZA: PEEKABOO: UNSUPERVISED OBJECT LOCALIZATION\n[13] M Everingham, S.M. Eslami, L Van Gool, CKI Williams, J Winn, and A Zisserman.\nThe pascal visual object classes challenge: A retrospective. International Journal of\nComputer Vision, 111:98–136, 2015.\n[14] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew\nZisserman. The pascal visual object classes (VOC) challenge. International Journal of\nComputer Vision, 88:303–338, 2010.\n[15] Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond,\nElena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad\nGheshlaghi Azar, et al. Bootstrap your own latent: a new approach to self-supervised\nlearning. In Advances in Neural Information Processing Systems, volume 33, pages\n21271–21284, 2020.\n[16] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick.\nMasked autoencoders are scalable vision learners. In Proc. IEEE Conference on Com-\nputer Vision and Pattern Recognition, pages 16000–16009, 2022.\n[17] Philipp Krähenbühl and Vladlen Koltun. Efficient inference in fully connected CRFs\nwith Gaussian edge potentials. In Advances in Neural Information Processing Systems,\n2011.\n[18] Nianyi Li, Bilin Sun, and Jingyi Yu. A weighted sparse coding framework for saliency\ndetection. In Proc. IEEE Conference on Computer Vision and Pattern Recognition,\npages 5216–5223, 2015.\n[19] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ra-\nmanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft COCO: Common objects in\ncontext. In Proc. European Conference on Computer Vision, pages 740–755, 2014.\n[20] Guilin Liu, Fitsum A Reda, Kevin J Shih, Ting-Chun Wang, Andrew Tao, and Bryan\nCatanzaro. Image inpainting for irregular holes using partial convolutions. In Proc.\nEuropean Conference on Computer Vision, pages 85–100, 2018.\n[21] Lizhao Liu, Zhuangwei Zhuang, Shangxin Huang, Xunlong Xiao, Tianhang Xiang,\nCen Chen, Jingdong Wang, and Mingkui Tan. CPCM: Contextual point cloud modeling\nfor weakly-supervised point cloud semantic segmentation. In Proc. IEEE International\nConference on Computer Vision, pages 18413–18422, 2023.\n[22] Yunqiu Lv, Jing Zhang, Nick Barnes, and Yuchao Dai. Weakly-supervised contrastive\nlearning for unsupervised object discovery. arXiv preprint arXiv:2307.03376, 2023.\n[23] Luke Melas-Kyriazi, Christian Rupprecht, Iro Laina, and Andrea Vedaldi. Finding an\nunsupervised image segmenter in each of your deep generative models. In International\nConference on Learning Representations, 2022.\n[24] Luke Melas-Kyriazi, Christian Rupprecht, Iro Laina, and Andrea Vedaldi. Deep spec-\ntral methods: A surprisingly strong baseline for unsupervised semantic segmentation\nand localization. In Proc. IEEE Conference on Computer Vision and Pattern Recogni-\ntion, pages 8364–8375, 2022.\nZUNAIR, HAMZA: PEEKABOO: UNSUPERVISED OBJECT LOCALIZATION\n13\n[25] Tam Nguyen, Maximilian Dax, Chaithanya Kumar Mummadi, Nhung Ngo, Thi\nHoai Phuong Nguyen, Zhongyu Lou, and Thomas Brox. DeepUSPS: Deep robust un-\nsupervised saliency prediction via self-supervision. In Advances in Neural Information\nProcessing Systems, 2019.\n[26] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A Efros.\nContext encoders: Feature learning by inpainting. In Proc. IEEE Conference on Com-\nputer Vision and Pattern Recognition, pages 2536–2544, 2016.\n[27] Maximilian Seitzer, Max Horn, Andrii Zadaianchuk, Dominik Zietlow, Tianjun Xiao,\nCarl-Johann Simon-Gabriel, Tong He, Zheng Zhang, Bernhard Schölkopf, Thomas\nBrox, et al. Bridging the gap to real-world object-centric learning. In International\nConference on Learning Representations, 2023.\n[28] Jianping Shi, Qiong Yan, Li Xu, and Jiaya Jia. Hierarchical image saliency detection\non extended CSSD. IEEE Transactions on Pattern Analysis and Machine Intelligence,\n38(4):717–729, 2015.\n[29] Gyungin Shin, Samuel Albanie, and Weidi Xie. Unsupervised salient object detection\nwith spectral cluster voting. In Proc. IEEE Conference on Computer Vision and Pattern\nRecognition, pages 3971–3980, 2022.\n[30] Oriane Siméoni, Gilles Puy, Huy V Vo, Simon Roburin, Spyros Gidaris, Andrei Bursuc,\nPatrick Pérez, Renaud Marlet, and Jean Ponce. Localizing objects with self-supervised\ntransformers and no labels. In Proc. British Machine Vision Conference, 2021.\n[31] Oriane Siméoni, Chloé Sekkat, Gilles Puy, Antonín Vobeck`y, Éloi Zablocki, and\nPatrick Pérez. Unsupervised object localization: Observing the background to dis-\ncover objects. In Proc. IEEE Conference on Computer Vision and Pattern Recognition,\npages 3176–3186, 2023.\n[32] Yeonghwan Song, Seokwoo Jang, Dina Katabi, and Jeany Son. Unsupervised object\nlocalization with representer point selection. In Proc. IEEE International Conference\non Computer Vision, pages 6534–6544, 2023.\n[33] Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin, Anastasia Remizova, Arsenii\nAshukha, Aleksei Silvestrov, Naejin Kong, Harshith Goka, Kiwoong Park, and Victor\nLempitsky. Resolution-robust large mask inpainting with fourier convolutions. In Proc.\nIEEE Winter Conference on Applications of Computer Vision, pages 2149–2159, 2022.\n[34] Jasper RR Uijlings, Koen EA Van De Sande, Theo Gevers, and Arnold WM Smeulders.\nSelective search for object recognition. International Journal of Computer Vision, 104:\n154–171, 2013.\n[35] Huy V Vo, Patrick Pérez, and Jean Ponce. Toward unsupervised, multi-object discovery\nin large-scale image collections. In Proc. European Conference on Computer Vision,\npages 779–795, 2020.\n[36] Van Huy Vo, Elena Sizikova, Cordelia Schmid, Patrick Pérez, and Jean Ponce. Large-\nscale unsupervised object discovery. In Advances in Neural Information Processing\nSystems, 2021.\n14\nZUNAIR, HAMZA: PEEKABOO: UNSUPERVISED OBJECT LOCALIZATION\n[37] Andrey Voynov, Stanislav Morozov, and Artem Babenko. Object segmentation with-\nout labels with large-scale generative models. In Proc. International Conference on\nMachine Learning, pages 10596–10606, 2021.\n[38] Lijun Wang, Huchuan Lu, Yifan Wang, Mengyang Feng, Dong Wang, Baocai Yin, and\nXiang Ruan. Learning to detect salient objects with image-level supervision. In Proc.\nIEEE Conference on Computer Vision and Pattern Recognition, pages 136–145, 2017.\n[39] Xinlong Wang, Zhiding Yu, Shalini De Mello, Jan Kautz, Anima Anandkumar, Chun-\nhua Shen, and Jose M Alvarez. FreeSOLO: Learning to segment objects without anno-\ntations. In Proc. IEEE Conference on Computer Vision and Pattern Recognition, pages\n14176–14186, 2022.\n[40] Yangtao Wang, Xi Shen, Shell Xu Hu, Yuan Yuan, James L Crowley, and Dominique\nVaufreydaz. Self-supervised transformers for unsupervised object discovery using nor-\nmalized cut. In Proc. IEEE Conference on Computer Vision and Pattern Recognition,\npages 14543–14553, 2022.\n[41] Xiu-Shen Wei, Chen-Lin Zhang, Jianxin Wu, Chunhua Shen, and Zhi-Hua Zhou. Un-\nsupervised object discovery and co-localization by deep descriptor transformation. Pat-\ntern Recognition, 88:113–126, 2019.\n[42] Qiong Yan, Li Xu, Jianping Shi, and Jiaya Jia. Hierarchical saliency detection. In\nProc. IEEE Conference on Computer Vision and Pattern Recognition, pages 1155–\n1162, 2013.\n[43] Chuan Yang, Lihe Zhang, Huchuan Lu, Xiang Ruan, and Ming-Hsuan Yang. Saliency\ndetection via graph-based manifold ranking. In Proc. IEEE Conference on Computer\nVision and Pattern Recognition, pages 3166–3173, 2013.\n[44] Andrii Zadaianchuk, Matthaeus Kleindessner, Yi Zhu, Francesco Locatello, and\nThomas Brox. Unsupervised semantic segmentation with self-supervised object-centric\nrepresentations. In International Conference on Learning Representations, 2023.\n[45] Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and Stéphane Deny. Barlow Twins:\nSelf-supervised learning via redundancy reduction. In International Conference on\nMachine Learning, pages 12310–12320. PMLR, 2021.\n[46] Runsheng Zhang, Yaping Huang, Mengyang Pu, Jian Zhang, Qingji Guan, Qi Zou,\nand Haibin Ling. Object discovery from a single unlabeled image by mining frequent\nitemsets with multi-scale features. IEEE Transactions on Image Processing, 29:8606–\n8621, 2020.\n[47] Wangjiang Zhu, Shuang Liang, Yichen Wei, and Jian Sun. Saliency optimization from\nrobust background detection. In Proc. IEEE Conference on Computer Vision and Pat-\ntern Recognition, pages 2814–2821, 2014.\n[48] C Lawrence Zitnick and Piotr Dollár. Edge boxes: Locating object proposals from\nedges. In Proc. European Conference on Computer Vision, pages 391–405, 2014.\n[49] Hasib Zunair and A Ben Hamza. Learning to recognize occluded and small objects with\npartial inputs. In Proc. IEEE/CVF Winter Conference on Applications of Computer\nVision, pages 675–684, 2024.\nZUNAIR, HAMZA: PEEKABOO: UNSUPERVISED OBJECT LOCALIZATION\n1\n—– Supplementary Material —–\n7\nImplementation Details\nData preprocessing.\nImages and masks are resized to 224 × 224 and normalized using\nmean and standard deviation of ImageNet. Similar to SelfMask [29], we apply basic data\naugmentation techniques, including random scaling within the range of [0.1,3.0] and Gaus-\nsian blurring with a probability of 0.5. The parameters of the bilateral solver are the same as\nthose provided in [4].\nArchitecture.\nWe construct PEEKABOO by using a frozen ViT-S/8 [12] architecture pre-\ntrained using DINO [7] as an encoder for feature extraction, from the last attention layer,\nwith a lightweight segmenter head that is a single 1×1 convolutional layer decoder having\nonly 770 learnable parameters.\nModel Training.\nPEEKABOO is trained in a single stage, requiring only a collection of\nimages. We use the Adam optimizer with a learning rate schedule to minimize the total loss\nfunction. In addition to the total loss, we also compute the binary cross-entropy loss between\nthe raw predicted masks from the unsupervised segmenter branch and their binarized version.\nThis encourages the predicted soft masks to closely resemble their binarized counterparts.\nWe set the trade-off hyperparameter to 4. The model is trained for 500 iterations on only\n10,553 images from the DUTS-TR dataset [38] with a batch size of 50, which corresponds\nto slightly more than 2 epochs. For image masking, we utilize the Irregular Mask (IMs)\ndataset [20], containing masks with random streaks and holes of various shapes, as illustrated\nin Figure 5.\nModel Testing.\nAfter training, given an input image, the model simply makes a prediction\nby outputting a segmentation mask for the salient object(s). During inference, the input im-\nage undergoes normalization only. Moreover, there is no random masking procedure applied,\nas is done during the training stage.\nHardware and software details.\nThe experiments were performed on a Linux workstation\nrunning 4.8Hz and 64GB RAM, equipped with a single NVIDIA RTX 3080Ti GPU featuring\n12GB of memory. All algorithms are implemented using the PyTorch framework.\nFigure 5: Visualization of masks during training in PEEKABOO. Some masks cover\nmore than 50% of the image. Images are from Irregular Masks Dataset [20] after applying\nbinary thresholding.\n2\nZUNAIR, HAMZA: PEEKABOO: UNSUPERVISED OBJECT LOCALIZATION\n8\nAdditional Visual Comparison Results\nIn Figure 6, we present additional experimental results on unsupervised object localization\nto further demonstrate the effectiveness of PEEKABOO in localizing salient objects. As can\nbe see, PEEKABOO consistently excels in localizing salient objects across all datasets. Its\nperformance is particularly noteworthy when dealing with small objects, reflective surfaces,\nand objects situated against complex or dimly illuminated backgrounds. This capability\nhighlights PEEKABOO’s robustness and adaptability in various challenging scenarios, fur-\nther validating its superiority over strong baselines in unsupervised object localization tasks.\nImage\nFOUND\nOurs\nGround Truth\nImage\nFOUND\nOurs\nGround Truth\nImage\nFOUND\nOurs\nGround Truth\nFigure 6: More examples of visual comparison of PEEKABOO and state-of-the-art\nFOUND [31] on ECSSD, DUT-OMRON and DUTS-TE datasets. Across all datasets,\nPEEKABOO excels in localizing salient objects, particularly when they are small, reflective,\nor situated against complex or dimly illuminated backgrounds. Zoom in to observe the results\nmore closely.\n9\nMethod Cost Discussion\nWe compare PEEKABOO against training-free and training-based methods, which we find\nhave significantly different costs at both training and inference time. Specifically, we demon-\nstrate the efficiency of our method. PEEKABOO is a segmenter head, on top of a frozen Self-\ndistillation with No Labels (DINO) [7] as an encoder, which consists of a lightweight single\n1 × 1 convolutional layer decoder having only 770 learnable parameters. The model is\ntrained for 2 epochs on 10,553 images from the DUTS-TR dataset [38] on a single con-\nsumer grade NVIDIA RTX 3080Ti GPU.\nInference with training-free methods like TokenCut [40] and Deep Spectral Methods\n(DSMs)[24] is slowed down due to the expensive computation of the Laplacian matrix eigen-\nvectors. LOST[30], while somewhat faster, notably lags behind PEEKABOO in terms of\nlocalization performance.\nAmong training-based methods, FreeSOLO [39] stands out with approximately 66 mil-\nlion learnable parameters, trained over 241 thousand unlabeled images for 60 thousand iter-\nations across 8 GPUs, making its training considerably more resource-intensive compared to\nZUNAIR, HAMZA: PEEKABOO: UNSUPERVISED OBJECT LOCALIZATION\n3\nours. Also, its backbone is pretrained on ImageNet with 1.28 million unlabeled images. Self-\nMask [29] utilizes 36 million learnable parameters and trains for 12 epochs on the DUTS-\nTR dataset [38]. COMUS [44] requires three days of training on two 8-GPU servers for its\nheavy segmentation backbone. DINOSAUR [27] is trained on over 300 thousand images\nfrom synthetic and real-world sources and demands 8 GPUs for training. DeepCut [1] has\n30K learnable parameters, and WSCUOD [22], which incorporates a DINO-ViT-S/16 back-\nbone, consists of 2 million learnable parameters and requires training on 6 GPUs, making it\nsubstantially more expensive to train compared to our method.\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2024-07-24",
  "updated": "2024-07-24"
}